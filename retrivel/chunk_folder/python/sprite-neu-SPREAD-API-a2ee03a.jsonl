{"filename": "print_dataset.py", "chunked_list": ["\"\"\"\n\tPrint various information about the dataset and its recordings\n\t\"\"\"\n\timport argparse\n\timport os\n\tfrom core import *\n\timport logging\n\tformatter = logging.Formatter('%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s')\n\tlogging.basicConfig(level=logging.DEBUG,\n\t                    format='%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s')\n", "log = logging.getLogger('spread')\n\tlog.setLevel(logging.INFO)\n\tdef process_info(dataset, recordings_list, filters, table_of_contents=False, no_show=False, recount_pictures=False):\n\t    \"\"\"\n\t    Process the dataset and print information\n\t    @param dataset: Root directory of the dataset\n\t    @param recordings_list: List of recordings to print (eg: [rec_23 rec_52 syn_104 noi_2])\n\t    @param filters: List of space separated filters for recordings (eg: [class=wifi channel=13])\n\t    @param table_of_contents: Boolean, print out the table of contents and save it to files\n\t    @param no_show: Boolean, only save the contents; don't print them\n", "    @param recount_pictures: Recount pictures for all recordings\n\t    @return: None\n\t    \"\"\"\n\t    # Initialize dataset\n\t    ds = Dataset(dataset, recount_pictures=recount_pictures)\n\t    if table_of_contents:\n\t        if not no_show:\n\t            log.info(\"\\n\\nTable of Contents\\n\\n%s\", ds.cont_table.get_table_str())\n\t        ds.cont_table.save_to_csv()\n\t        ds.cont_table.save_to_json()\n", "        ds.cont_table.save_reclist_to_json()\n\t        log.info(\"Dataset information are stored in global metadata: %s\", ds.metadata_dir)\n\t    # Print info about a list of recordings\n\t    if recordings_list:\n\t        for rec in recordings_list:\n\t            rec = Dataset.get_rec_name(rec)\n\t            try:\n\t                ds.recordings_dict[rec].print_info()\n\t            except KeyError:\n\t                log.error(\"Recording %s not found\", rec)\n", "    # Print info about filtered recordings based on given properties\n\t    if filters:\n\t        filtered = ds.filter_recordings(filters)\n\t        for rec in filtered:\n\t            rec.print_info()\n\t        log.info(\"List of filtered recordings: %s\", ' '.join([x.name for x in filtered]))\n\tdef main():\n\t    \"\"\"\n\t    Parse arguments\n\t    \"\"\"\n", "    parser = argparse.ArgumentParser(description=\"Print information about the dataset or specific recordings\")\n\t    parser.add_argument(\"--dataset\", required=True,\n\t                        help=\"Root directory of the dataset.\")\n\t    parser.add_argument(\"--recordings\", nargs=\"*\",\n\t                        help=\"List of recordings to print info for.\")\n\t    parser.add_argument(\"--show\", nargs=\"*\",\n\t                        help=\"Print recordings that satisfy some criteria (eg --show classes=wifi channel=3)\")\n\t    parser.add_argument(\"--contents\", action='store_true',\n\t                        help=\"Print out a table of contents for the dataset\")\n\t    parser.add_argument(\"--no-show\", action='store_true',\n", "                        help=\"Don't print contents, only save in in the dataset_root_dir/metadata\")\n\t    parser.add_argument(\"--recount-pictures\", action='store_true',\n\t                        help=\"Count the pictures for every recording during loading to refresh the recording and \"\n\t                             \"dataset metadata. Results in slower initialization of the dataset.\")\n\t    args = parser.parse_args()\n\t    process_info(args.dataset, args.recordings, args.show, args.contents, args.no_show, args.recount_pictures)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "generate_compressed_data.py", "chunked_list": ["\"\"\"This tool generates compressed pictures and compresses existing annotations of recordings\"\"\"\n\timport argparse\n\timport logging\n\tfrom core import *\n\tformatter = logging.Formatter('%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s')\n\tlogging.basicConfig(level=logging.DEBUG,\n\t                    format='%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s')\n\tlog = logging.getLogger('spread')\n\tlog.setLevel(logging.INFO)\n\tdef gen_compressed_data(dataset, recordings, pictures_only, annotations_only, compr_avg, compr_proc):\n", "    \"\"\"Compress recording pictures\"\"\"\n\t    # Initialize dataset\n\t    ds = Dataset(dataset)\n\t    # Print info about a list of recordings\n\t    if recordings:\n\t        recordings = [ds.recordings_dict.get(rec, None) for rec in recordings]\n\t    else:\n\t        recordings = [x for x in ds.recordings if not x.compressed_pic_list or len(x.compressed_pic_list) < 152]\n\t    mode = \"compressed\"\n\t    for rec in recordings:\n", "        if not annotations_only:\n\t            log.info(\"Compressing pictures for recording %s\", rec.name)\n\t            rec.generate_pictures(mode=mode, navg=compr_avg, nproc=compr_proc)\n\t        if not pictures_only:\n\t            log.info(\"Compressing annotations for recording %s\", rec.name)\n\t            rec.compress_annotations(compr_avg * compr_proc)\n\tdef main():\n\t    \"\"\"Parse args\"\"\"\n\t    parser = argparse.ArgumentParser(description=\"Generate compressed pictures and annotations\")\n\t    parser.add_argument(\"--dataset\", required=True,\n", "                        help=\"Root directory of the dataset.\")\n\t    parser.add_argument(\"--recordings\", nargs=\"*\",\n\t                        help=\"List of recordings to print info for.\")\n\t    parser.add_argument(\"--compr-avg\", type=int, default=3,\n\t                        help=\"Total compression factor is: `compr-avg * compr-proc`.\")\n\t    parser.add_argument(\"--compr-proc\", type=int, default=4,\n\t                        help=\"Total compression factor is: `compr-avg * compr-proc`.\")\n\t    parser.add_argument(\"--pictures-only\", action='store_true',\n\t                        help='Only generate compressed pictures.')\n\t    parser.add_argument(\"--annotations-only\", action='store_true',\n", "                        help='Only generate compressed annotations.')\n\t    args = parser.parse_args()\n\t    gen_compressed_data(args.dataset, args.recordings, args.pictures_only, args.annotations_only, args.compr_avg,\n\t                        args.compr_proc)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "create_synthetic.py", "chunked_list": ["\"\"\"\n\tCreate synthetic transmissions to augment the dataset for initial training.\n\tBased on pre-made molds for all classes, the user can create a small dataset of artificial data with the\n\tclasses of their choice.\n\tWorkflow:\n\t==============================================================\n\t             DATA AUGMENTATION ON PER-PACKET BASIS\n\t   PROCESSING FLOW:\n\t   MANUAL EXTRACT SAMPLES/MOLDS ==>  ADJUST SNR ==> ADJUST LENGTH\n\t                                                          |\n", "                                                          V\n\t                      VERTICAL PADDING <== HORIZONTAL PADDING\n\t==============================================================\n\t\"\"\"\n\tfrom __future__ import division\n\tfrom PIL import Image\n\timport numpy as np\n\tfrom random import randint, choice\n\timport argparse\n\timport os\n", "import datetime\n\tfrom core import data_clip\n\tfrom core import img_flip\n\tfrom core import stack_image_channels\n\tfrom core import check_collision\n\tfrom core import img_scale\n\tfrom core import constants\n\tfrom core import Frame\n\tfrom core import Packet\n\tdef gen_synthetic_single_emission(category, savepath, snr_range=None, nfft=512, nlines=512,\n", "                                  length_range=(62, 512), length_step=15, full_length_ratio=10):\n\t    \"\"\" Generate data for single class \"\"\"\n\t    if snr_range is None:\n\t        snr_range = [-10, 0, 10]\n\t    # Sanitizing save path\n\t    if not os.path.isdir(savepath):\n\t        os.makedirs(savepath)\n\t    # Report file\n\t    reportfile = os.path.dirname(savepath) + '/report_' + str(datetime.datetime.now().date()) + '.txt'\n\t    f_report = open(reportfile, 'a+')\n", "    f_report.write('Report for category ' + constants.CATEGORIES[category][\"main\"] + '\\n\\n')\n\t    # Load background mold\n\t    background_mold = dict()\n\t    for background in constants.CATEGORIES[-1]['element']:\n\t        background_mold[background] = np.load(constants.MOLD_PATHS[background])\n\t    print(\"===> Generating data for \" +str(constants.CATEGORIES[category][\"main\"]))\n\t    count = 0\n\t    for obj_key in constants.CATEGORIES[category][\"element\"]:\n\t        print(\"==>[ Processing object \"+str(obj_key)+\" ]\")\n\t        # Load main object mold\n", "        object_mold = np.load(constants.MOLD_PATHS[obj_key])\n\t        # Change the background\n\t        for background in background_mold:\n\t            print(\">! Change frame background to \"+str(background)+\" for \"+str(obj_key))\n\t            # Change the SNR variation\n\t            for snr in snr_range:\n\t                if obj_key == 'bt_1' and snr == snr_range[0]:\n\t                    continue\n\t                if obj_key == 'bt_2' and snr != snr_range[1]:\n\t                    continue\n", "                print(\">! Apply SNR variation of \"+str(snr))\n\t                # Writing counts to the report...\n\t                f_report.write('Start count for category ' + constants.CATEGORIES[category][\n\t                    'main'] + ' object ' + obj_key + ' with snr change ' + str(snr) + ':' + str(count) + '\\n')\n\t                # Start adjusting\n\t                if constants.VAR[obj_key]:  # Length can be adjusted\n\t                    for length in range(length_range[0], length_range[1] + 1, length_step):\n\t                        # Only do replication for full-length packets\n\t                        replicate = full_length_ratio\n\t                        if length != length_range[1]:\n", "                            replicate = 1\n\t                        while replicate > 0:\n\t                            print(\"! Change object length to \"+str(length))\n\t                            # Get the lower and upper bounds of objects in the frame \n\t                            x_start_point = constants.AUGMENT_CHANNELS[category]['start']\n\t                            y_start_point = 0\n\t                            x_end_point = constants.LIMIT_INDEX\n\t                            y_end_point = 512\n\t                            for i in range(x_start_point, x_end_point - object_mold.shape[1],\n\t                                           constants.AUGMENT_CHANNELS[category]['space'] *\n", "                                           constants.AUGMENT_CHANNELS[category][\n\t                                               'skip']):  # Avoid similar samples by frequency skipping\n\t                                # Vertical padding\n\t                                j = y_start_point\n\t                                while j + length <= y_end_point:  # Avoid similar samples by random time skipping\n\t                                    left_offset = i\n\t                                    top_offset = j\n\t                                    # Adjust main object\n\t                                    c_object = Packet(object_mold, category, constants.VAR[obj_key])\n\t                                    c_object.adjust_length(length)\n", "                                    c_object.adjust_snr(snr)\n\t                                    # Create and adjust frame\n\t                                    pathname = savepath + \"/\" + constants.CATEGORIES[category]['main'] + \"_\" + str(\n\t                                        count) + \".jpg\"\n\t                                    frame = Frame(pathname, background_mold[background], nfft, nlines)\n\t                                    current_box = frame.add_packet(c_object, left_offset, top_offset)\n\t                                    # Save image\n\t                                    data_clip(frame.frame_data, constants.VMIN, constants.VMAX)\n\t                                    image_data = img_scale(frame.frame_data, constants.VMIN, constants.VMAX)\n\t                                    image_data = img_flip(stack_image_channels(image_data), ax=0)\n", "                                    image = Image.fromarray(image_data)\n\t                                    image.save(pathname)\n\t                                    count += 1\n\t                                    # Time skipping\n\t                                    j += np.random.randint(10, 30, 1)[0]\n\t                            # Make sure to decrement the replication\n\t                            replicate -= 1\n\t                else:  # Length is fixed\n\t                    print(\"! Length is fixed...\")\n\t                    # Get the lower and upper bounds of objects in the frame \n", "                    x_start_point = constants.AUGMENT_CHANNELS[category]['start']\n\t                    y_start_point = 0\n\t                    x_end_point = constants.LIMIT_INDEX\n\t                    y_end_point = 512\n\t                    for i in range(x_start_point, x_end_point - object_mold.shape[1],\n\t                                   constants.AUGMENT_CHANNELS[category]['space'] * constants.AUGMENT_CHANNELS[category][\n\t                                       'skip']):  # Avoid similar samples by frequency skipping\n\t                        # Vertical padding\n\t                        j = y_start_point\n\t                        while j + object_mold.shape[0] < y_end_point:  # Avoid similar samples by random time skipping\n", "                            left_offset = i\n\t                            top_offset = j\n\t                            # Adjust main object\n\t                            c_object = Packet(object_mold, category, constants.VAR[obj_key])\n\t                            c_object.adjust_snr(snr)\n\t                            # Create and adjust frame\n\t                            pathname = savepath + \"/\" + constants.CATEGORIES[category]['main'] + \"_\" + str(\n\t                                count) + \".jpg\"\n\t                            frame = Frame(pathname, background_mold[background], nfft, nlines)\n\t                            current_box = frame.add_packet(c_object, left_offset, top_offset)\n", "                            # Save image\n\t                            data_clip(frame.frame_data, constants.VMIN, constants.VMAX)\n\t                            image_data = img_scale(frame.frame_data, constants.VMIN, constants.VMAX)\n\t                            image_data = img_flip(stack_image_channels(image_data), ax=0)\n\t                            image = Image.fromarray(image_data, 'RGB')\n\t                            image.save(pathname)\n\t                            count += 1\n\t                            # Time skipping\n\t                            j += np.random.randint(10, 30, 1)[0]\n\t                # Writing counts for the report...\n", "                f_report.write('Finish count for category ' + constants.CATEGORIES[category][\n\t                    'main'] + ' object ' + obj_key + ' with snr change ' + str(snr) + ':' + str(count) + '\\n')\n\t                f_report.write('==================================================\\n')\n\t    f_report.close()\n\t    print(\"> Done processing \"+str(constants.CATEGORIES[category]['main'])+\". \"+str(count)+\" elements generated\")\n\t    print(\"Images saved in: \"+savepath)\n\t    print(\"Processing report: \"+reportfile)\n\t    return savepath\n\tdef gen_synthetic_colliding_emission(categories, savepath, snr_range=None, nfft=512, nlines=512, num_coll_iter=500,\n\t                                     length_range=(62, 512), length_step=15, full_length_ratio=10):\n", "    \"\"\" Generate collisions for 2 classes \"\"\"\n\t    if snr_range is None:\n\t        snr_range = [-10, 0, 10]\n\t    # Parsing the classes\n\t    cat1, cat2 = categories\n\t    # Sanitizing save path\n\t    if not os.path.isdir(savepath):\n\t        os.makedirs(savepath)\n\t    # Report file\n\t    reportfile = os.path.dirname(savepath) + '/report_' + str(datetime.datetime.now().date()) + '.txt'\n", "    f_report = open(reportfile, 'a+')\n\t    f_report.write('Report for collision of ' + constants.CATEGORIES[cat1][\"main\"] + ' ' + constants.CATEGORIES[cat2][\n\t        \"main\"] + '\\n\\n')\n\t    # Load background mold\n\t    background_mold = dict()\n\t    for background in constants.CATEGORIES[-1]['element']:\n\t        background_mold[background] = np.load(constants.MOLD_PATHS[background])\n\t    print(\"===> Generating data for collision \"+str(constants.CATEGORIES[cat1][\"main\"])+\" \"+str(constants.CATEGORIES[cat2][\"main\"]))\n\t    count = 0\n\t    # Change the background\n", "    for background in background_mold:\n\t        print(\">! Change frame background to \"+str(background))\n\t        # Change the object for each category\n\t        for obj1 in constants.CATEGORIES[cat1]['element']:\n\t            object_mold1 = np.load(constants.MOLD_PATHS[obj1])\n\t            for snr_obj1 in snr_range:\n\t                if obj1 == 'bt_1' and snr_obj1 == snr_range[0]:\n\t                    continue\n\t                if obj1 == 'bt_2' and snr_obj1 != snr_range[1]:\n\t                    continue\n", "                print(\">! Apply SNR variation of \"+str(snr_obj1)+\" to \"+str(obj1))\n\t                for obj2 in constants.CATEGORIES[cat2]['element']:\n\t                    object_mold2 = np.load(constants.MOLD_PATHS[obj2])\n\t                    for snr_obj2 in snr_range:\n\t                        if obj2 == 'bt_1' and snr_obj2 == snr_range[0]:\n\t                            continue\n\t                        if obj2 == 'bt_2' and snr_obj2 != snr_range[1]:\n\t                            continue\n\t                        # Collision is not visible\n\t                        if (snr_obj1 == snr_range[0] and snr_obj2 == snr_range[2]) or (\n", "                                snr_obj1 == snr_range[2] and snr_obj2 == snr_range[0]):\n\t                            continue\n\t                        print(\">! Apply SNR variation of \"+str(snr_obj2)+\" to \"+str(obj2))\n\t                        f_report.write(\n\t                            'Start count for collision of ' + obj1 + ' and ' + obj2 + ' with snr change ' + str(\n\t                                snr_obj1) + ' and ' + str(snr_obj2) + ':' + str(count) + '\\n')\n\t                        \"\"\"\n\t                        One problem is that, it's not trivial to come up with all collision\n\t                        patterns, considering all settings of length, and snrs. So, the most\n\t                        efficient way would be generating a number of samples for each combination\n", "                        of SNRs and objects.\n\t                        \"\"\"\n\t                        iter_counts = 0\n\t                        while iter_counts < num_coll_iter:\n\t                            packet_obj1 = Packet(object_mold1, cat1, constants.VAR[obj1])\n\t                            packet_obj1.adjust_snr(snr_obj1)\n\t                            packet_obj2 = Packet(object_mold2, cat2, constants.VAR[obj2])\n\t                            packet_obj2.adjust_snr(snr_obj2)\n\t                            # Varying lengths if needed\n\t                            if constants.VAR[obj1]:\n", "                                packet_obj1.adjust_length(randint(100, 512))\n\t                            if constants.VAR[obj2]:\n\t                                packet_obj2.adjust_length(randint(100, 512))\n\t                            # Generate collision: Need to check if collision is possible\n\t                            collidable = False\n\t                            left_offset1 = None\n\t                            left_offset2 = None\n\t                            top_offset1 = None\n\t                            top_offset2 = None\n\t                            while collidable == False:\n", "                                # First, choose the location of the first packet\n\t                                left_offset1 = choice(range(constants.AUGMENT_CHANNELS[cat1]['start'],\n\t                                                            constants.LIMIT_INDEX - packet_obj1.width,\n\t                                                            constants.AUGMENT_CHANNELS[cat1]['space']))\n\t                                top_offset1 = choice(range(0, 512 - packet_obj1.length + 1, 1))\n\t                                range2 = range(constants.AUGMENT_CHANNELS[cat2]['start'],\n\t                                               constants.LIMIT_INDEX - packet_obj2.width,\n\t                                               constants.AUGMENT_CHANNELS[cat2]['space'])\n\t                                collidable, left_offset2 = check_collision(left_offset1, packet_obj1.width, range2,\n\t                                                                           packet_obj2.width)\n", "                            top_offset2 = choice(\n\t                                range(min(max(0, top_offset1 - int(packet_obj2.length / 2)), 512 - packet_obj2.length),\n\t                                      min(512 - packet_obj2.length, top_offset1 + int(packet_obj1.length / 2)) + 1, 1))\n\t                            # Collision not visible\n\t                            if (\n\t                                    left_offset1 <= left_offset2 and left_offset1 + packet_obj1.width >= left_offset2 + packet_obj2.width and\n\t                                    top_offset1 <= top_offset2 and top_offset1 + packet_obj1.length >= top_offset2 + packet_obj2.length and\n\t                                    snr_obj1 < snr_obj2 and obj2 != 'bt_2') or \\\n\t                                    (\n\t                                            left_offset2 <= left_offset1 and left_offset2 + packet_obj2.width >= left_offset1 + packet_obj1.width and\n", "                                            top_offset2 <= top_offset1 and top_offset2 + packet_obj2.length >= top_offset1 + packet_obj1.length and\n\t                                            snr_obj2 < snr_obj1 and obj1 != 'bt_2'):\n\t                                continue\n\t                            # Create and adjust frame\n\t                            pathname = savepath + \"/\" + \"collision_\" + constants.CATEGORIES[cat1][\"main\"] + \"_\" + \\\n\t                                       constants.CATEGORIES[cat2][\"main\"] + \"_\" + str(count) + \".jpg\"\n\t                            frame = Frame(pathname, background_mold[background], nfft, nlines)\n\t                            frame.add_packet(packet_obj1, left_offset1, top_offset1)\n\t                            frame.add_packet(packet_obj2, left_offset2, top_offset2)\n\t                            # Save image\n", "                            data_clip(frame.frame_data, constants.VMIN, constants.VMAX)\n\t                            image_data = img_scale(frame.frame_data, constants.VMIN, constants.VMAX)\n\t                            image_data = img_flip(stack_image_channels(image_data), ax=0)\n\t                            image = Image.fromarray(image_data)\n\t                            image.save(pathname)\n\t                            count += 1\n\t                            iter_counts += 1\n\t                        # Writing counts for the report...\n\t                        f_report.write(\n\t                            'Finish count for collision of ' + obj1 + ' and ' + obj2 + ' with snr change ' + str(\n", "                                snr_obj1) + ' and ' + str(snr_obj2) + ':' + str(count) + '\\n')\n\t                        f_report.write('==================================================\\n')\n\t    f_report.close()\n\t    print(\"> Done processing collisions of \"+str(constants.CATEGORIES[cat1]['main'])+\" \"+str(constants.CATEGORIES[cat2]['main'])+\". \"+str(count)+\" elements generated\")\n\t    print(\"Images saved in: \"+savepath)\n\t    print(\"Processing report: \"+reportfile)\n\t    return savepath\n\tdef gen_synthetic_data(category, save_path, snr_range=None, nfft=512, nlines=512, num_coll_iter=500, length_range=(62, 512),\n\t                       length_step=15, full_length_ratio=10):\n\t    \"\"\"\n", "    Generate synthetic dataset based on category and the corresponding mold. Return the directory of the dataset.\n\t    \"\"\"\n\t    if isinstance(category, int):\n\t        return gen_synthetic_single_emission(category, save_path, snr_range, nfft, nlines,\n\t                                             length_range, length_step, full_length_ratio)\n\t    elif isinstance(category, list) or isinstance(category, tuple) :\n\t        if len(category) == 1:\n\t            return gen_synthetic_single_emission(category[0], save_path, snr_range, nfft, nlines,\n\t                                                 length_range, length_step, full_length_ratio)\n\t        elif len(category) == 2:\n", "            return gen_synthetic_colliding_emission(category, save_path, snr_range, nfft, nlines, num_coll_iter, length_range,\n\t                                                    length_step, full_length_ratio)\n\t        else:\n\t            raise NotImplementedError(\"Collision with more than two emission categories is not supported.\")\n\t    else:\n\t        raise TypeError(\"Wrong format for emission category.\")\n\tdef main():\n\t    \"\"\" Parse args \"\"\"\n\t    parser = argparse.ArgumentParser(description=\"Create synthetic emission data.\",\n\t                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n", "    parser.add_argument(\"--save-path\", type=str, required=True,\n\t                        help=\"Directory path to save data.\")\n\t    parser.add_argument(\"--categories\", \"-c\", type=int,\n\t                        choices=[x for x in constants.CATEGORIES.keys() if x > -1], required=True, nargs=\"*\",\n\t                        help=\"Category(-ies) of synthetic emissions. Specifying two categories generates a collision.\")\n\t    parser.add_argument(\"--snr-range\", nargs=\"+\", type=int, default=[-10, 0, 10],\n\t                        help=\"SNR Values for the synthetic emissions.\")\n\t    parser.add_argument(\"--n-fft\", type=int, default=512,\n\t                        help=\"Nfft for image generation. SPREAD dataset uses 512.\")\n\t    parser.add_argument(\"--n-lines\", type=int, default=512,\n", "                        help=\"Nlines for image generation. SPREAD dataset uses 512.\")\n\t    parser.add_argument(\"--num-coll-iter\", \"-i\", type=int, default=500,\n\t                        help=\"Number of random iterations for each choice of collision setting regarding the location.\")\n\t    parser.add_argument(\"--length-range\", \"-l\", type=int, nargs=2, default=(62, 512),\n\t                        help=\"Range for adjustment of emissions length.\")\n\t    parser.add_argument(\"--length-step\", \"-s\", type=int, default=15,\n\t                        help=\"Length adjustment step. I.e.: For 100MHz recordings, a step of 10 corresponds to ~50us.\")\n\t    parser.add_argument(\"--full-length-ratio\", \"-r\", type=int, default=10,\n\t                        help=\"Ratio of full length packets to synthetic ones. Generating more full length packets \"\n\t                             \"improves training performance.\")\n", "    args = parser.parse_args()\n\t    assert len(args.categories) <= 2, \"A number of one or two categories may be selected.\"\n\t    gen_synthetic_data(args.categories, args.save_path, snr_range=args.snr_range, nfft=args.n_fft, nlines=args.n_lines,\n\t                       num_coll_iter=args.num_coll_iter, length_range=args.length_range, length_step=args.length_step,\n\t                       full_length_ratio=args.full_length_ratio)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "generate_pictures.py", "chunked_list": ["\"\"\"\n\tTool to generate pictures for recordings of SPREAD.\n\t\"\"\"\n\timport argparse\n\tfrom core import *\n\timport logging\n\tformatter = logging.Formatter('%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s')\n\tlogging.basicConfig(level=logging.DEBUG,\n\t                    format='%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s')\n\tlog = logging.getLogger('spread')\n", "log.setLevel(logging.INFO)\n\tdef process_gen_pics(dataset_dir, recordings=None, mode='grayscale', overwrite=False, log_noise=None, fft_size=512,\n\t                     img_limit=0, filters=None):\n\t    \"\"\"\n\t    Generates pictures of a recording and saves them in the picture directory of the dataset\n\t    \"\"\"\n\t    # Load the dataset\n\t    ds = Dataset(dataset_dir)\n\t    # Get the recording objects\n\t    if recordings:\n", "        to_pic = [ds.recordings_dict.get(Dataset.get_rec_name(x), None) for x in recordings]\n\t    elif filters:\n\t        to_pic = ds.filter_recordings(filters)\n\t    else:\n\t        to_pic = ds.recordings\n\t    for rec in to_pic:\n\t        if not rec:\n\t            log.error(\"No recording found\")\n\t            continue\n\t        else:\n", "            if not rec.metadata.no_of_pictures or rec.metadata.no_of_pictures == 0 or overwrite:\n\t                rec.generate_pictures(log_noise=log_noise, npics=img_limit, mode=mode,\n\t                                      nfft=fft_size)\n\t            else:\n\t                log.info(\"Skipping recording %s because pictures already exist. Specify \\\"--overwrite\\\" if desired.\",\n\t                         rec.name)\n\tdef main():\n\t    \"\"\"\n\t    Parse arguments\n\t    \"\"\"\n", "    parser = argparse.ArgumentParser(description='Generate pictures for all or selected recordings',\n\t                                     formatter_class=argparse.RawTextHelpFormatter)\n\t    parser.add_argument(\"--dataset\", required=True,\n\t                        help=\"Root directory of the dataset.\")\n\t    parser.add_argument(\"--recordings\", nargs=\"*\",\n\t                        help=\"List of recordings to generate pictures for. If no recordings are specified, all \\\n\t                             recordings with no pictures will be processed\")\n\t    parser.add_argument(\"--filter\", nargs=\"*\",\n\t                        help=\"Pass recordings with filters (eg: classes=3)\")\n\t    parser.add_argument(\"--mode\", type=str, default='grayscale', choices=['grayscale', 'compressed'],\n", "                        help='Choose mode: grayscale or compressed')\n\t    # parser.add_argument(\"--compr-factor\", type=int, default=12,\n\t    #                     help=\"Compression factor.\")\n\t    parser.add_argument(\"--overwrite\", action='store_true',\n\t                        help=\"Overwrites previously generated pictures\")\n\t    parser.add_argument(\"--log-noise\",\n\t                        help=\"Noise level in dB. If omitted, noise is read from the recording metadata.\")\n\t    parser.add_argument(\"--img-limit\", type=int, default=0,\n\t                        help='Number of pictures to generate (default: 0 (all pictures)')\n\t    parser.add_argument(\"--fft-size\", type=int, default=512,\n", "                        help=\"FFT size\")\n\t    args = parser.parse_args()\n\t    process_gen_pics(args.dataset, args.recordings, args.mode, args.overwrite, args.log_noise, args.fft_size,\n\t                     args.img_limit, args.filter)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "combine_recordings.py", "chunked_list": ["\"\"\"\n\tScript to create synthetic recordings in the Dataset from current recordings.\n\t\"\"\"\n\timport argparse\n\timport os\n\timport logging\n\tfrom random import choice as pick_one\n\tfrom time import time\n\tfrom multiprocessing import Process, Queue\n\tfrom core import *\n", "formatter = logging.Formatter('%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s')\n\tlogging.basicConfig(level=logging.DEBUG,\n\t                    format='%(asctime)s - %(filename)s:%(lineno)d - %(levelname)s - %(message)s')\n\tlog = logging.getLogger('spread')\n\tlog.setLevel(logging.DEBUG)\n\tdef get_obj_from_files(ds, from_files):\n\t    \"\"\"\n\t    Returns a list of objects based on their filenames\n\t    \"\"\"\n\t    from_files = [os.path.basename(x).split('.32fc')[0] for x in from_files]  # get rid of file path and extension\n", "    rec_obj_list = sorted([ds.recordings_dict[x] for x in from_files], key=lambda rec: rec.id)\n\t    return rec_obj_list\n\tdef get_obj_from_properties(ds, from_properties):\n\t    \"\"\"\n\t    Returns a list of objects based on their properties\n\t    \"\"\"\n\t    rec_obj_list = []\n\t    for rec_prop in from_properties:\n\t        # Split the filters in the required list format [key=value ...]\n\t        rec_prop = rec_prop.split(',')\n", "        # and pick a random one from the filtered recordings\n\t        filtered = ds.filter_recordings(rec_prop)\n\t        if not filtered:\n\t            log.error(\"No recording found that satisfies the properties: %s\", ','.join(rec_prop))\n\t            skipped_combos = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"skipped_combs.txt\")\n\t            with open(skipped_combos, 'a') as sc:\n\t                sc.write(\"%s\\n\" % ' '.join(from_properties))\n\t            return\n\t        which_one = pick_one(filtered)\n\t        rec_obj_list.append(which_one)\n", "    rec_obj_list.sort(key=lambda rec: rec.id)\n\t    return rec_obj_list\n\tdef combine_recordings(dataset, from_files, from_properties, to_files, ds=None,\n\t                       q=None, md_only=False):\n\t    \"\"\"\n\t    Creates synthetic data from the given recordings. This includes initializing\n\t    the recording and the metadata, and storing them in the dataset, depending on\n\t    the options given.\n\t    \"\"\"\n\t    if q:\n", "        ds = q.get()\n\t    if not ds:\n\t        ds = Dataset(dataset)\n\t    # Either get recordings explicitly from filenames\n\t    if from_files:\n\t        rec_obj_list = get_obj_from_files(ds, from_files)\n\t    # choose random recordings based on input filters\n\t    elif from_properties:\n\t        rec_obj_list = get_obj_from_properties(ds, from_properties)\n\t    else:\n", "        rec_obj_list = None\n\t    if not rec_obj_list:\n\t        log.info(\"No recordings to combine, skipping.\")\n\t        return\n\t    t = time()\n\t    to_file = to_files[0] if to_files else None\n\t    # Create the synthetic recording object and optionally the complex samples file\n\t    synthetic = Recording.merge_recordings(rec_obj_list, to_file, mockup=md_only)\n\t    # Create the synthetic metadata\n\t    if not synthetic:\n", "        log.error(\"Synthetic %s could not be found.\", to_file)\n\t        return\n\t    synthetic.metadata = RecordingMetadata.combine_metadata(synthetic, [x.metadata for x in rec_obj_list])\n\t    # Create the metafile for the recording and store the metadata\n\t    synthetic.metadata.store_metadata()\n\t    synthetic.print_info()\n\t    log.info(\"Time elapsed: %s minutes\", (time() - t) / 60)\n\tdef combine_annotations(dataset, synthetics=None, ds=None):\n\t    \"\"\"Creates synthetic annotations for synthetic recordings, by combining the annotations of the source recordings.\"\"\"\n\t    if not ds:\n", "        ds = Dataset(dataset)\n\t    if synthetics:\n\t        synthetics = [ds.recordings_dict.get(x, None) for x in synthetics]\n\t    else:\n\t        synthetics = [x for x in ds.recordings if x.metadata.synthetic]\n\t    for syn in synthetics:\n\t        if not syn:\n\t            log.error(\"Synthetic not found.\")\n\t            continue\n\t        log.info(\"Creating synthetic annotations for recording %s\", syn.name)\n", "        if not os.path.isdir(syn.rec_pics_dir):\n\t            try:\n\t                os.mkdir(syn.rec_pics_dir)\n\t            except OSError:\n\t                pass\n\t        if not os.path.isdir(syn.synthetic_annotations_dir):\n\t            os.mkdir(syn.synthetic_annotations_dir)\n\t        if not os.path.isdir(syn.compressed_pics_dir):\n\t            try:\n\t                os.mkdir(syn.compressed_pics_dir)\n", "            except OSError:\n\t                pass\n\t        sources = [ds.recordings_dict.get(x, None) for x in syn.metadata.sources]\n\t        # If a source is missing (i.e. removed from dataset)\n\t        if None in sources:\n\t            log.info(\"Synthetic %s is depending on a missing source: %s. Skipping.\", syn.name,\n\t                     ' '.join([x for x in syn.metadata.sources if not ds.recordings_dict.get(x, None)]))\n\t            continue\n\t        # Check if all source recordings have compressed annotations and use these first\n\t        if all([x.compressed_annotation_list for x in sources]):\n", "            log.info(\"Using compressed annotations.\")\n\t            for compr_ann in sources[0].compressed_annotation_list:\n\t                syn_pic_id = get_id_from_pic_name(os.path.basename(compr_ann))\n\t                pic_annotations = []\n\t                # Find the corresponding annotation of every source file\n\t                for src_rec in sources:\n\t                    src_ann_file = os.path.join(src_rec.compressed_pics_dir, src_rec.pic_prefix + \"_\" + str(syn_pic_id)\n\t                                                + \".txt\")\n\t                    try:\n\t                        with open(src_ann_file, 'r') as f:\n", "                            src_annot = f.read().strip().split('\\n')\n\t                    except IOError:\n\t                        log.error(\"File missing for source rec: %s\", src_ann_file)\n\t                        continue\n\t                    pic_annotations.extend(src_annot)\n\t                # Merge all the lines together\n\t                pic_annotations = '\\n'.join(pic_annotations)\n\t                # and save them in the synthetic annotation\n\t                outfile = os.path.join(syn.compressed_pics_dir,\n\t                                       syn.pic_prefix + \"_\" + str(syn_pic_id) + \".txt\")\n", "                with open(outfile, 'w') as f:\n\t                    f.write(pic_annotations)\n\t        # Else if not, for every picture in the synthetic file\n\t        else:\n\t            for syn_pic in syn.pic_list:\n\t                syn_pic_id = get_id_from_pic_name(os.path.basename(syn_pic))\n\t                pic_annotations = []\n\t                # Find the corresponding annotation of every source file\n\t                for src_rec in sources:\n\t                    if src_rec.fixed_label_list:\n", "                        src_dir = src_rec.fixed_labels_dir\n\t                    else:\n\t                        log.info(\"Fixed labels were not found for recording %s in %s. Skipping recording: %s.\",\n\t                                 src_rec.name,\n\t                                 src_rec.fixed_labels_dir,\n\t                                 syn.name)\n\t                        break\n\t                    src_ann_file = os.path.join(src_dir, src_rec.pic_prefix + \"_\" + str(syn_pic_id) + \".txt\")\n\t                    with open(src_ann_file, 'r') as f:\n\t                        src_annot = f.read().strip().split('\\n')\n", "                    pic_annotations.extend(src_annot)\n\t                else:\n\t                    # Merge all the lines together\n\t                    pic_annotations = '\\n'.join(pic_annotations)\n\t                    # and save them in the synthetic annotation\n\t                    outfile = os.path.join(syn.synthetic_annotations_dir,\n\t                                           syn.pic_prefix + \"_\" + str(syn_pic_id) + \".txt\")\n\t                    with open(outfile, 'w') as f:\n\t                        f.write(pic_annotations)\n\t                    continue\n", "                break\n\tdef main():\n\t    \"\"\"\n\t    Parse arguments\n\t    \"\"\"\n\t    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,\n\t                                     description=\"\"\"This tool creates synthetic data based on other real or synthetic \\\n\t        data of SPREAD.\n\t        The data can be produced by merging other sample files.\n\t        Example usage:\n", "        python combine_recordings.py --from source1.32fc source2.32fc --to combined.32fc\n\t        In order to provide property filters as input instead of recording names one can use:\n\t        --from-properties class=wifi,channel=3,duration=10 class=wmic class=bluetooth,transmission=continuous\n\t        In that case there will be chosen and merged a random:\n\t        wifi, channel 3, 10 second recording, with a\n\t        wireless microphone recording, and a\n\t        continuous bluetooth recording.\n\t        Pay attention to the comma and space separations to avoid merging wrong files.\n\t\"\"\")\n\t    parser.add_argument(\"--dataset\", required=True,\n", "                        help=\"Dataset root directory\")\n\t    source = parser.add_mutually_exclusive_group(required=True)\n\t    source.add_argument(\"--from-files\", nargs=\"+\",\n\t                        help='File(s) to create synthetic data from.')\n\t    source.add_argument(\"--from-properties\", nargs=\"+\",\n\t                        help='Specify source recordings depending on their properties giving a \\\n\t        comma-separated list of property=value for each recording. \\\n\t        A random dataset recording satisfying the properties (if any) will be used.')\n\t    parser.add_argument(\"--to-files\", nargs='+',\n\t                        help='File(s) to write synthetic data to.')\n", "    source.add_argument(\"--combine-annotations\", action='store_true',\n\t                        help=\"Create annotations for the specified synthetic recordings by combining the annotations\\\n\t                             of the source recordings.\")\n\t    parser.add_argument(\"--synthetics\", nargs=\"*\",\n\t                        help=\"Synthetic recordings to create annotations for. If not specified, all of them are\\\n\t                             generated.\")\n\t    parser.add_argument(\"--mock\", action=\"store_true\", help=\"Only print metadata of the resulting synthetic.\")\n\t    args = parser.parse_args()\n\t    if args.combine_annotations:\n\t        combine_annotations(args.dataset, args.synthetics)\n", "        return\n\t    combine_recordings(args.dataset, args.from_files, args.from_properties, args.to_files, md_only=args.mock)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "core/_dataset.py", "chunked_list": ["\"\"\"\n\tDefinition of the dataset top level structure: Dataset\n\t\"\"\"\n\timport os\n\timport glob\n\timport logging\n\tfrom ._recording import Recording\n\tfrom ._tables import DatasetTable\n\tfrom ._metadata import DatasetMetadata\n\tfrom ._utils import *\n", "log = logging.getLogger('spread')\n\t__all__ = ['Dataset']\n\tclass Dataset(object):\n\t    \"\"\"\n\t    Represents an instance of SPREAD\n\t    \"\"\"\n\t    @staticmethod\n\t    def get_rec_name(recname):\n\t        \"\"\"\n\t        Returns the name of a recording, omitting the 32fc/dat extension\n", "        \"\"\"\n\t        if recname.endswith('.32fc'):\n\t            return recname.split('.32fc')[0]\n\t        elif recname.endswith('.dat'):\n\t            return recname.split('.dat')[0]\n\t        else:\n\t            return recname\n\t    def __init__(self, root_dir, recount_pictures=False):\n\t        \"\"\"\n\t                Dataset structure\n", "                    dataset_root\n\t        recordings                      pictures                        metadata\n\trec_<id>.32fc  rec_<id>.json    rec<id>/rec_<id>_pic_<pic_id>       global_metadata_files\n\t        \"\"\"\n\t        # Root directory of the dataset\n\t        self.root_dir = os.path.abspath(root_dir)\n\t        if not os.path.isdir(self.root_dir):\n\t            log.error(\"Dataset root directory not found.\")\n\t        # Dataset directories\n\t        self.recordings_dir = os.path.join(self.root_dir, \"recordings\")\n", "        # Pictures\n\t        self.pictures_dir = os.path.join(self.root_dir, \"pictures\")\n\t        # Noise Calculation\n\t        self.noise_calc_dir = os.path.join(self.root_dir, \"noise_calculations\")\n\t        # Metadata dir, containing global metadata\n\t        self.metadata_dir = os.path.join(self.root_dir, \"metadata\")\n\t        # Dataset metadata instance\n\t        self.metadata = DatasetMetadata(self)\n\t        if not os.path.isdir(self.metadata_dir):\n\t            os.mkdir(self.metadata_dir)\n", "        # File naming\n\t        self.default_rec_name_prefix = \"rec_\"\n\t        self.default_syn_name_prefix = \"syn_\"\n\t        # Image naming <rec_ID>_<pic_ID>.jpg\n\t        # Dataset toolset\n\t        self.tools = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\t        self.core_dir = os.path.join(self.tools, 'core')\n\t        self.flowgraphs = os.path.join(self.core_dir, 'flowgraphs')\n\t        self.gen_dat_snr = os.path.join(self.flowgraphs, 'samples_fft.py')\n\t        self.gen_fft_samples = os.path.join(self.flowgraphs, 'samples_to_dat.py')\n", "        self.gen_pics_script = os.path.join(self.core_dir, 'gen_pics.py')\n\t        # Dictionary containing all the recording objects\n\t        self._recordings_dict = {}\n\t        # Table of contents\n\t        self.cont_table = None\n\t        self.load_recordings(recount_pictures=recount_pictures)\n\t        try:\n\t            self.load_content_table()\n\t        except Exception as e:\n\t            log.error(\"There was an error loading the table of contents: %s\", e)\n", "        self.print_info()\n\t    def load_recordings(self, recount_pictures=False):\n\t        \"\"\"\n\t        Loads and initializes the recordings of this dataset\n\t        \"\"\"\n\t        files = glob.glob(os.path.join(self.recordings_dir, '*.32fc'))\n\t        for rec in files:\n\t            rec_obj = Recording(rec, self, recount_pictures=recount_pictures)\n\t            self._recordings_dict[rec_obj.name] = rec_obj\n\t    def add_recording(self, rec_object=None, recname=None):\n", "        \"\"\"\n\t        Adds a recording to the dataset either by recording object or by recording name (ie: rec_145)\n\t        \"\"\"\n\t        if recname:\n\t            recfile = [os.path.join(self.recordings_dir, '%s.32fc' % recname)]\n\t            rec_obj = Recording(recfile, self)\n\t            self._recordings_dict[rec_obj.name] = rec_obj\n\t        elif rec_object:\n\t            self._recordings_dict[rec_object.name] = rec_object\n\t    def get_last_synthetic_index(self):\n", "        \"\"\"\n\t        Loads and initializes the recordings of this dataset\n\t        \"\"\"\n\t        files = glob.glob(os.path.join(self.recordings_dir, 'syn_*.32fc'))\n\t        if not files:\n\t            return 0\n\t        indexes = [x.split('syn_')[1].split('.')[0] for x in files]\n\t        try:\n\t            indexes = [int(x) for x in indexes]\n\t            return max(indexes)\n", "        except ValueError:\n\t            return None\n\t    @property\n\t    def recordings(self):\n\t        \"\"\"\n\t        Returns a list of all recording objects in the dataset\n\t        \"\"\"\n\t        return sorted(self._recordings_dict.values(), key=lambda x: (x.name[0], x.id))\n\t    @property\n\t    def recordings_dict(self):\n", "        \"\"\"\n\t        Returns the dictionary of recordings\n\t        \"\"\"\n\t        return self._recordings_dict\n\t    @property\n\t    def sorted_recordings(self):\n\t        \"\"\"\n\t        Returns a sorted list of recordings based on their name and ID\n\t        \"\"\"\n\t        return sorted(self._recordings_dict.values(), key=lambda x: (x.name[0], x.id))\n", "    @property\n\t    def sorted_recording_names(self):\n\t        \"\"\"\n\t        Returns a sorted list of recording names\n\t        \"\"\"\n\t        return sorted(self._recordings_dict.keys())\n\t    def get_synthetic_outfile(self):\n\t        \"\"\"\n\t        Returns the next available name for synthetic recordings using the default prefix\n\t        \"\"\"\n", "        last_index = self.get_last_synthetic_index()\n\t        if last_index is None:\n\t            return None\n\t        else:\n\t            return self.default_syn_name_prefix + str(last_index+1) + '.32fc'\n\t    def filter_recordings(self, filters):\n\t        \"\"\"\n\t        Return a list of recordings that satisfy the given filters.\n\t        :param filters: list of filters in the form of [key1=value1, key2=value2, ...]\n\t        :return: list of recordings objects that satisfy all the filters\n", "        \"\"\"\n\t        filtered = []\n\t        for rec in self.recordings:\n\t            for fltr in filters:\n\t                try:\n\t                    fltr_k, fltr_v = fltr.split('=')\n\t                except ValueError:\n\t                    log.error(\"Please make sure you are properly providing the filters in the format of key=value\")\n\t                    continue\n\t                # Additional properties search\n", "                if fltr_k == 'classes':\n\t                    try:\n\t                        fltr_v = int(fltr_v)\n\t                    except ValueError:\n\t                        log.error(\"Please provide an integer to filter the number of classes.\")\n\t                        continue\n\t                    if len(rec.metadata.sources) != fltr_v:\n\t                        break\n\t                elif fltr_k == 'sources':\n\t                    if fltr_v not in rec.metadata.sources:\n", "                        break\n\t                # If a filter is not satisfied, discard it\n\t                elif str(rec.metadata.metadata.get(fltr_k, '')) != fltr_v:\n\t                    break\n\t            else:\n\t                filtered.append(rec)\n\t                continue\n\t            continue\n\t        return sorted(filtered, key=lambda x: (x.name[0], x.id))\n\t    def load_content_table(self):\n", "        \"\"\"\n\t        Loads the table of contents\n\t        \"\"\"\n\t        self.cont_table = DatasetTable(self)\n\t    def get_total_size(self):\n\t        \"\"\"\n\t        Returns the total size of the dataset\n\t        \"\"\"\n\t        return total_size([x.file_size for x in self.recordings])\n\t    def get_total_duration(self):\n", "        \"\"\"\n\t        Returns total duration of recordings in seconds\n\t        \"\"\"\n\t        try:\n\t            return sum([int(x.metadata.duration) for x in self.recordings])\n\t        except TypeError:\n\t            return None\n\t    def get_total_no_pictures(self):\n\t        \"\"\"\n\t        Returns total amount of pictures in the dataset\n", "        \"\"\"\n\t        try:\n\t            return sum([x.metadata.no_of_pictures for x in self.recordings])\n\t        except TypeError:\n\t            return None\n\t    def print_info(self):\n\t        \"\"\"\n\t        Prints info about the dataset\n\t        \"\"\"\n\t        log.info(\"Loading SPREAD from %s\", self.root_dir)\n", "        # log.info(\"Dataset root directory: %s\", self.root_dir)\n\t        log.info(\"Total number of recordings: %s\", len(self.recordings))\n\t        log.info(\"Total duration of recordings in seconds: %s\", self.get_total_duration())\n\t        log.info(\"Total number of pictures: %s\", self.get_total_no_pictures())\n\t        log.info(\"Total size: %s\", self.get_total_size())\n"]}
{"filename": "core/_annotation.py", "chunked_list": ["\"\"\"\n\tDefinition of the annotation object\n\t\"\"\"\n\tfrom . import constants\n\tclass Annotation(object):\n\t    \"\"\"\n\t    Represents an instance of Annotation with information about the label and the region of the object\n\t    Label: Integer denoting the class index in the given dataset\n\t    Annotation region coordinates. They should always map to a region in the corresponding image normalized by the image\n\t    size. That is, point (322, 456) in a (512x512) image should have coordinates (0.6289062, 0.890625). If given values\n", "    are invalid they are adjusted to the range of [0,1].\n\t    Note: A 6 digit precision is recommended for use with Yolo tools.\n\t    x_c: Box center x coordinate\n\t    y_c: Box center y coordinate\n\t    Box width\n\t    Box height\n\t    \"\"\"\n\t    @staticmethod\n\t    def nrmlz(value, size):\n\t        \"\"\"\n", "        Normalize a given value by the size of the picture. Truncates to 6 decimals.\n\t        For example pixel position 322 in a 512 length scale would return 0.6289062.\n\t        \"\"\"\n\t        return round(float(value) / size, 6)\n\t    @staticmethod\n\t    def denormlz(value, size):\n\t        \"\"\"\n\t        Return the absolute value based on the image size.\n\t        For example pixel position 0.6289062 in a 512 length scale would return 322.\n\t        \"\"\"\n", "        return int(value * size)\n\t    @classmethod\n\t    def get_annotation_from_borders(cls, label, left, right, bottom, up):\n\t        \"\"\"\n\t        Return an annotation object based on given borders of the object region.\n\t        Note: Borders should be given normalized by the image size\n\t        \"\"\"\n\t        label = int(label)\n\t        width = right - left\n\t        height = bottom - up\n", "        x_c = float(left) + width / 2.0\n\t        y_c = float(up) + height / 2.0\n\t        if any([x > 1.0 for x in [x_c, y_c, width, height]]):\n\t            return None\n\t        else:\n\t            return cls(label, x_c, y_c, width, height)\n\t    @classmethod\n\t    def get_annotation_from_str(cls, annot_str):\n\t        \"\"\"\n\t        Return an annotation object based on given annotation string. Annotation string is assumed to follow the YOLO\n", "        convention as follows, with coordinates normalized by the the image size:\n\t        <label center_x center_y width height>\n\t        \"\"\"\n\t        label, x_c, y_c, width, height = annot_str.split()\n\t        return cls(int(label), float(x_c), float(y_c), float(width), float(height))\n\t    @classmethod\n\t    def empty(cls):\n\t        \"\"\"Returns an empty/invalid annotation instance\"\"\"\n\t        return cls(-1, 0.0, 0.0, 0.0, 0.0)\n\t    @classmethod\n", "    def combine_annotations(cls, i, j):\n\t        \"\"\"\n\t        Combines the coordinates of two annotations into a single one and returns a string representing the combined one\n\t        \"\"\"\n\t        # Create a new annotation with the same borders as the two old ones\n\t        new_up = min(i.up(), j.up())\n\t        new_down = max(i.down(), j.down())\n\t        new_left = min(i.left(), j.left())\n\t        new_right = max(i.right(), j.right())\n\t        new_width = new_right - new_left\n", "        new_height = new_down - new_up\n\t        return cls(\n\t            i.label,\n\t            new_left + new_width / 2.0,\n\t            new_up + new_height / 2.0,\n\t            new_width,\n\t            new_height\n\t        )\n\t    @staticmethod\n\t    def merge_annotations(annot_list):\n", "        \"\"\"\n\t        Given a list of annotation objects, returns a list of merged annotations based on their location and class\n\t        \"\"\"\n\t        # Remove possible empty or invalid annotations\n\t        annot_list = [ann for ann in annot_list if ann.label >= 0]\n\t        # Sort all the annotations based on the class index and on the start of transmission\n\t        annot_list.sort(key=lambda x: (x.label, x.y_c - x.height / 2.0))\n\t        # Iteratively merge annotations until necessary\n\t        while True:\n\t            for i in annot_list[:-1]:\n", "                for j in annot_list[annot_list.index(i) + 1:]:\n\t                    # Check same class\n\t                    if i.label == j.label:\n\t                        # Check similar left and right\n\t                        avg_width = (i.width + j.width) / 2\n\t                        if (abs(i.x_c - i.width / 2.0 - j.x_c + j.width / 2.0) <\n\t                            constants.SIDE_THRESHOLD[i.label] * avg_width and\n\t                            abs(i.x_c + i.width / 2.0 - j.x_c - j.width / 2.0) <\n\t                            constants.SIDE_THRESHOLD[i.label] * avg_width) or \\\n\t                                ((i.x_c - i.width / 2.0 - j.x_c + j.width / 2.0) *\n", "                                 (i.x_c + i.width / 2.0 - j.x_c - j.width / 2.0)) < 0:\n\t                            # Check beginning - end (this approach also merges overlapping transmissions in the same\n\t                            # channel which is intended since it would be hard to separate the weaker transmission in\n\t                            # the picture)\n\t                            if j.y_c - j.height / 2.0 - i.y_c - i.height / 2.0 < constants.TIME_THRESHOLD[i.label]:\n\t                                merged = Annotation.combine_annotations(i, j)\n\t                                break\n\t                # Inner for loop else clause\n\t                else:\n\t                    continue\n", "                # If inner for-loop breaks, break the outer for-loop in order to reconstruct the list and start over.\n\t                break\n\t            # When no more merging is needed\n\t            else:\n\t                # Break the while-loop\n\t                break\n\t            # Otherwise replace the merged elements with the new one and continue the while-loop.\n\t            annot_list.insert(annot_list.index(i), merged)\n\t            # Remove both merged annotations, mind that the index of j is increased by one.\n\t            annot_list.remove(i)\n", "            annot_list.remove(j)\n\t            continue\n\t        return annot_list\n\t    @classmethod\n\t    def compress_annotation(cls, annot, compr_factor, pic_index):\n\t        \"\"\"\n\t        Return a new, compressed annotation from a given annotation and a compression factor.\n\t        We assume that the original annotation was generated on flipped pictures. Therefore, no flipping should be done,\n\t        the boxes must be compressed by the factor in height and the center coordinate must be adjusted to reflect the\n\t        compression.\n", "        \"\"\"\n\t        # If the original annotation is empty or invalid, return an empty annotation\n\t        if annot.label < 0:\n\t            return cls.empty()\n\t        # Compress by the factor, setting a minimum threshold to avoid completely flattening the annotation\n\t        new_height = annot.height / compr_factor\n\t        # If annotation is compressed to 1 pixel or less, delete it\n\t        if new_height < 1 / 512.0:\n\t            return cls.empty()\n\t        # Adjust the new center. The compression is done by stacking <compr_factor> pictures vertically and compressing\n", "        # them, first input picture being moved at the bottom of the resulting one (to depict the flipping of pictures)\n\t        new_y_c = (annot.y_c + compr_factor - pic_index - 1) / float(compr_factor)\n\t        return cls(\n\t            annot.label,\n\t            annot.x_c,\n\t            new_y_c,\n\t            annot.width,\n\t            new_height\n\t        )\n\t    def __init__(self, label, x_c, y_c, width, height):\n", "        try:\n\t            self.label = int(label)\n\t            self.x_c = max(min(float(x_c), 1.0), 0.0)\n\t            self.y_c = max(min(float(y_c), 1.0), 0.0)\n\t            self.width = max(min(float(width), 1.0), 0.0)\n\t            self.height = max(min(float(height), 1.0), 0.0)\n\t        except ValueError as e:\n\t            raise e\n\t    # Set of helper properties to quickly check if the annotation region falls within valid numbers ([0,1])\n\t    @property\n", "    def left(self):\n\t        \"\"\"Return the left border of the object region in the annotation\"\"\"\n\t        return self.x_c - self.width / 2\n\t    @property\n\t    def right(self):\n\t        \"\"\"Return the right border of the object region in the annotation\"\"\"\n\t        return self.x_c + self.width / 2\n\t    @property\n\t    def bottom(self):\n\t        \"\"\"Return the lower border of the object region in the annotation\"\"\"\n", "        return self.y_c + self.height / 2\n\t    @property\n\t    def up(self):\n\t        \"\"\"Return the upper border of the object region in the annotation\"\"\"\n\t        return self.y_c - self.height / 2\n\t    def shift_center(self, new_center_point):\n\t        \"\"\"Shifts the annotation to a new center point given by a tuple of (new_x_c, new_y_c)\"\"\"\n\t        self.x_c, self.y_c = new_center_point\n\t    def scale_annotation(self, factor, ax=None):\n\t        \"\"\"\n", "        Scales the annotation (up/down) by a given factor along a given axes (x/y, w/h, width/height). If no axes is\n\t        provided, both dimensions are scaled\n\t        \"\"\"\n\t        if not ax:\n\t            ax = 'both'\n\t        if ax.lower() in ['x', 'w', 'width', 'both']:\n\t            self.width *= factor\n\t        elif ax.lower() in ['y', 'h', 'height', 'both']:\n\t            self.height *= factor\n\t    def get_annot_str(self):\n", "        \"\"\"\n\t        Returns a string for the annotation following the convention of YOLO with coordinates normalized in the range of\n\t        [0,1]:\n\t        <label center_x center_y width height>\n\t        \"\"\"\n\t        return \"%d %.6f %.6f %.6f %.6f\" % (\n\t            self.label,\n\t            self.x_c,\n\t            self.y_c,\n\t            self.width,\n", "            self.height\n\t        )\n\t# Define Box Class\n\t# Connect Annotation and Box classes both ways?\n\t# Draw boxes on images\n"]}
{"filename": "core/_tables.py", "chunked_list": ["\"\"\"\n\tDefinition of dataset tables\n\t\"\"\"\n\timport pandas as pd\n\timport logging\n\timport os\n\timport json\n\tfrom collections import defaultdict\n\tlog = logging.getLogger('spread')\n\tclass DatasetTable(object):\n", "    \"\"\"\n\t    Represents an instance of a dataset content table containing information about the dataset recordings\n\t    \"\"\"\n\t    # Table format\n\t    # Any changes here will reflect on the output of the table\n\t    # Mind that the separator must be comma and space (ie: \", \")\n\t    # in order to be parsed \n\t    table_format = '''\n\t    Recording, Synthetic, Sources, Class, Channel, Noise level (dB), SNR (dB; estimate)\n\t    '''.strip()\n", "    string_size_thres = 20\n\t    @staticmethod\n\t    def get_short_name(data_str):\n\t        \"\"\"\n\t        Return a short name for a given string, mainly by shortening the class names\n\t        in order to reduce the necessary table width and increase readability\n\t        :param data_str:    original string\n\t        :return:            short_version\n\t        \"\"\"\n\t        # Set a threshold to avoid shortening when not necessary\n", "        if len(str(data_str)) < DatasetTable.string_size_thres:\n\t            return data_str\n\t        else:\n\t            return data_str.replace('lightbridge', 'LiBr').replace('bluetooth', 'Blth').replace('zigbee', 'ZgB')\n\t    @staticmethod\n\t    def table_to_md_mapping(column, rec):\n\t        \"\"\"\n\t        Given a recording object, return the metadata info for the table columns\n\t        \"\"\"\n\t        # Mapping between table columns and recording attributes\n", "        table_rec_mapping = {\n\t                'Recording':            rec.name,\n\t                'Synthetic':            rec.metadata.synthetic,\n\t                'Sources':              '-' if not rec.metadata.synthetic else ','.join(rec.metadata.sources),\n\t                'Class':                DatasetTable.get_short_name(','.join(rec.metadata.d_class)),\n\t                'Channel':              DatasetTable.get_short_name(','.join(rec.metadata.channel)),\n\t                'Noise level (dB)':     round(rec.metadata.noise_pwr_db, 2),\n\t                'SNR (dB; estimate)':   DatasetTable.get_short_name(\"%s (%s)\" % (','.join(rec.metadata.snr),\n\t                                                                                 ','.join(rec.metadata.snr_range))),\n\t            }\n", "        return table_rec_mapping[column]\n\t    def __init__(self, dataset, csv_file=None, json_file=None):\n\t        # Dataset\n\t        self.dataset = dataset\n\t        # CSV file\n\t        self.csv_file = csv_file if csv_file else os.path.join(self.dataset.metadata_dir, \"table_of_contents.csv\")\n\t        # JSON file\n\t        self.json_file = json_file if json_file else os.path.join(self.dataset.metadata_dir, \"table_of_contents.json\")\n\t        self.reclist_json_file = os.path.join(self.dataset.metadata_dir, \"list_of_recordings.json\")\n\t        self.table_columns = self.table_format.split(', ')\n", "        self.table_dict = defaultdict(list)\n\t        for rec in self.dataset.sorted_recordings:\n\t            for column in self.table_columns:\n\t                try:\n\t                    self.table_dict[column].append(DatasetTable.table_to_md_mapping(column, rec))\n\t                except Exception as e:\n\t                    print(e, column)\n\t                    exit()\n\t        self.table = pd.DataFrame(data=self.table_dict)\n\t        self.table = self.table[self.table_format.split(', ')]\n", "    def get_table_str(self, index=False):\n\t        \"\"\"\n\t        Prints the table of contents\n\t        \"\"\"\n\t        return self.table.to_string(index=index)\n\t    def save_to_csv(self):\n\t        \"\"\"\n\t        Saves the table of contents in the csv file\n\t        \"\"\"\n\t        if not self.csv_file:\n", "            log.error(\"No csv file specified.\")\n\t            return\n\t        else:\n\t            self.table.to_csv(self.csv_file, index=False, encoding='utf8')\n\t    def save_to_json(self):\n\t        \"\"\"\n\t        Saves the table of contents in the json file\n\t        \"\"\"\n\t        if not self.json_file:\n\t            log.error(\"No json file specified.\")\n", "            return\n\t        else:\n\t            self.table.to_json(self.json_file)\n\t    def save_reclist_to_json(self):\n\t        \"\"\"\n\t        Saves the list of recording names to a json file\n\t        \"\"\"\n\t        with open(self.reclist_json_file, 'w') as jw:\n\t            json.dump(self.dataset.sorted_recording_names, jw)\n"]}
{"filename": "core/frame.py", "chunked_list": ["from __future__ import division\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\timport math\n\tfrom copy import deepcopy\n\tCOMMON_NOISE = -45\n\tclass Frame(object):\n\t    def __init__(self, pathname, background_data, width, height):\n\t        self.width = width\n\t        self.height = height\n", "        self.background = background_data\n\t        self.background_length = self.background.shape[0]\n\t        self.frame_data = deepcopy(background_data)  # Background needs to be full-frame\n\t        self.packets = []  # Packets in the frame\n\t        self.pathname = pathname\n\t        self.annotation_pathname = pathname[:-4] + \".txt\"\n\t    def add_packet(self, packet, left_offset=None, top_offset=None, noise=None):\n\t        \"\"\"\n\t        Add packet to current frame. the background need to be of full-frame.\n\t        \"\"\"\n", "        if not noise:\n\t            noise = COMMON_NOISE\n\t        packet_data = packet.data\n\t        assert isinstance(left_offset, int) and isinstance(top_offset, int), \\\n\t            \"The offsets are not properly set!\"\n\t        # Adding packet to the current frame\n\t        for i in range(top_offset, top_offset + packet.length):\n\t            for j in range(left_offset, left_offset + packet.width):\n\t                log_to_pow_bg = pow(10.0, (self.frame_data[i][j] + noise) / 10.0)\n\t                log_to_pow_trans = pow(10.0, (packet_data[i - top_offset][j - left_offset] + noise) / 10.0)\n", "                self.frame_data[i][j] = 10 * math.log10(log_to_pow_bg + log_to_pow_trans) - noise\n\t        bottom_offset = self.height - packet.length - top_offset\n\t        # Extracting object frame info\n\t        box_x_c = left_offset + packet.width / 2\n\t        # Y-axis will be flipped to make the waterfall plot.\n\t        box_y_c = self.height - (self.height - bottom_offset + top_offset) / 2\n\t        box_w = packet.width\n\t        box_h = packet.length\n\t        category = packet.category\n\t        # Append the bounding box to annotation file\n", "        fopen = open(self.annotation_pathname, 'a+')\n\t        fopen.write(str(category) + \" \" + str(round(box_x_c / self.width, 6)) + \" \" + str(\n\t            round(box_y_c / self.height, 6)) + \" \" + str(round(box_w / self.width, 6)) + \" \" + str(\n\t            round(box_h / self.height, 6)) + \"\\n\")\n\t        fopen.close()\n\t        self.packets.append(packet)\n\t        return\n\t    def convert_image(self, cmap, vmin, vmax):\n\t        \"\"\"\n\t        Directly convert 2D data into RGB image data of a spectrogram\n", "        \"\"\"\n\t        norm = plt.Normalize(vmin, vmax)\n\t        return np.flip(np.array(np.floor(cmap(norm(self.frame_data)) * 256)[:, :, :-1]).astype(np.uint8), axis=0)\n"]}
{"filename": "core/_metadata.py", "chunked_list": ["\"\"\"\n\tDefinition of the metadata structures of the dataset\n\tRecordingMetadata\n\tDatasetMetadata\n\t\"\"\"\n\timport os\n\timport json\n\timport datetime\n\timport logging\n\tfrom ._utils import *\n", "from . import constants\n\tfrom textwrap import dedent\n\t__all__ = ['RecordingMetadata', 'DatasetMetadata']\n\tlog = logging.getLogger('spread')\n\tclass RecordingMetadata(object):\n\t    \"\"\"\n\t    Represents an instance of recording metadata.\n\t    This includes information about the actual recording such as date created, duration, or transmission infromation.\n\t    If the recording is synthetic, it also includes information about the way the recording was created, the source\n\t    files, etc.\n", "    \"\"\"\n\t    @staticmethod\n\t    def get_rec_metafile(recfile):\n\t        \"\"\"\n\t        Returns the metadata file of the recording.\n\t        Looks for a JSON file in the same directory as the recording with the same prefix\n\t        :param recfile: path to recording file\n\t        :return: recording metadata file (.json)\n\t        \"\"\"\n\t        recfile = os.path.abspath(recfile)\n", "        return os.path.splitext(recfile)[0]+'.json'\n\t        # if os.path.isfile(os.path.splitext(recfile)[0]+'.json'):\n\t        #     return os.path.splitext(recfile)[0]+'.json'\n\t        # else:\n\t        #     return None\n\t    @classmethod\n\t    def combine_metadata(cls, recording, from_md):\n\t        \"\"\"\n\t        Combines multiple metadata objects into a new metadata object. Used for the creation of synthetic data.\n\t        @param recording: synthetic recording object to create metadata for\n", "        @param from_md: Source metadata objects\n\t        @return: Combined metadata object\n\t        \"\"\"\n\t        # Initial values for the synthetic metadata\n\t        combined = {\n\t            'date_recorded': datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S'),\n\t            'classes': ','.join([x for y in from_md for x in y.d_class]),\n\t            'noise_db': add_noise_levels([x.noise_pwr_db for x in from_md]),\n\t            'synthetic': True,\n\t            'sources': ','.join([x.rec_name for x in from_md]),\n", "            'rec_name': recording.name,\n\t            'duration': min([x.duration for x in from_md]),\n\t        }\n\t        initial_combined_keys = list(combined.keys())\n\t        for md in from_md:\n\t            for key in md.metadata:\n\t                if key in initial_combined_keys + ['freq_sweep', 'no_of_pictures', 'file_size']:\n\t                    continue\n\t                if key == \"tx\":\n\t                    new_value = md.metadata[key]\n", "                else:\n\t                    new_value = md.metadata['classes']+'_'+str(md.metadata[key])\n\t                # If the key already exists include the new metadata separated by commas\n\t                if key in combined:\n\t                    combined[key] += ',' + new_value\n\t                # Else create it\n\t                else:\n\t                    combined[key] = new_value\n\t        return cls(recording, md_from_dict=combined)\n\t    def __init__(self, recording, md_from_dict=None):\n", "        # Initial metadata\n\t        if md_from_dict:\n\t            self._metadata = md_from_dict              # Initialize metadata dict\n\t        else:\n\t            self._metadata = {}\n\t        self.recording = recording\n\t        # Metadata file\n\t        self.metafile = RecordingMetadata.get_rec_metafile(recording.recfile)\n\t        # Parse the recording metadata to retrieve properties\n\t        if not self._metadata:\n", "            if self.metafile:\n\t                if os.path.isfile(self.metafile):\n\t                    self.load_metadata()\n\t        self.rec_name = self._gets('rec_name')\n\t        self._synthetic = self._getb('synthetic')\n\t        self.sources = self._gets('sources').split(',')\n\t        self.date_recorded = self._gets('date_recorded')\n\t        self.d_class = (self._gets('class') or self._gets('classes')).split(',')\n\t        self.duration = self._geti('duration')\n\t        self.channel = (self._gets('channel') or self._gets('channels')).split(',')\n", "        self.cfreq = (self._gets('cfreq') or self._gets('fc')).split(',')\n\t        self.samp_rate = self._gets('samp_rate').split(',')\n\t        self.noise_pwr_db = self._getf('noise_pwr_db') or self._getf('noise_db')\n\t        self.noise_variance = self._getf('noise_variance')\n\t        self.snr = self._gets('snr').split(',')\n\t        self._snr_range = self._gets('snr_range').split(',')\n\t        if not self._snr_range or self._snr_range == [''] or self._snr_range == ['n/a'] or \\\n\t                ''.join(self._snr_range) == 'n/a':\n\t            self._set_md_value('snr_range', ','.join(self._get_snr_range()))\n\t            self._snr_range = self._gets('snr_range').split(',')\n", "        # If the transmission was a frequency sweep, the information is stored in this dictionary in order to be parsed\n\t        self.freq_sweep = self._get_md_value('freq_sweep')\n\t        self.transmission = self._gets('transmission')      # Transmission (continuous or interval)\n\t        self.type = self._gets('type')                      # Type of transmission(data, control, ping, n/a)\n\t        self.no_of_pictures = self._geti('no_of_pictures')\n\t        self.file_size = self._gets('file_size')\n\t    def load_metadata(self):\n\t        \"\"\"\n\t        Parses the metafile and stores metadata in a dictionary\n\t        \"\"\"\n", "        try:\n\t            with open(self.metafile, \"r\") as mf:\n\t                self._metadata = json.load(mf)\n\t        # Some metadata files have an extra closing bracket when multiprocessing is used\n\t        except ValueError:\n\t            # Try fixing a recognized pattern\n\t            with open(self.metafile, \"r\") as mf:\n\t                meta_contents = mf.read().strip()\n\t            if meta_contents[-2:] == \"}}\":\n\t                meta_contents = meta_contents[:-1]\n", "            try:\n\t                self._metadata = json.loads(meta_contents)\n\t            except ValueError:\n\t                recname = self.recording.name if self.recording else None\n\t                log.error(\"Error loading metadata for recording: %s\", recname)\n\t                self._metadata = {}\n\t        # Rename potentially old fields\n\t        renamed_keys = {\n\t            \"class\": \"classes\",\n\t            \"channel\": \"channels\",\n", "            \"cfreq\": \"fc\",\n\t            \"noise_pwr_db\": \"noise_db\",\n\t        }\n\t        for renamed_key in renamed_keys.keys():\n\t            if renamed_key in self._metadata.keys():\n\t                self._metadata[renamed_keys.get(renamed_key)] = self._metadata.pop(renamed_key)\n\t    def store_metadata(self):\n\t        \"\"\"\n\t        Stores the current metadata dictionary in the metafile, overwriting the previous contents.\n\t        \"\"\"\n", "        if self._metadata:\n\t            with open(self.metafile, \"w\") as mf:\n\t                json.dump(self._metadata, mf)\n\t    @property\n\t    def metadata(self):\n\t        \"\"\"\n\t        Returns the dictionary containing the recording metadata\n\t        \"\"\"\n\t        return self._metadata\n\t    @property\n", "    def synthetic(self):\n\t        \"\"\"\n\t        Returns True if the recording is synthetic\n\t        \"\"\"\n\t        return self._synthetic\n\t    @property\n\t    def snr_range(self):\n\t        \"\"\"\n\t        Returns the snr range of the recording out of 'low', 'mid', 'high'.\n\t        \"\"\"\n", "        return self._snr_range\n\t    def _get_snr_range(self):\n\t        \"\"\"\n\t        Categorizes the SNR of the recording in a range of LOW, MID, HIGH, depending on the class and SNR value\n\t        Different transmissions have different depiction in the pictures based on their type and snr value. This\n\t        classification as high, mid, or low is just meant to help with the analysis and prediction workflow.\n\t        \"\"\"\n\t        # SNR ranges are defined in the constants.py\n\t        snr_ranges = constants.SNR_RANGES\n\t        ret = []\n", "        for i in range(len(self.d_class)):\n\t            rec_class = self.d_class[i]\n\t            try:\n\t                snr = self.snr[i].lstrip(\"%s_\" % self.d_class[i])\n\t                snr = round(float(snr), 2)\n\t            except ValueError:\n\t                return 'n/a'\n\t            # For the class in question, identify the range the SNR falls into\n\t            for thres in snr_ranges[rec_class]:\n\t                if snr > thres:\n", "                    ret.append(snr_ranges['label'][snr_ranges[rec_class].index(thres)])\n\t                    break\n\t                continue\n\t        return ret\n\t    def _geti(self, value, default=0):\n\t        \"\"\"\n\t        Return the metadata value as an int\n\t        \"\"\"\n\t        try:\n\t            ret_value = int(self._get_md_value(value, default))\n", "        except ValueError:\n\t            ret_value = default\n\t            log.error(\"Error trying to load the proper value for %s. Loading default value: %s.\", value, default)\n\t        return ret_value\n\t    def _geti_list(self, value, default=None):\n\t        \"\"\"\n\t        Return the metadata value as a list of ints parsed from a comma separated string\n\t        \"\"\"\n\t        if default is None:\n\t            default = [0]\n", "        ret_value = self._get_md_value(value, default)\n\t        ret_value = ret_value.split(',')\n\t        return [int(x) if x else 0 for x in ret_value]\n\t    def _getf(self, value, default=0.0):\n\t        \"\"\"\n\t        Return the metadata value as a float\n\t        \"\"\"\n\t        try:\n\t            ret_value = float(self._get_md_value(value, default))\n\t        except ValueError:\n", "            ret_value = default\n\t            log.error(\"Error trying to load the proper value for %s. Loading default value: %s.\", value, default)\n\t        return ret_value\n\t    def _getf_list(self, value, default=None):\n\t        \"\"\"\n\t        Return the metadata value as a list of floats parsed from a comma separated string\n\t        \"\"\"\n\t        if default is None:\n\t            default = [0.0]\n\t        ret_value = self._get_md_value(value, default)\n", "        ret_value = ret_value.split(',')\n\t        return [float(x) if x else 0.0 for x in ret_value]\n\t    def _gets(self, value, default=''):\n\t        \"\"\"\n\t        Return the metadata value as a string\n\t        \"\"\"\n\t        try:\n\t            ret_value = str(self._get_md_value(value, default))\n\t        except ValueError:\n\t            ret_value = default\n", "            log.error(\"Error trying to load the proper value for %s. Loading default value: %s.\", value, default)\n\t        return ret_value\n\t    def _getb(self, value, default=None):\n\t        \"\"\"\n\t        Return the metadata value as boolean\n\t        \"\"\"\n\t        return True if self._get_md_value(value, default).lower() == 'true' else False\n\t    def _get_md_value(self, value, default=None):\n\t        return str(self._metadata.get(value, default))\n\t    def _set_md_value(self, key, value):\n", "        \"\"\"\n\t        Modifies or adds a metadata key value. Used only for development purposes\n\t        \"\"\"\n\t        self._metadata.update({key: value})\n\t    def get_md_string(self):\n\t        \"\"\"\n\t        Returns a string containing the metadata info for screen printing purposes\n\t        \"\"\"\n\t        md_str = \"\"\"\\\n\t                Name of recording: %s\n", "                Synthetic: %s\n\t                Sources: %s\n\t                Center frequency: %s\n\t                Sample rate: %s\n\t                Duration: %s\n\t                Class(es): %s\n\t                Type: %s\n\t                Channel(s): %s\n\t                Transmission(s): %s\n\t                Noise level in dB: %s\n", "                SNR: %s\n\t                SNR Range(s): %s\n\t                Number of pictures: %s\n\t                \"\"\" % (\n\t                        self.rec_name,\n\t                        self.synthetic,\n\t                        ','.join(self.sources),\n\t                        ','.join(self.cfreq),\n\t                        ','.join(self.samp_rate),\n\t                        self.duration,\n", "                        ','.join(self.d_class),\n\t                        self.type,\n\t                        ','.join(self.channel),\n\t                        self.transmission,\n\t                        self.noise_pwr_db,\n\t                        ','.join(self.snr),\n\t                        ','.join(self.snr_range),\n\t                        self.no_of_pictures\n\t                        )\n\t        return dedent(md_str)\n", "class DatasetMetadata(object):\n\t    \"\"\"\n\t    Represents an instance of dataset metadata.\n\t    This can be information like size, number of recordings or pictures, types of classes, and so on.\n\t    \"\"\"\n\t    def __init__(self, ds):\n\t        self.dataset = ds\n\t        self.ds_md_folder = os.path.join(self.dataset.root_dir, 'metadata')\n\t        if not os.path.isdir(self.ds_md_folder):\n\t            os.mkdir(self.ds_md_folder)\n", "        self.class_names_file = os.path.join(self.ds_md_folder, \"classes.json\")\n\t        self.classes = self.load_class_names()\n\t    def load_class_names(self):\n\t        \"\"\"Load the dictionary containing class names and indices from the dataset metadata.\"\"\"\n\t        if os.path.isfile(self.class_names_file):\n\t            with open(self.class_names_file, 'r') as cf:\n\t                class_dict = json.load(cf)\n\t        else:\n\t            class_dict = dict()\n\t        return class_dict\n"]}
{"filename": "core/packet.py", "chunked_list": ["from copy import deepcopy\n\timport numpy as np\n\tclass Packet(object):\n\t    def __init__(self, data, category, var_length=True):\n\t        self.data = deepcopy(data)                  # should be numpy array\n\t        self.length = data.shape[0]\n\t        self.width = data.shape[1]\n\t        self.category = category\n\t        self.var_length = var_length    # Indicate if packet length is variable\n\t    def adjust_snr(self, attennuation, limit_thres=0):\n", "        self.data[self.data>limit_thres] -= attennuation        # Decrease the snr globally\n\t        return\n\t    def adjust_length(self, target_length, cushion=20):\n\t        if target_length <= self.length:\n\t            tail = target_length/2\n\t            self.data = np.vstack((self.data[:tail,...], self.data[-(target_length-tail):, ...]))\n\t            self.length = target_length\n\t            assert self.length == self.data.shape[0]    # Check the length consistency\n\t        else:           # If we wish to extend, stacking the last part of a packet to the existing data...\n\t            if self.length < cushion:\n", "                print(\"Packet is too short. No need to adjust.\")\n\t                return\n\t            stacked_data = deepcopy(self.data[:-cushion,...])\n\t            while stacked_data.shape[0] < target_length:    # Stacking until we fill up the gap between target_length and data length.\n\t                gap = target_length-stacked_data.shape[0]\n\t                if gap < self.length-cushion:               # Partial stacking\n\t                    stacked_data = np.vstack((stacked_data, self.data[-gap:,...]))\n\t                else:                                       # Full stacking\n\t                    stacked_data = np.vstack((stacked_data, self.data[cushion:-cushion,...]))\n\t            # Check the data and update the attributes\n", "            assert stacked_data.shape[0] == target_length\n\t            self.data = stacked_data\n\t            self.length = target_length\n\t        return"]}
{"filename": "core/_utils.py", "chunked_list": ["\"\"\"\n\tUtils file that defines miscellaneous functions\n\t\"\"\"\n\timport math\n\timport struct\n\tfrom . import constants\n\timport numpy as np\n\tfrom random import choice\n\tfrom PIL import Image\n\tdef pwr_to_db(pwr):\n", "    \"\"\"\n\t    Returns the power in dB\n\t    \"\"\"\n\t    return 10*math.log10(pwr)\n\tdef db_to_pwr(db_lvl):\n\t    \"\"\"\n\t    Returns the absolute power\n\t    \"\"\"\n\t    return 10**(db_lvl/10.0)\n\tdef add_noise_levels(db_noise_levels):\n", "    \"\"\"\n\t    Gets a list of noise levels and returns the additive noise in dB\n\t    \"\"\"\n\t    absolute_noise_levels = [db_to_pwr(x) for x in db_noise_levels]\n\t    sum_noise = sum(absolute_noise_levels)\n\t    return pwr_to_db(sum_noise)\n\tdef load_bytes_from_fd(fd, start=None, end=None):\n\t    \"\"\"\n\t    Reads `batch` number of samples from a file descriptor into a tuple and returns the tuple\n\t    \"\"\"\n", "    if start:\n\t        fd.seek(start)\n\t    binary = fd.read(end-start)\n\t    syntax = str(len(binary) / 4) + \"f\"\n\t    try:\n\t        data = struct.unpack(syntax, binary)\n\t        return data\n\t    except struct.error:        # not enough bytes to unpack, end of binary\n\t        return None\n\tdef load_array_from_fd(fd):\n", "    \"\"\"Loads a numpy array from given file descriptor\"\"\"\n\t    try:\n\t        return np.load(fd)\n\t    except (IOError, ValueError):\n\t        return None\n\tdef data_reshape(data, step, nfft):\n\t    \"\"\"\n\t    Reshape the array of data to form I,Q pairs\n\t    \"\"\"\n\t    return np.reshape(data, (step, nfft))\n", "def append_samples_to_file(filename, samples):\n\t    \"\"\"\n\t    Appends the samples to file\n\t    \"\"\"\n\t    syntax = str(len(samples))+'f'\n\t    binary = struct.pack(syntax, *samples)\n\t    with open(filename, 'ab') as of:\n\t        of.write(binary)\n\tdef data_clip(data, min_snr, max_snr):\n\t    \"\"\"\n", "    Clip the lower and upper values in a matrix\n\t    \"\"\"\n\t    if min_snr is not None:\n\t        data[data < min_snr] = min_snr\n\t    if max_snr is not None:\n\t        data[data > max_snr] = max_snr\n\t    return data\n\tdef img_scale(data, min_snr, max_snr):\n\t    \"\"\"\n\t    Assuming data is already clipped\n", "    \"\"\"\n\t    return ((data-min_snr).astype(float)/(max_snr-min_snr)*255).astype(np.uint8)\n\tdef img_flip(data, ax=0):\n\t    \"\"\"\n\t    Flip array along an axis\n\t    \"\"\"\n\t    return np.flip(data, axis=ax)\n\tdef stack_image_channels(img_data):\n\t    \"\"\"\n\t    Stack image channels assuming array is 2D\n", "    \"\"\"\n\t    return np.stack((img_data, img_data, img_data), axis=-1)\n\tdef check_collision(left_offset1, width1, range2, width2, error=5):\n\t    \"\"\"\n\t    Checking if collision between two packets is possible\n\t    \"\"\"\n\t    lo2_choices = []\n\t    for lo2 in range2:\n\t        if left_offset1 > lo2 + width2 - error:\n\t            continue\n", "        if lo2 > left_offset1 + width1 - error:\n\t            break\n\t        lo2_choices.append(lo2)\n\t    if len(lo2_choices) < 1:    # Collision is not possible\n\t        return False, None\n\t    else:\n\t        return True, choice(lo2_choices)\n\tdef spectro_plot(data_img, img_name=None, display=True, save=False):\n\t    \"\"\"\n\t    Show or save an image from a given array\n", "    \"\"\"\n\t    im = Image.fromarray(data_img)\n\t    if save:\n\t        im.save(img_name)\n\t    elif display:\n\t        im.show()\n\t    return\n\tdef convert_size(size_bytes, back=False):\n\t    \"\"\"\n\t    Converts a size value to string and back using the hurry module. If hurry is not found, standard conversion is used.\n", "    @param size_bytes:\n\t    @param back:\n\t    @return:\n\t    \"\"\"\n\t    try:\n\t        # Try to import hurry filesize for more readable output\n\t        from hurry.filesize import size as h_size, si           # (system si assumes 1K == 1000 instead of 1024)\n\t        # For back conversion, return absolute bytes size given a string as input\n\t        if back:\n\t            # If si is used the mapping is\n", "            back_map = {x[1]: x[0] for x in si}\n\t            # # Else\n\t            # back_map = {'B': 1, 'G': 1073741824, 'K': 1024, 'M': 1048576, 'P': 1125899906842624, 'T': 1099511627776}\n\t            try:\n\t                return int(size_bytes[:-1])*back_map[size_bytes[-1]] if size_bytes != '0' else 0\n\t            except ValueError as e:\n\t                print (e)\n\t                return None\n\t        else:\n\t            return h_size(size_bytes, system=si)\n", "    # If package is not installed, print out in bytes\n\t    except ImportError:\n\t        if back:\n\t            return int(size_bytes[:-1]) * constants.UNITS[size_bytes[-1]] if size_bytes != '0' else 0\n\t        else:\n\t            return \"%sB\" % size_bytes\n\tdef total_size(size_strs):\n\t    \"\"\"\n\t    Given a list of strings [1G, 500M, 2.5T] it calculates and returns a string with the total size\n\t    \"\"\"\n", "    size_sum = sum([convert_size(x, back=True) for x in size_strs if x])\n\t    try:\n\t        # Try to import hurry filesize for more readable output\n\t        # noinspection PyUnresolvedReferences\n\t        from hurry.filesize import size as h_size, si           # (system si assumes 1K == 1000 instead of 1024)\n\t        total_size_str = h_size(size_sum, system=si)\n\t    except ImportError:\n\t        # Package not installed\n\t        total_size_str = \"%sB\\t(Please install hurry.filesize package (pip install hurry.filesize)\\\n\t for more readable output)\" % size_sum\n", "    return total_size_str\n\tdef convert_freq(freq, back=False):\n\t    \"\"\"Convert freq values from string to absolute value and back\"\"\"\n\t    if back:\n\t        return \"%s Hz\" % freq\n\t    else:\n\t        if not freq:\n\t            return 0.0\n\t        return float(freq[:-1]) * constants.UNITS[freq[-1]]  # if freq != '0.0' else 0.0\n\tdef get_pairs(item_list):\n", "    \"\"\"\n\t    Given a list of items, returns all possible pair combinations.\n\t    \"\"\"\n\t    pairs = []\n\t    for i in item_list[:-1]:\n\t        pairs.extend([(i, j) for j in item_list[item_list.index(i)+1:len(item_list)]])\n\t    return pairs\n\tdef get_id_from_pic_name(picname):\n\t    \"\"\"\n\t    Returns the ID of a (compressed) picture/annotation.\n", "    Naming format is: <recording prefix>_<rec_ID>_pic_<pic_ID>.<jpg,txt>\n\t    \"\"\"\n\t    pic_id = picname.split(\".\")[0].split(\"_\")[-1]\n\t    try:\n\t        if isinstance(pic_id, str) and \"grsc\" in pic_id:\n\t            pic_id = pic_id.replace(\"grsc\", \"\")\n\t        pic_id = int(pic_id)\n\t    except ValueError:\n\t        pic_id = -1\n\t    return pic_id\n", "def do_collide(transmissions):\n\t    \"\"\"\n\t    Returns true if any pair of transmission settings (class and channel) in the given list causes a collision.\n\t    \"\"\"\n\t    for i in transmissions[:-1]:\n\t        if i[0] == 1 or i[0] == 4:\n\t            continue\n\t        for j in transmissions[transmissions.index(i)+1:]:\n\t            if j[0] == 1 or j[0] == 4:\n\t                continue\n", "            i_cf = constants.CHANNELS[i[0]][0][i[1]]\n\t            i_bw = constants.CHANNELS[i[0]][1]\n\t            i_range = (i_cf - i_bw / 2.0, i_cf + i_bw / 2.0)\n\t            j_cf = constants.CHANNELS[j[0]][0][j[1]]\n\t            j_bw = constants.CHANNELS[j[0]][1]\n\t            j_range = (j_cf - j_bw / 2.0, j_cf + j_bw / 2.0)\n\t            # print(\"%s %s\" % ((i_range[0]-j_range[0]), (i_range[1]-i_range[1])))\n\t            if (i_range[0]-j_range[0]) * (i_range[1]-j_range[1]) < 0:\n\t                return True\n\t    return False\n"]}
{"filename": "core/_plotter.py", "chunked_list": ["\"\"\"\n\tHelper class to plot interactive images with matplotlib\n\t\"\"\"\n\tfrom os.path import join\n\timport logging\n\timport matplotlib.pyplot as plt\n\tfrom PIL import Image\n\tfrom matplotlib.widgets import RectangleSelector\n\t__all__ = ['Plotter']\n\tlogging.getLogger('matplotlib').setLevel(logging.WARNING)\n", "log = logging.getLogger('spread')\n\tclass Plotter(object):\n\t    \"\"\"\n\t    Represents an instance of plotter for the Recording class\n\t    \"\"\"\n\t    @staticmethod\n\t    def _get_image_from_data(data, mode=None):\n\t        try:\n\t            im = Image.fromarray(data, mode=mode)\n\t            return im\n", "        except AttributeError:\n\t            return None\n\t    @staticmethod\n\t    def onkeypress(event):\n\t        \"\"\"Experimental function to test key press\"\"\"\n\t        if event.key == 'q':\n\t            pass\n\t            # print('event is q')\n\t    @staticmethod\n\t    def toggle_selector(event):\n", "        \"\"\"Activate the mouse click\"\"\"\n\t        Plotter.toggle_selector.RS.set_active(True)\n\t    def __init__(self):\n\t        \"\"\"Instance of plotter helper class\"\"\"\n\t        self.selected_areas = None\n\t    def area_borders(self):\n\t        \"\"\"Return the borders of the selected area\"\"\"\n\t        left = int(min(self.selected_areas[0], self.selected_areas[2]))\n\t        right = int(max(self.selected_areas[0], self.selected_areas[2]))\n\t        bottom = int(max(self.selected_areas[1], self.selected_areas[3]))\n", "        up = int(min(self.selected_areas[1], self.selected_areas[3]))\n\t        return left, right, bottom, up\n\t    def pretty_area_print(self):\n\t        \"\"\"Return a string descrptions of the selected area for prettier output\"\"\"\n\t        return \"Left: %s\\n\" \\\n\t               \"Right: %s \\n\" \\\n\t               \"Bottom: %s \\n\" \\\n\t               \"Up: %s\" % self.area_borders()\n\t    def plot(self, im=None, data=None, subplot=None, outfile=None, figdir=None, resize=None, options=None):\n\t        \"\"\"\n", "        Plot function either plots in a given subplot and returns the subplot or saves the image to a file.\n\t        \"\"\"\n\t        im = im if im else Plotter._get_image_from_data(data)\n\t        if resize:\n\t            im = im.resize(resize)\n\t        if not im:\n\t            log.error(\"Nothing to plot.\")\n\t            return None\n\t        if subplot is not None:\n\t            self._plot(im, subplot, options)\n", "        else:\n\t            Plotter._save_image(im, outfile, figdir)\n\t    def _line_select_callback(self, clk, rls):\n\t        \"\"\"\n\t        Handles mouse events to trigger noise area selection or image selection\n\t        \"\"\"\n\t        self.selected_areas = [clk.xdata, clk.ydata, rls.xdata, rls.ydata]\n\t    def _plot(self, im, subplot, options):\n\t        \"\"\"Plot image either with pillow or matlab for more options\"\"\"\n\t        if subplot == \"pillow\":\n", "            im.show()\n\t        else:\n\t            subplot.imshow(im)\n\t            if options:\n\t                if options.get('noise_input', None):\n\t                    Plotter.toggle_selector.RS = RectangleSelector(\n\t                        subplot, self._line_select_callback, useblit=True, button=[1], minspanx=5,\n\t                        minspany=5, spancoords='pixels', interactive=True\n\t                    )\n\t                    noise_box = plt.connect('key_press_event', Plotter.toggle_selector)\n", "                if options.get('button', None):\n\t                    # Expect a list of dict for buttons\n\t                    for btn in options.get('button'):\n\t                        btn_label = btn.get('label', '')\n\t                        # Button position as in [left, bottom, width, height]\n\t                        btn_position = btn.get('position', None)\n\t                        btn_action = btn.get('action')()\n\t            plt.show()\n\t        pass\n\t    @staticmethod\n", "    def _save_image(im, outfile, figdir):\n\t        \"\"\"Save image to file\"\"\"\n\t        img_name = join(figdir, outfile)\n\t        im.save(img_name)\n"]}
{"filename": "core/__init__.py", "chunked_list": ["\"\"\"Core module\"\"\"\n\tfrom ._dataset import *\n\tfrom ._recording import *\n\tfrom ._metadata import *\n\tfrom ._utils import *\n\tfrom ._annotation import Annotation\n\tfrom . import constants\n\tfrom ._plotter import *\n\tfrom gen_pics import plot_recording\n\tfrom frame import Frame\n", "from packet import Packet\n"]}
{"filename": "core/constants.py", "chunked_list": ["\"\"\"\n\tConstant values for the dataset toolset\n\t\"\"\"\n\tCLASSES = {\n\t    0: 'wifi',\n\t    1: 'bluetooth',\n\t    2: 'zigbee',\n\t    3: 'lightbridge',\n\t    4: 'wmic'\n\t}\n", "CLASS_INDEX = {CLASSES[x]: x for x in CLASSES}\n\tCOLORS = {\n\t    0: (82, 250, 88),\n\t    1: (231, 242, 80),\n\t    2: (240, 31, 38),\n\t    3: (122, 57, 95),\n\t    4: (43, 168, 224),\n\t}\n\tUNITS = {\n\t    'Hz': 1,\n", "    'B': 1,\n\t    'K': 1e3,\n\t    'M': 1e6,\n\t    'G': 1e9,\n\t    'T': 1e12,\n\t}\n\t# SNR ranges for each class were identified heuristically and are defined here\n\tSNR_RANGES = {\n\t    'label': ['high', 'mid', 'low'],\n\t    'wifi': [27, 15, 0],\n", "    'bluetooth': [22, 13, 0],\n\t    'zigbee': [27, 13, 0],\n\t    'lightbridge': [27, 13, 0],\n\t    'wmic': [22, 15, 0],\n\t}\n\t# Thresholds for merging annotation boxes\n\t# (different for each class depending on the possible overlapping with the adjacent channels\n\tSIDE_THRESHOLD = {\n\t    0: 0.1,\n\t    1: 0.3,\n", "    2: 0.5,\n\t    3: 0.5,\n\t    4: 0.5,\n\t}\n\tTIME_THRESHOLD = {\n\t    0: 2 / 512.0,\n\t    1: 1 / 512.0,\n\t    2: 1 / 512.0,\n\t    3: 2 / 512.0,\n\t    4: 2 / 512.0,\n", "}\n\t# RF characteristics of the classes for all channels as a tuple of center frequencies and bw\n\tCHANNELS = {\n\t    0: ({x: y for x, y in zip(range(14), range(2412, 2475, 5))}, 22),\n\t    1: ({x: y for x, y in zip(['n/a'], ['n/a'])}, 1),\n\t    2: ({x: y for x, y in zip(range(11, 27), range(2405, 2485, 5))}, 2),\n\t    3: ({x: y for x, y in zip(range(13, 21), range(2406, 2476, 10))}, 10),\n\t    4: ({x: y for x, y in zip(['n/a'], ['n/a'])}, 2),\n\t}\n\t\"\"\"\n", "Constant values for the augmentation script\n\t\"\"\"\n\tCATEGORIES = {\n\t    -1: {\"main\": \"background\", \"element\": [\"background\"]},\n\t    0: {\"main\": \"wifi\", \"element\": [\"wifi_1\", \"wifi_2\"]},\n\t    1: {\"main\": \"bluetooth\", \"element\": [\"bt_1\", \"bt_2\"]},\n\t    2: {\"main\": \"zigbee\", \"element\": [\"zigbee\"]},\n\t    3: {\"main\": \"lightbridge\", \"element\": [\"lightbridge\"]},\n\t    4: {\"main\": \"wmic\", \"element\": [\"wmic\"]}\n\t}\n", "MOLD_PATHS = {\n\t    'background': 'augmentation_data/molds/background.npy',\n\t    'wifi_1': 'augmentation_data/molds/wifi_1.npy',\n\t    'wifi_2': 'augmentation_data/molds/wifi_2.npy',\n\t    'bt_1': 'augmentation_data/molds/bt_1.npy',\n\t    'bt_2': 'augmentation_data/molds/bt_2.npy',\n\t    'zigbee': 'augmentation_data/molds/zigbee.npy',\n\t    'lightbridge': 'augmentation_data/molds/lightbridge.npy',\n\t    'wmic': 'augmentation_data/molds/wmic.npy',\n\t}\n", "VAR = {\n\t    'background': None,\n\t    'wifi_1': True,\n\t    'wifi_2': False,\n\t    'bt_1': True,\n\t    'bt_2': True,\n\t    'zigbee': True,\n\t    'lightbridge': True,\n\t    'wmic': True,\n\t}\n", "# Mimicking wireless channels.\n\t# Recording frequency range 2390 - 2490\n\tAUGMENT_CHANNELS = {\n\t    0: {'start': 56, 'space': 25, 'skip': 2},\n\t    # WiFi: From 2401 MHz -> (2401-2390)/100*512 ~ 56, channel splace 5 MHz -> 5/100*512 ~25\n\t    1: {'start': 56, 'space': 10, 'skip': 5},\n\t    # Bluetooth (Considering BLE): From 2401 MHz -> (2401-2390)/100*512 ~ 56, channel splace 2 MHz -> 2/100*512 ~ 10\n\t    2: {'start': 71, 'space': 25, 'skip': 2},\n\t    # Zigbee: From 2404 MHz -> (2404-2390)/100*512 ~ 71, channel splace 5 MHz -> 5/100*512 ~25\n\t    3: {'start': 61, 'space': 51, 'skip': 1},\n", "    # Lightbridge: From 2402 MHz -> (2402-2390)/100*512 ~ 61, channel space 10 MHz -> 10/100*512 ~ 51\n\t    4: {'start': 56, 'space': 25, 'skip': 2},  # Wireless microphone: Avoid complexity, set it the same as Zigbee.\n\t}\n\tLIMIT_INDEX = 476  # Max. freq = 2483 MHz (WiFi), (2483-2390)/100*512 ~ 476\n\t# IMAGE MAPPING\n\tVMIN = -10\n\tVMAX = 50\n"]}
{"filename": "core/_recording.py", "chunked_list": ["\"\"\"\n\tRecording class and related functionalities to support the recording object of the Dataset\n\t\"\"\"\n\timport os\n\timport sys\n\timport glob\n\timport subprocess\n\timport json\n\timport time\n\timport logging\n", "import numpy as np\n\tfrom argparse import Namespace\n\tfrom matplotlib import pyplot as plt\n\ttry:\n\t    from . import flowgraphs\n\texcept SyntaxError:\n\t    raise Exception(\"Flowgraphs cannot be imported. Please try with Python2.7.\")\n\tfrom . import _metadata as metadata\n\tfrom . import _utils as utils\n\tfrom . import _annotation as annotation\n", "from ._plotter import Plotter\n\tfrom ._annotation import Annotation\n\tfrom gen_pics import plot_recording\n\t__all__ = ['Recording']\n\tlog = logging.getLogger('spread')\n\tclass Recording(object):\n\t    \"\"\"\n\t    Represents a recording of the dataset with data parsed from the recording and its metadata\n\t    \"\"\"\n\t    heuristic_noise_calculation = 5.568650501352949 ** 2\n", "    @staticmethod\n\t    def get_rec_id(name):\n\t        \"\"\"\n\t        Returns the id of the recording when the name format is rec_*\n\t        \"\"\"\n\t        try:\n\t            return int(name.split('_')[-1])\n\t        except ValueError:\n\t            return -1\n\t    @staticmethod\n", "    def get_recname(recfile):\n\t        \"\"\"\n\t        Returns recording name based on the given path. The recording name is the basename of\n\t        the path without any extension.\n\t        :param recfile: path to recording file\n\t        :return: recording name\n\t        \"\"\"\n\t        recfile = os.path.abspath(recfile)\n\t        return os.path.splitext(os.path.basename(recfile))[0]\n\t    @staticmethod\n", "    def get_rectype(recfile):\n\t        \"\"\"\n\t        Returns recording type based on the given path.\n\t        :param recfile: path to recording file\n\t        :return: recording type\n\t        \"\"\"\n\t        recfile = os.path.abspath(recfile)\n\t        return os.path.splitext(os.path.basename(recfile))[1]\n\t    @classmethod\n\t    def merge_recordings(cls, rec_objects, outfile=None, mockup=False):\n", "        \"\"\"\n\t        Takes multiple recording objects as input and returns one recording object as output\n\t        :param rec_objects: list of recording objects to merge\n\t        :param outfile: filepath to save the result\n\t        :param mockup: If true, don't create the recording, just display the resulting metadata\n\t        :return Recording: Combined recording object\n\t        \"\"\"\n\t        if len(rec_objects) < 2:\n\t            log.info(\"At least 2 recordings are needed in order to merge...\")\n\t            return None\n", "        elif len(rec_objects) > 5:\n\t            log.info(\"Merging of more than 5 recordings is not implemented\")\n\t            return None\n\t        else:\n\t            ds = rec_objects[0].dataset\n\t            if not outfile:\n\t                outfile = ds.get_synthetic_outfile()\n\t                if not outfile:\n\t                    log.error(\"Error determining synthetic filename\")\n\t                    sys.exit(-1)\n", "                outfile = os.path.join(ds.recordings_dir, outfile)\n\t            log.info(\"Merging recordings %s to create %s\", ' '.join([x.name for x in rec_objects]),\n\t                     os.path.basename(outfile))\n\t            if mockup:\n\t                with open(outfile, 'w') as ow:\n\t                    ow.write(\"\")\n\t            else:\n\t                # Use the appropriate GNUradio script to combine the recordings\n\t                flowgraph_dict = {\n\t                    2: flowgraphs.merge2recordings.main,\n", "                    3: flowgraphs.merge3recordings.main,\n\t                    4: flowgraphs.merge4recordings.main,\n\t                    5: flowgraphs.merge5recordings.main,\n\t                }\n\t                # Create the appropriate filenames to pass as arguments\n\t                filenames = {'outfile': outfile}\n\t                # One for each input file\n\t                for i in range(len(rec_objects)):\n\t                    filenames['file%s' % (i + 1)] = rec_objects[i].recfile\n\t                args = Namespace(**filenames)\n", "                # Call the proper flowgraph with the arguments mapping\n\t                t = time.time()\n\t                try:\n\t                    flowgraph_dict[len(rec_objects)](options=args)\n\t                    log.info(\"GNUradio merging time: %s\", time.time()-t)\n\t                except RuntimeError as e:\n\t                    log.error(\"GNUradio failed to merge recordings. Error: \", str(e))\n\t                    return\n\t            # Initialize  the recording\n\t            return cls(outfile, rec_objects[0].dataset, no_md=True)\n", "    def __init__(self, recfile, dataset, no_md=False, recount_pictures=False):\n\t        # Dataset istance that the recording is member of\n\t        self.dataset = dataset\n\t        self.plotter = None\n\t        # Recording files and names\n\t        # Absolute recording file path\n\t        self.recfile = os.path.abspath(recfile)\n\t        # File descriptor of recording file, used to read chunks of data when processing\n\t        # Should be closed after operation.\n\t        self.file_descriptor = None\n", "        # Name of the recording with no extension (eg: rec_43)\n\t        self.name = Recording.get_recname(recfile)\n\t        # Id of recording (eg: 43)\n\t        self.id = Recording.get_rec_id(self.name)\n\t        # Directory for files needed for noise calculation (fft samples, SNR values, pics, etc)\n\t        self.noise_calc_dir = os.path.join(self.dataset.noise_calc_dir, self.name)\n\t        # SNR values file and fft samples file\n\t        self.dat_file = os.path.join(self.noise_calc_dir, self.name + '.dat')\n\t        self.fft_file = os.path.join(self.noise_calc_dir, self.name + '_fft.32fc')\n\t        # Pictures directory and picture files prefix (eg: rec_43_pic_546)\n", "        self.rec_pics_dir = os.path.join(self.dataset.pictures_dir, self.name)  # Pictures directory for the recording\n\t        self.compressed_pics_dir = os.path.join(self.rec_pics_dir, 'compressed_pictures')\n\t        self.pic_prefix = \"%s_pic\" % self.name\n\t        self.corrected_annotations_dir = os.path.join(self.rec_pics_dir, \"corrected_annotations\")\n\t        self.synthetic_annotations_dir = os.path.join(self.rec_pics_dir, \"synthetic_annotations\")\n\t        self.fixed_labels_dir = os.path.join(self.rec_pics_dir, \"corrected_labels\")\n\t        self._annotation_list = None\n\t        self._synth_annotation_list = None\n\t        self._corrected_annotation_list = None\n\t        self._fixed_label_list = None\n", "        self._compr_annotation_list = None\n\t        self._pic_list = None\n\t        self._compr_pic_list = None\n\t        if no_md:\n\t            self.metadata = None\n\t        else:\n\t            self.metadata = metadata.RecordingMetadata(self)\n\t        if recount_pictures:\n\t            self.metadata.no_of_pictures = self._count_all_pictures()\n\t            self.metadata._metadata['no_of_pictures'] = self.metadata.no_of_pictures\n", "            self.metadata.store_metadata()\n\t    def _get_annot_list(self):\n\t        \"\"\"Get a list of annotations generated in the picture directory\"\"\"\n\t        ann_pattern = os.path.join(self.rec_pics_dir, '%s_*.txt' % self.pic_prefix)\n\t        return sorted(glob.glob(ann_pattern), key=lambda x: int(utils.get_id_from_pic_name(os.path.basename(x))))\n\t    def _get_synth_annot_list(self):\n\t        \"\"\"Get a list of synthetic annotations generated for synthetic recordings\"\"\"\n\t        ann_pattern = os.path.join(self.synthetic_annotations_dir, '%s_*.txt' % self.pic_prefix)\n\t        return sorted(glob.glob(ann_pattern), key=lambda x: int(utils.get_id_from_pic_name(os.path.basename(x))))\n\t    def _get_corr_annot_list(self):\n", "        \"\"\"Get a list of corrected annotations, if available\"\"\"\n\t        ann_pattern = os.path.join(self.corrected_annotations_dir, '%s_*.txt' % self.pic_prefix)\n\t        return sorted(glob.glob(ann_pattern), key=lambda x: int(utils.get_id_from_pic_name(os.path.basename(x))))\n\t    def _get_fixed_label_list(self):\n\t        \"\"\"Get a list of manually fixed labels\"\"\"\n\t        ann_pattern = os.path.join(self.fixed_labels_dir, '%s_*.txt' % self.pic_prefix)\n\t        return sorted(glob.glob(ann_pattern), key=lambda x: int(utils.get_id_from_pic_name(os.path.basename(x))))\n\t    def _get_compr_annot_list(self):\n\t        \"\"\"Get a list of compressed annotations generated in the compressed picture directory\"\"\"\n\t        ann_pattern = os.path.join(self.compressed_pics_dir, '%s_*.txt' % self.pic_prefix)\n", "        return sorted(glob.glob(ann_pattern), key=lambda x: int(utils.get_id_from_pic_name(os.path.basename(x))))\n\t    def _get_pic_list(self, prefix=''):\n\t        \"\"\"\n\t        Gets the pictures generated for the recording\n\t        \"\"\"\n\t        pic_pattern = os.path.join(self.rec_pics_dir, prefix + '*.jpg')\n\t        return sorted(glob.glob(pic_pattern), key=lambda x: int(utils.get_id_from_pic_name(os.path.basename(x))))\n\t    def _get_compr_pic_list(self, prefix=''):\n\t        \"\"\"\n\t        Gets the compressed pictures generated for the recording\n", "        \"\"\"\n\t        pic_pattern = os.path.join(self.compressed_pics_dir, prefix + '*.jpg')\n\t        return sorted(glob.glob(pic_pattern), key=lambda x: int(utils.get_id_from_pic_name(os.path.basename(x))))\n\t    def _get_file_size(self):\n\t        \"\"\"\n\t        Calculates the disk space occupied by this recording, including the pictures generated by it\n\t        \"\"\"\n\t        bytes_size = os.path.getsize(self.recfile)\n\t        try:\n\t            bytes_size += os.path.getsize(self.rec_pics_dir)\n", "        except OSError:\n\t            # No pictures directory\n\t            pass\n\t        return utils.convert_size(bytes_size)\n\t    def _count_all_pictures(self):\n\t        \"\"\"\n\t        Counts all pictures associated with this recording\n\t        \"\"\"\n\t        return len(self.compressed_pic_list) + len(self.pic_list)\n\t    @property\n", "    def pic_list(self):\n\t        \"\"\"Returns a list of the generated pictures for the recording\"\"\"\n\t        if not self._pic_list:\n\t            self._pic_list = self._get_pic_list()\n\t        return self._pic_list\n\t    @property\n\t    def annotation_list(self):\n\t        \"\"\"Returns a list of annotation files (absolute paths) located in the picture directory of the recording\"\"\"\n\t        if not self._annotation_list:\n\t            self._annotation_list = self._get_annot_list()\n", "        return self._annotation_list\n\t    @property\n\t    def synth_annotation_list(self):\n\t        \"\"\"Returns a list of synthetic annotation files (absolute paths)\"\"\"\n\t        if not self._synth_annotation_list:\n\t            self._synth_annotation_list = self._get_synth_annot_list()\n\t        return self._synth_annotation_list\n\t    @property\n\t    def corrected_annotation_list(self):\n\t        \"\"\"Return a list of corrected annotations\"\"\"\n", "        if not self._corrected_annotation_list:\n\t            self._corrected_annotation_list = self._get_corr_annot_list()\n\t        return self._corrected_annotation_list\n\t    @property\n\t    def fixed_label_list(self):\n\t        \"\"\"Return list of manually fixed labels\"\"\"\n\t        if not self._fixed_label_list:\n\t            self._fixed_label_list = self._get_fixed_label_list()\n\t        return self._fixed_label_list\n\t    @property\n", "    def compressed_annotation_list(self):\n\t        \"\"\"\n\t        Returns a list of compressed annotation files (absolute paths) located in the compressed picture directory of\n\t        the recording\n\t        \"\"\"\n\t        if not self._compr_annotation_list:\n\t            self._compr_annotation_list = self._get_compr_annot_list()\n\t        return self._compr_annotation_list\n\t    @property\n\t    def compressed_pic_list(self):\n", "        \"\"\"\n\t        Returns a list of compressed picture files (absolute paths) located in the compressed picture directory of\n\t        the recording\n\t        \"\"\"\n\t        if not self._compr_pic_list:\n\t            self._compr_pic_list = self._get_compr_pic_list()\n\t        return self._compr_pic_list\n\t    @property\n\t    def file_size(self):\n\t        \"\"\"\n", "        Returns the recording size including the complex samples and the generated pictures.\n\t        File size is read from metadata or if None, it's calculated by parsing the filesystem.\n\t        \"\"\"\n\t        # return self.metadata.metadata.get('file_size', '')\n\t        return self.metadata.metadata.get('file_size', str(self._get_file_size()))\n\t    def play_samples(self):\n\t        \"\"\"\n\t        Loads the complex samples and previews them with GNUradio\n\t        \"\"\"\n\t        log.info(\"Playing recording %s.\", self.name)\n", "        flowgraphs.view_samples_from_file.main(options=Namespace(filename=self.recfile, freq=2.44e9,\n\t                                                                 refresh_rate=30, samp_rate=100e6,\n\t                                                                 fft_size=512))\n\t        log.info(\"Done.\")\n\t        return\n\t    def gen_dat_file(self, noise_pwr_db=None, fft_size=512):\n\t        \"\"\"\n\t        Generates the SNR file\n\t        \"\"\"\n\t        # By default, get the noise level from metadata\n", "        if not noise_pwr_db:\n\t            noise_pwr_db = int(round(float(self.metadata.noise_pwr_db)))\n\t            # Noise metadata defaults to 0 if non existant.\n\t            if noise_pwr_db == 0:\n\t                noise_pwr_db = -50  # -50 is a reasonable starting point for noise level\n\t        # Create the directory if needed\n\t        if not os.path.isdir(self.noise_calc_dir):\n\t            os.makedirs(self.noise_calc_dir)\n\t        flowgraphs.samples_to_dat.main(options=Namespace(filename=self.recfile,\n\t                                                         output=os.path.splitext(self.dat_file)[0],\n", "                                                         noise_pwr_db=noise_pwr_db, fft_size=fft_size))\n\t    def gen_fft_file(self, fft_size=512):\n\t        \"\"\"\n\t        Generates the fft samples\n\t        \"\"\"\n\t        # Create the directory if needed\n\t        if not os.path.isdir(self.noise_calc_dir):\n\t            os.makedirs(self.noise_calc_dir)\n\t        flowgraphs.samples_fft.main(options=Namespace(filename=self.recfile, output=os.path.splitext(self.fft_file)[0],\n\t                                                      fft_size=fft_size))\n", "    def remove_dat_file(self):\n\t        \"\"\"\n\t        Deletes the SNR file\n\t        \"\"\"\n\t        try:\n\t            os.remove(self.dat_file)\n\t        except OSError:\n\t            pass\n\t    def remove_fft_file(self):\n\t        \"\"\"\n", "        Deletes the fft samples\n\t        \"\"\"\n\t        try:\n\t            os.remove(self.fft_file)\n\t        except OSError:\n\t            pass\n\t    def remove_pictures(self):\n\t        \"\"\"\n\t        Deletes all pictures and noise calculations of the recording\n\t        \"\"\"\n", "        cmd = \"rm -r %s %s\" % (self.noise_calc_dir, self.rec_pics_dir)\n\t        subprocess.call(cmd.split())\n\t    def create_artificial_data(self, mold=None, freq_steps=None, time_steps=None, prefix=None, figdir=None, label=None):\n\t        \"\"\"\n\t        Uses a given transmission area as a mold to create artificial data on various\n\t        \"\"\"\n\t        if not os.environ.get('DISPLAY', None):\n\t            log.warning(\"No available X server. Interactive mold creation cannot proceed without display.\\n\\\n\t                        Consider connecting with X server forwarding, otherwise consider passing an input array\"\n\t                        \"to use as a mold.\")\n", "            return None\n\t        if not self.metadata.noise_pwr_db:\n\t            log_noise, noise_variance = self.calculate_noise()\n\t            if not log_noise:\n\t                log.error(\"Recording %s: Noise level could not be determined, unable to create mold.\", self.name)\n\t                return None\n\t        else:\n\t            log_noise = self.metadata.noise_pwr_db\n\t            noise_variance = self.metadata.noise_variance\n\t            if not noise_variance:\n", "                noise_variance = 5.568650501352949 ** 2\n\t        log.info(\"Creating artificial data for recording %s\", self.name)\n\t        if label:\n\t            try:\n\t                # If label is given like an integer use this to annotate the pictures\n\t                label_index = int(label)\n\t            except ValueError:\n\t                # otherwise look for the dataset classes to find the corresponding index\n\t                label_index = self.dataset.metadata.ds_classes().get(label.upper(), None)\n\t            if not label_index:\n", "                log.error(\"Label not found in the dataset classes. Please update the file %s to include the desired\"\n\t                          \"label or provide a label index directly for a new class.\",\n\t                          self.dataset.metadata.class_names_file)\n\t                return\n\t        if not mold:\n\t            if not os.path.isfile(self.dat_file):\n\t                self.gen_dat_file()\n\t            log.info(\"Please draw a region in the following picture to identify mold\")\n\t            npoints = 512 * 512\n\t            nbytes = npoints * 4\n", "            # Preview the first 10 images for the user to pick a mold region\n\t            img_index = 0\n\t            with open(self.dat_file, 'rb') as df:\n\t                while True:\n\t                    if img_index > 9:\n\t                        break\n\t                    data = utils.load_bytes_from_fd(df, img_index * nbytes, (img_index + 1) * nbytes)\n\t                    if not data:  # No more data to unpack\n\t                        break\n\t                    data = utils.data_reshape(data, -1, 512)\n", "                    # Process the data before creating the image, create a copy to keep original intact\n\t                    img_data = utils.data_clip(np.copy(data), -10, 50)\n\t                    img_data = utils.img_scale(img_data, -10, 50)\n\t                    img_data = utils.img_flip(img_data)\n\t                    subplot = plt.subplot()\n\t                    pltr = Plotter()\n\t                    pltr.plot(data=img_data, subplot=subplot, options={'noise_input': True})\n\t                    # If input area was given, break\n\t                    if pltr.selected_areas:\n\t                        log.info(\"Mold region recognized:\\n%s\", pltr.pretty_area_print())\n", "                        left, right, bottom, up = pltr.area_borders()\n\t                        # Get the average SNR from the original data (mainly before scaling)\n\t                        mold = data[up:bottom, left:right]\n\t                        break\n\t        # Create a background noise array for the artificial data\n\t        mu = log_noise\n\t        sigma = float(noise_variance) ** 0.5\n\t        noise_array = np.random.normal(mu - log_noise, sigma, (512, 512))\n\t        # Prepare the prefix and save directories\n\t        if not prefix:\n", "            prefix = self.pic_prefix\n\t        if not figdir:\n\t            figdir = os.path.join('.', '%s_artificial_data' % self.name)\n\t        if not os.path.isdir(figdir):\n\t            os.makedirs(figdir)\n\t        # Prepare the annotations to be augmented along with the data\n\t        if label:\n\t            annot = Annotation.get_annotation_from_borders(\n\t                label_index,\n\t                Annotation.nrmlz(left, 512),\n", "                Annotation.nrmlz(right, 512),\n\t                Annotation.nrmlz(bottom, 512),\n\t                Annotation.nrmlz(up, 512)\n\t            )\n\t            if not annot:\n\t                log.error(\"There was an error creating the original annotation. Exiting.\")\n\t                return\n\t        img_index = 0\n\t        for tstep in time_steps:\n\t            # Printed (saved) image is actually flipped over the time axes to better illustrate the flow of packets over\n", "            # time. Thus, input time steps must also be corrected to follow the flipped orientation of the image.\n\t            tstep = 512 - tstep\n\t            t_start = tstep - mold.shape[0] / 2\n\t            t_end = t_start + mold.shape[0]\n\t            for fstep in freq_steps:\n\t                f_start = fstep - mold.shape[1] / 2\n\t                f_end = f_start + mold.shape[1]\n\t                artif_arr = np.copy(noise_array)\n\t                try:\n\t                    artif_arr[t_start:t_end, f_start:f_end] = mold\n", "                except ValueError:\n\t                    log.error(\"There was an error patching the requested region at the position with:\\n\"\n\t                              \"Center: (%s, %s).\\n Make sure the region can fit in the image. Skipping...\",\n\t                              fstep, tstep)\n\t                    continue\n\t                img_name = \"%s_%d.jpg\" % (prefix, img_index)\n\t                artif_arr = utils.data_clip(artif_arr, -10, 50)\n\t                artif_arr = utils.img_flip(utils.img_scale(artif_arr, -10, 50))\n\t                pltr = Plotter()\n\t                pltr.plot(data=artif_arr, outfile=img_name, figdir=figdir)\n", "                # Shift annotation accordingly and save to file\n\t                if label:\n\t                    annot.shift_center((Annotation.nrmlz(fstep, 512), Annotation.nrmlz(tstep, 512)))\n\t                    annot_str = annot.get_annot_str()\n\t                    ann_file = os.path.join(figdir, \"%s_%d.txt\" % (prefix, img_index))\n\t                    with open(ann_file, 'w') as af:\n\t                        af.write(annot_str)\n\t                img_index += 1\n\t        log.info(\"Artificial data created for recording %s\", self.name)\n\t    def calculate_noise(self):\n", "        \"\"\"\n\t        Calculate the noise level in dB in a given region of the picture\n\t        \"\"\"\n\t        if not os.environ.get('DISPLAY', None):\n\t            log.warning(\"No available X server. Interactive noise calculation cannot proceed without display.\\n\\\n\t                        Consider connecting with X server forwarding, otherwise manually calculate noise and update\\\n\t                        the recording metadata accordingly.\")\n\t            return None, None\n\t        # Always regenerate the dat file to make sure the ground truth is consistent\n\t        self.gen_dat_file(noise_pwr_db=-50)\n", "        npoints = 512 * 512\n\t        nbytes = npoints * 4\n\t        # Preview the first 10 images for the user to pick a noise region\n\t        log_noise = None\n\t        img_index = 0\n\t        with open(self.dat_file, 'rb') as df:\n\t            while True:\n\t                if img_index > 9:\n\t                    break\n\t                data = utils.load_bytes_from_fd(df, img_index * nbytes, (img_index + 1) * nbytes)\n", "                if not data:  # No more data to unpack\n\t                    break\n\t                data = utils.data_reshape(data, -1, 512)\n\t                # Process the data before creating the image, create a copy to keep original intact\n\t                img_data = utils.data_clip(np.copy(data), -10, 50)\n\t                img_data = utils.img_scale(img_data, -10, 50)\n\t                img_data = utils.img_flip(img_data)\n\t                subplot = plt.subplot()\n\t                pltr = Plotter()\n\t                pltr.plot(data=img_data, subplot=subplot, options={'noise_input': True})\n", "                # If input area was given, break\n\t                if pltr.selected_areas:\n\t                    log.info(\"Noise region recognized: %s\", pltr.pretty_area_print())\n\t                    left, right, bottom, up = pltr.area_borders()\n\t                    # Get the average SNR from the original data (mainly before scaling)\n\t                    cropped = data[up:bottom, left:right]\n\t                    # This is the avg SNR in the cropped region, we need to add it to the noise level that was used to\n\t                    # create the dat file in the first place\n\t                    avg_snr_db = np.mean(cropped)\n\t                    noise_variance = np.var(cropped)\n", "                    log_noise = -50 + avg_snr_db\n\t                    # Remove dat file because it was created with a default noise value rather than the real one\n\t                    self.remove_dat_file()\n\t                    log.info(\"Noise level calculated: %s dB\", log_noise)\n\t                    break\n\t                img_index += 1\n\t        return log_noise, noise_variance\n\t    def compress_annotations(self, compr_factor, merge=True):\n\t        \"\"\"\n\t        Compress annotations into the compressed picture directory. If no original annotations are found, an error is\n", "        returned and nothing is generated.\n\t        \"\"\"\n\t        to_compress = self.fixed_label_list if not self.metadata.synthetic else self.synth_annotation_list\n\t        if not to_compress:\n\t            log.info(\"No corrected labels found for recording %s. Nothing to compress...\", self.name)\n\t            return\n\t        if not os.path.isdir(self.compressed_pics_dir):\n\t            os.mkdir(self.compressed_pics_dir)\n\t        compressed_pic_annotations = []\n\t        # Fetch all original annotations for every picture\n", "        for pic_ann in to_compress:\n\t            new_pic_index = to_compress.index(pic_ann) / compr_factor\n\t            pic_index = to_compress.index(pic_ann) % compr_factor\n\t            with open(pic_ann, 'r') as orig_ann:\n\t                pic_annotations = orig_ann.read().strip().split('\\n')\n\t            extend_annot = [annotation.Annotation.get_annotation_from_str(x) for x in pic_annotations]\n\t            extend_annot = [annotation.Annotation.compress_annotation(x, compr_factor, pic_index) for x in\n\t                            extend_annot]\n\t            compressed_pic_annotations.extend(extend_annot)\n\t            # Save the compressed annotation\n", "            if pic_index == compr_factor - 1:\n\t                if merge:\n\t                    compressed_pic_annotations = annotation.Annotation.merge_annotations(compressed_pic_annotations)\n\t                compressed_pic_annotations = '\\n'.join([x.get_annot_str() for x in compressed_pic_annotations\n\t                                                        if x.get_annot_str()])\n\t                compressed_ann_file = os.path.join(self.compressed_pics_dir,\n\t                                                   self.pic_prefix + \"_\" + str(new_pic_index) + \".txt\")\n\t                with open(compressed_ann_file, 'w') as comp_ann:\n\t                    comp_ann.write(compressed_pic_annotations)\n\t                compressed_pic_annotations = []\n", "        log.info(\"Compressed annotations for recording %s were saved in: %s\", self.name, self.compressed_pics_dir)\n\t    def generate_pictures(self, log_noise=None, nfft=512, nlines=512, navg=3, nproc=4, npics=0, pic_prefix=None,\n\t                          mode='grayscale', expand=None, trim=50):\n\t        \"\"\"\n\t        Generates pictures from a recording file\n\t        \"\"\"\n\t        # Clipping parameters\n\t        min_snr = -10\n\t        max_snr = 50\n\t        log.info(\"Generating pictures for recording: %s\", self.name)\n", "        # Use recorded noise measurements unless overridden\n\t        noise_var = None\n\t        if not log_noise:\n\t            log_noise = int(round(self.metadata.noise_pwr_db))\n\t            if not log_noise:\n\t                log_noise, noise_var = self.calculate_noise()\n\t                if not log_noise:\n\t                    log.error(\"Recording %s: Noise level could not be determined, no pictures generated.\", self.name)\n\t                    return\n\t                else:\n", "                    self.metadata.noise_pwr_db = log_noise\n\t                    self.metadata.noise_variance = noise_var\n\t                    self.metadata._metadata['noise_db'] = self.metadata.noise_pwr_db\n\t                    self.metadata._metadata['noise_variance'] = self.metadata.noise_variance\n\t                    self.metadata.store_metadata()\n\t                    log_noise = int(round(float(self.metadata.noise_pwr_db)))\n\t        if not noise_var:\n\t            noise_var = float(self.metadata.noise_variance)\n\t            if not noise_var:\n\t                noise_var = Recording.heuristic_noise_calculation\n", "        # If expanding to a wider bandwidth, create array with noise values as background\n\t        if expand:\n\t            transm_freq = float(expand[0])\n\t            transm_rate = float(expand[1])\n\t            wide_freq = float(expand[2])\n\t            wide_rate = float(expand[3])\n\t            avg_factor = int(wide_rate / transm_rate)\n\t            mu = log_noise\n\t            sigma = float(noise_var) ** 0.5\n\t            noise_array = np.random.normal(mu-log_noise, sigma, (nlines, nfft * avg_factor))\n", "        if not os.path.isdir(self.rec_pics_dir):\n\t            os.makedirs(self.rec_pics_dir)\n\t        # Use default picture prefix unless specified\n\t        if not pic_prefix:\n\t            pic_prefix = self.pic_prefix\n\t        npoints = nfft * nlines * navg * nproc\n\t        if mode.lower() == 'grayscale':\n\t            if not os.path.isfile(self.dat_file):\n\t                self.gen_dat_file()\n\t            nbytes = npoints * 4\n", "            img_index = 0\n\t            with open(self.dat_file, \"rb\") as df:\n\t                while True:\n\t                    data = utils.load_bytes_from_fd(df, img_index * nbytes, (img_index + 1) * nbytes)\n\t                    if not data:  # No more data to unpack\n\t                        break\n\t                    # Reshape into an array of (nfft, nlines)\n\t                    data = utils.data_reshape(data, -1, nfft)\n\t                    # If expanding to a wider bandwidth average the loaded data accordingly and fit them into the\n\t                    # previously created noise array (background)\n", "                    if expand:\n\t                        if not trim:\n\t                            trim = 0\n\t                        # Position the transmission subarray in the new wider array\n\t                        new_start_freq = wide_freq - wide_rate / 2.0\n\t                        sub_array_center = int((transm_freq - new_start_freq) * (int(nfft) / wide_rate) * avg_factor)\n\t                        sub_array_size = int(nfft)\n\t                        sub_array_start = sub_array_center - sub_array_size / 2\n\t                        sub_array_end = sub_array_start + sub_array_size\n\t                        noise_array[:, sub_array_start + trim:sub_array_end - trim] = data[:, trim:-trim]\n", "                        data = noise_array\n\t                    greyscale_avg = navg * nproc\n\t                    if greyscale_avg > 1 and type(greyscale_avg) is int:\n\t                        avg_data = np.empty((int(data.shape[0] / greyscale_avg), data.shape[1]))\n\t                        for i in range(0, data.shape[0], greyscale_avg):\n\t                            try:\n\t                                avg_data[int(i / greyscale_avg)] = np.mean(data[i:i + greyscale_avg], axis=0, keepdims=True)\n\t                            except IndexError as e:\n\t                                if int(i / greyscale_avg) >= data.shape[0] / greyscale_avg:\n\t                                    # Last chunk of data reached\n", "                                    break\n\t                                else:\n\t                                    raise e\n\t                    else:\n\t                        avg_data = data\n\t                    avg_data = utils.data_clip(avg_data, min_snr, max_snr)\n\t                    avg_data = utils.img_flip(utils.img_scale(avg_data, min_snr, max_snr))\n\t                    img_name = \"%s_%d.jpg\" % (pic_prefix, img_index)\n\t                    pltr = Plotter()\n\t                    pltr.plot(data=avg_data, outfile=img_name, figdir=self.rec_pics_dir, resize=(nfft, nlines))\n", "                    img_index += 1\n\t                    # Check if img limit is reached and exit\n\t                    if npics and npics > 0:\n\t                        if img_index >= npics:\n\t                            break\n\t            self.remove_dat_file()\n\t        elif mode.lower() == 'compressed':\n\t            self.gen_fft_file()\n\t            plot_recording(self.fft_file, self.compressed_pics_dir, pic_prefix, nfft, nlines, navg, nproc,\n\t                           log_noise=log_noise, img_mode=mode, disp=\"save\", img_limit=npics)\n", "            self.remove_fft_file()\n\t        if not npics:\n\t            npics = 'All'\n\t        pic_out_dir = self.rec_pics_dir if mode == 'grayscale' else self.compressed_pics_dir\n\t        log.info(\"%s pictures were generated in the directory: %s\", npics, pic_out_dir)\n\t        return\n\t    def print_info(self):\n\t        \"\"\"\n\t        Prints info about the recording\n\t        \"\"\"\n", "        log.info(\"\\\n\tInformation about recording %s:\\n\\n\\\n\tRecorded on %s. \\n\\n\\\n\tNumber of pictures generated: %s\\n\\n\\\n\tMetadata \\n%s\", self.name,\n\t                 self.metadata.date_recorded,\n\t                 self.metadata.no_of_pictures,\n\t                 self.metadata.get_md_string())\n\t        log.info(json.dumps(self.metadata.metadata))\n"]}
{"filename": "core/_box.py", "chunked_list": ["\"\"\"\n\tDefinition of the box object\n\t\"\"\"\n"]}
{"filename": "core/gen_pics.py", "chunked_list": ["\"\"\"\n\tHelper script to generate images for recordings. Used by class `Recording`.\n\t\"\"\"\n\timport argparse\n\timport struct\n\timport sys\n\tfrom PIL import Image\n\timport numpy as np\n\timport os\n\tfrom . import _utils as utils\n", "# from core import img_scale, data_clip\n\tSNR_MIN = -10\n\tSNR_MAX = 50\n\tnp.set_printoptions(threshold=sys.maxsize)\n\tdef data_IO_snr(fopen, npoints, nfft, navg):\n\t    \"\"\"\n\t    IO from a SNR file.\n\t    \"\"\"\n\t    binary = fopen.read(npoints*4)\n\t    syntax = str(npoints) + \"f\"\n", "    data = struct.unpack(syntax, binary)\n\t    data = np.reshape(data, (-1, nfft))\n\t    if navg > 1 and type(navg) is int:\n\t        avg_data = np.empty((data.shape[0]/navg, data.shape[1]))\n\t        for i in range(0, data.shape[0], navg):\n\t            avg_data[i/navg] = np.mean(data[i:i+navg], axis=0, keepdims=True)\n\t        utils.data_clip(avg_data, SNR_MIN, SNR_MAX)\n\t        avg_data = np.flip(utils.img_scale(avg_data, SNR_MIN, SNR_MAX),axis=0)\n\t        return avg_data\n\t    else:\n", "        utils.data_clip(data, SNR_MIN, SNR_MAX)\n\t        data = np.flip(utils.img_scale(data, SNR_MIN, SNR_MAX),axis=0)\n\t        return data\n\tdef data_IO_raw_compressed(fopen, npoints, nfft, navg, nproc, log_noise):\n\t    \"\"\"\n\t    IO from an FFT-ed complex recording file.\n\t    \"\"\"\n\t    binary = fopen.read(npoints*4*2)\n\t    syntax = str(npoints*2) + \"f\"\n\t    data = struct.unpack(syntax, binary)\n", "    data = np.reshape(data, (-1, nfft*2))\n\t    real = np.take(data, np.arange(0, data.shape[1], 2), axis=1)\n\t    imge = np.take(data, np.arange(1, data.shape[1], 2), axis=1)\n\t    pwr = real**2+imge**2\n\t    # Window Averaging\n\t    avg_pwr = np.empty((pwr.shape[0]/navg, pwr.shape[1]))\n\t    for i in range(0, pwr.shape[0], navg):\n\t        avg_pwr[i/navg] = np.mean(pwr[i:i+navg,:], axis=0, keepdims=True)\n\t    # Window Max-Min-\n\t    max_pwr = np.empty((avg_pwr.shape[0]/nproc, avg_pwr.shape[1]))\n", "    min_pwr = np.empty((avg_pwr.shape[0]/nproc, avg_pwr.shape[1]))\n\t    avg_pwr_2 = np.empty((avg_pwr.shape[0]/nproc, avg_pwr.shape[1]))\n\t    for i in range(0, avg_pwr.shape[0], nproc):\n\t        max_pwr[i/nproc] = np.max(avg_pwr[i:i+nproc,:], axis=0, keepdims=True)\n\t        min_pwr[i/nproc] = np.min(avg_pwr[i:i+nproc,:], axis=0, keepdims=True)\n\t        avg_pwr_2[i/nproc] = np.mean(avg_pwr[i:i+nproc,:], axis=0, keepdims=True)\n\t    max_pwr = (10*np.log10(max_pwr)-log_noise).astype(int)\n\t    min_pwr = (10*np.log10(min_pwr)-log_noise).astype(int)\n\t    avg_pwr_2 = (10*np.log10(avg_pwr_2)-log_noise).astype(int)\n\t    # utils.data_clip, scaling\n", "    utils.data_clip(max_pwr, SNR_MIN, SNR_MAX)\n\t    utils.data_clip(min_pwr, SNR_MIN, SNR_MAX)\n\t    utils.data_clip(avg_pwr_2, SNR_MIN, SNR_MAX)\n\t    max_pwr = np.flip(utils.img_scale(max_pwr, SNR_MIN, SNR_MAX), axis=0)\n\t    min_pwr = np.flip(utils.img_scale(min_pwr, SNR_MIN, SNR_MAX), axis=0)\n\t    avg_pwr_2 = np.flip(utils.img_scale(avg_pwr_2, SNR_MIN, SNR_MAX), axis=0)\n\t    return max_pwr, min_pwr, avg_pwr_2\n\tdef spectro_plot(data_img, disp, img_name):\n\t    im = Image.fromarray(data_img)\n\t    if disp == 'save':\n", "        im.save(img_name)\n\t    elif disp == 'show':\n\t        im.show()\n\t    return\n\tdef plot_recording(file, figdir, prefix, nfft, nline, navg, nproc, log_noise, img_mode='grayscale', disp='save', img_limit=None):\n\t    \"\"\"\n\t        Plot the recorded data.\n\t        img_mode: 'grayscale' - Replicate SNR data in 3 channels\n\t                  'compressed' - Compress data for each channel\n\t    \"\"\"\n", "    NPOINTS = nfft*nline*navg*nproc\n\t    fopen = open(file, \"rb\")\n\t    if not os.path.isdir(figdir):\n\t        os.makedirs(figdir)\n\t    img_index = 0\n\t    while True:\n\t        try:\n\t            if img_mode == 'grayscale':\n\t                data = data_IO_snr(fopen, NPOINTS, nfft, navg)\n\t                data_img = np.stack((data, data, data), axis=-1)\n", "            elif img_mode == 'compressed':\n\t                data_ch1, data_ch2, data_ch3 = data_IO_raw_compressed(fopen, NPOINTS, nfft, navg, nproc, log_noise)\n\t                data_img = np.stack((data_ch1, data_ch2, data_ch3), axis=-1)\n\t            else:\n\t                print(\"Unrecognized mode: \", img_mode)\n\t                return\n\t            fname = figdir + \"/\" + prefix + \"_\" + str(img_index) + \".jpg\"\n\t            spectro_plot(data_img, disp, fname)\n\t            img_index += 1\n\t            # Check if img limit is reached and exit\n", "            if img_limit and img_limit>0:\n\t                if img_index == img_limit:\n\t                    print(\"Image limit reached: %s. Stopping...\", img_limit)\n\t                    break\n\t        except struct.error:\n\t            print(\"Done.\")\n\t            break\n\t    # Always close the file after done\n\t    fopen.close()\n\t    return\n", "if __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"file\", type=str, required=True,\n\t        help='Path to data: If mode is \"grayscale\" or \"discretized\" (experimental), the file should include SNR values (.dat file). If mode is \"compressed\", the file should include I&Q values after FFT.')\n\t    parser.add_argument(\"--figdir\", type=str, required=True,\n\t        help='Path to figure storage')\n\t    parser.add_argument(\"--prefix\", type=str, default='rec',\n\t        help='Prefix of the images')\n\t    parser.add_argument(\"--nfft\", type=int, default=512,\n\t        help='Num. of FFT points')\n", "    parser.add_argument(\"--nline\", type=int, default=512,\n\t        help='Num. of data lines to plot (after avg)')\n\t    parser.add_argument(\"--navg\", type=int, default=10,\n\t        help='Average window size')\n\t    parser.add_argument(\"--nproc\", type=int, default=10,\n\t        help='Max/min window size')\n\t    parser.add_argument(\"--log-noise\", type=int, default=-47,\n\t        help='Measured log-noise level.')\n\t    parser.add_argument(\"--img-mode\", type=str, default='grayscale',\n\t        help='Image mode: grayscale, compressed, discretized')\n", "    parser.add_argument(\"--disp\", type=str, default='save',\n\t        help='Display mode')\n\t    parser.add_argument(\"--img-limit\", type=int,\n\t        help='Limit the images to be generated.')\n\t    args = parser.parse_args()\n\t    plot_recording(args.file, args.figdir, args.prefix, args.nfft, args.nline, args.navg, args.nproc,\n\t                   args.log_noise, img_mode=args.img_mode, disp=args.disp, img_limit=args.img_limit)\n"]}
{"filename": "core/flowgraphs/merge5recordings.py", "chunked_list": ["#!/usr/bin/env python2\n\t# -*- coding: utf-8 -*-\n\t##################################################\n\t# GNU Radio Python Flow Graph\n\t# Title: Merge5Recordings\n\t# GNU Radio version: 3.7.13.5\n\t##################################################\n\tfrom gnuradio import blocks\n\tfrom gnuradio import eng_notation\n\tfrom gnuradio import gr\n", "from gnuradio.eng_option import eng_option\n\tfrom gnuradio.filter import firdes\n\tfrom optparse import OptionParser\n\timport pmt\n\tclass merge5recordings(gr.top_block):\n\t    def __init__(self, file1='', file2='', file3='', file4='', file5='', outfile=''):\n\t        gr.top_block.__init__(self, \"Merge5Recordings\")\n\t        ##################################################\n\t        # Parameters\n\t        ##################################################\n", "        self.file1 = file1\n\t        self.file2 = file2\n\t        self.file3 = file3\n\t        self.file4 = file4\n\t        self.file5 = file5\n\t        self.outfile = outfile\n\t        ##################################################\n\t        # Blocks\n\t        ##################################################\n\t        self.blocks_file_source_0_0_0_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file5, False)\n", "        try:\n\t            self.blocks_file_source_0_0_0_0_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_source_0_0_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file4, False)\n\t        try:\n\t            self.blocks_file_source_0_0_0_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n", "            pass\n\t        self.blocks_file_source_0_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file3, False)\n\t        try:\n\t            self.blocks_file_source_0_0_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_source_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file2, False)\n\t        try:\n\t            self.blocks_file_source_0_0.set_begin_tag(pmt.PMT_NIL)\n", "        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_source_0 = blocks.file_source(gr.sizeof_gr_complex*1, file1, False)\n\t        try:\n\t            self.blocks_file_source_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_sink_0 = blocks.file_sink(gr.sizeof_gr_complex*1, outfile, False)\n", "        self.blocks_file_sink_0.set_unbuffered(False)\n\t        self.blocks_add_xx_0 = blocks.add_vcc(1)\n\t        ##################################################\n\t        # Connections\n\t        ##################################################\n\t        self.connect((self.blocks_add_xx_0, 0), (self.blocks_file_sink_0, 0))\n\t        self.connect((self.blocks_file_source_0, 0), (self.blocks_add_xx_0, 0))\n\t        self.connect((self.blocks_file_source_0_0, 0), (self.blocks_add_xx_0, 1))\n\t        self.connect((self.blocks_file_source_0_0_0, 0), (self.blocks_add_xx_0, 2))\n\t        self.connect((self.blocks_file_source_0_0_0_0, 0), (self.blocks_add_xx_0, 3))\n", "        self.connect((self.blocks_file_source_0_0_0_0_0, 0), (self.blocks_add_xx_0, 4))\n\t    def get_file1(self):\n\t        return self.file1\n\t    def set_file1(self, file1):\n\t        self.file1 = file1\n\t        self.blocks_file_source_0.open(self.file1, False)\n\t    def get_file2(self):\n\t        return self.file2\n\t    def set_file2(self, file2):\n\t        self.file2 = file2\n", "        self.blocks_file_source_0_0.open(self.file2, False)\n\t    def get_file3(self):\n\t        return self.file3\n\t    def set_file3(self, file3):\n\t        self.file3 = file3\n\t        self.blocks_file_source_0_0_0.open(self.file3, False)\n\t    def get_file4(self):\n\t        return self.file4\n\t    def set_file4(self, file4):\n\t        self.file4 = file4\n", "        self.blocks_file_source_0_0_0_0.open(self.file4, False)\n\t    def get_file5(self):\n\t        return self.file5\n\t    def set_file5(self, file5):\n\t        self.file5 = file5\n\t        self.blocks_file_source_0_0_0_0_0.open(self.file5, False)\n\t    def get_outfile(self):\n\t        return self.outfile\n\t    def set_outfile(self, outfile):\n\t        self.outfile = outfile\n", "        self.blocks_file_sink_0.open(self.outfile)\n\tdef argument_parser():\n\t    parser = OptionParser(usage=\"%prog: [options]\", option_class=eng_option)\n\t    parser.add_option(\n\t        \"\", \"--file1\", dest=\"file1\", type=\"string\", default='',\n\t        help=\"Set file1 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--file2\", dest=\"file2\", type=\"string\", default='',\n\t        help=\"Set file2 [default=%default]\")\n\t    parser.add_option(\n", "        \"\", \"--file3\", dest=\"file3\", type=\"string\", default='',\n\t        help=\"Set file3 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--file4\", dest=\"file4\", type=\"string\", default='',\n\t        help=\"Set file4 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--file5\", dest=\"file5\", type=\"string\", default='',\n\t        help=\"Set file5 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--outfile\", dest=\"outfile\", type=\"string\", default='',\n", "        help=\"Set outfile [default=%default]\")\n\t    return parser\n\tdef main(top_block_cls=merge5recordings, options=None):\n\t    if options is None:\n\t        options, _ = argument_parser().parse_args()\n\t    tb = top_block_cls(file1=options.file1, file2=options.file2, file3=options.file3, file4=options.file4, file5=options.file5, outfile=options.outfile)\n\t    tb.start()\n\t    tb.wait()\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "core/flowgraphs/__init__.py", "chunked_list": ["\"\"\"\n\tInitialize package\n\t\"\"\"\n\tfrom . import view_samples_from_file\n\tfrom . import freq_samples_to_dat\n\tfrom . import samples_fft\n\tfrom . import samples_to_dat\n\tfrom . import merge2recordings\n\tfrom . import merge3recordings\n\tfrom . import merge4recordings\n", "from . import merge5recordings\n"]}
{"filename": "core/flowgraphs/merge4recordings.py", "chunked_list": ["#!/usr/bin/env python2\n\t# -*- coding: utf-8 -*-\n\t##################################################\n\t# GNU Radio Python Flow Graph\n\t# Title: Merge4Recordings\n\t# GNU Radio version: 3.7.13.5\n\t##################################################\n\tfrom gnuradio import blocks\n\tfrom gnuradio import eng_notation\n\tfrom gnuradio import gr\n", "from gnuradio.eng_option import eng_option\n\tfrom gnuradio.filter import firdes\n\tfrom optparse import OptionParser\n\timport pmt\n\tclass merge4recordings(gr.top_block):\n\t    def __init__(self, file1='', file2='', file3='', file4='', outfile=''):\n\t        gr.top_block.__init__(self, \"Merge4Recordings\")\n\t        ##################################################\n\t        # Parameters\n\t        ##################################################\n", "        self.file1 = file1\n\t        self.file2 = file2\n\t        self.file3 = file3\n\t        self.file4 = file4\n\t        self.outfile = outfile\n\t        ##################################################\n\t        # Blocks\n\t        ##################################################\n\t        self.blocks_file_source_0_0_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file4, False)\n\t        try:\n", "            self.blocks_file_source_0_0_0_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_source_0_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file3, False)\n\t        try:\n\t            self.blocks_file_source_0_0_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n", "        self.blocks_file_source_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file2, False)\n\t        try:\n\t            self.blocks_file_source_0_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_source_0 = blocks.file_source(gr.sizeof_gr_complex*1, file1, False)\n\t        try:\n\t            self.blocks_file_source_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n", "            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_sink_0 = blocks.file_sink(gr.sizeof_gr_complex*1, outfile, False)\n\t        self.blocks_file_sink_0.set_unbuffered(False)\n\t        self.blocks_add_xx_0 = blocks.add_vcc(1)\n\t        ##################################################\n\t        # Connections\n\t        ##################################################\n\t        self.connect((self.blocks_add_xx_0, 0), (self.blocks_file_sink_0, 0))\n\t        self.connect((self.blocks_file_source_0, 0), (self.blocks_add_xx_0, 0))\n", "        self.connect((self.blocks_file_source_0_0, 0), (self.blocks_add_xx_0, 1))\n\t        self.connect((self.blocks_file_source_0_0_0, 0), (self.blocks_add_xx_0, 2))\n\t        self.connect((self.blocks_file_source_0_0_0_0, 0), (self.blocks_add_xx_0, 3))\n\t    def get_file1(self):\n\t        return self.file1\n\t    def set_file1(self, file1):\n\t        self.file1 = file1\n\t        self.blocks_file_source_0.open(self.file1, False)\n\t    def get_file2(self):\n\t        return self.file2\n", "    def set_file2(self, file2):\n\t        self.file2 = file2\n\t        self.blocks_file_source_0_0.open(self.file2, False)\n\t    def get_file3(self):\n\t        return self.file3\n\t    def set_file3(self, file3):\n\t        self.file3 = file3\n\t        self.blocks_file_source_0_0_0.open(self.file3, False)\n\t    def get_file4(self):\n\t        return self.file4\n", "    def set_file4(self, file4):\n\t        self.file4 = file4\n\t        self.blocks_file_source_0_0_0_0.open(self.file4, False)\n\t    def get_outfile(self):\n\t        return self.outfile\n\t    def set_outfile(self, outfile):\n\t        self.outfile = outfile\n\t        self.blocks_file_sink_0.open(self.outfile)\n\tdef argument_parser():\n\t    parser = OptionParser(usage=\"%prog: [options]\", option_class=eng_option)\n", "    parser.add_option(\n\t        \"\", \"--file1\", dest=\"file1\", type=\"string\", default='',\n\t        help=\"Set file1 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--file2\", dest=\"file2\", type=\"string\", default='',\n\t        help=\"Set file2 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--file3\", dest=\"file3\", type=\"string\", default='',\n\t        help=\"Set file3 [default=%default]\")\n\t    parser.add_option(\n", "        \"\", \"--file4\", dest=\"file4\", type=\"string\", default='',\n\t        help=\"Set file4 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--outfile\", dest=\"outfile\", type=\"string\", default='',\n\t        help=\"Set outfile [default=%default]\")\n\t    return parser\n\tdef main(top_block_cls=merge4recordings, options=None):\n\t    if options is None:\n\t        options, _ = argument_parser().parse_args()\n\t    tb = top_block_cls(file1=options.file1, file2=options.file2, file3=options.file3, file4=options.file4, outfile=options.outfile)\n", "    tb.start()\n\t    tb.wait()\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "core/flowgraphs/samples_to_dat.py", "chunked_list": ["#!/usr/bin/env python2\n\t# -*- coding: utf-8 -*-\n\t##################################################\n\t# GNU Radio Python Flow Graph\n\t# Title: Samples To Dat\n\t# GNU Radio version: 3.7.13.5\n\t##################################################\n\tfrom gnuradio import blocks\n\tfrom gnuradio import eng_notation\n\tfrom gnuradio import fft\n", "from gnuradio import gr\n\tfrom gnuradio.eng_option import eng_option\n\tfrom gnuradio.fft import window\n\tfrom gnuradio.filter import firdes\n\tfrom optparse import OptionParser\n\timport pmt\n\tclass samples_to_dat(gr.top_block):\n\t    def __init__(self, fft_size=512, filename=\"\", noise_pwr_db=-50, output=\"testing\"):\n\t        gr.top_block.__init__(self, \"Samples To Dat\")\n\t        ##################################################\n", "        # Parameters\n\t        ##################################################\n\t        self.fft_size = fft_size\n\t        self.filename = filename\n\t        self.noise_pwr_db = noise_pwr_db\n\t        self.output = output\n\t        ##################################################\n\t        # Blocks\n\t        ##################################################\n\t        self.fft_vxx_0 = fft.fft_vcc(fft_size, True, (window.blackmanharris(fft_size)), True, 1)\n", "        self.blocks_stream_to_vector_0 = blocks.stream_to_vector(gr.sizeof_gr_complex*1, fft_size)\n\t        self.blocks_nlog10_ff_0 = blocks.nlog10_ff(10, fft_size, -noise_pwr_db)\n\t        self.blocks_file_source_0 = blocks.file_source(gr.sizeof_gr_complex*1, filename, False)\n\t        try:\n\t            self.blocks_file_source_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_sink_0 = blocks.file_sink(gr.sizeof_float*fft_size, output+\".dat\", False)\n\t        self.blocks_file_sink_0.set_unbuffered(False)\n", "        self.blocks_complex_to_mag_squared_0 = blocks.complex_to_mag_squared(fft_size)\n\t        ##################################################\n\t        # Connections\n\t        ##################################################\n\t        self.connect((self.blocks_complex_to_mag_squared_0, 0), (self.blocks_nlog10_ff_0, 0))\n\t        self.connect((self.blocks_file_source_0, 0), (self.blocks_stream_to_vector_0, 0))\n\t        self.connect((self.blocks_nlog10_ff_0, 0), (self.blocks_file_sink_0, 0))\n\t        self.connect((self.blocks_stream_to_vector_0, 0), (self.fft_vxx_0, 0))\n\t        self.connect((self.fft_vxx_0, 0), (self.blocks_complex_to_mag_squared_0, 0))\n\t    def get_fft_size(self):\n", "        return self.fft_size\n\t    def set_fft_size(self, fft_size):\n\t        self.fft_size = fft_size\n\t    def get_filename(self):\n\t        return self.filename\n\t    def set_filename(self, filename):\n\t        self.filename = filename\n\t        self.blocks_file_source_0.open(self.filename, False)\n\t    def get_noise_pwr_db(self):\n\t        return self.noise_pwr_db\n", "    def set_noise_pwr_db(self, noise_pwr_db):\n\t        self.noise_pwr_db = noise_pwr_db\n\t    def get_output(self):\n\t        return self.output\n\t    def set_output(self, output):\n\t        self.output = output\n\t        self.blocks_file_sink_0.open(self.output+\".dat\")\n\tdef argument_parser():\n\t    parser = OptionParser(usage=\"%prog: [options]\", option_class=eng_option)\n\t    parser.add_option(\n", "        \"\", \"--fft-size\", dest=\"fft_size\", type=\"intx\", default=512,\n\t        help=\"Set fft-size [default=%default]\")\n\t    parser.add_option(\n\t        \"-f\", \"--filename\", dest=\"filename\", type=\"string\", default=\"\",\n\t        help=\"Set file [default=%default]\")\n\t    parser.add_option(\n\t        \"-n\", \"--noise-pwr-db\", dest=\"noise_pwr_db\", type=\"intx\", default=-50,\n\t        help=\"Set noise_pwr_db [default=%default]\")\n\t    parser.add_option(\n\t        \"-o\", \"--output\", dest=\"output\", type=\"string\", default=\"testing\",\n", "        help=\"Set outfile [default=%default]\")\n\t    return parser\n\tdef main(top_block_cls=samples_to_dat, options=None):\n\t    if options is None:\n\t        options, _ = argument_parser().parse_args()\n\t    tb = top_block_cls(fft_size=options.fft_size, filename=options.filename, noise_pwr_db=options.noise_pwr_db, output=options.output)\n\t    tb.start()\n\t    tb.wait()\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "core/flowgraphs/merge3recordings.py", "chunked_list": ["#!/usr/bin/env python2\n\t# -*- coding: utf-8 -*-\n\t##################################################\n\t# GNU Radio Python Flow Graph\n\t# Title: Merge3Recordings\n\t# GNU Radio version: 3.7.13.5\n\t##################################################\n\tfrom gnuradio import blocks\n\tfrom gnuradio import eng_notation\n\tfrom gnuradio import gr\n", "from gnuradio.eng_option import eng_option\n\tfrom gnuradio.filter import firdes\n\tfrom optparse import OptionParser\n\timport pmt\n\tclass merge3recordings(gr.top_block):\n\t    def __init__(self, file1='', file2='', file3='', outfile=''):\n\t        gr.top_block.__init__(self, \"Merge3Recordings\")\n\t        ##################################################\n\t        # Parameters\n\t        ##################################################\n", "        self.file1 = file1\n\t        self.file2 = file2\n\t        self.file3 = file3\n\t        self.outfile = outfile\n\t        ##################################################\n\t        # Blocks\n\t        ##################################################\n\t        self.blocks_file_source_0_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file3, False)\n\t        try:\n\t            self.blocks_file_source_0_0_0.set_begin_tag(pmt.PMT_NIL)\n", "        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_source_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file2, False)\n\t        try:\n\t            self.blocks_file_source_0_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_source_0 = blocks.file_source(gr.sizeof_gr_complex*1, file1, False)\n", "        try:\n\t            self.blocks_file_source_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_sink_0 = blocks.file_sink(gr.sizeof_gr_complex*1, outfile, False)\n\t        self.blocks_file_sink_0.set_unbuffered(False)\n\t        self.blocks_add_xx_0 = blocks.add_vcc(1)\n\t        ##################################################\n\t        # Connections\n", "        ##################################################\n\t        self.connect((self.blocks_add_xx_0, 0), (self.blocks_file_sink_0, 0))\n\t        self.connect((self.blocks_file_source_0, 0), (self.blocks_add_xx_0, 0))\n\t        self.connect((self.blocks_file_source_0_0, 0), (self.blocks_add_xx_0, 1))\n\t        self.connect((self.blocks_file_source_0_0_0, 0), (self.blocks_add_xx_0, 2))\n\t    def get_file1(self):\n\t        return self.file1\n\t    def set_file1(self, file1):\n\t        self.file1 = file1\n\t        self.blocks_file_source_0.open(self.file1, False)\n", "    def get_file2(self):\n\t        return self.file2\n\t    def set_file2(self, file2):\n\t        self.file2 = file2\n\t        self.blocks_file_source_0_0.open(self.file2, False)\n\t    def get_file3(self):\n\t        return self.file3\n\t    def set_file3(self, file3):\n\t        self.file3 = file3\n\t        self.blocks_file_source_0_0_0.open(self.file3, False)\n", "    def get_outfile(self):\n\t        return self.outfile\n\t    def set_outfile(self, outfile):\n\t        self.outfile = outfile\n\t        self.blocks_file_sink_0.open(self.outfile)\n\tdef argument_parser():\n\t    parser = OptionParser(usage=\"%prog: [options]\", option_class=eng_option)\n\t    parser.add_option(\n\t        \"\", \"--file1\", dest=\"file1\", type=\"string\", default='',\n\t        help=\"Set file1 [default=%default]\")\n", "    parser.add_option(\n\t        \"\", \"--file2\", dest=\"file2\", type=\"string\", default='',\n\t        help=\"Set file2 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--file3\", dest=\"file3\", type=\"string\", default='',\n\t        help=\"Set file3 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--outfile\", dest=\"outfile\", type=\"string\", default='',\n\t        help=\"Set outfile [default=%default]\")\n\t    return parser\n", "def main(top_block_cls=merge3recordings, options=None):\n\t    if options is None:\n\t        options, _ = argument_parser().parse_args()\n\t    tb = top_block_cls(file1=options.file1, file2=options.file2, file3=options.file3, outfile=options.outfile)\n\t    tb.start()\n\t    tb.wait()\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "core/flowgraphs/view_samples_from_file.py", "chunked_list": ["#!/usr/bin/env python2\n\t# -*- coding: utf-8 -*-\n\t##################################################\n\t# GNU Radio Python Flow Graph\n\t# Title: View Samples From File\n\t# GNU Radio version: 3.7.13.5\n\t##################################################\n\tif __name__ == '__main__':\n\t    import ctypes\n\t    import sys\n", "    if sys.platform.startswith('linux'):\n\t        try:\n\t            x11 = ctypes.cdll.LoadLibrary('libX11.so')\n\t            x11.XInitThreads()\n\t        except:\n\t            print \"Warning: failed to XInitThreads()\"\n\tfrom PyQt4 import Qt\n\tfrom gnuradio import blocks\n\tfrom gnuradio import eng_notation\n\tfrom gnuradio import gr\n", "from gnuradio import qtgui\n\tfrom gnuradio.eng_option import eng_option\n\tfrom gnuradio.filter import firdes\n\tfrom optparse import OptionParser\n\timport pmt\n\timport sip\n\timport sys\n\tfrom gnuradio import qtgui\n\tclass view_samples_from_file(gr.top_block, Qt.QWidget):\n\t    def __init__(self, fft_size=512, freq=2.44e9, refresh_rate=30, samp_rate=100e6, filename=''):\n", "        gr.top_block.__init__(self, \"View Samples From File\")\n\t        Qt.QWidget.__init__(self)\n\t        self.setWindowTitle(\"View Samples From File\")\n\t        qtgui.util.check_set_qss()\n\t        try:\n\t            self.setWindowIcon(Qt.QIcon.fromTheme('gnuradio-grc'))\n\t        except:\n\t            pass\n\t        self.top_scroll_layout = Qt.QVBoxLayout()\n\t        self.setLayout(self.top_scroll_layout)\n", "        self.top_scroll = Qt.QScrollArea()\n\t        self.top_scroll.setFrameStyle(Qt.QFrame.NoFrame)\n\t        self.top_scroll_layout.addWidget(self.top_scroll)\n\t        self.top_scroll.setWidgetResizable(True)\n\t        self.top_widget = Qt.QWidget()\n\t        self.top_scroll.setWidget(self.top_widget)\n\t        self.top_layout = Qt.QVBoxLayout(self.top_widget)\n\t        self.top_grid_layout = Qt.QGridLayout()\n\t        self.top_layout.addLayout(self.top_grid_layout)\n\t        self.settings = Qt.QSettings(\"GNU Radio\", \"view_samples_from_file\")\n", "        self.restoreGeometry(self.settings.value(\"geometry\").toByteArray())\n\t        ##################################################\n\t        # Parameters\n\t        ##################################################\n\t        self.fft_size = fft_size\n\t        self.freq = freq\n\t        self.refresh_rate = refresh_rate\n\t        self.samp_rate = samp_rate\n\t        self.filename = filename\n\t        ##################################################\n", "        # Blocks\n\t        ##################################################\n\t        self.qtgui_sink_x_0 = qtgui.sink_c(\n\t        \tfft_size, #fftsize\n\t        \tfirdes.WIN_BLACKMAN_hARRIS, #wintype\n\t        \tfreq, #fc\n\t        \tsamp_rate, #bw\n\t        \t\"\", #name\n\t        \tTrue, #plotfreq\n\t        \tTrue, #plotwaterfall\n", "        \tFalse, #plottime\n\t        \tFalse, #plotconst\n\t        )\n\t        self.qtgui_sink_x_0.set_update_time(1.0/refresh_rate)\n\t        self._qtgui_sink_x_0_win = sip.wrapinstance(self.qtgui_sink_x_0.pyqwidget(), Qt.QWidget)\n\t        self.top_grid_layout.addWidget(self._qtgui_sink_x_0_win)\n\t        self.qtgui_sink_x_0.enable_rf_freq(True)\n\t        self.blocks_throttle_0 = blocks.throttle(gr.sizeof_gr_complex*1, samp_rate,True)\n\t        self.blocks_file_source_0 = blocks.file_source(gr.sizeof_gr_complex*1, filename, True)\n\t        try:\n", "            self.blocks_file_source_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        ##################################################\n\t        # Connections\n\t        ##################################################\n\t        self.connect((self.blocks_file_source_0, 0), (self.blocks_throttle_0, 0))\n\t        self.connect((self.blocks_throttle_0, 0), (self.qtgui_sink_x_0, 0))\n\t    def closeEvent(self, event):\n", "        self.settings = Qt.QSettings(\"GNU Radio\", \"view_samples_from_file\")\n\t        self.settings.setValue(\"geometry\", self.saveGeometry())\n\t        event.accept()\n\t    def get_fft_size(self):\n\t        return self.fft_size\n\t    def set_fft_size(self, fft_size):\n\t        self.fft_size = fft_size\n\t    def get_freq(self):\n\t        return self.freq\n\t    def set_freq(self, freq):\n", "        self.freq = freq\n\t        self.qtgui_sink_x_0.set_frequency_range(self.freq, self.samp_rate)\n\t    def get_refresh_rate(self):\n\t        return self.refresh_rate\n\t    def set_refresh_rate(self, refresh_rate):\n\t        self.refresh_rate = refresh_rate\n\t    def get_samp_rate(self):\n\t        return self.samp_rate\n\t    def set_samp_rate(self, samp_rate):\n\t        self.samp_rate = samp_rate\n", "        self.qtgui_sink_x_0.set_frequency_range(self.freq, self.samp_rate)\n\t        self.blocks_throttle_0.set_sample_rate(self.samp_rate)\n\t    def get_filename(self):\n\t        return self.filename\n\t    def set_filename(self, filename):\n\t        self.filename = filename\n\t        self.blocks_file_source_0.open(self.filename, True)\n\tdef argument_parser():\n\t    parser = OptionParser(usage=\"%prog: [options]\", option_class=eng_option)\n\t    parser.add_option(\n", "        \"\", \"--fft-size\", dest=\"fft_size\", type=\"intx\", default=512,\n\t        help=\"Set fft_size [default=%default]\")\n\t    parser.add_option(\n\t        \"-f\", \"--freq\", dest=\"freq\", type=\"eng_float\", default=eng_notation.num_to_str(2.44e9),\n\t        help=\"Set freq [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--refresh-rate\", dest=\"refresh_rate\", type=\"eng_float\", default=eng_notation.num_to_str(30),\n\t        help=\"Set refresh_rate [default=%default]\")\n\t    parser.add_option(\n\t        \"-s\", \"--samp-rate\", dest=\"samp_rate\", type=\"eng_float\", default=eng_notation.num_to_str(100e6),\n", "        help=\"Set samp_rate [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--filename\", dest=\"filename\", type=\"string\", default='',\n\t        help=\"Set filename [default=%default]\")\n\t    return parser\n\tdef main(top_block_cls=view_samples_from_file, options=None):\n\t    if options is None:\n\t        options, _ = argument_parser().parse_args()\n\t    from distutils.version import StrictVersion\n\t    if StrictVersion(Qt.qVersion()) >= StrictVersion(\"4.5.0\"):\n", "        style = gr.prefs().get_string('qtgui', 'style', 'raster')\n\t        Qt.QApplication.setGraphicsSystem(style)\n\t    qapp = Qt.QApplication(sys.argv)\n\t    tb = top_block_cls(fft_size=options.fft_size, freq=options.freq, refresh_rate=options.refresh_rate, samp_rate=options.samp_rate, filename=options.filename)\n\t    tb.start()\n\t    tb.show()\n\t    def quitting():\n\t        tb.stop()\n\t        tb.wait()\n\t    qapp.connect(qapp, Qt.SIGNAL(\"aboutToQuit()\"), quitting)\n", "    qapp.exec_()\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "core/flowgraphs/merge2recordings.py", "chunked_list": ["#!/usr/bin/env python2\n\t# -*- coding: utf-8 -*-\n\t##################################################\n\t# GNU Radio Python Flow Graph\n\t# Title: Merge2Recordings\n\t# GNU Radio version: 3.7.13.5\n\t##################################################\n\tfrom gnuradio import blocks\n\tfrom gnuradio import eng_notation\n\tfrom gnuradio import gr\n", "from gnuradio.eng_option import eng_option\n\tfrom gnuradio.filter import firdes\n\tfrom optparse import OptionParser\n\timport pmt\n\tclass merge2recordings(gr.top_block):\n\t    def __init__(self, file1='', file2='', outfile=''):\n\t        gr.top_block.__init__(self, \"Merge2Recordings\")\n\t        ##################################################\n\t        # Parameters\n\t        ##################################################\n", "        self.file1 = file1\n\t        self.file2 = file2\n\t        self.outfile = outfile\n\t        ##################################################\n\t        # Blocks\n\t        ##################################################\n\t        self.blocks_file_source_0_0 = blocks.file_source(gr.sizeof_gr_complex*1, file2, False)\n\t        try:\n\t            self.blocks_file_source_0_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n", "            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_source_0 = blocks.file_source(gr.sizeof_gr_complex*1, file1, False)\n\t        try:\n\t            self.blocks_file_source_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_sink_0 = blocks.file_sink(gr.sizeof_gr_complex*1, outfile, False)\n\t        self.blocks_file_sink_0.set_unbuffered(False)\n", "        self.blocks_add_xx_0 = blocks.add_vcc(1)\n\t        ##################################################\n\t        # Connections\n\t        ##################################################\n\t        self.connect((self.blocks_add_xx_0, 0), (self.blocks_file_sink_0, 0))\n\t        self.connect((self.blocks_file_source_0, 0), (self.blocks_add_xx_0, 0))\n\t        self.connect((self.blocks_file_source_0_0, 0), (self.blocks_add_xx_0, 1))\n\t    def get_file1(self):\n\t        return self.file1\n\t    def set_file1(self, file1):\n", "        self.file1 = file1\n\t        self.blocks_file_source_0.open(self.file1, False)\n\t    def get_file2(self):\n\t        return self.file2\n\t    def set_file2(self, file2):\n\t        self.file2 = file2\n\t        self.blocks_file_source_0_0.open(self.file2, False)\n\t    def get_outfile(self):\n\t        return self.outfile\n\t    def set_outfile(self, outfile):\n", "        self.outfile = outfile\n\t        self.blocks_file_sink_0.open(self.outfile)\n\tdef argument_parser():\n\t    parser = OptionParser(usage=\"%prog: [options]\", option_class=eng_option)\n\t    parser.add_option(\n\t        \"\", \"--file1\", dest=\"file1\", type=\"string\", default='',\n\t        help=\"Set file1 [default=%default]\")\n\t    parser.add_option(\n\t        \"\", \"--file2\", dest=\"file2\", type=\"string\", default='',\n\t        help=\"Set file2 [default=%default]\")\n", "    parser.add_option(\n\t        \"\", \"--outfile\", dest=\"outfile\", type=\"string\", default='',\n\t        help=\"Set outfile [default=%default]\")\n\t    return parser\n\tdef main(top_block_cls=merge2recordings, options=None):\n\t    if options is None:\n\t        options, _ = argument_parser().parse_args()\n\t    tb = top_block_cls(file1=options.file1, file2=options.file2, outfile=options.outfile)\n\t    tb.start()\n\t    tb.wait()\n", "if __name__ == '__main__':\n\t    main()\n"]}
{"filename": "core/flowgraphs/samples_fft.py", "chunked_list": ["#!/usr/bin/env python2\n\t# -*- coding: utf-8 -*-\n\t##################################################\n\t# GNU Radio Python Flow Graph\n\t# Title: Samples Fft\n\t# GNU Radio version: 3.7.13.5\n\t##################################################\n\tfrom gnuradio import blocks\n\tfrom gnuradio import eng_notation\n\tfrom gnuradio import fft\n", "from gnuradio import gr\n\tfrom gnuradio.eng_option import eng_option\n\tfrom gnuradio.fft import window\n\tfrom gnuradio.filter import firdes\n\tfrom optparse import OptionParser\n\timport pmt\n\tclass samples_fft(gr.top_block):\n\t    def __init__(self, fft_size=512, filename=\"\", output=\"testing_fft\"):\n\t        gr.top_block.__init__(self, \"Samples Fft\")\n\t        ##################################################\n", "        # Parameters\n\t        ##################################################\n\t        self.fft_size = fft_size\n\t        self.filename = filename\n\t        self.output = output\n\t        ##################################################\n\t        # Blocks\n\t        ##################################################\n\t        self.fft_vxx_0 = fft.fft_vcc(fft_size, True, (window.blackmanharris(fft_size)), True, 1)\n\t        self.blocks_stream_to_vector_0 = blocks.stream_to_vector(gr.sizeof_gr_complex*1, fft_size)\n", "        self.blocks_file_source_0 = blocks.file_source(gr.sizeof_gr_complex*1, filename, False)\n\t        try:\n\t            self.blocks_file_source_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_sink_2 = blocks.file_sink(gr.sizeof_gr_complex*fft_size, output+\".32fc\", False)\n\t        self.blocks_file_sink_2.set_unbuffered(False)\n\t        ##################################################\n\t        # Connections\n", "        ##################################################\n\t        self.connect((self.blocks_file_source_0, 0), (self.blocks_stream_to_vector_0, 0))\n\t        self.connect((self.blocks_stream_to_vector_0, 0), (self.fft_vxx_0, 0))\n\t        self.connect((self.fft_vxx_0, 0), (self.blocks_file_sink_2, 0))\n\t    def get_fft_size(self):\n\t        return self.fft_size\n\t    def set_fft_size(self, fft_size):\n\t        self.fft_size = fft_size\n\t    def get_filename(self):\n\t        return self.filename\n", "    def set_filename(self, filename):\n\t        self.filename = filename\n\t        self.blocks_file_source_0.open(self.filename, False)\n\t    def get_output(self):\n\t        return self.output\n\t    def set_output(self, output):\n\t        self.output = output\n\t        self.blocks_file_sink_2.open(self.output+\".32fc\")\n\tdef argument_parser():\n\t    parser = OptionParser(usage=\"%prog: [options]\", option_class=eng_option)\n", "    parser.add_option(\n\t        \"\", \"--fft-size\", dest=\"fft_size\", type=\"intx\", default=512,\n\t        help=\"Set fft-size [default=%default]\")\n\t    parser.add_option(\n\t        \"-f\", \"--filename\", dest=\"filename\", type=\"string\", default=\"\",\n\t        help=\"Set file [default=%default]\")\n\t    parser.add_option(\n\t        \"-o\", \"--output\", dest=\"output\", type=\"string\", default=\"testing_fft\",\n\t        help=\"Set outfile [default=%default]\")\n\t    return parser\n", "def main(top_block_cls=samples_fft, options=None):\n\t    if options is None:\n\t        options, _ = argument_parser().parse_args()\n\t    tb = top_block_cls(fft_size=options.fft_size, filename=options.filename, output=options.output)\n\t    tb.start()\n\t    tb.wait()\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "core/flowgraphs/freq_samples_to_dat.py", "chunked_list": ["#!/usr/bin/env python2\n\t# -*- coding: utf-8 -*-\n\t##################################################\n\t# GNU Radio Python Flow Graph\n\t# Title: Freq Samples To Dat\n\t# GNU Radio version: 3.7.13.5\n\t##################################################\n\tfrom gnuradio import blocks\n\tfrom gnuradio import eng_notation\n\tfrom gnuradio import gr\n", "from gnuradio.eng_option import eng_option\n\tfrom gnuradio.filter import firdes\n\tfrom optparse import OptionParser\n\timport pmt\n\tclass freq_samples_to_dat(gr.top_block):\n\t    def __init__(self, fft_size=512, filename=\"\", noise_pwr_db=-50, output=\"testing\"):\n\t        gr.top_block.__init__(self, \"Freq Samples To Dat\")\n\t        ##################################################\n\t        # Parameters\n\t        ##################################################\n", "        self.fft_size = fft_size\n\t        self.filename = filename\n\t        self.noise_pwr_db = noise_pwr_db\n\t        self.output = output\n\t        ##################################################\n\t        # Blocks\n\t        ##################################################\n\t        self.blocks_stream_to_vector_0 = blocks.stream_to_vector(gr.sizeof_gr_complex*1, fft_size)\n\t        self.blocks_nlog10_ff_0 = blocks.nlog10_ff(10, fft_size, -noise_pwr_db)\n\t        self.blocks_file_source_0 = blocks.file_source(gr.sizeof_gr_complex*1, filename, False)\n", "        try:\n\t            self.blocks_file_source_0.set_begin_tag(pmt.PMT_NIL)\n\t        except AttributeError:\n\t            # This is a new feature in 3.7.12 that's not backward compatible to 3.7.10.2.\n\t            pass\n\t        self.blocks_file_sink_0 = blocks.file_sink(gr.sizeof_float*fft_size, output+\".dat\", False)\n\t        self.blocks_file_sink_0.set_unbuffered(False)\n\t        self.blocks_complex_to_mag_squared_0 = blocks.complex_to_mag_squared(fft_size)\n\t        ##################################################\n\t        # Connections\n", "        ##################################################\n\t        self.connect((self.blocks_complex_to_mag_squared_0, 0), (self.blocks_nlog10_ff_0, 0))\n\t        self.connect((self.blocks_file_source_0, 0), (self.blocks_stream_to_vector_0, 0))\n\t        self.connect((self.blocks_nlog10_ff_0, 0), (self.blocks_file_sink_0, 0))\n\t        self.connect((self.blocks_stream_to_vector_0, 0), (self.blocks_complex_to_mag_squared_0, 0))\n\t    def get_fft_size(self):\n\t        return self.fft_size\n\t    def set_fft_size(self, fft_size):\n\t        self.fft_size = fft_size\n\t    def get_filename(self):\n", "        return self.filename\n\t    def set_filename(self, filename):\n\t        self.filename = filename\n\t        self.blocks_file_source_0.open(self.filename, False)\n\t    def get_noise_pwr_db(self):\n\t        return self.noise_pwr_db\n\t    def set_noise_pwr_db(self, noise_pwr_db):\n\t        self.noise_pwr_db = noise_pwr_db\n\t    def get_output(self):\n\t        return self.output\n", "    def set_output(self, output):\n\t        self.output = output\n\t        self.blocks_file_sink_0.open(self.output+\".dat\")\n\tdef argument_parser():\n\t    parser = OptionParser(usage=\"%prog: [options]\", option_class=eng_option)\n\t    parser.add_option(\n\t        \"\", \"--fft-size\", dest=\"fft_size\", type=\"intx\", default=512,\n\t        help=\"Set fft-size [default=%default]\")\n\t    parser.add_option(\n\t        \"-f\", \"--filename\", dest=\"filename\", type=\"string\", default=\"\",\n", "        help=\"Set file [default=%default]\")\n\t    parser.add_option(\n\t        \"-n\", \"--noise-pwr-db\", dest=\"noise_pwr_db\", type=\"intx\", default=-50,\n\t        help=\"Set noise_pwr_db [default=%default]\")\n\t    parser.add_option(\n\t        \"-o\", \"--output\", dest=\"output\", type=\"string\", default=\"testing\",\n\t        help=\"Set outfile [default=%default]\")\n\t    return parser\n\tdef main(top_block_cls=freq_samples_to_dat, options=None):\n\t    if options is None:\n", "        options, _ = argument_parser().parse_args()\n\t    tb = top_block_cls(fft_size=options.fft_size, filename=options.filename, noise_pwr_db=options.noise_pwr_db, output=options.output)\n\t    tb.start()\n\t    tb.wait()\n\tif __name__ == '__main__':\n\t    main()\n"]}
