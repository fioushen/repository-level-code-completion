{"filename": "main.py", "chunked_list": ["import hydra\n\tfrom transformers import (\n\t    AutoModelForSeq2SeqLM,\n\t    AutoTokenizer,\n\t    MBartForConditionalGeneration,\n\t)\n\tfrom src.bench import MTBenchmarker\n\tfrom src.dataset.flores_dataset import Flores\n\tfrom src.dataset.ittb_dataset import Ittb\n\tfrom src.dataset.iwslt_dataset import Iwslt\n", "from src.dataset.wmt_dataset import Wmt\n\tfrom src.ipi.decoders.autoregressive import AutoregressiveDecoder\n\tfrom src.ipi.decoders.beam_search import BeamSearchDecoder\n\tfrom src.ipi.decoders.gs_jacobi import GSJacobiDecoder\n\tfrom src.ipi.decoders.hybrid_jacobi import HybridJacobiDecoder\n\tfrom src.ipi.decoders.jacobi import JacobiDecoder\n\tfrom src.ipi.initializer import Initializer\n\tfrom src.ipi.decoders.mt_decoding import MTDecoder\n\tfrom src.utils.beam_search import BeamSearcher\n\tfrom src.utils.utils import retrieve_samples\n", "def load_tokenizer(cfg):\n\t    # MBart\n\t    mapping_dict = {\n\t        \"ar\": \"ar_AR\",\n\t        \"cs\": \"cs_CZ\",\n\t        \"de\": \"de_DE\",\n\t        \"en\": \"en_XX\",\n\t        \"es\": \"es_XX\",\n\t        \"et\": \"et_EE\",\n\t        \"fi\": \"fi_FI\",\n", "        \"fr\": \"fr_XX\",\n\t        \"gu\": \"gu_IN\",\n\t        \"hi\": \"hi_IN\",\n\t        \"it\": \"it_IT\",\n\t        \"ja\": \"ja_XX\",\n\t        \"kk\": \"kk_KZ\",\n\t        \"ko\": \"ko_KR\",\n\t        \"lt\": \"lt_LT\",\n\t        \"lv\": \"lv_LV\",\n\t        \"my\": \"my_MM\",\n", "        \"ne\": \"ne_NP\",\n\t        \"nl\": \"nl_XX\",\n\t        \"ro\": \"ro_RO\",\n\t        \"ru\": \"ru_RU\",\n\t        \"si\": \"si_LK\",\n\t        \"tr\": \"tr_TR\",\n\t        \"vi\": \"vi_VN\",\n\t        \"zh\": \"zh_CN\",\n\t        \"af\": \"af_ZA\",\n\t        \"az\": \"az_AZ\",\n", "        \"bn\": \"bn_IN\",\n\t        \"fa\": \"fa_IR\",\n\t        \"he\": \"he_IL\",\n\t        \"hr\": \"hr_HR\",\n\t        \"id\": \"id_ID\",\n\t        \"ka\": \"ka_GE\",\n\t        \"km\": \"km_KH\",\n\t        \"mk\": \"mk_MK\",\n\t        \"ml\": \"ml_IN\",\n\t        \"mn\": \"mn_MN\",\n", "        \"mr\": \"mr_IN\",\n\t        \"pl\": \"pl_PL\",\n\t        \"ps\": \"ps_AF\",\n\t        \"pt\": \"pt_XX\",\n\t        \"sv\": \"sv_SE\",\n\t        \"sw\": \"sw_KE\",\n\t        \"ta\": \"ta_IN\",\n\t        \"te\": \"te_IN\",\n\t        \"th\": \"th_TH\",\n\t        \"tl\": \"tl_XX\",\n", "        \"uk\": \"uk_UA\",\n\t        \"ur\": \"ur_PK\",\n\t        \"xh\": \"xh_ZA\",\n\t        \"gl\": \"gl_ES\",\n\t        \"sl\": \"sl_SI\",\n\t    }\n\t    if \"mbart\" in cfg.model_name:\n\t        tokenizer = AutoTokenizer.from_pretrained(\n\t            cfg.model_name,\n\t            src_lang=mapping_dict[cfg.src_lang],\n", "            tgt_lang=mapping_dict[cfg.tgt_lang],\n\t            use_fast=False,\n\t        )\n\t    else:\n\t        print(cfg.model_name)\n\t        tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n\t    return tokenizer\n\tdef load_model(cfg):\n\t    if \"mbart\" in cfg.model_name:\n\t        model = MBartForConditionalGeneration.from_pretrained(cfg.model_name).to(\n", "            cfg.device\n\t        )\n\t    else:\n\t        model = AutoModelForSeq2SeqLM.from_pretrained(cfg.model_name).to(cfg.device)\n\t    return model\n\tdef load_dataset(tokenizer, cfg):\n\t    # Wmt-xx-xx-xx\n\t    if cfg.name == \"wmt\":\n\t        split = cfg.split\n\t        if cfg.subset.use_subset:\n", "            split = f\"{cfg.split}[{cfg.subset.start}:{cfg.subset.end + 1}]\"\n\t        dataset = Wmt(\n\t            version=cfg.version,\n\t            src_lan=cfg.src_lang,\n\t            tgt_lan=cfg.tgt_lang,\n\t            hugginface_tokenizer=tokenizer,\n\t            split=split,\n\t        )\n\t    # Iwsltxx-xx-xx\n\t    elif cfg.name == \"iwslt\":\n", "        dataset = Iwslt(\n\t            data_dir=cfg.data_dir,\n\t            version=str(cfg.version),\n\t            src_lan=cfg.src_lang,\n\t            tgt_lan=cfg.tgt_lang,\n\t            hugginface_tokenizer=tokenizer,\n\t            split=cfg.split,\n\t        )\n\t    elif cfg.name == \"ittb\":\n\t        dataset = Ittb(\n", "            src_lan=cfg.src_lang,\n\t            tgt_lan=cfg.tgt_lang,\n\t            hugginface_tokenizer=tokenizer,\n\t            split=cfg.split,\n\t        )\n\t    elif cfg.name == \"flores\":\n\t        dataset = Flores(\n\t            src_lan=cfg.src_lang,\n\t            tgt_lan=cfg.tgt_lang,\n\t            hugginface_tokenizer=tokenizer,\n", "            split=cfg.split,\n\t        )\n\t    else:\n\t        raise ValueError(f\"{cfg.dataset.name} is not yet implemented\")\n\t    return dataset\n\tdef load_initializer(tokenizer, cfg):\n\t    if cfg.use_initializer:\n\t        initializer = Initializer(\n\t            src_len=cfg.src_lang,\n\t            tgt_len=cfg.tgt_lang,\n", "            hugginface_tokenizer=tokenizer,\n\t            use_init=cfg.use_init,\n\t            device=cfg.device,\n\t        )\n\t    else:\n\t        initializer = None\n\t    return initializer\n\tdef compute_beam_search(cfg, model, dataset):\n\t    initializer = load_initializer(dataset.tokenizer, cfg.initializer)\n\t    decoder = MTDecoder(\n", "        tokenizer=dataset.tokenizer,\n\t        model=model,\n\t        use_cache=cfg.decoder.use_cache,\n\t        gs_jaco_blocks=cfg.decoder.gs_jaco_blocks,\n\t        device=cfg.device,\n\t        initializer=initializer\n\t    )\n\t    beam_searcher = BeamSearcher(\n\t        model=model,\n\t        dataset=dataset,\n", "        initializer=initializer,\n\t        decoder=decoder,\n\t        batch_size=cfg.beam_search.batch_size,\n\t        num_beams=cfg.beam_search.num_beams,\n\t        device=cfg.beam_search.device,\n\t        no_repeat_ngram_size=2,\n\t        early_stopping=True,\n\t        result_dir=cfg.beam_search.result_dir,\n\t    )\n\t    beam_searcher.compute_beam_search(cfg)\n", "def load_decoders(cfg, tokenizer, model, initializer):\n\t    decoders = []\n\t    for decoder in cfg.decoder.decoders:\n\t        if decoder == \"autoregressive\":\n\t            dec = AutoregressiveDecoder(\n\t                tokenizer=tokenizer,\n\t                model=model,\n\t                initializer=initializer,\n\t                use_cache=cfg.decoder.use_cache,\n\t                device=cfg.decoder.device\n", "            )\n\t        elif decoder == \"jacobi\":\n\t            dec = JacobiDecoder(\n\t                tokenizer=tokenizer,\n\t                model=model,\n\t                initializer=initializer,\n\t                use_cache=cfg.decoder.use_cache,\n\t                device=cfg.decoder.device\n\t            )\n\t        elif decoder == \"gs_jacobi\":\n", "            dec = GSJacobiDecoder(\n\t                tokenizer=tokenizer,\n\t                model=model,\n\t                initializer=initializer,\n\t                gs_jaco_blocks=cfg.decoder.gs_jaco_blocks,\n\t                init_mode=\"\",\n\t                use_cache=cfg.decoder.use_cache,\n\t                device=cfg.decoder.device\n\t            )\n\t        elif decoder == \"h_jacobi\":\n", "            dec = HybridJacobiDecoder(\n\t                tokenizer=tokenizer,\n\t                model=model,\n\t                initializer=initializer,\n\t                init_mode=\"fixed\",\n\t                gs_jaco_blocks=cfg.decoder.gs_jaco_blocks,\n\t                use_cache=cfg.decoder.use_cache,\n\t                device=cfg.decoder.device\n\t            )\n\t        elif decoder == \"beam_search\":\n", "            dec = BeamSearchDecoder(\n\t                tokenizer=tokenizer,\n\t                model=model,\n\t                initializer=initializer,\n\t                num_beams=cfg.beam_search.num_beams,\n\t                early_stopping=True,\n\t            )\n\t        else:\n\t            raise ValueError(f\"{decoder} decoder have not been implemented yet.\")\n\t        decoders.append(dec)\n", "    return decoders\n\tdef compute_benchmark(cfg, tokenizer, dataset, model):\n\t    initializer = load_initializer(tokenizer, cfg.initializer)\n\t    decoders = load_decoders(cfg, tokenizer, model, initializer)\n\t    benchmarker = MTBenchmarker(\n\t        dataset=dataset,\n\t        decoders=decoders,\n\t        src_lang=cfg.model.src_lang,\n\t        tgt_lang=cfg.model.tgt_lang,\n\t        result_dir=cfg.bench.result_dir,\n", "        device=cfg.bench.device,\n\t        debug=True,\n\t    )\n\t    benchmarker.compute_total_time()\n\t@hydra.main(config_path=\"conf\", config_name=\"config\", version_base=\"1.1\")\n\tdef main(cfg):\n\t    tokenizer = load_tokenizer(cfg.model)\n\t    model = load_model(cfg.model)\n\t    dataset = load_dataset(tokenizer, cfg.dataset)\n\t    if \"benchmark\" in cfg.task:\n", "        compute_benchmark(cfg, tokenizer, dataset, model)\n\t    elif \"beam_search\" in cfg.task:\n\t        compute_beam_search(cfg, model, dataset)\n\t    elif \"sample\" in cfg.task:\n\t        retrieve_samples(cfg.sample.path, dataset)\n\t    else:\n\t        raise ValueError(f\"{cfg.task} is not yet implemented\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "exp/flops_calculator.py", "chunked_list": ["\"\"\"Computes the flops needed for training/running transformer networks. Adapted from https://github.com/google-research/electra/blob/master/flops_computation.py \"\"\"\n\timport collections\n\t# We checked this code with TensorFlow\"s FLOPs counting, although we had to\n\t# correct for this issue: https://github.com/tensorflow/tensorflow/issues/22071\n\t# Assumptions going into the FLOPs counting\n\t#   - An \"operation\" is a mathematical operation, not a machine instruction. So\n\t#     an \"exp\" takes one opp like and add, even though in practice an exp\n\t#     might be slower. This is not too bad an assumption because\n\t#     matrix-multiplies dominate the compute for most models, so minor details\n\t#     about activation functions don\"t matter too much. Similarly, we count\n", "#     matrix-multiplies as 2*m*n flops instead of m*n, as one might if\n\t#     if considering fused multiply-add ops.\n\t#   - Backward pass takes the same number of FLOPs as forward pass. No exactly\n\t#     right (e.g., for softmax cross entropy loss the backward pass is faster).\n\t#     Importantly, it really is the same for matrix-multiplies, which is most of\n\t#     the compute anyway.\n\t#   - We assume \"dense\" embedding lookups (i.e., multiplication by a one-hot\n\t#     vector). On some hardware accelerators, these dense operations are\n\t#     actually faster than sparse lookups.\n\t# Please open a github issue if you spot a problem with this code!\n", "# I am not sure if the below constants are 100% right, but they are only applied\n\t# to O(hidden_size) activations, which is generally a lot less compute than the\n\t# matrix-multiplies, which are O(hidden_size^2), so they don't affect the total\n\t# number of FLOPs much.\n\t# random number, >=, multiply activations by dropout mask, multiply activations\n\t# by correction (1 / (1 - dropout_rate))\n\tDROPOUT_FLOPS = 4\n\t# compute mean activation (sum), computate variance of activation\n\t# (square and sum), bias (add), scale (multiply)\n\tLAYER_NORM_FLOPS = 5\n", "# GELU: 0.5 * x * (1 + tanh(sqrt(2 / np.pi) * (x + 0.044715 * pow(x, 3))))\n\tACTIVATION_FLOPS = 8\n\t# max/substract (for stability), exp, sum, divide\n\tSOFTMAX_FLOPS = 5\n\tclass TransformerHparams(object):\n\t  \"\"\"Computes the train/inference FLOPs for transformers.\"\"\"\n\t  def __init__(self, h, l, s=512, v=30522, e=None, i=None, heads=None,\n\t      head_size=None, output_frac=0.15625, sparse_embed_lookup=False,\n\t      decoder=False):\n\t    self.h = h  # hidden size\n", "    self.l = l  # number of layers\n\t    self.s = s  # sequence length\n\t    self.v = v  # vocab size\n\t    self.e = h if e is None else e  # embedding size\n\t    self.i = h * 4 if i is None else i  # intermediate size\n\t    self.kqv = h if head_size is None else head_size * heads  # attn proj sizes\n\t    self.heads = max(h // 64, 1) if heads is None else heads  # attention heads\n\t    self.output_frac = output_frac  # percent of tokens using an output softmax\n\t    self.sparse_embed_lookup = sparse_embed_lookup  # sparse embedding lookups\n\t    self.decoder = decoder  # decoder has extra attn to encoder states\n", "  def get_block_flops(self):\n\t    \"\"\"Get the forward-pass FLOPs for a single transformer block.\"\"\"\n\t    attn_mul = 2 if self.decoder else 1\n\t    block_flops = dict(\n\t        kqv=3 * 2 * self.h * self.kqv * attn_mul,\n\t        kqv_bias=3 * self.kqv * attn_mul,\n\t        attention_scores=2 * self.kqv * self.s * attn_mul,\n\t        attn_softmax=SOFTMAX_FLOPS * self.s * self.heads * attn_mul,\n\t        attention_dropout=DROPOUT_FLOPS * self.s * self.heads * attn_mul,\n\t        attention_scale=self.s * self.heads * attn_mul,\n", "        attention_weighted_avg_values=2 * self.h * self.s * attn_mul,\n\t        attn_output=2 * self.h * self.h * attn_mul,\n\t        attn_output_bias=self.h * attn_mul,\n\t        attn_output_dropout=DROPOUT_FLOPS * self.h * attn_mul,\n\t        attn_output_residual=self.h * attn_mul,\n\t        attn_output_layer_norm=LAYER_NORM_FLOPS * attn_mul,\n\t        intermediate=2 * self.h * self.i,\n\t        intermediate_act=ACTIVATION_FLOPS * self.i,\n\t        intermediate_bias=self.i,\n\t        output=2 * self.h * self.i,\n", "        output_bias=self.h,\n\t        output_dropout=DROPOUT_FLOPS * self.h,\n\t        output_residual=self.h,\n\t        output_layer_norm=LAYER_NORM_FLOPS * self.h,\n\t    )\n\t    return sum(block_flops.values()) * self.s\n\t  def get_embedding_flops(self, output=False):\n\t    \"\"\"Get the forward-pass FLOPs the transformer inputs or output softmax.\"\"\"\n\t    embedding_flops = {}\n\t    if output or (not self.sparse_embed_lookup):\n", "      embedding_flops[\"main_multiply\"] = 2 * self.e * self.v\n\t    # input embedding post-processing\n\t    if not output:\n\t      embedding_flops.update(dict(\n\t          tok_type_and_position=2 * self.e * (self.s + 2),\n\t          add_tok_type_and_position=2 * self.e,\n\t          emb_layer_norm=LAYER_NORM_FLOPS * self.e,\n\t          emb_dropout=DROPOUT_FLOPS * self.e\n\t      ))\n\t    # projection layer if e != h\n", "    if self.e != self.h or output:\n\t      embedding_flops.update(dict(\n\t          hidden_kernel=2 * self.h * self.e,\n\t          hidden_bias=self.e if output else self.h\n\t      ))\n\t      # extra hidden layer and output softmax\n\t      if output:\n\t        embedding_flops.update(dict(\n\t            hidden_activation=ACTIVATION_FLOPS * self.e,\n\t            hidden_layernorm=LAYER_NORM_FLOPS * self.e,\n", "            output_softmax=SOFTMAX_FLOPS * self.v,\n\t            output_target_word=2 * self.v\n\t        ))\n\t        return self.output_frac * sum(embedding_flops.values()) * self.s\n\t    return sum(embedding_flops.values()) * self.s\n\t  def get_binary_classification_flops(self):\n\t    classification_flops = dict(\n\t        hidden=2 * self.h * self.h,\n\t        hidden_bias=self.h,\n\t        hidden_act=ACTIVATION_FLOPS * self.h,\n", "        logits=2 * self.h\n\t    )\n\t    return sum(classification_flops.values()) * self.s\n\t  def get_train_flops(self, batch_size, train_steps, discriminator=False, use_backprop=True):\n\t    \"\"\"Get the FLOPs for pre-training the transformer.\"\"\"\n\t    # 2* for forward/backward pass\n\t    if use_backprop:\n\t        mult = 2\n\t    else:\n\t        mult = 1\n", "    return mult * batch_size * train_steps * (\n\t        (self.l * self.get_block_flops()) +\n\t        self.get_embedding_flops(output=False) +\n\t        (self.get_binary_classification_flops() if discriminator else\n\t         self.get_embedding_flops(output=True))\n\t    )\n\t  def get_infer_flops(self):\n\t    \"\"\"Get the FLOPs for running inference with the transformer on a\n\t    classification task.\"\"\"\n\t    return ((self.l * self.get_block_flops()) +\n", "            self.get_embedding_flops(output=False) +\n\t            self.get_binary_classification_flops())\n\tdef get_electra_train_flops(\n\t    h_d, l_d, h_g, l_g, batch_size, train_steps, tied_embeddings,\n\t    e=None, s=512, output_frac=0.15625):\n\t  \"\"\"Get the FLOPs needed for  pre-training ELECTRA.\"\"\"\n\t  if e is None:\n\t    e = h_d\n\t  disc = TransformerHparams(\n\t      h_d, l_d, s=s, e=e,\n", "      output_frac=output_frac).get_train_flops(batch_size, train_steps, True)\n\t  gen = TransformerHparams(\n\t      h_g, l_g, s=s, e=e if tied_embeddings else None,\n\t      output_frac=output_frac).get_train_flops(batch_size, train_steps)\n\t  return disc + gen\n\tdef calculate_LASSFlops(prior_flop):\n\t    vq_vaeflops = 0\n\tMODEL_FLOPS = collections.OrderedDict([\n\t    # These runtimes were computed with tensorflow FLOPs counting instead of the\n\t    # script, as the neural architectures are quite different.\n", "    # 768648884 words in LM1b benchmark, 10 epochs with batch size 20,\n\t    # seq length 128, 568093262680 FLOPs per example.\n\t    (\"elmo\", 2 * 10 * 768648884 * 568093262680 / (20.0 * 128)),\n\t    # 15064773691518 is FLOPs for forward pass on 32 examples.\n\t    # Therefore 2 * steps * batch_size * 15064773691518 / 32 is XLNet compute\n\t    (\"xlnet\", 2 * 500000 * 8192 * 15064773691518 / 32.0),\n\t    # Runtimes computed with the script\n\t    (\"gpt\", TransformerHparams(768, 12, v=40000, output_frac=1.0).get_train_flops(\n\t        128, 960800)),\n\t    (\"jukebox\", TransformerHparams(1024, 48, s=8192, v=2048, output_frac=1.0, heads=1).get_infer_flops()),\n", "    (\"bert_small\", TransformerHparams(256, 12, e=128, s=128).get_train_flops(128, 1.45e6)),\n\t    (\"bert_base\", TransformerHparams(768, 12).get_train_flops(256, 1e6)),\n\t    (\"bert_large\", TransformerHparams(1024, 24).get_train_flops(256, 1e6)),\n\t    (\"electra_small\", get_electra_train_flops(256, 12, 64, 12, 128, 1e6, True, s=128, e=128)),\n\t    (\"electra_base\", get_electra_train_flops(768, 12, 256, 12, 256, 766000, True)),\n\t    (\"electra_400k\", get_electra_train_flops(1024, 24, 256, 24, 2048, 400000, True)),\n\t    (\"electra_1.75M\", get_electra_train_flops(1024, 24, 256, 24, 2048, 1750000, True)),\n\t    # RoBERTa, ALBERT, and T5 have  minor architectural differences from\n\t    # BERT/ELECTRA, but I believe they don't significantly effect the runtime,\n\t    # so we use this script for those models as well.\n", "    (\"roberta\", TransformerHparams(1024, 24, v=50265).get_train_flops(8000, 500000)),\n\t    (\"albert\", TransformerHparams(4096, 12, v=30000, e=128).get_train_flops(\n\t        4096, 1.5e6)),\n\t    (\"t5_11b\", TransformerHparams(\n\t        1024,  # hidden size\n\t        24,  # layers\n\t        v=32000,  # vocab size\n\t        i=65536,  # ff intermediate hidden size\n\t        heads=128, head_size=128,  # heads/head size\n\t        output_frac=0.0  # encoder has no output softmax\n", "    ).get_train_flops(2048, 1e6) +  # 1M steps with batch size 2048\n\t     TransformerHparams(\n\t         1024,\n\t         24,\n\t         v=32000,\n\t         i=65536,\n\t         heads=128, head_size=128,\n\t         output_frac=1.0,  # decoder has output softmax for all positions\n\t         decoder=True\n\t     ).get_train_flops(2048, 1e6)),\n", "    (\"Shallow\", TransformerHparams(512, 13, i=2048).get_train_flops(250, 300000)),\n\t    (\"Shallow + KD\", TransformerHparams(512, 13, i=2048).get_train_flops(250, 300000) + TransformerHparams(512, 13, i=2048,\n\t                                                                                                            heads=16).get_train_flops(\n\t                250, 300000, use_backprop=False)),\n\t    (\"DSLP\", TransformerHparams(512, 12, i=2048).get_train_flops(250, 300000) + TransformerHparams(512, 12, i=2048,\n\t                                                                                                        heads=16).get_train_flops(\n\t            250, 300000, use_backprop=False)),\n\t    (\"F-VAE\", TransformerHparams(512, 12, i=2048).get_train_flops(250, 300000) + TransformerHparams(512, 12, i=2048,\n\t                                                                                                    heads=16).get_train_flops(\n\t        250, 300000, use_backprop=False)),\n", "    (\"DisCo\", TransformerHparams(512, 12, i=2048).get_train_flops(250, 300000) + TransformerHparams(1024, 24, i=4096, heads=16).get_train_flops(250, 300000, use_backprop=False)),\n\t    (\"SUNDAE\", TransformerHparams(512, 12, i=2048).get_train_flops(4096, 10e6)),\n\t])\n\tdef main():\n\t  for k, v in MODEL_FLOPS.items():\n\t    print(k, v)\n\tif __name__ == \"__main__\":\n\t  main()\n"]}
{"filename": "src/bench.py", "chunked_list": ["import csv\n\timport logging\n\timport os\n\timport sys\n\timport time\n\timport torch\n\tfrom tabulate import tabulate\n\tfrom torch.utils.data import DataLoader, Dataset\n\tfrom tqdm import tqdm\n\tfrom src.utils import utils\n", "from src.utils.bench_scorer import Scorer\n\tfrom src.utils.utils import retrieve_model_name, check_zero_division\n\tlogging.basicConfig(\n\t    format=\"%(asctime)s | %(levelname)s | %(name)s |  [%(filename)s:%(lineno)d] %(message)s\",\n\t    datefmt=\"%Y-%m-%d %H:%M:%S\",\n\t    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n\t    stream=sys.stdout,\n\t)\n\tlogger = logging.getLogger(\"bench\")\n\tclass MTBenchmarker(object):\n", "    def __init__(\n\t            self,\n\t            dataset: Dataset,\n\t            decoders,\n\t            src_lang,\n\t            tgt_lang,\n\t            compare_to: str = \"autoregressive\",\n\t            result_dir: str = None,\n\t            device: str = \"cuda\",\n\t            debug: bool = False,\n", "    ):\n\t        self.dataset = dataset\n\t        self.decoders = decoders\n\t        self.device = device\n\t        self.debug = debug\n\t        self.compare_to = compare_to\n\t        self.src_lang = src_lang\n\t        self.tgt_lang = tgt_lang\n\t        self.model_name = self.decoders[0].model_name.split(\"/\")[1]\n\t        self.dataloader = DataLoader(\n", "            dataset, collate_fn=dataset.collate_fn, batch_size=1, shuffle=False\n\t        )\n\t        self.result_dir = result_dir\n\t        self.exp_dir = self._retrieve_exp_dir()\n\t    def _synchronize(self):\n\t        if self.device == \"cuda\":\n\t            torch.cuda.synchronize()\n\t    def _retrieve_exp_dir(self):\n\t        file_name = self._retrieve_file_name()\n\t        exp_dir = os.path.join(self.result_dir, \"benchmark\", file_name)\n", "        utils.makedirs(exp_dir)\n\t        return exp_dir\n\t    def _retrieve_file_name(self):\n\t        model_name = retrieve_model_name(self.model_name)\n\t        lang = f\"{self.src_lang}_{self.tgt_lang}\"\n\t        return f\"{model_name}/{self.device}/{self.dataset.name}/{lang}\"\n\t    @staticmethod\n\t    def _write_on_file(path, item, i):\n\t        utils.makedirs(path)\n\t        with open(path, \"a\") as file:\n", "            writer = csv.writer(file, delimiter=\"\\t\")\n\t            # write header\n\t            if os.stat(path).st_size == 0:\n\t                writer.writerow([\"i\", \"item\"])\n\t            writer.writerow([i, item])\n\t    def _compute_info_table(self, best_alg, i, grid):\n\t        # Table for the test info\n\t        info_table = tabulate([\n\t                [\n\t                    self.decoders[0].model_name,\n", "                    self.dataset.name,\n\t                    self.device,\n\t                    best_alg,\n\t                    i,\n\t                    f\"{self.src_lang}-{self.tgt_lang}\",\n\t                ]\n\t            ],\n\t            headers=[\n\t                \"Model\",\n\t                \"Dataset\",\n", "                \"Device\",\n\t                \"Best Algorithm\",\n\t                \"Sentences\",\n\t                \"Languages\",\n\t            ],\n\t            tablefmt=grid,\n\t        )\n\t        return info_table\n\t    def _compute_time_table(self, scorers, grid):\n\t        if len(scorers) == 1:\n", "            name, scorer = list(scorers.items())[0]\n\t            times_table = tabulate([\n\t                [\"Time\", scorer.tot_mean_time],\n\t                [\"Iteration\", scorer.tot_mean_iter],\n\t            ],\n\t                headers=[\"Metrics\", name], tablefmt=grid,\n\t            )\n\t        else:\n\t            tests_header = [\"Metrics\"] + [name for name in scorers]\n\t            comp_scorer = scorers.get(self.compare_to)\n", "            time_speedup = [check_zero_division(comp_scorer.tot_mean_time, scorer.tot_mean_time) for scorer in scorers.values()]\n\t            iter_speedup = [check_zero_division(comp_scorer.tot_mean_iter, scorer.tot_mean_iter) for scorer in scorers.values()]\n\t            times_table = tabulate(\n\t                [\n\t                    [\"Speedup T\"] + time_speedup,\n\t                    [\"Speedup I\"] + iter_speedup,\n\t                    [\"Time\"] + [scorer.tot_mean_time for scorer in scorers.values()],\n\t                    [\"Iter\"] + [scorer.tot_mean_iter for scorer in scorers.values()],\n\t                ],\n\t                headers=tests_header, tablefmt=grid,\n", "            )\n\t        return times_table\n\t    def _compute_bleu_table(self, scorers, grid):\n\t        bleu_scores = [scorer.compute_bleu_score() for scorer in scorers.values()]\n\t        # Table for the Bleu score\n\t        bleu_table = tabulate([\n\t            ['Score'] + [score.score for score in bleu_scores],\n\t            ['Counts'] + [score.counts for score in bleu_scores],\n\t            ['Totals'] + [score.totals for score in bleu_scores],\n\t            ['Precisions'] + [score.precisions for score in bleu_scores],\n", "            ['Bp'] + [score.bp for score in bleu_scores],\n\t            ['Sys_len'] + [score.sys_len for score in bleu_scores],\n\t            ['ref_len'] + [score.ref_len for score in bleu_scores],\n\t        ], headers=[\"Metrics\"] + [name for name in scorers], tablefmt=grid)\n\t        return bleu_table\n\t    def write_report(self, i, scorers, best_algorithm):\n\t        print(\"Writing report...\")\n\t        # Compute best algorithm\n\t        best_alg = max(best_algorithm, key=best_algorithm.get)\n\t        info_table_txt = self._compute_info_table(best_alg, i, grid=\"grid\")\n", "        info_table_tex = self._compute_info_table(best_alg, i, grid=\"latex\")\n\t        # Table for the benchmark times\n\t        times_table_txt = self._compute_time_table(scorers, grid=\"rst\")\n\t        times_table_tex = self._compute_time_table(scorers, grid=\"latex\")\n\t        # Table for the bleu score\n\t        bleu_table_txt = self._compute_bleu_table(scorers, grid=\"rst\")\n\t        print(self.exp_dir)\n\t        with open(os.path.join(self.exp_dir, \"report.txt\"), mode=\"w\") as report:\n\t            report.write(f\"Test Info\\n{info_table_txt}\\n\\n\")\n\t            report.write(f\"Benchmark\\n{times_table_txt}\\n\\n\")\n", "            report.write(f\"Bleu\\n{bleu_table_txt}\\n\\n\")\n\t        with open(os.path.join(self.exp_dir, \"latex.txt\"), mode=\"w\") as report:\n\t            report.write(f\"Test Info\\n{info_table_tex}\\n\\n\")\n\t            report.write(f\"Benchmark\\n{times_table_tex}\\n\\n\")\n\t    def write_inline(self, i: int, scorers, best_alg):\n\t        for name, scorer in scorers.items():\n\t            # Write times\n\t            path = os.path.join(self.exp_dir, name, f\"{name}.tsv\")\n\t            self._write_on_file(path, scorer.current_time, i)\n\t            # Write iterations\n", "            path = os.path.join(self.exp_dir, name, f\"iter_{name}.tsv\")\n\t            self._write_on_file(path, scorer.current_iter, i)\n\t            # Write translations\n\t            path = os.path.join(self.exp_dir, name, f\"trans_{name}.tsv\")\n\t            self._write_on_file(path, scorer.current_transl, i)\n\t            # Write initializations\n\t            path = os.path.join(self.exp_dir, name, f\"init_{name}.tsv\")\n\t            self._write_on_file(path, scorer.current_init, i)\n\t        # Write mean\n\t        path = os.path.join(self.exp_dir, \"meanvar.tsv\")\n", "        utils.makedirs(path)\n\t        with open(path, \"a\") as file:\n\t            writer = csv.writer(file, delimiter=\"\\t\")\n\t            # Write header\n\t            if os.stat(path).st_size == 0:\n\t                header = [\"#sentence\"] + [f\"mean_{name}\" for name in scorers] + [\"best_alg\"]\n\t                writer.writerow(header)\n\t            row = [i] + [scorer.current_time for scorer in scorers.values()] + [best_alg]\n\t            writer.writerow(row)\n\t    @staticmethod\n", "    def _compute_best_algorithm(scorers):\n\t        best = scorers.get(min(scorers, key=lambda x: scorers[x].current_time)).acronym\n\t        return best\n\t    def _compute_postfix(self, scorers, best_algorithms, curr_alg):\n\t        best_alg = max(best_algorithms, key=best_algorithms.get)\n\t        if len(scorers) == 1:\n\t            _, scorer = list(scorers.items())[0]\n\t            postfix = {\n\t                scorer.acronym: (\n\t                    round(scorer.tot_mean_time, 3),\n", "                    round(scorer.tot_mean_iter, 3)\n\t                ),\n\t                \"ca\": curr_alg,\n\t                \"ba\": best_alg,\n\t            }\n\t        else:\n\t            comp_scorer = scorers.get(self.compare_to)\n\t            postfix = {\n\t                scorer.acronym: (\n\t                    check_zero_division(comp_scorer.tot_mean_time, scorer.tot_mean_time),\n", "                    check_zero_division(comp_scorer.tot_mean_iter, scorer.tot_mean_iter)\n\t                ) for name, scorer in scorers.items() if name != self.compare_to\n\t            }\n\t            postfix.update({\"ca\": curr_alg, \"ba\": best_alg})\n\t        return postfix\n\t    def compute_total_time(self):\n\t        i = 0\n\t        scorers = {decoder.name: Scorer(decoder.name, decoder.acronym) for decoder in self.decoders}\n\t        best_algorithms = {decoder.acronym: 0 for decoder in self.decoders}\n\t        pbar = tqdm(self.dataloader, desc=\"Computing Benchmark...\")\n", "        for x in pbar:\n\t            try:\n\t                input_ids = x[\"source\"][\"input_ids\"].to(self.device)\n\t                attention_mask = x[\"source\"][\"attention_mask\"].to(self.device)\n\t                tgt_text = x['target']['sentences']\n\t                for decoder, name in zip(self.decoders, scorers):\n\t                    kwargs = decoder.compute_decode_kwargs(input_ids, attention_mask)\n\t                    start = time.perf_counter()\n\t                    self._synchronize()\n\t                    trans, iter = decoder.decode(input_ids, attention_mask, **kwargs)\n", "                    self._synchronize()\n\t                    end = time.perf_counter()\n\t                    sample_time = end - start\n\t                    init_tensor = kwargs[\"init_tensor\"] if \"init_tensor\" in kwargs else \"\"\n\t                    trans = self.dataset.tokenizer.batch_decode(\n\t                        trans, skip_special_tokens=True\n\t                    )[0]\n\t                    scorers[name].update_metrics(sample_time, iter, trans, tgt_text[0], init_tensor)\n\t                best_alg = self._compute_best_algorithm(scorers)\n\t                best_algorithms[best_alg] += 1\n", "                # Update tqdm bar\n\t                postfix_pbar = self._compute_postfix(scorers, best_algorithms, best_alg)\n\t                pbar.set_postfix(postfix_pbar)\n\t                self.write_inline(i, scorers, best_alg)\n\t            except Exception as e:\n\t                if self.debug:\n\t                    raise e\n\t                else:\n\t                    logger.error(e)\n\t                return\n", "            i += 1\n\t        self.write_report(i, scorers, best_algorithms)\n"]}
{"filename": "src/__init__.py", "chunked_list": ["import logging\n\timport os\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\timport dotenv\n\timport git\n\tpylogger = logging.getLogger(__name__)\n\tdef get_env(env_name: str, default: Optional[str] = None) -> str:\n\t    \"\"\"Safely read an environment variable.\n\t    Raises errors if it is not defined or it is empty.\n", "    :param env_name: the name of the environment variable\n\t    :param default: the default (optional) value for the environment variable\n\t    :return: the value of the environment variable\n\t    \"\"\"\n\t    if env_name not in os.environ:\n\t        if default is None:\n\t            message = f\"{env_name} not defined and no default value is present!\"\n\t            pylogger.error(message)\n\t            raise KeyError(message)\n\t        return default\n", "    env_value: str = os.environ[env_name]\n\t    if not env_value:\n\t        if default is None:\n\t            message = (\n\t                f\"{env_name} has yet to be configured and no default value is present!\"\n\t            )\n\t            pylogger.error(message)\n\t            raise ValueError(message)\n\t        return default\n\t    return env_value\n", "def load_envs(env_file: Optional[str] = None) -> None:\n\t    \"\"\"Load all the environment variables defined in the `env_file`.\n\t    This is equivalent to `. env_file` in bash.\n\t    It is possible to define all the system specific variables in the `env_file`.\n\t    :param env_file: the file that defines the environment variables to use. If None\n\t                     it searches for a `.env` file in the project.\n\t    \"\"\"\n\t    dotenv.load_dotenv(dotenv_path=env_file, override=True)\n\t# Load environment variables\n\tload_envs()\n", "if \"PROJECT_ROOT\" not in os.environ:\n\t    try:\n\t        PROJECT_ROOT = Path(\n\t            git.Repo(Path.cwd(), search_parent_directories=True).working_dir\n\t        )\n\t    except git.exc.InvalidGitRepositoryError:\n\t        PROJECT_ROOT = Path.cwd()\n\t    pylogger.debug(f\"Inferred project root: {PROJECT_ROOT}\")\n\t    os.environ[\"PROJECT_ROOT\"] = str(PROJECT_ROOT)\n\telse:\n", "    PROJECT_ROOT: Path = Path(os.environ[\"PROJECT_ROOT\"])\n\t__all__ = [\"__version__\", \"PROJECT_ROOT\"]\n"]}
{"filename": "src/viz/dependecy_graph.py", "chunked_list": ["from typing import Tuple\n\timport numpy as np\n\timport plotly.express as px\n\timport torch\n\tfrom transformers import MBart50Tokenizer\n\tclass DecodingDependencyGraph(object):\n\t    def __init__(self, model, tokenizer, gold_target: torch.Tensor):\n\t        self.model = model\n\t        self.tokenizer = tokenizer\n\t        self.init_matrix = []\n", "        self.output_probs = []\n\t        self.gold_target = gold_target\n\t        if isinstance(tokenizer, MBart50Tokenizer):\n\t            self.is_mbart = True\n\t            self.starting_index = 2\n\t        else:\n\t            self.is_mbart = False\n\t            self.starting_index = 1\n\t    def insert_one_element(\n\t        self, current_tensor, gold_tensor, index=None, output_probs=None\n", "    ):\n\t        self.init_matrix.append(\n\t            (\n\t                current_tensor[:, self.starting_index :]\n\t                == gold_tensor[:, self.starting_index :]\n\t            )\n\t        )\n\t        if output_probs is not None:\n\t            self.output_probs.append(output_probs)\n\t    def finalize_matrices(self) -> Tuple[torch.Tensor, torch.Tensor]:\n", "        return (\n\t            torch.stack(self.init_matrix).permute(1, 0, 2).cpu(),\n\t            torch.stack(self.output_probs).permute(1, 0, 2).cpu(),\n\t        )\n\t    def _create_labels(self, sample_index: int, method):\n\t        sample_target = self.gold_target[sample_index, :].squeeze(0)\n\t        if method == \"decoded_ids\":\n\t            labels = self.tokenizer.convert_ids_to_tokens(sample_target)\n\t            labels = [\n\t                f\"{i}:{id}\"\n", "                for i, id in zip(\n\t                    labels[self.starting_index - 1 :],\n\t                    sample_target[self.starting_index - 1 :],\n\t                )\n\t            ]\n\t        elif method == \"basic\":\n\t            labels = [f\"{i}\" for i in sample_target[self.starting_index - 1 :].tolist()]\n\t        return labels\n\t    def pretty_print(\n\t        self, sample_index: int, sentence_id: str, method=\"basic\", x_remap=\"text\"\n", "    ):\n\t        labels = self._create_labels(sample_index=sample_index, method=method)\n\t        iteration_matrix, probability_matrix = self.finalize_matrices()\n\t        iteration_matrix = iteration_matrix[sample_index, :, :].int().numpy()\n\t        probability_matrix = probability_matrix[sample_index, :, 1:].numpy()\n\t        mask: np.ndarray = np.zeros_like(iteration_matrix)\n\t        i, j = 0, 0\n\t        while i < iteration_matrix.shape[0] and j < iteration_matrix.shape[1]:\n\t            if iteration_matrix[i, j]:\n\t                mask[i, j] += 1\n", "                j += 1\n\t            else:\n\t                mask[i, j:] = iteration_matrix[i, j:]\n\t                i += 1\n\t        probability_matrix = mask * probability_matrix\n\t        fig = px.imshow(\n\t            iteration_matrix + mask,\n\t            binary_compression_level=0,\n\t            title=f\"Decoding Dependency Graph for sentence {sentence_id}\",\n\t            color_continuous_scale=\"Viridis\",\n", "        )\n\t        fig.update_xaxes(\n\t            tickmode=\"array\",\n\t            tickvals=list(range(len(labels[1:]))),\n\t            ticktext=[\n\t                self.tokenizer.convert_ids_to_tokens([x])[0] if x_remap else str(x)\n\t                for x in labels[1:]\n\t            ],\n\t            tickangle=45,\n\t        )\n", "        fig.update_traces(\n\t            text=[\n\t                [f\"{xy:.2f}\" if xy > 0 else \"\" for xy in x] for x in probability_matrix\n\t            ],\n\t            texttemplate=\"%{text}\",\n\t        )\n\t        fig.update_layout(\n\t            font=dict(family=\"Courier New, monospace\", size=22, color=\"Black\"),\n\t            showlegend=False,\n\t            coloraxis_showscale=False,\n", "        )\n\t        fig.show()\n\t    def plot_confusion_matrix(\n\t        self, cm, target_names, title=\"Confusion matrix\", cmap=None, normalize=True\n\t    ):\n\t        \"\"\"\n\t        given a sklearn confusion matrix (cm), make a nice plot\n\t        Arguments\n\t        ---------\n\t        cm:           confusion matrix from sklearn.metrics.confusion_matrix\n", "        target_names: given classification classes such as [0, 1, 2]\n\t                                  the class names, for example: ['high', 'medium', 'low']\n\t        title:        the text to display at the top of the matrix\n\t        cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n\t                                  see http://matplotlib.org/examples/color/colormaps_reference.html\n\t                                  plt.get_cmap('jet') or plt.cm.Blues\n\t        normalize:    If False, plot the raw numbers\n\t                                  If True, plot the proportions\n\t        Usage\n\t        -----\n", "        plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n\t                                                                                                                          # sklearn.metrics.confusion_matrix\n\t                                                  normalize    = True,                # show proportions\n\t                                                  target_names = y_labels_vals,       # list of names of the classes\n\t                                                  title        = best_estimator_name) # title of graph\n\t        Citiation\n\t        ---------\n\t        http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\t        \"\"\"\n\t        import itertools\n", "        import matplotlib.pyplot as plt\n\t        import numpy as np\n\t        accuracy = np.trace(cm) / np.sum(cm).astype(\"float\")\n\t        misclass = 1 - accuracy\n\t        if cmap is None:\n\t            cmap = plt.get_cmap(\"Blues\")\n\t        plt.figure(figsize=(8, 6))\n\t        plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n\t        plt.title(title)\n\t        plt.colorbar()\n", "        if target_names is not None:\n\t            tick_marks = np.arange(len(target_names))\n\t            plt.xticks(tick_marks, target_names, rotation=45)\n\t            plt.yticks(tick_marks, target_names)\n\t        if normalize:\n\t            cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n\t        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n\t        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n\t            if normalize:\n\t                plt.text(\n", "                    j,\n\t                    i,\n\t                    \"{:0.4f}\".format(cm[i, j]),\n\t                    horizontalalignment=\"center\",\n\t                    color=\"white\" if cm[i, j] > thresh else \"black\",\n\t                )\n\t            else:\n\t                plt.text(\n\t                    j,\n\t                    i,\n", "                    \"{:,}\".format(cm[i, j]),\n\t                    horizontalalignment=\"center\",\n\t                    color=\"white\" if cm[i, j] > thresh else \"black\",\n\t                )\n\t        plt.tight_layout()\n\t        plt.ylabel(\"True label\")\n\t        plt.xlabel(\n\t            \"Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}\".format(\n\t                accuracy, misclass\n\t            )\n", "        )\n\t        plt.show()\n"]}
{"filename": "src/viz/__init__.py", "chunked_list": []}
{"filename": "src/viz/visualize.py", "chunked_list": ["import functools\n\timport hashlib\n\timport json\n\tfrom pathlib import Path\n\timport argparse\n\timport datasets\n\timport numpy as np\n\timport plotly.express as px\n\timport torch.utils.data\n\tfrom datasets import load_from_disk, Dataset\n", "from transformers import (\n\t    AutoModelForSeq2SeqLM,\n\t    AutoTokenizer,\n\t    PreTrainedTokenizer,\n\t    PreTrainedModel,\n\t)\n\tfrom src import PROJECT_ROOT\n\tfrom src.ipi.decoders.jacobi import JacobiDecoder\n\tfrom src.ipi.initializer import Initializer\n\tfrom src.ipi.decoders.mt_decoding import MTDecoder, generate_target\n", "def add_iteration_matrix(\n\t    sample: dict, src_lang: str, tgt_lang: str, model, tokenizer, initializer, decoder\n\t):\n\t    source = sample[\"translation\"][src_lang]\n\t    target = sample[\"translation\"][tgt_lang]\n\t    encoded_source = tokenizer(\n\t        source,\n\t        return_tensors=\"pt\",\n\t    )\n\t    encoded_target = tokenizer(\n", "        target,\n\t        return_tensors=\"pt\",\n\t    )\n\t    x = {\n\t        \"source\": {\n\t            \"input_ids\": encoded_source[\"input_ids\"],\n\t            \"attention_mask\": encoded_source[\"attention_mask\"],\n\t            \"sentences\": source,\n\t        },\n\t        \"target\": {\n", "            \"input_ids\": encoded_target[\"input_ids\"],\n\t            \"attention_mask\": encoded_target[\"attention_mask\"],\n\t            \"sentences\": target,\n\t        },\n\t    }\n\t    input_ids = encoded_source[\"input_ids\"].to(device)\n\t    attention_mask = encoded_source[\"attention_mask\"].to(device)\n\t    gold_output = generate_target(\n\t        model=model,\n\t        tokenizer=tokenizer,\n", "        input_ids=input_ids.to(device),\n\t        attention_mask=attention_mask.to(device),\n\t        is_mbart=False,\n\t    )\n\t    init_tensor, _ = initializer.init_translation(gold_output.shape[-1])\n\t    return_tensor, index, ddg = decoder.decode(\n\t        input_ids=input_ids,\n\t        attention_mask=attention_mask,\n\t        init_tensor=init_tensor,\n\t        compute_ddg=True,\n", "    )\n\t    iteration_matrix, probability_matrix = ddg.finalize_matrices()\n\t    parallel_steps = iteration_matrix.shape[1]\n\t    gold_steps = gold_output.shape[1]\n\t    return dict(\n\t        gold_output=gold_output[0],\n\t        iteration_matrix=iteration_matrix[0],\n\t        probability_matrix=probability_matrix[0],\n\t        parallel_steps=parallel_steps,\n\t        gold_steps=gold_steps,\n", "        score=gold_steps - parallel_steps,\n\t    )\n\tdef enrich_dataset(\n\t    run_info: dict,\n\t    device: str,\n\t    src_lang: str,\n\t    tgt_lang: str,\n\t    dataset_name: str,\n\t    dataset_key: str,\n\t    tokenizer: PreTrainedTokenizer,\n", "    model: PreTrainedModel,\n\t    force_recompute: bool = False,\n\t) -> Dataset:\n\t    # MarianMT\n\t    run_dir: Path = run_info[\"run_dir\"]\n\t    dataset_path: Path = run_dir / \"dataset\"\n\t    if run_dir.exists() and not force_recompute:\n\t        dataset = load_from_disk(str(run_dir / \"dataset\"))\n\t    else:\n\t        initializer = Initializer(src_lang, tgt_lang, tokenizer, use_init=False)\n", "        model.eval()\n\t        # decoder = MTDecoder(\n\t        #     tokenizer=tokenizer,\n\t        #     model=model,\n\t        #     use_cache=False,\n\t        #     gs_jaco_blocks=5,\n\t        #     device=device,\n\t        #     initializer=initializer\n\t        # )\n\t        decoder = JacobiDecoder(\n", "            tokenizer=tokenizer,\n\t            model=model,\n\t            initializer=initializer,\n\t            use_cache=False,\n\t            device=device\n\t        )\n\t        dataset = datasets.load_dataset(\n\t            dataset_name,\n\t            dataset_key,\n\t            split=\"test[0:10]\",\n", "            data_dir=str(PROJECT_ROOT / \"hf_data\"),\n\t            cache_dir=str(PROJECT_ROOT / \"hf_cache\"),\n\t        ).map(\n\t            function=functools.partial(\n\t                add_iteration_matrix,\n\t                src_lang=src_lang,\n\t                tgt_lang=tgt_lang,\n\t                model=model,\n\t                initializer=initializer,\n\t                decoder=decoder,\n", "                tokenizer=tokenizer,\n\t            )\n\t        )\n\t        dataset.save_to_disk(str(dataset_path))\n\t        json.dump(\n\t            run_info,\n\t            fp=(run_dir / \"run_info.json\").open(\"w\", encoding=\"utf-8\"),\n\t            indent=4,\n\t            default=lambda x: str(x)\n\t        )\n", "    return dataset\n\tdef draw(sample: dict, tokenizer: PreTrainedTokenizer, starting_index: int):\n\t    labels = [f\"{i}\" for i in sample[\"gold_output\"][starting_index - 1 :]]\n\t    iteration_matrix = torch.as_tensor(sample[\"iteration_matrix\"])\n\t    probability_matrix = torch.as_tensor(sample[\"probability_matrix\"])\n\t    iteration_matrix = iteration_matrix[:, :].int().numpy()\n\t    probability_matrix = probability_matrix[:, 1:].numpy()\n\t    mask: np.ndarray = np.zeros_like(iteration_matrix)\n\t    i, j = 0, 0\n\t    while i < iteration_matrix.shape[0] and j < iteration_matrix.shape[1]:\n", "        if iteration_matrix[i, j]:\n\t            mask[i, j] += 1\n\t            j += 1\n\t        else:\n\t            mask[i, j:] = iteration_matrix[i, j:]\n\t            i += 1\n\t    probability_matrix = mask * probability_matrix\n\t    fig = px.imshow(\n\t        iteration_matrix + mask,\n\t        binary_compression_level=0,\n", "        # title=f\"Decoding Dependency Graph for sentence {sentence_id}\",\n\t        color_continuous_scale=\"Viridis\",\n\t    )\n\t    fig.update_xaxes(\n\t        tickmode=\"array\",\n\t        tickvals=list(range(len(labels[1:]))),\n\t        ticktext=[tokenizer.convert_ids_to_tokens([x])[0] for x in labels[1:]],\n\t        tickangle=45,\n\t    )\n\t    fig.update_traces(\n", "        text=[[f\"{xy:.2f}\" if xy > 0 else \"\" for xy in x] for x in probability_matrix],\n\t        texttemplate=\"%{text}\",\n\t    )\n\t    fig.update_layout(\n\t        font=dict(family=\"Courier New, monospace\", size=22, color=\"Black\"),\n\t        showlegend=False,\n\t        coloraxis_showscale=False,\n\t        margin=dict(l=0, r=0, b=0, t=0),\n\t        paper_bgcolor=\"rgba(0,0,0,0)\",\n\t        plot_bgcolor=\"rgba(0,0,0,0)\",\n", "    )\n\t    return fig\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description='Dependency Graph Visualizer (DDGviz)')\n\t    parser.add_argument('--src', default=\"ro\", help='src language')\n\t    parser.add_argument('--tgt', default=\"en\", help='target language')\n\t    parser.add_argument('--dataset', default=\"wmt16\", help='Dataset name')\n\t    # parser.add_argument('--examples', default=[1566, 960], help='Examples to print with DDGviz')\n\t    parser.add_argument('--examples', default=[1, 4], help='Examples to print with DDGviz')\n\t    args = parser.parse_args()\n", "    device = \"cpu\"\n\t    src_lang = args.src\n\t    tgt_lang = args.tgt\n\t    dataset_name: str = args.dataset\n\t    dataset_key: str = f\"{src_lang}-{tgt_lang}\"\n\t    langs = {src_lang, tgt_lang}\n\t    examples_to_print = args.examples\n\t    if \"en\" in langs:\n\t        dataset_key = (\n\t            f\"{src_lang}-{tgt_lang}\" if \"en\" == tgt_lang else f\"{tgt_lang}-{src_lang}\"\n", "        )\n\t    model_src_lang = src_lang\n\t    if model_src_lang == \"ro\":\n\t        model_src_lang: str = \"roa\"\n\t    dataset_split: str = \"test\"\n\t    model_name: str = f\"Helsinki-NLP/opus-mt-{model_src_lang}-{tgt_lang}\"\n\t    # model_name: str = \"zannabethl/opus-mt-en-ro-finetuned-en-to-ro\"\n\t    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\t    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n\t    run_info = dict(\n", "        model_name=model_name,\n\t        source_lang=src_lang,\n\t        target_lang=tgt_lang,\n\t        dataset_name=dataset_name,\n\t        dataset_key=dataset_key,\n\t        split=dataset_split,\n\t    )\n\t    run_info_hash: str = hashlib.md5(\n\t        json.dumps(run_info).encode(encoding=\"utf-8\")\n\t    ).hexdigest()\n", "    run_dir: Path = PROJECT_ROOT / \"iteration_matrix\" / run_info_hash\n\t    run_info[\"run_dir\"] = run_dir\n\t    dataset = (\n\t        enrich_dataset(\n\t            run_info=run_info,\n\t            device=device,\n\t            src_lang=src_lang,\n\t            tgt_lang=tgt_lang,\n\t            dataset_name=dataset_name,\n\t            dataset_key=dataset_key,\n", "            tokenizer=tokenizer,\n\t            model=model,\n\t        )\n\t        .map(function=lambda x, i: {\"index\": i}, with_indices=True)\n\t        .select(examples_to_print)\n\t    )\n\t    starting_index: int = 1\n\t    images_dir: Path = run_dir / \"images\"\n\t    images_dir.mkdir(exist_ok=True)\n\t    for sample in dataset:\n", "        fig = draw(sample=sample, tokenizer=tokenizer, starting_index=1)\n\t        # fig.show()\n\t        print(f\"Index: {sample['index']}\")\n\t        print(f\"Translations: {sample['translation']}\")\n\t        print(\n\t            f\"Gold output: {tokenizer.decode(sample['gold_output'], skip_special_tokens=True)}\"\n\t        )\n\t        print()\n\t        # input()\n\t        # continue\n", "        fig.write_image(images_dir / f\"{sample['index']}.png\", width=1800, height=1500)\n\t        x = {x: sample[x] for x in (\"translation\", \"gold_output\", \"index\", \"score\")}\n\t        x[\"gold_output\"] = tokenizer.decode(sample[\"gold_output\"])\n\t        (images_dir / f\"{sample['index']}.json\").write_text(\n\t            json.dumps(x, indent=4, sort_keys=True), encoding=\"utf-8\"\n\t        )\n"]}
{"filename": "src/ipi/stopping_condition.py", "chunked_list": ["import torch\n\tdef limit_past_key_values(past_key_values, limit):\n\t    new_list = []\n\t    for elem in past_key_values:\n\t        new_elem = list(elem)\n\t        new_elem[0] = elem[0][:, :, :limit, :]\n\t        new_elem[1] = elem[1][:, :, :limit, :]\n\t        new_list.append(tuple(new_elem))\n\t    return tuple(new_list)\n\tdef stopping_criterion(past_tensor, current_tensor, eos=None):\n", "    assert past_tensor.shape == current_tensor.shape\n\t    if torch.equal(past_tensor, current_tensor):\n\t        tensor = current_tensor\n\t        if eos is not None:\n\t            if eos in current_tensor[0]:\n\t                pos = (current_tensor[0] == eos).nonzero(as_tuple=True)[0]\n\t                if pos.shape[0] > 1:\n\t                    pos = pos[0].item()\n\t                else:\n\t                    pos = pos.item()\n", "                return True, tensor, pos\n\t            else:\n\t                return True, tensor, -1\n\t        return True, tensor\n\t    else:\n\t        if eos is not None:\n\t            return False, current_tensor, False\n\t        else:\n\t            return False, current_tensor\n\tdef check_stop_cond(tensor, eos):\n", "    if eos in tensor[0]:\n\t        pos = (tensor[0] == eos).nonzero(as_tuple=True)[0]\n\t        if pos.shape[0] > 1:\n\t            pos = pos[0].item()\n\t        else:\n\t            pos = pos.item()\n\t        return pos\n\t    else:\n\t        return -1\n"]}
{"filename": "src/ipi/initializer.py", "chunked_list": ["import torch\n\tfrom nltk.tokenize.toktok import ToktokTokenizer\n\tfrom nltk.tokenize.treebank import TreebankWordDetokenizer\n\tfrom transformers import (\n\t    M2M100Tokenizer,\n\t    MarianTokenizer,\n\t    MBart50Tokenizer,\n\t    MBartTokenizer,\n\t)\n\tclass Initializer(object):\n", "    def __init__(\n\t        self,\n\t        src_len,\n\t        tgt_len,\n\t        hugginface_tokenizer,\n\t        use_init=True,\n\t        device=\"cpu\",\n\t    ):\n\t        self.src_len = src_len\n\t        self.tgt_len = tgt_len\n", "        self.tokenizer = hugginface_tokenizer\n\t        self.pad_token_id = self.tokenizer.pad_token_id\n\t        self.tokenizer_nltk = ToktokTokenizer()\n\t        self.detokenizer_nltk = TreebankWordDetokenizer()\n\t        self.use_init = use_init\n\t        self.device = device\n\t    def init_translation(self, tgt_len=None):\n\t        final_translation = \"\"\n\t        with self.tokenizer.as_target_tokenizer():\n\t            if isinstance(self.tokenizer, MBartTokenizer):\n", "                tgt_tensor = self.tokenizer(\n\t                    final_translation, return_tensors=\"pt\", padding=True\n\t                ).data[\"input_ids\"]\n\t                if tgt_tensor.shape[-1] == 2:\n\t                    tgt_tensor = tgt_tensor[:, :1]\n\t            elif isinstance(self.tokenizer, MarianTokenizer):\n\t                bos = torch.tensor([self.pad_token_id]).unsqueeze(0)\n\t                tgt_tensor = bos\n\t            elif isinstance(self.tokenizer, MBart50Tokenizer) or isinstance(\n\t                self.tokenizer, M2M100Tokenizer\n", "            ):\n\t                bos = torch.tensor([self.tokenizer.eos_token_id]).unsqueeze(0)\n\t                tgt_tensor = self.tokenizer(\n\t                    final_translation, return_tensors=\"pt\", padding=True\n\t                ).data[\"input_ids\"]\n\t                tgt_tensor = torch.cat([bos, tgt_tensor], dim=-1)\n\t            else:\n\t                bos = torch.tensor([self.tokenizer.bos_token_id]).unsqueeze(0)\n\t                tgt_tensor = self.tokenizer(\n\t                    final_translation, return_tensors=\"pt\", padding=True\n", "                ).data[\"input_ids\"]\n\t                tgt_tensor = torch.cat([bos, tgt_tensor], dim=-1)\n\t        if tgt_len is not None:\n\t            tgt_tensor = self.trim_length(tgt_tensor, tgt_len)\n\t        return tgt_tensor.to(self.device), final_translation\n\t    def trim_length(self, tgt_tensor, tgt_len):\n\t        last_elem = tgt_tensor[:, -1].unsqueeze(0)\n\t        if tgt_tensor.shape[-1] > tgt_len:\n\t            return torch.cat([tgt_tensor[..., : tgt_len - 1], last_elem], dim=-1)\n\t        elif tgt_tensor.shape[-1] < tgt_len:\n", "            delta = tgt_len - tgt_tensor.shape[-1] - 1\n\t            init_tensor = torch.tensor(\n\t                [self.pad_token_id] * delta, dtype=tgt_tensor.dtype\n\t            ).unsqueeze(0)\n\t            return_tensor = torch.cat([tgt_tensor, init_tensor, last_elem], dim=-1)\n\t            return return_tensor\n\t        else:\n\t            return tgt_tensor\n"]}
{"filename": "src/ipi/__init__.py", "chunked_list": []}
{"filename": "src/ipi/decoders/autoregressive.py", "chunked_list": ["import torch\n\tfrom src.ipi.decoders.mt_decoding import MTDecoder\n\tclass AutoregressiveDecoder(MTDecoder):\n\t    def __init__(self, tokenizer, model, initializer, **kwargs):\n\t        super().__init__(tokenizer, model, initializer, **kwargs)\n\t        self.name = \"autoregressive\"\n\t        self.acronym = \"a\"\n\t    @torch.no_grad()\n\t    def decode(self, input_ids, attention_mask, init_tensor=None, logits_preprocessor=None, *args, **kwargs):\n\t        index = 0\n", "        if init_tensor is None:\n\t            init_tensor = torch.tensor(\n\t                [self.pad_token_id], device=self.device\n\t            ).unsqueeze(0)\n\t        elif self.is_mbart:\n\t            output = self.model(\n\t                input_ids,\n\t                attention_mask,\n\t                decoder_input_ids=init_tensor[:, 0].unsqueeze(0),\n\t                use_cache=True,\n", "            )\n\t            encoder_last_hidden_state = output.encoder_last_hidden_state\n\t            past_key_values = output.past_key_values\n\t            index += 1\n\t            total_res = torch.tensor(\n\t                [[init_tensor[:, 0], init_tensor[:, 1]]], device=self.device\n\t            )\n\t            init_tensor = init_tensor[:, 1].unsqueeze(0)\n\t        else:\n\t            init_tensor = init_tensor[:, 0].unsqueeze(0)\n", "        total_res = init_tensor.clone()\n\t        while True:\n\t            if self.use_cache and index > 0:\n\t                if index == 1024:\n\t                    print(total_res)\n\t                output = self.model(\n\t                    None,\n\t                    attention_mask,\n\t                    decoder_input_ids=init_tensor,\n\t                    encoder_outputs=(encoder_last_hidden_state, None, None),\n", "                    use_cache=True,\n\t                    past_key_values=past_key_values,\n\t                )\n\t            else:\n\t                output = self.model(\n\t                    input_ids,\n\t                    attention_mask,\n\t                    decoder_input_ids=init_tensor,\n\t                    use_cache=True,\n\t                )\n", "            encoder_last_hidden_state = output.encoder_last_hidden_state\n\t            past_key_values = output.past_key_values\n\t            logits = output.logits\n\t            if logits_preprocessor is not None:\n\t                logits = logits_preprocessor(total_res, logits[:,-1,:])\n\t            else:\n\t                logits = logits[:,-1,:]\n\t            max_value = torch.argmax(logits, dim=-1)\n\t            last = max_value\n\t            init_tensor = last.unsqueeze(0)\n", "            total_res = torch.cat((total_res, init_tensor), dim=1)\n\t            index += 1\n\t            if last[0].item() == self.eos_token_id or index == self.model.config.max_length - 1:\n\t                break\n\t        return total_res, index\n\t    def initialize(self):\n\t        if self.initializer is not None:\n\t            init_tensor, _ = self.initializer.init_translation()\n\t        else:\n\t            init_tensor = None\n", "        return init_tensor\n\t    def compute_decode_kwargs(self, input_ids, *args, **kwargs):\n\t        init_tensor = self.initialize()\n\t        logits_preprocessor = self.generate_logits_preprocessor(input_ids)\n\t        return {\n\t            \"init_tensor\": init_tensor.clone(),\n\t            \"logits_preprocessor\": logits_preprocessor\n\t        }\n"]}
{"filename": "src/ipi/decoders/jacobi.py", "chunked_list": ["import torch\n\tfrom src.ipi.decoders.mt_decoding import MTDecoder\n\tfrom src.viz.dependecy_graph import DecodingDependencyGraph\n\tclass JacobiDecoder(MTDecoder):\n\t    def __init__(self, tokenizer, model, initializer, **kwargs):\n\t        super().__init__(tokenizer, model, initializer, **kwargs)\n\t        self.name = \"jacobi\"\n\t        self.acronym = \"j\"\n\t    def generate_target(\n\t            self,\n", "            input_ids: torch.Tensor,\n\t            attention_mask: torch.Tensor,\n\t            is_mbart: bool,\n\t            decoding_method: str = \"greedy\",\n\t            remove_padding: bool = False,\n\t    ):\n\t        if decoding_method == \"greedy\":\n\t            if is_mbart:\n\t                with self.tokenizer.as_target_tokenizer():\n\t                    gold_output = self.model.generate(\n", "                        **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t                        num_beams=1,\n\t                        do_sample=False,\n\t                        forced_bos_token_id=self.tokenizer.cur_lang_code_id,\n\t                    )\n\t            else:\n\t                gold_output = self.model.generate(\n\t                    **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t                    num_beams=1,\n\t                    do_sample=False,\n", "                )\n\t        else:\n\t            raise NotImplementedError()\n\t        if remove_padding:\n\t            sample_lengths = (gold_output != self.tokenizer.pad_token_id).sum(dim=1)\n\t            gold_output = [\n\t                sample[:length] for sample, length in zip(gold_output, sample_lengths)\n\t            ]\n\t        return gold_output\n\t    @torch.no_grad()\n", "    def decode(\n\t        self,\n\t        input_ids,\n\t        attention_mask,\n\t        target_len=None,\n\t        gold_target=None,\n\t        init_tensor=None,\n\t        compute_ddg: bool = False,\n\t        logits_preprocessor=None,\n\t        *args,\n", "        **kwargs\n\t    ):\n\t        max_length = target_len\n\t        str_index = 0\n\t        key_cache = 0\n\t        if compute_ddg:\n\t            if gold_target is None:\n\t                gold_target = self.generate_target(\n\t                    input_ids=input_ids,\n\t                    attention_mask=attention_mask,\n", "                    is_mbart=self.is_mbart,\n\t                )\n\t            ddg = DecodingDependencyGraph(\n\t                model=self.model, tokenizer=self.tokenizer, gold_target=gold_target\n\t            )\n\t            max_length = gold_target.shape[-1]\n\t        if init_tensor is None:\n\t            init_tensor = torch.tensor(\n\t                [self.pad_token_id] * input_ids.size(0) * max_length,\n\t                device=self.device,\n", "            ).reshape(input_ids.size(0), max_length)\n\t        elif self.is_mbart:\n\t            if init_tensor.shape[0] == 1:\n\t                decoder_input_ids = init_tensor[:, 0].unsqueeze(0)\n\t            else:\n\t                decoder_input_ids = init_tensor[:, 0].unsqueeze(-1)\n\t            output = self.model(\n\t                input_ids,\n\t                attention_mask,\n\t                decoder_input_ids=decoder_input_ids,\n", "                use_cache=True,\n\t            )\n\t            encoder_last_hidden_state = output.encoder_last_hidden_state\n\t            past_key_values = output.past_key_values\n\t            str_index = 1\n\t            total_res = init_tensor\n\t            init_tensor = init_tensor[:, 1:]\n\t            key_cache = 1\n\t        output_probs = init_tensor.clone().float()\n\t        for index in range(str_index, max_length):\n", "            if self.use_cache and index > 0:\n\t                old_init_tensor = total_res.detach().clone()\n\t                init_tensor = total_res[:, index:]\n\t                output = self.model(\n\t                    input_ids,\n\t                    attention_mask,\n\t                    decoder_input_ids=init_tensor,\n\t                    encoder_outputs=(encoder_last_hidden_state, None, None),\n\t                    use_cache=True,\n\t                    past_key_values=self.limit_past_key_values(past_key_values, index + key_cache),\n", "                )\n\t            else:\n\t                old_init_tensor = init_tensor.detach().clone()\n\t                output = self.model(\n\t                    input_ids,\n\t                    attention_mask,\n\t                    decoder_input_ids=init_tensor,\n\t                    use_cache=True,\n\t                )\n\t            encoder_last_hidden_state = output.encoder_last_hidden_state\n", "            past_key_values = output.past_key_values\n\t            logits = output.logits\n\t            max_index = torch.argmax(logits, dim=-1)\n\t            max_value, max_i = torch.max(torch.softmax(logits, dim=-1), dim=-1)\n\t            if index > 0 and logits_preprocessor is not None:\n\t                logits_new = logits_preprocessor(total_res[:, : index + 1], logits[:, 0, :])\n\t                max_value_new = torch.argmax(logits_new, dim=-1)\n\t                max_index[:, 0] = max_value_new\n\t            if self.use_cache and index > 0:\n\t                init_tensor = max_index\n", "                total_res = torch.cat(\n\t                    (total_res[:, : index + 1], init_tensor[:, :-1]), dim=1\n\t                )\n\t            else:\n\t                init_tensor[:, index + 1 :] = max_index[:, index:-1]\n\t                total_res = init_tensor\n\t                output_probs[:, index + 1 :] = max_value[:, index:-1]\n\t            stop_condition, return_tensor = self.stopping_criterion(\n\t                old_init_tensor, total_res\n\t            )\n", "            if compute_ddg:\n\t                ddg.insert_one_element(\n\t                    total_res, gold_target, output_probs=output_probs\n\t                )\n\t            if stop_condition:\n\t                break\n\t        if compute_ddg:\n\t            return return_tensor, index, ddg\n\t        return return_tensor, index\n\t    def initialize(self, init_transl):\n", "        if self.initializer is not None:\n\t            init_tensor, _ = self.initializer.init_translation(init_transl.shape[-1])\n\t        else:\n\t            init_tensor = None\n\t        return init_tensor\n\t    def compute_decode_kwargs(self, input_ids, attention_mask, **kwargs):\n\t        gold_autoregressive = self.generate_gold_autoregressive(input_ids, attention_mask)\n\t        init_tensor = self.initialize(init_transl=gold_autoregressive)\n\t        logits_preprocessor = self.generate_logits_preprocessor(input_ids)\n\t        return {\n", "            \"init_tensor\": init_tensor.clone(),\n\t            \"target_len\": gold_autoregressive.shape[-1],\n\t            \"gold_target\": gold_autoregressive,\n\t            \"logits_preprocessor\": logits_preprocessor\n\t        }\n"]}
{"filename": "src/ipi/decoders/__init__.py", "chunked_list": []}
{"filename": "src/ipi/decoders/hybrid_jacobi.py", "chunked_list": ["import torch\n\tfrom more_itertools import sliced\n\tfrom src.ipi.decoders.mt_decoding import MTDecoder\n\tclass HybridJacobiDecoder(MTDecoder):\n\t    def __init__(self, tokenizer, model, gs_jaco_blocks, init_mode, initializer, percent=300, **kwargs):\n\t        super().__init__(tokenizer, model, initializer, **kwargs)\n\t        self.name = \"hybrid_jacobi\"\n\t        self.acronym = \"h\"\n\t        self.gs_jaco_blocks = gs_jaco_blocks\n\t        self.init_mode = init_mode\n", "        self.percent = percent\n\t    @torch.no_grad()\n\t    def decode(\n\t        self, input_ids, attention_mask, target_len, gold_target, init_tensor=None, logits_preprocessor=None, *args, **kwargs\n\t    ):\n\t        key_cache = 1\n\t        if init_tensor is None:\n\t            init_tensor = torch.tensor(\n\t                [self.pad_token_id] * target_len, device=self.device\n\t            )\n", "            blocks = list(sliced(init_tensor, self.gs_jaco_blocks))\n\t            init_tensor = init_tensor.unsqueeze(0)\n\t            total_past_key_values = None\n\t        elif self.is_mbart:\n\t            output = self.model(\n\t                input_ids,\n\t                attention_mask,\n\t                decoder_input_ids=init_tensor[:, 0].unsqueeze(0),\n\t                use_cache=True,\n\t            )\n", "            encoder_last_hidden_state = output.encoder_last_hidden_state\n\t            total_past_key_values = output.past_key_values\n\t            init_tensor = init_tensor[:, 1:]\n\t            blocks = list(sliced(init_tensor.squeeze(0), self.gs_jaco_blocks))\n\t            key_cache = 2\n\t        else:\n\t            init_tensor = init_tensor\n\t            blocks = list(sliced(init_tensor.squeeze(0), self.gs_jaco_blocks))\n\t            total_past_key_values = None\n\t        iteration_saved = 0\n", "        base_value = 0\n\t        for blocco in blocks:\n\t            max_len = blocco.shape[-1]\n\t            blocco_usr = init_tensor[:, base_value : base_value + max_len]\n\t            for index in range(max_len):\n\t                old_blocco = blocco_usr.detach().clone()\n\t                trig = self.trig_eos(\n\t                    old_blocco, self.eos_token_id, init_tensor, base_value\n\t                )\n\t                if trig is not None:\n", "                    return trig, (gold_target.shape[-1] - 1) - iteration_saved\n\t                blocco_usr_new = blocco_usr[:, index:]\n\t                if base_value == 0 and index == 0 and not self.is_mbart:\n\t                    output = self.model(\n\t                        input_ids,\n\t                        attention_mask,\n\t                        decoder_input_ids=blocco_usr_new,\n\t                        use_cache=True,\n\t                        past_key_values=total_past_key_values,\n\t                    )\n", "                    encoder_last_hidden_state = output.encoder_last_hidden_state\n\t                else:\n\t                    output = self.model(\n\t                        input_ids,\n\t                        attention_mask,\n\t                        decoder_input_ids=blocco_usr_new,\n\t                        encoder_outputs=(encoder_last_hidden_state, None, None),\n\t                        use_cache=True,\n\t                        past_key_values=total_past_key_values,\n\t                    )\n", "                total_past_key_values = self.limit_past_key_values(\n\t                    output.past_key_values,\n\t                    base_value + index + key_cache,\n\t                )\n\t                logits = output.logits\n\t                max_value = torch.argmax(logits, dim=-1)\n\t                if logits_preprocessor is not None:\n\t                    logits_new = logits_preprocessor(init_tensor[:, :base_value + index + 1], logits[:, 0, :])\n\t                    max_value_new = torch.argmax(logits_new, dim=-1)\n\t                    max_value[:,0] = max_value_new\n", "                if (\n\t                    max_value.shape[-1]\n\t                    == init_tensor[\n\t                        :, base_value + index + 1 : base_value + max_len + 1\n\t                    ].shape[-1]\n\t                ):\n\t                    init_tensor[\n\t                        :, base_value + index + 1 : base_value + max_len + 1\n\t                    ] = max_value[:, :]\n\t                else:\n", "                    # If last block remove the last token after EOS\n\t                    init_tensor[\n\t                        :, base_value + index + 1 : base_value + max_len + 1\n\t                    ] = max_value[:, :-1]\n\t                stop_condition, _, eos_cond = self.stopping_criterion(\n\t                    old_blocco, blocco_usr, eos=self.eos_token_id\n\t                )\n\t                if stop_condition:\n\t                    if eos_cond >= 0:\n\t                        return (\n", "                            init_tensor[:, : base_value + eos_cond + 1],\n\t                            (gold_target.shape[-1] - 1) - iteration_saved,\n\t                        )\n\t                    if index + 1 != max_len:\n\t                        iteration_saved += max_len - index - 1\n\t                        total_past_key_values = self.limit_past_key_values(\n\t                            output.past_key_values,\n\t                            base_value + max_len + 1,\n\t                        )\n\t                        break\n", "            base_value += max_len\n\t        total_res, total_iter = (\n\t            init_tensor,\n\t            (gold_target.shape[-1] - 1) - iteration_saved,\n\t        )\n\t        init_tensor = init_tensor[:, -1].clone().unsqueeze(0)\n\t        #Go autoregressive until [EOS]\n\t        while True and base_value != self.model.config.max_length - 1:\n\t            index = 0\n\t            output = self.model(\n", "                input_ids,\n\t                attention_mask,\n\t                decoder_input_ids=init_tensor,\n\t                encoder_outputs=(encoder_last_hidden_state, None, None),\n\t                use_cache=True,\n\t                past_key_values=total_past_key_values,\n\t            )\n\t            encoder_last_hidden_state = output.encoder_last_hidden_state\n\t            total_past_key_values = output.past_key_values\n\t            logits = output.logits\n", "            max_value = torch.argmax(logits, dim=-1)\n\t            last = max_value[:, -1]\n\t            if self.use_cache:\n\t                init_tensor = last.unsqueeze(0)\n\t                total_res = torch.cat((total_res, init_tensor), dim=1)\n\t            index += 1\n\t            if last[0].item() == self.eos_token_id:\n\t                break\n\t        return total_res, index + total_iter\n\t    def initialize(self, input_ids, gold_autoregressive):\n", "        if self.initializer is not None:\n\t            if self.init_mode == \"under\":\n\t                len = max(3, input_ids.shape[-1] - self.percent / 100 * input_ids.shape[-1])\n\t                m = int(len)\n\t            elif self.init_mode == \"over\":\n\t                len = input_ids.shape[-1] + self.percent / 100 * input_ids.shape[-1]\n\t                m = int(len)\n\t            elif self.init_mode == \"fixed\":\n\t                m = 511\n\t            else:\n", "                m = gold_autoregressive.shape[-1]\n\t            init_tensor, _ = self.initializer.init_translation(m)\n\t        else:\n\t            init_tensor = None\n\t        return init_tensor\n\t    def compute_decode_kwargs(self, input_ids, attention_mask, **kwargs):\n\t        gold_autoregressive = self.generate_gold_autoregressive(input_ids, attention_mask)\n\t        init_tensor = self.initialize(input_ids, gold_autoregressive)\n\t        logits_preprocessor = self.generate_logits_preprocessor(input_ids)\n\t        return{\n", "            \"init_tensor\": init_tensor.clone(),\n\t            \"gold_target\": gold_autoregressive,\n\t            \"target_len\": gold_autoregressive.shape[-1],\n\t            \"logits_preprocessor\": logits_preprocessor\n\t        }"]}
{"filename": "src/ipi/decoders/gs_jacobi.py", "chunked_list": ["import torch\n\tfrom src.ipi.decoders.mt_decoding import MTDecoder\n\tfrom more_itertools import sliced\n\tclass GSJacobiDecoder(MTDecoder):\n\t    def __init__(self, tokenizer, model, initializer, gs_jaco_blocks, init_mode, **kwargs):\n\t        super().__init__(tokenizer, model, initializer, **kwargs)\n\t        self.name = \"gs_jacobi\"\n\t        self.acronym = \"g\"\n\t        self.gs_jaco_blocks = gs_jaco_blocks\n\t        self.init_mode = init_mode\n", "    @torch.no_grad()\n\t    def decode(\n\t        self, input_ids, attention_mask, target_len, gold_target, init_tensor=None, logits_preprocessor=None, *args, **kwargs\n\t    ):\n\t        key_cache = 1\n\t        if init_tensor is None:\n\t            init_tensor = torch.tensor(\n\t                [self.pad_token_id] * target_len, device=self.device\n\t            )\n\t            blocks = list(sliced(init_tensor, self.gs_jaco_blocks))\n", "            init_tensor = init_tensor.unsqueeze(0)\n\t            total_past_key_values = None\n\t        elif self.is_mbart:\n\t            output = self.model(\n\t                input_ids,\n\t                attention_mask,\n\t                decoder_input_ids=init_tensor[:, 0].unsqueeze(0),\n\t                use_cache=True,\n\t            )\n\t            encoder_last_hidden_state = output.encoder_last_hidden_state\n", "            total_past_key_values = output.past_key_values\n\t            delta = 1\n\t            # total_res = init_tensor.to(self.device)\n\t            init_tensor = init_tensor[:, 1:]\n\t            blocks = list(sliced(init_tensor.squeeze(0), self.gs_jaco_blocks))\n\t            key_cache = 2\n\t        else:\n\t            init_tensor = init_tensor\n\t            blocks = list(sliced(init_tensor.squeeze(0), self.gs_jaco_blocks))\n\t            total_past_key_values = None\n", "        iteration_saved = 0\n\t        base_value = 0\n\t        for blocco in blocks:\n\t            max_len = blocco.shape[-1]\n\t            blocco_usr = init_tensor[:, base_value : base_value + max_len]\n\t            for index in range(max_len):\n\t                old_blocco = blocco_usr.detach().clone()\n\t                blocco_usr_new = blocco_usr[:, index:]\n\t                if base_value == 0 and index == 0 and not self.is_mbart:\n\t                    output = self.model(\n", "                        input_ids,\n\t                        attention_mask,\n\t                        decoder_input_ids=blocco_usr_new,\n\t                        use_cache=True,\n\t                        past_key_values=total_past_key_values,\n\t                    )\n\t                    encoder_last_hidden_state = output.encoder_last_hidden_state\n\t                else:\n\t                    output = self.model(\n\t                        input_ids,\n", "                        attention_mask,\n\t                        decoder_input_ids=blocco_usr_new,\n\t                        encoder_outputs=(encoder_last_hidden_state, None, None),\n\t                        use_cache=True,\n\t                        past_key_values=total_past_key_values,\n\t                    )\n\t                total_past_key_values = self.limit_past_key_values(\n\t                    output.past_key_values,\n\t                    base_value + index + key_cache,\n\t                )\n", "                logits = output.logits\n\t                max_value = torch.argmax(logits, dim=-1)\n\t                if logits_preprocessor is not None:\n\t                    logits_new = logits_preprocessor(init_tensor[:, :base_value + index +1], logits[:, 0, :])\n\t                    max_value_new = torch.argmax(logits_new, dim=-1)\n\t                    max_value[:, 0] = max_value_new\n\t                if (\n\t                    max_value.shape[-1]\n\t                    == init_tensor[\n\t                        :, base_value + index + 1 : base_value + max_len + 1\n", "                    ].shape[-1]\n\t                ):\n\t                    init_tensor[\n\t                        :, base_value + index + 1 : base_value + max_len + 1\n\t                    ] = max_value[:, :]\n\t                else:\n\t                    # If last block remove the last token after EOS\n\t                    init_tensor[\n\t                        :, base_value + index + 1 : base_value + max_len + 1\n\t                    ] = max_value[:, :-1]\n", "                stop_condition, _ = self.stopping_criterion(old_blocco, blocco_usr)\n\t                if stop_condition and index + 1 != max_len:\n\t                    total_past_key_values = self.limit_past_key_values(\n\t                        output.past_key_values,\n\t                        base_value + max_len + 1,\n\t                    )\n\t                    iteration_saved += max_len - index - 1\n\t                    break\n\t            base_value += max_len\n\t        return init_tensor, (gold_target.shape[-1] - 1) - iteration_saved\n", "    def initialize(self, input_ids, gold_autoregressive):\n\t        if self.initializer is not None:\n\t            if self.init_mode == \"overprov\":\n\t                m = int(input_ids.shape[-1] + 10 / 100 * input_ids.shape[-1])\n\t            else:\n\t                m = gold_autoregressive.shape[-1]\n\t            init_tensor, _ = self.initializer.init_translation(m)\n\t        else:\n\t            init_tensor = None\n\t        return init_tensor\n", "    def compute_decode_kwargs(self, input_ids, attention_mask, **kwargs):\n\t        gold_autoregressive = self.generate_gold_autoregressive(input_ids, attention_mask)\n\t        init_tensor = self.initialize(input_ids, gold_autoregressive)\n\t        logits_preprocessor = self.generate_logits_preprocessor(input_ids)\n\t        return{\n\t            \"init_tensor\": init_tensor.clone(),\n\t            \"gold_target\": gold_autoregressive,\n\t            \"target_len\": gold_autoregressive.shape[-1],\n\t            \"logits_preprocessor\": logits_preprocessor\n\t        }\n"]}
{"filename": "src/ipi/decoders/mt_decoding.py", "chunked_list": ["from typing import Dict, Optional\n\timport torch\n\tfrom transformers import MBartForConditionalGeneration\n\tfrom src.ipi import stopping_condition as sc\n\tfrom src.utils.utils import get_logits_preprocessor\n\tPREC_GOLD_AUTOREGRESSIVE: Dict[str, Optional[torch.Tensor]] = {\"input_ids\": None, \"gold\": None}\n\tPREC_LOGISTS_PREPROCESSOR: Dict[str, Optional[torch.Tensor]] = {\"input_ids\": None, \"logists\": None}\n\tclass MTDecoder:\n\t    def __init__(\n\t            self,\n", "            tokenizer,\n\t            model,\n\t            initializer,\n\t            use_cache: bool = True,\n\t            use_logits_preprocessor: bool = True,\n\t            device: str = \"cuda\",\n\t            **kwargs\n\t    ):\n\t        self.tokenizer = tokenizer\n\t        self.initializer = initializer\n", "        self.pad_token_id = self.tokenizer.pad_token_id\n\t        self.eos_token_id = self.tokenizer.eos_token_id\n\t        self.use_cache = use_cache\n\t        self.device = device\n\t        with torch.no_grad():\n\t            self.model = model\n\t            self.model_name = self.model.name_or_path\n\t            self.model.eval()\n\t        self.max_length = min(self.tokenizer.model_max_length, 511)\n\t        self.use_logits_preprocessor = use_logits_preprocessor\n", "        self.is_mbart = isinstance(self.model, MBartForConditionalGeneration)\n\t    def decode(self, input_ids, attention_mask, *args, **kwargs):\n\t        pass\n\t    def compute_decode_kwargs(self, *args, **kwargs):\n\t        pass\n\t    def initialize(self, **kwargs):\n\t        pass\n\t    def generate_gold_autoregressive(self, input_ids, attention_mask):\n\t        global PREC_GOLD_AUTOREGRESSIVE\n\t        if PREC_GOLD_AUTOREGRESSIVE['input_ids'] is None or not torch.equal(input_ids, PREC_GOLD_AUTOREGRESSIVE['input_ids']):\n", "            if self.is_mbart:\n\t                with self.tokenizer.as_target_tokenizer():\n\t                    try:\n\t                        lang_id = self.tokenizer.cur_lang_code_id\n\t                    except:\n\t                        lang_id = self.tokenizer.cur_lang_id\n\t                gold_autoregressive = self.model.generate(\n\t                    **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t                    num_beams=1,\n\t                    do_sample=False,\n", "                    use_cache=False,\n\t                    forced_bos_token_id=lang_id,\n\t                )\n\t            else:\n\t                gold_autoregressive = self.model.generate(\n\t                    **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t                    num_beams=1,\n\t                    do_sample=False,\n\t                    use_cache=False,\n\t                )\n", "            gold_autoregressive = gold_autoregressive[:, : self.max_length]\n\t            PREC_GOLD_AUTOREGRESSIVE['input_ids'] = input_ids\n\t            PREC_GOLD_AUTOREGRESSIVE['gold'] = gold_autoregressive\n\t        return PREC_GOLD_AUTOREGRESSIVE['gold']\n\t    def generate_logits_preprocessor(self, input_ids):\n\t        global PREC_LOGISTS_PREPROCESSOR\n\t        if self.use_logits_preprocessor:\n\t            if PREC_LOGISTS_PREPROCESSOR['input_ids'] is None or not torch.equal(input_ids, PREC_LOGISTS_PREPROCESSOR['input_ids']):\n\t                logits_preprocessor = get_logits_preprocessor(\n\t                    model=self.model,\n", "                    input_ids=input_ids,\n\t                    eos_token_id=self.eos_token_id\n\t                )\n\t                PREC_LOGISTS_PREPROCESSOR['input_ids'] = input_ids\n\t                PREC_LOGISTS_PREPROCESSOR['logists'] = logits_preprocessor\n\t        else:\n\t            return None\n\t        return PREC_LOGISTS_PREPROCESSOR['logists']\n\t    @staticmethod\n\t    def stopping_criterion(past_tensor, current_tensor, eos=None):\n", "        return sc.stopping_criterion(past_tensor, current_tensor, eos)\n\t    @staticmethod\n\t    def limit_past_key_values(past_key_values, limit):\n\t        return sc.limit_past_key_values(past_key_values, limit)\n\t    @staticmethod\n\t    def trig_eos(tensor, eos_token_id, init_tensor, base_value):\n\t        if tensor[:, 0].item() == eos_token_id:\n\t            return init_tensor[:, : base_value + 1]\n\t        else:\n\t            return None\n", "def generate_target(\n\t    tokenizer,\n\t    model,\n\t    input_ids: torch.Tensor,\n\t    attention_mask: torch.Tensor,\n\t    is_mbart: bool,\n\t    decoding_method: str = \"greedy\",\n\t    remove_padding: bool = False,\n\t):\n\t    if decoding_method == \"greedy\":\n", "        if is_mbart:\n\t            with tokenizer.as_target_tokenizer():\n\t                gold_output = model.generate(\n\t                    **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t                    num_beams=1,\n\t                    do_sample=False,\n\t                    forced_bos_token_id=tokenizer.cur_lang_code_id,\n\t                )\n\t        else:\n\t            gold_output = model.generate(\n", "                **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t                num_beams=1,\n\t                do_sample=False,\n\t            )\n\t    else:\n\t        raise NotImplementedError()\n\t    if remove_padding:\n\t        sample_lengths = (gold_output != tokenizer.pad_token_id).sum(dim=1)\n\t        gold_output = [\n\t            sample[:length] for sample, length in zip(gold_output, sample_lengths)\n", "        ]\n\t    return gold_output\n"]}
{"filename": "src/ipi/decoders/beam_search.py", "chunked_list": ["from src.ipi.decoders.mt_decoding import MTDecoder\n\tclass BeamSearchDecoder(MTDecoder):\n\t    def __init__(self, tokenizer, model, initializer, num_beams, early_stopping, **kwargs):\n\t        super().__init__(tokenizer, model, initializer, **kwargs)\n\t        self.name = \"beam_search\"\n\t        self.acronym = \"b\"\n\t        self.num_beams = num_beams\n\t        self.early_stopping = early_stopping\n\t    def decode(self, input_ids, attention_mask, *args, **kwargs):\n\t        if self.is_mbart:\n", "            with self.tokenizer.as_target_tokenizer():\n\t                try:\n\t                    lang_id = self.tokenizer.cur_lang_code_id\n\t                except:\n\t                    lang_id = self.tokenizer.cur_lang_id\n\t            beam_output = self.model.generate(\n\t                **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t                num_beams=self.num_beams,\n\t                early_stopping=self.early_stopping,\n\t                forced_bos_token_id=lang_id,\n", "            )\n\t        else:\n\t            beam_output = self.model.generate(\n\t                **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t                num_beams=self.num_beams,\n\t                early_stopping=self.early_stopping,\n\t            )\n\t        return beam_output, 0\n\t    def compute_decode_kwargs(self, *args, **kwargs):\n\t        return {}"]}
{"filename": "src/utils/bench_scorer.py", "chunked_list": ["from src.utils.bleu_calculator import BleuEvaluator, BleuValues\n\tclass Scorer(object):\n\t    def __init__(self, name, acronym):\n\t        self.name = name\n\t        self.acronym = acronym\n\t        self.bleu_scorer = BleuEvaluator()\n\t        # number of sentences\n\t        self.i = 0\n\t        # param bleu score\n\t        self.predictions = []\n", "        self.references = []\n\t        # benchmark values\n\t        self.tot_mean_time = 0\n\t        self.tot_mean_iter = 0\n\t        # inline values\n\t        self.current_init = None\n\t        self.current_transl = None\n\t        self.current_time = None\n\t        self.current_iter = None\n\t    def update_metrics(self, time, iter, translation, gold, init):\n", "        self.tot_mean_time += (time - self.tot_mean_time) / (self.i + 1)\n\t        self.tot_mean_iter += (iter - self.tot_mean_iter) / (self.i + 1)\n\t        self.predictions.append(translation)\n\t        self.references.append([gold])\n\t        self.current_init = init\n\t        self.current_transl = translation\n\t        self.current_time = time\n\t        self.current_iter = iter\n\t        self.i += 1\n\t    def compute_bleu_score(self):\n", "        bleu_score = self.bleu_scorer.final_score(\n\t            model_predictions=self.predictions,\n\t            gold_references=self.references\n\t        )\n\t        bleu_dict = {\n\t            'score': bleu_score['score'],\n\t            'counts': str(bleu_score['counts']),\n\t            'totals': str(bleu_score['totals']),\n\t            'precisions': str(['%.2f' % prec for prec in bleu_score['precisions']]),\n\t            'bp': bleu_score['bp'],\n", "            'sys_len': bleu_score['sys_len'],\n\t            'ref_len': bleu_score['ref_len']\n\t        }\n\t        return BleuValues(**bleu_dict)"]}
{"filename": "src/utils/__init__.py", "chunked_list": []}
{"filename": "src/utils/utils.py", "chunked_list": ["import csv\n\timport os\n\timport random\n\timport re\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom torch.nn.modules import linear\n\timport datasets\n\tdef makedirs(path):\n", "    if path.endswith((\".tsv\", \".csv\", \".txt\")):\n\t        path = \"/\".join(path.split(\"/\")[:-1])\n\t    if not os.path.exists(path):\n\t        os.makedirs(path)\n\tdef check_zero_division(a, b):\n\t    return \"na\" if b == 0 else round(a / b, 3)\n\tdef get_logits_preprocessor(model, input_ids, eos_token_id):\n\t    logits_preprocessor = model._get_logits_processor(\n\t        repetition_penalty=None,\n\t        no_repeat_ngram_size=None,\n", "        encoder_no_repeat_ngram_size=None,\n\t        input_ids_seq_length=1,\n\t        encoder_input_ids=input_ids,\n\t        bad_words_ids=None,\n\t        min_length=0,\n\t        max_length=model.config.max_length,\n\t        eos_token_id=eos_token_id,\n\t        forced_bos_token_id=None,\n\t        forced_eos_token_id=None,\n\t        prefix_allowed_tokens_fn=None,\n", "        num_beams=1,\n\t        num_beam_groups=1,\n\t        diversity_penalty=None,\n\t        remove_invalid_values=None,\n\t        exponential_decay_length_penalty=None,\n\t        logits_processor=[],\n\t        renormalize_logits=None\n\t    )\n\t    return logits_preprocessor\n\tdef retrieve_model_name(model_name):\n", "    if \"opus\" in model_name:\n\t        return \"opus\"\n\t    if \"mbart\" in model_name:\n\t        if \"50\" in model_name:\n\t            return \"mbart_50\"\n\t        return \"mbart\"\n\tdef seed_everything(seed: int):\n\t    random.seed(seed)\n\t    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\t    np.random.seed(seed)\n", "    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    torch.backends.cudnn.deterministic = True\n\t    torch.backends.cudnn.benchmark = True\n\tdef retrieve_map_languages_flores(lan):\n\t    lang_map = {\n\t        \"ab\": \"Abkhaz\",\n\t        \"aa\": \"Afar\",\n\t        \"af\": \"Afrikaans\",\n\t        \"ak\": \"Akan\",\n", "        \"sq\": \"Albanian\",\n\t        \"am\": \"Amharic\",\n\t        \"ar\": \"Arabic\",\n\t        \"an\": \"Aragonese\",\n\t        \"hy\": \"Armenian\",\n\t        \"as\": \"Assamese\",\n\t        \"av\": \"Avaric\",\n\t        \"ae\": \"Avestan\",\n\t        \"ay\": \"Aymara\",\n\t        \"az\": \"Azerbaijani\",\n", "        \"bm\": \"Bambara\",\n\t        \"ba\": \"Bashkir\",\n\t        \"eu\": \"Basque\",\n\t        \"be\": \"Belarusian\",\n\t        \"bn\": \"Bengali\",\n\t        \"bh\": \"Bihari\",\n\t        \"bi\": \"Bislama\",\n\t        \"bs\": \"Bosnian\",\n\t        \"br\": \"Breton\",\n\t        \"bg\": \"Bulgarian\",\n", "        \"my\": \"Burmese\",\n\t        \"ca\": \"Catalan\",\n\t        \"ch\": \"Chamorro\",\n\t        \"ce\": \"Chechen\",\n\t        \"ny\": \"Chichewa\",\n\t        \"zh\": \"Chinese\",\n\t        \"cv\": \"Chuvash\",\n\t        \"kw\": \"Cornish\",\n\t        \"co\": \"Corsican\",\n\t        \"cr\": \"Cree\",\n", "        \"hr\": \"Croatian\",\n\t        \"cs\": \"Czech\",\n\t        \"da\": \"Danish\",\n\t        \"dv\": \"Divehi\",\n\t        \"nl\": \"Dutch\",\n\t        \"dz\": \"Dzongkha\",\n\t        \"en\": \"English\",\n\t        \"eo\": \"Esperanto\",\n\t        \"et\": \"Estonian\",\n\t        \"ee\": \"Ewe\",\n", "        \"fo\": \"Faroese\",\n\t        \"fj\": \"Fijian\",\n\t        \"fi\": \"Finnish\",\n\t        \"fr\": \"Franch\",\n\t        \"ff\": \"Fula\",\n\t        \"gl\": \"Galician\",\n\t        \"ka\": \"Georgian\",\n\t        \"de\": \"German\",\n\t        \"el\": \"Greek\",\n\t        \"gn\": \"Guaraní\",\n", "        \"gu\": \"Gujarati\",\n\t        \"ht\": \"Haitian\",\n\t        \"ha\": \"Hausa\",\n\t        \"he\": \"Hebrew\",\n\t        \"hz\": \"Herero\",\n\t        \"hi\": \"Hindi\",\n\t        \"ho\": \"Hiri Motu\",\n\t        \"hu\": \"Hungarian\",\n\t        \"ia\": \"Interlingua\",\n\t        \"id\": \"Indonesian\",\n", "        \"ie\": \"Interlingue\",\n\t        \"ga\": \"Irish\",\n\t        \"ig\": \"Igbo\",\n\t        \"ik\": \"Inupiaq\",\n\t        \"io\": \"Ido\",\n\t        \"is\": \"Icelandic\",\n\t        \"it\": \"Italian\",\n\t        \"iu\": \"Inuktitut\",\n\t        \"ja\": \"Japanese\",\n\t        \"jv\": \"Javanese\",\n", "        \"kl\": \"Kalaallisut\",\n\t        \"kn\": \"Kannada\",\n\t        \"kr\": \"Kanuri\",\n\t        \"ks\": \"Kashmiri\",\n\t        \"kk\": \"Kazakh\",\n\t        \"km\": \"Khmer\",\n\t        \"ki\": \"Kikuyu\",\n\t        \"rw\": \"Kinyarwanda\",\n\t        \"ky\": \"Kirghiz\",\n\t        \"kv\": \"Komi\",\n", "        \"kg\": \"Kongo\",\n\t        \"ko\": \"Korean\",\n\t        \"ku\": \"Kurdish\",\n\t        \"kj\": \"Kwanyama\",\n\t        \"la\": \"Latin\",\n\t        \"lb\": \"Luxembourgish\",\n\t        \"lg\": \"Luganda\",\n\t        \"li\": \"Limburgish\",\n\t        \"ln\": \"Lingala\",\n\t        \"lo\": \"Lao\",\n", "        \"lt\": \"Lithuanian\",\n\t        \"lu\": \"Luba-Katanga\",\n\t        \"lv\": \"Latvian\",\n\t        \"gv\": \"Manx\",\n\t        \"mk\": \"Macedonian\",\n\t        \"mg\": \"Malagasy\",\n\t        \"ms\": \"Malay\",\n\t        \"ml\": \"Malayalam\",\n\t        \"mt\": \"Maltese\",\n\t        \"mi\": \"Māori\",\n", "        \"mr\": \"Marathi\",\n\t        \"mh\": \"Marshallese\",\n\t        \"mn\": \"Mongolian\",\n\t        \"na\": \"Nauru\",\n\t        \"nv\": \"Navajo\",\n\t        \"nb\": \"Norwegian Bokmål\",\n\t        \"nd\": \"North Ndebele\",\n\t        \"ne\": \"Nepali\",\n\t        \"ng\": \"Ndonga\",\n\t        \"nn\": \"Norwegian Nynorsk\",\n", "        \"no\": \"Norwegian\",\n\t        \"ii\": \"Nuosu\",\n\t        \"nr\": \"South Ndebele\",\n\t        \"oc\": \"Occitan\",\n\t        \"oj\": \"Ojibwe\",\n\t        \"cu\": \"Old Church Slavonic\",\n\t        \"om\": \"Oromo\",\n\t        \"or\": \"Oriya\",\n\t        \"os\": \"Ossetian\",\n\t        \"pa\": \"Panjabi\",\n", "        \"pi\": \"Pāli\",\n\t        \"fa\": \"Persian\",\n\t        \"pl\": \"Polish\",\n\t        \"ps\": \"Pashto\",\n\t        \"pt\": \"Portuguese\",\n\t        \"qu\": \"Quechua\",\n\t        \"rm\": \"Romansh\",\n\t        \"rn\": \"Kirundi\",\n\t        \"ro\": \"Romanian\",\n\t        \"ru\": \"Russian\",\n", "        \"sa\": \"Sanskrit\",\n\t        \"sc\": \"Sardinian\",\n\t        \"sd\": \"Sindhi\",\n\t        \"se\": \"Northern Sami\",\n\t        \"sm\": \"Samoan\",\n\t        \"sg\": \"Sango\",\n\t        \"sr\": \"Serbian\",\n\t        \"gd\": \"Scottish Gaelic\",\n\t        \"sn\": \"Shona\",\n\t        \"si\": \"Sinhala\",\n", "        \"sk\": \"Slovak\",\n\t        \"sl\": \"Slovene\",\n\t        \"so\": \"Somali\",\n\t        \"st\": \"Southern Sotho\",\n\t        \"es\": \"Spanish\",\n\t        \"su\": \"Sundanese\",\n\t        \"sw\": \"Swahili\",\n\t        \"ss\": \"Swati\",\n\t        \"sv\": \"Swedish\",\n\t        \"ta\": \"Tamil\",\n", "        \"te\": \"Telugu\",\n\t        \"tg\": \"Tajik\",\n\t        \"th\": \"Thai\",\n\t        \"ti\": \"Tigrinya\",\n\t        \"bo\": \"Tibetan\",\n\t        \"tk\": \"Turkmen\",\n\t        \"tl\": \"Tagalog\",\n\t        \"tn\": \"Tswana\",\n\t        \"to\": \"Tonga\",\n\t        \"tr\": \"Turkish\",\n", "        \"ts\": \"Tsonga\",\n\t        \"tt\": \"Tatar\",\n\t        \"tw\": \"Twi\",\n\t        \"ty\": \"Tahitian\",\n\t        \"ug\": \"Uighur\",\n\t        \"uk\": \"Ukrainian\",\n\t        \"ur\": \"Urdu\",\n\t        \"uz\": \"Uzbek\",\n\t        \"ve\": \"Venda\",\n\t        \"vi\": \"Vietnamese\",\n", "        \"vo\": \"Volapük\",\n\t        \"wa\": \"Walloon\",\n\t        \"cy\": \"Welsh\",\n\t        \"wo\": \"Wolof\",\n\t        \"fy\": \"Western Frisian\",\n\t        \"xh\": \"Xhosa\",\n\t        \"yi\": \"Yiddish\",\n\t        \"yo\": \"Yoruba\",\n\t        \"za\": \"Zhuang\",\n\t        \"zu\": \"Zulu\",\n", "    }\n\t    return lang_map[lan]\n\tdef read_csv(path_csv):\n\t    csv_reader = pd.read_csv(path_csv, sep=\"\\t\", header=0)\n\t    return csv_reader[\"times\"].tolist()\n\tdef retrieve_samples(path, dataset):\n\t    sacrebleu = datasets.load_metric(\"sacrebleu\")\n\t    decoders = [\"autoregressive\", \"jacobi\", \"gs_jacobi\", \"aw_jacobi\"]\n\t    decoder2file = {\n\t        \"autoregressive\": {\"time\": [], \"trans\": []},\n", "        \"jacobi\": {\"time\": [], \"trans\": []},\n\t        \"gs_jacobi\": {\"time\": [], \"trans\": []},\n\t        \"aw_jacobi\": {\"time\": [], \"trans\": []},\n\t    }\n\t    for folder in decoders:\n\t        subf = os.path.join(path, folder)\n\t        for root, dirs, files in os.walk(subf):\n\t            for filename in files:\n\t                if \"time\" in filename:\n\t                    times = read_csv(os.path.join(root, filename))\n", "                    decoder2file[folder]['time'].extend(times)\n\t                if 'trans' in filename:\n\t                    trans = read_csv(os.path.join(root, filename))\n\t                    decoder2file[folder]['trans'].extend(trans)\n\t    diff_times = np.array([auto-aw for auto, aw in zip(decoder2file['autoregressive']['time'], decoder2file['aw_jacobi']['time'])])\n\t    indices = diff_times.argsort()[::-1]\n\t    for i in indices:\n\t        target = dataset[int(i)][1]\n\t        source = dataset[int(i)][0]\n\t        print(\"gold\", \"nd\", source)\n", "        print(\"gold\", \"nd\", target)\n\t        for d in decoders:\n\t            prediction = decoder2file[d]['trans'][i]\n\t            results = sacrebleu.compute(predictions=[prediction], references=[[target]])\n\t            print(d, decoder2file[d]['time'][i], round(results['score'], 3), prediction)\n\t        input()\n\t        continue\n\tdef clean_text(text):\n\t    return re.sub(\"[!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~£−€¿]+\", \" \", text)\n\tdef read_wrong_beam_translations(path):\n", "    csv_reader = pd.read_csv(path, sep=\"\\t\", header=0)\n\t    idx = csv_reader['i'].tolist()\n\t    bleus = csv_reader['bleu'].tolist()\n\t    beams = csv_reader['beam'].tolist()\n\t    autos = csv_reader['auto'].tolist()\n\t    tgts = csv_reader['tgt'].tolist()\n\t    for x in zip(idx, bleus, beams, autos, tgts):\n\t        print(f\"{x[0]} {x[1]}\\nBeam: {x[2]}\\nAuto: {x[3]}\\nTgt: {x[4]}\")\n\t        input()\n\t        continue\n", "def write_sentences(path, data):\n\t    with open(path, 'w') as out_file:\n\t        output = csv.writer(out_file, delimiter=\"\\n\")\n\t        output.writerow(data)\n"]}
{"filename": "src/utils/bleu_calculator.py", "chunked_list": ["import os\n\timport datasets\n\timport pandas as pd\n\tfrom tabulate import tabulate\n\tclass BleuValues:\n\t    def __init__(self, **entries):\n\t        self.__dict__.update(entries)\n\tclass BleuEvaluator(object):\n\t    def __init__(self):\n\t        self.metric = datasets.load_metric(\"sacrebleu\")\n", "    def add_element(self, model_predictions, gold_references):\n\t        self.metric.add(predictions=model_predictions, references=gold_references)\n\t    def add_batch(self, predictions, references):\n\t        self.metric.add_batch(predictions=predictions, references=references)\n\t    def final_score(self, model_predictions, gold_references):\n\t        return self.metric.compute(predictions=model_predictions, references=gold_references)\n\tclass BleuCalculator:\n\t    def __init__(\n\t        self,\n\t        dataset,\n", "        result_dir,\n\t    ):\n\t        self.dataset = dataset\n\t        self.result_dir = result_dir\n\t    @staticmethod\n\t    def read_csv(path_csv):\n\t        csv_reader = pd.read_csv(path_csv, sep=\"\\t\", header=0)\n\t        return {\n\t            k: v\n\t            for k, v in zip(\n", "                csv_reader[\"#sentence\"].tolist(), csv_reader[\"times\"].tolist()\n\t            )\n\t        }\n\t    def _retrieve_files(self):\n\t        file2data = dict()\n\t        for root, dirs, files in os.walk(self.result_dir):\n\t            if any(map(lambda x: \"trans\" in x, files)) and \"initrans\" not in root:\n\t                trans_files_name = list(filter(lambda x: (\"trans\" in x), files))[0]\n\t                data = self.read_csv(path_csv=os.path.join(root, trans_files_name))\n\t                file2data.update({trans_files_name.split(\".\")[-2]: data})\n", "        return file2data\n\t    def _load_dataset(self):\n\t        return {i: x[1] for i, x in enumerate(self.dataset)}\n\t    @staticmethod\n\t    def _match_indices(method, gold):\n\t        new_gold = dict()\n\t        for k in gold:\n\t            if k in method:\n\t                new_gold.update({k: gold[k]})\n\t        return new_gold\n", "    @staticmethod\n\t    def _bleu_score_formatter(bleu_score):\n\t        bleu_dict = {\n\t            \"score\": bleu_score[\"score\"],\n\t            \"counts\": str(bleu_score[\"counts\"]),\n\t            \"totals\": str(bleu_score[\"totals\"]),\n\t            \"precisions\": str([\"%.2f\" % prec for prec in bleu_score[\"precisions\"]]),\n\t            \"bp\": bleu_score[\"bp\"],\n\t            \"sys_len\": bleu_score[\"sys_len\"],\n\t            \"ref_len\": bleu_score[\"ref_len\"],\n", "        }\n\t        return BleuValues(**bleu_dict)\n\t    def write_report(self, file2score):\n\t        print(\"Writing report...\")\n\t        # Table for the Bleu score\n\t        header = [\"Metrics\"] + [m[0] for m in file2score]\n\t        bleu_table = tabulate(\n\t            [\n\t                [\"Score\"] + [b[1].score for b in file2score],\n\t                [\"Counts\"] + [b[1].counts for b in file2score],\n", "                [\"Totals\"] + [b[1].totals for b in file2score],\n\t                [\"Precisions\"] + [b[1].precisions for b in file2score],\n\t                [\"Bp\"] + [b[1].bp for b in file2score],\n\t                [\"Sys_len\"] + [b[1].sys_len for b in file2score],\n\t                [\"ref_len\"] + [b[1].ref_len for b in file2score],\n\t            ],\n\t            headers=header,\n\t            tablefmt=\"rst\",\n\t        )\n\t        with open(os.path.join(self.result_dir, \"bleu_report.txt\"), mode=\"w\") as report:\n", "            report.write(f\"Bleu Score\\n{bleu_table}\\n\\n\")\n\t    def _compute_bleu_score(self, name, translations, gold):\n\t        scorer = BleuEvaluator()\n\t        translations = list(translations.values())\n\t        gold = list(gold.values())\n\t        gold = [[g] for g in gold]\n\t        # for t, g in zip(translations, gold):\n\t        #     scorer.add_element(t, [g])\n\t        score_value = scorer.final_score(translations, gold)\n\t        return name, self._bleu_score_formatter(score_value)\n", "    def compute_bleu_score(self):\n\t        file2data = self._retrieve_files()\n\t        gold = self._load_dataset()\n\t        if \"trans_beam\" in file2data:\n\t            beam_search = self._match_indices(\n\t                file2data[\"trans_gs_jacobi\"], file2data[\"trans_beam\"]\n\t            )\n\t            file2data[\"trans_beam\"] = beam_search\n\t        gold = self._match_indices(file2data[\"trans_gs_jacobi\"], gold)\n\t        file2score = [\n", "            self._compute_bleu_score(file, file2data[file], gold) for file in file2data\n\t        ]\n\t        self.write_report(file2score)\n"]}
{"filename": "src/utils/beam_search.py", "chunked_list": ["import csv\n\timport os\n\timport torch\n\timport time\n\tfrom tabulate import tabulate\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm import tqdm\n\tfrom transformers import MBartForConditionalGeneration, M2M100ForConditionalGeneration\n\timport numpy as np\n\tfrom src.utils.bench_scorer import Scorer\n", "from src.utils.utils import makedirs, retrieve_model_name, write_sentences, get_logits_preprocessor\n\tclass BeamSearcher(object):\n\t    def __init__(\n\t            self,\n\t            dataset,\n\t            model,\n\t            initializer,\n\t            decoder,\n\t            num_beams,\n\t            no_repeat_ngram_size,\n", "            early_stopping,\n\t            batch_size,\n\t            device,\n\t            result_dir,\n\t    ):\n\t        self.num_beams = num_beams\n\t        self.no_repeat_ngram_size = no_repeat_ngram_size\n\t        self.early_stopping = early_stopping\n\t        self.device = device\n\t        self.result_dir = result_dir\n", "        self.dataset = dataset\n\t        self.initializer = initializer\n\t        self.decoder = decoder\n\t        self.tokenizer = dataset.tokenizer\n\t        self.model = model\n\t        model.eval().to(self.device)\n\t        self.model_name = self.model.name_or_path\n\t        print(\"model name in beam_search\", self.model_name)\n\t        self.dataloader = DataLoader(\n\t            dataset, collate_fn=dataset.collate_fn, batch_size=1, shuffle=False\n", "        )\n\t        if isinstance(\n\t                self.model, MBartForConditionalGeneration\n\t        ) or isinstance(\n\t            self.model, M2M100ForConditionalGeneration\n\t        ):\n\t            self.is_mbart = True\n\t        else:\n\t            self.is_mbart = False\n\t        self.exp_dir = self._retrieve_exp_dir()\n", "    def _synchronize(self):\n\t        if self.device == \"cuda\":\n\t            torch.cuda.synchronize()\n\t    def _retrieve_exp_dir(self):\n\t        file_name = self._retrieve_file_name()\n\t        exp_dir = os.path.join(self.result_dir, 'beam_search', file_name)\n\t        makedirs(exp_dir)\n\t        return exp_dir\n\t    def _retrieve_file_name(self):\n\t        model_name = retrieve_model_name(self.model_name.split(\"/\")[1])\n", "        lang = f\"{self.dataset.src_lan}_{self.dataset.tgt_lan}\"\n\t        return f\"{model_name}/{self.dataset.name}/{lang}\"\n\t    def _beam_search(self, input_ids, attention_mask):\n\t      if self.is_mbart:\n\t        with self.tokenizer.as_target_tokenizer():\n\t            try:\n\t                lang_id = self.tokenizer.cur_lang_code_id\n\t            except:\n\t                lang_id = self.tokenizer.cur_lang_id\n\t        beam_output = self.model.generate(\n", "            **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t            num_beams=self.num_beams,\n\t            early_stopping=self.early_stopping,\n\t            # no_repeat_ngram_size=self.no_repeat_ngram_size,\n\t            forced_bos_token_id=lang_id,\n\t            # do_sample=False,\n\t            # use_cache=True,\n\t        )\n\t      else:\n\t          beam_output = self.model.generate(\n", "              **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n\t              num_beams=self.num_beams,\n\t              early_stopping=self.early_stopping,\n\t              # no_repeat_ngram_size=self.no_repeat_ngram_size,\n\t              # do_sample=False,\n\t              # use_cache=True,\n\t          )\n\t      return beam_output\n\t    def _bench_time(self, input_ids, attention_mask):\n\t        sample_time = []\n", "        for _ in range(1):\n\t          start = time.perf_counter()\n\t          self._synchronize()\n\t          beam_output = self._beam_search(input_ids, attention_mask)\n\t          self._synchronize()\n\t          end = time.perf_counter()\n\t          sample_time.append(end - start)\n\t        sample_mean = np.average(sample_time)\n\t        sample_variance = np.var(sample_time)\n\t        return sample_mean, sample_variance, beam_output\n", "    def _auto_time(self, input_ids, attention_mask, logits_preprocessor=None):\n\t        if self.initializer is not None:\n\t            init_tensor, _ = self.initializer.init_translation()\n\t        else:\n\t            init_tensor = None\n\t        sample_time = []\n\t        for _ in range(1):\n\t            init_new = init_tensor.clone()\n\t            start = time.perf_counter()\n\t            self._synchronize()\n", "            auto_output, _ = self.decoder.autoregressive(\n\t                input_ids, attention_mask, init_tensor=init_new, logits_preprocessor=logits_preprocessor\n\t            )\n\t            self._synchronize()\n\t            end = time.perf_counter()\n\t            sample_time.append(end - start)\n\t        sample_mean = np.average(sample_time)\n\t        sample_variance = np.var(sample_time)\n\t        return sample_mean, sample_variance, auto_output\n\t    def compute_beam_search(self, cfg):\n", "        beam_scorer, auto_scorer = Scorer(), Scorer()\n\t        worst_beam_translations = []\n\t        pbar = tqdm(self.dataloader, desc=\"Computing Beam Search...\")\n\t        for x in pbar:\n\t            input_ids = x[\"source\"][\"input_ids\"].to(self.device)\n\t            attention_mask = x[\"source\"][\"attention_mask\"].to(self.device)\n\t            tgt_text = x['target']['sentences']\n\t            if cfg.model.use_logits_preprocessor:\n\t                logits_preprocessor = get_logits_preprocessor(self.decoder, input_ids)\n\t            else:\n", "                logits_preprocessor = None\n\t            mean_beam, var_beam, beam_output = self._bench_time(input_ids, attention_mask)\n\t            mean_auto, var_auto, auto_output = self._auto_time(input_ids, attention_mask, logits_preprocessor)\n\t            translation_beam = self.tokenizer.batch_decode(\n\t                beam_output, skip_special_tokens=True\n\t            )\n\t            translation_auto = self.tokenizer.batch_decode(\n\t                auto_output, skip_special_tokens=True\n\t            )\n\t            beam_scorer.update_metrics(mean_beam, var_beam, 0, translation_beam[0], tgt_text[0])\n", "            auto_scorer.update_metrics(mean_auto, var_auto, 0, translation_auto[0], tgt_text[0])\n\t            worst_beam_translations.extend(\n\t                self._compute_tmp_bleu(\n\t                    translation_beam[0],\n\t                    translation_auto[0],\n\t                    tgt_text[0],\n\t                    beam_scorer.i\n\t                )\n\t            )\n\t        self.write_report(beam_scorer, auto_scorer, worst_beam_translations)\n", "    def write_report(self, beam_scorer, auto_scorer, worst_beam_translations):\n\t        print(\"Writing report...\")\n\t        beam_score = beam_scorer.compute_bleu_score()\n\t        auto_score = auto_scorer.compute_bleu_score()\n\t        worst_beam_translations.sort(key=lambda x: x[1])\n\t        # Table for the test info\n\t        info_table = tabulate([\n\t            ['Model', self.model_name],\n\t            ['Dataset', self.dataset.name],\n\t            ['Languages', f\"{self.dataset.src_lan}-{self.dataset.tgt_lan}\"],\n", "            ['Device', self.device],\n\t            ['Sentences', beam_scorer.i],\n\t            ['Num Beams', self.num_beams],\n\t            ['No Rep Ngram Size', self.no_repeat_ngram_size],\n\t            ['Early Stopping', self.early_stopping],\n\t            ['Do Sample', False],\n\t            ['Use Cache', True],\n\t        ], headers=['Info', 'Value'], tablefmt='grid')\n\t        # Table for the Time\n\t        time_table = tabulate([\n", "            ['Time', beam_scorer.tot_mean_time, auto_scorer.tot_mean_time, (auto_scorer.tot_mean_time / beam_scorer.tot_mean_time)],\n\t            ['Iter', beam_scorer.tot_mean_iter, auto_scorer.tot_mean_iter, 1],\n\t            ['Var', beam_scorer.tot_var_time, auto_scorer.tot_var_time, 1],\n\t        ], headers=['Metrics', 'Beam', 'Auto', 'Speedup'], tablefmt='grid')\n\t        # Table for the Bleu score\n\t        bleu_table = tabulate([\n\t            ['Score', beam_score.score, auto_score.score],\n\t            ['Counts', beam_score.counts, auto_score.counts],\n\t            ['Totals', beam_score.totals, auto_score.totals],\n\t            ['Precisions', beam_score.precisions, auto_score.precisions],\n", "            ['Bp', beam_score.bp, auto_score.bp],\n\t            ['Sys_len', beam_score.sys_len, auto_score.sys_len],\n\t            ['ref_len', beam_score.ref_len, auto_score.ref_len],\n\t        ], headers=['Metrics', 'Beam Search', 'Auto'], tablefmt='rst')\n\t        with open(os.path.join(self.exp_dir, \"report.txt\"), mode='w') as report:\n\t            report.write(f\"Test Info\\n{info_table}\\n\\n\")\n\t            report.write(f\"Time\\n{time_table}\\n\\n\")\n\t            report.write(f\"Bleu Score\\n{bleu_table}\\n\\n\")\n\t        with open(os.path.join(self.exp_dir, \"worst_beam_translation.csv\"), 'w') as csvfile:\n\t            writer = csv.writer(csvfile, delimiter='\\t')\n", "            writer.writerow(['i', 'bleu', 'beam', 'auto', 'tgt'])\n\t            for sample in worst_beam_translations:\n\t                writer.writerow(list(sample))\n\t        write_sentences(os.path.join(self.exp_dir, \"beam.txt\"), beam_scorer.predictions)\n\t        write_sentences(os.path.join(self.exp_dir, \"auto.txt\"), auto_scorer.predictions)\n\t        write_sentences(os.path.join(self.exp_dir, \"reference.txt\"), sum(auto_scorer.references, []))\n\t    def _compute_tmp_bleu(self, translation_beam, translation_auto, tgt_text, i):\n\t        beam_tmp_scorer, auto_tmp_scorer = Scorer(), Scorer()\n\t        beam_tmp_scorer.update_metrics(0, 0, 0, translation_beam, tgt_text)\n\t        auto_tmp_scorer.update_metrics(0, 0, 0, translation_auto, tgt_text)\n", "        beam_score = beam_tmp_scorer.compute_bleu_score()\n\t        auto_score = auto_tmp_scorer.compute_bleu_score()\n\t        if beam_score.score < auto_score.score:\n\t            return [(i, beam_score.score, translation_beam, translation_auto, tgt_text)]\n\t        else:\n\t            return []\n"]}
{"filename": "src/dataset/iwslt_dataset.py", "chunked_list": ["import os\n\timport typing as t\n\timport datasets\n\timport torch\n\tfrom datasets.utils.download_manager import DownloadManager\n\tfrom torch.utils.data.dataset import Dataset\n\tfrom src.utils.utils import clean_text\n\tclass Iwslt(Dataset):\n\t    def __init__(\n\t        self,\n", "        version: str = \"17\",\n\t        src_lan: str = \"en\",\n\t        tgt_lan: str = \"ro\",\n\t        data_dir: str = None,\n\t        hugginface_tokenizer=None,\n\t        split: str = None,\n\t    ):\n\t        self.version = version\n\t        self.src_lan = src_lan\n\t        self.tgt_lan = tgt_lan\n", "        self.max_length = 511\n\t        self.dl = DownloadManager()\n\t        self.name = f\"iwslt{self.version}\"\n\t        self.version2folder = {\n\t            \"15\": os.path.join(data_dir, \"2015-01/texts\"),\n\t            \"17\": os.path.join(data_dir, \"2017-01-trnted/texts\"),\n\t        }\n\t        self.version2years = {\n\t            \"15\": {\"train_and_test\": [2010, 2011, 2012, 2013], \"dev\": [2010]},\n\t            \"17\": {\n", "                \"train_and_test\": [2010, 2011, 2012, 2013, 2014, 2015],\n\t                \"dev\": [2010],\n\t            },\n\t        }\n\t        data_file = f\"{self.version2folder[version]}/{src_lan}/{tgt_lan}/{src_lan}-{tgt_lan}.tgz\"\n\t        splitted_generators = self._split_generators(data_file)\n\t        self.translation_dataset = self.load_dataset(splitted_generators, split=split)\n\t        with torch.no_grad():\n\t            self.tokenizer = hugginface_tokenizer\n\t    def load_dataset(\n", "        self,\n\t        splitted_generators: t.List[datasets.SplitGenerator],\n\t        split: str,\n\t    ) -> t.List[t.Dict]:\n\t        splitted_generators = self.concat_dataset(splitted_generators, split)\n\t        return list(\n\t            self._generate_examples(\n\t                source_files=splitted_generators.gen_kwargs[\"source_files\"],\n\t                target_files=splitted_generators.gen_kwargs[\"target_files\"],\n\t            )\n", "        )\n\t    @staticmethod\n\t    def concat_dataset(\n\t        splitted_generators: t.List[datasets.SplitGenerator],\n\t        split: str,\n\t    ) -> datasets.SplitGenerator:\n\t        split2ix = {\"train\": 0, \"test\": 1, \"validation\": 2}\n\t        assert (\n\t            split in split2ix\n\t        ), \"Iwslt: split must be either train or test on validation\"\n", "        if split is not None:\n\t            return splitted_generators[split2ix[split]]\n\t    def _split_generators(self, data_file: str) -> t.List[datasets.SplitGenerator]:\n\t        \"\"\"Returns SplitGenerators.\"\"\"\n\t        pair = f\"{self.src_lan}-{self.tgt_lan}\"\n\t        dl_dir = self.dl.extract(data_file)\n\t        data_dir = os.path.join(dl_dir, f\"{self.src_lan}-{self.tgt_lan}\")\n\t        years = self.version2years[self.version][\"train_and_test\"]\n\t        dev = self.version2years[self.version][\"dev\"]\n\t        return [\n", "            datasets.SplitGenerator(\n\t                name=datasets.Split.TRAIN,\n\t                # These kwargs will be passed to _generate_examples\n\t                gen_kwargs={\n\t                    \"source_files\": [\n\t                        os.path.join(\n\t                            data_dir,\n\t                            f\"train.tags.{pair}.{self.src_lan}\",\n\t                        )\n\t                    ],\n", "                    \"target_files\": [\n\t                        os.path.join(\n\t                            data_dir,\n\t                            f\"train.tags.{pair}.{self.tgt_lan}\",\n\t                        )\n\t                    ],\n\t                    \"split\": \"train\",\n\t                },\n\t            ),\n\t            datasets.SplitGenerator(\n", "                name=datasets.Split.TEST,\n\t                # These kwargs will be passed to _generate_examples\n\t                gen_kwargs={\n\t                    \"source_files\": [\n\t                        os.path.join(\n\t                            data_dir,\n\t                            f\"IWSLT{self.version}.TED.tst{year}.{pair}.{self.src_lan}.xml\",\n\t                        )\n\t                        for year in years\n\t                    ],\n", "                    \"target_files\": [\n\t                        os.path.join(\n\t                            data_dir,\n\t                            f\"IWSLT{self.version}.TED.tst{year}.{pair}.{self.tgt_lan}.xml\",\n\t                        )\n\t                        for year in years\n\t                    ],\n\t                    \"split\": \"test\",\n\t                },\n\t            ),\n", "            datasets.SplitGenerator(\n\t                name=datasets.Split.VALIDATION,\n\t                # These kwargs will be passed to _generate_examples\n\t                gen_kwargs={\n\t                    \"source_files\": [\n\t                        os.path.join(\n\t                            data_dir,\n\t                            f\"IWSLT{self.version}.TED.dev{year}.{pair}.{self.src_lan}.xml\",\n\t                        )\n\t                        for year in dev\n", "                    ],\n\t                    \"target_files\": [\n\t                        os.path.join(\n\t                            data_dir,\n\t                            f\"IWSLT{self.version}.TED.dev{year}.{pair}.{self.tgt_lan}.xml\",\n\t                        )\n\t                        for year in dev\n\t                    ],\n\t                    \"split\": \"validation\",\n\t                },\n", "            ),\n\t        ]\n\t    def _generate_examples(\n\t        self, source_files: t.List[str], target_files: t.List[str]\n\t    ) -> t.List[t.Dict]:\n\t        \"\"\"Yields examples.\"\"\"\n\t        for source_file, target_file in zip(source_files, target_files):\n\t            with open(source_file, \"r\", encoding=\"utf-8\") as sf:\n\t                with open(target_file, \"r\", encoding=\"utf-8\") as tf:\n\t                    for source_row, target_row in zip(sf, tf):\n", "                        source_row = source_row.strip()\n\t                        target_row = target_row.strip()\n\t                        if source_row.startswith(\"<\"):\n\t                            if source_row.startswith(\"<seg\"):\n\t                                # Remove <seg id=\"1\">.....</seg>\n\t                                # Very simple code instead of regex or xml parsing\n\t                                part1 = source_row.split(\">\")[1]\n\t                                source_row = part1.split(\"<\")[0]\n\t                                part1 = target_row.split(\">\")[1]\n\t                                target_row = part1.split(\"<\")[0]\n", "                                source_row = source_row.strip()\n\t                                target_row = target_row.strip()\n\t                            else:\n\t                                continue\n\t                        yield {\n\t                            \"translation\": {\n\t                                self.src_lan: source_row,\n\t                                self.tgt_lan: target_row,\n\t                            }\n\t                        }\n", "    def collate_fn(self, batch):\n\t        batch_source = [b[0] for b in batch]\n\t        batch_target = [b[1] for b in batch]\n\t        encoded_source = self.tokenizer(\n\t            batch_source,\n\t            padding=True,\n\t            return_tensors=\"pt\",\n\t        )\n\t        encoded_target = self.tokenizer(\n\t            batch_target,\n", "            padding=True,\n\t            return_tensors=\"pt\",\n\t        )\n\t        return {\n\t            \"source\": {\n\t                \"input_ids\": encoded_source[\"input_ids\"],\n\t                \"attention_mask\": encoded_source[\"attention_mask\"],\n\t                \"sentences\": batch_source,\n\t            },\n\t            \"target\": {\n", "                \"input_ids\": encoded_target[\"input_ids\"],\n\t                \"attention_mask\": encoded_target[\"attention_mask\"],\n\t                \"sentences\": batch_target,\n\t            },\n\t        }\n\t    def __len__(self) -> int:\n\t        return len(self.translation_dataset)\n\t    def __getitem__(self, idx: int) -> t.Tuple[str, str]:\n\t        sample = self.translation_dataset[idx]\n\t        source = sample[\"translation\"][self.src_lan]\n", "        target = sample[\"translation\"][self.tgt_lan]\n\t        return source, target\n"]}
{"filename": "src/dataset/wmt_dataset.py", "chunked_list": ["import typing as t\n\timport datasets\n\timport torch\n\tfrom torch.utils.data import Dataset\n\tfrom src.utils.utils import clean_text\n\tclass Wmt(Dataset):\n\t    \"\"\"\n\t    Wmt machine translation dataset reader\n\t    Input:\n\t        - version -> the dataset version dataset, by default '16' (dataset-16)\n", "        - src_lan -> the source language, by default 'ro' (Romanian)\n\t        - tgt_lan -> the target language, by default 'en' (English)\n\t        - tokenizer_model -> the tokenizer model\n\t        - split -> if not None, allows to split the dataset in following set: ['train', 'test', 'validation']\n\t        - concat -> if not None, make possible the concatenation of the specified set.\n\t                    Note: It works only if split is None\n\t                    It can be: ['train', 'test', 'validation']\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        version: str = \"16\",\n\t        src_lan: str = \"ro\",\n\t        tgt_lan: str = \"en\",\n\t        hugginface_tokenizer=None,\n\t        split: str = None,\n\t    ):\n\t        self.src_lan = src_lan\n\t        self.tgt_lan = tgt_lan\n\t        self.tokenizer_model = hugginface_tokenizer\n\t        self.max_length = 511\n", "        if src_lan == \"en\":\n\t            source2target = \"{}-{}\".format(self.tgt_lan, self.src_lan)\n\t        else:\n\t            source2target = \"{}-{}\".format(self.src_lan, self.tgt_lan)\n\t        if version == \"19\" and \"test\" in split:\n\t            split = \"validation\"\n\t        version = f\"wmt{version}\"\n\t        self.name = version\n\t        try:\n\t            self.translation_dataset = datasets.load_dataset(\n", "                version, source2target, split=split\n\t            )\n\t        except:\n\t            raise ValueError(\n\t                f\"{version} can read only the pairs cs-en, en-cs, de-en, en-de,\"\n\t                f\" fi-en, en-fi, ro-en, en-ro, ru-en, en-ru, tr-en, en-tr\"\n\t            )\n\t        with torch.no_grad():\n\t            self.tokenizer = hugginface_tokenizer\n\t    def collate_fn(self, batch):\n", "        batch_source = [b[0] for b in batch]\n\t        batch_target = [b[1] for b in batch]\n\t        encoded_source = self.tokenizer(\n\t            batch_source,\n\t            padding=True,\n\t            return_tensors=\"pt\",\n\t        )\n\t        encoded_target = self.tokenizer(\n\t            batch_target,\n\t            padding=True,\n", "            return_tensors=\"pt\",\n\t        )\n\t        return {\n\t            \"source\": {\n\t                \"input_ids\": encoded_source[\"input_ids\"],\n\t                \"attention_mask\": encoded_source[\"attention_mask\"],\n\t                \"sentences\": batch_source,\n\t            },\n\t            \"target\": {\n\t                \"input_ids\": encoded_target[\"input_ids\"],\n", "                \"attention_mask\": encoded_target[\"attention_mask\"],\n\t                \"sentences\": batch_target,\n\t            },\n\t        }\n\t    def __len__(self) -> int:\n\t        return len(self.translation_dataset)\n\t    def __getitem__(self, idx: int) -> t.Tuple[str, str]:\n\t        sample = self.translation_dataset[idx]\n\t        source = sample[\"translation\"][self.src_lan]\n\t        target = sample[\"translation\"][self.tgt_lan]\n", "        return source, target\n"]}
{"filename": "src/dataset/__init__.py", "chunked_list": []}
{"filename": "src/dataset/ittb_dataset.py", "chunked_list": ["import torch\n\tfrom datasets import load_dataset\n\tfrom torch.utils.data.dataset import Dataset\n\tfrom src.utils.utils import clean_text\n\tclass Ittb(Dataset):\n\t    def __init__(\n\t        self,\n\t        src_lan,\n\t        tgt_lan,\n\t        hugginface_tokenizer=None,\n", "        split: str = None,\n\t    ):\n\t        self.src_lan = src_lan\n\t        self.tgt_lan = tgt_lan\n\t        self.name = \"ittb\"\n\t        self.max_length = 511\n\t        assert (\n\t            src_lan == \"en\" or src_lan == \"hi\"\n\t        ), \"Ittb: src_lan must be either en or hi\"\n\t        assert (\n", "            tgt_lan == \"en\" or tgt_lan == \"hi\"\n\t        ), \"Ittb: tgt_lan must be either en or hi\"\n\t        assert src_lan != tgt_lan, \"Ittb: src_lan and tgt_lan cannot be the same\"\n\t        self.translation_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=split)\n\t        with torch.no_grad():\n\t            self.tokenizer = hugginface_tokenizer\n\t    def collate_fn(self, batch):\n\t        batch_source = [b[0] for b in batch]\n\t        batch_target = [b[1] for b in batch]\n\t        encoded_source = self.tokenizer(\n", "            batch_source,\n\t            padding=True,\n\t            return_tensors=\"pt\",\n\t        )\n\t        encoded_target = self.tokenizer(\n\t            batch_target,\n\t            padding=True,\n\t            return_tensors=\"pt\",\n\t        )\n\t        return {\n", "            \"source\": {\n\t                \"input_ids\": encoded_source[\"input_ids\"],\n\t                \"attention_mask\": encoded_source[\"attention_mask\"],\n\t                \"sentences\": batch_source,\n\t            },\n\t            \"target\": {\n\t                \"input_ids\": encoded_target[\"input_ids\"],\n\t                \"attention_mask\": encoded_target[\"attention_mask\"],\n\t                \"sentences\": batch_target,\n\t            },\n", "        }\n\t    def __len__(self):\n\t        return len(self.translation_dataset)\n\t    def __getitem__(self, idx: int):\n\t        source = self.translation_dataset[\"translation\"][idx][self.src_lan]\n\t        target = self.translation_dataset[\"translation\"][idx][self.tgt_lan]\n\t        return source, target\n"]}
{"filename": "src/dataset/flores_dataset.py", "chunked_list": ["import torch\n\tfrom torch.utils.data import Dataset\n\timport datasets\n\tfrom src.utils.utils import retrieve_map_languages_flores, clean_text\n\timport typing as t\n\tclass Flores(Dataset):\n\t    def __init__(\n\t        self,\n\t        src_lan: str = \"ro\",\n\t        tgt_lan: str = \"en\",\n", "        hugginface_tokenizer=None,\n\t        split: str = None,\n\t    ):\n\t        self.name = \"flores\"\n\t        self.max_length = 511\n\t        self.src_lan = retrieve_map_languages_flores(src_lan).lower()[:3]\n\t        self.tgt_lan = retrieve_map_languages_flores(tgt_lan).lower()[:3]\n\t        if \"test\" in split:\n\t            split = \"dev\" + split\n\t        self.translation_dataset_src = datasets.load_dataset(\n", "            \"gsarti/flores_101\", self.src_lan, split=split\n\t        )\n\t        self.translation_dataset_tgt = datasets.load_dataset(\n\t            \"gsarti/flores_101\", self.tgt_lan, split=split\n\t        )\n\t        with torch.no_grad():\n\t            self.tokenizer = hugginface_tokenizer\n\t    def collate_fn(self, batch):\n\t        batch_source = [b[0] for b in batch]\n\t        batch_target = [b[1] for b in batch]\n", "        encoded_source = self.tokenizer(\n\t            batch_source,\n\t            padding=True,\n\t            return_tensors=\"pt\",\n\t        )\n\t        encoded_target = self.tokenizer(\n\t            batch_target,\n\t            padding=True,\n\t            return_tensors=\"pt\",\n\t        )\n", "        return {\n\t            \"source\": {\n\t                \"input_ids\": encoded_source[\"input_ids\"],\n\t                \"attention_mask\": encoded_source[\"attention_mask\"],\n\t                \"sentences\": batch_source,\n\t            },\n\t            \"target\": {\n\t                \"input_ids\": encoded_target[\"input_ids\"],\n\t                \"attention_mask\": encoded_target[\"attention_mask\"],\n\t                \"sentences\": batch_target,\n", "            },\n\t        }\n\t    def __len__(self) -> int:\n\t        return self.translation_dataset_src.num_rows\n\t    def __getitem__(self, idx: int) -> t.Tuple[str, str]:\n\t        source = str(self.translation_dataset_src.data.column(6)[idx])\n\t        target = str(self.translation_dataset_tgt.data.column(6)[idx])\n\t        return source, target\n"]}
