{"filename": "qax/tracer.py", "chunked_list": ["import warnings\n\timport jax\n\tfrom jax.core import get_aval, Tracer, full_lower\n\tfrom .implicit_array import ImplicitArray\n\tclass ImplicitArrayTracer(Tracer):\n\t    def __init__(self, trace, value):\n\t        super().__init__(trace)\n\t        self.value = value\n\t    @property\n\t    def aval(self):\n", "        if isinstance(self.value, ImplicitArray):\n\t            return jax.ShapedArray(self.value.shape, self.value.dtype)\n\t        return get_aval(self.value)\n\t    def full_lower(self):\n\t        if isinstance(self.value, ImplicitArray):\n\t            return self\n\t        return full_lower(self.value)\n\tclass ImplicitArrayTrace(Trace):\n\t    pure = lift = lambda self, val: ImplicitArrayTracer(self, val)\n\t    def process_primitive(self, primitive, tracers, params):\n", "        vals = [t.value for t in tracers]\n\t        n_implicit = sum(isinstance(v, ImplicitArray) for v in vals)\n\t        assert 1 <= n_implicit <= 2\n\t        if n_implicit == 2:\n\t            warnings.warn(f'Encountered op {primitive.name} with two implicit inputs so second will be materialized.')\n\t            vals[1] = vals[1].materialize()\n"]}
{"filename": "qax/primitives.py", "chunked_list": ["from abc import ABC\n\tfrom itertools import count\n\timport jax\n\tfrom plum import Dispatcher, Function\n\tclass ArrayValue(ABC):\n\t    pass\n\tArrayValue.register(jax.Array)\n\t_dispatch = Dispatcher()\n\t_primitive_ids = count()\n\tdef get_lax_primitive_by_name(name):\n", "    return getattr(jax.lax, f'{name}_p')\n\tdef get_primitive_handler(primitive):\n\t    if isinstance(primitive, str):\n\t        primitive = get_lax_primitive_by_name(primitive)\n\t    handler = _dispatch.functions.get(primitive)\n\t    if handler is None:\n\t        def _not_impl_handler(primitive : jax.core.Primitive, *args, **kwargs):\n\t            return NotImplemented\n\t        _not_impl_handler.__doc__ = 'Default handler for {primitive.name}'\n\t        handler = Function(_not_impl_handler)\n", "        handler.register(_not_impl_handler, precedence=-1e9)\n\t        handler.__name__ = f'{primitive.name}_{next(_primitive_ids)}'\n\t        _dispatch.functions[primitive] = handler\n\t    return handler\n\tdef primitive_handler(primitives, precedence=0):\n\t    if isinstance(primitives, (str, jax.core.Primitive)):\n\t        primitives = [primitives]\n\t    def decorator(fn):\n\t        for primitive in primitives:\n\t            handler = get_primitive_handler(primitive)\n", "            handler.register(fn, precedence=precedence)\n\t    return decorator\n\tdef default_handler(primitive, *args, **params):\n\t    subfuns, bind_params = primitive.get_bind_params(params)\n\t    return primitive.bind(*subfuns, *args, **bind_params)\n"]}
{"filename": "qax/__init__.py", "chunked_list": ["from .implicit import implicit_array\n\tfrom .implicit.implicit_array import ImplicitArray, use_implicit_args, aux_field, UninitializedAval\n\tfrom .primitives import default_handler, primitive_handler, ArrayValue\n\tfrom .utils import EmptyNode, materialize_nested, freeze_keys\n\tfrom .common import type_utils\n\tfrom . import symbols\n"]}
{"filename": "qax/utils.py", "chunked_list": ["from .implicit.implicit_utils import *\n\tfrom .common.utils import *\n"]}
{"filename": "qax/constants.py", "chunked_list": ["# WARNING: This file is obviously super incomplete, and is\n\t# currently just for convenience in testing.\n\tCOMMUTATIVE_OPS = frozenset([\n\t    'add',\n\t    'bitwise_and',\n\t    'bitwise_or',\n\t    'bitwise_xor',\n\t    'eq',\n\t    'max',\n\t    'min',\n", "    'mul',\n\t    'ne',\n\t])\n\tELEMENTWISE_UNOPS = frozenset([\n\t    'abs',\n\t    'acos',\n\t    'acosh',\n\t    'asin',\n\t    'asinh',\n\t    'atan',\n", "    'atanh',\n\t    'bessel_i0e',\n\t    'bessel_i1e',\n\t    'cbrt',\n\t    'ceil',\n\t    'clz',\n\t    'conj',\n\t    'convert_element_type',\n\t    'copy',\n\t    'cos',\n", "    'cosh',\n\t    'digamma',\n\t    'erf_inv',\n\t    'erf',\n\t    'erfc',\n\t    'exp',\n\t    'expm1',\n\t    'floor',\n\t    'imag',\n\t    'integer_pow',\n", "    'is_finite',\n\t    'lgamma',\n\t    'log1p',\n\t    'log',\n\t    'logistic',\n\t    'neg',\n\t    'not',\n\t    'population_count',\n\t    'real',\n\t    'reduce_precision',\n", "    'round',\n\t    'rsqrt',\n\t    'sign',\n\t    'sin',\n\t    'sinh',\n\t    'sqrt',\n\t    'tan',\n\t    'tanh',\n\t])\n\tELEMENTWISE_BINOPS = frozenset([\n", "    'add',\n\t    'and',\n\t    'atan2',\n\t    'complex',\n\t    'div',\n\t    'eq',\n\t    'ge',\n\t    'gt',\n\t    'igamma_grad_a',\n\t    'igamma',\n", "    'igammac',\n\t    'le',\n\t    'lt',\n\t    'max',\n\t    'min',\n\t    'mul',\n\t    'ne',\n\t    'nextafter',\n\t    'or',\n\t    'pow',\n", "    'random_gamma_grad',\n\t    'rem',\n\t    'shift_left',\n\t    'shift_right_arithmetic',\n\t    'shift_right_logical',\n\t    'sub',\n\t    'xor',\n\t])\n\tREDUCTION_OPS = frozenset([\n\t    'argmax',\n", "    'argmin',\n\t    'reduce_and',\n\t    'reduce_max',\n\t    'reduce_min',\n\t    'reduce_or',\n\t    'reduce_prod',\n\t    'reduce_sum',\n\t    'reduce_xor',\n\t])\n\tCUMULATIVE_REDUCTION_OPS = frozenset([\n", "    'cumlogsumexp',\n\t    'cummax',\n\t    'cummin',\n\t    'cumprod',\n\t    'cumsum',\n\t])\n"]}
{"filename": "qax/symbols.py", "chunked_list": ["\"\"\"\n\tImplementation of SymbolicConstant, an ImplicitArray subclass representing jnp.full(shape, value, dtype).\n\tThe data is stored in pytree auxilary data, and all supported operations are run at compile time.\n\tThis is useful for forcing constant folding in cases where the JIT would not know that an argument\n\tis constant, or for lowering the cost of constant folding on large constant arrays.\n\tThese do not respect NaNs in certain ways, e.g. 0 * NaN = 0, max(inf, NaN) = inf\n\t\"\"\"\n\tfrom dataclasses import dataclass\n\tfrom functools import partial\n\tfrom typing import Any\n", "import jax\n\timport jax.numpy as jnp\n\timport numpy as np\n\timport optax\n\tfrom .implicit.implicit_array import ArrayValue, ImplicitArray, UninitializedAval, aux_field, use_implicit_args\n\tfrom .implicit import implicit_utils as iu\n\tfrom .primitives import get_primitive_handler, primitive_handler, default_handler\n\tfrom .common.type_utils import Complement\n\tfrom .constants import ELEMENTWISE_BINOPS, ELEMENTWISE_UNOPS\n\t_GENERAL = -2\n", "_SPECIALIZED = -1\n\tdef _get_shape_dtype(x, shape, dtype):\n\t    if shape is None:\n\t        shape = np.shape(x)\n\t    else:\n\t        shape = jax.core.canonicalize_shape(shape)\n\t    if dtype is None:\n\t        jax.lax.dtype(x)\n\t    return shape, dtype\n\tdef _out_shape_dtype(primitive, *args, **kwargs):\n", "    out_aval = jax.eval_shape(\n\t        partial(default_handler, primitive, **kwargs),\n\t        *(jax.core.get_aval(x) for x in args)\n\t    )\n\t    return jax.tree_map(\n\t        lambda x: (x.shape, x.dtype),\n\t        out_aval\n\t    )\n\tdef symbolic_zero_like(x, shape=None, dtype=None):\n\t    dtype = jax.lax.dtype(x) if dtype is None else dtype\n", "    return symbolic_full_like(x, 0, shape=shape, dtype=dtype)\n\tdef symbolic_full_like(x, fill_value, shape=None, dtype=None):\n\t    shape, _ = _get_shape_dtype(x, shape, None)\n\t    if dtype is None:\n\t        dtype = jax.lax.dtype(fill_value)\n\t    return SymbolicConstant(fill_value, shape=shape, dtype=dtype)\n\t@dataclass\n\tclass SymbolicConstant(ImplicitArray):\n\t    value : Any = aux_field()\n\t    weak_type : bool = aux_field(default=False)\n", "    def __post_init__(self):\n\t        super().__post_init__()\n\t        with jax.ensure_compile_time_eval():\n\t            self.value = jnp.asarray(self.value, dtype=self.dtype)\n\t    def compute_dtype(self):\n\t        return jax.lax.dtype(self.value)\n\t    def materialize(self):\n\t        return jnp.full(self.shape, self.value, dtype=self.dtype)\n\t    def copy(self):\n\t        return jax.tree_map(lambda x: x, self)\n", "@use_implicit_args\n\tdef broadcast_to(val, shape):\n\t    return jnp.broadcast_to(val, shape)\n\t@use_implicit_args\n\tdef astype(val, dtype):\n\t    return val.astype(dtype)\n\t@primitive_handler([\n\t    'reshape',\n\t    'broadcast_in_dim',\n\t    'reduce_min',\n", "    'reduce_max',\n\t    'reduce_or',\n\t    'reduce_and'\n\t])\n\tdef unchanged_value_op(primitive, sym : SymbolicConstant, **kwargs):\n\t    out_shape, out_dtype = _out_shape_dtype(primitive, sym, **kwargs)\n\t    return SymbolicConstant(sym.value, shape=out_shape, dtype=out_dtype)\n\tdef _op_and_reshape(primitive, lhs, rhs, flip=False):\n\t    \"\"\"\n\t    Close over one arg so we can do math at tracing time, but let the other one get traced\n", "    \"\"\"\n\t    if flip:\n\t        lhs, rhs = (rhs, lhs)\n\t    @use_implicit_args\n\t    def inner(arg):\n\t        other = rhs\n\t        if flip:\n\t            arg, other = (other, arg)\n\t        result = default_handler(primitive, arg, other)\n\t        return result\n", "    return inner(rhs)\n\tdef special_case_binop(name, identity=None, annihilator=None, flip=False):\n\t    lhs_type = SymbolicConstant\n\t    rhs_type = Complement[ArrayValue, SymbolicConstant]\n\t    if flip:\n\t        lhs_type, rhs_type = rhs_type, lhs_type\n\t    @primitive_handler(name, precedence=_SPECIALIZED)\n\t    def handler(primitive, lhs : lhs_type, rhs : rhs_type, **kwargs):\n\t        out_shape, out_dtype = _out_shape_dtype(primitive, lhs, rhs, **kwargs)\n\t        with jax.ensure_compile_time_eval():\n", "            if lhs.value == identity:\n\t                return broadcast_to(astype(rhs, out_dtype), out_shape)\n\t            if lhs.value == annihilator:\n\t                return SymbolicConstant(lhs.value, shape=out_shape, dtype=out_dtype)\n\t            return _op_and_reshape(primitive, lhs.value, rhs)\n\tspecial_case_binop('add', identity=0)\n\tspecial_case_binop('mul', identity=1, annihilator=0)\n\tspecial_case_binop('and', annihilator=0)\n\tspecial_case_binop('or', identity=0)\n\tspecial_case_binop('xor', identity=0)\n", "special_case_binop('sub', identity=0, flip=True)\n\tspecial_case_binop('div', identity=1, flip=True)\n\tspecial_case_binop('exp', identity=1, flip=True)\n\tspecial_case_binop('min', identity=float('inf'), annihilator=float('-inf'))\n\tspecial_case_binop('max', identity=float('-inf'), annihilator=float('inf'))\n\tdef eval_default_handler(primitive, *args, **kwargs):\n\t    with jax.ensure_compile_time_eval():\n\t        result = primitive.bind(*args, **kwargs)\n\t    return result\n\t@primitive_handler(ELEMENTWISE_UNOPS, precedence=_GENERAL)\n", "def handle_unop(primitive, sym : SymbolicConstant, **kwargs):\n\t    print(f'Handling {primitive} with {sym}')\n\t    new_val = eval_default_handler(primitive, sym.value, **kwargs)\n\t    return symbolic_full_like(sym, new_val)\n\t@primitive_handler(ELEMENTWISE_BINOPS, precedence=_GENERAL)\n\tdef handle_binop(primitive, lhs : SymbolicConstant, rhs : SymbolicConstant, **kwargs):\n\t    out_shape, out_dtype = _out_shape_dtype(primitive, lhs, rhs, **kwargs)\n\t    new_val = eval_default_handler(primitive, lhs.value, rhs.value, **kwargs)\n\t    return symbolic_full_like(lhs, new_val, shape=out_shape, dtype=out_dtype)\n\t@primitive_handler(['reduce_sum', 'reduce_prod'])\n", "def reduce_sum(primitive, sym : SymbolicConstant, *, axes):\n\t    out_shape, out_dtype = _out_shape_dtype(primitive, sym, axes=axes)\n\t    with jax.ensure_compile_time_eval():\n\t        if sym.value == 0:\n\t            return SymbolicConstant(0, shape=out_shape, dtype=out_dtype)\n\t        orig_size = np.prod(sym.shape)\n\t        new_size = np.prod(out_shape)\n\t        n_combined = orig_size // new_size\n\t        new_val = sym.value\n\t        if primitive.name == 'reduce_sum':\n", "            new_val = new_val * n_combined\n\t        else:\n\t            new_val = new_val ** n_combined\n\t    return SymbolicConstant(new_val, shape=out_shape, dtype=out_dtype)\n\t@primitive_handler('select_n')\n\tdef handle_select_n(primitive, cond_val, *arg_vals : SymbolicConstant):\n\t    if len(set(val.value.item() for val in arg_vals)) != 1:\n\t        return NotImplemented\n\t    out_shape, out_dtype = _out_shape_dtype(primitive, cond_val, *arg_vals)\n\t    return SymbolicConstant(arg_vals[0].value, shape=out_shape, dtype=out_dtype)\n"]}
{"filename": "qax/common/__init__.py", "chunked_list": []}
{"filename": "qax/common/utils.py", "chunked_list": ["from functools import wraps\n\timport jax\n\timport jax.numpy as jnp\n\tfrom jax import tree_util\n\tfrom jax.dtypes import float0\n\timport optax\n\tfrom ..implicit.implicit_array import use_implicit_args\n\tfrom ..symbols import SymbolicConstant\n\tdef vmap_all_but_one(f, axis, out_ndim=0):\n\t    \"\"\"\n", "    Repeatedly calls vmap to map over all axes except for `axis.`\n\t    All args will be mapped on the same dimensions.\n\t    \"\"\"\n\t    @wraps(f)\n\t    def inner(*args):\n\t        n_dim = args[0].ndim\n\t        if axis >= n_dim:\n\t            raise ValueError(f'Axis {axis} is out of bounds for array of dimension {n_dim}')\n\t        fn = f\n\t        vmap_dim = 1\n", "        out_dim = out_ndim\n\t        for i in reversed(range(n_dim)):\n\t            if i == axis:\n\t                vmap_dim = 0\n\t                out_dim = 0\n\t            else:\n\t                fn = jax.vmap(fn, vmap_dim, out_dim)\n\t        return fn(*args)\n\t    return inner\n\tdef freeze_subtrees(optimizer : optax.GradientTransformation, label_fn, use_scalar_zeros=False):\n", "    \"\"\"\n\t    Utility which wraps an optimizer such that subtrees specified by\n\t    label_fn will receive zeros as updates.\n\t    Subtrees to be frozen should be labeled with \"freeze\"\n\t    and all other subtrees should be labeled with \"train\"\n\t    \"\"\"\n\t    multi_transformed_optimizer = optax.multi_transform(\n\t        {\n\t            'freeze': set_to_zero_scalar() if use_scalar_zeros else optax.set_to_zero(),\n\t            'train': optimizer\n", "        },\n\t        label_fn\n\t    )\n\t    def new_update(grads, opt_state, params):\n\t        def map_float0(param, grad):\n\t            if grad.dtype == float0:\n\t                return jnp.zeros((), param.dtype) if use_scalar_zeros else jnp.zeros_like(param)\n\t            return grad\n\t        fixed_grads = jax.tree_map(map_float0, params, grads)\n\t        return multi_transformed_optimizer.update(fixed_grads, opt_state, params)\n", "    return optax.GradientTransformation(\n\t        multi_transformed_optimizer.init,\n\t        new_update\n\t    )\n\tdef freeze_keys(optimizer : optax.GradientTransformation, arr_type, keys, use_scalar_zeros=False) -> optax.GradientTransformation:\n\t    keys = set(keys)\n\t    def label_leaf(leaf):\n\t        if not isinstance(leaf, arr_type):\n\t            return 'train'\n\t        children, aux_data = leaf.tree_flatten_with_keys()\n", "        labels = ['freeze' if key in keys else 'train' for key, _ in children]\n\t        struct = leaf.tree_unflatten(aux_data, labels)\n\t        return struct\n\t    def label_fn(root):\n\t        return jax.tree_map(label_leaf, root, is_leaf=lambda x: isinstance(x, arr_type))\n\t    return freeze_subtrees(optimizer, label_fn, use_scalar_zeros=use_scalar_zeros)\n\tdef apply_updates(params : optax.Params, updates : optax.Updates) -> optax.Params:\n\t    \"\"\"\n\t    Like optax.apply_updates, but updates can be SymbolicConstant instances\n\t    \"\"\"\n", "    updates_flat, update_struct = tree_util.tree_flatten(updates, is_leaf=lambda x: isinstance(x, SymbolicConstant))\n\t    semi_flat_params = update_struct.flatten_up_to(params)\n\t    updated_flat = use_implicit_args(optax.apply_updates)(semi_flat_params, updates_flat)\n\t    updated = update_struct.unflatten(updated_flat)\n\t    return updated\n\tdef set_to_zero_scalar() -> optax.GradientTransformation:\n\t    \"\"\"\n\t    Returns a gradient transformation that sets all gradients to 0 in order to\n\t    make downstream constant folding cheaper.\n\t    \"\"\"\n", "    def init_fn(params):\n\t        del params\n\t        return optax.EmptyState()\n\t    def update_fn(updates, state, params=None):\n\t        return jax.tree_map(lambda x: jnp.zeros((), x.dtype), updates), state\n\t    return optax.GradientTransformation(init_fn, update_fn)\n"]}
{"filename": "qax/common/type_utils.py", "chunked_list": ["from typing import Any, Optional, Tuple\n\tfrom beartype.vale import IsInstance\n\tfrom plum import dispatch, parametric\n\tclass _ComplementMeta(type):\n\t    def __instancecheck__(self, x):\n\t        a, b = self.type_parameter\n\t        return (\n\t            a is None or (\n\t                isinstance(x, a) and not isinstance(x, b)\n\t            )\n", "        )\n\t@parametric\n\tclass Complement(metaclass=_ComplementMeta):\n\t    \"\"\"\n\t    Relative complement\n\t    I.e. Complement[A, B] = A - B\n\t    \"\"\"\n\t    @classmethod\n\t    @dispatch\n\t    def __init_type_parameter__(\n", "        cls,\n\t        a: Optional[Any],\n\t        b: Optional[Any],\n\t    ):\n\t        return a, b\n\t    @classmethod\n\t    @dispatch\n\t    def __le_type_parameter__(\n\t        cls,\n\t        left: Tuple[Optional[Any], Optional[Any]],\n", "        right: Tuple[Optional[Any], Optional[Any]],\n\t    ):\n\t        a_left, b_left = left\n\t        a_right, b_right = right\n\t        return issubclass(a_left, a_right) and issubclass(b_right, b_left)\n"]}
{"filename": "qax/implicit/implicit_array.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom contextlib import contextmanager\n\tfrom contextvars import ContextVar\n\tfrom dataclasses import dataclass, field, fields, is_dataclass\n\tfrom functools import partial, wraps\n\tfrom itertools import chain\n\tfrom typing import ClassVar, Optional\n\timport warnings\n\timport jax\n\tfrom jax.api_util import flatten_fun, flatten_fun_nokwargs\n", "from jax import core\n\timport jax.linear_util as lu\n\timport jax.interpreters.partial_eval as pe\n\tfrom jax.tree_util import register_pytree_with_keys_class\n\timport jax.numpy as jnp\n\tfrom jax._src.typing import DTypeLike, Shape\n\tfrom .. import constants\n\tfrom ..primitives import ArrayValue, get_primitive_handler\n\tfrom . import implicit_utils as iu\n\tdef _with_implicit_flat(fun: lu.WrappedFun) -> lu.WrappedFun:\n", "  # Splitting to avoid leaks based on https://github.com/google/jax/blob/0dffdf4645db7bf7a9fadd4bcfe9ec0368a8ecb9/jax/_src/interpreters/batching.py#L539\n\t    f = _implicit_inner(fun)\n\t    return _implicit_outer(f)\n\t@lu.transformation\n\tdef _implicit_outer(*in_vals):\n\t    with core.new_main(ImplicitArrayTrace) as main:\n\t        outs = yield (main, *in_vals), {}\n\t        del main\n\t    yield outs\n\t@lu.transformation\n", "def _implicit_inner(main, *in_vals):\n\t    trace = main.with_cur_sublevel()\n\t    in_tracers = [\n\t        ImplicitArrayTracer(trace, val) if isinstance(val, ImplicitArray) else val\n\t        for val in in_vals\n\t    ]\n\t    outs = yield in_tracers, {}\n\t    out_vals = [trace.full_raise(t).value for t in outs]\n\t    yield out_vals\n\tdef use_implicit_args(f):\n", "    \"\"\"\n\t    Decorator which allows a function to accept arguments which subclass ImplicitArray, possibly\n\t    including further ImplicitArray instances as children.\n\t    Any number of arguments (including 0) may be ImplicitArrays.\n\t    \"\"\"\n\t    @wraps(f)\n\t    def implicit_f(*args, **kwargs):\n\t        flat_args, in_tree = iu.tree_flatten_with_implicit((args, kwargs))\n\t        f_flat, out_tree = flatten_fun(lu.wrap_init(f), in_tree)\n\t        f_wrapped = _with_implicit_flat(f_flat)\n", "        outs_flat = f_wrapped.call_wrapped(*flat_args)\n\t        return out_tree().unflatten(outs_flat)\n\t    return implicit_f\n\tdef aux_field(metadata=None, **kwargs):\n\t    metadata = dict(metadata) if metadata else {}\n\t    metadata['implicit_array_aux'] = True\n\t    return field(metadata=metadata, **kwargs)\n\tclass UninitializedAval(Exception):\n\t    def __init__(self, kind):\n\t        super().__init__(_AVAL_ERROR_MESSAGE.format(kind))\n", "# This descriptor and the below context manager support discovering the aval\n\t# of an ImplicitArray. We don't want to throw an error just because a shape\n\t# wasn't passed, since it may be possible to infer it via materialization\n\tclass _AvalDescriptor:\n\t    def __set_name__(self, owner, name):\n\t        self._name = f'_{name}'\n\t    def __get__(self, obj, owner=None):\n\t        if obj is None:\n\t            return None\n\t        result = getattr(obj, self._name, None)\n", "        if result is None:\n\t            raise UninitializedAval(kind=self._name[1:])\n\t        return result\n\t    def __set__(self, obj, value):\n\t        setattr(obj, self._name, value)\n\t# Context manager used for disabling UninitializedAval errors\n\t# during tree flattening only\n\t_aval_discovery = ContextVar('aval_discovery', default=False)\n\t@contextmanager\n\tdef _aval_discovery_context():\n", "    token = _aval_discovery.set(True)\n\t    try:\n\t        yield\n\t    finally:\n\t        _aval_discovery.reset(token)\n\t@dataclass\n\tclass _ImplicitArrayBase(ArrayValue,ABC):\n\t    commute_ops : ClassVar[bool] = True\n\t    default_shape : ClassVar[Optional[Shape]] = None\n\t    default_dtype : ClassVar[Optional[DTypeLike]] = None\n", "    shape : Optional[Shape] = aux_field(kw_only=True, default=None)\n\t    dtype : DTypeLike = aux_field(kw_only=True, default=None)\n\t@dataclass\n\tclass ImplicitArray(_ImplicitArrayBase):\n\t    \"\"\"\n\t    Abstract class for representing an abstract array of a given shape/dtype without actually instantiating it.\n\t    Subclasses must implement the materialize method, which defines the relationship between the implicit array\n\t    and the value it represents. Subclasses are valid arguments to functions decorated with qax.use_implicit_args.\n\t    All subclasses are automatically registered as pytrees using jax.tree_util.register_pytree_with_keys_class.\n\t    Any dataclass attributes added will be included as children, unless they are decorated with qax.aux_field\n", "    in which case they are passed as auxiliary data during flattening.\n\t    The represented shape and dtype may be defined in any of the following ways:\n\t        - Explicitly passing shape/dtype keyword arguments at initialization\n\t        - Overriding the default_shape/default_dtype class variables\n\t        - Overriding the compute_shape/compute_dtype methods, which are called during __post_init__\n\t        - Overriding __post_init__ and manually setting shape/dtype before calling super().__post_init__\n\t        - None of the above, in which case an shape/dtype will be inferred by by running jax.eval_shape()\n\t          on the subclass's materialize method.\n\t    \"\"\"\n\t    shape = _AvalDescriptor()\n", "    dtype = _AvalDescriptor()\n\t    def __post_init__(self):\n\t        try:\n\t            aval = _get_materialization_aval(self)\n\t        except UninitializedAval:\n\t            # Materialization depends on currently uninitialized shape/dtype\n\t            aval = None\n\t        shape = None\n\t        try:\n\t            shape = self.shape\n", "        except UninitializedAval as e:\n\t            shape = self.shape = self.compute_shape()\n\t        if aval is not None:\n\t            if shape is None:\n\t                self.shape = aval.shape\n\t            elif shape != aval.shape:\n\t                warnings.warn(f'ImplicitArray shape {shape} does not match materialization shape {aval.shape}')\n\t        elif shape is None:\n\t            raise UninitializedAval('shape')\n\t        dtype = None\n", "        try:\n\t            dtype = self.dtype\n\t        except UninitializedAval as e:\n\t            dtype = self.dtype = self.compute_dtype()\n\t        if dtype is None and aval is None:\n\t            # We have a shape but not a dtype, try once again to infer the dtype\n\t            aval = _get_materialization_aval(self)\n\t        if aval is not None:\n\t            if dtype is None:\n\t                self.dtype = aval.dtype\n", "            elif dtype != aval.dtype:\n\t                warnings.warn(f'ImplicitArray dtype {dtype} does not match materialization dtype {aval.dtype}')\n\t        elif dtype is None:\n\t            raise UninitializedAval('dtype')\n\t    def compute_shape(self):\n\t        \"\"\"\n\t        Override this method if the subclass instance's shape should be computed based on its other properties.\n\t        Returns: shape\n\t        \"\"\"\n\t        return self.default_shape\n", "    def compute_dtype(self):\n\t        \"\"\"\n\t        Override this method if the subclass instance's dtype should be computed based on its other properties.\n\t        Returns: dtype\n\t        \"\"\"\n\t        return self.default_dtype\n\t    @property\n\t    def aval(self):\n\t        return core.ShapedArray(self.shape, self.dtype)\n\t    @classmethod\n", "    def default_handler(cls, primitive, *args, params=None):\n\t        if params is None:\n\t            params = {}\n\t        return materialize_handler(primitive, *args, params=params)\n\t    @abstractmethod\n\t    def materialize(self):\n\t        pass\n\t    def tree_flatten_with_keys(self):\n\t        children = []\n\t        aux_data = []\n", "        for name, is_aux in _get_names_and_aux(self):\n\t            try:\n\t                value = getattr(self, name)\n\t            except UninitializedAval:\n\t                if not _aval_discovery.get():\n\t                    raise\n\t                value = None\n\t            if is_aux:\n\t                aux_data.append(value)\n\t            else:\n", "                children.append((name, value))\n\t        return children, aux_data\n\t    @classmethod\n\t    def tree_unflatten(cls, aux_data, children):\n\t        child_it = iter(children)\n\t        aux_it = iter(aux_data)\n\t        obj = cls.__new__(cls)\n\t        for name, is_aux in _get_names_and_aux(cls):\n\t            value = next(aux_it if is_aux else child_it)\n\t            setattr(obj, name, value)\n", "        return obj\n\t    def handle_primitive(self, primitive, *args, params):\n\t        handler = lu.wrap_init(partial(get_primitive_handler(primitive), primitive))\n\t        use_params = params\n\t        if len(args) == 2 and self.commute_ops:\n\t            args, use_params = _maybe_swap_args(primitive.name, args, use_params)\n\t        #maybe_kwargs = {'params': params} if params else {}\n\t        flat_args, in_tree = iu.flatten_one_implicit_layer((args, params))\n\t        flat_handler, out_tree = flatten_fun(handler, in_tree)\n\t        result = use_implicit_args(flat_handler.call_wrapped)(*flat_args)\n", "        return jax.tree_util.tree_unflatten(out_tree(), result)\n\t    def __init_subclass__(cls, commute_ops=True, **kwargs):\n\t        super().__init_subclass__(**kwargs)\n\t        if not is_dataclass(cls):\n\t            raise TypeError(f'{cls.__name__} must be a dataclass')\n\t        core.pytype_aval_mappings[cls] = lambda x: x.aval\n\t        register_pytree_with_keys_class(cls)\n\t        return cls\n\tdef _get_names_and_aux(obj):\n\t    for val in fields(obj):\n", "        yield val.name, bool(val.metadata.get('implicit_array_aux'))\n\tdef _materialize_all(it):\n\t    return [iu.materialize_nested(val) if isinstance(val, ImplicitArray) else val for val in it]\n\tdef _maybe_swap_args(op_name, args, params):\n\t    if isinstance(args[0], ImplicitArray):\n\t        return args, params\n\t    if op_name in constants.COMMUTATIVE_OPS:\n\t        return args[::-1], params\n\t    return args, params\n\tclass ImplicitArrayTracer(core.Tracer):\n", "    def __init__(self, trace, value):\n\t        super().__init__(trace)\n\t        self.value = value\n\t    @property\n\t    def aval(self):\n\t        if isinstance(self.value, ImplicitArray):\n\t            return self.value.aval\n\t        return core.get_aval(self.value)\n\t    def full_lower(self):\n\t        if isinstance(self.value, ImplicitArray):\n", "            return self\n\t        return core.full_lower(self.value)\n\tclass ImplicitArrayTrace(core.Trace):\n\t    pure = lift = lambda self, val: ImplicitArrayTracer(self, val)\n\t    def process_primitive(self, primitive, tracers, params):\n\t        outs = NotImplemented\n\t        vals = [t.value for t in tracers]\n\t        implicit_idx = next(i for i, v in enumerate(vals) if isinstance(v, ImplicitArray))\n\t        # First try to handle the primitive using custom handlers\n\t        outs = vals[implicit_idx].handle_primitive(primitive, *vals, params=params)\n", "        if outs is NotImplemented:\n\t            # For higher order primitives most users won't implement custom\n\t            # logic, so there shouldn't be a warning\n\t            if primitive.name in _default_handlers:\n\t                outs = _default_handlers[primitive.name](primitive, *vals, params=params)\n\t            else:\n\t                warnings.warn(f'Primitive {primitive.name} was not handled by class {vals[implicit_idx].__class__.__name__}, so implicit args will be materialized.')\n\t        if outs is NotImplemented:\n\t            outs = vals[implicit_idx].default_handler(primitive, *vals, params=params)\n\t        if primitive.multiple_results:\n", "            return [ImplicitArrayTracer(self, out) for out in outs]\n\t        return ImplicitArrayTracer(self, outs)\n\tdef wrap_jaxpr(jaxpr, vals_with_implicits, return_closed=True):\n\t    if isinstance(jaxpr, jax.core.ClosedJaxpr):\n\t        literals = jaxpr.literals\n\t        jaxpr = jaxpr.jaxpr\n\t    else:\n\t        literals = []\n\t    wrapped_fn = lu.wrap_init(use_implicit_args(partial(core.eval_jaxpr, jaxpr)))\n\t    flat_args, in_tree = jax.tree_util.tree_flatten((literals, *vals_with_implicits))\n", "    flat_fn, out_tree = flatten_fun_nokwargs(wrapped_fn, in_tree)\n\t    new_jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(flat_fn, [core.get_aval(v) for v in flat_args])\n\t    ret = (jax.core.ClosedJaxpr(new_jaxpr, consts),) if return_closed else (new_jaxpr, consts)\n\t    return *ret, flat_args, out_tree()\n\tdef _transform_jaxpr_output(jaxpr, jaxpr_args, orig_out_struct, out_transform):\n\t    def eval_fn(literals, *args):\n\t        output = use_implicit_args(partial(core.eval_jaxpr, jaxpr.jaxpr))(literals, *args)\n\t        unflattened_output = orig_out_struct.unflatten(output)\n\t        return out_transform(unflattened_output)\n\t    wrapped = lu.wrap_init(eval_fn)\n", "    flat_args, in_tree = jax.tree_util.tree_flatten((jaxpr.literals, *jaxpr_args))\n\t    flat_fn, out_tree = flatten_fun_nokwargs(wrapped, in_tree)\n\t    new_jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(flat_fn, [core.get_aval(v) for v in flat_args])\n\t    return jax.core.ClosedJaxpr(new_jaxpr, consts), out_tree()\n\tdef _match_branches(branches, arg_vals):\n\t    out_avals = []\n\t    new_jaxprs = []\n\t    flat_inputs = None\n\t    branch_out_struct = None\n\t    for branch in branches:\n", "        new_jaxpr, flat_inputs, branch_out_struct = wrap_jaxpr(branch, arg_vals)\n\t        new_jaxprs.append((new_jaxpr, branch_out_struct))\n\t        out_avals.append(\n\t            branch_out_struct.unflatten(\n\t                jax.eval_shape(\n\t                    partial(core.eval_jaxpr, new_jaxpr.jaxpr), new_jaxpr.literals, *flat_inputs\n\t                )\n\t            )\n\t        )\n\t    out_transforms = iu.get_common_prefix_transforms(out_avals)\n", "    new_branches = []\n\t    out_struct = None\n\t    for (new_jaxpr, orig_out_struct), transform in zip(new_jaxprs, out_transforms):\n\t        new_jaxpr, out_struct = _transform_jaxpr_output(new_jaxpr, flat_inputs, orig_out_struct, transform)\n\t        new_branches.append(new_jaxpr)\n\t    return tuple(new_branches), out_struct, flat_inputs\n\tdef _handle_cond(primitive, *vals, params):\n\t    cond_val, *arg_vals = vals\n\t    subfuns, bind_params = primitive.get_bind_params(params)\n\t    new_branches, out_struct, flat_inputs = _match_branches(params['branches'], arg_vals)\n", "    bind_params['branches'] = new_branches\n\t    bind_params['linear'] = _broadcast_tuple(params['linear'], arg_vals)\n\t    outs = primitive.bind(*subfuns, cond_val, *flat_inputs, **bind_params)\n\t    return jax.tree_util.tree_unflatten(out_struct, outs)\n\tdef _handle_remat2(primitive, *vals, params):\n\t    subfuns, bind_params = primitive.get_bind_params(params)\n\t    new_jaxpr, consts, flat_inputs, out_tree = wrap_jaxpr(bind_params['jaxpr'], vals, return_closed=False)\n\t    new_jaxpr = pe.convert_constvars_jaxpr(new_jaxpr)\n\t    bind_params['jaxpr'] = new_jaxpr\n\t    outs = primitive.bind(*subfuns, *consts, *flat_inputs, **bind_params)\n", "    return jax.tree_util.tree_unflatten(out_tree, outs)\n\tdef _handle_pjit(primitive, *vals, params):\n\t    new_jaxpr, flat_inputs, out_tree = wrap_jaxpr(params['jaxpr'], vals)\n\t    donated_invars = _broadcast_tuple(params['donated_invars'], vals)\n\t    in_shardings = _broadcast_tuple(params['in_shardings'], vals)\n\t    out_shardings = _broadcast_tuple(params['out_shardings'], out_tree)\n\t    subfuns, bind_params = primitive.get_bind_params(params)\n\t    bind_params['jaxpr'] = new_jaxpr\n\t    bind_params['donated_invars'] = donated_invars\n\t    bind_params['in_shardings'] = in_shardings\n", "    bind_params['out_shardings'] = out_shardings\n\t    outs = primitive.bind(*subfuns, *flat_inputs, **bind_params)\n\t    return jax.tree_util.tree_unflatten(out_tree, outs)\n\t_default_handlers = {\n\t    'cond': _handle_cond,\n\t    'remat2': _handle_remat2,\n\t    'pjit': _handle_pjit,\n\t}\n\tdef materialize_handler(primitive, *vals, params):\n\t    vals = _materialize_all(vals)\n", "    subfuns, bind_params = primitive.get_bind_params(params)\n\t    result = use_implicit_args(primitive.bind)(*subfuns, *vals, **bind_params)\n\t    return result\n\tdef _broadcast_tuple(t, trees):\n\t    if isinstance(trees, jax.tree_util.PyTreeDef):\n\t        trees = jax.tree_util.tree_unflatten(trees, range(trees.num_leaves))\n\t    assert len(t) == len(trees)\n\t    return tuple(chain.from_iterable(\n\t        (tuple_val for _ in jax.tree_util.tree_leaves(tree))\n\t        for tuple_val, tree in zip(t, trees)\n", "    ))\n\tdef _get_materialization_aval(imp_arr):\n\t    with _aval_discovery_context(), _filter_materialization_warnings():\n\t        result = jax.eval_shape(\n\t            partial(iu.materialize_nested, full=True),\n\t            imp_arr\n\t        )\n\t    return result\n\t@contextmanager\n\tdef _filter_materialization_warnings():\n", "    with warnings.catch_warnings():\n\t        warnings.filterwarnings('ignore', message='Primitive.*was not handled')\n\t        yield\n\t_AVAL_ERROR_MESSAGE = (\n\t    '{} was not set during initialization. Shape and dtype may be set by:'\n\t    '\\n\\t1. Directly passing them as keyword arguments to ImplicitArray instances'\n\t    '\\n\\t2. Overriding the default_shape/default_dtype class attributes'\n\t    '\\n\\t3. Overriding the compute_shape/compute_dtype methods'\n\t    '\\n\\t4. Overriding __post_init__ and setting their values there'\n\t    '\\n\\t5. None of the above, in which case `materialize()` will be called in an attempt to infer them.'\n", "    ' If their values are required in order to compute the materialization this will be unsuccessful.'\n\t)\n"]}
{"filename": "qax/implicit/implicit_utils.py", "chunked_list": ["from functools import partial, wraps\n\tfrom itertools import chain\n\timport jax\n\tfrom jax.api_util import flatten_fun_nokwargs\n\tfrom jax import core\n\timport jax.linear_util as lu\n\tfrom jax import tree_util\n\tfrom . import implicit_array as ia\n\tclass _EmptyNodeCls:\n\t    _instance = None\n", "    def __new__(cls):\n\t        if cls._instance is None:\n\t            cls._instance = super().__new__(cls)\n\t        return cls._instance\n\tEmptyNode = _EmptyNodeCls()\n\ttree_util.register_pytree_node(\n\t    _EmptyNodeCls,\n\t    lambda node: ((), None),\n\t    lambda _, __: EmptyNode\n\t)\n", "def combine_leaf_predicate(base_fn, is_leaf):\n\t    @wraps(base_fn)\n\t    def new_fn(*args, new_is_leaf=None):\n\t        if new_is_leaf is None:\n\t            combined_is_leaf = is_leaf\n\t        else:\n\t            def combined_is_leaf(arg):\n\t                return is_leaf(arg) or new_is_leaf(arg)\n\t        return base_fn(*args, is_leaf=combined_is_leaf)\n\t    return new_fn\n", "def leaf_predicate(x):\n\t    return isinstance(x, (ia.ImplicitArray, _EmptyNodeCls))\n\ttree_map_with_implicit = combine_leaf_predicate(jax.tree_map, leaf_predicate)\n\ttree_map_with_path_with_implicit = combine_leaf_predicate(tree_util.tree_map_with_path, leaf_predicate)\n\ttree_flatten_with_implicit = combine_leaf_predicate(tree_util.tree_flatten, leaf_predicate)\n\ttree_flatten_with_path_with_implicit = combine_leaf_predicate(tree_util.tree_flatten_with_path, leaf_predicate)\n\ttree_leaves_with_implicit = combine_leaf_predicate(tree_util.tree_leaves, leaf_predicate)\n\ttree_structure_with_implicit = combine_leaf_predicate(tree_util.tree_structure, leaf_predicate)\n\tdef flatten_one_implicit_layer(tree):\n\t    def is_leaf_below_node(node, x):\n", "        return isinstance(x, ia.ImplicitArray) and x is not node\n\t    def replace_subtree_implicits(node):\n\t        return tree_util.tree_map(lambda _: 1, node, is_leaf=partial(is_leaf_below_node, node))\n\t    prototype = tree_map_with_implicit(replace_subtree_implicits, tree)\n\t    struct = tree_util.tree_structure(prototype)\n\t    leaves = tree_leaves_with_implicit(tree)\n\t    leaves = list(chain.from_iterable(\n\t        tree_util.tree_leaves(leaf, is_leaf=partial(is_leaf_below_node, leaf))\n\t        if isinstance(leaf, ia.ImplicitArray) else\n\t        [leaf] for leaf in leaves\n", "    ))\n\t    return leaves, struct\n\tdef implicit_depth(tree):\n\t    leaves = tree_leaves_with_implicit(tree)\n\t    depth = 0\n\t    while True:\n\t        next_leaves = []\n\t        any_implicit = False\n\t        for leaf in leaves:\n\t            if not isinstance(leaf, ia.ImplicitArray):\n", "                continue\n\t            any_implicit = True\n\t            next_leaves.extend(flatten_one_implicit_layer(leaf)[0])\n\t        if not any_implicit:\n\t            return depth\n\t        depth += 1\n\t        leaves = next_leaves\n\tdef _map_leaves_with_implicit_path(f, leaves, is_leaf, path_prefix=()):\n\t    mapped_leaves = []\n\t    for idx, leaf in enumerate(leaves):\n", "        path = path_prefix + (idx,)\n\t        if not isinstance(leaf, ia.ImplicitArray) or is_leaf(path, leaf):\n\t            mapped_leaves.append(f(leaf))\n\t            continue\n\t        subtree, substruct = flatten_one_implicit_layer(leaf)\n\t        mapped_subtree = _map_leaves_with_implicit_path(\n\t            f,\n\t            subtree,\n\t            is_leaf=is_leaf,\n\t            path_prefix=path\n", "        )\n\t        mapped_leaves.append(tree_util.tree_unflatten(substruct, mapped_subtree))\n\t    return mapped_leaves\n\tdef _get_pruning_transform(tree, materialization_paths):\n\t    if not materialization_paths:\n\t        return lambda x: x\n\t    def is_leaf(path, leaf):\n\t        return path in materialization_paths\n\t    def materialize_subtrees(tree):\n\t        leaves, struct = tree_flatten_with_implicit(tree)\n", "        mapped_leaves =  _map_leaves_with_implicit_path(partial(materialize_nested, full=True), leaves, is_leaf)\n\t        return tree_util.tree_unflatten(struct, mapped_leaves)\n\t    return materialize_subtrees\n\tdef get_common_prefix_transforms(trees):\n\t    \"\"\"\n\t    Given an iterable of pytrees which have the same structure after all\n\t    ImplicitArray instances are materialized, return a list of callables\n\t    which will transform each tree into the largest common structure\n\t    obtainable via materialization of ImplicitArrays.\n\t    \"\"\"\n", "    if len(trees) <= 1:\n\t        return [lambda x: x for _ in trees]\n\t    all_leaves, structures = zip(*(tree_flatten_with_implicit(tree) for tree in trees))\n\t    post_materialization_avals = [core.get_aval(leaf) for leaf in all_leaves[0]]\n\t    for i, (leaves, structure) in enumerate(zip(all_leaves[1:], structures[1:]), 1):\n\t        if structure != structures[0]:\n\t            raise ValueError('Trees do not have the same structure after materialization')\n\t        for leaf, expected_aval in zip(leaves, post_materialization_avals):\n\t            aval = core.get_aval(leaf)\n\t            if not (aval.shape == expected_aval.shape and aval.dtype == expected_aval.dtype):\n", "                raise ValueError(\n\t                    f'Trees do not have the same avals after materialization. Tree 0: {expected_aval}, Tree {i}: {aval}'\n\t                )\n\t    # Stack will contain tuples of (path, nodes)\n\t    # path = a sequence of integers specifying which child\n\t    # was taken at each _flatten_one_implicit_layer call\n\t    # or the first flatten_with_implicit call\n\t    # nodes = one node from each tree\n\t    stack = []\n\t    all_leaves = []\n", "    for tree in trees:\n\t        all_leaves.append(tree_leaves_with_implicit(tree))\n\t    for i, nodes in enumerate(zip(*all_leaves)):\n\t        stack.append(((i,), nodes))\n\t    materialization_paths = set()\n\t    while stack:\n\t        path_prefix, nodes = stack.pop()\n\t        if not any(isinstance(node, ia.ImplicitArray) for node in nodes):\n\t               continue\n\t        all_leaves, all_structures = zip(*(\n", "            flatten_one_implicit_layer(node) for node in nodes\n\t        ))\n\t        node_structures = set(all_structures)\n\t        if len(node_structures) > 1:\n\t            materialization_paths.add(path_prefix)\n\t            continue\n\t        aval_diff = False\n\t        for leaves in zip(*all_leaves):\n\t            first_aval = core.get_aval(leaves[0])\n\t            shape = first_aval.shape\n", "            dtype = first_aval.dtype\n\t            for leaf in leaves[1:]:\n\t                aval = core.get_aval(leaf)\n\t                if not (aval.shape == shape and aval.dtype == dtype):\n\t                    materialization_paths.add(path_prefix)\n\t                    aval_diff = True\n\t            if aval_diff:\n\t                break\n\t        if aval_diff:\n\t            continue\n", "        for i, leaf_group in enumerate(zip(*all_leaves)):\n\t            stack.append((path_prefix + (i,), leaf_group))\n\t    return [_get_pruning_transform(tree, materialization_paths) for tree in trees]\n\tdef materialize_nested(implicit_arr, full=False):\n\t    \"\"\"\n\t    Materialize an ImplicitArray instance, handling the case where implicit_arr.materialize()\n\t    involves further ImplicitArray instances.\n\t    Arguments:\n\t        implicit_arr: An ImplicitArray instance\n\t        full: If True, repeatedly materialize until the result is a concrete array\n", "    Returns:\n\t        The materialized array\n\t    \"\"\"\n\t    while isinstance(implicit_arr, ia.ImplicitArray):\n\t        wrapped = lu.wrap_init(type(implicit_arr).materialize)\n\t        flat, in_tree = flatten_one_implicit_layer((implicit_arr,))\n\t        flat_fn, out_tree = flatten_fun_nokwargs(wrapped, in_tree)\n\t        out_flat = ia.use_implicit_args(flat_fn.call_wrapped)(*flat)\n\t        implicit_arr = jax.tree_util.tree_unflatten(out_tree(), out_flat)\n\t        if not full:\n", "            break\n\t    return implicit_arr\n"]}
{"filename": "qax/implicit/__init__.py", "chunked_list": []}
{"filename": "tests/grad.py", "chunked_list": ["from dataclasses import dataclass\n\timport jax\n\timport jax.numpy as jnp\n\tfrom qax import ArrayValue, use_implicit_args, ImplicitArray, primitive_handler\n\t@dataclass\n\tclass TwoMatricesInATrenchcoat(ImplicitArray):\n\t    a : ArrayValue\n\t    b : ArrayValue\n\t    def materialize(self):\n\t        return self.a + self.b\n", "@primitive_handler('mul')\n\tdef handler(primitive, x : TwoMatricesInATrenchcoat, y : jax.Array):\n\t    return TwoMatricesInATrenchcoat(x.a * y, x.b * y)\n\tdef test_grad():\n\t    shape = (5, 7)\n\t    k1, k2, k3 = jax.random.split(jax.random.PRNGKey(0), 3)\n\t    x = TwoMatricesInATrenchcoat(\n\t        jax.random.normal(k1, shape),\n\t        jax.random.normal(k2, shape),\n\t    )\n", "    y = jax.random.normal(k3, shape)\n\t    @use_implicit_args\n\t    def f(x, y):\n\t        return jnp.sum(x * y)\n\t    def explicit_f(a, b, y):\n\t        return jnp.sum((a * y) + (b * y))\n\t    x_grad = jax.grad(f)(x, y)\n\t    y_grad = jax.grad(f, 1)(x, y)\n\t    a_grad_expected = jax.grad(explicit_f)(x.a, x.b, y)\n\t    b_grad_expected = jax.grad(explicit_f, 1)(x.b, x.a, y)\n", "    y_grad_expected = jax.grad(explicit_f, 2)(x.a, x.b, y)\n\t    assert jnp.allclose(x_grad.a, a_grad_expected)\n\t    assert jnp.allclose(x_grad.b, b_grad_expected)\n\t    assert jnp.allclose(y_grad, y_grad_expected)\n"]}
{"filename": "tests/primitives.py", "chunked_list": ["from dataclasses import dataclass\n\timport jax\n\timport jax.numpy as jnp\n\timport pytest\n\tfrom qax import ArrayValue, use_implicit_args, ImplicitArray, primitive_handler, default_handler\n\tfrom qax.constants import CUMULATIVE_REDUCTION_OPS, ELEMENTWISE_UNOPS, ELEMENTWISE_BINOPS, REDUCTION_OPS\n\tprimitive_example_params = {\n\t    'convert_element_type': {'new_dtype': jnp.float32, 'weak_type': False},\n\t    'integer_pow': {'y': 3},\n\t    'reduce_precision': {'exponent_bits': 4, 'mantissa_bits': 12},\n", "    'round': {'rounding_method': jax.lax.RoundingMethod.AWAY_FROM_ZERO}\n\t}\n\tfor op in REDUCTION_OPS:\n\t    primitive_example_params[op] = {'axes': (1,)}\n\tfor op in CUMULATIVE_REDUCTION_OPS:\n\t    primitive_example_params[op] = {'axis': 1, 'reverse': False}\n\tprimitive_example_params['argmax'] = primitive_example_params['argmin'] = {'axes': (1,), 'index_dtype': jnp.int32}\n\tinput_dtypes = {\n\t    'clz': jnp.int32,\n\t    'not': jnp.uint8,\n", "    'population_count': jnp.int32,\n\t    'imag': jnp.complex64,\n\t    'real': jnp.complex64,\n\t    'shift_right_logical': (jnp.int32, jnp.int32),\n\t    'shift_right_arithmetic': (jnp.int32, jnp.int32),\n\t    'shift_left': (jnp.int32, jnp.int32),\n\t    'or': (jnp.int32, jnp.int32),\n\t    'and': (jnp.int32, jnp.int32),\n\t    'xor': (jnp.int32, jnp.int32),\n\t}\n", "def make_class_for_primitive(primitive):\n\t    @dataclass\n\t    class StackedArray(ImplicitArray):\n\t        a : ArrayValue\n\t        b : ArrayValue\n\t        def materialize(self):\n\t            return jnp.concatenate((self.a, self.b), axis=0)\n\t        def __repr__(self):\n\t            return f'StackedArray({self.a}, {self.b})'\n\t    return StackedArray\n", "@pytest.mark.parametrize('primitive', ELEMENTWISE_UNOPS)\n\tdef test_unop(primitive):\n\t    StackedArray = make_class_for_primitive(primitive)\n\t    @primitive_handler(primitive)\n\t    def handler(primitive, arg : StackedArray, **kwargs):\n\t        new_a = default_handler(primitive, arg.a, **kwargs)\n\t        new_b = default_handler(primitive, arg.b, **kwargs)\n\t        return StackedArray(new_a, new_b,)\n\t    lax_primitive = getattr(jax.lax, f'{primitive}_p')\n\t    def f(x):\n", "        params = primitive_example_params.get(primitive, {})\n\t        return lax_primitive.bind(x, **params)\n\t    to_type = input_dtypes.get(primitive, jnp.float32)\n\t    x = jax.random.normal(jax.random.PRNGKey(0), (3, 7)).astype(to_type)\n\t    y = jax.random.normal(jax.random.PRNGKey(1), (9, 7)).astype(to_type)\n\t    stacked = StackedArray(x, y)\n\t    expected = f(stacked.materialize())\n\t    with_implicit = use_implicit_args(f)(stacked).materialize()\n\t    close = jnp.isclose(with_implicit, expected)\n\t    nan_agree = jnp.logical_and(jnp.isnan(with_implicit), jnp.isnan(expected))\n", "    assert jnp.all(close | nan_agree)\n\t@pytest.mark.parametrize('primitive', ELEMENTWISE_BINOPS)\n\tdef test_binop(primitive):\n\t    StackedArray = make_class_for_primitive(primitive)\n\t    @primitive_handler(primitive)\n\t    def handler(primitive, arg1 : StackedArray, arg2 : StackedArray, **kwargs):\n\t        new_a = default_handler(primitive, arg1.a, arg2.a, **kwargs)\n\t        new_b = default_handler(primitive, arg1.b, arg2.b, **kwargs)\n\t        return StackedArray(new_a, new_b)\n\t    lax_primitive = getattr(jax.lax, f'{primitive}_p')\n", "    def f(x, y):\n\t        params = primitive_example_params.get(primitive, {})\n\t        return lax_primitive.bind(x, y, **params)\n\t    lhs_type, rhs_type = input_dtypes.get(primitive, (jnp.float32, jnp.float32))\n\t    x = jax.random.normal(jax.random.PRNGKey(0), (3, 7)).astype(lhs_type)\n\t    y = jax.random.normal(jax.random.PRNGKey(1), (9, 7)).astype(lhs_type)\n\t    stacked1 = StackedArray(x, y)\n\t    z = jax.random.normal(jax.random.PRNGKey(2), (3, 7)).astype(rhs_type)\n\t    w = jax.random.normal(jax.random.PRNGKey(3), (9, 7)).astype(rhs_type)\n\t    stacked2 = StackedArray(z, w)\n", "    expected = f(stacked1.materialize(), stacked2.materialize())\n\t    with_implicit = use_implicit_args(f)(stacked1, stacked2).materialize()\n\t    close = jnp.isclose(with_implicit, expected)\n\t    nan_agree = jnp.logical_and(jnp.isnan(with_implicit), jnp.isnan(expected))\n\t    assert jnp.all(close | nan_agree)\n\t@pytest.mark.parametrize('primitive', REDUCTION_OPS)\n\tdef test_reduction(primitive):\n\t    StackedArray = make_class_for_primitive(primitive)\n\t    @primitive_handler(primitive)\n\t    def handler(primitive, arg : StackedArray, *, axes, **params):\n", "        if 0 in axes:\n\t            raise ValueError('Tests should use axis 1')\n\t        a_reduced = default_handler(primitive, arg.a, axes=axes, **params)\n\t        b_reduced = default_handler(primitive, arg.b, axes=axes, **params)\n\t        return StackedArray(a_reduced, b_reduced)\n\t    lax_primitive = getattr(jax.lax, f'{primitive}_p')\n\t    def f(x):\n\t        params = primitive_example_params.get(primitive, {})\n\t        return lax_primitive.bind(x, **params)\n\t    to_type = input_dtypes.get(primitive, jnp.int32)\n", "    x = jax.random.normal(jax.random.PRNGKey(0), (3, 7)).astype(to_type)\n\t    y = jax.random.normal(jax.random.PRNGKey(1), (9, 7)).astype(to_type)\n\t    stacked = StackedArray(x, y)\n\t    expected = f(stacked.materialize())\n\t    with_implicit = use_implicit_args(f)(stacked).materialize()\n\t    close = jnp.isclose(with_implicit, expected)\n\t    nan_agree = jnp.logical_and(jnp.isnan(with_implicit), jnp.isnan(expected))\n\t    assert jnp.all(close | nan_agree)\n\t@pytest.mark.parametrize('primitive', CUMULATIVE_REDUCTION_OPS)\n\tdef test_cumulative_reduction(primitive):\n", "    StackedArray = make_class_for_primitive(primitive)\n\t    @primitive_handler(primitive)\n\t    def handler(primitive, arg : StackedArray, *, axis, **params):\n\t        if axis != 1:\n\t            raise ValueError('Tests should use axis 1')\n\t        a_reduced = default_handler(primitive, arg.a, axis=axis, **params)\n\t        b_reduced = default_handler(primitive, arg.b, axis=axis, **params)\n\t        return StackedArray(a_reduced, b_reduced)\n\t    lax_primitive = getattr(jax.lax, f'{primitive}_p')\n\t    def f(x):\n", "        params = primitive_example_params.get(primitive, {})\n\t        return lax_primitive.bind(x, **params)\n\t    to_type = input_dtypes.get(primitive, jnp.float32)\n\t    x = jax.random.normal(jax.random.PRNGKey(0), (3, 7)).astype(to_type)\n\t    y = jax.random.normal(jax.random.PRNGKey(1), (9, 7)).astype(to_type)\n\t    stacked = StackedArray(x, y)\n\t    expected = f(stacked.materialize())\n\t    with_implicit = use_implicit_args(f)(stacked).materialize()\n\t    close = jnp.isclose(with_implicit, expected)\n\t    nan_agree = jnp.logical_and(jnp.isnan(with_implicit), jnp.isnan(expected))\n", "    assert jnp.all(close | nan_agree)\n"]}
{"filename": "tests/transform.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom functools import partial\n\tfrom typing import Any, Union\n\timport warnings\n\timport jax\n\tfrom jax.core import Primitive\n\tfrom jax.experimental import pjit\n\timport jax.numpy as jnp\n\timport numpy as np\n\timport pytest\n", "from qax import ImplicitArray, use_implicit_args, primitive_handler\n\tfrom qax import utils\n\tWARN_PATTERN = '.*implicit args will be materialized'\n\t@dataclass\n\tclass ImplicitConst(ImplicitArray):\n\t    default_dtype = jnp.float32\n\t    value : Any\n\t    dummy_val : Any\n\t    def materialize(self):\n\t        return jnp.full(self.shape, self.value, dtype=self.dtype)\n", "@primitive_handler([jax.lax.mul_p, jax.lax.sub_p])\n\tdef mul_handler(primitive : Primitive, a : ImplicitConst, b : Union[ImplicitConst, jax.Array], **params):\n\t    def op(lhs, rhs):\n\t        return lhs * rhs if primitive.name == 'mul' else lhs - rhs\n\t    assert not params\n\t    if isinstance(b, ImplicitConst):\n\t        return ImplicitConst(op(a.value, b.value), a.dummy_val, shape=a.shape, dtype=a.dtype)\n\t    if b.shape == ():\n\t        new_value = op(a.value, b)\n\t        return ImplicitConst(new_value, a.dummy_val, shape=a.shape, dtype=a.dtype)\n", "    return op(a.value, b)\n\t@pytest.fixture\n\tdef const():\n\t    shape = (2, 3)\n\t    return ImplicitConst(2, -173, shape=shape)\n\tdef test_transform(const):\n\t    @use_implicit_args\n\t    def f(x, y):\n\t        return x * y\n\t    print(f'Const: {const}')\n", "    assert f(const, jnp.ones(const.shape))[0, 0] == const.value\n\tdef test_pjit(const):\n\t    @use_implicit_args\n\t    @pjit.pjit\n\t    def f(x, y):\n\t        return x * y\n\t    assert f(const, jnp.ones(const.shape))[0, 0] == const.value\n\tdef test_remat(const):\n\t    @use_implicit_args\n\t    @jax.checkpoint\n", "    def f(x, y):\n\t        return x * y\n\t    result = f(const, jnp.ones(const.shape))\n\t    assert result.shape == const.shape\n\t    assert result[0, 0] == const.value\n\tdef test_materialize(const):\n\t    def f(x):\n\t        return 3 + x\n\t    with pytest.warns(UserWarning, match=WARN_PATTERN):\n\t        use_implicit_args(f)(const)\n", "def test_cond(const):\n\t    @use_implicit_args\n\t    def f(x, y):\n\t        def true_fn(x):\n\t            return x * jnp.ones(x.shape)\n\t        def false_fn(x):\n\t            return x * jnp.zeros(x.shape) + 5\n\t        return jnp.sum(jax.lax.cond(y, true_fn, false_fn, x))\n\t    with warnings.catch_warnings():\n\t        warnings.filterwarnings('error', message=WARN_PATTERN)\n", "        assert f(const, True) == const.value * np.prod(const.shape)\n\t        assert f(const, False) == 5 * np.prod(const.shape)\n\tdef test_cond_materialize_branch(const):\n\t    @use_implicit_args\n\t    def f(x, y):\n\t        def true_fn(x):\n\t            return x\n\t        def false_fn(x):\n\t            return jnp.ones(x.shape)\n\t        return jax.lax.cond(y, true_fn, false_fn, x)\n", "    result = f(const, True)\n\t    assert isinstance(result, jax.Array)\n\t    assert result.shape == const.shape\n\t    assert jnp.all(result == const.value)\n\tdef test_cond_partial_materialize_branch():\n\t    @use_implicit_args\n\t    def f(x, y, z):\n\t        def true_fn(x, y):\n\t            return y * y\n\t        def false_fn(x, y):\n", "            return x * x\n\t        return jax.lax.cond(z, true_fn, false_fn, x, y)\n\t    shape = (2, 3)\n\t    x = ImplicitConst(2., -173, shape=shape)\n\t    y = ImplicitConst(\n\t            value=ImplicitConst(1., -173, shape=()),\n\t            dummy_val=-173,\n\t            shape=shape\n\t    )\n\t    #y._materialize()\n", "    result = f(x, y, True)\n\t    assert isinstance(result, ImplicitConst)\n\t    assert isinstance(result.value, jax.Array)\n\t    assert result.shape == (2, 3)\n\t    assert jnp.all(result.value == 1)\n\tdef test_switch(const):\n\t    @use_implicit_args\n\t    def f(x, i):\n\t        branch_fn = lambda a, x: jnp.sum(a * x)\n\t        branches = [partial(branch_fn, jnp.asarray(i)) for i in range(3)]\n", "        return jax.lax.switch(i, branches, x)\n\t    with warnings.catch_warnings():\n\t        warnings.filterwarnings('error', message='.*switch was not handled')\n\t        assert f(const, 0) == 0\n\tdef test_no_implicit_args():\n\t    def f(x):\n\t        return jnp.sum(x ** 2)\n\t    assert use_implicit_args(f)(jnp.ones((3, 3))) == 9\n\tdef test_vmap():\n\t    def f(x, y):\n", "        return jnp.sum(x * y)\n\t    xs = ImplicitConst(\n\t        jnp.arange(3),\n\t        jnp.arange(-100, -97),\n\t        shape=(7, 11)\n\t    )\n\t    ys = jax.random.normal(jax.random.PRNGKey(0), (7, 11))\n\t    x_value = jnp.tile(jnp.arange(3)[:, None, None], (1, 7, 11))\n\t    vmapped_f = jax.vmap(f, in_axes=(0, None))\n\t    implicit_f = jax.vmap(use_implicit_args(f), in_axes=(0, None))\n", "    result = implicit_f(xs, ys)\n\t    expected_result = vmapped_f(x_value, ys)\n\t    assert jnp.allclose(result, expected_result)\n"]}
{"filename": "tests/utils.py", "chunked_list": ["from dataclasses import dataclass\n\timport jax\n\timport jax.numpy as jnp\n\tfrom jax.tree_util import tree_structure\n\timport pytest\n\timport optax\n\tfrom qax import ArrayValue, utils, ImplicitArray\n\t@dataclass\n\tclass Container(ImplicitArray):\n\t    a : ArrayValue\n", "    b : ArrayValue\n\t    def materialize(self):\n\t        return self.a\n\t@pytest.fixture(scope='module', params=[0, 1, 2, 3])\n\tdef container_with_depth(request):\n\t    a = jnp.arange(10)\n\t    for d in range(request.param):\n\t        a = Container(a, jnp.zeros(d))\n\t    return a, request.param\n\tdef test_count_depth(container_with_depth):\n", "    container, depth = container_with_depth\n\t    assert utils.implicit_depth(container) == depth\n\tdef test_flatten_one_layer(container_with_depth):\n\t    container, depth = container_with_depth\n\t    pytree = [{'x': container}, {'y': container}]\n\t    flat, struct = utils.flatten_one_implicit_layer(pytree)\n\t    unflattened = jax.tree_util.tree_unflatten(struct, flat)\n\t    assert jax.tree_util.tree_structure(unflattened) == jax.tree_util.tree_structure(pytree)\n\t    assert utils.implicit_depth(flat) == max(depth - 1, 0)\n\tdef _get_prefix(*containers):\n", "    return [transform(c) for c, transform in zip(containers, utils.get_common_prefix_transforms(containers))]\n\tdef test_prefix():\n\t    c1 = Container(\n\t        a=Container(jnp.zeros(10), jnp.zeros(10)),\n\t        b=Container(jnp.zeros(3), jnp.zeros(13))\n\t    )\n\t    c2 = Container(\n\t        a=Container(jnp.zeros(10), jnp.zeros(10)),\n\t        b=jnp.zeros(3)\n\t    )\n", "    full_materialized_c1, _ = _get_prefix(c1, jnp.ones(10))\n\t    assert isinstance(full_materialized_c1, jax.Array)\n\t    assert jnp.all(full_materialized_c1 == jnp.zeros(10))\n\t    c3 = Container(\n\t        a=Container(jnp.zeros(10), jnp.zeros(3)),\n\t        b=Container(jnp.zeros(3), jnp.zeros(13))\n\t    )\n\t    prefix_c1, prefix_c3 = _get_prefix(c1, c3)\n\t    expected = Container(a=jnp.zeros(10), b=Container(jnp.zeros(3), jnp.zeros(13)))\n\t    assert tree_structure(prefix_c1) == tree_structure(prefix_c3) == tree_structure(expected)\n", "    c4 = Container(\n\t        a=Container(\n\t            a=Container(jnp.ones(10), jnp.zeros(3)),\n\t            b=jnp.zeros(3)\n\t        ),\n\t        b=jnp.zeros(10)\n\t    )\n\t    c5 = Container(\n\t        a=jnp.zeros(10),\n\t        b=Container(\n", "            Container(jnp.zeros(10), jnp.zeros(3)),\n\t            Container(jnp.zeros(3), jnp.zeros(13))\n\t        )\n\t    )\n\t    prefix_c4, prefix_c5 = _get_prefix(c4, c5)\n\t    expected = Container(a=jnp.zeros(10), b=jnp.zeros(10))\n\t    assert tree_structure(prefix_c4) == tree_structure(prefix_c5) == tree_structure(expected)\n"]}
{"filename": "tests/nested.py", "chunked_list": ["from dataclasses import dataclass\n\timport warnings\n\timport jax\n\timport jax.numpy as jnp\n\tfrom qax import ArrayValue, ImplicitArray, use_implicit_args, primitive_handler\n\t@dataclass\n\tclass Outer(ImplicitArray):\n\t    x : ArrayValue\n\t    def materialize(self):\n\t        return 2 * (self.x ** 1)\n", "@primitive_handler(jax.lax.mul_p)\n\tdef mul(primitive, arg : Outer, other : jax.Array):\n\t    return Outer(arg.x * other)\n\t@dataclass\n\tclass Inner(ImplicitArray):\n\t    value : ArrayValue\n\t    def materialize(self):\n\t        return jnp.full(self.shape, self.value, dtype=self.dtype)\n\t@primitive_handler(jax.lax.integer_pow_p)\n\tdef pow(primitive, arg : Inner, *, y):\n", "    new_value = arg.value ** y\n\t    return Inner(new_value, shape=arg.shape, dtype=arg.dtype)\n\tdef test_nested():\n\t    @use_implicit_args\n\t    def f(x):\n\t        return jnp.sum(x)\n\t    inner = Inner(3, shape=(2, 3), dtype=jnp.float32)\n\t    nested = Outer(inner)\n\t    result = f(nested)\n\t    assert result == 36\n", "def test_nested_with_operation():\n\t    @use_implicit_args\n\t    def f(x):\n\t        return jnp.sum(x * jnp.ones(x.shape))\n\t    inner = Inner(3, shape=(2, 3), dtype=jnp.float32)\n\t    nested = Outer(inner)\n\t    with warnings.catch_warnings():\n\t        warnings.filterwarnings('error', message='Primitive mul was not handled by class Outer')\n\t        result = f(nested)\n\t    assert result == 36\n"]}
{"filename": "tests/symbols.py", "chunked_list": ["import itertools\n\timport operator as fn\n\timport pytest\n\timport jax\n\timport jax.numpy as jnp\n\timport numpy as np\n\timport qax\n\tfrom qax.symbols import SymbolicConstant\n\tadd = qax.use_implicit_args(fn.add)\n\tmul = qax.use_implicit_args(fn.mul)\n", "default_shape = (3, 5)\n\t@pytest.fixture\n\tdef arr():\n\t    return jax.random.normal(jax.random.PRNGKey(0), default_shape)\n\t@pytest.fixture\n\tdef zeros():\n\t    return SymbolicConstant(0, shape=default_shape, dtype=jnp.float32)\n\t@pytest.fixture\n\tdef ones():\n\t    return SymbolicConstant(1, shape=default_shape, dtype=jnp.float32)\n", "@pytest.fixture\n\tdef pis():\n\t    return SymbolicConstant(jnp.pi, shape=default_shape, dtype=jnp.float32)\n\tdef test_add(zeros, arr, pis):\n\t    z_plus_z = add(zeros, zeros)\n\t    assert isinstance(z_plus_z, SymbolicConstant)\n\t    assert z_plus_z.value == 0\n\t    assert jnp.allclose(add(zeros, arr), arr)\n\t    pi_plus_pi = add(pis, pis)\n\t    assert isinstance(pi_plus_pi, SymbolicConstant)\n", "    assert jnp.isclose(pi_plus_pi.value, 2 * jnp.pi)\n\tdef test_mul(zeros, ones, arr, pis):\n\t    zero_times_one = mul(zeros, ones)\n\t    assert isinstance(zero_times_one, SymbolicConstant)\n\t    assert zero_times_one.value == 0\n\t    assert jnp.all(mul(ones, arr) == arr)\n\t    pi_times_pi = mul(pis, pis)\n\t    assert isinstance(pi_times_pi, SymbolicConstant)\n\t    assert jnp.isclose(pi_times_pi.value, jnp.pi ** 2)\n\t    assert mul(pis, ones).value == jnp.pi\n", "_names = ['zeros', 'ones', 'pis']\n\t@pytest.mark.parametrize('fn,lhs,rhs', itertools.product(\n\t    [jax.lax.add, jax.lax.mul, jax.lax.sub, jax.lax.atan2, jax.lax.max],\n\t    _names,\n\t    _names\n\t))\n\tdef test_binop(fn, lhs, rhs, request):\n\t    lhs = request.getfixturevalue(lhs)\n\t    rhs = request.getfixturevalue(rhs)\n\t    expected = fn(lhs.materialize(), rhs.materialize())\n", "    result = qax.use_implicit_args(fn)(lhs, rhs)\n\t    assert isinstance(result, SymbolicConstant)\n\t    assert jnp.allclose(result.value, expected)\n\t    assert result.shape == expected.shape\n\t    assert result.dtype == expected.dtype\n\t@pytest.mark.parametrize('fn,arg', itertools.product(\n\t    [jnp.sum, jnp.prod, jnp.all, jnp.any, jnp.sin, jnp.isfinite],\n\t    _names\n\t))\n\tdef test_unop(fn, arg, request):\n", "    value = request.getfixturevalue(arg)\n\t    expected = fn(value.materialize())\n\t    result = qax.use_implicit_args(fn)(value)\n\t    assert isinstance(result, SymbolicConstant)\n\t    assert jnp.allclose(result.value, expected)\n\t    assert result.shape == expected.shape\n\t    assert result.dtype == expected.dtype\n\tdef test_select_n(zeros, ones):\n\t    @qax.use_implicit_args\n\t    def f(c, x, y):\n", "        return jax.lax.select_n(c, x, y)\n\t    assert isinstance(f(True, zeros, ones), jnp.ndarray)\n\t    assert isinstance(f(False, zeros, zeros), SymbolicConstant)\n"]}
{"filename": "examples/zero.py", "chunked_list": ["from typing import Union\n\timport jax\n\timport jax.numpy as jnp\n\tfrom qax import ImplicitArray, primitive_handler, use_implicit_args\n\tclass ImplicitZeros(ImplicitArray):\n\t    default_dtype = jnp.float32\n\t    def materialize(self):\n\t        return jnp.zeros(self.shape, dtype=self.dtype)\n\tdef binop_shape_dtype(x, y):\n\t    return {\n", "        'shape': jnp.broadcast_shapes(x.shape, y.shape),\n\t        'dtype': jnp.result_type(x.dtype, y.dtype),\n\t    }\n\t@primitive_handler(jax.lax.mul_p)\n\tdef do_mul(primitive, x : ImplicitZeros, y : jax.Array):\n\t    print('Invoked do_mul')\n\t    return ImplicitZeros(**binop_shape_dtype(x, y))\n\t@primitive_handler([jax.lax.add_p, jax.lax.mul_p])\n\tdef handle_both_implicit(primitive, x : ImplicitZeros, y : ImplicitZeros):\n\t    print('Invoked handle_both_implicit')\n", "    return ImplicitZeros(**binop_shape_dtype(x, y))\n\t@primitive_handler(jax.lax.add_p)\n\tdef handle_add_general(primitive, x : ImplicitZeros, y : jax.Array):\n\t    print('Invoked handle_add')\n\t    shape_dtype = binop_shape_dtype(x, y)\n\t    return jnp.broadcast_to(y, shape_dtype['shape']).astype(shape_dtype['dtype'])\n\tdef main():\n\t    @jax.jit\n\t    @use_implicit_args\n\t    def f(x, y):\n", "                                 # If x and y are both of type ImplicitZeros, the result will be:\n\t        z = x + y                # z: ImplicitZeros  output: Invoked handle_both_implicit\n\t        w = z * jnp.ones_like(z) # w: ImplicitZeros  output: Invoked do_mul\n\t        a = jnp.sum(w)           # a: jax.Array      output: UserWarning: Primitive reduce_sum was not\n\t                                 #                           handled by class ImplicitZeros, so implicit\n\t                                 #                           args will be materialized\n\t        b =  w + a               # b: jax.Array      output: Invoked handle_add\n\t        return b\n\t    zeros = ImplicitZeros(shape=(2, 3))\n\t    result = jax.jit(f)(zeros, zeros)\n", "    assert isinstance(result, jax.Array)\n\t    assert result.shape == zeros.shape\n\t    assert jnp.all(result == 0)\n\t    # The decorated f will also work with mixed arguments or non-implicit arguments\n\t    jnp_ones = jnp.ones(zeros.shape)\n\t    f(zeros, jnp_ones)\n\t    f(jnp_ones, zeros)\n\t    f(jnp_ones, jnp_ones)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "examples/identity.py", "chunked_list": ["from dataclasses import dataclass, InitVar\n\tfrom functools import partial\n\tfrom typing import Union\n\timport jax\n\timport jax.numpy as jnp\n\timport qax\n\tfrom examples.minimal_lora import LoraMatrix\n\t@dataclass\n\tclass Eye(qax.ImplicitArray):\n\t    dim : InitVar[int]\n", "    def __init__(self, dim, dtype=jnp.float32):\n\t        super().__init__(shape=(dim, dim), dtype=dtype)\n\t    def materialize(self):\n\t        return jnp.eye(self.shape[0], dtype=self.dtype)\n\t@qax.primitive_handler(jax.lax.dot_general_p)\n\tdef dot_handler(primitive, lhs : Union[Eye,jax.Array], rhs : Union[Eye,jax.Array], *, dimension_numbers, **kwargs):\n\t    lhs_aval = jax.core.get_aval(lhs)\n\t    rhs_aval = jax.core.get_aval(rhs)\n\t    out_aval = jax.eval_shape(\n\t        partial(qax.default_handler, primitive, dimension_numbers=dimension_numbers, **kwargs),\n", "        lhs_aval, rhs_aval\n\t    )\n\t    lhs_is_eye = isinstance(lhs, Eye)\n\t    rhs_is_eye = isinstance(rhs, Eye)\n\t    (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) = dimension_numbers\n\t    # It's 1 AM and I can only conceptualize dot_generals during the PM hours\n\t    # so I will only be implementing 1-2D x 2D matmuls\n\t    if not (\n\t        1 <= lhs.aval.ndim <= 2\n\t        and rhs.aval.ndim <= 2\n", "        and len(lhs_contract) == len(rhs_contract) == 1\n\t        and lhs_batch == rhs_batch == ()\n\t    ):\n\t        return NotImplemented\n\t    if lhs_is_eye and rhs_is_eye:\n\t        return Eye(out_aval.shape[0], dtype=out_aval.dtype)\n\t    result = rhs if lhs_is_eye else lhs\n\t    return result.astype(out_aval.dtype)\n\tdef main():\n\t    @qax.use_implicit_args\n", "    def f(a, b):\n\t        return a @ b\n\t    w = Eye(3)\n\t    x = jnp.arange(39, dtype=jnp.float32).reshape(3, 13)\n\t    print(f(w, x))\n\t    dim = 128\n\t    rank = 16\n\t    eye_plus_low_rank = LoraMatrix(\n\t        W=Eye(dim),\n\t        A=jax.random.normal(jax.random.PRNGKey(0), (dim, rank)),\n", "        B=jnp.zeros((dim, rank))\n\t    )\n\t    x = jax.random.normal(jax.random.PRNGKey(1), (73, dim))\n\t    print(jnp.sum(f(x, eye_plus_low_rank)))\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "examples/nullable_array.py", "chunked_list": ["\"\"\"\n\tExample of an ImplicitArray which represents an array + a boolean mask representing the validity of each entry.\n\tThis is a proof of concept and is not optimized for performance.\n\t\"\"\"\n\tfrom dataclasses import dataclass\n\timport jax\n\timport jax.numpy as jnp\n\tfrom qax import ArrayValue, ImplicitArray, default_handler, primitive_handler, use_implicit_args\n\tfrom qax.constants import ELEMENTWISE_BINOPS, ELEMENTWISE_UNOPS, REDUCTION_OPS\n\t@dataclass\n", "class NullableArray(ImplicitArray):\n\t    val : ArrayValue\n\t    mask : ArrayValue\n\t    def materialize(self):\n\t        return self.val\n\t@primitive_handler(ELEMENTWISE_UNOPS)\n\tdef handle_unop(primitive, nullable_val : NullableArray, **params):\n\t    val = default_handler(primitive, nullable_val.val, **params)\n\t    return NullableArray(val, nullable_val.mask)\n\t@primitive_handler(ELEMENTWISE_BINOPS)\n", "def handle_binop(primitive, lhs : ArrayValue, rhs : ArrayValue, **params):\n\t    lhs_is_nullable = isinstance(lhs, NullableArray)\n\t    rhs_is_nullable = isinstance(rhs, NullableArray)\n\t    mask = lhs.mask if lhs_is_nullable else None\n\t    if lhs_is_nullable:\n\t        lhs = lhs.val\n\t    if rhs_is_nullable:\n\t        mask = rhs.mask if mask is None else mask & rhs.mask\n\t        rhs = rhs.val\n\t    out_val = default_handler(primitive, lhs, rhs, **params)\n", "    return NullableArray(out_val, mask)\n\t@primitive_handler(REDUCTION_OPS)\n\tdef handle_reduction(primitive, null_arr : NullableArray, **params):\n\t    new_val = default_handler(primitive, null_arr.val, **params)\n\t    new_mask = default_handler(jax.lax.reduce_and_p, null_arr.mask, **params)\n\t    return NullableArray(new_val, new_mask)\n\t@jax.jit\n\t@use_implicit_args\n\tdef f(x, y):\n\t    return jnp.sum(-x * y, axis=0)\n", "if __name__ == '__main__':\n\t    x = NullableArray(\n\t        val=jnp.ones((2, 3)),\n\t        mask=jnp.asarray(\n\t            [[True, False, True],\n\t             [False, True, True]]\n\t        )\n\t    )\n\t    y = NullableArray(\n\t        val=jnp.full((2, 3), 3),\n", "        mask=jnp.asarray(\n\t            [[False, True, True],\n\t             [True, True, True]]\n\t        )\n\t    )\n\t    output = f(x, y)\n\t    print(f'Result: {output.val}')\n\t    print(f'Mask: {output.mask}')\n"]}
{"filename": "examples/combining.py", "chunked_list": ["from typing import Union\n\timport jax\n\timport jax.numpy as jnp\n\tfrom qax import use_implicit_args, primitive_handler\n\tfrom examples.zero import ImplicitZeros\n\tfrom examples.const import ImplicitConst\n\t@use_implicit_args\n\tdef f(x, y):\n\t    return x * y\n\tdef main():\n", "    shape = (2, 3)\n\t    zeros = ImplicitZeros(shape=shape, dtype=jnp.float32)\n\t    const = ImplicitConst(1., shape=shape)\n\t    assert isinstance(f(const, zeros), jax.Array)\n\t    @primitive_handler('mul')\n\t    def heterogenous_handler(primitive, x: Union[ImplicitZeros, ImplicitConst], y: Union[ImplicitZeros, ImplicitConst]):\n\t        out_shape = jnp.broadcast_shapes(x.shape, y.shape)\n\t        out_dtype = jnp.result_type(x.dtype, y.dtype)\n\t        return ImplicitZeros(shape=out_shape, dtype=out_dtype)\n\t    assert isinstance(f(const, zeros), ImplicitZeros)\n", "if __name__ == '__main__':\n\t    main()\n"]}
{"filename": "examples/nesting.py", "chunked_list": ["import jax\n\timport jax.numpy as jnp\n\tfrom qax import use_implicit_args\n\tfrom examples.nullable_array import NullableArray\n\tfrom examples.zero import ImplicitZeros\n\t@jax.jit\n\t@use_implicit_args\n\tdef f(x, y):\n\t    return (x * y) + 3\n\tshape = (2, 3)\n", "x = NullableArray(\n\t    val=ImplicitZeros(shape=shape),\n\t    mask=jnp.asarray(\n\t        [[True, False, True],\n\t         [False, True, True]],\n\t    )\n\t)\n\ty = NullableArray(\n\t    val=jnp.ones(shape),\n\t    mask=jnp.asarray(\n", "        [[True, True, True],\n\t         [False, False, True]]\n\t    )\n\t)\n\tresult = f(x, y)\n\tprint(f'Result:\\n{result.val}')\n\tprint(f'Mask:\\n{result.mask}')\n"]}
{"filename": "examples/const.py", "chunked_list": ["from dataclasses import dataclass\n\timport jax\n\timport jax.numpy as jnp\n\timport numpy as np\n\tfrom qax import aux_field, ArrayValue, ImplicitArray, primitive_handler, use_implicit_args, default_handler\n\t# To define the behavior we want, we subclass qax.ImplicitArray\n\t# To define additional fields we also need to mark this class as\n\t# a dataclass\n\t@dataclass\n\tclass ImplicitConst(ImplicitArray):\n", "    # Dataclass attributes may be used to define the arrays which\n\t    # determine the concrete array being represented\n\t    # In this case it's a single JAX scalar\n\t    value : ArrayValue\n\t    # ImplicitArrays are pytrees, so all attributes are automatically\n\t    # marked as pytree children. To instead mark one as auxiliary data\n\t    # use the qax.aux_field decorator\n\t    my_aux_value : str = aux_field(default='some_metadata')\n\t    # There are several ways to control the shape and dtype of an ImplicitArray\n\t    # They are:\n", "    # 1. Pass shape/dtype kwargs to the constructor\n\t    # 2. Override the compute_shape/commute_dtype methods\n\t    # 3. Override the default_shape/default_dtype class attributes\n\t    # 4. Manually override __post_init__ and set the self.shape/self.dtype values yourself\n\t    # 5. Do none of the above, in which case materialize() will be abstractly evaluated\n\t    # in an attempt to derive the values. That won't work in this case since we need\n\t    # to know them in order to call jnp.full\n\t    default_dtype = jnp.float32\n\t    def compute_dtype(self):\n\t        # We're doing this instead of just self.value.dtype since we might get\n", "        # a python scalar\n\t        return jax.core.get_aval(self.value).dtype\n\t    # The way we can guarantee that our ImplicitArrays will work\n\t    # with pre-existing code is that whenever we hit an op\n\t    # that we haven't written custom behavior for, the\n\t    # ImplicitArray instance will be materialized into a\n\t    # dense array and the default behavior will be used\n\t    def materialize(self):\n\t        return jnp.full(self.shape, self.value, dtype=self.dtype)\n\t# The way we define custom behavior is by writing a function\n", "# and decorating it with the primitive_handler decorator\n\t# The type annotations are used for multiple dispatch with\n\t# plum so make sure to get them right!\n\t#\n\t# For commutative ops, the ImplicitArray instance will always be made the\n\t# lhs, but this isn't true for non-commutative ops as we'll see below\n\t@primitive_handler('mul')\n\tdef mul(primitive, a: ImplicitConst, b: jax.Array):\n\t    \"\"\"\n\t    Arguments:\n", "        - primitive: A JAX primitive\n\t        - const: An argument guaranteed to be an ImplicitConst instance\n\t        - other: An argument which will either be an ImplicitConst or a JAX typj\n\t    \"\"\"\n\t    # Get the output shape in case there's any broadcasting going on\n\t    out_shape = jnp.broadcast_shapes(a.shape, b.shape)\n\t    if b.size == 1:\n\t        # If we get multiplied by a scalar, we can \n\t        # output another ImplicitConst instance\n\t        # rather than materializing the dense array\n", "        return ImplicitConst(a.value * b.reshape(1)[0], shape=out_shape)\n\t    # In the general case we just multiply our constant value by the other array\n\t    result = b * a.value\n\t    return jnp.broadcast_to(result, out_shape)\n\t# We can also define the case where both arguments are ImplicitConsts\n\t@primitive_handler('mul')\n\tdef mul(primitive, a: ImplicitConst, b: ImplicitConst):\n\t    out_shape = jnp.broadcast_shapes(a.shape, b.shape)\n\t    return ImplicitConst(a.value * b.value, shape=out_shape)\n\t# You can use one handler for multiple primitives by passing an iterable to the decorator\n", "@primitive_handler(['sin', 'cos', 'exp'])\n\tdef elementwise_unop(primitive, arg : ImplicitConst):\n\t    # In a lot of cases the logic doesn't have anything\n\t    # to do with the exact primitive being used so \n\t    # we can just use qax.default_handler to execute\n\t    result = default_handler(primitive, arg.value)\n\t    return ImplicitConst(result, shape=arg.shape)\n\t# If the primitive has any params (such as reduction axes) the handler will receive\n\t# them as a param kwarg\n\t#\n", "# The above handlers were registered using the primitive name, which is\n\t# is using the actual lax primitive under the hood. You can also use\n\t# the actual primitive, which is done here\n\t@primitive_handler(jax.lax.reduce_sum_p)\n\tdef reduce_sum(primitive, a: ImplicitConst, *, axes):\n\t    sum_result = np.prod([a.shape[i] for i in axes]) * a.value\n\t    new_shape = tuple(d for d in a.shape if d not in axes)\n\t    return ImplicitConst(sum_result, shape=new_shape)\n\t# This decorator makes it so that `f` can handle inputs which are instances\n\t# of ImplicitArray subclasses (or pytrees containing such instances)\n", "# You can also still call it with ordinary JAX inputs\n\t@use_implicit_args\n\tdef f(a, b):\n\t    c = a * b\n\t    d = jnp.sin(c)\n\t    return jnp.sum(d)\n\tdef main():\n\t    shape = (5, 7)\n\t    a_full = jnp.full(shape, 3.)\n\t    a_implicit = ImplicitConst(3., shape=shape)\n", "    b_full = jnp.full(shape, 2.)\n\t    b_implicit = ImplicitConst(2., shape=shape)\n\t    result = f(a_full, b_full)\n\t    full_implicit_result = f(a_implicit, b_implicit)\n\t    mixed_result = f(a_implicit, b_full)\n\t    # We get the same result each time (other than some floating point error)\n\t    # In the second case, we were able to avoid materializing the ImplicitConst\n\t    # so we get an ImplicitConst as an output\n\t    print(result)               # -9.779543\n\t    print(full_implicit_result) # ImplicitConst(-9.779541969299316, (5, 7))\n", "    print(mixed_result)         # -9.779543\n\t    # We can also nest ImplicitArray instances (even if they're different subclasses)\n\t    nested_b = ImplicitConst(\n\t        value=ImplicitConst(2., shape=()),\n\t        shape=shape\n\t    )\n\t    nested_result = f(a_implicit, nested_b)\n\t    print(nested_result) # ImplicitConst(ImplicitConst(-9.779541969299316, ()), (5, 7))\n\tif __name__ == '__main__':\n\t    main()\n"]}
