{"filename": "setup.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\tfrom setuptools import setup, find_packages\n\twith open('README.md', encoding='utf-8') as f:\n\t    long_description = f.read()\n\twith open('requirements.txt', encoding='utf-8') as f:\n\t    requirements = f.read().strip().split('\\n')\n\tpkgs = find_packages(where='src')\n\tprint(pkgs)\n\tsetup(\n\t    name='ask-api',\n", "    version='0.1.0',\n\t    url='https://github.com/yanqingmen/ask-api',\n\t    license=\"Apache License 2.0\",\n\t    long_description=long_description,\n\t    long_description_content_type='text/markdown',\n\t    author='yanqingmen',\n\t    python_requires='>=3.7',\n\t    install_requires=requirements,\n\t    package_dir={\"\": \"src\"},\n\t    packages=pkgs,\n", ")\n"]}
{"filename": "tests/llm/__init__.py", "chunked_list": []}
{"filename": "tests/llm/llm_util_test.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\tllm util test\n\t\"\"\"\n\tfrom ask_api.llm.base import LLMBase\n\tfrom ask_api.config import askapi_config\n\tfrom ask_api.llm.llm_util import build_default_llm\n\tdef split_text(text: str, sep: str = \" \") -> list:\n\t    \"\"\"\n\t    demo function\n", "    \"\"\"\n\t    return text.split(sep)\n\tdef test_build_default_llm():\n\t    llm = build_default_llm()\n\t    assert issubclass(llm.__class__, LLMBase)\n\tdef test_lang():\n\t    askapi_config.LANG = \"zh\"\n\t    zh_llm = build_default_llm()\n\t    print(\"zh_llm desc: \", zh_llm.desc(split_text))\n\t    askapi_config.LANG = \"en\"\n", "    en_llm = build_default_llm()\n\t    print(\"en_llm desc: \", en_llm.desc(split_text))\n"]}
{"filename": "tests/util/askapi_asyn_test.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\t测试异步调用函数相关工具\n\t\"\"\"\n\tfrom ask_api.util.askapi_asyn import run_async_task, wait_task\n\tdef test_run_async_task():\n\t    async def func():\n\t        return 1\n\t    task = run_async_task(func())\n\t    assert wait_task(task) == 1\n"]}
{"filename": "tests/util/__init__.py", "chunked_list": []}
{"filename": "src/ask_api/ask_api.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\task api, talk with your python code\n\t\"\"\"\n\tfrom ask_api.base.session import Session, Message\n\tfrom ask_api.llm.llm_util import build_default_llm\n\tfrom ask_api.base.role import FunctionRole, UserRole, Role\n\tfrom ask_api.util.askapi_log import logging\n\tGLOBAL_SESSIONS = {}\n\tFUNCTION_ROLES = {}\n", "# 用户角色，目前只用于占位，后续添加交互功能\n\tDEFAULT_USER_ROLE = UserRole(\"user\")\n\tdef create_func_role(func, name: str = None) -> Role:\n\t    \"\"\"\n\t    create function role\n\t    \"\"\"\n\t    if func in FUNCTION_ROLES:\n\t        return FUNCTION_ROLES[func]\n\t    logging.info(\"create function role for: {}\".format(func.__name__))\n\t    llm = build_default_llm()\n", "    role = FunctionRole(llm, func, name)\n\t    FUNCTION_ROLES[func] = role\n\t    return role\n\tdef ask_func(func,\n\t             message: str,\n\t             mode: str = \"free\",\n\t             user_role: Role = None,\n\t             func_name: str = None,\n\t             session: Session = None) -> Session:\n\t    \"\"\"\n", "    ask function\n\t    \"\"\"\n\t    if session is None:\n\t        session_id = str(id(func))\n\t        if session_id not in GLOBAL_SESSIONS:\n\t            GLOBAL_SESSIONS[session_id] = Session()\n\t        session = GLOBAL_SESSIONS[session_id]\n\t    if user_role is None:\n\t        user_role = DEFAULT_USER_ROLE\n\t    if not mode == \"desc\":\n", "        # 仅在非描述模式下，才将用户输入加入会话\n\t        question = Message(user_role, message)\n\t        session.add_message(question)\n\t    func_role = create_func_role(func, func_name)\n\t    # session 作为参数传入，方便异步函数调用时使用\n\t    answer = func_role.answer(message, mode, session)\n\t    session.add_message(answer)\n\t    return session\n\tdef get_session(func):\n\t    \"\"\"\n", "    get session\n\t    \"\"\"\n\t    session_id = str(id(func))\n\t    if session_id not in GLOBAL_SESSIONS:\n\t        GLOBAL_SESSIONS[session_id] = Session()\n\t    return GLOBAL_SESSIONS[session_id]\n"]}
{"filename": "src/ask_api/__init__.py", "chunked_list": []}
{"filename": "src/ask_api/config/__init__.py", "chunked_list": []}
{"filename": "src/ask_api/config/askapi_config.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\task api 相关基础配置\n\t\"\"\"\n\timport os\n\t# 语言配置，zh 为中文，en 为英文\n\tLANG = \"zh\"\n\t# llm模型阅读函数的最大长度\n\tMAX_FUNC_LEN = 1024\n\t# 是否为debug模型\n", "ASK_API_DEBUG = False\n\t# openai api key path\n\tOPENAI_KEY_PATH = \"config/open-ai.key\"\n\tOPENAI_KEY = None\n\t# DEFAULT_LLM_MODEL\n\tDEFAULT_LLM_MODEL = \"openai_chat\"\n\tDEFAULT_OPENAI_CHAT_MODEL = \"gpt-3.5-turbo\"\n"]}
{"filename": "src/ask_api/prompt/__init__.py", "chunked_list": []}
{"filename": "src/ask_api/prompt/basic.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\t基础prompt，包含 中文 以及 英文\n\t\"\"\"\n\tZH_BASIC_PROMPT = {\n\t    \"system_prompt\": \"下面是一段函数代码，现在由你扮演这个函数，使用第一人称回答用户对于这个函数的相关问题。\\n以下是函数代码： {code_source}\",\n\t    # 用于给llm进行示例的prompt，可以不提供，以减少token数量，可以对比不提供示例时的效果\n\t    \"system_example_prompt\": [\n\t        {\"role\": \"system\", \"name\": \"example_user\", \"content\": \"你好，请问你能帮我做什么？\"},\n\t        {\"role\": \"system\", \"name\": \"example_assistant\",\n", "         \"content\": \"你好，我是{函数名}，我能够帮助你{函数功能描述}，如果需要我的帮助，请对我说{指令说明}\"},\n\t    ],\n\t    \"desc_prompt\": \"你好，请问你能帮我做什么？\",\n\t    \"execute_prompt\": \"请将下面的用户请求转换为你可接收的参数，表示为json格式，若无法转换时，请回答缺少的参数信息。\\n用户请求：{message}\",\n\t    \"exception_prompt\": \"假设下面是你在执行时抛出的异常信息，请说明其内容.\\n异常信息：{message}\",\n\t    \"return_prompt\": \"假设下面是你的返回结果，请用说明一下返回的信息。\\n返回值：{message}\",\n\t    \"execute_message\": \"我已经开始处理任务了，请稍等。\",\n\t    \"exception_message\": \"我很抱歉，我无法完成这个任务，异常信息如下：\\n{message}\",\n\t    \"return_message\": \"我已经完成了任务，返回结果如下：\\n{message}\"\n\t}\n", "EN_BASIC_PROMPT = {\n\t    \"system_prompt\": \"The following is a piece of function code. Now you play this function,\"\n\t                     \" answer the user's questions about this function in the first person.\"\n\t                     \"\\nHere is the function code: {code_source}\",\n\t    # 用于给llm进行示例的prompt，可以不提供，以减少token数量，可以对比不提供示例时的效果\n\t    \"system_example_prompt\": [\n\t        {\"role\": \"system\", \"name\": \"example_user\", \"content\": \"Hello, what can you do for me?\"},\n\t        {\"role\": \"system\", \"name\": \"example_assistant\",\n\t         \"content\": \"Hello, I am {function name}, I can help you {function description}, \"\n\t                    \"if you need my help, please tell me {instruction description}\"},\n", "    ],\n\t    \"desc_prompt\": \"Hello, what can you do for me?\",\n\t    \"execute_prompt\": \"Please convert the following user request to the parameters you can accept, \"\n\t                      \"represented as json format. If you cannot convert it,\"\n\t                      \"please answer the missing parameter information.\\n\"\n\t                      \"User request: {message}\",\n\t    \"exception_prompt\": \"Assume the following is the exception information you throw when executing, \"\n\t                        \"please explain its content.\\n\"\n\t                        \"Exception information: {message}\",\n\t    \"return_prompt\": \"Assume the following is your return result, please explain the returned information.\\n\"\n", "                     \"Return value: {message}\",\n\t    \"execute_message\": \"I'm processing the task, please wait a moment.\",\n\t    \"exception_message\": \"I'm sorry, I can't do this task, the exception information is as follows:\\n {message}\",\n\t    \"return_message\": \"The task is completed, the result is as follows:\\n {message}\"\n\t}\n\tALL_BASIC_PROMPTS = {\n\t    \"zh\": ZH_BASIC_PROMPT,\n\t    \"en\": EN_BASIC_PROMPT\n\t}\n"]}
{"filename": "src/ask_api/llm/base.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\tllm 主要用于生成如下信息：\n\t1. 函数的功能描述\n\t2. 用户自然语言输入与函数参数的映射\n\t3. 函数的返回值描述\n\t4. 函数的异常信息描述\n\t5. 与函数进行自由回答\n\t\"\"\"\n\tfrom abc import ABC, abstractmethod\n", "from typing import Dict\n\tclass AskApiPrompt(ABC):\n\t    def __init__(self, prompt_config) -> None:\n\t        self.prompt = prompt_config\n\t    def __str__(self) -> str:\n\t        return self.prompt\n\t    @abstractmethod\n\t    def desc_prompt(self, func):\n\t        raise NotImplementedError(\"desc_prompt() is not implemented\")\n\t    @abstractmethod\n", "    def execute_prompt(self, func, message):\n\t        raise NotImplementedError(\"execute_prompt() is not implemented\")\n\t    @abstractmethod\n\t    def return_prompt(self, func, message):\n\t        raise NotImplementedError(\"return_prompt() is not implemented\")\n\t    @abstractmethod\n\t    def exception_prompt(self, func, message):\n\t        raise NotImplementedError(\"exception_prompt() is not implemented\")\n\t    @abstractmethod\n\t    def free_prompt(self, func, message):\n", "        raise NotImplementedError(\"free_prompt() is not implemented\")\n\t    @abstractmethod\n\t    def execute_message(self):\n\t        raise NotImplementedError(\"execute_message() is not implemented\")\n\t    @abstractmethod\n\t    def exception_message(self):\n\t        raise NotImplementedError(\"exception_message() is not implemented\")\n\tclass LLMBase(ABC):\n\t    def __init__(self, prompt: AskApiPrompt) -> None:\n\t        self.prompt = prompt\n", "    @abstractmethod\n\t    def desc(self, func) -> str:\n\t        raise NotImplementedError(\"desc() is not implemented\")\n\t    @abstractmethod\n\t    def execute_param(self, func, message) -> str:\n\t        raise NotImplementedError(\"execute() is not implemented\")\n\t    @abstractmethod\n\t    def return_value(self, func, message) -> str:\n\t        raise NotImplementedError(\"return_value() is not implemented\")\n\t    @abstractmethod\n", "    def exception(self, func, message) -> str:\n\t        raise NotImplementedError(\"exception() is not implemented\")\n\t    @abstractmethod\n\t    def free_answer(self, func, message) -> str:\n\t        raise NotImplementedError(\"free_answer() is not implemented\")\n\t    def execute_message(self):\n\t        \"\"\"\n\t        函数开始执行时的提示信息\n\t        :return:\n\t        \"\"\"\n", "        return self.prompt.execute_message()\n\t    def exception_message(self):\n\t        \"\"\"\n\t        函数执行异常时的提示信息\n\t        :return:\n\t        \"\"\"\n\t        return self.prompt.exception_message()\n\tclass LLMBuilder(ABC):\n\t    @staticmethod\n\t    def builder_name():\n", "        return \"llm_base\"\n\t    def __init__(self, name: str) -> None:\n\t        self._name = name\n\t    @property\n\t    def name(self) -> str:\n\t        return self._name\n\t    @abstractmethod\n\t    def __call__(self, prompt_config: Dict) -> LLMBase:\n\t        raise NotImplementedError(\"__call__() is not implemented\")\n\t    @name.setter\n", "    def name(self, value):\n\t        self._name = value\n"]}
{"filename": "src/ask_api/llm/openai.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\topenai based llm\n\t\"\"\"\n\tfrom typing import Dict\n\timport openai\n\timport os\n\tfrom ask_api.config.askapi_config import ASK_API_DEBUG, DEFAULT_OPENAI_CHAT_MODEL, OPENAI_KEY, OPENAI_KEY_PATH\n\tfrom ask_api.util.askapi_log import logging\n\tfrom ask_api.llm.base import LLMBase, AskApiPrompt, LLMBuilder\n", "from ask_api.util.askapi_inspect import get_func_source\n\t# openai api key\n\tif OPENAI_KEY is None:\n\t    if \"OPENAI_KEY\" in os.environ:\n\t        OPENAI_KEY = os.environ[\"OPENAI_KEY\"]\n\t    elif os.path.exists(OPENAI_KEY_PATH):\n\t        with open(OPENAI_KEY_PATH, \"r\") as f:\n\t            OPENAI_KEY = f.read().strip()\n\t    else:\n\t        logging.warning(\n", "            \"openai key not founded in both os.environ, OPENAI_KEY property and OPENAI_KEY_PATH;  OPENAI_KEY: {}, \"\n\t            \"OPENAI_KEY_PATH: {}\".format(\n\t                OPENAI_KEY, OPENAI_KEY_PATH))\n\t        raise ValueError(\"openai api key is not available\")\n\topenai.api_key = OPENAI_KEY\n\tclass OpenAIChatPrompt(AskApiPrompt):\n\t    \"\"\"\n\t    openai chat prompt\n\t    \"\"\"\n\t    def __init__(self, prompt_config) -> None:\n", "        \"\"\"\n\t        Args:\n\t            prompt_config (_type_): _description_\n\t        \"\"\"\n\t        super().__init__(prompt_config)\n\t        self._system_prompt = self.prompt[\"system_prompt\"]\n\t        self._system_example_prompt = self.prompt[\"system_example_prompt\"]\n\t        self._desc_prompt = self.prompt[\"desc_prompt\"]\n\t        self._execute_prompt = self.prompt[\"execute_prompt\"]\n\t        self._exception_prompt = self.prompt[\"exception_prompt\"]\n", "        self._return_prompt = self.prompt[\"return_prompt\"]\n\t    def _build_system_prompt(self, func, add_example: bool = True):\n\t        \"\"\"\n\t        build system prompt\n\t        \"\"\"\n\t        source = get_func_source(func)\n\t        system_prompt = self._system_prompt.format(code_source=source)\n\t        system_prompts = [{\"role\": \"system\", \"content\": system_prompt}]\n\t        if self._system_example_prompt and add_example:\n\t            system_prompts.extend(self._system_example_prompt)\n", "        if ASK_API_DEBUG:\n\t            logging.info(\"system_prompt: {}\".format(system_prompts))\n\t        return system_prompts\n\t    def desc_prompt(self, func):\n\t        desc_prompt = self._desc_prompt\n\t        system_prompts = self._build_system_prompt(func)\n\t        messages = system_prompts + [{\"role\": \"user\", \"content\": desc_prompt}]\n\t        if ASK_API_DEBUG:\n\t            logging.info(\"desc_prompt: {}\".format(messages))\n\t        return messages\n", "    def execute_prompt(self, func, message):\n\t        \"\"\"\n\t        解析参数的时候不需要拟人化\n\t        Args:\n\t            func (_type_): _description_\n\t            message (_type_): _description_\n\t        Returns:\n\t            _type_: _description_\n\t        \"\"\"\n\t        execute_prompt = self._execute_prompt.format(message=message)\n", "        system_prompts = self._build_system_prompt(func, add_example=False)\n\t        messages = system_prompts + [{\"role\": \"user\", \"content\": execute_prompt}]\n\t        if ASK_API_DEBUG:\n\t            logging.info(\"execute_prompt: {}\".format(messages))\n\t        return messages\n\t    def return_prompt(self, func, message):\n\t        return_prompt = self._return_prompt.format(message=message)\n\t        system_prompts = self._build_system_prompt(func)\n\t        messages = system_prompts + [{\"role\": \"user\", \"content\": return_prompt}]\n\t        if ASK_API_DEBUG:\n", "            logging.info(\"return_prompt: {}\".format(messages))\n\t        return messages\n\t    def exception_prompt(self, func, message):\n\t        exception_prompt = self._exception_prompt.format(message=message)\n\t        system_prompts = self._build_system_prompt(func)\n\t        messages = system_prompts + [{\"role\": \"user\", \"content\": exception_prompt}]\n\t        if ASK_API_DEBUG:\n\t            logging.info(\"exception_prompt: {}\".format(messages))\n\t        return messages\n\t    def free_prompt(self, func, message):\n", "        free_prompt = [{\"role\": \"user\", \"content\": message}]\n\t        system_prompts = self._build_system_prompt(func)\n\t        messages = system_prompts + free_prompt\n\t        if ASK_API_DEBUG:\n\t            logging.info(\"free_prompt: {}\".format(messages))\n\t        return messages\n\t    def execute_message(self):\n\t        return self.prompt[\"execute_message\"]\n\t    def exception_message(self):\n\t        return self.prompt[\"exception_message\"]\n", "class OpenAIChat(LLMBase):\n\t    \"\"\"\n\t    Args:\n\t        LLMBase (_type_): _description_\n\t    \"\"\"\n\t    def __init__(self, prompt: AskApiPrompt, model_name=\"gpt-3.5-turbo\") -> None:\n\t        super().__init__(prompt)\n\t        self.model_name = model_name\n\t    def desc(self, func) -> str:\n\t        desc_prompt_info = self.prompt.desc_prompt(func)\n", "        completion = openai.ChatCompletion.create(model=self.model_name, messages=desc_prompt_info)\n\t        desc_info = completion.choices[0].message.content\n\t        if ASK_API_DEBUG:\n\t            logging.info(\"desc: {}\".format(desc_info))\n\t        return desc_info\n\t    def execute_param(self, func, message) -> str:\n\t        execute_prompt_info = self.prompt.execute_prompt(func, message)\n\t        completion = openai.ChatCompletion.create(model=self.model_name, messages=execute_prompt_info)\n\t        param_info = completion.choices[0].message.content\n\t        if ASK_API_DEBUG:\n", "            logging.info(\"execute_param: {}\".format(param_info))\n\t        return param_info\n\t    def return_value(self, func, message) -> str:\n\t        return_prompt_info = self.prompt.return_prompt(func, message)\n\t        completion = openai.ChatCompletion.create(model=self.model_name, messages=return_prompt_info)\n\t        return_info = completion.choices[0].message.content\n\t        if ASK_API_DEBUG:\n\t            logging.info(\"return_value: {}\".format(return_info))\n\t        return return_info\n\t    def exception(self, func, message) -> str:\n", "        exception_prompt_info = self.prompt.exception_prompt(func, message)\n\t        completion = openai.ChatCompletion.create(model=self.model_name, messages=exception_prompt_info)\n\t        exception_info = completion.choices[0].message.content\n\t        if ASK_API_DEBUG:\n\t            logging.info(\"exception: {}\".format(exception_info))\n\t        return exception_info\n\t    def free_answer(self, func, message) -> str:\n\t        free_prompt_info = self.prompt.free_prompt(func, message)\n\t        completion = openai.ChatCompletion.create(model=self.model_name, messages=free_prompt_info)\n\t        free_answer = completion.choices[0].message.content\n", "        if ASK_API_DEBUG:\n\t            logging.info(\"free_answer: {}\".format(free_answer))\n\t        return free_answer\n\tclass OpenAIChatBuilder(LLMBuilder):\n\t    @staticmethod\n\t    def builder_name():\n\t        return \"openai_chat\"\n\t    \"\"\"\n\t    OpenAI Chat Builder\n\t    \"\"\"\n", "    def __init__(self, name: str = \"openai_chat\", model_name=DEFAULT_OPENAI_CHAT_MODEL) -> None:\n\t        super().__init__(name)\n\t        self.model_name = model_name\n\t    def __call__(self, prompt_config: Dict) -> LLMBase:\n\t        \"\"\"\n\t        :param prompt_config:\n\t        :return:\n\t        \"\"\"\n\t        prompt = OpenAIChatPrompt(prompt_config)\n\t        return OpenAIChat(prompt, model_name=self.model_name)\n"]}
{"filename": "src/ask_api/llm/__init__.py", "chunked_list": []}
{"filename": "src/ask_api/llm/llm_util.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\tllm 工具函数\n\t\"\"\"\n\tfrom typing import Dict\n\tfrom ask_api.util.askapi_log import logging\n\tfrom ask_api.config.askapi_config import DEFAULT_LLM_MODEL, LANG\n\tfrom ask_api.prompt.basic import ALL_BASIC_PROMPTS\n\tfrom .base import LLMBase, LLMBuilder\n\tLLM_BUILDERS = {}\n", "def register_llm_builder(name: str):\n\t    \"\"\"\n\t    注册 LLM builder\n\t    :param name:\n\t    :return:\n\t    \"\"\"\n\t    def _register_llm_builder(cls: LLMBuilder) -> LLMBuilder:\n\t        if name in LLM_BUILDERS:\n\t            raise ValueError(f\"Cannot register duplicate LLM builder ({name})\")\n\t        if not issubclass(cls.__class__, LLMBuilder):\n", "            raise ValueError(f\"LLM builder ({name}) must extend LLMBuilder\")\n\t        LLM_BUILDERS[name] = cls\n\t        return cls\n\t    return _register_llm_builder\n\tdef build_llm(name: str, prompt_config: Dict) -> LLMBase:\n\t    \"\"\"\n\t    构建 LLM\n\t    :param name:\n\t    :param prompt_config:\n\t    :return:\n", "    \"\"\"\n\t    if name not in LLM_BUILDERS:\n\t        raise ValueError(f\"Unregistered LLM builder ({name})\")\n\t    return LLM_BUILDERS[name](prompt_config)\n\tdef list_llm() -> list:\n\t    \"\"\"\n\t    获取所有的 LLM\n\t    :return:\n\t    \"\"\"\n\t    return list(LLM_BUILDERS.keys())\n", "# 注册各个 LLM Builder\n\t# 注册 OpenAIChatBuilder\n\ttry:\n\t    from .openai import OpenAIChatBuilder\n\t    register_llm_builder(OpenAIChatBuilder.builder_name())(OpenAIChatBuilder())\n\texcept ImportError:\n\t    logging.warning(\"OpenAIChatBuilder is not available, please install openai package\")\n\t    pass\n\texcept Exception as e:\n\t    logging.warning(\"OpenAIChatBuilder is not available, error: {}\".format(e))\n", "    pass\n\t# 构造 Default LLM\n\tdef build_default_llm() -> LLMBase:\n\t    \"\"\"\n\t    构造 Default LLM\n\t    :return:\n\t    \"\"\"\n\t    try:\n\t        prompt_config = ALL_BASIC_PROMPTS[LANG]\n\t    except KeyError:\n", "        raise ValueError(f\"Unsupported language ({LANG})\")\n\t    default_llm_name = DEFAULT_LLM_MODEL\n\t    return build_llm(default_llm_name, prompt_config)\n"]}
{"filename": "src/ask_api/util/askapi_util.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\tutil functions for askapi\n\t\"\"\"\n\tdef get_json_from_text(text: str):\n\t    \"\"\"\n\t    get json from text\n\t    \"\"\"\n\t    import json\n\t    json_start = text.find(\"{\")\n", "    json_end = text.rfind(\"}\")\n\t    if json_start == -1 or json_end == -1:\n\t        raise ValueError(\"json not found\")\n\t    else:\n\t        text = text[json_start:json_end + 1]\n\t        return json.loads(text)\n"]}
{"filename": "src/ask_api/util/askapi_log.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\tlog setting\n\t\"\"\"\n\timport logging\n\tlogging.basicConfig(level=logging.INFO,\n\t                    format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')\n"]}
{"filename": "src/ask_api/util/__init__.py", "chunked_list": []}
{"filename": "src/ask_api/util/askapi_inspect.py", "chunked_list": ["# -*- coding：utf-8 -*-\n\t\"\"\"\n\taskapi inspect\n\t\"\"\"\n\timport inspect\n\tfrom ask_api.config.askapi_config import MAX_FUNC_LEN\n\tdef get_func_name(func):\n\t    \"\"\"\n\t    get function name\n\t    \"\"\"\n", "    return func.__name__\n\tdef get_func_desc(func):\n\t    \"\"\"\n\t    get function desc\n\t    \"\"\"\n\t    return inspect.getdoc(func)\n\tdef get_func_source(func):\n\t    \"\"\"\n\t    get function source\n\t    \"\"\"\n", "    source = inspect.getsource(func)\n\t    # 超过 MAX_FUNC_LEN 个字符时，需要进行截断，保留前后各 MAX_FUNC_LEN/2 个字符\n\t    if len(source) > MAX_FUNC_LEN:\n\t        source = source[:int(MAX_FUNC_LEN/2)] + \"\\n...\\n\" + source[-int(MAX_FUNC_LEN/2):]\n\t    return source\n"]}
{"filename": "src/ask_api/util/askapi_asyn.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\t异步调用函数相关工具\n\t\"\"\"\n\timport nest_asyncio\n\tnest_asyncio.apply()\n\timport asyncio\n\tloop = asyncio.get_event_loop()\n\tasyncio.set_event_loop(loop)\n\tdef run_async_task(func):\n", "    \"\"\"\n\t    异步调用函数\n\t    \"\"\"\n\t    task = loop.create_task(func)\n\t    return task\n\tdef close_loop():\n\t    \"\"\"\n\t    关闭loop\n\t    \"\"\"\n\t    loop.close()\n", "def wait_task(task):\n\t    \"\"\"\n\t    等待task执行完成\n\t    \"\"\"\n\t    loop.run_until_complete(task)\n\t    return task.result()\n"]}
{"filename": "src/ask_api/base/role.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\taskapi role\n\t\"\"\"\n\tfrom ask_api.llm.base import LLMBase\n\tfrom ask_api.util.askapi_log import logging\n\tfrom ask_api.util.askapi_asyn import run_async_task, wait_task\n\tfrom ask_api.util.askapi_util import get_json_from_text\n\tfrom .session import Message, Session\n\timport json\n", "class Role(object):\n\t    def __init__(self, name) -> None:\n\t        self.name = name\n\t    def __str__(self) -> str:\n\t        return self.name\n\t    def __repr__(self) -> str:\n\t        return self.name\n\t    def answer(self, message: str, mode: str = 'free', session: Session = None) -> Message:\n\t        raise NotImplementedError(\"answer() is not implemented\")\n\t    def ask(self) -> str:\n", "        raise NotImplementedError(\"ask() is not implemented\")\n\tclass FunctionRole(Role):\n\t    \"\"\"\n\t    函数角色\n\t    Args:\n\t        Role (_type_): _description_\n\t    \"\"\"\n\t    def __init__(self, llm: LLMBase, func, name: str = None) -> None:\n\t        if name:\n\t            super().__init__(name)\n", "        else:\n\t            super().__init__(func.__name__)\n\t        self.func = func\n\t        self.llm = llm\n\t        self.desc = None\n\t    def answer(self, message: Message, mode: str = 'free', session: Session = None) -> Message:\n\t        \"\"\"\n\t        Args:\n\t            message (Message): message info\n\t            mode (str, optional): _description_. Defaults to 'free'.\n", "            session (Session, optional): _description_. Defaults to None.\n\t        Raises:\n\t            ValueError: _description_\n\t        Returns:\n\t            _type_: _description_\n\t        \"\"\"\n\t        if mode == \"desc\":\n\t            return self.ask()\n\t        elif mode == \"execute\":\n\t            return self.execute(message, session)\n", "        elif mode == \"free\":\n\t            answer = self.llm.free_answer(self.func, message)\n\t            return Message(role=self, text=answer)\n\t        else:\n\t            raise ValueError(\"mode must be one of desc, execute, free\")\n\t    def execute(self, message: Message, session: Session = None) -> Message:\n\t        \"\"\"\n\t        执行任务\n\t        Args:\n\t            message (str): a str message\n", "            session (Session, optional): _description_. Defaults to None.\n\t        Raises:\n\t            ValueError: _description_\n\t        Returns:\n\t            _type_: _description_\n\t        \"\"\"\n\t        param_json = self.llm.execute_param(self.func, message)\n\t        try:\n\t            param_dic = get_json_from_text(param_json)\n\t        except Exception as e:\n", "            return Message(role=self, text=param_json)\n\t        # 异步执行函数\n\t        async def execute():\n\t            try:\n\t                ret = self.func(**param_dic)\n\t                answer = self.llm.return_value(self.func, ret)\n\t                async_message = Message(role=self, text=answer)\n\t            except Exception as ex:\n\t                ex_desc = self.llm.exception(self.func, str(ex))\n\t                ex_message = self.llm.exception_message().format(message=ex_desc)\n", "                async_message = Message(role=self, text=ex_message)\n\t            if session:\n\t                session.add_message(async_message)\n\t            return async_message\n\t        task = run_async_task(execute())\n\t        execute_message = self.llm.execute_message()\n\t        return Message(role=self, text=execute_message, task=task)\n\t    def ask(self) -> Message:\n\t        \"\"\"\n\t        Returns:\n", "            str: _description_\n\t        \"\"\"\n\t        if not self.desc:\n\t            self.desc = self.llm.desc(self.func)\n\t        text = self.desc\n\t        return Message(role=self, text=text)\n\tclass UserRole(Role):\n\t    \"\"\"\n\t    用户角色（虚拟角色，只是用于占位，不进行任何调用）\n\t    Args:\n", "        Role (_type_): _description_\n\t    \"\"\"\n\t    def __init__(self, name) -> None:\n\t        super().__init__(name)\n\t    def answer(self, message: str, mode: str = 'free', session: Session = None) -> Message:\n\t        \"\"\"\n\t        Args:\n\t            message (str): _description_\n\t            mode (str, optional): _description_. Defaults to 'free'.\n\t            session (Session, optional): _description_. Defaults to None.\n", "        Raises:\n\t            ValueError: _description_\n\t        Returns:\n\t            _type_: _description_\n\t        \"\"\"\n\t        raise ValueError(\"user role can not answer\")\n\t    def ask(self) -> str:\n\t        \"\"\"\n\t        Returns:\n\t            str: _description_\n", "        \"\"\"\n\t        raise ValueError(\"user role can not ask\")\n\tclass CLIUserRole(UserRole):\n\t    \"\"\"\n\t    命令行用户角色(用于命令行交互式提问)\n\t    Args:\n\t        UserRole (_type_): _description_\n\t    \"\"\"\n\t    def __init__(self, name) -> None:\n\t        super().__init__(name)\n", "    def answer(self, message: str, mode: str = 'free', session: Session = None):\n\t        \"\"\"\n\t        Args:\n\t            message (str): _description_\n\t            mode (str, optional): _description_. Defaults to 'free'\n\t            session (Session, optional): _description_. Defaults to None.\n\t        Raises:\n\t            ValueError: _description_\n\t        Returns:\n\t            _type_: _description_\n", "        \"\"\"\n\t        # 从命令行获取输入\n\t        answer = input(self.name + \":\")\n\t        return answer\n\t    def ask(self) -> str:\n\t        \"\"\"\n\t        Returns:\n\t            str: _description_\n\t        \"\"\"\n\t        message = input(self.name + \":\")\n", "        return message\n"]}
{"filename": "src/ask_api/base/__init__.py", "chunked_list": []}
{"filename": "src/ask_api/base/session.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\t会话管理，记录历史会话信息\n\t\"\"\"\n\tfrom typing import List\n\tclass Message(object):\n\t    def __init__(self, role, text, task=None) -> None:\n\t        self.role = role\n\t        self.text = text\n\t        # 异步任务\n", "        self.task = task\n\t    def __str__(self) -> str:\n\t        return str(self.role) + \": \" + self.text\n\t    def __repr__(self) -> str:\n\t        return str(self.role) + \": \" + self.text\n\t    def get_role(self):\n\t        return self.role\n\t    def get_text(self):\n\t        return self.text\n\t    def get(self):\n", "        return self.role, self.text\n\t    def set_role(self, role):\n\t        self.role = role\n\t    def set_text(self, text):\n\t        self.text = text\n\t    def set(self, role, text):\n\t        self.role = role\n\t        self.text = text\n\t    def set_task(self, task):\n\t        self.task = task\n", "    def get_task(self):\n\t        return self.task\n\tclass Session(object):\n\t    def __init__(self) -> None:\n\t        self.messages = []\n\t    def add_message(self, message: Message):\n\t        self.messages.append(message)\n\t    def get_messages(self) -> List[Message]:\n\t        return self.messages\n\t    def print_messages(self):\n", "        print(\"*\" * 40 + \"Session\" + \"*\" * 40)\n\t        for message in self.messages:\n\t            print(message)\n\t        print(\"*\" * 40 + \"Session\" + \"*\" * 40)\n\t    def get_current(self) -> Message:\n\t        return self.messages[-1]\n"]}
