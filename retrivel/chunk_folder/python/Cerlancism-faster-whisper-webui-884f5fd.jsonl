{"filename": "app-shared.py", "chunked_list": ["# Run the app with no audio file restrictions\n\tfrom app import create_ui\n\tfrom src.config import ApplicationConfig\n\tcreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1, share=True))"]}
{"filename": "app.py", "chunked_list": ["from datetime import datetime\n\timport json\n\timport math\n\tfrom typing import Iterator, Union\n\timport argparse\n\tfrom io import StringIO\n\timport os\n\timport pathlib\n\timport tempfile\n\timport zipfile\n", "import numpy as np\n\timport torch\n\tfrom src.config import VAD_INITIAL_PROMPT_MODE_VALUES, ApplicationConfig, VadInitialPromptMode\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.hooks.subTaskProgressListener import SubTaskProgressListener\n\tfrom src.hooks.whisperProgressHook import create_progress_listener_handle\n\tfrom src.languages import get_language_names\n\tfrom src.modelCache import ModelCache\n\tfrom src.prompts.jsonPromptStrategy import JsonPromptStrategy\n\tfrom src.prompts.prependPromptStrategy import PrependPromptStrategy\n", "from src.source import get_audio_source_collection\n\tfrom src.vadParallel import ParallelContext, ParallelTranscription\n\t# External programs\n\timport ffmpeg\n\t# UI\n\timport gradio as gr\n\tfrom src.download import ExceededMaximumDuration, download_url\n\tfrom src.utils import optional_int, slugify, write_srt, write_vtt\n\tfrom src.vad import AbstractTranscription, NonSpeechStrategy, PeriodicTranscriptionConfig, TranscriptionConfig, VadPeriodicTranscription, VadSileroTranscription\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperContainer\n", "from src.whisper.whisperFactory import create_whisper_container\n\t# Configure more application defaults in config.json5\n\t# Gradio seems to truncate files without keeping the extension, so we need to truncate the file prefix ourself \n\tMAX_FILE_PREFIX_LENGTH = 17\n\t# Limit auto_parallel to a certain number of CPUs (specify vad_cpu_cores to get a higher number)\n\tMAX_AUTO_CPU_CORES = 8\n\tWHISPER_MODELS = [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v1\", \"large-v2\"]\n\tclass VadOptions:\n\t    def __init__(self, vad: str = None, vadMergeWindow: float = 5, vadMaxMergeSize: float = 150, vadPadding: float = 1, vadPromptWindow: float = 1, \n\t                                        vadInitialPromptMode: Union[VadInitialPromptMode, str] = VadInitialPromptMode.PREPREND_FIRST_SEGMENT):\n", "        self.vad = vad\n\t        self.vadMergeWindow = vadMergeWindow\n\t        self.vadMaxMergeSize = vadMaxMergeSize\n\t        self.vadPadding = vadPadding\n\t        self.vadPromptWindow = vadPromptWindow\n\t        self.vadInitialPromptMode = vadInitialPromptMode if isinstance(vadInitialPromptMode, VadInitialPromptMode) \\\n\t                                        else VadInitialPromptMode.from_string(vadInitialPromptMode)\n\tclass WhisperTranscriber:\n\t    def __init__(self, input_audio_max_duration: float = None, vad_process_timeout: float = None, \n\t                 vad_cpu_cores: int = 1, delete_uploaded_files: bool = False, output_dir: str = None, \n", "                 app_config: ApplicationConfig = None):\n\t        self.model_cache = ModelCache()\n\t        self.parallel_device_list = None\n\t        self.gpu_parallel_context = None\n\t        self.cpu_parallel_context = None\n\t        self.vad_process_timeout = vad_process_timeout\n\t        self.vad_cpu_cores = vad_cpu_cores\n\t        self.vad_model = None\n\t        self.inputAudioMaxDuration = input_audio_max_duration\n\t        self.deleteUploadedFiles = delete_uploaded_files\n", "        self.output_dir = output_dir\n\t        self.app_config = app_config\n\t    def set_parallel_devices(self, vad_parallel_devices: str):\n\t        self.parallel_device_list = [ device.strip() for device in vad_parallel_devices.split(\",\") ] if vad_parallel_devices else None\n\t    def set_auto_parallel(self, auto_parallel: bool):\n\t        if auto_parallel:\n\t            if torch.cuda.is_available():\n\t                self.parallel_device_list = [ str(gpu_id) for gpu_id in range(torch.cuda.device_count())]\n\t            self.vad_cpu_cores = min(os.cpu_count(), MAX_AUTO_CPU_CORES)\n\t            print(\"[Auto parallel] Using GPU devices \" + str(self.parallel_device_list) + \" and \" + str(self.vad_cpu_cores) + \" CPU cores for VAD/transcription.\")\n", "    # Entry function for the simple tab\n\t    def transcribe_webui_simple(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                                vad, vadMergeWindow, vadMaxMergeSize, \n\t                                word_timestamps: bool = False, highlight_words: bool = False):\n\t        return self.transcribe_webui_simple_progress(modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                                                     vad, vadMergeWindow, vadMaxMergeSize, \n\t                                                     word_timestamps, highlight_words)\n\t    # Entry function for the simple tab progress\n\t    def transcribe_webui_simple_progress(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                                         vad, vadMergeWindow, vadMaxMergeSize, \n", "                                         word_timestamps: bool = False, highlight_words: bool = False, \n\t                                         progress=gr.Progress()):\n\t        vadOptions = VadOptions(vad, vadMergeWindow, vadMaxMergeSize, self.app_config.vad_padding, self.app_config.vad_prompt_window, self.app_config.vad_initial_prompt_mode)\n\t        return self.transcribe_webui(modelName, languageName, urlData, multipleFiles, microphoneData, task, vadOptions, \n\t                                     word_timestamps=word_timestamps, highlight_words=highlight_words, progress=progress)\n\t    # Entry function for the full tab\n\t    def transcribe_webui_full(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                              vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode, \n\t                              # Word timestamps\n\t                              word_timestamps: bool, highlight_words: bool, prepend_punctuations: str, append_punctuations: str,\n", "                              initial_prompt: str, temperature: float, best_of: int, beam_size: int, patience: float, length_penalty: float, suppress_tokens: str, \n\t                              condition_on_previous_text: bool, fp16: bool, temperature_increment_on_fallback: float, \n\t                              compression_ratio_threshold: float, logprob_threshold: float, no_speech_threshold: float):\n\t        return self.transcribe_webui_full_progress(modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                                vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode,\n\t                                word_timestamps, highlight_words, prepend_punctuations, append_punctuations,\n\t                                initial_prompt, temperature, best_of, beam_size, patience, length_penalty, suppress_tokens,\n\t                                condition_on_previous_text, fp16, temperature_increment_on_fallback,\n\t                                compression_ratio_threshold, logprob_threshold, no_speech_threshold)\n\t    # Entry function for the full tab with progress\n", "    def transcribe_webui_full_progress(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                                        vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode,\n\t                                        # Word timestamps\n\t                                        word_timestamps: bool, highlight_words: bool, prepend_punctuations: str, append_punctuations: str,   \n\t                                        initial_prompt: str, temperature: float, best_of: int, beam_size: int, patience: float, length_penalty: float, suppress_tokens: str, \n\t                                        condition_on_previous_text: bool, fp16: bool, temperature_increment_on_fallback: float, \n\t                                        compression_ratio_threshold: float, logprob_threshold: float, no_speech_threshold: float, \n\t                                        progress=gr.Progress()):\n\t        # Handle temperature_increment_on_fallback\n\t        if temperature_increment_on_fallback is not None:\n", "            temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n\t        else:\n\t            temperature = [temperature]\n\t        vadOptions = VadOptions(vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode)\n\t        return self.transcribe_webui(modelName, languageName, urlData, multipleFiles, microphoneData, task, vadOptions,\n\t                                     initial_prompt=initial_prompt, temperature=temperature, best_of=best_of, beam_size=beam_size, patience=patience, length_penalty=length_penalty, suppress_tokens=suppress_tokens,\n\t                                     condition_on_previous_text=condition_on_previous_text, fp16=fp16,\n\t                                     compression_ratio_threshold=compression_ratio_threshold, logprob_threshold=logprob_threshold, no_speech_threshold=no_speech_threshold, \n\t                                     word_timestamps=word_timestamps, prepend_punctuations=prepend_punctuations, append_punctuations=append_punctuations, highlight_words=highlight_words,\n\t                                     progress=progress)\n", "    def transcribe_webui(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                         vadOptions: VadOptions, progress: gr.Progress = None, highlight_words: bool = False, \n\t                         **decodeOptions: dict):\n\t        try:\n\t            sources = self.__get_source(urlData, multipleFiles, microphoneData)\n\t            try:\n\t                selectedLanguage = languageName.lower() if len(languageName) > 0 else None\n\t                selectedModel = modelName if modelName is not None else \"base\"\n\t                model = create_whisper_container(whisper_implementation=self.app_config.whisper_implementation, \n\t                                                 model_name=selectedModel, compute_type=self.app_config.compute_type, \n", "                                                 cache=self.model_cache, models=self.app_config.models)\n\t                # Result\n\t                download = []\n\t                zip_file_lookup = {}\n\t                text = \"\"\n\t                vtt = \"\"\n\t                # Write result\n\t                downloadDirectory = tempfile.mkdtemp()\n\t                source_index = 0\n\t                outputDirectory = self.output_dir if self.output_dir is not None else downloadDirectory\n", "                # Progress\n\t                total_duration = sum([source.get_audio_duration() for source in sources])\n\t                current_progress = 0\n\t                # A listener that will report progress to Gradio\n\t                root_progress_listener = self._create_progress_listener(progress)\n\t                # Execute whisper\n\t                for source in sources:\n\t                    source_prefix = \"\"\n\t                    source_audio_duration = source.get_audio_duration()\n\t                    if (len(sources) > 1):\n", "                        # Prefix (minimum 2 digits)\n\t                        source_index += 1\n\t                        source_prefix = str(source_index).zfill(2) + \"_\"\n\t                        print(\"Transcribing \", source.source_path)\n\t                    scaled_progress_listener = SubTaskProgressListener(root_progress_listener, \n\t                                                   base_task_total=total_duration,\n\t                                                   sub_task_start=current_progress,\n\t                                                   sub_task_total=source_audio_duration)\n\t                    # Transcribe\n\t                    result = self.transcribe_file(model, source.source_path, selectedLanguage, task, vadOptions, scaled_progress_listener, **decodeOptions)\n", "                    filePrefix = slugify(source_prefix + source.get_short_name(), allow_unicode=True)\n\t                    # Update progress\n\t                    current_progress += source_audio_duration\n\t                    source_download, source_text, source_vtt = self.write_result(result, filePrefix, outputDirectory, highlight_words)\n\t                    if len(sources) > 1:\n\t                        # Add new line separators\n\t                        if (len(source_text) > 0):\n\t                            source_text += os.linesep + os.linesep\n\t                        if (len(source_vtt) > 0):\n\t                            source_vtt += os.linesep + os.linesep\n", "                        # Append file name to source text too\n\t                        source_text = source.get_full_name() + \":\" + os.linesep + source_text\n\t                        source_vtt = source.get_full_name() + \":\" + os.linesep + source_vtt\n\t                    # Add to result\n\t                    download.extend(source_download)\n\t                    text += source_text\n\t                    vtt += source_vtt\n\t                    if (len(sources) > 1):\n\t                        # Zip files support at least 260 characters, but we'll play it safe and use 200\n\t                        zipFilePrefix = slugify(source_prefix + source.get_short_name(max_length=200), allow_unicode=True)\n", "                        # File names in ZIP file can be longer\n\t                        for source_download_file in source_download:\n\t                            # Get file postfix (after last -)\n\t                            filePostfix = os.path.basename(source_download_file).split(\"-\")[-1]\n\t                            zip_file_name = zipFilePrefix + \"-\" + filePostfix\n\t                            zip_file_lookup[source_download_file] = zip_file_name\n\t                # Create zip file from all sources\n\t                if len(sources) > 1:\n\t                    downloadAllPath = os.path.join(downloadDirectory, \"All_Output-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \".zip\")\n\t                    with zipfile.ZipFile(downloadAllPath, 'w', zipfile.ZIP_DEFLATED) as zip:\n", "                        for download_file in download:\n\t                            # Get file name from lookup\n\t                            zip_file_name = zip_file_lookup.get(download_file, os.path.basename(download_file))\n\t                            zip.write(download_file, arcname=zip_file_name)\n\t                    download.insert(0, downloadAllPath)\n\t                return download, text, vtt\n\t            finally:\n\t                # Cleanup source\n\t                if self.deleteUploadedFiles:\n\t                    for source in sources:\n", "                        print(\"Deleting source file \" + source.source_path)\n\t                        try:\n\t                            os.remove(source.source_path)\n\t                        except Exception as e:\n\t                            # Ignore error - it's just a cleanup\n\t                            print(\"Error deleting source file \" + source.source_path + \": \" + str(e))\n\t        except ExceededMaximumDuration as e:\n\t            return [], (\"[ERROR]: Maximum remote video length is \" + str(e.maxDuration) + \"s, file was \" + str(e.videoDuration) + \"s\"), \"[ERROR]\"\n\t    def transcribe_file(self, model: AbstractWhisperContainer, audio_path: str, language: str, task: str = None, \n\t                        vadOptions: VadOptions = VadOptions(), \n", "                        progressListener: ProgressListener = None, **decodeOptions: dict):\n\t        initial_prompt = decodeOptions.pop('initial_prompt', None)\n\t        if progressListener is None:\n\t            # Default progress listener\n\t            progressListener = ProgressListener()\n\t        if ('task' in decodeOptions):\n\t            task = decodeOptions.pop('task')\n\t        print(\"[WhisperTranscriber] Task\", task)\n\t        print(\"[WhisperTranscriber] Model\", model.model_name)\n\t        initial_prompt_mode = vadOptions.vadInitialPromptMode\n", "        # Set default initial prompt mode\n\t        if (initial_prompt_mode is None):\n\t            initial_prompt_mode = VadInitialPromptMode.PREPREND_FIRST_SEGMENT\n\t        if (initial_prompt_mode == VadInitialPromptMode.PREPEND_ALL_SEGMENTS or \n\t            initial_prompt_mode == VadInitialPromptMode.PREPREND_FIRST_SEGMENT):\n\t            # Prepend initial prompt\n\t            prompt_strategy = PrependPromptStrategy(initial_prompt, initial_prompt_mode)\n\t        elif (vadOptions.vadInitialPromptMode == VadInitialPromptMode.JSON_PROMPT_MODE):\n\t            # Use a JSON format to specify the prompt for each segment\n\t            prompt_strategy = JsonPromptStrategy(initial_prompt)\n", "        else:\n\t            raise ValueError(\"Invalid vadInitialPromptMode: \" + initial_prompt_mode)\n\t        # Callable for processing an audio file\n\t        whisperCallable = model.create_callback(language, task, prompt_strategy=prompt_strategy, **decodeOptions)\n\t        # The results\n\t        if (vadOptions.vad == 'silero-vad'):\n\t            # Silero VAD where non-speech gaps are transcribed\n\t            process_gaps = self._create_silero_config(NonSpeechStrategy.CREATE_SEGMENT, vadOptions)\n\t            result = self.process_vad(audio_path, whisperCallable, self.vad_model, process_gaps, progressListener=progressListener)\n\t        elif (vadOptions.vad == 'silero-vad-skip-gaps'):\n", "            # Silero VAD where non-speech gaps are simply ignored\n\t            skip_gaps = self._create_silero_config(NonSpeechStrategy.SKIP, vadOptions)\n\t            result = self.process_vad(audio_path, whisperCallable, self.vad_model, skip_gaps, progressListener=progressListener)\n\t        elif (vadOptions.vad == 'silero-vad-expand-into-gaps'):\n\t            # Use Silero VAD where speech-segments are expanded into non-speech gaps\n\t            expand_gaps = self._create_silero_config(NonSpeechStrategy.EXPAND_SEGMENT, vadOptions)\n\t            result = self.process_vad(audio_path, whisperCallable, self.vad_model, expand_gaps, progressListener=progressListener)\n\t        elif (vadOptions.vad == 'periodic-vad'):\n\t            # Very simple VAD - mark every 5 minutes as speech. This makes it less likely that Whisper enters an infinite loop, but\n\t            # it may create a break in the middle of a sentence, causing some artifacts.\n", "            periodic_vad = VadPeriodicTranscription()\n\t            period_config = PeriodicTranscriptionConfig(periodic_duration=vadOptions.vadMaxMergeSize, max_prompt_window=vadOptions.vadPromptWindow)\n\t            result = self.process_vad(audio_path, whisperCallable, periodic_vad, period_config, progressListener=progressListener)\n\t        else:\n\t            if (self._has_parallel_devices()):\n\t                # Use a simple period transcription instead, as we need to use the parallel context\n\t                periodic_vad = VadPeriodicTranscription()\n\t                period_config = PeriodicTranscriptionConfig(periodic_duration=math.inf, max_prompt_window=1)\n\t                result = self.process_vad(audio_path, whisperCallable, periodic_vad, period_config, progressListener=progressListener)\n\t            else:\n", "                # Default VAD\n\t                result = whisperCallable.invoke(audio_path, 0, None, None, progress_listener=progressListener)\n\t        return result\n\t    def _create_progress_listener(self, progress: gr.Progress):\n\t        if (progress is None):\n\t            # Dummy progress listener\n\t            return ProgressListener()\n\t        class ForwardingProgressListener(ProgressListener):\n\t            def __init__(self, progress: gr.Progress):\n\t                self.progress = progress\n", "            def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t                # From 0 to 1\n\t                self.progress(current / total)\n\t            def on_finished(self):\n\t                self.progress(1)\n\t        return ForwardingProgressListener(progress)\n\t    def process_vad(self, audio_path, whisperCallable, vadModel: AbstractTranscription, vadConfig: TranscriptionConfig, \n\t                    progressListener: ProgressListener = None):\n\t        if (not self._has_parallel_devices()):\n\t            # No parallel devices, so just run the VAD and Whisper in sequence\n", "            return vadModel.transcribe(audio_path, whisperCallable, vadConfig, progressListener=progressListener)\n\t        gpu_devices = self.parallel_device_list\n\t        if (gpu_devices is None or len(gpu_devices) == 0):\n\t            # No GPU devices specified, pass the current environment variable to the first GPU process. This may be NULL.\n\t            gpu_devices = [os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)]\n\t        # Create parallel context if needed\n\t        if (self.gpu_parallel_context is None):\n\t            # Create a context wih processes and automatically clear the pool after 1 hour of inactivity\n\t            self.gpu_parallel_context = ParallelContext(num_processes=len(gpu_devices), auto_cleanup_timeout_seconds=self.vad_process_timeout)\n\t        # We also need a CPU context for the VAD\n", "        if (self.cpu_parallel_context is None):\n\t            self.cpu_parallel_context = ParallelContext(num_processes=self.vad_cpu_cores, auto_cleanup_timeout_seconds=self.vad_process_timeout)\n\t        parallel_vad = ParallelTranscription()\n\t        return parallel_vad.transcribe_parallel(transcription=vadModel, audio=audio_path, whisperCallable=whisperCallable,  \n\t                                                config=vadConfig, cpu_device_count=self.vad_cpu_cores, gpu_devices=gpu_devices, \n\t                                                cpu_parallel_context=self.cpu_parallel_context, gpu_parallel_context=self.gpu_parallel_context, \n\t                                                progress_listener=progressListener) \n\t    def _has_parallel_devices(self):\n\t        return (self.parallel_device_list is not None and len(self.parallel_device_list) > 0) or self.vad_cpu_cores > 1\n\t    def _concat_prompt(self, prompt1, prompt2):\n", "        if (prompt1 is None):\n\t            return prompt2\n\t        elif (prompt2 is None):\n\t            return prompt1\n\t        else:\n\t            return prompt1 + \" \" + prompt2\n\t    def _create_silero_config(self, non_speech_strategy: NonSpeechStrategy, vadOptions: VadOptions):\n\t        # Use Silero VAD \n\t        if (self.vad_model is None):\n\t            self.vad_model = VadSileroTranscription()\n", "        config = TranscriptionConfig(non_speech_strategy = non_speech_strategy, \n\t                max_silent_period=vadOptions.vadMergeWindow, max_merge_size=vadOptions.vadMaxMergeSize, \n\t                segment_padding_left=vadOptions.vadPadding, segment_padding_right=vadOptions.vadPadding, \n\t                max_prompt_window=vadOptions.vadPromptWindow)\n\t        return config\n\t    def write_result(self, result: dict, source_name: str, output_dir: str, highlight_words: bool = False, extras: str = \"\"):\n\t        if not os.path.exists(output_dir):\n\t            os.makedirs(output_dir)\n\t        text = result[\"text\"]\n\t        language = result[\"language\"]\n", "        languageMaxLineWidth = self.__get_max_line_width(language)\n\t        print(\"Max line width \" + str(languageMaxLineWidth))\n\t        vtt = self.__get_subs(result[\"segments\"], \"vtt\", languageMaxLineWidth, highlight_words=highlight_words)\n\t        srt = self.__get_subs(result[\"segments\"], \"srt\", languageMaxLineWidth, highlight_words=highlight_words)\n\t        json_result = json.dumps(result, indent=4, ensure_ascii=False)\n\t        output_name = source_name + extras \n\t        output_files = []\n\t        output_files.append(self.__create_file(srt, output_dir, output_name + \"-subs.srt\"));\n\t        output_files.append(self.__create_file(vtt, output_dir, output_name + \"-subs.vtt\"));\n\t        output_files.append(self.__create_file(text, output_dir, output_name + \"-transcript.txt\"));\n", "        output_files.append(self.__create_file(json_result, output_dir, output_name + \"-result.json\"));\n\t        return output_files, text, vtt\n\t    def clear_cache(self):\n\t        self.model_cache.clear()\n\t        self.vad_model = None\n\t    def __get_source(self, urlData, multipleFiles, microphoneData):\n\t        return get_audio_source_collection(urlData, multipleFiles, microphoneData, self.inputAudioMaxDuration)\n\t    def __get_max_line_width(self, language: str) -> int:\n\t        if (language and language.lower() in [\"japanese\", \"ja\", \"chinese\", \"zh\"]):\n\t            # Chinese characters and kana are wider, so limit line length to 40 characters\n", "            return 40\n\t        else:\n\t            # TODO: Add more languages\n\t            # 80 latin characters should fit on a 1080p/720p screen\n\t            return 80\n\t    def __get_subs(self, segments: Iterator[dict], format: str, maxLineWidth: int, highlight_words: bool = False) -> str:\n\t        segmentStream = StringIO()\n\t        if format == 'vtt':\n\t            write_vtt(segments, file=segmentStream, maxLineWidth=maxLineWidth, highlight_words=highlight_words)\n\t        elif format == 'srt':\n", "            write_srt(segments, file=segmentStream, maxLineWidth=maxLineWidth, highlight_words=highlight_words)\n\t        else:\n\t            raise Exception(\"Unknown format \" + format)\n\t        segmentStream.seek(0)\n\t        return segmentStream.read()\n\t    def __create_file(self, text: str, directory: str, fileName: str) -> str:\n\t        # Write the text to a file\n\t        with open(os.path.join(directory, fileName), 'w+', encoding=\"utf-8\") as file:\n\t            file.write(text)\n\t        return file.name\n", "    def close(self):\n\t        print(\"Closing parallel contexts\")\n\t        self.clear_cache()\n\t        if (self.gpu_parallel_context is not None):\n\t            self.gpu_parallel_context.close()\n\t        if (self.cpu_parallel_context is not None):\n\t            self.cpu_parallel_context.close()\n\tdef create_ui(app_config: ApplicationConfig):\n\t    ui = WhisperTranscriber(app_config.input_audio_max_duration, app_config.vad_process_timeout, app_config.vad_cpu_cores, \n\t                            app_config.delete_uploaded_files, app_config.output_dir, app_config)\n", "    # Specify a list of devices to use for parallel processing\n\t    ui.set_parallel_devices(app_config.vad_parallel_devices)\n\t    ui.set_auto_parallel(app_config.auto_parallel)\n\t    is_whisper = False\n\t    if app_config.whisper_implementation == \"whisper\":\n\t        implementation_name = \"Whisper\"\n\t        is_whisper = True\n\t    elif app_config.whisper_implementation in [\"faster-whisper\", \"faster_whisper\"]:\n\t        implementation_name = \"Faster Whisper\"\n\t    else:\n", "        # Try to convert from camel-case to title-case\n\t        implementation_name = app_config.whisper_implementation.title().replace(\"_\", \" \").replace(\"-\", \" \")\n\t    ui_description = implementation_name + \" is a general-purpose speech recognition model. It is trained on a large dataset of diverse \" \n\t    ui_description += \" audio and is also a multi-task model that can perform multilingual speech recognition \"\n\t    ui_description += \" as well as speech translation and language identification. \"\n\t    ui_description += \"\\n\\n\\n\\nFor longer audio files (>10 minutes) not in English, it is recommended that you select Silero VAD (Voice Activity Detector) in the VAD option.\"\n\t    # Recommend faster-whisper\n\t    if is_whisper:\n\t        ui_description += \"\\n\\n\\n\\nFor faster inference on GPU, try [faster-whisper](https://huggingface.co/spaces/aadnk/faster-whisper-webui).\"\n\t    if app_config.input_audio_max_duration > 0:\n", "        ui_description += \"\\n\\n\" + \"Max audio file length: \" + str(app_config.input_audio_max_duration) + \" s\"\n\t    ui_article = \"Read the [documentation here](https://gitlab.com/aadnk/whisper-webui/-/blob/main/docs/options.md).\"\n\t    whisper_models = app_config.get_model_names()\n\t    common_inputs = lambda : [\n\t        gr.Dropdown(choices=whisper_models, value=app_config.default_model_name, label=\"Model\"),\n\t        gr.Dropdown(choices=sorted(get_language_names()), label=\"Language\", value=app_config.language),\n\t        gr.Text(label=\"URL (YouTube, etc.)\"),\n\t        gr.File(label=\"Upload Files\", file_count=\"multiple\"),\n\t        gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Microphone Input\"),\n\t        gr.Dropdown(choices=[\"transcribe\", \"translate\"], label=\"Task\", value=app_config.task),\n", "    ]\n\t    common_vad_inputs = lambda : [\n\t        gr.Dropdown(choices=[\"none\", \"silero-vad\", \"silero-vad-skip-gaps\", \"silero-vad-expand-into-gaps\", \"periodic-vad\"], value=app_config.default_vad, label=\"VAD\"),\n\t        gr.Number(label=\"VAD - Merge Window (s)\", precision=0, value=app_config.vad_merge_window),\n\t        gr.Number(label=\"VAD - Max Merge Size (s)\", precision=0, value=app_config.vad_max_merge_size),\n\t    ]\n\t    common_word_timestamps_inputs = lambda : [\n\t        gr.Checkbox(label=\"Word Timestamps\", value=app_config.word_timestamps),\n\t        gr.Checkbox(label=\"Word Timestamps - Highlight Words\", value=app_config.highlight_words),\n\t    ]\n", "    is_queue_mode = app_config.queue_concurrency_count is not None and app_config.queue_concurrency_count > 0    \n\t    simple_transcribe = gr.Interface(fn=ui.transcribe_webui_simple_progress if is_queue_mode else ui.transcribe_webui_simple, \n\t                                     description=ui_description, article=ui_article, inputs=[\n\t        *common_inputs(),\n\t        *common_vad_inputs(),\n\t        *common_word_timestamps_inputs(),\n\t    ], outputs=[\n\t        gr.File(label=\"Download\"),\n\t        gr.Text(label=\"Transcription\"), \n\t        gr.Text(label=\"Segments\")\n", "    ])\n\t    full_description = ui_description + \"\\n\\n\\n\\n\" + \"Be careful when changing some of the options in the full interface - this can cause the model to crash.\"\n\t    full_transcribe = gr.Interface(fn=ui.transcribe_webui_full_progress if is_queue_mode else ui.transcribe_webui_full,\n\t                                   description=full_description, article=ui_article, inputs=[\n\t        *common_inputs(),\n\t        *common_vad_inputs(),\n\t        gr.Number(label=\"VAD - Padding (s)\", precision=None, value=app_config.vad_padding),\n\t        gr.Number(label=\"VAD - Prompt Window (s)\", precision=None, value=app_config.vad_prompt_window),\n\t        gr.Dropdown(choices=VAD_INITIAL_PROMPT_MODE_VALUES, label=\"VAD - Initial Prompt Mode\"),\n\t        *common_word_timestamps_inputs(),\n", "        gr.Text(label=\"Word Timestamps - Prepend Punctuations\", value=app_config.prepend_punctuations),\n\t        gr.Text(label=\"Word Timestamps - Append Punctuations\", value=app_config.append_punctuations),\n\t        gr.TextArea(label=\"Initial Prompt\"),\n\t        gr.Number(label=\"Temperature\", value=app_config.temperature),\n\t        gr.Number(label=\"Best Of - Non-zero temperature\", value=app_config.best_of, precision=0),\n\t        gr.Number(label=\"Beam Size - Zero temperature\", value=app_config.beam_size, precision=0),\n\t        gr.Number(label=\"Patience - Zero temperature\", value=app_config.patience),\n\t        gr.Number(label=\"Length Penalty - Any temperature\", value=app_config.length_penalty), \n\t        gr.Text(label=\"Suppress Tokens - Comma-separated list of token IDs\", value=app_config.suppress_tokens),\n\t        gr.Checkbox(label=\"Condition on previous text\", value=app_config.condition_on_previous_text),\n", "        gr.Checkbox(label=\"FP16\", value=app_config.fp16),\n\t        gr.Number(label=\"Temperature increment on fallback\", value=app_config.temperature_increment_on_fallback),\n\t        gr.Number(label=\"Compression ratio threshold\", value=app_config.compression_ratio_threshold),\n\t        gr.Number(label=\"Logprob threshold\", value=app_config.logprob_threshold),\n\t        gr.Number(label=\"No speech threshold\", value=app_config.no_speech_threshold),\n\t    ], outputs=[\n\t        gr.File(label=\"Download\"),\n\t        gr.Text(label=\"Transcription\"), \n\t        gr.Text(label=\"Segments\")\n\t    ])\n", "    demo = gr.TabbedInterface([simple_transcribe, full_transcribe], tab_names=[\"Simple\", \"Full\"])\n\t    # Queue up the demo\n\t    if is_queue_mode:\n\t        demo.queue(concurrency_count=app_config.queue_concurrency_count)\n\t        print(\"Queue mode enabled (concurrency count: \" + str(app_config.queue_concurrency_count) + \")\")\n\t    else:\n\t        print(\"Queue mode disabled - progress bars will not be shown.\")\n\t    demo.launch(share=app_config.share, server_name=app_config.server_name, server_port=app_config.server_port)\n\t    # Clean up\n\t    ui.close()\n", "if __name__ == '__main__':\n\t    default_app_config = ApplicationConfig.create_default()\n\t    whisper_models = default_app_config.get_model_names()\n\t    # Environment variable overrides\n\t    default_whisper_implementation = os.environ.get(\"WHISPER_IMPLEMENTATION\", default_app_config.whisper_implementation)\n\t    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t    parser.add_argument(\"--input_audio_max_duration\", type=int, default=default_app_config.input_audio_max_duration, \\\n\t                        help=\"Maximum audio file length in seconds, or -1 for no limit.\") # 600\n\t    parser.add_argument(\"--share\", type=bool, default=default_app_config.share, \\\n\t                        help=\"True to share the app on HuggingFace.\") # False\n", "    parser.add_argument(\"--server_name\", type=str, default=default_app_config.server_name, \\\n\t                        help=\"The host or IP to bind to. If None, bind to localhost.\") # None\n\t    parser.add_argument(\"--server_port\", type=int, default=default_app_config.server_port, \\\n\t                        help=\"The port to bind to.\") # 7860\n\t    parser.add_argument(\"--queue_concurrency_count\", type=int, default=default_app_config.queue_concurrency_count, \\\n\t                        help=\"The number of concurrent requests to process.\") # 1\n\t    parser.add_argument(\"--default_model_name\", type=str, choices=whisper_models, default=default_app_config.default_model_name, \\\n\t                        help=\"The default model name.\") # medium\n\t    parser.add_argument(\"--default_vad\", type=str, default=default_app_config.default_vad, \\\n\t                        help=\"The default VAD.\") # silero-vad\n", "    parser.add_argument(\"--vad_initial_prompt_mode\", type=str, default=default_app_config.vad_initial_prompt_mode, choices=VAD_INITIAL_PROMPT_MODE_VALUES, \\\n\t                        help=\"Whether or not to prepend the initial prompt to each VAD segment (prepend_all_segments), or just the first segment (prepend_first_segment)\") # prepend_first_segment\n\t    parser.add_argument(\"--vad_parallel_devices\", type=str, default=default_app_config.vad_parallel_devices, \\\n\t                        help=\"A commma delimited list of CUDA devices to use for parallel processing. If None, disable parallel processing.\") # \"\"\n\t    parser.add_argument(\"--vad_cpu_cores\", type=int, default=default_app_config.vad_cpu_cores, \\\n\t                        help=\"The number of CPU cores to use for VAD pre-processing.\") # 1\n\t    parser.add_argument(\"--vad_process_timeout\", type=float, default=default_app_config.vad_process_timeout, \\\n\t                        help=\"The number of seconds before inactivate processes are terminated. Use 0 to close processes immediately, or None for no timeout.\") # 1800\n\t    parser.add_argument(\"--auto_parallel\", type=bool, default=default_app_config.auto_parallel, \\\n\t                        help=\"True to use all available GPUs and CPU cores for processing. Use vad_cpu_cores/vad_parallel_devices to specify the number of CPU cores/GPUs to use.\") # False\n", "    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=default_app_config.output_dir, \\\n\t                        help=\"directory to save the outputs\")\n\t    parser.add_argument(\"--whisper_implementation\", type=str, default=default_whisper_implementation, choices=[\"whisper\", \"faster-whisper\"],\\\n\t                        help=\"the Whisper implementation to use\")\n\t    parser.add_argument(\"--compute_type\", type=str, default=default_app_config.compute_type, choices=[\"default\", \"auto\", \"int8\", \"int8_float16\", \"int16\", \"float16\", \"float32\"], \\\n\t                        help=\"the compute type to use for inference\")\n\t    parser.add_argument(\"--threads\", type=optional_int, default=0, \n\t                        help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n\t    args = parser.parse_args().__dict__\n\t    updated_config = default_app_config.update(**args)\n", "    if (threads := args.pop(\"threads\")) > 0:\n\t        torch.set_num_threads(threads)\n\t    create_ui(app_config=updated_config)"]}
{"filename": "app-network.py", "chunked_list": ["# Run the app with no audio file restrictions, and make it available on the network\n\tfrom app import create_ui\n\tfrom src.config import ApplicationConfig\n\tcreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1, server_name=\"0.0.0.0\"))"]}
{"filename": "app-local.py", "chunked_list": ["# Run the app with no audio file restrictions\n\tfrom app import create_ui\n\tfrom src.config import ApplicationConfig\n\tcreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1))"]}
{"filename": "cli.py", "chunked_list": ["import argparse\n\timport os\n\timport pathlib\n\tfrom urllib.parse import urlparse\n\timport warnings\n\timport numpy as np\n\timport threading\n\timport torch\n\tfrom app import VadOptions, WhisperTranscriber\n\tfrom src.config import VAD_INITIAL_PROMPT_MODE_VALUES, ApplicationConfig, VadInitialPromptMode\n", "from src.download import download_url\n\tfrom src.languages import get_language_names\n\tfrom src.utils import optional_float, optional_int, str2bool\n\tfrom src.whisper.whisperFactory import create_whisper_container\n\tdef cli():\n\t    app_config = ApplicationConfig.create_default()\n\t    whisper_models = app_config.get_model_names()\n\t    # For the CLI, we fallback to saving the output to the current directory\n\t    output_dir = app_config.output_dir if app_config.output_dir is not None else \".\"\n\t    # Environment variable overrides\n", "    default_whisper_implementation = os.environ.get(\"WHISPER_IMPLEMENTATION\", app_config.whisper_implementation)\n\t    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t    parser.add_argument(\"audio\", nargs=\"+\", type=str, \\\n\t                        help=\"audio file(s) to transcribe\")\n\t    parser.add_argument(\"--model\", nargs=\"+\", default=app_config.default_model_name, choices=whisper_models, \\\n\t                        help=\"name of the Whisper model to use\") # medium\n\t    parser.add_argument(\"--model_dir\", type=str, default=app_config.model_dir, \\\n\t                        help=\"the path to save model files; uses ~/.cache/whisper by default\")\n\t    parser.add_argument(\"--device\", default=app_config.device, \\\n\t                        help=\"device to use for PyTorch inference\")\n", "    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=output_dir, \\\n\t                        help=\"directory to save the outputs\")\n\t    parser.add_argument(\"--verbose\", type=str2bool, default=app_config.verbose, \\\n\t                        help=\"whether to print out the progress and debug messages\")\n\t    parser.add_argument(\"--whisper_implementation\", type=str, default=default_whisper_implementation, choices=[\"whisper\", \"faster-whisper\"],\\\n\t                        help=\"the Whisper implementation to use\")\n\t    parser.add_argument(\"--task\", nargs=\"+\", type=str, default=app_config.task, choices=[\"transcribe\", \"translate\", \"both\"], \\\n\t                        help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n\t    parser.add_argument(\"--language\", type=str, default=app_config.language, choices=sorted(get_language_names()), \\\n\t                        help=\"language spoken in the audio, specify None to perform language detection\")\n", "    parser.add_argument(\"--vad\", type=str, default=app_config.default_vad, choices=[\"none\", \"silero-vad\", \"silero-vad-skip-gaps\", \"silero-vad-expand-into-gaps\", \"periodic-vad\"], \\\n\t                        help=\"The voice activity detection algorithm to use\") # silero-vad\n\t    parser.add_argument(\"--vad_initial_prompt_mode\", type=str, default=app_config.vad_initial_prompt_mode, choices=VAD_INITIAL_PROMPT_MODE_VALUES, \\\n\t                        help=\"Whether or not to prepend the initial prompt to each VAD segment (prepend_all_segments), or just the first segment (prepend_first_segment)\") # prepend_first_segment\n\t    parser.add_argument(\"--vad_merge_window\", type=optional_float, default=app_config.vad_merge_window, \\\n\t                        help=\"The window size (in seconds) to merge voice segments\")\n\t    parser.add_argument(\"--vad_max_merge_size\", type=optional_float, default=app_config.vad_max_merge_size,\\\n\t                         help=\"The maximum size (in seconds) of a voice segment\")\n\t    parser.add_argument(\"--vad_padding\", type=optional_float, default=app_config.vad_padding, \\\n\t                        help=\"The padding (in seconds) to add to each voice segment\")\n", "    parser.add_argument(\"--vad_prompt_window\", type=optional_float, default=app_config.vad_prompt_window, \\\n\t                        help=\"The window size of the prompt to pass to Whisper\")\n\t    parser.add_argument(\"--vad_cpu_cores\", type=int, default=app_config.vad_cpu_cores, \\\n\t                        help=\"The number of CPU cores to use for VAD pre-processing.\") # 1\n\t    parser.add_argument(\"--vad_parallel_devices\", type=str, default=app_config.vad_parallel_devices, \\\n\t                        help=\"A commma delimited list of CUDA devices to use for parallel processing. If None, disable parallel processing.\") # \"\"\n\t    parser.add_argument(\"--auto_parallel\", type=bool, default=app_config.auto_parallel, \\\n\t                        help=\"True to use all available GPUs and CPU cores for processing. Use vad_cpu_cores/vad_parallel_devices to specify the number of CPU cores/GPUs to use.\") # False\n\t    parser.add_argument(\"--temperature\", type=float, default=app_config.temperature, \\\n\t                        help=\"temperature to use for sampling\")\n", "    parser.add_argument(\"--best_of\", type=optional_int, default=app_config.best_of, \\\n\t                        help=\"number of candidates when sampling with non-zero temperature\")\n\t    parser.add_argument(\"--beam_size\", type=optional_int, default=app_config.beam_size, \\\n\t                        help=\"number of beams in beam search, only applicable when temperature is zero\")\n\t    parser.add_argument(\"--patience\", type=float, default=app_config.patience, \\\n\t                        help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n\t    parser.add_argument(\"--length_penalty\", type=float, default=app_config.length_penalty, \\\n\t                        help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple lengt normalization by default\")\n\t    parser.add_argument(\"--suppress_tokens\", type=str, default=app_config.suppress_tokens, \\\n\t                        help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n", "    parser.add_argument(\"--initial_prompt\", type=str, default=app_config.initial_prompt, \\\n\t                        help=\"optional text to provide as a prompt for the first window.\")\n\t    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=app_config.condition_on_previous_text, \\\n\t                        help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n\t    # parser.add_argument(\"--fp16\", type=str2bool, default=app_config.fp16, \\\n\t    #                     help=\"whether to perform inference in fp16; True by default\")\n\t    parser.add_argument(\"--compute_type\", type=str, default=app_config.compute_type, choices=[\"default\", \"auto\", \"int8\", \"int8_float16\", \"int16\", \"float16\", \"float32\"], \\\n\t                        help=\"the compute type to use for inference\")\n\t    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=app_config.temperature_increment_on_fallback, \\\n\t                        help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n", "    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=app_config.compression_ratio_threshold, \\\n\t                        help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n\t    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=app_config.logprob_threshold, \\\n\t                        help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n\t    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=app_config.no_speech_threshold, \\\n\t                        help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n\t    parser.add_argument(\"--word_timestamps\", type=str2bool, default=app_config.word_timestamps, \n\t                        help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n\t    parser.add_argument(\"--prepend_punctuations\", type=str, default=app_config.prepend_punctuations, \n\t                        help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n", "    parser.add_argument(\"--append_punctuations\", type=str, default=app_config.append_punctuations, \n\t                        help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n\t    parser.add_argument(\"--highlight_words\", type=str2bool, default=app_config.highlight_words,\n\t                        help=\"(requires --word_timestamps True) underline each word as it is spoken in srt and vtt\")\n\t    parser.add_argument(\"--threads\", type=optional_int, default=0, \n\t                        help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n\t    args = parser.parse_args().__dict__\n\t    model_names: str = args.pop(\"model\")\n\t    model_dir: str = args.pop(\"model_dir\")\n\t    output_dir: str = args.pop(\"output_dir\")\n", "    device: str = args.pop(\"device\")\n\t    os.makedirs(output_dir, exist_ok=True)\n\t    if (threads := args.pop(\"threads\")) > 0:\n\t        torch.set_num_threads(threads)\n\t    whisper_implementation = args.pop(\"whisper_implementation\")\n\t    print(f\"Using {whisper_implementation} for Whisper\")\n\t    if model_names[0].endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n\t        warnings.warn(f\"{model_names[0]} is an English-only model but receipted '{args['language']}'; using English instead.\")\n\t        args[\"language\"] = \"en\"\n\t    temperature = args.pop(\"temperature\")\n", "    temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n\t    if temperature_increment_on_fallback is not None:\n\t        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n\t    else:\n\t        temperature = [temperature]\n\t    vad = args.pop(\"vad\")\n\t    vad_initial_prompt_mode = args.pop(\"vad_initial_prompt_mode\")\n\t    vad_merge_window = args.pop(\"vad_merge_window\")\n\t    vad_max_merge_size = args.pop(\"vad_max_merge_size\")\n\t    vad_padding = args.pop(\"vad_padding\")\n", "    vad_prompt_window = args.pop(\"vad_prompt_window\")\n\t    vad_cpu_cores = args.pop(\"vad_cpu_cores\")\n\t    auto_parallel = args.pop(\"auto_parallel\")\n\t    compute_type = args.pop(\"compute_type\")\n\t    tasks = args.pop(\"task\")\n\t    if \"both\" in tasks:\n\t        if len(tasks) > 1:\n\t            print(\"both is not supported with nargs, assuming only both\")\n\t            tasks = [\"both\"]\n\t    model_task_list = []\n", "    if tasks[0] == \"both\":\n\t        for model_name in model_names:\n\t            model_task_list.append({\"model\": model_name, \"task\": \"transcribe\"})\n\t            model_task_list.append({\"model\": model_name, \"task\": \"translate\"})\n\t    elif len(model_names) == len(tasks):\n\t        for model_name, task in zip(model_names, tasks):\n\t            model_task_list.append({\"model\": model_name, \"task\": task})\n\t    else:\n\t        print(\"bad model task combination\")\n\t        return\n", "    model_cache = {}\n\t    for model_name in model_names:\n\t        if model_name in model_cache:\n\t            continue\n\t        model_cache[model_name] = create_whisper_container(whisper_implementation=whisper_implementation, model_name=model_name, \n\t                                        device=device, compute_type=compute_type, download_root=model_dir, models=app_config.models)\n\t    highlight_words = args.pop(\"highlight_words\")\n\t    transcriber = WhisperTranscriber(delete_uploaded_files=False, vad_cpu_cores=vad_cpu_cores, app_config=app_config)\n\t    transcriber.set_parallel_devices(args.pop(\"vad_parallel_devices\"))\n\t    transcriber.set_auto_parallel(auto_parallel)\n", "    if (transcriber._has_parallel_devices()):\n\t        print(\"Using parallel devices:\", transcriber.parallel_device_list)\n\t    for audio_path in args.pop(\"audio\"):\n\t        sources = []\n\t        # Detect URL and download the audio\n\t        if (uri_validator(audio_path)):\n\t            # Download from YouTube/URL directly\n\t            for source_path in  download_url(audio_path, maxDuration=-1, destinationDirectory=output_dir, playlistItems=None):\n\t                source_name = os.path.basename(source_path)\n\t                sources.append({ \"path\": source_path, \"name\": source_name })\n", "        else:\n\t            sources.append({ \"path\": audio_path, \"name\": os.path.basename(audio_path) })\n\t        for source in sources:\n\t            source_path = source[\"path\"]\n\t            source_name = source[\"name\"]\n\t            for model_task in model_task_list:\n\t                model = model_cache[model_task[\"model\"]]\n\t                vadOptions = VadOptions(vad, vad_merge_window, vad_max_merge_size, vad_padding, vad_prompt_window, \n\t                                    VadInitialPromptMode.from_string(vad_initial_prompt_mode))\n\t                taskArgs = dict(args)\n", "                taskArgs[\"task\"] = model_task[\"task\"]\n\t                result = transcriber.transcribe_file(model, source_path, temperature=temperature, vadOptions=vadOptions, **taskArgs)\n\t                transcriber.write_result(result, source_name, output_dir, highlight_words, extras=\"_\" + model.model_name + \"_\" + taskArgs[\"task\"])\n\t    transcriber.close()\n\tdef uri_validator(x):\n\t    try:\n\t        result = urlparse(x)\n\t        return all([result.scheme, result.netloc])\n\t    except:\n\t        return False\n", "if __name__ == '__main__':\n\t    cli()"]}
{"filename": "tests/vad_test.py", "chunked_list": ["import pprint\n\timport unittest\n\timport numpy as np\n\timport sys\n\tsys.path.append('../whisper-webui')\n\tfrom src.vad import AbstractTranscription, TranscriptionConfig, VadSileroTranscription\n\tclass TestVad(unittest.TestCase):\n\t    def __init__(self, *args, **kwargs):\n\t        super(TestVad, self).__init__(*args, **kwargs)\n\t        self.transcribe_calls = []\n", "    def test_transcript(self):\n\t        mock = MockVadTranscription()\n\t        self.transcribe_calls.clear()\n\t        result = mock.transcribe(\"mock\", lambda segment : self.transcribe_segments(segment))\n\t        self.assertListEqual(self.transcribe_calls, [ \n\t            [30, 30],\n\t            [100, 100]\n\t        ])\n\t        self.assertListEqual(result['segments'],\n\t            [{'end': 50.0, 'start': 40.0, 'text': 'Hello world '},\n", "            {'end': 120.0, 'start': 110.0, 'text': 'Hello world '}]\n\t        )\n\t    def transcribe_segments(self, segment):\n\t        self.transcribe_calls.append(segment.tolist())\n\t        # Dummy text\n\t        return {\n\t            'text': \"Hello world \",\n\t            'segments': [\n\t                {\n\t                    \"start\": 10.0,\n", "                    \"end\": 20.0,\n\t                    \"text\": \"Hello world \"\n\t                }   \n\t            ],\n\t            'language': \"\"\n\t        }\n\tclass MockVadTranscription(AbstractTranscription):\n\t    def __init__(self):\n\t        super().__init__()\n\t    def get_audio_segment(self, str, start_time: str = None, duration: str = None):\n", "        start_time_seconds = float(start_time.removesuffix(\"s\"))\n\t        duration_seconds = float(duration.removesuffix(\"s\"))\n\t        # For mocking, this just returns a simple numppy array\n\t        return np.array([start_time_seconds, duration_seconds], dtype=np.float64)\n\t    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, duration: float):\n\t        result = []\n\t        result.append( {  'start': 30, 'end': 60 } )\n\t        result.append( {  'start': 100, 'end': 200 } )\n\t        return result\n\tif __name__ == '__main__':\n", "    unittest.main()"]}
{"filename": "tests/segments_test.py", "chunked_list": ["import sys\n\timport unittest\n\tsys.path.append('../whisper-webui')\n\tfrom src.segments import merge_timestamps\n\tclass TestSegments(unittest.TestCase):\n\t    def __init__(self, *args, **kwargs):\n\t        super(TestSegments, self).__init__(*args, **kwargs)\n\t    def test_merge_segments(self):\n\t        segments = [\n\t            {'start': 10.0, 'end': 20.0},\n", "            {'start': 22.0, 'end': 27.0},\n\t            {'start': 31.0, 'end': 35.0},\n\t            {'start': 45.0, 'end': 60.0},\n\t            {'start': 61.0, 'end': 65.0},\n\t            {'start': 68.0, 'end': 98.0},\n\t            {'start': 100.0, 'end': 102.0},\n\t            {'start': 110.0, 'end': 112.0}\n\t        ]\n\t        result = merge_timestamps(segments, merge_window=5, max_merge_size=30, padding_left=1, padding_right=1)\n\t        self.assertListEqual(result, [\n", "            {'start': 9.0, 'end': 36.0},\n\t            {'start': 44.0, 'end': 66.0},\n\t            {'start': 67.0, 'end': 99.0},\n\t            {'start': 99.0, 'end': 103.0},\n\t            {'start': 109.0, 'end': 113.0}\n\t        ])\n\t    def test_overlap_next(self):\n\t        segments = [\n\t            {'start': 5.0, 'end': 39.182},\n\t            {'start': 39.986, 'end': 40.814}\n", "        ]\n\t        result = merge_timestamps(segments, merge_window=5, max_merge_size=30, padding_left=1, padding_right=1)\n\t        self.assertListEqual(result, [\n\t            {'start': 4.0, 'end': 39.584},\n\t            {'start': 39.584, 'end': 41.814}\n\t        ])\n\tif __name__ == '__main__':\n\t    unittest.main()"]}
{"filename": "src/vad.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom collections import Counter, deque\n\timport time\n\tfrom typing import Any, Deque, Iterator, List, Dict\n\tfrom pprint import pprint\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.hooks.subTaskProgressListener import SubTaskProgressListener\n\tfrom src.hooks.whisperProgressHook import create_progress_listener_handle\n\tfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\n\tfrom src.segments import merge_timestamps\n", "from src.whisper.abstractWhisperContainer import AbstractWhisperCallback\n\t# Workaround for https://github.com/tensorflow/tensorflow/issues/48797\n\ttry:\n\t    import tensorflow as tf\n\texcept ModuleNotFoundError:\n\t    # Error handling\n\t    pass\n\timport torch\n\timport ffmpeg\n\timport numpy as np\n", "from src.utils import format_timestamp\n\tfrom enum import Enum\n\tvadCache = {}\n\tclass NonSpeechStrategy(Enum):\n\t    \"\"\"\n\t    Ignore non-speech frames segments.\n\t    \"\"\"\n\t    SKIP = 1\n\t    \"\"\"\n\t    Just treat non-speech segments as speech.\n", "    \"\"\"\n\t    CREATE_SEGMENT = 2\n\t    \"\"\"\n\t    Expand speech segments into subsequent non-speech segments.\n\t    \"\"\"\n\t    EXPAND_SEGMENT = 3\n\t# Defaults for Silero\n\tSPEECH_TRESHOLD = 0.3\n\t# Minimum size of segments to process\n\tMIN_SEGMENT_DURATION = 1\n", "# The maximum time for texts from old segments to be used in the next segment \n\tMAX_PROMPT_WINDOW = 0 # seconds (0 = disabled)\n\tPROMPT_NO_SPEECH_PROB = 0.1 # Do not pass the text from segments with a no speech probability higher than this\n\tVAD_MAX_PROCESSING_CHUNK = 60 * 60 # 60 minutes of audio\n\tclass TranscriptionConfig(ABC):\n\t    def __init__(self, non_speech_strategy: NonSpeechStrategy = NonSpeechStrategy.SKIP, \n\t                       segment_padding_left: float = None, segment_padding_right = None, max_silent_period: float = None, \n\t                       max_merge_size: float = None, max_prompt_window: float = None, initial_segment_index = -1):\n\t        self.non_speech_strategy = non_speech_strategy\n\t        self.segment_padding_left = segment_padding_left\n", "        self.segment_padding_right = segment_padding_right\n\t        self.max_silent_period = max_silent_period\n\t        self.max_merge_size = max_merge_size\n\t        self.max_prompt_window = max_prompt_window\n\t        self.initial_segment_index = initial_segment_index\n\tclass PeriodicTranscriptionConfig(TranscriptionConfig):\n\t    def __init__(self, periodic_duration: float, non_speech_strategy: NonSpeechStrategy = NonSpeechStrategy.SKIP, \n\t                       segment_padding_left: float = None, segment_padding_right = None, max_silent_period: float = None, \n\t                       max_merge_size: float = None, max_prompt_window: float = None, initial_segment_index = -1):\n\t        super().__init__(non_speech_strategy, segment_padding_left, segment_padding_right, max_silent_period, max_merge_size, max_prompt_window, initial_segment_index)\n", "        self.periodic_duration = periodic_duration\n\tclass AbstractTranscription(ABC):\n\t    def __init__(self, sampling_rate: int = 16000):\n\t        self.sampling_rate = sampling_rate\n\t    def get_audio_segment(self, str, start_time: str = None, duration: str = None):\n\t        return load_audio(str, self.sampling_rate, start_time, duration)\n\t    def is_transcribe_timestamps_fast(self):\n\t        \"\"\"\n\t        Determine if get_transcribe_timestamps is fast enough to not need parallelization.\n\t        \"\"\"\n", "        return False\n\t    @abstractmethod\n\t    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, end_time: float):\n\t        \"\"\"\n\t        Get the start and end timestamps of the sections that should be transcribed by this VAD method.\n\t        Parameters\n\t        ----------\n\t        audio: str\n\t            The audio file.\n\t        config: TranscriptionConfig\n", "            The transcription configuration.\n\t        Returns\n\t        -------\n\t        A list of start and end timestamps, in fractional seconds.\n\t        \"\"\"\n\t        return \n\t    def get_merged_timestamps(self, timestamps: List[Dict[str, Any]], config: TranscriptionConfig, total_duration: float):\n\t        \"\"\"\n\t        Get the start and end timestamps of the sections that should be transcribed by this VAD method,\n\t        after merging the given segments using the specified configuration.\n", "        Parameters\n\t        ----------\n\t        audio: str\n\t            The audio file. \n\t        config: TranscriptionConfig\n\t            The transcription configuration.\n\t        Returns\n\t        -------\n\t        A list of start and end timestamps, in fractional seconds.\n\t        \"\"\"\n", "        merged = merge_timestamps(timestamps, config.max_silent_period, config.max_merge_size, \n\t                                  config.segment_padding_left, config.segment_padding_right)\n\t        if config.non_speech_strategy != NonSpeechStrategy.SKIP:\n\t            # Expand segments to include the gaps between them\n\t            if (config.non_speech_strategy == NonSpeechStrategy.CREATE_SEGMENT):\n\t                # When we have a prompt window, we create speech segments betwen each segment if we exceed the merge size\n\t                merged = self.fill_gaps(merged, total_duration=total_duration, max_expand_size=config.max_merge_size)\n\t            elif config.non_speech_strategy == NonSpeechStrategy.EXPAND_SEGMENT: \n\t                # With no prompt window, it is better to just expand the segments (this effectively passes the prompt to the next segment)\n\t                merged = self.expand_gaps(merged, total_duration=total_duration)\n", "            else:\n\t                raise Exception(\"Unknown non-speech strategy: \" + str(config.non_speech_strategy))\n\t            print(\"Transcribing non-speech:\")\n\t            pprint(merged)\n\t        return merged\n\t    def get_vad_segments(self, audio: str, config: TranscriptionConfig):\n\t            if audio in vadCache:\n\t                print(\"[AbstractTranscription] Using vad cache\", audio)\n\t                return vadCache[audio]\n\t            max_audio_duration = self.get_audio_duration(audio, config)\n", "            timestamp_segments = self.get_transcribe_timestamps(audio, config, 0, max_audio_duration)\n\t            # Get speech timestamps from full audio file\n\t            merged = self.get_merged_timestamps(timestamp_segments, config, max_audio_duration)\n\t            print(\"Processing timestamps:\")\n\t            pprint(merged)\n\t            vadCache[audio] = merged\n\t            return merged\n\t    def transcribe(self, audio: str, whisperCallable: AbstractWhisperCallback, config: TranscriptionConfig, \n\t                   progressListener: ProgressListener = None):\n\t        \"\"\"\n", "        Transcribe the given audo file.\n\t        Parameters\n\t        ----------\n\t        audio: str\n\t            The audio file.\n\t        whisperCallable: WhisperCallback\n\t            A callback object to call to transcribe each segment.\n\t        Returns\n\t        -------\n\t        A list of start and end timestamps, in fractional seconds.\n", "        \"\"\"\n\t        try:\n\t            merged = self.get_vad_segments(audio, config)\n\t            # A deque of transcribed segments that is passed to the next segment as a prompt\n\t            prompt_window = deque()\n\t            languageCounter = Counter()\n\t            detected_language = None\n\t            segment_index = config.initial_segment_index\n\t            # Calculate progress \n\t            progress_start_offset = merged[0]['start'] if len(merged) > 0 else 0\n", "            progress_total_duration = sum([segment['end'] - segment['start'] for segment in merged])\n\t            result = {\n\t                'text': \"\",\n\t                'segments': [],\n\t                'language': \"\"\n\t            }\n\t            # For each time segment, run whisper\n\t            for segment in merged:\n\t                segment_index += 1\n\t                segment_start = segment['start']\n", "                segment_end = segment['end']\n\t                segment_expand_amount = segment.get('expand_amount', 0)\n\t                segment_gap = segment.get('gap', False)\n\t                segment_duration = segment_end - segment_start\n\t                if segment_duration < MIN_SEGMENT_DURATION:\n\t                    continue\n\t                # Audio to run on Whisper\n\t                segment_audio = self.get_audio_segment(audio, start_time = str(segment_start), duration = str(segment_duration))\n\t                # Previous segments to use as a prompt\n\t                segment_prompt = ' '.join([segment['text'] for segment in prompt_window]) if len(prompt_window) > 0 else None\n", "                # Detected language\n\t                detected_language = languageCounter.most_common(1)[0][0] if len(languageCounter) > 0 else None\n\t                print(\"Running whisper from \", format_timestamp(segment_start), \" to \", format_timestamp(segment_end), \", duration: \", \n\t                    segment_duration, \"expanded: \", segment_expand_amount, \"prompt: \", segment_prompt, \"language: \", detected_language)\n\t                perf_start_time = time.perf_counter()\n\t                scaled_progress_listener = SubTaskProgressListener(progressListener, base_task_total=progress_total_duration, \n\t                                                                   sub_task_start=segment_start - progress_start_offset, sub_task_total=segment_duration) \n\t                segment_result = whisperCallable.invoke(segment_audio, segment_index, segment_prompt, detected_language, progress_listener=scaled_progress_listener)\n\t                perf_end_time = time.perf_counter()\n\t                print(\"Whisper took {} seconds\".format(perf_end_time - perf_start_time))\n", "                adjusted_segments = self.adjust_timestamp(segment_result[\"segments\"], adjust_seconds=segment_start, max_source_time=segment_duration)\n\t                # Propagate expand amount to the segments\n\t                if (segment_expand_amount > 0):\n\t                    segment_without_expansion = segment_duration - segment_expand_amount\n\t                    for adjusted_segment in adjusted_segments:\n\t                        adjusted_segment_end = adjusted_segment['end']\n\t                        # Add expand amount if the segment got expanded\n\t                        if (adjusted_segment_end > segment_without_expansion):\n\t                            adjusted_segment[\"expand_amount\"] = adjusted_segment_end - segment_without_expansion\n\t                # Append to output\n", "                result['text'] += segment_result['text']\n\t                result['segments'].extend(adjusted_segments)\n\t                # Increment detected language\n\t                if not segment_gap:\n\t                    languageCounter[segment_result['language']] += 1\n\t                # Update prompt window\n\t                self.__update_prompt_window(prompt_window, adjusted_segments, segment_end, segment_gap, config)\n\t            if detected_language is not None:\n\t                result['language'] = detected_language\n\t        finally:\n", "            # Notify progress listener that we are done\n\t            if progressListener is not None:\n\t                progressListener.on_finished()\n\t        return result\n\t    def get_audio_duration(self, audio: str, config: TranscriptionConfig):\n\t        return get_audio_duration(audio)\n\t    def __update_prompt_window(self, prompt_window: Deque, adjusted_segments: List, segment_end: float, segment_gap: bool, config: TranscriptionConfig):\n\t        if (config.max_prompt_window is not None and config.max_prompt_window > 0):\n\t            # Add segments to the current prompt window (unless it is a speech gap)\n\t            if not segment_gap:\n", "                for segment in adjusted_segments:\n\t                    if segment.get('no_speech_prob', 0) <= PROMPT_NO_SPEECH_PROB:\n\t                        prompt_window.append(segment)\n\t            while (len(prompt_window) > 0):\n\t                first_end_time = prompt_window[0].get('end', 0)\n\t                # Time expanded in the segments should be discounted from the prompt window\n\t                first_expand_time = prompt_window[0].get('expand_amount', 0)\n\t                if (first_end_time - first_expand_time < segment_end - config.max_prompt_window):\n\t                    prompt_window.popleft()\n\t                else:\n", "                    break\n\t    def include_gaps(self, segments: Iterator[dict], min_gap_length: float, total_duration: float):\n\t        result = []\n\t        last_end_time = 0\n\t        for segment in segments:\n\t            segment_start = float(segment['start'])\n\t            segment_end = float(segment['end'])\n\t            if (last_end_time != segment_start):\n\t                delta = segment_start - last_end_time\n\t                if (min_gap_length is None or delta >= min_gap_length):\n", "                    result.append( { 'start': last_end_time, 'end': segment_start, 'gap': True } )\n\t            last_end_time = segment_end\n\t            result.append(segment)\n\t        # Also include total duration if specified\n\t        if (total_duration is not None and last_end_time < total_duration):\n\t            delta = total_duration - segment_start\n\t            if (min_gap_length is None or delta >= min_gap_length):\n\t                result.append( { 'start': last_end_time, 'end': total_duration, 'gap': True } )\n\t        return result\n\t    # Expand the end time of each segment to the start of the next segment\n", "    def expand_gaps(self, segments: List[Dict[str, Any]], total_duration: float):\n\t        result = []\n\t        if len(segments) == 0:\n\t            return result\n\t        # Add gap at the beginning if needed\n\t        if (segments[0]['start'] > 0):\n\t            result.append({ 'start': 0, 'end': segments[0]['start'], 'gap': True } )\n\t        for i in range(len(segments) - 1):\n\t            current_segment = segments[i]\n\t            next_segment = segments[i + 1]\n", "            delta = next_segment['start'] - current_segment['end']\n\t            # Expand if the gap actually exists\n\t            if (delta >= 0):\n\t                current_segment = current_segment.copy()\n\t                current_segment['expand_amount'] = delta\n\t                current_segment['end'] = next_segment['start']\n\t            result.append(current_segment)\n\t        # Add last segment\n\t        last_segment = segments[-1]\n\t        result.append(last_segment)\n", "        # Also include total duration if specified\n\t        if (total_duration is not None):\n\t            last_segment = result[-1]\n\t            if (last_segment['end'] < total_duration):\n\t                last_segment = last_segment.copy()\n\t                last_segment['end'] = total_duration\n\t                result[-1] = last_segment\n\t        return result\n\t    def fill_gaps(self, segments: List[Dict[str, Any]], total_duration: float, max_expand_size: float = None):\n\t        result = []\n", "        if len(segments) == 0:\n\t            return result\n\t        # Add gap at the beginning if needed\n\t        if (segments[0]['start'] > 0):\n\t            result.append({ 'start': 0, 'end': segments[0]['start'], 'gap': True } )\n\t        for i in range(len(segments) - 1):\n\t            expanded = False\n\t            current_segment = segments[i]\n\t            next_segment = segments[i + 1]\n\t            delta = next_segment['start'] - current_segment['end']\n", "            if (max_expand_size is not None and delta <= max_expand_size):\n\t                # Just expand the current segment\n\t                current_segment = current_segment.copy()\n\t                current_segment['expand_amount'] = delta\n\t                current_segment['end'] = next_segment['start']\n\t                expanded = True\n\t            result.append(current_segment)\n\t            # Add a gap to the next segment if needed\n\t            if (delta >= 0 and not expanded):\n\t                result.append({ 'start': current_segment['end'], 'end': next_segment['start'], 'gap': True } )\n", "        # Add last segment\n\t        last_segment = segments[-1]\n\t        result.append(last_segment)\n\t        # Also include total duration if specified\n\t        if (total_duration is not None):\n\t            last_segment = result[-1]\n\t            delta = total_duration - last_segment['end']\n\t            if (delta > 0):\n\t                if (max_expand_size is not None and delta <= max_expand_size):\n\t                    # Expand the last segment\n", "                    last_segment = last_segment.copy()\n\t                    last_segment['expand_amount'] = delta\n\t                    last_segment['end'] = total_duration\n\t                    result[-1] = last_segment\n\t                else:\n\t                    result.append({ 'start': last_segment['end'], 'end': total_duration, 'gap': True } )\n\t        return result\n\t    def adjust_timestamp(self, segments: Iterator[dict], adjust_seconds: float, max_source_time: float = None):\n\t        result = []\n\t        for segment in segments:\n", "            segment_start = float(segment['start'])\n\t            segment_end = float(segment['end'])\n\t            # Filter segments?\n\t            if (max_source_time is not None):\n\t                if (segment_start > max_source_time):\n\t                    continue\n\t                segment_end = min(max_source_time, segment_end)\n\t                new_segment = segment.copy()\n\t            # Add to start and end\n\t            new_segment['start'] = segment_start + adjust_seconds\n", "            new_segment['end'] = segment_end + adjust_seconds\n\t            # Handle words\n\t            if ('words' in new_segment):\n\t                for word in new_segment['words']:\n\t                    # Adjust start and end\n\t                    word['start'] = word['start'] + adjust_seconds\n\t                    word['end'] = word['end'] + adjust_seconds\n\t            result.append(new_segment)\n\t        return result\n\t    def multiply_timestamps(self, timestamps: List[Dict[str, Any]], factor: float):\n", "        result = []\n\t        for entry in timestamps:\n\t            start = entry['start']\n\t            end = entry['end']\n\t            result.append({\n\t                'start': start * factor,\n\t                'end': end * factor\n\t            })\n\t        return result\n\tclass VadSileroTranscription(AbstractTranscription):\n", "    def __init__(self, sampling_rate: int = 16000, cache: ModelCache = None):\n\t        super().__init__(sampling_rate=sampling_rate)\n\t        self.model = None\n\t        self.cache = cache\n\t        self._initialize_model()\n\t    def _initialize_model(self):\n\t        if (self.cache is not None):\n\t            model_key = \"VadSileroTranscription\"\n\t            self.model, self.get_speech_timestamps = self.cache.get(model_key, self._create_model)\n\t            print(\"Loaded Silerio model from cache.\")\n", "        else:\n\t            self.model, self.get_speech_timestamps = self._create_model()\n\t            print(\"Created Silerio model\")\n\t    def _create_model(self):\n\t        model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n\t        # Silero does not benefit from multi-threading\n\t        torch.set_num_threads(1) # JIT\n\t        (get_speech_timestamps, _, _, _, _) = utils\n\t        return model, get_speech_timestamps\n\t    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, end_time: float):\n", "        result = []\n\t        print(\"Getting timestamps from audio file: {}, start: {}, duration: {}\".format(audio, start_time, end_time))\n\t        perf_start_time = time.perf_counter()\n\t        # Divide procesisng of audio into chunks\n\t        chunk_start = start_time\n\t        while (chunk_start < end_time):\n\t            chunk_duration = min(end_time - chunk_start, VAD_MAX_PROCESSING_CHUNK)\n\t            print(\"Processing VAD in chunk from {} to {}\".format(format_timestamp(chunk_start), format_timestamp(chunk_start + chunk_duration)))\n\t            wav = self.get_audio_segment(audio, str(chunk_start), str(chunk_duration))\n\t            sample_timestamps = self.get_speech_timestamps(wav, self.model, sampling_rate=self.sampling_rate, threshold=SPEECH_TRESHOLD)\n", "            seconds_timestamps = self.multiply_timestamps(sample_timestamps, factor=1 / self.sampling_rate) \n\t            adjusted = self.adjust_timestamp(seconds_timestamps, adjust_seconds=chunk_start, max_source_time=chunk_start + chunk_duration)\n\t            #pprint(adjusted)\n\t            result.extend(adjusted)\n\t            chunk_start += chunk_duration\n\t        perf_end_time = time.perf_counter()\n\t        print(\"VAD processing took {} seconds\".format(perf_end_time - perf_start_time))\n\t        return result\n\t    def __getstate__(self):\n\t        # We only need the sampling rate\n", "        return { 'sampling_rate': self.sampling_rate }\n\t    def __setstate__(self, state):\n\t        self.sampling_rate = state['sampling_rate']\n\t        self.model = None\n\t        # Use the global cache\n\t        self.cache = GLOBAL_MODEL_CACHE\n\t        self._initialize_model()\n\t# A very simple VAD that just marks every N seconds as speech\n\tclass VadPeriodicTranscription(AbstractTranscription):\n\t    def __init__(self, sampling_rate: int = 16000):\n", "        super().__init__(sampling_rate=sampling_rate)\n\t    def is_transcribe_timestamps_fast(self):\n\t        # This is a very fast VAD - no need to parallelize it\n\t        return True\n\t    def get_transcribe_timestamps(self, audio: str, config: PeriodicTranscriptionConfig, start_time: float, end_time: float):\n\t        result = []\n\t        # Generate a timestamp every N seconds\n\t        start_timestamp = start_time\n\t        while (start_timestamp < end_time):\n\t            end_timestamp = min(start_timestamp + config.periodic_duration, end_time)\n", "            segment_duration = end_timestamp - start_timestamp\n\t            # Minimum duration is 1 second\n\t            if (segment_duration >= 1):\n\t                result.append( {  'start': start_timestamp, 'end': end_timestamp } )\n\t            start_timestamp = end_timestamp\n\t        return result\n\tdef get_audio_duration(file: str):\n\t    return float(ffmpeg.probe(file)[\"format\"][\"duration\"])\n\tdef load_audio(file: str, sample_rate: int = 16000, \n\t               start_time: str = None, duration: str = None):\n", "    \"\"\"\n\t    Open an audio file and read as mono waveform, resampling as necessary\n\t    Parameters\n\t    ----------\n\t    file: str\n\t        The audio file to open\n\t    sr: int\n\t        The sample rate to resample the audio if necessary\n\t    start_time: str\n\t        The start time, using the standard FFMPEG time duration syntax, or None to disable.\n", "    duration: str\n\t        The duration, using the standard FFMPEG time duration syntax, or None to disable.\n\t    Returns\n\t    -------\n\t    A NumPy array containing the audio waveform, in float32 dtype.\n\t    \"\"\"\n\t    try:\n\t        inputArgs = {'threads': 0}\n\t        if (start_time is not None):\n\t            inputArgs['ss'] = start_time\n", "        if (duration is not None):\n\t            inputArgs['t'] = duration\n\t        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n\t        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n\t        out, _ = (\n\t            ffmpeg.input(file, **inputArgs)\n\t            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sample_rate)\n\t            .run(cmd=\"ffmpeg\", capture_stdout=True, capture_stderr=True)\n\t        )\n\t    except ffmpeg.Error as e:\n", "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\")\n\t    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0"]}
{"filename": "src/segments.py", "chunked_list": ["from typing import Any, Dict, List\n\timport copy\n\tdef merge_timestamps(timestamps: List[Dict[str, Any]], merge_window: float = 5, max_merge_size: float = 30, padding_left: float = 1, padding_right: float = 1):\n\t    result = []\n\t    if len(timestamps) == 0:\n\t        return result\n\t    if max_merge_size is None:\n\t        return timestamps\n\t    if padding_left is None:\n\t        padding_left = 0\n", "    if padding_right is None:\n\t        padding_right = 0\n\t    processed_time = 0\n\t    current_segment = None\n\t    for i in range(len(timestamps)):\n\t        next_segment = timestamps[i]\n\t        delta = next_segment['start'] - processed_time\n\t        # Note that segments can still be longer than the max merge size, they just won't be merged in that case\n\t        if current_segment is None or (merge_window is not None and delta > merge_window) \\\n\t                 or next_segment['end'] - current_segment['start'] > max_merge_size:\n", "            # Finish the current segment\n\t            if current_segment is not None:\n\t                # Add right padding\n\t                finish_padding = min(padding_right, delta / 2) if delta < padding_left + padding_right else padding_right\n\t                current_segment['end'] += finish_padding\n\t                delta -= finish_padding\n\t                result.append(current_segment)\n\t            # Start a new segment\n\t            current_segment = copy.deepcopy(next_segment)\n\t            # Pad the segment\n", "            current_segment['start'] = current_segment['start'] - min(padding_left, delta)\n\t            processed_time = current_segment['end']\n\t        else:\n\t            # Merge the segment\n\t            current_segment['end'] = next_segment['end']\n\t            processed_time = current_segment['end']\n\t    # Add the last segment\n\t    if current_segment is not None:\n\t        current_segment['end'] += padding_right\n\t        result.append(current_segment)\n", "    return result"]}
{"filename": "src/vadParallel.py", "chunked_list": ["import multiprocessing\n\tfrom queue import Empty\n\timport threading\n\timport time\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.vad import AbstractTranscription, TranscriptionConfig, get_audio_duration\n\tfrom multiprocessing import Pool, Queue\n\tfrom typing import Any, Dict, List, Union\n\timport os\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback\n", "vadCache = {}\n\tvadCacheLock = threading.Lock()\n\tdownloadLock = threading.Lock()\n\tclass _ProgressListenerToQueue(ProgressListener):\n\t    def __init__(self, progress_queue: Queue):\n\t        self.progress_queue = progress_queue\n\t        self.progress_total = 0\n\t        self.prev_progress = 0\n\t    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t        delta = current - self.prev_progress\n", "        self.prev_progress = current\n\t        self.progress_total = total\n\t        self.progress_queue.put(delta)\n\t    def on_finished(self):\n\t        if self.progress_total > self.prev_progress:\n\t            delta = self.progress_total - self.prev_progress\n\t            self.progress_queue.put(delta)\n\t            self.prev_progress = self.progress_total\n\tclass ParallelContext:\n\t    def __init__(self, num_processes: int = None, auto_cleanup_timeout_seconds: float = None):\n", "        self.num_processes = num_processes\n\t        self.auto_cleanup_timeout_seconds = auto_cleanup_timeout_seconds\n\t        self.lock = threading.Lock()\n\t        self.ref_count = 0\n\t        self.pool = None\n\t        self.cleanup_timer = None\n\t    def get_pool(self):\n\t        # Initialize pool lazily\n\t        if (self.pool is None):\n\t            context = multiprocessing.get_context('spawn')\n", "            self.pool = context.Pool(self.num_processes)\n\t        self.ref_count = self.ref_count + 1\n\t        if (self.auto_cleanup_timeout_seconds is not None):\n\t            self._stop_auto_cleanup()\n\t        return self.pool\n\t    def return_pool(self, pool):\n\t        if (self.pool == pool and self.ref_count > 0):\n\t            self.ref_count = self.ref_count - 1\n\t            if (self.ref_count == 0):\n\t                if (self.auto_cleanup_timeout_seconds is not None):\n", "                    self._start_auto_cleanup()\n\t    def _start_auto_cleanup(self):\n\t        if (self.cleanup_timer is not None):\n\t            self.cleanup_timer.cancel()\n\t        self.cleanup_timer = threading.Timer(self.auto_cleanup_timeout_seconds, self._execute_cleanup)\n\t        self.cleanup_timer.start()\n\t        print(\"Started auto cleanup of pool in \" + str(self.auto_cleanup_timeout_seconds) + \" seconds\")\n\t    def _stop_auto_cleanup(self):\n\t        if (self.cleanup_timer is not None):\n\t            self.cleanup_timer.cancel()\n", "            self.cleanup_timer = None\n\t            print(\"Stopped auto cleanup of pool\")\n\t    def _execute_cleanup(self):\n\t        print(\"Executing cleanup of pool\")\n\t        if (self.ref_count == 0):\n\t            self.close()\n\t    def close(self):\n\t        self._stop_auto_cleanup()\n\t        if (self.pool is not None):\n\t            print(\"Closing pool of \" + str(self.num_processes) + \" processes\")\n", "            self.pool.close()\n\t            self.pool.join()\n\t        self.pool = None\n\tclass ParallelTranscriptionConfig(TranscriptionConfig):\n\t    def __init__(self, device_id: str, override_timestamps, initial_segment_index, copy: TranscriptionConfig = None):\n\t        super().__init__(copy.non_speech_strategy, copy.segment_padding_left, copy.segment_padding_right, copy.max_silent_period, copy.max_merge_size, copy.max_prompt_window, initial_segment_index)\n\t        self.device_id = device_id\n\t        self.override_timestamps = override_timestamps\n\tclass ParallelTranscription(AbstractTranscription):\n\t    # Silero VAD typically takes about 3 seconds per minute, so there's no need to split the chunks \n", "    # into smaller segments than 2 minute (min 6 seconds per CPU core)\n\t    MIN_CPU_CHUNK_SIZE_SECONDS = 2 * 60\n\t    def __init__(self, sampling_rate: int = 16000):\n\t        super().__init__(sampling_rate=sampling_rate)\n\t    def transcribe_parallel(self, transcription: AbstractTranscription, audio: str, whisperCallable: AbstractWhisperCallback, config: TranscriptionConfig, \n\t                            cpu_device_count: int, gpu_devices: List[str], cpu_parallel_context: ParallelContext = None, gpu_parallel_context: ParallelContext = None, \n\t                            progress_listener: ProgressListener = None):\n\t        total_duration = get_audio_duration(audio)\n\t        vadCacheLock.acquire()\n\t        if audio in vadCache:\n", "            print(\"[ParallelTranscription] Using vad cache\", audio)\n\t            merged = vadCache[audio]\n\t            vadCacheLock.release()\n\t        else:\n\t            # First, get the timestamps for the original audio\n\t            if (cpu_device_count > 1 and not transcription.is_transcribe_timestamps_fast()):\n\t                merged = self._get_merged_timestamps_parallel(transcription, audio, config, total_duration, cpu_device_count, cpu_parallel_context)\n\t            else:\n\t                timestamp_segments = transcription.get_transcribe_timestamps(audio, config, 0, total_duration)\n\t                merged = transcription.get_merged_timestamps(timestamp_segments, config, total_duration)\n", "            vadCache[audio] = merged\n\t            vadCacheLock.release()\n\t        # We must make sure the whisper model is downloaded\n\t        if (len(gpu_devices) > 1):\n\t            downloadLock.acquire()\n\t            whisperCallable.model_container.ensure_downloaded()\n\t            downloadLock.release()\n\t        # Split into a list for each device\n\t        # TODO: Split by time instead of by number of chunks\n\t        merged_split = list(self._split(merged, len(gpu_devices)))\n", "        # Parameters that will be passed to the transcribe function\n\t        parameters = []\n\t        segment_index = config.initial_segment_index\n\t        processing_manager = multiprocessing.Manager()\n\t        progress_queue = processing_manager.Queue()\n\t        for i in range(len(gpu_devices)):\n\t            # Note that device_segment_list can be empty. But we will still create a process for it,\n\t            # as otherwise we run the risk of assigning the same device to multiple processes.\n\t            device_segment_list = list(merged_split[i]) if i < len(merged_split) else []\n\t            device_id = gpu_devices[i]\n", "            print(\"Device \" + str(device_id) + \" (index \" + str(i) + \") has \" + str(len(device_segment_list)) + \" segments\")\n\t            # Create a new config with the given device ID\n\t            device_config = ParallelTranscriptionConfig(device_id, device_segment_list, segment_index, config)\n\t            segment_index += len(device_segment_list)\n\t            progress_listener_to_queue = _ProgressListenerToQueue(progress_queue)\n\t            parameters.append([audio, whisperCallable, device_config, progress_listener_to_queue]);\n\t        merged = {\n\t            'text': '',\n\t            'segments': [],\n\t            'language': None\n", "        }\n\t        created_context = False\n\t        perf_start_gpu = time.perf_counter()\n\t        # Spawn a separate process for each device\n\t        try:\n\t            if (gpu_parallel_context is None):\n\t                gpu_parallel_context = ParallelContext(len(gpu_devices))\n\t                created_context = True\n\t            # Get a pool of processes\n\t            pool = gpu_parallel_context.get_pool()\n", "            # Run the transcription in parallel\n\t            results_async = pool.starmap_async(self.transcribe, parameters)\n\t            total_progress = 0\n\t            while not results_async.ready():\n\t                try:\n\t                    delta = progress_queue.get(timeout=5)  # Set a timeout of 5 seconds\n\t                except Empty:\n\t                    continue\n\t                total_progress += delta\n\t                if progress_listener is not None:\n", "                    progress_listener.on_progress(total_progress, total_duration)\n\t            results = results_async.get()\n\t            # Call the finished callback\n\t            if progress_listener is not None:\n\t                progress_listener.on_finished()\n\t            for result in results:\n\t                # Merge the results\n\t                if (result['text'] is not None):\n\t                    merged['text'] += result['text']\n\t                if (result['segments'] is not None):\n", "                    merged['segments'].extend(result['segments'])\n\t                if (result['language'] is not None):\n\t                    merged['language'] = result['language']\n\t        finally:\n\t            # Return the pool to the context\n\t            if (gpu_parallel_context is not None):\n\t                gpu_parallel_context.return_pool(pool)\n\t            # Always close the context if we created it\n\t            if (created_context):\n\t                gpu_parallel_context.close()\n", "        perf_end_gpu = time.perf_counter()\n\t        print(\"Parallel transcription took \" + str(perf_end_gpu - perf_start_gpu) + \" seconds\")\n\t        return merged\n\t    def _get_merged_timestamps_parallel(self, transcription: AbstractTranscription, audio: str, config: TranscriptionConfig, total_duration: float, \n\t                                       cpu_device_count: int, cpu_parallel_context: ParallelContext = None):\n\t        parameters = []\n\t        chunk_size = max(total_duration / cpu_device_count, self.MIN_CPU_CHUNK_SIZE_SECONDS)\n\t        chunk_start = 0\n\t        cpu_device_id = 0\n\t        perf_start_time = time.perf_counter()\n", "        # Create chunks that will be processed on the CPU\n\t        while (chunk_start < total_duration):\n\t            chunk_end = min(chunk_start + chunk_size, total_duration)\n\t            if (chunk_end - chunk_start < 1):\n\t                # No need to process chunks that are less than 1 second\n\t                break\n\t            print(\"Parallel VAD: Executing chunk from \" + str(chunk_start) + \" to \" + \n\t                    str(chunk_end) + \" on CPU device \" + str(cpu_device_id))\n\t            parameters.append([audio, config, chunk_start, chunk_end]);\n\t            cpu_device_id += 1\n", "            chunk_start = chunk_end\n\t        created_context = False\n\t        # Spawn a separate process for each device\n\t        try:\n\t            if (cpu_parallel_context is None):\n\t                cpu_parallel_context = ParallelContext(cpu_device_count)\n\t                created_context = True\n\t            # Get a pool of processes\n\t            pool = cpu_parallel_context.get_pool()\n\t            # Run the transcription in parallel. Note that transcription must be picklable.\n", "            results = pool.starmap(transcription.get_transcribe_timestamps, parameters)\n\t            timestamps = []\n\t            # Flatten the results\n\t            for result in results:\n\t                timestamps.extend(result)\n\t            merged = transcription.get_merged_timestamps(timestamps, config, total_duration)\n\t            perf_end_time = time.perf_counter()\n\t            print(\"Parallel VAD processing took {} seconds\".format(perf_end_time - perf_start_time))\n\t            return merged\n\t        finally:\n", "            # Return the pool to the context\n\t            if (cpu_parallel_context is not None):\n\t                cpu_parallel_context.return_pool(pool)\n\t            # Always close the context if we created it\n\t            if (created_context):\n\t                cpu_parallel_context.close()\n\t    def get_transcribe_timestamps(self, audio: str, config: ParallelTranscriptionConfig, start_time: float, duration: float):\n\t        return []\n\t    def get_merged_timestamps(self,  timestamps: List[Dict[str, Any]], config: ParallelTranscriptionConfig, total_duration: float):\n\t        # Override timestamps that will be processed\n", "        if (config.override_timestamps is not None):\n\t            print(\"(get_merged_timestamps) Using override timestamps of size \" + str(len(config.override_timestamps)))\n\t            return config.override_timestamps\n\t        return super().get_merged_timestamps(timestamps, config, total_duration)\n\t    def transcribe(self, audio: str, whisperCallable: AbstractWhisperCallback, config: ParallelTranscriptionConfig, \n\t                   progressListener: ProgressListener = None):\n\t        # Override device ID the first time\n\t        if (os.environ.get(\"INITIALIZED\", None) is None):\n\t            os.environ[\"INITIALIZED\"] = \"1\"\n\t            # Note that this may be None if the user didn't specify a device. In that case, Whisper will\n", "            # just use the default GPU device.\n\t            if (config.device_id is not None):\n\t                print(\"Using device \" + config.device_id)\n\t                os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.device_id\n\t        return super().transcribe(audio, whisperCallable, config, progressListener)\n\t    def _split(self, a, n):\n\t        \"\"\"Split a list into n approximately equal parts.\"\"\"\n\t        k, m = divmod(len(a), n)\n\t        return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n"]}
{"filename": "src/config.py", "chunked_list": ["from enum import Enum\n\timport urllib\n\timport os\n\tfrom typing import List\n\tfrom urllib.parse import urlparse\n\timport json5\n\timport torch\n\tfrom tqdm import tqdm\n\tclass ModelConfig:\n\t    def __init__(self, name: str, url: str, path: str = None, type: str = \"whisper\"):\n", "        \"\"\"\n\t        Initialize a model configuration.\n\t        name: Name of the model\n\t        url: URL to download the model from\n\t        path: Path to the model file. If not set, the model will be downloaded from the URL.\n\t        type: Type of model. Can be whisper or huggingface.\n\t        \"\"\"\n\t        self.name = name\n\t        self.url = url\n\t        self.path = path\n", "        self.type = type\n\tVAD_INITIAL_PROMPT_MODE_VALUES=[\"prepend_all_segments\", \"prepend_first_segment\", \"json_prompt_mode\"]\n\tclass VadInitialPromptMode(Enum):\n\t    PREPEND_ALL_SEGMENTS = 1\n\t    PREPREND_FIRST_SEGMENT = 2\n\t    JSON_PROMPT_MODE = 3\n\t    @staticmethod\n\t    def from_string(s: str):\n\t        normalized = s.lower() if s is not None else None\n\t        if normalized == \"prepend_all_segments\":\n", "            return VadInitialPromptMode.PREPEND_ALL_SEGMENTS\n\t        elif normalized == \"prepend_first_segment\":\n\t            return VadInitialPromptMode.PREPREND_FIRST_SEGMENT\n\t        elif normalized == \"json_prompt_mode\":\n\t            return VadInitialPromptMode.JSON_PROMPT_MODE\n\t        elif normalized is not None and normalized != \"\":\n\t            raise ValueError(f\"Invalid value for VadInitialPromptMode: {s}\")\n\t        else:\n\t            return None\n\tclass ApplicationConfig:\n", "    def __init__(self, models: List[ModelConfig] = [], input_audio_max_duration: int = 600, \n\t                 share: bool = False, server_name: str = None, server_port: int = 7860, \n\t                 queue_concurrency_count: int = 1, delete_uploaded_files: bool = True,\n\t                 whisper_implementation: str = \"whisper\",\n\t                 default_model_name: str = \"medium\", default_vad: str = \"silero-vad\", \n\t                 vad_parallel_devices: str = \"\", vad_cpu_cores: int = 1, vad_process_timeout: int = 1800, \n\t                 auto_parallel: bool = False, output_dir: str = None,\n\t                 model_dir: str = None, device: str = None, \n\t                 verbose: bool = True, task: str = \"transcribe\", language: str = None,\n\t                 vad_initial_prompt_mode: str = \"prepend_first_segment \", \n", "                 vad_merge_window: float = 5, vad_max_merge_size: float = 30,\n\t                 vad_padding: float = 1, vad_prompt_window: float = 3,\n\t                 temperature: float = 0, best_of: int = 5, beam_size: int = 5,\n\t                 patience: float = None, length_penalty: float = None,\n\t                 suppress_tokens: str = \"-1\", initial_prompt: str = None,\n\t                 condition_on_previous_text: bool = True, fp16: bool = True,\n\t                 compute_type: str = \"float16\", \n\t                 temperature_increment_on_fallback: float = 0.2, compression_ratio_threshold: float = 2.4,\n\t                 logprob_threshold: float = -1.0, no_speech_threshold: float = 0.6,\n\t                 # Word timestamp settings\n", "                 word_timestamps: bool = False, prepend_punctuations: str = \"\\\"\\'“¿([{-\",\n\t                 append_punctuations: str = \"\\\"\\'.。,，!！?？:：”)]}、\", \n\t                 highlight_words: bool = False):\n\t        self.models = models\n\t        # WebUI settings\n\t        self.input_audio_max_duration = input_audio_max_duration\n\t        self.share = share\n\t        self.server_name = server_name\n\t        self.server_port = server_port\n\t        self.queue_concurrency_count = queue_concurrency_count\n", "        self.delete_uploaded_files = delete_uploaded_files\n\t        self.whisper_implementation = whisper_implementation\n\t        self.default_model_name = default_model_name\n\t        self.default_vad = default_vad\n\t        self.vad_parallel_devices = vad_parallel_devices\n\t        self.vad_cpu_cores = vad_cpu_cores\n\t        self.vad_process_timeout = vad_process_timeout\n\t        self.auto_parallel = auto_parallel\n\t        self.output_dir = output_dir\n\t        self.model_dir = model_dir\n", "        self.device = device\n\t        self.verbose = verbose\n\t        self.task = task\n\t        self.language = language\n\t        self.vad_initial_prompt_mode = vad_initial_prompt_mode\n\t        self.vad_merge_window = vad_merge_window\n\t        self.vad_max_merge_size = vad_max_merge_size\n\t        self.vad_padding = vad_padding\n\t        self.vad_prompt_window = vad_prompt_window\n\t        self.temperature = temperature\n", "        self.best_of = best_of\n\t        self.beam_size = beam_size\n\t        self.patience = patience\n\t        self.length_penalty = length_penalty\n\t        self.suppress_tokens = suppress_tokens\n\t        self.initial_prompt = initial_prompt\n\t        self.condition_on_previous_text = condition_on_previous_text\n\t        self.fp16 = fp16\n\t        self.compute_type = compute_type\n\t        self.temperature_increment_on_fallback = temperature_increment_on_fallback\n", "        self.compression_ratio_threshold = compression_ratio_threshold\n\t        self.logprob_threshold = logprob_threshold\n\t        self.no_speech_threshold = no_speech_threshold\n\t        # Word timestamp settings\n\t        self.word_timestamps = word_timestamps\n\t        self.prepend_punctuations = prepend_punctuations\n\t        self.append_punctuations = append_punctuations\n\t        self.highlight_words = highlight_words\n\t    def get_model_names(self):\n\t        return [ x.name for x in self.models ]\n", "    def update(self, **new_values):\n\t        result = ApplicationConfig(**self.__dict__)\n\t        for key, value in new_values.items():\n\t            setattr(result, key, value)\n\t        return result\n\t    @staticmethod\n\t    def create_default(**kwargs):\n\t        app_config = ApplicationConfig.parse_file(os.environ.get(\"WHISPER_WEBUI_CONFIG\", \"config.json5\"))\n\t        # Update with kwargs\n\t        if len(kwargs) > 0:\n", "            app_config = app_config.update(**kwargs)\n\t        return app_config\n\t    @staticmethod\n\t    def parse_file(config_path: str):\n\t        import json5\n\t        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n\t            # Load using json5\n\t            data = json5.load(f)\n\t            data_models = data.pop(\"models\", [])\n\t            models = [ ModelConfig(**x) for x in data_models ]\n", "            return ApplicationConfig(models, **data)\n"]}
{"filename": "src/__init__.py", "chunked_list": []}
{"filename": "src/utils.py", "chunked_list": ["import textwrap\n\timport unicodedata\n\timport re\n\timport zlib\n\tfrom typing import Iterator, TextIO, Union\n\timport tqdm\n\timport urllib3\n\tdef exact_div(x, y):\n\t    assert x % y == 0\n\t    return x // y\n", "def str2bool(string):\n\t    str2val = {\"True\": True, \"False\": False}\n\t    if string in str2val:\n\t        return str2val[string]\n\t    else:\n\t        raise ValueError(f\"Expected one of {set(str2val.keys())}, got {string}\")\n\tdef optional_int(string):\n\t    return None if string == \"None\" else int(string)\n\tdef optional_float(string):\n\t    return None if string == \"None\" else float(string)\n", "def compression_ratio(text) -> float:\n\t    return len(text) / len(zlib.compress(text.encode(\"utf-8\")))\n\tdef format_timestamp(seconds: float, always_include_hours: bool = False, fractionalSeperator: str = '.'):\n\t    assert seconds >= 0, \"non-negative timestamp expected\"\n\t    milliseconds = round(seconds * 1000.0)\n\t    hours = milliseconds // 3_600_000\n\t    milliseconds -= hours * 3_600_000\n\t    minutes = milliseconds // 60_000\n\t    milliseconds -= minutes * 60_000\n\t    seconds = milliseconds // 1_000\n", "    milliseconds -= seconds * 1_000\n\t    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n\t    return f\"{hours_marker}{minutes:02d}:{seconds:02d}{fractionalSeperator}{milliseconds:03d}\"\n\tdef write_txt(transcript: Iterator[dict], file: TextIO):\n\t    for segment in transcript:\n\t        print(segment['text'].strip(), file=file, flush=True)\n\tdef write_vtt(transcript: Iterator[dict], file: TextIO, \n\t              maxLineWidth=None, highlight_words: bool = False):\n\t    iterator  = __subtitle_preprocessor_iterator(transcript, maxLineWidth, highlight_words)\n\t    print(\"WEBVTT\\n\", file=file)\n", "    for segment in iterator:\n\t        text = segment['text'].replace('-->', '->')\n\t        print(\n\t            f\"{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}\\n\"\n\t            f\"{text}\\n\",\n\t            file=file,\n\t            flush=True,\n\t        )\n\tdef write_srt(transcript: Iterator[dict], file: TextIO, \n\t              maxLineWidth=None, highlight_words: bool = False):\n", "    \"\"\"\n\t    Write a transcript to a file in SRT format.\n\t    Example usage:\n\t        from pathlib import Path\n\t        from whisper.utils import write_srt\n\t        result = transcribe(model, audio_path, temperature=temperature, **args)\n\t        # save SRT\n\t        audio_basename = Path(audio_path).stem\n\t        with open(Path(output_dir) / (audio_basename + \".srt\"), \"w\", encoding=\"utf-8\") as srt:\n\t            write_srt(result[\"segments\"], file=srt)\n", "    \"\"\"\n\t    iterator  = __subtitle_preprocessor_iterator(transcript, maxLineWidth, highlight_words)\n\t    for i, segment in enumerate(iterator, start=1):\n\t        text = segment['text'].replace('-->', '->')\n\t        # write srt lines\n\t        print(\n\t            f\"{i}\\n\"\n\t            f\"{format_timestamp(segment['start'], always_include_hours=True, fractionalSeperator=',')} --> \"\n\t            f\"{format_timestamp(segment['end'], always_include_hours=True, fractionalSeperator=',')}\\n\"\n\t            f\"{text}\\n\",\n", "            file=file,\n\t            flush=True,\n\t        )\n\tdef __subtitle_preprocessor_iterator(transcript: Iterator[dict], maxLineWidth: int = None, highlight_words: bool = False): \n\t    for segment in transcript:\n\t        words = segment.get('words', [])\n\t        if len(words) == 0:\n\t            # Yield the segment as-is or processed\n\t            if maxLineWidth is None or maxLineWidth < 0:\n\t                yield segment\n", "            else:\n\t                yield {\n\t                    'start': segment['start'],\n\t                    'end': segment['end'],\n\t                    'text': process_text(segment['text'].strip(), maxLineWidth)\n\t                }\n\t            # We are done\n\t            continue\n\t        subtitle_start = segment['start']\n\t        subtitle_end = segment['end']\n", "        text_words = [ this_word[\"word\"] for this_word in words ]\n\t        subtitle_text = __join_words(text_words, maxLineWidth)\n\t        # Iterate over the words in the segment\n\t        if highlight_words:\n\t            last = subtitle_start\n\t            for i, this_word in enumerate(words):\n\t                start = this_word['start']\n\t                end = this_word['end']\n\t                if last != start:\n\t                    # Display the text up to this point\n", "                    yield {\n\t                        'start': last,\n\t                        'end': start,\n\t                        'text': subtitle_text\n\t                    }\n\t                # Display the text with the current word highlighted\n\t                yield {\n\t                    'start': start,\n\t                    'end': end,\n\t                    'text': __join_words(\n", "                        [\n\t                            {\n\t                                \"word\": re.sub(r\"^(\\s*)(.*)$\", r\"\\1<u>\\2</u>\", word)\n\t                                        if j == i\n\t                                        else word,\n\t                                # The HTML tags <u> and </u> are not displayed, \n\t                                # # so they should not be counted in the word length\n\t                                \"length\": len(word)\n\t                            } for j, word in enumerate(text_words)\n\t                        ], maxLineWidth)\n", "                }\n\t                last = end\n\t            if last != subtitle_end:\n\t                # Display the last part of the text\n\t                yield {\n\t                    'start': last,\n\t                    'end': subtitle_end,\n\t                    'text': subtitle_text\n\t                }\n\t        # Just return the subtitle text\n", "        else:\n\t            yield {\n\t                'start': subtitle_start,\n\t                'end': subtitle_end,\n\t                'text': subtitle_text\n\t            }\n\tdef __join_words(words: Iterator[Union[str, dict]], maxLineWidth: int = None):\n\t    if maxLineWidth is None or maxLineWidth < 0:\n\t        return \" \".join(words)\n\t    lines = []\n", "    current_line = \"\"\n\t    current_length = 0\n\t    for entry in words:\n\t        # Either accept a string or a dict with a 'word' and 'length' field\n\t        if isinstance(entry, dict):\n\t            word = entry['word']\n\t            word_length = entry['length']\n\t        else:\n\t            word = entry\n\t            word_length = len(word)\n", "        if current_length > 0 and current_length + word_length > maxLineWidth:\n\t            lines.append(current_line)\n\t            current_line = \"\"\n\t            current_length = 0\n\t        current_length += word_length\n\t        # The word will be prefixed with a space by Whisper, so we don't need to add one here\n\t        current_line += word\n\t    if len(current_line) > 0:\n\t        lines.append(current_line)\n\t    return \"\\n\".join(lines)\n", "def process_text(text: str, maxLineWidth=None):\n\t    if (maxLineWidth is None or maxLineWidth < 0):\n\t        return text\n\t    lines = textwrap.wrap(text, width=maxLineWidth, tabsize=4)\n\t    return '\\n'.join(lines)\n\tdef slugify(value, allow_unicode=False):\n\t    \"\"\"\n\t    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n\t    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n\t    dashes to single dashes. Remove characters that aren't alphanumerics,\n", "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n\t    trailing whitespace, dashes, and underscores.\n\t    \"\"\"\n\t    value = str(value)\n\t    if allow_unicode:\n\t        value = unicodedata.normalize('NFKC', value)\n\t    else:\n\t        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n\t    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n\t    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n", "def download_file(url: str, destination: str):\n\t        with urllib3.request.urlopen(url) as source, open(destination, \"wb\") as output:\n\t            with tqdm(\n\t                total=int(source.info().get(\"Content-Length\")),\n\t                ncols=80,\n\t                unit=\"iB\",\n\t                unit_scale=True,\n\t                unit_divisor=1024,\n\t            ) as loop:\n\t                while True:\n", "                    buffer = source.read(8192)\n\t                    if not buffer:\n\t                        break\n\t                    output.write(buffer)\n\t                    loop.update(len(buffer))"]}
{"filename": "src/download.py", "chunked_list": ["from tempfile import mkdtemp\n\tfrom typing import List\n\tfrom yt_dlp import YoutubeDL\n\timport yt_dlp\n\tfrom yt_dlp.postprocessor import PostProcessor\n\tclass FilenameCollectorPP(PostProcessor):\n\t    def __init__(self):\n\t        super(FilenameCollectorPP, self).__init__(None)\n\t        self.filenames = []\n\t    def run(self, information):\n", "        self.filenames.append(information[\"filepath\"])\n\t        return [], information\n\tdef download_url(url: str, maxDuration: int = None, destinationDirectory: str = None, playlistItems: str = \"1\") -> List[str]: \n\t    try:\n\t        return _perform_download(url, maxDuration=maxDuration, outputTemplate=None, destinationDirectory=destinationDirectory, playlistItems=playlistItems)\n\t    except yt_dlp.utils.DownloadError as e:\n\t        # In case of an OS error, try again with a different output template\n\t        if e.msg and e.msg.find(\"[Errno 36] File name too long\") >= 0:\n\t            return _perform_download(url, maxDuration=maxDuration, outputTemplate=\"%(title).10s %(id)s.%(ext)s\")\n\t        pass\n", "def _perform_download(url: str, maxDuration: int = None, outputTemplate: str = None, destinationDirectory: str = None, playlistItems: str = \"1\"):\n\t    # Create a temporary directory to store the downloaded files\n\t    if destinationDirectory is None:\n\t        destinationDirectory = mkdtemp()\n\t    ydl_opts = {\n\t        \"format\": \"bestaudio/best\",\n\t        'paths': {\n\t            'home': destinationDirectory\n\t        }\n\t    }\n", "    if (playlistItems):\n\t        ydl_opts['playlist_items'] = playlistItems\n\t    # Add output template if specified\n\t    if outputTemplate:\n\t        ydl_opts['outtmpl'] = outputTemplate\n\t    filename_collector = FilenameCollectorPP()\n\t    with YoutubeDL(ydl_opts) as ydl:\n\t        if maxDuration and maxDuration > 0:\n\t            info = ydl.extract_info(url, download=False)\n\t            entries = \"entries\" in info and info[\"entries\"] or [info]\n", "            total_duration = 0\n\t            # Compute total duration\n\t            for entry in entries:\n\t                total_duration += float(entry[\"duration\"])\n\t            if total_duration >= maxDuration:\n\t                raise ExceededMaximumDuration(videoDuration=total_duration, maxDuration=maxDuration, message=\"Video is too long\")\n\t        ydl.add_post_processor(filename_collector)\n\t        ydl.download([url])\n\t    if len(filename_collector.filenames) <= 0:\n\t        raise Exception(\"Cannot download \" + url)\n", "    result = []\n\t    for filename in filename_collector.filenames:\n\t        result.append(filename)\n\t        print(\"Downloaded \" + filename)\n\t    return result \n\tclass ExceededMaximumDuration(Exception):\n\t    def __init__(self, videoDuration, maxDuration, message):\n\t        self.videoDuration = videoDuration\n\t        self.maxDuration = maxDuration\n\t        super().__init__(message)"]}
{"filename": "src/languages.py", "chunked_list": ["class Language():\n\t    def __init__(self, code, name):\n\t        self.code = code\n\t        self.name = name\n\t    def __str__(self):\n\t        return \"Language(code={}, name={})\".format(self.code, self.name)\n\tLANGUAGES = [\n\t    Language('en', 'English'),\n\t    Language('zh', 'Chinese'),   \n\t    Language('de', 'German'),    \n", "    Language('es', 'Spanish'),   \n\t    Language('ru', 'Russian'),   \n\t    Language('ko', 'Korean'),    \n\t    Language('fr', 'French'),    \n\t    Language('ja', 'Japanese'),  \n\t    Language('pt', 'Portuguese'),\n\t    Language('tr', 'Turkish'),   \n\t    Language('pl', 'Polish'),    \n\t    Language('ca', 'Catalan'),   \n\t    Language('nl', 'Dutch'),     \n", "    Language('ar', 'Arabic'),\n\t    Language('sv', 'Swedish'),\n\t    Language('it', 'Italian'),\n\t    Language('id', 'Indonesian'),\n\t    Language('hi', 'Hindi'),\n\t    Language('fi', 'Finnish'),\n\t    Language('vi', 'Vietnamese'),\n\t    Language('he', 'Hebrew'),\n\t    Language('uk', 'Ukrainian'),\n\t    Language('el', 'Greek'),\n", "    Language('ms', 'Malay'),\n\t    Language('cs', 'Czech'),\n\t    Language('ro', 'Romanian'),\n\t    Language('da', 'Danish'),\n\t    Language('hu', 'Hungarian'),\n\t    Language('ta', 'Tamil'),\n\t    Language('no', 'Norwegian'),\n\t    Language('th', 'Thai'),\n\t    Language('ur', 'Urdu'),\n\t    Language('hr', 'Croatian'),\n", "    Language('bg', 'Bulgarian'),\n\t    Language('lt', 'Lithuanian'),\n\t    Language('la', 'Latin'),\n\t    Language('mi', 'Maori'),\n\t    Language('ml', 'Malayalam'),\n\t    Language('cy', 'Welsh'),\n\t    Language('sk', 'Slovak'),\n\t    Language('te', 'Telugu'),\n\t    Language('fa', 'Persian'),\n\t    Language('lv', 'Latvian'),\n", "    Language('bn', 'Bengali'),\n\t    Language('sr', 'Serbian'),\n\t    Language('az', 'Azerbaijani'),\n\t    Language('sl', 'Slovenian'),\n\t    Language('kn', 'Kannada'),\n\t    Language('et', 'Estonian'),\n\t    Language('mk', 'Macedonian'),\n\t    Language('br', 'Breton'),\n\t    Language('eu', 'Basque'),\n\t    Language('is', 'Icelandic'),\n", "    Language('hy', 'Armenian'),\n\t    Language('ne', 'Nepali'),\n\t    Language('mn', 'Mongolian'),\n\t    Language('bs', 'Bosnian'),\n\t    Language('kk', 'Kazakh'),\n\t    Language('sq', 'Albanian'),\n\t    Language('sw', 'Swahili'),\n\t    Language('gl', 'Galician'),\n\t    Language('mr', 'Marathi'),\n\t    Language('pa', 'Punjabi'),\n", "    Language('si', 'Sinhala'),\n\t    Language('km', 'Khmer'),\n\t    Language('sn', 'Shona'),\n\t    Language('yo', 'Yoruba'),\n\t    Language('so', 'Somali'),\n\t    Language('af', 'Afrikaans'),\n\t    Language('oc', 'Occitan'),\n\t    Language('ka', 'Georgian'),\n\t    Language('be', 'Belarusian'),\n\t    Language('tg', 'Tajik'),\n", "    Language('sd', 'Sindhi'),\n\t    Language('gu', 'Gujarati'),\n\t    Language('am', 'Amharic'),\n\t    Language('yi', 'Yiddish'),\n\t    Language('lo', 'Lao'),\n\t    Language('uz', 'Uzbek'),\n\t    Language('fo', 'Faroese'),\n\t    Language('ht', 'Haitian creole'),\n\t    Language('ps', 'Pashto'),\n\t    Language('tk', 'Turkmen'),\n", "    Language('nn', 'Nynorsk'),\n\t    Language('mt', 'Maltese'),\n\t    Language('sa', 'Sanskrit'),\n\t    Language('lb', 'Luxembourgish'),\n\t    Language('my', 'Myanmar'),\n\t    Language('bo', 'Tibetan'),\n\t    Language('tl', 'Tagalog'),\n\t    Language('mg', 'Malagasy'),\n\t    Language('as', 'Assamese'),\n\t    Language('tt', 'Tatar'),\n", "    Language('haw', 'Hawaiian'),\n\t    Language('ln', 'Lingala'),\n\t    Language('ha', 'Hausa'),\n\t    Language('ba', 'Bashkir'),\n\t    Language('jw', 'Javanese'),\n\t    Language('su', 'Sundanese')\n\t]\n\t_TO_LANGUAGE_CODE = {\n\t    **{language.code: language for language in LANGUAGES},\n\t    \"burmese\": \"my\",\n", "    \"valencian\": \"ca\",\n\t    \"flemish\": \"nl\",\n\t    \"haitian\": \"ht\",\n\t    \"letzeburgesch\": \"lb\",\n\t    \"pushto\": \"ps\",\n\t    \"panjabi\": \"pa\",\n\t    \"moldavian\": \"ro\",\n\t    \"moldovan\": \"ro\",\n\t    \"sinhalese\": \"si\",\n\t    \"castilian\": \"es\",\n", "}\n\t_FROM_LANGUAGE_NAME = {\n\t    **{language.name.lower(): language for language in LANGUAGES}\n\t}\n\tdef get_language_from_code(language_code, default=None) -> Language:\n\t    \"\"\"Return the language name from the language code.\"\"\"\n\t    return _TO_LANGUAGE_CODE.get(language_code, default)\n\tdef get_language_from_name(language, default=None) -> Language:\n\t    \"\"\"Return the language code from the language name.\"\"\"\n\t    return _FROM_LANGUAGE_NAME.get(language.lower() if language else None, default)\n", "def get_language_names():\n\t    \"\"\"Return a list of language names.\"\"\"\n\t    return [language.name for language in LANGUAGES]\n\tif __name__ == \"__main__\":\n\t    # Test lookup\n\t    print(get_language_from_code('en'))\n\t    print(get_language_from_name('English'))\n\t    print(get_language_names())"]}
{"filename": "src/modelCache.py", "chunked_list": ["class ModelCache:\n\t    def __init__(self):\n\t        self._cache = dict()\n\t    def get(self, model_key: str, model_factory):\n\t        result = self._cache.get(model_key)\n\t        if result is None:\n\t            result = model_factory()\n\t            self._cache[model_key] = result\n\t        return result\n\t    def clear(self):\n", "        self._cache.clear()\n\t# A global cache of models. This is mainly used by the daemon processes to avoid loading the same model multiple times.\n\tGLOBAL_MODEL_CACHE = ModelCache()"]}
{"filename": "src/source.py", "chunked_list": ["# Gradio seems to truncate files without keeping the extension, so we need to truncate the file prefix ourself \n\timport os\n\timport pathlib\n\tfrom typing import List\n\timport zipfile\n\timport ffmpeg\n\tfrom more_itertools import unzip\n\tfrom src.download import ExceededMaximumDuration, download_url\n\tMAX_FILE_PREFIX_LENGTH = 17\n\tclass AudioSource:\n", "    def __init__(self, source_path, source_name = None, audio_duration = None):\n\t        self.source_path = source_path\n\t        self.source_name = source_name\n\t        self._audio_duration = audio_duration\n\t        # Load source name if not provided\n\t        if (self.source_name is None):\n\t            file_path = pathlib.Path(self.source_path)\n\t            self.source_name = file_path.name\n\t    def get_audio_duration(self):\n\t        if self._audio_duration is None:\n", "            self._audio_duration = float(ffmpeg.probe(self.source_path)[\"format\"][\"duration\"])\n\t        return self._audio_duration\n\t    def get_full_name(self):\n\t        return self.source_name\n\t    def get_short_name(self, max_length: int = MAX_FILE_PREFIX_LENGTH):\n\t        file_path = pathlib.Path(self.source_name)\n\t        short_name = file_path.stem[:max_length] + file_path.suffix\n\t        return short_name\n\t    def __str__(self) -> str:\n\t        return self.source_path\n", "class AudioSourceCollection:\n\t    def __init__(self, sources: List[AudioSource]):\n\t        self.sources = sources\n\t    def __iter__(self):\n\t        return iter(self.sources)\n\tdef get_audio_source_collection(urlData: str, multipleFiles: List, microphoneData: str, input_audio_max_duration: float = -1) -> List[AudioSource]:\n\t    output: List[AudioSource] = []\n\t    if urlData:\n\t        # Download from YouTube. This could also be a playlist or a channel.\n\t        output.extend([ AudioSource(x) for x in download_url(urlData, input_audio_max_duration, playlistItems=None) ])\n", "    else:\n\t        # Add input files\n\t        if (multipleFiles is not None):\n\t            output.extend([ AudioSource(x.name) for x in multipleFiles ])\n\t        if (microphoneData is not None):\n\t            output.append(AudioSource(microphoneData))\n\t    total_duration = 0\n\t    # Calculate total audio length. We do this even if input_audio_max_duration\n\t    # is disabled to ensure that all the audio files are valid.\n\t    for source in output:\n", "        audioDuration = ffmpeg.probe(source.source_path)[\"format\"][\"duration\"]\n\t        total_duration += float(audioDuration)\n\t        # Save audio duration\n\t        source._audio_duration = float(audioDuration)\n\t    # Ensure the total duration of the audio is not too long\n\t    if input_audio_max_duration > 0:\n\t        if float(total_duration) > input_audio_max_duration:\n\t            raise ExceededMaximumDuration(videoDuration=total_duration, maxDuration=input_audio_max_duration, message=\"Video(s) is too long\")\n\t    # Return a list of audio sources\n\t    return output"]}
{"filename": "src/prompts/jsonPromptStrategy.py", "chunked_list": ["import json\n\tfrom typing import Dict\n\tfrom src.prompts.abstractPromptStrategy import AbstractPromptStrategy\n\tclass JsonPromptSegment():\n\t    def __init__(self, segment_index: int, prompt: str, format_prompt: bool = False):\n\t        self.prompt = prompt\n\t        self.segment_index = segment_index\n\t        self.format_prompt = format_prompt\n\tclass JsonPromptStrategy(AbstractPromptStrategy):\n\t    def __init__(self, initial_json_prompt: str):\n", "        \"\"\"\n\t        Parameters\n\t        ----------\n\t            initial_json_prompt: str\n\t                The initial prompts for each segment in JSON form.\n\t                Format:\n\t                [\n\t                    {\"segment_index\": 0, \"prompt\": \"Hello, how are you?\"},\n\t                    {\"segment_index\": 1, \"prompt\": \"I'm doing well, how are you?\"},\n\t                    {\"segment_index\": 2, \"prompt\": \"{0} Fine, thank you.\", \"format_prompt\": true}\n", "                ]\n\t        \"\"\"\n\t        parsed_json = json.loads(initial_json_prompt)\n\t        self.segment_lookup: Dict[str, JsonPromptSegment] = dict() \n\t        for prompt_entry in parsed_json:\n\t            segment_index = prompt_entry[\"segment_index\"]\n\t            prompt = prompt_entry[\"prompt\"]\n\t            format_prompt = prompt_entry.get(\"format_prompt\", False)\n\t            self.segment_lookup[str(segment_index)] = JsonPromptSegment(segment_index, prompt, format_prompt)\n\t    def get_segment_prompt(self, segment_index: int, whisper_prompt: str, detected_language: str) -> str:\n", "        # Lookup prompt\n\t        prompt = self.segment_lookup.get(str(segment_index), None)\n\t        if (prompt is None):\n\t            # No prompt found, return whisper prompt\n\t            print(f\"Could not find prompt for segment {segment_index}, returning whisper prompt\")\n\t            return whisper_prompt\n\t        if (prompt.format_prompt):\n\t            return prompt.prompt.format(whisper_prompt)\n\t        else:\n\t            return self._concat_prompt(prompt.prompt, whisper_prompt)\n"]}
{"filename": "src/prompts/prependPromptStrategy.py", "chunked_list": ["from src.config import VadInitialPromptMode\n\tfrom src.prompts.abstractPromptStrategy import AbstractPromptStrategy\n\tclass PrependPromptStrategy(AbstractPromptStrategy):\n\t    \"\"\"\n\t    A simple prompt strategy that prepends a single prompt to all segments of audio, or prepends the prompt to the first segment of audio.\n\t    \"\"\"\n\t    def __init__(self, initial_prompt: str, initial_prompt_mode: VadInitialPromptMode):\n\t        \"\"\"\n\t        Parameters\n\t        ----------\n", "            initial_prompt: str\n\t                The initial prompt to use for the transcription.\n\t            initial_prompt_mode: VadInitialPromptMode\n\t                The mode to use for the initial prompt. If set to PREPEND_FIRST_SEGMENT, the initial prompt will be prepended to the first segment of audio.\n\t                If set to PREPEND_ALL_SEGMENTS, the initial prompt will be prepended to all segments of audio.\n\t        \"\"\"\n\t        self.initial_prompt = initial_prompt\n\t        self.initial_prompt_mode = initial_prompt_mode\n\t        # This is a simple prompt strategy, so we only support these two modes\n\t        if initial_prompt_mode not in [VadInitialPromptMode.PREPEND_ALL_SEGMENTS, VadInitialPromptMode.PREPREND_FIRST_SEGMENT]:\n", "            raise ValueError(f\"Unsupported initial prompt mode {initial_prompt_mode}\")\n\t    def get_segment_prompt(self, segment_index: int, whisper_prompt: str, detected_language: str) -> str:\n\t        if (self.initial_prompt_mode == VadInitialPromptMode.PREPEND_ALL_SEGMENTS):\n\t            return self._concat_prompt(self.initial_prompt, whisper_prompt)\n\t        elif (self.initial_prompt_mode == VadInitialPromptMode.PREPREND_FIRST_SEGMENT):\n\t            return self._concat_prompt(self.initial_prompt, whisper_prompt) if segment_index == 0 else whisper_prompt\n\t        else:\n\t            raise ValueError(f\"Unknown initial prompt mode {self.initial_prompt_mode}\")"]}
{"filename": "src/prompts/abstractPromptStrategy.py", "chunked_list": ["import abc\n\tclass AbstractPromptStrategy:\n\t    \"\"\"\n\t    Represents a strategy for generating prompts for a given audio segment.\n\t    Note that the strategy must be picklable, as it will be serialized and sent to the workers.\n\t    \"\"\"\n\t    @abc.abstractmethod\n\t    def get_segment_prompt(self, segment_index: int, whisper_prompt: str, detected_language: str) -> str:\n\t        \"\"\"\n\t        Retrieves the prompt for a given segment.\n", "        Parameters\n\t        ----------\n\t        segment_index: int\n\t            The index of the segment.\n\t        whisper_prompt: str\n\t            The prompt for the segment generated by Whisper. This is typically concatenated with the initial prompt.\n\t        detected_language: str\n\t            The language detected for the segment.\n\t        \"\"\"\n\t        pass\n", "    @abc.abstractmethod\n\t    def on_segment_finished(self, segment_index: int, whisper_prompt: str, detected_language: str, result: dict):\n\t        \"\"\"\n\t        Called when a segment has finished processing.\n\t        Parameters\n\t        ----------\n\t        segment_index: int\n\t            The index of the segment.\n\t        whisper_prompt: str\n\t            The prompt for the segment generated by Whisper. This is typically concatenated with the initial prompt.\n", "        detected_language: str\n\t            The language detected for the segment.\n\t        result: dict\n\t            The result of the segment. It has the following format:\n\t                {\n\t                    \"text\": str,\n\t                    \"segments\": [\n\t                        {\n\t                            \"text\": str,\n\t                            \"start\": float,\n", "                            \"end\": float,\n\t                            \"words\": [words],\n\t                        }\n\t                    ],\n\t                    \"language\": str,\n\t                }\n\t        \"\"\"\n\t        pass\n\t    def _concat_prompt(self, prompt1, prompt2):\n\t        \"\"\"\n", "        Concatenates two prompts.\n\t        Parameters\n\t        ----------\n\t        prompt1: str\n\t            The first prompt.\n\t        prompt2: str\n\t            The second prompt.\n\t        \"\"\"\n\t        if (prompt1 is None):\n\t            return prompt2\n", "        elif (prompt2 is None):\n\t            return prompt1\n\t        else:\n\t            return prompt1 + \" \" + prompt2"]}
{"filename": "src/hooks/progressListener.py", "chunked_list": ["from typing import Union\n\tclass ProgressListener:\n\t    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t        self.total = total\n\t    def on_finished(self):\n\t        pass"]}
{"filename": "src/hooks/whisperProgressHook.py", "chunked_list": ["import sys\n\timport threading\n\tfrom typing import List, Union\n\timport tqdm\n\tfrom src.hooks.progressListener import ProgressListener\n\tclass ProgressListenerHandle:\n\t    def __init__(self, listener: ProgressListener):\n\t        self.listener = listener\n\t    def __enter__(self):\n\t        register_thread_local_progress_listener(self.listener)\n", "    def __exit__(self, exc_type, exc_val, exc_tb):\n\t        unregister_thread_local_progress_listener(self.listener)\n\t        if exc_type is None:\n\t            self.listener.on_finished()\n\tclass _CustomProgressBar(tqdm.tqdm):\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self._current = self.n  # Set the initial value\n\t    def update(self, n):\n\t        super().update(n)\n", "        # Because the progress bar might be disabled, we need to manually update the progress\n\t        self._current += n\n\t        # Inform listeners\n\t        listeners = _get_thread_local_listeners()\n\t        for listener in listeners:\n\t            listener.on_progress(self._current, self.total)\n\t_thread_local = threading.local()\n\tdef _get_thread_local_listeners():\n\t    if not hasattr(_thread_local, 'listeners'):\n\t        _thread_local.listeners = []\n", "    return _thread_local.listeners\n\t_hooked = False\n\tdef init_progress_hook():\n\t    global _hooked\n\t    if _hooked:\n\t        return\n\t    # Inject into tqdm.tqdm of Whisper, so we can see progress\n\t    import whisper.transcribe \n\t    transcribe_module = sys.modules['whisper.transcribe']\n\t    transcribe_module.tqdm.tqdm = _CustomProgressBar\n", "    _hooked = True\n\tdef register_thread_local_progress_listener(progress_listener: ProgressListener):\n\t    # This is a workaround for the fact that the progress bar is not exposed in the API\n\t    init_progress_hook()\n\t    listeners = _get_thread_local_listeners()\n\t    listeners.append(progress_listener)\n\tdef unregister_thread_local_progress_listener(progress_listener: ProgressListener):\n\t    listeners = _get_thread_local_listeners()\n\t    if progress_listener in listeners:\n\t        listeners.remove(progress_listener)\n", "def create_progress_listener_handle(progress_listener: ProgressListener):\n\t    return ProgressListenerHandle(progress_listener)\n\t# Example usage\n\tif __name__ == '__main__':\n\t    class PrintingProgressListener:\n\t        def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t            print(f\"Progress: {current}/{total}\")\n\t        def on_finished(self):\n\t            print(\"Finished\")\n\t    import whisper\n", "    model = whisper.load_model(\"medium\")\n\t    with create_progress_listener_handle(PrintingProgressListener()) as listener:\n\t        # Set verbose to None to disable the progress bar, as we are using our own\n\t        result = model.transcribe(\"J:\\\\Dev\\\\OpenAI\\\\whisper\\\\tests\\\\Noriko\\\\out.mka\", language=\"Japanese\", fp16=False, verbose=None)\n\t        print(result)\n\t    print(\"Done\")"]}
{"filename": "src/hooks/subTaskProgressListener.py", "chunked_list": ["from src.hooks.progressListener import ProgressListener\n\tfrom typing import Union\n\tclass SubTaskProgressListener(ProgressListener):\n\t    \"\"\"\n\t    A sub task listener that reports the progress of a sub task to a base task listener\n\t    Parameters\n\t    ----------\n\t    base_task_listener : ProgressListener\n\t        The base progress listener to accumulate overall progress in.\n\t    base_task_total : float\n", "        The maximum total progress that will be reported to the base progress listener.\n\t    sub_task_start : float\n\t        The starting progress of a sub task, in respect to the base progress listener.\n\t    sub_task_total : float\n\t        The total amount of progress a sub task will report to the base progress listener.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        base_task_listener: ProgressListener,\n\t        base_task_total: float,\n", "        sub_task_start: float,\n\t        sub_task_total: float,\n\t    ):\n\t        self.base_task_listener = base_task_listener\n\t        self.base_task_total = base_task_total\n\t        self.sub_task_start = sub_task_start\n\t        self.sub_task_total = sub_task_total\n\t    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t        sub_task_progress_frac = current / total\n\t        sub_task_progress = self.sub_task_start + self.sub_task_total * sub_task_progress_frac\n", "        self.base_task_listener.on_progress(sub_task_progress, self.base_task_total)\n\t    def on_finished(self):\n\t        self.base_task_listener.on_progress(self.sub_task_start + self.sub_task_total, self.base_task_total)"]}
{"filename": "src/whisper/whisperFactory.py", "chunked_list": ["from typing import List\n\tfrom src import modelCache\n\tfrom src.config import ModelConfig\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperContainer\n\tdef create_whisper_container(whisper_implementation: str, \n\t                             model_name: str, device: str = None, compute_type: str = \"float16\",\n\t                             download_root: str = None,\n\t                             cache: modelCache = None, models: List[ModelConfig] = []) -> AbstractWhisperContainer:\n\t    print(\"Creating whisper container for \" + whisper_implementation + \" model \" + model_name)\n\t    if (whisper_implementation == \"whisper\"):\n", "        from src.whisper.whisperContainer import WhisperContainer\n\t        return WhisperContainer(model_name=model_name, device=device, compute_type=compute_type, download_root=download_root, cache=cache, models=models)\n\t    elif (whisper_implementation == \"faster-whisper\" or whisper_implementation == \"faster_whisper\"):\n\t        from src.whisper.fasterWhisperContainer import FasterWhisperContainer\n\t        return FasterWhisperContainer(model_name=model_name, device=device, compute_type=compute_type, download_root=download_root, cache=cache, models=models)\n\t    else:\n\t        raise ValueError(\"Unknown Whisper implementation: \" + whisper_implementation)"]}
{"filename": "src/whisper/whisperContainer.py", "chunked_list": ["# External programs\n\timport abc\n\timport os\n\timport sys\n\tfrom typing import List\n\tfrom urllib.parse import urlparse\n\timport torch\n\timport urllib3\n\tfrom src.hooks.progressListener import ProgressListener\n\timport whisper\n", "from whisper import Whisper\n\tfrom src.config import ModelConfig, VadInitialPromptMode\n\tfrom src.hooks.whisperProgressHook import create_progress_listener_handle\n\tfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\n\tfrom src.prompts.abstractPromptStrategy import AbstractPromptStrategy\n\tfrom src.utils import download_file\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback, AbstractWhisperContainer\n\tclass WhisperContainer(AbstractWhisperContainer):\n\t    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n\t                 download_root: str = None,\n", "                 cache: ModelCache = None, models: List[ModelConfig] = []):\n\t        if device is None:\n\t            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\t        super().__init__(model_name, device, compute_type, download_root, cache, models)\n\t    def ensure_downloaded(self):\n\t        \"\"\"\n\t        Ensure that the model is downloaded. This is useful if you want to ensure that the model is downloaded before\n\t        passing the container to a subprocess.\n\t        \"\"\"\n\t        # Warning: Using private API here\n", "        try:\n\t            root_dir = self.download_root\n\t            model_config = self._get_model_config()\n\t            if root_dir is None:\n\t                root_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"whisper\")\n\t            if self.model_name in whisper._MODELS:\n\t                whisper._download(whisper._MODELS[self.model_name], root_dir, False)\n\t            else:\n\t                # If the model is not in the official list, see if it needs to be downloaded\n\t                model_config.download_url(root_dir)\n", "            return True\n\t        except Exception as e:\n\t            # Given that the API is private, it could change at any time. We don't want to crash the program\n\t            print(\"Error pre-downloading model: \" + str(e))\n\t            return False\n\t    def _get_model_config(self) -> ModelConfig:\n\t        \"\"\"\n\t        Get the model configuration for the model.\n\t        \"\"\"\n\t        for model in self.models:\n", "            if model.name == self.model_name:\n\t                return model\n\t        return None\n\t    def _create_model(self):\n\t        print(\"Loading whisper model \" + self.model_name)\n\t        model_config = self._get_model_config()\n\t        # Note that the model will not be downloaded in the case of an official Whisper model\n\t        model_path = self._get_model_path(model_config, self.download_root)\n\t        return whisper.load_model(model_path, device=self.device, download_root=self.download_root)\n\t    def create_callback(self, language: str = None, task: str = None, \n", "                        prompt_strategy: AbstractPromptStrategy = None,\n\t                        **decodeOptions: dict) -> AbstractWhisperCallback:\n\t        \"\"\"\n\t        Create a WhisperCallback object that can be used to transcript audio files.\n\t        Parameters\n\t        ----------\n\t        language: str\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n", "        prompt_strategy: AbstractPromptStrategy\n\t            The prompt strategy to use. If not specified, the prompt from Whisper will be used.\n\t        decodeOptions: dict\n\t            Additional options to pass to the decoder. Must be pickleable.\n\t        Returns\n\t        -------\n\t        A WhisperCallback object.\n\t        \"\"\"\n\t        return WhisperCallback(self, language=language, task=task, prompt_strategy=prompt_strategy, **decodeOptions)\n\t    def _get_model_path(self, model_config: ModelConfig, root_dir: str = None):\n", "        from src.conversion.hf_converter import convert_hf_whisper\n\t        \"\"\"\n\t        Download the model.\n\t        Parameters\n\t        ----------\n\t        model_config: ModelConfig\n\t            The model configuration.\n\t        \"\"\"\n\t        # See if path is already set\n\t        if model_config.path is not None:\n", "            return model_config.path\n\t        if root_dir is None:\n\t            root_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"whisper\")\n\t        model_type = model_config.type.lower() if model_config.type is not None else \"whisper\"\n\t        if model_type in [\"huggingface\", \"hf\"]:\n\t            model_config.path = model_config.url\n\t            destination_target = os.path.join(root_dir, model_config.name + \".pt\")\n\t            # Convert from HuggingFace format to Whisper format\n\t            if os.path.exists(destination_target):\n\t                print(f\"File {destination_target} already exists, skipping conversion\")\n", "            else:\n\t                print(\"Saving HuggingFace model in Whisper format to \" + destination_target)\n\t                convert_hf_whisper(model_config.url, destination_target)\n\t            model_config.path = destination_target\n\t        elif model_type in [\"whisper\", \"w\"]:\n\t            model_config.path = model_config.url\n\t            # See if URL is just a file\n\t            if model_config.url in whisper._MODELS:\n\t                # No need to download anything - Whisper will handle it\n\t                model_config.path = model_config.url\n", "            elif model_config.url.startswith(\"file://\"):\n\t                # Get file path\n\t                model_config.path = urlparse(model_config.url).path\n\t            # See if it is an URL\n\t            elif model_config.url.startswith(\"http://\") or model_config.url.startswith(\"https://\"):\n\t                # Extension (or file name)\n\t                extension = os.path.splitext(model_config.url)[-1]\n\t                download_target = os.path.join(root_dir, model_config.name + extension)\n\t                if os.path.exists(download_target) and not os.path.isfile(download_target):\n\t                    raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n", "                if not os.path.isfile(download_target):\n\t                    download_file(model_config.url, download_target)\n\t                else:\n\t                    print(f\"File {download_target} already exists, skipping download\")\n\t                model_config.path = download_target\n\t            # Must be a local file\n\t            else:\n\t                model_config.path = model_config.url\n\t        else:\n\t            raise ValueError(f\"Unknown model type {model_type}\")\n", "        return model_config.path\n\tclass WhisperCallback(AbstractWhisperCallback):\n\t    def __init__(self, model_container: WhisperContainer, language: str = None, task: str = None, \n\t                 prompt_strategy: AbstractPromptStrategy = None, \n\t                 **decodeOptions: dict):\n\t        self.model_container = model_container\n\t        self.language = language\n\t        self.task = task\n\t        self.prompt_strategy = prompt_strategy\n\t        self.decodeOptions = decodeOptions\n", "    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n\t        \"\"\"\n\t        Peform the transcription of the given audio file or data.\n\t        Parameters\n\t        ----------\n\t        audio: Union[str, np.ndarray, torch.Tensor]\n\t            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n\t        segment_index: int\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n", "            The task - either translate or transcribe.\n\t        progress_listener: ProgressListener\n\t            A callback to receive progress updates.\n\t        \"\"\"\n\t        model = self.model_container.get_model()\n\t        if progress_listener is not None:\n\t            with create_progress_listener_handle(progress_listener):\n\t                return self._transcribe(model, audio, segment_index, prompt, detected_language)\n\t        else:\n\t            return self._transcribe(model, audio, segment_index, prompt, detected_language)\n", "    def _transcribe(self, model: Whisper, audio, segment_index: int, prompt: str, detected_language: str):\n\t        decodeOptions = self.decodeOptions.copy()\n\t        # Add fp16\n\t        if self.model_container.compute_type in [\"fp16\", \"float16\"]:\n\t            decodeOptions[\"fp16\"] = True\n\t        initial_prompt = self.prompt_strategy.get_segment_prompt(segment_index, prompt, detected_language) \\\n\t                           if self.prompt_strategy else prompt\n\t        result = model.transcribe(audio, \\\n\t            language=self.language if self.language else detected_language, task=self.task, \\\n\t            initial_prompt=initial_prompt, \\\n", "            **decodeOptions\n\t        )\n\t        # If we have a prompt strategy, we need to increment the current prompt\n\t        if self.prompt_strategy:\n\t            self.prompt_strategy.on_segment_finished(segment_index, prompt, detected_language, result)\n\t        return result"]}
{"filename": "src/whisper/abstractWhisperContainer.py", "chunked_list": ["import abc\n\tfrom typing import List\n\tfrom src.config import ModelConfig, VadInitialPromptMode\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\n\tfrom src.prompts.abstractPromptStrategy import AbstractPromptStrategy\n\tclass AbstractWhisperCallback:\n\t    def __init__(self):\n\t        self.__prompt_mode_gpt = None\n\t    @abc.abstractmethod\n", "    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n\t        \"\"\"\n\t        Peform the transcription of the given audio file or data.\n\t        Parameters\n\t        ----------\n\t        audio: Union[str, np.ndarray, torch.Tensor]\n\t            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n\t        segment_index: int\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n", "            The task - either translate or transcribe.\n\t        progress_listener: ProgressListener\n\t            A callback to receive progress updates.\n\t        \"\"\"\n\t        raise NotImplementedError()\n\tclass AbstractWhisperContainer:\n\t    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n\t                 download_root: str = None,\n\t                 cache: ModelCache = None, models: List[ModelConfig] = []):\n\t        self.model_name = model_name\n", "        self.device = device\n\t        self.compute_type = compute_type\n\t        self.download_root = download_root\n\t        self.cache = cache\n\t        # Will be created on demand\n\t        self.model = None\n\t        # List of known models\n\t        self.models = models\n\t    def get_model(self):\n\t        if self.model is None:\n", "            if (self.cache is None):\n\t                self.model = self._create_model()\n\t            else:\n\t                model_key = \"WhisperContainer.\" + self.model_name + \":\" + (self.device if self.device else '')\n\t                self.model = self.cache.get(model_key, self._create_model)\n\t        return self.model\n\t    @abc.abstractmethod\n\t    def _create_model(self):\n\t        raise NotImplementedError()\n\t    def ensure_downloaded(self):\n", "        pass\n\t    @abc.abstractmethod\n\t    def create_callback(self, language: str = None, task: str = None, \n\t                        prompt_strategy: AbstractPromptStrategy = None, \n\t                        **decodeOptions: dict) -> AbstractWhisperCallback:\n\t        \"\"\"\n\t        Create a WhisperCallback object that can be used to transcript audio files.\n\t        Parameters\n\t        ----------\n\t        language: str\n", "            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n\t        prompt_strategy: AbstractPromptStrategy\n\t            The prompt strategy to use for the transcription.\n\t        decodeOptions: dict\n\t            Additional options to pass to the decoder. Must be pickleable.\n\t        Returns\n\t        -------\n\t        A WhisperCallback object.\n", "        \"\"\"\n\t        raise NotImplementedError()\n\t    # This is required for multiprocessing\n\t    def __getstate__(self):\n\t        return { \n\t            \"model_name\": self.model_name, \n\t            \"device\": self.device, \n\t            \"download_root\": self.download_root, \n\t            \"models\": self.models, \n\t            \"compute_type\": self.compute_type \n", "        }\n\t    def __setstate__(self, state):\n\t        self.model_name = state[\"model_name\"]\n\t        self.device = state[\"device\"]\n\t        self.download_root = state[\"download_root\"]\n\t        self.models = state[\"models\"]\n\t        self.compute_type = state[\"compute_type\"]\n\t        self.model = None\n\t        # Depickled objects must use the global cache\n\t        self.cache = GLOBAL_MODEL_CACHE"]}
{"filename": "src/whisper/fasterWhisperContainer.py", "chunked_list": ["import os\n\tfrom typing import List, Union\n\tfrom faster_whisper import WhisperModel, download_model\n\tfrom src.config import ModelConfig, VadInitialPromptMode\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.languages import get_language_from_name\n\tfrom src.modelCache import ModelCache\n\tfrom src.prompts.abstractPromptStrategy import AbstractPromptStrategy\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback, AbstractWhisperContainer\n\tfrom src.utils import format_timestamp\n", "class FasterWhisperContainer(AbstractWhisperContainer):\n\t    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n\t                       download_root: str = None,\n\t                       cache: ModelCache = None, models: List[ModelConfig] = []):\n\t        super().__init__(model_name, device, compute_type, download_root, cache, models)\n\t    def ensure_downloaded(self):\n\t        \"\"\"\n\t        Ensure that the model is downloaded. This is useful if you want to ensure that the model is downloaded before\n\t        passing the container to a subprocess.\n\t        \"\"\"\n", "        model_config = self._get_model_config()\n\t        if os.path.isdir(model_config.url):\n\t            model_config.path = model_config.url\n\t        else:\n\t            model_config.path = download_model(model_config.url, output_dir=self.download_root)\n\t    def _get_model_config(self) -> ModelConfig:\n\t        \"\"\"\n\t        Get the model configuration for the model.\n\t        \"\"\"\n\t        for model in self.models:\n", "            if model.name == self.model_name:\n\t                return model\n\t        return None\n\t    def _create_model(self):\n\t        print(\"Loading faster whisper model \" + self.model_name + \" for device \" + str(self.device))\n\t        model_config = self._get_model_config()\n\t        model_url = model_config.url\n\t        if model_config.type == \"whisper\":\n\t            if model_url not in [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v1\", \"large-v2\"]:\n\t                raise Exception(\"FasterWhisperContainer does not yet support Whisper models. Use ct2-transformers-converter to convert the model to a faster-whisper model.\")\n", "            if model_url == \"large\":\n\t                # large is an alias for large-v1\n\t                model_url = \"large-v1\"\n\t        device = self.device\n\t        if (device is None):\n\t            device = \"auto\"\n\t        model = WhisperModel(model_url, device=device, compute_type=self.compute_type)\n\t        return model\n\t    def create_callback(self, language: str = None, task: str = None, \n\t                        prompt_strategy: AbstractPromptStrategy = None, \n", "                        **decodeOptions: dict) -> AbstractWhisperCallback:\n\t        \"\"\"\n\t        Create a WhisperCallback object that can be used to transcript audio files.\n\t        Parameters\n\t        ----------\n\t        language: str\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n\t        prompt_strategy: AbstractPromptStrategy\n", "            The prompt strategy to use. If not specified, the prompt from Whisper will be used.\n\t        decodeOptions: dict\n\t            Additional options to pass to the decoder. Must be pickleable.\n\t        Returns\n\t        -------\n\t        A WhisperCallback object.\n\t        \"\"\"\n\t        return FasterWhisperCallback(self, language=language, task=task, prompt_strategy=prompt_strategy, **decodeOptions)\n\tclass FasterWhisperCallback(AbstractWhisperCallback):\n\t    def __init__(self, model_container: FasterWhisperContainer, language: str = None, task: str = None, \n", "                 prompt_strategy: AbstractPromptStrategy = None, \n\t                 **decodeOptions: dict):\n\t        self.model_container = model_container\n\t        self.language = language\n\t        self.task = task\n\t        self.prompt_strategy = prompt_strategy\n\t        self.decodeOptions = decodeOptions\n\t        self._printed_warning = False\n\t    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n\t        \"\"\"\n", "        Peform the transcription of the given audio file or data.\n\t        Parameters\n\t        ----------\n\t        audio: Union[str, np.ndarray, torch.Tensor]\n\t            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n\t        segment_index: int\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n\t        progress_listener: ProgressListener\n", "            A callback to receive progress updates.\n\t        \"\"\"\n\t        model: WhisperModel = self.model_container.get_model()\n\t        language_code = self._lookup_language_code(self.language) if self.language else None\n\t        # Copy decode options and remove options that are not supported by faster-whisper\n\t        decodeOptions = self.decodeOptions.copy()\n\t        verbose = decodeOptions.pop(\"verbose\", None)\n\t        logprob_threshold = decodeOptions.pop(\"logprob_threshold\", None)\n\t        patience = decodeOptions.pop(\"patience\", None)\n\t        length_penalty = decodeOptions.pop(\"length_penalty\", None)\n", "        suppress_tokens = decodeOptions.pop(\"suppress_tokens\", None)\n\t        if (decodeOptions.pop(\"fp16\", None) is not None):\n\t            if not self._printed_warning:\n\t                print(\"WARNING: fp16 option is ignored by faster-whisper - use compute_type instead.\")\n\t            self._printed_warning = True\n\t        # Fix up decode options\n\t        if (logprob_threshold is not None):\n\t            decodeOptions[\"log_prob_threshold\"] = logprob_threshold\n\t        decodeOptions[\"patience\"] = float(patience) if patience is not None else 1.0\n\t        decodeOptions[\"length_penalty\"] = float(length_penalty) if length_penalty is not None else 1.0\n", "        # See if supress_tokens is a string - if so, convert it to a list of ints\n\t        decodeOptions[\"suppress_tokens\"] = self._split_suppress_tokens(suppress_tokens)\n\t        initial_prompt = self.prompt_strategy.get_segment_prompt(segment_index, prompt, detected_language) \\\n\t                           if self.prompt_strategy else prompt\n\t        segments_generator, info = model.transcribe(audio, \\\n\t            language=language_code if language_code else detected_language, task=self.task, \\\n\t            initial_prompt=initial_prompt, \\\n\t            **decodeOptions\n\t        )\n\t        segments = []\n", "        for segment in segments_generator:\n\t            segments.append(segment)\n\t            if progress_listener is not None:\n\t                progress_listener.on_progress(segment.end, info.duration)\n\t            if verbose:\n\t                print(\"[{}->{}] {}\".format(format_timestamp(segment.start, True), format_timestamp(segment.end, True),\n\t                                          segment.text))\n\t        text = \" \".join([segment.text for segment in segments])\n\t        # Convert the segments to a format that is easier to serialize\n\t        whisper_segments = [{\n", "            \"text\": segment.text,\n\t            \"start\": segment.start,\n\t            \"end\": segment.end,\n\t            # Extra fields added by faster-whisper\n\t            \"words\": [{\n\t                \"start\": word.start,\n\t                \"end\": word.end,\n\t                \"word\": word.word,\n\t                \"probability\": word.probability\n\t            } for word in (segment.words if segment.words is not None else []) ]\n", "        } for segment in segments]\n\t        result = {\n\t            \"segments\": whisper_segments,\n\t            \"text\": text,\n\t            \"language\": info.language if info else None,\n\t            # Extra fields added by faster-whisper\n\t            \"language_probability\": info.language_probability if info else None,\n\t            \"duration\": info.duration if info else None\n\t        }\n\t        # If we have a prompt strategy, we need to increment the current prompt\n", "        if self.prompt_strategy:\n\t            self.prompt_strategy.on_segment_finished(segment_index, prompt, detected_language, result)\n\t        if progress_listener is not None:\n\t            progress_listener.on_finished()\n\t        return result\n\t    def _split_suppress_tokens(self, suppress_tokens: Union[str, List[int]]):\n\t        if (suppress_tokens is None):\n\t            return None\n\t        if (isinstance(suppress_tokens, list)):\n\t            return suppress_tokens\n", "        return [int(token) for token in suppress_tokens.split(\",\")]\n\t    def _lookup_language_code(self, language: str):\n\t        language = get_language_from_name(language)\n\t        if language is None:\n\t            raise ValueError(\"Invalid language: \" + language)\n\t        return language.code\n"]}
{"filename": "src/conversion/hf_converter.py", "chunked_list": ["# https://github.com/bayartsogt-ya/whisper-multiple-hf-datasets\n\tfrom copy import deepcopy\n\timport torch\n\tWHISPER_MAPPING = {\n\t    \"layers\": \"blocks\",\n\t    \"fc1\": \"mlp.0\",\n\t    \"fc2\": \"mlp.2\",\n\t    \"final_layer_norm\": \"mlp_ln\",\n\t    \"layers\": \"blocks\",\n\t    \".self_attn.q_proj\": \".attn.query\",\n", "    \".self_attn.k_proj\": \".attn.key\",\n\t    \".self_attn.v_proj\": \".attn.value\",\n\t    \".self_attn_layer_norm\": \".attn_ln\",\n\t    \".self_attn.out_proj\": \".attn.out\",\n\t    \".encoder_attn.q_proj\": \".cross_attn.query\",\n\t    \".encoder_attn.k_proj\": \".cross_attn.key\",\n\t    \".encoder_attn.v_proj\": \".cross_attn.value\",\n\t    \".encoder_attn_layer_norm\": \".cross_attn_ln\",\n\t    \".encoder_attn.out_proj\": \".cross_attn.out\",\n\t    \"decoder.layer_norm.\": \"decoder.ln.\",\n", "    \"encoder.layer_norm.\": \"encoder.ln_post.\",\n\t    \"embed_tokens\": \"token_embedding\",\n\t    \"encoder.embed_positions.weight\": \"encoder.positional_embedding\",\n\t    \"decoder.embed_positions.weight\": \"decoder.positional_embedding\",\n\t    \"layer_norm\": \"ln_post\",\n\t}\n\tdef rename_keys(s_dict):\n\t    keys = list(s_dict.keys())\n\t    for key in keys:\n\t        new_key = key\n", "        for k, v in WHISPER_MAPPING.items():\n\t            if k in key:\n\t                new_key = new_key.replace(k, v)\n\t        print(f\"{key} -> {new_key}\")\n\t        s_dict[new_key] = s_dict.pop(key)\n\t    return s_dict\n\tdef convert_hf_whisper(hf_model_name_or_path: str, whisper_state_path: str):\n\t    from transformers import WhisperForConditionalGeneration\n\t    transformer_model = WhisperForConditionalGeneration.from_pretrained(hf_model_name_or_path)\n\t    config = transformer_model.config\n", "    # first build dims\n\t    dims = {\n\t        'n_mels': config.num_mel_bins,\n\t        'n_vocab': config.vocab_size,\n\t        'n_audio_ctx': config.max_source_positions,\n\t        'n_audio_state': config.d_model,\n\t        'n_audio_head': config.encoder_attention_heads,\n\t        'n_audio_layer': config.encoder_layers,\n\t        'n_text_ctx': config.max_target_positions,\n\t        'n_text_state': config.d_model,\n", "        'n_text_head': config.decoder_attention_heads,\n\t        'n_text_layer': config.decoder_layers\n\t    }\n\t    state_dict = deepcopy(transformer_model.model.state_dict())\n\t    state_dict = rename_keys(state_dict)\n\t    torch.save({\"dims\": dims, \"model_state_dict\": state_dict}, whisper_state_path)"]}
