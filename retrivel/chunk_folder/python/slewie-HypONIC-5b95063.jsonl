{"filename": "setup.py", "chunked_list": ["import setuptools\n\tdef readme():\n\t    with open('README.md') as f:\n\t        return f.read()\n\tsetuptools.setup(\n\t    name='hyponic',\n\t    version='0.1.1',\n\t    author='Vladislav Kulikov, Daniel Satarov, Ivan Chernakov',\n\t    author_email='v.kulikov@innopolis.university, d.satarov@innopolis.university, i.chernakov@innopolis.university',\n\t    description='Hyperparameter Optimization with Nature-Inspired Computing',\n", "    long_description=readme(),\n\t    long_description_content_type='text/markdown',\n\t    url='https://github.com/slewie/HypONIC',\n\t    packages=setuptools.find_packages(),\n\t    install_requires=[\n\t        'numpy>=1.23.5',\n\t        'numexpr>=2.8.4',\n\t        'numba>=0.57.0',\n\t        'matplotlib>=3.6.3',\n\t    ],\n", "    classifiers=[\n\t        'Programming Language :: Python :: 3.11',\n\t        'License :: OSI Approved :: MIT License',\n\t        'Operating System :: OS Independent',\n\t    ],\n\t    python_requires='>=3.10',\n\t)\n"]}
{"filename": "documentations/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# For the full list of built-in configuration values, see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\timport os\n\timport sys\n\tsys.path.insert(0, os.path.abspath('.'))\n\tsys.path.insert(0, os.path.abspath('../../'))\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n", "project = 'HypONIC'\n\tcopyright = '2023, Vladislav Kulikov, Daniel Satarov, Ivan Chernakov'\n\tauthor = 'Vladislav Kulikov, Daniel Satarov, Ivan Chernakov'\n\t# -- General configuration ---------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\textensions = [\n\t    'sphinx.ext.autodoc',\n\t    'sphinx.ext.napoleon',\n\t    'sphinx.ext.viewcode',\n\t]\n", "templates_path = ['_templates']\n\texclude_patterns = []\n\t# -- Options for HTML output -------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\thtml_theme = 'sphinx_rtd_theme'\n\thtml_static_path = ['_static']\n"]}
{"filename": "hyponic/hyponic.py", "chunked_list": ["from warnings import warn\n\tfrom hyponic.optimizers.swarm_based.PSO import IWPSO\n\tfrom hyponic.metrics.decorators import METRICS_DICT\n\tfrom hyponic.utils.problem_identifier import ProblemIdentifier, ProblemType\n\tfrom hyponic import config\n\tfrom typing import Callable\n\tfrom hyponic.space import Space\n\tfrom functools import partial\n\timport numpy as np\n\tclass HypONIC:\n", "    \"\"\"\n\t    HypONIC (Hyperparameter Optimization using Nature-Inspired Computing)\n\t    Main class for hyperparameter optimization.\n\t    \"\"\"\n\t    def __init__(self, model, X, y, metric: Callable | str | None = None, optimizer=None, **kwargs):\n\t        self.model = model\n\t        try:\n\t            self.X = np.array(X)\n\t            self.y = np.ravel(np.array(y))\n\t        except Exception as e:\n", "            raise Exception(f\"X and y must be convertible to numpy array. Error: {e}\")\n\t        if isinstance(metric, str):\n\t            # Try to get metric from the METRICS_DICT\n\t            self.metric = METRICS_DICT.get(metric, None)\n\t            if self.metric is None:\n\t                raise Exception(f\"Metric {metric} is not found.\")\n\t        elif isinstance(metric, Callable):\n\t            self.metric = metric\n\t        elif metric is None:\n\t            # If metric is None, then try to get metric from the problem type\n", "            problem_type = ProblemIdentifier(self.y).get_problem_type()\n\t            match problem_type:\n\t                case ProblemType.REGRESSION:\n\t                    self.metric = METRICS_DICT[\"mse\"]\n\t                case ProblemType.BINARY_CLASSIFICATION:\n\t                    self.metric = METRICS_DICT[\"binary_crossentropy\"]\n\t                case ProblemType.MULTICLASS_CLASSIFICATION:\n\t                    self.metric = METRICS_DICT[\"log_loss\"]\n\t        else:\n\t            raise Exception(f\"Metric {metric} is not found.\")\n", "        try:\n\t            self.minmax = self.metric.__getattribute__(\"minmax\")\n\t        except AttributeError:\n\t            # If a metric does not have minmax attribute,\n\t            # then it is assumed to be a custom metric and will be minimized by default\n\t            warn(f\"Metric {metric.__name__} does not have minmax attribute. Minimize by default.\")\n\t            self.minmax = \"min\"\n\t        if kwargs is None:  # Default values for optimizer\n\t            kwargs = {\"epoch\": 10, \"pop_size\": 10}\n\t        if optimizer is None:\n", "            self.optimizer = IWPSO(**kwargs)\n\t        else:\n\t            self.optimizer = optimizer(**kwargs)\n\t        self.hyperparams_optimized = None\n\t        self.metric_optimized = None\n\t    @staticmethod\n\t    def warn_not_optimized(func):\n\t        \"\"\"\n\t        Decorator that warns if a method is called before optimization.\n\t        \"\"\"\n", "        def wrapper(*args, **kwargs):\n\t            if args[0].hyperparams_optimized is None:\n\t                raise Exception(\"Model is not optimized yet. Please call optimize method first\")\n\t            return func(*args, **kwargs)\n\t        return wrapper\n\t    def _fitness_wrapper(self, dimensions_names: list, mapping_funcs: dict, values: list) -> float:\n\t        # Map back to original space\n\t        hyperparams = {\n\t            dim: mapping_funcs[dim](val) for dim, val in zip(dimensions_names, values)\n\t        }\n", "        self.model.set_params(**hyperparams)\n\t        self.model.fit(self.X, self.y)\n\t        y_pred = self.model.predict(self.X)\n\t        return self.metric(self.y, y_pred)  # TODO: maybe cross-validation could be used instead\n\t    def optimize(self, hyperparams: dict | None = None, verbose=False,\n\t                 models_config=config.sklearn_models.models_dict) -> (dict, float):\n\t        print(self.model.__class__)\n\t        if hyperparams is None:\n\t            hyperparams = models_config.get(str(self.model.__class__), dict())\n\t        # Create a space for hyperparameters\n", "        hyperspace = Space(hyperparams)\n\t        if verbose:\n\t            print(\"Successfully created a space for hyperparameters optimization\")\n\t            print(f\"Using {self.optimizer.__class__.__name__} optimizer\")\n\t            print(f\"Metric {self.metric.__name__} is subject to {self.minmax}imization\")\n\t            print(hyperspace)\n\t        # Map hyperparameters to continuous space\n\t        mappings_with_bounds = hyperspace.get_continuous_mappings(origins=0)  # Make that all dimensions start from 0\n\t        # Split mappings and bounds\n\t        mapping_funcs = {}\n", "        low_bounds = []\n\t        highs_bounds = []\n\t        for name in hyperspace.dimensions_names:\n\t            mapping, (low, high) = mappings_with_bounds[name]\n\t            mapping_funcs[name] = mapping\n\t            low_bounds.append(low)\n\t            highs_bounds.append(high)\n\t        paramspace = {\n\t            \"fit_func\": partial(self._fitness_wrapper, hyperspace.dimensions_names, mapping_funcs),\n\t            \"lb\": low_bounds,\n", "            \"ub\": highs_bounds,\n\t            \"minmax\": self.minmax\n\t        }\n\t        hyperparams_optimized, metric_optimized = self.optimizer.solve(paramspace, verbose=verbose)\n\t        # Map back to the original space\n\t        hyperparams_optimized = {\n\t            dim: mapping_funcs[dim](val) for dim, val in zip(hyperspace.dimensions_names, hyperparams_optimized)\n\t        }\n\t        self.hyperparams_optimized = hyperparams_optimized\n\t        self.metric_optimized = metric_optimized\n", "        return hyperparams_optimized, metric_optimized\n\t    @warn_not_optimized\n\t    def get_optimized_model(self):\n\t        self.model.set_params(**self.hyperparams_optimized)\n\t        self.model.fit(self.X, self.y)\n\t        return self.model\n\t    @warn_not_optimized\n\t    def get_optimized_parameters(self) -> dict:\n\t        return self.hyperparams_optimized\n\t    @warn_not_optimized\n", "    def get_optimized_metric(self) -> float:\n\t        return self.metric_optimized\n\t    @warn_not_optimized\n\t    def visualize_history_fitness(self):\n\t        self.optimizer.visualize_history_fitness()\n\t    @warn_not_optimized\n\t    def visualize_history_time(self):\n\t        self.optimizer.visualize_history_time()\n"]}
{"filename": "hyponic/space.py", "chunked_list": ["\"\"\"\n\tThis module contains the Space class, which is used to define the search space.\n\t\"\"\"\n\timport numpy as np\n\tfrom typing import Callable, Any\n\tclass Space:\n\t    \"\"\"\n\t    A class that represents the search space of a hyperparameter optimization problem.\n\t    \"\"\"\n\t    def __init__(self, in_dict: dict[str, Any]):\n", "        self.__dict = dict\n\t        self.dimensions = {}\n\t        self.dimensions_names = []\n\t        for k, v in in_dict.items():\n\t            # Converting range to list\n\t            if isinstance(v, range):\n\t                v = list(v)\n\t            if isinstance(v, Discrete):\n\t                self.dimensions[k] = v\n\t            elif isinstance(v, Continuous):\n", "                self.dimensions[k] = v\n\t            elif isinstance(v, tuple):\n\t                if len(v) != 2:\n\t                    raise ValueError(f\"Value for key {k} is not valid\")\n\t                self.dimensions[k] = Continuous(*v, name=k)\n\t            elif isinstance(v, list):\n\t                self.dimensions[k] = Discrete(v, name=k)\n\t            else:\n\t                raise ValueError(f\"Value for key {k} is not valid\")\n\t            self.dimensions_names.append(k)\n", "    def get_continuous_mappings(\n\t            self, scales: dict | int | float = None, origins: dict | int | float = None\n\t    ) -> dict[str, (Callable, (float, float))]:\n\t        \"\"\"\n\t        Returns a function that maps a discrete value to a continuous value.\n\t        \"\"\"\n\t        if scales is None:\n\t            scales = {}\n\t        elif isinstance(scales, (int, float)):\n\t            scales = {key: scales for key in self.dimensions}\n", "        if origins is None:\n\t            origins = {}\n\t        elif isinstance(origins, (int, float)):\n\t            origins = {key: origins for key in self.dimensions}\n\t        mappings = {}\n\t        for key in self.dimensions:\n\t            mappings[key] = self.dimensions[key].get_continuous_mapping(\n\t                scales.get(key, 1),\n\t                origins.get(key, None)\n\t            )\n", "        return mappings\n\t    def map_to_original_space(self, values: list[float]) -> dict[str, Any]:\n\t        \"\"\"\n\t        Maps a list of values from the continuous space to the original space.\n\t        \"\"\"\n\t        mappings = self.get_continuous_mappings()\n\t        return {\n\t            name: mappings[name][0](value) for name, value in zip(self.dimensions_names, values)\n\t        }\n\t    def __str__(self):\n", "        if len(self.dimensions) == 0:\n\t            return \"Hyperparameter Search Space is empty.\"\n\t        # Pretty table of dimensions\n\t        dim_names = list(self.dimensions.keys())\n\t        offset = max([len(k) for k in dim_names])\n\t        out_strs = [f\"Hyperparameter Search Space with {len(dim_names)} dimensions:\"]\n\t        out_strs += [\n\t            f\"\\t{k}: {'-'*(offset - len(k))} {self.dimensions[k]}\" for k in dim_names\n\t        ]\n\t        return \"\\n\".join(out_strs)\n", "class Dimension:\n\t    def __init__(self, lbound, ubound, name=None):\n\t        self.name = name\n\t        self._lbound = lbound\n\t        self._ubound = ubound\n\t    def get_continuous_mapping(self, scale=1, origin=None) -> (Callable, (float, float)):\n\t        \"\"\"\n\t        Returns a function that maps set of values to a continuous value\n\t        \"\"\"\n\t        assert scale != 0, \"Scale cannot be 0.\"\n", "        if origin is None:\n\t            origin = self._lbound\n\t        low = origin\n\t        high = origin + (self._ubound - self._lbound) * scale\n\t        def mapping_func(x):\n\t            x = np.clip(x, low, high)\n\t            x = self._lbound + (x - origin) / scale\n\t            return self.get_value(x)\n\t        return mapping_func, (low, high)\n\t    def get_value(self, x):\n", "        raise NotImplementedError\n\t    def __str__(self):\n\t        return self.__class__.__name__\n\t    def __repr__(self):\n\t        return self.__str__()\n\tclass Continuous(Dimension):\n\t    def __init__(self, low, high, name=None):\n\t        super().__init__(low, high, name)\n\t    def get_value(self, x):\n\t        # Note that x is already in the correct range. No need to clip.\n", "        return x\n\t    def __str__(self):\n\t        return super().__str__() + f\"({self._lbound}, {self._ubound})\"\n\t    def __repr__(self):\n\t        return self.__str__()\n\tclass Discrete(Dimension):\n\t    def __init__(self, values, name=None):\n\t        super().__init__(0, len(values), name)\n\t        self.values = np.array(values)\n\t    def get_value(self, x):\n", "        x = np.clip(x, 0, len(self.values) - 1)\n\t        return self.values[int(x)]\n\t    def __str__(self):\n\t        return super().__str__() + f\"{self.values}\"\n\t    def __repr__(self):\n\t        return self.__str__()\n"]}
{"filename": "hyponic/__init__.py", "chunked_list": ["__version__ = '0.1.1'\n\tfrom .hyponic import HypONIC\n\tfrom .metrics.decorators import add_metric_to_dict, add_metric_info,\\\n\t    add_metric_aliases, minimize_metric, maximize_metric\n\tfrom .metrics.classification import accuracy, precision, recall, f1_score,\\\n\t    fbeta, confusion_matrix, binary_crossentropy, categorical_crossentropy, log_loss\n\tfrom .metrics.regression import mae, mse, rmse, rmsle, r2, adjusted_r2, huber_loss\n\tfrom .optimizers.genetic_based.GA import GA\n\tfrom .optimizers.physics_based.SA import SA\n\tfrom .optimizers.swarm_based.ABC import ABC\n", "from .optimizers.swarm_based.ACO import ACO\n\tfrom .optimizers.swarm_based.PSO import PSO, IWPSO\n\tfrom .optimizers.swarm_based.GWO import GWO\n\tfrom .optimizers.swarm_based.CS import CS\n\tfrom .optimizers.base_optimizer import BaseOptimizer\n\tfrom .space import Space, Dimension, Continuous, Discrete\n\tfrom .utils.history import History\n\tfrom .utils.problem_identifier import ProblemIdentifier, ProblemType\n"]}
{"filename": "hyponic/utils/symtable.py", "chunked_list": ["class Symbol:\n\t    def __init__(self, symtable, name, *args, **kwargs):\n\t        self._symtable = symtable\n\t        self._name = name\n\t        self._args = args\n\t        self._kwargs = kwargs  # TODO: Implement kwargs\n\t    def __call__(self, *args):\n\t        return Symbol(self._symtable, self._name, *args)\n\t    def __repr__(self):\n\t        return f\"{self._name}({', '.join(repr(arg) for arg in self._args)})\"\n", "    def eval(self):\n\t        args = [arg.eval() if isinstance(arg, Symbol) else arg for arg in self._args]\n\t        # If all arguments are literals, evaluate the function\n\t        # Otherwise, evaluate partially\n\t        if all(not isinstance(arg, Symbol) for arg in args):\n\t            if self._name in self._symtable.get_symbol_list():\n\t                return self._symtable.get_symbol(self._name)(*args)\n\t            else:\n\t                return self\n\t        else:\n", "            # Partially evaluate\n\t            self._args = args\n\t            return self\n\tclass SymbolTable:\n\t    def __init__(self):\n\t        self._symbols = {\n\t            \"int\": int,\n\t            \"float\": float,\n\t            \"add\": lambda x, y: x + y,\n\t        }\n", "    def get_symbol(self, name):\n\t        return self._symbols[name]\n\t    def get_symbol_list(self):\n\t        return self._symbols.keys()\n\t    def add_symbol(self, name, func):\n\t        self._symbols[name] = func\n\t    def __getattr__(self, name):\n\t        return Symbol(self, name)\n\t# scope = SymbolTable()\n\t# expr = scope.add(scope.int(1), scope.int(2.5))\n", "# expr.eval()\n"]}
{"filename": "hyponic/utils/history.py", "chunked_list": ["import numpy as np\n\timport matplotlib.pyplot as plt\n\tclass History:\n\t    \"\"\"\n\t    This class is used to track the history of the optimizer. It tracks the global best, current best, and the time of\n\t    the epoch.\n\t    Also, it provides methods to visualize the history of the optimizer.\n\t    \"\"\"\n\t    def __init__(self, **kwargs):\n\t        self.optimizer = kwargs['optimizer']\n", "        self.epoch = kwargs['epoch']\n\t        self.population_size = kwargs['population_size']\n\t        self.global_best_list = np.zeros((self.epoch, self.optimizer.dimensions))  # coordinates of the global best\n\t        self.global_best_fitness_list = np.zeros(self.epoch)  # fitness of the global best\n\t        self.current_best_list = np.zeros((self.epoch, self.optimizer.dimensions))  # coordinates of the current best\n\t        self.current_best_fitness_list = np.zeros(self.epoch)  # fitness of the current best\n\t        self.epoch_time_list = np.zeros(self.epoch)  # time of the current epoch\n\t    def update_history(self, epoch, epoch_time):\n\t        \"\"\"\n\t        This method updates the history of the optimizer\n", "        \"\"\"\n\t        self.global_best_list[epoch] = self.optimizer.get_best_solution()\n\t        self.global_best_fitness_list[epoch] = self.optimizer.get_best_score()\n\t        self.current_best_list[epoch] = self.optimizer.get_best_solution()\n\t        self.current_best_fitness_list[epoch] = self.optimizer.get_best_score()\n\t        self.epoch_time_list[epoch] = epoch_time\n\t    def get_history(self) -> dict:\n\t        \"\"\"\n\t        This method returns the history of the optimizer\n\t        \"\"\"\n", "        return {\n\t            'global_best_list': self.global_best_list,\n\t            'global_best_fitness_list': self.global_best_fitness_list,\n\t            'current_best_list': self.current_best_list,\n\t            'current_best_fitness_list': self.current_best_fitness_list,\n\t            'epoch_time_list': self.epoch_time_list\n\t        }\n\t    def get_global_best(self) -> tuple:\n\t        \"\"\"\n\t        This method returns the global best\n", "        \"\"\"\n\t        return self.global_best_list[-1], self.global_best_fitness_list[-1]\n\t    def get_current_best(self) -> tuple:\n\t        \"\"\"\n\t        This method returns the current best\n\t        \"\"\"\n\t        return self.current_best_list[-1], self.current_best_fitness_list[-1]\n\t    @staticmethod\n\t    def visualize(func):\n\t        \"\"\"\n", "        Decorator to visualize the history of the optimizer\n\t        :param func: function to be decorated\n\t        \"\"\"\n\t        def wrapper(*args, **kwargs):\n\t            plt.figure(figsize=(12, 8))\n\t            func(*args, **kwargs)\n\t            plt.legend()\n\t            plt.show()\n\t        return wrapper\n\t    @visualize\n", "    def visualize_fitness(self):\n\t        \"\"\"\n\t        This method visualizes the history of the optimizer\n\t        \"\"\"\n\t        plt.plot(self.global_best_fitness_list, label='Global Best')\n\t        plt.plot(self.current_best_fitness_list, label='Current Best')\n\t        plt.xlabel('Epochs')\n\t        plt.ylabel('Fitness')\n\t        plt.title('Fitness vs Epochs')\n\t    @visualize\n", "    def visualize_time(self):\n\t        \"\"\"\n\t        This method visualizes the history of the optimizer\n\t        \"\"\"\n\t        plt.plot(self.epoch_time_list, label='Epoch Time')\n\t        plt.xlabel('Epochs')\n\t        plt.ylabel('Time')\n\t        plt.title('Time vs Epochs')\n\t    def is_early_stopping(self, current_epoch, early_stopping: int):\n\t        \"\"\"\n", "        This method checks if the early stopping condition is met\n\t        :param current_epoch: current epoch\n\t        :param early_stopping: number of epochs to wait before stopping the optimization\n\t        :return: boolean\n\t        \"\"\"\n\t        if early_stopping is not None and current_epoch >= early_stopping:\n\t            last_scores = self.global_best_fitness_list[current_epoch - early_stopping + 1:current_epoch + 1]\n\t            if len(last_scores) == early_stopping and np.all(last_scores == last_scores[0]):\n\t                return True\n\t        return False\n"]}
{"filename": "hyponic/utils/problem_identifier.py", "chunked_list": ["import numpy as np\n\tfrom enum import Enum\n\tclass ProblemType(Enum):\n\t    \"\"\"\n\t    This class is used to identify the problem type using dataset. It can be used for regression and classification only\n\t    \"\"\"\n\t    REGRESSION = 1\n\t    BINARY_CLASSIFICATION = 2\n\t    MULTICLASS_CLASSIFICATION = 3\n\tclass ProblemIdentifier:\n", "    \"\"\"\n\t    This class is used to identify the problem type using dataset. It can be used for regression and classification only\n\t    \"\"\"\n\t    def __init__(self, y):\n\t        self.y = y\n\t    def get_problem_type(self):\n\t        \"\"\"\n\t        This method identifies the problem type using number of classes and data type of y.\n\t        If number of classes is 2 and data type of y is int64, then it is a binary classification problem\n\t        If number of classes is more than 2 and data type of y is int64, then it is a multiclass classification problem\n", "        If data type of y is not int64, then it is a regression problem\n\t        \"\"\"\n\t        target_type = type(self.y[0])\n\t        number_of_classes = len(np.unique(self.y))\n\t        if target_type in [np.int64, np.int32, np.int16, np.int8]:\n\t            if number_of_classes == 2:\n\t                return ProblemType.BINARY_CLASSIFICATION\n\t            else:\n\t                return ProblemType.MULTICLASS_CLASSIFICATION\n\t        else:\n", "            return ProblemType.REGRESSION\n"]}
{"filename": "hyponic/utils/__init__.py", "chunked_list": []}
{"filename": "hyponic/config/sklearn_models.py", "chunked_list": ["models_dict = {\n\t    \"<class 'sklearn.svm._classes.SVC'>\": {\n\t        \"C\": (0.01, 1),\n\t        \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n\t        \"degree\": range(2, 6),\n\t    },\n\t    \"<class 'sklearn.svm._classes.SVR'>\": {\n\t        \"C\": (0.01, 1),\n\t        \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n\t        \"degree\": range(2, 6),\n", "    },\n\t    \"<class 'sklearn.ensemble._forest.RandomForestClassifier'>\": {\n\t        \"n_estimators\": range(10, 100, 10),\n\t        \"criterion\": [\"gini\", \"entropy\"],\n\t        \"max_depth\": range(1, 10),\n\t        \"min_samples_split\": range(2, 10),\n\t        \"min_samples_leaf\": range(1, 10),\n\t        \"max_features\": [\"sqrt\", \"log2\"],\n\t    },\n\t    \"<class 'sklearn.ensemble._forest.RandomForestRegressor'>\": {\n", "        \"n_estimators\": range(10, 100, 10),\n\t        \"criterion\": [\"squared_error\", \"absolute_error\"],\n\t        \"max_depth\": range(1, 10),\n\t        \"min_samples_split\": range(2, 10),\n\t        \"min_samples_leaf\": range(1, 10),\n\t        \"max_features\": [\"sqrt\", \"log2\"],\n\t    },\n\t    \"<class 'sklearn.neighbors._classification.KNeighborsClassifier'>\": {\n\t        'n_neighbors': range(1, 20),\n\t        'weights': ['uniform', 'distance'],\n", "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n\t    },\n\t    \"<class 'sklearn.neighbors._regression.KNeighborsRegressor'>\": {\n\t        'n_neighbors': range(1, 20),\n\t        'weights': ['uniform', 'distance'],\n\t        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n\t    },\n\t    \"<class 'sklearn.tree._classes.DecisionTreeClassifier'>\": {\n\t        'criterion': ['gini', 'entropy'],\n\t        'splitter': ['best', 'random'],\n", "        'max_depth': range(1, 20),\n\t        'min_samples_split': range(2, 20),\n\t        'min_samples_leaf': range(1, 20),\n\t    },\n\t    \"<class 'sklearn.tree._classes.DecisionTreeRegressor'>\": {\n\t        'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n\t        'splitter': ['best', 'random'],\n\t        'max_depth': [i for i in range(1, 20)],\n\t        'min_samples_split': [i for i in range(2, 20)],\n\t        'min_samples_leaf': [i for i in range(1, 20)]\n", "    },\n\t    \"<class 'sklearn.linear_model._ridge.Ridge'>\": {\n\t        'alpha': (0, 20),\n\t        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n\t    },\n\t    \"<class 'sklearn.linear_model._logistic.LogisticRegression'>\": {\n\t        'C': (0.1, 20),\n\t        'solver': ['liblinear', 'sag', 'saga'],\n\t    },\n\t    \"<class 'sklearn.linear_model._coordinate_descent.Lasso'>\": {\n", "        'alpha': (0.01, 20),\n\t        'selection': ['cyclic', 'random'],\n\t    },\n\t    \"<class 'sklearn.naive_bayes.GaussianNB'>\": {\n\t        'var_smoothing': (1e-9, 1e-1),\n\t    },\n\t    \"<class 'sklearn.naive_bayes.MultinomialNB'>\": {\n\t        'alpha': (0.1, 20),\n\t        'fit_prior': [True, False],\n\t    },\n", "    \"<class 'sklearn.naive_bayes.BernoulliNB'>\": {\n\t        'alpha': (0.1, 20),\n\t        'fit_prior': [True, False],\n\t    },\n\t    \"<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>\": {\n\t        'loss': ['absolute_error', 'squared_error', 'huber', 'quantile'],\n\t        'learning_rate': (0.01, 1),\n\t        'n_estimators': range(10, 100, 10),\n\t        'criterion': ['friedman_mse', 'squared_error'],\n\t        'max_depth': range(1, 10),\n", "        'min_samples_split': range(2, 10),\n\t    },\n\t    \"<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>\": {\n\t        'learning_rate': (0.01, 1),\n\t        'n_estimators': range(10, 100, 10),\n\t        'criterion': ['friedman_mse', 'squared_error'],\n\t        'max_depth': range(1, 10),\n\t        'min_samples_split': range(2, 10),\n\t    },\n\t    \"<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>\": {\n", "        'n_estimators': range(10, 100, 10),\n\t        'learning_rate': (0.01, 1),\n\t        'algorithm': ['SAMME', 'SAMME.R'],\n\t    },\n\t    \"<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>\": {\n\t        'n_estimators': range(10, 100, 10),\n\t        'learning_rate': (0.01, 1),\n\t        'loss': ['linear', 'square', 'exponential'],\n\t    },\n\t    \"<class 'sklearn.ensemble._bagging.BaggingClassifier'>\": {\n", "        'n_estimators': range(10, 100, 10),\n\t        'max_samples': (0.01, 1),\n\t        'max_features': (0.01, 1),\n\t        'bootstrap': [True, False],\n\t        'bootstrap_features': [True, False],\n\t    },\n\t    \"<class 'sklearn.ensemble._bagging.BaggingRegressor'>\": {\n\t        'n_estimators': range(10, 100, 10),\n\t        'max_samples': (0.01, 1),\n\t        'max_features': (0.01, 1),\n", "        'bootstrap': [True, False],\n\t        'bootstrap_features': [True, False],\n\t    },\n\t}\n"]}
{"filename": "hyponic/config/__init__.py", "chunked_list": ["from .sklearn_models import models_dict\n"]}
{"filename": "hyponic/metrics/decorators.py", "chunked_list": ["\"\"\"\n\tThis module contains decorators for metrics.\n\tThe following decorators are available:\n\t- add_metric_to_dict -- metric is added to the dictionary of metrics (called automatically)\n\t- add_metric_aliases -- metric can be called by any of the string aliases\n\t- minimize_metric    -- metric is subject to minimization\n\t- maximize_metric    -- metric is subject to maximization\n\tThis module also contains a dictionary of metrics to which metrics are added\n\twhen they are decorated. This dictionary is used by the HypONIC class to\n\tretrieve the metrics passed to it as the string literals. This is helpful,\n", "because one metric can have multiple aliases, and the user can pass any of\n\tthem to the HypONIC class as a string.\n\t\"\"\"\n\timport warnings\n\tfrom collections import defaultdict\n\tfrom typing import Callable\n\t\"\"\"\n\tA dictionary of aliased metrics to which metrics are added when they are decorated.\n\t\"\"\"\n\tMETRICS_DICT = defaultdict()\n", "def add_metric_to_dict(metric: Callable) -> Callable:\n\t    \"\"\"\n\t    A decorator that adds the metric to the dictionary of metrics.\n\t    Called automatically by the minimize_metric and maximize_metric decorators.\n\t    :param metric: the metric that should be added to the dictionary\n\t    \"\"\"\n\t    METRICS_DICT[metric.__name__] = metric\n\t    if not hasattr(metric, \"minmax\"):\n\t        warnings.warn(f\"Metric {metric.__name__} has no minmax attribute. Using 'min' as default.\")\n\t        metric.minmax = \"min\"\n", "    return metric\n\tdef add_metric_aliases(*aliases) -> Callable:\n\t    \"\"\"\n\t    A decorator that adds aliases to the metric.\n\t    :param aliases: a list of aliases for the metric\n\t    :return: decorated metric\n\t    \"\"\"\n\t    def decorator(metric: Callable) -> Callable:\n\t        for alias in aliases:\n\t            METRICS_DICT[alias] = metric\n", "        return metric\n\t    return decorator\n\tdef add_metric_info(info: str) -> Callable:\n\t    \"\"\"\n\t    A decorator that adds information about the metric.\n\t    :param info: information about the metric\n\t    :return: decorated metric\n\t    \"\"\"\n\t    def decorator(metric: Callable) -> Callable:\n\t        metric.info = PrintableProperty(info)\n", "        return metric\n\t    return decorator\n\tdef maximize_metric(metric) -> Callable:\n\t    \"\"\"\n\t    A decorator for metrics that should be maximized.\n\t    :param metric: the metric that should be maximized\n\t    \"\"\"\n\t    metric.minmax = \"max\"\n\t    return add_metric_to_dict(metric)\n\tdef minimize_metric(metric) -> Callable:\n", "    \"\"\"\n\t    A decorator for metrics that should be minimized.\n\t    :param metric: the metric that should be minimized\n\t    \"\"\"\n\t    metric.minmax = \"min\"\n\t    return add_metric_to_dict(metric)\n\tclass PrintableProperty:\n\t    \"\"\"\n\t    A class for the properties that should either be printed or returned as a string.\n\t    Use case:\n", "    mse.info()  # will print the information about the metric\n\t    mse.info    # will return the information about the metric as a string\n\t    \"\"\"\n\t    def __init__(self, text):\n\t        self.__text = text\n\t    def __str__(self):\n\t        return self.__text\n\t    def __repr__(self):\n\t        return self.__text\n\t    def __call__(self, *args, **kwargs):\n", "        print(self.__text)\n\t        return self.__text\n"]}
{"filename": "hyponic/metrics/classification.py", "chunked_list": ["\"\"\"\n\tThis module contains metrics for evaluating classification models.\n\t\"\"\"\n\tfrom hyponic.metrics.decorators import add_metric_info, maximize_metric, minimize_metric\n\timport numpy as np\n\timport numba as nb\n\t@maximize_metric\n\t@add_metric_info(\"Accuracy is the fraction of predictions model got right.\")\n\t@nb.njit\n\tdef accuracy(y_true: np.array, y_pred: np.array) -> np.ndarray:\n", "    return np.mean(np.equal(y_true, y_pred))\n\t@maximize_metric\n\t@add_metric_info(\"Precision is the fraction of positive predictions that are correct.\")\n\t@nb.njit\n\tdef precision(y_true: np.array, y_pred: np.array) -> np.ndarray | float:\n\t    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n\t    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n\t    if tp + fp == 0:\n\t        return 0.0\n\t    return tp / (tp + fp)\n", "@maximize_metric\n\t@add_metric_info(\"Recall is the fraction of positive predictions that are correct\")\n\t@nb.njit\n\tdef recall(y_true: np.array, y_pred: np.array) -> np.ndarray | float:\n\t    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n\t    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n\t    if tp + fn == 0:\n\t        return 0.0\n\t    return tp / (tp + fn)\n\t@maximize_metric\n", "@add_metric_info(\"F1 score is the harmonic mean of precision and recall.\")\n\t@nb.njit\n\tdef f1_score(y_true: np.array, y_pred: np.array) -> np.ndarray | float:\n\t    p = precision(y_true, y_pred)\n\t    r = recall(y_true, y_pred)\n\t    if p + r == 0:\n\t        return 0.0\n\t    return 2 * p * r / (p + r)\n\t@maximize_metric\n\t@add_metric_info(\"F-beta score is the weighted harmonic mean of precision and recall.\")\n", "@nb.njit\n\tdef fbeta(y_true: np.array, y_pred: np.array, beta: float = 1.0) -> np.ndarray | float:\n\t    p = precision(y_true, y_pred)\n\t    r = recall(y_true, y_pred)\n\t    if p + r == 0:\n\t        return 0.0\n\t    return (1 + beta ** 2) * p * r / (beta ** 2 * p + r)\n\t@maximize_metric\n\t@add_metric_info(\n\t    \"Matthews correlation coefficient is a correlation coefficient between the observed and predicted binary classifications.\")\n", "@nb.njit\n\tdef confusion_matrix(y_true: np.array, y_pred: np.array) -> np.ndarray:\n\t    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n\t    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n\t    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n\t    tn = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n\t    return np.array([[tp, fp], [fn, tn]])\n\t@minimize_metric\n\t@add_metric_info(\n\t    \"Log loss is the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.\")\n", "@nb.njit\n\tdef log_loss(y_true: np.array, y_pred: np.array) -> np.ndarray:\n\t    return np.mean(np.log(1 + np.exp(-y_true * y_pred)))\n\t@minimize_metric\n\t@add_metric_info(\n\t    \"Binary crossentropy is the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.\")\n\t# @nb.njit\n\tdef binary_crossentropy(y_true: np.array, y_pred: np.array) -> np.ndarray:\n\t    # TODO: fix that logarithm can be negative or zero\n\t    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n", "@minimize_metric\n\t@add_metric_info(\n\t    \"Categorical crossentropy is the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.\")\n\t@nb.njit\n\tdef categorical_crossentropy(y_true: np.array, y_pred: np.array) -> np.ndarray:\n\t    return -np.mean(np.sum(y_true * np.log(y_pred), axis=-1))\n"]}
{"filename": "hyponic/metrics/regression.py", "chunked_list": ["\"\"\"\n\tThis module contains metrics for evaluating regression models.\n\t\"\"\"\n\tfrom hyponic.metrics.decorators import add_metric_aliases, add_metric_info, maximize_metric, minimize_metric\n\timport numpy as np\n\timport numexpr as ne\n\t@add_metric_aliases(\"mean_absolute_error\")\n\t@add_metric_info(\"Mean Absolute Error (MAE) is the average of the absolute errors.\")\n\t@minimize_metric\n\tdef mae(y_true: np.array, y_pred: np.array) -> np.ndarray:\n", "    ae = ne.evaluate(\"sum(abs(y_true - y_pred))\")  # for technical reasons we have to separate summation and division\n\t    return ne.evaluate(\"ae / length\", local_dict={\"ae\": ae, \"length\": len(y_true)})\n\t@add_metric_aliases(\"mean_squared_error\")\n\t@add_metric_info(\"Mean Squared Error (MSE) is the average of the squares of the errors.\")\n\t@minimize_metric\n\tdef mse(y_true: np.array, y_pred: np.array) -> np.ndarray:\n\t    se = ne.evaluate(\"sum((y_true - y_pred) ** 2)\")\n\t    return ne.evaluate(\"se / length\", local_dict={\"se\": se, \"length\": len(y_true)})\n\t@add_metric_aliases(\"root_mean_squared_error\")\n\t@add_metric_info(\"Root Mean Squared Error (RMSE) is the square root of the average of the squared errors.\")\n", "@minimize_metric\n\tdef rmse(y_true: np.array, y_pred: np.array) -> np.ndarray:\n\t    return np.sqrt(mse(y_true, y_pred))\n\t@add_metric_aliases(\"root_mean_squared_log_error\")\n\t@add_metric_info(\"Root Mean Squared Log Error (RMSLE) is the log of the square root of\"\n\t                 \"the average of the squared errors.\")\n\t@minimize_metric\n\tdef rmsle(y_true: np.array, y_pred: np.array) -> np.ndarray:\n\t    return np.log(rmse(y_true, y_pred))\n\t@maximize_metric\n", "@add_metric_info(\"R2 score is the proportion of the variance in the dependent variable that is predictable\")\n\tdef r2(y_true: np.array, y_pred: np.array) -> np.ndarray:\n\t    ss_res = ne.evaluate(\"sum((y_true - y_pred) ** 2)\")\n\t    sum_y_true = ne.evaluate(\"sum(y_true)\")\n\t    mean = ne.evaluate(\"sum_y_true / length\", local_dict={\"sum_y_true\": sum_y_true, \"length\": len(y_true)})\n\t    ss_tot = ne.evaluate(\"sum((y_true - mean) ** 2)\")\n\t    return ne.evaluate(\"1 - ss_res / ss_tot\")\n\t@maximize_metric\n\t@add_metric_info(\"Adjusted R2 score is the proportion of the variance in the dependent variable that is predictable\")\n\tdef adjusted_r2(y_true: np.array, y_pred: np.array) -> np.ndarray:\n", "    if len(y_true.shape) == 1:\n\t        return r2(y_true, y_pred)\n\t    return 1 - (1 - r2(y_true, y_pred)) * (len(y_true) - 1) / (len(y_true) - len(y_true[0]) - 1)\n\t@minimize_metric\n\t@add_metric_info(\"Huber loss is a loss function used in robust regression, that is less sensitive to outliers\")\n\tdef huber_loss(y_true: np.array, y_pred: np.array, delta: float = 1.0) -> np.ndarray:\n\t    error = y_true - y_pred\n\t    return ne.evaluate(\"sum(where(abs(error) <= delta, 0.5 * error ** 2, delta * (abs(error) - 0.5 * delta)))\")\n"]}
{"filename": "hyponic/metrics/__init__.py", "chunked_list": []}
{"filename": "hyponic/optimizers/base_optimizer.py", "chunked_list": ["from abc import ABC, abstractmethod\n\timport numpy as np\n\tfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\tfrom hyponic.utils.history import History\n\timport time\n\tclass BaseOptimizer(ABC):\n\t    \"\"\"\n\t    Base class for all optimizers. All optimizers should inherit from this class\n\t    \"\"\"\n\t    def __init__(self, **kwargs):\n", "        self.epoch = None\n\t        self.population_size = None\n\t        self.function = None\n\t        self.lb = None\n\t        self.ub = None\n\t        self.minmax = None\n\t        self.intervals = None\n\t        self.dimensions = None\n\t        self.verbose = None\n\t        self.mode = None\n", "        self.n_workers = None\n\t        self.coords = None\n\t        self.early_stopping = None\n\t    def _before_initialization(self):\n\t        \"\"\"\n\t        This method checks if the problem definition is correct\n\t        \"\"\"\n\t        if not isinstance(self.epoch, int) or self.epoch < 1:\n\t            raise ValueError(\"epoch should be a positive integer\")\n\t        if not isinstance(self.population_size, int) or self.population_size < 1:\n", "            raise ValueError(\"population_size should be a positive integer\")\n\t        if self.mode not in ['single', 'multithread']:\n\t            raise ValueError(\"mode should be either 'single' or 'multithread'\")\n\t        if self.n_workers < 1:  # TODO: n_workers can be -1, which means use all available cores\n\t            raise ValueError(\"n_workers should be a positive integer\")\n\t        if self.early_stopping is not None:\n\t            if not isinstance(self.early_stopping, int) or self.early_stopping < 1:\n\t                raise ValueError(\"early_stopping should be a positive integer or None\")\n\t    def _check_initialization(self):\n\t        \"\"\"\n", "        This method checks if the problem definition in initialization function is correct\n\t        \"\"\"\n\t        if self.lb is None or self.ub is None:\n\t            raise ValueError(\"lb and ub should be provided\")\n\t        if not isinstance(self.lb, np.ndarray) or not isinstance(self.ub, np.ndarray):\n\t            raise ValueError(\"lb and ub should be numpy arrays\")\n\t        if self.lb.shape != self.ub.shape:\n\t            raise ValueError(\"lb and ub should have the same shape\")\n\t        if np.any(self.lb > self.ub):\n\t            raise ValueError(\"lb should be less than ub\")\n", "        if self.minmax not in ['min', 'max']:\n\t            raise ValueError(\"minmax should be either 'min' or 'max'\")\n\t        if self.function is None:\n\t            raise ValueError(\"function should be provided\")\n\t        if not callable(self.function):\n\t            raise ValueError(\"function should be callable\")\n\t    def initialize(self, problem_dict):\n\t        \"\"\"\n\t        Initialize the optimizer with the problem dictionary\n\t        :param problem_dict: dictionary containing the problem definition\n", "        \"\"\"\n\t        # Unpack the problem dictionary\n\t        self.minmax = problem_dict['minmax'] if self.minmax is None else self.minmax\n\t        self.function = problem_dict[\"fit_func\"]\n\t        self.lb = np.array(problem_dict[\"lb\"])\n\t        self.ub = np.array(problem_dict[\"ub\"])\n\t        self._check_initialization()\n\t        self.intervals = self.ub - self.lb\n\t        self.dimensions = len(self.lb)\n\t        # TODO: add flag that tracks history or not\n", "        self.history = History(optimizer=self, epoch=self.epoch, population_size=self.population_size)\n\t        # Create the population\n\t        self.coords = self._create_population()\n\t    def _create_individual(self):\n\t        \"\"\"\n\t        Create an individual\n\t        :return: individual\n\t        \"\"\"\n\t        return np.random.uniform(self.lb, self.ub, self.dimensions)\n\t    def _create_population(self):\n", "        \"\"\"\n\t        Create a population of size self.population_size\n\t        This method can be parallelized using multithreading or multiprocessing(but multiprocessing doesn't work now)\n\t        :return: population\n\t        \"\"\"\n\t        coords = np.zeros((self.population_size, self.dimensions))\n\t        if self.mode == 'multithread':\n\t            with ThreadPoolExecutor(self.n_workers) as executor:\n\t                list_executor = [executor.submit(self._create_individual) for _ in range(self.population_size)]\n\t                for f in as_completed(list_executor):\n", "                    coords[list_executor.index(f)] = f.result()\n\t        else:\n\t            coords = np.array([self._create_individual() for _ in range(self.population_size)])\n\t        return coords\n\t    @abstractmethod\n\t    def evolve(self, current_epoch):\n\t        \"\"\"\n\t        Evolve the population for one epoch\n\t        :param current_epoch: current epoch number\n\t        \"\"\"\n", "        pass\n\t    @abstractmethod\n\t    def get_best_score(self):\n\t        \"\"\"\n\t        Get the best score of the current population\n\t        :return: best score of the fitness function\n\t        \"\"\"\n\t        pass\n\t    @abstractmethod\n\t    def get_best_solution(self):\n", "        \"\"\"\n\t        Get the best solution of the current population\n\t        :return: coordinates of the best solution\n\t        \"\"\"\n\t        pass\n\t    @abstractmethod\n\t    def get_current_best_score(self):\n\t        \"\"\"\n\t        Get the best score of the current population\n\t        :return: current best score of the fitness function\n", "        \"\"\"\n\t        pass\n\t    @abstractmethod\n\t    def get_current_best_solution(self):\n\t        \"\"\"\n\t        Get the best solution of the current population\n\t        :return: current coordinates of the best solution\n\t        \"\"\"\n\t        pass\n\t    def _minmax(self):\n", "        \"\"\"\n\t        :return: the min or max function, depending on the minmax parameter\n\t        \"\"\"\n\t        if self.minmax == 'min':\n\t            return np.min\n\t        return np.max\n\t    def _argminmax(self):\n\t        \"\"\"\n\t        :return: the argmin or argmax function, depending on the minmax parameter\n\t        \"\"\"\n", "        if self.minmax == 'min':\n\t            return np.argmin\n\t        return np.argmax\n\t    def solve(self, problem_dict, verbose=False):\n\t        \"\"\"\n\t        Solve the problem\n\t        :param problem_dict: dictionary containing the problem definition\n\t        :param verbose: if True, prints the best score at each epoch\n\t        :return: None\n\t        \"\"\"\n", "        if verbose:\n\t            self.verbose = True\n\t        self._before_initialization()\n\t        self.initialize(problem_dict)\n\t        for current_epoch in range(self.epoch):\n\t            start = time.time()\n\t            self.evolve(current_epoch)\n\t            end = time.time()\n\t            if self.verbose:\n\t                print(f'Epoch: {current_epoch}, Best Score: {self.get_best_score()}')\n", "            self.history.update_history(current_epoch, end - start)\n\t            if self.history.is_early_stopping(current_epoch, self.early_stopping):\n\t                if self.verbose:\n\t                    print(f'Early stopping at epoch {current_epoch}')\n\t                break\n\t        return self.get_best_solution(), self.get_best_score()\n\t    def get_history(self):\n\t        \"\"\"\n\t        Get the history of the optimizer\n\t        \"\"\"\n", "        return self.history.get_history()\n\t    def visualize_history_fitness(self):\n\t        \"\"\"\n\t        Visualize the fitness history\n\t        \"\"\"\n\t        self.history.visualize_fitness()\n\t    def visualize_history_time(self):\n\t        \"\"\"\n\t        Visualize the time history\n\t        \"\"\"\n", "        self.history.visualize_time()\n"]}
{"filename": "hyponic/optimizers/__init__.py", "chunked_list": []}
{"filename": "hyponic/optimizers/swarm_based/CS.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\timport numpy as np\n\timport numexpr as ne\n\tclass CS(BaseOptimizer):\n\t    \"\"\"\n\t    Cuckoo Search (CS) algorithm\n\t    Hyperparameters:\n\t        + pa(float), default=0.25: probability of cuckoo's egg to be discovered\n\t        + alpha(float), default=0.5: step size\n\t        + k(float), default=1: Levy multiplication coefficient\n", "    Example\n\t    ~~~~~~~\n\t    >>> from hyponic.optimizers.swarm_based.CS import CS\n\t    >>> import numpy as np\n\t    >>>\n\t    >>> def sphere(x):\n\t    >>>     return np.sum(x ** 2)\n\t    >>>\n\t    >>> problem_dict = {\n\t    >>>     'fit_func': sphere,\n", "    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n\t    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n\t    >>>     'minmax': 'min'\n\t    >>> }\n\t    >>>\n\t    >>> cs = CS(epoch=40, population_size=100, verbose=True, early_stopping=4)\n\t    >>> cs.solve(problem_dict)\n\t    >>> print(cs.get_best_score())\n\t    >>> print(cs.get_best_solution())\n\t    \"\"\"\n", "    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, verbose: bool = False,\n\t                 pa: float = 0.25, alpha: float = 0.5, k: float = 1,\n\t                 mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None, **kwargs):\n\t        \"\"\"\n\t        :param epoch: number of iterations\n\t        :param population_size: number of individuals in the population\n\t        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n\t        :param verbose: whether to print the progress, default is False\n\t        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n\t        :param n_workers: number of workers to use in multithreading mode\n", "        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n\t        stopping is not used\n\t        :param pa: probability of cuckoo's egg to be discovered\n\t        :param alpha: step size\n\t        :param k: Levy multiplication coefficient\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        self.epoch = epoch\n\t        self.population_size = population_size\n\t        self.minmax = minmax\n", "        self.verbose = verbose\n\t        self.mode = mode\n\t        self.n_workers = n_workers\n\t        self.early_stopping = early_stopping\n\t        self.pa = pa\n\t        self.alpha = alpha\n\t        self.k = k\n\t        self.nests = None\n\t        self.nests_fitness = None\n\t        self.cuckoo_coords = None\n", "    def _before_initialization(self):\n\t        super()._before_initialization()\n\t        if isinstance(self.pa, float) is False and isinstance(self.pa, int) is False:\n\t            raise TypeError('pa should be a float or an integer')\n\t        if isinstance(self.alpha, float) is False and isinstance(self.alpha, int) is False:\n\t            raise TypeError('alpha should be a float or an integer')\n\t        if isinstance(self.k, float) is False and isinstance(self.k, int) is False:\n\t            raise TypeError('k should be a float or an integer')\n\t    def initialize(self, problem_dict):\n\t        super().initialize(problem_dict)\n", "        self.nests = self.coords\n\t        self.nests_fitness = np.array([self.function(self.nests[i]) for i in range(self.population_size)])\n\t        self.cuckoo_coords = np.random.uniform(self.lb, self.ub, self.dimensions)\n\t    def _levy_flight(self, x):\n\t        u = np.random.normal(0, 1, size=self.dimensions)\n\t        v = np.random.normal(0, 1, size=self.dimensions)\n\t        best_coords = self.nests[self._argminmax()(self.nests_fitness)]\n\t        return ne.evaluate('x + k * u / (abs(v) ** (1 / 1.5)) * (best_coords - x)', local_dict={\n\t            'x': x, 'k': self.k, 'u': u, 'v': v, 'best_coords': best_coords\n\t        })\n", "    def evolve(self, current_epoch):\n\t        x_new = self._levy_flight(self.cuckoo_coords)\n\t        self.cuckoo_coords = np.clip(x_new, self.lb, self.ub)\n\t        next_nest = np.random.randint(0, self.population_size)\n\t        new_fitness = self.function(self.cuckoo_coords)\n\t        if new_fitness < self.nests_fitness[next_nest]:\n\t            self.nests[next_nest] = self.cuckoo_coords\n\t            self.nests_fitness[next_nest] = new_fitness\n\t        number_of_discovered_eggs = int(self.pa * self.population_size)\n\t        worst_nests = np.argsort(self.nests_fitness)[-number_of_discovered_eggs:]\n", "        self.nests[worst_nests] = np.random.uniform(self.lb, self.ub, (number_of_discovered_eggs, self.dimensions))\n\t        self.nests_fitness[worst_nests] = np.array([self.function(self.nests[i]) for i in worst_nests])\n\t    def get_best_score(self):\n\t        return self._minmax()(self.nests_fitness)\n\t    def get_best_solution(self):\n\t        return self.nests[self._argminmax()(self.nests_fitness)]\n\t    def get_current_best_score(self):\n\t        return self.get_best_score()\n\t    def get_current_best_solution(self):\n\t        return self.get_best_solution()\n"]}
{"filename": "hyponic/optimizers/swarm_based/__init__.py", "chunked_list": []}
{"filename": "hyponic/optimizers/swarm_based/PSO.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\timport numpy as np\n\timport numexpr as ne\n\tclass PSO(BaseOptimizer):\n\t    \"\"\"\n\t    Particle Swarm Optimization (PSO) algorithm\n\t    Hyperparameters:\n\t        + a1(float), default=0.5: acceleration parameter\n\t        + a2(float), default=0.5: acceleration parameter\n\t    Example\n", "    ~~~~~~~\n\t    >>> from hyponic.optimizers.swarm_based.PSO import PSO\n\t    >>> import numpy as np\n\t    >>>\n\t    >>> def sphere(x):\n\t    >>>     return np.sum(x ** 2)\n\t    >>>\n\t    >>> problem_dict = {\n\t    >>>     'fit_func': sphere,\n\t    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n", "    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n\t    >>>     'minmax': 'min'\n\t    >>> }\n\t    >>>\n\t    >>> a1, a2 = 0.8, 0.4\n\t    >>> pso = PSO(epoch=40, population_size=100, verbose=True, early_stopping=4, a1=a1, a2=a2)\n\t    >>> pso.solve(problem_dict)\n\t    >>> print(pso.get_best_score())\n\t    >>> print(pso.get_best_solution())\n\t    \"\"\"\n", "    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, a1: float = 0.5, a2: float = 0.5,\n\t                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n\t                 **kwargs):\n\t        \"\"\"\n\t        :param epoch: number of iterations\n\t        :param population_size: number of individuals in the population\n\t        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n\t        :param verbose: whether to print the progress, default is False\n\t        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n\t        :param n_workers: number of workers to use in multithreading mode\n", "        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n\t        stopping is not used\n\t        :param a1, a2: acceleration parameter\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        self.epoch = epoch\n\t        self.population_size = population_size\n\t        self.minmax = minmax\n\t        self.verbose = verbose\n\t        self.mode = mode\n", "        self.n_workers = n_workers\n\t        self.early_stopping = early_stopping\n\t        self.a1 = a1\n\t        self.a2 = a2\n\t        self.velocities = None\n\t        self.p_best = None\n\t        self.p_best_coords = None\n\t        self.g_best = None\n\t        self.g_best_coords = None\n\t    def _before_initialization(self):\n", "        super()._before_initialization()\n\t        if isinstance(self.a1, float) is False and isinstance(self.a1, int) is False:\n\t            raise ValueError(\"a1 should be a float or an integer\")\n\t        if isinstance(self.a2, float) is False and isinstance(self.a2, int) is False:\n\t            raise ValueError(\"a2 should be a float or an integer\")\n\t    def initialize(self, problem_dict):\n\t        # TODO: if lb and ub are not provided, use the default values\n\t        super().initialize(problem_dict)\n\t        self.g_best = np.inf if self.minmax == \"min\" else -np.inf\n\t        max_velocity = ne.evaluate(\"ub - lb\", local_dict={'ub': self.ub, 'lb': self.lb})\n", "        self.velocities = np.random.uniform(-max_velocity, max_velocity, size=(self.population_size, self.dimensions))\n\t        self.p_best_coords = self.coords\n\t        self.p_best = np.array([self.function(self.coords[i]) for i in range(self.population_size)])\n\t        self._update_global_best()\n\t    def evolve(self, epoch):\n\t        self.velocities = self._update_velocity()\n\t        self.coords = ne.evaluate(\"coords + velocities\",\n\t                                  local_dict={'coords': self.coords, 'velocities': self.velocities})\n\t        # TODO: if lb or ub is provided, clip the coordinates\n\t        self.coords = np.clip(self.coords, self.lb, self.ub)\n", "        fitness = np.array([self.function(self.coords[i]) for i in range(self.population_size)])\n\t        condition = all(self._minmax()(np.concatenate([self.p_best, fitness])) != self.p_best)\n\t        self.p_best_coords = np.where(condition, self.coords, self.p_best_coords)\n\t        self.p_best = ne.evaluate(\"where(condition, fitness, p_best)\", local_dict={'condition': condition,\n\t                                                                                   'fitness': fitness,\n\t                                                                                   'p_best': self.p_best})\n\t        self._update_global_best()\n\t    def _update_velocity(self):\n\t        r1 = np.random.random()\n\t        r2 = np.random.random()\n", "        expr = \"velocities + a1 * r1 * (p_best_coords - coords) + a2 * r2 * (g_best_coords - coords)\"\n\t        return ne.evaluate(expr,\n\t                           local_dict={'velocities': self.velocities, 'a1': self.a1, 'a2': self.a2, 'r1': r1, 'r2': r2,\n\t                                       'p_best_coords': self.p_best_coords, 'coords': self.coords,\n\t                                       'g_best_coords': self.g_best_coords})\n\t    def _update_global_best(self):\n\t        if self._minmax()(np.concatenate([self.p_best, [self.g_best]])) != self.g_best:\n\t            self.g_best = self._minmax()(self.p_best)\n\t            self.g_best_coords = self.p_best_coords[self._argminmax()(self.p_best)]\n\t    def get_best_score(self):\n", "        return self.g_best\n\t    def get_best_solution(self):\n\t        return self.g_best_coords\n\t    def get_current_best_score(self):\n\t        return self.p_best\n\t    def get_current_best_solution(self):\n\t        return self.p_best_coords\n\tclass IWPSO(PSO):\n\t    \"\"\"\n\t    Inertia Weight Particle Swarm Optimization\n", "    Hyperparameters:\n\t        + a1(float), default=0.5: acceleration parameter\n\t        + a2(float), default=0.5: acceleration parameter\n\t        + w(float), default=0.5: inertia weight\n\t    Example\n\t    ~~~~~~~\n\t    >>> from hyponic.optimizers.swarm_based.PSO import IWPSO\n\t    >>> import numpy as np\n\t    >>>\n\t    >>> def sphere(x):\n", "    >>>     return np.sum(x ** 2)\n\t    >>>\n\t    >>> problem_dict = {\n\t    >>>     'fit_func': sphere,\n\t    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n\t    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n\t    >>>     'minmax': 'min'\n\t    >>> }\n\t    >>>\n\t    >>> a1, a2 = 0.8, 0.4\n", "    >>> w = 0.3\n\t    >>> iwpso = IWPSO(epoch=40, population_size=100, verbose=True, early_stopping=4, a1=a1, a2=a2, w=w)\n\t    >>> iwpso.solve(problem_dict)\n\t    >>> print(iwpso.get_best_score())\n\t    >>> print(iwpso.get_best_solution())\n\t    \"\"\"\n\t    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, a1: float = 0.5, a2: float = 0.5,\n\t                 w: float = 0.8,\n\t                 verbose: bool = False,\n\t                 mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None, **kwargs):\n", "        \"\"\"\n\t        :param epoch: number of iterations\n\t        :param population_size: number of individuals in the population\n\t        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n\t        :param verbose: whether to print the progress, default is False\n\t        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n\t        :param n_workers: number of workers to use in multithreading mode\n\t        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n\t        stopping is not used\n\t        :param a1, a2: acceleration parameter\n", "        :param w: inertia\n\t        \"\"\"\n\t        super().__init__(epoch, population_size, minmax, a1, a2, verbose, mode, n_workers, early_stopping, **kwargs)\n\t        self.w = w\n\t    def _before_initialization(self):\n\t        super()._before_initialization()\n\t        if isinstance(self.w, float) is False and isinstance(self.w, int) is False:\n\t            raise ValueError(\"w should be a float or an integer\")\n\t    def _update_velocity(self):\n\t        r1 = np.random.random()\n", "        r2 = np.random.random()\n\t        expr = \"w * velocities + a1 * r1 * (p_best_coords - coords) + a2 * r2 * (g_best_coords - coords)\"\n\t        return ne.evaluate(expr, local_dict={'w': self.w, 'velocities': self.velocities, 'a1': self.a1, 'a2': self.a2,\n\t                                             'r1': r1, 'r2': r2, 'p_best_coords': self.p_best_coords,\n\t                                             'coords': self.coords, 'g_best_coords': self.g_best_coords})\n"]}
{"filename": "hyponic/optimizers/swarm_based/GWO.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\timport numpy as np\n\timport numexpr as ne\n\tclass GWO(BaseOptimizer):\n\t    \"\"\"\n\t    Grey Wolf Optimization (GWO) algorithm\n\t    Example\n\t    ~~~~~~~\n\t    >>> from hyponic.optimizers.swarm_based.GWO import GWO\n\t    >>> import numpy as np\n", "    >>>\n\t    >>> def sphere(x):\n\t    >>>     return np.sum(x ** 2)\n\t    >>>\n\t    >>> problem_dict = {\n\t    >>>     'fit_func': sphere,\n\t    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n\t    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n\t    >>>     'minmax': 'min'\n\t    >>> }\n", "    >>>\n\t    >>> gwo = GWO(epoch=40, population_size=100, verbose=True, early_stopping=4)\n\t    >>> gwo.solve(problem_dict)\n\t    >>> print(gwo.get_best_score())\n\t    >>> print(gwo.get_best_solution())\n\t    \"\"\"\n\t    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, verbose: bool = False,\n\t                 mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None, **kwargs):\n\t        \"\"\"\n\t        :param epoch: number of iterations\n", "        :param population_size: number of individuals in the population\n\t        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n\t        :param verbose: whether to print the progress, default is False\n\t        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n\t        :param n_workers: number of workers to use in multithreading mode\n\t        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n\t        stopping is not used\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        self.epoch = epoch\n", "        self.population_size = population_size\n\t        self.minmax = minmax\n\t        self.verbose = verbose\n\t        self.mode = mode\n\t        self.n_workers = n_workers\n\t        self.early_stopping = early_stopping\n\t        self.fitness = None\n\t        self.g_best = None\n\t        self.g_best_coords = None\n\t    def initialize(self, problem_dict):\n", "        super().initialize(problem_dict)\n\t        self.g_best = np.inf if self.minmax == 'min' else -np.inf\n\t        self.g_best_coords = np.zeros(self.dimensions)\n\t        self.fitness = np.array([self.function(x) for x in self.coords], dtype=np.float64)\n\t    def evolve(self, current_epoch):\n\t        a = 2 - current_epoch * (2 / self.epoch)\n\t        for i in range(self.population_size):\n\t            r1 = np.random.rand()\n\t            r2 = np.random.rand()\n\t            A = 2 * a * r1 - a\n", "            C = 2 * r2\n\t            D = np.abs(C * self.coords[np.random.randint(0, self.population_size)] - self.coords[np.random.randint(0, self.population_size)])\n\t            coords_new = self.coords[i] + A * D\n\t            fitness_new = self.function(coords_new)\n\t            if self._minmax()([fitness_new, self.fitness[i]]) == fitness_new:\n\t                self.coords[i] = coords_new\n\t                self.fitness[i] = fitness_new\n\t            if self._minmax()([fitness_new, self.g_best]) == fitness_new:\n\t                self.g_best = fitness_new\n\t                self.g_best_coords = coords_new\n", "    def get_best_score(self):\n\t        return self.g_best\n\t    def get_best_solution(self):\n\t        return self.g_best_coords\n\t    def get_current_best_score(self):\n\t        return self._minmax()(self.fitness)\n\t    def get_current_best_solution(self):\n\t        return self.coords[self._argminmax()(self.fitness)]\n"]}
{"filename": "hyponic/optimizers/swarm_based/ABC.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\timport numpy as np\n\timport numexpr as ne\n\tclass ABC(BaseOptimizer):\n\t    \"\"\"\n\t    Artificial Bee Colony (ABC) algorithm\n\t    Hyperparameters:\n\t        + limits(int), default=25: the number of trials before abandoning food source\n\t    Example\n\t    ~~~~~~~\n", "    >>> from hyponic.optimizers.swarm_based.ABC import ABC\n\t    >>> import numpy as np\n\t    >>>\n\t    >>> def sphere(x):\n\t    >>>     return np.sum(x ** 2)\n\t    >>>\n\t    >>> problem_dict = {\n\t    >>>     'fit_func': sphere,\n\t    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n\t    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n", "    >>>     'minmax': 'min'\n\t    >>> }\n\t    >>>\n\t    >>> abc = ABC(epoch=40, population_size=100, verbose=True, early_stopping=4)\n\t    >>> abc.solve(problem_dict)\n\t    >>> print(abc.get_best_score())\n\t    >>> print(abc.get_best_solution())\n\t    \"\"\"\n\t    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, limits=25,\n\t                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n", "                 **kwargs):\n\t        \"\"\"\n\t        :param epoch: number of iterations\n\t        :param population_size: number of individuals in the population\n\t        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n\t        :param verbose: whether to print the progress, default is False\n\t        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n\t        :param n_workers: number of workers to use in multithreading mode\n\t        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n\t        stopping is not used\n", "        :param limits: the number of trials before abandoning food source\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        self.epoch = epoch\n\t        self.population_size = population_size\n\t        self.minmax = minmax\n\t        self.verbose = verbose\n\t        self.mode = mode\n\t        self.n_workers = n_workers\n\t        self.early_stopping = early_stopping\n", "        self.limits = limits\n\t        self.fitness = None\n\t        self.g_best = None\n\t        self.g_best_coords = None\n\t        self.trials = None\n\t    def _before_initialization(self):\n\t        super()._before_initialization()\n\t        if not isinstance(self.limits, int) or self.limits < 1:\n\t            raise ValueError(\"limits should be a positive integer\")\n\t    def initialize(self, problem_dict):\n", "        super().initialize(problem_dict)\n\t        self.g_best = np.inf if self._minmax() == min else -np.inf\n\t        self.fitness = np.array([self.function(self.coords[i]) for i in range(self.population_size)])\n\t        self.trials = np.zeros(self.population_size)\n\t    def _coordinate_update_phase(self, i, k):\n\t        phi = np.random.uniform(-1, 1, self.dimensions)\n\t        new_coords = ne.evaluate(\"coords + phi * (coords - new_coords)\", local_dict={'coords': self.coords[i],\n\t                                                                                     'phi': phi,\n\t                                                                                     'new_coords': self.coords[k]})\n\t        new_coords = np.clip(new_coords, self.lb, self.ub)\n", "        new_fitness = self.function(new_coords)\n\t        if self._minmax()(np.array([self.fitness[i], new_fitness])) != self.fitness[i]:\n\t            self.coords[i] = new_coords\n\t            self.fitness[i] = new_fitness\n\t            self.trials[i] = 0\n\t        else:\n\t            self.trials[i] += 1\n\t    def _employed_bees_phase(self):\n\t        for i in range(self.population_size):\n\t            k = np.random.choice([j for j in range(self.population_size) if j != i])\n", "            self._coordinate_update_phase(i, k)\n\t    def _onlooker_bees_phase(self):\n\t        if np.all(self.fitness == 0):\n\t            probabilities = np.ones(self.population_size) / self.population_size\n\t        else:\n\t            probabilities = self.fitness / np.sum(self.fitness)\n\t        for i in range(self.population_size):\n\t            k = np.random.choice([j for j in range(self.population_size)], p=probabilities)\n\t            self._coordinate_update_phase(i, k)\n\t    def _scout_bees_phase(self):\n", "        for i in range(self.population_size):\n\t            if self.trials[i] > self.limits:\n\t                self.coords[i] = np.random.uniform(self.lb, self.ub, self.dimensions)\n\t                self.fitness[i] = self.function(self.coords[i])\n\t                self.trials[i] = 0\n\t    def evolve(self, epoch):\n\t        self._employed_bees_phase()\n\t        self._onlooker_bees_phase()\n\t        self._scout_bees_phase()\n\t        best_index = self._argminmax()(self.fitness)\n", "        self.g_best = self.fitness[best_index]\n\t        self.g_best_coords = self.coords[best_index]\n\t    def get_best_score(self):\n\t        return self.g_best\n\t    def get_best_solution(self):\n\t        return self.g_best_coords\n\t    def get_current_best_score(self):\n\t        return self._minmax()(self.fitness)\n\t    def get_current_best_solution(self):\n\t        best_index = self._argminmax()(self.fitness)\n", "        return self.coords[best_index]\n"]}
{"filename": "hyponic/optimizers/swarm_based/ACO.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\timport numpy as np\n\tclass ACO(BaseOptimizer):\n\t    \"\"\"\n\t    Ant Colony Optimization (ACO)\n\t    Hyperparameters:\n\t        + alpha(float), default=1: the importance of pheromone\n\t        + beta(float), default=1: the importance of heuristic information\n\t        + rho(float), default=0.5: the pheromone evaporation rate\n\t        + q(float), default=1: the pheromone intensity\n", "    Example\n\t    ~~~~~~~\n\t    >>> from hyponic.optimizers.swarm_based.ACO import ACO\n\t    >>> import numpy as np\n\t    >>>\n\t    >>> def sphere(x):\n\t    >>>     return np.sum(x ** 2)\n\t    >>>\n\t    >>> problem_dict = {\n\t    >>>     'fit_func': sphere,\n", "    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n\t    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n\t    >>>     'minmax': 'min'\n\t    >>> }\n\t    >>>\n\t    >>> alpha = 1\n\t    >>> beta = 1\n\t    >>> rho = 0.5\n\t    >>> q = 1\n\t    >>>\n", "    >>> aco = ACO(epoch=40, population_size=100, verbose=True, early_stopping=4, alpha=alpha, beta=beta, rho=rho, q=q)\n\t    >>> aco.solve(problem_dict)\n\t    >>> print(aco.get_best_score())\n\t    >>> print(aco.get_best_solution())\n\t    \"\"\"\n\t    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None, alpha: float = 1,\n\t                 beta: float = 1, rho: float = 0.5, q: float = 1,\n\t                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n\t                 **kwargs):\n\t        \"\"\"\n", "        :param epoch: number of iterations\n\t        :param population_size: number of individuals in the population\n\t        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n\t        :param verbose: whether to print the progress, default is False\n\t        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n\t        :param n_workers: number of workers to use in multithreading mode\n\t        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n\t        stopping is not used\n\t        :param alpha: the importance of pheromone\n\t        :param beta: the importance of heuristic information\n", "        :param rho: the pheromone evaporation rate\n\t        :param q: the pheromone intensity\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        self.epoch = epoch\n\t        self.population_size = population_size\n\t        self.minmax = minmax\n\t        self.verbose = verbose\n\t        self.mode = mode\n\t        self.n_workers = n_workers\n", "        self.early_stopping = early_stopping\n\t        self.alpha = alpha\n\t        self.beta = beta\n\t        self.rho = rho\n\t        self.q = q\n\t        self.pheromone = None\n\t        self.population = None\n\t        self.scores = None\n\t        self.best_score = None\n\t        self.best_solution = None\n", "    def _before_initialization(self):\n\t        super()._before_initialization()\n\t        if self.alpha < 0:\n\t            raise ValueError(\"alpha should be a positive float\")\n\t        if self.beta < 0:\n\t            raise ValueError(\"beta should be a positive float\")\n\t        if self.rho < 0 or self.rho > 1:\n\t            raise ValueError(\"rho should be a float between 0 and 1\")\n\t        if self.q < 0:\n\t            raise ValueError(\"q should be a positive float\")\n", "    def initialize(self, problem_dict):\n\t        super().initialize(problem_dict)\n\t        self.pheromone = np.ones((self.population_size, self.dimensions))\n\t        self.best_score = np.inf if self.minmax == 'min' else -np.inf\n\t        self.population = np.random.uniform(low=self.lb, high=self.ub, size=(self.population_size, self.dimensions))\n\t        self.scores = np.zeros(self.population_size)\n\t        for i in range(self.population_size):\n\t            self.scores[i] = self.function(self.population[i])\n\t            if self._minmax()(self.scores[i]) < self._minmax()(self.best_score):\n\t                self.best_score = self.scores[i]\n", "                self.best_solution = self.population[i]\n\t    def evolve(self, epoch):\n\t        new_population = np.zeros((self.population_size, self.dimensions))\n\t        new_scores = np.zeros(self.population_size)\n\t        for i in range(self.population_size):\n\t            ant_position = np.random.uniform(low=self.lb, high=self.ub, size=self.dimensions)\n\t            for j in range(self.dimensions):\n\t                pheromone_weights = self.pheromone[:, j] ** self.alpha\n\t                heuristic_weights = 1 / (np.abs(ant_position[j] - self.population[:, j]) + 1e-6) ** self.beta\n\t                probs = pheromone_weights * heuristic_weights\n", "                probs /= np.sum(probs)\n\t                ant_position[j] = np.random.choice(self.population[:, j], p=probs)\n\t            ant_score = self.function(ant_position)\n\t            self._update_pheromone(ant_score)\n\t            if self._minmax()(ant_score) < self._minmax()(self.best_score):\n\t                self.best_solution = ant_position\n\t                self.best_score = ant_score\n\t            new_population[i] = ant_position\n\t            new_scores[i] = ant_score\n\t        self.population = new_population\n", "        self.scores = new_scores\n\t    def get_best_score(self):\n\t        return self.best_score\n\t    def get_best_solution(self):\n\t        return self.best_solution\n\t    def get_current_best_score(self):\n\t        return self._minmax()(self.scores.min())\n\t    def get_current_best_solution(self):\n\t        return self.population[self.scores.argmin()]\n\t    def _initialize_pheromone(self):\n", "        self.pheromone = np.ones((self.population_size, self.dimensions))\n\t    def _update_pheromone(self, ant_score):\n\t        delta_pheromone = np.zeros((self.population_size, self.dimensions))\n\t        delta_pheromone[:, :] = self.q / ant_score\n\t        self.pheromone = (1 - self.rho) * self.pheromone + self.rho * delta_pheromone\n"]}
{"filename": "hyponic/optimizers/physics_based/SA.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\timport numpy as np\n\timport numexpr as ne\n\tclass SA(BaseOptimizer):\n\t    \"\"\"\n\t    Simulated Annealing\n\t    Example\n\t    ~~~~~~~\n\t    >>> from hyponic.optimizers.physics_based.SA import SA\n\t    >>> import numpy as np\n", "    >>>\n\t    >>> def sphere(x):\n\t    >>>     return np.sum(x ** 2)\n\t    >>>\n\t    >>> problem_dict = {\n\t    >>>     'fit_func': sphere,\n\t    >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n\t    >>>     'ub': [5.12, 5, 14, 6, 0.9],\n\t    >>>     'minmax': 'min'\n\t    >>> }\n", "    >>>\n\t    >>> sa = SA(epoch=40, population_size=100, verbose=True, early_stopping=4)\n\t    >>> sa.solve(problem_dict)\n\t    >>> print(sa.get_best_score())\n\t    >>> print(sa.get_best_solution())\n\t    \"\"\"\n\t    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None,\n\t                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n\t                 **kwargs):\n\t        \"\"\"\n", "        :param epoch: number of iterations\n\t        :param population_size: number of individuals in the population\n\t        :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n\t        :param verbose: whether to print the progress, default is False\n\t        :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n\t        :param n_workers: number of workers to use in multithreading mode\n\t        :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n\t        stopping is not used\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n", "        self.epoch = epoch\n\t        self.population_size = population_size\n\t        self.minmax = minmax\n\t        self.verbose = verbose\n\t        self.mode = mode\n\t        self.n_workers = n_workers\n\t        self.early_stopping = early_stopping\n\t        self.currents = None\n\t        self.best = None\n\t        self.best_score = None\n", "    def initialize(self, problem_dict):\n\t        super().initialize(problem_dict)\n\t        self.currents = np.random.uniform(self.lb, self.ub, (self.population_size, self.dimensions))\n\t        self.best = self.currents[0]\n\t        self.best_score = self.function(self.best)\n\t    def evolve(self, current_epoch):\n\t        progress = current_epoch / self.epoch\n\t        t = max(0.01, min(1, 1 - progress))\n\t        amplitudes = ne.evaluate(\"(_max - _min) * progress * 0.1\",\n\t                                 local_dict={'_max': np.max(self.intervals), '_min': np.min(self.intervals),\n", "                                             'progress': progress})\n\t        deltas = np.random.uniform(-amplitudes / 2, amplitudes / 2, (self.population_size, self.dimensions))\n\t        candidates = ne.evaluate(\"currents + deltas\", local_dict={'currents': self.currents, 'deltas': deltas})\n\t        for idx, candidate in enumerate(candidates):\n\t            candidate = np.clip(candidate, self.lb, self.ub)\n\t            candidate_score = self.function(candidate)\n\t            if candidate_score < self.best_score:  # TODO: check if the problem is minimization or maximization\n\t                self.best = candidate\n\t                self.best_score = candidate_score\n\t                self.currents[idx] = candidate\n", "            else:\n\t                score_abs_diff = ne.evaluate(\"abs(candidate_score - best_score)\",\n\t                                             local_dict={'candidate_score': candidate_score,\n\t                                                         'best_score': self.best_score})\n\t                if np.random.uniform() < np.exp(-score_abs_diff / t):\n\t                    self.currents[idx] = candidate\n\t    def get_best_score(self):\n\t        return self.best_score\n\t    def get_best_solution(self):\n\t        return self.best\n", "    def get_current_best_score(self):\n\t        return self.function(self.currents[0])\n\t    def get_current_best_solution(self):\n\t        return self.currents[0]\n"]}
{"filename": "hyponic/optimizers/physics_based/__init__.py", "chunked_list": []}
{"filename": "hyponic/optimizers/genetic_based/GA.py", "chunked_list": ["from hyponic.optimizers.base_optimizer import BaseOptimizer\n\timport numpy as np\n\timport numexpr as ne\n\tclass GA(BaseOptimizer):\n\t    \"\"\"\n\t    Genetic Algorithm(GA)\n\t    Example\n\t    ~~~~~~~\n\t        >>> from hyponic.optimizers.genetic_based.GA import GA\n\t        >>> import numpy as np\n", "        >>>\n\t        >>> def sphere(x):\n\t        >>>     return np.sum(x ** 2)\n\t        >>>\n\t        >>> problem_dict = {\n\t        >>>     'fit_func': sphere,\n\t        >>>     'lb': [-5.12, -5, -14, -6, -0.9],\n\t        >>>     'ub': [5.12, 5, 14, 6, 0.9],\n\t        >>>     'minmax': 'min'\n\t        >>> }\n", "        >>>\n\t        >>> ga = GA(epoch=40, population_size=100, verbose=True, early_stopping=4)\n\t        >>> ga.solve(problem_dict)\n\t        >>> print(ga.get_best_score())\n\t        >>> print(ga.get_best_solution())\n\t    \"\"\"\n\t    def __init__(self, epoch: int = 10, population_size: int = 10, minmax: str = None,\n\t                 verbose: bool = False, mode: str = 'single', n_workers: int = 4, early_stopping: int | None = None,\n\t                 **kwargs):\n\t        \"\"\"\n", "                :param epoch: number of iterations\n\t                :param population_size: number of individuals in the population\n\t                :param minmax: 'min' or 'max', depending on whether the problem is a minimization or maximization problem\n\t                :param verbose: whether to print the progress, default is False\n\t                :param mode: 'single' or 'multithread', depending on whether to use multithreading or not\n\t                :param n_workers: number of workers to use in multithreading mode\n\t                :param early_stopping: number of epochs to wait before stopping the optimization process. If None, then early\n\t                stopping is not used\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n", "        self.epoch = epoch\n\t        self.population_size = population_size\n\t        self.minmax = minmax\n\t        self.verbose = verbose\n\t        self.mode = mode\n\t        self.n_workers = n_workers\n\t        self.early_stopping = early_stopping\n\t        self.population = None\n\t        self.scores = None\n\t        self.best_score = None\n", "        self.best_solution = None\n\t    def initialize(self, problem_dict):\n\t        super().initialize(problem_dict)\n\t        self.population = np.random.uniform(low=self.lb, high=self.ub, size=(self.population_size, self.dimensions))\n\t        self.scores = np.array([self.function(self.population[i]) for i in range(self.population_size)])\n\t        best_idx = self._argminmax()(self.scores)\n\t        self.best_score = self.scores[best_idx]\n\t        self.best_solution = self.population[best_idx]\n\t    def evolve(self, epoch):\n\t        next_population = np.zeros_like(self.population)\n", "        next_scores = np.zeros(self.population_size)\n\t        # Elitism: keep the best solution from the previous generation\n\t        best_idx = self._argminmax()(self.scores)\n\t        next_population[0] = self.population[best_idx]\n\t        next_scores[0] = self.scores[best_idx]\n\t        # Roulette Wheel Selection\n\t        fitness = self.scores - np.min(self.scores) if self.minmax == 'min' else np.max(self.scores) - self.scores\n\t        total_fitness = np.sum(fitness)\n\t        if total_fitness == 0:\n\t            probs = np.ones(self.population_size) / self.population_size\n", "        else:\n\t            probs = fitness / total_fitness\n\t        cum_probs = np.cumsum(probs)\n\t        # Crossover and mutation\n\t        for i in range(1, self.population_size):\n\t            # Select two parents using roulette wheel selection\n\t            parent1_idx = np.searchsorted(cum_probs, np.random.rand())\n\t            parent2_idx = np.searchsorted(cum_probs, np.random.rand())\n\t            # Single point crossover\n\t            crossover_point = np.random.randint(1, self.dimensions)\n", "            next_population[i, :crossover_point] = self.population[parent1_idx, :crossover_point]\n\t            next_population[i, crossover_point:] = self.population[parent2_idx, crossover_point:]\n\t            # Mutation\n\t            mutation_strength = ne.evaluate(\"0.5 * (ub - lb)\", local_dict={\"ub\": self.ub, \"lb\": self.lb})\n\t            next_population[i] += np.random.normal(0, mutation_strength, size=self.dimensions)\n\t            next_population[i] = np.clip(next_population[i], self.lb, self.ub)\n\t        # evaluate the new population\n\t        next_scores = np.array([self.function(next_population[i]) for i in range(self.population_size)])\n\t        # update the best solution and score\n\t        best_idx = self._argminmax()(next_scores)\n", "        if self._minmax()(next_scores) < self._minmax()(self.scores):\n\t            self.best_solution = next_population[best_idx]\n\t            self.best_score = next_scores[best_idx]\n\t        # replace the old population with the new one\n\t        self.population = next_population\n\t        self.scores = next_scores\n\t    def get_best_score(self):\n\t        return self.best_score\n\t    def get_best_solution(self):\n\t        return self.best_solution\n", "    def get_current_best_score(self):\n\t        return self._minmax()(self.scores)\n\t    def get_current_best_solution(self):\n\t        return self.population[self._argminmax()(self.scores)]\n"]}
{"filename": "hyponic/optimizers/genetic_based/__init__.py", "chunked_list": []}
{"filename": "examples/automatic_hyperparameters.py", "chunked_list": ["from sklearn.ensemble import GradientBoostingClassifier\n\tfrom sklearn.datasets import load_wine\n\tfrom hyponic.hyponic import HypONIC\n\tfrom hyponic.optimizers.swarm_based.CS import CS\n\tX, y = load_wine(return_X_y=True)\n\tmodel = GradientBoostingClassifier()\n\toptimizer_kwargs = {\n\t    \"epoch\": 20,\n\t    \"pop_size\": 50,\n\t}\n", "hyponic = HypONIC(model, X, y, optimizer=CS, **optimizer_kwargs)\n\thyponic.optimize(verbose=True)\n\tprint(hyponic.get_optimized_parameters())\n\tprint(hyponic.get_optimized_metric())\n\tprint(hyponic.get_optimized_model())"]}
{"filename": "examples/random_forrest_simple.py", "chunked_list": ["from sklearn.datasets import load_diabetes\n\tfrom sklearn.ensemble import RandomForestRegressor\n\tfrom hyponic.metrics.regression import mse\n\tfrom hyponic.hyponic import HypONIC\n\tX, y = load_diabetes(return_X_y=True)\n\tmodel = RandomForestRegressor()\n\thyperparams = {\n\t    \"min_samples_split\": (0.01, 0.9),\n\t    \"min_samples_leaf\": (0.01, 0.9),\n\t    \"min_weight_fraction_leaf\": (0.0, 0.5),\n", "    \"min_impurity_decrease\": (0.0, 0.9),\n\t    \"criterion\": [\"absolute_error\", \"squared_error\"],\n\t}\n\toptimizer_kwargs = {\n\t    \"epoch\": 50,\n\t    \"population_size\": 50\n\t}\n\thyponic = HypONIC(model, X, y, mse, **optimizer_kwargs)\n\thyponic.optimize(hyperparams, verbose=True)\n\tprint(hyponic.get_optimized_parameters())\n", "print(hyponic.get_optimized_metric())\n\tprint(hyponic.get_optimized_model())\n"]}
{"filename": "examples/function_optimizing.py", "chunked_list": ["import numpy as np\n\tfrom hyponic.optimizers.swarm_based.ABC import ABC\n\tdef sphere_function(x) -> np.ndarray:\n\t    # min = 0 at x = [0, 0, 0, ...]\n\t    return np.sum(np.square(x))\n\tdef rosenbrock_function(x):\n\t    # min = 0 at x = [1, 1, 1, ...]\n\t    return np.sum(100 * (x[1:] - x[:-1] ** 2) ** 2 + (1 - x[:-1]) ** 2)\n\tdef rastrigin_function(x):\n\t    # min = 0 at x = [0, 0, 0, ...]\n", "    return np.sum(x ** 2 - 10 * np.cos(2 * np.pi * x) + 10)\n\tdef griewank_function(x):\n\t    # min = 0 at x = [0, 0, 0, ...]\n\t    return np.sum(x ** 2 / 4000) - np.prod(np.cos(x / np.sqrt(np.arange(1, len(x) + 1)))) + 1\n\tdef ackley_function(x):\n\t    # min = 0 at x = [0, 0, 0, ...]\n\t    return -20 * np.exp(-0.2 * np.sqrt(np.sum(x ** 2) / len(x))) - np.exp(\n\t        np.sum(np.cos(2 * np.pi * x)) / len(x)) + 20 + np.exp(1)\n\tabc = ABC(epoch=40, population_size=100, verbose=True, early_stopping=4)\n\tproblem_dict = {\n", "    'fit_func': ackley_function,\n\t    'lb': [-5.12, -5, -14, -6, -0.9],\n\t    'ub': [5.12, 5, 14, 6, 0.9],\n\t    'minmax': 'min'\n\t}\n\tabc.solve(problem_dict)\n\tprint(abc.get_best_score())\n\tprint(abc.get_best_solution())\n"]}
{"filename": "examples/svm_abc.py", "chunked_list": ["from sklearn.svm import SVC\n\tfrom sklearn.datasets import load_wine\n\tfrom hyponic.optimizers.swarm_based.ABC import ABC\n\tfrom hyponic.hyponic import HypONIC\n\tX, y = load_wine(return_X_y=True)\n\tmodel = SVC()\n\thyperparams = {\n\t    \"C\": (0.01, 1),\n\t    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n\t    \"degree\": [2, 3, 4, 5],\n", "}\n\toptimizer_kwargs = {\n\t    \"epoch\": 10,\n\t    \"pop_size\": 10,\n\t}\n\thyponic = HypONIC(model, X, y, optimizer=ABC, **optimizer_kwargs)\n\thyponic.optimize(hyperparams, verbose=True)\n\tprint(hyponic.get_optimized_parameters())\n\tprint(hyponic.get_optimized_metric())\n\tprint(hyponic.get_optimized_model())\n", "hyponic.visualize_history_time()\n\thyponic.visualize_history_fitness()\n"]}
{"filename": "experiments/test_parallelism.py", "chunked_list": ["\"\"\"\n\tin this file, we will test the performance of the model with multithreading and without it\n\tResults:\n\tSingle 1(50, 50): 44.98987\n\tSingle 2(50, 50): 4.24038\n\tSingle 3(50, 50): 68.50705\n\t8 threads 1(50, 50): 45.17619\n\t8 threads 2(50, 50): 3.01542\n\t8 threads 3(50, 50): 65.72666\n\tSingle 2(100, 200): 5.69452\n", "12 threads 2(100, 200): 6.37028\n\tnumba - 9.080\n\tnumexpr - 6.85\n\t\"\"\"\n\tfrom sklearn.datasets import load_diabetes, load_wine\n\tfrom sklearn.ensemble import RandomForestClassifier\n\tfrom sklearn.tree import DecisionTreeRegressor\n\tfrom sklearn.neighbors import KNeighborsRegressor\n\tfrom hyponic.metrics.regression import mse\n\tfrom hyponic.hyponic import HypONIC\n", "import time\n\timport numpy as np\n\timport warnings\n\twarnings.filterwarnings(\"ignore\")\n\tdef give_test_set1():\n\t    X, y = load_diabetes(return_X_y=True)\n\t    model = KNeighborsRegressor()\n\t    hyperparams = {\n\t        \"n_neighbors\": [i for i in range(1, 10)],\n\t        \"weights\": [\"uniform\", \"distance\"],\n", "        \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n\t        \"leaf_size\": [i for i in range(1, 50)],\n\t        \"p\": (1, 2)\n\t    }\n\t    return X, y, model, hyperparams\n\tdef give_test_set2():\n\t    X, y = load_diabetes(return_X_y=True)\n\t    model = DecisionTreeRegressor()\n\t    hyperparams = {\n\t        \"criterion\": [\"absolute_error\", \"friedman_mse\", \"squared_error\"],\n", "        \"splitter\": [\"best\", \"random\"],\n\t        \"max_depth\": [i for i in range(1, 50, 5)],\n\t        \"min_samples_split\": [i for i in range(2, 10)],\n\t        \"min_samples_leaf\": [i for i in range(1, 10)],\n\t        \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n\t    }\n\t    return X, y, model, hyperparams\n\tdef give_test_set3():\n\t    X, y = load_wine(return_X_y=True)\n\t    model = RandomForestClassifier()\n", "    hyperparams = {\n\t        \"n_estimators\": [i for i in range(1, 100, 5)],\n\t        \"max_depth\": [i for i in range(1, 50, 5)],\n\t        \"min_samples_split\": [i for i in range(2, 10)],\n\t    }\n\t    return X, y, model, hyperparams\n\tdef run(optimizer_kwargs, test_set, num_iter=15):\n\t    X, y, model, hyperparams = test_set\n\t    times = np.zeros(num_iter)\n\t    for i in range(num_iter):\n", "        hyponic = HypONIC(model, X, y, mse, **optimizer_kwargs)\n\t        start = time.time()\n\t        hyponic.optimize(hyperparams)\n\t        end = time.time()\n\t        times[i] = end - start\n\t        print(f\"\\rIteration {i + 1}/{num_iter} done, time: {times[i]}\", end=\"\")\n\t    print()\n\t    return np.mean(times)\n\tdef test_single():\n\t    optimizer_kwargs = {\n", "        \"epoch\": 100,\n\t        \"pop_size\": 200,\n\t        \"mode\": \"single\"\n\t    }\n\t    print(\"Single thread, KNN regression: \", run(optimizer_kwargs, give_test_set1(), num_iter=10))\n\t    print(\"Single thread, Decision Tree regression: \", run(optimizer_kwargs, give_test_set2(), num_iter=50))\n\t    print(\"Single thread, Random Forest classification: \", run(optimizer_kwargs, give_test_set3(), num_iter=5))\n\tdef test_threads():\n\t    n_workers = 12\n\t    optimizer_kwargs = {\n", "        \"epoch\": 100,\n\t        \"pop_size\": 200,\n\t        \"mode\": \"multithread\",\n\t        \"n_workers\": n_workers\n\t    }\n\t    print(f\"{n_workers} threads, KNN regression: \", run(optimizer_kwargs, give_test_set1(), num_iter=10))\n\t    print(f\"{n_workers} threads, Decision Tree regression: \", run(optimizer_kwargs, give_test_set2(), num_iter=15))\n\t    print(f\"{n_workers} threads, Random Forest classification: \", run(optimizer_kwargs, give_test_set3(), num_iter=5))\n\tif __name__ == \"__main__\":\n\t    test_single()\n", "    test_threads()\n"]}
