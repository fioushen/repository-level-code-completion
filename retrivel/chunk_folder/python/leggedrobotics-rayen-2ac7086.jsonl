{"filename": "rayen/constraint_module.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport torch\n\timport torch.nn as nn\n\tfrom . import utils\n\timport numpy as np\n\timport cvxpy as cp\n\timport math\n", "from cvxpylayers.torch import CvxpyLayer\n\timport random\n\timport copy\n\timport time\n\tclass ConstraintModule(torch.nn.Module):\n\t\tdef __init__(self, cs, input_dim=None, method='RAYEN', create_map=True, args_DC3=None):\n\t\t\tsuper().__init__()\n\t\t\tself.method=method\n\t\t\tif(self.method=='Bar' and cs.has_quadratic_constraints):\n\t\t\t\traise Exception(f\"Method {self.method} cannot be used with quadratic constraints\")\n", "\t\tif(self.method=='DC3' and (cs.has_soc_constraints or cs.has_lmi_constraints)):\n\t\t\t\traise NotImplementedError\n\t\t\tif(self.method=='DC3'):\n\t\t\t\tutils.verify(args_DC3 is not None)\n\t\t\t\tself.args_DC3=args_DC3\n\t\t\tself.cs=cs\n\t\t\tself.k=cs.k #Dimension of the ambient space\n\t\t\tself.n=cs.n #Dimension of the embedded space\n\t\t\tD=cs.A_p/((cs.b_p-cs.A_p@cs.z0)@np.ones((1,cs.n)))\n\t\t\tall_P, all_q, all_r = utils.getAllPqrFromQcs(cs.qcs)\n", "\t\tall_M, all_s, all_c, all_d= utils.getAllMscdFromSocs(cs.socs)\n\t\t\tif(cs.has_lmi_constraints):\n\t\t\t\tall_F=copy.deepcopy(cs.lmic.all_F)\n\t\t\t\tH=all_F[-1]\n\t\t\t\tfor i in range(cs.lmic.dim()):\n\t\t\t\t\tH += cs.y0[i,0]*cs.lmic.all_F[i]\n\t\t\t\tHinv=np.linalg.inv(H)\n\t\t\t\tmHinv=-Hinv;\n\t\t\t\tL=np.linalg.cholesky(Hinv) # Hinv = L @ L^T \n\t\t\t\tself.register_buffer(\"mHinv\", torch.Tensor(mHinv))\n", "\t\t\tself.register_buffer(\"L\", torch.Tensor(L))\n\t\t\telse:\n\t\t\t\tall_F=[]\n\t\t\t#See https://discuss.pytorch.org/t/model-cuda-does-not-convert-all-variables-to-cuda/114733/9\n\t\t\t# and https://discuss.pytorch.org/t/keeping-constant-value-in-module-on-correct-device/10129\n\t\t\tself.register_buffer(\"D\", torch.Tensor(D))\n\t\t\tself.register_buffer(\"all_P\", torch.Tensor(np.array(all_P)))\n\t\t\tself.register_buffer(\"all_q\", torch.Tensor(np.array(all_q)))\n\t\t\tself.register_buffer(\"all_r\", torch.Tensor(np.array(all_r)))\n\t\t\tself.register_buffer(\"all_M\", torch.Tensor(np.array(all_M)))\n", "\t\tself.register_buffer(\"all_s\", torch.Tensor(np.array(all_s)))\n\t\t\tself.register_buffer(\"all_c\", torch.Tensor(np.array(all_c)))\n\t\t\tself.register_buffer(\"all_d\", torch.Tensor(np.array(all_d)))\n\t\t\t# self.register_buffer(\"all_F\", torch.Tensor(np.array(all_F))) #This one dies (probably because out of memory) when all_F contains more than 7000 matrices 500x500 approx\n\t\t\tself.register_buffer(\"all_F\", torch.Tensor(all_F))\n\t\t\tself.register_buffer(\"A_p\", torch.Tensor(cs.A_p))\n\t\t\tself.register_buffer(\"b_p\", torch.Tensor(cs.b_p))\n\t\t\tself.register_buffer(\"yp\", torch.Tensor(cs.yp))\n\t\t\tself.register_buffer(\"NA_E\", torch.Tensor(cs.NA_E))\n\t\t\tself.register_buffer(\"z0\", torch.Tensor(cs.z0))\n", "\t\tself.register_buffer(\"y0\", torch.Tensor(cs.y0))\n\t\t\tif(self.method=='PP' or self.method=='UP'):\n\t\t\t\t#Section 8.1.1 of https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\n\t\t\t\tself.z_projected = cp.Variable((self.n,1))         #projected point\n\t\t\t\tself.z_to_be_projected = cp.Parameter((self.n,1))  #original point\n\t\t\t\tconstraints= self.cs.getConstraintsInSubspaceCvxpy(self.z_projected)\n\t\t\t\t#First option.\n\t\t\t\tobjective = cp.Minimize(cp.sum_squares(self.z_projected - self.z_to_be_projected))\n\t\t\t\t#Second option. Sometimes this may be preferred because of this: http://cvxr.com/cvx/doc/advanced.html#eliminating-quadratic-forms This may solve cases of (\"Solver ecos returned status Infeasible\" or \"Solver SCS returned status Infeasible\")\n\t\t\t\t# objective = cp.Minimize(cp.norm(self.z_projected - self.z_to_be_projected))\n", "\t\t\tself.prob_projection = cp.Problem(objective, constraints)\n\t\t\t\tassert self.prob_projection.is_dpp()\n\t\t\t\tself.proj_layer = CvxpyLayer(self.prob_projection, parameters=[self.z_to_be_projected], variables=[self.z_projected])\n\t\t\t\tif(self.cs.has_lmi_constraints):\n\t\t\t\t\tself.solver_projection='SCS' #slower, less accurate, supports LMI constraints\n\t\t\t\telse:\n\t\t\t\t\tself.solver_projection='ECOS' #fast, accurate, does not support LMI constraints\n\t\t\tif(self.method=='RAYEN' or self.method=='RAYEN_old'):\n\t\t\t\tif(cs.has_quadratic_constraints):\n\t\t\t\t\tall_delta=[]\n", "\t\t\t\tall_phi=[]\n\t\t\t\t\tfor i in range(self.all_P.shape[0]): #for each of the quadratic constraints\n\t\t\t\t\t\tP=self.all_P[i,:,:]\n\t\t\t\t\t\tq=self.all_q[i,:,:]\n\t\t\t\t\t\tr=self.all_r[i,:,:]\n\t\t\t\t\t\ty0=self.y0\n\t\t\t\t\t\tsigma=2*(0.5*y0.T@P@y0 + q.T@y0 + r)\n\t\t\t\t\t\tphi = -(y0.T@P + q.T)/sigma\n\t\t\t\t\t\tdelta= ( (y0.T@P + q.T).T@(y0.T@P + q.T) - 4*(0.5*y0.T@P@y0 + q.T@y0 + r)*0.5*P         )/torch.square(sigma)\n\t\t\t\t\t\tall_delta.append(delta)\n", "\t\t\t\t\tall_phi.append(phi)\n\t\t\t\t\tall_delta = torch.stack(all_delta)\n\t\t\t\t\tall_phi = torch.stack(all_phi)\n\t\t\t\t\tself.register_buffer(\"all_delta\", all_delta)\n\t\t\t\t\tself.register_buffer(\"all_phi\", all_phi)\n\t\t\tif(self.method=='Bar'):\n\t\t\t\tprint(\"Computing vertices and rays...\")\n\t\t\t\tV, R = utils.H_to_V(cs.A_p, cs.b_p);\n\t\t\t\tself.register_buffer(\"V\", torch.Tensor(V))\n\t\t\t\tself.register_buffer(\"R\", torch.Tensor(R))\n", "\t\t\tself.num_vertices=self.V.shape[1];\n\t\t\t\tself.num_rays=self.R.shape[1];\n\t\t\t\tassert (self.num_vertices+self.num_rays)>0\n\t\t\t\tprint(f\"Found {self.num_vertices} vertices and {self.num_rays} rays\")\n\t\t\tif(self.method=='DC3'):\n\t\t\t\tA2_DC3, b2_DC3=utils.removeRedundantEquationsFromEqualitySystem(cs.A_E, cs.b_E) \n\t\t\t\tself.register_buffer(\"A2_DC3\", torch.Tensor(A2_DC3))\n\t\t\t\tself.register_buffer(\"b2_DC3\", torch.Tensor(b2_DC3))\n\t\t\t\tself.register_buffer(\"A1_DC3\", torch.Tensor(cs.A_I))\n\t\t\t\tself.register_buffer(\"b1_DC3\", torch.Tensor(cs.b_I))\n", "\t\t\t#Constraints are now \n\t\t\t\t# A2_DC3 y = b2_DC3\n\t\t\t\t# A1_DC3 y <= b1_DC3\n\t\t\t\tself.neq_DC3 = self.A2_DC3.shape[0]\n\t\t\t\t#################################### Find partial_vars and other_vars\n\t\t\t\tif(A2_DC3.shape[0]==0): #There are no equality constraints\n\t\t\t\t\tself.partial_vars=np.arange(self.k)\n\t\t\t\t\tself.other_vars=np.setdiff1d( np.arange(self.k), self.partial_vars)\n\t\t\t\telse:\n\t\t\t\t\t# This is a more efficient way to do https://github.com/locuslab/DC3/blob/35437af7f22390e4ed032d9eef90cc525764d26f/utils.py#L67\n", "\t\t\t\t# Here, we follow  https://stackoverflow.com/a/27907936\n\t\t\t\t\t(A2_DC3_rref, pivots_pos, row_exchanges) = utils.rref(A2_DC3);\n\t\t\t\t\tself.other_vars = [i[1] for i in pivots_pos];\n\t\t\t\t\tself.partial_vars = np.setdiff1d( np.arange(self.k), self.other_vars)\n\t\t\t\t#######################################################\n\t\t\t\tA2p = self.A2_DC3[:, self.partial_vars]\n\t\t\t\tA2o = self.A2_DC3[:, self.other_vars]\n\t\t\t\t# assert np.linalg.matrix_rank(A2_DC3) == np.linalg.matrix_rank(A2o) == A2o.shape[-1]\n\t\t\t\tA2oi = torch.inverse(A2o)\t\t\t\n\t\t\t\t####################################################\n", "\t\t\t####################################################\n\t\t\t\tA1p=self.A1_DC3[:, self.partial_vars];\n\t\t\t\tA1o=self.A1_DC3[:, self.other_vars];\n\t\t\t\tA1_effective = A1p - A1o @ (A2oi @ A2p)\n\t\t\t\tb1_effective = self.b1_DC3 - A1o @ A2oi @ self.b2_DC3;\n\t\t\t\tall_P_effective=torch.Tensor(self.all_P.shape[0], len(self.partial_vars), len(self.partial_vars) )\n\t\t\t\tall_q_effective=torch.Tensor(self.all_q.shape[0], len(self.partial_vars), 1 )\n\t\t\t\tall_r_effective=torch.Tensor(self.all_q.shape[0], 1, 1 )\n\t\t\t\tself.register_buffer(\"A2oi\", A2oi)\n\t\t\t\tself.register_buffer(\"A2p\", A2p)\n", "\t\t\tself.register_buffer(\"A1_effective\", A1_effective)\n\t\t\t\tself.register_buffer(\"b1_effective\", b1_effective)\n\t\t\t\t####################\n\t\t\t\tfor i in range(self.all_P.shape[0]): #for each of the quadratic constraints\n\t\t\t\t\tP=self.all_P[i,:,:]\n\t\t\t\t\tq=self.all_q[i,:,:]\n\t\t\t\t\tr=self.all_r[i,:,:]\n\t\t\t\t\tPo=P[np.ix_(self.other_vars,self.other_vars)].view(len(self.other_vars),len(self.other_vars))\n\t\t\t\t\tPp=P[np.ix_(self.partial_vars,self.partial_vars)].view(len(self.partial_vars),len(self.partial_vars))\n\t\t\t\t\tPop=P[np.ix_(self.other_vars,self.partial_vars)].view(len(self.other_vars),len(self.partial_vars))\n", "\t\t\t\tqo=q[self.other_vars,0:1]\n\t\t\t\t\tqp=q[self.partial_vars,0:1]\n\t\t\t\t\tb2=self.b2_DC3\n\t\t\t\t\tP_effective=2*(-A2p.T@A2oi.T@Pop + 0.5*A2p.T@A2oi.T@Po@A2oi@A2p + 0.5*Pp)\n\t\t\t\t\tq_effective=(b2.T@A2oi.T@Pop + qp.T - qo.T@A2oi@A2p - b2.T@A2oi.T@Po@A2oi@A2p).T\n\t\t\t\t\tr_effective=qo.T@A2oi@b2 + 0.5*b2.T@A2oi.T@Po@A2oi@b2 + r\n\t\t\t\t\t###### QUICK CHECK\n\t\t\t\t\t# tmp=random.randint(1, 100) #number of elements in the batch\n\t\t\t\t\t# yp=torch.rand(tmp, len(self.partial_vars), 1) \n\t\t\t\t\t# y = torch.zeros((tmp, self.k, 1))\n", "\t\t\t\t# y[:, self.partial_vars, :] = yp\n\t\t\t\t\t# y[:, self.other_vars, :] = self.obtainyoFromypDC3(yp)\n\t\t\t\t\t# using_effective=utils.quadExpression(yp, P_effective, q_effective, r_effective)\n\t\t\t\t\t# using_original=utils.quadExpression(y, P, q, r)\n\t\t\t\t\t# assert torch.allclose(using_effective, using_original, atol=1e-05) \n\t\t\t\t\t###################\n\t\t\t\t\tall_P_effective[i,:,:]=P_effective\n\t\t\t\t\tall_q_effective[i,:,:]=q_effective\n\t\t\t\t\tall_r_effective[i,:,:]=r_effective\n\t\t\t\tself.register_buffer(\"all_P_effective\", all_P_effective)\n", "\t\t\tself.register_buffer(\"all_q_effective\", all_q_effective)\n\t\t\t\tself.register_buffer(\"all_r_effective\", all_r_effective)\n\t\t\t\t####################################################\n\t\t\t\t####################################################\n\t\t\tif(self.method=='RAYEN_old'):\n\t\t\t\tself.forwardForMethod=self.forwardForRAYENOld\n\t\t\t\tself.dim_after_map=(self.n+1)\n\t\t\telif(self.method=='RAYEN'):\n\t\t\t\tself.forwardForMethod=self.forwardForRAYEN\n\t\t\t\tself.dim_after_map=self.n\n", "\t\telif(self.method=='UU'):\n\t\t\t\tself.forwardForMethod=self.forwardForUU\n\t\t\t\tself.dim_after_map=self.k\n\t\t\telif(self.method=='Bar'):\n\t\t\t\tself.forwardForMethod=self.forwardForBar\n\t\t\t\tself.dim_after_map=(self.num_vertices + self.num_rays)\n\t\t\telif(self.method=='PP'):\n\t\t\t\tself.forwardForMethod=self.forwardForPP\n\t\t\t\tself.dim_after_map=(self.n)\n\t\t\telif(self.method=='UP'):\n", "\t\t\tself.forwardForMethod=self.forwardForUP\n\t\t\t\tself.dim_after_map=(self.n)\n\t\t\telif(self.method=='DC3'):\n\t\t\t\tself.forwardForMethod=self.forwardForDC3\n\t\t\t\tself.dim_after_map=(self.k - self.neq_DC3)\n\t\t\t\tassert (self.dim_after_map==self.n)\n\t\t\telse:\n\t\t\t\traise NotImplementedError\n\t\t\tif(create_map):\n\t\t\t\tutils.verify(input_dim is not None, \"input_dim needs to be provided\")\n", "\t\t\tself.mapper=nn.Linear(input_dim, self.dim_after_map);\n\t\t\telse:\n\t\t\t\tself.mapper=nn.Sequential(); #Mapper does nothing\n\t\tdef obtainyoFromypDC3(self, yp):\n\t\t\treturn self.A2oi @ (self.b2_DC3 - self.A2p @ yp)\n\t\tdef forwardForDC3(self, q):\n\t\t\t#### Complete partial\n\t\t\ty = torch.zeros((q.shape[0], self.k, 1), device=q.device)\n\t\t\ty[:, self.partial_vars, :] = q\n\t\t\ty[:, self.other_vars, :] = self.obtainyoFromypDC3(q)\n", "\t\t#### Grad steps all\n\t\t\ty_new = y\n\t\t\tstep_index = 0\n\t\t\told_y_step = 0\n\t\t\tif(self.training):\n\t\t\t\tmax_steps=self.args_DC3['max_steps_training'] #This is called corrTrainSteps in DC3 original code\n\t\t\telse:\n\t\t\t\tmax_steps=self.args_DC3['max_steps_testing'] #float(\"inf\") #This is called corrTestMaxSteps in DC3 original code\n\t\t\twhile True:\n\t\t\t\t################################################\n", "\t\t\t################################################ COMPUTE y_step\n\t\t\t\typ=y_new[:, self.partial_vars,:]\n\t\t\t\typT=torch.transpose(yp,1,2);\n\t\t\t\tgrad = 2 * self.A1_effective.T @ torch.relu(self.A1_effective@yp - self.b1_effective)\n\t\t\t\tfor i in range(self.all_P_effective.shape[0]): #for each of the quadratic constraints\n\t\t\t\t\tP_effective=self.all_P_effective[i,:,:]\n\t\t\t\t\tq_effective=self.all_q_effective[i,:,:]\n\t\t\t\t\tr_effective=self.all_r_effective[i,:,:]\n\t\t\t\t\ttmp1=(P_effective@yp + q_effective)\n\t\t\t\t\ttmp2=torch.relu(utils.quadExpression(yp, P_effective, q_effective, r_effective))\n", "\t\t\t\tgrad += 2*tmp1@tmp2 #The 2 is because of the squared norm\n\t\t\t\ty_step = torch.zeros_like(y)\n\t\t\t\ty_step[:, self.partial_vars, :] = grad\n\t\t\t\ty_step[:, self.other_vars, :] = - self.A2oi @ self.A2p @ grad\n\t\t\t\t################################################\n\t\t\t\t################################################\n\t\t\t\tnew_y_step = self.args_DC3['lr'] * y_step + self.args_DC3['momentum'] * old_y_step\n\t\t\t\ty_new = y_new - new_y_step\n\t\t\t\told_y_step = new_y_step\n\t\t\t\tstep_index += 1\n", "\t\t\t################################################\n\t\t\t\t################################################ COMPUTE current violation\n\t\t\t\tstacked=self.A1_DC3@y_new - self.b1_DC3\n\t\t\t\tfor i in range(self.all_P.shape[0]): #for each of the quadratic constraints\n\t\t\t\t\tstacked=torch.cat((stacked,utils.quadExpression(y_new, self.all_P[i,:,:], self.all_q[i,:,:], self.all_r[i,:,:])), dim=1)\n\t\t\t\tviolation=torch.max(torch.relu(stacked))\n\t\t\t\t################################################\n\t\t\t\t################################################\n\t\t\t\tconverged_ineq = (violation < self.args_DC3['eps_converge'])\n\t\t\t\tmax_iter_reached = (step_index >= max_steps)\n", "\t\t\tif(max_iter_reached):\n\t\t\t\t\tbreak\n\t\t\t\tif(converged_ineq):\n\t\t\t\t\tbreak\n\t\t\treturn y_new\n\t\tdef solveSecondOrderEq(self, a, b, c, is_quad_constraint):\n\t\t\tdiscriminant = torch.square(b) - 4*(a)*(c)\n\t\t\tassert torch.all(discriminant >= 0), f\"Smallest element is {torch.min(discriminant)}\"\n\t\t\tsol1=torch.div(  -(b)  - torch.sqrt(discriminant) , 2*a)  #note that for quad constraints the positive solution has the minus: (... - sqrt(...))/(...)\n\t\t\tif(is_quad_constraint):\n", "\t\t\treturn sol1\n\t\t\telse:\n\t\t\t\tsol2=torch.div(  -(b)  +  torch.sqrt(discriminant) , 2*a) \n\t\t\t\treturn torch.relu(torch.maximum(sol1, sol2))\n\t\tdef computeKappa(self,v_bar):\n\t\t\tkappa=torch.relu( torch.max(self.D@v_bar, dim=1, keepdim=True).values  )\n\t\t\tif(len(self.all_P)>0 or len(self.all_M)>0 or len(self.all_F)>0):\n\t\t\t\trho = self.NA_E@v_bar\n\t\t\t\trhoT=torch.transpose(rho,dim0=1, dim1=2)\n\t\t\t\tall_kappas_positives=torch.empty((v_bar.shape[0],0,1), device=v_bar.device)\n", "\t\t\tfor i in range(self.all_P.shape[0]): #for each of the quadratic constraints\n\t\t\t\t\t#FIRST WAY (slower, easier to understand)\n\t\t\t\t\t# P=self.all_P[i,:,:]\n\t\t\t\t\t# q=self.all_q[i,:,:]\n\t\t\t\t\t# r=self.all_r[i,:,:]\n\t\t\t\t\t# c_prime=0.5*rhoT@P@rho;\n\t\t\t\t\t# b_prime=(self.y0.T@P+ q.T)@rho;\n\t\t\t\t\t# a_prime=(0.5*self.y0.T@P@self.y0 + q.T@self.y0 +r) \n\t\t\t\t\t# kappa_positive_i_first_way=self.solveSecondOrderEq(a_prime, b_prime, c_prime, True) \n\t\t\t\t\t#SECOND WAY (faster)\n", "\t\t\t\tkappa_positive_i = self.all_phi[i,:,:]@rho + torch.sqrt(rhoT@self.all_delta[i,:,:]@rho)\n\t\t\t\t\t# assert torch.allclose(kappa_positive_i,kappa_positive_i_first_way, atol=1e-06), f\"{torch.max(torch.abs(kappa_positive_i-kappa_positive_i_first_way))}\"\n\t\t\t\t\tassert torch.all(kappa_positive_i >= 0), f\"Smallest element is {kappa_positive_i}\" #If not, then Z may not be feasible (note that z0 is in the interior of Z)\n\t\t\t\t\tall_kappas_positives = torch.cat((all_kappas_positives, kappa_positive_i), dim=1)\n\t\t\t\tfor i in range(self.all_M.shape[0]): #for each of the SOC constraints\n\t\t\t\t\tM=self.all_M[i,:,:]\n\t\t\t\t\ts=self.all_s[i,:,:]\n\t\t\t\t\tc=self.all_c[i,:,:]\n\t\t\t\t\td=self.all_d[i,:,:]\n\t\t\t\t\tbeta=M@self.y0+s\n", "\t\t\t\ttau=c.T@self.y0+d\n\t\t\t\t\tc_prime=rhoT@M.T@M@rho - torch.square(c.T@rho)\n\t\t\t\t\tb_prime=2*rhoT@M.T@beta - 2*(c.T@rho)@tau\n\t\t\t\t\ta_prime=beta.T@beta - torch.square(tau)\n\t\t\t\t\tkappa_positive_i=self.solveSecondOrderEq(a_prime, b_prime, c_prime, False)\n\t\t\t\t\tassert torch.all(kappa_positive_i >= 0) #If not, then either the feasible set is infeasible (note that z0 is inside the feasible set)\n\t\t\t\t\tall_kappas_positives = torch.cat((all_kappas_positives, kappa_positive_i), dim=1)\n\t\t\t\tif(len(self.all_F)>0): #If there are LMI constraints:\n\t\t\t\t\t############# OBTAIN S\n\t\t\t\t\t# First option (much slower)\n", "\t\t\t\t# S=self.all_F[0,:,:]*rho[:,0:(0+1),0:1]\n\t\t\t\t\t# for i in range(1,len(self.all_F)-1):\n\t\t\t\t\t# \t#See https://discuss.pytorch.org/t/scalar-matrix-multiplication-for-a-tensor-and-an-array-of-scalars/100174/2\t\n\t\t\t\t\t# \tS += self.all_F[i,:,:]*rho[:,i:(i+1),0:1] \n\t\t\t\t\t# Second option (much faster)\n\t\t\t\t\tS=torch.einsum('ajk,ial->ijk', [self.all_F[0:-1,:,:], rho]) #See the tutorial https://rockt.github.io/2018/04/30/einsum\n\t\t\t\t\t############# COMPUTE THE EIGENVALUES\n\t\t\t\t\t## Option 1: (compute whole spectrum of the matrix, using the non-symmetric matrix self.mHinv@S)\n\t\t\t\t\t# eigenvalues = torch.unsqueeze(torch.linalg.eigvals(self.mHinv@S),2) #Note that mHinv@M is not symmetric but always have real eigenvalues\n\t\t\t\t\t# assert (torch.all(torch.isreal(eigenvalues)))\n", "\t\t\t\t# largest_eigenvalue = torch.max(eigenvalues.real, dim=1, keepdim=True).values \n\t\t\t\t\tLTmSL=self.L.T @ (-S) @ self.L #This matrix is symmetric\n\t\t\t\t\t## Option 2: (compute whole spectrum of the matrix, using the symmetric matrix LTmSL). Much faster than Option 1\n\t\t\t\t\teigenvalues = torch.unsqueeze(torch.linalg.eigvalsh(LTmSL),2) #Note that L^T (-S) L is a symmetric matrix\n\t\t\t\t\tlargest_eigenvalue = torch.max(eigenvalues, dim=1, keepdim=True).values \n\t\t\t\t\t## Option 3: Use LOBPCG with A=LTmSL and B=I. The advantage of this method is that only the largest eigenvalue is computed. But, empirically, this option is faster than option 2 only for very big matrices (>1000x1000)\n\t\t\t\t\t# guess_lobpcg=torch.rand(1, H.shape[0], 1);\n\t\t\t\t\t# size_batch=v_bar.shape[0]\n\t\t\t\t\t# largest_eigenvalue, _ = torch.lobpcg(A=LTmSL, k=1, B=None, niter=-1) #, X=guess_lobpcg.expand(size_batch, -1, -1)\n\t\t\t\t\t# largest_eigenvalue=torch.unsqueeze(largest_eigenvalue, 1)\n", "\t\t\t\t## Option 4: Use power iteration to compute the largest eigenvalue. Often times is slower than just computing the whole spectrum, and sometimes it does not converge\n\t\t\t\t\t# guess_v = torch.nn.functional.normalize(torch.rand(S.shape[1],1), dim=0)\n\t\t\t\t\t# largest_eigenvalue=utils.findLargestEigenvalueUsingPowerIteration(self.mHinv@S, guess_v)\n\t\t\t\t\t## Option 5: Use LOBPCG with A=-S and B=H. There are two problems though:\n\t\t\t\t\t# --> This issue: https://github.com/pytorch/pytorch/issues/101075\n\t\t\t\t\t# --> Backward is not implemented for B!=I, see: https://github.com/pytorch/pytorch/blob/d54fcd571af48685b0699f6ac1e31b6871d0d768/torch/_lobpcg.py#L329 \n\t\t\t\t\t## Option 6: Use https://github.com/rfeinman/Torch-ARPACK with LTmSL. The problem is that backward() is not implemented yet \n\t\t\t\t\t## Option 7: Use https://github.com/buwantaiji/DominantSparseEigenAD. But it does not have support for batched matrices, see https://github.com/buwantaiji/DominantSparseEigenAD/issues/1\n\t\t\t\t\tkappa_positive_i = torch.relu( largest_eigenvalue )\n\t\t\t\t\tall_kappas_positives = torch.cat((all_kappas_positives, kappa_positive_i), dim=1)\n", "\t\t\tkappa_nonlinear_constraints=(torch.max(all_kappas_positives, dim=1, keepdim=True).values)\n\t\t\t\tkappa = torch.maximum(kappa, kappa_nonlinear_constraints)\n\t\t\tassert torch.all(kappa >= 0)\n\t\t\treturn kappa\n\t\tdef forwardForRAYENOld(self, q):\n\t\t\tv = q[:,  0:self.n,0:1]\n\t\t\tv_bar=torch.nn.functional.normalize(v, dim=1)\n\t\t\tkappa=self.computeKappa(v_bar)\n\t\t\tbeta= q[:, self.n:(self.n+1),0:1]\n\t\t\talpha=1/(torch.exp(beta) + kappa) \n", "\t\treturn self.getyFromz(self.z0 + alpha*v_bar)\n\t\tdef forwardForRAYEN(self, q):\n\t\t\tv = q[:,  0:self.n,0:1]\n\t\t\tv_bar=torch.nn.functional.normalize(v, dim=1)\n\t\t\tkappa=self.computeKappa(v_bar)\n\t\t\tnorm_v=torch.linalg.vector_norm(v, dim=(1,2), keepdim=True)\n\t\t\talpha=torch.minimum( 1/kappa , norm_v )\n\t\t\treturn self.getyFromz(self.z0 + alpha*v_bar)\n\t\tdef forwardForUU(self, q):\n\t\t\treturn q\n", "\tdef forwardForBar(self, q):\n\t\t\ttmp1 = q[:,  0:self.num_vertices,0:1] #0:1 to keep the dimension. \n\t\t\ttmp2 = q[:,  self.num_vertices:(self.num_vertices+self.num_rays),0:1] #0:1 to keep the dimension. \n\t\t\tlambdas=nn.functional.softmax(tmp1, dim=1)\n\t\t\tmus=torch.abs(tmp2)\n\t\t\treturn self.getyFromz(self.V@lambdas + self.R@mus)\n\t\tdef project(self, q):\n\t\t\t#If you use ECOS, you can set solver_args={'eps': 1e-6} (or smaller) for better solutions, see https://github.com/cvxpy/cvxpy/issues/880#issuecomment-557278620\n\t\t\tz, = self.proj_layer(q, solver_args={'solve_method':self.solver_projection}) # \"max_iters\": 10000\n\t\t\treturn z\n", "\tdef forwardForPP(self, q):\n\t\t\tz=self.project(q)\n\t\t\treturn self.getyFromz(z)\n\t\tdef forwardForUP(self, q):\n\t\t\tif(self.training==False):\n\t\t\t\tz=self.project(q)\n\t\t\telse:\n\t\t\t\tz = q\n\t\t\treturn self.getyFromz(z)\n\t\tdef getDimAfterMap(self):\n", "\t\treturn self.dim_after_map\n\t\tdef gety0(self):\n\t\t\treturn self.getyFromz(self.z0)\n\t\tdef getyFromz(self, z):\n\t\t\ty=self.NA_E@z + self.yp\n\t\t\treturn y\n\t\tdef getzFromy(self, y):\n\t\t\tz=self.NA_E.T@(y - self.yp)\n\t\t\treturn z\n\t\tdef forward(self, x):\n", "\t\t##################  MAPPER LAYER ####################\n\t\t\t#nsib denotes the number of samples in the batch\n\t\t\t# x has dimensions [nsib, numel_input_mapper, 1]. nsib is the number of samples in the batch (i.e., x.shape[0]=x.shape[0])\n\t\t\tq = self.mapper(x.view(x.size(0), -1)) #After this, q has dimensions [nsib, numel_output_mapper]\n\t\t\tq = torch.unsqueeze(q,dim=2)  #After this, q has dimensions [nsib, numel_output_mapper, 1]\n\t\t\t####################################################\n\t\t\ty=self.forwardForMethod(q)\n\t\t\tassert (torch.isnan(y).any())==False, f\"If you are using DC3, try reducing args_DC3[lr]. Right now it's {self.args_DC3['lr']}\"\n\t\t\treturn y"]}
{"filename": "rayen/__init__.py", "chunked_list": []}
{"filename": "rayen/utils.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport cdd\n\timport numpy as np\n\timport torch\n\tfrom colorama import Fore, Back, Style\n\timport torch.nn as nn\n\tdef printInBoldBlue(data_string):\n", "\tprint(Style.BRIGHT+Fore.BLUE+data_string+Style.RESET_ALL)\n\tdef printInBoldRed(data_string):\n\t\tprint(Style.BRIGHT+Fore.RED+data_string+Style.RESET_ALL)\n\tdef printInBoldGreen(data_string):\n\t\tprint(Style.BRIGHT+Fore.GREEN+data_string+Style.RESET_ALL)\n\tdef printInBoldWhite(data_string):\n\t\tprint(Style.BRIGHT+Fore.WHITE+data_string+Style.RESET_ALL)\n\tdef verify(condition, message=\"Condition not satisfied\"):\n\t\tif(condition==False):\n\t\t\traise RuntimeError(message)\n", "def getAllPqrFromQcs(qcs):\n\t\t\tall_P=[]\n\t\t\tall_q=[]\n\t\t\tall_r=[]\n\t\t\tfor qc in qcs:\n\t\t\t\tall_P.append(qc.P)\n\t\t\t\tall_q.append(qc.q)\n\t\t\t\tall_r.append(qc.r)\t\n\t\t\treturn all_P, all_q, all_r\n\tdef getAllMscdFromSocs(socs):\n", "\t\tall_M=[]\n\t\t\tall_s=[]\n\t\t\tall_c=[]\n\t\t\tall_d=[]\n\t\t\tfor soc in socs:\n\t\t\t\tall_M.append(soc.M)\n\t\t\t\tall_s.append(soc.s)\n\t\t\t\tall_c.append(soc.c)\n\t\t\t\tall_d.append(soc.d)\n\t\t\treturn all_M, all_s, all_c, all_d\n", "class CudaTimer():\n\t\tdef __init__(self):\n\t\t\t\tpass\n\t\tdef start(self):\n\t\t\t\t#See https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964\n\t\t\t\tself.start_event = torch.cuda.Event(enable_timing=True)\n\t\t\t\tself.end_event = torch.cuda.Event(enable_timing=True)\n\t\t\t\tself.start_event.record()\n\t\tdef endAndGetTimeSeconds(self):\n\t\t\t\tself.end_event.record()\n", "\t\t\ttorch.cuda.synchronize()\n\t\t\t\treturn  (1e-3)*self.start_event.elapsed_time(self.end_event) #Note that elapsed_time returns the time in milliseconds\n\t########################################################\n\t########################################################\n\t# Inspired by https://github.com/rfeinman/Torch-ARPACK/blob/master/arpack/power_iteration.py\n\t# See also https://ergodic.ugr.es/cphys/LECCIONES/FORTRAN/power_method.pdf\n\t# Note that powerIteration find the eigenvalue with largest absolute value (i.e., the dominant eigenvalue)\n\t# v is the initial guess of the eigenvector associated with the dominant eigenvalue\n\t# A should have shape [size_batch, n, n]\n\t# v should have shape [n, 1]\n", "# Everything in Pytorch\n\tdef powerIteration(A, v, tol=1e-5, max_iter=100000, eps=1e-12, check_freq=2):\n\t\t\tn_iter = 0\n\t\t\tv = torch.nn.functional.normalize(v)\n\t\t\twhile n_iter < max_iter:\n\t\t\t\t\tn_iter += 1\n\t\t\t\t\tu = torch.nn.functional.normalize(A@v, dim=1)\n\t\t\t\t\tif n_iter>1 and ((n_iter % check_freq) == 0):\n\t\t\t\t\t\t\tdistance=torch.mean(torch.abs(1 - torch.abs(  torch.transpose(v,1,2)@u ))) #Note: one disadvantage of this is that it will keep iterating until ALL the elements of the batch have converged\n\t\t\t\t\t\t\tif distance<tol:\n", "\t\t\t\t\t\t\t\tv = u\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\tv = u\n\t\t\telse:\n\t\t\t\t\tprint(f\"distance={distance}\")\n\t\t\t\t\tprint('Power iteration did not converge')\n\t\t\tlamb =  torch.transpose(v,1,2)@A@v #torch.dot(v, torch.mv(A, v))\n\t\t\treturn lamb\n\t#See https://math.stackexchange.com/a/835568 (note that there are some signs wrong in that answer, but otherwise is good)\n\t# v is the initial guess for the power iteration\n", "# A should have shape [size_batch, n, n]\n\t# v should have shape [n, 1]\n\t# Everything in Pytorch\n\tdef findLargestEigenvalueUsingPowerIteration(A, v):\n\t\t\tlamb = powerIteration(A, v)\n\t\t\tcondition=(lamb.flatten()<0)\n\t\t\tif (torch.all(condition == False)):\n\t\t\t\t\treturn lamb\n\t\t\tlamb[condition,:,:]+=powerIteration(A[condition,:,:]-lamb[condition,:,:]*torch.eye(A.shape[1]), v)\n\t\t\treturn lamb\n", "# Other links: \n\t# --> https://github.com/pytorch/pytorch/blob/main/torch/nn/utils/spectral_norm.py#L80\n\tdef isZero(A):\n\t\treturn (not np.any(A))\n\tdef checkMatrixisNotZero(A):\n\t\tverify(not isZero(A))\n\tdef checkMatrixisSymmetric(A):\n\t\tverify(A.shape[0]==A.shape[1])\n\t\tverify(np.allclose(A, A.T))\n\tdef checkMatrixisPsd(A, tol=0.0):\n", "\tcheckMatrixisSymmetric(A)\n\t\teigenvalues=np.linalg.eigvals(A);\n\t\tverify(np.all(eigenvalues >= -tol), f\"Matrix is not PSD, min eigenvalue is {np.amin(eigenvalues)}\")\n\tdef checkMatrixisPd(A):\n\t\tcheckMatrixisSymmetric(A)\n\t\teigenvalues=np.linalg.eigvals(A);\n\t\tverify(np.all(eigenvalues > 0.0), f\"Matrix is not PD, min eigenvalue is {np.amin(eigenvalues)}\")\n\tdef isMatrixSingular(A):\n\t\treturn (np.linalg.matrix_rank(A) < self.E.shape[0])\n\t#Taken from https://gist.github.com/sgsfak/77a1c08ac8a9b0af77393b24e44c9547\n", "#Compute the Reduced Row Echelon Form (RREF) in Python\n\tdef rref(B, tol=1e-8):\n\t\tA = B.copy()\n\t\trows, cols = A.shape\n\t\tr = 0\n\t\tpivots_pos = []\n\t\trow_exchanges = np.arange(rows)\n\t\tfor c in range(cols):\n\t\t\t## Find the pivot row:\n\t\t\tpivot = np.argmax (np.abs (A[r:rows,c])) + r\n", "\t\tm = np.abs(A[pivot, c])\n\t\t\tif m <= tol:\n\t\t\t\t## Skip column c, making sure the approximately zero terms are\n\t\t\t\t## actually zero.\n\t\t\t\tA[r:rows, c] = np.zeros(rows-r)\n\t\t\telse:\n\t\t\t\t## keep track of bound variables\n\t\t\t\tpivots_pos.append((r,c))\n\t\t\t\tif pivot != r:\n\t\t\t\t\t## Swap current row and pivot row\n", "\t\t\t\tA[[pivot, r], c:cols] = A[[r, pivot], c:cols]\n\t\t\t\t\trow_exchanges[[pivot,r]] = row_exchanges[[r,pivot]]\n\t\t\t\t## Normalize pivot row\n\t\t\t\tA[r, c:cols] = A[r, c:cols] / A[r, c];\n\t\t\t\t## Eliminate the current column\n\t\t\t\tv = A[r, c:cols]\n\t\t\t\t## Above (before row r):\n\t\t\t\tif r > 0:\n\t\t\t\t\tridx_above = np.arange(r)\n\t\t\t\t\tA[ridx_above, c:cols] = A[ridx_above, c:cols] - np.outer(v, A[ridx_above, c]).T\n", "\t\t\t## Below (after row r):\n\t\t\t\tif r < rows-1:\n\t\t\t\t\tridx_below = np.arange(r+1,rows)\n\t\t\t\t\tA[ridx_below, c:cols] = A[ridx_below, c:cols] - np.outer(v, A[ridx_below, c]).T\n\t\t\t\tr += 1\n\t\t\t## Check if done\n\t\t\tif r == rows:\n\t\t\t\tbreak;\n\t\treturn (A, pivots_pos, row_exchanges)\n\t# Input is the matrices (A,b) that define the system Ax=b\n", "# Ouput is the matrices (A',b') such that A'x=b' has the same solutions as Ax=b, and (A',b') have the smallest possible number of rows\n\t# See https://mathoverflow.net/a/48867  and the row echelon form in https://en.wikipedia.org/wiki/Gaussian_elimination#General_algorithm_to_compute_ranks_and_bases\n\tdef removeRedundantEquationsFromEqualitySystem(A, b):\n\t\tA_b=np.concatenate((A, b), axis=1)\n\t\tA_b_new, pivots_pos, row_exchanges = rref(A_b)\n\t\t##### Now remove the rows that are zero\n\t\trows_that_are_zero=[]\n\t\tfor i in range(A_b_new.shape[0]):\n\t\t\tif(np.linalg.norm(A_b_new[i,:])<1e-7):\n\t\t\t\trows_that_are_zero.append(i)\n", "\tA_b_new = np.delete(A_b_new, rows_that_are_zero, axis=0)\n\t\t#####################################\n\t\t##### Now get A_new and b_new from A_b_new\n\t\tA_new=A_b_new[:,:-1].reshape((-1, A.shape[1]))\n\t\tb_new=A_b_new[:,-1].reshape((-1, 1))\n\t\t#####################################\n\t\tif(A_b_new.shape[0]>0):\n\t\t\tassert np.linalg.matrix_rank(A_b_new)==A_b_new.shape[0]\n\t\treturn A_new, b_new\n\t#Everything in numpy\n", "#Ellipsoid is defined as {x | (x-c)'E(x-c)<=1}\n\t#Where E is a positive semidefinite matrix\n\tclass Ellipsoid():\n\t\tdef __init__(self, E, c):\n\t\t\tcheckMatrixisPsd(E);\n\t\t\tself.E=E;\n\t\t\tself.c=c;\n\t\tdef convertToQuadraticConstraint(self):\n\t\t\tP=2*self.E;\n\t\t\tq=(-2*self.E@self.c)\n", "\t\tr=self.c.T@self.E@self.c-1\n\t\t\treturn convexQuadraticConstraint(P,q,r)\n\t#Pytorch\n\tdef quadExpression(y, P, q, r):\n\t\tP = P.to(y.device)\n\t\tq = q.to(y.device)\n\t\tr = r.to(y.device)\n\t\tif (q.ndim==2):\n\t\t\tqT=q.T\n\t\telse: #q is a batch\n", "\t\tassert q.ndim==3 \n\t\t\tqT=torch.transpose(q,1,2)\n\t\tresult=0.5*torch.transpose(y,1,2)@P@y + qT@y + r;\n\t\tassert result.shape==(y.shape[0],1,1)\n\t\treturn result\n\t#https://stackoverflow.com/a/3844832\n\tdef all_equal(iterator):\n\t\titerator = iter(iterator)\n\t\ttry:\n\t\t\tfirst = next(iterator)\n", "\texcept StopIteration:\n\t\t\treturn True\n\t\treturn all(first == x for x in iterator)\n\tdef loadpickle(name_file):\n\t\timport pickle\n\t\twith open(name_file, \"rb\") as input_file:\n\t\t\tresult = pickle.load(input_file)\n\t\treturn result\t\n\tdef savepickle(variable, name_file):\n\t\timport pickle\n", "\twith open(name_file, 'wb') as output_file:\n\t\t\tpickle.dump(variable, output_file, pickle.HIGHEST_PROTOCOL)\t\n\t#https://stackoverflow.com/questions/65154622/sample-uniformly-at-random-from-a-simplex-in-python\n\tdef runif_in_simplex(n):\n\t\t''' Return uniformly random vector in the n-simplex '''\n\t\tk = np.random.exponential(scale=1.0, size=n)\n\t\treturn k / sum(k)\n\t#This function is taken from https://github.com/tfrerix/constrained-nets\n\tdef H_to_V(A, b):\n\t\t\"\"\"\n", "\tConverts a polyhedron in H-representation to\n\t\tone in V-representation using pycddlib.\n\t\t\"\"\"\n\t\t# define cdd problem and convert representation\n\t\tif len(b.shape) == 1:\n\t\t\tb = np.expand_dims(b, axis=1)\n\t\tmat_np = np.concatenate([b, -A], axis=1)\n\t\tif mat_np.dtype in [np.int32, np.int64]:\n\t\t\tnt = 'fraction'\n\t\telse:\n", "\t\tnt = 'float'\n\t\tmat_list = mat_np.tolist()\n\t\tmat_cdd = cdd.Matrix(mat_list, number_type=nt)\n\t\tmat_cdd.rep_type = cdd.RepType.INEQUALITY\n\t\tpoly = cdd.Polyhedron(mat_cdd)\n\t\tgen = poly.get_generators()\n\t\t# convert the cddlib output data structure to numpy\n\t\tV_list = []\n\t\tR_list = []\n\t\tlin_set = gen.lin_set\n", "\tV_lin_idx = []\n\t\tR_lin_idx = []\n\t\tfor i in range(gen.row_size):\n\t\t\tg = gen[i]\n\t\t\tg_type = g[0]\n\t\t\tg_vec = g[1:]\n\t\t\tif i in lin_set:\n\t\t\t\tis_linear = True\n\t\t\telse:\n\t\t\t\tis_linear = False\n", "\t\tif g_type == 1:\n\t\t\t\tV_list.append(g_vec)\n\t\t\t\tif is_linear:\n\t\t\t\t\tV_lin_idx.append(len(V_list) - 1)\n\t\t\telif g_type == 0:\n\t\t\t\tR_list.append(g_vec)\n\t\t\t\tif is_linear:\n\t\t\t\t\tR_lin_idx.append(len(R_list) - 1)\n\t\t\telse:\n\t\t\t\traise ValueError('Generator data structure is not valid.')\n", "\tV = np.asarray(V_list)\n\t\tR = np.asarray(R_list)\n\t\t# by convention of cddlib, those rays associated with R_lin_idx\n\t\t# are not constrained to non-negative coefficients\n\t\tif len(R) > 0:\n\t\t\tR = np.concatenate([R, -R[R_lin_idx, :]], axis=0)\n\t\tV=V.T; \n\t\tR=R.T;\n\t\tif(R.size==0):\n\t\t\tR=np.array([[]])#Simply add a dimension, so that both V and R are 2D matrices\n", "\tif(V.size==0):\n\t\t\tV=np.array([[]])#Simply add a dimension, so that both V and R are 2D matrices\n\t\t#Each column of V is a vertex\n\t\t#Each column of R is a ray\n\t\treturn V, R\n"]}
{"filename": "rayen/constraints.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\tfrom . import utils\n\timport cvxpy as cp\n\timport numpy as np\n\tfrom tqdm import tqdm\n\timport scipy\n\timport math\n", "################### CONSTRAINTS\n\t#everything is numpy\n\tclass LinearConstraint():\n\t\t#Constraint is A1<=b1, A2=b2\n\t\tdef __init__(self, A1, b1, A2, b2):\n\t\t\tself.A1 = A1\n\t\t\tself.b1 = b1\n\t\t\tself.A2 = A2\n\t\t\tself.b2 = b2\n\t\t\tutils.verify(self.hasEqConstraints() or self.hasIneqConstraints())\n", "\t\tif (self.hasIneqConstraints()):\n\t\t\t\tutils.verify(A1.ndim == 2)\n\t\t\t\tutils.verify(b1.ndim == 2)\n\t\t\t\tutils.verify(b1.shape[1] ==1)\n\t\t\t\tutils.verify(A1.shape[0] == b1.shape[0])\n\t\t\tif (self.hasEqConstraints()):\n\t\t\t\tutils.verify(A2.ndim == 2)\n\t\t\t\tutils.verify(b2.ndim == 2)\n\t\t\t\tutils.verify(b2.shape[1] ==1)\n\t\t\t\tutils.verify(A2.shape[0] == b2.shape[0])\n", "\t\tif (self.hasIneqConstraints() and self.hasEqConstraints()):\n\t\t\t\tutils.verify(A1.shape[1] == A2.shape[1])\n\t\tdef hasEqConstraints(self):\n\t\t\treturn (self.A2 is not None and self.b2 is not None)\n\t\tdef hasIneqConstraints(self): \n\t\t\treturn (self.A1 is not None and self.b1 is not None)\n\t\tdef dim(self):\n\t\t\tif (self.A1 is not None and self.b1 is not None):\n\t\t\t\treturn self.A1.shape[1]\n\t\t\telse:\n", "\t\t\treturn self.A2.shape[1]\n\t\tdef asCvxpy(self, y, epsilon=0.0):\n\t\t\tconstraints=[];\n\t\t\tif self.hasIneqConstraints():\n\t\t\t\tconstraints.append(self.A1@y<=self.b1)\n\t\t\tif self.hasEqConstraints():\n\t\t\t\tconstraints.append(self.A2@y==self.b2)\n\t\t\treturn constraints\n\tclass ConvexQuadraticConstraint():\n\t\t# Constraint is (1/2)x'Px + q'x +r <=0\n", "\tdef __init__(self, P, q, r, do_checks_P=True):\n\t\t\tself.P=P;\n\t\t\tself.q=q;\n\t\t\tself.r=r;\n\t\t\tif(do_checks_P==True):\n\t\t\t\tutils.checkMatrixisNotZero(self.P);\n\t\t\t\tutils.checkMatrixisSymmetric(self.P)\n\t\t\t\teigenvalues=np.linalg.eigvalsh(self.P);\n\t\t\t\tsmallest_eigenvalue= (np.amin(eigenvalues))\n\t\t\t\t######## Check that the matrix is PSD up to a tolerance\n", "\t\t\ttol=1e-7\n\t\t\t\tutils.verify(smallest_eigenvalue>-tol, f\"Matrix P is not PSD, smallest eigenvalue is {smallest_eigenvalue}\")\n\t\t\t\t#########################\n\t\t\t\t#Note: All the code assummes that P is a PSD matrix. This is specially important when:\n\t\t\t\t#--> Using  cp.quad_form(...) You can use the argument assume_PSD=True (see https://github.com/cvxpy/cvxpy/issues/407)\n\t\t\t\t#--> Computting kappa (if P is not a PSD matrix, you end up with a negative discriminant when solving the 2nd order equation)\n\t\t\t\t######### Correct for possible numerical errors\n\t\t\t\tif( (-tol)<=smallest_eigenvalue<0  ):\n\t\t\t\t\t#Correction due to numerical errors\n\t\t\t\t\t##Option 1\n", "\t\t\t\tself.P = self.P +np.abs(smallest_eigenvalue)*np.eye(self.P.shape[0]) \n\t\t\t\t\t##Option 2 https://stackoverflow.com/a/63131250  and https://math.stackexchange.com/a/1380345\n\t\t\t\t\t# C = (self.P + self.P.T)/2  #https://en.wikipedia.org/wiki/Symmetric_matrix#Decomposition_into_symmetric_and_skew-symmetric\n\t\t\t\t\t# eigval, eigvec = np.linalg.eigh(C)\n\t\t\t\t\t# eigval[eigval < 0] = 0\n\t\t\t\t\t# self.P=eigvec.dot(np.diag(eigval)).dot(eigvec.T)\n\t\t\t\t##########\n\t\tdef dim(self):\n\t\t\treturn self.P.shape[1]\n\t\tdef asCvxpy(self, y, epsilon=0.0):\n", "\t\treturn [0.5*cp.quad_form(y, self.P, assume_PSD=True) + self.q.T@y + self.r <= -epsilon]  #assume_PSD needs to be True because of this: https://github.com/cvxpy/cvxpy/issues/407. We have already checked that it is Psd within a tolerance\n\tclass SOCConstraint():\n\t\t#Constraint is ||My+s||<=c'y+d\n\t\tdef __init__(self, M, s, c, d):\n\t\t\tutils.checkMatrixisNotZero(M);\n\t\t\tutils.checkMatrixisNotZero(c);\n\t\t\tutils.verify(M.shape[1]==c.shape[0])\n\t\t\tutils.verify(M.shape[0]==s.shape[0])\n\t\t\tutils.verify(s.shape[1]==1)\n\t\t\tutils.verify(c.shape[1]==1)\n", "\t\tutils.verify(d.shape[0]==1)\n\t\t\tutils.verify(d.shape[1]==1)\n\t\t\tself.M = M\n\t\t\tself.s = s\n\t\t\tself.c = c\n\t\t\tself.d = d\n\t\tdef dim(self):\n\t\t\treturn self.M.shape[1]\n\t\tdef asCvxpy(self, y, epsilon=0.0):\n\t\t\treturn [cp.norm(self.M@y + self.s) - self.c.T@y - self.d <= -epsilon]\n", "class LMIConstraint():\n\t\t#Constraint is y0 F0 + y1 F1 + ... + ykm1 Fkm1 + Fk >=0\n\t\tdef __init__(self, all_F):\n\t\t\tfor F in all_F:\n\t\t\t\tutils.checkMatrixisSymmetric(F);\n\t\t\tfor F_i in all_F:\n\t\t\t\tutils.verify(F_i.shape==all_F[0].shape)\n\t\t\tself.all_F=all_F\n\t\tdef dim(self):\n\t\t\treturn (len(self.all_F)-1)\n", "\tdef asCvxpy(self, y, epsilon=0.0):\n\t\t\tlmi_left_hand_side=0;\n\t\t\tk=self.dim()\n\t\t\ttmp=self.all_F[0].shape[0]\n\t\t\tfor i in range(k):\n\t\t\t\tlmi_left_hand_side += y[i,0]*self.all_F[i]\n\t\t\tlmi_left_hand_side += self.all_F[k]\n\t\t\treturn [lmi_left_hand_side  >>  epsilon*np.eye(tmp)]\n\t######################################\n\tclass ConvexConstraints():\n", "\t# y0 (a point in the relative interior of the feasible set) can be provided or not\n\t\t# If it's not provided, this code will find one point in the relative interior of the feasible set\n\t\t# If it's provided, this code does not check whether or not that point is in the relative interior of the set. It's the user's responsibility to do that \n\t\t# do_preprocessing_linear can be set to True ONLY when the user knows beforehand that affine_hull{y:A1y<=b1} = R^k  . Again, it's the user's responsibility to ensure that that is actually the case \n\t\tdef __init__(self, lc=None, qcs=[], socs=[], lmic=None, y0=None, do_preprocessing_linear=True, print_debug_info=False):\n\t\t\tif(lc is not None):\n\t\t\t\tself.has_linear_eq_constraints=lc.hasEqConstraints();\n\t\t\t\tself.has_linear_ineq_constraints=lc.hasIneqConstraints();\n\t\t\t\tself.has_linear_constraints=self.has_linear_eq_constraints or self.has_linear_ineq_constraints;\n\t\t\telse:\n", "\t\t\tself.has_linear_eq_constraints=False\n\t\t\t\tself.has_linear_ineq_constraints=False\n\t\t\t\tself.has_linear_constraints=False\n\t\t\tself.has_quadratic_constraints=(len(qcs)>0)\n\t\t\tself.has_soc_constraints=(len(socs)>0)\n\t\t\tself.has_lmi_constraints=(lmic is not None)\n\t\t\tself.lc=lc\n\t\t\tself.qcs=qcs\n\t\t\tself.socs=socs\n\t\t\tself.lmic=lmic\n", "\t\tutils.verify((self.has_quadratic_constraints or self.has_linear_constraints or self.has_soc_constraints or self.has_lmi_constraints), \"There are no constraints!\")\n\t\t\t#Check that the dimensions of all the constraints are the same\n\t\t\tall_dim=[]\n\t\t\tif(self.has_linear_constraints):\n\t\t\t\tall_dim.append(lc.dim())\n\t\t\tfor qc in qcs:\n\t\t\t\tall_dim.append(qc.dim())\n\t\t\tfor soc in socs:\n\t\t\t\tall_dim.append(soc.dim())\n\t\t\tif(self.has_lmi_constraints):\n", "\t\t\tall_dim.append(lmic.dim())\n\t\t\tutils.verify(utils.all_equal(all_dim))\n\t\t\t#####################################3\n\t\t\tself.k=all_dim[0]\n\t\t\t##################### CHOOSE SOLVER\n\t\t\t###################################################\n\t\t\tinstalled_solvers=cp.installed_solvers();\n\t\t\tif ('GUROBI' in installed_solvers) and self.has_lmi_constraints==False:\n\t\t\t\tself.solver='GUROBI' #You need to do `python -m pip install gurobipy`\n\t\t\telif ('ECOS' in installed_solvers) and self.has_lmi_constraints==False:\n", "\t\t\tself.solver='ECOS'\n\t\t\telif ('SCS' in installed_solvers):\n\t\t\t\tself.solver='SCS'\n\t\t\t# elif 'OSQP' in installed_solvers:\n\t\t\t# \tself.solver='OSQP'\n\t\t\t# elif 'CVXOPT' in installed_solvers:\t\n\t\t\t# \tself.solver='CVXOPT'\n\t\t\telse:\n\t\t\t\t#TODO: There are more solvers, see https://www.cvxpy.org/tutorial/advanced/index.html#choosing-a-solver\n\t\t\t\traise Exception(f\"Which solver do you have installed?\")\n", "\t\t###################################################\n\t\t\tif(y0 is None):\n\t\t\t\t###########################################\n\t\t\t\t# Ensure that the feasible set is not empty\n\t\t\t\tz = cp.Variable((self.k,1))\n\t\t\t\tobjective = cp.Minimize(0.0)\n\t\t\t\tconstraints_cvxpy = self.getConstraintsCvxpy(z)\n\t\t\t\tprob = cp.Problem(objective, constraints_cvxpy)\n\t\t\t\tresult = prob.solve(verbose=False, solver=self.solver);\n\t\t\t\tif(prob.status != 'optimal'):\n", "\t\t\t\traise Exception(\"The feasible set is empty\")\n\t\t\t\t############################################\n\t\t\tif(self.has_linear_constraints):\n\t\t\t\t######################################## Stack the matrices so that the linear constraints look like Ax<=b \n\t\t\t\tif(self.has_linear_ineq_constraints):\n\t\t\t\t\tA=self.lc.A1;\n\t\t\t\t\tb=self.lc.b1;\n\t\t\t\t\tif(self.has_linear_eq_constraints): \n\t\t\t\t\t\t#Add the equality constraints as inequality constraints\n\t\t\t\t\t\tA=np.concatenate((A,self.lc.A2,-self.lc.A2), axis=0);\n", "\t\t\t\t\tb=np.concatenate((b,self.lc.b2,-self.lc.b2), axis=0);\t\t\t\t\n\t\t\t\telse:\n\t\t\t\t\t#Add the equality constraints as inequality constraints\n\t\t\t\t\tA=np.concatenate((self.lc.A2,-self.lc.A2), axis=0);\n\t\t\t\t\tb=np.concatenate((self.lc.b2,-self.lc.b2), axis=0);\n\t\t\t\tif(print_debug_info):\n\t\t\t\t\tutils.printInBoldGreen(f\"A is {A.shape} and b is {b.shape}\")\n\t\t\t\t########################################\n\t\t\t\tif(do_preprocessing_linear):\n\t\t\t\t\tTOL=1e-7;\n", "\t\t\t\tz = cp.Variable((self.k,1))\n\t\t\t\t\tif(A.shape[0]>1): #If there is more than one constraint\n\t\t\t\t\t\t#Remove redundant constraints\n\t\t\t\t\t\t################################################\n\t\t\t\t\t\t#Eq 1.5 of https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/167108/1/thesisFinal_MaySzedlak.pdf\n\t\t\t\t\t\t#See also https://mathoverflow.net/a/69667\n\t\t\t\t\t\tif(print_debug_info):\n\t\t\t\t\t\t\tutils.printInBoldBlue(\"Removing redundant constraints...\")\n\t\t\t\t\t\tindexes_const_removed=[]\n\t\t\t\t\t\treversed_indexes=list(reversed(range(A.shape[0])));\n", "\t\t\t\t\tfor i in tqdm(reversed_indexes):\n\t\t\t\t\t\t\tall_rows_but_i=[x for x in range(A.shape[0]) if x != i]\n\t\t\t\t\t\t\tobjective = cp.Maximize(A[i,:]@z)\n\t\t\t\t\t\t\tconstraints=[A[all_rows_but_i,:]@z<=b[all_rows_but_i,:],   A[i,:]@z<=(b[i,0]+1)]\n\t\t\t\t\t\t\tprob = cp.Problem(objective, constraints)\n\t\t\t\t\t\t\tresult = prob.solve(verbose=False, solver=self.solver);\n\t\t\t\t\t\t\tif(prob.status != 'optimal' and prob.status!='optimal_inaccurate'):\n\t\t\t\t\t\t\t\traise Exception(\"Value is not optimal\")\n\t\t\t\t\t\t\tif ((objective.value-b[i,0])<=TOL):\n\t\t\t\t\t\t\t\tindexes_const_removed.append(i)\n", "\t\t\t\t\t\t\tA = np.delete(A, (i), axis=0)\n\t\t\t\t\t\t\t\tb = np.delete(b, (i), axis=0)\n\t\t\t\t\t\tif(print_debug_info):\n\t\t\t\t\t\t\tutils.printInBoldBlue(f\"Removed {len(indexes_const_removed)} constraints \")\n\t\t\t\t\t\t\tutils.printInBoldGreen(f\"A is {A.shape} and b is {b.shape}\")\n\t\t\t\t\t\t################################################\n\t\t\t\t\t#Find equality set\n\t\t\t\t\t################################################\n\t\t\t\t\t# Section 5.2 of https://www.researchgate.net/publication/268373838_Polyhedral_Tools_for_Control\n\t\t\t\t\t# See also Definition 2.16 of https://sites.math.washington.edu/~thomas/teaching/m583_s2008_web/main.pdf\n", "\t\t\t\tE=[] #contains the indexes of the constraints in the equality set\n\t\t\t\t\tif(print_debug_info):\n\t\t\t\t\t\tutils.printInBoldBlue(\"Finding Affine Hull and projecting...\")\n\t\t\t\t\tfor i in tqdm(range(A.shape[0])):\n\t\t\t\t\t\tobjective = cp.Minimize(A[i,:]@z-b[i,0]) #I try to go far from the constraint, into the feasible set\n\t\t\t\t\t\tconstraints=[A@z<=b]\n\t\t\t\t\t\tprob = cp.Problem(objective, constraints)\n\t\t\t\t\t\tif(self.solver=='GUROBI'):\n\t\t\t\t\t\t\tresult = prob.solve(verbose=False, solver=self.solver, reoptimize=True)\n\t\t\t\t\t\telse:\n", "\t\t\t\t\t\tTOL=1e-5;\n\t\t\t\t\t\t\tresult = prob.solve(verbose=False, solver=self.solver)                # When using Gurobi, we need the reoptimize parameter because if not the solver cannot distinguish between infeasible or unbounded. This is the error you get:\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #   The problem is either infeasible or unbounded, but the solver\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #   cannot tell which. Disable any solver-specific presolve methods\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #   and re-solve to determine the precise problem status.\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #   For GUROBI and CPLEX you can automatically perform this re-solve\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #   with the keyword argument prob.solve(reoptimize=True, ...).\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #   warnings.warn(INF_OR_UNB_MESSAGE)\n\t\t\t\t\t\tobj_value=objective.value;\n\t\t\t\t\t\tif(prob.status=='unbounded'):\n", "\t\t\t\t\t\tobj_value=-math.inf #note that we are minimizing\n\t\t\t\t\t\tif(prob.status != 'optimal' and prob.status!='unbounded' and prob.status!='optimal_inaccurate'):\n\t\t\t\t\t\t\traise Exception(f\"prob.status={prob.status}\")\n\t\t\t\t\t\tutils.verify(obj_value<TOL, f\"The objective should be negative. It's {obj_value} right now\")\n\t\t\t\t\t\tif (obj_value>-TOL): #if the objective value is zero (I tried to go far from the constraint, but I couldn't)\n\t\t\t\t\t\t\tE.append(i)\n\t\t\t\telse:\n\t\t\t\t\t#Here we simply choose E such that\n\t\t\t\t\t# A_E == A2, b_E == b_2\n\t\t\t\t\t# A_I == A1, b_I == b_1\n", "\t\t\t\tif(self.has_linear_ineq_constraints):\n\t\t\t\t\t\tstart=self.lc.A1.shape[0]\n\t\t\t\t\telse:\n\t\t\t\t\t\tstart=0\n\t\t\t\t\tE=list(range(start, A.shape[0]))\n\t\t\t\tif(print_debug_info):\n\t\t\t\t\tutils.printInBoldGreen(f\"E={E}\")\n\t\t\t\tI=[i for i in range(A.shape[0]) if i not in E];\n\t\t\t\t#Obtain A_E, b_E and A_I, b_I\n\t\t\t\tif(len(E)>0):\n", "\t\t\t\tA_E=A[E,:];\n\t\t\t\t\tb_E=b[E,:];\n\t\t\t\telse:\n\t\t\t\t\tA_E=np.zeros((1,A.shape[1]));\n\t\t\t\t\tb_E=np.zeros((1,1));\t\n\t\t\t\tif(len(I)>0):\n\t\t\t\t\tA_I=A[I,:];\n\t\t\t\t\tb_I=b[I,:];\n\t\t\t\telse:\n\t\t\t\t\tA_I=np.zeros((1,A.shape[1])); # 0z<=1\n", "\t\t\t\tb_I=np.ones((1,1));\t\n\t\t\t\t#At this point, A_E, b_E, A_I, and b_I have at least one row\n\t\t\t\t#Project into the nullspace of A_E\n\t\t\t\t################################################\n\t\t\t\tNA_E=scipy.linalg.null_space(A_E);\n\t\t\t\t# n=NA_E.shape[1] #dimension of the subspace\n\t\t\t\typ=np.linalg.pinv(A_E)@b_E\n\t\t\t\tA_p=A_I@NA_E;\n\t\t\t\tb_p=b_I-A_I@yp\n\t\t\t\tutils.verify(A_p.ndim == 2, f\"A_p.shape={A_p.shape}\")\n", "\t\t\tutils.verify(b_p.ndim == 2, f\"b_p.shape={b_p.shape}\")\n\t\t\t\tutils.verify(b_p.shape[1] ==1)\n\t\t\t\tutils.verify(A_p.shape[0] == b_p.shape[0])\n\t\t\t\tif(print_debug_info):\n\t\t\t\t\tutils.printInBoldGreen(f\"A_p is {A_p.shape} and b_p is {b_p.shape}\")\n\t\t\t\tself.n=A_p.shape[1] #dimension of the linear subspace\n\t\t\telse:\n\t\t\t\tself.n=self.k\n\t\t\t\tNA_E=np.eye(self.n);\n\t\t\t\typ=np.zeros((self.n,1));\n", "\t\t\tA_p=np.zeros((1,self.n)) # 0z<=1\n\t\t\t\tb_p=np.ones((1,1))\n\t\t\t\tA_E=np.zeros((1,self.k)); # 0y=0\n\t\t\t\tb_E=np.zeros((1,1));\t\n\t\t\t\tA_I=np.zeros((1,self.k)); # 0y<=1\n\t\t\t\tb_I=np.ones((1,1));\t\n\t\t\tself.A_E=A_E\n\t\t\tself.b_E=b_E\n\t\t\tself.A_I=A_I\n\t\t\tself.b_I=b_I\n", "\t\tself.A_p=A_p\t\n\t\t\tself.b_p=b_p\t\n\t\t\tself.yp=yp\t\n\t\t\tself.NA_E=NA_E\t\n\t\t\tutils.verify(self.n==(self.k-np.linalg.matrix_rank(self.A_E)))\n\t\t\t#############Obtain a strictly feasible point z0\n\t\t\t###################################################\n\t\t\tif(y0 is None):\n\t\t\t\tepsilon=cp.Variable()\n\t\t\t\tz0 = cp.Variable((self.n,1))\n", "\t\t\tconstraints=self.getConstraintsInSubspaceCvxpy(z0, epsilon)\n\t\t\t\tconstraints.append(epsilon>=0)\n\t\t\t\tconstraints.append(epsilon<=0.5) #This constraint is needed for the case where the set is unbounded. Any positive value is valid\n\t\t\t\tobjective = cp.Minimize(-epsilon)\n\t\t\t\tprob = cp.Problem(objective, constraints)\n\t\t\t\tresult = prob.solve(verbose=False, solver=self.solver);\n\t\t\t\tif(prob.status != 'optimal' and prob.status!='optimal_inaccurate'):\n\t\t\t\t\traise Exception(f\"Value is not optimal, prob_status={prob.status}\")\n\t\t\t\tutils.verify(epsilon.value>1e-8) #If not, there are no strictly feasible points in the subspace\n\t\t\t\t\t\t\t\t\t\t  \t\t #TODO: change hand-coded tolerance\n", "\t\t\tself.z0 = z0.value\n\t\t\t\tself.y0 = self.NA_E@self.z0 + self.yp\t\n\t\t\telse:\n\t\t\t\tself.y0 = y0\n\t\t\t\tself.z0 = self.NA_E.T@(self.y0-self.yp)\n\t\t\tutils.verify(np.allclose(NA_E.T@NA_E, np.eye(NA_E.shape[1]))) #By definition, N'*N=I\n\t\t\t###################### SET UP PROBLEM FOR PROJECTION\n\t\t\t###################################################\n\t\t\t#Section 8.1.1 of https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\n\t\t\tself.y_projected = cp.Variable((self.k,1))         #projected point\n", "\t\tself.y_to_be_projected = cp.Parameter((self.k,1))  #original point\n\t\t\tconstraints=self.getConstraintsCvxpy(self.y_projected)\n\t\t\tobjective = cp.Minimize(cp.sum_squares(self.y_projected - self.y_to_be_projected))\n\t\t\tself.prob_projection = cp.Problem(objective, constraints)\n\t\t\t###################################################\n\t\tdef getDataAsDict(self):\n\t\t\tif self.has_linear_eq_constraints:\n\t\t\t\tA2=self.lc.A2;\n\t\t\t\tb2=self.lc.b2;\n\t\t\telse:\n", "\t\t\tA2=np.zeros((1, self.k))\n\t\t\t\tb2=np.array([[0]])\t\t\t #0==0 (i.e., no constraint)\n\t\t\tif self.has_linear_ineq_constraints:\n\t\t\t\tA1=self.lc.A1;\n\t\t\t\tb1=self.lc.b1;\n\t\t\telse:\n\t\t\t\tA1=np.zeros((1, self.k))\n\t\t\t\tb1=np.array([[1]])\t          #0<=0 (i.e., no constraint)\n\t\t\tif self.has_quadratic_constraints:\n\t\t\t\tall_P, all_q, all_r = utils.getAllPqrFromQcs(self.qcs)\t\n", "\t\telse:\n\t\t\t\tall_P=[np.zeros((self.k, self.k))]\n\t\t\t\tall_q=[np.zeros((self.k, 1))]\n\t\t\t\tall_r=[-np.ones((1, 1))]                    # -1<=0 (i.e., no constraint)\n\t\t\tif self.has_soc_constraints:\n\t\t\t\tall_M, all_s, all_c, all_d = utils.getAllMscdFromSocs(self.socs)\n\t\t\telse:\n\t\t\t\tall_M=[np.zeros((self.k, self.k))]\n\t\t\t\tall_s=[np.zeros((self.k, 1))]\n\t\t\t\tall_c=[np.zeros((self.k, 1))]                 \n", "\t\t\tall_d=[np.ones((1, 1))]                    # 0<=1 (i.e., no constraint)\n\t\t\tif(self.has_lmi_constraints):\n\t\t\t\tall_F=self.lmic.all_F\n\t\t\telse:\n\t\t\t\tall_F=[]\n\t\t\t\tfor i in range(self.k):\n\t\t\t\t\tall_F.append(np.zeros((self.k, self.k)))\n\t\t\t\tall_F.append(np.eye((self.k)))            # I<=0 (i.e., no constraint)\n\t\t\treturn dict(A2=A2, b2=b2, A1=A1, b1=b1, \n\t\t\t\t\t\tall_P=all_P, all_q=all_q, all_r=all_r, \n", "\t\t\t\t\tall_M=all_M, all_s=all_s, all_c=all_c, all_d=all_d,\n\t\t\t\t\t\tall_F=all_F)\n\t\t######################### CONSTRAINTS IN THE SUBSPACE\n\t\tdef getConstraintsInSubspaceCvxpy(self, z, epsilon=0.0):\n\t\t\tconstraints = [self.A_p@z - self.b_p <= -epsilon*np.ones((self.A_p.shape[0],1))]\n\t\t\ty=self.NA_E@z + self.yp\n\t\t\tconstraints+=self.getNonLinearConstraintsCvxpy(y, epsilon)\n\t\t\treturn constraints\n\t\t###########################################################################\n\t\tdef getNonLinearConstraintsCvxpy(self, y, epsilon=0.0):\n", "\t\tconstraints=[]\n\t\t\tfor qc in self.qcs:   \n\t\t\t\tconstraints += qc.asCvxpy(y, epsilon) \n\t\t\tfor soc in self.socs:   \n\t\t\t\tconstraints += soc.asCvxpy(y, epsilon) \n\t\t\tif(self.has_lmi_constraints):\n\t\t\t\tconstraints += self.lmic.asCvxpy(y, epsilon) \t\n\t\t\treturn constraints\t\n\t\tdef getConstraintsCvxpy(self, y, epsilon=0.0):\n\t\t\tconstraints=[]\n", "\t\tif(self.has_linear_constraints):\n\t\t\t\tconstraints += self.lc.asCvxpy(y, epsilon) \n\t\t\tconstraints+=self.getNonLinearConstraintsCvxpy(y, epsilon)\n\t\t\treturn constraints\n\t\t#######################################3\n\t\tdef project(self, y_to_be_projected):\n\t\t\tself.y_to_be_projected.value=y_to_be_projected;\n\t\t\tobj_value = self.prob_projection.solve(verbose=False, solver=self.solver);\n\t\t\tif(self.prob_projection.status != 'optimal' and self.prob_projection.status != 'optimal_inaccurate'):\n\t\t\t\traise Exception(f\"Value is not optimal, prob_status={self.prob_projection.status}\")\n", "\t\treturn self.y_projected.value, obj_value\t\n\t\tdef getViolation(self, y_to_be_projected):\n\t\t\tif(y_to_be_projected.ndim==1):\n\t\t\t\t#convert to a column vector\n\t\t\t\ty_to_be_projected=np.expand_dims(y_to_be_projected, axis=1)\n\t\t\t_, violation = self.project(y_to_be_projected)\n\t\t\t# assert violation>=0  #violation is nonnegative by definition\n\t\t\treturn violation;"]}
{"filename": "examples/cost_computer.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport torch\n\timport torch.nn as nn\n\timport numpy as np\n\timport scipy\n\timport cvxpy as cp\n\timport math\n", "from cvxpylayers.torch import CvxpyLayer\n\timport random\n\timport copy\n\timport fixpath #Following this example: https://github.com/tartley/colorama/blob/master/demos/demo01.py\n\tfrom rayen import utils\n\tclass CostComputer(nn.Module): #Using nn.Module to be able to use register_buffer (and hence to be able to have the to() method)\n\t\tdef __init__(self, cs):\n\t\t\tsuper().__init__()\n\t\t\tif(cs.has_quadratic_constraints):\n\t\t\t\tall_P, all_q, all_r = utils.getAllPqrFromQcs(cs.qcs)\n", "\t\t\tself.register_buffer(\"all_P\", torch.Tensor(np.array(all_P)))\n\t\t\t\tself.register_buffer(\"all_q\", torch.Tensor(np.array(all_q)))\n\t\t\t\tself.register_buffer(\"all_r\", torch.Tensor(np.array(all_r)))\n\t\t\tif(cs.has_soc_constraints):\n\t\t\t\tall_M, all_s, all_c, all_d = utils.getAllMscdFromSocs(cs.socs)\n\t\t\t\tself.register_buffer(\"all_M\", torch.Tensor(np.array(all_M)))\n\t\t\t\tself.register_buffer(\"all_s\", torch.Tensor(np.array(all_s)))\n\t\t\t\tself.register_buffer(\"all_c\", torch.Tensor(np.array(all_c)))\n\t\t\t\tself.register_buffer(\"all_d\", torch.Tensor(np.array(all_d)))\n\t\t\t#See https://discuss.pytorch.org/t/model-cuda-does-not-convert-all-variables-to-cuda/114733/9\n", "\t\t# and https://discuss.pytorch.org/t/keeping-constant-value-in-module-on-correct-device/10129\n\t\t\tself.register_buffer(\"A_p\", torch.Tensor(cs.A_p))\n\t\t\tself.register_buffer(\"b_p\", torch.Tensor(cs.b_p))\n\t\t\tself.register_buffer(\"yp\", torch.Tensor(cs.yp))\n\t\t\tself.register_buffer(\"NA_E\", torch.Tensor(cs.NA_E))\n\t\t\tself.register_buffer(\"z0\", torch.Tensor(cs.z0))\n\t\t\tself.has_linear_ineq_constraints=cs.has_linear_ineq_constraints\n\t\t\tself.has_linear_eq_constraints=cs.has_linear_eq_constraints\n\t\t\tself.has_quadratic_constraints=cs.has_quadratic_constraints\n\t\t\tself.has_soc_constraints=cs.has_soc_constraints\n", "\t\tself.has_lmi_constraints=cs.has_lmi_constraints\n\t\t\tif self.has_linear_ineq_constraints:\n\t\t\t\tself.register_buffer(\"A1\", torch.Tensor(cs.lc.A1))\n\t\t\t\tself.register_buffer(\"b1\", torch.Tensor(cs.lc.b1))\n\t\t\tif self.has_linear_eq_constraints:\n\t\t\t\tself.register_buffer(\"A2\", torch.Tensor(cs.lc.A2))\n\t\t\t\tself.register_buffer(\"b2\", torch.Tensor(cs.lc.b2))\n\t\tdef getyFromz(self, z):\n\t\t\ty=self.NA_E@z + self.yp\n\t\t\treturn y\n", "\t#This function below assumes that y is in the plane spanned by the columns of NA_E!!\n\t\t# def getzFromy(self, y):\n\t\t# \tz=self.NA_E.T@(y - self.yp)\n\t\t# \treturn z\n\t\tdef getSumSoftCostAllSamples(self, y):\n\t\t\t################## STACK THE VALUES OF ALL THE INEQUALITIES\n\t\t\tall_inequalities=torch.empty((y.shape[0],0,1), device=y.device)\n\t\t\t##### Ap*z<=bp\n\t\t\t# z=self.getzFromy(y); #I cannot use this function, since this function assumes that y lies in \\mathcal{Y}_L\n\t\t\t\t\t\t\t\t   #I could use it for DC3, but not for the \"UU method\"\n", "\t\t# all_inequalities=torch.cat((all_inequalities, self.A_p@z-self.b_p), dim=1);\n\t\t\t##### A1*y<=b1\n\t\t\tif self.has_linear_ineq_constraints:\n\t\t\t\tall_inequalities=torch.cat((all_inequalities, self.A1@y-self.b1), dim=1);\n\t\t\t##### g(y)<=0\n\t\t\tif(self.has_quadratic_constraints):\n\t\t\t\tfor i in range(self.all_P.shape[0]):\n\t\t\t\t\tP=self.all_P[i,:,:]\n\t\t\t\t\tq=self.all_q[i,:,:]\n\t\t\t\t\tr=self.all_r[i,:,:]\n", "\t\t\t\tall_inequalities=torch.cat((all_inequalities, utils.quadExpression(y=y,P=P,q=q,r=r)), dim=1)\n\t\t\tif(self.has_soc_constraints):\n\t\t\t\tfor i in range(self.all_M.shape[0]):\n\t\t\t\t\tM=self.all_M[i,:,:]\n\t\t\t\t\ts=self.all_s[i,:,:]\n\t\t\t\t\tc=self.all_c[i,:,:]\n\t\t\t\t\td=self.all_d[i,:,:]\n\t\t\t\t\tlhs=torch.linalg.vector_norm(M@y + s, dim=1, keepdim=True) - c.T@y - d\n\t\t\t\t\tall_inequalities=torch.cat((all_inequalities, lhs), dim=1)\n\t\t\tif(self.has_lmi_constraints):\n", "\t\t\traise NotImplementedError \n\t\t\tsoft_cost = torch.sum(torch.square(torch.nn.functional.relu(all_inequalities)))\n\t\t\tif self.has_linear_eq_constraints:\n\t\t\t\tsoft_cost_equalities = torch.sum(torch.square(self.A2@y-self.b2)); #This is zero for the DC3 method, and nonzero for the UU method\n\t\t\t\tsoft_cost += soft_cost_equalities\n\t\t\t########################################################################\n\t\t\treturn soft_cost\n\t\tdef getSumObjCostAllSamples(self, y, Pobj, qobj, robj):\n\t\t\ttmp=utils.quadExpression(y=y,P=Pobj,q=qobj,r=robj)\n\t\t\tassert tmp.shape==(y.shape[0], 1, 1)\n", "\t\treturn torch.sum(tmp)\n\t\tdef getSumSupervisedCostAllSamples(self, y, y_predicted):\n\t\t\treturn torch.sum(torch.square(y-y_predicted))\n\t\tdef getSumLossAllSamples(self, params, y, y_predicted, Pobj, qobj, robj, isTesting=False):\n\t\t\tloss=0.0;\n\t\t\tif(params['use_supervised']):\n\t\t\t\tloss += self.getSumSupervisedCostAllSamples(y, y_predicted)\n\t\t\telse:\n\t\t\t\tloss += self.getSumObjCostAllSamples(y_predicted, Pobj, qobj, robj)\n\t\t\tif (isTesting==False and params['weight_soft_cost']>0): #I need the term params['weight_soft_cost']>0 because the soft cost is not implemented for LMI constraints yet\n", "\t\t\tloss += params['weight_soft_cost']*self.getSumSoftCostAllSamples(y_predicted)\n\t\t\t# loss=params['use_supervised']*self.getSumSupervisedCostAllSamples(y, y_predicted) + \\\n\t\t\t# \t (1-isTesting)*params['weight_soft_cost']*self.getSumSoftCostAllSamples(y_predicted) + \\\n\t\t\t# \t (1-params['use_supervised'])*self.getSumObjCostAllSamples(y_predicted, Pobj, qobj, robj)\n\t\t\treturn loss\n"]}
{"filename": "examples/other_utils.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\t#It operates on numpy stuff \n\t#polytope defined as Ax<=b\n\tdef largestBallInPolytope(A,b, max_radius=None):\n\t\tif len(b.shape) == 1:\n\t\t\tb = np.expand_dims(b, axis=1) #Make b a column vector\n\t\tn=A.shape[1];\n", "\tr = cp.Variable()#A scalar\n\t\tx0 = cp.Variable((n,1))\n\t\tconstraints=[]\n\t\t#https://programtalk.com/vs2/python/2718/cvxpy/examples/chebyshev.py/\n\t\t#See also https://dkenefake.github.io/blog/ChebBall for when there are equality constraints\n\t\tfor i in range(A.shape[0]):\n\t\t\ta_i=A[i,:].T\n\t\t\tconstraints.append(r*cp.norm(a_i)+a_i.T@x0<=b[i,0])\n\t\tif(max_radius is not None):\n\t\t\tconstraints.append(r<=max_radius)\n", "\tobjective = cp.Minimize(-r) #This is just a linear program\n\t\tprob = cp.Problem(objective, constraints)\n\t\tprint(\"Calling solve...\")\n\t\tresult = prob.solve(verbose=False);\n\t\tprint(\"Solved!\")\n\t\tif(prob.status != 'optimal'):\n\t\t\traise Exception(\"Value is not optimal\")\n\t\tB=r*np.eye(n)\n\t\tprintInBoldGreen(f\"Found ball of radius r={r.value}\")\n\t\treturn B.value, x0.value\n", "# E representation --> {x s.t. (x-x0)'E(x-x0) <= 1}. Here, E is a psd matrix\n\t# B representation --> {x s.t. x=B*p_bar + x0, ||p_bar||<=1} \\equiv {x s.t. ||inv(B)(x-x0)||<=1} \\equiv {x s.t. (x-x0)'*inv(B)'*inv(B)*(x-x0)<=1}. \n\t# B is \\in R^nxn (although Boyd's book says we can assume B is psd (and therefore also symmetric) without loss of generality, see section 8.4.1\n\t# More info about the B representation: https://ieeexplore.ieee.org/abstract/document/7839930\n\t#It returns the ellipsoid in B representation\n\t#It operates on numpy stuff \n\tdef largestEllipsoidBInPolytope(A,b):\n\t\tif len(b.shape) == 1:\n\t\t\tb = np.expand_dims(b, axis=1) #Make b a column vector\n\t\tn=A.shape[1];\n", "\tB = cp.Variable((n,n), symmetric=True)\n\t\tx0 = cp.Variable((n,1))\n\t\tconstraints=[]\n\t\t#Eq. 8.15 of https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\n\t\t#Also http://web.cvxr.com/cvx/examples/cvxbook/Ch08_geometric_probs/html/max_vol_ellip_in_polyhedra.html\n\t\tfor i in range(A.shape[0]):\n\t\t\ta_i=A[i,:].T\n\t\t\tconstraints.append(cp.norm(B@a_i)+a_i.T@x0<=b[i,0])\n\t\tobjective = cp.Minimize(-cp.atoms.log_det(B))\n\t\tprob = cp.Problem(objective, constraints)\n", "\tprint(\"Calling solve...\")\n\t\tresult = prob.solve(verbose=True);\n\t\tprint(\"Solved!\")\n\t\treturn B.value, x0.value\n\tdef makeColumnVector(a):\n\t\treturn a[:,None]\n\t#Operates on torch stuff\n\tdef squared_norm_of_each_row(D):\n\t\t# print(f\"D**2.shape={(D**2).shape}\")\n\t\tresult=torch.sum(D**2, dim=2, keepdim=True)\n", "\t# print(f\"result.shape={result.shape}\")\n\t\treturn result  #@torch.ones((D.shape[1],1),device=D.device)\n\t#Operates on torch stuff\n\tdef scaleEllipsoidB(B,A,b,x0):\n\t\t# print(\"\\n\\n\")\n\t\t# # #====================First way==========================\n\t\t# # ========================================================\n\t\t# minimum_so_far=torch.Tensor([float(\"inf\")])\n\t\t# for i in range(torch.numel(b)):\n\t\t# \t# print(A[i,:])\n", "\t# \ta_i=makeColumnVector(A[i,:])\n\t\t# \ttmp=(b[i,0]-a_i.mT@x0)**2/(a_i.mT@B@B.T@a_i);\n\t\t# \t# print(f\"numerator={(b[i,0]-a_i.mT@x0)**2}, denominator={a_i.mT@B@B.T@a_i}, result={tmp}\")\n\t\t# \t# print(f\"tmp is {tmp}\")\n\t\t# \t# print(f\"tmp[0,0] is {tmp[0]}\")\n\t\t# \tminimum_so_far=torch.minimum(minimum_so_far, tmp[0,0])\n\t\t# # print(f\"-------> minimum so far={minimum_so_far}\")\n\t\t# result = B*torch.sqrt(minimum_so_far);\n\t\t# print(f\"First way: \\n {result}\")\n\t\t# # #===================Second way==========================\n", "\t# # ========================================================\n\t\t# c=squared_norm_of_each_row(A@B)\n\t\t# e=torch.min(((b-A@x0)**2)/c)\n\t\t# result=B*torch.sqrt(e)\n\t\t# print(f\"Second way: \\n {result}\")\n\t\t# #===================Third way==========================\n\t\t# ========================================================\n\t\tsqrt_c=torch.sqrt(squared_norm_of_each_row(A@B)) #This could be computed in the constructor (and not in each forward call)\n\t\t# print(f\"sqrt_c={sqrt_c}\")\n\t\tAx0=A@x0;\n", "\tb_minus_Ax0=torch.sub(torch.unsqueeze(b,dim=0),Ax0)\n\t\tabs_b_minus_Ax0=torch.abs(b_minus_Ax0) #Note that if x0 is inside the ellipsoid, then I don't need the abs(), since Ax0<=b --> b-Ax0>=0\n\t\tabs_b_minus_Ax0_divided_by_sqrt_c=torch.div(abs_b_minus_Ax0,sqrt_c)\n\t\ttmp=torch.min(abs_b_minus_Ax0_divided_by_sqrt_c,dim=1,keepdim=True)\n\t\tsqrt_e=tmp.values\n\t\tresult=B*sqrt_e\n\t\t# print(f\"Third way: \\n {result}\")\n\t\treturn result;\n\t\t# sqrt\n\t\t# print(f\"sqrt_e={sqrt_e}\")\n", "\t# print(f\"sqrt_e.shape={sqrt_e.shape}\")\n\t\t# print(f\"B.shape={B.shape}\")\n\t\t# print(\"-------------\\n\")\n\t\t# print(f\"x0.shape={x0.shape}\")\n\t\t# print(f\"B.shape={B.shape}\")\n\t\t# print(f\"A.shape={A.shape}\")\n\t\t# print(f\"A@B.shape={(A@B).shape}\")\n\t\t# print(f\"b.shape={b.shape}\")\n\t\t# print(\"==============================\")\n\t\t# print(f\"A={A}\")\n", "\t# print(f\"B={B}\")\n\t\t# print(f\"b={b}\")\n\t\t# print(f\"x0={x0}\\n\")\n\tdef plot2DEllipsoidB(B,x0,ax):\n\t\tx1, x2 = sympy.symbols('x1 x2')\n\t\tx=np.array([[x1],[x2]])\n\t\ttry:\n\t\t\tB_inv=np.linalg.inv(B);\n\t\texcept np.linalg.LinAlgError as err:\n\t\t\tprint(str(err))\n", "\t\treturn\n\t\ttmp=(x-x0).T@B_inv.T@B_inv@(x-x0)-1 #This is [[scalar]]\n\t\texpression=tmp[0,0]; \n\t\tf=sympy.lambdify([x1,x2], expression)\n\t\teigenvalues=np.linalg.eigvals(B)\n\t\ttmp_eig=np.amax(eigenvalues);\n\t\txx = np.linspace(x0[0,0]-tmp_eig, x0[0,0]+tmp_eig, 600)\n\t\tyy = np.linspace(x0[1,0]-tmp_eig, x0[1,0]+tmp_eig, 600)\n\t\txxx, yyy = np.meshgrid(xx, yy)\n\t\tresult=f(xxx, yyy)\n", "\tax.contour(xxx, yyy, result, levels=[0])\n\t\t#OTHER OPTION, but you run into this visualization issue: https://github.com/sympy/sympy/issues/20056\n\t\t# tmp=sympy.plot_implicit(expression,show=True,points=300, adaptive=False, depth = 2)\n\t\t# move_sympyplot_to_axes(tmp, ax)\n\t\t# pts = tmp.get_points()\n\t\t# plt.plot(pts[0], pts[1])"]}
{"filename": "examples/main.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport sys\n\timport os\n\timport time\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n", "import argparse\n\timport itertools\n\timport pandas as pd\n\tfrom torch.utils.data import DataLoader\n\tfrom early_stopping import EarlyStopping\n\tfrom cost_computer import CostComputer\n\tfrom create_dataset import createProjectionDataset, getCorridorDatasets, getCorridorConstraints\n\tfrom examples_sets import getExample\n\timport tqdm\n\timport waitGPU\n", "import random\n\timport time\n\tfrom torch.utils.tensorboard import SummaryWriter\n\timport uuid\n\timport scipy\n\timport fixpath #Following this example: https://github.com/tartley/colorama/blob/master/demos/demo01.py\n\tfrom rayen import constraints, constraint_module, utils\n\tclass SplittedDatasetAndGenerator():\n\t\tdef __init__(self, dataset, percent_train, percent_val, batch_size):\n\t\t\tassert percent_train<=1\n", "\t\tassert percent_val<=1\n\t\t\tassert (percent_train+percent_val)<=1\n\t\t\ttrain_size = int(percent_train * len(dataset))\n\t\t\tval_size = int(percent_val * len(dataset))\n\t\t\ttest_size = len(dataset) - train_size - val_size\n\t\t\t# train_size = 1\n\t\t\t# val_size = 1\n\t\t\t# test_size = 1\n\t\t\t#First option\n\t\t\t# self.train_dataset, self.val_dataset, self.test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n", "\t\t#Second option (Matlab is already doing the randomness). Don't randomize here so that all the methods use the same datasets\n\t\t\t#See https://stackoverflow.com/a/70278974\n\t\t\tself.train_dataset = torch.utils.data.Subset(dataset, range(train_size))\n\t\t\tself.val_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))\n\t\t\tself.test_dataset = torch.utils.data.Subset(dataset, range(train_size + val_size, train_size + val_size + test_size))\n\t\t\t# assert len(self.train_dataset)>0\n\t\t\t# assert len(self.val_dataset)>0\n\t\t\t# assert len(self.test_dataset)>0\n\t\t\tutils.printInBoldRed(f\"Elements [train, val, test]={[len(self.train_dataset), len(self.val_dataset), len(self.test_dataset)]}\")\n\t\t\tself.batch_size=batch_size\n", "\t\tif(len(self.train_dataset)>0):\n\t\t\t\tself.train_generator = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n\t\t\t\tutils.printInBoldRed(f\"Train batches {len(self.train_generator)}\")\n\t\t\tif(len(self.val_dataset)>0):\n\t\t\t\tself.val_generator = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n\t\t\t\tutils.printInBoldRed(f\"Val batches {len(self.val_generator)}\")\n\t\t\tif(len(self.test_dataset)>0): #len(self.test_dataset)\n\t\t\t\tself.test_generator = DataLoader(self.test_dataset, batch_size=len(self.test_dataset), shuffle=False) #One batch only for testing [better inference time estimation]\n\t\t\t\tutils.printInBoldRed(f\"Test batches {len(self.test_generator)}\")\n\tdef onePassOverDataset(model, params, sdag, my_type):\n", "\tcs=model[-1].cs\n\t\tcost_computer=CostComputer(cs)\n\t\tdevice = torch.device(params['device'])\n\t\tmodel = model.to(device)\n\t\tcost_computer = cost_computer.to(device)\n\t\tif(my_type=='train'):\n\t\t\tmodel.train()\n\t\t\tgenerator=sdag.train_generator\n\t\t\tenable_grad=True;\n\t\t\toptimizer = torch.optim.Adam(model.parameters(),lr=params['learning_rate'])\n", "\telif(my_type=='val'):\n\t\t\tmodel.eval()\n\t\t\tgenerator=sdag.val_generator\n\t\t\tenable_grad=False;\n\t\telif(my_type=='test'):\n\t\t\tmodel.eval()\n\t\t\tgenerator=sdag.test_generator\n\t\t\tenable_grad=False;\n\t\telse:\n\t\t\traise NotImplementedError\n", "\tsum_all_losses=0.0\n\t\tsum_time_s=0.0\n\t\tif(my_type=='test'):\n\t\t\tsum_all_violations=0.0\n\t\t\tsum_all_losses_optimization=0.0\n\t\t\tsum_all_time_s_optimization=0.0\n\t\t\tsum_all_violations_optimization=0.0\n\t\tcuda_timer=utils.CudaTimer()\n\t\tall_x=[]\n\t\tall_y=[]\n", "\tall_y_predicted=[]\n\t\tnum_nan_samples= 0\n\t\twith torch.set_grad_enabled(enable_grad):\n\t\t\tfor x, y, Pobj, qobj, robj, opt_time_s, cost in generator: #For each of the batches\t\n\t\t\t\t#----------------------\n\t\t\t\tx = x.to(device)\n\t\t\t\ty = y.to(device)\n\t\t\t\tcuda_timer.start()\n\t\t\t\ty_predicted = model(x)\n\t\t\t\tsum_time_s += cuda_timer.endAndGetTimeSeconds()\n", "\t\t\t#Remove samples that generated nans (happens sometimes in DC3)\n\t\t\t\tis_nan=torch.squeeze(torch.any(y_predicted.isnan(),dim=1)).cpu();\n\t\t\t\tnum_nan_samples += torch.sum(is_nan==True).item()\n\t\t\t\tx = x[~is_nan,:,:]\n\t\t\t\ty = y[~is_nan,:,:]\n\t\t\t\tPobj = Pobj[~is_nan,:,:]\n\t\t\t\tqobj = qobj[~is_nan,:,:]\n\t\t\t\trobj = robj[~is_nan,:,:]\n\t\t\t\topt_time_s = opt_time_s[~is_nan,:,:]\n\t\t\t\tcost = cost[~is_nan,:,:]\n", "\t\t\ty_predicted = y_predicted[~is_nan,:,:]\n\t\t\t\tloss=cost_computer.getSumLossAllSamples(params, y, y_predicted, Pobj, qobj, robj, isTesting=(my_type=='test'))\n\t\t\t\t# print(f\"Loss={loss.item()}\")\n\t\t\t\tsum_all_losses +=  loss.item();\n\t\t\t\t# Save all the values\n\t\t\t\tall_x.append(x.cpu().detach().numpy()) #Could also be accessed directly from the dataset\n\t\t\t\tall_y.append(y.cpu().detach().numpy()) #Could also be accessed directly from the dataset\n\t\t\t\tall_y_predicted.append(y_predicted.cpu().detach().numpy())\n\t\t\t\t#----------------------\n\t\t\t\tif(my_type=='train'):\n", "\t\t\t\tnum_samples_this_batch=x.shape[0];\n\t\t\t\t\tloss_per_sample_in_batch=loss/num_samples_this_batch;\n\t\t\t\t\toptimizer.zero_grad()\n\t\t\t\t\tloss_per_sample_in_batch.backward()\n\t\t\t\t\toptimizer.step()\n\t\t\t\tif(my_type=='test'):\n\t\t\t\t\tprint(\"Computing violations...\")\n\t\t\t\t\tsum_all_violations += np.sum(np.apply_along_axis(cs.getViolation,axis=1, arr=y_predicted.cpu().numpy())).item()\n\t\t\t\t\tprint(\"Violations computed\")\n\t\t\t\t\t###### compute the results from the optimization. TODO: Change to a different place?\n", "\t\t\t\ty_predicted=y\n\t\t\t\t\tloss_optimization=cost_computer.getSumLossAllSamples(params, y, y_predicted, Pobj, qobj, robj, isTesting=True)\n\t\t\t\t\tsum_all_losses_optimization += loss_optimization.item();\n\t\t\t\t\t# print(f\"Loss Opt={loss_optimization.item()}\")\n\t\t\t\t\t# print(f\"Original Loss Opt={torch.sum(cost).item()}\")\n\t\t\t\t\tassert abs(loss_optimization.item()-torch.sum(cost).item())<0.001\n\t\t\t\t\tprint(\"Computing violations optimization...\")\n\t\t\t\t\tsum_all_violations_optimization += np.sum(np.apply_along_axis(cs.getViolation,axis=1, arr=y_predicted.cpu().numpy())).item()\n\t\t\t\t\tprint(\"Violations computed\")\n\t\t\t\t\tsum_all_time_s_optimization += torch.sum(opt_time_s).item()\n", "\t\t\t\t#########################################################\n\t\tnum_samples_dataset=len(generator.dataset) - num_nan_samples;\n\t\t#############################\n\t\tresults={};\n\t\tresults['loss']=sum_all_losses/num_samples_dataset\n\t\tif(my_type=='test'):\n\t\t\tresults['violation']=                    sum_all_violations/num_samples_dataset\n\t\t\tresults['time_s']=                       sum_time_s/num_samples_dataset\n\t\t\tresults[\"optimization_loss\"]=            sum_all_losses_optimization/num_samples_dataset\n\t\t\tresults[\"optimization_violation\"]=       sum_all_violations_optimization/num_samples_dataset\n", "\t\tresults[\"optimization_time_s\"]=    sum_all_time_s_optimization/num_samples_dataset\n\t\t\tresults[\"all_x\"]=    all_x\n\t\t\tresults[\"all_y\"]=    all_y\n\t\t\tresults[\"all_y_predicted\"]=    all_y_predicted\n\t\t\tresults[\"percentage_converged\"]=    100*(1-num_nan_samples/len(generator.dataset))\n\t\t#############################\n\t\treturn results\n\tdef train_model(model, params, sdag, tensorboard_writer, cs):\n\t\tmodel = model.to(torch.device(params['device']))\n\t\toptimizer = torch.optim.Adam(model.parameters(),lr=params['learning_rate'])\n", "\tresults_all_epochs = {'train_loss': [], 'val_loss': []}\n\t\tmy_early_stopping = EarlyStopping(patience=1e100, verbose=False)\n\t\t#See https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n\t\t# with tqdm.trange(params['num_epochs'], ncols=120) as pbar:\n\t\t\t# for epoch in pbar:\n\t\t# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n\t\tfor epoch in range(params['num_epochs']):\n\t\t\t# pbar.set_description(f\"Epoch {epoch}\")\n\t\t\tresults_training_this_epoch = onePassOverDataset(model, params, sdag, 'train')\n\t\t\tresults_validation_this_epoch = onePassOverDataset(model, params, sdag, 'val')\n", "\t\tmy_early_stopping(results_validation_this_epoch['loss'], model)\n\t\t\tresults_all_epochs['train_loss'].append(results_training_this_epoch['loss']) \n\t\t\tresults_all_epochs['val_loss'].append(results_validation_this_epoch['loss']) \n\t\t\tif epoch % params['verbosity'] == 0:\n\t\t\t\tprint(f\"[{params['method']}, w={params['weight_soft_cost']}] {epoch}: train: {results_all_epochs['train_loss'][-1]:.4}, val: {results_all_epochs['val_loss'][-1]:.4}, best_val={my_early_stopping.val_loss_min:.4}\")\n\t\t\t# scheduler.step(results_validation_this_epoch['loss'])\n\t\t\t#This creates two separate plots\n\t\t\t# tensorboard_writer.add_scalar(\"Loss/train\", results_all_epochs['train_loss'][-1], epoch)\n\t\t\t# tensorboard_writer.add_scalar(\"Loss/val\", results_all_epochs['val_loss'][-1], epoch)\n\t\t\t#This createst one plot\n", "\t\ttensorboard_writer.add_scalars('loss', {'train':results_all_epochs['train_loss'][-1], 'val':results_all_epochs['val_loss'][-1]}, epoch)\n\t\t\t# pbar.set_postfix(loss=results_all_epochs['train_loss'][-1], val=results_all_epochs['val_loss'][-1])\n\t\t\t# if my_early_stopping.early_stop:\n\t\t\t# \tprint(\"Early stopping\")\n\t\t\t# \t#Delete the last elements, see https://stackoverflow.com/a/15715924\n\t\t\t# \tdel results_all_epochs['train_loss'][-my_early_stopping.patience:]\n\t\t\t# \tdel results_all_epochs['val_loss'][-my_early_stopping.patience:]\n\t\t\t# \tbreak\n\t\tmy_early_stopping.load_best_model(model)\n\t\t# results_validation_this_epoch = onePassOverDataset(model, params, sdag, 'val')\n", "\t# print(f\"results_validation_this_epoch={results_validation_this_epoch}\")\n\t\ttensorboard_writer.flush()\n\t\treturn results_all_epochs\n\tdef main(params):\n\t\t################# To launch tensorboard directly\n\t\t# import os\n\t\t# import subprocess\n\t\t# folder=\"runs\"\n\t\t# os.system(\"pkill -f tensorboard\")\n\t\t# os.system(\"rm -rf \"+folder)\n", "\t# proc1 = subprocess.Popen([\"tensorboard\",\"--logdir\",folder,\"--bind_all\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n\t\t# proc2 = subprocess.Popen([\"google-chrome\",\"http://localhost:6006/\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n\t\t############################################\n\t\ttorch.set_default_dtype(torch.float64) #This is very important. If you use float32, you may end up with a \"large\" negative discriminant (like -0.000259) in solveSecondOrderEq \n\t\t\t\t\t\t\t\t\t\t\t   #This is also related to the fact that the P matrices need to be PSD matrices (while numerically this is sometimes difficult to achieve)\n\t\ttensorboard_writer = SummaryWriter()\n\t\tmy_dataset, my_dataset_out_dist=getCorridorDatasets(params['dimension_dataset'])\n\t\tsdag=SplittedDatasetAndGenerator(my_dataset, percent_train=0.5045, percent_val=0.2, batch_size=params['batch_size'])\n\t\tsdag_out_dist=SplittedDatasetAndGenerator(my_dataset_out_dist, percent_train=0.0, percent_val=0.0, batch_size=params['batch_size'])\n\t\tif(params['method']=='DC3'):\n", "\t\targs_DC3={}\n\t\t\targs_DC3['lr'] = params['DC3_lr']\n\t\t\targs_DC3['eps_converge'] = params['DC3_eps_converge']\n\t\t\targs_DC3['momentum'] = params['DC3_momentum']\n\t\t\targs_DC3['max_steps_training'] = params['DC3_max_steps_training']\n\t\t\targs_DC3['max_steps_testing'] = params['DC3_max_steps_testing']\n\t\telse:\n\t\t\targs_DC3 = None\n\t\tfolder=\"./scripts/results/\"\n\t\tname_file='dataset'+str(params['dimension_dataset'])+'d_'+params['method']+\"_weight_soft_cost_\"+str(params[\"weight_soft_cost\"])    #+uuid.uuid4().hex #https://stackoverflow.com/a/62277811\n", "\tpath_policy = folder + name_file +\".pt\"\n\t\tpath_training_results = folder + \"results_train_\" + name_file +\".mat\"\n\t\tpath_testing_in_dist_results = folder + \"results_test_in_dist_\" + name_file +\".mat\"\n\t\tpath_testing_out_dist_results = folder + \"results_test_out_dist_\" + name_file +\".mat\"\n\t\tfor i in range(torch.cuda.device_count()):\n\t\t   print(torch.cuda.get_device_properties(i).name)\n\t\tsleep_time=random.randint(0,20)\n\t\tprint(f\"Sleeping for {sleep_time} s\")\n\t\ttime.sleep(sleep_time) #Without this, you get the errors CUBLAS_STATUS_NOT_INITIALIZED or CUDA out of memory when running several training processes in parallel\n\t\twaitGPU.wait(utilization=50, memory_ratio=0.5, interval=random.randint(1, 20), available_memory=2000) #This is to avoid errors like \"CUDA error: CUBLAS_STATUS_NOT_INITIALIZED\" when launching many trainings in parallel\n", "\tprint(\"Done waiting for GPU\")\n\t\tif(params['train']==True):\n\t\t\t## PROJECTION EXAMPLES\n\t\t\t# cs=getExample(4)\n\t\t\t# my_dataset=createProjectionDataset(200, cs, 4.0);\n\t\t\t# my_dataset_out_dist=createProjectionDataset(200, cs, 7.0);\n\t\t\t## CORRIDOR EXAMPLES\n\t\t\tcs=getCorridorConstraints(params['dimension_dataset'])\n\t\t\t#Slide 4 of https://fleuret.org/dlc/materials/dlc-handout-4-6-writing-a-module.pdf\n\t\t\tmodel = nn.Sequential(nn.Flatten(),\n", "\t\t\t\t\t\t\t  # nn.BatchNorm1d(my_dataset.getNumelX()),\n\t\t\t\t\t\t\t\t  nn.Linear(my_dataset.getNumelX(), 64), \n\t\t\t\t\t\t\t\t  nn.ReLU(),\n\t\t\t\t\t\t\t\t  nn.BatchNorm1d(64),\n\t\t\t\t\t\t\t\t  nn.Linear(64, 64),\n\t\t\t\t\t\t\t\t  nn.ReLU(),\n\t\t\t\t\t\t\t\t  nn.Linear(64, 64),\n\t\t\t\t\t\t\t\t  constraint_module.ConstraintModule(cs, input_dim=64, method=params['method'], create_map=True, args_DC3=args_DC3)) \n\t\t\ttraining_results = train_model(model, params, sdag, tensorboard_writer, cs)\n\t\t\t# utils.savepickle(training_results, path_training_results)\n", "\t\tscipy.io.savemat(path_training_results, training_results)\n\t\t\t# torch.save(model.state_dict(), path_policy)  #Save only weights. Will not work properly if the value of any class variable of ConstraintModule changes between different calls\n\t\t\ttorch.save(model, path_policy)                 #Save entire model, see https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model\n\t\tif(params['test']==True):\n\t\t\t# model.load_state_dict(torch.load(path_policy)) #See explanation in the save() function above\n\t\t\tmodel = torch.load(path_policy) #See # https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model\n\t\t\tutils.printInBoldGreen(\"Warming up GPU for a better estimate of the computation time...\")\n\t\t\tx_dummy=torch.Tensor(500, my_dataset.getNumelX(), 1).uniform_(-5.0, 5.0) #Just run some dummy operations on the GPU to warm it up\n\t\t\tx_dummy=x_dummy.to(torch.device(params['device']))\n\t\t\t_ = model(x_dummy)\n", "\t\tutils.printInBoldGreen(\"Testing model inside dist...\")\n\t\t\ttesting_results_in_dist = onePassOverDataset(model, params, sdag, 'test')\n\t\t\tutils.printInBoldGreen(\"Testing model outside dist...\")\n\t\t\ttesting_results_out_dist = onePassOverDataset(model, params, sdag_out_dist, 'test')\n\t\t\t# utils.savepickle(testing_results_in_dist, path_testing_in_dist_results)\n\t\t\t# utils.savepickle(testing_results_out_dist, path_testing_out_dist_results)\n\t\t\tscipy.io.savemat(path_testing_in_dist_results, testing_results_in_dist)\n\t\t\tscipy.io.savemat(path_testing_out_dist_results, testing_results_out_dist)\n\t\t\tnum_trainable_params=sum(\tp.numel() for p in model.parameters() if p.requires_grad)\n\t\t\td = {'method': \t\t\t\t              [name_file,                                       'dataset'+str(params['dimension_dataset'])+'d_'+'Optimization'],\n", "\t\t\t 'num_trainable_params':              [num_trainable_params,         \t                0], \n\t\t\t\t'[In dist] loss':                     [testing_results_in_dist['loss'],      \t        testing_results_in_dist['optimization_loss']      ], \n\t\t\t\t'[In dist] violation':                [testing_results_in_dist['violation'], \t        testing_results_in_dist['optimization_violation'] ],\n\t\t\t\t'[In dist] percentage_converged':     [testing_results_in_dist['percentage_converged'], 100 ],\n\t\t\t\t'[In dist] time_us':                  [1e6*testing_results_in_dist['time_s'],           1e6*testing_results_in_dist['optimization_time_s'] ],\n\t\t\t\t#\n\t\t\t\t'[Out dist] loss':                    [testing_results_out_dist['loss'],      \t  testing_results_out_dist['optimization_loss']      ], \n\t\t\t\t'[Out dist] violation':               [testing_results_out_dist['violation'], \t  testing_results_out_dist['optimization_violation'] ],\n\t\t\t\t'[Out dist] percentage_converged':    [testing_results_out_dist['percentage_converged'], 100 ],\n\t\t\t\t'[Out dist] time_us':                 [1e6*testing_results_out_dist['time_s'],     1e6*testing_results_out_dist['optimization_time_s'] ]}\n", "\t\tdf = pd.DataFrame(data=d)\n\t\t\tpd.set_option('display.max_columns', None)\n\t\t\tprint(df)\n\t\t\tpath_pkl = folder + name_file +\".pkl\"\n\t\t\tdf.to_pickle(path_pkl) \t\n\t\ttensorboard_writer.close()\n\t#See https://stackoverflow.com/a/43357954/6057617\n\tdef str2bool(v):\n\t\tif isinstance(v, bool):\n\t\t\treturn v\n", "\tif v.lower() in ('yes', 'true', 't', 'y', '1'):\n\t\t\treturn True\n\t\telif v.lower() in ('no', 'false', 'f', 'n', '0'):\n\t\t\treturn False\n\t\telse:\n\t\t\traise argparse.ArgumentTypeError('Boolean value expected.')\n\tif __name__ == '__main__':\n\t\tutils.printInBoldGreen(\"\\n\\n\\n==========================================\")\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument('--method', type=str, default='Bar') #RAYEN_old, RAYEN, Bar, UU, PP, UP, DC3\n", "\tparser.add_argument('--dimension_dataset', type=int, default=2)\n\t\tparser.add_argument('--use_supervised', type=str2bool, default=False)\n\t\tparser.add_argument('--weight_soft_cost', type=float, default=0.0)\n\t\tparser.add_argument('--device', type=str, default='cuda:0')\n\t\tparser.add_argument('--num_epochs', type=int, default=2000)\n\t\tparser.add_argument('--batch_size', type=int, default=256)\n\t\tparser.add_argument('--verbosity', type=int, default=1)\n\t\tparser.add_argument('--learning_rate', type=float, default=1e-4)\n\t\tparser.add_argument('--train', type=str2bool, default=True)\n\t\tparser.add_argument('--test', type=str2bool, default=True)\n", "\t#Parameters specific to DC3\n\t\tparser.add_argument('--DC3_lr', type=float, default=1e-5)            #Sometimes the DC3 inner gradient correction does not converge if this lr is high \n\t\tparser.add_argument('--DC3_eps_converge', type=float, default=4e-7)\n\t\tparser.add_argument('--DC3_momentum', type=float, default=0.5)\n\t\tparser.add_argument('--DC3_max_steps_training', type=int, default=10)\n\t\tparser.add_argument('--DC3_max_steps_testing', type=int, default=500) #float(\"inf\")\n\t\targs = parser.parse_args()\n\t\tparams = vars(args)\n\t\tshouldnt_have_soft_cost=(params['method']=='RAYEN_old' or params['method']=='RAYEN' or params['method']=='Bar' or params['method']=='PP')\n\t\t# should_have_soft_cost=(\n", "\t# \t\t\t\t\t\t#Note that DC3 should have soft cost when training, see third paragraph of Section 3.2 of the DC3 paper\n\t\t# \t\t\t\t\t\t(params['method']=='DC3') or\n\t\t# \t\t\t\t\t\t(params['method']=='UP' and params['use_supervised']==False) or\n\t\t# \t\t\t\t\t\t(params['method']=='UU' and params['use_supervised']==False)\n\t\t# \t\t\t\t\t\t)\n\t\t# if(should_have_soft_cost):\n\t\t# \tassert params['weight_soft_cost']>0\n\t\tif(shouldnt_have_soft_cost):\n\t\t\tassert params['weight_soft_cost']==0\n\t\tprint('Parameters:\\n', params)\n", "\tmain(params)"]}
{"filename": "examples/examples_sets.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport numpy as np\n\t# import sys, os\n\t# sys.path.insert(1, os.path.join(sys.path[0], '..'))  #See first comment of this answer: https://stackoverflow.com/a/11158224\n\t# import constraints\n\timport fixpath\n\tfrom rayen import constraints\n", "def getCube():\n\t\tA1=np.array([ [1.0, 0, 0],\n\t\t\t\t\t  [0, 1.0, 0],\n\t\t\t\t\t  [0, 0, 1.0],\n\t\t\t\t\t  [-1.0, 0, 0],\n\t\t\t\t\t  [0, -1.0, 0],\n\t\t\t\t\t  [0, 0, -1.0]]);\n\t\tb1=np.array([[1.0],\n\t\t\t\t\t[1.0],\n\t\t\t\t\t[1.0],\n", "\t\t\t\t[0],\n\t\t\t\t\t[0],\n\t\t\t\t\t[0]])\n\t\treturn A1, b1\n\t#Ellipsoid is defined as {x | (x-c)'E(x-c)<=1}\n\t#Where E is a positive semidefinite matrix\n\tdef getEllipsoidConstraint(E, c):\n\t\t#Convert to (1/2)x'P_ix + q_i'x +r_i <=0\n\t\tP=2*E;\n\t\tq=(-2*E@c)\n", "\tr=c.T@E@c-1\n\t\treturn constraints.ConvexQuadraticConstraint(P, q, r)\n\t#Sphere of radius r centered around c\n\tdef getSphereConstraint(r, c):\n\t\treturn getEllipsoidConstraint((1/(r*r))*np.eye(c.shape[0]),c)\n\tdef getParaboloid3DConstraint():\n\t\tP=np.array([[1.0, 0.0, 0.0],\n\t\t\t\t\t[0.0, 1.0, 0.0],\n\t\t\t\t\t[0.0, 0.0, 0.0]])\n\t\tq=np.array([[0.0],[0.0],[-1.0]])\n", "\tr=np.array([[0.0]])\n\t\treturn constraints.ConvexQuadraticConstraint(P,q,r)\n\tdef getSOC3DConstraint():\n\t\tM=np.array([[1.0, 0.0, 0.0],\n\t\t\t\t\t[0.0, 1.0, 0.0],\n\t\t\t\t\t[0.0, 0.0, 0.0]])\n\t\ts=np.array([[0.0],[0.0],[0.0]])\n\t\tc=np.array([[0.0],[0.0],[1.0]])\n\t\td=np.array([[0.0]])\n\t\treturn constraints.SOCConstraint(M, s, c, d)\n", "def getPSDCone3DConstraint():\n\t\t#[x y;y z] >> 0\n\t\tF0=np.array([[1.0, 0.0],\n\t\t\t\t\t [0.0, 0.0]])\n\t\tF1=np.array([[0.0, 1.0],\n\t\t\t\t\t [1.0, 0.0]])\n\t\tF2=np.array([[0.0, 0.0],\n\t\t\t\t\t [0.0, 1.0]])\n\t\tF3=np.array([[0.0, 0.0],\n\t\t\t\t\t [0.0, 0.0]])\n", "\treturn constraints.LMIConstraint([F0, F1, F2, F3])\n\tdef getNoneLinearConstraints():\n\t\treturn None, None, None, None\n\tdef getNoneQuadraticConstraints():\n\t\treturn [], [], []\n\tdef getExample(example):\n\t\t# A1, b1, A2, b2 = getNoneLinearConstraints()\n\t\t# all_P, all_q, all_r = getNoneQuadraticConstraints()\n\t\tlc=None\n\t\tqcs=[]\n", "\tsocs=[]\n\t\tlmic=None\n\t\tif example==0: #A 2D polygon embeded in 3D\n\t\t\tA1, b1=getCube()\n\t\t\tA2=np.array([[1.0, 1.0, 1.0]]);\n\t\t\tb2=np.array([[1.0]]);\n\t\t\tlc=constraints.LinearConstraint(A1, b1, A2, b2)\n\t\telif example==1: #A polygon embeded in 3D with an sphere\n\t\t\tA1, b1=getCube()\n\t\t\tA2=np.array([[1.0, 1.0, 1.0]]);\n", "\t\tb2=np.array([[1.0]]);\n\t\t\tlc=constraints.LinearConstraint(A1, b1, A2, b2)\n\t\t\tqcs.append(getSphereConstraint(0.8,np.zeros((3,1))))\n\t\telif example==2: #Just a sphere\n\t\t\tqcs.append(getSphereConstraint(2.0,np.zeros((3,1))))\n\t\telif example==3: #Just a paraboloid\n\t\t\tqcs.append(getParaboloid3DConstraint())\n\t\t#A 2d polyhedron \n\t\telif (example==4  \n\t\t#A 2d polyhedron with a cirle\n", "\t     or example==5):   \n\t\t\tA1=np.array([[-1,0],\n\t\t\t\t\t\t [0, -1.0],\n\t\t\t\t\t\t [0, 1.0],\n\t\t\t\t\t\t [0.6,    0.9701]]);\n\t\t\tb1=np.array([[0],\n\t\t\t\t\t\t[0],\n\t\t\t\t\t\t[1],\n\t\t\t\t\t\t[1.2127]])\n\t\t\tlc=constraints.LinearConstraint(A1, b1, None, None)\n", "\t\tif(example==5):\n\t\t\t\tqcs.append(getSphereConstraint(1.25,np.zeros((2,1))))\n\t\telif example==6: #The intersection between a cube and two planes \n\t\t\tA1, b1=getCube()\n\t\t\tA2=np.array([[1.0, 1.0, 1.0],\n\t\t\t\t\t\t  [-1.0, 1.0, 1.0] ]);\n\t\t\tb2=np.array([[1.0],[0.1]]);\n\t\t\tlc=constraints.LinearConstraint(A1, b1, A2, b2)\n\t\telif example==7: #Just a plane\n\t\t\tA2=np.array([[1.0, 1.0, 1.0]]);\n", "\t\tb2=np.array([[1.0]]);\t\n\t\t\tlc=constraints.LinearConstraint(None, None, A2, b2)\n\t\telif example==8: #Unbounded 2d polyhedron. It has two vertices and two rays\n\t\t\tA1=np.array([[0.0,-1.0], [2.0,-4.0], [-2.0,1.0]]);\n\t\t\tb1=np.array([[-2.0], [1.0], [-5.0]]);\n\t\t\tlc=constraints.LinearConstraint(A1, b1, None, None)\n\t\telif example==9: #A paraboloid and a plane\n\t\t\tqcs.append(getParaboloid3DConstraint())\n\t\t\tA2=np.array([[1.0, 1.0, 3.0]]);\n\t\t\tb2=np.array([[1.0]]);\t\n", "\t\tlc=constraints.LinearConstraint(None, None, A2, b2)\t\n\t\telif example==10: #A paraboloid and a shpere\n\t\t\tqcs.append(getParaboloid3DConstraint())\n\t\t\tqcs.append(getSphereConstraint(2.0,np.zeros((3,1))))\t\n\t\telif example==11: #A second-order cone \n\t\t\tsocs.append(getSOC3DConstraint())\n\t\telif example==12: #The PSD cone in 3D\n\t\t\tlmic = getPSDCone3DConstraint()\n\t\telif example==13: #Many of them\n\t\t\tA1=np.array([[-1.0,-1.0,-1.0]])\n", "\t\tb1=np.array([[-1.0]])\n\t\t\tlc=constraints.LinearConstraint(A1, b1, None, None)\n\t\t\tE_ellipsoid=np.array([[0.1,0,0],\n\t\t\t\t\t\t\t\t  [0.0,1.0,0.0],\n\t\t\t\t\t\t\t\t  [0.0,0.0,1.0]])\n\t\t\tqcs.append(getEllipsoidConstraint(E_ellipsoid, np.zeros((3,1))))\n\t\t\tsocs.append(getSOC3DConstraint())\n\t\t\tlmic = getPSDCone3DConstraint()\n\t\telif example==14: #Many of them\n\t\t\tA1=np.array([[-1.0,-1.0,-1.0],\n", "\t\t\t         [-1.0,2.0,2.0]])\n\t\t\tb1=np.array([[-1.0],[1.0]])\n\t\t\tlc=constraints.LinearConstraint(A1, b1, None, None)\n\t\t\tE_ellipsoid=np.array([[0.6,0,0],\n\t\t\t\t\t\t\t\t  [0.0,1.0,0.0],\n\t\t\t\t\t\t\t\t  [0.0,0.0,1.0]])\n\t\t\tqcs.append(getEllipsoidConstraint(E_ellipsoid, np.zeros((3,1))))\n\t\t\t# qcs.append(getParaboloid3DConstraint())\n\t\t\t# socs.append(getSOC3DConstraint())\n\t\t\t# lmic = getPSDCone3DConstraint()\n", "\telse:\n\t\t\traise Exception(\"Not implemented yet\")\n\t\treturn constraints.ConvexConstraints(lc=lc, qcs=qcs, socs=socs, lmic=lmic, print_debug_info=False)"]}
{"filename": "examples/__init__.py", "chunked_list": []}
{"filename": "examples/create_dataset.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\t# Import libraries\n\timport torch\n\tfrom torch.utils.data import Dataset, DataLoader\n\tfrom examples_sets import getExample\n\timport numpy as np\n\timport cvxpy as cp\n", "import matplotlib.pyplot as plt\n\timport scipy.io\n\timport time\n\timport fixpath #Following this example: https://github.com/tartley/colorama/blob/master/demos/demo01.py\n\tfrom rayen import constraints, utils\n\t# create custom dataset class\n\tclass CustomDataset(Dataset):\n\t\tdef __init__(self, all_x, all_y, all_Pobj, all_qobj, all_robj, all_times_s, all_costs):\n\t\t\tself.all_x = [torch.Tensor(item) for item in all_x] \n\t\t\tself.all_y = [torch.Tensor(item) for item in all_y] \n", "\t\tself.all_Pobj = [torch.Tensor(item) for item in all_Pobj] \n\t\t\tself.all_qobj = [torch.Tensor(item) for item in all_qobj] \n\t\t\tself.all_robj = [torch.Tensor(item) for item in all_robj] \n\t\t\tself.all_times_s = [torch.Tensor(item) for item in all_times_s]\n\t\t\tself.all_costs = [torch.Tensor(item) for item in all_costs] \n\t\tdef __len__(self):\n\t\t\treturn len(self.all_y)\n\t\tdef __getitem__(self, idx):\n\t\t\treturn self.all_x[idx], self.all_y[idx], self.all_Pobj[idx], self.all_qobj[idx], self.all_robj[idx], self.all_times_s[idx], self.all_costs[idx]\n\t\tdef getNumelX(self):\n", "\t\treturn self.all_x[0].shape[0] #Using the first element\n\t\tdef getNumelY(self):\n\t\t\treturn self.all_y[0].shape[0] #Using the first element\n\t\t# def plot(self, ax):\n\t\t# \tdim=self.all_x[0].shape[0]\n\t\t# \t# print(f\"x[0]={x[0]}\")\n\t\t# \tall_x_np=np.concatenate(self.all_x, axis=1 )\n\t\t# \tall_y_np=np.concatenate(self.all_y, axis=1 )\n\t\t# \tprint(f\"dim={dim}\")\n\t\t# \tprint(f\"dim={dim}\")\n", "\t# \tif(dim==3):\n\t\t# \t\tax.scatter3D(all_x_np[0,:], all_x_np[1,:], all_x_np[2,:],color='red')\n\t\t# \t\tax.scatter3D(all_y_np[0,:], all_y_np[1,:], all_y_np[2,:],color='blue')\n\t\t# \t\tprint(f\"all_x_np={all_x_np}\")\n\t\t# \tif(dim==2):\n\t\t# \t\tax.scatter(all_x_np[0,:], all_x_np[1,:],color='red')\n\t\t# \t\tax.scatter(all_y_np[0,:], all_y_np[1,:],color='blue')\n\tdef createProjectionDataset(num_samples, cs, bbox_half_side): \n\t\tall_x=[];\n\t\tall_y=[];\n", "\tall_Pobj=[];\n\t\tall_qobj=[];\n\t\tall_robj=[];\n\t\tall_times_s=[];\n\t\tall_costs=[];\n\t\tfor i in range(num_samples):\n\t\t\tx=np.random.uniform(low=-bbox_half_side, high=bbox_half_side, size=(cs.k,1))\n\t\t\tall_x.append(x)\n\t\t\tstart=time.time();\n\t\t\ty, cost = cs.project(x)\n", "\t\tall_times_s.append(time.time()-start)\n\t\t\tall_costs.append(cost)\n\t\t\tall_y.append(y)\n\t\t\tassert x.shape[1]==1\n\t\t\t# ||x-y||^2 = y'*y  -2x'*y + x'*x\n\t\t\t# Match with 0.5*y'*P_obj*y + q_obj'*y + r_obj \n\t\t\tPobj=2*np.eye(x.shape[0])\n\t\t\tqobj=-2*x\n\t\t\trobj=x.T@x\n\t\t\tassert qobj.shape[1]==1\n", "\t\tassert robj.shape[1]==1, f\"robj.shape={robj.shape}\"\n\t\t\tall_Pobj.append(Pobj)\n\t\t\tall_qobj.append(qobj)\n\t\t\tall_robj.append(robj)\n\t\tmy_dataset = CustomDataset(all_x, all_y, all_Pobj, all_qobj, all_robj, all_times_s, all_costs)\n\t\t###plotting stuff\n\t\t# fig = plt.figure()\n\t\t# if(dim_ambient_space==3):\n\t\t# \tax = fig.add_subplot(111, projection=\"3d\")\n\t\t# else:\n", "\t# \tax = fig.add_subplot(111)\n\t\t# my_dataset.plot(ax);\n\t\t# plt.show()\n\t\t#######\n\t\t# exit()\n\t\treturn my_dataset\n\tdef getCorridorConstraints(dimension):\n\t\tmat = scipy.io.loadmat('./scripts/matlab/corridor_dim'+str(dimension)+'.mat')\n\t\tA1=mat['A1'];\n\t\tb1=mat['b1'];\n", "\tA2=mat['A2'];\n\t\tb2=mat['b2'];\n\t\tif(len(mat[\"all_P\"])>0):\n\t\t\tall_P=list(mat[\"all_P\"][0])\n\t\t\tall_q=list(mat[\"all_q\"][0])\n\t\t\tall_r=list(mat[\"all_r\"][0])\n\t\telse:\n\t\t\tall_P=[]\n\t\t\tall_q=[]\n\t\t\tall_r=[]\n", "\tassert A1.ndim==2\n\t\tassert b1.ndim==2\n\t\tassert A2.ndim==2\n\t\tassert b2.ndim==2\n\t\tlc=constraints.LinearConstraint(A1=A1, b1=b1, A2=A2, b2=b2);\n\t\tqcs=[];\n\t\tfor i in range(len(all_P)):\n\t\t\tqc=constraints.ConvexQuadraticConstraint(P=all_P[i], q=all_q[i], r=all_r[i]);\n\t\t\tqcs.append(qc)\n\t\tcs=constraints.ConvexConstraints(lc=lc, qcs=qcs, socs=[], lmic=None)\n", "\treturn cs\n\tdef getCorridorDatasets(dimension):\n\t\tmat = scipy.io.loadmat('./scripts/matlab/corridor_dim'+str(dimension)+'.mat')\n\t\tall_x=list(mat[\"all_x\"][0])\n\t\tall_y=list(mat[\"all_y\"][0])\n\t\tall_Pobj=list(mat[\"all_Pobj\"][0])\n\t\tall_qobj=list(mat[\"all_qobj\"][0])\n\t\tall_robj=list(mat[\"all_robj\"][0])\n\t\tall_times_s=list(mat[\"all_times_s\"][0])\n\t\tall_costs=list(mat[\"all_costs\"][0])\n", "\tall_x_out_dist=list(mat[\"all_x_out_dist\"][0])\n\t\tall_y_out_dist=list(mat[\"all_y_out_dist\"][0])\n\t\tall_Pobj_out_dist=list(mat[\"all_Pobj_out_dist\"][0])\n\t\tall_qobj_out_dist=list(mat[\"all_qobj_out_dist\"][0])\n\t\tall_robj_out_dist=list(mat[\"all_robj_out_dist\"][0])\n\t\tall_times_s_out_dist=list(mat[\"all_times_s_out_dist\"][0])\n\t\tall_costs_out_dist=list(mat[\"all_costs_out_dist\"][0])\n\t\t# polyhedron=mat[\"polyhedron\"]\n\t\t# A1=polyhedron['A1'][0,0];\n\t\t# b1=polyhedron['b1'][0,0];\n", "\t# polyhedron=mat[\"polyhedron\"]\n\t\t# A1=polyhedron['A1'][0,0];\n\t\t# b1=polyhedron['b1'][0,0];\n\t\tassert all_y[0].shape[1]==1\n\t\tassert all_x[0].shape[1]==1\n\t\tassert all_x_out_dist[0].shape[1]==1\n\t\tassert all_y_out_dist[0].shape[1]==1\n\t\tmy_dataset = CustomDataset(all_x, all_y, all_Pobj, all_qobj, all_robj, all_times_s, all_costs)\n\t\tmy_dataset_out_dist = CustomDataset(all_x_out_dist, all_y_out_dist, all_Pobj_out_dist, all_qobj_out_dist, all_robj_out_dist, all_times_s_out_dist, all_costs_out_dist)\n\t\treturn my_dataset, my_dataset_out_dist\n"]}
{"filename": "examples/utils_examples.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport matplotlib.pyplot as plt\n\timport cvxpy as cp\n\timport numpy as np\n\timport cdd\n\timport fixpath\n\tfrom rayen import utils\n", "########################################################\n\t########################################################\n\t#Ellisoid is represented by {x | x'*E*x <=1}\n\tdef plotEllipsoid(E, x0, ax):\n\t\t#Partly taken from https://github.com/CircusMonkey/covariance-ellipsoid/blob/master/ellipsoid.py\n\t\t\"\"\"\n\t\tReturn the 3d points representing the covariance matrix\n\t\tcov centred at mu and scaled by the factor nstd.\n\t\tPlot on your favourite 3d axis. \n\t\tExample 1:  ax.plot_wireframe(X,Y,Z,alpha=0.1)\n", "\tExample 2:  ax.plot_surface(X,Y,Z,alpha=0.1)\n\t\t\"\"\"\n\t\tassert E.shape==(3,3)\n\t\tB=np.linalg.inv(scipy.linalg.sqrtm(E))\n\t\t#Ellisoid is now represented by { Bp+x0 | ||p|| <=1}\n\t\t# Find and sort eigenvalues \n\t\teigvals, eigvecs = np.linalg.eigh(B)\n\t\tidx = np.sum(B,axis=0).argsort()\n\t\teigvals_temp = eigvals[idx]\n\t\tidx = eigvals_temp.argsort()\n", "\teigvals = eigvals[idx]\n\t\teigvecs = eigvecs[:,idx]\n\t\t# Set of all spherical angles to draw our ellipsoid\n\t\tn_points = 100\n\t\ttheta = np.linspace(0, 2*np.pi, n_points)\n\t\tphi = np.linspace(0, np.pi, n_points)\n\t\t# Width, height and depth of ellipsoid\n\t\trx, ry, rz = np.sqrt(eigvals)\n\t\t# Get the xyz points for plotting\n\t\t# Cartesian coordinates that correspond to the spherical angles:\n", "\tX = rx * np.outer(np.cos(theta), np.sin(phi))\n\t\tY = ry * np.outer(np.sin(theta), np.sin(phi))\n\t\tZ = rz * np.outer(np.ones_like(theta), np.cos(phi))\n\t\t# Rotate ellipsoid for off axis alignment\n\t\told_shape = X.shape\n\t\t# Flatten to vectorise rotation\n\t\tX,Y,Z = X.flatten(), Y.flatten(), Z.flatten()\n\t\tX,Y,Z = np.matmul(eigvecs, np.array([X,Y,Z]))\n\t\tX,Y,Z = X.reshape(old_shape), Y.reshape(old_shape), Z.reshape(old_shape)\n\t\t# Add in offsets for the center\n", "\tX = X + x0[0]\n\t\tY = Y + x0[1]\n\t\tZ = Z + x0[2]\n\t\tax.plot_wireframe(X,Y,Z, color='r', alpha=0.1)\n\tdef plot3DPolytopeHRepresentation(A,b, limits, ax):\n\t\tpoints, R=utils.H_to_V(A,b)\n\t\tif(R.shape[1]>0):\n\t\t\tutils.printInBoldRed(\"Plotting 3D unbounded polyhedron not implemented yet\")\n\t\t\treturn\n\t\tplotConvexHullOf3DPoints(points, limits, ax)\n", "def plotConvexHullOf3DPoints(V, limits, ax):\n\t\tpoints=V.T\n\t\t## https://stackoverflow.com/a/71544694/6057617\n\t\t# to get the convex hull with cdd, one has to prepend a column of ones\n\t\tvertices = np.hstack((np.ones((points.shape[0],1)), points))\n\t\t# do the polyhedron\n\t\tmat = cdd.Matrix(vertices, linear=False, number_type=\"fraction\") \n\t\tmat.rep_type = cdd.RepType.GENERATOR\n\t\tpoly = cdd.Polyhedron(mat)\n\t\t# get the adjacent vertices of each vertex\n", "\tadjacencies = [list(x) for x in poly.get_input_adjacency()]\n\t\t# store the edges in a matrix (giving the indices of the points)\n\t\tedges = [None]*(8-1)\n\t\tfor i,indices in enumerate(adjacencies[:-1]):\n\t\t\tindices = list(filter(lambda x: x>i, indices))\n\t\t\tl = len(indices)\n\t\t\tcol1 = np.full((l, 1), i)\n\t\t\tindices = np.reshape(indices, (l, 1))\n\t\t\tedges[i] = np.hstack((col1, indices))\n\t\tEdges = np.vstack(tuple(edges))\n", "\t# plot\n\t\t# fig = plt.figure()\n\t\t# ax = fig.add_subplot(111, projection=\"3d\")\n\t\tstart = points[Edges[:,0]]\n\t\tend = points[Edges[:,1]]\n\t\tfor i in range(12):\n\t\t\tax.plot(\n\t\t\t\t[start[i,0], end[i,0]], \n\t\t\t\t[start[i,1], end[i,1]], \n\t\t\t\t[start[i,2], end[i,2]],\n", "\t\t\t\"blue\"\n\t\t\t)\n\t\t# ax.set_xlabel(\"x\")\n\t\t# ax.set_ylabel(\"y\")\n\t\t# ax.set_zlabel(\"z\")\n\t\t# ax.set_xlim3d(limits[0],limits[1])\n\t\t# ax.set_ylim3d(limits[2],limits[3])\n\t\t# ax.set_zlim3d(limits[4],limits[5])\n\tfrom mpl_toolkits.mplot3d import axes3d\n\t#Taken from here: https://stackoverflow.com/a/4687582/6057617\n", "def plot_implicit(fn, limits):\n\t\t''' create a plot of an implicit function\n\t\tfn  ...implicit function (plot where fn==0)\n\t\tbbox ..the x,y,and z limits of plotted interval'''\n\t\t# xmin, xmax, ymin, ymax, zmin, zmax = bbox*3\n\t\tfig = plt.figure()\n\t\tax = fig.add_subplot(111, projection='3d')\n\t\tA = np.linspace(limits[0],limits[1], 100) # resolution of the contour\n\t\tB = np.linspace(limits[0],limits[1], 15) # number of slices\n\t\tA1,A2 = np.meshgrid(A,A) # grid on which the contour is plotted\n", "\tfor z in B: # plot contours in the XY plane\n\t\t\tX,Y = A1,A2\n\t\t\tZ = fn(X,Y,z)\n\t\t\tcset = ax.contour(X, Y, Z+z, [z], zdir='z')\n\t\t\t# [z] defines the only level to plot for this contour for this value of z\n\t\tfor y in B: # plot contours in the XZ plane\n\t\t\tX,Z = A1,A2\n\t\t\tY = fn(X,y,Z)\n\t\t\tcset = ax.contour(X, Y+y, Z, [y], zdir='y')\n\t\tfor x in B: # plot contours in the YZ plane\n", "\t\tY,Z = A1,A2\n\t\t\tX = fn(x,Y,Z)\n\t\t\tcset = ax.contour(X+x, Y, Z, [x], zdir='x')\n\t\t# must set plot limits because the contour will likely extend\n\t\t# way beyond the displayed level.  Otherwise matplotlib extends the plot limits\n\t\t# to encompass all values in the contour.\n\t\tax.set_xlim3d(limits[0],limits[1])\n\t\tax.set_ylim3d(limits[2],limits[3])\n\t\tax.set_zlim3d(limits[4],limits[5])\n\t\tplt.show()\n", "def getVertexesRaysFromAb(A, b):\n\t\tbmA= np.concatenate([b, -A], axis=1) # See https://pycddlib.readthedocs.io/en/latest/matrix.html\n\t\tbmA_cdd = cdd.Matrix(bmA.tolist(), number_type='float')\n\t\tbmA_cdd.rep_type = cdd.RepType.INEQUALITY\n\t\tpoly = cdd.Polyhedron(bmA_cdd)\n\t\tgen = poly.get_generators()\n\t\t# print(gen)\n\t\tvertices, rays = getVertexesRaysFromGenerators(gen)\n\t\treturn vertices, rays \n\tdef getVertexesRaysFromGenerators(gen):\n", "\tgenerators=list(gen)\n\t\tvertices=np.array([[],[]]);\n\t\trays=np.array([[],[]]);\n\t\tfor i in range(len(generators)):\n\t\t\tgen_i=generators[i];\n\t\t\ttmp=np.asarray(gen_i[1:]).reshape((-1,1));\n\t\t\tif(gen_i[0]==1):\n\t\t\t\tvertices=np.append(vertices,tmp, axis=1)\n\t\t\telse: #it is zero\n\t\t\t\trays=np.append(rays,tmp, axis=1)\n", "\treturn vertices, rays\n\tdef uniformSampleInUnitSphere(dim):\n\t\t#Method 19 of http://extremelearning.com.au/how-to-generate-uniformly-random-points-on-n-spheres-and-n-balls/\n\t\tu = np.random.normal(loc=0.0, scale=1.0, size=(dim,1))\n\t\tu_normalized= u / np.linalg.norm(u)\n\t\treturn u_normalized\n\t# https://stackoverflow.com/a/69427715/6057617\n\t# def move_sympyplot_to_axes(p, ax):\n\t#     backend = p.backend(p)\n\t#     backend.ax = ax\n", "#     backend._process_series(backend.parent._series, ax, backend.parent)\n\t#     backend.ax.spines['right'].set_color('none')\n\t#     backend.ax.spines['top'].set_color('none')\n\t#     backend.ax.spines['bottom'].set_position('zero')\n\t#     plt.close(backend.fig)\n\tdef plot2DPolyhedron(A, b, ax):\n\t\t##FIRST WAY\n\t\t# npoints=300\n\t\t# d = np.linspace(-2,16,npoints)\n\t\t# x1,x2 = np.meshgrid(d,d)\n", "\t# tmp=1;\n\t\t# for i in range(A.shape[0]):\n\t\t# \ttmp=tmp & (A[i,0]*x1 + A[i,1]*x2 <=b[i,0]);\n\t\t# plt.imshow( tmp.astype(int), extent=(x1.min(),x1.max(),x2.min(),x2.max()),origin=\"lower\", cmap=\"Greys\", alpha = 0.3);\n\t\t# #second way, right now it only works if there are no rays\n\t\t# from scipy.spatial import ConvexHull \n\t\t# vertices, rays = getVertexesRaysFromAb(A, b)\n\t\t# #See example from https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.html\n\t\t# coord = vertices.T\n\t\t# hull = ConvexHull(coord)\n", "\t# for simplex in hull.simplices:\n\t\t# \tax.plot(coord[simplex, 0], coord[simplex, 1], 'r-')\n\t\t#third way\n\t\tnpoints=300\n\t\td = np.linspace(-2,16,npoints)\n\t\tx1,x2 = np.meshgrid(d,d)\n\t\ttmp=1;\n\t\tfor i in range(A.shape[0]):\n\t\t\ttmp=tmp & (A[i,0]*x1 + A[i,1]*x2 <=b[i,0]);\n\t\tplt.imshow( tmp.astype(int), extent=(x1.min(),x1.max(),x2.min(),x2.max()),origin=\"lower\", cmap=\"Greys\", alpha = 0.3);\n"]}
{"filename": "examples/early_stopping.py", "chunked_list": ["#Code taken from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n\t#Some minor modifications by jtorde@mit.edu\n\timport numpy as np\n\timport torch\n\timport uuid\n\timport os\n\tclass EarlyStopping:\n\t    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\t    def __init__(self, patience=20, verbose=False, delta=0, trace_func=print):\n\t        \"\"\"\n", "        Args:\n\t            patience (int): How long to wait after last time validation loss improved.\n\t                            Default: 7\n\t            verbose (bool): If True, prints a message for each validation loss improvement. \n\t                            Default: False\n\t            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n\t                            Default: 0\n\t            path (str): Path for the checkpoint to be saved to.\n\t                            Default: 'checkpoint.pt'\n\t            trace_func (function): trace print function.\n", "                            Default: print            \n\t        \"\"\"\n\t        self.patience = patience\n\t        self.verbose = verbose\n\t        self.counter = 0\n\t        self.best_score = None\n\t        self.early_stop = False\n\t        self.val_loss_min = np.Inf\n\t        self.delta = delta\n\t        folder=\"./early_stopping_checkpoints/\"\n", "        os.system(\"mkdir -p \"+folder)\n\t        self.path = folder+\"checkpoint_\"+uuid.uuid4().hex+\".pt\" #https://stackoverflow.com/a/62277811\n\t        self.trace_func = trace_func\n\t    def __call__(self, val_loss, model):\n\t        score = -val_loss\n\t        if self.best_score is None:\n\t            self.best_score = score\n\t            self.save_checkpoint(val_loss, model)\n\t        elif score < self.best_score + self.delta:\n\t            self.counter += 1\n", "            if self.verbose:\n\t                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n\t            if self.counter >= self.patience:\n\t                self.early_stop = True\n\t        else:\n\t            self.best_score = score\n\t            self.save_checkpoint(val_loss, model)\n\t            self.counter = 0\n\t    def save_checkpoint(self, val_loss, model):\n\t        '''Saves model when validation loss decrease.'''\n", "        if self.verbose:\n\t            # self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n\t            self.trace_func(f'Best validation loss so far: {self.val_loss_min:.6f}.  Saving model ...')\n\t        torch.save(model.state_dict(), self.path)\n\t        self.val_loss_min = val_loss\n\t    #Added by jtorde\n\t    def load_best_model(self, model):\n\t        model.load_state_dict(torch.load(self.path))\n\t    def __del__(self):\n\t        os.system(\"rm \"+self.path)\n", "      # body of destructor\n"]}
{"filename": "examples/test_layer.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport math\n\timport matplotlib.pyplot as plt\n\timport mpl_toolkits.mplot3d as a3\n", "import matplotlib.colors as colors\n\timport numpy as np\n\timport scipy\n\timport os\n\timport time\n\tfrom examples_sets import getExample\n\timport utils_examples\n\timport fixpath #Following this example: https://github.com/tartley/colorama/blob/master/demos/demo01.py\n\tfrom rayen import constraints, constraint_module, utils\n\tmethods=['RAYEN_old', 'RAYEN', 'Bar', 'UU', 'PP', 'UP', 'DC3']\n", "index_examples_to_run=list(range(15))\n\t###############\n\tnum_of_examples=len(index_examples_to_run)\n\trows=math.ceil(math.sqrt(num_of_examples))\n\tcols=rows\n\tfor method in methods:\n\t\tutils.printInBoldRed(f\"==================== METHOD: {method} ==========================\")\n\t\tfig = plt.figure()\n\t\tfig.suptitle(method, fontsize=10)\n\t\tfor i in range(num_of_examples):\n", "\t\tindex_example=index_examples_to_run[i]\n\t\t\tutils.printInBoldGreen(f\"==================== Example: {index_example} \")\n\t\t\tconstraint=getExample(index_example)\n\t\t\tif(method=='Bar' and (constraint.has_quadratic_constraints or constraint.has_soc_constraints or constraint.has_lmi_constraints)):\n\t\t\t\tcontinue\n\t\t\tif(method=='DC3' and (constraint.has_soc_constraints or constraint.has_lmi_constraints)):\n\t\t\t\tcontinue\n\t\t\t# fig = plt.figure()\n\t\t\tif(constraint.k==3):\n\t\t\t\tax = fig.add_subplot(rows,cols,i+1, projection=\"3d\")\n", "\t\t\tif(constraint.has_linear_ineq_constraints):\n\t\t\t\t\tutils_examples.plot3DPolytopeHRepresentation(constraint.lc.A1,constraint.lc.b1,[-1, 2, -1, 2, -1, 2], ax)\n\t\t\telse:\n\t\t\t\tax = fig.add_subplot(rows,cols,i+1) \n\t\t\tif(method=='DC3'):\n\t\t\t\targs_DC3={}\n\t\t\t\targs_DC3['lr'] = 1e-4\n\t\t\t\targs_DC3['eps_converge'] = 1e-4\n\t\t\t\targs_DC3['momentum'] = 0.5\n\t\t\t\targs_DC3['max_steps_training'] = 10\n", "\t\t\targs_DC3['max_steps_testing'] = 50000 #float(\"inf\")\n\t\t\telse:\n\t\t\t\targs_DC3 = None\n\t\t\tmy_layer=constraint_module.ConstraintModule(constraint, method=method, create_map=False, args_DC3=args_DC3)\n\t\t\tnumel_output_mapper=my_layer.getDimAfterMap()\n\t\t\tnum_samples=500 #12000\n\t\t\tx_batched=torch.Tensor(num_samples, numel_output_mapper, 1).uniform_(-5.0, 5.0)\n\t\t\t# mapper=nn.Sequential(nn.Linear(x_batched.shape[1], numel_output_mapper))\n\t\t\t# mapper=nn.Sequential() #do nothing.\n\t\t\t# my_layer.setMapper(mapper)\n", "\t\tmy_layer.eval() #This changes the self.training variable of the module\n\t\t\ttime_start=time.time()\n\t\t\tresult=my_layer(x_batched)\n\t\t\ttotal_time_per_sample= (time.time()-time_start)/num_samples\n\t\t\tresult=result.detach().numpy();\n\t\t\ty0=my_layer.gety0();\n\t\t\tif(constraint.k==3):\n\t\t\t\tax.scatter(y0[0,0], y0[1,0], y0[2,0],color='r',s=500)\n\t\t\t\tax.scatter(result[:,0,0], result[:,1,0], result[:,2,0])\n\t\t\tif(constraint.k==2):\n", "\t\t\tax.scatter(result[:,0,0], result[:,1,0])\n\t\t\t\tutils_examples.plot2DPolyhedron(constraint.lc.A1,constraint.lc.b1,ax)\n\t\t\t\tax.scatter(y0[0,0], y0[1,0])\n\t\t\t\t# utils_examples.plot2DEllipsoidB(my_layer.B.numpy(),my_layer.z0.numpy(),ax)\n\t\t\t\tax.set_aspect('equal')\n\t\t\t\tax.set_xlim(-0.5,8)\n\t\t\t\tax.set_ylim(-0.5,8)\n\t\t\tif method=='RAYEN':\n\t\t\t\tmy_dict=constraint.getDataAsDict();\n\t\t\t\tmy_dict[\"result\"]=result\n", "\t\t\tmy_dict[\"total_time_per_sample\"]=total_time_per_sample\n\t\t\t\tdirectory='./examples_mat'\n\t\t\t\tif not os.path.exists(directory):\n\t\t\t\t\tos.makedirs(directory)\n\t\t\t\tscipy.io.savemat(directory+'/example_'+str(index_example)+'.mat', my_dict)\n\t\t\tutils.printInBoldBlue(f\"Example {index_example}, total_time_per_sample={total_time_per_sample}\")\n\tplt.show()\n\t######OLD\n\t##This samples different angles\n\t# all_angles = np.arange(0,2*math.pi, 0.01)\n", "# x_batched=torch.empty(len(all_angles), numel_output_mapper, 1)\n\t# for i in range(x_batched.shape[0]): #for each element of the batch\n\t# \ttheta=all_angles[i]\n\t# \tif(my_layer.dim==2):\n\t# \t\ttmp=torch.Tensor(np.array([[math.cos(theta)],[math.sin(theta)],[3000]])); #Assumming my_layer.dim==2 here\n\t# \telse:\n\t# \t\traise(\"Not implemented yet\")\n\t# \ttmp=torch.unsqueeze(tmp, dim=0)\n\t# \tprint(f\"tmp.shape={tmp.shape}\")\n\t# \tx_batched[i,:,:]=tmp\n", "# E=1.7*np.eye(constraint.k)\n\t# ellipsoid=utils.Ellipsoid(E=E, c=np.zeros((constraint.k,1)))\n\t# cqc_list=[ellipsoid.convertToQuadraticConstraint()]\n\t# print(f\"P={cqc_list[0].P}\")\n\t# print(f\"q={cqc_list[0].q}\")\n\t# print(f\"r={cqc_list[0].r}\")\n\t# if(method=='RAYEN'):\n\t# \tnum_directions=500; #for each direction you have several samples\n\t# \tx_batched=torch.empty(0, numel_output_mapper, 1)\n\t# \tfor i in range(num_directions): #for each direction\n", "# \t\tdirection=utils.uniformSampleInUnitSphere(my_layer.dim)\n\t# \t\tfor scalar in list(np.linspace(-8.0, 8.0, num=100)):\n\t# \t\t\tscalar_np=np.array([[scalar]])\n\t# \t\t\tdirection_and_scalar=np.concatenate((direction,scalar_np), axis=0);\n\t# \t\t\ttmp=torch.Tensor(direction_and_scalar)\n\t# \t\t\ttmp=torch.unsqueeze(tmp, dim=0)\n\t# \t\t\t# print(f\"direction_and_scalar={direction_and_scalar}\")\n\t# \t\t\tx_batched=torch.cat((x_batched, tmp), axis=0)\n\t# if(method=='Bar' or method=='PP' or method=='UP'):\n\t# \tx_batched=torch.empty(0, numel_output_mapper, 1)\n", "# \tfor i in range(1000):\n\t# \t\t# sample_lambda = utils.runif_in_simplex(my_layer.num_vertices);\n\t# \t\t# sample_mu = np.random.uniform(0.0,2.5,my_layer.num_rays);\n\t# \t\t# sample=np.concatenate((sample_lambda, sample_mu));\n\t# \t\tsample=np.random.uniform(-5,5,numel_output_mapper);\n\t# \t\tsample=torch.Tensor(np.expand_dims(sample, axis=1))\n\t# \t\tsample=torch.unsqueeze(sample, dim=0) \n\t# \t\tx_batched=torch.cat((x_batched, sample), axis=0) \n"]}
{"filename": "examples/fixpath.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\t#https://github.com/tartley/colorama/blob/master/demos/fixpath.py\n\timport sys\n\tfrom os.path import normpath, dirname, join\n\tsys.path.insert(0, normpath(join(dirname(__file__), '..')))"]}
{"filename": "examples/first_figure.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport math\n\timport matplotlib.pyplot as plt\n\timport mpl_toolkits.mplot3d as a3\n", "import matplotlib.colors as colors\n\timport numpy as np\n\timport scipy\n\timport os\n\timport time\n\tfrom examples_sets import getExample\n\timport utils_examples\n\timport fixpath #Following this example: https://github.com/tartley/colorama/blob/master/demos/demo01.py\n\tfrom rayen import constraints, constraint_module, utils\n\tmethod='RAYEN'\n", "index_example=13\n\tfig = plt.figure()\n\tfig.suptitle(method, fontsize=10)\n\ttmp=getExample(index_example)\n\tconstraint=constraints.ConvexConstraints(lc=tmp.lc, qcs=tmp.qcs, socs=tmp.socs, lmic=tmp.lmic, y0=np.array([[0.5], [0.0], [0.8]]))\n\tax = fig.add_subplot(1,1,1, projection=\"3d\")\n\tmy_layer=constraint_module.ConstraintModule(constraint, method=method, create_map=False)\n\tnum_samples=500 #12000\n\tv_batched_x=torch.Tensor(num_samples, 1, 1).uniform_(-0.5, 0.6)\n\tv_batched_y=torch.Tensor(num_samples, 1, 1).uniform_(-0.55, 0.47)\n", "v_batched_z=torch.Tensor(num_samples, 1, 1).uniform_(-0.4, 0.3)\n\tv_batched = torch.cat((v_batched_x, v_batched_y, v_batched_z),1)\n\tmy_layer.eval() #This changes the self.training variable of the module\n\ttime_start=time.time()\n\tresult=my_layer(v_batched)\n\ttotal_time_per_sample= (time.time()-time_start)/num_samples\n\tresult=result.detach().numpy();\n\ty0=my_layer.gety0();\n\tax.scatter(y0[0,0], y0[1,0], y0[2,0],color='r',s=500)\n\tax.scatter(result[:,0,0], result[:,1,0], result[:,2,0])\n", "my_dict=constraint.getDataAsDict();\n\tmy_dict[\"result\"]=result\n\tmy_dict[\"total_time_per_sample\"]=total_time_per_sample\n\tmy_dict[\"y0\"]=constraint.y0\n\tmy_dict[\"v\"]=v_batched.detach().numpy()\n\tdirectory='./first_figure'\n\tif not os.path.exists(directory):\n\t\tos.makedirs(directory)\n\tscipy.io.savemat(directory+'/first_figure.mat', my_dict)\n\tutils.printInBoldBlue(f\"Example {index_example}, total_time_per_sample={total_time_per_sample}\")\n", "plt.show()"]}
{"filename": "examples/scripts/time_analysis.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport numpy as np\n\timport torch\n\timport time\n\tfrom joblib import Parallel, delayed\n\timport pandas as pd\n\timport sys\n", "import os\n\tsys.path.insert(1, os.path.join(sys.path[0], '..'))  #See first comment of this answer: https://stackoverflow.com/a/11158224\n\tfrom examples_sets import getExample\n\timport fixpath #Following this example: https://github.com/tartley/colorama/blob/master/demos/demo01.py\n\tfrom rayen import constraints, constraint_module, utils\n\tpath=\"./results/\"\n\tif not os.path.exists(path):\n\t   os.makedirs(path)\n\ttorch.set_default_dtype(torch.float64)\n\tdef getTime_sMethod(cs, num_samples):\n", "\tprint(\"Calling constructor for the layer...\")\n\t\tmy_layer=constraint_module.ConstraintModule(cs, method=\"RAYEN\", create_map=False)\n\t\tprint(\"Called\")\n\t\tnumel_output_mapper=my_layer.getDimAfterMap()\n\t\tx_batched=torch.Tensor(num_samples, numel_output_mapper, 1).uniform_(-1.0, 1.0)\n\t\tx_batched.to(torch.device('cuda:0'))\n\t\tprint(\"Calling method...\")\n\t\tcuda_timer=utils.CudaTimer()\n\t\tcuda_timer.start()\n\t\tresult=my_layer(x_batched)\n", "\ttotal_s_per_sample= cuda_timer.endAndGetTimeSeconds()/num_samples\n\t\tprint(\"Called\")\n\t\tprint(f\"total_s_per_sample={total_s_per_sample} s\")\n\t\treturn total_s_per_sample\n\t# all_k = [1, 10, 100, 1000, 10000]\n\t############################################ WARM UP THE GPU (for more accurate computation time)\n\tmy_layer=constraint_module.ConstraintModule(getExample(12), method=\"RAYEN\", create_map=False)\n\tx_dummy=torch.Tensor(300, my_layer.getDimAfterMap(), 1).uniform_(-5.0, 5.0)\n\t_ = my_layer(x_dummy)\n\t#######################################################\n", "num_samples=2000\n\t# ########################################### Linear CONSTRAINTS\n\ttimes_lin = []\n\tall_k = [1, 10, 100, 1000, 2000, 3000, 4000, 5000, 10000]\n\tall_r_A1 = [1, 10, 100, 500, 1000, 2000, 3000] \n\tfor r_A1 in all_r_A1:\n\t\tfor k in all_k:\n\t\t\tutils.printInBoldRed(f\"r_A1 = {r_A1}, k={k}\")\n\t\t\tA1=np.random.uniform(low=-1.0, high=1.0, size=(r_A1,k))\n\t\t\tb1=np.random.uniform(low= 0.1, high=1.0, size=(r_A1,1)) #In this way, y=0 is always a feasible solution\n", "\t\tlc=constraints.LinearConstraint(A1=A1, b1=b1, A2=None, b2=None);\n\t\t\tcs=constraints.ConvexConstraints(lc=lc, qcs=[], socs=[], lmic=None, y0=np.zeros((k,1)), do_preprocessing_linear=False)\n\t\t\ttotal_s=getTime_sMethod(cs, num_samples)\n\t\t\ttimes_lin.append({'k': k,'r_A1': r_A1,'Time': total_s})\n\ttimes_lin=pd.DataFrame(times_lin)\n\t# utils.savepickle(times_lin, path+\"times_lin.pkl\")\n\ttimes_lin.to_csv(path+\"times_lin.csv\",index=False)\n\t# ########################################### QP CONSTRAINTS\n\ttimes_qp = []\n\tall_eta = [1, 10, 50, 100, 500, 1000] #np.power(10, np.array([0,1,2,3]))\n", "all_k = [1, 10, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n\tfor eta in all_eta:\n\t\tfor k in all_k:\n\t\t\tutils.printInBoldRed(f\"QP constraints, eta = {eta}, k={k}\")\n\t\t\tqcs=[]\n\t\t\tdef createRandomQPConstraint(i):\n\t\t\t\ttmp=np.random.uniform(low=-1.0, high=1.0, size=(k,k))\n\t\t\t\t# tmp=(torch.empty((k,k), device=\"cuda:0\").uniform_(-1.0, 1.0)).cpu().numpy() #\n\t\t\t\tP=tmp@tmp.T #To make sure it's a (symmetric) PSD matrix\n\t\t\t\tq=np.random.uniform(low=-1.0, high=1.0, size=(k,1))\n", "\t\t\tr=np.random.uniform(low=-1.0, high=0.0, size=(1,1))\n\t\t\t\tqc=constraints.ConvexQuadraticConstraint(P=P, q=q, r=r, do_checks_P=False);\n\t\t\t\treturn qc\n\t\t\tprint(\"Creating random constraints\")\n\t\t\tqcs = Parallel(n_jobs=15)(delayed(createRandomQPConstraint)(i) for i in range(eta))\n\t\t\tprint(\"Created\")\n\t\t\tassert len(qcs)==eta\n\t\t\tcs=constraints.ConvexConstraints(lc=None, qcs=qcs, socs=[], lmic=None, y0=np.zeros((k,1)))\n\t\t\ttotal_s=getTime_sMethod(cs, num_samples)\n\t\t\ttimes_qp.append({'k': k,'eta': eta,'Time': total_s})\n", "times_qp=pd.DataFrame(times_qp)\n\t# utils.savepickle(times_qp, path+\"times_qp.pkl\")\n\ttimes_qp.to_csv(path+\"times_qp.csv\",index=False)\n\t########################################### SOC CONSTRAINTS\n\ttimes_soc = []\n\tall_r_M = [10, 100, 200, 300]\n\tall_mu = [10, 100, 300, 500]\n\tall_k = [10, 100, 500, 1000] \n\tfor r_M in all_r_M:\n\t\tfor mu in all_mu:\n", "\t\tfor k in all_k:\n\t\t\t\tutils.printInBoldRed(f\"r_M = {r_M}, mu={mu}, k={k}\")\n\t\t\t\tdef createRandomSOCConstraint(i):\n\t\t\t\t\tM = np.random.uniform(low=-1.0, high=1.0, size=(r_M,k))\n\t\t\t\t\ts = np.random.uniform(low=-1.0, high=1.0, size=(r_M,1))\n\t\t\t\t\tc = np.random.uniform(low=-1.0, high=1.0, size=(k,1))\n\t\t\t\t\td = np.linalg.norm(s)+np.array([[0.5]]) #This ensures that 0 will always be a point in the interior of the feasible set\n\t\t\t\t\treturn M, s, c, d\n\t\t\t\tprint(\"Creating random constraints\")\n\t\t\t\tall_Mscd = Parallel(n_jobs=15)(delayed(createRandomSOCConstraint)(i) for i in range(k))\n", "\t\t\tprint(\"Created\")\n\t\t\t\tsocs=[]\n\t\t\t\tfor Mscd in all_Mscd:\n\t\t\t\t\tsocs.append(constraints.SOCConstraint(Mscd[0], Mscd[1], Mscd[2], Mscd[3]))\n\t\t\t\tprint(\"Creating cs\")\n\t\t\t\tcs=constraints.ConvexConstraints(lc=None, qcs=[], socs=socs, lmic=None, y0=np.zeros((k,1)))\n\t\t\t\tprint(\"Created\")\n\t\t\t\ttotal_s=getTime_sMethod(cs, num_samples)\n\t\t\t\ttimes_soc.append({'k': k,'r_M': r_M,'mu': mu,'Time': total_s})\n\ttimes_soc=pd.DataFrame(times_soc)\n", "# utils.savepickle(times_soc, path+\"times_soc.pkl\")\n\ttimes_soc.to_csv(path+\"times_soc.csv\",index=False)\n\tprint(times_soc)\n\t########################################### LMI CONSTRAINTS\n\ttimes_lmi = []\n\tall_r_F = [10, 100, 200, 300]\n\tall_k = [100, 500, 1000, 2000, 5000, 7000, 10000]\n\tfor r_F in all_r_F:\n\t\tfor k in all_k:\n\t\t\tutils.printInBoldRed(f\"r_F = {r_F}, k={k}\")\n", "\t\tdef createRandomSymmetricMatrix(i):\n\t\t\t\ttmp=np.random.uniform(low=-1.0, high=1.0, size=(r_F,r_F))\n\t\t\t\treturn (tmp + tmp.T)/2 #To ensure that it is symmetric\n\t\t\tprint(\"Creating random constraints\")\n\t\t\tall_F = Parallel(n_jobs=15)(delayed(createRandomSymmetricMatrix)(i) for i in range(k))\n\t\t\ttmp = np.random.uniform(-1, 1, (r_F, r_F))\n\t\t\tF_k = np.dot(tmp, tmp.transpose()) + 0.5*np.eye(r_F) #F_k is symmetric positive definite by construction\n\t\t\tall_F.append(F_k)\n\t\t\tprint(\"Created\")\n\t\t\tprint(\"Creating lmic\")\n", "\t\tlmic=constraints.LMIConstraint(all_F)\n\t\t\tprint(\"Created lmic\")\n\t\t\tprint(\"Creating cs\")\n\t\t\tcs=constraints.ConvexConstraints(lc=None, qcs=[], socs=[], lmic=lmic, y0=np.zeros((k,1)))\n\t\t\tprint(\"Created\")\n\t\t\ttotal_s=getTime_sMethod(cs, num_samples)\n\t\t\ttimes_lmi.append({'k': k,'r_F': r_F,'Time': total_s})\n\ttimes_lmi=pd.DataFrame(times_lmi)\n\t# utils.savepickle(times_lmi, path+\"times_lmi.pkl\")\n\ttimes_lmi.to_csv(path+\"times_lmi.csv\",index=False)\n", "print(times_lmi)"]}
{"filename": "examples/scripts/merge_all_results.py", "chunked_list": ["# --------------------------------------------------------------------------\n\t# Jesus Tordesillas Torres, Robotic Systems Lab, ETH Zürich \n\t# See LICENSE file for the license information\n\t# -------------------------------------------------------------------------- \n\timport glob, os\n\timport pandas as pd\n\tos.chdir(\"./results\")\n\tall_pkl=[]\n\tfor file in glob.glob(\"dataset*.pkl\"):\n\t    all_pkl.append(file)\n", "all_pkl = sorted(all_pkl)\n\tall_dataframes=[]\n\tfor file in all_pkl:\n\t    all_dataframes.append(pd.read_pickle(file))\n\tdf = pd.concat(all_dataframes)\n\tdf=df.drop_duplicates(subset=['method'])\n\t# print(df)\n\tresults2d = df[df['method'].str.startswith('dataset2d')]\n\tresults3d = df[df['method'].str.startswith('dataset3d')]\n\tresults2d=results2d.set_index('method')\n", "results3d=results3d.set_index('method')\n\tprint(results2d)\n\tprint(results3d)\n\tresults2d[\"[In dist] loss\"]/=(results2d.loc[\"dataset2d_Optimization\"].at[\"[In dist] loss\"]) #Normalize the cost\n\tresults2d[\"[Out dist] loss\"]/=(results2d.loc[\"dataset2d_Optimization\"].at[\"[Out dist] loss\"]) #Normalize the cost\n\tresults3d[\"[In dist] loss\"]/=(results3d.loc[\"dataset3d_Optimization\"].at[\"[In dist] loss\"]) #Normalize the cost\n\tresults3d[\"[Out dist] loss\"]/=(results3d.loc[\"dataset3d_Optimization\"].at[\"[Out dist] loss\"]) #Normalize the cost\n\tdf = pd.concat([results2d, results3d])\n\tdf=df.rename(columns={\"[In dist] loss\": \"[In dist] n.loss\", \"[Out dist] loss\": \"[Out dist] n.loss\"})\n\tprint(df)\n", "# results2d['[In dist] loss']=results2d['[In dist] loss']/results2d[ results2d['method']=='dataset2d_Optimization'  ]\n\tdf.to_csv('./merged.csv')  \n"]}
{"filename": "examples/other/test_bug_logpcg.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport scipy\n\t# def using_power_iteration(C):\n\t#     b_k = np.random.rand(C.shape[0],1)\n\t#     tol = 1e-6\n\t#     max_iter = 100\n\t#     for i in range(max_iter):\n\t#         b_k1 = C @ b_k\n\t#         b_k1 = b_k1 / np.linalg.norm(b_k1)\n", "#         lam=(b_k1.T @ b_k)/(b_k.T @ b_k)\n\t#         print(lam)\n\t#         # x = C @ x / np.linalg.norm(C @ x)\n\t#         # lam = (x.T @ C @ x) / (x.T @ x)\n\t#         # # print(lam)\n\t#         # if np.abs(lam - lam_prev) < tol:\n\t#         #     break\n\t#         # lam_prev = lam\n\t#     return lam\n\tdef eigenvalue(A, v):\n", "\tAv = A.dot(v)\n\t\treturn v.dot(Av)\n\tdef power_iteration(A):\n\t\tn, d = A.shape\n\t\tv = np.ones(d) / np.sqrt(d)\n\t\tev = eigenvalue(A, v)\n\t\twhile True:\n\t\t\tAv = A.dot(v)\n\t\t\tv_new = Av / np.linalg.norm(Av)\n\t\t\tev_new = eigenvalue(A, v_new) #v_new.dot(A.dot(v_new)) #\n", "\t\tif np.abs(ev - ev_new) < 1e-12:\n\t\t\t\tbreak\n\t\t\tv = v_new\n\t\t\tev = ev_new\n\t\treturn ev_new #, v_new\n\tdim=3\n\twhile True:\n\t\tprint(\"\\n\\n-------\")\n\t\t#Random generation of matrices A and B\n\t\ttmp = np.random.uniform(-1, 1, (dim, dim))\n", "\tA=(tmp+tmp.T)/2.0  #A is symmetric by construction\n\t\ttmp = np.random.uniform(-1, 1, (dim, dim))\n\t\tB = np.dot(tmp, tmp.transpose()) + np.eye(dim) #B is symmetric positive definite by construction\n\t\tprint(f\"A={A}\\n\")\n\t\tprint(f\"B={B}\\n\")\n\t\tX=np.random.rand(A.shape[0], 1) #Initial guess for the eigenvector\n\t\t#### USING lobpcg\n\t\tlambda_lobpcg, _ = torch.lobpcg(A=torch.from_numpy(A), k=1, B=torch.from_numpy(B), niter=-1, X=torch.from_numpy(X), tol=1e-12)\n\t\tlambda_lobpcg = lambda_lobpcg.item()\n\t\tprint(f\"lambda_lobpcg={lambda_lobpcg}\")\n", "\t# #### USING Scipy\n\t\tlambda_scipy, _ =scipy.sparse.linalg.lobpcg(A=A, B=B, X=X, maxiter=10000)\n\t\tlambda_scipy=lambda_scipy[0]\n\t\tprint(f\"lambda_scipy={lambda_scipy}\")\n\t\t#### USING normal eigendecomposition\n\t\tall_lambdas, _=np.linalg.eig(np.linalg.inv(B)@A); \n\t\t# print(f\"all_lambdas={all_lambdas}\")\n\t\tlambda_eig=np.max(all_lambdas)\n\t\tprint(f\"lambda_eig={lambda_eig}\")\n\t\t#### USING Power iteration\n", "\tlambda_power_iter=power_iteration(np.linalg.inv(B)@A)\n\t\tprint(f\"lambda_power_iter={lambda_power_iter}\")\n\t\tassert abs(lambda_lobpcg-lambda_eig)<1e-6\n\t\tassert abs(lambda_scipy-lambda_eig)<1e-6\n\t\tassert abs(lambda_power_iter-lambda_eig)<1e-6\n"]}
{"filename": "examples/other/testing_sdp.py", "chunked_list": ["# Import packages.\n\timport cvxpy as cp\n\timport numpy as np\n\tfrom numpy import linalg as LA\n\timport utils\n\timport torch\n\timport scipy\n\tA=torch.Tensor([\n\t                [[2.0, -12.0],\n\t                 [1.0, -5.0]],\n", "                [[-7.0, 0.0],\n\t                 [0.0, 5.0]],\n\t                [[-2.0, 0.0],\n\t                 [0.0, 6.0]]\n\t                ])\n\tguess_v = torch.nn.functional.normalize(torch.rand(A.shape[1],1), dim=0)\n\tlamb = utils.findLargestEigenvalue(A, guess_v)\n\tL, V = torch.linalg.eig(A)\n\tprint(L)\n\tprint(f\"Found lambda={lamb}\")\n", "exit()\n\tdim=3\n\tfor trial in range(200):\n\t    # Generate a random SDP.\n\t    tmp = np.random.rand(dim, dim)\n\t    # tmp = np.random.randint(0, 20, size=(dim, dim))\n\t    H = np.dot(tmp, tmp.transpose()) + np.eye(dim) #H is psd by construction\n\t    tmp = np.random.rand(dim, dim)\n\t    # tmp = np.random.randint(0, 20, size=(dim, dim))\n\t    S=(tmp+tmp.T)/2.0  #M is symmetric by construction\n", "    # tmp = np.random.rand(dim, dim)\n\t    # M = np.dot(tmp, tmp.transpose()) #H is psd by construction\n\t    Hinv=np.linalg.inv(H)\n\t    print(\"\\n\\n-------\")\n\t    print(\"-------\")\n\t    print(f\"H={H}\\n\")\n\t    print(f\"S={S}\\n\")\n\t    # kappa_opt = cp.Variable()\n\t    # constraints = [(kappa_opt*H+S) >> 0, kappa_opt>=0]\n\t    # prob = cp.Problem(cp.Minimize(kappa_opt), constraints)\n", "    # prob.solve(solver=cp.SCS, verbose=True, eps=1e-7)\n\t    # if(prob.status=='unbounded'):\n\t    #     utils.printInBoldRed(\"Unbounded!!!!\") #When kappa is the decision variable, there is no way the result is unbounded\n\t    # else:\n\t    #     kappa_opt=kappa_opt.value\n\t    #     utils.printInBoldBlue(f\"kappa_opt={kappa_opt}\")\n\t    X=np.random.rand(S.shape[0], 1)\n\t    #### USING lobpcg\n\t    kappa_lobpcg, _ = torch.lobpcg(A=torch.from_numpy(-S), k=1, B=torch.from_numpy(H), niter=-1, X=torch.from_numpy(X))\n\t    # kappa_lobpcg = torch.relu(kappa_lobpcg).item()\n", "    kappa_lobpcg = kappa_lobpcg.item()\n\t    utils.printInBoldBlue(f\"kappa_lobpcg={kappa_lobpcg}\")\n\t    # #### USING Scipy\n\t    # kappa_scipy, _ =scipy.sparse.linalg.lobpcg(A=-S, B=H, X=X, maxiter=-1)\n\t    # kappa_scipy=kappa_scipy[0]\n\t    # utils.printInBoldBlue(f\"kappa_scipy={kappa_scipy}\")\n\t    #### USING normal eigendecomposition\n\t    all_kappas, _ = np.linalg.eig(-Hinv@S); #Be careful because Hinv@M is NOT symmetric (Hinv and M is)\n\t    print(f\"all_kappas={all_kappas}\")\n\t    # kappa_eig=np.maximum(np.max(all_kappas),0.0)\n", "    kappa_eig=np.max(all_kappas)\n\t    utils.printInBoldBlue(f\"kappa_eig={kappa_eig}\")\n\t    tol=1e-6\n\t    assert abs(kappa_lobpcg-kappa_eig)<tol\n\t    # assert abs(kappa_scipy-kappa_eig)<tol\n\t    # assert abs(kappa_opt-kappa_eig)<tol #This one sometimes fails due to inaccuracies in the optimization\n\t# LPBPCG algorithm is not applicable when the number of A rows (=2) is smaller than 3 x the number of requested eigenpairs (=1)\n\t#This should be zero\n\t# print(np.linalg.det(H+lam.value*M))\n\t# print(np.linalg.det(Minv@H+lam.value*np.eye(2)))\n", "# for i in range(all_kappas.shape[0]):\n\t#     tmp=all_kappas[i]\n\t#     # print(np.linalg.det(-Hinv@M-tmp*np.eye(dim)))\n\t#     print(f\"tmp={tmp}\")\n\t#     eigenvals,_=np.linalg.eig(tmp*H+S)\n\t#     print(eigenvals)\n\t# print(\"-------\")\n\t# print(\"Eigen Decomposition of H\")\n\t# wH, vH = LA.eig(H)\n\t# print(f\"Eigenvalues={wH}\")\n", "# print(f\"Eigenvectors=\\n{vH}\")\n\t# print(\"-------\")\n\t# print(\"Eigen Decomposition of M\")\n\t# wM, vM = LA.eig(M)\n\t# print(f\"Eigenvalues={wM}\")\n\t# print(f\"Eigenvectors=\\n{vM}\\n\")\n\t# for i in range(wH.shape[0]):\n\t#     for j in range(wM.shape[0]):\n\t#         print(wH[i]/wM[j])\n\t#         print(wM[j]/wH[i])\n", "#         print(wH[i]*wM[j])\n\t# beta = cp.Variable()\n\t# constraints = [H >> beta*M]\n\t# prob = cp.Problem(cp.Minimize(beta), constraints)\n\t# prob.solve()\n\t# print(f\"beta={beta.value}\")\n\t# Hinv=np.linalg.inv(H)\n\t# constraints = [(np.eye(2)+lam*Hinv@M) >> 0]\n\t# prob = cp.Problem(cp.Maximize(lam), constraints)\n\t# prob.solve()\n", "# print(f\"lam={lam.value}\")\n\t# L = np.linalg.cholesky(H)\n\t# # print(L@L.T-H) #good\n\t# Y=np.linalg.inv(L)@M@(np.linalg.inv(L).T)\n\t# constraints = [(Y+lam*np.eye(2)) >> 0]\n\t# prob = cp.Problem(cp.Maximize(lam), constraints)\n\t# prob.solve()\n\t# print(f\"lam={lam.value}\")\n\t#Inspired by https://github.com/rfeinman/Torch-ARPACK/blob/master/arpack/power_iteration.py\n\t# def power_iteration(A, tol=1e-20, max_iter=10000, eps=1e-12, check_freq=2):\n", "#     v = torch.nn.functional.normalize(torch.rand(A.shape[1],1), dim=0)\n\t#     n_iter = 0\n\t#     converging = torch.ones(A.shape[0], dtype=torch.bool)# [True True True ...]\n\t#     print(converging)\n\t#     while n_iter < max_iter:\n\t#         n_iter += 1\n\t#         if(n_iter==1):\n\t#             u = torch.nn.functional.normalize(A@v, dim=1)\n\t#             v = u\n\t#             continue\n", "#         else:\n\t#             print(f\"converging before={converging}\")\n\t#             u[converging,:,:] = torch.nn.functional.normalize(A[converging,:,:]@v[converging,:,:], dim=1)\n\t#             distance=torch.abs(1 - torch.abs(  torch.transpose(v,1,2)@u ))\n\t#             print(f\"distance={distance}\")\n\t#             v[converging,:,:] = u[converging,:,:]\n\t#             converging = (distance>tol).flatten()\n\t#             print(f\"converging after={converging}\")\n\t#             if (torch.all(converging == False)): #All of them converged\n\t#                 break\n", "#     else:\n\t#         warnings.warn('power iteration did not converge')\n\t#     lamb =  torch.transpose(v,1,2)@A@v #torch.dot(v, torch.mv(A, v))\n\t#     return lamb"]}
{"filename": "examples/other/plot_examples_with_mayavi.py", "chunked_list": ["import numpy as np\n\tfrom mayavi import mlab\n\timport utils\n\timport examples_sets\n\timport random   \n\timport copy \n\tfrom joblib import Parallel, delayed\n\tindex_example=1\n\tcs=examples_sets.getExample(index_example)\n\t# @np.vectorize\n", "def get_conditions(x):\n\t    conditions=[];\n\t    all_is_ineq_condition=[]\n\t    if(cs.has_linear_ineq_constraints):\n\t        for i in range(cs.lc.A1.shape[0]):\n\t            expression=cs.lc.A1[i,:]@x-cs.lc.b1[i,0]\n\t            conditions.append(expression[0])\n\t            all_is_ineq_condition.append(True)\n\t    if(cs.has_linear_eq_constraints):\n\t        for i in range(cs.lc.A2.shape[0]):\n", "            expression=cs.lc.A2[i,:]@x-cs.lc.b2[i,0]\n\t            conditions.append(expression[0])\n\t            all_is_ineq_condition.append(False)\n\t    if(cs.has_quadratic_constraints):\n\t        for qc in cs.qcs:\n\t            expression=0.5*x.T@qc.P@x + qc.q.T@x + qc.r \n\t            conditions.append(expression[0,0])\n\t            all_is_ineq_condition.append(True)\n\t    if(cs.has_soc_constraints):\n\t        for soc in cs.socs:\n", "            expression=np.linalg.norm(soc.M@x +soc.s) -soc.c.T@x  - soc.d \n\t            conditions.append(expression[0,0])\n\t            all_is_ineq_condition.append(True)\n\t    return conditions, all_is_ineq_condition\n\tnum_points=100j\n\tdist_min=0\n\tdist_max=2\n\tall_x, all_y, all_z = np.mgrid[-dist_min:dist_max:num_points, -dist_min:dist_max:num_points, -dist_min:dist_max:num_points]\n\tnum_points=int(num_points.imag)\n\ttmp, all_is_ineq_condition=get_conditions(np.zeros((3,1)))\n", "num_of_conditions=len(tmp)\n\t### Create dummy list of numpy arrays\n\tconditions=[];\n\tfor i in range(num_of_conditions):\n\t    zero_tensor=np.zeros((num_points,num_points,num_points))\n\t    conditions.append(zero_tensor)\n\tdef my_function(i, j, k, conditions):\n\t    print((i,j,k))\n\t    x=np.array([[all_x[i,j,k]],[all_y[i,j,k]],[all_z[i,j,k]]]);\n\t    tmp,_=get_conditions(x)\n", "    return tmp\n\tresult=Parallel(n_jobs=40)(delayed(my_function)(i, j, k, conditions) for i in range(all_x.shape[0]) for j in range(all_x.shape[1]) for k in range(all_x.shape[2]))\n\tprint(len(result))\n\tprint(len(result[0]))\n\tindex=0;\n\tfor i in range(all_x.shape[0]):\n\t    for j in range(all_x.shape[1]):\n\t        for k in range(all_x.shape[2]):\n\t            for index_cond in range(num_of_conditions):#For each condition\n\t                conditions[index_cond][i,j,k]=result[index][index_cond]\n", "            index+=1\n\tconditions_processed=[]\n\tfor i in range(len(conditions)):\n\t    cond_i=copy.deepcopy(conditions[i])\n\t    for j in range(len(conditions)):\n\t        if (i==j):\n\t            continue\n\t        cond_j=conditions[j]\n\t        cond_j_is_ineq=all_is_ineq_condition[j]\n\t        if(cond_j_is_ineq):\n", "            cond_i[cond_j>0.0]=None #See https://stackoverflow.com/questions/40461045/mayavi-combining-two-implicit-3d-surfaces\n\t        else:\n\t            cond_i[cond_j>0.0]=None\n\t            cond_i[cond_j<0.0]=None\n\t    conditions_processed.append(cond_i)\n\tfor i in range(len(conditions_processed)):\n\t    # print(i)\n\t    cond_i=conditions_processed[i]\n\t    # print(cond_i)\n\t    if(np.isnan(cond_i).all()):\n", "        continue\n\t    maximum=np.nanmax(cond_i)\n\t    minimum=np.nanmin(cond_i)\n\t    if(maximum<0 or minimum>0):\n\t        print(\"Continue\")\n\t        continue;\n\t    color=tuple(random.random() for _ in range(3))\n\t    tmp=mlab.contour3d(all_x,all_y,all_z,cond_i, contours = [0], color=color, opacity=1.0) \n\t    # tmp.actor.property.interpolation = 'phong' #https://stackoverflow.com/a/31754643\n\t    # tmp.actor.property.specular = 0.9\n", "    # tmp.actor.property.specular_power = 128\n\tlensoffset=0.0\n\txx = yy = zz = np.arange(0.0,1.5,0.1)\n\txy = xz = yx = yz = zx = zy = np.zeros_like(xx)    \n\tmlab.plot3d(yx,yy+lensoffset,yz,line_width=0.01,tube_radius=0.02)\n\tmlab.plot3d(zx,zy+lensoffset,zz,line_width=0.01,tube_radius=0.02)\n\tmlab.plot3d(xx,xy+lensoffset,xz,line_width=0.01,tube_radius=0.02)\n\tprint(\"Showing!\")\n\tmlab.show()\n"]}
