{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\tfrom setuptools import find_packages, setup\n\tsetup(\n\t    name=\"src\",\n\t    version=\"0.0.1\",\n\t    description=\"Slot Attention Lightning\",\n\t    author=\"Janghyuk Choi\",\n\t    author_email=\"janghyuk.ch@gmail.com\",\n\t    url=\"https://github.com/janghyuk-choi/slot-attention-lightning\",\n\t    install_requires=[\"pytorch-lightning\", \"hydra-core\"],\n", "    packages=find_packages(),\n\t    # use this to customize global commands available in the terminal after installing the package\n\t    entry_points={\n\t        \"console_scripts\": [\n\t            \"train_command = src.train:main\",\n\t            \"eval_command = src.eval:main\",\n\t        ]\n\t    },\n\t)\n"]}
{"filename": "configs/__init__.py", "chunked_list": ["# this file is needed here to include configs when building project as a package\n"]}
{"filename": "tests/test_datamodules.py", "chunked_list": ["from pathlib import Path\n\timport pytest\n\timport torch\n\tfrom src.data.mnist_datamodule import MNISTDataModule\n\t@pytest.mark.parametrize(\"batch_size\", [32, 128])\n\tdef test_mnist_datamodule(batch_size):\n\t    data_dir = \"data/\"\n\t    dm = MNISTDataModule(data_dir=data_dir, batch_size=batch_size)\n\t    dm.prepare_data()\n\t    assert not dm.data_train and not dm.data_val and not dm.data_test\n", "    assert Path(data_dir, \"MNIST\").exists()\n\t    assert Path(data_dir, \"MNIST\", \"raw\").exists()\n\t    dm.setup()\n\t    assert dm.data_train and dm.data_val and dm.data_test\n\t    assert dm.train_dataloader() and dm.val_dataloader() and dm.test_dataloader()\n\t    num_datapoints = len(dm.data_train) + len(dm.data_val) + len(dm.data_test)\n\t    assert num_datapoints == 70_000\n\t    batch = next(iter(dm.train_dataloader()))\n\t    x, y = batch\n\t    assert len(x) == batch_size\n", "    assert len(y) == batch_size\n\t    assert x.dtype == torch.float32\n\t    assert y.dtype == torch.int64\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_sweeps.py", "chunked_list": ["import pytest\n\tfrom tests.helpers.run_if import RunIf\n\tfrom tests.helpers.run_sh_command import run_sh_command\n\tstartfile = \"src/train.py\"\n\toverrides = [\"logger=[]\"]\n\t@RunIf(sh=True)\n\t@pytest.mark.slow\n\tdef test_experiments(tmp_path):\n\t    \"\"\"Test running all available experiment configs with fast_dev_run=True.\"\"\"\n\t    command = [\n", "        startfile,\n\t        \"-m\",\n\t        \"experiment=glob(*)\",\n\t        \"hydra.sweep.dir=\" + str(tmp_path),\n\t        \"++trainer.fast_dev_run=true\",\n\t    ] + overrides\n\t    run_sh_command(command)\n\t@RunIf(sh=True)\n\t@pytest.mark.slow\n\tdef test_hydra_sweep(tmp_path):\n", "    \"\"\"Test default hydra sweep.\"\"\"\n\t    command = [\n\t        startfile,\n\t        \"-m\",\n\t        \"hydra.sweep.dir=\" + str(tmp_path),\n\t        \"model.optimizer.lr=0.005,0.01\",\n\t        \"++trainer.fast_dev_run=true\",\n\t    ] + overrides\n\t    run_sh_command(command)\n\t@RunIf(sh=True)\n", "@pytest.mark.slow\n\tdef test_hydra_sweep_ddp_sim(tmp_path):\n\t    \"\"\"Test default hydra sweep with ddp sim.\"\"\"\n\t    command = [\n\t        startfile,\n\t        \"-m\",\n\t        \"hydra.sweep.dir=\" + str(tmp_path),\n\t        \"trainer=ddp_sim\",\n\t        \"trainer.max_epochs=3\",\n\t        \"+trainer.limit_train_batches=0.01\",\n", "        \"+trainer.limit_val_batches=0.1\",\n\t        \"+trainer.limit_test_batches=0.1\",\n\t        \"model.optimizer.lr=0.005,0.01,0.02\",\n\t    ] + overrides\n\t    run_sh_command(command)\n\t@RunIf(sh=True)\n\t@pytest.mark.slow\n\tdef test_optuna_sweep(tmp_path):\n\t    \"\"\"Test optuna sweep.\"\"\"\n\t    command = [\n", "        startfile,\n\t        \"-m\",\n\t        \"hparams_search=mnist_optuna\",\n\t        \"hydra.sweep.dir=\" + str(tmp_path),\n\t        \"hydra.sweeper.n_trials=10\",\n\t        \"hydra.sweeper.sampler.n_startup_trials=5\",\n\t        \"++trainer.fast_dev_run=true\",\n\t    ] + overrides\n\t    run_sh_command(command)\n\t@RunIf(wandb=True, sh=True)\n", "@pytest.mark.slow\n\tdef test_optuna_sweep_ddp_sim_wandb(tmp_path):\n\t    \"\"\"Test optuna sweep with wandb and ddp sim.\"\"\"\n\t    command = [\n\t        startfile,\n\t        \"-m\",\n\t        \"hparams_search=mnist_optuna\",\n\t        \"hydra.sweep.dir=\" + str(tmp_path),\n\t        \"hydra.sweeper.n_trials=5\",\n\t        \"trainer=ddp_sim\",\n", "        \"trainer.max_epochs=3\",\n\t        \"+trainer.limit_train_batches=0.01\",\n\t        \"+trainer.limit_val_batches=0.1\",\n\t        \"+trainer.limit_test_batches=0.1\",\n\t        \"logger=wandb\",\n\t    ]\n\t    run_sh_command(command)\n"]}
{"filename": "tests/test_train.py", "chunked_list": ["import os\n\timport pytest\n\tfrom hydra.core.hydra_config import HydraConfig\n\tfrom omegaconf import open_dict\n\tfrom src.train import train\n\tfrom tests.helpers.run_if import RunIf\n\tdef test_train_fast_dev_run(cfg_train):\n\t    \"\"\"Run for 1 train, val and test step.\"\"\"\n\t    HydraConfig().set_config(cfg_train)\n\t    with open_dict(cfg_train):\n", "        cfg_train.trainer.fast_dev_run = True\n\t        cfg_train.trainer.accelerator = \"cpu\"\n\t    train(cfg_train)\n\t@RunIf(min_gpus=1)\n\tdef test_train_fast_dev_run_gpu(cfg_train):\n\t    \"\"\"Run for 1 train, val and test step on GPU.\"\"\"\n\t    HydraConfig().set_config(cfg_train)\n\t    with open_dict(cfg_train):\n\t        cfg_train.trainer.fast_dev_run = True\n\t        cfg_train.trainer.accelerator = \"gpu\"\n", "    train(cfg_train)\n\t@RunIf(min_gpus=1)\n\t@pytest.mark.slow\n\tdef test_train_epoch_gpu_amp(cfg_train):\n\t    \"\"\"Train 1 epoch on GPU with mixed-precision.\"\"\"\n\t    HydraConfig().set_config(cfg_train)\n\t    with open_dict(cfg_train):\n\t        cfg_train.trainer.max_epochs = 1\n\t        cfg_train.trainer.accelerator = \"cpu\"\n\t        cfg_train.trainer.precision = 16\n", "    train(cfg_train)\n\t@pytest.mark.slow\n\tdef test_train_epoch_double_val_loop(cfg_train):\n\t    \"\"\"Train 1 epoch with validation loop twice per epoch.\"\"\"\n\t    HydraConfig().set_config(cfg_train)\n\t    with open_dict(cfg_train):\n\t        cfg_train.trainer.max_epochs = 1\n\t        cfg_train.trainer.val_check_interval = 0.5\n\t    train(cfg_train)\n\t@pytest.mark.slow\n", "def test_train_ddp_sim(cfg_train):\n\t    \"\"\"Simulate DDP (Distributed Data Parallel) on 2 CPU processes.\"\"\"\n\t    HydraConfig().set_config(cfg_train)\n\t    with open_dict(cfg_train):\n\t        cfg_train.trainer.max_epochs = 2\n\t        cfg_train.trainer.accelerator = \"cpu\"\n\t        cfg_train.trainer.devices = 2\n\t        cfg_train.trainer.strategy = \"ddp_spawn\"\n\t    train(cfg_train)\n\t@pytest.mark.slow\n", "def test_train_resume(tmp_path, cfg_train):\n\t    \"\"\"Run 1 epoch, finish, and resume for another epoch.\"\"\"\n\t    with open_dict(cfg_train):\n\t        cfg_train.trainer.max_epochs = 1\n\t    HydraConfig().set_config(cfg_train)\n\t    metric_dict_1, _ = train(cfg_train)\n\t    files = os.listdir(tmp_path / \"checkpoints\")\n\t    assert \"last.ckpt\" in files\n\t    assert \"epoch_000.ckpt\" in files\n\t    with open_dict(cfg_train):\n", "        cfg_train.ckpt_path = str(tmp_path / \"checkpoints\" / \"last.ckpt\")\n\t        cfg_train.trainer.max_epochs = 2\n\t    metric_dict_2, _ = train(cfg_train)\n\t    files = os.listdir(tmp_path / \"checkpoints\")\n\t    assert \"epoch_001.ckpt\" in files\n\t    assert \"epoch_002.ckpt\" not in files\n\t    assert metric_dict_1[\"train/acc\"] < metric_dict_2[\"train/acc\"]\n\t    assert metric_dict_1[\"val/acc\"] < metric_dict_2[\"val/acc\"]\n"]}
{"filename": "tests/test_eval.py", "chunked_list": ["import os\n\timport pytest\n\tfrom hydra.core.hydra_config import HydraConfig\n\tfrom omegaconf import open_dict\n\tfrom src.eval import evaluate\n\tfrom src.train import train\n\t@pytest.mark.slow\n\tdef test_train_eval(tmp_path, cfg_train, cfg_eval):\n\t    \"\"\"Train for 1 epoch with `train.py` and evaluate with `eval.py`\"\"\"\n\t    assert str(tmp_path) == cfg_train.paths.output_dir == cfg_eval.paths.output_dir\n", "    with open_dict(cfg_train):\n\t        cfg_train.trainer.max_epochs = 1\n\t        cfg_train.test = True\n\t    HydraConfig().set_config(cfg_train)\n\t    train_metric_dict, _ = train(cfg_train)\n\t    assert \"last.ckpt\" in os.listdir(tmp_path / \"checkpoints\")\n\t    with open_dict(cfg_eval):\n\t        cfg_eval.ckpt_path = str(tmp_path / \"checkpoints\" / \"last.ckpt\")\n\t    HydraConfig().set_config(cfg_eval)\n\t    test_metric_dict, _ = evaluate(cfg_eval)\n", "    assert test_metric_dict[\"test/acc\"] > 0.0\n\t    assert abs(train_metric_dict[\"test/acc\"].item() - test_metric_dict[\"test/acc\"].item()) < 0.001\n"]}
{"filename": "tests/test_configs.py", "chunked_list": ["import hydra\n\tfrom hydra.core.hydra_config import HydraConfig\n\tfrom omegaconf import DictConfig\n\tdef test_train_config(cfg_train: DictConfig):\n\t    assert cfg_train\n\t    assert cfg_train.data\n\t    assert cfg_train.model\n\t    assert cfg_train.trainer\n\t    HydraConfig().set_config(cfg_train)\n\t    hydra.utils.instantiate(cfg_train.data)\n", "    hydra.utils.instantiate(cfg_train.model)\n\t    hydra.utils.instantiate(cfg_train.trainer)\n\tdef test_eval_config(cfg_eval: DictConfig):\n\t    assert cfg_eval\n\t    assert cfg_eval.data\n\t    assert cfg_eval.model\n\t    assert cfg_eval.trainer\n\t    HydraConfig().set_config(cfg_eval)\n\t    hydra.utils.instantiate(cfg_eval.data)\n\t    hydra.utils.instantiate(cfg_eval.model)\n", "    hydra.utils.instantiate(cfg_eval.trainer)\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["\"\"\"This file prepares config fixtures for other tests.\"\"\"\n\timport pyrootutils\n\timport pytest\n\tfrom hydra import compose, initialize\n\tfrom hydra.core.global_hydra import GlobalHydra\n\tfrom omegaconf import DictConfig, open_dict\n\t@pytest.fixture(scope=\"package\")\n\tdef cfg_train_global() -> DictConfig:\n\t    with initialize(version_base=\"1.3\", config_path=\"../configs\"):\n\t        cfg = compose(config_name=\"train.yaml\", return_hydra_config=True, overrides=[])\n", "        # set defaults for all tests\n\t        with open_dict(cfg):\n\t            cfg.paths.root_dir = str(pyrootutils.find_root(indicator=\".project-root\"))\n\t            cfg.trainer.max_epochs = 1\n\t            cfg.trainer.limit_train_batches = 0.01\n\t            cfg.trainer.limit_val_batches = 0.1\n\t            cfg.trainer.limit_test_batches = 0.1\n\t            cfg.trainer.accelerator = \"cpu\"\n\t            cfg.trainer.devices = 1\n\t            cfg.data.num_workers = 0\n", "            cfg.data.pin_memory = False\n\t            cfg.extras.print_config = False\n\t            cfg.extras.enforce_tags = False\n\t            cfg.logger = None\n\t    return cfg\n\t@pytest.fixture(scope=\"package\")\n\tdef cfg_eval_global() -> DictConfig:\n\t    with initialize(version_base=\"1.3\", config_path=\"../configs\"):\n\t        cfg = compose(config_name=\"eval.yaml\", return_hydra_config=True, overrides=[\"ckpt_path=.\"])\n\t        # set defaults for all tests\n", "        with open_dict(cfg):\n\t            cfg.paths.root_dir = str(pyrootutils.find_root(indicator=\".project-root\"))\n\t            cfg.trainer.max_epochs = 1\n\t            cfg.trainer.limit_test_batches = 0.1\n\t            cfg.trainer.accelerator = \"cpu\"\n\t            cfg.trainer.devices = 1\n\t            cfg.data.num_workers = 0\n\t            cfg.data.pin_memory = False\n\t            cfg.extras.print_config = False\n\t            cfg.extras.enforce_tags = False\n", "            cfg.logger = None\n\t    return cfg\n\t# this is called by each test which uses `cfg_train` arg\n\t# each test generates its own temporary logging path\n\t@pytest.fixture(scope=\"function\")\n\tdef cfg_train(cfg_train_global, tmp_path) -> DictConfig:\n\t    cfg = cfg_train_global.copy()\n\t    with open_dict(cfg):\n\t        cfg.paths.output_dir = str(tmp_path)\n\t        cfg.paths.log_dir = str(tmp_path)\n", "    yield cfg\n\t    GlobalHydra.instance().clear()\n\t# this is called by each test which uses `cfg_eval` arg\n\t# each test generates its own temporary logging path\n\t@pytest.fixture(scope=\"function\")\n\tdef cfg_eval(cfg_eval_global, tmp_path) -> DictConfig:\n\t    cfg = cfg_eval_global.copy()\n\t    with open_dict(cfg):\n\t        cfg.paths.output_dir = str(tmp_path)\n\t        cfg.paths.log_dir = str(tmp_path)\n", "    yield cfg\n\t    GlobalHydra.instance().clear()\n"]}
{"filename": "tests/helpers/package_available.py", "chunked_list": ["import platform\n\timport pkg_resources\n\tfrom pytorch_lightning.accelerators import TPUAccelerator\n\tdef _package_available(package_name: str) -> bool:\n\t    \"\"\"Check if a package is available in your environment.\"\"\"\n\t    try:\n\t        return pkg_resources.require(package_name) is not None\n\t    except pkg_resources.DistributionNotFound:\n\t        return False\n\t_TPU_AVAILABLE = TPUAccelerator.is_available()\n", "_IS_WINDOWS = platform.system() == \"Windows\"\n\t_SH_AVAILABLE = not _IS_WINDOWS and _package_available(\"sh\")\n\t_DEEPSPEED_AVAILABLE = not _IS_WINDOWS and _package_available(\"deepspeed\")\n\t_FAIRSCALE_AVAILABLE = not _IS_WINDOWS and _package_available(\"fairscale\")\n\t_WANDB_AVAILABLE = _package_available(\"wandb\")\n\t_NEPTUNE_AVAILABLE = _package_available(\"neptune\")\n\t_COMET_AVAILABLE = _package_available(\"comet_ml\")\n\t_MLFLOW_AVAILABLE = _package_available(\"mlflow\")\n"]}
{"filename": "tests/helpers/__init__.py", "chunked_list": []}
{"filename": "tests/helpers/run_sh_command.py", "chunked_list": ["from typing import List\n\timport pytest\n\tfrom tests.helpers.package_available import _SH_AVAILABLE\n\tif _SH_AVAILABLE:\n\t    import sh\n\tdef run_sh_command(command: List[str]):\n\t    \"\"\"Default method for executing shell commands with pytest and sh package.\"\"\"\n\t    msg = None\n\t    try:\n\t        sh.python(command)\n", "    except sh.ErrorReturnCode as e:\n\t        msg = e.stderr.decode()\n\t    if msg:\n\t        pytest.fail(msg=msg)\n"]}
{"filename": "tests/helpers/run_if.py", "chunked_list": ["\"\"\"Adapted from:\n\thttps://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/helpers/runif.py\n\t\"\"\"\n\timport sys\n\tfrom typing import Optional\n\timport pytest\n\timport torch\n\tfrom packaging.version import Version\n\tfrom pkg_resources import get_distribution\n\tfrom tests.helpers.package_available import (\n", "    _COMET_AVAILABLE,\n\t    _DEEPSPEED_AVAILABLE,\n\t    _FAIRSCALE_AVAILABLE,\n\t    _IS_WINDOWS,\n\t    _MLFLOW_AVAILABLE,\n\t    _NEPTUNE_AVAILABLE,\n\t    _SH_AVAILABLE,\n\t    _TPU_AVAILABLE,\n\t    _WANDB_AVAILABLE,\n\t)\n", "class RunIf:\n\t    \"\"\"RunIf wrapper for conditional skipping of tests.\n\t    Fully compatible with `@pytest.mark`.\n\t    Example:\n\t        @RunIf(min_torch=\"1.8\")\n\t        @pytest.mark.parametrize(\"arg1\", [1.0, 2.0])\n\t        def test_wrapper(arg1):\n\t            assert arg1 > 0\n\t    \"\"\"\n\t    def __new__(\n", "        self,\n\t        min_gpus: int = 0,\n\t        min_torch: Optional[str] = None,\n\t        max_torch: Optional[str] = None,\n\t        min_python: Optional[str] = None,\n\t        skip_windows: bool = False,\n\t        sh: bool = False,\n\t        tpu: bool = False,\n\t        fairscale: bool = False,\n\t        deepspeed: bool = False,\n", "        wandb: bool = False,\n\t        neptune: bool = False,\n\t        comet: bool = False,\n\t        mlflow: bool = False,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n\t        Args:\n\t            min_gpus: min number of GPUs required to run test\n\t            min_torch: minimum pytorch version to run test\n", "            max_torch: maximum pytorch version to run test\n\t            min_python: minimum python version required to run test\n\t            skip_windows: skip test for Windows platform\n\t            tpu: if TPU is available\n\t            sh: if `sh` module is required to run the test\n\t            fairscale: if `fairscale` module is required to run the test\n\t            deepspeed: if `deepspeed` module is required to run the test\n\t            wandb: if `wandb` module is required to run the test\n\t            neptune: if `neptune` module is required to run the test\n\t            comet: if `comet` module is required to run the test\n", "            mlflow: if `mlflow` module is required to run the test\n\t            kwargs: native pytest.mark.skipif keyword arguments\n\t        \"\"\"\n\t        conditions = []\n\t        reasons = []\n\t        if min_gpus:\n\t            conditions.append(torch.cuda.device_count() < min_gpus)\n\t            reasons.append(f\"GPUs>={min_gpus}\")\n\t        if min_torch:\n\t            torch_version = get_distribution(\"torch\").version\n", "            conditions.append(Version(torch_version) < Version(min_torch))\n\t            reasons.append(f\"torch>={min_torch}\")\n\t        if max_torch:\n\t            torch_version = get_distribution(\"torch\").version\n\t            conditions.append(Version(torch_version) >= Version(max_torch))\n\t            reasons.append(f\"torch<{max_torch}\")\n\t        if min_python:\n\t            py_version = (\n\t                f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n\t            )\n", "            conditions.append(Version(py_version) < Version(min_python))\n\t            reasons.append(f\"python>={min_python}\")\n\t        if skip_windows:\n\t            conditions.append(_IS_WINDOWS)\n\t            reasons.append(\"does not run on Windows\")\n\t        if tpu:\n\t            conditions.append(not _TPU_AVAILABLE)\n\t            reasons.append(\"TPU\")\n\t        if sh:\n\t            conditions.append(not _SH_AVAILABLE)\n", "            reasons.append(\"sh\")\n\t        if fairscale:\n\t            conditions.append(not _FAIRSCALE_AVAILABLE)\n\t            reasons.append(\"fairscale\")\n\t        if deepspeed:\n\t            conditions.append(not _DEEPSPEED_AVAILABLE)\n\t            reasons.append(\"deepspeed\")\n\t        if wandb:\n\t            conditions.append(not _WANDB_AVAILABLE)\n\t            reasons.append(\"wandb\")\n", "        if neptune:\n\t            conditions.append(not _NEPTUNE_AVAILABLE)\n\t            reasons.append(\"neptune\")\n\t        if comet:\n\t            conditions.append(not _COMET_AVAILABLE)\n\t            reasons.append(\"comet\")\n\t        if mlflow:\n\t            conditions.append(not _MLFLOW_AVAILABLE)\n\t            reasons.append(\"mlflow\")\n\t        reasons = [rs for cond, rs in zip(conditions, reasons) if cond]\n", "        return pytest.mark.skipif(\n\t            condition=any(conditions),\n\t            reason=f\"Requires: [{' + '.join(reasons)}]\",\n\t            **kwargs,\n\t        )\n"]}
{"filename": "src/train.py", "chunked_list": ["from typing import List, Optional, Tuple\n\timport hydra\n\timport pyrootutils\n\timport pytorch_lightning as pl\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import Callback, LightningDataModule, LightningModule, Trainer\n\tfrom pytorch_lightning.loggers import Logger\n\tpyrootutils.setup_root(__file__, indicator=\".project-root\", pythonpath=True)\n\t# ------------------------------------------------------------------------------------ #\n\t# the setup_root above is equivalent to:\n", "# - adding project root dir to PYTHONPATH\n\t#       (so you don't need to force user to install project as a package)\n\t#       (necessary before importing any local modules e.g. `from src import utils`)\n\t# - setting up PROJECT_ROOT environment variable\n\t#       (which is used as a base for paths in \"configs/paths/default.yaml\")\n\t#       (this way all filepaths are the same no matter where you run the code)\n\t# - loading environment variables from \".env\" in root dir\n\t#\n\t# you can remove it if you:\n\t# 1. either install project as a package or move entry files to project root dir\n", "# 2. set `root_dir` to \".\" in \"configs/paths/default.yaml\"\n\t#\n\t# more info: https://github.com/ashleve/pyrootutils\n\t# ------------------------------------------------------------------------------------ #\n\tfrom src import utils\n\tlog = utils.get_pylogger(__name__)\n\t@utils.task_wrapper\n\tdef train(cfg: DictConfig) -> Tuple[dict, dict]:\n\t    \"\"\"Trains the model. Can additionally evaluate on a testset, using best weights obtained during\n\t    training.\n", "    This method is wrapped in optional @task_wrapper decorator which applies extra utilities\n\t    before and after the call.\n\t    Args:\n\t        cfg (DictConfig): Configuration composed by Hydra.\n\t    Returns:\n\t        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n\t    \"\"\"\n\t    # set seed for random number generators in pytorch, numpy and python.random\n\t    if cfg.get(\"seed\"):\n\t        pl.seed_everything(cfg.seed, workers=True)\n", "    log.info(f\"Instantiating datamodule <{cfg.data._target_}>\")\n\t    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n\t    log.info(f\"Instantiating model <{cfg.model._target_}>\")\n\t    model: LightningModule = hydra.utils.instantiate(cfg.model)\n\t    log.info(\"Instantiating callbacks...\")\n\t    callbacks: List[Callback] = utils.instantiate_callbacks(cfg.get(\"callbacks\"))\n\t    log.info(\"Instantiating loggers...\")\n\t    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n\t    log.info(f\"Instantiating trainer <{cfg.trainer._target_}>\")\n\t    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=logger)\n", "    object_dict = {\n\t        \"cfg\": cfg,\n\t        \"datamodule\": datamodule,\n\t        \"model\": model,\n\t        \"callbacks\": callbacks,\n\t        \"logger\": logger,\n\t        \"trainer\": trainer,\n\t    }\n\t    if logger:\n\t        log.info(\"Logging hyperparameters!\")\n", "        utils.log_hyperparameters(object_dict)\n\t    if cfg.get(\"train\"):\n\t        log.info(\"Starting training!\")\n\t        trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get(\"ckpt_path\"))\n\t    train_metrics = trainer.callback_metrics\n\t    if cfg.get(\"test\"):\n\t        log.info(\"Starting testing!\")\n\t        ckpt_path = trainer.checkpoint_callback.best_model_path\n\t        if ckpt_path == \"\":\n\t            log.warning(\"Best ckpt not found! Using current weights for testing...\")\n", "            ckpt_path = None\n\t        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)\n\t        log.info(f\"Best ckpt path: {ckpt_path}\")\n\t    test_metrics = trainer.callback_metrics\n\t    # merge train and test metrics\n\t    metric_dict = {**train_metrics, **test_metrics}\n\t    return metric_dict, object_dict\n\t@hydra.main(version_base=\"1.3\", config_path=\"../configs\", config_name=\"train.yaml\")\n\tdef main(cfg: DictConfig) -> Optional[float]:\n\t    # train the model\n", "    metric_dict, _ = train(cfg)\n\t    # safely retrieve metric value for hydra-based hyperparameter optimization\n\t    metric_value = utils.get_metric_value(\n\t        metric_dict=metric_dict, metric_name=cfg.get(\"optimized_metric\")\n\t    )\n\t    # return optimized metric\n\t    return metric_value\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "src/__init__.py", "chunked_list": []}
{"filename": "src/eval.py", "chunked_list": ["from typing import List, Tuple\n\timport hydra\n\timport pyrootutils\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import LightningDataModule, LightningModule, Trainer\n\tfrom pytorch_lightning.loggers import Logger\n\tpyrootutils.setup_root(__file__, indicator=\".project-root\", pythonpath=True)\n\t# ------------------------------------------------------------------------------------ #\n\t# the setup_root above is equivalent to:\n\t# - adding project root dir to PYTHONPATH\n", "#       (so you don't need to force user to install project as a package)\n\t#       (necessary before importing any local modules e.g. `from src import utils`)\n\t# - setting up PROJECT_ROOT environment variable\n\t#       (which is used as a base for paths in \"configs/paths/default.yaml\")\n\t#       (this way all filepaths are the same no matter where you run the code)\n\t# - loading environment variables from \".env\" in root dir\n\t#\n\t# you can remove it if you:\n\t# 1. either install project as a package or move entry files to project root dir\n\t# 2. set `root_dir` to \".\" in \"configs/paths/default.yaml\"\n", "#\n\t# more info: https://github.com/ashleve/pyrootutils\n\t# ------------------------------------------------------------------------------------ #\n\tfrom src import utils\n\tlog = utils.get_pylogger(__name__)\n\t@utils.task_wrapper\n\tdef evaluate(cfg: DictConfig) -> Tuple[dict, dict]:\n\t    \"\"\"Evaluates given checkpoint on a datamodule testset.\n\t    This method is wrapped in optional @task_wrapper decorator which applies extra utilities\n\t    before and after the call.\n", "    Args:\n\t        cfg (DictConfig): Configuration composed by Hydra.\n\t    Returns:\n\t        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n\t    \"\"\"\n\t    assert cfg.ckpt_path\n\t    log.info(f\"Instantiating datamodule <{cfg.data._target_}>\")\n\t    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n\t    log.info(f\"Instantiating model <{cfg.model._target_}>\")\n\t    model: LightningModule = hydra.utils.instantiate(cfg.model)\n", "    log.info(\"Instantiating loggers...\")\n\t    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n\t    log.info(f\"Instantiating trainer <{cfg.trainer._target_}>\")\n\t    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)\n\t    object_dict = {\n\t        \"cfg\": cfg,\n\t        \"datamodule\": datamodule,\n\t        \"model\": model,\n\t        \"logger\": logger,\n\t        \"trainer\": trainer,\n", "    }\n\t    if logger:\n\t        log.info(\"Logging hyperparameters!\")\n\t        utils.log_hyperparameters(object_dict)\n\t    log.info(\"Starting testing!\")\n\t    trainer.test(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)\n\t    # for predictions use trainer.predict(...)\n\t    # predictions = trainer.predict(model=model, dataloaders=dataloaders, ckpt_path=cfg.ckpt_path)\n\t    metric_dict = trainer.callback_metrics\n\t    return metric_dict, object_dict\n", "@hydra.main(version_base=\"1.3\", config_path=\"../configs\", config_name=\"eval.yaml\")\n\tdef main(cfg: DictConfig) -> None:\n\t    evaluate(cfg)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "src/utils/rich_utils.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Sequence\n\timport rich\n\timport rich.syntax\n\timport rich.tree\n\tfrom hydra.core.hydra_config import HydraConfig\n\tfrom omegaconf import DictConfig, OmegaConf, open_dict\n\tfrom pytorch_lightning.utilities import rank_zero_only\n\tfrom rich.prompt import Prompt\n\tfrom src.utils import pylogger\n", "log = pylogger.get_pylogger(__name__)\n\t@rank_zero_only\n\tdef print_config_tree(\n\t    cfg: DictConfig,\n\t    print_order: Sequence[str] = (\n\t        \"data\",\n\t        \"model\",\n\t        \"callbacks\",\n\t        \"logger\",\n\t        \"trainer\",\n", "        \"paths\",\n\t        \"extras\",\n\t    ),\n\t    resolve: bool = False,\n\t    save_to_file: bool = False,\n\t) -> None:\n\t    \"\"\"Prints content of DictConfig using Rich library and its tree structure.\n\t    Args:\n\t        cfg (DictConfig): Configuration composed by Hydra.\n\t        print_order (Sequence[str], optional): Determines in what order config components are printed.\n", "        resolve (bool, optional): Whether to resolve reference fields of DictConfig.\n\t        save_to_file (bool, optional): Whether to export config to the hydra output folder.\n\t    \"\"\"\n\t    style = \"dim\"\n\t    tree = rich.tree.Tree(\"CONFIG\", style=style, guide_style=style)\n\t    queue = []\n\t    # add fields from `print_order` to queue\n\t    for field in print_order:\n\t        queue.append(field) if field in cfg else log.warning(\n\t            f\"Field '{field}' not found in config. Skipping '{field}' config printing...\"\n", "        )\n\t    # add all the other fields to queue (not specified in `print_order`)\n\t    for field in cfg:\n\t        if field not in queue:\n\t            queue.append(field)\n\t    # generate config tree from queue\n\t    for field in queue:\n\t        branch = tree.add(field, style=style, guide_style=style)\n\t        config_group = cfg[field]\n\t        if isinstance(config_group, DictConfig):\n", "            branch_content = OmegaConf.to_yaml(config_group, resolve=resolve)\n\t        else:\n\t            branch_content = str(config_group)\n\t        branch.add(rich.syntax.Syntax(branch_content, \"yaml\"))\n\t    # print config tree\n\t    rich.print(tree)\n\t    # save config tree to file\n\t    if save_to_file:\n\t        with open(Path(cfg.paths.output_dir, \"config_tree.log\"), \"w\") as file:\n\t            rich.print(tree, file=file)\n", "@rank_zero_only\n\tdef enforce_tags(cfg: DictConfig, save_to_file: bool = False) -> None:\n\t    \"\"\"Prompts user to input tags from command line if no tags are provided in config.\"\"\"\n\t    if not cfg.get(\"tags\"):\n\t        if \"id\" in HydraConfig().cfg.hydra.job:\n\t            raise ValueError(\"Specify tags before launching a multirun!\")\n\t        log.warning(\"No tags provided in config. Prompting user to input tags...\")\n\t        tags = Prompt.ask(\"Enter a list of comma separated tags\", default=\"dev\")\n\t        tags = [t.strip() for t in tags.split(\",\") if t != \"\"]\n\t        with open_dict(cfg):\n", "            cfg.tags = tags\n\t        log.info(f\"Tags: {cfg.tags}\")\n\t    if save_to_file:\n\t        with open(Path(cfg.paths.output_dir, \"tags.log\"), \"w\") as file:\n\t            rich.print(cfg.tags, file=file)\n"]}
{"filename": "src/utils/__init__.py", "chunked_list": ["from src.utils.pylogger import get_pylogger\n\tfrom src.utils.rich_utils import enforce_tags, print_config_tree\n\tfrom src.utils.utils import (\n\t    close_loggers,\n\t    extras,\n\t    get_metric_value,\n\t    instantiate_callbacks,\n\t    instantiate_loggers,\n\t    log_hyperparameters,\n\t    save_file,\n", "    task_wrapper,\n\t)\n"]}
{"filename": "src/utils/utils.py", "chunked_list": ["import time\n\timport warnings\n\tfrom importlib.util import find_spec\n\tfrom pathlib import Path\n\tfrom typing import Any, Callable, Dict, List\n\timport hydra\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import Callback\n\tfrom pytorch_lightning.loggers import Logger\n\tfrom pytorch_lightning.utilities import rank_zero_only\n", "from src.utils import pylogger, rich_utils\n\tlog = pylogger.get_pylogger(__name__)\n\tdef task_wrapper(task_func: Callable) -> Callable:\n\t    \"\"\"Optional decorator that wraps the task function in extra utilities.\n\t    Makes multirun more resistant to failure.\n\t    Utilities:\n\t    - Calling the `utils.extras()` before the task is started\n\t    - Calling the `utils.close_loggers()` after the task is finished or failed\n\t    - Logging the exception if occurs\n\t    - Logging the output dir\n", "    \"\"\"\n\t    def wrap(cfg: DictConfig):\n\t        # execute the task\n\t        try:\n\t            # apply extra utilities\n\t            extras(cfg)\n\t            metric_dict, object_dict = task_func(cfg=cfg)\n\t        # things to do if exception occurs\n\t        except Exception as ex:\n\t            # save exception to `.log` file\n", "            log.exception(\"\")\n\t            # when using hydra plugins like Optuna, you might want to disable raising exception\n\t            # to avoid multirun failure\n\t            raise ex\n\t        # things to always do after either success or exception\n\t        finally:\n\t            # display output dir path in terminal\n\t            log.info(f\"Output dir: {cfg.paths.output_dir}\")\n\t            # close loggers (even if exception occurs so multirun won't fail)\n\t            close_loggers()\n", "        return metric_dict, object_dict\n\t    return wrap\n\tdef extras(cfg: DictConfig) -> None:\n\t    \"\"\"Applies optional utilities before the task is started.\n\t    Utilities:\n\t    - Ignoring python warnings\n\t    - Setting tags from command line\n\t    - Rich config printing\n\t    \"\"\"\n\t    # return if no `extras` config\n", "    if not cfg.get(\"extras\"):\n\t        log.warning(\"Extras config not found! <cfg.extras=null>\")\n\t        return\n\t    # disable python warnings\n\t    if cfg.extras.get(\"ignore_warnings\"):\n\t        log.info(\"Disabling python warnings! <cfg.extras.ignore_warnings=True>\")\n\t        warnings.filterwarnings(\"ignore\")\n\t    # prompt user to input tags from command line if none are provided in the config\n\t    if cfg.extras.get(\"enforce_tags\"):\n\t        log.info(\"Enforcing tags! <cfg.extras.enforce_tags=True>\")\n", "        rich_utils.enforce_tags(cfg, save_to_file=True)\n\t    # pretty print config tree using Rich library\n\t    if cfg.extras.get(\"print_config\"):\n\t        log.info(\"Printing config tree with Rich! <cfg.extras.print_config=True>\")\n\t        rich_utils.print_config_tree(cfg, resolve=True, save_to_file=True)\n\tdef instantiate_callbacks(callbacks_cfg: DictConfig) -> List[Callback]:\n\t    \"\"\"Instantiates callbacks from config.\"\"\"\n\t    callbacks: List[Callback] = []\n\t    if not callbacks_cfg:\n\t        log.warning(\"No callback configs found! Skipping..\")\n", "        return callbacks\n\t    if not isinstance(callbacks_cfg, DictConfig):\n\t        raise TypeError(\"Callbacks config must be a DictConfig!\")\n\t    for _, cb_conf in callbacks_cfg.items():\n\t        if isinstance(cb_conf, DictConfig) and \"_target_\" in cb_conf:\n\t            log.info(f\"Instantiating callback <{cb_conf._target_}>\")\n\t            callbacks.append(hydra.utils.instantiate(cb_conf))\n\t    return callbacks\n\tdef instantiate_loggers(logger_cfg: DictConfig) -> List[Logger]:\n\t    \"\"\"Instantiates loggers from config.\"\"\"\n", "    logger: List[Logger] = []\n\t    if not logger_cfg:\n\t        log.warning(\"No logger configs found! Skipping...\")\n\t        return logger\n\t    if not isinstance(logger_cfg, DictConfig):\n\t        raise TypeError(\"Logger config must be a DictConfig!\")\n\t    for _, lg_conf in logger_cfg.items():\n\t        if isinstance(lg_conf, DictConfig) and \"_target_\" in lg_conf:\n\t            log.info(f\"Instantiating logger <{lg_conf._target_}>\")\n\t            logger.append(hydra.utils.instantiate(lg_conf))\n", "    return logger\n\t@rank_zero_only\n\tdef log_hyperparameters(object_dict: dict) -> None:\n\t    \"\"\"Controls which config parts are saved by lightning loggers.\n\t    Additionally saves:\n\t    - Number of model parameters\n\t    \"\"\"\n\t    hparams = {}\n\t    cfg = object_dict[\"cfg\"]\n\t    model = object_dict[\"model\"]\n", "    trainer = object_dict[\"trainer\"]\n\t    if not trainer.logger:\n\t        log.warning(\"Logger not found! Skipping hyperparameter logging...\")\n\t        return\n\t    hparams[\"model\"] = cfg[\"model\"]\n\t    # save number of model parameters\n\t    hparams[\"model/params/total\"] = sum(p.numel() for p in model.parameters())\n\t    hparams[\"model/params/trainable\"] = sum(\n\t        p.numel() for p in model.parameters() if p.requires_grad\n\t    )\n", "    hparams[\"model/params/non_trainable\"] = sum(\n\t        p.numel() for p in model.parameters() if not p.requires_grad\n\t    )\n\t    hparams[\"data\"] = cfg[\"data\"]\n\t    hparams[\"trainer\"] = cfg[\"trainer\"]\n\t    hparams[\"callbacks\"] = cfg.get(\"callbacks\")\n\t    hparams[\"extras\"] = cfg.get(\"extras\")\n\t    hparams[\"task_name\"] = cfg.get(\"task_name\")\n\t    hparams[\"tags\"] = cfg.get(\"tags\")\n\t    hparams[\"ckpt_path\"] = cfg.get(\"ckpt_path\")\n", "    hparams[\"seed\"] = cfg.get(\"seed\")\n\t    # send hparams to all loggers\n\t    for logger in trainer.loggers:\n\t        logger.log_hyperparams(hparams)\n\tdef get_metric_value(metric_dict: dict, metric_name: str) -> float:\n\t    \"\"\"Safely retrieves value of the metric logged in LightningModule.\"\"\"\n\t    if not metric_name:\n\t        log.info(\"Metric name is None! Skipping metric value retrieval...\")\n\t        return None\n\t    if metric_name not in metric_dict:\n", "        raise Exception(\n\t            f\"Metric value not found! <metric_name={metric_name}>\\n\"\n\t            \"Make sure metric name logged in LightningModule is correct!\\n\"\n\t            \"Make sure `optimized_metric` name in `hparams_search` config is correct!\"\n\t        )\n\t    metric_value = metric_dict[metric_name].item()\n\t    log.info(f\"Retrieved metric value! <{metric_name}={metric_value}>\")\n\t    return metric_value\n\tdef close_loggers() -> None:\n\t    \"\"\"Makes sure all loggers closed properly (prevents logging failure during multirun).\"\"\"\n", "    log.info(\"Closing loggers...\")\n\t    if find_spec(\"wandb\"):  # if wandb is installed\n\t        import wandb\n\t        if wandb.run:\n\t            log.info(\"Closing wandb!\")\n\t            wandb.finish()\n\t@rank_zero_only\n\tdef save_file(path: str, content: str) -> None:\n\t    \"\"\"Save file in rank zero mode (only on one process in multi-GPU setup).\"\"\"\n\t    with open(path, \"w+\") as file:\n", "        file.write(content)\n"]}
{"filename": "src/utils/pylogger.py", "chunked_list": ["import logging\n\tfrom pytorch_lightning.utilities import rank_zero_only\n\tdef get_pylogger(name=__name__) -> logging.Logger:\n\t    \"\"\"Initializes multi-GPU-friendly python command line logger.\"\"\"\n\t    logger = logging.getLogger(name)\n\t    # this ensures all logging levels get marked with the rank zero decorator\n\t    # otherwise logs would get multiplied for each GPU process in multi-GPU setup\n\t    logging_levels = (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\")\n\t    for level in logging_levels:\n\t        setattr(logger, level, rank_zero_only(getattr(logger, level)))\n", "    return logger\n"]}
{"filename": "src/utils/vis_utils.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn.functional as F\n\tdef visualize(image, recon_combined, recons, pred_masks, gt_masks, attns, colored_box=True):\n\t    \"\"\"\n\t    `image`: [B, 3, H, W]\n\t    `recon_combined`: [B, 3, H, W]\n\t    `recons`: [B, K, H, W, C]\n\t    `pred_masks`: [B, K, H, W, 1]\n\t    `gt_masks`: [B, K, H, W, 1]\n", "    `attns`: (B, T, N_heads, N_in, K)\n\t    \"\"\"\n\t    img = image[:1]\n\t    recon_combined = recon_combined[:1]\n\t    recons = recons[:1]\n\t    pred_masks = pred_masks[:1]\n\t    gt_masks = gt_masks[:1]\n\t    attns = attns[:1]\n\t    _, K, H, W, _ = pred_masks.shape\n\t    _, T, _, _, _ = attns.shape\n", "    # get binarized masks\n\t    pred_mask_max_idxs = torch.argmax(pred_masks.squeeze(-1), dim=1)\n\t    # `mask_max_idxs`: (1, H, W)\n\t    pred_seg_masks = torch.zeros_like(pred_masks.squeeze(-1))\n\t    pred_seg_masks[\n\t        torch.arange(1)[:, None, None],\n\t        pred_mask_max_idxs,\n\t        torch.arange(H)[None, :, None],\n\t        torch.arange(W)[None, None, :],\n\t    ] = 1.0\n", "    pred_seg_masks = pred_seg_masks.unsqueeze(-1)\n\t    # `pred_seg_masks`: (1, K, H, W, 1)\n\t    pad = (0, 0, 2, 2, 2, 2)\n\t    # set colors\n\t    slot_colors = (\n\t        torch.tensor(\n\t            np.array(\n\t                [\n\t                    [255, 0, 0],\n\t                    [255, 127, 0],\n", "                    [255, 255, 0],\n\t                    [0, 255, 0],\n\t                    [0, 0, 255],\n\t                    [75, 0, 130],\n\t                    [148, 0, 211],\n\t                    [0, 255, 255],\n\t                    [153, 255, 153],\n\t                    [255, 153, 204],\n\t                    [102, 0, 51],\n\t                    [128, 128, 128],\n", "                    [255, 255, 255],\n\t                ]\n\t            ),\n\t            dtype=torch.float32,\n\t        )\n\t        / 255.0\n\t    )\n\t    # handle the multi-head attention\n\t    attns = torch.mean(attns[:, :], dim=2).permute(0, 1, 3, 2).view(1, -1, K, H, W).unsqueeze(-1)\n\t    # `attns`: (1, T, K, H, W, 1)\n", "    # reshape tensors\n\t    attns = attns.reshape(-1, K, H, W, 1)\n\t    # `attns`: (T, K, H, W, 1)\n\t    attns = torch.cat([attns, pred_masks, pred_seg_masks], dim=0)\n\t    N_row = attns.shape[0]\n\t    # `attns`: (N_row, K, H, W, 1)\n\t    # N_row = T + 2\n\t    # `attns` - attention maps over iterations\n\t    # `pred_masks` - alpha mask generated by decoder\n\t    # `pred_seg_masks` - binary mask from `pred_masks` with argmax\n", "    img = torch.einsum(\"nchw->nhwc\", img)\n\t    gt_col = torch.ones((N_row, H, W, 3), dtype=img.dtype, device=img.device)\n\t    gt_col[-2] = 0  # to draw seg mask by adding values\n\t    gt_col[-1] = img\n\t    # draw boundary box for the original image\n\t    gt_col = F.pad(gt_col, pad=pad, mode=\"constant\", value=1.0)\n\t    if colored_box:\n\t        gt_col[1:, :2, :] = gt_col[1:, -2:, :] = gt_col[1:, :, :2] = gt_col[\n\t            1:, :, -2:\n\t        ] = slot_colors[\n", "            -2\n\t        ]  # gray\n\t    gt_col = F.pad(gt_col, pad=pad, mode=\"constant\", value=1.0)\n\t    # `gt_col`: [T+2, H, W, C]\n\t    recon_combined = torch.einsum(\"nchw->nhwc\", recon_combined)\n\t    pred_col = torch.ones(\n\t        (N_row, H, W, 3), dtype=recon_combined.dtype, device=recon_combined.device\n\t    )\n\t    pred_col[-2] = 0  # to draw seg mask by adding values\n\t    pred_col[-1] = recon_combined\n", "    # draw boundary box for the reconstructed image\n\t    pred_col = F.pad(pred_col, pad=pad, mode=\"constant\", value=1.0)\n\t    if colored_box:\n\t        pred_col[1:, :2, :] = pred_col[1:, -2:, :] = pred_col[1:, :, :2] = pred_col[\n\t            1:, :, -2:\n\t        ] = slot_colors[\n\t            -2\n\t        ]  # gray\n\t    pred_col = F.pad(pred_col, pad=pad, mode=\"constant\", value=1.0)\n\t    # `pred_col`: [T+2, H, W, C]\n", "    for k in range(K):\n\t        # # get vis. of attention maps based on the original image\n\t        picture = torch.ones_like(img) * attns[:, k, :, :, :]\n\t        # overwrite vis. of alpha mask with the recon. image\n\t        picture[-2] = pred_seg_masks[:, k, :, :, :]\n\t        picture[-1] = recons[:, k, :, :, :] * pred_seg_masks[:, k, :, :, :] + (\n\t            1 - pred_seg_masks[:, k, :, :, :]\n\t        )\n\t        try:\n\t            gt_col[-2, 4:-4, 4:-4, :] += gt_masks[0, k, :, :, :] * slot_colors[k - 1].to(\n", "                pred_seg_masks.device\n\t            )  # `k-1` -> to give white color to background\n\t        except:\n\t            # when #slots > # objects. it is not the big deal\n\t            pass \n\t        pred_col[-2, 4:-4, 4:-4, :] += pred_seg_masks[0, k, :, :, :] * slot_colors[k - 1].to(\n\t            pred_seg_masks.device\n\t        )  # `k-1` -> to give white color to background\n\t        # draw boundary box for slots\n\t        picture = F.pad(picture, pad=pad, mode=\"constant\", value=1.0)\n", "        if colored_box:\n\t            picture[:, :2, :] = picture[:, -2:, :] = picture[:, :, :2] = picture[\n\t                :, :, -2:\n\t            ] = slot_colors[k]\n\t        picture = F.pad(picture, pad=pad, mode=\"constant\", value=1.0)\n\t        if k == 0:\n\t            log_img = torch.cat([picture], dim=2)\n\t        else:\n\t            log_img = torch.cat([log_img, picture], dim=2)\n\t    bg_mask = torch.where(\n", "        torch.sum(gt_col[-2, 4:-4, 4:-4, :], dim=-1, keepdim=True) == 0,\n\t        torch.ones_like(gt_col[-2, 4:-4, 4:-4, :]),\n\t        torch.zeros_like(gt_col[-2, 4:-4, 4:-4, :]),\n\t    )\n\t    gt_col[-2, 4:-4, 4:-4, :] += bg_mask * 0.5\n\t    log_img = torch.cat([gt_col, pred_col, log_img], dim=2)\n\t    log_img = log_img.permute(0, 3, 1, 2)\n\t    return log_img\n"]}
{"filename": "src/utils/evaluator.py", "chunked_list": ["import numpy as np\n\timport torch\n\tfrom scipy.optimize import linear_sum_assignment\n\tfrom scipy.special import comb\n\tclass ARIEvaluator:\n\t    \"\"\"\"\"\"\n\t    def __init__(self):\n\t        self.aris = []\n\t    def evaluate(self, pred, label):\n\t        \"\"\"\n", "        :param data: (image, mask)\n\t            image: (B, 3, H, W)\n\t            pred : (B, N0, H, W)\n\t            label: (B, N1, H, W)\n\t        :return: average ari\n\t        \"\"\"\n\t        B, K, H, W = pred.size()\n\t        # reduced to (B, K, H, W), with 1-0 values\n\t        # max_index (B, H, W)\n\t        max_index = torch.argmax(pred, dim=1)\n", "        # get binarized masks (B, K, H, W)\n\t        pred = torch.zeros_like(pred)\n\t        pred[\n\t            torch.arange(B)[:, None, None],\n\t            max_index,\n\t            torch.arange(H)[None, :, None],\n\t            torch.arange(W)[None, None, :],\n\t        ] = 1.0\n\t        for b in range(B):\n\t            this_ari = self.compute_mask_ari(label[b].to(pred.device), pred[b].to(pred.device))\n", "            self.aris.append(this_ari)\n\t    def reset(self):\n\t        self.aris = []\n\t    def get_results(self):\n\t        return np.mean(self.aris) if self.aris != [] else 0\n\t    def compute_ari(self, table):\n\t        \"\"\"Compute ari, given the index table.\n\t        :param table: (r, s)\n\t        :return:\n\t        \"\"\"\n", "        # # (r,)\n\t        # a = table.sum(axis=1)\n\t        # # (s,)\n\t        # b = table.sum(axis=0)\n\t        # n = a.sum()\n\t        # (r,)\n\t        a = table.sum(dim=1)\n\t        # (s,)\n\t        b = table.sum(dim=0)\n\t        n = a.sum()\n", "        comb_a = comb(a.detach().cpu().numpy(), 2).sum()\n\t        comb_b = comb(b.detach().cpu().numpy(), 2).sum()\n\t        comb_n = comb(n.detach().cpu().numpy(), 2)\n\t        comb_table = comb(table.detach().cpu().numpy(), 2).sum()\n\t        if comb_b == comb_a == comb_n == comb_table:\n\t            # the perfect case\n\t            ari = 1.0\n\t        else:\n\t            ari = (comb_table - comb_a * comb_b / comb_n) / (\n\t                0.5 * (comb_a + comb_b) - (comb_a * comb_b) / comb_n\n", "            )\n\t        return ari\n\t    def compute_mask_ari(self, mask0, mask1):\n\t        \"\"\"Given two sets of masks, compute ari.\n\t        :param mask0: ground truth mask, (N0, H, W)\n\t        :param mask1: predicted mask, (N1, H, W)\n\t        :return:\n\t        \"\"\"\n\t        # will first need to compute a table of shape (N0, N1)\n\t        # (N0, 1, H, W)\n", "        mask0 = mask0[:, None].byte()\n\t        # (1, N1, H, W)\n\t        mask1 = mask1[None, :].byte()\n\t        # (N0, N1, H, W)\n\t        agree = mask0 & mask1\n\t        # (N0, N1)\n\t        table = agree.sum(dim=-1).sum(dim=-1)\n\t        return self.compute_ari(table)\n\tclass mIoUEvaluator:\n\t    def __init__(self):\n", "        self.mious = []\n\t    def evaluate(self, pred, label):\n\t        \"\"\"\n\t        :param data: (image, mask)\n\t            image: (B, 3, H, W)\n\t            pred : (B, N0, H, W)\n\t            label: (B, N1, H, W)\n\t        :return: average miou\n\t        \"\"\"\n\t        from torch import arange as ar\n", "        B, K, H, W = pred.size()\n\t        # reduced to (B, K, H, W), with 1-0 values\n\t        # max_index (B, H, W)\n\t        max_index = torch.argmax(pred, dim=1)\n\t        # get binarized masks (B, K, H, W)\n\t        pred = torch.zeros_like(pred)\n\t        pred[ar(B)[:, None, None], max_index, ar(H)[None, :, None], ar(W)[None, None, :]] = 1.0\n\t        for b in range(B):\n\t            this_miou = self.compute_miou(label[b].to(pred.device), pred[b].to(pred.device))\n\t            self.mious.append(this_miou)\n", "    def reset(self):\n\t        self.mious = []\n\t    def get_results(self):\n\t        return np.mean(self.mious) if self.mious != [] else 0\n\t    def compute_miou(self, mask0, mask1):\n\t        \"\"\"\n\t        mask0: [N0, H, W]\n\t        mask1: [N1, H, W]\n\t        \"\"\"\n\t        mask0 = mask0.reshape(mask0.shape[0], -1)[:, None, :]  # [N0, 1, H*W]\n", "        mask1 = mask1.reshape(mask1.shape[0], -1)[None, :, :]  # [1, N1, H*W]\n\t        union = torch.sum(torch.clip(mask0 + mask1, 0, 1), dim=-1)  # [1, N0, N1]\n\t        intersection = torch.sum(mask0 * mask1, dim=-1)  # [1, N0, N1]\n\t        iou = intersection / (union + 1e-8)  # [N0, N1]\n\t        row_ind, col_ind = linear_sum_assignment(iou.cpu().detach().numpy() * -1)\n\t        miou = iou[row_ind, col_ind].mean().cpu().detach().numpy()\n\t        return miou\n"]}
{"filename": "src/data/clevrtex_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\timport torch\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader, Dataset\n\tfrom torchvision.transforms import transforms\n\tfrom src.data.components.clevrtex import CLEVRTEX\n\tclass CLEVRTEXDataModule(LightningDataModule):\n\t    \"\"\"LightningDataModule for CLEVRTEX dataset.\n\t    A DataModule implements 5 key methods:\n\t        def setup(self, stage):\n", "            # things to do on every process in DDP\n\t            # load data, set variables, etc...\n\t        def train_dataloader(self):\n\t            # return train dataloader\n\t        def val_dataloader(self):\n\t            # return validation dataloader\n\t        def test_dataloader(self):\n\t            # return test dataloader\n\t        def teardown(self):\n\t            # called on every process in DDP\n", "            # clean up after fit or test\n\t    This allows you to share a full dataset without explaining how to download,\n\t    split, transform and process the data.\n\t    Read the docs:\n\t        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        name: str = \"clvt\",\n\t        data_dir: str = \"data/\",\n", "        img_size: int = 128,\n\t        crop_size: int = 0,\n\t        batch_size: int = 64,\n\t        num_workers: int = 0,\n\t        pin_memory: bool = False,\n\t    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False)\n", "        # data transformations\n\t        transform_list = list()\n\t        if crop_size > 0:\n\t            transform_list.append(transforms.CenterCrop(crop_size))\n\t        transform_list.extend(\n\t            [\n\t                transforms.Resize((img_size, img_size)),\n\t            ]\n\t        )\n\t        self.transforms = transforms.Compose(transform_list)\n", "        self.data_train: Optional[Dataset] = None\n\t        self.data_val: Optional[Dataset] = None\n\t        self.data_test: Optional[Dataset] = None\n\t    def prepare_data(self):\n\t        \"\"\"Download data if needed.\n\t        Do not use it to assign state (self.x = y).\n\t        \"\"\"\n\t        pass\n\t    def setup(self, stage: Optional[str] = None):\n\t        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n", "        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n\t        careful not to execute things like random split twice!\n\t        \"\"\"\n\t        self.data_train = CLEVRTEX(\n\t            data_dir=self.hparams.data_dir,\n\t            img_size=self.hparams.img_size,\n\t            transform=self.transforms,\n\t            train=True,\n\t        )\n\t        self.data_val = CLEVRTEX(\n", "            data_dir=self.hparams.data_dir,\n\t            img_size=self.hparams.img_size,\n\t            transform=self.transforms,\n\t            train=False,\n\t        )\n\t    def train_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_train,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n", "            pin_memory=self.hparams.pin_memory,\n\t            shuffle=True,\n\t        )\n\t    def val_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n", "        )\n\t    def test_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n\t        )\n\t    def teardown(self, stage: Optional[str] = None):\n", "        \"\"\"Clean up after fit or test.\"\"\"\n\t        pass\n\t    def state_dict(self):\n\t        \"\"\"Extra things to save to checkpoint.\"\"\"\n\t        return {}\n\t    def load_state_dict(self, state_dict: Dict[str, Any]):\n\t        \"\"\"Things to do when loading checkpoint.\"\"\"\n\t        pass\n\tif __name__ == \"__main__\":\n\t    _ = CLEVRTEXDataModule()\n"]}
{"filename": "src/data/__init__.py", "chunked_list": []}
{"filename": "src/data/ptr_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\timport torch\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader, Dataset\n\tfrom torchvision.transforms import transforms\n\tfrom src.data.components.ptr import PTR\n\tclass PTRDataModule(LightningDataModule):\n\t    \"\"\"LightningDataModule for PTR dataset.\n\t    A DataModule implements 5 key methods:\n\t        def setup(self, stage):\n", "            # things to do on every process in DDP\n\t            # load data, set variables, etc...\n\t        def train_dataloader(self):\n\t            # return train dataloader\n\t        def val_dataloader(self):\n\t            # return validation dataloader\n\t        def test_dataloader(self):\n\t            # return test dataloader\n\t        def teardown(self):\n\t            # called on every process in DDP\n", "            # clean up after fit or test\n\t    This allows you to share a full dataset without explaining how to download,\n\t    split, transform and process the data.\n\t    Read the docs:\n\t        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        name: str = \"ptr\",\n\t        data_dir: str = \"data/\",\n", "        img_size: int = 128,\n\t        crop_size: int = 0,\n\t        batch_size: int = 64,\n\t        num_workers: int = 0,\n\t        pin_memory: bool = False,\n\t    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False)\n", "        # data transformations\n\t        transform_list = list()\n\t        if crop_size > 0:\n\t            transform_list.append(transforms.CenterCrop(crop_size))\n\t        transform_list.extend(\n\t            [\n\t                transforms.Resize((img_size, img_size)),\n\t            ]\n\t        )\n\t        self.transforms = transforms.Compose(transform_list)\n", "        self.data_train: Optional[Dataset] = None\n\t        self.data_val: Optional[Dataset] = None\n\t        self.data_test: Optional[Dataset] = None\n\t    def prepare_data(self):\n\t        \"\"\"Download data if needed.\n\t        Do not use it to assign state (self.x = y).\n\t        \"\"\"\n\t        pass\n\t    def setup(self, stage: Optional[str] = None):\n\t        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n", "        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n\t        careful not to execute things like random split twice!\n\t        \"\"\"\n\t        self.data_train = PTR(\n\t            data_dir=self.hparams.data_dir,\n\t            img_size=self.hparams.img_size,\n\t            transform=self.transforms,\n\t            train=True,\n\t        )\n\t        self.data_val = PTR(\n", "            data_dir=self.hparams.data_dir,\n\t            img_size=self.hparams.img_size,\n\t            transform=self.transforms,\n\t            train=False,\n\t        )\n\t    def train_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_train,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n", "            pin_memory=self.hparams.pin_memory,\n\t            shuffle=True,\n\t        )\n\t    def val_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n", "        )\n\t    def test_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n\t        )\n\t    def teardown(self, stage: Optional[str] = None):\n", "        \"\"\"Clean up after fit or test.\"\"\"\n\t        pass\n\t    def state_dict(self):\n\t        \"\"\"Extra things to save to checkpoint.\"\"\"\n\t        return {}\n\t    def load_state_dict(self, state_dict: Dict[str, Any]):\n\t        \"\"\"Things to do when loading checkpoint.\"\"\"\n\t        pass\n\tif __name__ == \"__main__\":\n\t    _ = PTRDataModule()\n"]}
{"filename": "src/data/clevr6_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\timport torch\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader, Dataset\n\tfrom torchvision.transforms import transforms\n\tfrom src.data.components.clevr6 import CLEVR6\n\tclass CLEVR6DataModule(LightningDataModule):\n\t    \"\"\"LightningDataModule for CLEVR6 dataset.\n\t    A DataModule implements 5 key methods:\n\t        def setup(self, stage):\n", "            # things to do on every process in DDP\n\t            # load data, set variables, etc...\n\t        def train_dataloader(self):\n\t            # return train dataloader\n\t        def val_dataloader(self):\n\t            # return validation dataloader\n\t        def test_dataloader(self):\n\t            # return test dataloader\n\t        def teardown(self):\n\t            # called on every process in DDP\n", "            # clean up after fit or test\n\t    This allows you to share a full dataset without explaining how to download,\n\t    split, transform and process the data.\n\t    Read the docs:\n\t        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        name: str = \"clv6\",\n\t        data_dir: str = \"data/\",\n", "        img_size: int = 128,\n\t        crop_size: int = 0,\n\t        batch_size: int = 64,\n\t        num_workers: int = 0,\n\t        pin_memory: bool = False,\n\t    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False)\n", "        # data transformations\n\t        transform_list = list()\n\t        if crop_size > 0:\n\t            transform_list.append(transforms.CenterCrop(crop_size))\n\t        transform_list.extend(\n\t            [\n\t                transforms.Resize((img_size, img_size)),\n\t            ]\n\t        )\n\t        self.transforms = transforms.Compose(transform_list)\n", "        self.data_train: Optional[Dataset] = None\n\t        self.data_val: Optional[Dataset] = None\n\t        self.data_test: Optional[Dataset] = None\n\t    def prepare_data(self):\n\t        \"\"\"Download data if needed.\n\t        Do not use it to assign state (self.x = y).\n\t        \"\"\"\n\t        pass\n\t    def setup(self, stage: Optional[str] = None):\n\t        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n", "        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n\t        careful not to execute things like random split twice!\n\t        \"\"\"\n\t        self.data_train = CLEVR6(\n\t            data_dir=self.hparams.data_dir,\n\t            img_size=self.hparams.img_size,\n\t            transform=self.transforms,\n\t            train=True,\n\t        )\n\t        self.data_val = CLEVR6(\n", "            data_dir=self.hparams.data_dir,\n\t            img_size=self.hparams.img_size,\n\t            transform=self.transforms,\n\t            train=False,\n\t        )\n\t    def train_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_train,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n", "            pin_memory=self.hparams.pin_memory,\n\t            shuffle=True,\n\t        )\n\t    def val_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n", "        )\n\t    def test_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n\t        )\n\t    def teardown(self, stage: Optional[str] = None):\n", "        \"\"\"Clean up after fit or test.\"\"\"\n\t        pass\n\t    def state_dict(self):\n\t        \"\"\"Extra things to save to checkpoint.\"\"\"\n\t        return {}\n\t    def load_state_dict(self, state_dict: Dict[str, Any]):\n\t        \"\"\"Things to do when loading checkpoint.\"\"\"\n\t        pass\n\tif __name__ == \"__main__\":\n\t    _ = CLEVR6DataModule()"]}
{"filename": "src/data/movi_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\timport torch\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader, Dataset\n\tfrom torchvision.transforms import transforms\n\tfrom src.data.components.movi import MOVi\n\tclass MOViDataModule(LightningDataModule):\n\t    \"\"\"LightningDataModule for MOVi dataset.\n\t    A DataModule implements 5 key methods:\n\t        def setup(self, stage):\n", "            # things to do on every process in DDP\n\t            # load data, set variables, etc...\n\t        def train_dataloader(self):\n\t            # return train dataloader\n\t        def val_dataloader(self):\n\t            # return validation dataloader\n\t        def test_dataloader(self):\n\t            # return test dataloader\n\t        def teardown(self):\n\t            # called on every process in DDP\n", "            # clean up after fit or test\n\t    This allows you to share a full dataset without explaining how to download,\n\t    split, transform and process the data.\n\t    Read the docs:\n\t        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        name: str = \"movi\",\n\t        data_dir: str = \"data/\",\n", "        img_size: int = 128,\n\t        crop_size: int = 0,\n\t        batch_size: int = 64,\n\t        num_workers: int = 0,\n\t        pin_memory: bool = False,\n\t    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False)\n", "        # data transformations\n\t        transform_list = list()\n\t        if crop_size > 0:\n\t            transform_list.append(transforms.CenterCrop(crop_size))\n\t        transform_list.extend(\n\t            [\n\t                transforms.Resize((img_size, img_size)),\n\t            ]\n\t        )\n\t        self.transforms = transforms.Compose(transform_list)\n", "        self.data_train: Optional[Dataset] = None\n\t        self.data_val: Optional[Dataset] = None\n\t        self.data_test: Optional[Dataset] = None\n\t    def prepare_data(self):\n\t        \"\"\"Download data if needed.\n\t        Do not use it to assign state (self.x = y).\n\t        \"\"\"\n\t        pass\n\t    def setup(self, stage: Optional[str] = None):\n\t        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n", "        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n\t        careful not to execute things like random split twice!\n\t        \"\"\"\n\t        self.data_train = MOVi(\n\t            data_dir=self.hparams.data_dir,\n\t            img_size=self.hparams.img_size,\n\t            transform=self.transforms,\n\t            train=True,\n\t        )\n\t        self.data_val = MOVi(\n", "            data_dir=self.hparams.data_dir,\n\t            img_size=self.hparams.img_size,\n\t            transform=self.transforms,\n\t            train=False,\n\t        )\n\t    def train_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_train,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n", "            pin_memory=self.hparams.pin_memory,\n\t            shuffle=True,\n\t        )\n\t    def val_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n", "        )\n\t    def test_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n\t        )\n\t    def teardown(self, stage: Optional[str] = None):\n", "        \"\"\"Clean up after fit or test.\"\"\"\n\t        pass\n\t    def state_dict(self):\n\t        \"\"\"Extra things to save to checkpoint.\"\"\"\n\t        return {}\n\t    def load_state_dict(self, state_dict: Dict[str, Any]):\n\t        \"\"\"Things to do when loading checkpoint.\"\"\"\n\t        pass\n\tif __name__ == \"__main__\":\n\t    _ = MOViDataModule()\n"]}
{"filename": "src/data/mnist_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\timport torch\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split\n\tfrom torchvision.datasets import MNIST\n\tfrom torchvision.transforms import transforms\n\tclass MNISTDataModule(LightningDataModule):\n\t    \"\"\"Example of LightningDataModule for MNIST dataset.\n\t    A DataModule implements 5 key methods:\n\t        def prepare_data(self):\n", "            # things to do on 1 GPU/TPU (not on every GPU/TPU in DDP)\n\t            # download data, pre-process, split, save to disk, etc...\n\t        def setup(self, stage):\n\t            # things to do on every process in DDP\n\t            # load data, set variables, etc...\n\t        def train_dataloader(self):\n\t            # return train dataloader\n\t        def val_dataloader(self):\n\t            # return validation dataloader\n\t        def test_dataloader(self):\n", "            # return test dataloader\n\t        def teardown(self):\n\t            # called on every process in DDP\n\t            # clean up after fit or test\n\t    This allows you to share a full dataset without explaining how to download,\n\t    split, transform and process the data.\n\t    Read the docs:\n\t        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n\t    \"\"\"\n\t    def __init__(\n", "        self,\n\t        data_dir: str = \"data/\",\n\t        train_val_test_split: Tuple[int, int, int] = (55_000, 5_000, 10_000),\n\t        batch_size: int = 64,\n\t        num_workers: int = 0,\n\t        pin_memory: bool = False,\n\t    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n", "        self.save_hyperparameters(logger=False)\n\t        # data transformations\n\t        self.transforms = transforms.Compose(\n\t            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n\t        )\n\t        self.data_train: Optional[Dataset] = None\n\t        self.data_val: Optional[Dataset] = None\n\t        self.data_test: Optional[Dataset] = None\n\t    @property\n\t    def num_classes(self):\n", "        return 10\n\t    def prepare_data(self):\n\t        \"\"\"Download data if needed.\n\t        Do not use it to assign state (self.x = y).\n\t        \"\"\"\n\t        MNIST(self.hparams.data_dir, train=True, download=True)\n\t        MNIST(self.hparams.data_dir, train=False, download=True)\n\t    def setup(self, stage: Optional[str] = None):\n\t        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\t        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n", "        careful not to execute things like random split twice!\n\t        \"\"\"\n\t        # load and split datasets only if not loaded already\n\t        if not self.data_train and not self.data_val and not self.data_test:\n\t            trainset = MNIST(self.hparams.data_dir, train=True, transform=self.transforms)\n\t            testset = MNIST(self.hparams.data_dir, train=False, transform=self.transforms)\n\t            dataset = ConcatDataset(datasets=[trainset, testset])\n\t            self.data_train, self.data_val, self.data_test = random_split(\n\t                dataset=dataset,\n\t                lengths=self.hparams.train_val_test_split,\n", "                generator=torch.Generator().manual_seed(42),\n\t            )\n\t    def train_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_train,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=True,\n\t        )\n", "    def val_dataloader(self):\n\t        return DataLoader(\n\t            dataset=self.data_val,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n\t        )\n\t    def test_dataloader(self):\n\t        return DataLoader(\n", "            dataset=self.data_test,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n\t            shuffle=False,\n\t        )\n\t    def teardown(self, stage: Optional[str] = None):\n\t        \"\"\"Clean up after fit or test.\"\"\"\n\t        pass\n\t    def state_dict(self):\n", "        \"\"\"Extra things to save to checkpoint.\"\"\"\n\t        return {}\n\t    def load_state_dict(self, state_dict: Dict[str, Any]):\n\t        \"\"\"Things to do when loading checkpoint.\"\"\"\n\t        pass\n\tif __name__ == \"__main__\":\n\t    _ = MNISTDataModule()\n"]}
{"filename": "src/data/components/clevrtex.py", "chunked_list": ["import json\n\timport os\n\tfrom collections import defaultdict\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.io import ImageReadMode, read_image\n\tfrom torchvision.transforms import transforms\n\tclass CLEVRTEX(Dataset):\n\t    def __init__(\n", "        self,\n\t        data_dir: str = \"data/clevrtex\",\n\t        img_size: int = 128,\n\t        transform: transforms.Compose = None,\n\t        train: bool = True,\n\t    ):\n\t        super().__init__()\n\t        self.img_size = img_size\n\t        self.train = train\n\t        self.stage = \"train\" if train else \"val\"\n", "        self.max_num_objs = 10\n\t        self.max_num_masks = self.max_num_objs + 1\n\t        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n\t        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n\t        self.scene_dir = os.path.join(data_dir, \"scenes\")\n\t        self.files = sorted(os.listdir(self.image_dir))\n\t        self.num_files = len(self.files)\n\t        self.transform = transform\n\t    def __getitem__(self, index):\n\t        image_filename = self.files[index]\n", "        img = (\n\t            read_image(os.path.join(self.image_dir, image_filename), ImageReadMode.RGB)\n\t            .float()\n\t            .div(255.0)\n\t        )\n\t        img = self.transform(img)\n\t        sample = {\"image\": img}\n\t        scene_name = image_filename[:-3] + \"json\"\n\t        metadata = json.load(open(os.path.join(self.scene_dir, self.stage, scene_name)))\n\t        if not self.train:\n", "            mask_filename = image_filename[:-4] + \"_flat.png\"\n\t            masks = read_image(os.path.join(self.mask_dir, mask_filename)).long().squeeze(0)\n\t            masks = F.one_hot(masks, self.max_num_masks).permute(2, 0, 1)\n\t            masks = self.transform(masks).unsqueeze(-1)\n\t            # masks: (max_num_masks, H, W, 1)\n\t            sample[\"masks\"] = masks.float()\n\t            sample[\"num_objects\"] = len(metadata[\"objects\"])\n\t        return sample\n\t    def __len__(self):\n\t        return self.num_files\n"]}
{"filename": "src/data/components/__init__.py", "chunked_list": []}
{"filename": "src/data/components/clevr6.py", "chunked_list": ["import json\n\timport os\n\tfrom collections import defaultdict\n\timport torch\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.io import ImageReadMode, read_image\n\tfrom torchvision.transforms import transforms\n\tclass CLEVR6(Dataset):\n\t    def __init__(\n\t        self,\n", "        data_dir: str = \"data/CLEVR6\",\n\t        img_size: int = 128,\n\t        transform: transforms.Compose = None,\n\t        train: bool = True,\n\t    ):\n\t        super().__init__()\n\t        self.img_size = img_size\n\t        self.train = train\n\t        self.stage = \"train\" if train else \"val\"\n\t        self.max_num_objs = 6\n", "        self.max_num_masks = self.max_num_objs + 1\n\t        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n\t        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n\t        self.scene_dir = os.path.join(data_dir, \"scenes\")\n\t        self.metadata = json.load(\n\t            open(os.path.join(self.scene_dir, f\"CLEVR_{self.stage}_scenes.json\"))\n\t        )\n\t        self.files = sorted(os.listdir(self.image_dir))\n\t        self.num_files = len(self.files)\n\t        self.transform = transform\n", "        if not train:\n\t            self.masks = defaultdict(list)\n\t            masks = sorted(os.listdir(self.mask_dir))\n\t            for mask in masks:\n\t                split = mask.split(\"_\")\n\t                filename = \"_\".join(split[:3]) + \".png\"\n\t                self.masks[filename].append(mask)\n\t            del masks\n\t    def __getitem__(self, index):\n\t        filename = self.metadata[\"scenes\"][index][\"image_filename\"]\n", "        img = (\n\t            read_image(os.path.join(self.image_dir, filename), ImageReadMode.RGB)\n\t            .float()\n\t            .div(255.0)\n\t        )\n\t        img = self.transform(img)\n\t        sample = {\"image\": img}\n\t        if not self.train:\n\t            masks = list()\n\t            for mask_filename in self.masks[filename]:\n", "                mask = (\n\t                    read_image(os.path.join(self.mask_dir, mask_filename), ImageReadMode.GRAY)\n\t                    .div(255)\n\t                    .long()\n\t                )\n\t                mask = self.transform(mask)\n\t                masks.append(mask)\n\t            masks = torch.cat(masks, dim=0).unsqueeze(-1)\n\t            # masks: (num_objects + 1, H, W, 1)\n\t            num_masks = masks.shape[0]\n", "            if num_masks < self.max_num_masks:\n\t                masks = torch.cat(\n\t                    (\n\t                        masks,\n\t                        torch.zeros(\n\t                            (self.max_num_masks - num_masks, self.img_size, self.img_size, 1)\n\t                        ),\n\t                    ),\n\t                    dim=0,\n\t                )\n", "            # masks: (max_num_masks, H, W, 1)\n\t            sample[\"masks\"] = masks.float()\n\t            sample[\"num_objects\"] = num_masks - 1\n\t        return sample\n\t    def __len__(self):\n\t        return self.num_files\n"]}
{"filename": "src/data/components/ptr.py", "chunked_list": ["import json\n\timport os\n\tfrom collections import defaultdict\n\timport torch\n\timport torch.nn.functional as F\n\tfrom pycocotools import mask as pycocotools_mask\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.io import ImageReadMode, read_image\n\tfrom torchvision.transforms import transforms\n\tclass PTR(Dataset):\n", "    def __init__(\n\t        self,\n\t        data_dir: str = \"data/PTR\",\n\t        img_size: int = 128,\n\t        transform: transforms.Compose = None,\n\t        train: bool = True,\n\t    ):\n\t        super().__init__()\n\t        self.max_num_objs = 6\n\t        self.max_num_masks = self.max_num_objs + 1\n", "        self.img_size = img_size\n\t        self.train = train\n\t        self.stage = \"train\" if train else \"val\"\n\t        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n\t        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n\t        self.scene_dir = os.path.join(data_dir, \"scenes\")\n\t        self.files = sorted(os.listdir(self.image_dir))\n\t        self.num_files = len(self.files)\n\t        self.transform = transform\n\t    def __getitem__(self, index):\n", "        image_filename = self.files[index]\n\t        img = (\n\t            read_image(os.path.join(self.image_dir, image_filename), ImageReadMode.RGB)\n\t            .float()\n\t            .div(255.0)\n\t        )\n\t        img = self.transform(img)\n\t        sample = {\"image\": img}\n\t        scene_name = image_filename[:-3] + \"json\"\n\t        metadata = json.load(open(os.path.join(self.scene_dir, self.stage, scene_name)))\n", "        if not self.train:\n\t            masks = list()\n\t            for obj in metadata[\"objects\"]:\n\t                masks.append(obj[\"obj_mask\"])\n\t            masks = torch.tensor(pycocotools_mask.decode(masks), dtype=torch.long)\n\t            masks = torch.einsum(\"hwn -> nhw\", masks)\n\t            masks = self.transform(masks.unsqueeze(1)).squeeze(1)\n\t            masks = torch.cat(\n\t                [(torch.sum(masks, dim=0, keepdim=True) == 0).long(), masks], dim=0\n\t            ).unsqueeze(-1)\n", "            # masks: (num_objects + 1, H, W, 1)\n\t            num_masks = masks.shape[0]\n\t            if num_masks < self.max_num_masks:\n\t                masks = torch.cat(\n\t                    (\n\t                        masks,\n\t                        torch.zeros(\n\t                            (self.max_num_masks - num_masks, self.img_size, self.img_size, 1)\n\t                        ),\n\t                    ),\n", "                    dim=0,\n\t                )\n\t            # masks: (max_num_masks, H, W, 1)\n\t            sample[\"masks\"] = masks.float()\n\t            sample[\"num_objects\"] = num_masks - 1\n\t        return sample\n\t    def __len__(self):\n\t        return self.num_files\n"]}
{"filename": "src/data/components/movi.py", "chunked_list": ["import json\n\timport os\n\tfrom collections import defaultdict\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.io import ImageReadMode, read_image\n\tfrom torchvision.transforms import transforms\n\tclass MOVi(Dataset):\n\t    def __init__(\n", "        self,\n\t        data_dir: str = \"data/MOVi\",\n\t        img_size: int = 128,\n\t        transform: transforms.Compose = None,\n\t        train: bool = True,\n\t    ):\n\t        super().__init__()\n\t        self.img_size = img_size\n\t        self.train = train\n\t        self.stage = \"train\" if train else \"val\"\n", "        self.max_num_objs = 6\n\t        self.max_num_masks = self.max_num_objs + 1\n\t        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n\t        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n\t        self.scene_dir = os.path.join(data_dir, \"scenes\")\n\t        self.files = sorted(os.listdir(self.image_dir))\n\t        self.num_files = len(self.files)\n\t        self.transform = transform\n\t    def __getitem__(self, index):\n\t        image_filename = self.files[index]\n", "        img = (\n\t            read_image(os.path.join(self.image_dir, image_filename), ImageReadMode.RGB)\n\t            .float()\n\t            .div(255.0)\n\t        )\n\t        img = self.transform(img)\n\t        sample = {\"image\": img}\n\t        scene_name = image_filename[:-3] + \"json\"\n\t        metadata = json.load(open(os.path.join(self.scene_dir, self.stage, scene_name)))\n\t        if not self.train:\n", "            masks = read_image(os.path.join(self.mask_dir, image_filename)).long().squeeze(0)\n\t            masks = F.one_hot(masks, self.max_num_masks).permute(2, 0, 1)\n\t            masks = self.transform(masks).unsqueeze(-1)\n\t            # masks: (max_num_masks, H, W, 1)\n\t            sample[\"masks\"] = masks.float()\n\t            sample[\"num_objects\"] = len(metadata[\"instances\"])\n\t        return sample\n\t    def __len__(self):\n\t        return self.num_files\n"]}
{"filename": "src/models/iodine_module.py", "chunked_list": ["\"\"\"\n\tsource: https://github.com/karazijal/clevrtex/blob/master/experiments/iodine.py\n\t\"\"\"\n\tfrom typing import Any, List\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision\n\timport wandb\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import LightningModule\n", "from torchmetrics import MaxMetric, MeanMetric\n\tfrom utils.evaluator import ARIEvaluator, mIoUEvaluator\n\tfrom src.utils.vis_utils import visualize\n\tclass LitIODINE(LightningModule):\n\t    def __init__(\n\t        self,\n\t        net: torch.nn.Module,\n\t        optimizer: torch.optim.Optimizer,\n\t        scheduler: DictConfig,  # torch.optim.lr_scheduler,\n\t        name: str = \"iodine\",\n", "    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\t        self.net = net\n\t        # metric objects for calculating and averaging accuracy across batches\n\t        self.train_fg_ari = ARIEvaluator()\n\t        self.train_ari = ARIEvaluator()\n\t        self.train_miou = mIoUEvaluator()\n", "        self.val_mse = MeanMetric()\n\t        self.val_fg_ari = ARIEvaluator()\n\t        self.val_ari = ARIEvaluator()\n\t        self.val_miou = mIoUEvaluator()\n\t        self.val_fg_ari_best = MaxMetric()\n\t        self.val_ari_best = MaxMetric()\n\t        self.val_miou_best = MaxMetric()\n\t        # for averaging loss across batches\n\t        self.train_loss = MeanMetric()\n\t        self.train_kl = MeanMetric()\n", "        self.train_recon_loss = MeanMetric()\n\t    def forward(self, x: torch.Tensor):\n\t        outputs = self.net(x)\n\t        return outputs\n\t    def on_train_start(self):\n\t        # by default lightning executes validation step sanity checks before training starts,\n\t        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n\t        self.val_fg_ari_best.reset()\n\t        self.val_ari_best.reset()\n\t        self.val_miou_best.reset()\n", "    def model_step(self, batch: Any):\n\t        img = batch[\"image\"]\n\t        outputs = self.net(img)\n\t        loss = outputs[\"loss\"]\n\t        return loss, outputs\n\t    def training_step(self, batch, batch_idx):\n\t        loss, outputs = self.model_step(batch)\n\t        self.train_loss(loss)\n\t        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.train_kl(outputs[\"kl\"])\n", "        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.train_recon_loss(outputs[\"recon_loss\"])\n\t        self.log(\n\t            \"train/recon_loss\", self.train_recon_loss, on_step=False, on_epoch=True, prog_bar=True\n\t        )\n\t        return {\"loss\": loss}\n\t    def validation_step(self, batch, batch_idx, dataloader_idx=None):\n\t        _, outputs = self.model_step(batch)\n\t        # Need to edit\n\t        self.val_mse(\n", "            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n\t        )\n\t        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n\t        masks = batch[\"masks\"].squeeze(-1)\n\t        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\t        self.val_miou.evaluate(pred_masks, masks)\n\t        self.val_ari.evaluate(pred_masks, masks)\n\t        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\t        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"patch\"])\n\t        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n", "        B, K, H, W, _ = pred_masks.shape\n\t        if batch_idx == 0:\n\t            n_sampels = 4\n\t            wandb_img_list = list()\n\t            for vis_idx in range(n_sampels):\n\t                vis = visualize(\n\t                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n\t                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n\t                    recons=recons[vis_idx].unsqueeze(0),\n\t                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n", "                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n\t                    attns=torch.zeros(\n\t                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n\t                    ),  # dummy attns\n\t                    colored_box=True,\n\t                )\n\t                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n\t                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n\t                wandb_img_list.append(wandb_img)\n\t            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n", "        return None\n\t    def validation_epoch_end(self, outputs: List[Any]):\n\t        val_fg_ari = self.val_fg_ari.get_results()\n\t        self.val_fg_ari.reset()\n\t        val_ari = self.val_ari.get_results()\n\t        self.val_ari.reset()\n\t        val_miou = self.val_miou.get_results()\n\t        self.val_miou.reset()\n\t        self.val_fg_ari_best(val_fg_ari)\n\t        self.val_ari_best(val_ari)\n", "        self.val_miou_best(val_miou)\n\t        self.log_dict(\n\t            {\n\t                \"val/fg-ari\": val_fg_ari,\n\t                \"val/ari\": val_ari,\n\t                \"val/miou\": val_miou,\n\t                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n\t                \"val/ari_best\": self.val_ari_best.compute(),\n\t                \"val/miou_best\": self.val_miou_best.compute(),\n\t            },\n", "            prog_bar=True,\n\t        )\n\t    def test_step(self, batch: Any, batch_idx: int):\n\t        self.validation_step(batch, batch_idx)\n\t    def test_epoch_end(self, outputs: List[Any]):\n\t        self.validation_epoch_end(outputs)\n\t    def configure_optimizers(self):\n\t        optimizer = self.hparams.optimizer(params=self.parameters())\n\t        if self.hparams.scheduler is not None:\n\t            scheduler = self.hparams.scheduler.scheduler(\n", "                optimizer=optimizer,\n\t            )\n\t            return {\n\t                \"optimizer\": optimizer,\n\t                \"lr_scheduler\": {\n\t                    \"scheduler\": scheduler,\n\t                    \"monitor\": \"val/loss\",\n\t                    \"interval\": \"epoch\",\n\t                    \"frequency\": 1,\n\t                },\n", "            }\n\t        return {\"optimizer\": optimizer}\n\tif __name__ == \"__main__\":\n\t    _ = LitIODINE(None, None, None)\n"]}
{"filename": "src/models/monet_module.py", "chunked_list": ["\"\"\"https://github.com/karazijal/clevrtex/blob/master/experiments/monet.py.\"\"\"\n\tfrom typing import Any, List\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision\n\timport wandb\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import LightningModule\n\tfrom torchmetrics import MaxMetric, MeanMetric\n\tfrom utils.evaluator import ARIEvaluator, mIoUEvaluator\n", "from src.utils.vis_utils import visualize\n\tclass LitMONet(LightningModule):\n\t    \"\"\"LightningModule for SlotAttentionAutoEncoder.\n\t    A LightningModule organizes your PyTorch code into 6 sections:\n\t        - Computations (init)\n\t        - Train loop (training_step)\n\t        - Validation loop (validation_step)\n\t        - Prediction Loop (predict_step)\n\t        - Optimizers and LR Schedulers (configure_optimizers)\n\t    Docs:\n", "        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        net: torch.nn.Module,\n\t        optimizer: torch.optim.Optimizer,\n\t        scheduler: DictConfig,  # torch.optim.lr_scheduler,\n\t        name: str = \"monet\",\n\t    ):\n\t        super().__init__()\n", "        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\t        self.net = net\n\t        # metric objects for calculating and averaging accuracy across batches\n\t        self.train_fg_ari = ARIEvaluator()\n\t        self.train_ari = ARIEvaluator()\n\t        self.train_miou = mIoUEvaluator()\n\t        self.val_mse = MeanMetric()\n\t        self.val_fg_ari = ARIEvaluator()\n", "        self.val_ari = ARIEvaluator()\n\t        self.val_miou = mIoUEvaluator()\n\t        self.val_fg_ari_best = MaxMetric()\n\t        self.val_ari_best = MaxMetric()\n\t        self.val_miou_best = MaxMetric()\n\t        # for averaging loss across batches\n\t        self.train_loss = MeanMetric()\n\t        self.train_kl = MeanMetric()\n\t        self.train_kl_mask = MeanMetric()\n\t        self.train_loss_comp_ratio = MeanMetric()\n", "        self.val_mse = MeanMetric()\n\t    def forward(self, x: torch.Tensor):\n\t        outputs = self.net(x, train=False)\n\t        return outputs\n\t    def on_train_start(self):\n\t        # by default lightning executes validation step sanity checks before training starts,\n\t        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n\t        self.val_fg_ari_best.reset()\n\t        self.val_ari_best.reset()\n\t        self.val_miou_best.reset()\n", "    def model_step(self, batch: Any, train: bool = False):\n\t        img = batch[\"image\"]\n\t        outputs = self.net(img, train)\n\t        if train:\n\t            loss = outputs[\"loss\"]\n\t        else:\n\t            loss = None\n\t        return loss, outputs\n\t    def training_step(self, batch, batch_idx):\n\t        loss, outputs = self.model_step(batch, train=True)\n", "        self.train_loss(loss)\n\t        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.train_kl(outputs[\"kl\"])\n\t        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.train_kl_mask(outputs[\"kl_mask\"])\n\t        self.log(\"train/kl_mask\", self.train_kl_mask, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.train_loss_comp_ratio(outputs[\"rec_loss\"] / outputs[\"kl\"])\n\t        self.log(\n\t            \"train/loss_comp_ratio\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True\n\t        )\n", "        return {\"loss\": loss}\n\t    def training_epoch_end(self, outputs: List[Any]):\n\t        # `outputs` is a list of dicts returned from `training_step()`\n\t        pass\n\t    def validation_step(self, batch: Any, batch_idx: int):\n\t        _, outputs = self.model_step(batch, train=False)\n\t        self.val_mse(\n\t            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n\t        )\n\t        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n", "        masks = batch[\"masks\"].squeeze(-1)\n\t        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\t        self.val_miou.evaluate(pred_masks, masks)\n\t        self.val_ari.evaluate(pred_masks, masks)\n\t        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\t        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"recons\"])\n\t        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n\t        B, K, H, W, _ = pred_masks.shape\n\t        if batch_idx == 0:\n\t            n_sampels = 4\n", "            wandb_img_list = list()\n\t            for vis_idx in range(n_sampels):\n\t                vis = visualize(\n\t                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n\t                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n\t                    recons=recons[vis_idx].unsqueeze(0),\n\t                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n\t                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n\t                    attns=torch.zeros(\n\t                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n", "                    ),  # dummy attns\n\t                    colored_box=True,\n\t                )\n\t                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n\t                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n\t                wandb_img_list.append(wandb_img)\n\t            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\t        return None\n\t    def validation_epoch_end(self, outputs: List[Any]):\n\t        val_fg_ari = self.val_fg_ari.get_results()\n", "        self.val_fg_ari.reset()\n\t        val_ari = self.val_ari.get_results()\n\t        self.val_ari.reset()\n\t        val_miou = self.val_miou.get_results()\n\t        self.val_miou.reset()\n\t        self.val_fg_ari_best(val_fg_ari)\n\t        self.val_ari_best(val_ari)\n\t        self.val_miou_best(val_miou)\n\t        self.log_dict(\n\t            {\n", "                \"val/fg-ari\": val_fg_ari,\n\t                \"val/ari\": val_ari,\n\t                \"val/miou\": val_miou,\n\t                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n\t                \"val/ari_best\": self.val_ari_best.compute(),\n\t                \"val/miou_best\": self.val_miou_best.compute(),\n\t            },\n\t            prog_bar=True,\n\t        )\n\t    def test_step(self, batch: Any, batch_idx: int):\n", "        self.validation_step(batch, batch_idx)\n\t    def test_epoch_end(self, outputs: List[Any]):\n\t        self.validation_epoch_end(outputs)\n\t    def configure_optimizers(self):\n\t        optimizer = self.hparams.optimizer(params=self.parameters())\n\t        if self.hparams.scheduler is not None:\n\t            scheduler = self.hparams.scheduler.scheduler(\n\t                optimizer=optimizer,\n\t            )\n\t            return {\n", "                \"optimizer\": optimizer,\n\t                \"lr_scheduler\": {\n\t                    \"scheduler\": scheduler,\n\t                    \"monitor\": \"val/loss\",\n\t                    \"interval\": \"epoch\",\n\t                    \"frequency\": 1,\n\t                },\n\t            }\n\t        return {\"optimizer\": optimizer}\n\tif __name__ == \"__main__\":\n", "    _ = LitMONet(None, None, None)\n"]}
{"filename": "src/models/genesis2_module.py", "chunked_list": ["\"\"\"\n\tsource: https://github.com/karazijal/clevrtex/blob/master/experiments/genesisv2.py\n\t\"\"\"\n\tfrom typing import Any, List\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision\n\timport wandb\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import LightningModule\n", "from torchmetrics import MaxMetric, MeanMetric\n\tfrom utils.evaluator import ARIEvaluator, mIoUEvaluator\n\tfrom src.utils.vis_utils import visualize\n\tclass LitGenesis2(LightningModule):\n\t    def __init__(\n\t        self,\n\t        net: torch.nn.Module,\n\t        optimizer: torch.optim.Optimizer,\n\t        scheduler: torch.optim.lr_scheduler,\n\t        name: str = \"gen2\",\n", "    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\t        self.net = net\n\t        # metric objects for calculating and averaging accuracy across batches\n\t        self.train_fg_ari = ARIEvaluator()\n\t        self.train_ari = ARIEvaluator()\n\t        self.train_miou = mIoUEvaluator()\n", "        self.val_mse = MeanMetric()\n\t        self.val_fg_ari = ARIEvaluator()\n\t        self.val_ari = ARIEvaluator()\n\t        self.val_miou = mIoUEvaluator()\n\t        self.val_fg_ari_best = MaxMetric()\n\t        self.val_ari_best = MaxMetric()\n\t        self.val_miou_best = MaxMetric()\n\t        # for averaging loss across batches\n\t        self.train_loss = MeanMetric()\n\t        self.train_elbo = MeanMetric()\n", "        self.train_kl = MeanMetric()\n\t        self.train_loss_comp_ratio = MeanMetric()\n\t        self.train_recon_loss = MeanMetric()\n\t    def forward(self, x: torch.Tensor):\n\t        outputs = self.net(x)\n\t        return outputs\n\t    def on_train_start(self):\n\t        # by default lightning executes validation step sanity checks before training starts,\n\t        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n\t        self.val_fg_ari_best.reset()\n", "        self.val_ari_best.reset()\n\t        self.val_miou_best.reset()\n\t    def model_step(self, batch: Any):\n\t        img = batch[\"image\"]\n\t        outputs = self.net(img)\n\t        loss = outputs[\"loss\"]\n\t        return loss, outputs\n\t    def training_step(self, batch, batch_idx):\n\t        loss, outputs = self.model_step(batch)\n\t        self.train_loss(loss)\n", "        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.train_elbo(outputs[\"elbo\"])\n\t        self.log(\"train/elbo\", self.train_elbo, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.train_kl(outputs[\"kl\"])\n\t        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.train_loss_comp_ratio(outputs[\"rec_loss\"] / outputs[\"kl\"])\n\t        self.log(\n\t            \"train/loss_comp_ratio\",\n\t            self.train_loss_comp_ratio,\n\t            on_step=False,\n", "            on_epoch=True,\n\t            prog_bar=True,\n\t        )\n\t        self.train_recon_loss(outputs[\"rec_loss\"])\n\t        self.log(\n\t            \"train/recon_loss\", self.train_recon_loss, on_step=False, on_epoch=True, prog_bar=True\n\t        )\n\t        return {\"loss\": loss}\n\t    def training_epoch_end(self, outputs: List[Any]):\n\t        # `outputs` is a list of dicts returned from `training_step()`\n", "        pass\n\t    def validation_step(self, batch, batch_idx, dataloader_idx=None):\n\t        _, outputs = self.model_step(batch)\n\t        self.val_mse(\n\t            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n\t        )\n\t        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n\t        masks = batch[\"masks\"].squeeze(-1)\n\t        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\t        self.val_miou.evaluate(pred_masks, masks)\n", "        self.val_ari.evaluate(pred_masks, masks)\n\t        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\t        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"patch\"])\n\t        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n\t        B, K, H, W, _ = pred_masks.shape\n\t        if batch_idx == 0:\n\t            n_sampels = 4\n\t            wandb_img_list = list()\n\t            for vis_idx in range(n_sampels):\n\t                vis = visualize(\n", "                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n\t                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n\t                    recons=recons[vis_idx].unsqueeze(0),\n\t                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n\t                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n\t                    attns=torch.zeros(\n\t                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n\t                    ),  # dummy attns\n\t                    colored_box=True,\n\t                )\n", "                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n\t                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n\t                wandb_img_list.append(wandb_img)\n\t            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\t        return None\n\t    def validation_epoch_end(self, outputs: List[Any]):\n\t        val_fg_ari = self.val_fg_ari.get_results()\n\t        self.val_fg_ari.reset()\n\t        val_ari = self.val_ari.get_results()\n\t        self.val_ari.reset()\n", "        val_miou = self.val_miou.get_results()\n\t        self.val_miou.reset()\n\t        self.val_fg_ari_best(val_fg_ari)\n\t        self.val_ari_best(val_ari)\n\t        self.val_miou_best(val_miou)\n\t        self.log_dict(\n\t            {\n\t                \"val/fg-ari\": val_fg_ari,\n\t                \"val/ari\": val_ari,\n\t                \"val/miou\": val_miou,\n", "                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n\t                \"val/ari_best\": self.val_ari_best.compute(),\n\t                \"val/miou_best\": self.val_miou_best.compute(),\n\t            },\n\t            prog_bar=True,\n\t        )\n\t    def test_step(self, batch: Any, batch_idx: int):\n\t        self.validation_step(batch, batch_idx)\n\t    def test_epoch_end(self, outputs: List[Any]):\n\t        self.validation_epoch_end(outputs)\n", "    def configure_optimizers(self):\n\t        optimizer = self.hparams.optimizer(params=self.parameters())\n\t        if self.hparams.scheduler is not None:\n\t            scheduler = self.hparams.scheduler(\n\t                optimizer=optimizer,\n\t            )\n\t            return {\n\t                \"optimizer\": optimizer,\n\t                \"lr_scheduler\": {\n\t                    \"scheduler\": scheduler,\n", "                    \"monitor\": \"val/loss\",\n\t                    \"interval\": \"epoch\",\n\t                    \"frequency\": 1,\n\t                },\n\t            }\n\t        return {\"optimizer\": optimizer}\n\tif __name__ == \"__main__\":\n\t    _ = LitGenesis2(None, None, None)\n"]}
{"filename": "src/models/__init__.py", "chunked_list": []}
{"filename": "src/models/slota_ae_module.py", "chunked_list": ["from typing import Any, List\n\timport torch\n\timport torchvision\n\timport wandb\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import LightningModule\n\tfrom torchmetrics import MaxMetric, MeanMetric\n\tfrom utils.evaluator import ARIEvaluator, mIoUEvaluator\n\tfrom src.utils.vis_utils import visualize\n\t# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n", "class LitSlotAttentionAutoEncoder(LightningModule):\n\t    \"\"\"LightningModule for SlotAttentionAutoEncoder.\n\t    A LightningModule organizes your PyTorch code into 6 sections:\n\t        - Computations (init)\n\t        - Train loop (training_step)\n\t        - Validation loop (validation_step)\n\t        - Prediction Loop (predict_step)\n\t        - Optimizers and LR Schedulers (configure_optimizers)\n\t    Docs:\n\t        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n", "    \"\"\"\n\t    def __init__(\n\t        self,\n\t        net: torch.nn.Module,\n\t        optimizer: torch.optim.Optimizer,\n\t        scheduler: DictConfig,  # torch.optim.lr_scheduler,\n\t        name: str = \"slota\",\n\t    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n", "        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\t        self.net = net\n\t        # loss function\n\t        self.criterion = torch.nn.MSELoss()\n\t        # metric objects for calculating and averaging accuracy across batches\n\t        self.train_fg_ari = ARIEvaluator()\n\t        self.val_fg_ari = ARIEvaluator()\n\t        self.train_ari = ARIEvaluator()\n\t        self.val_ari = ARIEvaluator()\n", "        self.train_miou = mIoUEvaluator()\n\t        self.val_miou = mIoUEvaluator()\n\t        # for averaging loss across batches\n\t        self.train_loss = MeanMetric()\n\t        self.val_loss = MeanMetric()\n\t        # for tracking best so far validation accuracy\n\t        self.val_fg_ari_best = MaxMetric()\n\t        self.val_ari_best = MaxMetric()\n\t        self.val_miou_best = MaxMetric()\n\t    def forward(self, x: torch.Tensor, train: bool = False):\n", "        outputs = self.net(x, train=train)\n\t        return outputs\n\t    def on_train_start(self):\n\t        # by default lightning executes validation step sanity checks before training starts,\n\t        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n\t        self.val_fg_ari_best.reset()\n\t        self.val_ari_best.reset()\n\t        self.val_miou_best.reset()\n\t    def model_step(self, batch: Any, train: bool = False):\n\t        img = batch[\"image\"]\n", "        outputs = self.forward(img, train=train)\n\t        loss = self.criterion(outputs[\"recon_combined\"], img)\n\t        return loss, outputs\n\t    def training_step(self, batch: Any, batch_idx: int):\n\t        loss, outputs = self.model_step(batch, train=True)\n\t        # update and log metrics\n\t        self.train_loss(loss)\n\t        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\t        # we can return here dict with any tensors\n\t        # and then read it in some callback or in `training_epoch_end()` below\n", "        # remember to always return loss from `training_step()` or backpropagation will fail!\n\t        return {\"loss\": loss}\n\t    def training_epoch_end(self, outputs: List[Any]):\n\t        # `outputs` is a list of dicts returned from `training_step()`\n\t        pass\n\t    def validation_step(self, batch: Any, batch_idx: int):\n\t        loss, outputs = self.model_step(batch, train=False)\n\t        # update and log metrics\n\t        self.val_loss(loss)\n\t        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n", "        self.val_fg_ari.evaluate(outputs[\"masks\"].squeeze(-1), batch[\"masks\"][:, 1:].squeeze(-1))\n\t        self.val_ari.evaluate(outputs[\"masks\"].squeeze(-1), batch[\"masks\"].squeeze(-1))\n\t        self.val_miou.evaluate(outputs[\"masks\"].squeeze(-1), batch[\"masks\"].squeeze(-1))\n\t        if batch_idx == 0:\n\t            n_sampels = 4\n\t            wandb_img_list = list()\n\t            for vis_idx in range(n_sampels):\n\t                vis = visualize(\n\t                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n\t                    recon_combined=outputs[\"recon_combined\"][vis_idx].unsqueeze(0),\n", "                    recons=outputs[\"recons\"][vis_idx].unsqueeze(0),\n\t                    pred_masks=outputs[\"masks\"][vis_idx].unsqueeze(0),\n\t                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n\t                    attns=outputs[\"attns\"][vis_idx, -1:].unsqueeze(0),\n\t                    colored_box=True,\n\t                )\n\t                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n\t                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n\t                wandb_img_list.append(wandb_img)\n\t            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n", "        return {\"loss\": loss}\n\t    def validation_epoch_end(self, outputs: List[Any]):\n\t        val_fg_ari = self.val_fg_ari.get_results()\n\t        self.val_fg_ari.reset()\n\t        val_ari = self.val_ari.get_results()\n\t        self.val_ari.reset()\n\t        val_miou = self.val_miou.get_results()\n\t        self.val_miou.reset()\n\t        self.val_fg_ari_best(val_fg_ari)\n\t        self.val_ari_best(val_ari)\n", "        self.val_miou_best(val_miou)\n\t        self.log_dict(\n\t            {\n\t                \"val/fg-ari\": val_fg_ari,\n\t                \"val/ari\": val_ari,\n\t                \"val/miou\": val_miou,\n\t                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n\t                \"val/ari_best\": self.val_ari_best.compute(),\n\t                \"val/miou_best\": self.val_miou_best.compute(),\n\t            },\n", "            prog_bar=True,\n\t        )\n\t    def test_step(self, batch: Any, batch_idx: int):\n\t        self.validation_step(batch, batch_idx)\n\t    def test_epoch_end(self, outputs: List[Any]):\n\t        self.validation_epoch_end(outputs)\n\t    def configure_optimizers(self):\n\t        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n\t        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n\t        Examples:\n", "            https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n\t        \"\"\"\n\t        optimizer = self.hparams.optimizer(params=self.parameters())\n\t        if self.hparams.scheduler is not None:\n\t            def lr_lambda(step):\n\t                if step < self.hparams.scheduler.warmup_steps:\n\t                    warmup_factor = float(step) / float(\n\t                        max(1.0, self.hparams.scheduler.warmup_steps)\n\t                    )\n\t                else:\n", "                    warmup_factor = 1.0\n\t                decay_factor = self.hparams.scheduler.decay_rate ** (\n\t                    step / self.hparams.scheduler.decay_steps\n\t                )\n\t                return warmup_factor * decay_factor\n\t            scheduler = self.hparams.scheduler.scheduler(\n\t                optimizer=optimizer,\n\t                lr_lambda=lr_lambda,\n\t            )\n\t            return {\n", "                \"optimizer\": optimizer,\n\t                \"lr_scheduler\": {\n\t                    \"scheduler\": scheduler,\n\t                    \"monitor\": \"val/loss\",\n\t                    \"interval\": \"epoch\",\n\t                    \"frequency\": 1,\n\t                },\n\t            }\n\t        return {\"optimizer\": optimizer}\n\tif __name__ == \"__main__\":\n", "    _ = LitSlotAttentionAutoEncoder(None, None, None)\n"]}
{"filename": "src/models/mnist_module.py", "chunked_list": ["from typing import Any, List\n\timport torch\n\tfrom pytorch_lightning import LightningModule\n\tfrom torchmetrics import MaxMetric, MeanMetric\n\tfrom torchmetrics.classification.accuracy import Accuracy\n\tclass MNISTLitModule(LightningModule):\n\t    \"\"\"Example of LightningModule for MNIST classification.\n\t    A LightningModule organizes your PyTorch code into 6 sections:\n\t        - Computations (init)\n\t        - Train loop (training_step)\n", "        - Validation loop (validation_step)\n\t        - Test loop (test_step)\n\t        - Prediction Loop (predict_step)\n\t        - Optimizers and LR Schedulers (configure_optimizers)\n\t    Docs:\n\t        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        net: torch.nn.Module,\n", "        optimizer: torch.optim.Optimizer,\n\t        scheduler: torch.optim.lr_scheduler,\n\t    ):\n\t        super().__init__()\n\t        # this line allows to access init params with 'self.hparams' attribute\n\t        # also ensures init params will be stored in ckpt\n\t        self.save_hyperparameters(logger=False)\n\t        self.net = net\n\t        # loss function\n\t        self.criterion = torch.nn.CrossEntropyLoss()\n", "        # metric objects for calculating and averaging accuracy across batches\n\t        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n\t        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n\t        self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n\t        # for averaging loss across batches\n\t        self.train_loss = MeanMetric()\n\t        self.val_loss = MeanMetric()\n\t        self.test_loss = MeanMetric()\n\t        # for tracking best so far validation accuracy\n\t        self.val_acc_best = MaxMetric()\n", "    def forward(self, x: torch.Tensor):\n\t        return self.net(x)\n\t    def on_train_start(self):\n\t        # by default lightning executes validation step sanity checks before training starts,\n\t        # so it's worth to make sure validation metrics don't store results from these checks\n\t        self.val_loss.reset()\n\t        self.val_acc.reset()\n\t        self.val_acc_best.reset()\n\t    def model_step(self, batch: Any):\n\t        x, y = batch\n", "        logits = self.forward(x)\n\t        loss = self.criterion(logits, y)\n\t        preds = torch.argmax(logits, dim=1)\n\t        return loss, preds, y\n\t    def training_step(self, batch: Any, batch_idx: int):\n\t        loss, preds, targets = self.model_step(batch)\n\t        # update and log metrics\n\t        self.train_loss(loss)\n\t        self.train_acc(preds, targets)\n\t        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n", "        self.log(\"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n\t        # we can return here dict with any tensors\n\t        # and then read it in some callback or in `training_epoch_end()` below\n\t        # remember to always return loss from `training_step()` or backpropagation will fail!\n\t        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\t    def training_epoch_end(self, outputs: List[Any]):\n\t        # `outputs` is a list of dicts returned from `training_step()`\n\t        # Warning: when overriding `training_epoch_end()`, lightning accumulates outputs from all batches of the epoch\n\t        # this may not be an issue when training on mnist\n\t        # but on larger datasets/models it's easy to run into out-of-memory errors\n", "        # consider detaching tensors before returning them from `training_step()`\n\t        # or using `on_train_epoch_end()` instead which doesn't accumulate outputs\n\t        pass\n\t    def validation_step(self, batch: Any, batch_idx: int):\n\t        loss, preds, targets = self.model_step(batch)\n\t        # update and log metrics\n\t        self.val_loss(loss)\n\t        self.val_acc(preds, targets)\n\t        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n", "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\t    def validation_epoch_end(self, outputs: List[Any]):\n\t        acc = self.val_acc.compute()  # get current val acc\n\t        self.val_acc_best(acc)  # update best so far val acc\n\t        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n\t        # otherwise metric would be reset by lightning after each epoch\n\t        self.log(\"val/acc_best\", self.val_acc_best.compute(), prog_bar=True)\n\t    def test_step(self, batch: Any, batch_idx: int):\n\t        loss, preds, targets = self.model_step(batch)\n\t        # update and log metrics\n", "        self.test_loss(loss)\n\t        self.test_acc(preds, targets)\n\t        self.log(\"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)\n\t        self.log(\"test/acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True)\n\t        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\t    def test_epoch_end(self, outputs: List[Any]):\n\t        pass\n\t    def configure_optimizers(self):\n\t        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n\t        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n", "        Examples:\n\t            https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n\t        \"\"\"\n\t        optimizer = self.hparams.optimizer(params=self.parameters())\n\t        if self.hparams.scheduler is not None:\n\t            scheduler = self.hparams.scheduler(optimizer=optimizer)\n\t            return {\n\t                \"optimizer\": optimizer,\n\t                \"lr_scheduler\": {\n\t                    \"scheduler\": scheduler,\n", "                    \"monitor\": \"val/loss\",\n\t                    \"interval\": \"epoch\",\n\t                    \"frequency\": 1,\n\t                },\n\t            }\n\t        return {\"optimizer\": optimizer}\n\tif __name__ == \"__main__\":\n\t    _ = MNISTLitModule(None, None, None)\n"]}
{"filename": "src/models/components/__init__.py", "chunked_list": []}
{"filename": "src/models/components/iodine.py", "chunked_list": ["\"\"\"\n\tsource: https://github.com/karazijal/clevrtex/blob/master/ool/picture/models/iodine.py\n\tImplementation of IODINE from\n\t\"Multi-Object Representation Learning with Iterative Variational Inference\"\n\tKlaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess,\n\tDaniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner\n\thttps://arxiv.org/abs/1903.00450\n\tThis (re)-implemetation is draws from re-implementations of\n\thttps://github.com/zhixuan-lin/IODINE\n\thttps://github.com/MichaelKevinKelly/IODINE/\n", "https://github.com/pemami4911/IODINE.pytorch/\n\tand is based on official code\n\thttps://github.com/deepmind/deepmind-research/tree/master/iodine\n\t\"\"\"\n\timport math\n\tfrom typing import Union\n\timport torch\n\timport torch.distributions as dist\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n", "from scipy.stats import truncnorm\n\tfrom torch.nn import init\n\tdef truncated_normal_initializer(shape, mean, stddev):\n\t    # compute threshold at 2 std devs\n\t    values = truncnorm.rvs(mean - 2 * stddev, mean + 2 * stddev, size=shape)\n\t    return torch.from_numpy(values).float()\n\tdef init_weights(net, init_type=\"normal\", init_gain=0.02):\n\t    \"\"\"Initialize network weights.\n\t    Modified from: https://github.com/baudm/MONet-pytorch/blob/master/models/networks.py\n\t    Parameters:\n", "        net (network)   -- network to be initialized\n\t        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n\t        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n\t    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n\t    work better for some applications. Feel free to try yourself.\n\t    \"\"\"\n\t    def init_func(m):  # define the initialization function\n\t        classname = m.__class__.__name__\n\t        if hasattr(m, \"weight\") and (\n\t            classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1\n", "        ):\n\t            if init_type == \"normal\":\n\t                init.normal_(m.weight.data, 0.0, init_gain)\n\t            elif init_type == \"xavier\":\n\t                init.xavier_normal_(m.weight.data, gain=init_gain)\n\t            elif init_type == \"kaiming\":\n\t                init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_in\")\n\t            elif init_type == \"orthogonal\":\n\t                init.orthogonal_(m.weight.data, gain=init_gain)\n\t            elif init_type == \"truncated_normal\":\n", "                m.weight.data = truncated_normal_initializer(m.weight.shape, 0.0, stddev=init_gain)\n\t            else:\n\t                raise NotImplementedError(\n\t                    \"initialization method [%s] is not implemented\" % init_type\n\t                )\n\t            if hasattr(m, \"bias\") and m.bias is not None:\n\t                init.constant_(m.bias.data, 0.0)\n\t        elif classname.find(\"BatchNorm2d\") != -1:\n\t            init.normal_(m.weight.data, 1.0, init_gain)\n\t            init.constant_(m.bias.data, 0.0)\n", "    net.apply(init_func)\n\tdef _softplus_to_std(softplus):\n\t    softplus = torch.min(softplus, torch.ones_like(softplus) * 80)\n\t    return torch.sqrt(torch.log(1.0 + softplus.exp()) + 1e-5)\n\tdef normal(loc, pre_softplus_var):\n\t    return dist.independent.Independent(\n\t        dist.normal.Normal(loc, torch.sqrt(F.softplus(pre_softplus_var))), 1\n\t    )\n\tdef unit_normal(shape, device):\n\t    loc = torch.zeros(shape).to(device)\n", "    scale = torch.ones(shape).to(device)\n\t    return dist.independent.Independent(dist.normal.Normal(loc, scale), 1)\n\tdef gmm_loglikelihood(x, x_loc, log_var, mask_logprobs):\n\t    \"\"\"\n\t    mask_logprobs: [N, K, 1, H, W]\n\t    \"\"\"\n\t    # NLL [batch_size, 1, H, W]\n\t    sq_err = (x.unsqueeze(1) - x_loc).pow(2)\n\t    # log N(x; x_loc, log_var): [N, K, C, H, W]\n\t    normal_ll = -0.5 * log_var - 0.5 * (sq_err / torch.exp(log_var))\n", "    # [N, K, C, H, W]\n\t    log_p_k = mask_logprobs + normal_ll\n\t    # logsumexp over slots [N, C, H, W]\n\t    log_p = torch.logsumexp(log_p_k, dim=1).sum(1, keepdim=True)\n\t    # [batch_size]\n\t    nll = -torch.sum(log_p, dim=[1, 2, 3])\n\t    return nll, {\"log_p_k\": log_p_k, \"normal_ll\": normal_ll, \"log_p\": log_p}\n\tdef gaussian_loglikelihood(x_t, x_loc, log_var):\n\t    sq_err = (x_t - x_loc).pow(2)  # [N,C,H,W]\n\t    # log N(x; x_loc, log_var): [N,C, H, W]\n", "    normal_ll = -0.5 * log_var - 0.5 * (sq_err / torch.exp(log_var))\n\t    nll = -torch.sum(normal_ll, dim=[1, 2, 3])  # [N]\n\t    return nll\n\tdef rename_state_dict(state_dict, old_strings, new_strings):\n\t    new_state_dict = {}\n\t    for old_string, new_string in zip(old_strings, new_strings):\n\t        for k, v in state_dict.items():\n\t            if old_string in k:\n\t                new_key = k.replace(old_string, new_string)\n\t                new_state_dict[new_key] = v\n", "    for k, v in state_dict.items():\n\t        for old_string in old_strings:\n\t            if old_string in k:\n\t                break\n\t        else:\n\t            new_state_dict[k] = v\n\t    return new_state_dict\n\tclass RefinementNetwork(nn.Module):\n\t    def __init__(\n\t        self,\n", "        input_shape,\n\t        z_size=64,\n\t        refinenet_channels_in=17,  # should be 17!!!\n\t        conv_channels=64,\n\t        lstm_dim=256,\n\t        k=3,\n\t        stride=2,\n\t    ):\n\t        super().__init__()\n\t        self.input_shape = input_shape\n", "        self.z_size = z_size\n\t        self.conv = nn.Sequential(\n\t            nn.Conv2d(refinenet_channels_in, conv_channels, k, stride, k // 2),\n\t            nn.ELU(True),\n\t            nn.Conv2d(conv_channels, conv_channels, k, stride, k // 2),\n\t            nn.ELU(True),\n\t            nn.Conv2d(conv_channels, conv_channels, k, stride, k // 2),\n\t            nn.ELU(True),\n\t            nn.Conv2d(conv_channels, conv_channels, k, stride, k // 2),\n\t            nn.ELU(True),\n", "            nn.AdaptiveAvgPool2d((1, 1)),\n\t            nn.Flatten(),\n\t            nn.Linear(conv_channels, lstm_dim),\n\t            nn.ELU(True),\n\t            # nn.Linear(lstm_dim, lstm_dim),  # Papers says only 1; Official repo has 2\n\t            # nn.ELU(True)\n\t        )\n\t        # self.input_proj = nn.Sequential(\n\t        #     nn.Linear(lstm_dim + 4 * self.z_size, lstm_dim),\n\t        #     nn.ELU(True)\n", "        # )\n\t        # self.lstm = nn.LSTM(lstm_dim, lstm_dim)\n\t        self.lstm = nn.LSTM(lstm_dim + 4 * self.z_size, lstm_dim)\n\t        # self.loc = nn.Linear(lstm_dim, z_size)\n\t        # self.softplus = nn.Linear(lstm_dim, z_size)\n\t        self.ref_head = nn.Linear(lstm_dim, 2 * z_size)\n\t    def forward(self, img_inputs, vec_inputs, h, c):\n\t        \"\"\"\n\t        img_inputs: [N * K, C, H, W]\n\t        vec_inputs: [N * K, 4*z_size]\n", "        \"\"\"\n\t        x = self.conv(img_inputs)\n\t        # concat with \\lambda and \\nabla \\lambda\n\t        x = torch.cat([x, vec_inputs], 1)\n\t        # x = self.input_proj(x)\n\t        x = x.unsqueeze(0)  # seq dim\n\t        self.lstm.flatten_parameters()\n\t        out, (h, c) = self.lstm(x, (h, c))\n\t        out = out.squeeze(0)\n\t        # loc = self.loc(out)\n", "        # softplus = self.softplus(out)\n\t        # lamda = torch.cat([loc, softplus], 1)\n\t        lamda = self.ref_head(out)\n\t        return lamda, (h, c)\n\tclass SpatialBroadcastDecoder(nn.Module):\n\t    \"\"\"Decodes the individual Gaussian image components into RGB and mask.\n\t    This is the architecture used for the Multi-dSprites experiment but I haven't seen any issues\n\t    with re-using it for CLEVR. In their paper they slightly modify it (e.g., uses 3x3 conv instead\n\t    of 5x5).\n\t    \"\"\"\n", "    def __init__(self, input_shape, z_size=64, conv_channels=64, k=3):\n\t        super().__init__()\n\t        self.h, self.w = input_shape[1], input_shape[2]\n\t        self.decode = nn.Sequential(\n\t            nn.Conv2d(z_size + 2, conv_channels, k, 1, padding=k // 2 - 1),\n\t            nn.ELU(True),\n\t            nn.Conv2d(conv_channels, conv_channels, k, 1, padding=k // 2 - 1),\n\t            nn.ELU(True),\n\t            nn.Conv2d(conv_channels, conv_channels, k, 1, padding=k // 2 - 1),\n\t            nn.ELU(True),\n", "            nn.Conv2d(conv_channels, conv_channels, k, 1, padding=k // 2 - 1),\n\t            nn.ELU(True),\n\t            nn.Conv2d(conv_channels, 4, 1, 1),\n\t        )\n\t    @staticmethod\n\t    def spatial_broadcast(z, h, w):\n\t        \"\"\"\n\t        source: https://github.com/baudm/MONet-pytorch/blob/master/models/networks.py\n\t        \"\"\"\n\t        # Batch size\n", "        n = z.shape[0]\n\t        # Expand spatially: (n, z_dim) -> (n, z_dim, h, w)\n\t        z_b = z.view((n, -1, 1, 1)).expand(-1, -1, h, w)\n\t        # Coordinate axes:\n\t        x = torch.linspace(-1, 1, w, device=z.device)\n\t        y = torch.linspace(-1, 1, h, device=z.device)\n\t        y_b, x_b = torch.meshgrid(y, x)\n\t        # Expand from (h, w) -> (n, 1, h, w)\n\t        x_b = x_b.expand(n, 1, -1, -1)\n\t        y_b = y_b.expand(n, 1, -1, -1)\n", "        # Concatenate along the channel dimension: final shape = (n, z_dim + 2, h, w)\n\t        z_sb = torch.cat((z_b, x_b, y_b), dim=1)\n\t        return z_sb\n\t    def forward(self, z):\n\t        z_sb = SpatialBroadcastDecoder.spatial_broadcast(z, self.h + 8, self.w + 8)\n\t        out = self.decode(z_sb)  # [batch_size * K, output_size, h, w]\n\t        return torch.sigmoid(out[:, :3]), out[:, 3]\n\tclass IODINE(nn.Module):\n\t    def __init__(\n\t        self,\n", "        input_shape: Union[tuple, list] = (3, 128, 128),\n\t        z_size: int = 64,\n\t        K: int = 7,\n\t        inference_iters: int = 5,\n\t        std: float = 0.10,\n\t        kl_beta=1,\n\t        lstm_dim: int = 256,\n\t        conv_channels: int = 128,\n\t        refinenet_channels_in: int = 17,  # should be 17\n\t    ):\n", "        super().__init__()\n\t        self.z_size = z_size\n\t        self.input_shape = input_shape\n\t        self.K = K\n\t        self.inference_iters = inference_iters\n\t        self.log_scale = math.log(std)\n\t        self.kl_beta = kl_beta\n\t        self.lstm_dim = lstm_dim\n\t        self.register_buffer(\"gmm_log_scale\", (self.log_scale * torch.ones(K)).view(1, K, 1, 1, 1))\n\t        self.image_decoder = SpatialBroadcastDecoder(\n", "            input_shape, z_size, conv_channels, k=3\n\t        )  # k HERE IS filter size\n\t        self.refine_net = RefinementNetwork(\n\t            input_shape, z_size, refinenet_channels_in, conv_channels, lstm_dim\n\t        )\n\t        init_weights(self.image_decoder, \"xavier\")\n\t        init_weights(self.refine_net, \"xavier\")\n\t        # learnable initial posterior distribution\n\t        # loc = 0, variance = 1\n\t        self.lamda_0 = nn.Parameter(\n", "            torch.cat([torch.zeros(1, self.z_size), torch.ones(1, self.z_size)], 1)\n\t        )\n\t        # layernorms for iterative inference input\n\t        affine = True  # Paper trains these parameters\n\t        n = self.input_shape[1]\n\t        self.layer_norms = torch.nn.ModuleList(\n\t            [\n\t                nn.LayerNorm((1, n, n), elementwise_affine=affine),\n\t                nn.LayerNorm((1, n, n), elementwise_affine=affine),\n\t                nn.LayerNorm((3, n, n), elementwise_affine=affine),\n", "                nn.LayerNorm((1, n, n), elementwise_affine=affine),\n\t                nn.LayerNorm((self.z_size,), elementwise_affine=affine),  # layer_norm_mean\n\t                nn.LayerNorm((self.z_size,), elementwise_affine=affine),  # layer_norm_log_scale\n\t            ]\n\t        )\n\t    def refinenet_inputs(self, image, means, masks, mask_logits, log_p, normal_ll, lamda, loss):\n\t        N, K, C, H, W = image.shape\n\t        # non-gradient inputs\n\t        # 1. image [N, K, C, H, W]\n\t        # 2. means [N, K, C, H, W]\n", "        # 3. masks  [N, K, 1, H, W] (log probs)\n\t        # 4. mask logits [N, K, 1, H, W]\n\t        # 5. mask posterior [N, K, 1, H, W]\n\t        # print(image.shape, means.shape, masks.shape, mask_logits.shape, log_p.shape, normal_ll.shape, lamda.shape, loss.shape)\n\t        mask_ll = normal_ll.sum(dim=2, keepdim=True)\n\t        mask_posterior = mask_ll - torch.logsumexp(mask_ll, dim=1, keepdim=True)  # logscale\n\t        # 6. pixelwise likelihood [N, K, 1, H, W]\n\t        # log_p_k = torch.logsumexp(log_p_k, dim=1).sum(1)\n\t        log_p_k = log_p.view(-1, 1, 1, H, W).expand(-1, K, -1, -1, -1)\n\t        # 7. LOO likelihood\n", "        # loo_px_l = torch.log(1e-6 + (log_p_k.exp()+1e-6 - (masks + normal_ll.unsqueeze(2).exp())+1e-6)) # [N,K,1,H,W]\n\t        # since counterfactuals have stop grad:\n\t        with torch.no_grad():\n\t            counterfactuals = []\n\t            for i in range(K):\n\t                pll = torch.cat((normal_ll[:, :i], normal_ll[:, i + 1 :]), dim=1)\n\t                msk = torch.cat((mask_logits[:, :i], mask_logits[:, i + 1 :]), dim=1)\n\t                counterfactuals.append(\n\t                    torch.logsumexp(pll + torch.log_softmax(msk, dim=1), dim=1).sum(\n\t                        1, keepdim=True\n", "                    )\n\t                )\n\t            counterfactuals = torch.stack(counterfactuals, dim=1).view(N, K, 1, H, W)\n\t        # 8. Coordinate channel\n\t        x_mesh, y_mesh = torch.meshgrid(\n\t            torch.linspace(-1, 1, H, device=image.device),\n\t            torch.linspace(-1, 1, W, device=image.device),\n\t        )\n\t        # Expand from (h, w) -> (n, k, 1, h, w)\n\t        x_mesh = x_mesh.expand(N, K, 1, -1, -1)\n", "        y_mesh = y_mesh.expand(N, K, 1, -1, -1)\n\t        # 9. \\partial L / \\partial means\n\t        # [N, K, C, H, W]\n\t        # 10. \\partial L/ \\partial masks\n\t        # [N, K, 1, H, W]\n\t        # 11. \\partial L/ \\partial lamda\n\t        # [N*K, 2 * self.z_size]\n\t        d_means, d_masks, d_lamda = torch.autograd.grad(\n\t            loss, [means, masks, lamda], retain_graph=self.training, only_inputs=True\n\t        )\n", "        d_loc_z, d_sp_z = d_lamda.chunk(2, dim=1)\n\t        # d_loc_z, d_sp_z = d_loc_z.contiguous(), d_sp_z.contiguous()\n\t        # Stop gradients and apply LayerNorm\n\t        # dmeans LN + SG\n\t        d_means = self.layer_norms[2](d_means.detach())\n\t        # dmasks LN + SG\n\t        d_masks = self.layer_norms[3](d_masks.detach())\n\t        # log_p LN + SG\n\t        log_p_k = self.layer_norms[0](log_p_k.detach())\n\t        # counterfactual SG + LN\n", "        loo_px_l = self.layer_norms[1](counterfactuals.detach())\n\t        # dzp LN + SG\n\t        d_loc_z = self.layer_norms[4](d_loc_z.detach())\n\t        d_sp_z = self.layer_norms[5](d_sp_z.detach())\n\t        # concat image-size and vector inputs\n\t        image_inputs = torch.cat(\n\t            [\n\t                image,  # 3\n\t                means,  # 3\n\t                masks,  # 1\n", "                mask_logits,  # code seems to provide probs, paper says logits # 1\n\t                mask_posterior,  # in code not in logscale; is here 1\n\t                d_means,  # 3\n\t                d_masks,  # 1\n\t                log_p_k,  # 1\n\t                loo_px_l,  # 1\n\t                x_mesh,  # 1\n\t                y_mesh,  # 1\n\t            ],\n\t            2,\n", "        )\n\t        vec_inputs = torch.cat([lamda, d_loc_z, d_sp_z], 1)\n\t        return image_inputs.view(N * K, -1, H, W), vec_inputs\n\t    def forward(self, x):\n\t        \"\"\"Evaluates the model as a whole, encodes and decodes and runs inference for T steps.\"\"\"\n\t        torch.set_grad_enabled(True)\n\t        B = x.shape[0]\n\t        C, H, W = self.input_shape[0], self.input_shape[1], self.input_shape[2]\n\t        # expand lambda_0\n\t        lamda = self.lamda_0.repeat(B * self.K, 1)  # [N*K, 2*z_size]\n", "        p_z = unit_normal(shape=[B * self.K, self.z_size], device=x.device)\n\t        total_loss = 0.0\n\t        losses = []\n\t        x_means = []\n\t        masks = []\n\t        h = torch.zeros(1, B * self.K, self.lstm_dim).to(x.device)\n\t        c = torch.zeros(1, B * self.K, self.lstm_dim).to(x.device)\n\t        for i in range(self.inference_iters):\n\t            # sample initial posterior\n\t            loc_z, sp_z = lamda.chunk(2, dim=1)\n", "            # loc_z, sp_z = loc_z.contiguous(), sp_z.contiguous()\n\t            q_z = normal(loc_z, sp_z)\n\t            z = q_z.rsample()\n\t            # Get means and masks\n\t            x_loc, mask_logits = self.image_decoder(z)  # [N*K, C, H, W]\n\t            x_loc = x_loc.view(B, self.K, C, H, W)\n\t            # softmax across slots\n\t            mask_logits = mask_logits.view(B, self.K, 1, H, W)\n\t            mask_logprobs = nn.functional.log_softmax(mask_logits, dim=1)\n\t            # NLL [batch_size, 1, H, W]\n", "            # log_var = (2 * self.gmm_log_scale)\n\t            nll, ll_outs = gmm_loglikelihood(x, x_loc, 2 * self.gmm_log_scale, mask_logprobs)\n\t            # KL div\n\t            kl_div = dist.kl.kl_divergence(q_z, p_z)\n\t            kl_div = kl_div.view(B, self.K).sum(1)\n\t            loss = nll + self.kl_beta * kl_div\n\t            loss = torch.mean(loss)\n\t            scaled_loss = ((i + 1.0) / self.inference_iters) * loss\n\t            losses += [scaled_loss]\n\t            total_loss += scaled_loss\n", "            x_means += [x_loc]\n\t            masks += [mask_logprobs]\n\t            # Refinement\n\t            if i == self.inference_iters - 1:\n\t                # after T refinement steps, just output final loss\n\t                continue\n\t            # compute refine inputs\n\t            x_ = x.repeat(self.K, 1, 1, 1).view(B, self.K, C, H, W)\n\t            img_inps, vec_inps = self.refinenet_inputs(\n\t                x_,\n", "                x_loc,\n\t                mask_logprobs,\n\t                mask_logits,\n\t                ll_outs[\"log_p\"],\n\t                ll_outs[\"normal_ll\"],\n\t                lamda,\n\t                loss,\n\t            )\n\t            delta, (h, c) = self.refine_net(img_inps, vec_inps, h, c)\n\t            lamda = lamda + delta\n", "        return {\n\t            \"canvas\": (x_loc * mask_logprobs.exp()).sum(dim=1),\n\t            \"loss\": total_loss,\n\t            \"recon_loss\": torch.mean(nll),\n\t            \"kl\": torch.mean(kl_div),\n\t            \"layers\": {\"patch\": x_loc, \"mask\": mask_logprobs.exp()},\n\t            \"z\": z,\n\t        }\n"]}
{"filename": "src/models/components/simple_dense_net.py", "chunked_list": ["from torch import nn\n\tclass SimpleDenseNet(nn.Module):\n\t    def __init__(\n\t        self,\n\t        input_size: int = 784,\n\t        lin1_size: int = 256,\n\t        lin2_size: int = 256,\n\t        lin3_size: int = 256,\n\t        output_size: int = 10,\n\t    ):\n", "        super().__init__()\n\t        self.model = nn.Sequential(\n\t            nn.Linear(input_size, lin1_size),\n\t            nn.BatchNorm1d(lin1_size),\n\t            nn.ReLU(),\n\t            nn.Linear(lin1_size, lin2_size),\n\t            nn.BatchNorm1d(lin2_size),\n\t            nn.ReLU(),\n\t            nn.Linear(lin2_size, lin3_size),\n\t            nn.BatchNorm1d(lin3_size),\n", "            nn.ReLU(),\n\t            nn.Linear(lin3_size, output_size),\n\t        )\n\t    def forward(self, x):\n\t        batch_size, channels, width, height = x.size()\n\t        # (batch, 1, width, height) -> (batch, 1*width*height)\n\t        x = x.view(batch_size, -1)\n\t        return self.model(x)\n\tif __name__ == \"__main__\":\n\t    _ = SimpleDenseNet()\n"]}
{"filename": "src/models/components/genesis2.py", "chunked_list": ["\"\"\"\n\tsource: https://github.com/karazijal/clevrtex/blob/master/ool/picture/models/thirdparty/genesis2/model.py\n\tGenesis-V2\n\thttps://arxiv.org/pdf/2104.09958\n\tBased on original implementation from\n\thttps://github.com/martinengelcke/genesis/blob/genesis-v2/models/genesisv2_config.py\n\t# =========================== A2I Copyright Header ===========================\n\t#\n\t# Copyright (c) 2003-2021 University of Oxford. All rights reserved.\n\t# Authors: Applied AI Lab, Oxford Robotics Institute, University of Oxford\n", "#          https://ori.ox.ac.uk/labs/a2i/\n\t#\n\t# This file is the property of the University of Oxford.\n\t# Redistribution and use in source and binary forms, with or without\n\t# modification, is not permitted without an explicit licensing agreement\n\t# (research or commercial). No warranty, explicit or implicit, provided.\n\t#\n\t# =========================== A2I Copyright Header ===========================\n\t\"\"\"\n\timport numpy as np\n", "import torch\n\timport torch.distributions as dist\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass ConvGNReLU(nn.Sequential):\n\t    def __init__(self, nin, nout, kernel, stride=1, padding=0, groups=8):\n\t        super().__init__(\n\t            nn.Conv2d(nin, nout, kernel, stride, padding, bias=False),\n\t            nn.GroupNorm(groups, nout),\n\t            nn.ReLU(inplace=True),\n", "        )\n\tclass BroadcastLayer(nn.Module):\n\t    def __init__(self, dim):\n\t        super().__init__()\n\t        self.dim = dim\n\t        c = torch.linspace(-1, 1, dim)\n\t        self.register_buffer(\"coords\", torch.stack(torch.meshgrid(c, c))[None])\n\t    def forward(self, x):\n\t        b_sz = x.size(0)\n\t        # Broadcast\n", "        if x.dim() == 2:\n\t            x = x.view(b_sz, -1, 1, 1)\n\t            x = x.expand(-1, -1, self.dim, self.dim)\n\t        else:\n\t            x = F.interpolate(x, self.dim)\n\t        return torch.cat([x, self.coords.expand(b_sz, -1, -1, -1)], dim=1)\n\tclass UNet(nn.Module):\n\t    def __init__(self, num_blocks, img_size=64, filter_start=32, in_chnls=4, out_chnls=1):\n\t        super().__init__()\n\t        c = filter_start\n", "        if num_blocks == 4:\n\t            enc_in = [in_chnls, c, 2 * c, 2 * c]\n\t            enc_out = [c, 2 * c, 2 * c, 2 * c]\n\t            dec_in = [4 * c, 4 * c, 4 * c, 2 * c]\n\t            dec_out = [2 * c, 2 * c, c, c]\n\t        elif num_blocks == 5:\n\t            enc_in = [in_chnls, c, c, 2 * c, 2 * c]\n\t            enc_out = [c, c, 2 * c, 2 * c, 2 * c]\n\t            dec_in = [4 * c, 4 * c, 4 * c, 2 * c, 2 * c]\n\t            dec_out = [2 * c, 2 * c, c, c, c]\n", "        elif num_blocks == 6:\n\t            enc_in = [in_chnls, c, c, c, 2 * c, 2 * c]\n\t            enc_out = [c, c, c, 2 * c, 2 * c, 2 * c]\n\t            dec_in = [4 * c, 4 * c, 4 * c, 2 * c, 2 * c, 2 * c]\n\t            dec_out = [2 * c, 2 * c, c, c, c, c]\n\t        self.down = []\n\t        self.up = []\n\t        # 3x3 kernels, stride 1, padding 1\n\t        for i, o in zip(enc_in, enc_out):\n\t            self.down.append(ConvGNReLU(i, o, 3, 1, 1))\n", "        for i, o in zip(dec_in, dec_out):\n\t            self.up.append(ConvGNReLU(i, o, 3, 1, 1))\n\t        self.down = nn.ModuleList(self.down)\n\t        self.up = nn.ModuleList(self.up)\n\t        self.featuremap_size = img_size // 2 ** (num_blocks - 1)\n\t        self.mlp = nn.Sequential(\n\t            nn.Flatten(),\n\t            nn.Linear(2 * c * self.featuremap_size**2, 128),\n\t            nn.ReLU(),\n\t            nn.Linear(128, 128),\n", "            nn.ReLU(),\n\t            nn.Linear(128, 2 * c * self.featuremap_size**2),\n\t            nn.ReLU(),\n\t        )\n\t        if out_chnls > 0:\n\t            self.final_conv = nn.Conv2d(c, out_chnls, 1)\n\t        else:\n\t            self.final_conv = nn.Identity()\n\t        self.out_chnls = out_chnls\n\t    def forward(self, x):\n", "        batch_size = x.size(0)\n\t        x_down = [x]\n\t        skip = []\n\t        # Down\n\t        for i, block in enumerate(self.down):\n\t            act = block(x_down[-1])\n\t            skip.append(act)\n\t            if i < len(self.down) - 1:\n\t                act = F.interpolate(act, scale_factor=0.5, mode=\"nearest\")\n\t            x_down.append(act)\n", "        # FC\n\t        x_up = self.mlp(x_down[-1])\n\t        x_up = x_up.view(batch_size, -1, self.featuremap_size, self.featuremap_size)\n\t        # Up\n\t        for i, block in enumerate(self.up):\n\t            features = torch.cat([x_up, skip[-1 - i]], dim=1)\n\t            x_up = block(features)\n\t            if i < len(self.up) - 1:\n\t                x_up = F.interpolate(x_up, scale_factor=2.0, mode=\"nearest\")\n\t        return self.final_conv(x_up), None\n", "@torch.no_grad()\n\tdef check_log_masks(log_m_k):\n\t    # Note: this seems to be checking for under(over)flow when moving log->exp space\n\t    # Adjusted this to run on GPU most of the time until the errors need to be printed\n\t    flat = torch.stack(log_m_k, dim=4).exp().sum(dim=4).view(-1)\n\t    diff = flat - torch.ones_like(flat)\n\t    max_diff, idx = diff.max(dim=0)\n\t    if torch.anu(max_diff > 1e-3) or torch.any(torch.isnan(flat)):\n\t        print(f\"Max diff: {max_diff.cpu().item()}\")\n\t        masks_k = log_m_k.view(log_m_k.shape[0], -1)[:, idx].exp().cpu().squeeze()\n", "        for i, v in enumerate(masks_k):\n\t            print(f\"Mask value at k={i}: {v}\")\n\t        # TODO: change this drop into an interactive debugger if interactive env.\n\t        raise ValueError(\"Masks do not sum to 1.0. Not close enough.\")\n\tclass SemiConv(nn.Conv2d):\n\t    def __init__(self, in_c, out_ch, size):\n\t        super().__init__(in_c, out_ch, 1)\n\t        self.gate = nn.Parameter(torch.zeros(1), requires_grad=True)\n\t        self.register_buffer(\n\t            \"uv\",\n", "            torch.cat(\n\t                [\n\t                    torch.zeros(out_ch - 2, size, size),\n\t                    torch.stack(torch.meshgrid(*([torch.linspace(-1, 1, size)] * 2))),\n\t                ]\n\t            )[None],\n\t            persistent=False,\n\t        )\n\t    def forward(self, input):\n\t        x = self.gate * super().forward(input)\n", "        return x + self.uv, x[:, -2:]\n\tdef clamp_st(x, lower, upper):\n\t    # From: http://docs.pyro.ai/en/0.3.3/_modules/pyro/distributions/iaf.html\n\t    return x + (x.clamp(lower, upper) - x).detach()\n\tdef euclidian_norm(x):\n\t    # Clamp before taking sqrt for numerical stability\n\t    return clamp_st((x**2).sum(1), 1e-10, 1e10).sqrt()\n\tdef euclidian_distance(ea, eb):\n\t    # Unflatten if needed if one is an image and the other a vector\n\t    if ea.dim() == 4 and eb.dim() == 2:\n", "        eb = eb.unsqueeze(-1).unsqueeze(-1)\n\t    if eb.dim() == 4 and ea.dim() == 2:\n\t        ea = ea.unsqueeze(-1).unsqueeze(-1)\n\t    return euclidian_norm(ea - eb)\n\tdef squared_distance(ea, eb):\n\t    if ea.dim() == 4 and eb.dim() == 2:\n\t        eb = eb.unsqueeze(-1).unsqueeze(-1)\n\t    if eb.dim() == 4 and ea.dim() == 2:\n\t        ea = ea.unsqueeze(-1).unsqueeze(-1)\n\t    return ((ea - eb) ** 2).sum(1)\n", "class InstanceColouringSBP(nn.Module):\n\t    def __init__(\n\t        self, img_size, kernel=\"gaussian\", colour_dim=8, K_steps=None, feat_dim=None, semiconv=True\n\t    ):\n\t        super().__init__()\n\t        # Config\n\t        self.img_size = img_size\n\t        self.kernel = kernel\n\t        self.colour_dim = colour_dim\n\t        # Initialise kernel sigma\n", "        if self.kernel == \"laplacian\":\n\t            sigma_init = 1.0 / (np.sqrt(K_steps) * np.log(2))\n\t        elif self.kernel == \"gaussian\":\n\t            sigma_init = 1.0 / (K_steps * np.log(2))\n\t        elif self.kernel == \"epanechnikov\":\n\t            sigma_init = 2.0 / K_steps\n\t        else:\n\t            return ValueError(\"No valid kernel.\")\n\t        self.log_sigma = nn.Parameter(torch.tensor(sigma_init).log())\n\t        # Colour head\n", "        if semiconv:\n\t            self.colour_head = SemiConv(feat_dim, self.colour_dim, img_size)\n\t        else:\n\t            self.colour_head = nn.Conv2d(feat_dim, self.colour_dim, 1)\n\t    def forward(self, features, steps_to_run, debug=False, dynamic_K=False, *args, **kwargs):\n\t        batch_size = features.size(0)\n\t        if dynamic_K:\n\t            assert batch_size == 1\n\t        # Get colours\n\t        colour_out = self.colour_head(features)\n", "        if isinstance(colour_out, tuple):\n\t            colour, delta = colour_out\n\t        else:\n\t            colour, delta = colour_out, None\n\t        # Sample from uniform to select random pixels as seeds\n\t        # rand_pixel = torch.empty(batch_size, 1, *colour.shape[2:])\n\t        # rand_pixel = rand_pixel.uniform_()\n\t        rand_pixel = torch.rand(batch_size, 1, *colour.shape[2:], device=features.device)\n\t        # Run SBP\n\t        seed_list = []\n", "        log_m_k = []\n\t        log_s_k = [\n\t            torch.zeros(batch_size, 1, self.img_size, self.img_size, device=features.device)\n\t        ]\n\t        for step in range(steps_to_run):\n\t            # Determine seed\n\t            scope = F.interpolate(\n\t                log_s_k[step].exp(), size=colour.shape[2:], mode=\"bilinear\", align_corners=False\n\t            )\n\t            pixel_probs = rand_pixel * scope\n", "            rand_max = pixel_probs.flatten(2).argmax(2).flatten()\n\t            # --TODO(martin): parallelise this--\n\t            seed = features.new_empty((batch_size, self.colour_dim))\n\t            for bidx in range(batch_size):\n\t                seed[bidx, :] = colour.flatten(2)[bidx, :, rand_max[bidx]]\n\t            # seed = colour.flatten(2).gather(-1, rand_max.view(-1, 1, 1).expand(-1, rand_max.shape[1], -1)).squeeze(-1)\n\t            seed_list.append(seed)\n\t            # Compute masks\n\t            # Note the distance here is in channel-wise\n\t            if self.kernel == \"laplacian\":\n", "                distance = euclidian_distance(colour, seed)  # [B, H, W]\n\t                alpha = torch.exp(-distance / self.log_sigma.exp())\n\t            elif self.kernel == \"gaussian\":\n\t                distance = squared_distance(colour, seed)  # [B, H, W]\n\t                alpha = torch.exp(-distance / self.log_sigma.exp())\n\t            elif self.kernel == \"epanechnikov\":\n\t                distance = squared_distance(colour, seed)  # [B, H, W]\n\t                alpha = (1 - distance / self.log_sigma.exp()).relu()\n\t            else:\n\t                raise ValueError(\"No valid kernel.\")\n", "            alpha = alpha.unsqueeze(1)\n\t            # Sanity checks\n\t            if debug:\n\t                assert alpha.max() <= 1, alpha.max()\n\t                assert alpha.min() >= 0, alpha.min()\n\t            # Clamp mask values to [0.01, 0.99] for numerical stability\n\t            # TODO(martin): clamp less aggressively?\n\t            alpha = clamp_st(alpha, 0.01, 0.99)\n\t            # SBP update\n\t            log_a = torch.log(alpha)\n", "            log_neg_a = torch.log(1 - alpha)\n\t            log_m = log_s_k[step] + log_a\n\t            if dynamic_K and log_m.exp().sum() < 20:\n\t                break\n\t            log_m_k.append(log_m)\n\t            log_s_k.append(log_s_k[step] + log_neg_a)\n\t        # Set mask equal to scope for last step\n\t        log_m_k.append(log_s_k[-1])\n\t        # Accumulate stats\n\t        stats = {\"colour\": colour, \"delta\": delta, \"seeds\": seed_list}\n", "        return log_m_k, log_s_k, stats\n\tdef genesis_x_loss(x, log_m_k, x_r_k, std, pixel_wise=False):\n\t    # 1.) Sum over steps for per pixel & channel (ppc) losses\n\t    p_xr_stack = dist.Normal(torch.stack(x_r_k, dim=4), std)\n\t    log_xr_stack = p_xr_stack.log_prob(x.unsqueeze(4))\n\t    log_m_stack = torch.stack(log_m_k, dim=4)\n\t    log_mx = log_m_stack + log_xr_stack\n\t    err_ppc = -log_mx.logsumexp(dim=4)\n\t    # 2.) Sum across channels and spatial dimensions\n\t    if pixel_wise:\n", "        return err_ppc\n\t    else:\n\t        return err_ppc.sum(dim=(1, 2, 3))\n\tdef genesis_mask_latent_loss(\n\t    q_zm_0_k, zm_0_k, zm_k_k=None, ldj_k=None, prior_lstm=None, prior_linear=None, debug=False\n\t):\n\t    num_steps = len(zm_0_k)\n\t    batch_size = zm_0_k[0].size(0)\n\t    latent_dim = zm_0_k[0].size(1)\n\t    if zm_k_k is None:\n", "        zm_k_k = zm_0_k\n\t    # -- Determine prior --\n\t    if prior_lstm is not None and prior_linear is not None:\n\t        # zm_seq shape: (att_steps-2, batch_size, ldim)\n\t        # Do not need the last element in z_k\n\t        zm_seq = torch.cat([zm.view(1, batch_size, -1) for zm in zm_k_k[:-1]], dim=0)\n\t        # lstm_out shape: (att_steps-2, batch_size, state_size)\n\t        # Note: recurrent state is handled internally by LSTM\n\t        lstm_out, _ = prior_lstm(zm_seq)\n\t        # linear_out shape: (att_steps-2, batch_size, 2*ldim)\n", "        linear_out = prior_linear(lstm_out)\n\t        linear_out = torch.chunk(linear_out, 2, dim=2)\n\t        mu_raw = torch.tanh(linear_out[0])\n\t        # Note: ditton about prior_linear\n\t        sigma_raw = torch.sigmoid(linear_out[1] + 4.0) + 1e-4\n\t        # Split into K steps, shape: (att_steps-2)*[1, batch_size, ldim]\n\t        mu_k = torch.split(mu_raw, 1, dim=0)\n\t        sigma_k = torch.split(sigma_raw, 1, dim=0)\n\t        # Use standard Normal as prior for first step\n\t        p_zm_k = [dist.Normal(0, 1)]\n", "        # Autoregressive prior for later steps\n\t        for mean, std in zip(mu_k, sigma_k):\n\t            # Remember to remove unit dimension at dim=0\n\t            p_zm_k += [\n\t                dist.Normal(mean.view(batch_size, latent_dim), std.view(batch_size, latent_dim))\n\t            ]\n\t        # Sanity checks\n\t        if debug:\n\t            assert zm_seq.size(0) == num_steps - 1\n\t    else:\n", "        p_zm_k = num_steps * [dist.Normal(0, 1)]\n\t    # -- Compute KL using Monte Carlo samples for every step k --\n\t    kl_m_k = []\n\t    for step, p_zm in enumerate(p_zm_k):\n\t        log_q = q_zm_0_k[step].log_prob(zm_0_k[step]).sum(dim=1)\n\t        log_p = p_zm.log_prob(zm_k_k[step]).sum(dim=1)\n\t        kld = log_q - log_p\n\t        if ldj_k is not None:\n\t            ldj = ldj_k[step].sum(dim=1)\n\t            kld = kld - ldj\n", "        kl_m_k.append(kld)\n\t    # -- Sanity check --\n\t    if debug:\n\t        assert len(p_zm_k) == num_steps\n\t        assert len(kl_m_k) == num_steps\n\t    return kl_m_k, p_zm_k\n\tdef monet_get_mask_recon_stack(m_r_logits_k, prior_mode, log):\n\t    if prior_mode == \"softmax\":\n\t        if log:\n\t            return F.log_softmax(torch.stack(m_r_logits_k, dim=4), dim=4)\n", "        return F.softmax(torch.stack(m_r_logits_k, dim=4), dim=4)\n\t    elif prior_mode == \"scope\":\n\t        log_m_r_k = []\n\t        log_s = torch.zeros_like(m_r_logits_k[0])\n\t        for step, logits in enumerate(m_r_logits_k):\n\t            if step == len(m_r_logits_k) - 1:\n\t                log_m_r_k.append(log_s)\n\t            else:\n\t                log_a = F.logsigmoid(logits)\n\t                log_neg_a = F.logsigmoid(-logits)\n", "                log_m_r_k.append(log_s + log_a)\n\t                log_s = log_s + log_neg_a\n\t        log_m_r_stack = torch.stack(log_m_r_k, dim=4)\n\t        return log_m_r_stack if log else log_m_r_stack.exp()\n\t    else:\n\t        raise ValueError(\"No valid prior mode.\")\n\tdef monet_kl_m_loss(log_m_k, log_m_r_k, debug=False):\n\t    if debug:\n\t        assert len(log_m_k) == len(log_m_r_k)\n\t    batch_size = log_m_k[0].size(0)\n", "    m_stack = torch.stack(log_m_k, dim=4).exp()\n\t    m_r_stack = torch.stack(log_m_r_k, dim=4).exp()\n\t    # Lower bound to 1e-5 to avoid infinities\n\t    m_stack = torch.max(m_stack, torch.tensor(1e-5))\n\t    m_r_stack = torch.max(m_r_stack, torch.tensor(1e-5))\n\t    q_m = dist.Categorical(m_stack.view(-1, len(log_m_k)))\n\t    p_m = dist.Categorical(m_r_stack.view(-1, len(log_m_k)))\n\t    kl_m_ppc = dist.kl_divergence(q_m, p_m).view(batch_size, -1)\n\t    return kl_m_ppc.sum(dim=1)\n\tclass Genesis2(nn.Module):\n", "    def __init__(\n\t        self,\n\t        feat_dim: int = 64,\n\t        kernel: str = \"gaussian\",\n\t        semiconv: bool = True,\n\t        dynamic_K: bool = False,\n\t        klm_loss: bool = False,\n\t        detach_mr_in_klm: bool = True,\n\t        g_goal: float = 0.5655,\n\t        g_lr: float = 1e-5,\n", "        g_alpha: float = 0.99,\n\t        g_init: float = 1.0,\n\t        g_min: float = 1e-10,\n\t        g_speedup: float = 10.0,\n\t        K_steps: int = 11,\n\t        img_size: int = 128,  # Clevr\n\t        autoreg_prior: bool = True,\n\t        pixel_bound: bool = True,\n\t        pixel_std: float = 0.7,\n\t        debug: bool = False,\n", "    ):\n\t        super().__init__()\n\t        # Configuration\n\t        self.K_steps = K_steps\n\t        self.pixel_bound = pixel_bound\n\t        self.feat_dim = feat_dim\n\t        self.klm_loss = klm_loss\n\t        self.detach_mr_in_klm = detach_mr_in_klm\n\t        self.dynamic_K = dynamic_K\n\t        self.debug = debug\n", "        # Encoder\n\t        self.encoder = UNet(\n\t            num_blocks=int(np.log2(img_size) - 1),\n\t            img_size=img_size,\n\t            filter_start=min(feat_dim, 64),\n\t            in_chnls=3,\n\t            out_chnls=-1,\n\t        )\n\t        self.att_process = InstanceColouringSBP(\n\t            img_size=img_size,\n", "            kernel=kernel,\n\t            colour_dim=8,\n\t            K_steps=self.K_steps,\n\t            feat_dim=feat_dim,\n\t            semiconv=semiconv,\n\t        )\n\t        self.seg_head = ConvGNReLU(feat_dim, feat_dim, 3, 1, 1)\n\t        self.feat_head = nn.Sequential(\n\t            ConvGNReLU(feat_dim, feat_dim, 3, 1, 1), nn.Conv2d(feat_dim, 2 * feat_dim, 1)\n\t        )\n", "        self.z_head = nn.Sequential(\n\t            nn.LayerNorm(2 * feat_dim),\n\t            nn.Linear(2 * feat_dim, 2 * feat_dim),\n\t            nn.ReLU(inplace=True),\n\t            nn.Linear(2 * feat_dim, 2 * feat_dim),\n\t        )\n\t        # Decoder\n\t        c = feat_dim\n\t        self.decoder_module = nn.Sequential(\n\t            BroadcastLayer(img_size // 16),\n", "            nn.ConvTranspose2d(feat_dim + 2, c, 5, 2, 2, 1),\n\t            nn.GroupNorm(8, c),\n\t            nn.ReLU(inplace=True),\n\t            nn.ConvTranspose2d(c, c, 5, 2, 2, 1),\n\t            nn.GroupNorm(8, c),\n\t            nn.ReLU(inplace=True),\n\t            nn.ConvTranspose2d(c, min(c, 64), 5, 2, 2, 1),\n\t            nn.GroupNorm(8, min(c, 64)),\n\t            nn.ReLU(inplace=True),\n\t            nn.ConvTranspose2d(min(c, 64), min(c, 64), 5, 2, 2, 1),\n", "            nn.GroupNorm(8, min(c, 64)),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(min(c, 64), 4, 1),\n\t        )\n\t        # --- Prior ---\n\t        self.autoreg_prior = autoreg_prior\n\t        self.prior_lstm, self.prior_linear = None, None\n\t        if self.autoreg_prior and self.K_steps > 1:\n\t            self.prior_lstm = nn.LSTM(feat_dim, 4 * feat_dim)\n\t            self.prior_linear = nn.Linear(4 * feat_dim, 2 * feat_dim)\n", "        # --- Output pixel distribution ---\n\t        # assert pixel_std1 == cfg.pixel_std2\n\t        self.std = pixel_std\n\t        self.geco = GECO(\n\t            g_goal * 3 * img_size**2,\n\t            g_lr * (64**2 / img_size**2),\n\t            g_alpha,\n\t            g_init,\n\t            g_min,\n\t            g_speedup,\n", "        )\n\t    def forward(self, x):\n\t        batch_size, _, H, W = x.shape\n\t        # --- Extract features ---\n\t        enc_feat, _ = self.encoder(x)\n\t        enc_feat = F.relu(enc_feat)\n\t        # --- Predict attention masks ---\n\t        if self.dynamic_K:\n\t            if batch_size > 1:\n\t                # Iterate over individual elements in batch\n", "                log_m_k = [[] for _ in range(self.K_steps)]\n\t                att_stats, log_s_k = None, None\n\t                for f in torch.split(enc_feat, 1, dim=0):\n\t                    log_m_k_b, _, _ = self.att_process(\n\t                        self.seg_head(f), self.K_steps - 1, dynamic_K=True\n\t                    )\n\t                    for step in range(self.K_steps):\n\t                        if step < len(log_m_k_b):\n\t                            log_m_k[step].append(log_m_k_b[step])\n\t                        else:\n", "                            log_m_k[step].append(-1e10 * torch.ones([1, 1, H, W]))\n\t                for step in range(self.K_steps):\n\t                    log_m_k[step] = torch.cat(log_m_k[step], dim=0)\n\t                if self.debug:\n\t                    assert len(log_m_k) == self.K_steps\n\t            else:\n\t                log_m_k, log_s_k, att_stats = self.att_process(\n\t                    self.seg_head(enc_feat), self.K_steps - 1, dynamic_K=True\n\t                )\n\t        else:\n", "            log_m_k, log_s_k, att_stats = self.att_process(\n\t                self.seg_head(enc_feat), self.K_steps - 1, dynamic_K=False\n\t            )\n\t            if self.debug:\n\t                assert len(log_m_k) == self.K_steps\n\t        # -- Object features, latents, and KL\n\t        comp_stats = dict(mu_k=[], sigma_k=[], z_k=[], kl_l_k=[], q_z_k=[])\n\t        for log_m in log_m_k:\n\t            mask = log_m.exp()\n\t            # Masked sum\n", "            obj_feat = mask * self.feat_head(enc_feat)\n\t            obj_feat = obj_feat.sum((2, 3))\n\t            # Normalise\n\t            obj_feat = obj_feat / (mask.sum((2, 3)) + 1e-5)\n\t            # Posterior\n\t            mu, sigma_ps = self.z_head(obj_feat).chunk(2, dim=1)\n\t            # Note: Not sure why sigma needs to biased by 0.5 here; leaving though\n\t            sigma = F.softplus(sigma_ps + 0.5) + 1e-8\n\t            q_z = dist.Normal(mu, sigma)\n\t            z = q_z.rsample()\n", "            comp_stats[\"mu_k\"].append(mu)\n\t            comp_stats[\"sigma_k\"].append(sigma)\n\t            comp_stats[\"z_k\"].append(z)\n\t            comp_stats[\"q_z_k\"].append(q_z)\n\t        # --- Decode latents ---\n\t        recon, x_r_k, log_m_r_k = self.decode_latents(comp_stats[\"z_k\"])\n\t        # --- Loss terms ---\n\t        losses = {}\n\t        # -- Reconstruction loss\n\t        losses[\"err\"] = genesis_x_loss(x, log_m_r_k, x_r_k, self.std)\n", "        mx_r_k = [x * logm.exp() for x, logm in zip(x_r_k, log_m_r_k)]\n\t        # -- Optional: Attention mask loss\n\t        if self.klm_loss:\n\t            if self.detach_mr_in_klm:\n\t                log_m_r_k = [m.detach() for m in log_m_r_k]\n\t            losses[\"kl_m\"] = monet_kl_m_loss(\n\t                log_m_k=log_m_k, log_m_r_k=log_m_r_k, debug=self.debug\n\t            )\n\t        # -- Component KL\n\t        losses[\"kl_l_k\"], p_z_k = genesis_mask_latent_loss(\n", "            comp_stats[\"q_z_k\"],\n\t            comp_stats[\"z_k\"],\n\t            prior_lstm=self.prior_lstm,\n\t            prior_linear=self.prior_linear,\n\t            debug=self.debug,\n\t        )\n\t        # Track quantities of interest\n\t        stats = dict(\n\t            recon=recon,\n\t            log_m_k=log_m_k,\n", "            log_s_k=log_s_k,\n\t            x_r_k=x_r_k,\n\t            log_m_r_k=log_m_r_k,\n\t            mx_r_k=mx_r_k,\n\t            instance_seg=torch.argmax(torch.cat(log_m_k, dim=1), dim=1),\n\t            instance_seg_r=torch.argmax(torch.cat(log_m_r_k, dim=1), dim=1),\n\t        )\n\t        # Sanity checks\n\t        if self.debug:\n\t            if not self.dynamic_K:\n", "                assert len(log_m_k) == self.K_steps\n\t                assert len(log_m_r_k) == self.K_steps\n\t            check_log_masks(log_m_k)\n\t            check_log_masks(log_m_r_k)\n\t        recon_loss = losses[\"err\"].mean()\n\t        kl_m, kl_l = torch.tensor(0), torch.tensor(0)\n\t        # -- KL stage 1\n\t        if \"kl_m\" in losses:\n\t            kl_m = losses[\"kl_m\"].mean(0)\n\t        elif \"kl_m_k\" in losses:\n", "            kl_m = torch.stack(losses[\"kl_m_k\"], dim=1).mean(dim=0).sum()\n\t        # -- KL stage 2\n\t        if \"kl_l\" in losses:\n\t            kl_l = losses[\"kl_l\"].mean(0)\n\t        elif \"kl_l_k\" in losses:\n\t            kl_l = torch.stack(losses[\"kl_l_k\"], dim=1).mean(dim=0).sum()\n\t        kl = (kl_l + kl_m).mean(0)\n\t        elbo = recon_loss + kl\n\t        loss = self.geco.loss(recon_loss, kl)\n\t        ret = {\n", "            \"canvas\": recon,\n\t            \"loss\": loss,\n\t            \"elbo\": elbo,\n\t            \"rec_loss\": recon_loss,\n\t            \"kl\": kl,\n\t            \"beta\": self.geco.beta,\n\t            \"layers\": {\n\t                \"mask\": torch.stack(log_m_r_k, dim=1).exp(),\n\t                \"patch\": torch.stack(x_r_k, dim=1),\n\t                \"other_mask\": torch.stack(log_m_k, dim=1).exp(),\n", "            },\n\t        }\n\t        return ret\n\t    def decode_latents(self, z_k):\n\t        # --- Reconstruct components and image ---\n\t        x_r_k, m_r_logits_k = [], []\n\t        for z in z_k:\n\t            dec = self.decoder_module(z)\n\t            x_r_k.append(dec[:, :3, :, :])\n\t            m_r_logits_k.append(dec[:, 3:, :, :])\n", "        # Optional: Apply pixelbound\n\t        if self.pixel_bound:\n\t            x_r_k = [torch.sigmoid(item) for item in x_r_k]\n\t        # --- Reconstruct masks ---\n\t        log_m_r_stack = monet_get_mask_recon_stack(m_r_logits_k, \"softmax\", log=True)\n\t        log_m_r_k = torch.split(log_m_r_stack, 1, dim=4)\n\t        log_m_r_k = [m[:, :, :, :, 0] for m in log_m_r_k]\n\t        # --- Reconstruct input image by marginalising (aka summing) ---\n\t        x_r_stack = torch.stack(x_r_k, dim=4)\n\t        m_r_stack = torch.stack(log_m_r_k, dim=4).exp()\n", "        recon = (m_r_stack * x_r_stack).sum(dim=4)\n\t        return recon, x_r_k, log_m_r_k\n\t    def sample(self, batch_size, K_steps=None):\n\t        K_steps = self.K_steps if K_steps is None else K_steps\n\t        # Sample latents\n\t        if self.autoreg_prior:\n\t            z_k = [dist.Normal(0, 1).sample([batch_size, self.feat_dim])]\n\t            state = None\n\t            for k in range(1, K_steps):\n\t                # TODO(martin): reuse code from forward method?\n", "                lstm_out, state = self.prior_lstm(z_k[-1].view(1, batch_size, -1), state)\n\t                linear_out = self.prior_linear(lstm_out)\n\t                linear_out = torch.chunk(linear_out, 2, dim=2)\n\t                linear_out = [item.squeeze(0) for item in linear_out]\n\t                mu = torch.tanh(linear_out[0])\n\t                # Note: the 4.0 bias seems to be for 0-initialised self.prior_linear.bias.\n\t                # This gives starting sigma as 1.0001.\n\t                # TODO: replace this with something else, like a direct init of bias.\n\t                # TODO: Sigmoid saturates outside of (-88, 16) (0. gradients, 0. or 1. output).\n\t                # TODO: Having a positive target +4.0 seems to somewhat reduce that.\n", "                sigma = torch.sigmoid(linear_out[1] + 4.0) + 1e-4\n\t                p_z = dist.Normal(\n\t                    mu.view([batch_size, self.feat_dim]), sigma.view([batch_size, self.feat_dim])\n\t                )\n\t                z_k.append(p_z.sample())\n\t        else:\n\t            p_z = dist.Normal(0, 1)\n\t            z_k = [p_z.sample([batch_size, self.feat_dim]) for _ in range(K_steps)]\n\t        # Decode latents\n\t        recon, x_r_k, log_m_r_k = self.decode_latents(z_k)\n", "        stats = dict(\n\t            x_k=x_r_k, log_m_k=log_m_r_k, mx_k=[x * m.exp() for x, m in zip(x_r_k, log_m_r_k)]\n\t        )\n\t        return recon, stats\n\tclass GECO(nn.Module):\n\t    def __init__(self, goal, step_size, alpha=0.99, beta_init=1.0, beta_min=1e-10, speedup=None):\n\t        super().__init__()\n\t        self.err_ema = None\n\t        # self.goal = goal\n\t        # self.step_size = step_size\n", "        # self.alpha = alpha\n\t        # self.beta = torch.tensor(beta_init)\n\t        # self.beta_min = torch.tensor(beta_min)\n\t        # self.beta_max = torch.tensor(1e10)\n\t        # self.speedup = speedup\n\t        self.register_buffer(\"goal\", torch.tensor(goal))\n\t        self.register_buffer(\"step_size\", torch.tensor(step_size))\n\t        self.register_buffer(\"alpha\", torch.tensor(alpha))\n\t        self.register_buffer(\"beta\", torch.tensor(beta_init))\n\t        self.register_buffer(\"beta_min\", torch.tensor(beta_min))\n", "        self.register_buffer(\"beta_max\", torch.tensor(1e10))\n\t        if speedup is not None:\n\t            self.register_buffer(\"speedup\", torch.tensor(speedup))\n\t    def to_cuda(self):\n\t        self.beta = self.beta.cuda()\n\t        if self.err_ema is not None:\n\t            self.err_ema = self.err_ema.cuda()\n\t    def loss(self, err, kld):\n\t        # Compute loss with current beta\n\t        loss = err + self.beta * kld\n", "        # Update beta without computing / backpropping gradients\n\t        with torch.no_grad():\n\t            if self.err_ema is None:\n\t                self.err_ema = err\n\t            else:\n\t                self.err_ema = (1.0 - self.alpha) * err + self.alpha * self.err_ema\n\t            constraint = self.goal - self.err_ema\n\t            if self.speedup is not None and constraint.item() > 0:\n\t                factor = torch.exp(self.speedup * self.step_size * constraint)\n\t            else:\n", "                factor = torch.exp(self.step_size * constraint)\n\t            self.beta = (factor * self.beta).clamp(self.beta_min, self.beta_max)\n\t        # Return loss\n\t        return loss\n"]}
{"filename": "src/models/components/slota/slota.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\tclass SlotAttention(nn.Module):\n\t    \"\"\"Slot Attention module.\n\t    Args:\n\t        num_slots: int - Number of slots in Slot Attention.\n\t        iterations: int - Number of iterations in Slot Attention.\n\t        num_attn_heads: int - Number of multi-head attention in Slot Attention,\n\t    \"\"\"\n\t    def __init__(\n", "        self,\n\t        num_slots: int = 7,\n\t        num_iterations: int = 3,\n\t        num_attn_heads: int = 1,\n\t        slot_dim: int = 64,\n\t        hid_dim: int = 64,\n\t        mlp_hid_dim: int = 128,\n\t        eps: float = 1e-8,\n\t    ):\n\t        super().__init__()\n", "        self.num_slots = num_slots\n\t        self.num_iterations = num_iterations\n\t        self.num_attn_heads = num_attn_heads\n\t        self.slot_dim = slot_dim\n\t        self.hid_dim = hid_dim\n\t        self.mlp_hid_dim = mlp_hid_dim\n\t        self.eps = eps\n\t        self.scale = (num_slots // num_attn_heads) ** -0.5\n\t        self.slots_mu = nn.Parameter(torch.randn(1, 1, self.slot_dim))\n\t        self.slots_sigma = nn.Parameter(torch.randn(1, 1, self.slot_dim))\n", "        self.norm_input = nn.LayerNorm(self.hid_dim)\n\t        self.norm_slot = nn.LayerNorm(self.slot_dim)\n\t        self.norm_mlp = nn.LayerNorm(self.slot_dim)\n\t        self.to_q = nn.Linear(self.slot_dim, self.slot_dim)\n\t        self.to_k = nn.Linear(self.hid_dim, self.slot_dim)\n\t        self.to_v = nn.Linear(self.hid_dim, self.slot_dim)\n\t        self.gru = nn.GRUCell(self.slot_dim, self.slot_dim)\n\t        self.mlp = nn.Sequential(\n\t            nn.Linear(self.slot_dim, self.mlp_hid_dim),\n\t            nn.ReLU(),\n", "            nn.Linear(self.mlp_hid_dim, self.slot_dim),\n\t        )\n\t    def forward(self, inputs, num_slots=None, train=False):\n\t        outputs = dict()\n\t        B, N_in, D_in = inputs.shape\n\t        K = num_slots if num_slots is not None else self.num_slots\n\t        D_slot = self.slot_dim\n\t        N_heads = self.num_attn_heads\n\t        mu = self.slots_mu.expand(B, K, -1)\n\t        sigma = self.slots_sigma.expand(B, K, -1)\n", "        slots = torch.normal(mu, torch.abs(sigma) + self.eps)\n\t        inputs = self.norm_input(inputs)\n\t        k = self.to_k(inputs).reshape(B, N_in, N_heads, -1).transpose(1, 2)\n\t        v = self.to_v(inputs).reshape(B, N_in, N_heads, -1).transpose(1, 2)\n\t        # k, v: (B, N_heads, N_in, D_slot // N_heads).\n\t        if not train:\n\t            attns = list()\n\t        for iter_idx in range(self.num_iterations):\n\t            slots_prev = slots\n\t            slots = self.norm_slot(slots)\n", "            q = self.to_q(slots).reshape(B, K, N_heads, -1).transpose(1, 2)\n\t            # q: (B, N_heads, K, slot_D // N_heads)\n\t            attn_logits = torch.einsum(\"bhid, bhjd->bhij\", k, q) * self.scale\n\t            attn = attn_logits.softmax(dim=-1) + self.eps  # Normalization over slots\n\t            # attn: (B, N_heads, N_in, K)\n\t            if not train:\n\t                attns.append(attn)\n\t            attn = attn / torch.sum(attn, dim=-2, keepdim=True)  # Weighted mean\n\t            # attn: (B, N_heads, N_in, K)\n\t            updates = torch.einsum(\"bhij,bhid->bhjd\", attn, v)\n", "            # updates: (B, N_heads, K, slot_D // N_heads)\n\t            updates = updates.transpose(1, 2).reshape(B, K, -1)\n\t            # updates: (B, K, slot_D)\n\t            slots = self.gru(updates.reshape(-1, D_slot), slots_prev.reshape(-1, D_slot))\n\t            slots = slots.reshape(B, -1, D_slot)\n\t            slots = slots + self.mlp(self.norm_mlp(slots))\n\t        outputs[\"slots\"] = slots\n\t        outputs[\"attn\"] = attn\n\t        if not train:\n\t            outputs[\"attns\"] = torch.stack(attns, dim=1)\n", "            # attns: (B, T, N_heads, N_in, K)\n\t        return outputs\n\tif __name__ == \"__main__\":\n\t    _ = SlotAttention()\n"]}
{"filename": "src/models/components/slota/slota_utils.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\timport numpy as np\n\timport torch\n\tfrom torch import nn\n\tclass SoftPositionEmbed(nn.Module):\n\t    \"\"\"Builds the soft position embedding layer with learnable projection.\n\t    Args:\n\t        hid_dim (int): Size of input feature dimension.\n\t        resolution (tuple): Tuple of integers specifying width and height of grid.\n\t    \"\"\"\n", "    def __init__(\n\t        self,\n\t        hid_dim: int = 64,\n\t        resolution: Tuple[int, int] = (128, 128),\n\t    ):\n\t        super().__init__()\n\t        self.embedding = nn.Linear(4, hid_dim, bias=True)\n\t        self.grid = self.build_grid(resolution)\n\t    def forward(self, inputs):\n\t        self.grid = self.grid.to(inputs.device)\n", "        grid = self.embedding(self.grid).to(inputs.device)\n\t        return inputs + grid\n\t    def build_grid(self, resolution):\n\t        ranges = [np.linspace(0.0, 1.0, num=res) for res in resolution]\n\t        grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n\t        grid = np.stack(grid, axis=-1)\n\t        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n\t        grid = np.expand_dims(grid, axis=0)\n\t        grid = grid.astype(np.float32)\n\t        return torch.from_numpy(np.concatenate([grid, 1.0 - grid], axis=-1))\n", "class Encoder(nn.Module):\n\t    def __init__(\n\t        self,\n\t        img_size: int = 128,\n\t        hid_dim: int = 64,\n\t        enc_depth: int = 4,\n\t    ):\n\t        super().__init__()\n\t        assert enc_depth > 2, \"Depth must be larger than 2.\"\n\t        convs = nn.ModuleList([nn.Conv2d(3, hid_dim, 5, padding=\"same\"), nn.ReLU()])\n", "        for _ in range(enc_depth - 2):\n\t            convs.extend([nn.Conv2d(hid_dim, hid_dim, 5, padding=\"same\"), nn.ReLU()])\n\t        convs.append(nn.Conv2d(hid_dim, hid_dim, 5, padding=\"same\"))\n\t        self.convs = nn.Sequential(*convs)\n\t        self.encoder_pos = SoftPositionEmbed(hid_dim, (img_size, img_size))\n\t        self.layer_norm = nn.LayerNorm([img_size * img_size, hid_dim])\n\t        self.mlp = nn.Sequential(\n\t            nn.Linear(hid_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim)\n\t        )\n\t    def forward(self, x):\n", "        x = self.convs(x)  # [B, D, H, W]\n\t        x = x.permute(0, 2, 3, 1)  # [B, H, W ,D]\n\t        x = self.encoder_pos(x)\n\t        x = torch.flatten(x, 1, 2)\n\t        x = self.layer_norm(x)\n\t        x = self.mlp(x)\n\t        return x\n\tclass Decoder(nn.Module):\n\t    def __init__(\n\t        self,\n", "        img_size: int = 128,\n\t        slot_dim: int = 64,\n\t        dec_hid_dim: int = 64,\n\t        dec_init_size: int = 8,\n\t        dec_depth: int = 6,\n\t    ):\n\t        super().__init__()\n\t        self.img_size = img_size\n\t        self.dec_init_size = dec_init_size\n\t        self.decoder_pos = SoftPositionEmbed(slot_dim, (dec_init_size, dec_init_size))\n", "        D_slot = slot_dim\n\t        D_hid = dec_hid_dim\n\t        upsample_step = int(np.log2(img_size // dec_init_size))\n\t        deconvs = nn.ModuleList()\n\t        count_layer = 0\n\t        for _ in range(upsample_step):\n\t            deconvs.extend(\n\t                [\n\t                    nn.ConvTranspose2d(\n\t                        D_hid if count_layer > 0 else D_slot,\n", "                        D_hid,\n\t                        5,\n\t                        stride=(2, 2),\n\t                        padding=2,\n\t                        output_padding=1,\n\t                    ),\n\t                    nn.ReLU(),\n\t                ]\n\t            )\n\t            count_layer += 1\n", "        for _ in range(dec_depth - upsample_step - 1):\n\t            deconvs.extend(\n\t                [\n\t                    nn.ConvTranspose2d(\n\t                        D_hid if count_layer > 0 else D_slot, D_hid, 5, stride=(1, 1), padding=2\n\t                    ),\n\t                    nn.ReLU(),\n\t                ]\n\t            )\n\t            count_layer += 1\n", "        deconvs.append(nn.ConvTranspose2d(D_hid, 4, 3, stride=(1, 1), padding=1))\n\t        self.deconvs = nn.Sequential(*deconvs)\n\t    def forward(self, x):\n\t        \"\"\"Broadcast slot features to a 2D grid and collapse slot dimension.\"\"\"\n\t        x = x.reshape(-1, x.shape[-1]).unsqueeze(1).unsqueeze(2)\n\t        x = x.repeat((1, self.dec_init_size, self.dec_init_size, 1))\n\t        x = self.decoder_pos(x)\n\t        x = x.permute(0, 3, 1, 2)\n\t        x = self.deconvs(x)\n\t        x = x[:, :, : self.img_size, : self.img_size]\n", "        x = x.permute(0, 2, 3, 1)\n\t        return x"]}
{"filename": "src/models/components/slota/slota_ae.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\timport numpy as np\n\timport torch\n\tfrom torch import nn\n\tfrom src.models.components.slota.slota import SlotAttention\n\tfrom src.models.components.slota.slota_utils import Decoder, Encoder\n\tclass SlotAttentionAutoEncoder(nn.Module):\n\t    \"\"\"Builds Slot Attention-based auto-encoder for object discovery.\n\t    Args:\n\t        num_slots (int): Number of slots in Slot Attention.\n", "    \"\"\"\n\t    def __init__(\n\t        self,\n\t        img_size: int = 128,\n\t        num_slots: int = 7,\n\t        num_iterations: int = 3,\n\t        num_attn_heads: int = 1,\n\t        hid_dim: int = 64,\n\t        slot_dim: int = 64,\n\t        mlp_hid_dim: int = 128,\n", "        eps: float = 1e-8,\n\t        enc_depth: int = 4,\n\t        dec_hid_dim: int = 64,\n\t        dec_init_size: int = 8,\n\t        dec_depth: int = 6,\n\t    ):\n\t        super().__init__()\n\t        self.num_slots = num_slots\n\t        self.encoder_cnn = Encoder(\n\t            img_size=img_size,\n", "            hid_dim=hid_dim,\n\t            enc_depth=enc_depth,\n\t        )\n\t        self.decoder_cnn = Decoder(\n\t            img_size=img_size,\n\t            slot_dim=slot_dim,\n\t            dec_hid_dim=dec_hid_dim,\n\t            dec_init_size=dec_init_size,\n\t            dec_depth=dec_depth,\n\t        )\n", "        self.slot_attention = SlotAttention(\n\t            num_slots=num_slots,\n\t            num_iterations=num_iterations,\n\t            num_attn_heads=num_attn_heads,\n\t            slot_dim=slot_dim,\n\t            hid_dim=hid_dim,\n\t            mlp_hid_dim=mlp_hid_dim,\n\t            eps=eps,\n\t        )\n\t    def forward(self, image, train=True):\n", "        # image: (batch_size, num_channels, height, width)\n\t        B, C, H, W = image.shape\n\t        # Convolutional encoder with position embedding\n\t        x = self.encoder_cnn(image)  # CNN Backbone\n\t        # x: (B, height * width, hid_dim)\n\t        # Slot Attention module.\n\t        slota_outputs = self.slot_attention(x, train=train)\n\t        slots = slota_outputs[\"slots\"]\n\t        # slots: (N, K, slot_dim)\n\t        x = self.decoder_cnn(slots)\n", "        # x: (B*K, height, width, num_channels+1)\n\t        # Undo combination of slot and batch dimension; split alpha masks\n\t        recons, masks = x.reshape(B, self.num_slots, H, W, C + 1).split([3, 1], dim=-1)\n\t        # recons: (B, K, height, width, num_channels)\n\t        # masks: (B, K, height, width, 1)\n\t        # Normalize alpha masks over slots.\n\t        masks = nn.Softmax(dim=1)(masks)\n\t        recon_combined = torch.sum(recons * masks, dim=1)  # Recombine image\n\t        recon_combined = recon_combined.permute(0, 3, 1, 2)\n\t        # recon_combined: (batch_size, num_channels, height, width)\n", "        outputs = dict()\n\t        outputs[\"recon_combined\"] = recon_combined\n\t        outputs[\"recons\"] = recons\n\t        outputs[\"masks\"] = masks\n\t        outputs[\"slots\"] = slots\n\t        outputs[\"attn\"] = slota_outputs[\"attn\"]\n\t        if not train:\n\t            outputs[\"attns\"] = slota_outputs[\"attns\"]\n\t            # attns: (B, T, N_heads, N_in, K)\n\t        return outputs\n", "if __name__ == \"__main__\":\n\t    _ = SlotAttentionAutoEncoder()\n"]}
{"filename": "src/models/components/monet/monet.py", "chunked_list": ["\"\"\"\n\tsource: https://github.com/karazijal/clevrtex/blob/master/ool/picture/models/monet.py\n\tReimplementation of MONet\n\t\"MONet: Unsupervised Scene Decomposition and Representation\"\n\tChristopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra,\n\tIrina Higgins, Matt Botvinick and Alexander Lerchner\n\thttps://arxiv.org/abs/1901.11390\n\tSomewhat based on implementation from\n\thttps://github.com/baudm/MONet-pytorch\n\t\"\"\"\n", "import itertools\n\tfrom typing import Union\n\timport numpy as np\n\t# import ipdb as ipdb\n\timport torch\n\timport torch.distributions as dist\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.distributions.kl import kl_divergence\n\tfrom src.models.components.monet.monet_base import MONetBase\n", "class ComponentEncoder(nn.Sequential):\n\t    \"\"\"The paper seems to to target the cell_width of 12x12 pixels.\"\"\"\n\t    def __init__(self, in_shape, z_dim=16):\n\t        inc, h, w = in_shape\n\t        h = ((((h + 1) // 2 + 1) // 2 + 1) // 2 + 1) // 2\n\t        w = ((((w + 1) // 2 + 1) // 2 + 1) // 2 + 1) // 2\n\t        super().__init__(\n\t            nn.Conv2d(inc + 1, 32, kernel_size=3, stride=2, padding=1),\n\t            nn.ReLU(),\n\t            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n", "            nn.ReLU(),\n\t            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n\t            nn.ReLU(),\n\t            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n\t            nn.ReLU(),\n\t            nn.Flatten(),\n\t            nn.Linear(h * w * 64, 256),\n\t            nn.ReLU(),\n\t            nn.Linear(256, 2 * z_dim),\n\t        )\n", "        self.z_dim = z_dim\n\t    def forward(self, img, mask):\n\t        x = torch.cat([img, mask], dim=1)\n\t        x = super().forward(x)\n\t        mu, logstd = x[:, : self.z_dim], x[:, self.z_dim :]\n\t        return mu, logstd.exp()\n\tclass ComponentDecoder(nn.Sequential):\n\t    def __init__(self, z_dim, out_shape):\n\t        super().__init__(\n\t            nn.Conv2d(z_dim + 2, 32, kernel_size=3, stride=1, padding=0),\n", "            nn.ReLU(),\n\t            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n\t            nn.ReLU(),\n\t            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n\t            nn.ReLU(),\n\t            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n\t            nn.ReLU(),\n\t            nn.Conv2d(32, out_shape[0] + 1, kernel_size=1, stride=1, padding=0),\n\t        )\n\t        self.out_shape = out_shape\n", "    def forward(self, x):\n\t        h, w = self.out_shape[-2:]\n\t        h += 8\n\t        w += 8\n\t        x = x.view(x.size(0), -1, 1, 1).repeat(1, 1, h, w)\n\t        hs = torch.linspace(-1, 1, h, device=x.device, dtype=x.dtype)\n\t        ws = torch.linspace(-1, 1, w, device=x.device, dtype=x.dtype)\n\t        c = torch.stack(torch.meshgrid(hs, ws)).view(1, 2, h, w).repeat(x.size(0), 1, 1, 1)\n\t        x = torch.cat([x, c], dim=1)\n\t        x = super().forward(x)\n", "        img = torch.sigmoid(x[:, : self.out_shape[0]])\n\t        msk = x[:, -1:]\n\t        return img, msk\n\tclass AttentionBlock(nn.Module):\n\t    def __init__(self, input_nc, output_nc, resize=True):\n\t        super().__init__()\n\t        self.conv = nn.Conv2d(input_nc, output_nc, 3, padding=1, bias=False)\n\t        self.norm = nn.InstanceNorm2d(output_nc, affine=True)\n\t        self._resize = resize\n\t    def comb(self, x, y):\n", "        if x.shape == y.shape:\n\t            return torch.cat([x, y], dim=1)\n\t        return torch.cat(\n\t            [F.pad(x, (0, y.size(-1) - x.size(-1), 0, y.size(-2) - x.size(-2)), \"constant\", 0), y],\n\t            dim=1,\n\t        )\n\t    def forward(self, *inputs):\n\t        downsampling = len(inputs) == 1\n\t        x = inputs[0] if downsampling else self.comb(*inputs)\n\t        x = self.conv(x)\n", "        x = self.norm(x)\n\t        x = skip = F.relu(x)\n\t        if self._resize:\n\t            x = F.interpolate(skip, scale_factor=0.5 if downsampling else 2.0, mode=\"nearest\")\n\t        return (x, skip) if downsampling else x\n\tclass Attention(nn.Module):\n\t    def __init__(self, n_blocks, in_shape, ngf=64):\n\t        super().__init__()\n\t        c, h, w = in_shape\n\t        x = torch.zeros(1, c + 1, h, w)\n", "        self.downblocks = nn.ModuleList([AttentionBlock(c + 1, ngf, resize=True)])  # Fist\n\t        x = self.downblocks[-1](x)[0]\n\t        # print(x.shape)\n\t        upblocks = [AttentionBlock(2 * ngf, ngf, resize=False)]  # Last\n\t        for i in range(1, n_blocks - 1):\n\t            self.downblocks.append(\n\t                AttentionBlock(ngf * 2 ** (i - 1), ngf * min(2**i, 8), resize=True)\n\t            )\n\t            x = self.downblocks[-1](x)[0]\n\t            # print(x.shape)\n", "            upblocks.append(\n\t                AttentionBlock(2 * ngf * min(2**i, 8), ngf * min(2 ** (i - 1), 8), resize=True)\n\t            )\n\t        self.downblocks.append(\n\t            AttentionBlock(\n\t                ngf * min(2 ** (n_blocks - 1), 8), ngf * min(2 ** (n_blocks - 1), 8), resize=False\n\t            )\n\t        )\n\t        x = self.downblocks[-1](x)[0]\n\t        # print(x.shape)\n", "        upblocks.append(\n\t            AttentionBlock(\n\t                2 * ngf * min(2 ** (n_blocks - 1), 8),\n\t                ngf * min(2 ** (n_blocks - 1), 8),\n\t                resize=True,\n\t            )\n\t        )\n\t        self.upblocks = nn.ModuleList(list(reversed(upblocks)))\n\t        inc = np.product(x.shape)\n\t        self.mlp = nn.Sequential(\n", "            nn.Flatten(),\n\t            nn.Linear(inc, 128),\n\t            nn.ReLU(),\n\t            nn.Linear(128, 128),\n\t            nn.ReLU(),\n\t            nn.Linear(128, inc),\n\t            nn.ReLU(),\n\t        )\n\t        self.output = nn.Conv2d(ngf, 1, kernel_size=1)\n\t    def forward(self, x, log_sk):\n", "        x = torch.cat((x, log_sk), dim=1)\n\t        skips = []\n\t        for l in self.downblocks:\n\t            x, skip = l(x)\n\t            skips.append(skip)\n\t            # print('down', x.shape, skip.shape)\n\t        x = self.mlp(x).view(skips[-1].shape)\n\t        for l, skip in zip(self.upblocks, reversed(skips)):\n\t            # print('up', x.shape, skip.shape)\n\t            x = l(x, skip)\n", "        logits = self.output(x)\n\t        return F.logsigmoid(logits), F.logsigmoid(\n\t            -logits\n\t        )  # log(sigmoid(logits)), log(1-sigmoid(logits))\n\tclass MONet(MONetBase):\n\t    def __init__(\n\t        self,\n\t        n_slots: int = 7,\n\t        n_blocks: int = 6,\n\t        shape: Union[tuple, list] = (3, 128, 128),\n", "        z_dim: int = 16,\n\t        bg_scl: int = 0.09,\n\t        fg_scl: int = 0.11,\n\t    ):\n\t        super().__init__(\n\t            pres_dist_name=\"unused\",\n\t            output_dist=\"unused\",\n\t            output_hparam=1.0,\n\t            n_particles=1,\n\t            z_pres_prior_p=1,\n", "            z_where_prior_loc=[0, 0, 0, 0],\n\t            z_where_prior_scale=[1, 1, 1, 1],\n\t            z_what_prior_loc=[0.0] * z_dim,\n\t            z_what_prior_scale=[1.0] * z_dim,\n\t            z_depth_prior_loc=0.0,\n\t            z_depth_prior_scale=1.0,\n\t        )\n\t        self.n = n_slots\n\t        self.shape = shape\n\t        self.enc = ComponentEncoder(shape, z_dim)\n", "        self.dec = ComponentDecoder(z_dim, shape)\n\t        self.att = Attention(n_blocks, shape)\n\t        self.beta = 0.5\n\t        self.gamma = 0.5\n\t        self.bg_scl = bg_scl\n\t        self.fg_scl = fg_scl\n\t    def forward(self, x, train=False):\n\t        n, c, h, w = x.shape\n\t        m = torch.zeros(n, self.n, 1, h, w, device=x.device, dtype=x.dtype)\n\t        x_til = torch.zeros(n, self.n, c, h, w, device=x.device, dtype=x.dtype)\n", "        m_til_logits = torch.zeros(n, self.n, 1, h, w, device=x.device, dtype=x.dtype)\n\t        log_s_k = torch.zeros(n, 1, h, w, device=x.device, dtype=x.dtype)\n\t        if train:\n\t            kl = torch.zeros(n, 1, device=x.device, dtype=x.dtype)\n\t            rec_loss = torch.zeros(n, self.n, c, h, w, device=x.device, dtype=x.dtype)\n\t        for k in range(self.n):\n\t            if k == self.n - 1:\n\t                log_m_k = log_s_k\n\t            else:\n\t                log_alpha_k, log_one_minus_alpha_k = self.att(x, log_s_k)\n", "                # print('log_alpha', log_alpha_k.shape)\n\t                log_m_k = log_s_k + log_alpha_k\n\t                log_s_k = log_s_k + log_one_minus_alpha_k\n\t            loc, scale = self.enc(x, log_m_k)\n\t            z_k_post = dist.Normal(loc, scale)\n\t            z_k = z_k_post.rsample()\n\t            x_til[:, k], m_til_logits[:, k] = self.dec(z_k)\n\t            # print('xt mt', x_til[:, k].shape, m_til_logits[:, k].shape)\n\t            m[:, k] = log_m_k.exp()\n\t            if train:\n", "                scl = self.bg_scl if k == 0 else self.fg_scl\n\t                rec_loss[:, k] = log_m_k + dist.Normal(x_til[:, k], scl).log_prob(x)\n\t                # print('rec loss', rec_loss[:, k].shape)\n\t                kl += (\n\t                    kl_divergence(z_k_post, self.what_prior(z_k_post.batch_shape)).sum(\n\t                        1, keepdims=True\n\t                    )\n\t                    * self.beta\n\t                )\n\t        m_rec = x_til * m\n", "        canvas = m_rec.sum(1)\n\t        m_til = torch.log_softmax(m_til_logits, dim=1)\n\t        # print('m_til', m_til.shape)\n\t        # print('m_rec', m_rec.shape)\n\t        r = {\n\t            \"canvas\": canvas,\n\t            \"layers\": {\"patch\": x_til, \"mask\": m, \"recons\": m_rec, \"other_mask\": m_til},\n\t        }\n\t        if train:\n\t            kl_mask = F.kl_div(m_til, m, reduction=\"none\").sum((1, 2, 3, 4)) * self.gamma\n", "            # print('mask', kl_mask.shape)\n\t            rec_loss = torch.logsumexp(rec_loss, dim=1).sum((1, 2, 3))\n\t            # print('rec', rec_loss.shape)\n\t            loss = -rec_loss + kl.squeeze(1) + kl_mask\n\t            loss = loss.mean()\n\t            r[\"loss\"] = loss\n\t            r[\"rec_loss\"] = (-rec_loss).mean()\n\t            r[\"kl\"] = kl.mean()\n\t            r[\"kl_mask\"] = kl_mask.mean()\n\t        return r\n", "    def param_groups(self):\n\t        return [{\"params\": self.parameters(), \"lr\": 1}]\n"]}
{"filename": "src/models/components/monet/monet_base.py", "chunked_list": ["\"\"\"\n\tsource: https://github.com/karazijal/clevrtex/blob/master/ool/picture/models/ool_base.py\n\t\"\"\"\n\timport warnings\n\timport numpy as np\n\timport torch\n\timport torch.distributions as dist\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.distributions.kl import kl_divergence, register_kl\n", "from torch.distributions.utils import logits_to_probs\n\tclass MONetBase(nn.Module):\n\t    def __init__(\n\t        self,\n\t        n_particles=1,\n\t        output_dist=\"normal\",\n\t        output_hparam=0.3,\n\t        pres_dist_name=\"bernoulli\",\n\t        z_pres_temperature=1.0,\n\t        z_pres_prior_p=0.01,\n", "        z_where_prior_loc=[-2.197, -2.197, 0, 0],\n\t        z_where_prior_scale=[0.5, 0.5, 1, 1],\n\t        z_what_prior_loc=None,\n\t        z_what_prior_scale=None,\n\t        z_depth_prior_loc=None,\n\t        z_depth_prior_scale=None,\n\t        z_bg_prior_loc=None,\n\t        z_bg_prior_scale=None,\n\t        z_sh_prior_loc=None,\n\t        z_sh_prior_scale=None,\n", "    ):\n\t        super().__init__()\n\t        self.n_particles = n_particles\n\t        self.pres_dist_name = pres_dist_name\n\t        self.output_dist_name = output_dist\n\t        self.output_hparam = output_hparam\n\t        self.z_pres_temperature = z_pres_temperature\n\t        self.z_pres_prior_p = z_pres_prior_p\n\t        self.z_where_prior_loc = z_where_prior_loc\n\t        self.z_where_prior_scale = z_where_prior_scale\n", "        if z_what_prior_loc is not None:\n\t            self.z_what_prior_loc = z_what_prior_loc\n\t        if z_what_prior_scale is not None:\n\t            self.z_what_prior_scale = z_what_prior_scale\n\t        if z_depth_prior_loc is not None:\n\t            self.z_depth_prior_loc = z_depth_prior_loc\n\t        if z_depth_prior_scale is not None:\n\t            self.z_depth_prior_scale = z_depth_prior_scale\n\t        if z_bg_prior_loc is not None:\n\t            self.z_bg_prior_loc = z_bg_prior_loc\n", "        if z_bg_prior_scale is not None:\n\t            self.z_bg_prior_scale = z_bg_prior_scale\n\t        if z_sh_prior_loc is not None:\n\t            self.z_sh_prior_loc = z_sh_prior_loc\n\t        if z_sh_prior_scale is not None:\n\t            self.z_sh_prior_scale = z_sh_prior_scale\n\t        self._prints = set()\n\t    def ensure_correct_tensor(self, maybe_tensor, other=None):\n\t        if not isinstance(maybe_tensor, torch.Tensor):\n\t            if isinstance(maybe_tensor, (int, float)):\n", "                mult = 1\n\t                if other is not None:\n\t                    mult = other.shape[-1]\n\t                maybe_tensor = torch.tensor([maybe_tensor] * mult)\n\t            else:\n\t                maybe_tensor = torch.tensor(maybe_tensor)\n\t        maybe_tensor = maybe_tensor.view(1, -1)\n\t        if other is not None:\n\t            maybe_tensor = maybe_tensor.to(other)\n\t        return maybe_tensor\n", "    def __set_tensor_value(self, maybe_tensor, dest: torch.Tensor):\n\t        if not isinstance(maybe_tensor, torch.Tensor):\n\t            if isinstance(maybe_tensor, (int, float)):\n\t                dest.fill_(maybe_tensor)\n\t                return\n\t            if isinstance(maybe_tensor, (list, tuple, np.ndarray)):\n\t                maybe_tensor = torch.from_numpy(np.array(maybe_tensor)).view(1, -1).to(dest)\n\t        dest.copy_(maybe_tensor, non_blocking=True)\n\t    @property\n\t    def z_pres_temperature(self):\n", "        return self._z_pres_temperature\n\t    @property\n\t    def z_pres_prior_p(self):\n\t        return self._z_pres_prior_p\n\t    @property\n\t    def z_where_prior_loc(self):\n\t        return self._z_where_prior_loc\n\t    @property\n\t    def z_where_prior_scale(self):\n\t        return self._z_where_prior_scale\n", "    @property\n\t    def z_what_prior_loc(self):\n\t        return self._z_what_prior_loc\n\t    @property\n\t    def z_what_prior_scale(self):\n\t        return self._z_what_prior_scale\n\t    @property\n\t    def z_depth_prior_loc(self):\n\t        return self._z_depth_prior_loc\n\t    @property\n", "    def z_depth_prior_scale(self):\n\t        return self._z_depth_prior_scale\n\t    @property\n\t    def z_bg_prior_loc(self):\n\t        return self._z_bg_prior_loc\n\t    @property\n\t    def z_bg_prior_scale(self):\n\t        return self._z_bg_prior_scale\n\t    @property\n\t    def z_sh_prior_loc(self):\n", "        return self._z_sh_prior_loc\n\t    @property\n\t    def z_sh_prior_scale(self):\n\t        return self._z_sh_prior_scale\n\t    @property\n\t    def _tensor_spec(self):\n\t        try:\n\t            p = next(self.parameters())\n\t        except StopIteration:\n\t            p = nn.Parameter(torch.zeros(1))\n", "        return dict(device=p.device, dtype=p.dtype)\n\t    def pres_dist(self, p=None, batch_shape=None, name=None, logits=None):\n\t        # 1. / (1.0 - torch.tensor(1., dtype=torch.float32, device='cuda').clamp(min=eps, max=1. - eps))\n\t        # Seems to give non inf on 1e-7,\n\t        pres_dist_name = name or self.pres_dist_name\n\t        # spec = self._tensor_spec\n\t        # device = spec['device']\n\t        eps = 1e-6\n\t        if pres_dist_name == \"bernoulli\":\n\t            d = dist.Bernoulli(p.clamp(min=eps, max=1.0 - eps))\n", "        elif pres_dist_name == \"relaxedbernoulli-hard\" or pres_dist_name == \"gumbelsoftmax-st\":\n\t            # Gumber-Softmax\n\t            p = p.clamp(min=eps, max=1.0 - eps)\n\t            d = dist.RelaxedBernoulli(self.z_pres_temperature, p)\n\t            def s():\n\t                y = d.rsample()\n\t                y_hard = torch.round(y).to(torch.float)\n\t                return (y_hard - y).detach() + y  # Straight through\n\t            d.sample = s\n\t            if not hasattr(self, \"defined_kl\"):\n", "                self.defined_kl = True\n\t                @register_kl(dist.RelaxedBernoulli, dist.RelaxedBernoulli)\n\t                def kl_gumbel_softmax(p, q):\n\t                    n_particles = self.n_particles\n\t                    s = p.rsample()\n\t                    logps = p.log_prob(s) / n_particles\n\t                    logqs = q.log_prob(s) / n_particles\n\t                    for _ in range(1, n_particles):\n\t                        s = p.rsample()\n\t                        logps += p.log_prob(s) / n_particles\n", "                        logqs += q.log_prob(s) / n_particles\n\t                    return logps - logqs\n\t        elif (\n\t            pres_dist_name == \"relaxedbernoulli\"\n\t            or pres_dist_name == \"concrete\"\n\t            or pres_dist_name == \"gumbel-softmax\"\n\t        ):\n\t            # Gumber-Softmax\n\t            p = p.clamp(min=eps, max=1.0 - eps)\n\t            d = dist.RelaxedBernoulli(self.z_pres_temperature, p)\n", "            def s():\n\t                y = d.rsample()\n\t                if self.training:\n\t                    return y\n\t                y_hard = torch.round(y).to(torch.float)\n\t                return (y_hard - y).detach() + y  # Straight through\n\t            d.sample = s\n\t            if not hasattr(self, \"defined_kl\"):\n\t                self.defined_kl = True\n\t                @register_kl(dist.RelaxedBernoulli, dist.RelaxedBernoulli)\n", "                def kl_gumbel_softmax(p, q):\n\t                    n_particles = self.n_particles\n\t                    s = p.rsample()\n\t                    logps = p.log_prob(s) / n_particles\n\t                    logqs = q.log_prob(s) / n_particles\n\t                    for _ in range(1, n_particles):\n\t                        s = p.rsample()\n\t                        logps += p.log_prob(s) / n_particles\n\t                        logqs += q.log_prob(s) / n_particles\n\t                    return logps - logqs\n", "        elif pres_dist_name == \"relaxedbernoulli-bern_kl\":\n\t            p = p.clamp(min=eps, max=1.0 - eps)\n\t            d = dist.RelaxedBernoulli(self.z_pres_temperature, p)\n\t            def s():\n\t                y = d.rsample()\n\t                if self.training:\n\t                    return y\n\t                y_hard = torch.round(y).to(torch.float)\n\t                return (\n\t                    y_hard - y\n", "                ).detach() + y  # Straight through -- return y_hard with gradients of y\n\t            d.sample = s\n\t            if not hasattr(self, \"defined_kl\"):\n\t                self.defined_kl = True\n\t                @register_kl(dist.RelaxedBernoulli, dist.RelaxedBernoulli)\n\t                def kl_gumbel_softmax(p, q):\n\t                    pb = dist.Bernoulli(p.probs)\n\t                    qb = dist.Bernoulli(q.probs)\n\t                    return kl_divergence(pb, qb)\n\t                @register_kl(dist.Bernoulli, dist.RelaxedBernoulli)\n", "                def kl_relbern_bern(p, q):\n\t                    qb = dist.Bernoulli(q.probs)\n\t                    return kl_divergence(p, qb)\n\t        elif pres_dist_name == \"continuousbernoulli\":\n\t            #  The continuous Bernoulli: fixing a pervasive error in variational autoencoders,\n\t            #  Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845\n\t            p = p.clamp(min=eps, max=1.0 - eps)\n\t            d = dist.ContinuousBernoulli(p)\n\t            d.sample = d.rsample\n\t        else:\n", "            raise ValueError(f\"Unknown distribution {pres_dist_name}\")\n\t        if batch_shape:\n\t            d = d.expand(batch_shape)\n\t        return d\n\t    def where_prior(self, batch_shape=None):\n\t        d = dist.Normal(self.z_where_prior_loc, self.z_where_prior_scale)\n\t        if batch_shape:\n\t            d = d.expand(batch_shape)\n\t        return d\n\t    def what_prior(self, batch_shape=None):\n", "        d = dist.Normal(self.z_what_prior_loc, self.z_what_prior_scale)\n\t        if batch_shape:\n\t            d = d.expand(batch_shape)\n\t        return d\n\t    def bg_prior(self, batch_shape=None):\n\t        d = dist.Normal(self.z_bg_prior_loc, self.z_bg_prior_scale)\n\t        if batch_shape:\n\t            d = d.expand(batch_shape)\n\t        return d\n\t    def depth_prior(self, batch_shape=None):\n", "        d = dist.Normal(self.z_depth_prior_loc, self.z_depth_prior_scale)\n\t        if batch_shape:\n\t            d = d.expand(batch_shape)\n\t        return d\n\t    def shape_prior(self, batch_shape=None):\n\t        d = dist.Normal(self.z_sh_prior_loc, self.z_sh_prior_scale)\n\t        if batch_shape:\n\t            d = d.expand(batch_shape)\n\t        return d\n\t    def output_dist(self, canvas):\n", "        eps = 1e-6\n\t        if self.output_dist_name == \"normal\":\n\t            d = dist.Normal(canvas, self.output_hparam)\n\t            d.sample = d.rsample\n\t        elif self.output_dist_name == \"bernoulli\":\n\t            d = dist.Bernoulli(canvas.clamp(min=eps, max=1.0 - eps))\n\t        elif self.output_dist_name == \"relaxedbernoulli\":\n\t            # Gumber-Softmax\n\t            d = dist.RelaxedBernoulli(\n\t                torch.tensor([self.output_hparam], device=canvas.device),\n", "                canvas.clamp(min=eps, max=1.0 - eps),\n\t            )\n\t            d.sample = d.rsample\n\t            d.mean = canvas.clamp(min=eps, max=1.0 - eps)\n\t        elif self.output_dist_name == \"continuousbernoulli\":\n\t            #  The continuous Bernoulli: fixing a pervasive error in variational autoencoders,\n\t            #  Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845\n\t            d = dist.ContinuousBernoulli(canvas.clamp(min=eps, max=1.0 - eps))\n\t            d.sample = d.rsample\n\t        elif self.output_dist_name == \"discmixlogistic\":\n", "            # warnings.warn(\"discmixlogistic distribution requires logits, which do not support currently support summation\")\n\t            if isinstance(self.output_hparam, (tuple, list)):\n\t                nmix, nbits = self.output_hparam\n\t                d = DiscMixLogistic(canvas, nmix, nbits)\n\t            else:\n\t                d = DiscMixLogistic(canvas, num_mix=self.output_hparam)\n\t        elif self.output_dist_name == \"disclogistic\":\n\t            d = DiscLogistic(canvas)\n\t        else:\n\t            raise ValueError(f\"Unknown output distribution {self.output_dist_name}\")\n", "        return d\n\t    @z_pres_temperature.setter\n\t    def z_pres_temperature(self, value):\n\t        if hasattr(self, \"_z_pres_temperature\"):\n\t            # self._z_pres_temperature = self.ensure_correct_tensor(value, self._z_pres_temperature)\n\t            self.__set_tensor_value(value, self._z_pres_temperature)\n\t        else:\n\t            self.register_buffer(\"_z_pres_temperature\", torch.tensor(value).view(1, -1))\n\t    @z_pres_prior_p.setter\n\t    def z_pres_prior_p(self, value):\n", "        if hasattr(self, \"_z_pres_prior_p\"):\n\t            # self._z_pres_prior_p = self.ensure_correct_tensor(value, self._z_pres_prior_p)\n\t            self.__set_tensor_value(value, self._z_pres_prior_p)\n\t        else:\n\t            self.register_buffer(\"_z_pres_prior_p\", torch.tensor(value).view(1, -1))\n\t    @z_where_prior_loc.setter\n\t    def z_where_prior_loc(self, value):\n\t        if hasattr(self, \"_z_where_prior_loc\"):\n\t            # self._z_where_prior_loc = self.ensure_correct_tensor(value, self._z_where_prior_loc)\n\t            self.__set_tensor_value(value, self._z_where_prior_loc)\n", "        else:\n\t            self.register_buffer(\"_z_where_prior_loc\", torch.tensor(value).view(1, -1))\n\t    @z_where_prior_scale.setter\n\t    def z_where_prior_scale(self, value):\n\t        if hasattr(self, \"_z_where_prior_scale\"):\n\t            # self._z_where_prior_scale = self.ensure_correct_tensor(value, self._z_where_prior_scale)\n\t            self.__set_tensor_value(value, self._z_where_prior_scale)\n\t        else:\n\t            self.register_buffer(\"_z_where_prior_scale\", torch.tensor(value).view(1, -1))\n\t    @z_what_prior_loc.setter\n", "    def z_what_prior_loc(self, value):\n\t        if hasattr(self, \"_z_what_prior_loc\"):\n\t            # self._z_what_prior_loc = self.ensure_correct_tensor(value, self._z_what_prior_loc)\n\t            self.__set_tensor_value(value, self._z_what_prior_loc)\n\t        else:\n\t            self.register_buffer(\"_z_what_prior_loc\", torch.tensor(value).view(1, -1))\n\t    @z_what_prior_scale.setter\n\t    def z_what_prior_scale(self, value):\n\t        if hasattr(self, \"_z_what_prior_scale\"):\n\t            # self._z_what_prior_scale = self.ensure_correct_tensor(value, self._z_what_prior_scale)\n", "            self.__set_tensor_value(value, self._z_what_prior_scale)\n\t        else:\n\t            self.register_buffer(\"_z_what_prior_scale\", torch.tensor(value).view(1, -1))\n\t    @z_depth_prior_loc.setter\n\t    def z_depth_prior_loc(self, value):\n\t        if hasattr(self, \"_z_depth_prior_loc\"):\n\t            # self._z_depth_prior_loc = self.ensure_correct_tensor(value, self._z_depth_prior_loc)\n\t            self.__set_tensor_value(value, self._z_depth_prior_loc)\n\t        else:\n\t            self.register_buffer(\"_z_depth_prior_loc\", torch.tensor(value).view(1, -1))\n", "    @z_depth_prior_scale.setter\n\t    def z_depth_prior_scale(self, value):\n\t        if hasattr(self, \"_z_depth_prior_scale\"):\n\t            # self._z_depth_prior_scale = self.ensure_correct_tensor(value, self._z_depth_prior_scale)\n\t            self.__set_tensor_value(value, self._z_depth_prior_scale)\n\t        else:\n\t            self.register_buffer(\"_z_depth_prior_scale\", torch.tensor(value).view(1, -1))\n\t    def baseline_parameters(self):\n\t        if hasattr(self, \"baseline\") and self.baseline is not None:\n\t            return self.baseline.parameters()\n", "        return ()\n\t    def model_parameters(self):\n\t        if hasattr(self, \"baseline\") and self.baseline is not None:\n\t            baseline_parameters = set(self.baseline_parameters())\n\t            for name, param in self.named_parameters():\n\t                if param not in baseline_parameters:\n\t                    yield param\n\t        else:\n\t            return self.parameters()\n\t    @z_bg_prior_loc.setter\n", "    def z_bg_prior_loc(self, value):\n\t        if hasattr(self, \"_z_bg_prior_loc\"):\n\t            # self._z_bg_prior_loc = self.ensure_correct_tensor(value, self._z_bg_prior_loc)\n\t            self.__set_tensor_value(value, self._z_bg_prior_loc)\n\t        else:\n\t            self.register_buffer(\"_z_bg_prior_loc\", self.ensure_correct_tensor(value))\n\t    @z_bg_prior_scale.setter\n\t    def z_bg_prior_scale(self, value):\n\t        if hasattr(self, \"_z_bg_prior_scale\"):\n\t            # self._z_bg_prior_scale = self.ensure_correct_tensor(value, self._z_bg_prior_scale)\n", "            self.__set_tensor_value(value, self._z_bg_prior_scale)\n\t        else:\n\t            self.register_buffer(\"_z_bg_prior_scale\", self.ensure_correct_tensor(value))\n\t    @z_sh_prior_loc.setter\n\t    def z_sh_prior_loc(self, value):\n\t        if hasattr(self, \"_z_sh_prior_loc\"):\n\t            # self._z_sh_prior_loc = self.ensure_correct_tensor(value, self._z_sh_prior_loc)\n\t            self.__set_tensor_value(value, self._z_sh_prior_loc)\n\t        else:\n\t            self.register_buffer(\"_z_sh_prior_loc\", self.ensure_correct_tensor(value))\n", "    @z_sh_prior_scale.setter\n\t    def z_sh_prior_scale(self, value):\n\t        if hasattr(self, \"_z_sh_prior_scale\"):\n\t            # self._z_sh_prior_scale = self.ensure_correct_tensor(value, self._z_sh_prior_scale)\n\t            self.__set_tensor_value(value, self._z_sh_prior_scale)\n\t        else:\n\t            self.register_buffer(\"_z_sh_prior_scale\", self.ensure_correct_tensor(value))\n\t    def onceprint(self, *args, **kwargs):\n\t        \"\"\"Just a useful debug function to see shapes when first running.\"\"\"\n\t        k = \"_\".join(str(a) for a in args)\n", "        if k not in self._prints:\n\t            print(*args, **kwargs)\n\t            self._prints.add(k)\n\tclass DiscLogistic:\n\t    def __init__(self, param):\n\t        # B, C, H, W = param.size()\n\t        # self.num_c = C // 2\n\t        self.means, self.log_scales = param.chunk(2, 1)\n\t        self.log_scales = self.log_scales.clamp(min=-7.0)\n\t    @property\n", "    def mean(self):\n\t        img = self.means / 2.0 + 0.5\n\t        return img\n\t    def log_prob(self, samples):\n\t        # assert torch.max(samples) <= 1.0 and torch.min(samples) >= 0.0\n\t        # convert samples to be in [-1, 1]\n\t        samples = 2 * samples - 1.0\n\t        # B, C, H, W = samples.size()\n\t        # assert C == self.num_c\n\t        centered = samples - self.means  # B, 3, H, W\n", "        inv_stdv = torch.exp(-self.log_scales)\n\t        plus_in = inv_stdv * (centered + 1.0 / 255.0)\n\t        cdf_plus = torch.sigmoid(plus_in)\n\t        min_in = inv_stdv * (centered - 1.0 / 255.0)\n\t        cdf_min = torch.sigmoid(min_in)\n\t        log_cdf_plus = plus_in - F.softplus(plus_in)\n\t        log_one_minus_cdf_min = -F.softplus(min_in)\n\t        cdf_delta = cdf_plus - cdf_min\n\t        mid_in = inv_stdv * centered\n\t        log_pdf_mid = mid_in - self.log_scales - 2.0 * F.softplus(mid_in)\n", "        log_prob_mid_safe = torch.where(\n\t            cdf_delta > 1e-5,\n\t            torch.log(torch.clamp(cdf_delta, min=1e-10)),\n\t            log_pdf_mid - np.log(127.5),\n\t        )\n\t        # woow the original implementation uses samples > 0.999, this ignores the largest possible pixel value (255)\n\t        # which is mapped to 0.9922\n\t        log_probs = torch.where(\n\t            samples < -0.999,\n\t            log_cdf_plus,\n", "            torch.where(samples > 0.99, log_one_minus_cdf_min, log_prob_mid_safe),\n\t        )  # B, 3, H, W\n\t        return log_probs\n\t    def sample(self):\n\t        u = torch.empty(self.means.size(), device=self.means.device).uniform_(\n\t            1e-5, 1.0 - 1e-5\n\t        )  # B, 3, H, W\n\t        x = self.means + torch.exp(self.log_scales) * (\n\t            torch.log(u) - torch.log(1.0 - u)\n\t        )  # B, 3, H, W\n", "        x = torch.clamp(x, -1, 1.0)\n\t        x = x / 2.0 + 0.5\n\t        return x\n\tclass DiscMixLogistic:\n\t    def __init__(self, param, num_mix=10, num_bits=8):\n\t        B, C, H, W = param.size()\n\t        self.num_mix = num_mix\n\t        self.logit_probs = param[:, :num_mix, :, :]  # B, M, H, W\n\t        l = param[:, num_mix:, :, :].view(B, 3, 3 * num_mix, H, W)  # B, 3, 3 * M, H, W\n\t        self.means = l[:, :, :num_mix, :, :]  # B, 3, M, H, W\n", "        self.log_scales = torch.clamp(\n\t            l[:, :, num_mix : 2 * num_mix, :, :], min=-7.0\n\t        )  # B, 3, M, H, W\n\t        self.coeffs = torch.tanh(l[:, :, 2 * num_mix : 3 * num_mix, :, :])  # B, 3, M, H, W\n\t        self.max_val = 2.0**num_bits - 1\n\t    def one_hot(self, indices, depth, dim):\n\t        indices = indices.unsqueeze(dim)\n\t        size = list(indices.size())\n\t        size[dim] = depth\n\t        y_onehot = torch.zeros(size).to(indices.device)\n", "        y_onehot.scatter_(dim, indices, 1)\n\t        return y_onehot\n\t    def log_prob(self, samples):\n\t        assert torch.max(samples) <= 1.0 and torch.min(samples) >= 0.0\n\t        # convert samples to be in [-1, 1]\n\t        samples = 2 * samples - 1.0\n\t        B, C, H, W = samples.size()\n\t        assert C == 3, \"only RGB images are considered.\"\n\t        # samples = samples.unsqueeze(4)  # B, 3, H , W\n\t        # samples = samples.expand(-1, -1, -1, -1, self.num_mix).permute(0, 1, 4, 2, 3)  # B, 3, M, H, W\n", "        samples = samples.unsqueeze(2).expand(-1, -1, self.num_mix, -1, -1)  # B, 3, M, H, W\n\t        mean1 = self.means[:, 0, :, :, :]  # B, M, H, W\n\t        mean2 = (\n\t            self.means[:, 1, :, :, :] + self.coeffs[:, 0, :, :, :] * samples[:, 0, :, :, :]\n\t        )  # B, M, H, W\n\t        mean3 = (\n\t            self.means[:, 2, :, :, :]\n\t            + self.coeffs[:, 1, :, :, :] * samples[:, 0, :, :, :]\n\t            + self.coeffs[:, 2, :, :, :] * samples[:, 1, :, :, :]\n\t        )  # B, M, H, W\n", "        # mean1 = mean1.unsqueeze(1)  # B, 1, M, H, W\n\t        # mean2 = mean2.unsqueeze(1)  # B, 1, M, H, W\n\t        # mean3 = mean3.unsqueeze(1)  # B, 1, M, H, W\n\t        means = torch.stack([mean1, mean2, mean3], dim=1)  # B, 3, M, H, W\n\t        centered = samples - means  # B, 3, M, H, W\n\t        inv_stdv = torch.exp(-self.log_scales)\n\t        plus_in = inv_stdv * (centered + 1.0 / self.max_val)\n\t        cdf_plus = torch.sigmoid(plus_in)\n\t        min_in = inv_stdv * (centered - 1.0 / self.max_val)\n\t        cdf_min = torch.sigmoid(min_in)\n", "        log_cdf_plus = plus_in - F.softplus(plus_in)\n\t        log_one_minus_cdf_min = -F.softplus(min_in)\n\t        cdf_delta = cdf_plus - cdf_min\n\t        mid_in = inv_stdv * centered\n\t        log_pdf_mid = mid_in - self.log_scales - 2.0 * F.softplus(mid_in)\n\t        log_prob_mid_safe = torch.where(\n\t            cdf_delta > 1e-5,\n\t            torch.log(torch.clamp(cdf_delta, min=1e-10)),\n\t            log_pdf_mid - np.log(self.max_val / 2),\n\t        )\n", "        # the original implementation uses samples > 0.999, this ignores the largest possible pixel value (255)\n\t        # which is mapped to 0.9922\n\t        log_probs = torch.where(\n\t            samples < -0.999,\n\t            log_cdf_plus,\n\t            torch.where(samples > 0.99, log_one_minus_cdf_min, log_prob_mid_safe),\n\t        )  # B, 3, M, H, W\n\t        log_probs = torch.sum(log_probs, 1) + F.log_softmax(self.logit_probs, dim=1)  # B, M, H, W\n\t        return torch.logsumexp(log_probs, dim=1, keepdim=True)  # B, 1, H, W\n\t    def sample(self, t=1.0):\n", "        # Form a (incomplete) sample from relaxed one-hot categorical and then select the highest value.\n\t        # This basically skips a couple of steps of sampling for speed (and memory)\n\t        gumbel = -torch.log(\n\t            -torch.log(torch.empty_like(self.logit_probs).uniform_(1e-5, 1.0 - 1e-5))\n\t        )  # B, M, H, W\n\t        sel = self.one_hot(\n\t            torch.argmax(self.logit_probs + gumbel, 1), self.num_mix, dim=1\n\t        )  # B, M, H, W\n\t        sel = sel.unsqueeze(1)  # B, 1, M, H, W\n\t        # select logistic parameters\n", "        means = torch.sum(self.means * sel, dim=2)  # B, 3, H, W\n\t        log_scales = torch.sum(self.log_scales * sel, dim=2)  # B, 3, H, W\n\t        coeffs = torch.sum(self.coeffs * sel, dim=2)  # B, 3, H, W\n\t        # cells from logistic & clip to interval\n\t        # we don't actually round to the nearest 8bit value when sampling\n\t        u = torch.empty_like(means).uniform_(1e-5, 1.0 - 1e-5)  # B, 3, H, W\n\t        x = means + torch.exp(log_scales) / t * (torch.log(u) - torch.log(1.0 - u))  # B, 3, H, W\n\t        x0 = torch.clamp(x[:, 0, :, :], -1, 1.0)  # B, H, W\n\t        x1 = torch.clamp(x[:, 1, :, :] + coeffs[:, 0, :, :] * x0, -1, 1)  # B, H, W\n\t        x2 = torch.clamp(\n", "            x[:, 2, :, :] + coeffs[:, 1, :, :] * x0 + coeffs[:, 2, :, :] * x1, -1, 1\n\t        )  # B, H, W\n\t        x0 = x0.unsqueeze(1)\n\t        x1 = x1.unsqueeze(1)\n\t        x2 = x2.unsqueeze(1)\n\t        x = torch.cat([x0, x1, x2], 1)\n\t        x = x / 2.0 + 0.5\n\t        return x\n\t    def rsample(self, t=1.0):\n\t        # form a sample from\n", "        gumbel = -torch.log(\n\t            -torch.log(torch.empty_like(self.logit_probs).uniform_(1e-5, 1.0 - 1e-5))\n\t        )  # B, M, H, W\n\t        sel = (self.logit_probs + gumbel / t).softmax(dim=1)\n\t        sel = sel.unsqueeze(1).clamp(min=1e-19)  # B, 1, M, H, W\n\t        # select logistic parameters\n\t        means = torch.sum(self.means * sel, dim=2)  # B, 3, H, W\n\t        log_scales = (\n\t            torch.logsumexp(2 * self.log_scales + 2 * sel.log(), dim=2) * 0.5\n\t        )  # B, 3, H, W\n", "        coeffs = torch.sum(self.coeffs * sel, dim=2)  # B, 3, H, W\n\t        # cells from logistic & clip to interval\n\t        # we don't actually round to the nearest 8bit value when sampling\n\t        u = torch.empty_like(means).uniform_(1e-5, 1.0 - 1e-5)  # B, 3, H, W\n\t        x = means + torch.exp(log_scales) / t * (torch.log(u) - torch.log(1.0 - u))  # B, 3, H, W\n\t        x0 = torch.clamp(x[:, 0, :, :], -1, 1.0)  # B, H, W\n\t        x1 = torch.clamp(x[:, 1, :, :] + coeffs[:, 0, :, :] * x0, -1, 1)  # B, H, W\n\t        x2 = torch.clamp(\n\t            x[:, 2, :, :] + coeffs[:, 1, :, :] * x0 + coeffs[:, 2, :, :] * x1, -1, 1\n\t        )  # B, H, W\n", "        x0 = x0.unsqueeze(1)\n\t        x1 = x1.unsqueeze(1)\n\t        x2 = x2.unsqueeze(1)\n\t        x = torch.cat([x0, x1, x2], 1)\n\t        x = x / 2.0 + 0.5\n\t        return x\n\t    @property\n\t    def mean(self):\n\t        probs = self.logit_probs.softmax(dim=1).unsqueeze(1)\n\t        means = torch.sum(self.means * probs, dim=2)\n", "        coeffs = torch.sum(self.coeffs * probs, dim=2)\n\t        x0 = means[:, 0].clamp(-1, 1.0)\n\t        x1 = (means[:, 1] + coeffs[:, 0] * x0).clamp(-1, 1.0)\n\t        x2 = (means[:, 2] + coeffs[:, 1] + x0 + coeffs[:, 2] * x1).clamp(-1, 1.0)\n\t        img = torch.stack([x0, x1, x2]).transpose(0, 1) / 2.0 + 0.5\n\t        return img\n\t    @staticmethod\n\t    def apply_mask(logits, mask, num_mix=10):\n\t        B, *other, H, W = logits.shape\n\t        lps = logits[..., :num_mix, :, :] + mask.clamp(min=1e-5).log()\n", "        l = logits[..., num_mix:, :, :]\n\t        l = l.view(B, *other[:-1], 3, 3 * num_mix, H, W)\n\t        means = l[..., :, :num_mix, :, :]\n\t        log_scales = l[..., :, num_mix : 2 * num_mix, :, :]\n\t        coeffs = l[..., :, 2 * num_mix : 3 * num_mix, :, :]\n\t        means = (means + 1) * mask.unsqueeze(-4) - 1  # push towards -1 rather than 0\n\t        log_scales = log_scales + mask.unsqueeze(-4).clamp(min=1e-5).log()\n\t        coeffs = coeffs * mask.unsqueeze(-4)\n\t        l = torch.cat([means, log_scales, coeffs], -4)\n\t        return torch.cat([lps, l.view(B, *other[:-1], -1, H, W)], -3)\n", "    @staticmethod\n\t    def sum(logits, dim=1, num_mix=10):\n\t        raise NotImplementedError()\n\t        B, *other, H, W = logits.shape\n\t        lps = logits[..., :num_mix, :, :]\n\t        lps = lps.sum(dim=dim)\n\t        l = logits[..., num_mix:, :, :]\n\t        l = l.view(B, *other[:-1], 3, 3 * num_mix, H, W)\n\t        means = l[..., :, :num_mix, :, :]\n\t        log_scales = l[..., :, num_mix : 2 * num_mix, :, :]\n", "        coeffs = l[..., :, 2 * num_mix : 3 * num_mix, :, :]\n\t        means = means.sum(dim=dim)\n\t        log_scales = torch.logsumexp(2 * log_scales, dim=dim) * 0.5\n\t        # This is how much channels add... cannot be a sum, but a mean is only a guess\n\t        # This this in after tanh\n\t        # tanh_coeffs = coeffs.tanh().mean(dim=dim).clamp(-1.+1e-5, 1.-1e-5)\n\t        # coeffs = .5 * ((1+tanh_coeffs).log() - (1-tanh_coeffs).log())\n\t        coeffs = coeffs.sum(dim=dim)\n\t        other = list(other)\n\t        del other[dim]\n", "        l = torch.cat([means, log_scales, coeffs], -4)\n\t        return torch.cat([lps, l.view(B, *other[:-1], -1, H, W)], -3)\n"]}
