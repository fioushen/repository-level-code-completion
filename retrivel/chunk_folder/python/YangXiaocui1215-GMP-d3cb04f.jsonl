{"filename": "twitter_ae_training_for_generated_prompt_multitasks.py", "chunked_list": ["import argparse\n\timport json\n\timport os\n\tfrom datetime import datetime\n\tfrom torch import optim\n\timport torch\n\timport torch.multiprocessing as mp\n\tfrom torch.cuda.amp import GradScaler\n\tfrom torch.nn.parallel import DistributedDataParallel as DDP\n\tfrom torch.utils.data import DataLoader, ConcatDataset\n", "from torch.utils.data.distributed import DistributedSampler\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom transformers import AdamW\n\timport random\n\tfrom src.data.collation_for_prompt_multitasks import Collator\n\tfrom src.data.dataset_for_prompt import MVSA_Dataset, Twitter_Dataset\n\tfrom src.data.tokenization_new_for_generated_prompt_multitasks import ConditionTokenizer\n\tfrom src.model.config import MultiModalBartConfig\n\tfrom src.model.MAESC_model_for_generated_aspect_prompt_multitasks import MultiModalBartModel_AESC\n\tfrom src.model.model import MultiModalBartModelForPretrain\n", "from src.training_multitasks import fine_tune\n\tfrom src.utils import Logger, save_training_data, load_training_data, setup_process, cleanup_process\n\tfrom src.model.metrics import AESCSpanMetric, OESpanMetric\n\tfrom src.model.generater_for_generated_prompt_multitasks import SequenceGeneratorModel\n\timport src.eval_utils_multitasks as eval_utils\n\timport numpy as np\n\timport torch.backends.cudnn as cudnn\n\tdef get_parameter_number(model):\n\t    total_num = sum(p.numel() for p in model.parameters())\n\t    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "    return {'Total': total_num, 'Trainable': trainable_num}\n\tdef main(rank, args):\n\t    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n\t    checkpoint_path = os.path.join(args.checkpoint_dir, timestamp)\n\t    tb_writer = None\n\t    add_name = 'epoch_num' + str(args.epochs)\n\t    add_name += 'last'\n\t    if args.is_sample:\n\t        add_name += 'sample_num' + str(args.sample_num)\n\t        add_name += 'start_idx' + str(args.start_idx)\n", "    if args.text_only:\n\t        add_name += ' only text'\n\t    else:\n\t        add_name += ' multi'\n\t    if args.bart_init == 0:\n\t        add_name += '_random_init_'\n\t    if args.checkpoint:\n\t        add_name = add_name + '-pretrain' + args.checkpoint.split('/')[-2]\n\t    add_name = add_name + str(args.lr)\n\t    log_dir = os.path.join(args.log_dir, timestamp + add_name)\n", "    # make log dir and tensorboard writer if log_dir is specified\n\t    if rank == 0 and args.log_dir is not None:\n\t        os.makedirs(log_dir)\n\t        tb_writer = SummaryWriter(log_dir=log_dir)\n\t    logger = Logger(log_dir=os.path.join(log_dir, 'log.txt'),\n\t                    enabled=(rank == 0))\n\t    # prf_logger = Logger(log_dir=os.path.join(log_dir, 'prf_log.txt'),\n\t    #                     enabled=(rank == 0))\n\t    # make checkpoint dir if not exist\n\t    if args.is_check == 1 and not os.path.isdir(checkpoint_path):\n", "        os.makedirs(checkpoint_path)\n\t        logger.info('Made checkpoint directory: \"{}\"'.format(checkpoint_path))\n\t    logger.info('Initialed with {} GPU(s)'.format(args.gpu_num), pad=True)\n\t    for k, v in vars(args).items():\n\t        logger.info('{}: {}'.format(k, v))\n\t    # =========================== model =============================\n\t    logger.info('Loading model...')\n\t    if args.cpu:\n\t        device = 'cpu'\n\t        map_location = device\n", "    else:\n\t        device = torch.device(\"cuda:{}\".format(rank))\n\t        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n\t    tokenizer = ConditionTokenizer(args)\n\t    label_ids = list(tokenizer.mapping2id.values())\n\t    senti_ids = list(tokenizer.senti2id.values())\n\t    # print(label_ids)\n\t    # print(tokenizer.convert_ids_to_tokens(label_ids))\n\t    if args.model_config is not None:\n\t        bart_config = MultiModalBartConfig.from_dict(\n", "            json.load(open(args.model_config)))\n\t    else:\n\t        bart_config = MultiModalBartConfig.from_pretrained(args.checkpoint)\n\t    if args.dropout is not None:\n\t        bart_config.dropout = args.dropout\n\t    if args.attention_dropout is not None:\n\t        bart_config.attention_dropout = args.attention_dropout\n\t    if args.classif_dropout is not None:\n\t        bart_config.classif_dropout = args.classif_dropout\n\t    if args.activation_dropout is not None:\n", "        bart_config.activation_dropout = args.activation_dropout\n\t    bos_token_id = 0  # 因为是特殊符号\n\t    eos_token_id = 1\n\t    if args.checkpoint:\n\t        pretrain_model = MultiModalBartModelForPretrain.from_pretrained(\n\t            args.checkpoint,\n\t            config=bart_config,\n\t            bart_model=args.bart_model,\n\t            tokenizer=tokenizer,\n\t            label_ids=label_ids,\n", "            senti_ids=senti_ids,\n\t            args=args,\n\t            error_on_mismatch=False)\n\t        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n\t                                                 args.bart_model, tokenizer,\n\t                                                 label_ids)\n\t        seq2seq_model.encoder.load_state_dict(\n\t            pretrain_model.encoder.state_dict())\n\t        seq2seq_model.decoder.load_state_dict(\n\t            pretrain_model.span_decoder.state_dict())\n", "        model = SequenceGeneratorModel(seq2seq_model,\n\t                                       bos_token_id=bos_token_id,\n\t                                       eos_token_id=eos_token_id,\n\t                                       max_length=args.max_len,\n\t                                       max_len_a=args.max_len_a,\n\t                                       num_beams=args.num_beams,\n\t                                       do_sample=False,\n\t                                       repetition_penalty=1,\n\t                                       length_penalty=1.0,\n\t                                       pad_token_id=eos_token_id,\n", "                                       restricter=None)\n\t    else:\n\t        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n\t                                                 args.bart_model, tokenizer,\n\t                                                 label_ids)\n\t        model = SequenceGeneratorModel(seq2seq_model,\n\t                                       bos_token_id=bos_token_id,\n\t                                       eos_token_id=eos_token_id,\n\t                                       max_length=args.max_len,\n\t                                       max_len_a=args.max_len_a,\n", "                                       num_beams=args.num_beams,\n\t                                       do_sample=False,\n\t                                       repetition_penalty=1,\n\t                                       length_penalty=1.0,\n\t                                       pad_token_id=eos_token_id,\n\t                                       restricter=None)\n\t        # model = MultiModalBartModel_AESC(bart_config, args.bart_model,\n\t        #                                  tokenizer, label_ids)\n\t    model.to(device)\n\t    parameters = get_parameter_number(model) ##{'Total': 169351685, 'Trainable': 169351685}\n", "    print(parameters)\n\t    optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\t    scaler = GradScaler() if args.amp else None\n\t    epoch = 0\n\t    logger.info('Loading data...')\n\t    collate_twitter_ae = Collator(\n\t                                  task=args.task,\n\t                                  tokenizer=tokenizer,\n\t                                  mlm_enabled=False,\n\t                                  senti_enabled=False,\n", "                                  ae_enabled=False,\n\t                                  oe_enabled=False,\n\t                                  aesc_enabled=False,\n\t                                  anp_enabled=False,\n\t                                  twitter_ae_enabled=True,\n\t                                  has_prompt=args.has_prompt,\n\t                                  max_img_num=args.num_image_tokens,\n\t                                  text_only=args.text_only,\n\t                                  use_caption=args.use_caption)\n\t    train_dataset = Twitter_Dataset(args.dataset[0][1], split='train')\n", "    dev_dataset = Twitter_Dataset(args.dataset[0][1], split='dev')\n\t    test_dataset = Twitter_Dataset(args.dataset[0][1], split='test')\n\t    train_loader = DataLoader(dataset=train_dataset,\n\t                              batch_size=args.batch_size,\n\t                              shuffle=True,\n\t                              num_workers=args.num_workers,\n\t                              pin_memory=True,\n\t                              collate_fn=collate_twitter_ae)\n\t    dev_loader = DataLoader(dataset=dev_dataset,\n\t                            batch_size=args.batch_size,\n", "                            shuffle=False,\n\t                            num_workers=args.num_workers,\n\t                            pin_memory=True,\n\t                            collate_fn=collate_twitter_ae)\n\t    test_loader = DataLoader(dataset=test_dataset,\n\t                             batch_size=args.batch_size,\n\t                             shuffle=False,\n\t                             num_workers=args.num_workers,\n\t                             pin_memory=True,\n\t                             collate_fn=collate_twitter_ae)\n", "    callback = None\n\t    metric = OESpanMetric(eos_token_id, num_labels=len(label_ids))\n\t    model.train()\n\t    start = datetime.now()\n\t    best_dev_res = None\n\t    best_dev_test_res = None\n\t    best_test_res = None\n\t    while epoch < args.epochs:\n\t        logger.info('Epoch {}'.format(epoch + 1), pad=True)\n\t        fine_tune(epoch=epoch,\n", "                  model=model,\n\t                  train_loader=train_loader,\n\t                  test_loader=test_loader,\n\t                  metric=metric,\n\t                  optimizer=optimizer,\n\t                  args=args,\n\t                  device=device,\n\t                  logger=logger,\n\t                  callback=callback,\n\t                  log_interval=1,\n", "                  tb_writer=tb_writer,\n\t                  tb_interval=1,\n\t                  scaler=scaler)\n\t        if (epoch + 1) % args.eval_every == 0:\n\t            # train_dev = eval_utils.eval(model, train_loader, metric, device)\n\t            res_dev, dev_aspects_num_acc = eval_utils.eval(args, model, dev_loader, metric, device)\n\t            res_test, test_aspects_num_acc = eval_utils.eval(args, model, test_loader, metric, device)\n\t            logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}, dev_aspects_num_acc: {:.4f}'.format(\n\t                res_dev['oe_pre'], res_dev['oe_rec'], res_dev['oe_f'], dev_aspects_num_acc))\n\t            logger.info('TEST  ae_p:{} ae_r:{} ae_f:{}, test_aspects_num_acc: {:.4f}'.format(\n", "                res_test['oe_pre'], res_test['oe_rec'], res_test['oe_f'], test_aspects_num_acc))\n\t            # logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t            #     res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n\t            save_flag = False\n\t            if best_dev_res is None:\n\t                best_dev_res = res_dev\n\t                best_dev_test_res = res_test\n\t            else:\n\t                if best_dev_res['oe_f'] < res_dev['oe_f']:\n\t                    best_dev_res = res_dev\n", "                    best_dev_test_res = res_test\n\t            if best_test_res is None:\n\t                best_test_res = res_test\n\t                save_flag = True\n\t            else:\n\t                if best_test_res['oe_f'] < res_test['oe_f']:\n\t                    best_test_res = res_test\n\t                    save_flag = True\n\t            if args.is_check == 1 and save_flag:\n\t                current_checkpoint_path = os.path.join(checkpoint_path,\n", "                                                       args.check_info)\n\t                model.seq2seq_model.save_pretrained(current_checkpoint_path)\n\t                print('save model!!!!!!!!!!!')\n\t        epoch += 1\n\t    logger.info(\"Training complete in: \" + str(datetime.now() - start),\n\t                pad=True)\n\t    logger.info('---------------------------')\n\t    logger.info('BEST DEV:-----')\n\t    logger.info('BEST DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t        best_dev_res['oe_pre'], best_dev_res['oe_rec'], best_dev_res['oe_f']))\n", "    logger.info('BEST DEV TEST:-----')\n\t    logger.info('BEST DEV--TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t        best_dev_test_res['oe_pre'], best_dev_test_res['oe_rec'],\n\t        best_dev_test_res['oe_f']))\n\t    logger.info('BEST TEST:-----')\n\t    logger.info('BEST TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t        best_test_res['oe_pre'], best_test_res['oe_rec'],\n\t        best_test_res['oe_f']))\n\t    # if not args.cpu:\n\t    #     cleanup_process()\n", "def parse_args():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--dataset',\n\t                        action='append',\n\t                        nargs=2,\n\t                        metavar=('DATASET_NAME', 'DATASET_PATH'),\n\t                        required=True,\n\t                        help='')\n\t    # required\n\t    parser.add_argument('--checkpoint_dir',\n", "                        required=True,\n\t                        type=str,\n\t                        help='where to save the checkpoint')\n\t    parser.add_argument('--bart_model',\n\t                        default='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n\t                        type=str,\n\t                        help='bart pretrain model')\n\t    # path\n\t    parser.add_argument(\n\t        '--log_dir',\n", "        default=None,\n\t        type=str,\n\t        help='path to output log files, not output to file if not specified')\n\t    parser.add_argument('--model_config',\n\t                        default=None,\n\t                        type=str,\n\t                        help='path to load model config')\n\t    parser.add_argument('--text_only',action='store_true', default=False, help='text_only')\n\t    parser.add_argument('--checkpoint',\n\t                        default=None,\n", "                        type=str,\n\t                        help='name or path to load weights')\n\t    parser.add_argument('--lr_decay_every',\n\t                        default=4,\n\t                        type=int,\n\t                        help='lr_decay_every')\n\t    parser.add_argument('--lr_decay_ratio',\n\t                        default=0.8,\n\t                        type=float,\n\t                        help='lr_decay_ratio')\n", "    # training and evaluation\n\t    parser.add_argument('--epochs',\n\t                        default=40,\n\t                        type=int,\n\t                        help='number of training epoch')\n\t    parser.add_argument('--eval_every', default=3, type=int, help='eval_every')\n\t    parser.add_argument('--lr', default=1e-2, type=float, help='learning rate')\n\t    parser.add_argument('--num_beams',\n\t                        default=4,\n\t                        type=int,\n", "                        help='level of beam search on validation')\n\t    parser.add_argument(\n\t        '--continue_training',\n\t        action='store_true',\n\t        help='continue training, load optimizer and epoch from checkpoint')\n\t    parser.add_argument('--warmup', default=0.1, type=float, help='warmup')\n\t    # dropout\n\t    parser.add_argument(\n\t        '--dropout',\n\t        default=None,\n", "        type=float,\n\t        help=\n\t        'dropout rate for the transformer. This overwrites the model config')\n\t    parser.add_argument(\n\t        '--classif_dropout',\n\t        default=None,\n\t        type=float,\n\t        help=\n\t        'dropout rate for the classification layers. This overwrites the model config'\n\t    )\n", "    parser.add_argument(\n\t        '--attention_dropout',\n\t        default=None,\n\t        type=float,\n\t        help=\n\t        'dropout rate for the attention layers. This overwrites the model config'\n\t    )\n\t    parser.add_argument(\n\t        '--activation_dropout',\n\t        default=None,\n", "        type=float,\n\t        help=\n\t        'dropout rate for the activation layers. This overwrites the model config'\n\t    )\n\t    # hardware and performance\n\t    parser.add_argument('--grad_clip', default=5, type=float, help='grad_clip')\n\t    parser.add_argument('--gpu_num',\n\t                        default=1,\n\t                        type=int,\n\t                        help='number of GPUs in total')\n", "    parser.add_argument('--cpu',\n\t                        action='store_true',\n\t                        help='if only use cpu to run the model')\n\t    parser.add_argument('--amp',\n\t                        action='store_true',\n\t                        help='whether or not to use amp')\n\t    parser.add_argument('--master_port',\n\t                        type=str,\n\t                        default='12355',\n\t                        help='master port for DDP')\n", "    parser.add_argument('--batch_size',\n\t                        type=int,\n\t                        default=64,\n\t                        help='training batch size')\n\t    parser.add_argument('--seed', type=int, default=42, help='seed')\n\t    parser.add_argument('--num_workers',\n\t                        type=int,\n\t                        default=0,\n\t                        help='#workers for data loader')\n\t    parser.add_argument('--max_len', type=int, default=10, help='max_len')\n", "    parser.add_argument('--max_len_a',\n\t                        type=float,\n\t                        default=0.6,\n\t                        help='max_len_a')\n\t    parser.add_argument('--ANP_loss_type',\n\t                        type=str,\n\t                        default='KL',\n\t                        help='ANP_loss_type')\n\t    parser.add_argument('--bart_init', type=int, default=1, help='bart_init')\n\t    parser.add_argument('--sample_num',\n", "                        type=int,\n\t                        default=500,\n\t                        help='sample_num')\n\t    parser.add_argument('--is_sample', type=int, default=1, help='is_sample')\n\t    parser.add_argument('--start_idx', type=int, default=0, help='start_idx')\n\t    parser.add_argument('--check_info', type=str, default='', help='start_idx')\n\t    parser.add_argument('--is_check', type=int, default=0, help='start_idx')\n\t    parser.add_argument('--task', type=str, default='twitter_ae', help='task')\n\t    parser.add_argument('--has_prompt',  action='store_true', default=False, help='whether has prompt')\n\t    parser.add_argument('--use_generated_prompt', action='store_true', default=False, help='whether use the generated prompt')\n", "    parser.add_argument('--use_different_senti_prompt', action='store_true', default=False, help='whether use different prompt for different sentiemnt in an instance')\n\t    parser.add_argument('--use_different_aspect_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n\t    parser.add_argument('--num_image_tokens', type=int, default=2, help='the length of image_tokens')\n\t    parser.add_argument('--use_multitasks', action='store_true', default=False, help='whether use multitasks')\n\t    parser.add_argument('--loss_lambda', default=0.1, type=float, help='the weight of aspect_num classification loss')\n\t    parser.add_argument('--use_caption', action='store_true', default=False, help='whether use image caption')\n\t    args = parser.parse_args()\n\t    if args.gpu_num != 1 and args.cpu:\n\t        raise ValueError('--gpu_num are not allowed if --cpu is set to true')\n\t    if args.checkpoint is None and args.model_config is None:\n", "        raise ValueError(\n\t            '--model_config and --checkpoint cannot be empty at the same time')\n\t    return args\n\tif __name__ == '__main__':\n\t    args = parse_args()\n\t    # mp.spawn(main, args=(args, ), nprocs=args.gpu_num, join=True)\n\t    torch.manual_seed(args.seed)\n\t    torch.cuda.manual_seed_all(args.seed)\n\t    torch.cuda.manual_seed(args.seed)\n\t    np.random.seed(args.seed)\n", "    random.seed(args.seed)\n\t    cudnn.deterministic = True\n\t    main(0, args)"]}
{"filename": "twitter_sc_training_for_generated_prompt.py", "chunked_list": ["import argparse\n\timport json\n\timport os\n\tfrom datetime import datetime\n\tfrom torch import optim\n\timport torch\n\timport torch.multiprocessing as mp\n\tfrom torch.cuda.amp import GradScaler\n\tfrom torch.nn.parallel import DistributedDataParallel as DDP\n\tfrom torch.utils.data import DataLoader, ConcatDataset\n", "from torch.utils.data.distributed import DistributedSampler\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom transformers import AdamW\n\timport random\n\tfrom src.data.collation_for_prompt import Collator\n\tfrom src.data.dataset_for_prompt import MVSA_Dataset, Twitter_Dataset\n\tfrom src.data.tokenization_new_for_generated_prompt import ConditionTokenizer\n\tfrom src.model.config import MultiModalBartConfig\n\tfrom src.model.MAESC_model_for_generated_senti_prompt import MultiModalBartModel_AESC\n\tfrom src.model.model import MultiModalBartModelForPretrain\n", "from src.training import fine_tune\n\tfrom src.utils import Logger, save_training_data, load_training_data, setup_process, cleanup_process\n\tfrom src.model.metrics import AESCSpanMetric, OESpanMetric\n\tfrom src.model.generater_for_generated_prompt import SequenceGeneratorModel\n\timport src.eval_utils as eval_utils\n\timport numpy as np\n\timport torch.backends.cudnn as cudnn\n\tdef get_parameter_number(model):\n\t    total_num = sum(p.numel() for p in model.parameters())\n\t    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "    return {'Total': total_num, 'Trainable': trainable_num}\n\tdef main(rank, args):\n\t    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n\t    checkpoint_path = os.path.join(args.checkpoint_dir, timestamp)\n\t    tb_writer = None\n\t    add_name = 'epoch_num' + str(args.epochs)\n\t    add_name += 'last'\n\t    if args.is_sample:\n\t        add_name += 'sample_num' + str(args.sample_num)\n\t        add_name += 'start_idx' + str(args.start_idx)\n", "    if args.text_only:\n\t        add_name += ' only text'\n\t    else:\n\t        add_name += ' multi'\n\t    if args.bart_init == 0:\n\t        add_name += '_random_init_'\n\t    if args.checkpoint:\n\t        add_name = add_name + '-pretrain' + args.checkpoint.split('/')[-2]\n\t    add_name = add_name + str(args.lr)\n\t    log_dir = os.path.join(args.log_dir, timestamp + add_name)\n", "    # make log dir and tensorboard writer if log_dir is specified\n\t    if rank == 0 and args.log_dir is not None:\n\t        os.makedirs(log_dir)\n\t        tb_writer = SummaryWriter(log_dir=log_dir)\n\t    logger = Logger(log_dir=os.path.join(log_dir, 'log.txt'),\n\t                    enabled=(rank == 0))\n\t    # make checkpoint dir if not exist\n\t    if args.is_check == 1 and not os.path.isdir(checkpoint_path):\n\t        os.makedirs(checkpoint_path)\n\t        logger.info('Made checkpoint directory: \"{}\"'.format(checkpoint_path))\n", "    logger.info('Initialed with {} GPU(s)'.format(args.gpu_num), pad=True)\n\t    for k, v in vars(args).items():\n\t        logger.info('{}: {}'.format(k, v))\n\t    # =========================== model =============================\n\t    logger.info('Loading model...')\n\t    if args.cpu:\n\t        device = 'cpu'\n\t        map_location = device\n\t    else:\n\t        device = torch.device(\"cuda:{}\".format(rank))\n", "        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n\t    tokenizer = ConditionTokenizer(args=args)\n\t    label_ids = list(tokenizer.mapping2id.values())\n\t    senti_ids = list(tokenizer.senti2id.values())\n\t    # print(label_ids)\n\t    # print(tokenizer.convert_ids_to_tokens(label_ids))\n\t    if args.model_config is not None:\n\t        bart_config = MultiModalBartConfig.from_dict(\n\t            json.load(open(args.model_config)))\n\t    else:\n", "        bart_config = MultiModalBartConfig.from_pretrained(args.checkpoint)\n\t    if args.dropout is not None:\n\t        bart_config.dropout = args.dropout\n\t    if args.attention_dropout is not None:\n\t        bart_config.attention_dropout = args.attention_dropout\n\t    if args.classif_dropout is not None:\n\t        bart_config.classif_dropout = args.classif_dropout\n\t    if args.activation_dropout is not None:\n\t        bart_config.activation_dropout = args.activation_dropout\n\t    bos_token_id = 0  # 因为是特殊符号\n", "    eos_token_id = 1\n\t    if args.checkpoint:\n\t        pretrain_model = MultiModalBartModelForPretrain.from_pretrained(\n\t            args.checkpoint,\n\t            config=bart_config,\n\t            bart_model=args.bart_model,\n\t            tokenizer=tokenizer,\n\t            label_ids=label_ids,\n\t            senti_ids=senti_ids,\n\t            args=args,\n", "            error_on_mismatch=False)\n\t        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n\t                                                 args.bart_model, tokenizer,\n\t                                                 label_ids)\n\t        seq2seq_model.encoder.load_state_dict(\n\t            pretrain_model.encoder.state_dict())\n\t        seq2seq_model.decoder.load_state_dict(\n\t            pretrain_model.span_decoder.state_dict())\n\t        model = SequenceGeneratorModel(seq2seq_model,\n\t                                       bos_token_id=bos_token_id,\n", "                                       eos_token_id=eos_token_id,\n\t                                       max_length=args.max_len,\n\t                                       max_len_a=args.max_len_a,\n\t                                       num_beams=args.num_beams,\n\t                                       do_sample=False,\n\t                                       sc_only=True,\n\t                                       repetition_penalty=1,\n\t                                       length_penalty=1.0,\n\t                                       pad_token_id=eos_token_id,\n\t                                       restricter=None)\n", "    else:\n\t        print('++++++++++++++++++ No Pretrain ++++++++++++++++++++++++++++++++')\n\t        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n\t                                                 args.bart_model, tokenizer,\n\t                                                 label_ids)\n\t        model = SequenceGeneratorModel(seq2seq_model,\n\t                                       bos_token_id=bos_token_id,\n\t                                       eos_token_id=eos_token_id,\n\t                                       max_length=args.max_len,\n\t                                       max_len_a=args.max_len_a,\n", "                                       num_beams=args.num_beams,\n\t                                       do_sample=False,\n\t                                       sc_only=True,\n\t                                       repetition_penalty=1,\n\t                                       length_penalty=1.0,\n\t                                       pad_token_id=eos_token_id,\n\t                                       restricter=None)\n\t        # model = MultiModalBartModel_AESC(bart_config, args.bart_model,\n\t        #                                  tokenizer, label_ids)\n\t    model.to(device)\n", "    optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\t    scaler = GradScaler() if args.amp else None\n\t    epoch = 0\n\t    logger.info('Loading data...')\n\t    collate_twitter_sc = Collator(args.task,\n\t                                  tokenizer,\n\t                                  mlm_enabled=False,\n\t                                  senti_enabled=False,\n\t                                  ae_enabled=False,\n\t                                  oe_enabled=False,\n", "                                  aesc_enabled=False,\n\t                                  anp_enabled=False,\n\t                                  twitter_sc_enabled=True,\n\t                                  max_img_num=args.num_image_tokens,\n\t                                  has_prompt=args.has_prompt,\n\t                                  text_only=args.text_only,\n\t                                  use_caption=args.use_caption)\n\t    train_dataset = Twitter_Dataset(args.dataset[0][1], split='train')\n\t    dev_dataset = Twitter_Dataset(args.dataset[0][1], split='dev')\n\t    test_dataset = Twitter_Dataset(args.dataset[0][1], split='test')\n", "    train_loader = DataLoader(dataset=train_dataset,\n\t                              batch_size=args.batch_size,\n\t                              shuffle=True,\n\t                              num_workers=args.num_workers,\n\t                              pin_memory=True,\n\t                              collate_fn=collate_twitter_sc)\n\t    dev_loader = DataLoader(dataset=dev_dataset,\n\t                            batch_size=args.batch_size,\n\t                            shuffle=False,\n\t                            num_workers=args.num_workers,\n", "                            pin_memory=True,\n\t                            collate_fn=collate_twitter_sc)\n\t    test_loader = DataLoader(dataset=test_dataset,\n\t                             batch_size=args.batch_size,\n\t                             shuffle=False,\n\t                             num_workers=args.num_workers,\n\t                             pin_memory=True,\n\t                             collate_fn=collate_twitter_sc)\n\t    callback = None\n\t    metric = AESCSpanMetric(eos_token_id,\n", "                            num_labels=len(label_ids),\n\t                            conflict_id=-1)\n\t    model.train()\n\t    start = datetime.now()\n\t    best_dev_res = None\n\t    best_dev_test_res = None\n\t    best_test_res = None\n\t    # res_dev = eval_utils.eval(args, model, dev_loader, metric, device)\n\t    # for name, param in model.named_parameters():\n\t    #     print(name, param.shape)\n", "    while epoch < args.epochs:\n\t        logger.info('Epoch {}'.format(epoch + 1), pad=True)\n\t        fine_tune(epoch=epoch,\n\t                  model=model,\n\t                  train_loader=train_loader,\n\t                  test_loader=test_loader,\n\t                  metric=metric,\n\t                  optimizer=optimizer,\n\t                  args=args,\n\t                  device=device,\n", "                  logger=logger,\n\t                  callback=callback,\n\t                  log_interval=1,\n\t                  tb_writer=tb_writer,\n\t                  tb_interval=1,\n\t                  scaler=scaler)\n\t        print('test!!!!!!!!!!!!!!')\n\t        if (epoch + 1) % args.eval_every == 0:\n\t            # train_dev = eval_utils.eval(model, train_loader, metric, device)\n\t            res_dev = eval_utils.eval(args, model, dev_loader, metric, device)\n", "            res_test = eval_utils.eval(args, model, test_loader, metric,\n\t                                       device)\n\t            # print('sc_all_num', res_test['sc_all_num'])\n\t            logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t                res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n\t            logger.info('DEV  sc_p:{} sc_r:{} sc_f:{}'.format(\n\t                res_dev['sc_pre'], res_dev['sc_rec'], res_dev['sc_f']))\n\t            logger.info('DEV  sc_acc:{}'.format(res_dev['sc_acc']))\n\t            logger.info('TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t                res_test['ae_pre'], res_test['ae_rec'], res_test['ae_f']))\n", "            logger.info('TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n\t                res_test['sc_pre'], res_test['sc_rec'], res_test['sc_f']))\n\t            logger.info('TEST  sc_acc:{}'.format(res_test['sc_acc']))\n\t            # logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t            #     res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n\t            save_flag = False\n\t            if best_dev_res is None:\n\t                best_dev_res = res_dev\n\t                best_dev_test_res = res_test\n\t            else:\n", "                if best_dev_res['sc_acc'] < res_dev['sc_acc']:\n\t                    best_dev_res = res_dev\n\t                    best_dev_test_res = res_test\n\t            if best_test_res is None:\n\t                best_test_res = res_test\n\t                save_flag = True\n\t            else:\n\t                if best_test_res['sc_acc'] < res_test['sc_acc']:\n\t                    best_test_res = res_test\n\t                    save_flag = True\n", "            if args.is_check == 1 and save_flag:\n\t                current_checkpoint_path = os.path.join(checkpoint_path,\n\t                                                       args.check_info)\n\t                model.seq2seq_model.save_pretrained(current_checkpoint_path)\n\t                print('save model!!!!!!!!!!!')\n\t        epoch += 1\n\t    logger.info(\"Training complete in: \" + str(datetime.now() - start),\n\t                pad=True)\n\t    logger.info('---------------------------')\n\t    logger.info('BEST DEV:-----')\n", "    logger.info('BEST DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t        best_dev_res['ae_pre'], best_dev_res['ae_rec'], best_dev_res['ae_f']))\n\t    logger.info('BEST DEV  sc_p:{} sc_r:{} sc_f:{}'.format(\n\t        best_dev_res['sc_pre'], best_dev_res['sc_rec'], best_dev_res['sc_f']))\n\t    logger.info('BEST DEV  sc_acc:{}'.format(best_dev_res['sc_acc']))\n\t    logger.info('BEST DEV TEST:-----')\n\t    logger.info('BEST DEV--TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t        best_dev_test_res['ae_pre'], best_dev_test_res['ae_rec'],\n\t        best_dev_test_res['ae_f']))\n\t    logger.info('BEST DEV--TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n", "        best_dev_test_res['sc_pre'], best_dev_test_res['sc_rec'],\n\t        best_dev_test_res['sc_f']))\n\t    logger.info('BEST DEV--TEST  sc_acc:{}'.format(\n\t        best_dev_test_res['sc_acc']))\n\t    logger.info('BEST TEST:-----')\n\t    logger.info('BEST TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n\t        best_test_res['ae_pre'], best_test_res['ae_rec'],\n\t        best_test_res['ae_f']))\n\t    logger.info('BEST TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n\t        best_test_res['sc_pre'], best_test_res['sc_rec'],\n", "        best_test_res['sc_f']))\n\t    logger.info('BEST TEST  sc_acc:{}'.format(best_test_res['sc_acc']))\n\t    if not args.cpu:\n\t        cleanup_process()\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--dataset',\n\t                        action='append',\n\t                        nargs=2,\n\t                        metavar=('DATASET_NAME', 'DATASET_PATH'),\n", "                        required=True,\n\t                        help='')\n\t    # required\n\t    parser.add_argument('--checkpoint_dir',\n\t                        required=True,\n\t                        type=str,\n\t                        help='where to save the checkpoint')\n\t    parser.add_argument('--bart_model',\n\t                        default='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n\t                        type=str,\n", "                        help='bart pretrain model')\n\t    # path\n\t    parser.add_argument(\n\t        '--log_dir',\n\t        default=None,\n\t        type=str,\n\t        help='path to output log files, not output to file if not specified')\n\t    parser.add_argument('--model_config',\n\t                        default=None,\n\t                        type=str,\n", "                        help='path to load model config')\n\t    # parser.add_argument('--text_only', default=0, type=int, help='text_only')\n\t    parser.add_argument('--checkpoint',\n\t                        default=None,\n\t                        type=str,\n\t                        help='name or path to load weights')\n\t    parser.add_argument('--lr_decay_every',\n\t                        default=4,\n\t                        type=int,\n\t                        help='lr_decay_every')\n", "    parser.add_argument('--lr_decay_ratio',\n\t                        default=0.8,\n\t                        type=float,\n\t                        help='lr_decay_ratio')\n\t    # training and evaluation\n\t    parser.add_argument('--epochs',\n\t                        default=40,\n\t                        type=int,\n\t                        help='number of training epoch')\n\t    parser.add_argument('--eval_every', default=3, type=int, help='eval_every')\n", "    parser.add_argument('--lr', default=1e-2, type=float, help='learning rate')\n\t    parser.add_argument('--num_beams',\n\t                        default=4,\n\t                        type=int,\n\t                        help='level of beam search on validation')\n\t    parser.add_argument(\n\t        '--continue_training',\n\t        action='store_true',\n\t        help='continue training, load optimizer and epoch from checkpoint')\n\t    parser.add_argument('--warmup', default=0.1, type=float, help='warmup')\n", "    # dropout\n\t    parser.add_argument(\n\t        '--dropout',\n\t        default=None,\n\t        type=float,\n\t        help=\n\t        'dropout rate for the transformer. This overwrites the model config')\n\t    parser.add_argument(\n\t        '--classif_dropout',\n\t        default=None,\n", "        type=float,\n\t        help=\n\t        'dropout rate for the classification layers. This overwrites the model config'\n\t    )\n\t    parser.add_argument(\n\t        '--attention_dropout',\n\t        default=None,\n\t        type=float,\n\t        help=\n\t        'dropout rate for the attention layers. This overwrites the model config'\n", "    )\n\t    parser.add_argument(\n\t        '--activation_dropout',\n\t        default=None,\n\t        type=float,\n\t        help=\n\t        'dropout rate for the activation layers. This overwrites the model config'\n\t    )\n\t    # hardware and performance\n\t    parser.add_argument('--grad_clip', default=5, type=float, help='grad_clip')\n", "    parser.add_argument('--gpu_num',\n\t                        default=1,\n\t                        type=int,\n\t                        help='number of GPUs in total')\n\t    parser.add_argument('--cpu',\n\t                        action='store_true',\n\t                        help='if only use cpu to run the model')\n\t    parser.add_argument('--amp',\n\t                        action='store_true',\n\t                        help='whether or not to use amp')\n", "    parser.add_argument('--master_port',\n\t                        type=str,\n\t                        default='12355',\n\t                        help='master port for DDP')\n\t    parser.add_argument('--batch_size',\n\t                        type=int,\n\t                        default=64,\n\t                        help='training batch size')\n\t    parser.add_argument('--seed', type=int, default=42, help='seed')\n\t    parser.add_argument('--num_workers',\n", "                        type=int,\n\t                        default=0,\n\t                        help='#workers for data loader')\n\t    parser.add_argument('--max_len', type=int, default=10, help='max_len')\n\t    parser.add_argument('--max_len_a',\n\t                        type=float,\n\t                        default=0.6,\n\t                        help='max_len_a')\n\t    parser.add_argument('--ANP_loss_type',\n\t                        type=str,\n", "                        default='KL',\n\t                        help='ANP_loss_type')\n\t    parser.add_argument('--bart_init', type=int, default=1, help='bart_init')\n\t    parser.add_argument('--sample_num',\n\t                        type=int,\n\t                        default=500,\n\t                        help='sample_num')\n\t    parser.add_argument('--is_sample', type=int, default=1, help='is_sample')\n\t    parser.add_argument('--start_idx', type=int, default=0, help='start_idx')\n\t    parser.add_argument('--check_info', type=str, default='', help='start_idx')\n", "    parser.add_argument('--is_check', type=int, default=0, help='start_idx')\n\t    parser.add_argument('--task', type=str, default='twitter_sc', help='task')\n\t    parser.add_argument('--num_image_tokens', type=int, default=2, help='the length of image_tokens')\n\t    parser.add_argument('--has_prompt',  action='store_true', default=False, help='whether has prompt')\n\t    parser.add_argument('--use_generated_prompt',  action='store_true', default=False, help='whether use the generated prompt')\n\t    parser.add_argument('--use_different_senti_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n\t    parser.add_argument('--use_different_aspect_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n\t    parser.add_argument('--text_only', action='store_true', default=False, help='whether only use text')\n\t    parser.add_argument('--use_caption', action='store_true', default=False, help='whether use image caption')\n\t    args = parser.parse_args()\n", "    if args.gpu_num != 1 and args.cpu:\n\t        raise ValueError('--gpu_num are not allowed if --cpu is set to true')\n\t    if args.checkpoint is None and args.model_config is None:\n\t        raise ValueError(\n\t            '--model_config and --checkpoint cannot be empty at the same time')\n\t    return args\n\tif __name__ == '__main__':\n\t    args = parse_args()\n\t    # mp.spawn(main, args=(args, ), nprocs=args.gpu_num, join=True)\n\t    torch.manual_seed(args.seed)\n", "    torch.cuda.manual_seed_all(args.seed)\n\t    torch.cuda.manual_seed(args.seed)\n\t    np.random.seed(args.seed)\n\t    random.seed(args.seed)\n\t    cudnn.deterministic = True\n\t    main(0, args)"]}
{"filename": "MAESC_training_for_generated_dual_prompts_multitasks_Aspect.py", "chunked_list": ["import argparse\n\timport json\n\timport os\n\tfrom datetime import datetime\n\tfrom torch import optim\n\timport torch\n\timport torch.multiprocessing as mp\n\tfrom torch.cuda.amp import GradScaler\n\tfrom torch.nn.parallel import DistributedDataParallel as DDP\n\tfrom torch.utils.data import DataLoader, ConcatDataset\n", "from torch.utils.data.distributed import DistributedSampler\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom transformers import AdamW\n\timport random\n\tfrom src.data.collation_for_prompt_multitasks import Collator\n\tfrom src.data.dataset_for_prompt import MVSA_Dataset, Twitter_Dataset\n\tfrom src.data.tokenization_new_for_generated_prompt_multitasks import ConditionTokenizer\n\tfrom src.model.config import MultiModalBartConfig\n\tfrom src.model.MAESC_model_for_generated_dual_prompts_multitasks_Aspect import MultiModalBartModel_AESC\n\tfrom src.model.model_for_prompt import MultiModalBartModelForPretrain\n", "from src.training_multitasks import fine_tune\n\tfrom src.utils import Logger, save_training_data, load_training_data, setup_process, cleanup_process\n\tfrom src.model.metrics import AESCSpanMetric\n\tfrom src.model.generater_for_generated_prompt_multitasks import SequenceGeneratorModel\n\timport src.eval_utils_multitasks as eval_utils\n\timport numpy as np\n\timport torch.backends.cudnn as cudnn\n\t# from thop import profile\n\tdef get_parameter_number(model):\n\t    total_num = sum(p.numel() for p in model.parameters())\n", "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\t    return {'Total': total_num, 'Trainable': trainable_num}\n\tdef main(rank, args):\n\t    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n\t    checkpoint_path = os.path.join(args.checkpoint_dir, timestamp)\n\t    tb_writer = None\n\t    add_name = ''\n\t    log_dir = os.path.join(args.log_dir, timestamp + add_name)\n\t    # make log dir and tensorboard writer if log_dir is specified\n\t    if rank == 0 and args.log_dir is not None:\n", "        os.makedirs(log_dir)\n\t        tb_writer = SummaryWriter(log_dir=log_dir)\n\t    logger = Logger(log_dir=os.path.join(log_dir, 'log.txt'),\n\t                    enabled=(rank == 0))\n\t    # make checkpoint dir if not exist\n\t    if args.is_check == 1 and not os.path.isdir(checkpoint_path):\n\t        os.makedirs(checkpoint_path)\n\t        logger.info('Made checkpoint directory: \"{}\"'.format(checkpoint_path))\n\t    logger.info('Initialed with {} GPU(s)'.format(args.gpu_num), pad=True)\n\t    for k, v in vars(args).items():\n", "        logger.info('{}: {}'.format(k, v))\n\t    # =========================== model =============================\n\t    logger.info('Loading model...')\n\t    if args.cpu:\n\t        device = 'cpu'\n\t        map_location = device\n\t    else:\n\t        device = torch.device(\"cuda:{}\".format(rank))\n\t        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n\t    tokenizer = ConditionTokenizer(args=args)\n", "    label_ids = list(tokenizer.mapping2id.values())\n\t    senti_ids = list(tokenizer.senti2id.values())\n\t    if args.model_config is not None:\n\t        bart_config = MultiModalBartConfig.from_dict(\n\t            json.load(open(args.model_config)))\n\t    else:\n\t        bart_config = MultiModalBartConfig.from_pretrained(args.checkpoint)\n\t    if args.dropout is not None:\n\t        bart_config.dropout = args.dropout\n\t    if args.attention_dropout is not None:\n", "        bart_config.attention_dropout = args.attention_dropout\n\t    if args.classif_dropout is not None:\n\t        bart_config.classif_dropout = args.classif_dropout\n\t    if args.activation_dropout is not None:\n\t        bart_config.activation_dropout = args.activation_dropout\n\t    bos_token_id = 0  # 因为是特殊符号\n\t    eos_token_id = 1\n\t    # import ipdb; ipdb.set_trace()\n\t    if args.checkpoint:\n\t        pretrain_model = MultiModalBartModelForPretrain.from_pretrained(\n", "            args.checkpoint,\n\t            config=bart_config,\n\t            bart_model=args.bart_model,\n\t            tokenizer=tokenizer,\n\t            label_ids=label_ids,\n\t            senti_ids=senti_ids,\n\t            args=args,\n\t            error_on_mismatch=False)\n\t        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n\t                                                 args.bart_model, tokenizer,\n", "                                                 label_ids)\n\t        seq2seq_model.encoder.load_state_dict(\n\t            pretrain_model.encoder.state_dict())\n\t        seq2seq_model.decoder.load_state_dict(\n\t            pretrain_model.span_decoder.state_dict())\n\t        model = SequenceGeneratorModel(seq2seq_model,\n\t                                       bos_token_id=bos_token_id,\n\t                                       eos_token_id=eos_token_id,\n\t                                       max_length=args.max_len,\n\t                                       max_len_a=args.max_len_a,\n", "                                       num_beams=args.num_beams,\n\t                                       do_sample=False,\n\t                                       repetition_penalty=1,\n\t                                       length_penalty=1.0,\n\t                                       pad_token_id=eos_token_id,\n\t                                       restricter=None)\n\t    else:\n\t        print('++++++++++++++++++ No Pretrain ++++++++++++++++++++++++++++++++')\n\t        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n\t                                                 args.bart_model, tokenizer,\n", "                                                 label_ids)\n\t        model = SequenceGeneratorModel(seq2seq_model,\n\t                                       bos_token_id=bos_token_id,\n\t                                       eos_token_id=eos_token_id,\n\t                                       max_length=args.max_len,\n\t                                       max_len_a=args.max_len_a,\n\t                                       num_beams=args.num_beams,\n\t                                       do_sample=False,\n\t                                       repetition_penalty=1,\n\t                                       length_penalty=1.0,\n", "                                       pad_token_id=eos_token_id,\n\t                                       restricter=None)\n\t        # model = MultiModalBartModel_AESC(bart_config, args.bart_model,\n\t        #                                  tokenizer, label_ids)\n\t    model.to(device)\n\t    parameters = get_parameter_number(model) ##{'Total': 169351685, 'Trainable': 169351685}\n\t    print(parameters)\n\t    optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\t    scaler = GradScaler() if args.amp else None\n\t    epoch = 0\n", "    logger.info('Loading data...')\n\t    collate_aesc = Collator(\n\t                            args.task,\n\t                            tokenizer,\n\t                            mlm_enabled=False,\n\t                            senti_enabled=False,\n\t                            ae_enabled=False,\n\t                            oe_enabled=False,\n\t                            aesc_enabled=True,\n\t                            anp_enabled=False,\n", "                            max_img_num=args.num_image_tokens,\n\t                            has_prompt=args.has_prompt,\n\t                            text_only=args.text_only)\n\t    train_dataset = Twitter_Dataset(args.dataset[0][1], split='train')\n\t    dev_dataset = Twitter_Dataset(args.dataset[0][1], split='dev')\n\t    test_dataset = Twitter_Dataset(args.dataset[0][1], split='test')\n\t    train_loader = DataLoader(dataset=train_dataset,\n\t                              batch_size=args.batch_size,\n\t                              shuffle=True,\n\t                              num_workers=args.num_workers,\n", "                              pin_memory=True,\n\t                              collate_fn=collate_aesc)\n\t    dev_loader = DataLoader(dataset=dev_dataset,\n\t                            batch_size=args.batch_size,\n\t                            shuffle=False,\n\t                            num_workers=args.num_workers,\n\t                            pin_memory=True,\n\t                            collate_fn=collate_aesc)\n\t    test_loader = DataLoader(dataset=test_dataset,\n\t                             batch_size=args.batch_size,\n", "                             shuffle=False,\n\t                             num_workers=args.num_workers,\n\t                             pin_memory=True,\n\t                             collate_fn=collate_aesc)\n\t    callback = None\n\t    metric = AESCSpanMetric(eos_token_id,\n\t                            num_labels=len(label_ids),\n\t                            conflict_id=-1)\n\t    model.train()\n\t    start = datetime.now()\n", "    best_dev_res = None\n\t    best_dev_test_res = None\n\t    best_test_res = None\n\t    # res_dev = eval_utils.eval(model, dev_loader, metric, device)\n\t    while epoch < args.epochs:\n\t        logger.info('Epoch {}'.format(epoch + 1), pad=True)\n\t        fine_tune(epoch=epoch,\n\t                  model=model,\n\t                  train_loader=train_loader,\n\t                  test_loader=test_loader,\n", "                  metric=metric,\n\t                  optimizer=optimizer,\n\t                  args=args,\n\t                  device=device,\n\t                  logger=logger,\n\t                  callback=callback,\n\t                  log_interval=1,\n\t                  tb_writer=tb_writer,\n\t                  tb_interval=1,\n\t                  scaler=scaler)\n", "        print('test!!!!!!!!!!!!!!')\n\t        if (epoch + 1) % args.eval_every == 0:\n\t            # train_dev = eval_utils.eval(model, train_loader, metric, device)\n\t            res_dev, dev_aspects_num_acc = eval_utils.eval(args, model, dev_loader, metric, device)\n\t            res_test, test_aspects_num_acc = eval_utils.eval(args, model, test_loader, metric,\n\t                                       device)\n\t            logger.info('DEV  aesc_p:{} aesc_r:{} aesc_f:{}, dev_aspects_num_acc: {:.4f}'.format(\n\t                res_dev['aesc_pre'], res_dev['aesc_rec'], res_dev['aesc_f'], dev_aspects_num_acc))\n\t            logger.info('TEST  aesc_p:{} aesc_r:{} aesc_f:{}, test_aspects_num_acc: {:.4f}'.format(\n\t                res_test['aesc_pre'], res_test['aesc_rec'],\n", "                res_test['aesc_f'], test_aspects_num_acc))\n\t            save_flag = False\n\t            if best_dev_res is None:\n\t                best_dev_res = res_dev\n\t                best_dev_test_res = res_test\n\t            else:\n\t                if best_dev_res['aesc_f'] < res_dev['aesc_f']:\n\t                    best_dev_res = res_dev\n\t                    best_dev_test_res = res_test\n\t            if best_test_res is None:\n", "                best_test_res = res_test\n\t                save_flag = True\n\t            else:\n\t                if best_test_res['aesc_f'] < res_test['aesc_f']:\n\t                    best_test_res = res_test\n\t                    save_flag = True\n\t            if args.is_check == 1 and save_flag:\n\t                current_checkpoint_path = os.path.join(checkpoint_path,\n\t                                                       args.check_info)\n\t                model.seq2seq_model.save_pretrained(current_checkpoint_path)\n", "                print('save model!!!!!!!!!!!')\n\t        epoch += 1\n\t    logger.info(\"Training complete in: \" + str(datetime.now() - start),\n\t                pad=True)\n\t    logger.info('---------------------------')\n\t    logger.info('BEST DEV:-----')\n\t    logger.info('BEST DEV  aesc_p:{} aesc_r:{} aesc_f:{}'.format(\n\t        best_dev_res['aesc_pre'], best_dev_res['aesc_rec'],\n\t        best_dev_res['aesc_f']))\n\t    logger.info('BEST DEV TEST:-----')\n", "    logger.info('BEST DEV--TEST  aesc_p:{} aesc_r:{} aesc_f:{}'.format(\n\t        best_dev_test_res['aesc_pre'], best_dev_test_res['aesc_rec'],\n\t        best_dev_test_res['aesc_f']))\n\t    logger.info('BEST TEST:-----')\n\t    logger.info('BEST TEST  aesc_p:{} aesc_r:{} aesc_f:{}'.format(\n\t        best_test_res['aesc_pre'], best_test_res['aesc_rec'],\n\t        best_test_res['aesc_f']))\n\t    # if not args.cpu:\n\t    #     cleanup_process()\n\tdef parse_args():\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--dataset',\n\t                        action='append',\n\t                        nargs=2,\n\t                        metavar=('DATASET_NAME', 'DATASET_PATH'),\n\t                        required=True,\n\t                        help='')\n\t    # required\n\t    parser.add_argument('--checkpoint_dir',\n\t                        required=True,\n", "                        type=str,\n\t                        help='where to save the checkpoint')\n\t    parser.add_argument('--bart_model',\n\t                        default='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n\t                        type=str,\n\t                        help='bart pretrain model')\n\t    # path\n\t    parser.add_argument(\n\t        '--log_dir',\n\t        default=None,\n", "        type=str,\n\t        help='path to output log files, not output to file if not specified')\n\t    parser.add_argument('--model_config',\n\t                        default=None,\n\t                        type=str,\n\t                        help='path to load model config')\n\t    parser.add_argument('--text_only', action='store_true', default=False, help='whether only use text')\n\t    parser.add_argument('--checkpoint',\n\t                        default=None,\n\t                        type=str,\n", "                        help='name or path to load weights')\n\t    parser.add_argument('--lr_decay_every',\n\t                        default=4,\n\t                        type=int,\n\t                        help='lr_decay_every')\n\t    parser.add_argument('--lr_decay_ratio',\n\t                        default=0.8,\n\t                        type=float,\n\t                        help='lr_decay_ratio')\n\t    # training and evaluation\n", "    parser.add_argument('--epochs',\n\t                        default=35,\n\t                        type=int,\n\t                        help='number of training epoch')\n\t    parser.add_argument('--eval_every', default=1, type=int, help='eval_every')\n\t    parser.add_argument('--lr', default=1e-2, type=float, help='learning rate')\n\t    parser.add_argument('--num_beams',\n\t                        default=4,\n\t                        type=int,\n\t                        help='level of beam search on validation')\n", "    parser.add_argument(\n\t        '--continue_training',\n\t        action='store_true',\n\t        help='continue training, load optimizer and epoch from checkpoint')\n\t    parser.add_argument('--warmup', default=0.1, type=float, help='warmup')\n\t    # dropout\n\t    parser.add_argument(\n\t        '--dropout',\n\t        default=None,\n\t        type=float,\n", "        help=\n\t        'dropout rate for the transformer. This overwrites the model config')\n\t    parser.add_argument(\n\t        '--classif_dropout',\n\t        default=None,\n\t        type=float,\n\t        help=\n\t        'dropout rate for the classification layers. This overwrites the model config'\n\t    )\n\t    parser.add_argument(\n", "        '--attention_dropout',\n\t        default=None,\n\t        type=float,\n\t        help=\n\t        'dropout rate for the attention layers. This overwrites the model config'\n\t    )\n\t    parser.add_argument(\n\t        '--activation_dropout',\n\t        default=None,\n\t        type=float,\n", "        help=\n\t        'dropout rate for the activation layers. This overwrites the model config'\n\t    )\n\t    # hardware and performance\n\t    parser.add_argument('--grad_clip', default=5, type=float, help='grad_clip')\n\t    parser.add_argument('--gpu_num',\n\t                        default=1,\n\t                        type=int,\n\t                        help='number of GPUs in total')\n\t    parser.add_argument('--cpu',\n", "                        action='store_true',\n\t                        help='if only use cpu to run the model')\n\t    parser.add_argument('--amp',\n\t                        action='store_true',\n\t                        help='whether or not to use amp')\n\t    parser.add_argument('--master_port',\n\t                        type=str,\n\t                        default='12355',\n\t                        help='master port for DDP')\n\t    parser.add_argument('--batch_size',\n", "                        type=int,\n\t                        default=16,\n\t                        help='training batch size')\n\t    parser.add_argument('--seed', type=int, default=42, help='seed')\n\t    parser.add_argument('--num_workers',\n\t                        type=int,\n\t                        default=4,\n\t                        help='#workers for data loader')\n\t    parser.add_argument('--max_len', type=int, default=10, help='max_len')\n\t    parser.add_argument('--num_image_tokens', type=int, default=2, help='the length of image_tokens')\n", "    parser.add_argument('--max_len_a',\n\t                        type=float,\n\t                        default=0.6,\n\t                        help='max_len_a')\n\t    parser.add_argument('--bart_init',\n\t                        type=int,\n\t                        default=1,\n\t                        help='use bart_init or not')\n\t    parser.add_argument('--check_info',\n\t                        type=str,\n", "                        default='',\n\t                        help='check path to save')\n\t    parser.add_argument('--is_check',\n\t                        type=int,\n\t                        default=0,\n\t                        help='save the model or not')\n\t    parser.add_argument('--task', type=str, default='AESC', help='task type')\n\t    parser.add_argument('--has_prompt',  action='store_true', default=False, help='whether has prompt')\n\t    parser.add_argument('--use_generated_aspect_prompt',  action='store_true', default=False, help='whether use the generated aspect prompt')\n\t    parser.add_argument('--use_generated_senti_prompt',  action='store_true', default=False, help='whether use the generated sentiment prompt')\n", "    parser.add_argument('--use_different_senti_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n\t    parser.add_argument('--use_different_aspect_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n\t    parser.add_argument('--use_multitasks', action='store_true', default=False, help='whether use multitasks')\n\t    parser.add_argument('--loss_lambda', default=0.1, type=float, help='the weight of aspect_num classification loss')\n\t    args = parser.parse_args()\n\t    if args.gpu_num != 1 and args.cpu:\n\t        raise ValueError('--gpu_num are not allowed if --cpu is set to true')\n\t    if args.checkpoint is None and args.model_config is None:\n\t        raise ValueError(\n\t            '--model_config and --checkpoint cannot be empty at the same time')\n", "    return args\n\tif __name__ == '__main__':\n\t    args = parse_args()\n\t    # mp.spawn(main, args=(args, ), nprocs=args.gpu_num, join=True)\n\t    torch.manual_seed(args.seed)\n\t    torch.cuda.manual_seed_all(args.seed)\n\t    torch.cuda.manual_seed(args.seed)\n\t    np.random.seed(args.seed)\n\t    random.seed(args.seed)\n\t    cudnn.deterministic = True\n", "    main(0, args)\n"]}
{"filename": "src/eval_utils_multitasks.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tdef eval(args, model, loader, metric, device):\n\t    num_correct =0 \n\t    model.eval()\n\t    for i, batch in enumerate(loader):\n\t        # Forward pass\n\t        if args.task == 'twitter_ae':\n\t            aesc_infos = {\n\t                key: value\n", "                for key, value in batch['TWITTER_AE'].items()\n\t            }\n\t        elif args.task == 'twitter_sc':\n\t            aesc_infos = {\n\t                key: value\n\t                for key, value in batch['TWITTER_SC'].items()\n\t            }\n\t        else:\n\t            aesc_infos = {key: value for key, value in batch['AESC'].items()}\n\t        # import ipdb; ipdb.set_trace()\n", "        predict, predict_aspects_num = model.predict(\n\t            input_ids=batch['input_ids'].to(device),\n\t            image_features=list(\n\t                map(lambda x: x.to(device), batch['image_features'])),\n\t            attention_mask=batch['attention_mask'].to(device),\n\t            aesc_infos=aesc_infos, \n\t            aspects_num=batch['aspects_num'])\n\t        target_aspects_num = torch.tensor(batch['aspects_num']).to(predict_aspects_num.device)\n\t        num_correct += torch.eq(predict_aspects_num, target_aspects_num).sum().float().item()\n\t        # print('predict is {}'.format(predict))\n", "        metric.evaluate(aesc_infos['spans'], predict,\n\t                        aesc_infos['labels'].to(device))\n\t        # break\n\t    aspects_num_eval_acc = num_correct/len(loader.dataset)\n\t    res = metric.get_metric()\n\t    model.train()\n\t    return res, aspects_num_eval_acc\n"]}
{"filename": "src/eval_utils.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tdef eval(args, model, loader, metric, device):\n\t    model.eval()\n\t    for i, batch in enumerate(loader):\n\t        # Forward pass\n\t        if args.task == 'twitter_ae':\n\t            aesc_infos = {\n\t                key: value\n\t                for key, value in batch['TWITTER_AE'].items()\n", "            }\n\t        elif args.task == 'twitter_sc':\n\t            aesc_infos = {\n\t                key: value\n\t                for key, value in batch['TWITTER_SC'].items()\n\t            }\n\t        else:\n\t            aesc_infos = {key: value for key, value in batch['AESC'].items()}\n\t        # import ipdb; ipdb.set_trace()\n\t        predict = model.predict(\n", "            input_ids=batch['input_ids'].to(device),\n\t            image_features=list(\n\t                map(lambda x: x.to(device), batch['image_features'])),\n\t            attention_mask=batch['attention_mask'].to(device),\n\t            aesc_infos=aesc_infos, \n\t            aspects_num=batch['aspects_num'])\n\t        # print('predict is {}'.format(predict))\n\t        metric.evaluate(aesc_infos['spans'], predict,\n\t                        aesc_infos['labels'].to(device))\n\t        # break\n", "    res = metric.get_metric()\n\t    model.train()\n\t    return res\n"]}
{"filename": "src/utils.py", "chunked_list": ["import logging\n\timport sys\n\timport os\n\timport torch\n\timport torch.distributed as dist\n\tdef setup_process(rank, world_size, master_port='12355'):\n\t    os.environ['MASTER_ADDR'] = 'localhost'\n\t    os.environ['MASTER_PORT'] = master_port\n\t    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\tdef cleanup_process():\n", "    dist.destroy_process_group()\n\tdef save_training_data(path, optimizer=None, scaler=None, epoch=None):\n\t    checkpoint = {\n\t        'optimizer': None if optimizer is None else optimizer.state_dict(),\n\t        'scaler': None if scaler is None else scaler.state_dict(),\n\t        'epoch': epoch\n\t    }\n\t    torch.save(checkpoint, os.path.join(path, 'training_data.pt'))\n\tdef load_training_data(path, optimizer=None, scaler=None, map_location=None):\n\t    checkpoint = torch.load(os.path.join(path, 'training_data.pt'), map_location=map_location)\n", "    if optimizer is not None and 'optimizer' in checkpoint:\n\t        optimizer.load_state_dict(checkpoint['optimizer'])\n\t    if scaler is not None and 'scaler' in checkpoint:\n\t        scaler.load_state_dict(checkpoint['scaler'])\n\t    return checkpoint\n\tclass Logger:\n\t    def __init__(self, log_dir=None, enabled=True, pad_length=50):\n\t        self._logger = self._get_logger(log_dir) if enabled else None\n\t        self._pad_length = pad_length\n\t    def _pad_message(self, message):\n", "        return (\" \" + message + \" \").center(self._pad_length, '=')\n\t    def info(self, message, pad=False):\n\t        if self._logger is not None:\n\t            message = self._pad_message(message) if pad else message\n\t            self._logger.info(message)\n\t    def line(self):\n\t        if self._logger is not None:\n\t            self._logger.info('=' * self._pad_length)\n\t    @staticmethod\n\t    def _get_logger(log_dir=None):\n", "        \"\"\"\n\t        get a logger for displaying information to console or log to file (optional)\n\t        :param log_dir: str, logging path. None for not log to file\n\t        :return: logger\n\t        \"\"\"\n\t        logger = logging.getLogger()\n\t        logger.setLevel(logging.DEBUG)\n\t        stream_handler = logging.StreamHandler(sys.stdout)\n\t        stream_handler.flush = sys.stdout.flush\n\t        logger.addHandler(stream_handler)\n", "        if log_dir is not None:\n\t            file_handler = logging.FileHandler(log_dir)\n\t            formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n\t            file_handler.setFormatter(formatter)\n\t            logger.addHandler(file_handler)\n\t        return logger\n\tclass TaskType:\n\t    AFTER = 'after'\n\t    BEFORE = 'before'\n\t    INTENT = 'intent'\n", "    CAPTION = 'caption'\n\t    REGION_CAPTION = 'region_caption'\n\t    ALL_TYPES = {AFTER, BEFORE, INTENT, CAPTION, REGION_CAPTION}\n"]}
{"filename": "src/training_multitasks.py", "chunked_list": ["from datetime import datetime\n\timport numpy as np\n\tfrom torch.cuda.amp import autocast\n\timport src.model.utils as utils\n\timport src.eval_utils as eval_utils\n\t# from src.utils import TaskType\n\timport torch\n\tdef pretrain(task_list,\n\t             epoch,\n\t             model,\n", "             train_loaders,\n\t             optimizer_dict,\n\t             device,\n\t             args,\n\t             logger=None,\n\t             callback=None,\n\t             log_interval=1,\n\t             tb_writer=None,\n\t             tb_interval=1,\n\t             scaler=None):\n", "    # assert len(task_list) == len(train_loaders)\n\t    total_step = len(train_loaders[0])\n\t    model.train()\n\t    total_loss = 0\n\t    start_time = datetime.now()\n\t    for i, batchs in enumerate(zip(*train_loaders)):\n\t        # Forward pass\n\t        with autocast(enabled=args.amp):\n\t            loss_all = []\n\t            total_loss = 0\n", "            for cnt, task in enumerate(task_list):\n\t                batch = batchs[cnt]\n\t                # print(batch.keys())\n\t                if task == 'Sentiment':\n\t                    loss, prelogits = model.forward(\n\t                        task,\n\t                        input_ids=batch['input_ids'].to(device),\n\t                        image_features=list(\n\t                            map(lambda x: x.to(device),\n\t                                batch['image_features'])),\n", "                        attention_mask=batch['attention_mask'].to(device),\n\t                        senti_infos={\n\t                            key: value.to(device)\n\t                            for key, value in batch['Sentiment'].items()\n\t                        })\n\t                else:\n\t                    loss = model.forward(\n\t                        task,\n\t                        input_ids=batch['input_ids'].to(device),\n\t                        image_features=list(\n", "                            map(lambda x: x.to(device),\n\t                                batch['image_features'])),\n\t                        attention_mask=batch['attention_mask'].to(device),\n\t                        mlm_infos={\n\t                            key: value.to(device)\n\t                            for key, value in batch['MLM'].items()\n\t                        } if 'MLM' in batch else None,\n\t                        mrm_infos={\n\t                            key: value\n\t                            for key, value in batch['MRM'].items()\n", "                        } if 'MRM' in batch else None,\n\t                        senti_infos={\n\t                            key: value.to(device)\n\t                            for key, value in batch['Sentiment'].items()\n\t                        } if 'Sentiment' in batch else None,\n\t                        ANP_infos={\n\t                            key: value.to(device)\n\t                            for key, value in batch['ANP'].items()\n\t                        } if 'ANP' in batch else None,\n\t                        ANP_generate_infos={\n", "                            key: value.to(device)\n\t                            for key, value in batch['ANP_generate'].items()\n\t                        } if 'ANP_generate' in batch else None,\n\t                        ae_oe_infos={\n\t                            key: value\n\t                            for key, value in batch['AE_OE'].items()\n\t                        } if 'AE_OE' in batch else None)\n\t                # print(loss.dtype)\n\t                loss_all.append(loss)\n\t                optimizer_dict.zero_grad()\n", "                loss.backward()\n\t                optimizer_dict.step()\n\t            for k, v in zip(task_list, loss_all):\n\t                print(k + ':', v.item(), end=' ')\n\t            print()\n\t        # Backward and optimize\n\t        if logger is not None and i % log_interval == 0:\n\t            logger.info('Epoch [{}/{}], Step [{}/{}]'.format(\n\t                epoch + 1, args.epochs, i + 1, total_step))\n\t            loss_text = ' '.join(\n", "                [k + ':' + str(v.item()) for k, v in zip(task_list, loss_all)])\n\t            logger.info(loss_text + '\\n')\n\tdef fine_tune(epoch,\n\t              model,\n\t              train_loader,\n\t              test_loader,\n\t              metric,\n\t              optimizer,\n\t              device,\n\t              args,\n", "              logger=None,\n\t              callback=None,\n\t              log_interval=1,\n\t              tb_writer=None,\n\t              tb_interval=1,\n\t              scaler=None):\n\t    total_step = len(train_loader)\n\t    model.train()\n\t    total_loss = 0\n\t    start_time = datetime.now()\n", "    num_correct =0\n\t    for i, batch in enumerate(train_loader):\n\t        # Forward pass\n\t        if args.task == 'twitter_ae':\n\t            aesc_infos = {\n\t                key: value\n\t                for key, value in batch['TWITTER_AE'].items()\n\t            }\n\t        elif args.task == 'twitter_sc':\n\t            aesc_infos = {\n", "                key: value\n\t                for key, value in batch['TWITTER_SC'].items()\n\t            }\n\t        else:\n\t            aesc_infos = {key: value for key, value in batch['AESC'].items()}\n\t            # import ipdb; ipdb.set_trace()\n\t            # print(\"+++++++++++++++++++++++++++++++++++++++++++\")\n\t            # print('aesc_infos is {}'.format(aesc_infos))\n\t        with autocast(enabled=args.amp):\n\t            loss, predict_aspects_num = model.forward(\n", "                input_ids=batch['input_ids'].to(device),\n\t                image_features=list(\n\t                    map(lambda x: x.to(device), batch['image_features'])),\n\t                attention_mask=batch['attention_mask'].to(device),\n\t                aesc_infos=aesc_infos, \n\t                aspects_num=batch['aspects_num'])\n\t            target_aspects_num = torch.tensor(batch['aspects_num']).to(predict_aspects_num.device)\n\t            num_correct += torch.eq(predict_aspects_num, target_aspects_num).sum().float().item()\n\t            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n\t                epoch + 1, args.epochs, i + 1, total_step, loss.item()))\n", "        train_acc = num_correct/len(train_loader.dataset)\n\t        print('The accuracy of aspects_num is {:.4f} !!!!'.format(train_acc))\n\t        # Backward and optimize\n\t        cur_step = i + 1 + epoch * total_step\n\t        t_step = args.epochs * total_step\n\t        liner_warm_rate = utils.liner_warmup(cur_step, t_step, args.warmup)\n\t        utils.set_lr(optimizer, liner_warm_rate * args.lr)\n\t        optimizer.zero_grad()\n\t        loss.backward()\n\t        utils.clip_gradient(optimizer, args.grad_clip)\n", "        optimizer.step()"]}
{"filename": "src/data/tokenization_new_for_generated_prompt.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom transformers import BartTokenizer, AutoTokenizer\n\tfrom itertools import chain\n\tfrom functools import cmp_to_key\n\t# from src.utils import TaskType\n\tdef cmp(v1, v2):\n\t    if v1[0] == v2[0]:\n\t        return v1[1] - v2[1]\n\t    return v1[0] - v2[0]\n", "class ConditionTokenizer:\n\t    \"\"\"\n\t    tokenizer for image features, event and task type\n\t    this is NOT inherent from transformers Tokenizer\n\t    \"\"\"\n\t    def __init__(self,\n\t                 args,\n\t                 pretrained_model_name='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n\t                 cls_token=\"<<cls>>\",\n\t                 mlm_token=\"<<mlm>>\",\n", "                 mrm_token=\"<<mrm>>\",\n\t                 begin_text=\"<<text>>\",\n\t                 end_text=\"<</text>>\",\n\t                 img_feat='<<img_feat>>',\n\t                 begin_img=\"<<img>>\",\n\t                 end_img=\"<</img>>\",\n\t                 img_caption='<<img_caption>>',\n\t                 begin_caption='<<cap>>',\n\t                 end_caption='<</cap>>',\n\t                 ae_token='<<AE>>',\n", "                 sc_token='<<SC>>',\n\t                 ae_oe_token=\"<<AOE>>\",\n\t                 sep_token=\"<<SEP>>\",\n\t                 aesc_token='<<AESC>>',\n\t                 pos_token='<<POS>>',\n\t                 neu_token='<<NEU>>',\n\t                 neg_token='<<NEG>>',\n\t                 aspect_prompt_token='<<AE_PROMPT>>',\n\t                 senti_prompt_token='<<SENTI_PROMPT>>',\n\t                 begin_prompt='<<prompt>>',\n", "                 end_prompt='<</prompt>>',\n\t                 senti_token='<<senti>>',\n\t                 ANP_token='<<ANP>>',\n\t                 ANP_generate_token='<<AOG>>'):\n\t        self._base_tokenizer = BartTokenizer.from_pretrained(\n\t            pretrained_model_name, )\n\t        # self._base_tokenizer = AutoTokenizer.from_pretrained(\n\t        #     pretrained_model_name)\n\t        self.additional_special_tokens = [\n\t            cls_token, mlm_token, mrm_token, begin_text, end_text, img_feat, begin_img, end_img, \n", "            img_caption, begin_caption, end_caption,\n\t            senti_token, ANP_token, ANP_generate_token,\n\t            pos_token, neu_token, neg_token, ae_oe_token, sep_token,\n\t            aesc_token, ae_token, sc_token, \n\t            aspect_prompt_token, senti_prompt_token, begin_prompt, end_prompt\n\t        ]\n\t        unique_no_split_tokens = self._base_tokenizer.unique_no_split_tokens\n\t        self._base_tokenizer.unique_no_split_tokens = unique_no_split_tokens + self.additional_special_tokens\n\t        self.unique_no_split_tokens = self._base_tokenizer.unique_no_split_tokens\n\t        print(self.unique_no_split_tokens)\n", "        self._base_tokenizer.add_tokens(self.additional_special_tokens)\n\t        self.cls_token = cls_token\n\t        self.mlm_token = mlm_token\n\t        self.mrm_token = mrm_token\n\t        self.begin_text = begin_text\n\t        self.end_text = end_text\n\t        self.img_feat = img_feat\n\t        self.begin_img = begin_img\n\t        self.end_img = end_img\n\t        self.img_caption = img_caption\n", "        self.begin_caption = begin_caption\n\t        self.end_caption = end_caption\n\t        self.ae_token = ae_token\n\t        self.sc_token = sc_token\n\t        self.ae_oe_token = ae_oe_token\n\t        self.sep_token = sep_token\n\t        self.senti_token = senti_token\n\t        self.ANP_token = ANP_token\n\t        self.ANP_generate_token = ANP_generate_token\n\t        self.aesc_token = aesc_token\n", "        self.pos_token = pos_token\n\t        self.neu_token = neu_token\n\t        self.neg_token = neg_token\n\t        self.aspect_prompt_token = aspect_prompt_token\n\t        self.senti_prompt_token = senti_prompt_token\n\t        self.begin_prompt = begin_prompt\n\t        self.end_prompt = end_prompt\n\t        self.cls_token_id = self.convert_tokens_to_ids(cls_token)\n\t        self.mlm_token_id = self.convert_tokens_to_ids(mlm_token)\n\t        self.mrm_token_id = self.convert_tokens_to_ids(mrm_token)\n", "        self.begin_text_id = self.convert_tokens_to_ids(begin_text)\n\t        self.end_text_id = self.convert_tokens_to_ids(end_text)\n\t        self.img_feat_id = self.convert_tokens_to_ids(img_feat)\n\t        self.begin_img_id = self.convert_tokens_to_ids(begin_img)\n\t        self.end_img_id = self.convert_tokens_to_ids(end_img)\n\t        self.img_caption_id = self.convert_tokens_to_ids(img_caption)\n\t        self.begin_caption_id = self.convert_tokens_to_ids(begin_caption)\n\t        self.end_caption_id = self.convert_tokens_to_ids(end_caption)\n\t        self.ae_token_id = self.convert_tokens_to_ids(ae_token)\n\t        self.sc_token_id = self.convert_tokens_to_ids(sc_token)\n", "        self.ae_oe_token_id = self.convert_tokens_to_ids(ae_oe_token)\n\t        self.sep_token_id = self.convert_tokens_to_ids(sep_token)\n\t        self.senti_token_id = self.convert_tokens_to_ids(senti_token)\n\t        self.ANP_token_id = self.convert_tokens_to_ids(ANP_token)\n\t        self.ANP_generate_token_id = self.convert_tokens_to_ids(\n\t            ANP_generate_token)\n\t        self.aesc_token_id = self.convert_tokens_to_ids(aesc_token)\n\t        self.pos_token_id = self.convert_tokens_to_ids(pos_token)\n\t        self.neu_token_id = self.convert_tokens_to_ids(neu_token)\n\t        self.neg_token_id = self.convert_tokens_to_ids(neg_token)\n", "        self.aspect_prompt_token_id = self.convert_tokens_to_ids(aspect_prompt_token)\n\t        self.senti_prompt_token_id = self.convert_tokens_to_ids(senti_prompt_token)\n\t        self.begin_prompt_id =  self.convert_tokens_to_ids(begin_prompt)\n\t        self.end_prompt_id =  self.convert_tokens_to_ids(end_prompt)\n\t        self.vocab_size = self._base_tokenizer.vocab_size\n\t        self.bos_token = self._base_tokenizer.bos_token\n\t        self.bos_token_id = self._base_tokenizer.bos_token_id\n\t        self.eos_token = self._base_tokenizer.eos_token\n\t        self.eos_token_id = self._base_tokenizer.eos_token_id\n\t        self.pad_token = self._base_tokenizer.pad_token\n", "        self.pad_token_id = self._base_tokenizer.pad_token_id\n\t        self.unk_token = self._base_tokenizer.unk_token\n\t        self.unk_token_id = self._base_tokenizer.unk_token_id\n\t        print('self.bos_token_id', self.bos_token_id)\n\t        print('self.eos_token_id', self.eos_token_id)\n\t        print('self.pad_token_id', self.pad_token_id)\n\t        print('self.begin_caption_token_id', self.begin_caption_id)\n\t        print('self.end_caption_token_id', self.end_caption_id)\n\t        print('self.aspect_prompt_token_id', self.aspect_prompt_token_id)\n\t        print('self.senti_prompt_token_id', self.senti_prompt_token_id)\n", "        print('self.begin_prompt_id', self.begin_prompt_id)\n\t        print('self.end_prompt_id', self.end_prompt_id)\n\t        if args.task == 'pretrain':\n\t            self.mapping = {'AE_OE': '<<AOE>>', 'SEP': '<<SEP>>'}\n\t        else:\n\t            if args.task == 'twitter_sc':\n\t                self.mapping = {\n\t                    'SC': '<<SC>>',\n\t                    'POS': '<<POS>>',\n\t                    'NEU': '<<NEU>>',\n", "                    'NEG': '<<NEG>>'\n\t                }\n\t            elif args.task == 'twitter_ae':\n\t                self.mapping = {\n\t                    'AE': '<<AE>>',\n\t                    'POS': '<<POS>>',\n\t                    'NEU': '<<NEU>>',\n\t                    'NEG': '<<NEG>>'\n\t                }\n\t            else:\n", "                self.mapping = {\n\t                    'AESC': '<<AESC>>',\n\t                    'POS': '<<POS>>',\n\t                    'NEU': '<<NEU>>',\n\t                    'NEG': '<<NEG>>'\n\t                }\n\t        self.senti = {'POS': '<<POS>>', 'NEU': '<<NEU>>', 'NEG': '<<NEG>>'}\n\t        self.senti2id = {}\n\t        for key, value in self.senti.items():\n\t            key_id = self._base_tokenizer.convert_tokens_to_ids(\n", "                self._base_tokenizer.tokenize(value))\n\t            assert len(key_id) == 1, value\n\t            # assert key_id[0] >= self.cur_num_tokens\n\t            self.senti2id[key] = key_id[0]\n\t        self.mapping2id = {}\n\t        self.mapping2targetid = {}\n\t        for key, value in self.mapping.items():\n\t            key_id = self._base_tokenizer.convert_tokens_to_ids(\n\t                self._base_tokenizer.tokenize(value))\n\t            assert len(key_id) == 1, value\n", "            # assert key_id[0] >= self.cur_num_tokens\n\t            self.mapping2id[key] = key_id[0]\n\t            self.mapping2targetid[key] = len(self.mapping2targetid) + 2\n\t        print(self.mapping2id)\n\t        '''\n\t        for AESC:\n\t        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n\t        '''\n\t    def encode(self, *args, **kwargs):\n\t        return self._base_tokenizer(*args, **kwargs)\n", "    def pad_tokens(self, tokens):\n\t        max_len = max([len(x) for x in tokens])\n\t        pad_result = torch.full((len(tokens), max_len),\n\t                                self.pad_token_id,\n\t                                dtype=torch.long)\n\t        mask = torch.zeros(pad_result.size(), dtype=torch.bool)\n\t        for i, x in enumerate(tokens):\n\t            pad_result[i, :len(x)] = torch.tensor(tokens[i], dtype=torch.long)\n\t            mask[i, :len(x)] = True\n\t        return pad_result, mask\n", "    def pad_tokens_with_maxlength(self, tokens, max_len):\n\t        pad_result = torch.full((len(tokens), max_len),\n\t                                self.pad_token_id,\n\t                                dtype=torch.long)\n\t        mask = torch.zeros(pad_result.size(), dtype=torch.bool)\n\t        for i, x in enumerate(tokens):\n\t            # print(x)\n\t            pad_result[i, :len(x)] = torch.tensor(tokens[i], dtype=torch.long)\n\t            mask[i, :len(x)] = True\n\t        # print(\"=====================pad_result=========================\")\n", "        # print(pad_result)\n\t        # print(mask)\n\t        return pad_result, mask\n\t    def encode_mlm_sentence(self, labels):\n\t        label_split = [x.split() for x in labels]\n\t        input_tokens = []\n\t        for split in label_split:\n\t            cur_num = 0\n\t            bpes = [self.bos_token_id]\n\t            for x in split:\n", "                tokens = self._base_tokenizer(x, add_prefix_space=True)\n\t                bpes = bpes + tokens\n\t            bpes.append(self.eos_token_id)\n\t            input_tokens.append(input_tokens)\n\t        return input_tokens\n\t    def encode_condition(self, task, img_num=None, caption=None, sentence=None, has_prompt=False, aspects_num=None,  text_only=False):\n\t        \"\"\"\n\t        tokenize text, image features and event\n\t        the output format (after decoded back):\n\t        task_type [<img> <img_feat> ... <img_feat> </img>] [<event> EVENT </event>] [<mlm> MLM </mlm>]\n", "        :param task_type: str or list[str]\n\t        :param img_num: int or list[int], the number of image features\n\t        :param event: str or list[str], event descriptions\n\t        :param mlm: str or list[str], sentence for masked language modeling\n\t        :return: dict {str: Tensor}, {\n\t                \"input_ids\": ...,\n\t                \"attention_mask\": ...,\n\t                \"event_mask\": ...,          only exist if event is given. 1 for the position with event tokens\n\t                \"mlm_mask\": ...,            only exist if mlm is given. 1 for the position with mlm tokens\n\t                \"img_mask\":...,             only exist if img_num is given. 1 for the position with img tokens\n", "            }\n\t        \"\"\"\n\t        '''\n\t        [image_features] + is + [image_caption] + [text] + [aspect_prompt_token]*len_1(最多为5) + has + [senti_prompt_token]*len_2 'sentiment'\n\t        prompt_1: [image_features] + is + [image_caption]  \n\t        + 'There is' <prompt> ([aspect_prompt_token]*len_1(最多为5) + of [senti_prompt_token]*len_2 + 'sentiment' + <sep>)*n </prompt> + 'in'\n\t        + [text]\n\t        prompt_2: [image_features] + is + [image_caption]  \n\t        + <prompt> ([aspect_prompt_token]*len_1(最多为5) + of [senti_prompt_token]*len_2)*n </prompt> \n\t        + [text]\n", "        prompt_3: [image_features] + is + [image_caption]  \n\t        + 'There is' <prompt> ([aspect_prompt_token]*len_1(最多为5) + of <<NEU>> or <<POS>> or <<NEG>> + 'sentiment' + <sep>)*n </prompt> + 'in'\n\t        + [text]\n\t        '''\n\t        image_text = None\n\t        if img_num is not None:\n\t            if not isinstance(img_num, list):\n\t                img_num = [img_num]\n\t            image_text = []\n\t            for index, value in enumerate(img_num):\n", "                image_text.append(self.begin_img + self.img_feat * value +  ###引入image_caption token\n\t                                  self.end_img)\n\t        # import ipdb; ipdb.set_trace()\n\t        if caption is not None:\n\t            if not isinstance(caption, list):\n\t                caption = [caption]\n\t            caption_split = [x.split() for x in caption]\n\t            image_caption_tokens = []\n\t            for split in caption_split:\n\t                '''\n", "                print(split)\n\t                为方便起见，固定caption文本长度\n\t                '''\n\t                # print(\"+++++++++++++++++++++split before ++++++++++++++++++++++++\")\n\t                # print(len(split))\n\t                if len(split)>10:\n\t                    split = split[:10]\n\t                # print(\"+++++++++++++++++++++split after ++++++++++++++++++++++++\")\n\t                # print(len(split))\n\t                # print(split)\n", "                is_bpes = self._base_tokenizer.tokenize('is',\n\t                                                         add_prefix_space=True)\n\t                is_bpes = self._base_tokenizer.convert_tokens_to_ids(is_bpes) ##[16]\n\t                caption_word_bpes = [is_bpes]\n\t                caption_word_bpes.append([self.begin_caption_id])          \n\t                for caption_word in split:\n\t                    caption_bpes = self._base_tokenizer.tokenize(caption_word,\n\t                                                         add_prefix_space=True)\n\t                    caption_bpes = self._base_tokenizer.convert_tokens_to_ids(caption_bpes)\n\t                    caption_word_bpes.append(caption_bpes)\n", "                caption_word_bpes.append([self.end_caption_id])\n\t                _caption_word_bpes = list(chain(*caption_word_bpes))\n\t                image_caption_tokens.append(_caption_word_bpes.copy())\n\t        if sentence is not None:\n\t            if not isinstance(sentence, list):\n\t                sentence = [sentence]\n\t            sentence_split = [x.split() for x in sentence]\n\t            input_sentence_tokens = []\n\t            for split in sentence_split:\n\t                word_bpes = [[self.bos_token_id]]\n", "                for word in split:\n\t                    bpes = self._base_tokenizer.tokenize(word,\n\t                                                         add_prefix_space=True)\n\t                    bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                    word_bpes.append(bpes)\n\t                word_bpes.append([self.eos_token_id])\n\t                _word_bpes = list(chain(*word_bpes))\n\t                input_sentence_tokens.append(_word_bpes.copy())\n\t        if has_prompt:\n\t            aspect_prompts_tokens = []     \n", "            for index, value in enumerate(aspects_num):\n\t                # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n\t                #                   self.end_prompt)\n\t                aspect_prompt_bpes = [[self.begin_prompt_id]]\n\t                if value == 1:\n\t                    _be = 'is'\n\t                else:\n\t                    _be = 'are'\n\t                there_bpes = self._base_tokenizer.tokenize('there',\n\t                                                        add_prefix_space=True)\n", "                there_bpes = self._base_tokenizer.convert_tokens_to_ids(there_bpes)\n\t                aspect_prompt_bpes.append(there_bpes)\n\t                _be_bpes = self._base_tokenizer.tokenize(_be,\n\t                                                        add_prefix_space=True)\n\t                _be_bpes = self._base_tokenizer.convert_tokens_to_ids(_be_bpes)\n\t                aspect_prompt_bpes.append(_be_bpes)\n\t                for i in range(value):\n\t                    if task == 'AESC':\n\t                        aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\t                        of_bpes = self._base_tokenizer.tokenize('of',\n", "                                                                add_prefix_space=True)\n\t                        of_bpes = self._base_tokenizer.convert_tokens_to_ids(of_bpes)\n\t                        aspect_prompt_bpes.append(of_bpes)\n\t                        aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\t                        senti_bpes = self._base_tokenizer.tokenize('sentiment',\n\t                                                                add_prefix_space=True)\n\t                        senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n\t                        aspect_prompt_bpes.append(senti_bpes)\n\t                        if i <(value-1):\n\t                            aspect_prompt_bpes.append([self.sep_token_id])\n", "                    elif task == 'twitter_sc':\n\t                        aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\t                        senti_bpes = self._base_tokenizer.tokenize('sentiment',\n\t                                                                add_prefix_space=True)\n\t                        senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n\t                        aspect_prompt_bpes.append(senti_bpes)\n\t                        if i <(value-1):\n\t                            aspect_prompt_bpes.append([self.sep_token_id])\n\t                    elif task == 'twitter_ae':\n\t                        aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n", "                        if i <(value-1):\n\t                            aspect_prompt_bpes.append([self.sep_token_id])\n\t                    else:\n\t                        print('Not is right task, please check code!!!')\n\t                aspect_prompt_bpes.append([self.end_prompt_id])\n\t                in_bpes =  self._base_tokenizer.tokenize('in',\n\t                                                        add_prefix_space=True)\n\t                in_bpes = self._base_tokenizer.convert_tokens_to_ids(in_bpes)\n\t                aspect_prompt_bpes.append(in_bpes)\n\t                _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n", "                aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n\t        if image_text is not None:\n\t            image_sentence = self.encode(image_text,\n\t                                         add_special_tokens=False,\n\t                                         return_tensors='pt',\n\t                                         padding=True)\n\t            image_ids = image_sentence['input_ids']\n\t            image_attention_mask = image_sentence['attention_mask']\n\t            image_caption_tokens, image_caption_mask = self.pad_tokens_with_maxlength(\n\t                image_caption_tokens, max_len=20)\n", "            input_sentence_tokens, input_sentence_mask = self.pad_tokens(\n\t                input_sentence_tokens)\n\t            aspect_prompts_tokens, aspect_prompts_mask = self.pad_tokens_with_maxlength(aspect_prompts_tokens, max_len=40)\n\t            if text_only:\n\t                image_attention_mask = torch.zeros(image_ids.size())\n\t                image_caption_mask = torch.zeros(image_caption_tokens.size())\n\t            input_ids = torch.cat((image_ids, image_caption_tokens, aspect_prompts_tokens, input_sentence_tokens), 1)\n\t            attention_mask = torch.cat(\n\t                (image_attention_mask, image_caption_mask, aspect_prompts_mask, input_sentence_mask), 1)\n\t        else:\n", "            input_sentence_tokens, input_sentence_mask = self.pad_tokens(\n\t                input_sentence_tokens)\n\t            input_ids = input_sentence_tokens\n\t            attention_mask = input_sentence_mask\n\t        encoded = {}\n\t        encoded['input_ids'] = input_ids\n\t        encoded['attention_mask'] = attention_mask\n\t        # build mlm mask\n\t        if sentence is not None:\n\t            sentence_mask = torch.zeros(input_ids.size(), dtype=torch.bool)\n", "            for index, value in enumerate(input_ids):\n\t                start = (value == self.bos_token_id).nonzero(as_tuple=True)[0]\n\t                end = (value == self.eos_token_id).nonzero(as_tuple=True)[0]\n\t                sentence_mask[index, start + 1:end] = True\n\t            encoded['sentence_mask'] = sentence_mask\n\t        # build img mask\n\t        if img_num is not None:\n\t            encoded['img_mask'] = encoded['input_ids'] == self.img_feat_id\n\t        return encoded\n\t    def encode_label(self, label, img_num=None):  #generate labels for MLM task\n", "        # build text label\n\t        if not isinstance(label, list):\n\t            label = [label]\n\t        label_split = [x.split() for x in label]\n\t        label_tokens = []\n\t        for split in label_split:\n\t            word_bpes = [[self.bos_token_id], [self.mlm_token_id]]\n\t            for word in split:\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n", "                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.eos_token_id])\n\t            _word_bpes = list(chain(*word_bpes))\n\t            label_tokens.append(_word_bpes)\n\t        input_ids, attention_mask = self.pad_tokens(label_tokens)\n\t        output_shape = input_ids[:, 2:].shape\n\t        labels = torch.empty(output_shape, dtype=torch.long)\n\t        decoder_input_ids = torch.empty(input_ids[:, 1:].shape,\n\t                                        dtype=torch.long)\n", "        decoder_attention_mask = torch.empty(input_ids[:, 1:].shape,\n\t                                             dtype=torch.long)\n\t        for i in range(labels.size(0)):\n\t            labels[i] = input_ids[i][(input_ids[i] != self.bos_token_id)\n\t                                     & (input_ids[i] != self.mlm_token_id)]\n\t            decoder_input_ids[i] = input_ids[i][\n\t                input_ids[i] != self.eos_token_id]\n\t            decoder_attention_mask[i] = attention_mask[i][\n\t                input_ids[i] != self.eos_token_id]\n\t        labels[(labels == self.pad_token_id) | (labels == self.begin_img_id) |\n", "               (labels == self.end_img_id) | (labels == self.mlm_token_id) |\n\t               (labels == self.img_feat_id)] = -100\n\t        output = {\n\t            'mlm_labels': labels,\n\t            'mlm_decoder_input_ids': decoder_input_ids,\n\t            'mlm_decoder_attention_mask': decoder_attention_mask\n\t        }\n\t        return output\n\t    def encode_senti(self, sentis):  #generate label for MSP task\n\t        senti_input_text = [\n", "            self.bos_token + self.senti_token for i in range(len(sentis))\n\t        ]\n\t        senti_input_text = self.encode(senti_input_text,\n\t                                       add_special_tokens=False,\n\t                                       return_tensors='pt',\n\t                                       padding=True)\n\t        senti_decoder_input_ids = senti_input_text['input_ids']\n\t        senti_decoder_attention_mask = senti_input_text['attention_mask']\n\t        sentiment = []\n\t        for senti in sentis:\n", "            sentiment.append(senti)\n\t            # else:\n\t            #     raise ValueError('sentiment label error!!')\n\t        output = {\n\t            'senti_labels': torch.from_numpy(np.array(sentiment)),\n\t            'senti_decoder_input_ids': senti_decoder_input_ids,\n\t            'senti_decoder_attention_mask': senti_decoder_attention_mask\n\t        }\n\t        return output\n\t    def encode_anp_dis(self, batch_size):\n", "        ANP_input_text = [\n\t            self.bos_token + self.ANP_token for i in range(batch_size)\n\t        ]\n\t        ANP_input_text = self.encode(ANP_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output = {}\n\t        output['ANP_decoder_input_ids'] = ANP_input_text['input_ids']\n\t        output['ANP_decoder_attention_mask'] = ANP_input_text['attention_mask']\n", "        return output\n\t    def encode_anp_generate(self, ANP_words):  #generate label for AOG task\n\t        label_split = [x.split() for x in ANP_words]\n\t        label_tokens = []\n\t        for split in label_split:\n\t            word_bpes = [[self.bos_token_id], [self.ANP_generate_token_id]]\n\t            for word in split:\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n", "                word_bpes.append(bpes)\n\t            word_bpes.append([self.eos_token_id])\n\t            _word_bpes = list(chain(*word_bpes))\n\t            label_tokens.append(_word_bpes)\n\t        input_ids, attention_mask = self.pad_tokens(label_tokens)\n\t        output_shape = input_ids[:, 2:].shape\n\t        labels = torch.empty(output_shape, dtype=torch.long)\n\t        decoder_input_ids = torch.empty(input_ids[:, 1:].shape,\n\t                                        dtype=torch.long)\n\t        decoder_attention_mask = torch.empty(input_ids[:, 1:].shape,\n", "                                             dtype=torch.long)\n\t        for i in range(labels.size(0)):\n\t            labels[i] = input_ids[i][\n\t                (input_ids[i] != self.bos_token_id)\n\t                & (input_ids[i] != self.ANP_generate_token_id)]\n\t            decoder_input_ids[i] = input_ids[i][\n\t                input_ids[i] != self.eos_token_id]\n\t            decoder_attention_mask[i] = attention_mask[i][\n\t                input_ids[i] != self.eos_token_id]\n\t        labels[(labels == self.pad_token_id) | (labels == self.begin_img_id) |\n", "               (labels == self.end_img_id) |\n\t               (labels == self.ANP_generate_token_id) |\n\t               (labels == self.img_feat_id)] = -100\n\t        output = {\n\t            'anp_generate_labels': labels,\n\t            'anp_generate_decoder_input_ids': decoder_input_ids,\n\t            'anp_generate_decoder_attention_mask': decoder_attention_mask\n\t        }\n\t        return output\n\t    def encode_aesc(self, label, aesc_spans, aesc_max_len):\n", "        # import ipdb; ipdb.set_trace()\n\t        target_shift = len(self.mapping2targetid) + 2\n\t        aesc_text = []\n\t        masks = []\n\t        gt_spans = []\n\t        flag = True\n\t        for text, span in zip(label, aesc_spans):\n\t            span = sorted(span, key=cmp_to_key(cmp))\n\t            word_bpes = [[self.begin_text_id]]\n\t            for word in text.split():\n", "                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.end_text_id])\n\t            lens = list(map(len, word_bpes))\n\t            cum_lens = np.cumsum(list(lens)).tolist()\n\t            # print(\"self.mapping2targetid is {}\".format(self.mapping2targetid))\n\t            '''\n\t            self.mapping2targetid: {'AESC': 2, 'POS': 3, 'NEU': 4, 'NEG': 5} \n", "            '''\n\t            cur_text = [\n\t                0, self.mapping2targetid['AESC'], self.mapping2targetid['AESC']\n\t            ]\n\t            # print(\"====================cur_text is {}=============================\".format(cur_text))\n\t            mask = [0, 0, 0]\n\t            gt = []\n\t            for x in span:\n\t                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1] - 1] + target_shift\n", "                polarity = self.mapping2targetid[x[2]]\n\t                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                cur_text.append(polarity)\n\t                gt.append((s_bpe, e_bpe, polarity))\n\t                mask.append(1)\n\t                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(1)\n\t            mask.append(1)\n", "            aesc_text.append(cur_text)\n\t            gt_spans.append(gt)\n\t            masks.append(mask)\n\t        span_max_len = max([len(x) for x in aesc_text])\n\t        for i in range(len(masks)):\n\t            add_len = span_max_len - len(masks[i])\n\t            masks[i] = masks[i] + [0 for ss in range(add_len)]\n\t            aesc_text[i] = aesc_text[i] + [1 for ss in range(add_len)]\n\t        output = {}\n\t        output['labels'] = torch.tensor(aesc_text)\n", "        output['masks'] = torch.tensor(masks)\n\t        output['spans'] = gt_spans\n\t        aspect_prompt_input_text = [\n\t            self.bos_token + self.aspect_prompt_token + self.aspect_prompt_token for i in range(len(label))\n\t        ]\n\t        aspect_prompt_input_text = self.encode(aspect_prompt_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['aspect_prompt_decoder_input_ids'] = aspect_prompt_input_text['input_ids']\n", "        output['aspect_prompt_decoder_attention_mask'] = aspect_prompt_input_text['attention_mask']\n\t        senti_prompt_input_text = [\n\t            self.bos_token + self.senti_prompt_token for i in range(len(label))\n\t        ]\n\t        senti_prompt_input_text = self.encode(senti_prompt_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['senti_prompt_decoder_input_ids'] = senti_prompt_input_text['input_ids']\n\t        output['senti_prompt_decoder_attention_mask'] = senti_prompt_input_text['attention_mask']\n", "        # print(\"---------------------output is {}------------------\".format(output))\n\t        # print('++++++++++++++++++++++output[labels] is {}+++++++++++++++++++++++++++++'.format(output['labels']))\n\t        return output\n\t    def encode_ae_oe(self, label, aspect_spans,\n\t                     opinion_spans):  #generate labels of AOE task\n\t        target_shift = len(self.mapping2targetid) + 2\n\t        ae_oe_text = []\n\t        masks = []\n\t        gt_spans = []\n\t        for text, ae_span, oe_span in zip(label, aspect_spans, opinion_spans):\n", "            word_bpes = [[self.begin_text_id]]\n\t            for word in text.split():\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.end_text_id])\n\t            lens = list(map(len, word_bpes))\n\t            cum_lens = np.cumsum(list(lens)).tolist()\n\t            cur_text = [\n", "                0, self.mapping2targetid['AE_OE'],\n\t                self.mapping2targetid['AE_OE']\n\t            ]\n\t            mask = [0, 0, 0]\n\t            gt = []\n\t            for x in ae_span:\n\t                # print(x[0])\n\t                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1]] + target_shift\n\t                cur_text.append(s_bpe)\n", "                cur_text.append(e_bpe)\n\t                gt.append((s_bpe, e_bpe))\n\t                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(self.mapping2targetid['SEP'])\n\t            mask.append(1)\n\t            for x in oe_span:\n\t                # print(x[0])\n\t                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1]] + target_shift\n", "                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                gt.append((s_bpe, e_bpe))\n\t                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(1)\n\t            mask.append(1)\n\t            ae_oe_text.append(cur_text)\n\t            masks.append(mask)\n\t            gt_spans.append(gt)\n", "        span_max_len = max(len(x) for x in ae_oe_text)\n\t        for i in range(len(masks)):\n\t            add_len = span_max_len - len(masks[i])\n\t            masks[i] = masks[i] + [0 for ss in range(add_len)]\n\t            ae_oe_text[i] = ae_oe_text[i] + [1 for ss in range(add_len)]\n\t        output = {}\n\t        output['labels'] = torch.tensor(ae_oe_text)\n\t        output['masks'] = torch.tensor(masks)\n\t        output['spans'] = gt_spans\n\t        return output\n", "    def encode_mrm(self, box_cls):\n\t        mrm_input_text = [\n\t            self.bos_token + self.mrm_token + self.img_feat * 36\n\t            for i in range(len(box_cls))\n\t        ]\n\t        mrm_input_text = self.encode(mrm_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        mrm_decoder_input_ids = mrm_input_text['input_ids']\n", "        mrm_decoder_attention_mask = mrm_input_text['attention_mask']\n\t        output = {\n\t            'mrm_labels': torch.from_numpy(np.array(box_cls)),\n\t            'mrm_decoder_input_ids': mrm_decoder_input_ids,\n\t            'mrm_decoder_attention_mask': mrm_decoder_attention_mask\n\t        }\n\t        return output\n\t    def encode_twitter_ae(self, label, aspect_spans, ae_max_len):\n\t        target_shift = len(self.mapping2targetid) + 2\n\t        ae_text = []\n", "        masks = []\n\t        gt_spans = []\n\t        for text, span in zip(label, aspect_spans):\n\t            word_bpes = [[self.begin_text_id]]\n\t            for word in text.split():\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.end_text_id])\n", "            lens = list(map(len, word_bpes))\n\t            cum_lens = np.cumsum(list(lens)).tolist()\n\t            # self.all_cum_lens.append(cum_lens)\n\t            # print(len(cum_lens), len(split))\n\t            cur_text = [\n\t                0, self.mapping2targetid['AE'], self.mapping2targetid['AE']\n\t            ]\n\t            mask = [0, 0, 0]\n\t            # print(text)\n\t            # print(len(cum_lens), len(text.split()))\n", "            gt = []\n\t            for x in span:\n\t                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1] - 1] + target_shift\n\t                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                gt.append((s_bpe, e_bpe))\n\t                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(1)\n", "            mask.append(1)\n\t            # cur_text = cur_text + [\n\t            #     1 for i in range(ae_max_len - len(cur_text))\n\t            # ]\n\t            # mask = mask + [0 for i in range(ae_max_len - len(mask))]\n\t            # print(cur_text)\n\t            ae_text.append(cur_text)\n\t            masks.append(mask)\n\t            gt_spans.append(gt)\n\t        span_max_len = max(len(x) for x in ae_text)\n", "        for i in range(len(masks)):\n\t            add_len = span_max_len - len(masks[i])\n\t            masks[i] = masks[i] + [0 for ss in range(add_len)]\n\t            ae_text[i] = ae_text[i] + [1 for ss in range(add_len)]\n\t        output = {}\n\t        output['labels'] = torch.tensor(ae_text)\n\t        output['masks'] = torch.tensor(masks)\n\t        output['spans'] = gt_spans\n\t        # output['AE_masks'][:, 2] = 1\n\t        aspect_prompt_input_text = [\n", "            self.bos_token + self.aspect_prompt_token + self.aspect_prompt_token for i in range(len(label))\n\t        ]\n\t        aspect_prompt_input_text = self.encode(aspect_prompt_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['aspect_prompt_decoder_input_ids'] = aspect_prompt_input_text['input_ids']\n\t        output['aspect_prompt_decoder_attention_mask'] = aspect_prompt_input_text['attention_mask']\n\t        return output\n\t    def encode_twitter_sc(self, label, aesc_spans, aesc_max_len):\n", "        target_shift = len(self.mapping2targetid) + 2\n\t        aesc_text = []\n\t        masks = []\n\t        gt_spans = []\n\t        # print(len(opinion_spans))\n\t        # print(len(self.all_cum_lens), len(opinion_spans))\n\t        flag = True\n\t        for text, span in zip(label, aesc_spans):\n\t            span = sorted(span, key=cmp_to_key(cmp))\n\t            word_bpes = [[self.begin_text_id]]\n", "            for word in text.split():\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.end_text_id])\n\t            lens = list(map(len, word_bpes))\n\t            cum_lens = np.cumsum(list(lens)).tolist()\n\t            # if flag:\n\t            #     # print(word_bpes)\n", "            #     print(cum_lens)\n\t            #     flag = False\n\t            cur_text = [\n\t                0, self.mapping2targetid['SC'], self.mapping2targetid['SC']\n\t            ]\n\t            mask = [0, 0, 0]\n\t            # print(text)\n\t            # print(len(cum_lens), len(text.split()))\n\t            gt = []\n\t            for x in span:\n", "                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1] - 1] + target_shift\n\t                # if s_bpe >= cum_lens[-1] or e_bpe >= cum_lens[-1]:\n\t                #     break\n\t                polarity = self.mapping2targetid[x[2]]\n\t                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                cur_text.append(polarity)\n\t                gt.append((s_bpe, e_bpe, polarity))\n\t                mask.append(1)\n", "                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(1)\n\t            mask.append(0)\n\t            # cur_text = cur_text + [\n\t            #     1 for i in range(aesc_max_len - len(cur_text))\n\t            # ]\n\t            # mask = mask + [0 for i in range(aesc_max_len - len(mask))]\n\t            # print(cur_text)\n\t            aesc_text.append(cur_text)\n", "            gt_spans.append(gt)\n\t            masks.append(mask)\n\t        span_max_len = max([len(x) for x in aesc_text])\n\t        for i in range(len(masks)):\n\t            add_len = span_max_len - len(masks[i])\n\t            masks[i] = masks[i] + [0 for ss in range(add_len)]\n\t            aesc_text[i] = aesc_text[i] + [1 for ss in range(add_len)]\n\t            # masks[i].extend([0 for ss in range(add_len)])\n\t            # aesc_text[i].extend([1 for ss in range(add_len)])\n\t        output = {}\n", "        # print(oe_text[0], len(oe_text))\n\t        # for xx in oe_text:\n\t        #     if xx == None:\n\t        #         print('opinion shit!!!!!!!!!!!!!!!')\n\t        # print(aesc_text[0])\n\t        # print(masks[0])\n\t        output['labels'] = torch.tensor(aesc_text)\n\t        output['masks'] = torch.tensor(masks)\n\t        output['spans'] = gt_spans\n\t        senti_prompt_input_text = [\n", "            self.bos_token + self.senti_prompt_token for i in range(len(label))\n\t        ]\n\t        senti_prompt_input_text = self.encode(senti_prompt_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['senti_prompt_decoder_input_ids'] = senti_prompt_input_text['input_ids']\n\t        output['senti_prompt_decoder_attention_mask'] = senti_prompt_input_text['attention_mask']\n\t        return output\n\t    def decode(self, token_ids, skip_special_tokens=False):\n", "        return self._base_tokenizer.decode(\n\t            token_ids, skip_special_tokens=skip_special_tokens)\n\t    def convert_tokens_to_ids(self, tokens):\n\t        return self._base_tokenizer.convert_tokens_to_ids(tokens)\n\t    def convert_ids_to_tokens(self, ids):\n\t        return self._base_tokenizer.convert_ids_to_tokens(ids)\n\t    def get_base_tokenizer(self):\n\t        return self._base_tokenizer\n\t    def __len__(self):\n\t        return len(self._base_tokenizer)"]}
{"filename": "src/data/collation_for_prompt.py", "chunked_list": ["import warnings\n\timport numpy as np\n\timport torch\n\tfrom itertools import chain\n\t# from src.utils import TaskType\n\tclass Collator:\n\t    \"\"\"\n\t    The collator for all types of dataset.\n\t    Remember to add the corresponding collation code after adding a new type of task.\n\t    \"\"\"\n", "    def __init__(self,\n\t                 task,\n\t                 tokenizer,\n\t                 is_mlm=False,\n\t                 has_label=True,\n\t                 mlm_enabled=False,\n\t                 mrm_enabled=False,\n\t                 senti_enabled=False,\n\t                 ae_enabled=False,\n\t                 oe_enabled=False,\n", "                 ae_oe_enabled=False,\n\t                 aesc_enabled=False,\n\t                 anp_enabled=False,\n\t                 anp_generate_enabled=False,\n\t                 twitter_ae_enabled=False,\n\t                 twitter_sc_enabled=False,\n\t                 has_prompt=False,\n\t                 text_only=False,\n\t                 use_caption=False,\n\t                 mlm_probability=0.0,\n", "                 mrm_probability=0.0,\n\t                 lm_max_len=30,\n\t                 max_img_num=2,\n\t                 max_span_len=20):\n\t        \"\"\"\n\t        :param tokenizer: ConditionTokenizer\n\t        :param mlm_enabled: bool, if use mlm for language modeling. False for autoregressive modeling\n\t        :param mrm_enabled: bool, if use mrm\n\t        :param rp_enabled: bool, if use relation prediction (VG)\n\t        :param ap_enabled: bool, if use attribute prediction (VG)\n", "        :param mlm_probability: float, probability to mask the tokens\n\t        :param mrm_probability: float, probability to mask the regions\n\t        \"\"\"\n\t        self.task = task\n\t        self._tokenizer = tokenizer\n\t        self._has_label = has_label\n\t        self._is_mlm = is_mlm\n\t        self._mrm_enabled = mrm_enabled\n\t        self._mlm_enabled = mlm_enabled\n\t        self._senti_enabled = senti_enabled\n", "        self._anp_enabled = anp_enabled\n\t        self._anp_generate_enabled = anp_generate_enabled\n\t        self._ae_enabled = ae_enabled\n\t        self._oe_enabled = oe_enabled\n\t        self._ae_oe_enabled = ae_oe_enabled\n\t        self._aesc_enabled = aesc_enabled\n\t        self._twitter_ae_enabled = twitter_ae_enabled\n\t        self._twitter_sc_enabled = twitter_sc_enabled\n\t        self._lm_max_len = lm_max_len\n\t        self._max_img_num = max_img_num\n", "        self._mlm_probability = mlm_probability\n\t        self._mrm_probability = mrm_probability\n\t        self._max_span_len = max_span_len\n\t        self.text_only = text_only\n\t        self.use_caption=use_caption\n\t        self.has_prompt=has_prompt\n\t        if mlm_enabled and not has_label:\n\t            raise ValueError(\n\t                'mlm_enabled can not be true while has_label is false. MLM need labels.'\n\t            )\n", "    def _clip_text(self, text, length):\n\t        tokenized = []\n\t        for i, word in enumerate(text.split()):\n\t            if i == 0:\n\t                bpes = self._tokenizer._base_tokenizer.tokenize(word)\n\t            else:\n\t                bpes = self._tokenizer._base_tokenizer.tokenize(\n\t                    word, add_prefix_space=True)\n\t            bpes = self._tokenizer._base_tokenizer.convert_tokens_to_ids(bpes)\n\t            tokenized.append(bpes)\n", "        _tokenized = list(chain(*tokenized))\n\t        return self._tokenizer.get_base_tokenizer().decode(_tokenized[:length])\n\t    def __call__(self, batch):\n\t        batch = [entry for entry in batch if entry is not None]\n\t        # image_features = [\n\t        #     torch.from_numpy(x['img_feat'][:self._max_img_num])\n\t        #     if 'img_feat' in x else torch.empty(0) for x in batch\n\t        # ]\n\t        # import ipdb; ipdb.set_trace()\n\t        image_features = [\n", "                        x['image_pixel_values']\n\t                        if 'image_pixel_values' in x else torch.empty(0) for x in batch\n\t        ]\n\t        image_caption = [x['caption'] for x in batch]\n\t        aspects_num = [x['aspects_num'] for x in batch]\n\t        img_num = [self._max_img_num for x in image_features]\n\t        target = [x['sentence'] for x in batch]\n\t        sentence = list(target)\n\t        encoded_conditions = self._tokenizer.encode_condition(\n\t            task=self.task,\n", "            img_num=img_num, use_caption=self.use_caption,\n\t            caption=image_caption, sentence=sentence, has_prompt=self.has_prompt, aspects_num=aspects_num, text_only=self.text_only)\n\t        input_ids = encoded_conditions['input_ids']\n\t        output = {}\n\t        if self._is_mlm:\n\t            input_ids = self._mask_tokens(\n\t                inputs=input_ids,\n\t                input_mask=encoded_conditions['sentence_mask'])\n\t        condition_img_mask = encoded_conditions['img_mask']\n\t        if self._mrm_enabled:\n", "            encode_mrm = self._tokenizer.encode_mrm([x['cls'] for x in batch])\n\t            mrm_labels_all = encode_mrm['mrm_labels']\n\t            probability_matrix = torch.full(input_ids.shape,\n\t                                            self._mrm_probability,\n\t                                            dtype=torch.float)\n\t            masked_regions = torch.bernoulli(probability_matrix).bool()\n\t            input_ids[masked_regions\n\t                      & condition_img_mask] = self._tokenizer.cls_token_id\n\t            decoder_input_ids = encode_mrm['mrm_decoder_input_ids']\n\t            for i in range(input_ids.size(0)):\n", "                for j in range(36):\n\t                    if input_ids[i, j + 1] == self._tokenizer.cls_token_id:\n\t                        decoder_input_ids[i, j +\n\t                                          2] = self._tokenizer.cls_token_id\n\t            mrm_labels = []\n\t            for i in range(len(batch)):\n\t                # create mrm_labels\n\t                masked_indices = masked_regions[i][\n\t                    condition_img_mask[i]].nonzero(as_tuple=False)\n\t                mrm_label = mrm_labels_all[i]\n", "                mrm_labels.append(mrm_label[masked_indices].clone())\n\t                if len(image_features[i]) > 0:\n\t                    image_features[i][masked_indices] = torch.zeros(\n\t                        (len(masked_indices), 1, 2048),\n\t                        dtype=image_features[i].dtype)\n\t            MRM = {}\n\t            MRM['mrm_labels'] = mrm_labels\n\t            MRM['mrm_decoder_input_ids'] = decoder_input_ids\n\t            MRM['mrm_masks'] = decoder_input_ids == self._tokenizer.cls_token_id\n\t            MRM['mrm_decoder_attention_mask'] = encode_mrm[\n", "                'mrm_decoder_attention_mask']\n\t            output['MRM'] = MRM\n\t            output['task'] = 'MRM'\n\t        output['input_ids'] = input_ids\n\t        output['attention_mask'] = encoded_conditions['attention_mask']\n\t        output['image_features'] = image_features\n\t        output['input_ids'] = input_ids\n\t        output['aspects_num']=aspects_num\n\t        if self._has_label:\n\t            # encode mrm and mlm labels\n", "            if self._mlm_enabled:\n\t                mlm_output = self._tokenizer.encode_label(label=target,\n\t                                                          img_num=img_num)\n\t                output['MLM'] = mlm_output\n\t                output['task'] = 'MLM'\n\t            if self._senti_enabled:\n\t                output['Sentiment'] = self._tokenizer.encode_senti(\n\t                    [x['sentiment'] for x in batch])\n\t                output['task'] = 'Sentiment'\n\t            if self._anp_generate_enabled:\n", "                output['ANP_generate'] = self._tokenizer.encode_anp_generate(\n\t                    [x['ANP_words'] for x in batch])\n\t                output['task'] = 'ANP_generate'\n\t            if self._aesc_enabled:\n\t                output['AESC'] = self._tokenizer.encode_aesc(\n\t                    target, [x['aesc_spans'] for x in batch],\n\t                    self._max_span_len)\n\t                output['task'] = 'AESC'\n\t            if self._ae_oe_enabled:\n\t                output['AE_OE'] = self._tokenizer.encode_ae_oe(\n", "                    target, [x['aspect_spans'] for x in batch],\n\t                    [x['opinion_spans'] for x in batch])\n\t                output['task'] = 'AE_OE'\n\t            if self._twitter_ae_enabled:\n\t                output['TWITTER_AE'] = self._tokenizer.encode_twitter_ae(\n\t                    target, [x['aesc_spans'] for x in batch],\n\t                    self._max_span_len)\n\t            if self._twitter_sc_enabled:\n\t                output['TWITTER_SC'] = self._tokenizer.encode_twitter_sc(\n\t                    target, [x['aesc_spans'] for x in batch],\n", "                    self._max_span_len)\n\t        output['image_id'] = [x['image_id'] for x in batch]\n\t        output['gt'] = [x['gt'] for x in batch]\n\t        return output\n\t    def _mask_tokens(self, inputs, input_mask):\n\t        \"\"\"\n\t        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n\t        :param inputs: torch.LongTensor, batch data\n\t        :param input_mask: torch.Tensor, mask for the batch, False for the position with 0% probability to be masked\n\t        \"\"\"\n", "        labels = inputs.clone()\n\t        tokenizer = self._tokenizer.get_base_tokenizer()\n\t        # We sample a few tokens in each sequence for masked-LM training\n\t        probability_matrix = torch.full(labels.shape,\n\t                                        self._mlm_probability,\n\t                                        dtype=torch.float)\n\t        special_tokens_mask = [\n\t            tokenizer.get_special_tokens_mask(val,\n\t                                              already_has_special_tokens=True)\n\t            for val in labels.tolist()\n", "        ]\n\t        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask,\n\t                                                     dtype=torch.bool),\n\t                                        value=0.0)\n\t        if tokenizer.pad_token is not None:\n\t            padding_mask = labels.eq(tokenizer.pad_token_id)\n\t            probability_matrix.masked_fill_(padding_mask, value=0.0)\n\t        masked_indices = torch.bernoulli(probability_matrix).bool()\n\t        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n\t        indices_replaced = torch.bernoulli(torch.full(\n", "            labels.shape, 0.8)).bool() & masked_indices\n\t        inputs[indices_replaced & input_mask] = tokenizer.mask_token_id\n\t        # 10% of the time, we replace masked input tokens with random word\n\t        indices_random = torch.bernoulli(torch.full(\n\t            labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n\t        random_words = torch.randint(tokenizer.vocab_size,\n\t                                     labels.shape,\n\t                                     dtype=torch.long)\n\t        inputs[indices_random & input_mask] = random_words[indices_random\n\t                                                           & input_mask]\n", "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n\t        return inputs"]}
{"filename": "src/data/tokenization_new_for_generated_prompt_multitasks.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom transformers import BartTokenizer, AutoTokenizer\n\tfrom itertools import chain\n\tfrom functools import cmp_to_key\n\t# from src.utils import TaskType\n\tdef cmp(v1, v2):\n\t    if v1[0] == v2[0]:\n\t        return v1[1] - v2[1]\n\t    return v1[0] - v2[0]\n", "class ConditionTokenizer:\n\t    \"\"\"\n\t    tokenizer for image features, event and task type\n\t    this is NOT inherent from transformers Tokenizer\n\t    \"\"\"\n\t    def __init__(self,\n\t                 args,\n\t                 pretrained_model_name='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n\t                 cls_token=\"<<cls>>\",\n\t                 mlm_token=\"<<mlm>>\",\n", "                 mrm_token=\"<<mrm>>\",\n\t                 begin_text=\"<<text>>\",\n\t                 end_text=\"<</text>>\",\n\t                 img_feat='<<img_feat>>',\n\t                 begin_img=\"<<img>>\",\n\t                 end_img=\"<</img>>\",\n\t                 img_caption='<<img_caption>>',\n\t                 begin_caption='<<cap>>',\n\t                 end_caption='<</cap>>',\n\t                 ae_token='<<AE>>',\n", "                 sc_token='<<SC>>',\n\t                 ae_oe_token=\"<<AOE>>\",\n\t                 sep_token=\"<<SEP>>\",\n\t                 aesc_token='<<AESC>>',\n\t                 pos_token='<<POS>>',\n\t                 neu_token='<<NEU>>',\n\t                 neg_token='<<NEG>>',\n\t                 aspect_prompt_token='<<AE_PROMPT>>',\n\t                 senti_prompt_token='<<SENTI_PROMPT>>',\n\t                 begin_prompt='<<prompt>>',\n", "                 end_prompt='<</prompt>>',\n\t                 senti_token='<<senti>>',\n\t                 aspects_num_token='<<aspects_num>>',\n\t                 ANP_token='<<ANP>>',\n\t                 ANP_generate_token='<<AOG>>'):\n\t        self._base_tokenizer = BartTokenizer.from_pretrained(\n\t            pretrained_model_name, )\n\t        # self._base_tokenizer = AutoTokenizer.from_pretrained(\n\t        #     pretrained_model_name)\n\t        self.additional_special_tokens = [\n", "            cls_token, mlm_token, mrm_token, begin_text, end_text, img_feat, begin_img, end_img, \n\t            img_caption, begin_caption, end_caption, aspects_num_token,\n\t            senti_token, ANP_token, ANP_generate_token,\n\t            pos_token, neu_token, neg_token, ae_oe_token, sep_token,\n\t            aesc_token, ae_token, sc_token, \n\t            aspect_prompt_token, senti_prompt_token, begin_prompt, end_prompt\n\t        ]\n\t        unique_no_split_tokens = self._base_tokenizer.unique_no_split_tokens\n\t        self._base_tokenizer.unique_no_split_tokens = unique_no_split_tokens + self.additional_special_tokens\n\t        self.unique_no_split_tokens = self._base_tokenizer.unique_no_split_tokens\n", "        print(self.unique_no_split_tokens)\n\t        self._base_tokenizer.add_tokens(self.additional_special_tokens)\n\t        self.cls_token = cls_token\n\t        self.mlm_token = mlm_token\n\t        self.mrm_token = mrm_token\n\t        self.begin_text = begin_text\n\t        self.end_text = end_text\n\t        self.img_feat = img_feat\n\t        self.begin_img = begin_img\n\t        self.end_img = end_img\n", "        self.img_caption = img_caption\n\t        self.begin_caption = begin_caption\n\t        self.end_caption = end_caption\n\t        self.ae_token = ae_token\n\t        self.sc_token = sc_token\n\t        self.ae_oe_token = ae_oe_token\n\t        self.sep_token = sep_token\n\t        self.senti_token = senti_token\n\t        self.ANP_token = ANP_token\n\t        self.ANP_generate_token = ANP_generate_token\n", "        self.aesc_token = aesc_token\n\t        self.pos_token = pos_token\n\t        self.neu_token = neu_token\n\t        self.neg_token = neg_token\n\t        self.aspect_prompt_token = aspect_prompt_token\n\t        self.senti_prompt_token = senti_prompt_token\n\t        self.begin_prompt = begin_prompt\n\t        self.end_prompt = end_prompt\n\t        self.aspects_num_token = aspects_num_token\n\t        self.cls_token_id = self.convert_tokens_to_ids(cls_token)\n", "        self.mlm_token_id = self.convert_tokens_to_ids(mlm_token)\n\t        self.mrm_token_id = self.convert_tokens_to_ids(mrm_token)\n\t        self.begin_text_id = self.convert_tokens_to_ids(begin_text)\n\t        self.end_text_id = self.convert_tokens_to_ids(end_text)\n\t        self.img_feat_id = self.convert_tokens_to_ids(img_feat)\n\t        self.begin_img_id = self.convert_tokens_to_ids(begin_img)\n\t        self.end_img_id = self.convert_tokens_to_ids(end_img)\n\t        self.img_caption_id = self.convert_tokens_to_ids(img_caption)\n\t        self.begin_caption_id = self.convert_tokens_to_ids(begin_caption)\n\t        self.end_caption_id = self.convert_tokens_to_ids(end_caption)\n", "        self.ae_token_id = self.convert_tokens_to_ids(ae_token)\n\t        self.sc_token_id = self.convert_tokens_to_ids(sc_token)\n\t        self.ae_oe_token_id = self.convert_tokens_to_ids(ae_oe_token)\n\t        self.sep_token_id = self.convert_tokens_to_ids(sep_token)\n\t        self.senti_token_id = self.convert_tokens_to_ids(senti_token)\n\t        self.ANP_token_id = self.convert_tokens_to_ids(ANP_token)\n\t        self.ANP_generate_token_id = self.convert_tokens_to_ids(\n\t            ANP_generate_token)\n\t        self.aesc_token_id = self.convert_tokens_to_ids(aesc_token)\n\t        self.pos_token_id = self.convert_tokens_to_ids(pos_token)\n", "        self.neu_token_id = self.convert_tokens_to_ids(neu_token)\n\t        self.neg_token_id = self.convert_tokens_to_ids(neg_token)\n\t        self.aspect_prompt_token_id = self.convert_tokens_to_ids(aspect_prompt_token)\n\t        self.senti_prompt_token_id = self.convert_tokens_to_ids(senti_prompt_token)\n\t        self.begin_prompt_id =  self.convert_tokens_to_ids(begin_prompt)\n\t        self.end_prompt_id =  self.convert_tokens_to_ids(end_prompt)\n\t        self.aspects_num_token_id = self.convert_tokens_to_ids(aspects_num_token)\n\t        self.vocab_size = self._base_tokenizer.vocab_size\n\t        self.bos_token = self._base_tokenizer.bos_token\n\t        self.bos_token_id = self._base_tokenizer.bos_token_id\n", "        self.eos_token = self._base_tokenizer.eos_token\n\t        self.eos_token_id = self._base_tokenizer.eos_token_id\n\t        self.pad_token = self._base_tokenizer.pad_token\n\t        self.pad_token_id = self._base_tokenizer.pad_token_id\n\t        self.unk_token = self._base_tokenizer.unk_token\n\t        self.unk_token_id = self._base_tokenizer.unk_token_id\n\t        print('self.bos_token_id', self.bos_token_id)\n\t        print('self.eos_token_id', self.eos_token_id)\n\t        print('self.pad_token_id', self.pad_token_id)\n\t        print('self.begin_caption_token_id', self.begin_caption_id)\n", "        print('self.end_caption_token_id', self.end_caption_id)\n\t        print('self.aspect_prompt_token_id', self.aspect_prompt_token_id)\n\t        print('self.senti_prompt_token_id', self.senti_prompt_token_id)\n\t        print('self.begin_prompt_id', self.begin_prompt_id)\n\t        print('self.end_prompt_id', self.end_prompt_id)\n\t        print('self.aspects_num_token_id', self.aspects_num_token_id)\n\t        if args.task == 'pretrain':\n\t            self.mapping = {'AE_OE': '<<AOE>>', 'SEP': '<<SEP>>'}\n\t        else:\n\t            if args.task == 'twitter_sc':\n", "                self.mapping = {\n\t                    'SC': '<<SC>>',\n\t                    'POS': '<<POS>>',\n\t                    'NEU': '<<NEU>>',\n\t                    'NEG': '<<NEG>>'\n\t                }\n\t            elif args.task == 'twitter_ae':\n\t                self.mapping = {\n\t                    'AE': '<<AE>>',\n\t                    'POS': '<<POS>>',\n", "                    'NEU': '<<NEU>>',\n\t                    'NEG': '<<NEG>>'\n\t                }\n\t            else:\n\t                self.mapping = {\n\t                    'AESC': '<<AESC>>',\n\t                    'POS': '<<POS>>',\n\t                    'NEU': '<<NEU>>',\n\t                    'NEG': '<<NEG>>'\n\t                }\n", "        self.senti = {'POS': '<<POS>>', 'NEU': '<<NEU>>', 'NEG': '<<NEG>>'}\n\t        self.senti2id = {}\n\t        for key, value in self.senti.items():\n\t            key_id = self._base_tokenizer.convert_tokens_to_ids(\n\t                self._base_tokenizer.tokenize(value))\n\t            assert len(key_id) == 1, value\n\t            # assert key_id[0] >= self.cur_num_tokens\n\t            self.senti2id[key] = key_id[0]\n\t        self.mapping2id = {}\n\t        self.mapping2targetid = {}\n", "        for key, value in self.mapping.items():\n\t            key_id = self._base_tokenizer.convert_tokens_to_ids(\n\t                self._base_tokenizer.tokenize(value))\n\t            assert len(key_id) == 1, value\n\t            # assert key_id[0] >= self.cur_num_tokens\n\t            self.mapping2id[key] = key_id[0]\n\t            self.mapping2targetid[key] = len(self.mapping2targetid) + 2\n\t        print(self.mapping2id)\n\t        '''\n\t        for AESC:\n", "        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n\t        '''\n\t    def encode(self, *args, **kwargs):\n\t        return self._base_tokenizer(*args, **kwargs)\n\t    def pad_tokens(self, tokens):\n\t        max_len = max([len(x) for x in tokens])\n\t        pad_result = torch.full((len(tokens), max_len),\n\t                                self.pad_token_id,\n\t                                dtype=torch.long)\n\t        mask = torch.zeros(pad_result.size(), dtype=torch.bool)\n", "        for i, x in enumerate(tokens):\n\t            pad_result[i, :len(x)] = torch.tensor(tokens[i], dtype=torch.long)\n\t            mask[i, :len(x)] = True\n\t        return pad_result, mask\n\t    def pad_tokens_with_maxlength(self, tokens, max_len):\n\t        pad_result = torch.full((len(tokens), max_len),\n\t                                self.pad_token_id,\n\t                                dtype=torch.long)\n\t        mask = torch.zeros(pad_result.size(), dtype=torch.bool)\n\t        for i, x in enumerate(tokens):\n", "            # print(x)\n\t            pad_result[i, :len(x)] = torch.tensor(tokens[i], dtype=torch.long)\n\t            mask[i, :len(x)] = True\n\t        # print(\"=====================pad_result=========================\")\n\t        # print(pad_result)\n\t        # print(mask)\n\t        return pad_result, mask\n\t    def encode_mlm_sentence(self, labels):\n\t        label_split = [x.split() for x in labels]\n\t        input_tokens = []\n", "        for split in label_split:\n\t            cur_num = 0\n\t            bpes = [self.bos_token_id]\n\t            for x in split:\n\t                tokens = self._base_tokenizer(x, add_prefix_space=True)\n\t                bpes = bpes + tokens\n\t            bpes.append(self.eos_token_id)\n\t            input_tokens.append(input_tokens)\n\t        return input_tokens\n\t    def encode_condition(self, task, img_num=None, use_caption=True, caption=None, sentence=None, has_prompt=False, max_aspects_num=None,  text_only=False):\n", "        \"\"\"\n\t        tokenize text, image features and event\n\t        the output format (after decoded back):\n\t        task_type [<img> <img_feat> ... <img_feat> </img>] [<event> EVENT </event>] [<mlm> MLM </mlm>]\n\t        :param task_type: str or list[str]\n\t        :param img_num: int or list[int], the number of image features\n\t        :param event: str or list[str], event descriptions\n\t        :param mlm: str or list[str], sentence for masked language modeling\n\t        :return: dict {str: Tensor}, {\n\t                \"input_ids\": ...,\n", "                \"attention_mask\": ...,\n\t                \"event_mask\": ...,          only exist if event is given. 1 for the position with event tokens\n\t                \"mlm_mask\": ...,            only exist if mlm is given. 1 for the position with mlm tokens\n\t                \"img_mask\":...,             only exist if img_num is given. 1 for the position with img tokens\n\t            }\n\t        \"\"\"\n\t        '''\n\t        [image_features] + is + [image_caption] + [text] + [aspect_prompt_token]*len_1(最多为5) + has + [senti_prompt_token]*len_2 'sentiment'\n\t        prompt_1: [image_features] + is + [image_caption]  \n\t        + 'There is' <prompt> ([aspect_prompt_token]*len_1(最多为5) + of [senti_prompt_token]*len_2 + 'sentiment' + <sep>)*n </prompt> + 'in'\n", "        + [text]\n\t        prompt_2: [image_features] + is + [image_caption]  \n\t        + <prompt> ([aspect_prompt_token]*len_1(最多为5) + of [senti_prompt_token]*len_2)*n </prompt> \n\t        + [text]\n\t        prompt_3: [image_features] + is + [image_caption]  \n\t        + 'There is' <prompt> ([aspect_prompt_token]*len_1(最多为5) + of <<NEU>> or <<POS>> or <<NEG>> + 'sentiment' + <sep>)*n </prompt> + 'in'\n\t        + [text]\n\t        '''\n\t        image_text = None\n\t        if img_num is not None:\n", "            if not isinstance(img_num, list):\n\t                img_num = [img_num]\n\t            image_text = []\n\t            for index, value in enumerate(img_num):\n\t                image_text.append(self.begin_img + self.img_feat * value +  ###引入image_caption token\n\t                                  self.end_img)\n\t        # import ipdb; ipdb.set_trace()\n\t        if caption is not None:\n\t            if not isinstance(caption, list):\n\t                caption = [caption]\n", "            caption_split = [x.split() for x in caption]\n\t            image_caption_tokens = []\n\t            for split in caption_split:\n\t                '''\n\t                print(split)\n\t                为方便起见，固定caption文本长度\n\t                '''\n\t                # print(\"+++++++++++++++++++++split before ++++++++++++++++++++++++\")\n\t                # print(len(split))\n\t                if len(split)>10:\n", "                    split = split[:10]\n\t                # print(\"+++++++++++++++++++++split after ++++++++++++++++++++++++\")\n\t                # print(len(split))\n\t                # print(split)\n\t                is_bpes = self._base_tokenizer.tokenize('is',\n\t                                                         add_prefix_space=True)\n\t                is_bpes = self._base_tokenizer.convert_tokens_to_ids(is_bpes) ##[16]\n\t                caption_word_bpes = [is_bpes]\n\t                caption_word_bpes.append([self.begin_caption_id])          \n\t                for caption_word in split:\n", "                    caption_bpes = self._base_tokenizer.tokenize(caption_word,\n\t                                                         add_prefix_space=True)\n\t                    caption_bpes = self._base_tokenizer.convert_tokens_to_ids(caption_bpes)\n\t                    caption_word_bpes.append(caption_bpes)\n\t                caption_word_bpes.append([self.end_caption_id])\n\t                _caption_word_bpes = list(chain(*caption_word_bpes))\n\t                image_caption_tokens.append(_caption_word_bpes.copy())\n\t        if sentence is not None:\n\t            if not isinstance(sentence, list):\n\t                sentence = [sentence]\n", "            sentence_split = [x.split() for x in sentence]\n\t            input_sentence_tokens = []\n\t            for split in sentence_split:\n\t                word_bpes = [[self.bos_token_id]]\n\t                for word in split:\n\t                    bpes = self._base_tokenizer.tokenize(word,\n\t                                                         add_prefix_space=True)\n\t                    bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                    word_bpes.append(bpes)\n\t                word_bpes.append([self.eos_token_id])\n", "                _word_bpes = list(chain(*word_bpes))\n\t                input_sentence_tokens.append(_word_bpes.copy())\n\t        '''\n\t        for prompt_1:  prompt_1: [image_features] + is + [image_caption]  \n\t        + 'There is' <prompt> ([aspect_prompt_token]*len_1(最多为5) + of [senti_prompt_token]*len_2 + 'sentiment' + <sep>)*n </prompt> + 'in'\n\t        + [text] 效果很好 \n\t        in input_ids: input_ids[:][24:64] is the related to aspect prompts, so in generated prompt stage, the attention_mask[:][24:64]=0\n\t        '''\n\t        # if has_prompt:\n\t        #     aspect_prompts_tokens = []     \n", "        #     for index, value in enumerate(aspects_num):\n\t        #         # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n\t        #         #                   self.end_prompt)\n\t        #         aspect_prompt_bpes = [[self.begin_prompt_id]]\n\t        #         if value == 1:\n\t        #             _be = 'is'\n\t        #         else:\n\t        #             _be = 'are'\n\t        #         there_bpes = self._base_tokenizer.tokenize('there',\n\t        #                                                 add_prefix_space=True)\n", "        #         there_bpes = self._base_tokenizer.convert_tokens_to_ids(there_bpes)\n\t        #         aspect_prompt_bpes.append(there_bpes)\n\t        #         _be_bpes = self._base_tokenizer.tokenize(_be,\n\t        #                                                 add_prefix_space=True)\n\t        #         _be_bpes = self._base_tokenizer.convert_tokens_to_ids(_be_bpes)\n\t        #         aspect_prompt_bpes.append(_be_bpes)\n\t        #         for i in range(value):\n\t        #             aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\t        #             of_bpes = self._base_tokenizer.tokenize('of',\n\t        #                                                     add_prefix_space=True)\n", "        #             of_bpes = self._base_tokenizer.convert_tokens_to_ids(of_bpes)\n\t        #             aspect_prompt_bpes.append(of_bpes)\n\t        #             aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\t        #             senti_bpes = self._base_tokenizer.tokenize('sentiment',\n\t        #                                                     add_prefix_space=True)\n\t        #             senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n\t        #             aspect_prompt_bpes.append(senti_bpes)\n\t        #             if i <(value-1):\n\t        #                 aspect_prompt_bpes.append([self.sep_token_id])\n\t        #         aspect_prompt_bpes.append([self.end_prompt_id])\n", "        #         in_bpes =  self._base_tokenizer.tokenize('in',\n\t        #                                                 add_prefix_space=True)\n\t        #         in_bpes = self._base_tokenizer.convert_tokens_to_ids(in_bpes)\n\t        #         aspect_prompt_bpes.append(in_bpes)\n\t        #         _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n\t        #         aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n\t        '''\n\t        for prompt_4:  prompt_1: [image_features] + is + [image_caption]  \n\t        + <prompt> ([aspect_prompt_token]*len_1(最多为5) + of [senti_prompt_token]*len_2)*n </prompt> + 'in'\n\t        + [text] 效果很好 \n", "        in input_ids: input_ids[:][24:64] is the related to aspect prompts, so in generated prompt stage, the attention_mask[:][24:64]=0\n\t        '''\n\t        if has_prompt:\n\t            aspect_prompts_tokens = []     \n\t            for index, value in enumerate(max_aspects_num):\n\t                # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n\t                #                   self.end_prompt)\n\t                aspect_prompt_bpes = [[self.begin_prompt_id]]\n\t                if value == 1:\n\t                    _be = 'is'\n", "                else:\n\t                    _be = 'are'\n\t                there_bpes = self._base_tokenizer.tokenize('there',\n\t                                                        add_prefix_space=True)\n\t                there_bpes = self._base_tokenizer.convert_tokens_to_ids(there_bpes)\n\t                aspect_prompt_bpes.append(there_bpes)\n\t                _be_bpes = self._base_tokenizer.tokenize(_be,\n\t                                                        add_prefix_space=True)\n\t                _be_bpes = self._base_tokenizer.convert_tokens_to_ids(_be_bpes)\n\t                aspect_prompt_bpes.append(_be_bpes)\n", "                for i in range(value):\n\t                    if task == 'AESC':\n\t                        aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\t                        of_bpes = self._base_tokenizer.tokenize('of',\n\t                                                                add_prefix_space=True)\n\t                        of_bpes = self._base_tokenizer.convert_tokens_to_ids(of_bpes)\n\t                        aspect_prompt_bpes.append(of_bpes)\n\t                        aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\t                        senti_bpes = self._base_tokenizer.tokenize('sentiment',\n\t                                                                add_prefix_space=True)\n", "                        senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n\t                        aspect_prompt_bpes.append(senti_bpes)\n\t                        if i <(value-1):\n\t                            aspect_prompt_bpes.append([self.sep_token_id])\n\t                    elif task == 'twitter_sc':\n\t                        aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\t                        senti_bpes = self._base_tokenizer.tokenize('sentiment',\n\t                                                                add_prefix_space=True)\n\t                        senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n\t                        aspect_prompt_bpes.append(senti_bpes)\n", "                        if i <(value-1):\n\t                            aspect_prompt_bpes.append([self.sep_token_id])\n\t                    elif task == 'twitter_ae':\n\t                        aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\t                        if i <(value-1):\n\t                            aspect_prompt_bpes.append([self.sep_token_id])\n\t                    else:\n\t                        print('Not is right task, please check code!!!')\n\t                aspect_prompt_bpes.append([self.end_prompt_id])\n\t                in_bpes =  self._base_tokenizer.tokenize('in',\n", "                                                        add_prefix_space=True)\n\t                in_bpes = self._base_tokenizer.convert_tokens_to_ids(in_bpes)\n\t                aspect_prompt_bpes.append(in_bpes)\n\t                _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n\t                aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n\t        # import ipdb; ipdb.set_trace()\n\t        '''\n\t        for prompt_2\n\t        prompt_2: [image_features] + is + [image_caption]  \n\t        + <prompt> ([aspect_prompt_token]*len_1(最多为5) + [senti_prompt_token]*len_2)*n </prompt> \n", "        + [text] 试一试\n\t        '''\n\t        # if has_prompt:\n\t        #     aspect_prompts_tokens = []     \n\t        #     for index, value in enumerate(aspects_num):\n\t        #         # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n\t        #         #                   self.end_prompt)\n\t        #         aspect_prompt_bpes = [[self.begin_prompt_id]]\n\t        #         for i in range(value):\n\t        #             aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n", "        #             aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\t        #             if i <(value-1):\n\t        #                 aspect_prompt_bpes.append([self.sep_token_id])\n\t        #         aspect_prompt_bpes.append([self.end_prompt_id])\n\t        #         _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n\t        #         aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n\t        # import ipdb; ipdb.set_trace()\n\t        '''\n\t        prompt_3: [image_features] + is + [image_caption]  \n\t        + 'There is' <prompt> ([aspect_prompt_token]*len_1(最多为5) + of <<NEU>> or <<POS>> or <<NEG>> + 'sentiment' + <sep>)*n </prompt> + 'in'\n", "        + [text]\n\t        '''\n\t        # if has_prompt:\n\t        #     aspect_prompts_tokens = []     \n\t        #     for index, value in enumerate(aspects_num):\n\t        #         # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n\t        #         #                   self.end_prompt)\n\t        #         aspect_prompt_bpes = [[self.begin_prompt_id]]\n\t        #         if value == 1:\n\t        #             _be = 'is'\n", "        #         else:\n\t        #             _be = 'are'\n\t        #         there_bpes = self._base_tokenizer.tokenize('there',\n\t        #                                                 add_prefix_space=True)\n\t        #         there_bpes = self._base_tokenizer.convert_tokens_to_ids(there_bpes)\n\t        #         aspect_prompt_bpes.append(there_bpes)\n\t        #         _be_bpes = self._base_tokenizer.tokenize(_be,\n\t        #                                                 add_prefix_space=True)\n\t        #         _be_bpes = self._base_tokenizer.convert_tokens_to_ids(_be_bpes)\n\t        #         aspect_prompt_bpes.append(_be_bpes)\n", "        #         for i in range(value):\n\t        #             aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\t        #             of_bpes = self._base_tokenizer.tokenize('of',\n\t        #                                                     add_prefix_space=True)\n\t        #             of_bpes = self._base_tokenizer.convert_tokens_to_ids(of_bpes)\n\t        #             aspect_prompt_bpes.append(of_bpes)\n\t        #             or_bpes = self._base_tokenizer.tokenize('or',\n\t        #                                                     add_prefix_space=True)\n\t        #             or_bpes = self._base_tokenizer.convert_tokens_to_ids(or_bpes)\n\t        #             aspect_prompt_bpes.append([self.neu_token_id])\n", "        #             aspect_prompt_bpes.append(or_bpes)\n\t        #             aspect_prompt_bpes.append([self.pos_token_id])\n\t        #             aspect_prompt_bpes.append(or_bpes) \n\t        #             aspect_prompt_bpes.append([self.neg_token_id])\n\t        #             senti_bpes = self._base_tokenizer.tokenize('sentiment',\n\t        #                                                     add_prefix_space=True)\n\t        #             senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n\t        #             aspect_prompt_bpes.append(senti_bpes)\n\t        #             if i <(value-1):\n\t        #                 aspect_prompt_bpes.append([self.sep_token_id])\n", "        #         aspect_prompt_bpes.append([self.end_prompt_id])\n\t        #         in_bpes =  self._base_tokenizer.tokenize('in',\n\t        #                                                 add_prefix_space=True)\n\t        #         in_bpes = self._base_tokenizer.convert_tokens_to_ids(in_bpes)\n\t        #         aspect_prompt_bpes.append(in_bpes)\n\t        #         _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n\t        #         aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n\t        if image_text is not None:\n\t            image_sentence = self.encode(image_text,\n\t                                         add_special_tokens=False,\n", "                                         return_tensors='pt',\n\t                                         padding=True)\n\t            image_ids = image_sentence['input_ids']\n\t            image_attention_mask = image_sentence['attention_mask']\n\t            image_caption_tokens, image_caption_mask = self.pad_tokens_with_maxlength(\n\t                image_caption_tokens, max_len=20)\n\t            # print(\"========================image_caption_tokens====================================\")\n\t            # print(image_caption_tokens)\n\t            # print('the length of image_caption_tokens is {}'.format(image_caption_tokens.shape))\n\t            # print(image_caption_mask)\n", "            input_sentence_tokens, input_sentence_mask = self.pad_tokens(\n\t                input_sentence_tokens)\n\t            aspect_prompts_tokens, aspect_prompts_mask = self.pad_tokens_with_maxlength(aspect_prompts_tokens, max_len=40)\n\t            if text_only:\n\t                image_attention_mask = torch.zeros(image_ids.size())\n\t                image_caption_mask = torch.zeros(image_caption_tokens.size())\n\t            if not use_caption:\n\t                image_caption_mask = torch.zeros(image_caption_tokens.size())\n\t                image_attention_mask = image_attention_mask\n\t            input_ids = torch.cat((image_ids, image_caption_tokens, aspect_prompts_tokens, input_sentence_tokens), 1)\n", "            attention_mask = torch.cat(\n\t                (image_attention_mask, image_caption_mask, aspect_prompts_mask, input_sentence_mask), 1)\n\t        else:\n\t            input_sentence_tokens, input_sentence_mask = self.pad_tokens(\n\t                input_sentence_tokens)\n\t            input_ids = input_sentence_tokens\n\t            attention_mask = input_sentence_mask\n\t        # import ipdb; ipdb.set_trace()\n\t        encoded = {}\n\t        encoded['input_ids'] = input_ids\n", "        encoded['attention_mask'] = attention_mask\n\t        # build mlm mask\n\t        if sentence is not None:\n\t            sentence_mask = torch.zeros(input_ids.size(), dtype=torch.bool)\n\t            for index, value in enumerate(input_ids):\n\t                start = (value == self.bos_token_id).nonzero(as_tuple=True)[0]\n\t                end = (value == self.eos_token_id).nonzero(as_tuple=True)[0]\n\t                sentence_mask[index, start + 1:end] = True\n\t            encoded['sentence_mask'] = sentence_mask\n\t        # build img mask\n", "        if img_num is not None:\n\t            encoded['img_mask'] = encoded['input_ids'] == self.img_feat_id\n\t        return encoded\n\t    def encode_label(self, label, img_num=None):  #generate labels for MLM task\n\t        # build text label\n\t        if not isinstance(label, list):\n\t            label = [label]\n\t        label_split = [x.split() for x in label]\n\t        label_tokens = []\n\t        for split in label_split:\n", "            word_bpes = [[self.bos_token_id], [self.mlm_token_id]]\n\t            for word in split:\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.eos_token_id])\n\t            _word_bpes = list(chain(*word_bpes))\n\t            label_tokens.append(_word_bpes)\n\t        input_ids, attention_mask = self.pad_tokens(label_tokens)\n", "        output_shape = input_ids[:, 2:].shape\n\t        labels = torch.empty(output_shape, dtype=torch.long)\n\t        decoder_input_ids = torch.empty(input_ids[:, 1:].shape,\n\t                                        dtype=torch.long)\n\t        decoder_attention_mask = torch.empty(input_ids[:, 1:].shape,\n\t                                             dtype=torch.long)\n\t        for i in range(labels.size(0)):\n\t            labels[i] = input_ids[i][(input_ids[i] != self.bos_token_id)\n\t                                     & (input_ids[i] != self.mlm_token_id)]\n\t            decoder_input_ids[i] = input_ids[i][\n", "                input_ids[i] != self.eos_token_id]\n\t            decoder_attention_mask[i] = attention_mask[i][\n\t                input_ids[i] != self.eos_token_id]\n\t        labels[(labels == self.pad_token_id) | (labels == self.begin_img_id) |\n\t               (labels == self.end_img_id) | (labels == self.mlm_token_id) |\n\t               (labels == self.img_feat_id)] = -100\n\t        output = {\n\t            'mlm_labels': labels,\n\t            'mlm_decoder_input_ids': decoder_input_ids,\n\t            'mlm_decoder_attention_mask': decoder_attention_mask\n", "        }\n\t        return output\n\t    def encode_senti(self, sentis):  #generate label for MSP task\n\t        senti_input_text = [\n\t            self.bos_token + self.senti_token for i in range(len(sentis))\n\t        ]\n\t        senti_input_text = self.encode(senti_input_text,\n\t                                       add_special_tokens=False,\n\t                                       return_tensors='pt',\n\t                                       padding=True)\n", "        senti_decoder_input_ids = senti_input_text['input_ids']\n\t        senti_decoder_attention_mask = senti_input_text['attention_mask']\n\t        sentiment = []\n\t        for senti in sentis:\n\t            sentiment.append(senti)\n\t            # else:\n\t            #     raise ValueError('sentiment label error!!')\n\t        output = {\n\t            'senti_labels': torch.from_numpy(np.array(sentiment)),\n\t            'senti_decoder_input_ids': senti_decoder_input_ids,\n", "            'senti_decoder_attention_mask': senti_decoder_attention_mask\n\t        }\n\t        return output\n\t    def encode_anp_dis(self, batch_size):\n\t        ANP_input_text = [\n\t            self.bos_token + self.ANP_token for i in range(batch_size)\n\t        ]\n\t        ANP_input_text = self.encode(ANP_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n", "                                     padding=True)\n\t        output = {}\n\t        output['ANP_decoder_input_ids'] = ANP_input_text['input_ids']\n\t        output['ANP_decoder_attention_mask'] = ANP_input_text['attention_mask']\n\t        return output\n\t    def encode_anp_generate(self, ANP_words):  #generate label for AOG task\n\t        label_split = [x.split() for x in ANP_words]\n\t        label_tokens = []\n\t        for split in label_split:\n\t            word_bpes = [[self.bos_token_id], [self.ANP_generate_token_id]]\n", "            for word in split:\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.eos_token_id])\n\t            _word_bpes = list(chain(*word_bpes))\n\t            label_tokens.append(_word_bpes)\n\t        input_ids, attention_mask = self.pad_tokens(label_tokens)\n\t        output_shape = input_ids[:, 2:].shape\n", "        labels = torch.empty(output_shape, dtype=torch.long)\n\t        decoder_input_ids = torch.empty(input_ids[:, 1:].shape,\n\t                                        dtype=torch.long)\n\t        decoder_attention_mask = torch.empty(input_ids[:, 1:].shape,\n\t                                             dtype=torch.long)\n\t        for i in range(labels.size(0)):\n\t            labels[i] = input_ids[i][\n\t                (input_ids[i] != self.bos_token_id)\n\t                & (input_ids[i] != self.ANP_generate_token_id)]\n\t            decoder_input_ids[i] = input_ids[i][\n", "                input_ids[i] != self.eos_token_id]\n\t            decoder_attention_mask[i] = attention_mask[i][\n\t                input_ids[i] != self.eos_token_id]\n\t        labels[(labels == self.pad_token_id) | (labels == self.begin_img_id) |\n\t               (labels == self.end_img_id) |\n\t               (labels == self.ANP_generate_token_id) |\n\t               (labels == self.img_feat_id)] = -100\n\t        output = {\n\t            'anp_generate_labels': labels,\n\t            'anp_generate_decoder_input_ids': decoder_input_ids,\n", "            'anp_generate_decoder_attention_mask': decoder_attention_mask\n\t        }\n\t        return output\n\t    def encode_aesc(self, label, aesc_spans, aesc_max_len):\n\t        # import ipdb; ipdb.set_trace()\n\t        target_shift = len(self.mapping2targetid) + 2\n\t        aesc_text = []\n\t        masks = []\n\t        gt_spans = []\n\t        flag = True\n", "        for text, span in zip(label, aesc_spans):\n\t            span = sorted(span, key=cmp_to_key(cmp))\n\t            word_bpes = [[self.begin_text_id]]\n\t            for word in text.split():\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.end_text_id])\n\t            lens = list(map(len, word_bpes))\n", "            cum_lens = np.cumsum(list(lens)).tolist()\n\t            # print(\"self.mapping2targetid is {}\".format(self.mapping2targetid))\n\t            '''\n\t            self.mapping2targetid: {'AESC': 2, 'POS': 3, 'NEU': 4, 'NEG': 5} \n\t            '''\n\t            cur_text = [\n\t                0, self.mapping2targetid['AESC'], self.mapping2targetid['AESC']\n\t            ]\n\t            # print(\"====================cur_text is {}=============================\".format(cur_text))\n\t            mask = [0, 0, 0]\n", "            gt = []\n\t            for x in span:\n\t                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1] - 1] + target_shift\n\t                polarity = self.mapping2targetid[x[2]]\n\t                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                cur_text.append(polarity)\n\t                gt.append((s_bpe, e_bpe, polarity))\n\t                mask.append(1)\n", "                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(1)\n\t            mask.append(1)\n\t            aesc_text.append(cur_text)\n\t            gt_spans.append(gt)\n\t            masks.append(mask)\n\t        span_max_len = max([len(x) for x in aesc_text])\n\t        for i in range(len(masks)):\n\t            add_len = span_max_len - len(masks[i])\n", "            masks[i] = masks[i] + [0 for ss in range(add_len)]\n\t            aesc_text[i] = aesc_text[i] + [1 for ss in range(add_len)]\n\t        output = {}\n\t        output['labels'] = torch.tensor(aesc_text)\n\t        output['masks'] = torch.tensor(masks)\n\t        output['spans'] = gt_spans\n\t        aspect_prompt_input_text = [\n\t            self.bos_token + self.aspect_prompt_token + self.aspect_prompt_token for i in range(len(label))\n\t        ]\n\t        aspect_prompt_input_text = self.encode(aspect_prompt_input_text,\n", "                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['aspect_prompt_decoder_input_ids'] = aspect_prompt_input_text['input_ids']\n\t        output['aspect_prompt_decoder_attention_mask'] = aspect_prompt_input_text['attention_mask']\n\t        senti_prompt_input_text = [\n\t            self.bos_token + self.senti_prompt_token for i in range(len(label))\n\t        ]\n\t        senti_prompt_input_text = self.encode(senti_prompt_input_text,\n\t                                     add_special_tokens=False,\n", "                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['senti_prompt_decoder_input_ids'] = senti_prompt_input_text['input_ids']\n\t        output['senti_prompt_decoder_attention_mask'] = senti_prompt_input_text['attention_mask']\n\t        aspects_num_input_text = [\n\t            self.bos_token + self.aspect_prompt_token for i in range(len(label))\n\t        ]\n\t        aspects_num_input_text = self.encode(aspects_num_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n", "                                     padding=True)\n\t        output['aspects_num_decoder_input_ids'] = aspects_num_input_text['input_ids']\n\t        output['aspects_num_decoder_attention_mask'] = aspects_num_input_text['attention_mask']\n\t        # print(\"---------------------output is {}------------------\".format(output))\n\t        # print('++++++++++++++++++++++output[labels] is {}+++++++++++++++++++++++++++++'.format(output['labels']))\n\t        return output\n\t    def encode_ae_oe(self, label, aspect_spans,\n\t                     opinion_spans):  #generate labels of AOE task\n\t        target_shift = len(self.mapping2targetid) + 2\n\t        ae_oe_text = []\n", "        masks = []\n\t        gt_spans = []\n\t        for text, ae_span, oe_span in zip(label, aspect_spans, opinion_spans):\n\t            word_bpes = [[self.begin_text_id]]\n\t            for word in text.split():\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.end_text_id])\n", "            lens = list(map(len, word_bpes))\n\t            cum_lens = np.cumsum(list(lens)).tolist()\n\t            cur_text = [\n\t                0, self.mapping2targetid['AE_OE'],\n\t                self.mapping2targetid['AE_OE']\n\t            ]\n\t            mask = [0, 0, 0]\n\t            gt = []\n\t            for x in ae_span:\n\t                # print(x[0])\n", "                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1]] + target_shift\n\t                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                gt.append((s_bpe, e_bpe))\n\t                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(self.mapping2targetid['SEP'])\n\t            mask.append(1)\n\t            for x in oe_span:\n", "                # print(x[0])\n\t                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1]] + target_shift\n\t                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                gt.append((s_bpe, e_bpe))\n\t                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(1)\n\t            mask.append(1)\n", "            ae_oe_text.append(cur_text)\n\t            masks.append(mask)\n\t            gt_spans.append(gt)\n\t        span_max_len = max(len(x) for x in ae_oe_text)\n\t        for i in range(len(masks)):\n\t            add_len = span_max_len - len(masks[i])\n\t            masks[i] = masks[i] + [0 for ss in range(add_len)]\n\t            ae_oe_text[i] = ae_oe_text[i] + [1 for ss in range(add_len)]\n\t        output = {}\n\t        output['labels'] = torch.tensor(ae_oe_text)\n", "        output['masks'] = torch.tensor(masks)\n\t        output['spans'] = gt_spans\n\t        return output\n\t    def encode_mrm(self, box_cls):\n\t        mrm_input_text = [\n\t            self.bos_token + self.mrm_token + self.img_feat * 36\n\t            for i in range(len(box_cls))\n\t        ]\n\t        mrm_input_text = self.encode(mrm_input_text,\n\t                                     add_special_tokens=False,\n", "                                     return_tensors='pt',\n\t                                     padding=True)\n\t        mrm_decoder_input_ids = mrm_input_text['input_ids']\n\t        mrm_decoder_attention_mask = mrm_input_text['attention_mask']\n\t        output = {\n\t            'mrm_labels': torch.from_numpy(np.array(box_cls)),\n\t            'mrm_decoder_input_ids': mrm_decoder_input_ids,\n\t            'mrm_decoder_attention_mask': mrm_decoder_attention_mask\n\t        }\n\t        return output\n", "    def encode_twitter_ae(self, label, aspect_spans, ae_max_len):\n\t        target_shift = len(self.mapping2targetid) + 2\n\t        ae_text = []\n\t        masks = []\n\t        gt_spans = []\n\t        for text, span in zip(label, aspect_spans):\n\t            word_bpes = [[self.begin_text_id]]\n\t            for word in text.split():\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n", "                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.end_text_id])\n\t            lens = list(map(len, word_bpes))\n\t            cum_lens = np.cumsum(list(lens)).tolist()\n\t            # self.all_cum_lens.append(cum_lens)\n\t            # print(len(cum_lens), len(split))\n\t            cur_text = [\n\t                0, self.mapping2targetid['AE'], self.mapping2targetid['AE']\n\t            ]\n", "            mask = [0, 0, 0]\n\t            # print(text)\n\t            # print(len(cum_lens), len(text.split()))\n\t            gt = []\n\t            for x in span:\n\t                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1] - 1] + target_shift\n\t                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                gt.append((s_bpe, e_bpe))\n", "                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(1)\n\t            mask.append(1)\n\t            # cur_text = cur_text + [\n\t            #     1 for i in range(ae_max_len - len(cur_text))\n\t            # ]\n\t            # mask = mask + [0 for i in range(ae_max_len - len(mask))]\n\t            # print(cur_text)\n\t            ae_text.append(cur_text)\n", "            masks.append(mask)\n\t            gt_spans.append(gt)\n\t        span_max_len = max(len(x) for x in ae_text)\n\t        for i in range(len(masks)):\n\t            add_len = span_max_len - len(masks[i])\n\t            masks[i] = masks[i] + [0 for ss in range(add_len)]\n\t            ae_text[i] = ae_text[i] + [1 for ss in range(add_len)]\n\t        output = {}\n\t        output['labels'] = torch.tensor(ae_text)\n\t        output['masks'] = torch.tensor(masks)\n", "        output['spans'] = gt_spans\n\t        # output['AE_masks'][:, 2] = 1\n\t        aspect_prompt_input_text = [\n\t            self.bos_token + self.aspect_prompt_token + self.aspect_prompt_token for i in range(len(label))\n\t        ]\n\t        aspect_prompt_input_text = self.encode(aspect_prompt_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['aspect_prompt_decoder_input_ids'] = aspect_prompt_input_text['input_ids']\n", "        output['aspect_prompt_decoder_attention_mask'] = aspect_prompt_input_text['attention_mask']\n\t        aspects_num_input_text = [\n\t            self.bos_token + self.aspect_prompt_token for i in range(len(label))\n\t        ]\n\t        aspects_num_input_text = self.encode(aspects_num_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['aspects_num_decoder_input_ids'] = aspects_num_input_text['input_ids']\n\t        output['aspects_num_decoder_attention_mask'] = aspects_num_input_text['attention_mask']\n", "        return output\n\t    def encode_twitter_sc(self, label, aesc_spans, aesc_max_len):\n\t        target_shift = len(self.mapping2targetid) + 2\n\t        aesc_text = []\n\t        masks = []\n\t        gt_spans = []\n\t        # print(len(opinion_spans))\n\t        # print(len(self.all_cum_lens), len(opinion_spans))\n\t        flag = True\n\t        for text, span in zip(label, aesc_spans):\n", "            span = sorted(span, key=cmp_to_key(cmp))\n\t            word_bpes = [[self.begin_text_id]]\n\t            for word in text.split():\n\t                bpes = self._base_tokenizer.tokenize(word,\n\t                                                     add_prefix_space=True)\n\t                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n\t                word_bpes.append(bpes)\n\t            word_bpes.append([self.end_text_id])\n\t            lens = list(map(len, word_bpes))\n\t            cum_lens = np.cumsum(list(lens)).tolist()\n", "            # if flag:\n\t            #     # print(word_bpes)\n\t            #     print(cum_lens)\n\t            #     flag = False\n\t            cur_text = [\n\t                0, self.mapping2targetid['SC'], self.mapping2targetid['SC']\n\t            ]\n\t            mask = [0, 0, 0]\n\t            # print(text)\n\t            # print(len(cum_lens), len(text.split()))\n", "            gt = []\n\t            for x in span:\n\t                s_bpe = cum_lens[x[0]] + target_shift\n\t                e_bpe = cum_lens[x[1] - 1] + target_shift\n\t                # if s_bpe >= cum_lens[-1] or e_bpe >= cum_lens[-1]:\n\t                #     break\n\t                polarity = self.mapping2targetid[x[2]]\n\t                cur_text.append(s_bpe)\n\t                cur_text.append(e_bpe)\n\t                cur_text.append(polarity)\n", "                gt.append((s_bpe, e_bpe, polarity))\n\t                mask.append(1)\n\t                mask.append(1)\n\t                mask.append(1)\n\t            cur_text.append(1)\n\t            mask.append(0)\n\t            # cur_text = cur_text + [\n\t            #     1 for i in range(aesc_max_len - len(cur_text))\n\t            # ]\n\t            # mask = mask + [0 for i in range(aesc_max_len - len(mask))]\n", "            # print(cur_text)\n\t            aesc_text.append(cur_text)\n\t            gt_spans.append(gt)\n\t            masks.append(mask)\n\t        span_max_len = max([len(x) for x in aesc_text])\n\t        for i in range(len(masks)):\n\t            add_len = span_max_len - len(masks[i])\n\t            masks[i] = masks[i] + [0 for ss in range(add_len)]\n\t            aesc_text[i] = aesc_text[i] + [1 for ss in range(add_len)]\n\t            # masks[i].extend([0 for ss in range(add_len)])\n", "            # aesc_text[i].extend([1 for ss in range(add_len)])\n\t        output = {}\n\t        # print(oe_text[0], len(oe_text))\n\t        # for xx in oe_text:\n\t        #     if xx == None:\n\t        #         print('opinion shit!!!!!!!!!!!!!!!')\n\t        # print(aesc_text[0])\n\t        # print(masks[0])\n\t        output['labels'] = torch.tensor(aesc_text)\n\t        output['masks'] = torch.tensor(masks)\n", "        output['spans'] = gt_spans\n\t        senti_prompt_input_text = [\n\t            self.bos_token + self.senti_prompt_token for i in range(len(label))\n\t        ]\n\t        senti_prompt_input_text = self.encode(senti_prompt_input_text,\n\t                                     add_special_tokens=False,\n\t                                     return_tensors='pt',\n\t                                     padding=True)\n\t        output['senti_prompt_decoder_input_ids'] = senti_prompt_input_text['input_ids']\n\t        output['senti_prompt_decoder_attention_mask'] = senti_prompt_input_text['attention_mask']\n", "        return output\n\t    def decode(self, token_ids, skip_special_tokens=False):\n\t        return self._base_tokenizer.decode(\n\t            token_ids, skip_special_tokens=skip_special_tokens)\n\t    def convert_tokens_to_ids(self, tokens):\n\t        return self._base_tokenizer.convert_tokens_to_ids(tokens)\n\t    def convert_ids_to_tokens(self, ids):\n\t        return self._base_tokenizer.convert_ids_to_tokens(ids)\n\t    def get_base_tokenizer(self):\n\t        return self._base_tokenizer\n", "    def __len__(self):\n\t        return len(self._base_tokenizer)"]}
{"filename": "src/data/collation_for_prompt_multitasks.py", "chunked_list": ["import warnings\n\timport numpy as np\n\timport torch\n\tfrom itertools import chain\n\t# from src.utils import TaskType\n\tclass Collator:\n\t    \"\"\"\n\t    The collator for all types of dataset.\n\t    Remember to add the corresponding collation code after adding a new type of task.\n\t    \"\"\"\n", "    def __init__(self,\n\t                 task,\n\t                 tokenizer,\n\t                 is_mlm=False,\n\t                 has_label=True,\n\t                 mlm_enabled=False,\n\t                 mrm_enabled=False,\n\t                 senti_enabled=False,\n\t                 ae_enabled=False,\n\t                 oe_enabled=False,\n", "                 ae_oe_enabled=False,\n\t                 aesc_enabled=False,\n\t                 anp_enabled=False,\n\t                 anp_generate_enabled=False,\n\t                 twitter_ae_enabled=False,\n\t                 twitter_sc_enabled=False,\n\t                 has_prompt=False,\n\t                 text_only=False,\n\t                 use_caption=False,\n\t                 mlm_probability=0.0,\n", "                 mrm_probability=0.0,\n\t                 lm_max_len=30,\n\t                 max_img_num=2,\n\t                 max_span_len=20):\n\t        \"\"\"\n\t        :param tokenizer: ConditionTokenizer\n\t        :param mlm_enabled: bool, if use mlm for language modeling. False for autoregressive modeling\n\t        :param mrm_enabled: bool, if use mrm\n\t        :param rp_enabled: bool, if use relation prediction (VG)\n\t        :param ap_enabled: bool, if use attribute prediction (VG)\n", "        :param mlm_probability: float, probability to mask the tokens\n\t        :param mrm_probability: float, probability to mask the regions\n\t        \"\"\"\n\t        self.task = task\n\t        self._tokenizer = tokenizer\n\t        self._has_label = has_label\n\t        self._is_mlm = is_mlm\n\t        self._mrm_enabled = mrm_enabled\n\t        self._mlm_enabled = mlm_enabled\n\t        self._senti_enabled = senti_enabled\n", "        self._anp_enabled = anp_enabled\n\t        self._anp_generate_enabled = anp_generate_enabled\n\t        self._ae_enabled = ae_enabled\n\t        self._oe_enabled = oe_enabled\n\t        self._ae_oe_enabled = ae_oe_enabled\n\t        self._aesc_enabled = aesc_enabled\n\t        self._twitter_ae_enabled = twitter_ae_enabled\n\t        self._twitter_sc_enabled = twitter_sc_enabled\n\t        self._lm_max_len = lm_max_len\n\t        self._max_img_num = max_img_num\n", "        self._mlm_probability = mlm_probability\n\t        self._mrm_probability = mrm_probability\n\t        self._max_span_len = max_span_len\n\t        self.text_only = text_only\n\t        self.use_caption = use_caption\n\t        self.has_prompt=has_prompt\n\t        if mlm_enabled and not has_label:\n\t            raise ValueError(\n\t                'mlm_enabled can not be true while has_label is false. MLM need labels.'\n\t            )\n", "    def _clip_text(self, text, length):\n\t        tokenized = []\n\t        for i, word in enumerate(text.split()):\n\t            if i == 0:\n\t                bpes = self._tokenizer._base_tokenizer.tokenize(word)\n\t            else:\n\t                bpes = self._tokenizer._base_tokenizer.tokenize(\n\t                    word, add_prefix_space=True)\n\t            bpes = self._tokenizer._base_tokenizer.convert_tokens_to_ids(bpes)\n\t            tokenized.append(bpes)\n", "        _tokenized = list(chain(*tokenized))\n\t        return self._tokenizer.get_base_tokenizer().decode(_tokenized[:length])\n\t    def __call__(self, batch):\n\t        batch = [entry for entry in batch if entry is not None]\n\t        # image_features = [\n\t        #     torch.from_numpy(x['img_feat'][:self._max_img_num])\n\t        #     if 'img_feat' in x else torch.empty(0) for x in batch\n\t        # ]\n\t        # import ipdb; ipdb.set_trace()\n\t        image_features = [\n", "                        x['image_pixel_values']\n\t                        if 'image_pixel_values' in x else torch.empty(0) for x in batch\n\t        ]\n\t        image_caption = [x['caption'] for x in batch]\n\t        aspects_num = [x['aspects_num']-1 for x in batch]\n\t        max_aspects_num = [5 for x in batch]\n\t        img_num = [self._max_img_num for x in image_features]\n\t        target = [x['sentence'] for x in batch]\n\t        sentence = list(target)\n\t        encoded_conditions = self._tokenizer.encode_condition(\n", "            task=self.task,\n\t            img_num=img_num, use_caption=self.use_caption, caption=image_caption, \n\t            sentence=sentence, has_prompt=self.has_prompt, max_aspects_num=max_aspects_num, text_only=self.text_only)\n\t        input_ids = encoded_conditions['input_ids']\n\t        output = {}\n\t        if self._is_mlm:\n\t            input_ids = self._mask_tokens(\n\t                inputs=input_ids,\n\t                input_mask=encoded_conditions['sentence_mask'])\n\t        condition_img_mask = encoded_conditions['img_mask']\n", "        if self._mrm_enabled:\n\t            encode_mrm = self._tokenizer.encode_mrm([x['cls'] for x in batch])\n\t            mrm_labels_all = encode_mrm['mrm_labels']\n\t            probability_matrix = torch.full(input_ids.shape,\n\t                                            self._mrm_probability,\n\t                                            dtype=torch.float)\n\t            masked_regions = torch.bernoulli(probability_matrix).bool()\n\t            input_ids[masked_regions\n\t                      & condition_img_mask] = self._tokenizer.cls_token_id\n\t            decoder_input_ids = encode_mrm['mrm_decoder_input_ids']\n", "            for i in range(input_ids.size(0)):\n\t                for j in range(36):\n\t                    if input_ids[i, j + 1] == self._tokenizer.cls_token_id:\n\t                        decoder_input_ids[i, j +\n\t                                          2] = self._tokenizer.cls_token_id\n\t            mrm_labels = []\n\t            for i in range(len(batch)):\n\t                # create mrm_labels\n\t                masked_indices = masked_regions[i][\n\t                    condition_img_mask[i]].nonzero(as_tuple=False)\n", "                mrm_label = mrm_labels_all[i]\n\t                mrm_labels.append(mrm_label[masked_indices].clone())\n\t                if len(image_features[i]) > 0:\n\t                    image_features[i][masked_indices] = torch.zeros(\n\t                        (len(masked_indices), 1, 2048),\n\t                        dtype=image_features[i].dtype)\n\t            MRM = {}\n\t            MRM['mrm_labels'] = mrm_labels\n\t            MRM['mrm_decoder_input_ids'] = decoder_input_ids\n\t            MRM['mrm_masks'] = decoder_input_ids == self._tokenizer.cls_token_id\n", "            MRM['mrm_decoder_attention_mask'] = encode_mrm[\n\t                'mrm_decoder_attention_mask']\n\t            output['MRM'] = MRM\n\t            output['task'] = 'MRM'\n\t        output['input_ids'] = input_ids\n\t        output['attention_mask'] = encoded_conditions['attention_mask']\n\t        output['image_features'] = image_features\n\t        output['input_ids'] = input_ids\n\t        output['aspects_num'] = aspects_num\n\t        if self._has_label:\n", "            # encode mrm and mlm labels\n\t            if self._mlm_enabled:\n\t                mlm_output = self._tokenizer.encode_label(label=target,\n\t                                                          img_num=img_num)\n\t                output['MLM'] = mlm_output\n\t                output['task'] = 'MLM'\n\t            if self._senti_enabled:\n\t                output['Sentiment'] = self._tokenizer.encode_senti(\n\t                    [x['sentiment'] for x in batch])\n\t                output['task'] = 'Sentiment'\n", "            if self._anp_generate_enabled:\n\t                output['ANP_generate'] = self._tokenizer.encode_anp_generate(\n\t                    [x['ANP_words'] for x in batch])\n\t                output['task'] = 'ANP_generate'\n\t            if self._aesc_enabled:\n\t                output['AESC'] = self._tokenizer.encode_aesc(\n\t                    target, [x['aesc_spans'] for x in batch],\n\t                    self._max_span_len)\n\t                output['task'] = 'AESC'\n\t            if self._ae_oe_enabled:\n", "                output['AE_OE'] = self._tokenizer.encode_ae_oe(\n\t                    target, [x['aspect_spans'] for x in batch],\n\t                    [x['opinion_spans'] for x in batch])\n\t                output['task'] = 'AE_OE'\n\t            if self._twitter_ae_enabled:\n\t                output['TWITTER_AE'] = self._tokenizer.encode_twitter_ae(\n\t                    target, [x['aesc_spans'] for x in batch],\n\t                    self._max_span_len)\n\t            if self._twitter_sc_enabled:\n\t                output['TWITTER_SC'] = self._tokenizer.encode_twitter_sc(\n", "                    target, [x['aesc_spans'] for x in batch],\n\t                    self._max_span_len)\n\t        output['image_id'] = [x['image_id'] for x in batch]\n\t        output['gt'] = [x['gt'] for x in batch]\n\t        return output\n\t    def _mask_tokens(self, inputs, input_mask):\n\t        \"\"\"\n\t        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n\t        :param inputs: torch.LongTensor, batch data\n\t        :param input_mask: torch.Tensor, mask for the batch, False for the position with 0% probability to be masked\n", "        \"\"\"\n\t        labels = inputs.clone()\n\t        tokenizer = self._tokenizer.get_base_tokenizer()\n\t        # We sample a few tokens in each sequence for masked-LM training\n\t        probability_matrix = torch.full(labels.shape,\n\t                                        self._mlm_probability,\n\t                                        dtype=torch.float)\n\t        special_tokens_mask = [\n\t            tokenizer.get_special_tokens_mask(val,\n\t                                              already_has_special_tokens=True)\n", "            for val in labels.tolist()\n\t        ]\n\t        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask,\n\t                                                     dtype=torch.bool),\n\t                                        value=0.0)\n\t        if tokenizer.pad_token is not None:\n\t            padding_mask = labels.eq(tokenizer.pad_token_id)\n\t            probability_matrix.masked_fill_(padding_mask, value=0.0)\n\t        masked_indices = torch.bernoulli(probability_matrix).bool()\n\t        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n", "        indices_replaced = torch.bernoulli(torch.full(\n\t            labels.shape, 0.8)).bool() & masked_indices\n\t        inputs[indices_replaced & input_mask] = tokenizer.mask_token_id\n\t        # 10% of the time, we replace masked input tokens with random word\n\t        indices_random = torch.bernoulli(torch.full(\n\t            labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n\t        random_words = torch.randint(tokenizer.vocab_size,\n\t                                     labels.shape,\n\t                                     dtype=torch.long)\n\t        inputs[indices_random & input_mask] = random_words[indices_random\n", "                                                           & input_mask]\n\t        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n\t        return inputs"]}
{"filename": "src/data/dataset_for_prompt.py", "chunked_list": ["import torch\n\timport numpy as np\n\timport json\n\timport csv\n\timport os\n\timport json\n\timport torch.utils.data as data\n\tfrom torch.utils.data import DataLoader\n\tfrom transformers import AutoTokenizer\n\tfrom PIL import Image, ImageFile, UnidentifiedImageError\n", "from timm.data.transforms_factory import create_transform\n\tfrom torchvision.transforms import Compose, Lambda\n\tfrom transformers import GPT2Tokenizer, AutoFeatureExtractor, CLIPFeatureExtractor\n\timport logging\n\tlogging.getLogger('PIL').setLevel(logging.WARNING)\n\tTIMM_CONFIGS = {\n\t    'nf_resnet50':  {\n\t        'input_size': (3, 256, 256),\n\t        'interpolation': 'bicubic',\n\t        'mean': (0.485, 0.456, 0.406),\n", "        'std': (0.229, 0.224, 0.225),\n\t        'crop_pct': 0.94,\n\t    },\n\t}\n\tclass MVSA_Dataset(data.Dataset):\n\t    def __init__(self, infos):\n\t        # print(infos)\n\t        infos = json.load(open(infos, 'r'))\n\t        self.text_dir = infos['text_dir']\n\t        self.img_region_dir = infos['img_region_dir']\n", "        self.senti_dir = infos['senti_dir']\n\t        # self.BIO_dir = infos['BIO_dir']\n\t        self.aspect_span_dict = json.load(open(infos['aspect_span_path'], 'r'))\n\t        self.opinion_span_dict = json.load(\n\t            open(infos['opinion_span_path'], 'r'))\n\t        self.ANP_dir = infos['ANP_dir']\n\t        self.ANP_class_dir = infos['ANP_class_dir']\n\t        self.ANP_class = json.load(open(self.ANP_class_dir, 'r'))\n\t        self.ANP_class = {i: anp for i, anp in enumerate(self.ANP_class)}\n\t        self.ANP2idx = {anp: idx for idx, anp in self.ANP_class.items()}\n", "        self.ANP_len = len(self.ANP_class)\n\t        self.id2senti = json.load(open(self.senti_dir, 'r'))\n\t        self.idx2ANP = json.load(open(self.ANP_dir, 'r'))['images']\n\t        self.create_id2idx()\n\t    def __len__(self):\n\t        return len(self.ids)\n\t    def create_id2idx(self):\n\t        ignore = [\n\t            '3151', '3910', '5995'\n\t        ]  #Pictures of these ids can not be opened, so we remove them.\n", "        self.ids = list(sorted(self.id2senti.keys(), key=lambda x: int(x)))\n\t        for x in ignore:\n\t            self.ids.remove(x)\n\t        self.idx2id = {i: id for i, id in enumerate(self.ids)}\n\t    def get_img_region_box(self, id):\n\t        region_feat = np.load(\n\t            os.path.join(self.img_region_dir + '/_att', id + '.npz'))['feat']\n\t        box = np.load(os.path.join(self.img_region_dir + '/_box', id + '.npy'))\n\t        return region_feat, box\n\t    def process_ANP_distribution(self, distribution):\n", "        result = np.empty([1, self.ANP_len], dtype=float)\n\t        for anp, prob in distribution.items():\n\t            result[0, self.ANP2idx[anp]] = prob\n\t        return result\n\t    def get_ANP_word(self, distribution):\n\t        anp_word = list(distribution.items())[0][0].replace('_', ' ')\n\t        # print(anp_word)\n\t        return anp_word\n\t    def get_img_ANP(self, idx):\n\t        distribution = self.idx2ANP[idx]['bi-concepts']\n", "        words = self.get_ANP_word(distribution)\n\t        dis = self.process_ANP_distribution(distribution)\n\t        return dis, words\n\t    def get_sentiment(self, id):\n\t        sentiment = self.id2senti[id]\n\t        return sentiment\n\t    def get_sentence(self, id):\n\t        sentence = open(os.path.join(self.text_dir,\n\t                                     id + '.txt')).read().strip()\n\t        return sentence\n", "    def get_aspect_spans(self, id):\n\t        aspect_spans = self.aspect_span_dict[id]['aspect_spans']\n\t        return aspect_spans\n\t    def get_opinion_spans(self, id):\n\t        opinion_spans = self.opinion_span_dict[id]['opinion_spans']\n\t        return opinion_spans\n\t    def get_cls(self, id):\n\t        cls_prob = np.load(\n\t            os.path.join(self.img_region_dir + '/_cls_again',\n\t                         id + '.npz'))['feat']\n", "        # _cls = np.argmax(cls_prob, axis=-1)\n\t        return cls_prob\n\t    def __getitem__(self, index):\n\t        output = {}\n\t        data_id = self.idx2id[index]\n\t        region_feat, box = self.get_img_region_box(data_id)\n\t        img_feature = region_feat\n\t        output['img_feat'] = img_feature\n\t        sentence = self.get_sentence(data_id)\n\t        output['sentence'] = sentence\n", "        ANP_dis, ANP_words = self.get_img_ANP(index)\n\t        output['ANP_dis'] = ANP_dis\n\t        output['ANP_words'] = ANP_words\n\t        sentiment = self.get_sentiment(data_id)\n\t        output['sentiment'] = sentiment\n\t        aspect_spans = self.get_aspect_spans(data_id)\n\t        output['aspect_spans'] = aspect_spans\n\t        opinion_spans = self.get_opinion_spans(data_id)\n\t        output['opinion_spans'] = opinion_spans\n\t        output['cls'] = self.get_cls(data_id)\n", "        output['image_id'] = data_id\n\t        output['gt'] = None\n\t        return output\n\tclass Twitter_Dataset(data.Dataset):\n\t    def __init__(self, infos, split, image_model_name='nf_resnet50'):\n\t        self.infos = json.load(open(infos, 'r'))\n\t        if split == 'train':\n\t            self.data_set = json.load(\n\t                open(self.infos['data_dir'] + '/train.json', 'r'))\n\t            self.img_region_dir = self.infos['img_region_dir'] + '/train'\n", "        elif split == 'dev':\n\t            self.data_set = json.load(\n\t                open(self.infos['data_dir'] + '/dev.json', 'r'))\n\t            self.img_region_dir = self.infos['img_region_dir'] + '/dev'\n\t        elif split == 'test':\n\t            self.data_set = json.load(\n\t                open(self.infos['data_dir'] + '/test.json', 'r'))\n\t            self.img_region_dir = self.infos['img_region_dir'] + '/test'\n\t        else:\n\t            raise RuntimeError(\"split type is not exist!!!\")\n", "        self.image_model_name = image_model_name\n\t        self.image_transform = self.get_image_transform(self.image_model_name)\n\t    def __len__(self):\n\t        return len(self.data_set)\n\t    def get_img_region_box(self, id):\n\t        region_feat = np.load(\n\t            os.path.join(self.img_region_dir + '/_att',\n\t                         id[:-4] + '.npz'))['feat']\n\t        box = np.load(\n\t            os.path.join(self.img_region_dir + '/_box', id[:-4] + '.npy'))\n", "        return region_feat, box\n\t    def is_clip_model(self, model_name):\n\t        return model_name.startswith('openai/clip-')\n\t    def get_image_transform(self, model_name):\n\t        if model_name in TIMM_CONFIGS.keys():\n\t            config = TIMM_CONFIGS[model_name]\n\t            transform = create_transform(**config)\n\t            transform.transforms.append(\n\t                Lambda(lambda x: x.unsqueeze(0)),\n\t            )\n", "        elif self.is_clip_model(model_name):\n\t            transform = CLIPFeatureExtractor.from_pretrained(model_name)\n\t        else:\n\t            transform = AutoFeatureExtractor.from_pretrained(model_name)\n\t        return transform\n\t    def _read_image(self, image_path):\n\t        raw = Image.open(image_path)\n\t        raw = raw.convert('RGB') if raw.mode != 'RGB' else raw\n\t        if isinstance(self.image_transform, Compose):\n\t            image = self.image_transform(raw)\n", "        elif self.image_transform is not None:  # HuggingFace\n\t            image = self.image_transform(raw, return_tensors='pt')\n\t            image = image['pixel_values']\n\t        return image\n\t    def get_aesc_spans(self, dic):\n\t        aesc_spans = []\n\t        for x in dic:\n\t            aesc_spans.append((x['from'], x['to'], x['polarity']))\n\t        return aesc_spans\n\t    def get_gt_aspect_senti(self, dic):\n", "        gt = []\n\t        for x in dic:\n\t            gt.append((' '.join(x['term']), x['polarity']))\n\t        return gt\n\t    def __getitem__(self, index):\n\t        output = {}\n\t        data = self.data_set[index]\n\t        img_id = data['image_id']\n\t        output['sentence'] = ' '.join(data['words'])\n\t        aesc_spans = self.get_aesc_spans(data['aspects'])\n", "        output['aesc_spans'] = aesc_spans\n\t        gt = self.get_gt_aspect_senti(data['aspects'])\n\t        output['gt'] = gt\n\t        output['image_id'] = img_id\n\t        output['caption'] = data['caption']\n\t        image_path = data['image_path']\n\t        image_pixel_values = self._read_image(image_path)\n\t        output['image_pixel_values'] = image_pixel_values.squeeze()\n\t        output['aspects_num'] = data['aspects_num']\n\t        # import ipdb; ipdb.set_trace()\n", "        # print(\"-----------------------output-------------------------\")\n\t        # print(output)\n\t        return output\n\t# if __name__ == '__main__':\n\t#     dataset = '/home/xiaocui/code/VLP-MABSA/src/data/jsons/twitter15_info.json'\n\t#     dev_dataset = Twitter_Dataset(dataset, split='dev')\n\t#     dev_loader = DataLoader(dataset=dev_dataset,\n\t#                             batch_size=4,\n\t#                             shuffle=False,\n\t#                             num_workers=2,\n", "#                             pin_memory=True,\n\t#                             collate_fn=collate_aesc)"]}
{"filename": "src/model/MAESC_model_for_generated_dual_prompts_multitasks_Aspect.py", "chunked_list": ["from typing import Optional, Tuple\n\tfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\n\tfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\n\tfrom fastNLP.modules.torch import State\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n\t                                     BartDecoder, BartModel,\n\t                                     BartClassificationHead,\n", "                                     _make_linear_from_emb,\n\t                                     _prepare_bart_decoder_inputs)\n\tfrom transformers import BartTokenizer\n\tfrom src.model.config import MultiModalBartConfig\n\t#from src.model.mixins import GenerationMixin, FromPretrainedMixin\n\tfrom src.model.modules_for_prompt_multitasks import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_Dual_prompts, MultiModalBartDecoder_generate_sentiment_prompt, MultiModalBartDecoder_generate_aspect_prompt\n\tfrom src.model.modules_for_prompt_multitasks import MultiModalBartDecoder_aspects_num \n\tclass MultiModalBartModel_AESC(PretrainedBartModel):\n\t    def build_model(self,\n\t                    args,\n", "                    bart_model,\n\t                    tokenizer,\n\t                    label_ids,\n\t                    config,\n\t                    decoder_type=None,\n\t                    copy_gate=False,\n\t                    use_encoder_mlp=False,\n\t                    use_recur_pos=False,\n\t                    tag_first=False):\n\t        if args.bart_init:\n", "            model = BartModel.from_pretrained(bart_model)\n\t            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n\t            print('num_tokens', num_tokens)\n\t            model.resize_token_embeddings(\n\t                len(tokenizer.unique_no_split_tokens) + num_tokens)\n\t            encoder = model.encoder\n\t            decoder = model.decoder\n\t            padding_idx = config.pad_token_id\n\t            encoder.embed_tokens.padding_idx = padding_idx\n\t            # if use_recur_pos:\n", "            #     decoder.set_position_embedding(label_ids[0], tag_first)\n\t            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\t            for token in tokenizer.unique_no_split_tokens:\n\t                if token[:2] == '<<':  # 特殊字符\n\t                    index = tokenizer.convert_tokens_to_ids(\n\t                        tokenizer._base_tokenizer.tokenize(token))\n\t                    if len(index) > 1:\n\t                        raise RuntimeError(f\"{token} wrong split\")\n\t                    else:\n\t                        index = index[0]\n", "                    assert index >= num_tokens, (index, num_tokens, token)\n\t                    indexes = _tokenizer.convert_tokens_to_ids(\n\t                        _tokenizer.tokenize(token[2:-2]))\n\t                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n\t                    for i in indexes[1:]:\n\t                        embed += model.decoder.embed_tokens.weight.data[i]\n\t                    embed /= len(indexes)\n\t                    model.decoder.embed_tokens.weight.data[index] = embed\n\t        else:\n\t            raise RuntimeError(\"error init!!!!!!!\")\n", "        multimodal_encoder_for_generated_aspect_prompt = MultiModalBartEncoder(config, encoder,\n\t                                                   tokenizer.img_feat_id,\n\t                                                   tokenizer.cls_token_id,\n\t                                                   args.num_image_tokens)\n\t        multimodal_encoder_for_generated_aspects_num = MultiModalBartEncoder(config, encoder,\n\t                                                   tokenizer.img_feat_id,\n\t                                                   tokenizer.cls_token_id,\n\t                                                   args.num_image_tokens)\n\t        multimodal_encoder_for_generated_senti_prompt = MultiModalBartEncoder(config, encoder,\n\t                                                   tokenizer.img_feat_id,\n", "                                                   tokenizer.cls_token_id,\n\t                                                   args.num_image_tokens)\n\t        multimodal_encoder = MultiModalBartEncoder_for_Generating_Dual_prompts(\n\t                                                                         use_generated_aspect_prompt=args.use_generated_aspect_prompt, \n\t                                                                         use_generated_senti_prompt=args.use_generated_senti_prompt, \n\t                                                                         config=config, \n\t                                                                         encoder = encoder,\n\t                                                                         img_feat_id = tokenizer.img_feat_id,\n\t                                                                         aspect_prompt_token_id=tokenizer.aspect_prompt_token_id,\n\t                                                                         senti_prompt_token_id=tokenizer.senti_prompt_token_id,\n", "                                                                         cls_token_id = tokenizer.cls_token_id,\n\t                                                                         num_image_tokens = args.num_image_tokens,\n\t                                                                         use_different_aspect_prompt = args.use_different_aspect_prompt, use_different_senti_prompt=args.use_different_senti_prompt,\n\t                                                                         NEU_id = tokenizer.neu_token_id,\n\t                                                                         POS_id = tokenizer.pos_token_id,\n\t                                                                         NEG_id = tokenizer.neg_token_id)\n\t        return (multimodal_encoder_for_generated_aspect_prompt, multimodal_encoder_for_generated_aspects_num, multimodal_encoder_for_generated_senti_prompt, multimodal_encoder, decoder)\n\t    def __init__(self, config: MultiModalBartConfig, args, bart_model,\n\t                 tokenizer, label_ids):\n\t        super().__init__(config)\n", "        self.config = config\n\t        self.tokenizer = tokenizer\n\t        label_ids = sorted(label_ids)\n\t        multimodal_encoder_for_generated_aspect_prompt, multimodal_encoder_for_generated_aspects_num, multimodal_encoder_for_generated_senti_prompt, multimodal_encoder, share_decoder = self.build_model(\n\t            args, bart_model, self.tokenizer, label_ids, config)\n\t        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n\t        self.causal_mask = causal_mask.triu(diagonal=1)\n\t        self.aspect_prompt_encoder = multimodal_encoder_for_generated_aspect_prompt\n\t        self.aspects_num_encoder = multimodal_encoder_for_generated_aspects_num\n\t        self.senti_prompt_encoder = multimodal_encoder_for_generated_senti_prompt\n", "        self.encoder = multimodal_encoder\n\t        self.use_generated_senti_prompt = args.use_generated_senti_prompt\n\t        self.use_multitasks = args.use_multitasks\n\t        self.num_image_tokens = args.num_image_tokens\n\t        self.loss_lambda = args.loss_lambda\n\t        only_sc = False\n\t        # need_tag = True  #if predict the sentiment or not\n\t        if args.task == 'twitter_ae':\n\t            need_tag = False\n\t        else:\n", "            need_tag = True\n\t            # if args.task == 'twitter_sc':\n\t            #     only_sc = True\n\t        self.aspect_prompt_decoder = MultiModalBartDecoder_generate_aspect_prompt(self.config, share_decoder)\n\t        if self.use_generated_senti_prompt:\n\t            self.senti_prompt_decoder = MultiModalBartDecoder_generate_sentiment_prompt(self.config, share_decoder)\n\t        if self.use_multitasks:\n\t            self.aspect_num_decoder = MultiModalBartDecoder_aspects_num(self.config, share_decoder)\n\t        self.decoder = MultiModalBartDecoder_span(self.config,\n\t                                                  self.tokenizer,\n", "                                                  share_decoder,\n\t                                                  self.tokenizer.pad_token_id,\n\t                                                  label_ids,\n\t                                                  self.causal_mask,\n\t                                                  num_image_tokens=self.num_image_tokens,\n\t                                                  need_tag=need_tag,\n\t                                                  only_sc=False)\n\t        self.span_loss_fct = Span_loss()\n\t        # self.aspect_num_linear = nn.Linear(768, 5) ##max_aspects_num=5\n\t    def prepare_state(self,\n", "                      input_ids,\n\t                      image_features,\n\t                      attention_mask=None,\n\t                      aesc_infos=None,\n\t                      aspects_nums=None,\n\t                      first=None):\n\t        ##generate prompt for each instance\n\t        prompt_attention_mask = attention_mask\n\t        # import ipdb; ipdb.set_trace()\n\t        if self.num_image_tokens==4:\n", "            end_index = 66\n\t            begin_index = 26\n\t        elif self.num_image_tokens==3:\n\t            end_index = 65\n\t            begin_index = 25\n\t        elif self.num_image_tokens==2:\n\t            end_index = 64\n\t            begin_index = 24\n\t        elif self.num_image_tokens==1:\n\t            end_index = 63\n", "            begin_index = 23\n\t        elif self.num_image_tokens==0:\n\t            end_index = 62\n\t            begin_index = 22\n\t        for i in range(len(prompt_attention_mask)):\n\t            mask = prompt_attention_mask[i]\n\t            mask[begin_index:end_index]=torch.zeros_like(mask[begin_index:end_index]) ##26:66 是aspect提示的位置\n\t            prompt_attention_mask[i]=mask\n\t        '''dict_for_senti_prompt'''\n\t        dict_for_aspect_prompt = self.aspect_prompt_encoder(input_ids=input_ids,\n", "                                              image_features=image_features,\n\t                                              attention_mask=prompt_attention_mask,\n\t                                              output_hidden_states=True,\n\t                                              return_dict=True)\n\t        dict_for_aspects_num = self.aspects_num_encoder(input_ids=input_ids,\n\t                                              image_features=image_features,\n\t                                              attention_mask=prompt_attention_mask,\n\t                                              output_hidden_states=True,\n\t                                              return_dict=True)\n\t        dict_for_senti_prompt = self.senti_prompt_encoder(input_ids=input_ids,\n", "                                              image_features=image_features,\n\t                                              attention_mask=prompt_attention_mask,\n\t                                              output_hidden_states=True,\n\t                                              return_dict=True)\n\t        '''generated_aspect_prompt'''\n\t        aspect_prompt_decoder_input_ids, aspect_prompt_decoder_attention_mask = [\n\t            aesc_infos['aspect_prompt_decoder_input_ids'].to(input_ids.device),\n\t            aesc_infos['aspect_prompt_decoder_attention_mask'].to(input_ids.device)]\n\t        generated_aspect_prompt = self.aspect_prompt_decoder(\n\t                                            encoder_outputs=dict_for_aspect_prompt.last_hidden_state, \n", "                                            attention_mask=attention_mask,\n\t                                            decoder_input_ids =aspect_prompt_decoder_input_ids, decoder_attention_mask=aspect_prompt_decoder_attention_mask)\n\t        generated_aspect_prompt = generated_aspect_prompt[:, 1:, :] ##(batch_size, 2, 768)\n\t        # import ipdb; ipdb.set_trace()\n\t        '''aspects_num'''\n\t        aspects_num_decoder_input_ids, aspects_num_decoder_attention_mask = [\n\t            aesc_infos['aspects_num_decoder_input_ids'].to(input_ids.device),\n\t            aesc_infos['aspects_num_decoder_attention_mask'].to(input_ids.device)]\n\t        # import ipdb; ipdb.set_trace()\n\t        if self.use_multitasks:\n", "            aspects_num_loss, predict_aspects_num_logits = self.aspect_num_decoder(aspects_num_labels=aspects_nums,\n\t                                                                            encoder_outputs=dict_for_aspects_num[0], \n\t                                                                            attention_mask=attention_mask,\n\t                                                                            aspects_num_decoder_input_ids=aspects_num_decoder_input_ids)\n\t            predict_aspects_num = torch.argmax(predict_aspects_num_logits, dim=1)\n\t            new_predict_aspects_num = predict_aspects_num + torch.ones_like(predict_aspects_num)\n\t        else:\n\t            aspects_num_loss =0\n\t            new_predict_aspects_num = []\n\t            predict_aspects_num = []\n", "            for i in range(len(input_ids)):\n\t                new_predict_aspects_num.append(5)\n\t                predict_aspects_num.append(4)\n\t            new_predict_aspects_num = torch.tensor(new_predict_aspects_num)\n\t            predict_aspects_num = torch.tensor(predict_aspects_num)\n\t        generated_senti_prompt = None\n\t        if self.use_generated_senti_prompt:\n\t            senti_prompt_decoder_input_ids, senti_prompt_decoder_attention_mask = [\n\t                aesc_infos['senti_prompt_decoder_input_ids'].to(input_ids.device),\n\t                aesc_infos['senti_prompt_decoder_attention_mask'].to(input_ids.device)]\n", "            generated_senti_prompt = self.senti_prompt_decoder(\n\t                                                encoder_outputs=dict_for_senti_prompt.last_hidden_state, \n\t                                                attention_mask=attention_mask,\n\t                                                decoder_input_ids =senti_prompt_decoder_input_ids, decoder_attention_mask=senti_prompt_decoder_attention_mask)\n\t            generated_senti_prompt = generated_senti_prompt[:, 1:, :] ##(batch_size, 2, 768)\n\t        dict = self.encoder(\n\t                            input_ids=input_ids,\n\t                            image_features=image_features,\n\t                            attention_mask=attention_mask,\n\t                            generated_aspect_prompt= generated_aspect_prompt,\n", "                            generated_senti_prompt=generated_senti_prompt,\n\t                            aspects_num = new_predict_aspects_num,\n\t                            output_hidden_states=True,\n\t                            return_dict=True)\n\t        encoder_outputs = dict.last_hidden_state\n\t        hidden_states = dict.hidden_states\n\t        encoder_mask = attention_mask\n\t        src_embed_outputs = hidden_states[0]\n\t        state = BartState(\n\t            encoder_outputs,\n", "            encoder_mask,\n\t            input_ids[:,\n\t                      end_index:],  #the text features start from index 38, the front are image features.\n\t            first,\n\t            src_embed_outputs)\n\t        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n\t        return state, aspects_num_loss, predict_aspects_num\n\t    def forward(\n\t            self,\n\t            input_ids,\n", "            image_features,\n\t            attention_mask=None,\n\t            aesc_infos=None,\n\t            aspects_num=None,\n\t            encoder_outputs: Optional[Tuple] = None,\n\t            use_cache=None,\n\t            output_attentions=None,\n\t            output_hidden_states=None,\n\t    ):\n\t        ### for prompt\n", "        # import ipdb; ipdb.set_trace()\n\t        ## for aspect-spans\n\t        aspects_num = torch.tensor(aspects_num).to(input_ids.device)\n\t        state, aspects_num_loss, predict_aspects_num = self.prepare_state( input_ids, image_features, attention_mask, aesc_infos, aspects_num)\n\t        spans, span_mask = [ \n\t            aesc_infos['labels'].to(input_ids.device),\n\t            aesc_infos['masks'].to(input_ids.device)\n\t        ]\n\t        logits = self.decoder(spans, state) ## spans: (2, 13) logits: (2, 12, 40)\n\t        span_loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n", "        all_loss = span_loss + self.loss_lambda*aspects_num_loss\n\t        return all_loss, predict_aspects_num\n\tclass BartState(State):\n\t    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n\t                 src_embed_outputs):\n\t        super().__init__(encoder_output, encoder_mask)\n\t        self.past_key_values = None\n\t        self.src_tokens = src_tokens\n\t        self.first = first\n\t        self.src_embed_outputs = src_embed_outputs\n", "    def reorder_state(self, indices: torch.LongTensor):\n\t        super().reorder_state(indices)\n\t        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n\t        if self.first is not None:\n\t            self.first = self._reorder_state(self.first, indices)\n\t        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n\t                                                     indices)\n\t        if self.past_key_values is not None:\n\t            new = []\n\t            for layer in self.past_key_values:\n", "                new_layer = {}\n\t                for key1 in list(layer.keys()):\n\t                    new_layer_ = {}\n\t                    for key2 in list(layer[key1].keys()):\n\t                        if layer[key1][key2] is not None:\n\t                            layer[key1][key2] = self._reorder_state(\n\t                                layer[key1][key2], indices)\n\t                            # print(key1, key2, layer[key1][key2].shape)\n\t                        new_layer_[key2] = layer[key1][key2]\n\t                    new_layer[key1] = new_layer_\n", "                new.append(new_layer)\n\t            self.past_key_values = new\n"]}
{"filename": "src/model/generater_for_generated_prompt.py", "chunked_list": ["r\"\"\"undocumented\"\"\"\n\timport torch\n\tfrom torch import nn\n\tfrom fastNLP.models.torch.seq2seq_model import Seq2SeqModel\n\tfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\n\timport torch.nn.functional as F\n\t# from fastNLP.core.utils import _get_model_device\n\tfrom functools import partial\n\tdef _get_model_device(model):\n\t    r\"\"\"\n", "    传入一个nn.Module的模型，获取它所在的device\n\t    :param model: nn.Module\n\t    :return: torch.device,None 如果返回值为None，说明这个模型没有任何参数。\n\t    \"\"\"\n\t    # TODO 这个函数存在一定的风险，因为同一个模型可能存在某些parameter不在显卡中，比如BertEmbedding. 或者跨显卡\n\t    assert isinstance(model, nn.Module)\n\t    parameters = list(model.parameters())\n\t    if len(parameters) == 0:\n\t        return None\n\t    else:\n", "        return parameters[0].device\n\tclass SequenceGeneratorModel(nn.Module):\n\t    \"\"\"\n\t    用于封装Seq2SeqModel使其可以做生成任务\n\t    \"\"\"\n\t    def __init__(self,\n\t                 seq2seq_model: Seq2SeqModel,\n\t                 bos_token_id,\n\t                 eos_token_id=None,\n\t                 max_length=30,\n", "                 max_len_a=0.0,\n\t                 num_beams=1,\n\t                 do_sample=True,\n\t                 sc_only=False,\n\t                 repetition_penalty=1,\n\t                 length_penalty=1.0,\n\t                 pad_token_id=0,\n\t                 restricter=None):\n\t        \"\"\"\n\t        :param Seq2SeqModel seq2seq_model: 序列到序列模型. 会使用seq2seq_model的decoder进行生成\n", "        :param int,None bos_token_id: 句子开头的token id\n\t        :param int,None eos_token_id: 句子结束的token id\n\t        :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n\t        :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n\t        :param int num_beams: beam search的大小\n\t        :param bool do_sample: 是否通过采样的方式生成\n\t        :param float temperature: 只有在do_sample为True才有意义\n\t        :param int top_k: 只从top_k中采样\n\t        :param float top_p: 只从top_p的token中采样，nucles sample\n\t        :param float repetition_penalty: 多大程度上惩罚重复的token\n", "        :param float length_penalty: 对长度的惩罚，小于1鼓励长句，大于1鼓励短剧\n\t        :param int pad_token_id: 当某句话生成结束之后，之后生成的内容用pad_token_id补充\n\t        \"\"\"\n\t        super().__init__()\n\t        self.seq2seq_model = seq2seq_model\n\t        self.restricter = restricter\n\t        self.sc_only = sc_only\n\t        self.generator = SequenceGenerator(\n\t            seq2seq_model.decoder,\n\t            max_length=max_length,\n", "            max_len_a=max_len_a,\n\t            num_beams=num_beams,\n\t            do_sample=do_sample,\n\t            sc_only=sc_only,\n\t            bos_token_id=bos_token_id,\n\t            eos_token_id=eos_token_id,\n\t            repetition_penalty=repetition_penalty,\n\t            length_penalty=length_penalty,\n\t            pad_token_id=pad_token_id,\n\t            restricter=restricter)\n", "    def forward(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                aesc_infos=None,\n\t                aspects_num=None,\n\t                first=None):\n\t        \"\"\"\n\t        透传调用seq2seq_model的forward\n\t        :param torch.LongTensor src_tokens: bsz x max_len\n", "        :param torch.LongTensor tgt_tokens: bsz x max_len'\n\t        :param torch.LongTensor src_seq_len: bsz\n\t        :param torch.LongTensor tgt_seq_len: bsz\n\t        :return:\n\t        \"\"\"\n\t        return self.seq2seq_model(input_ids=input_ids,\n\t                                  image_features=image_features,\n\t                                  attention_mask=attention_mask,\n\t                                  aesc_infos=aesc_infos,\n\t                                  aspects_num=aspects_num)\n", "    def predict(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                aesc_infos=None,\n\t                aspects_num=None):\n\t        \"\"\"\n\t        给定source的内容，输出generate的内容\n\t        :param torch.LongTensor src_tokens: bsz x max_len\n\t        :param torch.LongTensor src_seq_len: bsz\n", "        :return:\n\t        \"\"\"\n\t        # import ipdb; ipdb.set_trace()\n\t        state  = self.seq2seq_model.prepare_state(\n\t                                                 input_ids, image_features,\n\t                                                 attention_mask,\n\t                                                 aesc_infos,\n\t                                                 aspects_num)\n\t        tgt_tokens = aesc_infos['labels'].to(input_ids.device)\n\t        # print()\n", "        result = self.generator.generate(\n\t            state,\n\t            tokens=tgt_tokens[:, :3])  # the prompt is provided to the model\n\t        return result\n\tr\"\"\"\n\t\"\"\"\n\t__all__ = ['SequenceGenerator']\n\tclass SequenceGenerator:\n\t    \"\"\"\n\t    给定一个Seq2SeqDecoder，decode出句子\n", "    \"\"\"\n\t    def __init__(self,\n\t                 decoder: Seq2SeqDecoder,\n\t                 max_length=20,\n\t                 max_len_a=0.0,\n\t                 num_beams=1,\n\t                 do_sample=False,\n\t                 sc_only=False,\n\t                 bos_token_id=None,\n\t                 eos_token_id=None,\n", "                 repetition_penalty=1,\n\t                 length_penalty=1.0,\n\t                 pad_token_id=0,\n\t                 restricter=None):\n\t        \"\"\"\n\t        :param Seq2SeqDecoder decoder: Decoder对象\n\t        :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n\t        :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n\t        :param int num_beams: beam search的大小\n\t        :param bool do_sample: 是否通过采样的方式生成\n", "        :param float temperature: 只有在do_sample为True才有意义\n\t        :param int top_k: 只从top_k中采样\n\t        :param float top_p: 只从top_p的token中采样，nucles sample\n\t        :param int,None bos_token_id: 句子开头的token id\n\t        :param int,None eos_token_id: 句子结束的token id\n\t        :param float repetition_penalty: 多大程度上惩罚重复的token\n\t        :param float length_penalty: 对长度的惩罚，小于1鼓励长句，大于1鼓励短剧\n\t        :param int pad_token_id: 当某句话生成结束之后，之后生成的内容用pad_token_id补充\n\t        \"\"\"\n\t        self.generate_func = partial(greedy_generate,\n", "                                     decoder=decoder,\n\t                                     max_length=max_length,\n\t                                     max_len_a=max_len_a,\n\t                                     num_beams=num_beams,\n\t                                     sc_only=sc_only,\n\t                                     bos_token_id=bos_token_id,\n\t                                     eos_token_id=eos_token_id,\n\t                                     repetition_penalty=repetition_penalty,\n\t                                     length_penalty=length_penalty,\n\t                                     pad_token_id=pad_token_id,\n", "                                     restricter=restricter)\n\t        self.do_sample = do_sample\n\t        self.max_length = max_length\n\t        self.num_beams = num_beams\n\t        self.bos_token_id = bos_token_id\n\t        self.eos_token_id = eos_token_id\n\t        self.repetition_penalty = repetition_penalty\n\t        self.length_penalty = length_penalty\n\t        self.decoder = decoder\n\t        self.pad_token_id = pad_token_id\n", "        self.restricter = restricter\n\t        self.max_len_a = max_len_a\n\t    def set_new_generator(self,\n\t                          max_length=-1,\n\t                          max_len_a=-1,\n\t                          num_beams=-1,\n\t                          repetition_penalty=-1,\n\t                          length_penalty=-1,\n\t                          restricter=-1):\n\t        if max_length == -1:\n", "            max_length = self.max_length\n\t        if max_len_a == -1:\n\t            max_len_a = self.max_len_a\n\t        if num_beams == -1:\n\t            num_beams = self.num_beams\n\t        if repetition_penalty == -1:\n\t            repetition_penalty = self.repetition_penalty\n\t        if length_penalty == -1:\n\t            length_penalty = self.length_penalty\n\t        if restricter == -1:\n", "            restricter = self.restricter\n\t        self.generate_func = partial(greedy_generate,\n\t                                     decoder=self.decoder,\n\t                                     max_length=max_length,\n\t                                     max_len_a=max_len_a,\n\t                                     num_beams=num_beams,\n\t                                     sc_only=sc_only,\n\t                                     bos_token_id=self.bos_token_id,\n\t                                     eos_token_id=self.eos_token_id,\n\t                                     repetition_penalty=repetition_penalty,\n", "                                     length_penalty=length_penalty,\n\t                                     pad_token_id=self.pad_token_id,\n\t                                     restricter=restricter)\n\t    @torch.no_grad()\n\t    def generate(self, state, tokens=None, gt_tokens=None):\n\t        \"\"\"\n\t        :param State state: encoder结果的State, 是与Decoder配套是用的\n\t        :param torch.LongTensor,None tokens: batch_size x length, 开始的token\n\t        :return: bsz x max_length' 生成的token序列。如果eos_token_id不为None, 每个sequence的结尾一定是eos_token_id\n\t        \"\"\"\n", "        return self.generate_func(tokens=tokens,\n\t                                  gt_tokens=gt_tokens,\n\t                                  state=state)\n\t@torch.no_grad()\n\tdef greedy_generate(decoder,\n\t                    tokens=None,\n\t                    gt_tokens=None,\n\t                    state=None,\n\t                    sc_eval=False,\n\t                    max_length=20,\n", "                    max_len_a=0.0,\n\t                    num_beams=1,\n\t                    sc_only=False,\n\t                    bos_token_id=None,\n\t                    eos_token_id=None,\n\t                    pad_token_id=0,\n\t                    repetition_penalty=1,\n\t                    length_penalty=1.0,\n\t                    restricter=None):\n\t    \"\"\"\n", "    贪婪地搜索句子\n\t    :param Decoder decoder: Decoder对象\n\t    :param torch.LongTensor tokens: batch_size x len, decode的输入值，如果为None，则自动从bos_token_id开始生成\n\t    :param State state: 应该包含encoder的一些输出。\n\t    :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n\t    :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n\t    :param int num_beams: 使用多大的beam进行解码。\n\t    :param int bos_token_id: 如果tokens传入为None，则使用bos_token_id开始往后解码。\n\t    :param int eos_token_id: 结束的token，如果为None，则一定会解码到max_length这么长。\n\t    :param int pad_token_id: pad的token id\n", "    :param float repetition_penalty: 对重复出现的token多大的惩罚。\n\t    :param float length_penalty: 对每个token（除了eos）按照长度进行一定的惩罚。\n\t    :return:\n\t    \"\"\"\n\t    # import ipdb; ipdb.set_trace()\n\t    if sc_only:\n\t        token_ids = sc_generate(decoder,\n\t                                tokens=tokens,\n\t                                gt_tokens=gt_tokens,\n\t                                state=state,\n", "                                max_length=max_length,\n\t                                max_len_a=max_len_a,\n\t                                bos_token_id=bos_token_id,\n\t                                eos_token_id=eos_token_id,\n\t                                repetition_penalty=repetition_penalty,\n\t                                length_penalty=length_penalty,\n\t                                pad_token_id=pad_token_id,\n\t                                restricter=restricter)\n\t        return token_ids\n\t    if num_beams == 1:\n", "        token_ids = _no_beam_search_generate(\n\t            decoder,\n\t            tokens=tokens,\n\t            state=state,\n\t            max_length=max_length,\n\t            max_len_a=max_len_a,\n\t            bos_token_id=bos_token_id,\n\t            eos_token_id=eos_token_id,\n\t            repetition_penalty=repetition_penalty,\n\t            length_penalty=length_penalty,\n", "            pad_token_id=pad_token_id,\n\t            restricter=restricter)\n\t    else:\n\t        token_ids = _beam_search_generate(\n\t            decoder,\n\t            tokens=tokens,\n\t            state=state,\n\t            max_length=max_length,\n\t            max_len_a=max_len_a,\n\t            num_beams=num_beams,\n", "            bos_token_id=bos_token_id,\n\t            eos_token_id=eos_token_id,\n\t            do_sample=False,\n\t            repetition_penalty=repetition_penalty,\n\t            length_penalty=length_penalty,\n\t            pad_token_id=pad_token_id,\n\t            restricter=restricter)\n\t    return token_ids\n\tdef _no_beam_search_generate(decoder: Seq2SeqDecoder,\n\t                             state,\n", "                             tokens=None,\n\t                             max_length=20,\n\t                             max_len_a=0.0,\n\t                             bos_token_id=None,\n\t                             eos_token_id=None,\n\t                             repetition_penalty=1.0,\n\t                             length_penalty=1.0,\n\t                             pad_token_id=0,\n\t                             restricter=None):\n\t    device = _get_model_device(decoder)\n", "    if tokens is None:\n\t        if bos_token_id is None:\n\t            raise RuntimeError(\n\t                \"You have to specify either `tokens` or `bos_token_id`.\")\n\t        batch_size = state.num_samples\n\t        if batch_size is None:\n\t            raise RuntimeError(\n\t                \"Cannot infer the number of samples from `state`.\")\n\t        tokens = torch.full([batch_size, 1],\n\t                            fill_value=bos_token_id,\n", "                            dtype=torch.long).to(device)\n\t    batch_size = tokens.size(0)\n\t    if state.num_samples:\n\t        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\t    if eos_token_id is None:\n\t        _eos_token_id = -1\n\t    else:\n\t        _eos_token_id = eos_token_id\n\t    scores = decoder.decode(tokens=tokens, state=state)  # 主要是为了update state\n\t    # 这里需要考虑如果在第一个位置就结束的情况\n", "    # if _eos_token_id!=-1:\n\t    #     scores[:, _eos_token_id] = -1e12\n\t    if restricter is not None:\n\t        _, next_tokens = restricter(state, tokens, scores, num_beams=1)\n\t    else:\n\t        next_tokens = scores.argmax(dim=-1, keepdim=True)\n\t    token_ids = torch.cat([tokens, next_tokens], dim=1)\n\t    cur_len = token_ids.size(1)\n\t    dones = token_ids.new_zeros(batch_size).eq(1).__or__(\n\t        next_tokens.squeeze(1).eq(eos_token_id))\n", "    # tokens = tokens[:, -1:]\n\t    if max_len_a != 0:\n\t        # (bsz x num_beams, )\n\t        if state.encoder_mask is not None:\n\t            max_lengths = (state.encoder_mask.sum(dim=1).float() *\n\t                           max_len_a).long() + max_length\n\t        else:\n\t            max_lengths = tokens.new_full((tokens.size(0), ),\n\t                                          fill_value=max_length,\n\t                                          dtype=torch.long)\n", "        real_max_length = max_lengths.max().item()\n\t    else:\n\t        real_max_length = max_length\n\t        if state.encoder_mask is not None:\n\t            max_lengths = state.encoder_mask.new_ones(\n\t                state.encoder_mask.size(0)).long() * max_length\n\t        else:\n\t            max_lengths = tokens.new_full((tokens.size(0), ),\n\t                                          fill_value=max_length,\n\t                                          dtype=torch.long)\n", "    while cur_len < real_max_length:\n\t        scores = decoder.decode(tokens=token_ids,\n\t                                state=state)  # batch_size x vocab_size\n\t        if repetition_penalty != 1.0:\n\t            token_scores = scores.gather(dim=1, index=token_ids)\n\t            lt_zero_mask = token_scores.lt(0).float()\n\t            ge_zero_mask = lt_zero_mask.eq(0).float()\n\t            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n\t            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n\t        if eos_token_id is not None and length_penalty != 1.0:\n", "            token_scores = scores / cur_len**length_penalty  # batch_size x vocab_size\n\t            eos_mask = scores.new_ones(scores.size(1))\n\t            eos_mask[eos_token_id] = 0\n\t            eos_mask = eos_mask.unsqueeze(0).eq(1)\n\t            scores = scores.masked_scatter(\n\t                eos_mask, token_scores)  # 也即除了eos，其他词的分数经过了放大/缩小\n\t        if restricter is not None:\n\t            _, next_tokens = restricter(state, token_ids, scores, 1)\n\t        else:\n\t            next_tokens = scores.argmax(dim=-1, keepdim=True)\n", "        next_tokens = next_tokens.squeeze(-1)\n\t        # 如果已经达到对应的sequence长度了，就直接填为eos了\n\t        if _eos_token_id != -1:\n\t            next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len + 1),\n\t                                                  _eos_token_id)\n\t        next_tokens = next_tokens.masked_fill(\n\t            dones, pad_token_id)  # 对已经搜索完成的sample做padding\n\t        tokens = next_tokens.unsqueeze(1)\n\t        token_ids = torch.cat([token_ids, tokens],\n\t                              dim=-1)  # batch_size x max_len\n", "        end_mask = next_tokens.eq(_eos_token_id)\n\t        dones = dones.__or__(end_mask)\n\t        cur_len += 1\n\t        if dones.min() == 1:\n\t            break\n\t    return token_ids\n\tdef sc_generate(decoder: Seq2SeqDecoder,\n\t                state,\n\t                tokens=None,\n\t                gt_tokens=None,\n", "                max_length=20,\n\t                max_len_a=0.0,\n\t                bos_token_id=None,\n\t                eos_token_id=None,\n\t                repetition_penalty=1.0,\n\t                length_penalty=1.0,\n\t                pad_token_id=0,\n\t                restricter=None):\n\t    device = _get_model_device(decoder)\n\t    if tokens is None:\n", "        if bos_token_id is None:\n\t            raise RuntimeError(\n\t                \"You have to specify either `tokens` or `bos_token_id`.\")\n\t        batch_size = state.num_samples\n\t        if batch_size is None:\n\t            raise RuntimeError(\n\t                \"Cannot infer the number of samples from `state`.\")\n\t        tokens = torch.full([batch_size, 1],\n\t                            fill_value=bos_token_id,\n\t                            dtype=torch.long).to(device)\n", "    batch_size = tokens.size(0)\n\t    # print(state.num_samples, batch_size)\n\t    if state.num_samples:\n\t        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\t    if eos_token_id is None:\n\t        _eos_token_id = -1\n\t    else:\n\t        _eos_token_id = eos_token_id\n\t    aspect_cnt = 3\n\t    next_tokens = gt_tokens[:, aspect_cnt:aspect_cnt + 2]\n", "    token_ids = torch.cat([tokens, next_tokens], dim=1)\n\t    cur_len = token_ids.size(1)\n\t    dones = token_ids.new_zeros(batch_size).eq(1)\n\t    # tokens = tokens[:, -1:]\n\t    max_len_a = 0\n\t    max_length = gt_tokens.size(1)\n\t    gt_mask = gt_tokens.eq(1).eq(0)\n\t    max_lengths = gt_mask.sum(dim=1)\n\t    while cur_len < max_length:\n\t        scores = decoder.decode(tokens=token_ids, state=state,\n", "                                only_sc=True)  # batch_size x vocab_size\n\t        if restricter is not None:\n\t            _, next_tokens = restricter(state, token_ids, scores, 1)\n\t        else:\n\t            next_tokens = scores.argmax(dim=-1, keepdim=True)\n\t        next_tokens = next_tokens.squeeze(-1)\n\t        # 如果已经达到对应的sequence长度了，就直接填为eos了\n\t        # if _eos_token_id != -1:\n\t        #     next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len + 1),\n\t        #                                           _eos_token_id)\n", "        next_tokens = next_tokens.masked_fill(\n\t            dones, pad_token_id)  # 对已经搜索完成的sample做padding\n\t        tokens = next_tokens.unsqueeze(1)\n\t        token_ids = torch.cat([token_ids, tokens],\n\t                              dim=-1)  # batch_size x max_len\n\t        # end_mask = next_tokens.eq(_eos_token_id)\n\t        # dones = dones.__or__(end_mask)\n\t        dones = gt_tokens[:, cur_len + 1].eq(1)\n\t        cur_len += 1\n\t        aspect_cnt += 3\n", "        if aspect_cnt + 2 < max_length:\n\t            token_ids = torch.cat(\n\t                [token_ids, gt_tokens[:, aspect_cnt:aspect_cnt + 2]], dim=-1)\n\t        cur_len += 2\n\t        if dones.min() == 1:\n\t            break\n\t    ones = token_ids.new_ones(batch_size).unsqueeze(-1)\n\t    token_ids = torch.cat([token_ids, ones], dim=-1)\n\t    # if eos_token_id is not None:\n\t    #     tokens.scatter(index=max_lengths[:, None], dim=1, value=eos_token_id)  # 将最大长度位置设置为eos\n", "    # if cur_len == max_length:\n\t    #     token_ids[:, -1].masked_fill_(~dones, eos_token_id)  # 若到最长长度仍未到EOS，则强制将最后一个词替换成eos\n\t    return token_ids\n\tdef _beam_search_generate(decoder: Seq2SeqDecoder,\n\t                          tokens=None,\n\t                          state=None,\n\t                          max_length=20,\n\t                          max_len_a=0.0,\n\t                          num_beams=4,\n\t                          bos_token_id=None,\n", "                          eos_token_id=None,\n\t                          do_sample=True,\n\t                          repetition_penalty=1.0,\n\t                          length_penalty=None,\n\t                          pad_token_id=0,\n\t                          restricter=None) -> torch.LongTensor:\n\t    assert do_sample is False\n\t    # 进行beam search\n\t    # import ipdb; ipdb.set_trace()\n\t    device = _get_model_device(decoder)\n", "    if tokens is None:\n\t        if bos_token_id is None:\n\t            raise RuntimeError(\n\t                \"You have to specify either `tokens` or `bos_token_id`.\")\n\t        batch_size = state.num_samples\n\t        if batch_size is None:\n\t            raise RuntimeError(\n\t                \"Cannot infer the number of samples from `state`.\")\n\t        tokens = torch.full([batch_size, 1],\n\t                            fill_value=bos_token_id,\n", "                            dtype=torch.long).to(device)\n\t    batch_size = tokens.size(0)\n\t    if state.num_samples:\n\t        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\t    if eos_token_id is None:\n\t        _eos_token_id = -1\n\t    else:\n\t        _eos_token_id = eos_token_id\n\t    scores = decoder.decode(tokens=tokens, state=state)  # 这里要传入的是整个句子的长度\n\t    # 这里需要考虑如果在第一个位置就结束的情况\n", "    # if _eos_token_id!=-1:\n\t    #     scores[:, _eos_token_id] = -1e12\n\t    vocab_size = scores.size(1)\n\t    assert vocab_size >= num_beams, \"num_beams should be smaller than the number of vocabulary size.\"\n\t    scores = F.log_softmax(scores, dim=-1)  # (batch_size, vocab_size)\n\t    # 得到(batch_size, num_beams), (batch_size, num_beams)\n\t    # TODO 把限制写到这个位置, 加1是因为需要考虑输出就是eos的情况\n\t    if restricter is not None:\n\t        _next_scores, _next_tokens = restricter(state, tokens, scores,\n\t                                                num_beams + 1)\n", "    else:\n\t        # 是bsz x (num_beams+1)大小的东西\n\t        _next_scores, _next_tokens = torch.topk(scores,\n\t                                                num_beams + 1,\n\t                                                dim=1,\n\t                                                largest=True,\n\t                                                sorted=True)\n\t    # 根据index来做顺序的调转\n\t    indices = torch.arange(batch_size, dtype=torch.long).to(device)\n\t    indices = indices.repeat_interleave(num_beams)\n", "    state.reorder_state(indices)\n\t    tokens = tokens.index_select(\n\t        dim=0, index=indices)  # batch_size * num_beams x length\n\t    if max_len_a != 0:\n\t        # (bsz x num_beams, )\n\t        if state.encoder_mask is not None:\n\t            max_lengths = (state.encoder_mask.sum(dim=1).float() *\n\t                           max_len_a).long() + max_length\n\t        else:\n\t            max_lengths = tokens.new_full((batch_size * num_beams, ),\n", "                                          fill_value=max_length,\n\t                                          dtype=torch.long)\n\t        real_max_length = max_lengths.max().item()\n\t    else:\n\t        real_max_length = max_length\n\t        if state.encoder_mask is not None:\n\t            max_lengths = state.encoder_mask.new_ones(\n\t                state.encoder_mask.size(0)).long() * max_length\n\t        else:\n\t            max_lengths = tokens.new_full((batch_size * num_beams, ),\n", "                                          fill_value=max_length,\n\t                                          dtype=torch.long)\n\t    hypos = [\n\t        BeamHypotheses(num_beams,\n\t                       real_max_length,\n\t                       length_penalty,\n\t                       early_stopping=False) for _ in range(batch_size)\n\t    ]\n\t    not_eos_mask = _next_tokens.ne(_eos_token_id)  # 为1的地方不是eos\n\t    keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # 为1的地方需要保留\n", "    keep_mask = not_eos_mask.__and__(keep_mask)  # 为1的地方是需要进行下一步search的\n\t    next_tokens = _next_tokens.masked_select(keep_mask).view(\n\t        batch_size, num_beams)  # 这是真的接下来要继续的\n\t    next_scores = _next_scores.masked_select(keep_mask).view(\n\t        batch_size, num_beams)\n\t    rows, cols = not_eos_mask.eq(0)[:, :num_beams].nonzero(as_tuple=True)\n\t    if len(rows) > 0:  # 说明有的开头就结束了\n\t        for row, col in zip(rows.tolist(), cols.tolist()):\n\t            _token = torch.cat(\n\t                [tokens[row * num_beams], _next_tokens[row, col:col + 1]],\n", "                dim=0)\n\t            hypos[row].add(_token.clone(), _next_scores[row, col].item())\n\t    # 记录生成好的token (batch_size', cur_len)\n\t    token_ids = torch.cat([tokens, next_tokens.view(-1, 1)], dim=-1)\n\t    dones = [False] * batch_size\n\t    beam_scores = next_scores.view(-1)  # batch_size * num_beams\n\t    #  用来记录已经生成好的token的长度\n\t    cur_len = token_ids.size(1)\n\t    # 0, num_beams, 2*num_beams, ...\n\t    batch_inds_with_numbeams_interval = (torch.arange(batch_size) *\n", "                                         num_beams).view(-1, 1).to(token_ids)\n\t    while cur_len < real_max_length:\n\t        scores = decoder.decode(token_ids,\n\t                                state)  # (bsz x num_beams, vocab_size)\n\t        if repetition_penalty != 1.0:\n\t            token_scores = scores.gather(dim=1, index=token_ids)\n\t            lt_zero_mask = token_scores.lt(0).float()\n\t            ge_zero_mask = lt_zero_mask.eq(0).float()\n\t            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n\t            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n", "        if _eos_token_id != -1:\n\t            max_len_eos_mask = max_lengths.eq(cur_len + 1)\n\t            eos_scores = scores[:, _eos_token_id]\n\t            # 如果已经达到最大长度，就把eos的分数加大\n\t            scores[:, _eos_token_id] = torch.where(max_len_eos_mask,\n\t                                                   eos_scores + 1e32,\n\t                                                   eos_scores)\n\t        scores = F.log_softmax(scores,\n\t                               dim=-1)  # (batch_size * num_beams, vocab_size)\n\t        _scores = scores + beam_scores[:,\n", "                                       None]  # (batch_size * num_beams, vocab_size)\n\t        _scores = _scores.view(batch_size,\n\t                               -1)  # (batch_size, num_beams*vocab_size)\n\t        # TODO 把限制加到这个位置\n\t        if restricter is not None:\n\t            next_scores, ids = restricter(state, token_ids, _scores,\n\t                                          2 * num_beams)\n\t        else:\n\t            next_scores, ids = torch.topk(_scores,\n\t                                          2 * num_beams,\n", "                                          dim=1,\n\t                                          largest=True,\n\t                                          sorted=True)  # (bsz, 2*num_beams)\n\t        from_which_beam = ids // vocab_size  # (batch_size, 2*num_beams)\n\t        next_tokens = ids % vocab_size  # (batch_size, 2*num_beams)\n\t        not_eos_mask = next_tokens.ne(_eos_token_id)  # 为1的地方不是eos\n\t        keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # 为1的地方需要保留\n\t        keep_mask = not_eos_mask.__and__(keep_mask)  # 为1的地方是需要进行下一步search的\n\t        _next_tokens = next_tokens.masked_select(keep_mask).view(-1, 1)\n\t        _from_which_beam = from_which_beam.masked_select(keep_mask).view(\n", "            batch_size, num_beams)  # 上面的token是来自哪个beam\n\t        _next_scores = next_scores.masked_select(keep_mask).view(\n\t            batch_size, num_beams)\n\t        beam_scores = _next_scores.view(-1)\n\t        flag = True\n\t        if cur_len + 1 == real_max_length:\n\t            eos_batch_idx = torch.arange(batch_size).to(\n\t                next_tokens).repeat_interleave(repeats=num_beams, dim=0)\n\t            eos_beam_ind = torch.arange(num_beams).to(token_ids).repeat(\n\t                batch_size)  # 表示的是indice\n", "            eos_beam_idx = from_which_beam[:, :num_beams].reshape(\n\t                -1)  # 表示的是从哪个beam获取得到的\n\t        else:\n\t            # 将每个batch中在num_beam内的序列添加到结束中, 为1的地方需要结束了\n\t            effective_eos_mask = next_tokens[:, :num_beams].eq(\n\t                _eos_token_id)  # batch_size x num_beams\n\t            if effective_eos_mask.sum().gt(0):\n\t                eos_batch_idx, eos_beam_ind = effective_eos_mask.nonzero(\n\t                    as_tuple=True)\n\t                # 是由于from_which_beam是 (batch_size, 2*num_beams)的，所以需要2*num_beams\n", "                eos_beam_idx = eos_batch_idx * num_beams * 2 + eos_beam_ind\n\t                eos_beam_idx = from_which_beam.view(-1)[\n\t                    eos_beam_idx]  # 获取真实的从哪个beam获取的eos\n\t            else:\n\t                flag = False\n\t        if flag:\n\t            _token_ids = torch.cat([token_ids, _next_tokens], dim=-1)\n\t            for batch_idx, beam_ind, beam_idx in zip(eos_batch_idx.tolist(),\n\t                                                     eos_beam_ind.tolist(),\n\t                                                     eos_beam_idx.tolist()):\n", "                if not dones[batch_idx]:\n\t                    score = next_scores[batch_idx, beam_ind].item()\n\t                    # 之后需要在结尾新增一个eos\n\t                    if _eos_token_id != -1:\n\t                        hypos[batch_idx].add(\n\t                            _token_ids[batch_idx * num_beams +\n\t                                       beam_idx, :cur_len].clone(), score)\n\t                    else:\n\t                        hypos[batch_idx].add(\n\t                            _token_ids[batch_idx * num_beams +\n", "                                       beam_idx].clone(), score)\n\t        # 更改state状态, 重组token_ids\n\t        reorder_inds = (batch_inds_with_numbeams_interval +\n\t                        _from_which_beam).view(-1)  # flatten成一维\n\t        state.reorder_state(reorder_inds)\n\t        # 重新组织token_ids的状态\n\t        token_ids = torch.cat(\n\t            [token_ids.index_select(index=reorder_inds, dim=0), _next_tokens],\n\t            dim=-1)\n\t        for batch_idx in range(batch_size):\n", "            dones[batch_idx] = dones[batch_idx] or hypos[batch_idx].is_done(next_scores[batch_idx, 0].item()) or \\\n\t                               max_lengths[batch_idx*num_beams]==cur_len+1\n\t        cur_len += 1\n\t        if all(dones):\n\t            break\n\t    # select the best hypotheses\n\t    tgt_len = token_ids.new_zeros(batch_size)\n\t    best = []\n\t    for i, hypotheses in enumerate(hypos):\n\t        best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n", "        # 把上面替换为非eos的词替换回eos\n\t        if _eos_token_id != -1:\n\t            best_hyp = torch.cat(\n\t                [best_hyp, best_hyp.new_ones(1) * _eos_token_id])\n\t        tgt_len[i] = len(best_hyp)\n\t        best.append(best_hyp)\n\t    # generate target batch\n\t    decoded = token_ids.new_zeros(batch_size,\n\t                                  tgt_len.max().item()).fill_(pad_token_id)\n\t    for i, hypo in enumerate(best):\n", "        decoded[i, :tgt_len[i]] = hypo\n\t    return decoded\n\tclass BeamHypotheses(object):\n\t    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n\t        \"\"\"\n\t        Initialize n-best list of hypotheses.\n\t        \"\"\"\n\t        self.max_length = max_length - 1  # ignoring bos_token\n\t        self.length_penalty = length_penalty\n\t        self.early_stopping = early_stopping\n", "        self.num_beams = num_beams\n\t        self.hyp = []\n\t        self.worst_score = 1e9\n\t    def __len__(self):\n\t        \"\"\"\n\t        Number of hypotheses in the list.\n\t        \"\"\"\n\t        return len(self.hyp)\n\t    def add(self, hyp, sum_logprobs):\n\t        \"\"\"\n", "        Add a new hypothesis to the list.\n\t        \"\"\"\n\t        score = sum_logprobs / len(hyp)**self.length_penalty\n\t        if len(self) < self.num_beams or score > self.worst_score:\n\t            self.hyp.append((score, hyp))\n\t            if len(self) > self.num_beams:\n\t                sorted_scores = sorted([\n\t                    (s, idx) for idx, (s, _) in enumerate(self.hyp)\n\t                ])\n\t                del self.hyp[sorted_scores[0][1]]\n", "                self.worst_score = sorted_scores[1][0]\n\t            else:\n\t                self.worst_score = min(score, self.worst_score)\n\t    def is_done(self, best_sum_logprobs):\n\t        \"\"\"\n\t        If there are enough hypotheses and that none of the hypotheses being generated\n\t        can become better than the worst one in the heap, then we are done with this sentence.\n\t        \"\"\"\n\t        if len(self) < self.num_beams:\n\t            return False\n", "        elif self.early_stopping:\n\t            return True\n\t        else:\n\t            return self.worst_score >= best_sum_logprobs / self.max_length**self.length_penalty\n"]}
{"filename": "src/model/model.py", "chunked_list": ["# Based on transformers.modeling_bart\n\tfrom typing import Optional, Tuple\n\tfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\n\tfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\n\tfrom fastNLP.modules.torch import State\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n\t                                     BartDecoder, BartModel,\n", "                                     BartClassificationHead,\n\t                                     _make_linear_from_emb,\n\t                                     _prepare_bart_decoder_inputs)\n\tfrom transformers import BartTokenizer\n\tfrom src.model.config import MultiModalBartConfig\n\tfrom src.model.mixins import GenerationMixin, FromPretrainedMixin\n\tfrom src.model.modules import MultiModalBartEncoder, MultiModalBartDecoder_span, MultiModalBartDecoder_MLM, MultiModalBartDecoder_sentiment, Span_loss, MultiModalBartDecoder_MRM, MultiModalBartDecoder_ANP_generate\n\t# This is based on transformers.BartModel\n\t# The modifications are:\n\t# - BartConfig -> MultiModalBartConfig\n", "# - BartEncoder -> MultiModalBartEncoder\n\t# - added image_features in forward\n\t# def generate_span_mask(spans):\n\t#     max_len = max([len(x) for x in spans])\n\t#     mask = torch.ones(())\n\tclass MultiModalBartModelForPretrain(FromPretrainedMixin, PretrainedBartModel):\n\t    def build_model(self,\n\t                    args,\n\t                    bart_model,\n\t                    tokenizer,\n", "                    label_ids,\n\t                    config,\n\t                    decoder_type=None,\n\t                    copy_gate=False,\n\t                    use_encoder_mlp=False,\n\t                    use_recur_pos=False,\n\t                    tag_first=False):\n\t        if args.bart_init:\n\t            model = BartModel.from_pretrained(bart_model)\n\t            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n", "            model.resize_token_embeddings(\n\t                len(tokenizer.unique_no_split_tokens) + num_tokens)\n\t            encoder = model.encoder\n\t            decoder = model.decoder\n\t            padding_idx = config.pad_token_id\n\t            encoder.embed_tokens.padding_idx = padding_idx\n\t            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\t            for token in tokenizer.unique_no_split_tokens:\n\t                if token[:2] == '<<':  # 特殊字符\n\t                    index = tokenizer.convert_tokens_to_ids(\n", "                        tokenizer._base_tokenizer.tokenize(token))\n\t                    if len(index) > 1:\n\t                        raise RuntimeError(f\"{token} wrong split\")\n\t                    else:\n\t                        index = index[0]\n\t                    assert index >= num_tokens, (index, num_tokens, token)\n\t                    indexes = _tokenizer.convert_tokens_to_ids(\n\t                        _tokenizer.tokenize(token[2:-2]))\n\t                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n\t                    for i in indexes[1:]:\n", "                        embed += model.decoder.embed_tokens.weight.data[i]\n\t                    embed /= len(indexes)\n\t                    model.decoder.embed_tokens.weight.data[index] = embed\n\t        else:\n\t            raise RuntimeError(\"error init!!!!!!!\")\n\t        multimodal_encoder = MultiModalBartEncoder(config, encoder,\n\t                                                   tokenizer.img_feat_id,\n\t                                                   tokenizer.cls_token_id)\n\t        return (multimodal_encoder, decoder)\n\t    def __init__(self, config: MultiModalBartConfig, bart_model, tokenizer,\n", "                 label_ids, senti_ids, args):\n\t        super().__init__(config)\n\t        self.config = config\n\t        label_ids = sorted(label_ids)\n\t        multimodal_encoder, share_decoder = self.build_model(\n\t            args, bart_model, tokenizer, label_ids, config)\n\t        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n\t        self.causal_mask = causal_mask.triu(diagonal=1)\n\t        self.encoder = multimodal_encoder\n\t        self.mlm_decoder = MultiModalBartDecoder_MLM(self.config,\n", "                                                     share_decoder)\n\t        self.mrm_decoder = MultiModalBartDecoder_MRM(self.config,\n\t                                                     share_decoder,\n\t                                                     self.causal_mask, args)\n\t        self.span_decoder = MultiModalBartDecoder_span(self.config, tokenizer,\n\t                                                       share_decoder,\n\t                                                       tokenizer.pad_token_id,\n\t                                                       label_ids,\n\t                                                       self.causal_mask)\n\t        self.span_loss_fct = Span_loss()\n", "        self.anp_generate_decoder = MultiModalBartDecoder_ANP_generate(\n\t            self.config, share_decoder)\n\t        self.senti_decoder = MultiModalBartDecoder_sentiment(\n\t            self.config, share_decoder, senti_ids)\n\t    def prepare_state(self,\n\t                      input_ids,\n\t                      image_features,\n\t                      attention_mask=None,\n\t                      first=None):\n\t        dict = self.encoder(input_ids=input_ids,\n", "                            image_features=image_features,\n\t                            attention_mask=attention_mask,\n\t                            output_hidden_states=True,\n\t                            return_dict=True)\n\t        encoder_outputs = dict.last_hidden_state\n\t        hidden_states = dict.hidden_states\n\t        encoder_mask = attention_mask\n\t        src_embed_outputs = hidden_states[0]\n\t        state = BartState(encoder_outputs, encoder_mask, input_ids[:, 38:],\n\t                          first, src_embed_outputs)\n", "        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n\t        return state\n\t    def forward(\n\t            self,\n\t            task_type,\n\t            input_ids,\n\t            image_features,\n\t            attention_mask=None,\n\t            mlm_infos=None,\n\t            mrm_infos=None,\n", "            senti_infos=None,\n\t            ANP_infos=None,\n\t            ANP_generate_infos=None,\n\t            ae_infos=None,\n\t            oe_infos=None,\n\t            ae_oe_infos=None,\n\t            encoder_outputs: Optional[Tuple] = None,\n\t            use_cache=None,\n\t            output_attentions=None,\n\t            output_hidden_states=None,\n", "    ):\n\t        if encoder_outputs is None:\n\t            encoder_outputs = self.encoder(\n\t                input_ids=input_ids,\n\t                image_features=image_features,\n\t                attention_mask=attention_mask,\n\t                output_attentions=output_attentions,\n\t                output_hidden_states=output_hidden_states,\n\t            )\n\t        assert isinstance(encoder_outputs, tuple)\n", "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n\t        if task_type == 'MLM':\n\t            labels, decoder_input_ids, decoder_attention_mask = [\n\t                mlm_infos['mlm_labels'], mlm_infos['mlm_decoder_input_ids'],\n\t                mlm_infos['mlm_decoder_attention_mask']\n\t            ]\n\t            loss = self.mlm_decoder(labels, input_ids, encoder_outputs[0],\n\t                                    attention_mask, decoder_input_ids,\n\t                                    decoder_attention_mask)\n\t        elif task_type == 'MRM':\n", "            mrm_labels, mrm_masks, decoder_input_ids, decoder_attention_mask = [\n\t                mrm_infos['mrm_labels'],\n\t                mrm_infos['mrm_masks'].to(input_ids.device),\n\t                mrm_infos['mrm_decoder_input_ids'].to(input_ids.device),\n\t                mrm_infos['mrm_decoder_attention_mask'].to(input_ids.device)\n\t            ]\n\t            loss = self.mrm_decoder(mrm_labels, mrm_masks, encoder_outputs[0],\n\t                                    attention_mask, decoder_input_ids,\n\t                                    decoder_attention_mask)\n\t        elif task_type == 'Sentiment':\n", "            senti_labels, decoder_input_ids, decoder_attention_mask = [\n\t                senti_infos['senti_labels'],\n\t                senti_infos['senti_decoder_input_ids'],\n\t                senti_infos['senti_decoder_attention_mask']\n\t            ]\n\t            loss, predict_senti = self.senti_decoder(senti_labels,\n\t                                                     encoder_outputs[0],\n\t                                                     attention_mask,\n\t                                                     decoder_input_ids)\n\t        elif task_type == 'ANP_generate':\n", "            labels, decoder_input_ids, decoder_attention_mask = [\n\t                ANP_generate_infos['anp_generate_labels'],\n\t                ANP_generate_infos['anp_generate_decoder_input_ids'],\n\t                ANP_generate_infos['anp_generate_decoder_attention_mask']\n\t            ]\n\t            loss = self.anp_generate_decoder(labels, input_ids,\n\t                                             encoder_outputs[0],\n\t                                             attention_mask, decoder_input_ids,\n\t                                             decoder_attention_mask)\n\t        elif task_type == 'AE_OE':\n", "            spans, span_mask = [\n\t                ae_oe_infos['labels'].to(input_ids.device),\n\t                ae_oe_infos['masks'].to(input_ids.device)\n\t            ]\n\t            state = self.prepare_state(input_ids, image_features,\n\t                                       attention_mask)\n\t            logits = self.span_decoder(spans, state)\n\t            loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n\t        else:\n\t            raise RuntimeError(\"task type error!!!!!!!\")\n", "        if task_type == 'Sentiment':\n\t            return loss, predict_senti\n\t        return loss\n\tclass BartState(State):\n\t    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n\t                 src_embed_outputs):\n\t        super().__init__(encoder_output, encoder_mask)\n\t        self.past_key_values = None\n\t        self.src_tokens = src_tokens\n\t        self.first = first\n", "        self.src_embed_outputs = src_embed_outputs\n\t    def reorder_state(self, indices: torch.LongTensor):\n\t        super().reorder_state(indices)\n\t        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n\t        if self.first is not None:\n\t            self.first = self._reorder_state(self.first, indices)\n\t        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n\t                                                     indices)\n\t        if self.past_key_values is not None:\n\t            new = []\n", "            for layer in self.past_key_values:\n\t                new_layer = {}\n\t                for key1 in list(layer.keys()):\n\t                    new_layer_ = {}\n\t                    for key2 in list(layer[key1].keys()):\n\t                        if layer[key1][key2] is not None:\n\t                            layer[key1][key2] = self._reorder_state(\n\t                                layer[key1][key2], indices)\n\t                        new_layer_[key2] = layer[key1][key2]\n\t                    new_layer[key1] = new_layer_\n", "                new.append(new_layer)\n\t            self.past_key_values = new"]}
{"filename": "src/model/modules_for_prompt_multitasks.py", "chunked_list": ["import random\n\timport math\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom transformers.models.bart.modeling_bart import *\n\tfrom src.model.modeling_bart import (\n\t    SinusoidalPositionalEmbedding,\n\t    LearnedPositionalEmbedding,\n\t    invert_mask,\n", "    EncoderLayer,\n\t    LayerNorm,\n\t)\n\tfrom src.model.modeling_bart import (PretrainedBartModel, BartDecoder,\n\t                                     BartClassificationHead,\n\t                                     _make_linear_from_emb,\n\t                                     _prepare_bart_decoder_inputs)\n\tfrom src.model.config import MultiModalBartConfig\n\tfrom transformers import AutoConfig, AutoModel, CLIPVisionModel, CLIPVisionConfig\n\timport timm\n", "from src.model.attention import Attention_for_Senti_Prompt\n\tTIMM_MODELS = {\n\t    'nf_resnet50': 2048,\n\t}\n\tdef is_clip_model(model_name):\n\t    return model_name.startswith('openai/clip-')\n\timage_model_name =  'nf_resnet50'\n\tif image_model_name in TIMM_MODELS.keys():\n\t    image_encoder = timm.create_model(image_model_name, pretrained=True, num_classes=0)\n\telif is_clip_model(image_model_name):\n", "    ###model_name ='openai/clip-vit-base-patch32'\n\t    config = CLIPVisionConfig.from_pretrained(image_model_name)\n\t    image_encoder = CLIPVisionModel.from_pretrained(\n\t            image_model_name,\n\t            config=config,\n\t        )\n\telse:\n\t    image_encoder = AutoModel.from_pretrained(image_model_name)\n\tdef init_image_encoder(image_model_name, frozen_image_encoder, num_image_tokens, d_text_encoder):\n\t    # image_encoder = get_image_encoder(image_model_name)\n", "    d_image_encoder = _d_image_encoder(image_model_name, image_encoder)\n\t    if frozen_image_encoder:\n\t        for p in image_encoder.parameters():\n\t            p.requires_grad = False\n\t            image_encoder.eval()\n\t    proj_image_features = nn.Linear(\n\t            in_features=d_image_encoder,\n\t            out_features=num_image_tokens * d_text_encoder,\n\t        )\n\t    return proj_image_features.cuda(), d_image_encoder\n", "def _d_image_encoder(image_model_name, image_encoder):\n\t    ##image_model_name默认为： 'microsoft/resnet-50'\n\t    model_name = image_model_name\n\t    if model_name in TIMM_MODELS.keys():\n\t        return TIMM_MODELS[model_name]\n\t    elif is_clip_model(model_name):\n\t        return image_encoder.config.hidden_size\n\t    elif model_name.startswith('microsoft/resnet-'):\n\t        return image_encoder.config.hidden_sizes[-1]\n\t    else:\n", "        return image_encoder.config.hidden_size\n\tdef encode_images(image_encoder, proj_image_features, frozen_image_encoder, pixel_values, d_image_encoder):\n\t    image_encoder = image_encoder.cuda()\n\t    pixel_values = pixel_values.cuda()\n\t    # print('the shape of pixel_values is {}'.format(pixel_values.shape))\n\t    batch_size = pixel_values.shape[0]\n\t    if frozen_image_encoder:\n\t        with torch.no_grad():\n\t            image_encoder.eval()\n\t            visual = image_encoder(pixel_values)\n", "    else:\n\t        visual = image_encoder(pixel_values)\n\t    if not isinstance(visual, torch.Tensor):  # HuggingFace model\n\t        visual = visual.pooler_output\n\t    visual = visual.reshape(batch_size, d_image_encoder)\n\t    visual = proj_image_features(visual).cuda()\n\t    return visual\n\tclass ImageEmbedding(nn.Module):\n\t    def __init__(self, image_dim, final_dim, image_model_name, frozen_image_encoder=False, num_image_tokens=2):\n\t        super(ImageEmbedding, self).__init__()\n", "        self.frozen_image_encoder = frozen_image_encoder\n\t        self.final_dim = final_dim\n\t        self.linear = nn.Linear(final_dim, final_dim)\n\t        self.d_image_encoder = _d_image_encoder(image_model_name, image_encoder)\n\t        if frozen_image_encoder:\n\t            for p in image_encoder.parameters():\n\t                p.requires_grad = False\n\t                image_encoder.eval()\n\t        self.proj_image_features = nn.Linear(\n\t                in_features=self.d_image_encoder,\n", "                out_features=num_image_tokens * final_dim,\n\t            )\n\t    def forward(self, image_pixel_values):\n\t        # import ipdb; ipdb.set_trace()\n\t        image_pixel_values = torch.stack(image_pixel_values)\n\t        batch_size = image_pixel_values.size(0)\n\t        image_features = encode_images(image_encoder=image_encoder, \n\t                                           proj_image_features=self.proj_image_features, \n\t                                           frozen_image_encoder=self.frozen_image_encoder, \n\t                                           pixel_values=image_pixel_values, \n", "                                           d_image_encoder=self.d_image_encoder)\n\t        ###image_features: (batch_size, num_image_tokens*1024) (4, 2048)\n\t        # print(\"======================================the shape of image_features is {}=====================================\".format(image_features.shape))\n\t        image_features = image_features.reshape(batch_size, -1, self.final_dim) ### (4, num_image_tokens, 1024(d_model))\n\t        img_len = list(map(len, image_features))\n\t        non_empty_features = list(filter(lambda x: len(x) > 0, image_features))\n\t        embedded = None\n\t        if len(non_empty_features) > 0:\n\t            img_tensor = torch.cat(non_empty_features, dim=0)\n\t            embedded = self.linear(img_tensor)\n", "        output = []\n\t        index = 0\n\t        for l in img_len:\n\t            if l > 0:\n\t                output.append(embedded[index:index + l])\n\t            else:\n\t                output.append(torch.empty(0))\n\t            index += l\n\t        return output\n\tclass MultiModalBartEncoder(nn.Module):\n", "    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n\t        config: MultiModalBartConfig\n\t    \"\"\"\n\t    def __init__(self, config: MultiModalBartConfig, encoder, img_feat_id,\n\t                 cls_token_id, num_image_tokens):\n\t        super().__init__()\n\t        self.img_feat_id = img_feat_id\n", "        self.cls_token_id = cls_token_id\n\t        embed_tokens = encoder.embed_tokens\n\t        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n\t        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n\t        self.embed_tokens = embed_tokens\n", "        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n\t        self.embed_positions = encoder.embed_positions\n\t        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t    def _embed_multi_modal(self, input_ids, image_features):\n\t        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n\t        mask = (input_ids == self.img_feat_id) | (\n\t            input_ids == self.cls_token_id)\n", "        # print(mask.shape)\n\t        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n\t        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n\t        for index, value in enumerate(embedded_images):\n\t            if len(value) > 0:\n\t                embedded[index, mask[index]] = value\n\t        return embedded\n", "    def forward(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n\t        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n\t        :param image_features: list[FloatTensor], image roi features with length of batch\n", "        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n\t        :param output_attentions:\n\t        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n\t              Only populated if output_hidden_states: is True.\n\t            - all_attentions (List[Tensor]): Attention weights for each layer.\n", "            During training might not be of length n_layers because of layer dropout.\n\t        \"\"\"\n\t        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n\t        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n", "        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        # B x T x C -> T x B x C\n\t        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n", "                                  self.layerdrop):  # skip the layer\n\t                attn = None\n\t            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n\t        if self.layer_norm:\n\t            x = self.layer_norm(x)\n", "        if output_hidden_states:\n\t            encoder_states.append(x)\n\t        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n", "        return BaseModelOutput(last_hidden_state=x,\n\t                               hidden_states=encoder_states,\n\t                               attentions=all_attentions)\n\tclass MultiModalBartEncoder_for_Generating_aspect_prompt(nn.Module):\n\t    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n\t        config: MultiModalBartConfig\n\t    \"\"\"\n", "    def __init__(self, \n\t                use_generated_prompt,\n\t                config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n\t                 cls_token_id, num_image_tokens, use_different_aspect_prompt):\n\t        super().__init__()\n\t        self.use_generated_prompt= use_generated_prompt\n\t        self.aspect_prompt_token_id = aspect_prompt_token_id\n\t        self.senti_prompt_token_id = senti_prompt_token_id\n\t        self.use_different_aspect_prompt = use_different_aspect_prompt\n\t        self.img_feat_id = img_feat_id\n", "        self.cls_token_id = cls_token_id\n\t        embed_tokens = encoder.embed_tokens\n\t        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n\t        self.num_image_tokens = num_image_tokens\n\t        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n", "        self.embed_tokens = embed_tokens\n\t        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n\t        self.embed_positions = encoder.embed_positions\n\t        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t        # self.aspect_linear = nn.Linear(768, 768)\n\t        # self.aspect_relu = nn.LeakyReLU()\n\t    def _embed_multi_modal(self, generated_aspect_prompt, aspects_num, input_ids, image_features):\n", "        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n\t        # import ipdb; ipdb.set_trace()\n\t        mask = (input_ids == self.img_feat_id) | (\n\t            input_ids == self.cls_token_id)\n\t        # print(mask.shape)\n\t        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n\t        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n", "        for index, value in enumerate(embedded_images):\n\t            if len(value) > 0:\n\t                embedded[index, mask[index]] = value\n\t        new_input_ids =[]\n\t        # import ipdb; ipdb.set_trace()\n\t        for i in range(len(aspects_num)):\n\t            aspect_num = aspects_num[i]\n\t            # print('the aspect_num is {}'.format(aspect_num))\n\t            input_id = input_ids[i]\n\t            if self.num_image_tokens==0:\n", "                prompt_begin_index = 25\n\t                prompt_end_index = 39\n\t            elif self.num_image_tokens==1:\n\t                prompt_begin_index = 26\n\t                prompt_end_index = 40\n\t            elif self.num_image_tokens==2:\n\t                prompt_begin_index = 27\n\t                prompt_end_index = 41\n\t            elif self.num_image_tokens==3:\n\t                prompt_begin_index = 28\n", "                prompt_end_index = 42\n\t            elif self.num_image_tokens==4:\n\t                prompt_begin_index = 29\n\t                prompt_end_index = 43\n\t            elif self.num_image_tokens==5:\n\t                prompt_begin_index = 30\n\t                prompt_end_index = 44\n\t            elif self.num_image_tokens==6:\n\t                prompt_begin_index = 31\n\t                prompt_end_index = 45\n", "            elif self.num_image_tokens==7:\n\t                prompt_begin_index = 32\n\t                prompt_end_index = 46\n\t            # print('before')\n\t            # print(len(input_id))\n\t            # import ipdb; ipdb.set_trace()\n\t            reserve_aspect_id = input_id[prompt_begin_index:prompt_begin_index+3*aspect_num]\n\t            if aspect_num ==5:\n\t                # print('aspect_num is 5')\n\t                # print(reserve_aspect_id)\n", "                new_input_id = torch.cat([input_id[:prompt_begin_index], reserve_aspect_id, input_id[prompt_end_index+1:]])\n\t            else:\n\t                cut_aspect_id = torch.ones_like(input_id[prompt_begin_index+3*aspect_num:prompt_end_index])\n\t                new_input_id = torch.cat([input_id[:prompt_begin_index], reserve_aspect_id, cut_aspect_id, input_id[prompt_end_index:]])\n\t            # print(\"++++++++++++++++++++cut_aspect_id++++++++++++++++++++++++\")\n\t            # print(cut_aspect_id)\n\t            # print(input_id[58:])\n\t            new_input_ids.append(new_input_id)\n\t            # print('the shape of new_input_id is {}'.format(new_input_id.shape))\n\t            # print(new_input_id[58:])\n", "            # print(\"+++++++++++++++++++++++input_id length is {}+++++++++++++++++++++++\".format(len(input_id)))\n\t            # print(input_id)\n\t            # print(\"+++++++++++++++++++++++new_input_id length is {}+++++++++++++++++++++++\".format(len(input_id)))\n\t            # print(new_input_id)\n\t        new_input_ids = torch.stack(new_input_ids)\n\t        prompt_mask = (new_input_ids == self.aspect_prompt_token_id)\n\t        if self.use_generated_prompt:\n\t            if self.use_different_aspect_prompt:\n\t                # self.aspect_linear = self.aspect_linear.to(generated_aspect_prompt.device)\n\t                # self.aspect_relu = self.aspect_relu.to(generated_aspect_prompt.device)\n", "                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n\t                    prompt_embedding_list = []\n\t                    for j in range(aspect_num):\n\t                        aspect_linear = nn.Linear(768, 768).to(generated_aspect_prompt.device) ##每个aspect有自己的变换，为每个aspect设计特定的prompt\n\t                        aspect_relu = nn.LeakyReLU().to(generated_aspect_prompt.device)\n\t                        prompt_embedding = aspect_linear(generated_aspect_prompt[index])\n\t                        prompt_embedding = aspect_relu(prompt_embedding)\n\t                        ###可以加入激活函数\n", "                        # prompt_embedding = nn.LeakyReLU(prompt_embedding)\n\t                        prompt_embedding_list.append(prompt_embedding)\n\t                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n\t                    embedded[index, prompt_mask[index]] = prompt_embedding_\n\t            else:\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    prompt_embedding_ = generated_aspect_prompt[index].repeat(aspect_num, 1)\n\t                    embedded[index, prompt_mask[index]] = prompt_embedding_\n\t        return embedded\n", "    def forward(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                generated_prompt=None,\n\t                aspects_num=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n", "        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n\t        :param image_features: list[FloatTensor], image roi features with length of batch\n\t        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n\t        :param output_attentions:\n\t        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n", "              Only populated if output_hidden_states: is True.\n\t            - all_attentions (List[Tensor]): Attention weights for each layer.\n\t            During training might not be of length n_layers because of layer dropout.\n\t        \"\"\"\n\t        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(generated_prompt, aspects_num,\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n", "        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        # B x T x C -> T x B x C\n\t        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n", "            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n\t                                  self.layerdrop):  # skip the layer\n\t                attn = None\n\t            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n", "        if self.layer_norm:\n\t            x = self.layer_norm(x)\n\t        if output_hidden_states:\n\t            encoder_states.append(x)\n\t        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n", "            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n\t        return BaseModelOutput(last_hidden_state=x,\n\t                               hidden_states=encoder_states,\n\t                               attentions=all_attentions)\n\tclass MultiModalBartEncoder_for_Generating_sentiment_prompt(nn.Module):\n\t    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n", "        config: MultiModalBartConfig\n\t    \"\"\"\n\t    def __init__(self, use_generated_prompt,\n\t                 config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n\t                 cls_token_id, num_image_tokens, use_different_senti_prompt):\n\t        super().__init__()\n\t        self.use_generated_prompt = use_generated_prompt\n\t        self.aspect_prompt_token_id = aspect_prompt_token_id\n\t        self.senti_prompt_token_id = senti_prompt_token_id\n\t        self.use_different_senti_prompt = use_different_senti_prompt\n", "        self.img_feat_id = img_feat_id\n\t        self.cls_token_id = cls_token_id\n\t        embed_tokens = encoder.embed_tokens\n\t        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n\t        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n", "        self.embed_tokens = embed_tokens\n\t        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n\t        self.embed_positions = encoder.embed_positions\n\t        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t        # self.aspect_linear = nn.Linear(768, 768)\n\t        # self.aspect_relu = nn.LeakyReLU()\n\t    def _embed_multi_modal(self, generated_senti_prompt, aspects_num, input_ids, image_features):\n", "        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n\t        # import ipdb; ipdb.set_trace()\n\t        mask = (input_ids == self.img_feat_id) | (\n\t            input_ids == self.cls_token_id)\n\t        # print(mask.shape)\n\t        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n\t        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n", "        for index, value in enumerate(embedded_images):\n\t            if len(value) > 0:\n\t                embedded[index, mask[index]] = value\n\t        if self.use_generated_prompt:\n\t            senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n\t            # import ipdb; ipdb.set_trace()\n\t            if self.use_different_senti_prompt:\n\t                # self.aspect_linear = self.aspect_linear.to(generated_senti_prompt.device)\n\t                # self.aspect_relu = self.aspect_relu.to(generated_senti_prompt.device)\n\t                for index in range(len(aspects_num)):\n", "                    aspect_num = aspects_num[index]\n\t                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n\t                    prompt_embedding_list = []\n\t                    for j in range(aspect_num):\n\t                        aspect_linear = nn.Linear(768, 768).to(generated_senti_prompt.device)\n\t                        aspect_relu = nn.LeakyReLU().to(generated_senti_prompt.device)\n\t                        prompt_embedding = aspect_linear(generated_senti_prompt[index])\n\t                        prompt_embedding = aspect_relu(prompt_embedding)\n\t                        ###可以加入激活函数å\n\t                        # prompt_embedding = nn.LeakyReLU(prompt_embedding)\n", "                        prompt_embedding_list.append(prompt_embedding)\n\t                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n\t                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n\t            else:\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    prompt_embedding_ = generated_senti_prompt[index].repeat(aspect_num, 1)\n\t                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n\t        return embedded\n\t    def forward(self,\n", "                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                generated_prompt=None,\n\t                aspects_num=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n\t        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n", "        :param image_features: list[FloatTensor], image roi features with length of batch\n\t        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n\t        :param output_attentions:\n\t        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n\t              Only populated if output_hidden_states: is True.\n", "            - all_attentions (List[Tensor]): Attention weights for each layer.\n\t            During training might not be of length n_layers because of layer dropout.\n\t        \"\"\"\n\t        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(generated_prompt, aspects_num,\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n\t        x = inputs_embeds + embed_pos\n", "        x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        # B x T x C -> T x B x C\n\t        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n", "            if self.training and (dropout_probability <\n\t                                  self.layerdrop):  # skip the layer\n\t                attn = None\n\t            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n\t        if self.layer_norm:\n", "            x = self.layer_norm(x)\n\t        if output_hidden_states:\n\t            encoder_states.append(x)\n\t        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n", "                         if v is not None)\n\t        return BaseModelOutput(last_hidden_state=x,\n\t                               hidden_states=encoder_states,\n\t                               attentions=all_attentions)\n\tclass MultiModalBartEncoder_for_Generating_Dual_prompts(nn.Module):\n\t    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n\t        config: MultiModalBartConfig\n", "    \"\"\"\n\t    def __init__(self, \n\t                 use_generated_aspect_prompt, use_generated_senti_prompt, \n\t                 config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n\t                 cls_token_id, num_image_tokens, use_different_aspect_prompt, use_different_senti_prompt, \n\t                 NEU_id, POS_id, NEG_id):\n\t        super().__init__()\n\t        self.use_generated_aspect_prompt= use_generated_aspect_prompt\n\t        self.use_generated_senti_prompt = use_generated_senti_prompt\n\t        self.aspect_prompt_token_id = aspect_prompt_token_id\n", "        self.senti_prompt_token_id = senti_prompt_token_id\n\t        self.use_different_aspect_prompt = use_different_aspect_prompt\n\t        self.use_different_senti_prompt = use_different_senti_prompt\n\t        # if self.use_different_senti_prompt:\n\t        #     self.attention_for_senti_prompt = Attention_for_Senti_Prompt(n_head=8, model_dim=768, drop_rate=0.2)\n\t        self.img_feat_id = img_feat_id\n\t        self.cls_token_id = cls_token_id\n\t        self.neu_id = NEU_id\n\t        self.pos_id = POS_id\n\t        self.neg_id = NEG_id\n", "        embed_tokens = encoder.embed_tokens\n\t        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n\t        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n\t        self.embed_tokens = embed_tokens\n\t        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n", "        self.embed_positions = encoder.embed_positions\n\t        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t        self.num_image_tokens = num_image_tokens\n\t        # self.aspect_linear = nn.Linear(768, 768)\n\t        # self.aspect_relu = nn.LeakyReLU()\n\t        # self.aspect_linear = nn.Sequential(nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768))\n\t    def _embed_multi_modal(self, generated_aspect_prompt, generated_senti_prompt, aspects_num, input_ids, image_features):\n", "        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n\t        # import ipdb; ipdb.set_trace()\n\t        device = generated_aspect_prompt.device\n\t        batch_size = input_ids.size(0)\n\t        mask = (input_ids == self.img_feat_id) | (\n\t            input_ids == self.cls_token_id)\n\t        # print(mask.shape)\n\t        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n", "        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n\t        for index, value in enumerate(embedded_images):\n\t            if len(value) > 0:\n\t                embedded[index, mask[index]] = value\n\t        new_input_ids =[]\n\t        for i in range(len(aspects_num)):\n\t            aspect_num = aspects_num[i]\n\t            # print('the aspect_num is {}'.format(aspect_num))\n\t            input_id = input_ids[i]\n", "            if self.num_image_tokens==0:\n\t                prompt_begin_index = 25\n\t                prompt_end_index = 54\n\t            elif self.num_image_tokens==1:\n\t                prompt_begin_index = 26\n\t                prompt_end_index = 55\n\t            elif self.num_image_tokens==2:\n\t                prompt_begin_index = 27\n\t                prompt_end_index = 56\n\t            elif self.num_image_tokens==3:\n", "                prompt_begin_index = 28\n\t                prompt_end_index = 57\n\t            elif self.num_image_tokens==4:\n\t                prompt_begin_index = 29\n\t                prompt_end_index = 58\n\t            elif self.num_image_tokens==5:\n\t                prompt_begin_index = 30\n\t                prompt_end_index = 59\n\t            elif self.num_image_tokens==6:\n\t                prompt_begin_index = 31\n", "                prompt_end_index = 60\n\t            elif self.num_image_tokens==7:\n\t                prompt_begin_index = 32\n\t                prompt_end_index = 61 \n\t            # print('before')\n\t            # print(len(input_id))\n\t            # import ipdb; ipdb.set_trace()\n\t            reserve_aspect_id = input_id[prompt_begin_index:prompt_begin_index+6*aspect_num]\n\t            if aspect_num ==5:\n\t                # print('aspect_num is 5')\n", "                # print(reserve_aspect_id)\n\t                new_input_id = torch.cat([input_id[:prompt_begin_index], reserve_aspect_id, input_id[prompt_end_index+1:]])\n\t            else:\n\t                cut_aspect_id = torch.ones_like(input_id[prompt_begin_index+6*aspect_num:prompt_end_index])\n\t                new_input_id = torch.cat([input_id[:prompt_begin_index], reserve_aspect_id, cut_aspect_id, input_id[prompt_end_index:]])\n\t            # print(\"++++++++++++++++++++cut_aspect_id++++++++++++++++++++++++\")\n\t            # print(cut_aspect_id)\n\t            # print(input_id[58:])\n\t            new_input_ids.append(new_input_id)\n\t            # print('the shape of new_input_id is {}'.format(new_input_id.shape))\n", "            # print(new_input_id[58:])\n\t        new_input_ids = torch.stack(new_input_ids)\n\t        if self.use_generated_aspect_prompt:\n\t            ##aspect_prompt\n\t            # import ipdb; ipdb.set_trace()\n\t            aspect_prompt_mask = (new_input_ids == self.aspect_prompt_token_id) ##[29:58]: 一共5组:[50288, 50288,     9, 50289,  5702, 50284,]\n\t            if self.use_different_aspect_prompt:\n\t                # self.aspect_linear = self.aspect_linear.to(device)\n\t                # self.aspect_relu = self.aspect_relu.to(device)\n\t                for index in range(len(aspects_num)):\n", "                    aspect_num = aspects_num[index]\n\t                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n\t                    aspect_prompt_embedding_list = []\n\t                    for j in range(aspect_num):\n\t                        aspect_linear = nn.Linear(768, 768).to(generated_aspect_prompt.device) ##每个aspect有自己的变换，为每个aspect设计特定的prompt\n\t                        aspect_relu = nn.LeakyReLU().to(generated_aspect_prompt.device)\n\t                        aspect_prompt_embedding = aspect_linear(generated_aspect_prompt[index])\n\t                        aspect_prompt_embedding = aspect_relu(aspect_prompt_embedding)\n\t                        aspect_prompt_embedding_list.append(aspect_prompt_embedding)\n\t                    aspect_prompt_embedding_ = torch.cat(aspect_prompt_embedding_list, dim=0)\n", "                    embedded[index, aspect_prompt_mask[index]] = aspect_prompt_embedding_\n\t            else:\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    aspect_prompt_embedding_ = generated_aspect_prompt[index].repeat(aspect_num, 1)\n\t                    embedded[index, aspect_prompt_mask[index]] = aspect_prompt_embedding_\n\t        ##sentiment_prompt\n\t        if self.use_generated_senti_prompt:\n\t            '''\n\t            # if self.use_different_senti_prompt:\n", "            以下使用的是attention机制，senti_prompt_token和sentiments_embdedding\n\t            # sentiments_ids = torch.tensor([self.neu_id, self.pos_id, self.neg_id]).to(device)\n\t            # sentiments_embdedding = self.embed_tokens(sentiments_ids)\n\t            # senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n\t            # for index in range(len(aspects_num)):\n\t            #     aspect_num = aspects_num[index]\n\t            #     expanded_sentiments_embdedding = sentiments_embdedding.expand(aspect_num, sentiments_embdedding.size(0), sentiments_embdedding.size(1))\n\t            #     original_senti_prompt = embedded[index, senti_prompt_mask[index]].unsqueeze(1)\n\t            #     new_senti_prompt = self.attention_for_senti_prompt(original_senti_prompt, expanded_sentiments_embdedding, expanded_sentiments_embdedding).squeeze()\n\t            #     # import ipdb; ipdb.set_trace()\n", "            #     embedded[index, senti_prompt_mask[index]] = new_senti_prompt\n\t            '''\n\t            ##换成senti_prompt也是生成形式看看\n\t            senti_prompt_mask = (new_input_ids == self.senti_prompt_token_id)\n\t            # import ipdb; ipdb.set_trace()\n\t            if self.use_different_senti_prompt:\n\t                # self.aspect_linear = self.aspect_linear.to(generated_senti_prompt.device)\n\t                # self.aspect_relu = self.aspect_relu.to(generated_senti_prompt.device)\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n", "                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n\t                    prompt_embedding_list = []\n\t                    for j in range(aspect_num):\n\t                        senti_linear = nn.Linear(768, 768).to(generated_senti_prompt.device)\n\t                        senti_relu = nn.LeakyReLU().to(generated_senti_prompt.device)\n\t                        prompt_embedding = senti_linear(generated_senti_prompt[index])\n\t                        prompt_embedding = senti_relu(prompt_embedding)\n\t                        prompt_embedding_list.append(prompt_embedding)\n\t                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n\t                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n", "            else:\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    prompt_embedding_ = generated_senti_prompt[index].repeat(aspect_num, 1)\n\t                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n\t        return embedded\n\t    def forward(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n", "                generated_aspect_prompt=None,\n\t                generated_senti_prompt=None,\n\t                aspects_num=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n\t        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n\t        :param image_features: list[FloatTensor], image roi features with length of batch\n\t        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n", "        :param output_attentions:\n\t        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n\t              Only populated if output_hidden_states: is True.\n\t            - all_attentions (List[Tensor]): Attention weights for each layer.\n\t            During training might not be of length n_layers because of layer dropout.\n", "        \"\"\"\n\t        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(generated_aspect_prompt, generated_senti_prompt, aspects_num,\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n\t        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n", "        # B x T x C -> T x B x C\n\t        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n\t                                  self.layerdrop):  # skip the layer\n", "                attn = None\n\t            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n\t        if self.layer_norm:\n\t            x = self.layer_norm(x)\n\t        if output_hidden_states:\n", "            encoder_states.append(x)\n\t        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n\t        return BaseModelOutput(last_hidden_state=x,\n", "                               hidden_states=encoder_states,\n\t                               attentions=all_attentions)\n\tclass MultiModalBartDecoder_span(nn.Module\n\t                                 ):  #AOE task and all downstream tasks\n\t    def __init__(self,\n\t                 config: MultiModalBartConfig,\n\t                 tokenizer,\n\t                 decoder,\n\t                 pad_token_id,\n\t                 label_ids,\n", "                 causal_mask,\n\t                 num_image_tokens=2,\n\t                 need_tag=True,\n\t                 only_sc=False,\n\t                 avg_feature=False,\n\t                 use_encoder_mlp=True):\n\t        super().__init__()\n\t        self.decoder = decoder\n\t        self.tokenizer = tokenizer\n\t        self.causal_mask = causal_mask\n", "        self.register_buffer('causal_masks', causal_mask.float())\n\t        self.pad_token_id = pad_token_id\n\t        # label_ids = sorted(label_ids, reverse=False)\n\t        self.label_start_id = min(label_ids)\n\t        self.label_end_id = max(label_ids) + 1\n\t        self.need_tag = need_tag\n\t        self.only_sc = only_sc\n\t        self.num_image_tokens = num_image_tokens\n\t        mapping = torch.LongTensor([0, 2] + label_ids)\n\t        ###mapping: [0, 2, 50276, 50277, 50278, 50281]\n", "        self.register_buffer('mapping', mapping)\n\t        self.src_start_index = len(mapping)  # 加上一个\n\t        hidden_size = decoder.embed_tokens.weight.size(1)\n\t        self.dropout_layer = nn.Dropout(0.1)\n\t        self.end_text_id = tokenizer.end_text_id\n\t        self.avg_feature = avg_feature\n\t        if use_encoder_mlp:\n\t            self.encoder_mlp = nn.Sequential(\n\t                nn.Linear(hidden_size, hidden_size), nn.Dropout(0.3),\n\t                nn.ReLU(), nn.Linear(hidden_size, hidden_size))\n", "    def forward(self, tokens, state, only_sc=False):\n\t        # import ipdb; ipdb.set_trace()\n\t        '''\n\t        tokens: [[0, 2, 2, 16, 16, 4, 18, 18, 4, 1, 1, 1, 1],\n\t                 [0, 2, 2, 15, 16, 3, 25, 26, 5, 28, 28, 4, 1]]\n\t        '''\n\t        # import ipdb; ipdb.set_trace()\n\t        bsz, max_len = tokens.size()\n\t        encoder_outputs = state.encoder_output ##(batch, 72=38(len(image_token+begin_image+end_image(36+1+1)))+34(max_tex_len(包含begin_text_id(0) and end_text_id(2)) in batch), 768)\n\t        encoder_pad_mask = state.encoder_mask ##(batch, 72)\n", "        first = state.first\n\t        # tokens之后的0全是padding，因为1是eos, 在pipe中规定的\n\t        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n\t        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n\t        # 把输入做一下映射\n\t        mapping_token_mask = tokens.lt(\n\t            self.src_start_index)  # 为1的地方应该从mapping中取index\n\t        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n\t        tag_mapped_tokens = self.mapping[mapped_tokens]\n\t        src_tokens_index = tokens - self.src_start_index  # bsz x num_src_token\n", "        src_tokens_index = src_tokens_index.masked_fill(\n\t            src_tokens_index.lt(0), 0)\n\t        src_tokens = state.src_tokens \n\t        # print(src_tokens.shape): (2, 34)\n\t        if first is not None:\n\t            src_tokens = src_tokens.gather(index=first, dim=1) ###Sequence\n\t        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n\t        # print('word_mapped_tokens', word_mapped_tokens)\n\t        tokens = torch.where(mapping_token_mask, tag_mapped_tokens,\n\t                             word_mapped_tokens)\n", "        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n\t        '''\n\t        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n\t        tensor([[0, 50276, 50276, 4644, 4644, 50278, 798, 798, 50278, 2, 1, 1, 1],\n\t                [0, 50276, 50276, 9517, 957, 50277, 2561, 7772, 50281, 2762, 2762, 50278, 2]])\n\t        将tokens中的index以及标签都转化为vocabulary中的token_id\n\t        '''\n\t        if self.training:\n\t            tokens = tokens[:, :-1]\n\t            decoder_pad_mask = tokens.eq(\n", "                self.pad_token_id)  # decoder需要让pad位置为1\n\t            dict = self.decoder(input_ids=tokens,\n\t                                encoder_hidden_states=encoder_outputs,\n\t                                encoder_padding_mask=encoder_pad_mask,\n\t                                decoder_padding_mask=decoder_pad_mask,\n\t                                decoder_causal_mask=self.\n\t                                causal_masks[:tokens.size(1), :tokens.size(1)],\n\t                                return_dict=True)\n\t        else:\n\t            past_key_values = state.past_key_values\n", "            dict = self.decoder(input_ids=tokens,\n\t                                encoder_hidden_states=encoder_outputs,\n\t                                encoder_padding_mask=encoder_pad_mask,\n\t                                decoder_padding_mask=None,\n\t                                decoder_causal_mask=self.\n\t                                causal_masks[:tokens.size(1), :tokens.size(1)],\n\t                                return_dict=True)\n\t        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size (2, 12(去掉了 end_token_id), 768)\n\t        hidden_state = self.dropout_layer(hidden_state)\n\t        if not self.training:\n", "            state.past_key_values = dict.past_key_values\n\t        logits = hidden_state.new_full(\n\t            (hidden_state.size(0), hidden_state.size(1),\n\t             self.src_start_index + src_tokens.size(-1)),\n\t            fill_value=-1e24)\n\t        ##建立空的logits\n\t        # print('logits', logits.shape) (bsz, max_len,  self.src_start_index + src_tokens.size(-1)) -> (2, 12, 40=6+34)\n\t        # 首先计算的是\n\t        if self.need_tag:\n\t            '''\n", "            self.decoder.embed_tokens.weight: (50289, 768)\n\t            self.label_start_id: 50276\n\t            '''\n\t            tag_scores = F.linear(\n\t                hidden_state,\n\t                self.dropout_layer(\n\t                    self.decoder.embed_tokens.\n\t                    weight[self.label_start_id:self.label_start_id +\n\t                           3]))  # bsz x max_len x num_class\n\t            logits[:, :, 3:self.src_start_index] = tag_scores ###给情感的position赋值[:, :, (3, 4, 5)]\n", "        if not only_sc:\n\t            eos_scores = F.linear(\n\t                hidden_state,\n\t                self.dropout_layer(self.decoder.embed_tokens.weight[2:3])) \n\t            '''\n\t            ['</s>(eos_token)', '<mask>', '<pad>', '<s>(bos_token)', '<unk>']\n\t            [2, 50264, 1, 0, 3]\n\t            '''\n\t            # bsz x max_bpe_len(image_len + text_len) x hidden_size: (2, 72, 768)\n\t            src_outputs = state.encoder_output \n", "            if self.num_image_tokens==0:\n\t                end_index = 62\n\t            elif self.num_image_tokens==1:\n\t                end_index = 63\n\t            elif self.num_image_tokens==2:\n\t                end_index = 64\n\t            elif self.num_image_tokens==3:\n\t                end_index = 65\n\t            elif self.num_image_tokens==4:\n\t                end_index = 66\n", "            elif self.num_image_tokens==5:\n\t                end_index = 67\n\t            elif self.num_image_tokens==6:\n\t                end_index = 68\n\t            elif self.num_image_tokens==7:\n\t                end_index = 69\n\t            if hasattr(self, 'encoder_mlp') and not only_sc:\n\t                src_outputs = self.encoder_mlp(src_outputs)\n\t            if first is not None:\n\t                mask = first.eq(0)\n", "                src_outputs = src_outputs.gather(\n\t                    index=first.unsqueeze(2).repeat(1, 1,\n\t                                                    src_outputs.size(-1)),\n\t                    dim=1)\n\t            else:\n\t                mask = state.encoder_mask[:, end_index:].eq(0)\n\t                # src_outputs = self.decoder.embed_tokens(src_tokens)\n\t            mask = mask.unsqueeze(1) ## bsz x 1 x max_word_len: (2, 1, 34)\n\t            input_embed = self.decoder.embed_tokens(\n\t                src_tokens)  #bsz x max_word_len x hidden_size: (2, 34, 768); src_tokens: (2, 34)\n", "            input_embed = self.dropout_layer(input_embed)\n\t            if self.avg_feature:  # 先把feature合并一下\n\t                src_outputs = (src_outputs[:, end_index:] + input_embed) / 2\n\t            word_scores = torch.einsum(\n\t                'blh,bnh->bln', hidden_state,\n\t                src_outputs[:, end_index:])  # bsz x max_len x max_word_len: (2, 12, 34)\n\t            if not self.avg_feature:\n\t                gen_scores = torch.einsum(\n\t                    'blh,bnh->bln', hidden_state,\n\t                    input_embed)  # bsz x max_len x max_word_len: (2, 12, 34)\n", "                word_scores = (gen_scores + word_scores) / 2 \n\t            mask = mask.__or__(\n\t                src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1)) ###(2, 1, 34)\n\t            word_scores = word_scores.masked_fill(mask, -1e32) ###(bts, max_len, max_word_len)\n\t            logits[:, :, self.src_start_index:] = word_scores\n\t            ###logits.shape (bts, max_len, max_word_len+6): (2, 12, 40)\n\t            logits[:, :, 1:2] = eos_scores\n\t        # print(torch.argmax(logits[0], dim=-1))\n\t        return logits\n\t    def decode(self, tokens, state, only_sc=False):\n", "        return self(tokens, state, only_sc)[:, -1]\n\tclass Span_loss(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t        # self.loss_fct = nn.CrossEntropyLoss()\n\t        self.fc = nn.LogSoftmax(dim=-1)\n\t    def forward(self, tgt_tokens, pred, mask):\n\t        '''\n\t        tgt_tokens: (2 (batch-size), 12 (max_len+1))\n\t        pred: (2, 12, 40 (max_word_len))\n", "        '''\n\t        tgt_tokens = tgt_tokens.masked_fill(mask.eq(0), -100)\n\t        output = F.cross_entropy(target=tgt_tokens, input=pred.transpose(1, 2)) ##每一个词都有12种类别， input= (40, 12)\n\t        return output\n\tclass MultiModalBartDecoder_MLM(nn.Module):\n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.register_buffer(\n", "            \"final_logits_bias\",\n\t            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\t    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n\t        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n\t            self.config,\n\t            input_ids,\n\t            decoder_input_ids=decoder_input_ids,\n\t            decoder_padding_mask=decoder_attention_mask,\n\t            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n", "        decoder_outputs = self.decoder(\n\t            decoder_input_ids,\n\t            encoder_outputs,\n\t            attention_mask,\n\t            decoder_padding_mask,\n\t            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n\t                                            decoder_input_ids.size(1)],\n\t        )\n\t        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n\t                             self.decoder.embed_tokens.weight,\n", "                             bias=self.final_logits_bias)\n\t        lm_loss = 0\n\t        # compute lm loss if labels is given\n\t        if labels is not None:\n\t            labels = labels.clone()\n\t            loss_fct = nn.CrossEntropyLoss()\n\t            lm_loss = loss_fct(\n\t                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n\t                labels.reshape(-1))\n\t            return lm_loss\n", "class MultiModalBartDecoder_ANP_generate(nn.Module):  #AOG task\n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.register_buffer(\n\t            \"final_logits_bias\",\n\t            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\t    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n", "        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n\t            self.config,\n\t            input_ids,\n\t            decoder_input_ids=decoder_input_ids,\n\t            decoder_padding_mask=decoder_attention_mask,\n\t            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n\t        decoder_outputs = self.decoder(\n\t            decoder_input_ids,\n\t            encoder_outputs,\n\t            attention_mask,\n", "            decoder_padding_mask,\n\t            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n\t                                            decoder_input_ids.size(1)],\n\t        )\n\t        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n\t                             self.decoder.embed_tokens.weight,\n\t                             bias=self.final_logits_bias)\n\t        lm_loss = 0\n\t        # compute lm loss if labels is given\n\t        if labels is not None:\n", "            labels = labels.clone()\n\t            # labels[labels == self.cls_token_id] = -100\n\t            loss_fct = nn.CrossEntropyLoss()\n\t            lm_loss = loss_fct(\n\t                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n\t                labels.reshape(-1))\n\t            return lm_loss\n\tclass MultiModalBartDecoder_sentiment(nn.Module):  #MSP task\n\t    def __init__(self,\n\t                 config: MultiModalBartConfig,\n", "                 decoder,\n\t                 senti_ids,\n\t                 senti_nums=3):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.senti_ids = senti_ids\n\t        self.dropout_layer = nn.Dropout(0.1)\n\t        self.senti_head = BartClassificationHead(config.d_model,\n\t                                                 config.d_model, senti_nums,\n", "                                                 config.classif_dropout)\n\t    def _init_weights(self, module):\n\t        module.weight.data.normal_(mean=0.0, std=0.02)\n\t        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    def forward(self, senti_labels, encoder_outputs, attention_mask,\n\t                senti_decoder_input_ids):\n\t        decoder_outputs = self.decoder(\n\t            input_ids=senti_decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n", "            encoder_padding_mask=attention_mask,\n\t            decoder_padding_mask=None,\n\t            decoder_causal_mask=None,\n\t        )\n\t        # predict_senti = F.linear(\n\t        #     decoder_outputs[0][:, 1],\n\t        #     self.dropout_layer(self.decoder.embed_tokens.\n\t        #                        weight[self.senti_ids[0]:self.senti_ids[2] +\n\t        #                               1]))  # bsz\n\t        # predict_senti = torch.flip(predict_senti, dims=[-1])\n", "        predict_senti = self.senti_head(decoder_outputs[0][:, 1])\n\t        loss_fct = nn.CrossEntropyLoss()\n\t        senti_loss = loss_fct(predict_senti, senti_labels)\n\t        return senti_loss, predict_senti\n\tclass MultiModalBartDecoder_MRM(nn.Module):\n\t    def __init__(self, config: MultiModalBartConfig, decoder, causal_mask,\n\t                 args):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n", "        self.causal_mask = causal_mask\n\t        self.args = args\n\t        self.mrm_head = BartClassificationHead(\n\t            config.d_model,\n\t            config.d_model,\n\t            config.num_labels,\n\t            config.classif_dropout,\n\t        )\n\t        self._init_weights(self.mrm_head.dense)\n\t        self._init_weights(self.mrm_head.out_proj)\n", "    def _init_weights(self, module):\n\t        module.weight.data.normal_(mean=0.0, std=0.02)\n\t        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    def forward(self, mrm_labels, mrm_masks, encoder_outputs, attention_mask,\n\t                mrm_decoder_input_ids, mrm_decoder_attention_mask):\n\t        decoder_padding_mask = mrm_decoder_attention_mask.eq(0)\n\t        decoder_outputs = self.decoder(\n\t            input_ids=mrm_decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n", "            encoder_padding_mask=attention_mask,\n\t            decoder_padding_mask=decoder_padding_mask,\n\t            decoder_causal_mask=self.causal_mask[:mrm_decoder_input_ids.size(\n\t                1), :mrm_decoder_input_ids.size(1)].to(\n\t                    mrm_decoder_input_ids.device),\n\t        )\n\t        region_representation = decoder_outputs[0][mrm_masks.bool()]\n\t        if len(region_representation) > 0:\n\t            predict_cls = self.mrm_head(region_representation)\n\t            loss_fct = nn.CrossEntropyLoss()\n", "            mrm_labels = torch.cat(mrm_labels,\n\t                                   dim=0).to(encoder_outputs.device)\n\t            if self.args.mrm_loss_type == 'KL':\n\t                predict_cls = F.log_softmax(predict_cls, dim=-1)\n\t                mrm_loss = F.kl_div(predict_cls.double(),\n\t                                    mrm_labels.double().squeeze(1),\n\t                                    reduction='batchmean')\n\t            else:\n\t                raise RuntimeError(\"wrong mrm type\")\n\t        else:\n", "            mrm_loss = 0\n\t        return mrm_loss\n\t'''\n\tgenerate_aspect_prompt based on the multimodal context\n\t'''\n\tclass MultiModalBartDecoder_generate_aspect_prompt(nn.Module): \n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n", "        self.aspect_prompt_linear = nn.Linear(768, 768)\n\t    def forward(self, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n\t        # import ipdb; ipdb.set_trace()\n\t        decoder_outputs = self.decoder(\n\t            input_ids=decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask.eq(0),\n\t            decoder_padding_mask=decoder_attention_mask.eq(0),\n\t            decoder_causal_mask=None,\n", "        )\n\t        prompt_logits = decoder_outputs[0]\n\t        aspect_prompt_logits = self.aspect_prompt_linear(prompt_logits)\n\t        return aspect_prompt_logits \n\t'''\n\tgenerate_sentiment_prompt based on the multimodal context\n\t'''\n\tclass MultiModalBartDecoder_generate_sentiment_prompt(nn.Module): \n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n", "        self.config = config\n\t        self.decoder = decoder\n\t        self.senti_prompt_linear = nn.Linear(768, 768)\n\t    def forward(self, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n\t        # import ipdb; ipdb.set_trace()\n\t        decoder_outputs = self.decoder(\n\t            input_ids=decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask.eq(0),\n", "            decoder_padding_mask=decoder_attention_mask.eq(0),\n\t            decoder_causal_mask=None,\n\t        )\n\t        prompt_logits = decoder_outputs[0]\n\t        senti_prompt_logits = self.senti_prompt_linear(prompt_logits)\n\t        return senti_prompt_logits\n\tclass MultiModalBartDecoder_aspects_num(nn.Module):  #MSP task\n\t    def __init__(self,\n\t                 config: MultiModalBartConfig,\n\t                 decoder,\n", "                 max_aspects_nums=5):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.dropout_layer = nn.Dropout(0.1)\n\t        self.aspects_num_head = BartClassificationHead(config.d_model,\n\t                                                 config.d_model, max_aspects_nums,\n\t                                                 config.classif_dropout)\n\t    def _init_weights(self, module):\n\t        module.weight.data.normal_(mean=0.0, std=0.02)\n", "        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    def forward(self, aspects_num_labels, encoder_outputs, attention_mask,\n\t                aspects_num_decoder_input_ids):\n\t        decoder_outputs = self.decoder(\n\t            input_ids=aspects_num_decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask,\n\t            decoder_padding_mask=None,\n\t            decoder_causal_mask=None,\n", "        )\n\t        # predict_aspects_num = F.linear(\n\t        #     decoder_outputs[0][:, 1],\n\t        #     self.dropout_layer(self.decoder.embed_tokens.\n\t        #                        weight[self.aspects_num_ids[0]:self.aspects_num_ids[2] +\n\t        #                               1]))  # bsz\n\t        # predict_aspects_num = torch.flip(predict_aspects_num, dims=[-1])\n\t        predict_aspects_num_logits = self.aspects_num_head(decoder_outputs[0][:, 1])\n\t        loss_fct = nn.CrossEntropyLoss()\n\t        aspects_num_labels = torch.tensor(aspects_num_labels).to(predict_aspects_num_logits.device)\n", "        aspects_num_loss = loss_fct(predict_aspects_num_logits, aspects_num_labels)\n\t        return aspects_num_loss, predict_aspects_num_logits"]}
{"filename": "src/model/mixins.py", "chunked_list": ["import os\n\timport itertools\n\tfrom typing import Optional\n\timport torch\n\tfrom torch import nn\n\tfrom torch.nn import Parameter\n\t# from transformers.generation_utils import logger, Iterable\n\tfrom typing import Iterable\n\timport logging as logger\n\t# from transformers.models.bart.modeling_bart import _make_linear_from_emb\n", "# from transformers.modeling_bart import (\n\t#     _reorder_buffer,\n\t#     _make_linear_from_emb\n\t# )\n\t# from transformers.utils.hub import hf_bucket_url\n\tfrom transformers.modeling_utils import (\n\t    # hf_bucket_url,\n\t    # cached_path,\n\t    TF2_WEIGHTS_NAME,\n\t    WEIGHTS_NAME,\n", "    TF_WEIGHTS_NAME,\n\t    is_remote_url,\n\t    PretrainedConfig\n\t)\n\tdef _make_linear_from_emb(emb):\n\t    vocab_size, emb_size = emb.weight.shape\n\t    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n\t    lin_layer.weight.data = emb.weight.data\n\t    return lin_layer\n\tdef _reorder_buffer(attn_cache, new_order):\n", "    for k, input_buffer_k in attn_cache.items():\n\t        if input_buffer_k is not None:\n\t            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n\t    return attn_cache\n\t# This is based on transformers.generation_utils\n\t# The modifications are:\n\t# - image_features parameter\n\t# - removed unused code\n\t# - added image_features in prepare_inputs_for_generation\n\tclass GenerationMixin:\n", "    @torch.no_grad()\n\t    def generate(\n\t            self,\n\t            input_ids: Optional[torch.LongTensor] = None,\n\t            image_features=None,\n\t            max_length: Optional[int] = None,\n\t            min_length: Optional[int] = None,\n\t            do_sample: Optional[bool] = None,\n\t            early_stopping: Optional[bool] = None,\n\t            num_beams: Optional[int] = None,\n", "            temperature: Optional[float] = None,\n\t            top_k: Optional[int] = None,\n\t            top_p: Optional[float] = None,\n\t            repetition_penalty: Optional[float] = None,\n\t            bad_words_ids: Optional[Iterable[int]] = None,\n\t            bos_token_id: Optional[int] = None,\n\t            pad_token_id: Optional[int] = None,\n\t            eos_token_id: Optional[int] = None,\n\t            length_penalty: Optional[float] = None,\n\t            no_repeat_ngram_size: Optional[int] = None,\n", "            num_return_sequences: Optional[int] = None,\n\t            attention_mask: Optional[torch.LongTensor] = None,\n\t            decoder_start_token_id: Optional[int] = None,\n\t            use_cache: Optional[bool] = None,\n\t            **model_specific_kwargs\n\t    ) -> torch.LongTensor:\n\t        \"\"\"\n\t        Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search\n\t        decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n\t        Parameters:\n", "            input_ids: (optional) torch.LongTensor of shape (batch_size, sequence_length)\n\t                The sequence used as a prompt for the generation. If None the method initializes\n\t                it as an empty torch.LongTensor of shape (1,).\n\t            image_features: (optional) list of torch.LongTensor\n\t                The image ROI features generated by R-CNN\n\t            max_length: (optional) int\n\t                The max length of the sequence to be generated.  Between min_length and infinity. Default to 20.\n\t            min_length: (optional) int\n\t                The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\n\t            do_sample: (optional) bool\n", "                If set to False greedy decoding is used. Otherwise sampling is used. Defaults to False as defined\n\t                in configuration_utils.PretrainedConfig.\n\t            early_stopping: (optional) bool\n\t                if set to True beam search is stopped when at least num_beams sentences finished per batch.\n\t                Defaults to False as defined in configuration_utils.PretrainedConfig.\n\t            num_beams: (optional) int\n\t                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\t            temperature: (optional) float\n\t                The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n\t            top_k: (optional) int\n", "                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity.\n\t                Default to 50.\n\t            top_p: (optional) float\n\t                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus\n\t                sampling. Must be between 0 and 1. Default to 1.\n\t            repetition_penalty: (optional) float\n\t                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\t            pad_token_id: (optional) int\n\t                Padding token. Default to specicic model pad_token_id or None if it does not exist.\n\t            bos_token_id: (optional) int\n", "                BOS token. Defaults to bos_token_id as defined in the models config.\n\t            eos_token_id: (optional) int\n\t                EOS token. Defaults to eos_token_id as defined in the models config.\n\t            length_penalty: (optional) float\n\t                Exponential penalty to the length. Default to 1.\n\t            no_repeat_ngram_size: (optional) int\n\t                If set to int > 0, all ngrams of size no_repeat_ngram_size can only occur once.\n\t            bad_words_ids: (optional) list of lists of int\n\t                bad_words_ids contains tokens that are not allowed to be generated. In order to get the tokens of the\n\t                words that should not appear in the generated text, use\n", "                tokenizer.encode(bad_word, add_prefix_space=True).\n\t            num_return_sequences: (optional) int\n\t                The number of independently computed returned sequences for each element in the batch. Default to 1.\n\t            attention_mask (optional) obj: torch.LongTensor of same shape as input_ids\n\t                Mask to avoid performing attention on padding token indices.\n\t                Mask values selected in [0, 1]:\n\t                1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n\t                Defaults to None.\n\t                What are attention masks? <../glossary.html#attention-mask>__\n\t            decoder_start_token_id=None: (optional) int\n", "                If an encoder-decoder model starts decoding with a different token than BOS.\n\t                Defaults to None and is changed to BOS later.\n\t            use_cache: (optional) bool\n\t                If use_cache is True, past key values are used to speed up decoding if applicable to model.\n\t                Defaults to True.\n\t            model_specific_kwargs: (optional) dict\n\t                Additional model specific kwargs will be forwarded to the forward function of the model.\n\t        Return:\n\t            output: torch.LongTensor of shape (batch_size * num_return_sequences, sequence_length)\n\t                sequence_length is either equal to max_length or shorter if all batches finished early due to\n", "                the eos_token_id\n\t        \"\"\"\n\t        max_length = max_length if max_length is not None else self.config.max_length\n\t        min_length = min_length if min_length is not None else self.config.min_length\n\t        do_sample = do_sample if do_sample is not None else self.config.do_sample\n\t        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n\t        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\t        num_beams = num_beams if num_beams is not None else self.config.num_beams\n\t        temperature = temperature if temperature is not None else self.config.temperature\n\t        top_k = top_k if top_k is not None else self.config.top_k\n", "        top_p = top_p if top_p is not None else self.config.top_p\n\t        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n\t        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n\t        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n\t        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n\t        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n\t        no_repeat_ngram_size = (\n\t            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n\t        )\n\t        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\n", "        num_return_sequences = (\n\t            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n\t        )\n\t        decoder_start_token_id = (\n\t            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id\n\t        )\n\t        if input_ids is not None:\n\t            batch_size = input_ids.shape[0]  # overriden by the input batch_size\n\t        else:\n\t            batch_size = 1\n", "        assert isinstance(max_length, int) and max_length > 0, \"max_length should be a strictly positive integer.\"\n\t        assert isinstance(min_length, int) and min_length >= 0, \"min_length should be a positive integer.\"\n\t        assert isinstance(do_sample, bool), \"do_sample should be a boolean.\"\n\t        assert isinstance(early_stopping, bool), \"early_stopping should be a boolean.\"\n\t        assert isinstance(use_cache, bool), \"use_cache should be a boolean.\"\n\t        assert isinstance(num_beams, int) and num_beams > 0, \"num_beams should be a strictly positive integer.\"\n\t        assert temperature > 0, \"temperature should be strictly positive.\"\n\t        assert isinstance(top_k, int) and top_k >= 0, \"top_k should be a positive integer.\"\n\t        assert 0 <= top_p <= 1, \"top_p should be between 0 and 1.\"\n\t        assert repetition_penalty >= 1.0, \"repetition_penalty should be >= 1.\"\n", "        assert input_ids is not None or (\n\t                isinstance(bos_token_id, int) and bos_token_id >= 0\n\t        ), \"If input_ids is not defined, bos_token_id should be a positive integer.\"\n\t        assert pad_token_id is None or (\n\t                isinstance(pad_token_id, int) and (pad_token_id >= 0)\n\t        ), \"pad_token_id should be a positive integer.\"\n\t        assert (eos_token_id is None) or (\n\t                isinstance(eos_token_id, int) and (eos_token_id >= 0)\n\t        ), \"eos_token_id should be a positive integer.\"\n\t        assert length_penalty > 0, \"length_penalty should be strictly positive.\"\n", "        assert (\n\t                isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0\n\t        ), \"no_repeat_ngram_size should be a positive integer.\"\n\t        assert (\n\t                isinstance(num_return_sequences, int) and num_return_sequences > 0\n\t        ), \"num_return_sequences should be a strictly positive integer.\"\n\t        assert (\n\t                bad_words_ids is None or isinstance(bad_words_ids, list) and isinstance(bad_words_ids[0], list)\n\t        ), \"bad_words_ids is either None or a list of lists of tokens that should not be generated\"\n\t        if input_ids is None:\n", "            assert isinstance(bos_token_id, int) and bos_token_id >= 0, (\n\t                \"you should either supply a context to complete as input_ids input \"\n\t                \"or a bos_token_id (integer >= 0) as a first token to start the generation.\"\n\t            )\n\t            input_ids = torch.full(\n\t                (batch_size, 1), bos_token_id, dtype=torch.long, device=next(self.parameters()).device,\n\t            )\n\t        else:\n\t            assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\n\t        # not allow to duplicate outputs when greedy decoding\n", "        if do_sample is False:\n\t            if num_beams == 1:\n\t                # no_beam_search greedy generation conditions\n\t                assert (\n\t                        num_return_sequences == 1\n\t                ), \"Greedy decoding will always produce the same output for num_beams == 1 and \" \\\n\t                   \"num_return_sequences > 1. Please set num_return_sequences = 1\"\n\t            else:\n\t                # beam_search greedy generation conditions\n\t                assert (\n", "                        num_beams >= num_return_sequences\n\t                ), \"Greedy beam search decoding cannot return more sequences than it has beams. \" \\\n\t                   \"Please set num_beams >= num_return_sequences\"\n\t        # create attention mask if necessary\n\t        # TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140\n\t        if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids):\n\t            attention_mask = input_ids.ne(pad_token_id).long()\n\t        elif attention_mask is None:\n\t            attention_mask = input_ids.new_ones(input_ids.shape)\n\t        # set pad_token_id to eos_token_id if not set. Important that this is done after\n", "        # attention_mask is created\n\t        if pad_token_id is None and eos_token_id is not None:\n\t            logger.warning(\n\t                \"Setting pad_token_id to {} (first eos_token_id) to generate sequence\".format(eos_token_id)\n\t            )\n\t            pad_token_id = eos_token_id\n\t        # current position and vocab size\n\t        if hasattr(self.config, \"vocab_size\"):\n\t            vocab_size = self.config.vocab_size\n\t        elif (\n", "                self.config.is_encoder_decoder\n\t                and hasattr(self.config, \"decoder\")\n\t                and hasattr(self.config.decoder, \"vocab_size\")\n\t        ):\n\t            vocab_size = self.config.decoder.vocab_size\n\t        # set effective batch size and effective batch multiplier according to do_sample\n\t        if do_sample:\n\t            effective_batch_size = batch_size * num_return_sequences\n\t            effective_batch_mult = num_return_sequences\n\t        else:\n", "            effective_batch_size = batch_size\n\t            effective_batch_mult = 1\n\t        if self.config.is_encoder_decoder:\n\t            if decoder_start_token_id is None:\n\t                decoder_start_token_id = bos_token_id\n\t            assert (\n\t                    decoder_start_token_id is not None\n\t            ), \"decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation\"\n\t            assert hasattr(self, \"get_encoder\"), \"{} should have a 'get_encoder' function defined\".format(self)\n\t            assert callable(self.get_encoder), \"{} should be a method\".format(self.get_encoder)\n", "            # get encoder and store encoder outputs\n\t            encoder = self.get_encoder()\n\t            encoder_outputs: tuple = encoder(input_ids, image_features=image_features, attention_mask=attention_mask)\n\t        # Expand input ids if num_beams > 1 or num_return_sequences > 1\n\t        if num_return_sequences > 1 or num_beams > 1:\n\t            input_ids_len = input_ids.shape[-1]\n\t            input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_ids_len)\n\t            attention_mask = attention_mask.unsqueeze(1).expand(\n\t                batch_size, effective_batch_mult * num_beams, input_ids_len\n\t            )\n", "            input_ids = input_ids.contiguous().view(\n\t                effective_batch_size * num_beams, input_ids_len\n\t            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n\t            attention_mask = attention_mask.contiguous().view(\n\t                effective_batch_size * num_beams, input_ids_len\n\t            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n\t        if self.config.is_encoder_decoder:\n\t            # create empty decoder_input_ids\n\t            input_ids = torch.full(\n\t                (effective_batch_size * num_beams, 1),\n", "                decoder_start_token_id,\n\t                dtype=torch.long,\n\t                device=next(self.parameters()).device,\n\t            )\n\t            cur_len = 1\n\t            assert (\n\t                    batch_size == encoder_outputs[0].shape[0]\n\t            ), f\"expected encoder_outputs[0] to have 1st dimension bs={batch_size}, got {encoder_outputs[0].shape[0]} \"\n\t            # expand batch_idx to assign correct encoder output for expanded\n\t            # input_ids (due to num_beams > 1 and num_return_sequences > 1)\n", "            expanded_batch_idxs = (\n\t                torch.arange(batch_size)\n\t                    .view(-1, 1)\n\t                    .repeat(1, num_beams * effective_batch_mult)\n\t                    .view(-1)\n\t                    .to(input_ids.device)\n\t            )\n\t            # expand encoder_outputs\n\t            encoder_outputs = (encoder_outputs[0].index_select(0, expanded_batch_idxs), *encoder_outputs[1:])\n\t        else:\n", "            encoder_outputs = None\n\t            cur_len = input_ids.shape[-1]\n\t        assert (\n\t                cur_len < max_length\n\t        ), f\"The context has {cur_len} number of tokens, but max_length is only {max_length}. \" \\\n\t            f\"Please make sure that max_length is bigger than the number of tokens, \" \\\n\t            f\"by setting either generate(max_length=...,...) or config.max_length = ...\"\n\t        if num_beams > 1:\n\t            output = self._generate_beam_search(\n\t                input_ids,\n", "                cur_len=cur_len,\n\t                max_length=max_length,\n\t                min_length=min_length,\n\t                do_sample=do_sample,\n\t                early_stopping=early_stopping,\n\t                temperature=temperature,\n\t                top_k=top_k,\n\t                top_p=top_p,\n\t                repetition_penalty=repetition_penalty,\n\t                no_repeat_ngram_size=no_repeat_ngram_size,\n", "                bad_words_ids=bad_words_ids,\n\t                pad_token_id=pad_token_id,\n\t                eos_token_id=eos_token_id,\n\t                batch_size=effective_batch_size,\n\t                num_return_sequences=num_return_sequences,\n\t                length_penalty=length_penalty,\n\t                num_beams=num_beams,\n\t                vocab_size=vocab_size,\n\t                encoder_outputs=encoder_outputs,\n\t                attention_mask=attention_mask,\n", "                use_cache=use_cache,\n\t                model_specific_kwargs=model_specific_kwargs,\n\t            )\n\t        else:\n\t            output = self._generate_no_beam_search(\n\t                input_ids,\n\t                cur_len=cur_len,\n\t                max_length=max_length,\n\t                min_length=min_length,\n\t                do_sample=do_sample,\n", "                temperature=temperature,\n\t                top_k=top_k,\n\t                top_p=top_p,\n\t                repetition_penalty=repetition_penalty,\n\t                no_repeat_ngram_size=no_repeat_ngram_size,\n\t                bad_words_ids=bad_words_ids,\n\t                pad_token_id=pad_token_id,\n\t                eos_token_id=eos_token_id,\n\t                batch_size=effective_batch_size,\n\t                encoder_outputs=encoder_outputs,\n", "                attention_mask=attention_mask,\n\t                use_cache=use_cache,\n\t                model_specific_kwargs=model_specific_kwargs,\n\t            )\n\t        return output\n\t    def prepare_inputs_for_generation(self, decoder_input_ids, past, attention_mask, use_cache, **kwargs):\n\t        assert past is not None, \"past has to be defined for encoder_outputs\"\n\t        encoder_outputs, decoder_cached_states = past\n\t        return {\n\t            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n", "            \"image_features\": None,  # encoder_outputs is defined. image_features not needed\n\t            \"encoder_outputs\": encoder_outputs,\n\t            \"decoder_cached_states\": decoder_cached_states,\n\t            \"decoder_input_ids\": decoder_input_ids,\n\t            \"attention_mask\": attention_mask,\n\t            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n\t        }\n\t    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n\t        if cur_len == 1:\n\t            self._force_token_ids_generation(logits, self.config.bos_token_id)\n", "        if cur_len == max_length - 1 and self.config.eos_token_id is not None:\n\t            self._force_token_ids_generation(logits, self.config.eos_token_id)\n\t        return logits\n\t    def _force_token_ids_generation(self, scores, token_ids) -> None:\n\t        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0\"\"\"\n\t        if isinstance(token_ids, int):\n\t            token_ids = [token_ids]\n\t        all_but_token_ids_mask = torch.tensor(\n\t            [x for x in range(self.config.vocab_size) if x not in token_ids],\n\t            dtype=torch.long,\n", "            device=next(self.parameters()).device,\n\t        )\n\t        assert len(scores.shape) == 2, \"scores should be of rank 2 with shape: [batch_size, vocab_size]\"\n\t        scores[:, all_but_token_ids_mask] = -float(\"inf\")\n\t    @staticmethod\n\t    def _reorder_cache(past, beam_idx):\n\t        ((enc_out, enc_mask), decoder_cached_states) = past\n\t        reordered_past = []\n\t        for layer_past in decoder_cached_states:\n\t            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n", "            layer_past_new = {\n\t                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()\n\t            }\n\t            reordered_past.append(layer_past_new)\n\t        new_enc_out = enc_out if enc_out is None else enc_out.index_select(0, beam_idx)\n\t        new_enc_mask = enc_mask if enc_mask is None else enc_mask.index_select(0, beam_idx)\n\t        past = ((new_enc_out, new_enc_mask), reordered_past)\n\t        return past\n\t    def get_encoder(self):\n\t        return self.model.encoder\n", "    def get_output_embeddings(self):\n\t        return _make_linear_from_emb(self.model.shared)  # make it on the fly\n\t    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n\t        old_num_tokens = self.model.shared.num_embeddings\n\t        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n\t        self.model.shared = new_embeddings\n\t        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n\t        return new_embeddings\n\t    def _resize_final_logits_bias(self, new_num_tokens: int, old_num_tokens: int) -> None:\n\t        if new_num_tokens <= old_num_tokens:\n", "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n\t        else:\n\t            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n\t            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n\t        self.register_buffer(\"final_logits_bias\", new_bias)\n\tclass FromPretrainedMixin:\n\t    # This is based on torch.nn.module\n\t    # The modifications are:\n\t    # - if the parameter name is in config.partial_load, load only the part the state_dict saved\n\t    @staticmethod\n", "    def _load_from_state_dict(module, state_dict, prefix, local_metadata, strict,\n\t                              missing_keys, unexpected_keys, error_msgs, partial_loads, config):\n\t        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n\t        this module, but not its descendants. This is called on every submodule\n\t        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n\t        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n\t        For state dicts without metadata, :attr:`local_metadata` is empty.\n\t        Subclasses can achieve class-specific backward compatible loading using\n\t        the version number at `local_metadata.get(\"version\", None)`.\n\t        .. note::\n", "            :attr:`state_dict` is not the same object as the input\n\t            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n\t            it can be modified.\n\t        Arguments:\n\t            state_dict (dict): a dict containing parameters and\n\t                persistent buffers.\n\t            prefix (str): the prefix for parameters and buffers used in this\n\t                module\n\t            local_metadata (dict): a dict containing the metadata for this module.\n\t                See\n", "            strict (bool): whether to strictly enforce that the keys in\n\t                :attr:`state_dict` with :attr:`prefix` match the names of\n\t                parameters and buffers in this module\n\t            missing_keys (list of str): if ``strict=True``, add missing keys to\n\t                this list\n\t            unexpected_keys (list of str): if ``strict=True``, add unexpected\n\t                keys to this list\n\t            error_msgs (list of str): error messages should be added to this\n\t                list, and will be reported together in\n\t                :meth:`~torch.nn.Module.load_state_dict`\n", "        \"\"\"\n\t        for hook in module._load_state_dict_pre_hooks.values():\n\t            hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n\t        local_name_params = itertools.chain(module._parameters.items(), module._buffers.items())\n\t        local_state = {k: v.data for k, v in local_name_params if v is not None}\n\t        for name, param in local_state.items():\n\t            key = prefix + name\n\t            if key in state_dict:\n\t                input_param = state_dict[key]\n\t                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n", "                if len(param.shape) == 0 and len(input_param.shape) == 1:\n\t                    input_param = input_param[0]\n\t                if input_param.shape != param.shape:\n\t                    # local shape should match the one in checkpoint\n\t                    if key in config.partial_load:\n\t                        partial_loads.append('partially loaded parameter {} ({} => {})'\n\t                                             .format(key, input_param.shape, param.shape))\n\t                    else:\n\t                        error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n\t                                          'the shape in current model is {}.'\n", "                                          .format(key, input_param.shape, param.shape))\n\t                        continue\n\t                if isinstance(input_param, Parameter):\n\t                    # backwards compatibility for serialized parameters\n\t                    input_param = input_param.data\n\t                try:\n\t                    if key in config.partial_load:\n\t                        # copy only the parameters that is defined in input_param to param\n\t                        param[tuple(map(slice, input_param.size()))].copy_(input_param)\n\t                    else:\n", "                        param.copy_(input_param)\n\t                except Exception:\n\t                    error_msgs.append('While copying the parameter named \"{}\", '\n\t                                      'whose dimensions in the model are {} and '\n\t                                      'whose dimensions in the checkpoint are {}.'\n\t                                      .format(key, param.size(), input_param.size()))\n\t            elif strict:\n\t                missing_keys.append(key)\n\t        if strict:\n\t            for key in state_dict.keys():\n", "                if key.startswith(prefix):\n\t                    input_name = key[len(prefix):]\n\t                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n\t                    if input_name not in module._modules and input_name not in local_state:\n\t                        unexpected_keys.append(key)\n\t    # This is based on transformers.modeling_utils\n\t    # The modifications are:\n\t    # - changed module._load_from_state_dict to cls._load_from_state_dict\n\t    # - warning on partial loads\n\t    @classmethod\n", "    def from_pretrained(cls, pretrained_model_name_or_path, error_on_mismatch=True, *model_args, **kwargs):\n\t        r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\t        The model is set in evaluation mode by default using model.eval() (Dropout modules are deactivated)\n\t        To train the model, you should first set it back in training mode with model.train()\n\t        The warning Weights from XXX not initialized from pretrained model means that the weights of XXX do not come\n\t        pre-trained with the rest of the model.\n\t        It is up to you to train those weights with a downstream fine-tuning task.\n\t        The warning Weights from XXX not used in YYY means that the layer XXX is not used by YYY, therefore\n\t        those weights are discarded.\n\t        Parameters:\n", "            pretrained_model_name_or_path: either:\n\t              - a string with the shortcut name of a pre-trained model to load from cache or download,\n\t                e.g.: bert-base-uncased.\n\t              - a string with the identifier name of a pre-trained model that was user-uploaded to our S3,\n\t                e.g.: dbmdz/bert-base-german-cased.\n\t              - a path to a directory containing model weights saved using\n\t                :func:~transformers.PreTrainedModel.save_pretrained, e.g.: ./my_model_directory/.\n\t              - a path or url to a tensorflow index checkpoint file (e.g. ./tf_model/model.ckpt.index).\n\t                In this case, from_tf should be set to True and a configuration object should be provided as config\n\t                argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model\n", "                using the provided conversion scripts and loading the PyTorch model afterwards.\n\t              - None if you are both providing the configuration and state dictionary\n\t                (resp. with keyword arguments config and state_dict)\n\t            error_on_mismatch: (optional) boolean:\n\t                Set to False to only warn the mismatch on loading weights, instead of error.\n\t            model_args: (optional) Sequence of positional arguments:\n\t                All remaning positional arguments will be passed to the underlying model's __init__ method\n\t            config: (optional) one of:\n\t                - an instance of a class derived from :class:~transformers.PretrainedConfig, or\n\t                - a string valid as input to :func:~transformers.PretrainedConfig.from_pretrained()\n", "                Configuration for the model to use instead of an automatically loaded configuation.\n\t                Configuration can be automatically loaded when:\n\t                    - the model is a model provided by the library (loaded with the shortcut-name string of\n\t                      a pretrained model), or\n\t                    - the model was saved using :func:~transformers.PreTrainedModel.save_pretrained and is\n\t                      reloaded by suppling the save directory.\n\t                    - the model is loaded by suppling a local directory as pretrained_model_name_or_path and\n\t                      a configuration JSON file named config.json is found in the directory.\n\t            state_dict: (optional) dict:\n\t                an optional state dictionnary for the model to use instead of a state dictionary loaded from\n", "                saved weights file. This option can be used if you want to create a model from a pretrained\n\t                configuration but load your own weights. In this case though, you should check if using\n\t                :func:~transformers.PreTrainedModel.save_pretrained and\n\t                :func:~transformers.PreTrainedModel.from_pretrained is not a simpler option.\n\t            cache_dir: (optional) string:\n\t                Path to a directory in which a downloaded pre-trained model\n\t                configuration should be cached if the standard cache should not be used.\n\t            force_download: (optional) boolean, default False:\n\t                Force to (re-)download the model weights and configuration files and override the cached versions\n\t                if they exists.\n", "            resume_download: (optional) boolean, default False:\n\t                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n\t            proxies: (optional) dict, default None:\n\t                A dictionary of proxy servers to use by protocol or endpoint,\n\t                e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n\t                The proxies are used on each request.\n\t            output_loading_info: (optional) boolean:\n\t                Set to True to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\t            kwargs: (optional) Remaining dictionary of keyword arguments:\n\t                Can be used to update the configuration object (after it being loaded) and initiate the model.\n", "                (e.g. output_attention=True). Behave differently depending on whether a config is provided\n\t                or automatically loaded:\n\t                - If a configuration is provided with config, **kwargs will be directly passed to the underlying\n\t                model's __init__ method (we assume all relevant updates to the configuration have already been done)\n\t                - If a configuration is not provided, kwargs will be first passed to the configuration class\n\t                initialization function (:func:~transformers.PretrainedConfig.from_pretrained).\n\t                Each key of kwargs that corresponds to a configuration attribute will be used to override said\n\t                attribute with the supplied kwargs value. Remaining keys that do not correspond to any\n\t                configuration attribute will be passed to the underlying model's __init__ function.\n\t        \"\"\"\n", "        config = kwargs.pop(\"config\", None)\n\t        state_dict = kwargs.pop(\"state_dict\", None)\n\t        cache_dir = kwargs.pop(\"cache_dir\", None)\n\t        from_tf = kwargs.pop(\"from_tf\", False)\n\t        force_download = kwargs.pop(\"force_download\", False)\n\t        resume_download = kwargs.pop(\"resume_download\", False)\n\t        proxies = kwargs.pop(\"proxies\", None)\n\t        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n\t        local_files_only = kwargs.pop(\"local_files_only\", False)\n\t        use_cdn = kwargs.pop(\"use_cdn\", True)\n", "        # Load config if we don't provide a configuration\n\t        if not isinstance(config, PretrainedConfig):\n\t            config_path = config if config is not None else pretrained_model_name_or_path\n\t            config, model_kwargs = cls.config_class.from_pretrained(\n\t                config_path,\n\t                *model_args,\n\t                cache_dir=cache_dir,\n\t                return_unused_kwargs=True,\n\t                force_download=force_download,\n\t                resume_download=resume_download,\n", "                proxies=proxies,\n\t                local_files_only=local_files_only,\n\t                **kwargs,\n\t            )\n\t        else:\n\t            model_kwargs = kwargs\n\t        # Load model\n\t        if pretrained_model_name_or_path is not None:\n\t            if os.path.isdir(pretrained_model_name_or_path):\n\t                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")):\n", "                    # Load from a TF 1.0 checkpoint\n\t                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n\t                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n\t                    # Load from a TF 2.0 checkpoint\n\t                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n\t                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n\t                    # Load from a PyTorch checkpoint\n\t                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n\t                else:\n\t                    raise EnvironmentError(\n", "                        \"Error no file named {} found in directory {} or from_tf set to False\".format(\n\t                            [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + \".index\"],\n\t                            pretrained_model_name_or_path,\n\t                        )\n\t                    )\n\t            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n\t                archive_file = pretrained_model_name_or_path\n\t            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n\t                assert (\n\t                    from_tf\n", "                ), \"We found a TensorFlow checkpoint at {}, please set \" \\\n\t                   \"from_tf to True to load from this checkpoint\".format(\n\t                    pretrained_model_name_or_path + \".index\"\n\t                )\n\t                archive_file = pretrained_model_name_or_path + \".index\"\n\t            else:\n\t                archive_file = hf_bucket_url(\n\t                    pretrained_model_name_or_path,\n\t                    filename=(TF2_WEIGHTS_NAME if from_tf else WEIGHTS_NAME),\n\t                    use_cdn=use_cdn,\n", "                )\n\t            try:\n\t                # Load from URL or cache if already cached\n\t                resolved_archive_file = cached_path(\n\t                    archive_file,\n\t                    cache_dir=cache_dir,\n\t                    force_download=force_download,\n\t                    proxies=proxies,\n\t                    resume_download=resume_download,\n\t                    local_files_only=local_files_only,\n", "                )\n\t                if resolved_archive_file is None:\n\t                    raise EnvironmentError\n\t            except EnvironmentError:\n\t                msg = (\n\t                    f\"Can't load weights for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n\t                    f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on \"\n\t                    f\"'https://huggingface.co/models'\\n\\n\"\n\t                    f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory \"\n\t                    f\"containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\n", "                )\n\t                raise EnvironmentError(msg)\n\t            if resolved_archive_file == archive_file:\n\t                logger.info(\"loading weights file {}\".format(archive_file))\n\t            else:\n\t                logger.info(\"loading weights file {} from cache at {}\".format(archive_file, resolved_archive_file))\n\t        else:\n\t            resolved_archive_file = None\n\t        # Instantiate model.\n\t        model = cls(config, *model_args, **model_kwargs)\n", "        if state_dict is None and not from_tf:\n\t            try:\n\t                state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n\t            except Exception:\n\t                raise OSError(\n\t                    \"Unable to load weights from pytorch checkpoint file. \"\n\t                    \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n\t                )\n\t        missing_keys = []\n\t        unexpected_keys = []\n", "        error_msgs = []\n\t        partial_loads = []\n\t        if from_tf:\n\t            if resolved_archive_file.endswith(\".index\"):\n\t                # Load from a TensorFlow 1.X checkpoint - provided by original authors\n\t                model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the '.index'\n\t            else:\n\t                # Load from our TensorFlow 2.0 checkpoints\n\t                try:\n\t                    from transformers import load_tf2_checkpoint_in_pytorch_model\n", "                    model = load_tf2_checkpoint_in_pytorch_model(model, resolved_archive_file, allow_missing_keys=True)\n\t                except ImportError:\n\t                    logger.error(\n\t                        \"Loading a TensorFlow model in PyTorch, requires both PyTorch and \"\n\t                        \"TensorFlow to be installed. Please see \"\n\t                        \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n\t                    )\n\t                    raise\n\t        else:\n\t            # Convert old format to new format if needed from a PyTorch state_dict\n", "            old_keys = []\n\t            new_keys = []\n\t            for key in state_dict.keys():\n\t                new_key = None\n\t                if \"gamma\" in key:\n\t                    new_key = key.replace(\"gamma\", \"weight\")\n\t                if \"beta\" in key:\n\t                    new_key = key.replace(\"beta\", \"bias\")\n\t                if new_key:\n\t                    old_keys.append(key)\n", "                    new_keys.append(new_key)\n\t            for old_key, new_key in zip(old_keys, new_keys):\n\t                state_dict[new_key] = state_dict.pop(old_key)\n\t            # copy state_dict so _load_from_state_dict can modify it\n\t            metadata = getattr(state_dict, \"_metadata\", None)\n\t            state_dict = state_dict.copy()\n\t            if metadata is not None:\n\t                state_dict._metadata = metadata\n\t            # PyTorch's _load_from_state_dict does not copy parameters in a module's descendants\n\t            # so we need to apply the function recursively.\n", "            def load(module: nn.Module, prefix=\"\"):\n\t                local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n\t                cls._load_from_state_dict(\n\t                    module=module,\n\t                    state_dict=state_dict,\n\t                    prefix=prefix,\n\t                    local_metadata=local_metadata,\n\t                    strict=True,\n\t                    missing_keys=missing_keys,\n\t                    unexpected_keys=unexpected_keys,\n", "                    error_msgs=error_msgs,\n\t                    partial_loads=partial_loads,\n\t                    config=config\n\t                )\n\t                for name, child in module._modules.items():\n\t                    if child is not None:\n\t                        load(child, prefix + name + \".\")\n\t            # Make sure we are able to load base models as well as derived models (with heads)\n\t            start_prefix = \"\"\n\t            model_to_load = model\n", "            has_prefix_module = any(s.startswith(cls.base_model_prefix) for s in state_dict.keys())\n\t            if not hasattr(model, cls.base_model_prefix) and has_prefix_module:\n\t                start_prefix = cls.base_model_prefix + \".\"\n\t            if hasattr(model, cls.base_model_prefix) and not has_prefix_module:\n\t                model_to_load = getattr(model, cls.base_model_prefix)\n\t            load(model_to_load, prefix=start_prefix)\n\t            if model.__class__.__name__ != model_to_load.__class__.__name__:\n\t                base_model_state_dict = model_to_load.state_dict().keys()\n\t                head_model_state_dict_without_base_prefix = [\n\t                    key.split(cls.base_model_prefix + \".\")[-1] for key in model.state_dict().keys()\n", "                ]\n\t                missing_keys.extend(head_model_state_dict_without_base_prefix - base_model_state_dict)\n\t            if len(unexpected_keys) > 0:\n\t                logger.warning(\n\t                    f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when \"\n\t                    f\"initializing {model.__class__.__name__}: {unexpected_keys}\\n\"\n\t                    f\"- This IS expected if you are initializing {model.__class__.__name__} from the checkpoint \"\n\t                    f\"of a model trained on another task \"\n\t                    f\"or with another architecture (e.g. initializing a BertForSequenceClassification model from \"\n\t                    f\"a BertForPretraining model).\\n\"\n", "                    f\"- This IS NOT expected if you are initializing {model.__class__.__name__} from the checkpoint \"\n\t                    f\"of a model that you expect \"\n\t                    f\"to be exactly identical (initializing a BertForSequenceClassification model from \"\n\t                    f\"a BertForSequenceClassification model).\"\n\t                )\n\t            else:\n\t                logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n\t            if len(missing_keys) > 0:\n\t                logger.warning(\n\t                    f\"Some weights of {model.__class__.__name__} were not initialized from the \"\n", "                    f\"model checkpoint at {pretrained_model_name_or_path} \"\n\t                    f\"and are newly initialized: {missing_keys}\\n\"\n\t                    f\"You should probably TRAIN this model on a down-stream task to be able \"\n\t                    f\"to use it for predictions and inference.\"\n\t                )\n\t            else:\n\t                logger.info(\n\t                    f\"All the weights of {model.__class__.__name__} were initialized from \"\n\t                    f\"the model checkpoint at {pretrained_model_name_or_path}.\\n\"\n\t                    f\"If your task is similar to the task the model of the ckeckpoint was trained on, \"\n", "                    f\"you can already use {model.__class__.__name__} for predictions without further training.\"\n\t                )\n\t            if len(error_msgs) > 0:\n\t                RuntimeError(\"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\t                    model.__class__.__name__, \"\\n\\t\".join(error_msgs)\n\t                ))\n\t            if len(partial_loads) > 0:\n\t                logger.info(\"Partial loads while loading state_dict for {}:\\n\\t{}\".format(\n\t                    model.__class__.__name__, \"\\n\\t\".join(partial_loads)\n\t                ))\n", "        model.tie_weights()  # make sure token embedding weights are still tied if needed\n\t        # Set model in evaluation mode to deactivate DropOut modules by default\n\t        model.eval()\n\t        if output_loading_info:\n\t            loading_info = {\n\t                \"missing_keys\": missing_keys,\n\t                \"unexpected_keys\": unexpected_keys,\n\t                \"error_msgs\": error_msgs,\n\t            }\n\t            return model, loading_info\n", "        if hasattr(config, \"xla_device\") and config.xla_device:\n\t            import torch_xla.core.xla_model as xm\n\t            model = xm.send_cpu_data_to_device(model, xm.xla_device())\n\t            model.to(xm.xla_device())\n\t        return model\n"]}
{"filename": "src/model/modules_for_prompt.py", "chunked_list": ["import random\n\timport math\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom transformers.models.bart.modeling_bart import *\n\tfrom src.model.modeling_bart import (\n\t    SinusoidalPositionalEmbedding,\n\t    LearnedPositionalEmbedding,\n\t    invert_mask,\n", "    EncoderLayer,\n\t    LayerNorm,\n\t)\n\tfrom src.model.modeling_bart import (PretrainedBartModel, BartDecoder,\n\t                                     BartClassificationHead,\n\t                                     _make_linear_from_emb,\n\t                                     _prepare_bart_decoder_inputs)\n\tfrom src.model.config import MultiModalBartConfig\n\tfrom transformers import AutoConfig, AutoModel, CLIPVisionModel, CLIPVisionConfig\n\timport timm\n", "from src.model.attention import Attention_for_Senti_Prompt\n\tTIMM_MODELS = {\n\t    'nf_resnet50': 2048,\n\t}\n\tdef is_clip_model(model_name):\n\t    return model_name.startswith('openai/clip-')\n\timage_model_name =  'nf_resnet50'\n\tif image_model_name in TIMM_MODELS.keys():\n\t    image_encoder = timm.create_model(image_model_name, pretrained=True, num_classes=0)\n\telif is_clip_model(image_model_name):\n", "    ###model_name ='openai/clip-vit-base-patch32'\n\t    config = CLIPVisionConfig.from_pretrained(image_model_name)\n\t    image_encoder = CLIPVisionModel.from_pretrained(\n\t            image_model_name,\n\t            config=config,\n\t        )\n\telse:\n\t    image_encoder = AutoModel.from_pretrained(image_model_name)\n\tdef init_image_encoder(image_model_name, frozen_image_encoder, num_image_tokens, d_text_encoder):\n\t    # image_encoder = get_image_encoder(image_model_name)\n", "    d_image_encoder = _d_image_encoder(image_model_name, image_encoder)\n\t    if frozen_image_encoder:\n\t        for p in image_encoder.parameters():\n\t            p.requires_grad = False\n\t            image_encoder.eval()\n\t    proj_image_features = nn.Linear(\n\t            in_features=d_image_encoder,\n\t            out_features=num_image_tokens * d_text_encoder,\n\t        )\n\t    return proj_image_features.cuda(), d_image_encoder\n", "def _d_image_encoder(image_model_name, image_encoder):\n\t    ##image_model_name默认为： 'microsoft/resnet-50'\n\t    model_name = image_model_name\n\t    if model_name in TIMM_MODELS.keys():\n\t        return TIMM_MODELS[model_name]\n\t    elif is_clip_model(model_name):\n\t        return image_encoder.config.hidden_size\n\t    elif model_name.startswith('microsoft/resnet-'):\n\t        return image_encoder.config.hidden_sizes[-1]\n\t    else:\n", "        return image_encoder.config.hidden_size\n\tdef encode_images(image_encoder, proj_image_features, frozen_image_encoder, pixel_values, d_image_encoder):\n\t    image_encoder = image_encoder.cuda()\n\t    pixel_values = pixel_values.cuda()\n\t    # print('the shape of pixel_values is {}'.format(pixel_values.shape))\n\t    batch_size = pixel_values.shape[0]\n\t    if frozen_image_encoder:\n\t        with torch.no_grad():\n\t            image_encoder.eval()\n\t            visual = image_encoder(pixel_values)\n", "    else:\n\t        visual = image_encoder(pixel_values)\n\t    if not isinstance(visual, torch.Tensor):  # HuggingFace model\n\t        visual = visual.pooler_output\n\t    visual = visual.reshape(batch_size, d_image_encoder)\n\t    visual = proj_image_features(visual).cuda()\n\t    return visual\n\tclass ImageEmbedding(nn.Module):\n\t    def __init__(self, image_dim, final_dim, image_model_name, frozen_image_encoder=False, num_image_tokens=2):\n\t        super(ImageEmbedding, self).__init__()\n", "        self.frozen_image_encoder = frozen_image_encoder\n\t        self.final_dim = final_dim\n\t        self.linear = nn.Linear(final_dim, final_dim)\n\t        self.d_image_encoder = _d_image_encoder(image_model_name, image_encoder)\n\t        if frozen_image_encoder:\n\t            for p in image_encoder.parameters():\n\t                p.requires_grad = False\n\t                image_encoder.eval()\n\t        self.proj_image_features = nn.Linear(\n\t                in_features=self.d_image_encoder,\n", "                out_features=num_image_tokens * final_dim,\n\t            )\n\t    def forward(self, image_pixel_values):\n\t        # import ipdb; ipdb.set_trace()\n\t        image_pixel_values = torch.stack(image_pixel_values)\n\t        batch_size = image_pixel_values.size(0)\n\t        image_features = encode_images(image_encoder=image_encoder, \n\t                                           proj_image_features=self.proj_image_features, \n\t                                           frozen_image_encoder=self.frozen_image_encoder, \n\t                                           pixel_values=image_pixel_values, \n", "                                           d_image_encoder=self.d_image_encoder)\n\t        ###image_features: (batch_size, num_image_tokens*1024) (4, 2048)\n\t        # print(\"======================================the shape of image_features is {}=====================================\".format(image_features.shape))\n\t        image_features = image_features.reshape(batch_size, -1, self.final_dim) ### (4, num_image_tokens, 1024(d_model))\n\t        img_len = list(map(len, image_features))\n\t        non_empty_features = list(filter(lambda x: len(x) > 0, image_features))\n\t        embedded = None\n\t        if len(non_empty_features) > 0:\n\t            img_tensor = torch.cat(non_empty_features, dim=0)\n\t            embedded = self.linear(img_tensor)\n", "        output = []\n\t        index = 0\n\t        for l in img_len:\n\t            if l > 0:\n\t                output.append(embedded[index:index + l])\n\t            else:\n\t                output.append(torch.empty(0))\n\t            index += l\n\t        return output\n\tclass MultiModalBartEncoder(nn.Module):\n", "    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n\t        config: MultiModalBartConfig\n\t    \"\"\"\n\t    def __init__(self, config: MultiModalBartConfig, encoder, img_feat_id,\n\t                 cls_token_id, num_image_tokens):\n\t        super().__init__()\n\t        self.img_feat_id = img_feat_id\n", "        self.cls_token_id = cls_token_id\n\t        embed_tokens = encoder.embed_tokens\n\t        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n\t        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n\t        self.embed_tokens = embed_tokens\n", "        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n\t        self.embed_positions = encoder.embed_positions\n\t        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t    def _embed_multi_modal(self, input_ids, image_features):\n\t        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n\t        mask = (input_ids == self.img_feat_id) | (\n\t            input_ids == self.cls_token_id)\n", "        # print(mask.shape)\n\t        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n\t        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n\t        for index, value in enumerate(embedded_images):\n\t            if len(value) > 0:\n\t                embedded[index, mask[index]] = value\n\t        return embedded\n", "    def forward(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n\t        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n\t        :param image_features: list[FloatTensor], image roi features with length of batch\n", "        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n\t        :param output_attentions:\n\t        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n\t              Only populated if output_hidden_states: is True.\n\t            - all_attentions (List[Tensor]): Attention weights for each layer.\n", "            During training might not be of length n_layers because of layer dropout.\n\t        \"\"\"\n\t        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n\t        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n", "        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        # B x T x C -> T x B x C\n\t        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n", "                                  self.layerdrop):  # skip the layer\n\t                attn = None\n\t            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n\t        if self.layer_norm:\n\t            x = self.layer_norm(x)\n", "        if output_hidden_states:\n\t            encoder_states.append(x)\n\t        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n", "        return BaseModelOutput(last_hidden_state=x,\n\t                               hidden_states=encoder_states,\n\t                               attentions=all_attentions)\n\tclass MultiModalBartEncoder_for_Generating_aspect_prompt(nn.Module):\n\t    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n\t        config: MultiModalBartConfig\n\t    \"\"\"\n", "    def __init__(self, \n\t                use_generated_prompt,\n\t                config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n\t                 cls_token_id, num_image_tokens, use_different_aspect_prompt):\n\t        super().__init__()\n\t        self.use_generated_prompt= use_generated_prompt\n\t        self.aspect_prompt_token_id = aspect_prompt_token_id\n\t        self.senti_prompt_token_id = senti_prompt_token_id\n\t        self.use_different_aspect_prompt = use_different_aspect_prompt\n\t        self.img_feat_id = img_feat_id\n", "        self.cls_token_id = cls_token_id\n\t        embed_tokens = encoder.embed_tokens\n\t        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n\t        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n\t        self.embed_tokens = embed_tokens\n", "        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n\t        self.embed_positions = encoder.embed_positions\n\t        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t        # self.aspect_linear = nn.Linear(768, 768)\n\t        # self.aspect_relu = nn.LeakyReLU()\n\t    def _embed_multi_modal(self, generated_aspect_prompt, aspects_num, input_ids, image_features):\n\t        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n", "        # import ipdb; ipdb.set_trace()\n\t        mask = (input_ids == self.img_feat_id) | (\n\t            input_ids == self.cls_token_id)\n\t        # print(mask.shape)\n\t        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n\t        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n\t        for index, value in enumerate(embedded_images):\n", "            if len(value) > 0:\n\t                embedded[index, mask[index]] = value\n\t        prompt_mask = (input_ids == self.aspect_prompt_token_id)\n\t        if self.use_generated_prompt:\n\t            if self.use_different_aspect_prompt:\n\t                # self.aspect_linear = self.aspect_linear.to(generated_aspect_prompt.device)\n\t                # self.aspect_relu = self.aspect_relu.to(generated_aspect_prompt.device)\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n", "                    prompt_embedding_list = []\n\t                    for j in range(aspect_num):\n\t                        aspect_linear = nn.Linear(768, 768).to(generated_aspect_prompt.device) ##每个aspect有自己的变换，为每个aspect设计特定的prompt\n\t                        aspect_relu = nn.LeakyReLU().to(generated_aspect_prompt.device)\n\t                        prompt_embedding = aspect_linear(generated_aspect_prompt[index])\n\t                        prompt_embedding = aspect_relu(prompt_embedding)\n\t                        ###可以加入激活函数\n\t                        # prompt_embedding = nn.LeakyReLU(prompt_embedding)\n\t                        prompt_embedding_list.append(prompt_embedding)\n\t                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n", "                    embedded[index, prompt_mask[index]] = prompt_embedding_\n\t            else:\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    prompt_embedding_ = generated_aspect_prompt[index].repeat(aspect_num, 1)\n\t                    embedded[index, prompt_mask[index]] = prompt_embedding_\n\t        return embedded\n\t    def forward(self,\n\t                input_ids,\n\t                image_features,\n", "                attention_mask=None,\n\t                generated_prompt=None,\n\t                aspects_num=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n\t        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n\t        :param image_features: list[FloatTensor], image roi features with length of batch\n\t        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n", "        :param output_attentions:\n\t        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n\t              Only populated if output_hidden_states: is True.\n\t            - all_attentions (List[Tensor]): Attention weights for each layer.\n\t            During training might not be of length n_layers because of layer dropout.\n", "        \"\"\"\n\t        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(generated_prompt, aspects_num,\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n\t        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n", "        # B x T x C -> T x B x C\n\t        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n\t                                  self.layerdrop):  # skip the layer\n", "                attn = None\n\t            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n\t        if self.layer_norm:\n\t            x = self.layer_norm(x)\n\t        if output_hidden_states:\n", "            encoder_states.append(x)\n\t        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n\t        return BaseModelOutput(last_hidden_state=x,\n", "                               hidden_states=encoder_states,\n\t                               attentions=all_attentions)\n\tclass MultiModalBartEncoder_for_Generating_sentiment_prompt(nn.Module):\n\t    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n\t        config: MultiModalBartConfig\n\t    \"\"\"\n\t    def __init__(self, use_generated_prompt,\n", "                 config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n\t                 cls_token_id, num_image_tokens, use_different_senti_prompt):\n\t        super().__init__()\n\t        self.use_generated_prompt = use_generated_prompt\n\t        self.aspect_prompt_token_id = aspect_prompt_token_id\n\t        self.senti_prompt_token_id = senti_prompt_token_id\n\t        self.use_different_senti_prompt = use_different_senti_prompt\n\t        self.img_feat_id = img_feat_id\n\t        self.cls_token_id = cls_token_id\n\t        embed_tokens = encoder.embed_tokens\n", "        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n\t        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n\t        self.embed_tokens = embed_tokens\n\t        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n\t        self.embed_positions = encoder.embed_positions\n", "        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t        # self.aspect_linear = nn.Linear(768, 768)\n\t        # self.aspect_relu = nn.LeakyReLU()\n\t    def _embed_multi_modal(self, generated_senti_prompt, aspects_num, input_ids, image_features):\n\t        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n\t        # import ipdb; ipdb.set_trace()\n\t        mask = (input_ids == self.img_feat_id) | (\n", "            input_ids == self.cls_token_id)\n\t        # print(mask.shape)\n\t        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n\t        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n\t        for index, value in enumerate(embedded_images):\n\t            if len(value) > 0:\n\t                embedded[index, mask[index]] = value\n", "        if self.use_generated_prompt:\n\t            senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n\t            # import ipdb; ipdb.set_trace()\n\t            if self.use_different_senti_prompt:\n\t                # self.aspect_linear = self.aspect_linear.to(generated_senti_prompt.device)\n\t                # self.aspect_relu = self.aspect_relu.to(generated_senti_prompt.device)\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n\t                    prompt_embedding_list = []\n", "                    for j in range(aspect_num):\n\t                        aspect_linear = nn.Linear(768, 768).to(generated_senti_prompt.device)\n\t                        aspect_relu = nn.LeakyReLU().to(generated_senti_prompt.device)\n\t                        prompt_embedding = aspect_linear(generated_senti_prompt[index])\n\t                        prompt_embedding = aspect_relu(prompt_embedding)\n\t                        ###可以加入激活函数å\n\t                        # prompt_embedding = nn.LeakyReLU(prompt_embedding)\n\t                        prompt_embedding_list.append(prompt_embedding)\n\t                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n\t                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n", "            else:\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    prompt_embedding_ = generated_senti_prompt[index].repeat(aspect_num, 1)\n\t                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n\t        return embedded\n\t    def forward(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n", "                generated_prompt=None,\n\t                aspects_num=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n\t        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n\t        :param image_features: list[FloatTensor], image roi features with length of batch\n\t        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n\t        :param output_attentions:\n", "        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n\t              Only populated if output_hidden_states: is True.\n\t            - all_attentions (List[Tensor]): Attention weights for each layer.\n\t            During training might not be of length n_layers because of layer dropout.\n\t        \"\"\"\n", "        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(generated_prompt, aspects_num,\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n\t        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        # B x T x C -> T x B x C\n", "        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n\t                                  self.layerdrop):  # skip the layer\n\t                attn = None\n", "            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n\t        if self.layer_norm:\n\t            x = self.layer_norm(x)\n\t        if output_hidden_states:\n\t            encoder_states.append(x)\n", "        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n\t        return BaseModelOutput(last_hidden_state=x,\n\t                               hidden_states=encoder_states,\n", "                               attentions=all_attentions)\n\tclass MultiModalBartEncoder_for_Generating_Dual_prompts(nn.Module):\n\t    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n\t        config: MultiModalBartConfig\n\t    \"\"\"\n\t    def __init__(self, \n\t                 use_generated_aspect_prompt, use_generated_senti_prompt, \n", "                 config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n\t                 cls_token_id, num_image_tokens, use_different_aspect_prompt, use_different_senti_prompt, \n\t                 NEU_id, POS_id, NEG_id):\n\t        super().__init__()\n\t        self.use_generated_aspect_prompt= use_generated_aspect_prompt\n\t        self.use_generated_senti_prompt = use_generated_senti_prompt\n\t        self.aspect_prompt_token_id = aspect_prompt_token_id\n\t        self.senti_prompt_token_id = senti_prompt_token_id\n\t        self.use_different_aspect_prompt = use_different_aspect_prompt\n\t        self.use_different_senti_prompt = use_different_senti_prompt\n", "        # if self.use_different_senti_prompt:\n\t        #     self.attention_for_senti_prompt = Attention_for_Senti_Prompt(n_head=8, model_dim=768, drop_rate=0.2)\n\t        self.img_feat_id = img_feat_id\n\t        self.cls_token_id = cls_token_id\n\t        self.neu_id = NEU_id\n\t        self.pos_id = POS_id\n\t        self.neg_id = NEG_id\n\t        embed_tokens = encoder.embed_tokens\n\t        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n", "        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n\t        self.embed_tokens = embed_tokens\n\t        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n\t        self.embed_positions = encoder.embed_positions\n\t        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n", "        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t        # self.aspect_linear = nn.Linear(768, 768)\n\t        # self.aspect_relu = nn.LeakyReLU()\n\t        # self.aspect_linear = nn.Sequential(nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768))\n\t    def _embed_multi_modal(self, generated_aspect_prompt, generated_senti_prompt, aspects_num, input_ids, image_features):\n\t        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n\t        # import ipdb; ipdb.set_trace()\n\t        device = generated_aspect_prompt.device\n\t        batch_size = input_ids.size(0)\n", "        mask = (input_ids == self.img_feat_id) | (\n\t            input_ids == self.cls_token_id)\n\t        # print(mask.shape)\n\t        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n\t        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n\t        for index, value in enumerate(embedded_images):\n\t            if len(value) > 0:\n", "                embedded[index, mask[index]] = value\n\t        import ipdb; ipdb.set_trace()\n\t        if self.use_generated_aspect_prompt:\n\t            ##aspect_prompt\n\t            aspect_prompt_mask = (input_ids == self.aspect_prompt_token_id)\n\t            if self.use_different_aspect_prompt:\n\t                # self.aspect_linear = self.aspect_linear.to(device)\n\t                # self.aspect_relu = self.aspect_relu.to(device)\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n", "                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n\t                    aspect_prompt_embedding_list = []\n\t                    for j in range(aspect_num):\n\t                        aspect_linear = nn.Linear(768, 768).to(generated_aspect_prompt.device) ##每个aspect有自己的变换，为每个aspect设计特定的prompt\n\t                        aspect_relu = nn.LeakyReLU().to(generated_aspect_prompt.device)\n\t                        aspect_prompt_embedding = aspect_linear(generated_aspect_prompt[index])\n\t                        aspect_prompt_embedding = aspect_relu(aspect_prompt_embedding)\n\t                        aspect_prompt_embedding_list.append(aspect_prompt_embedding)\n\t                    aspect_prompt_embedding_ = torch.cat(aspect_prompt_embedding_list, dim=0)\n\t                    embedded[index, aspect_prompt_mask[index]] = aspect_prompt_embedding_\n", "            else:\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    aspect_prompt_embedding_ = generated_aspect_prompt[index].repeat(aspect_num, 1)\n\t                    embedded[index, aspect_prompt_mask[index]] = aspect_prompt_embedding_\n\t        ##sentiment_prompt\n\t        if self.use_generated_senti_prompt:\n\t            '''\n\t            # if self.use_different_senti_prompt:\n\t            以下使用的是attention机制，senti_prompt_token和sentiments_embdedding\n", "            # sentiments_ids = torch.tensor([self.neu_id, self.pos_id, self.neg_id]).to(device)\n\t            # sentiments_embdedding = self.embed_tokens(sentiments_ids)\n\t            # senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n\t            # for index in range(len(aspects_num)):\n\t            #     aspect_num = aspects_num[index]\n\t            #     expanded_sentiments_embdedding = sentiments_embdedding.expand(aspect_num, sentiments_embdedding.size(0), sentiments_embdedding.size(1))\n\t            #     original_senti_prompt = embedded[index, senti_prompt_mask[index]].unsqueeze(1)\n\t            #     new_senti_prompt = self.attention_for_senti_prompt(original_senti_prompt, expanded_sentiments_embdedding, expanded_sentiments_embdedding).squeeze()\n\t            #     # import ipdb; ipdb.set_trace()\n\t            #     embedded[index, senti_prompt_mask[index]] = new_senti_prompt\n", "            '''\n\t            ##换成senti_prompt也是生成形式看看\n\t            senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n\t            # import ipdb; ipdb.set_trace()\n\t            if self.use_different_senti_prompt:\n\t                # self.aspect_linear = self.aspect_linear.to(generated_senti_prompt.device)\n\t                # self.aspect_relu = self.aspect_relu.to(generated_senti_prompt.device)\n\t                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n", "                    prompt_embedding_list = []\n\t                    for j in range(aspect_num):\n\t                        senti_linear = nn.Linear(768, 768).to(generated_senti_prompt.device)\n\t                        senti_relu = nn.LeakyReLU().to(generated_senti_prompt.device)\n\t                        prompt_embedding = senti_linear(generated_senti_prompt[index])\n\t                        prompt_embedding = senti_relu(prompt_embedding)\n\t                        prompt_embedding_list.append(prompt_embedding)\n\t                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n\t                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n\t            else:\n", "                for index in range(len(aspects_num)):\n\t                    aspect_num = aspects_num[index]\n\t                    prompt_embedding_ = generated_senti_prompt[index].repeat(aspect_num, 1)\n\t                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n\t        return embedded\n\t    def forward(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                generated_aspect_prompt=None,\n", "                generated_senti_prompt=None,\n\t                aspects_num=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n\t        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n\t        :param image_features: list[FloatTensor], image roi features with length of batch\n\t        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n\t        :param output_attentions:\n", "        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n\t              Only populated if output_hidden_states: is True.\n\t            - all_attentions (List[Tensor]): Attention weights for each layer.\n\t            During training might not be of length n_layers because of layer dropout.\n\t        \"\"\"\n", "        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(generated_aspect_prompt, generated_senti_prompt, aspects_num,\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n\t        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        # B x T x C -> T x B x C\n", "        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n\t                                  self.layerdrop):  # skip the layer\n\t                attn = None\n", "            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n\t        if self.layer_norm:\n\t            x = self.layer_norm(x)\n\t        if output_hidden_states:\n\t            encoder_states.append(x)\n", "        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n\t        return BaseModelOutput(last_hidden_state=x,\n\t                               hidden_states=encoder_states,\n", "                               attentions=all_attentions)\n\tclass MultiModalBartDecoder_span(nn.Module\n\t                                 ):  #AOE task and all downstream tasks\n\t    def __init__(self,\n\t                 config: MultiModalBartConfig,\n\t                 tokenizer,\n\t                 decoder,\n\t                 pad_token_id,\n\t                 label_ids,\n\t                 causal_mask,\n", "                 need_tag=True,\n\t                 only_sc=False,\n\t                 avg_feature=False,\n\t                 use_encoder_mlp=True):\n\t        super().__init__()\n\t        self.decoder = decoder\n\t        self.tokenizer = tokenizer\n\t        self.causal_mask = causal_mask\n\t        self.register_buffer('causal_masks', causal_mask.float())\n\t        self.pad_token_id = pad_token_id\n", "        # label_ids = sorted(label_ids, reverse=False)\n\t        self.label_start_id = min(label_ids)\n\t        self.label_end_id = max(label_ids) + 1\n\t        self.need_tag = need_tag\n\t        self.only_sc = only_sc\n\t        mapping = torch.LongTensor([0, 2] + label_ids)\n\t        ###mapping: [0, 2, 50276, 50277, 50278, 50281]\n\t        self.register_buffer('mapping', mapping)\n\t        self.src_start_index = len(mapping)  # 加上一个\n\t        hidden_size = decoder.embed_tokens.weight.size(1)\n", "        self.dropout_layer = nn.Dropout(0.1)\n\t        self.end_text_id = tokenizer.end_text_id\n\t        self.avg_feature = avg_feature\n\t        if use_encoder_mlp:\n\t            self.encoder_mlp = nn.Sequential(\n\t                nn.Linear(hidden_size, hidden_size), nn.Dropout(0.3),\n\t                nn.ReLU(), nn.Linear(hidden_size, hidden_size))\n\t    def forward(self, tokens, state, only_sc=False):\n\t        # import ipdb; ipdb.set_trace()\n\t        '''\n", "        tokens: [[0, 2, 2, 16, 16, 4, 18, 18, 4, 1, 1, 1, 1],\n\t                 [0, 2, 2, 15, 16, 3, 25, 26, 5, 28, 28, 4, 1]]\n\t        '''\n\t        # import ipdb; ipdb.set_trace()\n\t        bsz, max_len = tokens.size()\n\t        encoder_outputs = state.encoder_output ##(batch, 72=38(len(image_token+begin_image+end_image(36+1+1)))+34(max_tex_len(包含begin_text_id(0) and end_text_id(2)) in batch), 768)\n\t        encoder_pad_mask = state.encoder_mask ##(batch, 72)\n\t        first = state.first\n\t        # tokens之后的0全是padding，因为1是eos, 在pipe中规定的\n\t        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n", "        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n\t        # 把输入做一下映射\n\t        mapping_token_mask = tokens.lt(\n\t            self.src_start_index)  # 为1的地方应该从mapping中取index\n\t        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n\t        tag_mapped_tokens = self.mapping[mapped_tokens]\n\t        src_tokens_index = tokens - self.src_start_index  # bsz x num_src_token\n\t        src_tokens_index = src_tokens_index.masked_fill(\n\t            src_tokens_index.lt(0), 0)\n\t        src_tokens = state.src_tokens \n", "        # print(src_tokens.shape): (2, 34)\n\t        if first is not None:\n\t            src_tokens = src_tokens.gather(index=first, dim=1) ###Sequence\n\t        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n\t        # print('word_mapped_tokens', word_mapped_tokens)\n\t        tokens = torch.where(mapping_token_mask, tag_mapped_tokens,\n\t                             word_mapped_tokens)\n\t        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n\t        '''\n\t        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n", "        tensor([[0, 50276, 50276, 4644, 4644, 50278, 798, 798, 50278, 2, 1, 1, 1],\n\t                [0, 50276, 50276, 9517, 957, 50277, 2561, 7772, 50281, 2762, 2762, 50278, 2]])\n\t        将tokens中的index以及标签都转化为vocabulary中的token_id\n\t        '''\n\t        if self.training:\n\t            tokens = tokens[:, :-1]\n\t            decoder_pad_mask = tokens.eq(\n\t                self.pad_token_id)  # decoder需要让pad位置为1\n\t            dict = self.decoder(input_ids=tokens,\n\t                                encoder_hidden_states=encoder_outputs,\n", "                                encoder_padding_mask=encoder_pad_mask,\n\t                                decoder_padding_mask=decoder_pad_mask,\n\t                                decoder_causal_mask=self.\n\t                                causal_masks[:tokens.size(1), :tokens.size(1)],\n\t                                return_dict=True)\n\t        else:\n\t            past_key_values = state.past_key_values\n\t            dict = self.decoder(input_ids=tokens,\n\t                                encoder_hidden_states=encoder_outputs,\n\t                                encoder_padding_mask=encoder_pad_mask,\n", "                                decoder_padding_mask=None,\n\t                                decoder_causal_mask=self.\n\t                                causal_masks[:tokens.size(1), :tokens.size(1)],\n\t                                return_dict=True)\n\t        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size (2, 12(去掉了 end_token_id), 768)\n\t        hidden_state = self.dropout_layer(hidden_state)\n\t        if not self.training:\n\t            state.past_key_values = dict.past_key_values\n\t        logits = hidden_state.new_full(\n\t            (hidden_state.size(0), hidden_state.size(1),\n", "             self.src_start_index + src_tokens.size(-1)),\n\t            fill_value=-1e24)\n\t        ##建立空的logits\n\t        # print('logits', logits.shape) (bsz, max_len,  self.src_start_index + src_tokens.size(-1)) -> (2, 12, 40=6+34)\n\t        # 首先计算的是\n\t        if self.need_tag:\n\t            '''\n\t            self.decoder.embed_tokens.weight: (50289, 768)\n\t            self.label_start_id: 50276\n\t            '''\n", "            tag_scores = F.linear(\n\t                hidden_state,\n\t                self.dropout_layer(\n\t                    self.decoder.embed_tokens.\n\t                    weight[self.label_start_id:self.label_start_id +\n\t                           3]))  # bsz x max_len x num_class\n\t            logits[:, :, 3:self.src_start_index] = tag_scores ###给情感的position赋值[:, :, (3, 4, 5)]\n\t        if not only_sc:\n\t            eos_scores = F.linear(\n\t                hidden_state,\n", "                self.dropout_layer(self.decoder.embed_tokens.weight[2:3])) \n\t            '''\n\t            ['</s>(eos_token)', '<mask>', '<pad>', '<s>(bos_token)', '<unk>']\n\t            [2, 50264, 1, 0, 3]\n\t            '''\n\t            # bsz x max_bpe_len(image_len + text_len) x hidden_size: (2, 72, 768)\n\t            src_outputs = state.encoder_output \n\t            if hasattr(self, 'encoder_mlp') and not only_sc:\n\t                src_outputs = self.encoder_mlp(src_outputs)\n\t            if first is not None:\n", "                mask = first.eq(0)\n\t                src_outputs = src_outputs.gather(\n\t                    index=first.unsqueeze(2).repeat(1, 1,\n\t                                                    src_outputs.size(-1)),\n\t                    dim=1)\n\t            else:\n\t                mask = state.encoder_mask[:, 64:].eq(0)\n\t                # src_outputs = self.decoder.embed_tokens(src_tokens)\n\t            mask = mask.unsqueeze(1) ## bsz x 1 x max_word_len: (2, 1, 34)\n\t            input_embed = self.decoder.embed_tokens(\n", "                src_tokens)  #bsz x max_word_len x hidden_size: (2, 34, 768); src_tokens: (2, 34)\n\t            input_embed = self.dropout_layer(input_embed)\n\t            if self.avg_feature:  # 先把feature合并一下\n\t                src_outputs = (src_outputs[:, 64:] + input_embed) / 2\n\t            word_scores = torch.einsum(\n\t                'blh,bnh->bln', hidden_state,\n\t                src_outputs[:, 64:])  # bsz x max_len x max_word_len: (2, 12, 34)\n\t            if not self.avg_feature:\n\t                gen_scores = torch.einsum(\n\t                    'blh,bnh->bln', hidden_state,\n", "                    input_embed)  # bsz x max_len x max_word_len: (2, 12, 34)\n\t                word_scores = (gen_scores + word_scores) / 2 \n\t            mask = mask.__or__(\n\t                src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1)) ###(2, 1, 34)\n\t            word_scores = word_scores.masked_fill(mask, -1e32) ###(bts, max_len, max_word_len)\n\t            logits[:, :, self.src_start_index:] = word_scores\n\t            ###logits.shape (bts, max_len, max_word_len+6): (2, 12, 40)\n\t            logits[:, :, 1:2] = eos_scores\n\t        # print(torch.argmax(logits[0], dim=-1))\n\t        return logits\n", "    def decode(self, tokens, state, only_sc=False):\n\t        return self(tokens, state, only_sc)[:, -1]\n\tclass Span_loss(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t        # self.loss_fct = nn.CrossEntropyLoss()\n\t        self.fc = nn.LogSoftmax(dim=-1)\n\t    def forward(self, tgt_tokens, pred, mask):\n\t        '''\n\t        tgt_tokens: (2 (batch-size), 12 (max_len+1))\n", "        pred: (2, 12, 40 (max_word_len))\n\t        '''\n\t        tgt_tokens = tgt_tokens.masked_fill(mask.eq(0), -100)\n\t        output = F.cross_entropy(target=tgt_tokens, input=pred.transpose(1, 2)) ##每一个词都有12种类别， input= (40, 12)\n\t        return output\n\tclass MultiModalBartDecoder_MLM(nn.Module):\n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n", "        self.register_buffer(\n\t            \"final_logits_bias\",\n\t            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\t    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n\t        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n\t            self.config,\n\t            input_ids,\n\t            decoder_input_ids=decoder_input_ids,\n\t            decoder_padding_mask=decoder_attention_mask,\n", "            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n\t        decoder_outputs = self.decoder(\n\t            decoder_input_ids,\n\t            encoder_outputs,\n\t            attention_mask,\n\t            decoder_padding_mask,\n\t            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n\t                                            decoder_input_ids.size(1)],\n\t        )\n\t        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n", "                             self.decoder.embed_tokens.weight,\n\t                             bias=self.final_logits_bias)\n\t        lm_loss = 0\n\t        # compute lm loss if labels is given\n\t        if labels is not None:\n\t            labels = labels.clone()\n\t            loss_fct = nn.CrossEntropyLoss()\n\t            lm_loss = loss_fct(\n\t                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n\t                labels.reshape(-1))\n", "            return lm_loss\n\tclass MultiModalBartDecoder_ANP_generate(nn.Module):  #AOG task\n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.register_buffer(\n\t            \"final_logits_bias\",\n\t            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\t    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n", "                decoder_input_ids, decoder_attention_mask):\n\t        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n\t            self.config,\n\t            input_ids,\n\t            decoder_input_ids=decoder_input_ids,\n\t            decoder_padding_mask=decoder_attention_mask,\n\t            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n\t        decoder_outputs = self.decoder(\n\t            decoder_input_ids,\n\t            encoder_outputs,\n", "            attention_mask,\n\t            decoder_padding_mask,\n\t            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n\t                                            decoder_input_ids.size(1)],\n\t        )\n\t        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n\t                             self.decoder.embed_tokens.weight,\n\t                             bias=self.final_logits_bias)\n\t        lm_loss = 0\n\t        # compute lm loss if labels is given\n", "        if labels is not None:\n\t            labels = labels.clone()\n\t            # labels[labels == self.cls_token_id] = -100\n\t            loss_fct = nn.CrossEntropyLoss()\n\t            lm_loss = loss_fct(\n\t                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n\t                labels.reshape(-1))\n\t            return lm_loss\n\tclass MultiModalBartDecoder_sentiment(nn.Module):  #MSP task\n\t    def __init__(self,\n", "                 config: MultiModalBartConfig,\n\t                 decoder,\n\t                 senti_ids,\n\t                 senti_nums=3):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.senti_ids = senti_ids\n\t        self.dropout_layer = nn.Dropout(0.1)\n\t        self.senti_head = BartClassificationHead(config.d_model,\n", "                                                 config.d_model, senti_nums,\n\t                                                 config.classif_dropout)\n\t    def _init_weights(self, module):\n\t        module.weight.data.normal_(mean=0.0, std=0.02)\n\t        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    def forward(self, senti_labels, encoder_outputs, attention_mask,\n\t                senti_decoder_input_ids):\n\t        decoder_outputs = self.decoder(\n\t            input_ids=senti_decoder_input_ids,\n", "            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask,\n\t            decoder_padding_mask=None,\n\t            decoder_causal_mask=None,\n\t        )\n\t        # predict_senti = F.linear(\n\t        #     decoder_outputs[0][:, 1],\n\t        #     self.dropout_layer(self.decoder.embed_tokens.\n\t        #                        weight[self.senti_ids[0]:self.senti_ids[2] +\n\t        #                               1]))  # bsz\n", "        # predict_senti = torch.flip(predict_senti, dims=[-1])\n\t        predict_senti = self.senti_head(decoder_outputs[0][:, 1])\n\t        loss_fct = nn.CrossEntropyLoss()\n\t        senti_loss = loss_fct(predict_senti, senti_labels)\n\t        return senti_loss, predict_senti\n\tclass MultiModalBartDecoder_MRM(nn.Module):\n\t    def __init__(self, config: MultiModalBartConfig, decoder, causal_mask,\n\t                 args):\n\t        super().__init__()\n\t        self.config = config\n", "        self.decoder = decoder\n\t        self.causal_mask = causal_mask\n\t        self.args = args\n\t        self.mrm_head = BartClassificationHead(\n\t            config.d_model,\n\t            config.d_model,\n\t            config.num_labels,\n\t            config.classif_dropout,\n\t        )\n\t        self._init_weights(self.mrm_head.dense)\n", "        self._init_weights(self.mrm_head.out_proj)\n\t    def _init_weights(self, module):\n\t        module.weight.data.normal_(mean=0.0, std=0.02)\n\t        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    def forward(self, mrm_labels, mrm_masks, encoder_outputs, attention_mask,\n\t                mrm_decoder_input_ids, mrm_decoder_attention_mask):\n\t        decoder_padding_mask = mrm_decoder_attention_mask.eq(0)\n\t        decoder_outputs = self.decoder(\n\t            input_ids=mrm_decoder_input_ids,\n", "            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask,\n\t            decoder_padding_mask=decoder_padding_mask,\n\t            decoder_causal_mask=self.causal_mask[:mrm_decoder_input_ids.size(\n\t                1), :mrm_decoder_input_ids.size(1)].to(\n\t                    mrm_decoder_input_ids.device),\n\t        )\n\t        region_representation = decoder_outputs[0][mrm_masks.bool()]\n\t        if len(region_representation) > 0:\n\t            predict_cls = self.mrm_head(region_representation)\n", "            loss_fct = nn.CrossEntropyLoss()\n\t            mrm_labels = torch.cat(mrm_labels,\n\t                                   dim=0).to(encoder_outputs.device)\n\t            if self.args.mrm_loss_type == 'KL':\n\t                predict_cls = F.log_softmax(predict_cls, dim=-1)\n\t                mrm_loss = F.kl_div(predict_cls.double(),\n\t                                    mrm_labels.double().squeeze(1),\n\t                                    reduction='batchmean')\n\t            else:\n\t                raise RuntimeError(\"wrong mrm type\")\n", "        else:\n\t            mrm_loss = 0\n\t        return mrm_loss\n\t'''\n\tgenerate_aspect_prompt based on the multimodal context\n\t'''\n\tclass MultiModalBartDecoder_generate_aspect_prompt(nn.Module): \n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n\t        self.config = config\n", "        self.decoder = decoder\n\t        self.aspect_prompt_linear = nn.Linear(768, 768)\n\t    def forward(self, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n\t        # import ipdb; ipdb.set_trace()\n\t        decoder_outputs = self.decoder(\n\t            input_ids=decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask.eq(0),\n\t            decoder_padding_mask=decoder_attention_mask.eq(0),\n", "            decoder_causal_mask=None,\n\t        )\n\t        prompt_logits = decoder_outputs[0]\n\t        aspect_prompt_logits = self.aspect_prompt_linear(prompt_logits)\n\t        return aspect_prompt_logits \n\t'''\n\tgenerate_sentiment_prompt based on the multimodal context\n\t'''\n\tclass MultiModalBartDecoder_generate_sentiment_prompt(nn.Module): \n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n", "        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.senti_prompt_linear = nn.Linear(768, 768)\n\t    def forward(self, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n\t        # import ipdb; ipdb.set_trace()\n\t        decoder_outputs = self.decoder(\n\t            input_ids=decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n", "            encoder_padding_mask=attention_mask.eq(0),\n\t            decoder_padding_mask=decoder_attention_mask.eq(0),\n\t            decoder_causal_mask=None,\n\t        )\n\t        prompt_logits = decoder_outputs[0]\n\t        senti_prompt_logits = self.senti_prompt_linear(prompt_logits)\n\t        return senti_prompt_logits\n\tclass MultiModalBartDecoder_aspects_num(nn.Module):  #MSP task\n\t    def __init__(self,\n\t                 config: MultiModalBartConfig,\n", "                 decoder,\n\t                 max_aspects_nums=5):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.dropout_layer = nn.Dropout(0.1)\n\t        self.aspects_num_head = BartClassificationHead(config.d_model,\n\t                                                 config.d_model, max_aspects_nums,\n\t                                                 config.classif_dropout)\n\t    def _init_weights(self, module):\n", "        module.weight.data.normal_(mean=0.0, std=0.02)\n\t        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    def forward(self, aspects_num_labels, encoder_outputs, attention_mask,\n\t                aspects_num_decoder_input_ids):\n\t        decoder_outputs = self.decoder(\n\t            input_ids=aspects_num_decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask,\n\t            decoder_padding_mask=None,\n", "            decoder_causal_mask=None,\n\t        )\n\t        # predict_aspects_num = F.linear(\n\t        #     decoder_outputs[0][:, 1],\n\t        #     self.dropout_layer(self.decoder.embed_tokens.\n\t        #                        weight[self.aspects_num_ids[0]:self.aspects_num_ids[2] +\n\t        #                               1]))  # bsz\n\t        # predict_aspects_num = torch.flip(predict_aspects_num, dims=[-1])\n\t        predict_aspects_num_logits = self.aspects_num_head(decoder_outputs[0][:, 1])\n\t        loss_fct = nn.CrossEntropyLoss()\n", "        aspects_num_loss = loss_fct(predict_aspects_num_logits, aspects_num_labels)\n\t        return aspects_num_loss, predict_aspects_num_logits"]}
{"filename": "src/model/config.py", "chunked_list": ["from transformers import BartConfig\n\tclass MultiModalBartConfig(BartConfig):\n\t    def __init__(\n\t            self,\n\t            activation_dropout=0.0,\n\t            extra_pos_embeddings=2,\n\t            activation_function=\"gelu\",\n\t            vocab_size=50323,\n\t            image_feature_size=2048 + 4,\n\t            d_model=1024,\n", "            encoder_ffn_dim=4096,\n\t            encoder_layers=12,\n\t            encoder_attention_heads=16,\n\t            decoder_ffn_dim=4096,\n\t            decoder_layers=12,\n\t            decoder_attention_heads=16,\n\t            encoder_layerdrop=0.0,\n\t            decoder_layerdrop=0.0,\n\t            attention_dropout=0.0,\n\t            dropout=0.1,\n", "            max_position_embeddings=1024,\n\t            init_std=0.02,\n\t            classif_dropout=0.0,\n\t            num_labels=1,\n\t            num_attributes=1,\n\t            num_relations=1,\n\t            is_encoder_decoder=True,\n\t            pad_token_id=1,\n\t            bos_token_id=0,\n\t            eos_token_id=2,\n", "            img_feat_id=50273,\n\t            cls_token_id=50276,\n\t            normalize_before=False,\n\t            add_final_layer_norm=False,\n\t            scale_embedding=False,\n\t            normalize_embedding=True,\n\t            static_position_embeddings=False,\n\t            add_bias_logits=False,\n\t            decoder_start_token_id=0,\n\t            partial_load=(),\n", "            lm_loss_factor=1.0,\n\t            mrm_loss_factor=1.0,\n\t            attribute_loss_factor=1.0,\n\t            relation_loss_factor=1.0,\n\t            num_aspects =1,\n\t            **common_kwargs\n\t    ):\n\t        super(MultiModalBartConfig, self).__init__(\n\t            activation_dropout=activation_dropout,\n\t            extra_pos_embeddings=extra_pos_embeddings,\n", "            activation_function=activation_function,\n\t            vocab_size=vocab_size,\n\t            d_model=d_model,\n\t            encoder_ffn_dim=encoder_ffn_dim,\n\t            encoder_layers=encoder_layers,\n\t            encoder_attention_heads=encoder_attention_heads,\n\t            decoder_ffn_dim=decoder_ffn_dim,\n\t            decoder_layers=decoder_layers,\n\t            decoder_attention_heads=decoder_attention_heads,\n\t            encoder_layerdrop=encoder_layerdrop,\n", "            decoder_layerdrop=decoder_layerdrop,\n\t            attention_dropout=attention_dropout,\n\t            dropout=dropout,\n\t            max_position_embeddings=max_position_embeddings,\n\t            init_std=init_std,\n\t            classif_dropout=classif_dropout,\n\t            num_labels=num_labels,\n\t            is_encoder_decoder=is_encoder_decoder,\n\t            pad_token_id=pad_token_id,\n\t            bos_token_id=bos_token_id,\n", "            eos_token_id=eos_token_id,\n\t            normalize_before=normalize_before,\n\t            add_final_layer_norm=add_final_layer_norm,\n\t            scale_embedding=scale_embedding,\n\t            normalize_embedding=normalize_embedding,\n\t            static_position_embeddings=static_position_embeddings,\n\t            add_bias_logits=add_bias_logits,\n\t            decoder_start_token_id=decoder_start_token_id,\n\t            **common_kwargs\n\t        )\n", "        self.image_feature_size = image_feature_size\n\t        self.img_feat_id = img_feat_id\n\t        self.cls_token_id = cls_token_id\n\t        self.partial_load = partial_load\n\t        self.num_attributes = num_attributes\n\t        self.num_relations = num_relations\n\t        self.lm_loss_factor = lm_loss_factor\n\t        self.mrm_loss_factor = mrm_loss_factor\n\t        self.attribute_loss_factor = attribute_loss_factor\n\t        self.relation_loss_factor = relation_loss_factor\n", "        self.num_aspects = num_aspects\n"]}
{"filename": "src/model/model_for_prompt.py", "chunked_list": ["# Based on transformers.modeling_bart\n\tfrom typing import Optional, Tuple\n\tfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\n\tfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\n\tfrom fastNLP.modules.torch import State\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n\t                                     BartDecoder, BartModel,\n", "                                     BartClassificationHead,\n\t                                     _make_linear_from_emb,\n\t                                     _prepare_bart_decoder_inputs)\n\tfrom transformers import BartTokenizer\n\tfrom src.model.config import MultiModalBartConfig\n\tfrom src.model.mixins import GenerationMixin, FromPretrainedMixin\n\tfrom src.model.modules_for_prompt import MultiModalBartEncoder, MultiModalBartDecoder_span, MultiModalBartDecoder_MLM, MultiModalBartDecoder_sentiment, Span_loss, MultiModalBartDecoder_MRM, MultiModalBartDecoder_ANP_generate\n\t# This is based on transformers.BartModel\n\t# The modifications are:\n\t# - BartConfig -> MultiModalBartConfig\n", "# - BartEncoder -> MultiModalBartEncoder\n\t# - added image_features in forward\n\t# def generate_span_mask(spans):\n\t#     max_len = max([len(x) for x in spans])\n\t#     mask = torch.ones(())\n\tclass MultiModalBartModelForPretrain(FromPretrainedMixin, PretrainedBartModel):\n\t    def build_model(self,\n\t                    args,\n\t                    bart_model,\n\t                    tokenizer,\n", "                    label_ids,\n\t                    config,\n\t                    decoder_type=None,\n\t                    copy_gate=False,\n\t                    use_encoder_mlp=False,\n\t                    use_recur_pos=False,\n\t                    tag_first=False):\n\t        if args.bart_init:\n\t            model = BartModel.from_pretrained(bart_model)\n\t            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n", "            model.resize_token_embeddings(\n\t                len(tokenizer.unique_no_split_tokens) + num_tokens)\n\t            encoder = model.encoder\n\t            decoder = model.decoder\n\t            padding_idx = config.pad_token_id\n\t            encoder.embed_tokens.padding_idx = padding_idx\n\t            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\t            for token in tokenizer.unique_no_split_tokens:\n\t                if token[:2] == '<<':  # 特殊字符\n\t                    index = tokenizer.convert_tokens_to_ids(\n", "                        tokenizer._base_tokenizer.tokenize(token))\n\t                    if len(index) > 1:\n\t                        raise RuntimeError(f\"{token} wrong split\")\n\t                    else:\n\t                        index = index[0]\n\t                    assert index >= num_tokens, (index, num_tokens, token)\n\t                    indexes = _tokenizer.convert_tokens_to_ids(\n\t                        _tokenizer.tokenize(token[2:-2]))\n\t                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n\t                    for i in indexes[1:]:\n", "                        embed += model.decoder.embed_tokens.weight.data[i]\n\t                    embed /= len(indexes)\n\t                    model.decoder.embed_tokens.weight.data[index] = embed\n\t        else:\n\t            raise RuntimeError(\"error init!!!!!!!\")\n\t        multimodal_encoder = MultiModalBartEncoder(config, encoder,\n\t                                                   tokenizer.img_feat_id,\n\t                                                   tokenizer.cls_token_id)\n\t        return (multimodal_encoder, decoder)\n\t    def __init__(self, config: MultiModalBartConfig, bart_model, tokenizer,\n", "                 label_ids, senti_ids, args):\n\t        super().__init__(config)\n\t        self.config = config\n\t        label_ids = sorted(label_ids)\n\t        multimodal_encoder, share_decoder = self.build_model(\n\t            args, bart_model, tokenizer, label_ids, config)\n\t        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n\t        self.causal_mask = causal_mask.triu(diagonal=1)\n\t        self.encoder = multimodal_encoder\n\t        self.mlm_decoder = MultiModalBartDecoder_MLM(self.config,\n", "                                                     share_decoder)\n\t        self.mrm_decoder = MultiModalBartDecoder_MRM(self.config,\n\t                                                     share_decoder,\n\t                                                     self.causal_mask, args)\n\t        self.span_decoder = MultiModalBartDecoder_span(self.config, tokenizer,\n\t                                                       share_decoder,\n\t                                                       tokenizer.pad_token_id,\n\t                                                       label_ids,\n\t                                                       self.causal_mask)\n\t        self.span_loss_fct = Span_loss()\n", "        self.anp_generate_decoder = MultiModalBartDecoder_ANP_generate(\n\t            self.config, share_decoder)\n\t        self.senti_decoder = MultiModalBartDecoder_sentiment(\n\t            self.config, share_decoder, senti_ids)\n\t    def prepare_state(self,\n\t                      input_ids,\n\t                      image_features,\n\t                      attention_mask=None,\n\t                      first=None):\n\t        dict = self.encoder(input_ids=input_ids,\n", "                            image_features=image_features,\n\t                            attention_mask=attention_mask,\n\t                            output_hidden_states=True,\n\t                            return_dict=True)\n\t        encoder_outputs = dict.last_hidden_state\n\t        hidden_states = dict.hidden_states\n\t        encoder_mask = attention_mask\n\t        src_embed_outputs = hidden_states[0]\n\t        state = BartState(encoder_outputs, encoder_mask, input_ids[:, 64:],\n\t                          first, src_embed_outputs)\n", "        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n\t        return state\n\t    def forward(\n\t            self,\n\t            task_type,\n\t            input_ids,\n\t            image_features,\n\t            attention_mask=None,\n\t            mlm_infos=None,\n\t            mrm_infos=None,\n", "            senti_infos=None,\n\t            ANP_infos=None,\n\t            ANP_generate_infos=None,\n\t            ae_infos=None,\n\t            oe_infos=None,\n\t            ae_oe_infos=None,\n\t            encoder_outputs: Optional[Tuple] = None,\n\t            use_cache=None,\n\t            output_attentions=None,\n\t            output_hidden_states=None,\n", "    ):\n\t        if encoder_outputs is None:\n\t            encoder_outputs = self.encoder(\n\t                input_ids=input_ids,\n\t                image_features=image_features,\n\t                attention_mask=attention_mask,\n\t                output_attentions=output_attentions,\n\t                output_hidden_states=output_hidden_states,\n\t            )\n\t        assert isinstance(encoder_outputs, tuple)\n", "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n\t        if task_type == 'MLM':\n\t            labels, decoder_input_ids, decoder_attention_mask = [\n\t                mlm_infos['mlm_labels'], mlm_infos['mlm_decoder_input_ids'],\n\t                mlm_infos['mlm_decoder_attention_mask']\n\t            ]\n\t            loss = self.mlm_decoder(labels, input_ids, encoder_outputs[0],\n\t                                    attention_mask, decoder_input_ids,\n\t                                    decoder_attention_mask)\n\t        elif task_type == 'MRM':\n", "            mrm_labels, mrm_masks, decoder_input_ids, decoder_attention_mask = [\n\t                mrm_infos['mrm_labels'],\n\t                mrm_infos['mrm_masks'].to(input_ids.device),\n\t                mrm_infos['mrm_decoder_input_ids'].to(input_ids.device),\n\t                mrm_infos['mrm_decoder_attention_mask'].to(input_ids.device)\n\t            ]\n\t            loss = self.mrm_decoder(mrm_labels, mrm_masks, encoder_outputs[0],\n\t                                    attention_mask, decoder_input_ids,\n\t                                    decoder_attention_mask)\n\t        elif task_type == 'Sentiment':\n", "            senti_labels, decoder_input_ids, decoder_attention_mask = [\n\t                senti_infos['senti_labels'],\n\t                senti_infos['senti_decoder_input_ids'],\n\t                senti_infos['senti_decoder_attention_mask']\n\t            ]\n\t            loss, predict_senti = self.senti_decoder(senti_labels,\n\t                                                     encoder_outputs[0],\n\t                                                     attention_mask,\n\t                                                     decoder_input_ids)\n\t        elif task_type == 'ANP_generate':\n", "            labels, decoder_input_ids, decoder_attention_mask = [\n\t                ANP_generate_infos['anp_generate_labels'],\n\t                ANP_generate_infos['anp_generate_decoder_input_ids'],\n\t                ANP_generate_infos['anp_generate_decoder_attention_mask']\n\t            ]\n\t            loss = self.anp_generate_decoder(labels, input_ids,\n\t                                             encoder_outputs[0],\n\t                                             attention_mask, decoder_input_ids,\n\t                                             decoder_attention_mask)\n\t        elif task_type == 'AE_OE':\n", "            spans, span_mask = [\n\t                ae_oe_infos['labels'].to(input_ids.device),\n\t                ae_oe_infos['masks'].to(input_ids.device)\n\t            ]\n\t            state = self.prepare_state(input_ids, image_features,\n\t                                       attention_mask)\n\t            logits = self.span_decoder(spans, state)\n\t            loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n\t        else:\n\t            raise RuntimeError(\"task type error!!!!!!!\")\n", "        if task_type == 'Sentiment':\n\t            return loss, predict_senti\n\t        return loss\n\tclass BartState(State):\n\t    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n\t                 src_embed_outputs):\n\t        super().__init__(encoder_output, encoder_mask)\n\t        self.past_key_values = None\n\t        self.src_tokens = src_tokens\n\t        self.first = first\n", "        self.src_embed_outputs = src_embed_outputs\n\t    def reorder_state(self, indices: torch.LongTensor):\n\t        super().reorder_state(indices)\n\t        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n\t        if self.first is not None:\n\t            self.first = self._reorder_state(self.first, indices)\n\t        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n\t                                                     indices)\n\t        if self.past_key_values is not None:\n\t            new = []\n", "            for layer in self.past_key_values:\n\t                new_layer = {}\n\t                for key1 in list(layer.keys()):\n\t                    new_layer_ = {}\n\t                    for key2 in list(layer[key1].keys()):\n\t                        if layer[key1][key2] is not None:\n\t                            layer[key1][key2] = self._reorder_state(\n\t                                layer[key1][key2], indices)\n\t                        new_layer_[key2] = layer[key1][key2]\n\t                    new_layer[key1] = new_layer_\n", "                new.append(new_layer)\n\t            self.past_key_values = new"]}
{"filename": "src/model/metrics.py", "chunked_list": ["from collections import Counter\n\timport numpy as np\n\timport torch\n\tclass AESCSpanMetric(object):\n\t    def __init__(self,\n\t                 eos_token_id,\n\t                 num_labels,\n\t                 conflict_id,\n\t                 opinion_first=False):\n\t        super(AESCSpanMetric, self).__init__()\n", "        self.eos_token_id = eos_token_id\n\t        self.word_start_index = num_labels + 2\n\t        self.aesc_fp = 0\n\t        self.aesc_tp = 0\n\t        self.aesc_fn = 0\n\t        self.ae_fp = 0\n\t        self.ae_tp = 0\n\t        self.ae_fn = 0\n\t        self.sc_fp = Counter()\n\t        self.sc_tp = Counter()\n", "        self.sc_fn = Counter()\n\t        self.sc_right = 0\n\t        self.sc_all_num = 0\n\t        self.em = 0\n\t        self.total = 0\n\t        self.invalid = 0\n\t        self.conflict_id = conflict_id\n\t        # assert opinion_first is False, \"Current metric only supports aspect first\"\n\t    def evaluate(self, aesc_target_span, pred, tgt_tokens):\n\t        # print('aesc_target_span', aesc_target_span[0])\n", "        # print(pred[0])\n\t        # print(tgt_tokens[0])\n\t        self.total += pred.size(0)\n\t        pred_eos_index = pred.flip(dims=[1]).eq(\n\t            self.eos_token_id).cumsum(dim=1).long()\n\t        target_eos_index = tgt_tokens.flip(dims=[1]).eq(\n\t            self.eos_token_id).cumsum(dim=1).long()\n\t        pred = pred[:, 1:]  # 去掉</s>\n\t        tgt_tokens = tgt_tokens[:, 1:]\n\t        pred_seq_len = pred_eos_index.flip(dims=[1]).eq(\n", "            pred_eos_index[:, -1:]).sum(dim=1)  # bsz\n\t        pred_seq_len = (pred_seq_len - 2).tolist()\n\t        target_seq_len = target_eos_index.flip(dims=[1]).eq(\n\t            target_eos_index[:, -1:]).sum(dim=1)  # bsz\n\t        target_seq_len = (target_seq_len - 2).tolist()\n\t        pred_spans = []\n\t        flag = True\n\t        for i, (ts, ps) in enumerate(zip(aesc_target_span, pred.tolist())):\n\t            em = 0\n\t            assert ps[0] == tgt_tokens[i, 0]\n", "            ps = ps[2:pred_seq_len[i]]\n\t            if pred_seq_len[i] == target_seq_len[i]:\n\t                em = int(tgt_tokens[i, :target_seq_len[i]].eq(\n\t                    pred[i, :target_seq_len[i]]).sum().item() ==\n\t                         target_seq_len[i])\n\t            self.em += em\n\t            invalid = 0\n\t            pairs = []\n\t            cur_pair = []\n\t            if len(ps):\n", "                for index, j in enumerate(ps):\n\t                    if j < self.word_start_index:\n\t                        cur_pair.append(j)\n\t                        if len(cur_pair) != 3 or cur_pair[0] > cur_pair[1]:\n\t                            invalid = 1\n\t                        else:\n\t                            pairs.append(tuple(cur_pair))\n\t                        cur_pair = []\n\t                    else:\n\t                        cur_pair.append(j)\n", "            pred_spans.append(pairs.copy())\n\t            # print(pred_spans)\n\t            self.invalid += invalid\n\t            aesc_target_counter = Counter()\n\t            aesc_pred_counter = Counter()\n\t            ae_target_counter = Counter()\n\t            ae_pred_counter = Counter()\n\t            conflicts = set()\n\t            # if flag:\n\t            #     print(tgt_tokens[0])\n", "            #     print(pred[0])\n\t            #     print(ts)\n\t            #     print(pairs)\n\t            #     flag = False\n\t            for t in ts:\n\t                ae_target_counter[(t[0], t[1])] = 1\n\t                if t[2] != self.conflict_id:\n\t                    aesc_target_counter[(t[0], t[1])] = t[2]\n\t                else:\n\t                    conflicts.add((t[0], t[1]))\n", "            for p in pairs:\n\t                ae_pred_counter[(p[0], p[1])] = 1\n\t                if (p[0], p[1]) not in conflicts and p[-1] not in (\n\t                        0, 1, self.conflict_id):\n\t                    aesc_pred_counter[(p[0], p[1])] = p[-1]\n\t            # 这里相同的pair会被计算多次\n\t            tp, fn, fp = _compute_tp_fn_fp(\n\t                [(key[0], key[1], value)\n\t                 for key, value in aesc_pred_counter.items()],\n\t                [(key[0], key[1], value)\n", "                 for key, value in aesc_target_counter.items()])\n\t            self.aesc_fn += fn\n\t            self.aesc_fp += fp\n\t            self.aesc_tp += tp\n\t            tp, fn, fp = _compute_tp_fn_fp(list(aesc_pred_counter.keys()),\n\t                                           list(aesc_target_counter.keys()))\n\t            self.ae_fn += fn\n\t            self.ae_fp += fp\n\t            self.ae_tp += tp\n\t            # sorry, this is a very wrongdoing, but to make it comparable with previous work, we have to stick to the\n", "            #   error\n\t            for key in aesc_pred_counter:\n\t                if key not in aesc_target_counter:\n\t                    continue\n\t                self.sc_all_num += 1\n\t                if aesc_target_counter[key] == aesc_pred_counter[key]:\n\t                    self.sc_tp[aesc_pred_counter[key]] += 1\n\t                    self.sc_right += 1\n\t                    aesc_target_counter.pop(key)\n\t                else:\n", "                    self.sc_fp[aesc_pred_counter[key]] += 1\n\t                    self.sc_fn[aesc_target_counter[key]] += 1\n\t    def pri(self):\n\t        print('aesc_fp tp fn', self.aesc_fp, self.aesc_tp, self.aesc_fn)\n\t        print('ae_fp tp fn', self.ae_fp, self.ae_tp, self.ae_fn)\n\t    def get_metric(self, reset=True):\n\t        res = {}\n\t        f, pre, rec = _compute_f_pre_rec(1, self.aesc_tp, self.aesc_fn,\n\t                                         self.aesc_fp)\n\t        res['aesc_f'] = round(f * 100, 2)\n", "        res['aesc_rec'] = round(rec * 100, 2)\n\t        res['aesc_pre'] = round(pre * 100, 2)\n\t        f, pre, rec = _compute_f_pre_rec(1, self.ae_tp, self.ae_fn, self.ae_fp)\n\t        res['ae_f'] = round(f * 100, 2)\n\t        res['ae_rec'] = round(rec * 100, 2)\n\t        res['ae_pre'] = round(pre * 100, 2)\n\t        tags = set(self.sc_tp.keys())\n\t        tags.update(set(self.sc_fp.keys()))\n\t        tags.update(set(self.sc_fn.keys()))\n\t        f_sum = 0\n", "        pre_sum = 0\n\t        rec_sum = 0\n\t        for tag in tags:\n\t            assert tag not in (0, 1, self.conflict_id), (tag, self.conflict_id)\n\t            tp = self.sc_tp[tag]\n\t            fn = self.sc_fn[tag]\n\t            fp = self.sc_fp[tag]\n\t            f, pre, rec = _compute_f_pre_rec(1, tp, fn, fp)\n\t            f_sum += f\n\t            pre_sum += pre\n", "            rec_sum += rec\n\t        rec_sum /= (len(tags) + 1e-12)\n\t        pre_sum /= (len(tags) + 1e-12)\n\t        res['sc_f'] = round(\n\t            2 * pre_sum * rec_sum / (pre_sum + rec_sum + 1e-12) * 100, 2)\n\t        res['sc_rec'] = round(rec_sum * 100, 2)\n\t        res['sc_pre'] = round(pre_sum * 100, 2)\n\t        res['sc_acc'] = round(\n\t            1.0 * self.sc_right / (self.sc_all_num + 1e-12) * 100, 2)\n\t        res['sc_all_num'] = self.sc_all_num\n", "        res['em'] = round(self.em / self.total, 4)\n\t        res['invalid'] = round(self.invalid / self.total, 4)\n\t        if reset:\n\t            self.aesc_fp = 0\n\t            self.aesc_tp = 0\n\t            self.aesc_fn = 0\n\t            self.ae_fp = 0\n\t            self.ae_tp = 0\n\t            self.ae_fn = 0\n\t            self.sc_all_num = 0\n", "            self.sc_right = 0\n\t            self.sc_fp = Counter()\n\t            self.sc_tp = Counter()\n\t            self.sc_fn = Counter()\n\t        return res\n\tdef _compute_f_pre_rec(beta_square, tp, fn, fp):\n\t    r\"\"\"\n\t    :param tp: int, true positive\n\t    :param fn: int, false negative\n\t    :param fp: int, false positive\n", "    :return: (f, pre, rec)\n\t    \"\"\"\n\t    pre = tp / (fp + tp + 1e-13)\n\t    rec = tp / (fn + tp + 1e-13)\n\t    f = (1 + beta_square) * pre * rec / (beta_square * pre + rec + 1e-13)\n\t    return f, pre, rec\n\tdef _compute_tp_fn_fp(ps, ts):\n\t    ps = ps.copy()\n\t    tp = 0\n\t    fp = 0\n", "    fn = 0\n\t    # print(ts)\n\t    # print(ps)\n\t    if isinstance(ts, (list, set)):\n\t        ts = {key: 1 for key in list(ts)}\n\t    if isinstance(ps, (list, set)):\n\t        ps = {key: 1 for key in list(ps)}\n\t    for key in ts.keys():\n\t        # print(key)\n\t        t_num = ts[key]\n", "        if key not in ps:\n\t            p_num = 0\n\t        else:\n\t            p_num = ps[key]\n\t        # print(p_num, t_num)\n\t        tp += min(p_num, t_num)\n\t        fp += max(p_num - t_num, 0)\n\t        fn += max(t_num - p_num, 0)\n\t        # print(fp, tp, fn)\n\t        if key in ps:\n", "            ps.pop(key)\n\t    fp += sum(ps.values())\n\t    # print(fp, tp, fn)\n\t    return tp, fn, fp\n\tclass OESpanMetric(object):\n\t    def __init__(self, eos_token_id, num_labels, opinion_first=True):\n\t        super(OESpanMetric, self).__init__()\n\t        self.eos_token_id = eos_token_id\n\t        self.word_start_index = num_labels + 2\n\t        self.oe_fp = 0\n", "        self.oe_tp = 0\n\t        self.oe_fn = 0\n\t        self.em = 0\n\t        self.total = 0\n\t        self.invalid = 0\n\t        # assert opinion_first is False, \"Current metric only supports aspect first\"\n\t        self.opinin_first = opinion_first\n\t    def evaluate(self, oe_target_span, pred, tgt_tokens):\n\t        self.total += pred.size(0)\n\t        pred_eos_index = pred.flip(dims=[1]).eq(\n", "            self.eos_token_id).cumsum(dim=1).long()\n\t        target_eos_index = tgt_tokens.flip(dims=[1]).eq(\n\t            self.eos_token_id).cumsum(dim=1).long()\n\t        pred = pred[:, 1:]  # 去掉</s>\n\t        tgt_tokens = tgt_tokens[:, 1:]\n\t        pred_seq_len = pred_eos_index.flip(dims=[1]).eq(\n\t            pred_eos_index[:, -1:]).sum(dim=1)  # bsz\n\t        pred_seq_len = (pred_seq_len - 2).tolist()\n\t        target_seq_len = target_eos_index.flip(dims=[1]).eq(\n\t            target_eos_index[:, -1:]).sum(dim=1)  # bsz\n", "        target_seq_len = (target_seq_len - 2).tolist()\n\t        pred_spans = []\n\t        flag = True\n\t        for i, (ts, ps) in enumerate(zip(oe_target_span, pred.tolist())):\n\t            em = 0\n\t            assert ps[0] == tgt_tokens[i, 0]\n\t            ps = ps[2:pred_seq_len[i]]\n\t            if pred_seq_len[i] == target_seq_len[i]:\n\t                em = int(tgt_tokens[i, :target_seq_len[i]].eq(\n\t                    pred[i, :target_seq_len[i]]).sum().item() ==\n", "                         target_seq_len[i])\n\t            self.em += em\n\t            invalid = 0\n\t            pairs = []\n\t            cur_pair = []\n\t            if len(ps):\n\t                for index, j in enumerate(ps, start=1):\n\t                    if index % 2 == 0:\n\t                        cur_pair.append(j)\n\t                        if cur_pair[0]>cur_pair[1] or cur_pair[0]<self.word_start_index\\\n", "                                or cur_pair[1]<self.word_start_index:\n\t                            invalid = 1\n\t                        else:\n\t                            pairs.append(tuple(cur_pair))\n\t                        cur_pair = []\n\t                    else:\n\t                        cur_pair.append(j)\n\t            self.invalid += invalid\n\t            oe_target_counter = Counter([tuple(t) for t in ts])\n\t            oe_pred_counter = Counter(pairs)\n", "            # if flag:\n\t            #     print(tgt_tokens[0])\n\t            #     print(pred[0])\n\t            #     print(ts)\n\t            #     print(pairs)\n\t            #     flag = False\n\t            # 这里相同的pair会被计算多次\n\t            tp, fn, fp = _compute_tp_fn_fp(set(list(oe_pred_counter.keys())),\n\t                                           set(list(oe_target_counter.keys())))\n\t            self.oe_fn += fn\n", "            self.oe_fp += fp\n\t            self.oe_tp += tp\n\t    def get_metric(self, reset=True):\n\t        res = {}\n\t        f, pre, rec = _compute_f_pre_rec(1, self.oe_tp, self.oe_fn, self.oe_fp)\n\t        res['oe_f'] = round(f * 100, 2)\n\t        res['oe_rec'] = round(rec * 100, 2)\n\t        res['oe_pre'] = round(pre * 100, 2)\n\t        res['em'] = round(self.em / self.total, 4)\n\t        res['invalid'] = round(self.invalid / self.total, 4)\n", "        if reset:\n\t            self.oe_fp = 0\n\t            self.oe_tp = 0\n\t            self.oe_fn = 0\n\t        return res\n\t# metric = AESCSpanMetric(1, 3, -1)\n\t# spans = [[(6, 7, 3), (9, 10, 4)]]\n\t# pred = torch.tensor([[0, 2, 2, 6, 7, 3, 9, 9, 4, 1, 1]])\n\t# print(pred.size())\n\t# tgt = torch.tensor([[0, 2, 2, 6, 7, 3, 9, 10, 4, 1, 1]])\n", "# metric.evaluate(spans, pred, tgt)\n\t# metric.pri()"]}
{"filename": "src/model/generater_for_generated_prompt_multitasks.py", "chunked_list": ["r\"\"\"undocumented\"\"\"\n\timport torch\n\tfrom torch import nn\n\tfrom fastNLP.models.torch.seq2seq_model import Seq2SeqModel\n\tfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\n\timport torch.nn.functional as F\n\t# from fastNLP.core.utils import _get_model_device\n\tfrom functools import partial\n\tdef _get_model_device(model):\n\t    r\"\"\"\n", "    传入一个nn.Module的模型，获取它所在的device\n\t    :param model: nn.Module\n\t    :return: torch.device,None 如果返回值为None，说明这个模型没有任何参数。\n\t    \"\"\"\n\t    # TODO 这个函数存在一定的风险，因为同一个模型可能存在某些parameter不在显卡中，比如BertEmbedding. 或者跨显卡\n\t    assert isinstance(model, nn.Module)\n\t    parameters = list(model.parameters())\n\t    if len(parameters) == 0:\n\t        return None\n\t    else:\n", "        return parameters[0].device\n\tclass SequenceGeneratorModel(nn.Module):\n\t    \"\"\"\n\t    用于封装Seq2SeqModel使其可以做生成任务\n\t    \"\"\"\n\t    def __init__(self,\n\t                 seq2seq_model: Seq2SeqModel,\n\t                 bos_token_id,\n\t                 eos_token_id=None,\n\t                 max_length=30,\n", "                 max_len_a=0.0,\n\t                 num_beams=1,\n\t                 do_sample=True,\n\t                 sc_only=False,\n\t                 repetition_penalty=1,\n\t                 length_penalty=1.0,\n\t                 pad_token_id=0,\n\t                 restricter=None):\n\t        \"\"\"\n\t        :param Seq2SeqModel seq2seq_model: 序列到序列模型. 会使用seq2seq_model的decoder进行生成\n", "        :param int,None bos_token_id: 句子开头的token id\n\t        :param int,None eos_token_id: 句子结束的token id\n\t        :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n\t        :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n\t        :param int num_beams: beam search的大小\n\t        :param bool do_sample: 是否通过采样的方式生成\n\t        :param float temperature: 只有在do_sample为True才有意义\n\t        :param int top_k: 只从top_k中采样\n\t        :param float top_p: 只从top_p的token中采样，nucles sample\n\t        :param float repetition_penalty: 多大程度上惩罚重复的token\n", "        :param float length_penalty: 对长度的惩罚，小于1鼓励长句，大于1鼓励短剧\n\t        :param int pad_token_id: 当某句话生成结束之后，之后生成的内容用pad_token_id补充\n\t        \"\"\"\n\t        super().__init__()\n\t        self.seq2seq_model = seq2seq_model\n\t        self.restricter = restricter\n\t        self.sc_only = sc_only\n\t        self.generator = SequenceGenerator(\n\t            seq2seq_model.decoder,\n\t            max_length=max_length,\n", "            max_len_a=max_len_a,\n\t            num_beams=num_beams,\n\t            do_sample=do_sample,\n\t            sc_only=sc_only,\n\t            bos_token_id=bos_token_id,\n\t            eos_token_id=eos_token_id,\n\t            repetition_penalty=repetition_penalty,\n\t            length_penalty=length_penalty,\n\t            pad_token_id=pad_token_id,\n\t            restricter=restricter)\n", "    def forward(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                aesc_infos=None,\n\t                aspects_num=None,\n\t                first=None):\n\t        \"\"\"\n\t        透传调用seq2seq_model的forward\n\t        :param torch.LongTensor src_tokens: bsz x max_len\n", "        :param torch.LongTensor tgt_tokens: bsz x max_len'\n\t        :param torch.LongTensor src_seq_len: bsz\n\t        :param torch.LongTensor tgt_seq_len: bsz\n\t        :return:\n\t        \"\"\"\n\t        return self.seq2seq_model(input_ids=input_ids,\n\t                                  image_features=image_features,\n\t                                  attention_mask=attention_mask,\n\t                                  aesc_infos=aesc_infos,\n\t                                  aspects_num=aspects_num)\n", "    def predict(self,\n\t                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                aesc_infos=None,\n\t                aspects_num=None):\n\t        \"\"\"\n\t        给定source的内容，输出generate的内容\n\t        :param torch.LongTensor src_tokens: bsz x max_len\n\t        :param torch.LongTensor src_seq_len: bsz\n", "        :return:\n\t        \"\"\"\n\t        # import ipdb; ipdb.set_trace()\n\t        state, aspects_num_loss, predict_aspects_num = self.seq2seq_model.prepare_state(\n\t                                                 input_ids, image_features,\n\t                                                 attention_mask,\n\t                                                 aesc_infos,\n\t                                                 aspects_num)\n\t        tgt_tokens = aesc_infos['labels'].to(input_ids.device)\n\t        # print()\n", "        result = self.generator.generate(\n\t            state,\n\t            tokens=tgt_tokens[:, :3])  # the prompt is provided to the model\n\t        return result, predict_aspects_num\n\tr\"\"\"\n\t\"\"\"\n\t__all__ = ['SequenceGenerator']\n\tclass SequenceGenerator:\n\t    \"\"\"\n\t    给定一个Seq2SeqDecoder，decode出句子\n", "    \"\"\"\n\t    def __init__(self,\n\t                 decoder: Seq2SeqDecoder,\n\t                 max_length=20,\n\t                 max_len_a=0.0,\n\t                 num_beams=1,\n\t                 do_sample=False,\n\t                 sc_only=False,\n\t                 bos_token_id=None,\n\t                 eos_token_id=None,\n", "                 repetition_penalty=1,\n\t                 length_penalty=1.0,\n\t                 pad_token_id=0,\n\t                 restricter=None):\n\t        \"\"\"\n\t        :param Seq2SeqDecoder decoder: Decoder对象\n\t        :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n\t        :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n\t        :param int num_beams: beam search的大小\n\t        :param bool do_sample: 是否通过采样的方式生成\n", "        :param float temperature: 只有在do_sample为True才有意义\n\t        :param int top_k: 只从top_k中采样\n\t        :param float top_p: 只从top_p的token中采样，nucles sample\n\t        :param int,None bos_token_id: 句子开头的token id\n\t        :param int,None eos_token_id: 句子结束的token id\n\t        :param float repetition_penalty: 多大程度上惩罚重复的token\n\t        :param float length_penalty: 对长度的惩罚，小于1鼓励长句，大于1鼓励短剧\n\t        :param int pad_token_id: 当某句话生成结束之后，之后生成的内容用pad_token_id补充\n\t        \"\"\"\n\t        self.generate_func = partial(greedy_generate,\n", "                                     decoder=decoder,\n\t                                     max_length=max_length,\n\t                                     max_len_a=max_len_a,\n\t                                     num_beams=num_beams,\n\t                                     sc_only=sc_only,\n\t                                     bos_token_id=bos_token_id,\n\t                                     eos_token_id=eos_token_id,\n\t                                     repetition_penalty=repetition_penalty,\n\t                                     length_penalty=length_penalty,\n\t                                     pad_token_id=pad_token_id,\n", "                                     restricter=restricter)\n\t        self.do_sample = do_sample\n\t        self.max_length = max_length\n\t        self.num_beams = num_beams\n\t        self.bos_token_id = bos_token_id\n\t        self.eos_token_id = eos_token_id\n\t        self.repetition_penalty = repetition_penalty\n\t        self.length_penalty = length_penalty\n\t        self.decoder = decoder\n\t        self.pad_token_id = pad_token_id\n", "        self.restricter = restricter\n\t        self.max_len_a = max_len_a\n\t        self.sc_only = sc_only\n\t    def set_new_generator(self,\n\t                          max_length=-1,\n\t                          max_len_a=-1,\n\t                          num_beams=-1,\n\t                          repetition_penalty=-1,\n\t                          length_penalty=-1,\n\t                          restricter=-1):\n", "        if max_length == -1:\n\t            max_length = self.max_length\n\t        if max_len_a == -1:\n\t            max_len_a = self.max_len_a\n\t        if num_beams == -1:\n\t            num_beams = self.num_beams\n\t        if repetition_penalty == -1:\n\t            repetition_penalty = self.repetition_penalty\n\t        if length_penalty == -1:\n\t            length_penalty = self.length_penalty\n", "        if restricter == -1:\n\t            restricter = self.restricter\n\t        self.generate_func = partial(greedy_generate,\n\t                                     decoder=self.decoder,\n\t                                     max_length=max_length,\n\t                                     max_len_a=max_len_a,\n\t                                     num_beams=num_beams,\n\t                                     sc_only=self.sc_only,\n\t                                     bos_token_id=self.bos_token_id,\n\t                                     eos_token_id=self.eos_token_id,\n", "                                     repetition_penalty=repetition_penalty,\n\t                                     length_penalty=length_penalty,\n\t                                     pad_token_id=self.pad_token_id,\n\t                                     restricter=restricter)\n\t    @torch.no_grad()\n\t    def generate(self, state, tokens=None, gt_tokens=None):\n\t        \"\"\"\n\t        :param State state: encoder结果的State, 是与Decoder配套是用的\n\t        :param torch.LongTensor,None tokens: batch_size x length, 开始的token\n\t        :return: bsz x max_length' 生成的token序列。如果eos_token_id不为None, 每个sequence的结尾一定是eos_token_id\n", "        \"\"\"\n\t        return self.generate_func(tokens=tokens,\n\t                                  gt_tokens=gt_tokens,\n\t                                  state=state)\n\t@torch.no_grad()\n\tdef greedy_generate(decoder,\n\t                    tokens=None,\n\t                    gt_tokens=None,\n\t                    state=None,\n\t                    sc_eval=False,\n", "                    max_length=20,\n\t                    max_len_a=0.0,\n\t                    num_beams=1,\n\t                    sc_only=False,\n\t                    bos_token_id=None,\n\t                    eos_token_id=None,\n\t                    pad_token_id=0,\n\t                    repetition_penalty=1,\n\t                    length_penalty=1.0,\n\t                    restricter=None):\n", "    \"\"\"\n\t    贪婪地搜索句子\n\t    :param Decoder decoder: Decoder对象\n\t    :param torch.LongTensor tokens: batch_size x len, decode的输入值，如果为None，则自动从bos_token_id开始生成\n\t    :param State state: 应该包含encoder的一些输出。\n\t    :param int max_length: 生成句子的最大长度, 每句话的decode长度为max_length + max_len_a*src_len\n\t    :param float max_len_a: 每句话的decode长度为max_length + max_len_a*src_len。 如果不为0，需要保证State中包含encoder_mask\n\t    :param int num_beams: 使用多大的beam进行解码。\n\t    :param int bos_token_id: 如果tokens传入为None，则使用bos_token_id开始往后解码。\n\t    :param int eos_token_id: 结束的token，如果为None，则一定会解码到max_length这么长。\n", "    :param int pad_token_id: pad的token id\n\t    :param float repetition_penalty: 对重复出现的token多大的惩罚。\n\t    :param float length_penalty: 对每个token（除了eos）按照长度进行一定的惩罚。\n\t    :return:\n\t    \"\"\"\n\t    # import ipdb; ipdb.set_trace()\n\t    if sc_only:\n\t        token_ids = sc_generate(decoder,\n\t                                tokens=tokens,\n\t                                gt_tokens=gt_tokens,\n", "                                state=state,\n\t                                max_length=max_length,\n\t                                max_len_a=max_len_a,\n\t                                bos_token_id=bos_token_id,\n\t                                eos_token_id=eos_token_id,\n\t                                repetition_penalty=repetition_penalty,\n\t                                length_penalty=length_penalty,\n\t                                pad_token_id=pad_token_id,\n\t                                restricter=restricter)\n\t        return token_ids\n", "    if num_beams == 1:\n\t        token_ids = _no_beam_search_generate(\n\t            decoder,\n\t            tokens=tokens,\n\t            state=state,\n\t            max_length=max_length,\n\t            max_len_a=max_len_a,\n\t            bos_token_id=bos_token_id,\n\t            eos_token_id=eos_token_id,\n\t            repetition_penalty=repetition_penalty,\n", "            length_penalty=length_penalty,\n\t            pad_token_id=pad_token_id,\n\t            restricter=restricter)\n\t    else:\n\t        token_ids = _beam_search_generate(\n\t            decoder,\n\t            tokens=tokens,\n\t            state=state,\n\t            max_length=max_length,\n\t            max_len_a=max_len_a,\n", "            num_beams=num_beams,\n\t            bos_token_id=bos_token_id,\n\t            eos_token_id=eos_token_id,\n\t            do_sample=False,\n\t            repetition_penalty=repetition_penalty,\n\t            length_penalty=length_penalty,\n\t            pad_token_id=pad_token_id,\n\t            restricter=restricter)\n\t    return token_ids\n\tdef _no_beam_search_generate(decoder: Seq2SeqDecoder,\n", "                             state,\n\t                             tokens=None,\n\t                             max_length=20,\n\t                             max_len_a=0.0,\n\t                             bos_token_id=None,\n\t                             eos_token_id=None,\n\t                             repetition_penalty=1.0,\n\t                             length_penalty=1.0,\n\t                             pad_token_id=0,\n\t                             restricter=None):\n", "    device = _get_model_device(decoder)\n\t    if tokens is None:\n\t        if bos_token_id is None:\n\t            raise RuntimeError(\n\t                \"You have to specify either `tokens` or `bos_token_id`.\")\n\t        batch_size = state.num_samples\n\t        if batch_size is None:\n\t            raise RuntimeError(\n\t                \"Cannot infer the number of samples from `state`.\")\n\t        tokens = torch.full([batch_size, 1],\n", "                            fill_value=bos_token_id,\n\t                            dtype=torch.long).to(device)\n\t    batch_size = tokens.size(0)\n\t    if state.num_samples:\n\t        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\t    if eos_token_id is None:\n\t        _eos_token_id = -1\n\t    else:\n\t        _eos_token_id = eos_token_id\n\t    scores = decoder.decode(tokens=tokens, state=state)  # 主要是为了update state\n", "    # 这里需要考虑如果在第一个位置就结束的情况\n\t    # if _eos_token_id!=-1:\n\t    #     scores[:, _eos_token_id] = -1e12\n\t    if restricter is not None:\n\t        _, next_tokens = restricter(state, tokens, scores, num_beams=1)\n\t    else:\n\t        next_tokens = scores.argmax(dim=-1, keepdim=True)\n\t    token_ids = torch.cat([tokens, next_tokens], dim=1)\n\t    cur_len = token_ids.size(1)\n\t    dones = token_ids.new_zeros(batch_size).eq(1).__or__(\n", "        next_tokens.squeeze(1).eq(eos_token_id))\n\t    # tokens = tokens[:, -1:]\n\t    if max_len_a != 0:\n\t        # (bsz x num_beams, )\n\t        if state.encoder_mask is not None:\n\t            max_lengths = (state.encoder_mask.sum(dim=1).float() *\n\t                           max_len_a).long() + max_length\n\t        else:\n\t            max_lengths = tokens.new_full((tokens.size(0), ),\n\t                                          fill_value=max_length,\n", "                                          dtype=torch.long)\n\t        real_max_length = max_lengths.max().item()\n\t    else:\n\t        real_max_length = max_length\n\t        if state.encoder_mask is not None:\n\t            max_lengths = state.encoder_mask.new_ones(\n\t                state.encoder_mask.size(0)).long() * max_length\n\t        else:\n\t            max_lengths = tokens.new_full((tokens.size(0), ),\n\t                                          fill_value=max_length,\n", "                                          dtype=torch.long)\n\t    while cur_len < real_max_length:\n\t        scores = decoder.decode(tokens=token_ids,\n\t                                state=state)  # batch_size x vocab_size\n\t        if repetition_penalty != 1.0:\n\t            token_scores = scores.gather(dim=1, index=token_ids)\n\t            lt_zero_mask = token_scores.lt(0).float()\n\t            ge_zero_mask = lt_zero_mask.eq(0).float()\n\t            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n\t            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n", "        if eos_token_id is not None and length_penalty != 1.0:\n\t            token_scores = scores / cur_len**length_penalty  # batch_size x vocab_size\n\t            eos_mask = scores.new_ones(scores.size(1))\n\t            eos_mask[eos_token_id] = 0\n\t            eos_mask = eos_mask.unsqueeze(0).eq(1)\n\t            scores = scores.masked_scatter(\n\t                eos_mask, token_scores)  # 也即除了eos，其他词的分数经过了放大/缩小\n\t        if restricter is not None:\n\t            _, next_tokens = restricter(state, token_ids, scores, 1)\n\t        else:\n", "            next_tokens = scores.argmax(dim=-1, keepdim=True)\n\t        next_tokens = next_tokens.squeeze(-1)\n\t        # 如果已经达到对应的sequence长度了，就直接填为eos了\n\t        if _eos_token_id != -1:\n\t            next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len + 1),\n\t                                                  _eos_token_id)\n\t        next_tokens = next_tokens.masked_fill(\n\t            dones, pad_token_id)  # 对已经搜索完成的sample做padding\n\t        tokens = next_tokens.unsqueeze(1)\n\t        token_ids = torch.cat([token_ids, tokens],\n", "                              dim=-1)  # batch_size x max_len\n\t        end_mask = next_tokens.eq(_eos_token_id)\n\t        dones = dones.__or__(end_mask)\n\t        cur_len += 1\n\t        if dones.min() == 1:\n\t            break\n\t    return token_ids\n\tdef sc_generate(decoder: Seq2SeqDecoder,\n\t                state,\n\t                tokens=None,\n", "                gt_tokens=None,\n\t                max_length=20,\n\t                max_len_a=0.0,\n\t                bos_token_id=None,\n\t                eos_token_id=None,\n\t                repetition_penalty=1.0,\n\t                length_penalty=1.0,\n\t                pad_token_id=0,\n\t                restricter=None):\n\t    device = _get_model_device(decoder)\n", "    if tokens is None:\n\t        if bos_token_id is None:\n\t            raise RuntimeError(\n\t                \"You have to specify either `tokens` or `bos_token_id`.\")\n\t        batch_size = state.num_samples\n\t        if batch_size is None:\n\t            raise RuntimeError(\n\t                \"Cannot infer the number of samples from `state`.\")\n\t        tokens = torch.full([batch_size, 1],\n\t                            fill_value=bos_token_id,\n", "                            dtype=torch.long).to(device)\n\t    batch_size = tokens.size(0)\n\t    # print(state.num_samples, batch_size)\n\t    if state.num_samples:\n\t        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\t    if eos_token_id is None:\n\t        _eos_token_id = -1\n\t    else:\n\t        _eos_token_id = eos_token_id\n\t    aspect_cnt = 3\n", "    next_tokens = gt_tokens[:, aspect_cnt:aspect_cnt + 2]\n\t    token_ids = torch.cat([tokens, next_tokens], dim=1)\n\t    cur_len = token_ids.size(1)\n\t    dones = token_ids.new_zeros(batch_size).eq(1)\n\t    # tokens = tokens[:, -1:]\n\t    max_len_a = 0\n\t    max_length = gt_tokens.size(1)\n\t    gt_mask = gt_tokens.eq(1).eq(0)\n\t    max_lengths = gt_mask.sum(dim=1)\n\t    while cur_len < max_length:\n", "        scores = decoder.decode(tokens=token_ids, state=state,\n\t                                only_sc=True)  # batch_size x vocab_size\n\t        if restricter is not None:\n\t            _, next_tokens = restricter(state, token_ids, scores, 1)\n\t        else:\n\t            next_tokens = scores.argmax(dim=-1, keepdim=True)\n\t        next_tokens = next_tokens.squeeze(-1)\n\t        # 如果已经达到对应的sequence长度了，就直接填为eos了\n\t        # if _eos_token_id != -1:\n\t        #     next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len + 1),\n", "        #                                           _eos_token_id)\n\t        next_tokens = next_tokens.masked_fill(\n\t            dones, pad_token_id)  # 对已经搜索完成的sample做padding\n\t        tokens = next_tokens.unsqueeze(1)\n\t        token_ids = torch.cat([token_ids, tokens],\n\t                              dim=-1)  # batch_size x max_len\n\t        # end_mask = next_tokens.eq(_eos_token_id)\n\t        # dones = dones.__or__(end_mask)\n\t        dones = gt_tokens[:, cur_len + 1].eq(1)\n\t        cur_len += 1\n", "        aspect_cnt += 3\n\t        if aspect_cnt + 2 < max_length:\n\t            token_ids = torch.cat(\n\t                [token_ids, gt_tokens[:, aspect_cnt:aspect_cnt + 2]], dim=-1)\n\t        cur_len += 2\n\t        if dones.min() == 1:\n\t            break\n\t    ones = token_ids.new_ones(batch_size).unsqueeze(-1)\n\t    token_ids = torch.cat([token_ids, ones], dim=-1)\n\t    # if eos_token_id is not None:\n", "    #     tokens.scatter(index=max_lengths[:, None], dim=1, value=eos_token_id)  # 将最大长度位置设置为eos\n\t    # if cur_len == max_length:\n\t    #     token_ids[:, -1].masked_fill_(~dones, eos_token_id)  # 若到最长长度仍未到EOS，则强制将最后一个词替换成eos\n\t    return token_ids\n\tdef _beam_search_generate(decoder: Seq2SeqDecoder,\n\t                          tokens=None,\n\t                          state=None,\n\t                          max_length=20,\n\t                          max_len_a=0.0,\n\t                          num_beams=4,\n", "                          bos_token_id=None,\n\t                          eos_token_id=None,\n\t                          do_sample=True,\n\t                          repetition_penalty=1.0,\n\t                          length_penalty=None,\n\t                          pad_token_id=0,\n\t                          restricter=None) -> torch.LongTensor:\n\t    assert do_sample is False\n\t    # 进行beam search\n\t    # import ipdb; ipdb.set_trace()\n", "    device = _get_model_device(decoder)\n\t    if tokens is None:\n\t        if bos_token_id is None:\n\t            raise RuntimeError(\n\t                \"You have to specify either `tokens` or `bos_token_id`.\")\n\t        batch_size = state.num_samples\n\t        if batch_size is None:\n\t            raise RuntimeError(\n\t                \"Cannot infer the number of samples from `state`.\")\n\t        tokens = torch.full([batch_size, 1],\n", "                            fill_value=bos_token_id,\n\t                            dtype=torch.long).to(device)\n\t    batch_size = tokens.size(0)\n\t    if state.num_samples:\n\t        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\t    if eos_token_id is None:\n\t        _eos_token_id = -1\n\t    else:\n\t        _eos_token_id = eos_token_id\n\t    scores = decoder.decode(tokens=tokens, state=state)  # 这里要传入的是整个句子的长度\n", "    # 这里需要考虑如果在第一个位置就结束的情况\n\t    # if _eos_token_id!=-1:\n\t    #     scores[:, _eos_token_id] = -1e12\n\t    vocab_size = scores.size(1)\n\t    assert vocab_size >= num_beams, \"num_beams should be smaller than the number of vocabulary size.\"\n\t    scores = F.log_softmax(scores, dim=-1)  # (batch_size, vocab_size)\n\t    # 得到(batch_size, num_beams), (batch_size, num_beams)\n\t    # TODO 把限制写到这个位置, 加1是因为需要考虑输出就是eos的情况\n\t    if restricter is not None:\n\t        _next_scores, _next_tokens = restricter(state, tokens, scores,\n", "                                                num_beams + 1)\n\t    else:\n\t        # 是bsz x (num_beams+1)大小的东西\n\t        _next_scores, _next_tokens = torch.topk(scores,\n\t                                                num_beams + 1,\n\t                                                dim=1,\n\t                                                largest=True,\n\t                                                sorted=True)\n\t    # 根据index来做顺序的调转\n\t    indices = torch.arange(batch_size, dtype=torch.long).to(device)\n", "    indices = indices.repeat_interleave(num_beams)\n\t    state.reorder_state(indices)\n\t    tokens = tokens.index_select(\n\t        dim=0, index=indices)  # batch_size * num_beams x length\n\t    if max_len_a != 0:\n\t        # (bsz x num_beams, )\n\t        if state.encoder_mask is not None:\n\t            max_lengths = (state.encoder_mask.sum(dim=1).float() *\n\t                           max_len_a).long() + max_length\n\t        else:\n", "            max_lengths = tokens.new_full((batch_size * num_beams, ),\n\t                                          fill_value=max_length,\n\t                                          dtype=torch.long)\n\t        real_max_length = max_lengths.max().item()\n\t    else:\n\t        real_max_length = max_length\n\t        if state.encoder_mask is not None:\n\t            max_lengths = state.encoder_mask.new_ones(\n\t                state.encoder_mask.size(0)).long() * max_length\n\t        else:\n", "            max_lengths = tokens.new_full((batch_size * num_beams, ),\n\t                                          fill_value=max_length,\n\t                                          dtype=torch.long)\n\t    hypos = [\n\t        BeamHypotheses(num_beams,\n\t                       real_max_length,\n\t                       length_penalty,\n\t                       early_stopping=False) for _ in range(batch_size)\n\t    ]\n\t    not_eos_mask = _next_tokens.ne(_eos_token_id)  # 为1的地方不是eos\n", "    keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # 为1的地方需要保留\n\t    keep_mask = not_eos_mask.__and__(keep_mask)  # 为1的地方是需要进行下一步search的\n\t    next_tokens = _next_tokens.masked_select(keep_mask).view(\n\t        batch_size, num_beams)  # 这是真的接下来要继续的\n\t    next_scores = _next_scores.masked_select(keep_mask).view(\n\t        batch_size, num_beams)\n\t    rows, cols = not_eos_mask.eq(0)[:, :num_beams].nonzero(as_tuple=True)\n\t    if len(rows) > 0:  # 说明有的开头就结束了\n\t        for row, col in zip(rows.tolist(), cols.tolist()):\n\t            _token = torch.cat(\n", "                [tokens[row * num_beams], _next_tokens[row, col:col + 1]],\n\t                dim=0)\n\t            hypos[row].add(_token.clone(), _next_scores[row, col].item())\n\t    # 记录生成好的token (batch_size', cur_len)\n\t    token_ids = torch.cat([tokens, next_tokens.view(-1, 1)], dim=-1)\n\t    dones = [False] * batch_size\n\t    beam_scores = next_scores.view(-1)  # batch_size * num_beams\n\t    #  用来记录已经生成好的token的长度\n\t    cur_len = token_ids.size(1)\n\t    # 0, num_beams, 2*num_beams, ...\n", "    batch_inds_with_numbeams_interval = (torch.arange(batch_size) *\n\t                                         num_beams).view(-1, 1).to(token_ids)\n\t    while cur_len < real_max_length:\n\t        scores = decoder.decode(token_ids,\n\t                                state)  # (bsz x num_beams, vocab_size)\n\t        if repetition_penalty != 1.0:\n\t            token_scores = scores.gather(dim=1, index=token_ids)\n\t            lt_zero_mask = token_scores.lt(0).float()\n\t            ge_zero_mask = lt_zero_mask.eq(0).float()\n\t            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n", "            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n\t        if _eos_token_id != -1:\n\t            max_len_eos_mask = max_lengths.eq(cur_len + 1)\n\t            eos_scores = scores[:, _eos_token_id]\n\t            # 如果已经达到最大长度，就把eos的分数加大\n\t            scores[:, _eos_token_id] = torch.where(max_len_eos_mask,\n\t                                                   eos_scores + 1e32,\n\t                                                   eos_scores)\n\t        scores = F.log_softmax(scores,\n\t                               dim=-1)  # (batch_size * num_beams, vocab_size)\n", "        _scores = scores + beam_scores[:,\n\t                                       None]  # (batch_size * num_beams, vocab_size)\n\t        _scores = _scores.view(batch_size,\n\t                               -1)  # (batch_size, num_beams*vocab_size)\n\t        # TODO 把限制加到这个位置\n\t        if restricter is not None:\n\t            next_scores, ids = restricter(state, token_ids, _scores,\n\t                                          2 * num_beams)\n\t        else:\n\t            next_scores, ids = torch.topk(_scores,\n", "                                          2 * num_beams,\n\t                                          dim=1,\n\t                                          largest=True,\n\t                                          sorted=True)  # (bsz, 2*num_beams)\n\t        from_which_beam = ids // vocab_size  # (batch_size, 2*num_beams)\n\t        next_tokens = ids % vocab_size  # (batch_size, 2*num_beams)\n\t        not_eos_mask = next_tokens.ne(_eos_token_id)  # 为1的地方不是eos\n\t        keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # 为1的地方需要保留\n\t        keep_mask = not_eos_mask.__and__(keep_mask)  # 为1的地方是需要进行下一步search的\n\t        _next_tokens = next_tokens.masked_select(keep_mask).view(-1, 1)\n", "        _from_which_beam = from_which_beam.masked_select(keep_mask).view(\n\t            batch_size, num_beams)  # 上面的token是来自哪个beam\n\t        _next_scores = next_scores.masked_select(keep_mask).view(\n\t            batch_size, num_beams)\n\t        beam_scores = _next_scores.view(-1)\n\t        flag = True\n\t        if cur_len + 1 == real_max_length:\n\t            eos_batch_idx = torch.arange(batch_size).to(\n\t                next_tokens).repeat_interleave(repeats=num_beams, dim=0)\n\t            eos_beam_ind = torch.arange(num_beams).to(token_ids).repeat(\n", "                batch_size)  # 表示的是indice\n\t            eos_beam_idx = from_which_beam[:, :num_beams].reshape(\n\t                -1)  # 表示的是从哪个beam获取得到的\n\t        else:\n\t            # 将每个batch中在num_beam内的序列添加到结束中, 为1的地方需要结束了\n\t            effective_eos_mask = next_tokens[:, :num_beams].eq(\n\t                _eos_token_id)  # batch_size x num_beams\n\t            if effective_eos_mask.sum().gt(0):\n\t                eos_batch_idx, eos_beam_ind = effective_eos_mask.nonzero(\n\t                    as_tuple=True)\n", "                # 是由于from_which_beam是 (batch_size, 2*num_beams)的，所以需要2*num_beams\n\t                eos_beam_idx = eos_batch_idx * num_beams * 2 + eos_beam_ind\n\t                eos_beam_idx = from_which_beam.view(-1)[\n\t                    eos_beam_idx]  # 获取真实的从哪个beam获取的eos\n\t            else:\n\t                flag = False\n\t        if flag:\n\t            _token_ids = torch.cat([token_ids, _next_tokens], dim=-1)\n\t            for batch_idx, beam_ind, beam_idx in zip(eos_batch_idx.tolist(),\n\t                                                     eos_beam_ind.tolist(),\n", "                                                     eos_beam_idx.tolist()):\n\t                if not dones[batch_idx]:\n\t                    score = next_scores[batch_idx, beam_ind].item()\n\t                    # 之后需要在结尾新增一个eos\n\t                    if _eos_token_id != -1:\n\t                        hypos[batch_idx].add(\n\t                            _token_ids[batch_idx * num_beams +\n\t                                       beam_idx, :cur_len].clone(), score)\n\t                    else:\n\t                        hypos[batch_idx].add(\n", "                            _token_ids[batch_idx * num_beams +\n\t                                       beam_idx].clone(), score)\n\t        # 更改state状态, 重组token_ids\n\t        reorder_inds = (batch_inds_with_numbeams_interval +\n\t                        _from_which_beam).view(-1)  # flatten成一维\n\t        state.reorder_state(reorder_inds)\n\t        # 重新组织token_ids的状态\n\t        token_ids = torch.cat(\n\t            [token_ids.index_select(index=reorder_inds, dim=0), _next_tokens],\n\t            dim=-1)\n", "        for batch_idx in range(batch_size):\n\t            dones[batch_idx] = dones[batch_idx] or hypos[batch_idx].is_done(next_scores[batch_idx, 0].item()) or \\\n\t                               max_lengths[batch_idx*num_beams]==cur_len+1\n\t        cur_len += 1\n\t        if all(dones):\n\t            break\n\t    # select the best hypotheses\n\t    tgt_len = token_ids.new_zeros(batch_size)\n\t    best = []\n\t    for i, hypotheses in enumerate(hypos):\n", "        best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n\t        # 把上面替换为非eos的词替换回eos\n\t        if _eos_token_id != -1:\n\t            best_hyp = torch.cat(\n\t                [best_hyp, best_hyp.new_ones(1) * _eos_token_id])\n\t        tgt_len[i] = len(best_hyp)\n\t        best.append(best_hyp)\n\t    # generate target batch\n\t    decoded = token_ids.new_zeros(batch_size,\n\t                                  tgt_len.max().item()).fill_(pad_token_id)\n", "    for i, hypo in enumerate(best):\n\t        decoded[i, :tgt_len[i]] = hypo\n\t    return decoded\n\tclass BeamHypotheses(object):\n\t    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n\t        \"\"\"\n\t        Initialize n-best list of hypotheses.\n\t        \"\"\"\n\t        self.max_length = max_length - 1  # ignoring bos_token\n\t        self.length_penalty = length_penalty\n", "        self.early_stopping = early_stopping\n\t        self.num_beams = num_beams\n\t        self.hyp = []\n\t        self.worst_score = 1e9\n\t    def __len__(self):\n\t        \"\"\"\n\t        Number of hypotheses in the list.\n\t        \"\"\"\n\t        return len(self.hyp)\n\t    def add(self, hyp, sum_logprobs):\n", "        \"\"\"\n\t        Add a new hypothesis to the list.\n\t        \"\"\"\n\t        score = sum_logprobs / len(hyp)**self.length_penalty\n\t        if len(self) < self.num_beams or score > self.worst_score:\n\t            self.hyp.append((score, hyp))\n\t            if len(self) > self.num_beams:\n\t                sorted_scores = sorted([\n\t                    (s, idx) for idx, (s, _) in enumerate(self.hyp)\n\t                ])\n", "                del self.hyp[sorted_scores[0][1]]\n\t                self.worst_score = sorted_scores[1][0]\n\t            else:\n\t                self.worst_score = min(score, self.worst_score)\n\t    def is_done(self, best_sum_logprobs):\n\t        \"\"\"\n\t        If there are enough hypotheses and that none of the hypotheses being generated\n\t        can become better than the worst one in the heap, then we are done with this sentence.\n\t        \"\"\"\n\t        if len(self) < self.num_beams:\n", "            return False\n\t        elif self.early_stopping:\n\t            return True\n\t        else:\n\t            return self.worst_score >= best_sum_logprobs / self.max_length**self.length_penalty\n"]}
{"filename": "src/model/utils.py", "chunked_list": ["from transformers import top_k_top_p_filtering\n\timport torch\n\tfrom torch.nn import functional as F\n\tdef set_lr(optimizer, lr):\n\t    for group in optimizer.param_groups:\n\t        group['lr'] = lr\n\tdef get_lr(optimizer):\n\t    for group in optimizer.param_groups:\n\t        return group['lr']\n\tdef clip_gradient(optimizer, grad_clip):\n", "    for group in optimizer.param_groups:\n\t        for param in group['params']:\n\t            # print(param.shape)\n\t            if param.grad == None:\n\t                continue\n\t            param.grad.data.clamp_(-grad_clip, grad_clip)\n\tdef liner_warmup(cur_step, t_step, warmup):\n\t    progress = cur_step / t_step\n\t    if progress < warmup:\n\t        return progress / warmup\n", "    return max((progress - 1.) / (warmup - 1.), 0.)\n\tdef sample_sentence(model,\n\t                    input_ids,\n\t                    image_features,\n\t                    attention_mask,\n\t                    tokenizer,\n\t                    top_k=50,\n\t                    top_p=1.0,\n\t                    max_length=20):\n\t    batch_size = input_ids.shape[0]\n", "    encoder = model.get_encoder()\n\t    encoder_outputs = encoder(input_ids,\n\t                              image_features,\n\t                              attention_mask=attention_mask)\n\t    unfinished_sents = input_ids.new(batch_size).fill_(1)\n\t    sent_lengths = input_ids.new(batch_size).fill_(max_length)\n\t    logprobs = []\n\t    decoder_input_ids = input_ids.new(batch_size,\n\t                                      1).fill_(tokenizer.bos_token_id)\n\t    cur_len = 1\n", "    while cur_len < max_length:\n\t        model_inputs = {\n\t            \"input_ids\": None,\n\t            \"decoder_input_ids\": decoder_input_ids,\n\t            \"image_features\": image_features,\n\t            \"attention_mask\": attention_mask,\n\t            \"encoder_outputs\": encoder_outputs,\n\t            \"use_cache\": False\n\t        }\n\t        outputs = model(**model_inputs)\n", "        next_token_logits = outputs[0][:, -1, :]\n\t        next_token_logits = top_k_top_p_filtering(next_token_logits,\n\t                                                  top_k=top_k,\n\t                                                  top_p=top_p)\n\t        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1),\n\t                                       num_samples=1).squeeze(1)\n\t        _scores = F.log_softmax(next_token_logits, dim=-1)\n\t        _scores = torch.gather(_scores, -1, next_token.unsqueeze(-1))\n\t        logprobs.append(_scores)\n\t        tokens_to_add = next_token * unfinished_sents + (\n", "            tokenizer.pad_token_id) * (1 - unfinished_sents)\n\t        decoder_input_ids = torch.cat(\n\t            [decoder_input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n\t        cur_len = cur_len + 1\n\t        eos_in_sents = tokens_to_add == tokenizer.eos_token_id\n\t        # if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length\n\t        is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(\n\t            eos_in_sents.long()).bool()\n\t        sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos,\n\t                                  cur_len)\n", "        # unfinished_sents is set to zero if eos in sentence\n\t        unfinished_sents.mul_((~eos_in_sents).long())\n\t        if unfinished_sents.max() == 0:\n\t            break\n\t    logprobs = torch.cat(logprobs, dim=1)\n\t    for i in range(batch_size):\n\t        logprobs[i, sent_lengths[i] - 1:] = 0\n\t    sum_logprobs = logprobs.sum(dim=1)\n\t    return decoder_input_ids, sum_logprobs.unsqueeze(1)\n"]}
{"filename": "src/model/MAESC_model_for_generated_aspect_prompt_multitasks.py", "chunked_list": ["from typing import Optional, Tuple\n\tfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\n\tfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\n\tfrom fastNLP.modules.torch import State\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n\t                                     BartDecoder, BartModel,\n\t                                     BartClassificationHead,\n", "                                     _make_linear_from_emb,\n\t                                     _prepare_bart_decoder_inputs)\n\tfrom transformers import BartTokenizer\n\tfrom src.model.config import MultiModalBartConfig\n\t#from src.model.mixins import GenerationMixin, FromPretrainedMixin\n\tfrom src.model.modules_for_prompt_multitasks import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_aspect_prompt, MultiModalBartDecoder_generate_aspect_prompt\n\tfrom src.model.modules_for_prompt_multitasks import MultiModalBartDecoder_aspects_num \n\tclass MultiModalBartModel_AESC(PretrainedBartModel):\n\t    def build_model(self,\n\t                    args,\n", "                    bart_model,\n\t                    tokenizer,\n\t                    label_ids,\n\t                    config,\n\t                    decoder_type=None,\n\t                    copy_gate=False,\n\t                    use_encoder_mlp=False,\n\t                    use_recur_pos=False,\n\t                    tag_first=False):\n\t        if args.bart_init:\n", "            model = BartModel.from_pretrained(bart_model)\n\t            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n\t            print('num_tokens', num_tokens)\n\t            model.resize_token_embeddings(\n\t                len(tokenizer.unique_no_split_tokens) + num_tokens)\n\t            encoder = model.encoder\n\t            decoder = model.decoder\n\t            padding_idx = config.pad_token_id\n\t            encoder.embed_tokens.padding_idx = padding_idx\n\t            # if use_recur_pos:\n", "            #     decoder.set_position_embedding(label_ids[0], tag_first)\n\t            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\t            for token in tokenizer.unique_no_split_tokens:\n\t                if token[:2] == '<<':  # 特殊字符\n\t                    index = tokenizer.convert_tokens_to_ids(\n\t                        tokenizer._base_tokenizer.tokenize(token))\n\t                    if len(index) > 1:\n\t                        raise RuntimeError(f\"{token} wrong split\")\n\t                    else:\n\t                        index = index[0]\n", "                    assert index >= num_tokens, (index, num_tokens, token)\n\t                    indexes = _tokenizer.convert_tokens_to_ids(\n\t                        _tokenizer.tokenize(token[2:-2]))\n\t                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n\t                    for i in indexes[1:]:\n\t                        embed += model.decoder.embed_tokens.weight.data[i]\n\t                    embed /= len(indexes)\n\t                    model.decoder.embed_tokens.weight.data[index] = embed\n\t        else:\n\t            raise RuntimeError(\"error init!!!!!!!\")\n", "        multimodal_encoder_for_generated_aspect_prompt = MultiModalBartEncoder(config, encoder,\n\t                                                   tokenizer.img_feat_id,\n\t                                                   tokenizer.cls_token_id,\n\t                                                   args.num_image_tokens)\n\t        multimodal_encoder = MultiModalBartEncoder_for_Generating_aspect_prompt(\n\t                                                                         use_generated_prompt=args.use_generated_prompt,\n\t                                                                         config=config, \n\t                                                                         encoder = encoder,\n\t                                                                         img_feat_id = tokenizer.img_feat_id,\n\t                                                                         aspect_prompt_token_id=tokenizer.aspect_prompt_token_id,\n", "                                                                         senti_prompt_token_id=tokenizer.senti_prompt_token_id,\n\t                                                                         cls_token_id = tokenizer.cls_token_id,\n\t                                                                         num_image_tokens = args.num_image_tokens,\n\t                                                                         use_different_aspect_prompt=args.use_different_aspect_prompt \n\t                                                   )\n\t        return (multimodal_encoder_for_generated_aspect_prompt, multimodal_encoder, decoder)\n\t    def __init__(self, config: MultiModalBartConfig, args, bart_model,\n\t                 tokenizer, label_ids):\n\t        super().__init__(config)\n\t        self.config = config\n", "        self.tokenizer = tokenizer\n\t        label_ids = sorted(label_ids)\n\t        multimodal_encoder_for_generated_aspect_prompt, multimodal_encoder, share_decoder = self.build_model(\n\t            args, bart_model, self.tokenizer, label_ids, config)\n\t        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n\t        self.causal_mask = causal_mask.triu(diagonal=1)\n\t        self.use_multitasks = args.use_multitasks\n\t        self.loss_lambda = args.loss_lambda\n\t        self.num_image_tokens = args.num_image_tokens\n\t        self.aspect_prompt_encoder = multimodal_encoder_for_generated_aspect_prompt\n", "        self.encoder = multimodal_encoder\n\t        only_sc = False\n\t        # need_tag = True  #if predict the sentiment or not\n\t        if args.task == 'twitter_ae':\n\t            need_tag = False\n\t        else:\n\t            need_tag = True\n\t            # if args.task == 'twitter_sc':\n\t            #     only_sc = True\n\t        self.prompt_decoder = MultiModalBartDecoder_generate_aspect_prompt(self.config, share_decoder)\n", "        if self.use_multitasks:\n\t            self.aspect_num_decoder = MultiModalBartDecoder_aspects_num(self.config, share_decoder)\n\t        self.decoder = MultiModalBartDecoder_span(self.config,\n\t                                                  self.tokenizer,\n\t                                                  share_decoder,\n\t                                                  self.tokenizer.pad_token_id,\n\t                                                  label_ids,\n\t                                                  self.causal_mask,\n\t                                                  num_image_tokens=self.num_image_tokens,\n\t                                                  need_tag=need_tag,\n", "                                                  only_sc=False)\n\t        self.span_loss_fct = Span_loss()\n\t    def prepare_state(self,\n\t                      input_ids,\n\t                      image_features,\n\t                      attention_mask=None,\n\t                      aesc_infos=None,\n\t                      aspects_num=None,\n\t                      first=None):\n\t        ##generate prompt for each instance\n", "        prompt_attention_mask = attention_mask\n\t        if self.num_image_tokens==0:\n\t            end_index = 62\n\t            begin_index = 22\n\t        elif self.num_image_tokens==1:\n\t            end_index = 63\n\t            begin_index = 23\n\t        elif self.num_image_tokens==2:\n\t            end_index = 64\n\t            begin_index = 24\n", "        elif self.num_image_tokens==3:\n\t            end_index = 65\n\t            begin_index = 25\n\t        elif self.num_image_tokens==4:\n\t            end_index = 66\n\t            begin_index = 26\n\t        elif self.num_image_tokens==5:\n\t            end_index = 67\n\t            begin_index = 27\n\t        elif self.num_image_tokens==6:\n", "            end_index = 68\n\t            begin_index = 28\n\t        elif self.num_image_tokens==7:\n\t            end_index = 69\n\t            begin_index = 29\n\t        for i in range(len(prompt_attention_mask)):\n\t            mask = prompt_attention_mask[i]\n\t            mask[begin_index:end_index]=torch.zeros_like(mask[begin_index:end_index]) ##26:66 是aspect提示的位置\n\t            prompt_attention_mask[i]=mask\n\t        ''' aspects_prompt '''\n", "        dict_for_prompt = self.aspect_prompt_encoder(input_ids=input_ids,\n\t                                              image_features=image_features,\n\t                                              attention_mask=prompt_attention_mask,\n\t                                              output_hidden_states=True,\n\t                                              return_dict=True)\n\t        aspect_prompt_decoder_input_ids, aspect_prompt_decoder_attention_mask = [\n\t            aesc_infos['aspect_prompt_decoder_input_ids'].to(input_ids.device),\n\t            aesc_infos['aspect_prompt_decoder_attention_mask'].to(input_ids.device)]\n\t        generated_prompt = self.prompt_decoder(\n\t                                            encoder_outputs=dict_for_prompt.last_hidden_state, \n", "                                            attention_mask=attention_mask,\n\t                                            decoder_input_ids =aspect_prompt_decoder_input_ids, decoder_attention_mask=aspect_prompt_decoder_attention_mask)\n\t        generated_prompt = generated_prompt[:, 1:, :] ##(batch_size, 2, 768)\n\t        '''aspects_num'''\n\t        aspects_num_decoder_input_ids, aspects_num_decoder_attention_mask = [\n\t            aesc_infos['aspects_num_decoder_input_ids'].to(input_ids.device),\n\t            aesc_infos['aspects_num_decoder_attention_mask'].to(input_ids.device)]\n\t        # import ipdb; ipdb.set_trace()\n\t        if self.use_multitasks:\n\t            aspects_num_loss, predict_aspects_num_logits = self.aspect_num_decoder(aspects_num_labels=aspects_num,\n", "                                                                            encoder_outputs=dict_for_prompt[0], \n\t                                                                            attention_mask=attention_mask,\n\t                                                                            aspects_num_decoder_input_ids=aspects_num_decoder_input_ids)\n\t            predict_aspects_num = torch.argmax(predict_aspects_num_logits, dim=1)\n\t            new_predict_aspects_num = predict_aspects_num + torch.ones_like(predict_aspects_num)\n\t        else:\n\t            aspects_num_loss =0\n\t            new_predict_aspects_num = []\n\t            predict_aspects_num = []\n\t            for i in range(len(input_ids)):\n", "                new_predict_aspects_num.append(5)\n\t                predict_aspects_num.append(4)\n\t            new_predict_aspects_num = torch.tensor(new_predict_aspects_num)\n\t            predict_aspects_num = torch.tensor(predict_aspects_num)\n\t        dict = self.encoder(\n\t                            input_ids=input_ids,\n\t                            image_features=image_features,\n\t                            attention_mask=attention_mask,\n\t                            generated_prompt= generated_prompt,\n\t                            aspects_num = new_predict_aspects_num,\n", "                            output_hidden_states=True,\n\t                            return_dict=True)\n\t        encoder_outputs = dict.last_hidden_state\n\t        hidden_states = dict.hidden_states\n\t        encoder_mask = attention_mask\n\t        src_embed_outputs = hidden_states[0]\n\t        state = BartState(\n\t            encoder_outputs,\n\t            encoder_mask,\n\t            input_ids[:,\n", "                      end_index:],  #the text features start from index 38, the front are image features.\n\t            first,\n\t            src_embed_outputs)\n\t        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n\t        return state, aspects_num_loss, predict_aspects_num\n\t    def forward(\n\t            self,\n\t            input_ids,\n\t            image_features,\n\t            attention_mask=None,\n", "            aesc_infos=None,\n\t            aspects_num=None,\n\t            encoder_outputs: Optional[Tuple] = None,\n\t            use_cache=None,\n\t            output_attentions=None,\n\t            output_hidden_states=None,\n\t    ):\n\t        ### for prompt\n\t        # import ipdb; ipdb.set_trace()\n\t        ## for aspect-spans\n", "        aspects_num = torch.tensor(aspects_num).to(input_ids.device)\n\t        state, aspects_num_loss, predict_aspects_num = self.prepare_state( input_ids, image_features, attention_mask, aesc_infos, aspects_num)\n\t        spans, span_mask = [ \n\t            aesc_infos['labels'].to(input_ids.device),\n\t            aesc_infos['masks'].to(input_ids.device)\n\t        ]\n\t        logits = self.decoder(spans, state) ## spans: (2, 13) logits: (2, 12, 40)\n\t        span_loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n\t        all_loss = span_loss + self.loss_lambda*aspects_num_loss\n\t        return all_loss, predict_aspects_num\n", "class BartState(State):\n\t    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n\t                 src_embed_outputs):\n\t        super().__init__(encoder_output, encoder_mask)\n\t        self.past_key_values = None\n\t        self.src_tokens = src_tokens\n\t        self.first = first\n\t        self.src_embed_outputs = src_embed_outputs\n\t    def reorder_state(self, indices: torch.LongTensor):\n\t        super().reorder_state(indices)\n", "        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n\t        if self.first is not None:\n\t            self.first = self._reorder_state(self.first, indices)\n\t        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n\t                                                     indices)\n\t        if self.past_key_values is not None:\n\t            new = []\n\t            for layer in self.past_key_values:\n\t                new_layer = {}\n\t                for key1 in list(layer.keys()):\n", "                    new_layer_ = {}\n\t                    for key2 in list(layer[key1].keys()):\n\t                        if layer[key1][key2] is not None:\n\t                            layer[key1][key2] = self._reorder_state(\n\t                                layer[key1][key2], indices)\n\t                            # print(key1, key2, layer[key1][key2].shape)\n\t                        new_layer_[key2] = layer[key1][key2]\n\t                    new_layer[key1] = new_layer_\n\t                new.append(new_layer)\n\t            self.past_key_values = new\n"]}
{"filename": "src/model/modeling_bart.py", "chunked_list": ["# coding=utf-8\n\t# Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t\"\"\"PyTorch BART model, ported from the fairseq repo.\"\"\"\n\timport math\n\timport random\n\timport warnings\n\tfrom typing import Dict, List, Optional, Tuple\n\timport numpy as np\n", "import torch\n\timport torch.nn.functional as F\n\tfrom torch import Tensor, nn\n\tfrom torch.nn import CrossEntropyLoss\n\tfrom transformers.models.bart.modeling_bart import *\n\tfrom transformers.modeling_outputs import BaseModelOutputWithPast, BaseModelOutput\n\t# from transformer_my.modeling_bart import *\n\t# logger = logging.get_logger(__name__)\n\t_CONFIG_FOR_DOC = \"BartConfig\"\n\t_TOKENIZER_FOR_DOC = \"BartTokenizer\"\n", "BART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n\t    \"facebook/bart-base\",\n\t    \"facebook/bart-large\",\n\t    \"facebook/bart-large-mnli\",\n\t    \"facebook/bart-large-cnn\",\n\t    \"facebook/bart-large-xsum\",\n\t    \"facebook/mbart-large-en-ro\",\n\t]\n\t# This list is incomplete. See all BART models at https://huggingface.co/models?filter=bart\n\tBART_START_DOCSTRING = r\"\"\"\n", "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n\t    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n\t    pruning heads etc.)\n\t    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass.\n\t    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general\n\t    usage and behavior.\n\t    Parameters:\n\t        config (:class:`~transformers.BartConfig`): Model configuration class with all the parameters of the model.\n\t            Initializing with a config file does not load the weights associated with the model, only the configuration.\n\t            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n", "\"\"\"\n\tBART_GENERATION_EXAMPLE = r\"\"\"\n\t    Summarization example::\n\t        >>> from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n\t        >>> # see ``examples/summarization/bart/run_eval.py`` for a longer example\n\t        >>> model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n\t        >>> tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\t        >>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n\t        >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n\t        >>> # Generate Summary\n", "        >>> summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n\t        >>> print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n\t\"\"\"\n\tBART_INPUTS_DOCSTRING = r\"\"\"\n\t    Args:\n\t        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n\t            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n\t            it.\n\t            Indices can be obtained using :class:`~transformers.BartTokenizer`.\n\t            See :meth:`transformers.PreTrainedTokenizer.encode` and\n", "            :meth:`transformers.PreTrainedTokenizer.__call__` for details.\n\t            `What are input IDs? <../glossary.html#input-ids>`__\n\t        attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n\t            Mask to avoid performing attention on padding token indices.\n\t            Mask values selected in ``[0, 1]``:\n\t            - 1 for tokens that are **not masked**,\n\t            - 0 for tokens that are **masked**.\n\t            `What are attention masks? <../glossary.html#attention-mask>`__\n\t        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):\n\t            Provide for translation and summarization training. By default, the model will create this tensor by\n", "            shifting the :obj:`input_ids` to the right, following the paper.\n\t        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`):\n\t            Default behavior: generate a tensor that ignores pad tokens in :obj:`decoder_input_ids`. Causal mask will\n\t            also be used by default.\n\t            If you want to change padding behavior, you should read :func:`modeling_bart._prepare_decoder_inputs` and\n\t            modify to your needs. See diagram 1 in `the paper <https://arxiv.org/abs/1910.13461>`__ for more\n\t            information on the default strategy.\n\t        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`):\n\t            Tuple consists of (:obj:`last_hidden_state`, `optional`: :obj:`hidden_states`, `optional`: :obj:`attentions`)\n\t            :obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) is a\n", "            sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of\n\t            the decoder.\n\t        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n\t            Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.\n\t            If :obj:`past_key_values` are used, the user can optionally input only the last\n\t            ``decoder_input_ids`` (those that don't have their past key value states given to this model) of shape\n\t            :obj:`(batch_size, 1)` instead of all ``decoder_input_ids`` of shape :obj:`(batch_size, sequence_length)`.\n\t        use_cache (:obj:`bool`, `optional`):\n\t            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n\t            decoding (see :obj:`past_key_values`).\n", "        output_attentions (:obj:`bool`, `optional`):\n\t            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n\t            tensors for more detail.\n\t        output_hidden_states (:obj:`bool`, `optional`):\n\t            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n\t            more detail.\n\t        return_dict (:obj:`bool`, `optional`):\n\t            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n\t\"\"\"\n\tdef invert_mask(attention_mask):\n", "    \"\"\"Turns 1->0, 0->1, False->True, True-> False\"\"\"\n\t    assert attention_mask.dim() == 2\n\t    return attention_mask.eq(0)\n\tdef _prepare_bart_decoder_inputs(config,\n\t                                 input_ids,\n\t                                 decoder_input_ids=None,\n\t                                 decoder_padding_mask=None,\n\t                                 causal_mask_dtype=torch.float32):\n\t    \"\"\"Prepare masks that ignore padding tokens in the decoder and a causal mask for the decoder if\n\t    none are provided. This mimics the default behavior in fairseq. To override it pass in masks.\n", "    Note: this is not called during generation\n\t    \"\"\"\n\t    pad_token_id = config.pad_token_id\n\t    if decoder_input_ids is None:\n\t        decoder_input_ids = shift_tokens_right(input_ids, pad_token_id)\n\t    bsz, tgt_len = decoder_input_ids.size()\n\t    if decoder_padding_mask is None:\n\t        decoder_padding_mask = make_padding_mask(decoder_input_ids,\n\t                                                 pad_token_id)\n\t    else:\n", "        decoder_padding_mask = invert_mask(decoder_padding_mask)\n\t    if decoder_padding_mask is not None and decoder_padding_mask.shape[1] > 1:\n\t        # never mask leading token, even if it is pad\n\t        decoder_padding_mask[:, 0] = decoder_padding_mask[:, 1]\n\t    tmp = fill_with_neg_inf(torch.zeros(tgt_len, tgt_len))\n\t    mask = torch.arange(tmp.size(-1))\n\t    tmp.masked_fill_(mask < (mask + 1).view(tmp.size(-1), 1), 0)\n\t    causal_mask = tmp.to(dtype=causal_mask_dtype,\n\t                         device=decoder_input_ids.device)\n\t    return decoder_input_ids, decoder_padding_mask, causal_mask\n", "class PretrainedBartModel(PreTrainedModel):\n\t    config_class = BartConfig\n\t    base_model_prefix = \"model\"\n\t    def _init_weights(self, module):\n\t        std = self.config.init_std\n\t        if isinstance(module, nn.Linear):\n\t            module.weight.data.normal_(mean=0.0, std=std)\n\t            if module.bias is not None:\n\t                module.bias.data.zero_()\n\t        elif isinstance(module, SinusoidalPositionalEmbedding):\n", "            pass\n\t        elif isinstance(module, nn.Embedding):\n\t            module.weight.data.normal_(mean=0.0, std=std)\n\t            if module.padding_idx is not None:\n\t                module.weight.data[module.padding_idx].zero_()\n\t    @property\n\t    def dummy_inputs(self):\n\t        pad_token = self.config.pad_token_id\n\t        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]],\n\t                                 device=self.device)\n", "        dummy_inputs = {\n\t            \"attention_mask\": input_ids.ne(pad_token),\n\t            \"input_ids\": input_ids,\n\t        }\n\t        return dummy_inputs\n\tdef _make_linear_from_emb(emb):\n\t    vocab_size, emb_size = emb.weight.shape\n\t    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n\t    lin_layer.weight.data = emb.weight.data\n\t    return lin_layer\n", "# Helper Functions, mostly for making masks\n\tdef _check_shapes(shape_1, shape2):\n\t    if shape_1 != shape2:\n\t        raise AssertionError(\"shape mismatch: {} != {}\".format(\n\t            shape_1, shape2))\n\tdef shift_tokens_right(input_ids, pad_token_id):\n\t    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n\t    prev_output_tokens = input_ids.clone()\n\t    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n\t    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n", "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n\t    return prev_output_tokens\n\tdef make_padding_mask(input_ids, padding_idx=1):\n\t    \"\"\"True for pad tokens\"\"\"\n\t    padding_mask = input_ids.eq(padding_idx)\n\t    if not padding_mask.any():\n\t        padding_mask = None\n\t    return padding_mask\n\t# Helper Modules\n\tclass EncoderLayer(nn.Module):\n", "    def __init__(self, config: BartConfig):\n\t        super().__init__()\n\t        self.embed_dim = config.d_model\n\t        self.self_attn = Attention(self.embed_dim,\n\t                                   config.encoder_attention_heads,\n\t                                   dropout=config.attention_dropout)\n\t        self.normalize_before = config.normalize_before\n\t        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n\t        self.dropout = config.dropout\n\t        self.activation_fn = ACT2FN[config.activation_function]\n", "        self.activation_dropout = config.activation_dropout\n\t        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n\t        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n\t        self.final_layer_norm = LayerNorm(self.embed_dim)\n\t    def forward(self, x, encoder_padding_mask, output_attentions=False):\n\t        \"\"\"\n\t        Args:\n\t            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n\t            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n\t                `(batch, src_len)` where padding elements are indicated by ``1``.\n", "            for t_tgt, t_src is excluded (or masked out), =0 means it is\n\t            included in attention\n\t        Returns:\n\t            encoded output of shape `(seq_len, batch, embed_dim)`\n\t        \"\"\"\n\t        residual = x\n\t        if self.normalize_before:\n\t            x = self.self_attn_layer_norm(x)\n\t        x, attn_weights = self.self_attn(query=x,\n\t                                         key=x,\n", "                                         key_padding_mask=encoder_padding_mask,\n\t                                         output_attentions=output_attentions)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = residual + x\n\t        if not self.normalize_before:\n\t            x = self.self_attn_layer_norm(x)\n\t        residual = x\n\t        if self.normalize_before:\n\t            x = self.final_layer_norm(x)\n\t        x = self.activation_fn(self.fc1(x))\n", "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n\t        x = self.fc2(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = residual + x\n\t        if not self.normalize_before:\n\t            x = self.final_layer_norm(x)\n\t        if torch.isinf(x).any() or torch.isnan(x).any():\n\t            clamp_value = torch.finfo(x.dtype).max - 1000\n\t            x = torch.clamp(x, min=-clamp_value, max=clamp_value)\n\t        return x, attn_weights\n", "class BartEncoder(nn.Module):\n\t    \"\"\"\n\t    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:`EncoderLayer`.\n\t    Args:\n\t        config: BartConfig\n\t    \"\"\"\n\t    def __init__(self, config: BartConfig, embed_tokens):\n\t        super().__init__()\n\t        self.dropout = config.dropout\n", "        self.layerdrop = config.encoder_layerdrop\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = math.sqrt(\n\t            embed_dim) if config.scale_embedding else 1.0\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = config.max_position_embeddings\n\t        self.embed_tokens = embed_tokens\n\t        if config.static_position_embeddings:\n\t            self.embed_positions = SinusoidalPositionalEmbedding(\n\t                config.max_position_embeddings, embed_dim, self.padding_idx)\n", "        else:\n\t            self.embed_positions = LearnedPositionalEmbedding(\n\t                config.max_position_embeddings,\n\t                embed_dim,\n\t                self.padding_idx,\n\t                config.extra_pos_embeddings,\n\t            )\n\t        self.layers = nn.ModuleList(\n\t            [EncoderLayer(config) for _ in range(config.encoder_layers)])\n\t        self.layernorm_embedding = LayerNorm(\n", "            embed_dim) if config.normalize_embedding else nn.Identity()\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = LayerNorm(\n\t            config.d_model) if config.add_final_layer_norm else None\n\t    def forward(self,\n\t                input_ids,\n\t                attention_mask=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n", "        \"\"\"\n\t        Args:\n\t            input_ids (LongTensor): tokens in the source language of shape\n\t                `(batch, src_len)`\n\t            attention_mask (torch.LongTensor): indicating which indices are padding tokens.\n\t        Returns:\n\t            BaseModelOutput or Tuple comprised of:\n\t                - **x** (Tensor): the last encoder layer's output of\n\t                  shape `(src_len, batch, embed_dim)`\n\t                - **encoder_states** (tuple(torch.FloatTensor)): all intermediate\n", "                  hidden states of shape `(src_len, batch, embed_dim)`.\n\t                  Only populated if *output_hidden_states:* is True.\n\t                - **all_attentions** (tuple(torch.FloatTensor)): Attention weights for each layer.\n\t                During training might not be of length n_layers because of layer dropout.\n\t        \"\"\"\n\t        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n", "        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        # B x T x C -> T x B x C\n\t        x = x.transpose(0, 1)\n\t        encoder_states = [] if output_hidden_states else None\n\t        all_attentions = () if output_attentions else None\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n", "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n\t                                  self.layerdrop):  # skip the layer\n\t                attn = None\n\t            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n", "                all_attentions = all_attentions + (attn, )\n\t        if self.layer_norm:\n\t            x = self.layer_norm(x)\n\t        if output_hidden_states:\n\t            encoder_states.append(x)\n\t            # T x B x C -> B x T x C\n\t            encoder_states = tuple(\n\t                hidden_state.transpose(0, 1)\n\t                for hidden_state in encoder_states)\n\t        # T x B x C -> B x T x C\n", "        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n\t        return BaseModelOutput(last_hidden_state=x,\n\t                               hidden_states=encoder_states,\n\t                               attentions=all_attentions)\n\tclass DecoderLayer(nn.Module):\n\t    def __init__(self, config: BartConfig):\n\t        super().__init__()\n", "        self.embed_dim = config.d_model\n\t        self.self_attn = Attention(\n\t            embed_dim=self.embed_dim,\n\t            num_heads=config.decoder_attention_heads,\n\t            dropout=config.attention_dropout,\n\t        )\n\t        self.dropout = config.dropout\n\t        self.activation_fn = ACT2FN[config.activation_function]\n\t        self.activation_dropout = config.activation_dropout\n\t        self.normalize_before = config.normalize_before\n", "        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n\t        self.encoder_attn = Attention(\n\t            self.embed_dim,\n\t            config.decoder_attention_heads,\n\t            dropout=config.attention_dropout,\n\t            encoder_decoder_attention=True,\n\t        )\n\t        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n\t        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n\t        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n", "        self.final_layer_norm = LayerNorm(self.embed_dim)\n\t    def forward(\n\t            self,\n\t            x,\n\t            encoder_hidden_states,\n\t            encoder_attn_mask=None,\n\t            layer_state=None,\n\t            causal_mask=None,\n\t            decoder_padding_mask=None,\n\t            output_attentions=False,\n", "    ):\n\t        residual = x\n\t        if layer_state is None:\n\t            layer_state = {}\n\t        if self.normalize_before:\n\t            x = self.self_attn_layer_norm(x)\n\t        # Self Attention\n\t        x, self_attn_weights = self.self_attn(\n\t            query=x,\n\t            key=x,\n", "            layer_state=layer_state,  # adds keys to layer state\n\t            key_padding_mask=decoder_padding_mask,\n\t            attn_mask=causal_mask,\n\t            output_attentions=output_attentions,\n\t        )\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = residual + x\n\t        if not self.normalize_before:\n\t            x = self.self_attn_layer_norm(x)\n\t        # Cross attention\n", "        residual = x\n\t        assert self.encoder_attn.cache_key != self.self_attn.cache_key\n\t        if self.normalize_before:\n\t            x = self.encoder_attn_layer_norm(x)\n\t        x, _ = self.encoder_attn(\n\t            query=x,\n\t            key=encoder_hidden_states,\n\t            key_padding_mask=encoder_attn_mask,\n\t            layer_state=layer_state,  # mutates layer state\n\t        )\n", "        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = residual + x\n\t        if not self.normalize_before:\n\t            x = self.encoder_attn_layer_norm(x)\n\t        # Fully Connected\n\t        residual = x\n\t        if self.normalize_before:\n\t            x = self.final_layer_norm(x)\n\t        x = self.activation_fn(self.fc1(x))\n\t        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n", "        x = self.fc2(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        x = residual + x\n\t        if not self.normalize_before:\n\t            x = self.final_layer_norm(x)\n\t        return (\n\t            x,\n\t            self_attn_weights,\n\t            layer_state,\n\t        )  # just self_attn weights for now, following t5, layer_state = cache for decoding\n", "class BartDecoder(nn.Module):\n\t    \"\"\"\n\t    Transformer decoder consisting of *config.decoder_layers* layers. Each layer\n\t    is a :class:`DecoderLayer`.\n\t    Args:\n\t        config: BartConfig\n\t        embed_tokens (torch.nn.Embedding): output embedding\n\t    \"\"\"\n\t    def __init__(self, config: BartConfig, embed_tokens: nn.Embedding):\n\t        super().__init__()\n", "        self.dropout = config.dropout\n\t        self.layerdrop = config.decoder_layerdrop\n\t        self.do_blenderbot_90_layernorm = config.do_blenderbot_90_layernorm  # layernorm variant\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_target_positions = config.max_position_embeddings\n\t        self.embed_scale = math.sqrt(\n\t            config.d_model) if config.scale_embedding else 1.0\n\t        self.embed_tokens = embed_tokens\n\t        if config.static_position_embeddings:\n\t            self.embed_positions = SinusoidalPositionalEmbedding(\n", "                config.max_position_embeddings, config.d_model,\n\t                config.pad_token_id)\n\t        else:\n\t            self.embed_positions = LearnedPositionalEmbedding(\n\t                config.max_position_embeddings, config.d_model,\n\t                self.padding_idx, config.extra_pos_embeddings)\n\t        self.layers = nn.ModuleList([\n\t            DecoderLayer(config) for _ in range(config.decoder_layers)\n\t        ])  # type: List[DecoderLayer]\n\t        self.layernorm_embedding = LayerNorm(\n", "            config.d_model) if config.normalize_embedding else nn.Identity()\n\t        self.layer_norm = LayerNorm(\n\t            config.d_model) if config.add_final_layer_norm else None\n\t        self.config = config\n\t    def set_position_embedding(self, special_tag_start_id, tag_first=True):\n\t        if tag_first:\n\t            embed_positions = DecoderLearnedPositionalEmbedding(\n\t                self.config.max_position_embeddings, self.config.d_model,\n\t                self.padding_idx, self.config.extra_pos_embeddings,\n\t                special_tag_start_id)\n", "        else:\n\t            embed_positions = DecoderLearnedPositionalEmbedding2(\n\t                self.config.max_position_embeddings, self.config.d_model,\n\t                self.padding_idx, self.config.extra_pos_embeddings,\n\t                special_tag_start_id)\n\t        embed_positions.weight.data = self.embed_positions.weight.data\n\t        self.embed_positions = embed_positions\n\t    def forward(\n\t            self,\n\t            input_ids,\n", "            encoder_hidden_states,\n\t            encoder_padding_mask,\n\t            decoder_padding_mask,\n\t            decoder_causal_mask,\n\t            past_key_values=None,\n\t            use_cache=False,\n\t            output_attentions=False,\n\t            output_hidden_states=False,\n\t            return_dict=False,\n\t            use_pos_cache=False,\n", "            **unused,\n\t    ):\n\t        \"\"\"\n\t        Includes several features from \"Jointly Learning to Align and\n\t        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n\t        Args:\n\t            input_ids (LongTensor): previous decoder outputs of shape\n\t                `(batch, tgt_len)`, for teacher forcing\n\t            encoder_hidden_states: output from the encoder, used for\n\t                encoder-side attention\n", "            encoder_padding_mask: for ignoring pad tokens\n\t            past_key_values (dict or None): dictionary used for storing state during generation\n\t        Returns:\n\t            BaseModelOutputWithPast or tuple:\n\t                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n\t                - the cache\n\t                - hidden states\n\t                - attentions\n\t        \"\"\"\n\t        if \"decoder_cached_states\" in unused:\n", "            warnings.warn(\n\t                \"The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n\t                FutureWarning,\n\t            )\n\t            past_key_values = unused.pop(\"decoder_cached_states\")\n\t        if \"decoder_past_key_values\" in unused:\n\t            warnings.warn(\n\t                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n\t                FutureWarning,\n\t            )\n", "            past_key_values = unused.pop(\"decoder_past_key_values\")\n\t        # check attention mask and invert\n\t        if encoder_padding_mask is not None:\n\t            encoder_padding_mask = invert_mask(encoder_padding_mask)\n\t        # embed positions\n\t        positions = self.embed_positions(input_ids, use_cache=use_pos_cache)\n\t        if use_pos_cache:\n\t            input_ids = input_ids[:, -1:]\n\t            positions = positions[:, -1:]\n\t        x = self.embed_tokens(input_ids) * self.embed_scale\n", "        if self.do_blenderbot_90_layernorm:\n\t            x = self.layernorm_embedding(x)\n\t            x += positions\n\t        else:\n\t            x += positions\n\t            x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n\t        # Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n\t        x = x.transpose(0, 1)\n\t        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n", "        # decoder layers\n\t        all_hidden_states = () if output_hidden_states else None\n\t        all_self_attns = () if output_attentions else None\n\t        next_decoder_cache = []\n\t        for idx, decoder_layer in enumerate(self.layers):\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            if output_hidden_states:\n\t                all_hidden_states += (x, )\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability < self.layerdrop):\n", "                continue\n\t            layer_state = past_key_values[\n\t                idx] if past_key_values is not None else None\n\t            x, layer_self_attn, layer_past = decoder_layer(\n\t                x,\n\t                encoder_hidden_states,\n\t                encoder_attn_mask=encoder_padding_mask,\n\t                decoder_padding_mask=decoder_padding_mask,\n\t                layer_state=layer_state,\n\t                causal_mask=decoder_causal_mask,\n", "                output_attentions=output_attentions,\n\t            )\n\t            if use_cache:\n\t                next_decoder_cache.append(layer_past.copy())\n\t            if output_attentions:\n\t                all_self_attns += (layer_self_attn, )\n\t        if self.layer_norm:  # if config.add_final_layer_norm (mBART)\n\t            x = self.layer_norm(x)\n\t        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n\t        if output_hidden_states:\n", "            all_hidden_states = tuple(\n\t                hidden_state.transpose(0, 1)\n\t                for hidden_state in all_hidden_states)\n\t        x = x.transpose(0, 1)\n\t        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n\t        next_cache = next_decoder_cache if use_cache else None\n\t        if not return_dict:\n\t            return tuple(\n\t                v for v in [x, next_cache, all_hidden_states, all_self_attns]\n\t                if v is not None)\n", "        return BaseModelOutputWithPast(last_hidden_state=x,\n\t                                       past_key_values=next_cache,\n\t                                       hidden_states=all_hidden_states,\n\t                                       attentions=all_self_attns)\n\tdef _reorder_buffer(attn_cache, new_order):\n\t    for k, input_buffer_k in attn_cache.items():\n\t        if input_buffer_k is not None:\n\t            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n\t    return attn_cache\n\tclass Attention(nn.Module):\n", "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\t    def __init__(\n\t            self,\n\t            embed_dim,\n\t            num_heads,\n\t            dropout=0.0,\n\t            bias=True,\n\t            encoder_decoder_attention=False,  # otherwise self_attention\n\t    ):\n\t        super().__init__()\n", "        self.embed_dim = embed_dim\n\t        self.num_heads = num_heads\n\t        self.dropout = dropout\n\t        self.head_dim = embed_dim // num_heads\n\t        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n\t        self.scaling = self.head_dim**-0.5\n\t        self.encoder_decoder_attention = encoder_decoder_attention\n\t        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\t        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\t        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n", "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\t        self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n\t    def _shape(self, tensor, seq_len, bsz):\n\t        return tensor.contiguous().view(seq_len, bsz * self.num_heads,\n\t                                        self.head_dim).transpose(0, 1)\n\t    def forward(\n\t            self,\n\t            query,\n\t            key: Optional[Tensor],\n\t            key_padding_mask: Optional[Tensor] = None,\n", "            layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n\t            attn_mask: Optional[Tensor] = None,\n\t            output_attentions=False,\n\t    ) -> Tuple[Tensor, Optional[Tensor]]:\n\t        \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n\t        static_kv: bool = self.encoder_decoder_attention\n\t        tgt_len, bsz, embed_dim = query.size()\n\t        assert embed_dim == self.embed_dim\n\t        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n\t        # get here for encoder decoder cause of static_kv\n", "        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n\t            saved_state = layer_state.get(self.cache_key, {})\n\t            if \"prev_key\" in saved_state and static_kv:\n\t                # previous time steps are cached - no need to recompute key and value if they are static\n\t                key = None\n\t        else:\n\t            saved_state = None\n\t            layer_state = {}\n\t        q = self.q_proj(query) * self.scaling\n\t        if static_kv:\n", "            if key is None:\n\t                k = v = None\n\t            else:\n\t                k = self.k_proj(key)\n\t                v = self.v_proj(key)\n\t        else:\n\t            k = self.k_proj(query)\n\t            v = self.v_proj(query)\n\t        q = self._shape(q, tgt_len, bsz)\n\t        if k is not None:\n", "            k = self._shape(k, -1, bsz)\n\t        if v is not None:\n\t            v = self._shape(v, -1, bsz)\n\t        if saved_state is not None:\n\t            k, v, key_padding_mask = self._use_saved_state(\n\t                k, v, saved_state, key_padding_mask, static_kv, bsz)\n\t        # Update cache\n\t        layer_state[self.cache_key] = {\n\t            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n\t            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n", "            \"prev_key_padding_mask\":\n\t            key_padding_mask if not static_kv else None,\n\t        }\n\t        assert k is not None\n\t        src_len = k.size(1)\n\t        attn_weights = torch.bmm(q, k.transpose(1, 2))\n\t        assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n\t        if attn_mask is not None:\n\t            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n\t                                             src_len) + attn_mask\n", "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len,\n\t                                             src_len)\n\t        # This is part of a workaround to get around fork/join parallelism not supporting Optional types.\n\t        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n\t            key_padding_mask = None\n\t        assert key_padding_mask is None or key_padding_mask.size()[:2] == (\n\t            bsz,\n\t            src_len,\n\t        )\n\t        if key_padding_mask is not None:  # don't attend to padding symbols\n", "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n\t                                             src_len)\n\t            reshaped = key_padding_mask.unsqueeze(1).unsqueeze(2)\n\t            attn_weights = attn_weights.masked_fill(reshaped, float(\"-inf\"))\n\t            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len,\n\t                                             src_len)\n\t        attn_weights = F.softmax(attn_weights, dim=-1)\n\t        attn_probs = F.dropout(\n\t            attn_weights,\n\t            p=self.dropout,\n", "            training=self.training,\n\t        )\n\t        assert v is not None\n\t        attn_output = torch.bmm(attn_probs, v)\n\t        assert attn_output.size() == (bsz * self.num_heads, tgt_len,\n\t                                      self.head_dim)\n\t        attn_output = attn_output.transpose(0, 1).contiguous().view(\n\t            tgt_len, bsz, embed_dim)\n\t        attn_output = self.out_proj(attn_output)\n\t        if output_attentions:\n", "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n\t                                             src_len)\n\t        else:\n\t            attn_weights = None\n\t        return attn_output, attn_weights\n\t    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv,\n\t                         bsz):\n\t        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n\t        if \"prev_key\" in saved_state:\n\t            _prev_key = saved_state[\"prev_key\"]\n", "            assert _prev_key is not None\n\t            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n\t            if static_kv:\n\t                k = prev_key\n\t            else:\n\t                assert k is not None\n\t                k = torch.cat([prev_key, k], dim=1)\n\t        if \"prev_value\" in saved_state:\n\t            _prev_value = saved_state[\"prev_value\"]\n\t            assert _prev_value is not None\n", "            prev_value = _prev_value.view(bsz * self.num_heads, -1,\n\t                                          self.head_dim)\n\t            if static_kv:\n\t                v = prev_value\n\t            else:\n\t                assert v is not None\n\t                v = torch.cat([prev_value, v], dim=1)\n\t        assert k is not None and v is not None\n\t        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\n\t            \"prev_key_padding_mask\", None)\n", "        if prev_key_padding_mask is not None:\n\t            if static_kv:\n\t                new_key_padding_mask = prev_key_padding_mask\n\t            else:\n\t                new_key_padding_mask = torch.cat(\n\t                    [prev_key_padding_mask, key_padding_mask], dim=1)\n\t        else:\n\t            new_key_padding_mask = key_padding_mask\n\t        return k, v, new_key_padding_mask\n\tclass BartClassificationHead(nn.Module):\n", "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n\t    # This can trivially be shared with RobertaClassificationHead\n\t    def __init__(\n\t            self,\n\t            input_dim,\n\t            inner_dim,\n\t            num_classes,\n\t            pooler_dropout,\n\t    ):\n\t        super().__init__()\n", "        self.dense = nn.Linear(input_dim, inner_dim)\n\t        self.dropout = nn.Dropout(p=pooler_dropout)\n\t        self.out_proj = nn.Linear(inner_dim, num_classes)\n\t    def forward(self, x):\n\t        x = self.dropout(x)\n\t        x = self.dense(x)\n\t        x = torch.tanh(x)\n\t        x = self.dropout(x)\n\t        x = self.out_proj(x)\n\t        return x\n", "class LearnedPositionalEmbedding(nn.Embedding):\n\t    \"\"\"\n\t    This module learns positional embeddings up to a fixed maximum size.\n\t    Padding ids are ignored by either offsetting based on padding_idx\n\t    or by setting padding_idx to None and ensuring that the appropriate\n\t    position ids are passed to the forward function.\n\t    \"\"\"\n\t    def __init__(self, num_embeddings: int, embedding_dim: int,\n\t                 padding_idx: int, offset):\n\t        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n", "        # and adjust num_embeddings appropriately. Other models dont have this hack\n\t        self.offset = offset\n\t        assert padding_idx is not None\n\t        num_embeddings += offset\n\t        super().__init__(num_embeddings,\n\t                         embedding_dim,\n\t                         padding_idx=padding_idx)\n\t    def forward(self, input_ids, use_cache=False):\n\t        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n\t        bsz, seq_len = input_ids.shape[:2]\n", "        if use_cache:\n\t            positions = input_ids.data.new(1, 1).fill_(\n\t                seq_len - 1)  # called before slicing\n\t        else:\n\t            # starts at 0, ends at 1-seq_len\n\t            positions = torch.arange(seq_len,\n\t                                     dtype=torch.long,\n\t                                     device=self.weight.device)\n\t        return super().forward(positions + self.offset)\n\tclass DecoderLearnedPositionalEmbedding(nn.Embedding):\n", "    \"\"\"\n\t    主要修改是，position的是循环的\n\t    This module learns positional embeddings up to a fixed maximum size.\n\t    Padding ids are ignored by either offsetting based on padding_idx\n\t    or by setting padding_idx to None and ensuring that the appropriate\n\t    position ids are passed to the forward function.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 num_embeddings: int,\n\t                 embedding_dim: int,\n", "                 padding_idx: int,\n\t                 offset,\n\t                 special_tag_start_id=None):\n\t        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n\t        # and adjust num_embeddings appropriately. Other models dont have this hack\n\t        self.offset = offset\n\t        assert padding_idx is not None\n\t        num_embeddings += offset\n\t        self.special_tag_start_id = special_tag_start_id  # 这个id之后的词是特殊词汇\n\t        super().__init__(num_embeddings,\n", "                         embedding_dim,\n\t                         padding_idx=padding_idx)\n\t    def forward(self, input_ids, use_cache=False):\n\t        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n\t        if self.special_tag_start_id is None or input_ids.size(1) < 2:\n\t            bsz, seq_len = input_ids.shape[:2]\n\t            if use_cache:\n\t                positions = input_ids.data.new(1, 1).fill_(\n\t                    seq_len - 1)  # called before slicing\n\t            else:\n", "                # starts at 0, ends at 1-seq_len\n\t                positions = torch.arange(seq_len,\n\t                                         dtype=torch.long,\n\t                                         device=self.weight.device)\n\t        else:\n\t            # 实现的是每个位置重新开始position\n\t            \"\"\"\n\t                大概意思是，假设input_ids中假设大于4是特殊符号，那么输入是\n\t                [[2, 4, 1, 2, 3, 5, 1],\n\t                 [2, 5, 3, 3, 0, 0, 0]]时，输出为\n", "                [[0, 1, 2, 3, 4, 1, 2],\n\t                 [0, 1, 2, 3, 4, 5, 6] 每个大于4的位置都会重置\n\t            \"\"\"\n\t            _input_ids = input_ids[:, 1:]\n\t            bsz, seq_len = _input_ids.shape[:2]\n\t            special_tag_mask = _input_ids.ge(\n\t                self.special_tag_start_id)  # bsz x max_len\n\t            if special_tag_mask.sum() > 0:\n\t                num_masks = special_tag_mask.cumsum(dim=1).max()  # 表示最长的\n\t                arange_indices = torch.arange(seq_len).to(\n", "                    _input_ids).expand_as(_input_ids)  # bsz x max_len\n\t                special_tag_indice = arange_indices.masked_select(\n\t                    special_tag_mask)  # a vector只包含所有的special的indice\n\t                indices = torch.arange(num_masks).to(_input_ids)[None].repeat(\n\t                    bsz, 1)  # bsz x mask_len\n\t                mask = indices.lt(special_tag_mask.sum(dim=1, keepdim=True))\n\t                indices = indices.masked_scatter(mask, special_tag_indice)\n\t                _, inverted_indices = special_tag_mask.cumsum(dim=-1).unique(\n\t                    return_inverse=True)\n\t                inverted_indices = inverted_indices - inverted_indices[:, :1]\n", "                inverted_indices = inverted_indices.masked_fill(\n\t                    inverted_indices.ge(indices.size(1)),\n\t                    max(indices.size(1) - 1, 0))\n\t                positions = indices.gather(index=inverted_indices, dim=1)\n\t                positions = (arange_indices - positions) + 1\n\t            else:\n\t                positions = torch.arange(seq_len + 1,\n\t                                         dtype=torch.long,\n\t                                         device=self.weight.device)[None]\n\t            if use_cache:\n", "                positions = positions[:, -1:]\n\t            else:\n\t                positions = torch.cat([input_ids.new_zeros(bsz, 1), positions],\n\t                                      dim=1)\n\t        return super().forward(positions + self.offset)\n\tclass DecoderLearnedPositionalEmbedding2(nn.Embedding):\n\t    \"\"\"\n\t    主要修改是，position的是循环的, 和上面的区别是tag所在的位置不同\n\t    This module learns positional embeddings up to a fixed maximum size.\n\t    Padding ids are ignored by either offsetting based on padding_idx\n", "    or by setting padding_idx to None and ensuring that the appropriate\n\t    position ids are passed to the forward function.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 num_embeddings: int,\n\t                 embedding_dim: int,\n\t                 padding_idx: int,\n\t                 offset,\n\t                 special_tag_start_id=None):\n\t        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n", "        # and adjust num_embeddings appropriately. Other models dont have this hack\n\t        self.offset = offset\n\t        assert padding_idx is not None\n\t        num_embeddings += offset\n\t        self.special_tag_start_id = special_tag_start_id  # 这个id之后的词是特殊词汇\n\t        super().__init__(num_embeddings,\n\t                         embedding_dim,\n\t                         padding_idx=padding_idx)\n\t    def forward(self, input_ids, use_cache=False):\n\t        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n", "        if self.special_tag_start_id is None or input_ids.size(1) < 2:\n\t            bsz, seq_len = input_ids.shape[:2]\n\t            if use_cache:\n\t                positions = input_ids.data.new(1, 1).fill_(\n\t                    seq_len - 1)  # called before slicing\n\t            else:\n\t                # starts at 0, ends at 1-seq_len\n\t                positions = torch.arange(seq_len,\n\t                                         dtype=torch.long,\n\t                                         device=self.weight.device)\n", "        else:\n\t            # 实现的是每个位置重新开始position\n\t            \"\"\"\n\t                大概意思是，假设input_ids中假设大于4是特殊符号，那么输入是\n\t                [[2, 1, 2, 3, 4, 1, 5],\n\t                 [2, 3, 3, 5, 0, 0, 0]]时，输出为\n\t                [[0, 1, 2, 3, 4, 1, 2],\n\t                 [0, 1, 2, 3, 4, 5, 6] 每个大于4的位置都会重置\n\t            \"\"\"\n\t            _input_ids = input_ids[:, 1:]  # 把sos去掉\n", "            bsz, seq_len = _input_ids.shape[:2]\n\t            special_tag_mask = _input_ids.ge(\n\t                self.special_tag_start_id)  # bsz x max_len\n\t            if special_tag_mask.sum() > 0:\n\t                num_masks = special_tag_mask.cumsum(dim=1)  # 表示最长的\n\t                num_masks_value = num_masks.max()\n\t                arange_indices = torch.arange(seq_len).to(\n\t                    _input_ids).expand_as(_input_ids)  # bsz x max_len\n\t                special_tag_indice = arange_indices.masked_select(\n\t                    special_tag_mask)  # a vector只包含所有的special的indice\n", "                indices = torch.arange(num_masks_value).to(\n\t                    _input_ids)[None].repeat(bsz, 1)  # bsz x mask_len\n\t                mask = indices.lt(special_tag_mask.sum(dim=-1, keepdim=True))\n\t                special_tag_indice = indices.masked_scatter(\n\t                    mask, special_tag_indice)\n\t                indices = torch.cat([\n\t                    special_tag_indice.new_zeros(bsz, 1),\n\t                    special_tag_indice[:, :-1] + 1\n\t                ],\n\t                                    dim=1)\n", "                _, inverted_indices = special_tag_mask.flip(dims=[1]).cumsum(\n\t                    dim=-1).flip(dims=[1]).unique(return_inverse=True)\n\t                values = inverted_indices[:, 0]  # bsz\n\t                inverted_indices = values[:, None] - inverted_indices\n\t                inverted_indices = inverted_indices.masked_fill(\n\t                    inverted_indices.ge(indices.size(1)),\n\t                    indices.size(1) - 1)\n\t                positions = indices.gather(index=inverted_indices, dim=1)\n\t                positions = arange_indices - positions + 1\n\t            else:\n", "                positions = torch.arange(seq_len + 1,\n\t                                         dtype=torch.long,\n\t                                         device=self.weight.device)[None]\n\t        if use_cache:\n\t            positions = positions[:, -1:]\n\t        else:\n\t            positions = torch.cat([input_ids.new_zeros(bsz, 1), positions],\n\t                                  dim=1)\n\t        return super().forward(positions + self.offset)\n\tdef LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True):\n", "    if torch.cuda.is_available():\n\t        try:\n\t            from apex.normalization import FusedLayerNorm\n\t            return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n\t        except ImportError:\n\t            pass\n\t    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n\tdef fill_with_neg_inf(t):\n\t    \"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"\n\t    return t.float().fill_(float(\"-inf\")).type_as(t)\n", "# Public API\n\tdef _get_shape(t):\n\t    return getattr(t, \"shape\", None)\n\t@add_start_docstrings(\n\t    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n\t    BART_START_DOCSTRING,\n\t)\n\tclass BartModel(PretrainedBartModel):\n\t    def __init__(self, config: BartConfig):\n\t        super().__init__(config)\n", "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n\t        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n\t        self.encoder = BartEncoder(config, self.shared)\n\t        self.decoder = BartDecoder(config, self.shared)\n\t        self.init_weights()\n\t    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n\t    @add_code_sample_docstrings(\n\t        # tokenizer_class=_TOKENIZER_FOR_DOC,\n\t        checkpoint=\"facebook/bart-large\",\n\t        output_type=Seq2SeqModelOutput,\n", "        config_class=_CONFIG_FOR_DOC,\n\t    )\n\t    def forward(\n\t            self,\n\t            input_ids,\n\t            attention_mask=None,\n\t            decoder_input_ids=None,\n\t            decoder_attention_mask=None,\n\t            encoder_outputs: Optional[Tuple] = None,\n\t            past_key_values=None,\n", "            use_cache=None,\n\t            output_attentions=None,\n\t            output_hidden_states=None,\n\t            return_dict=None,\n\t            **kwargs,\n\t    ):\n\t        if \"decoder_past_key_values\" in kwargs:\n\t            warnings.warn(\n\t                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n\t                FutureWarning,\n", "            )\n\t            past_key_values = kwargs.pop(\"decoder_past_key_values\")\n\t        if decoder_input_ids is None:\n\t            use_cache = False\n\t        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n\t        output_hidden_states = (output_hidden_states\n\t                                if output_hidden_states is not None else\n\t                                self.config.output_hidden_states)\n\t        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\t        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n", "        # make masks if user doesn't supply\n\t        if not use_cache:\n\t            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n\t                self.config,\n\t                input_ids,\n\t                decoder_input_ids=decoder_input_ids,\n\t                decoder_padding_mask=decoder_attention_mask,\n\t                causal_mask_dtype=self.shared.weight.dtype,\n\t            )\n\t        else:\n", "            decoder_padding_mask, causal_mask = None, None\n\t        assert decoder_input_ids is not None\n\t        if encoder_outputs is None:\n\t            encoder_outputs = self.encoder(\n\t                input_ids=input_ids,\n\t                attention_mask=attention_mask,\n\t                output_attentions=output_attentions,\n\t                output_hidden_states=output_hidden_states,\n\t                return_dict=return_dict,\n\t            )\n", "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOuput when return_dict=False\n\t        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n\t            encoder_outputs = BaseModelOutput(\n\t                last_hidden_state=encoder_outputs[0],\n\t                hidden_states=encoder_outputs[1]\n\t                if len(encoder_outputs) > 1 else None,\n\t                attentions=encoder_outputs[2]\n\t                if len(encoder_outputs) > 2 else None,\n\t            )\n\t        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n", "        decoder_outputs = self.decoder(\n\t            decoder_input_ids,\n\t            encoder_outputs[0],\n\t            attention_mask,\n\t            decoder_padding_mask,\n\t            decoder_causal_mask=causal_mask,\n\t            past_key_values=past_key_values,\n\t            use_cache=use_cache,\n\t            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n", "            return_dict=return_dict,\n\t        )\n\t        if not return_dict:\n\t            return decoder_outputs + encoder_outputs\n\t        return Seq2SeqModelOutput(\n\t            last_hidden_state=decoder_outputs.last_hidden_state,\n\t            past_key_values=decoder_outputs.past_key_values,\n\t            decoder_hidden_states=decoder_outputs.hidden_states,\n\t            decoder_attentions=decoder_outputs.attentions,\n\t            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n", "            encoder_hidden_states=encoder_outputs.hidden_states,\n\t            encoder_attentions=encoder_outputs.attentions,\n\t        )\n\t    def get_input_embeddings(self):\n\t        return self.shared\n\t    def set_input_embeddings(self, value):\n\t        self.shared = value\n\t        self.encoder.embed_tokens = self.shared\n\t        self.decoder.embed_tokens = self.shared\n\t    def get_output_embeddings(self):\n", "        return _make_linear_from_emb(self.shared)  # make it on the fly\n\t@add_start_docstrings(\n\t    \"The BART Model with a language modeling head. Can be used for summarization.\",\n\t    BART_START_DOCSTRING)\n\tclass BartForConditionalGeneration(PretrainedBartModel):\n\t    base_model_prefix = \"model\"\n\t    authorized_missing_keys = [\n\t        r\"final_logits_bias\", r\"encoder\\.version\", r\"decoder\\.version\"\n\t    ]\n\t    def __init__(self, config: BartConfig):\n", "        super().__init__(config)\n\t        base_model = BartModel(config)\n\t        self.model = base_model\n\t        self.register_buffer(\n\t            \"final_logits_bias\",\n\t            torch.zeros((1, self.model.shared.num_embeddings)))\n\t    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n\t        old_num_tokens = self.model.shared.num_embeddings\n\t        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n\t        self.model.shared = new_embeddings\n", "        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n\t        return new_embeddings\n\t    def _resize_final_logits_bias(self, new_num_tokens: int,\n\t                                  old_num_tokens: int) -> None:\n\t        if new_num_tokens <= old_num_tokens:\n\t            new_bias = self.final_logits_bias[:, :new_num_tokens]\n\t        else:\n\t            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens),\n\t                                     device=self.final_logits_bias.device)\n\t            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n", "        self.register_buffer(\"final_logits_bias\", new_bias)\n\t    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n\t    @replace_return_docstrings(output_type=Seq2SeqLMOutput,\n\t                               config_class=_CONFIG_FOR_DOC)\n\t    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n\t    def forward(\n\t            self,\n\t            input_ids,\n\t            attention_mask=None,\n\t            decoder_input_ids=None,\n", "            decoder_attention_mask=None,\n\t            encoder_outputs=None,\n\t            past_key_values=None,\n\t            labels=None,\n\t            use_cache=None,\n\t            output_attentions=None,\n\t            output_hidden_states=None,\n\t            return_dict=None,\n\t            **unused,\n\t    ):\n", "        r\"\"\"\n\t        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n\t            Labels for computing the masked language modeling loss.\n\t            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).\n\t            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens\n\t            with labels in ``[0, ..., config.vocab_size]``.\n\t        Returns:\n\t        Conditional generation example::\n\t            >>> # Mask filling only works for bart-large\n\t            >>> from transformers import BartTokenizer, BartForConditionalGeneration\n", "            >>> tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n\t            >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n\t            >>> model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n\t            >>> input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']\n\t            >>> logits = model(input_ids).logits\n\t            >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n\t            >>> probs = logits[0, masked_index].softmax(dim=0)\n\t            >>> values, predictions = probs.topk(5)\n\t            >>> tokenizer.decode(predictions).split()\n\t            >>> # ['good', 'great', 'all', 'really', 'very']\n", "        \"\"\"\n\t        if \"lm_labels\" in unused:\n\t            warnings.warn(\n\t                \"The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n\t                FutureWarning,\n\t            )\n\t            labels = unused.pop(\"lm_labels\")\n\t        if \"decoder_cached_states\" in unused:\n\t            warnings.warn(\n\t                \"The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n", "                FutureWarning,\n\t            )\n\t            past_key_values = unused.pop(\"decoder_cached_states\")\n\t        if \"decoder_past_key_values\" in unused:\n\t            warnings.warn(\n\t                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n\t                FutureWarning,\n\t            )\n\t            past_key_values = unused.pop(\"decoder_past_key_values\")\n\t        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n", "        if labels is not None:\n\t            use_cache = False\n\t            if decoder_input_ids is None:\n\t                decoder_input_ids = shift_tokens_right(\n\t                    labels, self.config.pad_token_id)\n\t        outputs = self.model(\n\t            input_ids,\n\t            attention_mask=attention_mask,\n\t            decoder_input_ids=decoder_input_ids,\n\t            encoder_outputs=encoder_outputs,\n", "            decoder_attention_mask=decoder_attention_mask,\n\t            past_key_values=past_key_values,\n\t            use_cache=use_cache,\n\t            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n\t        )\n\t        lm_logits = F.linear(outputs[0],\n\t                             self.model.shared.weight,\n\t                             bias=self.final_logits_bias)\n", "        masked_lm_loss = None\n\t        if labels is not None:\n\t            loss_fct = CrossEntropyLoss()\n\t            # TODO(SS): do we need to ignore pad tokens in labels?\n\t            masked_lm_loss = loss_fct(\n\t                lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\t        if not return_dict:\n\t            output = (lm_logits, ) + outputs[1:]\n\t            return ((masked_lm_loss, ) +\n\t                    output) if masked_lm_loss is not None else output\n", "        return Seq2SeqLMOutput(\n\t            loss=masked_lm_loss,\n\t            logits=lm_logits,\n\t            past_key_values=outputs.past_key_values,\n\t            decoder_hidden_states=outputs.decoder_hidden_states,\n\t            decoder_attentions=outputs.decoder_attentions,\n\t            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n\t            encoder_hidden_states=outputs.encoder_hidden_states,\n\t            encoder_attentions=outputs.encoder_attentions,\n\t        )\n", "    def prepare_inputs_for_generation(self, decoder_input_ids, past,\n\t                                      attention_mask, use_cache,\n\t                                      encoder_outputs, **kwargs):\n\t        return {\n\t            \"input_ids\":\n\t            None,  # encoder_outputs is defined. input_ids not needed\n\t            \"encoder_outputs\": encoder_outputs,\n\t            \"past_key_values\": past,\n\t            \"decoder_input_ids\": decoder_input_ids,\n\t            \"attention_mask\": attention_mask,\n", "            \"use_cache\":\n\t            use_cache,  # change this to avoid caching (presumably for debugging)\n\t        }\n\t    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n\t        if cur_len == 1 and self.config.force_bos_token_to_be_generated:\n\t            self._force_token_ids_generation(logits, self.config.bos_token_id)\n\t        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:\n\t            self._force_token_ids_generation(logits, self.config.eos_token_id)\n\t        return logits\n\t    def _force_token_ids_generation(self, scores, token_id) -> None:\n", "        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float(\"inf\"))\"\"\"\n\t        scores[:, [x for x in range(self.config.vocab_size)\n\t                   if x != token_id]] = -float(\"inf\")\n\t    @staticmethod\n\t    def _reorder_cache(past, beam_idx):\n\t        reordered_past = []\n\t        for layer_past in past:\n\t            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n\t            layer_past_new = {\n\t                attn_key: _reorder_buffer(attn_cache, beam_idx)\n", "                for attn_key, attn_cache in layer_past.items()\n\t            }\n\t            reordered_past.append(layer_past_new)\n\t        return reordered_past\n\t    def get_encoder(self):\n\t        return self.model.encoder\n\t    def get_output_embeddings(self):\n\t        return _make_linear_from_emb(self.model.shared)  # make it on the fly\n\t@add_start_docstrings(\n\t    \"\"\"Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks. \"\"\",\n", "    BART_START_DOCSTRING,\n\t)\n\tclass BartForSequenceClassification(PretrainedBartModel):\n\t    def __init__(self, config: BartConfig, **kwargs):\n\t        super().__init__(config, **kwargs)\n\t        self.model = BartModel(config)\n\t        self.classification_head = BartClassificationHead(\n\t            config.d_model,\n\t            config.d_model,\n\t            config.num_labels,\n", "            config.classifier_dropout,\n\t        )\n\t        self.model._init_weights(self.classification_head.dense)\n\t        self.model._init_weights(self.classification_head.out_proj)\n\t    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n\t    @add_code_sample_docstrings(\n\t        # tokenizer_class=_TOKENIZER_FOR_DOC,\n\t        checkpoint=\"facebook/bart-large\",\n\t        output_type=Seq2SeqSequenceClassifierOutput,\n\t        config_class=_CONFIG_FOR_DOC,\n", "    )\n\t    def forward(\n\t            self,\n\t            input_ids,\n\t            attention_mask=None,\n\t            decoder_input_ids=None,\n\t            decoder_attention_mask=None,\n\t            encoder_outputs=None,\n\t            labels=None,\n\t            use_cache=None,\n", "            output_attentions=None,\n\t            output_hidden_states=None,\n\t            return_dict=None,\n\t    ):\n\t        r\"\"\"\n\t        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n\t            Labels for computing the sequence classification/regression loss.\n\t            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n\t            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n\t        \"\"\"\n", "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\t        if labels is not None:\n\t            use_cache = False\n\t        outputs = self.model(\n\t            input_ids,\n\t            attention_mask=attention_mask,\n\t            decoder_input_ids=decoder_input_ids,\n\t            decoder_attention_mask=decoder_attention_mask,\n\t            encoder_outputs=encoder_outputs,\n\t            use_cache=use_cache,\n", "            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n\t        )\n\t        x = outputs[0]  # last hidden state\n\t        eos_mask = input_ids.eq(self.config.eos_token_id)\n\t        if len(torch.unique(eos_mask.sum(1))) > 1:\n\t            raise ValueError(\n\t                \"All examples must have the same number of <eos> tokens.\")\n\t        sentence_representation = x[eos_mask, :].view(x.size(0), -1,\n", "                                                      x.size(-1))[:, -1, :]\n\t        logits = self.classification_head(sentence_representation)\n\t        loss = None\n\t        if labels is not None:\n\t            loss_fct = CrossEntropyLoss()\n\t            loss = loss_fct(logits.view(-1, self.config.num_labels),\n\t                            labels.view(-1))\n\t        if not return_dict:\n\t            output = (logits, ) + outputs[1:]\n\t            return ((loss, ) + output) if loss is not None else output\n", "        return Seq2SeqSequenceClassifierOutput(\n\t            loss=loss,\n\t            logits=logits,\n\t            past_key_values=outputs.past_key_values,\n\t            decoder_hidden_states=outputs.decoder_hidden_states,\n\t            decoder_attentions=outputs.decoder_attentions,\n\t            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n\t            encoder_hidden_states=outputs.encoder_hidden_states,\n\t            encoder_attentions=outputs.encoder_attentions,\n\t        )\n", "@add_start_docstrings(\n\t    \"\"\"BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layer on top of\n\t    the hidden-states output to compute `span start logits` and `span end logits`). \"\"\",\n\t    BART_START_DOCSTRING,\n\t)\n\tclass BartForQuestionAnswering(PretrainedBartModel):\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t        config.num_labels = 2\n\t        self.num_labels = config.num_labels\n", "        self.model = BartModel(config)\n\t        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\t        self.model._init_weights(self.qa_outputs)\n\t    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n\t    @add_code_sample_docstrings(\n\t        # tokenizer_class=_TOKENIZER_FOR_DOC,\n\t        checkpoint=\"facebook/bart-large\",\n\t        output_type=Seq2SeqQuestionAnsweringModelOutput,\n\t        config_class=_CONFIG_FOR_DOC,\n\t    )\n", "    def forward(\n\t            self,\n\t            input_ids,\n\t            attention_mask=None,\n\t            decoder_input_ids=None,\n\t            decoder_attention_mask=None,\n\t            encoder_outputs=None,\n\t            start_positions=None,\n\t            end_positions=None,\n\t            use_cache=None,\n", "            output_attentions=None,\n\t            output_hidden_states=None,\n\t            return_dict=None,\n\t    ):\n\t        r\"\"\"\n\t        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n\t            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n\t            Positions are clamped to the length of the sequence (`sequence_length`).\n\t            Position outside of the sequence are not taken into account for computing the loss.\n\t        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n", "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n\t            Positions are clamped to the length of the sequence (`sequence_length`).\n\t            Position outside of the sequence are not taken into account for computing the loss.\n\t        \"\"\"\n\t        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\t        if start_positions is not None and end_positions is not None:\n\t            use_cache = False\n\t        outputs = self.model(\n\t            input_ids,\n\t            attention_mask=attention_mask,\n", "            decoder_input_ids=decoder_input_ids,\n\t            decoder_attention_mask=decoder_attention_mask,\n\t            encoder_outputs=encoder_outputs,\n\t            use_cache=use_cache,\n\t            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n\t        )\n\t        sequence_output = outputs[0]\n\t        logits = self.qa_outputs(sequence_output)\n", "        start_logits, end_logits = logits.split(1, dim=-1)\n\t        start_logits = start_logits.squeeze(-1)\n\t        end_logits = end_logits.squeeze(-1)\n\t        total_loss = None\n\t        if start_positions is not None and end_positions is not None:\n\t            # If we are on multi-GPU, split add a dimension\n\t            if len(start_positions.size()) > 1:\n\t                start_positions = start_positions.squeeze(-1)\n\t            if len(end_positions.size()) > 1:\n\t                end_positions = end_positions.squeeze(-1)\n", "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n\t            ignored_index = start_logits.size(1)\n\t            start_positions.clamp_(0, ignored_index)\n\t            end_positions.clamp_(0, ignored_index)\n\t            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n\t            start_loss = loss_fct(start_logits, start_positions)\n\t            end_loss = loss_fct(end_logits, end_positions)\n\t            total_loss = (start_loss + end_loss) / 2\n\t        if not return_dict:\n\t            output = (\n", "                start_logits,\n\t                end_logits,\n\t            ) + outputs[1:]\n\t            return ((total_loss, ) +\n\t                    output) if total_loss is not None else output\n\t        return Seq2SeqQuestionAnsweringModelOutput(\n\t            loss=total_loss,\n\t            start_logits=start_logits,\n\t            end_logits=end_logits,\n\t            past_key_values=outputs.past_key_values,\n", "            decoder_hidden_states=outputs.decoder_hidden_states,\n\t            decoder_attentions=outputs.decoder_attentions,\n\t            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n\t            encoder_hidden_states=outputs.encoder_hidden_states,\n\t            encoder_attentions=outputs.encoder_attentions,\n\t        )\n\tclass SinusoidalPositionalEmbedding(nn.Embedding):\n\t    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n\t    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n\t        super().__init__(num_positions, embedding_dim)\n", "        if embedding_dim % 2 != 0:\n\t            raise NotImplementedError(\n\t                f\"odd embedding_dim {embedding_dim} not supported\")\n\t        self.weight = self._init_weight(self.weight)\n\t    @staticmethod\n\t    def _init_weight(out: nn.Parameter):\n\t        \"\"\"Identical to the XLM create_sinusoidal_embeddings except features are not interleaved.\n\t        The cos features are in the 2nd half of the vector. [dim // 2:]\n\t        \"\"\"\n\t        n_pos, dim = out.shape\n", "        position_enc = np.array(\n\t            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n\t             for pos in range(n_pos)])\n\t        out[:, 0:dim // 2] = torch.FloatTensor(np.sin(\n\t            position_enc[:, 0::2]))  # This line breaks for odd n_pos\n\t        out[:, dim // 2:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n\t        out.detach_()\n\t        out.requires_grad = False\n\t        return out\n\t    @torch.no_grad()\n", "    def forward(self, input_ids, use_cache=False):\n\t        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n\t        bsz, seq_len = input_ids.shape[:2]\n\t        if use_cache:\n\t            positions = input_ids.data.new(1, 1).fill_(\n\t                seq_len - 1)  # called before slicing\n\t        else:\n\t            # starts at 0, ends at 1-seq_len\n\t            positions = torch.arange(seq_len,\n\t                                     dtype=torch.long,\n", "                                     device=self.weight.device)\n\t        return super().forward(positions)\n"]}
{"filename": "src/model/modules.py", "chunked_list": ["import random\n\timport math\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom transformers.models.bart.modeling_bart import *\n\tfrom src.model.modeling_bart import (\n\t    SinusoidalPositionalEmbedding,\n\t    LearnedPositionalEmbedding,\n\t    invert_mask,\n", "    EncoderLayer,\n\t    LayerNorm,\n\t)\n\tfrom src.model.modeling_bart import (PretrainedBartModel, BartDecoder,\n\t                                     BartClassificationHead,\n\t                                     _make_linear_from_emb,\n\t                                     _prepare_bart_decoder_inputs)\n\tfrom src.model.config import MultiModalBartConfig\n\tclass ImageEmbedding(nn.Module):\n\t    def __init__(self, image_dim, final_dim):\n", "        super(ImageEmbedding, self).__init__()\n\t        self.linear = nn.Linear(image_dim, final_dim)\n\t    def forward(self, image_features):\n\t        img_len = list(map(len, image_features))\n\t        non_empty_features = list(filter(lambda x: len(x) > 0, image_features))\n\t        embedded = None\n\t        if len(non_empty_features) > 0:\n\t            img_tensor = torch.cat(non_empty_features, dim=0)\n\t            embedded = self.linear(img_tensor)\n\t        output = []\n", "        index = 0\n\t        for l in img_len:\n\t            if l > 0:\n\t                output.append(embedded[index:index + l])\n\t            else:\n\t                output.append(torch.empty(0))\n\t            index += l\n\t        return output\n\tclass MultiModalBartEncoder(nn.Module):\n\t    \"\"\"\n", "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n\t    is a :class:EncoderLayer.\n\t    Args:\n\t        config: MultiModalBartConfig\n\t    \"\"\"\n\t    def __init__(self, config: MultiModalBartConfig, encoder, img_feat_id,\n\t                 cls_token_id):\n\t        super().__init__()\n\t        self.img_feat_id = img_feat_id\n\t        self.cls_token_id = cls_token_id\n", "        embed_tokens = encoder.embed_tokens\n\t        self.dropout = encoder.dropout\n\t        self.layerdrop = encoder.layerdrop\n\t        self.indentity = nn.Identity()\n\t        embed_dim = embed_tokens.embedding_dim\n\t        self.embed_scale = encoder.embed_scale\n\t        self.padding_idx = embed_tokens.padding_idx\n\t        self.max_source_positions = encoder.max_source_positions\n\t        self.embed_tokens = embed_tokens\n\t        self.embed_images = ImageEmbedding(2048, embed_dim)\n", "        self.embed_positions = encoder.embed_positions\n\t        self.layers = encoder.layers\n\t        self.layernorm_embedding = encoder.layernorm_embedding\n\t        # mbart has one extra layer_norm\n\t        self.layer_norm = encoder.layer_norm\n\t    def _embed_multi_modal(self, input_ids, image_features):\n\t        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n\t        mask = (input_ids == self.img_feat_id) | (\n\t            input_ids == self.cls_token_id)\n\t        # print(mask.shape)\n", "        embedded_images = self.embed_images(image_features)\n\t        embedded = self.embed_tokens(input_ids)\n\t        # print('mask shape', mask.shape)\n\t        if not embedded_images[0].dtype == torch.float32:\n\t            embedded = embedded.half()\n\t        for index, value in enumerate(embedded_images):\n\t            if len(value) > 0:\n\t                embedded[index, mask[index]] = value\n\t        return embedded\n\t    def forward(self,\n", "                input_ids,\n\t                image_features,\n\t                attention_mask=None,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t                return_dict=False):\n\t        \"\"\"\n\t        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n\t        :param image_features: list[FloatTensor], image roi features with length of batch\n\t        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n", "        :param output_attentions:\n\t        :param output_hidden_states:\n\t        :return: Tuple comprised of:\n\t            - x (Tensor): the last encoder layer's output of\n\t              shape (src_len, batch, embed_dim)\n\t            - encoder_states (List[Tensor]): all intermediate\n\t              hidden states of shape (src_len, batch, embed_dim).\n\t              Only populated if output_hidden_states: is True.\n\t            - all_attentions (List[Tensor]): Attention weights for each layer.\n\t            During training might not be of length n_layers because of layer dropout.\n", "        \"\"\"\n\t        # check attention mask and invert\n\t        if attention_mask is not None:\n\t            attention_mask = invert_mask(attention_mask)\n\t        inputs_embeds = self._embed_multi_modal(\n\t            input_ids, image_features) * self.embed_scale\n\t        embed_pos = self.embed_positions(input_ids)\n\t        x = inputs_embeds + embed_pos\n\t        x = self.layernorm_embedding(x)\n\t        x = F.dropout(x, p=self.dropout, training=self.training)\n", "        # B x T x C -> T x B x C\n\t        x = x.transpose(0, 1)\n\t        encoder_states, all_attentions = [], []\n\t        for encoder_layer in self.layers:\n\t            if output_hidden_states:\n\t                encoder_states.append(x)\n\t            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n\t            dropout_probability = random.uniform(0, 1)\n\t            if self.training and (dropout_probability <\n\t                                  self.layerdrop):  # skip the layer\n", "                attn = None\n\t            else:\n\t                x, attn = encoder_layer(x,\n\t                                        attention_mask,\n\t                                        output_attentions=output_attentions)\n\t            if output_attentions:\n\t                all_attentions.append(attn)\n\t        if self.layer_norm:\n\t            x = self.layer_norm(x)\n\t        if output_hidden_states:\n", "            encoder_states.append(x)\n\t        # T x B x C -> B x T x C\n\t        encoder_states = [\n\t            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n\t        ]\n\t        x = x.transpose(0, 1)\n\t        if not return_dict:\n\t            return tuple(v for v in [x, encoder_states, all_attentions]\n\t                         if v is not None)\n\t        return BaseModelOutput(last_hidden_state=x,\n", "                               hidden_states=encoder_states,\n\t                               attentions=all_attentions)\n\tclass MultiModalBartDecoder_span(nn.Module\n\t                                 ):  #AOE task and all downstream tasks\n\t    def __init__(self,\n\t                 config: MultiModalBartConfig,\n\t                 tokenizer,\n\t                 decoder,\n\t                 pad_token_id,\n\t                 label_ids,\n", "                 causal_mask,\n\t                 need_tag=True,\n\t                 only_sc=False,\n\t                 avg_feature=False,\n\t                 use_encoder_mlp=True):\n\t        super().__init__()\n\t        self.decoder = decoder\n\t        self.tokenizer = tokenizer\n\t        self.causal_mask = causal_mask\n\t        self.register_buffer('causal_masks', causal_mask.float())\n", "        self.pad_token_id = pad_token_id\n\t        # label_ids = sorted(label_ids, reverse=False)\n\t        self.label_start_id = min(label_ids)\n\t        self.label_end_id = max(label_ids) + 1\n\t        self.need_tag = need_tag\n\t        self.only_sc = only_sc\n\t        mapping = torch.LongTensor([0, 2] + label_ids)\n\t        ###mapping: [0, 2, 50276, 50277, 50278, 50281]\n\t        self.register_buffer('mapping', mapping)\n\t        self.src_start_index = len(mapping)  # 加上一个\n", "        hidden_size = decoder.embed_tokens.weight.size(1)\n\t        self.dropout_layer = nn.Dropout(0.1)\n\t        self.end_text_id = tokenizer.end_text_id\n\t        self.avg_feature = avg_feature\n\t        if use_encoder_mlp:\n\t            self.encoder_mlp = nn.Sequential(\n\t                nn.Linear(hidden_size, hidden_size), nn.Dropout(0.3),\n\t                nn.ReLU(), nn.Linear(hidden_size, hidden_size))\n\t    def forward(self, tokens, state, only_sc=False):\n\t        # import ipdb; ipdb.set_trace()\n", "        '''\n\t        tokens: [[0, 2, 2, 16, 16, 4, 18, 18, 4, 1, 1, 1, 1],\n\t                 [0, 2, 2, 15, 16, 3, 25, 26, 5, 28, 28, 4, 1]]\n\t        '''\n\t        # import ipdb; ipdb.set_trace()\n\t        bsz, max_len = tokens.size()\n\t        encoder_outputs = state.encoder_output ##(batch, 72=38(len(image_token+begin_image+end_image(36+1+1)))+34(max_tex_len(包含begin_text_id(0) and end_text_id(2)) in batch), 768)\n\t        encoder_pad_mask = state.encoder_mask ##(batch, 72)\n\t        first = state.first\n\t        # tokens之后的0全是padding，因为1是eos, 在pipe中规定的\n", "        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n\t        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n\t        # 把输入做一下映射\n\t        mapping_token_mask = tokens.lt(\n\t            self.src_start_index)  # 为1的地方应该从mapping中取index\n\t        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n\t        tag_mapped_tokens = self.mapping[mapped_tokens]\n\t        src_tokens_index = tokens - self.src_start_index  # bsz x num_src_token\n\t        src_tokens_index = src_tokens_index.masked_fill(\n\t            src_tokens_index.lt(0), 0)\n", "        src_tokens = state.src_tokens \n\t        # print(src_tokens.shape): (2, 34)\n\t        if first is not None:\n\t            src_tokens = src_tokens.gather(index=first, dim=1) ###Sequence\n\t        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n\t        #print('word_mapped_tokens', word_mapped_tokens[0])\n\t        tokens = torch.where(mapping_token_mask, tag_mapped_tokens,\n\t                             word_mapped_tokens)\n\t        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n\t        '''\n", "        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n\t        tensor([[0, 50276, 50276, 4644, 4644, 50278, 798, 798, 50278, 2, 1, 1, 1],\n\t                [0, 50276, 50276, 9517, 957, 50277, 2561, 7772, 50281, 2762, 2762, 50278, 2]])\n\t        将tokens中的index以及标签都转化为vocabulary中的token_id\n\t        '''\n\t        if self.training:\n\t            tokens = tokens[:, :-1]\n\t            decoder_pad_mask = tokens.eq(\n\t                self.pad_token_id)  # decoder需要让pad位置为1\n\t            dict = self.decoder(input_ids=tokens,\n", "                                encoder_hidden_states=encoder_outputs,\n\t                                encoder_padding_mask=encoder_pad_mask,\n\t                                decoder_padding_mask=decoder_pad_mask,\n\t                                decoder_causal_mask=self.\n\t                                causal_masks[:tokens.size(1), :tokens.size(1)],\n\t                                return_dict=True)\n\t        else:\n\t            past_key_values = state.past_key_values\n\t            dict = self.decoder(input_ids=tokens,\n\t                                encoder_hidden_states=encoder_outputs,\n", "                                encoder_padding_mask=encoder_pad_mask,\n\t                                decoder_padding_mask=None,\n\t                                decoder_causal_mask=self.\n\t                                causal_masks[:tokens.size(1), :tokens.size(1)],\n\t                                return_dict=True)\n\t        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size (2, 12(去掉了 end_token_id), 768)\n\t        hidden_state = self.dropout_layer(hidden_state)\n\t        if not self.training:\n\t            state.past_key_values = dict.past_key_values\n\t        logits = hidden_state.new_full(\n", "            (hidden_state.size(0), hidden_state.size(1),\n\t             self.src_start_index + src_tokens.size(-1)),\n\t            fill_value=-1e24)\n\t        ##建立空的logits\n\t        # print('logits', logits.shape) (bsz, max_len,  self.src_start_index + src_tokens.size(-1)) -> (2, 12, 40=6+34)\n\t        # 首先计算的是\n\t        if self.need_tag:\n\t            '''\n\t            self.decoder.embed_tokens.weight: (50289, 768)\n\t            self.label_start_id: 50276\n", "            '''\n\t            tag_scores = F.linear(\n\t                hidden_state,\n\t                self.dropout_layer(\n\t                    self.decoder.embed_tokens.\n\t                    weight[self.label_start_id:self.label_start_id +\n\t                           3]))  # bsz x max_len x num_class\n\t            logits[:, :, 3:self.src_start_index] = tag_scores ###给情感的position赋值[:, :, (3, 4, 5)]\n\t        if not only_sc:\n\t            eos_scores = F.linear(\n", "                hidden_state,\n\t                self.dropout_layer(self.decoder.embed_tokens.weight[2:3])) \n\t            '''\n\t            ['</s>(eos_token)', '<mask>', '<pad>', '<s>(bos_token)', '<unk>']\n\t            [2, 50264, 1, 0, 3]\n\t            '''\n\t            # bsz x max_bpe_len(image_len + text_len) x hidden_size: (2, 72, 768)\n\t            src_outputs = state.encoder_output \n\t            if hasattr(self, 'encoder_mlp') and not only_sc:\n\t                src_outputs = self.encoder_mlp(src_outputs)\n", "            if first is not None:\n\t                mask = first.eq(0)\n\t                src_outputs = src_outputs.gather(\n\t                    index=first.unsqueeze(2).repeat(1, 1,\n\t                                                    src_outputs.size(-1)),\n\t                    dim=1)\n\t            else:\n\t                mask = state.encoder_mask[:, 38:].eq(0)\n\t                # src_outputs = self.decoder.embed_tokens(src_tokens)\n\t            mask = mask.unsqueeze(1) ## bsz x 1 x max_word_len: (2, 1, 34)\n", "            input_embed = self.decoder.embed_tokens(\n\t                src_tokens)  #bsz x max_word_len x hidden_size: (2, 34, 768); src_tokens: (2, 34)\n\t            input_embed = self.dropout_layer(input_embed)\n\t            if self.avg_feature:  # 先把feature合并一下\n\t                src_outputs = (src_outputs[:, 38:] + input_embed) / 2\n\t            word_scores = torch.einsum(\n\t                'blh,bnh->bln', hidden_state,\n\t                src_outputs[:, 38:])  # bsz x max_len x max_word_len: (2, 12, 34)\n\t            if not self.avg_feature:\n\t                gen_scores = torch.einsum(\n", "                    'blh,bnh->bln', hidden_state,\n\t                    input_embed)  # bsz x max_len x max_word_len: (2, 12, 34)\n\t                word_scores = (gen_scores + word_scores) / 2 \n\t            mask = mask.__or__(\n\t                src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1)) ###(2, 1, 34)\n\t            word_scores = word_scores.masked_fill(mask, -1e32) ###(bts, max_len, max_word_len)\n\t            logits[:, :, self.src_start_index:] = word_scores\n\t            ###logits.shape (bts, max_len, max_word_len+6): (2, 12, 40)\n\t            logits[:, :, 1:2] = eos_scores\n\t        # print(torch.argmax(logits[0], dim=-1))\n", "        return logits\n\t    def decode(self, tokens, state, only_sc=False):\n\t        return self(tokens, state, only_sc)[:, -1]\n\tclass Span_loss(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t        # self.loss_fct = nn.CrossEntropyLoss()\n\t        self.fc = nn.LogSoftmax(dim=-1)\n\t    def forward(self, tgt_tokens, pred, mask):\n\t        '''\n", "        tgt_tokens: (2 (batch-size), 12 (max_len+1))\n\t        pred: (2, 12, 40 (max_word_len))\n\t        '''\n\t        tgt_tokens = tgt_tokens.masked_fill(mask.eq(0), -100)\n\t        output = F.cross_entropy(target=tgt_tokens, input=pred.transpose(1, 2)) ##每一个词都有12种类别， input= (40, 12)\n\t        return output\n\tclass MultiModalBartDecoder_MLM(nn.Module):\n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n\t        self.config = config\n", "        self.decoder = decoder\n\t        self.register_buffer(\n\t            \"final_logits_bias\",\n\t            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\t    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n\t        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n\t            self.config,\n\t            input_ids,\n\t            decoder_input_ids=decoder_input_ids,\n", "            decoder_padding_mask=decoder_attention_mask,\n\t            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n\t        decoder_outputs = self.decoder(\n\t            decoder_input_ids,\n\t            encoder_outputs,\n\t            attention_mask,\n\t            decoder_padding_mask,\n\t            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n\t                                            decoder_input_ids.size(1)],\n\t        )\n", "        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n\t                             self.decoder.embed_tokens.weight,\n\t                             bias=self.final_logits_bias)\n\t        lm_loss = 0\n\t        # compute lm loss if labels is given\n\t        if labels is not None:\n\t            labels = labels.clone()\n\t            loss_fct = nn.CrossEntropyLoss()\n\t            lm_loss = loss_fct(\n\t                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n", "                labels.reshape(-1))\n\t            return lm_loss\n\tclass MultiModalBartDecoder_ANP_generate(nn.Module):  #AOG task\n\t    def __init__(self, config: MultiModalBartConfig, decoder):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.register_buffer(\n\t            \"final_logits_bias\",\n\t            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n", "    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n\t                decoder_input_ids, decoder_attention_mask):\n\t        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n\t            self.config,\n\t            input_ids,\n\t            decoder_input_ids=decoder_input_ids,\n\t            decoder_padding_mask=decoder_attention_mask,\n\t            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n\t        decoder_outputs = self.decoder(\n\t            decoder_input_ids,\n", "            encoder_outputs,\n\t            attention_mask,\n\t            decoder_padding_mask,\n\t            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n\t                                            decoder_input_ids.size(1)],\n\t        )\n\t        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n\t                             self.decoder.embed_tokens.weight,\n\t                             bias=self.final_logits_bias)\n\t        lm_loss = 0\n", "        # compute lm loss if labels is given\n\t        if labels is not None:\n\t            labels = labels.clone()\n\t            # labels[labels == self.cls_token_id] = -100\n\t            loss_fct = nn.CrossEntropyLoss()\n\t            lm_loss = loss_fct(\n\t                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n\t                labels.reshape(-1))\n\t            return lm_loss\n\tclass MultiModalBartDecoder_sentiment(nn.Module):  #MSP task\n", "    def __init__(self,\n\t                 config: MultiModalBartConfig,\n\t                 decoder,\n\t                 senti_ids,\n\t                 senti_nums=3):\n\t        super().__init__()\n\t        self.config = config\n\t        self.decoder = decoder\n\t        self.senti_ids = senti_ids\n\t        self.dropout_layer = nn.Dropout(0.1)\n", "        self.senti_head = BartClassificationHead(config.d_model,\n\t                                                 config.d_model, senti_nums,\n\t                                                 config.classif_dropout)\n\t    def _init_weights(self, module):\n\t        module.weight.data.normal_(mean=0.0, std=0.02)\n\t        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    def forward(self, senti_labels, encoder_outputs, attention_mask,\n\t                senti_decoder_input_ids):\n\t        decoder_outputs = self.decoder(\n", "            input_ids=senti_decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask,\n\t            decoder_padding_mask=None,\n\t            decoder_causal_mask=None,\n\t        )\n\t        # predict_senti = F.linear(\n\t        #     decoder_outputs[0][:, 1],\n\t        #     self.dropout_layer(self.decoder.embed_tokens.\n\t        #                        weight[self.senti_ids[0]:self.senti_ids[2] +\n", "        #                               1]))  # bsz\n\t        # predict_senti = torch.flip(predict_senti, dims=[-1])\n\t        predict_senti = self.senti_head(decoder_outputs[0][:, 1])\n\t        loss_fct = nn.CrossEntropyLoss()\n\t        senti_loss = loss_fct(predict_senti, senti_labels)\n\t        return senti_loss, predict_senti\n\tclass MultiModalBartDecoder_MRM(nn.Module):\n\t    def __init__(self, config: MultiModalBartConfig, decoder, causal_mask,\n\t                 args):\n\t        super().__init__()\n", "        self.config = config\n\t        self.decoder = decoder\n\t        self.causal_mask = causal_mask\n\t        self.args = args\n\t        self.mrm_head = BartClassificationHead(\n\t            config.d_model,\n\t            config.d_model,\n\t            config.num_labels,\n\t            config.classif_dropout,\n\t        )\n", "        self._init_weights(self.mrm_head.dense)\n\t        self._init_weights(self.mrm_head.out_proj)\n\t    def _init_weights(self, module):\n\t        module.weight.data.normal_(mean=0.0, std=0.02)\n\t        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    def forward(self, mrm_labels, mrm_masks, encoder_outputs, attention_mask,\n\t                mrm_decoder_input_ids, mrm_decoder_attention_mask):\n\t        decoder_padding_mask = mrm_decoder_attention_mask.eq(0)\n\t        decoder_outputs = self.decoder(\n", "            input_ids=mrm_decoder_input_ids,\n\t            encoder_hidden_states=encoder_outputs,\n\t            encoder_padding_mask=attention_mask,\n\t            decoder_padding_mask=decoder_padding_mask,\n\t            decoder_causal_mask=self.causal_mask[:mrm_decoder_input_ids.size(\n\t                1), :mrm_decoder_input_ids.size(1)].to(\n\t                    mrm_decoder_input_ids.device),\n\t        )\n\t        region_representation = decoder_outputs[0][mrm_masks.bool()]\n\t        if len(region_representation) > 0:\n", "            predict_cls = self.mrm_head(region_representation)\n\t            loss_fct = nn.CrossEntropyLoss()\n\t            mrm_labels = torch.cat(mrm_labels,\n\t                                   dim=0).to(encoder_outputs.device)\n\t            if self.args.mrm_loss_type == 'KL':\n\t                predict_cls = F.log_softmax(predict_cls, dim=-1)\n\t                mrm_loss = F.kl_div(predict_cls.double(),\n\t                                    mrm_labels.double().squeeze(1),\n\t                                    reduction='batchmean')\n\t            else:\n", "                raise RuntimeError(\"wrong mrm type\")\n\t        else:\n\t            mrm_loss = 0\n\t        return mrm_loss\n"]}
{"filename": "src/model/attention.py", "chunked_list": ["import torch.nn as nn\n\timport torch\n\timport numpy as np\n\tclass Attention_for_Senti_Prompt(nn.Module):\n\t    def __init__(self, n_head=8, model_dim=768, drop_rate=0.2):\n\t        # n_head 有几层注意力机制\n\t        # model_dim 模型的维度\n\t        # drop_rate 随机丢弃率\n\t        super().__init__()\n\t        self.n_head = n_head\n", "        self.head_dim = model_dim // n_head     # 32//4=8\n\t        self.wq = nn.Linear(model_dim, n_head * self.head_dim)  # [4*8]\n\t        self.wk = nn.Linear(model_dim, n_head * self.head_dim)\n\t        self.wv = nn.Linear(model_dim, n_head * self.head_dim)\n\t        self.o_dense = nn.Linear(model_dim, model_dim)\n\t        self.o_drop = nn.Dropout(drop_rate)\n\t        self.layer_norm = nn.LayerNorm(model_dim)\n\t    def forward(self, query, k, v, mask=None):\n\t        # residual connect\n\t        # q: [4, 1, 768]\n", "        # k=v=[batch_size,seq_len, emb_dim]=[4, 3, 768]\n\t        residual = query    # 残差\n\t        # linear projection\n\t        key = self.wk(k)    # [batch_size,seq_len, num_heads * head_dim]\n\t        value = self.wv(v)  # [batch_size,seq_len, num_heads * head_dim]\n\t        query = self.wq(query)  # [batch_size,seq_len, num_heads * head_dim]\n\t        # 将头分离出来\n\t        # [step,n_head,n,head_dim] = [batch_size,头的数量，seq_len,每个头的维度]\n\t        query = self.split_heads(query) # [4,1,8,96]\n\t        key = self.split_heads(key)     # [4,3,8,96]\n", "        value = self.split_heads(value) # [4,3,8,96]\n\t        # 自注意力机制 点乘 \n\t        context = self.scaled_dot_product_attention(\n\t            query, key, value, mask)    # [batch_size,seq_len, model_dim]\n\t        # 再经过一个线性变化\n\t        o = self.o_dense(context)       # [batch_size,seq_len, model_dim]\n\t        # 随机使得一些权重失效\n\t        o = self.o_drop(o)\n\t        # layer normalization\n\t        o = self.layer_norm(residual+o)\n", "        return o\n\t    def split_heads(self, x):\n\t        x = torch.reshape(\n\t            x, (x.shape[0], x.shape[1], self.n_head, self.head_dim))\n\t        # x = [step,n_head,n,head_dim]\n\t        return x.permute(0, 2, 1, 3)\n\t    def scaled_dot_product_attention(self, query, k, v, mask=None):\n\t        # query: [4, 8, 1, 96]\n\t        # k=v: [4, 8, 3, 96]\n\t        dk = torch.tensor(k.shape[-1]).type(torch.float) ##96\n", "        # import pdb; pdb.set_trace()\n\t        score = torch.matmul(query, k.permute(0, 1, 3, 2)) / (torch.sqrt(dk) + 1e-8)                 # [step, n_head, n, n]=[32, 4, 11, 11]\n\t        if mask is not None:\n\t            score = score.masked_fill_(mask, -np.inf) ##[4, 8, 1, 3]\n\t        self.attention = torch.softmax(score, dim=-1)    ##[4, 8, 1, 3]\n\t        context = torch.matmul(self.attention, v)   # [step, num_head, n, head_dim]: [4, 8, 1, 96]\n\t        context = context.permute(0, 2, 1, 3)       # [batch_size,seq_len, num_head, head_dim]: [4, 1, 8, 96]\n\t        context = context.reshape((context.shape[0], context.shape[1], -1)) ##[4, 1, 768]\n\t        return context                              # [batch_size,seq_len, model_dim]\n\tif __name__ == \"__main__\":\n", "    attention = Attention_for_Senti_Prompt()\n\t    device = torch.device('cuda:0' )\n\t    query = torch.randn(4, 1, 768)\n\t    key = torch.randn(4, 3, 768)\n\t    value = key\n\t    xx = attention(query, key, value)\n\t    print(xx.shape)\n"]}
{"filename": "src/model/MAESC_model_for_generated_senti_prompt.py", "chunked_list": ["from typing import Optional, Tuple\n\tfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\n\tfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\n\tfrom fastNLP.modules.torch import State\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n\t                                     BartDecoder, BartModel,\n\t                                     BartClassificationHead,\n", "                                     _make_linear_from_emb,\n\t                                     _prepare_bart_decoder_inputs)\n\tfrom transformers import BartTokenizer\n\tfrom src.model.config import MultiModalBartConfig\n\t#from src.model.mixins import GenerationMixin, FromPretrainedMixin\n\tfrom src.model.modules_for_prompt import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_sentiment_prompt, MultiModalBartDecoder_generate_sentiment_prompt\n\tclass MultiModalBartModel_AESC(PretrainedBartModel):\n\t    def build_model(self,\n\t                    args,\n\t                    bart_model,\n", "                    tokenizer,\n\t                    label_ids,\n\t                    config,\n\t                    decoder_type=None,\n\t                    copy_gate=False,\n\t                    use_encoder_mlp=False,\n\t                    use_recur_pos=False,\n\t                    tag_first=False):\n\t        if args.bart_init:\n\t            model = BartModel.from_pretrained(bart_model)\n", "            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n\t            print('num_tokens', num_tokens)\n\t            model.resize_token_embeddings(\n\t                len(tokenizer.unique_no_split_tokens) + num_tokens)\n\t            encoder = model.encoder\n\t            decoder = model.decoder\n\t            padding_idx = config.pad_token_id\n\t            encoder.embed_tokens.padding_idx = padding_idx\n\t            # if use_recur_pos:\n\t            #     decoder.set_position_embedding(label_ids[0], tag_first)\n", "            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\t            for token in tokenizer.unique_no_split_tokens:\n\t                if token[:2] == '<<':  # 特殊字符\n\t                    index = tokenizer.convert_tokens_to_ids(\n\t                        tokenizer._base_tokenizer.tokenize(token))\n\t                    if len(index) > 1:\n\t                        raise RuntimeError(f\"{token} wrong split\")\n\t                    else:\n\t                        index = index[0]\n\t                    assert index >= num_tokens, (index, num_tokens, token)\n", "                    indexes = _tokenizer.convert_tokens_to_ids(\n\t                        _tokenizer.tokenize(token[2:-2]))\n\t                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n\t                    for i in indexes[1:]:\n\t                        embed += model.decoder.embed_tokens.weight.data[i]\n\t                    embed /= len(indexes)\n\t                    model.decoder.embed_tokens.weight.data[index] = embed\n\t        else:\n\t            raise RuntimeError(\"error init!!!!!!!\")\n\t        multimodal_encoder_for_generated_senti_prompt = MultiModalBartEncoder(config, encoder,\n", "                                                   tokenizer.img_feat_id,\n\t                                                   tokenizer.cls_token_id,\n\t                                                   args.num_image_tokens)\n\t        multimodal_encoder = MultiModalBartEncoder_for_Generating_sentiment_prompt(\n\t                                                                         use_generated_prompt=args.use_generated_prompt, \n\t                                                                         config=config,\n\t                                                                         encoder = encoder,\n\t                                                                         img_feat_id = tokenizer.img_feat_id,\n\t                                                                         aspect_prompt_token_id=tokenizer.aspect_prompt_token_id,\n\t                                                                         senti_prompt_token_id=tokenizer.senti_prompt_token_id,\n", "                                                                         cls_token_id = tokenizer.cls_token_id,\n\t                                                                         num_image_tokens = args.num_image_tokens,\n\t                                                                         use_different_senti_prompt=args.use_different_senti_prompt \n\t                                                   )\n\t        return (multimodal_encoder_for_generated_senti_prompt, multimodal_encoder, decoder)\n\t    def __init__(self, config: MultiModalBartConfig, args, bart_model,\n\t                 tokenizer, label_ids):\n\t        super().__init__(config)\n\t        self.config = config\n\t        self.tokenizer = tokenizer\n", "        label_ids = sorted(label_ids)\n\t        multimodal_encoder_for_generated_senti_prompt, multimodal_encoder, share_decoder = self.build_model(\n\t            args, bart_model, self.tokenizer, label_ids, config)\n\t        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n\t        self.causal_mask = causal_mask.triu(diagonal=1)\n\t        self.num_image_tokens = args.num_image_tokens\n\t        self.senti_prompt_encoder = multimodal_encoder_for_generated_senti_prompt\n\t        self.encoder = multimodal_encoder\n\t        only_sc = False\n\t        # need_tag = True  #if predict the sentiment or not\n", "        if args.task == 'twitter_ae':\n\t            need_tag = False\n\t        else:\n\t            need_tag = True\n\t            # if args.task == 'twitter_sc':\n\t            #     only_sc = True\n\t        self.senti_prompt_decoder = MultiModalBartDecoder_generate_sentiment_prompt(self.config, share_decoder)\n\t        self.decoder = MultiModalBartDecoder_span(self.config,\n\t                                                  self.tokenizer,\n\t                                                  share_decoder,\n", "                                                  self.tokenizer.pad_token_id,\n\t                                                  label_ids,\n\t                                                  self.causal_mask,\n\t                                                  num_image_tokens=self.num_image_tokens,\n\t                                                  need_tag=need_tag,\n\t                                                  only_sc=False)\n\t        self.span_loss_fct = Span_loss()\n\t    def prepare_state(self,\n\t                      input_ids,\n\t                      image_features,\n", "                      attention_mask=None,\n\t                      aesc_infos=None,\n\t                      aspects_num=None,\n\t                      first=None):\n\t        ##generate prompt for each instance\n\t        prompt_attention_mask = attention_mask\n\t        if self.num_image_tokens==0:\n\t            end_index = 62\n\t            begin_index = 22\n\t        elif self.num_image_tokens==1:\n", "            end_index = 63\n\t            begin_index = 23\n\t        elif self.num_image_tokens==2:\n\t            end_index = 64\n\t            begin_index = 24\n\t        elif self.num_image_tokens==3:\n\t            end_index = 65\n\t            begin_index = 25\n\t        elif self.num_image_tokens==4:\n\t            end_index = 66\n", "            begin_index = 26\n\t        elif self.num_image_tokens==5:\n\t            end_index = 67\n\t            begin_index = 27\n\t        elif self.num_image_tokens==6:\n\t            end_index = 68\n\t            begin_index = 28\n\t        elif self.num_image_tokens==7:\n\t            end_index = 69\n\t            begin_index = 29\n", "        for i in range(len(prompt_attention_mask)):\n\t            mask = prompt_attention_mask[i]\n\t            mask[begin_index:end_index]=torch.zeros_like(mask[begin_index:end_index]) ##26:66 是aspect提示的位置\n\t            prompt_attention_mask[i]=mask\n\t        dict_for_prompt = self.senti_prompt_encoder(input_ids=input_ids,\n\t                                              image_features=image_features,\n\t                                              attention_mask=prompt_attention_mask,\n\t                                              output_hidden_states=True,\n\t                                              return_dict=True)\n\t        prompt_decoder_input_ids, prompt_decoder_attention_mask = [\n", "            aesc_infos['senti_prompt_decoder_input_ids'].to(input_ids.device),\n\t            aesc_infos['senti_prompt_decoder_attention_mask'].to(input_ids.device)]\n\t        generated_prompt = self.senti_prompt_decoder(\n\t                                            encoder_outputs=dict_for_prompt.last_hidden_state, \n\t                                            attention_mask=attention_mask,\n\t                                            decoder_input_ids =prompt_decoder_input_ids, decoder_attention_mask=prompt_decoder_attention_mask)\n\t        generated_prompt = generated_prompt[:, 1:, :] ##(batch_size, 2, 768)\n\t        dict = self.encoder(\n\t                            input_ids=input_ids,\n\t                            image_features=image_features,\n", "                            attention_mask=attention_mask,\n\t                            generated_prompt= generated_prompt,\n\t                            aspects_num = aspects_num,\n\t                            output_hidden_states=True,\n\t                            return_dict=True)\n\t        encoder_outputs = dict.last_hidden_state\n\t        hidden_states = dict.hidden_states\n\t        encoder_mask = attention_mask\n\t        src_embed_outputs = hidden_states[0]\n\t        state = BartState(\n", "            encoder_outputs,\n\t            encoder_mask,\n\t            input_ids[:,\n\t                      end_index:],  #the text features start from index 38, the front are image features.\n\t            first,\n\t            src_embed_outputs)\n\t        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n\t        return state\n\t    def forward(\n\t            self,\n", "            input_ids,\n\t            image_features,\n\t            attention_mask=None,\n\t            aesc_infos=None,\n\t            aspects_num=None,\n\t            encoder_outputs: Optional[Tuple] = None,\n\t            use_cache=None,\n\t            output_attentions=None,\n\t            output_hidden_states=None,\n\t    ):\n", "        ### for prompt\n\t        # import ipdb; ipdb.set_trace()\n\t        ## for aspect-spans\n\t        aspects_num = torch.tensor(aspects_num).to(input_ids.device)\n\t        state = self.prepare_state( input_ids, image_features, attention_mask, aesc_infos, aspects_num)\n\t        spans, span_mask = [ \n\t            aesc_infos['labels'].to(input_ids.device),\n\t            aesc_infos['masks'].to(input_ids.device)\n\t        ]\n\t        logits = self.decoder(spans, state) ## spans: (2, 13) logits: (2, 12, 40)\n", "        loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n\t        return loss\n\tclass BartState(State):\n\t    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n\t                 src_embed_outputs):\n\t        super().__init__(encoder_output, encoder_mask)\n\t        self.past_key_values = None\n\t        self.src_tokens = src_tokens\n\t        self.first = first\n\t        self.src_embed_outputs = src_embed_outputs\n", "    def reorder_state(self, indices: torch.LongTensor):\n\t        super().reorder_state(indices)\n\t        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n\t        if self.first is not None:\n\t            self.first = self._reorder_state(self.first, indices)\n\t        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n\t                                                     indices)\n\t        if self.past_key_values is not None:\n\t            new = []\n\t            for layer in self.past_key_values:\n", "                new_layer = {}\n\t                for key1 in list(layer.keys()):\n\t                    new_layer_ = {}\n\t                    for key2 in list(layer[key1].keys()):\n\t                        if layer[key1][key2] is not None:\n\t                            layer[key1][key2] = self._reorder_state(\n\t                                layer[key1][key2], indices)\n\t                            # print(key1, key2, layer[key1][key2].shape)\n\t                        new_layer_[key2] = layer[key1][key2]\n\t                    new_layer[key1] = new_layer_\n", "                new.append(new_layer)\n\t            self.past_key_values = new\n"]}
