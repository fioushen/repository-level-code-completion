{"filename": "src/superbig/base.py", "chunked_list": ["import re\n\tfrom typing import Any\n\timport torch\n\tclass Embedding():\n\t    def __init__(self, tensors: list[torch.Tensor] = [], embedder=None) -> None:\n\t        self.embeddings = tensors\n\t        self.embedder = embedder\n\tclass Chunk():\n\t    def __init__(self, text: str, embeddings: list[torch.Tensor] = []) -> None:\n\t        self.text = text\n", "        self.embeddings = embeddings\n\t        self.metadatas = {}\n\t        self.id = 0\n\tclass Bucket():\n\t    def __init__(self, name: str, chunks: list[Chunk]) -> None:\n\t        self.name = name\n\t        self.chunks = chunks\n\t        self.ids = []\n\t        self.embeddings = []\n\t        self.metadatas = []\n", "        self.documents = []\n\tclass Page():\n\t    def __init__(self, title: str) -> None:\n\t        self.title = title\n\t        self.contents: list[Chunk] = []\n\t    def add_chunk(self, chunk: Chunk):\n\t        self.contents.append(chunk)\n\t    def add_chunks(self, chunks: list[Chunk]):\n\t        self.contents.extend(chunks)\n\t    def remove_chunk(self, chunk: Chunk):\n", "        pass\n\t    def scroll(self, scroll_direction: int, scroll_speed: int):\n\t        pass\n\t    def view(self, size: int = -1) -> str:\n\t        return '\\n'.join([self.title] + [chunk.text.replace('\\n', '') for chunk in self.contents])[0:500]\n\tclass Window():\n\t    def __init__(self, pages: list[Page] = []) -> None:\n\t        self.pages = pages\n\t    def add_page(self, page: Page):\n\t        pass\n", "    def remove_page(self, page: Page):\n\t        pass\n\t    def freeze(self, indices: list[int]):\n\t        pass\n\t    def scroll(self, indicies: list[int], scroll_directions: list[int], scroll_speeds: list[int]):\n\t        pass\n\t    def view(self) -> str:\n\t        return '\\n'.join([page.view() for page in self.pages])[0:4000]\n\tclass InjectionPoint():\n\t    def __init__(self, name) -> None:\n", "        self.name = f'[[[{name}]]]'\n\t        self.real_name = name\n\t        self.target = self.name\n\tclass SearchResult():\n\t    def __init__(self, result) -> None:\n\t        self.result = result\n\tclass SourceMetadata():\n\t    def __init__(self) -> None:\n\t        self.attributes = {\n\t            'title': '',\n", "            'description': ''\n\t        }\n\t    def get(self, attribute) -> Any:\n\t        if attribute in self.attributes:\n\t            return self.attributes[attribute]\n\t        return None\n\t    def add(self, attribute: str, value: Any):\n\t        self.attributes[attribute] = value\n\tclass Source():\n\t    def __init__(self, name: str = ''):\n", "        self.name = name\n\t        self.contents: str | None = None\n\t        self.loaded = False\n\t        self.chunked = False\n\t        self.cache_key = ''\n\t        self.metadata = SourceMetadata()\n\t    def get(self) -> str:\n\t        self.loaded = True\n\t        return self.contents or ''\n\t    def set(self, str: str = ''):\n", "        self.loaded = False\n\t    def invalidate(self, cache_key=''):\n\t        self.chunked = False\n\t        self.loaded = False\n\t        self.cache_key = cache_key\n\tclass PreparedPrompt():\n\t    def __init__(self):\n\t        pass\n\t    def from_prompt(self, prompt: str, formatted_prompt: dict[str, Any] = {}) -> dict:\n\t        if formatted_prompt is None:\n", "            raise NotImplementedError\n\t        pattern = r'\\[\\[\\[.*?\\]\\]\\]'\n\t        matches = re.findall(pattern, prompt)\n\t        injection_point_names: list[str] = []\n\t        for match in matches:\n\t            injection_point_names.append(match)\n\t            formatted_prompt[match] = ''\n\t        self.source_prompt = prompt\n\t        self.prepared_prompt = formatted_prompt\n\t        self.injection_point_names = injection_point_names\n", "        self.injected_prompt: dict | None = None\n\t        return self.prepared_prompt\n\t    def get_search_strings(self) -> list[str]:\n\t        ...\n\t    def get_injection_points(self) -> list[str]:\n\t        if self.prepared_prompt is None:\n\t            raise ValueError\n\t        return self.injection_point_names\n\t    def get(self) -> dict[str, str]:\n\t        return self.prepared_prompt\n", "    def inject(self, injection_point: InjectionPoint, text: str):\n\t        injected_prompt = self.prepared_prompt\n\t        for key, section in self.prepared_prompt.items():\n\t            if type(section) != str:\n\t                continue\n\t            injected_prompt[key] = section.replace(\n\t                injection_point.target, text)\n\t        self.injected_prompt = injected_prompt\n\t        return injected_prompt\n\t    def rebuild(self) -> str:\n", "        ...\n\tclass Collecter():\n\t    def __init__(self):\n\t        pass\n\t    def add(self, texts: list[Bucket]):\n\t        pass\n\t    def get(self, search_strings: list[str], n_results: int, bucket_key: str = '') -> dict[InjectionPoint, list[dict]]:\n\t        ...\n\t    def get_ids(self, search_strings: list[str], n_results: int, bucket_key: str = '', exclude_ids: list[int] = []) -> dict[InjectionPoint, list[int]]:\n\t        ...\n", "    def get_chunks(self, search_strings: list[str], n_results: int, injection_point: InjectionPoint, exclude_chunks: list[Chunk] = []) -> dict[InjectionPoint, list[Chunk]]:\n\t        ...\n\t    def get_texts(self, search_strings: list[str], n_results: int, bucket_key: str = '', exclude_texts: list[str] = []) -> dict[InjectionPoint, list[str]]:\n\t        ...\n\t    def clear(self):\n\t        pass\n\tclass Chunker():\n\t    def __init__(self, chunk_len: int, first_len: int, last_len: int):\n\t        self.chunk_len = chunk_len\n\t        self.first_len = first_len\n", "        self.last_len = last_len\n\t    def chunk(self, text: str) -> list[Chunk]:\n\t        ...\n\t    def make_chunks(self, text: str) -> Bucket:\n\t        ...\n\t    def get_chunks(self) -> list[Chunk]:\n\t        ...\n\tclass Retriever():\n\t    def __init__(self):\n\t        pass\n", "    def retrieve(self) -> list[str]:\n\t        ...\n\tclass Embedder():\n\t    def __init__(self):\n\t        pass\n\t    def embed(self, text: str) -> list[torch.Tensor]:\n\t        ...\n\tclass Injector():\n\t    def __init__(self, chunker: Chunker, collector: Collecter, embedder: Embedder, sources: dict[str, Source]):\n\t        self.auto_infer_settings = {}\n", "    def add_source(self, injection_point: InjectionPoint, source: Source):\n\t        ...\n\t    def prepare(self, text: str) -> PreparedPrompt:\n\t        ...\n\t    def inject(self, prepared_prompt: PreparedPrompt) -> str:\n\t        ...\n\tclass Focuser():\n\t    def __init__(self):\n\t        pass\n\t    def focus(self, texts: list[str]) -> list[str]:\n", "        ...\n\tclass Searcher():\n\t    def __init__(self, embedder: Embedder | None = None) -> None:\n\t        pass\n\t    def search(self, query: list[str] | list[Embedding], over_input: list[str] | list[Embedding]) -> list[SearchResult]:\n\t        ...\n"]}
{"filename": "src/superbig/__init__.py", "chunked_list": ["from . import chunker, collector, embedder, injector, provider\n\tfrom . import base"]}
{"filename": "src/superbig/chunker/paragraph.py", "chunked_list": ["from ..base import Chunker\n\tclass ParagraphChunker(Chunker):\n\t    \"\"\"\n\t    Paragraph chunker creates chunks by spliting on newlines\n\t    \"\"\""]}
{"filename": "src/superbig/chunker/naive.py", "chunked_list": ["from ..base import Chunker, Chunk\n\tclass NaiveChunker(Chunker):\n\t    def __init__(self, chunk_len: int, first_len: int, last_len: int):\n\t        super().__init__(chunk_len=chunk_len, first_len=first_len, last_len=last_len)\n\t        self.chunks = []\n\t    def chunk(self, text: str) -> list[Chunk]:  \n\t        first_chunk = text[:self.first_len]\n\t        last_chunk = text[-self.last_len:]\n\t        middle_portion = text[self.first_len:-self.last_len]\n\t        middle_chunks = [Chunk(middle_portion[i:i + self.chunk_len]) for i in range(0, len(middle_portion), self.chunk_len)]\n", "        return [Chunk(first_chunk)] + middle_chunks + [Chunk(last_chunk)]\n\t    def make_chunks(self, text: str) -> list[str]:\n\t        self.chunks = self.chunk(text)\n\t        return self.chunks\n\t    def get_chunks(self) -> list[str]:\n\t        return self.chunks"]}
{"filename": "src/superbig/chunker/__init__.py", "chunked_list": ["from .naive import NaiveChunker\n\tfrom .paragraph import ParagraphChunker"]}
{"filename": "src/superbig/metadata/__init__.py", "chunked_list": ["from .builder import MetadataBuilder"]}
{"filename": "src/superbig/metadata/builder.py", "chunked_list": ["from ..base import Source, Chunk\n\tclass MetadataChunk(Chunk):\n\t    def add_meta(self, name: str, value: str):\n\t        if self.meta is None:\n\t            self.meta = {}\n\t        self.meta[name] = value\n\tclass MetadataBuilder():\n\t    \"\"\"\n\t    Given a source, chunk it and add metadata\n\t    Metadata makes it easier to find chunks that are related\n", "    \"\"\"\n\t    def enrich(self, chunk: Chunk) -> MetadataChunk:\n\t        chunk = self.meta_id(chunk)\n\t        return chunk\n\t    def meta_related(self, chunk: Chunk) -> Chunk:\n\t        \"\"\"\n\t        Add metadata of related chunks\n\t        \"\"\"\n\t        pass\n\t    def meta_id(self, chunk: Chunk) -> Chunk:\n", "        \"\"\"\n\t        Add metadata of related chunks\n\t        \"\"\"\n\t        chunk.metadatas['id'] = chunk.id\n\t        return chunk\n\t    def meta_content_type(self, chunk: Chunk) -> Chunk:\n\t        \"\"\"\n\t        Add content type\n\t        \"\"\"\n\t        pass\n"]}
{"filename": "src/superbig/embedder/sentence_transformer.py", "chunked_list": ["from sentence_transformers import SentenceTransformer\n\timport torch\n\tfrom ..base import Embedder, Embedding\n\tclass SentenceTransformerEmbedder(Embedder):\n\t    def __init__(self) -> None:\n\t        self.model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n\t        self.embed = self.model.encode"]}
{"filename": "src/superbig/embedder/__init__.py", "chunked_list": ["from .sentence_transformer import SentenceTransformerEmbedder"]}
{"filename": "src/superbig/collector/custom.py", "chunked_list": ["import posthog\n\tprint('Intercepting all calls to posthog :)')\n\tposthog.capture = lambda *args, **kwargs: None\n\timport chromadb\n\tfrom chromadb.api import Collection\n\tfrom chromadb.config import Settings\n\tfrom ..base import Chunk, Collecter, Embedder, Bucket, InjectionPoint\n\tclass ChromaCollector(Collecter):\n\t    def __init__(self, embedder: Embedder):\n\t        super().__init__()\n", "        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n\t        self.embedder = embedder\n\t        self.collections: dict[str, Collection] = {}\n\t        self.buckets: dict[str, Bucket] = {}\n\t    def rejoin_any_split_chunks(self, bucket_name: str, chunks: list[Chunk]):\n\t        return chunks\n\t    def add(self, buckets: list[Bucket]):\n\t        for bucket in buckets:\n\t            collection = self.chroma_client.create_collection(name=bucket.name, embedding_function=self.embedder.embed)\n\t            self.buckets[bucket.name] = bucket\n", "            collection.add(documents=bucket.documents, embeddings=bucket.embeddings, metadatas=bucket.metadatas, ids=bucket.ids)\n\t            self.collections[bucket.name] = collection\n\t    def get(self, search_strings: list[str], n_results: int, injection_point: InjectionPoint) -> dict[InjectionPoint, list[dict]]:\n\t        results = self.collections[injection_point.real_name].query(query_texts=search_strings, n_results=n_results)\n\t        results = {injection_point: results}\n\t        return results\n\t    def get_ids(self, search_strings: list[str], n_results: int, injection_point: InjectionPoint, exclude_ids: list[int] = []) -> dict[InjectionPoint, list[int]]:\n\t        where_not_in_ids = {\"$and\": [{\"id\": {\"$ne\": id}} for id in exclude_ids]} if len(exclude_ids) > 0 else None\n\t        results = self.collections[injection_point.real_name].query(query_texts=search_strings, n_results=n_results, where=where_not_in_ids)['ids'][0]\n\t        results = [int(result.split('id')[1]) for result in results]\n", "        results = {injection_point: results}\n\t        return results\n\t    def get_chunks(self, search_strings: list[str], n_results: int, injection_point: InjectionPoint, exclude_chunks: list[Chunk] = []) -> dict[InjectionPoint, list[Chunk]]:\n\t        where_not_in_ids = [chunk.id for chunk in exclude_chunks]\n\t        ids = self.get_ids(search_strings, n_results, injection_point, where_not_in_ids)[injection_point]\n\t        corresponding_bucket = self.buckets[injection_point.real_name]\n\t        corresponding_chunks = [corresponding_bucket.chunks[id] for id in ids]\n\t        return {injection_point: corresponding_chunks}\n\t    def get_collection(self, bucket: Bucket):\n\t        return self.collections[bucket.name]\n", "    def clear(self):\n\t        for bucket_name, bucket in self.buckets.items():\n\t            collection = self.collections[bucket_name]\n\t            collection.delete(bucket.ids)"]}
{"filename": "src/superbig/collector/chroma.py", "chunked_list": ["import posthog\n\tprint('Intercepting all calls to posthog :)')\n\tposthog.capture = lambda *args, **kwargs: None\n\timport chromadb\n\tfrom chromadb.api import Collection\n\tfrom chromadb.config import Settings\n\tfrom chromadb.api.types import Where\n\tfrom ..base import Chunk, Collecter, Embedder, Bucket, InjectionPoint\n\tclass ChromaCollector(Collecter):\n\t    def __init__(self, embedder: Embedder):\n", "        super().__init__()\n\t        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n\t        self.embedder = embedder\n\t        self.collections: dict[str, Collection] = {}\n\t        self.buckets: dict[str, Bucket] = {}\n\t    def rejoin_any_split_chunks(self, bucket_name: str, chunks: list[Chunk]):\n\t        return chunks\n\t    def add(self, buckets: list[Bucket]):\n\t        for bucket in buckets:\n\t            if bucket.name in self.buckets:\n", "                self.chroma_client.delete_collection(bucket.name)\n\t            collection = self.chroma_client.create_collection(name=bucket.name, embedding_function=self.embedder.embed)\n\t            self.buckets[bucket.name] = bucket\n\t            collection.add(documents=bucket.documents, embeddings=bucket.embeddings, metadatas=bucket.metadatas, ids=bucket.ids)\n\t            self.collections[bucket.name] = collection\n\t    def get(self, search_strings: list[str], n_results: int, injection_point: InjectionPoint) -> dict[InjectionPoint, list[dict]]:\n\t        num_embeddings = self.collections[injection_point.real_name].count()\n\t        results = self.collections[injection_point.real_name].query(query_texts=search_strings, n_results=min(n_results, num_embeddings))\n\t        results = {injection_point: results}\n\t        return results\n", "    def get_ids(self, search_strings: list[str], n_results: int, injection_point: InjectionPoint, exclude_ids: list[int] = []) -> dict[InjectionPoint, list[int]]:\n\t        num_embeddings = self.collections[injection_point.real_name].count()\n\t        if len(exclude_ids) >= num_embeddings:\n\t            return {}\n\t        where_not_in_ids: Where | None = {\"$and\": [{\"id\": {\"$ne\": id}} for id in exclude_ids]} if len(exclude_ids) > 0 else None\n\t        results = self.collections[injection_point.real_name].query(query_texts=search_strings, n_results=min(n_results, num_embeddings), where=where_not_in_ids)['ids'][0]\n\t        results = [int(result.split('id')[1]) for result in results]\n\t        results = {injection_point: results}\n\t        return results\n\t    def get_chunks(self, search_strings: list[str], n_results: int, injection_point: InjectionPoint, exclude_chunks: list[Chunk] = []) -> dict[InjectionPoint, list[Chunk]]:\n", "        where_not_in_ids = [chunk.id for chunk in exclude_chunks]\n\t        injections_to_ids = self.get_ids(search_strings, n_results, injection_point, where_not_in_ids)\n\t        if injection_point not in injections_to_ids:\n\t            return {}\n\t        ids = injections_to_ids[injection_point]\n\t        corresponding_bucket = self.buckets[injection_point.real_name]\n\t        corresponding_chunks = [corresponding_bucket.chunks[id] for id in ids]\n\t        return {injection_point: corresponding_chunks}\n\t    def get_collection(self, bucket: Bucket):\n\t        return self.collections[bucket.name]\n", "    def clear(self):\n\t        for bucket_name, bucket in self.buckets.items():\n\t            collection = self.collections[bucket_name]\n\t            collection.delete(bucket.ids)"]}
{"filename": "src/superbig/collector/__init__.py", "chunked_list": ["from .chroma import ChromaCollector"]}
{"filename": "src/superbig/injector/generic.py", "chunked_list": ["from typing import Tuple\n\tfrom ..searcher import CosineSimilaritySearcher\n\tfrom ..source import SourceBuilder\n\tfrom ..base import Bucket, Collecter, Chunker, InjectionPoint, Injector, Page, PreparedPrompt, Source, Embedder, Chunk, Window\n\tfrom ..prepared_prompt import AlpacaPreparedPrompt, GenericPreparedPrompt\n\tfrom ..metadata import MetadataBuilder\n\timport re\n\tclass GenericInjector(Injector):\n\t    \"\"\"\n\t    Prepares prompts, chunks data sources, collects them into DBs, and injects them back into prompts\n", "    \"\"\"\n\t    def __init__(self, chunker: Chunker, collector: Collecter, embedder: Embedder, sources: dict[InjectionPoint, Source]):\n\t        self.chunker = chunker\n\t        self.collector = collector\n\t        self.sources = sources\n\t        self.inferred_source_mappings = {}\n\t        self.embedder = embedder\n\t        self.prepared_output = ''\n\t        self.metadata_builder = MetadataBuilder()\n\t        self.source_builder = SourceBuilder()\n", "        self.searcher = CosineSimilaritySearcher()\n\t        self.hollow_injection_points = {}\n\t        self.auto_infer_settings: dict[type, bool] = {}\n\t        self.injection_point_name_to_point: dict[str, InjectionPoint] = {}\n\t    def get_prepared_prompt(self, text: str) -> PreparedPrompt:\n\t        prepared_prompt = self.get_inferred_prompt(text)\n\t        prepared_prompt.from_prompt(text)\n\t        return prepared_prompt\n\t    def get_inferred_prompt(self, text: str) -> PreparedPrompt:\n\t        if(text.find('### Instruction') != 1):\n", "            return AlpacaPreparedPrompt()\n\t        else:\n\t            return GenericPreparedPrompt()\n\t    def add_and_infer_hollow_injection_points(self, hollow_injection_points: list[str]) -> list[InjectionPoint]:\n\t        real_injection_points = []\n\t        for hollow_injection_point in hollow_injection_points:\n\t            if hollow_injection_point not in self.hollow_injection_points:\n\t                real_injection_point = self.add_generic_source(hollow_injection_point)\n\t                if real_injection_point is None:\n\t                    continue\n", "                self.hollow_injection_points[hollow_injection_point] = real_injection_point\n\t            real_injection_points.append(self.hollow_injection_points[hollow_injection_point])\n\t        return real_injection_points\n\t    def add_generic_source(self, text: str) -> InjectionPoint | None:\n\t        source = self.source_builder.from_text(text, self.auto_infer_settings)\n\t        if source is not None:\n\t            source_name = source.metadata.get('inferred_injection_point_name')\n\t            inferred_from = source.metadata.get('inferred_from')\n\t            self.inferred_source_mappings[inferred_from] = source\n\t            injection_point = InjectionPoint(source_name)\n", "            injection_point.target = text\n\t            self.add_source(injection_point, source)\n\t            return injection_point\n\t        return None\n\t    def add_source(self, injection_point: InjectionPoint, source: Source):\n\t        self.injection_point_name_to_point[injection_point.name] = injection_point\n\t        self.sources[injection_point] = source   \n\t    def get_source_from_injection_point(self, injection_point: InjectionPoint) -> Source:\n\t        return self.sources[self.injection_point_name_to_point[injection_point.name]]\n\t    def load_and_cache(self, injection_points: list[InjectionPoint]):\n", "        all_buckets = []\n\t        for injection_point in injection_points:\n\t            real_source = self.get_source_from_injection_point(injection_point)\n\t            if real_source.chunked:\n\t                continue\n\t            print(real_source.name, \" is not chunked. Chunking it now...\")\n\t            loaded_data = real_source.get()\n\t            data_chunks = self.chunker.make_chunks(loaded_data)\n\t            bucket = self.make_bucket(data_chunks, injection_point)\n\t            real_source.chunked = True\n", "            all_buckets.append(bucket)\n\t        print('Adding ', len(all_buckets), ' collections')\n\t        self.collector.add(all_buckets)\n\t    def make_bucket(self, chunks: list[Chunk], injection_point: InjectionPoint):\n\t        ids = []\n\t        embeddings = []\n\t        metadatas = []\n\t        documents = []\n\t        for idx, chunk in enumerate(chunks):\n\t            chunk.embeddings = self.embedder.embed(chunk.text)\n", "            chunk.id = f\"id{idx}\"\n\t            chunk = self.metadata_builder.enrich(chunk)\n\t            ids.append(chunk.id)\n\t            embeddings.append(chunk.embeddings)\n\t            metadatas.append(chunk.metadatas)\n\t            documents.append(chunk.text)\n\t        bucket = Bucket(injection_point.real_name, chunks)\n\t        bucket.ids = ids\n\t        bucket.embeddings = embeddings\n\t        bucket.metadatas = metadatas\n", "        bucket.documents = documents\n\t        return bucket\n\t    def prepare(self, text: str) -> PreparedPrompt:\n\t        print('Preparing prompt...')\n\t        prepared_prompt: PreparedPrompt = self.get_prepared_prompt(text)\n\t        print('Getting injections...')\n\t        injection_points = []\n\t        injection_points += [self.injection_point_name_to_point[name] for name in prepared_prompt.get_injection_points()]\n\t        print(prepared_prompt.get_injection_points())\n\t        hollow_injection_points = self.source_builder.get_hollow_injection_points(prepared_prompt)\n", "        print('Inferring injections...')\n\t        injection_points += self.add_and_infer_hollow_injection_points(hollow_injection_points)\n\t        print('Loading and caching injections...')\n\t        self.load_and_cache(injection_points)\n\t        return prepared_prompt\n\t    def choose_best_source(self, prepared_prompt: PreparedPrompt) -> list[Tuple[InjectionPoint, Source]]:\n\t        source_description_embeddings = [self.embedder.embed(source.metadata.get('description')) for _, source in self.sources.items()]\n\t        search_string_embeddings = [self.embedder.embed(search_string) for search_string in prepared_prompt.get_search_strings()]\n\t        results = self.searcher.search(search_string_embeddings, source_description_embeddings)\n\t        results = [list(self.sources.items())[result.result] for result in results]\n", "        return results\n\t    def parse_injection_levels(self, string: str) -> Tuple[bool, list[int]]:\n\t        parts = string.split(':')\n\t        is_expansion = False\n\t        if \"+\" in parts[1]:\n\t            is_expansion = True\n\t            parts[1] = parts[1].replace('+', '')\n\t        return (is_expansion, [int(parts[1])] * int(parts[0]))\n\t    def get_relevant_context(self, search_strings: list[str], injection_levels: list[int] | str, injection_point: InjectionPoint, additional_information: str):\n\t        optimized_context = []\n", "        previous_relevant_context = search_strings\n\t        search_results_page = Page(additional_information)\n\t        is_expansion = False\n\t        collected_chunks: list[Chunk] = []\n\t        if isinstance(injection_levels, str):\n\t            is_expansion, injection_levels = self.parse_injection_levels(injection_levels)\n\t        for injection_level in injection_levels:\n\t            relevant_chunks = self.collector.get_chunks(previous_relevant_context, injection_level, injection_point, exclude_chunks=collected_chunks)\n\t            if injection_point in relevant_chunks:\n\t                relevant_chunks_for_injection = relevant_chunks[injection_point]\n", "                search_results_page.add_chunks(relevant_chunks_for_injection)\n\t                collected_chunks.extend(relevant_chunks[injection_point])\n\t                relevant_portion = [chunk.text for chunk in relevant_chunks_for_injection]\n\t                optimized_context.append(search_results_page)\n\t                if is_expansion:\n\t                    previous_relevant_context += relevant_portion\n\t                else:\n\t                    previous_relevant_context = relevant_portion\n\t                search_results_page = Page('More information:')\n\t        results_window = Window(optimized_context)\n", "        return {injection_point: results_window}\n\t    def inject(self, prepared_prompt: PreparedPrompt) -> str:\n\t        if len(prepared_prompt.injection_point_names) > 0:\n\t            print('Choosing the best information source...')\n\t            best_source_injection_point, best_source = self.choose_best_source(prepared_prompt)[0]\n\t            print(\"The best source seems to be \", best_source_injection_point.target)\n\t            print(\"Searching...\")\n\t            relevant_context = self.get_relevant_context(prepared_prompt.get_search_strings(), \"10:3+\", best_source_injection_point, best_source.metadata.get('description'))\n\t            for injection_point, data in relevant_context.items():\n\t                prepared_prompt.inject(injection_point, data.view())\n", "            print(\"Injecting...\")\n\t        prompt = prepared_prompt.rebuild()\n\t        return prompt"]}
{"filename": "src/superbig/injector/__init__.py", "chunked_list": ["from .generic import GenericInjector"]}
{"filename": "src/superbig/source/source_selector.py", "chunked_list": ["from ..base import Embedder, Source\n\tclass SourceSelector():\n\t    def __init__(self, embedder: Embedder) -> None:\n\t        self.embedder = embedder\n\t    def find_best_source(self, sources: list[Source], input: str):\n\t        description_embeddings = [self.embedder.embed(source.metadata.attributes.get('description')) for source in sources]\n\t        input_embeddings = self.embedder.embed(input)"]}
{"filename": "src/superbig/source/url_source.py", "chunked_list": ["import requests\n\tfrom ..base import Source\n\tfrom lxml.html.clean import Cleaner\n\timport unicodedata\n\timport hashlib\n\tclass UrlSource(Source):\n\t    def __init__(self, url=''):\n\t        super().__init__()\n\t        if len(url) > 1:\n\t            self.set(url)\n", "    def get(self) -> str:\n\t        data = ''\n\t        if self.contents is not None:\n\t            data = self.contents\n\t        response = requests.get(self.url, headers={\"User-Agent\": \"SuperBIG\"})\n\t        if response.status_code == 200:\n\t            data = self.sanitize(unicodedata.normalize('NFKC', response.text))\n\t            hash = hashlib.md5(data.encode()).hexdigest()\n\t            if self.cache_key != hash:\n\t                self.invalidate(hash)\n", "        else:\n\t            print(\"Couldn't fetch resource\")\n\t            print(response)\n\t        self.contents = data\n\t        return super().get()\n\t    def set(self, url: str):\n\t        self.url = url\n\t        super().set()\n\t    def sanitize(self, dirty_html):\n\t        cleaner = Cleaner(page_structure=True,\n", "                    meta=True,\n\t                    embedded=True,\n\t                    links=True,\n\t                    style=True,\n\t                    processing_instructions=True,\n\t                    inline_style=True,\n\t                    scripts=True,\n\t                    javascript=True,\n\t                    comments=True,\n\t                    frames=True,\n", "                    forms=True,\n\t                    annoying_tags=True,\n\t                    remove_unknown_tags=True,\n\t                    safe_attrs_only=True,\n\t                    safe_attrs=frozenset(['src','color', 'href', 'title', 'class', 'name', 'id']),\n\t                    remove_tags=('span', 'font', 'div', 'a'),\n\t                    kill_tags=['svg', 'img', 'header']\n\t                    )\n\t        clean = str(cleaner.clean_html(dirty_html))\n\t        return clean.replace('\\t', '').replace('\\r','')"]}
{"filename": "src/superbig/source/__init__.py", "chunked_list": ["from .file_source import FileSource\n\tfrom .url_source import UrlSource\n\tfrom .text_source import TextSource\n\tfrom .source_builder import SourceBuilder"]}
{"filename": "src/superbig/source/source_builder.py", "chunked_list": ["import random\n\timport string\n\tfrom ..base import PreparedPrompt, Source\n\tfrom ..source import TextSource, UrlSource\n\tfrom bs4 import BeautifulSoup\n\tfrom urlextract import URLExtract\n\tclass SourceBuilder():\n\t    def __init__(self) -> None:\n\t        pass\n\t    def from_text(self, text: str, infer_settings: dict) -> Source | None:\n", "        return self.infer(text, infer_settings)\n\t    def infer(self, string: str, infer_settings: dict) -> Source | None:\n\t        inferred_source = None\n\t        extractor = URLExtract()\n\t        if extractor.has_urls(string) and UrlSource in infer_settings and infer_settings[UrlSource] == True:\n\t            inferred_source = UrlSource(string)\n\t            soup = BeautifulSoup(inferred_source.get(), features=\"html.parser\")\n\t            metas = soup.find_all('meta')\n\t            descriptions = [meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description']\n\t            # Todo - page title, index page by title semantic search by page name\n", "            # titles = [meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'title']\n\t            injection_point_name = ''\n\t            injection_point_name = self.get_random_short_hash()\n\t            inferred_source.metadata.add('inferred_injection_point_name', injection_point_name)\n\t            inferred_source.metadata.add('descriptions', descriptions)\n\t            inferred_source.metadata.add('description', string)\n\t            inferred_source.metadata.add('inferred_from', string)\n\t            inferred_source.name = injection_point_name\n\t        elif TextSource in infer_settings and infer_settings[TextSource] == True:\n\t            inferred_source = TextSource(string)\n", "        return inferred_source\n\t    def get_hollow_injection_points(self, prepared_prompt: PreparedPrompt) -> list[str]:\n\t        extractor = URLExtract()\n\t        hollow_injection_points = []\n\t        urls = extractor.find_urls(prepared_prompt.source_prompt)\n\t        if len(urls) > 0:\n\t            for url in urls:\n\t                hollow_injection_points.append(url)\n\t        return hollow_injection_points\n\t    def get_random_short_hash(self) -> str:\n", "        alphabet = string.ascii_lowercase + string.digits\n\t        return ''.join(random.choices(alphabet, k=8))\n"]}
{"filename": "src/superbig/source/text_source.py", "chunked_list": ["import requests\n\tfrom ..base import Source\n\tclass TextSource(Source):\n\t    def __init__(self, text=''):\n\t        super().__init__()\n\t        if len(text) > 1:\n\t            self.set(text)\n\t    def get(self) -> str:\n\t        if self.contents is not None:\n\t            return self.contents\n", "        self.contents = self.text\n\t        return super().get()\n\t    def set(self, text: str):\n\t        self.text = text\n\t        super().set()"]}
{"filename": "src/superbig/source/file_source.py", "chunked_list": ["from ..base import Source\n\tclass FileSource(Source):\n\t    def __init__(self):\n\t        super().__init__()\n\t    def get(self):\n\t        if self.contents is not None:\n\t            return self.contents\n\t        with open(self.path) as f:\n\t            self.contents = f.read()\n\t        return super().get()\n", "    def set(self, path: str):\n\t        self.path = path\n\t        super().set()"]}
{"filename": "src/superbig/prepared_prompt/generic.py", "chunked_list": ["from ..base import PreparedPrompt\n\timport re\n\tclass GenericPreparedPrompt(PreparedPrompt):\n\t    \"\"\"\n\t    Format Generic prompts\n\t    \"\"\"\n\t    def from_prompt(self, prompt: str) -> dict:\n\t        pattern = r'\\[\\[\\[.*?\\]\\]\\]'\n\t        prompt_without_injection_points = re.sub(pattern, '', prompt)\n\t        return super().from_prompt(prompt, {\n", "            'prompt': prompt_without_injection_points\n\t        })"]}
{"filename": "src/superbig/prepared_prompt/alpaca.py", "chunked_list": ["from ..base import PreparedPrompt\n\timport re\n\tclass AlpacaPreparedPrompt(PreparedPrompt):\n\t    \"\"\"\n\t    Format Alpaca-style prompts\n\t    \"\"\"\n\t    ALPACA_FORMAT_STRING_INSTRUCTION = '### Instruction:\\n'\n\t    ALPACA_FORMAT_STRING_INPUT = '### Input:\\n'\n\t    ALPACA_FORMAT_STRING_RESPONSE = '### Response:\\n'\n\t    ALPACA_FORMAT_DELIMITER = \"###\"\n", "    def from_prompt(self, prompt: str) -> dict:\n\t        def get_substring_between(source: str, substr1: str, substr2: str):\n\t            match = re.search(fr'({substr1}.+?){substr2}', source, flags=re.DOTALL)\n\t            if match:\n\t                return match.group(1).removeprefix(substr1).removesuffix(substr2)\n\t            return ''\n\t        instruction_piece_idx = prompt.find(self.ALPACA_FORMAT_STRING_INSTRUCTION)\n\t        input_piece_idx = prompt.find(self.ALPACA_FORMAT_STRING_INPUT)\n\t        response_piece_idx = prompt.find(self.ALPACA_FORMAT_STRING_RESPONSE)\n\t        has_instruction: bool = instruction_piece_idx != -1\n", "        has_input: bool = input_piece_idx != -1\n\t        instruction_piece = ''\n\t        input_piece = ''\n\t        response_piece = prompt[response_piece_idx:]\n\t        preprompt_piece = prompt[0:instruction_piece_idx]\n\t        if has_instruction:\n\t            instruction_piece = get_substring_between(prompt,self.ALPACA_FORMAT_STRING_INSTRUCTION,self.ALPACA_FORMAT_DELIMITER)\n\t        if has_input:\n\t            input_piece = get_substring_between(prompt,self.ALPACA_FORMAT_STRING_INPUT,self.ALPACA_FORMAT_DELIMITER)\n\t        return super().from_prompt(prompt, {\n", "            'preprompt': preprompt_piece,\n\t            'instruction': instruction_piece,\n\t            'input': input_piece,\n\t            'response': response_piece,\n\t            'has_instruction': has_instruction,\n\t            'has_input': has_input\n\t        })\n\t    def get_search_strings(self) -> list[str]:\n\t        if len(self.prepared_prompt['input']) > 0:\n\t            return [self.prepared_prompt['input']]\n", "        else:\n\t            return [self.prepared_prompt['instruction']]\n\t    def rebuild(self) -> str:\n\t        rebuild_prompt = self.prepared_prompt\n\t        if self.injected_prompt is not None:\n\t            rebuild_prompt = self.injected_prompt\n\t        final_string = f\"{rebuild_prompt['preprompt']}\"\n\t        if rebuild_prompt['has_instruction']:\n\t            final_string += f\"{self.ALPACA_FORMAT_STRING_INSTRUCTION}{rebuild_prompt['instruction']}\"\n\t        if rebuild_prompt['has_input']:\n", "            final_string += f\"{self.ALPACA_FORMAT_STRING_INPUT}{rebuild_prompt['input']}\"\n\t        final_string += f\"{rebuild_prompt['response']}\"\n\t        return final_string"]}
{"filename": "src/superbig/prepared_prompt/__init__.py", "chunked_list": ["from .alpaca import AlpacaPreparedPrompt\n\tfrom .generic import GenericPreparedPrompt"]}
{"filename": "src/superbig/searcher/__init__.py", "chunked_list": ["from .cosine_similarity import CosineSimilaritySearcher\n\tfrom .asymmetric_similarity import AsymmetricSimilaritySearcher"]}
{"filename": "src/superbig/searcher/asymmetric_similarity.py", "chunked_list": ["from ..base import Embedding, SearchResult, Searcher\n\tclass AsymmetricSimilaritySearcher(Searcher):\n\t    def search(self, query: list[str] | list[Embedding], over_input: list[str] | list[Embedding]) -> list[SearchResult]:\n\t        ..."]}
{"filename": "src/superbig/searcher/cosine_similarity.py", "chunked_list": ["import hnswlib\n\tfrom ..base import Embedding, SearchResult, Searcher\n\t# https://github.com/nmslib/hnswlib\n\t# ef_construction - controls index search speed/build speed tradeoff\n\t#\n\t# M - is tightly connected with internal dimensionality of the data. Strongly affects memory consumption (~M)\n\t# Higher M leads to higher accuracy/run_time at fixed ef/efConstruction\n\tclass CosineSimilaritySearcher(Searcher):\n\t    def search(self, query: list[str] | list[Embedding], over_input: list[str] | list[Embedding], k = 1) -> list[SearchResult]:\n\t        index = hnswlib.Index(space = 'cosine', dim = len(over_input[0]))\n", "        index.init_index(max_elements = len(over_input), ef_construction = 200, M = 8)\n\t        index.add_items(over_input, range(len(over_input)))\n\t        index.set_ef(50)\n\t        labels, _ = index.knn_query(query, k = k)\n\t        results = [SearchResult(int(i)) for i in labels[0]]\n\t        del index\n\t        return results"]}
{"filename": "src/superbig/provider/__init__.py", "chunked_list": ["from .provider import PseudocontextProvider"]}
{"filename": "src/superbig/provider/provider.py", "chunked_list": ["from ..base import Chunker, Embedder, Collecter, InjectionPoint, Retriever, Injector, Source\n\tfrom ..chunker import NaiveChunker\n\tfrom ..embedder import SentenceTransformerEmbedder\n\tfrom ..collector import ChromaCollector\n\tfrom ..injector import GenericInjector\n\tclass PseudocontextProvider():\n\t    def __init__(self,\n\t                 prompt: str = '',\n\t                 collector: Collecter | None = None, \n\t                 chunker: Chunker | None = None, \n", "                 retriever: Retriever | None = None, \n\t                 embedder: Embedder | None = None, \n\t                 injector: Injector | None = None,\n\t                 chunk_len: int = 500, \n\t                 first_len: int = 300, \n\t                 last_len:int = 300):\n\t        self.prompt = prompt\n\t        self.chunker = chunker or NaiveChunker(chunk_len=chunk_len, first_len=first_len, last_len=last_len)\n\t        self.embedder = embedder or SentenceTransformerEmbedder()\n\t        self.collector = collector or ChromaCollector(self.embedder)\n", "        self.injector = injector or GenericInjector(self.chunker, self.collector, self.embedder, {})\n\t    def __enter__(self):\n\t        return self.with_pseudocontext(self.prompt)\n\t    def __exit__(self, type, value, trace):\n\t        pass\n\t    def add_source(self, injection_point_name: str, source: Source):\n\t        self.injector.add_source(InjectionPoint(injection_point_name), source)\n\t    def with_pseudocontext(self, prompt: str, auto_infer_sources={}):\n\t        self.injector.auto_infer_settings = auto_infer_sources\n\t        prepared_prompt = self.injector.prepare(prompt)\n", "        new_prompt = self.injector.inject(prepared_prompt)\n\t        return new_prompt"]}
{"filename": "src/superbig/focuser/__init__.py", "chunked_list": []}
{"filename": "src/superbig/focuser/basic.py", "chunked_list": ["from ..base import Focuser\n\tclass BasicFocuser(Focuser):\n\t    pass"]}
