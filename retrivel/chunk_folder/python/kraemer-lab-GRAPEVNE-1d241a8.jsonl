{"filename": "backend/app_test.py", "chunked_list": ["from app import main\n\tdef test_main():\n\t    assert main() == \"<p>GRAPEVNE flask server is running</p>\"\n"]}
{"filename": "backend/app.py", "chunked_list": ["import base64\n\timport json\n\timport filesystem\n\tfrom flask import Flask\n\tfrom flask import request\n\tfrom flask import Response\n\tfrom flask_cors import CORS\n\tfrom flask_cors import cross_origin\n\timport builder\n\timport runner\n", "app = Flask(__name__)\n\tCORS(app)\n\t@app.route(\"/api/post\", methods=[\"POST\"])\n\t@cross_origin()\n\tdef post():\n\t    \"\"\"Handles POST requests from the frontend.\"\"\"\n\t    try:\n\t        app.logger.debug(f\"POST request: {request.json}\")\n\t        # File system queries\n\t        if request.json[\"query\"] == \"display/folderinfo\":\n", "            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": json.dumps(filesystem.GetFolderItems(request.json[\"data\"])),\n\t            }\n\t        # Builder queries\n\t        elif request.json[\"query\"] == \"builder/compile-to-json\":\n\t            data = request.json[\"data\"]\n\t            js = json.loads(data[\"content\"])\n\t            with open(\"workflow.json\", \"w\") as f:  # dump config file to disk for debug\n\t                json.dump(js, f, indent=4)\n", "            memory_zip, _ = builder.BuildFromJSON(js)\n\t            return Response(\n\t                base64.b64encode(memory_zip),\n\t                mimetype=\"application/zip\",\n\t                headers={\"Content-Disposition\": \"attachment;filename=workflow.zip\"},\n\t            )\n\t        elif request.json[\"query\"] == \"builder/get-remote-modules\":\n\t            data = request.json[\"data\"]\n\t            js = json.loads(data[\"content\"])\n\t            data = {\n", "                \"query\": request.json[\"query\"],\n\t                \"body\": builder.GetModulesList(js[\"url\"]),\n\t            }\n\t        # Runner queries\n\t        elif request.json[\"query\"] == \"runner/build\":\n\t            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": runner.Build(request.json[\"data\"]),\n\t            }\n\t        elif request.json[\"query\"] == \"runner/deleteresults\":\n", "            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": json.dumps(runner.DeleteAllOutput(request.json[\"data\"])),\n\t            }\n\t        elif request.json[\"query\"] == \"runner/lint\":\n\t            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": json.dumps(runner.LintContents(request.json[\"data\"])),\n\t            }\n\t        elif request.json[\"query\"] == \"runner/loadworkflow\":\n", "            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": json.dumps(runner.LoadWorkflow(request.json[\"data\"])),\n\t            }\n\t        elif request.json[\"query\"] == \"runner/tokenize\":\n\t            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": json.dumps(runner.Tokenize(request.json[\"data\"])),\n\t            }\n\t        elif request.json[\"query\"] == \"runner/tokenize_load\":\n", "            try:\n\t                # Try full-tokenize (may fail if dependencies not present)\n\t                body = runner.FullTokenizeFromFile(request.json[\"data\"])\n\t            except BaseException:\n\t                # ...then try in-situ tokenization\n\t                body = runner.TokenizeFromFile(request.json[\"data\"])\n\t            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": json.dumps(body),\n\t            }\n", "        elif request.json[\"query\"] == \"runner/jobstatus\":\n\t            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": json.dumps(runner.TokenizeFromFile(request.json[\"data\"])),\n\t            }\n\t        elif request.json[\"query\"] == \"runner/launch\":\n\t            app.logger.debug(f\"Data: {request.json['data']}\")\n\t            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": json.dumps(runner.Launch(request.json[\"data\"])),\n", "            }\n\t        elif request.json[\"query\"] == \"runner/check-node-dependencies\":\n\t            data = {\n\t                \"query\": request.json[\"query\"],\n\t                \"body\": runner.CheckNodeDependencies(request.json[\"data\"]),\n\t            }\n\t        else:\n\t            return Response(f\"Query not supported: {request.json['query']}\", status=400)\n\t    except ValueError as err:\n\t        # can be raised by runner module (provided with invalid file format)\n", "        app.logger.error(err)\n\t        return Response(err, status=400)\n\t    except TypeError as err:\n\t        app.logger.error(err)\n\t        return Response(\"Server error\", status=500)\n\t    return json.dumps(data)\n\t@app.route(\"/\")\n\tdef main() -> str:\n\t    \"\"\"Returns a simple message to indicate that the server is running.\"\"\"\n\t    return \"<p>GRAPEVNE flask server is running</p>\"\n"]}
{"filename": "backend/filesystem.py", "chunked_list": ["import os\n\timport shutil\n\tdef DeleteResults(data) -> dict:\n\t    \"\"\"Remove the local 'results' folder\"\"\"\n\t    dirname = data[\"content\"]\n\t    dirname = f\"{dirname}/results\"\n\t    shutil.rmtree(dirname)\n\t    return {}\n\tdef GetFolderItems(data) -> dict:\n\t    \"\"\"Get the contents of a folder\"\"\"\n", "    dirname = data[\"content\"]\n\t    dirlist = [\n\t        filename\n\t        for filename in os.listdir(dirname)\n\t        if os.path.isdir(os.path.join(dirname, filename))\n\t    ]\n\t    dirlist = [name for name in dirlist if name[0] != \".\"]\n\t    dirlist.sort()\n\t    isdir = len(dirlist) * [True]\n\t    filelist = [\n", "        filename\n\t        for filename in os.listdir(dirname)\n\t        if not os.path.isdir(os.path.join(dirname, filename))\n\t    ]\n\t    filelist = [name for name in filelist if name[0] != \".\"]\n\t    filelist.sort()\n\t    isdir.extend(len(filelist) * [False])\n\t    contents = [*dirlist, *filelist]\n\t    contents = [{\"name\": name, \"isdir\": isdir} for name, isdir in zip(contents, isdir)]\n\t    js = {\"foldername\": dirname, \"contents\": contents}\n", "    return js\n"]}
{"filename": "builder/setup.py", "chunked_list": ["#!/usr/bin/env python\n\tfrom setuptools import setup\n\tsetup(\n\t    name=\"builder\",\n\t    version=\"0.1\",\n\t    description=\"GRAPEVNE builder\",\n\t    packages=[\"builder\"],\n\t    zip_safe=False,\n\t)\n"]}
{"filename": "builder/builder/__init__.py", "chunked_list": ["from .builder import BuildFromFile  # noqa: F401\n\tfrom .builder import BuildFromJSON  # noqa: F401\n\tfrom .builder import CleanBuildFolder  # noqa: F401\n\tfrom .builder_web import GetModulesList  # noqa: F401\n\tfrom .ImportWorkflow import ImportWorkflowDir  # noqa: F401\n"]}
{"filename": "builder/builder/ImportWorkflow.py", "chunked_list": ["import re\n\timport yaml\n\tfrom builder.builder import Model\n\tfrom builder.builder_web import GetWorkflowFiles\n\tdef ImportWorkflowDir(\n\t    workflow_dir: str,\n\t) -> Model:\n\t    \"\"\"Import linked modules snakemake workflow as Model object\n\t    Args:\n\t        filename: Path to the workflow file\n", "        levels: Number of levels to import (default -1 for all)\n\t    \"\"\"\n\t    workflow_str, config_str = GetWorkflowFiles(f\"'{workflow_dir}'\")\n\t    workflow_config = yaml.safe_load(config_str)\n\t    modules = re.findall(\"^module (.*):\", workflow_str, re.MULTILINE)\n\t    # Build model\n\t    m = Model()\n\t    for rulename in modules:\n\t        # Namespaces are not ordinary parameters\n\t        config = workflow_config[rulename][\"config\"]\n", "        c = config.copy()\n\t        c.pop(\"input_namespace\", None)\n\t        c.pop(\"output_namespace\", None)\n\t        node = m.AddModule(rulename, {\"config\": c})\n\t        # Retain namespace mapping\n\t        node.input_namespace = config.get(\"input_namespace\", node.input_namespace)\n\t        node.output_namespace = config.get(\"output_namespace\", node.output_namespace)\n\t        node.snakefile = workflow_config[rulename].get(\"snakefile\", node.snakefile)\n\t    # Expand modules\n\t    m.ExpandAllModules()\n", "    return m\n\tdef ParseFunctionSignature(signature: str):\n\t    def f(*args, **kwargs):\n\t        return args, kwargs\n\t    name, args = signature.split(\"(\", 1)\n\t    return name, *eval(\"f(\" + args)\n"]}
{"filename": "builder/builder/builder.py", "chunked_list": ["import argparse\n\timport copy\n\timport json\n\timport logging\n\timport os\n\timport pathlib\n\timport re\n\timport shutil\n\timport tempfile\n\tfrom typing import List\n", "from typing import Optional\n\tfrom typing import Tuple\n\tfrom typing import Union\n\timport requests\n\timport yaml\n\tNamespace = Union[str, None, dict]\n\t# Set up logging\n\tlogfile = os.path.expanduser(\"~\") + \"/GRAPEVNE.log\"\n\tlogging.basicConfig(\n\t    filename=logfile,\n", "    encoding=\"utf-8\",\n\t    level=logging.DEBUG,\n\t)\n\tlogging.info(\"Working directory: %s\", os.getcwd())\n\tclass Node:\n\t    \"\"\"Node class for use with the workflow Model\"\"\"\n\t    def __init__(\n\t        self,\n\t        name: str,\n\t        rulename: str,\n", "        nodetype: str,\n\t        snakefile: Union[\n\t            str, dict\n\t        ] = \"\",  # str | {function: str, args: List, kwargs: dict}\n\t        config={},\n\t        input_namespace: Namespace = \"\",\n\t        output_namespace: str = \"\",\n\t    ):\n\t        \"\"\"Initialise a Node object, the parent class for Modules\n\t        Args:\n", "            name (str): Name of the node\n\t            rulename (str): Name of the rule\n\t            nodetype (str): Type of node (module, connector, etc.)\n\t            snakefile (str|dict): str location or dict representing function call\n\t            config (dict): Configuration (parameters) for the Snakefile\n\t            input_namespace (str): Input namespace\n\t            output_namespace (str): Output namespace\n\t        \"\"\"\n\t        self.name = name\n\t        self.rulename = rulename\n", "        self.nodetype = nodetype\n\t        self.snakefile = snakefile\n\t        self.config = config\n\t        self.input_namespace = input_namespace\n\t        self.output_namespace = output_namespace\n\t    def GetOutputNamespace(self) -> str:\n\t        \"\"\"Returns the output namespace\"\"\"\n\t        return self.output_namespace\n\t    def GetInputNamespace(self) -> Namespace:\n\t        \"\"\"Returns the input namespace, which can be a string or dictionary\"\"\"\n", "        return self.input_namespace\n\tclass Module(Node):\n\t    \"\"\"Module class for use with the workflow Model\"\"\"\n\t    def __init__(self, name: str, kwargs: dict):\n\t        \"\"\"Initialise a Module object\n\t        Args:\n\t            name (str): Name of the module\n\t            kwargs: See Node class for kwargs\n\t        \"\"\"\n\t        kwargs[\"nodetype\"] = kwargs.get(\"nodetype\", \"module\")\n", "        kwargs[\"config\"] = kwargs.get(\"config\", {})\n\t        kwargs[\"input_namespace\"] = kwargs[\"config\"].get(\"input_namespace\", None)\n\t        super().__init__(name, **kwargs)\n\t    def _GetConfigFileinfo(self) -> Union[str, dict]:\n\t        \"\"\"Returns the config filename, or an equivalent dict for remote files\"\"\"\n\t        workflow_filename = \"workflow/Snakefile\"\n\t        config_filename = \"config/config.yaml\"\n\t        if isinstance(self.snakefile, str):\n\t            # Local file\n\t            filename = self.snakefile\n", "            filename = filename.replace(workflow_filename, config_filename)\n\t            return filename\n\t        if isinstance(self.snakefile, dict):\n\t            # Remote file\n\t            c = copy.deepcopy(self.snakefile)\n\t            c[\"kwargs\"][\"path\"] = c[\"kwargs\"][\"path\"].replace(\n\t                workflow_filename, config_filename\n\t            )\n\t            return c\n\t        raise ValueError(\"Invalid snakefile type\")\n", "    def _ReadFile(self, fileinfo: Union[str, dict]) -> str:\n\t        \"\"\"Helper function that reads a file, either local or remote\"\"\"\n\t        if isinstance(fileinfo, str):\n\t            # Local file\n\t            with open(fileinfo, \"r\") as file:\n\t                contents = file.read()\n\t            return contents\n\t        if isinstance(fileinfo, dict):\n\t            # Remote file\n\t            if fileinfo.get(\"function\", None) not in [\"github\"]:\n", "                raise ValueError(\n\t                    \"Only github function is currently supported for remote files\"\n\t                )\n\t            url_github: str = \"https://raw.githubusercontent.com\"\n\t            repo = fileinfo[\"args\"][0]\n\t            branch = fileinfo.get(\"kwargs\", {}).get(\"branch\", \"main\")\n\t            path = fileinfo.get(\"kwargs\", {}).get(\"path\", \"\")\n\t            url: str = f\"{url_github}/{repo}/{branch}/{path}\"\n\t            workflow_file = requests.get(url).text\n\t            return workflow_file\n", "        raise ValueError(\"Invalid snakefile type\")\n\t    def ReadWorkflowFile(self):\n\t        return self._ReadFile(self.snakefile)\n\t    def ReadConfigFile(self):\n\t        return yaml.safe_load(self._ReadFile(self._GetConfigFileinfo()))\n\tclass Model:\n\t    \"\"\"Model class for the workflow\"\"\"\n\t    def __init__(self) -> None:\n\t        \"\"\"Initialise the model\"\"\"\n\t        self.nodes: List[Node] = []  # List of Node objects\n", "        self.partial_build: bool = False\n\t    def SetPartialBuild(self, partial_build: bool) -> None:\n\t        \"\"\"Sets the partial build flag (does not throw if a node is missing)\"\"\"\n\t        self.partial_build = partial_build\n\t    def BuildSnakefile(\n\t        self,\n\t        configfile: str = \"config/config.yaml\",\n\t    ) -> str:\n\t        \"\"\"Builds the workflow Snakefile (links modules)\"\"\"\n\t        s = \"\"\n", "        if configfile:\n\t            s = f'configfile: \"{configfile}\"\\n'\n\t        # Build Snakefile\n\t        for node in self.nodes:\n\t            s += \"\\n\"\n\t            s += f\"module {node.rulename}:\\n\"\n\t            s += \"    snakefile:\\n\"\n\t            if isinstance(node.snakefile, str):\n\t                # String denoted a local file\n\t                s += f'        config[\"{node.rulename}\"][\"snakefile\"]\\n'\n", "            else:\n\t                # Dynamic evaluation of function specified in config file\n\t                s += \"        eval(\\n\"\n\t                s += f'            f\\'{{config[\"{node.rulename}\"][\"snakefile\"][\"function\"]}}\\'\\n'\n\t                s += f'            \\'(*config[\"{node.rulename}\"][\"snakefile\"][\"args\"],\\'\\n'\n\t                s += f'            \\'**config[\"{node.rulename}\"][\"snakefile\"][\"kwargs\"])\\'\\n'\n\t                s += \"        )\\n\"\n\t            s += \"    config:\\n\"\n\t            s += f'        config[\"{node.rulename}\"][\"config\"]\\n'\n\t            s += f\"use rule * from {node.rulename} as {node.rulename}_*\\n\"\n", "        return s\n\t    def BuildSnakefileConfig(self) -> str:\n\t        \"\"\"Builds the workflow configuration as YAML\"\"\"\n\t        c = self.ConstructSnakefileConfig()\n\t        return yaml.dump(c)\n\t    def ConstructSnakefileConfig(self) -> dict:\n\t        \"\"\"Builds the workflow configuration as a dictionary\"\"\"\n\t        c: dict = {}\n\t        c[\"input_namespace\"] = self.ExposeOrphanInputs()\n\t        module_output_namespaces = self.ExposeOrphanOutputs()\n", "        # only a single output_namespace is currently supported\n\t        if len(module_output_namespaces) == 0:\n\t            # Model has no orphan outputs, so will form a Terminal module\n\t            # This will most likely need marking in the config somewhere.\n\t            ...\n\t        if len(module_output_namespaces) == 1:\n\t            c[\"output_namespace\"] = module_output_namespaces[0]\n\t        else:\n\t            # Could support multiple output namespaces by automatically adding\n\t            # a convergence module, but this might be better left to the user\n", "            # as a deliberate action.\n\t            # raise ValueError(\"Multiple output namespaces not currently supported. \"\n\t            #                 \"Requested: \", module_output_namespaces)\n\t            print(\n\t                \"Multiple output namespaces not currently supported. Request: \",\n\t                module_output_namespaces,\n\t            )\n\t            print(\"Continuing for debug purposes...\")\n\t        # Add configurations for each module\n\t        for node in self.nodes:\n", "            cnode = node.config.copy()\n\t            # Input namespace\n\t            if node.input_namespace:\n\t                cnode[\"input_namespace\"] = cnode.get(\n\t                    \"input_namespace\", node.input_namespace\n\t                )\n\t                if isinstance(cnode[\"input_namespace\"], dict):\n\t                    if not isinstance(node.input_namespace, dict):\n\t                        node.input_namespace = {}\n\t                    for k, v in cnode[\"input_namespace\"].items():\n", "                        if node.input_namespace.get(k, None):\n\t                            cnode[\"input_namespace\"][k] = node.input_namespace[k]\n\t                        # Don't use 'null' for input namespaces\n\t                        if not cnode[\"input_namespace\"][k]:\n\t                            cnode[\"input_namespace\"][k] = k\n\t                if isinstance(node.input_namespace, str):\n\t                    cnode[\"input_namespace\"] = node.input_namespace\n\t            else:\n\t                cnode[\"input_namespace\"] = None\n\t            # Output namespace\n", "            cnode[\"output_namespace\"] = node.output_namespace\n\t            # Save\n\t            c[node.rulename] = {\n\t                \"name\": node.name,\n\t                \"type\": node.nodetype,\n\t                \"snakefile\": node.snakefile,\n\t                \"config\": cnode,\n\t            }\n\t        return c\n\t    def SaveWorkflow(\n", "        self,\n\t        build_path: str = \"build\",\n\t        clean_build: bool = True,\n\t    ) -> str:\n\t        \"\"\"Saves the workflow to the build directory\"\"\"\n\t        if clean_build:  # Delete build directory before rebuilding\n\t            shutil.rmtree(build_path, ignore_errors=True)\n\t        pathlib.Path(f\"{build_path}/config\").mkdir(parents=True, exist_ok=True)\n\t        pathlib.Path(f\"{build_path}/workflow\").mkdir(parents=True, exist_ok=True)\n\t        with open(f\"{build_path}/workflow/Snakefile\", \"w\") as file:\n", "            file.write(self.BuildSnakefile())\n\t        with open(f\"{build_path}/config/config.yaml\", \"w\") as file:\n\t            file.write(self.BuildSnakefileConfig())\n\t        return build_path\n\t    def WrangleName(self, basename: str, subname: str = \"\") -> str:\n\t        \"\"\"Wrangles a valid and unique rule name\"\"\"\n\t        rulename = self.WrangleRuleName(basename)\n\t        name = f\"{rulename}\"\n\t        if subname:\n\t            name = f\"{name}_{subname}\"\n", "        offset = 1\n\t        wrangledName = name\n\t        while wrangledName in self.WrangledNameList():\n\t            # NOTE: hash is not deterministic across runs\n\t            # wrangledName = f\"{name}_{hash(name + str(offset)) % (2**31)}\"\n\t            wrangledName = f\"{name}_{str(offset)}\"\n\t            offset += 1\n\t        return wrangledName\n\t    def WrangledNameList(self) -> List[str]:\n\t        \"\"\"Returns a list of all wrangled names\"\"\"\n", "        return [n.rulename for n in self.nodes]\n\t    def WrangleRuleName(self, name: str) -> str:\n\t        \"\"\"Wrangles a valid rulename (separate from the human readable name)\"\"\"\n\t        return (\n\t            name.replace(\" \", \"_\")\n\t            .replace(\"/\", \"_\")\n\t            .replace(\".\", \"_\")\n\t            .replace(\"(\", \"\")\n\t            .replace(\")\", \"\")\n\t            .lower()\n", "        )\n\t    def AddModule(self, name: str, module: dict) -> Module:\n\t        print(\"=== Add module\")\n\t        print(name)\n\t        print(module)\n\t        \"\"\"Adds a module to the workflow\"\"\"\n\t        kwargs = module.copy()\n\t        if \"rulename\" not in kwargs:\n\t            kwargs[\"rulename\"] = self.WrangleName(name)\n\t        node = Module(name, kwargs)\n", "        self.nodes.append(node)\n\t        node.output_namespace = node.rulename\n\t        return node\n\t    def AddConnector(self, name, connector) -> None:\n\t        \"\"\"Adds a connection between modules\n\t        Connectors map all inputs to a module. If the first element of\n\t        connector is a string then only a single input_namespace need be\n\t        considered. If the first element is a dictionary then key-value pairs\n\t        represent the input_namespaces / ports, and their associated input\n\t        modules.\n", "        Example: Connect the output of module2 to the single input on module1\n\t            connector = [ \"module2\", \"module1\" ]\n\t        Example: Connect the output of module2 to the named input on module1\n\t            connector = [ {\"input1\": \"module2\"}, \"module1\" ]\n\t        Error behaviour depends on the partial_build flag. If False then an\n\t        error is thrown if a node is not found. If True then the connector is\n\t        ignored and None returned.\n\t        Args:\n\t            name (str): Name of the connector\n\t            connector (list): Connector definition\n", "        \"\"\"\n\t        mapping = connector.get(\"map\", None)\n\t        node_to = self.GetNodeByName(mapping[1])\n\t        if not node_to:\n\t            if self.partial_build:\n\t                return\n\t            raise ValueError(\n\t                \"No matching node found for connector source: \"\n\t                \"Requested '\" + mapping[1] + \"'\"\n\t            )\n", "        if isinstance(mapping[0], dict):\n\t            assert isinstance(\n\t                node_to.input_namespace, dict\n\t            ), \"Connector mapping is a dictionary but the destination node does not have a dictionary input namespace\"\n\t            for k, v in mapping[0].items():\n\t                incoming_node = self.GetNodeByName(v)\n\t                if not incoming_node:\n\t                    if self.partial_build:\n\t                        return\n\t                    raise ValueError(\n", "                        \"No matching node found for connector source: \" + v\n\t                    )\n\t                node_to.input_namespace[k] = incoming_node.output_namespace\n\t        else:\n\t            node_from = self.GetNodeByName(mapping[0])\n\t            if not node_from:\n\t                if self.partial_build:\n\t                    return\n\t                raise ValueError(\n\t                    \"No matching node found for connector destination: \" + mapping[0]\n", "                )\n\t            node_to.input_namespace = node_from.output_namespace\n\t        return None\n\t    def GetNodeByName(self, name: str) -> Optional[Node]:\n\t        \"\"\"Returns a node object by name\"\"\"\n\t        name = name.casefold()\n\t        for node in self.nodes:\n\t            if node.name.casefold() == name:\n\t                return node\n\t        return None\n", "    def GetNodeByRuleName(self, rulename: str) -> Optional[Node]:\n\t        \"\"\"Returns a node object by name\"\"\"\n\t        rulename = rulename.casefold()\n\t        for node in self.nodes:\n\t            if node.rulename == rulename:\n\t                return node\n\t        return None\n\t    def NodeIsTerminus(self, node: Node) -> bool:\n\t        \"\"\"Returns true if the given node is a terminus\"\"\"\n\t        # Check for onward connections from the given node\n", "        for n in self.nodes:\n\t            nodes_in = n.input_namespace\n\t            if isinstance(nodes_in, str):\n\t                nodes_in = {\"in\": nodes_in}\n\t            if isinstance(nodes_in, dict):\n\t                if node.rulename in nodes_in.values():\n\t                    return False\n\t        return True\n\t    def ExposeOrphanInputs(self) -> Namespace:\n\t        \"\"\"Find orphan inputs and return as a valid input_namespace\"\"\"\n", "        module_input_namespace: dict = {}\n\t        all_output_namespaces = self.GetRuleNames()\n\t        for node in self.nodes:\n\t            ref_rulename = node.rulename + \"$\"\n\t            if isinstance(node.input_namespace, str):\n\t                if node.input_namespace not in all_output_namespaces:\n\t                    module_input_namespace[ref_rulename] = node.input_namespace\n\t            elif isinstance(node.input_namespace, dict):\n\t                module_input_namespace[ref_rulename] = {}\n\t                for k, v in node.input_namespace.items():\n", "                    if v not in all_output_namespaces:\n\t                        # namespace should be unique to avoid clashes\n\t                        module_input_namespace[ref_rulename + k] = self.WrangleName(v)\n\t                if not module_input_namespace[ref_rulename]:\n\t                    del module_input_namespace[ref_rulename]\n\t            elif node.input_namespace is None:\n\t                pass\n\t            else:\n\t                raise ValueError(\"Invalid input_namespace type\")\n\t        if len(module_input_namespace) == 0:\n", "            return None\n\t        return module_input_namespace\n\t    def ExposeOrphanInputsList(self) -> List[str]:\n\t        \"\"\"Find orphan inputs and return as a valid input_namespace\"\"\"\n\t        orphans: List[str] = []\n\t        all_output_namespaces = self.GetRuleNames()\n\t        for node in self.nodes:\n\t            if isinstance(node.input_namespace, str):\n\t                if node.input_namespace not in all_output_namespaces:\n\t                    orphans.append(node.rulename)\n", "            elif isinstance(node.input_namespace, dict):\n\t                for k, v in node.input_namespace.items():\n\t                    if v not in all_output_namespaces:\n\t                        # namespace should be unique to avoid clashes\n\t                        orphans.append(v)\n\t            elif node.input_namespace is None:\n\t                # No input_namespace - source node/module\n\t                pass\n\t            else:\n\t                raise ValueError(\"Invalid input_namespace type\")\n", "        return orphans\n\t    def ExposeOrphanOutputs(self) -> List[str]:\n\t        \"\"\"Find orphan output and return as a valid output_namespace\"\"\"\n\t        module_output_namespaces: List[str] = []\n\t        all_input_namespaces = self.GetInputNamespaces()\n\t        for node in self.nodes:\n\t            if node.output_namespace not in all_input_namespaces:\n\t                module_output_namespaces.append(node.rulename)\n\t        return module_output_namespaces\n\t    def ExpandAllModules(self) -> None:\n", "        \"\"\"Expand all modules recursively\"\"\"\n\t        module_list: List[str] = []\n\t        while (modules := self.GetModuleNames()) != module_list:\n\t            module_list = modules\n\t            for rulename in modules:\n\t                self.ExpandModule(rulename)\n\t    def ExpandModule(self, rulename: str):\n\t        \"\"\"Expands a module into its constituent part\"\"\"\n\t        # Identify node\n\t        node = self.GetNodeByRuleName(rulename)\n", "        if not node:\n\t            raise ValueError(\"No matching node found for rulename: \" + rulename)\n\t        if not isinstance(node, Module):\n\t            raise ValueError(\"Node is not a module: \" + rulename)\n\t        # Read module spec (Snakefile, configfile) from source\n\t        workflow_contents = node.ReadWorkflowFile()\n\t        modules_list = re.findall(\"^module (.*):\", workflow_contents, re.MULTILINE)\n\t        config = node.ReadConfigFile()\n\t        # Narrow list of modules to those with valid GRAPEVNE entries in config\n\t        modules_list = [\n", "            m\n\t            for m in modules_list\n\t            if (m in config)  # GRAPEVNE config entry requirements here\n\t        ]\n\t        if not modules_list:\n\t            # No valid modules found, return original node\n\t            return node\n\t        # Keep record of orphan namespaces before expansion\n\t        orphan_inputs_prior = self.ExposeOrphanInputsList()\n\t        orphan_outputs_prior = self.ExposeOrphanOutputs()\n", "        # Add new nodes\n\t        rulemapping = {}\n\t        new_nodes: List[Node] = []\n\t        for n in modules_list:\n\t            new_node = self.AddModule(n, {\"config\": config[n].get(\"config\", {})})\n\t            # Retain namespace mapping\n\t            new_node.input_namespace = config[n][\"config\"].get(\n\t                \"input_namespace\", new_node.input_namespace\n\t            )\n\t            new_node.output_namespace = config[n][\"config\"].get(\n", "                \"output_namespace\", new_node.output_namespace\n\t            )\n\t            new_node.snakefile = config[n].get(\"snakefile\", new_node.snakefile)\n\t            # Record new node and rulename mapping, if different\n\t            new_nodes.append(new_node)\n\t            if n != new_node.rulename:\n\t                rulemapping[n] = new_node.rulename\n\t        print(\n\t            \"Attempting to expand module\",\n\t            node.rulename,\n", "            \" from \",\n\t            modules_list,\n\t            \" into \",\n\t            [n.rulename for n in new_nodes],\n\t        )\n\t        # Ensure namespace consistency between new nodes after rename\n\t        for n in new_nodes:\n\t            # output_namespace\n\t            if n.output_namespace in rulemapping.keys():\n\t                n.output_namespace = rulemapping[n.output_namespace]\n", "            # input_namespace\n\t            if isinstance(n.input_namespace, str):\n\t                if n.input_namespace in rulemapping.keys():\n\t                    n.input_namespace = rulemapping[n.input_namespace]\n\t            elif isinstance(n.input_namespace, dict):\n\t                for k, v in n.input_namespace.items():\n\t                    if k in rulemapping.keys():\n\t                        n.input_namespace[k] = rulemapping[v]\n\t            elif n.input_namespace is None:\n\t                pass\n", "            else:\n\t                raise ValueError(\"Namespace type not recognised\")\n\t        # Find orphan inputs and outputs from new node network\n\t        # Sort to prevent reording of nodes that afffect rule name wrangling\n\t        #  (important for testing)\n\t        new_orphan_inputs = sorted(\n\t            list(set(self.ExposeOrphanInputsList()) - set(orphan_inputs_prior))\n\t        )\n\t        new_orphan_outputs = sorted(\n\t            list(set(self.ExposeOrphanOutputs()) - set(orphan_outputs_prior))\n", "        )\n\t        assert (\n\t            len(new_orphan_outputs) <= 1\n\t        ), \"More than one new orphan output found: \" + str(new_orphan_outputs)\n\t        # Preserve incoming connections to parent node\n\t        if len(new_orphan_inputs) == 0:\n\t            # Now orphan inputs - source module\n\t            node.input_namespace = None\n\t        elif isinstance(node.input_namespace, str):\n\t            orphan_node = self.GetNodeByRuleName(list(new_orphan_inputs)[0])\n", "            if orphan_node:\n\t                orphan_node.input_namespace = node.input_namespace\n\t            else:\n\t                raise ValueError(\n\t                    \"No matching node found for name: \" + list(new_orphan_inputs)[0]\n\t                )\n\t        elif isinstance(node.input_namespace, dict):\n\t            # raise ValueError(\"Input dictionary namespaces not supported yet\")\n\t            print(\"Input dictionary namespaces not supported yet\")\n\t            print(\"Continuing for debug purposes...\")\n", "        elif node.input_namespace is None:\n\t            # Module is a Source and (no incoming connections)\n\t            pass\n\t        else:\n\t            raise ValueError(\"Namespace type not recognised\")\n\t        # Preserve outgoing connections from parent node\n\t        for n in self.nodes:\n\t            if isinstance(n.input_namespace, str):\n\t                if n.input_namespace == node.output_namespace:\n\t                    assert len(new_orphan_outputs) == 1, (\n", "                        \"Expanding node has one input, but \"\n\t                        + str(len(new_orphan_outputs))\n\t                        + \" new orphan outputs found\"\n\t                    )\n\t                    n.input_namespace = list(new_orphan_outputs)[0]\n\t            elif isinstance(n.input_namespace, dict):\n\t                for k, v in n.input_namespace.items():\n\t                    if v == node.output_namespace:\n\t                        n.input_namespace[k] = list(new_orphan_outputs)[0]\n\t            elif n.input_namespace is None:\n", "                pass\n\t            else:\n\t                raise ValueError(\"Namespace type not recognised\")\n\t        # Remove expanded node from model\n\t        self.nodes.remove(node)\n\t        # Return new nodes\n\t        return new_nodes\n\t    def GetModuleNames(self) -> List[str]:\n\t        return [n.rulename for n in self.nodes if isinstance(n, Module)]\n\t    def GetInputNamespaces(self) -> List[str]:\n", "        input_namespaces: List[str] = []\n\t        for n in self.nodes:\n\t            if isinstance(n.input_namespace, str):\n\t                input_namespaces.append(n.input_namespace)\n\t            elif isinstance(n.input_namespace, dict):\n\t                for k, v in n.input_namespace.items():\n\t                    input_namespaces.append(v)\n\t            elif n.input_namespace is None:\n\t                continue\n\t            else:\n", "                raise ValueError(\"Namespace type not recognised\")\n\t        return [name for name in input_namespaces if name]\n\t    def GetRuleNames(self) -> List[str]:\n\t        return [n.rulename for n in self.nodes]\n\t    def LookupRuleName(self, name: str) -> Optional[str]:\n\t        for n in self.nodes:\n\t            if name == n.name:\n\t                return n.rulename\n\t        return None\n\t    def LookupRuleNames(self, names: List[str]) -> List[Optional[str]]:\n", "        \"\"\"Lookup rule name given full name, can take a single string or list\"\"\"\n\t        return [self.LookupRuleName(name) for name in names]\n\tdef YAMLToConfig(content: str) -> str:\n\t    \"\"\"Transcribes YAML to a (Snakemake readable) dictionary config syntax\"\"\"\n\t    yl = yaml.safe_load(content)\n\t    def parse_struct(yl: dict):\n\t        \"\"\"Recursively parses a YAML structure\"\"\"\n\t        c = \"\"\n\t        for key, value in yl.items():\n\t            if isinstance(value, dict):\n", "                vv = parse_struct(value)\n\t                vv = [v for v in vv.split(\"\\n\") if v]\n\t                c += f'[\"{key}\"]={{}}\\n'  # Create empty dict\n\t                c += \"\\n\".join([f'[\"{key}\"]{v}' for v in vv]) + \"\\n\"\n\t            elif isinstance(value, list):\n\t                c += f'[\"{key}\"]=[]\\n'  # Create empty list\n\t                for item in value:\n\t                    c += f'[\"{key}\"].append(\"{item}\")\\n'\n\t                # raise Exception(\"Lists not supported in config\")\n\t            elif not value:\n", "                # Null\n\t                c += f'[\"{key}\"]=\"None\"\\n'\n\t            else:\n\t                # Some primitive type\n\t                c += f'[\"{key}\"]=\"{value}\"\\n'\n\t        return c\n\t    c = parse_struct(yl)\n\t    c = [\"config\" + s for s in c.split(\"\\n\") if s]\n\t    c = \"\\n\".join(c) + \"\\n\"\n\t    c = \"config={}\\n\" + c\n", "    return c\n\tdef BuildFromFile(\n\t    filename: str, **kwargs\n\t) -> Tuple[Union[Tuple[str, str], bytes], Model, str]:\n\t    \"\"\"Builds a workflow from a JSON specification file\"\"\"\n\t    try:\n\t        with open(filename, \"r\") as file:\n\t            config = json.load(file)\n\t    except FileNotFoundError:\n\t        print(f\"Configuration file not found: {filename}\")\n", "        exit(1)\n\t    except json.decoder.JSONDecodeError:\n\t        print(f\"Invalid JSON file: {filename}\")\n\t        exit(1)\n\t    return BuildFromJSON(config, **kwargs)\n\tdef CleanBuildFolder(build_path: str = \"\") -> None:\n\t    \"\"\"Deletes the build folder, if it exists\"\"\"\n\t    if build_path:\n\t        shutil.rmtree(build_path, ignore_errors=True)\n\tdef BuildFromJSON(\n", "    config: dict,\n\t    singlefile: bool = False,\n\t    expand: bool = True,\n\t    build_path: str = \"build\",\n\t    clean_build: bool = True,\n\t    partial_build: bool = False,  # Don't throw an error if node is missing\n\t) -> Tuple[Union[Tuple[str, str], bytes], Model, str]:\n\t    \"\"\"Builds a workflow from a JSON specification\n\t    Returns a tuple of the workflow and the workflow model object.\n\t    With singlefile=True the workflow is a tuple of (config, snakefile) strings\n", "    With singlefile=False the workflow is a (zipped) directory structure.\n\t    \"\"\"\n\t    logging.debug(\"BuildFromJSON\")\n\t    logging.debug(\n\t        f\"{config=}, {singlefile=}, {expand=}, {build_path=}, \"\n\t        f\"{clean_build=}, {partial_build=}\"\n\t    )\n\t    m = Model()\n\t    m.SetPartialBuild(partial_build)\n\t    # Add modules first to ensure all namespaces are defined before connectors\n", "    for item in config:\n\t        if item[\"type\"].casefold() in [\"module\", \"source\", \"terminal\"]:\n\t            logging.debug(\"=== Add module (call)\")\n\t            logging.debug(item)\n\t            m.AddModule(\n\t                item[\"name\"],\n\t                item[\"config\"],\n\t            )\n\t    # Add connectors\n\t    for item in config:\n", "        if item[\"type\"].casefold() in [\"connector\"]:\n\t            m.AddConnector(\n\t                item[\"name\"],\n\t                item[\"config\"],\n\t            )\n\t    if expand:\n\t        logging.debug(\"Expanding modules...\")\n\t        m.ExpandAllModules()\n\t    if singlefile:\n\t        # Return composite string\n", "        logging.debug(\"Returning single file build...\")\n\t        logging.debug(f\"{m.BuildSnakefileConfig()}, {m.BuildSnakefile()}\")\n\t        return ((m.BuildSnakefileConfig(), m.BuildSnakefile())), m, \"\"\n\t    else:\n\t        # Create (zipped) workflow and return as binary object\n\t        logging.debug(\"Creating zip file...\")\n\t        build_path = m.SaveWorkflow(build_path, clean_build)\n\t        zipfilename = tempfile.gettempdir() + \"/build\"\n\t        shutil.make_archive(zipfilename, \"zip\", build_path)\n\t        with open(f\"{zipfilename}.zip\", \"rb\") as file:\n", "            contents = file.read()\n\t        logging.debug(f\"Returning zip file: {zipfilename}.zip\")\n\t        return contents, m, zipfilename + \".zip\"\n\tif __name__ == \"__main__\":\n\t    \"\"\"Builds a workflow given a JSON specification file\"\"\"\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"filename\", help=\"Filename of json configuration\")\n\t    args = parser.parse_args()\n\t    BuildFromFile(args.filename)\n"]}
{"filename": "builder/builder/builder_web.py", "chunked_list": ["from os import listdir\n\tfrom os.path import abspath\n\tfrom os.path import isdir\n\tfrom os.path import join\n\tfrom typing import List\n\tfrom typing import Tuple\n\timport requests\n\timport yaml\n\tdef GetModulesList(url: dict) -> List[dict]:\n\t    \"\"\"Get modules list from url\"\"\"\n", "    if url[\"type\"] == \"github\":\n\t        return GetRemoteModulesGithub(url[\"repo\"], url[\"listing_type\"])\n\t    elif url[\"type\"] == \"local\":\n\t        return GetLocalModules(url[\"repo\"])\n\t    else:\n\t        raise Exception(\"Invalid url type.\")\n\tdef GetLocalModules(path: str) -> List[dict]:\n\t    \"\"\"Get local modules from path\"\"\"\n\t    path_base: str = f\"{path}/workflows\"\n\t    # Get list of local filesystem directories in path\n", "    orgs = sorted([f for f in listdir(path_base) if isdir(join(path_base, f))])\n\t    modules = []\n\t    # First-level (organisation) listing\n\t    for org in orgs:\n\t        org_path = f\"{path_base}/{org}\"\n\t        module_types = reversed(\n\t            sorted([f for f in listdir(org_path) if isdir(join(org_path, f))])\n\t        )\n\t        # Second-level (module type) listing\n\t        for module_type in module_types:\n", "            module_type_path = f\"{org_path}/{module_type}\"\n\t            workflows = sorted(\n\t                [\n\t                    f\n\t                    for f in listdir(module_type_path)\n\t                    if isdir(join(module_type_path, f))\n\t                ]\n\t            )\n\t            # Third-level (module/workflow) listing\n\t            for workflow in workflows:\n", "                url_workflow = (\n\t                    f\"{path_base}/{org}/{module_type}/{workflow}/workflow/Snakefile\"\n\t                )\n\t                config_file = (\n\t                    f\"{path_base}/{org}/{module_type}/{workflow}/config/config.yaml\"\n\t                )\n\t                config = {}\n\t                try:\n\t                    with open(config_file, \"r\") as file:\n\t                        config = yaml.safe_load(file)\n", "                except FileNotFoundError:\n\t                    print(f\"Config file not found - assuming blank: {file}\")\n\t                module_classification = GetModuleClassification(config)\n\t                modules.append(\n\t                    {\n\t                        \"name\": f\"({org}) {FormatName(workflow)}\",\n\t                        # \"type\": module_type[:-1],  # remove plural\n\t                        \"type\": module_classification,\n\t                        \"config\": {\n\t                            \"snakefile\": abspath(url_workflow),\n", "                            \"config\": config,\n\t                        },\n\t                    }\n\t                )\n\t    return modules\n\tdef GetRemoteModulesGithub(\n\t    repo: str, listing_type: str = \"BranchListing\"\n\t) -> List[dict]:\n\t    \"\"\"Get remote modules from url\n\t    Function currently supports workflows by directory listing, but could also\n", "    support listings by branches in the future.\n\t    Args:\n\t        repo: repository string in the format: \"owner/repository\"\n\t        listing_type: type of listing to use (default: \"BranchListing\").\n\t                      Options: \"BranchListing\", \"DirectoryListing\"\n\t    Returns:\n\t        List of modules\n\t    \"\"\"\n\t    if listing_type == \"DirectoryListing\":\n\t        return GetRemoteModulesGithubDirectoryListing(repo)\n", "    elif listing_type == \"BranchListing\":\n\t        return GetRemoteModulesGithubBranchListing(repo)\n\t    else:\n\t        raise Exception(\"Invalid Github listing type.\")\n\tdef GetRemoteModulesGithubDirectoryListing(repo: str) -> List[dict]:\n\t    \"\"\"Get remote modules from url (by directory listing)\n\t    Args:\n\t        repo: repository string in the format: \"owner/repository\"\n\t    Returns:\n\t        List of modules\n", "    \"\"\"\n\t    url_github: str = \"https://api.github.com/repos\"\n\t    url_base: str = f\"{url_github}/{repo}/contents/workflows\"\n\t    r_base = requests.get(url_base)\n\t    if r_base.status_code != 200:\n\t        raise Exception(\"Github API request failed (getting organisation listing).\")\n\t    modules = []\n\t    branch = \"main\"\n\t    # First-level (organisation) listing\n\t    for org in r_base.json():\n", "        if org[\"type\"] != \"dir\":\n\t            continue\n\t        url_org = f\"{url_base}/{org['name']}\"\n\t        print(url_org)\n\t        r_org = requests.get(url_org)\n\t        if r_org.status_code != 200:\n\t            raise Exception(\"Github API request failed (getting module type listing).\")\n\t        # Second-level (module type) listing\n\t        for module_type in reversed(r_org.json()):\n\t            if module_type[\"type\"] != \"dir\":\n", "                continue\n\t            url_workflow = f\"{url_org}/{module_type['name']}\"\n\t            r_workflow = requests.get(url_workflow)\n\t            if r_workflow.status_code != 200:\n\t                raise Exception(\n\t                    \"Github API request failed (getting module/workflow listing).\"\n\t                )\n\t            # Third-level (module/workflow) listing\n\t            for workflow in r_workflow.json():\n\t                if workflow[\"type\"] != \"dir\":\n", "                    continue\n\t                url_workflow = (\n\t                    f\"{repo}/workflows/\"\n\t                    f\"{org['name']}/\"\n\t                    f\"{module_type['name']}/\"\n\t                    f\"{workflow['name']}/workflow/Snakefile\"\n\t                )\n\t                url_config = (\n\t                    f\"https://raw.githubusercontent.com/{repo}/\"\n\t                    f\"{branch}/workflows/\"\n", "                    f\"{org['name']}/\"\n\t                    f\"{module_type['name']}/\"\n\t                    f\"{workflow['name']}/config/config.yaml\"\n\t                )\n\t                r_config = requests.get(url_config)\n\t                if r_config.status_code != 200:\n\t                    raise Exception(\n\t                        \"Github API request failed (getting workflow config file).\"\n\t                    )\n\t                config = yaml.safe_load(r_config.text)\n", "                # Determine module type by config file, rather than directory name\n\t                module_classification = GetModuleClassification(config)\n\t                modules.append(\n\t                    {\n\t                        \"name\": f\"({org['name']}) {FormatName(workflow['name'])}\",\n\t                        # \"type\": module_type[\"name\"][:-1],  # remove plural\n\t                        \"type\": module_classification,\n\t                        \"config\": {\n\t                            \"snakefile\": {\n\t                                \"function\": \"github\",\n", "                                \"args\": [repo],\n\t                                \"kwargs\": {\n\t                                    \"path\": f\"workflows/{org['name']}/\"\n\t                                    f\"{module_type['name']}/\"\n\t                                    f\"{workflow['name']}/\"\n\t                                    \"workflow/Snakefile\",\n\t                                    \"branch\": branch,\n\t                                },\n\t                            },\n\t                            \"config\": config,\n", "                        },\n\t                    }\n\t                )\n\t    return modules\n\tdef GetRemoteModulesGithubBranchListing(repo: str) -> List[dict]:\n\t    \"\"\"Get remote modules from url (by branches)\n\t    Args:\n\t        repo: repository string in the format: \"owner/repository\"\n\t    Returns:\n\t        List of modules\n", "    \"\"\"\n\t    url_github: str = \"https://api.github.com/repos\"\n\t    url_base: str = f\"{url_github}/{repo}/branches\"\n\t    r_base = requests.get(url_base)\n\t    if r_base.status_code != 200:\n\t        raise Exception(\"Github API request failed (getting GitHub branches).\")\n\t    modules = []\n\t    # Branch listing\n\t    module_types = {\"m\": \"module\", \"c\": \"connector\", \"s\": \"source\", \"t\": \"terminal\"}\n\t    for branch in r_base.json():\n", "        if branch[\"name\"] == \"main\":\n\t            continue\n\t        [module_type, module_org_and_name] = branch[\"name\"].split(\"/\", 1)\n\t        [module_org, module_name] = module_org_and_name.split(\"/\", 1)\n\t        if module_type not in module_types:\n\t            raise Exception(\n\t                f\"Invalid module type '{module_type}' in branch '{branch['name']}'.\"\n\t            )\n\t        url_config = (\n\t            f\"https://raw.githubusercontent.com/{repo}/\"\n", "            f\"{branch['name']}/config/config.yaml\"\n\t        )\n\t        r_config = requests.get(url_config)\n\t        if r_config.status_code != 200:\n\t            raise Exception(\"Github API request failed (getting workflow config file).\")\n\t        config = yaml.safe_load(r_config.text)\n\t        module_classification = GetModuleClassification(config)\n\t        modules.append(\n\t            {\n\t                \"name\": branch[\"name\"],\n", "                # \"type\": module_types[module_type],\n\t                \"type\": module_classification,\n\t                \"config\": {\n\t                    \"snakefile\": {\n\t                        \"function\": \"github\",\n\t                        \"args\": [repo],\n\t                        \"kwargs\": {\n\t                            \"path\": \"/workflow/Snakefile\",\n\t                            \"branch\": branch[\"name\"],\n\t                        },\n", "                    },\n\t                    \"config\": config,\n\t                },\n\t            }\n\t        )\n\t    return modules\n\tdef GetModuleClassification(config: dict) -> str:\n\t    \"\"\"Determine the module classification from the config file\n\t    Args:\n\t        config: module config file\n", "    \"\"\"\n\t    # If config is None, then default to module\n\t    if config is None:\n\t        return \"module\"\n\t    # If the input namespace exists and is anything other than None, then it is\n\t    # a module\n\t    if config.get(\"input_namespace\", \"blank\") is None:\n\t        return \"source\"\n\t    return \"module\"\n\tdef GetWorkflowFiles(\n", "    load_command: str,\n\t) -> Tuple[str, str]:\n\t    \"\"\"Read workflow and config files\n\t    Args:\n\t        load_command: command used to load the module in the Snakemake file\n\t    \"\"\"\n\t    # Determine if workflow is local or remote\n\t    if load_command[0] in [\"'\", '\"']:\n\t        return GetWorkflowFilesLocal(load_command)\n\t    else:\n", "        return GetWorkflowFilesRemote(load_command)\n\tdef GetWorkflowFilesLocal(\n\t    load_command: str,\n\t) -> Tuple[str, str]:\n\t    \"\"\"Read workflow and config files from local directory\n\t    Args:\n\t        load_command: command used to load the module in the Snakemake file\n\t    \"\"\"\n\t    # Isolate string containing workflow directory\n\t    workflow_dir = eval(load_command)\n", "    if isinstance(workflow_dir, tuple):  # account for trailing comma\n\t        workflow_dir = workflow_dir[0]\n\t    workflow_file = abspath(join(workflow_dir, \"workflow/Snakefile\"))\n\t    with open(workflow_file, \"r\") as file:\n\t        workflow_str = file.read()\n\t    # Parse workflow for configfile directive\n\t    # Import config file (if specified)\n\t    configfile = \"config/config.yaml\"\n\t    config_file = abspath(join(workflow_dir, configfile))\n\t    with open(config_file, \"r\") as file:\n", "        config_str = file.read()\n\t    return workflow_str, config_str\n\tdef GetWorkflowFilesRemote(\n\t    load_command: str,\n\t) -> Tuple[str, str]:\n\t    \"\"\"Read workflow and config files from remote source\n\t    Args:\n\t        load_command: command used to load the module in the Snakemake file\n\t    \"\"\"\n\t    raise Exception(\n", "        \"Only loading from local sources (load_command contains a string) \"\n\t        \"is currently supported.\"\n\t    )\n\tdef FormatName(name: str) -> str:\n\t    \"\"\"Formats name to (human readable) title case\"\"\"\n\t    return \" \".join([n[0].upper() + n[1:] for n in name.replace(\"_\", \" \").split(\" \")])\n\tif __name__ == \"__main__\":\n\t    \"\"\"Test function using default repository\"\"\"\n\t    print(\n\t        GetModulesList(\n", "            {\n\t                \"type\": \"local\",\n\t                \"listing_type\": \"DirectoryListing\",\n\t                \"path\": \"../../snakeshack\",\n\t            }\n\t        )\n\t    )\n"]}
{"filename": "builder/builder/tests/builder_web_test.py", "chunked_list": ["import unittest.mock\n\tfrom builder.builder_web import GetRemoteModulesGithub\n\t# from builder.builder_web import GetLocalModules\n\t# from builder.builder_web import GetModulesList\n\t# from builder.builder_web import GetRemoteModulesGithubDirectoryListing\n\tclass TestBuilderWeb(unittest.TestCase):\n\t    def test_GetModulesList(self) -> None:\n\t        # url: dict = {}\n\t        # assert GetModulesList(url) == ...\n\t        ...\n", "    def test_GetLocalModules(self) -> None:\n\t        # path: str = \"\"\n\t        # assert GetLocalModules(path) == ...\n\t        ...\n\t    def test_GetRemoteModulesGithub(self) -> None:\n\t        repo: str = \"owner/repo\"\n\t        with self.assertRaises(Exception):\n\t            GetRemoteModulesGithub(repo, \"Not a valid listing type\")\n\t            GetRemoteModulesGithub(repo, \"BranchListing\")\n\t        listing_type = \"DirectoryListing\"\n", "        with unittest.mock.patch(\n\t            \"builder.builder_web.GetRemoteModulesGithubDirectoryListing\",\n\t            return_value=[{\"name\": \"test\"}],\n\t        ) as mock:\n\t            assert GetRemoteModulesGithub(repo, listing_type) == mock.return_value\n\t    def test_GetRemoteModulesGithubDirectoryListing(self) -> None:\n\t        # repo: str = \"\"\n\t        # assert GetRemoteModulesGithubDirectoryListing(repo) == ...\n\t        ...\n\t    def test_GetRemoteModulesGithubBranches(self) -> None:\n", "        # GetRemoteModulesGithubBranches(repo)\n\t        ...\n"]}
{"filename": "builder/builder/tests/test_local.py", "chunked_list": ["# import builder\n\t# filename = \"test_local.json\"\n\t# builder.BuildFromFile(filename)\n"]}
{"filename": "builder/builder/tests/builder_workflow_test.py", "chunked_list": ["from typing import List\n\tfrom typing import Tuple\n\timport pytest\n\timport yaml\n\tfrom builder import builder\n\tworkflow_folder = \"builder/tests/workflow_import_test\"\n\t@pytest.mark.parametrize(\n\t    \"name\",\n\t    [\n\t        (\"connect_source_sleep1input\", \"srcsleep1in\"),\n", "        (\"connect_sleep1input_sleep1input\", \"c2sleep1in\"),\n\t        (\"connect_c2sleep1in_c2sleep1in\", \"c2c2sleep1in\"),\n\t        (\"connect_source_c2sleep1in\", \"srcc2sleep1in\"),\n\t        (\"connect_2source_sleep2inputs\", \"2srcsleep2in\"),\n\t        (\"connect_1source_sleep2inputs\", \"1srcsleep2in\"),\n\t        (\"connect_2source_2sleep2inputs\", \"2src2sleep2in\"),\n\t    ],\n\t)\n\tdef test_Workflow_Imports(name: Tuple[List[str], str]):\n\t    modules, module_out = name\n", "    build, m, _ = builder.BuildFromFile(\n\t        f\"{workflow_folder}/workflow_imports/{modules}.json\",\n\t        singlefile=True,\n\t        expand=True,\n\t    )\n\t    configfile: str = str(build[0])\n\t    snakefile: str = str(build[1])\n\t    # Remove last newline (test files stripped by syntax formatter)\n\t    # snakefile = snakefile[:-1]\n\t    configfileYAML = yaml.safe_load(configfile)\n", "    with open(f\"{workflow_folder}/modules/{module_out}/config/config.yaml\") as file:\n\t        configfileYAML_expected = yaml.safe_load(file.read())\n\t    for key in [\"input_namespace\", \"output_namespace\"]:\n\t        assert configfileYAML[key] == configfileYAML_expected[key]\n\t    with open(f\"{workflow_folder}/modules/{module_out}/workflow/Snakefile\") as file:\n\t        assert str(snakefile) == file.read()\n"]}
{"filename": "builder/builder/tests/builder_test.py", "chunked_list": ["import pytest\n\tfrom builder.builder import Model\n\tfrom builder.builder import YAMLToConfig\n\tdef test_BuildSnakefile():\n\t    m = Model()\n\t    # Add modules\n\t    m.AddModule(\"module1\", {\"snakefile\": \"snakefile1\"})\n\t    m.AddModule(\"module2\", {\"snakefile\": \"snakefile2\"})\n\t    m.AddModule(\"module3\", {\"snakefile\": \"snakefile3\"})\n\t    m.AddConnector(\"conn23\", {\"map\": [\"module2\", \"module3\"]})\n", "    # Verify Snakefile\n\t    target_snakefile = \"\"\"configfile: \"config/config.yaml\"\n\tmodule module1:\n\t    snakefile:\n\t        config[\"module1\"][\"snakefile\"]\n\t    config:\n\t        config[\"module1\"][\"config\"]\n\tuse rule * from module1 as module1_*\n\tmodule module2:\n\t    snakefile:\n", "        config[\"module2\"][\"snakefile\"]\n\t    config:\n\t        config[\"module2\"][\"config\"]\n\tuse rule * from module2 as module2_*\n\tmodule module3:\n\t    snakefile:\n\t        config[\"module3\"][\"snakefile\"]\n\t    config:\n\t        config[\"module3\"][\"config\"]\n\tuse rule * from module3 as module3_*\n", "\"\"\"\n\t    assert m.BuildSnakefile() == target_snakefile\n\tdef test_ConstructSnakefileConfig():\n\t    m = Model()\n\t    m.AddModule(\"module1\", {\"snakefile\": \"snakefile1\", \"config\": {\"param1\": \"value1\"}})\n\t    m.AddModule(\n\t        \"module2\", {\"snakefile\": \"snakefile2\", \"input_namespace\": \"in3\", \"config\": {}}\n\t    )\n\t    m.AddModule(\n\t        \"module3\", {\"snakefile\": \"snakefile2\", \"input_namespace\": \"in3\", \"config\": {}}\n", "    )\n\t    # Namespace connector\n\t    m.AddConnector(\"conn12\", {\"map\": [\"module1\", \"module2\"]})\n\t    m.AddConnector(\"conn23\", {\"map\": [\"module2\", \"module3\"]})\n\t    c = m.ConstructSnakefileConfig()\n\t    # Verify config\n\t    assert c[\"module1\"][\"config\"].get(\"param1\", None) == \"value1\"\n\t    assert (\n\t        c[\"module2\"][\"config\"][\"input_namespace\"]\n\t        == c[\"module1\"][\"config\"][\"output_namespace\"]\n", "    )\n\t    assert (\n\t        c[\"module3\"][\"config\"][\"input_namespace\"]\n\t        == c[\"module2\"][\"config\"][\"output_namespace\"]\n\t    )\n\t@pytest.mark.skip(reason=\"Not implemented\")\n\tdef test_BuildSnakefileConfig():\n\t    # m = Model()\n\t    # m.BuildSnakefileConfig()\n\t    ...\n", "@pytest.mark.skip(reason=\"Not implemented\")\n\tdef test_SaveWorkflow():\n\t    # m = Model()\n\t    # m.SaveWorkflow()\n\t    ...\n\tdef test_WrangleName():\n\t    # WrangleName should produce a unique name each time\n\t    m = Model()\n\t    basename = \"basename\"\n\t    subname = None\n", "    names = []\n\t    for _ in range(3):\n\t        newname = m.WrangleName(basename, subname)\n\t        assert newname not in names\n\t        m.AddModule(newname, {})\n\t    subname = \"subname\"\n\t    for _ in range(3):\n\t        newname = m.WrangleName(basename, subname)\n\t        assert newname not in names\n\t        m.AddModule(newname, {})\n", "def test_WrangledNameList():\n\t    # Produces a list of all namespaces\n\t    m = Model()\n\t    m.AddModule(\"module1\", {})\n\t    m.AddModule(\"module2\", {})\n\t    m.AddModule(\"module3\", {})\n\t    wrangledNameList = m.WrangledNameList()\n\t    assert len(set(wrangledNameList)) == 3\n\tdef test_WrangleRuleName():\n\t    m = Model()\n", "    rulename_in = \"replace/special.and.(remove).brackets/but_not_underscores\"\n\t    rulename_out = \"replace_special_and_remove_brackets_but_not_underscores\"\n\t    assert m.WrangleRuleName(rulename_in) == rulename_out\n\tdef test_AddModule_SingleInputNamespace():\n\t    m = Model()\n\t    name = \"module1\"\n\t    module = {\n\t        \"rulename\": \"module1\",\n\t        \"nodetype\": \"moduletype1\",\n\t        \"snakefile\": \"snakefile1\",\n", "        \"config\": {\n\t            \"input_namespace\": \"input_namespace1\",\n\t            \"output_namespace\": \"output_namespace1\",\n\t            \"param1\": \"value1\",\n\t        },\n\t    }\n\t    m.AddModule(name, module)\n\t    # Verify module assigned correctly\n\t    assert m.nodes[0].name == name\n\t    assert m.nodes[0].nodetype == \"moduletype1\"\n", "    # Verify module attributes assigned correctly\n\t    for key in module:\n\t        # output_namespace is wrangled\n\t        if key not in [\"output_namespace\"]:\n\t            assert getattr(m.nodes[0], key) == module[key]\n\tdef test_AddModule_MultipleInputNamespaces():\n\t    m = Model()\n\t    name = \"module2\"\n\t    module = {\n\t        \"rulename\": \"module2\",\n", "        \"nodetype\": \"moduletype2\",\n\t        \"snakefile\": \"snakefile2\",\n\t        \"config\": {\n\t            \"param2\": \"value2\",\n\t            \"input_namespace\": {\n\t                \"in2a\": \"input_namespace2a\",\n\t                \"in2b\": \"input_namespace2b\",\n\t            },\n\t            \"output_namespace\": \"output_namespace2\",\n\t        },\n", "    }\n\t    m.AddModule(name, module)\n\t    # Verify module assigned correctly\n\t    assert m.nodes[0].name == name\n\t    assert m.nodes[0].nodetype == \"moduletype2\"\n\t    # Verify module attributes assigned correctly\n\t    for key in module:\n\t        # output_namespace is wrangled\n\t        if key not in [\"output_namespace\"]:\n\t            assert getattr(m.nodes[0], key) == module[key]\n", "def test_AddModule_DuplicateName():\n\t    m = Model()\n\t    m.AddModule(\"module\", {})\n\t    m.AddModule(\"module\", {})\n\t    m.AddModule(\"module\", {})\n\t    assert len(m.nodes) == 3\n\t    assert len(set([n.rulename for n in m.nodes])) == 3\n\tdef test_AddConnector_SingleInput():\n\t    # Namespace connector\n\t    m = Model()\n", "    module1 = m.AddModule(\"module1\", {})\n\t    module2 = m.AddModule(\"module2\", {})\n\t    m.AddConnector(\"conn12\", {\"map\": [\"module1\", \"module2\"]})\n\t    # Verify module namespaces connect appropriately\n\t    assert module1.output_namespace == module2.input_namespace\n\tdef test_AddConnector_MultiInput():\n\t    # Namespace connector\n\t    m = Model()\n\t    module1 = m.AddModule(\n\t        \"module1\",\n", "        {\n\t            \"snakefile\": \"snakefile1\",\n\t            \"config\": {\n\t                \"input_namespace\": \"in1\",\n\t                \"output_namespace\": \"out1\",\n\t            },\n\t        },\n\t    )\n\t    module2 = m.AddModule(\n\t        \"module2\",\n", "        {\n\t            \"snakefile\": \"snakefile2\",\n\t            \"config\": {\n\t                \"input_namespace\": {\n\t                    \"in2a\": \"input2_A\",\n\t                    \"in2b\": \"input2_B\",\n\t                },\n\t                \"output_namespace\": \"out2\",\n\t            },\n\t        },\n", "    )\n\t    # Connect the single output from module1 to the first input of module2\n\t    m.AddConnector(\"conn12\", {\"map\": [{\"in2a\": \"module1\"}, \"module2\"]})\n\t    # Verify module namespaces connect appropriately\n\t    assert module1.output_namespace == module2.input_namespace[\"in2a\"]\n\tdef test_GetNodeByName():\n\t    m = Model()\n\t    node = m.AddModule(\"module1\", {})\n\t    assert m.GetNodeByName(\"module1\") == node\n\tdef test_NodeIsTerminus():\n", "    # Terminus nodes have no forward connections\n\t    m = Model()\n\t    node1 = m.AddModule(\"module1\", {})  # Link to node2\n\t    node2 = m.AddModule(\"module2\", {})  # Link to node3\n\t    node3 = m.AddModule(\"module3\", {})  # No links\n\t    node4 = m.AddModule(\"module4\", {})  # Isolated\n\t    m.AddConnector(\"conn12\", {\"map\": [\"module1\", \"module2\"]})\n\t    m.AddConnector(\"conn23\", {\"map\": [\"module2\", \"module3\"]})\n\t    assert m.NodeIsTerminus(node1) is False\n\t    assert m.NodeIsTerminus(node2) is False\n", "    assert m.NodeIsTerminus(node3)\n\t    assert m.NodeIsTerminus(node4)\n\tdef test_YAMLToConfig():\n\t    content = \"\"\"singleton: alone\n\tmodules:\n\t    name1: first\n\t    name2: second\n\t\"\"\"\n\t    target = \"\"\"config={}\n\tconfig[\"singleton\"]=\"alone\"\n", "config[\"modules\"]={}\n\tconfig[\"modules\"][\"name1\"]=\"first\"\n\tconfig[\"modules\"][\"name2\"]=\"second\"\n\t\"\"\"\n\t    assert YAMLToConfig(content) == target\n"]}
{"filename": "builder/builder/tests/ImportWorkflow_test.py", "chunked_list": ["from builder.ImportWorkflow import ImportWorkflowDir\n\tfrom builder.ImportWorkflow import ParseFunctionSignature\n\t\"\"\" Check for github (or other) snakefiles ---- CURRENTLY UNUSED\n\tassert isinstance(m.nodes[1], dict)\n\tassert m.nodes[1].snakefile.get(\"function\", None) == \"calling_fcn\"\n\tassert m.nodes[1].snakefile.get(\"args\", []) == [\"arg1\", \"arg2\"]\n\tassert m.nodes[1].snakefile.get(\"kwargs\", {}) == {\n\t    \"kwarg1\": \"kwval1\",\n\t    \"kwarg2\": \"kwval2\",\n\t}\n", "\"\"\"\n\tdef test_ImportWorkflow_ImportWorkflowDir() -> None:\n\t    m = ImportWorkflowDir(\n\t        \"builder/tests/workflow_import_test/modules/skeleton\",\n\t    )\n\t    assert len(m.nodes) == 3\n\t    assert m.nodes[0].name == \"module1\"\n\t    assert (\n\t        m.nodes[0].snakefile\n\t        == \"builder/tests/workflow_import_test/modules/source/workflow/Snakefile\"\n", "    )\n\t    assert m.nodes[0].output_namespace == \"module1\"\n\t    assert m.nodes[1].name == \"module2\"\n\t    assert (\n\t        m.nodes[1].snakefile\n\t        == \"builder/tests/workflow_import_test/modules/sleep1input/workflow/Snakefile\"\n\t    )\n\t    assert m.nodes[1].input_namespace == \"module1\"\n\t    assert m.nodes[1].output_namespace == \"module2\"\n\t    assert m.nodes[2].name == \"module3\"\n", "    assert (\n\t        m.nodes[2].snakefile\n\t        == \"builder/tests/workflow_import_test/modules/sleep2inputs/workflow/Snakefile\"\n\t    )\n\t    assert m.nodes[2].input_namespace == {\"in1\": \"module1\", \"in2\": \"module2\"}\n\tdef test_ParesFunctionSignature():\n\t    c = 'fcn(\"pos1\", 2, kw1=\"val3\", kw2=4)'\n\t    name, args, kwargs = ParseFunctionSignature(c)\n\t    assert name == \"fcn\"\n\t    assert args == (\"pos1\", 2)\n", "    assert kwargs == {\"kw1\": \"val3\", \"kw2\": 4}\n"]}
{"filename": "runner/setup.py", "chunked_list": ["#!/usr/bin/env python\n\tfrom setuptools import setup\n\tsetup(\n\t    name=\"runner\",\n\t    version=\"0.1\",\n\t    description=\"GRAPEVNE runner\",\n\t    packages=[\"runner\", \"runner.snakemake_runner\"],\n\t    zip_safe=False,\n\t)\n"]}
{"filename": "runner/runner/runner.py", "chunked_list": ["import json\n\tfrom .snakemake_runner import snakefile\n\tdef Build(data: dict) -> str:\n\t    \"\"\"Builds a workflow from a JSON object.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        build_data: str = snakefile.Build(json.loads(data[\"content\"]))\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return build_data\n\tdef DeleteAllOutput(data: dict) -> dict:\n", "    \"\"\"Deletes all output files from a workflow.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        del_data: dict = snakefile.DeleteAllOutput(data[\"content\"])\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return del_data\n\tdef Lint(data: dict) -> dict:\n\t    \"\"\"Lints a workflow given a directory location.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        lint_response: dict = snakefile.LintContents(data[\"content\"])\n", "    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return lint_response\n\tdef LintContents(data: dict) -> dict:\n\t    \"\"\"Lints a workflow given a Snakefile as string.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        build_data: str = snakefile.Build(json.loads(data[\"content\"]))\n\t        lint_response: dict = snakefile.LintContents(build_data)\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n", "    return lint_response\n\tdef Launch(data: dict, **kwargs) -> dict:\n\t    \"\"\"Launches a workflow in a given location.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        launch_response: dict = snakefile.Launch(data[\"content\"], **kwargs)\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return launch_response\n\tdef Launch_cmd(data: dict, *args, **kwargs) -> dict:\n\t    \"\"\"Returns the launch command for a workflow\"\"\"\n", "    if data[\"format\"] == \"Snakefile\":\n\t        snakemake_args = data.get(\"args\", \"\").split(\" \")\n\t        args = *args, *data.get(\"targets\", [])\n\t        cmd, workdir = snakefile.Launch_cmd(\n\t            data[\"content\"],\n\t            *snakemake_args,\n\t            *args,\n\t            **kwargs,\n\t        )\n\t        launch_response: dict = {\"command\": cmd, \"workdir\": workdir}\n", "    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return launch_response\n\tdef Tokenize(data: dict) -> dict:\n\t    \"\"\"Tokenizes a workflow given a Snakefile as string.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        tokenized_data: dict = snakefile.SplitByRulesFileContent(data[\"content\"])\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return tokenized_data\n", "def LoadWorkflow(data: dict) -> dict:\n\t    \"\"\"Tokenizes a workflow and returns rule/module nodes.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        tokenized_data: dict = snakefile.LoadWorkflow(data[\"content\"])\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return tokenized_data\n\tdef TokenizeFromFile(data: dict) -> dict:\n\t    \"\"\"Tokenizes a workflow given a workflow location.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n", "        tokenized_data: dict = snakefile.LoadWorkflow(data[\"content\"])\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return tokenized_data\n\tdef FullTokenizeFromFile(data: dict) -> dict:\n\t    \"\"\"Creates a working copy of the Snakefile and tokenizes.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        tokenized_data: dict = snakefile.FullTokenizeFromFile(data[\"content\"])\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n", "    return tokenized_data\n\tdef CheckNodeDependencies(data: dict) -> dict:\n\t    \"\"\"Checks the dependencies of a node, given the node and first-order inputs.\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        js = json.loads(data[\"content\"])\n\t        snakemake_launcher = data.get(\"backend\", \"\")\n\t        response: dict = snakefile.CheckNodeDependencies(\n\t            js,\n\t            snakemake_launcher,\n\t        )\n", "    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return response\n\tdef SnakemakeRun(data: dict) -> dict:\n\t    \"\"\"Run snakemake using a customised snakemake package import\"\"\"\n\t    if data[\"format\"] == \"Snakefile\":\n\t        capture_output = data[\"content\"].get(\"capture_output\", False)\n\t        stdout, stderr = snakefile.snakemake_run(\n\t            data[\"content\"][\"command\"].split(\" \"),\n\t            data[\"content\"][\"workdir\"],\n", "            capture_output=capture_output,\n\t            snakemake_launcher=data[\"content\"].get(\"backend\", \"\"),\n\t        )\n\t        response = {\n\t            \"stdout\": stdout,\n\t            \"stderr\": stderr,\n\t        }\n\t    else:\n\t        raise ValueError(f\"Format not supported: {data['format']}\")\n\t    return response\n"]}
{"filename": "runner/runner/TokenizeFile.py", "chunked_list": ["import io\n\timport tokenize\n\tfrom typing import List\n\tfrom typing import Tuple\n\tclass TokenizeFile:\n\t    \"\"\"Tokenize file and calculate indentation levels\"\"\"\n\t    def __init__(self, content: str):\n\t        \"\"\"Tokenize file and calculate indentation levels\"\"\"\n\t        self.content: str = \"\"\n\t        self.tokens: List[Tuple] = []\n", "        self.indent_levels: List[int] = []\n\t        self.blocks: List[int] = []\n\t        self.rootblock: List[int] = []\n\t        self.content = content\n\t        file = io.BytesIO(bytes(content, \"utf-8\"))\n\t        self.tokens = list(tokenize.tokenize(file.readline))\n\t        self.CalcIndentLevels()\n\t        self.CalcBlocks()\n\t    def PrintTokens(self, log_fn=print, filter_token=None):\n\t        \"\"\"Pretty print tokens, with optional filter (token number or string)\"\"\"\n", "        for ix, token in enumerate(self.tokens):\n\t            toknum, tokval, start, end, line = token\n\t            match = False\n\t            if not filter_token:\n\t                match = True\n\t            if isinstance(filter_token, int) and toknum == filter_token:\n\t                match = True\n\t            if isinstance(filter_token, str) and tokval == filter_token:\n\t                match = True\n\t            if match:\n", "                log_fn(ix, token)\n\t    def PrintLineInfo(self):\n\t        \"\"\"Print line number, indent level and line\"\"\"\n\t        lineno = 0\n\t        print(\"Block Indent Line\")\n\t        with io.StringIO(self.content) as file:\n\t            for line in file:\n\t                blockno = self.rootblock[lineno + 1]\n\t                indentlevel = self.GetIndentLevelFromLinenumber(lineno + 1)\n\t                print(f\"{blockno} {indentlevel} {line}\", end=\"\")\n", "                lineno = lineno + 1\n\t    def FindTokenSequence(self, search_seq) -> List[int]:\n\t        \"\"\"Find token sequence in longer sequence and return indices\"\"\"\n\t        if not len(search_seq) or not len(self.tokens):\n\t            return []\n\t        searchpos = 0\n\t        foundlist = []\n\t        for ix, token in enumerate(self.tokens):\n\t            toknum, tokval, _, _, _ = token\n\t            searchpos = (\n", "                searchpos + 1 if (toknum, tokval) == search_seq[searchpos] else 0\n\t            )\n\t            # Search term found\n\t            if searchpos == len(search_seq):\n\t                foundlist.append(ix - searchpos + 1)\n\t                searchpos = 0\n\t        return foundlist\n\t    def GetLineNumberFromTokenIndex(self, pos) -> int:\n\t        \"\"\"Return line number from token index\"\"\"\n\t        _, _, start, _, _ = self.tokens[pos]\n", "        row, _ = start\n\t        return row\n\t    def GetIndentLevelFromLinenumber(self, linenumber):\n\t        \"\"\"Return indentation level from line number\"\"\"\n\t        return self.indent_levels[linenumber]\n\t    def GetIndentLevelFromTokenIndex(self, pos):\n\t        \"\"\"Return indentation level from token index\"\"\"\n\t        indent = 0\n\t        for token in self.tokens[: pos - 1]:\n\t            toknum, _, _, _, _ = token\n", "            if toknum == 5:  # 5 = indent\n\t                indent += 1\n\t            if toknum == 6:  # 6 = dedent\n\t                indent -= 1\n\t        return indent\n\t    def GetFirstTokenIndexOfLine(self, linenumber):\n\t        \"\"\"Return first token index of line\"\"\"\n\t        for ix, token in enumerate(self.tokens):\n\t            _, _, start, _, _ = token\n\t            row, col = start\n", "            if row == linenumber:\n\t                return ix\n\t        return None\n\t    def GetContentBetweenTokenIndices(self, pos_from, pos_to):\n\t        \"\"\"Return content between token indices\"\"\"\n\t        tokens = []\n\t        startrow = self.tokens[pos_from][2][0] - 1\n\t        endrow = self.tokens[pos_from][3][0] - 1\n\t        for token in self.tokens[pos_from : pos_to + 1]:\n\t            toknum, tokval, start, end, line = token\n", "            tokens.append(\n\t                tokenize.TokenInfo(\n\t                    toknum,\n\t                    tokval,\n\t                    (start[0] - startrow, start[1]),\n\t                    (end[0] - endrow, end[1]),\n\t                    line,\n\t                )\n\t            )\n\t        return tokenize.untokenize(tokens)\n", "    def GetBlockFromIndex(self, blockno: int):\n\t        \"\"\"Return content of block from block number\"\"\"\n\t        return self.GetContentBetweenLines(\n\t            self.rootblock.index(blockno),\n\t            len(self.rootblock) - self.rootblock[::-1].index(blockno) - 1,\n\t        )\n\t    def GetContentBetweenLines(self, line_from, line_to):\n\t        \"\"\"Return content between line numbers\"\"\"\n\t        content = \"\"\n\t        for line in range(line_from, line_to + 1):\n", "            token = self.tokens[self.GetFirstTokenIndexOfLine(line)]\n\t            toknum, tokval, start, end, line = token\n\t            content += line\n\t        return content\n\t    def GetContentOfIndentBlock(self, linenumber, indentlevel):\n\t        \"\"\"Return content of indent block\"\"\"\n\t        block_list = self.GetBlockList(indentlevel)\n\t        blockno = block_list[linenumber]\n\t        line_from = block_list.index(blockno)\n\t        line_to = len(block_list) - block_list[::-1].index(blockno) - 1\n", "        return self.GetContentBetweenLines(line_from, line_to)\n\t    def GetBlock(self, search_seq, ignore_tokens):\n\t        \"\"\"Return content of block containing search sequence\"\"\"\n\t        pos = self.FindTokenSequence(search_seq)\n\t        if not pos:\n\t            return None\n\t        linenumber = self.GetLineNumberFromTokenIndex(pos[0]) + 1\n\t        indentlevel = self.GetIndentLevelFromLinenumber(linenumber)\n\t        return self.GetContentOfIndentBlock(linenumber, indentlevel)\n\t    def CalcIndentLevels(self):\n", "        \"\"\"Return indentation levels for each line in the file\"\"\"\n\t        blockno = 0\n\t        lastlinenumber = self.tokens[-1][2][0]\n\t        indent_list = [None] * (lastlinenumber + 1)\n\t        rootblock = [None] * (lastlinenumber + 1)\n\t        indent = 0\n\t        lastindent = 1\n\t        for token in self.tokens:\n\t            toknum, _, start, _, _ = token\n\t            row, _ = start\n", "            if toknum == 5:  # 5 = indent\n\t                indent += 1\n\t            if toknum == 6:  # 6 = dedent\n\t                indent -= 1\n\t            # Update indent for line\n\t            indent_list[row] = indent\n\t            if lastindent == 0 and indent > 0:\n\t                blockno = blockno + 1\n\t                rootblock[row - 1] = blockno\n\t            rootblock[row] = blockno\n", "            lastindent = indent\n\t        self.indent_levels = indent_list\n\t        self.rootblock = rootblock\n\t    def GetIndentLevels(self):\n\t        \"\"\"Return indentation levels for each line in the file\"\"\"\n\t        return self.indent_levels\n\t    def CalcBlocks(self):\n\t        \"\"\"Return block membership by number\"\"\"\n\t        self.blocks = self.GetMembershipFromList(self.indent_levels)\n\t    def GetBlockList(self, level=1):\n", "        \"\"\"Return block membership by number, separated at the specified level\"\"\"\n\t        if level:\n\t            blocks = [\n\t                block if block <= level else level for block in self.indent_levels\n\t            ]\n\t        else:\n\t            blocks = self.blocks\n\t        return self.GetMembershipFromList(blocks)\n\t    def GetMembershipFromList(self, blocks):\n\t        \"\"\"Return membership indices indicating which block each line belongs to\"\"\"\n", "        index = 0\n\t        lastblock = 0\n\t        block_membership = [None] * len(blocks)\n\t        for ix, block in enumerate(blocks):\n\t            if block != lastblock:\n\t                index += 1\n\t                lastblock = block\n\t            block_membership[ix] = index\n\t        return block_membership\n"]}
{"filename": "runner/runner/__init__.py", "chunked_list": ["from .runner import Build  # noqa: F401\n\tfrom .runner import CheckNodeDependencies  # noqa: F401\n\tfrom .runner import DeleteAllOutput  # noqa: F401\n\tfrom .runner import FullTokenizeFromFile  # noqa: F401\n\tfrom .runner import Launch  # noqa: F401\n\tfrom .runner import Launch_cmd  # noqa: F401\n\tfrom .runner import Lint  # noqa: F401\n\tfrom .runner import LintContents  # noqa: F401\n\tfrom .runner import LoadWorkflow  # noqa: F401\n\tfrom .runner import SnakemakeRun  # noqa: F401\n", "from .runner import Tokenize  # noqa: F401\n\tfrom .runner import TokenizeFromFile  # noqa: F401\n"]}
{"filename": "runner/runner/TokenizeFile_test.py", "chunked_list": ["import pytest\n\tfrom runner.TokenizeFile import TokenizeFile\n\tcontent = \"\"\"\\\n\trule:\n\t    rule_item\n\tlevel2:\n\t    level3:\n\t        level3_item\n\tinput:\n\t    input_item\n", "output:\n\t    output_item\n\t\"\"\"\n\tdef test_FindTokenSequence():\n\t    tf = TokenizeFile(content)\n\t    search_seq = [(1, \"rule\"), (54, \":\")]\n\t    assert tf.FindTokenSequence(search_seq) == [1]\n\t    search_seq = [(5, \"    \")]  # only finds single indents\n\t    assert tf.FindTokenSequence(search_seq) == [4, 11, 23, 30]\n\tdef test_GetLineNumberFromTokenIndex():\n", "    tf = TokenizeFile(content)\n\t    for ix, token in enumerate(tf.tokens):\n\t        _, _, start, _, _ = token\n\t        row, _ = start\n\t        assert row == tf.GetLineNumberFromTokenIndex(ix)\n\tdef test_GetIndentLevelFromLinenumber():\n\t    tf = TokenizeFile(content)\n\t    expected = [0, 0, 1, 0, 1, 2, 0, 1, 0, 1]\n\t    for linenumber, _ in enumerate(expected):\n\t        assert tf.GetIndentLevelFromLinenumber(linenumber) == expected[linenumber]\n", "def test_GetIndentLevelFromTokenIndex():\n\t    tf = TokenizeFile(content)\n\t    pos = [1]\n\t    expected = [0]\n\t    for ix in range(len(pos)):\n\t        assert tf.GetIndentLevelFromTokenIndex(pos[ix]) == expected[ix]\n\tdef test_GetFirstTokenIndexOfLine():\n\t    tf = TokenizeFile(content)\n\t    expected_list = [0, 1, 4, 7, 11, 15, 18, 23, 26, 30, 33]\n\t    for linenumber, expected in enumerate(expected_list):\n", "        assert tf.GetFirstTokenIndexOfLine(linenumber) == expected\n\tdef test_GetContentBetweenTokensIndices():\n\t    tf = TokenizeFile(content)\n\t    indices = [\n\t        [1, 2],\n\t    ]\n\t    expected = [\n\t        \"rule:\",\n\t    ]\n\t    for ix in range(len(indices)):\n", "        assert (\n\t            tf.GetContentBetweenTokenIndices(indices[ix][0], indices[ix][1])\n\t            == expected[ix]\n\t        )\n\tdef test_GetContentBetweenLines():\n\t    tf = TokenizeFile(content)\n\t    lines = [\n\t        [1, 2],\n\t    ]\n\t    expected = [\n", "        \"rule:\\n    rule_item\\n\",\n\t    ]\n\t    for ix in range(len(lines)):\n\t        assert tf.GetContentBetweenLines(lines[ix][0], lines[ix][1]) == expected[ix]\n\tdef test_GetContentOfIndentBlock():\n\t    tf = TokenizeFile(content)\n\t    linenumber = [4]\n\t    indent = [1]\n\t    expected_list = [\"    level3:\\n        level3_item\\n\"]\n\t    # can introduce zip(..., strict=True) once python 3.10 is the min version\n", "    for line, indent, expected in zip(linenumber, indent, expected_list):\n\t        assert tf.GetContentOfIndentBlock(line, indent) == expected\n\t@pytest.mark.parametrize(\n\t    \"blockno,expected\",\n\t    [\n\t        (1, \"rule:\\n    rule_item\\n\"),\n\t        (2, \"level2:\\n    level3:\\n        level3_item\\n\"),\n\t        (3, \"input:\\n    input_item\\n\"),\n\t        (4, \"output:\\n    output_item\\n\"),\n\t    ],\n", ")\n\tdef test_GetBlockFromNumber(blockno, expected):\n\t    tf = TokenizeFile(content)\n\t    assert tf.GetBlockFromIndex(blockno) == expected\n\tdef test_GetBlock():\n\t    tf = TokenizeFile(content)\n\t    search_seq = [(1, \"level2\"), (54, \":\")]\n\t    ignore_tokens = []\n\t    block = tf.GetBlock(search_seq, ignore_tokens)\n\t    expected = \"    level3:\\n        level3_item\\n\"\n", "    assert block == expected\n\tdef test_GetIndentLevels():\n\t    tf = TokenizeFile(content)\n\t    indent_level = tf.GetIndentLevels()\n\t    # index[0]=encoding; index[-1]=dedent-to-start\n\t    expected_list = [0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0]\n\t    assert indent_level == expected_list\n\tdef test_GetBlockList():\n\t    tf = TokenizeFile(content)\n\t    block_list = tf.GetBlockList(level=1)\n", "    # index[0]=encoding; index[-1]=dedent-to-start\n\t    expected_list = [0, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8]\n\t    assert block_list == expected_list\n\tdef test_GetMembershipFromList():\n\t    tf = TokenizeFile(content)\n\t    indent_levels = [1, 1, 1, 2, 2, 2, 3, 2, 3]\n\t    expected_list = [1, 1, 1, 2, 2, 2, 3, 4, 5]\n\t    member_list = tf.GetMembershipFromList(indent_levels)\n\t    assert member_list == expected_list\n"]}
{"filename": "runner/runner/snakemake_runner_test/snakefile_test.py", "chunked_list": ["import json\n\timport os\n\timport shutil\n\tfrom unittest import mock\n\timport pytest\n\tfrom runner.snakemake_runner.snakefile import Build\n\tfrom runner.snakemake_runner.snakefile import FullTokenizeFromFile\n\tfrom runner.snakemake_runner.snakefile import GetMissingFileDependencies_FromContents\n\tfrom runner.snakemake_runner.snakefile import IsolatedTempFile\n\tfrom runner.snakemake_runner.snakefile import LintContents\n", "from runner.snakemake_runner.snakefile import SplitByRulesFileContent\n\tfrom runner.snakemake_runner.snakefile import SplitByRulesFromFile\n\tsnakemakerunner = shutil.which(\"snakemake\")\n\tdef test_Snakefile_Build():\n\t    rules = {\n\t        \"block\": [\n\t            {\"name\": \"name1\", \"type\": \"type1\", \"content\": \"code1a\\ncode1b\"},\n\t            {\"name\": \"name2\", \"type\": \"type2\", \"content\": \"code2\"},\n\t            {\"name\": \"name3\", \"type\": \"type3\", \"content\": \"\\ncode3\\n\"},\n\t        ]\n", "    }\n\t    expected = \"code1a\\ncode1b\\ncode2\\n\\ncode3\\n\\n\"\n\t    contents = Build(rules)\n\t    assert contents == expected\n\tdef test_Snakefile_Lint():\n\t    # Load test case\n\t    with open(\"runner/snakemake_runner_test/Snakefile\", \"r\") as file:\n\t        contents = file.read()\n\t    lint_return = LintContents(contents)\n\t    with open(\"runner/snakemake_runner_test/Snakefile_lint\", \"r\") as file:\n", "        expected = json.load(file)\n\t    # Linters will differ by their Snakefile filenames:\n\t    for rule in lint_return[\"rules\"]:\n\t        rule[\"for\"][\"snakefile\"] = \"runner/Snakefile\"\n\t    assert lint_return == expected\n\tdef test_Snakefile_SplitByRules():\n\t    # Load test case\n\t    with open(\"runner/snakemake_runner_test/Snakefile\", \"r\") as file:\n\t        contents = file.read()\n\t    # Tokenise and split by rule\n", "    with mock.patch(\n\t        \"runner.snakemake_runner.snakefile.DagLocal\",\n\t        return_value={\"nodes\": [], \"links\": []},\n\t    ):\n\t        result = SplitByRulesFileContent(contents)\n\t    assert isinstance(result, dict)\n\t    assert isinstance(result[\"block\"], list)\n\t    assert len(result[\"block\"]) == 6\n\t    for block in result[\"block\"]:\n\t        assert \"name\" in block\n", "        assert \"content\" in block\n\tdef test_FullTokenizeFromFile():\n\t    filename = os.path.abspath(\"../examples/snakemake-short-tutorial/Snakefile\")\n\t    blocks = FullTokenizeFromFile(filename)\n\t    expected_links = [\n\t        [\"call_variants\", \"all\"],\n\t        [\"plot_quals\", \"all\"],\n\t        [\"in_genome\", \"call_variants\"],\n\t        [\"sort_alignments\", \"call_variants\"],\n\t        [\"sort_alignments\", \"call_variants\"],\n", "        [\"sort_alignments\", \"call_variants\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n\t        [\"in_genome\", \"map_reads\"],\n\t        [\"in_samples\", \"map_reads\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n\t        [\"in_genome\", \"map_reads\"],\n\t        [\"in_samples\", \"map_reads\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n\t        [\"in_genome\", \"map_reads\"],\n\t        [\"in_samples\", \"map_reads\"],\n", "        [\"call_variants\", \"plot_quals\"],\n\t    ]\n\t    assert blocks[\"links\"][\"content\"] == expected_links\n\tdef test_SplitByRulesFromFile():\n\t    filename = os.path.abspath(\"../examples/snakemake-short-tutorial/Snakefile\")\n\t    blocks = SplitByRulesFromFile(filename)\n\t    # Some datafiles are present, so those links are not included in the DAG\n\t    expected_links = [\n\t        [\"call_variants\", \"all\"],\n\t        [\"plot_quals\", \"all\"],\n", "        [\"sort_alignments\", \"call_variants\"],\n\t        [\"sort_alignments\", \"call_variants\"],\n\t        [\"sort_alignments\", \"call_variants\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n\t        [\"call_variants\", \"plot_quals\"],\n\t    ]\n\t    assert blocks[\"links\"][\"content\"] == expected_links\n\tdef test_SplitByRulesFromFile_submodules():\n", "    filename = os.path.abspath(\"../examples/submodules/Snakefile\")\n\t    blocks = SplitByRulesFromFile(filename)\n\t    # Some datafiles are present, so those links are not included in the DAG\n\t    expected_links = [\n\t        [\"rule_6\", \"all\"],\n\t        [\"other_rule_5\", \"rule_6\"],\n\t        [\"other_rule_4\", \"other_rule_5\"],\n\t        [\"other_rule_3\", \"other_rule_4\"],\n\t        [\"other_rule_2\", \"other_rule_3\"],\n\t        [\"rule_1\", \"other_rule_2\"],\n", "    ]\n\t    assert blocks[\"links\"][\"content\"] == expected_links\n\tdef test_SplitByRulesFileContent():\n\t    filename = os.path.abspath(\"../examples/snakemake-short-tutorial/Snakefile\")\n\t    with open(filename, \"r\") as file:\n\t        blocks = SplitByRulesFileContent(file.read())\n\t    expected_links = [\n\t        [\"call_variants\", \"all\"],\n\t        [\"plot_quals\", \"all\"],\n\t        [\"in_genome\", \"call_variants\"],\n", "        [\"sort_alignments\", \"call_variants\"],\n\t        [\"sort_alignments\", \"call_variants\"],\n\t        [\"sort_alignments\", \"call_variants\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n\t        [\"in_genome\", \"map_reads\"],\n\t        [\"in_samples\", \"map_reads\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n\t        [\"in_genome\", \"map_reads\"],\n\t        [\"in_samples\", \"map_reads\"],\n\t        [\"map_reads\", \"sort_alignments\"],\n", "        [\"in_genome\", \"map_reads\"],\n\t        [\"in_samples\", \"map_reads\"],\n\t        [\"call_variants\", \"plot_quals\"],\n\t    ]\n\t    assert blocks[\"links\"][\"content\"] == expected_links\n\tdef test_IsolatedTempFile():\n\t    contents = \"Sample contents\"\n\t    with IsolatedTempFile(contents) as temp_filename:\n\t        filename = temp_filename\n\t        assert os.path.exists(filename)\n", "        with open(filename, \"r\") as infile:\n\t            assert infile.read() == contents\n\t    assert not os.path.exists(filename)\n\t@pytest.mark.skipif(not snakemakerunner, reason=\"Snakemake not installed\")\n\tdef test_GetMissingFileDependencies() -> None:\n\t    # Provide a list of files with no associated inputs, then mark them as\n\t    # outputs of a later rule one at a time; lists should correspond\n\t    target = [\"a.txt\", \"b.txt\", \"c.txt\"]\n\t    test_snakefile: str = \"\"\"\n\trule all:\n", "    input:\n\t\"\"\"\n\t    for trgt in target:\n\t        test_snakefile += f'        \"{trgt}\",\\n'\n\t    deps = GetMissingFileDependencies_FromContents(test_snakefile)\n\t    assert len(deps) == len(target)\n\t    assert set(deps) == set(target)\n\t    test_snakefile += \"\"\"\n\trule missing_targets:\n\t    output:\n", "\"\"\"\n\t    # Recurse through, emptying list\n\t    for _ in range(len(target)):\n\t        test_snakefile += f'        \"{target[0]}\",\\n'\n\t        target.pop(0)\n\t        deps = GetMissingFileDependencies_FromContents(test_snakefile)\n\t        assert len(deps) == len(target)\n\t        assert set(deps) == set(target)\n\t    assert len(target) == 0\n\tdef test_GetMissingFileDependencies_FullReturn() -> None:\n", "    # Returns all dependencies\n\t    test_snakefile: str = \"\"\n\t    with mock.patch(\n\t        \"runner.snakemake_runner.snakefile.GetMissingFileDependencies_FromFile\",\n\t        side_effect=[[\"a.txt\"], [\"b.txt\"], [\"c.txt\"], []],\n\t    ):\n\t        depsAll = GetMissingFileDependencies_FromContents(test_snakefile)\n\t        assert depsAll == [\"a.txt\", \"b.txt\", \"c.txt\"]\n\tdef test_GetMissingFileDependencies_TruncatedReturn() -> None:\n\t    # Returns only dependencies up to target namespace\n", "    test_snakefile: str = \"\"\n\t    with mock.patch(\n\t        \"runner.snakemake_runner.snakefile.GetMissingFileDependencies_FromFile\",\n\t        side_effect=[[\"a.txt\"], [\"b.txt\"], [\"c.txt\"], []],\n\t    ):\n\t        depsTruncated = GetMissingFileDependencies_FromContents(\n\t            test_snakefile, target_namespaces=[\"a.txt\"]\n\t        )\n\t        assert depsTruncated == [\"a.txt\"]\n"]}
{"filename": "runner/runner/snakemake_runner/snakefile.py", "chunked_list": ["import contextlib\n\timport io\n\timport json\n\timport logging\n\timport os\n\timport platform\n\timport shutil\n\timport subprocess\n\timport tempfile\n\tfrom contextlib import redirect_stderr\n", "from contextlib import redirect_stdout\n\tfrom pathlib import Path\n\tfrom typing import Dict\n\tfrom typing import List\n\tfrom typing import Optional\n\tfrom typing import Tuple\n\tfrom typing import Union\n\timport snakemake.remote.HTTP\n\timport snakemake.remote.S3\n\tfrom snakemake import main as snakemake_main\n", "from snakemake.remote import AUTO  # noqa: F401\n\tfrom snakemake.remote import AutoRemoteProvider\n\tfrom builder.builder import BuildFromJSON\n\tfrom builder.builder import YAMLToConfig\n\tfrom runner.TokenizeFile import TokenizeFile\n\t# ##############################################################################\n\t# Log file\n\t# ##############################################################################\n\tlogfile = os.path.expanduser(\"~\") + \"/GRAPEVNE.log\"\n\t# Set up logging\n", "logging.basicConfig(\n\t    filename=logfile,\n\t    encoding=\"utf-8\",\n\t    level=logging.DEBUG,\n\t)\n\tlogging.info(\"Working directory: %s\", os.getcwd())\n\t# ##############################################################################\n\t# Snakemake customizations\n\t# ##############################################################################\n\t# Override snakemake's 'AUTO' protocol mapper (this replaces the normal dynamic\n", "# import behaviour which is not supported when building PyInstaller binaries)\n\t@property  # type: ignore\n\tdef protocol_mapping(self):\n\t    provider_list = [\n\t        snakemake.remote.HTTP.RemoteProvider,\n\t        snakemake.remote.S3.RemoteProvider,\n\t    ]\n\t    # assemble scheme mapping\n\t    protocol_dict = {}\n\t    for Provider in provider_list:\n", "        try:\n\t            for protocol in Provider().available_protocols:\n\t                protocol_short = protocol[:-3]  # remove \"://\" suffix\n\t                protocol_dict[protocol_short] = Provider\n\t        except Exception as e:\n\t            # If for any reason Provider() fails (e.g. missing python\n\t            # packages or config env vars), skip this provider.\n\t            print(f\"Instantiating {Provider.__class__.__name__} failed: {e}\")\n\t    return protocol_dict\n\t# Method replacement in snakemake.remote package\n", "AutoRemoteProvider.protocol_mapping = protocol_mapping\n\t# ##############################################################################\n\t# Queries\n\t# ##############################################################################\n\tdef Build(data: dict) -> str:\n\t    \"\"\"Build Snakefile from a dictionary of 'block' elements\"\"\"\n\t    contents: str = \"\"\n\t    for block in data[\"block\"]:\n\t        contents += block[\"content\"] + \"\\n\"\n\t    return contents\n", "def DeleteAllOutput(filename: str) -> dict:\n\t    \"\"\"Ask snakemake to remove all output files\"\"\"\n\t    filename, workdir = GetFileAndWorkingDirectory(filename)\n\t    stdout, stderr = snakemake_launch(\n\t        filename,\n\t        \"--delete-all-output\",\n\t        workdir=workdir,\n\t    )\n\t    return {\n\t        \"status\": \"ok\" if not stderr else \"error\",\n", "        \"content\": {\"stdout\": stdout, \"stderr\": stderr},\n\t    }\n\tdef Launch_cmd(filename: str, *args, **kwargs) -> Tuple[List[str], str]:\n\t    \"\"\"Return the snakemake launch command and working directory\"\"\"\n\t    filename, workdir = GetFileAndWorkingDirectory(filename)\n\t    return snakemake_cmd(\n\t        filename,\n\t        workdir=workdir,\n\t        *args,\n\t        **kwargs,\n", "    )\n\tdef Launch(filename: str, *args, **kwargs) -> dict:\n\t    \"\"\"Launch snakemake workflow given a [locally accessible] location\"\"\"\n\t    cmd, workdir = Launch_cmd(filename, *args, **kwargs)\n\t    stdout, stderr = snakemake_run(cmd, workdir)\n\t    return {\n\t        \"status\": \"ok\" if not stderr else \"error\",\n\t        \"content\": {\"stdout\": stdout, \"stderr\": stderr},\n\t    }\n\tdef Lint(snakefile: str) -> dict:\n", "    \"\"\"Lint a Snakefile using the snakemake library, returns JSON\"\"\"\n\t    try:\n\t        stdout, stderr = snakemake_launch(snakefile, \"--lint\", \"json\")\n\t    except BaseException as e:\n\t        return {\"error\": str(e)}\n\t    # strip first and last lines as needed (snakemake returns)\n\t    sl = stdout.split(\"\\n\")\n\t    if sl[0] in {\"True\", \"False\"}:\n\t        sl = sl[1:]\n\t    if sl[-1] in {\"True\", \"False\"}:\n", "        sl = sl[:-1]\n\t    return json.loads(\"\\n\".join(sl))\n\tdef LintContents(content: str, tempdir: Optional[str] = None) -> dict:\n\t    \"\"\"Lint Snakefile contents using the snakemake library, returns JSON\"\"\"\n\t    with IsolatedTempFile(content) as snakefile:\n\t        lint_return = Lint(snakefile)\n\t    return lint_return\n\tdef LoadWorkflow(filename: str) -> dict:\n\t    \"\"\"Load workflow from provided folder\n\t    Valid Snakefile locations:\n", "        ./Snakefile\n\t        ./workflow/Snakefile\n\t    \"\"\"\n\t    filename, workdir = GetFileAndWorkingDirectory(filename)\n\t    return SplitByDagFromFile(filename, workdir)\n\tdef FullTokenizeFromFile(filename: str) -> dict:\n\t    \"\"\"Copy Snakefile contents to a new temporary file and analyze in isolation\"\"\"\n\t    with open(filename, \"r\") as infile:\n\t        with IsolatedTempFile(infile.read()) as tempfile:\n\t            return SplitByRulesFromFile(tempfile)\n", "def SplitByRulesFromFile(filename: str, workdir: str = \"\") -> dict:\n\t    \"\"\"\n\t    Tokenize snakefile, split by 'rule' segments\n\t    This function is conceptually similar to SplitByRulesFile, but takes the\n\t    full filename of the Snakefile, which requires the file to be accessible by\n\t    the local filesystem, allowing the runner to interrogate the originating\n\t    folder for data files. This permits construction of directed acyclic graphs\n\t    (DAGs)\n\t    \"\"\"\n\t    fullfilename = os.path.abspath(filename)\n", "    return SplitByIndent(fullfilename, workdir, get_dag=True)\n\tdef SplitByDagFromFile(filename: str, workdir: str = \"\") -> dict:\n\t    \"\"\"Tokenize input using rules derived from dag graph\"\"\"\n\t    print(\"here\")\n\t    # Query snakemake for DAG of graph (used for connections)\n\t    dag = DagLocal(os.path.abspath(filename), workdir)\n\t    # Get rules text from origin Snakefile\n\t    blocks = SplitByIndent(filename, workdir, get_dag=False)\n\t    # Build JSON representation\n\t    rules: Dict = {\"block\": []}\n", "    for node in dag[\"nodes\"]:\n\t        # construct dictionary for block and add to list\n\t        block = {\n\t            \"id\": node[\"id\"],\n\t            \"name\": node[\"value\"][\"rule\"],\n\t            \"type\": \"rule\",\n\t            \"content\": \"(module)\",\n\t        }\n\t        # cross-reference with block runner\n\t        for b in blocks[\"block\"]:\n", "            if b[\"name\"] == block[\"name\"]:\n\t                block[\"content\"] = b[\"content\"]\n\t        rules[\"block\"].append(block)\n\t    # include config nodes\n\t    for b in blocks[\"block\"]:\n\t        if b[\"type\"] == \"config\" or b[\"type\"] == \"module\":\n\t            rules[\"block\"].append(\n\t                {\n\t                    \"id\": len(rules[\"block\"]),\n\t                    \"name\": b[\"name\"],\n", "                    \"type\": b[\"type\"],\n\t                    \"content\": b[\"content\"],\n\t                }\n\t            )\n\t    # Return links, as determined by snakemake DAG\n\t    links = []\n\t    for link in dag[\"links\"]:\n\t        try:\n\t            links.append(\n\t                [\n", "                    GetRuleFromID(rules[\"block\"], link[\"u\"]),\n\t                    GetRuleFromID(rules[\"block\"], link[\"v\"]),\n\t                ]\n\t            )\n\t        except KeyError:\n\t            pass\n\t    rules[\"links\"] = {\n\t        \"id\": -1,\n\t        \"name\": \"links\",\n\t        \"type\": \"links\",\n", "        \"content\": links,\n\t    }\n\t    return rules\n\tdef GetRuleFromID(blocks: List[dict], id: int) -> str:\n\t    \"\"\"Get rule name from block ID\"\"\"\n\t    for block in blocks:\n\t        if block[\"id\"] == id:\n\t            return block[\"name\"]\n\t    return \"\"\n\tdef SplitByRulesFileContent(content: str) -> dict:\n", "    \"\"\"Tokenize Snakefile, split into 'rules', return as dict list of rules\"\"\"\n\t    with IsolatedTempFile(content) as snakefile:\n\t        rules = SplitByRulesFromFile(snakefile)\n\t    return rules\n\tdef SplitByIndent(filename: str, workdir: str = \"\", get_dag: bool = False):\n\t    \"\"\"Tokenize input, splitting chunks by indentation level\"\"\"\n\t    # Tokenize into blocks by indentation level\n\t    with open(filename, \"r\") as file:\n\t        contents = file.read()\n\t    tf = TokenizeFile(contents)\n", "    blockcount = max(tf.rootblock)\n\t    # Query snakemake for DAG of graph (used for connections)\n\t    if get_dag:\n\t        dag = DagLocal(os.path.abspath(file.name), workdir)\n\t    else:\n\t        dag = {\"nodes\": [], \"links\": []}\n\t    # Build JSON representation\n\t    rules: Dict = {\"block\": []}\n\t    for block_index in range(blockcount + 1):\n\t        content = tf.GetBlockFromIndex(block_index)\n", "        words = content.split()\n\t        if not words:\n\t            continue\n\t        if words[0] == \"rule\":\n\t            blocktype = \"rule\"\n\t            name = words[1].replace(\":\", \"\")\n\t        elif words[0] == \"module\":\n\t            blocktype = \"module\"\n\t            name = words[1].replace(\":\", \"\")\n\t        else:\n", "            blocktype = \"config\"\n\t            name = \"config\"\n\t        # construct dictionary for block and add to list\n\t        block = {\n\t            \"id\": block_index,\n\t            \"name\": name,\n\t            \"type\": blocktype,\n\t            \"content\": content,\n\t        }\n\t        rules[\"block\"].append(block)\n", "    # Return links, as determined by snakemake DAG\n\t    links = []\n\t    dagnodes = [\n\t        {\"id\": d[\"value\"][\"jobid\"], \"name\": d[\"value\"][\"rule\"]} for d in dag[\"nodes\"]\n\t    ]\n\t    for link in dag[\"links\"]:\n\t        try:\n\t            links.append(\n\t                [GetRuleFromID(dagnodes, link[\"u\"]), GetRuleFromID(dagnodes, link[\"v\"])]\n\t            )\n", "        except KeyError:\n\t            pass\n\t    rules[\"links\"] = {\n\t        \"id\": -1,\n\t        \"name\": \"links\",\n\t        \"type\": \"links\",\n\t        \"content\": links,\n\t    }\n\t    return rules\n\tdef CheckNodeDependencies(jsDeps: dict, snakemake_launcher: str = \"\") -> dict:\n", "    \"\"\"Check if all dependencies are resolved for a given node\"\"\"\n\t    # Build model from JSON (for dependency analysis)\n\t    build, model, _ = BuildFromJSON(jsDeps, singlefile=True, partial_build=True)\n\t    # Determine input namespaces for target node\n\t    input_namespaces = model.ConstructSnakefileConfig()[\n\t        model.GetNodeByName(model.nodes[0].name).rulename  # first (target) node\n\t    ][\"config\"].get(\"input_namespace\", {})\n\t    if isinstance(input_namespaces, str):\n\t        input_namespaces = {\"In\": input_namespaces}\n\t    if not input_namespaces:\n", "        input_namespaces = {\"In\": \"In\"}\n\t    input_namespaces = set(input_namespaces.values())\n\t    # Determine unresolved dependencies (and their source namespaces)\n\t    target_namespaces = set([f\"results/{n}\" for n in input_namespaces])\n\t    missing_deps = set(\n\t        GetMissingFileDependencies_FromContents(\n\t            build,\n\t            list(target_namespaces),\n\t            snakemake_launcher,\n\t        )\n", "    )\n\t    unresolved_dep_sources = set(\n\t        s.split(\"/\")[1] for s in missing_deps if s.startswith(\"results/\")\n\t    )\n\t    # Target dependencies are resolved if there is no overlap between missing\n\t    # dependencies and the target node's input namespaces\n\t    unresolved_deps = unresolved_dep_sources.intersection(input_namespaces)\n\t    if unresolved_deps:\n\t        return {\n\t            \"status\": \"missing\",\n", "            \"unresolved\": list(unresolved_deps),\n\t        }\n\t    return {\"status\": \"ok\"}\n\t# ##############################################################################\n\t# Utility functions\n\t# ##############################################################################\n\tdef DagFileContent(content: str) -> dict:\n\t    \"\"\"Returns DAG as JSON from Snakefile content\"\"\"\n\t    with IsolatedTempFile(content) as snakefile:\n\t        dag = DagLocal(snakefile)\n", "    return dag\n\tdef DagLocal(filename: str, workdir: str = \"\") -> dict:\n\t    \"\"\"Returns DAG as JSON from Snakefile\"\"\"\n\t    kwargs = {\"workdir\": workdir} if workdir else {}\n\t    stdout, stderr = snakemake_launch(filename, \"--d3dag\", **kwargs)\n\t    # strip first and last lines as needed (snakemake returns True/False)\n\t    sl = stdout.split(\"\\n\")\n\t    if sl[0] in {\"True\", \"False\"}:\n\t        sl = sl[1:]\n\t    if sl[-1] in {\"True\", \"False\"}:\n", "        sl = sl[:-1]\n\t    return json.loads(\"\\n\".join(sl))\n\tdef GetFileAndWorkingDirectory(filename: str) -> Tuple[str, str]:\n\t    \"\"\"Get file and working directory from filename\"\"\"\n\t    if os.path.isdir(filename):\n\t        workdir = os.path.abspath(filename)\n\t        filelist = [\n\t            f\"{workdir}/Snakefile\",\n\t            f\"{workdir}/workflow/Snakefile\",\n\t        ]\n", "        for file in filelist:\n\t            if os.path.exists(file):\n\t                filename = file\n\t                break\n\t    else:\n\t        workdir = \"\"\n\t    return filename, workdir\n\tdef GetMissingFileDependencies_FromContents(\n\t    content: Union[Tuple[dict, str], str],\n\t    target_namespaces: List[str] = [],\n", "    snakemake_launcher: str = \"\",\n\t) -> List[str]:\n\t    \"\"\"Get missing file dependencies from snakemake\n\t    Recursively find missing dependencies for rulesets. Once missing\n\t    dependencies are found, touch those file (in an isolated environment) and\n\t    repeat the process to capture all missing dependencies.\n\t    If target_namespaces is provided, return as soon as any target dependencies\n\t    are found.\n\t    \"\"\"\n\t    if isinstance(content, tuple):\n", "        # Flattern config and Snakemake files together\n\t        workflow_lines = content[1].split(\"\\n\")\n\t        workflow_lines = [\n\t            line for line in workflow_lines if not line.startswith(\"configfile:\")\n\t        ]\n\t        content = (content[0], \"\\n\".join(workflow_lines))\n\t        content_str: str = YAMLToConfig(content[0]) + \"\\n\" + content[1]\n\t    else:\n\t        content_str = content\n\t    deps = []\n", "    with IsolatedTempFile(content_str) as snakefile:\n\t        path = os.path.dirname(os.path.abspath(snakefile))\n\t        while file_list := GetMissingFileDependencies_FromFile(\n\t            snakefile, snakemake_launcher\n\t        ):\n\t            deps.extend(file_list)\n\t            # Return early if target dependencies are not resolved\n\t            if set(deps).intersection(target_namespaces):\n\t                return deps\n\t            # Touch missing files\n", "            for dep in deps:\n\t                target = os.path.abspath(f\"{path}/{dep}\")\n\t                Path(os.path.dirname(target)).mkdir(parents=True, exist_ok=True)\n\t                Path(target).touch()\n\t    return deps\n\tdef GetMissingFileDependencies_FromFile(\n\t    filename: str,\n\t    *args,\n\t    snakemake_launcher: str = \"\",\n\t    **kwargs,\n", ") -> List[str]:\n\t    \"\"\"Get missing file dependencies from snakemake (single file)\n\t    Find missing dependencies for one run of a ruleset; for recursive /\n\t    multiple runs use the corresponding _FromContents function.\n\t    \"\"\"\n\t    if \"--d3dag\" not in args:\n\t        args = args + (\"--d3dag\",)\n\t    stdout, stderr = snakemake_launch(\n\t        filename,\n\t        *args,\n", "        snakemake_launcher=snakemake_launcher,\n\t        **kwargs,\n\t    )\n\t    stderr = \"\\n\".join(stderr.split(\"\\n\"))\n\t    if stdout:\n\t        # build succeeded\n\t        return []\n\t    if not stderr:\n\t        return []\n\t    exceptions = set(\n", "        [\n\t            ex\n\t            for ex in [line.split(\" \")[0] for line in stderr.split(\"\\n\")[0:2]]\n\t            if ex.endswith(\"Exception\")\n\t        ]\n\t    )\n\t    permitted_exceptions = set(\n\t        [\n\t            \"MissingInputException\",\n\t        ]\n", "    )\n\t    if len(exceptions - permitted_exceptions) > 0:\n\t        raise Exception(\n\t            f\"A non-expected error has been detected: \\nstdout={stdout}\\nstderr={stderr}\"\n\t        )\n\t    fileslist = list(filter(None, map(str.strip, stderr.split(\"\\n\"))))\n\t    try:\n\t        ix = fileslist.index(\"affected files:\")\n\t    except ValueError:\n\t        raise Exception(\"No affected files found: \" + stdout + \"\\n\" + stderr)\n", "    return fileslist[(ix + 1) :]\n\t@contextlib.contextmanager\n\tdef IsolatedTempFile(content: str, tempdir=None):\n\t    \"\"\"Create isolated file with passed content placed into a new blank folder\"\"\"\n\t    snakefile_dir = tempfile.TemporaryDirectory(dir=tempdir)\n\t    snakefile_file = tempfile.NamedTemporaryFile(\n\t        dir=snakefile_dir.name, mode=\"w\", encoding=\"utf-8\", delete=False\n\t    )\n\t    snakefile: str = snakefile_file.name\n\t    snakefile_file.write(content)\n", "    snakefile_file.seek(0)\n\t    snakefile_file.close\n\t    # Yield filename as context\n\t    yield snakefile\n\t    # Cleanup\n\t    os.remove(snakefile)\n\t    shutil.rmtree(snakefile_dir.name)\n\tdef WrapCommandForTerminal(cmd: List[str], workdir: str) -> List[str]:\n\t    \"\"\"Wrap command for terminal execution\n\t    This function takes a command and wraps it in a terminal execution command\n", "    for the current platform.\n\t    \"\"\"\n\t    cmdstr = \" \".join(cmd)\n\t    if platform.system() == \"Windows\":\n\t        # Launch terminal process on Windows\n\t        cmd = [\n\t            \"cmd.exe\",\n\t            \"/k\",  # Keep terminal open after execution\n\t            f'\"cd {workdir}\\n{cmdstr}\"',\n\t        ]\n", "    elif platform.system() == \"Darwin\":\n\t        # Launch terminal process on macOS\n\t        cmd = [\n\t            \"osascript\",\n\t            \"-e\",\n\t            f'tell app \"Terminal\" to do script \"cd {workdir}\\n{cmdstr}\"',\n\t        ]\n\t    elif platform.system() == \"Linux\":\n\t        # Launch terminal process on Linux\n\t        cmd = [\"x-terminal-emulator\", \"-e\", f'\"cd {workdir}\\n{cmdstr}\"']\n", "    else:\n\t        raise Exception(f\"Unsupported platform: {platform.system()}.\")\n\t    return cmd\n\tdef snakemake_cmd(filename: str, *args, **kwargs) -> Tuple[List[str], str]:\n\t    \"\"\"Determine snakemake command to launch as subprocess\n\t    This function takes optional arguments that are passed through to the\n\t    snakemake executable, with the exception of:\n\t        workdir: sets the working directory for job execution\n\t        terminal: if True, run snakemake in a terminal window\n\t    \"\"\"\n", "    # Get Snakefile path\n\t    snakefile = os.path.abspath(filename)\n\t    # Check for work directory, otherwise default to Snakefile directory\n\t    workdir = kwargs.get(\"workdir\", \"\")\n\t    if not workdir:\n\t        workdir = os.path.dirname(snakefile)\n\t    try:\n\t        del kwargs[\"workdir\"]\n\t    except KeyError:\n\t        pass\n", "    # Check for terminal flag, then omit from kwargs\n\t    terminal = kwargs.get(\"terminal\", None)\n\t    try:\n\t        del kwargs[\"terminal\"]\n\t    except KeyError:\n\t        pass\n\t    # Collate arguments list\n\t    arglist = list(args)\n\t    for k, v in kwargs.items():\n\t        arglist.extend([k, v])\n", "    # Launch process and wait for return\n\t    cmd = [\n\t        \"snakemake\",\n\t        \"--snakefile\",\n\t        snakefile,\n\t        *arglist,\n\t    ]\n\t    if terminal:\n\t        cmd = WrapCommandForTerminal(cmd, workdir)\n\t    print(cmd)\n", "    return cmd, workdir\n\tdef snakemake_run(\n\t    cmd: List[str],\n\t    workdir: str,\n\t    capture_output: bool = True,\n\t    snakemake_launcher: str = \"\",\n\t) -> Tuple[str, str]:\n\t    \"\"\"Run the snakemake command by the selected launch method\"\"\"\n\t    logging.info(\"Launching snakemake [%s]: %s\", snakemake_launcher, \" \".join(cmd))\n\t    snakemake_launcher = \"builtin\" if not snakemake_launcher else snakemake_launcher\n", "    if snakemake_launcher == \"system\":\n\t        p = subprocess.run(\n\t            cmd,\n\t            cwd=workdir,\n\t            capture_output=capture_output,\n\t        )\n\t        return p.stdout.decode(\"utf-8\"), p.stderr.decode(\"utf-8\")\n\t    elif snakemake_launcher == \"builtin\":\n\t        cmd_str = \" \".join(cmd[1:])  # strip snakemake executable\n\t        if capture_output:\n", "            with redirect_stdout(io.StringIO()) as f_stdout:\n\t                with redirect_stderr(io.StringIO()) as f_stderr:\n\t                    try:\n\t                        snakemake_main(\n\t                            cmd_str + \" -d \" + workdir,\n\t                        )\n\t                    except SystemExit:\n\t                        # SystemExit is raised by snakemake upon exit\n\t                        pass\n\t            return f_stdout.getvalue(), f_stderr.getvalue()\n", "        else:\n\t            try:\n\t                snakemake_main(\n\t                    cmd_str + \" -d \" + workdir,\n\t                )\n\t            except SystemExit:\n\t                # SystemExit is raised by snakemake upon exit\n\t                pass\n\t            return \"\", \"\"\n\t    else:\n", "        raise Exception(f\"Unsupported launcher: {snakemake_launcher}.\")\n\tdef snakemake_launch(\n\t    filename: str,\n\t    *args,\n\t    snakemake_launcher: str = \"\",\n\t    **kwargs,\n\t) -> Tuple[str, str]:\n\t    \"\"\"Construct the snakemake command and then launch\n\t    See snakemake_cmd for details on arguments.\n\t    \"\"\"\n", "    cmd, workdir = snakemake_cmd(filename, *args, **kwargs)\n\t    return snakemake_run(cmd, workdir, snakemake_launcher=snakemake_launcher)\n"]}
{"filename": "electron-app/src/python/backend.py", "chunked_list": ["import contextlib\n\timport json\n\timport logging\n\timport os\n\timport sys\n\timport tempfile\n\timport filesystem\n\timport builder\n\timport runner\n\tdefault_build_path = tempfile.gettempdir() + \"/workflows/build\"\n", "default_testbuild_path = tempfile.gettempdir() + \"/workflows/testbuild\"\n\tlogfile = os.path.expanduser(\"~\") + \"/GRAPEVNE.log\"\n\t# Set up logging\n\tlogging.basicConfig(\n\t    filename=logfile,\n\t    encoding=\"utf-8\",\n\t    level=logging.DEBUG,\n\t)\n\tlogging.info(\"Working directory: %s\", os.getcwd())\n\tdef post(request):\n", "    \"\"\"Handles POST requests from the frontend.\"\"\"\n\t    request_js = json.loads(request)\n\t    query = request_js[\"query\"]\n\t    data = request_js[\"data\"]\n\t    logging.info(\"Received query: %s\", query)\n\t    logging.info(\"Received data: %s\", data)\n\t    if query == \"runner/snakemake-run\":\n\t        # Special query - does not capture stdout, stderr\n\t        runner.SnakemakeRun(data)\n\t        return None\n", "    # Suppress stdout as we only want to control the return data from this\n\t    # PyInstaller executable which is called from electron.\n\t    with contextlib.redirect_stdout(None):\n\t        # File system queries\n\t        if query == \"display/folderinfo\":\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": json.dumps(filesystem.GetFolderItems(data)),\n\t            }\n\t        # Builder queries\n", "        elif query == \"builder/compile-to-json\":\n\t            js = data[\"content\"]\n\t            # with open(default_build_path + \"/workflow.json\", \"w\") as f:  # dump config file to disk for debug\n\t            #     json.dump(js, f, indent=4)\n\t            memory_zip, _, zipfilename = builder.BuildFromJSON(\n\t                js,\n\t                build_path=default_build_path,\n\t            )\n\t            # Binary return is not used when passing the information over stdout.\n\t            # Instead, the zip file is read back off the disk and forwarded by\n", "            # electron / nodejs.\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": {\n\t                    \"zipfile\": zipfilename,\n\t                },\n\t            }\n\t        elif query == \"builder/build-and-run\":\n\t            # First, build the workflow\n\t            logging.info(\"Building workflow\")\n", "            js = data[\"content\"]\n\t            # with open(\"workflow.json\", \"w\") as f:  # dump config file to disk for debug\n\t            #     json.dump(js, f, indent=4)\n\t            build_path = default_testbuild_path\n\t            logging.info(\"BuildFromJSON\")\n\t            memory_zip, m, _ = builder.BuildFromJSON(\n\t                js,\n\t                build_path=build_path,\n\t                clean_build=False,  # Do not overwrite existing build\n\t            )\n", "            targets = data.get(\"targets\", [])\n\t            target_modules = m.LookupRuleNames(targets)\n\t            # Get list of snakemake rules, cross-reference with target_modules\n\t            # and select 'target' or all rules, then pass on to command\n\t            data_list = runner.Launch_cmd(\n\t                {\n\t                    \"format\": data[\"format\"],\n\t                    \"content\": build_path,\n\t                    \"args\": \"--list\",\n\t                },\n", "                terminal=False,\n\t            )\n\t            # Stringify command\n\t            data_list[\"command\"] = \" \".join(data_list[\"command\"])\n\t            logging.info(\"List command: %s\", data_list[\"command\"])\n\t            response = runner.SnakemakeRun(\n\t                {\n\t                    \"format\": data[\"format\"],\n\t                    \"content\": {\n\t                        \"command\": data_list[\"command\"],\n", "                        \"workdir\": data_list[\"workdir\"],\n\t                        \"capture_output\": True,\n\t                        \"backend\": data.get(\"backend\", \"\"),\n\t                    },\n\t                }\n\t            )\n\t            snakemake_list = response[\"stdout\"].split(\"\\n\")\n\t            logging.debug(\"snakemake --list output: %s\", snakemake_list)\n\t            target_rules = []\n\t            for target in target_modules:\n", "                target_rule = f\"{target}_target\"\n\t                if target_rule in snakemake_list:\n\t                    target_rules.append(target_rule)\n\t                else:\n\t                    target_rules.extend(\n\t                        [\n\t                            rulename\n\t                            for rulename in snakemake_list\n\t                            if rulename.startswith(target)\n\t                        ]\n", "                    )\n\t            # Second, return the launch command\n\t            logging.info(\"Generating launch command\")\n\t            data = runner.Launch_cmd(\n\t                {\n\t                    \"format\": data[\"format\"],\n\t                    \"content\": build_path,\n\t                    \"targets\": target_rules,\n\t                    \"args\": data.get(\"args\", \"\"),\n\t                },\n", "                terminal=False,\n\t            )\n\t            # Stringify command\n\t            data[\"command\"] = \" \".join(data[\"command\"])\n\t            logging.info(\"Launch command: %s\", data[\"command\"])\n\t            # Return the launch command\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": data,\n\t            }\n", "        elif query == \"builder/clean-build-folder\":\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": builder.CleanBuildFolder(default_testbuild_path),\n\t            }\n\t        elif query == \"builder/get-remote-modules\":\n\t            js = data[\"content\"]\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": builder.GetModulesList(js[\"url\"]),\n", "            }\n\t        # Runner queries\n\t        elif query == \"runner/build\":\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": runner.Build(data),\n\t            }\n\t        elif query == \"runner/deleteresults\":\n\t            data = {\n\t                \"query\": query,\n", "                \"body\": json.dumps(runner.DeleteAllOutput(data)),\n\t            }\n\t        elif query == \"runner/lint\":\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": json.dumps(runner.LintContents(data)),\n\t            }\n\t        elif query == \"runner/loadworkflow\":\n\t            data = {\n\t                \"query\": query,\n", "                \"body\": json.dumps(runner.LoadWorkflow(data)),\n\t            }\n\t        elif query == \"runner/tokenize\":\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": json.dumps(runner.Tokenize(data)),\n\t            }\n\t        elif query == \"runner/tokenize_load\":\n\t            try:\n\t                # Try full-tokenize (may fail if dependencies not present)\n", "                body = runner.FullTokenizeFromFile(data)\n\t            except BaseException:\n\t                # ...then try in-situ tokenization\n\t                body = runner.TokenizeFromFile(data)\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": json.dumps(body),\n\t            }\n\t        elif query == \"runner/jobstatus\":\n\t            data = {\n", "                \"query\": query,\n\t                \"body\": json.dumps(runner.TokenizeFromFile(data)),\n\t            }\n\t        elif query == \"runner/launch\":\n\t            data = {\n\t                \"query\": query,\n\t                \"body\": json.dumps(runner.Launch(data)),\n\t            }\n\t        elif query == \"runner/check-node-dependencies\":\n\t            data = {\n", "                \"query\": query,\n\t                \"body\": runner.CheckNodeDependencies(data),\n\t            }\n\t        else:\n\t            raise NotImplementedError(f\"Unknown query: {query}\")\n\t    # Return via stdout\n\t    rtn = json.dumps(data)\n\t    print(rtn)\n\t    logging.info(\"Query response: %s\", rtn)\n\tpost(sys.argv[1])\n"]}
{"filename": "electron-app/src/python/filesystem.py", "chunked_list": ["import os\n\timport shutil\n\tdef DeleteResults(data) -> dict:\n\t    \"\"\"Remove the local 'results' folder\"\"\"\n\t    dirname = data[\"content\"]\n\t    dirname = f\"{dirname}/results\"\n\t    shutil.rmtree(dirname)\n\t    return {}\n\tdef GetFolderItems(data) -> dict:\n\t    \"\"\"Get the contents of a folder\"\"\"\n", "    dirname = data[\"content\"]\n\t    dirlist = [\n\t        filename\n\t        for filename in os.listdir(dirname)\n\t        if os.path.isdir(os.path.join(dirname, filename))\n\t    ]\n\t    dirlist = [name for name in dirlist if name[0] != \".\"]\n\t    dirlist.sort()\n\t    isdir = len(dirlist) * [True]\n\t    filelist = [\n", "        filename\n\t        for filename in os.listdir(dirname)\n\t        if not os.path.isdir(os.path.join(dirname, filename))\n\t    ]\n\t    filelist = [name for name in filelist if name[0] != \".\"]\n\t    filelist.sort()\n\t    isdir.extend(len(filelist) * [False])\n\t    contents = [*dirlist, *filelist]\n\t    contents = [{\"name\": name, \"isdir\": isdir} for name, isdir in zip(contents, isdir)]\n\t    js = {\"foldername\": dirname, \"contents\": contents}\n", "    return js\n"]}
{"filename": "docs/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# For the full list of built-in configuration values, see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\tproject = \"GRAPEVNE\"\n\tcopyright = \"2023, kraemer-lab\"\n\tauthor = \"kraemer-lab\"\n\t# -- General configuration ---------------------------------------------------\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\textensions = [\n\t    \"sphinx.ext.autodoc\",\n\t    \"sphinx.ext.napoleon\",\n\t    \"sphinx.ext.coverage\",\n\t    \"sphinx.ext.graphviz\",\n\t    \"myst_parser\",\n\t    \"sphinx_rtd_theme\",\n\t]\n\ttemplates_path = [\n", "    \"_templates\",\n\t]\n\texclude_patterns = [\n\t    \"_build\",\n\t    \"Thumbs.db\",\n\t    \".DS_Store\",\n\t    \"venv\",\n\t    \"README.md\",\n\t]\n\t# -- Options for HTML output -------------------------------------------------\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\thtml_theme = \"sphinx_rtd_theme\"\n\thtml_static_path = [\n\t    \"_static\",\n\t]\n\tgraphviz_output_format = \"svg\"\n"]}
