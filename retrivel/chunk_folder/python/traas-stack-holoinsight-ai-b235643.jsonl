{"filename": "serving.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'serving'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/7 15:44'\n\t__info__ =\n\t\"\"\"\n\tfrom flask import Flask, request\n\tfrom handlers.run_main import run_main\n\tapp = Flask(__name__)\n", "@app.route('/anomaly_detect', methods=['POST'])\n\tdef anomaly_detect():\n\t    body = request.json\n\t    result = run_main(body)\n\t    trace_id = body.get(\"traceId\")\n\t    detect_time = body.get(\"detectTime\")\n\t    result[\"traceId\"] = trace_id\n\t    result[\"detectTime\"] = detect_time\n\t    result[\"errorCode\"] = {}\n\t    return result\n", "if __name__ == '__main__':\n\t    app.run(host='0.0.0.0', port=8000, debug=True)\n\t    pass\n"]}
{"filename": "test/test_down_cs_noise.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_8():\n\t    \"\"\"\n\t    Generates data for a cold-start scenario,\n\t    tests the similarity filter's ability to filter the noise.\n\t    :return:\n\t    \"\"\"\n\t    detect_time = 1681711200000\n\t    ts = TestDataCreator.create_stable_ts(detect_time, 1 * 1440, 60000, 500, 600)\n\t    # Add anomaly value at the detection time and a value 500 intervals before the detection time\n\t    ts[str(detect_time)] = 0\n", "    ts[str(detect_time - 500 * 60000)] = 0\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n\t            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"down\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1, \"customChangeRate\": 0.1}}\n\t    result = run_main(body)\n\t    return result\n\tclass TestFunction(unittest.TestCase):\n\t    def test(self):\n\t        self.assertEqual(run_8().get(\"isException\"), False)\n", "        pass\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "test/test_down_periodic_dd.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_2():\n\t    \"\"\"\n\t    Generates periodic data for a normal scenario,\n\t    tests the data-driven detector's ability to detect anomalies.\n\t    :return:\n\t    \"\"\"\n\t    detect_time = 1681711200000\n\t    ts = TestDataCreator.create_periodic_ts(end_time=detect_time, ts_length=5 * 1440, period=60000, median_value=1000)\n\t    ts[str(detect_time)] = 500\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n", "            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"down\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1,\n\t                           \"customChangeRate\": 0.05}}\n\t    result = run_main(body)\n\t    return result\n\tclass TestFunction(unittest.TestCase):\n\t    def test(self):\n\t        self.assertEqual(run_2().get(\"isException\"), True)\n\t        pass\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "test/test_up_periodic_dd.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_5():\n\t    \"\"\"\n\t    Generates periodic data for a normal scenario,\n\t    tests the data-driven detector's ability to detect anomalies.\n\t    :return:\n\t    \"\"\"\n\t    detect_time = 1681711200000\n\t    ts = TestDataCreator.create_periodic_ts(end_time=detect_time, ts_length=5 * 1440, period=60000, median_value=1000)\n\t    ts[str(detect_time)] = 1100\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n", "            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"up\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1,\n\t                           \"customChangeRate\": 0.05}}\n\t    result = run_main(body)\n\t    return result\n\tclass TestFunction(unittest.TestCase):\n\t    def test(self):\n\t        self.assertEqual(run_5().get(\"isException\"), True)\n\t        pass\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "test/assert_test_cases.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/18 11:56'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom test.test_down_cs import run_1\n\tfrom test.test_down_cs_noise import run_8\n", "from test.test_down_periodic_dd import run_2\n\tfrom test.test_down_stable_dd import run_3\n\tfrom test.test_up_cs import run_4\n\tfrom test.test_up_cs_noise import run_9\n\tfrom test.test_up_periodic_dd import run_5\n\tfrom test.test_up_stable_dd import run_6\n\tfrom test.test_up_stable_dd_noise import run_7\n\tclass TestFunction(unittest.TestCase):\n\t    \"\"\"\n\t    All test cases.\n", "    \"\"\"\n\t    def test1(self):\n\t        self.assertEqual(run_1().get(\"isException\"), True)\n\t        pass\n\t    def test2(self):\n\t        self.assertEqual(run_2().get(\"isException\"), True)\n\t        pass\n\t    def test3(self):\n\t        self.assertEqual(run_3().get(\"isException\"), True)\n\t        pass\n", "    def test4(self):\n\t        self.assertEqual(run_4().get(\"isException\"), True)\n\t        pass\n\t    def test5(self):\n\t        self.assertEqual(run_5().get(\"isException\"), True)\n\t        pass\n\t    def test6(self):\n\t        self.assertEqual(run_6().get(\"isException\"), True)\n\t        pass\n\t    def test7(self):\n", "        self.assertEqual(run_7().get(\"isException\"), False)\n\t        pass\n\t    def test8(self):\n\t        self.assertEqual(run_8().get(\"isException\"), False)\n\t        pass\n\t    def test9(self):\n\t        self.assertEqual(run_9().get(\"isException\"), False)\n\t        pass\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "test/test_up_stable_dd.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_6():\n\t    \"\"\"\n\t    Generates stable data for a normal scenario,\n\t    tests the data-driven detector's ability to detect anomalies.\n\t    :return:\n\t    \"\"\"\n\t    detect_time = 1681711200000\n\t    ts = TestDataCreator.create_stable_ts(end_time=detect_time, ts_length=5 * 1440, period=60000, down=500, up=600)\n\t    ts[str(detect_time)] = 1000\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n", "            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"up\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1,\n\t                           \"customChangeRate\": 0.1}\n\t            }\n\t    result = run_main(body)\n\t    return result\n\tclass TestFunction(unittest.TestCase):\n\t    def test(self):\n\t        self.assertEqual(run_6().get(\"isException\"), True)\n", "        pass\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "test/test_data_creator.py", "chunked_list": ["# !/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'data_process'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/12 10:50'\n\t__info__ =\n\t\"\"\"\n\timport random\n", "import numpy as np\n\timport matplotlib.pyplot as plt\n\tfrom typing import Dict\n\tfrom common.utils import Utils\n\tclass TestDataCreator:\n\t    @staticmethod\n\t    def create_stable_ts(end_time: int, ts_length: int, period: int, down: int, up: int) -> Dict[str, float]:\n\t        \"\"\"\n\t        :param up:\n\t        :param down:\n", "        :param end_time:\n\t        :param ts_length:\n\t        :param period: 粒度，此处默认为60000\n\t        :return:\n\t        \"\"\"\n\t        ts = {}\n\t        start_time = end_time - ts_length * period\n\t        random.seed(0)\n\t        for i in range(start_time, end_time + period, period):  # 确保能够取到detect time\n\t            ts[str(i)] = random.randint(down, up)\n", "        return ts\n\t    @staticmethod\n\t    def create_periodic_ts(end_time: int, ts_length: int, period: int, median_value: int) -> Dict[str, float]:\n\t        \"\"\"\n\t        :param median_value:\n\t        :param end_time:\n\t        :param ts_length:\n\t        :param period: 粒度，此处默认为60000\n\t        :return:\n\t        \"\"\"\n", "        days = int(np.ceil(ts_length / 1440))\n\t        start_time = end_time - ts_length * period\n\t        all_ts = []\n\t        for i in range(days):\n\t            time_points = [i * (2 * np.pi / 1440) for i in range(1440)]\n\t            all_ts += [median_value + -int(100 * np.sin(t) + int(100 * random.uniform(-0.1, 0.1))) for t in time_points]\n\t        time_stamps = list(range(start_time + period, end_time + period, period))\n\t        ts = {}\n\t        for i, v in enumerate(time_stamps):\n\t            ts[str(v)] = all_ts[i]\n", "        return ts\n\t    @staticmethod\n\t    def plot(ts, detect_time, period):\n\t        start = min([int(key) for key in ts.keys()])\n\t        ts = Utils().time_series_min_str(ts, start, detect_time + 60000, period)\n\t        plt.plot(ts)\n\t        plt.show()\n\tif __name__ == \"__main__\":\n\t    ts = TestDataCreator.create_periodic_ts(1681711200000, 3 * 1440 + 1, 60000, 1000)\n\t    TestDataCreator.plot(ts, 1681711200000, 60000)\n", "    pass\n"]}
{"filename": "test/__init__.py", "chunked_list": []}
{"filename": "test/test_down_cs.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_1():\n\t    \"\"\"\n\t    Generates data for a cold-start scenario,\n\t    tests the cold_start's ability to detect anomalies.\n\t    :return:\n\t    \"\"\"\n\t    # Set the detection time\n\t    detect_time = 1681711200000\n\t    # Create a stable time series with specified parameters\n\t    ts = TestDataCreator.create_stable_ts(detect_time, 1 * 1440, 60000, 500, 600)\n", "    # Add anomaly value at the detection time\n\t    ts[str(detect_time)] = 0\n\t    # Create the request body with input time series, detection time, and algorithm and rule configurations\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n\t            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"down\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1, \"customChangeRate\": 0.1}}\n\t    # Run the main function with the request body and return the result\n\t    result = run_main(body)\n\t    return result\n", "class TestFunction(unittest.TestCase):\n\t    def test(self):\n\t        self.assertEqual(run_1().get(\"isException\"), True)\n\t        pass\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "test/test_up_stable_dd_noise.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_7():\n\t    \"\"\"\n\t    Generates stable data for a normal scenario,\n\t    tests the data-driven detector's ability to filter the noise.\n\t    :return:\n\t    \"\"\"\n\t    detect_time = 1681711200000\n\t    ts = TestDataCreator.create_stable_ts(end_time=detect_time, ts_length=5 * 1440, period=60000, down=500, up=600)\n\t    # Add values at specific times to create a periodic noise\n\t    ts[str(detect_time)] = 1000\n", "    ts[str(detect_time - 1440 * 60000)] = 1000\n\t    ts[str(detect_time - 2 * 1440 * 60000)] = 1000\n\t    ts[str(detect_time - 3 * 1440 * 60000)] = 1000\n\t    ts[str(detect_time - 4 * 1440 * 60000)] = 1000\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n\t            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"up\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1,\n\t                           \"customChangeRate\": 0.1}\n\t            }\n", "    result = run_main(body)\n\t    return result\n\tclass TestFunction(unittest.TestCase):\n\t    def test(self):\n\t        self.assertEqual(run_7().get(\"isException\"), False)\n\t        pass\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "test/test_down_stable_dd.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_3():\n\t    \"\"\"\n\t    Generates stable data for a normal scenario,\n\t    tests the data-driven detector's ability to detect anomalies.\n\t    :return:\n\t    \"\"\"\n\t    detect_time = 1681711200000\n\t    ts = TestDataCreator.create_stable_ts(end_time=detect_time, ts_length=5 * 1440, period=60000, down=500, up=600)\n\t    ts[str(detect_time)] = 0\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n", "            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"down\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1,\n\t                           \"customChangeRate\": 0.1}}\n\t    result = run_main(body)\n\t    return result\n\tclass TestFunction(unittest.TestCase):\n\t    def test(self):\n\t        self.assertEqual(run_3().get(\"isException\"), True)\n\t        pass\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "test/test_up_cs.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_4():\n\t    \"\"\"\n\t    Generates data for a cold-start scenario,\n\t    tests the cold_start's ability to detect anomalies.\n\t    :return:\n\t    \"\"\"\n\t    detect_time = 1681711200000\n\t    ts = TestDataCreator.create_stable_ts(detect_time, 1 * 1440, 60000, 500, 600)\n\t    ts[str(detect_time)] = 1000\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n", "            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"up\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1,\n\t                           \"customChangeRate\": 0.1}}\n\t    result = run_main(body)\n\t    return result\n\t# class TestFunction(unittest.TestCase):\n\t#\n\t#     def test(self):\n\t#         self.assertEqual(run_4().get(\"isException\"), True)\n", "#         pass\n\tif __name__ == \"__main__\":\n\t    run_4()\n\t    pass\n"]}
{"filename": "test/test_up_cs_noise.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'test_up'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 14:33'\n\t__info__ =\n\t\"\"\"\n\timport unittest\n\tfrom handlers.run_main import run_main\n\tfrom test.test_data_creator import TestDataCreator\n", "def run_9():\n\t    \"\"\"\n\t    Generates data for a cold-start scenario,\n\t    tests the rule filter's ability to filter noise.\n\t    :return:\n\t    \"\"\"\n\t    detect_time = 1681711200000\n\t    ts = TestDataCreator.create_stable_ts(detect_time, 1 * 1440, 60000, 500, 600)\n\t    ts[str(detect_time)] = 1000\n\t    body = {\"InputTimeSeries\": ts, \"intervalTime\": 60000,\n", "            \"detectTime\": detect_time,\n\t            \"algorithmConfig\": {\"algorithmType\": \"up\", \"sensitivity\": \"mid\"},\n\t            \"ruleConfig\": {\"defaultDuration\": 1,\n\t                           # \"customUpThreshold\": 0,\n\t                           \"customDownThreshold\": 1001,  # rule filer\n\t                           \"customChangeRate\": 0.1}}\n\t    result = run_main(body)\n\t    # TestDataCreator.plot(ts, detect_time, 60000)\n\t    return result\n\tclass TestFunction(unittest.TestCase):\n", "    def test(self):\n\t        self.assertEqual(run_9().get(\"isException\"), False)\n\t        pass\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "common/logger.py", "chunked_list": ["\"\"\"\n\t__project_ = 'holoinsight-ai'\n\t__file_name__ = 'common_logger'\n\t__author__ = 'luyuan'\n\t__time__ = '2021-12-15 22:24'\n\t__product_name = PyCharm\n\t__info__:\n\t\"\"\"\n\timport logging\n\tclass Logger(object):\n", "    def __init__(self):\n\t        super()\n\t    @staticmethod\n\t    def get_logger():\n\t        logging.basicConfig(level=logging.INFO)\n\t        return logging\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "common/__init__.py", "chunked_list": []}
{"filename": "common/utils.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'utils'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/11 17:38'\n\t__info__ = the utils\n\t\"\"\"\n\timport math\n\timport numpy as np\n\tfrom numpy import nan\n", "from typing import List, Dict, Callable, Any\n\tclass Utils:\n\t    def diff_feature_calc(self, input_data: List[float], search_length: int) -> list:\n\t        \"\"\"\n\t        Calculates the difference feature for a given input list.\n\t        :param input_data: A list of floats with length greater than search_length.\n\t        :param search_length: The maximum range for which to calculate the difference feature.\n\t        :return: A list of floats representing the difference feature.\n\t        \"\"\"\n\t        diff = []\n", "        for i in range(len(input_data) - 1, search_length - 1, -1):\n\t            if input_data[i] - input_data[i - 1] < 0:\n\t                search_list = input_data[i - search_length: i + 1]\n\t                duration = self.monotonic_duration(search_list, True)\n\t                diff.append(input_data[i] - input_data[i - duration + 1])\n\t            else:\n\t                search_list = input_data[i - search_length: i + 1]\n\t                duration = self.monotonic_duration(search_list)\n\t                diff.append(input_data[i] - input_data[i - duration + 1])\n\t        diff.reverse()\n", "        out_put_diffs = search_length * [0] + diff\n\t        return out_put_diffs\n\t    @staticmethod\n\t    def monotonic_duration(lst: List[float], reverse=False):\n\t        \"\"\"\n\t        Calculates the duration of a monotonic sequence in a given list.\n\t        :param lst: A list of floats.\n\t        :param reverse: If True, treats the list as reversed for calculation.\n\t        :return: An integer representing the duration of the monotonic sequence.\n\t        \"\"\"\n", "        if reverse:\n\t            lst = [-v for v in lst]\n\t        if len(lst) == 0:\n\t            return 0\n\t        if len(lst) == 1:\n\t            return 1\n\t        count = 1\n\t        for i in range(len(lst) - 1, 0, -1):\n\t            if lst[i] > lst[i - 1]:\n\t                count += 1\n", "            else:\n\t                break\n\t        return count\n\t    @staticmethod\n\t    def turkey_box_plot(input_data: List[float], delta=1.5) -> list:\n\t        \"\"\"\n\t        Calculates the Tukey box plot for a given input list.\n\t        :param input_data: A list of floats.\n\t        :param delta: The delta value to use for the box plot calculation.\n\t        :return: A list of floats representing the Tukey box plot.\n", "        \"\"\"\n\t        q1 = np.percentile(input_data, 25)\n\t        q3 = np.percentile(input_data, 75)\n\t        q2 = np.percentile(input_data, 50)\n\t        iqr = q3 - q1\n\t        return [q1, q2, q3, q1 - delta * iqr, q3 + delta * iqr]\n\t    def time_series_min_str(self, p_data: Dict[str, float], start: int, end: int, period: int) -> list:\n\t        \"\"\"\n\t        Generates a complete minute-level time series.\n\t        @param p_data: A dictionary with index as key and amplitude as value.\n", "        @param start: The start time of the time series in milliseconds.\n\t        @param end: The end time of the time series in milliseconds.\n\t        @param period: The period of each time step in milliseconds.\n\t        @return: A list representing the complete time series.\n\t        \"\"\"\n\t        out_time_series = []\n\t        for i_time in range(start, end, period):\n\t            if str(i_time) in p_data.keys():\n\t                out_time_series.append(p_data[str(i_time)])\n\t            else:\n", "                out_time_series.append(None)\n\t        out_time_series = self.time_series_imputation(out_time_series)\n\t        return out_time_series\n\t    @staticmethod\n\t    def time_series_imputation(input_data: List[float]) -> list:\n\t        \"\"\"\n\t         Imputes missing values in a time series.\n\t         @param input_data: A list of floats representing the time series.\n\t         @return: A list of floats with imputed missing values.\n\t         \"\"\"\n", "        if None in input_data:\n\t            if input_data.count(None) == len(input_data):\n\t                return []\n\t            if input_data[0] is None:\n\t                input_data[0] = next(item for item in input_data if item is not None)\n\t            for i in range(1, len(input_data)):\n\t                if input_data[i] is None or math.isnan(input_data[i]):\n\t                    input_data[i] = input_data[i - 1]\n\t        return input_data\n\t    @staticmethod\n", "    def agg_diff_fe_calc(input_data: List[float], agg_length: int) -> list:\n\t        \"\"\"\n\t        Calculates aggregated difference features for a given input list.\n\t        @param input_data: A list of floats representing the input data.\n\t        @param agg_length: The length of the aggregation window.\n\t        @return: A list of floats representing the aggregated difference features.\n\t        \"\"\"\n\t        diff_func: Callable[[Any, Any], None] = lambda a, b: np.sum(a) - np.sum(b)\n\t        diff = []\n\t        for i in range(len(input_data) - 2 * agg_length + 1):\n", "            post = input_data[i + agg_length:i + 2 * agg_length]\n\t            pre = input_data[i:i + agg_length]\n\t            diff.append(diff_func(post, pre))\n\t        return diff\n\t    @staticmethod\n\t    def longest_continuous(lst, target) -> int:\n\t        \"\"\"\n\t        Finds the length of the longest continuous sequence in a list that meets a given target condition.\n\t        @param lst: A list of values to search.\n\t        @param target: The target value to search for.\n", "        @return: The length of the longest continuous sequence that meets the target condition.\n\t        \"\"\"\n\t        count = 0\n\t        max_count = 0\n\t        for num in lst:\n\t            if num <= target:\n\t                count += 1\n\t                max_count = max(max_count, count)\n\t            else:\n\t                count = 0\n", "        return max_count\n\t    @staticmethod\n\t    def diff_percentile_func(data: list, step: int, is_down=True) -> list:\n\t        \"\"\"\n\t        Calculates the percentile difference for a given data list and step size.\n\t        @param data: A list of data values.\n\t        @param step: The step size for calculating the percentile difference.\n\t        @param is_down: A boolean indicating whether the percentile difference should be negative.\n\t        @return: A list of percentile differences.\n\t        \"\"\"\n", "        diff_list = []\n\t        for i in range(2 * step, len(data)):\n\t            if step == 1:\n\t                if data[i - step] != 0:\n\t                    v = 100 * (data[i] - data[i - step]) / data[i - step]\n\t                    if is_down:\n\t                        diff_list.append(v if v < 0 else 0)\n\t                    else:\n\t                        diff_list.append(-v if v > 0 else 0)\n\t                else:\n", "                    diff_list.append(nan)\n\t            else:\n\t                j = i + 1\n\t                if np.mean(data[j - 2 * step: j - step]) != 0:\n\t                    v = 100 * (np.mean(data[j - step:j]) - np.mean(data[j - 2 * step: j - step])) / np.mean(\n\t                        data[j - 2 * step: j - step])\n\t                    if is_down:\n\t                        diff_list.append(v if v < 0 else 0)\n\t                    else:\n\t                        diff_list.append(-v if v > 0 else 0)\n", "                else:\n\t                    diff_list.append(nan)\n\t        diff_array = np.array(diff_list)\n\t        if diff_array.size - np.isnan(diff_array).sum() == 0:\n\t            fill_value = -100\n\t        else:\n\t            fill_value = np.nanmean(diff_array)\n\t        np.nan_to_num(diff_array, nan=fill_value, copy=False)\n\t        diff_list = diff_array.tolist()\n\t        return (2 * step) * [diff_list[0]] + diff_list\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "common/constants.py", "chunked_list": ["\"\"\"\n\t__project_ = 'holoinsight-ai'\n\t__file_name__ = 'contants'\n\t__author__ = 'luyuan'\n\t__time__ = '2021-08-09 16:30'\n\t__product_name = PyCharm\n\t__info__:\n\t\"\"\"\n\tfrom enum import Enum\n\tfrom math import inf\n", "class Constants(Enum):\n\t    \"\"\"\n\t    A class containing various constants used in the AD system.\n\t    \"\"\"\n\t    WINDOW_LIST = [1, 3, 5, 7, 9]   # A list of window sizes used in the AD system.\n\t    MIN_DURATION_DEFAULT = 1  # The default minimum duration used in the AD system.\n\t    CUSTOM_UP_THRESHOLD = float(inf)  # The default upper threshold used in the AD system.\n\t    CUSTOM_DOWN_THRESHOLD = -float(inf)  # The default lower threshold used in the AD system.\n\t    CUSTOM_CHANGE_RATE_DEFAULT = 0.05  # The default change rate used in the AD system.\n\t    ALGORITHM_TYPE_UP = \"up\"  # the algorithm_type up\n", "    ALGORITHM_TYPE_DOWN = \"down\"  # the algorithm_type down\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "common/classes.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'base'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/12 20:45'\n\t__info__ =\n\t\"\"\"\n\tfrom dataclasses import dataclass, field\n\tfrom typing import List\n\t@dataclass\n", "class DetectInfo:\n\t    \"\"\"\n\t    Information about the sensitive data and algorithm type used for anomaly detection.\n\t    \"\"\"\n\t    sensitive: str\n\t    algorithm_type: str  # \"up\"/\"down\"\n\t@dataclass\n\tclass RuleInfo:\n\t    \"\"\"\n\t    Information about the anomaly detection rules, including minimum duration, thresholds, and change rate.\n", "    \"\"\"\n\t    min_duration: int\n\t    up_threshold: float\n\t    down_threshold: float\n\t    change_rate: float\n\t@dataclass\n\tclass ErrorInfo:\n\t    \"\"\"\n\t    Information about any errors encountered during the anomaly detection process.\n\t    \"\"\"\n", "    errorOrNot: bool = False\n\t    msg: str = ''\n\t@dataclass\n\tclass Request4AD:\n\t    \"\"\"\n\t    Request information for time series anomaly detection, including trace ID, data by day, detection info,\n\t    and rule info.\n\t    \"\"\"\n\t    trace_id: str\n\t    data_by_day: dict\n", "    detect_info: DetectInfo\n\t    rule_info: RuleInfo\n\t@dataclass\n\tclass StatusInOut:\n\t    \"\"\"\n\t    Pipeline status information for anomaly detection.\n\t    \"\"\"\n\t    alarmOrNot: bool = False  # True indicates current alarm, default is False\n\t    needNext: bool = False  # True indicates no need for next step, default is False\n\t    duration: int = 0  # Default is 0\n", "    passReason: str = ''  # Default is ''\n\t    alarmReason: str = ''  # Default is ''\n\t@dataclass\n\tclass Response4AD:\n\t    \"\"\"\n\t    Results of time series anomaly detection, including whether an anomaly was detected, success status, duration,\n\t    and various metrics.\n\t    \"\"\"\n\t    isException: bool = False\n\t    success: bool = True  # Whether the detection ran successfully\n", "    duration: int = 0\n\t    alarmCategory: str = \"\"\n\t    filterReason: str = \"\"\n\t    alarmMsg: str = \"\"\n\t    currentValue: float = 0.0\n\t    changeRate: float = 0.0  # # Not percentage format\n\t    extremeValue: float = 0.0\n\t    baseLineValue: float = 0.0\n\t    anomalyData: List[float] = field(default_factory=lambda: [])\n\t    def get_msg(self):\n", "        \"\"\"\n\t        Returns a dictionary containing the isException and success attributes of the Response4AD object.\n\t        \"\"\"\n\t        return {\"isException\": self.isException, \"isSuccessful\": self.success}\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "common/request_builder.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'context_builder'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/12 20:35'\n\t__info__ = build the request\n\t\"\"\"\n\tfrom typing import Dict\n\tfrom common.classes import DetectInfo, RuleInfo, Request4AD\n\tfrom common.constants import Constants\n", "from common.utils import Utils\n\tclass RequestBuilder:\n\t    def __init__(self, input_body: dict):\n\t        self.body = input_body\n\t        self.req = None\n\t    def build_req(self):\n\t        \"\"\"\n\t        Builds and returns an AD request object based on the input body.\n\t        @return: The built AD request object.\n\t        \"\"\"\n", "        # Data processing\n\t        ts = self.body.get(\"InputTimeSeries\")\n\t        detect_time = self.body.get(\"detectTime\")\n\t        period = self.body.get(\"intervalTime\")\n\t        data_by_data = self.data_process(ts, detect_time, period, detect_length=self.period_mapper(period))\n\t        # Detect information\n\t        algorithm_type = self.body.get(\"algorithmConfig\").get(\"algorithmType\")\n\t        detect_info = DetectInfo(sensitive=self.body.get(\"algorithmConfig\").get(\"sensitivity\", \"mid\"),\n\t                                 algorithm_type=algorithm_type\n\t                                 )\n", "        # Rule information\n\t        if self.body.get(\"ruleConfig\") is None:\n\t            self.body[\"ruleConfig\"] = {}\n\t        up_threshold = Constants.CUSTOM_UP_THRESHOLD.value\n\t        down_threshold = Constants.CUSTOM_DOWN_THRESHOLD.value\n\t        rule_info = RuleInfo(\n\t            min_duration=self.body.get(\"ruleConfig\").get(\"defaultDuration\", Constants.MIN_DURATION_DEFAULT.value),\n\t            up_threshold=self.body.get(\"ruleConfig\").get(\"customUpThreshold\", up_threshold),\n\t            down_threshold=self.body.get(\"ruleConfig\").get(\"customDownThreshold\", down_threshold),\n\t            change_rate=self.body.get(\"ruleConfig\").get(\"customChangeRate\", Constants.CUSTOM_CHANGE_RATE_DEFAULT.value),\n", "        )\n\t        # Build and return the AD request object\n\t        self.req = Request4AD(trace_id=self.body.get(\"traceId\"),\n\t                              data_by_day=data_by_data,\n\t                              detect_info=detect_info,\n\t                              rule_info=rule_info\n\t                              )\n\t        return self.req\n\t    @staticmethod\n\t    def data_process(time_series: Dict[str, float], detect_time: int, period: int, detect_length) -> dict:\n", "        \"\"\"\n\t        Transforms input test data into a list of tuples representing the time periods that need to be analyzed.\n\t        @param time_series: A dictionary containing the input time series data.\n\t        @param detect_time: The detection time.\n\t        @param period: The period of the input data.\n\t        @param detect_length: The detection length.\n\t        @return: A dictionary containing the processed data grouped by day.\n\t        \"\"\"\n\t        detect_time += period  # make sure to get the running time\n\t        detect_left_time = detect_time - detect_length * period\n", "        earliest_time = min([int(key) for key in list(time_series.keys())])\n\t        day_num = int((detect_left_time - earliest_time) / (1440 * 60000))\n\t        data_groups = []\n\t        while len(data_groups) < day_num:\n\t            if len(data_groups) == 0:\n\t                data_groups.append((detect_time - detect_length * period, detect_time))\n\t            else:\n\t                cur_start, cur_end = data_groups[-1][0], data_groups[-1][1]\n\t                data_groups.append((cur_start - 1440 * 60000, cur_end - 1440 * 60000))\n\t        data_by_day = {}\n", "        for i, (left, right) in enumerate(data_groups):\n\t            data_by_day[str(i)] = Utils().time_series_min_str(time_series, left, right, period)\n\t        if len(data_groups) == 0:\n\t            data_by_day = {str(0): Utils().time_series_min_str(time_series, earliest_time, detect_time, period)}\n\t        return data_by_day\n\t    @staticmethod\n\t    def period_mapper(period) -> int:\n\t        \"\"\"\n\t        Maps the period of the input data to the corresponding analysis length.\n\t        @param period: The period of the input data.\n", "        @return: The corresponding analysis length.\n\t        \"\"\"\n\t        if period == 60000:\n\t            return 24 * 60\n\t        else:\n\t            return 24 * 60\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "handlers/detect_handlers.py", "chunked_list": ["\"\"\"\n\t__project_ = 'holoinsight-ai'\n\t__file_name__ = 'handler base'\n\t__author__ = 'LuYuan'\n\t__time__ = '2021-08-09 14:02'\n\t__product_name = PyCharm\n\t__info__: handler\n\t\"\"\"\n\tfrom abc import ABC\n\tfrom algorithm.cs_module import ColdStartModule\n", "from algorithm.dt_module import DynamicThresholdModule\n\tfrom common.classes import Request4AD, Response4AD\n\tclass BaseHandler(ABC):\n\t    def __init__(self, req: Request4AD):\n\t        \"\"\"\n\t        Initializes the BaseHandler with a request object and sets the run_success attribute to True.\n\t        :param req: A Request4AD object containing data to be processed.\n\t        \"\"\"\n\t        self.req = req\n\t        self.run_success = True\n", "    @staticmethod\n\t    def run(self):\n\t        \"\"\"\n\t        Runs the detection pipeline.\n\t        This method is abstract and must be implemented by child classes.\n\t        \"\"\"\n\tclass ColdStartDetectHandler(BaseHandler):\n\t    \"\"\"\n\t    Handles detection of a single dimension value increase.\n\t    \"\"\"\n", "    def run(self) -> Response4AD:\n\t        \"\"\"\n\t        Runs the ColdStartModule pipeline and returns the result.\n\t        :return: A Response4AD object containing the result of the pipeline.\n\t        \"\"\"\n\t        cs = ColdStartModule(self.req)\n\t        cs.pipeline()\n\t        return cs.resp\n\tclass DynamicThresholdDetectHandler(BaseHandler):\n\t    \"\"\"\n", "    Handles detection of a single dimension value decrease.\n\t    \"\"\"\n\t    def run(self) -> Response4AD:\n\t        \"\"\"\n\t        Runs the DataDrivenModule pipeline and returns the result.\n\t        :return: A Response4AD object containing the result of the pipeline.\n\t        \"\"\"\n\t        dd = DynamicThresholdModule(self.req)\n\t        dd.pipeline()\n\t        return dd.resp\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "handlers/__init__.py", "chunked_list": []}
{"filename": "handlers/run_main.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'run_detector'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/2/1 16:25'\n\t__info__ =\n\t\"\"\"\n\tfrom common.classes import Request4AD\n\tfrom common.request_builder import RequestBuilder\n\tfrom handlers.detect_handlers import ColdStartDetectHandler, DynamicThresholdDetectHandler\n", "def run_main(body):\n\t    \"\"\"\n\t    Runs the detection pipeline on the input request body.\n\t    :param body: A dictionary containing data to be processed\n\t    :return: A string message containing the results of the detection pipeline\n\t    \"\"\"\n\t    # Builds a request object from the input body\n\t    req = RequestBuilder(body).build_req()\n\t    # Maps the request to the appropriate handler based on the data by day\n\t    target_handler = handler_mapper(req=req)\n", "    # Runs the detection pipeline using the target handler\n\t    resp = target_handler(req).run()\n\t    # Returns the result message from the response\n\t    return resp.get_msg()\n\tdef handler_mapper(req: Request4AD):\n\t    \"\"\"\n\t    Maps the request to the appropriate handler based on the data by day\n\t    \"\"\"\n\t    if len(req.data_by_day) == 1:\n\t        # Use ColdStartDetectHandler for single-day data\n", "        return ColdStartDetectHandler\n\t    elif len(req.data_by_day) > 1:\n\t        # Use DynamicThresholdDetectHandler for multi-day data\n\t        return DynamicThresholdDetectHandler\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/sensitivity.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'sensitivity_mapper'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/18 15:17'\n\t__info__ =\n\t\"\"\"\n\t# todo\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/pipeline.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'pipeline'\n\t__author__ = 'LuYuan'\n\t__time__ = '2022/11/7 17:44'\n\t__info__ = pipeLine\n\t\"\"\"\n\timport collections\n\tfrom common.classes import StatusInOut\n\tclass DetectPipeLine:\n", "    def __init__(self, status: StatusInOut):\n\t        \"\"\"\n\t        Initializes the DetectPipeLine object with a given status.\n\t        :param status: The initial status for the pipeline\n\t        \"\"\"\n\t        self.q = None\n\t        self.status = status\n\t    def start(self, func):\n\t        \"\"\"\n\t        Initializes the pipeline queue with the given function.\n", "        :param func: The function to be added to the queue\n\t        :return: None\n\t        \"\"\"\n\t        self.q = collections.deque()\n\t        self.q.append(func)\n\t    def add(self, func):\n\t        \"\"\"\n\t        Adds a function to the end of the pipeline queue.\n\t        :param func: The function to be added to the queue\n\t        :return: None\n", "        \"\"\"\n\t        self.q.append(func)\n\t    def handler(self):\n\t        \"\"\"\n\t        Iterates through the pipeline queue, passing the status object to each function and updating the status object.\n\t        :return: None\n\t        \"\"\"\n\t        while self.q:\n\t            self.status = self.q[0](self.status)\n\t            self.q.popleft()\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/__init__.py", "chunked_list": []}
{"filename": "algorithm/module.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'abnormal_detect'\n\t__author__ = 'LuYuan'\n\t__time__ = '2021-08-09 11:52'\n\t__product_name = PyCharm\n\t__info__: algo module\n\t\"\"\"\n\tfrom abc import ABC\n\tfrom common.classes import Response4AD, Request4AD, StatusInOut\n", "from common.logger import Logger\n\tclass BaseModule(ABC):\n\t    def __init__(self, req: Request4AD):\n\t        \"\"\"\n\t        Initializes the BaseModule object with a given request object, response object, logger object, and run success flag.\n\t        :param req: The initial request object for the module\n\t        \"\"\"\n\t        self.run_success = True\n\t        self.req = req\n\t        self.resp = Response4AD()\n", "        self.logger = Logger.get_logger()\n\t    @staticmethod\n\t    def pipeline():\n\t        \"\"\"The core processing pipeline for the module\"\"\"\n\t    @staticmethod\n\t    def filter(status: StatusInOut) -> StatusInOut:\n\t        \"\"\"Noise filter\"\"\"\n\t    @staticmethod\n\t    def detector(status: StatusInOut) -> StatusInOut:\n\t        \"\"\"Abnormal detector\"\"\"\n", "    @staticmethod\n\t    def msg_builder(status: StatusInOut) -> StatusInOut:\n\t        \"\"\"Alarm information builder\"\"\"\n\t    @staticmethod\n\t    def to_resp(status: StatusInOut) -> StatusInOut:\n\t        \"\"\"Output the result\"\"\"\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/dt_module.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'outlier_detector'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/13 15:43'\n\t__info__ =\n\t\"\"\"\n\tfrom algorithm.module import BaseModule\n\tfrom algorithm.pipeline import DetectPipeLine\n\tfrom algorithm.dyn_thresh.dyn_thresh_detector import DynamicThresholdDetector\n", "from algorithm.dyn_thresh.rule_checker import RuleChecker\n\tfrom common.classes import StatusInOut\n\tclass DynamicThresholdModule(BaseModule):\n\t    def pipeline(self):\n\t        \"\"\"\n\t        The core processing pipeline for the DynamicThresholdModule\n\t        @return:\n\t        \"\"\"\n\t        status = StatusInOut()\n\t        try:\n", "            dpp = DetectPipeLine(status)\n\t            dpp.start(self.detector)\n\t            dpp.add(self.filter)\n\t            dpp.add(self.msg_builder)\n\t            dpp.add(self.to_resp)\n\t            dpp.handler()\n\t        except Exception as e:\n\t            self.run_success = False\n\t            self.logger.info(f\"traceId: {self.req.trace_id}, runSuccess: {self.run_success}, errorInfo: {e}\")\n\t        finally:\n", "            self.logger.info(f\"traceId: {self.req.trace_id}, runSuccess: {self.run_success}\")\n\t    def detector(self, status: StatusInOut) -> StatusInOut:\n\t        \"\"\"\n\t        Detects anomalies in the input data and sets the alarmOrNot flag in the status object\n\t        :param status: The current status object\n\t        :return: The updated status object\n\t        \"\"\"\n\t        detect_data = self.req.data_by_day.get(\"0\")\n\t        train_data = {k: v for k, v in self.req.data_by_day.items() if k != \"0\"}\n\t        rule_result = RuleChecker(detect_data, self.req).detector()\n", "        if rule_result:\n\t            status.alarmOrNot = rule_result\n\t            return status\n\t        dt = DynamicThresholdDetector(detect_data, train_data, self.req.detect_info.algorithm_type).run()\n\t        if dt:\n\t            status.alarmOrNot = dt\n\t            status.needNext = True\n\t        return status\n\t    def filter(self, status: StatusInOut) -> StatusInOut:\n\t        \"\"\"\n", "        Filters out false positives in the input data\n\t        :param status: The current status object\n\t        :return: The updated status object\n\t        \"\"\"\n\t        if status.needNext is False:\n\t            return status\n\t        detect_data = self.req.data_by_day.get(\"0\")\n\t        rre = RuleChecker(detect_data, self.req).filter()\n\t        if rre:\n\t            status.alarmOrNot = False\n", "            status.needNext = False\n\t        return status\n\t    def msg_builder(self, status: StatusInOut) -> StatusInOut:\n\t        \"\"\"\n\t        Builds the alarm message for the input data\n\t        :param status: The current status object\n\t        :return: The updated status object\n\t        \"\"\"\n\t        return status\n\t    def to_resp(self, status):\n", "        \"\"\"\n\t        Returns the final response object based on the input data and the status object\n\t        :param status: The current status object\n\t        :return: The updated status object\n\t        \"\"\"\n\t        if status.alarmOrNot:\n\t            self.resp.isException = True\n\t        return status\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/cs_module.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'cold_start_algo'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/13 10:19'\n\t__info__ =\n\t\"\"\"\n\tfrom algorithm.module import BaseModule\n\tfrom algorithm.cold_start.diff_outlier_detector import DiffOutlierDetector\n\tfrom algorithm.cold_start.rule_checker import RuleChecker\n", "from algorithm.cold_start.similarity_filter import SimilarityFilter\n\tfrom algorithm.pipeline import DetectPipeLine\n\tfrom common.classes import StatusInOut\n\tclass ColdStartModule(BaseModule):\n\t    def pipeline(self):\n\t        \"\"\"\n\t        The core processing pipeline for the ColdStartModule\n\t        @return:\n\t        \"\"\"\n\t        status = StatusInOut()\n", "        try:\n\t            dpp = DetectPipeLine(status)\n\t            dpp.start(self.detector)\n\t            dpp.add(self.filter)\n\t            dpp.add(self.msg_builder)\n\t            dpp.add(self.to_resp)\n\t            dpp.handler()\n\t        except Exception as e:\n\t            self.run_success = False\n\t            self.logger.info(f\"traceId: {self.req.trace_id}, runSuccess: {self.run_success}, errorInfo: {e}\")\n", "        finally:\n\t            self.logger.info(f\"traceId: {self.req.trace_id}, runSuccess: {self.run_success}\")\n\t    def detector(self, status: StatusInOut) -> StatusInOut:\n\t        \"\"\"\n\t        Detects anomalies in the input data and sets the alarmOrNot flag in the status object\n\t        :param status: The current status object\n\t        :return: The updated status object\n\t        \"\"\"\n\t        detect_data = self.req.data_by_day.get(\"0\")\n\t        rule_result = RuleChecker(detect_data, self.req).detector()\n", "        if rule_result:\n\t            status.alarmOrNot = rule_result\n\t            return status\n\t        p_d_a_d = DiffOutlierDetector(detect_data, self.req.detect_info.algorithm_type)\n\t        re = p_d_a_d.run()\n\t        if re:\n\t            status.alarmOrNot = re\n\t            status.needNext = True\n\t            status.duration = p_d_a_d.real_duration\n\t        return status\n", "    def filter(self, status: StatusInOut) -> StatusInOut:\n\t        \"\"\"\n\t        Filters out false positives in the input data\n\t        :param status: The current status object\n\t        :return: The updated status object\n\t        \"\"\"\n\t        if status.needNext is False:\n\t            return status\n\t        detect_data = self.req.data_by_day.get(\"0\")\n\t        sre = SimilarityFilter(detect_data, self.req.detect_info.algorithm_type, status.duration).run()\n", "        rre = RuleChecker(detect_data, self.req).filter(status.duration)\n\t        if sre or rre:\n\t            status.alarmOrNot = False\n\t            status.needNext = False\n\t        return status\n\t    def msg_builder(self, status: StatusInOut) -> StatusInOut:\n\t        \"\"\"\n\t        Builds the alarm message for the input data\n\t        :param status: The current status object\n\t        :return: The updated status object\n", "        \"\"\"\n\t        return status\n\t    def to_resp(self, status):\n\t        \"\"\"\n\t        Returns the final response object based on the input data and the status object\n\t        :param status: The current status object\n\t        :return: The updated status object\n\t        \"\"\"\n\t        if status.alarmOrNot:\n\t            self.resp.isException = True\n", "        return status\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/cold_start/similarity_filter.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'outlier_detector'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/13 15:43'\n\t__info__ =\n\t\"\"\"\n\tfrom typing import List\n\tfrom common.constants import Constants\n\tfrom common.utils import Utils\n", "RATE = 2\n\tclass SimilarityFilter:\n\t    def __init__(self, detect_data: List[float], algorithm_type: str, anomaly_duration: int):\n\t        self.algorithm_type = algorithm_type\n\t        self.detect_data = self.minus_data(detect_data)\n\t        self.anomaly_duration = anomaly_duration\n\t    def run(self):\n\t        \"\"\"\n\t        Check if the current data is similar to the historical data.\n\t        :return: True if the current data is similar to the historical data.\n", "        \"\"\"\n\t        agg_list = Utils.agg_diff_fe_calc(self.detect_data, self.anomaly_duration)\n\t        if agg_list[-1] < RATE * min(agg_list[:-self.anomaly_duration]):\n\t            return False\n\t        return True\n\t    def minus_data(self, input_data: List[float]) -> List[float]:\n\t        \"\"\"\n\t        If the algorithm is \"up\", invert the input data.\n\t        :param input_data: List of input data.\n\t        :return: List of input data with inverted values if the algorithm is \"up\".\n", "        \"\"\"\n\t        if self.algorithm_type == Constants.ALGORITHM_TYPE_UP.value:\n\t            return [-value for value in input_data]\n\t        return input_data\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/cold_start/diff_outlier_detector.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'outlier_detector'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/13 15:43'\n\t__info__ =\n\t\"\"\"\n\timport numpy as np\n\tfrom typing import List\n\tfrom common.constants import Constants\n", "from common.utils import Utils\n\tclass DiffOutlierDetector:\n\t    def __init__(self, detect_data: List[float], algorithm_type: str):\n\t        self.algorithm_type = algorithm_type\n\t        self.detect_data = self.minus_data(detect_data)\n\t        self.default_point = 4\n\t        self.alarm_last_time = 15\n\t        self.tk_delta = 2.0\n\t        self.default_duration = 1\n\t        # output\n", "        self.real_duration = 0\n\t    def run(self):\n\t        \"\"\"\n\t        Detect an anomaly using the previous difference.\n\t        :return: True if an anomaly is detected.\n\t        \"\"\"\n\t        potential_indexes, down_threshold = self.prev_diff_outlier(self.detect_data)\n\t        if len(potential_indexes) == 0 or potential_indexes is None:\n\t            return False\n\t        for cur_index in potential_indexes:\n", "            self.real_duration = len(self.detect_data) - cur_index\n\t            pre = self.detect_data[cur_index - self.real_duration: cur_index]\n\t            post = self.detect_data[-self.real_duration:]\n\t            real_threshold = max(np.median(pre) + down_threshold, self.detect_data[-self.real_duration - 1])\n\t            if max(post) < real_threshold:\n\t                if self.real_duration >= self.default_duration:\n\t                    return True\n\t        return False\n\t    def prev_diff_outlier(self, detect_data: List[float]):\n\t        \"\"\"\n", "        Calculate the potential indexes of anomalies and the down threshold for the previous difference.\n\t        :param detect_data: List of data to detect anomalies from.\n\t        :return: A tuple of the potential indexes of anomalies and the down threshold for the previous difference.\n\t        \"\"\"\n\t        detect_data_diff = Utils().diff_feature_calc(detect_data, self.default_point)\n\t        down_threshold = Utils.turkey_box_plot(detect_data_diff, self.tk_delta)[3]\n\t        cp_indexes = []\n\t        for index, value in enumerate(detect_data_diff):\n\t            if value < down_threshold:\n\t                cp_indexes.append(index)\n", "        cp_indexes = [c_i for c_i in cp_indexes if c_i > len(detect_data) - self.alarm_last_time]\n\t        return cp_indexes, down_threshold\n\t    def minus_data(self, input_data: List[float]) -> List[float]:\n\t        \"\"\"\n\t        Invert the input data if the algorithm is \"up\".\n\t        :param input_data: List of input data.\n\t        :return: List of input data with inverted values if the algorithm is \"up\".\n\t        \"\"\"\n\t        if self.algorithm_type == Constants.ALGORITHM_TYPE_UP.value:\n\t            return [-value for value in input_data]\n", "        return input_data\n\t    def set_default_duration(self, input_duration):\n\t        \"\"\"\n\t        Set the default duration for an anomaly.\n\t        :param input_duration: The duration to set as default.\n\t        \"\"\"\n\t        self.default_duration = input_duration\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/cold_start/__init__.py", "chunked_list": []}
{"filename": "algorithm/cold_start/rule_checker.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'rule_filter'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/14 10:14'\n\t__info__ =\n\t\"\"\"\n\timport numpy as np\n\tfrom typing import List\n\tfrom common.classes import Request4AD\n", "from common.constants import Constants\n\tclass RuleChecker:\n\t    def __init__(self, detect_data: List[float], req: Request4AD):\n\t        self.req = req\n\t        self.detect_data = detect_data\n\t        self.up = self.req.rule_info.up_threshold\n\t        self.down = self.req.rule_info.down_threshold\n\t        self.algorithm_type = self.req.detect_info.algorithm_type\n\t    def detector(self):\n\t        \"\"\"\n", "        Excessive alarm detection\n\t        :return: Boolean indicating if the data exceeds the threshold\n\t        \"\"\"\n\t        if self.algorithm_type == Constants.ALGORITHM_TYPE_UP.value:\n\t            if self.detect_data[-1] > self.up:\n\t                return True\n\t        elif self.algorithm_type == Constants.ALGORITHM_TYPE_DOWN.value:\n\t            if self.detect_data[-1] < self.down:\n\t                return True\n\t        return False\n", "    def filter(self, duration):\n\t        \"\"\"\n\t        Rule filtering\n\t        :return: Boolean indicating if the data violates the rules\n\t        \"\"\"\n\t        custom_change_rate = self.req.rule_info.change_rate\n\t        post = self.detect_data[-duration:]\n\t        pre = self.detect_data[-2 * duration: - duration]\n\t        if self.algorithm_type == \"down\":\n\t            real_change_rate = 1.0 if max(pre) == 0 else -(min(post) - max(pre)) / max(pre)\n", "        else:\n\t            real_change_rate = 1.0 if np.percentile(pre, 90) == 0 else (max(post) - np.percentile(pre, 90)) \\\n\t                                                                       / np.percentile(pre, 90)\n\t        if custom_change_rate > real_change_rate:\n\t            return True\n\t        if self.algorithm_type == Constants.ALGORITHM_TYPE_UP.value and self.detect_data[-1] < self.down:\n\t            return True\n\t        elif self.algorithm_type == Constants.ALGORITHM_TYPE_DOWN.value and self.detect_data[-1] > self.up:\n\t            return True\n\t        return False\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/dyn_thresh/__init__.py", "chunked_list": []}
{"filename": "algorithm/dyn_thresh/rule_checker.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'rule_filter'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/14 10:14'\n\t__info__ =\n\t\"\"\"\n\timport numpy as np\n\tfrom typing import List\n\tfrom common.classes import Request4AD\n", "from common.constants import Constants\n\tclass RuleChecker:\n\t    def __init__(self, detect_data: List[float], req: Request4AD):\n\t        self.req = req\n\t        self.detect_data = detect_data\n\t        self.up = self.req.rule_info.up_threshold\n\t        self.down = self.req.rule_info.down_threshold\n\t        self.algorithm_type = self.req.detect_info.algorithm_type\n\t    def detector(self):\n\t        \"\"\"\n", "        Excessive alarm detection\n\t        :return: Boolean indicating if the data exceeds the threshold\n\t        \"\"\"\n\t        if self.algorithm_type == Constants.ALGORITHM_TYPE_UP.value:\n\t            if self.detect_data[-1] > self.up:\n\t                return True\n\t        elif self.algorithm_type == Constants.ALGORITHM_TYPE_DOWN.value:\n\t            if self.detect_data[-1] < self.down:\n\t                return True\n\t        return False\n", "    def filter(self):\n\t        \"\"\"\n\t        Rule filtering\n\t        :return: Boolean indicating if the data violates the rules\n\t        \"\"\"\n\t        if self.algorithm_type == Constants.ALGORITHM_TYPE_UP.value and self.detect_data[-1] < self.down:\n\t            return True\n\t        elif self.algorithm_type == Constants.ALGORITHM_TYPE_DOWN.value and self.detect_data[-1] > self.up:\n\t            return True\n\t        custom_change_rate = self.req.rule_info.change_rate\n", "        train_data = {k: v for k, v in self.req.data_by_day.items() if k != \"0\"}\n\t        compare_values = []\n\t        for k, v in train_data.items():\n\t            compare_values.append(v[-1])\n\t        baseline = np.max(compare_values)\n\t        if baseline == 0:\n\t            return True\n\t        if self.algorithm_type == \"down\":\n\t            if custom_change_rate > -(self.detect_data[-1] - baseline) / baseline:\n\t                return True\n", "        else:\n\t            if custom_change_rate > (self.detect_data[-1] - baseline) / baseline:\n\t                return True\n\t        return False\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/dyn_thresh/dyn_thresh_detector.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'anomaly_detector'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/17 13:35'\n\t__info__ =\n\t\"\"\"\n\tfrom typing import List, Dict\n\tfrom algorithm.dyn_thresh.dyn_thresh_algo.features import Features\n\tfrom algorithm.dyn_thresh.dyn_thresh_algo.threshold import ThresholdCalc\n", "from common.constants import Constants\n\tfrom common.utils import Utils\n\tclass DynamicThresholdDetector:\n\t    def __init__(self, detect_data: List[float], train_data: Dict[str, List[float]], algorithm_type: str):\n\t        self.algorithm_type = algorithm_type\n\t        self.detect_data = detect_data\n\t        self.train_data = train_data\n\t        self.minus_data()\n\t        self.smoothness = True\n\t    def run(self):\n", "        \"\"\"\n\t        Detect an anomaly using the dynamic threshold algo.\n\t        :return: True if an anomaly is detected.\n\t        \"\"\"\n\t        fe = Features(self.train_data, self.algorithm_type)\n\t        features = fe.run()\n\t        self.smoothness = fe.smoothness\n\t        is_down = True if self.algorithm_type == \"down\" else False\n\t        if self.smoothness:\n\t            for k, v in features.items():\n", "                cur_fe = Utils.diff_percentile_func(self.detect_data, int(k), is_down)[-1]\n\t                target_th = ThresholdCalc(v).run()\n\t                if cur_fe < target_th:\n\t                    return True\n\t        else:\n\t            target_th = ThresholdCalc(features).run()\n\t            if self.detect_data[-1] < target_th:\n\t                return True\n\t        return False\n\t    def minus_data(self):\n", "        \"\"\"\n\t        Invert the input data if the algorithm is \"up\".\n\t        :return: None\n\t        \"\"\"\n\t        if self.algorithm_type == Constants.ALGORITHM_TYPE_UP.value:\n\t            self.detect_data = [-value for value in self.detect_data]\n\t            new_train_data = {}\n\t            for k, v in self.train_data.items():\n\t                new_train_data[k] = [-value for value in v]\n\t            self.train_data = new_train_data\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/dyn_thresh/dyn_thresh_algo/events.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'DynamicThreshold'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/11 17:37'\n\t__info__ =\n\t\"\"\"\n\timport numpy as np\n\timport pandas as pd\n\tfrom typing import Dict, List\n", "from pandas import DataFrame\n\tfrom algorithm.dyn_thresh.dyn_thresh_algo.node import Node\n\tfrom common.utils import Utils\n\tNEIGHBOR = 30\n\tTHRESHOLD_LEVEL_RATE = 0.2\n\tclass PeriodicEventDetector:\n\t    def __init__(self, data_by_day: Dict[str, List[float]], steps: int, init_per: float, similar_index: int,\n\t                 cont_len: int):\n\t        self.dod_data = data_by_day\n\t        self.history_keys = list(self.dod_data.keys())\n", "        self.data_len = None\n\t        self.steps = steps\n\t        self.init_per = init_per\n\t        self.similar_index = similar_index\n\t        self.cont_len = cont_len\n\t        #\n\t        self.neighbor = NEIGHBOR\n\t        self.threshold_level_rate = THRESHOLD_LEVEL_RATE\n\t        # out_put\n\t        self.th_list = []\n", "    def run(self):\n\t        df = pd.DataFrame.from_dict(self.dod_data, orient=\"index\")\n\t        self.data_len = df.shape[1]\n\t        min_value, max_value = np.percentile(df.values, 0), np.percentile(df.values, self.init_per)\n\t        if min_value == max_value:\n\t            self.th_list = [min_value]\n\t            return [Node(level=-1, left=0, right=self.data_len - 1)]\n\t        th_list = [min_value - i * (min_value - max_value) / self.steps for i in range(self.steps)]\n\t        self.th_list = th_list\n\t        events = self.find_periodic_events(df, th_list)\n", "        return events\n\t    def find_periodic_events(self, df: DataFrame, th_list: List[float]) -> List[float]:\n\t        \"\"\"\n\t        Identify periodic events.\n\t        @param df: The raw data.\n\t        @param th_list: A list of threshold values to use for slicing the data.\n\t        @return: A list of periodic events.\n\t        \"\"\"\n\t        pre_level_nodes = []\n\t        for i, cur_th in enumerate(th_list):\n", "            raw_nodes = self.raw_nodes_search(df, cur_th, i)\n\t            if len(raw_nodes) == 0:\n\t                continue\n\t            raw_nodes_with_parents = self.node_parents_update(raw_nodes, pre_level_nodes)\n\t            cur_level_nodes = []\n\t            for r_node in raw_nodes_with_parents:\n\t                if not r_node.parents:\n\t                    cur_level_nodes.append(r_node)\n\t                elif len(r_node.parents) == 1:\n\t                    mid_left_nodes = self.modify_node_boundary(r_node, 0)\n", "                    mid_right_nodes = self.modify_node_boundary(mid_left_nodes[-1], -1)\n\t                    if len(mid_left_nodes) == 1:\n\t                        nodes_with_processed = mid_right_nodes\n\t                    else:\n\t                        nodes_with_processed = [mid_left_nodes[0]] + mid_right_nodes\n\t                    cur_level_nodes += nodes_with_processed\n\t                elif len(r_node.parents) > 1:\n\t                    processed_node_cluster = self.nodes_process(r_node, self.node_cluster(r_node.parents))\n\t                    cur_level_nodes += processed_node_cluster\n\t            pre_level_nodes = cur_level_nodes\n", "        return pre_level_nodes\n\t    def raw_nodes_search(self, df: DataFrame, cur_th: float, level: int) -> List[Node]:\n\t        \"\"\"\n\t        Obtain raw nodes by dividing the data according to the specified threshold.\n\t        @param df: The raw data.\n\t        @param cur_th: The current threshold.\n\t        @param level: The current level of the tree.\n\t        @return: A list of raw nodes.\n\t        \"\"\"\n\t        outlier_list = (df < cur_th).sum(axis=0)\n", "        duration = Utils.longest_continuous(outlier_list.tolist(), 0)\n\t        if duration < self.cont_len:\n\t            return []\n\t        raw_nodes = self.get_raw_nodes(outlier_list=outlier_list.tolist(),\n\t                                       count=max(1, int(len(self.history_keys) / self.similar_index)),\n\t                                       level=level,\n\t                                       neighbor=self.neighbor)\n\t        return raw_nodes\n\t    def nodes_process(self, completed_node: Node, node_clu_list: List[List[Node]]) -> List[Node]:\n\t        \"\"\"\n", "        Split/merge nodes with multiple parents; split the two sides of the overall node!\n\t        Splitting scenario: for example, when two parents are very large in size.\n\t        @param completed_node: The entire Node after a split.\n\t        @param node_clu_list: Clustering of parents corresponding to the overall node according to certain rules.\n\t        @return: A list of processed nodes.\n\t        \"\"\"\n\t        processed_node_cluster = []\n\t        for n_list in node_clu_list:\n\t            processed_node_cluster += self.node_process_within_group(n_list)\n\t        processed_node_cluster = self.node_process_between_group(processed_node_cluster)\n", "        if completed_node.left != processed_node_cluster[0].left:\n\t            new_nodes = self.modify_node_boundary(\n\t                Node(level=completed_node.level, left=completed_node.left, right=processed_node_cluster[0].right,\n\t                     parents=[processed_node_cluster[0]]), 0)\n\t            processed_node_cluster = new_nodes + processed_node_cluster[1:]\n\t        else:\n\t            copy_node = processed_node_cluster[0].copy_node()\n\t            copy_node.parents = [processed_node_cluster[0]]\n\t            processed_node_cluster[0] = copy_node\n\t        if completed_node.right != processed_node_cluster[-1].right:\n", "            new_nodes = self.modify_node_boundary(\n\t                Node(level=completed_node.level, left=processed_node_cluster[-1].left, right=completed_node.right,\n\t                     parents=[processed_node_cluster[-1]]), -1)\n\t            processed_node_cluster = processed_node_cluster[:-1] + new_nodes\n\t        else:\n\t            copy_node = processed_node_cluster[-1].copy_node()\n\t            copy_node.parents = [processed_node_cluster[-1]]\n\t            processed_node_cluster[-1] = copy_node\n\t        for p_n in processed_node_cluster:\n\t            p_n.level = completed_node.level\n", "        return processed_node_cluster\n\t    def node_process_within_group(self, node_list: List[Node]) -> List[Node]:\n\t        \"\"\"\n\t        The input is a clustered list of nodes with levels within a certain range,\n\t        arranged from left to right by default.\n\t        @param node_list: A list of nodes with the same parent.\n\t        @return: A list of processed nodes.\n\t        \"\"\"\n\t        new_node_list = [node_list[0]]  # todo\n\t        for node in node_list[1:]:\n", "            l1, l2 = new_node_list[-1].drill_down_to_node(-1).level, node.drill_down_to_node(0).level\n\t            pre_r_index, post_l_index = new_node_list[-1].right, node.left\n\t            if post_l_index - pre_r_index > 1:\n\t                if node.level - max(l1, l2) < int(self.threshold_level_rate * self.steps):\n\t                    new_node_list[-1] = self.node_merge(new_node_list[-1], node)\n\t                else:\n\t                    mid_node = Node(level=node.level, left=pre_r_index + 1, right=post_l_index - 1)\n\t                    if l1 > l2:\n\t                        new_node_list[-1] = self.node_merge(new_node_list[-1], mid_node)\n\t                        new_node_list.append(node)\n", "                    else:\n\t                        new_node_list.append(self.node_merge(mid_node, node))\n\t            else:\n\t                new_node_list[-1] = self.node_merge(new_node_list[-1], node)\n\t        return new_node_list\n\t    @staticmethod\n\t    def node_process_between_group(node_list: List[Node]) -> List[Node]:\n\t        \"\"\"\n\t        The input is a list of processed nodes between clustered groups,\n\t        arranged from left to right by default.\n", "        @param node_list: A list of nodes with the same parent.\n\t        @return: A list of processed nodes.\n\t        \"\"\"\n\t        new_node_list = [node_list[0]]\n\t        for node in node_list[1:]:\n\t            pre_r_index, post_l_index = new_node_list[-1].right, node.left\n\t            if post_l_index - pre_r_index > 1:\n\t                mid_node = Node(level=node.level, left=pre_r_index + 1, right=post_l_index - 1)\n\t                new_node_list.append(mid_node)\n\t                new_node_list.append(node)\n", "            else:\n\t                new_node_list.append(node)\n\t        return new_node_list\n\t    def modify_node_boundary(self, node: Node, pos: int) -> List[Node]:\n\t        \"\"\"\n\t        Boundary processing.\n\t        @param node: The node to be modified.\n\t        @param pos: If pos=0, modify the left boundary; if pos=-1, modify the right boundary.\n\t        @return: A list of modified nodes.\n\t        \"\"\"\n", "        if not node.parents:\n\t            return [node]\n\t        if node.level - node.drill_down_to_node(pos).level < int(self.threshold_level_rate * self.steps):\n\t            return [node]\n\t        if pos == 0:\n\t            if node.left == node.parents[0].left:\n\t                return [node]\n\t            else:\n\t                ts = node.drill_down_to_level(pos)\n\t                ts.reverse()\n", "                if self.change_point_detect(ts, pos):\n\t                    new_node_list = [\n\t                        Node(level=node.level, left=node.left, right=node.parents[0].left - 1)]\n\t                    node.left = node.parents[0].left\n\t                    new_node_list.append(node)\n\t                    return new_node_list\n\t        elif pos == -1:\n\t            if node.right == node.parents[-1].right:\n\t                return [node]\n\t            else:\n", "                ts = node.drill_down_to_level(pos)\n\t                ts.reverse()\n\t                if self.change_point_detect(ts, pos):\n\t                    new_node_list = [\n\t                        Node(level=node.level, left=node.left, right=node.parents[-1].right,\n\t                             parents=node.parents),\n\t                        Node(level=node.level, left=node.parents[-1].right + 1, right=node.right)]\n\t                    return new_node_list\n\t        return [node]\n\t    def get_raw_nodes(self, level: int, outlier_list: List[int], count: int, neighbor: int) -> List[Node]:\n", "        \"\"\"\n\t        Select potential event nodes based on specific conditions.\n\t        @param level: The current level.\n\t        @param outlier_list: The list of outlier points.\n\t        @param count: Condition 1.\n\t        @param neighbor: Condition 2.\n\t        @return: A list of potential event nodes.\n\t        \"\"\"\n\t        event_clusters = self.event_cluster(outlier_list, count, neighbor)\n\t        if len(event_clusters) == 0:\n", "            return []\n\t        node_list = []\n\t        for clu in event_clusters:\n\t            node_list.append(Node(level=level, left=clu[0][0], right=clu[-1][0]))  # 初始parents为空\n\t        return node_list\n\t    @staticmethod\n\t    def node_parents_update(raw_nodes: List[Node], pre_level_nodes: List[Node]) -> List[Node]:\n\t        \"\"\"\n\t        Find the parents of each raw_node.\n\t        @param raw_nodes: A list of raw nodes.\n", "        @param pre_level_nodes: A list of nodes from the previous level.\n\t        @return: A list of nodes with updated parent information.\n\t        \"\"\"\n\t        for en in raw_nodes:\n\t            for f_en in pre_level_nodes:\n\t                en.add_parent(f_en)\n\t        return raw_nodes\n\t    @staticmethod\n\t    def node_merge(pre_node: Node, post_node: Node) -> Node:\n\t        \"\"\"\n", "        Merge two nodes in order.\n\t        @param pre_node: The previous node.\n\t        @param post_node: The next node.\n\t        @return: The merged node.\n\t        \"\"\"\n\t        node = Node(level=None, left=pre_node.left, right=post_node.right,\n\t                    parents=[pre_node, post_node])\n\t        return node\n\t    @staticmethod\n\t    def node_cluster(lst: List[Node]) -> List[List[Node]]:\n", "        \"\"\"\n\t        Cluster the nodes, preserving the original level.\n\t        @param lst: A list of nodes.\n\t        @return: A list of clustered nodes.\n\t        \"\"\"\n\t        if not lst:\n\t            return []\n\t        result = []\n\t        i = 0\n\t        while i < len(lst):\n", "            j = i\n\t            while j < len(lst) - 1 and abs(\n\t                    lst[i].drill_down_to_node(-1).level - lst[j + 1].drill_down_to_node(0).level) < 10:\n\t                j += 1\n\t            result.append(lst[i:j + 1])  # todo 父节点\n\t            i = j + 1\n\t        return result\n\t    @staticmethod\n\t    def change_point_detect(ts, pos) -> bool:\n\t        \"\"\"\n", "        Find the change point!\n\t        @param pos: The position to check for a change point.\n\t        @param ts: The time series data.\n\t        @return: True if there is a change point at the specified position, False otherwise.\n\t        \"\"\"\n\t        for i in range(1, 3, 1):\n\t            diffs = Utils().diff_feature_calc(ts, i)\n\t            if pos == 0:\n\t                down_threshold = Utils.turkey_box_plot(diffs[:-1], 2)[3]\n\t                if diffs[-1] < down_threshold and diffs[-1] < min(diffs[:-1]):\n", "                    return True\n\t            elif pos == -1:\n\t                up_threshold = Utils.turkey_box_plot(diffs[:-1], 2)[4]\n\t                if diffs[-1] > up_threshold and diffs[-1] > max(diffs[:-1]):\n\t                    return True\n\t        return False\n\t    @staticmethod\n\t    def event_cluster(lst, count: int, interval):\n\t        \"\"\"\n\t        Cluster events!\n", "        @param lst: The list of events to cluster.\n\t        @param count: The maximum number of clusters to create.\n\t        @param interval: The time interval to use for clustering.\n\t        @return: The clustered events as a list of lists.\n\t        \"\"\"\n\t        clu = []\n\t        current_cluster = []\n\t        i = 0\n\t        while i < len(lst):\n\t            if len(current_cluster) == 0:\n", "                if lst[i] >= count:  # fixme\n\t                    current_cluster.append((i, lst[i]))\n\t            else:\n\t                start_loc = current_cluster[-1][0] + 1\n\t                end_loc = min(start_loc + interval, len(lst))\n\t                slice_lst = lst[start_loc:end_loc]\n\t                slice_idx = [start_loc + j for j in range(len(slice_lst)) if slice_lst[j] >= count]\n\t                if slice_idx:\n\t                    current_cluster += [(k, lst[k]) for k in slice_idx]\n\t                else:\n", "                    clu.append(current_cluster)\n\t                    current_cluster = []\n\t                    i = end_loc - 1\n\t            i += 1\n\t        if current_cluster:\n\t            clu.append(current_cluster)\n\t        return clu\n\t    def set_neighbor_len(self, neighbor):\n\t        \"\"\"\n\t        Set the length of the neighbor list.\n", "        @param neighbor: The length of the neighbor list.\n\t        @return: None\n\t        \"\"\"\n\t        self.neighbor = neighbor\n\t    def set_threshold_level_rate(self, threshold_level_rate):\n\t        \"\"\"\n\t        Set the threshold level rate.\n\t        @param threshold_level_rate: The threshold level rate.\n\t        @return: None\n\t        \"\"\"\n", "        self.threshold_level_rate = threshold_level_rate\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/dyn_thresh/dyn_thresh_algo/node.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'node.py'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/11 17:36'\n\t__info__ = 'Define the core class Node and implement functions for parent-child relationships, drilling down, copying,\n\t            and matching nodes in an interval tree.'\n\t\"\"\"\n\tclass Node:\n\t    \"\"\"\n", "    Core class representing a node in an interval tree.\n\t    \"\"\"\n\t    def __init__(self, level, left, right, parents=None):\n\t        \"\"\"\n\t        Initialize a new Node with the given level, left endpoint, right endpoint, and parent nodes.\n\t        :param level: The current level of the node.\n\t        :param left: The left endpoint of the interval (closed interval).\n\t        :param right: The right endpoint of the interval (closed interval).\n\t        :param parents: A list of parent nodes, with the left parent at index 0 and the right parent at index -1.\n\t                        Defaults to an empty list.\n", "        \"\"\"\n\t        if parents is None:\n\t            parents = []\n\t        self.level = level\n\t        self.left = left\n\t        self.right = right\n\t        self.parents = parents\n\t    def add_parent(self, parent_node):\n\t        \"\"\"\n\t        Add a parent node to the node's list of parent nodes if the parent node fully contains the current node.\n", "        :param parent_node: The parent node to add.\n\t        :return: None.\n\t        \"\"\"\n\t        if self.left <= parent_node.left and self.right >= parent_node.right:\n\t            self.parents.append(parent_node)\n\t    def drill_down_to_node(self, direction):\n\t        \"\"\"\n\t        Drill down from the current node to the node in the specified direction.\n\t        :param direction: The direction to drill down, with direction=0 indicating the left direction and direction=-1\n\t                          indicating the right direction.\n", "        :return: The node in the specified direction.\n\t        \"\"\"\n\t        current_node = self\n\t        while current_node.parents:\n\t            current_node = current_node.parents[direction]\n\t        return current_node\n\t    def drill_down_to_level(self, direction):\n\t        \"\"\"\n\t        Drill down from the current node to the level in the specified direction.\n\t        :param direction: The direction to drill down, with direction=0 indicating the left direction and direction=-1\n", "        indicating the right direction.\n\t        :return: The level in the specified direction.\n\t        \"\"\"\n\t        def get_endpoint(node, direction):\n\t            if direction == 0:\n\t                return node.left\n\t            else:\n\t                return node.right\n\t        ts = []\n\t        current_node = self\n", "        ts.append(get_endpoint(current_node, direction))\n\t        while current_node.parents:\n\t            current_node = current_node.parents[direction]\n\t            ts.append(get_endpoint(current_node, direction))\n\t        return ts\n\t    def copy_node(self):\n\t        \"\"\"\n\t        Create a copy of the current node.\n\t        :return: A new Node object with the same level, left endpoint, and right endpoint as the original node.\n\t        \"\"\"\n", "        new_node = Node(self.level, self.left, self.right)\n\t        return new_node\n\t    def matches_interval(self, ll, rr):\n\t        \"\"\"\n\t        Check if the current node matches the given interval.\n\t        :param ll: The left endpoint of the interval.\n\t        :param rr: The right endpoint of the interval.\n\t        :return: True if the current node's interval matches the given interval, False otherwise.\n\t        \"\"\"\n\t        if self.left == ll and self.right == rr:\n", "            return True\n\t        return False\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/dyn_thresh/dyn_thresh_algo/__init__.py", "chunked_list": []}
{"filename": "algorithm/dyn_thresh/dyn_thresh_algo/threshold.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'threshold'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/16 19:27'\n\t__info__ =\n\t\"\"\"\n\tfrom typing import List, Dict\n\timport pandas as pd\n\timport numpy as np\n", "from algorithm.dyn_thresh.dyn_thresh_algo.events import PeriodicEventDetector\n\tfrom algorithm.dyn_thresh.dyn_thresh_algo.node import Node\n\tfrom common.utils import Utils\n\tclass ThresholdCalc:\n\t    def __init__(self, data_by_day: Dict[str, List[float]], boundary=1440):\n\t        self.data_by_day = data_by_day\n\t        # Initialization\n\t        self.boundary = boundary  # Maximum number of data points in a day\n\t        self.steps = 50   # Number of steps to use when calculating threshold values\n\t        self.init_per = 90  # Initial percentile to use when calculating threshold values\n", "        self.similar_index = 1  # Controls the similarity of the threshold values at different levels of the tree\n\t        self.cont_len = 120  # Length of continuous time intervals to break when doing threshold searching\n\t    def run(self):\n\t        df = pd.DataFrame.from_dict(self.data_by_day, orient=\"index\")\n\t        period = self.pp_detect(list(df.min()))  # Detect the periodicity of the data\n\t        if period != -1:\n\t            self.cont_len = int(self.boundary / period / 2)\n\t        dt = PeriodicEventDetector(data_by_day=self.data_by_day,\n\t                                   steps=self.steps,\n\t                                   init_per=self.init_per,\n", "                                   similar_index=self.similar_index,\n\t                                   cont_len=self.cont_len\n\t                                   )\n\t        node_events = dt.run()   # Detect periodic events in the data\n\t        intervals_with_th = self.slice_th_creator(node_events, dt.th_list)\n\t        return self.regression(df, intervals_with_th[-1])\n\t    def slice_th_creator(self, node_events: List[Node], th_list: List[float]):\n\t        \"\"\"\n\t        Create intervals and their corresponding threshold values.\n\t        @param node_events: A list of periodic event nodes.\n", "        @param th_list: A list of threshold values.\n\t        @return: A list of tuples containing each interval and its corresponding threshold value.\n\t        \"\"\"\n\t        index_stack = []\n\t        start = 0\n\t        max_level = 0\n\t        for n in node_events:\n\t            max_level = max(n.level, max_level)\n\t            if n.left > start:\n\t                index_stack.append((start, n.left - 1))\n", "            index_stack.append((n.left, n.right))\n\t            start = n.right + 1\n\t        if start < self.boundary:\n\t            index_stack.append((start, self.boundary - 1))\n\t        out_put = []\n\t        if len(th_list) == 1:  # Handle extreme cases\n\t            out_put.append((index_stack[0][0], index_stack[-1][-1], th_list[-1], None))\n\t            return out_put\n\t        for ll, rr in index_stack:\n\t            cur_th = th_list[max_level]\n", "            node = None\n\t            for nn in node_events:\n\t                if nn.matches_interval(ll, rr):\n\t                    node = nn\n\t                    cur_th = min(th_list[nn.drill_down_to_node(0).level], th_list[nn.drill_down_to_node(-1).level])\n\t                    continue\n\t            out_put.append((ll, rr, cur_th, node))\n\t        return out_put\n\t    @staticmethod\n\t    def regression(df, interval_with_th):\n", "        \"\"\"\n\t        Calculate the target threshold using regression.\n\t        @param df: A pandas dataframe.\n\t        @param interval_with_th: A tuple containing an interval and its corresponding threshold value.\n\t        @return: The target threshold value.\n\t        \"\"\"\n\t        ll, rr = interval_with_th[0], interval_with_th[1]\n\t        target_th = df.iloc[:, ll:rr + 1].min().min()\n\t        return target_th\n\t    @staticmethod\n", "    def pp_detect(envelope, min_win=140, min_period_interval=15):\n\t        \"\"\"\n\t         Detect whether the data has a periodic pattern using FFT.\n\t         @param envelope: A list of data points.\n\t         @param min_win: The minimum window size to use when calculating FFT.\n\t         @param min_period_interval: The minimum interval between periodic patterns.\n\t         @return: The number of data points per period, or -1 if no periodic pattern is detected.\n\t         \"\"\"\n\t        fft_values = np.fft.fft(envelope)\n\t        freq = [abs(v) for v in fft_values[:len(envelope) // 2]]\n", "        search_range = range(int(len(envelope) / min_win), int(len(envelope) / min_period_interval))\n\t        up_threshold = Utils.turkey_box_plot([freq[k] for k in search_range])[4]\n\t        up_threshold = max(1 / 3 * max([freq[k] for k in search_range]), up_threshold)\n\t        index_in = []\n\t        for i, v in enumerate(freq):\n\t            if v > up_threshold and i in search_range:\n\t                index_in.append(i)\n\t        potential_index = []\n\t        for v in index_in:\n\t            if v != max(index_in) and max(index_in) % v == 0:\n", "                potential_index.append(v)\n\t        if len(potential_index) > 0:\n\t            return min(potential_index)\n\t        return -1\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "algorithm/dyn_thresh/dyn_thresh_algo/features.py", "chunked_list": ["\"\"\"\n\t__project__ = 'holoinsight-ai'\n\t__file_name__ = 'features'\n\t__author__ = 'LuYuan'\n\t__time__ = '2023/4/16 20:52'\n\t__info__ =\n\t\"\"\"\n\tfrom typing import Dict, List\n\timport numpy as np\n\tfrom common.utils import Utils\n", "from common.constants import Constants\n\tclass Features:\n\t    def __init__(self, data_by_day: Dict[str, List[float]], algorithm_type: str):\n\t        self.data_by_day = data_by_day\n\t        self.smoothness = True   # A flag indicating whether the waveform is smooth or not\n\t        self.algorithm_type = algorithm_type\n\t    def run(self):\n\t        self.smoothness = self.waveform_smoothness_checker()\n\t        if self.smoothness:\n\t            features = self.one_diff()\n", "        else:\n\t            features = self.zero_diff()\n\t        return features\n\t    def one_diff(self):\n\t        features_by_duration = {}\n\t        for duration in Constants.WINDOW_LIST.value:\n\t            features_by_duration[str(duration)] = self.do_cutoff(data_by_day=self.data_by_day, duration=duration)\n\t        return features_by_duration\n\t    def zero_diff(self):\n\t        return self.data_by_day  # If the waveform is not smooth, return the raw data\n", "    def do_cutoff(self, data_by_day: Dict[str, List[float]], duration: int) -> Dict[str, List[float]]:\n\t        \"\"\"\n\t        Use a layering algorithm to determine whether the data should be sliced.\n\t        @param duration: The length of the fixed window to use for slicing.\n\t        @param data_by_day: The data to be analyzed, grouped by day.\n\t        @return: A dictionary of features, grouped by day.\n\t        \"\"\"\n\t        features = {}\n\t        is_down = True if self.algorithm_type == \"down\" else False\n\t        for k, v in data_by_day.items():\n", "            features[k] = Utils.diff_percentile_func(v, duration, is_down)\n\t        return features\n\t    def waveform_smoothness_checker(self):\n\t        \"\"\"\n\t        Evaluate the smoothness of a time series.\n\t        @return: A flag indicating whether the waveform is smooth or not.\n\t        \"\"\"\n\t        diff_values = []\n\t        for k, v in self.data_by_day.items():\n\t            diff_values += Utils.diff_percentile_func(v, 1)\n", "        diff_values = [abs(value) for value in diff_values]\n\t        if np.percentile(diff_values, 60) < 10:  # todo test 为小流量最好准备！\n\t            return True\n\t        else:\n\t            return False\n\tif __name__ == \"__main__\":\n\t    pass\n"]}
