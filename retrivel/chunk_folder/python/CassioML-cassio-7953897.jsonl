{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\timport pathlib\n\there = pathlib.Path(__file__).parent.resolve()\n\tsetup(\n\t    name='cassio',\n\t    version='0.1.0',\n\t    author='Stefano Lottini',\n\t    author_email='stefano.lottini@datastax.com',\n\t    package_dir={\"\": \"src\"},\n\t    packages=find_packages(where='src'),\n", "    # entry_points={\n\t    #     \"console_scripts\": [\n\t    #         # will we ever have a command-line cassio script?\n\t    #         \"cassio=cassio:main\",\n\t    #     ],\n\t    # },\n\t    url='https://github.com/hemidactylus/cassio',\n\t    license='LICENSE.txt',\n\t    description='A framework-agnostic Python library to seamlessly integrate Apache Cassandra(R) with ML/LLM/genAI workloads.',\n\t    long_description=(here / \"README.md\").read_text(encoding=\"utf-8\"),\n", "    long_description_content_type=\"text/markdown\",\n\t    install_requires=[\n\t        \"numpy>=1.0\",\n\t        \"cassandra-driver>=3.28.0\",\n\t    ],\n\t    python_requires=\">=3.8\",\n\t    classifiers=[\n\t        \"Development Status :: 4 - Beta\",\n\t        \"Intended Audience :: Developers\",\n\t        \"License :: OSI Approved :: Apache Software License\",\n", "        #\n\t        \"Programming Language :: Python :: 3\",\n\t        \"Programming Language :: Python :: 3.8\",\n\t        \"Programming Language :: Python :: 3.9\",\n\t        \"Programming Language :: Python :: 3.10\",\n\t        \"Programming Language :: Python :: 3.11\",\n\t        \"Programming Language :: Python :: 3.12\",\n\t        \"Programming Language :: Python :: 3 :: Only\",\n\t        \"Topic :: Scientific/Engineering\",\n\t        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n", "    ],\n\t    keywords=\"cassandra, ai, llm\",\n\t)\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/conftest.py", "chunked_list": ["\"\"\"\n\tfixtures for testing\n\t\"\"\"\n\timport os\n\timport pytest\n\tfrom cassandra.cluster import Cluster  # type: ignore\n\tfrom cassandra.auth import PlainTextAuthProvider  # type: ignore\n\tfrom cassio.table.cql import MockDBSession\n\t# DB session (as per settings detected in env vars)\n\tdbSession = None\n", "def createDBSessionSingleton():\n\t    global dbSession\n\t    if dbSession is None:\n\t        mode = os.environ[\"TEST_DB_MODE\"]\n\t        # the proper DB session is created as required\n\t        if mode == \"ASTRA_DB\":\n\t            ASTRA_DB_SECURE_BUNDLE_PATH = os.environ[\"ASTRA_DB_SECURE_BUNDLE_PATH\"]\n\t            ASTRA_DB_CLIENT_ID = \"token\"\n\t            ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n\t            cluster = Cluster(\n", "                cloud={\n\t                    \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n\t                },\n\t                auth_provider=PlainTextAuthProvider(\n\t                    ASTRA_DB_CLIENT_ID,\n\t                    ASTRA_DB_APPLICATION_TOKEN,\n\t                ),\n\t            )\n\t            dbSession = cluster.connect()\n\t        elif mode == \"LOCAL_CASSANDRA\":\n", "            CASSANDRA_USERNAME = os.environ.get(\"CASSANDRA_USERNAME\")\n\t            CASSANDRA_PASSWORD = os.environ.get(\"CASSANDRA_PASSWORD\")\n\t            if CASSANDRA_USERNAME and CASSANDRA_PASSWORD:\n\t                auth_provider = PlainTextAuthProvider(\n\t                    CASSANDRA_USERNAME,\n\t                    CASSANDRA_PASSWORD,\n\t                )\n\t            else:\n\t                auth_provider = None\n\t            CASSANDRA_CONTACT_POINTS = os.environ.get(\"CASSANDRA_CONTACT_POINTS\")\n", "            if CASSANDRA_CONTACT_POINTS:\n\t                contact_points = [\n\t                    cp.strip() for cp in CASSANDRA_CONTACT_POINTS.split(\",\")\n\t                ]\n\t            else:\n\t                contact_points = None\n\t            #\n\t            cluster = Cluster(\n\t                contact_points,\n\t                auth_provider=auth_provider,\n", "            )\n\t            localSession = cluster.connect()\n\t            return localSession\n\t        else:\n\t            raise NotImplementedError\n\t    return dbSession\n\tdef getDBKeyspace():\n\t    mode = os.environ[\"TEST_DB_MODE\"]\n\t    if mode == \"ASTRA_DB\":\n\t        ASTRA_DB_KEYSPACE = os.environ[\"ASTRA_DB_KEYSPACE\"]\n", "        return ASTRA_DB_KEYSPACE\n\t    elif mode == \"LOCAL_CASSANDRA\":\n\t        CASSANDRA_KEYSPACE = os.environ[\"CASSANDRA_KEYSPACE\"]\n\t        return CASSANDRA_KEYSPACE\n\t# Fixtures\n\t@pytest.fixture(scope=\"session\")\n\tdef db_session():\n\t    return createDBSessionSingleton()\n\t@pytest.fixture(scope=\"session\")\n\tdef db_keyspace():\n", "    return getDBKeyspace()\n\t@pytest.fixture(scope=\"function\")\n\tdef mock_db_session():\n\t    return MockDBSession()\n"]}
{"filename": "tests/integration/test_vector_table.py", "chunked_list": ["\"\"\"\n\tVector search tests\n\t\"\"\"\n\timport time\n\timport pytest\n\tfrom cassio.vector import VectorTable\n\t@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\n\tclass TestVectorTable:\n\t    \"\"\"\n\t    DB-backed tests for VectorTable\n", "    \"\"\"\n\t    def test_put_and_get(self, db_session, db_keyspace):\n\t        vtable_name1 = \"vector_table_1\"\n\t        v_emb_dim_1 = 3\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name1};\")\n\t        v_table = VectorTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table=vtable_name1,\n\t            embedding_dimension=v_emb_dim_1,\n", "            primary_key_type=\"TEXT\",\n\t        )\n\t        v_table.put(\n\t            \"document\",\n\t            [1, 2, 3],\n\t            \"doc_id\",\n\t            {\"a\": \"value_1\"},\n\t            None,\n\t        )\n\t        assert v_table.get(\"doc_id\") == {\n", "            \"document_id\": \"doc_id\",\n\t            \"metadata\": {\"a\": \"value_1\"},\n\t            \"document\": \"document\",\n\t            \"embedding_vector\": [1, 2, 3],\n\t        }\n\t    def test_put_and_search(self, db_session, db_keyspace):\n\t        vtable_name_2 = \"vector_table_2\"\n\t        v_emb_dim_2 = 3\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name_2};\")\n\t        v_table = VectorTable(\n", "            db_session,\n\t            db_keyspace,\n\t            table=vtable_name_2,\n\t            embedding_dimension=v_emb_dim_2,\n\t            primary_key_type=\"TEXT\",\n\t        )\n\t        v_table.put(\n\t            \"document\",\n\t            [5, 5, 10],\n\t            \"doc_id1\",\n", "            {\"a\": 1},\n\t            None,\n\t        )\n\t        v_table.put(\n\t            \"document\",\n\t            [\n\t                10,\n\t                5,\n\t                5,\n\t            ],\n", "            \"doc_id2\",\n\t            {\"a\": 2},\n\t            None,\n\t        )\n\t        v_table.put(\n\t            \"document\",\n\t            [5, 10, 5],\n\t            \"doc_id3\",\n\t            {\"a\": 3},\n\t            None,\n", "        )\n\t        matches = v_table.search(\n\t            [6, 10, 6],\n\t            1,\n\t            \"cos\",\n\t            0.5,\n\t        )\n\t        assert len(matches) == 1\n\t        assert matches[0][\"document_id\"] == \"doc_id3\"\n\t    def test_put_and_search_async(self, db_session, db_keyspace):\n", "        vtable_name_2a = \"vector_table_2async\"\n\t        v_emb_dim_2a = 3\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name_2a};\")\n\t        v_table = VectorTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table=vtable_name_2a,\n\t            embedding_dimension=v_emb_dim_2a,\n\t            primary_key_type=\"TEXT\",\n\t        )\n", "        futures = [\n\t            v_table.put_async(\n\t                \"document\",\n\t                [5, 5, 10],\n\t                \"doc_id1\",\n\t                {\"a\": 1},\n\t                None,\n\t            ),\n\t            v_table.put_async(\n\t                \"document\",\n", "                [\n\t                    10,\n\t                    5,\n\t                    5,\n\t                ],\n\t                \"doc_id2\",\n\t                {\"a\": 2},\n\t                None,\n\t            ),\n\t            v_table.put_async(\n", "                \"document\",\n\t                [5, 10, 5],\n\t                \"doc_id3\",\n\t                {\"a\": 3},\n\t                None,\n\t            ),\n\t        ]\n\t        for f in futures:\n\t            _ = f.result()\n\t        matches = v_table.search(\n", "            [6, 10, 6],\n\t            1,\n\t            \"cos\",\n\t            0.5,\n\t        )\n\t        assert len(matches) == 1\n\t        assert matches[0][\"document_id\"] == \"doc_id3\"\n\t    def test_put_intpk_and_get(self, db_session, db_keyspace):\n\t        vtable_name_3 = \"vector_table_3\"\n\t        v_emb_dim_3 = 6\n", "        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name_3};\")\n\t        v_table = VectorTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table=vtable_name_3,\n\t            embedding_dimension=v_emb_dim_3,\n\t            primary_key_type=\"INT\",\n\t        )\n\t        v_table.put(\n\t            \"document_int\",\n", "            [0.1] * v_emb_dim_3,\n\t            9999,\n\t            {\"a\": \"value_1\"},\n\t            None,\n\t        )\n\t        match = v_table.get(9999)\n\t        assert match[\"document\"] == \"document_int\"\n\t        assert match[\"metadata\"] == {\"a\": \"value_1\"}\n\t        match_no = v_table.get(123)\n\t        assert match_no is None\n", "    def test_null_json(self, db_session, db_keyspace):\n\t        vtable_name4 = \"vector_table_4\"\n\t        v_emb_dim_4 = 3\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name4};\")\n\t        v_table = VectorTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table=vtable_name4,\n\t            embedding_dimension=v_emb_dim_4,\n\t            primary_key_type=\"TEXT\",\n", "        )\n\t        v_table.put(\n\t            \"document\",\n\t            [1, 2, 3],\n\t            \"doc_id\",\n\t            None,\n\t            None,\n\t        )\n\t        assert v_table.get(\"doc_id\") == {\n\t            \"document_id\": \"doc_id\",\n", "            \"metadata\": {},\n\t            \"document\": \"document\",\n\t            \"embedding_vector\": [1, 2, 3],\n\t        }\n\t    def test_nullsearch_results(self, db_session, db_keyspace):\n\t        vtable_name5 = \"vector_table_5\"\n\t        v_emb_dim_5 = 5\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name5};\")\n\t        v_table = VectorTable(\n\t            db_session,\n", "            db_keyspace,\n\t            table=vtable_name5,\n\t            embedding_dimension=v_emb_dim_5,\n\t            primary_key_type=\"INT\",\n\t        )\n\t        v_table.put(\"boasting\", [2, 2, 2, 2, 2], 123)\n\t        assert v_table.search([1, 0, 0, 0, 0], 10, \"cos\", 1.01) == []\n\t        # cannot use zero-vectors with cosine similarity:\n\t        with pytest.raises(ValueError):\n\t            _ = v_table.search([0, 0, 0, 0, 0], 10, \"cos\", 1.01)\n", "        v_table.clear()\n\t    def test_ttl(self, db_session, db_keyspace):\n\t        vtable_name6 = \"vector_table_6\"\n\t        v_emb_dim_6 = 2\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{vtable_name6};\")\n\t        v_table = VectorTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table=vtable_name6,\n\t            embedding_dimension=v_emb_dim_6,\n", "            primary_key_type=\"TEXT\",\n\t        )\n\t        #\n\t        v_table.put(\"this is short lived\", [1, 0], \"short_lived\", ttl_seconds=2)\n\t        v_table.put(\"this is long lived\", [0, 1], \"long_lived\", ttl_seconds=5)\n\t        time.sleep(0.2)\n\t        assert len(v_table.search([0.5, 0.5], 3, \"cos\", 0.01)) == 2\n\t        time.sleep(2.5)\n\t        assert len(v_table.search([0.5, 0.5], 3, \"cos\", 0.01)) == 1\n\t        time.sleep(3.0)\n", "        assert len(v_table.search([0.5, 0.5], 3, \"cos\", 0.01)) == 0\n\tif __name__ == \"__main__\":\n\t    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\t    s = createDBSessionSingleton()\n\t    k = getDBKeyspace()\n\t    TestVectorTable().test_null_json(s, k)\n"]}
{"filename": "tests/integration/test_tableclasses_clusteredmetadatavectorcassandratable.py", "chunked_list": ["\"\"\"\n\tTable classes integration test - ClusteredMetadataVectorCassandraTable\n\t\"\"\"\n\timport math\n\timport pytest\n\tfrom cassandra import InvalidRequest  # type: ignore\n\tfrom cassio.table.tables import (\n\t    ClusteredMetadataVectorCassandraTable,\n\t)\n\tN = 16\n", "@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\n\tclass TestClusteredMetadataVectorCassandraTable:\n\t    def test_crud(self, db_session, db_keyspace):\n\t        table_name = \"c_m_v_ct\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n\t        #\n\t        # \"INT\" here means: partition_id is a number (for fun)\n\t        t = ClusteredMetadataVectorCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n", "            table_name,\n\t            vector_dimension=2,\n\t            primary_key_type=[\"INT\", \"TEXT\"],\n\t            partition_id=0,\n\t        )\n\t        for n_theta in range(N):\n\t            theta = n_theta * math.pi * 2 / N\n\t            group = \"odd\" if n_theta % 2 == 1 else \"even\"\n\t            t.put(\n\t                row_id=f\"theta_{n_theta}\",\n", "                body_blob=f\"theta = {theta:.4f}\",\n\t                vector=[math.cos(theta), math.sin(theta)],\n\t                metadata={\n\t                    group: True,\n\t                    \"n_theta_mod_2\": n_theta % 2,\n\t                    \"group\": group,\n\t                },\n\t            )\n\t        # fill another partition (999 = \"the other one\")\n\t        for n_theta in range(N):\n", "            theta = n_theta * math.pi * 2 / N\n\t            group = \"odd\" if n_theta % 2 == 1 else \"even\"\n\t            t.put(\n\t                row_id=f\"Z_theta_{n_theta}\",\n\t                body_blob=f\"Z_theta = {theta:.4f}\",\n\t                vector=[math.cos(theta), math.sin(theta)],\n\t                partition_id=999,\n\t                metadata={\n\t                    group: True,\n\t                    \"n_theta_mod_2\": n_theta % 2,\n", "                    \"group\": group,\n\t                },\n\t            )\n\t        # retrieval\n\t        theta_1 = t.get(row_id=\"theta_1\")\n\t        assert abs(theta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n\t        assert abs(theta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n\t        assert theta_1[\"partition_id\"] == 0\n\t        # retrieval with metadata filtering\n\t        theta_1b = t.get(row_id=\"theta_1\", metadata={\"odd\": True})\n", "        assert theta_1b == theta_1\n\t        theta_1n = t.get(row_id=\"theta_1\", metadata={\"even\": True})\n\t        assert theta_1n is None\n\t        # ANN\n\t        # a vector halfway between 0 and 1 inserted above\n\t        query_theta = 1 * math.pi * 2 / (2 * N)\n\t        ref_vector = [math.cos(query_theta), math.sin(query_theta)]\n\t        ann_results1 = list(t.ann_search(ref_vector, n=4))\n\t        assert {r[\"row_id\"] for r in ann_results1[:2]} == {\"theta_1\", \"theta_0\"}\n\t        assert {r[\"row_id\"] for r in ann_results1[2:4]} == {\"theta_2\", \"theta_15\"}\n", "        # ANN with metadata filtering\n\t        ann_results_md1 = list(t.ann_search(ref_vector, n=4, metadata={\"odd\": True}))\n\t        assert {r[\"row_id\"] for r in ann_results_md1[:2]} == {\"theta_1\", \"theta_15\"}\n\t        assert {r[\"row_id\"] for r in ann_results_md1[2:4]} == {\"theta_3\", \"theta_13\"}\n\t        # and in another way...\n\t        ann_results_md2 = list(t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\"}))\n\t        assert {r[\"row_id\"] for r in ann_results_md2[:2]} == {\"theta_1\", \"theta_15\"}\n\t        assert {r[\"row_id\"] for r in ann_results_md2[2:4]} == {\"theta_3\", \"theta_13\"}\n\t        # with two conditions ...\n\t        ann_results_md3 = list(\n", "            t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\", \"odd\": True})\n\t        )\n\t        assert {r[\"row_id\"] for r in ann_results_md3[:2]} == {\"theta_1\", \"theta_15\"}\n\t        assert {r[\"row_id\"] for r in ann_results_md3[2:4]} == {\"theta_3\", \"theta_13\"}\n\t        # retrieval on 999\n\t        ztheta_1 = t.get(row_id=\"Z_theta_1\", partition_id=999)\n\t        assert abs(ztheta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n\t        assert abs(ztheta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n\t        assert ztheta_1[\"partition_id\"] == 999\n\t        # retrieval with metadata filtering on 999\n", "        ztheta_1b = t.get(row_id=\"Z_theta_1\", metadata={\"odd\": True}, partition_id=999)\n\t        assert ztheta_1b == ztheta_1\n\t        ztheta_1n = t.get(row_id=\"Z_theta_1\", metadata={\"even\": True}, partition_id=999)\n\t        assert ztheta_1n is None\n\t        # \"theta_1\" is not an ID on 999:\n\t        ztheta_1n2 = t.get(row_id=\"theta_1\", metadata={\"odd\": True}, partition_id=999)\n\t        assert ztheta_1n2 is None\n\t        # ANN on 999\n\t        # a vector halfway between 0 and 1 inserted above\n\t        zquery_theta = 1 * math.pi * 2 / (2 * N)\n", "        zref_vector = [math.cos(zquery_theta), math.sin(zquery_theta)]\n\t        zann_results1 = list(t.ann_search(zref_vector, n=4, partition_id=999))\n\t        assert {r[\"row_id\"] for r in zann_results1[:2]} == {\"Z_theta_1\", \"Z_theta_0\"}\n\t        assert {r[\"row_id\"] for r in zann_results1[2:4]} == {\"Z_theta_2\", \"Z_theta_15\"}\n\t        # ANN with metadata filtering\n\t        zann_results_md1 = list(\n\t            t.ann_search(zref_vector, n=4, metadata={\"odd\": True}, partition_id=999)\n\t        )\n\t        assert {r[\"row_id\"] for r in zann_results_md1[:2]} == {\n\t            \"Z_theta_1\",\n", "            \"Z_theta_15\",\n\t        }\n\t        assert {r[\"row_id\"] for r in zann_results_md1[2:4]} == {\n\t            \"Z_theta_3\",\n\t            \"Z_theta_13\",\n\t        }\n\t        # and in another way...\n\t        zann_results_md2 = list(\n\t            t.ann_search(zref_vector, n=4, metadata={\"group\": \"odd\"}, partition_id=999)\n\t        )\n", "        assert {r[\"row_id\"] for r in zann_results_md2[:2]} == {\n\t            \"Z_theta_1\",\n\t            \"Z_theta_15\",\n\t        }\n\t        assert {r[\"row_id\"] for r in zann_results_md2[2:4]} == {\n\t            \"Z_theta_3\",\n\t            \"Z_theta_13\",\n\t        }\n\t        # with two conditions ...\n\t        zann_results_md3 = list(\n", "            t.ann_search(\n\t                zref_vector,\n\t                n=4,\n\t                metadata={\"group\": \"odd\", \"odd\": True},\n\t                partition_id=999,\n\t            )\n\t        )\n\t        assert {r[\"row_id\"] for r in zann_results_md3[:2]} == {\n\t            \"Z_theta_1\",\n\t            \"Z_theta_15\",\n", "        }\n\t        assert {r[\"row_id\"] for r in zann_results_md3[2:4]} == {\n\t            \"Z_theta_3\",\n\t            \"Z_theta_13\",\n\t        }\n\t        # cross-partition ANN search test\n\t        t_xpart = ClusteredMetadataVectorCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name,\n", "            vector_dimension=2,\n\t            primary_key_type=[\"INT\", \"TEXT\"],\n\t            skip_provisioning=True,\n\t        )\n\t        # a vector at 1/4 step from the \"_0\" for both partitions\n\t        xp_query_theta = 1 * math.pi * 2 / (4 * N)\n\t        xp_vector = [math.cos(xp_query_theta), math.sin(xp_query_theta)]\n\t        xpart_results = list(\n\t            t_xpart.ann_search(\n\t                xp_vector,\n", "                n=2,\n\t            )\n\t        )\n\t        assert {r[\"row_id\"] for r in xpart_results} == {\"theta_0\", \"Z_theta_0\"}\n\t        # \"cross partition GET\" (i.e. partition_id not specified).\n\t        # Outside of ANN this should throw an error\n\t        with pytest.raises(InvalidRequest):\n\t            _ = t.get(row_id=\"not_enough_info\", partition_id=None)\n\t        with pytest.raises(InvalidRequest):\n\t            _ = t_xpart.get(row_id=\"not_enough_info\")\n", "        t.clear()\n\tif __name__ == \"__main__\":\n\t    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m \\\n\t    #   tests.integration.test_tableclasses_clusteredmetadatavectorcassandratable\n\t    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\t    s = createDBSessionSingleton()\n\t    k = getDBKeyspace()\n\t    TestClusteredMetadataVectorCassandraTable().test_crud(s, k)\n"]}
{"filename": "tests/integration/test_tableclasses_metadatavectorcassandratable.py", "chunked_list": ["\"\"\"\n\tTable classes integration test - MetadataVectorCassandraTable\n\t\"\"\"\n\timport math\n\timport pytest\n\tfrom cassio.table.tables import (\n\t    MetadataVectorCassandraTable,\n\t)\n\tN = 16\n\t@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\n", "class TestMetadataVectorCassandraTable:\n\t    def test_crud(self, db_session, db_keyspace):\n\t        table_name = \"m_v_ct\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n\t        #\n\t        t = MetadataVectorCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name,\n\t            vector_dimension=2,\n", "            primary_key_type=\"TEXT\",\n\t        )\n\t        for n_theta in range(N):\n\t            theta = n_theta * math.pi * 2 / N\n\t            group = \"odd\" if n_theta % 2 == 1 else \"even\"\n\t            t.put(\n\t                row_id=f\"theta_{n_theta}\",\n\t                body_blob=f\"theta = {theta:.4f}\",\n\t                vector=[math.cos(theta), math.sin(theta)],\n\t                metadata={\n", "                    group: True,\n\t                    \"n_theta_mod_2\": n_theta % 2,\n\t                    \"group\": group,\n\t                },\n\t            )\n\t        # retrieval\n\t        theta_1 = t.get(row_id=\"theta_1\")\n\t        assert abs(theta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n\t        assert abs(theta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n\t        # retrieval with metadata filtering\n", "        theta_1b = t.get(row_id=\"theta_1\", metadata={\"odd\": True})\n\t        assert theta_1b == theta_1\n\t        theta_1n = t.get(row_id=\"theta_1\", metadata={\"even\": True})\n\t        assert theta_1n is None\n\t        # ANN\n\t        # a vector halfway between 0 and 1 inserted above\n\t        query_theta = 1 * math.pi * 2 / (2 * N)\n\t        ref_vector = [math.cos(query_theta), math.sin(query_theta)]\n\t        ann_results1 = list(t.ann_search(ref_vector, n=4))\n\t        assert {r[\"row_id\"] for r in ann_results1[:2]} == {\"theta_1\", \"theta_0\"}\n", "        assert {r[\"row_id\"] for r in ann_results1[2:4]} == {\"theta_2\", \"theta_15\"}\n\t        # ANN with metadata filtering\n\t        ann_results_md1 = list(t.ann_search(ref_vector, n=4, metadata={\"odd\": True}))\n\t        assert {r[\"row_id\"] for r in ann_results_md1[:2]} == {\"theta_1\", \"theta_15\"}\n\t        assert {r[\"row_id\"] for r in ann_results_md1[2:4]} == {\"theta_3\", \"theta_13\"}\n\t        # and in another way...\n\t        ann_results_md2 = list(t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\"}))\n\t        assert {r[\"row_id\"] for r in ann_results_md2[:2]} == {\"theta_1\", \"theta_15\"}\n\t        assert {r[\"row_id\"] for r in ann_results_md2[2:4]} == {\"theta_3\", \"theta_13\"}\n\t        # with two conditions ...\n", "        ann_results_md3 = list(\n\t            t.ann_search(ref_vector, n=4, metadata={\"group\": \"odd\", \"odd\": True})\n\t        )\n\t        assert {r[\"row_id\"] for r in ann_results_md3[:2]} == {\"theta_1\", \"theta_15\"}\n\t        assert {r[\"row_id\"] for r in ann_results_md3[2:4]} == {\"theta_3\", \"theta_13\"}\n\t        # metric search testing\n\t        # a vector between 15 and 1 inserted above, but closer to the 1\n\t        query_theta_mt = 1 * math.pi * 2 / (2 * N + 1)\n\t        ref_vector_mt = [math.cos(query_theta_mt), math.sin(query_theta_mt)]\n\t        ann_results_mt = list(\n", "            t.metric_ann_search(\n\t                ref_vector_mt,\n\t                n=2,\n\t                metric=\"cos\",\n\t                metric_threshold=0.8,\n\t                metadata={\"group\": \"odd\"},\n\t            )\n\t        )\n\t        assert [r[\"row_id\"] for r in ann_results_mt] == [\"theta_1\", \"theta_15\"]\n\t        assert ann_results_mt[0][\"distance\"] > ann_results_mt[1][\"distance\"]\n", "        # a max distance makes this call return just two results despite the n=3:\n\t        ann_results_mt_l2 = list(\n\t            t.metric_ann_search(\n\t                ref_vector_mt,\n\t                n=3,\n\t                metric=\"l2\",\n\t                metric_threshold=0.6,\n\t                metadata={\"group\": \"odd\"},\n\t            )\n\t        )\n", "        assert [r[\"row_id\"] for r in ann_results_mt_l2] == [\"theta_1\", \"theta_15\"]\n\t        assert ann_results_mt_l2[0][\"distance\"] < ann_results_mt_l2[1][\"distance\"]\n\t        t.clear()\n\tif __name__ == \"__main__\":\n\t    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n\t    #   tests.integration.test_tableclasses_metadatavectorcassandratable\n\t    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\t    s = createDBSessionSingleton()\n\t    k = getDBKeyspace()\n\t    TestMetadataVectorCassandraTable().test_crud(s, k)\n"]}
{"filename": "tests/integration/test_tableclasses_plaincassandratable.py", "chunked_list": ["\"\"\"\n\tTable classes integration test - PlainCassandraTable\n\t\"\"\"\n\timport pytest\n\tfrom cassio.table.tables import (\n\t    PlainCassandraTable,\n\t)\n\t@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\n\tclass TestPlainCassandraTable:\n\t    def test_crud(self, db_session, db_keyspace):\n", "        table_name = \"ct\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n\t        #\n\t        t = PlainCassandraTable(\n\t            db_session, db_keyspace, table_name, primary_key_type=\"TEXT\"\n\t        )\n\t        t.put(row_id=\"empty_row\")\n\t        gotten1 = t.get(row_id=\"empty_row\")\n\t        assert gotten1 == {\"row_id\": \"empty_row\", \"body_blob\": None}\n\t        t.put(row_id=\"full_row\", body_blob=\"body_blob\")\n", "        gotten2 = t.get(row_id=\"full_row\")\n\t        assert gotten2 == {\"row_id\": \"full_row\", \"body_blob\": \"body_blob\"}\n\t        t.delete(row_id=\"full_row\")\n\t        gotten2n = t.get(row_id=\"full_row\")\n\t        assert gotten2n is None\n\t        t.clear()\n\t        gotten1n = t.get(row_id=\"empty_row\")\n\t        assert gotten1n is None\n\tif __name__ == \"__main__\":\n\t    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n", "    #   tests.integration.test_tableclasses_plaincassandratable\n\t    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\t    s = createDBSessionSingleton()\n\t    k = getDBKeyspace()\n\t    TestPlainCassandraTable().test_crud(s, k)\n"]}
{"filename": "tests/integration/test_tableclasses_clusteredcassandratable.py", "chunked_list": ["\"\"\"\n\tTable classes integration test - ClusteredCassandraTable\n\t\"\"\"\n\timport pytest\n\tfrom cassio.table.tables import (\n\t    ClusteredCassandraTable,\n\t)\n\t@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\n\tclass TestClusteredCassandraTable:\n\t    def test_crud(self, db_session, db_keyspace):\n", "        table_name = \"c_ct\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n\t        #\n\t        t = ClusteredCassandraTable(\n\t            db_session, db_keyspace, table_name, partition_id=\"my_part\"\n\t        )\n\t        t.put(row_id=\"reg_row\", body_blob=\"reg_blob\")\n\t        gotten1 = t.get(row_id=\"reg_row\")\n\t        assert gotten1 == {\n\t            \"row_id\": \"reg_row\",\n", "            \"partition_id\": \"my_part\",\n\t            \"body_blob\": \"reg_blob\",\n\t        }\n\t        t.put(row_id=\"irr_row\", partition_id=\"other_p\", body_blob=\"irr_blob\")\n\t        gotten2n = t.get(row_id=\"irr_row\")\n\t        assert gotten2n is None\n\t        gotten2 = t.get(row_id=\"irr_row\", partition_id=\"other_p\")\n\t        assert gotten2 == {\n\t            \"row_id\": \"irr_row\",\n\t            \"partition_id\": \"other_p\",\n", "            \"body_blob\": \"irr_blob\",\n\t        }\n\t        #\n\t        t.delete(row_id=\"reg_row\")\n\t        assert t.get(row_id=\"reg_row\") is None\n\t        t.delete(row_id=\"irr_row\")\n\t        assert t.get(row_id=\"irr_row\", partition_id=\"other_p\") is not None\n\t        t.delete(row_id=\"irr_row\", partition_id=\"other_p\")\n\t        assert t.get(row_id=\"irr_row\", partition_id=\"other_p\") is None\n\t        #\n", "        t.put(row_id=\"nr1\")\n\t        t.put(row_id=\"nr2\", partition_id=\"another_p\")\n\t        assert t.get(row_id=\"nr1\") is not None\n\t        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is not None\n\t        t.delete_partition()\n\t        assert t.get(row_id=\"nr1\") is None\n\t        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is not None\n\t        t.clear()\n\t    def test_partition_ordering(self, db_session, db_keyspace):\n\t        table_name_asc = \"c_ct_asc\"\n", "        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_asc};\")\n\t        table_name_desc = \"c_ct_desc\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_desc};\")\n\t        t_asc = ClusteredCassandraTable(\n\t            db_session, db_keyspace, table_name_asc, partition_id=\"my_part\"\n\t        )\n\t        t_desc = ClusteredCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name_desc,\n", "            partition_id=\"my_part\",\n\t            ordering_in_partition=\"desc\",\n\t        )\n\t        #\n\t        t_asc.put(row_id=\"row1\", body_blob=\"blob1\")\n\t        t_asc.put(row_id=\"row2\", body_blob=\"blob1\")\n\t        t_asc.put(row_id=\"row3\", body_blob=\"blob1\")\n\t        part_rows = t_asc.get_partition(n=2)\n\t        assert [gotten[\"row_id\"] for gotten in part_rows] == [\"row1\", \"row2\"]\n\t        assert len(list(t_asc.get_partition())) == 3\n", "        assert len(list(t_asc.get_partition(n=10))) == 3\n\t        #\n\t        t_desc.put(row_id=\"row1\", body_blob=\"blob1\")\n\t        t_desc.put(row_id=\"row2\", body_blob=\"blob1\")\n\t        t_desc.put(row_id=\"row3\", body_blob=\"blob1\")\n\t        part_rows = t_desc.get_partition(n=2)\n\t        assert [gotten[\"row_id\"] for gotten in part_rows] == [\"row3\", \"row2\"]\n\t        assert len(list(t_desc.get_partition())) == 3\n\t        assert len(list(t_desc.get_partition(n=10))) == 3\n\t        #\n", "        t_asc.clear()\n\t        t_desc.clear()\n\t    def test_crud_async(self, db_session, db_keyspace):\n\t        table_name = \"c_ct_asy\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n\t        #\n\t        t = ClusteredCassandraTable(\n\t            db_session, db_keyspace, table_name, partition_id=\"my_part\"\n\t        )\n\t        rf1 = t.put_async(row_id=\"reg_row\", body_blob=\"reg_blob\")\n", "        _ = rf1.result()\n\t        gotten1 = t.get(row_id=\"reg_row\")\n\t        assert gotten1 == {\n\t            \"row_id\": \"reg_row\",\n\t            \"partition_id\": \"my_part\",\n\t            \"body_blob\": \"reg_blob\",\n\t        }\n\t        rf2 = t.put_async(\n\t            row_id=\"irr_row\", partition_id=\"other_p\", body_blob=\"irr_blob\"\n\t        )\n", "        _ = rf2.result()\n\t        gotten2n = t.get(row_id=\"irr_row\")\n\t        assert gotten2n is None\n\t        gotten2 = t.get(row_id=\"irr_row\", partition_id=\"other_p\")\n\t        assert gotten2 == {\n\t            \"row_id\": \"irr_row\",\n\t            \"partition_id\": \"other_p\",\n\t            \"body_blob\": \"irr_blob\",\n\t        }\n\t        #\n", "        rf3 = t.delete_async(row_id=\"reg_row\")\n\t        _ = rf3.result()\n\t        assert t.get(row_id=\"reg_row\") is None\n\t        rf4 = t.delete_async(row_id=\"irr_row\")\n\t        _ = rf4.result()\n\t        assert t.get(row_id=\"irr_row\", partition_id=\"other_p\") is not None\n\t        rf5 = t.delete_async(row_id=\"irr_row\", partition_id=\"other_p\")\n\t        _ = rf5.result()\n\t        assert t.get(row_id=\"irr_row\", partition_id=\"other_p\") is None\n\t        #\n", "        rf6 = t.put_async(row_id=\"nr1\")\n\t        _ = rf6.result()\n\t        rf7 = t.put_async(row_id=\"nr2\", partition_id=\"another_p\")\n\t        _ = rf7.result()\n\t        assert t.get(row_id=\"nr1\") is not None\n\t        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is not None\n\t        rf8 = t.delete_partition_async()\n\t        _ = rf8.result()\n\t        assert t.get(row_id=\"nr1\") is None\n\t        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is not None\n", "        rf9 = t.clear_async()\n\t        _ = rf9.result()\n\t        assert t.get(row_id=\"nr2\", partition_id=\"another_p\") is None\n\tif __name__ == \"__main__\":\n\t    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\t    s = createDBSessionSingleton()\n\t    k = getDBKeyspace()\n\t    TestClusteredCassandraTable().test_crud(s, k)\n"]}
{"filename": "tests/integration/test_tableclasses_vectorcassandratable.py", "chunked_list": ["\"\"\"\n\tTable classes integration test - VectorCassandraTable\n\t\"\"\"\n\timport math\n\timport pytest\n\tfrom cassio.table.tables import (\n\t    VectorCassandraTable,\n\t)\n\tN = 8\n\t@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\n", "class TestVectorCassandraTable:\n\t    def test_crud(self, db_session, db_keyspace):\n\t        table_name = \"v_ct\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n\t        #\n\t        t = VectorCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name,\n\t            vector_dimension=2,\n", "            primary_key_type=\"TEXT\",\n\t        )\n\t        for n_theta in range(N):\n\t            theta = n_theta * math.pi * 2 / N\n\t            t.put(\n\t                row_id=f\"theta_{n_theta}\",\n\t                body_blob=f\"theta = {theta:.4f}\",\n\t                vector=[math.cos(theta), math.sin(theta)],\n\t            )\n\t        # retrieval\n", "        theta_1 = t.get(row_id=\"theta_1\")\n\t        assert abs(theta_1[\"vector\"][0] - math.cos(math.pi * 2 / N)) < 3.0e-8\n\t        assert abs(theta_1[\"vector\"][1] - math.sin(math.pi * 2 / N)) < 3.0e-8\n\t        # ANN\n\t        # a vector halfway between 0 and 1 inserted above\n\t        query_theta = 1 * math.pi * 2 / (2 * N)\n\t        ref_vector = [math.cos(query_theta), math.sin(query_theta)]\n\t        ann_results = list(t.ann_search(ref_vector, n=4))\n\t        assert {r[\"row_id\"] for r in ann_results[:2]} == {\"theta_1\", \"theta_0\"}\n\t        assert {r[\"row_id\"] for r in ann_results[2:4]} == {\"theta_2\", \"theta_7\"}\n", "        t.clear()\n\tif __name__ == \"__main__\":\n\t    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n\t    #   tests.integration.test_tableclasses_vectorcassandratable\n\t    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\t    s = createDBSessionSingleton()\n\t    k = getDBKeyspace()\n\t    TestVectorCassandraTable().test_crud(s, k)\n"]}
{"filename": "tests/integration/test_tableclasses_metadatacassandratable.py", "chunked_list": ["\"\"\"\n\tTable classes integration test - MetadataCassandraTable\n\t\"\"\"\n\timport pytest\n\tfrom cassio.table.tables import (\n\t    MetadataCassandraTable,\n\t)\n\t@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\n\tclass TestMetadataCassandraTable:\n\t    def test_crud(self, db_session, db_keyspace):\n", "        table_name = \"m_ct\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n\t        #\n\t        t = MetadataCassandraTable(\n\t            db_session, db_keyspace, table_name, primary_key_type=\"TEXT\"\n\t        )\n\t        t.put(row_id=\"row1\", body_blob=\"bb1\")\n\t        gotten1 = t.get(row_id=\"row1\")\n\t        assert gotten1 == {\"row_id\": \"row1\", \"body_blob\": \"bb1\", \"metadata\": {}}\n\t        gotten1_s = list(t.find_entries(row_id=\"row1\", n=1))[0]\n", "        assert gotten1_s == {\"row_id\": \"row1\", \"body_blob\": \"bb1\", \"metadata\": {}}\n\t        t.put(row_id=\"row2\", metadata={})\n\t        gotten2 = t.get(row_id=\"row2\")\n\t        assert gotten2 == {\"row_id\": \"row2\", \"body_blob\": None, \"metadata\": {}}\n\t        md3 = {\"a\": 1, \"b\": \"Bee\", \"c\": True}\n\t        md3_string = {\"a\": \"1.0\", \"b\": \"Bee\", \"c\": \"true\"}\n\t        t.put(row_id=\"row3\", metadata=md3)\n\t        gotten3 = t.get(row_id=\"row3\")\n\t        assert gotten3 == {\"row_id\": \"row3\", \"body_blob\": None, \"metadata\": md3_string}\n\t        md4 = {\"c1\": True, \"c2\": True, \"c3\": True}\n", "        md4_string = {\"c1\": \"true\", \"c2\": \"true\", \"c3\": \"true\"}\n\t        t.put(row_id=\"row4\", metadata=md4)\n\t        gotten4 = t.get(row_id=\"row4\")\n\t        assert gotten4 == {\"row_id\": \"row4\", \"body_blob\": None, \"metadata\": md4_string}\n\t        # metadata searches:\n\t        md_gotten3a = t.get(metadata={\"a\": 1})\n\t        assert md_gotten3a == gotten3\n\t        md_gotten3b = t.get(metadata={\"b\": \"Bee\", \"c\": True})\n\t        assert md_gotten3b == gotten3\n\t        md_gotten4a = t.get(metadata={\"c1\": True, \"c3\": True})\n", "        assert md_gotten4a == gotten4\n\t        md_gotten4b = t.get(row_id=\"row4\", metadata={\"c1\": True, \"c3\": True})\n\t        assert md_gotten4b == gotten4\n\t        # 'search' proper\n\t        t.put(row_id=\"twin_a\", metadata={\"twin\": True, \"index\": 0})\n\t        t.put(row_id=\"twin_b\", metadata={\"twin\": True, \"index\": 1})\n\t        md_twins_gotten = sorted(\n\t            t.find_entries(metadata={\"twin\": True}, n=3),\n\t            key=lambda res: int(float(res[\"metadata\"][\"index\"])),\n\t        )\n", "        expected = [\n\t            {\n\t                \"metadata\": {\"twin\": \"true\", \"index\": \"0.0\"},\n\t                \"row_id\": \"twin_a\",\n\t                \"body_blob\": None,\n\t            },\n\t            {\n\t                \"metadata\": {\"twin\": \"true\", \"index\": \"1.0\"},\n\t                \"row_id\": \"twin_b\",\n\t                \"body_blob\": None,\n", "            },\n\t        ]\n\t        assert md_twins_gotten == expected\n\t        assert list(t.find_entries(row_id=\"fake\", n=10)) == []\n\t        #\n\t        t.clear()\n\t    def test_md_routing(self, db_session, db_keyspace):\n\t        test_md = {\"mds\": \"string\", \"mdn\": 255, \"mdb\": True}\n\t        test_md_string = {\"mds\": \"string\", \"mdn\": \"255.0\", \"mdb\": \"true\"}\n\t        #\n", "        table_name_all = \"m_ct_rtall\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_all};\")\n\t        t_all = MetadataCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name_all,\n\t            primary_key_type=\"TEXT\",\n\t            metadata_indexing=\"all\",\n\t        )\n\t        t_all.put(row_id=\"row1\", body_blob=\"bb1\", metadata=test_md)\n", "        gotten_all = list(t_all.find_entries(metadata={\"mds\": \"string\"}, n=1))[0]\n\t        assert gotten_all[\"metadata\"] == test_md_string\n\t        t_all.clear()\n\t        #\n\t        table_name_none = \"m_ct_rtnone\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_none};\")\n\t        t_none = MetadataCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name_none,\n", "            primary_key_type=\"TEXT\",\n\t            metadata_indexing=\"none\",\n\t        )\n\t        t_none.put(row_id=\"row1\", body_blob=\"bb1\", metadata=test_md)\n\t        with pytest.raises(ValueError):\n\t            # querying on non-indexed metadata fields:\n\t            t_none.find_entries(metadata={\"mds\": \"string\"}, n=1)\n\t        gotten_none = t_none.get(row_id=\"row1\")\n\t        assert gotten_none[\"metadata\"] == test_md_string\n\t        t_none.clear()\n", "        #\n\t        test_md_allowdeny = {\n\t            \"mdas\": \"MDAS\",\n\t            \"mdds\": \"MDDS\",\n\t            \"mdan\": 255,\n\t            \"mddn\": 127,\n\t            \"mdab\": True,\n\t            \"mddb\": True,\n\t        }\n\t        test_md_allowdeny_string = {\n", "            \"mdas\": \"MDAS\",\n\t            \"mdds\": \"MDDS\",\n\t            \"mdan\": \"255.0\",\n\t            \"mddn\": \"127.0\",\n\t            \"mdab\": \"true\",\n\t            \"mddb\": \"true\",\n\t        }\n\t        #\n\t        table_name_allow = \"m_ct_rtallow\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_allow};\")\n", "        t_allow = MetadataCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name_allow,\n\t            primary_key_type=\"TEXT\",\n\t            metadata_indexing=(\"allow\", {\"mdas\", \"mdan\", \"mdab\"}),\n\t        )\n\t        t_allow.put(row_id=\"row1\", body_blob=\"bb1\", metadata=test_md_allowdeny)\n\t        with pytest.raises(ValueError):\n\t            t_allow.find_entries(metadata={\"mdds\": \"MDDS\"}, n=1)\n", "        gotten_allow = list(t_allow.find_entries(metadata={\"mdas\": \"MDAS\"}, n=1))[0]\n\t        assert gotten_allow[\"metadata\"] == test_md_allowdeny_string\n\t        t_allow.clear()\n\t        #\n\t        table_name_deny = \"m_ct_rtdeny\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name_deny};\")\n\t        t_deny = MetadataCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name_deny,\n", "            primary_key_type=\"TEXT\",\n\t            metadata_indexing=(\"deny\", {\"mdds\", \"mddn\", \"mddb\"}),\n\t        )\n\t        t_deny.put(row_id=\"row1\", body_blob=\"bb1\", metadata=test_md_allowdeny)\n\t        with pytest.raises(ValueError):\n\t            t_deny.find_entries(metadata={\"mdds\": \"MDDS\"}, n=1)\n\t        gotten_deny = list(t_deny.find_entries(metadata={\"mdas\": \"MDAS\"}, n=1))[0]\n\t        assert gotten_deny[\"metadata\"] == test_md_allowdeny_string\n\t        t_deny.clear()\n\tif __name__ == \"__main__\":\n", "    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n\t    #   tests.integration.test_tableclasses_MetadataCassandraTable\n\t    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\t    s = createDBSessionSingleton()\n\t    k = getDBKeyspace()\n\t    TestMetadataCassandraTable().test_crud(s, k)\n\t    TestMetadataCassandraTable().test_md_routing(s, k)\n"]}
{"filename": "tests/integration/test_tableclasses_elasticcassandratable.py", "chunked_list": ["\"\"\"\n\tTable classes integration test - ElasticCassandraTable\n\t\"\"\"\n\timport pytest\n\tfrom cassio.table.tables import (\n\t    ElasticCassandraTable,\n\t)\n\t@pytest.mark.usefixtures(\"db_session\", \"db_keyspace\")\n\tclass TestElasticCassandraTable:\n\t    def test_crud(self, db_session, db_keyspace):\n", "        table_name = \"e_ct\"\n\t        db_session.execute(f\"DROP TABLE IF EXISTS {db_keyspace}.{table_name};\")\n\t        #\n\t        t = ElasticCassandraTable(\n\t            db_session,\n\t            db_keyspace,\n\t            table_name,\n\t            keys=[\"k1\", \"k2\"],\n\t            primary_key_type=[\"INT\", \"TEXT\"],\n\t        )\n", "        t.put(k1=1, k2=\"one\", body_blob=\"bb_1\")\n\t        gotten1 = t.get(k1=1, k2=\"one\")\n\t        assert gotten1 == {\"k1\": 1, \"k2\": \"one\", \"body_blob\": \"bb_1\"}\n\t        t.clear()\n\tif __name__ == \"__main__\":\n\t    # TEST_DB_MODE=LOCAL_CASSANDRA python -m pdb -m  \\\n\t    #   tests.integration.test_tableclasses_elasticcassandratable\n\t    from ..conftest import createDBSessionSingleton, getDBKeyspace\n\t    s = createDBSessionSingleton()\n\t    k = getDBKeyspace()\n", "    TestElasticCassandraTable().test_crud(s, k)\n"]}
{"filename": "tests/unit/test_metadata_policy_normalization.py", "chunked_list": ["\"\"\"\n\tNormalization of metadata policy specification options\n\t\"\"\"\n\tfrom cassio.table.table_types import (\n\t    MetadataIndexingMode,\n\t)\n\tfrom cassio.table.mixins import MetadataMixin\n\tclass TestNormalizeMetadataPolicy:\n\t    def test_normalize_metadata_policy(self):\n\t        #\n", "        mdp1 = MetadataMixin._normalize_metadata_indexing_policy(\"all\")\n\t        assert mdp1 == (MetadataIndexingMode.DEFAULT_TO_SEARCHABLE, set())\n\t        #\n\t        mdp2 = MetadataMixin._normalize_metadata_indexing_policy(\"none\")\n\t        assert mdp2 == (MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE, set())\n\t        #\n\t        mdp3 = MetadataMixin._normalize_metadata_indexing_policy(\n\t            (\"default_to_Unsearchable\", [\"x\", \"y\"]),\n\t        )\n\t        assert mdp3 == (MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE, {\"x\", \"y\"})\n", "        #\n\t        mdp4 = MetadataMixin._normalize_metadata_indexing_policy(\n\t            (\"DenyList\", [\"z\"]),\n\t        )\n\t        assert mdp4 == (MetadataIndexingMode.DEFAULT_TO_SEARCHABLE, {\"z\"})\n\t        # s\n\t        mdp5 = MetadataMixin._normalize_metadata_indexing_policy(\n\t            (\"deny_LIST\", \"singlefield\")\n\t        )\n\t        assert mdp5 == (MetadataIndexingMode.DEFAULT_TO_SEARCHABLE, {\"singlefield\"})\n"]}
{"filename": "tests/unit/test_tableclasses_cql_generation.py", "chunked_list": ["\"\"\"\n\tCQL for mixin-based table classes tests\n\t\"\"\"\n\tfrom cassio.table.tables import (\n\t    VectorCassandraTable,\n\t    ClusteredElasticMetadataVectorCassandraTable,\n\t)\n\tclass TestTableClassesCQLGeneration:\n\t    def test_vector_cassandra_table(self, mock_db_session):\n\t        vt = VectorCassandraTable(\n", "            mock_db_session, \"k\", \"tn\", vector_dimension=765, primary_key_type=\"UUID\"\n\t        )\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"CREATE TABLE IF NOT EXISTS k.tn (  row_id UUID,   body_blob TEXT,   vector VECTOR<FLOAT,765>, PRIMARY KEY ( ( row_id )   )) ;\",  # noqa: E501\n\t                    tuple(),\n\t                ),\n\t                (\n\t                    \"CREATE CUSTOM INDEX IF NOT EXISTS idx_vector_tn ON k.tn (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n", "                    tuple(),\n\t                ),\n\t            ]\n\t        )\n\t        vt.delete(row_id=\"ROWID\")\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"DELETE FROM k.tn WHERE row_id = ?;\",\n\t                    (\"ROWID\",),\n", "                ),\n\t            ]\n\t        )\n\t        vt.get(row_id=\"ROWID\")\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE row_id = ? ;\",\n\t                    (\"ROWID\",),\n\t                ),\n", "            ]\n\t        )\n\t        vt.put(row_id=\"ROWID\", body_blob=\"BODYBLOB\", vector=\"VECTOR\")\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"INSERT INTO k.tn (body_blob, vector, row_id) VALUES (?, ?, ?)  ;\",\n\t                    (\n\t                        \"BODYBLOB\",\n\t                        \"VECTOR\",\n", "                        \"ROWID\",\n\t                    ),\n\t                ),\n\t            ]\n\t        )\n\t        vt.ann_search([10, 11], 2)\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn ORDER BY vector ANN OF ?  LIMIT ?;\",\n", "                    (\n\t                        [10, 11],\n\t                        2,\n\t                    ),\n\t                ),\n\t            ]\n\t        )\n\t        vt.clear()\n\t        mock_db_session.assert_last_equal(\n\t            [\n", "                (\n\t                    \"TRUNCATE TABLE k.tn;\",\n\t                    tuple(),\n\t                ),\n\t            ]\n\t        )\n\t    def test_clustered_elastic_metadata_vector_cassandra_table(self, mock_db_session):\n\t        cemvt = ClusteredElasticMetadataVectorCassandraTable(\n\t            mock_db_session,\n\t            \"k\",\n", "            \"tn\",\n\t            keys=[\"a\", \"b\"],\n\t            vector_dimension=765,\n\t            primary_key_type=[\"PUUID\", \"AT\", \"BT\"],\n\t            ttl_seconds=123,\n\t            partition_id=\"PRE-PART-ID\",\n\t        )\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n", "                    \"CREATE TABLE IF NOT EXISTS k.tn (  partition_id PUUID,   key_desc TEXT,   key_vals TEXT,   body_blob TEXT,   vector VECTOR<FLOAT,765>, attributes_blob TEXT,  metadata_s MAP<TEXT,TEXT>, PRIMARY KEY ( ( partition_id ) , key_desc, key_vals )) WITH CLUSTERING ORDER BY (key_desc ASC, key_vals ASC);\",  # noqa: E501\n\t                    tuple(),\n\t                ),\n\t                (\n\t                    \"CREATE CUSTOM INDEX IF NOT EXISTS idx_vector_tn ON k.tn (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n\t                    tuple(),\n\t                ),\n\t                (\n\t                    \"CREATE CUSTOM INDEX IF NOT EXISTS eidx_metadata_s_tn ON k.tn (ENTRIES(metadata_s)) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\",  # noqa: E501\n\t                    tuple(),\n", "                ),\n\t            ]\n\t        )\n\t        cemvt.delete(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\")\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"DELETE FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ?;\",  # noqa: E501\n\t                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\"),\n\t                ),\n", "            ]\n\t        )\n\t        cemvt.get(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\")\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n\t                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\"),\n\t                ),\n\t            ]\n", "        )\n\t        cemvt.delete_partition(partition_id=\"PARTITIONID\")\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"DELETE FROM k.tn WHERE partition_id = ?;\",\n\t                    (\"PARTITIONID\",),\n\t                ),\n\t            ]\n\t        )\n", "        cemvt.put(\n\t            partition_id=\"PARTITIONID\",\n\t            a=\"A\",\n\t            b=\"B\",\n\t            body_blob=\"BODYBLOB\",\n\t            vector=\"VECTOR\",\n\t        )\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n", "                    \"INSERT INTO k.tn (body_blob, vector, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n\t                    (\n\t                        \"BODYBLOB\",\n\t                        \"VECTOR\",\n\t                        '[\"a\",\"b\"]',\n\t                        '[\"A\",\"B\"]',\n\t                        \"PARTITIONID\",\n\t                        123,\n\t                    ),\n\t                ),\n", "            ]\n\t        )\n\t        md1 = {\"num1\": 123, \"num2\": 456, \"str1\": \"STR1\", \"tru1\": True}\n\t        md2 = {\"tru1\": True, \"tru2\": True}\n\t        cemvt.put(\n\t            partition_id=\"PARTITIONID\",\n\t            a=\"A\",\n\t            b=\"B\",\n\t            body_blob=\"BODYBLOB\",\n\t            vector=\"VECTOR\",\n", "            metadata=md1,\n\t        )\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"INSERT INTO k.tn (body_blob, vector, metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n\t                    (\n\t                        \"BODYBLOB\",\n\t                        \"VECTOR\",\n\t                        {\n", "                            \"str1\": \"STR1\",\n\t                            \"num1\": \"123.0\",\n\t                            \"num2\": \"456.0\",\n\t                            \"tru1\": \"true\",\n\t                        },\n\t                        '[\"a\",\"b\"]',\n\t                        '[\"A\",\"B\"]',\n\t                        \"PARTITIONID\",\n\t                        123,\n\t                    ),\n", "                ),\n\t            ]\n\t        )\n\t        cemvt.put(\n\t            partition_id=\"PARTITIONID\",\n\t            a=\"A\",\n\t            b=\"B\",\n\t            body_blob=\"BODYBLOB\",\n\t            vector=\"VECTOR\",\n\t            metadata=md2,\n", "        )\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"INSERT INTO k.tn (body_blob, vector, metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n\t                    (\n\t                        \"BODYBLOB\",\n\t                        \"VECTOR\",\n\t                        {\"tru2\": \"true\", \"tru1\": \"true\"},\n\t                        '[\"a\",\"b\"]',\n", "                        '[\"A\",\"B\"]',\n\t                        \"PARTITIONID\",\n\t                        123,\n\t                    ),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.put(partition_id=\"PARTITIONID\", a=\"A\", b=\"B\", metadata=md2)\n\t        mock_db_session.assert_last_equal(\n\t            [\n", "                (\n\t                    \"INSERT INTO k.tn (metadata_s, key_desc, key_vals, partition_id) VALUES (?, ?, ?, ?) USING TTL ? ;\",  # noqa: E501\n\t                    (\n\t                        {\"tru2\": \"true\", \"tru1\": \"true\"},\n\t                        '[\"a\",\"b\"]',\n\t                        '[\"A\",\"B\"]',\n\t                        \"PARTITIONID\",\n\t                        123,\n\t                    ),\n\t                ),\n", "            ]\n\t        )\n\t        cemvt.get_partition(partition_id=\"PARTITIONID\", n=10)\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE partition_id = ? LIMIT ?;\",\n\t                    (\"PARTITIONID\", 10),\n\t                ),\n\t            ]\n", "        )\n\t        cemvt.get_partition(partition_id=\"PARTITIONID\")\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE partition_id = ? ;\",\n\t                    (\"PARTITIONID\",),\n\t                ),\n\t            ]\n\t        )\n", "        cemvt.get_partition()\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE partition_id = ? ;\",\n\t                    (\"PRE-PART-ID\",),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.ann_search([10, 11], 2, a=\"A\", b=\"B\", partition_id=\"PARTITIONID\")\n", "        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n\t                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PARTITIONID\", [10, 11], 2),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.ann_search([10, 11], 2, a=\"A\", b=\"B\")\n\t        mock_db_session.assert_last_equal(\n", "            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n\t                    ('[\"a\",\"b\"]', '[\"A\",\"B\"]', \"PRE-PART-ID\", [10, 11], 2),\n\t                ),\n\t            ]\n\t        )\n\t        search_md = {\"mdks\": \"mdv\", \"mdkn\": 123, \"mdke\": True}\n\t        cemvt.get(partition_id=\"MDPART\", a=\"MDA\", b=\"MDB\", metadata=search_md)\n\t        mock_db_session.assert_last_equal(\n", "            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n\t                    (\"true\", \"123.0\", \"mdv\", '[\"a\",\"b\"]', '[\"MDA\",\"MDB\"]', \"MDPART\"),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.ann_search(\n\t            [100, 101], 9, a=\"MDA\", b=\"MDB\", partition_id=\"MDPART\", metadata=search_md\n\t        )\n", "        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n\t                    (\n\t                        \"true\",\n\t                        \"123.0\",\n\t                        \"mdv\",\n\t                        '[\"a\",\"b\"]',\n\t                        '[\"MDA\",\"MDB\"]',\n", "                        \"MDPART\",\n\t                        [100, 101],\n\t                        9,\n\t                    ),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.ann_search([100, 101], 9, a=\"MDA\", b=\"MDB\", metadata=search_md)\n\t        mock_db_session.assert_last_equal(\n\t            [\n", "                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n\t                    (\n\t                        \"true\",\n\t                        \"123.0\",\n\t                        \"mdv\",\n\t                        '[\"a\",\"b\"]',\n\t                        '[\"MDA\",\"MDB\"]',\n\t                        \"PRE-PART-ID\",\n\t                        [100, 101],\n", "                        9,\n\t                    ),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.get_partition(partition_id=\"MDPART\", metadata=search_md)\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND partition_id = ? ;\",  # noqa: E501\n", "                    (\"true\", \"123.0\", \"mdv\", \"MDPART\"),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.get_partition(metadata=search_md)\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdkn'] = ? AND metadata_s['mdks'] = ? AND partition_id = ? ;\",  # noqa: E501\n\t                    (\"true\", \"123.0\", \"mdv\", \"PRE-PART-ID\"),\n", "                ),\n\t            ]\n\t        )\n\t        search_md_part = {\"mdke\": True, \"mdke2\": True}\n\t        cemvt.get(partition_id=\"MDPART\", a=\"MDA\", b=\"MDB\", metadata=search_md_part)\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ;\",  # noqa: E501\n\t                    (\"true\", \"true\", '[\"a\",\"b\"]', '[\"MDA\",\"MDB\"]', \"MDPART\"),\n", "                ),\n\t            ]\n\t        )\n\t        cemvt.ann_search(\n\t            [100, 101],\n\t            9,\n\t            a=\"MDA\",\n\t            b=\"MDB\",\n\t            partition_id=\"MDPART\",\n\t            metadata=search_md_part,\n", "        )\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n\t                    (\n\t                        \"true\",\n\t                        \"true\",\n\t                        '[\"a\",\"b\"]',\n\t                        '[\"MDA\",\"MDB\"]',\n", "                        \"MDPART\",\n\t                        [100, 101],\n\t                        9,\n\t                    ),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.ann_search([100, 101], 9, a=\"MDA\", b=\"MDB\", metadata=search_md_part)\n\t        mock_db_session.assert_last_equal(\n\t            [\n", "                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND key_desc = ? AND key_vals = ? AND partition_id = ? ORDER BY vector ANN OF ? LIMIT ?;\",  # noqa: E501\n\t                    (\n\t                        \"true\",\n\t                        \"true\",\n\t                        '[\"a\",\"b\"]',\n\t                        '[\"MDA\",\"MDB\"]',\n\t                        \"PRE-PART-ID\",\n\t                        [100, 101],\n\t                        9,\n", "                    ),\n\t                ),\n\t            ]\n\t        )\n\t        cemvt.get_partition(partition_id=\"MDPART\", metadata=search_md_part)\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND partition_id = ? ;\",  # noqa: E501\n\t                    (\"true\", \"true\", \"MDPART\"),\n", "                ),\n\t            ]\n\t        )\n\t        cemvt.get_partition(metadata=search_md_part)\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"SELECT * FROM k.tn WHERE metadata_s['mdke'] = ? AND metadata_s['mdke2'] = ? AND partition_id = ? ;\",  # noqa: E501\n\t                    (\"true\", \"true\", \"PRE-PART-ID\"),\n\t                ),\n", "            ]\n\t        )\n\t        cemvt.clear()\n\t        mock_db_session.assert_last_equal(\n\t            [\n\t                (\n\t                    \"TRUNCATE TABLE k.tn;\",\n\t                    tuple(),\n\t                ),\n\t            ]\n", "        )\n"]}
{"filename": "tests/unit/test_metadata_string_coercion.py", "chunked_list": ["\"\"\"\n\tStringification of everything in the simple metadata handling\n\t\"\"\"\n\tfrom cassio.table.mixins import MetadataMixin\n\tclass TestNormalizeMetadataPolicy:\n\t    def test_normalize_metadata_policy(self):\n\t        md_mixin = MetadataMixin(\"s\", \"k\", \"t\", skip_provisioning=True)\n\t        stringified = md_mixin._split_metadata_fields(\n\t            {\n\t                \"integer\": 1,\n", "                \"float\": 2.0,\n\t                \"boolean\": True,\n\t                \"null\": None,\n\t                \"string\": \"letter E\",\n\t                \"something\": RuntimeError(\"You cannot do this!\"),\n\t            }\n\t        )\n\t        expected = {\n\t            \"integer\": \"1.0\",\n\t            \"float\": \"2.0\",\n", "            \"boolean\": \"true\",\n\t            \"null\": \"null\",\n\t            \"string\": \"letter E\",\n\t            \"something\": str(RuntimeError(\"You cannot do this!\")),\n\t        }\n\t        assert stringified == {\"metadata_s\": expected}\n"]}
{"filename": "tests/unit/test_imports.py", "chunked_list": ["\"\"\"\n\tJust importing everything to smoke-test python3.8+ syntax issues, etc.\n\tTODO: make this more robust in making sure all code is imported.\n\t\"\"\"\n\tclass TestImports:\n\t    def test_import_db_extractor(self):\n\t        from cassio.db_extractor import CassandraExtractor  # type: ignore  # noqa: F401\n\t    def test_import_history(self):\n\t        from cassio.history import StoredBlobHistory  # type: ignore  # noqa: F401\n\t    def test_import_keyvalue(self):\n", "        from cassio.keyvalue import KVCache  # type: ignore  # noqa: F401\n\t    def test_import_table(self):\n\t        from cassio.table.tables import PlainCassandraTable  # noqa: F401\n\t    def test_import_utils(self):\n\t        from cassio.utils import distance_metrics  # noqa: F401\n\t    def test_import_vector(self):\n\t        from cassio.vector import VectorTable  # noqa: F401\n"]}
{"filename": "src/cassio/__init__.py", "chunked_list": ["import cassio.vector\n\timport cassio.keyvalue\n\timport cassio.db_extractor\n\timport cassio.history\n"]}
{"filename": "src/cassio/db_extractor/__init__.py", "chunked_list": ["from cassio.db_extractor.cassandra_extractor import CassandraExtractor\n"]}
{"filename": "src/cassio/db_extractor/cassandra_extractor.py", "chunked_list": ["\"\"\"\n\tAn extractor able to resolve single-row lookups from Cassandra tables in\n\ta keyspace, with a fair amount of metadata inspection.\n\t\"\"\"\n\tfrom functools import reduce\n\tfrom typing import List\n\tfrom cassandra.query import SimpleStatement\n\tRETRIEVE_ONE_ROW_CQL_TEMPLATE = 'SELECT * FROM {keyspace}.{table} WHERE {whereClause} LIMIT 1'\n\tdef _table_primary_key_columns(session, keyspace, table) -> List[str]:\n\t    table = session.cluster.metadata.keyspaces[keyspace].tables[table]\n", "    return [\n\t        col.name for col in table.partition_key\n\t    ] + [\n\t        col.name for col in table.clustering_key\n\t    ]\n\tclass CassandraExtractor:\n\t    def __init__(self, session, keyspace, field_mapper, literal_nones):\n\t        self.session = session\n\t        self.keyspace = keyspace\n\t        self.field_mapper = field_mapper\n", "        self.literal_nones = literal_nones  # TODO: handle much better\n\t        # derived fields\n\t        self.tables_needed = {fmv[0] for fmv in field_mapper.values()}\n\t        self.primary_key_map = {\n\t            table: _table_primary_key_columns(self.session, self.keyspace, table)\n\t            for table in self.tables_needed\n\t        }\n\t        # all primary-key values needed across tables\n\t        self.requiredParameters = list(reduce(lambda accum, nw: accum | set(nw), self.primary_key_map.values(), set()))\n\t        # TODOs:\n", "        #   move this getter creation someplace else\n\t        #   query a table only once (grouping required variables by source table,\n\t        #   selecting only those unless function passed)\n\t        def _getter(**kwargs):\n\t            def _retrieve_field(_table2, _key_columns, _column_or_extractor, _key_value_map):\n\t                selector = SimpleStatement(RETRIEVE_ONE_ROW_CQL_TEMPLATE.format(\n\t                    keyspace=keyspace,\n\t                    table=_table2,\n\t                    whereClause=' AND '.join(\n\t                        f'{kc} = %s'\n", "                        for kc in _key_columns\n\t                    ),\n\t                ))\n\t                values = tuple([\n\t                    _key_value_map[kc]\n\t                    for kc in _key_columns\n\t                ])\n\t                row = session.execute(selector, values).one()\n\t                if row:\n\t                    if callable(_column_or_extractor):\n", "                        return _column_or_extractor(row)\n\t                    else:\n\t                        return getattr(row, _column_or_extractor)\n\t                else:\n\t                    if literal_nones:\n\t                        return None\n\t                    else:\n\t                        raise ValueError('No data found for %s from %s.%s' % (\n\t                            str(_column_or_extractor),\n\t                            keyspace,\n", "                            _table2,\n\t                        ))\n\t            return {\n\t                field: _retrieve_field(table, self.primary_key_map[table], columnOrExtractor, kwargs)\n\t                for field, (table, columnOrExtractor) in field_mapper.items()\n\t            }\n\t        self.getter = _getter\n\t    def __call__(self, **kwargs):\n\t        return self.getter(**kwargs)\n"]}
{"filename": "src/cassio/table/mixins.py", "chunked_list": ["from operator import itemgetter\n\timport json\n\tfrom typing import (\n\t    Any,\n\t    List,\n\t    Dict,\n\t    Iterable,\n\t    Optional,\n\t    Set,\n\t    Tuple,\n", "    Union,\n\t)\n\tfrom cassandra.cluster import ResponseFuture  # type: ignore\n\tfrom cassio.utils.vector.distance_metrics import distance_metrics\n\tfrom cassio.table.cql import (\n\t    CQLOpType,\n\t    DELETE_CQL_TEMPLATE,\n\t    SELECT_CQL_TEMPLATE,\n\t    CREATE_INDEX_CQL_TEMPLATE,\n\t    # CREATE_KEYS_INDEX_CQL_TEMPLATE,\n", "    CREATE_ENTRIES_INDEX_CQL_TEMPLATE,\n\t    SELECT_ANN_CQL_TEMPLATE,\n\t)\n\tfrom cassio.table.table_types import (\n\t    ColumnSpecType,\n\t    RowType,\n\t    RowWithDistanceType,\n\t    normalize_type_desc,\n\t    rearrange_pk_type,\n\t    MetadataIndexingMode,\n", "    MetadataIndexingPolicy,\n\t    is_metadata_field_indexed,\n\t)\n\tfrom cassio.table.base_table import BaseTable\n\tclass BaseTableMixin(BaseTable):\n\t    \"\"\"All other mixins should inherit from this one.\"\"\"\n\t    pass\n\tclass ClusteredMixin(BaseTableMixin):\n\t    def __init__(\n\t        self,\n", "        *pargs: Any,\n\t        partition_id_type: Union[str, List[str]] = [\"TEXT\"],\n\t        partition_id: Optional[Any] = None,\n\t        ordering_in_partition: str = \"ASC\",\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        self.partition_id_type = normalize_type_desc(partition_id_type)\n\t        self.partition_id = partition_id\n\t        self.ordering_in_partition = ordering_in_partition.upper()\n\t        super().__init__(*pargs, **kwargs)\n", "    def _schema_pk(self) -> List[ColumnSpecType]:\n\t        assert len(self.partition_id_type) == 1\n\t        return [\n\t            (\"partition_id\", self.partition_id_type[0]),\n\t        ]\n\t    def _schema_cc(self) -> List[ColumnSpecType]:\n\t        return self._schema_row_id()\n\t    def _extract_where_clause_blocks(\n\t        self, args_dict: Any\n\t    ) -> Tuple[Any, List[str], Tuple[Any, ...]]:\n", "        \"\"\"\n\t        If a null partition_id arrives to WHERE construction, it is silently\n\t        discarded from the set of conditions to create.\n\t        This enables e.g. ANN vector search across partitions of a clustered table.\n\t        It is the database's responsibility to raise an error if unacceptable.\n\t        \"\"\"\n\t        if \"partition_id\" in args_dict and args_dict[\"partition_id\"] is None:\n\t            cleaned_args_dict = {\n\t                k: v for k, v in args_dict.items() if k != \"partition_id\"\n\t            }\n", "        else:\n\t            cleaned_args_dict = args_dict\n\t        #\n\t        return super()._extract_where_clause_blocks(cleaned_args_dict)\n\t    def _delete_partition(\n\t        self, is_async: bool, partition_id: Optional[str] = None\n\t    ) -> None:\n\t        _partition_id = self.partition_id if partition_id is None else partition_id\n\t        #\n\t        where_clause = \"WHERE \" + \"partition_id = %s\"\n", "        delete_cql_vals = (_partition_id,)\n\t        delete_cql = DELETE_CQL_TEMPLATE.format(\n\t            where_clause=where_clause,\n\t        )\n\t        if is_async:\n\t            return self.execute_cql_async(\n\t                delete_cql, args=delete_cql_vals, op_type=CQLOpType.WRITE\n\t            )\n\t        else:\n\t            self.execute_cql(delete_cql, args=delete_cql_vals, op_type=CQLOpType.WRITE)\n", "            return\n\t    def delete_partition(self, partition_id: Optional[str] = None) -> None:\n\t        self._delete_partition(is_async=False, partition_id=partition_id)\n\t        return None\n\t    def delete_partition_async(\n\t        self, partition_id: Optional[str] = None\n\t    ) -> ResponseFuture:\n\t        return self._delete_partition(is_async=True, partition_id=partition_id)\n\t    def _normalize_kwargs(self, args_dict: Dict[str, Any]) -> Dict[str, Any]:\n\t        # if partition id provided in call, takes precedence over instance value\n", "        arg_pid = args_dict.get(\"partition_id\")\n\t        instance_pid = self.partition_id\n\t        _partition_id = instance_pid if arg_pid is None else arg_pid\n\t        new_args_dict = {\n\t            **{\"partition_id\": _partition_id},\n\t            **args_dict,\n\t        }\n\t        return super()._normalize_kwargs(new_args_dict)\n\t    def get_partition(\n\t        self, partition_id: Optional[str] = None, n: Optional[int] = None, **kwargs: Any\n", "    ) -> Iterable[RowType]:\n\t        _partition_id = self.partition_id if partition_id is None else partition_id\n\t        get_p_cql_vals: Tuple[Any, ...] = tuple()\n\t        #\n\t        # TODO: work on a columns: Optional[List[str]] = None\n\t        # (but with nuanced handling of the column-magic we have here)\n\t        columns = None\n\t        if columns is None:\n\t            columns_desc = \"*\"\n\t        else:\n", "            # TODO: handle translations here?\n\t            # columns_desc = \", \".join(columns)\n\t            raise NotImplementedError(\"Column selection is not implemented.\")\n\t        # WHERE can admit other sources (e.g. medata if the corresponding mixin)\n\t        # so we escalate to standard WHERE-creation route and reinject the partition\n\t        n_kwargs = self._normalize_kwargs(\n\t            {\n\t                **{\"partition_id\": _partition_id},\n\t                **kwargs,\n\t            }\n", "        )\n\t        (args_dict, wc_blocks, wc_vals) = self._extract_where_clause_blocks(n_kwargs)\n\t        # check for exhaustion:\n\t        assert args_dict == {}\n\t        where_clause = \"WHERE \" + \" AND \".join(wc_blocks)\n\t        where_cql_vals = list(wc_vals)\n\t        #\n\t        if n is None:\n\t            limit_clause = \"\"\n\t            limit_cql_vals = []\n", "        else:\n\t            limit_clause = \"LIMIT %s\"\n\t            limit_cql_vals = [n]\n\t        #\n\t        select_cql = SELECT_CQL_TEMPLATE.format(\n\t            columns_desc=columns_desc,\n\t            where_clause=where_clause,\n\t            limit_clause=limit_clause,\n\t        )\n\t        get_p_cql_vals = tuple(where_cql_vals + limit_cql_vals)\n", "        return (\n\t            self._normalize_row(raw_row)\n\t            for raw_row in self.execute_cql(\n\t                select_cql,\n\t                args=get_p_cql_vals,\n\t                op_type=CQLOpType.READ,\n\t            )\n\t        )\n\t    def get_partition_async(\n\t        self, partition_id: Optional[str] = None, n: Optional[int] = None, **kwargs: Any\n", "    ) -> ResponseFuture:\n\t        raise NotImplementedError(\"Asynchronous reads are not supported.\")\n\tclass MetadataMixin(BaseTableMixin):\n\t    def __init__(\n\t        self,\n\t        *pargs: Any,\n\t        metadata_indexing: Union[Tuple[str, Iterable[str]], str] = \"all\",\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        self.metadata_indexing_policy = self._normalize_metadata_indexing_policy(\n", "            metadata_indexing\n\t        )\n\t        super().__init__(*pargs, **kwargs)\n\t    @staticmethod\n\t    def _normalize_metadata_indexing_policy(\n\t        metadata_indexing: Union[Tuple[str, Iterable[str]], str]\n\t    ) -> MetadataIndexingPolicy:\n\t        mode: MetadataIndexingMode\n\t        fields: Set[str]\n\t        # metadata indexing policy normalization:\n", "        if isinstance(metadata_indexing, str):\n\t            if metadata_indexing.lower() == \"all\":\n\t                mode, fields = (MetadataIndexingMode.DEFAULT_TO_SEARCHABLE, set())\n\t            elif metadata_indexing.lower() == \"none\":\n\t                mode, fields = (MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE, set())\n\t            else:\n\t                raise ValueError(\n\t                    f\"Unsupported metadata_indexing value '{metadata_indexing}'\"\n\t                )\n\t        else:\n", "            assert len(metadata_indexing) == 2\n\t            # it's a 2-tuple (mode, fields) still to normalize\n\t            _mode, _field_spec = metadata_indexing\n\t            fields = {_field_spec} if isinstance(_field_spec, str) else set(_field_spec)\n\t            if _mode.lower() in {\n\t                \"default_to_unsearchable\",\n\t                \"allowlist\",\n\t                \"allow\",\n\t                \"allow_list\",\n\t            }:\n", "                mode = MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE\n\t            elif _mode.lower() in {\n\t                \"default_to_searchable\",\n\t                \"denylist\",\n\t                \"deny\",\n\t                \"deny_list\",\n\t            }:\n\t                mode = MetadataIndexingMode.DEFAULT_TO_SEARCHABLE\n\t            else:\n\t                raise ValueError(\n", "                    f\"Unsupported metadata indexing mode specification '{_mode}'\"\n\t                )\n\t        return (mode, fields)\n\t    def _schema_da(self) -> List[ColumnSpecType]:\n\t        return super()._schema_da() + [\n\t            (\"attributes_blob\", \"TEXT\"),\n\t            (\"metadata_s\", \"MAP<TEXT,TEXT>\"),\n\t        ]\n\t    def db_setup(self) -> None:\n\t        # Currently: an 'entries' index on the metadata_s column\n", "        super().db_setup()\n\t        #\n\t        entries_index_columns = [\"metadata_s\"]\n\t        for entries_index_column in entries_index_columns:\n\t            index_name = f\"eidx_{entries_index_column}\"\n\t            index_column = f\"{entries_index_column}\"\n\t            create_index_cql = CREATE_ENTRIES_INDEX_CQL_TEMPLATE.format(\n\t                index_name=index_name,\n\t                index_column=index_column,\n\t            )\n", "            self.execute_cql(create_index_cql, op_type=CQLOpType.SCHEMA)\n\t        #\n\t        return\n\t    @staticmethod\n\t    def _serialize_md_dict(md_dict: Dict[str, Any]) -> str:\n\t        return json.dumps(md_dict, separators=(\",\", \":\"), sort_keys=True)\n\t    @staticmethod\n\t    def _deserialize_md_dict(md_string: str) -> Dict[str, Any]:\n\t        return json.loads(md_string)\n\t    @staticmethod\n", "    def _coerce_string(value: Any) -> str:\n\t        if isinstance(value, str):\n\t            return value\n\t        elif isinstance(value, bool):\n\t            # bool MUST come before int in this chain of ifs!\n\t            return json.dumps(value)\n\t        elif isinstance(value, int):\n\t            # we don't want to store '1' and '1.0' differently\n\t            # for the sake of metadata-filtered retrieval:\n\t            return json.dumps(float(value))\n", "        elif isinstance(value, float):\n\t            return json.dumps(value)\n\t        elif value is None:\n\t            return json.dumps(value)\n\t        else:\n\t            # when all else fails ...\n\t            return str(value)\n\t    def _split_metadata_fields(self, md_dict: Dict[str, Any]) -> Dict[str, Any]:\n\t        \"\"\"\n\t        Split the *indexed* part of the metadata in separate parts,\n", "        one per Cassandra column.\n\t        Currently: everything gets cast to a string and goes to a single table\n\t        column. This means:\n\t            - strings are fine\n\t            - floats and integers v: they are cast to str(v)\n\t            - booleans: 'true'/'false' (JSON style)\n\t            - None => 'null' (JSON style)\n\t            - anything else v => str(v), no questions asked\n\t        Caveat: one gets strings back when reading metadata\n\t        \"\"\"\n", "        # TODO: more care about types here\n\t        stringy_part = {k: self._coerce_string(v) for k, v in md_dict.items()}\n\t        return {\n\t            \"metadata_s\": stringy_part,\n\t        }\n\t    def _normalize_row(self, raw_row: Any) -> Dict[str, Any]:\n\t        md_columns_defaults: Dict[str, Any] = {\n\t            \"metadata_s\": {},\n\t        }\n\t        pre_normalized = super()._normalize_row(raw_row)\n", "        #\n\t        row_rest = {\n\t            k: v\n\t            for k, v in pre_normalized.items()\n\t            if k not in md_columns_defaults\n\t            if k != \"attributes_blob\"\n\t        }\n\t        mergee_md_fields = {\n\t            k: v for k, v in pre_normalized.items() if k in md_columns_defaults\n\t        }\n", "        normalized_mergee_md_fields = {\n\t            k: v if v is not None else md_columns_defaults[k]\n\t            for k, v in mergee_md_fields.items()\n\t        }\n\t        r_md_from_s = {\n\t            k: v for k, v in normalized_mergee_md_fields[\"metadata_s\"].items()\n\t        }\n\t        #\n\t        raw_attr_blob = pre_normalized.get(\"attributes_blob\")\n\t        if raw_attr_blob is not None:\n", "            r_attrs = self._deserialize_md_dict(raw_attr_blob)\n\t        else:\n\t            r_attrs = {}\n\t        #\n\t        row_metadata = {\n\t            \"metadata\": {\n\t                **r_attrs,\n\t                **r_md_from_s,\n\t            },\n\t        }\n", "        #\n\t        normalized = {\n\t            **row_metadata,\n\t            **row_rest,\n\t        }\n\t        return normalized\n\t    def _normalize_kwargs(self, args_dict: Dict[str, Any]) -> Dict[str, Any]:\n\t        _metadata_input_dict = args_dict.get(\"metadata\", {})\n\t        # separate indexed and non-indexed (=attributes) as per indexing policy\n\t        metadata_indexed_dict = {\n", "            k: v\n\t            for k, v in _metadata_input_dict.items()\n\t            if is_metadata_field_indexed(k, self.metadata_indexing_policy)\n\t        }\n\t        attributes_dict = {\n\t            k: self._coerce_string(v)\n\t            for k, v in _metadata_input_dict.items()\n\t            if not is_metadata_field_indexed(k, self.metadata_indexing_policy)\n\t        }\n\t        #\n", "        if attributes_dict != {}:\n\t            attributes_fields = {\n\t                \"attributes_blob\": self._serialize_md_dict(attributes_dict)\n\t            }\n\t        else:\n\t            attributes_fields = {}\n\t        #\n\t        new_metadata_fields = {\n\t            k: v\n\t            for k, v in self._split_metadata_fields(metadata_indexed_dict).items()\n", "            if v != {} and v != set()\n\t        }\n\t        #\n\t        new_args_dict = {\n\t            **{k: v for k, v in args_dict.items() if k != \"metadata\"},\n\t            **attributes_fields,\n\t            **new_metadata_fields,\n\t        }\n\t        return super()._normalize_kwargs(new_args_dict)\n\t    def _extract_where_clause_blocks(\n", "        self, args_dict: Any\n\t    ) -> Tuple[Any, List[str], Tuple[Any, ...]]:\n\t        # This always happens after a corresponding _normalize_kwargs,\n\t        # so the metadata, if present, appears as split-fields.\n\t        assert \"metadata\" not in args_dict\n\t        if \"attributes_blob\" in args_dict:\n\t            raise ValueError(\"Non-indexed metadata fields cannot be used in queries.\")\n\t        md_keys = {\"metadata_s\"}\n\t        new_args_dict = {k: v for k, v in args_dict.items() if k not in md_keys}\n\t        # Here the \"metadata\" entry is made into specific where clauses\n", "        split_metadata = {k: v for k, v in args_dict.items() if k in md_keys}\n\t        these_wc_blocks: List[str] = []\n\t        these_wc_vals_list: List[Any] = []\n\t        # WHERE creation:\n\t        for k, v in sorted(split_metadata.get(\"metadata_s\", {}).items()):\n\t            these_wc_blocks.append(f\"metadata_s['{k}'] = %s\")\n\t            these_wc_vals_list.append(v)\n\t        # no new kwargs keys are created, all goes to WHERE\n\t        this_args_dict: Dict[str, Any] = {}\n\t        these_wc_vals = tuple(these_wc_vals_list)\n", "        # ready to defer to superclass(es), then collate-and-return\n\t        (s_args_dict, s_wc_blocks, s_wc_vals) = super()._extract_where_clause_blocks(\n\t            new_args_dict\n\t        )\n\t        return (\n\t            {**this_args_dict, **s_args_dict},\n\t            these_wc_blocks + s_wc_blocks,\n\t            tuple(list(these_wc_vals) + list(s_wc_vals)),\n\t        )\n\t    def find_entries(self, n: int, **kwargs: Any) -> Iterable[RowType]:\n", "        columns_desc, where_clause, get_cql_vals = self._parse_select_core_params(\n\t            **kwargs\n\t        )\n\t        limit_clause = \"LIMIT %s\"\n\t        limit_cql_vals = [n]\n\t        select_vals = tuple(list(get_cql_vals) + limit_cql_vals)\n\t        #\n\t        select_cql = SELECT_CQL_TEMPLATE.format(\n\t            columns_desc=columns_desc,\n\t            where_clause=where_clause,\n", "            limit_clause=limit_clause,\n\t        )\n\t        result_set = self.execute_cql(\n\t            select_cql, args=select_vals, op_type=CQLOpType.READ\n\t        )\n\t        return (self._normalize_row(result) for result in result_set)\n\t    def find_entries_async(self, n: int, **kwargs: Any) -> ResponseFuture:\n\t        raise NotImplementedError(\"Asynchronous reads are not supported.\")\n\tclass VectorMixin(BaseTableMixin):\n\t    def __init__(self, *pargs: Any, vector_dimension: int, **kwargs: Any) -> None:\n", "        self.vector_dimension = vector_dimension\n\t        super().__init__(*pargs, **kwargs)\n\t    def _schema_da(self) -> List[ColumnSpecType]:\n\t        return super()._schema_da() + [\n\t            (\"vector\", f\"VECTOR<FLOAT,{self.vector_dimension}>\")\n\t        ]\n\t    def db_setup(self) -> None:\n\t        super().db_setup()\n\t        # index on the vector column:\n\t        index_name = \"idx_vector\"\n", "        index_column = \"vector\"\n\t        create_index_cql = CREATE_INDEX_CQL_TEMPLATE.format(\n\t            index_name=index_name,\n\t            index_column=index_column,\n\t        )\n\t        self.execute_cql(create_index_cql, op_type=CQLOpType.SCHEMA)\n\t        return\n\t    def ann_search(\n\t        self, vector: List[float], n: int, **kwargs: Any\n\t    ) -> Iterable[RowType]:\n", "        n_kwargs = self._normalize_kwargs(kwargs)\n\t        # TODO: work on a columns: Optional[List[str]] = None\n\t        # (but with nuanced handling of the column-magic we have here)\n\t        columns = None\n\t        if columns is None:\n\t            columns_desc = \"*\"\n\t        else:\n\t            # TODO: handle translations here?\n\t            # columns_desc = \", \".join(columns)\n\t            raise NotImplementedError(\"Column selection is not implemented.\")\n", "        #\n\t        if all(x == 0 for x in vector):\n\t            # TODO: lift/relax this constraint when non-cosine metrics are there.\n\t            raise ValueError(\"Cannot use identically-zero vectors in cos/ANN search.\")\n\t        #\n\t        vector_column = \"vector\"\n\t        vector_cql_vals = [vector]\n\t        #\n\t        (\n\t            rest_kwargs,\n", "            where_clause_blocks,\n\t            where_cql_vals,\n\t        ) = self._extract_where_clause_blocks(n_kwargs)\n\t        assert rest_kwargs == {}\n\t        if where_clause_blocks == []:\n\t            where_clause = \"\"\n\t        else:\n\t            where_clause = \"WHERE \" + \" AND \".join(where_clause_blocks)\n\t        #\n\t        limit_clause = \"LIMIT %s\"\n", "        limit_cql_vals = [n]\n\t        #\n\t        select_ann_cql = SELECT_ANN_CQL_TEMPLATE.format(\n\t            columns_desc=columns_desc,\n\t            vector_column=vector_column,\n\t            where_clause=where_clause,\n\t            limit_clause=limit_clause,\n\t        )\n\t        #\n\t        select_ann_cql_vals = tuple(\n", "            list(where_cql_vals) + vector_cql_vals + limit_cql_vals\n\t        )\n\t        result_set = self.execute_cql(\n\t            select_ann_cql, args=select_ann_cql_vals, op_type=CQLOpType.READ\n\t        )\n\t        return (self._normalize_row(result) for result in result_set)\n\t    def ann_search_async(\n\t        self, vector: List[float], n: int, **kwargs: Any\n\t    ) -> ResponseFuture:\n\t        raise NotImplementedError(\"Asynchronous reads are not supported.\")\n", "    def metric_ann_search(\n\t        self,\n\t        vector: List[float],\n\t        n: int,\n\t        metric: str,\n\t        metric_threshold: Optional[float] = None,\n\t        **kwargs: Any,\n\t    ) -> Iterable[RowWithDistanceType]:\n\t        rows = list(self.ann_search(vector, n, **kwargs))\n\t        if rows == []:\n", "            return []\n\t        else:\n\t            # sort, cut, validate and prepare for returning\n\t            # evaluate metric\n\t            distance_function, distance_reversed = distance_metrics[metric]\n\t            row_vectors = [row[\"vector\"] for row in rows]\n\t            # enrich with their metric score\n\t            rows_with_metric = list(\n\t                zip(\n\t                    distance_function(row_vectors, vector),\n", "                    rows,\n\t                )\n\t            )\n\t            # sort rows by metric score. First handle metric/threshold\n\t            if metric_threshold is not None:\n\t                if distance_reversed:\n\t                    def _thresholder(mtx, thr):\n\t                        return mtx >= thr\n\t                else:\n\t                    def _thresholder(mtx, thr):\n", "                        return mtx <= thr\n\t            else:\n\t                # no hits are discarded\n\t                def _thresholder(mtx, thr):\n\t                    return True\n\t            #\n\t            sorted_passing_rows = sorted(\n\t                (\n\t                    pair\n\t                    for pair in rows_with_metric\n", "                    if _thresholder(pair[0], metric_threshold)\n\t                ),\n\t                key=itemgetter(0),\n\t                reverse=distance_reversed,\n\t            )\n\t            # return a list of hits with their distance (as JSON)\n\t            enriched_hits = (\n\t                {\n\t                    **hit,\n\t                    **{\"distance\": distance},\n", "                }\n\t                for distance, hit in sorted_passing_rows\n\t            )\n\t            return enriched_hits\n\t    def metric_ann_search_async(\n\t        self, vector: List[float], n: int, **kwargs: Any\n\t    ) -> ResponseFuture:\n\t        raise NotImplementedError(\"Asynchronous reads are not supported.\")\n\tclass ElasticKeyMixin(BaseTableMixin):\n\t    def __init__(self, *pargs: Any, keys: List[str], **kwargs: Any) -> None:\n", "        if \"row_id_type\" in kwargs:\n\t            raise ValueError(\"'row_id_type' not allowed for elastic tables.\")\n\t        self.keys = keys\n\t        self.key_desc = self._serialize_key_list(self.keys)\n\t        row_id_type = [\"TEXT\", \"TEXT\"]\n\t        new_kwargs = {\n\t            **{\"row_id_type\": row_id_type},\n\t            **kwargs,\n\t        }\n\t        super().__init__(*pargs, **new_kwargs)\n", "    @staticmethod\n\t    def _serialize_key_list(key_vals: List[Any]) -> str:\n\t        return json.dumps(key_vals, separators=(\",\", \":\"), sort_keys=True)\n\t    @staticmethod\n\t    def _deserialize_key_list(keys_str: str) -> List[Any]:\n\t        return json.loads(keys_str)\n\t    def _normalize_row(self, raw_row: Any) -> Dict[str, Any]:\n\t        key_fields = {\"key_desc\", \"key_vals\"}\n\t        pre_normalized = super()._normalize_row(raw_row)\n\t        row_key = {k: v for k, v in pre_normalized.items() if k in key_fields}\n", "        row_rest = {k: v for k, v in pre_normalized.items() if k not in key_fields}\n\t        if row_key == {}:\n\t            key_dict = {}\n\t        else:\n\t            # unpack the keys\n\t            assert len(row_key) == 2\n\t            assert self._deserialize_key_list(row_key[\"key_desc\"]) == self.keys\n\t            key_dict = {\n\t                k: v\n\t                for k, v in zip(\n", "                    self.keys,\n\t                    self._deserialize_key_list(row_key[\"key_vals\"]),\n\t                )\n\t            }\n\t        return {\n\t            **key_dict,\n\t            **row_rest,\n\t        }\n\t    def _normalize_kwargs(self, args_dict: Dict[str, Any]) -> Dict[str, Any]:\n\t        # transform provided \"keys\" into the elastic-representation two-val form\n", "        key_args = {k: v for k, v in args_dict.items() if k in self.keys}\n\t        # the \"key\" is passed all-or-nothing:\n\t        assert set(key_args.keys()) == set(self.keys) or key_args == {}\n\t        if key_args != {}:\n\t            key_vals = self._serialize_key_list(\n\t                [key_args[key_col] for key_col in self.keys]\n\t            )\n\t            #\n\t            key_args_dict = {\n\t                \"key_vals\": key_vals,\n", "                \"key_desc\": self.key_desc,\n\t            }\n\t            other_args_dict = {k: v for k, v in args_dict.items() if k not in self.keys}\n\t            new_args_dict = {\n\t                **key_args_dict,\n\t                **other_args_dict,\n\t            }\n\t        else:\n\t            new_args_dict = args_dict\n\t        return super()._normalize_kwargs(new_args_dict)\n", "    @staticmethod\n\t    def _schema_row_id() -> List[ColumnSpecType]:\n\t        return [\n\t            (\"key_desc\", \"TEXT\"),\n\t            (\"key_vals\", \"TEXT\"),\n\t        ]\n\tclass TypeNormalizerMixin(BaseTableMixin):\n\t    clustered: bool = False\n\t    elastic: bool = False\n\t    def __init__(self, *pargs: Any, **kwargs: Any) -> None:\n", "        if \"primary_key_type\" in kwargs:\n\t            pk_arg = kwargs[\"primary_key_type\"]\n\t            num_elastic_keys = len(kwargs[\"keys\"]) if self.elastic else None\n\t            col_type_map = rearrange_pk_type(pk_arg, self.clustered, num_elastic_keys)\n\t            new_kwargs = {\n\t                **col_type_map,\n\t                **{k: v for k, v in kwargs.items() if k != \"primary_key_type\"},\n\t            }\n\t        else:\n\t            new_kwargs = kwargs\n", "        super().__init__(*pargs, **new_kwargs)\n"]}
{"filename": "src/cassio/table/__init__.py", "chunked_list": ["from cassio.table.tables import PlainCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ClusteredCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ClusteredMetadataCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import MetadataCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import VectorCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ClusteredVectorCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ClusteredMetadataVectorCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import MetadataVectorCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ElasticCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ClusteredElasticCassandraTable  # noqa: F401\n", "from cassio.table.tables import ClusteredElasticMetadataCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ElasticMetadataCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ElasticVectorCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import ClusteredElasticVectorCassandraTable  # noqa: F401\n\tfrom cassio.table.tables import (\n\t    ClusteredElasticMetadataVectorCassandraTable,  # noqa: F401\n\t)\n\tfrom cassio.table.tables import ElasticMetadataVectorCassandraTable  # noqa: F401\n"]}
{"filename": "src/cassio/table/tables.py", "chunked_list": ["from cassio.table.base_table import BaseTable\n\tfrom cassio.table.mixins import (\n\t    ClusteredMixin,\n\t    MetadataMixin,\n\t    VectorMixin,\n\t    ElasticKeyMixin,\n\t    #\n\t    TypeNormalizerMixin,\n\t)\n\tclass PlainCassandraTable(TypeNormalizerMixin, BaseTable):\n", "    pass\n\tclass ClusteredCassandraTable(TypeNormalizerMixin, ClusteredMixin, BaseTable):\n\t    clustered = True\n\t    pass\n\tclass ClusteredMetadataCassandraTable(\n\t    TypeNormalizerMixin, MetadataMixin, ClusteredMixin, BaseTable\n\t):\n\t    clustered = True\n\t    pass\n\tclass MetadataCassandraTable(TypeNormalizerMixin, MetadataMixin, BaseTable):\n", "    pass\n\tclass VectorCassandraTable(TypeNormalizerMixin, VectorMixin, BaseTable):\n\t    pass\n\tclass ClusteredVectorCassandraTable(\n\t    TypeNormalizerMixin, VectorMixin, ClusteredMixin, BaseTable\n\t):\n\t    clustered = True\n\t    pass\n\tclass ClusteredMetadataVectorCassandraTable(\n\t    TypeNormalizerMixin, MetadataMixin, ClusteredMixin, VectorMixin, BaseTable\n", "):\n\t    clustered = True\n\t    pass\n\tclass MetadataVectorCassandraTable(\n\t    TypeNormalizerMixin, MetadataMixin, VectorMixin, BaseTable\n\t):\n\t    pass\n\tclass ElasticCassandraTable(TypeNormalizerMixin, ElasticKeyMixin, BaseTable):\n\t    elastic = True\n\t    pass\n", "class ClusteredElasticCassandraTable(\n\t    TypeNormalizerMixin, ClusteredMixin, ElasticKeyMixin, BaseTable\n\t):\n\t    clustered = True\n\t    elastic = True\n\t    pass\n\tclass ClusteredElasticMetadataCassandraTable(\n\t    TypeNormalizerMixin, MetadataMixin, ElasticKeyMixin, ClusteredMixin, BaseTable\n\t):\n\t    clustered = True\n", "    elastic = True\n\t    pass\n\tclass ElasticMetadataCassandraTable(\n\t    TypeNormalizerMixin, MetadataMixin, ElasticKeyMixin, BaseTable\n\t):\n\t    elastic = True\n\t    pass\n\tclass ElasticVectorCassandraTable(\n\t    TypeNormalizerMixin, VectorMixin, ElasticKeyMixin, BaseTable\n\t):\n", "    elastic = True\n\t    pass\n\tclass ClusteredElasticVectorCassandraTable(\n\t    TypeNormalizerMixin, ClusteredMixin, ElasticKeyMixin, VectorMixin, BaseTable\n\t):\n\t    clustered = True\n\t    elastic = True\n\t    pass\n\tclass ClusteredElasticMetadataVectorCassandraTable(\n\t    TypeNormalizerMixin,\n", "    MetadataMixin,\n\t    ElasticKeyMixin,\n\t    ClusteredMixin,\n\t    VectorMixin,\n\t    BaseTable,\n\t):\n\t    clustered = True\n\t    elastic = True\n\t    pass\n\tclass ElasticMetadataVectorCassandraTable(\n", "    MetadataMixin, ElasticKeyMixin, VectorMixin, BaseTable\n\t):\n\t    elastic = True\n\t    pass\n"]}
{"filename": "src/cassio/table/cql.py", "chunked_list": ["from typing import Union\n\tfrom enum import Enum\n\tfrom cassandra.query import SimpleStatement, PreparedStatement  # type: ignore\n\tclass CQLOpType(Enum):\n\t    SCHEMA = 1\n\t    WRITE = 2\n\t    READ = 3\n\tCREATE_TABLE_CQL_TEMPLATE = \"\"\"CREATE TABLE IF NOT EXISTS {{table_fqname}} ({columns_spec} PRIMARY KEY {primkey_spec}) {clustering_spec};\"\"\"  # noqa: E501\n\tTRUNCATE_TABLE_CQL_TEMPLATE = \"\"\"TRUNCATE TABLE {{table_fqname}};\"\"\"\n\tDELETE_CQL_TEMPLATE = \"\"\"DELETE FROM {{table_fqname}} {where_clause};\"\"\"\n", "SELECT_CQL_TEMPLATE = (\n\t    \"\"\"SELECT {columns_desc} FROM {{table_fqname}} {where_clause} {limit_clause};\"\"\"\n\t)\n\tINSERT_ROW_CQL_TEMPLATE = \"\"\"INSERT INTO {{table_fqname}} ({columns_desc}) VALUES ({value_placeholders}) {ttl_spec} ;\"\"\"  # noqa: E501\n\tCREATE_INDEX_CQL_TEMPLATE = \"\"\"CREATE CUSTOM INDEX IF NOT EXISTS {index_name}_{{table_name}} ON {{table_fqname}} ({index_column}) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\"\"\"  # noqa: E501\n\tCREATE_KEYS_INDEX_CQL_TEMPLATE = \"\"\"CREATE CUSTOM INDEX IF NOT EXISTS {index_name}_{{table_name}} ON {{table_fqname}} (KEYS({index_column})) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\"\"\"  # noqa: E501\n\tCREATE_ENTRIES_INDEX_CQL_TEMPLATE = \"\"\"CREATE CUSTOM INDEX IF NOT EXISTS {index_name}_{{table_name}} ON {{table_fqname}} (ENTRIES({index_column})) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';\"\"\"  # noqa: E501\n\tSELECT_ANN_CQL_TEMPLATE = \"\"\"SELECT {columns_desc} FROM {{table_fqname}} {where_clause} ORDER BY {vector_column} ANN OF %s {limit_clause};\"\"\"  # noqa: E501\n\tCQLStatementType = Union[str, SimpleStatement, PreparedStatement]\n\t# Mock DB session\n", "class MockDBSession:\n\t    def __init__(self, verbose=False):\n\t        self.verbose = verbose\n\t        self.statements = []\n\t    @staticmethod\n\t    def getStatementBody(statement: CQLStatementType) -> str:\n\t        if isinstance(statement, str):\n\t            _statement = statement\n\t        elif isinstance(statement, SimpleStatement):\n\t            _statement = statement.query_string\n", "        elif isinstance(statement, PreparedStatement):\n\t            _statement = statement.query_string\n\t        else:\n\t            raise ValueError()\n\t        return _statement\n\t    @staticmethod\n\t    def normalizeCQLStatement(statement: CQLStatementType) -> str:\n\t        _statement = MockDBSession.getStatementBody(statement)\n\t        _s = (\n\t            _statement.replace(\";\", \" \")\n", "            .replace(\"%s\", \" %s \")\n\t            .replace(\"?\", \" ? \")\n\t            .replace(\"=\", \" = \")\n\t            .replace(\")\", \" ) \")\n\t            .replace(\"(\", \" ( \")\n\t            .replace(\"\\n\", \" \")\n\t        )\n\t        return \" \".join(\n\t            tok.lower() for tok in (_t.strip() for _t in _s.split(\" \") if _t.strip())\n\t        )\n", "    @staticmethod\n\t    def prepare(statement):\n\t        # A very unusable 'prepared statement' just for tracing/debugging:\n\t        return PreparedStatement(None, 0, 0, statement, \"keyspace\", None, None, None)\n\t    def execute(self, statement, arguments=tuple()):\n\t        if self.verbose:\n\t            #\n\t            st_body = self.getStatementBody(statement)\n\t            if isinstance(statement, str):\n\t                st_type = \"STR\"\n", "                placeholder_count = st_body.count(\"%s\")\n\t                assert \"?\" not in st_body\n\t            elif isinstance(statement, SimpleStatement):\n\t                st_type = \"SIM\"\n\t                placeholder_count = st_body.count(\"%s\")\n\t                assert \"?\" not in st_body\n\t            elif isinstance(statement, PreparedStatement):\n\t                st_type = \"PRE\"\n\t                placeholder_count = st_body.count(\"?\")\n\t                assert \"%s\" not in st_body\n", "            #\n\t            assert placeholder_count == len(arguments)\n\t            #\n\t            print(f\"CQL_EXECUTE [{st_type}]:\")\n\t            print(f\"    {st_body}\")\n\t            if arguments:\n\t                print(f\"    {str(arguments)}\")\n\t        self.statements.append((statement, arguments))\n\t        return []\n\t    def last_raw(self, n):\n", "        if n <= 0:\n\t            return []\n\t        else:\n\t            return self.statements[-n:]\n\t    def last(self, n):\n\t        return [\n\t            (\n\t                self.normalizeCQLStatement(stmt),\n\t                data,\n\t            )\n", "            for stmt, data in self.last_raw(n)\n\t        ]\n\t    def assert_last_equal(self, expected_statements):\n\t        # used for testing\n\t        last_executed = self.last(len(expected_statements))\n\t        assert len(last_executed) == len(expected_statements)\n\t        for s_exe, s_expe in zip(last_executed, expected_statements):\n\t            assert s_exe[1] == s_expe[1], f\"EXE#{str(s_exe[1])}# != EXPE#{s_expe[1]}#\"\n\t            exe_cql = self.normalizeCQLStatement(s_exe[0])\n\t            expe_cql = self.normalizeCQLStatement(s_expe[0])\n", "            assert exe_cql == expe_cql, f\"EXE#{exe_cql}# != EXPE#{expe_cql}#\"\n"]}
{"filename": "src/cassio/table/table_types.py", "chunked_list": ["from enum import Enum\n\tfrom typing import Any, Dict, List, Optional, Set, Tuple, Union\n\tColumnSpecType = Tuple[str, str]\n\tRowType = Dict[str, Any]\n\tRowWithDistanceType = Dict[str, Any]\n\tSessionType = Any\n\tclass MetadataIndexingMode(Enum):\n\t    DEFAULT_TO_UNSEARCHABLE = 1\n\t    DEFAULT_TO_SEARCHABLE = 2\n\tMetadataIndexingPolicy = Tuple[MetadataIndexingMode, Set[str]]\n", "def is_metadata_field_indexed(field_name: str, policy: MetadataIndexingPolicy) -> bool:\n\t    p_mode, p_fields = policy\n\t    if p_mode == MetadataIndexingMode.DEFAULT_TO_UNSEARCHABLE:\n\t        return field_name in p_fields\n\t    elif p_mode == MetadataIndexingMode.DEFAULT_TO_SEARCHABLE:\n\t        return field_name not in p_fields\n\t    else:\n\t        raise ValueError(f\"Unexpected metadata indexing mode {p_mode}\")\n\tdef normalize_type_desc(type_desc: Union[str, List[str]]) -> List[str]:\n\t    if isinstance(type_desc, str):\n", "        return [type_desc]\n\t    else:\n\t        return type_desc\n\tdef rearrange_pk_type(\n\t    pk_type: Union[str, List[str]],\n\t    clustered: bool = False,\n\t    num_elastic_keys: Optional[int] = None,\n\t) -> Dict[str, List[str]]:\n\t    \"\"\"A compatibility layer with the 'primary_key_type' specifier on init.\"\"\"\n\t    _pk_type = normalize_type_desc(pk_type)\n", "    if clustered:\n\t        pk_type, rest_type = _pk_type[0:1], _pk_type[1:]\n\t        if num_elastic_keys:\n\t            assert len(rest_type) == num_elastic_keys\n\t            return {\n\t                \"partition_id_type\": pk_type,\n\t            }\n\t        else:\n\t            return {\n\t                \"row_id_type\": rest_type,\n", "                \"partition_id_type\": pk_type,\n\t            }\n\t    else:\n\t        if num_elastic_keys:\n\t            assert len(_pk_type) == num_elastic_keys\n\t            return {}\n\t        else:\n\t            return {\n\t                \"row_id_type\": _pk_type,\n\t            }\n"]}
{"filename": "src/cassio/table/base_table.py", "chunked_list": ["from typing import Any, List, Dict, Iterable, Optional, Set, Tuple, Union\n\tfrom cassandra.query import SimpleStatement, PreparedStatement  # type: ignore\n\tfrom cassandra.cluster import ResultSet  # type: ignore\n\tfrom cassandra.cluster import ResponseFuture  # type: ignore\n\tfrom cassio.table.table_types import (\n\t    ColumnSpecType,\n\t    RowType,\n\t    SessionType,\n\t    normalize_type_desc,\n\t)\n", "from cassio.table.cql import (\n\t    CQLOpType,\n\t    CREATE_TABLE_CQL_TEMPLATE,\n\t    TRUNCATE_TABLE_CQL_TEMPLATE,\n\t    DELETE_CQL_TEMPLATE,\n\t    SELECT_CQL_TEMPLATE,\n\t    INSERT_ROW_CQL_TEMPLATE,\n\t)\n\tclass BaseTable:\n\t    ordering_in_partition: Optional[str] = None\n", "    def __init__(\n\t        self,\n\t        session: SessionType,\n\t        keyspace: str,\n\t        table: str,\n\t        ttl_seconds: Optional[int] = None,\n\t        row_id_type: Union[str, List[str]] = [\"TEXT\"],\n\t        skip_provisioning=False,\n\t    ) -> None:\n\t        self.session = session\n", "        self.keyspace = keyspace\n\t        self.table = table\n\t        self.ttl_seconds = ttl_seconds\n\t        self.row_id_type = normalize_type_desc(row_id_type)\n\t        self.skip_provisioning = skip_provisioning\n\t        self._prepared_statements: Dict[str, PreparedStatement] = {}\n\t        self.db_setup()\n\t    def _schema_row_id(self) -> List[ColumnSpecType]:\n\t        assert len(self.row_id_type) == 1\n\t        return [\n", "            (\"row_id\", self.row_id_type[0]),\n\t        ]\n\t    def _schema_pk(self) -> List[ColumnSpecType]:\n\t        return self._schema_row_id()\n\t    def _schema_cc(self) -> List[ColumnSpecType]:\n\t        return []\n\t    def _schema_da(self) -> List[ColumnSpecType]:\n\t        return [\n\t            (\"body_blob\", \"TEXT\"),\n\t        ]\n", "    def _schema(self) -> Dict[str, List[ColumnSpecType]]:\n\t        return {\n\t            \"pk\": self._schema_pk(),\n\t            \"cc\": self._schema_cc(),\n\t            \"da\": self._schema_da(),\n\t        }\n\t    def _schema_primary_key(self) -> List[ColumnSpecType]:\n\t        return self._schema_pk() + self._schema_cc()\n\t    def _schema_collist(self) -> List[ColumnSpecType]:\n\t        full_list = self._schema_da() + self._schema_cc() + self._schema_pk()\n", "        return full_list\n\t    def _schema_colnameset(self) -> Set[str]:\n\t        full_list = self._schema_collist()\n\t        full_set = set(col for col, _ in full_list)\n\t        assert len(full_list) == len(full_set)\n\t        return full_set\n\t    def _desc_table(self) -> str:\n\t        columns = self._schema()\n\t        col_str = (\n\t            \"[(\"\n", "            + \", \".join(\"%s(%s)\" % colspec for colspec in columns[\"pk\"])\n\t            + \") \"\n\t            + \", \".join(\"%s(%s)\" % colspec for colspec in columns[\"cc\"])\n\t            + \"] \"\n\t            + \", \".join(\"%s(%s)\" % colspec for colspec in columns[\"da\"])\n\t        )\n\t        return col_str\n\t    def _extract_where_clause_blocks(\n\t        self, args_dict: Any\n\t    ) -> Tuple[Any, List[str], Tuple[Any, ...]]:\n", "        # Removes some of the passed kwargs and returns the remaining,\n\t        # plus the pieces for a WHERE\n\t        _allowed_colspecs = self._schema_collist()\n\t        passed_columns = sorted(\n\t            [col for col, _ in _allowed_colspecs if col in args_dict]\n\t        )\n\t        residual_args = {k: v for k, v in args_dict.items() if k not in passed_columns}\n\t        where_clause_blocks = [f\"{col} = %s\" for col in passed_columns]\n\t        where_clause_vals = tuple([args_dict[col] for col in passed_columns])\n\t        return (\n", "            residual_args,\n\t            where_clause_blocks,\n\t            where_clause_vals,\n\t        )\n\t    def _normalize_kwargs(self, args_dict: Dict[str, Any]) -> Dict[str, Any]:\n\t        return args_dict\n\t    def _normalize_row(self, raw_row: Any) -> Dict[str, Any]:\n\t        if isinstance(raw_row, dict):\n\t            dict_row = raw_row\n\t        else:\n", "            dict_row = raw_row._asdict()\n\t        #\n\t        return dict_row\n\t    def _delete(self, is_async: bool, **kwargs: Any) -> Union[None, ResponseFuture]:\n\t        n_kwargs = self._normalize_kwargs(kwargs)\n\t        (\n\t            rest_kwargs,\n\t            where_clause_blocks,\n\t            delete_cql_vals,\n\t        ) = self._extract_where_clause_blocks(n_kwargs)\n", "        assert rest_kwargs == {}\n\t        where_clause = \"WHERE \" + \" AND \".join(where_clause_blocks)\n\t        delete_cql = DELETE_CQL_TEMPLATE.format(\n\t            where_clause=where_clause,\n\t        )\n\t        if is_async:\n\t            return self.execute_cql_async(\n\t                delete_cql, args=delete_cql_vals, op_type=CQLOpType.WRITE\n\t            )\n\t        else:\n", "            self.execute_cql(delete_cql, args=delete_cql_vals, op_type=CQLOpType.WRITE)\n\t            return None\n\t    def delete(self, **kwargs: Any) -> None:\n\t        self._delete(is_async=False, **kwargs)\n\t        return None\n\t    def delete_async(self, **kwargs: Any) -> ResponseFuture:\n\t        return self._delete(is_async=True, **kwargs)\n\t    def _clear(self, is_async: bool) -> Union[None, ResponseFuture]:\n\t        truncate_table_cql = TRUNCATE_TABLE_CQL_TEMPLATE.format()\n\t        if is_async:\n", "            return self.execute_cql_async(\n\t                truncate_table_cql, args=tuple(), op_type=CQLOpType.WRITE\n\t            )\n\t        else:\n\t            self.execute_cql(truncate_table_cql, args=tuple(), op_type=CQLOpType.WRITE)\n\t            return None\n\t    def clear(self) -> None:\n\t        self._clear(is_async=False)\n\t        return None\n\t    def clear_async(self) -> ResponseFuture:\n", "        return self._clear(is_async=True)\n\t    def _parse_select_core_params(\n\t        self, **kwargs: Any\n\t    ) -> Tuple[str, str, Tuple[Any, ...]]:\n\t        n_kwargs = self._normalize_kwargs(kwargs)\n\t        # TODO: work on a columns: Optional[List[str]] = None\n\t        # (but with nuanced handling of the column-magic we have here)\n\t        columns = None\n\t        if columns is None:\n\t            columns_desc = \"*\"\n", "        else:\n\t            # TODO: handle translations here?\n\t            # columns_desc = \", \".join(columns)\n\t            raise NotImplementedError(\"Column selection is not implemented.\")\n\t        #\n\t        (\n\t            rest_kwargs,\n\t            where_clause_blocks,\n\t            select_cql_vals,\n\t        ) = self._extract_where_clause_blocks(n_kwargs)\n", "        assert rest_kwargs == {}\n\t        where_clause = \"WHERE \" + \" AND \".join(where_clause_blocks)\n\t        return columns_desc, where_clause, select_cql_vals\n\t    def get(self, **kwargs: Any) -> Optional[RowType]:\n\t        columns_desc, where_clause, get_cql_vals = self._parse_select_core_params(\n\t            **kwargs\n\t        )\n\t        limit_clause = \"\"\n\t        limit_cql_vals: List[Any] = []\n\t        select_vals = tuple(list(get_cql_vals) + limit_cql_vals)\n", "        #\n\t        select_cql = SELECT_CQL_TEMPLATE.format(\n\t            columns_desc=columns_desc,\n\t            where_clause=where_clause,\n\t            limit_clause=limit_clause,\n\t        )\n\t        # dancing around the result set (to comply with type checking):\n\t        result_set = self.execute_cql(\n\t            select_cql, args=select_vals, op_type=CQLOpType.READ\n\t        )\n", "        if isinstance(result_set, ResultSet):\n\t            result = result_set.one()\n\t        else:\n\t            result = None\n\t        #\n\t        if result is None:\n\t            return result\n\t        else:\n\t            return self._normalize_row(result)\n\t    def get_async(self, **kwargs) -> ResponseFuture:\n", "        raise NotImplementedError(\"Asynchronous reads are not supported.\")\n\t    def _put(self, is_async: bool, **kwargs: Any) -> Union[None, ResponseFuture]:\n\t        n_kwargs = self._normalize_kwargs(kwargs)\n\t        primary_key = self._schema_primary_key()\n\t        assert set(col for col, _ in primary_key) - set(n_kwargs.keys()) == set()\n\t        columns = [col for col, _ in self._schema_collist() if col in n_kwargs]\n\t        columns_desc = \", \".join(columns)\n\t        insert_cql_vals = [n_kwargs[col] for col in columns]\n\t        value_placeholders = \", \".join(\"%s\" for _ in columns)\n\t        #\n", "        ttl_seconds = (\n\t            n_kwargs[\"ttl_seconds\"] if \"ttl_seconds\" in n_kwargs else self.ttl_seconds\n\t        )\n\t        if ttl_seconds is not None:\n\t            ttl_spec = \"USING TTL %s\"\n\t            ttl_vals = [ttl_seconds]\n\t        else:\n\t            ttl_spec = \"\"\n\t            ttl_vals = []\n\t        #\n", "        insert_cql_args = tuple(insert_cql_vals + ttl_vals)\n\t        insert_cql = INSERT_ROW_CQL_TEMPLATE.format(\n\t            columns_desc=columns_desc,\n\t            value_placeholders=value_placeholders,\n\t            ttl_spec=ttl_spec,\n\t        )\n\t        #\n\t        if is_async:\n\t            return self.execute_cql_async(\n\t                insert_cql, args=insert_cql_args, op_type=CQLOpType.WRITE\n", "            )\n\t        else:\n\t            self.execute_cql(insert_cql, args=insert_cql_args, op_type=CQLOpType.WRITE)\n\t            return None\n\t    def put(self, **kwargs: Any) -> None:\n\t        self._put(is_async=False, **kwargs)\n\t        return None\n\t    def put_async(self, **kwargs: Any) -> ResponseFuture:\n\t        return self._put(is_async=True, **kwargs)\n\t    def db_setup(self) -> None:\n", "        _schema = self._schema()\n\t        column_specs = [\n\t            f\"{col_spec[0]} {col_spec[1]}\"\n\t            for _schema_grp in [\"pk\", \"cc\", \"da\"]\n\t            for col_spec in _schema[_schema_grp]\n\t        ]\n\t        pk_spec = \", \".join(col for col, _ in _schema[\"pk\"])\n\t        cc_spec = \", \".join(col for col, _ in _schema[\"cc\"])\n\t        primkey_spec = f\"( ( {pk_spec} ) {',' if _schema['cc'] else ''} {cc_spec} )\"\n\t        if _schema[\"cc\"]:\n", "            clu_core = \", \".join(\n\t                f\"{col} {self.ordering_in_partition}\" for col, _ in _schema[\"cc\"]\n\t            )\n\t            clustering_spec = f\"WITH CLUSTERING ORDER BY ({clu_core})\"\n\t        else:\n\t            clustering_spec = \"\"\n\t        #\n\t        create_table_cql = CREATE_TABLE_CQL_TEMPLATE.format(\n\t            columns_spec=\" \".join(f\"  {cs},\" for cs in column_specs),\n\t            primkey_spec=primkey_spec,\n", "            clustering_spec=clustering_spec,\n\t        )\n\t        self.execute_cql(create_table_cql, op_type=CQLOpType.SCHEMA)\n\t    def _finalize_cql_semitemplate(self, cql_semitemplate: str) -> str:\n\t        table_fqname = f\"{self.keyspace}.{self.table}\"\n\t        table_name = self.table\n\t        final_cql = cql_semitemplate.format(\n\t            table_fqname=table_fqname, table_name=table_name\n\t        )\n\t        return final_cql\n", "    def _obtain_prepared_statement(self, final_cql) -> PreparedStatement:\n\t        # TODO: improve this placeholder handling\n\t        _preparable_cql = final_cql.replace(\"%s\", \"?\")\n\t        # handle the cache of prepared statements\n\t        if _preparable_cql not in self._prepared_statements:\n\t            self._prepared_statements[_preparable_cql] = self.session.prepare(\n\t                _preparable_cql\n\t            )\n\t        return self._prepared_statements[_preparable_cql]\n\t    def execute_cql(\n", "        self,\n\t        cql_semitemplate: str,\n\t        op_type: CQLOpType,\n\t        args: Tuple[Any, ...] = tuple(),\n\t    ) -> Iterable[RowType]:\n\t        final_cql = self._finalize_cql_semitemplate(cql_semitemplate)\n\t        #\n\t        if op_type == CQLOpType.SCHEMA and self.skip_provisioning:\n\t            # these operations are not executed for this instance:\n\t            return []\n", "        else:\n\t            if op_type == CQLOpType.SCHEMA:\n\t                # schema operations are not to be 'prepared'\n\t                statement = SimpleStatement(final_cql)\n\t            else:\n\t                statement = self._obtain_prepared_statement(final_cql)\n\t            #\n\t            return self.session.execute(statement, args)\n\t    def execute_cql_async(\n\t        self,\n", "        cql_semitemplate: str,\n\t        op_type: CQLOpType,\n\t        args: Tuple[Any, ...] = tuple(),\n\t    ) -> ResponseFuture:\n\t        final_cql = self._finalize_cql_semitemplate(cql_semitemplate)\n\t        #\n\t        if op_type == CQLOpType.SCHEMA:\n\t            raise RuntimeError(\"Schema operations cannot be asynchronous\")\n\t        else:\n\t            statement = self._obtain_prepared_statement(final_cql)\n", "            #\n\t            return self.session.execute_async(statement, args)\n"]}
{"filename": "src/cassio/utils/__init__.py", "chunked_list": ["from cassio.utils.vector import distance_metrics  # noqa: F401\n"]}
{"filename": "src/cassio/utils/vector/distance_metrics.py", "chunked_list": ["from typing import List, Dict, Tuple, Callable\n\timport numpy as np\n\t# distance definitions. These all work batched in the first argument.\n\tdef distance_dot_product(\n\t    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n\t) -> List[float]:\n\t    \"\"\"\n\t    Given a list [emb_i] and a reference rEmb vector,\n\t    return a list [distance_i] where each distance is\n\t        distance_i = distance(emb_i, rEmb)\n", "    At the moment only the dot product is supported\n\t    (which for unitary vectors is the cosine difference).\n\t    Not particularly optimized.\n\t    \"\"\"\n\t    v1s = np.array(embedding_vectors, dtype=float)\n\t    v2 = np.array(reference_embedding_vector, dtype=float)\n\t    return list(\n\t        np.dot(\n\t            v1s,\n\t            v2.T,\n", "        )\n\t    )\n\tdef distance_cos_difference(\n\t    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n\t) -> List[float]:\n\t    v1s = np.array(embedding_vectors, dtype=float)\n\t    v2 = np.array(reference_embedding_vector, dtype=float)\n\t    return list(\n\t        np.dot(\n\t            v1s,\n", "            v2.T,\n\t        )\n\t        / (np.linalg.norm(v1s, axis=1) * np.linalg.norm(v2))\n\t    )\n\tdef distance_l1(\n\t    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n\t) -> List[float]:\n\t    v1s = np.array(embedding_vectors, dtype=float)\n\t    v2 = np.array(reference_embedding_vector, dtype=float)\n\t    return list(np.linalg.norm(v1s - v2, axis=1, ord=1))\n", "def distance_l2(\n\t    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n\t) -> List[float]:\n\t    v1s = np.array(embedding_vectors, dtype=float)\n\t    v2 = np.array(reference_embedding_vector, dtype=float)\n\t    return list(np.linalg.norm(v1s - v2, axis=1, ord=2))\n\tdef distance_max(\n\t    embedding_vectors: List[List[float]], reference_embedding_vector: List[float]\n\t) -> List[float]:\n\t    v1s = np.array(embedding_vectors, dtype=float)\n", "    v2 = np.array(reference_embedding_vector, dtype=float)\n\t    return list(np.linalg.norm(v1s - v2, axis=1, ord=np.inf))\n\t# The tuple is:\n\t#   (\n\t#       function,\n\t#       sorting 'reverse' argument, nearest-to-farthest\n\t#   )\n\t# (i.e. True means that:\n\t#     - in that metric higher is closer and that\n\t#     - cutoff should be metric > threshold)\n", "distance_metrics: Dict[str, Tuple[Callable, bool]] = {\n\t    \"cos\": (\n\t        distance_cos_difference,\n\t        True,\n\t    ),\n\t    \"dot\": (\n\t        distance_dot_product,\n\t        True,\n\t    ),\n\t    \"l1\": (\n", "        distance_l1,\n\t        False,\n\t    ),\n\t    \"l2\": (\n\t        distance_l2,\n\t        False,\n\t    ),\n\t    \"max\": (\n\t        distance_max,\n\t        False,\n", "    ),\n\t}\n"]}
{"filename": "src/cassio/utils/vector/__init__.py", "chunked_list": ["from cassio.utils.vector.distance_metrics import distance_metrics  # noqa: F401\n"]}
{"filename": "src/cassio/keyvalue/k_v_cache.py", "chunked_list": ["\"\"\"\n\thandling of key-value storage on a Cassandra table.\n\tOne row per partition, serializes a multiple partition key into a string\n\t\"\"\"\n\tfrom warnings import warn\n\tfrom typing import Any, Dict, List, Optional\n\tfrom cassandra.cluster import Session  # type: ignore\n\tfrom cassio.table.tables import ElasticCassandraTable\n\tclass KVCache:\n\t    \"\"\"\n", "    This class is a rewriting of the KVCache created for use in LangChain\n\t    integration, this time relying on the class-table-hierarchy (cassio.table.*).\n\t    It mostly provides a translation layer between parameters and key names,\n\t    using a clustered table class internally.\n\t    Additional kwargs, for use in this new table class, are passed as they are\n\t    in order to enable their usage already before adapting the LangChain\n\t    integration code.\n\t    \"\"\"\n\t    DEPRECATION_MESSAGE = (\n\t        \"Class `KVCache` is a legacy construct and \"\n", "        \"will be deprecated in future versions of CassIO.\"\n\t    )\n\t    def __init__(self, session: Session, keyspace: str, table: str, keys: List[Any]):\n\t        #\n\t        warn(self.DEPRECATION_MESSAGE, DeprecationWarning, stacklevel=2)\n\t        # for LangChain this is what we expect - no other uses are planned:\n\t        assert all(isinstance(k, str) for k in keys)\n\t        p_k_type = [\"TEXT\"] * len(keys)\n\t        #\n\t        self.table = ElasticCassandraTable(\n", "            session,\n\t            keyspace,\n\t            table,\n\t            keys=keys,\n\t            primary_key_type=p_k_type,\n\t        )\n\t    def clear(self) -> None:\n\t        self.table.clear()\n\t        return None\n\t    def put(\n", "        self,\n\t        key_dict: Dict[str, str],\n\t        cache_value: str,\n\t        ttl_seconds: Optional[int] = None,\n\t    ) -> None:\n\t        self.table.put(body_blob=cache_value, ttl_seconds=ttl_seconds, **key_dict)\n\t        return None\n\t    def get(self, key_dict) -> Optional[str]:\n\t        entry = self.table.get(**key_dict)\n\t        if entry is None:\n", "            return None\n\t        else:\n\t            return entry[\"body_blob\"]\n\t    def delete(self, key_dict) -> None:\n\t        \"\"\"Will not complain if the row does not exist.\"\"\"\n\t        self.table.delete(**key_dict)\n\t        return None\n"]}
{"filename": "src/cassio/keyvalue/__init__.py", "chunked_list": ["from cassio.keyvalue.k_v_cache import KVCache  # noqa: F401\n"]}
{"filename": "src/cassio/vector/vector_table.py", "chunked_list": ["\"\"\"\n\tCompatibility layer for legacy VectorTable (used by LangChain integration\n\t(as of August 2023).\n\tNote: This is to be replaced by direct usage of the table-class-hierarchy classes.\n\t\"\"\"\n\tfrom warnings import warn\n\tfrom typing import List, Dict, Any, Optional\n\tfrom cassandra.cluster import ResponseFuture  # type: ignore\n\tfrom cassio.table.table_types import RowType\n\tfrom cassio.table.tables import (\n", "    MetadataVectorCassandraTable,\n\t)\n\tnew_columns_to_legacy = {\n\t    \"row_id\": \"document_id\",\n\t    \"body_blob\": \"document\",\n\t    \"vector\": \"embedding_vector\",\n\t}\n\tlegacy_columns_to_new = {v: k for k, v in new_columns_to_legacy.items()}\n\tclass VectorTable:\n\t    \"\"\"\n", "    This class is a rewriting of the VectorTable created for use in LangChain\n\t    integration, this time relying on the class-table-hierarchy (cassio.table.*).\n\t    It mostly provides a translation layer between parameters and key names,\n\t    using a metadata+vector table class internally.\n\t    Additional kwargs, for use in this new table class, are passed as they are\n\t    in order to enable their usage already before adapting the LangChain\n\t    integration code.\n\t    \"\"\"\n\t    DEPRECATION_MESSAGE = (\n\t        \"Class `VectorTable` is a legacy construct and \"\n", "        \"will be deprecated in future versions of CassIO.\"\n\t    )\n\t    def __init__(self, *pargs: Any, **kwargs: Dict[str, Any]):\n\t        #\n\t        warn(self.DEPRECATION_MESSAGE, DeprecationWarning, stacklevel=2)\n\t        #\n\t        if \"embedding_dimension\" in kwargs:\n\t            vector_dimension = kwargs[\"embedding_dimension\"]\n\t            new_kwargs = {\n\t                **{\n", "                    k: v\n\t                    for k, v in kwargs.items()\n\t                    if k != \"embedding_dimension\"\n\t                    # let's get rid of the infamous 'auto_id' here:\n\t                    if k != \"auto_id\"\n\t                },\n\t                **{\"vector_dimension\": vector_dimension},\n\t            }\n\t        else:\n\t            new_kwargs = kwargs\n", "        # this legacy VectorTable will have everything indexed for search:\n\t        md_kwargs = {\n\t            **{\"metadata_indexing\": \"all\"},\n\t            **new_kwargs,\n\t        }\n\t        #\n\t        self.table = MetadataVectorCassandraTable(*pargs, **md_kwargs)\n\t    def search(\n\t        self,\n\t        embedding_vector: List[float],\n", "        top_k: int,\n\t        metric: str = \"cos\",\n\t        metric_threshold: Optional[float] = None,\n\t        **kwargs: Any,\n\t    ) -> List[RowType]:\n\t        # get rows by ANN\n\t        enriched_hits = self.table.metric_ann_search(\n\t            vector=embedding_vector,\n\t            n=top_k,\n\t            metric=metric,\n", "            metric_threshold=metric_threshold,\n\t            **kwargs,\n\t        )\n\t        #\n\t        return [self._make_dict_legacy(rich_hit) for rich_hit in enriched_hits]\n\t    def put(\n\t        self,\n\t        document: str,\n\t        embedding_vector: List[float],\n\t        document_id: Any,\n", "        metadata: Dict[str, Any] = {},\n\t        ttl_seconds: Optional[int] = None,\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        self.table.put(\n\t            row_id=document_id,\n\t            body_blob=document,\n\t            vector=embedding_vector,\n\t            metadata=metadata or {},\n\t            ttl_seconds=ttl_seconds,\n", "            **kwargs,\n\t        )\n\t    def put_async(\n\t        self,\n\t        document: str,\n\t        embedding_vector: List[float],\n\t        document_id: Any,\n\t        metadata: Dict[str, Any],\n\t        ttl_seconds: int,\n\t        **kwargs: Any,\n", "    ) -> ResponseFuture:\n\t        return self.table.put_async(\n\t            row_id=document_id,\n\t            body_blob=document,\n\t            vector=embedding_vector,\n\t            metadata=metadata or {},\n\t            ttl_seconds=ttl_seconds,\n\t            **kwargs,\n\t        )\n\t    def get(self, document_id: Any, **kwargs: Any) -> Optional[RowType]:\n", "        row_or_none = self.table.get(row_id=document_id, **kwargs)\n\t        if row_or_none:\n\t            return self._make_dict_legacy(row_or_none)\n\t        else:\n\t            return row_or_none\n\t    def delete(self, document_id: Any, **kwargs: Any) -> None:\n\t        self.table.delete(row_id=document_id, **kwargs)\n\t        return None\n\t    def clear(self) -> None:\n\t        self.table.clear()\n", "        return None\n\t    @staticmethod\n\t    def _make_dict_legacy(new_dict: RowType) -> RowType:\n\t        return {new_columns_to_legacy.get(k, k): v for k, v in new_dict.items()}\n"]}
{"filename": "src/cassio/vector/__init__.py", "chunked_list": ["from cassio.vector.vector_table import VectorTable  # noqa: F401\n"]}
{"filename": "src/cassio/history/stored_blob_history.py", "chunked_list": ["\"\"\"\n\tmanagement of \"history\" of stored blobs, grouped\n\tby some 'session id'. Overwrites are not supported by design.\n\t\"\"\"\n\timport uuid\n\tfrom warnings import warn\n\tfrom typing import Any, Dict, Iterable, Optional\n\tfrom cassandra.cluster import Session  # type: ignore\n\tfrom cassio.table.tables import ClusteredCassandraTable\n\tclass StoredBlobHistory:\n", "    \"\"\"\n\t    This class is a rewriting of the StoredBlobHistory created for use in LangChain\n\t    integration, this time relying on the class-table-hierarchy (cassio.table.*).\n\t    It mostly provides a translation layer between parameters and key names,\n\t    using a clustered table class internally.\n\t    Additional kwargs, for use in this new table class, are passed as they are\n\t    in order to enable their usage already before adapting the LangChain\n\t    integration code.\n\t    \"\"\"\n\t    DEPRECATION_MESSAGE = (\n", "        \"Class `StoredBlobHistory` is a legacy construct and \"\n\t        \"will be deprecated in future versions of CassIO.\"\n\t    )\n\t    def __init__(\n\t        self, session: Session, keyspace: str, table_name: str, **kwargs: Dict[str, Any]\n\t    ):\n\t        #\n\t        warn(self.DEPRECATION_MESSAGE, DeprecationWarning, stacklevel=2)\n\t        # specifications are added such as the type of the row_id\n\t        full_kwargs = {\n", "            **kwargs,\n\t            **{\n\t                \"primary_key_type\": [\"TEXT\", \"TIMEUUID\"],\n\t                # latest entries are returned first\n\t                \"ordering_in_partition\": \"DESC\",\n\t            },\n\t        }\n\t        self.table = ClusteredCassandraTable(\n\t            session=session,\n\t            keyspace=keyspace,\n", "            table=table_name,\n\t            **full_kwargs,\n\t        )\n\t    def store(self, session_id: str, blob: str, ttl_seconds: Optional[int]):\n\t        this_row_id = uuid.uuid1()\n\t        self.table.put(\n\t            partition_id=session_id,\n\t            row_id=this_row_id,\n\t            body_blob=blob,\n\t            ttl_seconds=ttl_seconds,\n", "        )\n\t    def retrieve(\n\t        self, session_id: str, max_count: Optional[int] = None\n\t    ) -> Iterable[str]:\n\t        # The latest are returned, in chronological order\n\t        return [\n\t            row[\"body_blob\"]\n\t            for row in self.table.get_partition(\n\t                partition_id=session_id,\n\t                n=max_count,\n", "            )\n\t        ][::-1]\n\t    def clear_session_id(self, session_id: str) -> None:\n\t        self.table.delete_partition(session_id)\n\t        return None\n"]}
{"filename": "src/cassio/history/__init__.py", "chunked_list": ["from cassio.history.stored_blob_history import StoredBlobHistory  # noqa: F401\n"]}
