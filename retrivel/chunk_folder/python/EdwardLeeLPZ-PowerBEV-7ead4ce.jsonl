{"filename": "train.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport os\n\timport socket\n\timport time\n", "import pytorch_lightning as pl\n\timport torch\n\tfrom powerbev.config import get_cfg, get_parser\n\tfrom powerbev.data import prepare_powerbev_dataloaders\n\tfrom powerbev.trainer import TrainingModule\n\tfrom pytorch_lightning.callbacks import ModelCheckpoint\n\tfrom pytorch_lightning.plugins import DDPPlugin\n\tdef main():\n\t    args = get_parser().parse_args()\n\t    cfg = get_cfg(args)\n", "    trainloader, valloader = prepare_powerbev_dataloaders(cfg)\n\t    model = TrainingModule(cfg.convert_to_dict())\n\t    if cfg.PRETRAINED.LOAD_WEIGHTS:\n\t        # Load single-image instance segmentation model.\n\t        pretrained_model_weights = torch.load(\n\t            cfg.PRETRAINED.PATH , map_location='cpu'\n\t        )['state_dict']\n\t        model.load_state_dict(pretrained_model_weights, strict=False)\n\t        print(f'Loaded single-image model weights from {cfg.PRETRAINED.PATH}')\n\t    save_dir = os.path.join(\n", "        cfg.LOG_DIR, time.strftime('%d%B%Yat%H:%M:%S%Z') + '_' + socket.gethostname() + '_' + cfg.TAG\n\t    ) \n\t    tb_logger = pl.loggers.TensorBoardLogger(save_dir=save_dir)\n\t    checkpoint_callback = ModelCheckpoint(monitor='vpq', save_top_k=5, mode='max')\n\t    trainer = pl.Trainer(\n\t        gpus=cfg.GPUS,\n\t        accelerator='ddp',\n\t        precision=cfg.PRECISION,\n\t        sync_batchnorm=True,\n\t        gradient_clip_val=cfg.GRAD_NORM_CLIP,\n", "        max_epochs=cfg.EPOCHS,\n\t        weights_summary='full',\n\t        logger=tb_logger,\n\t        log_every_n_steps=cfg.LOGGING_INTERVAL,\n\t        plugins=DDPPlugin(find_unused_parameters=True),\n\t        profiler='simple',\n\t        callbacks=[checkpoint_callback],\n\t    )\n\t    trainer.fit(model, trainloader, valloader)\n\tif __name__ == \"__main__\":\n", "    main()"]}
{"filename": "visualise.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport os\n\tfrom argparse import ArgumentParser\n\tfrom glob import glob\n", "import cv2\n\timport matplotlib as mpl\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport torch\n\timport torchvision\n\tfrom PIL import Image\n\tfrom powerbev.config import get_cfg, get_parser\n\tfrom powerbev.data import prepare_powerbev_dataloaders\n\tfrom powerbev.trainer import TrainingModule\n", "from powerbev.utils.instance import predict_instance_segmentation\n\tfrom powerbev.utils.network import NormalizeInverse\n\tfrom powerbev.utils.visualisation import (convert_figure_numpy,\n\t                                          generate_instance_colours,\n\t                                          make_contour, plot_instance_map)\n\tdef plot_prediction(image, output, cfg):\n\t    # Process predictions\n\t    consistent_instance_seg, matched_centers = predict_instance_segmentation(\n\t        output, compute_matched_centers=True, spatial_extent=(cfg.LIFT.X_BOUND[1], cfg.LIFT.Y_BOUND[1])\n\t    )\n", "    first_instance_seg = consistent_instance_seg[0, 1]\n\t    # Plot future trajectories\n\t    unique_ids = torch.unique(first_instance_seg).cpu().long().numpy()[1:]\n\t    instance_map = dict(zip(unique_ids, unique_ids))\n\t    instance_colours = generate_instance_colours(instance_map)\n\t    vis_image = plot_instance_map(first_instance_seg.cpu().numpy(), instance_map)\n\t    trajectory_img = np.zeros(vis_image.shape, dtype=np.uint8)\n\t    for instance_id in unique_ids:\n\t        path = matched_centers[instance_id]\n\t        for t in range(len(path) - 1):\n", "            color = instance_colours[instance_id].tolist()\n\t            cv2.line(trajectory_img, tuple(path[t]), tuple(path[t + 1]),\n\t                     color, 4)\n\t    # Overlay arrows\n\t    temp_img = cv2.addWeighted(vis_image, 0.7, trajectory_img, 0.3, 1.0)\n\t    mask = ~ np.all(trajectory_img == 0, axis=2)\n\t    vis_image[mask] = temp_img[mask]\n\t    # Plot present RGB frames and predictions\n\t    val_w = 2.99\n\t    cameras = cfg.IMAGE.NAMES\n", "    image_ratio = cfg.IMAGE.FINAL_DIM[0] / cfg.IMAGE.FINAL_DIM[1]\n\t    val_h = val_w * image_ratio\n\t    fig = plt.figure(figsize=(4 * val_w, 2 * val_h))\n\t    width_ratios = (val_w, val_w, val_w, val_w)\n\t    gs = mpl.gridspec.GridSpec(2, 4, width_ratios=width_ratios)\n\t    gs.update(wspace=0.0, hspace=0.0, left=0.0, right=1.0, top=1.0, bottom=0.0)\n\t    denormalise_img = torchvision.transforms.Compose(\n\t        (NormalizeInverse(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t         torchvision.transforms.ToPILImage(),)\n\t    )\n", "    for imgi, img in enumerate(image[0, -1]):\n\t        ax = plt.subplot(gs[imgi // 3, imgi % 3])\n\t        showimg = denormalise_img(img.cpu())\n\t        if imgi > 2:\n\t            showimg = showimg.transpose(Image.FLIP_LEFT_RIGHT)\n\t        plt.annotate(cameras[imgi].replace('_', ' ').replace('CAM ', ''), (0.01, 0.87), c='white',\n\t                     xycoords='axes fraction', fontsize=14)\n\t        plt.imshow(showimg)\n\t        plt.axis('off')\n\t    ax = plt.subplot(gs[:, 3])\n", "    plt.imshow(make_contour(vis_image[::-1, ::-1]))\n\t    plt.axis('off')\n\t    plt.draw()\n\t    figure_numpy = convert_figure_numpy(fig)\n\t    plt.close()\n\t    return figure_numpy\n\tdef visualise():\n\t    args = get_parser().parse_args()\n\t    cfg = get_cfg(args)\n\t    _, valloader = prepare_powerbev_dataloaders(cfg)\n", "    trainer = TrainingModule(cfg.convert_to_dict())\n\t    if cfg.PRETRAINED.LOAD_WEIGHTS:\n\t        # Load single-image instance segmentation model.\n\t        weights_path = cfg.PRETRAINED.PATH\n\t        pretrained_model_weights = torch.load(\n\t            weights_path , map_location='cpu'\n\t        )['state_dict']\n\t        trainer.load_state_dict(pretrained_model_weights, strict=False)\n\t        print(f'Loaded single-image model weights from {weights_path}')\n\t    device = torch.device('cuda:0')\n", "    trainer = trainer.to(device)\n\t    trainer.eval()\n\t    for i, batch in enumerate(valloader):\n\t        image = batch['image'].to(device)\n\t        intrinsics = batch['intrinsics'].to(device)\n\t        extrinsics = batch['extrinsics'].to(device)\n\t        future_egomotions = batch['future_egomotion'].to(device)\n\t        # Forward pass\n\t        with torch.no_grad():\n\t            output = trainer.model(image, intrinsics, extrinsics, future_egomotions)\n", "        figure_numpy = plot_prediction(image, output, trainer.cfg)\n\t        os.makedirs(os.path.join(cfg.VISUALIZATION.OUTPUT_PATH), exist_ok=True)\n\t        output_filename = os.path.join(cfg.VISUALIZATION.OUTPUT_PATH, 'sample_'+str(i)) + '.png'\n\t        Image.fromarray(figure_numpy).save(output_filename)\n\t        print(f'Saved output in {output_filename}')\n\t        if i >= cfg.VISUALIZATION.SAMPLE_NUMBER-1:\n\t            return\n\tif __name__ == '__main__':\n\t    visualise()"]}
{"filename": "test.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport os\n\timport socket\n\timport time\n", "import pytorch_lightning as pl\n\timport torch\n\tfrom powerbev.config import get_cfg, get_parser\n\tfrom powerbev.data import prepare_powerbev_dataloaders\n\tfrom powerbev.trainer import TrainingModule\n\tfrom pytorch_lightning.plugins import DDPPlugin\n\tdef main():\n\t    args = get_parser().parse_args()\n\t    cfg = get_cfg(args)\n\t    _, valloader = prepare_powerbev_dataloaders(cfg)\n", "    model = TrainingModule(cfg.convert_to_dict())\n\t    if cfg.PRETRAINED.LOAD_WEIGHTS:\n\t        # Load single-image instance segmentation model.\n\t        pretrained_model_weights = torch.load(\n\t            cfg.PRETRAINED.PATH , map_location='cpu'\n\t        )['state_dict']\n\t        model.load_state_dict(pretrained_model_weights, strict=False)\n\t        print(f'Loaded single-image model weights from {cfg.PRETRAINED.PATH}')\n\t    save_dir = os.path.join(\n\t        cfg.LOG_DIR, time.strftime('%d%B%Yat%H:%M:%S%Z') + '_' + socket.gethostname() + '_' + cfg.TAG\n", "    ) \n\t    tb_logger = pl.loggers.TensorBoardLogger(save_dir=save_dir)\n\t    trainer = pl.Trainer(\n\t        gpus=cfg.GPUS,\n\t        accelerator='ddp',\n\t        precision=cfg.PRECISION,\n\t        sync_batchnorm=True,\n\t        gradient_clip_val=cfg.GRAD_NORM_CLIP,\n\t        max_epochs=cfg.EPOCHS,\n\t        weights_summary='full',\n", "        logger=tb_logger,\n\t        log_every_n_steps=cfg.LOGGING_INTERVAL,\n\t        plugins=DDPPlugin(find_unused_parameters=True),\n\t        profiler='simple',\n\t    )\n\t    trainer.test(model, valloader)\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "powerbev/losses.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n", "class SpatialRegressionLoss(nn.Module):\n\t    def __init__(self, norm, ignore_index=255, future_discount=1.0):\n\t        super(SpatialRegressionLoss, self).__init__()\n\t        self.norm = norm\n\t        self.ignore_index = ignore_index\n\t        self.future_discount = future_discount\n\t        if norm == 1:\n\t            self.loss_fn = F.l1_loss\n\t        elif norm == 2:\n\t            self.loss_fn = F.mse_loss\n", "        elif norm == 1.5:\n\t            self.loss_fn = F.smooth_l1_loss\n\t        else:\n\t            raise ValueError(f'Expected norm 1 or 2, but got norm={norm}')\n\t    def forward(self, prediction, target):       \n\t        assert len(prediction.shape) == 5, 'Must be a 5D tensor'\n\t        # ignore_index is the same across all channels\n\t        mask = target[:, :, :1] != self.ignore_index\n\t        if mask.sum() == 0:\n\t            return prediction.new_zeros(1)[0].float()\n", "        loss = self.loss_fn(prediction, target, reduction='none')\n\t        # Sum channel dimension\n\t        loss = torch.sum(loss, dim=-3, keepdims=True)\n\t        seq_len = loss.shape[1]\n\t        future_discounts = self.future_discount ** torch.arange(seq_len, device=loss.device, dtype=loss.dtype)\n\t        future_discounts = future_discounts.view(1, seq_len, 1, 1, 1)\n\t        loss = loss * future_discounts\n\t        return loss[mask].mean()\n\tclass SegmentationLoss(nn.Module):\n\t    def __init__(self, class_weights, ignore_index=255, use_top_k=False, top_k_ratio=1.0, future_discount=1.0):\n", "        super().__init__()\n\t        self.class_weights = class_weights\n\t        self.ignore_index = ignore_index\n\t        self.use_top_k = use_top_k\n\t        self.top_k_ratio = top_k_ratio\n\t        self.future_discount = future_discount\n\t    def forward(self, prediction, target):\n\t        if target.shape[-3] != 1:\n\t            raise ValueError('segmentation label must be an index-label with channel dimension = 1.')\n\t        b, s, c, h, w = prediction.shape\n", "        prediction = prediction.view(b * s, c, h, w)\n\t        target = target.view(b * s, h, w)\n\t        loss = F.cross_entropy(\n\t            prediction,\n\t            target,\n\t            ignore_index=self.ignore_index,\n\t            reduction='none',\n\t            weight=self.class_weights.to(target.device),\n\t        )\n\t        loss = loss.view(b, s, h, w)\n", "        future_discounts = self.future_discount ** torch.arange(s, device=loss.device, dtype=loss.dtype)\n\t        future_discounts = future_discounts.view(1, s, 1, 1)\n\t        loss = loss * future_discounts\n\t        loss = loss.view(b, s, -1)\n\t        if self.use_top_k:\n\t            # Penalises the top-k hardest pixels\n\t            k = int(self.top_k_ratio * loss.shape[2])\n\t            loss, _ = torch.sort(loss, dim=2, descending=True)\n\t            loss = loss[:, :, :k]\n\t        return torch.mean(loss)"]}
{"filename": "powerbev/config.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport argparse\n\tfrom fvcore.common.config import CfgNode as _CfgNode\n\tdef convert_to_dict(cfg_node, key_list=[]):\n", "    \"\"\"Convert a config node to dictionary.\"\"\"\n\t    _VALID_TYPES = {tuple, list, str, int, float, bool}\n\t    if not isinstance(cfg_node, _CfgNode):\n\t        if type(cfg_node) not in _VALID_TYPES:\n\t            print(\n\t                'Key {} with value {} is not a valid type; valid types: {}'.format(\n\t                    '.'.join(key_list), type(cfg_node), _VALID_TYPES\n\t                ),\n\t            )\n\t        return cfg_node\n", "    else:\n\t        cfg_dict = dict(cfg_node)\n\t        for k, v in cfg_dict.items():\n\t            cfg_dict[k] = convert_to_dict(v, key_list + [k])\n\t        return cfg_dict\n\tclass CfgNode(_CfgNode):\n\t    \"\"\"Remove once https://github.com/rbgirshick/yacs/issues/19 is merged.\"\"\"\n\t    def convert_to_dict(self):\n\t        return convert_to_dict(self)\n\tCN = CfgNode\n", "_C = CN()\n\t_C.LOG_DIR = 'tensorboard_logs'\n\t_C.TAG = 'default'\n\t_C.GPUS = [0]  # which gpus to use\n\t_C.PRECISION = 32  # 16bit or 32bit\n\t_C.BATCHSIZE = 2\n\t_C.EPOCHS = 20\n\t_C.N_WORKERS = 5\n\t_C.VIS_INTERVAL = 5000\n\t_C.LOGGING_INTERVAL = 500\n", "_C.PRETRAINED = CN()\n\t_C.PRETRAINED.LOAD_WEIGHTS = False\n\t_C.PRETRAINED.PATH = ''\n\t_C.DATASET = CN()\n\t_C.DATASET.DATAROOT = './nuscenes/'\n\t_C.DATASET.VERSION = 'trainval'\n\t_C.DATASET.NAME = 'nuscenes'\n\t_C.DATASET.IGNORE_INDEX = 255  # Ignore index when creating flow/offset labels\n\t_C.DATASET.FILTER_INVISIBLE_VEHICLES = True  # Filter vehicles that are not visible from the cameras\n\t_C.TIME_RECEPTIVE_FIELD = 3  # how many frames of temporal context (1 for single timeframe)\n", "_C.N_FUTURE_FRAMES = 4  # how many time steps into the future to predict\n\t_C.IMAGE = CN()\n\t_C.IMAGE.FINAL_DIM = (224, 480)\n\t_C.IMAGE.RESIZE_SCALE = 0.3\n\t_C.IMAGE.TOP_CROP = 46\n\t_C.IMAGE.ORIGINAL_HEIGHT = 900  # Original input RGB camera height\n\t_C.IMAGE.ORIGINAL_WIDTH = 1600  # Original input RGB camera width\n\t_C.IMAGE.NAMES = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']\n\t_C.LIFT = CN()  # image to BEV lifting\n\t_C.LIFT.X_BOUND = [-50.0, 50.0, 0.5]  # Forward\n", "_C.LIFT.Y_BOUND = [-50.0, 50.0, 0.5]  # Sides\n\t_C.LIFT.Z_BOUND = [-10.0, 10.0, 20.0]  # Height\n\t_C.LIFT.D_BOUND = [2.0, 50.0, 1.0]\n\t_C.MODEL = CN()\n\t_C.MODEL.ENCODER = CN()\n\t_C.MODEL.ENCODER.DOWNSAMPLE = 8\n\t_C.MODEL.ENCODER.NAME = 'efficientnet-b4'\n\t_C.MODEL.ENCODER.OUT_CHANNELS = 64\n\t_C.MODEL.ENCODER.USE_DEPTH_DISTRIBUTION = True\n\t_C.MODEL.STCONV = CN()\n", "_C.MODEL.STCONV.LATENT_DIM = 16\n\t_C.MODEL.STCONV.NUM_FEATURES = [16, 24, 32, 48, 64]\n\t_C.MODEL.STCONV.NUM_BLOCKS = 3\n\t_C.MODEL.STCONV.INPUT_EGOPOSE = True\n\t_C.MODEL.TEMPORAL_MODEL = CN()\n\t_C.MODEL.TEMPORAL_MODEL.NAME = 'temporal_block'  # type of temporal model\n\t_C.MODEL.TEMPORAL_MODEL.START_OUT_CHANNELS = 64\n\t_C.MODEL.TEMPORAL_MODEL.EXTRA_IN_CHANNELS = 0\n\t_C.MODEL.TEMPORAL_MODEL.INBETWEEN_LAYERS = 0\n\t_C.MODEL.TEMPORAL_MODEL.PYRAMID_POOLING = True\n", "_C.MODEL.TEMPORAL_MODEL.INPUT_EGOPOSE = True\n\t_C.MODEL.DISTRIBUTION = CN()\n\t_C.MODEL.DISTRIBUTION.LATENT_DIM = 32\n\t_C.MODEL.DISTRIBUTION.MIN_LOG_SIGMA = -5.0\n\t_C.MODEL.DISTRIBUTION.MAX_LOG_SIGMA = 5.0\n\t_C.MODEL.FUTURE_PRED = CN()\n\t_C.MODEL.FUTURE_PRED.N_GRU_BLOCKS = 3\n\t_C.MODEL.FUTURE_PRED.N_RES_LAYERS = 3\n\t_C.MODEL.DECODER = CN()\n\t_C.MODEL.BN_MOMENTUM = 0.1\n", "_C.MODEL.SUBSAMPLE = False  # Subsample frames for Lyft\n\t_C.SEMANTIC_SEG = CN()\n\t_C.SEMANTIC_SEG.WEIGHTS = [1.0, 2.0]  # per class cross entropy weights (bg, dynamic, drivable, lane)\n\t_C.SEMANTIC_SEG.USE_TOP_K = True  # backprop only top-k hardest pixels\n\t_C.SEMANTIC_SEG.TOP_K_RATIO = 0.25\n\t_C.INSTANCE_SEG = CN()\n\t_C.INSTANCE_FLOW = CN()\n\t_C.INSTANCE_FLOW.ENABLED = True\n\t_C.PROBABILISTIC = CN()\n\t_C.PROBABILISTIC.ENABLED = False  # learn a distribution over futures\n", "_C.PROBABILISTIC.WEIGHT = 100.0\n\t_C.PROBABILISTIC.FUTURE_DIM = 6  # number of dimension added (future flow, future centerness, offset, seg)\n\t_C.FUTURE_DISCOUNT = 0.95\n\t_C.OPTIMIZER = CN()\n\t_C.OPTIMIZER.LR = 3e-4\n\t_C.OPTIMIZER.WEIGHT_DECAY = 1e-7\n\t_C.GRAD_NORM_CLIP = 5\n\t_C.VISUALIZATION = CN()\n\t_C.VISUALIZATION.OUTPUT_PATH = './visualization_outputs'\n\t_C.VISUALIZATION.SAMPLE_NUMBER = 1000\n", "def get_parser():\n\t    parser = argparse.ArgumentParser(description='PowerBEV training')\n\t    parser.add_argument('--config-file', default='', metavar='FILE', help='path to config file')\n\t    parser.add_argument(\n\t        'opts', help='Modify config options using the command-line', default=None, nargs=argparse.REMAINDER,\n\t    )\n\t    return parser\n\tdef get_cfg(args=None, cfg_dict=None):\n\t    \"\"\" First get default config. Then merge cfg_dict. Then merge according to args. \"\"\"\n\t    cfg = _C.clone()\n", "    if cfg_dict is not None:\n\t        cfg.merge_from_other_cfg(CfgNode(cfg_dict))\n\t    if args is not None:\n\t        if args.config_file:\n\t            cfg.merge_from_file(args.config_file)\n\t        cfg.merge_from_list(args.opts)\n\t        cfg.freeze()\n\t    return cfg\n"]}
{"filename": "powerbev/metrics.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\tfrom typing import Optional\n\timport torch\n\tfrom pytorch_lightning.metrics.functional.classification import \\\n", "    stat_scores_multiple_classes\n\tfrom pytorch_lightning.metrics.functional.reduction import reduce\n\tfrom pytorch_lightning.metrics.metric import Metric\n\tclass IntersectionOverUnion(Metric):\n\t    \"\"\"Computes intersection-over-union.\"\"\"\n\t    def __init__(\n\t        self,\n\t        n_classes: int,\n\t        ignore_index: Optional[int] = None,\n\t        absent_score: float = 0.0,\n", "        reduction: str = 'none',\n\t        compute_on_step: bool = False,\n\t    ):\n\t        super().__init__(compute_on_step=compute_on_step)\n\t        self.n_classes = n_classes\n\t        self.ignore_index = ignore_index\n\t        self.absent_score = absent_score\n\t        self.reduction = reduction\n\t        self.add_state('true_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\t        self.add_state('false_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n", "        self.add_state('false_negative', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\t        self.add_state('support', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\t    def update(self, prediction: torch.Tensor, target: torch.Tensor):\n\t        tps, fps, _, fns, sups = stat_scores_multiple_classes(prediction, target, self.n_classes)\n\t        self.true_positive += tps\n\t        self.false_positive += fps\n\t        self.false_negative += fns\n\t        self.support += sups\n\t    def compute(self):\n\t        scores = torch.zeros(self.n_classes, device=self.true_positive.device, dtype=torch.float32)\n", "        for class_idx in range(self.n_classes):\n\t            if class_idx == self.ignore_index:\n\t                continue\n\t            tp = self.true_positive[class_idx]\n\t            fp = self.false_positive[class_idx]\n\t            fn = self.false_negative[class_idx]\n\t            sup = self.support[class_idx]\n\t            # If this class is absent in the target (no support) AND absent in the pred (no true or false\n\t            # positives), then use the absent_score for this class.\n\t            if sup + tp + fp == 0:\n", "                scores[class_idx] = self.absent_score\n\t                continue\n\t            denominator = tp + fp + fn\n\t            score = tp.to(torch.float) / denominator\n\t            scores[class_idx] = score\n\t        # Remove the ignored class index from the scores.\n\t        if (self.ignore_index is not None) and (0 <= self.ignore_index < self.n_classes):\n\t            scores = torch.cat([scores[:self.ignore_index], scores[self.ignore_index+1:]])\n\t        return reduce(scores, reduction=self.reduction)\n\tclass PanopticMetric(Metric):\n", "    def __init__(\n\t        self,\n\t        n_classes: int,\n\t        temporally_consistent: bool = True,\n\t        vehicles_id: int = 1,\n\t        compute_on_step: bool = False,\n\t    ):\n\t        super().__init__(compute_on_step=compute_on_step)\n\t        self.n_classes = n_classes\n\t        self.temporally_consistent = temporally_consistent\n", "        self.vehicles_id = vehicles_id\n\t        self.keys = ['iou', 'true_positive', 'false_positive', 'false_negative']\n\t        self.add_state('iou', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\t        self.add_state('true_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\t        self.add_state('false_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\t        self.add_state('false_negative', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\t    def update(self, pred_instance, gt_instance):\n\t        \"\"\"\n\t        Update state with predictions and targets.\n\t        Parameters\n", "        ----------\n\t            pred_instance: (b, s, h, w)\n\t                Temporally consistent instance segmentation prediction.\n\t            gt_instance: (b, s, h, w)\n\t                Ground truth instance segmentation.\n\t        \"\"\"\n\t        batch_size, sequence_length = gt_instance.shape[:2]\n\t        # Process labels\n\t        assert gt_instance.min() == 0, 'ID 0 of gt_instance must be background'\n\t        pred_segmentation = (pred_instance > 0).long()\n", "        gt_segmentation = (gt_instance > 0).long()\n\t        for b in range(batch_size):\n\t            unique_id_mapping = {}\n\t            for t in range(sequence_length):\n\t                result = self.panoptic_metrics(\n\t                    pred_segmentation[b, t].detach(),\n\t                    pred_instance[b, t].detach(),\n\t                    gt_segmentation[b, t],\n\t                    gt_instance[b, t],\n\t                    unique_id_mapping,\n", "                )\n\t                self.iou += result['iou']\n\t                self.true_positive += result['true_positive']\n\t                self.false_positive += result['false_positive']\n\t                self.false_negative += result['false_negative']\n\t    def compute(self):\n\t        denominator = torch.maximum(\n\t            (self.true_positive + self.false_positive / 2 + self.false_negative / 2),\n\t            torch.ones_like(self.true_positive)\n\t        )\n", "        pq = self.iou / denominator\n\t        sq = self.iou / torch.maximum(self.true_positive, torch.ones_like(self.true_positive))\n\t        rq = self.true_positive / denominator\n\t        return {'pq': pq,\n\t                'sq': sq,\n\t                'rq': rq,\n\t                # If 0, it means there wasn't any detection.\n\t                'denominator': (self.true_positive + self.false_positive / 2 + self.false_negative / 2),\n\t                }\n\t    def panoptic_metrics(self, pred_segmentation, pred_instance, gt_segmentation, gt_instance, unique_id_mapping):\n", "        \"\"\"\n\t        Computes panoptic quality metric components.\n\t        Parameters\n\t        ----------\n\t            pred_segmentation: [H, W] range {0, ..., n_classes-1} (>= n_classes is void)\n\t            pred_instance: [H, W] range {0, ..., n_instances} (zero means background)\n\t            gt_segmentation: [H, W] range {0, ..., n_classes-1} (>= n_classes is void)\n\t            gt_instance: [H, W] range {0, ..., n_instances} (zero means background)\n\t            unique_id_mapping: instance id mapping to check consistency\n\t        \"\"\"\n", "        n_classes = self.n_classes\n\t        result = {key: torch.zeros(n_classes, dtype=torch.float32, device=gt_instance.device) for key in self.keys}\n\t        assert pred_segmentation.dim() == 2\n\t        assert pred_segmentation.shape == pred_instance.shape == gt_segmentation.shape == gt_instance.shape\n\t        n_instances = int(torch.cat([pred_instance, gt_instance]).max().item())\n\t        n_all_things = n_instances + n_classes  # Classes + instances.\n\t        n_things_and_void = n_all_things + 1\n\t        # Now 1 is background; 0 is void (not used). 2 is vehicle semantic class but since it overlaps with\n\t        # instances, it is not present.\n\t        # and the rest are instance ids starting from 3\n", "        prediction, pred_to_cls = self.combine_mask(pred_segmentation, pred_instance, n_classes, n_all_things)\n\t        target, target_to_cls = self.combine_mask(gt_segmentation, gt_instance, n_classes, n_all_things)\n\t        # Compute ious between all stuff and things\n\t        # hack for bincounting 2 arrays together\n\t        x = prediction + n_things_and_void * target\n\t        bincount_2d = torch.bincount(x.long(), minlength=n_things_and_void ** 2)\n\t        if bincount_2d.shape[0] != n_things_and_void ** 2:\n\t            raise ValueError('Incorrect bincount size.')\n\t        conf = bincount_2d.reshape((n_things_and_void, n_things_and_void))\n\t        # Drop void class\n", "        conf = conf[1:, 1:]\n\t        # Confusion matrix contains intersections between all combinations of classes\n\t        union = conf.sum(0).unsqueeze(0) + conf.sum(1).unsqueeze(1) - conf\n\t        iou = torch.where(union > 0, (conf.float() + 1e-9) / (union.float() + 1e-9), torch.zeros_like(union).float())\n\t        # In the iou matrix, first dimension is target idx, second dimension is pred idx.\n\t        # Mapping will contain a tuple that maps prediction idx to target idx for segments matched by iou.\n\t        mapping = (iou > 0.5).nonzero(as_tuple=False)\n\t        # Check that classes match.\n\t        is_matching = pred_to_cls[mapping[:, 1]] == target_to_cls[mapping[:, 0]]\n\t        mapping = mapping[is_matching]\n", "        tp_mask = torch.zeros_like(conf, dtype=torch.bool)\n\t        tp_mask[mapping[:, 0], mapping[:, 1]] = True\n\t        # First ids correspond to \"stuff\" i.e. semantic seg.\n\t        # Instance ids are offset accordingly\n\t        for target_id, pred_id in mapping:\n\t            cls_id = pred_to_cls[pred_id]\n\t            if self.temporally_consistent and cls_id == self.vehicles_id:\n\t                if target_id.item() in unique_id_mapping and unique_id_mapping[target_id.item()] != pred_id.item():\n\t                    # Not temporally consistent\n\t                    result['false_negative'][target_to_cls[target_id]] += 1\n", "                    result['false_positive'][pred_to_cls[pred_id]] += 1\n\t                    unique_id_mapping[target_id.item()] = pred_id.item()\n\t                    continue\n\t            result['true_positive'][cls_id] += 1\n\t            result['iou'][cls_id] += iou[target_id][pred_id]\n\t            unique_id_mapping[target_id.item()] = pred_id.item()\n\t        for target_id in range(n_classes, n_all_things):\n\t            # If this is a true positive do nothing.\n\t            if tp_mask[target_id, n_classes:].any():\n\t                continue\n", "            # If this target instance didn't match with any predictions and was present set it as false negative.\n\t            if target_to_cls[target_id] != -1:\n\t                result['false_negative'][target_to_cls[target_id]] += 1\n\t        for pred_id in range(n_classes, n_all_things):\n\t            # If this is a true positive do nothing.\n\t            if tp_mask[n_classes:, pred_id].any():\n\t                continue\n\t            # If this predicted instance didn't match with any prediction, set that predictions as false positive.\n\t            if pred_to_cls[pred_id] != -1 and (conf[:, pred_id] > 0).any():\n\t                result['false_positive'][pred_to_cls[pred_id]] += 1\n", "        return result\n\t    def combine_mask(self, segmentation: torch.Tensor, instance: torch.Tensor, n_classes: int, n_all_things: int):\n\t        \"\"\"Shifts all things ids by num_classes and combines things and stuff into a single mask\n\t        Returns a combined mask + a mapping from id to segmentation class.\n\t        \"\"\"\n\t        instance = instance.view(-1)\n\t        instance_mask = instance > 0\n\t        instance = instance - 1 + n_classes\n\t        segmentation = segmentation.clone().view(-1)\n\t        segmentation_mask = segmentation < n_classes  # Remove void pixels.\n", "        # Build an index from instance id to class id.\n\t        instance_id_to_class_tuples = torch.cat(\n\t            (\n\t                instance[instance_mask & segmentation_mask].unsqueeze(1),\n\t                segmentation[instance_mask & segmentation_mask].unsqueeze(1),\n\t            ),\n\t            dim=1,\n\t        )\n\t        instance_id_to_class = -instance_id_to_class_tuples.new_ones((n_all_things,))\n\t        instance_id_to_class[instance_id_to_class_tuples[:, 0]] = instance_id_to_class_tuples[:, 1]\n", "        instance_id_to_class[torch.arange(n_classes, device=segmentation.device)] = torch.arange(\n\t            n_classes, device=segmentation.device\n\t        )\n\t        segmentation[instance_mask] = instance[instance_mask]\n\t        segmentation += 1  # Shift all legit classes by 1.\n\t        segmentation[~segmentation_mask] = 0  # Shift void class to zero.\n\t        return segmentation, instance_id_to_class\n"]}
{"filename": "powerbev/data.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport os\n\tfrom operator import rshift\n\timport cv2\n", "import numpy as np\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision\n\tfrom cv2 import fastNlMeansDenoising\n\tfrom lyft_dataset_sdk.lyftdataset import LyftDataset\n\tfrom nuscenes.nuscenes import NuScenes\n\tfrom nuscenes.utils.data_classes import Box\n\tfrom nuscenes.utils.splits import create_splits_scenes\n\tfrom PIL import Image\n", "from powerbev.utils.geometry import (calculate_birds_eye_view_parameters,\n\t                                     convert_egopose_to_matrix_numpy,\n\t                                     invert_matrix_egopose_numpy, mat2pose_vec,\n\t                                     pose_vec2mat, resize_and_crop_image,\n\t                                     update_intrinsics)\n\tfrom powerbev.utils.instance import \\\n\t    convert_instance_mask_to_center_and_offset_label\n\tfrom powerbev.utils.lyft_splits import TRAIN_LYFT_INDICES, VAL_LYFT_INDICES\n\tfrom pyquaternion import Quaternion\n\tfrom scipy.spatial.transform import Rotation as R\n", "class FuturePredictionDataset(torch.utils.data.Dataset):\n\t    def __init__(self, nusc, is_train, cfg):\n\t        self.nusc = nusc\n\t        self.is_train = is_train\n\t        self.cfg = cfg\n\t        self.is_lyft = isinstance(nusc, LyftDataset)\n\t        if self.is_lyft:\n\t            self.dataroot = self.nusc.data_path\n\t        else:\n\t            self.dataroot = self.nusc.dataroot\n", "        self.mode = 'train' if self.is_train else 'val'\n\t        self.sequence_length = cfg.TIME_RECEPTIVE_FIELD + cfg.N_FUTURE_FRAMES\n\t        self.scenes = self.get_scenes()\n\t        self.ixes = self.get_samples()\n\t        self.indices = self.get_indices()\n\t        # Image resizing and cropping\n\t        self.augmentation_parameters = self.get_resizing_and_cropping_parameters()\n\t        # Normalising input images\n\t        self.normalise_image = torchvision.transforms.Compose(\n\t            [torchvision.transforms.ToTensor(),\n", "             torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t            ]\n\t        )\n\t        # Bird's-eye view parameters\n\t        bev_resolution, bev_start_position, bev_dimension = calculate_birds_eye_view_parameters(\n\t            cfg.LIFT.X_BOUND, cfg.LIFT.Y_BOUND, cfg.LIFT.Z_BOUND\n\t        )\n\t        self.bev_resolution, self.bev_start_position, self.bev_dimension = (\n\t            bev_resolution.numpy(), bev_start_position.numpy(), bev_dimension.numpy()\n\t        )\n", "        # Spatial extent in bird's-eye view, in meters\n\t        self.spatial_extent = (self.cfg.LIFT.X_BOUND[1], self.cfg.LIFT.Y_BOUND[1])\n\t    def get_scenes(self):\n\t        \"\"\"\n\t        Obtain the list of scenes names in the given split.\n\t        \"\"\"\n\t        if self.is_lyft:\n\t            scenes = [row['name'] for row in self.nusc.scene]\n\t            # Split in train/val\n\t            indices = TRAIN_LYFT_INDICES if self.is_train else VAL_LYFT_INDICES\n", "            scenes = [scenes[i] for i in indices]\n\t        else:\n\t            # filter by scene split\n\t            split = {'v1.0-trainval': {True: 'train', False: 'val'},\n\t                     'v1.0-mini': {True: 'mini_train', False: 'mini_val'},}[\n\t                self.nusc.version\n\t            ][self.is_train]\n\t            scenes = create_splits_scenes()[split]\n\t        return scenes\n\t    def get_samples(self):\n", "        \"\"\"\n\t        Find and sort the samples in the given split by scene.\n\t        \"\"\"\n\t        samples = [sample for sample in self.nusc.sample]\n\t        # remove samples that aren't in this split\n\t        samples = [sample for sample in samples if self.nusc.get('scene', sample['scene_token'])['name'] in self.scenes]\n\t        # sort by scene, timestamp (only to make chronological viz easier)\n\t        samples.sort(key=lambda x: (x['scene_token'], x['timestamp']))\n\t        return samples\n\t    def get_indices(self):\n", "        \"\"\"\n\t        Group the sample indices by sequence length.\n\t        \"\"\"\n\t        indices = []\n\t        for index in range(len(self.ixes)):\n\t            is_valid_data = True\n\t            previous_rec = None\n\t            current_indices = []\n\t            for t in range(self.sequence_length):\n\t                index_t = index + t\n", "                # Going over the dataset size limit.\n\t                if index_t >= len(self.ixes):\n\t                    is_valid_data = False\n\t                    break\n\t                rec = self.ixes[index_t]\n\t                # Check if scene is the same\n\t                if (previous_rec is not None) and (rec['scene_token'] != previous_rec['scene_token']):\n\t                    is_valid_data = False\n\t                    break\n\t                current_indices.append(index_t)\n", "                previous_rec = rec\n\t            if is_valid_data:\n\t                indices.append(current_indices)\n\t        return np.asarray(indices)\n\t    def get_resizing_and_cropping_parameters(self):\n\t        \"\"\"\n\t        Determine the parameters for preprocessing of the input image, e.g. image size after scaling, cropping area.\n\t        \"\"\"\n\t        original_height, original_width = self.cfg.IMAGE.ORIGINAL_HEIGHT, self.cfg.IMAGE.ORIGINAL_WIDTH\n\t        final_height, final_width = self.cfg.IMAGE.FINAL_DIM\n", "        resize_scale = self.cfg.IMAGE.RESIZE_SCALE\n\t        resize_dims = (int(original_width * resize_scale), int(original_height * resize_scale))\n\t        resized_width, resized_height = resize_dims\n\t        crop_h = self.cfg.IMAGE.TOP_CROP\n\t        crop_w = int(max(0, (resized_width - final_width) / 2))\n\t        # Left, top, right, bottom crops.\n\t        crop = (crop_w, crop_h, crop_w + final_width, crop_h + final_height)\n\t        if resized_width != final_width:\n\t            print('Zero padding left and right parts of the image.')\n\t        if crop_h + final_height != resized_height:\n", "            print('Zero padding bottom part of the image.')\n\t        return {'scale_width': resize_scale,\n\t                'scale_height': resize_scale,\n\t                'resize_dims': resize_dims,\n\t                'crop': crop,\n\t                }\n\t    def get_input_data(self, rec):\n\t        \"\"\"\n\t        Obtain the input image as well as the intrinsics and extrinsics parameters of the corresponding camera.\n\t        Parameters\n", "        ----------\n\t            rec: nuscenes identifier for a given timestamp\n\t        Returns\n\t        -------\n\t            images: torch.Tensor<float> (N, 3, H, W)\n\t            intrinsics: torch.Tensor<float> (3, 3)\n\t            extrinsics: torch.Tensor(N, 4, 4)\n\t        \"\"\"\n\t        images = []\n\t        intrinsics = []\n", "        extrinsics = []\n\t        cameras = self.cfg.IMAGE.NAMES\n\t        # The extrinsics we want are from the camera sensor to \"flat egopose\" as defined\n\t        # https://github.com/nutonomy/nuscenes-devkit/blob/9b492f76df22943daf1dc991358d3d606314af27/python-sdk/nuscenes/nuscenes.py#L279\n\t        # which corresponds to the position of the lidar.\n\t        # This is because the labels are generated by projecting the 3D bounding box in this lidar's reference frame.\n\t        # From lidar egopose to world.\n\t        lidar_sample = self.nusc.get('sample_data', rec['data']['LIDAR_TOP'])\n\t        lidar_pose = self.nusc.get('ego_pose', lidar_sample['ego_pose_token'])\n\t        yaw = Quaternion(lidar_pose['rotation']).yaw_pitch_roll[0]\n", "        lidar_rotation = Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)])\n\t        lidar_translation = np.array(lidar_pose['translation'])[:, None]\n\t        lidar_to_world = np.vstack([\n\t            np.hstack((lidar_rotation.rotation_matrix, lidar_translation)),\n\t            np.array([0, 0, 0, 1])\n\t        ])\n\t        for cam in cameras:\n\t            camera_sample = self.nusc.get('sample_data', rec['data'][cam])\n\t            # Transformation from world to egopose\n\t            car_egopose = self.nusc.get('ego_pose', camera_sample['ego_pose_token'])\n", "            egopose_rotation = Quaternion(car_egopose['rotation']).inverse\n\t            egopose_translation = -np.array(car_egopose['translation'])[:, None]\n\t            world_to_car_egopose = np.vstack([\n\t                np.hstack((egopose_rotation.rotation_matrix, egopose_rotation.rotation_matrix @ egopose_translation)),\n\t                np.array([0, 0, 0, 1])\n\t            ])\n\t            # From egopose to sensor\n\t            sensor_sample = self.nusc.get('calibrated_sensor', camera_sample['calibrated_sensor_token'])\n\t            intrinsic = torch.Tensor(sensor_sample['camera_intrinsic'])\n\t            sensor_rotation = Quaternion(sensor_sample['rotation'])\n", "            sensor_translation = np.array(sensor_sample['translation'])[:, None]\n\t            car_egopose_to_sensor = np.vstack([\n\t                np.hstack((sensor_rotation.rotation_matrix, sensor_translation)),\n\t                np.array([0, 0, 0, 1])\n\t            ])\n\t            car_egopose_to_sensor = np.linalg.inv(car_egopose_to_sensor)\n\t            # Combine all the transformation.\n\t            # From sensor to lidar.\n\t            lidar_to_sensor = car_egopose_to_sensor @ world_to_car_egopose @ lidar_to_world\n\t            sensor_to_lidar = torch.from_numpy(np.linalg.inv(lidar_to_sensor)).float()\n", "            # Load image\n\t            image_filename = os.path.join(self.dataroot, camera_sample['filename'])\n\t            img = Image.open(image_filename)\n\t            # Resize and crop\n\t            img = resize_and_crop_image(\n\t                img, resize_dims=self.augmentation_parameters['resize_dims'], crop=self.augmentation_parameters['crop']\n\t            )\n\t            # Normalise image\n\t            normalised_img = self.normalise_image(img)\n\t            # Combine resize/cropping in the intrinsics\n", "            top_crop = self.augmentation_parameters['crop'][1]\n\t            left_crop = self.augmentation_parameters['crop'][0]\n\t            intrinsic = update_intrinsics(\n\t                intrinsic, top_crop, left_crop,\n\t                scale_width=self.augmentation_parameters['scale_width'],\n\t                scale_height=self.augmentation_parameters['scale_height']\n\t            )\n\t            images.append(normalised_img.unsqueeze(0).unsqueeze(0))\n\t            intrinsics.append(intrinsic.unsqueeze(0).unsqueeze(0))\n\t            extrinsics.append(sensor_to_lidar.unsqueeze(0).unsqueeze(0))\n", "        images, intrinsics, extrinsics = (torch.cat(images, dim=1),\n\t                                          torch.cat(intrinsics, dim=1),\n\t                                          torch.cat(extrinsics, dim=1)\n\t                                          )\n\t        return images, intrinsics, extrinsics\n\t    def _get_top_lidar_pose(self, rec):\n\t        \"\"\"\n\t        Obtain the vehicle attitude at the current moment.\n\t        \"\"\"\n\t        egopose = self.nusc.get('ego_pose', self.nusc.get('sample_data', rec['data']['LIDAR_TOP'])['ego_pose_token'])\n", "        trans = -np.array(egopose['translation'])\n\t        yaw = Quaternion(egopose['rotation']).yaw_pitch_roll[0]\n\t        rot = Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)]).inverse\n\t        return trans, rot\n\t    def record_instance(self, rec, instance_map):\n\t        \"\"\"\n\t        Record information about each visible instance in the sequence and assign a unique ID to it.\n\t        \"\"\"\n\t        translation, rotation = self._get_top_lidar_pose(rec)\n\t        self.egopose_list.append([translation, rotation])\n", "        for annotation_token in rec['anns']:\n\t            annotation = self.nusc.get('sample_annotation', annotation_token)\n\t            if not self.is_lyft:\n\t                # NuScenes filter\n\t                # Filter out all non vehicle instances\n\t                if 'vehicle' not in annotation['category_name']:\n\t                    continue\n\t                # Filter out invisible vehicles\n\t                if self.cfg.DATASET.FILTER_INVISIBLE_VEHICLES and int(annotation['visibility_token']) == 1 and annotation['instance_token'] not in self.visible_instance_set:\n\t                    continue\n", "                # Filter out vehicles that have not been seen in the past\n\t                if self.counter >= self.cfg.TIME_RECEPTIVE_FIELD and annotation['instance_token'] not in self.visible_instance_set:\n\t                    continue\n\t                self.visible_instance_set.add(annotation['instance_token'])\n\t            else:\n\t                # Lyft filter\n\t                # Filter out all non vehicle instances\n\t                if annotation['category_name'] not in ['bus', 'car', 'construction_vehicle', 'trailer', 'truck']:\n\t                    continue\n\t            if annotation['instance_token'] not in instance_map:\n", "                instance_map[annotation['instance_token']] = len(instance_map) + 1\n\t            instance_id = instance_map[annotation['instance_token']]\n\t            if not self.is_lyft:\n\t                instance_attribute = int(annotation['visibility_token'])\n\t            else:\n\t                instance_attribute = 0\n\t            if annotation['instance_token'] not in self.instance_dict:\n\t                # For the first occurrence of an instance\n\t                self.instance_dict[annotation['instance_token']] = {\n\t                    'timestep': [self.counter],\n", "                    'translation': [annotation['translation']],\n\t                    'rotation': [annotation['rotation']],\n\t                    'size': annotation['size'],\n\t                    'instance_id': instance_id,\n\t                    'attribute_label': [instance_attribute],\n\t                }\n\t            else:\n\t                # For the instance that have appeared before\n\t                self.instance_dict[annotation['instance_token']]['timestep'].append(self.counter)\n\t                self.instance_dict[annotation['instance_token']]['translation'].append(annotation['translation'])\n", "                self.instance_dict[annotation['instance_token']]['rotation'].append(annotation['rotation'])\n\t                self.instance_dict[annotation['instance_token']]['attribute_label'].append(instance_attribute)\n\t        return instance_map\n\t    @staticmethod\n\t    def _check_consistency(translation, prev_translation, threshold=1.0):\n\t        \"\"\"\n\t        Check for significant displacement of the instance adjacent moments.\n\t        \"\"\"\n\t        x, y = translation[:2]\n\t        prev_x, prev_y = prev_translation[:2]\n", "        if abs(x - prev_x) > threshold or abs(y - prev_y) > threshold:\n\t            return False\n\t        return True\n\t    def refine_instance_poly(self, instance):\n\t        \"\"\"\n\t        Fix the missing frames and disturbances of ground truth caused by noise.\n\t        \"\"\"\n\t        pointer = 1\n\t        for i in range(instance['timestep'][0] + 1, self.sequence_length):\n\t            # Fill in the missing frames\n", "            if i not in instance['timestep']:\n\t                instance['timestep'].insert(pointer, i)\n\t                instance['translation'].insert(pointer, instance['translation'][pointer-1])\n\t                instance['rotation'].insert(pointer, instance['rotation'][pointer-1])\n\t                instance['attribute_label'].insert(pointer, instance['attribute_label'][pointer-1])\n\t                pointer += 1\n\t                continue\n\t            # Eliminate observation disturbances\n\t            if self._check_consistency(instance['translation'][pointer], instance['translation'][pointer-1]):\n\t                instance['translation'][pointer] = instance['translation'][pointer-1]\n", "                instance['rotation'][pointer] = instance['rotation'][pointer-1]\n\t                instance['attribute_label'][pointer] = instance['attribute_label'][pointer-1]\n\t            pointer += 1\n\t        return instance\n\t    @staticmethod\n\t    def generate_flow(flow, instance_img, instance, instance_id):\n\t        \"\"\"\n\t        Generate ground truth for the flow of each instance based on instance segmentation.\n\t        \"\"\"\n\t        _, h, w = instance_img.shape\n", "        x, y = torch.meshgrid(torch.arange(h, dtype=torch.float), torch.arange(w, dtype=torch.float))\n\t        grid = torch.stack((x, y), dim=0)\n\t        # Set the first frame\n\t        instance_mask = (instance_img[0] == instance_id)\n\t        flow[0, 1, instance_mask] = grid[0, instance_mask].mean(dim=0, keepdim=True).round() - grid[0, instance_mask]\n\t        flow[0, 0, instance_mask] = grid[1, instance_mask].mean(dim=0, keepdim=True).round() - grid[1, instance_mask]\n\t        for i, timestep in enumerate(instance['timestep']):\n\t            if i == 0:\n\t                continue\n\t            instance_mask = (instance_img[timestep] == instance_id)\n", "            prev_instance_mask = (instance_img[timestep-1] == instance_id)\n\t            if instance_mask.sum() == 0 or prev_instance_mask.sum() == 0:\n\t                continue\n\t            # Centripetal backward flow is defined as displacement vector from each foreground pixel at time t to the object center of the associated instance identity at time t−1\n\t            flow[timestep, 1, instance_mask] = grid[0, prev_instance_mask].mean(dim=0, keepdim=True).round() - grid[0, instance_mask]\n\t            flow[timestep, 0, instance_mask] = grid[1, prev_instance_mask].mean(dim=0, keepdim=True).round() - grid[1, instance_mask]\n\t        return flow\n\t    def get_flow_label(self, instance_img, instance_map, ignore_index=255):\n\t        \"\"\"\n\t        Generate the global map of the flow ground truth.\n", "        \"\"\"\n\t        seq_len, h, w = instance_img.shape\n\t        flow = ignore_index * torch.ones(seq_len, 2, h, w)\n\t        for token, instance in self.instance_dict.items():\n\t            flow = self.generate_flow(flow, instance_img, instance, instance_map[token])\n\t        return flow\n\t    def _get_poly_region_in_image(self, instance_annotation, present_egopose):\n\t        \"\"\"\n\t        Obtain the bounding box polygon of the instance.\n\t        \"\"\"\n", "        present_ego_translation, present_ego_rotation = present_egopose\n\t        box = Box(\n\t            instance_annotation['translation'], instance_annotation['size'], Quaternion(instance_annotation['rotation'])\n\t        )\n\t        box.translate(present_ego_translation)\n\t        box.rotate(present_ego_rotation)\n\t        pts = box.bottom_corners()[:2].T\n\t        if self.cfg.LIFT.X_BOUND[0] <= pts.min(axis=0)[0] and pts.max(axis=0)[0] <= self.cfg.LIFT.X_BOUND[1] and self.cfg.LIFT.Y_BOUND[0] <= pts.min(axis=0)[1] and pts.max(axis=0)[1] <= self.cfg.LIFT.Y_BOUND[1]:\n\t            pts = np.round((pts - self.bev_start_position[:2] + self.bev_resolution[:2] / 2.0) / self.bev_resolution[:2]).astype(np.int32)\n\t            pts[:, [1, 0]] = pts[:, [0, 1]]\n", "            z = box.bottom_corners()[2, 0]\n\t            return pts, z\n\t        else:\n\t            return None, None\n\t    def get_label(self):\n\t        \"\"\"\n\t        Generate labels for semantic segmentation, instance segmentation, z position, attribute from the raw data of nuScenes.\n\t        \"\"\"\n\t        timestep = self.counter\n\t        segmentation = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n", "        # Background is ID 0\n\t        instance = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n\t        z_position = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n\t        attribute_label = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n\t        for instance_token, instance_annotation in self.instance_dict.items():\n\t            if timestep not in instance_annotation['timestep']:\n\t                continue\n\t            pointer = instance_annotation['timestep'].index(timestep)\n\t            annotation = {\n\t                'translation': instance_annotation['translation'][pointer],\n", "                'rotation': instance_annotation['rotation'][pointer],\n\t                'size': instance_annotation['size'],\n\t            }\n\t            poly_region, z = self._get_poly_region_in_image(annotation, self.egopose_list[self.cfg.TIME_RECEPTIVE_FIELD - 1]) \n\t            if isinstance(poly_region, np.ndarray):\n\t                if self.counter >= self.cfg.TIME_RECEPTIVE_FIELD and instance_token not in self.visible_instance_set:\n\t                    continue\n\t                self.visible_instance_set.add(instance_token)\n\t                cv2.fillPoly(instance, [poly_region], instance_annotation['instance_id'])\n\t                cv2.fillPoly(segmentation, [poly_region], 1.0)\n", "                cv2.fillPoly(z_position, [poly_region], z)\n\t                cv2.fillPoly(attribute_label, [poly_region], instance_annotation['attribute_label'][pointer]) \n\t        segmentation = torch.from_numpy(segmentation).long().unsqueeze(0).unsqueeze(0)\n\t        instance = torch.from_numpy(instance).long().unsqueeze(0)\n\t        z_position = torch.from_numpy(z_position).float().unsqueeze(0).unsqueeze(0)\n\t        attribute_label = torch.from_numpy(attribute_label).long().unsqueeze(0).unsqueeze(0)\n\t        return segmentation, instance, z_position, attribute_label\n\t    def get_future_egomotion(self, rec, index):\n\t        \"\"\"\n\t        Obtain the egomotion in the corresponding sequence from the raw data of nuScenes.\n", "        \"\"\"\n\t        rec_t0 = rec\n\t        # Identity\n\t        future_egomotion = np.eye(4, dtype=np.float32)\n\t        if index < len(self.ixes) - 1:\n\t            rec_t1 = self.ixes[index + 1]\n\t            if rec_t0['scene_token'] == rec_t1['scene_token']:\n\t                egopose_t0 = self.nusc.get(\n\t                    'ego_pose', self.nusc.get('sample_data', rec_t0['data']['LIDAR_TOP'])['ego_pose_token']\n\t                )\n", "                egopose_t1 = self.nusc.get(\n\t                    'ego_pose', self.nusc.get('sample_data', rec_t1['data']['LIDAR_TOP'])['ego_pose_token']\n\t                )\n\t                egopose_t0 = convert_egopose_to_matrix_numpy(egopose_t0)\n\t                egopose_t1 = convert_egopose_to_matrix_numpy(egopose_t1)\n\t                future_egomotion = invert_matrix_egopose_numpy(egopose_t1).dot(egopose_t0)\n\t                future_egomotion[3, :3] = 0.0\n\t                future_egomotion[3, 3] = 1.0\n\t        future_egomotion = torch.Tensor(future_egomotion).float()\n\t        # Convert to 6DoF vector\n", "        future_egomotion = mat2pose_vec(future_egomotion)\n\t        return future_egomotion.unsqueeze(0)\n\t    def __len__(self):\n\t        return len(self.indices)\n\t    def __getitem__(self, index):\n\t        \"\"\"\n\t        Returns\n\t        -------\n\t            data: dict with the following keys:\n\t                image: torch.Tensor<float> (T, N, 3, H, W)\n", "                    normalised cameras images with T the sequence length, and N the number of cameras.\n\t                intrinsics: torch.Tensor<float> (T, N, 3, 3)\n\t                    intrinsics containing resizing and cropping parameters.\n\t                extrinsics: torch.Tensor<float> (T, N, 4, 4)\n\t                    6 DoF pose from world coordinates to camera coordinates.\n\t                segmentation: torch.Tensor<int64> (T, 1, H_bev, W_bev)\n\t                    (H_bev, W_bev) are the pixel dimensions in bird's-eye view.\n\t                instance: torch.Tensor<int64> (T, 1, H_bev, W_bev)\n\t                centerness: torch.Tensor<float> (T, 1, H_bev, W_bev)\n\t                offset: torch.Tensor<float> (T, 2, H_bev, W_bev)\n", "                flow: torch.Tensor<float> (T, 2, H_bev, W_bev)\n\t                future_egomotion: torch.Tensor<float> (T, 6)\n\t                    6 DoF egomotion t -> t+1\n\t                sample_token: List<str> (T,)\n\t                'z_position': list_z_position,\n\t                'attribute': list_attribute_label,\n\t        \"\"\"\n\t        data = {}\n\t        keys = ['image', 'intrinsics', 'extrinsics',\n\t                'segmentation', 'instance', 'centerness', 'offset', 'flow', 'future_egomotion',\n", "                'sample_token',\n\t                'z_position', 'attribute'\n\t                ]\n\t        for key in keys:\n\t            data[key] = []\n\t        instance_map = {}\n\t        # The visible instance must have high visibility in the time receptive field\n\t        self.visible_instance_set = set()\n\t        # Record all valid instance\n\t        self.instance_dict = {}\n", "        # Record translation and rotation of the ego-vehicle for the entire time period\n\t        self.egopose_list = []\n\t        # Generate input data\n\t        for self.counter, index_t in enumerate(self.indices[index]):\n\t            rec = self.ixes[index_t]\n\t            images, intrinsics, extrinsics = self.get_input_data(rec)\n\t            instance_map = self.record_instance(rec, instance_map)\n\t            future_egomotion = self.get_future_egomotion(rec, index_t)\n\t            data['image'].append(images)\n\t            data['intrinsics'].append(intrinsics)\n", "            data['extrinsics'].append(extrinsics)\n\t            data['future_egomotion'].append(future_egomotion)\n\t            data['sample_token'].append(rec['token'])\n\t        # Refine the generated instance polygons    \n\t        for token in self.instance_dict.keys():\n\t            self.instance_dict[token] = self.refine_instance_poly(self.instance_dict[token])\n\t        # The visible instance must have high visibility in the time receptive field\n\t        self.visible_instance_set = set()\n\t        # Generate instance ground truth\n\t        for self.counter in range(self.sequence_length):\n", "            segmentation, instance, z_position, attribute_label = self.get_label()\n\t            data['segmentation'].append(segmentation)\n\t            data['instance'].append(instance)\n\t            data['z_position'].append(z_position)\n\t            data['attribute'].append(attribute_label)\n\t        for key, value in data.items():\n\t            if key in ['sample_token', 'centerness', 'offset', 'flow']:\n\t                continue\n\t            data[key] = torch.cat(value, dim=0)\n\t        # Generate centripetal backward flow ground truth from the instance ground truth\n", "        data['flow'] = self.get_flow_label(data['instance'], instance_map, ignore_index=self.cfg.DATASET.IGNORE_INDEX)\n\t        # If lyft need to subsample, and update future_egomotions\n\t        if self.cfg.MODEL.SUBSAMPLE:\n\t            for key, value in data.items():\n\t                if key in ['future_egomotion', 'sample_token', 'centerness', 'offset', 'flow']:\n\t                    continue\n\t                data[key] = data[key][::2].clone()\n\t            data['sample_token'] = data['sample_token'][::2]\n\t            # Update future egomotions\n\t            future_egomotions_matrix = pose_vec2mat(data['future_egomotion'])\n", "            future_egomotion_accum = torch.zeros_like(future_egomotions_matrix)\n\t            future_egomotion_accum[:-1] = future_egomotions_matrix[:-1] @ future_egomotions_matrix[1:]\n\t            future_egomotion_accum = mat2pose_vec(future_egomotion_accum)\n\t            data['future_egomotion'] = future_egomotion_accum[::2].clone()\n\t        # Generate the ground truth of centerness and offset\n\t        instance_centerness, instance_offset = convert_instance_mask_to_center_and_offset_label(\n\t            data['instance'], num_instances=len(instance_map), ignore_index=self.cfg.DATASET.IGNORE_INDEX,\n\t        )\n\t        data['centerness'] = instance_centerness\n\t        data['offset'] = instance_offset\n", "        data['num_instances'] = torch.tensor([len(instance_map)])\n\t        return data\n\tdef prepare_powerbev_dataloaders(cfg, return_dataset=False):\n\t    \"\"\"\n\t    Prepare the dataloader of PowerBEV.\n\t    \"\"\"\n\t    version = cfg.DATASET.VERSION\n\t    train_on_training_data = True\n\t    if cfg.DATASET.NAME == 'nuscenes':\n\t        # 28130 train and 6019 val\n", "        dataroot = os.path.join(cfg.DATASET.DATAROOT, version)\n\t        nusc = NuScenes(version='v1.0-{}'.format(cfg.DATASET.VERSION), dataroot=dataroot, verbose=False)\n\t    elif cfg.DATASET.NAME == 'lyft':\n\t        # train contains 22680 samples\n\t        # we split in 16506 6174\n\t        dataroot = os.path.join(cfg.DATASET.DATAROOT, 'trainval')\n\t        nusc = LyftDataset(data_path=dataroot,\n\t                           json_path=os.path.join(dataroot, 'train_data'),\n\t                           verbose=True)\n\t    train_data = FuturePredictionDataset(nusc, train_on_training_data, cfg)\n", "    val_data = FuturePredictionDataset(nusc, False, cfg)\n\t    if cfg.DATASET.VERSION == 'mini':\n\t        train_data.indices = train_data.indices[:10]\n\t        val_data.indices = val_data.indices[:10]\n\t    nworkers = cfg.N_WORKERS\n\t    train_loader = torch.utils.data.DataLoader(\n\t        train_data, batch_size=cfg.BATCHSIZE, shuffle=True, num_workers=nworkers, pin_memory=True, drop_last=True\n\t    )\n\t    val_loader = torch.utils.data.DataLoader(\n\t        val_data, batch_size=1, shuffle=False, num_workers=nworkers, pin_memory=True, drop_last=False)\n", "    if return_dataset:\n\t        return train_loader, val_loader, train_data, val_data\n\t    else:\n\t        return train_loader, val_loader\n"]}
{"filename": "powerbev/trainer.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport time\n\timport pytorch_lightning as pl\n\timport torch\n", "import torch.nn as nn\n\tfrom powerbev.config import get_cfg\n\tfrom powerbev.losses import SegmentationLoss, SpatialRegressionLoss\n\tfrom powerbev.metrics import IntersectionOverUnion, PanopticMetric\n\tfrom powerbev.models.powerbev import PowerBEV\n\tfrom powerbev.utils.instance import predict_instance_segmentation\n\tfrom powerbev.utils.visualisation import visualise_output\n\tfrom thop import profile\n\tclass TrainingModule(pl.LightningModule):\n\t    def __init__(self, hparams):\n", "        super().__init__()\n\t        # see config.py for details\n\t        self.hparams = hparams\n\t        # pytorch lightning does not support saving YACS CfgNone\n\t        cfg = get_cfg(cfg_dict=self.hparams)\n\t        self.cfg = cfg\n\t        self.n_classes = len(self.cfg.SEMANTIC_SEG.WEIGHTS)\n\t        # Bird's-eye view extent in meters\n\t        assert self.cfg.LIFT.X_BOUND[1] > 0 and self.cfg.LIFT.Y_BOUND[1] > 0\n\t        self.spatial_extent = (self.cfg.LIFT.X_BOUND[1], self.cfg.LIFT.Y_BOUND[1])\n", "        # Model\n\t        self.model = PowerBEV(cfg)\n\t        self.calculate_flops = True\n\t        # Losses\n\t        self.losses_fn = nn.ModuleDict()\n\t        self.losses_fn['segmentation'] = SegmentationLoss(\n\t            class_weights=torch.Tensor(self.cfg.SEMANTIC_SEG.WEIGHTS),\n\t            use_top_k=self.cfg.SEMANTIC_SEG.USE_TOP_K,\n\t            top_k_ratio=self.cfg.SEMANTIC_SEG.TOP_K_RATIO,\n\t            future_discount=self.cfg.FUTURE_DISCOUNT,\n", "        )\n\t        self.losses_fn['instance_flow'] = SpatialRegressionLoss(\n\t            norm=1.5, \n\t            future_discount=self.cfg.FUTURE_DISCOUNT, \n\t            ignore_index=self.cfg.DATASET.IGNORE_INDEX,\n\t        )\n\t        # Uncertainty weighting\n\t        self.model.segmentation_weight = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n\t        self.model.flow_weight = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n\t        # Metrics\n", "        self.metric_iou_val = IntersectionOverUnion(self.n_classes)\n\t        self.metric_panoptic_val = PanopticMetric(n_classes=self.n_classes)\n\t        self.training_step_count = 0\n\t        # Run time\n\t        self.perception_time, self.prediction_time, self.postprocessing_time = [], [], []\n\t    def shared_step(self, batch, is_train):\n\t        image = batch['image']\n\t        intrinsics = batch['intrinsics']\n\t        extrinsics = batch['extrinsics']\n\t        future_egomotion = batch['future_egomotion']\n", "        # Warp labels\n\t        labels, future_distribution_inputs = self.prepare_future_labels(batch)\n\t        # Calculate FLOPs\n\t        if self.calculate_flops:\n\t            flops, _ = profile(self.model, inputs=(image, intrinsics, extrinsics, future_egomotion, future_distribution_inputs))\n\t            print('{:.2f} G \\tTotal FLOPs'.format(flops/1000**3))\n\t            self.calculate_flops = False\n\t        # Forward pass\n\t        output = self.model(image, intrinsics, extrinsics, future_egomotion, future_distribution_inputs)\n\t        # Calculate loss\n", "        loss = self.calculate_loss(output, labels)\n\t        if not is_train:\n\t            # Perform warping-based pixel-level association\n\t            start_time = time.time()\n\t            pred_consistent_instance_seg = predict_instance_segmentation(output, spatial_extent=self.spatial_extent)\n\t            end_time = time.time()\n\t            # Calculate metrics\n\t            self.metric_iou_val(torch.argmax(output['segmentation'].detach(), dim=2, keepdims=True)[:, 1:], labels['segmentation'][:, 1:])\n\t            self.metric_panoptic_val(pred_consistent_instance_seg[:, 1:], labels['instance'][:, 1:])\n\t            # Record run time\n", "            self.perception_time.append(output['perception_time'])\n\t            self.prediction_time.append(output['prediction_time'])\n\t            self.postprocessing_time.append(end_time-start_time)\n\t        return output, labels, loss\n\t    def calculate_loss(self, output, labels):\n\t        loss = {}\n\t        segmentation_factor = 100 / torch.exp(self.model.segmentation_weight)\n\t        loss['segmentation'] = segmentation_factor * self.losses_fn['segmentation'](\n\t            output['segmentation'], \n\t            labels['segmentation'], \n", "        )\n\t        loss[f'segmentation_uncertainty'] = 0.5 * self.model.segmentation_weight\n\t        flow_factor = 0.1 / (2*torch.exp(self.model.flow_weight))\n\t        loss['instance_flow'] = flow_factor * self.losses_fn['instance_flow'](\n\t            output['instance_flow'], \n\t            labels['flow']\n\t        )\n\t        loss['flow_uncertainty'] = 0.5 * self.model.flow_weight\n\t        return loss\n\t    def prepare_future_labels(self, batch):\n", "        labels = {}\n\t        future_distribution_inputs = []\n\t        segmentation_labels = batch['segmentation']\n\t        instance_center_labels = batch['centerness']\n\t        instance_offset_labels = batch['offset']\n\t        instance_flow_labels = batch['flow']\n\t        gt_instance = batch['instance']\n\t        label_time_range = self.model.receptive_field - 2  # See section 3.4 in paper for details.\n\t        segmentation_labels = segmentation_labels[:, label_time_range:].long().contiguous()\n\t        labels['segmentation'] = segmentation_labels\n", "        future_distribution_inputs.append(segmentation_labels)\n\t        gt_instance = gt_instance[:, label_time_range:].long().contiguous()\n\t        labels['instance'] = gt_instance\n\t        instance_center_labels = instance_center_labels[:, label_time_range:].contiguous()\n\t        labels['centerness'] = instance_center_labels\n\t        future_distribution_inputs.append(instance_center_labels)\n\t        instance_offset_labels = instance_offset_labels[:, label_time_range:].contiguous()\n\t        labels['offset'] = instance_offset_labels\n\t        future_distribution_inputs.append(instance_offset_labels)\n\t        instance_flow_labels = instance_flow_labels[:, label_time_range:]\n", "        labels['flow'] = instance_flow_labels\n\t        future_distribution_inputs.append(instance_flow_labels)\n\t        if len(future_distribution_inputs) > 0:\n\t            future_distribution_inputs = torch.cat(future_distribution_inputs, dim=2)\n\t        labels['future_egomotion'] = batch['future_egomotion']\n\t        return labels, future_distribution_inputs\n\t    def visualise(self, labels, output, batch_idx, prefix='train'):\n\t        visualisation_video = visualise_output(labels, output, self.cfg)\n\t        name = f'{prefix}_outputs'\n\t        if prefix == 'val':\n", "            name = name + f'_{batch_idx}'\n\t        self.logger.experiment.add_video(name, visualisation_video, global_step=self.training_step_count, fps=2)\n\t    def training_step(self, batch, batch_idx):\n\t        output, labels, loss = self.shared_step(batch, True)\n\t        self.training_step_count += 1\n\t        for key, value in loss.items():\n\t            self.logger.experiment.add_scalar('train_loss/' + key, value, global_step=self.training_step_count)\n\t        if self.training_step_count % self.cfg.VIS_INTERVAL == 0:\n\t            self.visualise(labels, output, batch_idx, prefix='train')\n\t        return sum(loss.values())\n", "    def validation_step(self, batch, batch_idx):\n\t        output, labels, loss = self.shared_step(batch, False)\n\t        for key, value in loss.items():\n\t            self.log('val_loss/' + key, value)\n\t        if batch_idx == 0:\n\t            self.visualise(labels, output, batch_idx, prefix='val')\n\t    def test_step(self, batch, batch_idx):\n\t        output, labels, loss = self.shared_step(batch, False)\n\t        for key, value in loss.items():\n\t            self.log('test_loss/' + key, value)\n", "        if batch_idx == 0:\n\t            self.visualise(labels, output, batch_idx, prefix='test')\n\t    def shared_epoch_end(self, step_outputs, is_train):\n\t        # Log per class iou metrics\n\t        class_names = ['background', 'dynamic']\n\t        if not is_train:\n\t            print(\"========================== Metrics ==========================\")\n\t            scores = self.metric_iou_val.compute()\n\t            for key, value in zip(class_names, scores):\n\t                self.logger.experiment.add_scalar('metrics/val_iou_' + key, value, global_step=self.training_step_count)\n", "                print(f\"val_iou_{key}: {value}\")\n\t            self.metric_iou_val.reset()\n\t            scores = self.metric_panoptic_val.compute()\n\t            for key, value in scores.items():\n\t                for instance_name, score in zip(class_names, value):\n\t                    if instance_name != 'background':\n\t                        self.logger.experiment.add_scalar(f'metrics/val_{key}_{instance_name}', score.item(),\n\t                                                          global_step=self.training_step_count)\n\t                        print(f\"val_{key}_{instance_name}: {score.item()}\")\n\t                    # Log VPQ metric for the model checkpoint monitor \n", "                    if key == 'pq' and instance_name == 'dynamic':\n\t                        self.log('vpq', score.item())\n\t            self.metric_panoptic_val.reset()\n\t            print(\"========================== Runtime ==========================\")\n\t            perception_time = sum(self.perception_time) / (len(self.perception_time) + 1e-8)\n\t            prediction_time = sum(self.prediction_time) / (len(self.prediction_time) + 1e-8)\n\t            postprocessing_time = sum(self.postprocessing_time) / (len(self.postprocessing_time) + 1e-8)\n\t            print(f\"perception_time: {perception_time}\")\n\t            print(f\"prediction_time: {prediction_time}\")\n\t            print(f\"postprocessing_time: {postprocessing_time}\")\n", "            print(f\"total_time: {perception_time + prediction_time + postprocessing_time}\")\n\t            print(\"=============================================================\")\n\t            self.perception_time, self.prediction_time, self.postprocessing_time = [], [], []\n\t        self.logger.experiment.add_scalar('weights/segmentation_weight', 1 / (torch.exp(self.model.segmentation_weight)),\n\t                                          global_step=self.training_step_count)\n\t        self.logger.experiment.add_scalar('weights/flow_weight', 1 / (2 * torch.exp(self.model.flow_weight)),\n\t                                          global_step=self.training_step_count)\n\t    def training_epoch_end(self, step_outputs):\n\t        self.shared_epoch_end(step_outputs, True)\n\t    def validation_epoch_end(self, step_outputs):\n", "        self.shared_epoch_end(step_outputs, False)\n\t    def test_epoch_end(self, step_outputs):\n\t        self.shared_epoch_end(step_outputs, False)\n\t    def configure_optimizers(self):\n\t        params = self.model.parameters()\n\t        optimizer = torch.optim.Adam(\n\t            params, lr=self.cfg.OPTIMIZER.LR, weight_decay=self.cfg.OPTIMIZER.WEIGHT_DECAY\n\t        )\n\t        return optimizer"]}
{"filename": "powerbev/utils/instance.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\tfrom typing import Tuple\n\timport numpy as np\n\timport torch\n", "import torch.nn.functional as F\n\tfrom powerbev.utils.geometry import flow_warp\n\tfrom scipy.optimize import linear_sum_assignment\n\t# set ignore index to 0 for vis\n\tdef convert_instance_mask_to_center_and_offset_label(instance_img, num_instances, ignore_index=255, sigma=3):\n\t    seq_len, h, w = instance_img.shape\n\t    center_label = torch.zeros(seq_len, 1, h, w)\n\t    offset_label = ignore_index * torch.ones(seq_len, 2, h, w)\n\t    # x is vertical displacement, y is horizontal displacement\n\t    x, y = torch.meshgrid(torch.arange(h, dtype=torch.float), torch.arange(w, dtype=torch.float))\n", "    # Ignore id 0 which is the background\n\t    for instance_id in range(1, num_instances+1):\n\t        for t in range(seq_len):\n\t            instance_mask = (instance_img[t] == instance_id)\n\t            xc = x[instance_mask].mean().round().long()\n\t            yc = y[instance_mask].mean().round().long()\n\t            off_x = xc - x\n\t            off_y = yc - y\n\t            g = torch.exp(-(off_x ** 2 + off_y ** 2) / sigma ** 2)\n\t            center_label[t, 0] = torch.maximum(center_label[t, 0], g)\n", "            offset_label[t, 0, instance_mask] = off_x[instance_mask]\n\t            offset_label[t, 1, instance_mask] = off_y[instance_mask]\n\t    return center_label, offset_label\n\tdef find_instance_centers(center_prediction: torch.Tensor, conf_threshold: float = 0.1, nms_kernel_size: float = 3):\n\t    assert len(center_prediction.shape) == 3\n\t    center_prediction = F.threshold(center_prediction, threshold=conf_threshold, value=-1)\n\t    nms_padding = (nms_kernel_size - 1) // 2\n\t    maxpooled_center_prediction = F.max_pool2d(\n\t        center_prediction, kernel_size=nms_kernel_size, stride=1, padding=nms_padding\n\t    )\n", "    # Filter all elements that are not the maximum (i.e. the center of the heatmap instance)\n\t    center_prediction[center_prediction != maxpooled_center_prediction] = -1\n\t    return torch.nonzero(center_prediction > 0)[:, 1:]\n\tdef group_pixels(centers: torch.Tensor, offset_predictions: torch.Tensor) -> torch.Tensor:\n\t    width, height = offset_predictions.shape[-2:]\n\t    x_grid = (\n\t        torch.arange(width, dtype=offset_predictions.dtype, device=offset_predictions.device)\n\t        .view(1, width, 1)\n\t        .repeat(1, 1, height)\n\t    )\n", "    y_grid = (\n\t        torch.arange(height, dtype=offset_predictions.dtype, device=offset_predictions.device)\n\t        .view(1, 1, height)\n\t        .repeat(1, width, 1)\n\t    )\n\t    pixel_grid = torch.cat((x_grid, y_grid), dim=0)\n\t    offset = torch.stack([offset_predictions[1], offset_predictions[0]], dim=0)\n\t    center_locations = (pixel_grid + offset).view(2, width * height, 1).permute(2, 1, 0)\n\t    centers = centers.view(-1, 1, 2)\n\t    distances = torch.norm(centers - center_locations, dim=-1)\n", "    instance_id = torch.argmin(distances, dim=0).reshape(1, width, height) + 1\n\t    return instance_id\n\tdef get_instance_segmentation_and_centers(\n\t    center_predictions: torch.Tensor,\n\t    offset_predictions: torch.Tensor,\n\t    foreground_mask: torch.Tensor,\n\t    conf_threshold: float = 0.1,\n\t    nms_kernel_size: float = 5,\n\t    max_n_instance_centers: int = 100,\n\t) -> Tuple[torch.Tensor, torch.Tensor]:\n", "    width, height = offset_predictions.shape[-2:]\n\t    center_predictions = center_predictions.view(1, width, height)\n\t    offset_predictions = offset_predictions.view(2, width, height)\n\t    foreground_mask = foreground_mask.view(1, width, height)\n\t    centers = find_instance_centers(center_predictions, conf_threshold=conf_threshold, nms_kernel_size=nms_kernel_size)\n\t    if not len(centers):\n\t        return torch.zeros(center_predictions.shape, dtype=torch.int64, device=center_predictions.device)\n\t    if len(centers) > max_n_instance_centers:\n\t        # print(f'There are a lot of detected instance centers: {centers.shape}')\n\t        centers = centers[:max_n_instance_centers].clone()\n", "    instance_ids = group_pixels(centers, offset_predictions * foreground_mask.float())\n\t    instance_seg = (instance_ids * foreground_mask.float()).long()\n\t    # Make the indices of instance_seg consecutive\n\t    instance_seg = make_instance_seg_consecutive(instance_seg)\n\t    return instance_seg.long()\n\tdef update_instance_ids(instance_seg, old_ids, new_ids):\n\t    \"\"\"\n\t    Parameters\n\t    ----------\n\t        instance_seg: torch.Tensor arbitrary shape\n", "        old_ids: 1D tensor containing the list of old ids, must be all present in instance_seg.\n\t        new_ids: 1D tensor with the new ids, aligned with old_ids\n\t    Returns\n\t        new_instance_seg: torch.Tensor same shape as instance_seg with new ids\n\t    \"\"\"\n\t    indices = torch.arange(old_ids.max() + 1, device=instance_seg.device)\n\t    for old_id, new_id in zip(old_ids, new_ids):\n\t        indices[old_id] = new_id\n\t    return indices[instance_seg].long()\n\tdef make_instance_seg_consecutive(instance_seg):\n", "    # Make the indices of instance_seg consecutive\n\t    unique_ids = torch.unique(instance_seg)\n\t    new_ids = torch.arange(len(unique_ids), device=instance_seg.device)\n\t    instance_seg = update_instance_ids(instance_seg, unique_ids, new_ids)\n\t    return instance_seg\n\tdef make_instance_id_temporally_consecutive(pred_inst, preds, backward_flow, ignore_index=255.0):\n\t    assert pred_inst.shape[0] == 1, 'Assumes batch size = 1'\n\t    # Initialise instance segmentations with prediction corresponding to the present\n\t    consistent_instance_seg = [pred_inst[:, 0:1]]\n\t    backward_flow = backward_flow.clone().detach()\n", "    backward_flow[backward_flow == ignore_index] = 0.0\n\t    _, seq_len, _, h, w = preds.shape\n\t    for t in range(1, seq_len):\n\t        init_warped_instance_seg = flow_warp(consistent_instance_seg[-1].unsqueeze(2).float(), backward_flow[:, t:t+1]).squeeze(2).int()\n\t        warped_instance_seg = init_warped_instance_seg * preds[:, t:t+1, 0]\n\t        consistent_instance_seg.append(warped_instance_seg)\n\t    consistent_instance_seg = torch.cat(consistent_instance_seg, dim=1)\n\t    return consistent_instance_seg\n\tdef predict_instance_segmentation(output, compute_matched_centers=False,  vehicles_id=1, spatial_extent=[50, 50]):\n\t    preds = output['segmentation'].detach()\n", "    preds = torch.argmax(preds, dim=2, keepdims=True)\n\t    foreground_masks = preds.squeeze(2) == vehicles_id\n\t    batch_size, seq_len = preds.shape[:2]\n\t    pred_inst = []\n\t    for b in range(batch_size):\n\t        pred_inst_batch = get_instance_segmentation_and_centers(\n\t            torch.softmax(output['segmentation'], dim=2)[b, 0:1, vehicles_id].detach(),\n\t            output['instance_flow'][b, 1:2].detach(),\n\t            foreground_masks[b, 1:2].detach(),\n\t            nms_kernel_size=round(350/spatial_extent[0]),\n", "        )\n\t        pred_inst.append(pred_inst_batch)\n\t    pred_inst = torch.stack(pred_inst).squeeze(2)\n\t    if output['instance_flow'] is None:\n\t        print('Using zero flow because instance_future_output is None')\n\t        output['instance_flow'] = torch.zeros_like(output['instance_flow'])\n\t    consistent_instance_seg = []\n\t    for b in range(batch_size):\n\t        consistent_instance_seg.append(\n\t            make_instance_id_temporally_consecutive(\n", "                pred_inst[b:b+1],\n\t                preds[b:b+1, 1:],\n\t                output['instance_flow'][b:b+1, 1:].detach(),\n\t                )\n\t        )\n\t    consistent_instance_seg = torch.cat(consistent_instance_seg, dim=0)\n\t    consistent_instance_seg = torch.cat([torch.zeros_like(pred_inst), consistent_instance_seg], dim=1)\n\t    if compute_matched_centers:\n\t        assert batch_size == 1\n\t        # Generate trajectories\n", "        matched_centers = {}\n\t        _, seq_len, h, w = consistent_instance_seg.shape\n\t        grid = torch.stack(torch.meshgrid(\n\t            torch.arange(h, dtype=torch.float, device=preds.device),\n\t            torch.arange(w, dtype=torch.float, device=preds.device)\n\t        ))\n\t        for instance_id in torch.unique(consistent_instance_seg[0, 1])[1:].cpu().numpy():\n\t            for t in range(seq_len):\n\t                instance_mask = consistent_instance_seg[0, t] == instance_id\n\t                if instance_mask.sum() > 0:\n", "                    matched_centers[instance_id] = matched_centers.get(instance_id, []) + [\n\t                        grid[:, instance_mask].mean(dim=-1)]\n\t        for key, value in matched_centers.items():\n\t            matched_centers[key] = torch.stack(value).cpu().numpy()[:, ::-1]\n\t        return consistent_instance_seg, matched_centers\n\t    return consistent_instance_seg.long()\n"]}
{"filename": "powerbev/utils/network.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport torch\n\timport torch.nn as nn\n\timport torchvision\n", "def pack_sequence_dim(x):\n\t    b, s = x.shape[:2]\n\t    return x.view(b * s, *x.shape[2:])\n\tdef unpack_sequence_dim(x, b, s):\n\t    return x.view(b, s, *x.shape[1:])\n\tdef preprocess_batch(batch, device, unsqueeze=False):\n\t    for key, value in batch.items():\n\t        if key != 'sample_token':\n\t            batch[key] = value.to(device)\n\t            if unsqueeze:\n", "                batch[key] = batch[key].unsqueeze(0)\n\tdef set_module_grad(module, requires_grad=False):\n\t    for p in module.parameters():\n\t        p.requires_grad = requires_grad\n\tdef set_bn_momentum(model, momentum=0.1):\n\t    for m in model.modules():\n\t        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n\t            m.momentum = momentum\n\tclass NormalizeInverse(torchvision.transforms.Normalize):\n\t    #  https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/8\n", "    def __init__(self, mean, std):\n\t        mean = torch.as_tensor(mean)\n\t        std = torch.as_tensor(std)\n\t        std_inv = 1 / (std + 1e-7)\n\t        mean_inv = -mean * std_inv\n\t        super().__init__(mean=mean_inv, std=std_inv)\n\t    def __call__(self, tensor):\n\t        return super().__call__(tensor.clone())\n"]}
{"filename": "powerbev/utils/visualisation.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport matplotlib.pylab\n\timport numpy as np\n\timport torch\n", "from powerbev.utils.instance import predict_instance_segmentation\n\tDEFAULT_COLORMAP = matplotlib.pylab.cm.jet\n\tdef flow_to_image(flow: np.ndarray, autoscale: bool = False) -> np.ndarray:\n\t    \"\"\"\n\t    Applies colour map to flow which should be a 2 channel image tensor HxWx2. Returns a HxWx3 numpy image\n\t    Code adapted from: https://github.com/liruoteng/FlowNet/blob/master/models/flownet/scripts/flowlib.py\n\t    \"\"\"\n\t    u = flow[0, :, :]\n\t    v = flow[1, :, :]\n\t    # Convert to polar coordinates\n", "    rad = np.sqrt(u ** 2 + v ** 2)\n\t    maxrad = np.max(rad)\n\t    # Normalise flow maps\n\t    if autoscale:\n\t        u /= maxrad + np.finfo(float).eps\n\t        v /= maxrad + np.finfo(float).eps\n\t    # visualise flow with cmap\n\t    return np.uint8(compute_color(u, v) * 255)\n\tdef _normalise(image: np.ndarray) -> np.ndarray:\n\t    lower = np.min(image)\n", "    delta = np.max(image) - lower\n\t    if delta == 0:\n\t        delta = 1\n\t    image = (image.astype(np.float32) - lower) / delta\n\t    return image\n\tdef apply_colour_map(\n\t    image: np.ndarray, cmap: matplotlib.colors.LinearSegmentedColormap = DEFAULT_COLORMAP, autoscale: bool = False\n\t) -> np.ndarray:\n\t    \"\"\"\n\t    Applies a colour map to the given 1 or 2 channel numpy image. if 2 channel, must be 2xHxW.\n", "    Returns a HxWx3 numpy image\n\t    \"\"\"\n\t    if image.ndim == 2 or (image.ndim == 3 and image.shape[0] == 1):\n\t        if image.ndim == 3:\n\t            image = image[0]\n\t        # grayscale scalar image\n\t        if autoscale:\n\t            image = _normalise(image)\n\t        return cmap(image)[:, :, :3]\n\t    if image.shape[0] == 2:\n", "        # 2 dimensional UV\n\t        return flow_to_image(image, autoscale=autoscale)\n\t    if image.shape[0] == 3:\n\t        # normalise rgb channels\n\t        if autoscale:\n\t            image = _normalise(image)\n\t        return np.transpose(image, axes=[1, 2, 0])\n\t    raise Exception('Image must be 1, 2 or 3 channel to convert to colour_map (CxHxW)')\n\tdef heatmap_image(\n\t    image: np.ndarray, cmap: matplotlib.colors.LinearSegmentedColormap = DEFAULT_COLORMAP, autoscale: bool = True\n", ") -> np.ndarray:\n\t    \"\"\"Colorize an 1 or 2 channel image with a colourmap.\"\"\"\n\t    if not issubclass(image.dtype.type, np.floating):\n\t        raise ValueError(f\"Expected a ndarray of float type, but got dtype {image.dtype}\")\n\t    if not (image.ndim == 2 or (image.ndim == 3 and image.shape[0] in [1, 2])):\n\t        raise ValueError(f\"Expected a ndarray of shape [H, W] or [1, H, W] or [2, H, W], but got shape {image.shape}\")\n\t    heatmap_np = apply_colour_map(image, cmap=cmap, autoscale=autoscale)\n\t    heatmap_np = np.uint8(heatmap_np * 255)\n\t    return heatmap_np\n\tdef compute_color(u: np.ndarray, v: np.ndarray) -> np.ndarray:\n", "    assert u.shape == v.shape\n\t    [h, w] = u.shape\n\t    img = np.zeros([h, w, 3])\n\t    nan_mask = np.isnan(u) | np.isnan(v)\n\t    u[nan_mask] = 0\n\t    v[nan_mask] = 0\n\t    colorwheel = make_color_wheel()\n\t    ncols = np.size(colorwheel, 0)\n\t    rad = np.sqrt(u ** 2 + v ** 2)\n\t    a = np.arctan2(-v, -u) / np.pi\n", "    f_k = (a + 1) / 2 * (ncols - 1) + 1\n\t    k_0 = np.floor(f_k).astype(int)\n\t    k_1 = k_0 + 1\n\t    k_1[k_1 == ncols + 1] = 1\n\t    f = f_k - k_0\n\t    for i in range(0, np.size(colorwheel, 1)):\n\t        tmp = colorwheel[:, i]\n\t        col0 = tmp[k_0 - 1] / 255\n\t        col1 = tmp[k_1 - 1] / 255\n\t        col = (1 - f) * col0 + f * col1\n", "        idx = rad <= 1\n\t        col[idx] = 1 - rad[idx] * (1 - col[idx])\n\t        notidx = np.logical_not(idx)\n\t        col[notidx] *= 0.75\n\t        img[:, :, i] = col * (1 - nan_mask)\n\t    return img\n\tdef make_color_wheel() -> np.ndarray:\n\t    \"\"\"\n\t    Create colour wheel.\n\t    Code adapted from https://github.com/liruoteng/FlowNet/blob/master/models/flownet/scripts/flowlib.py\n", "    \"\"\"\n\t    red_yellow = 15\n\t    yellow_green = 6\n\t    green_cyan = 4\n\t    cyan_blue = 11\n\t    blue_magenta = 13\n\t    magenta_red = 6\n\t    ncols = red_yellow + yellow_green + green_cyan + cyan_blue + blue_magenta + magenta_red\n\t    colorwheel = np.zeros([ncols, 3])\n\t    col = 0\n", "    # red_yellow\n\t    colorwheel[0:red_yellow, 0] = 255\n\t    colorwheel[0:red_yellow, 1] = np.transpose(np.floor(255 * np.arange(0, red_yellow) / red_yellow))\n\t    col += red_yellow\n\t    # yellow_green\n\t    colorwheel[col : col + yellow_green, 0] = 255 - np.transpose(\n\t        np.floor(255 * np.arange(0, yellow_green) / yellow_green)\n\t    )\n\t    colorwheel[col : col + yellow_green, 1] = 255\n\t    col += yellow_green\n", "    # green_cyan\n\t    colorwheel[col : col + green_cyan, 1] = 255\n\t    colorwheel[col : col + green_cyan, 2] = np.transpose(np.floor(255 * np.arange(0, green_cyan) / green_cyan))\n\t    col += green_cyan\n\t    # cyan_blue\n\t    colorwheel[col : col + cyan_blue, 1] = 255 - np.transpose(np.floor(255 * np.arange(0, cyan_blue) / cyan_blue))\n\t    colorwheel[col : col + cyan_blue, 2] = 255\n\t    col += cyan_blue\n\t    # blue_magenta\n\t    colorwheel[col : col + blue_magenta, 2] = 255\n", "    colorwheel[col : col + blue_magenta, 0] = np.transpose(np.floor(255 * np.arange(0, blue_magenta) / blue_magenta))\n\t    col += +blue_magenta\n\t    # magenta_red\n\t    colorwheel[col : col + magenta_red, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, magenta_red) / magenta_red))\n\t    colorwheel[col : col + magenta_red, 0] = 255\n\t    return colorwheel\n\tdef make_contour(img, colour=[0, 0, 0], double_line=False):\n\t    h, w = img.shape[:2]\n\t    out = img.copy()\n\t    # Vertical lines\n", "    out[np.arange(h), np.repeat(0, h)] = colour\n\t    out[np.arange(h), np.repeat(w - 1, h)] = colour\n\t    # Horizontal lines\n\t    out[np.repeat(0, w), np.arange(w)] = colour\n\t    out[np.repeat(h - 1, w), np.arange(w)] = colour\n\t    if double_line:\n\t        out[np.arange(h), np.repeat(1, h)] = colour\n\t        out[np.arange(h), np.repeat(w - 2, h)] = colour\n\t        # Horizontal lines\n\t        out[np.repeat(1, w), np.arange(w)] = colour\n", "        out[np.repeat(h - 2, w), np.arange(w)] = colour\n\t    return out\n\tdef plot_instance_map(instance_image, instance_map, instance_colours=None, bg_image=None):\n\t    if isinstance(instance_image, torch.Tensor):\n\t        instance_image = instance_image.cpu().numpy()\n\t    assert isinstance(instance_image, np.ndarray)\n\t    if instance_colours is None:\n\t        instance_colours = generate_instance_colours(instance_map)\n\t    if len(instance_image.shape) > 2:\n\t        instance_image = instance_image.reshape((instance_image.shape[-2], instance_image.shape[-1]))\n", "    if bg_image is None:\n\t        plot_image = 255 * np.ones((instance_image.shape[0], instance_image.shape[1], 3), dtype=np.uint8)\n\t    else:\n\t        plot_image = bg_image\n\t    for key, value in instance_colours.items():\n\t        plot_image[instance_image == key] = value\n\t    return plot_image\n\tdef visualise_output(labels, output, cfg):\n\t    semantic_colours = np.array([[255, 255, 255], [0, 0, 0]], dtype=np.uint8)\n\t    consistent_instance_seg = predict_instance_segmentation(output, spatial_extent=(cfg.LIFT.X_BOUND[1], cfg.LIFT.Y_BOUND[1]))\n", "    sequence_length = consistent_instance_seg.shape[1]\n\t    b = 0\n\t    video = []\n\t    for t in range(1, sequence_length):\n\t        out_t = []\n\t        # Ground truth\n\t        unique_ids = torch.unique(labels['instance'][b, t]).cpu().numpy()[1:]\n\t        instance_map = dict(zip(unique_ids, unique_ids))\n\t        instance_plot = plot_instance_map(labels['instance'][b, t].cpu(), instance_map)[::-1, ::-1]\n\t        instance_plot = make_contour(instance_plot)\n", "        semantic_seg = labels['segmentation'].squeeze(2).cpu().numpy()\n\t        semantic_plot = semantic_colours[semantic_seg[b, t][::-1, ::-1]]\n\t        semantic_plot = make_contour(semantic_plot)\n\t        future_flow_plot = labels['flow'][b, t].cpu().numpy()\n\t        future_flow_plot[:, semantic_seg[b, t] != 1] = 0\n\t        future_flow_plot = flow_to_image(future_flow_plot)[::-1, ::-1]\n\t        future_flow_plot = make_contour(future_flow_plot)\n\t        out_t.append(np.concatenate([instance_plot, future_flow_plot, semantic_plot], axis=0))\n\t        # Predictions\n\t        unique_ids = torch.unique(consistent_instance_seg[b, t]).cpu().numpy()[1:]\n", "        instance_map = dict(zip(unique_ids, unique_ids))\n\t        instance_plot = plot_instance_map(consistent_instance_seg[b, t].cpu(), instance_map)[::-1, ::-1]\n\t        instance_plot = make_contour(instance_plot)\n\t        semantic_seg = torch.argmax(output['segmentation'], dim=2, keepdims=True)\n\t        semantic_seg = semantic_seg.squeeze(2).detach().cpu().numpy()\n\t        semantic_plot = semantic_colours[semantic_seg[b, t][::-1, ::-1]]\n\t        semantic_plot = make_contour(semantic_plot)\n\t        future_flow_plot = output['instance_flow'][b, t].detach().cpu().numpy()\n\t        future_flow_plot[:, semantic_seg[b, t] != 1] = 0\n\t        future_flow_plot = flow_to_image(future_flow_plot)[::-1, ::-1]\n", "        future_flow_plot = make_contour(future_flow_plot)\n\t        out_t.append(np.concatenate([instance_plot, future_flow_plot, semantic_plot], axis=0))\n\t        out_t = np.concatenate(out_t, axis=1)\n\t        # Shape (C, H, W)\n\t        out_t = out_t.transpose((2, 0, 1))\n\t        video.append(out_t)\n\t    # Shape (B, T, C, H, W)\n\t    video = np.stack(video)[None]\n\t    return video\n\tdef convert_figure_numpy(figure):\n", "    \"\"\" Convert figure to numpy image \"\"\"\n\t    figure_np = np.frombuffer(figure.canvas.tostring_rgb(), dtype=np.uint8)\n\t    figure_np = figure_np.reshape(figure.canvas.get_width_height()[::-1] + (3,))\n\t    return figure_np\n\tdef generate_instance_colours(instance_map):\n\t    # Most distinct 22 colors (kelly colors from https://stackoverflow.com/questions/470690/how-to-automatically-generate\n\t    # -n-distinct-colors)\n\t    # plus some colours from AD40k\n\t    INSTANCE_COLOURS = np.asarray([\n\t        [0, 0, 0],\n", "        [255, 179, 0],\n\t        [128, 62, 117],\n\t        [255, 104, 0],\n\t        [166, 189, 215],\n\t        [193, 0, 32],\n\t        [206, 162, 98],\n\t        [129, 112, 102],\n\t        [0, 125, 52],\n\t        [246, 118, 142],\n\t        [0, 83, 138],\n", "        [255, 122, 92],\n\t        [83, 55, 122],\n\t        [255, 142, 0],\n\t        [179, 40, 81],\n\t        [244, 200, 0],\n\t        [127, 24, 13],\n\t        [147, 170, 0],\n\t        [89, 51, 21],\n\t        [241, 58, 19],\n\t        [35, 44, 22],\n", "        [112, 224, 255],\n\t        [70, 184, 160],\n\t        [153, 0, 255],\n\t        [71, 255, 0],\n\t        [255, 0, 163],\n\t        [255, 204, 0],\n\t        [0, 255, 235],\n\t        [255, 0, 235],\n\t        [255, 0, 122],\n\t        [255, 245, 0],\n", "        [10, 190, 212],\n\t        [214, 255, 0],\n\t        [0, 204, 255],\n\t        [20, 0, 255],\n\t        [255, 255, 0],\n\t        [0, 153, 255],\n\t        [0, 255, 204],\n\t        [41, 255, 0],\n\t        [173, 0, 255],\n\t        [0, 245, 255],\n", "        [71, 0, 255],\n\t        [0, 255, 184],\n\t        [0, 92, 255],\n\t        [184, 255, 0],\n\t        [255, 214, 0],\n\t        [25, 194, 194],\n\t        [92, 0, 255],\n\t        [220, 220, 220],\n\t        [255, 9, 92],\n\t        [112, 9, 255],\n", "        [8, 255, 214],\n\t        [255, 184, 6],\n\t        [10, 255, 71],\n\t        [255, 41, 10],\n\t        [7, 255, 255],\n\t        [224, 255, 8],\n\t        [102, 8, 255],\n\t        [255, 61, 6],\n\t        [255, 194, 7],\n\t        [0, 255, 20],\n", "        [255, 8, 41],\n\t        [255, 5, 153],\n\t        [6, 51, 255],\n\t        [235, 12, 255],\n\t        [160, 150, 20],\n\t        [0, 163, 255],\n\t        [140, 140, 140],\n\t        [250, 10, 15],\n\t        [20, 255, 0],\n\t    ])\n", "    return {instance_id: INSTANCE_COLOURS[global_instance_id % len(INSTANCE_COLOURS)] for\n\t            instance_id, global_instance_id in instance_map.items()\n\t            }\n"]}
{"filename": "powerbev/utils/lyft_splits.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\tTRAIN_LYFT_INDICES = [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16,\n\t                      17, 18, 19, 20, 21, 23, 24, 27, 28, 29, 30, 31, 32,\n\t                      33, 35, 36, 37, 39, 41, 43, 44, 45, 46, 47, 48, 49,\n", "                      50, 51, 52, 53, 55, 56, 59, 60, 62, 63, 65, 68, 69,\n\t                      70, 71, 72, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84,\n\t                      86, 87, 88, 89, 93, 95, 97, 98, 99, 103, 104, 107, 108,\n\t                      109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 121, 122, 124,\n\t                      127, 128, 130, 131, 132, 134, 135, 136, 137, 138, 139, 143, 144,\n\t                      146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159,\n\t                      161, 162, 165, 166, 167, 171, 172, 173, 174, 175, 176, 177, 178,\n\t                      179]\n\tVAL_LYFT_INDICES = [0, 2, 4, 13, 22, 25, 26, 34, 38, 40, 42, 54, 57,\n\t                    58, 61, 64, 66, 67, 77, 80, 85, 90, 91, 92, 94, 96,\n", "                    100, 101, 102, 105, 106, 112, 120, 123, 125, 126, 129, 133, 140,\n\t                    141, 142, 145, 155, 160, 163, 164, 168, 169, 170]\n"]}
{"filename": "powerbev/utils/geometry.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport numpy as np\n\timport PIL\n\timport torch\n", "import torch.nn.functional as F\n\tfrom pyquaternion import Quaternion\n\tdef resize_and_crop_image(img, resize_dims, crop):\n\t    # Bilinear resizing followed by cropping\n\t    img = img.resize(resize_dims, resample=PIL.Image.BILINEAR)\n\t    img = img.crop(crop)\n\t    return img\n\tdef update_intrinsics(intrinsics, top_crop=0.0, left_crop=0.0, scale_width=1.0, scale_height=1.0):\n\t    \"\"\"\n\t    Parameters\n", "    ----------\n\t        intrinsics: torch.Tensor (3, 3)\n\t        top_crop: float\n\t        left_crop: float\n\t        scale_width: float\n\t        scale_height: float\n\t    \"\"\"\n\t    updated_intrinsics = intrinsics.clone()\n\t    # Adjust intrinsics scale due to resizing\n\t    updated_intrinsics[0, 0] *= scale_width\n", "    updated_intrinsics[0, 2] *= scale_width\n\t    updated_intrinsics[1, 1] *= scale_height\n\t    updated_intrinsics[1, 2] *= scale_height\n\t    # Adjust principal point due to cropping\n\t    updated_intrinsics[0, 2] -= left_crop\n\t    updated_intrinsics[1, 2] -= top_crop\n\t    return updated_intrinsics\n\tdef calculate_birds_eye_view_parameters(x_bounds, y_bounds, z_bounds):\n\t    \"\"\"\n\t    Parameters\n", "    ----------\n\t        x_bounds: Forward direction in the ego-car.\n\t        y_bounds: Sides\n\t        z_bounds: Height\n\t    Returns\n\t    -------\n\t        bev_resolution: Bird's-eye view bev_resolution\n\t        bev_start_position Bird's-eye view first element\n\t        bev_dimension Bird's-eye view tensor spatial dimension\n\t    \"\"\"\n", "    bev_resolution = torch.tensor([row[2] for row in [x_bounds, y_bounds, z_bounds]])\n\t    bev_start_position = torch.tensor([row[0] + row[2] / 2.0 for row in [x_bounds, y_bounds, z_bounds]])\n\t    bev_dimension = torch.tensor([(row[1] - row[0]) / row[2] for row in [x_bounds, y_bounds, z_bounds]],\n\t                                 dtype=torch.long)\n\t    return bev_resolution, bev_start_position, bev_dimension\n\tdef convert_egopose_to_matrix_numpy(egopose):\n\t    transformation_matrix = np.zeros((4, 4), dtype=np.float32)\n\t    rotation = Quaternion(egopose['rotation']).rotation_matrix\n\t    translation = np.array(egopose['translation'])\n\t    transformation_matrix[:3, :3] = rotation\n", "    transformation_matrix[:3, 3] = translation\n\t    transformation_matrix[3, 3] = 1.0\n\t    return transformation_matrix\n\tdef invert_matrix_egopose_numpy(egopose):\n\t    \"\"\" Compute the inverse transformation of a 4x4 egopose numpy matrix.\"\"\"\n\t    inverse_matrix = np.zeros((4, 4), dtype=np.float32)\n\t    rotation = egopose[:3, :3]\n\t    translation = egopose[:3, 3]\n\t    inverse_matrix[:3, :3] = rotation.T\n\t    inverse_matrix[:3, 3] = -np.dot(rotation.T, translation)\n", "    inverse_matrix[3, 3] = 1.0\n\t    return inverse_matrix\n\tdef mat2pose_vec(matrix: torch.Tensor):\n\t    \"\"\"\n\t    Converts a 4x4 pose matrix into a 6-dof pose vector\n\t    Args:\n\t        matrix (ndarray): 4x4 pose matrix\n\t    Returns:\n\t        vector (ndarray): 6-dof pose vector comprising translation components (tx, ty, tz) and\n\t        rotation components (rx, ry, rz)\n", "    \"\"\"\n\t    # M[1, 2] = -sinx*cosy, M[2, 2] = +cosx*cosy\n\t    rotx = torch.atan2(-matrix[..., 1, 2], matrix[..., 2, 2])\n\t    # M[0, 2] = +siny, M[1, 2] = -sinx*cosy, M[2, 2] = +cosx*cosy\n\t    cosy = torch.sqrt(matrix[..., 1, 2] ** 2 + matrix[..., 2, 2] ** 2)\n\t    roty = torch.atan2(matrix[..., 0, 2], cosy)\n\t    # M[0, 0] = +cosy*cosz, M[0, 1] = -cosy*sinz\n\t    rotz = torch.atan2(-matrix[..., 0, 1], matrix[..., 0, 0])\n\t    rotation = torch.stack((rotx, roty, rotz), dim=-1)\n\t    # Extract translation params\n", "    translation = matrix[..., :3, 3]\n\t    return torch.cat((translation, rotation), dim=-1)\n\tdef euler2mat(angle: torch.Tensor):\n\t    \"\"\"Convert euler angles to rotation matrix.\n\t    Reference: https://github.com/pulkitag/pycaffe-utils/blob/master/rot_utils.py#L174\n\t    Args:\n\t        angle: rotation angle along 3 axis (in radians) [Bx3]\n\t    Returns:\n\t        Rotation matrix corresponding to the euler angles [Bx3x3]\n\t    \"\"\"\n", "    shape = angle.shape\n\t    angle = angle.view(-1, 3)\n\t    x, y, z = angle[:, 0], angle[:, 1], angle[:, 2]\n\t    cosz = torch.cos(z)\n\t    sinz = torch.sin(z)\n\t    zeros = torch.zeros_like(z)\n\t    ones = torch.ones_like(z)\n\t    zmat = torch.stack([cosz, -sinz, zeros, sinz, cosz, zeros, zeros, zeros, ones], dim=1).view(-1, 3, 3)\n\t    cosy = torch.cos(y)\n\t    siny = torch.sin(y)\n", "    ymat = torch.stack([cosy, zeros, siny, zeros, ones, zeros, -siny, zeros, cosy], dim=1).view(-1, 3, 3)\n\t    cosx = torch.cos(x)\n\t    sinx = torch.sin(x)\n\t    xmat = torch.stack([ones, zeros, zeros, zeros, cosx, -sinx, zeros, sinx, cosx], dim=1).view(-1, 3, 3)\n\t    rot_mat = xmat.bmm(ymat).bmm(zmat)\n\t    rot_mat = rot_mat.view(*shape[:-1], 3, 3)\n\t    return rot_mat\n\tdef pose_vec2mat(vec: torch.Tensor):\n\t    \"\"\"\n\t    Convert 6DoF parameters to transformation matrix.\n", "    Args:\n\t        vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz [B,6]\n\t    Returns:\n\t        A transformation matrix [B,4,4]\n\t    \"\"\"\n\t    translation = vec[..., :3].unsqueeze(-1)  # [...x3x1]\n\t    rot = vec[..., 3:].contiguous()  # [...x3]\n\t    rot_mat = euler2mat(rot)  # [...,3,3]\n\t    transform_mat = torch.cat([rot_mat, translation], dim=-1)  # [...,3,4]\n\t    transform_mat = torch.nn.functional.pad(transform_mat, [0, 0, 0, 1], value=0)  # [...,4,4]\n", "    transform_mat[..., 3, 3] = 1.0\n\t    return transform_mat\n\tdef invert_pose_matrix(x):\n\t    \"\"\"\n\t    Parameters\n\t    ----------\n\t        x: [B, 4, 4] batch of pose matrices\n\t    Returns\n\t    -------\n\t        out: [B, 4, 4] batch of inverse pose matrices\n", "    \"\"\"\n\t    assert len(x.shape) == 3 and x.shape[1:] == (4, 4), 'Only works for batch of pose matrices.'\n\t    transposed_rotation = torch.transpose(x[:, :3, :3], 1, 2)\n\t    translation = x[:, :3, 3:]\n\t    inverse_mat = torch.cat([transposed_rotation, -torch.bmm(transposed_rotation, translation)], dim=-1) # [B,3,4]\n\t    inverse_mat = torch.nn.functional.pad(inverse_mat, [0, 0, 0, 1], value=0)  # [B,4,4]\n\t    inverse_mat[..., 3, 3] = 1.0\n\t    return inverse_mat\n\tdef warp_features(x, flow, mode='nearest', spatial_extent=None):\n\t    \"\"\" Applies a rotation and translation to feature map x.\n", "        Args:\n\t            x: (b, c, h, w) feature map\n\t            flow: (b, 6) 6DoF vector (only uses the xy poriton)\n\t            mode: use 'nearest' when dealing with categorical inputs\n\t        Returns:\n\t            in plane transformed feature map\n\t        \"\"\"\n\t    if flow is None:\n\t        return x\n\t    b, c, h, w = x.shape\n", "    # z-rotation\n\t    angle = flow[:, 5].clone()  # torch.atan2(flow[:, 1, 0], flow[:, 0, 0])\n\t    # x-y translation\n\t    translation = flow[:, :2].clone()  # flow[:, :2, 3]\n\t    # Normalise translation. Need to divide by how many meters is half of the image.\n\t    # because translation of 1.0 correspond to translation of half of the image.\n\t    translation[:, 0] /= spatial_extent[0]\n\t    translation[:, 1] /= spatial_extent[1]\n\t    # forward axis is inverted\n\t    translation[:, 0] *= -1\n", "    cos_theta = torch.cos(angle)\n\t    sin_theta = torch.sin(angle)\n\t    # output = Rot.input + translation\n\t    # tx and ty are inverted as is the case when going from real coordinates to numpy coordinates\n\t    # translation_pos_0 -> positive value makes the image move to the left\n\t    # translation_pos_1 -> positive value makes the image move to the top\n\t    # Angle -> positive value in rad makes the image move in the trigonometric way\n\t    transformation = torch.stack([cos_theta, -sin_theta, translation[:, 1],\n\t                                  sin_theta, cos_theta, translation[:, 0]], dim=-1).view(b, 2, 3)\n\t    # Note that a rotation will preserve distances only if height = width. Otherwise there's\n", "    # resizing going on. e.g. rotation of pi/2 of a 100x200 image will make what's in the center of the image\n\t    # elongated.\n\t    grid = torch.nn.functional.affine_grid(transformation, size=x.shape, align_corners=False)\n\t    warped_x = torch.nn.functional.grid_sample(x, grid.float(), mode=mode, padding_mode='zeros', align_corners=False)\n\t    return warped_x\n\tdef cumulative_warp_features(x, flow, mode='nearest', spatial_extent=None):\n\t    \"\"\" Warps a sequence of feature maps by accumulating incremental 2d flow.\n\t    x[:, -1] remains unchanged\n\t    x[:, -2] is warped using flow[:, -2]\n\t    x[:, -3] is warped using flow[:, -3] @ flow[:, -2]\n", "    ...\n\t    x[:, 0] is warped using flow[:, 0] @ ... @ flow[:, -3] @ flow[:, -2]\n\t    Args:\n\t        x: (b, t, c, h, w) sequence of feature maps\n\t        flow: (b, t, 6) sequence of 6 DoF pose\n\t            from t to t+1 (only uses the xy poriton)\n\t    \"\"\"\n\t    sequence_length = x.shape[1]\n\t    if sequence_length == 1:\n\t        return x\n", "    flow = pose_vec2mat(flow)\n\t    out = [x[:, -1]]\n\t    cum_flow = flow[:, -2]\n\t    for t in reversed(range(sequence_length - 1)):\n\t        out.append(warp_features(x[:, t], mat2pose_vec(cum_flow), mode=mode, spatial_extent=spatial_extent))\n\t        # @ is the equivalent of torch.bmm\n\t        cum_flow = flow[:, t - 1] @ cum_flow\n\t    return torch.stack(out[::-1], 1)\n\tdef cumulative_warp_features_reverse(x, flow, mode='nearest', spatial_extent=None):\n\t    \"\"\" Warps a sequence of feature maps by accumulating incremental 2d flow.\n", "    x[:, 0] remains unchanged\n\t    x[:, 1] is warped using flow[:, 0].inverse()\n\t    x[:, 2] is warped using flow[:, 0].inverse() @ flow[:, 1].inverse()\n\t    ...\n\t    Args:\n\t        x: (b, t, c, h, w) sequence of feature maps\n\t        flow: (b, t, 6) sequence of 6 DoF pose\n\t            from t to t+1 (only uses the xy poriton)\n\t    \"\"\"\n\t    flow = pose_vec2mat(flow)\n", "    out = [x[:,0]]\n\t    for i in range(1, x.shape[1]):\n\t        if i==1:\n\t            cum_flow = invert_pose_matrix(flow[:, 0])\n\t        else:\n\t            cum_flow = cum_flow @ invert_pose_matrix(flow[:,i-1])\n\t        out.append( warp_features(x[:,i], mat2pose_vec(cum_flow), mode, spatial_extent=spatial_extent))\n\t    return torch.stack(out, 1)\n\tdef flow_warp(occupancy, flow, mode='nearest', padding_mode='zeros'):\n\t    \"\"\"Warps ground-truth flow-origin occupancies according to predicted flows.\n", "    Performs bilinear interpolation and samples from 4 pixels for each flow\n\t    vector.\n\t    Args:\n\t      occupancy: occupancy as float32 tensors with the shape of BxTx1xHxW\n\t      flow: flow as float32 tensors with the shape of BxTx2xHxW\n\t      mode: mode of grid sample\n\t    Returns:\n\t      warped_occupancy: occupancy grids for vehicles as float32 tensors with the shape of BxTx1xHxW.\n\t    Note: the flow must always be 1 timestep ahead of the corresponding occupancy\n\t    \"\"\"\n", "    _, num_waypoints, _, grid_height_cells, grid_width_cells = occupancy.size()\n\t    h = torch.linspace(-1, 1, steps=grid_height_cells)\n\t    w = torch.linspace(-1, 1, steps=grid_width_cells)\n\t    h_idx, w_idx = torch.meshgrid(h, w)\n\t    # These indices map each (x, y) location to the pixel (x, y).\n\t    identity_indices = torch.stack((w_idx, h_idx), dim=0).to(device=occupancy.device)  # 2xHxW, storing x, y coordinates.\n\t    warped_occupancy = []\n\t    for k in range(num_waypoints):\n\t        flow_origin_occupancy = occupancy[:, k]  # BxTx1xHxW -> Bx1xHxW\n\t        pred_flow = flow[:, k]  # BxTx2xHxW -> Bx2xHxW\n", "        # Normalize along the width and height direction\n\t        normalize_pred_flow = torch.stack(\n\t            (2.0 * pred_flow[:, 0] / (grid_width_cells - 1),  \n\t            2.0 * pred_flow[:, 1] / (grid_height_cells - 1)),\n\t            dim=1,\n\t        )\n\t        # Shift the identity grid indices according to predicted flow tells us\n\t        # the source (origin) grid cell for each flow vector. We simply sample\n\t        # occupancy values from these locations.\n\t        warped_indices = identity_indices + normalize_pred_flow  # Bx2xHxW\n", "        warped_indices = warped_indices.permute(0, 2, 3, 1)  # Bx2xHxW -> BxHxWx2\n\t        sampled_occupancy = F.grid_sample(\n\t            input=flow_origin_occupancy,\n\t            grid=warped_indices,\n\t            mode=mode,\n\t            padding_mode='zeros',\n\t            align_corners=True,\n\t        )\n\t        warped_occupancy.append(sampled_occupancy)\n\t    warped_occupancy = torch.stack(warped_occupancy, dim=1)\n", "    return warped_occupancy\n\tclass VoxelsSumming(torch.autograd.Function):\n\t    \"\"\"Adapted from https://github.com/nv-tlabs/lift-splat-shoot/blob/master/src/tools.py#L193\"\"\"\n\t    @staticmethod\n\t    def forward(ctx, x, geometry, ranks):\n\t        \"\"\"The features `x` and `geometry` are ranked by voxel positions.\"\"\"\n\t        # Cumulative sum of all features.\n\t        x = x.cumsum(0)\n\t        # Indicates the change of voxel.\n\t        mask = torch.ones(x.shape[0], device=x.device, dtype=torch.bool)\n", "        mask[:-1] = ranks[1:] != ranks[:-1]\n\t        x, geometry = x[mask], geometry[mask]\n\t        # Calculate sum of features within a voxel.\n\t        x = torch.cat((x[:1], x[1:] - x[:-1]))\n\t        ctx.save_for_backward(mask)\n\t        ctx.mark_non_differentiable(geometry)\n\t        return x, geometry\n\t    @staticmethod\n\t    def backward(ctx, grad_x, grad_geometry):\n\t        (mask,) = ctx.saved_tensors\n", "        # Since the operation is summing, we simply need to send gradient\n\t        # to all elements that were part of the summation process.\n\t        indices = torch.cumsum(mask, 0)\n\t        indices[mask] -= 1\n\t        output_grad = grad_x[indices]\n\t        return output_grad, None, None\n"]}
{"filename": "powerbev/layers/temporal.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\tfrom collections import OrderedDict\n\timport torch\n\timport torch.nn as nn\n", "from powerbev.layers.convolutions import ConvBlock\n\tfrom powerbev.utils.geometry import warp_features\n\tclass SpatialGRU(nn.Module):\n\t    \"\"\"A GRU cell that takes an input tensor [BxTxCxHxW] and an optional previous state and passes a\n\t    convolutional gated recurrent unit over the data\"\"\"\n\t    def __init__(self, input_size, hidden_size, gru_bias_init=0.0, norm='bn', activation='relu'):\n\t        super().__init__()\n\t        self.input_size = input_size\n\t        self.hidden_size = hidden_size\n\t        self.gru_bias_init = gru_bias_init\n", "        self.conv_update = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, bias=True, padding=1)\n\t        self.conv_reset = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, bias=True, padding=1)\n\t        self.conv_state_tilde = ConvBlock(\n\t            input_size + hidden_size, hidden_size, kernel_size=3, bias=False, norm=norm, activation=activation\n\t        )\n\t    def forward(self, x, state=None, flow=None, mode='bilinear'):\n\t        # pylint: disable=unused-argument, arguments-differ\n\t        # Check size\n\t        assert len(x.size()) == 5, 'Input tensor must be BxTxCxHxW.'\n\t        b, timesteps, c, h, w = x.size()\n", "        assert c == self.input_size, f'feature sizes must match, got input {c} for layer with size {self.input_size}'\n\t        # recurrent layers\n\t        rnn_output = []\n\t        rnn_state = torch.zeros(b, self.hidden_size, h, w, device=x.device) if state is None else state\n\t        for t in range(timesteps):\n\t            x_t = x[:, t]\n\t            if flow is not None:\n\t                rnn_state = warp_features(rnn_state, flow[:, t], mode=mode)\n\t            # propagate rnn state\n\t            rnn_state = self.gru_cell(x_t, rnn_state)\n", "            rnn_output.append(rnn_state)\n\t        # reshape rnn output to batch tensor\n\t        return torch.stack(rnn_output, dim=1)\n\t    def gru_cell(self, x, state):\n\t        # Compute gates\n\t        x_and_state = torch.cat([x, state], dim=1)\n\t        update_gate = self.conv_update(x_and_state)\n\t        reset_gate = self.conv_reset(x_and_state)\n\t        # Add bias to initialise gate as close to identity function\n\t        update_gate = torch.sigmoid(update_gate + self.gru_bias_init)\n", "        reset_gate = torch.sigmoid(reset_gate + self.gru_bias_init)\n\t        # Compute proposal state, activation is defined in norm_act_config (can be tanh, ReLU etc)\n\t        state_tilde = self.conv_state_tilde(torch.cat([x, (1.0 - reset_gate) * state], dim=1))\n\t        output = (1.0 - update_gate) * state + update_gate * state_tilde\n\t        return output\n\tclass CausalConv3d(nn.Module):\n\t    def __init__(self, in_channels, out_channels, kernel_size=(2, 3, 3), dilation=(1, 1, 1), bias=False):\n\t        super().__init__()\n\t        assert len(kernel_size) == 3, 'kernel_size must be a 3-tuple.'\n\t        time_pad = (kernel_size[0] - 1) * dilation[0]\n", "        height_pad = ((kernel_size[1] - 1) * dilation[1]) // 2\n\t        width_pad = ((kernel_size[2] - 1) * dilation[2]) // 2\n\t        # Pad temporally on the left\n\t        self.pad = nn.ConstantPad3d(padding=(width_pad, width_pad, height_pad, height_pad, time_pad, 0), value=0)\n\t        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, dilation=dilation, stride=1, padding=0, bias=bias)\n\t        self.norm = nn.BatchNorm3d(out_channels)\n\t        self.activation = nn.ReLU(inplace=True)\n\t    def forward(self, *inputs):\n\t        (x,) = inputs\n\t        x = self.pad(x)\n", "        x = self.conv(x)\n\t        x = self.norm(x)\n\t        x = self.activation(x)\n\t        return x\n\tclass CausalMaxPool3d(nn.Module):\n\t    def __init__(self, kernel_size=(2, 3, 3)):\n\t        super().__init__()\n\t        assert len(kernel_size) == 3, 'kernel_size must be a 3-tuple.'\n\t        time_pad = kernel_size[0] - 1\n\t        height_pad = (kernel_size[1] - 1) // 2\n", "        width_pad = (kernel_size[2] - 1) // 2\n\t        # Pad temporally on the left\n\t        self.pad = nn.ConstantPad3d(padding=(width_pad, width_pad, height_pad, height_pad, time_pad, 0), value=0)\n\t        self.max_pool = nn.MaxPool3d(kernel_size, stride=1)\n\t    def forward(self, *inputs):\n\t        (x,) = inputs\n\t        x = self.pad(x)\n\t        x = self.max_pool(x)\n\t        return x\n\tdef conv_1x1x1_norm_activated(in_channels, out_channels):\n", "    \"\"\"1x1x1 3D convolution, normalization and activation layer.\"\"\"\n\t    return nn.Sequential(\n\t        OrderedDict(\n\t            [\n\t                ('conv', nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False)),\n\t                ('norm', nn.BatchNorm3d(out_channels)),\n\t                ('activation', nn.ReLU(inplace=True)),\n\t            ]\n\t        )\n\t    )\n", "class Bottleneck3D(nn.Module):\n\t    \"\"\"\n\t    Defines a bottleneck module with a residual connection\n\t    \"\"\"\n\t    def __init__(self, in_channels, out_channels=None, kernel_size=(2, 3, 3), dilation=(1, 1, 1)):\n\t        super().__init__()\n\t        bottleneck_channels = in_channels // 2\n\t        out_channels = out_channels or in_channels\n\t        self.layers = nn.Sequential(\n\t            OrderedDict(\n", "                [\n\t                    # First projection with 1x1 kernel\n\t                    ('conv_down_project', conv_1x1x1_norm_activated(in_channels, bottleneck_channels)),\n\t                    # Second conv block\n\t                    (\n\t                        'conv',\n\t                        CausalConv3d(\n\t                            bottleneck_channels,\n\t                            bottleneck_channels,\n\t                            kernel_size=kernel_size,\n", "                            dilation=dilation,\n\t                            bias=False,\n\t                        ),\n\t                    ),\n\t                    # Final projection with 1x1 kernel\n\t                    ('conv_up_project', conv_1x1x1_norm_activated(bottleneck_channels, out_channels)),\n\t                ]\n\t            )\n\t        )\n\t        if out_channels != in_channels:\n", "            self.projection = nn.Sequential(\n\t                nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n\t                nn.BatchNorm3d(out_channels),\n\t            )\n\t        else:\n\t            self.projection = None\n\t    def forward(self, *args):\n\t        (x,) = args\n\t        x_residual = self.layers(x)\n\t        x_features = self.projection(x) if self.projection is not None else x\n", "        return x_residual + x_features\n\tclass PyramidSpatioTemporalPooling(nn.Module):\n\t    \"\"\" Spatio-temporal pyramid pooling.\n\t        Performs 3D average pooling followed by 1x1x1 convolution to reduce the number of channels and upsampling.\n\t        Setting contains a list of kernel_size: usually it is [(2, h, w), (2, h//2, w//2), (2, h//4, w//4)]\n\t    \"\"\"\n\t    def __init__(self, in_channels, reduction_channels, pool_sizes):\n\t        super().__init__()\n\t        self.features = []\n\t        for pool_size in pool_sizes:\n", "            assert pool_size[0] == 2, (\n\t                \"Time kernel should be 2 as PyTorch raises an error when\" \"padding with more than half the kernel size\"\n\t            )\n\t            stride = (1, *pool_size[1:])\n\t            padding = (pool_size[0] - 1, 0, 0)\n\t            self.features.append(\n\t                nn.Sequential(\n\t                    OrderedDict(\n\t                        [\n\t                            # Pad the input tensor but do not take into account zero padding into the average.\n", "                            (\n\t                                'avgpool',\n\t                                torch.nn.AvgPool3d(\n\t                                    kernel_size=pool_size, stride=stride, padding=padding, count_include_pad=False\n\t                                ),\n\t                            ),\n\t                            ('conv_bn_relu', conv_1x1x1_norm_activated(in_channels, reduction_channels)),\n\t                        ]\n\t                    )\n\t                )\n", "            )\n\t        self.features = nn.ModuleList(self.features)\n\t    def forward(self, *inputs):\n\t        (x,) = inputs\n\t        b, _, t, h, w = x.shape\n\t        # Do not include current tensor when concatenating\n\t        out = []\n\t        for f in self.features:\n\t            # Remove unnecessary padded values (time dimension) on the right\n\t            x_pool = f(x)[:, :, :-1].contiguous()\n", "            c = x_pool.shape[1]\n\t            x_pool = nn.functional.interpolate(\n\t                x_pool.view(b * t, c, *x_pool.shape[-2:]), (h, w), mode='bilinear', align_corners=False\n\t            )\n\t            x_pool = x_pool.view(b, c, t, h, w)\n\t            out.append(x_pool)\n\t        out = torch.cat(out, 1)\n\t        return out\n\tclass TemporalBlock(nn.Module):\n\t    \"\"\" Temporal block with the following layers:\n", "        - 2x3x3, 1x3x3, spatio-temporal pyramid pooling\n\t        - dropout\n\t        - skip connection.\n\t    \"\"\"\n\t    def __init__(self, in_channels, out_channels=None, use_pyramid_pooling=False, pool_sizes=None):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.half_channels = in_channels // 2\n\t        self.out_channels = out_channels or self.in_channels\n\t        self.kernels = [(2, 3, 3), (1, 3, 3)]\n", "        # Flag for spatio-temporal pyramid pooling\n\t        self.use_pyramid_pooling = use_pyramid_pooling\n\t        # 3 convolution paths: 2x3x3, 1x3x3, 1x1x1\n\t        self.convolution_paths = []\n\t        for kernel_size in self.kernels:\n\t            self.convolution_paths.append(\n\t                nn.Sequential(\n\t                    conv_1x1x1_norm_activated(self.in_channels, self.half_channels),\n\t                    CausalConv3d(self.half_channels, self.half_channels, kernel_size=kernel_size),\n\t                )\n", "            )\n\t        self.convolution_paths.append(conv_1x1x1_norm_activated(self.in_channels, self.half_channels))\n\t        self.convolution_paths = nn.ModuleList(self.convolution_paths)\n\t        agg_in_channels = len(self.convolution_paths) * self.half_channels\n\t        if self.use_pyramid_pooling:\n\t            assert pool_sizes is not None, \"setting must contain the list of kernel_size, but is None.\"\n\t            reduction_channels = self.in_channels // 3\n\t            self.pyramid_pooling = PyramidSpatioTemporalPooling(self.in_channels, reduction_channels, pool_sizes)\n\t            agg_in_channels += len(pool_sizes) * reduction_channels\n\t        # Feature aggregation\n", "        self.aggregation = nn.Sequential(\n\t            conv_1x1x1_norm_activated(agg_in_channels, self.out_channels),)\n\t        if self.out_channels != self.in_channels:\n\t            self.projection = nn.Sequential(\n\t                nn.Conv3d(self.in_channels, self.out_channels, kernel_size=1, bias=False),\n\t                nn.BatchNorm3d(self.out_channels),\n\t            )\n\t        else:\n\t            self.projection = None\n\t    def forward(self, *inputs):\n", "        (x,) = inputs\n\t        x_paths = []\n\t        for conv in self.convolution_paths:\n\t            x_paths.append(conv(x))\n\t        x_residual = torch.cat(x_paths, dim=1)\n\t        if self.use_pyramid_pooling:\n\t            x_pool = self.pyramid_pooling(x)\n\t            x_residual = torch.cat([x_residual, x_pool], dim=1)\n\t        x_residual = self.aggregation(x_residual)\n\t        if self.out_channels != self.in_channels:\n", "            x = self.projection(x)\n\t        x = x + x_residual\n\t        return x\n"]}
{"filename": "powerbev/layers/convolutions.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\tfrom collections import OrderedDict\n\tfrom functools import partial\n\timport torch\n", "import torch.nn as nn\n\timport torch.nn.functional as F\n\tclass ConvBlock(nn.Module):\n\t    \"\"\"2D convolution followed by\n\t         - an optional normalisation (batch norm or instance norm)\n\t         - an optional activation (ReLU, LeakyReLU, or tanh)\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels,\n", "        out_channels=None,\n\t        kernel_size=3,\n\t        stride=1,\n\t        norm='bn',\n\t        activation='relu',\n\t        bias=False,\n\t        transpose=False,\n\t    ):\n\t        super().__init__()\n\t        out_channels = out_channels or in_channels\n", "        padding = int((kernel_size - 1) / 2)\n\t        self.conv = nn.Conv2d if not transpose else partial(nn.ConvTranspose2d, output_padding=1)\n\t        self.conv = self.conv(in_channels, out_channels, kernel_size, stride, padding=padding, bias=bias)\n\t        if norm == 'bn':\n\t            self.norm = nn.BatchNorm2d(out_channels)\n\t        elif norm == 'in':\n\t            self.norm = nn.InstanceNorm2d(out_channels)\n\t        elif norm == 'none':\n\t            self.norm = None\n\t        else:\n", "            raise ValueError('Invalid norm {}'.format(norm))\n\t        if activation == 'relu':\n\t            self.activation = nn.ReLU(inplace=True)\n\t        elif activation == 'lrelu':\n\t            self.activation = nn.LeakyReLU(0.1, inplace=True)\n\t        elif activation == 'elu':\n\t            self.activation = nn.ELU(inplace=True)\n\t        elif activation == 'tanh':\n\t            self.activation = nn.Tanh(inplace=True)\n\t        elif activation == 'none':\n", "            self.activation = None\n\t        else:\n\t            raise ValueError('Invalid activation {}'.format(activation))\n\t    def forward(self, x):\n\t        x = self.conv(x)\n\t        if self.norm:\n\t            x = self.norm(x)\n\t        if self.activation:\n\t            x = self.activation(x)\n\t        return x\n", "class Bottleneck(nn.Module):\n\t    \"\"\"\n\t    Defines a bottleneck module with a residual connection\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels,\n\t        out_channels=None,\n\t        kernel_size=3,\n\t        dilation=1,\n", "        groups=1,\n\t        upsample=False,\n\t        downsample=False,\n\t        dropout=0.0,\n\t    ):\n\t        super().__init__()\n\t        self._downsample = downsample\n\t        bottleneck_channels = int(in_channels / 2)\n\t        out_channels = out_channels or in_channels\n\t        padding_size = ((kernel_size - 1) * dilation + 1) // 2\n", "        # Define the main conv operation\n\t        assert dilation == 1\n\t        if upsample:\n\t            assert not downsample, 'downsample and upsample not possible simultaneously.'\n\t            bottleneck_conv = nn.ConvTranspose2d(\n\t                bottleneck_channels,\n\t                bottleneck_channels,\n\t                kernel_size=kernel_size,\n\t                bias=False,\n\t                dilation=1,\n", "                stride=2,\n\t                output_padding=padding_size,\n\t                padding=padding_size,\n\t                groups=groups,\n\t            )\n\t        elif downsample:\n\t            bottleneck_conv = nn.Conv2d(\n\t                bottleneck_channels,\n\t                bottleneck_channels,\n\t                kernel_size=kernel_size,\n", "                bias=False,\n\t                dilation=dilation,\n\t                stride=2,\n\t                padding=padding_size,\n\t                groups=groups,\n\t            )\n\t        else:\n\t            bottleneck_conv = nn.Conv2d(\n\t                bottleneck_channels,\n\t                bottleneck_channels,\n", "                kernel_size=kernel_size,\n\t                bias=False,\n\t                dilation=dilation,\n\t                padding=padding_size,\n\t                groups=groups,\n\t            )\n\t        self.layers = nn.Sequential(\n\t            OrderedDict(\n\t                [\n\t                    # First projection with 1x1 kernel\n", "                    ('conv_down_project', nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, bias=False)),\n\t                    ('abn_down_project', nn.Sequential(nn.BatchNorm2d(bottleneck_channels),\n\t                                                       nn.ReLU(inplace=True))),\n\t                    # Second conv block\n\t                    ('conv', bottleneck_conv),\n\t                    ('abn', nn.Sequential(nn.BatchNorm2d(bottleneck_channels), nn.ReLU(inplace=True))),\n\t                    # Final projection with 1x1 kernel\n\t                    ('conv_up_project', nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, bias=False)),\n\t                    ('abn_up_project', nn.Sequential(nn.BatchNorm2d(out_channels),\n\t                                                     nn.ReLU(inplace=True))),\n", "                    # Regulariser\n\t                    ('dropout', nn.Dropout2d(p=dropout)),\n\t                ]\n\t            )\n\t        )\n\t        if out_channels == in_channels and not downsample and not upsample:\n\t            self.projection = None\n\t        else:\n\t            projection = OrderedDict()\n\t            if upsample:\n", "                projection.update({'upsample_skip_proj': Interpolate(scale_factor=2)})\n\t            elif downsample:\n\t                projection.update({'upsample_skip_proj': nn.MaxPool2d(kernel_size=2, stride=2)})\n\t            projection.update(\n\t                {\n\t                    'conv_skip_proj': nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n\t                    'bn_skip_proj': nn.BatchNorm2d(out_channels),\n\t                }\n\t            )\n\t            self.projection = nn.Sequential(projection)\n", "    # pylint: disable=arguments-differ\n\t    def forward(self, *args):\n\t        (x,) = args\n\t        x_residual = self.layers(x)\n\t        if self.projection is not None:\n\t            if self._downsample:\n\t                # pad h/w dimensions if they are odd to prevent shape mismatch with residual layer\n\t                x = nn.functional.pad(x, (0, x.shape[-1] % 2, 0, x.shape[-2] % 2), value=0)\n\t            return x_residual + self.projection(x)\n\t        return x_residual + x\n", "class Interpolate(nn.Module):\n\t    def __init__(self, scale_factor: int = 2):\n\t        super().__init__()\n\t        self._interpolate = nn.functional.interpolate\n\t        self._scale_factor = scale_factor\n\t    # pylint: disable=arguments-differ\n\t    def forward(self, x):\n\t        return self._interpolate(x, scale_factor=self._scale_factor, mode='bilinear', align_corners=False)\n\tclass UpsamplingConcat(nn.Module):\n\t    def __init__(self, in_channels, out_channels, scale_factor=2):\n", "        super().__init__()\n\t        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)\n\t        self.conv = nn.Sequential(\n\t            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n\t            nn.BatchNorm2d(out_channels),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n\t            nn.BatchNorm2d(out_channels),\n\t            nn.ReLU(inplace=True),\n\t        )\n", "    def forward(self, x_to_upsample, x):\n\t        x_to_upsample = self.upsample(x_to_upsample)\n\t        x_to_upsample = torch.cat([x, x_to_upsample], dim=1)\n\t        return self.conv(x_to_upsample)\n\tclass UpsamplingAdd(nn.Module):\n\t    def __init__(self, in_channels, out_channels, scale_factor=2):\n\t        super().__init__()\n\t        self.upsample_layer = nn.Sequential(\n\t            nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False),\n\t            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n", "            nn.BatchNorm2d(out_channels),\n\t        )\n\t    def forward(self, x, x_skip):\n\t        x = self.upsample_layer(x)\n\t        return x + x_skip\n"]}
{"filename": "powerbev/layers/spatial_temporal.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\tfrom collections import OrderedDict\n\tfrom torch import nn\n\tclass Residual(nn.Module):\n", "    def __init__(\n\t        self,\n\t        in_channels,\n\t        out_channels=None,\n\t        kernel_size=3,\n\t        dilation=1,\n\t        upsample=False,\n\t        downsample=False,\n\t    ):\n\t        super().__init__()\n", "        self._downsample = downsample\n\t        out_channels = out_channels or in_channels\n\t        padding_size = ((kernel_size - 1) * dilation + 1) // 2\n\t        if upsample:\n\t            assert not downsample, 'downsample and upsample not possible simultaneously.'\n\t            conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, bias=False, dilation=1, stride=2, output_padding=padding_size, padding=padding_size)\n\t        elif downsample:\n\t            conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=False, dilation=dilation, stride=2, padding=padding_size)\n\t        else:\n\t            conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=False, dilation=dilation, padding=padding_size)\n", "        self.layers = nn.Sequential(conv, nn.BatchNorm2d(out_channels), nn.LeakyReLU(inplace=True))\n\t        if out_channels == in_channels and not downsample and not upsample:\n\t            self.projection = None\n\t        else:\n\t            projection = OrderedDict()\n\t            if upsample:\n\t                projection.update({'upsample_skip_proj': nn.Upsample(scale_factor=2, mode='bilinear')})\n\t            elif downsample:\n\t                projection.update({'upsample_skip_proj': nn.MaxPool2d(kernel_size=2, stride=2)})\n\t            projection.update(\n", "                {\n\t                    'conv_skip_proj': nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n\t                    'bn_skip_proj': nn.BatchNorm2d(out_channels),\n\t                }\n\t            )\n\t            self.projection = nn.Sequential(projection)\n\t    def forward(self, *args):\n\t        (x,) = args\n\t        x_residual = self.layers(x)\n\t        if self.projection is not None:\n", "            if self._downsample:\n\t                # pad h/w dimensions if they are odd to prevent shape mismatch with residual layer\n\t                x = nn.functional.pad(x, (0, x.shape[-1] % 2, 0, x.shape[-2] % 2), value=0)\n\t            return x_residual + self.projection(x)\n\t        return x_residual + x\n\tclass Bottleneck(nn.Module):\n\t    \"\"\"expand + depthwise + pointwise\"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels,\n", "        out_channels=None,\n\t        kernel_size=3,\n\t        dilation=1,\n\t        upsample=False,\n\t        downsample=False,\n\t        expand_ratio=2,\n\t        dropout=0.0,\n\t    ):\n\t        super().__init__()\n\t        self._downsample = downsample\n", "        expand_channels = round(in_channels * expand_ratio)\n\t        out_channels = out_channels or in_channels\n\t        padding_size = ((kernel_size - 1) * dilation + 1) // 2\n\t        if upsample:\n\t            assert not downsample, 'downsample and upsample not possible simultaneously.'\n\t            conv = nn.ConvTranspose2d(expand_channels, expand_channels, kernel_size=kernel_size, bias=False, dilation=1, stride=2, output_padding=padding_size, padding=padding_size, groups=expand_channels)\n\t        elif downsample:\n\t            conv = nn.Conv2d(expand_channels, expand_channels, kernel_size=kernel_size, bias=False, dilation=dilation, stride=2, padding=padding_size, groups=expand_channels)\n\t        else:\n\t            conv = nn.Conv2d(expand_channels, expand_channels, kernel_size=kernel_size, bias=False, dilation=dilation, padding=padding_size, groups=expand_channels)\n", "        self.layers = nn.Sequential(\n\t            # pw\n\t            nn.Conv2d(in_channels, expand_channels, kernel_size=1, stride=1, padding=0, bias=False),\n\t            nn.BatchNorm2d(expand_channels),\n\t            nn.Hardswish(inplace=True),\n\t            # dw\n\t            conv,\n\t            nn.BatchNorm2d(expand_channels),\n\t            nn.Hardswish(inplace=True),\n\t            # pw-linear\n", "            nn.Conv2d(expand_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n\t            nn.BatchNorm2d(out_channels),\n\t            nn.Dropout2d(p=dropout),\n\t            # SeModule(out_channels),\n\t        )\n\t        if out_channels == in_channels and not downsample and not upsample:\n\t            self.projection = None\n\t        else:\n\t            projection = OrderedDict()\n\t            if upsample:\n", "                projection.update({'upsample_skip_proj': nn.Upsample(scale_factor=2, mode='bilinear')})\n\t            elif downsample:\n\t                projection.update({'upsample_skip_proj': nn.MaxPool2d(kernel_size=2, stride=2)})\n\t            projection.update(\n\t                {\n\t                    'conv_skip_proj': nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n\t                    'bn_skip_proj': nn.BatchNorm2d(out_channels),\n\t                }\n\t            )\n\t            self.projection = nn.Sequential(projection)\n", "    def forward(self, *args):\n\t        (x,) = args\n\t        x_residual = self.layers(x)\n\t        if self.projection is not None:\n\t            if self._downsample:\n\t                # pad h/w dimensions if they are odd to prevent shape mismatch with residual layer\n\t                x = nn.functional.pad(x, (0, x.shape[-1] % 2, 0, x.shape[-2] % 2), value=0)\n\t            return x_residual + self.projection(x)\n\t        return x_residual + x"]}
{"filename": "powerbev/models/decoder.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport torch.nn as nn\n\tfrom powerbev.layers.convolutions import UpsamplingAdd\n\tfrom torchvision.models.resnet import resnet18\n", "class Decoder(nn.Module):\n\t    def __init__(self, in_channels, n_classes, predict_future_flow):\n\t        super().__init__()\n\t        backbone = resnet18(pretrained=False, zero_init_residual=True)\n\t        self.first_conv = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\t        self.bn1 = backbone.bn1\n\t        self.relu = backbone.relu\n\t        self.layer1 = backbone.layer1\n\t        self.layer2 = backbone.layer2\n\t        self.layer3 = backbone.layer3\n", "        self.predict_future_flow = predict_future_flow\n\t        shared_out_channels = in_channels\n\t        self.up3_skip = UpsamplingAdd(256, 128, scale_factor=2)\n\t        self.up2_skip = UpsamplingAdd(128, 64, scale_factor=2)\n\t        self.up1_skip = UpsamplingAdd(64, shared_out_channels, scale_factor=2)\n\t        self.segmentation_head = nn.Sequential(\n\t            nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n\t            nn.BatchNorm2d(shared_out_channels),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(shared_out_channels, n_classes, kernel_size=1, padding=0),\n", "        )\n\t        self.instance_offset_head = nn.Sequential(\n\t            nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n\t            nn.BatchNorm2d(shared_out_channels),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(shared_out_channels, 2, kernel_size=1, padding=0),\n\t        )\n\t        self.instance_center_head = nn.Sequential(\n\t            nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n\t            nn.BatchNorm2d(shared_out_channels),\n", "            nn.ReLU(inplace=True),\n\t            nn.Conv2d(shared_out_channels, 1, kernel_size=1, padding=0),\n\t            nn.Sigmoid(),\n\t        )\n\t        if self.predict_future_flow:\n\t            self.instance_future_head = nn.Sequential(\n\t                nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n\t                nn.BatchNorm2d(shared_out_channels),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(shared_out_channels, 2, kernel_size=1, padding=0),\n", "            )\n\t    def forward(self, x):\n\t        b, s, c, h, w = x.shape\n\t        x = x.view(b * s, c, h, w)\n\t        # (H, W)\n\t        skip_x = {'1': x}\n\t        x = self.first_conv(x)\n\t        x = self.bn1(x)\n\t        x = self.relu(x)\n\t        # (H/4, W/4)\n", "        x = self.layer1(x)\n\t        skip_x['2'] = x\n\t        x = self.layer2(x)\n\t        skip_x['3'] = x\n\t        # (H/8, W/8)\n\t        x = self.layer3(x)\n\t        # First upsample to (H/4, W/4)\n\t        x = self.up3_skip(x, skip_x['3'])\n\t        # Second upsample to (H/2, W/2)\n\t        x = self.up2_skip(x, skip_x['2'])\n", "        # Third upsample to (H, W)\n\t        x = self.up1_skip(x, skip_x['1'])\n\t        segmentation_output = self.segmentation_head(x)\n\t        instance_center_output = self.instance_center_head(x)\n\t        instance_offset_output = self.instance_offset_head(x)\n\t        instance_future_output = self.instance_future_head(x) if self.predict_future_flow else None\n\t        return {\n\t            'segmentation': segmentation_output.view(b, s, *segmentation_output.shape[1:]),\n\t            'instance_center': instance_center_output.view(b, s, *instance_center_output.shape[1:]),\n\t            'instance_offset': instance_offset_output.view(b, s, *instance_offset_output.shape[1:]),\n", "            'instance_flow': instance_future_output.view(b, s, *instance_future_output.shape[1:])\n\t            if instance_future_output is not None else None,\n\t        }\n"]}
{"filename": "powerbev/models/temporal_model.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport torch.nn as nn\n\tclass TemporalModelIdentity(nn.Module):\n\t    def __init__(self, in_channels, receptive_field):\n", "        super().__init__()\n\t        self.receptive_field = receptive_field\n\t        self.out_channels = in_channels\n\t    def forward(self, x):\n\t        return x[:, (self.receptive_field - 1):]\n"]}
{"filename": "powerbev/models/stconv.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport torch\n\timport torch.nn.functional as F\n\tfrom powerbev.layers.spatial_temporal import Bottleneck, Residual\n", "from torch import nn\n\tconv_block = Residual # [Residual, Bottleneck]\n\tclass MultiBranchSTconv(torch.nn.Module):\n\t    \"\"\"\"\"\"\n\t    def __init__(\n\t        self,\n\t        cfg,\n\t        in_channels,\n\t    ):\n\t        super().__init__()\n", "        self.cfg = cfg\n\t        self.latent_dim = self.cfg.MODEL.STCONV.LATENT_DIM\n\t        self.segmentation_branch = STconv(cfg, in_channels, self.latent_dim)\n\t        self.segmentation_head = Head(self.latent_dim, len(self.cfg.SEMANTIC_SEG.WEIGHTS))\n\t        self.flow_branch = STconv(cfg, in_channels, self.latent_dim)\n\t        self.flow_head = Head(self.latent_dim, 2)\n\t        self.parameter_init()\n\t    def parameter_init(self):\n\t        if isinstance(self.segmentation_head.last_conv, nn.Conv2d):\n\t            self.segmentation_head.last_conv.bias = nn.parameter.Parameter(torch.tensor([4.6, 0.0], requires_grad=True))\n", "        if isinstance(self.flow_head.last_conv, nn.Conv2d):\n\t            self.flow_head.last_conv.bias = nn.parameter.Parameter(torch.tensor([0.0, 0.0], requires_grad=True))\n\t    def forward(self, x):\n\t        output = {}\n\t        segmentation_branch_output = self.branch_forward(x, self.segmentation_branch)\n\t        output['segmentation'] = self.head_forward(segmentation_branch_output, self.segmentation_head)\n\t        flow_branch_output = self.branch_forward(x, self.flow_branch)\n\t        output['instance_flow'] = self.head_forward(flow_branch_output, self.flow_head)\n\t        return output\n\t    @staticmethod\n", "    def branch_forward(x, branch):\n\t        return branch(x)\n\t    @staticmethod\n\t    def head_forward(x, head):\n\t        return head(x)\n\tclass STconv(torch.nn.Module):\n\t    def __init__(\n\t        self,\n\t        cfg,\n\t        in_channels,\n", "        out_channels,\n\t    ):\n\t        super().__init__()\n\t        # Data configs\n\t        self.cfg = cfg\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.num_past_curr_steps = self.cfg.TIME_RECEPTIVE_FIELD\n\t        self.num_waypoints = self.cfg.N_FUTURE_FRAMES + 2 # See section 3.4 in paper for details.\n\t        # Model configs\n", "        self.num_features = [f for f in self.cfg.MODEL.STCONV.NUM_FEATURES]\n\t        self.num_blocks = self.cfg.MODEL.STCONV.NUM_BLOCKS\n\t        # BEV Encoder\n\t        self.BEV_encoder = STEncoder(\n\t            num_features=[self.in_channels] + self.num_features,\n\t            num_timesteps=self.num_past_curr_steps,\n\t            num_blocks=self.num_blocks,\n\t        )\n\t        # BEV Predictor\n\t        self.BEV_predictor = STPredictor(\n", "            num_features=self.num_features,\n\t            in_timesteps=self.num_past_curr_steps,\n\t            out_timesteps=self.num_waypoints,\n\t        )\n\t        # BEV Decoder\n\t        self.BEV_decoder = STDecoder(\n\t            num_features=self.num_features[::-1] + [self.out_channels],\n\t            num_timesteps=self.num_waypoints,\n\t            num_blocks=self.num_blocks,\n\t        )\n", "    def forward(self, f_in):      \n\t        # BEV Encoder\n\t        f_enc = self.BEV_encoder(f_in)  # list of N x T_in x C_i x H_i x W_i\n\t        # BEV Predictor\n\t        f_dec = self.BEV_predictor(f_enc)  # list of N x T_out x C_i x H_i x W_i\n\t        # BEV Decoder\n\t        f_out = self.BEV_decoder(f_dec)  # N x T_out x C x H x W\n\t        # Output\n\t        return f_out\n\tclass STEncoder(nn.Module):\n", "    def __init__(\n\t        self,\n\t        num_features,\n\t        num_timesteps,\n\t        num_blocks=3,\n\t    ):\n\t        super().__init__()\n\t        self.num_features = num_features\n\t        self.num_timesteps = num_timesteps\n\t        self.conv = nn.ModuleList()\n", "        for i in range(1, len(self.num_features)):\n\t            stage = nn.Sequential()\n\t            for j in range(1, num_blocks+1):\n\t                stage.add_module(\n\t                    f'downconv_{i}_{j}', conv_block(\n\t                        in_channels=self.num_features[i - 1] * self.num_timesteps if j == 1 else self.num_features[i] * self.num_timesteps,\n\t                        out_channels=self.num_features[i] * self.num_timesteps,\n\t                        downsample=True if j == num_blocks else False\n\t                    )\n\t                )\n", "            self.conv.append(stage)\n\t    def forward(self, x):\n\t        b, t, _, _, _ = x.shape\n\t        x = x.reshape(b, -1, *x.shape[3:])\n\t        feature_pyramid = []\n\t        for _, stage in enumerate(self.conv):\n\t            x = stage(x)\n\t            feature_pyramid.append(x.reshape(b, t, -1, *x.shape[2:]))\n\t        return feature_pyramid\n\tclass STPredictor(nn.Module):\n", "    def __init__(\n\t        self,\n\t        num_features,\n\t        in_timesteps,\n\t        out_timesteps,\n\t    ):\n\t        super().__init__()\n\t        self.predictor = nn.ModuleList()\n\t        for nf in num_features:\n\t            self.predictor.append(nn.Sequential(\n", "                conv_block(nf * in_timesteps, nf * in_timesteps),\n\t                conv_block(nf * in_timesteps, nf * in_timesteps),\n\t                conv_block(nf * in_timesteps, nf * out_timesteps),\n\t                conv_block(nf * out_timesteps, nf * out_timesteps),\n\t                conv_block(nf * out_timesteps, nf * out_timesteps),\n\t            ))\n\t    def forward(self, x):\n\t        assert len(x) == len(self.predictor), f'The number of input feature tensors ({len(x)}) must be the same as the number of STPredictor blocks {len(self.predictor)}.'\n\t        y = []\n\t        for i in range(len(x)):\n", "            b, _, c, _, _ = x[i].shape\n\t            x_temp = x[i].reshape(b, -1, *x[i].shape[3:])\n\t            y.append(self.predictor[i](x_temp).reshape(b, -1, c, *x_temp.shape[2:]))\n\t        return y\n\tclass STDecoder(nn.Module):\n\t    def __init__(\n\t        self,\n\t        num_features,\n\t        num_timesteps,\n\t        num_blocks=3,\n", "    ):\n\t        super().__init__()\n\t        self.num_features = num_features\n\t        self.num_timesteps = num_timesteps\n\t        self.conv = nn.ModuleList()\n\t        for i in range(1, len(self.num_features)):\n\t            stage = nn.Sequential()\n\t            for j in range(1, num_blocks+1):\n\t                stage.add_module(\n\t                    f'upconv_{i}_{j}', conv_block(\n", "                        in_channels=self.num_features[i - 1] * 2 * self.num_timesteps if j == 1 else self.num_features[i] * self.num_timesteps,\n\t                        out_channels=self.num_features[i] * self.num_timesteps,\n\t                        upsample=True if j == 1 else False\n\t                    )\n\t                )\n\t            self.conv.append(stage)\n\t    def forward(self, x):\n\t        assert isinstance(x, list)\n\t        for i, stage in enumerate(self.conv):\n\t            b, t, _, _, _ = x[-1-i].shape\n", "            x_temp = x[-1-i]\n\t            x_temp = x_temp.reshape(b, -1, *x_temp.shape[3:])\n\t            if i == 0:\n\t                y = x_temp.repeat(1, 2, 1, 1)\n\t            else:\n\t                if y.shape != x_temp.shape:\n\t                    y = F.interpolate(y, size=x_temp.shape[2:], mode='bilinear', align_corners=True)\n\t                y = torch.cat((y, x_temp), dim=1)\n\t            y = stage(y)\n\t        y = y.reshape((b, t, -1, *y.shape[2:]))\n", "        return y\n\tclass Head(nn.Module):\n\t    def __init__(self, in_channels, out_channels, sigmoid=False):\n\t        super().__init__()\n\t        self.sigmoid = sigmoid\n\t        self.head = nn.Sequential(\n\t            conv_block(in_channels, in_channels//2),\n\t            conv_block(in_channels//2, in_channels//2),\n\t            conv_block(in_channels//2, in_channels//4),\n\t            conv_block(in_channels//4, in_channels//4),\n", "        )\n\t        self.last_conv = nn.Conv2d(in_channels//4, out_channels, kernel_size=3, padding=1, bias=True)\n\t        if self.sigmoid:\n\t            self.last_sigmoid = nn.Sigmoid()\n\t    def forward(self, x):\n\t        y = x.clone()\n\t        b, t, _, _, _ = y.shape\n\t        y = y.reshape(-1, *y.shape[2:])\n\t        y = self.head(y)\n\t        y = self.last_conv(y)\n", "        if self.sigmoid:\n\t            y = self.last_sigmoid(y)\n\t        return y.reshape(b, t, *y.shape[1:])"]}
{"filename": "powerbev/models/encoder.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport torch.nn as nn\n\tfrom efficientnet_pytorch import EfficientNet\n\tfrom powerbev.layers.convolutions import UpsamplingConcat\n", "class Encoder(nn.Module):\n\t    def __init__(self, cfg, D):\n\t        super().__init__()\n\t        self.D = D\n\t        self.C = cfg.OUT_CHANNELS\n\t        self.use_depth_distribution = cfg.USE_DEPTH_DISTRIBUTION\n\t        self.downsample = cfg.DOWNSAMPLE\n\t        self.version = cfg.NAME.split('-')[1]\n\t        self.backbone = EfficientNet.from_pretrained(cfg.NAME)\n\t        self.delete_unused_layers()\n", "        if self.downsample == 16:\n\t            if self.version == 'b0':\n\t                upsampling_in_channels = 320 + 112\n\t            elif self.version == 'b4':\n\t                upsampling_in_channels = 448 + 160\n\t            upsampling_out_channels = 512\n\t        elif self.downsample == 8:\n\t            if self.version == 'b0':\n\t                upsampling_in_channels = 112 + 40\n\t            elif self.version == 'b4':\n", "                upsampling_in_channels = 160 + 56\n\t            upsampling_out_channels = 128\n\t        else:\n\t            raise ValueError(f'Downsample factor {self.downsample} not handled.')\n\t        self.upsampling_layer = UpsamplingConcat(upsampling_in_channels, upsampling_out_channels)\n\t        if self.use_depth_distribution:\n\t            self.depth_layer = nn.Conv2d(upsampling_out_channels, self.C + self.D, kernel_size=1, padding=0)\n\t        else:\n\t            self.depth_layer = nn.Conv2d(upsampling_out_channels, self.C, kernel_size=1, padding=0)\n\t    def delete_unused_layers(self):\n", "        indices_to_delete = []\n\t        for idx in range(len(self.backbone._blocks)):\n\t            if self.downsample == 8:\n\t                if self.version == 'b0' and idx > 10:\n\t                    indices_to_delete.append(idx)\n\t                if self.version == 'b4' and idx > 21:\n\t                    indices_to_delete.append(idx)\n\t        for idx in reversed(indices_to_delete):\n\t            del self.backbone._blocks[idx]\n\t        del self.backbone._conv_head\n", "        del self.backbone._bn1\n\t        del self.backbone._avg_pooling\n\t        del self.backbone._dropout\n\t        del self.backbone._fc\n\t    def get_features(self, x):\n\t        # Adapted from https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/model.py#L231\n\t        endpoints = dict()\n\t        # Stem\n\t        x = self.backbone._swish(self.backbone._bn0(self.backbone._conv_stem(x)))\n\t        prev_x = x\n", "        # Blocks\n\t        for idx, block in enumerate(self.backbone._blocks):\n\t            drop_connect_rate = self.backbone._global_params.drop_connect_rate\n\t            if drop_connect_rate:\n\t                drop_connect_rate *= float(idx) / len(self.backbone._blocks)\n\t            x = block(x, drop_connect_rate=drop_connect_rate)\n\t            if prev_x.size(2) > x.size(2):\n\t                endpoints['reduction_{}'.format(len(endpoints) + 1)] = prev_x\n\t            prev_x = x\n\t            if self.downsample == 8:\n", "                if self.version == 'b0' and idx == 10:\n\t                    break\n\t                if self.version == 'b4' and idx == 21:\n\t                    break\n\t        # Head\n\t        endpoints['reduction_{}'.format(len(endpoints) + 1)] = x\n\t        if self.downsample == 16:\n\t            input_1, input_2 = endpoints['reduction_5'], endpoints['reduction_4']\n\t        elif self.downsample == 8:\n\t            input_1, input_2 = endpoints['reduction_4'], endpoints['reduction_3']\n", "        x = self.upsampling_layer(input_1, input_2)\n\t        return x\n\t    def forward(self, x):\n\t        x = self.get_features(x)  # get feature vector\n\t        x = self.depth_layer(x)  # feature and depth head\n\t        if self.use_depth_distribution:\n\t            depth = x[:, : self.D].softmax(dim=1)\n\t            x = depth.unsqueeze(1) * x[:, self.D : (self.D + self.C)].unsqueeze(2)  # outer product depth and features\n\t        else:\n\t            x = x.unsqueeze(2).repeat(1, 1, self.D, 1, 1)\n", "        return x\n"]}
{"filename": "powerbev/models/powerbev.py", "chunked_list": ["# ------------------------------------------------------------------------\n\t# PowerBEV\n\t# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\t# Modified from FIERY (https://github.com/wayveai/fiery)\n\t# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n\t# ------------------------------------------------------------------------\n\timport time\n\timport torch\n\timport torch.nn as nn\n", "from powerbev.models.decoder import Decoder\n\tfrom powerbev.models.encoder import Encoder\n\tfrom powerbev.models.stconv import MultiBranchSTconv\n\tfrom powerbev.models.temporal_model import TemporalModelIdentity\n\tfrom powerbev.utils.geometry import (VoxelsSumming,\n\t                                     calculate_birds_eye_view_parameters,\n\t                                     cumulative_warp_features)\n\tfrom powerbev.utils.network import (pack_sequence_dim, set_bn_momentum,\n\t                                    unpack_sequence_dim)\n\tclass PowerBEV(nn.Module):\n", "    def __init__(self, cfg):\n\t        super().__init__()\n\t        self.cfg = cfg\n\t        bev_resolution, bev_start_position, bev_dimension = calculate_birds_eye_view_parameters(\n\t            self.cfg.LIFT.X_BOUND, self.cfg.LIFT.Y_BOUND, self.cfg.LIFT.Z_BOUND\n\t        )\n\t        self.bev_resolution = nn.Parameter(bev_resolution, requires_grad=False)\n\t        self.bev_start_position = nn.Parameter(bev_start_position, requires_grad=False)\n\t        self.bev_dimension = nn.Parameter(bev_dimension, requires_grad=False)\n\t        self.encoder_downsample = self.cfg.MODEL.ENCODER.DOWNSAMPLE\n", "        self.encoder_out_channels = self.cfg.MODEL.ENCODER.OUT_CHANNELS\n\t        self.frustum = self.create_frustum()\n\t        self.depth_channels, _, _, _ = self.frustum.shape\n\t        if self.cfg.TIME_RECEPTIVE_FIELD == 1:\n\t            assert self.cfg.MODEL.TEMPORAL_MODEL.NAME == 'identity'\n\t        # temporal block\n\t        self.receptive_field = self.cfg.TIME_RECEPTIVE_FIELD\n\t        self.n_future = self.cfg.N_FUTURE_FRAMES\n\t        if self.cfg.MODEL.SUBSAMPLE:\n\t            assert self.cfg.DATASET.NAME == 'lyft'\n", "            self.receptive_field = 3\n\t            self.n_future = 5\n\t        # Spatial extent in bird's-eye view, in meters\n\t        self.spatial_extent = (self.cfg.LIFT.X_BOUND[1], self.cfg.LIFT.Y_BOUND[1])\n\t        self.bev_size = (self.bev_dimension[0].item(), self.bev_dimension[1].item())\n\t        # Encoder\n\t        self.encoder = Encoder(cfg=self.cfg.MODEL.ENCODER, D=self.depth_channels)\n\t        # Temporal model\n\t        temporal_in_channels = self.encoder_out_channels\n\t        temporal_out_channels = temporal_in_channels    \n", "        if self.n_future == 0:\n\t            self.temporal_model = TemporalModelIdentity(temporal_in_channels, self.receptive_field)\n\t            # Decoder\n\t            self.future_pred_in_channels = temporal_out_channels\n\t            self.decoder = Decoder(\n\t                in_channels=self.future_pred_in_channels,\n\t                n_classes=len(self.cfg.SEMANTIC_SEG.WEIGHTS),\n\t                predict_future_flow=self.cfg.INSTANCE_FLOW.ENABLED,\n\t            )\n\t        elif self.n_future > 0:\n", "            if self.cfg.MODEL.STCONV.INPUT_EGOPOSE:\n\t                temporal_in_channels += 6\n\t            # Multi-branch STConv\n\t            self.stconv = MultiBranchSTconv(cfg=self.cfg, in_channels = temporal_in_channels)\n\t        else:\n\t            raise NotImplementedError(f'Number of future frames {self.n_future}.')\n\t        set_bn_momentum(self, self.cfg.MODEL.BN_MOMENTUM)\n\t    def create_frustum(self):\n\t        # Create grid in image plane\n\t        h, w = self.cfg.IMAGE.FINAL_DIM\n", "        downsampled_h, downsampled_w = h // self.encoder_downsample, w // self.encoder_downsample\n\t        # Depth grid\n\t        depth_grid = torch.arange(*self.cfg.LIFT.D_BOUND, dtype=torch.float)\n\t        depth_grid = depth_grid.view(-1, 1, 1).expand(-1, downsampled_h, downsampled_w)\n\t        n_depth_slices = depth_grid.shape[0]\n\t        # x and y grids\n\t        x_grid = torch.linspace(0, w - 1, downsampled_w, dtype=torch.float)\n\t        x_grid = x_grid.view(1, 1, downsampled_w).expand(n_depth_slices, downsampled_h, downsampled_w)\n\t        y_grid = torch.linspace(0, h - 1, downsampled_h, dtype=torch.float)\n\t        y_grid = y_grid.view(1, downsampled_h, 1).expand(n_depth_slices, downsampled_h, downsampled_w)\n", "        # Dimension (n_depth_slices, downsampled_h, downsampled_w, 3)\n\t        # containing data points in the image: left-right, top-bottom, depth\n\t        frustum = torch.stack((x_grid, y_grid, depth_grid), -1)\n\t        return nn.Parameter(frustum, requires_grad=False)\n\t    def forward(self, image, intrinsics, extrinsics, future_egomotion, future_distribution_inputs=None, noise=None):\n\t        output = {}\n\t        start_time = time.time()\n\t        # Only process features from the past and present\n\t        image = image[:, :self.receptive_field].contiguous()\n\t        intrinsics = intrinsics[:, :self.receptive_field].contiguous()\n", "        extrinsics = extrinsics[:, :self.receptive_field].contiguous()\n\t        future_egomotion = future_egomotion[:, :self.receptive_field].contiguous()\n\t        # Lifting features and project to bird's-eye view\n\t        x = self.calculate_birds_eye_view_features(image, intrinsics, extrinsics)\n\t        # Warp past features to the present's reference frame\n\t        x = cumulative_warp_features(\n\t            x.clone(), future_egomotion,\n\t            mode='bilinear', spatial_extent=self.spatial_extent,\n\t        )\n\t        perception_time = time.time()\n", "        # Temporal model\n\t        if self.n_future == 0:\n\t            states = self.temporal_model(x)\n\t            bev_output = self.decoder(states[:, -1:])\n\t        elif self.n_future > 0:\n\t            if self.cfg.MODEL.STCONV.INPUT_EGOPOSE:\n\t                b, s, c = future_egomotion.shape\n\t                h, w = x.shape[-2:]\n\t                future_egomotions_spatial = future_egomotion.view(b, s, c, 1, 1).expand(b, s, c, h, w)\n\t                # at time 0, no egomotion so feed zero vector\n", "                future_egomotions_spatial = torch.cat([torch.zeros_like(future_egomotions_spatial[:, :1]),\n\t                                                    future_egomotions_spatial[:, :(self.receptive_field-1)]], dim=1)\n\t                x = torch.cat([x, future_egomotions_spatial], dim=-3)\n\t            bev_output = self.stconv(x)\n\t        else:\n\t            raise NotImplementedError(f'Number of future frames {self.n_future}.')\n\t        prediction_time = time.time()\n\t        output['perception_time'] = perception_time - start_time\n\t        output['prediction_time'] = prediction_time - perception_time\n\t        output['total_time'] = output['perception_time'] + output['prediction_time']\n", "        output = {**output, **bev_output}\n\t        return output\n\t    def get_geometry(self, intrinsics, extrinsics):\n\t        \"\"\"Calculate the (x, y, z) 3D position of the features.\n\t        \"\"\"\n\t        rotation, translation = extrinsics[..., :3, :3], extrinsics[..., :3, 3]\n\t        B, N, _ = translation.shape\n\t        # Add batch, camera dimension, and a dummy dimension at the end\n\t        points = self.frustum.unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n\t        # Camera to ego reference frame\n", "        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3], points[:, :, :, :, :, 2:3]), 5)\n\t        combined_transformation = rotation.matmul(torch.inverse(intrinsics))\n\t        points = combined_transformation.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)\n\t        points += translation.view(B, N, 1, 1, 1, 3)\n\t        # The 3 dimensions in the ego reference frame are: (forward, sides, height)\n\t        return points\n\t    def encoder_forward(self, x):\n\t        # batch, n_cameras, channels, height, width\n\t        b, n, c, h, w = x.shape\n\t        x = x.view(b * n, c, h, w)\n", "        x = self.encoder(x)\n\t        x = x.view(b, n, *x.shape[1:])\n\t        x = x.permute(0, 1, 3, 4, 5, 2)\n\t        return x\n\t    def projection_to_birds_eye_view(self, x, geometry):\n\t        \"\"\" Adapted from https://github.com/nv-tlabs/lift-splat-shoot/blob/master/src/models.py#L200\"\"\"\n\t        # batch, n_cameras, depth, height, width, channels\n\t        batch, n, d, h, w, c = x.shape\n\t        output = torch.zeros(\n\t            (batch, c, self.bev_dimension[0], self.bev_dimension[1]), dtype=torch.float, device=x.device\n", "        )\n\t        # Number of 3D points\n\t        N = n * d * h * w\n\t        for b in range(batch):\n\t            # flatten x\n\t            x_b = x[b].reshape(N, c)\n\t            # Convert positions to integer indices\n\t            geometry_b = ((geometry[b] - (self.bev_start_position - self.bev_resolution / 2.0)) / self.bev_resolution)\n\t            geometry_b = geometry_b.view(N, 3).long()\n\t            # Mask out points that are outside the considered spatial extent.\n", "            mask = (\n\t                    (geometry_b[:, 0] >= 0)\n\t                    & (geometry_b[:, 0] < self.bev_dimension[0])\n\t                    & (geometry_b[:, 1] >= 0)\n\t                    & (geometry_b[:, 1] < self.bev_dimension[1])\n\t                    & (geometry_b[:, 2] >= 0)\n\t                    & (geometry_b[:, 2] < self.bev_dimension[2])\n\t            )\n\t            x_b = x_b[mask]\n\t            geometry_b = geometry_b[mask]\n", "            # Sort tensors so that those within the same voxel are consecutives.\n\t            ranks = (\n\t                    geometry_b[:, 0] * (self.bev_dimension[1] * self.bev_dimension[2])\n\t                    + geometry_b[:, 1] * (self.bev_dimension[2])\n\t                    + geometry_b[:, 2]\n\t            )\n\t            ranks_indices = ranks.argsort()\n\t            x_b, geometry_b, ranks = x_b[ranks_indices], geometry_b[ranks_indices], ranks[ranks_indices]\n\t            # Project to bird's-eye view by summing voxels.\n\t            x_b, geometry_b = VoxelsSumming.apply(x_b, geometry_b, ranks)\n", "            bev_feature = torch.zeros((self.bev_dimension[2], self.bev_dimension[0], self.bev_dimension[1], c),\n\t                                      device=x_b.device)\n\t            bev_feature[geometry_b[:, 2], geometry_b[:, 0], geometry_b[:, 1]] = x_b\n\t            # Put channel in second position and remove z dimension\n\t            bev_feature = bev_feature.permute((0, 3, 1, 2))\n\t            bev_feature = bev_feature.squeeze(0)\n\t            output[b] = bev_feature\n\t        return output\n\t    def calculate_birds_eye_view_features(self, x, intrinsics, extrinsics):\n\t        b, s, n, c, h, w = x.shape\n", "        # Reshape\n\t        x = pack_sequence_dim(x)\n\t        intrinsics = pack_sequence_dim(intrinsics)\n\t        extrinsics = pack_sequence_dim(extrinsics)\n\t        geometry = self.get_geometry(intrinsics, extrinsics)\n\t        x = self.encoder_forward(x)\n\t        x = self.projection_to_birds_eye_view(x, geometry)\n\t        x = unpack_sequence_dim(x, b, s)\n\t        return x\n"]}
