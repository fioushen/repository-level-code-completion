{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\tsetup(\n\t    name='uss',\n\t    version='0.0.4',\n\t    description='Universal source separation (USS) with weakly labelled data.',\n\t    author='Qiuqiang Kong',\n\t    author_email='qiuqiangkong@gmail.com',\n\t    license='Apache2.0',\n\t    packages=find_packages(),\n\t    url=\"https://github.com/bytedance/uss\",\n", "    include_package_data=True,\n\t    install_requires=[\n\t        \"torch>=2.0.0\",\n\t        \"lightning>=2.0.0\",\n\t        \"panns_inference>=0.1.0\",\n\t        \"transformers\",\n\t        \"h5py\",\n\t        \"librosa>=0.10.0.post2\",\n\t        \"pandas\",\n\t        \"tensorboard\",\n", "        \"einops\",\n\t    ],\n\t    python_requires='>=3.8',\n\t    entry_points={\n\t        'console_scripts': ['uss=uss.uss_inference:main'],\n\t    },\n\t)\n"]}
{"filename": "panns/utilities.py", "chunked_list": ["import os\n\timport logging\n\timport h5py\n\timport soundfile\n\timport librosa\n\timport numpy as np\n\timport pandas as pd\n\tfrom scipy import stats \n\timport datetime\n\timport pickle\n", "def create_folder(fd):\n\t    if not os.path.exists(fd):\n\t        os.makedirs(fd)\n\tdef get_filename(path):\n\t    path = os.path.realpath(path)\n\t    na_ext = path.split('/')[-1]\n\t    na = os.path.splitext(na_ext)[0]\n\t    return na\n\tdef get_sub_filepaths(folder):\n\t    paths = []\n", "    for root, dirs, files in os.walk(folder):\n\t        for name in files:\n\t            path = os.path.join(root, name)\n\t            paths.append(path)\n\t    return paths\n\tdef create_logging(log_dir, filemode):\n\t    create_folder(log_dir)\n\t    i1 = 0\n\t    while os.path.isfile(os.path.join(log_dir, '{:04d}.log'.format(i1))):\n\t        i1 += 1\n", "    log_path = os.path.join(log_dir, '{:04d}.log'.format(i1))\n\t    logging.basicConfig(\n\t        level=logging.DEBUG,\n\t        format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n\t        datefmt='%a, %d %b %Y %H:%M:%S',\n\t        filename=log_path,\n\t        filemode=filemode)\n\t    # Print to console\n\t    console = logging.StreamHandler()\n\t    console.setLevel(logging.INFO)\n", "    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n\t    console.setFormatter(formatter)\n\t    logging.getLogger('').addHandler(console)\n\t    return logging\n\tdef read_metadata(csv_path, classes_num, id_to_ix):\n\t    \"\"\"Read metadata of AudioSet from a csv file.\n\t    Args:\n\t      csv_path: str\n\t    Returns:\n\t      meta_dict: {'audio_name': (audios_num,), 'target': (audios_num, classes_num)}\n", "    \"\"\"\n\t    with open(csv_path, 'r') as fr:\n\t        lines = fr.readlines()\n\t        lines = lines[3:]   # Remove heads\n\t    audios_num = len(lines)\n\t    targets = np.zeros((audios_num, classes_num), dtype=bool)\n\t    audio_names = []\n\t    for n, line in enumerate(lines):\n\t        items = line.split(', ')\n\t        \"\"\"items: ['--4gqARaEJE', '0.000', '10.000', '\"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\\n']\"\"\"\n", "        audio_name = 'Y{}.wav'.format(items[0])   # Audios are started with an extra 'Y' when downloading\n\t        label_ids = items[3].split('\"')[1].split(',')\n\t        audio_names.append(audio_name)\n\t        # Target\n\t        for id in label_ids:\n\t            ix = id_to_ix[id]\n\t            targets[n, ix] = 1\n\t    meta_dict = {'audio_name': np.array(audio_names), 'target': targets}\n\t    return meta_dict\n\tdef float32_to_int16(x):\n", "    assert np.max(np.abs(x)) <= 1.2\n\t    x = np.clip(x, -1, 1)\n\t    return (x * 32767.).astype(np.int16)\n\tdef int16_to_float32(x):\n\t    return (x / 32767.).astype(np.float32)\n\tdef pad_or_truncate(x, audio_length):\n\t    \"\"\"Pad all audio to specific length.\"\"\"\n\t    if len(x) <= audio_length:\n\t        return np.concatenate((x, np.zeros(audio_length - len(x))), axis=0)\n\t    else:\n", "        return x[0 : audio_length]\n"]}
{"filename": "panns/config.py", "chunked_list": ["import numpy as np\n\timport csv\n\tsample_rate = 32000\n\tclip_samples = sample_rate * 10     # Audio clips are 10-second\n\t# Load label\n\twith open('metadata/class_labels_indices.csv', 'r') as f:\n\t    reader = csv.reader(f, delimiter=',')\n\t    lines = list(reader)\n\tlabels = []\n\tids = []    # Each label has a unique id such as \"/m/068hy\"\n", "for i1 in range(1, len(lines)):\n\t    id = lines[i1][1]\n\t    label = lines[i1][2]\n\t    ids.append(id)\n\t    labels.append(label)\n\tclasses_num = len(labels)\n\tlb_to_ix = {label : i for i, label in enumerate(labels)}\n\tix_to_lb = {i : label for i, label in enumerate(labels)}\n\tid_to_ix = {id : i for i, id in enumerate(ids)}\n\tix_to_id = {i : id for i, id in enumerate(ids)}\n", "full_samples_per_class = np.array([\n\t        937432,  16344,   7822,  10271,   2043,  14420,    733,   1511,\n\t         1258,    424,   1751,    704,    369,    590,   1063,   1375,\n\t         5026,    743,    853,   1648,    714,   1497,   1251,   2139,\n\t         1093,    133,    224,  39469,   6423,    407,   1559,   4546,\n\t         6826,   7464,   2468,    549,   4063,    334,    587,    238,\n\t         1766,    691,    114,   2153,    236,    209,    421,    740,\n\t          269,    959,    137,   4192,    485,   1515,    655,    274,\n\t           69,    157,   1128,    807,   1022,    346,     98,    680,\n\t          890,    352,   4169,   2061,   1753,   9883,   1339,    708,\n", "        37857,  18504,  12864,   2475,   2182,    757,   3624,    677,\n\t         1683,   3583,    444,   1780,   2364,    409,   4060,   3097,\n\t         3143,    502,    723,    600,    230,    852,   1498,   1865,\n\t         1879,   2429,   5498,   5430,   2139,   1761,   1051,    831,\n\t         2401,   2258,   1672,   1711,    987,    646,    794,  25061,\n\t         5792,   4256,     96,   8126,   2740,    752,    513,    554,\n\t          106,    254,   1592,    556,    331,    615,   2841,    737,\n\t          265,   1349,    358,   1731,   1115,    295,   1070,    972,\n\t          174, 937780, 112337,  42509,  49200,  11415,   6092,  13851,\n\t         2665,   1678,  13344,   2329,   1415,   2244,   1099,   5024,\n", "         9872,  10948,   4409,   2732,   1211,   1289,   4807,   5136,\n\t         1867,  16134,  14519,   3086,  19261,   6499,   4273,   2790,\n\t         8820,   1228,   1575,   4420,   3685,   2019,    664,    324,\n\t          513,    411,    436,   2997,   5162,   3806,   1389,    899,\n\t         8088,   7004,   1105,   3633,   2621,   9753,   1082,  26854,\n\t         3415,   4991,   2129,   5546,   4489,   2850,   1977,   1908,\n\t         1719,   1106,   1049,    152,    136,    802,    488,    592,\n\t         2081,   2712,   1665,   1128,    250,    544,    789,   2715,\n\t         8063,   7056,   2267,   8034,   6092,   3815,   1833,   3277,\n\t         8813,   2111,   4662,   2678,   2954,   5227,   1472,   2591,\n", "         3714,   1974,   1795,   4680,   3751,   6585,   2109,  36617,\n\t         6083,  16264,  17351,   3449,   5034,   3931,   2599,   4134,\n\t         3892,   2334,   2211,   4516,   2766,   2862,   3422,   1788,\n\t         2544,   2403,   2892,   4042,   3460,   1516,   1972,   1563,\n\t         1579,   2776,   1647,   4535,   3921,   1261,   6074,   2922,\n\t         3068,   1948,   4407,    712,   1294,   1019,   1572,   3764,\n\t         5218,    975,   1539,   6376,   1606,   6091,   1138,   1169,\n\t         7925,   3136,   1108,   2677,   2680,   1383,   3144,   2653,\n\t         1986,   1800,   1308,   1344, 122231,  12977,   2552,   2678,\n\t         7824,    768,   8587,  39503,   3474,    661,    430,    193,\n", "         1405,   1442,   3588,   6280,  10515,    785,    710,    305,\n\t          206,   4990,   5329,   3398,   1771,   3022,   6907,   1523,\n\t         8588,  12203,    666,   2113,   7916,    434,   1636,   5185,\n\t         1062,    664,    952,   3490,   2811,   2749,   2848,  15555,\n\t          363,    117,   1494,   1647,   5886,   4021,    633,   1013,\n\t         5951,  11343,   2324,    243,    372,    943,    734,    242,\n\t         3161,    122,    127,    201,   1654,    768,    134,   1467,\n\t          642,   1148,   2156,   1368,   1176,    302,   1909,     61,\n\t          223,   1812,    287,    422,    311,    228,    748,    230,\n\t         1876,    539,   1814,    737,    689,   1140,    591,    943,\n", "          353,    289,    198,    490,   7938,   1841,    850,    457,\n\t        814,    146,    551,    728,   1627,    620,    648,   1621,\n\t         2731,    535,     88,   1736,    736,    328,    293,   3170,\n\t          344,    384,   7640,    433,    215,    715,    626,    128,\n\t         3059,   1833,   2069,   3732,   1640,   1508,    836,    567,\n\t         2837,   1151,   2068,    695,   1494,   3173,    364,     88,\n\t          188,    740,    677,    273,   1533,    821,   1091,    293,\n\t          647,    318,   1202,    328,    532,   2847,    526,    721,\n\t          370,    258,    956,   1269,   1641,    339,   1322,   4485,\n\t          286,   1874,    277,    757,   1393,   1330,    380,    146,\n", "          377,    394,    318,    339,   1477,   1886,    101,   1435,\n\t          284,   1425,    686,    621,    221,    117,     87,   1340,\n\t          201,   1243,   1222,    651,   1899,    421,    712,   1016,\n\t         1279,    124,    351,    258,   7043,    368,    666,    162,\n\t         7664,    137,  70159,  26179,   6321,  32236,  33320,    771,\n\t         1169,    269,   1103,    444,    364,   2710,    121,    751,\n\t         1609,    855,   1141,   2287,   1940,   3943,    289])"]}
{"filename": "panns/dataset.py", "chunked_list": ["import numpy as np\n\timport argparse\n\timport csv\n\timport os\n\timport glob\n\timport datetime\n\timport time\n\timport logging\n\timport h5py\n\timport librosa\n", "from panns.utilities import (create_folder, get_filename, create_logging, \n\t    float32_to_int16, pad_or_truncate, read_metadata)\n\tfrom panns import config\n\tdef pack_waveforms_to_hdf5(args):\n\t    \"\"\"Pack waveform and target of several audio clips to a single hdf5 file. \n\t    This can speed up loading and training.\n\t    \"\"\"\n\t    # Arguments & parameters\n\t    audios_dir = args.audios_dir\n\t    csv_path = args.csv_path\n", "    waveforms_hdf5_path = args.waveforms_hdf5_path\n\t    mini_data = args.mini_data\n\t    clip_samples = config.clip_samples\n\t    classes_num = config.classes_num\n\t    sample_rate = config.sample_rate\n\t    id_to_ix = config.id_to_ix\n\t    # Paths\n\t    if mini_data:\n\t        prefix = 'mini_'\n\t        waveforms_hdf5_path += '.mini'\n", "    else:\n\t        prefix = ''\n\t    create_folder(os.path.dirname(waveforms_hdf5_path))\n\t    logs_dir = '_logs/pack_waveforms_to_hdf5/{}{}'.format(prefix, get_filename(csv_path))\n\t    create_folder(logs_dir)\n\t    create_logging(logs_dir, filemode='w')\n\t    logging.info('Write logs to {}'.format(logs_dir))\n\t    # Read csv file\n\t    meta_dict = read_metadata(csv_path, classes_num, id_to_ix)\n\t    if mini_data:\n", "        mini_num = 10\n\t        for key in meta_dict.keys():\n\t            meta_dict[key] = meta_dict[key][0 : mini_num]\n\t    audios_num = len(meta_dict['audio_name'])\n\t    # Pack waveform to hdf5\n\t    total_time = time.time()\n\t    with h5py.File(waveforms_hdf5_path, 'w') as hf:\n\t        hf.create_dataset('audio_name', shape=((audios_num,)), dtype='S20')\n\t        hf.create_dataset('waveform', shape=((audios_num, clip_samples)), dtype=np.int16)\n\t        hf.create_dataset('target', shape=((audios_num, classes_num)), dtype=bool)\n", "        hf.attrs.create('sample_rate', data=sample_rate, dtype=np.int32)\n\t        # Pack waveform & target of several audio clips to a single hdf5 file\n\t        for n in range(audios_num):\n\t            audio_path = os.path.join(audios_dir, meta_dict['audio_name'][n])\n\t            if os.path.isfile(audio_path):\n\t                logging.info('{} {}'.format(n, audio_path))\n\t                (audio, _) = librosa.core.load(audio_path, sr=sample_rate, mono=True)\n\t                audio = pad_or_truncate(audio, clip_samples)\n\t                hf['audio_name'][n] = meta_dict['audio_name'][n].encode()\n\t                hf['waveform'][n] = float32_to_int16(audio)\n", "                hf['target'][n] = meta_dict['target'][n]\n\t            else:\n\t                logging.info('{} File does not exist! {}'.format(n, audio_path))\n\t    logging.info('Write to {}'.format(waveforms_hdf5_path))\n\t    logging.info('Pack hdf5 time: {:.3f}'.format(time.time() - total_time))\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    subparsers = parser.add_subparsers(dest='mode')\n\t    parser_pack_wavs = subparsers.add_parser('pack_waveforms_to_hdf5')\n\t    parser_pack_wavs.add_argument('--csv_path', type=str, required=True, help='Path of csv file containing audio info to be downloaded.')\n", "    parser_pack_wavs.add_argument('--audios_dir', type=str, required=True, help='Directory to save out downloaded audio.')\n\t    parser_pack_wavs.add_argument('--waveforms_hdf5_path', type=str, required=True, help='Path to save out packed hdf5.')\n\t    parser_pack_wavs.add_argument('--mini_data', action='store_true', default=False, help='Set true to only download 10 audios for debugging.')\n\t    args = parser.parse_args()\n\t    if args.mode == 'pack_waveforms_to_hdf5':\n\t        pack_waveforms_to_hdf5(args)\n\t    else:\n\t        raise Exception('Incorrect arguments!')\n"]}
{"filename": "panns/create_indexes.py", "chunked_list": ["import numpy as np\n\timport argparse\n\timport csv\n\timport os\n\timport glob\n\timport datetime\n\timport time\n\timport logging\n\timport h5py\n\timport librosa\n", "from uss.config import CLASSES_NUM\n\tdef get_sub_filepaths(folder):\n\t    paths = []\n\t    for root, dirs, files in os.walk(folder):\n\t        for name in files:\n\t            path = os.path.join(root, name)\n\t            paths.append(path)\n\t    return paths\n\tdef create_indexes(args):\n\t    r\"\"\"Create indexes a for dataloader to read for training. When users have \n", "    a new task and their own data, they need to create similar indexes. The \n\t    indexes contain meta information of \"where to find the data for training\".\n\t    \"\"\"\n\t    # Arguments & parameters\n\t    waveforms_hdf5_path = args.waveforms_hdf5_path\n\t    indexes_hdf5_path = args.indexes_hdf5_path\n\t    # Paths\n\t    os.makedirs(os.path.dirname(indexes_hdf5_path), exist_ok=True)\n\t    with h5py.File(waveforms_hdf5_path, 'r') as hr:\n\t        with h5py.File(indexes_hdf5_path, 'w') as hw:\n", "            audios_num = len(hr['audio_name'])\n\t            hw.create_dataset('audio_name', data=hr['audio_name'][:], dtype='S20')\n\t            hw.create_dataset('target', data=hr['target'][:], dtype=bool)\n\t            hw.create_dataset('hdf5_path', data=[waveforms_hdf5_path.encode()] * audios_num, dtype='S200')\n\t            hw.create_dataset('index_in_hdf5', data=np.arange(audios_num), dtype=np.int32)\n\t    print('Write to {}'.format(indexes_hdf5_path))\n\tdef combine_full_indexes(args):\n\t    r\"\"\"Combine all balanced and unbalanced indexes hdf5s to a single hdf5. This \n\t    combined indexes hdf5 is used for training with full data (~20k balanced \n\t    audio clips + ~1.9m unbalanced audio clips).\n", "    \"\"\"\n\t    # Arguments & parameters\n\t    indexes_hdf5s_dir = args.indexes_hdf5s_dir\n\t    full_indexes_hdf5_path = args.full_indexes_hdf5_path\n\t    classes_num = CLASSES_NUM\n\t    # Paths\n\t    paths = get_sub_filepaths(indexes_hdf5s_dir)\n\t    paths = [path for path in paths if (\n\t        'train' in path and 'full_train' not in path and 'mini' not in path)]\n\t    print('Total {} hdf5 to combine.'.format(len(paths)))\n", "    with h5py.File(full_indexes_hdf5_path, 'w') as full_hf:\n\t        full_hf.create_dataset(\n\t            name='audio_name', \n\t            shape=(0,), \n\t            maxshape=(None,), \n\t            dtype='S20')\n\t        full_hf.create_dataset(\n\t            name='target', \n\t            shape=(0, classes_num), \n\t            maxshape=(None, classes_num), \n", "            dtype=bool)\n\t        full_hf.create_dataset(\n\t            name='hdf5_path', \n\t            shape=(0,), \n\t            maxshape=(None,), \n\t            dtype='S200')\n\t        full_hf.create_dataset(\n\t            name='index_in_hdf5', \n\t            shape=(0,), \n\t            maxshape=(None,), \n", "            dtype=np.int32)\n\t        for path in paths:\n\t            with h5py.File(path, 'r') as part_hf:\n\t                print(path)\n\t                n = len(full_hf['audio_name'][:])\n\t                new_n = n + len(part_hf['audio_name'][:])\n\t                full_hf['audio_name'].resize((new_n,))\n\t                full_hf['audio_name'][n : new_n] = part_hf['audio_name'][:]\n\t                full_hf['target'].resize((new_n, classes_num))\n\t                full_hf['target'][n : new_n] = part_hf['target'][:]\n", "                full_hf['hdf5_path'].resize((new_n,))\n\t                full_hf['hdf5_path'][n : new_n] = part_hf['hdf5_path'][:]\n\t                full_hf['index_in_hdf5'].resize((new_n,))\n\t                full_hf['index_in_hdf5'][n : new_n] = part_hf['index_in_hdf5'][:]\n\t    print('Write combined full hdf5 to {}'.format(full_indexes_hdf5_path))\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    subparsers = parser.add_subparsers(dest='mode')\n\t    parser_create_indexes = subparsers.add_parser('create_indexes')\n\t    parser_create_indexes.add_argument('--waveforms_hdf5_path', type=str, required=True, help='Path of packed waveforms hdf5.')\n", "    parser_create_indexes.add_argument('--indexes_hdf5_path', type=str, required=True, help='Path to write out indexes hdf5.')\n\t    parser_combine_full_indexes = subparsers.add_parser('combine_full_indexes')\n\t    parser_combine_full_indexes.add_argument('--indexes_hdf5s_dir', type=str, required=True, help='Directory containing indexes hdf5s to be combined.')\n\t    parser_combine_full_indexes.add_argument('--full_indexes_hdf5_path', type=str, required=True, help='Path to write out full indexes hdf5 file.')\n\t    args = parser.parse_args()\n\t    if args.mode == 'create_indexes':\n\t        create_indexes(args)\n\t    elif args.mode == 'combine_full_indexes':\n\t        combine_full_indexes(args)\n\t    else:\n", "        raise Exception('Incorrect arguments!')\n"]}
{"filename": "uss/parse_ontology.py", "chunked_list": ["import json\n\tfrom typing import List\n\tfrom uss.config import ID_TO_IX, IDS, ROOT_CLASS_ID_DICT\n\tclass Node:\n\t    def __init__(self, data, level):\n\t        r\"\"\"Sound class Node.\n\t        Args:\n\t            data: dict, e.g., {\n\t                \"id\": \"/m/0dgw9r\",\n\t                \"name\": \"Human sounds\",\n", "                \"description\": ...,\n\t                child_ids: [...],\n\t                ...}\n\t        \"\"\"\n\t        self.class_id = data[\"id\"]\n\t        self.data = data\n\t        self.children = []\n\t        self.level = level\n\t    @staticmethod\n\t    def search(node, class_id: str):  # -> Union[Node, None]:\n", "        r\"\"\"Search the node with class_id in the ontology tree.\"\"\"\n\t        if node.class_id == class_id:\n\t            return node\n\t        else:\n\t            for child in node.children:\n\t                result = Node.search(node=child, class_id=class_id)\n\t                if result:\n\t                    return result\n\t        return None\n\t    @staticmethod\n", "    def search_parent(node, class_id: str):  # -> Union[Node, None]:\n\t        r\"\"\"Search the parent of a node with class_id.\"\"\"\n\t        if class_id in node.data[\"child_ids\"]:\n\t            return node\n\t        else:\n\t            for child in node.children:\n\t                result = Node.search_parent(node=child, class_id=class_id)\n\t                if result:\n\t                    return result\n\t        return None\n", "    @staticmethod\n\t    def traverse(node):  # -> List[Node]:\n\t        r\"\"\"Traver all children of a Node including itself.\"\"\"\n\t        nodes = [node]\n\t        for child in node.children:\n\t            nodes.extend(Node.traverse(node=child))\n\t        return nodes\n\tdef get_ontology_tree(ontology_path: str) -> Node:\n\t    r\"\"\"Parse and build the AudioSet ontology tree.\"\"\"\n\t    with open(ontology_path) as f:\n", "        data_list = json.load(f)    # len: 632\n\t    root_class_ids = list(ROOT_CLASS_ID_DICT.keys())\n\t    data = {\n\t        \"id\": \"root\",\n\t        \"name\": \"root\",\n\t        \"child_ids\": root_class_ids,\n\t    }\n\t    root = Node(data=data, level=0)\n\t    for data in data_list:\n\t        # E.g., data: {\"id\": \"/m/0dgw9r\", \"name\": \"Human sounds\",\n", "        # \"description\": ..., child_ids: [...]}\n\t        father = Node.search_parent(node=root, class_id=data[\"id\"])\n\t        child = Node(data=data, level=father.level + 1)\n\t        father.children.append(child)\n\t    return root\n\tdef get_subclass_indexes(root: Node, id: str) -> List[int]:\n\t    r\"\"\"Get class indexes of all children of a node with id=id.\n\t    Args:\n\t        root: Node\n\t        id: str\n", "    Returns:\n\t        index_list: list of int\n\t    \"\"\"\n\t    node = root.search(id=id)\n\t    nodes, layers = root.traverse(node)\n\t    id_list = []\n\t    for node in nodes:\n\t        id_list.append(node.data[\"id\"])\n\t    index_list = []\n\t    for id in id_list:\n", "        if id in IDS:\n\t            index_list.append(ID_TO_IX[id])\n\t    return index_list\n\tif __name__ == \"__main__\":\n\t    get_ontology_tree(ontology_path=\"./metadata/ontology.json\")\n"]}
{"filename": "uss/losses.py", "chunked_list": ["from typing import Callable, Dict\n\timport torch\n\timport torch.nn as nn\n\tfrom torchlibrosa.stft import STFT\n\tfrom uss.models.base import Base\n\tdef l1(output: torch. Tensor, target: torch.Tensor) -> torch.float:\n\t    r\"\"\"L1 distance between the output and target.\"\"\"\n\t    return torch.mean(torch.abs(output - target))\n\tdef l1_wav(output_dict: Dict, target_dict: Dict) -> torch.float:\n\t    r\"\"\"L1 distance between the output waveform and target waveform.\n", "    Args:\n\t        output_dict (Dict): e.g., {\"segment\": (batch_size, segments_num)}\n\t        target_dict (Dict): e.g., {\"segment\": (batch_size, segments_num)}\n\t    Returns:\n\t        loss: torch.float\n\t    \"\"\"\n\t    return l1(output_dict[\"segment\"], target_dict[\"segment\"])\n\tclass L1_Wav_L1_Sp(nn.Module, Base):\n\t    def __init__(self) -> None:\n\t        r\"\"\"Waveform domain L1 and spectrogram domain L1 losses.\"\"\"\n", "        super(L1_Wav_L1_Sp, self).__init__()\n\t        self.window_size = 2048\n\t        hop_size = 320\n\t        center = True\n\t        pad_mode = \"reflect\"\n\t        window = \"hann\"\n\t        self.stft = STFT(\n\t            n_fft=self.window_size,\n\t            hop_length=hop_size,\n\t            win_length=self.window_size,\n", "            window=window,\n\t            center=center,\n\t            pad_mode=pad_mode,\n\t            freeze_parameters=True,\n\t        )\n\t    def __call__(\n\t            self, output_dict, target_dict) -> torch.float:\n\t        r\"\"\"L1 loss in the time-domain and in the spectrogram.\n\t        Args:\n\t            output_dict (Dict): e.g., {\"segment\": (batch_size, segments_num)}\n", "            target_dict (Dict): e.g., {\"segment\": (batch_size, segments_num)}\n\t        Returns:\n\t            loss: torch.float\n\t        \"\"\"\n\t        # L1 loss in the time-domain\n\t        wav_loss = l1_wav(output_dict, target_dict)\n\t        # L1 loss on the spectrogram\n\t        sp_loss = l1(\n\t            self.wav_to_spectrogram(output_dict[\"segment\"], eps=1e-8),\n\t            self.wav_to_spectrogram(target_dict[\"segment\"], eps=1e-8),\n", "        )\n\t        # Total loss\n\t        return wav_loss + sp_loss\n\tdef get_loss_function(loss_type: str) -> Callable:\n\t    r\"\"\"Get loss function.\"\"\"\n\t    if loss_type == \"l1_wav\":\n\t        return l1_wav\n\t    elif loss_type == \"l1_wav_l1_sp\":\n\t        return L1_Wav_L1_Sp()\n\t    else:\n", "        raise NotImplementedError(\"Error!\")\n"]}
{"filename": "uss/inference.py", "chunked_list": ["import argparse\n\timport os\n\timport pickle\n\timport time\n\timport warnings\n\tfrom pathlib import Path\n\tfrom typing import Dict, List\n\timport librosa\n\timport lightning.pytorch as pl\n\timport matplotlib.pyplot as plt\n", "import numpy as np\n\timport soundfile\n\timport torch\n\timport torch.nn as nn\n\tfrom uss.config import (ID_TO_IX, IX_TO_LB, LB_TO_IX, csv_paths_dict,\n\t                        panns_paths_dict)\n\tfrom uss.models.pl_modules import LitSeparation, get_model_class\n\tfrom uss.models.query_nets import initialize_query_net\n\tfrom uss.parse_ontology import Node, get_ontology_tree\n\tfrom uss.utils import (get_audioset632_id_to_lb, get_path,\n", "                       load_pretrained_panns, parse_yaml, remove_silence,\n\t                       repeat_to_length)\n\tdef separate(args) -> None:\n\t    r\"\"\"Do separation for active sound classes.\"\"\"\n\t    # Arguments & parameters\n\t    audio_path = args.audio_path\n\t    levels = args.levels\n\t    class_ids = args.class_ids\n\t    queries_dir = args.queries_dir\n\t    query_emb_path = args.query_emb_path\n", "    config_yaml = args.config_yaml\n\t    checkpoint_path = args.checkpoint_path\n\t    output_dir = args.output_dir\n\t    non_sil_threshold = 1e-6\n\t    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\t    ontology_path = get_path(csv_paths_dict[\"ontology.csv\"])\n\t    configs = parse_yaml(config_yaml)\n\t    sample_rate = configs[\"data\"][\"sample_rate\"]\n\t    segment_seconds = configs[\"data\"][\"segment_seconds\"]\n\t    segment_samples = int(sample_rate * segment_seconds)\n", "    print(\"Using {}.\".format(device))\n\t    # Create directory\n\t    if not output_dir:\n\t        output_dir = os.path.join(\n\t            \"separated_results\",\n\t            Path(audio_path).stem)\n\t    # Load pretrained universal source separation model\n\t    print(\"Loading model ...\")\n\t    warnings.filterwarnings(\"ignore\", category=UserWarning)\n\t    pl_model = load_ss_model(\n", "        configs=configs,\n\t        checkpoint_path=checkpoint_path,\n\t    ).to(device)\n\t    # Load audio\n\t    audio, fs = librosa.load(path=audio_path, sr=sample_rate, mono=True)\n\t    # Load pretrained audio tagging model\n\t    at_model_type = \"Cnn14\"\n\t    at_model = load_pretrained_panns(\n\t        model_type=at_model_type,\n\t        checkpoint_path=get_path(panns_paths_dict[at_model_type]),\n", "        freeze=True,\n\t    ).to(device)\n\t    flag_sum = sum([\n\t        len(levels) > 0,\n\t        len(class_ids) > 0,\n\t        len(queries_dir) > 0,\n\t        len(query_emb_path) > 0,\n\t    ])\n\t    assert flag_sum in [0, 1], \"Please use only `levels` or `class_ids` or \\\n\t        `queries_dir` or `query_emb_path` arguments.\"\n", "    if flag_sum == 0:\n\t        levels = [1, 2, 3]\n\t    print(\"Separating ...\")\n\t    # Separate by hierarchy\n\t    if len(levels) > 0:\n\t        separate_by_hierarchy(\n\t            audio=audio,\n\t            sample_rate=sample_rate,\n\t            segment_samples=segment_samples,\n\t            at_model=at_model,\n", "            pl_model=pl_model,\n\t            device=device,\n\t            levels=levels,\n\t            ontology_path=ontology_path,\n\t            non_sil_threshold=non_sil_threshold,\n\t            output_dir=output_dir\n\t        )\n\t    # Separate by class IDs\n\t    elif len(class_ids) > 0:\n\t        separate_by_class_ids(\n", "            audio=audio,\n\t            sample_rate=sample_rate,\n\t            segment_samples=segment_samples,\n\t            at_model=at_model,\n\t            pl_model=pl_model,\n\t            device=device,\n\t            class_ids=class_ids,\n\t            output_dir=output_dir\n\t        )\n\t    # Calculate query embedding and do separation\n", "    elif len(queries_dir) > 0:\n\t        print(\"Calculate query condition ...\")\n\t        query_time = time.time()\n\t        query_condition = calculate_query_emb(\n\t            queries_dir=queries_dir,\n\t            pl_model=pl_model,\n\t            sample_rate=sample_rate,\n\t            remove_sil=True,\n\t            segment_samples=segment_samples,\n\t        )\n", "        print(\"Time: {:.3f} s\".format(time.time() - query_time))\n\t        pickle_path = os.path.join(\n\t            \"./query_conditions\",\n\t            \"config={}\".format(\n\t                Path(config_yaml).stem),\n\t            \"{}.pkl\".format(\n\t                Path(queries_dir).stem))\n\t        os.makedirs(os.path.dirname(pickle_path), exist_ok=True)\n\t        pickle.dump(query_condition, open(pickle_path, 'wb'))\n\t        print(\"Write query condition to {}\".format(pickle_path))\n", "        output_path = os.path.join(\n\t            output_dir, \"query={}.wav\".format(\n\t                Path(queries_dir).stem))\n\t        separate_by_query_condition(\n\t            audio=audio,\n\t            segment_samples=segment_samples,\n\t            sample_rate=sample_rate,\n\t            query_condition=query_condition,\n\t            pl_model=pl_model,\n\t            output_path=output_path,\n", "        )\n\t    # Load pre-calculated query embedding and do separation\n\t    elif Path(query_emb_path).is_file():\n\t        query_condition = pickle.load(open(query_emb_path, 'rb'))\n\t        output_path = os.path.join(\n\t            output_dir, \"query={}.wav\".format(\n\t                Path('111').stem))\n\t        separate_by_query_condition(\n\t            audio=audio,\n\t            segment_samples=segment_samples,\n", "            sample_rate=sample_rate,\n\t            query_condition=query_condition,\n\t            pl_model=pl_model,\n\t            output_path=output_path,\n\t        )\n\tdef load_ss_model(\n\t    configs: Dict,\n\t    checkpoint_path: str,\n\t) -> nn.Module:\n\t    r\"\"\"Load trained universal source separation model.\n", "    Args:\n\t        configs (Dict)\n\t        checkpoint_path (str): path of the checkpoint to load\n\t        device (str): e.g., \"cpu\" | \"cuda\"\n\t    Returns:\n\t        pl_model: pl.LightningModule\n\t    \"\"\"\n\t    # Initialize query net\n\t    query_net = initialize_query_net(\n\t        configs=configs,\n", "    )\n\t    ss_model_type = configs[\"ss_model\"][\"model_type\"]\n\t    input_channels = configs[\"ss_model\"][\"input_channels\"]\n\t    output_channels = configs[\"ss_model\"][\"output_channels\"]\n\t    condition_size = configs[\"query_net\"][\"outputs_num\"]\n\t    # Initialize separation model\n\t    SsModel = get_model_class(model_type=ss_model_type)\n\t    ss_model = SsModel(\n\t        input_channels=input_channels,\n\t        output_channels=output_channels,\n", "        condition_size=condition_size,\n\t    )\n\t    # Load PyTorch Lightning model\n\t    pl_model = LitSeparation.load_from_checkpoint(\n\t        checkpoint_path=checkpoint_path,\n\t        strict=False,\n\t        ss_model=ss_model,\n\t        anchor_segment_detector=None,\n\t        anchor_segment_mixer=None,\n\t        query_net=query_net,\n", "        loss_function=None,\n\t        optimizer_type=None,\n\t        learning_rate=None,\n\t        lr_lambda_func=None,\n\t        map_location=\"cpu\",\n\t    )\n\t    return pl_model\n\tdef separate_by_hierarchy(\n\t    audio: np.ndarray,\n\t    sample_rate: int,\n", "    segment_samples: int,\n\t    at_model: nn.Module,\n\t    pl_model: pl.LightningModule,\n\t    device: str,\n\t    levels: List[int],\n\t    ontology_path: str,\n\t    non_sil_threshold: float,\n\t    output_dir: str,\n\t) -> None:\n\t    r\"\"\"Separate by hierarchy.\"\"\"\n", "    audioset632_id_to_lb = get_audioset632_id_to_lb(\n\t        ontology_path=ontology_path)\n\t    at_probs = calculate_segment_at_probs(\n\t        audio=audio,\n\t        segment_samples=segment_samples,\n\t        at_model=at_model,\n\t        device=device,\n\t    )\n\t    # at_probs: (segments_num, condition_dim)\n\t    # Parse and build AudioSet ontology tree\n", "    root = get_ontology_tree(ontology_path=ontology_path)\n\t    nodes = Node.traverse(root)\n\t    for level in levels:\n\t        print(\"------ Level {} ------\".format(level))\n\t        nodes_level_n = get_nodes_with_level_n(nodes=nodes, level=level)\n\t        hierarchy_at_probs = []\n\t        for node in nodes_level_n:\n\t            class_id = node.class_id\n\t            subclass_indexes = get_children_indexes(node=node)\n\t            # E.g., [0, 1, ..., 71]\n", "            if len(subclass_indexes) == 0:\n\t                continue\n\t            sep_audio = separate_by_query_conditions(\n\t                audio=audio,\n\t                segment_samples=segment_samples,\n\t                at_probs=at_probs,\n\t                subclass_indexes=subclass_indexes,\n\t                pl_model=pl_model,\n\t                device=device,\n\t            )\n", "            # sep_audio: (audio_samples,)\n\t            # Write out separated audio\n\t            label = audioset632_id_to_lb[class_id]\n\t            output_name = \"{}.wav\".format(label)\n\t            # if label in LB_TO_IX.keys():\n\t            #     output_name = \"classid={}_{}.wav\".format(LB_TO_IX[label], label)\n\t            # else:\n\t            #     output_name = \"classid=unknown_{}.wav\".format(label)\n\t            output_path = os.path.join(\n\t                output_dir,\n", "                \"level={}\".format(level),\n\t                output_name,\n\t            )\n\t            if np.max(sep_audio) > non_sil_threshold:\n\t                write_audio(\n\t                    audio=sep_audio,\n\t                    output_path=output_path,\n\t                    sample_rate=sample_rate,\n\t                )\n\t            hierarchy_at_prob = np.max(at_probs[:, subclass_indexes], axis=-1)\n", "            hierarchy_at_probs.append(hierarchy_at_prob)\n\t        hierarchy_at_probs = np.stack(hierarchy_at_probs, axis=-1)\n\t        plt.matshow(\n\t            hierarchy_at_probs.T,\n\t            origin=\"lower\",\n\t            aspect=\"auto\",\n\t            cmap=\"jet\")\n\t        plt.savefig(\"_zz_{}.pdf\".format(level))\n\tdef separate_by_class_ids(\n\t    audio: np.ndarray,\n", "    sample_rate: int,\n\t    segment_samples: int,\n\t    at_model: nn.Module,\n\t    pl_model: pl.LightningModule,\n\t    device: str,\n\t    class_ids: List[int],\n\t    output_dir: str,\n\t) -> None:\n\t    r\"\"\"Separate by class IDs.\"\"\"\n\t    at_probs = calculate_segment_at_probs(\n", "        audio=audio,\n\t        segment_samples=segment_samples,\n\t        at_model=at_model,\n\t        device=device,\n\t    )\n\t    # at_probs: (segments_num, condition_dim)\n\t    sep_audio = separate_by_query_conditions(\n\t        audio=audio,\n\t        segment_samples=segment_samples,\n\t        at_probs=at_probs,\n", "        subclass_indexes=class_ids,\n\t        pl_model=pl_model,\n\t        device=device,\n\t    )\n\t    # sep_audio: (audio_samples,)\n\t    # Write out separated audio\n\t    output_name = \";\".join(\n\t        [\"{}_{}\".format(class_id, IX_TO_LB[class_id]) for class_id in class_ids])\n\t    output_name += \".wav\"\n\t    output_path = os.path.join(\n", "        output_dir,\n\t        output_name,\n\t    )\n\t    write_audio(\n\t        audio=sep_audio,\n\t        output_path=output_path,\n\t        sample_rate=sample_rate,\n\t    )\n\tdef calculate_query_emb(\n\t    queries_dir: str,\n", "    pl_model: pl.LightningModule,\n\t    sample_rate: int,\n\t    remove_sil: bool,\n\t    segment_samples: int,\n\t    batch_size=8,\n\t) -> np.ndarray:\n\t    r\"\"\"Calculate the query embddings of audio files in a directory.\"\"\"\n\t    audio_names = sorted(os.listdir(queries_dir))\n\t    avg_query_conditions = []\n\t    # Average query conditions of all audios\n", "    for audio_index, audio_name in enumerate(audio_names):\n\t        print(\"{} / {}, {}\".format(audio_index, len(audio_names), audio_name))\n\t        audio_path = os.path.join(queries_dir, audio_name)\n\t        audio, fs = librosa.load(path=audio_path, sr=sample_rate, mono=True)\n\t        # Remove silence\n\t        if remove_sil:\n\t            audio = remove_silence(audio=audio, sample_rate=sample_rate)\n\t        audio_samples = audio.shape[0]\n\t        segments_num = int(np.ceil(audio_samples / segment_samples))\n\t        segments = []\n", "        # Get all segments\n\t        for segment_index in range(segments_num):\n\t            begin_sample = segment_index * segment_samples\n\t            end_sample = begin_sample + segment_samples\n\t            segment = audio[begin_sample: end_sample]\n\t            segment = repeat_to_length(\n\t                audio=segment, segment_samples=segment_samples)\n\t            segments.append(segment)\n\t        if len(segments) == 0:\n\t            continue\n", "        segments = np.stack(segments, axis=0)\n\t        # Calcualte query conditions in mini-batch\n\t        pointer = 0\n\t        query_conditions = []\n\t        while pointer < len(segments):\n\t            batch_segments = segments[pointer: pointer + batch_size]\n\t            query_condition = _do_query_in_minibatch(\n\t                batch_segments=batch_segments,\n\t                query_net=pl_model.query_net,\n\t            )\n", "            query_conditions.extend(query_condition)\n\t            pointer += batch_size\n\t        avg_query_condition = np.mean(query_conditions, axis=0)\n\t        avg_query_conditions.append(avg_query_condition)\n\t    # Average query conditions of all audio files\n\t    avg_query_condition = np.mean(avg_query_conditions, axis=0)\n\t    return avg_query_condition\n\tdef calculate_segment_at_probs(\n\t    audio: np.ndarray,\n\t    segment_samples: int,\n", "    at_model: nn.Module,\n\t    device: str,\n\t) -> np.ndarray:\n\t    r\"\"\"Split audio into short segments. Calcualte the audio tagging\n\t    predictions of all segments.\n\t    Args:\n\t        audio (np.ndarray): (audio_samples,)\n\t        segment_samples (int): short segment duration\n\t        at_model (nn.Module): pretrained audio tagging model\n\t        device (str): \"cpu\" | \"cuda\"\n", "    Returns:\n\t        at_probs (np.ndarray): audio tagging probabilities on all segments,\n\t            (segments_num, classes_num)\n\t    \"\"\"\n\t    audio_samples = audio.shape[-1]\n\t    pointer = 0\n\t    at_probs = []\n\t    while pointer < audio_samples:\n\t        segment = librosa.util.fix_length(\n\t            data=audio[pointer: pointer + segment_samples],\n", "            size=segment_samples,\n\t            axis=0,\n\t        )\n\t        segments = torch.Tensor(segment).unsqueeze(dim=0).to(device)\n\t        # segments: (batch_size=1, segment_samples)\n\t        with torch.no_grad():\n\t            at_model.eval()\n\t            at_prob = at_model(input=segments)[\"clipwise_output\"]\n\t        at_prob = at_prob.squeeze(dim=0).data.cpu().numpy()\n\t        # at_prob: (classes_num,)\n", "        at_probs.append(at_prob)\n\t        pointer += segment_samples\n\t    at_probs = np.stack(at_probs, axis=0)\n\t    # at_probs: (segments_num, condition_dim)\n\t    return at_probs\n\tdef get_nodes_with_level_n(nodes: List[Node], level: int) -> List[Node]:\n\t    r\"\"\"Return nodes with level=N.\"\"\"\n\t    nodes_level_n = []\n\t    for node in nodes:\n\t        if node.level == level:\n", "            nodes_level_n.append(node)\n\t    return nodes_level_n\n\tdef get_children_indexes(node: Node) -> List[int]:\n\t    r\"\"\"Get class indexes of all children of a node.\"\"\"\n\t    nodes_level_n_children = Node.traverse(node=node)\n\t    subclass_indexes = [ID_TO_IX[node.class_id]\n\t                        for node in nodes_level_n_children if node.class_id in ID_TO_IX]\n\t    return subclass_indexes\n\tdef separate_by_query_conditions(\n\t    audio: np.ndarray,\n", "    segment_samples: int,\n\t    at_probs: np.ndarray,\n\t    subclass_indexes: List[int],\n\t    pl_model: pl.LightningModule,\n\t    device: str,\n\t) -> np.ndarray:\n\t    r\"\"\"Do separation for active segments depending on the subclass_indexes.\n\t    Args:\n\t        audio (np.ndarray): audio clip\n\t        segment_samples (int): segment samples\n", "        at_probs (np.ndarray): predicted audio tagging probability on segments,\n\t            (segments_num, classes_num)\n\t        subclass_indexes (List[int]): all values in subclass_indexes are\n\t            remained to build the condition\n\t        pl_model (pl.LightningModule): trained universal source separation model\n\t        device (str), e.g., \"cpu\" | \"cuda\"\n\t    Returns:\n\t        sep_audio (np.ndarray): separated audio\n\t    \"\"\"\n\t    audio_samples = audio.shape[-1]\n", "    at_threshold = 0.2\n\t    batch_size = 8\n\t    segments_num = int(np.ceil(audio_samples / segment_samples))\n\t    active_segment_indexes = []\n\t    active_segments = []\n\t    # Collect active segments\n\t    for segment_index in range(segments_num):\n\t        max_prob = np.max(at_probs[segment_index, subclass_indexes])\n\t        if max_prob >= at_threshold:\n\t            # Only do separation for active segments\n", "            begin_sample = segment_index * segment_samples\n\t            end_sample = begin_sample + segment_samples\n\t            segment = librosa.util.fix_length(\n\t                data=audio[begin_sample: end_sample],\n\t                size=segment_samples,\n\t                axis=0,\n\t            )\n\t            active_segments.append(segment)\n\t            active_segment_indexes.append(segment_index)\n\t    if len(active_segments) > 0:\n", "        active_segments = np.stack(active_segments, axis=0)\n\t        active_segment_indexes = np.stack(active_segment_indexes, axis=0)\n\t    # Do separation in mini-batch\n\t    pointer = 0\n\t    active_sep_segments = []\n\t    while pointer < len(active_segments):\n\t        batch_segments = active_segments[pointer: pointer + batch_size]\n\t        batch_sep_segments = _do_sep_by_id_in_minibatch(\n\t            batch_segments=batch_segments,\n\t            subclass_indexes=subclass_indexes,\n", "            pl_model=pl_model,\n\t        )\n\t        active_sep_segments.extend(batch_sep_segments)\n\t        pointer += batch_size\n\t    # Get separated segments\n\t    sep_segments = np.zeros((segments_num, segment_samples))\n\t    for i in range(len(active_segment_indexes)):\n\t        sep_segments[active_segment_indexes[i]] = active_sep_segments[i]\n\t    sep_audio = sep_segments.flatten()[0: audio_samples]\n\t    return sep_audio\n", "def separate_by_query_condition(\n\t    audio: np.ndarray,\n\t    segment_samples: int,\n\t    sample_rate: int,\n\t    query_condition: np.ndarray,\n\t    pl_model: pl.LightningModule,\n\t    output_path: str,\n\t    batch_size: int = 8,\n\t) -> np.ndarray:\n\t    r\"\"\"Do separation for active segments depending on the subclass_indexes.\n", "    Args:\n\t        audio (np.ndarray): audio clip\n\t        segment_samples (int): segment samples\n\t        at_probs (np.ndarray): predicted audio tagging probability on segments,\n\t            (segments_num, classes_num)\n\t        subclass_indexes (List[int]): all values in subclass_indexes are\n\t            remained to build the condition\n\t        pl_model (pl.LightningModule): trained universal source separation model\n\t        device (str), e.g., \"cpu\" | \"cuda\"\n\t    Returns:\n", "        sep_audio (np.ndarray): separated audio\n\t    \"\"\"\n\t    audio_samples = audio.shape[-1]\n\t    segments_num = int(np.ceil(audio_samples / segment_samples))\n\t    segments = []\n\t    # Collect active segments\n\t    for segment_index in range(segments_num):\n\t        begin_sample = segment_index * segment_samples\n\t        end_sample = begin_sample + segment_samples\n\t        segment = librosa.util.fix_length(\n", "            data=audio[begin_sample: end_sample],\n\t            size=segment_samples,\n\t            axis=0,\n\t        )\n\t        segments.append(segment)\n\t    segments = np.stack(segments, axis=0)\n\t    # Do separation in mini-batch\n\t    pointer = 0\n\t    sep_segments = []\n\t    while pointer < len(segments):\n", "        batch_segments = segments[pointer: pointer + batch_size]\n\t        batch_sep_segments = _do_sep_by_query_in_minibatch(\n\t            batch_segments=batch_segments,\n\t            query_condition=query_condition,\n\t            ss_model=pl_model.ss_model,\n\t        )\n\t        sep_segments.extend(batch_sep_segments)\n\t        pointer += batch_size\n\t    sep_segments = np.concatenate(sep_segments, axis=0)\n\t    sep_audio = sep_segments.flatten()[0: audio_samples]\n", "    if output_path:\n\t        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\t        soundfile.write(\n\t            file=output_path,\n\t            data=sep_audio,\n\t            samplerate=sample_rate)\n\t        print(\"Write out separated file to {}\".format(output_path))\n\t    return sep_audio\n\tdef _do_sep_by_id_in_minibatch(\n\t    batch_segments: np.ndarray,\n", "    subclass_indexes: List[int],\n\t    pl_model: pl.LightningModule,\n\t) -> np.ndarray:\n\t    r\"\"\"Separate by class IDs in mini-batch.\n\t    Args:\n\t        batch_segments (np.ndarray): (batch_size, segment_samples)\n\t        subclass_indexes (List[int]): a list of subclasses\n\t        pl_model (pl.LightningModule): universal separation model\n\t    Returns:\n\t        batch_sep_segments (np.ndarray): separated mini-batch segments\n", "    \"\"\"\n\t    device = pl_model.device\n\t    batch_segments = torch.Tensor(batch_segments).to(device)\n\t    # shape: (batch_size, segment_samples)\n\t    with torch.no_grad():\n\t        pl_model.query_net.eval()\n\t        bottleneck = pl_model.query_net.forward_base(source=batch_segments)\n\t        # bottleneck: (batch_size, bottleneck_dim)\n\t        masked_bottleneck = torch.zeros_like(bottleneck)\n\t        masked_bottleneck[:,\n", "                          subclass_indexes] = bottleneck[:,\n\t                                                         subclass_indexes]\n\t        condition = pl_model.query_net.forward_adaptor(masked_bottleneck)\n\t        # condition: (batch_size, condition_dim)\n\t    input_dict = {\n\t        \"mixture\": torch.Tensor(batch_segments.unsqueeze(1)),\n\t        \"condition\": torch.Tensor(condition),\n\t    }\n\t    with torch.no_grad():\n\t        pl_model.ss_model.eval()\n", "        output_dict = pl_model.ss_model(input_dict=input_dict)\n\t    batch_sep_segments = output_dict[\"waveform\"].squeeze(\n\t        dim=1).data.cpu().numpy()\n\t    # (batch_size, segment_samples)\n\t    return batch_sep_segments\n\tdef _do_sep_by_query_in_minibatch(\n\t    batch_segments: np.ndarray,\n\t    query_condition: np.ndarray,\n\t    ss_model: nn.Module,\n\t) -> np.ndarray:\n", "    r\"\"\"Separate by query condition in mini-batch.\n\t    Args:\n\t        batch_segments (np.ndarray): (batch_size, segment_samples)\n\t        query_condition (np.ndarray): (batch_size, embedding_dim)\n\t        pl_model (pl.LightningModule): universal separation model\n\t    Returns:\n\t        batch_sep_segments (np.ndarray): separated mini-batch segments\n\t    \"\"\"\n\t    device = next(ss_model.parameters()).device\n\t    batch_segments = torch.Tensor(batch_segments).to(device).unsqueeze(dim=1)\n", "    # shape: (batch_size, 1, segment_samples)\n\t    query_condition = torch.Tensor(query_condition).to(device).unsqueeze(dim=0)\n\t    input_dict = {\n\t        \"mixture\": batch_segments,\n\t        \"condition\": query_condition,\n\t    }\n\t    with torch.no_grad():\n\t        ss_model.eval()\n\t        output_dict = ss_model(input_dict=input_dict)\n\t    batch_sep_segments = output_dict[\"waveform\"].squeeze(\n", "        dim=1).data.cpu().numpy()\n\t    # (batch_size, segment_samples)\n\t    return batch_sep_segments\n\tdef _do_query_in_minibatch(\n\t    batch_segments: np.ndarray,\n\t    query_net: nn.Module,\n\t) -> np.ndarray:\n\t    r\"\"\"Separate by mini-batch.\n\t    Args:\n\t        batch_segments (np.ndarray): (batch_size, segment_samples)\n", "        pl_model (pl.LightningModule): universal separation model\n\t    Returns:\n\t        batch_condition (np.ndarray): mini-batch conditions.\n\t    \"\"\"\n\t    device = next(query_net.parameters()).device\n\t    batch_segments = torch.Tensor(batch_segments).to(device)\n\t    # shape: (batch_size, segment_samples)\n\t    with torch.no_grad():\n\t        query_net.eval()\n\t        batch_condition = query_net.forward(source=batch_segments)[\"output\"]\n", "        # condition: (batch_size, condition_dim)\n\t    batch_condition = batch_condition.data.cpu().numpy()\n\t    return batch_condition\n\tdef write_audio(\n\t    audio: np.ndarray,\n\t    output_path: str,\n\t    sample_rate: int,\n\t):\n\t    r\"\"\"Write audio to disk.\n\t    Args:\n", "        audio (np.ndarray): audio to write out\n\t        output_path (str): path to write out the audio\n\t        sample_rate (int)\n\t    Returns:\n\t        None\n\t    \"\"\"\n\t    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\t    soundfile.write(file=output_path, data=audio, samplerate=sample_rate)\n\t    print(\"Write out to {}\".format(output_path))\n\tif __name__ == \"__main__\":\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--audio_path\", type=str, required=True)\n\t    parser.add_argument(\"--levels\", nargs=\"*\", type=int, default=[])\n\t    parser.add_argument(\"--class_ids\", nargs=\"*\", type=int, default=[])\n\t    parser.add_argument(\"--queries_dir\", type=str, default=\"\")\n\t    parser.add_argument(\"--query_emb_path\", type=str, default=\"\")\n\t    parser.add_argument(\"--config_yaml\", type=str, default=\"\")\n\t    parser.add_argument(\"--checkpoint_path\", type=str, default=\"\")\n\t    parser.add_argument(\"--output_dir\", type=str)\n\t    args = parser.parse_args()\n", "    separate(args)\n"]}
{"filename": "uss/train.py", "chunked_list": ["import argparse\n\timport logging\n\timport os\n\timport pathlib\n\tfrom typing import List\n\timport lightning.pytorch as pl\n\timport torch\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom uss.callbacks.base import CheckpointEveryNSteps\n\tfrom uss.callbacks.evaluate import EvaluateCallback\n", "from uss.config import CLIP_SECONDS, FRAMES_PER_SECOND, panns_paths_dict\n\tfrom uss.data.anchor_segment_detectors import AnchorSegmentDetector\n\tfrom uss.data.anchor_segment_mixers import AnchorSegmentMixer\n\tfrom uss.data.datamodules import DataModule\n\tfrom uss.data.datasets import Dataset\n\tfrom uss.data.samplers import BalancedSampler\n\tfrom uss.losses import get_loss_function\n\tfrom uss.models.pl_modules import LitSeparation, get_model_class\n\tfrom uss.models.query_nets import initialize_query_net\n\tfrom uss.optimizers.lr_schedulers import get_lr_lambda\n", "from uss.utils import (create_logging, get_path, load_pretrained_panns,\n\t                       parse_yaml)\n\tdef train(args) -> None:\n\t    r\"\"\"Train, evaluate, and save checkpoints.\n\t    Args:\n\t        workspace (str): directory of workspace\n\t        config_yaml (str): config yaml path\n\t    Returns:\n\t        None\n\t    \"\"\"\n", "    # Arguments & parameters\n\t    workspace = args.workspace\n\t    config_yaml = args.config_yaml\n\t    filename = args.filename\n\t    # GPUs number\n\t    devices_num = torch.cuda.device_count()\n\t    # Read config file\n\t    configs = parse_yaml(config_yaml)\n\t    # Configurations of pretrained sound event detection model from PANNs\n\t    sed_model_type = configs[\"sound_event_detection\"][\"model_type\"]\n", "    # Configuration of data to train the universal source separation system\n\t    clip_seconds = CLIP_SECONDS\n\t    frames_per_second = FRAMES_PER_SECOND\n\t    sample_rate = configs[\"data\"][\"sample_rate\"]\n\t    classes_num = configs[\"data\"][\"classes_num\"]\n\t    segment_seconds = configs[\"data\"][\"segment_seconds\"]\n\t    anchor_segment_detect_mode = configs[\"data\"][\"anchor_segment_detect_mode\"]\n\t    mix_num = configs[\"data\"][\"mix_num\"]\n\t    match_energy = configs[\"data\"][\"augmentation\"][\"match_energy\"]\n\t    # Configuration of the universal source separation model\n", "    ss_model_type = configs[\"ss_model\"][\"model_type\"]\n\t    input_channels = configs[\"ss_model\"][\"input_channels\"]\n\t    output_channels = configs[\"ss_model\"][\"output_channels\"]\n\t    condition_size = configs[\"query_net\"][\"outputs_num\"]\n\t    # Configuration of the trainer\n\t    num_workers = configs[\"train\"][\"num_workers\"]\n\t    loss_type = configs[\"train\"][\"loss_type\"]\n\t    optimizer_type = configs[\"train\"][\"optimizer\"][\"optimizer_type\"]\n\t    learning_rate = float(configs[\"train\"][\"optimizer\"][\"learning_rate\"])\n\t    lr_lambda_type = configs[\"train\"][\"optimizer\"][\"lr_lambda_type\"]\n", "    warm_up_steps = configs[\"train\"][\"optimizer\"][\"warm_up_steps\"]\n\t    reduce_lr_steps = configs[\"train\"][\"optimizer\"][\"reduce_lr_steps\"]\n\t    save_step_frequency = configs[\"train\"][\"save_step_frequency\"]\n\t    evaluate_step_frequency = configs[\"train\"][\"evaluate_step_frequency\"]\n\t    resume_checkpoint_path = configs[\"train\"][\"resume_checkpoint_path\"]\n\t    if resume_checkpoint_path == \"\":\n\t        resume_checkpoint_path = None\n\t    # Configuration of the evaluation\n\t    balanced_train_eval_dir = os.path.join(\n\t        workspace, configs[\"evaluate\"][\"balanced_train_eval_dir\"])\n", "    test_eval_dir = os.path.join(\n\t        workspace, configs[\"evaluate\"][\"test_eval_dir\"])\n\t    max_eval_per_class = configs[\"evaluate\"][\"max_eval_per_class\"]\n\t    # Get directories and paths\n\t    checkpoints_dir, logs_dir, tf_logs_dir, statistics_path = get_dirs(\n\t        workspace, filename, config_yaml, devices_num,\n\t    )\n\t    # Create a PyTorch Lightning datamodule\n\t    datamodule = get_datamodule(\n\t        workspace=workspace,\n", "        config_yaml=config_yaml,\n\t        num_workers=num_workers,\n\t        devices_num=devices_num,\n\t    )\n\t    sed_model = load_pretrained_panns(\n\t        model_type=sed_model_type,\n\t        checkpoint_path=get_path(panns_paths_dict[sed_model_type]),\n\t        freeze=True,\n\t    )\n\t    # Initialize query net\n", "    query_net = initialize_query_net(\n\t        configs=configs,\n\t    )\n\t    # Initialize separation model\n\t    SsModel = get_model_class(model_type=ss_model_type)\n\t    ss_model = SsModel(\n\t        input_channels=input_channels,\n\t        output_channels=output_channels,\n\t        condition_size=condition_size,\n\t    )\n", "    # Loss function\n\t    loss_function = get_loss_function(loss_type=loss_type)\n\t    # Anchor segment detector\n\t    anchor_segment_detector = AnchorSegmentDetector(\n\t        sed_model=sed_model,\n\t        clip_seconds=clip_seconds,\n\t        segment_seconds=segment_seconds,\n\t        frames_per_second=frames_per_second,\n\t        sample_rate=sample_rate,\n\t        detect_mode=anchor_segment_detect_mode,\n", "    )\n\t    # Anchor segment mixer\n\t    anchor_segment_mixer = AnchorSegmentMixer(\n\t        mix_num=mix_num,\n\t        match_energy=match_energy,\n\t    )\n\t    # Learning rate scaler\n\t    lr_lambda_func = get_lr_lambda(\n\t        lr_lambda_type=lr_lambda_type,\n\t        warm_up_steps=warm_up_steps,\n", "        reduce_lr_steps=reduce_lr_steps,\n\t    )\n\t    # PyTorch Lightning model\n\t    pl_model = LitSeparation(\n\t        ss_model=ss_model,\n\t        anchor_segment_detector=anchor_segment_detector,\n\t        anchor_segment_mixer=anchor_segment_mixer,\n\t        query_net=query_net,\n\t        loss_function=loss_function,\n\t        optimizer_type=optimizer_type,\n", "        learning_rate=learning_rate,\n\t        lr_lambda_func=lr_lambda_func,\n\t    )\n\t    # Checkpoint\n\t    checkpoint_every_n_steps = CheckpointEveryNSteps(\n\t        checkpoints_dir=checkpoints_dir,\n\t        save_step_frequency=save_step_frequency,\n\t    )\n\t    # Summary writer\n\t    summary_writer = SummaryWriter(log_dir=tf_logs_dir)\n", "    # Evaluation callback\n\t    evaluate_callback = EvaluateCallback(\n\t        pl_model=pl_model,\n\t        balanced_train_eval_dir=balanced_train_eval_dir,\n\t        test_eval_dir=test_eval_dir,\n\t        classes_num=classes_num,\n\t        max_eval_per_class=max_eval_per_class,\n\t        evaluate_step_frequency=evaluate_step_frequency,\n\t        summary_writer=summary_writer,\n\t        statistics_path=statistics_path,\n", "    )\n\t    # All callbacks\n\t    callbacks = [checkpoint_every_n_steps, evaluate_callback]\n\t    trainer = pl.Trainer(\n\t        accelerator=\"auto\",\n\t        devices=\"auto\",\n\t        num_nodes=1,\n\t        precision=\"32-true\",\n\t        logger=None,\n\t        callbacks=callbacks,\n", "        fast_dev_run=False,\n\t        max_epochs=-1,\n\t        use_distributed_sampler=False,\n\t        sync_batchnorm=True,\n\t        num_sanity_val_steps=2,\n\t        enable_checkpointing=False,\n\t        enable_progress_bar=True,\n\t        enable_model_summary=True,\n\t        strategy=\"ddp_find_unused_parameters_true\",\n\t    )\n", "    # Fit, evaluate, and save checkpoints\n\t    trainer.fit(\n\t        model=pl_model,\n\t        train_dataloaders=None,\n\t        val_dataloaders=None,\n\t        datamodule=datamodule,\n\t        ckpt_path=resume_checkpoint_path,\n\t    )\n\tdef get_dirs(\n\t    workspace: str,\n", "    filename: str,\n\t    config_yaml: str,\n\t    devices_num: int\n\t) -> List[str]:\n\t    r\"\"\"Get directories and paths.\n\t    Args:\n\t        workspace (str): directory of workspace\n\t        filename (str): filename of current .py file.\n\t        config_yaml (str): config yaml path\n\t        devices_num (int): 0 for cpu and 8 for training with 8 GPUs\n", "    Returns:\n\t        checkpoints_dir (str): directory to save checkpoints\n\t        logs_dir (str), directory to save logs\n\t        tf_logs_dir (str), directory to save TensorBoard logs\n\t        statistics_path (str), directory to save statistics\n\t    \"\"\"\n\t    yaml_name = pathlib.Path(config_yaml).stem\n\t    # Directory to save checkpoints\n\t    checkpoints_dir = os.path.join(\n\t        workspace,\n", "        \"checkpoints\",\n\t        filename,\n\t        \"{},devices={}\".format(yaml_name, devices_num),\n\t    )\n\t    os.makedirs(checkpoints_dir, exist_ok=True)\n\t    # Directory to save logs\n\t    logs_dir = os.path.join(\n\t        workspace,\n\t        \"logs\",\n\t        filename,\n", "        \"{},devices={}\".format(yaml_name, devices_num),\n\t    )\n\t    os.makedirs(logs_dir, exist_ok=True)\n\t    # Directory to save TensorBoard logs\n\t    create_logging(logs_dir, filemode=\"w\")\n\t    logging.info(args)\n\t    tf_logs_dir = os.path.join(\n\t        workspace,\n\t        \"tf_logs\",\n\t        filename,\n", "        \"{},devices={}\".format(yaml_name, devices_num),\n\t    )\n\t    # Directory to save statistics\n\t    statistics_path = os.path.join(\n\t        workspace,\n\t        \"statistics\",\n\t        filename,\n\t        \"{},devices={}\".format(yaml_name, devices_num),\n\t        \"statistics.pkl\",\n\t    )\n", "    os.makedirs(os.path.dirname(statistics_path), exist_ok=True)\n\t    return checkpoints_dir, logs_dir, tf_logs_dir, statistics_path\n\tdef get_datamodule(\n\t    workspace: str,\n\t    config_yaml: str,\n\t    num_workers: int,\n\t    devices_num: int,\n\t) -> DataModule:\n\t    r\"\"\"Create a PyTorch Lightning datamodule for yielding mini-batches of data.\n\t    Args:\n", "        workspace (str): directory of workspace\n\t        config_yaml (str): config yaml path\n\t        num_workers (int): e.g., 16 for using multiple cpu cores for preparing\n\t            data in parallel\n\t        devices_num (int): the number of GPUs to run\n\t    Returns:\n\t        datamodule: DataModule\n\t    Examples::\n\t        >>> data_module.setup()\n\t        >>> for batch_data_dict in datamodule:\n", "        >>>     print(batch_data_dict.keys())\n\t        >>>     break\n\t    \"\"\"\n\t    # Read configs\n\t    configs = parse_yaml(config_yaml)\n\t    indexes_hdf5_path = os.path.join(\n\t        workspace, configs[\"data\"][\"indexes_dict\"])\n\t    batch_size = configs[\"train\"][\"batch_size_per_device\"] * devices_num\n\t    steps_per_epoch = configs[\"train\"][\"steps_per_epoch\"]\n\t    # dataset\n", "    train_dataset = Dataset(\n\t        steps_per_epoch=steps_per_epoch,\n\t    )\n\t    # sampler\n\t    train_sampler = BalancedSampler(\n\t        indexes_hdf5_path=indexes_hdf5_path,\n\t        batch_size=batch_size,\n\t        steps_per_epoch=steps_per_epoch,\n\t    )\n\t    # data module\n", "    data_module = DataModule(\n\t        train_sampler=train_sampler,\n\t        train_dataset=train_dataset,\n\t        num_workers=num_workers,\n\t    )\n\t    return data_module\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--workspace\", type=str, required=True, help=\"Directory of workspace.\"\n", "    )\n\t    parser.add_argument(\n\t        \"--config_yaml\",\n\t        type=str,\n\t        required=True,\n\t        help=\"Path of config file for training.\",\n\t    )\n\t    args = parser.parse_args()\n\t    args.filename = pathlib.Path(__file__).stem\n\t    train(args)\n"]}
{"filename": "uss/config.py", "chunked_list": ["from pathlib import Path\n\timport pandas as pd\n\tfrom uss.utils import get_path\n\tcsv_paths_dict = {\n\t    \"class_labels_indices.csv\": {\n\t        \"path\": Path(Path.home(), \".cache/uss/metadata/class_labels_indices.csv\"),\n\t        \"remote_path\": \"https://sandbox.zenodo.org/record/1186898/files/class_labels_indices.csv?download=1\",\n\t        \"size\": 14675,\n\t    },\n\t    \"ontology.csv\": {\n", "        \"path\": Path(Path.home(), \".cache/uss/metadata/ontology.json\"),\n\t        \"remote_path\": \"https://sandbox.zenodo.org/record/1186898/files/ontology.json?download=1\",\n\t        \"size\": 342780,\n\t    },\n\t}\n\tpanns_paths_dict = {\n\t    \"Cnn14\": {\n\t        \"path\": Path(Path.home(), \".cache/panns/Cnn14_mAP=0.431.pth\"),\n\t        \"remote_path\": \"https://zenodo.org/record/3987831/files/Cnn14_mAP%3D0.431.pth?download=1\",\n\t        \"size\": 327428481,\n", "    },\n\t    \"Cnn14_DecisionLevelMax\": {\n\t        \"path\": Path(Path.home(), \".cache/panns/Cnn14_DecisionLevelMax_mAP=0.385.pth\"),\n\t        \"remote_path\": \"https://zenodo.org/record/3987831/files/Cnn14_DecisionLevelMax_mAP%3D0.385.pth?download=1\",\n\t        \"size\": 327428481,\n\t    },\n\t}\n\tSAMPLE_RATE = 32000\n\tCLIP_SECONDS = 10.\n\tCLIP_SAMPLES = int(SAMPLE_RATE * CLIP_SECONDS)\n", "FRAMES_PER_SECOND = 100\n\t# Parse metadata\n\tmeta_csv_path = get_path(meta=csv_paths_dict[\"class_labels_indices.csv\"])\n\tdf = pd.read_csv(meta_csv_path, sep=',')\n\tIDS = df[\"mid\"].tolist()\n\tLABELS = df[\"display_name\"].tolist()\n\tCLASSES_NUM = len(LABELS)\n\tLB_TO_IX = {label: i for i, label in enumerate(LABELS)}\n\tIX_TO_LB = {i: label for i, label in enumerate(LABELS)}\n\tID_TO_IX = {id: i for i, id in enumerate(IDS)}\n", "IX_TO_ID = {i: id for i, id in enumerate(IDS)}\n\tROOT_CLASS_ID_DICT = {\n\t    \"/m/0dgw9r\": \"Human sounds\",\n\t    \"/m/0jbk\": \"Animal\",\n\t    \"/m/04rlf\": \"Music\",\n\t    \"/m/059j3w\": \"Natural sounds\",\n\t    \"/t/dd00041\": \"Sounds of things\",\n\t    \"/t/dd00098\": \"Source-ambiguous sounds\",\n\t    \"/t/dd00123\": \"Channel, environment and background\",\n\t}\n"]}
{"filename": "uss/evaluate.py", "chunked_list": ["import logging\n\timport os\n\timport pathlib\n\timport re\n\tfrom typing import Dict, List\n\timport librosa\n\timport lightning.pytorch as pl\n\timport numpy as np\n\timport torch\n\tfrom uss.config import IX_TO_LB\n", "from uss.inference import load_ss_model\n\tfrom uss.utils import (calculate_sdr, create_logging, get_mean_sdr_from_dict,\n\t                       parse_yaml)\n\tclass AudioSetEvaluator:\n\t    def __init__(\n\t        self,\n\t        audios_dir: str,\n\t        classes_num: int,\n\t        max_eval_per_class=None,\n\t    ) -> None:\n", "        r\"\"\"AudioSet evaluator.\n\t        Args:\n\t            audios_dir (str): directory of evaluation segments\n\t            classes_num (int): the number of sound classes\n\t            max_eval_per_class (int), the number of samples to evaluate for each sound class\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        self.audios_dir = audios_dir\n\t        self.classes_num = classes_num\n", "        self.max_eval_per_class = max_eval_per_class\n\t    @torch.no_grad()\n\t    def __call__(\n\t        self,\n\t        pl_model: pl.LightningModule\n\t    ) -> Dict:\n\t        r\"\"\"Evalute.\"\"\"\n\t        sdrs_dict = {class_id: [] for class_id in range(self.classes_num)}\n\t        sdris_dict = {class_id: [] for class_id in range(self.classes_num)}\n\t        for class_id in range(self.classes_num):\n", "            sub_dir = os.path.join(\n\t                self.audios_dir,\n\t                \"class_id={}\".format(class_id))\n\t            audio_names = self._get_audio_names(audios_dir=sub_dir)\n\t            for audio_index, audio_name in enumerate(audio_names):\n\t                if audio_index == self.max_eval_per_class:\n\t                    break\n\t                source_path = os.path.join(\n\t                    sub_dir, \"{},source.wav\".format(audio_name))\n\t                mixture_path = os.path.join(\n", "                    sub_dir, \"{},mixture.wav\".format(audio_name))\n\t                source, fs = librosa.load(source_path, sr=None, mono=True)\n\t                mixture, fs = librosa.load(mixture_path, sr=None, mono=True)\n\t                sdr_no_sep = calculate_sdr(ref=source, est=mixture)\n\t                device = pl_model.device\n\t                conditions = pl_model.query_net(\n\t                    source=torch.Tensor(source)[None, :].to(device),\n\t                )[\"output\"]\n\t                # conditions: (batch_size=1, condition_dim)\n\t                input_dict = {\n", "                    \"mixture\": torch.Tensor(mixture)[None, None, :].to(device),\n\t                    \"condition\": conditions,\n\t                }\n\t                pl_model.eval()\n\t                sep_segment = pl_model.ss_model(input_dict)[\"waveform\"]\n\t                # sep_segment: (batch_size=1, channels_num=1, segment_samples)\n\t                sep_segment = sep_segment.squeeze(\n\t                    dim=(0, 1)).data.cpu().numpy()\n\t                # sep_segment: (segment_samples,)\n\t                sdr = calculate_sdr(ref=source, est=sep_segment)\n", "                sdri = sdr - sdr_no_sep\n\t                sdrs_dict[class_id].append(sdr)\n\t                sdris_dict[class_id].append(sdri)\n\t            logging.info(\n\t                \"Class ID: {} / {}, SDR: {:.3f}, SDRi: {:.3f}\".format(\n\t                    class_id, self.classes_num, np.mean(\n\t                        sdrs_dict[class_id]), np.mean(\n\t                        sdris_dict[class_id])))\n\t        stats_dict = {\n\t            \"sdrs_dict\": sdrs_dict,\n", "            \"sdris_dict\": sdris_dict,\n\t        }\n\t        return stats_dict\n\t    def _get_audio_names(self, audios_dir: str) -> List[str]:\n\t        r\"\"\"Get evaluation audio names.\"\"\"\n\t        audio_names = sorted(os.listdir(audios_dir))\n\t        audio_names = [\n\t            re.search(\n\t                \"(.*),(mixture|source).wav\",\n\t                audio_name).group(1) for audio_name in audio_names]\n", "        audio_names = sorted(list(set(audio_names)))\n\t        return audio_names\n\t    @staticmethod\n\t    def get_median_metrics(stats_dict, metric_type):\n\t        class_ids = stats_dict[metric_type].keys()\n\t        median_stats_dict = {\n\t            class_id: np.nanmedian(\n\t                stats_dict[metric_type][class_id]) for class_id in class_ids}\n\t        return median_stats_dict\n\tdef test_evaluate(config_yaml: str, workspace: str):\n", "    r\"\"\"Evaluate using pretrained checkpoint.\n\t    Args:\n\t        config_yaml (str), path of the config yaml file\n\t        workspace (str), directory of workspace\n\t    Returns:\n\t        None\n\t    \"\"\"\n\t    device = \"cuda\"\n\t    configs = parse_yaml(config_yaml)\n\t    classes_num = configs[\"data\"][\"classes_num\"]\n", "    audios_dir = os.path.join(\n\t        workspace, \"evaluation/audioset/2s_segments_test\")\n\t    create_logging(\"_tmp_log\", filemode=\"w\")\n\t    # Evlauator\n\t    evaluator = AudioSetEvaluator(\n\t        audios_dir=audios_dir,\n\t        classes_num=classes_num,\n\t        max_eval_per_class=10\n\t    )\n\t    steps = [1]\n", "    for step in steps:\n\t        # Checkpoint path\n\t        checkpoint_path = os.path.join(\n\t            workspace, \"checkpoints/train/config={},devices=1/step={}.ckpt\".format(\n\t                pathlib.Path(config_yaml).stem, step))\n\t        # Load model\n\t        pl_model = load_ss_model(\n\t            configs=configs,\n\t            checkpoint_path=checkpoint_path\n\t        ).to(device)\n", "        # Evaluate statistics\n\t        stats_dict = evaluator(pl_model=pl_model)\n\t        median_sdris = {}\n\t        for class_id in range(classes_num):\n\t            median_sdris[class_id] = np.nanmedian(\n\t                stats_dict[\"sdris_dict\"][class_id])\n\t            print(\n\t                \"{} {}: {:.3f}\".format(\n\t                    class_id,\n\t                    IX_TO_LB[class_id],\n", "                    median_sdris[class_id]))\n\t        mean_sdri = get_mean_sdr_from_dict(median_sdris)\n\t        # final_sdri = np.nanmean([mean_sdris[class_id]\n\t        # for class_id in range(classes_num)])\n\t        print(\"--------\")\n\t        print(\"Average SDRi: {:.3f}\".format(mean_sdri))\n\tif __name__ == \"__main__\":\n\t    test_evaluate(\n\t        config_yaml=\"./scripts/train/ss_model=resunet30,querynet=at_soft,gpus=1.yaml\",\n\t        workspace=\"./workspaces/uss\",\n", "    )\n"]}
{"filename": "uss/uss_inference.py", "chunked_list": ["import argparse\n\tfrom pathlib import Path\n\tfrom uss.inference import separate\n\tfrom uss.utils import get_path\n\tmodel_paths_dict = {\n\t    \"at_soft\": {\n\t        \"config_yaml\": {\n\t            \"path\": Path(Path.home(), \".cache/uss/scripts/ss_model=resunet30,querynet=at_soft,data=full.yaml\"),\n\t            \"remote_path\": \"https://huggingface.co/RSNuts/Universal_Source_Separation/resolve/main/uss_material/ss_model%3Dresunet30%2Cquerynet%3Dat_soft%2Cdata%3Dfull.yaml?download=1\",\n\t            \"size\": 1558,\n", "        },\n\t        \"checkpoint\": {\n\t            \"path\": Path(Path.home(), \".cache/uss/checkpoints/ss_model=resunet30,querynet=at_soft,data=full,devices=8,step=1000000.ckpt\"),\n\t            \"remote_path\": \"https://huggingface.co/RSNuts/Universal_Source_Separation/resolve/main/uss_material/ss_model%3Dresunet30%2Cquerynet%3Dat_soft%2Cdata%3Dfull%2Cdevices%3D8%2Cstep%3D1000000.ckpt\",\n\t            \"size\": 1121024828,\n\t        },\n\t    }\n\t}\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n", "    parser.add_argument(\"-i\", \"--audio_path\", type=str, required=True)\n\t    parser.add_argument(\n\t        \"-c\",\n\t        \"--condition_type\",\n\t        type=str,\n\t        default=\"at_soft\",\n\t        choices=[\n\t            \"at_soft\",\n\t            \"embedding\"])\n\t    parser.add_argument(\"--levels\", nargs=\"*\", type=int, default=[])\n", "    parser.add_argument(\"--class_ids\", nargs=\"*\", type=int, default=[])\n\t    parser.add_argument(\"--queries_dir\", type=str, default=\"\")\n\t    parser.add_argument(\"--query_emb_path\", type=str, default=\"\")\n\t    parser.add_argument(\"--output_dir\", type=str, default=\"\")\n\t    args = parser.parse_args()\n\t    condition_type = args.condition_type\n\t    # Use default pretrained models\n\t    if condition_type == \"at_soft\":\n\t        args.config_yaml = get_path(\n\t            meta=model_paths_dict[condition_type][\"config_yaml\"])\n", "        args.checkpoint_path = get_path(\n\t            meta=model_paths_dict[condition_type][\"checkpoint\"])\n\t    elif condition_type == \"embedding\":\n\t        pass\n\t    else:\n\t        raise NotImplementedError\n\t    separate(args)\n"]}
{"filename": "uss/utils.py", "chunked_list": ["import datetime\n\timport json\n\timport logging\n\timport os\n\timport pickle\n\tfrom pathlib import Path\n\tfrom typing import Dict\n\timport librosa\n\timport numpy as np\n\timport torch\n", "import torch.nn as nn\n\timport yaml\n\tfrom panns_inference.models import Cnn14, Cnn14_DecisionLevelMax\n\tdef create_logging(log_dir, filemode):\n\t    os.makedirs(log_dir, exist_ok=True)\n\t    i1 = 0\n\t    while os.path.isfile(os.path.join(log_dir, \"{:04d}.log\".format(i1))):\n\t        i1 += 1\n\t    log_path = os.path.join(log_dir, \"{:04d}.log\".format(i1))\n\t    logging.basicConfig(\n", "        level=logging.DEBUG,\n\t        format=\"%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\",\n\t        datefmt=\"%a, %d %b %Y %H:%M:%S\",\n\t        filename=log_path,\n\t        filemode=filemode,\n\t    )\n\t    # Print to console\n\t    console = logging.StreamHandler()\n\t    console.setLevel(logging.INFO)\n\t    formatter = logging.Formatter(\"%(name)-12s: %(levelname)-8s %(message)s\")\n", "    console.setFormatter(formatter)\n\t    logging.getLogger(\"\").addHandler(console)\n\t    return logging\n\tdef float32_to_int16(x: float) -> int:\n\t    x = np.clip(x, a_min=-1, a_max=1)\n\t    return (x * 32767.0).astype(np.int16)\n\tdef int16_to_float32(x: int) -> float:\n\t    return (x / 32767.0).astype(np.float32)\n\tdef parse_yaml(config_yaml: str) -> Dict:\n\t    r\"\"\"Parse yaml file.\n", "    Args:\n\t        config_yaml (str): config yaml path\n\t    Returns:\n\t        yaml_dict (Dict): parsed yaml file\n\t    \"\"\"\n\t    with open(config_yaml, \"r\") as fr:\n\t        return yaml.load(fr, Loader=yaml.FullLoader)\n\tdef get_audioset632_id_to_lb(ontology_path: str) -> Dict:\n\t    r\"\"\"Get AudioSet 632 classes ID to label mapping.\"\"\"\n\t    audioset632_id_to_lb = {}\n", "    with open(ontology_path) as f:\n\t        data_list = json.load(f)\n\t    for e in data_list:\n\t        audioset632_id_to_lb[e[\"id\"]] = e[\"name\"]\n\t    return audioset632_id_to_lb\n\tdef load_pretrained_panns(\n\t    model_type: str,\n\t    checkpoint_path: str,\n\t    freeze: bool\n\t) -> nn.Module:\n", "    r\"\"\"Load pretrained pretrained audio neural networks (PANNs).\n\t    Args:\n\t        model_type: str, e.g., \"Cnn14\"\n\t        checkpoint_path, str, e.g., \"Cnn14_mAP=0.431.pth\"\n\t        freeze: bool\n\t    Returns:\n\t        model: nn.Module\n\t    \"\"\"\n\t    if model_type == \"Cnn14\":\n\t        Model = Cnn14\n", "    elif model_type == \"Cnn14_DecisionLevelMax\":\n\t        Model = Cnn14_DecisionLevelMax\n\t    else:\n\t        raise NotImplementedError\n\t    model = Model(sample_rate=32000, window_size=1024, hop_size=320,\n\t                  mel_bins=64, fmin=50, fmax=14000, classes_num=527)\n\t    if checkpoint_path:\n\t        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n\t        model.load_state_dict(checkpoint[\"model\"])\n\t    if freeze:\n", "        for param in model.parameters():\n\t            param.requires_grad = False\n\t    return model\n\tdef energy(x):\n\t    return torch.mean(x ** 2)\n\tdef magnitude_to_db(x):\n\t    eps = 1e-10\n\t    return 20. * np.log10(max(x, eps))\n\tdef db_to_magnitude(x):\n\t    return 10. ** (x / 20)\n", "def ids_to_hots(ids, classes_num, device):\n\t    hots = torch.zeros(classes_num).to(device)\n\t    for id in ids:\n\t        hots[id] = 1\n\t    return hots\n\tdef calculate_sdr(\n\t    ref: np.ndarray,\n\t    est: np.ndarray,\n\t    eps=1e-10\n\t) -> float:\n", "    r\"\"\"Calculate SDR between reference and estimation.\n\t    Args:\n\t        ref (np.ndarray), reference signal\n\t        est (np.ndarray), estimated signal\n\t    \"\"\"\n\t    noise = est - ref\n\t    numerator = np.clip(a=np.mean(ref ** 2), a_min=eps, a_max=None)\n\t    denominator = np.clip(a=np.mean(noise ** 2), a_min=eps, a_max=None)\n\t    sdr = 10. * np.log10(numerator / denominator)\n\t    return sdr\n", "class StatisticsContainer(object):\n\t    def __init__(self, statistics_path):\n\t        self.statistics_path = statistics_path\n\t        self.backup_statistics_path = \"{}_{}.pkl\".format(\n\t            os.path.splitext(self.statistics_path)[0],\n\t            datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n\t        )\n\t        self.statistics_dict = {\"balanced_train\": [], \"test\": []}\n\t    def append(self, steps, statistics, split, flush=True):\n\t        statistics[\"steps\"] = steps\n", "        self.statistics_dict[split].append(statistics)\n\t        if flush:\n\t            self.flush()\n\t    def flush(self):\n\t        pickle.dump(self.statistics_dict, open(self.statistics_path, \"wb\"))\n\t        pickle.dump(\n\t            self.statistics_dict, open(\n\t                self.backup_statistics_path, \"wb\"))\n\t        logging.info(\"    Dump statistics to {}\".format(self.statistics_path))\n\t        logging.info(\n", "            \"    Dump statistics to {}\".format(\n\t                self.backup_statistics_path))\n\tdef get_mean_sdr_from_dict(sdris_dict):\n\t    mean_sdr = np.nanmean(list(sdris_dict.values()))\n\t    return mean_sdr\n\tdef remove_silence(audio: np.ndarray, sample_rate: int) -> np.ndarray:\n\t    r\"\"\"Remove silent frames.\"\"\"\n\t    window_size = int(sample_rate * 0.1)\n\t    threshold = 0.02\n\t    frames = librosa.util.frame(\n", "        x=audio,\n\t        frame_length=window_size,\n\t        hop_length=window_size).T\n\t    # shape: (frames_num, window_size)\n\t    new_frames = get_active_frames(frames, threshold)\n\t    # shape: (new_frames_num, window_size)\n\t    new_audio = new_frames.flatten()\n\t    # shape: (new_audio_samples,)\n\t    return new_audio\n\tdef get_active_frames(frames: np.ndarray, threshold: float) -> np.ndarray:\n", "    r\"\"\"Get active frames.\"\"\"\n\t    energy = np.max(np.abs(frames), axis=-1)\n\t    # shape: (frames_num,)\n\t    active_indexes = np.where(energy > threshold)[0]\n\t    # shape: (new_frames_num,)\n\t    new_frames = frames[active_indexes]\n\t    # shape: (new_frames_num,)\n\t    return new_frames\n\tdef repeat_to_length(audio: np.ndarray, segment_samples: int) -> np.ndarray:\n\t    r\"\"\"Repeat audio to length.\"\"\"\n", "    repeats_num = (segment_samples // audio.shape[-1]) + 1\n\t    audio = np.tile(audio, repeats_num)[0: segment_samples]\n\t    return audio\n\tdef get_path(meta, re_download=False):\n\t    path = meta[\"path\"]\n\t    remote_path = meta[\"remote_path\"]\n\t    size = meta[\"size\"]\n\t    if not Path(path).is_file() or Path(\n\t            path).stat().st_size != size or re_download:\n\t        Path(path).parents[0].mkdir(parents=True, exist_ok=True)\n", "        os.system(\"wget -O {} {}\".format(path, remote_path))\n\t        print(\"Download to {}\".format(path))\n\t    return path\n"]}
{"filename": "uss/data/samplers.py", "chunked_list": ["import logging\n\timport time\n\tfrom typing import Dict, List\n\timport h5py\n\timport numpy as np\n\timport torch.distributed as dist\n\tfrom torch.utils.data.sampler import Sampler\n\tclass Base:\n\t    def __init__(self,\n\t                 indexes_hdf5_path: str,\n", "                 batch_size: int,\n\t                 steps_per_epoch: int,\n\t                 random_seed: int,\n\t                 ):\n\t        r\"\"\"Base class of train samplers.\n\t        Args:\n\t            indexes_hdf5_path (str)\n\t            batch_size (int)\n\t            steps_per_epoch (int)\n\t            random_seed (int)\n", "        Returns:\n\t            None\n\t        \"\"\"\n\t        self.batch_size = batch_size\n\t        self.random_state = np.random.RandomState(random_seed)\n\t        self.steps_per_epoch = steps_per_epoch\n\t        # Load targets of training data\n\t        load_time = time.time()\n\t        with h5py.File(indexes_hdf5_path, 'r') as hf:\n\t            self.audio_names = [audio_name.decode()\n", "                                for audio_name in hf['audio_name'][:]]\n\t            self.hdf5_paths = [hdf5_path.decode()\n\t                               for hdf5_path in hf['hdf5_path'][:]]\n\t            self.indexes_in_hdf5 = hf['index_in_hdf5'][:]\n\t            self.targets = hf['target'][:].astype(np.float32)\n\t            # self.targets: (audios_num, classes_num)\n\t        self.audios_num, self.classes_num = self.targets.shape\n\t        logging.info('Training number: {}'.format(self.audios_num))\n\t        logging.info(\n\t            'Load target time: {:.3f} s'.format(\n", "                time.time() - load_time))\n\t        # Number of training samples of different sound classes\n\t        self.samples_num_per_class = np.sum(self.targets, axis=0)\n\t        logging.info('samples_num_per_class: {}'.format(\n\t            self.samples_num_per_class.astype(np.int32)))\n\tclass BalancedSampler(Base, Sampler):\n\t    def __init__(self,\n\t                 indexes_hdf5_path: str,\n\t                 batch_size: int,\n\t                 steps_per_epoch: int,\n", "                 random_seed: int = 1234,\n\t                 ) -> None:\n\t        r\"\"\"Balanced sampler. Generate mini-batches meta for training. Data are\n\t        evenly sampled from different sound classes.\n\t        Args:\n\t            indexes_hdf5_path (str)\n\t            batch_size (int)\n\t            steps_per_epoch (int)\n\t            random_seed (int)\n\t        Returns:\n", "            None\n\t        \"\"\"\n\t        super(BalancedSampler, self).__init__(\n\t            indexes_hdf5_path=indexes_hdf5_path,\n\t            batch_size=batch_size,\n\t            steps_per_epoch=steps_per_epoch,\n\t            random_seed=random_seed,\n\t        )\n\t        # Training indexes of all sound classes. E.g.:\n\t        # [[0, 11, 12, ...], [3, 4, 15, 16, ...], [7, 8, ...], ...]\n", "        self.indexes_per_class = []\n\t        for k in range(self.classes_num):\n\t            self.indexes_per_class.append(\n\t                np.where(self.targets[:, k] == 1)[0])\n\t        # Shuffle indexes\n\t        for k in range(self.classes_num):\n\t            self.random_state.shuffle(self.indexes_per_class[k])\n\t        self.queue = []  # Contains sound class IDs\n\t        self.pointers_of_classes = [0] * self.classes_num\n\t    def expand_queue(self, queue) -> List:\n", "        r\"\"\"Append more class IDs to the queue.\n\t        Args:\n\t            queue: List, e.g., [431, 73]\n\t        Returns:\n\t            queue: List, e.g., [431, 73, 2, 54, 379, ...]\n\t        \"\"\"\n\t        classes_set = np.arange(self.classes_num).tolist()\n\t        self.random_state.shuffle(classes_set)\n\t        queue.extend(classes_set)\n\t        return queue\n", "    def __iter__(self) -> List[Dict]:\n\t        r\"\"\"Yield mini-batch meta.\n\t        Args:\n\t            None\n\t        Returns:\n\t            batch_meta: e.g.: [\n\t                {\"audio_name\": \"YfWBzCRl6LUs.wav\",\n\t                 \"hdf5_path\": \"xx/balanced_train.h5\",\n\t                 \"index_in_hdf5\": 15734,\n\t                 \"target\": [0, 1, 0, 0, ...]},\n", "            ...]\n\t        \"\"\"\n\t        batch_size = self.batch_size\n\t        while True:\n\t            batch_meta = []\n\t            while len(batch_meta) < batch_size:\n\t                if len(self.queue) == 0:\n\t                    self.queue = self.expand_queue(self.queue)\n\t                # Pop a class ID and get the audio index\n\t                class_id = self.queue.pop(0)\n", "                pointer = self.pointers_of_classes[class_id]\n\t                self.pointers_of_classes[class_id] += 1\n\t                index = self.indexes_per_class[class_id][pointer]\n\t                # When finish one epoch of a sound class, then shuffle its\n\t                # indexes and reset pointer.\n\t                if self.pointers_of_classes[class_id] >= self.samples_num_per_class[class_id]:\n\t                    self.pointers_of_classes[class_id] = 0\n\t                    self.random_state.shuffle(self.indexes_per_class[class_id])\n\t                batch_meta.append({\n\t                    'hdf5_path': self.hdf5_paths[index],\n", "                    'index_in_hdf5': self.indexes_in_hdf5[index],\n\t                    'class_id': class_id})\n\t            yield batch_meta\n\t    def __len__(self) -> int:\n\t        return self.steps_per_epoch\n\tclass DistributedSamplerWrapper:\n\t    def __init__(self, sampler: object) -> None:\n\t        r\"\"\"Distributed wrapper of sampler.\n\t        Args:\n\t            sampler (Sampler object)\n", "        Returns:\n\t            None\n\t        \"\"\"\n\t        self.sampler = sampler\n\t    def __iter__(self) -> List:\n\t        r\"\"\"Yield a part of mini-batch meta on each device.\n\t        Args:\n\t            None\n\t        Returns:\n\t            list_meta (List), a part of mini-batch meta.\n", "        \"\"\"\n\t        if dist.is_initialized():\n\t            num_replicas = dist.get_world_size()\n\t            rank = dist.get_rank()\n\t        else:\n\t            num_replicas = 1\n\t            rank = 0\n\t        for list_meta in self.sampler:\n\t            yield list_meta[rank:: num_replicas]\n\t    def __len__(self) -> int:\n", "        return len(self.sampler)\n"]}
{"filename": "uss/data/datamodules.py", "chunked_list": ["from typing import Dict, List, Optional\n\timport lightning.pytorch as pl\n\timport numpy as np\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom uss.data.samplers import DistributedSamplerWrapper\n\tclass DataModule(pl.LightningDataModule):\n\t    def __init__(\n\t        self,\n\t        train_sampler: object,\n", "        train_dataset: object,\n\t        num_workers: int,\n\t    ) -> None:\n\t        r\"\"\"PyTorch Lightning Data module. A wrapper of the DataLoader. Can be\n\t        used to yield mini-batches of train, validation, and test data.\n\t        Args:\n\t            train_sampler (Sampler object)\n\t            train_dataset (Dataset object)\n\t            num_workers: int\n\t        Returns:\n", "            None\n\t        Examples::\n\t            >>> data_module.setup()\n\t            >>> for batch_data_dict in datamodule.train_dataloader():\n\t            >>>     print(batch_data_dict.keys())\n\t            >>>     break\n\t        \"\"\"\n\t        super().__init__()\n\t        self._train_sampler = train_sampler\n\t        self._train_dataset = train_dataset\n", "        self.num_workers = num_workers\n\t        self.collate_fn = collate_fn\n\t    def setup(self, stage: Optional[str] = None) -> None:\n\t        r\"\"\"called on every GPU.\"\"\"\n\t        self.train_dataset = self._train_dataset\n\t        # The sampler yields a part of mini-batch meta on each device\n\t        self.train_sampler = DistributedSamplerWrapper(self._train_sampler)\n\t    def train_dataloader(self) -> torch.utils.data.DataLoader:\n\t        r\"\"\"Get train loader.\"\"\"\n\t        if self.num_workers > 0:\n", "            persistent_workers = True\n\t        else:\n\t            persistent_workers = False\n\t        train_loader = DataLoader(\n\t            dataset=self.train_dataset,\n\t            batch_sampler=self.train_sampler,\n\t            collate_fn=self.collate_fn,\n\t            num_workers=self.num_workers,\n\t            pin_memory=True,\n\t            persistent_workers=persistent_workers,\n", "        )\n\t        return train_loader\n\t    def val_dataloader(self):\n\t        r\"\"\"We use `uss.callbacks.evaluate` to evaluate on the train / test\n\t        dataset\"\"\"\n\t        pass\n\tdef collate_fn(list_data_dict: List[Dict]) -> Dict:\n\t    r\"\"\"Collate a mini-batch of data.\n\t    Args:\n\t        list_data_dict (List[Dict]): e.g., [\n", "            {\"hdf5_path\": \"xx/balanced_train.h5\",\n\t             \"index_in_hdf5\": 11072,\n\t             ...},\n\t        ...]\n\t    Returns:\n\t        data_dict (Dict): e.g., {\n\t            \"hdf5_path\": [\"xx/balanced_train.h5\", \"xx/balanced_train.h5\", ...]\n\t            \"index_in_hdf5\": [11072, 17251, ...],\n\t        }\n\t    \"\"\"\n", "    data_dict = {}\n\t    for key in list_data_dict[0].keys():\n\t        data_dict[key] = np.array([data_dict[key]\n\t                                  for data_dict in list_data_dict])\n\t        if str(data_dict[key].dtype) in [\"float32\"]:\n\t            data_dict[key] = torch.Tensor(data_dict[key])\n\t    return data_dict\n"]}
{"filename": "uss/data/datasets.py", "chunked_list": ["from typing import Dict\n\timport h5py\n\timport numpy as np\n\tfrom uss.utils import int16_to_float32\n\tclass Dataset:\n\t    def __init__(self, steps_per_epoch) -> None:\n\t        r\"\"\"This class takes the meta as input, and return the waveform, target\n\t        and other information of the audio clip. This class is used by\n\t        DataLoader.\n\t        Args:\n", "            steps_per_epoch (int): the number of steps in an epoch\n\t        \"\"\"\n\t        self.steps_per_epoch = steps_per_epoch\n\t    def __getitem__(self, meta) -> Dict:\n\t        \"\"\"Load waveform, target and other information of an audio clip.\n\t        Args:\n\t            meta (Dict): {\n\t                \"hdf5_path\": str,\n\t                \"index_in_hdf5\": int,\n\t                \"class_id\": int,\n", "            }\n\t        Returns:\n\t            data_dict (Dict): {\n\t                \"hdf5_path\": str,\n\t                \"index_in_hdf5\": int,\n\t                \"audio_name\": str,\n\t                \"waveform\": (clip_samples,),\n\t                \"target\": (classes_num,),\n\t                \"class_id\": int,\n\t            }\n", "        \"\"\"\n\t        hdf5_path = meta[\"hdf5_path\"]\n\t        index_in_hdf5 = meta[\"index_in_hdf5\"]\n\t        class_id = meta[\"class_id\"]\n\t        with h5py.File(hdf5_path, 'r') as hf:\n\t            audio_name = hf[\"audio_name\"][index_in_hdf5].decode()\n\t            waveform = int16_to_float32(hf[\"waveform\"][index_in_hdf5])\n\t            waveform = waveform\n\t            # shape: (clip_samples,)\n\t            target = hf[\"target\"][index_in_hdf5].astype(np.float32)\n", "            # shape: (classes_num,)\n\t        data_dict = {\n\t            \"hdf5_path\": hdf5_path,\n\t            \"index_in_hdf5\": index_in_hdf5,\n\t            \"audio_name\": audio_name,\n\t            \"waveform\": waveform,\n\t            \"target\": target,\n\t            \"class_id\": class_id,\n\t        }\n\t        return data_dict\n", "    def __len__(self) -> int:\n\t        return self.steps_per_epoch\n"]}
{"filename": "uss/data/anchor_segment_mixers.py", "chunked_list": ["from typing import Dict\n\timport torch\n\timport torch.nn as nn\n\tclass AnchorSegmentMixer(nn.Module):\n\t    def __init__(\n\t        self,\n\t        mix_num: int,\n\t        match_energy: bool,\n\t    ) -> None:\n\t        r\"\"\"Anchor segment mixer. Used to mix multiple sources into a mixture.\n", "        Args:\n\t            mix_num (int): the number of sources to mix\n\t            match_energy (bool): set to `True` to rescale segments to have the\n\t                same energy before mixing\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        super(AnchorSegmentMixer, self).__init__()\n\t        self.mix_num = mix_num\n\t        self.match_energy = match_energy\n", "    def __call__(self, waveforms: torch.Tensor) -> Dict:\n\t        r\"\"\"Mix multiple sources to mixture.\n\t        Args:\n\t            waveforms (torch.Tensor): (batch_size, segment_samples)\n\t        Returns:\n\t            mixtures (torch.Tensor): (batch_size, segment_samples)\n\t            targets (torch.Tensor): (batch_size, segment_samples)\n\t        \"\"\"\n\t        batch_size = waveforms.shape[0]\n\t        targets = []\n", "        mixtures = []\n\t        for n in range(0, batch_size):\n\t            segment = waveforms[n].clone()\n\t            mixture = segment.clone()\n\t            for i in range(1, self.mix_num):\n\t                next_segment = waveforms[(n + i) % batch_size].clone()\n\t                if self.match_energy:\n\t                    # Rescale the energy of the next_segment to match the energy of\n\t                    # the segment\n\t                    next_segment = rescale_to_match_energy(\n", "                        segment, next_segment)\n\t                mixture += next_segment\n\t            targets.append(segment)\n\t            mixtures.append(mixture)\n\t        targets = torch.stack(targets, dim=0)\n\t        mixtures = torch.stack(mixtures, dim=0)\n\t        return mixtures, targets\n\tdef rescale_to_match_energy(\n\t    segment1: torch.Tensor,\n\t    segment2: torch.Tensor,\n", ") -> torch.Tensor:\n\t    r\"\"\"Rescale segment2 to match the energy of segment1.\n\t    Args:\n\t        segment1 (torch.Tensor), signal\n\t        segment2 (torch.Tensor), signal\n\t    \"\"\"\n\t    ratio = get_energy_ratio(segment1, segment2)\n\t    recaled_segment2 = segment2 * ratio\n\t    return recaled_segment2\n\tdef get_energy(x: torch.Tensor) -> torch.float:\n", "    r\"\"\"Calculate the energy of a signal.\"\"\"\n\t    return torch.mean(x ** 2)\n\tdef get_energy_ratio(\n\t    segment1: torch.Tensor,\n\t    segment2: torch.Tensor,\n\t    eps=1e-10\n\t) -> float:\n\t    r\"\"\"Calculate ratio = sqrt(E(s1) / E(s2)).\"\"\"\n\t    energy1 = get_energy(segment1)\n\t    energy2 = get_energy(segment2)\n", "    ratio = (energy1 / max(energy2, eps)) ** 0.5\n\t    ratio = torch.clamp(ratio, 0.02, 50)\n\t    return ratio\n"]}
{"filename": "uss/data/anchor_segment_detectors.py", "chunked_list": ["import random\n\tfrom typing import Dict, List, Tuple\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass AnchorSegmentDetector(nn.Module):\n\t    def __init__(self,\n\t                 sed_model: nn.Module,\n\t                 clip_seconds: float,\n\t                 segment_seconds: float,\n", "                 frames_per_second: int,\n\t                 sample_rate: float,\n\t                 detect_mode: str,\n\t                 ) -> None:\n\t        r\"\"\"The anchor segment detector is used to detect 2-second anchor\n\t        segments from 10-second weakly labelled audio clips during training.\n\t        Args:\n\t            sed_model (nn.Module): pretrained sound event detection model\n\t            clip_seconds (float): audio clip duration, e.g., 10.\n\t            segment_seconds (float): anchor segment duration, e.g., 2.\n", "            frames_per_second (int):, e.g., 100\n\t            sample_rate (int)\n\t            detect_mode (str), \"max_area\" | \"random\"\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        super(AnchorSegmentDetector, self).__init__()\n\t        assert detect_mode in [\"max_area\", \"random\"]\n\t        self.sed_model = sed_model\n\t        self.clip_frames = int(clip_seconds * frames_per_second)\n", "        self.segment_frames = int(segment_seconds * frames_per_second + 1)\n\t        self.hop_samples = sample_rate // frames_per_second\n\t        self.sample_rate = sample_rate\n\t        self.detect_mode = detect_mode\n\t        # Used to the area under the probability curve of anchor segments\n\t        self.anchor_segment_scorer = AnchorSegmentScorer(\n\t            segment_frames=self.segment_frames,\n\t        )\n\t    def __call__(\n\t        self,\n", "        waveforms: torch.Tensor,\n\t        class_ids: List,\n\t        debug: bool = False,\n\t    ) -> Dict:\n\t        r\"\"\"Input a batch of 10-second audio clips. Mine 2-second anchor\n\t        segments.\n\t        Args:\n\t            waveforms (torch.Tensor): (batch_size, clip_samples)\n\t            class_ids (List): (batch_size,)\n\t            debug (bool)\n", "        Returns:\n\t            segments_dict (Dict): e.g., {\n\t                \"waveform\": (batch_size, segment_samples),\n\t                \"class_id\": (batch_size,),\n\t                \"bgn_sample\": (batch_size,),\n\t                \"end_sample\": (batch_size,),\n\t            }\n\t        \"\"\"\n\t        batch_size, _ = waveforms.shape\n\t        # Sound event detection\n", "        with torch.no_grad():\n\t            self.sed_model.eval()\n\t            framewise_output = self.sed_model(\n\t                input=waveforms,\n\t            )['framewise_output']\n\t            # (batch_size, clip_frames, classes_num)\n\t        segments = []\n\t        bgn_samples = []\n\t        end_samples = []\n\t        # Detect the anchor segments of a mini-batch of audio clips one by one\n", "        for n in range(batch_size):\n\t            # Class ID of the current 10-second clip.\n\t            class_id = class_ids[n]\n\t            # There can be multiple tags in the 10-second clip. We only detect\n\t            # the anchor segment of #class_id\n\t            prob_array = framewise_output[n, :, class_id]\n\t            # shape: (segment_frames,)\n\t            if self.detect_mode == \"max_area\":\n\t                # The area of the probability curve in anchor segments\n\t                anchor_segment_scores = self.anchor_segment_scorer(\n", "                    prob_array=prob_array,\n\t                )\n\t                anchor_index = torch.argmax(anchor_segment_scores)\n\t                if debug:\n\t                    _debug_plot_anchor_segment(\n\t                        waveform=waveforms[0],\n\t                        anchor_segment_scores=anchor_segment_scores,\n\t                        class_id=class_id,\n\t                    )\n\t            elif self.detect_mode == \"random\":\n", "                anchor_index = random.randint(0, self.segment_frames - 1)\n\t                anchor_index = torch.tensor(anchor_index).to(waveforms.device)\n\t            else:\n\t                raise NotImplementedError\n\t            # Get begin and end samples of an anchor segment.\n\t            bgn_sample, end_sample = self.get_segment_bgn_end_samples(\n\t                anchor_index=anchor_index,\n\t                clip_frames=self.clip_frames,\n\t            )\n\t            segment = waveforms[n, bgn_sample: end_sample]\n", "            segments.append(segment)\n\t            bgn_samples.append(bgn_sample)\n\t            end_samples.append(end_sample)\n\t        segments = torch.stack(segments, dim=0)\n\t        # (batch_size, segment_samples)\n\t        bgn_samples = torch.stack(bgn_samples, dim=0)\n\t        end_samples = torch.stack(end_samples, dim=0)\n\t        segments_dict = {\n\t            'waveform': segments,\n\t            'class_id': class_ids,\n", "            'bgn_sample': bgn_samples,\n\t            'end_sample': end_samples,\n\t        }\n\t        return segments_dict\n\t    def get_segment_bgn_end_samples(\n\t        self,\n\t        anchor_index: int,\n\t        clip_frames: int\n\t    ) -> Tuple[int, int]:\n\t        r\"\"\"Get begin and end samples of an anchor segment.\n", "        Args:\n\t            anchor_index (torch.int): e.g., 155\n\t        Returns:\n\t            bgn_sample (torch.int), e.g., 17600\n\t            end_sample (torch.int): e.g., 81600\n\t        \"\"\"\n\t        anchor_index = torch.clamp(\n\t            input=anchor_index,\n\t            min=self.segment_frames // 2,\n\t            max=clip_frames - self.segment_frames // 2,\n", "        )\n\t        bgn_frame = anchor_index - self.segment_frames // 2\n\t        end_frame = anchor_index + self.segment_frames // 2\n\t        bgn_sample = bgn_frame * self.hop_samples\n\t        end_sample = end_frame * self.hop_samples\n\t        return bgn_sample, end_sample\n\tdef _debug_plot_anchor_segment(\n\t    waveform: torch.Tensor,\n\t    anchor_segment_scores: torch.Tensor,\n\t    class_id: int,\n", ") -> None:\n\t    r\"\"\"For debug only. Plot anchor segment prediction.\"\"\"\n\t    import os\n\t    import matplotlib.pyplot as plt\n\t    import soundfile\n\t    from uss.config import IX_TO_LB\n\t    sample_rate = 32000\n\t    n = 0\n\t    audio_path = os.path.join(\"_debug_anchor_segment{}.wav\".format(n))\n\t    fig_path = os.path.join(\"_debug_anchor_segment{}.pdf\".format(n))\n", "    soundfile.write(\n\t        file=audio_path,\n\t        data=waveform.data.cpu().numpy(),\n\t        samplerate=sample_rate,\n\t    )\n\t    print(\"Write out to {}\".format(audio_path))\n\t    plt.figure()\n\t    plt.plot(anchor_segment_scores.data.cpu().numpy())\n\t    plt.title(IX_TO_LB[class_id])\n\t    plt.ylim(0, 1)\n", "    plt.savefig(fig_path)\n\t    print(\"Write out to {}\".format(fig_path))\n\t    os._exit(0)\n\tclass AnchorSegmentScorer(nn.Module):\n\t    def __init__(self,\n\t                 segment_frames: int,\n\t                 ) -> None:\n\t        r\"\"\"Calculate the area under the probabiltiy curve of an anchor segment.\n\t        Args:\n\t            segment_frames (int)\n", "        Returns:\n\t            None\n\t        \"\"\"\n\t        super(AnchorSegmentScorer, self).__init__()\n\t        self.segment_frames = segment_frames\n\t        filter_len = self.segment_frames\n\t        self.register_buffer(\n\t            'smooth_filter', torch.ones(\n\t                1, 1, filter_len) / filter_len)\n\t    def __call__(self, prob_array: torch.Tensor):\n", "        r\"\"\"Calculate the area under the probabiltiy curve of an anchor segment.\n\t        Args:\n\t            prob_array (torch.Tensor): (clip_frames,), sound event\n\t                detection probability of a specific sound class.\n\t        Returns:\n\t            output: (clip_frames,), smoothed probability, equivalent to the\n\t                area of probability of anchor segments.\n\t        \"\"\"\n\t        x = F.pad(\n\t            input=prob_array[None, None, :],\n", "            pad=(self.segment_frames // 2, self.segment_frames // 2),\n\t            mode='replicate'\n\t        )\n\t        # shape: (1, 1, clip_frames)\n\t        output = torch.conv1d(\n\t            input=x,\n\t            weight=self.smooth_filter,\n\t            padding=0,\n\t        )\n\t        # shape: (1, 1, clip_frames)\n", "        output = output.squeeze(dim=(0, 1))\n\t        # (clip_frames,)\n\t        return output\n"]}
{"filename": "uss/models/query_nets.py", "chunked_list": ["from typing import Dict\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom uss.config import panns_paths_dict\n\tfrom uss.models.base import init_layer\n\tfrom uss.utils import get_path, load_pretrained_panns\n\tdef initialize_query_net(configs):\n\t    r\"\"\"Initialize query net.\n\t    Args:\n", "        configs (Dict)\n\t    Returns:\n\t        model (nn.Module)\n\t    \"\"\"\n\t    model_type = configs[\"query_net\"][\"model_type\"]\n\t    bottleneck_type = configs[\"query_net\"][\"bottleneck_type\"]\n\t    # base_checkpoint_path = configs[\"query_net\"][\"base_checkpoint_path\"]\n\t    base_checkpoint_type = configs[\"query_net\"][\"base_checkpoint_type\"]\n\t    freeze_base = configs[\"query_net\"][\"freeze_base\"]\n\t    outputs_num = configs[\"query_net\"][\"outputs_num\"]\n", "    base_checkpoint_path = get_path(panns_paths_dict[base_checkpoint_type])\n\t    if model_type == \"Cnn14_Wrapper\":\n\t        model = Cnn14_Wrapper(\n\t            bottleneck_type=bottleneck_type,\n\t            base_checkpoint_path=base_checkpoint_path,\n\t            freeze_base=freeze_base,\n\t        )\n\t    elif model_type == \"AdaptiveCnn14_Wrapper\":\n\t        model = AdaptiveCnn14_Wrapper(\n\t            bottleneck_type=bottleneck_type,\n", "            base_checkpoint_path=base_checkpoint_path,\n\t            freeze_base=freeze_base,\n\t            freeze_adaptor=configs[\"query_net\"][\"freeze_adaptor\"],\n\t            outputs_num=outputs_num,\n\t        )\n\t    elif model_type == \"YourOwn_QueryNet\":\n\t        model = YourOwn_QueryNet(outputs_num=outputs_num)\n\t    else:\n\t        raise NotImplementedError\n\t    return model\n", "def get_panns_bottleneck_type(bottleneck_type: str) -> str:\n\t    r\"\"\"Get PANNs bottleneck name.\n\t    Args:\n\t        bottleneck_type (str)\n\t    Returns:\n\t        panns_bottleneck_type (str)\n\t    \"\"\"\n\t    if bottleneck_type == \"at_soft\":\n\t        panns_bottleneck_type = \"clipwise_output\"\n\t    else:\n", "        panns_bottleneck_type = bottleneck_type\n\t    return panns_bottleneck_type\n\tclass Cnn14_Wrapper(nn.Module):\n\t    def __init__(self,\n\t                 bottleneck_type: str,\n\t                 base_checkpoint_path: str,\n\t                 freeze_base: bool,\n\t                 ) -> None:\n\t        r\"\"\"Query Net based on Cnn14 of PANNs. There are no extra learnable\n\t        parameters.\n", "        Args:\n\t            bottleneck_type (str), \"at_soft\" | \"embedding\"\n\t            base_checkpoint_path (str), Cnn14 checkpoint path\n\t            freeze_base (bool), whether to freeze the parameters of the Cnn14\n\t        \"\"\"\n\t        super(Cnn14_Wrapper, self).__init__()\n\t        self.panns_bottleneck_type = get_panns_bottleneck_type(bottleneck_type)\n\t        self.base = load_pretrained_panns(\n\t            model_type=\"Cnn14\",\n\t            checkpoint_path=base_checkpoint_path,\n", "            freeze=freeze_base,\n\t        )\n\t        self.freeze_base = freeze_base\n\t    def forward_base(self, source: torch.Tensor) -> torch.Tensor:\n\t        r\"\"\"Forward a source into a the base part of the query net.\n\t        Args:\n\t            source (torch.Tensor), (batch_size, audio_samples)\n\t        Returns:\n\t            bottleneck (torch.Tensor), (bottleneck_dim,)\n\t        \"\"\"\n", "        if self.freeze_base:\n\t            self.base.eval()\n\t            with torch.no_grad():\n\t                base_output_dict = self.base(source)\n\t        else:\n\t            self.base.train()\n\t            base_output_dict = self.base(source)\n\t        bottleneck = base_output_dict[self.panns_bottleneck_type]\n\t        return bottleneck\n\t    def forward_adaptor(self, bottleneck: torch.Tensor) -> torch.Tensor:\n", "        r\"\"\"Forward a bottleneck into a the adaptor part of the query net.\n\t        Args:\n\t            bottleneck (torch.Tensor), (bottleneck_dim,)\n\t        Returns:\n\t            output (torch.Tensor), (output_dim,)\n\t        \"\"\"\n\t        output = bottleneck\n\t        return output\n\t    def forward(self, source: torch.Tensor) -> Dict:\n\t        r\"\"\"Forward a source into a query net.\n", "        Args:\n\t            source (torch.Tensor), (batch_size, audio_samples)\n\t        Returns:\n\t            output_dict (Dict), {\n\t                \"bottleneck\": (bottleneck_dim,)\n\t                \"output\": (output_dim,)\n\t            }\n\t        \"\"\"\n\t        bottleneck = self.forward_base(source=source)\n\t        output = self.forward_adaptor(bottleneck=bottleneck)\n", "        output_dict = {\n\t            \"bottleneck\": bottleneck,\n\t            \"output\": output,\n\t        }\n\t        return output_dict\n\tclass AdaptiveCnn14_Wrapper(nn.Module):\n\t    def __init__(self,\n\t                 bottleneck_type: str,\n\t                 base_checkpoint_path: str,\n\t                 freeze_base: bool,\n", "                 freeze_adaptor: bool,\n\t                 outputs_num: int,\n\t                 ) -> None:\n\t        r\"\"\"Query Net based on Cnn14 of PANNs. There are no extra learnable\n\t        parameters.\n\t        Args:\n\t            bottleneck_type (str), \"at_soft\" | \"embedding\"\n\t            base_checkpoint_path (str), Cnn14 checkpoint path\n\t            freeze_base (bool), whether to freeze the parameters of the Cnn14\n\t            freeze_adaptor (bool), whether to freeze the parameters of the\n", "                adaptor\n\t            outputs_num (int), output dimension\n\t        \"\"\"\n\t        super(AdaptiveCnn14_Wrapper, self).__init__()\n\t        self.freeze_base = freeze_base\n\t        self.panns_bottleneck_type = get_panns_bottleneck_type(bottleneck_type)\n\t        self.base = load_pretrained_panns(\n\t            model_type=\"Cnn14\",\n\t            checkpoint_path=base_checkpoint_path,\n\t            freeze=freeze_base,\n", "        )\n\t        bottleneck_units = self._get_bottleneck_units(\n\t            self.panns_bottleneck_type)\n\t        self.fc1 = nn.Linear(bottleneck_units, 2048)\n\t        self.fc2 = nn.Linear(2048, outputs_num)\n\t        if freeze_adaptor:\n\t            for param in self.fc1.parameters():\n\t                param.requires_grad = False\n\t            for param in self.fc2.parameters():\n\t                param.requires_grad = False\n", "        self.init_weights()\n\t    def _get_bottleneck_units(self, panns_bottleneck_type) -> int:\n\t        if panns_bottleneck_type == \"embedding\":\n\t            bottleneck_hid_units = self.base.fc_audioset.in_features\n\t        elif panns_bottleneck_type == \"clipwise_output\":\n\t            bottleneck_hid_units = self.base.fc_audioset.out_features\n\t        else:\n\t            raise NotImplementedError\n\t        return bottleneck_hid_units\n\t    def init_weights(self):\n", "        r\"\"\"Initialize weights.\"\"\"\n\t        init_layer(self.fc1)\n\t        init_layer(self.fc2)\n\t    def forward_base(self, source: torch.Tensor) -> torch.Tensor:\n\t        r\"\"\"Forward a source into a the base part of the query net.\n\t        Args:\n\t            source (torch.Tensor), (batch_size, audio_samples)\n\t        Returns:\n\t            bottleneck (torch.Tensor), (bottleneck_dim,)\n\t        \"\"\"\n", "        if self.freeze_base:\n\t            self.base.eval()\n\t            with torch.no_grad():\n\t                base_output_dict = self.base(source)\n\t        else:\n\t            self.base.train()\n\t            base_output_dict = self.base(source)\n\t        bottleneck = base_output_dict[self.panns_bottleneck_type]\n\t        return bottleneck\n\t    def forward_adaptor(self, bottleneck: torch.Tensor) -> torch.Tensor:\n", "        r\"\"\"Forward a bottleneck into a the adaptor part of the query net.\n\t        Args:\n\t            bottleneck (torch.Tensor), (bottleneck_dim,)\n\t        Returns:\n\t            output (torch.Tensor), (output_dim,)\n\t        \"\"\"\n\t        x = F.leaky_relu(self.fc1(bottleneck), negative_slope=0.01)\n\t        x = F.dropout(x, p=0.5, training=self.training, inplace=True)\n\t        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n\t        output = F.dropout(x, p=0.5, training=self.training, inplace=True)\n", "        return output\n\t    def forward(self, source: torch.Tensor) -> Dict:\n\t        r\"\"\"Forward a source into a query net.\n\t        Args:\n\t            source (torch.Tensor), (batch_size, audio_samples)\n\t        Returns:\n\t            output_dict (Dict), {\n\t                \"bottleneck\": (bottleneck_dim,)\n\t                \"output\": (output_dim,)\n\t            }\n", "        \"\"\"\n\t        bottleneck = self.forward_base(source=source)\n\t        output = self.forward_adaptor(bottleneck=bottleneck)\n\t        output_dict = {\n\t            \"bottleneck\": bottleneck,\n\t            \"output\": output,\n\t        }\n\t        return output_dict\n\tclass YourOwn_QueryNet(nn.Module):\n\t    def __init__(self, outputs_num: int) -> None:\n", "        r\"\"\"User defined query net.\"\"\"\n\t        super(YourOwn_QueryNet, self).__init__()\n\t        self.fc1 = nn.Linear(1, outputs_num)\n\t    def forward(self, source: torch.Tensor) -> Dict:\n\t        r\"\"\"Forward a source into a query net.\n\t        Args:\n\t            source (torch.Tensor), (batch_size, audio_samples)\n\t        Returns:\n\t            output_dict (Dict), {\n\t                \"bottleneck\": (bottleneck_dim,)\n", "                \"output\": (output_dim,)\n\t            }\n\t        \"\"\"\n\t        x = torch.mean(source, dim=-1, keepdim=True)\n\t        bottleneck = self.fc1(x)\n\t        output_dict = {\n\t            \"bottleneck\": bottleneck,\n\t            \"output\": bottleneck,\n\t        }\n\t        return output_dict\n"]}
{"filename": "uss/models/base.py", "chunked_list": ["import math\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torchlibrosa.stft import magphase\n\tdef init_layer(layer):\n\t    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n\t    nn.init.xavier_uniform_(layer.weight)\n\t    if hasattr(layer, \"bias\"):\n", "        if layer.bias is not None:\n\t            layer.bias.data.fill_(0.0)\n\tdef init_bn(bn):\n\t    \"\"\"Initialize a Batchnorm layer. \"\"\"\n\t    bn.bias.data.fill_(0.0)\n\t    bn.weight.data.fill_(1.0)\n\tdef init_embedding(layer):\n\t    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n\t    nn.init.uniform_(layer.weight, -1., 1.)\n\t    if hasattr(layer, 'bias'):\n", "        if layer.bias is not None:\n\t            layer.bias.data.fill_(0.)\n\tdef init_gru(rnn):\n\t    \"\"\"Initialize a GRU layer. \"\"\"\n\t    def _concat_init(tensor, init_funcs):\n\t        (length, fan_out) = tensor.shape\n\t        fan_in = length // len(init_funcs)\n\t        for (i, init_func) in enumerate(init_funcs):\n\t            init_func(tensor[i * fan_in: (i + 1) * fan_in, :])\n\t    def _inner_uniform(tensor):\n", "        fan_in = nn.init._calculate_correct_fan(tensor, \"fan_in\")\n\t        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n\t    for i in range(rnn.num_layers):\n\t        _concat_init(\n\t            getattr(rnn, \"weight_ih_l{}\".format(i)),\n\t            [_inner_uniform, _inner_uniform, _inner_uniform],\n\t        )\n\t        torch.nn.init.constant_(getattr(rnn, \"bias_ih_l{}\".format(i)), 0)\n\t        _concat_init(\n\t            getattr(rnn, \"weight_hh_l{}\".format(i)),\n", "            [_inner_uniform, _inner_uniform, nn.init.orthogonal_],\n\t        )\n\t        torch.nn.init.constant_(getattr(rnn, \"bias_hh_l{}\".format(i)), 0)\n\tdef act(x, activation):\n\t    if activation == \"relu\":\n\t        return F.relu_(x)\n\t    elif activation == \"leaky_relu\":\n\t        return F.leaky_relu_(x, negative_slope=0.01)\n\t    elif activation == \"swish\":\n\t        return x * torch.sigmoid(x)\n", "    else:\n\t        raise Exception(\"Incorrect activation!\")\n\tclass Base:\n\t    def __init__(self):\n\t        pass\n\t    def spectrogram(self, input, eps=0.):\n\t        (real, imag) = self.stft(input)\n\t        return torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5\n\t    def spectrogram_phase(self, input, eps=0.):\n\t        (real, imag) = self.stft(input)\n", "        mag = torch.clamp(real ** 2 + imag ** 2, eps, np.inf) ** 0.5\n\t        cos = real / mag\n\t        sin = imag / mag\n\t        return mag, cos, sin\n\t    def wav_to_spectrogram_phase(self, input, eps=1e-10):\n\t        \"\"\"Waveform to spectrogram.\n\t        Args:\n\t          input: (batch_size, segment_samples, channels_num)\n\t        Outputs:\n\t          output: (batch_size, channels_num, time_steps, freq_bins)\n", "        \"\"\"\n\t        sp_list = []\n\t        cos_list = []\n\t        sin_list = []\n\t        channels_num = input.shape[1]\n\t        for channel in range(channels_num):\n\t            mag, cos, sin = self.spectrogram_phase(\n\t                input[:, channel, :], eps=eps)\n\t            sp_list.append(mag)\n\t            cos_list.append(cos)\n", "            sin_list.append(sin)\n\t        sps = torch.cat(sp_list, dim=1)\n\t        coss = torch.cat(cos_list, dim=1)\n\t        sins = torch.cat(sin_list, dim=1)\n\t        return sps, coss, sins\n\t    def wav_to_spectrogram(self, input, eps=0.):\n\t        \"\"\"Waveform to spectrogram.\n\t        Args:\n\t          input: (batch_size, segment_samples, channels_num)\n\t        Outputs:\n", "          output: (batch_size, channels_num, time_steps, freq_bins)\n\t        \"\"\"\n\t        sp_list = []\n\t        channels_num = input.shape[1]\n\t        for channel in range(channels_num):\n\t            sp_list.append(self.spectrogram(input[:, channel, :], eps=eps))\n\t        output = torch.cat(sp_list, dim=1)\n\t        return output\n\t    def spectrogram_to_wav(self, input, spectrogram, length=None):\n\t        \"\"\"Spectrogram to waveform.\n", "        Args:\n\t          input: (batch_size, segment_samples, channels_num)\n\t          spectrogram: (batch_size, channels_num, time_steps, freq_bins)\n\t        Outputs:\n\t          output: (batch_size, segment_samples, channels_num)\n\t        \"\"\"\n\t        channels_num = input.shape[1]\n\t        wav_list = []\n\t        for channel in range(channels_num):\n\t            (real, imag) = self.stft(input[:, channel, :])\n", "            (_, cos, sin) = magphase(real, imag)\n\t            wav_list.append(self.istft(spectrogram[:,\n\t                                                   channel: channel + 1,\n\t                                                   :,\n\t                                                   :] * cos,\n\t                                       spectrogram[:,\n\t                                                   channel: channel + 1,\n\t                                                   :,\n\t                                                   :] * sin,\n\t                                       length))\n", "        output = torch.stack(wav_list, dim=1)\n\t        return output\n"]}
{"filename": "uss/models/film.py", "chunked_list": ["from typing import Dict, List\n\timport torch\n\timport torch.nn as nn\n\tfrom uss.models.base import init_layer\n\tdef get_film_meta(module: nn.Module) -> Dict:\n\t    r\"\"\"Get FiLM meta dict of a module.\n\t    Args:\n\t        module (nn.Module), the module to extract meta dict\n\t    Returns:\n\t        film_meta (Dict), FiLM meta dict\n", "    \"\"\"\n\t    film_meta = {}\n\t    if hasattr(module, 'has_film'):\\\n\t        if module.has_film:\n\t            film_meta['beta1'] = module.bn1.num_features\n\t            film_meta['beta2'] = module.bn2.num_features\n\t        else:\n\t            film_meta['beta1'] = 0\n\t            film_meta['beta2'] = 0\n\t    # Pre-order traversal of modules\n", "    for child_name, child_module in module.named_children():\n\t        child_meta = get_film_meta(child_module)\n\t        if len(child_meta) > 0:\n\t            film_meta[child_name] = child_meta\n\t    return film_meta\n\tclass FiLM(nn.Module):\n\t    def __init__(\n\t        self,\n\t        film_meta: Dict,\n\t        condition_size: int,\n", "    ) -> None:\n\t        r\"\"\"Create FiLM modules from film meta dict.\n\t        Args:\n\t            film_meta (Dict), e.g.,\n\t                {'encoder_block1': {'conv_block1': {'beta1': 32, 'beta2': 32}},\n\t                 ...}\n\t            condition_size: int\n\t        Returns:\n\t            None\n\t        \"\"\"\n", "        super(FiLM, self).__init__()\n\t        self.condition_size = condition_size\n\t        self.modules, _ = self._create_film_modules(\n\t            film_meta=film_meta,\n\t            prefix_names=[],\n\t        )\n\t    def _create_film_modules(\n\t        self,\n\t        film_meta: Dict,\n\t        prefix_names: List[str],\n", "    ):\n\t        r\"\"\"Create FiLM modules.\n\t        Args:\n\t            film_meta (Dict), e.g.,\n\t                {\"encoder_block1\": {\"conv_block1\": {\"beta1\": 32, \"beta2\": 32}},\n\t                 ...}\n\t            prefix_names (str), only used to get correct module name, e.g.,\n\t                [\"encoder_block1\", \"conv_block1\"]\n\t        \"\"\"\n\t        modules = {}\n", "        # Pre-order traversal of modules\n\t        for module_name, value in film_meta.items():\n\t            if isinstance(value, dict):\n\t                prefix_names.append(module_name)\n\t                modules[module_name], _ = self._create_film_modules(\n\t                    film_meta=value,\n\t                    prefix_names=prefix_names,\n\t                )\n\t            elif isinstance(value, int):\n\t                prefix_names.append(module_name)\n", "                unique_module_name = '->'.join(prefix_names)\n\t                modules[module_name] = self._add_film_layer_to_module(\n\t                    num_features=value,\n\t                    unique_module_name=unique_module_name,\n\t                )\n\t            prefix_names.pop()\n\t        return modules, prefix_names\n\t    def _add_film_layer_to_module(\n\t        self,\n\t        num_features: int,\n", "        unique_module_name: str,\n\t    ) -> nn.Module:\n\t        r\"\"\"Add a FiLM layer.\"\"\"\n\t        layer = nn.Linear(self.condition_size, num_features)\n\t        init_layer(layer)\n\t        self.add_module(name=unique_module_name, module=layer)\n\t        return layer\n\t    def _calculate_film_data(self, conditions, modules):\n\t        film_data = {}\n\t        # Pre-order traversal of modules\n", "        for module_name, module in modules.items():\n\t            if isinstance(module, dict):\n\t                film_data[module_name] = self._calculate_film_data(\n\t                    conditions, module)\n\t            elif isinstance(module, nn.Module):\n\t                film_data[module_name] = module(conditions)[:, :, None, None]\n\t        return film_data\n\t    def forward(self, conditions: torch.Tensor) -> Dict:\n\t        r\"\"\"Forward conditions to all FiLM layers to get FiLM data.\n\t        Args:\n", "            conditions (torch.Tensor): query net outputs,\n\t                (batch_size, condition_dim)\n\t        Returns:\n\t            film_dict (Dict): e.g., {\n\t                \"encoder_block1\": {\n\t                    \"conv_block1\": {\n\t                        \"beta1\": (16, 32, 1, 1),\n\t                        \"beta2\": (16, 32, 1, 1),\n\t                    },\n\t                    ...,\n", "                },\n\t                ...,\n\t            }\n\t        \"\"\"\n\t        film_dict = self._calculate_film_data(\n\t            conditions=conditions,\n\t            modules=self.modules,\n\t        )\n\t        return film_dict\n"]}
{"filename": "uss/models/resunet.py", "chunked_list": ["from typing import Dict, Tuple\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom einops import rearrange\n\tfrom torchlibrosa.stft import ISTFT, STFT, magphase\n\tfrom uss.models.base import Base, init_bn, init_layer\n\tfrom uss.models.film import FiLM, get_film_meta\n\tclass ConvBlockRes(nn.Module):\n", "    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        out_channels: int,\n\t        kernel_size: Tuple,\n\t        momentum: float,\n\t        has_film: bool,\n\t    ) -> None:\n\t        r\"\"\"Residual convolutional block.\"\"\"\n\t        super(ConvBlockRes, self).__init__()\n", "        padding = [kernel_size[0] // 2, kernel_size[1] // 2]\n\t        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n\t        self.bn2 = nn.BatchNorm2d(out_channels, momentum=momentum)\n\t        self.conv1 = nn.Conv2d(\n\t            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=kernel_size,\n\t            stride=(1, 1),\n\t            dilation=(1, 1),\n\t            padding=padding,\n", "            bias=False,\n\t        )\n\t        self.conv2 = nn.Conv2d(\n\t            in_channels=out_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=kernel_size,\n\t            stride=(1, 1),\n\t            dilation=(1, 1),\n\t            padding=padding,\n\t            bias=False,\n", "        )\n\t        if in_channels != out_channels:\n\t            self.shortcut = nn.Conv2d(\n\t                in_channels=in_channels,\n\t                out_channels=out_channels,\n\t                kernel_size=(1, 1),\n\t                stride=(1, 1),\n\t                padding=(0, 0),\n\t            )\n\t            self.is_shortcut = True\n", "        else:\n\t            self.is_shortcut = False\n\t        self.has_film = has_film\n\t        self.init_weights()\n\t    def init_weights(self) -> None:\n\t        r\"\"\"Initialize weights.\"\"\"\n\t        init_bn(self.bn1)\n\t        init_bn(self.bn2)\n\t        init_layer(self.conv1)\n\t        init_layer(self.conv2)\n", "        if self.is_shortcut:\n\t            init_layer(self.shortcut)\n\t    def forward(self,\n\t                input_tensor: torch.Tensor,\n\t                film_dict: Dict\n\t                ) -> torch.Tensor:\n\t        r\"\"\"Forward input feature maps to the encoder block.\n\t        Args:\n\t            input_tensor (torch.Tensor), (B, C, T, F)\n\t            film_dict (Dict)\n", "        Returns:\n\t            output (torch.Tensor), (B, C, T, F)\n\t        \"\"\"\n\t        b1 = film_dict['beta1']\n\t        b2 = film_dict['beta2']\n\t        x = self.conv1(\n\t            F.leaky_relu_(\n\t                self.bn1(input_tensor) + b1,\n\t                negative_slope=0.01))\n\t        x = self.conv2(F.leaky_relu_(self.bn2(x) + b2, negative_slope=0.01))\n", "        if self.is_shortcut:\n\t            output = self.shortcut(input_tensor) + x\n\t        else:\n\t            output = input_tensor + x\n\t        return output\n\tclass EncoderBlockRes1B(nn.Module):\n\t    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        out_channels: int,\n", "        kernel_size: Tuple,\n\t        downsample: Tuple,\n\t        momentum: float,\n\t        has_film: bool,\n\t    ) -> None:\n\t        r\"\"\"Encoder block.\"\"\"\n\t        super(EncoderBlockRes1B, self).__init__()\n\t        self.conv_block1 = ConvBlockRes(\n\t            in_channels, out_channels, kernel_size, momentum, has_film,\n\t        )\n", "        self.downsample = downsample\n\t    def forward(self,\n\t                input_tensor: torch.Tensor,\n\t                film_dict: Dict\n\t                ) -> torch.Tensor:\n\t        r\"\"\"Forward input feature maps to the encoder block.\n\t        Args:\n\t            input_tensor (torch.Tensor), (B, C_in, T, F)\n\t            film_dict (Dict)\n\t        Returns:\n", "            encoder (torch.Tensor): (B, C_out, T, F)\n\t            encoder_pool (torch.Tensor): (B, C_out, T / downsample, F / downsample)\n\t        \"\"\"\n\t        encoder = self.conv_block1(input_tensor, film_dict['conv_block1'])\n\t        encoder_pool = F.avg_pool2d(encoder, kernel_size=self.downsample)\n\t        return encoder_pool, encoder\n\tclass DecoderBlockRes1B(nn.Module):\n\t    def __init__(\n\t        self,\n\t        in_channels: int,\n", "        out_channels: int,\n\t        kernel_size: Tuple,\n\t        upsample: Tuple,\n\t        momentum: float,\n\t        has_film: bool,\n\t    ):\n\t        r\"\"\"Decoder block.\"\"\"\n\t        super(DecoderBlockRes1B, self).__init__()\n\t        self.kernel_size = kernel_size\n\t        self.stride = upsample\n", "        self.bn1 = nn.BatchNorm2d(in_channels, momentum=momentum)\n\t        self.bn2 = nn.BatchNorm2d(in_channels, momentum=momentum)\n\t        # Do not delate the dummy self.bn2. FiLM need self.bn2 to parse the\n\t        # FiLM meta correctly.\n\t        self.conv1 = torch.nn.ConvTranspose2d(\n\t            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=self.stride,\n\t            stride=self.stride,\n\t            padding=(0, 0),\n", "            bias=False,\n\t            dilation=(1, 1),\n\t        )\n\t        self.conv_block2 = ConvBlockRes(\n\t            in_channels=out_channels * 2,\n\t            out_channels=out_channels,\n\t            kernel_size=kernel_size,\n\t            momentum=momentum,\n\t            has_film=has_film,\n\t        )\n", "        self.has_film = has_film\n\t        self.init_weights()\n\t    def init_weights(self):\n\t        r\"\"\"Initialize weights.\"\"\"\n\t        init_bn(self.bn1)\n\t        init_layer(self.conv1)\n\t    def forward(\n\t        self,\n\t        input_tensor: torch.Tensor,\n\t        concat_tensor: torch.Tensor,\n", "        film_dict: Dict,\n\t    ) -> torch.Tensor:\n\t        r\"\"\"Forward input feature maps to the decoder block.\n\t        Args:\n\t            input_tensor (torch.Tensor), (B, C_in, T, F)\n\t            film_dict (Dict)\n\t        Returns:\n\t            output (torch.Tensor): (B, C_out, T * upsample, F * upsample)\n\t        \"\"\"\n\t        b1 = film_dict['beta1']\n", "        x = self.conv1(F.leaky_relu_(self.bn1(input_tensor) + b1))\n\t        # (B, C_out, T * upsample, F * upsample)\n\t        x = torch.cat((x, concat_tensor), dim=1)\n\t        # (B, C_out * 2, T * upsample, F * upsample)\n\t        output = self.conv_block2(x, film_dict['conv_block2'])\n\t        # output: (B, C_out, T * upsample, F * upsample)\n\t        return output\n\tclass ResUNet30_Base(nn.Module, Base):\n\t    def __init__(self,\n\t                 input_channels: int,\n", "                 output_channels: int,\n\t                 ) -> None:\n\t        r\"\"\"Base separation model.\n\t        Args:\n\t            input_channels (int), audio channels, e.g., 1 | 2\n\t            output_channels (int), audio channels, e.g., 1 | 2\n\t        \"\"\"\n\t        super(ResUNet30_Base, self).__init__()\n\t        window_size = 2048\n\t        hop_size = 320\n", "        center = True\n\t        pad_mode = \"reflect\"\n\t        window = \"hann\"\n\t        momentum = 0.01\n\t        self.output_channels = output_channels\n\t        self.K = 3  # mag, cos, sin\n\t        # This number equals 2^{#encoder_blcoks}\n\t        self.time_downsample_ratio = 2 ** 5\n\t        self.stft = STFT(\n\t            n_fft=window_size,\n", "            hop_length=hop_size,\n\t            win_length=window_size,\n\t            window=window,\n\t            center=center,\n\t            pad_mode=pad_mode,\n\t            freeze_parameters=True,\n\t        )\n\t        self.istft = ISTFT(\n\t            n_fft=window_size,\n\t            hop_length=hop_size,\n", "            win_length=window_size,\n\t            window=window,\n\t            center=center,\n\t            pad_mode=pad_mode,\n\t            freeze_parameters=True,\n\t        )\n\t        self.bn0 = nn.BatchNorm2d(window_size // 2 + 1, momentum=momentum)\n\t        self.pre_conv = nn.Conv2d(\n\t            in_channels=input_channels,\n\t            out_channels=32,\n", "            kernel_size=(1, 1),\n\t            stride=(1, 1),\n\t            padding=(0, 0),\n\t            bias=True,\n\t        )\n\t        self.encoder_block1 = EncoderBlockRes1B(\n\t            in_channels=32,\n\t            out_channels=32,\n\t            kernel_size=(3, 3),\n\t            downsample=(2, 2),\n", "            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.encoder_block2 = EncoderBlockRes1B(\n\t            in_channels=32,\n\t            out_channels=64,\n\t            kernel_size=(3, 3),\n\t            downsample=(2, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n", "        )\n\t        self.encoder_block3 = EncoderBlockRes1B(\n\t            in_channels=64,\n\t            out_channels=128,\n\t            kernel_size=(3, 3),\n\t            downsample=(2, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.encoder_block4 = EncoderBlockRes1B(\n", "            in_channels=128,\n\t            out_channels=256,\n\t            kernel_size=(3, 3),\n\t            downsample=(2, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.encoder_block5 = EncoderBlockRes1B(\n\t            in_channels=256,\n\t            out_channels=384,\n", "            kernel_size=(3, 3),\n\t            downsample=(2, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.encoder_block6 = EncoderBlockRes1B(\n\t            in_channels=384,\n\t            out_channels=384,\n\t            kernel_size=(3, 3),\n\t            downsample=(1, 2),\n", "            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.conv_block7a = EncoderBlockRes1B(\n\t            in_channels=384,\n\t            out_channels=384,\n\t            kernel_size=(3, 3),\n\t            downsample=(1, 1),\n\t            momentum=momentum,\n\t            has_film=True,\n", "        )\n\t        self.decoder_block1 = DecoderBlockRes1B(\n\t            in_channels=384,\n\t            out_channels=384,\n\t            kernel_size=(3, 3),\n\t            upsample=(1, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.decoder_block2 = DecoderBlockRes1B(\n", "            in_channels=384,\n\t            out_channels=384,\n\t            kernel_size=(3, 3),\n\t            upsample=(2, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.decoder_block3 = DecoderBlockRes1B(\n\t            in_channels=384,\n\t            out_channels=256,\n", "            kernel_size=(3, 3),\n\t            upsample=(2, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.decoder_block4 = DecoderBlockRes1B(\n\t            in_channels=256,\n\t            out_channels=128,\n\t            kernel_size=(3, 3),\n\t            upsample=(2, 2),\n", "            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.decoder_block5 = DecoderBlockRes1B(\n\t            in_channels=128,\n\t            out_channels=64,\n\t            kernel_size=(3, 3),\n\t            upsample=(2, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n", "        )\n\t        self.decoder_block6 = DecoderBlockRes1B(\n\t            in_channels=64,\n\t            out_channels=32,\n\t            kernel_size=(3, 3),\n\t            upsample=(2, 2),\n\t            momentum=momentum,\n\t            has_film=True,\n\t        )\n\t        self.after_conv = nn.Conv2d(\n", "            in_channels=32,\n\t            out_channels=output_channels * self.K,\n\t            kernel_size=(1, 1),\n\t            stride=(1, 1),\n\t            padding=(0, 0),\n\t            bias=True,\n\t        )\n\t        self.init_weights()\n\t    def init_weights(self):\n\t        r\"\"\"Initialize weights.\"\"\"\n", "        init_bn(self.bn0)\n\t        init_layer(self.pre_conv)\n\t        init_layer(self.after_conv)\n\t    def feature_maps_to_wav(\n\t        self,\n\t        input_tensor: torch.Tensor,\n\t        sp: torch.Tensor,\n\t        sin_in: torch.Tensor,\n\t        cos_in: torch.Tensor,\n\t        audio_length: int,\n", "    ) -> torch.Tensor:\n\t        r\"\"\"Convert feature maps to waveform.\n\t        Args:\n\t            input_tensor: (B, input_channels, T, F)\n\t            sp: (B, output_channels, T, F)\n\t            sin_in: (B, output_channels, T, F)\n\t            cos_in: (B, output_channels, T, F)\n\t            (There is input_channels == output_channels for the source separation task.)\n\t        Outputs:\n\t            waveform: (B, output_channels, audio_samples)\n", "        \"\"\"\n\t        x = rearrange(\n\t            input_tensor,\n\t            'b (c k) t f -> b c k t f',\n\t            c=self.output_channels)\n\t        mask_mag = torch.sigmoid(x[:, :, 0, :, :])\n\t        mask_real = torch.tanh(x[:, :, 1, :, :])\n\t        mask_imag = torch.tanh(x[:, :, 2, :, :])\n\t        _, mask_cos, mask_sin = magphase(mask_real, mask_imag)\n\t        # mask_cos, mask_sin: (B, output_channels, T, F)\n", "        # Y = |Y|cosY + j|Y|sinY\n\t        #   = |Y|cos(X + M) + j|Y|sin(X + M)\n\t        #   = |Y|(cosX cosM - sinX sinM) + j|Y|(sinX cosM + cosX sinM)\n\t        out_cos = (\n\t            cos_in * mask_cos - sin_in * mask_sin\n\t        )\n\t        out_sin = (\n\t            sin_in * mask_cos + cos_in * mask_sin\n\t        )\n\t        # out_cos: (B, output_channels, T, F)\n", "        # out_sin: (B, output_channels, T, F)\n\t        # Calculate |Y|.\n\t        out_mag = F.relu_(sp * mask_mag)\n\t        # out_mag: (B, output_channels, T, F)\n\t        # Calculate Y_{real} and Y_{imag} for ISTFT.\n\t        out_real = out_mag * out_cos\n\t        out_imag = out_mag * out_sin\n\t        # out_real, out_imag: (B, output_channels, T, F)\n\t        # Reshape to (N, 1, T, F) for ISTFT\n\t        out_real = rearrange(out_real, 'b c t f -> (b c) t f').unsqueeze(1)\n", "        out_imag = rearrange(out_imag, 'b c t f -> (b c) t f').unsqueeze(1)\n\t        # ISTFT\n\t        x = self.istft(out_real, out_imag, audio_length)\n\t        # (B * output_channels, audio_samples)\n\t        # Reshape to (B, output_channels, audio_samples)\n\t        waveform = rearrange(x, '(b c) t -> b c t', c=self.output_channels)\n\t        return waveform\n\t    def forward(self, mixtures, film_dict):\n\t        r\"\"\"Forward mixtures and conditions to separate target sources.\n\t        Args:\n", "            input (torch.Tensor): (batch_size, output_channels, segment_samples)\n\t        Outputs:\n\t            output_dict: {\n\t                \"waveform\": (batch_size, output_channels, segment_samples),\n\t            }\n\t        \"\"\"\n\t        mag, cos_in, sin_in = self.wav_to_spectrogram_phase(mixtures)\n\t        x = mag\n\t        # Batch normalization\n\t        x = x.transpose(1, 3)\n", "        x = self.bn0(x)\n\t        x = x.transpose(1, 3)   # shape: (B, input_channels, T, F)\n\t        # Pad spectrogram to be evenly divided by downsample ratio\n\t        origin_len = x.shape[2]\n\t        pad_len = (int(np.ceil(x.shape[2] /\n\t                               self.time_downsample_ratio)) *\n\t                   self.time_downsample_ratio -\n\t                   origin_len)\n\t        x = F.pad(x, pad=(0, 0, 0, pad_len))\n\t        # x: (B, input_channels, T, F)\n", "        # Let frequency bins be evenly divided by 2, e.g., 513 -> 512\n\t        x = x[..., 0: x.shape[-1] - 1]  # (B, input_channels, T, F)\n\t        # UNet\n\t        x = self.pre_conv(x)\n\t        x1_pool, x1 = self.encoder_block1(\n\t            x, film_dict['encoder_block1'])  # x1_pool: (B, 32, T / 2, F / 2)\n\t        x2_pool, x2 = self.encoder_block2(\n\t            x1_pool, film_dict['encoder_block2'])  # x2_pool: (B, 64, T / 4, F / 4)\n\t        x3_pool, x3 = self.encoder_block3(\n\t            x2_pool, film_dict['encoder_block3'])  # x3_pool: (B, 128, T / 8, F / 8)\n", "        # x4_pool: (B, 256, T / 16, F / 16)\n\t        x4_pool, x4 = self.encoder_block4(x3_pool, film_dict['encoder_block4'])\n\t        # x5_pool: (B, 384, T / 32, F / 32)\n\t        x5_pool, x5 = self.encoder_block5(x4_pool, film_dict['encoder_block5'])\n\t        # x6_pool: (B, 384, T / 32, F / 64)\n\t        x6_pool, x6 = self.encoder_block6(x5_pool, film_dict['encoder_block6'])\n\t        x_center, _ = self.conv_block7a(\n\t            x6_pool, film_dict['conv_block7a'])  # (B, 384, T / 32, F / 64)\n\t        # (B, 384, T / 32, F / 32)\n\t        x7 = self.decoder_block1(x_center, x6, film_dict['decoder_block1'])\n", "        # (B, 384, T / 16, F / 16)\n\t        x8 = self.decoder_block2(x7, x5, film_dict['decoder_block2'])\n\t        x9 = self.decoder_block3(\n\t            x8, x4, film_dict['decoder_block3'])  # (B, 256, T / 8, F / 8)\n\t        x10 = self.decoder_block4(\n\t            x9, x3, film_dict['decoder_block4'])  # (B, 128, T / 4, F / 4)\n\t        x11 = self.decoder_block5(\n\t            x10, x2, film_dict['decoder_block5'])  # (B, 64, T / 2, F / 2)\n\t        x12 = self.decoder_block6(\n\t            x11, x1, film_dict['decoder_block6'])  # (B, 32, T, F)\n", "        x = self.after_conv(x12)\n\t        # Recover shape\n\t        x = F.pad(x, pad=(0, 1))\n\t        x = x[:, :, 0:origin_len, :]\n\t        audio_length = mixtures.shape[2]\n\t        # Convert feature maps to waveform\n\t        separated_audio = self.feature_maps_to_wav(\n\t            input_tensor=x,\n\t            sp=mag,\n\t            sin_in=sin_in,\n", "            cos_in=cos_in,\n\t            audio_length=audio_length,\n\t        )\n\t        # shape:B, output_channels, segment_samples)\n\t        output_dict = {'waveform': separated_audio}\n\t        return output_dict\n\tclass ResUNet30(nn.Module):\n\t    def __init__(self,\n\t                 input_channels: int,\n\t                 output_channels: int,\n", "                 condition_size: int,\n\t                 ) -> None:\n\t        r\"\"\"Universal separation model.\n\t        Args:\n\t            input_channels (int), audio channels, e.g., 1 | 2\n\t            output_channels (int), audio channels, e.g., 1 | 2\n\t            condition_size (int), FiLM condition size, e.g., 527 | 2048\n\t        \"\"\"\n\t        super(ResUNet30, self).__init__()\n\t        self.base = ResUNet30_Base(\n", "            input_channels=input_channels,\n\t            output_channels=output_channels,\n\t        )\n\t        self.film_meta = get_film_meta(\n\t            module=self.base,\n\t        )\n\t        self.film = FiLM(\n\t            film_meta=self.film_meta,\n\t            condition_size=condition_size\n\t        )\n", "    def forward(self, input_dict: Dict) -> Dict:\n\t        r\"\"\"Forward mixtures and conditions to separate target sources.\n\t        Args:\n\t            input_dict (Dict): {\n\t                \"mixture\": (batch_size, audio_channels, audio_samples),\n\t                \"condition\": (batch_size, condition_dim),\n\t            }\n\t        Returns:\n\t            output_dict (Dict): {\n\t                \"waveform\": (batch_size, audio_channels, audio_samples)\n", "            }\n\t        \"\"\"\n\t        mixtures = input_dict['mixture']\n\t        conditions = input_dict['condition']\n\t        film_dict = self.film(\n\t            conditions=conditions,\n\t        )\n\t        output_dict = self.base(\n\t            mixtures=mixtures,\n\t            film_dict=film_dict,\n", "        )\n\t        return output_dict\n"]}
{"filename": "uss/models/pl_modules.py", "chunked_list": ["from typing import Callable, Dict\n\timport lightning.pytorch as pl\n\timport torch\n\timport torch.nn as nn\n\timport torch.optim as optim\n\tfrom torch.optim.lr_scheduler import LambdaLR\n\tclass LitSeparation(pl.LightningModule):\n\t    def __init__(\n\t        self,\n\t        ss_model: nn.Module,\n", "        anchor_segment_detector: nn.Module,\n\t        anchor_segment_mixer: nn.Module,\n\t        query_net: nn.Module,\n\t        loss_function: Callable,\n\t        optimizer_type: str,\n\t        learning_rate: float,\n\t        lr_lambda_func: Callable,\n\t    ) -> None:\n\t        r\"\"\"Pytorch Lightning wrapper of PyTorch model, including forward,\n\t        optimization of model, etc.\n", "        Args:\n\t            ss_model (nn.Module): universal source separation module\n\t            anchor_segment_detector (nn.Module): used to detect anchor segments\n\t                from audio clips\n\t            anchor_segment_mixer (nn.Module): used to mix segments into mixtures\n\t            query_net (nn.Module): used to extract conditions for separation\n\t            loss_function (Callable): loss function to train the separation model\n\t            optimizer_type (str): e.g., \"AdamW\"\n\t            learning_rate (float)\n\t            lr_lambda_func (Callable), learning rate scaler\n", "        \"\"\"\n\t        super().__init__()\n\t        self.ss_model = ss_model\n\t        self.anchor_segment_detector = anchor_segment_detector\n\t        self.anchor_segment_mixer = anchor_segment_mixer\n\t        self.query_net = query_net\n\t        self.loss_function = loss_function\n\t        self.optimizer_type = optimizer_type\n\t        self.learning_rate = learning_rate\n\t        self.lr_lambda_func = lr_lambda_func\n", "    def training_step(\n\t        self,\n\t        batch_data_dict: Dict,\n\t        batch_idx: int\n\t    ) -> torch.float:\n\t        r\"\"\"Forward a mini-batch data to model, calculate loss function, and\n\t        train for one step. A mini-batch data is evenly distributed on multiple\n\t        devices (if there are) for parallel training.\n\t        Args:\n\t            batch_data_dict (Dict): e.g. {\n", "                'hdf5_path': (batch_size,),\n\t                'index_in_hdf5': (batch_size,),\n\t                'audio_name': (batch_size,),\n\t                'waveform': (batch_size,),\n\t                'target': (batch_size,),\n\t                'class_id': (batch_size,),\n\t            }\n\t            batch_idx: int\n\t        Returns:\n\t            loss (torch.float): loss function of this mini-batch\n", "        \"\"\"\n\t        # Mine anchor segments from audio clips\n\t        segments_dict = self.anchor_segment_detector(\n\t            waveforms=batch_data_dict['waveform'],\n\t            class_ids=batch_data_dict['class_id'],\n\t        )\n\t        # segments_dict: {\n\t        #     \"waveform\": (batch_size, segment_samples),\n\t        #     \"class_id\": (batch_size,),\n\t        #     \"bgn_sample\": (batch_size,),\n", "        #     \"end_sample\": (batch_size,),\n\t        # }\n\t        # Mix segments into mixtures and execute energy augmentation\n\t        mixtures, segments = self.anchor_segment_mixer(\n\t            waveforms=segments_dict['waveform'],\n\t        )\n\t        # mixtures: (batch_size, segment_samples)\n\t        # segments: (batch_size, segment_samples)\n\t        # Use query net to calculate conditional embedding\n\t        conditions = self.query_net(\n", "            source=segments,\n\t        )['output']\n\t        # conditions: (batch_size, condition_dim)\n\t        input_dict = {\n\t            'mixture': mixtures[:, None, :],\n\t            'condition': conditions,\n\t        }\n\t        target_dict = {\n\t            'segment': segments[:, None, :],\n\t        }\n", "        # Do separation using mixtures and conditions as input\n\t        self.ss_model.train()\n\t        sep_segment = self.ss_model(input_dict)['waveform']\n\t        # sep_segment: (batch_size, 1, segment_samples)\n\t        output_dict = {\n\t            'segment': sep_segment,\n\t        }\n\t        # Calculate loss\n\t        loss = self.loss_function(output_dict, target_dict)\n\t        return loss\n", "    def configure_optimizers(self) -> Dict:\n\t        r\"\"\"Configure optimizer.\n\t        \"\"\"\n\t        if self.optimizer_type == \"AdamW\":\n\t            optimizer = optim.AdamW(\n\t                params=self.ss_model.parameters(),\n\t                lr=self.learning_rate,\n\t                betas=(0.9, 0.999),\n\t                eps=1e-08,\n\t                weight_decay=0.0,\n", "                amsgrad=True,\n\t            )\n\t        else:\n\t            raise NotImplementedError\n\t        scheduler = LambdaLR(optimizer, self.lr_lambda_func)\n\t        output_dict = {\n\t            \"optimizer\": optimizer,\n\t            \"lr_scheduler\": {\n\t                'scheduler': scheduler,\n\t                'interval': 'step',\n", "                'frequency': 1,\n\t            }\n\t        }\n\t        return output_dict\n\tdef get_model_class(model_type: str) -> nn.Module:\n\t    r\"\"\"Get separation module by model_type.\"\"\"\n\t    if model_type == 'ResUNet30':\n\t        from uss.models.resunet import ResUNet30\n\t        return ResUNet30\n\t    else:\n", "        raise NotImplementedError\n"]}
{"filename": "uss/callbacks/base.py", "chunked_list": ["import os\n\timport lightning.pytorch as pl\n\tfrom lightning.pytorch.utilities import rank_zero_only\n\tclass CheckpointEveryNSteps(pl.Callback):\n\t    def __init__(\n\t        self,\n\t        checkpoints_dir,\n\t        save_step_frequency,\n\t    ) -> None:\n\t        r\"\"\"Save a checkpoint every N steps.\n", "        Args:\n\t            checkpoints_dir (str): directory to save checkpoints\n\t            save_step_frequency (int): save checkpoint every N step\n\t        \"\"\"\n\t        self.checkpoints_dir = checkpoints_dir\n\t        self.save_step_frequency = save_step_frequency\n\t    @rank_zero_only\n\t    def on_train_batch_end(self, *args, **kwargs) -> None:\n\t        r\"\"\"Save a checkpoint every N steps.\"\"\"\n\t        trainer = args[0]\n", "        global_step = trainer.global_step\n\t        if global_step == 1 or global_step % self.save_step_frequency == 0:\n\t            ckpt_path = os.path.join(\n\t                self.checkpoints_dir,\n\t                \"steps={}.ckpt\".format(global_step))\n\t            trainer.save_checkpoint(ckpt_path)\n\t            print(\"Save checkpoint to {}\".format(ckpt_path))\n"]}
{"filename": "uss/callbacks/evaluate.py", "chunked_list": ["import logging\n\timport lightning.pytorch as pl\n\tfrom lightning.pytorch.utilities import rank_zero_only\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom uss.evaluate import AudioSetEvaluator\n\tfrom uss.utils import StatisticsContainer, get_mean_sdr_from_dict\n\tclass EvaluateCallback(pl.Callback):\n\t    def __init__(\n\t        self,\n\t        pl_model: pl.LightningModule,\n", "        balanced_train_eval_dir: str,\n\t        test_eval_dir: str,\n\t        classes_num: int,\n\t        max_eval_per_class: int,\n\t        evaluate_step_frequency: int,\n\t        summary_writer: SummaryWriter,\n\t        statistics_path: str,\n\t    ) -> None:\n\t        \"\"\"Evaluate on AudioSet separation.\n\t        Args:\n", "            pl_model (pl.LightningModule): universal source separation module\n\t            balanced_train_eval_dir (str): directory of balanced train set for evaluation\n\t            test_eval_dir (str): directory of test set for evaluation\n\t            classes_num (int): sound classes number\n\t            max_eval_per_class (int): the number of samples to evaluate for each sound class\n\t            evaluate_step_frequency (int): evaluate every N steps\n\t            summary_writer (SummaryWriter): used to write TensorBoard logs\n\t            statistics_path (str): path to write statistics\n\t        Returns:\n\t            None\n", "        \"\"\"\n\t        # Evaluators\n\t        self.balanced_train_evaluator = AudioSetEvaluator(\n\t            audios_dir=balanced_train_eval_dir,\n\t            classes_num=classes_num,\n\t            max_eval_per_class=max_eval_per_class,\n\t        )\n\t        self.test_evaluator = AudioSetEvaluator(\n\t            audios_dir=test_eval_dir,\n\t            classes_num=classes_num,\n", "            max_eval_per_class=max_eval_per_class,\n\t        )\n\t        self.pl_model = pl_model\n\t        self.evaluate_step_frequency = evaluate_step_frequency\n\t        self.summary_writer = summary_writer\n\t        # Statistics container\n\t        self.statistics_container = StatisticsContainer(statistics_path)\n\t    @rank_zero_only\n\t    def on_train_batch_end(self, *args, **kwargs):\n\t        r\"\"\"Evaluate every #evaluate_step_frequency steps.\"\"\"\n", "        trainer = args[0]\n\t        epoch = trainer.current_epoch\n\t        global_step = trainer.global_step\n\t        if global_step == 1 or global_step % self.evaluate_step_frequency == 0:\n\t            for split, evaluator in zip([\"balanced_train\", \"test\"], [\n\t                                        self.balanced_train_evaluator, self.test_evaluator]):\n\t                logging.info(\"------ {} ------\".format(split))\n\t                stats_dict = evaluator(pl_model=self.pl_model)\n\t                median_sdris_dict = AudioSetEvaluator.get_median_metrics(\n\t                    stats_dict=stats_dict,\n", "                    metric_type=\"sdris_dict\",\n\t                )\n\t                median_sdri = get_mean_sdr_from_dict(median_sdris_dict)\n\t                logging.info(\"Average SDRi: {:.3f}\".format(median_sdri))\n\t                self.summary_writer.add_scalar(\n\t                    \"SDRi/{}\".format(split),\n\t                    global_step=global_step,\n\t                    scalar_value=median_sdri)\n\t                logging.info(\n\t                    \"    Flush tensorboard logs to {}\".format(\n", "                        self.summary_writer.log_dir))\n\t                self.statistics_container.append(\n\t                    steps=global_step,\n\t                    statistics={\"sdri_dict\": median_sdris_dict},\n\t                    split=split,\n\t                    flush=True,\n\t                )\n"]}
{"filename": "uss/dataset_creation/create_audioset_evaluation_meta.py", "chunked_list": ["import argparse\n\timport os\n\timport multiprocessing\n\timport pathlib\n\timport numpy as np\n\timport soundfile\n\tfrom torch.utils.data import DataLoader\n\tfrom uss.config import (CLASSES_NUM, CLIP_SECONDS, FRAMES_PER_SECOND,\n\t                        SAMPLE_RATE, panns_paths_dict)\n\tfrom uss.data.anchor_segment_detectors import AnchorSegmentDetector\n", "from uss.data.anchor_segment_mixers import AnchorSegmentMixer\n\tfrom uss.data.datamodules import collate_fn\n\tfrom uss.data.datasets import Dataset\n\tfrom uss.data.samplers import BalancedSampler\n\tfrom uss.utils import get_path, load_pretrained_panns\n\tdef create_evaluation_meta(args):\n\t    r\"\"\"Create csv containing information of anchor segments for creating\n\t    mixtures. For each sound class k, we select M anchor segments that will be\n\t    randomly mixed with anchor segments that do not contain sound class k. In\n\t    total, there are classes_num x M mixtures to separate. Anchor segments are\n", "    short segments (such as 2 s) detected by a pretrained sound event detection\n\t    system on 10-second audio clips from AudioSet. All time stamps of anchor\n\t    segments are written into a csv file. E.g.,\n\t    .. code-block:: csv\n\t        index_in_hdf5   audio_name  bgn_sample  end_sample  class_id    mix_rank\n\t        4768    YC0j69NCIKfw.wav    140480  204480  347 0\n\t        15640   Yip4ZCCgoVXc.wav    81920   145920  496 1\n\t        10614   YTRxF5y6hFbE.wav    130240  194240  270 0\n\t        9969    YRN1ho4G-W0o.wav    256000  320000  305 1\n\t        ...\n", "    When creating mixtures, for example:\n\t        mixture_0 = YC0j69NCIKfw.wav + Yip4ZCCgoVXc.wav\n\t        mixture_1 = YTRxF5y6hFbE.wav + YRN1ho4G-W0o.wav\n\t        ...\n\t    Args:\n\t        workspace: str, path\n\t        split: str, 'balanced_train' | 'test'\n\t        gpus: int\n\t        config_yaml: str, path of config file\n\t    \"\"\"\n", "    # arguments & parameters\n\t    workspace = args.workspace\n\t    split = args.split\n\t    output_audios_dir = args.output_audios_dir\n\t    output_meta_csv_path = args.output_meta_csv_path\n\t    device = args.device\n\t    sample_rate = SAMPLE_RATE\n\t    frames_per_second = FRAMES_PER_SECOND\n\t    clip_seconds = CLIP_SECONDS\n\t    classes_num = CLASSES_NUM\n", "    eval_segments_per_class = 100\n\t    segment_seconds = 2.\n\t    anchor_segment_detect_mode = \"max_area\"\n\t    match_energy = True\n\t    mix_num = 2\n\t    batch_size = 32\n\t    steps_per_epoch = 10000\n\t    num_workers = min(16, multiprocessing.cpu_count())\n\t    sed_model_type = \"Cnn14_DecisionLevelMax\"\n\t    if split == 'balanced_train':\n", "        indexes_hdf5_path = os.path.join(\n\t            workspace, \"hdf5s/indexes/balanced_train.h5\")\n\t    elif split == 'test':\n\t        indexes_hdf5_path = os.path.join(workspace, \"hdf5s/indexes/eval.h5\")\n\t    # E.g., indexes_hdf5 looks like: {\n\t    #     'audio_name': (audios_num,),\n\t    #     'hdf5_path': (audios_num,),\n\t    #     'index_in_hdf5': (audios_num,),\n\t    #     'target': (audios_num, classes_num)\n\t    # }\n", "    sed_model = load_pretrained_panns(\n\t        model_type=sed_model_type,\n\t        checkpoint_path=get_path(panns_paths_dict[sed_model_type]),\n\t        freeze=True,\n\t    ).to(device)\n\t    # dataset\n\t    dataset = Dataset(\n\t        steps_per_epoch=steps_per_epoch,\n\t    )\n\t    # sampler\n", "    sampler = BalancedSampler(\n\t        indexes_hdf5_path=indexes_hdf5_path,\n\t        batch_size=batch_size,\n\t        steps_per_epoch=steps_per_epoch,\n\t    )\n\t    dataloader = DataLoader(\n\t        dataset=dataset,\n\t        batch_sampler=sampler,\n\t        collate_fn=collate_fn,\n\t        num_workers=num_workers,\n", "        pin_memory=True,\n\t        persistent_workers=False,\n\t    )\n\t    anchor_segment_detector = AnchorSegmentDetector(\n\t        sed_model=sed_model,\n\t        clip_seconds=clip_seconds,\n\t        segment_seconds=segment_seconds,\n\t        frames_per_second=frames_per_second,\n\t        sample_rate=sample_rate,\n\t        detect_mode=anchor_segment_detect_mode,\n", "    ).to(device)\n\t    anchor_segment_mixer = AnchorSegmentMixer(\n\t        mix_num=mix_num,\n\t        match_energy=match_energy,\n\t    ).to(device)\n\t    count_dict = {class_id: 0 for class_id in range(classes_num)}\n\t    meta_dict = {\n\t        'audio_name': [],\n\t    }\n\t    for i in range(mix_num):\n", "        meta_dict['source{}_name'.format(i + 1)] = []\n\t        meta_dict['source{}_class_id'.format(i + 1)] = []\n\t        meta_dict['source{}_onset'.format(i + 1)] = []\n\t    for class_id in range(classes_num):\n\t        sub_dir = os.path.join(\n\t            output_audios_dir,\n\t            \"class_id={}\".format(class_id))\n\t        os.makedirs(sub_dir, exist_ok=True)\n\t    for batch_index, batch_data_dict in enumerate(dataloader):\n\t        batch_data_dict['waveform'] = batch_data_dict['waveform'].to(device)\n", "        # (batch_size, clip_samples)\n\t        segments_dict = anchor_segment_detector(\n\t            waveforms=batch_data_dict['waveform'],\n\t            class_ids=batch_data_dict['class_id'],\n\t        )\n\t        mixtures, segments = anchor_segment_mixer(\n\t            waveforms=segments_dict['waveform'],\n\t        )\n\t        mixtures = mixtures.data.cpu().numpy()\n\t        segments = segments.data.cpu().numpy()\n", "        source_names = batch_data_dict['audio_name']\n\t        class_ids = segments_dict['class_id']\n\t        bgn_samples = segments_dict['bgn_sample'].data.cpu().numpy()\n\t        for n in range(batch_size):\n\t            class_id = class_ids[n]\n\t            if count_dict[class_id] < eval_segments_per_class:\n\t                mixture_name = \"class_id={},index={:03d},mixture.wav\".format(\n\t                    class_id, count_dict[class_id])\n\t                source_name = \"class_id={},index={:03d},source.wav\".format(\n\t                    class_id, count_dict[class_id])\n", "                mixture_path = os.path.join(\n\t                    output_audios_dir,\n\t                    \"class_id={}\".format(class_id),\n\t                    mixture_name)\n\t                source_path = os.path.join(\n\t                    output_audios_dir,\n\t                    \"class_id={}\".format(class_id),\n\t                    source_name)\n\t                soundfile.write(\n\t                    file=mixture_path,\n", "                    data=mixtures[n],\n\t                    samplerate=sample_rate)\n\t                soundfile.write(\n\t                    file=source_path,\n\t                    data=segments[n],\n\t                    samplerate=sample_rate)\n\t                print(\"Write out to {}\".format(mixture_path))\n\t                print(\"Write out to {}\".format(source_path))\n\t                # Write mixing information into a csv file.\n\t                meta_dict['audio_name'].append(mixture_name)\n", "                for i in range(mix_num):\n\t                    meta_dict['source{}_name'.format(\n\t                        i + 1)].append(source_names[(n + i) % batch_size])\n\t                    meta_dict['source{}_onset'.format(\n\t                        i + 1)].append(bgn_samples[(n + i) % batch_size] / sample_rate)\n\t                    meta_dict['source{}_class_id'.format(\n\t                        i + 1)].append(class_ids[(n + i) % batch_size])\n\t                ###\n\t                meta_dict['audio_name'].append(source_name)\n\t                meta_dict['source1_name'].append(source_names[n])\n", "                meta_dict['source1_onset'].append(bgn_samples[n] / sample_rate)\n\t                meta_dict['source1_class_id'].append(class_ids[n])\n\t                for i in range(1, mix_num):\n\t                    meta_dict['source{}_name'.format(i + 1)].append(\"\")\n\t                    meta_dict['source{}_onset'.format(i + 1)].append(\"\")\n\t                    meta_dict['source{}_class_id'.format(i + 1)].append(\"\")\n\t                count_dict[class_id] += 1\n\t        finished_n = np.sum([count_dict[class_id]\n\t                            for class_id in range(classes_num)])\n\t        print('Finished: {} / {}'.format(finished_n,\n", "              eval_segments_per_class * classes_num))\n\t        if all_classes_finished(count_dict, eval_segments_per_class):\n\t            break\n\t    write_meta_dict_to_csv(meta_dict, output_meta_csv_path)\n\t    print(\"Write csv to {}\".format(output_meta_csv_path))\n\tdef all_classes_finished(count_dict, segments_per_class):\n\t    r\"\"\"Check if all sound classes have #segments_per_class segments in\n\t    count_dict.\n\t    Args:\n\t        count_dict: dict, e.g., {\n", "            0: 12,\n\t            1: 4,\n\t            ...,\n\t            526: 33,\n\t        }\n\t        segments_per_class: int\n\t    Returns:\n\t        bool\n\t    \"\"\"\n\t    for class_id in count_dict.keys():\n", "        if count_dict[class_id] < segments_per_class:\n\t            return False\n\t    return True\n\tdef write_meta_dict_to_csv(meta_dict, output_meta_csv_path):\n\t    r\"\"\"Write meta dict into a csv file.\n\t    Args:\n\t        meta_dict: dict, e.g., {\n\t            'index_in_hdf5': (segments_num,),\n\t            'audio_name': (segments_num,),\n\t            'class_id': (segments_num,),\n", "        }\n\t        output_csv_path: str\n\t    \"\"\"\n\t    keys = list(meta_dict.keys())\n\t    items_num = len(meta_dict[keys[0]])\n\t    os.makedirs(os.path.dirname(output_meta_csv_path), exist_ok=True)\n\t    with open(output_meta_csv_path, 'w') as fw:\n\t        fw.write(','.join(keys) + \"\\n\")\n\t        for n in range(items_num):\n\t            fw.write(\",\".join([str(meta_dict[key][n]) for key in keys]) + \"\\n\")\n", "    print('Write out to {}'.format(output_meta_csv_path))\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description=\"\")\n\t    subparsers = parser.add_subparsers(dest=\"mode\")\n\t    parser_train = subparsers.add_parser(\"create_evaluation_meta\")\n\t    parser_train.add_argument(\"--workspace\", type=str, required=True)\n\t    parser_train.add_argument(\n\t        \"--split\",\n\t        type=str,\n\t        required=True,\n", "        choices=[\n\t            'balanced_train',\n\t            'test'])\n\t    parser_train.add_argument(\"--output_audios_dir\", type=str, required=True)\n\t    parser_train.add_argument(\n\t        \"--output_meta_csv_path\",\n\t        type=str,\n\t        required=True)\n\t    parser_train.add_argument(\n\t        \"--device\",\n", "        type=str,\n\t        default=\"cuda\")\n\t    args = parser.parse_args()\n\t    args.filename = pathlib.Path(__file__).stem\n\t    if args.mode == \"create_evaluation_meta\":\n\t        create_evaluation_meta(args)\n\t    else:\n\t        raise Exception(\"Error argument!\")\n"]}
{"filename": "uss/optimizers/lr_schedulers.py", "chunked_list": ["from functools import partial\n\tfrom typing import Callable\n\tdef linear_warm_up(\n\t    step: int,\n\t    warm_up_steps: int,\n\t    reduce_lr_steps: int\n\t) -> float:\n\t    r\"\"\"Get linear warm up scheduler for LambdaLR.\n\t    Args:\n\t        step (int): global step\n", "        warm_up_steps (int): steps for warm up\n\t        reduce_lr_steps (int): reduce learning rate by a factor of 0.9 #reduce_lr_steps step\n\t    .. code-block: python\n\t        >>> lr_lambda = partial(linear_warm_up, warm_up_steps=1000, reduce_lr_steps=10000)\n\t        >>> from torch.optim.lr_scheduler import LambdaLR\n\t        >>> LambdaLR(optimizer, lr_lambda)\n\t    Returns:\n\t        lr_scale (float): learning rate scaler\n\t    \"\"\"\n\t    if step <= warm_up_steps:\n", "        lr_scale = step / warm_up_steps\n\t    else:\n\t        lr_scale = 0.9 ** (step // reduce_lr_steps)\n\t    return lr_scale\n\tdef constant_warm_up(\n\t    step: int,\n\t    warm_up_steps: int,\n\t    reduce_lr_steps: int\n\t) -> float:\n\t    r\"\"\"Get constant warm up scheduler for LambdaLR.\n", "    Args:\n\t        step (int): global step\n\t        warm_up_steps (int): steps for warm up\n\t        reduce_lr_steps (int): reduce learning rate by a factor of 0.9 #reduce_lr_steps step\n\t    .. code-block: python\n\t        >>> lr_lambda = partial(constant_warm_up, warm_up_steps=1000, reduce_lr_steps=10000)\n\t        >>> from torch.optim.lr_scheduler import LambdaLR\n\t        >>> LambdaLR(optimizer, lr_lambda)\n\t    Returns:\n\t        lr_scale (float): learning rate scaler\n", "    \"\"\"\n\t    if 0 <= step < warm_up_steps:\n\t        lr_scale = 0.001\n\t    elif warm_up_steps <= step < 2 * warm_up_steps:\n\t        lr_scale = 0.01\n\t    elif 2 * warm_up_steps <= step < 3 * warm_up_steps:\n\t        lr_scale = 0.1\n\t    else:\n\t        lr_scale = 1\n\t    return lr_scale\n", "def get_lr_lambda(\n\t    lr_lambda_type: str,\n\t    **kwargs\n\t) -> Callable:\n\t    r\"\"\"Get learning scheduler.\n\t    Args:\n\t        lr_lambda_type (str), e.g., \"constant_warm_up\" | \"linear_warm_up\"\n\t    Returns:\n\t        lr_lambda_func (Callable)\n\t    \"\"\"\n", "    if lr_lambda_type == \"constant_warm_up\":\n\t        lr_lambda_func = partial(\n\t            constant_warm_up,\n\t            warm_up_steps=kwargs[\"warm_up_steps\"],\n\t            reduce_lr_steps=kwargs[\"reduce_lr_steps\"],\n\t        )\n\t    elif lr_lambda_type == \"linear_warm_up\":\n\t        lr_lambda_func = partial(\n\t            linear_warm_up,\n\t            warm_up_steps=kwargs[\"warm_up_steps\"],\n", "            reduce_lr_steps=kwargs[\"reduce_lr_steps\"],\n\t        )\n\t    else:\n\t        raise NotImplementedError\n\t    return lr_lambda_func\n"]}
