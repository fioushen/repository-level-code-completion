{"filename": "write_jobs_age.py", "chunked_list": ["import json\n\tfrom datetime import datetime\n\twith open('jobs.json', 'r') as f:\n\t    jobs_json = json.load(f)\n\t    jobs_data = jobs_json.get('data', [])\n\t    print(f'Number of jobs: {len(jobs_data)}')\n\twith open('jobs_age.json', 'r') as age_file:\n\t    age_data = json.load(age_file)\n\t    print(f'Number of jobs ages stored: {len(age_data)}')\n\tcurrent_jobs = {\"Total Jobs\": len(jobs_data)}\n", "for j in jobs_data:\n\t    company_name = j.get('company')\n\t    current_jobs[company_name] = current_jobs.get(company_name, 0) + 1\n\twith open(\"current.json\", \"w\") as file:\n\t    json.dump(current_jobs, file, indent=4)\n\tfor job in jobs_data:\n\t    link = job.get('link')\n\t    num = age_data.get(link, 0)\n\t    if num != 0:\n\t        age_data[link] = num + 1\n", "    else:\n\t        age_data[link] = 1\n\tprint(f'Number of jobs processed: {len(age_data)}')\n\twith open('jobs_age.json', 'w') as file:\n\t    json.dump(age_data, file, indent=4)\n\tdef write_history(current_jobs_json: dict):\n\t    now = f'{datetime.date(datetime.now())}'\n\t    with open('history.json', 'r') as f:\n\t        jobs_data_json = json.load(f)\n\t    for c_job in current_jobs_json:\n", "        it = jobs_data_json.get(c_job, {})\n\t        it[now] = current_jobs_json[c_job]\n\t        jobs_data_json[c_job] = it\n\t    with open('history.json', 'w') as history_file:\n\t        json.dump(jobs_data_json, history_file, indent=4)\n\twith open('current.json', 'r') as current_json_file:\n\t    c_data = json.load(current_json_file)\n\t    write_history(dict(c_data))\n\twith open('companies.json', 'r') as companies_file:\n\t    companies_file_data = json.load(companies_file)\n", "print('--- No jobs found for following companies: ---')\n\tfor c in companies_file_data:\n\t    c_name = c.get('company_name')\n\t    if c_data.get(c_name, 0) == 0:\n\t        print(f'Company: {c_name} Jobs board: {c.get(\"jobs_url\")}')\n\tprint('^^^ No jobs found for the above companies: ^^^')\n"]}
{"filename": "crawler.py", "chunked_list": ["import time\n\tfrom datetime import datetime\n\tfrom selenium import webdriver\n\tfrom src.company_list import get_company_list\n\tfrom src.company_list import write_companies\n\tcompany_list = get_company_list()\n\tprint(f'[CRAWLER] Number of companies: {len(company_list)}')\n\twrite_companies('companies.json')\n\twith open('jobs.json', 'w') as f:\n\t    f.write('{}')\n", "# setup headless webdriver\n\tchrome_options = webdriver.ChromeOptions()\n\tchrome_options.add_argument('--no-sandbox')\n\tchrome_options.add_argument('--disable-dev-shm-usage')\n\tchrome_options.add_argument('--headless')\n\tchrome_options.add_argument('--disable-extensions')\n\tdriver = webdriver.Chrome(options=chrome_options)\n\tn = 1\n\tfor company in company_list:\n\t    now = datetime.date(datetime.now())\n", "    st = time.time()\n\t    print(f'[CRAWLER] scrape {n} of {len(company_list)}')\n\t    n = n + 1\n\t    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t    print(f'[CRAWLER] Company {company.company_name} has {len(jobs_data)} open positions on {now}')\n\t    print('[CRAWLER] Execution time:', (time.time() - st), 'seconds')\n\tdriver.close()\n"]}
{"filename": "crawler_empty.py", "chunked_list": ["import time\n\tfrom selenium import webdriver\n\tfrom src.company_list_empty import get_company_list\n\tfrom src.company_list_empty import write_companies\n\tcompany_list = get_company_list()\n\tprint(f'[CRAWLER] Number of companies: {len(company_list)}')\n\twrite_companies('companies_empty.json')\n\twith open('jobs_empty.json', 'w') as f:\n\t    f.write('{}')\n\t# setup headless webdriver\n", "chrome_options = webdriver.ChromeOptions()\n\tchrome_options.add_argument('--no-sandbox')\n\tchrome_options.add_argument('--disable-dev-shm-usage')\n\tchrome_options.add_argument('--headless')\n\tchrome_options.add_argument('--disable-extensions')\n\tdriver = webdriver.Chrome(options=chrome_options)\n\tcurrent_jobs = {}\n\tn = 1\n\tfor company in company_list:\n\t    st = time.time()\n", "    print(f'[CRAWLER] scrape {n} of {len(company_list)}')\n\t    n = n + 1\n\t    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t    print('[CRAWLER] Execution time:', (time.time() - st), 'seconds')\n\tdriver.close()\n"]}
{"filename": "crawler_async.py", "chunked_list": ["from src.company_item import CompanyItem\n\tfrom src.scrape_ashbyhq import ScrapeAshbyhqAsync\n\tfrom src.scrape_lever import ScrapeLeverAsync\n\tfrom src.scrape_greenhouse import ScrapeGreenhouseAsync\n\timport time\n\tfrom caqui import synchronous\n\timport asyncio\n\tMAX_CONCURRENCY = 5  # number of WebDriver instances\n\tsem = asyncio.Semaphore(MAX_CONCURRENCY)\n\tasync def __schedule_tasks(companies):\n", "    tasks = [asyncio.ensure_future(__collect_data(company)) for company in companies]\n\t    await asyncio.gather(*tasks)\n\tasync def __collect_data(company):\n\t    async with sem:\n\t        driver_url = \"http://127.0.0.1:9515\"\n\t        capabilities = {\n\t            \"desiredCapabilities\": {\n\t                \"name\": \"webdriver\",\n\t                \"browserName\": \"firefox\",\n\t                \"marionette\": True,\n", "                \"acceptInsecureCerts\": True,\n\t                # uncomment to set headless\n\t                \"goog:chromeOptions\": {\"extensions\": [], \"args\": [\"--headless\"]},\n\t            }\n\t        }\n\t        session = synchronous.get_session(driver_url, capabilities)\n\t        driver = [driver_url, session]\n\t        data = await company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t        for entry in data:\n\t            print(entry)\n", "        synchronous.close_session(*driver)\n\tasync def __schedule_tasks():\n\t    companies = [\n\t        CompanyItem('evmos', 'https://boards.eu.greenhouse.io/evmos', ScrapeGreenhouseAsync, 'https://evmos.org',\n\t                    'Cross-Chain Connectivity'),\n\t        CompanyItem('orderlynetwork', 'https://boards.greenhouse.io/orderlynetwork', ScrapeGreenhouseAsync,\n\t                    'https://orderly.network', 'Exchange'),\n\t        CompanyItem(\"0x\", \"https://boards.greenhouse.io/0x\", ScrapeGreenhouseAsync, \"https://0x.org\",\n\t                    \"Blockchain\"),\n\t        CompanyItem('econetwork', 'https://boards.greenhouse.io/econetwork', ScrapeGreenhouseAsync,\n", "                    'https://eco.com', 'Web3 wallet'),\n\t        CompanyItem(\"bitcoin\", \"https://www.bitcoin.com/jobs/#joblist\", ScrapeGreenhouseAsync,\n\t                    \"https://www.bitcoin.com\", 'Exchange'),\n\t        CompanyItem('rain', 'https://jobs.ashbyhq.com/rain', ScrapeAshbyhqAsync, 'https://www.raincards.xyz',\n\t                    'Web3 cards'),\n\t        CompanyItem('exponential', 'https://jobs.ashbyhq.com/exponential', ScrapeAshbyhqAsync, 'https://exponential.fi',\n\t                    'DeFi'),\n\t        CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhqAsync, 'https://conduit.xyz',\n\t                    'Infrastructure'),\n\t        CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhqAsync, 'https://www.kiln.fi',\n", "                    'Staking & Infra'),\n\t        CompanyItem(\"flashbots\", \"https://jobs.ashbyhq.com/flashbots.net\", ScrapeAshbyhqAsync,\n\t                    \"https://www.flashbots.net\", \"ETH MEV\"),\n\t        CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhqAsync, 'https://www.paradigm.xyz',\n\t                    'Web3 financing'),\n\t        CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhqAsync, 'https://dune.com',\n\t                    'Web3 data'),\n\t        CompanyItem(\"solanafoundation\", \"https://jobs.ashbyhq.com/Solana%20Foundation\", ScrapeAshbyhqAsync,\n\t                    \"https://solana.org\", \"Blockchain\"),\n\t        CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhqAsync, 'https://syndica.io',\n", "                    'Infrastructure'),\n\t        CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhqAsync,\n\t                    'https://ellipsislabs.xyz', 'Trading Protocol'),\n\t        CompanyItem(\"kraken\", \"https://jobs.lever.co/kraken\", ScrapeLeverAsync, \"https://kraken.com\", \"Exchange\"),\n\t        CompanyItem('arbitrumfoundation', 'https://jobs.lever.co/arbitrumfoundation', ScrapeLeverAsync,\n\t                    'https://arbitrum.foundation', 'Layer 2'),\n\t        CompanyItem(\"chainlink\", \"https://jobs.lever.co/chainlink\", ScrapeLeverAsync, \"https://chain.link\",\n\t                    \"Blockchain\"),\n\t        CompanyItem('ethglobal', 'https://jobs.lever.co/ETHGlobal', ScrapeLeverAsync, 'https://ethglobal.com',\n\t                    'Community'),\n", "        CompanyItem('multiversx', 'https://jobs.lever.co/multiversx', ScrapeLeverAsync, 'https://multiversx.com',\n\t                    'Blockchain'),\n\t        CompanyItem('sprucesystems', 'https://jobs.lever.co/sprucesystems', ScrapeLeverAsync, 'https://spruceid.com',\n\t                    'Web3 ID'),\n\t        CompanyItem('BlockSwap', 'https://jobs.lever.co/BlockSwap', ScrapeLeverAsync, 'https://www.blockswap.network',\n\t                    'Infra'),\n\t        CompanyItem('Metatheory', 'https://jobs.lever.co/Metatheory', ScrapeLeverAsync,\n\t                    'https://www.duskbreakers.gg', 'Web3 game'),\n\t        CompanyItem('axiomzen', 'https://jobs.lever.co/axiomzen', ScrapeLeverAsync, 'https://www.axiomzen.com', 'Web3'),\n\t        CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLeverAsync, 'https://www.fuel.network',\n", "                    'Blockchain'),\n\t        CompanyItem('harmony', 'https://jobs.lever.co/harmony', ScrapeLeverAsync, 'https://www.harmony.one',\n\t                    'Blockchain'),\n\t        CompanyItem('wintermute', 'https://jobs.lever.co/wintermute-trading', ScrapeLeverAsync,\n\t                    'https://www.wintermute.com',\n\t                    'Trading'),\n\t        CompanyItem(\"kaiko\", \"https://jobs.eu.lever.co/kaiko\", ScrapeLeverAsync, \"https://www.kaiko.com\", \"Data\"),\n\t        CompanyItem('bebop', 'https://jobs.lever.co/Bebop', ScrapeLeverAsync, 'https://bebop.xyz', 'DeFi Exchange'),\n\t        CompanyItem(\"Coinshift\", \"https://jobs.lever.co/Coinshift\", ScrapeLeverAsync, \"https://coinshift.xyz\",\n\t                    \"Custody software\"),\n", "        CompanyItem(\"swissborg\", \"https://jobs.lever.co/swissborg\", ScrapeLeverAsync, \"https://swissborg.com\",\n\t                    \"Exchange\"),\n\t        CompanyItem(\"OpenSea\", \"https://jobs.lever.co/OpenSea\", ScrapeLeverAsync, \"https://opensea.io\", \"NFT\"),\n\t        CompanyItem(\"storyprotocol\", \"https://jobs.lever.co/storyprotocol\", ScrapeLeverAsync,\n\t                    \"https://www.storyprotocol.xyz\", \"Protocol\"),\n\t        CompanyItem(\"ethereumfoundation\", \"https://jobs.lever.co/ethereumfoundation\", ScrapeLeverAsync,\n\t                    \"https://ethereum.org\", \"Blockchain\"),\n\t        CompanyItem(\"aave\", \"https://jobs.eu.lever.co/aave\", ScrapeLeverAsync, \"https://aave.com\", \"Protocol\"),\n\t        CompanyItem(\"crypto\", \"https://jobs.lever.co/crypto\", ScrapeLeverAsync, \"https://crypto.com\", \"Exchange\"),\n\t        CompanyItem(\"Luxor\", \"https://jobs.lever.co/LuxorTechnology\", ScrapeLeverAsync, \"https://www.luxor.tech\",\n", "                    \"Mining\"),\n\t        CompanyItem(\"anchorage\", \"https://jobs.lever.co/anchorage\", ScrapeLeverAsync, \"https://www.anchorage.com\",\n\t                    \"Trading\"),\n\t        CompanyItem(\"biconomy\", \"https://jobs.lever.co/biconomy\", ScrapeLeverAsync, \"https://www.biconomy.io\",\n\t                    \"Infra\"),\n\t        CompanyItem('enso', 'https://jobs.lever.co/Enso', ScrapeLeverAsync, 'https://www.enso.finance', 'DeFi'),\n\t        CompanyItem(\"Polygon\", \"https://jobs.lever.co/Polygon\", ScrapeLeverAsync, \"https://polygon.technology\",\n\t                    \"Blockchain\"),\n\t        CompanyItem(\"tokenmetrics\", \"https://jobs.lever.co/tokenmetrics\", ScrapeLeverAsync,\n\t                    \"https://www.tokenmetrics.com\", \"Information\"),\n", "        CompanyItem(\"offchainlabs\", \"https://jobs.lever.co/offchainlabs\", ScrapeLeverAsync,\n\t                    \"https://offchainlabs.com\", \"Protocol\"),\n\t        CompanyItem(\"subspacelabs\", \"https://jobs.lever.co/subspacelabs\", ScrapeLeverAsync,\n\t                    \"https://subspace.network\", \"Blockchain Infra\"),\n\t        CompanyItem('3boxlabs', 'https://jobs.lever.co/3box', ScrapeLeverAsync, 'https://3boxlabs.com',\n\t                    'Infra'),\n\t        CompanyItem(\"ramp.network\", \"https://jobs.lever.co/careers.ramp.network\", ScrapeLeverAsync,\n\t                    \"https://ramp.network\", \"Payments\"),\n\t        CompanyItem('risklabs', 'https://jobs.lever.co/risklabs', ScrapeLeverAsync, 'https://risklabs.foundation',\n\t                    'Protocol'),\n", "        CompanyItem('celestia', 'https://jobs.lever.co/celestia', ScrapeLeverAsync, 'https://celestia.org',\n\t                    'Modular Blockchain'),\n\t        CompanyItem('polymerlabs', 'https://jobs.lever.co/polymerlabs', ScrapeLeverAsync, 'https://www.polymerlabs.org',\n\t                    'Modular Blockchain'),\n\t        CompanyItem('royal', 'https://jobs.lever.co/Royal', ScrapeLeverAsync, 'https://royal.io', 'Web3 + Music'),\n\t        CompanyItem('gauntlet', 'https://jobs.lever.co/gauntlet', ScrapeLeverAsync, 'https://gauntlet.network',\n\t                    'Web3 + Financial Modelling'),\n\t        CompanyItem(\"ledger\", \"https://jobs.lever.co/ledger\", ScrapeLeverAsync, \"https://www.ledger.com\", \"Wallet\"),\n\t        CompanyItem(\"request\", \"https://jobs.lever.co/request\", ScrapeLeverAsync, \"https://request.network\",\n\t                    \"Payments\"),\n", "        CompanyItem(\"immutable\", \"https://jobs.lever.co/immutable\", ScrapeLeverAsync, \"https://www.immutable.com\",\n\t                    \"NFT\"),\n\t        CompanyItem(\"web3auth\", \"https://jobs.lever.co/TorusLabs\", ScrapeLeverAsync, \"https://web3auth.io\", \"Auth\"),\n\t        CompanyItem(\"cere-network\", \"https://jobs.lever.co/cere-network\", ScrapeLeverAsync, \"https://cere.network\",\n\t                    \"Infra\"),\n\t        CompanyItem('matterlabs', 'https://jobs.eu.lever.co/matterlabs', ScrapeLeverAsync, 'https://matter-labs.io',\n\t                    'Protocol'),\n\t        CompanyItem(\"hiro\", \"https://jobs.lever.co/hiro\", ScrapeLeverAsync, \"https://www.hiro.so\", \"Infra\"),\n\t        CompanyItem('AQX', 'https://jobs.lever.co/presto', ScrapeLeverAsync, 'https://aqx.com', 'Exchange and Web3'),\n\t        CompanyItem('ultra', 'https://jobs.lever.co/ultra', ScrapeLeverAsync,\n", "                    'https://ultra.io', 'Web3 Gaming'),\n\t        CompanyItem('bitwise', 'https://jobs.lever.co/bitwiseinvestments', ScrapeLeverAsync,\n\t                    'https://bitwiseinvestments.com', 'Asset Management'),\n\t    ]\n\t    tasks = [asyncio.ensure_future(__collect_data(company)) for company in companies]\n\t    await asyncio.gather(*tasks)\n\tif __name__ == \"__main__\":\n\t    start = time.time()\n\t    loop = asyncio.get_event_loop()\n\t    try:\n", "        loop.run_until_complete(__schedule_tasks())\n\t    finally:\n\t        loop.run_until_complete(loop.shutdown_asyncgens())\n\t        loop.close()\n\t        end = time.time()\n\t        print(f\"Time: {end - start:.2f} sec\")\n\t        # 66.46 sec for 56 company names\n"]}
{"filename": "test/test_consensys_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.scrape_consensys import ScrapeConsensys\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tjobs = ScrapeConsensys().getJobs(driver, \"https://consensys.net/open-roles\")\n\tfor job in jobs:\n\t    print(job)\n\tdriver.close()\n"]}
{"filename": "test/test_ashbyhq_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_ashbyhq import ScrapeAshbyhq\n\timport time \n\tstart = time.time()\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tcompanies = [\n\t    CompanyItem('flashbots', 'https://jobs.ashbyhq.com/flashbots.net', ScrapeAshbyhq, '', ''),\n", "    CompanyItem('rain', 'https://jobs.ashbyhq.com/rain', ScrapeAshbyhq, 'https://www.raincards.xyz', 'Web3 cards'),\n\t    CompanyItem('exponential', 'https://jobs.ashbyhq.com/exponential', ScrapeAshbyhq, 'https://exponential.fi', 'DeFi'),\n\t    CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhq, 'https://www.kiln.fi', 'Staking'),\n\t    CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhq, 'https://dune.com',\n\t                'Web3 data'),\n\t    CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhq, 'https://conduit.xyz',\n\t                'Infrastructure'),\n\t    CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhq, 'https://www.paradigm.xyz',\n\t                'Web3 data'),\n\t    CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhq, 'https://www.sygnum.com',\n", "                'Crypto bank'),\n\t    CompanyItem('solana-foundation', 'https://jobs.ashbyhq.com/Solana%20Foundation', ScrapeAshbyhq,\n\t                'https://www.sygnum.com',\n\t                'Crypto bank'),\n\t    CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhq,\n\t                'https://ellipsislabs.xyz', 'Trading Protocol')\n\t]\n\tfor company in companies:\n\t    data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t    for entry in data:\n", "        print(entry)\n\tdriver.close()\n\tend = time.time()\n\tprint(f\"Time: {end-start:.2f} sec\")"]}
{"filename": "test/test_any_scraper.py", "chunked_list": ["import unittest\n\tfrom selenium import webdriver\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_ashbyhq import ScrapeAshbyhq\n\tfrom src.scrape_greenhouse import ScrapeGreenhouse\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tcompany_list = [\n\t    CompanyItem('Blockworks', 'https://jobs.ashbyhq.com/Blockworks', ScrapeAshbyhq,\n", "                'https://blockworks.co', 'Web3 News'),\n\t    CompanyItem('21co', 'https://boards.greenhouse.io/21co', ScrapeGreenhouse,\n\t                'https://www.21.co', 'Web3 DeFi ETP'),\n\t    CompanyItem('xapo', 'https://boards.greenhouse.io/xapo61', ScrapeGreenhouse,\n\t                'https://www.xapobank.com', 'Web3 bank')\n\t]\n\tfor company in company_list:\n\t    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t    for entry in jobs_data:\n\t        print(entry)\n", "driver.close()\n\tclass TestScraper(unittest.TestCase):\n\t    def test_upper(self):\n\t        self.assertGreater(len(jobs_data), 1)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "test/test_company_list.py", "chunked_list": ["import json\n\tfrom src.company_list import get_company_list\n\tfrom src.company_list import get_company\n\tcompany_list = get_company_list()\n\tprint(f'Number of companies: {len(company_list)}')\n\tresult_list = []\n\tfor company in company_list:\n\t    company_item = {\n\t        \"company_name\": company.company_name,\n\t        \"company_url\": company.company_url,\n", "        \"jobs_url\": company.jobs_url,\n\t    }\n\t    result_list.append(company_item)\n\tprint(f'Number of companies in JSON: {len(result_list)}')\n\twith open('companies.json', 'w') as f:\n\t    json.dump(result_list, f, indent=4)\n\tprint(get_company('kraken'))\n"]}
{"filename": "test/test_greenhouse_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_greenhouse import ScrapeGreenhouse\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tcompany_list = [\n\t    CompanyItem('grayscaleinvestments', 'https://boards.greenhouse.io/grayscaleinvestments', ScrapeGreenhouse,\n\t                'https://grayscale.com', 'Web3 Asset Manager'),\n\t    CompanyItem('dragonflycapital', 'https://boards.greenhouse.io/dragonflycapital', ScrapeGreenhouse,\n", "                'https://www.dragonfly.xyz', 'Web3 funding'),\n\t    CompanyItem('penumbralabs', 'https://boards.greenhouse.io/penumbralabs', ScrapeGreenhouse,\n\t                'https://eco.com', 'Web3 trading'),\n\t    CompanyItem('econetwork', 'https://boards.greenhouse.io/econetwork', ScrapeGreenhouse,\n\t                'https://eco.com', 'Web3 wallet'),\n\t    CompanyItem('outlierventures', 'https://boards.eu.greenhouse.io/outlierventures', ScrapeGreenhouse,\n\t                'https://outlierventures.io', 'Web3 Ventures'),\n\t    CompanyItem('evmos', 'https://boards.eu.greenhouse.io/evmos', ScrapeGreenhouse, 'https://evmos.org',\n\t                'Cross-Chain Connectivity'),\n\t    CompanyItem('magic', 'https://boards.greenhouse.io/magic', ScrapeGreenhouse, 'https://magic.link', 'Web3 Wallets'),\n", "    CompanyItem('trmlabs', 'https://www.trmlabs.com/careers-list', ScrapeGreenhouse,\n\t                'https://www.trmlabs.com', 'Web3 Information'),\n\t    CompanyItem('foundrydigital', 'https://boards.greenhouse.io/foundrydigital', ScrapeGreenhouse,\n\t                'https://foundrydigital.com', 'Web3'),\n\t    CompanyItem('o1labs', 'https://boards.greenhouse.io/o1labs', ScrapeGreenhouse, 'https://o1labs.org',\n\t                'Web3'),\n\t    CompanyItem('orderlynetwork', 'https://boards.greenhouse.io/orderlynetwork', ScrapeGreenhouse,\n\t                'https://orderly.network', 'Exchange'),\n\t    CompanyItem('paradigm.co', 'https://boards.greenhouse.io/paradigm62', ScrapeGreenhouse, 'https://www.paradigm.co',\n\t                'Liquidity'),\n", "    CompanyItem('immunefi', 'https://boards.greenhouse.io/immunefi', ScrapeGreenhouse, 'https://immunefi.com',\n\t                'Bug bounty platform'),\n\t    CompanyItem('protocollabs', 'https://boards.greenhouse.io/protocollabs', ScrapeGreenhouse, 'https://protocol.ai/about',\n\t                'Web3 IPFS research platform'),\n\t    CompanyItem('taxbit', 'https://boards.greenhouse.io/taxbit', ScrapeGreenhouse, 'https://taxbit.com', 'Accounting'),\n\t    CompanyItem('osmosisdex', 'https://boards.greenhouse.io/osmosisdex', ScrapeGreenhouse, 'https://osmosis.zone',\n\t                'Exchange'),\n\t    CompanyItem('stellar', 'https://boards.greenhouse.io/stellar', ScrapeGreenhouse, 'https://stellar.org',\n\t                'Blockchain'),\n\t    CompanyItem('bitfury', 'https://boards.greenhouse.io/bitfury', ScrapeGreenhouse, 'https://bitfury.com',\n", "                'Web3'),\n\t    CompanyItem('mobilecoin', 'https://boards.greenhouse.io/mobilecoin', ScrapeGreenhouse,\n\t                'https://mobilecoin.com', 'Blockchain'),\n\t    CompanyItem('chia', 'https://www.chia.net/careers', ScrapeGreenhouse,\n\t                'https://www.chia.net', 'Blockchain'),\n\t    CompanyItem('okcoin', 'https://boards.greenhouse.io/okcoin', ScrapeGreenhouse, 'https://www.okcoin.com',\n\t                'Exchange'),\n\t    CompanyItem(\"solanafoundation\", \"https://boards.greenhouse.io/solanafoundation\", ScrapeGreenhouse,\n\t                \"https://solana.org\", \"Blockchain\"),\n\t    CompanyItem(\"worldcoinorg\", \"https://boards.greenhouse.io/worldcoinorg\", ScrapeGreenhouse,\n", "                \"https://worldcoin.org\", \"Blockchain\"),\n\t    CompanyItem(\"edgeandnode\", \"https://boards.greenhouse.io/edgeandnode\", ScrapeGreenhouse,\n\t                \"https://edgeandnode.com\", \"Infra\"),\n\t    CompanyItem(\"clearmatics\", \"https://boards.greenhouse.io/clearmatics\", ScrapeGreenhouse,\n\t                \"https://www.clearmatics.com\", \"Protocol\"),\n\t    CompanyItem(\"aztec\", \"https://boards.eu.greenhouse.io/aztec\", ScrapeGreenhouse, \"https://aztec.network\",\n\t                \"Protocol\"),\n\t    CompanyItem(\"avalabs\", \"https://boards.greenhouse.io/avalabs\", ScrapeGreenhouse,\n\t                \"https://www.avalabs.org\", \"Blockchain\"),\n\t    CompanyItem(\"galaxydigitalservices\", \"https://boards.greenhouse.io/galaxydigitalservices\",\n", "                ScrapeGreenhouse, \"https://www.galaxy.com\", 'Trading'),\n\t    CompanyItem(\"bittrex\", \"https://boards.greenhouse.io/bittrex\", ScrapeGreenhouse,\n\t                \"https://global.bittrex.com\", 'Exchange'),\n\t    CompanyItem(\"bitcoin\", \"https://www.bitcoin.com/jobs/#joblist\", ScrapeGreenhouse,\n\t                \"https://www.bitcoin.com\", 'Exchange'),\n\t    CompanyItem(\"EigenLabs\", \"https://boards.greenhouse.io/layrlabs\", ScrapeGreenhouse,\n\t                \"https://www.v1.eigenlayer.xyz\", \"Infra\"),\n\t    CompanyItem(\"kadena\", \"https://boards.greenhouse.io/kadenallc\", ScrapeGreenhouse, \"https://kadena.io\",\n\t                \"PoW chain\"),\n\t    CompanyItem(\"poap\", \"https://boards.greenhouse.io/poaptheproofofattendanceprotocol\", ScrapeGreenhouse,\n", "                \"https://poap.xyz\", \"Protocol\"),\n\t    CompanyItem(\"chainsafesystems\", \"https://boards.greenhouse.io/chainsafesystems\", ScrapeGreenhouse,\n\t                \"https://chainsafe.io\", \"Infra\"),\n\t    CompanyItem(\"status\", \"https://jobs.status.im\", ScrapeGreenhouse, \"https://status.im\", \"Messanger\"),\n\t    CompanyItem(\"digitalasset\", \"https://boards.greenhouse.io/digitalasset\", ScrapeGreenhouse,\n\t                \"https://www.digitalasset.com\", \"Custody\"),\n\t    CompanyItem(\"copperco\", \"https://boards.eu.greenhouse.io/copperco\", ScrapeGreenhouse,\n\t                \"https://copper.co\", \"Custody\"),\n\t    CompanyItem(\"messari\", \"https://boards.greenhouse.io/messari\", ScrapeGreenhouse, \"https://messari.io\",\n\t                \"Information\"),\n", "    CompanyItem(\"layerzerolabs\", \"https://boards.greenhouse.io/layerzerolabs\", ScrapeGreenhouse,\n\t                \"https://layerzero.network\", \"Infra\"),\n\t    CompanyItem(\"jumpcrypto\", \"https://boards.greenhouse.io/jumpcrypto\", ScrapeGreenhouse,\n\t                \"https://jumpcrypto.com\", \"Infra\"),\n\t    CompanyItem(\"oasisnetwork\", \"https://boards.greenhouse.io/oasisnetwork\", ScrapeGreenhouse,\n\t                \"https://oasisprotocol.org\", \"Protocol\")]\n\tfor company in company_list:\n\t    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t    for entry in jobs_data:\n\t        print(entry)\n", "driver.close()\n"]}
{"filename": "test/test_bamboohr_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_bamboohr import ScrapeBamboohr\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tcompany_list = [\n\t    CompanyItem('wirex', 'https://wirex.bamboohr.com/careers', ScrapeBamboohr, 'https://wirexapp.com', 'Web3 card'),\n\t    CompanyItem('sygnum', 'https://sygnum.bamboohr.com/careers', ScrapeBamboohr, 'https://www.sygnum.com',\n\t                'Crypto bank'),\n", "    CompanyItem('iofinnet', 'https://iofinnethr.bamboohr.com/jobs/?source=bamboohr', ScrapeBamboohr,\n\t                'https://www.iofinnet.com', 'Custody'),\n\t    CompanyItem('web3', 'https://web3.bamboohr.com/jobs', ScrapeBamboohr, 'https://web3.foundation',\n\t                'web3'),\n\t    CompanyItem('dappradar', 'https://dappradar.bamboohr.com/careers', ScrapeBamboohr,\n\t                'https://dappradar.com', 'Exchange & NFT'),\n\t    CompanyItem(\"cexio\", \"https://cexio.bamboohr.com/jobs\", ScrapeBamboohr, \"https://cex.io\", \"Exchange\"),\n\t    CompanyItem(\"chainstack\", \"https://chainstack.bamboohr.com/jobs\", ScrapeBamboohr,\n\t                \"https://chainstack.com\", \"Infra\")\n\t]\n", "for company in company_list:\n\t    print(company.jobs_url)\n\t    data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t    for entry in data:\n\t        print(entry)\n\tdriver.close()\n"]}
{"filename": "test/test_lever_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_lever import ScrapeLever\n\tcompany_list = [\n\t    CompanyItem('ethenalabs', 'https://jobs.lever.co/ethenalabs', ScrapeLever,\n\t                'https://www.ethena.fi', 'Web3 bonds'),\n\t    CompanyItem('arbitrumfoundation', 'https://jobs.lever.co/arbitrumfoundation', ScrapeLever,\n\t                'https://arbitrum.foundation', 'Layer 2'),\n\t    CompanyItem('3boxlabs', 'https://jobs.lever.co/3box', ScrapeLever, 'https://3boxlabs.com',\n\t                'Infra'),\n", "    CompanyItem('BlockSwap', 'https://jobs.lever.co/BlockSwap', ScrapeLever, 'https://www.blockswap.network',\n\t                'Infra'),\n\t    CompanyItem('wintermute', 'https://jobs.lever.co/wintermute-trading', ScrapeLever, 'https://www.wintermute.com',\n\t                'Trading'),\n\t    CompanyItem('sprucesystems', 'https://jobs.lever.co/sprucesystems', ScrapeLever, 'https://spruceid.com',\n\t                'Web3 ID'),\n\t    CompanyItem('royal', 'https://jobs.lever.co/Royal', ScrapeLever, 'https://royal.io', 'Web3 + Music'),\n\t    CompanyItem('enso', 'https://jobs.lever.co/Enso', ScrapeLever, 'https://www.enso.finance', 'DeFi'),\n\t    CompanyItem('gauntlet', 'https://jobs.lever.co/gauntlet', ScrapeLever, 'https://gauntlet.network',\n\t                'Web3 + Financial Modelling'),\n", "    CompanyItem('AQX', 'https://jobs.lever.co/presto', ScrapeLever, 'https://aqx.com', 'Exchange and Web3'),\n\t    CompanyItem('multiversx', 'https://jobs.lever.co/multiversx', ScrapeLever, 'https://multiversx.com', 'Blockchain'),\n\t    CompanyItem('matterlabs', 'https://jobs.eu.lever.co/matterlabs', ScrapeLever, 'https://matter-labs.io', 'Protocol'),\n\t    CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLever, 'https://www.fuel.network', 'Blockchain'),\n\t    CompanyItem(\"Luxor\", \"https://jobs.lever.co/LuxorTechnology\", ScrapeLever, \"https://www.luxor.tech\", \"Mining\"),\n\t    CompanyItem(\"anchorage\", \"https://jobs.lever.co/anchorage\", ScrapeLever, \"https://www.anchorage.com\", \"Trading\"),\n\t    CompanyItem(\"biconomy\", \"https://jobs.lever.co/biconomy\", ScrapeLever, \"https://www.biconomy.io\", \"Infra\"),\n\t    CompanyItem(\"kraken\", \"https://jobs.lever.co/kraken\", ScrapeLever, \"https://kraken.com\", \"Exchange\"),\n\t    CompanyItem(\"chainlink\", \"https://jobs.lever.co/chainlink\", ScrapeLever, \"https://chain.link\", \"Blockchain\"),\n\t    CompanyItem(\"hiro\", \"https://jobs.lever.co/hiro\", ScrapeLever, \"https://www.hiro.so\", \"Infra\"),\n", "    CompanyItem(\"kaiko\", \"https://jobs.eu.lever.co/kaiko\", ScrapeLever, \"https://www.kaiko.com\", \"Data\"),\n\t    CompanyItem(\"tessera\", \"https://jobs.lever.co/ftc\", ScrapeLever, \"https://tessera.co\", \"NFT\"),\n\t    CompanyItem(\"cere-network\", \"https://jobs.lever.co/cere-network\", ScrapeLever, \"https://cere.network\", \"Infra\"),\n\t    CompanyItem(\"ramp.network\", \"https://jobs.lever.co/careers.ramp.network\", ScrapeLever, \"https://ramp.network\",\n\t                \"Payments\"),\n\t    CompanyItem(\"ledger\", \"https://jobs.lever.co/ledger\", ScrapeLever, \"https://www.ledger.com\", \"Wallet\"),\n\t    CompanyItem(\"request\", \"https://jobs.lever.co/request\", ScrapeLever, \"https://request.network\", \"Payments\"),\n\t    CompanyItem(\"immutable\", \"https://jobs.lever.co/immutable\", ScrapeLever, \"https://www.immutable.com\", \"NFT\"),\n\t    CompanyItem(\"web3auth\", \"https://jobs.lever.co/TorusLabs\", ScrapeLever, \"https://web3auth.io\", \"Auth\")]\n\toptions = webdriver.ChromeOptions()\n", "options.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tfor company in company_list:\n\t    data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t    for entry in data:\n\t        print(entry)\n\tdriver.close()\n"]}
{"filename": "test/test_recruitee_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_recruitee import ScrapeRecruitee\n\tcompanies = [\n\t    CompanyItem(\"ramp.network\", \"https://metrika.recruitee.com\", ScrapeRecruitee, \"https://ramp.network\", \"Payments\"),\n\t    CompanyItem(\"tether\", \"https://tether.recruitee.com\", ScrapeRecruitee, \"https://tether.to/en\", \"Stable Coin\")\n\t]\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n", "for company in companies:\n\t    print(company.jobs_url)\n\t    data = company.scraper_type().getJobs(driver, company.jobs_url)\n\t    for entry in data:\n\t        print(entry)\n\tdriver.close()\n"]}
{"filename": "test/test_paxos_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.scrape_paxos import ScrapePaxos\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\turl1 = \"https://paxos.com/job-posts/?_sft_department=compliance,engineering,finance-accounting,hr-talent,information-technology,legal,operations,product&_sft_office=us\"\n\turl2 = url1 + \"&sf_paged=2\"\n\tjobs = ScrapePaxos().getJobs(driver, url1)\n\tfor job in jobs:\n\t    print(job)\n", "driver.close()\n"]}
{"filename": "test/test_jobs_reader.py", "chunked_list": ["import json\n\twith open('jobs.json', 'r') as f:\n\t    jobs_json = json.load(f)\n\t    jobs_data = jobs_json.get('data', [])\n\t    print(len(jobs_data))\n\tprint(jobs_data[0])\n\tdata = list(filter(lambda jd: jd.get('company') == 'wintermute', jobs_data))\n\tprint(data)\n\t# write age\n\twith open('age.json', 'r') as age_file:\n", "    age_data = json.load(age_file)\n\tprint(age_data)\n\tfor job in jobs_data:\n\t    link = job.get('link')\n\t    print(link)\n\t    num = age_data.get(link, 0)\n\t    if num != 0:\n\t        age_data[link] = num+1\n\t    else:\n\t        age_data[link] = 1\n", "with open('age.json', 'w') as file:\n\t    json.dump(age_data, file, indent=4)\n"]}
{"filename": "test/test_binance_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.scrape_binance import ScrapeBinance\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tjobs = ScrapeBinance().getJobs(driver, \"https://www.binance.com/en/careers/job-openings\")\n\tfor job in jobs:\n\t    print(job)\n\tdriver.close()\n"]}
{"filename": "test/test_smartrecruiters_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.scrape_smartrecruiters import ScrapeSmartrecruiters\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tjobs = ScrapeSmartrecruiters().getJobs(driver, \"https://careers.smartrecruiters.com/B6/coinmarketcap\", 'xxx')\n\tfor job in jobs:\n\t    print(job)\n\tdriver.close()\n"]}
{"filename": "test/test_ripple_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.scrape_ripple import ScrapeRipple\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tjobs = ScrapeRipple().getJobs(driver, \"https://ripple.com/careers/all-jobs\")\n\tfor job in jobs:\n\t    print(job)\n\tdriver.close()\n"]}
{"filename": "test/test_workable_scraper.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_workable import ScrapeWorkable\n\toptions = webdriver.ChromeOptions()\n\toptions.add_argument('--headless')\n\tdriver = webdriver.Chrome(options=options)\n\tcompany_list = [\n\t    CompanyItem('dydxopsdao', 'https://apply.workable.com/dydx-operations-trust', ScrapeWorkable,\n\t                'https://dydxopsdao.com', 'Web3 DAO'),\n\t    CompanyItem('almanak', 'https://apply.workable.com/almanak-blockchain-labs-ag', ScrapeWorkable,\n", "                'https://almanak.co', 'Web3 Simulator'),\n\t    CompanyItem('walletconnect', 'https://apply.workable.com/walletconnect', ScrapeWorkable,\n\t                'https://walletconnect.com', 'Web3 Wallet Infra'),\n\t    CompanyItem('cryptofinance', 'https://apply.workable.com/cryptofinance', ScrapeWorkable,\n\t                'https://www.crypto-finance.com', 'Exchange'),\n\t    CompanyItem('bitstamp', 'https://apply.workable.com/bitstamp/#jobs', ScrapeWorkable,\n\t                'https://www.bitstamp.net', 'Exchange'),\n\t    CompanyItem('smart-token-labs', 'https://apply.workable.com/smart-token-labs', ScrapeWorkable,\n\t                'https://smarttokenlabs.com', 'Web3 bridge'),\n\t    CompanyItem('avantgarde', 'https://apply.workable.com/avantgarde', ScrapeWorkable,\n", "                'https://avantgarde.finance', 'Asset Management'),\n\t    CompanyItem('stably', 'https://apply.workable.com/stably', ScrapeWorkable, 'https://stably.io',\n\t                'Stable Coin')\n\t]\n\t# company_list.append(CompanyItem('bitget', 'https://apply.workable.com/bitget', ScrapeWorkable, 'https://www.bitget.com/en', 'Exchange'))\n\tfor company in company_list:\n\t    print(company.jobs_url)\n\t    data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t    for entry in data:\n\t        print(entry)\n", "driver.close()\n"]}
{"filename": "test/test_ashbyhq_scraper_async.py", "chunked_list": ["from selenium import webdriver\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_ashbyhq import ScrapeAshbyhqAsync\n\timport time\n\tfrom caqui import synchronous\n\tfrom pytest import mark\n\t@mark.asyncio\n\tasync def test_ashbyhq_async():\n\t    start = time.time()\n\t    # options = webdriver.ChromeOptions()\n", "    # options.add_argument('--headless')\n\t    # driver = webdriver.Chrome(options=options)\n\t    driver_url = \"http://127.0.0.1:9999\"\n\t    capabilities = {\n\t        \"desiredCapabilities\": {\n\t            \"name\": \"webdriver\",\n\t            \"browserName\": \"firefox\",\n\t            \"marionette\": True,\n\t            \"acceptInsecureCerts\": True,\n\t            # uncomment to set headless\n", "            \"goog:chromeOptions\": {\"extensions\": [], \"args\": [\"--headless\"]},\n\t        }\n\t    }\n\t    session = synchronous.get_session(driver_url, capabilities)\n\t    driver = [driver_url, session]\n\t    companies = [\n\t        CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhqAsync, 'https://www.kiln.fi', 'Staking'),\n\t        CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhqAsync, 'https://dune.com',\n\t                    'Web3 data'),\n\t        CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhqAsync, 'https://conduit.xyz',\n", "                    'Infrastructure'),\n\t        CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhqAsync, 'https://www.paradigm.xyz',\n\t                    'Web3 data'),\n\t        CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhqAsync, 'https://www.sygnum.com',\n\t                    'Crypto bank'),\n\t        CompanyItem('solana-foundation', 'https://jobs.ashbyhq.com/Solana%20Foundation', ScrapeAshbyhqAsync,\n\t                    'https://www.sygnum.com',\n\t                    'Crypto bank'),\n\t        CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhqAsync,\n\t                    'https://ellipsislabs.xyz', 'Trading Protocol')\n", "    ]\n\t    for company in companies:\n\t        data = await company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n\t        for entry in data:\n\t            print(entry)\n\t    synchronous.close_session(*driver)\n\t    end = time.time()\n\t    print(f\"Time: {end-start:.2f} sec\")\n"]}
{"filename": "src/scrape_binance.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom selenium.webdriver.support.wait import WebDriverWait\n\tfrom selenium.webdriver.support import expected_conditions as EC\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tdef clean_location(location: str):\n\t    location = location.replace(\"/ Full-time\", \"\")\n\t    if 'global' in location.lower() or 'remote : remote' in location.lower():\n\t        return \"REMOTE\"\n\t    set_of_locations = set(([x.strip() for x in location.split(',')]))\n\t    result_locations = set()\n", "    for loc in set_of_locations:\n\t        if loc.endswith('/'):\n\t            result_locations.add(loc[0:-1].strip())\n\t            break\n\t        result_locations.add(loc)\n\t    return ' '.join(result_locations)\n\tclass ScrapeBinance(ScrapeIt):\n\t    def getJobs(self, driver, web_page, company='binance') -> []:\n\t        print(f'[BINANCE] Scrap page: {web_page}')\n\t        driver.get(web_page)\n", "        wait = WebDriverWait(driver, 120)\n\t        apply_buttons = wait.until(EC.presence_of_all_elements_located((By.XPATH, '//a[.=\"Apply\"]')))\n\t        group_elements = driver.find_elements(By.XPATH, '//div[contains(@class,\"posting\")]')\n\t        print(f'[BINANCE] Found {len(apply_buttons)} jobs on {web_page}')\n\t        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n\t            location_elem = elem.find_element(By.CSS_SELECTOR, 'div[data-bn-type=\"text\"]')\n\t            job_url = link_elem.get_attribute('href')\n\t            location = clean_location(location_elem.text)\n", "            job_name = link_elem.text\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": job_name,\n\t                \"location\": location,\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n\t        print(f'[BINANCE] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n", "        return result\n"]}
{"filename": "src/scrape_recruitee.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tdef clean_location(location):\n\t    if 'remote' in location.lower() or 'global' in location.lower():\n\t        return \"REMOTE\"\n\t    joined = ' '.join(set(([x.strip() for x in location.split(',')])))\n\t    return joined\n\tclass ScrapeRecruitee(ScrapeIt):\n\t    def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[RECRUITEE] Scrap page: {web_page}')\n", "        driver.get(web_page)\n\t        group_elements = driver.find_elements(By.CSS_SELECTOR, 'div [class=\"job\"]')\n\t        print(f'[RECRUITEE] Found {len(group_elements)} jobs.')\n\t        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem.find_element(By.CSS_SELECTOR, '[class=\"job-title\"] a')\n\t            location_elem = elem.find_element(By.CSS_SELECTOR, '[class=\"job-location\"]')\n\t            job_url = link_elem.get_attribute('href')\n\t            location = clean_location(location_elem.text)\n\t            job_name = link_elem.text\n", "            job = {\n\t                \"company\": company,\n\t                \"title\": job_name,\n\t                \"location\": location,\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n\t        print(f'[RECRUITEE] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n"]}
{"filename": "src/scrape_lever.py", "chunked_list": ["from caqui import asynchronous\n\tfrom selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tCSS_SELECTOR = \"css\"  # for ChromeDriver\n\tdef clean_location(location):\n\t    locations = set(filter(None, ([x.strip() for x in location.split(',')])))\n\t    if len(locations) == 1:\n\t        return next(iter(locations)).strip().lstrip('-').title()\n\t    joined = ' '.join(locations).lower()\n\t    if joined.count('remote') > 1:\n", "        return joined.replace('remote', '', 1).strip().lstrip('-').title()\n\t    return joined.strip().lstrip('-').title()\n\tclass ScrapeLever(ScrapeIt):\n\t    def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[LEVER] Scrap page: {web_page}')\n\t        driver.get(web_page)\n\t        group_elements = driver.find_elements(By.CSS_SELECTOR, 'a[class=\"posting-title\"]')\n\t        print(f'[LEVER] Found {len(group_elements)} jobs.')\n\t        result = []\n\t        for elem in group_elements:\n", "            link_elem = elem.find_element(By.CSS_SELECTOR, '[data-qa=\"posting-name\"]')\n\t            location_elem = elem.find_elements(By.CSS_SELECTOR, '[class*=\"location\"]')\n\t            workplace_elem = elem.find_elements(By.CSS_SELECTOR, '[class*=\"workplaceTypes\"]')\n\t            job_url = elem.get_attribute('href')\n\t            if len(location_elem) > 0:\n\t                location = location_elem[0].text\n\t            else:\n\t                location = ''\n\t            if len(workplace_elem) > 0:\n\t                workplace = workplace_elem[0].text\n", "                merge_location = f'{location},{workplace}'\n\t            else:\n\t                merge_location = location\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": link_elem.text,\n\t                \"location\": clean_location(merge_location),\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n", "        print(f'[LEVER] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n\tclass ScrapeLeverAsync(ScrapeIt):\n\t    name = 'Lever'\n\t    async def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n\t        await asynchronous.go_to_page(*driver, web_page)\n\t        await asynchronous.set_timeouts(*driver, 5000)\n\t        group_elements = await asynchronous.find_elements(*driver, CSS_SELECTOR, 'a[class=\"posting-title\"]')\n", "        print(f'[LEVER] Found {len(group_elements)} jobs.')\n\t        result = []\n\t        for elem in group_elements:\n\t            link_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, '[data-qa=\"posting-name\"]')\n\t            location_elem = await asynchronous.find_children_elements(*driver, elem, CSS_SELECTOR, '[class*=\"location\"]')\n\t            workplace_elem = await asynchronous.find_children_elements(*driver, elem, CSS_SELECTOR, '[class*=\"workplaceTypes\"]')\n\t            job_url = await asynchronous.get_attribute(*driver, elem, \"href\")\n\t            title_text = await asynchronous.get_text(*driver, link_elem)\n\t            if len(location_elem) > 0:\n\t                location = await asynchronous.get_text(*driver, location_elem[0])\n", "            else:\n\t                location = ''\n\t            if len(workplace_elem) > 0:\n\t                workplace = await asynchronous.get_text(*driver, workplace_elem[0])\n\t                merge_location = f'{location},{workplace}'\n\t            else:\n\t                merge_location = location\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": title_text,\n", "                \"location\": clean_location(merge_location),\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n\t        print(f'[LEVER] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n"]}
{"filename": "src/scrape_smartrecruiters.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\timport time\n\tdef clean_location(location):\n\t    if 'remote' in location.lower() or 'global' in location.lower():\n\t        return \"REMOTE\"\n\t    locations = set(([x.strip() for x in location.split(',')]))\n\t    joined = ' '.join(locations)\n\t    return joined\n\tdef show_more(driver, locator):\n", "    print(f'[SmartRecruiters] Show more jobs..')\n\t    show_more_button = driver.find_elements(By.XPATH, locator)\n\t    if len(show_more_button) > 0:\n\t        driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button[0])\n\t        time.sleep(5)\n\t        driver.execute_script(\"arguments[0].click();\", show_more_button[0])\n\t        time.sleep(5)\n\t        show_more(driver, locator)\n\tclass ScrapeSmartrecruiters(ScrapeIt):\n\t    def getJobs(self, driver, web_page, company) -> []:\n", "        print(f'[SmartRecruiters] Scrap page: {web_page}')\n\t        driver.get(web_page)\n\t        more_links = '//a[.=\"Show more jobs\"]'\n\t        show_more(driver, more_links)\n\t        elements_locator = '//li[contains(@class,\"opening-job\") and not(contains(@class,\"js-more-container\"))]'\n\t        group_elements = driver.find_elements(By.XPATH, elements_locator)\n\t        print(f'[SmartRecruiters] Found jobs: {len(group_elements)}')\n\t        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n", "            job_title = elem.find_element(By.CSS_SELECTOR, 'h4').text\n\t            location_elem = elem.find_element(By.CSS_SELECTOR, 'span')\n\t            job_url = link_elem.get_attribute('href')\n\t            location = clean_location(location_elem.text)\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": job_title,\n\t                \"location\": location,\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n", "            result.append(job)\n\t        print(f'[SmartRecruiters] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n"]}
{"filename": "src/scrape_bamboohr.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tdef clean_location(location):\n\t    locations = set(filter(None, ([x.strip() for x in location.split(',')])))\n\t    if len(locations) == 1:\n\t        return next(iter(locations))\n\t    joined = ' '.join(locations)\n\t    if joined.count('remote') > 1:\n\t        return joined.replace('remote', '', 1).strip().strip('-').title()\n\t    return joined.strip().strip('-').title()\n", "class ScrapeBamboohr(ScrapeIt):\n\t    name = 'bamboohr'\n\t    def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n\t        driver.get(web_page)\n\t        driver.implicitly_wait(5)\n\t        group_elements = driver.find_elements(By.CSS_SELECTOR, 'div[itemscope].row')\n\t        job_location_locator = 'div[itemprop=\"jobLocation\"]'\n\t        if len(group_elements) == 0:\n\t            group_elements = driver.find_elements(By.XPATH, '//li/div/a/..')\n", "            job_location_locator = 'div[class=\"jss-e78\"]'\n\t        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n\t        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n\t            job_name_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n\t            location_elem = elem.find_element(By.CSS_SELECTOR, job_location_locator)\n\t            job_url = link_elem.get_attribute('href')\n\t            job_name = job_name_elem.text\n\t            location = location_elem.text\n", "            cleaned_location = location.replace('\\n', ', ')\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": job_name,\n\t                \"location\": clean_location(cleaned_location),\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n\t        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n", "        return result\n"]}
{"filename": "src/scrape_workable.py", "chunked_list": ["import time\n\tfrom selenium.webdriver.support import expected_conditions as EC\n\tfrom selenium.webdriver.common.by import By\n\tfrom selenium.webdriver.support.wait import WebDriverWait\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tdef show_more(driver, locator):\n\t    print(f'[workable] Show more jobs..')\n\t    show_more_button = driver.find_elements(By.CSS_SELECTOR, locator)\n\t    if len(show_more_button) > 0:\n\t        driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button[0])\n", "        time.sleep(5)\n\t        driver.execute_script(\"arguments[0].click();\", show_more_button[0])\n\t        time.sleep(5)\n\t        show_more(driver, locator)\n\tdef clean_location(location):\n\t    return ' '.join(set(([x.strip() for x in location.split(',')])))\n\tclass ScrapeWorkable(ScrapeIt):\n\t    name = 'workable'.upper()\n\t    def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n", "        driver.get(web_page)\n\t        driver.implicitly_wait(20)\n\t        wait = WebDriverWait(driver, 20)\n\t        result = []\n\t        show_more_locator = 'button[data-ui=\"load-more-button\"]'\n\t        job_root_locator = 'li[data-ui^=\"job\"]'\n\t        show_more_buttons = driver.find_elements(By.CSS_SELECTOR, show_more_locator)\n\t        if len(show_more_buttons) > 0:\n\t            print(f'[{self.name}] Show more Jobs button found..')\n\t            show_more(driver, show_more_locator)\n", "        # just try to find elements and exit if none\n\t        temp_elements = driver.find_elements(By.CSS_SELECTOR, job_root_locator)\n\t        if len(temp_elements) == 0:\n\t            print(f'[{self.name}] Found 0 jobs on {web_page}')\n\t            return result\n\t        group_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, job_root_locator)))\n\t        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n\t        driver.implicitly_wait(5)\n\t        for elem in group_elements:\n\t            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n", "            remote_elem = elem.find_elements(By.CSS_SELECTOR, '[data-ui=\"job-remote\"]')\n\t            job_name_elem = elem.find_element(By.CSS_SELECTOR, '[data-ui=\"job-title\"],[data-id=\"job-item\"]')\n\t            location_elem = elem.find_element(By.CSS_SELECTOR, 'span[data-ui=\"job-location\"]')\n\t            job_url = link_elem.get_attribute('href')\n\t            job_name = job_name_elem.text\n\t            location = location_elem.text\n\t            if len(remote_elem) > 0:\n\t                location = location + ' REMOTE'\n\t            job = {\n\t                \"company\": company,\n", "                \"title\": job_name,\n\t                \"location\": clean_location(location),\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n\t        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n"]}
{"filename": "src/company_list.py", "chunked_list": ["import json\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_lever import ScrapeLever\n\tfrom src.scrape_greenhouse import ScrapeGreenhouse\n\tfrom src.scrape_smartrecruiters import ScrapeSmartrecruiters\n\tfrom src.scrape_recruitee import ScrapeRecruitee\n\tfrom src.scrape_binance import ScrapeBinance\n\tfrom src.scrape_bamboohr import ScrapeBamboohr\n\tfrom src.scrape_consensys import ScrapeConsensys\n\tfrom src.scrape_ripple import ScrapeRipple\n", "from src.scrape_workable import ScrapeWorkable\n\tfrom src.scrape_ashbyhq import ScrapeAshbyhq\n\tfrom src.scrape_paxos import ScrapePaxos\n\tdef get_company_list() -> []:\n\t    return [CompanyItem(\"kraken\", \"https://jobs.lever.co/kraken\", ScrapeLever, \"https://kraken.com\", \"Exchange\"),\n\t            CompanyItem('arbitrumfoundation', 'https://jobs.lever.co/arbitrumfoundation', ScrapeLever,\n\t                        'https://arbitrum.foundation', 'Layer 2'),\n\t            CompanyItem(\"chainlink\", \"https://jobs.lever.co/chainlink\", ScrapeLever, \"https://chain.link\",\n\t                        \"Blockchain\"),\n\t            CompanyItem('ethglobal', 'https://jobs.lever.co/ETHGlobal', ScrapeLever, 'https://ethglobal.com',\n", "                        'Community'),\n\t            CompanyItem('multiversx', 'https://jobs.lever.co/multiversx', ScrapeLever, 'https://multiversx.com',\n\t                        'Blockchain'),\n\t            CompanyItem('sprucesystems', 'https://jobs.lever.co/sprucesystems', ScrapeLever, 'https://spruceid.com',\n\t                        'Web3 ID'),\n\t            CompanyItem('BlockSwap', 'https://jobs.lever.co/BlockSwap', ScrapeLever, 'https://www.blockswap.network',\n\t                        'Infra'),\n\t            CompanyItem('Metatheory', 'https://jobs.lever.co/Metatheory', ScrapeLever,\n\t                        'https://www.duskbreakers.gg', 'Web3 game'),\n\t            CompanyItem('axiomzen', 'https://jobs.lever.co/axiomzen', ScrapeLever, 'https://www.axiomzen.com', 'Web3'),\n", "            CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLever, 'https://www.fuel.network',\n\t                        'Blockchain'),\n\t            CompanyItem('harmony', 'https://jobs.lever.co/harmony', ScrapeLever, 'https://www.harmony.one',\n\t                        'Blockchain'),\n\t            CompanyItem('wintermute', 'https://jobs.lever.co/wintermute-trading', ScrapeLever,\n\t                        'https://www.wintermute.com',\n\t                        'Trading'),\n\t            CompanyItem(\"kaiko\", \"https://jobs.eu.lever.co/kaiko\", ScrapeLever, \"https://www.kaiko.com\", \"Data\"),\n\t            CompanyItem('bebop', 'https://jobs.lever.co/Bebop', ScrapeLever, 'https://bebop.xyz', 'DeFi Exchange'),\n\t            CompanyItem(\"Coinshift\", \"https://jobs.lever.co/Coinshift\", ScrapeLever, \"https://coinshift.xyz\",\n", "                        \"Custody software\"),\n\t            CompanyItem(\"swissborg\", \"https://jobs.lever.co/swissborg\", ScrapeLever, \"https://swissborg.com\",\n\t                        \"Exchange\"),\n\t            CompanyItem(\"OpenSea\", \"https://jobs.lever.co/OpenSea\", ScrapeLever, \"https://opensea.io\", \"NFT\"),\n\t            CompanyItem(\"storyprotocol\", \"https://jobs.lever.co/storyprotocol\", ScrapeLever,\n\t                        \"https://www.storyprotocol.xyz\", \"Protocol\"),\n\t            CompanyItem(\"ethereumfoundation\", \"https://jobs.lever.co/ethereumfoundation\", ScrapeLever,\n\t                        \"https://ethereum.org\", \"Blockchain\"),\n\t            CompanyItem(\"aave\", \"https://jobs.eu.lever.co/aave\", ScrapeLever, \"https://aave.com\", \"Protocol\"),\n\t            CompanyItem(\"crypto\", \"https://jobs.lever.co/crypto\", ScrapeLever, \"https://crypto.com\", \"Exchange\"),\n", "            CompanyItem(\"Luxor\", \"https://jobs.lever.co/LuxorTechnology\", ScrapeLever, \"https://www.luxor.tech\",\n\t                        \"Mining\"),\n\t            CompanyItem(\"anchorage\", \"https://jobs.lever.co/anchorage\", ScrapeLever, \"https://www.anchorage.com\",\n\t                        \"Trading\"),\n\t            CompanyItem(\"biconomy\", \"https://jobs.lever.co/biconomy\", ScrapeLever, \"https://www.biconomy.io\",\n\t                        \"Infra\"),\n\t            CompanyItem('enso', 'https://jobs.lever.co/Enso', ScrapeLever, 'https://www.enso.finance', 'DeFi'),\n\t            CompanyItem(\"Polygon\", \"https://jobs.lever.co/Polygon\", ScrapeLever, \"https://polygon.technology\",\n\t                        \"Blockchain\"),\n\t            CompanyItem(\"tokenmetrics\", \"https://jobs.lever.co/tokenmetrics\", ScrapeLever,\n", "                        \"https://www.tokenmetrics.com\", \"Information\"),\n\t            CompanyItem(\"offchainlabs\", \"https://jobs.lever.co/offchainlabs\", ScrapeLever,\n\t                        \"https://offchainlabs.com\", \"Protocol\"),\n\t            CompanyItem(\"subspacelabs\", \"https://jobs.lever.co/subspacelabs\", ScrapeLever,\n\t                        \"https://subspace.network\", \"Blockchain Infra\"),\n\t            CompanyItem('3boxlabs', 'https://jobs.lever.co/3box', ScrapeLever, 'https://3boxlabs.com',\n\t                        'Infra'),\n\t            CompanyItem(\"ramp.network\", \"https://jobs.lever.co/careers.ramp.network\", ScrapeLever,\n\t                        \"https://ramp.network\", \"Payments\"),\n\t            CompanyItem('risklabs', 'https://jobs.lever.co/risklabs', ScrapeLever, 'https://risklabs.foundation',\n", "                        'Protocol'),\n\t            CompanyItem('celestia', 'https://jobs.lever.co/celestia', ScrapeLever, 'https://celestia.org',\n\t                        'Modular Blockchain'),\n\t            CompanyItem('polymerlabs', 'https://jobs.lever.co/polymerlabs', ScrapeLever, 'https://www.polymerlabs.org',\n\t                        'Modular Blockchain'),\n\t            CompanyItem('royal', 'https://jobs.lever.co/Royal', ScrapeLever, 'https://royal.io', 'Web3 + Music'),\n\t            CompanyItem('gauntlet', 'https://jobs.lever.co/gauntlet', ScrapeLever, 'https://gauntlet.network',\n\t                        'Web3 + Financial Modelling'),\n\t            CompanyItem(\"ledger\", \"https://jobs.lever.co/ledger\", ScrapeLever, \"https://www.ledger.com\", \"Wallet\"),\n\t            CompanyItem(\"request\", \"https://jobs.lever.co/request\", ScrapeLever, \"https://request.network\",\n", "                        \"Payments\"),\n\t            CompanyItem(\"immutable\", \"https://jobs.lever.co/immutable\", ScrapeLever, \"https://www.immutable.com\",\n\t                        \"NFT\"),\n\t            CompanyItem(\"web3auth\", \"https://jobs.lever.co/TorusLabs\", ScrapeLever, \"https://web3auth.io\", \"Auth\"),\n\t            CompanyItem(\"cere-network\", \"https://jobs.lever.co/cere-network\", ScrapeLever, \"https://cere.network\",\n\t                        \"Infra\"),\n\t            CompanyItem('matterlabs', 'https://jobs.eu.lever.co/matterlabs', ScrapeLever, 'https://matter-labs.io',\n\t                        'Protocol'),\n\t            CompanyItem(\"hiro\", \"https://jobs.lever.co/hiro\", ScrapeLever, \"https://www.hiro.so\", \"Infra\"),\n\t            CompanyItem('AQX', 'https://jobs.lever.co/presto', ScrapeLever, 'https://aqx.com', 'Exchange and Web3'),\n", "            CompanyItem('ultra', 'https://jobs.lever.co/ultra', ScrapeLever,\n\t                        'https://ultra.io', 'Web3 Gaming'),\n\t            CompanyItem('ethenalabs', 'https://jobs.lever.co/ethenalabs', ScrapeLever,\n\t                        'https://www.ethena.fi', 'Web3 bonds'),\n\t            CompanyItem('bitwise', 'https://jobs.lever.co/bitwiseinvestments', ScrapeLever,\n\t                        'https://bitwiseinvestments.com', 'Asset Management'),\n\t            CompanyItem('grayscaleinvestments', 'https://boards.greenhouse.io/grayscaleinvestments', ScrapeGreenhouse,\n\t                        'https://grayscale.com', 'Web3 Asset Manager'),\n\t            CompanyItem(\"0x\", \"https://boards.greenhouse.io/0x\", ScrapeGreenhouse, \"https://0x.org\",\n\t                        \"Blockchain\"),\n", "            CompanyItem('econetwork', 'https://boards.greenhouse.io/econetwork', ScrapeGreenhouse,\n\t                        'https://eco.com', 'Web3 wallet'),\n\t            CompanyItem(\"bitcoin\", \"https://www.bitcoin.com/jobs/#joblist\", ScrapeGreenhouse,\n\t                        \"https://www.bitcoin.com\", 'Exchange'),\n\t            CompanyItem('magic', 'https://boards.greenhouse.io/magic', ScrapeGreenhouse, 'https://magic.link',\n\t                        'Web3 Wallets'),\n\t            CompanyItem(\"chainstack\", \"https://chainstack.bamboohr.com/careers\", ScrapeBamboohr,\n\t                        \"https://chainstack.com\", \"Infra\"),\n\t            CompanyItem(\"coinmarketcap\", \"https://careers.smartrecruiters.com/B6/coinmarketcap\",\n\t                        ScrapeSmartrecruiters, \"https://coinmarketcap.com\", \"Information\"),\n", "            CompanyItem('evmos', 'https://boards.eu.greenhouse.io/evmos', ScrapeGreenhouse, 'https://evmos.org',\n\t                        'Cross-Chain Connectivity'),\n\t            CompanyItem('orderlynetwork', 'https://boards.greenhouse.io/orderlynetwork', ScrapeGreenhouse,\n\t                        'https://orderly.network', 'Exchange'),\n\t            CompanyItem(\"paxos\",\n\t                        \"https://paxos.com/job-posts/?_sft_department=engineering-data,finance-accounting,hr-talent,information-technology,legal,operations,product-management,risk-compliance&_sft_office=us\",\n\t                        ScrapePaxos, \"https://paxos.com\",\n\t                        \"Stable Coin\"),\n\t            CompanyItem('dydxopsdao', 'https://apply.workable.com/dydx-operations-trust', ScrapeWorkable,\n\t                        'https://dydxopsdao.com', 'Web3 DAO'),\n", "            CompanyItem('almanak', 'https://apply.workable.com/almanak-blockchain-labs-ag', ScrapeWorkable,\n\t                        'https://almanak.co', 'Web3 Simulator'),\n\t            CompanyItem(\"zora\", \"https://boards.greenhouse.io/zora\", ScrapeGreenhouse, \"https://zora.co\", \"NFT\"),\n\t            CompanyItem('bitfury', 'https://boards.greenhouse.io/bitfury', ScrapeGreenhouse, 'https://bitfury.com',\n\t                        'Web3'),\n\t            CompanyItem(\"cexio\", \"https://cexio.bamboohr.com/jobs\", ScrapeBamboohr, \"https://cex.io\", \"Exchange\"),\n\t            CompanyItem(\"circle\", \"https://boards.greenhouse.io/circle\", ScrapeGreenhouse, \"https://circle.com\",\n\t                        \"Stable Coin\"),\n\t            CompanyItem(\"status\", \"https://jobs.status.im\", ScrapeGreenhouse, \"https://status.im\", \"Messanger\"),\n\t            CompanyItem(\"OKX\", \"https://boards.greenhouse.io/OKX\", ScrapeGreenhouse, \"https://okx.com\",\n", "                        \"Exchange\"),\n\t            CompanyItem(\"bittrex\", \"https://boards.greenhouse.io/bittrex\", ScrapeGreenhouse,\n\t                        \"https://global.bittrex.com\", 'Exchange'),\n\t            CompanyItem(\"bitmex\", \"https://boards.greenhouse.io/bitmex\", ScrapeGreenhouse, \"https://bitmex.com\",\n\t                        \"Exchange\"),\n\t            CompanyItem(\"bitgo\", \"https://boards.greenhouse.io/bitgo\", ScrapeGreenhouse, \"https://bitgo.com\",\n\t                        \"Exchange\"),\n\t            CompanyItem(\"bitpanda\", \"https://boards.eu.greenhouse.io/bitpanda\", ScrapeGreenhouse,\n\t                        \"https://bitpanda.com\", \"Exchange\"),\n\t            CompanyItem(\"uniswaplabs\", \"https://boards.greenhouse.io/uniswaplabs\", ScrapeGreenhouse,\n", "                        \"https://uniswap.org\", \"Exchange Protocol\"),\n\t            CompanyItem('osmosisdex', 'https://boards.greenhouse.io/osmosisdex', ScrapeGreenhouse,\n\t                        'https://osmosis.zone',\n\t                        'Exchange'),\n\t            CompanyItem(\"moonpay\", \"https://boards.greenhouse.io/moonpay\", ScrapeGreenhouse,\n\t                        \"https://www.moonpay.com\", \"Payments\"),\n\t            CompanyItem('penumbralabs', 'https://boards.greenhouse.io/penumbralabs', ScrapeGreenhouse,\n\t                        'https://eco.com', 'Web3 trading'),\n\t            CompanyItem(\"blockdaemon\", \"https://boards.greenhouse.io/blockdaemon\", ScrapeGreenhouse,\n\t                        \"https://www.blockdaemon.com\", \"Staking & Infra\"),\n", "            CompanyItem(\"figment\", \"https://boards.greenhouse.io/figment\", ScrapeGreenhouse,\n\t                        \"https://www.figment.io\", \"Staking & Infra\"),\n\t            CompanyItem('rain', 'https://jobs.ashbyhq.com/rain', ScrapeAshbyhq, 'https://www.raincards.xyz',\n\t                        'Web3 cards'),\n\t            CompanyItem('exponential', 'https://jobs.ashbyhq.com/exponential', ScrapeAshbyhq, 'https://exponential.fi',\n\t                        'DeFi'),\n\t            CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhq, 'https://conduit.xyz',\n\t                        'Infrastructure'),\n\t            CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhq, 'https://www.kiln.fi',\n\t                        'Staking & Infra'),\n", "            CompanyItem(\"flashbots\", \"https://jobs.ashbyhq.com/flashbots.net\", ScrapeAshbyhq,\n\t                        \"https://www.flashbots.net\", \"ETH MEV\"),\n\t            CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhq, 'https://www.paradigm.xyz',\n\t                        'Web3 financing'),\n\t            CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhq, 'https://dune.com',\n\t                        'Web3 data'),\n\t            CompanyItem(\"solanafoundation\", \"https://jobs.ashbyhq.com/Solana%20Foundation\", ScrapeAshbyhq,\n\t                        \"https://solana.org\", \"Blockchain\"),\n\t            CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhq, 'https://syndica.io',\n\t                        'Infrastructure'),\n", "            CompanyItem('Blockworks', 'https://jobs.ashbyhq.com/Blockworks', ScrapeAshbyhq,\n\t                        'https://blockworks.co', 'Web3 News'),\n\t            CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhq,\n\t                        'https://ellipsislabs.xyz', 'Trading Protocol'),\n\t            CompanyItem(\"quiknodeinc\", \"https://boards.greenhouse.io/quiknodeinc\", ScrapeGreenhouse,\n\t                        \"https://www.quicknode.com\", \"Staking & Infra\"),\n\t            CompanyItem('21co', 'https://boards.greenhouse.io/21co', ScrapeGreenhouse,\n\t                        'https://www.21.co', 'Web3 DeFi ETP'),\n\t            CompanyItem('xapo', 'https://boards.greenhouse.io/xapo61', ScrapeGreenhouse,\n\t                        'https://www.xapobank.com', 'Web3 bank'),\n", "            CompanyItem('dragonflycapital', 'https://boards.greenhouse.io/dragonflycapital', ScrapeGreenhouse,\n\t                        'https://www.dragonfly.xyz', 'Web3 funding'),\n\t            CompanyItem(\"exodus54\", \"https://boards.greenhouse.io/exodus54\", ScrapeGreenhouse,\n\t                        \"https://www.exodus.com\", \"Wallet\"),\n\t            CompanyItem(\"alchemy\", \"https://boards.greenhouse.io/alchemy\", ScrapeGreenhouse,\n\t                        \"https://www.alchemy.com\", \"Dev & Infra\"),\n\t            CompanyItem(\"chainalysis\", \"https://boards.greenhouse.io/chainalysis\", ScrapeGreenhouse,\n\t                        \"https://www.chainalysis.com\", \"Crypto Research\"),\n\t            CompanyItem(\"magiceden\", \"https://boards.greenhouse.io/magiceden\", ScrapeGreenhouse,\n\t                        \"https://www.magiceden.io\", \"NFT\"),\n", "            CompanyItem(\"aztec\", \"https://boards.eu.greenhouse.io/aztec\", ScrapeGreenhouse,\n\t                        \"https://aztec.network\", \"Protocol\"),\n\t            CompanyItem(\"nethermind\", \"https://boards.eu.greenhouse.io/nethermind\", ScrapeGreenhouse,\n\t                        \"https://nethermind.io\", \"Crypto software\"),\n\t            CompanyItem(\"dfinity\", \"https://boards.greenhouse.io/dfinity\", ScrapeGreenhouse, \"https://dfinity.org\",\n\t                        \"Blockchain\"),\n\t            CompanyItem('stellar', 'https://boards.greenhouse.io/stellar', ScrapeGreenhouse,\n\t                        'https://stellar.org', 'Blockchain'),\n\t            CompanyItem(\"parity\", \"https://boards.greenhouse.io/parity\", ScrapeGreenhouse, \"https://www.parity.io\",\n\t                        \"Infra\"),\n", "            CompanyItem(\"optimism\", \"https://boards.greenhouse.io/optimism\", ScrapeGreenhouse,\n\t                        \"https://www.optimism.io\", \"L2 protocol\"),\n\t            CompanyItem('coinmetrics', 'https://boards.greenhouse.io/coinmetrics', ScrapeGreenhouse,\n\t                        'https://coinmetrics.io', 'Web3 Data'),\n\t            CompanyItem(\"oplabs\", \"https://boards.greenhouse.io/oplabs\", ScrapeGreenhouse, \"https://www.oplabs.co\",\n\t                        \"L2 protocol\"),\n\t            CompanyItem('goldsky', 'https://boards.greenhouse.io/goldsky', ScrapeGreenhouse,\n\t                        'https://goldsky.com', 'Web3 Data'),\n\t            CompanyItem('outlierventures', 'https://boards.eu.greenhouse.io/outlierventures', ScrapeGreenhouse,\n\t                        'https://outlierventures.io', 'Web3 Ventures'),\n", "            CompanyItem('walletconnect', 'https://apply.workable.com/walletconnect', ScrapeWorkable,\n\t                        'https://walletconnect.com', 'Web3 Wallet Infra'),\n\t            CompanyItem(\"bitfinex\", \"https://bitfinex.recruitee.com\", ScrapeRecruitee, \"https://www.bitfinex.com\",\n\t                        \"Exchange\"),\n\t            CompanyItem('o1labs', 'https://boards.greenhouse.io/o1labs', ScrapeGreenhouse, 'https://o1labs.org',\n\t                        'Web3'),\n\t            CompanyItem('paradigm.co', 'https://boards.greenhouse.io/paradigm62', ScrapeGreenhouse,\n\t                        'https://www.paradigm.co',\n\t                        'Liquidity'),\n\t            CompanyItem(\"binance\", \"https://www.binance.com/en/careers/job-openings\", ScrapeBinance,\n", "                        \"https://www.binance.com\", \"Exchange\"),\n\t            CompanyItem(\"trustwallet\", \"https://careers.smartrecruiters.com/B6/trustwallet\", ScrapeSmartrecruiters,\n\t                        \"https://trustwallet.com\", \"Wallet\"),\n\t            CompanyItem(\"Swissquote\", \"https://careers.smartrecruiters.com/Swissquote\", ScrapeSmartrecruiters,\n\t                        \"https://en.swissquote.com\", \"Exchange\"),\n\t            CompanyItem('taxbit', 'https://boards.greenhouse.io/taxbit', ScrapeGreenhouse, 'https://taxbit.com',\n\t                        'Accounting'),\n\t            CompanyItem(\"avalabs\", \"https://boards.greenhouse.io/avalabs\", ScrapeGreenhouse,\n\t                        \"https://www.avalabs.org\", \"Blockchain\"),\n\t            CompanyItem(\"aptoslabs\", \"https://boards.greenhouse.io/aptoslabs\", ScrapeGreenhouse,\n", "                        \"https://aptoslabs.com\",\n\t                        \"Blockchain\"),\n\t            CompanyItem(\"filecoinfoundation\", \"https://boards.greenhouse.io/filecoinfoundation\", ScrapeGreenhouse,\n\t                        \"https://fil.org\", \"Blockchain\"),\n\t            CompanyItem('foundrydigital', 'https://boards.greenhouse.io/foundrydigital', ScrapeGreenhouse,\n\t                        'https://foundrydigital.com', 'Web3 Infra'),\n\t            CompanyItem('immunefi', 'https://boards.greenhouse.io/immunefi', ScrapeGreenhouse, 'https://immunefi.com',\n\t                        'Bug bounty platform'),\n\t            CompanyItem('wirex', 'https://wirex.bamboohr.com/careers', ScrapeBamboohr, 'https://wirexapp.com',\n\t                        'Web3 card'),\n", "            CompanyItem('protocollabs', 'https://boards.greenhouse.io/protocollabs', ScrapeGreenhouse,\n\t                        'https://protocol.ai/about',\n\t                        'Web3 IPFS research platform'),\n\t            CompanyItem('trmlabs', 'https://www.trmlabs.com/careers-list', ScrapeGreenhouse,\n\t                        'https://www.trmlabs.com', 'Web3 Information'),\n\t            CompanyItem(\"messari\", \"https://boards.greenhouse.io/messari\", ScrapeGreenhouse, \"https://messari.io\",\n\t                        \"Web3 Information\"),\n\t            CompanyItem(\"serotonin\", \"https://boards.greenhouse.io/serotonin\", ScrapeGreenhouse, \"https://serotonin.co\",\n\t                        \"Information\"),\n\t            CompanyItem(\"copperco\", \"https://boards.eu.greenhouse.io/copperco\", ScrapeGreenhouse,\n", "                        \"https://copper.co\", \"Custody\"),\n\t            CompanyItem(\"digitalasset\", \"https://boards.greenhouse.io/digitalasset\", ScrapeGreenhouse,\n\t                        \"https://www.digitalasset.com\", \"Custody\"),\n\t            CompanyItem(\"layerzerolabs\", \"https://boards.greenhouse.io/layerzerolabs\", ScrapeGreenhouse,\n\t                        \"https://layerzero.network\", \"Infra\"),\n\t            CompanyItem('okcoin', 'https://boards.greenhouse.io/okcoin', ScrapeGreenhouse,\n\t                        'https://www.okcoin.com', 'Exchange'),\n\t            CompanyItem(\"oasisnetwork\", \"https://boards.greenhouse.io/oasisnetwork\", ScrapeGreenhouse,\n\t                        \"https://oasisprotocol.org\", \"Protocol\"),\n\t            CompanyItem(\"consensys\", \"https://consensys.net/open-roles\", ScrapeConsensys, \"https://consensys.net\",\n", "                        \"Infra\"),\n\t            CompanyItem(\"ankr\", \"https://boards.greenhouse.io/ankrnetwork\", ScrapeGreenhouse,\n\t                        \"https://www.ankr.com\", \"Web3 Staking Protocol\"),\n\t            CompanyItem(\"chainsafesystems\", \"https://boards.greenhouse.io/chainsafesystems\", ScrapeGreenhouse,\n\t                        \"https://chainsafe.io\", \"Infra\"),\n\t            CompanyItem(\"ripple\", \"https://ripple.com/careers/all-jobs\", ScrapeRipple, \"https://ripple.com\",\n\t                        \"Blockchain\"),\n\t            CompanyItem(\"kadena\", \"https://boards.greenhouse.io/kadenallc\", ScrapeGreenhouse, \"https://kadena.io\",\n\t                        \"Blockchain\"),\n\t            CompanyItem(\"eigenlabs\", \"https://boards.greenhouse.io/eigenlabs\", ScrapeGreenhouse,\n", "                        \"https://www.eigenlayer.xyz\", \"Infra\"),\n\t            CompanyItem('sygnum', 'https://sygnum.bamboohr.com/careers', ScrapeBamboohr, 'https://www.sygnum.com',\n\t                        'Crypto bank'),\n\t            CompanyItem(\"galaxydigitalservices\", \"https://boards.greenhouse.io/galaxydigitalservices\",\n\t                        ScrapeGreenhouse, \"https://www.galaxy.com\", 'Trading'),\n\t            CompanyItem('web3', 'https://web3.bamboohr.com/jobs', ScrapeBamboohr, 'https://web3.foundation',\n\t                        'web3'),\n\t            CompanyItem(\"solana\", \"https://boards.greenhouse.io/solana\", ScrapeGreenhouse,\n\t                        \"https://solana.com\", \"Blockchain\"),\n\t            CompanyItem('mobilecoin', 'https://boards.greenhouse.io/mobilecoin', ScrapeGreenhouse,\n", "                        'https://mobilecoin.com', 'Blockchain'),\n\t            CompanyItem('chia', 'https://www.chia.net/careers', ScrapeGreenhouse,\n\t                        'https://www.chia.net', 'Blockchain'),\n\t            CompanyItem(\"worldcoin\", \"https://boards.greenhouse.io/worldcoinorg\", ScrapeGreenhouse,\n\t                        \"https://worldcoin.org\", \"Blockchain\"),\n\t            CompanyItem(\"edgeandnode\", \"https://boards.greenhouse.io/edgeandnode\", ScrapeGreenhouse,\n\t                        \"https://edgeandnode.com\", \"Infra\"),\n\t            CompanyItem(\"clearmatics\", \"https://boards.greenhouse.io/clearmatics\", ScrapeGreenhouse,\n\t                        \"https://www.clearmatics.com\", \"Protocol\"),\n\t            CompanyItem('bitstamp', 'https://apply.workable.com/bitstamp/#jobs', ScrapeWorkable,\n", "                        'https://www.bitstamp.net', 'Exchange'),\n\t            CompanyItem('yugalabs', 'https://boards.greenhouse.io/yugalabs', ScrapeGreenhouse,\n\t                        'https://yuga.com', 'NFT'),\n\t            CompanyItem('cryptofinance', 'https://apply.workable.com/crypto-finance', ScrapeWorkable,\n\t                        'https://www.crypto-finance.com', 'Exchange'),\n\t            CompanyItem('bitget', 'https://apply.workable.com/bitget', ScrapeWorkable, 'https://www.bitget.com/en',\n\t                        'Exchange')]\n\tdef get_company(name) -> CompanyItem:\n\t    company_list = get_company_list()\n\t    companies = list(filter(lambda jd: jd.company_name == name, company_list))\n", "    if len(companies) > 1:\n\t        raise NameError(f'Duplicated company name: {name}')\n\t    return companies[0]\n\tdef write_companies(file_name):\n\t    result_list = []\n\t    for com in get_company_list():\n\t        company_item = {\n\t            \"company_name\": com.company_name,\n\t            \"company_url\": com.company_url,\n\t            \"jobs_url\": com.jobs_url,\n", "        }\n\t        result_list.append(company_item)\n\t    print(f'[COMPANY_LIST] Number of Companies writen {len(result_list)}')\n\t    with open(file_name, 'w') as companies_file:\n\t        json.dump(result_list, companies_file, indent=4)\n"]}
{"filename": "src/company_item.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom src.scrape_it import ScrapeIt\n\t@dataclass(init=True)\n\tclass CompanyItem:\n\t    company_name: str\n\t    jobs_url: str\n\t    scraper_type: ScrapeIt\n\t    company_url: str\n\t    company_type: str\n"]}
{"filename": "src/company_list_empty.py", "chunked_list": ["import json\n\tfrom src.company_item import CompanyItem\n\tfrom src.scrape_bamboohr import ScrapeBamboohr\n\tfrom src.scrape_greenhouse import ScrapeGreenhouse\n\tfrom src.scrape_lever import ScrapeLever\n\tfrom src.scrape_workable import ScrapeWorkable\n\tdef get_company_list() -> []:\n\t    return [\n\t        CompanyItem(\"archblock\", \"https://jobs.lever.co/archblock\", ScrapeLever, \"https://www.archblock.com\",\n\t                    \"Stable Coin\"),\n", "        CompanyItem(\"tessera\", \"https://jobs.lever.co/ftc\", ScrapeLever, \"https://tessera.co\", \"NFT\"),\n\t        CompanyItem(\"moonwalk\", \"https://boards.greenhouse.io/moonwalk\", ScrapeGreenhouse,\n\t                    \"https://www.moonwalk.com\", \"Platform\"),\n\t        CompanyItem(\"tron\", \"https://boards.greenhouse.io/rainberry\", ScrapeGreenhouse, \"https://tron.network\",\n\t                    \"Blockchain\"),\n\t        CompanyItem(\"jumpcrypto\", \"https://boards.greenhouse.io/jumpcrypto\", ScrapeGreenhouse,\n\t                    \"https://jumpcrypto.com\", \"Infra\"),\n\t        CompanyItem(\"poap\", \"https://boards.greenhouse.io/poaptheproofofattendanceprotocol\", ScrapeGreenhouse,\n\t                    \"https://poap.xyz\", \"Protocol\"),\n\t        CompanyItem('smart-token-labs', 'https://apply.workable.com/smart-token-labs', ScrapeWorkable,\n", "                    'https://smarttokenlabs.com', 'Web3 bridge'),\n\t        CompanyItem('avantgarde', 'https://apply.workable.com/avantgarde', ScrapeWorkable,\n\t                    'https://avantgarde.finance', 'Asset Management'),\n\t        CompanyItem('stably', 'https://apply.workable.com/stably', ScrapeWorkable, 'https://stably.io',\n\t                    'Stable Coin'),\n\t        CompanyItem('dappradar', 'https://dappradar.bamboohr.com/careers', ScrapeBamboohr,\n\t                    'https://dappradar.com', 'Exchange & NFT'),\n\t        CompanyItem('iofinnet', 'https://iofinnethr.bamboohr.com/jobs/?source=bamboohr', ScrapeBamboohr,\n\t                    'https://www.iofinnet.com', 'Custody'),\n\t        CompanyItem(\"amun\", \"https://boards.greenhouse.io/amun\", ScrapeGreenhouse, \"https://amun.com\", \"DeFi\"),\n", "    ]\n\tdef get_company(name) -> CompanyItem:\n\t    company_list = get_company_list()\n\t    companies = list(filter(lambda jd: jd.company_name == name, company_list))\n\t    return companies[0]\n\tdef write_companies(file_name):\n\t    result_list = []\n\t    for com in get_company_list():\n\t        company_item = {\n\t            \"company_name\": com.company_name,\n", "            \"company_url\": com.company_url,\n\t            \"jobs_url\": com.jobs_url,\n\t        }\n\t        result_list.append(company_item)\n\t    print(f'[COMPANY_LIST] Number of Companies writen {len(result_list)}')\n\t    with open(file_name, 'w') as companies_file:\n\t        json.dump(result_list, companies_file, indent=4)\n"]}
{"filename": "src/scrape_ashbyhq.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tfrom caqui import synchronous, asynchronous\n\tCSS_SELECTOR = \"css\"  # for ChromeDriver\n\tdef clean_location(location):\n\t    locations = list(filter(None, ([x.strip() for x in location.split('')])))\n\t    result = locations[1]\n\t    return result.strip().title()\n\tclass ScrapeAshbyhqAsync(ScrapeIt):\n\t    name = 'ashbyhq'\n", "    async def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n\t        # driver.get(web_page)\n\t        await asynchronous.go_to_page(*driver, web_page)\n\t        # driver.implicitly_wait(5)\n\t        await asynchronous.set_timeouts(*driver, 5000)\n\t        # group_elements = driver.find_elements(By.CSS_SELECTOR, 'a[class*=\"container_\"]')\n\t        group_elements = await asynchronous.find_elements(*driver, CSS_SELECTOR, 'a[class*=\"container_\"]')\n\t        job_location_locator = 'div p'\n\t        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n", "        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem\n\t            # job_name_elem = elem.find_element(By.CSS_SELECTOR, 'h3')\n\t            job_name_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, \"h3\")\n\t            # location_elem = elem.find_element(By.CSS_SELECTOR, job_location_locator)\n\t            location_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, job_location_locator)\n\t            # job_url = link_elem.get_attribute('href')\n\t            job_url = await asynchronous.get_attribute(*driver, link_elem, \"href\")\n\t            # job_name = job_name_elem.text\n", "            job_name = await asynchronous.get_text(*driver, job_name_elem)\n\t            # location = location_elem.text\n\t            location = await asynchronous.get_text(*driver, location_elem)\n\t            cleaned_location = location.replace('\\n', ', ')\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": job_name,\n\t                \"location\": clean_location(cleaned_location),\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n", "            result.append(job)\n\t        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n\tclass ScrapeAshbyhq(ScrapeIt):\n\t    name = 'ashbyhq'\n\t    def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n\t        driver.get(web_page)\n\t        driver.implicitly_wait(5)\n", "        group_elements = driver.find_elements(By.CSS_SELECTOR, 'a[class*=\"container_\"]')\n\t        job_location_locator = 'div p'\n\t        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n\t        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem\n\t            job_name_elem = elem.find_element(By.CSS_SELECTOR, 'h3')\n\t            location_elem = elem.find_element(By.CSS_SELECTOR, job_location_locator)\n\t            job_url = link_elem.get_attribute('href')\n\t            job_name = job_name_elem.text\n", "            location = location_elem.text\n\t            cleaned_location = location.replace('\\n', ', ')\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": job_name,\n\t                \"location\": clean_location(cleaned_location),\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n\t        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n", "        write_jobs(result)\n\t        return result\n"]}
{"filename": "src/scrape_paxos.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tdef to_records(group_elements, company) -> []:\n\t    result = []\n\t    for elem in group_elements:\n\t        job_url = elem.get_attribute('href')\n\t        job = {\n\t            \"company\": company,\n\t            \"title\": elem.text,\n\t            \"location\": 'US',\n", "            \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t        }\n\t        result.append(job)\n\t    return result\n\tclass ScrapePaxos(ScrapeIt):\n\t    def getJobs(self, driver, web_page, company='paxos') -> []:\n\t        print(f'[PAXOS] Scrap page: {web_page}')\n\t        driver.get(web_page)\n\t        next_link = 'a[class=\"page-numbers\"]'\n\t        next_links = driver.find_elements(By.CSS_SELECTOR, next_link)\n", "        group_elements = driver.find_elements(By.CSS_SELECTOR, 'h3 a[href]')\n\t        result = to_records(group_elements, company)\n\t        i = 1\n\t        for nxt in next_links:\n\t            i += 1\n\t            driver.get(f'{web_page}&sf_paged={i}')\n\t            group_elements = driver.find_elements(By.CSS_SELECTOR, 'h3 a[href]')\n\t            result += to_records(group_elements, company)\n\t        print(f'[PAXOS] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n", "        return result\n"]}
{"filename": "src/scrape_it.py", "chunked_list": ["import json\n\tdef write_jobs(jobs):\n\t    with open('jobs.json', 'r') as f:\n\t        jobs_json = json.load(f)\n\t    jobs_data = jobs_json.get('data', [])\n\t    for job in jobs:\n\t        jobs_data.append(job)\n\t    jobs_json['data'] = jobs_data\n\t    with open('jobs.json', 'w') as file:\n\t        json.dump(jobs_json, file, indent=4)\n", "class ScrapeIt:\n\t    def getJobs(self, driver, web_page, company):\n\t        pass\n"]}
{"filename": "src/scrape_consensys.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tclass ScrapeConsensys(ScrapeIt):\n\t    name = 'CONSENSYS'\n\t    def getJobs(self, driver, web_page, company='consensys') -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n\t        driver.get(web_page)\n\t        driver.get(web_page)\n\t        group_elements = driver.find_elements(By.XPATH, '//div[@id=\"careers\"]//div[contains(@class, \"careersSectionItem_itemOuter\")]')\n\t        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n", "        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n\t            job_name_elem = elem.find_element(By.CSS_SELECTOR, 'h5')\n\t            location_elem = elem.find_element(By.XPATH, '//div[contains(@class, \"careersSectionItem_location\")]')\n\t            job_url = link_elem.get_attribute('href')\n\t            job_name = job_name_elem.text\n\t            location = location_elem.text\n\t            job = {\n\t                \"company\": company,\n", "                \"title\": job_name,\n\t                \"location\": location,\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n\t        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n"]}
{"filename": "src/scrape_greenhouse.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tfrom caqui import synchronous, asynchronous\n\timport time\n\tCSS_SELECTOR = \"css\"  # for ChromeDriver\n\tdef clean_location(location):\n\t    locations = set(filter(None, ([x.strip() for x in location.split(',')])))\n\t    if len(locations) == 1:\n\t        return next(iter(locations))\n\t    joined = ' '.join(locations).lower()\n", "    if joined.count('remote') > 1:\n\t        return joined.replace('remote', '', 1).title()\n\t    return joined.strip().strip('-').title()\n\tclass ScrapeGreenhouse(ScrapeIt):\n\t    name = 'GREENHOUSE'\n\t    def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n\t        driver.get(web_page)\n\t        iframe = driver.find_elements(By.TAG_NAME, 'iframe')\n\t        if len(iframe) > 0:\n", "            print(f'[{self.name}] iFrame detected..')\n\t            time.sleep(3)\n\t            driver.switch_to.frame(iframe[0])\n\t            time.sleep(5)\n\t        group_elements = driver.find_elements(By.CSS_SELECTOR, 'div [class=\"opening\"]')\n\t        print(f'[{self.name}] Found {len(group_elements)} jobs.')\n\t        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n\t            location_elem = elem.find_element(By.CSS_SELECTOR, 'span')\n", "            job_url = link_elem.get_attribute('href')\n\t            location = location_elem.text\n\t            job_name = link_elem.text\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": job_name,\n\t                \"location\": clean_location(location),\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n", "        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n\tclass ScrapeGreenhouseAsync(ScrapeIt):\n\t    name = 'GREENHOUSE'\n\t    async def getJobs(self, driver, web_page, company) -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n\t        await asynchronous.go_to_page(*driver, web_page)\n\t        await asynchronous.set_timeouts(*driver, 5000)\n\t        iframe = await asynchronous.find_elements(*driver, By.TAG_NAME, 'iframe')\n", "        if len(iframe) > 0:\n\t            print(f'[{self.name}] iFrame detected..')\n\t            time.sleep(3)\n\t            await asynchronous.switch_to_frame(*driver, iframe[0])\n\t            time.sleep(5)\n\t        group_elements = await asynchronous.find_elements(*driver, CSS_SELECTOR, 'div [class=\"opening\"]')\n\t        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n\t        result = []\n\t        for elem in group_elements:\n\t            link_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, 'a')\n", "            location_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, 'span')\n\t            job_url = await asynchronous.get_attribute(*driver, link_elem, \"href\")\n\t            job_name = await asynchronous.get_text(*driver, link_elem)\n\t            location = await asynchronous.get_text(*driver, location_elem)\n\t            cleaned_location = location.replace('\\n', ', ')\n\t            job = {\n\t                \"company\": company,\n\t                \"title\": job_name,\n\t                \"location\": clean_location(cleaned_location),\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n", "            }\n\t            result.append(job)\n\t        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n"]}
{"filename": "src/scrape_ripple.py", "chunked_list": ["from selenium.webdriver.common.by import By\n\tfrom src.scrape_it import ScrapeIt, write_jobs\n\tclass ScrapeRipple(ScrapeIt):\n\t    name = 'RIPPLE'\n\t    def getJobs(self, driver, web_page, company='ripple') -> []:\n\t        print(f'[{self.name}] Scrap page: {web_page}')\n\t        driver.get(web_page)\n\t        # use reverse strategy from a link to a title\n\t        group_elements = driver.find_elements(By.XPATH, '//div/a[contains(@class, \"body3\")]')\n\t        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n", "        result = []\n\t        for elem in group_elements:\n\t            link_elem = elem\n\t            job_name_elem = elem.find_element(By.XPATH, './../../../div[contains(@class, \"heading3\")]')\n\t            location_elem = elem\n\t            job_url = link_elem.get_attribute('href')\n\t            job_name = job_name_elem.text\n\t            location = location_elem.text\n\t            job = {\n\t                \"company\": company,\n", "                \"title\": job_name,\n\t                \"location\": location,\n\t                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n\t            }\n\t            result.append(job)\n\t        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n\t        write_jobs(result)\n\t        return result\n"]}
