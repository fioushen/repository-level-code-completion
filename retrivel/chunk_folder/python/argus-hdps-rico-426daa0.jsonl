{"filename": "main.py", "chunked_list": ["import datetime\n\tfrom fastapi import FastAPI\n\tfrom pydantic import BaseModel\n\tclass Gracie(BaseModel):\n\t    name: str\n\t    hello_utc: datetime.datetime\n\tapp = FastAPI()\n\t@app.get(\"/goodnight\", response_model=Gracie)\n\tasync def goodnight() -> Gracie:\n\t    goodnight = Gracie(name=\"Gracie\", hello_utc=datetime.datetime.now())\n", "    return goodnight\n"]}
{"filename": "tests/test_rico.py", "chunked_list": ["from rico import __version__\n\tdef test_version() -> None:\n\t    assert __version__ == \"0.1.0\"\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# For the full list of built-in configuration values, see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\tproject = \"Rico\"\n\tcopyright = \"2023, Hank Corbett\"\n\tauthor = \"Hank Corbett\"\n\trelease = \"0.1\"\n", "# -- General configuration ---------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\textensions = [\n\t    \"sphinx.ext.todo\",\n\t    \"sphinx.ext.viewcode\",\n\t    \"sphinx.ext.autodoc\",\n\t    \"sphinx.ext.napoleon\",\n\t    \"sphinx_click\",\n\t    \"sphinxcontrib.autodoc_pydantic\",\n\t]\n", "templates_path = [\"_templates\"]\n\texclude_patterns = []\n\t# -- Options for HTML output -------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\thtml_theme = \"furo\"\n\thtml_static_path = [\"_static\"]\n\thtml_theme_options = {\n\t    \"light_logo\": \"argus_logo_light.png\",\n\t    \"dark_logo\": \"argus_logo_dark.png\",\n\t    \"announcement\": \"<em>Important</em>: This package is currently under construction! Contact us for more info.\",\n", "}\n\thtml_title = \"Rico Docs\"\n\tautodoc_pydantic_model_show_json = True\n\tautodoc_pydantic_settings_show_json = False\n"]}
{"filename": "src/argus_rico/images.py", "chunked_list": ["__all__ = [\"images_containing\", \"get_image_meta\", \"EVRImageProducer\", \"EVRImageLoader\"]\n\timport datetime\n\timport glob\n\timport os\n\tfrom typing import Any, Dict, Union\n\timport astropy.io.fits as fits\n\timport pandas as pd\n\timport pymongoarrow.monkey\n\tfrom pymongo import MongoClient\n\tfrom pymongoarrow.api import Schema\n", "from . import config, models\n\tfrom .models import fitspath_to_constructor\n\tfrom .producer import Producer\n\tpymongoarrow.monkey.patch_all()\n\tdef images_containing(\n\t    ra: float, dec: float, date_start: datetime.datetime, date_end: datetime.datetime\n\t) -> pd.DataFrame:\n\t    \"\"\"\n\t    Retrieve images containing a given position within a specified time range.\n\t    Args:\n", "        ra (float): Right ascension (ICRS) of the target position.\n\t        dec (float): Declination (ICRS) of the target position.\n\t        date_start (datetime.datetime): Start date of the time range.\n\t        date_end (datetime.datetime): End date of the time range.\n\t    Returns:\n\t        pd.DataFrame: A DataFrame containing the retrieved images.\n\t    \"\"\"\n\t    client = MongoClient(config.MONGODB_URI)\n\t    collection = client[config.MONGO_DBNAME].evr_images\n\t    images = collection.find_pandas_all(\n", "        {\n\t            \"$and\": [{\"rawpath\": {\"$ne\": None}}, {\"wcspath\": {\"$ne\": None}}],\n\t            \"footprint\": {\n\t                \"$geoIntersects\": {\n\t                    \"$geometry\": {\"type\": \"Point\", \"coordinates\": [ra - 180.0, dec]}\n\t                }\n\t            },\n\t            \"obstime\": {\"$gte\": date_start, \"$lte\": date_end},\n\t            \"image_type\": \"object\",\n\t        },\n", "        schema=Schema(\n\t            {\n\t                \"obstime\": datetime.datetime,\n\t                \"image_type\": str,\n\t                \"camera\": str,\n\t                \"rawpath\": str,\n\t                \"wcspath\": str,\n\t            }\n\t        ),\n\t    )\n", "    return images\n\tdef get_image_meta(\n\t    path: str,\n\t) -> Dict[str, Any]:\n\t    \"\"\"\n\t    Retrieve image metadata by filename.\n\t    Args:\n\t        path (str): Filename for the image\n\t    Returns:\n\t        Dict: Dictionary of image metadata\n", "    \"\"\"\n\t    basename = os.path.basename(path).split(\".\")[0]\n\t    client = MongoClient(config.MONGODB_URI)\n\t    collection = client[config.MONGO_DBNAME].evr_images\n\t    image = collection.find_one({\"basename\": basename})\n\t    return image\n\tclass EVRImageProducer(Producer):\n\t    \"\"\"\n\t    A Kafka producer for sending EVR images.\n\t    Args:\n", "        None\n\t    Attributes:\n\t        host (str): The Kafka host address.\n\t        port (int): The Kafka port number.\n\t    \"\"\"\n\t    def __init__(self) -> None:\n\t        \"\"\"\n\t        Initialize the EVRImageProducer.\n\t        Args:\n\t            None\n", "        Returns:\n\t            None\n\t        \"\"\"\n\t        super().__init__(\n\t            host=config.KAFKA_ADDR,\n\t            port=config.KAFKA_PORT,\n\t        )\n\t    def send_image(self, image: Union[str, fits.HDUList]) -> None:\n\t        \"\"\"\n\t        Send an image to the EVR image topic.\n", "        Args:\n\t            image (Union[str, fits.HDUList]): The image to be sent. It can be either a string representing the\n\t                path to a FITS file or a `fits.HDUList` object.\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        image_dict = fitspath_to_constructor(image)\n\t        self.send_json(image_dict, config.EVR_IMAGE_TOPIC)\n\tclass EVRImageLoader:\n\t    \"\"\"Class for loading EVRImage data into a MongoDB collection.\"\"\"\n", "    def __init__(self, create_client: bool = True) -> None:\n\t        \"\"\"\n\t        Initialize EVRImageLoader.\n\t        Args:\n\t            create_client (bool): Whether to create a MongoDB client. Default is True.\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        self.client: MongoClient[Dict[str, Any]]\n\t        self.client_loaded: bool = create_client\n", "        if create_client:\n\t            self.client = MongoClient(config.MONGODB_URI, uuidRepresentation=\"standard\")\n\t    def load_fits(self, path: str) -> None:\n\t        \"\"\"\n\t        Load FITS file data into the MongoDB collection.\n\t        Args:\n\t            path (str): Path to the FITS file.\n\t        Returns:\n\t            None\n\t        \"\"\"\n", "        img_coll = self.client[config.MONGO_DBNAME].evr_images\n\t        db_img = img_coll.find_one({\"basename\": os.path.basename(path).split(\".\")[0]})\n\t        if db_img is None:\n\t            img_new = models.EVRImage.from_fits(path)\n\t            _ = img_coll.insert_one(img_new.dict())\n\t        else:\n\t            img_existing = models.EVRImageUpdate.from_fits(path)\n\t            _ = img_coll.update_one(\n\t                {\"_id\": db_img[\"_id\"]}, {\"$set\": img_existing.dict(exclude_unset=True)}\n\t            )\n", "    def load_directory(self, dirname: str) -> None:\n\t        \"\"\"\n\t        Load all FITS files from a directory into the MongoDB collection.\n\t        Args:\n\t            dirname (str): Directory path containing FITS files.\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        if not self.client_loaded:\n\t            self.client = MongoClient(config.MONGODB_URI, uuidRepresentation=\"standard\")\n", "        images = glob.glob(os.path.join(dirname, \"*.fits\"))\n\t        for image in images:\n\t            self.load_fits(image)\n"]}
{"filename": "src/argus_rico/slack.py", "chunked_list": ["__all__ = [\"slack_app\", \"slack_app_starter\"]\n\timport datetime\n\timport os\n\timport time\n\tfrom typing import Any, Callable, Dict\n\timport slack_sdk as sk\n\tfrom sanitize_filename import sanitize\n\tfrom slack_bolt import App\n\tfrom . import config, get_logger\n\tfrom . import images as rimages\n", "from .efte import EFTERunner\n\tlog = get_logger(\"slack_bot\")\n\tif config.SLACK_BOT_TOKEN is not None:\n\t    slack_app = App(\n\t        token=config.SLACK_BOT_TOKEN,\n\t        signing_secret=config.SLACK_SIGNING_SECRET,\n\t    )\n\t@slack_app.event(\"message\")\n\tdef handle_message_events(body: Dict[str, Any]) -> None:\n\t    \"\"\"Handle message events in the Slack app.\n", "    Args:\n\t        body (Dict[str, Any]): The payload containing the event data.\n\t    Returns:\n\t        None.\n\t    \"\"\"\n\t    pass\n\t@slack_app.message(\"hi Rico\")\n\tdef message_hello(message: Dict[str, Any], say: Callable[..., None]) -> None:\n\t    \"\"\"Handle the \"hi Rico\" message event in the Slack app.\n\t    Args:\n", "        message (Dict[str, Any]): The incoming message data.\n\t        say (Callable[..., None]): Function to send a message as a response.\n\t    Returns:\n\t        None.\n\t    \"\"\"\n\t    say(f\"Hey there <@{message['user']}>!\", thread_ts=message[\"ts\"])\n\t@slack_app.event(\"reaction_added\")\n\tdef telescope_trigger(\n\t    body: Dict[str, Any], ack: Callable[..., None], say: Callable[..., None]\n\t) -> None:\n", "    \"\"\"\n\t    Triggered when a reaction is added in the Slack app.\n\t    Args:\n\t        body (dict): The payload containing the event data.\n\t        ack (function): Acknowledgment function to confirm the event was received.\n\t        say (function): Function to send a message as a response.\n\t    Returns:\n\t        None.\n\t    \"\"\"\n\t    # Acknowledge the action\n", "    ack()\n\t    if \"telescope\" in body[\"event\"][\"reaction\"]:\n\t        event = body[\"event\"]\n\t        thread_ts = event[\"item\"][\"ts\"]\n\t        say(\n\t            f\"<@{event['user']}> initiated a follow-up request.\",\n\t            thread_ts=thread_ts,\n\t        )\n\t@slack_app.shortcut(\"rico_lightcurve_req\")\n\tdef open_lcv_modal(ack, shortcut, client):\n", "    \"\"\"Open a modal for requesting a lightcurve in the Slack app.\n\t    Args:\n\t        ack (Callable[..., None]): Acknowledgment function to confirm the shortcut request.\n\t        shortcut (Dict[str, Any]): The shortcut data.\n\t        client: The Slack WebClient instance.\n\t    Returns:\n\t        None.\n\t    \"\"\"\n\t    # Acknowledge the shortcut request\n\t    ack()\n", "    # Call the views_open method using the built-in WebClient\n\t    client.views_open(\n\t        trigger_id=shortcut[\"trigger_id\"],\n\t        # A simple view payload for a modal\n\t        view={\n\t            \"type\": \"modal\",\n\t            \"callback_id\": \"modal-lcv-req\",\n\t            \"title\": {\"type\": \"plain_text\", \"text\": \"Forced Photometry\", \"emoji\": True},\n\t            \"submit\": {\"type\": \"plain_text\", \"text\": \"Submit\", \"emoji\": True},\n\t            \"close\": {\"type\": \"plain_text\", \"text\": \"Cancel\", \"emoji\": True},\n", "            \"blocks\": [\n\t                {\n\t                    \"type\": \"input\",\n\t                    \"block_id\": \"target_name\",\n\t                    \"element\": {\n\t                        \"type\": \"plain_text_input\",\n\t                        \"action_id\": \"title\",\n\t                        \"placeholder\": {\"type\": \"plain_text\", \"text\": \"TRAPPIST-1\"},\n\t                    },\n\t                    \"label\": {\"type\": \"plain_text\", \"text\": \"Target Name\"},\n", "                },\n\t                {\n\t                    \"type\": \"actions\",\n\t                    \"block_id\": \"time_range\",\n\t                    \"elements\": [\n\t                        {\n\t                            \"type\": \"datepicker\",\n\t                            \"initial_date\": \"2015-07-01\",\n\t                            \"placeholder\": {\n\t                                \"type\": \"plain_text\",\n", "                                \"text\": \"Select a date\",\n\t                                \"emoji\": True,\n\t                            },\n\t                            \"action_id\": \"start_date\",\n\t                        },\n\t                        {\n\t                            \"type\": \"datepicker\",\n\t                            \"initial_date\": \"2030-01-01\",\n\t                            \"placeholder\": {\n\t                                \"type\": \"plain_text\",\n", "                                \"text\": \"Select a date\",\n\t                                \"emoji\": True,\n\t                            },\n\t                            \"action_id\": \"end_date\",\n\t                        },\n\t                    ],\n\t                },\n\t                {\n\t                    \"type\": \"input\",\n\t                    \"block_id\": \"right_ascension\",\n", "                    \"element\": {\n\t                        \"type\": \"number_input\",\n\t                        \"is_decimal_allowed\": True,\n\t                        \"action_id\": \"number_input-action\",\n\t                    },\n\t                    \"label\": {\n\t                        \"type\": \"plain_text\",\n\t                        \"text\": \"Right Ascension\",\n\t                        \"emoji\": True,\n\t                    },\n", "                },\n\t                {\n\t                    \"type\": \"input\",\n\t                    \"block_id\": \"declination\",\n\t                    \"element\": {\n\t                        \"type\": \"number_input\",\n\t                        \"is_decimal_allowed\": True,\n\t                        \"action_id\": \"number_input-action\",\n\t                    },\n\t                    \"label\": {\n", "                        \"type\": \"plain_text\",\n\t                        \"text\": \"Declination\",\n\t                        \"emoji\": True,\n\t                    },\n\t                },\n\t            ],\n\t        },\n\t    )\n\t@slack_app.view(\"modal-lcv-req\")\n\tdef handle_submission(\n", "    ack: Callable[..., None], body: Dict[str, Any], client: sk.WebClient\n\t) -> None:\n\t    \"\"\"Handle the submission of the lightcurve request modal in the Slack app.\n\t    Args:\n\t        ack (Callable[..., None]): Acknowledgment function to confirm the submission.\n\t        body (Dict[str, Any]): The payload containing the submission data.\n\t        client: The Slack WebClient instance.\n\t    Returns:\n\t        None.\n\t    \"\"\"\n", "    values = body[\"view\"][\"state\"][\"values\"]\n\t    user = body[\"user\"][\"id\"]\n\t    print(values)\n\t    target_name = sanitize(values[\"target_name\"][\"title\"][\"value\"])\n\t    start_date = values[\"time_range\"][\"start_date\"][\"selected_date\"]\n\t    end_date = values[\"time_range\"][\"end_date\"][\"selected_date\"]\n\t    right_ascension = float(values[\"right_ascension\"][\"number_input-action\"][\"value\"])\n\t    declination = float(values[\"declination\"][\"number_input-action\"][\"value\"])\n\t    errors = {}\n\t    if (right_ascension < 0) or (right_ascension > 360):\n", "        errors[\"right_ascension\"] = \"Right ascension out of range.\"\n\t    if (declination < -90) or (declination > 90):\n\t        errors[\"declination\"] = \"Declination out of range.\"\n\t    if len(errors) > 0:\n\t        ack(response_action=\"errors\", errors=errors)\n\t        return\n\t    # Acknowledge the view_submission request and close the modal\n\t    ack()\n\t    # Do whatever you want with the input data - here we're querying the DB and\n\t    # then passing off the lcv gen to a background worker\n", "    # TODO: probably can refactor this to only hit the DB once, but what's 15\n\t    # seconds out of 5 hours?\n\t    nimages = len(\n\t        rimages.images_containing(\n\t            right_ascension,\n\t            declination,\n\t            datetime.datetime.strptime(start_date, \"%Y-%m-%d\"),\n\t            datetime.datetime.strptime(end_date, \"%Y-%m-%d\"),\n\t        )\n\t    )\n", "    uncompleteable = False\n\t    # Message to send user\n\t    msg = \"\"\n\t    try:\n\t        if nimages >= 1:\n\t            msg = f\"Hi <@{user}>! I have received your request to produce the following lightcurve:\\n*{target_name}* at RA: {right_ascension}, Dec: {declination}.\\nFrom {start_date} to {end_date}.\\n\\n *Please note that I don't know how long this will take! Your table will be uploaded here ASAP.*\"\n\t        else:\n\t            msg = f\"Hi <@{user}>! I received your forced photometry request, but didn't find any images.\"\n\t            uncompleteable = True\n\t    except Exception as e:\n", "        # Handle error\n\t        msg = f\"There was an error with your submission: {e}\"\n\t    #\n\t    # Message the user\n\t    try:\n\t        response = client.chat_postMessage(channel=user, text=msg)\n\t    except Exception as e:\n\t        print(f\"Failed to post a message {e}\")\n\t        return\n\t    if uncompleteable:\n", "        return\n\t    dm_channel = response[\"channel\"]\n\t    print(\n\t        f\"efte -n 72 autophot --mindate {start_date} --maxdate {end_date} -o {target_name} {right_ascension} -- {declination}\"\n\t    )\n\t    er = EFTERunner()\n\t    er.run(\n\t        f\"efte -n 72 autophot --mindate {start_date} --maxdate {end_date} -o {target_name} {right_ascension} -- {declination}\"\n\t    )\n\t    output = (\n", "        target_name\n\t        + f\"_{right_ascension:.2f}_{declination:.2f}\".replace(\".\", \"d\").replace(\n\t            \"-\", \"m\"\n\t        )\n\t        + \".fits\"\n\t    )\n\t    while not os.path.isfile(output):\n\t        time.sleep(60)\n\t    client.files_upload_v2(\n\t        file=f\"./{output}\",\n", "        channel=dm_channel,\n\t        title=output,\n\t        initial_comment=\"Your lightcurve is ready!\",\n\t    )\n\tdef slack_app_starter() -> None:\n\t    log.info(\"Slack integration starting up...\")\n\t    slack_app.start(port=int(config.SLACK_PORT))\n"]}
{"filename": "src/argus_rico/producer.py", "chunked_list": ["from typing import Any, Dict, Optional, Union\n\tfrom uuid import uuid4\n\timport confluent_kafka as ck\n\timport orjson\n\tfrom . import get_logger\n\tclass Producer:\n\t    def __init__(\n\t        self,\n\t        host: str,\n\t        port: Union[int, str],\n", "    ) -> None:\n\t        \"\"\"\n\t        Initialize the Producer instance.\n\t        Args:\n\t            host (str): The hostname or IP address of the Kafka broker.\n\t            port (Union[int, str]): The port number of the Kafka broker.\n\t            topic (str): The name of the topic to produce messages to.\n\t        \"\"\"\n\t        self.p = ck.Producer({\"bootstrap.servers\": f\"{host}:{port}\"})\n\t        self.log = get_logger(__name__)\n", "    def delivery_report(self, err: Optional[ck.KafkaError], msg: ck.Message) -> None:\n\t        \"\"\"\n\t        Reports the failure or success of a message delivery.\n\t        Args:\n\t            err (Optional[ck.KafkaError]): The error that occurred, or None on success.\n\t            msg (ck.Message): The message that was produced or failed.\n\t        Note:\n\t            In the delivery report callback, the Message.key() and Message.value()\n\t            will be in binary format as encoded by any configured Serializers and\n\t            not the same object that was passed to produce().\n", "            If you wish to pass the original object(s) for key and value to the delivery\n\t            report callback, we recommend using a bound callback or lambda where you pass\n\t            the objects along.\n\t        \"\"\"\n\t        if err is not None:\n\t            self.log.error(f\"Delivery failed for User record {msg.key()}: {err}\")\n\t            return\n\t        self.log.info(\n\t            f\"Record {msg.key()} successfully produced to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\"\n\t        )\n", "    def send_json(self, message: Dict[Any, Any], topic: str) -> None:\n\t        \"\"\"\n\t        Send a JSON message to the Kafka topic.\n\t        Args:\n\t            message (dict): The message to be sent, represented as a dictionary.\n\t            topic (str): The name of the topic to send the message to.\n\t        \"\"\"\n\t        self.p.produce(\n\t            topic=topic,\n\t            key=str(uuid4()),\n", "            value=orjson.dumps(message),\n\t            on_delivery=self.delivery_report,\n\t        )\n\t        self.p.flush()\n\t    def send_binary(self, payload: bytes, topic: str) -> None:\n\t        \"\"\"\n\t        Send a binary payload to the Kafka topic.\n\t        Args:\n\t            payload (bytes): The payload to be sent, represented as a bytestring.\n\t            topic (str): The name of the topic to send the payload to.\n", "        \"\"\"\n\t        self.p.produce(\n\t            topic=topic,\n\t            key=str(uuid4()),\n\t            value=payload,\n\t            on_delivery=self.delivery_report,\n\t        )\n\t        self.p.flush()\n"]}
{"filename": "src/argus_rico/__init__.py", "chunked_list": ["\"\"\"Top-level package for Evryscope-Argus Transient Reporter.\"\"\"\n\t__author__ = \"\"\"Hank Corbett\"\"\"\n\t__email__ = \"htc@unc.edu\"\n\t__all__ = [\"get_logger\", \"config\"]\n\timport importlib.metadata\n\timport logging\n\timport os\n\tfrom dotenv import load_dotenv\n\t__version__ = importlib.metadata.version(\"argus-rico\")\n\tbasedir = os.path.abspath(os.path.dirname(__file__))\n", "if os.path.isfile(os.path.join(os.path.expanduser(\"~\"), \".ricoenv\")):\n\t    load_dotenv(os.path.join(os.path.expanduser(\"~\"), \".ricoenv\"))\n\telse:\n\t    load_dotenv(os.path.join(basedir, \".env\"))\n\tclass Config(object):\n\t    \"\"\"\n\t    Configuration class for managing application settings.\n\t    Attributes:\n\t        LOCAL_ADDR (str): The local address to bind the application to. Defaults to \"127.0.0.1\" if not provided in the environment.\n\t        KAFKA_ADDR (str): The address of the Kafka server. Defaults to \"152.2.38.172\" if not provided in the environment.\n", "        KAFKA_PORT (str): The port of the Kafka server. Defaults to \"9092\" if not provided in the environment.\n\t        HBEAT_TOPIC (str): The topic name for heartbeat messages in Kafka. Defaults to \"rico-hearbeat\" if not provided in the environment.\n\t    \"\"\"\n\t    SLACK_SIGNING_SECRET = os.environ.get(\"SLACK_SIGNING_SECRET\") or None\n\t    SLACK_BOT_TOKEN = os.environ.get(\"SLACK_BOT_TOKEN\") or None\n\t    SLACK_PORT = os.environ.get(\"SLACK_PORT\") or 3000\n\t    LOCAL_ADDR = os.environ.get(\"LOCAL_ADDR\") or \"127.0.0.1\"\n\t    KAFKA_ADDR = os.environ.get(\"KAFKA_ADDR\") or \"127.0.0.1\"\n\t    KAFKA_PORT = os.environ.get(\"KAFKA_PORT\") or \"9092\"\n\t    HBEAT_TOPIC = os.environ.get(\"HBEAT_TOPIC\") or \"rico.heartbeat\"\n", "    RAW_TOPIC_BASE = os.environ.get(\"RAW_TOPIC_BASE\") or \"rico.candidates.raw\"\n\t    EFTE_TOPIC_BASE = os.environ.get(\"EFTE_TOPIC_BASE\") or \"rico.efte.alerts\"\n\t    EVR_IMAGE_TOPIC = os.environ.get(\"EVR_IMAGE_TOPIC\") or \"rico.images.evr\"\n\t    MONGODB_URI = os.environ.get(\"MONGODB_URI\") or None\n\t    MONGO_DBNAME = os.environ.get(\"MONGO_DBNAME\") or \"hdps\"\n\t    EFTE_DB_ADDR = os.environ.get(\"EFTE_DB_ADDR\") or \"127.0.0.1\"\n\t    EFTE_DB_PORT = os.environ.get(\"EFTE_DB_PORT\") or 5432\n\t    EFTE_DB_NAME = os.environ.get(\"EFTE_DB_NAME\") or \"transients\"\n\t    EFTE_DB_USER = os.environ.get(\"EFTE_DB_USER\") or None\n\t    EFTE_DB_PASS = os.environ.get(\"EFTE_DB_PASS\") or None\n", "    WASABI_KEY_ID = os.environ.get(\"WASABI_KEY_ID\") or None\n\t    WASABI_SECRET_KEY = os.environ.get(\"WASABI_SECRET_KEY\") or None\n\t    WASABI_ENDPOINT = os.environ.get(\"WASABI_ENDPOINT\") or None\n\t    RICO_CACHE_DIR = os.environ.get(\"RICO_CACHE_DIR\") or None\n\t    if RICO_CACHE_DIR is None:\n\t        RICO_CACHE_DIR = os.path.join(os.path.expanduser(\"~\"), \".rico_cache\")\n\t    if not os.path.isdir(RICO_CACHE_DIR):\n\t        os.makedirs(RICO_CACHE_DIR)\n\tconfig = Config()\n\tdef get_logger(name: str) -> logging.Logger:\n", "    \"\"\"\n\t    Shortcut to grab a logger instance with the specified name.\n\t    Args:\n\t        name (str): The name of the logger.\n\t    Returns:\n\t        logging.Logger: A logger instance configured with the specified name.\n\t    \"\"\"\n\t    logger = logging.getLogger(name)\n\t    if not logger.handlers:\n\t        # Prevent logging from propagating to the root logger\n", "        logger.propagate = False\n\t        logger.setLevel((\"INFO\"))\n\t        console = logging.StreamHandler()\n\t        logger.addHandler(console)\n\t        formatter = logging.Formatter(\n\t            \"%(asctime)s — %(name)s — %(levelname)s — %(funcName)s:%(lineno)d — %(message)s\"\n\t        )\n\t        console.setFormatter(formatter)\n\t    return logger\n\t# For testing\n", "get_logger(__name__).addHandler(logging.StreamHandler())\n\t# Silenced\n\t# get_logger(__name__).addHandler(logging.NullHandler())\n"]}
{"filename": "src/argus_rico/utils.py", "chunked_list": ["__all__ = [\"Timer\"]\n\timport time\n\tclass Timer:\n\t    def __init__(self, log=None):\n\t        self.start_time = time.perf_counter()\n\t        self.last_ping = time.perf_counter()\n\t        self.log = log\n\t    def start(self):\n\t        \"\"\"Total runtime start.\"\"\"\n\t        self.start_time = time.perf_counter()\n", "    @property\n\t    def t(self):\n\t        \"\"\"Runtime relative to start.\"\"\"\n\t        return time.perf_counter() - self.start_time\n\t    @property\n\t    def t_since_last_ping(self):\n\t        \"\"\"Runtime relative to the last ping.\"\"\"\n\t        return time.perf_counter() - self.last_ping\n\t    def ping(self, message):\n\t        if self.log is not None:\n", "            self.log.info(\n\t                f\"@ {self.t * 1000:0.0f} ms ({self.t_since_last_ping * 1000:0.0f} ms delta): {message}\",\n\t            )\n\t        self.last_ping = time.perf_counter()\n"]}
{"filename": "src/argus_rico/heartbeat.py", "chunked_list": ["__all__ = [\"RicoHeartBeat\"]\n\timport datetime\n\timport threading\n\tfrom typing import Any, Callable, Dict, Optional, Tuple\n\tfrom . import config\n\tfrom .producer import Producer\n\tclass RepeatTimer(threading.Timer):\n\t    finished: threading.Event\n\t    interval: float\n\t    function: Callable[..., Any]\n", "    args: Tuple[Any, ...]\n\t    kwargs: Dict[str, Any]\n\t    def run(self) -> None:\n\t        while not self.finished.wait(self.interval):\n\t            assert self.function is not None\n\t            assert self.args is not None\n\t            assert self.kwargs is not None\n\t            self.function(*self.args, **self.kwargs)\n\tclass RicoHeartBeat(Producer):\n\t    def __init__(self) -> None:\n", "        \"\"\"\n\t        Initialize the RicoHeartBeat instance.\n\t        It inherits from the Producer class and configures the Kafka connection parameters\n\t        using values from the config dictionary.\n\t        The host, port, and topic are retrieved from the config dictionary\n\t        using the keys 'KAFKA_ADDR', 'KAFKA_PORT', and 'HBEAT_TOPIC' respectively.\n\t        \"\"\"\n\t        super().__init__(\n\t            host=config.KAFKA_ADDR,\n\t            port=config.KAFKA_PORT,\n", "        )\n\t        self.heartbeat_thread: Optional[RepeatTimer] = None\n\t    def send_heartbeat(self) -> None:\n\t        \"\"\"\n\t        Send a heartbeat message to the Kafka topic.\n\t        The message is a dictionary containing the current UTC time.\n\t        \"\"\"\n\t        self.send_json({\"utc\": datetime.datetime.utcnow()}, config.HBEAT_TOPIC)\n\t    def start(self, rate: int = 30) -> None:\n\t        \"\"\"\n", "        Start sending heartbeat messages periodically.\n\t        Args:\n\t            rate (int): The rate at which heartbeat messages should be sent, in seconds.\n\t                        Defaults to 30 seconds.\n\t        \"\"\"\n\t        self.heartbeat_thread = RepeatTimer(rate, self.send_heartbeat)\n\t        self.heartbeat_thread.start()\n\t    def stop(self) -> None:\n\t        \"\"\"\n\t        Stop sending heartbeat messages periodically.\n", "        \"\"\"\n\t        if self.heartbeat_thread is not None:\n\t            self.heartbeat_thread.cancel()\n"]}
{"filename": "src/argus_rico/s3.py", "chunked_list": ["__all__ = [\"S3AuthException\", \"S3Share\"]\n\t\"\"\"S3 Share module for handling Wasabi S3 interactions.\"\"\"\n\timport os\n\timport tarfile\n\tfrom typing import NoReturn\n\timport boto3\n\tfrom . import config\n\tclass S3AuthException(Exception):\n\t    \"\"\"Exception raised for S3 authentication errors.\"\"\"\n\t    pass\n", "class S3Share:\n\t    def __init__(self) -> None:\n\t        \"\"\"Initialize the S3Share class.\n\t        Raises:\n\t            S3AuthException: If Wasabi credentials are missing in ricoenv.\n\t        \"\"\"\n\t        if config.WASABI_KEY_ID is None or config.WASABI_SECRET_KEY is None:\n\t            raise S3AuthException(\"No Wasabi credentials in ricoenv\")\n\t        self.s3 = boto3.resource(\n\t            \"s3\",\n", "            aws_access_key_id=config.WASABI_KEY_ID,\n\t            aws_secret_access_key=config.WASABI_SECRET_KEY,\n\t            endpoint_url=config.WASABI_ENDPOINT,\n\t        )\n\t        self.vetnet_bucket = self.s3.Bucket(\"efte.vetnet\")\n\t        self.catalog_bucket = self.s3.Bucket(\"hera.catalogs\")\n\t        self.stamps_bucket = \"efte.stamps\"\n\t    def upload_stamp(self, id: str) -> str:\n\t        \"\"\"\n\t        Upload a stamp for to S3.\n", "        Args:\n\t            id (str): S3 key for the stamp (candidate ID)\n\t        Returns:\n\t            str: Pre-authenticated URL for the stamp file.\n\t        \"\"\"\n\t        url = self.s3.generate_presigned_url(\n\t            ClientMethod=\"get_object\",\n\t            Params={\"Bucket\": self.stamps_bucket, \"Key\": \"invoice.pdf\"},\n\t        )\n\t        return url\n", "    def download_vetnet(self) -> NoReturn:\n\t        \"\"\"Download Vetnet data from S3.\n\t        Downloads the hypersky_v7_v0.tar.gz file from the S3 bucket,\n\t        extracts its contents, and saves it in the cache directory.\n\t        Raises:\n\t            botocore.exceptions.ClientError: If the file download fails.\n\t        \"\"\"\n\t        cached_path = os.path.join(\n\t            os.path.expanduser(\"~\"), \".rico_cache\", \"hypersky_v7_v0.tar.gz\"\n\t        )\n", "        self.vetnet_bucket.download_file(\n\t            \"hypersky_v7_v0.tar.gz\",\n\t            cached_path,\n\t        )\n\t        outpath = os.path.dirname(cached_path)\n\t        file = tarfile.open(cached_path)\n\t        file.extractall(outpath)\n\t        file.close()\n\t    def download_atlas(self) -> NoReturn:\n\t        \"\"\"Download ATLAS-RefCat 2 data from S3.\n", "        Downloads the atlas_feathercat.tar.gz file from the S3 bucket,\n\t        extracts its contents, and saves it in the cache directory.\n\t        Raises:\n\t            botocore.exceptions.ClientError: If the file download fails.\n\t        \"\"\"\n\t        cached_path = os.path.join(config.RICO_CACHE_DIR, \"atlas_feathercat.tar.gz\")\n\t        self.catalog_bucket.download_file(\n\t            \"atlas_feathercat.tar.gz\",\n\t            cached_path,\n\t        )\n", "        outpath = os.path.dirname(cached_path)\n\t        file = tarfile.open(cached_path)\n\t        file.extractall(outpath)\n\t        file.close()\n"]}
{"filename": "src/argus_rico/consumer.py", "chunked_list": ["from typing import Union\n\timport confluent_kafka as ck\n\tfrom . import get_logger\n\tclass Consumer:\n\t    def __init__(\n\t        self,\n\t        host: str,\n\t        port: Union[int, str],\n\t        group_id: str,\n\t        topic: str,\n", "    ) -> None:\n\t        \"\"\"Initialize the Consumer class.\n\t        Args:\n\t            host (str): The Kafka broker host.\n\t            port (Union[int, str]): The Kafka broker port.\n\t            group_id (str): The Kafka consumer group ID.\n\t            topic (str): The Kafka topic to consume messages from.\n\t        \"\"\"\n\t        # Unlike producer, consumer is created on poll\n\t        self.config = {\n", "            \"bootstrap.servers\": f\"{host}:{port}\",\n\t            \"group.id\": group_id,\n\t            \"enable.auto.commit\": False,\n\t            \"auto.offset.reset\": \"latest\",\n\t        }\n\t        self.log = get_logger(__name__)\n\t        self.topic = topic\n\t        self.polling = False\n\t    def get_consumer(self) -> ck.Consumer:\n\t        \"\"\"Create and return a new Kafka consumer instance.\n", "        Returns:\n\t            ck.Consumer: A new Kafka consumer instance configured with the specified settings.\n\t        \"\"\"\n\t        return ck.Consumer(self.config)\n"]}
{"filename": "src/argus_rico/cli.py", "chunked_list": ["import sys\n\tfrom typing import Sequence\n\timport click\n\t@click.group()\n\tdef main() -> None:\n\t    \"\"\"Console scripts for rico.\"\"\"\n\t    pass\n\tclass Config(object):\n\t    \"\"\"Contains global options used for all pipeline actions.\"\"\"\n\t    def __init__(self) -> None:\n", "        self.verbose = False\n\t        self.ncpus = 72\n\tpass_config = click.make_pass_decorator(Config, ensure=True)\n\tif __name__ == \"__main__\":\n\t    sys.exit(main())  # pragma: no cover\n\t@click.command(\"slack\", short_help=\"Start Rico Slack server.\")\n\tdef slack() -> None:\n\t    \"\"\"Starts the Rico Slack server.\"\"\"\n\t    from .slack import slack_app_starter\n\t    slack_app_starter()\n", "@click.command(\"loadcats\", short_help=\"Start Rico monitor for EFTE catalogs.\")\n\t@click.argument(\"directory\", type=click.Path(exists=True))\n\tdef loadcats(directory: str) -> None:\n\t    \"\"\"Starts the Rico directory monitor.\"\"\"\n\t    from .efte.watchdog import EFTEWatcher\n\t    ew = EFTEWatcher(watch_path=directory)\n\t    ew.watch()\n\t@click.command(\"stream_json\", short_help=\"Record event stream to local JSON.\")\n\t@click.option(\n\t    \"--filter\",\n", "    \"-f\",\n\t    type=click.Path(exists=True, dir_okay=False),\n\t    default=None,\n\t    help=\"File containing filters to apply to alert stream\",\n\t)\n\t@click.option(\n\t    \"--outdir\",\n\t    \"-o\",\n\t    type=click.Path(file_okay=False),\n\t    default=\"rico_alerts\",\n", "    help=\"Output directory\",\n\t)\n\t@click.option(\"--group\", \"-g\", type=str, default=\"argus-spec\", help=\"Kafka group ID\")\n\tdef stream_json(filter: str, outdir: str, group: str) -> None:\n\t    \"\"\"Monitor the alert stream and record candidates to local disk in JSON\n\t    format. Optionally, apply a filter to the events before recording, based on\n\t    xmatch info.\"\"\"\n\t    import os\n\t    from .efte.stream import EFTEAlertReceiver\n\t    if not os.path.isdir(outdir):\n", "        os.makedirs(outdir)\n\t    ear = EFTEAlertReceiver(\n\t        output_path=outdir,\n\t        filter_path=filter,\n\t        group=group,\n\t    )\n\t    ear.poll_and_record()\n\t@click.command(\"index_images\", short_help=\"Index EVR images directly into MongoDB.\")\n\t@click.argument(\"directories\", nargs=-1, type=click.Path(exists=True))\n\t@pass_config\n", "def index_images(cli_config: Config, directories: Sequence[str]) -> None:\n\t    \"\"\"Indexes EVR images from the specified directories.\n\t    Args:\n\t        cli_config: Config object containing global options.\n\t        directories: Iterable of directories containing EVR images.\n\t    \"\"\"\n\t    import multiprocessing as mp\n\t    from . import EVRImageLoader\n\t    image_loader = EVRImageLoader(create_client=False)\n\t    pool = mp.Pool(cli_config.ncpus)\n", "    directories = sorted(directories)\n\t    n_directories = len(directories)\n\t    out = []\n\t    with click.progressbar(\n\t        pool.imap_unordered(image_loader.load_directory, directories),\n\t        label=\"Loading: \",\n\t        length=n_directories,\n\t    ) as pbar:\n\t        for _ in pbar:\n\t            out.append(_)\n", "    pool.close()\n\t    pool.join()\n\tmain.add_command(slack)\n\tmain.add_command(index_images)\n\tmain.add_command(loadcats)\n\tmain.add_command(stream_json)\n"]}
{"filename": "src/argus_rico/catalogs/__init__.py", "chunked_list": ["__all__ = [\n\t    \"ATLASRefcat2\",\n\t]\n\tfrom .atlas import ATLASRefcat2\n"]}
{"filename": "src/argus_rico/catalogs/atlas.py", "chunked_list": ["__all__ = [\"MissingDirectoryError\", \"ATLASRefcat2\"]\n\timport glob\n\timport os\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom typing import List, Tuple, Union\n\timport astropy.coordinates as crds\n\timport astropy.units as u\n\timport click\n\timport numpy as np\n\timport pandas as pd\n", "import pyarrow as pa\n\timport pyarrow.dataset as pds\n\timport pyarrow.feather as pf\n\timport pyarrow.parquet as pq\n\tfrom astropy_healpix import HEALPix\n\tfrom .. import config, s3\n\tclass MissingDirectoryError(Exception):\n\t    \"\"\"Exception raised when a directory is missing.\n\t    Attributes:\n\t        message (str): Explanation of the error.\n", "    \"\"\"\n\t    def __init__(self, message: str = \"Tree basepath is not set!\"):\n\t        self.message = message\n\t        super().__init__(self.message)\n\tdef haversine(\n\t    lon1: np.ndarray, lat1: np.ndarray, lon2: np.ndarray, lat2: np.ndarray\n\t) -> np.ndarray:\n\t    \"\"\"Calculate the great circle distance between two points on the earth.\n\t    Args:\n\t        lon1 (np.ndarray): Longitudes of the first points in decimal degrees.\n", "        lat1 (np.ndarray): Latitudes of the first points in decimal degrees.\n\t        lon2 (np.ndarray): Longitudes of the second points in decimal degrees.\n\t        lat2 (np.ndarray): Latitudes of the second points in decimal degrees.\n\t    Returns:\n\t        np.ndarray: The great circle distances in degrees.\n\t    \"\"\"\n\t    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\t    dlon = lon2 - lon1\n\t    dlat = lat2 - lat1\n\t    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n", "    c = 2 * np.arcsin(np.sqrt(a))\n\t    return np.rad2deg(c)\n\tclass ATLASRefcat2:\n\t    def __init__(self):\n\t        self.h4 = HEALPix(nside=4, order=\"nested\", frame=crds.ICRS())\n\t        self.h16 = HEALPix(nside=16, order=\"nested\", frame=crds.ICRS())\n\t        self.table: pa.Table = None\n\t        self.dataset: str = None\n\t        if not os.path.isdir(os.path.join(config.RICO_CACHE_DIR, \"atlas_refcat2\")):\n\t            if click.confirm(\n", "                \"ATLAS RefCat not found, do you want to download it (82 GB)?\"\n\t            ):\n\t                s3share = s3.S3Share()\n\t                s3share.download_atlas()\n\t            else:\n\t                raise MissingDirectoryError(\"ATLAS Refcat is not installed\")\n\t        self.dataset = os.path.join(config.RICO_CACHE_DIR, \"atlas_refcat2\")\n\t    def radial(\n\t        self,\n\t        ra: float,\n", "        dec: float,\n\t        radius: float,\n\t        min_g: float = 11.0,\n\t        max_g: float = 16.0,\n\t        return_area: bool = False,\n\t        grab_closest: bool = False,\n\t        as_pandas: bool = True,\n\t    ) -> Union[pd.DataFrame, pa.Table]:\n\t        \"\"\"Perform radial search on the dataset.\n\t        Args:\n", "            ra (float): Right ascension in degrees.\n\t            dec (float): Declination in degrees.\n\t            radius (float): Radius of the search in degrees.\n\t            min_g (float, optional): Minimum 'g' value for filtering. Defaults to 11.0.\n\t            max_g (float, optional): Maximum 'g' value for filtering. Defaults to 16.0.\n\t            return_area (bool, optional): Whether to return the area covered by the search.\n\t                Defaults to True.\n\t            grab_closest (bool, optional): Whether to return only the closest match.\n\t                Defaults to False.\n\t            as_pandas (bool, optional): Whether to return the result as a pandas DataFrame.\n", "                If False, the result is returned as a pyarrow Table. Defaults to True.\n\t        Returns:\n\t            Union[pd.DataFrame, pa.Table]: The filtered dataset within the specified radius.\n\t        \"\"\"\n\t        hpx_4 = self.h4.cone_search_lonlat(ra * u.deg, dec * u.deg, radius * u.deg)\n\t        hpx_16 = self.h16.cone_search_lonlat(ra * u.deg, dec * u.deg, radius * u.deg)\n\t        imfiles = []\n\t        for i4 in hpx_4:\n\t            files_4 = [\n\t                glob.glob(\n", "                    os.path.join(\n\t                        self.dataset, str(i4) + \"/\" + str(i) + \"/\" + \"*.feather\"\n\t                    )\n\t                )\n\t                for i in hpx_16\n\t            ]\n\t            imfiles += files_4\n\t        imfiles = [x[0] for x in imfiles if len(x) > 0]\n\t        if as_pandas:\n\t            with ThreadPoolExecutor() as threads:\n", "                t_res = threads.map(\n\t                    self._from_featherfile_pandas,\n\t                    zip(\n\t                        imfiles,\n\t                        [\n\t                            [min_g, max_g],\n\t                        ]\n\t                        * len(imfiles),\n\t                    ),\n\t                )\n", "            t = pd.concat(t_res)\n\t            separations = haversine(t[\"ra\"], t[\"dec\"], ra, dec)\n\t            t[\"separation\"] = separations\n\t            if grab_closest:\n\t                t = t.iloc[[np.argmin(separations)]]\n\t            else:\n\t                t = t[separations < radius]\n\t        else:\n\t            with ThreadPoolExecutor() as threads:\n\t                t_res = threads.map(\n", "                    self._from_featherfile,\n\t                    zip(\n\t                        imfiles,\n\t                        [\n\t                            [min_g, max_g],\n\t                        ]\n\t                        * len(imfiles),\n\t                    ),\n\t                )\n\t            t = pa.concat_tables(t_res)\n", "            separations = haversine(t[\"ra\"].to_numpy(), t[\"dec\"].to_numpy(), ra, dec)\n\t            t.append_column(\"separation\", [pa.array(separations)])\n\t            t = t.filter(separations < radius)\n\t        if return_area:\n\t            return t, np.pi * radius**2\n\t        else:\n\t            return t\n\t    @staticmethod\n\t    def _from_featherfile_pandas(packet: Tuple[str, List[float]]) -> pd.DataFrame:\n\t        \"\"\"Read a feather file and return the DataFrame after filtering.\n", "        Args:\n\t            packet (Tuple[str, List[float]]): A tuple containing the feather file path\n\t                and the range of 'g' values for filtering.\n\t        Returns:\n\t            pd.DataFrame: The filtered DataFrame.\n\t        \"\"\"\n\t        path, [min_g, max_g] = packet\n\t        t = pf.read_feather(path)\n\t        t = t[(t[\"g\"] < max_g) & (t[\"g\"] > min_g)]\n\t        return t\n", "    @staticmethod\n\t    def _from_featherfile(packet: Tuple[str, List[float]]) -> pa.Table:\n\t        \"\"\"Read a feather file and return the Table.\n\t        Args:\n\t            packet (Tuple[str, List[float]]): A tuple containing the feather file path\n\t                and dummy list.\n\t        Returns:\n\t            pa.Table: The Table read from the feather file.\n\t        \"\"\"\n\t        path, [_, _] = packet\n", "        t = pf.read_table(path)\n\t        return t\n\t    def to_parquet(self, outpath: str) -> None:\n\t        \"\"\"Write the Table to a parquet file.\n\t        Args:\n\t            outpath (str): The output path for the parquet file.\n\t        \"\"\"\n\t        pq.write_table(self.table, outpath)\n\t    def from_parquet(self, path: str) -> None:\n\t        \"\"\"Load a parquet file into the class.\n", "        Args:\n\t            path (str): The path to the parquet file.\n\t        Raises:\n\t            FileNotFoundError: If the parquet file at the given path is not found.\n\t        \"\"\"\n\t        try:\n\t            self.table = pq.read_table(path)\n\t        except FileNotFoundError:\n\t            raise FileNotFoundError(f\"Parquet file not found at path: {path}\")\n\t    def to_ds(self, outdir: str) -> None:\n", "        \"\"\"Write the Table to a dataset in feather format.\n\t        Args:\n\t            outdir (str): The output directory for the dataset.\n\t        \"\"\"\n\t        part = pds.partitioning(\n\t            pa.schema(\n\t                [\n\t                    (\"h4\", pa.int32()),\n\t                    (\"h16\", pa.int32()),\n\t                    # (\"h32\", pa.int32()),\n", "                    # (\"h64\", pa.int32()),\n\t                    # (\"h256\", pa.int32()),\n\t                ]\n\t            ),\n\t        )\n\t        pds.write_dataset(\n\t            self.table,\n\t            outdir,\n\t            format=\"feather\",\n\t            max_partitions=786432,\n", "            partitioning=part,\n\t            max_open_files=786432,\n\t        )\n\t    def to_segment_ds(self, outdir: str, nside_base: int) -> None:\n\t        \"\"\"Write the Table to a segmented dataset in feather format.\n\t        Args:\n\t            outdir (str): The output directory for the segmented dataset.\n\t            nside_base (int): The base nside value for segmentation.\n\t        \"\"\"\n\t        part = pds.partitioning(\n", "            pa.schema(\n\t                [\n\t                    (\"h4\", pa.int32()),\n\t                    (\"h16\", pa.int32()),\n\t                    (\"h64\", pa.int32()),\n\t                    (\"h256\", pa.int32()),\n\t                ]\n\t            ),\n\t        )\n\t        pds.write_dataset(\n", "            self.table,\n\t            outdir,\n\t            format=\"feather\",\n\t            max_partitions=786432,\n\t            partitioning=part,\n\t            max_open_files=786432,\n\t        )\n"]}
{"filename": "src/argus_rico/efte/vetnet.py", "chunked_list": ["__all__ = [\"VetNet\"]\n\t\"\"\"Wrapper for the MC EFTE Vetnet model.\"\"\"\n\timport os\n\tfrom typing import Tuple\n\tos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\timport astropy.visualization as viz  # noqa: E402\n\timport numpy as np  # noqa: E402\n\timport tensorflow as tf  # noqa: E402\n\timport tensorflow.keras.models as models  # noqa: E402\n\tfrom .. import s3  # noqa: E402\n", "class VetNet:\n\t    def __init__(self) -> None:\n\t        \"\"\"Initialize the Vetnet class.\n\t        Loads the pretrained model and sets class variables.\n\t        \"\"\"\n\t        try:\n\t            if not os.path.isdir(os.path.join(os.path.expanduser(\"~\"), \".rico_cache\")):\n\t                os.mkdir(os.path.join(os.path.expanduser(\"~\"), \".rico_cache\"))\n\t            self.model = models.load_model(\n\t                os.path.join(os.path.expanduser(\"~\"), \".rico_cache\", \"hypersky_v7_v0\")\n", "            )\n\t        except OSError:\n\t            s3share = s3.S3Share()\n\t            s3share.download_vetnet()\n\t            self.model = models.load_model(\n\t                os.path.join(os.path.expanduser(\"~\"), \".rico_cache\", \"hypersky_v7_v0\")\n\t            )\n\t        self.BATCH_SIZE: int  # The batch size for data processing\n\t        self.AUTOTUNE: int = tf.data.AUTOTUNE  # The buffer size for prefetching\n\t        self.zed = viz.ZScaleInterval()\n", "    def prep_ds(self, ds: tf.data.Dataset) -> tf.data.Dataset:\n\t        \"\"\"Preprocess the input dataset.\n\t        Batch the dataset and apply buffered prefetching.\n\t        Args:\n\t            ds (tf.data.Dataset): The input dataset.\n\t        Returns:\n\t            tf.data.Dataset: The preprocessed dataset.\n\t        \"\"\"\n\t        # Batch all datasets\n\t        ds = ds.batch(self.BATCH_SIZE)\n", "        # Use buffered prefetching on all datasets\n\t        return ds.prefetch(buffer_size=self.AUTOTUNE)\n\t    def _normalize_triplet(self, data: np.ndarray) -> np.ndarray:\n\t        \"\"\"Perform stamp normalization. Processes the reference, science, and\n\t        difference images separately.\n\t        Args:\n\t            data (np.ndarray): The input data for prediction.\n\t        Returns:\n\t            np.ndarray: Recursively normalized postage stamp cutout.\n\t        \"\"\"\n", "        for i in range(data.shape[-1]):\n\t            data[:, :, i] = self.zed(data[:, :, i])\n\t        return data\n\t    def _normalize_stamps(self, data: np.ndarray) -> np.ndarray:\n\t        \"\"\"Perform recursive stamp normalization. Processes the reference,\n\t        science, and difference images separately.\n\t        Args:\n\t            data (np.ndarray): The input data for prediction.\n\t        Returns:\n\t            np.ndarray: Recursively normalized postage stamp cutouts.\n", "        \"\"\"\n\t        for i in range(data.shape[0]):\n\t            data[i, :, :, :] = self._normalize_triplet(data[i, :, :, :])\n\t        return data\n\t    def mc_predict(\n\t        self, data: np.ndarray, n_posterior_draws: int\n\t    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n\t        \"\"\"Perform Monte Carlo prediction.\n\t        Args:\n\t            data (np.ndarray): The input data for prediction.\n", "            n_posterior_draws (int): The number of posterior draws.\n\t        Returns:\n\t            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: A tuple containing:\n\t            - mean_pred (np.ndarray): The mean prediction.\n\t            - low_pred (np.ndarray): The lower prediction bound.\n\t            - high_pred (np.ndarray): The upper prediction bound.\n\t            - confidence (np.ndarray): The confidence measure.\n\t        \"\"\"\n\t        data = self._normalize_stamps(data)\n\t        pred_dists = np.array(\n", "            [self.model(data, training=False) for _ in range(n_posterior_draws)]\n\t        )\n\t        pred_dists[pred_dists == 0] += 1e-5\n\t        pred_dists[pred_dists == 1] -= 1e-5\n\t        entropy = (-1 * pred_dists * np.log2(pred_dists)) - (\n\t            (1 - pred_dists) * np.log2(1 - pred_dists)\n\t        )\n\t        low_pred = np.rint(np.percentile(pred_dists, (50.0 - 95.4 / 2), axis=0))\n\t        high_pred = np.rint(np.percentile(pred_dists, (50.0 + 95.4 / 2), axis=0))\n\t        mean_pred = np.rint(np.mean(pred_dists, axis=0))\n", "        confidence = 1 - (1 / n_posterior_draws) * np.sum(entropy, axis=0)\n\t        return mean_pred, low_pred, high_pred, confidence\n"]}
{"filename": "src/argus_rico/efte/efte_runner.py", "chunked_list": ["\"\"\"(very thin) wrapper for running EFTE CLI utilities programmatically in a thread\"\"\"\n\timport subprocess\n\timport threading\n\tfrom typing import Optional\n\tclass EFTERunner(threading.Thread):\n\t    \"\"\"A class representing a thread that runs a command using subprocess.\"\"\"\n\t    def __init__(self) -> None:\n\t        \"\"\"Initialize the EFTERunner thread.\"\"\"\n\t        self.stdout: Optional[bytes] = None\n\t        self.stderr: Optional[bytes] = None\n", "        threading.Thread.__init__(self)\n\t    def run(self, command: str) -> None:\n\t        \"\"\"Execute the specified command in a separate thread.\n\t        Args:\n\t            command (str): The command to be executed.\n\t        Returns:\n\t            None.\n\t        \"\"\"\n\t        p = subprocess.Popen(\n\t            command.split(), shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n", "        )\n\t        self.stdout, self.stderr = p.communicate()\n"]}
{"filename": "src/argus_rico/efte/processor.py", "chunked_list": ["\"\"\"Ray-parallelizable EFTE catalog reducer.\"\"\"\n\timport os\n\timport astropy.table as tbl\n\timport ray\n\tfrom .. import catalogs, get_logger, utils\n\tfrom .stream import EFTEAlertStreamer\n\tfrom .vetnet import VetNet\n\t@ray.remote\n\tclass EFTECatalogProcessor:\n\t    def __init__(self):\n", "        \"\"\"Initialize the EFTECatalogProcessor class.\"\"\"\n\t        self.vetnet = VetNet()\n\t        self.atlas = catalogs.ATLASRefcat2()\n\t        self.producer = EFTEAlertStreamer()\n\t        self.log = get_logger(__name__)\n\t    def process(self, filepath: str) -> None:\n\t        \"\"\"Perform vetting and crossmatching for the given FITS table.\n\t        Args:\n\t            filepath (str): The path to the FITS table.\n\t        Returns:\n", "            Table: Returns the FITS table with normalized stamps, scores.\n\t        \"\"\"\n\t        clock = utils.Timer(log=self.log)\n\t        name = os.path.basename(filepath)\n\t        table: tbl.Table = tbl.Table.read(filepath, format=\"fits\")\n\t        clock.ping(f\"Read {len(table)} candidates from {name}\")\n\t        stamps = table[\"stamp\"].data\n\t        mean_pred, _, _, confidence = self.vetnet.mc_predict(stamps, 10)\n\t        clock.ping(f\"Vetted candidates from {name}\")\n\t        table[\"vetnet_score\"] = confidence[:, 0]\n", "        table = table[mean_pred[:, 0] > 0.5]\n\t        table = table[table[\"vetnet_score\"] > 0.4]  # fairly arbitrary...\n\t        clock.ping(f\"Reporting {len(table)} candidates in {name}\")\n\t        if len(table) == 0:\n\t            return\n\t        crossmatches = []\n\t        for r in table:\n\t            phot = self.atlas.radial(\n\t                r[\"ra\"], r[\"dec\"], 0.01, max_g=25, return_area=False\n\t            )\n", "            phot = phot.sort_values(by=\"separation\")\n\t            crossmatches.append(phot.to_dict(orient=\"records\"))\n\t        clock.ping(f\"Xmatched {name} with ATLAS Refcat\")\n\t        self.producer.push_alert(table, crossmatches)\n"]}
{"filename": "src/argus_rico/efte/db.py", "chunked_list": ["__all__ = [\"insert_efte_candidates\"]\n\timport datetime\n\timport os\n\timport astropy.table as tbl\n\timport astropy.time as atime\n\timport astropy.visualization as viz\n\timport numpy as np\n\timport psycopg\n\timport skimage as sk\n\tfrom . import config, get_logger\n", "log = get_logger(\"__name__\")\n\tdef insert_efte_candidates(catalog):\n\t    if len(catalog) == 0:\n\t        return False\n\t    conn = psycopg.connect(\n\t        f\"\"\"host='{config.EFTE_DB_ADDR}'\n\t            port='{config.EFTE_DB_PORT}'\n\t            dbname='{config.EFTE_DB_NAME}'\n\t            user='{config.EFTE_DB_USER}'\n\t            password='{config.EFTE_DB_PASS}'\n", "        \"\"\",\n\t        # cursor_factory=psycopg.ClientCursor,\n\t    )\n\t    c = conn.cursor()\n\t    now = datetime.datetime.utcnow()\n\t    imgepoch = atime.Time(catalog.meta[\"EPOCH\"], format=\"isot\", scale=\"utc\")\n\t    epoch = [imgepoch.to_datetime()] * len(catalog)\n\t    lag = [np.abs((now - imgepoch.to_datetime()).total_seconds())] * len(catalog)\n\t    camera = [catalog.meta[\"CCD\"]] * len(catalog)\n\t    ratchet = [catalog.meta[\"ratchetnum\"]] * len(catalog)\n", "    filename = [catalog.meta[\"FILENAME\"]] * len(catalog)\n\t    zed = viz.ZScaleInterval()\n\t    stamps = [\n\t        sk.img_as_ubyte(zed(x)).astype(np.uint8).tobytes()\n\t        for x in catalog[\"stamp\"].data\n\t    ]\n\t    if \"bstamps\" not in catalog.colnames:\n\t        catalog.add_column(tbl.Column(stamps, name=\"bstamps\"))\n\t    ra = catalog[\"ra\"].data.astype(float)\n\t    dec = catalog[\"dec\"].data.astype(float)\n", "    x = catalog[\"xcentroid\"].data.astype(float)\n\t    y = catalog[\"ycentroid\"].data.astype(float)\n\t    sign_ratio = catalog[\"sign_ratio\"].data.astype(float)\n\t    srcsnr = catalog[\"srcsnr\"].data.astype(float)\n\t    gmag = catalog[\"gmag\"].data.astype(float)\n\t    difsnr = catalog[\"difsnr\"].data.astype(float)\n\t    vetscore = catalog[\"vetnet\"].data.astype(float)\n\t    stamps = catalog[\"bstamps\"].data\n\t    site = [\"mlo\"] * len(stamps)\n\t    rows = tuple(zip(epoch, stamps))\n", "    try:\n\t        q = \"\"\"\n\t               INSERT INTO stamps (epoch, cutout)\n\t               VALUES (%s, %s)\n\t               RETURNING id\"\"\"\n\t        c.executemany(q, rows, returning=True)\n\t        stamp_ids = []\n\t        while True:\n\t            stamp_ids.append(c.fetchone()[0])\n\t            if not c.nextset():\n", "                break\n\t    except Exception as e:\n\t        log.error(\n\t            \"Stamp insert failed for {}  aborting: {} {}\".format(\n\t                os.path.basename(catalog.meta[\"FILENAME\"]), q, e\n\t            )\n\t        )\n\t        c.close()\n\t        conn.close()\n\t        return False\n", "    rows = list(\n\t        zip(\n\t            epoch,\n\t            ra,\n\t            dec,\n\t            x,\n\t            y,\n\t            sign_ratio,\n\t            vetscore,\n\t            srcsnr,\n", "            difsnr,\n\t            gmag,\n\t            camera,\n\t            ratchet,\n\t            filename,\n\t            lag,\n\t            site,\n\t            stamp_ids,\n\t        )\n\t    )\n", "    q = \"\"\"\n\t            INSERT INTO  candidates(\n\t                               epoch,\n\t                               raj2000,\n\t                               decj2000,\n\t                               x_position,\n\t                               y_position,\n\t                               sign_ratio,\n\t                               vetnet_score,\n\t                               srcsnr,\n", "                               difsnr,\n\t                               mag_g,\n\t                               camera,\n\t                               ratchet,\n\t                               detection_image,\n\t                               detection_lag,\n\t                               detected_from,\n\t                               stamp_id)\n\t            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n\t            \"\"\"\n", "    c.executemany(q, rows)\n\t    c.close()\n\t    conn.commit()\n\t    conn.close()\n"]}
{"filename": "src/argus_rico/efte/__init__.py", "chunked_list": ["__all__ = [\n\t    \"EFTERunner\",\n\t    \"EFTECatalogProcessor\",\n\t    \"EFTECatalogHandler\",\n\t    \"EFTEWatcher\",\n\t    \"VetNet\",\n\t    \"EFTEAlertStreamer\",\n\t    \"EFTEAlertReceiver\",\n\t]\n\tfrom .efte_runner import EFTERunner\n", "from .processor import EFTECatalogProcessor\n\tfrom .stream import EFTEAlertReceiver, EFTEAlertStreamer\n\tfrom .vetnet import VetNet\n\tfrom .watchdog import EFTECatalogHandler, EFTEWatcher\n"]}
{"filename": "src/argus_rico/efte/stream.py", "chunked_list": ["__all__ = [\"EFTEAlertReceiver\", \"EFTEAlertStreamer\"]\n\timport base64\n\timport io\n\timport os\n\tfrom typing import Any, Dict, List, Optional\n\tfrom uuid import uuid4\n\timport astropy.table as tbl\n\timport blosc\n\timport fastavro as fa\n\timport orjson\n", "import pandas as pd\n\tfrom confluent_kafka import KafkaException\n\tfrom .. import config, get_logger\n\tfrom ..consumer import Consumer\n\tfrom ..producer import Producer\n\tPATH = os.path.realpath(os.path.dirname(__file__))\n\tclass EFTEAlertStreamer(Producer):\n\t    def __init__(self) -> None:\n\t        \"\"\"\n\t        Initialize the EFTEAlertStreamer instance.\n", "        It inherits from the Producer class and configures the Kafka connection parameters\n\t        using values from the config dictionary.\n\t        This class is used for streaming raw candidate detections from the\n\t        observatory to the Rico Kafka cluster.\n\t        The host, port, and topic are retrieved from the config dictionary\n\t        using the keys 'KAFKA_ADDR', 'KAFKA_PORT', and 'HBEAT_TOPIC' respectively.\n\t        \"\"\"\n\t        super().__init__(\n\t            host=config.KAFKA_ADDR,\n\t            port=config.KAFKA_PORT,\n", "        )\n\t        self.topic_base = config.EFTE_TOPIC_BASE\n\t        self.parsed_alert_schema = fa.schema.load_schema(\n\t            f\"{PATH}/schemas/efte.alert.avsc\"\n\t        )\n\t    def _parse_catalog(\n\t        self, catalog: tbl.Table, xmatches: List[Dict[str, List]]\n\t    ) -> bytes:\n\t        \"\"\"\n\t        Parses a catalog file and returns the serialized Avro data.\n", "        Args:\n\t            catalog_path (str): The path to the catalog file.\n\t        Returns:\n\t            bytes: The serialized Avro data.\n\t            dict: Catalog metadata.\n\t        \"\"\"\n\t        tab = catalog\n\t        if \"MJD\" not in tab.meta:\n\t            tab.meta[\"MJD\"] = 60000.1\n\t        if \"CCDDETID\" not in tab.meta:\n", "            tab.meta[\"CCDDETID\"] = \"ML3103817\"\n\t        mjd = tab.meta[\"MJD\"]\n\t        camera_id = tab.meta[\"CCDDETID\"]\n\t        records = []\n\t        for i, r in enumerate(tab):\n\t            alert_data = {\n\t                \"schemavsn\": self.parsed_alert_schema[\"version\"],\n\t                \"publisher\": \"rico.efte_generator\",\n\t                \"objectId\": str(uuid4()),\n\t            }\n", "            stamp = blosc.compress(r[\"stamp\"].data.tobytes())\n\t            candidate = dict(r)\n\t            candidate[\"stamp_bytes\"] = stamp\n\t            candidate[\"epoch\"] = mjd\n\t            candidate[\"camera\"] = camera_id\n\t            alert_data[\"candidate\"] = candidate\n\t            alert_data[\"xmatch\"] = xmatches[i]\n\t            records.append(alert_data)\n\t        fo = io.BytesIO()\n\t        fa.writer(fo, self.parsed_alert_schema, records)\n", "        fo.seek(0)\n\t        return fo.read()\n\t    def push_alert(self, catalog: tbl.Table, xmatches: List[Dict[str, List]]) -> None:\n\t        \"\"\"\n\t        Pushes data from a catalog file to a camera-specific topic.\n\t        Args:\n\t            catalog_path (str): The path to the catalog file.\n\t        Returns:\n\t            None\n\t        \"\"\"\n", "        avro_data = self._parse_catalog(catalog, xmatches)\n\t        topic = self.topic_base\n\t        self.send_binary(avro_data, topic=topic)\n\tclass EFTEAlertReceiver(Consumer):\n\t    def __init__(\n\t        self, group: str, output_path: str, filter_path: Optional[str]\n\t    ) -> None:\n\t        \"\"\"Initialize the EFTEAlertReceiver class.\n\t        Args:\n\t            group (str): The Kafka consumer group ID.\n", "            output_path (str): The path where filtered candidate data will be written.\n\t            filter_path (Optional[str]): The path to the text file containing filter conditions for xmatch data.\n\t                If provided, candidates will be filtered based on the conditions in the file.\n\t                Each condition should be in the format: 'column_name operator value'.\n\t        \"\"\"\n\t        super().__init__(\n\t            host=config.KAFKA_ADDR,\n\t            port=config.KAFKA_PORT,\n\t            topic=config.EFTE_TOPIC_BASE,\n\t            group_id=group,\n", "        )\n\t        self.parsed_alert_schema = fa.schema.load_schema(\n\t            f\"{PATH}/schemas/efte.alert.avsc\"\n\t        )\n\t        self.filter_path = filter_path\n\t        self.output_path = output_path\n\t        self.log = get_logger(__name__)\n\t    def poll_and_record(self) -> None:\n\t        \"\"\"Start polling for Kafka messages and process the candidates based on filter conditions.\"\"\"\n\t        c = self.get_consumer()\n", "        c.subscribe(\n\t            [\n\t                self.topic,\n\t            ]\n\t        )\n\t        try:\n\t            while True:\n\t                event = c.poll(1.0)\n\t                if event is None:\n\t                    continue\n", "                if event.error():\n\t                    raise KafkaException(event.error())\n\t                else:\n\t                    alerts = self._decode(event.value())\n\t                    self._filter_to_disk(alerts)\n\t        except KeyboardInterrupt:\n\t            print(\"Canceled by user.\")\n\t        finally:\n\t            c.close()\n\t    def _decode(self, message: bytes) -> List[Dict[str, Any]]:\n", "        \"\"\"Decode the AVRO message into a list of dictionaries.\n\t        Args:\n\t            message (bytes): The AVRO message received from Kafka.\n\t        Returns:\n\t            List[Dict[str, Any]]: A list of candidate dictionaries.\n\t        \"\"\"\n\t        stringio = io.BytesIO(message)\n\t        stringio.seek(0)\n\t        records = []\n\t        for record in fa.reader(stringio):\n", "            records.append(record)\n\t        return records\n\t    def _write_candidate(self, alert: Dict[str, Any]) -> None:\n\t        \"\"\"Write the candidate data to a JSON file.\n\t        Args:\n\t            candidate (Dict[str, Any]): The candidate dictionary to be written to the JSON file.\n\t        \"\"\"\n\t        alert[\"candidate\"][\"stamp_bytes\"] = base64.b64encode(\n\t            alert[\"candidate\"][\"stamp_bytes\"]\n\t        ).decode(\"utf-8\")\n", "        with open(\n\t            os.path.join(self.output_path, f\"{alert['objectId']}.json\"), \"wb\"\n\t        ) as f:\n\t            f.write(orjson.dumps(alert))\n\t        self.log.info(f'New candidate: {alert[\"objectId\"]}.json')\n\t    def _filter_to_disk(self, alerts: List[Dict[str, Any]]) -> None:\n\t        \"\"\"Filter the candidates based on the specified filter conditions.\n\t        Args:\n\t            candidates (List[Dict[str, Any]]): The list of candidate dictionaries to be filtered.\n\t        Note:\n", "            If the 'filter_path' is provided, the candidates will be filtered based on the conditions\n\t            specified in the text file. If 'xmatch' field is empty or 'filter_path' is not provided,\n\t            all candidates will be written to the output.\n\t        \"\"\"\n\t        if self.filter_path is not None:\n\t            with open(self.filter_path, \"r\") as file:\n\t                filter_conditions = file.read().splitlines()\n\t            filter_conditions = [f for f in filter_conditions if len(f) > 3]\n\t        for alert in alerts:\n\t            if self.filter_path is not None:\n", "                if len(alert[\"xmatch\"]) > 0:\n\t                    xmatch = pd.DataFrame.from_records(alert[\"xmatch\"])\n\t                    xmatch[\"g-r\"] = xmatch[\"g\"] - xmatch[\"r\"]\n\t                    for condition in filter_conditions:\n\t                        column_name, operator, value = condition.split()\n\t                        xmatch = xmatch.query(f\"{column_name} {operator} {value}\")\n\t                    if len(xmatch) > 0:\n\t                        self._write_candidate(alert)\n\t                else:\n\t                    return\n", "            else:\n\t                self._write_candidate(alert)\n"]}
{"filename": "src/argus_rico/efte/watchdog.py", "chunked_list": ["__all__ = [\"EFTEWatcher\", \"EFTECatalogHandler\"]\n\timport os\n\timport time\n\tfrom collections import defaultdict\n\timport ray\n\tfrom watchdog.events import FileSystemEvent, FileSystemEventHandler\n\tfrom watchdog.observers.polling import PollingObserver\n\tfrom .. import get_logger\n\tfrom .processor import EFTECatalogProcessor\n\tclass EFTEWatcher:\n", "    def __init__(self, watch_path: str) -> None:\n\t        \"\"\"Initialize the EFTEWatcher class.\n\t        Args:\n\t            watch_path (str): The path to the directory to watch for catalog files.\n\t            format (str, optional): The format of catalog files. Defaults to \"fits\".\n\t        \"\"\"\n\t        self.watch_path = os.path.abspath(watch_path)\n\t    def watch(self) -> None:\n\t        \"\"\"Start watching the specified directory for catalog files.\"\"\"\n\t        ray.init()\n", "        event_handler = EFTECatalogHandler()\n\t        observer = PollingObserver()\n\t        observer.schedule(event_handler, self.watch_path, recursive=False)\n\t        observer.start()\n\t        try:\n\t            while True:\n\t                time.sleep(1)\n\t        except KeyboardInterrupt:\n\t            observer.stop()\n\t        observer.join()\n", "class EFTECatalogHandler(FileSystemEventHandler):\n\t    def __init__(self):\n\t        \"\"\"Initialize the EFTECatalogHandler class.\"\"\"\n\t        self.efte_processors = defaultdict(EFTECatalogProcessor.remote)\n\t        self.log = get_logger(__name__)\n\t    def on_created(self, event: FileSystemEvent) -> None:\n\t        \"\"\"Process the newly created catalog file.\n\t        Args:\n\t            event (FileSystemEvent): The event object representing the file creation.\n\t        Returns:\n", "            None: This method does not return any value; it processes the catalog file.\n\t        \"\"\"\n\t        filepath = event.src_path\n\t        if filepath[-4:] != \".cat\":\n\t            return\n\t        camera_id = os.path.basename(filepath)[:9]\n\t        self.log.info(f\"New cat for {camera_id}: {filepath}\")\n\t        self.efte_processors[camera_id].process.remote(filepath)\n"]}
{"filename": "src/argus_rico/models/__init__.py", "chunked_list": ["__all__ = [\n\t    \"EVRImage\",\n\t    \"EVRImageUpdate\",\n\t    \"EVRImageType\",\n\t    \"EVRImageUpdateType\",\n\t    \"fitspath_to_constructor\",\n\t    \"EFTEAlert\",\n\t    \"EFTECandidate\",\n\t    \"XMatchItem\",\n\t]\n", "from .efte_alert import EFTEAlert, EFTECandidate, XMatchItem\n\tfrom .evr_image import EVRImage, EVRImageUpdate, fitspath_to_constructor\n"]}
{"filename": "src/argus_rico/models/efte_alert.py", "chunked_list": ["import base64\n\tfrom typing import List, Type, TypeVar\n\timport blosc\n\timport numpy as np\n\timport orjson\n\tfrom pydantic import BaseModel\n\tEFTEAlertType = TypeVar(\"EFTEAlertType\", bound=\"EFTEAlert\")\n\tclass EFTECandidate(BaseModel):\n\t    stamp_bytes: bytes\n\t    epoch: float\n", "    camera: str\n\t    thresh: float\n\t    vetnet_score: float\n\t    npix: int\n\t    tnpix: int\n\t    xmin: int\n\t    xmax: int\n\t    ymin: int\n\t    ymax: int\n\t    xcentroid: float\n", "    ycentroid: float\n\t    x2: float\n\t    y2: float\n\t    xy: float\n\t    errx2: float\n\t    erry2: float\n\t    errxy: float\n\t    a: float\n\t    b: float\n\t    theta: float\n", "    cxx: float\n\t    cyy: float\n\t    cxy: float\n\t    cflux: float\n\t    flux: float\n\t    cpeak: float\n\t    peak: float\n\t    xcpeak: int\n\t    ycpeak: int\n\t    xpeak: int\n", "    ypeak: int\n\t    flag: int\n\t    aper_sum_bkgsub: float\n\t    annulus_rms: float\n\t    difsnr: float\n\t    hfd: float\n\t    sign_ratio: float\n\t    insmag: float\n\t    zpoint: float\n\t    gmag: float\n", "    srcsnr: float\n\t    ra: float\n\t    dec: float\n\t    @property\n\t    def stamp(self):\n\t        return (\n\t            np.frombuffer(\n\t                blosc.decompress(\n\t                    base64.b64decode(\n\t                        self.stamp_bytes,\n", "                    )\n\t                ),\n\t                dtype=np.float32,\n\t            )\n\t            .reshape((30, 30, 3))\n\t            .byteswap()\n\t        )\n\t# Create a Pydantic model for the 'xmatch' list of dictionaries\n\tclass XMatchItem(BaseModel):\n\t    ra: float\n", "    dec: float\n\t    parallax: float\n\t    pmra: float\n\t    pmdec: float\n\t    g: float\n\t    r: float\n\t    i: float\n\t    separation: float\n\tclass EFTEAlert(BaseModel):\n\t    schemavsn: str\n", "    publisher: str\n\t    objectId: str\n\t    candidate: EFTECandidate\n\t    xmatch: List[XMatchItem]\n\t    @classmethod\n\t    def from_json(cls: Type[EFTEAlertType], json_path: str) -> EFTEAlertType:\n\t        with open(json_path, \"rb\") as f:\n\t            alert = orjson.loads(f.read())\n\t        return cls(**alert)\n"]}
{"filename": "src/argus_rico/models/evr_image.py", "chunked_list": ["import datetime\n\timport os\n\timport socket\n\timport uuid\n\timport warnings\n\tfrom typing import Any, Dict, Optional, Type, TypeVar, Union\n\timport astropy.io.fits as fits\n\timport astropy.wcs as awcs\n\timport numpy as np\n\timport qlsc\n", "from geojson_pydantic import Polygon\n\tfrom pydantic import BaseModel, Field, ValidationError\n\twarnings.simplefilter(\"ignore\", category=awcs.FITSFixedWarning)\n\tQ = qlsc.QLSC(depth=30)\n\tHOSTNAME = socket.gethostname()\n\tEVRImageType = TypeVar(\"EVRImageType\", bound=\"EVRImage\")\n\tEVRImageUpdateType = TypeVar(\"EVRImageUpdateType\", bound=\"EVRImageUpdate\")\n\tdef _wcs_to_footprint(header: fits.Header) -> Dict[str, Any]:\n\t    \"\"\"\n\t     Converts the WCS header information to a GeoJSON polygon footprint.\n", "    Args:\n\t        header: FITS header containing WCS information.\n\t    Returns:\n\t        Dictionary representing the GeoJSON polygon footprint.\n\t    \"\"\"\n\t    w = awcs.WCS(header)\n\t    im_center = w.all_pix2world(3288, 2192, 0)\n\t    header[\"CRVAL1\"] = float(im_center[0])\n\t    header[\"CRVAL2\"] = float(im_center[1])\n\t    x = np.arange(0, 6575, 274)\n", "    y = np.arange(0, 4384, 274)\n\t    bottom_edge = list(zip([0] * len(x), x)) + [(0, 6575)]\n\t    right_edge = list(zip(y, [6575] * len(y)))[1:] + [(4383, 6575)]\n\t    top_edge = list(zip([4383] * len(x), x)) + [(4383, 6575)]\n\t    top_edge = list(reversed(top_edge))[1:]\n\t    left_edge = list(zip(y, [0] * len(y))) + [(4383, 0)]\n\t    left_edge = list(reversed(left_edge))[1:]\n\t    poly = np.array(bottom_edge + right_edge + top_edge + left_edge)\n\t    poly = np.array([poly.T[1], poly.T[0]]).T\n\t    radecs = w.all_pix2world(poly, 0)\n", "    # Re-orient RA from [-180,180]\n\t    radecs[:, 0] -= 180\n\t    radecs = [list(r) for r in radecs]\n\t    footprint = {\"type\": \"Polygon\", \"coordinates\": [radecs]}\n\t    return footprint\n\tdef fitspath_to_constructor(fitspath: Union[str, fits.HDUList]) -> Dict[str, Any]:\n\t    \"\"\"\n\t    Convert a FITS header into a Pydantic constructor dictionary.\n\t    Args:\n\t        fitspath (str or astropy.io.fits.HDUList): The path to the FITS file or\n", "        and HDUList object.\n\t    Returns:\n\t        Dict[str, Any]: The Pydantic constructor dictionary representing the FITS header.\n\t    Raises:\n\t        KeyError: If any required FITS header fields are missing.\n\t    \"\"\"\n\t    if type(fitspath) is str:\n\t        hdulist = fits.open(fitspath)\n\t    if len(hdulist) == 1:\n\t        hdu_num = 0\n", "    else:\n\t        hdu_num = 1\n\t    header = hdulist[hdu_num].header.copy()\n\t    hdulist.close()\n\t    if \"FIELDID\" not in header:\n\t        header[\"FIELDID\"] = 400\n\t    constructor_dict = {}\n\t    constructor_dict[\"camera\"] = header[\"CCDDETID\"]\n\t    constructor_dict[\"filter_name\"] = header[\"FILTER\"]\n\t    constructor_dict[\"image_type\"] = header[\"IMGTYPE\"].strip()\n", "    constructor_dict[\"obstime\"] = header[\"DATE-OBS\"] + \"T\" + header[\"TIME-OBS\"]\n\t    if \"GPSTIME\" in header and \"Z\" in header[\"GPSTIME\"]:\n\t        constructor_dict[\"gpstime\"] = header[\"GPSTIME\"]\n\t    constructor_dict[\"ccd_set_temp\"] = header[\"SETTEMP\"]\n\t    constructor_dict[\"ccd_temp\"] = header[\"CCDTEMP\"]\n\t    constructor_dict[\"exp_time\"] = header[\"EXPTIME\"]\n\t    constructor_dict[\"site_name\"] = header[\"TELOBS\"]\n\t    constructor_dict[\"server_name\"] = HOSTNAME\n\t    constructor_dict[\"basename\"] = header[\"ORIGNAME\"].split(\".\")[0]\n\t    constructor_dict[\"fieldid\"] = header[\"FIELDID\"]\n", "    constructor_dict[\"ratchnum\"] = header[\"RATCHNUM\"]\n\t    constructor_dict[\"mount_ha\"] = header[\"MOUNTHA\"]\n\t    constructor_dict[\"sha1\"] = header[\"CHECKSUM\"]\n\t    constructor_dict[\"ccd_ext_temp\"] = header[\"CCDETEMP\"]\n\t    constructor_dict[\"wind_dir\"] = header[\"WINDDIR\"]\n\t    constructor_dict[\"wind_speed\"] = header[\"WINDSPED\"]\n\t    constructor_dict[\"rel_humidity\"] = header[\"OUTRELHU\"]\n\t    constructor_dict[\"dew_point\"] = header[\"OUTDEWPT\"]\n\t    constructor_dict[\"air_pressure\"] = header[\"OUTPRESS\"]\n\t    constructor_dict[\"mushroom_temp\"] = header[\"INR1TEMP\"]\n", "    constructor_dict[\"inc_x\"] = header[\"INR1INCX\"]\n\t    constructor_dict[\"inc_y\"] = header[\"INR1INCY\"]\n\t    constructor_dict[\"inc_z\"] = header[\"INR1INCZ\"]\n\t    if \"CRVAL1\" in header:\n\t        constructor_dict[\"qid\"] = Q.ang2ipix(header[\"CRVAL1\"], header[\"CRVAL2\"])\n\t        constructor_dict[\"footprint\"] = Polygon(**_wcs_to_footprint(header))\n\t        constructor_dict[\"wcspath\"] = os.path.abspath(fitspath)\n\t    else:\n\t        constructor_dict[\"rawpath\"] = os.path.abspath(fitspath)\n\t    return constructor_dict\n", "class EVRImage(BaseModel):\n\t    \"\"\"Class representing an EVR image.\"\"\"\n\t    id: str = Field(default_factory=uuid.uuid4, alias=\"_id\")\n\t    camera: Optional[str] = Field(...)\n\t    filter_name: str = Field(...)\n\t    obstime: datetime.datetime = Field(...)\n\t    gpstime: Optional[datetime.datetime] = None\n\t    ccd_set_temp: float = Field(...)\n\t    ccd_temp: float = Field(...)\n\t    exp_time: float = Field(...)\n", "    site_name: str = Field(...)\n\t    server_name: str = Field(...)\n\t    image_type: str = Field(...)\n\t    rawpath: Optional[str] = None\n\t    wcspath: Optional[str] = None\n\t    basename: str = Field(...)\n\t    fieldid: float = Field(...)\n\t    ratchnum: str = Field(...)\n\t    mount_ha: Optional[float] = Field(...)\n\t    sha1: str = Field(...)\n", "    ccd_ext_temp: float = Field(...)\n\t    wind_dir: float = Field(...)\n\t    wind_speed: float = Field(...)\n\t    rel_humidity: float = Field(...)\n\t    dew_point: float = Field(...)\n\t    air_pressure: float = Field(...)\n\t    mushroom_temp: float = Field(...)\n\t    inc_x: Optional[float] = Field(...)\n\t    inc_y: Optional[float] = Field(...)\n\t    inc_z: Optional[float] = Field(...)\n", "    footprint: Optional[Polygon] = Field(...)\n\t    qid: Optional[int] = Field(...)\n\t    class Config:\n\t        \"\"\"Pydantic configuration.\"\"\"\n\t        allow_population_by_field_name = True\n\t    @classmethod\n\t    def from_fits(cls: Type[EVRImageType], fitspath: str) -> EVRImageType:\n\t        \"\"\"Create an instance of EVRImage from a FITS file.\n\t        Args:\n\t            fitspath: Path to the FITS file.\n", "        Returns:\n\t            An instance of EVRImage.\n\t        \"\"\"\n\t        constructor_dict = fitspath_to_constructor(fitspath)\n\t        # This is duplicated boiler plate between the Image/ImageUpdate classes\n\t        try:\n\t            inst = cls(**constructor_dict)\n\t        except ValidationError as e:\n\t            for error in e.errors():\n\t                field = error[\"loc\"][0]\n", "                constructor_dict[field] = None\n\t            inst = cls(**constructor_dict)\n\t        return inst\n\tclass EVRImageUpdate(BaseModel):\n\t    \"\"\"Class representing an update to an EVR image.\"\"\"\n\t    camera: Optional[str]\n\t    filter_name: Optional[str]\n\t    obstime: Optional[datetime.datetime]\n\t    gpstime: Optional[datetime.datetime]\n\t    ccd_set_temp: Optional[float]\n", "    ccd_temp: Optional[float]\n\t    exp_time: Optional[float]\n\t    site_name: Optional[str]\n\t    server_name: Optional[str]\n\t    image_type: Optional[str]\n\t    rawpath: Optional[str]\n\t    wcspath: Optional[str]\n\t    basename: Optional[str]\n\t    fieldid: Optional[float]\n\t    ratchnum: Optional[str]\n", "    sha1: Optional[str]\n\t    ccd_ext_temp: Optional[float]\n\t    wind_dir: Optional[float]\n\t    wind_speed: Optional[float]\n\t    rel_humidity: Optional[float]\n\t    dew_point: Optional[float]\n\t    air_pressure: Optional[float]\n\t    mushroom_temp: Optional[float]\n\t    inc_x: Optional[float]\n\t    inc_y: Optional[float]\n", "    inc_z: Optional[float]\n\t    footprint: Optional[Polygon]\n\t    qid: Optional[int]\n\t    class Config:\n\t        \"\"\"Pydantic configuration.\"\"\"\n\t        allow_population_by_field_name = True\n\t    @classmethod\n\t    def from_fits(cls: Type[EVRImageUpdateType], fitspath: str) -> EVRImageUpdateType:\n\t        \"\"\"Create an instance of EVRImageUpdate from a FITS file.\n\t        Args:\n", "            fitspath: Path to the FITS file.\n\t        Returns:\n\t            An instance of EVRImageUpdate.\n\t        \"\"\"\n\t        constructor_dict = fitspath_to_constructor(fitspath)\n\t        # This is duplicated boiler plate between the Image/ImageUpdate classes\n\t        try:\n\t            inst = cls(**constructor_dict)\n\t        except ValidationError as e:\n\t            for error in e.errors():\n", "                field = error[\"loc\"][0]\n\t                constructor_dict[field] = None\n\t            inst = cls(**constructor_dict)\n\t        return inst\n"]}
