{"filename": "docs/source/conf.py", "chunked_list": ["#!/usr/bin/env python3\n\t# kedro_graphql documentation build\n\t# configuration file, created by sphinx-quickstart.\n\t#\n\t# This file is execfile()d with the current directory set to its\n\t# containing dir.\n\t#\n\t# Note that not all possible configuration values are present in this\n\t# autogenerated file.\n\t#\n", "# All configuration values have a default; values that are commented out\n\t# serve to show the default.\n\t# If extensions (or modules to document with autodoc) are in another directory,\n\t# add these directories to sys.path here. If the directory is relative to the\n\t# documentation root, use os.path.abspath to make it absolute, like shown here.\n\t#\n\timport re\n\tfrom kedro.framework.cli.utils import find_stylesheets\n\tfrom kedro_graphql import __version__ as release\n\t# -- Project information -----------------------------------------------------\n", "project = \"kedro_graphql\"\n\tauthor = \"Kedro\"\n\t# The short X.Y version.\n\tversion = re.match(r\"^([0-9]+\\.[0-9]+).*\", release).group(1)\n\t# -- General configuration ---------------------------------------------------\n\t# If your documentation needs a minimal Sphinx version, state it here.\n\t#\n\t# needs_sphinx = '1.0'\n\t# Add any Sphinx extension module names here, as strings. They can be\n\t# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n", "# ones.\n\textensions = [\n\t    \"sphinx.ext.autodoc\",\n\t    \"sphinx.ext.napoleon\",\n\t    \"sphinx_autodoc_typehints\",\n\t    \"sphinx.ext.doctest\",\n\t    \"sphinx.ext.todo\",\n\t    \"sphinx.ext.coverage\",\n\t    \"sphinx.ext.mathjax\",\n\t    \"sphinx.ext.ifconfig\",\n", "    \"sphinx.ext.viewcode\",\n\t    \"sphinx.ext.mathjax\",\n\t    \"nbsphinx\",\n\t    \"myst_parser\",\n\t    \"sphinx_copybutton\",\n\t]\n\t# enable autosummary plugin (table of contents for modules/classes/class\n\t# methods)\n\tautosummary_generate = True\n\t# Add any paths that contain templates here, relative to this directory.\n", "templates_path = [\"_templates\"]\n\t# The suffix(es) of source filenames.\n\t# You can specify multiple suffix as a list of string:\n\t#\n\tsource_suffix = {\".rst\": \"restructuredtext\", \".md\": \"markdown\"}\n\t# The master toctree document.\n\tmaster_doc = \"index\"\n\t# The language for content autogenerated by Sphinx. Refer to documentation\n\t# for a list of supported languages.\n\t#\n", "# This is also used if you do content translation via gettext catalogs.\n\t# Usually you set \"language\" from the command line for these cases.\n\tlanguage = None\n\t# List of patterns, relative to source directory, that match files and\n\t# directories to ignore when looking for source files.\n\t# This pattern also affects html_static_path and html_extra_path .\n\texclude_patterns = [\"_build\", \"**.ipynb_checkpoints\"]\n\t# The name of the Pygments (syntax highlighting) style to use.\n\tpygments_style = \"sphinx\"\n\t# -- Options for HTML output -------------------------------------------------\n", "# The theme to use for HTML and HTML Help pages.  See the documentation for\n\t# a list of builtin themes.\n\t#\n\thtml_theme = \"sphinx_rtd_theme\"\n\t# Theme options are theme-specific and customize the look and feel of a theme\n\t# further.  For a list of options available for each theme, see the\n\t# documentation.\n\t#\n\thtml_theme_options = {\"collapse_navigation\": False, \"style_external_links\": True}\n\t# Add any paths that contain custom static files (such as style sheets) here,\n", "# relative to this directory. They are copied after the builtin static files,\n\t# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n\thtml_static_path = [\"_static\"]\n\t# Custom sidebar templates, must be a dictionary that maps document names\n\t# to template names.\n\t#\n\t# The default sidebars (for documents that don't match any pattern) are\n\t# defined by theme itself.  Builtin themes are using these templates by\n\t# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',\n\t# 'searchbox.html']``.\n", "#\n\t# html_sidebars = {}\n\thtml_show_sourcelink = False\n\t# -- Options for HTMLHelp output ---------------------------------------------\n\t# Output file base name for HTML help builder.\n\thtmlhelp_basename = \"kedro_graphqldoc\"\n\t# -- Options for LaTeX output ------------------------------------------------\n\tlatex_elements = {\n\t    # The paper size ('letterpaper' or 'a4paper').\n\t    #\n", "    # 'papersize': 'letterpaper',\n\t    #\n\t    # The font size ('10pt', '11pt' or '12pt').\n\t    #\n\t    # 'pointsize': '10pt',\n\t    #\n\t    # Additional stuff for the LaTeX preamble.\n\t    #\n\t    # 'preamble': '',\n\t    #\n", "    # Latex figure (float) alignment\n\t    #\n\t    # 'figure_align': 'htbp',\n\t}\n\t# Grouping the document tree into LaTeX files. List of tuples\n\t# (source start file, target name, title,\n\t#  author, documentclass [howto, manual, or own class]).\n\tlatex_documents = [\n\t    (\n\t        master_doc,\n", "        \"kedro_graphql.tex\",\n\t        \"kedro_graphql Documentation\",\n\t        \"Kedro\",\n\t        \"manual\",\n\t    )\n\t]\n\t# -- Options for manual page output ------------------------------------------\n\t# One entry per manual page. List of tuples\n\t# (source start file, name, description, authors, manual section).\n\tman_pages = [\n", "    (\n\t        master_doc,\n\t        \"kedro_graphql\",\n\t        \"kedro_graphql Documentation\",\n\t        [author],\n\t        1,\n\t    )\n\t]\n\t# -- Options for Texinfo output ----------------------------------------------\n\t# Grouping the document tree into Texinfo files. List of tuples\n", "# (source start file, target name, title, author,\n\t#  dir menu entry, description, category)\n\ttexinfo_documents = [\n\t    (\n\t        master_doc,\n\t        \"kedro_graphql\",\n\t        \"kedro_graphql Documentation\",\n\t        author,\n\t        \"kedro_graphql\",\n\t        \"Project kedro_graphql codebase.\",\n", "        \"Data-Science\",\n\t    )\n\t]\n\t# -- Options for todo extension ----------------------------------------------\n\t# If true, `todo` and `todoList` produce output, else they produce nothing.\n\ttodo_include_todos = False\n\t# -- Extension configuration -------------------------------------------------\n\t# nbsphinx_prolog = \"\"\"\n\t# see here for prolog/epilog details:\n\t# https://nbsphinx.readthedocs.io/en/0.3.1/prolog-and-epilog.html\n", "# \"\"\"\n\t# -- NBconvert kernel config -------------------------------------------------\n\tnbsphinx_kernel_name = \"python3\"\n\tdef remove_arrows_in_examples(lines):\n\t    for i, line in enumerate(lines):\n\t        lines[i] = line.replace(\">>>\", \"\")\n\tdef autodoc_process_docstring(app, what, name, obj, options, lines):\n\t    remove_arrows_in_examples(lines)\n\tdef skip(app, what, name, obj, skip, options):\n\t    if name == \"__init__\":\n", "        return False\n\t    return skip\n\tdef setup(app):\n\t    app.connect(\"autodoc-process-docstring\", autodoc_process_docstring)\n\t    app.connect(\"autodoc-skip-member\", skip)\n\t    # add Kedro stylesheets\n\t    for stylesheet in find_stylesheets():\n\t        app.add_css_file(stylesheet)\n"]}
{"filename": "src/setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\t#entry_point = (\n\t#    \"kedro-graphql = kedro_graphql.__main__:main\"\n\t#)\n\t# get the dependencies and installs\n\t##with open(\"requirements.txt\", encoding=\"utf-8\") as f:\n\t##    # Make sure we strip all comments and options (e.g \"--extra-index-url\")\n\t##    # that arise from a modified pip.conf file that configure global options\n\t##    # when running kedro build-reqs\n\t##    requires = []\n", "##    for line in f:\n\t##        req = line.split(\"#\", 1)[0].strip()\n\t##        if req and not req.startswith(\"--\"):\n\t##            requires.append(req)\n\tsetup(\n\t    name=\"kedro_graphql\",\n\t    packages=find_packages(exclude=[\"tests\"]),\n\t    ##entry_points={\n\t    ##    \"console_scripts\": [entry_point],\n\t    ##    \"kedro.project_commands\": [\"kedro-graphql = kedro_graphql.commands:commands\"]\n", "    ##    },\n\t#    install_requires=requires,\n\t    extras_require={\n\t        \"docs\": [\n\t            \"docutils<0.18.0\",\n\t            \"sphinx~=3.4.3\",\n\t            \"sphinx_rtd_theme==0.5.1\",\n\t            \"nbsphinx==0.8.1\",\n\t            \"nbstripout~=0.4\",\n\t            \"sphinx-autodoc-typehints==1.11.1\",\n", "            \"sphinx_copybutton==0.3.1\",\n\t            \"ipykernel>=5.3, <7.0\",\n\t            \"Jinja2<3.1.0\",\n\t            \"myst-parser~=0.17.2\",\n\t        ]\n\t    },\n\t)\n"]}
{"filename": "src/tests/test_schema_query.py", "chunked_list": ["\"\"\"\n\t\"\"\"\n\timport pytest\n\tfrom kedro_graphql.schema import build_schema\n\tschema = build_schema()\n\t@pytest.mark.usefixtures('celery_session_app')\n\t@pytest.mark.usefixtures('celery_session_worker')\n\tclass TestSchemaQuery:\n\t    @pytest.mark.asyncio\n\t    async def test_pipeline(self, mock_info_context, mock_pipeline):\n", "        query = \"\"\"\n\t        query TestQuery($id: String!) {\n\t          pipeline(id: $id){\n\t            id\n\t          }\n\t        }\n\t        \"\"\"\n\t        resp = await schema.execute(query, variable_values = {\"id\": str(mock_pipeline.id)})\n\t        assert resp.errors is None\n\t    @pytest.mark.asyncio\n", "    async def test_pipeline_templates(self):\n\t        query = \"\"\"\n\t        query TestQuery {\n\t          pipelineTemplates {\n\t            name\n\t            describe\n\t            inputs {\n\t              name\n\t              filepath\n\t              type\n", "            }\n\t            nodes {\n\t              name\n\t              inputs\n\t              outputs\n\t              tags\n\t            }\n\t            outputs {\n\t              filepath\n\t              name\n", "              type\n\t            }\n\t            parameters {\n\t              name\n\t              value\n\t            }\n\t          }\n\t        }\n\t        \"\"\"\n\t        resp = await schema.execute(query)\n", "        assert resp.errors is None\n"]}
{"filename": "src/tests/test_events.py", "chunked_list": ["\"\"\"\n\tThis module contains an example test.\n\tTests should be placed in ``src/tests``, in modules that mirror your\n\tproject's structure, and in files named test_*.py. They are simply functions\n\tnamed ``test_*`` which test a unit of logic.\n\tTo run the tests, run ``kedro test`` from the project root directory.\n\t\"\"\"\n\timport pytest\n\tfrom kedro_graphql.events import PipelineEventMonitor\n\tfrom kedro_graphql.tasks import run_pipeline\n", "import time\n\tfrom celery.states import ALL_STATES\n\tfrom celery.result import AsyncResult \n\t@pytest.mark.usefixtures('celery_session_app')\n\t@pytest.mark.usefixtures('celery_session_worker')\n\t@pytest.mark.usefixtures('celery_includes')\n\tclass TestPipelineEventMonitor:\n\t    @pytest.mark.asyncio\n\t    async def test_consume_default(self, mocker, celery_session_app, mock_pipeline):\n\t        \"\"\"\n", "        Requires Redis to run.\n\t        \"\"\"\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.before_start\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_success\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_retry\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_failure\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.after_return\")\n\t        async for e in PipelineEventMonitor(app = celery_session_app, task_id = mock_pipeline.task_id).start():\n\t            print(e)\n\t            assert e[\"status\"] in ALL_STATES\n", "    @pytest.mark.asyncio\n\t    async def test_consume_short_timeout(self, mocker, celery_session_app, mock_pipeline):\n\t        \"\"\"\n\t        Requires Redis to run.\n\t        Test with shorter timeout to test Exception handling\n\t        \"\"\"\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.before_start\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_success\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_retry\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_failure\")\n", "        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.after_return\")\n\t        async for e in PipelineEventMonitor(app = celery_session_app, task_id = mock_pipeline.task_id, timeout = 0.01).start():\n\t            print(e)\n\t            assert e[\"status\"] in ALL_STATES\n\t    @pytest.mark.asyncio\n\t    async def test_consume_exception(self, mocker, celery_session_app, mock_pipeline):\n\t        \"\"\"\n\t        Requires Redis to run.\n\t        Let task finish before starting monitor test Exception handling\n\t        \"\"\"\n", "        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.before_start\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_success\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_retry\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_failure\")\n\t        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.after_return\")\n\t        result = AsyncResult(mock_pipeline.task_id).wait()\n\t        async for e in PipelineEventMonitor(app = celery_session_app, task_id = mock_pipeline.task_id).start():\n\t            print(e)\n\t            assert e[\"status\"] in ALL_STATES\n"]}
{"filename": "src/tests/test_models.py", "chunked_list": ["\"\"\"\n\tThis module contains an example test.\n\tTests should be placed in ``src/tests``, in modules that mirror your\n\tproject's structure, and in files named test_*.py. They are simply functions\n\tnamed ``test_*`` which test a unit of logic.\n\tTo run the tests, run ``kedro test`` from the project root directory.\n\t\"\"\"\n\timport pytest\n\tfrom kedro_graphql.models import DataSet, Parameter\n\tclass TestDataSet:\n", "    def test_serialize(self):\n\t        params = {\"name\": \"text_in\", \n\t                  \"type\": \"text.TextDataSet\", \n\t                  \"filepath\": \"/tmp/test_in.csv\",\n\t                  \"load_args\":[Parameter(**{\n\t                    \"name\": \"delimiter\",\n\t                    \"value\": \"\\t\" \n\t                  })],\n\t                  \"save_args\":[Parameter(**{\n\t                    \"name\":\"delimiter\",\n", "                    \"value\": \"\\t\"  \n\t                  })]}\n\t        expected = {\"text_in\":{ \n\t                      \"type\": \"text.TextDataSet\", \n\t                      \"filepath\": \"/tmp/test_in.csv\",\n\t                      \"load_args\":{\n\t                        \"delimiter\" :\"\\t\" \n\t                      },\n\t                      \"save_args\":{\n\t                        \"delimiter\": \"\\t\"  \n", "                      }\n\t                    }\n\t                   }\n\t        d = DataSet(**params)\n\t        output = d.serialize()\n\t        assert output == expected\n\tclass TestParameter:\n\t    def test_serialize_string(self):\n\t        params = {\n\t                    \"name\": \"delimiter\",\n", "                    \"value\": \"\\t\",\n\t                    \"type\": \"string\"\n\t                  }\n\t        expected = {\n\t                    \"delimiter\": \"\\t\" \n\t                   }\n\t        p = Parameter(**params)\n\t        output = p.serialize()\n\t        assert output == expected\n\t    def test_serialize_int(self):\n", "        params = {\n\t                    \"name\": \"delimiter\",\n\t                    \"value\": \"1\",\n\t                    \"type\": \"integer\"\n\t                  }\n\t        expected = {\n\t                    \"delimiter\": 1\n\t                   }\n\t        p = Parameter(**params)\n\t        output = p.serialize()\n", "        assert output == expected\n\t    def test_serialize_int_exception(self):\n\t        params = {\n\t                    \"name\": \"delimiter\",\n\t                    \"value\": \"0.1\",\n\t                    \"type\": \"integer\"\n\t                  }\n\t        p = Parameter(**params)\n\t        try:\n\t            output = p.serialize()\n", "        except ValueError as e:\n\t            assert True\n\t    def test_serialize_float(self):\n\t        params = {\n\t                    \"name\": \"delimiter\",\n\t                    \"value\": \"0.1\",\n\t                    \"type\": \"float\"\n\t                  }\n\t        expected = {\n\t                    \"delimiter\": 0.1\n", "                   }\n\t        p = Parameter(**params)\n\t        output = p.serialize()\n\t        assert output == expected\n\t    def test_serialize_float_exception(self):\n\t        params = {\n\t                    \"name\": \"delimiter\",\n\t                    \"value\": \"hello\",\n\t                    \"type\": \"float\"\n\t                  }\n", "        p = Parameter(**params)\n\t        try:\n\t            output = p.serialize()\n\t        except ValueError as e:\n\t            assert True\n\t    def test_serialize_bool(self):\n\t        params = {\n\t                    \"name\": \"delimiter\",\n\t                    \"value\": \"true\",\n\t                    \"type\": \"boolean\"\n", "                  }\n\t        expected = {\n\t                    \"delimiter\": True\n\t                   }\n\t        p = Parameter(**params)\n\t        output = p.serialize()\n\t        assert output == expected\n\t        params = {\n\t                    \"name\": \"delimiter\",\n\t                    \"value\": \"True\",\n", "                    \"type\": \"boolean\"\n\t                  }\n\t        expected = {\n\t                    \"delimiter\": True\n\t                   }\n\t        p = Parameter(**params)\n\t        output = p.serialize()\n\t        assert output == expected\n\t        params = {\n\t                    \"name\": \"delimiter\",\n", "                    \"value\": \"false\",\n\t                    \"type\": \"boolean\"\n\t                  }\n\t        expected = {\n\t                    \"delimiter\": False\n\t                   }\n\t        p = Parameter(**params)\n\t        output = p.serialize()\n\t        assert output == expected\n\t        params = {\n", "                    \"name\": \"delimiter\",\n\t                    \"value\": \"False\",\n\t                    \"type\": \"boolean\"\n\t                  }\n\t        expected = {\n\t                    \"delimiter\": False\n\t                   }\n\t        p = Parameter(**params)\n\t        output = p.serialize()\n\t        assert output == expected\n", "    def test_serialize_bool_exception(self):\n\t        params = {\n\t                    \"name\": \"delimiter\",\n\t                    \"value\": \"rue\",\n\t                    \"type\": \"boolean\"\n\t                  }\n\t        p = Parameter(**params)\n\t        try:\n\t            output = p.serialize()\n\t        except ValueError as e:\n", "            assert True"]}
{"filename": "src/tests/test_schema_mutation.py", "chunked_list": ["\"\"\"\n\t\"\"\"\n\timport pytest\n\tfrom kedro_graphql.schema import build_schema\n\tschema = build_schema()\n\tclass TestSchemaMutations:\n\t    mutation = \"\"\"\n\t        mutation TestMutation($pipeline: PipelineInput!) {\n\t          pipeline(pipeline: $pipeline) {\n\t            name\n", "            describe\n\t            inputs {\n\t              name\n\t              filepath\n\t              type\n\t            }\n\t            nodes {\n\t              name\n\t              inputs\n\t              outputs\n", "              tags\n\t            }\n\t            outputs {\n\t              filepath\n\t              name\n\t              type\n\t            }\n\t            parameters {\n\t              name\n\t              value\n", "            }\n\t            status\n\t            tags {\n\t              key\n\t              value\n\t            }\n\t            taskId\n\t            taskName\n\t            taskArgs\n\t            taskKwargs\n", "            taskRequest\n\t            taskException\n\t            taskTraceback\n\t            taskEinfo\n\t            taskResult\n\t          }\n\t        }\n\t        \"\"\"\n\t    @pytest.mark.usefixtures('celery_session_app')\n\t    @pytest.mark.usefixtures('celery_session_worker')\n", "    @pytest.mark.asyncio\n\t    async def test_pipeline(self, mock_info_context, mock_text_in, mock_text_out):\n\t        resp = await schema.execute(self.mutation, \n\t                                    variable_values = {\"pipeline\": {\n\t                                      \"name\": \"example00\",\n\t                                      \"inputs\": [{\"name\": \"text_in\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_in)}],\n\t                                      \"outputs\": [{\"name\": \"text_out\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_out)}],\n\t                                      \"parameters\": [{\"name\":\"example\", \"value\":\"hello\"},\n\t                                                     {\"name\": \"duration\", \"value\": \"0.1\", \"type\": \"FLOAT\"}],\n\t                                      \"tags\": [{\"key\": \"author\", \"value\": \"opensean\"},{\"key\":\"package\", \"value\":\"kedro-graphql\"}]\n", "                                    }})\n\t        assert resp.errors is None\n\t    @pytest.mark.usefixtures('celery_session_app')\n\t    @pytest.mark.usefixtures('celery_session_worker')\n\t    @pytest.mark.asyncio\n\t    async def test_pipeline2(self, mock_info_context, mock_text_in_tsv, mock_text_out_tsv):\n\t        resp = await schema.execute(self.mutation, \n\t                                    variable_values = {\"pipeline\": {\n\t                                      \"name\": \"example00\",\n\t                                      \"inputs\": [{\"name\": \"text_in\", \n", "                                                  \"type\": \"pandas.CSVDataSet\", \n\t                                                  \"filepath\": str(mock_text_in_tsv),\n\t                                                  \"loadArgs\":[\n\t                                                      {\"name\": \"sep\", \"value\": \"\\t\"}\n\t                                                  ],\n\t                                                  \"saveArgs\":[\n\t                                                      {\"name\": \"sep\", \"value\": \"\\t\"}\n\t                                                  ]\n\t                                                }],\n\t                                      \"outputs\": [{\"name\": \"text_out\", \n", "                                                   \"type\": \"pandas.CSVDataSet\", \n\t                                                   \"filepath\": str(mock_text_out_tsv),\n\t                                                   \"loadArgs\":[\n\t                                                      {\"name\": \"sep\", \"value\": \"\\t\"}\n\t                                                  ],\n\t                                                  \"saveArgs\":[\n\t                                                      {\"name\": \"sep\", \"value\": \"\\t\"}\n\t                                                  ]}],\n\t                                      \"parameters\": [{\"name\":\"example\", \"value\":\"hello\"}],\n\t                                      \"tags\": [{\"key\": \"author\", \"value\": \"opensean\"},{\"key\":\"package\", \"value\":\"kedro-graphql\"}]\n", "                                    }})\n\t        assert resp.errors is None\n"]}
{"filename": "src/tests/__init__.py", "chunked_list": []}
{"filename": "src/tests/test_logs.py", "chunked_list": ["\"\"\"\n\t\"\"\"\n\timport pytest\n\tfrom kedro_graphql.logs.logger import PipelineLogStream\n\t@pytest.mark.usefixtures('celery_session_app')\n\t@pytest.mark.usefixtures('celery_session_worker')\n\tclass TestPipelineLogStream:\n\t    @pytest.mark.asyncio\n\t    async def test_consume(self, mock_pipeline):\n\t        \"\"\"Requires Redis to run.\n", "        \"\"\"\n\t        task_id = mock_pipeline.task_id\n\t        subscriber = await PipelineLogStream().create(task_id=task_id)\n\t        async for e in subscriber.consume():\n\t            assert set(e.keys()) == set([\"task_id\", \"message_id\", \"message\", \"time\"])"]}
{"filename": "src/tests/test_run.py", "chunked_list": ["\"\"\"\n\tThis module contains an example test.\n\tTests should be placed in ``src/tests``, in modules that mirror your\n\tproject's structure, and in files named test_*.py. They are simply functions\n\tnamed ``test_*`` which test a unit of logic.\n\tTo run the tests, run ``kedro test`` from the project root directory.\n\t\"\"\"\n\tfrom pathlib import Path\n\timport pytest\n\tfrom kedro.framework.project import settings\n", "from kedro.config import ConfigLoader\n\tfrom kedro.framework.context import KedroContext\n\tfrom kedro.framework.hooks import _create_hook_manager\n\t@pytest.fixture\n\tdef config_loader():\n\t    return ConfigLoader(conf_source=str(Path.cwd() / settings.CONF_SOURCE))\n\t@pytest.fixture\n\tdef project_context(config_loader):\n\t    return KedroContext(\n\t        package_name=\"kedro_graphql\",\n", "        project_path=Path.cwd(),\n\t        config_loader=config_loader,\n\t        hook_manager=_create_hook_manager(),\n\t    )\n\t# The tests below are here for the demonstration purpose\n\t# and should be replaced with the ones testing the project\n\t# functionality\n\tclass TestProjectContext:\n\t    def test_project_path(self, project_context):\n\t        assert project_context.project_path == Path.cwd()\n"]}
{"filename": "src/tests/test_schema_subscription.py", "chunked_list": ["\"\"\"\n\t\"\"\"\n\timport pytest\n\tfrom kedro_graphql.schema import build_schema\n\tschema = build_schema()\n\t@pytest.mark.usefixtures('celery_session_app')\n\t@pytest.mark.usefixtures('celery_session_worker')\n\tclass TestSchemaSubscriptions:\n\t    @pytest.mark.asyncio\n\t    async def test_pipeline(self, mock_info_context, mock_pipeline):\n", "        \"\"\"Requires Redis to run.\n\t        \"\"\"\n\t        query = \"\"\"\n\t    \t  subscription {\n\t          \tpipeline(id:\"\"\"+ '\"' + str(mock_pipeline.id) + '\"' + \"\"\") {\n\t              id\n\t              taskId\n\t              status\n\t              result\n\t              timestamp\n", "              traceback\n\t            }\n\t    \t  }\n\t        \"\"\"\n\t        sub = await schema.subscribe(query)\n\t        async for result in sub:\n\t            assert not result.errors\n\t    @pytest.mark.asyncio\n\t    async def test_pipeline_logs(self, mock_info_context, mock_pipeline, mock_pipeline2):\n\t        \"\"\"Requires Redis to run.\n", "        This test runs two pipelines simultaneously to ensure logs messages are scoped\n\t        to the correct pipeline.\n\t        \"\"\"\n\t        query = \"\"\"\n\t    \t  subscription {\n\t          \tpipelineLogs(id:\"\"\"+ '\"' + str(mock_pipeline.id) + '\"' + \"\"\") {\n\t              id\n\t              message\n\t              messageId\n\t              taskId\n", "              time\n\t            }\n\t    \t  }\n\t        \"\"\"\n\t        sub = await schema.subscribe(query)\n\t        async for result in sub:\n\t            #print(result)\n\t            assert not result.errors\n\t            assert result.data[\"pipelineLogs\"][\"id\"] == str(mock_pipeline.id)\n\t            assert result.data[\"pipelineLogs\"][\"taskId\"] == str(mock_pipeline.task_id)\n", "        query2 = \"\"\"\n\t    \t  subscription {\n\t          \tpipelineLogs(id:\"\"\"+ '\"' + str(mock_pipeline2.id) + '\"' + \"\"\") {\n\t              id\n\t              message\n\t              messageId\n\t              taskId\n\t              time\n\t            }\n\t    \t  }\n", "        \"\"\"\n\t        sub2 = await schema.subscribe(query2)\n\t        async for result in sub2:\n\t            #print(result)\n\t            assert not result.errors\n\t            assert result.data[\"pipelineLogs\"][\"id\"] == str(mock_pipeline2.id)\n\t            assert result.data[\"pipelineLogs\"][\"taskId\"] == str(mock_pipeline2.task_id)"]}
{"filename": "src/tests/conftest.py", "chunked_list": ["import pytest\n\tfrom pathlib import Path\n\tfrom kedro.framework.project import settings\n\tfrom kedro.config import ConfigLoader\n\tfrom kedro.framework.context import KedroContext\n\tfrom kedro.framework.hooks import _create_hook_manager\n\tfrom kedro_graphql.backends import init_backend\n\tfrom kedro_graphql.tasks import run_pipeline\n\tfrom kedro_graphql.models import Pipeline, DataSet, Parameter, Tag\n\tfrom unittest.mock import patch\n", "@pytest.fixture(scope=\"session\")\n\tdef config_loader():\n\t    return ConfigLoader(conf_source=str(Path.cwd() / settings.CONF_SOURCE))\n\t@pytest.fixture(scope='session')\n\tdef project_context(config_loader):\n\t    return KedroContext(\n\t        package_name=\"kedro_graphql\",\n\t        project_path=Path.cwd(),\n\t        config_loader=config_loader,\n\t        hook_manager=_create_hook_manager(),\n", "    )\n\t@pytest.fixture(scope='session')\n\tdef celery_config():\n\t    return {\n\t        'broker_url': 'redis://',\n\t        'result_backend': 'redis://',\n\t        'result_extened': True,\n\t        'worker_send_task_events': True,\n\t        'task_send_sent_event': True,\n\t        'task_store_eager_result': True,\n", "        'task_always_eager': False,\n\t        'task_ignore_result': False,\n\t        'imports': [\"kedro_graphql.config\", \"kedro_graphql.tasks\"]\n\t    }\n\t@pytest.fixture(scope=\"session\")\n\tdef celery_worker_parameters():\n\t    return {\"without_heartbeat\": False}\n\t@pytest.fixture\n\tdef mock_backend():\n\t    return init_backend()\n", "@pytest.fixture\n\tdef mock_info_context(mock_backend):\n\t    class App():\n\t        backend = mock_backend\n\t    class Request():\n\t        app = App()\n\t    with patch(\"strawberry.types.Info.context\", {\"request\": Request()}) as m:\n\t        yield m\n\t## refer to https://docs.pytest.org/en/7.1.x/how-to/tmp_path.html for info on tmp_path fixture\n\t@pytest.fixture\n", "def mock_text_in(tmp_path):\n\t    #tmp_path.mkdir()\n\t    text_in = tmp_path / \"text_in.txt\"\n\t    text_in.write_text(\"hello\")\n\t    return text_in\n\t@pytest.fixture\n\tdef mock_text_out(tmp_path):\n\t    #tmp_path.mkdir()\n\t    text_out = tmp_path / \"text_out.txt\"\n\t    text_out.write_text(\"good bye\")\n", "    return text_out\n\t@pytest.fixture\n\tdef mock_text_in_tsv(tmp_path):\n\t    #tmp_path.mkdir()\n\t    text = tmp_path / \"text_in.tsv\"\n\t    text.write_text(\"Some parameter\\tOther parameter\\tLast parameter\\nCONST\\t123456\\t12.45\")\n\t    return text\n\t@pytest.fixture\n\tdef mock_text_out_tsv(tmp_path):\n\t    #tmp_path.mkdir()\n", "    text = tmp_path / \"text_out.tsv\"\n\t    text.write_text(\"Some parameter\\tOther parameter\\tLast parameter\\nCONST\\t123456\\t12.45\")\n\t    return text\n\t@pytest.fixture\n\tdef mock_pipeline(mock_backend, tmp_path, mock_text_in, mock_text_out):\n\t    inputs = [{\"name\": \"text_in\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_in)}]\n\t    outputs = [{\"name\":\"text_out\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_out)}]\n\t    parameters = [{\"name\":\"example\", \"value\":\"hello\"}]\n\t    tags = [{\"key\": \"author\", \"value\": \"opensean\"},{\"key\":\"package\", \"value\":\"kedro-graphql\"}]\n\t    p = Pipeline(\n", "        name = \"example00\",\n\t        inputs = [DataSet(**i) for i in inputs],\n\t        outputs = [DataSet(**o) for o in outputs],\n\t        parameters = [Parameter(**p) for p in parameters],\n\t        tags = [Tag(**p) for p in tags],\n\t        task_name = str(run_pipeline),\n\t    )\n\t    serial = p.serialize()\n\t    result = run_pipeline.apply_async(kwargs = {\"name\": \"example00\", \n\t                                                 \"inputs\": serial[\"inputs\"], \n", "                                                 \"outputs\": serial[\"outputs\"],\n\t                                                 \"parameters\": serial[\"parameters\"]}, countdown=0.1)\n\t    p.task_id = result.id\n\t    p.status = result.status\n\t    p.task_kwargs = str(\n\t            {\"name\": serial[\"name\"], \n\t            \"inputs\": serial[\"inputs\"], \n\t            \"outputs\": serial[\"outputs\"], \n\t            \"parameters\": serial[\"parameters\"]}\n\t    )\n", "    print(f'Starting {p.name} pipeline with task_id: ' + str(p.task_id))\n\t    p = mock_backend.create(p)\n\t    return p\n\t@pytest.fixture\n\tdef mock_pipeline2(mock_backend, tmp_path, mock_text_in, mock_text_out):\n\t    inputs = [{\"name\": \"text_in\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_in)}]\n\t    outputs = [{\"name\":\"text_out\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_out)}]\n\t    parameters = [{\"name\":\"example\", \"value\":\"hello\"}]\n\t    tags = [{\"key\": \"author\", \"value\": \"opensean\"},{\"key\":\"package\", \"value\":\"kedro-graphql\"}]\n\t    p = Pipeline(\n", "        name = \"example00\",\n\t        inputs = [DataSet(**i) for i in inputs],\n\t        outputs = [DataSet(**o) for o in outputs],\n\t        parameters = [Parameter(**p) for p in parameters],\n\t        tags = [Tag(**p) for p in tags],\n\t        task_name = str(run_pipeline),\n\t    )\n\t    serial = p.serialize()\n\t    result = run_pipeline.apply_async(kwargs = {\"name\": \"example00\", \n\t                                                 \"inputs\": serial[\"inputs\"], \n", "                                                 \"outputs\": serial[\"outputs\"],\n\t                                                 \"parameters\": serial[\"parameters\"]}, countdown=0.1)\n\t    p.task_id = result.id\n\t    p.status = result.status\n\t    p.task_kwargs = str(\n\t            {\"name\": serial[\"name\"], \n\t            \"inputs\": serial[\"inputs\"], \n\t            \"outputs\": serial[\"outputs\"], \n\t            \"parameters\": serial[\"parameters\"]}\n\t    )\n", "    print(f'Starting {p.name} pipeline with task_id: ' + str(p.task_id))\n\t    p = mock_backend.create(p)\n\t    return p"]}
{"filename": "src/tests/pipelines/__init__.py", "chunked_list": []}
{"filename": "src/tests/pipelines/example00/__init__.py", "chunked_list": []}
{"filename": "src/tests/pipelines/example00/test_pipeline.py", "chunked_list": ["\"\"\"\n\tThis is a boilerplate test file for pipeline 'example00'\n\tgenerated using Kedro 0.18.4.\n\tPlease add your pipeline tests here.\n\tKedro recommends using `pytest` framework, more info about it can be found\n\tin the official documentation:\n\thttps://docs.pytest.org/en/latest/getting-started.html\n\t\"\"\"\n"]}
{"filename": "src/kedro_graphql/decorators.py", "chunked_list": ["import os\n\tfrom .config import RESOLVER_PLUGINS\n\tfrom .config import TYPE_PLUGINS\n\tclass NameConflictError(BaseException):\n\t    \"\"\"Raise for errors in adding plugins do to the same name.\"\"\"\n\tdef gql_resolver(name):\n\t    \"\"\"\n\t    \"\"\"\n\t    if name in RESOLVER_PLUGINS:\n\t        raise NameConflictError(\n", "            f\"Plugin name conflict: '{name}'. Double check\" \\\n\t            \" that all plugins have unique names.\"\n\t        )\n\t    def register_plugin(plugin_class):\n\t        plugin = plugin_class()\n\t        RESOLVER_PLUGINS[name] = plugin\n\t        print(\"registered resolver plugin\",\"'\" + name + \"'\", RESOLVER_PLUGINS[name])\n\t        return plugin\n\t    return register_plugin\n\tdef gql_query():\n", "    \"\"\"\n\t    \"\"\"\n\t    ## raise warning if same class is registered twice?\n\t    ##if type not in TYPE_PLUGINS.keys():\n\t    ##    raise KeyError(\n\t    ##        f\"Type plugin error: '{type}', must be one of ['query', 'mutation', or 'subscription']\"\n\t    ##    )\n\t    def register_plugin(plugin_class):\n\t        TYPE_PLUGINS[\"query\"].append(plugin_class)\n\t        print(\"registered type plugin 'query':\", plugin_class)\n", "        return plugin_class\n\t    return register_plugin\n\tdef gql_mutation():\n\t    \"\"\"\n\t    \"\"\"\n\t    ## raise warning if same class is registered twice?\n\t    ##if type not in TYPE_PLUGINS.keys():\n\t    ##    raise KeyError(\n\t    ##        f\"Type plugin error: '{type}', must be one of ['query', 'mutation', or 'subscription']\"\n\t    ##    )\n", "    def register_plugin(plugin_class):\n\t        TYPE_PLUGINS[\"mutation\"].append(plugin_class)\n\t        print(\"registered type plugin 'query':\", plugin_class)\n\t        return plugin_class\n\t    return register_plugin\n\tdef gql_subscription():\n\t    \"\"\"\n\t    \"\"\"\n\t    ## raise warning if same class is registered twice?\n\t    ##if type not in TYPE_PLUGINS.keys():\n", "    ##    raise KeyError(\n\t    ##        f\"Type plugin error: '{type}', must be one of ['query', 'mutation', or 'subscription']\"\n\t    ##    )\n\t    def register_plugin(plugin_class):\n\t        TYPE_PLUGINS[\"subscription\"].append(plugin_class)\n\t        print(\"registered type plugin 'query':\", plugin_class)\n\t        return plugin_class\n\t    return register_plugin"]}
{"filename": "src/kedro_graphql/schema.py", "chunked_list": ["from .config import PIPELINES, TYPE_PLUGINS\n\tfrom .events import PipelineEventMonitor\n\timport strawberry\n\tfrom strawberry.tools import merge_types\n\tfrom strawberry.types import Info\n\tfrom typing import AsyncGenerator, List\n\tfrom .celeryapp import app as APP_CELERY\n\tfrom .tasks import run_pipeline\n\tfrom .models import Parameter, DataSet, Pipeline, PipelineInput, PipelineEvent, PipelineLogMessage, PipelineTemplate, Tag\n\tfrom .logs.logger import logger, PipelineLogStream\n", "from fastapi.encoders import jsonable_encoder\n\t@strawberry.type\n\tclass Query:\n\t    @strawberry.field\n\t    def pipeline_templates(self) -> List[PipelineTemplate]:\n\t        pipes = []\n\t        for k,v in PIPELINES.items():\n\t            pipes.append(PipelineTemplate(name = k))\n\t        return pipes\n\t    @strawberry.field\n", "    def pipeline(self, id: str, info: Info) -> Pipeline:\n\t        return info.context[\"request\"].app.backend.load(id)\n\t@strawberry.type\n\tclass Mutation:\n\t    @strawberry.mutation\n\t    def pipeline(self, pipeline: PipelineInput, info: Info) -> Pipeline:\n\t        \"\"\"\n\t        - is validation against template needed, e.g. check DataSet type or at least check dataset names\n\t        \"\"\"\n\t        d = jsonable_encoder(pipeline)\n", "        p = Pipeline.from_dict(d)\n\t        p.task_name = str(run_pipeline)\n\t        serial = p.serialize()\n\t        result = run_pipeline.delay(\n\t            name = serial[\"name\"], \n\t            inputs = serial[\"inputs\"], \n\t            outputs = serial[\"outputs\"], \n\t            parameters = serial[\"parameters\"]\n\t        )  \n\t        p.task_id = result.id\n", "        p.status = result.status\n\t        p.task_kwargs = str(\n\t                {\"name\": serial[\"name\"], \n\t                \"inputs\": serial[\"inputs\"], \n\t                \"outputs\": serial[\"outputs\"], \n\t                \"parameters\": serial[\"parameters\"]}\n\t        )\n\t        ## PLACE HOLDER for future reolver plugins\n\t        ## testing plugin_resolvers, \n\t        #RESOLVER_PLUGINS[\"text_in\"].__input__(\"called text_in resolver\")\n", "        logger.info(f'Starting {p.name} pipeline with task_id: ' + str(p.task_id))\n\t        p = info.context[\"request\"].app.backend.create(p)\n\t        return p\n\t@strawberry.type\n\tclass Subscription:\n\t    @strawberry.subscription\n\t    async def pipeline(self, id: str, info: Info, interval: float = 0.5) -> AsyncGenerator[PipelineEvent, None]:\n\t        \"\"\"Subscribe to pipeline events.\n\t        \"\"\"\n\t        p  = info.context[\"request\"].app.backend.load(id=id)\n", "        if p:\n\t            async for e in PipelineEventMonitor(app = APP_CELERY, task_id = p.task_id).start(interval=interval):\n\t                e[\"id\"] = id\n\t                yield PipelineEvent(**e)\n\t    @strawberry.subscription\n\t    async def pipeline_logs(self, id: str, info: Info) -> AsyncGenerator[PipelineLogMessage, None]:\n\t        p  = info.context[\"request\"].app.backend.load(id=id)\n\t        if p:\n\t            stream = await PipelineLogStream().create(task_id = p.task_id )\n\t            async for e in stream.consume():\n", "                e[\"id\"] = id\n\t                yield PipelineLogMessage(**e)\n\tdef build_schema():\n\t    ComboQuery = merge_types(\"Query\", tuple([Query] + TYPE_PLUGINS[\"query\"]))\n\t    ComboMutation = merge_types(\"Mutation\", tuple([Mutation] + TYPE_PLUGINS[\"mutation\"]))\n\t    ComboSubscription = merge_types(\"Subscription\", tuple([Subscription] + TYPE_PLUGINS[\"subscription\"]))\n\t    return strawberry.Schema(query=ComboQuery, mutation=ComboMutation, subscription=ComboSubscription)\n"]}
{"filename": "src/kedro_graphql/settings.py", "chunked_list": ["\"\"\"Project settings. There is no need to edit this file unless you want to change values\n\tfrom the Kedro defaults. For further information, including these default values, see\n\thttps://kedro.readthedocs.io/en/stable/kedro_project_setup/settings.html.\"\"\"\n\t# Instantiated project hooks.\n\t# from kedro_graphql.hooks import ProjectHooks\n\t# HOOKS = (ProjectHooks(),)\n\t# Installed plugins for which to disable hook auto-registration.\n\t# DISABLE_HOOKS_FOR_PLUGINS = (\"kedro-viz\",)\n\t# Class that manages storing KedroSession data.\n\t# from kedro.framework.session.shelvestore import ShelveStore\n", "# SESSION_STORE_CLASS = ShelveStore\n\t# Keyword arguments to pass to the `SESSION_STORE_CLASS` constructor.\n\t# SESSION_STORE_ARGS = {\n\t#     \"path\": \"./sessions\"\n\t# }\n\t# Class that manages Kedro's library components.\n\t# from kedro.framework.context import KedroContext\n\t# CONTEXT_CLASS = KedroContext\n\t# Directory that holds configuration.\n\t# CONF_SOURCE = \"conf\"\n", "# Class that manages how configuration is loaded.\n\t# CONFIG_LOADER_CLASS = ConfigLoader\n\t# Keyword arguments to pass to the `CONFIG_LOADER_CLASS` constructor.\n\t# CONFIG_LOADER_ARGS = {\n\t#       \"config_patterns\": {\n\t#           \"spark\" : [\"spark*/\"],\n\t#           \"parameters\": [\"parameters*\", \"parameters*/**\", \"**/parameters*\"],\n\t#       }\n\t# }\n\t# Class that manages the Data Catalog.\n", "# from kedro.io import DataCatalog\n\t# DATA_CATALOG_CLASS = DataCatalog\n"]}
{"filename": "src/kedro_graphql/events.py", "chunked_list": ["import asyncio\n\tfrom queue import Queue\n\tfrom queue import Empty as QueueEmptyException\n\tfrom threading import Thread\n\timport logging\n\tfrom typing import AsyncGenerator\n\timport time\n\tfrom celery.states import READY_STATES, EXCEPTION_STATES\n\tlogger = logging.getLogger(\"kedro-graphql\")\n\tclass PipelineEventMonitor:\n", "    def __init__(self, app = None, task_id = None, timeout = 1):\n\t        \"\"\"\n\t        Kwargs:\n\t            app (Celery): celery application instance.\n\t            uuid (str): a celery task id.\n\t            timeout (float): See https://docs.python.org/3/library/queue.html#queue.Queue.get\n\t        \"\"\"\n\t        self.task_id = task_id\n\t        self.app = app\n\t        self.timeout = timeout\n", "    @staticmethod\n\t    def _task_event_receiver(app, queue, task_id):\n\t        \"\"\"\n\t        Recieves task events from backend broker and puts them in a \n\t        Queue.  Incoming tasks are filtered and only tasks with a \n\t        root_id or uuid matching the provided id are put in the Queue.\n\t        Example event payloads:\n\t        {'hostname': 'gen36975@alligator', 'utcoffset': 5, 'pid': 36975, 'clock': 7864, 'uuid': 'd8253d45-ce28-4719-b2ba-8e266dfdaf04', 'root_id': 'd8253d45-ce28-4719-b2ba-8e266dfdaf04', 'parent_id': None, 'name': 'kedro_graphql.tasks.run_pipeline', 'args': '()', 'kwargs': \"{'name': 'example00', 'inputs': {'text_in': {'type': 'text.TextDataSet', 'filepath': './data/01_raw/text_in.txt'}}, 'outputs': {'text_out': {'type': 'text.TextDataSet', 'filepath': './data/02_intermediate/text_out.txt'}}}\", 'retries': 0, 'eta': None, 'expires': None, 'queue': 'celery', 'exchange': '', 'routing_key': 'celery', 'timestamp': 1672860581.1371481, 'type': 'task-sent', 'local_received': 1672860581.138474}\n\t        {'hostname': 'celery@alligator', 'utcoffset': 5, 'pid': 37029, 'clock': 7867, 'uuid': 'd8253d45-ce28-4719-b2ba-8e266dfdaf04', 'timestamp': 1672860581.1411166, 'type': 'task-started', 'local_received': 1672860581.144976}\n\t        {'hostname': 'celery@alligator', 'utcoffset': 5, 'pid': 37029, 'clock': 7870, 'uuid': 'd8253d45-ce28-4719-b2ba-8e266dfdaf04', 'result': \"'success'\", 'runtime': 2.013245126003312, 'timestamp': 1672860583.1549191, 'type': 'task-succeeded', 'local_received': 1672860583.158338}\n", "        Args:\n\t            app (Celery): celery application instance.\n\t            queue (Queue):  a python queue.Queue.\n\t            task_id (str):  celery task id.\n\t        \"\"\"\n\t        def process_tasks(event):\n\t            if event.get(\"root_id\", \"\") == task_id or event.get(\"uuid\") == task_id:    \n\t                queue.put(event)\n\t        with app.connection() as connection:\n\t            recv = app.events.Receiver(connection, handlers={\n", "                    'task-sent': process_tasks,\n\t                    'task-recieved': process_tasks,\n\t                    'task-started': process_tasks,\n\t                    'task-succeeded': process_tasks,\n\t                    'task-failed': process_tasks,\n\t                    'task-rejected': process_tasks,\n\t                    'task-revoked': process_tasks,\n\t                    'task-retried': process_tasks\n\t            })\n\t            recv.capture(limit=None, timeout=None, wakeup=True)\n", "    def _start_task_event_receiver_thread(self, queue):\n\t        \"\"\"\n\t        Start the task event receiver in a thread.\n\t        Args:\n\t            queue (Queue): a python queue.Queue.\n\t        Returns:\n\t            worker (threading.Thread): a python thread object.\n\t        \"\"\"\n\t        worker = Thread(target=self._task_event_receiver, args=(self.app,queue,self.task_id))\n\t        worker.daemon = True\n", "        worker.start()\n\t        logger.info(\"started event reciever thread\")\n\t        return worker\n\t    async def consume(self) -> AsyncGenerator[dict, None]:\n\t        \"\"\"\n\t        \"\"\"\n\t        q = Queue()\n\t        event_thread = self._start_task_event_receiver_thread(q)\n\t        ## https://docs.celeryq.dev/en/stable/reference/celery.events.state.html#module-celery.events.state\n\t        state = self.app.events.State()\n", "        while True:\n\t            try:\n\t                event = q.get(timeout = self.timeout)\n\t                state.event(event)\n\t                # task name is sent only with -received event, and state\n\t                # will keep track of this for us.\n\t                task = state.tasks.get(event['uuid'])\n\t                yield {\"task_id\": task.id, \"status\": task.state, \"result\": task.result, \"timestamp\": task.timestamp, \"traceback\": task.traceback}\n\t                q.task_done()\n\t                if task.state in READY_STATES:\n", "                    break\n\t            except QueueEmptyException:\n\t                if self.app.AsyncResult(self.task_id).status in READY_STATES:\n\t                    break\n\t                else:\n\t                    continue\n\t        event_thread.join(timeout = 0.1)\n\t    async def start(self, interval = 0.5) -> AsyncGenerator[dict, None]:\n\t        \"\"\"\n\t        A simplified but fully async version of the PipelineEventMonitor().consume() method.\n", "        The PipelineEventMonitor.consume() method relies on celery's native\n\t        real time event processing approach which is syncronous and blocking.\n\t        https://docs.celeryq.dev/en/stable/userguide/monitoring.html#real-time-processing\n\t        \"\"\"\n\t        while True:\n\t            task = self.app.AsyncResult(self.task_id)\n\t            yield {\"task_id\": task.id, \"status\": task.state, \"result\": task.result, \"timestamp\": time.time(), \"traceback\": task.traceback}\n\t            if self.app.AsyncResult(self.task_id).status in READY_STATES:\n\t                break\n\t            await asyncio.sleep(interval)\n"]}
{"filename": "src/kedro_graphql/__main__.py", "chunked_list": ["\"\"\"kedro-graphql file for ensuring the package is executable\n\tas `kedro-graphql` and `python -m kedro_graphql`\n\t\"\"\"\n\timport importlib\n\tfrom pathlib import Path\n\tfrom kedro.framework.cli.utils import KedroCliError, load_entry_points\n\tfrom kedro.framework.project import configure_project\n\tdef _find_run_command(package_name):\n\t    try:\n\t        project_cli = importlib.import_module(f\"{package_name}.cli\")\n", "        # fail gracefully if cli.py does not exist\n\t    except ModuleNotFoundError as exc:\n\t        if f\"{package_name}.cli\" not in str(exc):\n\t            raise\n\t        plugins = load_entry_points(\"project\")\n\t        run = _find_run_command_in_plugins(plugins) if plugins else None\n\t        if run:\n\t            # use run command from installed plugin if it exists\n\t            return run\n\t        # use run command from `kedro.framework.cli.project`\n", "        from kedro.framework.cli.project import run\n\t        return run\n\t    # fail badly if cli.py exists, but has no `cli` in it\n\t    if not hasattr(project_cli, \"cli\"):\n\t        raise KedroCliError(f\"Cannot load commands from {package_name}.cli\")\n\t    return project_cli.run\n\tdef _find_run_command_in_plugins(plugins):\n\t    for group in plugins:\n\t        if \"run\" in group.commands:\n\t            return group.commands[\"run\"]\n", "def main(*args, **kwargs):\n\t    package_name = Path(__file__).parent.name\n\t    configure_project(package_name)\n\t    run = _find_run_command(package_name)\n\t    run(*args, **kwargs)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "src/kedro_graphql/pipeline_registry.py", "chunked_list": ["\"\"\"Project pipelines.\"\"\"\n\tfrom typing import Dict\n\tfrom kedro.framework.project import find_pipelines\n\tfrom kedro.pipeline import Pipeline\n\tdef register_pipelines() -> Dict[str, Pipeline]:\n\t    \"\"\"Register the project's pipelines.\n\t    Returns:\n\t        A mapping from pipeline names to ``Pipeline`` objects.\n\t    \"\"\"\n\t    pipelines = find_pipelines()\n", "    pipelines[\"__default__\"] = sum(pipelines.values())\n\t    return pipelines\n"]}
{"filename": "src/kedro_graphql/tasks.py", "chunked_list": ["from kedro.framework.project import pipelines\n\tfrom kedro.io import DataCatalog\n\tfrom kedro.runner import SequentialRunner\n\tfrom celery import shared_task, Task\n\tfrom .backends import init_backend\n\tfrom fastapi.encoders import jsonable_encoder\n\tclass KedroGraphqlTask(Task):\n\t    _db = None\n\t    @property\n\t    def db(self):\n", "        if self._db is None:\n\t            self._db = init_backend()\n\t        return self._db\n\t    def before_start(self, task_id, args, kwargs):\n\t        \"\"\"Handler called before the task starts.\n\t        .. versionadded:: 5.2\n\t        Arguments:\n\t            task_id (str): Unique id of the task to execute.\n\t            args (Tuple): Original arguments for the task to execute.\n\t            kwargs (Dict): Original keyword arguments for the task to execute.\n", "        Returns:\n\t            None: The return value of this handler is ignored.\n\t        \"\"\"\n\t        self.db.update(task_id = task_id, values = {\"status\": \"STARTED\"})\n\t    def on_success(self, retval, task_id, args, kwargs):\n\t        \"\"\"Success handler.\n\t        Run by the worker if the task executes successfully.\n\t        Arguments:\n\t            retval (Any): The return value of the task.\n\t            task_id (str): Unique id of the executed task.\n", "            args (Tuple): Original arguments for the executed task.\n\t            kwargs (Dict): Original keyword arguments for the executed task.\n\t        Returns:\n\t            None: The return value of this handler is ignored.\n\t        \"\"\"\n\t        self.db.update(task_id = task_id, values = {\"status\": \"SUCCESS\"})\n\t    def on_retry(self, exc, task_id, args, kwargs, einfo):\n\t        \"\"\"Retry handler.\n\t        This is run by the worker when the task is to be retried.\n\t        Arguments:\n", "            exc (Exception): The exception sent to :meth:`retry`.\n\t            task_id (str): Unique id of the retried task.\n\t            args (Tuple): Original arguments for the retried task.\n\t            kwargs (Dict): Original keyword arguments for the retried task.\n\t            einfo (~billiard.einfo.ExceptionInfo): Exception information.\n\t        Returns:\n\t            None: The return value of this handler is ignored.\n\t        \"\"\"\n\t        self.db.update(task_id = task_id, values = {\"status\": \"RETRY\", \"task_exception\": str(exc), \"task_einfo\": str(einfo)})\n\t    def on_failure(self, exc, task_id, args, kwargs, einfo):\n", "        \"\"\"Error handler.\n\t        This is run by the worker when the task fails.\n\t        Arguments:\n\t            exc (Exception): The exception raised by the task.\n\t            task_id (str): Unique id of the failed task.\n\t            args (Tuple): Original arguments for the task that failed.\n\t            kwargs (Dict): Original keyword arguments for the task that failed.\n\t            einfo (~billiard.einfo.ExceptionInfo): Exception information.\n\t        Returns:\n\t            None: The return value of this handler is ignored.\n", "        \"\"\"\n\t        self.db.update(task_id = task_id, values = {\"status\": \"FAILURE\", \"task_exception\": str(exc), \"task_einfo\": str(einfo)})\n\t    def after_return(self, status, retval, task_id, args, kwargs, einfo):\n\t        \"\"\"Handler called after the task returns.\n\t        Arguments:\n\t            status (str): Current task state.\n\t            retval (Any): Task return value/exception.\n\t            task_id (str): Unique id of the task.\n\t            args (Tuple): Original arguments for the task.\n\t            kwargs (Dict): Original keyword arguments for the task.\n", "            einfo (~billiard.einfo.ExceptionInfo): Exception information.\n\t        Returns:\n\t            None: The return value of this handler is ignored.\n\t        \"\"\"\n\t        self.db.update(task_id = task_id, values = {\"status\": status, \"task_einfo\": str(einfo)})\n\t@shared_task(bind = True, base = KedroGraphqlTask)\n\tdef run_pipeline(self, name: str, inputs: dict, outputs: dict, parameters: dict):\n\t    catalog = {**inputs, **outputs}\n\t    io = DataCatalog().from_config(catalog = catalog)\n\t    ## add parameters to DataCatalog e.g. {\"params:myparam\":\"value\"}\n", "    params = {\"params:\"+k:v for k,v in parameters.items()}\n\t    params[\"parameters\"] = parameters\n\t    io.add_feed_dict(params)\n\t    SequentialRunner().run(pipelines[name], catalog = io)\n\t    return \"success\""]}
{"filename": "src/kedro_graphql/config.py", "chunked_list": ["from kedro.framework.startup import bootstrap_project\n\tfrom kedro.framework.session import KedroSession\n\tfrom kedro.framework.project import pipelines as PIPELINES\n\tfrom pathlib import Path\n\tfrom dotenv import dotenv_values\n\timport os\n\tfrom importlib import import_module\n\t## pyproject.toml located in parent directory, required for project config\n\tproject_path = Path.cwd()\n\tmetadata = bootstrap_project(project_path)\n", "session = KedroSession.create(metadata.package_name, project_path=project_path)\n\tcontext = session.load_context()\n\tconf_catalog = context.config_loader[\"catalog\"]\n\tconf_parameters = context.config_loader[\"parameters\"]\n\t## define defaults\n\tconfig = {\n\t    \"MONGO_URI\": \"mongodb://root:example@localhost:27017/\",\n\t    \"MONGO_DB_NAME\": \"pipelines\",\n\t    \"KEDRO_GRAPHQL_IMPORTS\": \"kedro_graphql.plugins.plugins,\",\n\t    \"KEDRO_GRAPHQL_APP\": \"kedro_graphql.asgi.KedroGraphQL\",\n", "    \"KEDRO_GRAPHQL_BACKEND\": \"kedro_graphql.backends.mongodb.MongoBackend\",\n\t    \"KEDRO_GRAPHQL_BROKER\": \"redis://localhost\",\n\t    \"KEDRO_GRAPHQL_CELERY_RESULT_BACKEND\": \"redis://localhost\",\n\t    }\n\tload_config = {\n\t    **dotenv_values(\".env\"),  # load \n\t    **os.environ,  # override loaded values with environment variables\n\t}\n\t## override defaults\n\tconfig.update(load_config)\n", "RESOLVER_PLUGINS = {}\n\tTYPE_PLUGINS = {\"query\":[],\n\t                \"mutation\":[],\n\t                \"subscription\":[]}\n\tdef discover_plugins():\n\t    ## discover plugins e.g. decorated functions e.g @gql_query, etc...\n\t    imports = [i.strip() for i in config[\"KEDRO_GRAPHQL_IMPORTS\"].split(\",\") if len(i.strip()) > 0]\n\t    for i in imports:\n\t        import_module(i)   "]}
{"filename": "src/kedro_graphql/celeryapp.py", "chunked_list": ["from celery import Celery\n\tfrom .config import config\n\tapp = Celery()\n\tclass Config:\n\t    broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]\n\t    result_backend = config[\"KEDRO_GRAPHQL_CELERY_RESULT_BACKEND\"]\n\t    result_extended = True\n\t    task_serializer = 'json'\n\t    result_serializer = 'json'\n\t    accept_content = ['json']\n", "    enable_utc = True\n\t    worker_send_task_events = True\n\t    task_send_sent_event = True\n\t    imports = \"kedro_graphql.tasks\"\n\tapp.config_from_object(Config)"]}
{"filename": "src/kedro_graphql/models.py", "chunked_list": ["import strawberry\n\tfrom enum import Enum\n\tfrom typing import List, Optional\n\timport uuid\n\tfrom .config import conf_catalog, conf_parameters, PIPELINES\n\t@strawberry.type\n\tclass Tag:\n\t    key: str\n\t    value: str\n\t@strawberry.input\n", "class TagInput:\n\t    key: str\n\t    value: str\n\t@strawberry.enum\n\tclass ParameterType(Enum):\n\t    STRING = \"string\"\n\t    BOOLEAN = \"boolean\"\n\t    INTEGER = \"integer\"\n\t    FLOAT = \"float\"\n\t@strawberry.type\n", "class Parameter:\n\t    name: str\n\t    value: str\n\t    type: Optional[ParameterType] = ParameterType.STRING\n\t    def serialize(self) -> dict:\n\t        \"\"\"\n\t        Returns serializable dict in format compatible with kedro.\n\t        \"\"\"\n\t        value = self.value\n\t        if self.type == \"boolean\":\n", "            value = self.value.lower()\n\t            if value == \"true\":\n\t                value = True\n\t            elif value == \"false\":\n\t                value = False\n\t            else:\n\t                raise ValueError(\"Parameter of type BOOL must be one of 'True', 'true', 'False', or 'false'\")\n\t        elif self.type == \"integer\":\n\t            value = int(self.value)\n\t        elif self.type == \"float\":\n", "            value = float(self.value)\n\t        return {self.name: value}\n\t@strawberry.input\n\tclass ParameterInput:\n\t    name: str\n\t    value: str\n\t    type: Optional[ParameterType] = ParameterType.STRING\n\t@strawberry.type\n\tclass DataSet:\n\t    name: str\n", "    type: str\n\t    filepath: str\n\t    save_args: Optional[List[Parameter]] = None\n\t    load_args: Optional[List[Parameter]] = None\n\t    def serialize(self) -> dict:\n\t        \"\"\"\n\t        Returns serializable dict in format compatible with kedro.\n\t        \"\"\"\n\t        temp = self.__dict__.copy()\n\t        temp.pop(\"name\")\n", "        if not temp[\"save_args\"]:\n\t            temp.pop(\"save_args\")\n\t        else:\n\t            temp[\"save_args\"] = {k:v for p in self.__dict__[\"save_args\"] for k,v in p.serialize().items() }\n\t        if not temp[\"load_args\"]:\n\t            temp.pop(\"load_args\")\n\t        else:\n\t            temp[\"load_args\"] = {k:v for p in self.__dict__[\"save_args\"] for k,v in p.serialize().items() }\n\t        return {self.name: temp}\n\t    @staticmethod\n", "    def from_dict(payload):\n\t        \"\"\"\n\t        Return a new DataSet from a dictionary.\n\t        Args:\n\t            payload (dict): dict representing DataSet e.g.\n\t                {\n\t                  \"name\": \"text_in\",\n\t                  \"filepath\": \"./data/01_raw/text_in.txt\",\n\t                  \"type\": \"text.TextDataSet\",\n\t                  \"save_args\":[{\"name\": \"say\", \"value\": \"hello\"}],\n", "                  \"load_args\":[{\"name\": \"say\", \"value\": \"hello\"}]\n\t                }\n\t        \"\"\"\n\t        if payload.get(\"save_args\", False):\n\t            save_args = [Parameter(**p) for p in payload[\"save_args\"]]\n\t        else:\n\t            save_args = None\n\t        if payload.get(\"load_args\", False):\n\t            load_args = [Parameter(**p) for p in payload[\"load_args\"]]\n\t        else:\n", "            load_args = None\n\t        return DataSet(\n\t            name = payload[\"name\"],\n\t            type = payload[\"type\"],\n\t            filepath = payload[\"filepath\"],\n\t            save_args = save_args,\n\t            load_args = load_args\n\t        )\n\t@strawberry.input\n\tclass DataSetInput:\n", "    name: str\n\t    type: str\n\t    filepath: str\n\t    save_args: Optional[List[ParameterInput]] = None\n\t    load_args: Optional[List[ParameterInput]] = None\n\t@strawberry.type\n\tclass Node:\n\t    name: str\n\t    inputs: List[str]\n\t    outputs: List[str]\n", "    tags: List[str]\n\t@strawberry.type(description = \"PipelineTemplates are definitions of Pipelines.  They represent the supported interface for executing a Pipeline.\")\n\tclass PipelineTemplate:\n\t    name: str\n\t    @strawberry.field\n\t    def describe(self) -> str:\n\t        return PIPELINES[self.name].describe()\n\t    @strawberry.field\n\t    def nodes(self) -> List[Node]:\n\t        nodes = PIPELINES[self.name].nodes\n", "        return [Node(name = n.name, inputs = n.inputs, outputs = n.outputs, tags = n.tags) for n in nodes]\n\t    @strawberry.field\n\t    def parameters(self) -> List[Parameter]:\n\t        ## keep track of parameters to avoid returning duplicates    \n\t        params = {}\n\t        for n in PIPELINES[self.name].inputs():\n\t            if n.startswith(\"params:\"):\n\t                name = n.split(\"params:\")[1]\n\t                value = conf_parameters[name]\n\t                if not params.get(name, False):\n", "                    params[name] = value\n\t            elif n == \"parameters\":\n\t                for k,v in conf_parameters.items():\n\t                    if not params.get(k, False):\n\t                        params[k] = v\n\t        return [Parameter(name = k, value = v) for k,v in params.items()]\n\t    @strawberry.field\n\t    def inputs(self) -> List[DataSet]:\n\t        inputs_resolved = []\n\t        for n in PIPELINES[self.name].inputs():\n", "            if not n.startswith(\"params:\") and n != \"parameters\":\n\t                config = conf_catalog[n]\n\t                inputs_resolved.append(DataSet(name = n, filepath = config[\"filepath\"], type = config[\"type\"], save_args = config.get(\"save_args\", None), load_args = config.get(\"load_args\", None)))\n\t        return inputs_resolved\n\t    @strawberry.field\n\t    def outputs(self) -> List[DataSet]:\n\t        outputs_resolved = []\n\t        for n in PIPELINES[self.name].outputs():    \n\t            config = conf_catalog[n]\n\t            outputs_resolved.append(DataSet(name = n, filepath = config[\"filepath\"], type = config[\"type\"], save_args = config.get(\"save_args\", None), load_args = config.get(\"load_args\", None)))\n", "        return outputs_resolved\n\t@strawberry.input(description = \"PipelineInput\")\n\tclass PipelineInput:\n\t    name: str\n\t    parameters: List[ParameterInput]\n\t    inputs: List[DataSetInput]\n\t    outputs: List[DataSetInput]\n\t    tags: Optional[List[TagInput]] = None\n\t@strawberry.type\n\tclass Pipeline:\n", "    id: Optional[uuid.UUID] = None\n\t    inputs: List[DataSet]\n\t    name: str\n\t    outputs: List[DataSet]\n\t    parameters: List[Parameter]\n\t    status: Optional[str] = None\n\t    tags: Optional[List[Tag]] = None\n\t    task_id: Optional[str] = None\n\t    task_name: Optional[str] = None\n\t    task_args: Optional[str] = None\n", "    task_kwargs: Optional[str] = None\n\t    task_request: Optional[str] = None\n\t    task_exception: Optional[str] = None\n\t    task_traceback: Optional[str] = None\n\t    task_einfo: Optional[str] = None\n\t    task_result: Optional[str] = None\n\t    @strawberry.field\n\t    def template(self) -> PipelineTemplate:\n\t        return PipelineTemplate(name = self.name)\n\t    @strawberry.field\n", "    def describe(self) -> str:\n\t        return self.template().describe()\n\t    @strawberry.field\n\t    def nodes(self) -> List[Node]:\n\t        return self.template().nodes()\n\t    def serialize(self):\n\t        inputs = {}\n\t        outputs = {}\n\t        parameters = {}\n\t        for i in self.inputs:\n", "            s = i.serialize()\n\t            inputs.update(s)\n\t        for o in self.outputs:\n\t            s = o.serialize()\n\t            outputs.update(s)\n\t        for p in self.parameters:\n\t            s = p.serialize()\n\t            parameters.update(s)\n\t        return {\n\t            \"id\": self.id,\n", "            \"name\": self.name,\n\t            \"inputs\": inputs,\n\t            \"outputs\": outputs,\n\t            \"parameters\": parameters,\n\t        }\n\t    @staticmethod\n\t    def from_dict(payload):\n\t        if payload[\"tags\"]:\n\t            tags = [Tag(**t) for t in payload[\"tags\"]]\n\t        else:\n", "            tags = None\n\t        return Pipeline(\n\t            id = payload.get(\"id\", None),\n\t            name = payload[\"name\"],\n\t            inputs = [DataSet.from_dict(i) for i in payload[\"inputs\"]],\n\t            outputs = [DataSet.from_dict(o) for o in payload[\"outputs\"]],\n\t            parameters = [Parameter(**p) for p in payload[\"parameters\"]],\n\t            status = payload.get(\"status\", None),\n\t            tags = tags,\n\t            task_id = payload.get(\"task_id\", None),\n", "            task_name = payload.get(\"task_name\", None),\n\t            task_args = payload.get(\"task_args\", None),\n\t            task_kwargs = payload.get(\"task_kwargs\", None),\n\t            task_request = payload.get(\"task_request\", None),\n\t            task_exception = payload.get(\"task_exception\", None),\n\t            task_traceback = payload.get(\"task_traceback\", None),\n\t            task_einfo = payload.get(\"task_einfo\", None),\n\t        )\n\t@strawberry.type\n\tclass PipelineEvent:\n", "    id: str\n\t    task_id: str\n\t    status: str\n\t    result: Optional[str] = None\n\t    timestamp: str\n\t    traceback: Optional[str] = None\n\t@strawberry.type\n\tclass PipelineLogMessage:\n\t    id: str\n\t    message: str\n", "    message_id: str\n\t    task_id: str\n\t    time: str"]}
{"filename": "src/kedro_graphql/__init__.py", "chunked_list": ["\"\"\"kedro-graphql\n\t\"\"\"\n\t__version__ = \"0.3.5\"\n\tfrom .asgi import KedroGraphQL\n"]}
{"filename": "src/kedro_graphql/asgi.py", "chunked_list": ["from fastapi import FastAPI\n\tfrom strawberry.fastapi import GraphQLRouter\n\tfrom .backends import init_backend\n\tfrom .schema import build_schema       \n\tclass KedroGraphQL(FastAPI):\n\t    def __init__(self):\n\t        super(KedroGraphQL, self).__init__()\n\t        self.backend = init_backend()\n\t        @self.on_event(\"startup\")\n\t        def startup_backend():\n", "            self.backend.startup()\n\t        @self.on_event(\"shutdown\")\n\t        def shutdown_backend():\n\t            self.backend.shutdown()\n\t        graphql_app = GraphQLRouter(build_schema())\n\t        self.include_router(graphql_app, prefix = \"/graphql\")\n\t        self.add_websocket_route(\"/graphql\", graphql_app)\n"]}
{"filename": "src/kedro_graphql/commands.py", "chunked_list": ["import click\n\timport uvicorn\n\tfrom importlib import import_module\n\tfrom .config import config, discover_plugins\n\t@click.group(name=\"kedro-graphql\")\n\tdef commands():\n\t    pass\n\t@commands.command()\n\t@click.pass_obj\n\t@click.option(\n", "    \"--app\",\n\t    \"-a\",\n\t    default = config[\"KEDRO_GRAPHQL_APP\"],\n\t    help=\"Application import path\"\n\t)\n\t@click.option(\n\t    \"--imports\",\n\t    \"-i\",\n\t    default = config[\"KEDRO_GRAPHQL_IMPORTS\"],\n\t    help=\"Additional import paths\"\n", ")\n\t@click.option(\n\t    \"--worker\",\n\t    \"-w\",\n\t    is_flag=True,\n\t    default=False,\n\t    help=\"Start a celery worker.\"\n\t)\n\tdef gql(metadata, app, imports, worker):\n\t    \"\"\"Commands for working with kedro-graphql.\"\"\"\n", "    if worker:\n\t        from .celeryapp import app\n\t        worker = app.Worker()\n\t        worker.start()\n\t    else:\n\t        config[\"KEDRO_GRAPHQL_IMPORTS\"] = imports\n\t        config[\"KEDRO_GRAPHQL_APP\"] = app\n\t        discover_plugins()\n\t        module, class_name = config[\"KEDRO_GRAPHQL_APP\"].rsplit(\".\", 1)\n\t        module = import_module(module)\n", "        class_inst = getattr(module, class_name)\n\t        a = class_inst() \n\t        uvicorn.run(a, host=\"0.0.0.0\", port=5000, log_level=\"info\")\n"]}
{"filename": "src/kedro_graphql/plugins/plugins.py", "chunked_list": ["from abc import ABC, abstractmethod\n\timport asyncio\n\tfrom kedro_graphql.decorators import gql_resolver, gql_query, gql_mutation, gql_subscription\n\tfrom kedro_graphql.models import ParameterInput, DataSetInput\n\timport strawberry\n\tfrom typing import AsyncGenerator\n\tclass IOResolverPlugin(ABC):\n\t    \"\"\"\n\t    Implement this class to define custom behavior for pipeline inputs and\n\t    outputs.  The methods of the class will called prior to pipeline \n", "    execution.\n\t    \"\"\"\n\t    @abstractmethod\n\t    def __input__(self, input: ParameterInput | DataSetInput) -> [ParameterInput | DataSetInput]:\n\t        pass\n\t    @abstractmethod \n\t    def __submit__(self, input: ParameterInput | DataSetInput) -> ParameterInput | DataSetInput:\n\t        pass\n\t@gql_resolver(name = \"text_in\")\n\tclass ExampleTextInPlugin(IOResolverPlugin):\n", "    def __input__(self, input: ParameterInput | DataSetInput) -> [ParameterInput | DataSetInput]:\n\t        print(\"plugin example\", input)\n\t        return [input]\n\t    def __submit__(self, input: ParameterInput | DataSetInput) -> ParameterInput | DataSetInput:\n\t        return input\n\t@gql_query()\n\t@strawberry.type\n\tclass ExampleQueryTypePlugin():\n\t    @strawberry.field\n\t    def hello_world(self) -> str:\n", "        return \"Hello World\"\n\t@gql_mutation()\n\t@strawberry.type\n\tclass ExampleMutationTypePlugin():\n\t    @strawberry.mutation\n\t    def hello_world(self, message: str = \"World\") -> str:\n\t        return \"Hello \" + message\n\t@gql_subscription()\n\t@strawberry.type\n\tclass ExampleSubscriptionTypePlugin():\n", "    @strawberry.subscription\n\t    async def hello_world(self, message: str = \"World\", target: int = 11) -> AsyncGenerator[str, None]:\n\t        for i in range(target):\n\t            yield str(i) + \" Hello \" + message\n\t            await asyncio.sleep(0.5)"]}
{"filename": "src/kedro_graphql/plugins/__init__.py", "chunked_list": []}
{"filename": "src/kedro_graphql/pipelines/__init__.py", "chunked_list": []}
{"filename": "src/kedro_graphql/pipelines/example00/pipeline.py", "chunked_list": ["\"\"\"\n\tThis is a boilerplate pipeline 'example00'\n\tgenerated using Kedro 0.18.4\n\t\"\"\"\n\tfrom kedro.pipeline import Pipeline, node, pipeline\n\tfrom .nodes import echo\n\tdef create_pipeline(**kwargs) -> Pipeline:\n\t    return pipeline([\n\t        node(\n\t            func=echo,\n", "            inputs=[\"text_in\", \"params:example\", \"parameters\"],\n\t            outputs=\"text_out\",\n\t            name=\"echo_node\"\n\t        )\n\t    ])\n"]}
{"filename": "src/kedro_graphql/pipelines/example00/__init__.py", "chunked_list": ["\"\"\"\n\tThis is a boilerplate pipeline 'example00'\n\tgenerated using Kedro 0.18.4\n\t\"\"\"\n\tfrom .pipeline import create_pipeline\n\t__all__ = [\"create_pipeline\"]\n\t__version__ = \"0.1\"\n"]}
{"filename": "src/kedro_graphql/pipelines/example00/nodes.py", "chunked_list": ["\"\"\"\n\tThis is a boilerplate pipeline 'example00'\n\tgenerated using Kedro 0.18.4\n\t\"\"\"\n\timport time\n\tdef echo(text: str, example: str, parameters: dict) -> str:\n\t    time.sleep(int(parameters.get(\"duration\", 1)))\n\t    return text"]}
{"filename": "src/kedro_graphql/logs/logger.py", "chunked_list": ["import redis\n\timport redis.asyncio as redis_asyncio\n\timport logging\n\tfrom logging import LogRecord\n\tfrom .json_log_formatter import JSONFormatter ## VerboseJSONFormatter also available\n\tfrom celery.signals import task_prerun, task_postrun\n\tfrom kedro_graphql.config import config\n\tfrom typing import AsyncGenerator\n\tfrom celery.result import AsyncResult\n\tfrom celery.states import READY_STATES\n", "import json\n\timport os\n\tfrom inspect import currentframe, getframeinfo\n\tlogger = logging.getLogger(\"kedro-graphql\")\n\tclass RedisLogStreamPublisher(object):\n\t    def __init__(self, topic, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n\t        self.connection = redis.Redis.from_url(broker_url)\n\t        self.topic = topic\n\t        if not self.connection.exists(self.topic):\n\t            self.connection.xadd(self.topic, json.loads(JSONFormatter().format(LogRecord(\"kedro-graphql\", 20, os.path.abspath(__file__), getframeinfo(currentframe()).lineno, \"Starting log stream\", None, None))))\n", "            ## stream will expire in 24 hours (safety mechanism in case task_postrun fails to delete)\n\t            self.connection.expire(self.topic, 86400)\n\t    def publish(self, data):\n\t        self.connection.xadd(self.topic, data)\n\tclass RedisLogStreamSubscriber(object):\n\t    @classmethod\n\t    async def create(cls, topic, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n\t        \"\"\"Factory method for async instantiation RedisLogStreamSubscriber objects.\n\t        \"\"\"\n\t        self = RedisLogStreamSubscriber()\n", "        self.topic = topic\n\t        self.broker_url = broker_url\n\t        self.connection = await redis_asyncio.from_url(broker_url)\n\t        return self\n\t    async def consume(self, count = 1, start_id = 0):\n\t        r = await self.connection.xread(count = count, streams={self.topic:start_id})\n\t        return r\n\tclass KedroGraphQLLogHandler(logging.StreamHandler):\n\t    def __init__(self, topic, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n\t        logging.StreamHandler.__init__(self)\n", "        self.broker_url = broker_url\n\t        self.topic = topic\n\t        self.broker = RedisLogStreamPublisher(topic, broker_url)\n\t        self.setFormatter(JSONFormatter())\n\t    def emit(self, record):\n\t        msg = self.format(record)\n\t        self.broker.publish(json.loads(msg))\n\t@task_prerun.connect\n\tdef setup_task_logger(task_id, task, args, **kwargs):\n\t    logger = logging.getLogger(\"kedro\")\n", "    handler = KedroGraphQLLogHandler(task_id)\n\t    logger.addHandler(handler)\n\t@task_postrun.connect\n\tdef cleanup_task_logger(task_id, task, args, **kwargs):\n\t    logger = logging.getLogger(\"kedro\")\n\t    logger.info(\"Closing log stream\")\n\t    for handler in logger.handlers:\n\t        if isinstance(handler, KedroGraphQLLogHandler) and handler.topic == task_id:\n\t            handler.flush()\n\t            handler.close()\n", "            handler.broker.connection.delete(task_id) ## delete stream\n\t            handler.broker.connection.close()\n\t    logger.handlers = []\n\tclass PipelineLogStream():\n\t    @classmethod\n\t    async def create(cls, task_id, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n\t        \"\"\"Factory method for async instantiation PipelineLogStream objects.\n\t        \"\"\"\n\t        self = PipelineLogStream()\n\t        self.task_id = task_id\n", "        self.broker_url = broker_url\n\t        self.broker = await RedisLogStreamSubscriber().create(task_id, broker_url)\n\t        return self\n\t    async def consume(self) -> AsyncGenerator[dict, None]:\n\t        start_id = 0\n\t        while True:\n\t            stream_data = await self.broker.consume(count = 1, start_id = start_id)\n\t            if len(stream_data) > 0:\n\t                for id, value in stream_data[0][1]:\n\t                    yield {\"task_id\": self.task_id, \"message_id\": id.decode(), \"message\": value[b\"message\"].decode(), \"time\": value[b\"time\"].decode()}\n", "                ## https://redis-py.readthedocs.io/en/stable/examples/redis-stream-example.html#read-more-data-from-the-stream\n\t                start_id = stream_data[0][1][-1][0]\n\t            else:\n\t                ## check task status\n\t                r = AsyncResult(self.task_id)\n\t                if r.status in READY_STATES:\n\t                    await self.broker.connection.close()\n\t                    break"]}
{"filename": "src/kedro_graphql/logs/__init__.py", "chunked_list": []}
{"filename": "src/kedro_graphql/logs/json_log_formatter.py", "chunked_list": ["## All code below this file is attributed to:\n\t##\n\t## The MIT License (MIT)\n\t## \n\t## Copyright (c) 2015 Marsel Mavletkulov\n\t## \n\t## Permission is hereby granted, free of charge, to any person obtaining a copy\n\t## of this software and associated documentation files (the \"Software\"), to deal\n\t## in the Software without restriction, including without limitation the rights\n\t## to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n", "## copies of the Software, and to permit persons to whom the Software is\n\t## furnished to do so, subject to the following conditions:\n\t## \n\t## The above copyright notice and this permission notice shall be included in all\n\t## copies or substantial portions of the Software.\n\t## \n\t## THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\t## IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\t## FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\t## AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n", "## LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\t## OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\t## SOFTWARE.\n\timport logging\n\tfrom datetime import datetime\n\timport json\n\tBUILTIN_ATTRS = {\n\t    'args',\n\t    'asctime',\n\t    'created',\n", "    'exc_info',\n\t    'exc_text',\n\t    'filename',\n\t    'funcName',\n\t    'levelname',\n\t    'levelno',\n\t    'lineno',\n\t    'module',\n\t    'msecs',\n\t    'message',\n", "    'msg',\n\t    'name',\n\t    'pathname',\n\t    'process',\n\t    'processName',\n\t    'relativeCreated',\n\t    'stack_info',\n\t    'thread',\n\t    'threadName',\n\t}\n", "class JSONFormatter(logging.Formatter):\n\t    \"\"\"JSON log formatter.\n\t    Usage example::\n\t        import logging\n\t        import json_log_formatter\n\t        json_handler = logging.FileHandler(filename='/var/log/my-log.json')\n\t        json_handler.setFormatter(json_log_formatter.JSONFormatter())\n\t        logger = logging.getLogger('my_json')\n\t        logger.addHandler(json_handler)\n\t        logger.info('Sign up', extra={'referral_code': '52d6ce'})\n", "    The log file will contain the following log record (inline)::\n\t        {\n\t            \"message\": \"Sign up\",\n\t            \"time\": \"2015-09-01T06:06:26.524448\",\n\t            \"referral_code\": \"52d6ce\"\n\t        }\n\t    \"\"\"\n\t    json_lib = json\n\t    def format(self, record):\n\t        message = record.getMessage()\n", "        extra = self.extra_from_record(record)\n\t        json_record = self.json_record(message, extra, record)\n\t        mutated_record = self.mutate_json_record(json_record)\n\t        # Backwards compatibility: Functions that overwrite this but don't\n\t        # return a new value will return None because they modified the\n\t        # argument passed in.\n\t        if mutated_record is None:\n\t            mutated_record = json_record\n\t        return self.to_json(mutated_record)\n\t    def to_json(self, record):\n", "        \"\"\"Converts record dict to a JSON string.\n\t        It makes best effort to serialize a record (represents an object as a string)\n\t        instead of raising TypeError if json library supports default argument.\n\t        Note, ujson doesn't support it.\n\t        ValueError and OverflowError are also caught to avoid crashing an app,\n\t        e.g., due to circular reference.\n\t        Override this method to change the way dict is converted to JSON.\n\t        \"\"\"\n\t        try:\n\t            return self.json_lib.dumps(record, default=_json_serializable)\n", "        # ujson doesn't support default argument and raises TypeError.\n\t        # \"ValueError: Circular reference detected\" is raised\n\t        # when there is a reference to object inside the object itself.\n\t        except (TypeError, ValueError, OverflowError):\n\t            try:\n\t                return self.json_lib.dumps(record)\n\t            except (TypeError, ValueError, OverflowError):\n\t                return '{}'\n\t    def extra_from_record(self, record):\n\t        \"\"\"Returns `extra` dict you passed to logger.\n", "        The `extra` keyword argument is used to populate the `__dict__` of\n\t        the `LogRecord`.\n\t        \"\"\"\n\t        return {\n\t            attr_name: record.__dict__[attr_name]\n\t            for attr_name in record.__dict__\n\t            if attr_name not in BUILTIN_ATTRS\n\t        }\n\t    def json_record(self, message, extra, record):\n\t        \"\"\"Prepares a JSON payload which will be logged.\n", "        Override this method to change JSON log format.\n\t        :param message: Log message, e.g., `logger.info(msg='Sign up')`.\n\t        :param extra: Dictionary that was passed as `extra` param\n\t            `logger.info('Sign up', extra={'referral_code': '52d6ce'})`.\n\t        :param record: `LogRecord` we got from `JSONFormatter.format()`.\n\t        :return: Dictionary which will be passed to JSON lib.\n\t        \"\"\"\n\t        extra['message'] = message\n\t        if 'time' not in extra:\n\t            extra['time'] = datetime.utcnow()\n", "        if record.exc_info:\n\t            extra['exc_info'] = self.formatException(record.exc_info)\n\t        return extra\n\t    def mutate_json_record(self, json_record):\n\t        \"\"\"Override it to convert fields of `json_record` to needed types.\n\t        Default implementation converts `datetime` to string in ISO8601 format.\n\t        \"\"\"\n\t        for attr_name in json_record:\n\t            attr = json_record[attr_name]\n\t            if isinstance(attr, datetime):\n", "                json_record[attr_name] = attr.isoformat()\n\t        return json_record\n\tdef _json_serializable(obj):\n\t    try:\n\t        return obj.__dict__\n\t    except AttributeError:\n\t        return str(obj)\n\tclass VerboseJSONFormatter(JSONFormatter):\n\t    \"\"\"JSON log formatter with built-in log record attributes such as log level.\n\t    Usage example::\n", "        import logging\n\t        import json_log_formatter\n\t        json_handler = logging.FileHandler(filename='/var/log/my-log.json')\n\t        json_handler.setFormatter(json_log_formatter.VerboseJSONFormatter())\n\t        logger = logging.getLogger('my_verbose_json')\n\t        logger.addHandler(json_handler)\n\t        logger.error('An error has occured')\n\t    The log file will contain the following log record (inline)::\n\t        {\n\t            \"filename\": \"tests.py\",\n", "            \"funcName\": \"test_file_name_is_testspy\",\n\t            \"levelname\": \"ERROR\",\n\t            \"lineno\": 276,\n\t            \"module\": \"tests\",\n\t            \"name\": \"my_verbose_json\",\n\t            \"pathname\": \"/Users/bob/json-log-formatter/tests.py\",\n\t            \"process\": 3081,\n\t            \"processName\": \"MainProcess\",\n\t            \"stack_info\": null,\n\t            \"thread\": 4664270272,\n", "            \"threadName\": \"MainThread\",\n\t            \"message\": \"An error has occured\",\n\t            \"time\": \"2021-07-04T21:05:42.767726\"\n\t        }\n\t    Read more about the built-in log record attributes\n\t    https://docs.python.org/3/library/logging.html#logrecord-attributes.\n\t    \"\"\"\n\t    def json_record(self, message, extra, record):\n\t        extra['filename'] = record.filename\n\t        extra['funcName'] = record.funcName\n", "        extra['levelname'] = record.levelname\n\t        extra['lineno'] = record.lineno\n\t        extra['module'] = record.module\n\t        extra['name'] = record.name\n\t        extra['pathname'] = record.pathname\n\t        extra['process'] = record.process\n\t        extra['processName'] = record.processName\n\t        if hasattr(record, 'stack_info'):\n\t            extra['stack_info'] = record.stack_info\n\t        else:\n", "            extra['stack_info'] = None\n\t        extra['thread'] = record.thread\n\t        extra['threadName'] = record.threadName\n\t        return super(VerboseJSONFormatter, self).json_record(message, extra, record)"]}
{"filename": "src/kedro_graphql/backends/mongodb.py", "chunked_list": ["from .base import BaseBackend\n\tfrom pymongo import MongoClient\n\tfrom kedro_graphql.models import Pipeline\n\timport uuid\n\tfrom bson.objectid import ObjectId\n\tfrom fastapi.encoders import jsonable_encoder\n\tclass MongoBackend(BaseBackend):\n\t    def __init__(self, uri = None, db = None):\n\t        #self.client = MongoClient(app.config[\"MONGO_URI\"])\n\t        self.client = MongoClient(uri)\n", "        #self.db = self.client[app.config[\"MONGO_DB_NAME\"]]\n\t        self.db = self.client[db]\n\t        #self.app = app\n\t    def startup(self, **kwargs):\n\t        \"\"\"Startup hook.\"\"\"\n\t        print(\"Connected to the MongoDB database!\")\n\t    def shutdown(self, **kwargs):\n\t        \"\"\"Shutdown hook.\"\"\"\n\t        self.client.close()\n\t    def load(self, id: uuid.UUID = None, task_id: str = None):\n", "        \"\"\"Load a pipeline by id or task_id\"\"\"\n\t        if task_id:\n\t            r = self.db[\"pipelines\"].find_one({\"task_id\": task_id})\n\t        else:\n\t            r = self.db[\"pipelines\"].find_one({\"_id\": ObjectId(id)})\n\t        if r:\n\t            id = r.pop(\"_id\")\n\t            p = Pipeline.from_dict(r)\n\t            p.id = str(id)\n\t            return p\n", "        else:\n\t            return None\n\t    def create(self, pipeline: Pipeline):\n\t        \"\"\"Save a pipeline\"\"\"\n\t        j = jsonable_encoder(pipeline)\n\t        created = self.db[\"pipelines\"].insert_one(j)\n\t        created = self.db[\"pipelines\"].find_one({\"_id\": created.inserted_id})\n\t        pipeline.id = created[\"_id\"]\n\t        return pipeline\n\t    def update(self, id: uuid.UUID = None, task_id: str = None, values = {}):\n", "        \"\"\"Update a pipeline using id or task id\"\"\"\n\t        if task_id:\n\t            filter = {'task_id': task_id }\n\t        else:\n\t            filter = {'_id': ObjectId(id)}\n\t        newvalues = { \"$set\": values }\n\t        self.db[\"pipelines\"].update_one(filter, newvalues)\n\t        if task_id:\n\t            p = self.load(task_id = task_id)\n\t        else:\n", "            p = self.load(id = id)\n\t        return p"]}
{"filename": "src/kedro_graphql/backends/base.py", "chunked_list": ["import abc\n\tfrom kedro_graphql.models import Pipeline\n\timport uuid\n\tclass BaseBackend(metaclass=abc.ABCMeta):\n\t    @abc.abstractmethod\n\t    def startup(self, **kwargs):\n\t        \"\"\"Startup hook.\"\"\"\n\t        raise NotImplementedError\n\t    @abc.abstractmethod\n\t    def shutdown(self, **kwargs):\n", "        \"\"\"Shutdown hook.\"\"\"\n\t        raise NotImplementedError\n\t    @abc.abstractmethod\n\t    def load(self, id: uuid.UUID = None, task_id: str = None):\n\t        \"\"\"Load a pipeline by id\"\"\"\n\t        raise NotImplementedError\n\t    @abc.abstractmethod\n\t    def create(self, pipeline: Pipeline):\n\t        \"\"\"Save a pipeline\"\"\"\n\t        raise NotImplementedError\n", "    @abc.abstractmethod\n\t    def update(self, id: uuid.UUID = None, task_id: str = None, values: dict = None):\n\t        \"\"\"Update a pipeline\"\"\"\n\t        raise NotImplementedError"]}
{"filename": "src/kedro_graphql/backends/__init__.py", "chunked_list": ["from kedro_graphql.config import config\n\tfrom importlib import import_module\n\tdef init_backend():\n\t    backend_module, backend_class = config[\"KEDRO_GRAPHQL_BACKEND\"].rsplit(\".\", 1)\n\t    backend_kwargs = {\"uri\": config.get(\"MONGO_URI\"), \"db\": config.get(\"MONGO_DB_NAME\")}\n\t    backend_module = import_module(backend_module)\n\t    backend = getattr(backend_module, backend_class)\n\t    return backend(**backend_kwargs)"]}
{"filename": "src/kedro_graphql/example/app.py", "chunked_list": ["from fastapi.middleware.cors import CORSMiddleware\n\tfrom kedro_graphql import KedroGraphQL\n\tclass MyApp(KedroGraphQL):\n\t    def __init__(self): \n\t        super(MyApp, self).__init__()\n\t        origins = [\n\t            \"http://localhost\",\n\t            \"http://localhost:8080\",\n\t        ]\n\t        self.add_middleware(\n", "            CORSMiddleware,\n\t            allow_origins=origins,\n\t            allow_credentials=True,\n\t            allow_methods=[\"*\"],\n\t            allow_headers=[\"*\"],\n\t        )\n\t        print(\"added CORSMiddleware\")"]}
{"filename": "src/kedro_graphql/example/__init__.py", "chunked_list": []}
