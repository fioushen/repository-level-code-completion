{"filename": "main.py", "chunked_list": ["import sys\n\tfrom app.config import load_openai_api_key\n\tfrom app.input_reader import get_text\n\tfrom app.memory import memorize\n\tfrom app.chat import chat\n\tif len(sys.argv) < 2:\n\t    print(\"Usage: python3 main.py <path_to_document>\")\n\t    sys.exit(1)\n\tdocument_path = sys.argv[1]\n\tload_openai_api_key()\n", "text = get_text(document_path)\n\tmemory_path = memorize(text)\n\tchat(memory_path)\n"]}
{"filename": "install_nltk_data.py", "chunked_list": ["import nltk\n\tdef download_nltk_data():\n\t    nltk.download('punkt')\n\tif __name__ == \"__main__\":\n\t    download_nltk_data()\n"]}
{"filename": "run_predefined.py", "chunked_list": ["from app.config import load_openai_api_key\n\tfrom app.input_reader import get_text\n\tfrom app.memory import memorize\n\tfrom app.chat import chat\n\tdocument_path = 'https://www.mattprd.com/p/the-complete-beginners-guide-to-autonomous-agents'\n\tload_openai_api_key()\n\ttext = get_text(document_path)\n\tmemory_path = memorize(text)\n\tchat(memory_path)\n"]}
{"filename": "processors/audio_processor.py", "chunked_list": ["import os\n\timport tempfile\n\timport math\n\tfrom pydub import AudioSegment\n\tfrom pydub.silence import split_on_silence\n\tfrom api.api_whisper import api_get_transcript\n\tdef process_audio(text_path):\n\t    print('Transcripting the audio file...')\n\t    file_size = os.path.getsize(text_path)\n\t    whisper_api_size_limit = 25 * 1024 * 1024\n", "    if file_size > whisper_api_size_limit:\n\t        print('File is bigger than 25MB. Processing in chunks')\n\t        transcript = process_large_audio(text_path)\n\t    else:\n\t        transcript = api_get_transcript(text_path)\n\t    return transcript\n\tdef process_large_audio(text_path):\n\t    extension = get_extension(text_path)\n\t    sound = AudioSegment.from_file(text_path, format=extension)\n\t    chunks = split_audio_into_chunks(sound)\n", "    return process_each_chunk_and_get_full_transcript(chunks, extension)\n\tdef process_each_chunk_and_get_full_transcript(chunks, extension):\n\t    full_transcript = ''\n\t    for idx, chunk in enumerate(chunks):\n\t        with tempfile.NamedTemporaryFile(delete=True, suffix=f'.{extension}') as temp_file:\n\t            chunk.export(temp_file.name, format=extension)\n\t            transcript_chunk = api_get_transcript(temp_file.name)\n\t            full_transcript += f\" [Chunk {idx + 1}] \" + transcript_chunk\n\t    return full_transcript\n\t# https://github.com/jiaaro/pydub/issues/169\n", "def calculate_silence_thresh(dbfs):\n\t    rounded_down_value = math.floor(dbfs)\n\t    result = rounded_down_value - 2\n\t    return result\n\tdef split_audio_into_chunks(sound):\n\t    chunks = split_on_silence(\n\t        sound,\n\t        min_silence_len=1000,\n\t        silence_thresh=calculate_silence_thresh(sound.dBFS),\n\t        keep_silence=100,\n", "        seek_step=100\n\t    )\n\t    ten_minutes = 10 * 60 * 1000  # 10 minutes will produce files smaller than 25 MB\n\t    target_length = ten_minutes\n\t    output_chunks = [chunks[0]]\n\t    for chunk in chunks[1:]:\n\t        if len(output_chunks[-1]) < target_length:\n\t            output_chunks[-1] += chunk\n\t        else:\n\t            output_chunks.append(chunk)\n", "    return output_chunks\n\tdef get_extension(text_path):\n\t    return os.path.splitext(text_path)[1][1:]\n"]}
{"filename": "processors/pdf_processor.py", "chunked_list": ["import fitz\n\tdef process_pdf(text_path):\n\t    full_text = \"\"\n\t    num_pages = 0\n\t    with fitz.open(text_path) as doc:\n\t        for page in doc:\n\t            num_pages += 1\n\t            text = page.get_text()\n\t            full_text += text + \"\\n\"\n\t    return f\"This is a {num_pages}-page document.\\n\" + full_text\n"]}
{"filename": "processors/txt_processor.py", "chunked_list": ["def process_txt(text_path):\n\t    with open(text_path, 'r', encoding='utf8') as f:\n\t        lines = f.readlines()\n\t    return '\\n'.join(lines)"]}
{"filename": "processors/doc_processor.py", "chunked_list": ["import docx\n\tdef process_doc(text_path):\n\t    doc = docx.Document(text_path)\n\t    full_text = []\n\t    for para in doc.paragraphs:\n\t        full_text.append(para.text)\n\t    return '\\n'.join(full_text)\n"]}
{"filename": "processors/epub_processor.py", "chunked_list": ["import os\n\timport ebooklib\n\tfrom bs4 import BeautifulSoup\n\tfrom ebooklib import epub\n\tdef process_epub(epub_path):\n\t    if not os.path.exists(epub_path):\n\t        raise FileNotFoundError(f\"File not found: {epub_path}\")\n\t    chapters = epub2thtml(epub_path)\n\t    text = thtml2ttext(chapters)\n\t    delimiter = \" \"\n", "    return delimiter.join(text)\n\tdef epub2thtml(epub_path):\n\t    book = epub.read_epub(epub_path)\n\t    chapters = []\n\t    for item in book.get_items():\n\t        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n\t            chapters.append(item.get_content())\n\t    return chapters\n\tblacklist = [\n\t    '[document]',\n", "    'noscript',\n\t    'header',\n\t    'html',\n\t    'meta',\n\t    'head',\n\t    'input',\n\t    'script',\n\t]\n\tdef chap2text(chap):\n\t    output = ''\n", "    soup = BeautifulSoup(chap, 'html.parser')\n\t    text = soup.find_all(text=True)\n\t    for t in text:\n\t        if t.parent.name not in blacklist:\n\t            output += '{} '.format(t)\n\t    return output\n\tdef thtml2ttext(thtml):\n\t    output = []\n\t    for html in thtml:\n\t        text = chap2text(html)\n", "        output.append(text)\n\t    return output\n"]}
{"filename": "processors/__init__.py", "chunked_list": []}
{"filename": "processors/url_processor.py", "chunked_list": ["import requests\n\tfrom bs4 import BeautifulSoup\n\tdummy_headers = {\n\t    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n\t}\n\tdef process_url(url):\n\t    print('URL detected, downloading & parsing...')\n\t    response = requests.get(url, headers=dummy_headers)\n\t    if response.status_code == 200:\n\t        soup = BeautifulSoup(response.content, \"html.parser\")\n", "        text = soup.get_text()\n\t    else:\n\t        raise ValueError(f\"Invalid URL! Status code {response.status_code}.\")\n\t    return text\n"]}
{"filename": "actions/embeddings.py", "chunked_list": ["import os\n\tfrom api.api_embeddings import api_get_embeddings\n\tfrom opensource.instructor_embeddings import get_free_embeddings\n\tuse_open_ai_api = os.getenv('USE_OPEN_AI_API') == 'True'\n\tdef get_embeddings(text):\n\t    if use_open_ai_api:\n\t        return api_get_embeddings(text)\n\t    return get_free_embeddings(text)\n"]}
{"filename": "actions/__init__.py", "chunked_list": []}
{"filename": "api/api_embeddings.py", "chunked_list": ["import openai\n\timport backoff\n\tmodel = \"text-embedding-ada-002\"\n\t@backoff.on_exception(backoff.expo, (openai.error.RateLimitError, openai.error.OpenAIError), max_tries=10)\n\tdef api_get_embeddings(text):\n\t    text = text.replace(\"\\n\", \" \")\n\t    return openai.Embedding.create(\n\t        input=[text],\n\t        model=model)['data'][0]['embedding']\n"]}
{"filename": "api/__init__.py", "chunked_list": []}
{"filename": "api/api_whisper.py", "chunked_list": ["import openai\n\timport backoff\n\t@backoff.on_exception(backoff.expo, (openai.error.RateLimitError, openai.error.OpenAIError), max_tries=10)\n\tdef api_get_transcript(text_path):\n\t    with open(text_path, 'rb') as f:\n\t        response = openai.Audio.transcribe(\n\t            file=f,\n\t            model='whisper-1',\n\t            response_format='text'\n\t        )\n", "    return response\n"]}
{"filename": "api/api_completion.py", "chunked_list": ["import openai\n\timport backoff\n\t@backoff.on_exception(backoff.expo, (openai.error.RateLimitError, openai.error.OpenAIError), max_tries=10)\n\tdef api_get_completion(content):\n\t    messages = [\n\t        {\"role\": \"user\", \"content\": content}\n\t    ]\n\t    completion = openai.ChatCompletion.create(\n\t        model='gpt-3.5-turbo',\n\t        messages=messages,\n", "        temperature=0.2,\n\t        top_p=0.95,\n\t        # max_tokens=2000,\n\t        frequency_penalty=0.0,\n\t        presence_penalty=0.0\n\t    )\n\t    return completion.choices[0].message.content\n"]}
{"filename": "opensource/__init__.py", "chunked_list": []}
{"filename": "opensource/instructor_embeddings.py", "chunked_list": ["from InstructorEmbedding import INSTRUCTOR\n\tmodel = INSTRUCTOR('hkunlp/instructor-xl')\n\tdef get_free_embeddings(text):\n\t    return model.encode(text).tolist()\n"]}
{"filename": "app/config.py", "chunked_list": ["import os\n\timport openai\n\timport tiktoken\n\ttokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\tdef load_openai_api_key():\n\t    openai.api_key = os.environ.get(\"OPEN_API_TOKEN\")\n\t    if openai.api_key:\n\t        print(\"Loaded openai api key.\")\n\t    else:\n\t        raise ValueError(\"Environment variable 'OPEN_API_TOKEN' not found.\")\n", "class bcolors:\n\t    HEADER = '\\033[95m'\n\t    OKBLUE = '\\033[94m'\n\t    OKCYAN = '\\033[96m'\n\t    OKGREEN = '\\033[92m'\n\t    WARNING = '\\033[93m'\n\t    FAIL = '\\033[91m'\n\t    ENDC = '\\033[0m'\n\t    BOLD = '\\033[1m'\n\t    UNDERLINE = '\\033[4m'\n"]}
{"filename": "app/__init__.py", "chunked_list": []}
{"filename": "app/memory.py", "chunked_list": ["import hashlib\n\timport json\n\timport os\n\timport jsonlines\n\tfrom tqdm import tqdm\n\timport time\n\tfrom actions.embeddings import get_embeddings\n\tfrom api.api_completion import api_get_completion\n\tfrom app.config import tokenizer\n\tdef is_uninformative(chunk):\n", "    too_short = len(chunk) < 10\n\t    worthless = len(tokenizer.encode(chunk)) > len(chunk) * 3\n\t    return too_short | worthless\n\tdef process_chunk(chunk, info):\n\t    if is_uninformative(chunk):\n\t        print(\"Skipped an uninformative chunk.\")\n\t        print(chunk)\n\t        return info\n\t    summary = get_summary(chunk)\n\t    embd = get_embeddings(chunk)\n", "    summary_embd = get_embeddings(summary)\n\t    item = {\n\t        \"id\": len(info),\n\t        \"text\": chunk,\n\t        \"embd\": embd,\n\t        \"summary\": summary,\n\t        \"summary_embd\": summary_embd,\n\t    }\n\t    info.append(item)\n\t    return info\n", "def store_info(text, memory_path, chunk_sz=800, max_memory=150):\n\t    info = []\n\t    text = text.replace(\"\\n\", \" \").split()\n\t    if (len(text) / chunk_sz) >= max_memory:\n\t        raise ValueError(\"Processing is aborted due to high anticipated costs.\")\n\t    for idx in tqdm(range(0, len(text), chunk_sz)):\n\t        chunk = \" \".join(text[idx: idx + chunk_sz])\n\t        info = process_chunk(chunk, info)\n\t        time.sleep(3)  # up to 20 api calls per min\n\t    with jsonlines.open(memory_path, mode=\"w\") as f:\n", "        f.write(info)\n\t        print(\"Finish storing info.\")\n\tdef load_info(memory_path):\n\t    with open(memory_path, 'r', encoding='utf8') as f:\n\t        for line in f:\n\t            info = json.loads(line)\n\t    return info\n\tdef memorize(text):\n\t    sha = hashlib.sha256(text.encode('UTF-8')).hexdigest()\n\t    memory_path = f\"memory/{sha}.json\"\n", "    file_exists = os.path.exists(memory_path)\n\t    if file_exists:\n\t        print(\"Detected cached memories.\")\n\t    else:\n\t        print(\"Memorizing...\")\n\t        store_info(text, memory_path)\n\t    return memory_path\n\tdef get_summary(chunk):\n\t    content = \"The following is a passage fragment. Please summarize what information the readers can take away from it:\"\n\t    content += \"\\n\" + chunk\n", "    return api_get_completion(content)\n"]}
{"filename": "app/input_reader.py", "chunked_list": ["import os\n\timport validators\n\tfrom app.github_reader import is_github_url, clone_repository, process_repository, remove_repositories\n\tfrom app.youtube_url_downloader import is_youtube_url, download_audio\n\tfrom processors.audio_processor import process_audio\n\tfrom processors.doc_processor import process_doc\n\tfrom processors.epub_processor import process_epub\n\tfrom processors.pdf_processor import process_pdf\n\tfrom processors.txt_processor import process_txt\n\tfrom processors.url_processor import process_url\n", "def unsupported_file_type(_):\n\t    raise ValueError(\"Invalid document path!\")\n\tdef get_text(text_path):\n\t    suffix = os.path.splitext(text_path)[-1].lower()\n\t    if validators.url(text_path):\n\t        if is_youtube_url(text_path):\n\t            audio_path = download_audio(text_path)\n\t            text = process_audio(audio_path)\n\t        elif is_github_url(text_path):\n\t            repo_directory = \"cloned_repo\"\n", "            clone_repository(text_path, repo_directory)\n\t            text = process_repository(repo_directory)\n\t            remove_repositories(repo_directory)\n\t        else:\n\t            text = process_url(text_path)\n\t    else:\n\t        process_map = {\n\t            \".epub\": process_epub,\n\t            \".pdf\": process_pdf,\n\t            \".doc\": process_doc,\n", "            \".docx\": process_doc,\n\t            \".txt\": process_txt,\n\t            \".wav\": process_audio,\n\t            \".mp3\": process_audio,\n\t            \".m4a\": process_audio,\n\t        }\n\t        processor = process_map.get(suffix, unsupported_file_type)\n\t        text = processor(text_path)\n\t    text = \" \".join(text.split())\n\t    return text\n"]}
{"filename": "app/youtube_url_downloader.py", "chunked_list": ["import os\n\timport re\n\tfrom pytube import YouTube\n\tdef download_audio(yt_url):\n\t    print('Detected YouTube URL. Attempting to download the video...')\n\t    yt = YouTube(yt_url, use_oauth=True, allow_oauth_cache=True)\n\t    audio_streams = yt.streams.filter(only_audio=True)\n\t    audio_stream = audio_streams.order_by(\"abr\").desc().first()\n\t    output_dir = \"downloads\"\n\t    file_name = f\"{yt.title}-{yt.video_id}.{audio_stream.subtype}\"\n", "    full_file_path = os.path.join(output_dir, file_name)\n\t    if not os.path.exists(full_file_path):\n\t        audio_stream.download(output_path=output_dir, filename=file_name)\n\t        print(f'File downloaded: {file_name}')\n\t    else:\n\t        print(f'File already exists: {file_name}')\n\t    return full_file_path\n\tdef is_youtube_url(url):\n\t    youtube_pattern = re.compile(\n\t        r'(https?://)?(www\\.)?'\n", "        r'(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n\t        r'(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})'\n\t    )\n\t    return bool(youtube_pattern.match(url))\n"]}
{"filename": "app/chat.py", "chunked_list": ["import time\n\timport numpy as np\n\tfrom numpy.linalg import norm\n\tfrom actions.embeddings import get_embeddings\n\tfrom api.api_completion import api_get_completion\n\tfrom app.config import tokenizer, bcolors\n\tfrom app.memory import load_info\n\tdef get_question():\n\t    q = input(\"Enter your question: \")\n\t    return q\n", "def retrieve(q_embd, info):\n\t    # return the indices of top three related texts\n\t    text_embds = []\n\t    summary_embds = []\n\t    for item in info:\n\t        text_embds.append(item[\"embd\"])\n\t        summary_embds.append(item[\"summary_embd\"])\n\t    # compute the cos sim between info_embds and q_embd\n\t    text_cos_sims = np.dot(text_embds, q_embd) / (norm(text_embds, axis=1) * norm(q_embd))\n\t    summary_cos_sims = np.dot(summary_embds, q_embd) / (norm(summary_embds, axis=1) * norm(q_embd))\n", "    cos_sims = text_cos_sims + summary_cos_sims\n\t    top_args = np.argsort(cos_sims).tolist()\n\t    top_args.reverse()\n\t    indices = top_args[0:3]\n\t    return indices\n\tdef get_qa_content(q, retrieved_text):\n\t    content = \"After reading some relevant passage fragments from the same document, please respond to the following query. Note that there may be typographical errors in the passages due to the text being fetched from a PDF file or web page.\"\n\t    content += \"\\nQuery: \" + q\n\t    for i in range(len(retrieved_text)):\n\t        content += \"\\nPassage \" + str(i + 1) + \": \" + retrieved_text[i]\n", "    content += \"\\nAvoid explicitly using terms such as 'passage 1, 2 or 3' in your answer as the questioner may not know how the fragments are retrieved. You can use your own knowledge in addition to the provided information to enhance your response. Please use the same language as in the query to respond, to ensure that the questioner can understand.\"\n\t    return content\n\tdef generate_answer(q, retrieved_indices, info):\n\t    while True:\n\t        sorted_indices = sorted(retrieved_indices)\n\t        retrieved_text = [info[idx][\"text\"] for idx in sorted_indices]\n\t        content = get_qa_content(q, retrieved_text)\n\t        if len(tokenizer.encode(content)) > 3800:\n\t            retrieved_indices = retrieved_indices[:-1]\n\t            print(\"Contemplating...\")\n", "            if not retrieved_indices:\n\t                raise ValueError(\"Failed to respond.\")\n\t        else:\n\t            break\n\t    return api_get_completion(content)\n\tdef answer(q, info):\n\t    q_embd = get_embeddings(q)\n\t    retrieved_indices = retrieve(q_embd, info)\n\t    return generate_answer(q, retrieved_indices, info)\n\tdef chat(memory_path):\n", "    info = load_info(memory_path)\n\t    while True:\n\t        q = get_question()\n\t        if len(tokenizer.encode(q)) > 200:\n\t            raise ValueError(\"Input query is too long!\")\n\t        response = answer(q, info)\n\t        print()\n\t        print(f\"{bcolors.OKGREEN}{response}{bcolors.ENDC}\")\n\t        print()\n\t        time.sleep(3)  # up to 20 api calls per min\n"]}
{"filename": "app/github_reader.py", "chunked_list": ["import os\n\timport shutil\n\timport subprocess\n\tdef is_github_url(url):\n\t    return \"github.com\" in url\n\tdef clone_repository(url, directory):\n\t    subprocess.check_call([\"git\", \"clone\", url, directory])\n\tdef process_repository(repo_directory):\n\t    if not os.path.exists(\"gpt-repository-loader\"):\n\t        clone_repository(\"https://github.com/mpoon/gpt-repository-loader\", \"gpt-repository-loader\")\n", "    absolute_repo_directory = os.path.abspath(repo_directory)\n\t    subprocess.check_call([\"python3\", \"gpt_repository_loader.py\", absolute_repo_directory, \"-o\", \"output.txt\"],\n\t                          cwd=\"gpt-repository-loader\")\n\t    with open(\"gpt-repository-loader/output.txt\", \"r\") as f:\n\t        return f.read()\n\tdef remove_repositories(repo_directory):\n\t    try:\n\t        shutil.rmtree(repo_directory)\n\t    except FileNotFoundError:\n\t        print(f\"Warning: {repo_directory} not found\")\n", "    try:\n\t        shutil.rmtree(\"gpt-repository-loader\")\n\t    except FileNotFoundError:\n\t        print(\"Warning: gpt-repository-loader not found\")\n"]}
{"filename": "app/tests/test_youtube_file_downloader.py", "chunked_list": ["from app.youtube_url_downloader import is_youtube_url\n\tdef test_is_youtube_url():\n\t    assert is_youtube_url(\"https://www.youtube.com/watch?v=8OAPLk20epo\")\n\t    assert is_youtube_url(\"http://youtube.com/watch?v=8OAPLk20epo\")\n\t    assert is_youtube_url(\"www.youtube.com/watch?v=8OAPLk20epo\")\n\t    assert is_youtube_url(\"youtube.com/watch?v=8OAPLk20epo\")\n\t    assert is_youtube_url(\"https://youtu.be/8OAPLk20epo\")\n\t    assert is_youtube_url(\"https://www.youtube-nocookie.com/embed/8OAPLk20epo\")\n\t    assert not is_youtube_url(\"https://www.example.com/watch?v=8OAPLk20epo\")\n\t    assert not is_youtube_url(\"https://www.youtubee.com/watch?v=8OAPLk20epo\")\n", "    assert not is_youtube_url(\"https://www.youtu.be.com/watch?v=8OAPLk20epo\")\n"]}
{"filename": "app/tests/test_memory.py", "chunked_list": ["import pytest\n\tfrom app.memory import process_chunk, store_info, load_info, memorize, get_summary\n\tdef test_process_chunk_uninformative(mocker):\n\t    mocker.patch(\"app.memory.get_summary\")\n\t    mocker.patch(\"app.memory.get_embedding\")\n\t    info = []\n\t    process_chunk(\"abcd\", info)\n\t    assert len(info) == 0\n\tdef test_store_info_too_much_text():\n\t    text = \"word \" * 200000\n", "    with pytest.raises(ValueError, match=\"Processing is aborted due to high anticipated costs.\"):\n\t        store_info(text, \"memory/test.json\")\n\tdef test_load_info(tmpdir):\n\t    memory_file = tmpdir.join(\"memory.json\")\n\t    memory_file.write('[{\"id\": 1, \"text\": \"test\"}]')\n\t    info = load_info(memory_file.strpath)\n\t    assert len(info) == 1\n\t    assert info[0][\"id\"] == 1\n\t    assert info[0][\"text\"] == \"test\"\n\tdef test_memorize_existing_memory(mocker):\n", "    mocker.patch(\"os.path.exists\", return_value=True)\n\t    store_info_mock = mocker.patch(\"app.memory.store_info\")\n\t    memory_path = memorize(\"test text\")\n\t    store_info_mock.assert_not_called()\n\t    assert \"memory/\" in memory_path\n\tdef test_memorize_new_memory(mocker):\n\t    mocker.patch(\"os.path.exists\", return_value=False)\n\t    mocked_store_info = mocker.patch(\"app.memory.store_info\")\n\t    memory_path = memorize(\"test text\")\n\t    assert \"memory/\" in memory_path\n", "    mocked_store_info.assert_called_once()\n\tdef test_get_summary(mocker):\n\t    completion = mocker.patch(\"app.memory.get_completion\", return_value=\"summary\")\n\t    content = \"test content\"\n\t    summary = get_summary(content)\n\t    assert summary == \"summary\"\n\t    completion.assert_called_once_with(\n\t        f\"The following is a passage fragment. Please summarize what information the readers can take away from it:\\n{content}\"\n\t    )\n"]}
{"filename": "app/tests/__init__.py", "chunked_list": []}
{"filename": "app/tests/test_config.py", "chunked_list": ["import pytest\n\tfrom app.config import load_openai_api_key\n\tdef test_load_openai_api_key_success(monkeypatch):\n\t    monkeypatch.setenv(\"OPEN_API_TOKEN\", \"test_api_key\")\n\t    try:\n\t        load_openai_api_key()\n\t    except ValueError:\n\t        pytest.fail(\"load_openai_api_key() raised ValueError unexpectedly!\")\n\tdef test_load_openai_api_key_failure(monkeypatch):\n\t    monkeypatch.delenv(\"OPEN_API_TOKEN\", raising=False)\n", "    with pytest.raises(ValueError):\n\t        load_openai_api_key()\n"]}
{"filename": "app/tests/test_input_reader.py", "chunked_list": ["from app.input_reader import get_text\n\timport pytest\n\tdef test_get_text_url(mocker):\n\t    test_url = \"https://www.example.com\"\n\t    mock_process_url = mocker.patch('app.input_reader.process_url')\n\t    get_text(test_url)\n\t    mock_process_url.assert_called_once_with(test_url)\n\tdef test_get_text_youtube_url(mocker):\n\t    test_youtube_url = \"https://www.youtube.com/watch?v=8OAPLk20epo\"\n\t    mock_download_audio = mocker.patch('app.input_reader.download_audio')\n", "    mock_process_audio = mocker.patch('app.input_reader.process_audio')\n\t    get_text(test_youtube_url)\n\t    mock_download_audio.assert_called_once_with(test_youtube_url)\n\t    mock_process_audio.assert_called_once()\n\tdef test_get_text_epub(mocker):\n\t    test_epub_path = \"path/to/test_epub.epub\"\n\t    mock_process_epub = mocker.patch('app.input_reader.process_epub')\n\t    get_text(test_epub_path)\n\t    mock_process_epub.assert_called_once_with(test_epub_path)\n\tdef test_get_text_pdf(mocker):\n", "    test_pdf_path = \"path/to/test_pdf.pdf\"\n\t    mock_process_pdf = mocker.patch('app.input_reader.process_pdf')\n\t    get_text(test_pdf_path)\n\t    mock_process_pdf.assert_called_once_with(test_pdf_path)\n\tdef test_get_text_doc(mocker):\n\t    test_doc_path = \"path/to/test_doc.doc\"\n\t    mock_process_doc = mocker.patch('app.input_reader.process_doc')\n\t    get_text(test_doc_path)\n\t    mock_process_doc.assert_called_once_with(test_doc_path)\n\tdef test_get_text_txt(mocker):\n", "    test_txt_path = \"path/to/test_txt.txt\"\n\t    mock_process_txt = mocker.patch('app.input_reader.process_txt')\n\t    get_text(test_txt_path)\n\t    mock_process_txt.assert_called_once_with(test_txt_path)\n\tdef test_get_text_audio(mocker):\n\t    test_audio_path = \"path/to/test_audio.wav\"\n\t    mock_process_audio = mocker.patch('app.input_reader.process_audio')\n\t    get_text(test_audio_path)\n\t    mock_process_audio.assert_called_once_with(test_audio_path)\n\tdef test_get_text_unsupported_file_type():\n", "    unsupported_file_path = \"path/to/unsupported_file.mp4\"\n\t    with pytest.raises(ValueError, match=\"Invalid document path!\"):\n\t        get_text(unsupported_file_path)\n"]}
