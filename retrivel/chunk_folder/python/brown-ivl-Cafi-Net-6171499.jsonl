{"filename": "nerf/load_brics.py", "chunked_list": ["import os\n\timport torch\n\timport numpy as np\n\timport imageio \n\timport json\n\timport torch.nn.functional as F\n\timport cv2\n\timport glob\n\timport random\n\timport matplotlib.pyplot as plt\n", "import pickle\n\timport h5py\n\tfrom scipy.spatial.transform import Rotation as R\n\tos.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\n\ttrans_t = lambda t : torch.Tensor([\n\t    [1,0,0,0],\n\t    [0,1,0,0],\n\t    [0,0,1,t],\n\t    [0,0,0,1]]).float()\n\trot_phi = lambda phi : torch.Tensor([\n", "    [1,0,0,0],\n\t    [0,np.cos(phi),-np.sin(phi),0],\n\t    [0,np.sin(phi), np.cos(phi),0],\n\t    [0,0,0,1]]).float()\n\trot_theta = lambda th : torch.Tensor([\n\t    [np.cos(th),0,-np.sin(th),0],\n\t    [0,1,0,0],\n\t    [np.sin(th),0, np.cos(th),0],\n\t    [0,0,0,1]]).float()\n\tdef pose_spherical(theta, phi, radius, cam_pose):\n", "    c2w = trans_t(radius)\n\t    c2w = rot_phi(phi/180.*np.pi) @ c2w\n\t    c2w = rot_theta(theta/180.*np.pi) @ c2w\n\t    c2w = torch.Tensor(np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])) @ c2w\n\t    device = c2w.device\n\t    c2w = torch.tensor(c2w).to(device)\n\t    return c2w\n\tdef read_pickle_file(path):\n\t    objects = []\n\t    with open(path, \"rb\") as fp:\n", "        while True:\n\t            try:\n\t                obj = pickle.load(fp)\n\t                objects.append(obj)\n\t            except EOFError:\n\t                break\n\t    return objects \n\tdef load_dataset(directory, canonical_pose = None, input_pose = None):\n\t    cam_data_path = os.path.join(directory, \"cam_data.pkl\")\n\t    cam_data = read_pickle_file(cam_data_path)[0]\n", "    cams = {\"width\": 1280, \"height\": 720}\n\t    imgs = {}\n\t    image_dir = os.path.join(directory, \"render/\")\n\t    images = glob.glob(image_dir + \"**/*.png\", recursive = True)\n\t    images.sort()\n\t    depth_dir = os.path.join(directory, \"depth/\")\n\t    for i in range(len(images)):\n\t        image_current = images[i]\n\t        image_id = os.path.basename(image_current).split(\".\")[0]\n\t        image_parent_dir = image_current.split(\"/\")[-2]\n", "        cam = cam_data[image_id][\"K\"]\n\t        [cams[\"fx\"], cams[\"fy\"], cams[\"cx\"], cams[\"cy\"]] = cam\n\t        c2w = cam_data[image_id][\"extrinsics_opencv\"]\n\t        c2w = np.vstack([c2w, np.array([0, 0, 0, 1])])\n\t        c2w = np.linalg.inv(c2w)\n\t        pose = c2w\n\t        imgs[i] = {\n\t            \"camera_id\": image_id,\n\t            \"t\": pose[:3, 3].reshape(3, 1),\n\t            \"R\": pose[:3, :3],\n", "            \"path\": images[i],\n\t            \"pose\": pose\n\t        }\n\t        imgs[i][\"depth_path\"] = os.path.join(depth_dir, \"%s/%s_depth.npz\" % (image_parent_dir, image_id))\n\t    return imgs, cams\n\tdef main_loader(root_dir, scale, canonical_pose = None, input_pose = None):\n\t    imgs, cams = load_dataset(root_dir, canonical_pose, input_pose)\n\t    cams[\"fx\"] = fx = cams[\"fx\"] * scale\n\t    cams[\"fy\"] = fy = cams[\"fy\"] * scale\n\t    cams[\"cx\"] = cx = cams[\"cx\"] * scale\n", "    cams[\"cy\"] = cy = cams[\"cy\"] * scale\n\t    rand_key = random.choice(list(imgs))\n\t    test_img = cv2.imread(imgs[rand_key][\"path\"])\n\t    h, w = test_img.shape[:2]\n\t    cams[\"height\"] = round(h * scale)\n\t    cams[\"width\"] = round(w * scale)\n\t    cams[\"intrinsic_mat\"] = np.array([\n\t        [fx, 0, cx],\n\t        [0, -fy, cy],\n\t        [0, 0, -1]\n", "        ])\n\t    return imgs, cams \n\tdef load_brics_data(basedir, res = 1, skip = 1, max_ind = 54, canonical_pose = None, input_pose = None, num_poses = 300):\n\t    imgs, cams = main_loader(basedir, res, canonical_pose, input_pose)\n\t    all_ids = []\n\t    all_imgs = []\n\t    all_poses = []\n\t    all_depths = []\n\t    for index in range(0, max_ind, skip):\n\t        all_ids.append(imgs[index][\"camera_id\"])\n", "        n_image = imageio.imread(imgs[index][\"path\"]) / 255.0\n\t        h, w = n_image.shape[:2]\n\t        resized_h = round(h * res)\n\t        resized_w = round(w * res)\n\t        n_image = cv2.resize(n_image, (resized_w, resized_h), interpolation=cv2.INTER_AREA)\n\t        all_imgs.append(n_image)\n\t        n_pose = imgs[index][\"pose\"]\n\t        all_poses.append(n_pose)\n\t        n_depth = np.load(imgs[index][\"depth_path\"])['arr_0']\n\t        n_depth = np.where(n_depth == np.inf, 0, n_depth)\n", "        n_depth = np.where(n_depth > 100, 0, n_depth)\n\t        n_depth = cv2.resize(n_depth, (resized_w, resized_h), interpolation=cv2.INTER_AREA)\n\t        all_depths.append(n_depth)\n\t    all_poses = np.array(all_poses)\n\t    all_imgs = np.array(all_imgs).astype(np.float32)\n\t    all_depths = np.array(all_depths).astype(np.float32)\n\t    c2w = all_poses[all_ids.index(\"left_5\"), :, :]\n\t    input_pose_4 = np.identity(4)\n\t    canonical_pose_4 = np.identity(4)\n\t    transform = False\n", "    canonical_poses = []\n\t    if input_pose is not None:\n\t        input_pose_4[:3, :3] = input_pose\n\t        transform = True\n\t    if canonical_pose is not None:\n\t        canonical_pose_4[:3, :3] = canonical_pose\n\t        transform = True\n\t    if transform:\n\t        t = np.array([0.0, -0.5, 4.5]).T  #known from the brics simulator\n\t        nerf_w_2_transform_w = np.identity(4)\n", "        nerf_w_2_transform_w[:3, -1] = -t\n\t        temp = nerf_w_2_transform_w @ c2w \n\t        for i in range(num_poses):\n\t            angle = np.linspace(0, 360, num_poses)[i]\n\t            circular_pose = pose_spherical(0.0, angle, 0.0, c2w).cpu().numpy() \n\t            pose = np.linalg.inv(canonical_pose_4 @ input_pose_4) @ np.linalg.inv(circular_pose) @ temp\n\t            # pose = np.linalg.inv(canonical_pose_4) @ np.linalg.inv(circular_pose) @ temp\n\t            pose[:3, -1] += t\n\t            canonical_poses.append(pose)\n\t        all_poses = np.array(canonical_poses)\n", "    i_val = []\n\t    sides = [\"back\", \"bottom\", \"front\", \"left\", \"right\", \"top\"]\n\t    for side_idx in range(len(sides)):\n\t        panel_idx = np.random.randint(1, 10) \n\t        val_camera_id = \"%s_%d\" % (sides[side_idx], panel_idx)\n\t        val_idx = all_ids.index(val_camera_id)\n\t        i_val.append(val_idx)\n\t    indices = np.arange(len(all_imgs))\n\t    i_train = np.array(list(set(indices).difference(set(i_val))))\n\t    i_test = i_val\n", "    i_split = [i_train, i_val, i_test]\n\t    render_poses = np.array(all_poses)\n\t    return all_imgs, all_poses, render_poses, cams, all_depths, i_split\n"]}
{"filename": "nerf/density_vis.py", "chunked_list": ["import numpy as np\n\timport open3d as o3d\n\timport argparse\n\timport os\n\tfrom sklearn.cluster import KMeans\n\tdef cluster_sigmas(sigmas, n_clusters=2, power=1.0, exp=False, scale=1.0):\n\t    print(\"Number of clusters = \", n_clusters)\n\t    # dim, _, _ = sigmas.shape\n\t    # sigmas = sigmas.reshape((-1, 1))\n\t    # sigmas = sigmas + 1e2\n", "    print(\"Sigmas range = \", np.min(sigmas), np.max(sigmas))\n\t    relu_sigmas = np.where(sigmas > 0, sigmas, 0)\n\t    powered_sigmas = relu_sigmas ** power\n\t    print(\"Sigmas powered range = \", np.min(powered_sigmas), np.max(powered_sigmas))\n\t    if exp:\n\t        sigmas = 1. - np.exp(-scale * powered_sigmas)\n\t    print(\"Sigmas final range = \", np.min(sigmas), np.max(sigmas))\n\t    model = KMeans(init=\"k-means++\", n_clusters=n_clusters)\n\t    model.fit(sigmas)\n\t    print(\"Cluster centers = \", model.cluster_centers_)\n", "    labels = model.predict(sigmas)\n\t    (clusters, counts) = np.unique(labels, return_counts=True)\n\t    bg_label = clusters[np.where(counts == counts.max())[0]]\n\t    clustered_sigmas = np.where(labels == bg_label, 0, 1)\n\t    return clustered_sigmas\n\t# .reshape((dim, dim, dim))\n\tdef visualize(sigmas_path, samples_path, sigma_thresh):\n\t    sigmas = np.load(sigmas_path).reshape((-1, 1))\n\t    samples = np.load(samples_path).reshape((-1, 3))\n\t    # occ = np.where(sigmas > sigma_thresh)[0]\n", "    # print(\"Thresholding with %f: Total = %d, Occupied = %d, Occupancy = %f\" % (sigma_thresh, len(sigmas), len(occ), len(occ) / len(sigmas)))\n\t    # thresh_pcd = o3d.geometry.PointCloud()\n\t    # thresh_points = samples[np.where(sigmas > sigma_thresh)[0]]\n\t    # thresh_pcd.points = o3d.utility.Vector3dVector(thresh_points)\n\t    # o3d.visualization.draw_geometries([thresh_pcd])\n\t    clustered_sigmas = cluster_sigmas(sigmas, 2, 2.0, True, 0.3109375)\n\t    occ = np.where(clustered_sigmas != 0)[0]\n\t    print(\"Clustering: Total = %d, Occupied = %d, Occupancy = %f\" % (len(sigmas), len(occ), len(occ) / len(sigmas)))\n\t    fg_pcd = o3d.geometry.PointCloud()\n\t    fg_points = samples[np.where(clustered_sigmas != 0)[0]]\n", "    fg_pcd.points = o3d.utility.Vector3dVector(fg_points)\n\t    o3d.visualization.draw_geometries([fg_pcd])\n\tif __name__==\"__main__\":\n\t    parser = argparse.ArgumentParser(description=\"NeRF density field visualization\")\n\t    parser.add_argument(\"--input\", required=True, type = str)\n\t    parser.add_argument(\"--res\", default=32, type = int)\n\t    parser.add_argument(\"--sigma_thresh\", default=10.0, type = float)\n\t    parser.add_argument(\"--max_files\", default=10, type = int)\n\t    args = parser.parse_args()\n\t    count = 0\n", "    for path, dirs, files in os.walk(args.input):\n\t        for file in files:\n\t            if \"sigmas_%d.npy\" % (args.res) not in file:\n\t                continue\n\t            print(\"Processing %s %s\" % (path, file))\n\t            sigmas_path = os.path.join(path, file)\n\t            samples_path = os.path.join(path, file.replace(\"sigmas\", \"samples\"))\n\t            visualize(sigmas_path, samples_path, args.sigma_thresh)\n\t            count += 1\n\t            if count >= args.max_files:\n", "                break\n"]}
{"filename": "nerf/run_nerf_helpers.py", "chunked_list": ["import torch\n\t# torch.autograd.set_detect_anomaly(True)\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport numpy as np\n\t# Misc\n\timg2mse = lambda x, y : torch.mean((x - y) ** 2)\n\tmse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\n\tto8b = lambda x : (255*np.clip(x,0,1)).astype(np.uint8)\n\t# Positional encoding (section 5.1)\n", "class Embedder:\n\t    def __init__(self, **kwargs):\n\t        self.kwargs = kwargs\n\t        self.create_embedding_fn()\n\t    def create_embedding_fn(self):\n\t        embed_fns = []\n\t        d = self.kwargs['input_dims']\n\t        out_dim = 0\n\t        if self.kwargs['include_input']:\n\t            embed_fns.append(lambda x : x)\n", "            out_dim += d\n\t        max_freq = self.kwargs['max_freq_log2']\n\t        N_freqs = self.kwargs['num_freqs']\n\t        if self.kwargs['log_sampling']:\n\t            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n\t        else:\n\t            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n\t        for freq in freq_bands:\n\t            for p_fn in self.kwargs['periodic_fns']:\n\t                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n", "                out_dim += d\n\t        self.embed_fns = embed_fns\n\t        self.out_dim = out_dim\n\t    def embed(self, inputs):\n\t        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n\tdef get_embedder(multires, i=0):\n\t    if i == -1:\n\t        return nn.Identity(), 3\n\t    embed_kwargs = {\n\t                'include_input' : True,\n", "                'input_dims' : 3,\n\t                'max_freq_log2' : multires-1,\n\t                'num_freqs' : multires,\n\t                'log_sampling' : True,\n\t                'periodic_fns' : [torch.sin, torch.cos],\n\t    }\n\t    embedder_obj = Embedder(**embed_kwargs)\n\t    embed = lambda x, eo=embedder_obj : eo.embed(x)\n\t    return embed, embedder_obj.out_dim\n\t# Model\n", "class NeRF(nn.Module):\n\t    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4, skips=[4], use_viewdirs=False, semantic_en=False, num_classes=2):\n\t        \"\"\" \n\t        \"\"\"\n\t        super(NeRF, self).__init__()\n\t        self.D = D\n\t        self.W = W\n\t        self.input_ch = input_ch\n\t        self.input_ch_views = input_ch_views\n\t        self.skips = skips\n", "        self.use_viewdirs = use_viewdirs\n\t        self.semantic_en = semantic_en \n\t        self.C = num_classes\n\t        self.pts_linears = nn.ModuleList(\n\t            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + input_ch, W) for i in range(D-1)])\n\t        ### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)\n\t        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//2)])\n\t        ### Implementation according to the paper\n\t        # self.views_linears = nn.ModuleList(\n\t        #     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])\n", "        if use_viewdirs:\n\t            self.feature_linear = nn.Linear(W, W)\n\t            self.alpha_linear = nn.Linear(W, 1)\n\t            self.rgb_linear = nn.Linear(W//2, 3)\n\t            if semantic_en:\n\t                self.semantic = nn.Sequential(\n\t                        nn.Linear(W, W),\n\t                        nn.ReLU(True),\n\t                        nn.Linear(W, W//2),\n\t                        nn.ReLU(True),\n", "                        nn.Linear(W//2, self.C),\n\t                        #nn.Softmax(-1)\n\t                    )\n\t        else:\n\t            self.output_linear = nn.Linear(W, output_ch)\n\t    def query_density(self, input_pts):\n\t        h = input_pts\n\t        for i, l in enumerate(self.pts_linears):\n\t            h = self.pts_linears[i](h)\n\t            h = F.relu(h)\n", "            if i in self.skips:\n\t                h = torch.cat([input_pts, h], -1)\n\t        alpha = self.alpha_linear(h)\n\t        return alpha\n\t    def forward(self, x):\n\t        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1)\n\t        h = input_pts\n\t        for i, l in enumerate(self.pts_linears):\n\t            h = self.pts_linears[i](h)\n\t            h = F.relu(h)\n", "            if i in self.skips:\n\t                h = torch.cat([input_pts, h], -1)\n\t        pts_embedding = h.clone()\n\t        if self.use_viewdirs:\n\t            alpha = self.alpha_linear(h)\n\t            feature = self.feature_linear(h)\n\t            h = torch.cat([feature, input_views], -1)\n\t            for i, l in enumerate(self.views_linears):\n\t                h = self.views_linears[i](h)\n\t                h = F.relu(h)\n", "            rgb = self.rgb_linear(h)\n\t            outputs = torch.cat([rgb, alpha], -1)\n\t            if self.semantic_en:\n\t               semantic_map = self.semantic(pts_embedding) \n\t               outputs = torch.cat([rgb, alpha, semantic_map], -1)\n\t        else:\n\t            outputs = self.output_linear(h)\n\t        return outputs    \n\t    def load_weights_from_keras(self, weights):\n\t        assert self.use_viewdirs, \"Not implemented if use_viewdirs=False\"\n", "        # Load pts_linears\n\t        for i in range(self.D):\n\t            idx_pts_linears = 2 * i\n\t            self.pts_linears[i].weight.data = torch.from_numpy(np.transpose(weights[idx_pts_linears]))    \n\t            self.pts_linears[i].bias.data = torch.from_numpy(np.transpose(weights[idx_pts_linears+1]))\n\t        # Load feature_linear\n\t        idx_feature_linear = 2 * self.D\n\t        self.feature_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_feature_linear]))\n\t        self.feature_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_feature_linear+1]))\n\t        # Load views_linears\n", "        idx_views_linears = 2 * self.D + 2\n\t        self.views_linears[0].weight.data = torch.from_numpy(np.transpose(weights[idx_views_linears]))\n\t        self.views_linears[0].bias.data = torch.from_numpy(np.transpose(weights[idx_views_linears+1]))\n\t        # Load rgb_linear\n\t        idx_rbg_linear = 2 * self.D + 4\n\t        self.rgb_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear]))\n\t        self.rgb_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear+1]))\n\t        # Load alpha_linear\n\t        idx_alpha_linear = 2 * self.D + 6\n\t        self.alpha_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear]))\n", "        self.alpha_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear+1]))\n\t# Ray helpers\n\tdef get_rays(H, W, K, c2w):\n\t    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H))  # pytorch's meshgrid has indexing='ij'\n\t    i = i.t()\n\t    j = j.t()\n\t    dirs = torch.stack([(i-K[0][2])/K[0][0], -(j-K[1][2])/K[1][1], -torch.ones_like(i)/K[2][2]], -1)\n\t    # Rotate ray directions from camera frame to the world frame\n\t    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n\t    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n", "    rays_o = c2w[:3,-1].expand(rays_d.shape)\n\t    return rays_o, rays_d\n\tdef get_rays_np(H, W, K, c2w):\n\t    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n\t    dirs = np.stack([(i-K[0][2])/K[0][0], -(j-K[1][2])/K[1][1], -np.ones_like(i)], -1)\n\t    # Rotate ray directions from camera frame to the world frame\n\t    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n\t    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n\t    rays_o = np.broadcast_to(c2w[:3,-1], np.shape(rays_d))\n\t    return rays_o, rays_d\n", "# Hierarchical sampling (section 5.2)\n\tdef sample_pdf(bins, weights, N_samples, det=False, pytest=False):\n\t    # Get pdf\n\t    weights = weights + 1e-5 # prevent nans\n\t    pdf = weights / torch.sum(weights, -1, keepdim=True)\n\t    cdf = torch.cumsum(pdf, -1)\n\t    cdf = torch.cat([torch.zeros_like(cdf[...,:1]), cdf], -1)  # (batch, len(bins))\n\t    # Take uniform samples\n\t    if det:\n\t        u = torch.linspace(0., 1., steps=N_samples)\n", "        u = u.expand(list(cdf.shape[:-1]) + [N_samples])\n\t    else:\n\t        u = torch.rand(list(cdf.shape[:-1]) + [N_samples])\n\t    # Pytest, overwrite u with numpy's fixed random numbers\n\t    if pytest:\n\t        np.random.seed(0)\n\t        new_shape = list(cdf.shape[:-1]) + [N_samples]\n\t        if det:\n\t            u = np.linspace(0., 1., N_samples)\n\t            u = np.broadcast_to(u, new_shape)\n", "        else:\n\t            u = np.random.rand(*new_shape)\n\t        u = torch.Tensor(u)\n\t    # Invert CDF\n\t    u = u.contiguous()\n\t    inds = torch.searchsorted(cdf, u, right=True)\n\t    below = torch.max(torch.zeros_like(inds-1), inds-1)\n\t    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)\n\t    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n\t    # cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n", "    # bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n\t    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n\t    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\t    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\t    denom = (cdf_g[...,1]-cdf_g[...,0])\n\t    denom = torch.where(denom<1e-5, torch.ones_like(denom), denom)\n\t    t = (u-cdf_g[...,0])/denom\n\t    samples = bins_g[...,0] + t * (bins_g[...,1]-bins_g[...,0])\n\t    return samples\n"]}
{"filename": "nerf/run_nerf.py", "chunked_list": ["import os, sys\n\timport numpy as np\n\timport imageio\n\timport json\n\timport random\n\timport time\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom tqdm import tqdm, trange\n", "import matplotlib.pyplot as plt\n\tfrom run_nerf_helpers import *\n\tfrom load_brics import load_brics_data\n\timport open3d as o3d\n\timport wandb\n\timport gc\n\timport copy\n\timport cv2\n\tfrom PIL import Image\n\timport mcubes\n", "from plyfile import PlyData, PlyElement\n\timport math\n\tfrom sklearn.cluster import KMeans\n\timport h5py\n\timport pickle\n\tfrom scipy.spatial.transform import Rotation as R\n\tdevice_idx = 0\n\tgc.collect()\n\ttorch.cuda.empty_cache()\n\tdevice = torch.device(\"cuda:%d\" % (device_idx) if torch.cuda.is_available() else \"cpu\")\n", "DEBUG = False\n\tdef read_pickle_file(path):\n\t    objects = []\n\t    with open(path, \"rb\") as fp:\n\t        while True:\n\t            try:\n\t                obj = pickle.load(fp)\n\t                objects.append(obj)\n\t            except EOFError:\n\t                break\n", "    return objects\n\tdef load_models(path):\n\t    models = []\n\t    with open(path, \"r\") as f:\n\t        lines = f.readlines()\n\t        for line in lines:\n\t            model = os.path.basename(line[:-1])\n\t            model = model[:-15]\n\t            models.append(model) \n\t    return models\n", "def load_h5(path):\n\t    fx_input = h5py.File(path, 'r')\n\t    x = fx_input['data'][:]\n\t    fx_input.close()\n\t    return x\n\tdef batchify(fn, chunk):\n\t    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n\t    \"\"\"\n\t    if chunk is None:\n\t        return fn\n", "    def ret(inputs):\n\t        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n\t    return ret\n\tdef run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n\t    \"\"\"Prepares inputs and applies network 'fn'.\n\t    \"\"\"\n\t    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n\t    embedded = embed_fn(inputs_flat)\n\t    if viewdirs is not None:\n\t        input_dirs = viewdirs[:,None].expand(inputs.shape)\n", "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n\t        embedded_dirs = embeddirs_fn(input_dirs_flat)\n\t        embedded = torch.cat([embedded, embedded_dirs], -1)\n\t    # embedded.requires_grad = True\n\t    outputs_flat = batchify(fn, netchunk)(embedded)\n\t    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n\t    # outputs_flat.backward()\n\t    return outputs\n\tdef batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n\t    \"\"\"Render rays in smaller minibatches to avoid OOM.\n", "    \"\"\"\n\t    all_ret = {}\n\t    for i in range(0, rays_flat.shape[0], chunk):\n\t        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n\t        for k in ret:\n\t            if k not in all_ret:\n\t                all_ret[k] = []\n\t            all_ret[k].append(ret[k])\n\t    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n\t    return all_ret\n", "def render(H, W, K, chunk=1024*32, rays=None, c2w=None,\n\t                  near=0., far=1.,\n\t                  use_viewdirs=False, c2w_staticcam=None, gt_image=None, gt_depth=None,\n\t                  **kwargs):\n\t    \"\"\"Render rays\n\t    Args:\n\t      H: int. Height of image in pixels.\n\t      W: int. Width of image in pixels.\n\t      focal: float. Focal length of pinhole camera.\n\t      chunk: int. Maximum number of rays to process simultaneously. Used to\n", "        control maximum memory usage. Does not affect final results.\n\t      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n\t        each example in batch.\n\t      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n\t      near: float or array of shape [batch_size]. Nearest distance for a ray.\n\t      far: float or array of shape [batch_size]. Farthest distance for a ray.\n\t      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n\t      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n\t       camera while using other c2w argument for viewing directions.\n\t    Returns:\n", "      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n\t      disp_map: [batch_size]. Disparity map. Inverse of depth.\n\t      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n\t      extras: dict with everything returned by render_rays().\n\t    \"\"\"\n\t    if c2w is not None:\n\t        # special case to render full image\n\t        c2w = torch.tensor(c2w).to(device)\n\t        rays_o, rays_d = get_rays(H, W, K, c2w)\n\t    else:\n", "        # use provided ray batch\n\t        rays_o, rays_d = rays\n\t    if use_viewdirs:\n\t        # provide ray directions as input\n\t        viewdirs = rays_d\n\t        if c2w_staticcam is not None:\n\t            # special case to visualize effect of viewdirs\n\t            rays_o, rays_d = get_rays(H, W, K, c2w_staticcam)\n\t        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n\t        viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n", "    sh = rays_d.shape # [..., 3]\n\t    # Create ray batch\n\t    rays_o = torch.reshape(rays_o, [-1,3]).float()\n\t    rays_d = torch.reshape(rays_d, [-1,3]).float()\n\t    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n\t    rays = torch.cat([rays_o, rays_d, near, far], -1)\n\t    if use_viewdirs:\n\t        rays = torch.cat([rays, viewdirs], -1)\n\t    # Render and reshape\n\t    if gt_depth is not None:\n", "        gt_depth = torch.tensor(gt_depth).to(device).reshape((-1, 1))\n\t        points = rays_o + gt_depth * rays_d\n\t        all_ret = {}\n\t        all_ret['rgb_map'] = torch.tensor(gt_image).to(device)\n\t        all_ret['disp_map'] = torch.tensor([]).to(device)\n\t        all_ret['acc_map'] = torch.tensor([]).to(device)\n\t        all_ret['weights'] = torch.tensor([]).to(device)\n\t        all_ret['sigma_map'] = torch.tensor([]).to(device)\n\t        all_ret['sample_points'] = torch.tensor([]).to(device)\n\t        all_ret['depth_map'] = gt_depth\n", "        all_ret['points'] = points\n\t    else:\n\t        all_ret = batchify_rays(rays, chunk, **kwargs)\n\t        for k in all_ret:\n\t            k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n\t            all_ret[k] = torch.reshape(all_ret[k], k_sh)\n\t    all_ret['K'] = K\n\t    all_ret['c2w'] = c2w\n\t    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n\t    ret_list = [all_ret[k] for k in k_extract]\n", "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n\t    return ret_list + [ret_dict]\n\tdef render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0, gt_depths=None, model=None, category=\"\"):\n\t    H, W, focal = hwf\n\t    if render_factor!=0:\n\t        # Render downsampled for speed\n\t        H = H//render_factor\n\t        W = W//render_factor\n\t        focal = focal/render_factor\n\t    rgbs = []\n", "    disps = []\n\t    depths = []\n\t    pcds = []\n\t    Ks = []\n\t    c2ws = []\n\t    weights = []\n\t    sigmas = []\n\t    sample_points = []\n\t    t = time.time()\n\t    for i, c2w in enumerate(tqdm(render_poses)):\n", "        print(i, time.time() - t)\n\t        t = time.time()\n\t        if gt_depths is not None:\n\t            rgb, disp, acc, extras = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], gt_image=gt_imgs[i], gt_depth=gt_depths[i], **render_kwargs)\n\t        else:\n\t            rgb, disp, acc, extras = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n\t        rgbs.append(rgb.detach().cpu().numpy())\n\t        disps.append(disp.detach().cpu().numpy())\n\t        if render_kwargs['retdepth']:\n\t            weights.append(extras['weights'].detach().cpu().numpy())\n", "            sigmas.append(extras['sigma_map'].detach().cpu().numpy())\n\t            sample_points.append(extras['sample_points'].detach().cpu().numpy())\n\t            depths.append(extras['depth_map'].detach().cpu().numpy())\n\t            points = extras['points'].detach().cpu().numpy().reshape(-1, 3)\n\t            pcd = o3d.geometry.PointCloud()\n\t            pcd.points = o3d.utility.Vector3dVector(points)\n\t            pcd.colors = o3d.utility.Vector3dVector(rgbs[-1].reshape(-1, 3))\n\t            pcds.append(pcd)\n\t            Ks.append(extras['K'])\n\t            c2ws.append(extras['c2w'].detach().cpu().numpy())\n", "        if i==0:\n\t            print(rgb.shape, disp.shape)\n\t        \"\"\"\n\t        if gt_imgs is not None and render_factor==0:\n\t            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n\t            print(p)\n\t        \"\"\"\n\t        if savedir is not None:\n\t            rgb8 = to8b(rgbs[-1])\n\t            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n", "            imageio.imwrite(filename, rgb8)\n\t            clustered_sigmas = cluster(extras['sigma_map'].detach().cpu().numpy(), 2)\n\t            samples = extras['sample_points'].detach().cpu().numpy()\n\t            occ_inds = np.where(clustered_sigmas > 0)\n\t            occ_samples = samples[occ_inds[0], occ_inds[1], occ_inds[2], :]\n\t            min_corner = np.array([np.min(occ_samples[:, 0]), np.min(occ_samples[:, 1]), np.min(occ_samples[:, 2])])\n\t            max_corner = np.array([np.max(occ_samples[:, 0]), np.max(occ_samples[:, 1]), np.max(occ_samples[:, 2])])\n\t            can_save_path = \"canonical_renderings/%s/%s_%d.png\" % (category, model, i)\n\t            if not os.path.exists(os.path.dirname(can_save_path)):\n\t                os.makedirs(os.path.dirname(can_save_path))\n", "            imageio.imwrite(can_save_path, rgb8)\n\t            if render_kwargs['retdepth']:\n\t                # weight8 = weights[-1]\n\t                # weights_filename = os.path.join(savedir, 'weights_{:03d}.npy'.format(i))\n\t                # np.save(weights_filename, weight8)\n\t                # sigma8 = sigmas[-1]\n\t                # sigmas_filename = os.path.join(savedir, 'sigmas_{:03d}.npy'.format(i))\n\t                # np.save(sigmas_filename, sigma8)\n\t                # sample8 = sample_points[-1]\n\t                # samples_filename = os.path.join(savedir, 'samples_{:03d}.npy'.format(i))\n", "                # np.save(samples_filename, sample8)\n\t                depth8 = depths[-1]\n\t                depth_filename = os.path.join(savedir, 'depth_{:03d}.npy'.format(i))\n\t                np.save(depth_filename, depth8)\n\t                pcd_filename = os.path.join(savedir, '{:03d}.ply'.format(i))\n\t                o3d.io.write_point_cloud(pcd_filename, pcds[-1])\n\t                c2w8 = c2ws[-1]\n\t                c2w_filename = os.path.join(savedir, 'c2w_{:03d}.npy'.format(i))\n\t                np.save(c2w_filename, c2w8)\n\t                K8 = Ks[-1]\n", "                K_filename = os.path.join(savedir, 'K_{:03d}.npy'.format(i))\n\t                np.save(K_filename, K8)\n\t        del rgb, disp, acc, extras\n\t        torch.cuda.empty_cache()\n\t    rgbs = np.stack(rgbs, 0)\n\t    disps = np.stack(disps, 0)\n\t    if render_kwargs['retdepth']:\n\t        depths = np.stack(depths, 0)\n\t    return rgbs, disps, depths\n\tdef create_nerf(args):\n", "    \"\"\"Instantiate NeRF's MLP model.\n\t    \"\"\"\n\t    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n\t    input_ch_views = 0\n\t    embeddirs_fn = None\n\t    if args.use_viewdirs:\n\t        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, args.i_embed)\n\t    output_ch = 5 if args.N_importance > 0 else 4\n\t    skips = [4]\n\t    model = NeRF(D=args.netdepth, W=args.netwidth,\n", "                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n\t                 input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n\t    grad_vars = list(model.parameters())\n\t    model_fine = None\n\t    if args.N_importance > 0:\n\t        model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine,\n\t                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n\t                          input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n\t        grad_vars += list(model_fine.parameters())\n\t    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n", "                                                                embed_fn=embed_fn,\n\t                                                                embeddirs_fn=embeddirs_fn,\n\t                                                                netchunk=args.netchunk\n\t                                                                )\n\t    # Create optimizer\n\t    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n\t    start = 0\n\t    basedir = args.basedir\n\t    expname = args.expname\n\t    ##########################\n", "    # Load checkpoints\n\t    if args.ft_path is not None and args.ft_path!='None':\n\t        ckpts = [args.ft_path]\n\t    else:\n\t        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n\t    print('Found ckpts', ckpts)\n\t    if len(ckpts) > 0 and not args.no_reload:\n\t        ckpt_path = ckpts[-1]\n\t        print('Reloading from', ckpt_path)\n\t        ckpt = torch.load(ckpt_path, map_location=device)\n", "        start = ckpt['global_step']\n\t        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n\t        # Load model\n\t        model.load_state_dict(ckpt['network_fn_state_dict'])\n\t        if model_fine is not None:\n\t            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n\t    ##########################\n\t    render_kwargs_train = {\n\t        'embed_fn': embed_fn, \n\t        'embeddirs_fn': embeddirs_fn,\n", "        'network_query_fn' : network_query_fn,\n\t        'perturb' : args.perturb,\n\t        'N_importance' : args.N_importance,\n\t        'network_fine' : model_fine,\n\t        'N_samples' : args.N_samples,\n\t        'network_fn' : model,\n\t        'use_viewdirs' : args.use_viewdirs,\n\t        'white_bkgd' : args.white_bkgd,\n\t        'raw_noise_std' : args.raw_noise_std,\n\t        'retdepth': True,\n", "    }\n\t    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n\t    render_kwargs_test['perturb'] = False\n\t    render_kwargs_test['raw_noise_std'] = 0.\n\t    render_kwargs_test['retdepth'] = True\n\t    render_kwargs_test['N_importance'] = args.N_importance // 2\n\t    render_kwargs_test['N_samples'] = args.N_samples // 2\n\t    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer\n\tdef raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False):\n\t    \"\"\"Transforms model's predictions to semantically meaningful values.\n", "    Args:\n\t        raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n\t        z_vals: [num_rays, num_samples along ray]. Integration time.\n\t        rays_d: [num_rays, 3]. Direction of each ray.\n\t    Returns:\n\t        rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n\t        disp_map: [num_rays]. Disparity map. Inverse of depth map.\n\t        acc_map: [num_rays]. Sum of weights along each ray.\n\t        weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n\t        depth_map: [num_rays]. Estimated distance to object.\n", "    \"\"\"\n\t    raw2alpha = lambda raw, dists, act_fn=F.relu: 1.-torch.exp(-act_fn(raw)*dists)\n\t    dists = z_vals[...,1:] - z_vals[...,:-1]\n\t    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1)  # [N_rays, N_samples]\n\t    dists = dists * torch.norm(rays_d[...,None,:], dim=-1)\n\t    rgb = torch.sigmoid(raw[...,:3])  # [N_rays, N_samples, 3]\n\t    noise = 0.\n\t    if raw_noise_std > 0.:\n\t        noise = torch.randn(raw[...,3].shape) * raw_noise_std\n\t        # Overwrite randomly sampled data if pytest\n", "        if pytest:\n\t            np.random.seed(0)\n\t            noise = np.random.rand(*list(raw[...,3].shape)) * raw_noise_std\n\t            noise = torch.Tensor(noise)\n\t    alpha = raw2alpha(raw[...,3] + noise, dists)  # [N_rays, N_samples]\n\t    # weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n\t    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n\t    rgb_map = torch.sum(weights[...,None] * rgb, -2)  # [N_rays, 3]\n\t    depth_map = torch.sum(weights * z_vals, -1)\n\t    disp_map = 1./torch.max(1e-10 * torch.ones_like(depth_map), depth_map / torch.sum(weights, -1))\n", "    acc_map = torch.sum(weights, -1)\n\t    sigma_map = raw[..., 3]\n\t    if white_bkgd:\n\t        rgb_map = rgb_map + (1.-acc_map[...,None])\n\t    return rgb_map, disp_map, acc_map, weights, depth_map, sigma_map\n\tdef render_rays(ray_batch,\n\t                embed_fn,\n\t                embeddirs_fn,\n\t                network_fn,\n\t                network_query_fn,\n", "                N_samples,\n\t                retraw=True,\n\t                retdepth=True,\n\t                lindisp=False,\n\t                perturb=0.,\n\t                N_importance=0,\n\t                network_fine=None,\n\t                white_bkgd=False,\n\t                raw_noise_std=0.,\n\t                verbose=False,\n", "                pytest=False):\n\t    \"\"\"Volumetric rendering.\n\t    Args:\n\t      ray_batch: array of shape [batch_size, ...]. All information necessary\n\t        for sampling along a ray, including: ray origin, ray direction, min\n\t        dist, max dist, and unit-magnitude viewing direction.\n\t      network_fn: function. Model for predicting RGB and density at each point\n\t        in space.\n\t      network_query_fn: function used for passing queries to network_fn.\n\t      N_samples: int. Number of different times to sample along each ray.\n", "      retraw: bool. If True, include model's raw, unprocessed predictions.\n\t      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n\t      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n\t        random points in time.\n\t      N_importance: int. Number of additional times to sample along each ray.\n\t        These samples are only passed to network_fine.\n\t      network_fine: \"fine\" network with same spec as network_fn.\n\t      white_bkgd: bool. If True, assume a white background.\n\t      raw_noise_std: ...\n\t      verbose: bool. If True, print more debugging info.\n", "    Returns:\n\t      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n\t      disp_map: [num_rays]. Disparity map. 1 / depth.\n\t      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n\t      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n\t      rgb0: See rgb_map. Output for coarse model.\n\t      disp0: See disp_map. Output for coarse model.\n\t      acc0: See acc_map. Output for coarse model.\n\t      z_std: [num_rays]. Standard deviation of distances along ray for each\n\t        sample.\n", "    \"\"\"\n\t    N_rays = ray_batch.shape[0]\n\t    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n\t    viewdirs = ray_batch[:,-3:] if ray_batch.shape[-1] > 8 else None\n\t    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n\t    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n\t    t_vals = torch.linspace(0., 1., steps=N_samples)\n\t    near = near.to(device)\n\t    far = far.to(device)\n\t    if not lindisp:\n", "        z_vals = near * (1.-t_vals) + far * (t_vals)\n\t    else:\n\t        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n\t    z_vals = z_vals.expand([N_rays, N_samples])\n\t    if perturb > 0.:\n\t        # get intervals between samples\n\t        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n\t        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n\t        lower = torch.cat([z_vals[...,:1], mids], -1)\n\t        # stratified samples in those intervals\n", "        t_rand = torch.rand(z_vals.shape)\n\t        # Pytest, overwrite u with numpy's fixed random numbers\n\t        if pytest:\n\t            np.random.seed(0)\n\t            t_rand = np.random.rand(*list(z_vals.shape))\n\t            t_rand = torch.Tensor(t_rand)\n\t        z_vals = lower + (upper - lower) * t_rand\n\t    rays_o = rays_o.to(device)\n\t    rays_d = rays_d.to(device)\n\t    viewdirs = viewdirs.to(device)\n", "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n\t#     raw = run_network(pts)\n\t    raw = network_query_fn(pts, viewdirs, network_fn)\n\t    rgb_map, disp_map, acc_map, weights, depth_map, sigma_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n\t    points = rays_o + depth_map.unsqueeze(1) * rays_d\n\t    if N_importance > 0:\n\t        rgb_map_0, disp_map_0, acc_map_0, weights_0, depth_map_0, sigma_map_0, raw_0, points_0 = rgb_map, disp_map, acc_map, weights, depth_map, sigma_map, raw, points\n\t        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n\t        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0.), pytest=pytest)\n\t        z_samples = z_samples.detach()\n", "        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n\t        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n\t        run_fn = network_fn if network_fine is None else network_fine\n\t#         raw = run_network(pts, fn=run_fn)\n\t        raw = network_query_fn(pts, viewdirs, run_fn)\n\t        rgb_map, disp_map, acc_map, weights, depth_map, sigma_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n\t        points = rays_o + depth_map.unsqueeze(1) * rays_d\n\t    ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map}\n\t    if retraw:\n\t        ret['raw'] = raw\n", "    if retdepth:\n\t        ret['weights'] = weights \n\t        ret['sigma_map'] = sigma_map\n\t        ret['sample_points'] = pts\n\t        ret['depth_map'] = depth_map\n\t        ret['points'] = points\n\t    if N_importance > 0:\n\t        ret['rgb0'] = rgb_map_0\n\t        ret['disp0'] = disp_map_0\n\t        ret['acc0'] = acc_map_0\n", "        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n\t        if retraw:\n\t            ret['raw0'] = raw_0\n\t        if retdepth:\n\t            ret['weights0'] = weights_0\n\t            ret['sigma0'] = sigma_map_0\n\t            ret['depth0'] = depth_map_0\n\t            ret['points0'] = points_0\n\t    for k in ret:\n\t        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n", "            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n\t    return ret\n\tdef config_parser():\n\t    import configargparse\n\t    parser = configargparse.ArgumentParser()\n\t    parser.add_argument('--config', is_config_file=True, \n\t                        help='config file path')\n\t    parser.add_argument(\"--expname\", type=str, \n\t                        help='experiment name')\n\t    parser.add_argument(\"--basedir\", type=str, default='./logs/', \n", "                        help='where to store ckpts and logs')\n\t    parser.add_argument(\"--datadir\", type=str, default='./data/brics_renderings/chair', \n\t                        help='input data directory')\n\t    # training options\n\t    parser.add_argument(\"--netdepth\", type=int, default=8, \n\t                        help='layers in network')\n\t    parser.add_argument(\"--netwidth\", type=int, default=256, \n\t                        help='channels per layer')\n\t    parser.add_argument(\"--netdepth_fine\", type=int, default=8, \n\t                        help='layers in fine network')\n", "    parser.add_argument(\"--netwidth_fine\", type=int, default=256, \n\t                        help='channels per layer in fine network')\n\t    parser.add_argument(\"--N_rand\", type=int, default=32*32*4, \n\t                        help='batch size (number of random rays per gradient step)')\n\t    parser.add_argument(\"--lrate\", type=float, default=5e-4, \n\t                        help='learning rate')\n\t    parser.add_argument(\"--lrate_decay\", type=int, default=250, \n\t                        help='exponential learning rate decay (in 1000 steps)')\n\t    parser.add_argument(\"--chunk\", type=int, default=1024*32, \n\t                        help='number of rays processed in parallel, decrease if running out of memory')\n", "    parser.add_argument(\"--netchunk\", type=int, default=1024*64, \n\t                        help='number of pts sent through network in parallel, decrease if running out of memory')\n\t    parser.add_argument(\"--no_batching\", action='store_true', \n\t                        help='only take random rays from 1 image at a time')\n\t    parser.add_argument(\"--no_reload\", action='store_true', \n\t                        help='do not reload weights from saved ckpt')\n\t    parser.add_argument(\"--ft_path\", type=str, default=None, \n\t                        help='specific weights npy file to reload for coarse network')\n\t    # loss weights\n\t    parser.add_argument(\"--loss_param\", type=float, default=1,\n", "                        help='exponential term parameter for depth weighting')\n\t    # rendering options\n\t    parser.add_argument(\"--N_samples\", type=int, default=64, \n\t                        help='number of coarse samples per ray')\n\t    parser.add_argument(\"--N_importance\", type=int, default=0,\n\t                        help='number of additional fine samples per ray')\n\t    parser.add_argument(\"--near\", type=float, default=0.,\n\t                        help='closest point to sample during ray rendering')\n\t    parser.add_argument(\"--far\", type=float, default=1.,\n\t                        help='farthest point to sample during ray rendering')\n", "    parser.add_argument(\"--perturb\", type=float, default=1.,\n\t                        help='set to 0. for no jitter, 1. for jitter')\n\t    parser.add_argument(\"--use_viewdirs\", action='store_true', \n\t                        help='use full 5D input instead of 3D')\n\t    parser.add_argument(\"--i_embed\", type=int, default=0, \n\t                        help='set 0 for default positional encoding, -1 for none')\n\t    parser.add_argument(\"--multires\", type=int, default=10, \n\t                        help='log2 of max freq for positional encoding (3D location)')\n\t    parser.add_argument(\"--multires_views\", type=int, default=4, \n\t                        help='log2 of max freq for positional encoding (2D direction)')\n", "    parser.add_argument(\"--raw_noise_std\", type=float, default=0., \n\t                        help='std dev of noise added to regularize sigma_a output, 1e0 recommended')\n\t    parser.add_argument(\"--multi_scene\", action='store_true', \n\t                        help='render multiple scenes')\n\t    parser.add_argument(\"--root_dir\", type=str, default='./logs/', \n\t                        help='path to directory containing all the scenes to be rendered')\n\t    parser.add_argument(\"--render_only\", action='store_true', \n\t                        help='do not optimize, reload weights and render out render_poses path')\n\t    parser.add_argument(\"--render_test\", action='store_true', \n\t                        help='render the test set instead of render_poses path')\n", "    parser.add_argument(\"--render_factor\", type=int, default=0, \n\t                        help='downsampling factor to speed up rendering, set 4 or 8 for fast preview')\n\t    parser.add_argument(\"--canonical_path\", type=str, default=None, \n\t                        help='canonical data directory')\n\t    parser.add_argument(\"--num_render_poses\", type=int, default=300, \n\t                        help='number of poses to render from')\n\t    parser.add_argument(\"--model_name\", type=str, default=\"\", \n\t                        help='model name')\n\t    parser.add_argument(\"--category\", type=str, default=\"\", \n\t                        help='category name')\n", "    parser.add_argument(\"--gen_sigmas\", action='store_true', \n\t                        help='extract the sigmas, i.e., the density field for the model')\n\t    # training options\n\t    parser.add_argument(\"--precrop_iters\", type=int, default=0,\n\t                        help='number of steps to train on central crops')\n\t    parser.add_argument(\"--precrop_frac\", type=float,\n\t                        default=.5, help='fraction of img taken for central crops') \n\t    parser.add_argument(\"--iters\", type=int, default=10000,\n\t                        help='number of steps to train for')\n\t    # dataset options\n", "    parser.add_argument(\"--dataset_type\", type=str, default='blender', \n\t                        help='options: brics')\n\t    parser.add_argument(\"--testskip\", type=int, default=8, \n\t                        help='will load 1/N images from test/val sets, useful for large datasets like deepvoxels')\n\t    parser.add_argument(\"--max_ind\", type=int, default=100,\n\t                        help='max index used in loader')\n\t    # sigma mesh flags\n\t    parser.add_argument('--x_range', nargs=\"+\", type=float, default=[-1.0, 1.0],\n\t                        help='x range of the object')\n\t    parser.add_argument('--y_range', nargs=\"+\", type=float, default=[-1.0, 1.0],\n", "                        help='x range of the object')\n\t    parser.add_argument('--z_range', nargs=\"+\", type=float, default=[-1.0, 1.0],\n\t                        help='x range of the object')\n\t    parser.add_argument('--sigma_threshold', type=float, default=20.0,\n\t                        help='threshold to consider a location is occupied')\n\t    ## blender flags\n\t    parser.add_argument(\"--white_bkgd\", action='store_true', \n\t                        help='set to render synthetic data on a white bkgd (always use for dvoxels)')\n\t    parser.add_argument(\"--res\", type=float, default=1.0,\n\t                        help='load blender synthetic data at given resolution instead of 800x800')\n", "    # logging/saving options\n\t    parser.add_argument(\"--wand_en\", action='store_true',  \n\t                        help='wandb logging enabled')\n\t    parser.add_argument(\"--i_print\",   type=int, default=100, \n\t                        help='frequency of console printout and metric loggin')\n\t    parser.add_argument(\"--i_img\",     type=int, default=100, \n\t                        help='frequency of tensorboard image logging')\n\t    parser.add_argument(\"--i_weights\", type=int, default=10000, \n\t                        help='frequency of weight ckpt saving')\n\t    parser.add_argument(\"--i_testset\", type=int, default=5000000, \n", "                        help='frequency of testset saving')\n\t    parser.add_argument(\"--i_video\",   type=int, default=5000000, \n\t                        help='frequency of render_poses video saving')\n\t    return parser\n\tdef get_max_cube(minCorner, maxCorner):\n\t    minPt, maxPt = copy.deepcopy(minCorner), copy.deepcopy(maxCorner)\n\t    diagLen = math.dist(minPt, maxPt)\n\t    for i in range(len(minPt)):\n\t        midPt = (minPt[i] + maxPt[i]) / 2\n\t        minPt[i] = midPt - diagLen / 2\n", "        maxPt[i] = midPt + diagLen / 2\n\t    return minPt, maxPt\n\tdef get_coords(minCoord, maxCoord, sampleCtr=128):\n\t    xdists = np.linspace(minCoord[0], maxCoord[0], sampleCtr)\n\t    ydists = np.linspace(minCoord[1], maxCoord[1], sampleCtr)\n\t    zdists = np.linspace(minCoord[2], maxCoord[2], sampleCtr)\n\t    coords = np.stack(np.meshgrid(xdists, ydists, zdists, indexing='ij'), axis=-1).astype(np.float32)\n\t    pcd = o3d.geometry.PointCloud()\n\t    pcd.points = o3d.utility.Vector3dVector(coords.reshape((-1, 3)))\n\t    return pcd, coords\n", "def cluster(sigmas, n_clusters=2, power=2.0, scale=1.0):\n\t    print(\"Number of clusters = \", n_clusters)\n\t    dim1, dim2, dim3 = sigmas.shape\n\t    sigmas = sigmas.reshape((-1, 1))\n\t    #sigmas = sigmas + 1e2\n\t    relu_sigmas = np.where(sigmas > 0, sigmas, 0)\n\t    powered_sigmas = relu_sigmas ** power\n\t    print(\"Sigmas powered range = \", np.min(powered_sigmas), np.max(powered_sigmas))\n\t    sigmas = 1. - np.exp(-scale * powered_sigmas)\n\t    print(\"Sigmas final range = \", np.min(sigmas), np.max(sigmas))\n", "    # model = GaussianMixture(n_components=2,init_params=\"k-means++\",weights_init=[0.9,0.1])\n\t    model = KMeans(init=\"k-means++\", n_clusters=n_clusters)\n\t    model.fit(sigmas)\n\t    labels = model.predict(sigmas)\n\t    (clusters, counts) = np.unique(labels, return_counts=True)\n\t    fg_label = clusters[np.where(counts == counts.min())[0]]\n\t    clustered_sigmas = np.where(labels == fg_label, 1, 0)\n\t    return clustered_sigmas.reshape((dim1, dim2, dim3))\n\tdef plot_sigmas(sigmas, save_path, plot_file_name):\n\t    sigma_hist_vals = sigmas.astype(int).reshape(-1)\n", "    plt.figure()\n\t    plt.hist(sigma_hist_vals)\n\t    plt.show()\n\t    fig_file_path = os.path.join(save_path, plot_file_name)\n\t    plt.savefig(fig_file_path)\n\tdef translate_obj(pts):\n\t    mean = np.mean(pts, axis=0)\n\t    pts = pts - mean\n\t    return pts\n\tdef extract_sigmas(N_samples, x_range, y_range, z_range, sigma_threshold, network_query_fn, network_fn, min_b, max_b, save_path, kwargs, use_vertex_normal = True, near_t = 1.0):\n", "    # define the dense grid for query\n\t    N = N_samples\n\t    xmin, xmax = x_range\n\t    ymin, ymax = y_range\n\t    zmin, zmax = z_range\n\t    center = np.array([0, -0.5, 4.5])\n\t    # assert xmax-xmin == ymax-ymin == zmax-zmin, 'the ranges must have the same length!'\n\t    x = np.linspace(xmin, xmax, N)\n\t    y = np.linspace(ymin, ymax, N)\n\t    z = np.linspace(zmin, zmax, N)\n", "    samples = np.stack(np.meshgrid(x, y, z), -1)\n\t    xyz_ = torch.FloatTensor(samples.reshape(N ** 2, N, 3)).cuda()\n\t    # sigma is independent of direction, so any value here will produce the same result \n\t    dir_ = torch.zeros(N ** 2, 3).cuda()\n\t    # predict sigma (occupancy) for each grid location\n\t    print('Predicting occupancy ...')\n\t    with torch.no_grad():\n\t        raw = network_query_fn(xyz_, dir_, network_fn)\n\t    # raw[..., 3] = 1. - torch.exp(-raw[..., 3] * 0.05)\n\t    sigma = raw[..., 3].detach().cpu().numpy().reshape((N, N, N))\n", "    clustered_sigma = cluster(sigma, 2)\n\t    # occ_inds = np.where(sigma > sigma_threshold)\n\t    occ_inds = np.where(clustered_sigma > 0)\n\t    occ_samples = samples[occ_inds[0], occ_inds[1], occ_inds[2], :]\n\t    min_corner = np.array([np.min(occ_samples[..., 0]), np.min(occ_samples[..., 1]), np.min(occ_samples[..., 2])])\n\t    max_corner = np.array([np.max(occ_samples[..., 0]), np.max(occ_samples[..., 1]), np.max(occ_samples[..., 2])])\n\t    min_pt, max_pt = get_max_cube(min_corner, max_corner)\n\t    box_pcd, coords = get_coords(min_pt, max_pt, N)\n\t    xyz_ = torch.FloatTensor(coords.reshape(N ** 2, N, 3)).cuda()\n\t    # sigma is independent of direction, so any value here will produce the same result\n", "    dir_ = torch.zeros(N ** 2, 3).cuda()\n\t    # predict sigma (occupancy) for each grid location\n\t    print('Predicting occupancy for resized cube...')\n\t    with torch.no_grad():\n\t        raw = network_query_fn(xyz_, dir_, network_fn)\n\t    # raw[..., 3] = 1. - torch.exp(-raw[..., 3] * 0.05)\n\t    sigma = raw[..., 3].detach().cpu().numpy().reshape((N, N, N))\n\t    # plot_sigmas(sigma, save_path, 'resampled_sigmas.png')\n\t    sigmas_filename = os.path.join(save_path, 'sigmas_%d.npy' % (N))\n\t    np.save(sigmas_filename, sigma)\n", "    samples = coords.reshape((-1, 3))\n\t    samples = translate_obj(samples)\n\t    min_corner = np.array([np.min(samples[:, 0]), np.min(samples[:, 1]), np.min(samples[:, 2])])\n\t    max_corner = np.array([np.max(samples[:, 0]), np.max(samples[:, 1]), np.max(samples[:, 2])])\n\t    abs_max = np.max(np.vstack([np.abs(min_corner), np.abs(max_corner)]), axis=0)\n\t    samples = samples / abs_max\n\t    samples = samples.reshape((N, N, N, 3))\n\t    samples_filename = os.path.join(save_path, 'samples_%d.npy' % (N))\n\t    np.save(samples_filename, samples)\n\t    # print(\"Hi, visualizing now!\")\n", "    # clustered_sigma = cluster(sigma, 2)\n\t    # occ_inds = np.where(clustered_sigma > 0)\n\t    # occ_samples = samples[occ_inds[0], occ_inds[1], occ_inds[2], :]\n\t    # pcd = o3d.geometry.PointCloud()\n\t    # pcd.points = o3d.utility.Vector3dVector(occ_samples.reshape(-1, 3))\n\t    # o3d.visualization.draw_geometries([pcd])\n\t    print('Done!')\n\tdef train(args):\n\t    # Load data\n\t    K = None\n", "    if args.dataset_type == 'brics':\n\t        canonical_pose = None\n\t        input_pose = None\n\t        if args.canonical_path is not None:\n\t            input_poses_path = os.path.join(args.canonical_path, \"%s_input_rot.h5\" % (args.category)) \n\t            canonical_poses_path = os.path.join(args.canonical_path, \"%s_canonical.h5\" % (args.category)) \n\t            canonical_models_path = os.path.join(args.canonical_path, \"%s_files.txt\" % (args.category))\n\t            input_poses = load_h5(input_poses_path)\n\t            canonical_poses = load_h5(canonical_poses_path)\n\t            canonical_models = load_models(canonical_models_path)\n", "            if args.model_name not in canonical_models:\n\t                print(\"%s not found in canonical data\" % (args.model_name))\n\t                return\n\t            input_pose = input_poses[canonical_models.index(args.model_name)]\n\t            canonical_pose = canonical_poses[canonical_models.index(args.model_name)]\n\t        images, poses, render_poses, meta, gt_depths, i_split = load_brics_data(args.datadir, args.res, args.testskip, args.max_ind, canonical_pose, input_pose, args.num_render_poses)\n\t        K = meta['intrinsic_mat']\n\t        hwf = [meta['height'], meta['width'], meta['fx']]\n\t        print('Loaded brics', images.shape, poses.shape, render_poses.shape, K, hwf, args.datadir)\n\t        i_train, i_val, i_test = i_split\n", "        near = args.near\n\t        far = args.far\n\t        if args.white_bkgd:\n\t            images = images[..., :3] * images[..., -1:] + (1. - images[..., -1:])\n\t        else:\n\t            images = images[..., :3]\n\t    else:\n\t        print('Unknown dataset type', args.dataset_type, 'exiting')\n\t        return\n\t    # Cast intrinsics to right types\n", "    H, W, focal = hwf\n\t    H, W = int(H), int(W)\n\t    hwf = [H, W, focal]\n\t    if K is None:\n\t        K = np.array([\n\t            [focal, 0, 0.5*W],\n\t            [0, focal, 0.5*H],\n\t            [0, 0, 1]\n\t        ])\n\t    # if args.render_test:\n", "        # render_poses = np.array(poses[i_test])\n\t    # Create log dir and copy the config file\n\t    basedir = args.basedir\n\t    expname = args.expname\n\t    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n\t    f = os.path.join(basedir, expname, 'args.txt')\n\t    with open(f, 'w') as file:\n\t        for arg in sorted(vars(args)):\n\t            attr = getattr(args, arg)\n\t            file.write('{} = {}\\n'.format(arg, attr))\n", "    if args.config is not None:\n\t        f = os.path.join(basedir, expname, 'config.txt')\n\t        with open(f, 'w') as file:\n\t            file.write(open(args.config, 'r').read())\n\t    # Create nerf model\n\t    render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)\n\t    global_step = start\n\t    bds_dict = {\n\t        'near' : near,\n\t        'far' : far,\n", "    }\n\t    render_kwargs_train.update(bds_dict)\n\t    render_kwargs_test.update(bds_dict)\n\t    # Move testing data to GPU\n\t    # render_poses = torch.Tensor(render_poses).to(device)\n\t    # Short circuit if only rendering out from trained model\n\t    if args.render_only:\n\t        print('RENDER ONLY')\n\t        if args.render_test:\n\t            # render_test switches to test poses\n", "            images = images\n\t        else:\n\t            # Default is smoother render_poses path\n\t            images = None\n\t        testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format('test' if args.render_test else 'path', start))\n\t        os.makedirs(testsavedir, exist_ok=True)\n\t        print('test poses shape', render_poses.shape)\n\t        if args.canonical_path is not None:\n\t            with torch.no_grad():\n\t                rgbs, disps, depths = render_path(poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor, model=args.model_name, category=args.category)\n", "        elif args.render_test:\n\t            with torch.no_grad():\n\t                rgbs, disps, depths = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor, model=args.model_name)\n\t        elif args.gen_sigmas:\n\t            extract_sigmas(args.N_samples, args.x_range, args.y_range, args.z_range, args.sigma_threshold, render_kwargs_train['network_query_fn'], render_kwargs_train['network_fn'], near, far, testsavedir, render_kwargs_test)\n\t        print('Done rendering', testsavedir)\n\t        return\n\t    if args.wand_en:\n\t            wandb.init(project=\"NeRF\",\n\t                       entity=\"rrc_3d\",\n", "                       name=expname)\n\t    # Prepare raybatch tensor if batching random rays\n\t    N_rand = args.N_rand\n\t    use_batching = not args.no_batching\n\t    if use_batching:\n\t        # For random ray batching\n\t        print('get rays')\n\t        rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n\t        print('done, concats')\n\t        rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n", "        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n\t        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n\t        rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n\t        rays_rgb = rays_rgb.astype(np.float32)\n\t        if N_rand:\n\t            print('shuffle rays')\n\t            np.random.shuffle(rays_rgb)\n\t        print('done')\n\t        i_batch = 0\n\t    # Move training data to GPU\n", "    if use_batching:\n\t        images = torch.Tensor(images).to(device)\n\t    poses = torch.Tensor(poses).to(device)\n\t    if use_batching:\n\t        rays_rgb = torch.Tensor(rays_rgb).to(device)\n\t    N_iters = args.iters + 1\n\t    print('Begin')\n\t    print('TRAIN views are', i_train)\n\t    print('TEST views are', i_test)\n\t    print('VAL views are', i_val)\n", "    # Summary writers\n\t    # writer = SummaryWriter(os.path.join(basedir, 'summaries', expname))\n\t    if not N_rand:\n\t        N_rand = H * W\n\t    start = start + 1\n\t    for i in trange(start, N_iters):\n\t        time0 = time.time()\n\t        # Sample random ray batch\n\t        if use_batching:\n\t            # Random over all images\n", "            batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n\t            batch = torch.transpose(batch, 0, 1)\n\t            batch_rays, target_s = batch[:2], batch[2]\n\t            i_batch += N_rand\n\t            if i_batch >= rays_rgb.shape[0]:\n\t                # print(\"Shuffle data after an epoch!\")\n\t                # rand_idx = torch.randperm(rays_rgb.shape[0])\n\t                # rays_rgb = rays_rgb[rand_idx]\n\t                i_batch = 0\n\t        else:\n", "            # Random from one image\n\t            # img_i = np.random.choice(i_train)\n\t            img_i = i % len(i_train)\n\t            target = images[img_i]\n\t            target = torch.Tensor(target).to(device)\n\t            pose = poses[img_i, :3,:4]\n\t            if not (i % len(i_train)) and (i / len(i_train) > 0):\n\t                print(\"Completed %d epochs!\" % (i // len(i_train)))\n\t            if N_rand is not None:\n\t                rays_o, rays_d = get_rays(H, W, K, torch.Tensor(pose))  # (H, W, 3), (H, W, 3)\n", "                if i < args.precrop_iters:\n\t                    dH = int(H//2 * args.precrop_frac)\n\t                    dW = int(W//2 * args.precrop_frac)\n\t                    coords = torch.stack(\n\t                        torch.meshgrid(\n\t                            torch.linspace(H//2 - dH, H//2 + dH - 1, 2*dH), \n\t                            torch.linspace(W//2 - dW, W//2 + dW - 1, 2*dW)\n\t                        ), -1)\n\t                    if i == start:\n\t                        print(f\"[Config] Center cropping of size {2*dH} x {2*dW} is enabled until iter {args.precrop_iters}\")                \n", "                else:\n\t                    coords = torch.stack(torch.meshgrid(torch.linspace(0, H-1, H), torch.linspace(0, W-1, W)), -1)  # (H, W, 2)\n\t                coords = torch.reshape(coords, [-1,2])  # (H * W, 2)\n\t                select_inds = np.random.choice(coords.shape[0], size=[N_rand], replace=False)  # (N_rand,)\n\t                select_coords = coords[select_inds].long()  # (N_rand, 2)\n\t                rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n\t                rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n\t                batch_rays = torch.stack([rays_o, rays_d], 0)\n\t                # target_s = target.reshape((H * W, 3))\n\t                target_s = target[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n", "        #####  Core optimization loop  #####\n\t        rgb, disp, acc, extras = render(H, W, K, chunk=args.chunk, rays=batch_rays,\n\t                                                verbose=i < 10, retraw=True,\n\t                                                **render_kwargs_train)\n\t        optimizer.zero_grad()\n\t        depths = extras['depth_map'].detach().cpu()\n\t        weights = torch.exp(-args.loss_param * depths).to(device)\n\t        weights = weights.unsqueeze(-1).repeat(1, 3)\n\t        img_loss = img2mse(rgb, target_s)\n\t        trans = extras['raw'][...,-1]\n", "        loss = img_loss\n\t        psnr = mse2psnr(img_loss)\n\t        if 'rgb0' in extras:\n\t            depths = extras['depth0'].detach().cpu()\n\t            weights = torch.exp(-args.loss_param * depths).to(device)\n\t            weights = weights.unsqueeze(-1).repeat(1, 3)\n\t            img_loss0 = img2mse(extras['rgb0'], target_s)\n\t            loss = loss + img_loss0\n\t            psnr0 = mse2psnr(img_loss0)\n\t        loss.backward()\n", "        optimizer.step()\n\t        # NOTE: IMPORTANT!\n\t        ###   update learning rate   ###\n\t        decay_rate = 0.1\n\t        decay_steps = args.lrate_decay * 1000\n\t        new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n\t        for param_group in optimizer.param_groups:\n\t            param_group['lr'] = new_lrate\n\t        ################################\n\t        dt = time.time()-time0\n", "        # print(f\"Step: {global_step}, Loss: {loss}, Time: {dt}\")\n\t        #####           end            #####\n\t        # Rest is logging\n\t        # print(\"Training rgb info: \", type(rgb), rgb.shape, target_s.shape)\n\t        # print(np.unique(rgb.detach().cpu().numpy(), return_counts=True))\n\t        # print(np.unique(target_s.detach().cpu().numpy(), return_counts=True))\n\t        # tqdm.write(f\"Info: {rgb.shape}, {target_s.shape}, \\nUnique GT Values: {np.unique(target_s.detach().cpu().numpy(), return_counts=True)} \\nUnique Rendered Values: {np.unique(rgb.detach().cpu().numpy(), return_counts=True)}\")\n\t        if args.wand_en:\n\t            render_H = int(N_rand ** 0.5)\n\t            render_W = int(N_rand ** 0.5)\n", "            if N_rand == H * W:\n\t                render_H = H\n\t                render_W = W\n\t            log_dict = {\n\t                \"Train Loss\": loss.item(),\n\t                \"Train PSNR\": psnr.item(),\n\t                \"Rendered vs GT Train Image\": [wandb.Image(rgb.detach().cpu().numpy().reshape((render_H, render_W, 3))), wandb.Image(target_s.detach().cpu().numpy().reshape((render_H, render_W, 3)))]\n\t                }\n\t            wandb.log(log_dict)\n\t        if i%args.i_weights==0:\n", "            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n\t            torch.save({\n\t                'global_step': global_step,\n\t                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n\t                'network_fine_state_dict': render_kwargs_train['network_fine'].state_dict(),\n\t                'optimizer_state_dict': optimizer.state_dict(),\n\t            }, path)\n\t            print('Saved checkpoints at', path)\n\t        if i%args.i_video==0 and i > 0:\n\t            # Turn on testing mode\n", "            with torch.no_grad():\n\t                rgbs, disps, depths = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test)\n\t            print('Done, saving', rgbs.shape, disps.shape)\n\t            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n\t            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n\t            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n\t            # if args.use_viewdirs:\n\t            #     render_kwargs_test['c2w_staticcam'] = render_poses[0][:3,:4]\n\t            #     with torch.no_grad():\n\t            #         rgbs_still, _ = render_path(render_poses, hwf, args.chunk, render_kwargs_test)\n", "            #     render_kwargs_test['c2w_staticcam'] = None\n\t            #     imageio.mimwrite(moviebase + 'rgb_still.mp4', to8b(rgbs_still), fps=30, quality=8)\n\t        if i%args.i_testset==0 and i > 0:\n\t            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n\t            os.makedirs(testsavedir, exist_ok=True)\n\t            print('test poses shape', poses[i_test].shape)\n\t            with torch.no_grad():\n\t                render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, args.chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n\t            print('Saved test set')\n\t        if i%args.i_print==0:\n", "            tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n\t            if i%args.i_img==0:\n\t                # Log a rendered validation view\n\t                img_i=np.random.choice(i_val)\n\t                target = images[img_i]\n\t                pose = poses[img_i, :3,:4]\n\t                with torch.no_grad():\n\t                    rgb, disp, acc, extras = render(H, W, K, chunk=args.chunk, c2w=pose,\n\t                                                        **render_kwargs_test)\n\t                img_loss = img2mse(rgb, torch.tensor(target))\n", "                loss = img_loss\n\t                psnr = mse2psnr(img_loss)\n\t                if 'rgb0' in extras:\n\t                    img_loss0 = img2mse(extras['rgb0'], torch.tensor(target))\n\t                    loss = loss + img_loss0\n\t                    psnr0 = mse2psnr(img_loss0)\n\t                tqdm.write(f\"[TRAIN] Iter: {i} Validation Loss: {loss.item()}  Validation PSNR: {psnr.item()}\")\n\t                if args.wand_en:\n\t                    log_dict = {\n\t                        \"Validation Loss\": loss.item(),\n", "                        \"Validation PSNR\": psnr.item(),\n\t                        \"Rendered vs GT Image\": [wandb.Image(rgb.detach().cpu().numpy().reshape((H, W, 3))), wandb.Image(target)],\n\t                        \"Disparity\": [wandb.Image(disp.detach().cpu().numpy())],\n\t                        \"Opacity\": [wandb.Image(acc.detach().cpu().numpy())],\n\t                        \"Depth\": [wandb.Image(extras['depth_map'].detach().cpu().numpy())],\n\t                        }\n\t                    wandb.log(log_dict)\n\t        \"\"\"\n\t            print(expname, i, psnr.numpy(), loss.numpy(), global_step.numpy())\n\t            print('iter time {:.05f}'.format(dt))\n", "            with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_print):\n\t                tf.contrib.summary.scalar('loss', loss)\n\t                tf.contrib.summary.scalar('psnr', psnr)\n\t                tf.contrib.summary.histogram('tran', trans)\n\t                if args.N_importance > 0:\n\t                    tf.contrib.summary.scalar('psnr0', psnr0)\n\t            if i%args.i_img==0:\n\t                # Log a rendered validation view to Tensorboard\n\t                img_i=np.random.choice(i_val)\n\t                target = images[img_i]\n", "                pose = poses[img_i, :3,:4]\n\t                with torch.no_grad():\n\t                    rgb, disp, acc, extras = render(H, W, focal, chunk=args.chunk, c2w=pose,\n\t                                                        **render_kwargs_test)\n\t                psnr = mse2psnr(img2mse(rgb, target))\n\t                with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_img):\n\t                    tf.contrib.summary.image('rgb', to8b(rgb)[tf.newaxis])\n\t                    tf.contrib.summary.image('disp', disp[tf.newaxis,...,tf.newaxis])\n\t                    tf.contrib.summary.image('acc', acc[tf.newaxis,...,tf.newaxis])\n\t                    tf.contrib.summary.scalar('psnr_holdout', psnr)\n", "                    tf.contrib.summary.image('rgb_holdout', target[tf.newaxis])\n\t                if args.N_importance > 0:\n\t                    with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_img):\n\t                        tf.contrib.summary.image('rgb0', to8b(extras['rgb0'])[tf.newaxis])\n\t                        tf.contrib.summary.image('disp0', extras['disp0'][tf.newaxis,...,tf.newaxis])\n\t                        tf.contrib.summary.image('z_std', extras['z_std'][tf.newaxis,...,tf.newaxis])\n\t        \"\"\"\n\t        global_step += 1\n\t        # del batch, batch_rays, target_s, img_loss, rgb, disp, acc, extras\n\tif __name__=='__main__':\n", "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n\t    torch.cuda.set_device(device_idx)\n\t    parser = config_parser()\n\t    args = parser.parse_args()\n\t    sel = []\n\t    if args.multi_scene and args.render_only:\n\t        for dir_name in sorted(os.listdir(args.root_dir)):\n\t            args.expname = dir_name\n\t            category_name = dir_name.split(\"_\")[1]\n\t            args.model_name = dir_name.split(\"_\")[2] + \"_\" + dir_name.split(\"_\")[3]\n", "            # if args.model_name not in sel:\n\t                # continue\n\t            print(\"Processing \", args.model_name)\n\t            args.ft_path = os.path.join(args.root_dir, dir_name, f\"{args.iters:06d}.tar\")\n\t            if not os.path.exists(args.ft_path):\n\t                print(\"Skipping %s as no saved model found!\" % (args.model_name))\n\t                continue\n\t            train(args)\n\t    else:\n\t        train(args)\n"]}
{"filename": "nerf/grad_vis.py", "chunked_list": ["import open3d as o3d\n\timport numpy as np\n\timport os\n\timport argparse\n\timport sys\n\timport open3d as o3d\n\timport seaborn as sns\n\timport numpy as np\n\tfrom scipy.spatial.transform import Rotation as Rotation\n\timport torch\n", "from scipy.spatial import distance\n\timport torch.nn\n\tfrom scipy.ndimage import gaussian_filter1d\n\tfrom scipy.ndimage import gaussian_filter\n\tdef get_gradient_density(x):\n\t    # x - B, H, W, D\n\t    B, H, W, D = x.shape\n\t    d_x = x[..., 1:, :, :] - x[..., :-1, :, :] # B, H - 1, W, D\n\t    d_y = x[..., :, 1:, :] - x[..., :, :-1, :] # B, H, W - 1, D\n\t    d_y = -d_y\n", "    d_z = x[..., :, :, 1:] - x[..., :, :, :-1] # B, H, W, D - 1\n\t    d_x = torch.nn.functional.interpolate(d_x.unsqueeze(1), (H, W, D)).squeeze(1)\n\t    d_y = torch.nn.functional.interpolate(d_y.unsqueeze(1), x.shape[1:]).squeeze(1)\n\t    d_z = torch.nn.functional.interpolate(d_z.unsqueeze(1), x.shape[1:]).squeeze(1)\n\t    gradient = torch.stack([d_x, d_y, d_z], 1)\n\t    return gradient # B, 3, H, W, D\n\tdef draw_oriented_pointcloud(x, n, t=1.0):\n\t    a = x\n\t    b = x + t * n\n\t    points = []\n", "    lines = []\n\t    for i in range(a.shape[0]):\n\t        ai = [a[i, 0], a[i, 1], a[i, 2]]\n\t        bi = [b[i, 0], b[i, 1], b[i, 2]]\n\t        points.append(ai)\n\t        points.append(bi)\n\t        lines.append([2*i, 2*i+1])\n\t    colors = [[1, 0, 0] for i in range(len(lines))]\n\t    pcd = o3d.geometry.PointCloud()\n\t    point_colors = np.ones(x.shape)\n", "    pcd.points = o3d.utility.Vector3dVector(a)\n\t    pcd.colors = o3d.utility.Vector3dVector(point_colors)\n\t    line_set = o3d.geometry.LineSet(\n\t        points=o3d.utility.Vector3dVector(points),\n\t        lines=o3d.utility.Vector2iVector(lines),\n\t    )\n\t    line_set.colors = o3d.utility.Vector3dVector(colors)\n\t    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n\t    size=0.6, origin=[-1, -1, -1])\n\t    o3d.visualization.draw_geometries([line_set, pcd,mesh_frame])\n", "parser = argparse.ArgumentParser(description=\"NeRF density gradient field visualization\")\n\tparser.add_argument(\"--input_dir\", required=True, type = str)\n\tparser.add_argument(\"--res\", default=32, type = int)\n\tparser.add_argument(\"--max_files\", default=10, type = int)\n\targs = parser.parse_args()\n\tcount = 0\n\tfor file in os.listdir(args.input_dir):\n\t    if \"_sigmas_%d.npy\" % (args.res) not in file:\n\t        continue\n\t    sigmas_path = os.path.join(args.input_dir, file)\n", "    samples_file = file.replace(\"sigmas\", \"samples\")\n\t    samples_path = os.path.join(args.input_dir, samples_file)\n\t    density = np.load(sigmas_path)\n\t    coords = np.load(samples_path)\n\t    density = density.transpose(2, 1, 0)\n\t    density = gaussian_filter(density, sigma=1)\n\t    density = torch.from_numpy(density)\n\t    coords = coords.transpose(2, 1, 0, 3)\n\t    coords = coords.reshape(-1, 3)\n\t    scale_ = 0.1\n", "    grad = get_gradient_density(density.unsqueeze(0)).squeeze(0)\n\t    grad = grad.permute(1, 2, 3, 0).reshape(-1,3)\n\t    grad = grad.detach().numpy()\n\t    draw_oriented_pointcloud(coords, grad, scale_)\n\t    count += 1\n\t    if count >= args.max_files:\n\t        break\n"]}
{"filename": "cafi_net/metrics_test.py", "chunked_list": ["import hydra, logging\n\timport torch, glob, os\n\timport numpy as np\n\tfrom trainers import *\n\tfrom models import *\n\tfrom pytorch_lightning import Trainer, seed_everything\n\tfrom pytorch_lightning.loggers import CometLogger, TensorBoardLogger, WandbLogger\n\tfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\tlog = logging.getLogger(__name__)\n\t@hydra.main(config_path=\"configs\", config_name=\"Canonical_fields.yaml\")\n", "def run(cfg):\n\t    seed_everything(cfg.utils.seed)\n\t    train_logger = eval(cfg.logging.type)(project = cfg.logging.project)\n\t    log.info(cfg)\n\t    print(os.getcwd())\n\t    model = getattr(eval(cfg.trainer_file.file), cfg.trainer_file.type).load_from_checkpoint(os.path.join(hydra.utils.get_original_cwd(), cfg.test.weights)).eval().cuda()\n\t    model.run_metrics(cfg)\n\tif __name__ == '__main__':\n\t    run()\n"]}
{"filename": "cafi_net/main.py", "chunked_list": ["import hydra, logging\n\timport torch, glob, os\n\timport numpy as np\n\tfrom trainers import *\n\tfrom models import *\n\tfrom pytorch_lightning import Trainer, seed_everything\n\tfrom pytorch_lightning.loggers import CometLogger, TensorBoardLogger, WandbLogger\n\tfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\tlog = logging.getLogger(__name__)\n\t@hydra.main(config_path=\"configs\", config_name=\"Canonical_fields.yaml\")\n", "def run(cfg):\n\t    seed_everything(cfg.utils.seed)\n\t    train_logger = eval(cfg.logging.type)(project = cfg.logging.project)\n\t    log.info(cfg)\n\t    print(os.getcwd())\n\t    checkpoint_callback = ModelCheckpoint(**cfg.callback.model_checkpoint.segmentation.args)\n\t    model = getattr(eval(cfg.trainer_file.file), cfg.trainer_file.type)(configs = cfg)\n\t    trainer = Trainer(**cfg.trainer, callbacks = [checkpoint_callback], logger = train_logger)\n\t    trainer.fit(model)\n\tif __name__ == '__main__':\n", "    run()\n"]}
{"filename": "cafi_net/canonical_render.py", "chunked_list": ["import hydra, logging\n\timport torch, glob, os\n\timport numpy as np\n\tfrom trainers import *\n\tfrom models import *\n\tfrom pytorch_lightning import Trainer, seed_everything\n\tfrom pytorch_lightning.loggers import CometLogger, TensorBoardLogger, WandbLogger\n\tfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\tlog = logging.getLogger(__name__)\n\t@hydra.main(config_path=\"configs\", config_name=\"Canonical_fields.yaml\")\n", "def run(cfg):\n\t    seed_everything(cfg.utils.seed)\n\t    train_logger = eval(cfg.logging.type)(project = cfg.logging.project)\n\t    log.info(cfg)\n\t    print(os.getcwd())\n\t    model = getattr(eval(cfg.trainer_file.file), cfg.trainer_file.type).load_from_checkpoint(os.path.join(hydra.utils.get_original_cwd(), cfg.test.weights)).eval().cuda()\n\t    model.run_canonica_render(cfg)\n\tif __name__ == '__main__':\n\t    run()\n"]}
{"filename": "cafi_net/tester.py", "chunked_list": ["import hydra, logging\n\timport torch, glob, os\n\timport numpy as np\n\tfrom trainers import *\n\tfrom models import *\n\tfrom pytorch_lightning import Trainer, seed_everything\n\tfrom pytorch_lightning.loggers import CometLogger, TensorBoardLogger, WandbLogger\n\tfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\tlog = logging.getLogger(__name__)\n\t@hydra.main(config_path=\"configs\", config_name=\"Canonical_fields.yaml\")\n", "def run(cfg):\n\t    seed_everything(cfg.utils.seed)\n\t    train_logger = eval(cfg.logging.type)(project = cfg.logging.project)\n\t    log.info(cfg)\n\t    print(os.getcwd())\n\t    model = getattr(eval(cfg.trainer_file.file), cfg.trainer_file.type).load_from_checkpoint(os.path.join(hydra.utils.get_original_cwd(), cfg.test.weights)).eval().cuda()\n\t    model.run_test(cfg)\n\tif __name__ == '__main__':\n\t    run()\n"]}
{"filename": "cafi_net/vis_utils.py", "chunked_list": ["import seaborn as sns\n\timport open3d as o3d\n\timport numpy as np\n\timport torch\n\timport argparse, os, sys\n\timport glob, os\n\tdef create_color_samples(N):\n\t    '''\n\t    Creates N distinct colors\n\t    N x 3 output\n", "    '''\n\t    palette = sns.color_palette(None, N)\n\t    palette = np.array(palette)\n\t    return palette\n\tdef convert_tensor_2_numpy(x):\n\t    '''\n\t    Convert pytorch tensor to numpy\n\t    '''\n\t    out = torch.squeeze(x, axis = 0).numpy()\n\t    return out \n", "def save_pointcloud(x, filename = \"./pointcloud.ply\"):\n\t    '''\n\t    Save point cloud to the destination given in the filename\n\t    x can be list of inputs (Nx3) capsules or numpy array of N x 3\n\t    '''\n\t    label_map = []\n\t    if isinstance(x, list):\n\t        pointcloud = []\n\t        labels = create_color_samples(len(x))\n\t        for i in range(len(x)):\n", "            pts = x[i]\n\t            # print(pts.shape, \"vis\")\n\t            pts = convert_tensor_2_numpy(pts)\n\t            pointcloud.append(pts)\n\t            label_map.append(np.repeat(labels[i:(i + 1)], pts.shape[0], axis = 0))\n\t        # x = np.concatenate(x, axis = 0)\n\t        pointcloud = np.concatenate(pointcloud, axis = 0)\n\t        x = pointcloud.copy()\n\t        label_map = np.concatenate(label_map, axis = 0)\n\t    else:\n", "        x = convert_tensor_2_numpy(x)\n\t        # print(x.shape)\n\t        label_map = np.ones((len(x), 3)) * 0.5\n\t    pcd = o3d.geometry.PointCloud()\n\t    pcd.points = o3d.utility.Vector3dVector(x)\n\t    pcd.colors = o3d.utility.Vector3dVector(label_map)\n\t    o3d.io.write_point_cloud(filename, pcd)\n\tdef visualize_outputs(base_path, pointcloud_name, start = 0, max_num = 10, spacing = 1.0, skip = 1):\n\t    '''\n\t    Visualize point clouds in open3D \n", "    '''\n\t    filename = glob.glob(os.path.join(base_path, \"\")  + pointcloud_name)\n\t    filename.sort()\n\t    print(filename)\n\t    filename_2 = []\n\t    for f in filename:\n\t        #print(f)\n\t        print(os.path.join(base_path, \"\") + f.split(\"/\")[-1].split(\"_\")[0] +\"_\" + pointcloud_name, f)\n\t        #if (os.path.join(base_path, \"\") + f.split(\"/\")[-1].split(\"_\")[0] +\"_\" + pointcloud_name) == f:\n\t        filename_2.append(f)\n", "    filename = filename_2\n\t    filename = filename[start::skip]\n\t    filename = filename[:max_num]\n\t    # print(filename)\n\t    num_pcds = len(filename)\n\t    if skip != 1:\n\t        num_pcds = len(filename) // skip\n\t    rows = np.floor(np.sqrt(num_pcds))\n\t    pcd_list = []\n\t    arrow_list = []\n", "    pcd_iter = 0\n\t    pcd_index = 0\n\t    for pcd_file in filename:\n\t        print(pcd_file)\n\t        if pcd_iter % skip == 0:\n\t            pcd = o3d.io.read_point_cloud(pcd_file)\n\t            pcd.colors = pcd.points\n\t            column_num = pcd_index // rows\n\t            row_num = pcd_index % rows\n\t            vector = (row_num * spacing, column_num * spacing, 0)\n", "            #vector = [0, 0, 0]\n\t            # print(vector)\n\t            pcd.translate(vector)\n\t            pcd_list.append(pcd)\n\t            pcd_index += 1\n\t            U, S, V = torch.pca_lowrank(torch.tensor(np.array(pcd.points)))\n\t            arrow = get_arrow(torch.tensor([0, 0, 0]) + torch.tensor(list(vector)), V[:, 0] + torch.tensor(list(vector)))\n\t            arrow_list.append(arrow)\n\t        pcd_iter +=1\n\t    #o3d.visualization.draw_geometries(arrow_list)\n", "    o3d.visualization.draw_geometries(pcd_list)\n\tdef draw_geometries(pcds):\n\t    \"\"\"\n\t    Draw Geometries\n\t    Args:\n\t        - pcds (): [pcd1,pcd2,...]\n\t    \"\"\"\n\t    o3d.visualization.draw_geometries(pcds)\n\tdef get_o3d_FOR(origin=[0, 0, 0],size=10):\n\t    \"\"\" \n", "    Create a FOR that can be added to the open3d point cloud\n\t    \"\"\"\n\t    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(\n\t    size=size)\n\t    mesh_frame.translate(origin)\n\t    return(mesh_frame)\n\tdef vector_magnitude(vec):\n\t    \"\"\"\n\t    Calculates a vector's magnitude.\n\t    Args:\n", "        - vec (): \n\t    \"\"\"\n\t    magnitude = np.sqrt(np.sum(vec**2))\n\t    return(magnitude)\n\tdef calculate_zy_rotation_for_arrow(vec):\n\t    \"\"\"\n\t    Calculates the rotations required to go from the vector vec to the \n\t    z axis vector of the original FOR. The first rotation that is \n\t    calculated is over the z axis. This will leave the vector vec on the\n\t    XZ plane. Then, the rotation over the y axis. \n", "    Returns the angles of rotation over axis z and y required to\n\t    get the vector vec into the same orientation as axis z\n\t    of the original FOR\n\t    Args:\n\t        - vec (): \n\t    \"\"\"\n\t    # Rotation over z axis of the FOR\n\t    gamma = np.arctan(vec[1]/vec[0])\n\t    Rz = np.array([[np.cos(gamma),-np.sin(gamma),0],\n\t                   [np.sin(gamma),np.cos(gamma),0],\n", "                   [0,0,1]])\n\t    # Rotate vec to calculate next rotation\n\t    vec = Rz.T@vec.reshape(-1,1)\n\t    vec = vec.reshape(-1)\n\t    # Rotation over y axis of the FOR\n\t    beta = np.arctan(vec[0]/vec[2])\n\t    Ry = np.array([[np.cos(beta),0,np.sin(beta)],\n\t                   [0,1,0],\n\t                   [-np.sin(beta),0,np.cos(beta)]])\n\t    return(Rz, Ry)\n", "def create_arrow(scale=10):\n\t    \"\"\"\n\t    Create an arrow in for Open3D\n\t    \"\"\"\n\t    cone_height = scale*0.2\n\t    cylinder_height = scale*0.8\n\t    cone_radius = scale/10\n\t    cylinder_radius = scale/20\n\t    mesh_frame = o3d.geometry.TriangleMesh.create_arrow(cone_radius=0.2,\n\t        cone_height=0.1,\n", "        cylinder_radius=0.1,\n\t        cylinder_height=0.5)\n\t    return(mesh_frame)\n\tdef get_arrow(origin=[0, 0, 0], end=None, vec=None):\n\t    \"\"\"\n\t    Creates an arrow from an origin point to an end point,\n\t    or create an arrow from a vector vec starting from origin.\n\t    Args:\n\t        - end (): End point. [x,y,z]\n\t        - vec (): Vector. [i,j,k]\n", "    \"\"\"\n\t    scale = 10\n\t    Ry = Rz = np.eye(3)\n\t    T = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n\t    T[:3, -1] = origin\n\t    if end is not None:\n\t        vec = np.array(end) - np.array(origin)\n\t    elif vec is not None:\n\t        vec = np.array(vec)\n\t    if end is not None or vec is not None:\n", "        scale = vector_magnitude(vec)\n\t        Rz, Ry = calculate_zy_rotation_for_arrow(vec)\n\t    scale = 1\n\t    mesh = create_arrow(scale)\n\t    # Create the arrow\n\t    mesh.rotate(Ry, center=np.array([0, 0, 0]))\n\t    mesh.rotate(Rz, center=np.array([0, 0, 0]))\n\t    mesh.translate(origin)\n\t    return(mesh)\n\tif __name__ == \"__main__\":\n", "    # Argument parser\n\t    parser = argparse.ArgumentParser(description = \"Visualization script\")\n\t    parser.add_argument(\"--base_path\", help = \"Base path to folder\", required = True)\n\t    parser.add_argument(\"--pcd\", help = \"PCD string to visualize\", required = True)\n\t    parser.add_argument(\"--spacing\", help = \"Spacing\", default = 2.0, type = float)\n\t    parser.add_argument(\"--num_list\", help = \"indices\", nargs=\"+\", default = list(range(9)), type = int)\n\t    parser.add_argument(\"--start\", help = \"start index\", default = 0, type = int)\n\t    parser.add_argument(\"--num\", help = \"number of models\", default = 9, type = int)\n\t    parser.add_argument(\"--skip\", help = \"number of models to skip\", default = 1, type = int)\n\t    args = parser.parse_args()\n", "    #######################################################################\n\t    visualize_outputs(args.base_path, args.pcd, spacing = args.spacing, start = args.start, max_num = args.num, skip = args.skip)\n"]}
{"filename": "cafi_net/utils/pooling.py", "chunked_list": ["import torch\n\tdef kd_pooling_1d(x, pool_size, pool_mode='avg'):\n\t    \"\"\"\n\t    Expects kd tree indexed points\n\t    x - [B, N_{in}, 3]\n\t    out - [B, N_{out}, 3]\n\t    \"\"\"\n\t    #assert (isPowerOfTwo(pool_size))\n\t    pool_size = pool_size\n\t    if pool_mode == 'max':\n", "        pool = torch.nn.MaxPool1d(pool_size)#, stride = 1)\n\t    else:\n\t        pool = torch.nn.AvgPool1d(pool_size)#, stride = 1)\n\t    if isinstance(x, list):\n\t        y = []\n\t        for i in range(len(x)):\n\t            x_pool = pool(x[i].permute(0, 2, 1)).permute(0, 2, 1)\n\t            x.append(x_pool)\n\t    elif isinstance(x, dict):\n\t        y = dict()\n", "        for l in x:\n\t            if isinstance(l, int):\n\t                x_pool = pool(x[l].permute(0, 2, 1)).permute(0, 2, 1)\n\t                y[l] = x_pool\n\t    else:\n\t        x_pool = pool(x.permute(0, 2, 1)).permute(0, 2, 1)\n\t        y = x_pool\n\t    return y\n\tdef kd_pooling_3d(x, pool_size, pool_mode='avg'):\n\t    \"\"\"\n", "    Expects kd tree indexed points\n\t    x - [B, H_{in}, W_{in}, D_{in}, 3]\n\t    out - [B, H_{out}, W_{out}, D_{out}, 3]\n\t    \"\"\"\n\t    pool_size = pool_size\n\t    if pool_mode == 'max':\n\t        pool = torch.nn.MaxPool3d(pool_size, stride = pool_size)#, stride = 1)\n\t    else:\n\t        pool = torch.nn.AvgPool3d(pool_size, stride = pool_size)#, stride = 1)\n\t    if isinstance(x, list):\n", "        y = []\n\t        for i in range(len(x)):\n\t            x_pool = pool(x[i].permute(0, 4, 1, 2, 3)).permute(0, 2, 3, 4, 1)\n\t            x.append(x_pool)\n\t    elif isinstance(x, dict):\n\t        y = dict()\n\t        for l in x:\n\t            if isinstance(l, int):\n\t                x_pool = pool(x[l].permute(0, 4, 1, 2, 3)).permute(0, 2, 3, 4, 1)\n\t                y[l] = x_pool\n", "    else:\n\t        x_pool = pool(x.permute(0, 4, 1, 2, 3)).permute(0, 2, 3, 4, 1)\n\t        y = x_pool\n\t    return y\n\tif __name__ == \"__main__\":\n\t    # x = torch.randn((2, 4, 3))\n\t    x = torch.randn((2, 16, 16, 16, 3))\n\t    out = kd_pooling_3d(x, 2)\n\t    print(x, out)\n\t    print(x.shape, out.shape)\n"]}
{"filename": "cafi_net/utils/losses.py", "chunked_list": ["import torch\n\timport numpy as np\n\timport sys\n\tsys.path.append('../')\n\tfrom utils.pointcloud_utils import sq_distance_mat,compute_centroids\n\tdef equilibrium_loss(unnormalized_capsules):\n\t    a = torch.mean(unnormalized_capsules, dim=1, keepdims=False)\n\t    am = torch.mean(a, dim=-1, keepdims=True)\n\t    l = torch.subtract(a, am)\n\t    l = l*l\n", "    return torch.mean(l)\n\tdef localization_loss_new(points, capsules, centroids):\n\t    points_centered = points[:, :, None] - centroids[:, None, :] # B, N, K, 3\n\t    points_centered_activated = capsules[:, :, :, None] * points_centered\n\t    l = points_centered.permute(0,2,1,3) # B, K, N, 3\n\t    l_1 = points_centered_activated.permute(0,2,3,1) # B, K, 3, N\n\t    covariance = l_1 @ l\n\t    loss = torch.mean(torch.diagonal(covariance))\n\t    return loss\n\tdef l2_distance_batch(x, y):\n", "    z = x - y\n\t    z = z * z\n\t    z = torch.sum(z, dim=-1)\n\t    z = torch.sqrt(z)\n\t    return z\n\tdef chamfer_distance_l2_batch(X, Y, sq_dist_mat=None):\n\t    if sq_dist_mat is None:\n\t        # compute distance mat\n\t        D2 = sq_distance_mat(X, Y)\n\t    else:\n", "        D2 = sq_dist_mat\n\t    dXY = torch.sqrt(torch.max(torch.min(D2, dim=-1, keepdims=False).values, torch.tensor(0.000001)))\n\t    dXY = torch.mean(dXY, dim=1, keepdims=False)\n\t    dYX = torch.sqrt(torch.max(torch.min(D2, dim=1, keepdims=False).values, torch.tensor(0.000001)))\n\t    dYX = torch.mean(dYX, dim=-1, keepdims=False)\n\t    d = dXY + dYX\n\t    return 0.5*d"]}
{"filename": "cafi_net/utils/__init__.py", "chunked_list": ["#from . import pointcloud_utils\n\t#from . import diffusion_utils\n\t#from . import diffusion_geometry"]}
{"filename": "cafi_net/utils/train_utils.py", "chunked_list": ["from scipy.spatial.transform import Rotation\n\timport torch\n\tdef random_rotate(x):\n\t    \"\"\"\n\t    x - B, N, 3\n\t    out - B, N, 3\n\t    Randomly rotate point cloud\n\t    \"\"\"\n\t    out = perform_rotation(torch.from_numpy(Rotation.random(x.shape[0]).as_matrix()).type_as(x), x)\n\t    return out\n", "def mean_center(x):\n\t    \"\"\"\n\t    x - B, N, 3\n\t    x_mean - B, N, 3\n\t    Mean center point cloud\n\t    \"\"\"\n\t    out = x - x.mean(-2, keepdims = True)\n\t    return out\n\tdef perform_rotation(R, x):\n\t    '''\n", "    Perform rotation on point cloud\n\t    R - B, 3, 3\n\t    x - B, N, 3\n\t    out - B, N, 3\n\t    '''\n\t    out = torch.einsum(\"bij,bpj->bpi\", R.type_as(x), x)\n\t    return out\n\tdef orthonormalize_basis(basis):\n\t    \"\"\"\n\t    Returns orthonormal basis vectors\n", "    basis - B, 3, 3\n\t    out - B, 3, 3\n\t    \"\"\"\n\t    try:\n\t        u, s, v = torch.svd(basis)\n\t    except:                     # torch.svd may have convergence issues for GPU and CPU.\n\t        u, s, v = torch.svd(basis + 1e-3*basis.mean()*torch.rand_like(basis).type_as(basis))\n\t    # u, s, v = torch.svd(basis)\n\t    out = u @ v.transpose(-2, -1)    \n\t    return out"]}
{"filename": "cafi_net/utils/pointcloud_utils.py", "chunked_list": ["import torch\n\tfrom utils.group_points import gather_idx\n\timport numpy as np\n\timport h5py\n\timport open3d as o3d\n\timport seaborn as sns\n\tfrom spherical_harmonics.spherical_cnn import zernike_monoms\n\tfrom spherical_harmonics.spherical_cnn import torch_fibonnacci_sphere_sampling\n\tfrom sklearn.mixture import GaussianMixture\n\tfrom sklearn.cluster import KMeans\n", "def kron(a, b):\n\t    \"\"\"\n\t    Kronecker product of matrices a and b with leading batch dimensions.\n\t    Batch dimensions are broadcast. The number of them mush\n\t    :type a: torch.Tensor\n\t    :type b: torch.Tensor\n\t    :rtype: torch.Tensor\n\t    \"\"\"\n\t    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n\t    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n", "    siz0 = res.shape[:-4]\n\t    return res.reshape(siz0 + siz1)\n\tdef compute_centroids(points, capsules):\n\t    return torch.einsum('bij,bic->bcj', points, capsules)\n\tdef sq_distance_mat(pcd_1, pcd_2):\n\t    r0 = pcd_1 * pcd_1\n\t    r0 = torch.sum(r0, dim=2, keepdims=True)\n\t    r1 = (pcd_2 * pcd_2)\n\t    r1 = torch.sum(r1, dim=2, keepdims=True)\n\t    r1 = r1.permute(0, 2, 1)\n", "    sq_distance_mat = r0 - 2. * (pcd_1 @ pcd_2.permute(0, 2, 1)) + r1\n\t    return sq_distance_mat.squeeze(-1)\n\tdef convert_yzx_to_xyz_basis(basis):\n\t    # basis - N, 3, 3\n\t    rot_y = torch.tensor([[np.cos(np.pi / 2), 0, np.sin(np.pi / 2)]\n\t              ,[0,              1,                      0], \n\t              [-np.sin(np.pi / 2), 0, np.cos(np.pi / 2)]]).type_as(basis)\n\t    rot_z = torch.tensor([\n\t                    [np.cos(np.pi / 2), -np.sin(np.pi / 2), 0],\n\t                    [np.sin(np.pi / 2), np.cos(np.pi / 2), 0],\n", "                    [0, 0, 1]\n\t                    ]).type_as(basis)\n\t    transform = (rot_y @ rot_z).unsqueeze(0).type_as(basis)\n\t    transform = transform.repeat(basis.shape[0], 1, 1)\n\t    if len(basis.shape) == 4:\n\t        transform = transform.unsqueeze(1)\n\t    return transform @ basis\n\tdef create_color_samples(N):\n\t    '''\n\t    Creates N distinct colors\n", "    N x 3 output\n\t    '''\n\t    palette = sns.color_palette(None, N)\n\t    palette = np.array(palette)\n\t    return palette\n\tdef convert_tensor_2_numpy(x):\n\t    '''\n\t    Convert pytorch tensor to numpy\n\t    '''\n\t    out = x.squeeze(0).detach().cpu().numpy()\n", "    return out \n\tdef save_density(x, filename = \"./pointcloud.ply\"):\n\t    density_int = x.cpu().numpy()\n\t    shape_ = density_int.shape \n\t    density_int = density_int.reshape(-1,1)\n\t    mask_density_int = np.ones_like(density_int) * -1\n\t    model = KMeans(init=\"k-means++\",n_clusters=2)\n\t    model.fit(density_int)\n\t    label = model.predict(density_int)\n\t    clusters = np.unique(label)\n", "    if np.mean(density_int[np.where(label == 1)[0]]) > np.mean(density_int[np.where(label == 0)[0]]):\n\t        fg_idx = np.where(label == 1)[0]\n\t        bg_idx = np.where(label == 0)[0]\n\t    else:\n\t        fg_idx = np.where(label == 0)[0]\n\t        bg_idx = np.where(label == 1)[0]\n\t    mask_density_int[fg_idx] = 1.\n\t    mask_density_int[bg_idx] = 0.\n\t    mask_density_tensor = torch.from_numpy(mask_density_int)\n\t    mask_density_int = mask_density_int.reshape(shape_)\n", "    sampling_grid = get_xyz_grid(x)\n\t    sampling_grid_np = sampling_grid.detach().cpu().numpy()\n\t    pts = sampling_grid_np[mask_density_int == 1., :]\n\t    pcd = o3d.geometry.PointCloud()\n\t    pcd.points = o3d.utility.Vector3dVector(pts)\n\t    label_map = np.ones((pts.shape[0], 3)) * 0.5\n\t    pcd.colors = o3d.utility.Vector3dVector(label_map)\n\t    o3d.io.write_point_cloud(filename, pcd)\n\tdef save_pointcloud(x, filename = \"./pointcloud.ply\"):\n\t    '''\n", "    Save point cloud to the destination given in the filename\n\t    x can be list of inputs (Nx3) capsules or numpy array of N x 3\n\t    '''\n\t    label_map = []\n\t    if isinstance(x, list):\n\t        pointcloud = []\n\t        labels = create_color_samples(len(x))\n\t        for i in range(len(x)):\n\t            pts = x[i]\n\t            # print(pts.shape, \"vis\")\n", "            pts = convert_tensor_2_numpy(pts)\n\t            pointcloud.append(pts)\n\t            label_map.append(np.repeat(labels[i:(i + 1)], pts.shape[0], axis = 0))\n\t        # x = np.concatenate(x, axis = 0)\n\t        pointcloud = np.concatenate(pointcloud, axis = 0)\n\t        x = pointcloud.copy()\n\t        label_map = np.concatenate(label_map, axis = 0)\n\t    else:\n\t        x = convert_tensor_2_numpy(x)\n\t        label_map = np.ones((len(x), 3)) * 0.5\n", "    pcd = o3d.geometry.PointCloud()\n\t    pcd.points = o3d.utility.Vector3dVector(x)\n\t    pcd.colors = o3d.utility.Vector3dVector(label_map)\n\t    o3d.io.write_point_cloud(filename, pcd)\n\tdef diameter(x, axis=-2, keepdims=True):\n\t    return torch.max(x, dim=axis, keepdims=keepdims).values - torch.min(x, dim=axis, keepdims=keepdims).values\n\tdef kdtree_indexing(x, depth=None, return_idx = False):\n\t    num_points = x.shape[1]\n\t    #assert isPowerOfTwo(num_points)\n\t    if depth is None:\n", "        depth = int(np.log(num_points) / np.log(2.) + 0.1)\n\t    y = x\n\t    batch_idx = torch.arange(x.shape[0]).to(torch.int32).to(x.device)\n\t    batch_idx = torch.reshape(batch_idx, (-1, 1))\n\t    batch_idx = batch_idx.repeat(1, x.shape[1])\n\t    points_idx = torch.arange(num_points).type_as(x).to(torch.int64)\n\t    points_idx = torch.reshape(points_idx, (1, -1, 1))\n\t    points_idx = points_idx.repeat(x.shape[0], 1, 1)\n\t    for i in range(depth):\n\t        y_shape = list(y.shape)\n", "        diam = diameter(y)\n\t        split_idx = torch.argmax(diam, dim=-1).to(torch.long).to(x.device)\n\t        split_idx = split_idx.repeat(1, y.shape[1])\n\t        idx = torch.arange(y.shape[0]).to(torch.long).to(x.device)\n\t        idx = idx.unsqueeze(-1)\n\t        idx = idx.repeat(1, y.shape[1])\n\t        branch_idx = torch.arange(y.shape[1]).to(torch.long).to(x.device)\n\t        branch_idx = branch_idx.unsqueeze(0)\n\t        branch_idx = branch_idx.repeat(y.shape[0], 1)\n\t        split_idx = torch.stack([idx, branch_idx, split_idx], dim=-1)\n", "        # print(split_idx, split_idx.shape)\n\t        # Gather implementation required\n\t        # m = tf.gather_nd(y, split_idx)\n\t        # print(y.shape)\n\t        m = gather_idx(y, split_idx)\n\t        # print(m.shape)\n\t        sort_idx = torch.argsort(m, dim=-1)\n\t        sort_idx = torch.stack([idx, sort_idx], dim=-1)\n\t        # Gather required\n\t        points_idx = gather_idx(points_idx, sort_idx)\n", "        points_idx = torch.reshape(points_idx, (-1, int(y.shape[1] // 2), 1))\n\t        # Gather\n\t        y = gather_idx(y, sort_idx)\n\t        y = torch.reshape(y, (-1, int(y.shape[1] // 2), 3))\n\t    y = torch.reshape(y, x.shape)\n\t    if not return_idx:\n\t        return y\n\t    points_idx = torch.reshape(points_idx, (x.shape[0], x.shape[1]))\n\t    points_idx_inv = torch.argsort(points_idx, dim=-1)\n\t    points_idx = torch.stack([batch_idx, points_idx], dim=-1)\n", "    points_idx_inv = torch.stack([batch_idx, points_idx_inv], dim=-1)\n\t    return y, points_idx, points_idx_inv\n\tdef kdtree_indexing_sdf(x, sdf, depth=None, return_idx = False):\n\t    num_points = x.shape[1]\n\t    #assert isPowerOfTwo(num_points)\n\t    if depth is None:\n\t        depth = int(np.log(num_points) / np.log(2.) + 0.1)\n\t    y = x\n\t    batch_idx = torch.arange(x.shape[0]).to(torch.int32).to(x.device)\n\t    batch_idx = torch.reshape(batch_idx, (-1, 1))\n", "    batch_idx = batch_idx.repeat(1, x.shape[1])\n\t    points_idx = torch.arange(num_points).type_as(x).to(torch.int64)\n\t    points_idx = torch.reshape(points_idx, (1, -1, 1))\n\t    points_idx = points_idx.repeat(x.shape[0], 1, 1)\n\t    for i in range(depth):\n\t        y_shape = list(y.shape)\n\t        diam = diameter(y)\n\t        split_idx = torch.argmax(diam, dim=-1).to(torch.long).to(x.device)\n\t        split_idx = split_idx.repeat(1, y.shape[1])\n\t        idx = torch.arange(y.shape[0]).to(torch.long).to(x.device)\n", "        idx = idx.unsqueeze(-1)\n\t        idx = idx.repeat(1, y.shape[1])\n\t        branch_idx = torch.arange(y.shape[1]).to(torch.long).to(x.device)\n\t        branch_idx = branch_idx.unsqueeze(0)\n\t        branch_idx = branch_idx.repeat(y.shape[0], 1)\n\t        split_idx = torch.stack([idx, branch_idx, split_idx], dim=-1)\n\t        # print(split_idx, split_idx.shape)\n\t        # Gather implementation required\n\t        # m = tf.gather_nd(y, split_idx)\n\t        # print(y.shape)\n", "        m = gather_idx(y, split_idx)\n\t        # print(m.shape)\n\t        sort_idx = torch.argsort(m, dim=-1)\n\t        sort_idx = torch.stack([idx, sort_idx], dim=-1)\n\t        # Gather required\n\t        points_idx = gather_idx(points_idx, sort_idx)\n\t        points_idx = torch.reshape(points_idx, (-1, int(y.shape[1] // 2), 1))\n\t        # Gather\n\t        y = gather_idx(y, sort_idx)\n\t        y = torch.reshape(y, (-1, int(y.shape[1] // 2), 3))\n", "    y = torch.reshape(y, x.shape)\n\t    if not return_idx:\n\t        return y\n\t    points_idx = torch.reshape(points_idx, (x.shape[0], x.shape[1]))\n\t    points_idx_inv = torch.argsort(points_idx, dim=-1)\n\t    points_idx = torch.stack([batch_idx, points_idx], dim=-1)\n\t    points_idx_inv = torch.stack([batch_idx, points_idx_inv], dim=-1)\n\t    return y, points_idx, points_idx_inv\n\tdef get_xyz_grid(grid, rotation = None):\n\t    \"\"\"\n", "    Returns an xyz grid in the normalized coordinates to perform density convolution\n\t    grid - B, C, H, W, D\n\t    out - B, C, H, W, 3\n\t    \"\"\"\n\t    if len(grid.shape) == 4:\n\t        out = grid.unsqueeze(1)\n\t    else:\n\t        out = grid\n\t    if rotation is not None:\n\t        theta = rotation\n", "    else: \n\t        theta = torch.eye(3).unsqueeze(0).repeat(out.shape[0], 1, 1).type_as(grid)\n\t    t = torch.tensor([0, 0, 0]).unsqueeze(0).unsqueeze(2).repeat(theta.shape[0], 1, 1).type_as(grid)\n\t    theta = torch.cat([theta, t], dim = -1)\n\t    out_grid = torch.nn.functional.affine_grid(theta, out.shape, align_corners = True)\n\t    return out_grid\n\tdef get_equivariant_density(density_field, normalize = True, scale_grid = True):\n\t    \"\"\"\n\t    Get equivariant density grid\n\t    density_field - B, H, W, D\n", "    \"\"\"\n\t    out = density_field\n\t    if len(density_field.shape) == 4:\n\t        out = density_field.unsqueeze(1)\n\t    B, _1, H, W, D = out.shape\n\t    grid = get_xyz_grid(density_field) # B, H, W, D, 3\n\t    grid_pts = grid.reshape(B, H*W*D, 3)\n\t    out_density = out.reshape(B, H*W*D, 1)\n\t    if normalize:\n\t        density_max = torch.max(out_density, axis = 1, keepdims = True)\n", "        out_density = out_density / (density_max.values + 1e-8)\n\t    # eq_grid_pts = zernike_monoms(grid_pts, 3)[1][..., 1:] # B, N, 3, 1\n\t    # eq_grid_pts = out_density * eq_grid_pts[..., 0] # B, N, 3\n\t    eq_grid_pts = grid_pts.unsqueeze(-1) #zernike_monoms(grid_pts, 3)[1][..., 1:] # B, N, 3, 1\n\t    if scale_grid:\n\t        eq_grid_pts = out_density * eq_grid_pts[..., 0] # B, N, 3\n\t    else:\n\t        eq_grid_pts = eq_grid_pts[..., 0]\n\t    eq_grid_pts = eq_grid_pts.reshape(B, H, W, D, 3)\n\t    return eq_grid_pts\n", "class GroupPoints_grid(torch.nn.Module):\n\t    def __init__(self, radius, patch_size_source, radius_target=None, patch_size_target=None,\n\t                 spacing_source=0, spacing_target=0):\n\t        super(GroupPoints_grid, self).__init__()\n\t        \"\"\"\n\t        Group points and different scales for pooling\n\t        \"\"\"\n\t        self.radius = radius\n\t        self.radius_target = radius_target\n\t        self.patch_size_source = patch_size_source\n", "        self.patch_size_target = patch_size_target\n\t        self.spacing_source = spacing_source\n\t        self.spacing_target = spacing_target\n\t        self.sphere_sampling = torch_fibonnacci_sphere_sampling(patch_size_source) # 14, 3\n\t        # self.sphere_sampling = torch.cat([self.sphere_sampling, self.sphere_sampling*2, self.sphere_sampling*3], 0)\n\t    def forward(self, x):\n\t        \"\"\"\n\t        source, target - B, H, W, D, 3\n\t        :param x: [source, target]\n\t        :return: [patches_idx_source, num_incident_points_target]\n", "        Returns:\n\t            source patches - B, N, K, 3\n\t            patches idx source - B, N, K, 2\n\t            patches size source - B, N\n\t            patches radius source - B, 1, 1\n\t            patches dist source - B, N, K\n\t        \"\"\"\n\t        assert isinstance(x, dict)\n\t        source = x[\"source points\"]\n\t        target = x[\"target points\"]\n", "        B, N_s, _ = source.shape\n\t        B, N_t, _ = target.shape\n\t        H_s = int(np.cbrt(N_s))\n\t        H_t = int(np.cbrt(N_t))\n\t        source = source.reshape(B, H_s, H_s, H_s, 3)\n\t        target = target.reshape(B, H_t, H_t, H_t, 3)\n\t        self.sphere_sampling = self.sphere_sampling.type_as(source)\n\t        source_mask = None\n\t        if \"source mask\" in x:\n\t            source_mask = x[\"source mask\"]\n", "        target_mask = None\n\t        if \"target mask\" in x:\n\t            target_mask = x[\"target mask\"]\n\t        num_points_source = source.shape[1]\n\t        # assert (num_points_source >= self.patch_size_source)\n\t        if self.patch_size_target is not None:\n\t            num_points_target = target.shape[1]\n\t            # assert (num_points_target >= self.patch_size_source)\n\t        # Compute spheres of radius around each point\n\t        grid_target = get_xyz_grid(target[..., 0].unsqueeze(1))\n", "        # grid_target = get_xyz_grid(target) # B, H, W, D, 3\n\t        grid_target = grid_target.unsqueeze(-2).type_as(source) # B, H, W, D, 1, 3\n\t        B, H, W, D, _, _ = grid_target.shape\n\t        sphere_samples = self.sphere_sampling.reshape(1, 1, 1, 1, -1, 3).type_as(source)\n\t        sampling_grid = grid_target + sphere_samples / (grid_target.shape[1] + 1e-8) * self.radius # B, H, W, D, patch_size, 3\n\t        sampling_grid = sampling_grid.permute((0, -1, 1, 2, 3, 4))\n\t        sampling_grid = sampling_grid.reshape(B, 3, H, W, -1)\n\t        sampling_grid = sampling_grid.permute((0, 2, 3, 4, 1))\n\t        # sampling_grid = sampling_grid.reshape(B, H, W, -1, 3)\n\t        patches = torch.nn.functional.grid_sample(target.transpose(1, -1), sampling_grid, align_corners = True)\n", "        patches = patches.reshape(B, -1, H, W, D, self.patch_size_source)\n\t        patches = patches.permute(0, 2, 3, 4, 5, 1)\n\t        # patches = patches.reshape(B, H, W, D, self.patch_size_source, 3)\n\t        sampling_grid = sampling_grid.permute((0, 4, 1, 2, 3))\n\t        sampling_grid = sampling_grid.reshape(B, 3, H, W, D, -1)\n\t        sampling_grid = sampling_grid.permute((0, 2, 3, 4, 5, 1))\n\t        y = {}\n\t        y[\"patches source\"] = patches\n\t        y[\"patches idx source\"] = sampling_grid\n\t        patches_dist = torch.sum(torch.square(target.unsqueeze(-2) - patches), axis = -1) # B, H, W, D, patch, 3\n", "        patches_dist = torch.sqrt(torch.maximum(patches_dist, torch.tensor(0.000000001).type_as(patches_dist)))\n\t        y[\"patches dist source\"] = patches_dist\n\t        return y\n\tdef get_gradient_density(x):\n\t    # x - B, H, W, D\n\t    B, H, W, D = x.shape\n\t    data = x.clone()\n\t    data = data.cpu().detach().numpy()\n\t    x_grad = torch.from_numpy(np.gradient(data,axis=3,edge_order=2)).to(x.device)\n\t    y_grad = torch.from_numpy(np.gradient(data,axis=2,edge_order=2)).to(x.device)\n", "    z_grad = torch.from_numpy(np.gradient(data,axis=1,edge_order=2)).to(x.device)\n\t    gradient = torch.stack([y_grad, z_grad, x_grad], 1)\n\t    return gradient # B, 3, H, W, D\n\tclass GroupPoints_density(torch.nn.Module):\n\t    def __init__(self, radius, patch_size_source, radius_target=None, patch_size_target=None,\n\t                 spacing_source=0, spacing_target=0):\n\t        super(GroupPoints_density, self).__init__()\n\t        \"\"\"\n\t        Group points and different scales for pooling\n\t        \"\"\"\n", "        #import pdb\n\t        #pdb.set_trace()\n\t        self.radius = radius\n\t        self.radius_target = radius_target\n\t        self.patch_size_source = patch_size_source \n\t        self.patch_size_target = patch_size_target #* 3\n\t        self.spacing_source = spacing_source\n\t        self.spacing_target = spacing_target\n\t        self.sphere_sampling = torch_fibonnacci_sphere_sampling(patch_size_source) # 14, 3\n\t        # print(self.sphere_sampling.shape)\n", "    def forward(self, x):\n\t        \"\"\"\n\t        source, target - B, H, W, D, 3\n\t        :param x: [source, target]\n\t        :return: [patches_idx_source, num_incident_points_target]\n\t        Returns:\n\t            source patches - B, N, K, 3\n\t            patches idx source - B, N, K, 2\n\t            patches size source - B, N\n\t            patches radius source - B, 1, 1\n", "            patches dist source - B, N, K\n\t        \"\"\"\n\t        assert isinstance(x, dict)\n\t        source = x[\"source points\"]\n\t        target = x[\"target points\"]\n\t        source_density = x[\"source density\"]\n\t        target_density = x[\"target density\"]\n\t        B, N_s, _ = source.shape\n\t        B, N_t, _ = target.shape\n\t        H_s = int(np.cbrt(N_s))\n", "        H_t = int(np.cbrt(N_t))\n\t        source = source.reshape(B, H_s, H_s, H_s, 3)\n\t        target = target.reshape(B, H_t, H_t, H_t, 3)\n\t        source_density = source_density.reshape(B, H_s, H_s, H_s, 1)\n\t        target_density = target_density.reshape(B, H_t, H_t, H_t, 1)\n\t        self.sphere_sampling = self.sphere_sampling.type_as(source)\n\t        rad = self.radius * torch.ones((B, 1, 1, 1, 1)).type_as(source)\n\t        # Compute spheres of radius around each point\n\t        grid_target = get_xyz_grid(target[..., 0].unsqueeze(1))\n\t        # grid_target = get_xyz_grid(target) # B, H, W, D, 3\n", "        grid_target = grid_target.unsqueeze(-2).type_as(source) # B, H, W, D, 1, 3\n\t        B, H, W, D, _, _ = grid_target.shape\n\t        sphere_samples = self.sphere_sampling.reshape(1, 1, 1, 1, -1, 3).type_as(source)# / (rad.squeeze() + 1e-8)\n\t        sampling_grid = grid_target + sphere_samples / (grid_target.shape[1] + 1e-8) / (rad.unsqueeze(-2) + 1e-6) # B, H, W, D, patch_size, 3\n\t        sampling_grid = sampling_grid.permute((0, -1, 1, 2, 3, 4))\n\t        sampling_grid = sampling_grid.reshape(B, 3, H, W, -1)\n\t        sampling_grid = sampling_grid.permute((0, 2, 3, 4, 1))\n\t        # sampling_grid = sampling_grid.reshape(B, H, W, -1, 3)\n\t        source = source / (rad + 1e-6)\n\t        target = target / (rad + 1e-6)\n", "        patches = torch.nn.functional.grid_sample(source.transpose(1, -1), sampling_grid, align_corners = True)\n\t        patches_density = torch.nn.functional.grid_sample(source_density.transpose(1, -1), sampling_grid, align_corners = True)\n\t        # print(patches_density.shape)\n\t        patches_density = patches_density.reshape(B, 1, H, W, D, self.patch_size_source)\n\t        patches_density = patches_density.permute(0, 2, 3, 4, 5, 1) # B, H, W, D, K, 1\n\t        patches = patches.reshape(B, -1, H, W, D, self.patch_size_source) \n\t        patches = patches.permute(0, 2, 3, 4, 5, 1) # B, H, W, D, K, 3\n\t        _,_,_,_,d,c = patches.shape\n\t        patches_flat = patches.reshape(1,-1,d,c)\n\t        patches = patches - target.unsqueeze(-2) # B, H, W, D, K, 3\n", "        # patches_density = torch.abs(patches_density - target_density.unsqueeze(-2)) # B, H, W, D, K, 1\n\t        patches_density = torch.sqrt( 1e-8 + (patches_density * target_density.unsqueeze(-2))) # B, H, W, D, K, 1\n\t        # patches_density = patches_density # B, H, W, D, K, 1\n\t        # patches = patches.reshape(B, H, W, D, self.patch_size_source, 3)\n\t        sampling_grid = sampling_grid.permute((0, 4, 1, 2, 3))\n\t        sampling_grid = sampling_grid.reshape(B, 3, H, W, D, -1)\n\t        sampling_grid = sampling_grid.permute((0, 2, 3, 4, 5, 1))\n\t        patches_dist = torch.sum(torch.square(patches), axis = -1) # B, H, W, D, patch, 3\n\t        patches_dist = torch.sqrt(torch.maximum(patches_dist, torch.tensor(0.000000001).type_as(patches_dist)))\n\t        mask = torch.lt(rad.type_as(patches_dist) ** 2, (patches_dist * rad)**2)\n", "        # print(mask.shape)\n\t        # sampling_grid[mask, :] = -10\n\t        # Reject where mask is 1\n\t        patches[mask, :] = 0\n\t        patches_density[mask, :] = 0\n\t        y = {}\n\t        y[\"patches source\"] = patches\n\t        y[\"patches source density\"] = patches_density # B, H, W, D, K, 1\n\t        y[\"patches idx source\"] = sampling_grid\n\t        # patches_dist = patches_dist #/ (rad + 1e-6)\n", "        y[\"patches dist source\"] = patches_dist\n\t        y[\"patches mask\"] = mask\n\t        y[\"patches size\"] = torch.sum( 1 - 1*mask, -1)\n\t        return y\n\tdef rotate_density(rotation, density_field, affine = True):\n\t    \"\"\"\n\t    rotation - B, 3, 3\n\t    density_field - B, H, W, D or B, C, H, W, D\n\t    \"\"\"\n\t    if len(density_field.shape) == 4:\n", "        out = density_field.unsqueeze(1)\n\t    else:\n\t        out = density_field\n\t    rotation = rotation.type_as(density_field)\n\t    t = torch.tensor([0, 0, 0]).unsqueeze(0).unsqueeze(2).repeat(rotation.shape[0], 1, 1).type_as(density_field)\n\t    theta = torch.cat([rotation, t], dim = -1)\n\t    if affine == True:\n\t        rot_grid = torch.nn.functional.affine_grid(theta, out.shape, align_corners = True)\n\t    else:\n\t        x = torch.linspace(-1, 1, density_field.shape[2]).type_as(density_field)\n", "        grid = torch.stack(torch.meshgrid(x, x, x), axis = -1).unsqueeze(0).repeat(out.shape[0], 1, 1, 1, 1) \n\t        # print(grid.shape, rotation.shape)\n\t        rot_grid = torch.einsum(\"bij, bhwdj-> bhwdi\", rotation, grid)\n\t    #print(rot_grid)\n\t    rotated_grid = torch.nn.functional.grid_sample(out, rot_grid, align_corners = True, mode=\"nearest\")#, padding_mode = \"border\")\n\t    if len(density_field.shape) == 4:\n\t        rotated_grid = rotated_grid.squeeze(1)\n\t    return rotated_grid\n\tdef patches_radius(radius, sq_norm):\n\t    batch_size = sq_norm.shape[0]\n", "    rad = radius\n\t    if isinstance(radius, float):\n\t        rad = radius * torch.ones((batch_size, 1, 1))\n\t    if isinstance(radius, str):\n\t        rad = torch.sqrt(torch.maximum(torch.max(sq_norm, dim=2, keepdims=False), torch.tensor(0.0000001).type_as(sq_norm)))\n\t        if radius == \"avg\":\n\t            rad = torch.mean(rad, dim=-1, keepdims=False)\n\t        elif radius == 'min':\n\t            rad = torch.min(rad, dim=-1, keepdims=False)\n\t        elif radius.isnumeric():\n", "            rad = torch.sort(rad, dim=-1)\n\t            i = int((float(int(radius)) / 100.) * sq_norm.shape[1])\n\t            i = max(i, 1)\n\t            rad = torch.mean(rad[:, :i], dim=-1, keepdims=False)\n\t        rad = torch.reshape(rad, (batch_size, 1, 1))\n\t    return rad\n\tdef gather_idx(x, idx):\n\t    \"\"\"\n\t    x - B, N, 3\n\t    idx - B, N, K, 2/3\n", "    out - B, N, K, 3\n\t    \"\"\"\n\t    num_idx = idx.shape[-1]\n\t    if idx.shape[-1] == 3:\n\t        if len(x.shape) == 3:\n\t            out = x[idx[..., 0], idx[..., 1], idx[..., 2]]\n\t            out[(idx[..., 2] < 0) * (idx[..., 1] < 0)] = 0\n\t            return out\n\t    if len(x.shape) == 2:\n\t        out = x[idx[..., 0], idx[..., 1]]\n", "        out[idx[..., 1] < 0] = 0\n\t    else:\n\t        out = x[idx[..., 0], idx[..., 1], :]\n\t        out[idx[..., 1] < 0, :] = 0\n\t    # print(idx[..., 1].shape, out.shape)\n\t    return out\n\tdef gather_idx_density(x, idx):\n\t    \"\"\"\n\t    x - B, N, 3\n\t    idx - B, N, K, 2/3\n", "    out - B, N, K, 3\n\t    \"\"\"\n\t    num_idx = idx.shape[-1]\n\t    if idx.shape[-1] == 3:\n\t        if len(x.shape) == 3:\n\t            out = x[idx[..., 0], idx[..., 1], idx[..., 2]]\n\t            out[(idx[..., 2] < 0) * (idx[..., 1] < 0)] = 0\n\t            return out\n\t    if len(x.shape) == 2:\n\t        out = x[idx[..., 0], idx[..., 1]]\n", "        out[idx[..., 1] < 0] = 0\n\t    else:\n\t        out = x[idx[..., 0], idx[..., 1], :]\n\t        out[idx[..., 1] < 0, :] = 0\n\t    # print(idx[..., 1].shape, out.shape)\n\t    return out\n\tdef compute_patches(source, target, sq_distance_mat, sq_distance_mat_sel, num_samples, spacing, radius, source_mask=None,source_density=None):\n\t    batch_size = source.shape[0]\n\t    num_points_source = source.shape[1]\n\t    num_points_target = target.shape[1]\n", "    assert (num_samples * (spacing + 1) <= num_points_source)\n\t    sq_patches_dist, patches_idx = torch.topk(-sq_distance_mat, k=num_samples * (spacing + 1))\n\t    #B,N,_  = sq_distance_mat.shape\n\t    #batch_idx         =  torch.arange(0,B).repeat(patches_idx.shape[-1],1).T.unsqueeze(1).repeat(1,N,1).to(torch.int64)\n\t    #point_idx         = torch.arange(0,N).reshape(N,1).repeat(1,patches_idx.shape[-1]).unsqueeze(0).repeat(B,1,1).to(torch.int64)\n\t    #sq_patches_dist   = sq_distance_mat[batch_idx,point_idx,patches_idx]\n\t    sq_patches_dist = -sq_patches_dist\n\t    if spacing > 0:\n\t        sq_patches_dist = sq_patches_dist[:, :, 0::(spacing + 1), ...]\n\t        patches_idx = patches_idx[:, :, 0::(spacing + 1), ...]\n", "    rad = patches_radius(radius, sq_patches_dist).type_as(sq_distance_mat)\n\t    patches_size = patches_idx.shape[-1]\n\t    # mask = sq_patches_dist < radius ** 2\n\t    mask = torch.greater_equal(rad.type_as(sq_distance_mat) ** 2, sq_patches_dist)\n\t    patches_idx = (torch.where(mask, patches_idx, torch.tensor(-1).type_as(patches_idx))).to(torch.int64)\n\t    if source_mask is not None:\n\t        source_mask = source_mask < 1\n\t        source_mask = source_mask.unsqueeze(-1).repeat(1, 1, patches_idx.shape[-1])\n\t        patches_idx = torch.where(source_mask, patches_idx, torch.tensor(-1).type_as(patches_idx))\n\t    batch_idx = torch.arange(batch_size).type_as(patches_idx)\n", "    batch_idx = torch.reshape(batch_idx, (batch_size, 1, 1))\n\t    batch_idx = batch_idx.repeat(1, num_points_target, num_samples)\n\t    patches_idx = torch.stack([batch_idx, patches_idx], dim = -1).to(torch.long)\n\t    source = (source / (rad + 1e-6))\n\t    target = (target / (rad + 1e-6))\n\t    # patches = source[batch_idx.to(torch.long), patches_idx.to(torch.long)]\n\t    patches = gather_idx(source, patches_idx)\n\t    b,n,c,_ = patches.shape\n\t    #density = torch.ones(b,n,c,1)\n\t    density = gather_idx_density(source_density, patches_idx)\n", "    # patches = source[patches_idx[..., 0], patches_idx[..., 1], :]\n\t    # print(patches.shape, \"patch\")\n\t    patches = patches - target.unsqueeze(-2)\n\t    if source_mask is not None:\n\t        mask = source_mask\n\t    else:\n\t        mask = torch.ones((batch_size, num_points_source)).type_as(patches)\n\t    patch_size = gather_idx(mask, patches_idx.to(torch.long))\n\t    # patch_size = mask[patches_idx[..., 0], patches_idx[..., 1]]\n\t    patches_size = torch.sum(patch_size, dim=-1, keepdims=False)\n", "    patches_dist = torch.sqrt(torch.maximum(sq_patches_dist, torch.tensor(0.000000001).type_as(sq_patches_dist)))\n\t    patches_dist = patches_dist / (rad + 1e-6)\n\t    mask_cnt     =  torch.count_nonzero(patches_idx[..., 1] <0,dim=-1).unsqueeze(-1)\n\t    weight_density= torch.sum(density,dim=-2) / mask_cnt\n\t    return {\"patches\": patches, \"patches idx\": patches_idx, \"patches size\": patches_size, \"patches radius\": rad,\n\t            \"patches dist\": patches_dist, \"source_density\":density, \"weight_density\":weight_density}\n\tdef create_mask(density):\n\t    shape_ = density.shape\n\t    B,H,W,D = density.shape\n\t    mask_density_list = []\n", "    for i in range(shape_[0]):\n\t        density_int = density[i].cpu().numpy().reshape(-1,1)\n\t        mask_density_int = np.ones_like(density_int) * -1\n\t        model = KMeans(init=\"k-means++\",n_clusters=2)\n\t        model.fit(density_int)\n\t        label = model.predict(density_int)\n\t        clusters = np.unique(label)\n\t        if np.mean(density_int[np.where(label == 1)[0]]) > np.mean(density_int[np.where(label == 0)[0]]):\n\t            fg_idx = np.where(label == 1)[0]\n\t            bg_idx = np.where(label == 0)[0]\n", "        else:\n\t            fg_idx = np.where(label == 0)[0]\n\t            bg_idx = np.where(label == 1)[0]\n\t        mask_density_int[fg_idx] = 1.\n\t        mask_density_int[bg_idx] = 0.\n\t        mask_density_tensor = torch.from_numpy(mask_density_int)\n\t        mask_density_list.append(mask_density_tensor)\n\t    mask_density = torch.stack(mask_density_list).reshape(shape_)\n\t    return mask_density \n\tclass GroupPoints_euclidean_density(torch.nn.Module):\n", "    def __init__(self, radius, patch_size_source, radius_target=None, patch_size_target=None,\n\t                 spacing_source=0, spacing_target=0):\n\t        super(GroupPoints_euclidean_density, self).__init__()\n\t        \"\"\"\n\t        Group points and different scales for pooling\n\t        \"\"\"\n\t        self.radius = radius\n\t        self.radius_target = radius_target\n\t        self.patch_size_source = patch_size_source \n\t        self.patch_size_target = patch_size_target #* 3\n", "        self.spacing_source = spacing_source\n\t        self.spacing_target = spacing_target\n\t        self.sphere_sampling = torch_fibonnacci_sphere_sampling(patch_size_source) # 14, 3\n\t        #self.sphere_sampling = torch.cat([self.sphere_sampling, self.sphere_sampling*2, self.sphere_sampling*3], 0)\n\t    def forward(self, x):\n\t        \"\"\"\n\t        source, target - B, H, W, D, 3\n\t        :param x: [source, target]\n\t        :return: [patches_idx_source, num_incident_points_target]\n\t        Returns:\n", "            source patches - B, N, K, 3\n\t            patches idx source - B, N, K, 2\n\t            patches size source - B, N\n\t            patches radius source - B, 1, 1\n\t            patches dist source - B, N, K\n\t        \"\"\"\n\t        assert isinstance(x, dict)\n\t        source = x[\"source points\"]\n\t        target = x[\"target points\"]\n\t        source_density = x[\"source density\"]\n", "        target_density = x[\"target density\"]\n\t        source_mask = None\n\t        if \"source mask\" in x:\n\t            source_mask = x[\"source mask\"]\n\t        #source =  (target.unsqueeze(-2) + self.sphere_sampling).reshape(1,-1,3).to(torch.float64)\n\t        #source = self.sphere_sampling.unsqueeze(0).to(torch.float64)\n\t        B, N_s, _ = source.shape\n\t        B, N_t, _ = target.shape\n\t        num_points_source = N_s\n\t        # compute distance mat\n", "        r0 = target * target\n\t        r0 = torch.sum(r0, dim=2, keepdims=True)\n\t        r1 = (source * source)\n\t        r1 = torch.sum(r1, dim=2, keepdims=True)\n\t        r1 = r1.permute(0, 2, 1)\n\t        sq_distance_mat = r0 - 2. * (target @ source.permute(0, 2, 1)) + r1\n\t        sq_distance_mat_sel = sq_distance_mat/((target_density @ source_density.permute(0,2,1)) + 1e-8)\n\t        # Returns\n\t        '''\n\t        mask_density = torch.ones_like(source_density)\n", "        mask_density[source_density == 0]= 1e10\n\t        mask_density = mask_density.squeeze(-1)\n\t        mask_denisty = mask_density.unsqueeze(-2)\n\t        sq_distance_mat = sq_distance_mat * mask_density.unsqueeze(-2)'''\n\t        #index = torch.where(source_density.squeeze(-1) == 0)\n\t        #sq_distance_mat[index[0],:,index[1]] = 1e10\n\t        patches = compute_patches(source, target, sq_distance_mat,sq_distance_mat_sel,\n\t                                   min(self.patch_size_source, num_points_source),\n\t                                   self.spacing_source, self.radius,\n\t                                   source_mask=source_mask,source_density=source_density)\n", "        H_t = int(np.cbrt(N_t))\n\t        y = {}\n\t        # concatinate points and density\n\t        B,N,K,_ = patches[\"patches\"].shape\n\t        y[\"patches source\"] = patches[\"patches\"].reshape(B,H_t,H_t,H_t,K,-1) # B, N, K, 3\n\t        #y[\"patches source density\"] = patches[\"source_density\"].reshape(B,H_t,H_t,H_t,K,-1)\n\t        y[\"patches idx source\"] = patches[\"patches idx\"].reshape(B,H_t,H_t,H_t,K,-1)\n\t        y[\"patches size source\"] = patches[\"patches size\"]\n\t        y[\"patches radius source\"] = patches[\"patches radius\"]\n\t        y[\"patches dist source\"] = patches[\"patches dist\"].reshape(B,H_t,H_t,H_t,K)\n", "        y[\"weighted density\"]    = patches[\"weight_density\"]\n\t        return y\n\tif __name__==\"__main__\":\n\t    # gi = GroupPoints_density(0.4, 32)\n\t    # in_dict = {}\n\t    # x = torch.randn(2, 32 *32 *32, 3).cuda()\n\t    # y = torch.randn(2, 16 *16 *16, 3).cuda()\n\t    # x_d = torch.randn(2, 32 *32 *32, 1).cuda()\n\t    # y_d = torch.randn(2, 16 *16 *16, 1).cuda()\n\t    # in_dict[\"source points\"] = x\n", "    # in_dict[\"target points\"] = y\n\t    # in_dict[\"source density\"] = x_d\n\t    # in_dict[\"target density\"] = y_d\n\t    # out = gi(in_dict)\n\t    # for key in out:\n\t    #     print(key, \" \", out[key].shape)\n\t    #x = torch.randn(2, 32, 32, 32).cuda()\n\t    # out = get_gradient_density(x)\n\t    #out = rotate_density(torch.eye(3).unsqueeze(0).repeat(2, 1, 1), x)\n\t    #out_2 = rotate_density(torch.eye(3).unsqueeze(0).repeat(2, 1, 1), x, affine = False)\n", "    # print(out - out_2)\n\t    # print(torch.norm(out, dim = 1).shape)\n\t    gi = GroupPoints_density(0.4, 32)\n\t    in_dict = {}\n\t    x = torch.randn(2, 32 *32 *32, 3)#.cuda()\n\t    y = torch.randn(2, 16 *16 *16, 3)#.cuda()\n\t    x_d = torch.randn(2, 32 *32 *32, 1)#.cuda()\n\t    y_d = torch.randn(2, 16 *16 *16, 1)#.cuda()\n\t    in_dict[\"source points\"] = x\n\t    in_dict[\"target points\"] = y\n", "    in_dict[\"source density\"] = x_d\n\t    in_dict[\"target density\"] = y_d\n\t    out = gi(in_dict)\n\t    for key in out:\n\t        print(key, \" \", out[key].shape)\n\t    # # x = (torch.ones((2, 1024, 3)) * torch.reshape(torch.arange(1024), (1, -1, 1))).cuda()\n\t    # # x = torch.randn((2, 1024, 3)).cuda()\n\t    # filename = \"/home/rahul/research/data/sapien_processed/train_refrigerator.h5\"\n\t    # f = h5py.File(filename, \"r\")\n\t    # x = torch.from_numpy(f[\"data\"][:2]).cuda()\n", "    # # x2 = torch.from_numpy(f[\"data\"][2:4]).cuda()\n\t    # y, kd, kd_2 = kdtree_indexing(x, return_idx = True)\n\t    # print(x, x.shape, y, y.shape)\n\t    # print(kd, kd.sha\n"]}
{"filename": "cafi_net/utils/group_points.py", "chunked_list": ["import torch\n\timport numpy as np\n\t# from utils.pointcloud_utils import get_xyz_grid\n\tdef patches_radius(radius, sq_norm):\n\t    batch_size = sq_norm.shape[0]\n\t    rad = radius\n\t    if isinstance(radius, float):\n\t        rad = radius * torch.ones((batch_size, 1, 1))\n\t    if isinstance(radius, str):\n\t        rad = torch.sqrt(torch.maximum(torch.max(sq_norm, dim=2, keepdims=False), torch.tensor(0.0000001).type_as(sq_norm)))\n", "        if radius == \"avg\":\n\t            rad = torch.mean(rad, dim=-1, keepdims=False)\n\t        elif radius == 'min':\n\t            rad = torch.min(rad, dim=-1, keepdims=False)\n\t        elif radius.isnumeric():\n\t            rad = torch.sort(rad, dim=-1)\n\t            i = int((float(int(radius)) / 100.) * sq_norm.shape[1])\n\t            i = max(i, 1)\n\t            rad = torch.mean(rad[:, :i], dim=-1, keepdims=False)\n\t        rad = torch.reshape(rad, (batch_size, 1, 1))\n", "    return rad\n\tdef gather_idx(x, idx):\n\t    \"\"\"\n\t    x - B, N, 3\n\t    idx - B, N, K, 2/3\n\t    out - B, N, K, 3\n\t    \"\"\"\n\t    num_idx = idx.shape[-1]\n\t    if idx.shape[-1] == 3:\n\t        if len(x.shape) == 3:\n", "            out = x[idx[..., 0], idx[..., 1], idx[..., 2]]\n\t            out[(idx[..., 2] < 0) * (idx[..., 1] < 0)] = 0\n\t            return out\n\t    if len(x.shape) == 2:\n\t        out = x[idx[..., 0], idx[..., 1]]\n\t        out[idx[..., 1] < 0] = 0\n\t    else:\n\t        out = x[idx[..., 0], idx[..., 1], :]\n\t        out[idx[..., 1] < 0, :] = 0\n\t    # print(idx[..., 1].shape, out.shape)\n", "    return out\n\tdef compute_patches_(source, target, sq_distance_mat, num_samples, spacing, radius, source_mask=None):\n\t    batch_size = source.shape[0]\n\t    num_points_source = source.shape[1]\n\t    num_points_target = target.shape[1]\n\t    assert (num_samples * (spacing + 1) <= num_points_source)\n\t    sq_patches_dist, patches_idx = torch.topk(-sq_distance_mat, k=num_samples * (spacing + 1))\n\t    sq_patches_dist = -sq_patches_dist\n\t    if spacing > 0:\n\t        sq_patches_dist = sq_patches_dist[:, :, 0::(spacing + 1), ...]\n", "        patches_idx = patches_idx[:, :, 0::(spacing + 1), ...]\n\t    rad = patches_radius(radius, sq_patches_dist).type_as(sq_distance_mat)\n\t    patches_size = patches_idx.shape[-1]\n\t    # mask = sq_patches_dist < radius ** 2\n\t    mask = torch.greater_equal(rad.type_as(sq_distance_mat) ** 2, sq_patches_dist)\n\t    patches_idx = (torch.where(mask, patches_idx, torch.tensor(-1).type_as(patches_idx))).to(torch.int64)\n\t    if source_mask is not None:\n\t        source_mask = source_mask < 1\n\t        source_mask = source_mask.unsqueeze(-1).repeat(1, 1, patches_idx.shape[-1])\n\t        patches_idx = torch.where(source_mask, patches_idx, torch.tensor(-1).type_as(patches_idx))\n", "    batch_idx = torch.arange(batch_size).type_as(patches_idx)\n\t    batch_idx = torch.reshape(batch_idx, (batch_size, 1, 1))\n\t    batch_idx = batch_idx.repeat(1, num_points_target, num_samples)\n\t    patches_idx = torch.stack([batch_idx, patches_idx], dim = -1).to(torch.long)\n\t    source = (source / (rad + 1e-6))\n\t    target = (target / (rad + 1e-6))\n\t    # patches = source[batch_idx.to(torch.long), patches_idx.to(torch.long)]\n\t    patches = gather_idx(source, patches_idx)\n\t    # patches = source[patches_idx[..., 0], patches_idx[..., 1], :]\n\t    # print(patches.shape, \"patch\")\n", "    patches = patches - target.unsqueeze(-2)\n\t    if source_mask is not None:\n\t        mask = source_mask\n\t    else:\n\t        mask = torch.ones((batch_size, num_points_source)).type_as(patches)\n\t    patch_size = gather_idx(mask, patches_idx.to(torch.long))\n\t    # patch_size = mask[patches_idx[..., 0], patches_idx[..., 1]]\n\t    patches_size = torch.sum(patch_size, dim=-1, keepdims=False)\n\t    patches_dist = torch.sqrt(torch.maximum(sq_patches_dist, torch.tensor(0.000000001).type_as(sq_patches_dist)))\n\t    patches_dist = patches_dist / (rad + 1e-6)\n", "    return {\"patches\": patches, \"patches idx\": patches_idx, \"patches size\": patches_size, \"patches radius\": rad,\n\t            \"patches dist\": patches_dist}\n\tclass GroupPoints(torch.nn.Module):\n\t    def __init__(self, radius, patch_size_source, radius_target=None, patch_size_target=None,\n\t                 spacing_source=0, spacing_target=0):\n\t        super(GroupPoints, self).__init__()\n\t        \"\"\"\n\t        Group points and different scales for pooling\n\t        \"\"\"\n\t        self.radius = radius\n", "        self.radius_target = radius_target\n\t        self.patch_size_source = patch_size_source\n\t        self.patch_size_target = patch_size_target\n\t        self.spacing_source = spacing_source\n\t        self.spacing_target = spacing_target\n\t    def forward(self, x):\n\t        \"\"\"\n\t        :param x: [source, target]\n\t        :return: [patches_idx_source, num_incident_points_target]\n\t        \"\"\"\n", "        assert isinstance(x, dict)\n\t        source = x[\"source points\"]\n\t        target = x[\"target points\"]\n\t        source_mask = None\n\t        if \"source mask\" in x:\n\t            source_mask = x[\"source mask\"]\n\t        target_mask = None\n\t        if \"target mask\" in x:\n\t            target_mask = x[\"target mask\"]\n\t        num_points_source = source.shape[1]\n", "        # assert (num_points_source >= self.patch_size_source)\n\t        if self.patch_size_target is not None:\n\t            num_points_target = target.shape[1]\n\t            # assert (num_points_target >= self.patch_size_source)\n\t        # compute distance mat\n\t        r0 = target * target\n\t        r0 = torch.sum(r0, dim=2, keepdims=True)\n\t        r1 = (source * source)\n\t        r1 = torch.sum(r1, dim=2, keepdims=True)\n\t        r1 = r1.permute(0, 2, 1)\n", "        sq_distance_mat = r0 - 2. * (target @ source.permute(0, 2, 1)) + r1\n\t        # Returns \n\t        patches = compute_patches_(source, target, sq_distance_mat,\n\t                                   min(self.patch_size_source, num_points_source),\n\t                                   self.spacing_source, self.radius,\n\t                                   source_mask=source_mask)\n\t        # print(patches[\"patches\"].shape)\n\t        y = dict()\n\t        y[\"patches source\"] = patches[\"patches\"] # B, N, K, 3\n\t        y[\"patches idx source\"] = patches[\"patches idx\"]\n", "        y[\"patches size source\"] = patches[\"patches size\"]\n\t        y[\"patches radius source\"] = patches[\"patches radius\"]\n\t        y[\"patches dist source\"] = patches[\"patches dist\"]\n\t        # y = [patches_source, patches_idx_source, patches_size_source]\n\t        if self.patch_size_target is not None:\n\t            sq_distance_mat_t = sq_distance_mat.permute(0, 2, 1)\n\t            patches = compute_patches_(target, source, sq_distance_mat_t,\n\t                                       min(self.patch_size_target, num_points_target),\n\t                                       self.spacing_target, self.radius_target,\n\t                                       source_mask=target_mask)\n", "            # y += [patches_target, patches_idx_target, patches_size_target]\n\t            y[\"patches target\"] = patches[\"patches\"]\n\t            y[\"patches idx target\"] = patches[\"patches idx\"]\n\t            y[\"patches size target\"] = patches[\"patches size\"]\n\t            y[\"patches radius target\"] = patches[\"patches radius\"]\n\t            y[\"patches dist target\"] = patches[\"patches dist\"]\n\t        # y.append(radius)\n\t        return y\n\tif __name__ == \"__main__\":\n\t    gi = GroupPoints(0.2, 14)\n", "    x = torch.randn(2, 32, 32, 32)\n\t    gi(x)\n\t    # N_pts = 10\n\t    # start = 10\n\t    # x = torch.ones((2, N_pts, 3)) * torch.arange(N_pts).unsqueeze(-1).unsqueeze(0)\n\t    # y = torch.ones((2, N_pts, 3)) * torch.arange(start, N_pts + start).unsqueeze(-1).unsqueeze(0)\n\t    # # print(x, y)\n\t    # gi = GroupPoints(0.2, 32)\n\t    # out = gi({\"source points\": x, \"target points\": y})\n\t    # for k in out:\n", "    #     print(out[k], out[k].shape, \" \", k)"]}
{"filename": "cafi_net/evaluators/metrics.py", "chunked_list": ["from canonicalization_metrics import *\n\timport torch\n\timport h5py\n\timport os, sys, argparse\n\timport numpy as np\n\tsys.path.append(\"../\")\n\tfrom utils.losses import chamfer_distance_l2_batch, l2_distance_batch\n\tif __name__==\"__main__\":\n\t    # Argument parser\n\t    parser = argparse.ArgumentParser(\n", "        description=\"Parser for generating frames\")\n\t    parser.add_argument(\"--path\", type = str, required = True)\n\t    parser.add_argument(\"--pc_path\", type = str)\n\t    parser.add_argument(\"--shape_idx\", type=str, default = None)\n\t    parser.add_argument(\"--rot_idx\", type=str, default = None)\n\t    parser.add_argument(\"--category\", type=str, default=None)\n\t    parser.add_argument(\"--n_iter\", default=20, type = int)\n\t    parser.add_argument(\"--device\", default = \"cpu\")\n\t    args = parser.parse_args()\n\t    ########################################################################\n", "    AtlasNetClasses = [\"car.h5\"]\n\t    if args.category is not None:\n\t        print(\"single category\")\n\t        AtlasNetClasses = [args.category+\".h5\"]\n\t    else:\n\t        print(\"multi category\")\n\t    ma = 0.\n\t    mb = 0.\n\t    mc = 0.\n\t    k = 0.\n", "    shpe_idx_array = np.asarray([[ 8, 16, 12,  4, 14,  5, 10, 18, 19, 17,  3,  6,  9,  7,  1, 13,  0,  2,15, 11], [10,  1, 15, 13,  2, 19, 18,  7, 11,  5, 12,  6,  4, 17,  3, 16,  8,  0,9, 14],[19, 12, 14,  9,  6,  0,  5, 13, 17, 18,  2, 10, 15,  8,  7, 16,  3, 11,1,  4],[ 3, 19, 18, 14, 10, 12, 11, 13,  8,  2,  0,  7,  4,  6,  1, 15, 16, 17,9,  5],[ 8,  0, 18, 17,  2,  6,  5, 13, 10, 11,  1,  7, 12, 15, 19, 16,  9,  3,14,  4],[ 7, 18,  0, 15, 12,  5, 19,  8,  9, 17, 13, 11,  1,  4, 14, 10,  6, 16,3,  2],[11, 15,  4,  5, 14, 18,  3, 17,  6, 13, 12,  2, 10,  7, 16,  0,  9, 19,1,  8],[14,  4, 13, 11, 17, 16,  7, 10,  3,  8,  2, 18,  0, 19,  6, 12,  5,  1,15,  9],[ 3,  4,  0, 12, 14,  9, 10,  1,  6,  7,  2,  8, 17, 16, 13, 15, 19, 18,5, 11],[17, 18,  7, 16,  3, 13, 15,  0,  1,  2, 14, 11,  8, 19,  6, 12,  9, 10,4,  5]])\n\t    args.pc_path = args.path\n\t    for i in range(len(AtlasNetClasses)):\n\t        print(AtlasNetClasses[i])\n\t        a = class_consistency_metric(AtlasNetClasses[i], args.path,args.pc_path,shapes_idx_path=args.shape_idx, batch_size=20, n_iter = args.n_iter, device = args.device)\n\t        print(\"Category-Level Consistency: \", a)\n\t        ma += a\n\t        b = equivariance_metric(AtlasNetClasses[i], args.path,args.pc_path,idx_path=args.rot_idx, batch_size=20, n_iter = args.n_iter, device = args.device)\n\t        print(\"Instance-Level Consistency: \", b)\n\t        mb += b\n", "        c = class_consistency_metric_new(AtlasNetClasses[i], args.path,args.pc_path,shpe_idx_array,shapes_idx_path=args.shape_idx, batch_size=20, n_iter = args.n_iter, device = args.device)\n\t        print(\"Ground Truth Equivariance Consistency: \", c)\n\t\tmc = mc + c\n\t        k += 1.\n"]}
{"filename": "cafi_net/evaluators/canonicalization_metrics.py", "chunked_list": ["import torch\n\timport h5py\n\timport os, sys\n\timport numpy as np\n\tsys.path.append(\"../\")\n\tfrom utils.losses import chamfer_distance_l2_batch, l2_distance_batch\n\timport open3d as o3d\n\tfrom pytorch3d.loss import chamfer_distance\n\timport open3d as o3d\n\tdistance_metric = chamfer_distance_l2_batch\n", "# distance_metric = l2_distance_batch\n\tdef orient(r):\n\t    \"\"\"\n\t    shape = list(r.shape)\n\t    shape = shape[:-2]\n\t    _, u, v = tf.linalg.svd(r)\n\t    R = tf.einsum('bij,bkj->bik', u, v)\n\t    s = tf.stack([tf.ones(shape), tf.ones(shape), tf.sign(tf.linalg.det(R))], axis=-1)\n\t    # u = tf.einsum('bj,bij->bij', s, u)\n\t    u = tf.multiply(tf.expand_dims(s, axis=-1), u)\n", "    # v = tf.multiply(tf.expand_dims(s, axis=1), v)\n\t    R = tf.einsum('bij,bkj->bik', u, v)\n\t    \"\"\"\n\t    return r\n\tdef save_h5_(h5_filename, data,data_dtype='float32'):\n\t    h5_fout = h5py.File(h5_filename,\"w\")\n\t    h5_fout.create_dataset(\n\t        'data', data=data,\n\t        compression='gzip', compression_opts=4,\n\t        dtype=data_dtype)\n", "    h5_fout.close()\n\tdef load_h5(h5_filename):\n\t    f = h5py.File(h5_filename)\n\t    print(h5_filename)\n\t    data = f['data'][:]\n\t    label = f['label'][:]\n\t    return (data, label)\n\tdef save_h5(h5_filename, data, normals=None, subsamplings_idx=None, part_label=None,\n\t            class_label=None, data_dtype='float32', label_dtype='uint8'):\n\t    h5_fout = h5py.File(h5_filename)\n", "    h5_fout.create_dataset(\n\t        'data', data=data,\n\t        compression='gzip', compression_opts=4,\n\t        dtype=data_dtype)\n\t    if normals is not None:\n\t        h5_fout.create_dataset(\n\t            'normal', data=normals,\n\t            compression='gzip', compression_opts=4,\n\t            dtype=data_dtype)\n\t    if subsamplings_idx is not None:\n", "        for i in range(len(subsamplings_idx)):\n\t            name = 'sub_idx_' + str(subsamplings_idx[i].shape[1])\n\t            h5_fout.create_dataset(\n\t                name, data=subsamplings_idx[i],\n\t                compression='gzip', compression_opts=1,\n\t                dtype='int32')\n\t    if part_label is not None:\n\t        h5_fout.create_dataset(\n\t            'pid', data=part_label,\n\t            compression='gzip', compression_opts=1,\n", "            dtype=label_dtype)\n\t    if class_label is not None:\n\t        h5_fout.create_dataset(\n\t            'label', data=class_label,\n\t            compression='gzip', compression_opts=1,\n\t            dtype=label_dtype)\n\t    h5_fout.close()\n\tdef torch_random_rotation(shape):\n\t    if isinstance(shape, int):\n\t        shape = [shape]\n", "    batch_size = shape[0]\n\t    t = torch.random(shape + [3])\n\t    c1 = torch.cos(2 * np.pi * t[:, 0])\n\t    s1 = torch.sin(2 * np.pi * t[:, 0])\n\t    c2 = torch.cos(2 * np.pi * t[:, 1])\n\t    s2 = torch.sin(2 * np.pi * t[:, 1])\n\t    z = torch.zeros(shape)\n\t    o = torch.ones(shape)\n\t    R = torch.stack([c1, s1, z, -s1, c1, z, z, z, o], dim=-1)\n\t    R = torch.reshape(R, shape + [3, 3])\n", "    v1 = torch.sqrt(t[:, -1])\n\t    v3 = torch.sqrt(1-t[:, -1])\n\t    v = torch.stack([c2 * v1, s2 * v1, v3], dim=-1)\n\t    H = torch.tile(torch.unsqueeze(torch.eye(3), 0), (batch_size, 1, 1)) - 2.* torch.einsum('bi,bj->bij', v, v)\n\t    M = -torch.einsum('bij,bjk->bik', H, R)\n\t    return M\n\tdef batch_of_frames(n_frames, filename, path):\n\t    I = torch.unsqueeze(torch.eye(3),0)\n\t    R = torch_random_rotation(n_frames - 1)\n\t    R = torch.cat([I, R], dim=0)\n", "    print(\"R shape\")\n\t    print(R.shape)\n\t    print(R)\n\t    h5_fout = h5py.File(os.path.join(path, filename), 'w')\n\t    h5_fout.create_dataset(\n\t        'data', data=R,\n\t        compression='gzip', compression_opts=4,\n\t        dtype='float32')\n\t    h5_fout.close()\n\t# batch_of_frames(n_frames=128, filename=\"rotations.h5\", path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024\")\n", "AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"table.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\tdef save_rotation(h5_filename, src_path, tar_path, rots_per_shape=512, batch_size=512):\n\t    filename = os.path.join(src_path, h5_filename)\n\t    f = h5py.File(filename)\n\t    print(filename)\n\t    data = f['data'][:]\n\t    num_shapes = data.shape[0]\n\t    num_batches = num_shapes // batch_size\n\t    residual = num_shapes % batch_size\n\t    R = []\n", "    \"\"\"\n\t    if num_batches == 0:\n\t        batch = tf_random_rotation(rots_per_shape * num_shapes)\n\t        batch = tf.reshape(batch, (-1, rots_per_shape, 3, 3))\n\t        R.append(np.asarray(batch, dtype=np.float))\n\t    \"\"\"\n\t    for i in range(num_batches):\n\t        a = i*batch_size\n\t        b = min((i+1)*batch_size, num_shapes)\n\t        if a < b:\n", "            batch = torch_random_rotation((b - a)*rots_per_shape)\n\t            batch = torch.reshape(batch, (-1, rots_per_shape, 3, 3))\n\t            batch = np.asarray(batch, dtype=np.float)\n\t            R.append(batch)\n\t    if residual > 0:\n\t        batch = torch_random_rotation(residual * rots_per_shape)\n\t        batch = torch.reshape(batch, (-1, rots_per_shape, 3, 3))\n\t        batch = np.asarray(batch, dtype=np.float)\n\t        R.append(batch)\n\t    # R = tf.concat(R, axis=0)\n", "    R = np.concatenate(R, axis=0)\n\t    print(data.shape)\n\t    print(R.shape)\n\t    # R = np.asarray(R, dtype=np.float)\n\t    h5_fout = h5py.File(os.path.join(tar_path, h5_filename), 'w')\n\t    h5_fout.create_dataset(\n\t        'data', data=R,\n\t        compression='gzip', compression_opts=4,\n\t        dtype='float32')\n\t    h5_fout.close()\n", "\"\"\"\n\tAtlasNetPath = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024\"\n\tfor name in AtlasNetClasses:\n\t    save_rotation(name, os.path.join(AtlasNetPath, 'valid'), os.path.join(AtlasNetPath, 'rotations_valid'))\n\t    save_rotation(name, os.path.join(AtlasNetPath, 'train'), os.path.join(AtlasNetPath, 'rotations_train'))\n\texit(666)\n\t\"\"\"\n\tdef mean(x, batch_size=512):\n\t    num_shapes = x.shape[0]\n\t    num_batches = num_shapes // batch_size\n", "    remainder = num_shapes // batch_size\n\t    m = []\n\t    k = 0.\n\t    for i in range(num_batches):\n\t        a = i * batch_size\n\t        b = min((i + 1) * batch_size, num_shapes)\n\t        if a < b:\n\t            k += float(b - a)\n\t            batch = x[a:b, ...]\n\t            m.append(torch.sum(batch, dim=0, keepdims=True))\n", "    if remainder > 0:\n\t        a = num_batches * batch_size\n\t        b = num_shapes\n\t        if a < b:\n\t            k += float(b - a)\n\t            batch = x[a:b, ...]\n\t            m.append(torch.sum(batch, dim=0, keepdims=True))\n\t    m = torch.cat(m, dim=0)\n\t    m = torch.sum(m, dim=0, keepdims=False)\n\t    m /= k\n", "    return m\n\tdef var(x, batch_size=512):\n\t    num_shapes = x.shape[0]\n\t    num_batches = num_shapes // batch_size\n\t    remainder = num_shapes // batch_size\n\t    v = []\n\t    k = 0.\n\t    m = torch.unsqueeze(mean(x, batch_size=512), dim=0)\n\t    for i in range(num_batches):\n\t        a = i * batch_size\n", "        b = min((i + 1) * batch_size, num_shapes)\n\t        if a < b:\n\t            k += float(b - a)\n\t            xi = x[a:b, ...]\n\t            vi = torch.sub(xi, m)\n\t            vi = vi * vi\n\t            vi = torch.sum(vi)\n\t            v.append(vi)\n\t    if remainder > 0:\n\t        a = num_batches * batch_size\n", "        b = num_shapes\n\t        if a < b:\n\t            k += float(b - a)\n\t            xi = x[a:b, ...]\n\t            vi = torch.sub(xi, m)\n\t            vi = vi * vi\n\t            vi = torch.sum(vi)\n\t            v.append(vi)\n\t    v = torch.stack(v, dim=0)\n\t    v = torch.sum(v)\n", "    v /= k\n\t    return v\n\tdef std(x, batch_size=512):\n\t    return torch.sqrt(var(x, batch_size))\n\tdef sq_dist_mat(x, y):\n\t    r0 = torch.mul(x, x)\n\t    r0 = torch.sum(r0, axis=2, keepdims=True)\n\t    r1 = torch.mul(y, y)\n\t    r1 = torch.sum(r1, axis=2, keepdims=True)\n\t    r1 = torch.permute(r1,(0, 2, 1))\n", "    sq_distance_mat = r0 - 2. * torch.matmul(x, y.permute(0, 2, 1)) + r1\n\t    return sq_distance_mat\n\tdef var_(x, axis_mean=0, axis_norm=1):\n\t    mean = torch.mean(x, dim=axis_mean, keepdims=True)\n\t    y = torch.sub(x, mean)\n\t    yn = torch.sum(y * y, dim=axis_norm, keepdims=False)\n\t    yn = torch.mean(yn, dim=axis_mean, keepdims=False)\n\t    return yn, mean\n\tdef std_(x, axis_mean=0, axis_norm=1):\n\t    yn, mean = var_(x, axis_mean=axis_mean, axis_norm=axis_norm)\n", "    return torch.sqrt(yn), mean\n\tdef pca_align(x):\n\t    c = torch.mean(x, dim=1, keepdims=True)\n\t    centred_x = torch.sub(x, c)\n\t    covar_mat = torch.mean(torch.einsum('bvi,bvj->bvij', centred_x, centred_x), dim=1, keepdims=False)\n\t    _, v = np.linalg.eigh(covar_mat.detach().numpy())\n\t    v = torch.from_numpy(v)\n\t    x = torch.einsum('bij,bvi->bvj', v, centred_x)\n\t    return x, v.permute(0,2,1)\n\tdef visualize_outputs(data, start = 0, max_num = 10, spacing = 2.0, skip = 1):\n", "    '''\n\t    Visualize point clouds in open3D \n\t    '''\n\t    num_pcds = 10\n\t    rows = np.floor(np.sqrt(num_pcds))\n\t    pcd_list = []\n\t    arrow_list = []\n\t    pcd_iter = 0\n\t    pcd_index = 0\n\t    for _ in range(num_pcds):\n", "        pts = data[_].cpu().detach().numpy()\n\t        pcd = o3d.geometry.PointCloud()\n\t        pcd.points = o3d.utility.Vector3dVector(pts)\n\t        pcd.colors = pcd.points\n\t        column_num = pcd_index // rows\n\t        row_num = pcd_index % rows\n\t        vector = (row_num * spacing, column_num * spacing, 0)\n\t        pcd.translate(vector)\n\t        pcd_list.append(pcd)\n\t        pcd_index += 1\n", "    o3d.visualization.draw_geometries(pcd_list)\n\tdef save_pca_frames_(filename, x_path, batch_size=20, num_rots=128):\n\t    obj_name = filename.split('.')[0]\n\t    x = loadh5(os.path.join(x_path, filename))\n\t    x_data = x.copy()\n\t    r = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n\t    x = torch.from_numpy(x)\n\t    r = torch.from_numpy(r)\n\t    num_shapes = x.shape[0]\n\t    x = extend_(x, batch_size)\n", "    save_directory = x_path + \"pca\"\n\t    if not os.path.exists(save_directory):\n\t        os.mkdir(save_directory)\n\t    # r = extend_(r, batch_size)\n\t    h5_filename = save_directory + \"/\"+ obj_name +\".h5\"\n\t    save_h5_(h5_filename,x_data)\n\t    h5_filename = save_directory + \"/\"+ obj_name +\"_rotations.h5\"\n\t    save_h5_(h5_filename, r.cpu().detach().numpy())\n\t    num_batches = x.shape[0] // batch_size\n\t    R = []\n", "    for i in range(num_batches):\n\t        print(100.*i/num_batches)\n\t        xi = x[i * batch_size:(i + 1) * batch_size, ...]\n\t        Ri = []\n\t        canonical_frame = []\n\t        data_ = []\n\t        input_data= []\n\t        for j in range(num_rots):\n\t            xij = torch.einsum(\"ij,bvj->bvi\", r[j, ...], xi)\n\t            # yij = pca_frame(xij)\n", "            data,frame = pca_align(xij)\n\t            Ri.append(data)\n\t            if j%10 == 0:\n\t                data_.append(data[0])\n\t                input_data.append(xij[0])\n\t            canonical_frame.append(frame)\n\t        #data_ = torch.stack(data_,axis=0)\n\t        #input_data = torch.stack(input_data,axis=0)\n\t        #visualize_outputs(data_)  \n\t        #visualize_outputs(input_data)          \n", "        Ri = torch.stack(Ri, axis=1)\n\t        R.append(np.asarray(Ri, dtype=np.float))\n\t    R = np.concatenate(R, axis=0)\n\t    R = R[:num_shapes, ...]\n\t    canonical_frame = torch.stack(canonical_frame,axis=0).permute(1,0,2,3)\n\t    h5_filename = save_directory + \"/\"+ obj_name +\"_canonical.h5\"\n\t    save_h5_(h5_filename, canonical_frame.cpu().detach().numpy())\n\t    '''\n\t    filename_ = obj_name + \"_pca.h5\"\n\t    h5_fout = h5py.File(os.path.join(x_path + \"pca\", filename), 'w')\n", "    h5_fout.create_dataset(\n\t        'data', data=R,\n\t        compression='gzip', compression_opts=4,\n\t        dtype='float32')\n\t    h5_fout.close()'''\n\tdef normalize(x):\n\t    s, m = std_(x, axis_mean=1, axis_norm=-1)\n\t    x = torch.div(torch.sub(x, m), s)\n\t    return x\n\tdef orth_procrustes(x, y):\n", "    x = normalize(x)\n\t    y = normalize(y)\n\t    xty = torch.einsum('bvi,bvj->bij', y, x)\n\t    s, u, v = torch.linalg.svd(xty)\n\t    r = torch.einsum('bij,bkj->bik', u, v)\n\t    return r\n\tdef extend_(x, batch_size):\n\t    last_batch = x.shape[0] % batch_size\n\t    if last_batch > 0:\n\t        X_append = []\n", "        for i in range(batch_size - last_batch):\n\t            X_append.append(x[i, ...])\n\t        X_append = torch.stack(X_append, dim=0)\n\t        y = torch.cat([x, X_append], dim=0)\n\t    else:\n\t        y = x\n\t    return y\n\tdef xyz2yzx(x):\n\t    return torch.stack([x[..., 1], x[..., 2], x[..., 0]], dim=-1)\n\tdef yzx2xyz(x):\n", "    return torch.stack([x[..., 2], x[..., 0], x[..., 1]], dim=-1)\n\tdef yzx2xyzConj(R):\n\t    R = yzx2xyz(torch.linalg.matrix_transpose(R))\n\t    R = torch.linalg.matrix_transpose(R)\n\t    # return xyz2yzx(R)\n\t    return R\n\tdef class_consistency_frames(r, r_can, batch_size=32):\n\t    num_batches = r.shape[0] // batch_size\n\t    num_shapes = r.shape[0]\n\t    num_rots = min(r.shape[1], r_can.shape[1])\n", "    r = r[:, :num_rots, ...]\n\t    r_can = r_can[:, :num_rots, ...]\n\t    r = extend_(r, batch_size)\n\t    r_can = extend_(r_can, batch_size)\n\t    R = []\n\t    for i in range(num_batches):\n\t        a = i * batch_size\n\t        b = (i + 1) * batch_size\n\t        rj = r[a:b, ...]\n\t        r_can_j = r_can[a:b, ...]\n", "        # Ri = tf.matmul(r_can_j, rj, transpose_a=True)\n\t        #  Ri = tf.matmul(r_can_j, rj, transpose_b=True)\n\t        # Ri = tf.matmul(r_can_j, r_can_j, transpose_b=True)\n\t        Ri = r_can_j\n\t        Ri = orient(Ri)\n\t        # Ri = tf.matmul(r_can_j, rj)\n\t        # Ri = np.stack(np.asarray(Ri, dtype=np.float32), axis=1)\n\t        R.append(np.asarray(Ri, dtype=np.float32))\n\t    R = np.concatenate(R, axis=0)\n\t    R = R[:num_shapes, ...]\n", "    # print(R)\n\t    return R\n\tdef visualize_method(filename, x_path,):\n\t    obj_name = filename.split('.')[0]\n\t    x = loadh5(os.path.join(x_path, filename))\n\t    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n\t    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n\t    x = torch.from_numpy(x)\n\t    r_can = torch.from_numpy(r_can)\n\t    r_input = torch.from_numpy(r_input)\n", "    pcds_ = []\n\t    pcds_1 = []\n\t    for i in range(x.shape[0]):\n\t        for j in range(r_input.shape[0]):\n\t            rj = r_input[j, ...]\n\t            xij = torch.einsum(\"ij,bvj->bvi\", rj, x)\n\t            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can[:,j,:]), xij)\n\t            if j%10 == 0:\n\t               pcds_.append(y0i[4]) \n\t               pcds_1.append(xij[4]) \n", "    input_data = torch.stack(pcds_,axis=0)\n\t    input_data_1 = torch.stack(pcds_1,axis=0)\n\t    visualize_outputs(input_data)  \n\t    visualize_outputs(input_data_1)  \n\tdef class_consistency_metric_(x, r_input, r_can, val_points,idx=None, batch_size=32):\n\t    num_rots = r_input.shape[0]\n\t    n_shapes = x.shape[0]\n\t    if idx is None:\n\t        idx = torch.randperm(n_shapes)\n\t    r_can_0 = r_can\n", "    r_can_1 = r_can[idx]\n\t    x = extend_(x, batch_size)\n\t    r_can_0 = extend_(r_can_0, batch_size)\n\t    r_can_1 = extend_(r_can_1, batch_size)\n\t    num_batches = x.shape[0] // batch_size\n\t    D = []\n\t    for j in range(num_rots):\n\t        rj = r_input[j, ...]\n\t        d = []\n\t        d_ = []\n", "        for i in range(num_batches):\n\t            r_can_0_ij = r_can_0[i * batch_size:(i + 1) * batch_size, j, ...]\n\t            r_can_1_ij = r_can_1[i * batch_size:(i + 1) * batch_size, j, ...]\n\t            xi = x[i * batch_size:(i + 1) * batch_size, ...]\n\t            #points_val = val_points[i * batch_size:(i + 1) * batch_size, ...]\n\t            points_val =[1024]*batch_size\n\t            xij = torch.einsum(\"ij,bvj->bvi\", rj, xi)\n\t            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_0_ij), xij)\n\t            y1i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_1_ij), xij)\n\t            for _ in range(y0i.shape[0]):\n", "                d_.append(float(chamfer_distance(y0i[_][:int(points_val[_])].unsqueeze(0),y1i[_][:int(points_val[_])].unsqueeze(0))[0]))\n\t        #d = np.concatenate(d, axis=0)\n\t        d = np.asarray(d_)\n\t        d = d[:n_shapes, ...]\n\t        D.append(np.mean(d))\n\t    D = np.stack(D, axis=0)\n\t    D = np.mean(D)\n\t    return float(D)\n\tdef class_consistency_metric_new_(x, r_input, r_can,val_points=None, idx=None, batch_size=32):\n\t    num_rots = r_input.shape[0]\n", "    n_shapes = x.shape[0]\n\t    if idx is None:\n\t        idx = torch.randperm(n_shapes)\n\t    rot_idx = torch.randperm(num_rots)\n\t    r_input_0 = r_input\n\t    r_input_1  = r_input[rot_idx]\n\t    r_can_0 = r_can\n\t    r_can_1 = r_can[idx]\n\t    r_can_1 = r_can_1[:,rot_idx,...]\n\t    x = extend_(x, batch_size)\n", "    r_can_0 = extend_(r_can_0, batch_size)\n\t    r_can_1 = extend_(r_can_1, batch_size)\n\t    num_batches = x.shape[0] // batch_size\n\t    D = []\n\t    for j in range(num_rots):\n\t        rj0 = r_input_0[j, ...]\n\t        rj1 = r_input_1[j, ...]\n\t        d = []\n\t        d_ = []\n\t        for i in range(num_batches):\n", "            r_can_0_ij = r_can_0[i * batch_size:(i + 1) * batch_size, j, ...]\n\t            r_can_1_ij = r_can_1[i * batch_size:(i + 1) * batch_size, j, ...]\n\t            xi = x[i * batch_size:(i + 1) * batch_size, ...]\n\t            #points_val = val_points[i * batch_size:(i + 1) * batch_size, ...]\n\t            points_val =[1024]*batch_size\n\t            xij0 = torch.einsum(\"ij,bvj->bvi\", rj0, xi)\n\t            xij1 = torch.einsum(\"ij,bvj->bvi\", rj1, xi)\n\t            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_0_ij), xij0)\n\t            y1i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_1_ij), xij1)\n\t            for _ in range(y0i.shape[0]):\n", "                d_.append(float(chamfer_distance(y0i[_][:int(points_val[_])].unsqueeze(0),y1i[_][:int(points_val[_])].unsqueeze(0))[0]))\n\t        #d = np.concatenate(d, axis=0)\n\t        d = np.asarray(d_)\n\t        d = d[:n_shapes, ...]\n\t        D.append(np.mean(d))\n\t    D = np.stack(D, axis=0)\n\t    D = np.mean(D)\n\t    return float(D)\n\tdef loadh5(path):\n\t    fx_input = h5py.File(path, 'r')\n", "    x = fx_input['data'][:]\n\t    fx_input.close()\n\t    return x\n\tdef load_file_names(path,name):\n\t    filename = glob.glob(os.path.join(path, \"\")  + name)\n\t    filename.sort()\n\t    file_list = []\n\t    for f in filename:\n\t        print(os.path.join(base_path, \"\") + f.split(\"/\")[-1].split(\"_\")[0] +\"_\" + name, f)\n\t        file_list.append(f)\n", "    return file_list\n\tdef class_consistency_metric_new(filename, x_path, pc_path,shape_idx_array, shapes_idx_path=None, batch_size=32, n_iter=10, device = \"cpu\"):\n\t    obj_name = filename.split('.')[0]\n\t    x = loadh5(os.path.join(pc_path, obj_name +\".h5\"))\n\t    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n\t    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n\t    #val_points = loadh5(os.path.join(pc_path, obj_name +\"_val_points.h5\"))\n\t    val_points = None\n\t    x = torch.from_numpy(x).to(device)\n\t    x = (x - torch.mean(x , axis=1,keepdim=True))\n", "    r_can = torch.from_numpy(r_can).to(device)\n\t    r_input = torch.from_numpy(r_input).to(device)\n\t    m = 0.\n\t    if shapes_idx_path is not None:\n\t        idx = loadh5(os.path.join(shapes_idx_path, filename))\n\t        idx = torch.from_numpy(idx).to(torch.int64).to(device)\n\t        n_iter = min(n_iter, idx.shape[0])\n\t        for i in range(n_iter):\n\t            m += class_consistency_metric_new_(x, r_input,\n\t                                           r_can,val_points,idx[i, ...], batch_size)\n", "    else:\n\t        idx = None\n\t        for i in range(n_iter):\n\t            m += class_consistency_metric_new_(x, r_input, r_can,val_points, idx, batch_size)\n\t    return m / n_iter\n\tdef class_consistency_metric(filename, x_path, pc_path,shapes_idx_path=None, batch_size=32, n_iter=10, device = \"cpu\"):\n\t    obj_name = filename.split('.')[0]\n\t    x = loadh5(os.path.join(pc_path, obj_name +\".h5\"))\n\t    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n\t    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n", "    #val_points = loadh5(os.path.join(pc_path, obj_name +\"_val_points.h5\"))\n\t    val_points = None\n\t    x = torch.from_numpy(x).to(device)\n\t    x = (x - torch.mean(x , axis=1,keepdim=True))\n\t    r_can = torch.from_numpy(r_can).to(device)\n\t    r_input = torch.from_numpy(r_input).to(device)\n\t    m = 0.\n\t    if shapes_idx_path is not None:\n\t        idx = loadh5(os.path.join(shapes_idx_path, filename))\n\t        idx = torch.from_numpy(idx).to(torch.int64).to(device)\n", "        n_iter = min(n_iter, idx.shape[0])\n\t        for i in range(n_iter):\n\t            m += class_consistency_metric_(x, r_input,\n\t                                           r_can,val_points,idx[i, ...], batch_size)\n\t    else:\n\t        idx = None\n\t        for i in range(n_iter):\n\t            m += class_consistency_metric_(x, r_input, r_can, val_points,idx, batch_size)\n\t    return m / n_iter\n\tdef equivariance_metric_(x, r_input, r_can, val_points=None, batch_size=20, idx=None):\n", "    num_shapes = x.shape[0]\n\t    num_rots = r_input.shape[0]\n\t    if idx is None:\n\t        idx = torch.randperm(num_rots)\n\t    r_can_0   = r_can\n\t    r_can_1   = r_can[:,idx]\n\t    r_input_0 = r_input\n\t    r_input_1 = r_input[idx]\n\t    x = extend_(x, batch_size)\n\t    r_can_0 = extend_(r_can_0, batch_size)\n", "    r_can_1 = extend_(r_can_1, batch_size)\n\t    # r_input_0 = extend_(r_input_0, batch_size)\n\t    # r_input_1 = extend_(r_input_1, batch_size)\n\t    num_batches = x.shape[0] // batch_size\n\t    D = []\n\t    for i in range(num_batches):\n\t        d = []\n\t        d_ = []\n\t        for j in range(num_rots):\n\t            r0j = r_input_0[j, ...]\n", "            r1j = r_input_1[j, ...]\n\t            r_can_0_ij = r_can_0[i * batch_size:(i + 1) * batch_size, j, ...]\n\t            r_can_1_ij = r_can_1[i * batch_size:(i + 1) * batch_size, j, ...]\n\t            xi = x[i * batch_size:(i + 1) * batch_size, ...]\n\t            x0ij = torch.einsum(\"ij,bvj->bvi\", r0j, xi)\n\t            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_0_ij), x0ij)\n\t            x1ij = torch.einsum(\"ij,bvj->bvi\", r1j, xi)\n\t            y1i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_1_ij), x1ij)\n\t            #points_val = val_points[i * batch_size:(i + 1) * batch_size, ...]\n\t            points_val = [1024]*batch_size\n", "            d_int = []\n\t            for _ in range(y0i.shape[0]):\n\t                d_int.append(float(chamfer_distance(y0i[_][:int(points_val[_])].unsqueeze(0),y1i[_][:int(points_val[_])].unsqueeze(0))[0]))\n\t            d_.append(d_int)\n\t        #d = np.stack(d, axis=1)\n\t        d = np.asarray(d_)\n\t        d = np.mean(d, axis=1, keepdims=False)\n\t        D.append(d)\n\t    D = np.concatenate(D, axis=0)\n\t    D = D[:num_shapes, ...]\n", "    D = np.mean(D)\n\t    return float(D)\n\tdef equivariance_metric(filename, x_path, pc_path,batch_size, idx_path=None, n_iter=10, device = \"cpu\"):\n\t    obj_name = filename.split('.')[0]\n\t    x = loadh5(os.path.join(pc_path, obj_name +\".h5\"))\n\t    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n\t    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n\t    #val_points = loadh5(os.path.join(pc_path, obj_name +\"_val_points.h5\"))\n\t    val_points = None\n\t    x = torch.from_numpy(x).to(device)\n", "    x = (x - torch.mean(x , axis=1,keepdim=True))\n\t    r_can = torch.from_numpy(r_can).to(device)\n\t    r_input = torch.from_numpy(r_input).to(device)\n\t    spacing = 1.0\n\t    '''\n\t    for _ in range(r_can.shape[1]):\n\t        #random_rot_pcd = torch.matmul(r_input,x.permute(1,0)).permute(0,2,1)\n\t        random_rot_pcd = torch.matmul(r_input[0],x.permute(0,2,1)).permute(0,2,1)\n\t        can_pcd        = torch.matmul(r_can[:,_],random_rot_pcd.permute(0,2,1)).permute(0,2,1)\n\t        can_pcd_numpy  = can_pcd.detach().numpy()\n", "        num_pcds = can_pcd_numpy.shape[0]\n\t        rows = np.floor(np.sqrt(num_pcds))\n\t        pcd_list = []\n\t        arrow_list = []\n\t        pcd_iter = 0\n\t        pcd_index = 0\n\t        for k in range(num_pcds):\n\t            pcd = o3d.geometry.PointCloud()\n\t            pcd.points = o3d.utility.Vector3dVector(can_pcd_numpy[k])\n\t            pcd.colors = pcd.points\n", "            column_num = pcd_index // rows\n\t            row_num = pcd_index % rows\n\t            vector = (row_num * spacing, column_num * spacing, 0)\n\t            pcd.translate(vector)\n\t            pcd_list.append(pcd)\n\t            pcd_index += 1\n\t        #o3d.visualization.draw_geometries(pcd_list)'''\n\t    m = 0.\n\t    if idx_path is None:\n\t        idx = None\n", "        for i in range(n_iter):\n\t            m += equivariance_metric_(x, r_input, r_can,val_points,batch_size, idx=idx)\n\t    else:\n\t        idx = loadh5(idx_path)\n\t        idx = torch.from_numpy(idx).to(torch.int64)     \n\t        n_iter = min(n_iter, idx.shape[0])\n\t        for i in range(n_iter):\n\t            m += equivariance_metric_(x, r_input, r_can, batch_size, idx=idx[i, ...])\n\t    return m / n_iter\n\tdef class_consistency_umetric_(x, r_input, r_can, val_points=None,idx_shapes=None, idx_rots=None, batch_size=32):\n", "    num_rots = min(r_input.shape[0], r_can.shape[0])\n\t    n_shapes = x.shape[0]\n\t    if idx_shapes is None:\n\t        idx_shapes = torch.randperm(n_shapes)\n\t    if idx_rots is None:\n\t        idx_rots = torch.randperm(num_rots)\n\t    else:\n\t        idx_rots = idx_rots[:num_rots, ...]\n\t    r_can_0 = r_can\n\t    r_can_1 = r_can[:,idx_rots]\n", "    r_can_1 = r_can_1[idx_shapes]\n\t    r_input_0 = r_input\n\t    r_input_1 = r_input[idx_rots]\n\t    x_0 = x\n\t    x_1 = x[idx_shapes]\n\t    #val_points_0 = val_points\n\t    #val_points_1 = val_points[idx_shapes]\n\t    val_points_0 = [1024]*batch_size\n\t    val_points_1 = [1024]*batch_size\n\t    x_0 = extend_(x_0, batch_size)\n", "    x_1 = extend_(x_1, batch_size)\n\t    r_can_0 = extend_(r_can_0, batch_size)\n\t    r_can_1 = extend_(r_can_1, batch_size)\n\t    num_batches = x.shape[0] // batch_size\n\t    D = []\n\t    d_ = []\n\t    for j in range(num_rots):\n\t        r0j = r_input_0[j, ...]\n\t        r1j = r_input_1[j, ...]\n\t        d = []\n", "        for i in range(num_batches):\n\t            r_can_0_ij = r_can_0[i * batch_size:(i + 1) * batch_size, j, ...]\n\t            r_can_1_ij = r_can_1[i * batch_size:(i + 1) * batch_size, j, ...]\n\t            x0i = x_0[i * batch_size:(i + 1) * batch_size, ...]\n\t            x1i = x_1[i * batch_size:(i + 1) * batch_size, ...]\n\t            x0ij = torch.einsum(\"ij,bvj->bvi\", r0j, x0i)\n\t            x1ij = torch.einsum(\"ij,bvj->bvi\", r1j, x1i)\n\t            y0i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_0_ij), x0ij)\n\t            y1i = torch.einsum(\"bij,bvj->bvi\", orient(r_can_1_ij), x1ij)\n\t            d_int = []\n", "            for _ in range(y0i.shape[0]):\n\t                d_int.append(float(chamfer_distance(y0i[_][:int(val_points_0[_])].unsqueeze(0),y1i[_][:int(val_points_1[_])].unsqueeze(0))[0]))\n\t            d_.append(d_int)\n\t        d = np.asarray(d_)\n\t        d = d[:n_shapes, ...]\n\t        D.append(np.mean(d))\n\t    D = np.stack(D, axis=0)\n\t    D = np.mean(D)\n\t    return float(D)\n\tdef class_consistency_umetric(filename, x_path,pc_path, idx_shapes_path=None, idx_rots_path=None, batch_size=32, n_iter=10, device = \"cpu\"):\n", "    obj_name = filename.split('.')[0]\n\t    x = loadh5(os.path.join(pc_path, obj_name +\".h5\"))\n\t    r_can = loadh5(os.path.join(x_path, obj_name +\"_canonical.h5\"))\n\t    r_input = loadh5(os.path.join(x_path, obj_name +\"_rotations.h5\"))\n\t    #val_points = loadh5(os.path.join(pc_path, obj_name +\"_val_points.h5\"))\n\t    val_points = None\n\t    x = torch.from_numpy(x).to(device)\n\t    r_can = torch.from_numpy(r_can).to(device)\n\t    r_input = torch.from_numpy(r_input).to(device)\n\t    x = (x - torch.mean(x , axis=1,keepdim=True))\n", "    if idx_shapes_path is not None:\n\t        idx_shapes = loadh5(os.path.join(idx_shapes_path, filename))\n\t        idx_shapes = torch.from_numpy(idx_shapes).to(torch.int64)\n\t        n_iter = min(n_iter, idx_shapes.shape[0])\n\t    else:\n\t        idx_shapes = None\n\t    if idx_rots_path is not None:\n\t        idx_rots = loadh5(idx_rots_path)\n\t        idx_rots = torch.from_numpy(idx_rots).to(torch.int64)\n\t        n_iter = min(n_iter, idx_rots.shape[0])\n", "    else:\n\t        idx_rots = None\n\t    m = 0.\n\t    for i in range(n_iter):\n\t        ri = None\n\t        si = None\n\t        if idx_rots is not None:\n\t            ri = idx_rots[i, ...]\n\t        if idx_shapes is not None:\n\t            si = idx_shapes[i, ...]\n", "        m += class_consistency_umetric_(x, r_input, r_can,val_points,\n\t                                        idx_shapes=si, idx_rots=ri, batch_size=batch_size)\n\t    return m / n_iter\n\tdef icp_class_consistency_metric(x, batch_size=32, n_shuffles=10, n_iter=5):\n\t    \"\"\"\n\t    :param x: canonicalized shapes (num_shapes, num_points, 3)\n\t    :param batch_size:\n\t    :param n_shuffles: number of times we shuffle x for self comparison\n\t    :param n_iter: number of icp iterations\n\t    :return:\n", "    \"\"\"\n\t    b = x.shape[0]\n\t    u_ = b % batch_size\n\t    n = b // batch_size\n\t    var_ = 0.\n\t    m = torch.unsqueeze(torch.reshape(torch.eye(3), (9,)), dim=1)\n\t    for j in range(n_shuffles):\n\t        idx = np.random.permutation(x.shape[0])\n\t        y_ = np.take(x, indices=idx, axis=0)\n\t        k = 0.\n", "        varj = 0.\n\t        for i in range(n):\n\t            k += 1.\n\t            r = icp(x[i * batch_size:(i + 1) * batch_size, ...], y_[i * batch_size:(i + 1) * batch_size, ...], n_iter=n_iter)\n\t            r = torch.reshape(r, (r.shape[0], -1))\n\t            r_m = torch.sub(r, m)\n\t            r_m = r_m * r_m\n\t            rn = torch.sum(r_m, dim=-1)\n\t            varj += float(torch.mean(rn))\n\t        if u_ > 0:\n", "            k += u_ / float(batch_size)\n\t            r = icp(x[n * batch_size:, ...], y_[n * batch_size:, ...], n_iter=n_iter)\n\t            r = torch.reshape(r, (r.shape[0], -1))\n\t            r_m = torch.sub(r, m)\n\t            r_m = r_m * r_m\n\t            rn = torch.sum(r_m, dim=-1)\n\t            varj += float(torch.mean(rn))\n\t        varj /= k\n\t    var_ /= float(n_shuffles)\n\t    return np.sqrt(var_)\n", "def shapes_permutations(filename, src_path, tar_path):\n\t    x = loadh5(os.path.join(src_path, filename))\n\t    n_shapes = x.shape[0]\n\t    idx = torch.randperm(n_shapes)\n\t    idx = np.asarray(idx, dtype=np.int)\n\t    h5_fout = h5py.File(os.path.join(tar_path, filename), \"w\")\n\t    h5_fout.create_dataset(\n\t        'data', data=idx,\n\t        compression='gzip', compression_opts=1,\n\t        dtype='uint8')\n", "    h5_fout.close()\n\tdef rot_permutations(tar_path, num_rots):\n\t    filename = \"rotations_permutations.h5\"\n\t    idx = torch.randperm(num_rots)\n\t    idx = np.asarray(idx, dtype=np.int)\n\t    h5_fout = h5py.File(os.path.join(tar_path, filename), \"w\")\n\t    h5_fout.create_dataset(\n\t        'data', data=idx,\n\t        compression='gzip', compression_opts=1,\n\t        dtype='uint8')\n", "    h5_fout.close()\n\tif __name__==\"__main__\":\n\t    AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"table.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\t    # AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\t    # AtlasNetClasses = [\"plane.h5\"]\n\t    AtlasNetShapesPath = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/valid\"\n\t    AtlasNetRotPath = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_valid\"\n\t    r_input_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations.h5\"\n\t    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_full\"\n\t    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_full_multicategory\"\n", "    partial_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_partial\"\n\t    partial_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_partial_multicategory\"\n\t    \"\"\"\"\n\t    for f in AtlasNetClasses:\n\t        shapes_permutations(f, AtlasNetShapesPath, \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\")\n\t    \"\"\"\n\t    # rot_permutations(\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024\", 128)\n\t    # exit(666)\n\t    \"\"\"\n\t    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_full\"\n", "    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_full_multicategory\"\n\t    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_consistency_full\"\n\t    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_consistency_full_multicategory\"\n\t    \"\"\"\n\t    # full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_consistency_full\"\n\t    # full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_consistency_full_multicategory\"\n\t    # ull_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/pca_full\"\n\t    # full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/pca_full_multicategory\"\n\t    # multicategory full shapes\n\t    print(\"multi category\")\n", "    ma = 0.\n\t    mb = 0.\n\t    k = 0.\n\t    for i in range(len(AtlasNetClasses)):\n\t        print(AtlasNetClasses[i])\n\t        a = class_consistency_metric(AtlasNetClasses[i], AtlasNetRotPath, full_multi_pred_path, batch_size=32)\n\t        print(\"consistency: \", a)\n\t        ma += a\n\t        b = equivariance_metric(AtlasNetClasses[i], AtlasNetRotPath, full_multi_pred_path, batch_size=32)\n\t        print(\"equivariance: \", b)\n", "        mb += b\n\t        k += 1.\n\t    print(\"mean class consistency: \", ma / k)\n\t    print(\"mean class equivariance: \", mb / k)\n\t    print(\"category specific\")\n\t    ma = 0.\n\t    mb = 0.\n\t    k = 0.\n\t    for i in range(len(AtlasNetClasses)):\n\t        print(AtlasNetClasses[i])\n", "        a = class_consistency_metric(AtlasNetClasses[i], AtlasNetRotPath, full_pred_path, batch_size=32)\n\t        print(\"consistency: \", a)\n\t        ma += a\n\t        b = equivariance_metric(AtlasNetClasses[i], AtlasNetRotPath, full_pred_path, batch_size=32)\n\t        print(\"equivariance: \", b)\n\t        mb += b\n\t        k += 1.\n\t    print(\"mean class consistency: \", ma / k)\n\t    print(\"mean class equivariance: \", mb / k)\n\t    '''\n", "    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_full\"\n\t    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/spherical_cnns_full_multicategory\"\n\t    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/pca_full\"\n\t    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/pca_full_multicategory\"\n\t    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/caca_full\"\n\t    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/caca_full_multicategory\"\n\t    \"\"\"\n\t    full_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_full\"\n\t    full_multi_pred_path = \"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/preds/tfn_full_multicategory\"\n\t    \"\"\"\n", "    AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"table.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\t    print(\"multi category\")\n\t    ma = 0.\n\t    mb = 0.\n\t    mc = 0.\n\t    k = 0.\n\t    for i in range(len(AtlasNetClasses)):\n\t        print(AtlasNetClasses[i])\n\t        a = class_consistency_metric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_multi_pred_path,\n\t                                shapes_idx_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\", batch_size=32)\n", "        print(\"consistency: \", a)\n\t        ma += a\n\t        b = equivariance_metric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_multi_pred_path,\n\t                                idx_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_permutations.h5\", batch_size=32)\n\t        print(\"equivariance: \", b)\n\t        mb += b\n\t        c = class_consistency_umetric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_multi_pred_path,\n\t                                    idx_shapes_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\",\n\t                                    idx_rots_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_permutations.h5\",\n\t                                    batch_size=32)\n", "        mc += c\n\t        print(\"u_consistency: \", c)\n\t        k += 1.\n\t    print(\"mean multi class consistency: \", ma / k)\n\t    print(\"mean multi class equivariance: \", mb / k)\n\t    print(\"mean multi class uconsistency: \", mc / k)\n\t    AtlasNetClasses = [\"plane.h5\", \"chair.h5\"]\n\t    AtlasNetClasses = [\"plane.h5\", \"bench.h5\", \"cabinet.h5\", \"car.h5\", \"chair.h5\", \"monitor.h5\", \"lamp.h5\", \"speaker.h5\", \"firearm.h5\", \"couch.h5\", \"table.h5\", \"cellphone.h5\", \"watercraft.h5\"]\n\t    print(\"category specific\")\n\t    ma = 0.\n", "    mb = 0.\n\t    mc = 0.\n\t    k = 0.\n\t    for i in range(len(AtlasNetClasses)):\n\t        print(AtlasNetClasses[i])\n\t        a = class_consistency_metric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_pred_path,\n\t                                    shapes_idx_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\",\n\t                                    batch_size=32)\n\t        print(\"consistency: \", a)\n\t        ma += a\n", "        b = equivariance_metric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_pred_path,\n\t                                idx_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_permutations.h5\",\n\t                                batch_size=32)\n\t        print(\"equivariance: \", b)\n\t        mb += b\n\t        c = class_consistency_umetric(AtlasNetClasses[i], AtlasNetShapesPath, r_input_path, full_pred_path,\n\t                                    idx_shapes_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/shapes_permutations\",\n\t                                    idx_rots_path=\"I:/Datasets/Shapes/ShapeNetAtlasNetH5_1024/rotations_permutations.h5\",\n\t                                    batch_size=32)\n\t        mc += c\n", "        print(\"u_consistency: \", c)\n\t        k += 1.\n\t    print(\"mean class consistency: \", ma / k)\n\t    print(\"mean class equivariance: \", mb / k)\n\t    print(\"mean multi class uconsistency: \", mc / k)\n\t    '''\n"]}
{"filename": "cafi_net/spherical_harmonics/clebsch_gordan_decomposition.py", "chunked_list": ["import numpy as np\n\timport torch\n\tfrom spherical_harmonics.wigner_matrix import complex_wigner_, complex_D_wigner, real_D_wigner, euler_rot_zyz\n\tfrom spherical_harmonics.wigner_matrix import complex_to_real_sh, real_to_complex_sh\n\timport scipy\n\tfrom scipy import linalg, matrix, special\n\t\"\"\"\n\tClebsch-Gordan coefficients allows to decompose tensor products of irreducible \n\trepresentations of SO(3)\n\thttps://en.wikipedia.org/wiki/Table_of_Clebsch%E2%80%93Gordan_coefficients#_j2=0\n", " j1 m1 j2 m2 | j3 m3 \n\t j1,j2; m1,m2 | j1,j2; J,M \n\tj1 = j1, j2 = j2, m1 = m1, m2 = m2, j3 = J, m3 = M\n\tsymmetries:\n\t j1 m1 j2 m2 | j3 m3  = (-1)**(j3-j1-j2)* j1 -m1 j2 -m2 | j3 -m3 \n\t j1 m1 j2 m2 | j3 m3  = (-1)**(j3-j1-j2)* j2 m1 j1 m2 | j3 m3 \n\twhen j2 = 0 the ClebschGordan coefficients are given by deltaj3j1*deltam3m1\n\t\"\"\"\n\t\"\"\"\n\tcomputes  j1 m1 j2 m2 | J M  for j1, m1, j2, m2 >= 0\n", "\"\"\"\n\tdef clebsch_gordan_(j1, j2, J, m1, m2, M):\n\t    # d = float((M == m1 + m2))\n\t    if M != m1 + m2:\n\t        return 0.0\n\t    A = float((2*J+1)*np.math.factorial(J+j1-j2)*np.math.factorial(J-j1+j2)*np.math.factorial(j1+j2-J))\n\t    A /= np.math.factorial(J+j1+j2+1)\n\t    B = float(np.math.factorial(J+M)*np.math.factorial(J-M)*np.math.factorial(j1-m1)*\n\t              np.math.factorial(j1+m1)*np.math.factorial(j2-m2)*np.math.factorial(j2+m2))\n\t    C = 0.\n", "    b0 = (j1+j2-J)\n\t    b1 = (j1-m1)\n\t    b2 = (j2+m2)\n\t    a0 = 0\n\t    a1 = (J-j2+m1)\n\t    a2 = (J-j1-m2)\n\t    k2 = np.min([b0, b1, b2])\n\t    k1 = np.max([-a0, -a1, -a2])\n\t    for k in range(k1, k2+1):\n\t        a0_ = np.math.factorial(k+a0)\n", "        a1_ = np.math.factorial(k+a1)\n\t        a2_ = np.math.factorial(k+a2)\n\t        b0_ = np.math.factorial(b0-k)\n\t        b1_ = np.math.factorial(b1-k)\n\t        b2_ = np.math.factorial(b2-k)\n\t        C += ((-1)**k)/(float(a0_*a1_*a2_*b0_*b1_*b2_))\n\t    return np.sqrt(A * B) * C\n\t\"\"\"\n\tcomputes  j1 m1 j2 m2 | J M \n\t\"\"\"\n", "def clebsch_gordan_coeff(j1, j2, J, m1, m2, M):\n\t    if M < 0:\n\t        if j1 >= j2:\n\t            return (-1.)**(J-j1-j2)*clebsch_gordan_(j1, j2, J, -m1, -m2, -M)\n\t        else:\n\t            return clebsch_gordan_(j2, j1, J, -m2, -m1, -M)\n\t    else:\n\t        if j1 >= j2:\n\t            return clebsch_gordan_(j1, j2, J, m1, m2, M)\n\t        else:\n", "            return (-1.) ** (J - j1 - j2) * clebsch_gordan_(j2, j1, J, m2, m1, M)\n\t\"\"\"\n\tcomputes the projection from type (j1, j2) to type J \n\tQ*kron(Dj1, Dj2)*Q.T = DJ\n\t\"\"\"\n\tdef np_real_clebsch_gordan_projector(j1, j2, J, matrix_shape=True, dtype=np.float32):\n\t    #Q = np.zeros(shape=(2 * J + 1, (2 * j1 + 1) * (2 * j2 + 1)), dtype=dtype)\n\t    Q = np.zeros(shape=(2 * J + 1, 2 * j1 + 1, 2 * j2 + 1), dtype=dtype)\n\t    for m1 in range(-j1, j1 + 1):\n\t        for m2 in range(-j2, j2 + 1):\n", "            m3 = m1 + m2\n\t            if -J <= m3 <= J:\n\t                #Q[m3 + J, (2 * j2 + 1) * (m1 + j1) + (m2 + j2)] = clebsch_gordan_coeff(j1, j2, J, m1, m2, m3)\n\t                Q[m3 + J, m1 + j1, m2 + j2] = clebsch_gordan_coeff(j1, j2, J, m1, m2, m3)\n\t    Q = np.reshape(Q, newshape=(2*J+1, -1))\n\t    CRj1 = complex_to_real_sh(j1)\n\t    RCj1 = np.conjugate(CRj1.T)\n\t    CRj2 = complex_to_real_sh(j2)\n\t    RCj2 = np.conjugate(CRj2.T)\n\t    CRJ = complex_to_real_sh(J)\n", "    # RCJ = np.conjugate(CRJ.T)\n\t    Q = np.matmul(np.matmul(CRJ, Q), np.kron(RCj1, RCj2))\n\t    Q = np.real(Q)\n\t    Q = Q.astype(dtype=dtype)\n\t    if not matrix_shape:\n\t        Q = np.reshape(Q, newshape=(2*J+1, 2*j1+1, 2*j2+1))\n\t    return Q\n\tdef sparse_matrix(M, eps=0.00001):\n\t    m = np.shape(M)[0]\n\t    n = np.shape(M)[1]\n", "    idx_ = []\n\t    coeffs_ = []\n\t    n_ = 0\n\t    for i in range(m):\n\t        idx_.append([])\n\t        coeffs_.append([])\n\t        for j in range(n):\n\t            if np.abs(M[i, j]) > eps:\n\t                idx_[-1].append(j)\n\t                coeffs_[-1].append(M[i, j])\n", "        n_ = max(n_, len(idx_[-1]))\n\t    idx = np.zeros((m, n_), dtype=np.int32)\n\t    coeffs = np.zeros((m, n_), dtype=np.float32)\n\t    for i in range(m):\n\t        for j in range(len(idx_[i])):\n\t            idx[i, j] = idx_[i][j]\n\t            coeffs[i, j] = coeffs_[i][j]\n\t    return coeffs, idx\n\t\"\"\"\n\tComputes the Clebsch Gordan decomposition \n", "\"\"\"\n\tdef np_clebsch_gordan_decomposition(j1, j2, matrix_shape=True, l_max=None, dtype=np.float32):\n\t    Q = []\n\t    for J in range(abs(j1-j2), min(j1+j2+1, l_max+1)):\n\t        Q.append(np_real_clebsch_gordan_projector(j1, j2, J, matrix_shape=matrix_shape, dtype=dtype))\n\t    Q = np.concatenate(Q, axis=0)\n\t    return Q\n\tdef torch_clebsch_gordan_decomposition_(j1, j2, sparse=False, l_max=None, dtype=torch.float32):\n\t    Q = np_clebsch_gordan_decomposition(j1, j2, matrix_shape=True, l_max=l_max, dtype=np.float32)\n\t    if sparse:\n", "        coeffs, idx = sparse_matrix(Q)\n\t        idx = torch.from_numpy(idx).to(torch.int64)\n\t        coeffs = torch.from_numpy(coeffs).to(torch.float32)\n\t        return coeffs, idx\n\t    else:\n\t        return torch.from_numpy(Q).to(dtype)\n\tdef representation_type(x):\n\t    j1 = int((x.shape[-3] - 1) / 2)\n\t    j2 = int((x.shape[-2] - 1) / 2)\n\t    return (j1, j2)\n", "def decompose_(x, Q):\n\t    j1, j2 = representation_type(x)\n\t    # Q = Q[(j1, j2)]\n\t    s = list(x.shape)\n\t    c = s[-1]\n\t    s.pop()\n\t    s[-2] = -1\n\t    s[-1] = c\n\t    x = torch.reshape(x, shape=s)\n\t    x = torch.einsum('ij,...jk->...ik', Q.type_as(x), x)\n", "    y = []\n\t    p = 0\n\t    for J in range(abs(j1-j2), j1+j2+1):\n\t        y.append(x[..., p:p+2*J+1, :])\n\t        p += 2*J+1\n\t    return y\n\tdef sparse_decompose_(x, coeffs, idx):\n\t    j1, j2 = representation_type(x)\n\t    # coeffs = Q[(j1, j2)][0]\n\t    # idx = Q[(j1, j2)][0]\n", "    s = list(x.shape)\n\t    c = s[-1]\n\t    s.pop()\n\t    s[-2] = -1\n\t    s[-1] = c\n\t    x = torch.reshape(x, shape=s)\n\t    x = (x[..., idx, :])\n\t    x = torch.einsum('ij,...ijk->...ik', coeffs, x)\n\t    y = []\n\t    p = 0\n", "    for J in range(abs(j1-j2), j1+j2+1):\n\t        y.append(x[..., p:p+2*J+1, :])\n\t        p += 2*J+1\n\t    return y\n\tclass torch_clebsch_gordan_decomposition:\n\t    def __init__(self, l_max, l_max_out=None, sparse=False, output_type='dict'):\n\t        self.dtype = torch.float32\n\t        self.l_max = l_max\n\t        self.sparse = sparse\n\t        self.output_type = output_type\n", "        if l_max_out is None:\n\t            self.l_max_out = 2*l_max\n\t        else:\n\t            self.l_max_out = l_max_out\n\t        self.Q = dict()\n\t        for j1 in range(self.l_max+1):\n\t            for j2 in range(self.l_max+1):\n\t                if self.l_max_out < abs(j1-j2):\n\t                    pass\n\t                elif self.sparse:\n", "                    coeffs, idx = torch_clebsch_gordan_decomposition_(j1, j2, l_max=self.l_max_out,\n\t                                                                   sparse=True, dtype=self.dtype)\n\t                    self.Q[(j1, j2)] = [coeffs, idx]\n\t                else:\n\t                    self.Q[(j1, j2)] = torch_clebsch_gordan_decomposition_(j1, j2, l_max=self.l_max_out,\n\t                                                                        sparse=False, dtype=self.dtype)\n\t    def decompose(self, x):\n\t        if not isinstance(x, list):\n\t            x = [x]\n\t        y = dict()\n", "        for i in range(len(x)):\n\t            j1, j2 = representation_type(x[i])\n\t            if self.sparse:\n\t                yi = sparse_decompose_(x[i], self.Q[(j1, j2)][0], self.Q[(j1, j2)][1])\n\t            else:\n\t                yi = decompose_(x[i], self.Q[(j1, j2)])\n\t            for J in range(abs(j1-j2), min(j1+j2+1, self.l_max_out+1)):\n\t                if not str(J) in y:\n\t                    y[str(J)] = []\n\t                y[str(J)].append(yi[J-abs(j1-j2)])\n", "        for J in y:\n\t            y[J] = torch.cat(y[J], dim=-1)\n\t        if self.output_type == 'list':\n\t            return y.values()\n\t        else:\n\t            return y\n\t\"\"\"\n\tcomputes the Clebsch Gordan decomposition of the Zernike basis (up to degree d) tensored by a dimension 2*k+1\n\tequivariant feature.\n\t\"\"\"\n", "\"\"\"\n\tdef np_zernike_clebsch_gordan_decomposition(d, k, matrix_shape=True, l_max=None, dtype=np.float32):\n\t    zerinke_basis_idx = []\n\t    size_in = 0\n\t    size_out = 0\n\t    num_out_features = [0]*(d+1+k+1)\n\t    for l in range(1, d + 1):\n\t        for n in range(min(2 * d - l + 1, l + 1)):\n\t            if (n - l) % 2 == 0:\n\t                size_in += 2*l + 1\n", "                zerinke_basis_idx.append((n, l))\n\t                np_clebsch_gordan_decomposition(l, k, matrix_shape=True, l_max=l_max, dtype=np.float32)\n\t                for J in range(abs(l-k), min(l+k+1, l_max+1)):\n\t                    num_out_features[J] += 1\n\t                    size_out\n\t    Q = np.zeros()\n\t    for i in range(len())\n\t    for J in range(abs(j1-j2), min(j1+j2+1, l_max+1)):\n\t        Q.append(np_real_clebsch_gordan_projector(j1, j2, J, matrix_shape=matrix_shape, dtype=dtype))\n\t    Q = np.concatenate(Q, axis=0)\n", "    return Q\n\t\"\"\"\n\tdef real_conj(A, Q):\n\t    return np.matmul(Q.T, np.matmul(A, Q))\n\tdef complex_conj(A, Q):\n\t    return np.matmul(np.conjugate(Q.T), np.matmul(A, Q))\n\tdef unit_test4():\n\t    j1 = 2\n\t    j2 = 2\n\t    J = 2\n", "    # cb_dict = clebsch_gordan_dict()\n\t    Q = np.asmatrix(clebsch_gordan_matrix(j1, j2, J, dtype=np.complex64))\n\t    # Q = np.sqrt(2.) * Q\n\t    # Q_ = np.asmatrix(Q_from_cb_dict(j1, j2, J, cb_dict, dtype=np.complex64))\n\t    # Q_ = np.sqrt(2.)*Q_\n\t    # Q__ = tensorProductDecompose_(j1, j2, J)\n\t    # Q__ = np.sqrt(1./2.49634557e-02)*Q__\n\t    angles = np.random.rand(3)\n\t    # angles = [1., 0., 0.]\n\t    Dj1 = complex_wigner_(j1, angles[0], angles[1], angles[2])\n", "    Dj2 = complex_wigner_(j2, angles[0], angles[1], angles[2])\n\t    DJ = complex_wigner_(J, angles[0], angles[1], angles[2])\n\t    print('eee')\n\t    prod = np.kron(Dj1, Dj2)\n\t    y = np.matmul(np.matmul(Q, prod), Q.T) - DJ\n\t    # print(y)\n\t    # print(np.matmul(Q.T, Q))\n\t    # print(np.matmul(Q, Q.T))\n\t    # print(np.real(prod))\n\t    # print(np.real(Q))\n", "    # print(np.real(Q__))\n\t    # y = np.matmul(Q, prod) - np.matmul(DJ, Q)\n\t    # y = np.matmul(y, Q.T)\n\t    # print(np.linalg.norm(Q - Q_, 'fro'))\n\t    print(np.linalg.norm(y))\n\t    print(np.linalg.norm(DJ))\n\tdef unit_test5():\n\t    j1 = 1\n\t    j2 = 1\n\t    angles = np.random.rand(3)\n", "    # angles = [1.0, 0.0, 0.]\n\t    D0 = np.asmatrix([[1.]], dtype=np.complex64)\n\t    D1 = complex_wigner_(1, angles[0], angles[1], angles[2])\n\t    D2 = complex_wigner_(2, angles[0], angles[1], angles[2])\n\t    D = [D0, D1, D2]\n\t    prod = np.kron(D[j1], D[j2])\n\t    # prod = np.kron(D[j2], D[j1])\n\t    c = 0.0\n\t    for m1 in range(-j1, j1+1):\n\t        for k1 in range(-j1, j1+1):\n", "            for m2 in range(-j2, j2+1):\n\t                for k2 in range(-j2, j2+1):\n\t                    a = D[j1][j1 + m1, j1 + k1] * D[j2][j2 + m2, j2 + k2]\n\t                    b = 0.\n\t                    # b = prod[(2*j2+1)*(m1+j1) + (m2+j2), (2*j2+1)*(k1+j1) + (k2+j2)]\n\t                    for J in range(abs(j1-j2), j1+j2+1):\n\t                        if(2*J >= m1+m2+J >= 0 and 2*J >= k1+k2+J >= 0):\n\t                            b += D[J][m1+m2+J, k1+k2+J] * clebsch_gordan_coeff(j1, j2, J, m1, m2, m1 + m2) * clebsch_gordan_coeff(j1, j2, J, k1, k2, k1 + k2)\n\t                    print('zz')\n\t                    print(a)\n", "                    print(b)\n\t                    print(a-b)\n\t                    c += abs(np.real(a-b))*abs(np.real(a-b))+abs(np.imag(a-b))*abs(np.imag(a-b))\n\t    print('rr')\n\t    print(np.sqrt(c))\n\tdef unit_test6():\n\t    angles = np.random.rand(3)\n\t    # angles = [0., 1., 0.]\n\t    Q0 = np.asmatrix(clebsch_gordan_matrix(1, 1, 0, dtype=np.complex64))\n\t    Q1 = np.asmatrix(clebsch_gordan_matrix(1, 1, 1, dtype=np.complex64))\n", "    Q2 = np.asmatrix(clebsch_gordan_matrix(1, 1, 2, dtype=np.complex64))\n\t    D0 = np.asmatrix([[1.]], dtype=np.complex64)\n\t    D1 = complex_wigner_(1, angles[0], angles[1], angles[2])\n\t    D2 = complex_wigner_(2, angles[0], angles[1], angles[2])\n\t    y = np.kron(D1, D1) - real_conj(D2, Q2) - real_conj(D1, Q1) - real_conj(D0, Q0)\n\t    print(np.linalg.norm(y))\n\tdef tensor_decomposition_unit_test___(j, k, J, a, b, c):\n\t    Dj = complex_D_wigner(j, a, b, c)\n\t    Dk = complex_D_wigner(k, a, b, c)\n\t    DJ = complex_D_wigner(k, a, b, c)\n", "    assert(j+k >= J >= abs(k-j))\n\t    QJ = clebsch_gordan_matrix(j, k, J)\n\t    y = real_conj(np.kron(Dj, Dk), QJ.T) - DJ\n\t    print(np.linalg.norm(y))\n\tdef tensor_decomposition_unit_test__(j, k, a, b, c):\n\t    Dj = complex_D_wigner(j, a, b, c)\n\t    Dk = complex_D_wigner(k, a, b, c)\n\t    D_ = np.zeros(shape=((2*j+1)*(2*k+1), (2*j+1)*(2*k+1)), dtype=np.complex64)\n\t    D = np.kron(Dj, Dk)\n\t    for J in range(abs(k-j), k+j+1):\n", "        print('j = ', j, 'k = ', k, 'J = ', J)\n\t        DJ = complex_D_wigner(J, a, b, c)\n\t        QJ = clebsch_gordan_matrix(j, k, J)\n\t        y = real_conj(np.kron(Dj, Dk), QJ.T) - DJ\n\t        # print(np.linalg.norm(DJ))\n\t        print(np.linalg.norm(y))\n\t        D_ += real_conj(DJ, QJ)\n\t    print('decompose j = ', j, 'k = ', k)\n\t    print(np.linalg.norm(D - D_))\n\tdef tensor_decomposition_unit_test(l):\n", "    for i in range(10):\n\t        angles = np.random.rand(3)\n\t        a = angles[0]\n\t        b = angles[1]\n\t        c = angles[2]\n\t        for j in range(l+1):\n\t            for k in range(l+1):\n\t                tensor_decomposition_unit_test__(j, k, a, b, c)\n\tdef invariant_feature(equivariant_features, p, q, Q):\n\t    # y = tf.einsum('bvqmrc,bvqnrc->bvqmnrc', equivariant_features[p[0]], equivariant_features[p[1]])\n", "    # the equivariant channels must in the last dimesion\n\t    #y = tf.einsum('bvqrcm,bvqrcn->bvqrcmn', equivariant_features[p[0]], equivariant_features[p[1]])\n\t    \"\"\"\n\t    nb = y.get_shape()[0].value\n\t    nv = y.get_shape()[1].value\n\t    nq = y.get_shape()[2].value\n\t    nr = y.get_shape()[3].value\n\t    nc = y.get_shape()[4].value\n\t    y = tf.reshape(y, shape=(nb, nv, nq, nr, nc, -1))\n\t    \"\"\"\n", "def higher_product_matrix(p, q):\n\t    Q = npClebschGordanMatrices(3)\n\t    \"\"\"\n\t    res = np.eye((2*abs(q[0])+1)*(2*abs(p[1])+1))\n\t    I = np.eye(1)\n\t    res = np.real(np.reshape(Q.getMatrix(q[0], p[1], q[1]), newshape=(2*abs(q[1])+1, -1)))\n\t    for i in range(len(p)-1):\n\t        Qi_ = np.real(np.reshape(Q.getMatrix(q[i+1], p[i+2], q[i+2]), newshape=(2*abs(q[i+1])+1, -1)))\n\t        Qi = np.kron(Qi_, I)\n\t        res = np.matmul(Qi_, np.kron(res, I))\n", "        I = np.kron(I, np.eye(2 * abs(p[i + 1]) + 1))\n\t    \"\"\"\n\t    Q1 = np.reshape(Q.getMatrix(q[0], p[1], q[1]), newshape=(2*abs(q[1])+1, -1))\n\t    Q2 = np.reshape(Q.getMatrix(q[1], p[2], q[2]), newshape=(2 * abs(q[2]) + 1, -1))\n\t    M = np.real(Q1)\n\t    M = np.kron(M, np.eye(2*abs(p[2])+1))\n\t    M = np.matmul(np.real(Q2), M)\n\t    print(M)\n\t    print(np.matmul(M, M.transpose()))\n\t    return\n", "def higher_product(R, X, p, q, Q):\n\t    # print(np.linalg.norm(y))\n\t    # print(y)\n\t    X = np.asmatrix(np.random.rand(1, 3))\n\t    X /= (np.linalg.norm(X))\n\t    X *= 10.0\n\t    X_rot = (np.matmul(R.T, X.T)).T\n\t    y = complex_sh_(abs(p[0]), X)\n\t    y_rot = complex_sh_(abs(p[0]), X_rot)\n\t    for i in range(len(p)-1):\n", "        \"\"\"\n\t        print('uuu')\n\t        print(X.shape)\n\t        print(y.shape)\n\t        print(p)\n\t        print(complex_sh_(abs(p[i+1]), X).shape)\n\t        print(Q.getMatrix(q[i], p[i+1], q[i+1]).shape)\n\t        print('aaa')\n\t        \"\"\"\n\t        \"\"\"\n", "        print('aaaaaa')\n\t        print(q[i], p[i+1], q[i+1])\n\t        print(Q.getMatrix(q[i], p[i+1], q[i+1]))\n\t        print('bbbbbb')\n\t        \"\"\"\n\t        X = np.asmatrix(np.random.rand(1, 3))\n\t        X /= (np.linalg.norm(X))\n\t        X *= 10.0\n\t        X_rot = (np.matmul(R.T, X.T)).T\n\t        z = np.einsum('jmn,jmn->j', Q.getMatrix(q[i], p[i+1], q[i+1]), Q.getMatrix(q[i], p[i+1], q[i+1]))\n", "        print('qi, pi+1, qi+1 = ', q[i], p[i + 1], q[i + 1])\n\t        print('norm z= ', np.linalg.norm(z))\n\t        y = np.einsum('vm,vn->vmn', y, complex_sh_(abs(p[i+1]), X))\n\t        y_rot = np.einsum('vm,vn->vmn', y_rot, complex_sh_(abs(p[i+1]), X_rot))\n\t        y = np.einsum('jmn,vmn->vj', Q.getMatrix(q[i], p[i+1], q[i+1]), y)\n\t        y_rot = np.einsum('jmn,vmn->vj', Q.getMatrix(q[i], p[i + 1], q[i + 1]), y_rot)\n\t        # print(y)\n\t        # print(np.linalg.norm(y))\n\t    return y\n\tdef higher_tensor_decomposition_unit_test():\n", "    Q = npClebschGordanMatrices(3)\n\t    p = []\n\t    q = []\n\t    # degree 1 invariants\n\t    p.append(np.zeros(shape=(1, 1), dtype=np.int32))\n\t    q.append(np.zeros(shape=(1, 1), dtype=np.int32))\n\t    p.append(np.array([[1, 1], [2, 2]], dtype=np.int32))\n\t    q.append(np.array([[1, 0], [2, 0]], dtype=np.int32))\n\t    p.append(np.array([[1, 1, 1],\n\t                       [1, 1, 2],\n", "                       [1, 2, 2],\n\t                       [2, 2, 2]], dtype=np.int32))\n\t    q.append(np.array([[1, 1, 0],\n\t                       [1, 2, 0],\n\t                       [1, 2, 0],\n\t                       [2, 2, 0]], dtype=np.int32))\n\t    for i in range(10):\n\t        angles = np.random.rand(3)\n\t        X = np.asmatrix(np.random.rand(1, 3))\n\t        X /= (np.linalg.norm(X))\n", "        a = angles[0]\n\t        b = angles[1]\n\t        c = angles[2]\n\t        R = euler_rot_zyz(a, b, c)\n\t        for d in range(len(p)):\n\t            for j in range(np.size(p[d], 0)):\n\t                print(p[d][j, :])\n\t                print(q[d][j, :])\n\t                z = higher_product(R, X, p[d][j, :], q[d][j, :], Q)\n\t                print('norm output = ', np.linalg.norm(z))\n", "                # print(np.linalg.norm(X))\n\tdef real_tensor_decomposition_unit_test__(j, k, a, b, c):\n\t    # CRj = complex_to_real_sh(j)\n\t    # CRk = complex_to_real_sh(k)\n\t    # K = np.kron(CRj, CRk)\n\t    # K_T = np.conjugate(K.T)\n\t    Dj = real_D_wigner(j, a, b, c)\n\t    Dk = real_D_wigner(k, a, b, c)\n\t    D_ = np.zeros(shape=((2*j+1)*(2*k+1), (2*j+1)*(2*k+1)), dtype=np.complex64)\n\t    D = np.kron(Dj, Dk)\n", "    for J in range(abs(k-j), k+j+1):\n\t        print('j = ', j, 'k = ', k, 'J = ', J)\n\t        # CRJ = complex_to_real_sh(J)\n\t        # RCJ = np.conjugate(CRJ.T)\n\t        DJ = real_D_wigner(J, a, b, c)\n\t        QJ = real_Q_from_cb(j, k, J, dtype=np.complex64)\n\t        y = complex_conj(np.kron(Dj, Dk), np.conjugate(QJ.T)) - DJ\n\t        # print(np.linalg.norm(DJ))\n\t        print(np.linalg.norm(y))\n\t        D_ += real_conj(DJ, QJ)\n", "    print('decompose j = ', j, 'k = ', k)\n\t    print(np.linalg.norm(D - D_))\n\tdef real_tensor_decomposition_unit_test(l):\n\t    for i in range(3):\n\t        angles = np.random.rand(3)\n\t        a = angles[0]\n\t        b = angles[1]\n\t        c = angles[2]\n\t        for j in range(l+1):\n\t            for k in range(l+1):\n", "                real_tensor_decomposition_unit_test__(j, k, a, b, c)\n"]}
{"filename": "cafi_net/spherical_harmonics/spherical_cnn.py", "chunked_list": ["from spherical_harmonics.kernels import *\n\timport numpy as np\n\timport torch\n\tdef torch_fibonnacci_sphere_sampling(num_pts):\n\t    indices = np.arange(0, num_pts, dtype=float) + 0.5\n\t    phi = np.arccos(1 - 2*indices/num_pts)\n\t    theta = np.pi * (1 + 5**0.5) * indices\n\t    x, y, z = np.cos(theta) * np.sin(phi), np.sin(theta) * np.sin(phi), np.cos(phi)\n\t    S2 = np.stack([x, y, z], axis=-1)\n\t    return torch.from_numpy(S2).to(torch.float32)\n", "def monoms_3D(d):\n\t    monoms_basis = []\n\t    for I in range((d + 1) ** 3):\n\t        i = I % (d + 1)\n\t        a = int((I - i) / (d + 1))\n\t        j = a % (d + 1)\n\t        k = int((a - j) / (d + 1))\n\t        if (i + j + k == d):\n\t            monoms_basis.append((i, j, k))\n\t    monoms_basis = list(set(monoms_basis))\n", "    monoms_basis = sorted(monoms_basis)\n\t    return monoms_basis\n\tdef torch_eval_monoms_3D(x, d, axis=-1):\n\t    m = monoms_3D(d)\n\t    pows = np.zeros((3, len(m)), dtype=np.int32)\n\t    for i in range(len(m)):\n\t        for j in range(3):\n\t            pows[j, i] = m[i][j]\n\t    pows = torch.from_numpy(pows).to(torch.float32).to(x.device)\n\t    n = len(list(x.shape))\n", "    axis = axis % n\n\t    shape = [1]*(n+1)\n\t    shape[axis] = 3\n\t    shape[-1] = len(m)\n\t    pows = torch.reshape(pows, shape)\n\t    x = x.unsqueeze(-1)\n\t    y = torch.pow(x, pows)\n\t    y = torch.prod(y, dim = axis, keepdims=False)\n\t    return y\n\tdef np_monomial_basis_coeffs(polynomials, monoms_basis):\n", "    n_ = len(monoms_basis)\n\t    m_ = len(polynomials)\n\t    M = np.zeros((m_, n_))\n\t    for i in range(m_):\n\t        for j in range(n_):\n\t            M[i, j] = re(polynomials[i].coeff_monomial(monoms_basis[j]))\n\t    return M\n\tdef torch_monomial_basis_coeffs(polynomials, monoms_basis, dtype=torch.float32):\n\t    return torch.from_numpy(np_monomial_basis_coeffs(polynomials, monoms_basis)).to(dtype)\n\tdef torch_spherical_harmonics_(l, matrix_format=True):\n", "    monoms = monoms_3D(l)\n\t    sph_polys = []\n\t    x, y, z = symbols(\"x y z\")\n\t    for m in range(2*l+1):\n\t        sph_polys.append(real_spherical_harmonic(l, m-l, x, y, z, poly = True))\n\t    coeffs = torch_monomial_basis_coeffs(sph_polys, monoms)\n\t    if matrix_format:\n\t        coeffs = torch.reshape(coeffs, (2*l+1, -1))\n\t    return coeffs\n\tclass zernike_monoms:\n", "    def __init__(self,max_deg=3):\n\t        self.max_deg = max_deg\n\t        self.harmonics = torch_spherical_harmonics(l_max = max_deg)\n\t    def compute(self,x):\n\t        max_deg = self.max_deg\n\t        m = int(max_deg / 2.)\n\t        n2 = torch.sum(x * x, dim=-1, keepdims=True)\n\t        n2 = n2.unsqueeze(-1)\n\t        p = [torch.ones(n2.shape).type_as(x)]\n\t        for m_ in range(m):\n", "            p.append(p[-1] * n2)\n\t        y = torch_spherical_harmonics(l_max=max_deg).compute(x)\n\t        for l in y:\n\t            y[l] = y[l].unsqueeze(-1)\n\t        z = dict()\n\t        for d in range(max_deg + 1):\n\t            z[d] = []\n\t        for l in y:\n\t            l_ = int(l)\n\t            for d in range(m + 1):\n", "                d_ = 2 * d + l_\n\t                if d_ <= max_deg:\n\t                    # print(p[d].shape)\n\t                    # print(y[l].shape)\n\t                    zd = (p[d] * y[l])\n\t                    z[l_].append(zd)\n\t        for d in z:\n\t            z[d] = torch.cat(z[d], dim=-1)\n\t        return z\n\tclass torch_zernike_monoms:\n", "    def __init__(self, max_deg):\n\t        self.max_deg = max_deg\n\t        self.sph_harmonics = torch_spherical_harmonics(l_max=max_deg)\n\t        pass\n\t    def compute(self, x):\n\t        m = int(self.max_deg / 2.)\n\t        n2 = torch.sum(x * x, dim=-1, keepdims=True)\n\t        n2 = n2.unsqueeze(-1)\n\t        p = [torch.ones(n2.shape).type_as(x)]\n\t        for m_ in range(m):\n", "            p.append(p[-1] * n2)\n\t        y = self.sph_harmonics.compute(x)\n\t        for l in y:\n\t            y[l] = y[l].unsqueeze(-1)\n\t        z = dict()\n\t        for d in range(self.max_deg + 1):\n\t            z[d] = []\n\t        for l in y:\n\t            l_ = int(l)\n\t            for d in range(m + 1):\n", "                d_ = 2 * d + l_\n\t                if d_ <= self.max_deg:\n\t                    # print(p[d].shape)\n\t                    # print(y[l].shape)\n\t                    zd = (p[d] * y[l])\n\t                    z[l_].append(zd)\n\t        for d in z:\n\t            z[d] = torch.cat(z[d], dim=-1)\n\t        return z\n\tclass torch_spherical_harmonics:\n", "    def __init__(self, l_max=3, l_list=None):\n\t        if l_list is None:\n\t            self.l_list = range(l_max+1)\n\t        else:\n\t            self.l_list = l_list\n\t        self.l_max = max(self.l_list)\n\t        self.Y = dict()\n\t        for l in self.l_list:\n\t            self.Y[str(l)] = torch_spherical_harmonics_(l)\n\t    def compute(self, x):\n", "        Y = dict()\n\t        for l in self.l_list:\n\t            ml = torch_eval_monoms_3D(x, l)\n\t            Y[str(l)] = torch.einsum('mk,...k->...m', self.Y[str(l)].type_as(ml), ml)\n\t        return Y\n\tdef np_polyhedrons(poly):\n\t    C0 = 3 * np.sqrt(2) / 4\n\t    C1 = 9 * np.sqrt(2) / 8\n\t    tetrakis_hexahedron = np.array([[0.0, 0.0, C1],\n\t                                    [0.0, 0.0, -C1],\n", "                                    [C1, 0.0, 0.0],\n\t                                    [-C1, 0.0, 0.0],\n\t                                    [0.0, C1, 0.0],\n\t                                    [0.0, -C1, 0.0],\n\t                                    [C0, C0, C0],\n\t                                    [C0, C0, -C0],\n\t                                    [C0, -C0, C0],\n\t                                    [C0, -C0, -C0],\n\t                                    [-C0, C0, C0],\n\t                                    [-C0, C0, -C0],\n", "                                    [-C0, -C0, C0],\n\t                                    [-C0, -C0, -C0]], dtype=np.float32)\n\t    C0 = (1 + np.sqrt(5)) / 4\n\t    C1 = (3 + np.sqrt(5)) / 4\n\t    regular_dodecahedron = np.array([[0.0, 0.5, C1], [0.0, 0.5, -C1], [0.0, -0.5, C1], [0.0, -0.5, -C1],\n\t                      [C1, 0.0, 0.5], [C1, 0.0, -0.5], [-C1, 0.0, 0.5], [-C1, 0.0, -0.5],\n\t                      [0.5, C1, 0.0], [0.5, -C1, 0.0], [-0.5, C1, 0.0], [-0.5, -C1, 0.0],\n\t                      [C0, C0, C0], [C0, C0, -C0], [C0, -C0, C0], [C0, -C0, -C0],\n\t                      [-C0, C0, C0], [-C0, C0, -C0], [-C0, -C0, C0], [-C0, -C0, -C0]], dtype=np.float32)\n\t    C0 = 3 * (np.sqrt(5) - 1) / 4\n", "    C1 = 9 * (9 + np.sqrt(5)) / 76\n\t    C2 = 9 * (7 + 5 * np.sqrt(5)) / 76\n\t    C3 = 3 * (1 + np.sqrt(5)) / 4\n\t    pentakis_dodecahedron = np.array([[0.0, C0, C3], [0.0, C0, -C3], [0.0, -C0, C3], [0.0, -C0, -C3],\n\t                      [C3, 0.0, C0], [C3, 0.0, -C0], [-C3, 0.0, C0], [-C3, 0.0, -C0],\n\t                      [C0, C3, 0.0], [C0, -C3, 0.0], [-C0, C3, 0.0], [-C0, -C3, 0.0],\n\t                      [C1, 0.0, C2], [C1, 0.0, -C2], [-C1, 0.0, C2], [-C1, 0.0, -C2],\n\t                      [C2, C1, 0.0], [C2, -C1, 0.0], [-C2, C1, 0.0], [-C2, -C1, 0.0],\n\t                      [0.0, C2, C1], [0.0, C2, -C1], [0.0, -C2, C1], [0.0, -C2, -C1],\n\t                      [1.5, 1.5, 1.5], [1.5, 1.5, -1.5], [1.5, -1.5, 1.5], [1.5, -1.5, -1.5],\n", "                      [-1.5, 1.5, 1.5], [-1.5, 1.5, -1.5], [-1.5, -1.5, 1.5], [-1.5, -1.5, -1.5]],\n\t                     dtype=np.float)\n\t    C0 = 3 * (15 + np.sqrt(5)) / 44\n\t    C1 = (5 - np.sqrt(5)) / 2\n\t    C2 = 3 * (5 + 4 * np.sqrt(5)) / 22\n\t    C3 = 3 * (5 + np.sqrt(5)) / 10\n\t    C4 = np.sqrt(5)\n\t    C5 = (75 + 27 * np.sqrt(5)) / 44\n\t    C6 = (15 + 9 * np.sqrt(5)) / 10\n\t    C7 = (5 + np.sqrt(5)) / 2\n", "    C8 = 3 * (5 + 4 * np.sqrt(5)) / 11\n\t    disdyakis_triacontahedron = np.array([[0.0, 0.0, C8], [0.0, 0.0, -C8], [C8, 0.0, 0.0], [-C8, 0.0, 0.0],\n\t                                          [0.0, C8, 0.0], [0.0, -C8, 0.0], [0.0, C1, C7], [0.0, C1, -C7],\n\t                                          [0.0, -C1, C7], [0.0, -C1, -C7], [C7, 0.0, C1], [C7, 0.0, -C1],\n\t                                          [-C7, 0.0, C1], [-C7, 0.0, -C1], [C1, C7, 0.0], [C1, -C7, 0.0],\n\t                                          [-C1, C7, 0.0], [-C1, -C7, 0.0], [C3, 0.0, C6], [C3, 0.0, -C6],\n\t                                          [-C3, 0.0, C6], [-C3, 0.0, -C6], [C6, C3, 0.0], [C6, -C3, 0.0],\n\t                                          [-C6, C3, 0.0], [-C6, -C3, 0.0], [0.0, C6, C3], [0.0, C6, -C3],\n\t                                          [0.0, -C6, C3], [0.0, -C6, -C3], [C0, C2, C5], [C0, C2, -C5],\n\t                                          [C0, -C2, C5], [C0, -C2, -C5], [-C0, C2, C5], [-C0, C2, -C5],\n", "                                          [-C0, -C2, C5], [-C0, -C2, -C5], [C5, C0, C2], [C5, C0, -C2],\n\t                                          [C5, -C0, C2], [C5, -C0, -C2], [-C5, C0, C2], [-C5, C0, -C2],\n\t                                          [-C5, -C0, C2], [-C5, -C0, -C2], [C2, C5, C0], [C2, C5, -C0],\n\t                                          [C2, -C5, C0], [C2, -C5, -C0], [-C2, C5, C0], [-C2, C5, -C0],\n\t                                          [-C2, -C5, C0], [-C2, -C5, -C0], [C4, C4, C4], [C4, C4, -C4],\n\t                                          [C4, -C4, C4], [C4, -C4, -C4], [-C4, C4, C4], [-C4, C4, -C4],\n\t                                          [-C4, -C4, C4], [-C4, -C4, -C4]], dtype=np.float32)\n\t    P = {'tetrakis_hexahedron':tetrakis_hexahedron,\n\t         'regular_dodecahedron':regular_dodecahedron,\n\t         'pentakis_dodecahedron':pentakis_dodecahedron,\n", "         'disdyakis_triacontahedron':disdyakis_triacontahedron}\n\t    p = P[poly]\n\t    c = np.mean(p, axis=0, keepdims=True)\n\t    p = np.subtract(p, c)\n\t    n = np.linalg.norm(p, axis=-1, keepdims=True)\n\t    p = np.divide(p, n)\n\t    return p\n\tdef torch_polyhedrons(poly):\n\t    return torch.from_numpy(np_polyhedrons(poly)).to(torch.float32)\n\tclass SphericalHarmonicsEval:\n", "    \"\"\"\n\t    Inverse Spherical Harmonics Transform layer\n\t    \"\"\"\n\t    def __init__(self, base='pentakis', l_max=3, l_list=None, sph_fn=None):\n\t        self.base = base\n\t        if sph_fn is not None:\n\t            if l_list is not None:\n\t                self.l_list = l_list\n\t                self.l_max = max(l_list)\n\t            else:\n", "                self.l_list = range(l_max+1)\n\t                self.l_max = l_max\n\t            self.sph_fn = sph_fn\n\t        else:\n\t            self.sph_fn = torch_spherical_harmonics(l_max=l_max, l_list=l_list)\n\t        if isinstance(base, str):\n\t            S2 = torch_polyhedrons(self.base)\n\t        else:\n\t            S2 = base\n\t        y = self.sph_fn.compute(S2)\n", "        self.types = y.keys()\n\t        Y = []\n\t        for l in self.types:\n\t            Y.append(torch.reshape(y[l], (-1, 2*int(l)+1)))\n\t        self.Y = torch.cat(Y, dim=-1)\n\t    def compute(self, x):\n\t        X = []\n\t        for l in self.types:\n\t            X.append(x[l])\n\t        X = torch.cat(X, dim=-2)\n", "        return torch.einsum('vm,...mc->...vc', self.Y.type_as(X), X)\n\tclass SphericalHarmonicsCoeffs:\n\t    \"\"\"\n\t    Spherical Harmonics Transform layer\n\t    \"\"\"\n\t    def __init__(self, base='pentakis', l_max=3, l_list=None, sph_fn=None):\n\t        self.base = base\n\t        if l_list is not None:\n\t            self.l_list = l_list\n\t            self.l_max = max(l_list)\n", "        else:\n\t            self.l_list = list(range(l_max + 1))\n\t            self.l_max = l_max\n\t        self.split_size = []\n\t        for i in range(len(self.l_list)):\n\t            self.split_size.append(2*self.l_list[i] + 1)\n\t        if sph_fn is not None:\n\t            self.sph_fn = sph_fn\n\t        else:\n\t            self.sph_fn = torch_spherical_harmonics(l_max=l_max, l_list=l_list)\n", "        if isinstance(self.base, str):\n\t            S2 = torch_polyhedrons(self.base)\n\t        else:\n\t            S2 = self.base\n\t        y = self.sph_fn.compute(S2)\n\t        self.types = list(y.keys())\n\t        Y = []\n\t        for l in self.types:\n\t            Y.append(torch.reshape(y[l], (-1, 2*int(l)+1)))\n\t        self.Y = torch.cat(Y, dim=-1)\n", "        self.S2 = S2\n\t    def compute(self, x):\n\t        X = []\n\t        c = torch.einsum('vm,...vc->...mc', self.Y.type_as(x), x) / (self.Y.shape[0] / (4*np.pi))\n\t        c = torch.split(c, split_size_or_sections=self.split_size, dim=-2)\n\t        C = dict()\n\t        for i in range(len(self.types)):\n\t            l = self.types[i]\n\t            sl = list(x.shape)\n\t            sl[-2] = 2*int(l)+1\n", "            C[l] = torch.reshape(c[i], sl)\n\t        return C\n\t    def get_samples(self):\n\t        return self.S2\n\tif __name__==\"__main__\":\n\t    device = \"cuda:0\"\n\t    S2 = torch_fibonnacci_sphere_sampling(64).to(device)\n\t    # print(S2.shape, \"samples\", S2)\n\t    y = {}\n\t    for i in range(4):\n", "        y[str(i)] = (torch.ones((2, 16, 2*i + 1, 128)) * torch.arange(16).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)).to(device)\n\t    x = y.copy()\n\t    # Inverse Spherical Harmonics Transform\n\t    y = SphericalHarmonicsEval(l_max=3, base=S2).compute(y)\n\t    # print(y, y.shape)\n\t    # Spherical Harmonics Transform\n\t    y = SphericalHarmonicsCoeffs(l_max=3, base=S2).compute(y)\n\t    # for key in y:\n\t    #     print(y[key], y[key].shape)\n\t    pass"]}
{"filename": "cafi_net/spherical_harmonics/wigner_matrix.py", "chunked_list": ["import numpy as np\n\timport scipy\n\tfrom sympy import *\n\t# import tensorflow as tf\n\tfrom scipy import linalg, matrix, special\n\t# from spherical_harmonics.np_spherical_harmonics import complex_sh_, real_sh_, complex_to_real_sh\n\t# from sympy.physics.quantum.spin import Rotation\n\tfrom scipy.spatial.transform.rotation import Rotation\n\t\"\"\"\n\tGiven a rotation matrix R and Y_{lk} the spherical harmonics basis \n", "for l in NN and k in [|-l, l|] we have \n\tY_l( R^{-1} x) = D^l(R)Y_l(x) where D^l is the wigner matrix \n\tSee https://en.wikipedia.org/wiki/Wigner_D-matrix\n\tIn particular complex and real version of the Wigner D matrix and \n\tits relation to the Wigner d matrix\n\t\"\"\"\n\t# change of basis from real to complex spherical harmonics basis\n\tdef real_to_complex_sh(l):\n\t    C = np.zeros(shape=(2*l+1, 2*l+1), dtype=np.complex64)\n\t    c = 1./np.sqrt(2.)\n", "    for m in range(1, l+1):\n\t        C[l + m, l + m] = -1j * c\n\t        C[l + m, l - m] = c\n\t    for m in range(-l, 0):\n\t        C[l + m, l + m] = ((-1)**m)*c\n\t        C[l + m, l - m] = 1j*((-1) ** m)*c\n\t    C[l, l] = 1.\n\t    C = np.flip(C, 0)\n\t    C = np.flip(C, 1)\n\t    # print(C)\n", "    return np.asmatrix(C)\n\t# change of basis from complex to real spherical harmonics basis\n\tdef complex_to_real_sh(l):\n\t    return (real_to_complex_sh(l).conjugate()).T\n\t# rotation matrix around z axis\n\tdef z_rot(a):\n\t    c = np.cos(a)\n\t    s = np.sin(a)\n\t    return np.matrix([[c, -s, 0.], [s, c, 0.], [0., 0., 1.]])\n\t# rotation matrix around y axis\n", "def y_rot(a):\n\t    c = np.cos(a)\n\t    s = np.sin(a)\n\t    return np.matrix([[c, 0., s], [0., 1., 0.], [-s, 0., c]])\n\t# rotation matrix for Euler angles in z-y-z convention\n\tdef euler_rot_zyz(a, b ,c):\n\t    return np.matmul(np.matmul(z_rot(a), y_rot(b)), z_rot(c))\n\tdef complex_wigner_2_(a, b, c):\n\t    ea = np.exp(1j*a)\n\t    eb = np.exp(1j*b)\n", "    ec = np.exp(1j*c)\n\t    e_a = np.exp(-1j*a)\n\t    e_b = np.exp(-1j*b)\n\t    e_c = np.exp(-1j*c)\n\t    e2a = np.exp(1j*2.*a)\n\t    e2b = np.exp(1j*2.*b)\n\t    e2c = np.exp(1j*2.*c)\n\t    e_2a = np.exp(-1j*2.*a)\n\t    e_2b = np.exp(-1j*2.*b)\n\t    e_2c = np.exp(-1j*2.*c)\n", "    sa = np.imag(ea)\n\t    ca = np.real(ea)\n\t    # sb = np.imag(eb)\n\t    # cb = np.real(eb)\n\t    sb = np.sin(b)\n\t    cb = np.cos(b)\n\t    sc = np.imag(ec)\n\t    cc = np.real(ec)\n\t    # c2b = np.real(e2b)\n\t    # s2b = np.imag(e2b)\n", "    c2b = np.cos(2.*b)\n\t    s2b = np.sin(2.*b)\n\t    d22 = ((1+cb)*(1.+cb))/4.\n\t    d21 = -sb*(1.+cb)/2.\n\t    d20 = np.sqrt(3./8.)*sb*sb\n\t    d2_1 = -sb*(1.-cb)/2.\n\t    d2_2 = (1.-cb)*(1.-cb)/4.\n\t    d11 = (2.*cb*cb+cb-1.)/2.\n\t    d10 = -np.sqrt(3./8.)*s2b\n\t    d1_1 = (-2.*cb*cb+cb+1.)/2.\n", "    d00 = (3.*cb*cb-1.)/2.\n\t    d = np.asmatrix([[d22, -d21, d20, -d2_1, d2_2],\n\t                     [d21, d11, -d10, d1_1, -d2_1],\n\t                     [d20, d10, d00, -d10, d20],\n\t                     [d2_1, d1_1, d10, d11, -d21],\n\t                     [d2_2, d2_1, d20, d21, d22]], dtype=np.complex64)\n\t    # debug d\n\t    d = d.T\n\t    \"\"\"\n\t    for i in range(-2, 3):\n", "        for j in range(-2, 3):\n\t            print( str(i) + ' ' + str(j))\n\t            print(str(np.real(d[i+2, j+2])) + ' ' + str(((-1.)**(j-i))*np.real(d[j+2, i+2])) + ' ' + str(np.real(d[2-j, 2-i])))\n\t    \"\"\"\n\t    Ea = np.asmatrix([[e_2a, 0., 0., 0., 0.],\n\t                      [0., e_a, 0., 0., 0.],\n\t                      [0., 0., 1., 0., 0.],\n\t                      [0., 0., 0., ea, 0.],\n\t                      [0., 0., 0., 0., e2a]], dtype=np.complex64)\n\t    Ec = np.asmatrix([[e_2c, 0., 0., 0., 0.],\n", "                      [0., e_c, 0., 0., 0.],\n\t                      [0., 0., 1., 0., 0.],\n\t                      [0., 0., 0., ec, 0.],\n\t                      [0., 0., 0., 0., e2c]], dtype=np.complex64)\n\t    \"\"\"\n\t    Ea = np.asmatrix([[e2a, 0., 0., 0., 0.],\n\t                      [0., ea, 0., 0., 0.],\n\t                      [0., 0., 1., 0., 0.],\n\t                      [0., 0., 0., e_a, 0.],\n\t                      [0., 0., 0., 0., e_2a]], dtype=np.complex64)\n", "    Ec = np.asmatrix([[e2c, 0., 0., 0., 0.],\n\t                      [0., ec, 0., 0., 0.],\n\t                      [0., 0., 1., 0., 0.],\n\t                      [0., 0., 0., e_c, 0.],\n\t                      [0., 0., 0., 0., e_2c]], dtype=np.complex64)\n\t    \"\"\"\n\t    return np.matmul(np.matmul(Ea, d), Ec)\n\tdef complex_wigner_1_(a, b, c):\n\t    cb = np.cos(b)\n\t    sb = np.sin(b)\n", "    ea = np.exp(1j * a)\n\t    ec = np.exp(1j * c)\n\t    e_a = np.exp(-1j * a)\n\t    e_c = np.exp(-1j * c)\n\t    d11 = (1.+cb)/2.\n\t    d10 = -sb/(np.sqrt(2.))\n\t    d1_1 = (1.-cb)/2.\n\t    d00 = cb\n\t    d = np.asmatrix([[d11, -d10, d1_1],\n\t                     [d10, d00, -d10],\n", "                     [d1_1, d10, d11]], dtype=np.complex64)\n\t    d = d.T\n\t    Ea = np.asmatrix([[e_a, 0., 0.],\n\t                     [0., 1., 0.],\n\t                     [0., 0., ea]], dtype=np.complex64)\n\t    Ec = np.asmatrix([[e_c, 0., 0.],\n\t                     [0., 1., 0.],\n\t                     [0., 0., ec]], dtype=np.complex64)\n\t    return np.matmul(np.matmul(Ea, d), Ec)\n\tdef complex_wigner_(l, a, b, c):\n", "    assert (l == 0 or l == 1 or l == 2)\n\t    if l == 0:\n\t        return np.asmatrix([[1.]], dtype=np.complex64)\n\t    if l == 1:\n\t        return complex_wigner_1_(a, b, c)\n\t    if l == 2:\n\t        return complex_wigner_2_(a, b, c)\n\t\"\"\"\n\tcompute the coefficient d^l_{jk} of the wigner d matrix encoding the action of a rotation of angle b \n\taround the y axis on the real (and complex) spherical harmonic basis of degree l\n", "\"\"\"\n\tdef wigner_d_matrix_coeffs(l, j, k, b):\n\t    p = np.math.factorial(l+j)*np.math.factorial(l-j)*np.math.factorial(l+k)*np.math.factorial(l-k)\n\t    p = np.sqrt(p)\n\t    # l + k - s >= 0\n\t    # s >= 0\n\t    # j - k + s >= 0\n\t    # l - j - s >= 0\n\t    # l + k >= s\n\t    # s >= 0\n", "    # s >= k - j\n\t    # l - j >= s\n\t    s1 = np.max([0, k-j])\n\t    s2 = np.min([l+k, l-j])\n\t    s_ = np.sin(b/2.)\n\t    c_ = np.cos(b/2.)\n\t    d = 0.\n\t    for s in range(s1, s2+1):\n\t        q = np.math.factorial(l+k-s)*np.math.factorial(s)*np.math.factorial(j-k+s)*np.math.factorial(l-j-s)\n\t        x = (1.*p)/(1.*q)\n", "        x *= (-1)**(j-k+s)\n\t        x *= (c_**(2*l+k-j-2*s))*(s_**(j-k+2*s))\n\t        d += x\n\t    return d\n\t\"\"\"\n\tcompute the wigner d matrix d^l encoding the action of a rotation of angle b \n\taround the y axis on the real (and complex) spherical harmonic basis of degree l\n\t\"\"\"\n\tdef wigner_d_matrix(l, b, dtype=np.float32):\n\t    d = np.zeros(shape=(2*l+1, 2*l+1), dtype=dtype)\n", "    \"\"\"\n\t    for m in range((2*l+1)*(2*l+1)):\n\t        k = m % (2*l+1)\n\t        j = np.int((m - k) / (2*l+1))\n\t        d[j, k] = wigner_d_matrix_coeffs(l, j-l, k-l, b)\n\t    \"\"\"\n\t    for j in range(2*l+1):\n\t        for k in range(2*l+1):\n\t            d[j, k] = wigner_d_matrix_coeffs(l, j-l, k-l, b)\n\t    return np.asmatrix(d)\n", "\"\"\"\n\tcompute the action of rotation of angle a around the z axis on the \n\tcomplex spherical harmonic basis of degree l\n\t\"\"\"\n\tdef diag_exp(l, a):\n\t    e = np.zeros(shape=(2*l+1, 2*l+1), dtype=np.complex64)\n\t    for m in range(l+1):\n\t        e[m + l, m + l] = np.exp(m * 1j * a)\n\t        e[m, m] = np.exp((m - l) * 1j * a)\n\t    return np.asmatrix(e)\n", "\"\"\"\n\tdef complex_D_wigner(l, a, b, c):\n\t    D = diag_exp(l, a)*wigner_d_matrix(l, b, dtype=np.complex64)*diag_exp(l, c)\n\t    return np.conjugate(D)\n\t\"\"\"\n\tdef complex_D_wigner(l, a, b, c):\n\t    d = wigner_d_matrix(l, b, dtype=np.complex64)\n\t    # ea = diag_exp(l, a)\n\t    # ec = diag_exp(l, c)\n\t    # D = np.matmul(np.matmul(ea, d), ec)\n", "    D = d\n\t    # print(d)\n\t    for p in range(2*l+1):\n\t        for q in range(2*l+1):\n\t            # D[q, p] *= np.exp(-(p-l)*1j*a)*np.exp(-(q-l)*1j*c)\n\t            D[p, q] *= np.exp(-(p - l) * 1j * a) * np.exp(-(q - l) * 1j * c)\n\t    # np.conjugate(D)\n\t    # print(D)\n\t    # D = np.flip(D, axis=0)\n\t    # D = np.flip(D, axis=1)\n", "    # D = np.conjugate(D)\n\t    return D\n\tdef real_D_wigner(l, a, b, c):\n\t    C = complex_to_real_sh(l)\n\t    D = complex_D_wigner(l, a, b, c)\n\t    D = np.real(C*D*np.conjugate(C.T))\n\t    \"\"\"\n\t    Da = complex_D_wigner(l, a, 0., 0.)\n\t    Da = np.real(C * Da * np.conjugate(C.T))\n\t    Db = complex_D_wigner(l, 0., b, 0.)\n", "    Db = np.real(C * Db * np.conjugate(C.T))\n\t    Dc = complex_D_wigner(l, 0., 0., c)\n\t    Dc = np.real(C * Dc * np.conjugate(C.T))\n\t    \"\"\"\n\t    # return np.conjugate(C.T)*D*C\n\t    # return np.real(C*D*np.conjugate(C.T))\n\t    # return np.real(Da*Db*Dc)\n\t    return D\n\t\"\"\"\n\treturn a list of real wigner matrices D^l for l in [|0, l_max|]\n", "\"\"\"\n\tdef real_D_wigner_from_euler(l_max, a, b, c):\n\t    D = np.zeros(((l_max+1)**2, (l_max+1)**2))\n\t    k = 0\n\t    for l in range(l_max+1):\n\t        D[k:k+(2*l+1), k:k+(2*l+1)] = real_D_wigner(l, a, b, c)\n\t        k += 2*l+1\n\t    return D\n\t\"\"\"\n\treturn a list of real wigner matrices D^l for l in [|0, l_max|]\n", "parametrized by a quaternion q\n\t\"\"\"\n\tdef real_D_wigner_from_quaternion(l_max, q):\n\t    r = Rotation(q)\n\t    euler = r.as_euler('zyz')\n\t    return real_D_wigner_from_euler(l_max, euler[0], euler[1], euler[2])\n\t# to debug\n\t\"\"\"\n\tdef wigner_d_matrix_(l, k1, k2, b):\n\t    k = np.min([l + k2, l - k2, l + k1, l - k1])\n", "    a = 0\n\t    lbd = 0\n\t    if k == l + k2:\n\t        a = k1 - k2\n\t        lbd = k1 - k2\n\t    if k == l - k2:\n\t        a = k2 - k1\n\t        lbd = 0\n\t    if k == l + k1:\n\t        a = k2 - k1\n", "        lbd = 0\n\t    if k == l - k1:\n\t        a = k1 - k2\n\t        lbd = k1 - k2\n\t    s_ = np.sin(b/2.)\n\t    c_ = np.sin(b/2.)\n\t    c = np.cos(b)\n\t    b = 2*(l-k)-a\n\t    d = scipy.special.jacobi(a, b, k)(c)\n\t    d *= (s_**a)*(c_**b)\n", "    d *= np.sqrt(scipy.special.binom(2*l-k, k+a)/scipy.special.binom(k+b, b))\n\t    d *= (-1)**lbd\n\t    return d\n\tdef wigner_d_matrix(l, b, dtype=np.float32):\n\t    d = np.zeros(shape=(2*l+1, 2*l+1), dtype=dtype)\n\t    for k1 in range(-l, l+1):\n\t        for k2 in range(-l, l+1):\n\t            d[k1+l, k2+l] = wigner_d_matrix_(l, k1, k2, b)\n\t    return d\n\tdef wigner_D_matrix(l, a, b, c):\n", "    D = np.asmatrix(wigner_d_matrix(l, b, dtype=np.complex64))\n\t    for k1 in range(-l, l+1):\n\t        D[k1, :] = np.exp(-k1*1j*a)*D[k1, :]\n\t    for k2 in range(-l, l+1):\n\t        D[:, k2] = np.exp(-k2 * 1j * c) * D[:, k2]\n\t    return D\n\t\"\"\"\n\t\"\"\"\n\tdef unit_test3():\n\t    angles = np.random.rand(3)\n", "    # angles[0] = 0.0\n\t    # angles[1] = 0.0\n\t    # angles[2] = angles[0]\n\t    # angles = [0., np.pi/2., 0.]\n\t    # angles = [0., 1., 0.]\n\t    R = euler_rot_zyz(angles[0], angles[1], angles[2])\n\t    print(R)\n\t    X = np.asmatrix(np.random.rand(1, 3))\n\t    X_rot = (np.matmul(R, X.T)).T\n\t    D = complex_wigner_2_(angles[0], angles[1], angles[2])\n", "    # D = wigner_D_matrix(2, angles[0], angles[1], angles[2])\n\t    print(angles[0])\n\t    print(angles[1])\n\t    print(np.exp(1j*angles[2]))\n\t    print(D*D.T)\n\t    print('orthogonality')\n\t    print(np.linalg.norm(D * (D.conjugate()).T-np.eye(5)))\n\t    # print(np.linalg.norm(D_bis * (D_bis.conjugate()).T - np.eye(5)))\n\t    sh = complex_sh_(2, X)\n\t    sh_rot = complex_sh_(2, X_rot)\n", "    D_inv = (D.conjugate()).T\n\t    y = np.matmul(D, sh.T) - sh_rot.T\n\t    print(np.linalg.norm(y))\n\t    print(np.real(y))\n\t    print(np.real(sh-sh_rot))\n\t    print(np.imag(y))\n\t    print(np.imag(sh - sh_rot))\n\t    # print(complex_sh_2_(np.asmatrix([[0., 1., 0.]])))\n\t    # print(sh_rot)\n\t    # print(np.sqrt(np.multiply(np.real(y), np.real(y)) + np.multiply(np.imag(y), np.imag(y))))\n", "    # print(y)\n\t    # print(D+D.conjugate())\n\t\"\"\"\n\tdef complex_wigner_matrix_unit_test_(l, a, b, c, X):\n\t    # angles = np.random.rand(3)\n\t    # a = angles[0]\n\t    # b = angles[1]\n\t    # c = angles[2]\n\t    D = complex_D_wigner(l, a, b, c)\n\t    # D_ = complex_wigner_(l, a, b, c)\n", "    # D_ = np.conjugate(D_.T)\n\t    # print('u')\n\t    # print(np.linalg.norm(D-D_))\n\t    R = euler_rot_zyz(a, b, c)\n\t    X_rot = (np.matmul(R.T, X.T)).T\n\t    Y = complex_sh_(l, X)\n\t    Y_rot = complex_sh_(l, X_rot)\n\t    y = np.matmul(D, Y) - Y_rot\n\t    # print(np.linalg.norm(Y))\n\t    print(np.linalg.norm(y))\n", "def complex_wigner_matrix_unit_test():\n\t    for i in range(10):\n\t        angles = np.random.rand(3)\n\t        a = angles[0]\n\t        b = angles[1]\n\t        c = angles[2]\n\t        # a = 0.\n\t        # b = 0.\n\t        # c = 0.\n\t        X = np.asmatrix(np.random.rand(1, 3))\n", "        print('wigner D test ', i)\n\t        for l in range(4):\n\t            print('l = ', l)\n\t            complex_wigner_matrix_unit_test_(l, a, b, c, X)\n\tdef real_wigner_matrix_unit_test_(l, a, b, c, X):\n\t    # angles = np.random.rand(3)\n\t    # a = angles[0]\n\t    # b = angles[1]\n\t    # c = angles[2]\n\t    D = real_D_wigner(l, a, b, c)\n", "    # D_ = complex_wigner_(l, a, b, c)\n\t    # D_ = np.conjugate(D_.T)\n\t    # print('u')\n\t    # print(np.linalg.norm(D-D_))\n\t    R = euler_rot_zyz(a, b, c)\n\t    X_rot = (np.matmul(R.T, X.T)).T\n\t    Y = real_sh_(l, X)\n\t    Y_rot = real_sh_(l, X_rot)\n\t    y = np.matmul(D, Y) - Y_rot\n\t    # print(np.linalg.norm(Y))\n", "    print(np.linalg.norm(y))\n\tdef real_wigner_matrix_unit_test():\n\t    for i in range(10):\n\t        angles = np.random.rand(3)\n\t        a = angles[0]\n\t        b = angles[1]\n\t        c = angles[2]\n\t        # a = 0.\n\t        # b = 0.\n\t        # c = 0.\n", "        X = np.asmatrix(np.random.rand(1, 3))\n\t        print('wigner D test ', i)\n\t        for l in range(4):\n\t            print('l = ', l)\n\t            real_wigner_matrix_unit_test_(l, a, b, c, X)\n"]}
{"filename": "cafi_net/spherical_harmonics/kernels_density.py", "chunked_list": ["from spherical_harmonics.kernels import *\n\tfrom utils.pointcloud_utils import GroupPoints_grid, GroupPoints_density\n\tclass SphericalHarmonicsGaussianKernels_density(torch.nn.Module):\n\t    def __init__(self, l_max, gaussian_scale, num_shells, transpose=False, bound=True):\n\t        super(SphericalHarmonicsGaussianKernels_density, self).__init__()\n\t        self.l_max = l_max\n\t        self.monoms_idx = torch_monomial_basis_3D_idx(l_max)\n\t        self.gaussian_scale = gaussian_scale\n\t        self.num_shells = num_shells\n\t        self.transpose = True\n", "        self.Y = torch_spherical_harmonics_basis(l_max, concat=True)\n\t        self.split_size = []\n\t        self.sh_idx = []\n\t        self.bound = bound\n\t        for l in range(l_max + 1):\n\t            self.split_size.append(2*l+1)\n\t            self.sh_idx += [l]*(2*l+1)\n\t        self.sh_idx = torch.from_numpy(np.array(self.sh_idx)).to(torch.int64)\n\t    def forward(self, x):\n\t        #target_density = x[\"target_density\"]\n", "        #target_density = target_density.squeeze(-1)\n\t        #target_density[target_density > 0.] = 1\n\t        #target_density[target_density <= 0.]  = 0\n\t        if \"patches dist\" in x:\n\t            patches_dist = x[\"patches dist\"].unsqueeze(-1)\n\t        else:\n\t            patches_dist = torch.linalg.norm(x[\"patches\"], dim=-1, keepdims=True)\n\t        normalized_patches = x[\"patches\"] / torch.maximum(patches_dist, torch.tensor(0.000001).type_as(x[\"patches\"]))\n\t        if self.transpose:\n\t            normalized_patches = -normalized_patches\n", "        # print(normalized_patches.shape)\n\t        monoms_patches = torch_eval_monom_basis(normalized_patches, self.l_max, idx=self.monoms_idx)\n\t        # print(self.Y.shape)\n\t        #x[\"patches_density\"][target_density == 0, ...] = 0.\n\t        #sh_patches = x[\"patches_density\"] * torch.einsum('ij,bvpj->bvpi', self.Y.type_as(monoms_patches), monoms_patches)\n\t        sh_patches =  torch.einsum('ij,bvpj->bvpi', self.Y.type_as(monoms_patches), monoms_patches)\n\t        # print(sh_patches.shape)\n\t        #return sh_patches\n\t        shells_rad = torch.arange(self.num_shells).type_as(monoms_patches) / (self.num_shells-1)\n\t        shells_rad = torch.reshape(shells_rad, (1, 1, 1, -1))\n", "        shells = patches_dist - shells_rad\n\t        shells = torch.exp(-self.gaussian_scale*(shells * shells))\n\t        shells_sum = torch.sum(shells, dim=-1, keepdims=True)\n\t        shells = (shells / torch.maximum(shells_sum, torch.tensor(0.000001).type_as(shells)))\n\t        shells = shells.unsqueeze(-2)\n\t        if self.bound:\n\t            shells = torch.where(patches_dist.unsqueeze(-1) <= torch.tensor(1.).type_as(shells), shells, torch.tensor(0.).type_as(shells))\n\t        sh_patches = sh_patches.unsqueeze(-1)\n\t        sh_patches = shells * sh_patches\n\t        # L2 norm\n", "        l2_norm = torch.sum((sh_patches * sh_patches), dim=2, keepdims=True)\n\t        l2_norm = torch.split(l2_norm, split_size_or_sections=self.split_size, dim=-2)\n\t        Y = []\n\t        for l in range(len(l2_norm)):\n\t            ml = torch.sum(l2_norm[l], dim=-2, keepdims=True)\n\t            ml = torch.sqrt(ml + 1e-7)\n\t            Y.append(ml)\n\t        l2_norm = torch.cat(Y, dim=-2)\n\t        l2_norm = torch.mean(l2_norm, dim=1, keepdims=True)\n\t        l2_norm = torch.maximum(l2_norm, torch.tensor(1e-8).type_as(l2_norm))\n", "        # print(l2_norm.shape)\n\t        l2_norm = l2_norm[..., self.sh_idx, :]\n\t        sh_patches = (sh_patches / (l2_norm + 1e-6))\n\t        return sh_patches\n\tclass ShGaussianKernelConv_grid(torch.nn.Module):\n\t    def __init__(self, l_max, l_max_out=None, transpose=False, num_source_points=None):\n\t        super(ShGaussianKernelConv_grid, self).__init__()\n\t        self.l_max = l_max\n\t        self.split_size = []\n\t        for l in range(l_max + 1):\n", "            self.split_size.append(2 * l + 1)\n\t        # self.output_type = output_type\n\t        self.l_max_out = l_max_out\n\t        # self.transpose = transpose\n\t        self.num_source_points = num_source_points\n\t        self.Q = torch_clebsch_gordan_decomposition(l_max=max(l_max_out, l_max),\n\t                                                 sparse=False,\n\t                                                 output_type='dict',\n\t                                                 l_max_out=l_max_out)\n\t    def forward(self, x):\n", "        assert (isinstance(x, dict))\n\t        signal = []\n\t        features_type = []\n\t        channels_split_size = []\n\t        for l in x:\n\t            if l.isnumeric():\n\t                features_type.append(int(l))\n\t                channels_split_size.append(x[l].shape[-2] * x[l].shape[-1])\n\t                signal.append(torch.reshape(x[l], (x[l].shape[0], x[l].shape[1], -1)))\n\t        signal = torch.cat(signal, dim=-1)\n", "        batch_size = signal.shape[0]\n\t        patch_size = x[\"kernels\"].shape[2]\n\t        num_shells = x[\"kernels\"].shape[-1]\n\t        # Changed and removed transpose here\n\t        if \"patches idx\" in x:\n\t            # print(signal.shape, \"signal\")\n\t            B, N, _ = signal.shape\n\t            H = int(np.cbrt(N))\n\t            # print(x[\"patches idx\"].shape, \"idx\")\n\t            B2, N_s, K, _ = x[\"patches idx\"].shape\n", "            H_s = int(np.cbrt(N_s))\n\t            signal = signal.reshape(B, H, H, H, -1).permute(0, 4, 1, 2, 3)\n\t            id_sample = x[\"patches idx\"]\n\t            id_sample = id_sample.permute((0, -1, 1, 2))\n\t            id_sample = id_sample.reshape(B, -1, H_s, H_s, H_s*K)\n\t            id_sample = id_sample.permute((0, 2, 3, 4, 1))\n\t            signal = torch.nn.functional.grid_sample(signal, id_sample, align_corners = True).reshape(B, -1, H_s, H_s, H_s, K).permute(0, 2, 3, 4, 5, 1)\n\t            # print(\"patches mask found\") \n\t            if \"patches mask\" in x:\n\t                # print(\"patches mask found\") \n", "                mask_patches = x[\"patches mask\"]\n\t                signal[mask_patches, :] = 0 \n\t            signal = signal.reshape(B, H_s*H_s*H_s, K, -1)\n\t        num_points_target = signal.shape[1]\n\t        kernels = torch.reshape(x[\"kernels\"], (batch_size, num_points_target, patch_size, -1))\n\t        y = torch.einsum('bvpy,bvpc->bvyc', kernels, signal)\n\t        # split y\n\t        # print(channels_split_size, y.shape)\n\t        y_ = torch.split(y, split_size_or_sections=channels_split_size, dim=-1)\n\t        y = {str(j): [] for j in range(self.l_max_out + 1)}\n", "        y_cg = []\n\t        for i in range(len(channels_split_size)):\n\t            l = features_type[i]\n\t            yi = torch.reshape(y_[i], (batch_size, num_points_target, -1, num_shells, 2 * l + 1, x[str(l)].shape[-1]))\n\t            yi = yi.permute(0, 1, 2, 4, 3, 5)\n\t            yi = torch.reshape(yi, (batch_size, num_points_target, -1, 2 * l + 1, num_shells*x[str(l)].shape[-1]))\n\t            yi = torch.split(yi, split_size_or_sections=self.split_size, dim=2)\n\t            for j in range(len(self.split_size)):\n\t                yij = yi[j]\n\t                if l == 0:\n", "                    y[str(j)].append(yij[:, :, :, 0, :])\n\t                elif j == 0:\n\t                    y[str(l)].append(yij[:, :, 0, :, :])\n\t                else:\n\t                    y_cg.append(yij)\n\t        y_cg = self.Q.decompose(y_cg)\n\t        for J in y_cg:\n\t            if J not in y:\n\t                y[J] = []\n\t            y[J].append(y_cg[J])\n", "        for J in y:\n\t            y[J] = torch.cat(y[J], dim=-1)\n\t        return y\n\tif __name__==\"__main__\":\n\t    gi = GroupPoints_density(0.4, 32)\n\t    in_dict = {}\n\t    x = torch.randn(2, 32 *32 *32, 3).cuda()\n\t    y = torch.randn(2, 16 *16 *16, 3).cuda()\n\t    x_d = torch.randn(2, 32 *32 *32, 1).cuda()\n\t    y_d = torch.randn(2, 16 *16 *16, 1).cuda()\n", "    in_dict[\"source points\"] = x\n\t    in_dict[\"target points\"] = y\n\t    in_dict[\"source density\"] = x_d\n\t    in_dict[\"target density\"] = y_d\n\t    x2 = y\n\t    out = gi(in_dict)\n\t    for key in out:\n\t        print(key, \" \", out[key].shape)\n\t    B = x.shape[0]\n\t    H, W, D= 32, 32, 32\n", "    K = out[\"patches source\"].shape[-2]\n\t    # B, H, W, D, K, _ = out[\"patches source\"].shape\n\t    k = SphericalHarmonicsGaussianKernels_density(l_max = 3, gaussian_scale = 0.1, num_shells = 3, bound = True).cuda()\n\t    print(out[\"patches source\"].shape)\n\t    out_2 = k({\"patches\": out[\"patches source\"].reshape(B, -1, K, 3), \"patches dist\": out[\"patches dist source\"].reshape(B, -1, K), \"patches_density\": out[\"patches source density\"].reshape(B, -1, K, 1)}) # B, H*W*D, K, 16, 3\n\t    conv_layer = ShGaussianKernelConv_grid(l_max=3, l_max_out=3).cuda()\n\t    y = {}\n\t    x2 = x2.reshape(B, -1, 3)\n\t    y[\"source points\"] = x.reshape(B, -1, 3)\n\t    y[\"target points\"] = x2\n", "    y[\"patches idx\"] = out[\"patches idx source\"].reshape(B, -1, K, 3)\n\t    y[\"patches dist source\"] = out[\"patches dist source\"].reshape(B, -1, K)\n\t    y[\"kernels\"] = out_2\n\t    # w = gauss_normalization(y[\"patches dist source\"], 1./3.)\n\t    if '1' in y:\n\t        y['1'] = torch.cat([y['1'], x2], dim=-1)\n\t    else:\n\t        y['1'] = x2.unsqueeze(-1)\n\t    y = conv_layer(y)\n\t    # for key in y:\n", "    #     print(y[key], \" \", key, \" \", y[key].shape)\n"]}
{"filename": "cafi_net/spherical_harmonics/kernels.py", "chunked_list": ["from sympy import *\n\timport numpy as np\n\timport torch, h5py\n\tfrom utils.group_points import gather_idx, GroupPoints\n\tfrom spherical_harmonics.clebsch_gordan_decomposition import torch_clebsch_gordan_decomposition\n\tdef associated_legendre_polynomial(l, m, z, r2):\n\t    P = 0\n\t    if l < m:\n\t        return P\n\t    for k in range(int((l-m)/2)+1):\n", "        pk = (-1.)**k * (2.)**(-l) * binomial(l, k) * binomial(2*l-2*k, l)\n\t        pk *= (factorial(l-2*k) / factorial(l-2*k-m)) * r2**k * z**(l-2*k-m)\n\t        P += pk\n\t    P *= np.sqrt(float(factorial(l-m)/factorial(l+m)))\n\t    return P\n\tdef A(m, x, y):\n\t    a = 0\n\t    for p in range(m+1):\n\t        a += binomial(m, p) * x**p * y**(m-p) * cos((m-p)*(pi/2.))\n\t    return a\n", "def B(m, x, y):\n\t    b = 0\n\t    for p in range(m+1):\n\t        b += binomial(m, p) * x**p * y**(m-p) * sin((m-p)*(pi/2.))\n\t    return b\n\t\"\"\"\n\tcomputes the unnormalized real spherical harmonic Y_{lm} as a polynomial\n\tof the euclidean coordinates x, y, z\n\t\"\"\"\n\tdef real_spherical_harmonic(l, m, x, y, z, poly = False):\n", "    K = np.sqrt((2*l+1)/(2*np.pi))\n\t    r2 = x**2 + y**2 + z**2\n\t    if m > 0:\n\t        Ylm = K * associated_legendre_polynomial(l, m, z, r2) * A(m, x, y)\n\t    elif m < 0:\n\t        Ylm = K * associated_legendre_polynomial(l, -m, z, r2) * B(-m, x, y)\n\t    else:\n\t        K = np.sqrt((2 * l + 1) / (4 * np.pi))\n\t        Ylm = K * associated_legendre_polynomial(l, 0, z, r2)\n\t    if poly:\n", "        Ylm = Poly(simplify(expand(Ylm)), x, y, z)\n\t    return Ylm\n\tdef binom(n, k):\n\t    if k == 0.:\n\t        return 1.\n\t    return gamma(n + 1) / (gamma(n-k+1)*gamma(k+1))\n\t\"\"\"\n\tcomputes radial Zernike polynomials (divided by r^{2l})\n\t\"\"\"\n\tdef zernike_polynomial_radial(n, l, D, r2):\n", "    if (l > n):\n\t        return 0\n\t    if ((n-l) % 2 != 0):\n\t        return 0\n\t    R = 0\n\t    for s in range(int((n-l) / 2) + 1):\n\t        R += (-1)**s * binom((n-l)/2, s)*binom(s-1+(n+l+D)/2., (n-l)/2)*r2**s\n\t    R *= (-1)**((n-l)/2)*np.sqrt(2*n+D)\n\t    return R\n\t\"\"\"\n", "computes the 3D Zernike polynomials.\n\t\"\"\"\n\tdef zernike_kernel_3D(n, l, m, x, y, z):\n\t    r2 = x**2 + y**2 + z**2\n\t    return zernike_polynomial_radial(n, l, 3, r2)*real_spherical_harmonic(l, m, x, y, z)\n\t    # return real_spherical_harmonic(l, m, x, y, z)\n\t\"\"\"\n\tcomputes the monomial basis in x, y, z up to degree d\n\t\"\"\"\n\tdef monomial_basis_3D(d):\n", "    monoms_basis = []\n\t    for I in range((d + 1) ** 3):\n\t        i = I % (d + 1)\n\t        a = int((I - i) / (d + 1))\n\t        j = a % (d + 1)\n\t        k = int((a - j) / (d + 1))\n\t        if (i + j + k <= d):\n\t            monoms_basis.append((i, j, k))\n\t    monoms_basis = list(set(monoms_basis))\n\t    monoms_basis = sorted(monoms_basis)\n", "    return monoms_basis\n\tdef torch_monomial_basis_3D_idx(d):\n\t    m = monomial_basis_3D(d)\n\t    idx = np.zeros((len(m), 3), dtype=np.int32)\n\t    for i in range(len(m)):\n\t        for j in range(3):\n\t            idx[i, j] = m[i][j]\n\t    return torch.from_numpy(idx).to(torch.int64)\n\t\"\"\"\n\tcomputes the coefficients of a list of polynomials in the monomial basis\n", "\"\"\"\n\tdef np_monomial_basis_coeffs(polynomials, monoms_basis):\n\t    n_ = len(monoms_basis)\n\t    m_ = len(polynomials)\n\t    M = np.zeros((m_, n_))\n\t    for i in range(m_):\n\t        for j in range(n_):\n\t            M[i, j] = polynomials[i].coeff_monomial(monoms_basis[j])\n\t    return M\n\tclass ShGaussianKernelConv(torch.nn.Module):\n", "    def __init__(self, l_max, l_max_out=None, transpose=False, num_source_points=None):\n\t        super(ShGaussianKernelConv, self).__init__()\n\t        self.l_max = l_max\n\t        self.split_size = []\n\t        for l in range(l_max + 1):\n\t            self.split_size.append(2 * l + 1)\n\t        # self.output_type = output_type\n\t        self.l_max_out = l_max_out\n\t        # self.transpose = transpose\n\t        self.num_source_points = num_source_points\n", "        self.Q = torch_clebsch_gordan_decomposition(l_max=max(l_max_out, l_max),\n\t                                                 sparse=False,\n\t                                                 output_type='dict',\n\t                                                 l_max_out=l_max_out)\n\t    def forward(self, x):\n\t        assert (isinstance(x, dict))\n\t        signal = []\n\t        features_type = []\n\t        channels_split_size = []\n\t        for l in x:\n", "            if l.isnumeric():\n\t                features_type.append(int(l))\n\t                channels_split_size.append(x[l].shape[-2] * x[l].shape[-1])\n\t                signal.append(torch.reshape(x[l], (x[l].shape[0], x[l].shape[1], -1)))\n\t        signal = torch.cat(signal, dim=-1)\n\t        batch_size = signal.shape[0]\n\t        patch_size = x[\"kernels\"].shape[2]\n\t        num_shells = x[\"kernels\"].shape[-1]\n\t        # Changed and removed transpose here\n\t        if \"patches idx\" in x:\n", "            # print(signal.shape, \"signal shape\", x[\"patches idx\"].shape) B, N, 3\n\t            signal = gather_idx(signal, x[\"patches idx\"])\n\t            # print(signal.shape)\n\t        num_points_target = signal.shape[1]\n\t        kernels = torch.reshape(x[\"kernels\"], (batch_size, num_points_target, patch_size, -1))\n\t        y = torch.einsum('bvpy,bvpc->bvyc', kernels, signal)\n\t        # split y\n\t        # print(channels_split_size, y.shape)\n\t        y_ = torch.split(y, split_size_or_sections=channels_split_size, dim=-1)\n\t        y = {str(j): [] for j in range(self.l_max_out + 1)}\n", "        y_cg = []\n\t        for i in range(len(channels_split_size)):\n\t            l = features_type[i]\n\t            yi = torch.reshape(y_[i], (batch_size, num_points_target, -1, num_shells, 2 * l + 1, x[str(l)].shape[-1]))\n\t            yi = yi.permute(0, 1, 2, 4, 3, 5)\n\t            yi = torch.reshape(yi, (batch_size, num_points_target, -1, 2 * l + 1, num_shells*x[str(l)].shape[-1]))\n\t            yi = torch.split(yi, split_size_or_sections=self.split_size, dim=2)\n\t            for j in range(len(self.split_size)):\n\t                yij = yi[j]\n\t                if l == 0:\n", "                    y[str(j)].append(yij[:, :, :, 0, :])\n\t                elif j == 0:\n\t                    y[str(l)].append(yij[:, :, 0, :, :])\n\t                else:\n\t                    y_cg.append(yij)\n\t        y_cg = self.Q.decompose(y_cg)\n\t        for J in y_cg:\n\t            if J not in y:\n\t                y[J] = []\n\t            y[J].append(y_cg[J])\n", "        for J in y:\n\t            y[J] = torch.cat(y[J], dim=-1)\n\t        return y\n\t\"\"\"\n\tcomputes the coefficients of the spherical harmonics in the monomial basis\n\t\"\"\"\n\tdef spherical_harmonics_3D_monomial_basis(l, monoms_basis):\n\t    x, y, z = symbols(\"x y z\")\n\t    n_ = len(monoms_basis)\n\t    M = np.zeros((2*l+1, n_))\n", "    for m in range(2*l+1):\n\t        Y = real_spherical_harmonic(l, m-l, x, y, z)\n\t        Y = expand(Y)\n\t        Y = poly(Y, x, y, z)\n\t        for i in range(n_):\n\t            M[m, i] = N(Y.coeff_monomial(monoms_basis[i]))\n\t    return M\n\t\"\"\"\n\tcomputes the coefficients of the Zernike polynomials in the monomial basis\n\t\"\"\"\n", "def zernike_kernel_3D_monomial_basis(n, l, monoms_basis):\n\t    x, y, z = symbols(\"x y z\")\n\t    n_ = len(monoms_basis)\n\t    M = np.zeros((2*l+1, n_))\n\t    for m in range(2*l+1):\n\t        Z = zernike_kernel_3D(n, l, m-l, x, y, z)\n\t        Z = expand(Z)\n\t        Z = poly(Z, x, y, z)\n\t        for i in range(n_):\n\t            M[m, i] = N(Z.coeff_monomial(monoms_basis[i]))\n", "    return M\n\t\"\"\"\n\tcomputes the matrix of an offset in the monomial basis (up to degree d)\n\t(m_1(x-a), ..., m_k(x-a)) = A(a).(m_1(x), ..., m_k(x))\n\t\"\"\"\n\tdef np_monom_basis_offset(d):\n\t    monoms_basis = monomial_basis_3D(d)\n\t    n = len(monoms_basis)\n\t    idx = np.full(fill_value=-1, shape=(n, n), dtype=np.int32)\n\t    coeffs = np.zeros(shape=(n, n))\n", "    for i in range(n):\n\t        pi = monoms_basis[i][0]\n\t        qi = monoms_basis[i][1]\n\t        ri = monoms_basis[i][2]\n\t        for j in range(n):\n\t            pj = monoms_basis[j][0]\n\t            qj = monoms_basis[j][1]\n\t            rj = monoms_basis[j][2]\n\t            if (pj >= pi) and (qj >= qi) and (rj >= ri):\n\t                idx[j, i] = monoms_basis.index((pj-pi, qj-qi, rj-ri))\n", "                coeffs[j, i] = binomial(pj, pi)*binomial(qj, qi)*binomial(rj, ri)*((-1.)**(pj-pi+qj-qi+rj-ri))\n\t    return coeffs, idx\n\t\"\"\"\n\tcomputes the 3D zernike basis up to degree d\n\t\"\"\"\n\tdef np_zernike_kernel_basis(d):\n\t    monoms_basis = monomial_basis_3D(d)\n\t    Z = []\n\t    for l in range(d+1):\n\t        Zl = []\n", "        # for n in range(min(2*d - l + 1, l + 1)):\n\t        #    if (n - l) % 2 == 0:\n\t        for n in range(l, d+1):\n\t            if (n - l) % 2 == 0 and d >= n:\n\t                Zl.append(zernike_kernel_3D_monomial_basis(n, l, monoms_basis))\n\t        Z.append(np.stack(Zl, axis=0))\n\t    return Z\n\t\"\"\"\n\tcomputes the 3D spherical harmonics basis up to degree l_max\n\t\"\"\"\n", "def torch_spherical_harmonics_basis(l_max, concat=False):\n\t    monoms_basis = monomial_basis_3D(l_max)\n\t    Y = []\n\t    for l in range(l_max+1):\n\t        Yl = spherical_harmonics_3D_monomial_basis(l, monoms_basis)\n\t        Y.append(torch.from_numpy(Yl).to(torch.float32))\n\t    if concat:\n\t        Y = torch.cat(Y, dim=0)\n\t    return Y\n\tdef np_zernike_kernel(d, n, l):\n", "    monoms_basis = monomial_basis_3D(d)\n\t    assert (n >= l and (n - l) % 2 == 0)\n\t    return zernike_kernel_3D_monomial_basis(n, l, monoms_basis)\n\tdef torch_eval_monom_basis(x, d, idx=None):\n\t    \"\"\"\n\t    evaluate monomial basis up to degree d\n\t    \"\"\"\n\t    batch_size = x.shape[0]\n\t    num_points = x.shape[1]\n\t    if idx is None:\n", "        idx = torch_monomial_basis_3D_idx(d)\n\t    y = []\n\t    for i in range(3):\n\t        pows = torch.reshape(torch.arange(d+1), (1, 1, d+1)).to(torch.float32)\n\t        yi = torch.pow(x[..., i].unsqueeze(-1), pows.type_as(x))\n\t        y.append(yi[..., idx[..., i]])\n\t    y = torch.stack(y, dim=-1)\n\t    y = torch.prod(y, dim=-1, keepdims=False)\n\t    return y\n\tclass SphericalHarmonicsGaussianKernels(torch.nn.Module):\n", "    def __init__(self, l_max, gaussian_scale, num_shells, transpose=False, bound=True):\n\t        super(SphericalHarmonicsGaussianKernels, self).__init__()\n\t        self.l_max = l_max\n\t        self.monoms_idx = torch_monomial_basis_3D_idx(l_max)\n\t        self.gaussian_scale = gaussian_scale\n\t        self.num_shells = num_shells\n\t        self.transpose = True\n\t        self.Y = torch_spherical_harmonics_basis(l_max, concat=True)\n\t        self.split_size = []\n\t        self.sh_idx = []\n", "        self.bound = bound\n\t        for l in range(l_max + 1):\n\t            self.split_size.append(2*l+1)\n\t            self.sh_idx += [l]*(2*l+1)\n\t        self.sh_idx = torch.from_numpy(np.array(self.sh_idx)).to(torch.int64)\n\t    def forward(self, x):\n\t        if \"patches dist\" in x:\n\t            patches_dist = x[\"patches dist\"].unsqueeze(-1)\n\t        else:\n\t            patches_dist = torch.linalg.norm(x[\"patches\"], dim=-1, keepdims=True)\n", "        normalized_patches = x[\"patches\"] / torch.maximum(patches_dist, torch.tensor(0.000001).type_as(x[\"patches\"]))\n\t        if self.transpose:\n\t            normalized_patches = -normalized_patches\n\t        # print(normalized_patches.shape)\n\t        monoms_patches = torch_eval_monom_basis(normalized_patches, self.l_max, idx=self.monoms_idx)\n\t        # print(self.Y.shape)\n\t        sh_patches = torch.einsum('ij,bvpj->bvpi', self.Y.type_as(monoms_patches), monoms_patches)\n\t        shells_rad = torch.arange(self.num_shells).type_as(monoms_patches) / (self.num_shells-1)\n\t        shells_rad = torch.reshape(shells_rad, (1, 1, 1, -1))\n\t        shells = patches_dist - shells_rad\n", "        shells = torch.exp(-self.gaussian_scale*(shells * shells))\n\t        shells_sum = torch.sum(shells, dim=-1, keepdims=True)\n\t        shells = (shells / torch.maximum(shells_sum, torch.tensor(0.000001).type_as(shells)))\n\t        shells = shells.unsqueeze(-2)\n\t        if self.bound:\n\t            shells = torch.where(patches_dist.unsqueeze(-1) <= torch.tensor(1.).type_as(shells), shells, torch.tensor(0.).type_as(shells))\n\t        sh_patches = sh_patches.unsqueeze(-1)\n\t        sh_patches = shells * sh_patches\n\t        # L2 norm\n\t        l2_norm = torch.sum((sh_patches * sh_patches), dim=2, keepdims=True)\n", "        l2_norm = torch.split(l2_norm, split_size_or_sections=self.split_size, dim=-2)\n\t        Y = []\n\t        for l in range(len(l2_norm)):\n\t            ml = torch.sum(l2_norm[l], dim=-2, keepdims=True)\n\t            ml = torch.sqrt(ml + 1e-7)\n\t            Y.append(ml)\n\t        l2_norm = torch.cat(Y, dim=-2)\n\t        l2_norm = torch.mean(l2_norm, dim=1, keepdims=True)\n\t        l2_norm = torch.maximum(l2_norm, torch.tensor(1e-8).type_as(l2_norm))\n\t        # print(l2_norm.shape)\n", "        l2_norm = l2_norm[..., self.sh_idx, :]\n\t        sh_patches = (sh_patches / (l2_norm + 1e-6))\n\t        return sh_patches\n\tif __name__==\"__main__\":\n\t    filename = \"/home/rahul/research/data/sapien_processed/train_refrigerator.h5\"\n\t    f = h5py.File(filename, \"r\")\n\t    x = torch.from_numpy(f[\"data\"][:2]).cuda()\n\t    x2 = torch.from_numpy(f[\"data\"][2:4]).cuda()\n\t    # print(x.shape)\n\t    # x = (torch.ones((1, 4, 3)) * torch.arange(4).unsqueeze(0).unsqueeze(-1)).cuda()\n", "    # y = torch_eval_monom_basis(x, 3)\n\t    # print(x)\n\t    # print(y, y.shape)\n\t    gi = GroupPoints(0.2, 32)\n\t    out = gi({\"source points\": x, \"target points\": x2})\n\t    sph_kernels = SphericalHarmonicsGaussianKernels(l_max = 3, gaussian_scale = 0.1, num_shells = 3, bound = True).cuda()\n\t    # x = (torch.ones((1, 100, 32, 3)) * torch.arange(3).unsqueeze(0).unsqueeze(1).unsqueeze(2)).cuda()\n\t    # print(x.shape)\n\t    patches = sph_kernels({\"patches\": out[\"patches source\"], \"patches dist\": out[\"patches dist source\"]})\n\t    # print(patches.shape)\n", "    conv_layer = ShGaussianKernelConv(l_max=3, l_max_out=3).cuda()\n\t    y = {}\n\t    y[\"source points\"] = x\n\t    y[\"target points\"] = x2\n\t    y[\"patches idx\"] = out[\"patches idx source\"]\n\t    y[\"patches dist source\"] = out[\"patches dist source\"]\n\t    y[\"kernels\"] = patches\n\t    # w = gauss_normalization(y[\"patches dist source\"], 1./3.)\n\t    if '1' in y:\n\t        y['1'] = torch.cat([y['1'], x2], dim=-1)\n", "    else:\n\t        y['1'] = x2.unsqueeze(-1)\n\t    y = conv_layer(y)\n\t    # print(y, y.shape)\n\t    # for key in y:\n\t        # print(y[key], \" \", key, \" \", y[key].shape)\n\t    # print(patches, patches.shape)\n"]}
{"filename": "cafi_net/models/TFN_density.py", "chunked_list": ["import torch\n\tfrom utils.group_points import GroupPoints\n\tfrom spherical_harmonics.spherical_cnn import torch_fibonnacci_sphere_sampling, SphericalHarmonicsEval, SphericalHarmonicsCoeffs\n\tfrom spherical_harmonics.kernels import SphericalHarmonicsGaussianKernels, ShGaussianKernelConv\n\tfrom spherical_harmonics.kernels_density import ShGaussianKernelConv_grid, SphericalHarmonicsGaussianKernels_density\n\tfrom models.layers import MLP, MLP_layer, set_sphere_weights, apply_layers\n\tfrom utils.pooling import kd_pooling_3d\n\tfrom utils.pointcloud_utils import GroupPoints_density, GroupPoints_grid ,GroupPoints_euclidean_density, rotate_density, get_gradient_density,kron\n\tfrom spherical_harmonics.wigner_matrix import *\n\tfrom e3nn import o3\n", "from utils.train_utils import mean_center\n\tfrom spherical_harmonics.clebsch_gordan_decomposition import torch_clebsch_gordan_decomposition\n\timport open3d as o3d\n\tclass TFN_grid_density(torch.nn.Module):\n\t    \"\"\"\n\t    TFN layer for prediction in Pytorch\n\t    \"\"\"\n\t    def __init__(self, sphere_samples = 64, bn_momentum = 0.75, mlp_units = [[32, 32], [64, 64], [128, 256]], l_max = [3, 3, 3], l_max_out = [3, 3, 3], num_shells = [3, 3, 3], radius = [0.4]):\n\t        super(TFN_grid_density, self).__init__()\n\t        self.l_max = l_max\n", "        self.l_max_out = l_max_out\n\t        self.num_shells = num_shells\n\t        self.gaussian_scale = []\n\t        for i in range(len(self.num_shells)):\n\t            self.gaussian_scale.append(0.69314718056 * ((self.num_shells[i]) ** 2))\n\t        self.radius = [0.2,0.4,0.8]\n\t        self.bounded = [True, True, True]\n\t        self.S2 = torch_fibonnacci_sphere_sampling(sphere_samples)\n\t        self.factor = [2, 2, 2]\n\t        self.patch_size = [1024,512,512]\n", "        self.spacing = [0, 0, 0]\n\t        self.equivariant_units = [32, 64, 128]\n\t        self.in_equivariant_channels = [[6, 13, 12, 9], [387, 874, 1065, 966], [771, 1738, 2121, 1926]]\n\t        self.mlp_units =  mlp_units\n\t        self.in_mlp_units = [32, 64, 128]\n\t        self.bn_momentum = bn_momentum\n\t        self.grouping_layers = []\n\t        self.grouping_layers_e = []\n\t        self.kernel_layers = []\n\t        self.conv_layers = []\n", "        self.eval = []\n\t        self.coeffs = []\n\t        for i in range(len(self.radius)):\n\t            gi = GroupPoints_density(radius=self.radius[i],\n\t                             patch_size_source=self.patch_size[i],\n\t                             spacing_source=self.spacing[i])\n\t            gi_e = GroupPoints_euclidean_density(radius=self.radius[i],\n\t                             patch_size_source=self.patch_size[i],\n\t                             spacing_source=self.spacing[i])\n\t            self.grouping_layers.append(gi)\n", "            self.grouping_layers_e.append(gi_e)\n\t            ki = SphericalHarmonicsGaussianKernels_density(l_max=self.l_max[i],\n\t                                                   gaussian_scale=self.gaussian_scale[i],\n\t                                                   num_shells=self.num_shells[i],\n\t                                                   bound=self.bounded[i])\n\t            ci = ShGaussianKernelConv(l_max=self.l_max[i], l_max_out=self.l_max_out[i])\n\t            self.kernel_layers.append(ki)\n\t            self.conv_layers.append(ci)\n\t        self.conv_layers = torch.nn.Sequential(*self.conv_layers)\n\t        self.kernel_layers = torch.nn.Sequential(*self.kernel_layers)\n", "        self.mlp = []\n\t        self.equivariant_weights = []\n\t        self.bn = []\n\t        for i in range(len(self.radius)):\n\t            self.bn.append(torch.nn.BatchNorm2d(self.equivariant_units[i], momentum=self.bn_momentum))\n\t            types = [str(l) for l in range(self.l_max_out[i] + 1)]\n\t            self.equivariant_weights.append(set_sphere_weights(self.in_equivariant_channels[i], self.equivariant_units[i], types=types))\n\t            self.mlp.append(MLP_layer(self.in_mlp_units[i], self.mlp_units[i], bn_momentum = self.bn_momentum))\n\t        self.mlp = torch.nn.Sequential(*self.mlp)\n\t        self.bn = torch.nn.Sequential(*self.bn)\n", "        self.equivariant_weights = torch.nn.Sequential(*self.equivariant_weights)\n\t        self.iSHT = []\n\t        self.fSHT = []\n\t        for i in range(len(self.l_max_out)):\n\t            self.iSHT.append(SphericalHarmonicsEval(l_max=self.l_max_out[i], base=self.S2))\n\t            self.fSHT.append(SphericalHarmonicsCoeffs(l_max=self.l_max_out[i], base=self.S2))\n\t        self.Q = torch_clebsch_gordan_decomposition(l_max=l_max_out[-1],\n\t                                         sparse=False,\n\t                                         output_type='dict',\n\t                                         l_max_out=l_max_out[-1])\n", "    def forward(self, x_grid, x_density):\n\t        \"\"\"\n\t        Input:\n\t            x_grid - [B, H, W, D, 3] - Equivariant density field\n\t            x_density - [B, H, W, D] - Equivariant density field\n\t        Returns:\n\t            TFN features - F\n\t        \"\"\"\n\t        if len(x_density.shape) == 4:\n\t            x_density = x_density.unsqueeze(-1) # B, H, W, D, 1\n", "        points_ = [x_grid]\n\t        density_ = [x_density]\n\t        grouped_points = []\n\t        kernels = []\n\t        for i in range(len(self.radius)):\n\t            pi = kd_pooling_3d(points_[-1], self.factor)\n\t            di = kd_pooling_3d(density_[-1], self.factor,'max')\n\t            points_.append(pi)\n\t            density_.append(di)\n\t        points = []\n", "        density = []\n\t        for i in range(len(points_)):\n\t            pi = points_[i]\n\t            di = density_[i]            \n\t            B, H, W, D, _ = pi.shape\n\t            points.append(pi.reshape(B, -1, 3))\n\t            density.append(di.reshape(B, -1, 1)) # B, N, 1\n\t        yzx = []\n\t        for i in range(len(points)):\n\t            yzx_i = torch.stack([points[i][..., 1], points[i][..., 2], points[i][..., 0]], dim=-1)\n", "            yzx.append(yzx_i.unsqueeze(-1)) # B, N, 3, 1\n\t        weighted_density = []\n\t        for i in range(len(self.radius)):\n\t            # Finding nearest neighbors of each point to compute graph features\n\t            gi = self.grouping_layers_e[i]({\"source points\": points[i], \"target points\": points[i + 1], \"source density\": density[i], \"target density\": density[i + 1]})\n\t            weighted_density.append(gi['weighted density'])\n\t            B, H, W, D, K, _ = gi[\"patches source\"].shape\n\t            # Computing kernels for patch neighbors\n\t            ki = self.kernel_layers[i]({\"patches\": gi[\"patches source\"].reshape(B, H*W*D, K, 3), \"patches dist\": gi[\"patches dist source\"].reshape(B, H*W*D, K)})\n\t            # Storing outputs\n", "            grouped_points.append(gi)\n\t            kernels.append(ki)\n\t        ki = kernels[0]\n\t        features = {}\n\t        types = [0,1,2,3]\n\t        ki = ki.squeeze(0)\n\t        ki = ki.squeeze(-1)\n\t        dim_start = 0\n\t        ones_singal =torch.ones((points[0].shape[0], points[0].shape[1], 1, 1)).type_as(x_grid).reshape(-1)\n\t        ones_singal  = density[0].reshape(-1)\n", "        y = {'0': ones_singal.reshape(points[0].shape[0], points[0].shape[1], 1, 1).type_as(x_grid)}\n\t        for i in range(len(self.radius)):\n\t            y[\"source points\"] = points[i]\n\t            y[\"target points\"] = points[i + 1]\n\t            B, H, W, D, K, _ = grouped_points[i][\"patches idx source\"].shape\n\t            y[\"patches idx\"] = grouped_points[i][\"patches idx source\"].reshape(B, H*W*D, K, -1)\n\t            y[\"patches dist source\"] = grouped_points[i][\"patches dist source\"].reshape(B, H*W*D, K)\n\t            y[\"kernels\"] = kernels[i]\n\t            if \"patches mask\" in grouped_points[i]:\n\t                y[\"patches mask\"] = grouped_points[i][\"patches mask\"]\n", "            shape_ = density_[i].shape\n\t            gradient_density_signal_int = get_gradient_density(density[i].reshape(shape_[0],shape_[1],shape_[2],shape_[3]))\n\t            gradient_density_signal     = gradient_density_signal_int.permute(0,2,3,4,1).reshape(shape_[0],-1,3,1)\n\t            if '1' in y:\n\t                y['1'] = torch.cat([y['1'], gradient_density_signal], dim=-1)\n\t            else:\n\t                y['1'] =  gradient_density_signal\n\t            y = self.conv_layers[i](y)\n\t            shape_ = density_[i+1].shape\n\t            gradient_density_signal_int_1 = get_gradient_density(density[i+1].reshape(shape_[0],shape_[1],shape_[2],shape_[3]))\n", "            gradient_density_signal_1       = gradient_density_signal_int_1.permute(0,2,3,4,1).reshape(shape_[0],-1,3,1)\n\t            if '1' in y:\n\t                y['1'] = torch.cat([y['1'],  gradient_density_signal_1], dim=-1)\n\t            else:\n\t                y['1'] =  gradient_density_signal_1\n\t            for key in y.keys():\n\t                if key.isnumeric():\n\t                    y[key] = y[key]* density[i+1].unsqueeze(-1)\n\t            y = apply_layers(y, self.equivariant_weights[i]) # B, d, 2*l + 1, C\n\t            y = self.iSHT[i].compute(y)\n", "            y = y.permute(0, 3, 1, 2)\n\t            y = self.bn[i](y)\n\t            y = torch.nn.ReLU(True)(y)\n\t            y = y.permute(0, 2, 3, 1)\n\t            y = self.mlp[i](y)\n\t            if i < len(self.radius) - 1:\n\t                y = self.fSHT[i].compute(y)\n\t        F = torch.max(y, dim=1, keepdims=False).values # B, samples, feature_dim\n\t        return F\n\tif __name__ == \"__main__\":\n", "    sdfPath = \"/home2/aditya.sharm/brics_fields/res_64/plane/train/rotated_fields/02691156_807d735fb9860fd1c863ab010b80d9ed_64_8_sdf.npy\"\n\t    ptsPath = \"/home2/aditya.sharm/brics_fields/res_64/plane/train/rotated_fields/02691156_807d735fb9860fd1c863ab010b80d9ed_64_8_pts.npy\"\n\t    sdf = np.load(sdfPath,allow_pickle=False)\n\t    coords = np.load(ptsPath,allow_pickle=False)\n\t    scale_factor = (coords.max())\n\t    coords = coords / scale_factor\n\t    x_in = torch.from_numpy(coords)\n\t    x_density = torch.from_numpy(sdf)\n\t    x = x_in\n\t    x_density = x_density.to(torch.float64)\n", "    model = TFN_grid_density()\n\t    type_features = model(x.unsqueeze(0).to(torch.float32), x_density.unsqueeze(0).to(torch.float32))\n\t    print(type_features.shape) \n\t    euler_angles_tensor = torch.tensor([2.0*0.785398,0,0])\n\t    rot_mat = euler_rot_zyz(2.0*0.785398,0,0)\n\t    types= [1,2,3]\n\t    wigner_rot_dict = {}\n\t    print(rot_mat)\n\t    for type_ in types:\n\t        wigner_rot = o3.wigner_D(type_, euler_angles_tensor[0], euler_angles_tensor[1], euler_angles_tensor[2])\n", "        wigner_rot = wigner_rot\n\t        print(wigner_rot)\n\t        wigner_rot_dict[str(type_)] = wigner_rot.to(torch.float64)\n\t    rotated_features_dict = {}\n\t    for type_ in types:\n\t        rot_features = torch.matmul(wigner_rot_dict[str(type_)].to(torch.float32),type_features[str(type_)])\n\t        rotated_features_dict[str(type_)] = rot_features\n\t    rot_mat = torch.from_numpy(rot_mat)\n\t    rot_mat = rot_mat\n\t    rot_mat = rot_mat.type(torch.float32)\n", "    rot_mat = rot_mat.type_as(x_density)\n\t    rot_mat = rot_mat.unsqueeze(0)\n\t    un_rot_density_zyx = x_density.unsqueeze(0).unsqueeze(0)\n\t    un_rot_density_zyx = un_rot_density_zyx.permute(0,1,4,3,2)\n\t    x_density_rot_zyx = rotate_density(torch.inverse(rot_mat), un_rot_density_zyx)\n\t    x_density_rot_zyx = x_density_rot_zyx.squeeze(0)\n\t    x_density_rot_zyx = x_density_rot_zyx.squeeze(0)\n\t    x_density_rot = x_density_rot_zyx.permute(2,1,0)\n\t    x_density_rot[x_density_rot >= 0.5] = 1\n\t    x_density_rot[x_density_rot < 0.5]  = 0\n", "    rot_type_features = model(x.unsqueeze(0).to(torch.float32), x_density_rot.unsqueeze(0).to(torch.float32))\n\t    '''\n\t    for type_ in types:\n\t        rotated_features_dict[str(type_)] = torch.sum(rotated_features_dict[str(type_)],dim=-1)\n\t        rot_type_features[str(type_)] = torch.sum(rot_type_features[str(type_)],dim=-1)\n\t    '''\n\t    main_idx = 0\n\t    max_dist_list = []\n\t    channels = [12,12,9]\n\t    for type_ in types:\n", "        for ch_dim in range(channels[type_-1]):\n\t            rot_feature = rot_type_features[str(type_)][:,:,:,ch_dim].squeeze(-1)\n\t            dim = (2*type_)+1\n\t            rot_feature = rot_feature.reshape(16,16,16,dim).unsqueeze(0).permute(0,4,3,2,1)\n\t            rot_feature_ = rotate_density(rot_mat, rot_feature).squeeze(0)\n\t            rot_sigma_feature_ = rot_feature_.permute(3,2,1,0).reshape(-1,dim)\n\t            rot_point_feature = rotated_features_dict[str(type_)][:,:,:,ch_dim].squeeze(-1).squeeze(0)\n\t            match = 0\n\t            un_match = 0\n\t            non_zero_features = 0\n", "            for index in range(rot_sigma_feature_.shape[0]):\n\t              if torch.count_nonzero(rot_sigma_feature_[index].type(torch.LongTensor)) > 0:\n\t                non_zero_features = non_zero_features + 1\n\t              else:\n\t                continue\n\t              if  torch.allclose(rot_sigma_feature_[index],rot_point_feature[index],atol=1e-04):\n\t                  match = match +1 \n\t              else:\n\t                  un_match = un_match + 1\n\t            print(\"matched features for the type \", type_ ,\" = \",match)\n", "            #print(\"significant features for the type \", type_ ,\" = \",non_zero_features)\n\t    '''  \n\t    for type_ in types:\n\t        rot_feature = rotated_features_dict[str(type_)]\n\t        rot_point_feature = rot_type_features[str(type_)]\n\t        if  torch.allclose(rot_feature,rot_point_feature,atol=1e-03):\n\t            print(\"Equal for the feature type \",str(type_))\n\t            print(\"Differece max \",torch.max(torch.abs(rot_feature - rot_point_feature)))\n\t            print(\"Differece sum \",torch.sum(torch.abs(rot_feature - rot_point_feature)))\n\t            print(\"Differece min \",torch.min(torch.abs(rot_feature - rot_point_feature)))\n", "            print(\"Differece mean \",torch.mean(torch.abs(rot_feature - rot_point_feature)))\n\t            print(\"Differece median \",torch.median(torch.abs(rot_feature - rot_point_feature)))\n\t        else:\n\t            print(\"Not Equal for the feature type \",str(type_))\n\t            print(\"Differece Norm \",torch.linalg.norm(torch.abs(rot_feature - rot_point_feature), dim=-1).max(),)\n\t            print(\"Differece max \",torch.max(torch.abs(rot_feature - rot_point_feature)))\n\t            print(\"Differece sum \",torch.sum(torch.abs(rot_feature - rot_point_feature)))\n\t            print(\"Differece min \",torch.min(torch.abs(rot_feature - rot_point_feature)))\n\t            print(\"Differece mean \",torch.mean(torch.abs(rot_feature - rot_point_feature)))\n\t            print(\"Differece median \",torch.median(torch.abs(rot_feature - rot_point_feature)))\n", "            print(\"Differece Mode \",torch.mode(torch.abs(rot_feature - rot_point_feature)))\n\t    for type_ in types:\n\t        rot_feature = rotated_features_dict[str(type_)][:,:,:,5:6].squeeze(-1).squeeze(0)\n\t        rot_point_feature = rot_type_features[str(type_)]\n\t        match = 0\n\t        un_match = 0\n\t        for index in range(rot_feature.shape[0]):\n\t          if  torch.allclose(rot_feature[index],rot_point_feature[index],atol=1e-03):\n\t              match = match +1 \n\t          else:\n", "              un_match = un_match + 1\n\t        print(\"matched features for the type \", type_ ,\" = \",match)'''\n\t    '''\n\t    main_idx = 0\n\t    for type_ in types:\n\t        rot_feature = rotated_features_dict[str(type_)].squeeze(0)\n\t        rot_point_feature = rot_type_features[str(type_)].squeeze(0)\n\t        search_idx = []\n\t        for main_idx in range(rot_feature.shape[0]):\n\t            main_feature = rot_feature[:,:,0:1].squeeze(-1)[main_idx]\n", "            found = 0\n\t            for second_idx in range(rot_point_feature.shape[0]):\n\t                second_feature = rot_point_feature[second_idx]\n\t                if second_idx in search_idx:\n\t                    continue\n\t                if  torch.allclose(main_feature,second_feature,atol=1e-03):\n\t                    #print(\"Equal for the feature type  \",str(type_),\" at the idx\", main_idx)\n\t                    search_idx.append(second_idx)\n\t                    found = 1\n\t                    break\n", "            if not found:\n\t                print(\"Not Fo\")'''\n"]}
{"filename": "cafi_net/models/layers.py", "chunked_list": ["import torch.nn as nn\n\timport torch\n\tfrom spherical_harmonics.spherical_cnn import SphericalHarmonicsCoeffs\n\tdef apply_layers(x, layers):\n\t    y = dict()\n\t    # print(x.keys())\n\t    for l in x:\n\t        # print(x[l].shape, l)\n\t        if l.isnumeric():\n\t            y[l] = layers[int(l)](x[l])\n", "    return y\n\tdef type_0(x, S2):\n\t    \"\"\"\n\t    Spherical Harmonics Transform to extract type 0 features\n\t    \"\"\"\n\t    y = SphericalHarmonicsCoeffs(l_list=[0], base=S2).compute(x)\n\t    return y['0']\n\tdef type_1(x, S2):\n\t    \"\"\"\n\t    Spherical Harmonics Transform to extract type 1 features that are equivariant to SO(3)\n", "    \"\"\"\n\t    y = SphericalHarmonicsCoeffs(l_list=[1], base=S2).compute(x)\n\t    return y['1']\n\tdef set_sphere_weights(in_channel, out_channel, types):\n\t    weights = []\n\t    for l in types:\n\t        if int(l) == 0:\n\t            weights.append(nn.Linear(in_channel[int(l)], out_channel))\n\t        else:\n\t            weights.append(nn.Linear(in_channel[int(l)], out_channel, bias=False))\n", "    return torch.nn.Sequential(*weights)\n\tclass MLP(nn.Module):\n\t    def __init__(self, in_channels, out_channels, bn_momentum = 0.75, apply_norm = True, activation = None):\n\t        super(MLP, self).__init__()\n\t        self.mlp = nn.Linear(in_channels, out_channels)\n\t        if apply_norm:\n\t            self.batchnorm = nn.BatchNorm1d(out_channels, momentum=bn_momentum)\n\t        self.activation = activation\n\t        self.apply_norm = apply_norm\n\t    def forward(self, x):\n", "        out = self.mlp(x)        \n\t        if self.activation is not None:\n\t            out = self.activation(out)\n\t        if self.apply_norm:\n\t            out = self.batchnorm(out.transpose(1, -1)).transpose(1, -1)\n\t        return out\n\tclass MLP_layer(nn.Module):\n\t    def __init__(self, in_channels, units = [32, 64, 128], bn_momentum = 0.75, apply_norm = False, activation = None):\n\t        super(MLP_layer, self).__init__()\n\t        self.input_layer = nn.Linear(in_channels, units[0])\n", "        self.mlp = []\n\t        self.batchnorm = []\n\t        for i in range(1, len(units)):\n\t            self.mlp.append(MLP(units[i-1], units[i], bn_momentum = bn_momentum, apply_norm = apply_norm, activation = activation))\n\t        self.mlp = nn.Sequential(*self.mlp)\n\t    def forward(self, x):\n\t        out = self.input_layer(x)\n\t        out = self.mlp(out)\n\t        return out"]}
{"filename": "cafi_net/models/__init__.py", "chunked_list": ["from . import TFN_density\n\tfrom . import Cafi_model\n"]}
{"filename": "cafi_net/models/Cafi_model.py", "chunked_list": ["import pytorch_lightning as pl\n\timport torch\n\tfrom models.TFN_density import TFN_grid_density\n\tfrom models.layers import MLP, MLP_layer\n\tfrom utils.group_points import GroupPoints\n\tfrom spherical_harmonics.spherical_cnn import torch_fibonnacci_sphere_sampling, SphericalHarmonicsEval, SphericalHarmonicsCoeffs, zernike_monoms, torch_zernike_monoms\n\tfrom spherical_harmonics.kernels import SphericalHarmonicsGaussianKernels, ShGaussianKernelConv\n\tfrom models.layers import MLP, MLP_layer, set_sphere_weights, apply_layers, type_1\n\tfrom utils.pooling import kd_pooling_3d\n\tclass Cafi_model(torch.nn.Module):\n", "    def __init__(self, num_capsules = 10, num_frames = 1, sphere_samples = 64, bn_momentum = 0.75, mlp_units = [[32, 32], [64, 64], [128, 256]], radius = [0.4, 0.8, 1.5]):\n\t        super(Cafi_model, self).__init__()\n\t        self.radius = radius\n\t        self.bn_momentum = 0.75\n\t        self.basis_dim = 3\n\t        self.l_max = [3, 3, 3]\n\t        self.l_max_out = [3, 3, 3]\n\t        self.num_shells = [3, 3, 3]\n\t        self.num_capsules = num_capsules\n\t        self.num_frames = num_frames\n", "        self.mlp_units = mlp_units\n\t        self.TFN_arch = TFN_grid_density(sphere_samples = sphere_samples, bn_momentum = bn_momentum, mlp_units = [[32, 32], [64, 64], [128, 256]], l_max = self.l_max, l_max_out = self.l_max_out, num_shells = self.num_shells, radius = self.radius)\n\t        self.S2 = torch_fibonnacci_sphere_sampling(sphere_samples)\n\t        self.basis_mlp = []\n\t        self.basis_layer = []\n\t        self.basis_units = [64]\n\t        for frame_num in range(num_frames):\n\t            self.basis_mlp.append(MLP_layer(in_channels = self.mlp_units[-1][-1], units = self.basis_units, bn_momentum = self.bn_momentum))\n\t            self.basis_layer.append(MLP(in_channels = self.basis_units[-1], out_channels=self.basis_dim, apply_norm = False))\n\t        self.basis_mlp = torch.nn.Sequential(*self.basis_mlp)\n", "        self.basis_layer = torch.nn.Sequential(*self.basis_layer)\n\t        self.code_dim = 64\n\t        self.code_layer_params = [128]\n\t        self.code_mlp = MLP_layer(in_channels = self.mlp_units[-1][-1], units = self.code_layer_params, bn_momentum = self.bn_momentum)\n\t        self.code_layer = MLP(in_channels = self.code_layer_params[-1], out_channels=self.code_dim, apply_norm = False)\n\t        self.points_inv_layer = MLP(in_channels = 128, out_channels=3, apply_norm = False)\n\t        self.num_frames = num_frames\n\t        self.zernike_harmonics = torch_zernike_monoms(self.l_max_out[-1])\n\t        self.fSHT = SphericalHarmonicsCoeffs(l_max=self.l_max_out[-1], base=self.S2)\n\t        self.type_1_basis = SphericalHarmonicsCoeffs(l_list=[1], base=self.S2)\n", "    def forward(self, x, x_density):\n\t        \"\"\"\n\t        x - B, N, 3 - Batch of point clouds that are kdtree indexed for pooling\n\t        \"\"\"\n\t        # Compute TFN features\n\t        F = self.TFN_arch(x, x_density)\n\t        x_density_in = x_density        \n\t        # Equivariant Basis\n\t        E = []\n\t        # Compute equivariant layers\n", "        for frame_num in range(self.num_frames):\n\t            basis = self.basis_mlp[frame_num](F)\n\t            basis = self.basis_layer[frame_num](basis)\n\t            basis = self.type_1_basis.compute(basis)[\"1\"]\n\t            basis = torch.nn.functional.normalize(basis, dim=-1, p = 2, eps = 1e-6)\n\t            E.append(basis)\n\t        B, H, W, D, _ = x.shape\n\t        x = x.reshape(B, -1, 3)\n\t        x_density = x_density.reshape(B, -1, 1)\n\t        latent_code = self.code_mlp(F)\n", "        latent_code = self.code_layer(latent_code)\n\t        latent_code = self.fSHT.compute(latent_code)\n\t        z = self.zernike_harmonics.compute(x)\n\t        points_code = []\n\t        points_inv = None\n\t        for l in latent_code:\n\t            p = torch.einsum('bmi,bvmj->bvij', latent_code[l], z[int(l)])\n\t            shape = list(p.shape)\n\t            shape = shape[:-1]\n\t            shape[-1] = -1\n", "            p = torch.reshape(p, shape)\n\t            points_code.append(p)\n\t            if int(l) == 1:\n\t                points_inv = p\n\t        points_code = torch.cat(points_code, dim=-1)\n\t        points_inv = self.points_inv_layer(points_inv)\n\t        if len(x_density_in.shape) == 4:\n\t            x_density_in = x_density_in.unsqueeze(-1)\n\t        coords = points_inv.reshape(B, H, W, D, 3)\n\t        points_inv = torch.nn.functional.grid_sample(x_density_in.permute(0, -1, 1, 2, 3), coords, align_corners=True).squeeze(1)\n", "        out = {\"points_inv\": points_inv, \"E\": E, \"coords\": coords}\n\t        return out\n"]}
{"filename": "cafi_net/trainers/Canonical_fields_trainer.py", "chunked_list": ["from pkg_resources import declare_namespace\n\timport torch\n\timport os\n\tfrom models import *\n\tfrom utils import *\n\tfrom utils.train_utils import random_rotate, mean_center, perform_rotation, orthonormalize_basis\n\tfrom utils.losses import equilibrium_loss , localization_loss_new\n\timport datasets\n\timport numpy as np\n\timport pytorch_lightning as pl\n", "from torch.utils.data import Dataset, DataLoader\n\timport hydra\n\tfrom utils.pointcloud_utils import kdtree_indexing, save_pointcloud, convert_yzx_to_xyz_basis, get_equivariant_density, rotate_density, sq_distance_mat, save_density, get_gradient_density,create_mask\n\tfrom scipy.spatial.transform import Rotation\n\tfrom random import randint\n\timport h5py\n\tfrom sklearn.mixture import GaussianMixture\n\tfrom sklearn.cluster import KMeans\n\timport open3d as o3d\n\tfrom pytorch3d.loss import chamfer_distance\n", "import random, sys\n\tclass Canonical_fields_trainer(pl.LightningModule):\n\t    '''\n\t    Segmentation trainer to mimic NeSF\n\t    '''\n\t    def __init__(self, configs):\n\t        super().__init__()\n\t        self.save_hyperparameters()\n\t        self.hparam_config = configs\n\t        self.Cafinet = getattr(eval(self.hparam_config.model[\"file\"]), \n", "                                  self.hparam_config.model[\"type\"])(**self.hparam_config.model[\"args\"])\n\t        self.hparam_config.dataset.args.dataset_path = os.path.join(hydra.utils.get_original_cwd(), self.hparam_config.dataset.args.dataset_path)\n\t        self.loss_weights = self.hparam_config.loss\n\t    def train_dataloader(self):\n\t        train_data_set = getattr(getattr(datasets, self.hparam_config.dataset.file), self.hparam_config.dataset.type)(**self.hparam_config.dataset.args)\n\t        train_dataloader = DataLoader(train_data_set, **self.hparam_config.dataset.loader.args, shuffle = True)\n\t        return train_dataloader\n\t    def val_dataloader(self):\n\t        val_data_set = getattr(getattr(datasets, self.hparam_config.val_dataset.file), self.hparam_config.val_dataset.type)(**self.hparam_config.val_dataset.args)\n\t        val_dataloader = DataLoader(val_data_set, **self.hparam_config.val_dataset.loader.args, shuffle = False)\n", "        return val_dataloader\n\t    def forward_pass(self, batch, batch_idx, return_outputs=False, rot_mat=None):\n\t        density = batch[\"density\"][0].clone()\n\t        coords  = batch[\"coords\"][0].clone()\n\t        iter_ =2   \n\t        if return_outputs == True :\n\t            iter_ = 1\n\t        loss_dictionary_list = []   \n\t        frame_list = []\n\t        inv_embed_list = []\n", "        for ii in range(iter_):\n\t            if rot_mat is None :\n\t                rotation_1 = torch.from_numpy(Rotation.random(density.shape[0]).as_matrix()).type_as(density)\n\t            else:\n\t                rotation_1 = rot_mat.type_as(density)\n\t            density = batch[\"density\"][ii].clone()\n\t            coords  = batch[\"coords\"][ii].clone()\n\t            density = rotate_density(rotation_1, density)\n\t            B,_,_,_ = density.shape\n\t            shape_ = density.shape\n", "            mask_density = create_mask(density).cuda()   \n\t            out_dict = self.Cafinet(coords, density)\n\t            batch[\"eq_input_1\"] = coords\n\t            batch[\"x_input\"] = density\n\t            batch[\"density_rotated_1\"] = density        \n\t            batch[\"rotation_1\"] = rotation_1\n\t            batch[\"mask_density\"] = mask_density\n\t            B,_,_,_,_ = out_dict[\"coords\"].shape\n\t            if return_outputs:\n\t                out_dict[\"x_input\"] = density\n", "                loss_dictionary, frame = self.compute_loss(batch, out_dict, return_frame = True)\n\t                out_dict[\"E\"] = frame\n\t                out_dict[\"coords\"] = out_dict[\"coords\"].reshape(B,-1,3)\n\t                out_dict[\"input_rotation\"] = rotation_1\n\t                return loss_dictionary, out_dict\n\t            else:\n\t                loss_dictionary , frame  = self.compute_loss(batch, out_dict, return_frame=True)\n\t                loss_dictionary_list.append(loss_dictionary)\n\t                frame_list.append(frame)\n\t                pts_list = []\n", "                for _indx in range(B):\n\t                    pts_list.append(out_dict[\"coords\"][_indx][mask_density[_indx] >= 0.5].unsqueeze(0))\n\t                inv_embed_list.append(pts_list)\n\t        for ii in range(1,len(loss_dictionary_list)):\n\t            for key in loss_dictionary_list[0].keys():\n\t                loss_dictionary_list[0][key] = loss_dictionary_list[0][key] + loss_dictionary_list[ii][key]\n\t        for key in loss_dictionary_list[0].keys():\n\t                loss_dictionary_list[0][key] = loss_dictionary_list[0][key] / (len(loss_dictionary_list))\n\t        inv_loss = 0.0\n\t        for _indx in range(B):\n", "            inv_loss = inv_loss + chamfer_distance(inv_embed_list[0][_indx],inv_embed_list[1][_indx])[0]\n\t        inv_loss = inv_loss / float(B)\n\t        loss_dictionary_list[0][\"loss\"]  = loss_dictionary_list[0][\"loss\"] + inv_loss\n\t        loss_dictionary_list[0][\"inv_loss\"]  = inv_loss\n\t        return loss_dictionary_list[0]\n\t    def compute_loss(self, batch, outputs, return_frame = False):\n\t        \"\"\"\n\t        Computing losses for \n\t        \"\"\"\n\t        loss_dictionary = {}\n", "        loss = 0.0\n\t        out_density_1 = batch[\"mask_density\"]\n\t        rotation_1 = batch[\"rotation_1\"]\n\t        # First branch\n\t        density_canonical = outputs[\"points_inv\"]\n\t        basis_1 = outputs[\"E\"]\n\t        canonical_coords = outputs[\"coords\"]\n\t        basis_1 = torch.stack(basis_1, dim = 1) # B, num_frames, 3, 3\n\t        B, H, W, D, _3 = canonical_coords.shape\n\t        orth_basis_1 = (orthonormalize_basis(basis_1))\n", "        basis_canonical_to_input = convert_yzx_to_xyz_basis((orth_basis_1))\n\t        eq_input_1 =batch[\"eq_input_1\"].reshape(B,-1,3)\n\t        B, num_frames = orth_basis_1.shape[0], orth_basis_1.shape[1]\n\t        eq_coords_pred = torch.einsum(\"bfij, bhwdj->bfhwdi\", basis_canonical_to_input, canonical_coords).reshape(B, num_frames, -1, _3)\n\t        mask_object = 1.0*(out_density_1 >= 0.5).reshape(B, 1, -1)\n\t        error_full = torch.sum(torch.sqrt(torch.sum(torch.square(eq_coords_pred - eq_input_1[:, None]), -1) + 1e-8) * mask_object, -1) / (torch.sum(mask_object, -1) + 1e-10)\n\t        values, indices = torch.topk(-error_full, k = 1)\n\t        orth_basis_frames = basis_canonical_to_input\n\t        basis_canonical_to_input = basis_canonical_to_input[torch.arange(indices.shape[0]), indices[:, 0]]\n\t        eq_input_pred_best = torch.einsum(\"bij, bhwdj->bhwdi\", basis_canonical_to_input, canonical_coords).reshape(B, -1, _3)\n", "        l2_loss = torch.mean(torch.sum(torch.sqrt(torch.sum(torch.square(eq_input_pred_best - eq_input_1), -1) + 1e-8) * mask_object.squeeze(1),-1) / (torch.sum(mask_object.squeeze(1),-1) + 1e-10))\n\t        chamfer_loss = 0.0\n\t        for idx in range(B):\n\t            int_mask = mask_object[idx].permute(1,0)\n\t            int_pts_idx,_ = torch.where(int_mask >=0.5)\n\t            chamfer_loss = chamfer_loss + chamfer_distance(eq_input_pred_best[idx][int_pts_idx,:].unsqueeze(0),eq_input_1[idx][int_pts_idx,:].unsqueeze(0))[0]\n\t        chamfer_loss = chamfer_loss / float(B)  \n\t        orth_loss = torch.mean(torch.abs(basis_1 - orth_basis_1.detach()))\n\t        I = torch.eye(num_frames).type_as(basis_1).unsqueeze(0)\n\t        ones = torch.ones(B, num_frames, num_frames).type_as(basis_1)\n", "        weights = ones - I\n\t        separation_loss = torch.sum(torch.mean(torch.mean(torch.exp(-(torch.abs(basis_1[:, :, None] - basis_1[:, None]))), -1), -1) * weights) / (torch.sum(weights) + 1e-10)\n\t        if self.loss_weights.l2_loss > 0.0:\n\t            loss += self.loss_weights.l2_loss * l2_loss\n\t        if self.loss_weights.chamfer_loss > 0.0:\n\t            loss += self.loss_weights.chamfer_loss * chamfer_loss\n\t        if self.loss_weights.separation_loss > 0.0:\n\t            loss += self.loss_weights.separation_loss * separation_loss\n\t        if self.loss_weights.orth_loss > 0.0:\n\t            loss += self.loss_weights.orth_loss * orth_loss\n", "        loss_dictionary[\"loss\"] = loss\n\t        loss_dictionary[\"l2_loss\"] = l2_loss\n\t        loss_dictionary[\"orth_loss\"] = orth_loss  \n\t        loss_dictionary[\"separation_loss\"] = separation_loss\n\t        loss_dictionary[\"chamfer_loss\"] = chamfer_loss  \n\t        if return_frame:\n\t            return loss_dictionary, basis_canonical_to_input\n\t        return loss_dictionary\n\t    def training_step(self, batch, batch_idx):\n\t        loss_dictionary = self.forward_pass(batch, batch_idx)\n", "        self.log_loss_dict(loss_dictionary)\n\t        return loss_dictionary[\"loss\"]\n\t    def validation_step(self, batch, batch_idx):\n\t        loss_dictionary = self.forward_pass(batch, batch_idx)\n\t        self.log_loss_dict(loss_dictionary, val = True)\n\t        return loss_dictionary[\"loss\"]\n\t    def configure_optimizers(self):\n\t        optimizer1 = getattr(torch.optim, self.hparam_config.optimizer.type)(list(self.Cafinet.parameters()), **self.hparam_config.optimizer.args)\n\t        scheduler1 = getattr(torch.optim.lr_scheduler, self.hparam_config.scheduler.type)(optimizer1, **self.hparam_config.scheduler.args)\n\t        return [optimizer1], [scheduler1]\n", "    def log_loss_dict(self, loss_dictionary, val = False):\n\t        for key in loss_dictionary:\n\t            if val:\n\t                self.log(\"val_\" + key, loss_dictionary[key], **self.hparam_config.logging.args)\n\t            else:\n\t                self.log(key, loss_dictionary[key], **self.hparam_config.logging.args)\n\t    def test_step(self, x):\n\t        '''\n\t        Input:\n\t            x - B, N, 3\n", "        Output:\n\t            output_dictionary - dictionary with all outputs and inputs\n\t        '''\n\t        output_dictionary = self.forward_pass(x)\n\t        return output_dictionary\n\t    def save_outputs(self, save_file_name, out_dict):\n\t        \"\"\"\n\t        Save outputs to file\n\t        \"\"\"\n\t        save_keys = [\"x_input\", \"canonical_density\"]\n", "        for key in out_dict:\n\t            if key in save_keys:\n\t                pcd_name = save_file_name + \"_\" + key + \".ply\"\n\t                save_density(out_dict[key], pcd_name)\n\t    def loadh5(self,path):\n\t        fx_input = h5py.File(path, 'r')\n\t        x = fx_input['data'][:]\n\t        fx_input.close()\n\t        return x\n\t    def save_h5(self,h5_filename, data,data_dtype='float32'):\n", "        h5_fout = h5py.File(h5_filename,\"w\")\n\t        h5_fout.create_dataset(\n\t            'data', data=data,\n\t            compression='gzip', compression_opts=4,\n\t            dtype=data_dtype)\n\t        h5_fout.close()\n\t    def run_test(self, cfg ,dataset_num = 1, save_directory = \"./pointclouds\", max_iters = None, skip = 1 , num_rots=1):\n\t        self.hparam_config = cfg\n\t        self.hparam_config.val_dataset.loader.args.batch_size = 1\n\t        loader = self.val_dataloader()\n", "        if max_iters is not None:\n\t            max_iters = min(max_iters, len(loader))\n\t        else:\n\t            max_iters = len(loader)\n\t        if not os.path.exists(save_directory):\n\t            os.makedirs(save_directory)\n\t        with torch.no_grad():\n\t            for i, batch in enumerate(loader):\n\t                if i % skip == 0:\n\t                    batch[\"density\"][0] = batch[\"density\"][0].cuda()\n", "                    batch[\"coords\"][0]  = batch[\"coords\"][0].cuda()\n\t                    x = batch\n\t                    for _ in range(num_rots):\n\t                        random_rots = torch.from_numpy(Rotation.random(num_rots).as_matrix())\n\t                        rot_mat = random_rots[_].reshape(1,3,3)                      \n\t                        out, output_dict = self.forward_pass(x, 0, return_outputs = True, rot_mat=torch.inverse(rot_mat))\n\t                        density_1 = output_dict[\"x_input\"]\n\t                        basis_1 = output_dict[\"E\"]\n\t                        canonical_density = rotate_density(basis_1, density_1)\n\t                        output_dict[\"canonical_density\"] = canonical_density                      \n", "                        save_file_name = os.path.join(save_directory, \"\") + str(_) + \"_ \"+\"%d\" % i \n\t                        self.save_outputs(save_file_name, output_dict)\n\t    def run_metrics(self, cfg ,dataset_num = 1, save_directory = \"./pointclouds\", max_iters = None, skip = 1):\n\t        self.hparam_config = cfg\n\t        self.hparam_config.val_dataset.loader.args.batch_size = 1\n\t        loader = self.val_dataloader()\n\t        if max_iters is not None:\n\t            max_iters = min(max_iters, len(loader))\n\t        else:\n\t            max_iters = len(loader)\n", "        if not os.path.exists(save_directory):\n\t            os.makedirs(save_directory)\n\t        random_rots = self.loadh5(self.hparam_config.test_args.rotation_file)\n\t        random_rots = torch.from_numpy(random_rots)\n\t        category_name = self.hparam_config.test_args.category_name\n\t        h5_filename = save_directory + \"/\"+ category_name +\"_rotations.h5\"\n\t        self.save_h5(h5_filename, random_rots)\n\t        canonical_frame=[]\n\t        with torch.no_grad():\n\t            for i, batch in enumerate(loader):\n", "                print(\"processing for Nerf Model \",i)\n\t                if i % skip == 0:\n\t                    batch[\"density\"][0] = batch[\"density\"][0].cuda()\n\t                    batch[\"coords\"][0] = batch[\"coords\"][0].cuda()\n\t                    x = batch\n\t                    canonical_frame_obj = []\n\t                    for _ in range(random_rots.shape[0]):\n\t                        rot_mat = random_rots[_].reshape(1,3,3)                                            \n\t                        out, output_dict = self.forward_pass(x, 0, return_outputs = True, rot_mat=torch.inverse(rot_mat))\n\t                        basis_1 = output_dict[\"E\"]\n", "                        canonical_frame_obj.append(torch.inverse(basis_1.squeeze(0)))\n\t                    canonical_frame.append(torch.stack(canonical_frame_obj))\n\t            canonical_frame = torch.stack(canonical_frame)\n\t            h5_filename = save_directory + \"/\"+ category_name +\"_canonical.h5\"\n\t            self.save_h5(h5_filename, canonical_frame.cpu().detach().numpy())\n\t    def run_canonica_render(self, cfg ,dataset_num = 1, save_directory = \"./pointclouds\", max_iters = None, skip = 1):\n\t        self.hparam_config = cfg\n\t        self.hparam_config.val_dataset.loader.args.batch_size = 1\n\t        loader = self.val_dataloader()\n\t        if max_iters is not None:\n", "            max_iters = min(max_iters, len(loader))\n\t        else:\n\t            max_iters = len(loader)\n\t        if not os.path.exists(save_directory):\n\t            os.makedirs(save_directory)\n\t        category_name = self.hparam_config.test_args.category_name\n\t        rotation_list = []\n\t        canonical_frame=[]\n\t        file_name_list=[]\n\t        with torch.no_grad():\n", "            for i, batch in enumerate(loader):\n\t                print(\"processing for Nerf Model \",i)\n\t                if i % skip == 0:\n\t                    batch[\"density\"][0] = batch[\"density\"][0].cuda()\n\t                    batch[\"coords\"][0] = batch[\"coords\"][0].cuda()\n\t                    file_name_list.append(batch[\"file_path\"][0])\n\t                    x = batch\n\t                    canonical_frame_obj = []\n\t                    for _ in range(1):\n\t                        rot_mat = torch.eye(3).reshape(1,3,3).to(torch.float32).cuda() \n", "                        random_rots = torch.from_numpy(Rotation.random(1).as_matrix())\n\t                        rot_mat = random_rots[_].reshape(1,3,3) \n\t                        rotation_list.append(rot_mat)\n\t                        out, output_dict = self.forward_pass(x, 0, return_outputs = True, rot_mat=torch.inverse(rot_mat))\n\t                        basis_1 = output_dict[\"E\"]\n\t                        canonical_frame_obj.append(torch.inverse(basis_1.squeeze(0)))\n\t                    canonical_frame.append(torch.stack(canonical_frame_obj))\n\t            rotation_list = torch.stack(rotation_list,axis=0)\n\t            h5_filename = save_directory + \"/\"+ category_name +\"_input_rot.h5\"\n\t            self.save_h5(h5_filename, rotation_list.cpu().detach().numpy())\n", "            file_name = save_directory + \"/\" + category_name +\"_files.txt\"\n\t            file_prt = open(file_name,\"w+\")\n\t            for _ in file_name_list:\n\t                file_prt.write(_+\"\\n\")\n\t            file_prt.close()\n\t            canonical_frame = torch.stack(canonical_frame)\n\t            h5_filename = save_directory + \"/\"+ category_name +\"_canonical.h5\"\n\t            self.save_h5(h5_filename, canonical_frame.cpu().detach().numpy())\n"]}
{"filename": "cafi_net/trainers/__init__.py", "chunked_list": ["from . import Canonical_fields_trainer\n"]}
{"filename": "cafi_net/datasets/__init__.py", "chunked_list": ["from . import density_dataset\n"]}
{"filename": "cafi_net/datasets/density_dataset.py", "chunked_list": ["import sys\n\tsys.path.append(\"../\")\n\timport os\n\timport json\n\timport glob\n\timport torch\n\timport numpy as np\n\timport torchvision.transforms as transforms\n\timport torch.utils.data as data\n\tfrom torch.utils.data import Dataset, DataLoader\n", "import random, sys\n\tfrom scipy.ndimage import zoom\n\timport utils.pointcloud_utils as pcd_utils\n\tfrom scipy.spatial.transform import Rotation\n\tfrom utils.pointcloud_utils import get_xyz_grid\n\timport open3d as o3d\n\tfrom random import randrange\n\tclass Density_loader_shapenet(Dataset):\n\t    def __init__(self, dataset_path,sigmas_file_pattern):\n\t        super(Density_loader_shapenet, self).__init__()\n", "        self.dataset_path = dataset_path\n\t        self.files = glob.glob(os.path.join(dataset_path, \"\") + sigmas_file_pattern)\n\t        self.files.sort()\n\t        self.dimensions = [32, 32, 32]\n\t    def __len__(self):\n\t        return len(self.files)\n\t    def __getitem__(self, key):\n\t        grid = np.load(self.files[key])\n\t        grid = np.where(grid <=0,0,grid)\n\t        grid = grid * grid \n", "        grid = np.exp(-1.0 * grid)\n\t        grid_new = 1 - grid\n\t        grid_new = np.clip(grid_new, 0, 1)\n\t        density_grid = torch.from_numpy(grid_new).to(torch.float32).permute(2,1,0)\n\t        pair_idx = randrange(int(len(self.files)))\n\t        grid_1 = np.load(self.files[pair_idx])\n\t        grid_1 = np.where(grid_1 <=0,0,grid_1)\n\t        grid_1 = grid_1 * grid_1 \n\t        grid_1 = np.exp(-1.0 * grid_1)\n\t        grid_new_1= 1 - grid_1\n", "        grid_new_1 = np.clip(grid_new_1, 0, 1)\n\t        density_grid_1 = torch.from_numpy(grid_new_1).to(torch.float32).permute(2,1,0)\n\t        density_list = [density_grid,density_grid_1]\n\t        x = np.linspace(-1,1,self.dimensions[0])\n\t        y = np.linspace(-1,1,self.dimensions[1])\n\t        z = np.linspace(-1,1,self.dimensions[2])\n\t        x_coords, y_coords,z_coords =  np.meshgrid(x,y,z,indexing='ij')\n\t        coords = torch.from_numpy(np.stack((x_coords,y_coords,z_coords),axis=-1)).to(torch.float32).permute(2,1,0,3)\n\t        coords_list =  [coords,coords]\n\t        return {\"density\": density_list , \"coords\":coords_list , \"file_path\": self.files[key]}\n"]}
