{"filename": "config.py", "chunked_list": ["import argparse\n\tfrom tokenize import group\n\tdef get_config():\n\t    \"\"\"\n\t    The configuration parser for common hyperparameters of all environment.\n\t    Please reach each `scripts/train/<env>_runner.py` file to find private hyperparameters\n\t    only used in <env>.\n\t    \"\"\"\n\t    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter)\n\t    parser = _get_prepare_config(parser)\n", "    parser = _get_replaybuffer_config(parser)\n\t    parser = _get_network_config(parser)\n\t    parser = _get_recurrent_config(parser)\n\t    parser = _get_optimizer_config(parser)\n\t    parser = _get_ppo_config(parser)\n\t    parser = _get_selfplay_config(parser)\n\t    parser = _get_pbt_config(parser)\n\t    parser = _get_eval_config(parser)\n\t    return parser\n\tdef _get_prepare_config(parser: argparse.ArgumentParser):\n", "    \"\"\"\n\t    Prepare parameters:\n\t        --env-name <str>\n\t            specify the name of environment\n\t        --algorithm-name <str>\n\t            specifiy the algorithm, including `[\"ppo\", \"mappo\"]`\n\t        --experiment-name <str>\n\t            an identifier to distinguish different experiment.\n\t        --seed <int>\n\t            set seed for numpy and torch\n", "        --cuda\n\t            by default False, will use CPU to train; or else will use GPU;\n\t        --num-env-steps <float>\n\t            number of env steps to train (default: 1e7)\n\t        --model-dir <str>\n\t            by default None. set the path to pretrained model.\n\t        --use-wandb\n\t            [for wandb usage], by default False, if set, will log date to wandb server.\n\t        --user-name <str>\n\t            [for wandb usage], to specify user's name for simply collecting training data.\n", "        --wandb-name <str>\n\t            [for wandb usage], to specify user's name for simply collecting training data.\n\t    \"\"\"\n\t    group = parser.add_argument_group(\"Prepare parameters\")\n\t    group.add_argument(\"--env-name\", type=str, default='SumoAnts-v0',\n\t                       help=\"specify the name of environment\")\n\t    group.add_argument(\"--algorithm-name\", type=str, default='ppo', choices=[\"ppo\", \"mappo\"],\n\t                       help=\"Specifiy the algorithm (default ppo)\")\n\t    group.add_argument(\"--experiment-name\", type=str, default=\"check\",\n\t                       help=\"An identifier to distinguish different experiment.\")\n", "    group.add_argument(\"--seed\", type=int, default=1,\n\t                       help=\"Random seed for numpy/torch\")\n\t    group.add_argument(\"--cuda\", action='store_true', default=False,\n\t                       help=\"By default False, will use CPU to train; or else will use GPU;\")\n\t    group.add_argument(\"--num-env-steps\", type=float, default=1e7,\n\t                       help='Number of environment steps to train (default: 1e7)')\n\t    group.add_argument(\"--model-dir\", type=str, default=None,\n\t                       help=\"By default None. set the path to pretrained model.\")\n\t    group.add_argument(\"--use-wandb\", action='store_true', default=False,\n\t                       help=\"[for wandb usage], by default False, if set, will log date to wandb server.\")\n", "    group.add_argument(\"--user-name\", type=str, default='jyh',\n\t                       help=\"[for wandb usage], to specify user's name for simply collecting training data.\")\n\t    group.add_argument(\"--wandb-name\", type=str, default='jyh',\n\t                       help=\"[for wandb usage], to specify user's name for simply collecting training data.\")\n\t    group.add_argument(\"--capture-video\", action='store_true', default=False,\n\t                       help=\"use wandb to capture video\")\n\t    return parser\n\tdef _get_replaybuffer_config(parser: argparse.ArgumentParser):\n\t    \"\"\"\n\t    Replay Buffer parameters:\n", "        --gamma <float>\n\t            discount factor for rewards (default: 0.99)\n\t        --buffer-size <int>\n\t            the maximum storage in the buffer.\n\t        --use-proper-time-limits\n\t            by default, the return value does consider limits of time. If set, compute returns with considering time limits factor.\n\t        --use-gae\n\t            by default, use generalized advantage estimation. If set, do not use gae.\n\t        --gae-lambda <float>\n\t            gae lambda parameter (default: 0.95)\n", "    \"\"\"\n\t    group = parser.add_argument_group(\"Replay Buffer parameters\")\n\t    group.add_argument(\"--gamma\", type=float, default=0.99,\n\t                       help='discount factor for rewards (default: 0.99)')\n\t    group.add_argument(\"--buffer-size\", type=int, default=200,\n\t                       help=\"maximum storage in the buffer.\")\n\t    group.add_argument(\"--use-gae\", action='store_false', default=True,\n\t                       help='Whether to use generalized advantage estimation')\n\t    group.add_argument(\"--gae-lambda\", type=float, default=0.95,\n\t                       help='gae lambda parameter (default: 0.95)')\n", "    group.add_argument(\"--use-risk-sensitive\", action='store_true', default=False)\n\t    group.add_argument(\"--use-reward-hyper\", action='store_true', default=False)\n\t    group.add_argument(\"--tau-list\", type=str, default=\"0.1 0.4 0.5 0.6 0.9\")\n\t    group.add_argument(\"--reward-list\", type=str, default=\"0.05 0.2 1.0 5.0 20.0\")\n\t    return parser\n\tdef _get_network_config(parser: argparse.ArgumentParser):\n\t    \"\"\"\n\t    Network parameters:\n\t        --hidden-size <str>\n\t            dimension of hidden layers for mlp pre-process networks\n", "        --act-hidden-size <int>\n\t            dimension of hidden layers for actlayer\n\t        --activation-id\n\t            choose 0 to use Tanh, 1 to use ReLU, 2 to use LeakyReLU, 3 to use ELU\n\t        --use-feature-normalization\n\t            by default False, otherwise apply LayerNorm to normalize feature extraction inputs.\n\t        --gain\n\t            by default 0.01, use the gain # of last action layer\n\t    \"\"\"\n\t    group = parser.add_argument_group(\"Network parameters\")\n", "    group.add_argument(\"--hidden-size\", type=str, default='128 128',\n\t                       help=\"Dimension of hidden layers for mlp pre-process networks (default '128 128')\")\n\t    group.add_argument(\"--act-hidden-size\", type=str, default='128 128',\n\t                       help=\"Dimension of hidden layers for actlayer (default '128 128')\")\n\t    group.add_argument(\"--activation-id\", type=int, default=1,\n\t                       help=\"Choose 0 to use Tanh, 1 to use ReLU, 2 to use LeakyReLU, 3 to use ELU (default 1)\")\n\t    group.add_argument(\"--use-feature-normalization\", action='store_true', default=False,\n\t                       help=\"Whether to apply LayerNorm to the feature extraction inputs\")\n\t    group.add_argument(\"--gain\", type=float, default=0.01,\n\t                       help=\"The gain # of last action layer\")\n", "    group.add_argument(\"--use-cnn\", action='store_true', default=False,\n\t                       help=\"Whether to use cnn\")\n\t    return parser\n\tdef _get_recurrent_config(parser: argparse.ArgumentParser):\n\t    \"\"\"\n\t    Recurrent parameters:\n\t        --use-recurrent-policy\n\t            by default, use Recurrent Policy. If set, do not use.\n\t        --recurrent-hidden-size <int>\n\t            Dimension of hidden layers for recurrent layers (default 128).\n", "        --recurrent-hidden-layers <int>\n\t            The number of recurrent layers (default 1).\n\t        --data-chunk-length <int>\n\t            Time length of chunks used to train a recurrent_policy, default 10.\n\t    \"\"\"\n\t    group = parser.add_argument_group(\"Recurrent parameters\")\n\t    group.add_argument(\"--use-recurrent-policy\", action='store_false', default=True,\n\t                       help='Whether to use a recurrent policy')\n\t    group.add_argument(\"--recurrent-hidden-size\", type=int, default=128,\n\t                       help=\"Dimension of hidden layers for recurrent layers (default 128)\")\n", "    group.add_argument(\"--recurrent-hidden-layers\", type=int, default=1,\n\t                       help=\"The number of recurrent layers (default 1)\")\n\t    group.add_argument(\"--data-chunk-length\", type=int, default=10,\n\t                       help=\"Time length of chunks used to train a recurrent_policy (default 10)\")\n\t    return parser\n\tdef _get_optimizer_config(parser: argparse.ArgumentParser):\n\t    \"\"\"\n\t    Optimizer parameters:\n\t        --lr <float>\n\t            learning rate parameter (default: 3e-4, fixed).\n", "    \"\"\"\n\t    group = parser.add_argument_group(\"Optimizer parameters\")\n\t    group.add_argument(\"--lr\", type=float, default=3e-4,\n\t                       help='learning rate (default: 3e-4)')\n\t    return parser\n\tdef _get_ppo_config(parser: argparse.ArgumentParser):\n\t    \"\"\"\n\t    PPO parameters:\n\t        --ppo-epoch <int>\n\t            number of ppo epochs (default: 10)\n", "        --clip-param <float>\n\t            ppo clip parameter (default: 0.2)\n\t        --use-clipped-value-loss\n\t            by default false. If set, clip value loss.\n\t        --num-mini-batch <int>\n\t            number of batches for ppo (default: 1)\n\t        --value-loss-coef <float>\n\t            ppo value loss coefficient (default: 1)\n\t        --entropy-coef <float>\n\t            ppo entropy term coefficient (default: 0.01)\n", "        --use-max-grad-norm\n\t            by default, use max norm of gradients. If set, do not use.\n\t        --max-grad-norm <float>\n\t            max norm of gradients (default: 0.5)\n\t    \"\"\"\n\t    group = parser.add_argument_group(\"PPO parameters\")\n\t    group.add_argument(\"--ppo-epoch\", type=int, default=4,\n\t                       help='number of ppo epochs (default: 4)')\n\t    group.add_argument(\"--clip-param\", type=float, default=0.2,\n\t                       help='ppo clip parameter (default: 0.2)')\n", "    group.add_argument(\"--use-clipped-value-loss\", action='store_true', default=False,\n\t                       help=\"By default false. If set, clip value loss.\")\n\t    group.add_argument(\"--num-mini-batch\", type=int, default=5,\n\t                       help='number of batches for ppo (default: 5)')\n\t    group.add_argument(\"--value-loss-coef\", type=float, default=1,\n\t                       help='ppo value loss coefficient (default: 1)')\n\t    group.add_argument(\"--entropy-coef\", type=float, default=0.01,\n\t                       help='entropy term coefficient (default: 0.01)')\n\t    group.add_argument(\"--use-max-grad-norm\", action='store_false', default=True,\n\t                       help=\"By default, use max norm of gradients. If set, do not use.\")\n", "    group.add_argument(\"--max-grad-norm\", type=float, default=2,\n\t                       help='max norm of gradients (default: 2)')\n\t    return parser\n\tdef _get_selfplay_config(parser: argparse.ArgumentParser):\n\t    \"\"\"\n\t    Selfplay parameters:\n\t        --use-selfplay\n\t            by default false. If set, use selfplay algorithms.\n\t        --selfplay-algorithm <str>\n\t            specifiy the selfplay algorithm, including `[\"sp\", \"fsp\"]`\n", "        --n-choose-opponents <int>\n\t            number of different opponents chosen for rollout. (default 1)\n\t        --init-elo <float>\n\t            initial ELO for policy performance. (default 1000.0)\n\t    \"\"\"\n\t    group = parser.add_argument_group(\"Selfplay parameters\")\n\t    group.add_argument(\"--use-selfplay\", action='store_true', default=False,\n\t                       help=\"By default false. If set, use selfplay algorithms.\")\n\t    group.add_argument(\"--selfplay-algorithm\", type=str, default=\"fsp\", help=\"selfplay algorithm\")\n\t    group.add_argument(\"--random-side\", action='store_true', default=False, help=\"random agent side when env reset\")\n", "    group.add_argument('--init-elo', type=float, default=1000.0,\n\t                       help=\"initial ELO for policy performance. (default 1000.0)\")\n\t    return parser\n\tdef _get_pbt_config(parser: argparse.ArgumentParser):\n\t    \"\"\"\n\t    PBT parameters:\n\t        --population-size\n\t            by default 1\n\t        --num-parallel-each-agent \n\t            by default 4\n", "    \"\"\"\n\t    group = parser.add_argument_group(\"PBT parameters\")\n\t    group.add_argument(\"--population-size\", type=int, default=1,\n\t                       help=\"number of agents in the population\")\n\t    group.add_argument(\"--num-parallel-each-agent\", type=int, default=1,\n\t                       help=\"number of subprocesses for each agent\")\n\t    group.add_argument(\"--exploit-elo-threshold\", type=int, default=500,\n\t                       help=\"\")\n\t    return parser\n\tdef _get_eval_config(parser: argparse.ArgumentParser):\n", "    \"\"\"\n\t    Eval parameters:\n\t        --eval-episodes <int>\n\t            number of episodes of a single evaluation.\n\t    \"\"\"\n\t    group = parser.add_argument_group(\"Eval parameters\")\n\t    group.add_argument(\"--eval-episodes\", type=int, default=1,\n\t                       help=\"number of episodes of a single evaluation. (default 1)\")\n\t    return parser\n\tif __name__ == \"__main__\":\n", "    parser = get_config()\n\t    all_args = parser.parse_args()\n"]}
{"filename": "main_selfplay_test.py", "chunked_list": ["import gym\n\timport torch\n\timport numpy as np\n\timport os\n\timport sys\n\timport logging\n\timport time\n\tfrom pathlib import Path\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom ppo.ppo_trainer import PPOTrainer\n", "from ppo.ppo_policy import PPOPolicy\n\tfrom ppo.ppo_data_collectors import BaseDataCollector, SelfPlayDataCollector, make_env\n\tfrom config import get_config\n\timport envs\n\timport wandb\n\tdef main(args):\n\t    parser = get_config()\n\t    all_args = parser.parse_known_args(args)[0]\n\t    all_args.buffer_size = 1000\n\t    all_args.env_name = 'SumoAnts-v0'\n", "    all_args.eval_episodes = 1\n\t    all_args.num_env_steps = 1e6\n\t    all_args.num_mini_batch = 1\n\t    all_args.ppo_epoch = 4\n\t    all_args.cuda = True\n\t    all_args.lr = 1e-4\n\t    # all_args.use_risk_sensitive = True\n\t    all_args.use_gae = True\n\t    all_args.tau = 0.5\n\t    all_args.seed = 0\n", "    all_args.use_wandb = False\n\t    all_args.capture_video = False\n\t    all_args.env_id = 0\n\t    str_time = time.strftime(\"%b%d-%H%M%S\", time.localtime())\n\t    run_dir = Path(os.path.dirname(os.path.abspath(__file__))) / \"runs\" / all_args.env_name / all_args.experiment_name\n\t    if all_args.use_wandb:\n\t        wandb.init(\n\t            project=all_args.env_name, \n\t            entity=all_args.wandb_name, \n\t            name=all_args.experiment_name, \n", "            monitor_gym=True, \n\t            config=all_args,\n\t            dir=str(run_dir))\n\t        s = str(wandb.run.dir).split('/')[:-1]\n\t        s.append('models')\n\t        save_dir= '/'.join(s)\n\t    else:\n\t        run_dir = run_dir / str_time\n\t        save_dir = str(run_dir)\n\t    if not os.path.exists(save_dir):\n", "        os.makedirs(save_dir)\n\t    all_args.save_dir = save_dir\n\t    env = make_env(all_args.env_name)\n\t    collector = SelfPlayDataCollector(all_args)\n\t    trainer = PPOTrainer(all_args, env.observation_space, env.action_space)\n\t    # writer = SummaryWriter(run_dir)\n\t    # writer.add_text(\n\t    #     \"hyperparameters\",\n\t    #     \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(all_args).items()])),\n\t    # )\n", "    torch.save(trainer.policy.params(), f\"{save_dir}/agent_0.pt\")\n\t    num_epochs = int(all_args.num_env_steps // all_args.buffer_size)\n\t    for epoch in range(num_epochs):\n\t        # train\n\t        params = torch.load(f\"{str(save_dir)}/agent_{epoch}.pt\")\n\t        buffer = collector.collect_data(ego_params=params, enm_params=params, hyper_params={'tau':0.5})\n\t        params, train_info = trainer.train(params=params, buffer=buffer)\n\t        # eval and record info\n\t        elo_gain, eval_info = collector.evaluate_data(ego_params=params, enm_params=params)\n\t        cur_steps = (epoch + 1) * all_args.buffer_size\n", "        info = {**train_info, **eval_info}\n\t        if all_args.use_wandb:\n\t            for k, v in info.items():\n\t                wandb.log({k: v}, step=cur_steps)\n\t        train_reward, eval_reward = train_info['episode_reward'], eval_info['episode_reward']\n\t        print(f\"Epoch {epoch} / {num_epochs} , train episode reward {train_reward}, evaluation episode reward {eval_reward}\")\n\t        # save\n\t        torch.save(params, f\"{str(save_dir)}/agent_{epoch+1}.pt\")\n\t        # save\n\t    # writer.close()\n", "if __name__ == '__main__':\n\t    main(sys.argv[1:])\n"]}
{"filename": "main_pbt_selfplay.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport ray\n\timport time\n\timport os\n\timport sys\n\timport gym\n\timport random\n\timport logging\n\timport wandb\n", "import setproctitle\n\tfrom config import get_config\n\timport envs\n\tfrom pathlib import Path\n\tfrom ppo.ppo_data_collectors import DataCollectorMix, make_env\n\tfrom ppo.ppo_trainer import PBTPPOTrainer, PPOTrainer\n\tfrom ppo.ppo_policy import PPOPolicy\n\tfrom util.util_population import population_based_exploit_explore\n\tfrom util.util_selfplay import get_algorithm\n\t'''\n", "Population: N agents\n\t    for each agent, we have M data-collector, 1 trainer:\n\t    for each trainning step t:\n\t        for each agent i:\n\t            1. select K opponent to collect S samples\n\t            2. run_results = data-collector.collect.remote(ego_model, enm_model, hypyer_param)\n\t                2.1 load model\n\t                2.2 collect data via inner environment\n\t                2.3 save data into inner PPOBuffer\n\t                2.4 return PPOBuffer\n", "        3. buffer_list = [[ray.get(run_results) for _ in range(M)] for _ in range(N)]\n\t        for each agent i:\n\t            4. info = trainer.update.remote(ego_model, buffer_list, hyper_param)\n\t                4.1 load model\n\t                4.2 ppo update with buffer data\n\t                4.3 save model to file\n\t                4.4 return trainning info\n\t        5. info = [[ray.get(info) for _ in range(M)] for _ in range(N)]\n\t        6. eval + elo_update\n\t        7. population exploit\n", "'''\n\tdef load_enm_params(run_dir: str, enm_idx: tuple):\n\t    agent_id, t = enm_idx\n\t    return torch.load(f\"{run_dir}/agent{agent_id}_history{t}.pt\", map_location=torch.device('cpu'))\n\tdef main(args):\n\t    # init config\n\t    parser = get_config()\n\t    all_args = parser.parse_known_args(args)[0]\n\t    random.seed(all_args.seed)\n\t    np.random.seed(all_args.seed)\n", "    torch.manual_seed(all_args.seed)\n\t    torch.cuda.manual_seed(all_args.seed)\n\t    tau_hyper = [float(tau) for tau in all_args.tau_list.split(' ')]\n\t    reward_hyper = [float(r) for r in all_args.reward_list.split(' ')]\n\t    setproctitle.setproctitle(str(all_args.env_name)+'@'+ str(all_args.user_name))\n\t    env = make_env(all_args.env_name)\n\t    str_time = time.strftime(\"%b%d-%H%M%S\", time.localtime())\n\t    run_dir = Path(os.path.dirname(os.path.abspath(__file__))) / \"runs\" / all_args.env_name / all_args.experiment_name\n\t    if not os.path.exists(run_dir):\n\t        os.makedirs(run_dir)\n", "    if all_args.use_wandb:\n\t        wandb.init(\n\t            project=all_args.env_name, \n\t            entity=all_args.wandb_name, \n\t            name=all_args.experiment_name, \n\t            group=all_args.experiment_name,\n\t            job_type='charts',\n\t            config=all_args,\n\t            dir=str(run_dir))\n\t        s = str(wandb.run.dir).split('/')[:-1]\n", "        s.append('models')\n\t        save_dir= '/'.join(s)\n\t    else:\n\t        run_dir = run_dir / str_time\n\t        save_dir = str(run_dir)\n\t    if not os.path.exists(save_dir):\n\t        os.makedirs(save_dir)\n\t    all_args.save_dir = save_dir\n\t    logging.basicConfig(\n\t        level=logging.INFO,\n", "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n\t        handlers=[\n\t            logging.FileHandler(save_dir + \"_ouput.log\"),\n\t            logging.StreamHandler()\n\t        ]\n\t    )\n\t    # init population\n\t    ray.init()\n\t    selfplay_algo = get_algorithm(all_args.selfplay_algorithm)\n\t    population = {}\n", "    population_elos = {}\n\t    population_hypers = {}\n\t    for agent_id in range(all_args.population_size):\n\t        population[agent_id] = PPOPolicy(all_args, env.observation_space, env.action_space)\n\t        if all_args.model_dir is not None:\n\t            population[agent_id].restore_from_path(all_args.model_dir, epoch='latest')\n\t        params = population[agent_id].params()\n\t        torch.save(params, f'{save_dir}/agent{agent_id}_history0.pt')\n\t        torch.save(params, f'{save_dir}/agent{agent_id}_latest.pt')\n\t        population_elos[agent_id] = {0: all_args.init_elo}\n", "        population_hypers[agent_id] = {}\n\t        if all_args.use_risk_sensitive:\n\t            population_hypers[agent_id]['tau'] = tau_hyper[agent_id]\n\t        if all_args.use_reward_hyper:\n\t            population_hypers[agent_id]['reward'] = reward_hyper[agent_id]\n\t    M = all_args.num_parallel_each_agent\n\t    N = all_args.population_size\n\t    data_collector_pools = []\n\t    for agent_id in range(N):\n\t        data_collectors = []\n", "        for i in range(M):\n\t            all_args.env_id = agent_id * M + i\n\t            data_collectors.append(DataCollectorMix.remote(all_args))\n\t        data_collector_pools.append(data_collectors)\n\t    ppo_trainers = [PBTPPOTrainer.remote(all_args, env.observation_space, env.action_space) for _ in range(N)]\n\t    logging.info(\"Init over.\")\n\t    num_epochs = int(all_args.num_env_steps // M // all_args.buffer_size)\n\t    for epoch in range(num_epochs):\n\t        cur_steps = epoch * all_args.buffer_size * M\n\t        # data collect\n", "        data_results = []\n\t        for agent_id in range(N):\n\t            enm_idxs, enm_elos = selfplay_algo.choose_opponents(agent_id, population_elos, M)\n\t            ego_model = population[agent_id].params(device='cpu')\n\t            results = []\n\t            for i in range(M):  \n\t                enm_model = load_enm_params(save_dir, enm_idxs[i])\n\t                res = data_collector_pools[agent_id][i].collect_data.remote(\n\t                    ego_params=ego_model, \n\t                    enm_params=enm_model, \n", "                    hyper_params=population_hypers[agent_id]\n\t                )\n\t                results.append(res)\n\t            data_results.append(results)\n\t        buffers = [[ray.get(data_results[agent_id][i]) for i in range(M)] for agent_id in range(N)]\n\t        # ppo train\n\t        train_results = []\n\t        for agent_id in range(N):\n\t            ego_model = population[agent_id].params(device='cuda')\n\t            res = ppo_trainers[agent_id].train.remote(\n", "                buffer=buffers[agent_id], \n\t                params=ego_model, \n\t                hyper_params=population_hypers[agent_id]\n\t            )\n\t            train_results.append(res)\n\t        train_infos = [ray.get(train_results[i]) for i in range(N)]\n\t        for agent_id, (param, info) in enumerate(train_infos):\n\t            population[agent_id].restore_from_params(param)\n\t            torch.save(param, f'{save_dir}/agent{agent_id}_history{epoch+1}.pt')\n\t            torch.save(param, f'{save_dir}/agent{agent_id}_latest.pt')\n", "            train_reward = info[\"episode_reward\"]\n\t            logging.info(f\"Epoch {epoch} / {num_epochs}, Agent{agent_id}, train episode reward {train_reward}\")\n\t            if all_args.use_wandb and agent_id == 0:\n\t                for k, v in info.items():\n\t                    wandb.log({k: v}, step=cur_steps)\n\t        # evaluate and update elo\n\t        eval_results = []\n\t        enm_infos = {}\n\t        for agent_id in range(N):\n\t            enm_idxs, enm_elos = selfplay_algo.choose_opponents(agent_id, population_elos, M)\n", "            enm_infos[agent_id] = enm_idxs\n\t            ego_model = population[agent_id].params(device='cpu')\n\t            results = []\n\t            for i in range(M):  \n\t                enm_model = load_enm_params(save_dir, enm_idxs[i])\n\t                res = data_collector_pools[agent_id][i].evaluate_data.remote(\n\t                    ego_params=ego_model, \n\t                    enm_params=enm_model, \n\t                    hyper_params=population_hypers[agent_id],\n\t                    ego_elo=population_elos[agent_id][epoch],\n", "                    enm_elo=enm_elos[i],\n\t                )\n\t                results.append(res)\n\t            eval_results.append(results)\n\t        eval_datas = [[ray.get(eval_results[agent_id][i]) for i in range(M)] for agent_id in range(N)]\n\t        for agent_id in range(N):\n\t            elo_gains, eval_infos = list(zip(*eval_datas[agent_id]))\n\t            population_elos[agent_id][epoch+1] = population_elos[agent_id][epoch]\n\t            for i, (enm_id, enm_t) in enumerate(enm_infos[agent_id]):\n\t                population_elos[agent_id][epoch+1] += elo_gains[i]\n", "                population_elos[enm_id][enm_t] -= elo_gains[i]\n\t            eval_reward = np.mean([info['episode_reward'] for info in eval_infos])\n\t            logging.info(f\"Epoch {epoch} / {num_epochs}, Agent{agent_id}, eval episode reward {eval_reward}\")\n\t            if all_args.use_wandb and agent_id == 0:\n\t                wandb.log({\"eval_episode_reward\": eval_reward}, step=cur_steps)\n\t        # exploit and explore\n\t        population_elos, population_hypers = population_based_exploit_explore(epoch, population_elos, population_hypers, save_dir, all_args.exploit_elo_threshold)\n\t        # save checkoutpoint\n\t        checkpoint = {\n\t            \"epoch\": epoch,\n", "            \"population_elos\": population_elos,\n\t            \"population_hypers\": population_hypers\n\t        }\n\t        if all_args.use_wandb:\n\t            for agent_id in range(N):\n\t                wandb.log({f\"agent{agent_id}_tau\": population_hypers[agent_id]['tau']}, step=cur_steps)\n\t                wandb.log({f\"agent{agent_id}_elo\": population_elos[agent_id][epoch]}, step=cur_steps)\n\t        torch.save(checkpoint, f\"{save_dir}/checkpoint_latest.pt\")\n\tif __name__ == \"__main__\":\n\t    main(sys.argv[1:])\n"]}
{"filename": "render/render_sumoant.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport gym\n\timport os\n\timport sys\n\tfrom pathlib import Path\n\tbase_dir = str(Path(__file__).resolve().parent.parent)\n\tsys.path.append(base_dir)\n\timport envs\n\tfrom PIL import Image\n", "from config import get_config\n\tfrom ppo.ppo_buffer import PPOBuffer\n\tfrom ppo.ppo_policy import PPOPolicy\n\tfrom gym.wrappers import RecordVideo\n\tfrom ppo.ppo_data_collectors import make_env\n\tdef _t2n(x):\n\t    return x.detach().cpu().numpy()\n\tepisode_rewards = 0\n\trender_video = True\n\trender_image = False\n", "num_agents = 2\n\targs = sys.argv[1:]\n\tparser = get_config()\n\tall_args = parser.parse_known_args(args)[0]\n\tall_args.env_name = 'SumoAnts-v0'\n\tenv = make_env(all_args.env_name)\n\tego_policy = PPOPolicy(all_args, env.observation_space, env.action_space)\n\tenm_policy = PPOPolicy(all_args, env.observation_space, env.action_space)\n\tego_dir = \"\"\n\tenm_dir = \"\"\n", "ego_path = \"\"\n\tenm_path = \"\"\n\tego_policy.restore_from_params(torch.load(ego_dir+ego_path))\n\tenm_policy.restore_from_params(torch.load(enm_dir+enm_path))\n\tif render_video == True:\n\t    env = RecordVideo(env, video_folder=\"render/render_videos\", name_prefix=f\"0.1vs0.9\")\n\tenv.seed(0)\n\tprint(\"Start render\")\n\tobs = env.reset()\n\tstep = 0\n", "if render_image and step % 1 == 0:\n\t    arr = env.render(mode=\"rgb_array\")\n\t    img = Image.fromarray(arr)\n\t    img.save(f\"render/images/step{step}.png\")\n\tego_rnn_states = np.zeros((1, 1, 128), dtype=np.float32)\n\tenm_obs =  obs[num_agents // 2:, ...]\n\tego_obs =  obs[:num_agents // 2, ...]\n\tenm_rnn_states = np.zeros_like(ego_rnn_states, dtype=np.float32)\n\tmasks = np.ones((num_agents//2, 1))\n\twhile True:\n", "    step += 1\n\t    # env.render()\n\t    ego_actions, ego_rnn_states = ego_policy.act(ego_obs, ego_rnn_states, masks, deterministic=False)\n\t    ego_actions = _t2n(ego_actions)\n\t    ego_rnn_states = _t2n(ego_rnn_states)\n\t    # print(ego_actions)\n\t    enm_actions, enm_rnn_states = enm_policy.act(enm_obs, enm_rnn_states, masks, deterministic=False)\n\t    enm_actions = _t2n(enm_actions)\n\t    enm_rnn_states = _t2n(enm_rnn_states)\n\t    actions = np.concatenate((ego_actions, enm_actions), axis=0)\n", "    obs, rewards, dones, infos = env.step(actions)\n\t    rewards = rewards[:num_agents // 2, ...]\n\t    episode_rewards += rewards\n\t    if render_image and step % 1 == 0:\n\t        arr = env.render(mode=\"rgb_array\")\n\t        img = Image.fromarray(arr)\n\t        img.save(f\"render/images/step{step}.png\")\n\t    if dones.all():\n\t        print(infos)\n\t        break\n", "    enm_obs =  obs[num_agents // 2:, ...]\n\t    ego_obs =  obs[:num_agents // 2, ...]\n"]}
{"filename": "ppo/ppo_trainer.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport numpy as np\n\timport ray\n\tfrom typing import Union, List\n\tfrom util.util_util import check, get_gard_norm\n\tfrom ppo.ppo_policy import PPOPolicy\n\tfrom ppo.ppo_buffer import PPOBuffer\n\tclass PPOTrainer():\n\t    def __init__(self, args, obs_space, act_space):\n", "        self.device = torch.device(\"cuda:0\") if args.cuda and torch.cuda.is_available() \\\n\t                                            else torch.device(\"cpu\")\n\t        self.tpdv = dict(dtype=torch.float32, device=self.device)\n\t        # ppo config\n\t        self.ppo_epoch = args.ppo_epoch\n\t        self.clip_param = args.clip_param\n\t        self.use_clipped_value_loss = args.use_clipped_value_loss\n\t        self.num_mini_batch = args.num_mini_batch\n\t        self.value_loss_coef = args.value_loss_coef\n\t        self.entropy_coef = args.entropy_coef\n", "        self.use_max_grad_norm = args.use_max_grad_norm\n\t        self.max_grad_norm = args.max_grad_norm\n\t        # rnn configs\n\t        self.use_recurrent_policy = args.use_recurrent_policy\n\t        self.data_chunk_length = args.data_chunk_length\n\t        self.buffer_size = args.buffer_size\n\t        self.policy = PPOPolicy(args, obs_space, act_space)\n\t    def ppo_update(self, sample):\n\t        obs_batch, actions_batch, masks_batch, old_action_log_probs_batch, advantages_batch, \\\n\t            returns_batch, value_preds_batch, rnn_states_actor_batch, rnn_states_critic_batch = sample\n", "        old_action_log_probs_batch = check(old_action_log_probs_batch).to(**self.tpdv)\n\t        advantages_batch = check(advantages_batch).to(**self.tpdv)\n\t        returns_batch = check(returns_batch).to(**self.tpdv)\n\t        value_preds_batch = check(value_preds_batch).to(**self.tpdv)\n\t        # Reshape to do in a single forward pass for all steps\n\t        values, action_log_probs, dist_entropy = self.policy.evaluate_actions(obs_batch,\n\t                                                                         rnn_states_actor_batch,\n\t                                                                         rnn_states_critic_batch,\n\t                                                                         actions_batch,\n\t                                                                         masks_batch)\n", "        # Obtain the loss function\n\t        ratio = torch.exp(action_log_probs - old_action_log_probs_batch)\n\t        surr1 = ratio * advantages_batch\n\t        surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantages_batch\n\t        policy_loss = torch.sum(torch.min(surr1, surr2), dim=-1, keepdim=True)\n\t        policy_loss = -policy_loss.mean()\n\t        if self.use_clipped_value_loss:\n\t            value_pred_clipped = value_preds_batch + (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)\n\t            value_losses = (values - returns_batch).pow(2)\n\t            value_losses_clipped = (value_pred_clipped - returns_batch).pow(2)\n", "            value_loss = 0.5 * torch.max(value_losses, value_losses_clipped)\n\t        else:\n\t            value_loss = 0.5 * (returns_batch - values).pow(2)\n\t        value_loss = value_loss.mean()\n\t        policy_entropy_loss = -dist_entropy.mean()\n\t        loss = policy_loss + value_loss * self.value_loss_coef + policy_entropy_loss * self.entropy_coef\n\t        # Optimize the loss function\n\t        self.policy.optimizer.zero_grad()\n\t        loss.backward()\n\t        if self.use_max_grad_norm:\n", "            actor_grad_norm = nn.utils.clip_grad_norm_(self.policy.actor.parameters(), self.max_grad_norm).item()\n\t            critic_grad_norm = nn.utils.clip_grad_norm_(self.policy.critic.parameters(), self.max_grad_norm).item()\n\t        else:\n\t            actor_grad_norm = get_gard_norm(self.policy.actor.parameters())\n\t            critic_grad_norm = get_gard_norm(self.policy.critic.parameters())\n\t        self.policy.optimizer.step()\n\t        return policy_loss, value_loss, policy_entropy_loss, ratio, actor_grad_norm, critic_grad_norm\n\t    def train(self, buffer: Union[List[PPOBuffer], PPOBuffer], params, hyper_params={}):\n\t        if 'entropy' in hyper_params:\n\t            self.entropy_coef = hyper_params['entropy']\n", "        self.policy.restore_from_params(params)\n\t        buffer = [buffer] if isinstance(buffer, PPOBuffer) else buffer\n\t        train_info = {}\n\t        train_info['value_loss'] = 0\n\t        train_info['policy_loss'] = 0\n\t        train_info['policy_entropy_loss'] = 0\n\t        train_info['actor_grad_norm'] = 0\n\t        train_info['critic_grad_norm'] = 0\n\t        train_info['ratio'] = 0\n\t        train_info['episode_reward'] = 0\n", "        train_info['episode_length'] = 0\n\t        for _ in range(self.ppo_epoch):\n\t            if self.use_recurrent_policy:\n\t                data_generator = PPOBuffer.recurrent_generator(buffer, self.num_mini_batch, self.data_chunk_length)\n\t            else:\n\t                raise NotImplementedError\n\t            for sample in data_generator:\n\t                policy_loss, value_loss, policy_entropy_loss, ratio, \\\n\t                    actor_grad_norm, critic_grad_norm = self.ppo_update(sample)\n\t                train_info['value_loss'] += value_loss.item()\n", "                train_info['policy_loss'] += policy_loss.item()\n\t                train_info['policy_entropy_loss'] += policy_entropy_loss.item()\n\t                train_info['actor_grad_norm'] += actor_grad_norm\n\t                train_info['critic_grad_norm'] += critic_grad_norm\n\t                train_info['ratio'] += ratio.mean().item()\n\t        num_updates = self.ppo_epoch * self.num_mini_batch\n\t        for k in train_info.keys():\n\t            train_info[k] /= num_updates\n\t        episode_dones = max(np.sum(np.concatenate([buf.masks==0 for buf in buffer])),1)\n\t        episode_reward = np.sum(np.concatenate([buf.rewards for buf in buffer])) / episode_dones\n", "        episode_length = self.buffer_size * len(buffer) / episode_dones\n\t        train_info['episode_reward'] = episode_reward\n\t        train_info['episode_length'] = episode_length\n\t        return self.policy.params(), train_info\n\t@ray.remote(num_gpus=0.2)\n\tclass PBTPPOTrainer(PPOTrainer):\n\t    def __init__(self, args, obs_space, act_space):\n\t        PPOTrainer.__init__(self, args, obs_space, act_space)\n"]}
{"filename": "ppo/ppo_data_collectors.py", "chunked_list": ["import imp\n\timport numpy as np\n\timport torch\n\timport time\n\timport gym\n\timport ray\n\timport envs\n\tfrom ppo.ppo_buffer import PPOBuffer\n\tfrom ppo.ppo_policy import PPOPolicy\n\tfrom gym.core import Wrapper\n", "import wandb\n\tdef _t2n(x):\n\t    return x.detach().cpu().numpy()\n\tclass GymEnv(Wrapper):\n\t    def __init__(self, env):\n\t        super().__init__(env)\n\t        self.action_shape = self.env.action_space.shape\n\t        self.num_agents = 1\n\t    def reset(self):\n\t        observation = self.env.reset()\n", "        return np.array(observation).reshape((1, -1))\n\t    def step(self, action):\n\t        action = np.array(action).reshape(self.action_shape)\n\t        observation, reward, done, info = self.env.step(action)\n\t        observation = np.array(observation).reshape((1, -1))\n\t        done = np.array(done).reshape((1,-1))\n\t        reward = np.array(reward).reshape((1, -1))\n\t        info['score'] = 0.5\n\t        return observation, reward, done, info\n\tdef make_env(env_name):\n", "    if env_name in ['Ant-v2', 'Hopper-v2']: # single-agent\n\t        env = GymEnv(gym.make(env_name))\n\t    else: # selfplay\n\t        env = gym.make(env_name)\n\t    return env\n\tclass BaseDataCollector(object):\n\t    def __init__(self, args) -> None:\n\t        self.args = args\n\t        self.env = make_env(args.env_name)\n\t        if args.capture_video and args.use_wandb and args.env_id == 0:\n", "            wandb.init(\n\t                project=args.env_name, \n\t                entity=args.wandb_name, \n\t                name=args.experiment_name, \n\t                group=args.experiment_name,\n\t                job_type=\"video\",\n\t                monitor_gym=True)\n\t            self.env = gym.wrappers.RecordVideo(self.env, args.save_dir+\"_videos\")\n\t        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n\t        self.recurrent_hidden_size = args.recurrent_hidden_size\n", "        self.buffer_size = args.buffer_size\n\t        self.num_agents = getattr(self.env, 'num_agents', 1)\n\t        self.random_side = args.random_side\n\t        self.buffer = PPOBuffer(args, self.num_agents, self.env.observation_space, self.env.action_space)\n\t        self.ego = PPOPolicy(args, self.env.observation_space, self.env.action_space)\n\t    def collect_data(self, ego_params, hyper_params={}):\n\t        self.buffer.clear()\n\t        if 'tau' in hyper_params:\n\t            self.buffer.tau = hyper_params['tau'] \n\t        if 'reward' in hyper_params:\n", "            self.buffer.reward_hyper = hyper_params['reward']\n\t        self.ego.restore_from_params(ego_params)\n\t        obs = self.reset()\n\t        self.buffer.obs[0] = obs.copy()\n\t        for step in range(self.buffer_size):\n\t            # 1. get actions\n\t            values, actions, action_log_probs, rnn_states_actor, rnn_states_critic = \\\n\t                self.ego.get_actions(self.buffer.obs[step],\n\t                                     self.buffer.rnn_states_actor[step],\n\t                                     self.buffer.rnn_states_critic[step],\n", "                                     self.buffer.masks[step])\n\t            values = _t2n(values)\n\t            actions = _t2n(actions)\n\t            action_log_probs = _t2n(action_log_probs)\n\t            rnn_states_actor = _t2n(rnn_states_actor)\n\t            rnn_states_critic = _t2n(rnn_states_critic)\n\t            # 2. env step\n\t            obs, rewards, dones, info = self.step(actions)\n\t            if np.all(dones):\n\t                obs = self.reset()\n", "                rnn_states_actor = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n\t                rnn_states_critic = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n\t                masks = np.zeros((self.num_agents, 1), dtype=np.float32)\n\t            else:\n\t                masks = np.ones((self.num_agents, 1), dtype=np.float32)\n\t            # 3. insert experience in buffer\n\t            self.buffer.insert(obs, actions, rewards, masks, action_log_probs, values, rnn_states_actor, rnn_states_critic)\n\t        status_code = 0 if step > 0 else 1\n\t        last_value = self.ego.get_values(self.buffer.obs[-1], self.buffer.rnn_states_critic[-1], self.buffer.masks[-1])\n\t        self.buffer.compute_returns(_t2n(last_value))\n", "        return self.buffer\n\t    @torch.no_grad()\n\t    def evaluate_data(self, ego_params, hyper_params={}, ego_elo=0, enm_elo=0):\n\t        self.ego.restore_from_params(ego_params)\n\t        total_episode_rewards = []\n\t        eval_scores = []\n\t        max_episode_length = self.args.buffer_size\n\t        episode_reward = 0\n\t        eval_episodes = self.args.eval_episodes\n\t        for _ in range(eval_episodes):\n", "            obs = self.reset()\n\t            rnn_states_actor = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n\t            masks = np.ones((self.num_agents, 1))\n\t            step = 0\n\t            while True:\n\t                action, rnn_states_actor = self.ego.act(obs, rnn_states_actor, masks)\n\t                action = _t2n(action)\n\t                rnn_states_actor = _t2n(rnn_states_actor)\n\t                obs, reward, done, info = self.step(action)\n\t                step += 1\n", "                episode_reward += reward\n\t                if np.all(done):\n\t                    total_episode_rewards.append(episode_reward)\n\t                    episode_reward = 0\n\t                    if isinstance(info, tuple) or isinstance(info, list):\n\t                        score = info[0]['score']\n\t                    elif isinstance(info, dict):\n\t                        score = info['score']\n\t                    else:\n\t                        raise NotImplementedError\n", "                    eval_scores.append(score)\n\t                    break\n\t                if step >= max_episode_length:\n\t                    break\n\t        expected_score = 1 / (1 + 10 ** ((enm_elo - ego_elo) / 400))\n\t        elo_gain = 32 * (np.mean(eval_scores) - expected_score)\n\t        eval_info = {}\n\t        eval_info[\"episode_reward\"] = np.mean(total_episode_rewards)\n\t        return  elo_gain, eval_info\n\t    def reset(self):\n", "        return self.env.reset()\n\t    def step(self, action):\n\t        return self.env.step(action)\n\tclass SelfPlayDataCollector(BaseDataCollector):\n\t    def __init__(self, args):\n\t        super().__init__(args)\n\t        self.enm = PPOPolicy(args, self.env.observation_space, self.env.action_space)\n\t        self.num_agents = self.num_agents // 2\n\t        self.buffer = PPOBuffer(args, self.num_agents, self.env.observation_space, self.env.action_space)\n\t    def collect_data(self, ego_params, enm_params, hyper_params={}):\n", "        self.enm.restore_from_params(enm_params)\n\t        return super().collect_data(ego_params, hyper_params)\n\t    @torch.no_grad()\n\t    def evaluate_data(self, ego_params, enm_params, hyper_params={}, ego_elo=0, enm_elo=0):\n\t        self.enm.restore_from_params(enm_params)\n\t        return super().evaluate_data(ego_params, hyper_params, ego_elo, enm_elo)\n\t    def reset(self):\n\t        if self.random_side:\n\t            self.ego_side = np.random.randint(2) # shuffle rl control side\n\t        else:\n", "            self.ego_side = 0\n\t        self.enm_rnn_states_actor = np.zeros((self.num_agents, self.recurrent_hidden_layers, self.recurrent_hidden_size))\n\t        self.enm_masks = np.ones((self.num_agents, 1))\n\t        obs = super().reset()\n\t        return self._parse_obs(obs)\n\t    def step(self, action):\n\t        enm_action, enm_rnn_states_actor = self.enm.act(self.enm_obs, self.enm_rnn_states_actor, self.enm_masks)\n\t        enm_action = _t2n(enm_action)\n\t        self.enm_rnn_states_actor = _t2n(enm_rnn_states_actor)\n\t        actions = np.concatenate((action, enm_action), axis=0)\n", "        n_obs, rewards, done, info = self.env.step(actions)\n\t        if self.ego_side == 0:\n\t            ego_rewards = rewards[:self.num_agents, ...]\n\t            ego_done = done[:self.num_agents, ...]\n\t        else:\n\t            ego_rewards = rewards[self.num_agents:, ...]\n\t            ego_done = done[self.num_agents:, ...]\n\t        return self._parse_obs(n_obs), ego_rewards, ego_done, info\n\t    def _parse_obs(self, obs):\n\t        if self.ego_side == 0:\n", "            self.enm_obs = obs[self.num_agents:, ...]\n\t            ego_obs = obs[:self.num_agents, ...]\n\t        else:\n\t            self.enm_obs = obs[:self.num_agents, ...]\n\t            ego_obs = obs[self.num_agents:, ...]\n\t        return ego_obs\n\t@ray.remote(num_cpus=0.5)\n\tclass DataCollectorMix(object):\n\t    def __init__(self, args) -> None:\n\t        self.collector = None\n", "        self.mode = None\n\t        self.args = args\n\t    def set_collector(self, mode):\n\t        if self.mode == mode:\n\t            return\n\t        self.mode = mode\n\t        if self.collector is not None:\n\t            self.collector.close()\n\t        if self.mode == 'base':\n\t            self.collector = BaseDataCollector(self.args)\n", "        elif self.mode == 'selfplay':\n\t            self.collector = SelfPlayDataCollector(self.args)\n\t        else:\n\t            raise NotImplementedError\n\t    def collect_data(self, ego_params, enm_params=None, hyper_params={}):\n\t        if enm_params == None:\n\t            mode = 'base'\n\t        elif isinstance(enm_params, dict):\n\t            mode = 'selfplay'\n\t        else:\n", "            raise NotImplementedError\n\t        self.set_collector(mode)\n\t        if self.mode == 'base':\n\t            return self.collector.collect_data(ego_params, hyper_params)\n\t        elif self.mode == 'selfplay':\n\t            return self.collector.collect_data(ego_params, enm_params, hyper_params)\n\t        else: \n\t            raise NotImplementedError\n\t    def evaluate_data(self, ego_params, enm_params=None, hyper_params={}, ego_elo=0, enm_elo=0):\n\t        if enm_params == None:\n", "            mode = 'base'\n\t        elif isinstance(enm_params, dict):\n\t            mode = 'selfplay'\n\t        else:\n\t            raise NotImplementedError\n\t        self.set_collector(mode)\n\t        if self.mode == 'base':\n\t            return self.collector.evaluate_data(ego_params, hyper_params, ego_elo, enm_elo)\n\t        elif self.mode == 'selfplay':\n\t            return self.collector.evaluate_data(ego_params, enm_params, hyper_params, ego_elo, enm_elo)\n", "        else: \n\t            raise NotImplementedError"]}
{"filename": "ppo/ppo_policy.py", "chunked_list": ["import torch\n\tfrom ppo.ppo_actor import PPOActor\n\tfrom ppo.ppo_critic import PPOCritic\n\tclass PPOPolicy:\n\t    def __init__(self, args, obs_space, act_space):\n\t        self.args = args\n\t        self.device = torch.device(\"cuda:0\") if args.cuda and torch.cuda.is_available() \\\n\t                                            else torch.device(\"cpu\")\n\t        # optimizer config\n\t        self.lr = args.lr\n", "        self.obs_space = obs_space\n\t        self.act_space = act_space\n\t        self.actor = PPOActor(args, self.obs_space, self.act_space, self.device)\n\t        self.critic = PPOCritic(args, self.obs_space, self.device)\n\t        self.optimizer = torch.optim.Adam([\n\t            {'params': self.actor.parameters()},\n\t            {'params': self.critic.parameters()}\n\t        ], lr=self.lr)\n\t    def get_actions(self, obs, rnn_states_actor, rnn_states_critic, masks):\n\t        \"\"\"\n", "        Returns:\n\t            values, actions, action_log_probs, rnn_states_actor, rnn_states_critic\n\t        \"\"\"\n\t        actions, action_log_probs, rnn_states_actor = self.actor(obs, rnn_states_actor, masks)\n\t        values, rnn_states_critic = self.critic(obs, rnn_states_critic, masks)\n\t        return values, actions, action_log_probs, rnn_states_actor, rnn_states_critic\n\t    def get_values(self, obs, rnn_states_critic, masks):\n\t        \"\"\"\n\t        Returns:\n\t            values\n", "        \"\"\"\n\t        values, _ = self.critic(obs, rnn_states_critic, masks)\n\t        return values\n\t    def evaluate_actions(self, obs, rnn_states_actor, rnn_states_critic, action, masks, active_masks=None):\n\t        \"\"\"\n\t        Returns:\n\t            values, action_log_probs, dist_entropy\n\t        \"\"\"\n\t        action_log_probs, dist_entropy = self.actor.evaluate_actions(obs, rnn_states_actor, action, masks, active_masks)\n\t        values, _ = self.critic(obs, rnn_states_critic, masks)\n", "        return values, action_log_probs, dist_entropy\n\t    def act(self, obs, rnn_states_actor, masks, deterministic=False):\n\t        \"\"\"\n\t        Returns:\n\t            actions, rnn_states_actor\n\t        \"\"\"\n\t        actions, _, rnn_states_actor = self.actor(obs, rnn_states_actor, masks, deterministic)\n\t        return actions, rnn_states_actor\n\t    def prep_training(self):\n\t        self.actor.train()\n", "        self.critic.train()\n\t    def prep_rollout(self):\n\t        self.actor.eval()\n\t        self.critic.eval()\n\t    def copy(self):\n\t        return PPOPolicy(self.args, self.obs_space, self.act_space, self.device)\n\t    def params(self, device='cuda'):\n\t        checkpoint = {}\n\t        if device == 'cuda':\n\t            checkpoint['actor_state_dict'] = self.actor.state_dict()\n", "            checkpoint['critic_state_dict'] = self.critic.state_dict()\n\t        elif device == 'cpu':\n\t            checkpoint['actor_state_dict'] = {k: v.cpu() for k, v in self.actor.state_dict().items()}\n\t            checkpoint['critic_state_dict'] = {k: v.cpu() for k, v in self.critic.state_dict().items()}\n\t        return checkpoint\n\t    def save(self, rootpath, epoch=0):\n\t        checkpoint = {}\n\t        checkpoint['actor_state_dict'] = self.actor.state_dict()\n\t        checkpoint['critic_state_dict'] = self.critic.state_dict()\n\t        torch.save(checkpoint, str(rootpath) + f'/agent_{epoch}.pt')\n", "        torch.save(checkpoint, str(rootpath) + f'/agent_latest.pt')\n\t    def restore_from_path(self, rootpath, epoch=''):\n\t        checkpoint = torch.load(str(rootpath) + f'/agent0_{epoch}.pt')\n\t        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n\t        if 'critic_state_dict' in checkpoint.keys():\n\t            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n\t    def restore_from_params(self, params):\n\t        self.actor.load_state_dict(params['actor_state_dict'])\n\t        if 'critic_state_dict' in params.keys():\n\t            self.critic.load_state_dict(params['critic_state_dict'])"]}
{"filename": "ppo/ppo_buffer.py", "chunked_list": ["import numpy as np\n\timport torch\n\tfrom typing import Union, List\n\tfrom util.util_util import get_shape_from_space\n\tclass PPOBuffer():\n\t    @staticmethod\n\t    def _flatten(T: int, N: int, x: np.ndarray):\n\t        return x.reshape(T * N, *x.shape[2:])\n\t    @staticmethod\n\t    def _cast(x: np.ndarray):\n", "        return x.transpose(1, 0, *range(2, x.ndim)).reshape(-1, *x.shape[2:])\n\t    def __init__(self, args, num_agents, obs_space, act_space):\n\t        # buffer config\n\t        self.buffer_size = args.buffer_size\n\t        self.num_agents = num_agents\n\t        self.gamma = args.gamma\n\t        self.use_gae = args.use_gae\n\t        self.gae_lambda = args.gae_lambda\n\t        self.use_risk_sensitive = args.use_risk_sensitive\n\t        self.reward_hyper = 1\n", "        # rnn config\n\t        self.recurrent_hidden_size = args.recurrent_hidden_size\n\t        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n\t        obs_shape = get_shape_from_space(obs_space)\n\t        act_shape = get_shape_from_space(act_space)\n\t        # (o_0, a_0, r_0, d_1, o_1, ... , d_T, o_T)\n\t        self.obs = np.zeros((self.buffer_size + 1, self.num_agents, *obs_shape), dtype=np.float32)\n\t        self.actions = np.zeros((self.buffer_size, self.num_agents, *act_shape), dtype=np.float32)\n\t        self.rewards = np.zeros((self.buffer_size, self.num_agents, 1), dtype=np.float32)\n\t        # NOTE: masks[t] = 1 - dones[t-1], which represents whether obs[t] is a terminal state\n", "        self.masks = np.ones((self.buffer_size + 1, self.num_agents, 1), dtype=np.float32)\n\t        # NOTE: bad_masks[t] = 'bad_transition' in info[t-1], which indicates whether obs[t] a true terminal state or time limit end state\n\t        self.bad_masks = np.ones((self.buffer_size + 1, self.num_agents, 1), dtype=np.float32)\n\t        # pi(a)\n\t        self.action_log_probs = np.zeros((self.buffer_size, self.num_agents, 1), dtype=np.float32)\n\t        # V(o), R(o) while advantage = returns - value_preds\n\t        self.value_preds = np.zeros((self.buffer_size + 1, self.num_agents, 1), dtype=np.float32)\n\t        self.returns = np.zeros((self.buffer_size + 1, self.num_agents, 1), dtype=np.float32)\n\t        # rnn\n\t        self.rnn_states_actor = np.zeros((self.buffer_size + 1, self.num_agents,\n", "                                          self.recurrent_hidden_layers, self.recurrent_hidden_size), dtype=np.float32)\n\t        self.rnn_states_critic = np.zeros_like(self.rnn_states_actor)\n\t        self.step = 0\n\t    @property\n\t    def advantages(self) -> np.ndarray:\n\t        advantages = self.returns[:-1] - self.value_preds[:-1]  # type: np.ndarray\n\t        return (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n\t    def insert(self,\n\t               obs: np.ndarray,\n\t               actions: np.ndarray,\n", "               rewards: np.ndarray,\n\t               masks: np.ndarray,\n\t               action_log_probs: np.ndarray,\n\t               value_preds: np.ndarray,\n\t               rnn_states_actor: np.ndarray,\n\t               rnn_states_critic: np.ndarray,\n\t               bad_masks: Union[np.ndarray, None] = None,\n\t               **kwargs):\n\t        \"\"\"Insert numpy data.\n\t        Args:\n", "            obs:                o_{t+1}\n\t            actions:            a_{t}\n\t            rewards:            r_{t}\n\t            masks:              mask[t+1] = 1 - done_{t}\n\t            action_log_probs:   log_prob(a_{t})\n\t            value_preds:        value(o_{t})\n\t            rnn_states_actor:   ha_{t+1}\n\t            rnn_states_critic:  hc_{t+1}\n\t        \"\"\"\n\t        self.obs[self.step + 1] = obs.copy()\n", "        self.actions[self.step] = actions.copy()\n\t        self.rewards[self.step] = rewards.copy()\n\t        self.masks[self.step + 1] = masks.copy()\n\t        self.action_log_probs[self.step] = action_log_probs.copy()\n\t        self.value_preds[self.step] = value_preds.copy()\n\t        self.rnn_states_actor[self.step + 1] = rnn_states_actor.copy()\n\t        self.rnn_states_critic[self.step + 1] = rnn_states_critic.copy()\n\t        if bad_masks is not None:\n\t            self.bad_masks[self.step + 1] = bad_masks.copy()\n\t        self.step = (self.step + 1) % self.buffer_size\n", "    def after_update(self):\n\t        \"\"\"Copy last timestep data to first index. Called after update to model.\"\"\"\n\t        self.obs[0] = self.obs[-1].copy()\n\t        self.masks[0] = self.masks[-1].copy()\n\t        self.bad_masks[0] = self.bad_masks[-1].copy()\n\t        self.rnn_states_actor[0] = self.rnn_states_actor[-1].copy()\n\t        self.rnn_states_critic[0] = self.rnn_states_critic[-1].copy()\n\t    def clear(self):\n\t        self.step = 0\n\t        self.obs = np.zeros_like(self.obs, dtype=np.float32)\n", "        self.actions = np.zeros_like(self.actions, dtype=np.float32)\n\t        self.rewards = np.zeros_like(self.rewards, dtype=np.float32)\n\t        self.masks = np.ones_like(self.masks, dtype=np.float32)\n\t        self.bad_masks = np.ones_like(self.bad_masks, dtype=np.float32)\n\t        self.action_log_probs = np.zeros_like(self.action_log_probs, dtype=np.float32)\n\t        self.value_preds = np.zeros_like(self.value_preds, dtype=np.float32)\n\t        self.returns = np.zeros_like(self.returns, dtype=np.float32)\n\t        self.rnn_states_actor = np.zeros_like(self.rnn_states_critic)\n\t        self.rnn_states_critic = np.zeros_like(self.rnn_states_actor)\n\t    def dict(self):\n", "        return {'buffer': self.step}\n\t    def compute_returns(self, next_value: np.ndarray):\n\t        \"\"\"\n\t        Compute returns either as discounted sum of rewards, or using GAE.\n\t        Args:\n\t            next_value(np.ndarray): value predictions for the step after the last episode step.\n\t        \"\"\"\n\t        self.rewards[self.rewards==1] = self.rewards[self.rewards==1] * self.reward_hyper\n\t        self.rewards[self.rewards==-1] = self.rewards[self.rewards==-1] / self.reward_hyper\n\t        if self.use_risk_sensitive:\n", "            max_depth = min(100, self.buffer_size)\n\t            max_depth_vec = np.zeros((self.num_agents, 1)) + max_depth\n\t            depths = np.zeros((self.buffer_size+1, self.num_agents, 1))\n\t            for i in reversed(range(self.buffer_size)):\n\t                depths[i] = np.minimum(depths[i+1] + 1, max_depth_vec)\n\t                depths[i][self.masks[i+1]==0] = 1\n\t            values_table = np.zeros((max_depth, self.buffer_size, self.num_agents, 1))\n\t            def operator(reward, value, nextvalue, mask):\n\t                delta = reward + self.gamma * nextvalue * mask - value\n\t                delta = self.tau * np.maximum(delta, np.zeros_like(delta)) + \\\n", "                        (1-self.tau) * np.minimum(delta, np.zeros_like(delta))\n\t                alpha = 1 / (2 * max(self.tau, (1-self.tau)))\n\t                delta = 2 * alpha * delta\n\t                return value + delta\n\t            if self.use_gae:\n\t                self.value_preds[-1] = next_value\n\t                for t in reversed(range(self.rewards.shape[0])):\n\t                    values_table[0, t] = operator(self.rewards[t], self.value_preds[t], self.value_preds[t+1], self.masks[t+1])\n\t                    for h in range(1, int(depths[t].item())):\n\t                        values_table[h, t] = operator(self.rewards[t], values_table[h-1][t], values_table[h-1][t+1], self.masks[t+1])\n", "                    for h in reversed(range(int(depths[t].item()))):\n\t                        self.returns[t] = values_table[h, t] + self.gae_lambda * self.returns[t]\n\t                    self.returns[t] *= (1-self.gae_lambda) / (1-self.gae_lambda**(int(depths[t].item()))) \n\t            else:\n\t                self.value_preds[-1] = next_value\n\t                for step in reversed(range(self.rewards.shape[0])):\n\t                    self.returns[step] = operator(self.rewards[step], self.value_preds[step], self.value_preds[step+1], self.masks[step])\n\t        else:\n\t            if self.use_gae:\n\t                self.value_preds[-1] = next_value\n", "                gae = 0\n\t                for step in reversed(range(self.rewards.shape[0])):\n\t                    td_delta = self.rewards[step] + self.gamma * self.value_preds[step + 1] * self.masks[step + 1] - self.value_preds[step]\n\t                    gae = td_delta + self.gamma * self.gae_lambda * self.masks[step + 1] * gae\n\t                    self.returns[step] = gae + self.value_preds[step]\n\t            else:\n\t                self.returns[-1] = next_value\n\t                for step in reversed(range(self.rewards.shape[0])):\n\t                    self.returns[step] = self.returns[step + 1] * self.gamma * self.masks[step + 1] + self.rewards[step]\n\t    @staticmethod\n", "    def recurrent_generator(buffer, num_mini_batch: int, data_chunk_length: int):\n\t        \"\"\"\n\t        A recurrent generator that yields training data for chunked RNN training arranged in mini batches.\n\t        This generator shuffles the data by sequences.\n\t        Args:\n\t            buffers (Buffer or List[Buffer]) \n\t            num_mini_batch (int): number of minibatches to split the batch into.\n\t            data_chunk_length (int): length of sequence chunks with which to train RNN.\n\t        Returns:\n\t            (obs_batch, actions_batch, masks_batch, old_action_log_probs_batch, advantages_batch, \\\n", "                returns_batch, value_preds_batch, rnn_states_actor_batch, rnn_states_critic_batch)\n\t        \"\"\"\n\t        buffer = [buffer] if isinstance(buffer, PPOBuffer) else buffer  \n\t        buffer_size = buffer[0].buffer_size\n\t        num_agents = buffer[0].num_agents\n\t        assert all([b.buffer_size == buffer_size for b in buffer]) \\\n\t            and all([b.num_agents == num_agents for b in buffer]) \\\n\t            and all([isinstance(b, PPOBuffer) for b in buffer]), \\\n\t            \"Input buffers must has the same type and shape\"\n\t        buffer_size = buffer_size * len(buffer)\n", "        assert buffer_size >= data_chunk_length, (\n\t            \"PPO requires the number of buffer size ({}) * num_agents ({})\"\n\t            \"to be greater than or equal to the number of \"\n\t            \"data chunk length ({}).\".format(buffer_size, num_agents, data_chunk_length))\n\t        # Transpose and reshape parallel data into sequential data\n\t        obs = np.vstack([PPOBuffer._cast(buf.obs[:-1]) for buf in buffer])\n\t        actions = np.vstack([PPOBuffer._cast(buf.actions) for buf in buffer])\n\t        masks = np.vstack([PPOBuffer._cast(buf.masks[:-1]) for buf in buffer])\n\t        old_action_log_probs = np.vstack([PPOBuffer._cast(buf.action_log_probs) for buf in buffer])\n\t        advantages = np.vstack([PPOBuffer._cast(buf.advantages) for buf in buffer])\n", "        returns = np.vstack([PPOBuffer._cast(buf.returns[:-1]) for buf in buffer])\n\t        value_preds = np.vstack([PPOBuffer._cast(buf.value_preds[:-1]) for buf in buffer])\n\t        rnn_states_actor = np.vstack([PPOBuffer._cast(buf.rnn_states_actor[:-1]) for buf in buffer])\n\t        rnn_states_critic = np.vstack([PPOBuffer._cast(buf.rnn_states_critic[:-1]) for buf in buffer])\n\t        # Get mini-batch size and shuffle chunk data\n\t        data_chunks = buffer_size // data_chunk_length\n\t        mini_batch_size = data_chunks // num_mini_batch\n\t        rand = torch.randperm(data_chunks).numpy()\n\t        sampler = [rand[i * mini_batch_size:(i + 1) * mini_batch_size] for i in range(num_mini_batch)]\n\t        for indices in sampler:\n", "            obs_batch = []\n\t            actions_batch = []\n\t            masks_batch = []\n\t            old_action_log_probs_batch = []\n\t            advantages_batch = []\n\t            returns_batch = []\n\t            value_preds_batch = []\n\t            rnn_states_actor_batch = []\n\t            rnn_states_critic_batch = []\n\t            for index in indices:\n", "                ind = index * data_chunk_length\n\t                # size [T+1, N, Dim] => [T, N, Dim] => [N, T, Dim] => [N * T, Dim] => [L, Dim]\n\t                obs_batch.append(obs[ind:ind + data_chunk_length])\n\t                actions_batch.append(actions[ind:ind + data_chunk_length])\n\t                masks_batch.append(masks[ind:ind + data_chunk_length])\n\t                old_action_log_probs_batch.append(old_action_log_probs[ind:ind + data_chunk_length])\n\t                advantages_batch.append(advantages[ind:ind + data_chunk_length])\n\t                returns_batch.append(returns[ind:ind + data_chunk_length])\n\t                value_preds_batch.append(value_preds[ind:ind + data_chunk_length])\n\t                # size [T+1, N, Dim] => [T, N, Dim] => [N, T, Dim] => [N * T, Dim] => [1, Dim]\n", "                rnn_states_actor_batch.append(rnn_states_actor[ind])\n\t                rnn_states_critic_batch.append(rnn_states_critic[ind])\n\t            L, N = data_chunk_length, mini_batch_size\n\t            # These are all from_numpys of size (L, N, Dim)\n\t            obs_batch = np.stack(obs_batch, axis=1)\n\t            actions_batch = np.stack(actions_batch, axis=1)\n\t            masks_batch = np.stack(masks_batch, axis=1)\n\t            old_action_log_probs_batch = np.stack(old_action_log_probs_batch, axis=1)\n\t            advantages_batch = np.stack(advantages_batch, axis=1)\n\t            returns_batch = np.stack(returns_batch, axis=1)\n", "            value_preds_batch = np.stack(value_preds_batch, axis=1)\n\t            # States is just a (N, -1) from_numpy\n\t            rnn_states_actor_batch = np.stack(rnn_states_actor_batch).reshape(N, *buffer[0].rnn_states_actor.shape[2:])\n\t            rnn_states_critic_batch = np.stack(rnn_states_critic_batch).reshape(N, *buffer[0].rnn_states_critic.shape[2:])\n\t            # Flatten the (L, N, ...) from_numpys to (L * N, ...)\n\t            obs_batch = PPOBuffer._flatten(L, N, obs_batch)\n\t            actions_batch = PPOBuffer._flatten(L, N, actions_batch)\n\t            masks_batch = PPOBuffer._flatten(L, N, masks_batch)\n\t            old_action_log_probs_batch = PPOBuffer._flatten(L, N, old_action_log_probs_batch)\n\t            advantages_batch = PPOBuffer._flatten(L, N, advantages_batch)\n", "            returns_batch = PPOBuffer._flatten(L, N, returns_batch)\n\t            value_preds_batch = PPOBuffer._flatten(L, N, value_preds_batch)\n\t            yield obs_batch, actions_batch, masks_batch, old_action_log_probs_batch, advantages_batch, \\\n\t                returns_batch, value_preds_batch, rnn_states_actor_batch, rnn_states_critic_batch"]}
{"filename": "ppo/ppo_actor.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom util.util_mlp import MLPBase\n\tfrom util.util_gru import GRULayer\n\tfrom util.util_act import ACTLayer\n\tfrom util.util_util import check\n\tclass PPOActor(nn.Module):\n\t    def __init__(self, args, obs_space, act_space, device=torch.device(\"cpu\")):\n\t        super(PPOActor, self).__init__()\n\t        # network config\n", "        self.gain = args.gain\n\t        self.hidden_size = args.hidden_size\n\t        self.act_hidden_size = args.act_hidden_size\n\t        self.activation_id = args.activation_id\n\t        self.use_feature_normalization = args.use_feature_normalization\n\t        self.use_recurrent_policy = args.use_recurrent_policy\n\t        self.recurrent_hidden_size = args.recurrent_hidden_size\n\t        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n\t        self.use_cnn = args.use_cnn\n\t        self.tpdv = dict(dtype=torch.float32, device=device)\n", "        # (1) feature extraction module\n\t        if self.use_cnn: # just hard code obs_dim 4 * 40 * 40\n\t            self.base = nn.Sequential(\n\t            nn.Conv2d(4, 8, 4, stride=2), ## (40+2-4)/2+1 = 20\n\t            nn.ReLU(),\n\t            nn.MaxPool2d(3, stride=2),\n\t            nn.Conv2d(8, 16, kernel_size=2, stride=1), # (20-2)/2+1 = 10\n\t            nn.ReLU(),\n\t            nn.MaxPool2d(2, 1),\n\t            nn.Flatten(),\n", "            nn.Linear(16*7*7, 128),\n\t            nn.ReLU()\n\t        )\n\t            input_size = 128\n\t        else:\n\t            self.base = MLPBase(obs_space, self.hidden_size, self.activation_id, self.use_feature_normalization)\n\t            input_size = self.base.output_size\n\t        # (2) rnn module\n\t        if self.use_recurrent_policy:\n\t            self.rnn = GRULayer(input_size, self.recurrent_hidden_size, self.recurrent_hidden_layers)\n", "            input_size = self.rnn.output_size\n\t        # (3) act module\n\t        self.act = ACTLayer(act_space, input_size, self.act_hidden_size, self.activation_id, self.gain)\n\t        self.to(device)\n\t    def forward(self, obs, rnn_states, masks, deterministic=False):\n\t        obs = check(obs).to(**self.tpdv)\n\t        rnn_states = check(rnn_states).to(**self.tpdv)\n\t        masks = check(masks).to(**self.tpdv)\n\t        actor_features = self.base(obs)\n\t        if self.use_recurrent_policy:\n", "            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n\t        actions, action_log_probs = self.act(actor_features, deterministic)\n\t        return actions, action_log_probs, rnn_states\n\t    def evaluate_actions(self, obs, rnn_states, action, masks, active_masks=None):\n\t        obs = check(obs).to(**self.tpdv)\n\t        rnn_states = check(rnn_states).to(**self.tpdv)\n\t        action = check(action).to(**self.tpdv)\n\t        masks = check(masks).to(**self.tpdv)\n\t        if active_masks is not None:\n\t            active_masks = check(active_masks).to(**self.tpdv)\n", "        actor_features = self.base(obs)\n\t        if self.use_recurrent_policy:\n\t            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n\t        action_log_probs, dist_entropy = self.act.evaluate_actions(actor_features, action, active_masks)\n\t        return action_log_probs, dist_entropy\n"]}
{"filename": "ppo/ppo_critic.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom util.util_mlp import MLPBase, MLPLayer\n\tfrom util.util_gru import GRULayer\n\tfrom util.util_act import ACTLayer\n\tfrom util.util_util import check\n\tclass PPOCritic(nn.Module):\n\t    def __init__(self, args, obs_space, device=torch.device(\"cpu\")):\n\t        super(PPOCritic, self).__init__()\n\t        # network config\n", "        self.hidden_size = args.hidden_size\n\t        self.act_hidden_size = args.act_hidden_size\n\t        self.activation_id = args.activation_id\n\t        self.use_feature_normalization = args.use_feature_normalization\n\t        self.use_recurrent_policy = args.use_recurrent_policy\n\t        self.recurrent_hidden_size = args.recurrent_hidden_size\n\t        self.recurrent_hidden_layers = args.recurrent_hidden_layers\n\t        self.tpdv = dict(dtype=torch.float32, device=device)\n\t        self.use_cnn = args.use_cnn\n\t        # (1) feature extraction module\n", "        if self.use_cnn: # just hard code obs_dim 4 * 40 * 40\n\t            self.base = nn.Sequential(\n\t            nn.Conv2d(4, 8, 4, stride=2), # (40+2-4)/2+1 = 20\n\t            nn.ReLU(),\n\t            nn.MaxPool2d(3, stride=2),\n\t            nn.Conv2d(8, 16, kernel_size=2, stride=1), # (20-2)/2 + 1 = 10\n\t            nn.ReLU(),\n\t            nn.MaxPool2d(2, 1),\n\t            nn.Flatten(),\n\t            nn.Linear(16*7*7, 128),\n", "            nn.ReLU()\n\t        )\n\t            input_size = 128\n\t        else:\n\t            self.base = MLPBase(obs_space, self.hidden_size, self.activation_id, self.use_feature_normalization)\n\t            input_size = self.base.output_size\n\t        # (2) rnn module\n\t        if self.use_recurrent_policy:\n\t            self.rnn = GRULayer(input_size, self.recurrent_hidden_size, self.recurrent_hidden_layers)\n\t            input_size = self.rnn.output_size\n", "        # (3) value module\n\t        if len(self.act_hidden_size) > 0:\n\t            self.mlp = MLPLayer(input_size, self.act_hidden_size, self.activation_id)\n\t        self.value_out = nn.Linear(input_size, 1)\n\t        self.to(device)\n\t    def forward(self, obs, rnn_states, masks):\n\t        obs = check(obs).to(**self.tpdv)\n\t        rnn_states = check(rnn_states).to(**self.tpdv)\n\t        masks = check(masks).to(**self.tpdv)\n\t        critic_features = self.base(obs)\n", "        if self.use_recurrent_policy:\n\t            critic_features, rnn_states = self.rnn(critic_features, rnn_states, masks)\n\t        if len(self.act_hidden_size) > 0:\n\t            critic_features = self.mlp(critic_features)\n\t        values = self.value_out(critic_features)\n\t        return values, rnn_states\n"]}
{"filename": "util/util_distributions.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom util.util_util import init\n\t\"\"\"\n\tModify standard PyTorch distributions so they are compatible with this code.\n\t\"\"\"\n\t# Standardize distribution interfaces\n\t# Categorical\n\tclass FixedCategorical(torch.distributions.Categorical):\n\t    def sample(self):\n", "        return super().sample().unsqueeze(-1)\n\t    def log_probs(self, actions):\n\t        # Single: [1] => [] => [] => [1, 1] => [1] => [1]\n\t        # Batch: [N]/[N, 1] => [N] => [N] => [N, 1] => [N] => [N, 1]\n\t        return (\n\t            super()\n\t            .log_prob(actions.squeeze(-1))\n\t            .view(actions.squeeze(-1).unsqueeze(-1).size())\n\t            .sum(-1, keepdim=True)\n\t        )\n", "    def mode(self):\n\t        return self.probs.argmax(dim=-1, keepdim=True)\n\t    def entropy(self):\n\t        return super().entropy().unsqueeze(-1)\n\t# Normal\n\tclass FixedNormal(torch.distributions.Normal):\n\t    def log_probs(self, actions):\n\t        return super().log_prob(actions).sum(-1, keepdim=True)\n\t    def entropy(self):\n\t        return super().entropy().sum(-1, keepdim=True)\n", "    def mode(self):\n\t        return self.mean\n\t# Bernoulli\n\tclass FixedBernoulli(torch.distributions.Bernoulli):\n\t    def log_probs(self, actions):\n\t        # Single: [K] => [K] => [1]\n\t        # Batch: [N, K] => [N, K] => [N, 1]\n\t        return super().log_prob(actions).sum(-1, keepdim=True)\n\t    def entropy(self):\n\t        return super().entropy().sum(-1, keepdim=True)\n", "    def mode(self):\n\t        return torch.gt(self.probs, 0.5).float()\n\tclass Categorical(nn.Module):\n\t    def __init__(self, num_inputs, num_outputs, gain=0.01):\n\t        super(Categorical, self).__init__()\n\t        def init_(m):\n\t            return init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain)\n\t        self.logits_net = init_(nn.Linear(num_inputs, num_outputs))\n\t    def forward(self, x):\n\t        x = self.logits_net(x)\n", "        return FixedCategorical(logits=x)\n\t    @property\n\t    def output_size(self) -> int:\n\t        return 1\n\tclass DiagGaussian(nn.Module):\n\t    def __init__(self, num_inputs, num_outputs, gain=0.01):\n\t        super(DiagGaussian, self).__init__()\n\t        def init_(m):\n\t            return init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain)\n\t        self.mu_net = init_(nn.Linear(num_inputs, num_outputs))\n", "        self.log_std = nn.Parameter(torch.zeros(num_outputs))\n\t        self._num_outputs = num_outputs\n\t    def forward(self, x):\n\t        action_mean = self.mu_net(x)\n\t        return FixedNormal(action_mean, self.log_std.exp())\n\t    @property\n\t    def output_size(self) -> int:\n\t        return self._num_outputs\n\tclass Bernoulli(nn.Module):\n\t    def __init__(self, num_inputs, num_outputs, gain=0.01):\n", "        super(Bernoulli, self).__init__()\n\t        def init_(m):\n\t            return init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain)\n\t        self.logits_net = init_(nn.Linear(num_inputs, num_outputs))\n\t        self._num_outputs = num_outputs\n\t    def forward(self, x):\n\t        x = self.logits_net(x)\n\t        return FixedBernoulli(logits=x)\n\t    @property\n\t    def output_size(self) -> int:\n", "        return self._num_outputs\n"]}
{"filename": "util/util_selfplay.py", "chunked_list": ["from typing import Dict, List, Tuple\n\timport numpy as np\n\timport random\n\tfrom abc import ABC, abstractstaticmethod\n\tdef get_algorithm(algo_name):\n\t    if algo_name == 'sp':\n\t        return SP()\n\t    elif algo_name == 'fsp':\n\t        return FSP()\n\t    elif algo_name == 'pfsp':\n", "        return PFSP()\n\t    else:\n\t        raise NotImplementedError(\"Unknown algorithm {}\".format(algo_name))\n\tclass SelfplayAlgorithm(ABC):\n\t    def __init__(self) -> None:\n\t        pass\n\t    def choose_opponents(self, agent_idx: int, agent_elos: dict, num_opponents: int):\n\t        enm_idxs, enm_history_idxs, enm_elos = [], [], []\n\t        num_total = 1\n\t        while True:\n", "            enm_idx = random.choice([k for k in list(agent_elos.keys())])\n\t            # 1) choose the opponent agent from populations.\n\t            enm_idxs.append(enm_idx)\n\t            # 2) choose the history copy from the current agent according to ELO\n\t            enm_history_idx = self._choose_history(agent_elos[enm_idx])\n\t            enm_history_idxs.append(enm_history_idx)\n\t            enm_elos.append(agent_elos[enm_idx][enm_history_idx])\n\t            num_total += 1\n\t            if num_total > num_opponents:\n\t                break\n", "        enms = []\n\t        for agent, itr in zip(enm_idxs, enm_history_idxs):\n\t                enms.append((agent, itr))\n\t        return enms, enm_elos\n\t    @abstractstaticmethod\n\t    def _choose_history(self, agents_elo: Dict[str, float]):\n\t        pass\n\tclass SP(SelfplayAlgorithm):\n\t    def __init__(self) -> None:\n\t        super().__init__()\n", "    def _choose_history(self, agents_elo: Dict[str, float]):\n\t        return list(agents_elo.keys())[-1]\n\tclass FSP(SelfplayAlgorithm):\n\t    def __init__(self) -> None:\n\t         super().__init__()\n\t    def _choose_history(self, agents_elo: Dict[str, float]):\n\t        return np.random.choice(list(agents_elo.keys()))\n\tclass PFSP(SelfplayAlgorithm):\n\t    def __init__(self) -> None:\n\t        super().__init__()\n", "        self.lam = 1.\n\t        self.s = 100.\n\t    def _choose_history(self, agents_elo: Dict[str, float]):\n\t        history_elo = np.array(list(agents_elo.values()))\n\t        sample_probs = 1. / (1. + 10. ** (-(history_elo - np.median(history_elo)) / 400.)) * self.s\n\t        \"\"\" meta-solver \"\"\"\n\t        k = float(len(sample_probs) + 1)\n\t        meta_solver_probs = np.exp(self.lam / k * sample_probs) / np.sum(np.exp(self.lam / k * sample_probs))\n\t        opponent_idx = np.random.choice(a=list(agents_elo.keys()), size=1, p=meta_solver_probs).item()\n\t        return opponent_idx\n", "if __name__ == '__main__':\n\t    ranks = {\n\t        0: {0: 1000, 1: 1200, 2: 1300, 3: 1400, 4: 1500, 5: 1800, 6: 2000},\n\t        1: {0: 1000, 1: 1200, 2: 1300, 3: 1400, 4: 1500, 5: 1800, 6: 2000},\n\t        2: {0: 1000, 1: 1200, 2: 1300, 3: 1400, 4: 1500, 5: 1800, 6: 2000},\n\t        3: {0: 1000, 1: 1200, 2: 1300, 3: 1400, 4: 1500, 5: 1800, 6: 2000},\n\t    }\n\t    algo = get_algorithm(\"pfsp\")\n\t    ret = algo.choose_opponents(agent_idx=0, agent_elos=ranks, num_opponents=4)\n\t    print(ret[0], ret[1])\n"]}
{"filename": "util/util_mlp.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom util.util_util import build_flattener\n\tclass MLPLayer(nn.Module):\n\t    def __init__(self, input_dim, hidden_size, activation_id):\n\t        super(MLPLayer, self).__init__()\n\t        self._size = [input_dim] + list(map(int, hidden_size.split(' ')))\n\t        self._hidden_layers = len(self._size) - 1\n\t        active_func = [nn.Tanh(), nn.ReLU(), nn.LeakyReLU(), nn.ELU()][activation_id]\n\t        fc_h = []\n", "        for j in range(len(self._size) - 1):\n\t            fc_h += [\n\t                nn.Linear(self._size[j], self._size[j + 1]), active_func, nn.LayerNorm(self._size[j + 1])\n\t            ]\n\t        self.fc = nn.Sequential(*fc_h)\n\t    def forward(self, x: torch.Tensor):\n\t        x = self.fc(x)\n\t        return x\n\t    @property\n\t    def output_size(self) -> int:\n", "        return self._size[-1]\n\t# Feature extraction module\n\tclass MLPBase(nn.Module):\n\t    def __init__(self, obs_space, hidden_size, activation_id, use_feature_normalization):\n\t        super(MLPBase, self).__init__()\n\t        self._hidden_size = hidden_size\n\t        self._activation_id = activation_id\n\t        self._use_feature_normalization = use_feature_normalization\n\t        self.obs_flattener = build_flattener(obs_space)\n\t        input_dim = self.obs_flattener.size\n", "        if self._use_feature_normalization:\n\t            self.feature_norm = nn.LayerNorm(input_dim)\n\t        self.mlp = MLPLayer(input_dim, self._hidden_size, self._activation_id)\n\t    def forward(self, x: torch.Tensor):\n\t        if self._use_feature_normalization:\n\t            x = self.feature_norm(x)\n\t        x = self.mlp(x)\n\t        return x\n\t    @property\n\t    def output_size(self) -> int:\n", "        return self.mlp.output_size\n"]}
{"filename": "util/util_population.py", "chunked_list": ["import os\n\timport random\n\timport shutil\n\timport numpy as np\n\tfrom copy import deepcopy\n\tdef population_based_exploit_explore(epoch: int, elos: dict, hypers: dict, run_dir: str, elo_threshold):\n\t    topk = int(np.ceil(0.2 * len(elos)))\n\t    elos_new = deepcopy(elos)\n\t    hypers_new = deepcopy(hypers)\n\t    ranks = {agent_id: elos[agent_id][epoch+1] for agent_id in elos.keys()}\n", "    sorted_ranks_idxs = [k for k in sorted(ranks, key=ranks.__getitem__, reverse=True)]\n\t    topk_idxs = list(sorted_ranks_idxs[:topk])\n\t    for agent_id in elos_new.keys():\n\t        agent_elo = elos[agent_id][epoch+1]\n\t        better_agent_id = random.sample(topk_idxs, 1)[0]\n\t        if len(sorted_ranks_idxs) == 1:\n\t            # population size = 1, no exploit\n\t            break\n\t        if ranks[better_agent_id] - agent_elo < elo_threshold or agent_id in topk_idxs:\n\t            # the agent is already good enough\n", "            continue\n\t        elos_new[agent_id][epoch+1] = elos[better_agent_id][epoch+1]\n\t        os.remove(f\"{run_dir}/agent{agent_id}_history{epoch+1}.pt\")\n\t        os.remove(f\"{run_dir}/agent{agent_id}_latest.pt\")\n\t        shutil.copy(f\"{run_dir}/agent{better_agent_id}_latest.pt\", f\"{run_dir}/agent{agent_id}_history{epoch+1}.pt\")\n\t        shutil.copy(f\"{run_dir}/agent{better_agent_id}_latest.pt\", f\"{run_dir}/agent{agent_id}_latest.pt\")\n\t        for s in hypers[agent_id].keys():\n\t            hyper_ = hypers[agent_id][s]\n\t            new_hyper_ = hypers[better_agent_id][s]\n\t            inherit_prob = np.random.binomial(1, 0.5, 1)[0]\n", "            hyper_tmp = (1. - inherit_prob) * hyper_ + inherit_prob * new_hyper_\n\t            hypers_new[agent_id][s] = np.clip(hyper_tmp + np.random.uniform(-0.2, 0.2), 0., 1.)\n\t    return elos_new, hypers_new\n"]}
{"filename": "util/util_gru.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tclass GRULayer(nn.Module):\n\t    def __init__(self, input_size: int, hidden_size: int, num_layers: int):\n\t        super(GRULayer, self).__init__()\n\t        self._hidden_size = hidden_size\n\t        self._num_layers = num_layers\n\t        self.gru = nn.GRU(input_size=input_size,\n\t                          hidden_size=hidden_size,\n\t                          num_layers=num_layers)\n", "        # NOTE: self.gru(x, hxs) needs x=[T, N, input_size] and hxs=[L, N, hidden_size]\n\t        self.norm = nn.LayerNorm(hidden_size)\n\t    def forward(self, x: torch.Tensor, hxs: torch.Tensor, masks: torch.Tensor):\n\t        # NOTE: N = mini_batch_size; T = recurrent chunk length; L = gru layers\n\t        # (T=1) x: [N, input_size], hxs: [N, L, hidden_size], masks: [N, 1]\n\t        if x.size(0) == hxs.size(0):\n\t            # masks: [N, 1] => [N, L] => [N, L, 1]\n\t            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks.repeat(1, self._num_layers).unsqueeze(-1)).transpose(0, 1).contiguous())\n\t            x = x.squeeze(0)            # [1, N, input_size] => [N, input_size]\n\t            hxs = hxs.transpose(0, 1)   # [L, N, hidden_size] => [N, L, hidden_size]\n", "        # (T>1): x=[T * N, input_size], hxs=[N, L, hidden_size], masks=[T * N, 1]\n\t        else:\n\t            # Mannual reset hxs to zero at ternimal states might be too slow to calculate\n\t            # We need to tackle the problem more efficiently\n\t            # x is a (T, N, input_size) tensor that has been flatten to (T * N, -1)\n\t            N = hxs.size(0)\n\t            T = int(x.size(0) / N)\n\t            # unflatten x and masks\n\t            x = x.view(T, N, x.size(1))  # [T * N, input_size] => [T, N, input_size]\n\t            masks = masks.view(T, N)     # [T * N, 1] => [T, N]\n", "            # Let's figure out which steps in the sequence have a zero for any agent\n\t            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n\t            has_zeros = ((masks[1:] == 0.0)\n\t                         .any(dim=-1)       # [T, N] => [T, 1]\n\t                         .nonzero(as_tuple=False)\n\t                         .squeeze(dim=-1)   # [T, 1] => [T]\n\t                         .cpu())\n\t            # +1 to correct the masks[1:]\n\t            has_zeros = (has_zeros + 1).numpy().tolist()\n\t            # add t=0 and t=T to the list\n", "            has_zeros = [0] + has_zeros + [T]\n\t            hxs = hxs.transpose(0, 1)   # [N, L, hidden_size] => [L, N, hidden_size]\n\t            outputs = []\n\t            for i in range(len(has_zeros) - 1):\n\t                # We can now process steps that don't have any zeros in masks together!\n\t                start_idx = has_zeros[i]\n\t                end_idx = has_zeros[i + 1]\n\t                # masks[start_idx]: [N] => [1, N, 1] => [L, N, 1]\n\t                temp = (hxs * masks[start_idx].view(1, -1, 1).repeat(self._num_layers, 1, 1)).contiguous()\n\t                rnn_scores, hxs = self.gru(x[start_idx:end_idx], temp)\n", "                outputs.append(rnn_scores)\n\t            # x is a (T, N, -1) tensor\n\t            x = torch.cat(outputs, dim=0)\n\t            # flatten\n\t            x = x.view(T * N, -1)       # [T, N, input_size] => [T * N, input_size]\n\t            hxs = hxs.transpose(0, 1)   # [L, N, hidden_size] => [N, L, hidden_size]\n\t        x = self.norm(x)\n\t        return x, hxs\n\t    @property\n\t    def output_size(self):\n", "        return self._hidden_size\n"]}
{"filename": "util/util_util.py", "chunked_list": ["import copy\n\timport math\n\timport gym.spaces\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom collections import OrderedDict\n\tdef check(input):\n\t    output = torch.from_numpy(input) if type(input) == np.ndarray else input\n\t    return output\n", "def get_shape_from_space(space):\n\t    if isinstance(space, gym.spaces.Discrete):\n\t        return (1,)\n\t    elif isinstance(space, gym.spaces.Box) \\\n\t            or isinstance(space, gym.spaces.MultiDiscrete) \\\n\t            or isinstance(space, gym.spaces.MultiBinary):\n\t        return space.shape\n\t    elif isinstance(space,gym.spaces.Tuple) and \\\n\t           isinstance(space[0], gym.spaces.MultiDiscrete) and \\\n\t               isinstance(space[1], gym.spaces.Discrete):\n", "        return (space[0].shape[0] + 1,)\n\t    else:\n\t        raise NotImplementedError(f\"Unsupported action space type: {type(space)}!\")\n\tdef get_gard_norm(it):\n\t    sum_grad = 0\n\t    for x in it:\n\t        if x.grad is None:\n\t            continue\n\t        sum_grad += x.grad.norm() ** 2\n\t    return math.sqrt(sum_grad)\n", "def init(module: nn.Module, weight_init, bias_init, gain=1):\n\t    weight_init(module.weight.data, gain=gain)\n\t    bias_init(module.bias.data)\n\t    return module\n\tdef get_clones(module, N):\n\t    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\tdef build_flattener(space):\n\t    if isinstance(space, gym.spaces.Dict):\n\t        return DictFlattener(space)\n\t    elif isinstance(space, gym.spaces.Box) \\\n", "            or isinstance(space, gym.spaces.MultiDiscrete):\n\t        return BoxFlattener(space)\n\t    elif isinstance(space, gym.spaces.Discrete):\n\t        return DiscreteFlattener(space)\n\t    else:\n\t        raise NotImplementedError\n\tclass DictFlattener():\n\t    \"\"\"DictVector\n\t    \"\"\"\n\t    def __init__(self, ori_space):\n", "        self.space = ori_space\n\t        assert isinstance(ori_space, gym.spaces.Dict)\n\t        self.size = 0\n\t        self.flatteners = OrderedDict()\n\t        for name, space in self.space.spaces.items():\n\t            if isinstance(space, gym.spaces.Box):\n\t                flattener = BoxFlattener(space)\n\t            elif isinstance(space, gym.spaces.Discrete):\n\t                flattener = DiscreteFlattener(space)\n\t            elif isinstance(space, gym.spaces.Dict):\n", "                flattener = DictFlattener(space)\n\t            self.flatteners[name] = flattener\n\t            self.size += flattener.size\n\t    def __call__(self, observation):\n\t        \"\"\"DictVector\n\t        \"\"\"\n\t        assert isinstance(observation, OrderedDict)\n\t        batch = self.get_batch(observation, self)\n\t        if batch == 1:\n\t            array = np.zeros(self.size,)\n", "        else:\n\t            array = np.zeros(self.size)\n\t        self.write(observation, array, 0)\n\t        return array\n\t    def inv(self, observation):\n\t        \"\"\"VectorDict\n\t        \"\"\"\n\t        offset_start, offset_end = 0, 0\n\t        output = OrderedDict()\n\t        for n, f in self.flatteners.items():\n", "            offset_end += f.size\n\t            output[n] = f.inv(observation[..., offset_start:offset_end])\n\t            offset_start = offset_end\n\t        return output\n\t    def write(self, observation, array, offset):\n\t        for o, f in zip(observation.values(), self.flatteners.values()):\n\t            f.write(o, array, offset)\n\t            offset += f.size\n\t    def get_batch(self, observation, flattener):\n\t        if isinstance(observation, dict):\n", "            # batch\n\t            for o, f in zip(observation.values(), flattener.flatteners.values()):\n\t                return self.get_batch(o, f)\n\t        else:\n\t            return np.asarray(observation).size // flattener.size\n\tclass BoxFlattener():\n\t    \"\"\"Box/MultiDiscreteVector\n\t    \"\"\"\n\t    def __init__(self, ori_space):\n\t        self.space = ori_space\n", "        assert isinstance(ori_space, gym.spaces.Box) \\\n\t            or isinstance(ori_space, gym.spaces.MultiDiscrete)\n\t        self.size = np.product(ori_space.shape)\n\t    def __call__(self, observation):\n\t        array = np.array(observation, copy=False)\n\t        if array.size // self.size == 1:\n\t            return array.ravel()\n\t        else:\n\t            return array.reshape(-1, self.size)\n\t    def inv(self, observation):\n", "        array = np.array(observation, copy=False)\n\t        if array.size // self.size == 1:\n\t            return array.reshape(self.space.shape)\n\t        else:\n\t            return array.reshape((-1,) + self.space.shape)\n\t    def write(self, observation, array, offset):\n\t        array[..., offset:offset + self.size] = self(observation)\n\tclass DiscreteFlattener():\n\t    \"\"\"DiscreteVector\n\t    \"\"\"\n", "    def __init__(self, ori_space):\n\t        self.space = ori_space\n\t        assert isinstance(ori_space, gym.spaces.Discrete)\n\t        self.size = 1\n\t    def __call__(self, observation):\n\t        array = np.array(observation, copy=False)\n\t        if array.size == 1:\n\t            return array.item()\n\t        else:\n\t            return array.reshape(-1, 1)\n", "    def inv(self, observation):\n\t        array = np.array(observation, dtype=np.int, copy=False)\n\t        if array.size == 1:\n\t            return array.item()\n\t        else:\n\t            return array.reshape(-1, 1)\n\t    def write(self, observation, array, offset):\n\t        array[..., offset:offset + 1] = self(observation)"]}
{"filename": "util/util_act.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport gym.spaces\n\tfrom util.util_mlp import MLPLayer\n\tfrom util.util_distributions import Categorical, DiagGaussian, Bernoulli\n\tclass ACTLayer(nn.Module):\n\t    def __init__(self, act_space, input_dim, hidden_size, activation_id, gain):\n\t        super(ACTLayer, self).__init__()\n\t        self._mlp_actlayer = False\n\t        self._continuous_action = False\n", "        self._multidiscrete_action = False\n\t        self._mixed_action = False\n\t        if len(hidden_size) > 0:\n\t            self._mlp_actlayer = True\n\t            self.mlp = MLPLayer(input_dim, hidden_size, activation_id)\n\t            input_dim = self.mlp.output_size\n\t        if isinstance(act_space, gym.spaces.Discrete):\n\t            action_dim = act_space.n\n\t            self.action_out = Categorical(input_dim, action_dim, gain)\n\t        elif isinstance(act_space, gym.spaces.Box):\n", "            self._continuous_action = True\n\t            action_dim = act_space.shape[0]\n\t            self.action_out = DiagGaussian(input_dim, action_dim, gain)\n\t        elif isinstance(act_space, gym.spaces.MultiBinary):\n\t            action_dim = act_space.shape[0]\n\t            self.action_out = Bernoulli(input_dim, action_dim, gain)\n\t        elif isinstance(act_space, gym.spaces.MultiDiscrete):\n\t            self._multidiscrete_action = True\n\t            action_dims = act_space.nvec\n\t            action_outs = []\n", "            for action_dim in action_dims:\n\t                action_outs.append(Categorical(input_dim, action_dim, gain))\n\t            self.action_outs = nn.ModuleList(action_outs)\n\t        else: \n\t            raise NotImplementedError(f\"Unsupported action space type: {type(act_space)}!\")\n\t    def forward(self, x, deterministic=False, **kwargs):\n\t        \"\"\"\n\t        Compute actions and action logprobs from given input.\n\t        Args:\n\t            x (torch.Tensor): input to network.\n", "            deterministic (bool): whether to sample from action distribution or return the mode.\n\t        Returns:\n\t            actions (torch.Tensor): actions to take.\n\t            action_log_probs (torch.Tensor): log probabilities of taken actions.\n\t        \"\"\"\n\t        if self._mlp_actlayer:\n\t            x = self.mlp(x)\n\t        if self._multidiscrete_action:\n\t            actions = []\n\t            action_log_probs = []\n", "            for action_out in self.action_outs:\n\t                action_dist = action_out(x)\n\t                action = action_dist.mode() if deterministic else action_dist.sample()\n\t                action_log_prob = action_dist.log_probs(action)\n\t                actions.append(action)\n\t                action_log_probs.append(action_log_prob)\n\t            actions = torch.cat(actions, dim=-1)\n\t            action_log_probs = torch.cat(action_log_probs, dim=-1).sum(dim=-1, keepdim=True)\n\t        else:\n\t            action_dists = self.action_out(x)\n", "            actions = action_dists.mode() if deterministic else action_dists.sample()\n\t            action_log_probs = action_dists.log_probs(actions)\n\t        return actions, action_log_probs\n\t    def evaluate_actions(self, x, action, active_masks=None, **kwargs):\n\t        \"\"\"\n\t        Compute log probability and entropy of given actions.\n\t        Args:\n\t            x (torch.Tensor): input to network.\n\t            action (torch.Tensor): actions whose entropy and log probability to evaluate.\n\t            active_masks (torch.Tensor): denotes whether an agent is active or dead.\n", "        Returns:\n\t            action_log_probs (torch.Tensor): log probabilities of the input actions.\n\t            dist_entropy (torch.Tensor): action distribution entropy for the given inputs.\n\t        \"\"\"\n\t        if self._mlp_actlayer:\n\t            x = self.mlp(x)\n\t        if self._multidiscrete_action:\n\t            action = torch.transpose(action, 0, 1)\n\t            action_log_probs = []\n\t            dist_entropy = []\n", "            for action_out, act in zip(self.action_outs, action):\n\t                action_dist = action_out(x)\n\t                action_log_probs.append(action_dist.log_probs(act.unsqueeze(-1)))\n\t                if active_masks is not None:\n\t                    dist_entropy.append((action_dist.entropy() * active_masks) / active_masks.sum())\n\t                else:\n\t                    dist_entropy.append(action_dist.entropy() / action_log_probs[-1].size(0))\n\t            action_log_probs = torch.cat(action_log_probs, dim=-1).sum(dim=-1, keepdim=True)\n\t            dist_entropy = torch.cat(dist_entropy, dim=-1).sum(dim=-1, keepdim=True)\n\t        else:\n", "            action_dist = self.action_out(x)\n\t            action_log_probs = action_dist.log_probs(action)\n\t            if active_masks is not None:\n\t                dist_entropy = (action_dist.entropy() * active_masks) / active_masks.sum()\n\t            else:\n\t                dist_entropy = action_dist.entropy() / action_log_probs.size(0)\n\t        return action_log_probs, dist_entropy\n\t    def get_probs(self, x):\n\t        \"\"\"\n\t        Compute action probabilities from inputs.\n", "        Args:\n\t            x (torch.Tensor): input to network.\n\t        Return:\n\t            action_probs (torch.Tensor):\n\t        \"\"\"\n\t        if self._mlp_actlayer:\n\t            x = self.mlp(x)\n\t        if self._multidiscrete_action:\n\t            action_probs = []\n\t            for action_out in self.action_outs:\n", "                action_dist = action_out(x)\n\t                action_prob = action_dist.probs\n\t                action_probs.append(action_prob)\n\t            action_probs = torch.cat(action_probs, dim=-1)\n\t        elif self._continuous_action:\n\t            raise ValueError(\"Normal distribution has no `probs` attribute!\")\n\t        else:\n\t            action_dists = self.action_out(x)\n\t            action_probs = action_dists.probs\n\t        return action_probs\n", "    @property\n\t    def output_size(self) -> int:\n\t        if self._multidiscrete_action:\n\t            return len(self.action_outs)\n\t        else:\n\t            return self.action_out.output_size\n"]}
{"filename": "envs/__init__.py", "chunked_list": ["from gym.envs.registration import register\n\tfrom envs.slimevolley.slimevolley_wrapper import VolleyballEnv\n\tregister(\n\t    id=\"ToyEnv-v0\",\n\t    entry_point=\"envs.toy:ToyEnv\",\n\t    kwargs={'wind':True, 'onehot_state':True},\n\t)\n\tregister(\n\t    id='SlimeVolley-v0',\n\t    entry_point='envs:VolleyballEnv'\n", ")\n\tregister(\n\t    id='SumoAnts-v0',\n\t    entry_point='envs.robosumo.envs:SumoEnv',\n\t    kwargs={\n\t        'agent_names': ['ant', 'ant'],\n\t        'agent_densities': [13., 13.],\n\t        'tatami_size': 2.0,\n\t        'timestep_limit': 500,\n\t        'reward_shape': 1. \n", "    },\n\t)"]}
{"filename": "envs/toy.py", "chunked_list": ["import numpy as np\n\tfrom gym import Env, spaces\n\tfrom gym.utils import seeding\n\tfrom contextlib import closing\n\tfrom io import StringIO\n\timport sys\n\tUP = 0\n\tRIGHT = 1\n\tDOWN = 2\n\tLEFT = 3\n", "DELTA = np.array([\n\t    (-1, 0),\n\t    (0, 1),\n\t    (1, 0),\n\t    (0, -1)\n\t])\n\tclass ToyEnv(Env):\n\t    def __init__(self, wind = True, onehot_state=True) -> None:\n\t        self.onehot_state = onehot_state\n\t        self.shape = (4, 4)\n", "        self.nS = np.prod(self.shape)\n\t        self.nA = 4\n\t        # self.observation_space = spaces.Discrete(self.nS)\n\t        self.observation_space = spaces.Box(low=0, high=1, shape=(16,)) if self.onehot_state else spaces.Discrete(self.nS)\n\t        self.action_space = spaces.Discrete(self.nA)\n\t        self.use_wind = wind\n\t        self._wind = np.ones(self.shape)\n\t        self._water = np.zeros(self.shape)\n\t        self._water[3, :] = 1\n\t        self.start = (2, 0)\n", "        self.terminate = (2, 3)\n\t        self.seed()\n\t    def seed(self, seed=None):\n\t        self.np_random, seed = seeding.np_random(seed)\n\t        return [seed]\n\t    def reset(self):\n\t        self.visited = np.zeros(self.shape)\n\t        self.current_step = 0\n\t        self.s = np.array(self.start)\n\t        self.visited[tuple(self.s)] += 1\n", "        state = np.ravel_multi_index(self.s, self.shape)\n\t        if self.onehot_state:\n\t            state = np.eye(self.nS)[state]\n\t        return state\n\t    def step(self, action):\n\t        self.current_step += 1\n\t        reward = 0\n\t        done = False\n\t        if self.use_wind and self._wind[tuple(self.s)]:\n\t            if self.np_random.random() >= 0.5:\n", "                action = self.np_random.choice([UP, RIGHT, DOWN, LEFT])\n\t        new_pos1 = self.s + DELTA[action]\n\t        new_pos1 = self._limit_coordinates(new_pos1).astype(int)\n\t        if tuple(new_pos1) == self.terminate:\n\t            reward = 1\n\t            done = True\n\t        elif self._water[tuple(new_pos1)]:\n\t            reward = -1\n\t        if self.current_step >= 25:\n\t            done = True\n", "        self.visited[tuple(new_pos1)] += 1\n\t        self.s = new_pos1\n\t        state = np.ravel_multi_index(self.s, self.shape)\n\t        if self.onehot_state:\n\t            state = np.eye(self.nS)[state]\n\t        return state, reward, done, {\"step\": self.current_step}\n\t    def render(self, mode=\"human\"):\n\t        outfile = StringIO() if mode == \"ansi\" else sys.stdout\n\t        for s in range(self.nS):\n\t            position = np.unravel_index(s, self.shape)\n", "            if tuple(self.s) == position:\n\t                output = \" x \"\n\t            # Print terminal state\n\t            elif position == self.terminate:\n\t                output = \" T \"\n\t            elif self._water[position]:\n\t                output = \" W \"\n\t            else:\n\t                output = \" o \"\n\t            if position[1] == 0:\n", "                output = output.lstrip()\n\t            if position[1] == self.shape[1] - 1:\n\t                output = output.rstrip()\n\t                output += \"\\n\"\n\t            outfile.write(output)\n\t        outfile.write(\"\\n\")\n\t        # No need to return anything for human\n\t        if mode != \"human\":\n\t            with closing(outfile):\n\t                return outfile.getvalue()\n", "    def _limit_coordinates(self, coord):\n\t        \"\"\"\n\t        Prevent the agent from falling out of the grid world\n\t        :param coord:\n\t        :return:\n\t        \"\"\"\n\t        coord[0] = min(coord[0], self.shape[0] - 1)\n\t        coord[0] = max(coord[0], 0)\n\t        coord[1] = min(coord[1], self.shape[1] - 1)\n\t        coord[1] = max(coord[1], 0)\n", "        return coord\n\tif __name__ == \"__main__\":\n\t    env = ToyEnv()\n\t    env.seed(0)\n\t    env.action_space.seed(0)\n\t    state_freq = np.zeros(env.shape)\n\t    s = env.reset()\n\t    env.render()\n\t    pos = np.unravel_index(s, env.shape)\n\t    while True:\n", "        a = env.action_space.sample()\n\t        s,r,done,info = env.step(a)    \n\t        env.render()\n\t        pos = np.unravel_index(s, env.shape)\n\t        if done:\n\t            state_freq += env.visited\n\t            print(np.unravel_index(s, env.shape))\n\t            print(info)\n\t            break\n\t    print(state_freq)   "]}
{"filename": "envs/slimevolley/slimevolley_wrapper.py", "chunked_list": ["from .slimevolleygym import SlimeVolleyEnv\n\timport numpy as np\n\tclass VolleyballEnv(SlimeVolleyEnv):\n\t    def __init__(self) -> None:\n\t        super().__init__()\n\t        self.from_pixels = False # super setting\n\t        self.atari_mode = True   # super setting\n\t        self.survival_reward = True\n\t        self.num_agents = 2\n\t        self.act_shape = (self.num_agents, 1)\n", "        self.obs_shape = (self.num_agents, *self.observation_space.shape)\n\t        self.done_shape = (self.num_agents, 1)\n\t        self.reward_shape = (self.num_agents, 1)\n\t        self.is_vector_env = True\n\t    def reset(self):\n\t        obs = super().reset()\n\t        return np.array([obs, obs], dtype=np.float32)\n\t    def step(self, action: np.ndarray):\n\t        action = action.squeeze()\n\t        _obs, _reward, _done, info = super().step(action[0], action[1])\n", "        obs = np.array([_obs, info[\"otherObs\"]], dtype=np.float32)\n\t        done = np.array([[_done], [_done]], dtype=np.float32)\n\t        reward = np.array([[_reward], [-_reward]], dtype=np.float32)\n\t        if self.survival_reward:\n\t            reward += 0\n\t        if np.all(done):\n\t            info['score'] = (np.sign(info['ale.lives'] - info['ale.otherLives'])) / 2 + 0.5\n\t        return obs, reward, done, info"]}
{"filename": "envs/slimevolley/slimevolleygym.py", "chunked_list": ["\"\"\"\n\tCode from https://github.com/hardmaru/slimevolleygym\n\tPort of Neural Slime Volleyball to Python Gym Environment\n\tDavid Ha (2020)\n\tOriginal version:\n\thttps://otoro.net/slimevolley\n\thttps://blog.otoro.net/2015/03/28/neural-slime-volleyball/\n\thttps://github.com/hardmaru/neuralslimevolley\n\tNo dependencies apart from Numpy and Gym\n\t\"\"\"\n", "import logging\n\timport math\n\timport gym\n\tfrom gym import spaces\n\tfrom gym.utils import seeding\n\tfrom gym.envs.registration import register\n\timport numpy as np\n\timport cv2 # installed with gym anyways\n\tfrom collections import deque\n\tnp.set_printoptions(threshold=20, precision=3, suppress=True, linewidth=200)\n", "# game settings:\n\tRENDER_MODE = True\n\tREF_W = 24*2\n\tREF_H = REF_W\n\tREF_U = 1.5 # ground height\n\tREF_WALL_WIDTH = 1.0 # wall width\n\tREF_WALL_HEIGHT = 3.5\n\tPLAYER_SPEED_X = 10*1.75\n\tPLAYER_SPEED_Y = 10*1.35\n\tMAX_BALL_SPEED = 15*1.5\n", "TIMESTEP = 1/30.\n\tNUDGE = 0.1\n\tFRICTION = 1.0 # 1 means no FRICTION, less means FRICTION\n\tINIT_DELAY_FRAMES = 30\n\tGRAVITY = -9.8*2*1.5\n\tMAXLIVES = 5 # game ends when one agent loses this many games\n\tWINDOW_WIDTH = 1200\n\tWINDOW_HEIGHT = 500\n\tFACTOR = WINDOW_WIDTH / REF_W\n\t# if set to true, renders using cv2 directly on numpy array\n", "# (otherwise uses pyglet / opengl -> much smoother for human player)\n\tPIXEL_MODE = False \n\tPIXEL_SCALE = 4 # first render at multiple of Pixel Obs resolution, then downscale. Looks better.\n\tPIXEL_WIDTH = 84*2*1\n\tPIXEL_HEIGHT = 84*1\n\tdef setNightColors():\n\t  ### night time color:\n\t  global BALL_COLOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR\n\t  global PIXEL_AGENT_LEFT_COLOR, PIXEL_AGENT_RIGHT_COLOR\n\t  global BACKGROUND_COLOR, FENCE_COLOR, COIN_COLOR, GROUND_COLOR\n", "  BALL_COLOR = (217, 79, 0)\n\t  AGENT_LEFT_COLOR = (35, 93, 188)\n\t  AGENT_RIGHT_COLOR = (255, 236, 0)\n\t  PIXEL_AGENT_LEFT_COLOR = (255, 191, 0) # AMBER\n\t  PIXEL_AGENT_RIGHT_COLOR = (255, 191, 0) # AMBER\n\t  BACKGROUND_COLOR = (11, 16, 19)\n\t  FENCE_COLOR = (102, 56, 35)\n\t  COIN_COLOR = FENCE_COLOR\n\t  GROUND_COLOR = (116, 114, 117)\n\tdef setDayColors():\n", "  ### day time color:\n\t  ### note: do not use day time colors for pixel-obs training.\n\t  global BALL_COLOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR\n\t  global PIXEL_AGENT_LEFT_COLOR, PIXEL_AGENT_RIGHT_COLOR\n\t  global BACKGROUND_COLOR, FENCE_COLOR, COIN_COLOR, GROUND_COLOR\n\t  global PIXEL_SCALE, PIXEL_WIDTH, PIXEL_HEIGHT\n\t  PIXEL_SCALE = int(4*1.0)\n\t  PIXEL_WIDTH = int(84*2*1.0)\n\t  PIXEL_HEIGHT = int(84*1.0)\n\t  BALL_COLOR = (255, 200, 20)\n", "  AGENT_LEFT_COLOR = (240, 75, 0)\n\t  AGENT_RIGHT_COLOR = (0, 150, 255)\n\t  PIXEL_AGENT_LEFT_COLOR = (240, 75, 0)\n\t  PIXEL_AGENT_RIGHT_COLOR = (0, 150, 255)\n\t  BACKGROUND_COLOR = (255, 255, 255)\n\t  FENCE_COLOR = (240, 210, 130)\n\t  COIN_COLOR = FENCE_COLOR\n\t  GROUND_COLOR = (128, 227, 153)\n\tsetNightColors()\n\t# by default, don't load rendering (since we want to use it in headless cloud machines)\n", "rendering = None\n\tdef checkRendering():\n\t  global rendering\n\t  if rendering is None:\n\t    from gym.envs.classic_control import rendering as rendering\n\tdef setPixelObsMode():\n\t  \"\"\"\n\t  used for experimental pixel-observation mode\n\t  note: new dim's chosen to be PIXEL_SCALE (2x) as Pixel Obs dims (will be downsampled)\n\t  also, both agent colors are identical, to potentially facilitate multiagent\n", "  \"\"\"\n\t  global WINDOW_WIDTH, WINDOW_HEIGHT, FACTOR, AGENT_LEFT_COLOR, AGENT_RIGHT_COLOR, PIXEL_MODE\n\t  PIXEL_MODE = True\n\t  WINDOW_WIDTH = PIXEL_WIDTH * PIXEL_SCALE\n\t  WINDOW_HEIGHT = PIXEL_HEIGHT * PIXEL_SCALE\n\t  FACTOR = WINDOW_WIDTH / REF_W\n\t  AGENT_LEFT_COLOR = PIXEL_AGENT_LEFT_COLOR\n\t  AGENT_RIGHT_COLOR = PIXEL_AGENT_RIGHT_COLOR\n\tdef upsize_image(img):\n\t  return cv2.resize(img, (PIXEL_WIDTH * PIXEL_SCALE, PIXEL_HEIGHT * PIXEL_SCALE), interpolation=cv2.INTER_NEAREST)\n", "def downsize_image(img):\n\t  return cv2.resize(img, (PIXEL_WIDTH, PIXEL_HEIGHT), interpolation=cv2.INTER_AREA)\n\t# conversion from space to pixels (allows us to render to diff resolutions)\n\tdef toX(x):\n\t  return (x+REF_W/2)*FACTOR\n\tdef toP(x):\n\t  return (x)*FACTOR\n\tdef toY(y):\n\t  return y*FACTOR\n\tclass DelayScreen:\n", "  \"\"\" initially the ball is held still for INIT_DELAY_FRAMES(30) frames \"\"\"\n\t  def __init__(self, life=INIT_DELAY_FRAMES):\n\t    self.life = 0\n\t    self.reset(life)\n\t  def reset(self, life=INIT_DELAY_FRAMES):\n\t    self.life = life\n\t  def status(self):\n\t    if (self.life == 0):\n\t      return True\n\t    self.life -= 1\n", "    return False\n\tdef make_half_circle(radius=10, res=20, filled=True):\n\t  \"\"\" helper function for pyglet renderer\"\"\"\n\t  points = []\n\t  for i in range(res+1):\n\t    ang = math.pi-math.pi*i / res\n\t    points.append((math.cos(ang)*radius, math.sin(ang)*radius))\n\t  if filled:\n\t    return rendering.FilledPolygon(points)\n\t  else:\n", "    return rendering.PolyLine(points, True)\n\tdef _add_attrs(geom, color):\n\t  \"\"\" help scale the colors from 0-255 to 0.0-1.0 (pyglet renderer) \"\"\"\n\t  r = color[0]\n\t  g = color[1]\n\t  b = color[2]\n\t  geom.set_color(r/255., g/255., b/255.)\n\tdef create_canvas(canvas, c):\n\t  if PIXEL_MODE:\n\t    result = np.ones((WINDOW_HEIGHT, WINDOW_WIDTH, 3), dtype=np.uint8)\n", "    for channel in range(3):\n\t      result[:, :, channel] *= c[channel]\n\t    return result\n\t  else:\n\t    rect(canvas, 0, 0, WINDOW_WIDTH, -WINDOW_HEIGHT, color=BACKGROUND_COLOR)\n\t    return canvas\n\tdef rect(canvas, x, y, width, height, color):\n\t  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n\t  if PIXEL_MODE:\n\t    canvas = cv2.rectangle(canvas, (round(x), round(WINDOW_HEIGHT-y)),\n", "      (round(x+width), round(WINDOW_HEIGHT-y+height)),\n\t      color, thickness=-1, lineType=cv2.LINE_AA)\n\t    return canvas\n\t  else:\n\t    box = rendering.make_polygon([(0,0), (0,-height), (width, -height), (width,0)])\n\t    trans = rendering.Transform()\n\t    trans.set_translation(x, y)\n\t    _add_attrs(box, color)\n\t    box.add_attr(trans)\n\t    canvas.add_onetime(box)\n", "    return canvas\n\tdef half_circle(canvas, x, y, r, color):\n\t  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n\t  if PIXEL_MODE:\n\t    return cv2.ellipse(canvas, (round(x), WINDOW_HEIGHT-round(y)),\n\t      (round(r), round(r)), 0, 0, -180, color, thickness=-1, lineType=cv2.LINE_AA)\n\t  else:\n\t    geom = make_half_circle(r)\n\t    trans = rendering.Transform()\n\t    trans.set_translation(x, y)\n", "    _add_attrs(geom, color)\n\t    geom.add_attr(trans)\n\t    canvas.add_onetime(geom)\n\t    return canvas\n\tdef circle(canvas, x, y, r, color):\n\t  \"\"\" Processing style function to make it easy to port p5.js program to python \"\"\"\n\t  if PIXEL_MODE:\n\t    return cv2.circle(canvas, (round(x), round(WINDOW_HEIGHT-y)), round(r),\n\t      color, thickness=-1, lineType=cv2.LINE_AA)\n\t  else:\n", "    geom = rendering.make_circle(r, res=40)\n\t    trans = rendering.Transform()\n\t    trans.set_translation(x, y)\n\t    _add_attrs(geom, color)\n\t    geom.add_attr(trans)\n\t    canvas.add_onetime(geom)\n\t    return canvas\n\tclass Particle:\n\t  \"\"\" used for the ball, and also for the round stub above the fence \"\"\"\n\t  def __init__(self, x, y, vx, vy, r, c):\n", "    self.x = x\n\t    self.y = y\n\t    self.prev_x = self.x\n\t    self.prev_y = self.y\n\t    self.vx = vx\n\t    self.vy = vy\n\t    self.r = r\n\t    self.c = c\n\t  def display(self, canvas):\n\t    return circle(canvas, toX(self.x), toY(self.y), toP(self.r), color=self.c)\n", "  def move(self):\n\t    self.prev_x = self.x\n\t    self.prev_y = self.y\n\t    self.x += self.vx * TIMESTEP\n\t    self.y += self.vy * TIMESTEP\n\t  def applyAcceleration(self, ax, ay):\n\t    self.vx += ax * TIMESTEP\n\t    self.vy += ay * TIMESTEP\n\t  def checkEdges(self):\n\t    if (self.x<=(self.r-REF_W/2)):\n", "      self.vx *= -FRICTION\n\t      self.x = self.r-REF_W/2+NUDGE*TIMESTEP\n\t    if (self.x >= (REF_W/2-self.r)):\n\t      self.vx *= -FRICTION;\n\t      self.x = REF_W/2-self.r-NUDGE*TIMESTEP\n\t    if (self.y<=(self.r+REF_U)):\n\t      self.vy *= -FRICTION\n\t      self.y = self.r+REF_U+NUDGE*TIMESTEP\n\t      if (self.x <= 0):\n\t        return -1\n", "      else:\n\t        return 1\n\t    if (self.y >= (REF_H-self.r)):\n\t      self.vy *= -FRICTION\n\t      self.y = REF_H-self.r-NUDGE*TIMESTEP\n\t    # fence:\n\t    if ((self.x <= (REF_WALL_WIDTH/2+self.r)) and (self.prev_x > (REF_WALL_WIDTH/2+self.r)) and (self.y <= REF_WALL_HEIGHT)):\n\t      self.vx *= -FRICTION\n\t      self.x = REF_WALL_WIDTH/2+self.r+NUDGE*TIMESTEP\n\t    if ((self.x >= (-REF_WALL_WIDTH/2-self.r)) and (self.prev_x < (-REF_WALL_WIDTH/2-self.r)) and (self.y <= REF_WALL_HEIGHT)):\n", "      self.vx *= -FRICTION\n\t      self.x = -REF_WALL_WIDTH/2-self.r-NUDGE*TIMESTEP\n\t    return 0;\n\t  def getDist2(self, p): # returns distance squared from p\n\t    dy = p.y - self.y\n\t    dx = p.x - self.x\n\t    return (dx*dx+dy*dy)\n\t  def isColliding(self, p): # returns true if it is colliding w/ p\n\t    r = self.r+p.r\n\t    return (r*r > self.getDist2(p)) # if distance is less than total radius, then colliding.\n", "  def bounce(self, p): # bounce two balls that have collided (this and that)\n\t    abx = self.x-p.x\n\t    aby = self.y-p.y\n\t    abd = math.sqrt(abx*abx+aby*aby)\n\t    abx /= abd # normalize\n\t    aby /= abd\n\t    nx = abx # reuse calculation\n\t    ny = aby\n\t    abx *= NUDGE\n\t    aby *= NUDGE\n", "    while(self.isColliding(p)):\n\t      self.x += abx\n\t      self.y += aby\n\t    ux = self.vx - p.vx\n\t    uy = self.vy - p.vy\n\t    un = ux*nx + uy*ny\n\t    unx = nx*(un*2.) # added factor of 2\n\t    uny = ny*(un*2.) # added factor of 2\n\t    ux -= unx\n\t    uy -= uny\n", "    self.vx = ux + p.vx\n\t    self.vy = uy + p.vy\n\t  def limitSpeed(self, minSpeed, maxSpeed):\n\t    mag2 = self.vx*self.vx+self.vy*self.vy;\n\t    if (mag2 > (maxSpeed*maxSpeed) ):\n\t      mag = math.sqrt(mag2)\n\t      self.vx /= mag\n\t      self.vy /= mag\n\t      self.vx *= maxSpeed\n\t      self.vy *= maxSpeed\n", "    if (mag2 < (minSpeed*minSpeed) ):\n\t      mag = math.sqrt(mag2)\n\t      self.vx /= mag\n\t      self.vy /= mag\n\t      self.vx *= minSpeed\n\t      self.vy *= minSpeed\n\tclass Wall:\n\t  \"\"\" used for the fence, and also the ground \"\"\"\n\t  def __init__(self, x, y, w, h, c):\n\t    self.x = x;\n", "    self.y = y;\n\t    self.w = w;\n\t    self.h = h;\n\t    self.c = c\n\t  def display(self, canvas):\n\t    return rect(canvas, toX(self.x-self.w/2), toY(self.y+self.h/2), toP(self.w), toP(self.h), color=self.c)\n\tclass RelativeState:\n\t  \"\"\"\n\t  keeps track of the obs.\n\t  Note: the observation is from the perspective of the agent.\n", "  an agent playing either side of the fence must see obs the same way\n\t  \"\"\"\n\t  def __init__(self):\n\t    # agent\n\t    self.x = 0\n\t    self.y = 0\n\t    self.vx = 0\n\t    self.vy = 0\n\t    # ball\n\t    self.bx = 0\n", "    self.by = 0\n\t    self.bvx = 0\n\t    self.bvy = 0\n\t    # opponent\n\t    self.ox = 0\n\t    self.oy = 0\n\t    self.ovx = 0\n\t    self.ovy = 0\n\t  def getObservation(self):\n\t    result = [self.x, self.y, self.vx, self.vy,\n", "              self.bx, self.by, self.bvx, self.bvy,\n\t              self.ox, self.oy, self.ovx, self.ovy]\n\t    scaleFactor = 10.0  # scale inputs to be in the order of magnitude of 10 for neural network.\n\t    result = np.array(result) / scaleFactor\n\t    return result\n\tclass Agent:\n\t  \"\"\" keeps track of the agent in the game. note this is not the policy network \"\"\"\n\t  def __init__(self, dir, x, y, c):\n\t    self.dir = dir # -1 means left, 1 means right player for symmetry.\n\t    self.x = x\n", "    self.y = y\n\t    self.r = 1.5\n\t    self.c = c\n\t    self.vx = 0\n\t    self.vy = 0\n\t    self.desired_vx = 0\n\t    self.desired_vy = 0\n\t    self.state = RelativeState()\n\t    self.emotion = \"happy\"; # hehe...\n\t    self.life = MAXLIVES\n", "  def lives(self):\n\t    return self.life\n\t  def setAction(self, action):\n\t    forward = False\n\t    backward = False\n\t    jump = False\n\t    if action[0] > 0:\n\t      forward = True\n\t    if action[1] > 0:\n\t      backward = True\n", "    if action[2] > 0:\n\t      jump = True\n\t    self.desired_vx = 0\n\t    self.desired_vy = 0\n\t    if (forward and (not backward)):\n\t      self.desired_vx = -PLAYER_SPEED_X\n\t    if (backward and (not forward)):\n\t      self.desired_vx = PLAYER_SPEED_X\n\t    if jump:\n\t      self.desired_vy = PLAYER_SPEED_Y\n", "  def move(self):\n\t    self.x += self.vx * TIMESTEP\n\t    self.y += self.vy * TIMESTEP\n\t  def step(self):\n\t    self.x += self.vx * TIMESTEP\n\t    self.y += self.vy * TIMESTEP\n\t  def update(self):\n\t    self.vy += GRAVITY * TIMESTEP\n\t    if (self.y <= REF_U + NUDGE*TIMESTEP):\n\t      self.vy = self.desired_vy\n", "    self.vx = self.desired_vx*self.dir\n\t    self.move()\n\t    if (self.y <= REF_U):\n\t      self.y = REF_U;\n\t      self.vy = 0;\n\t    # stay in their own half:\n\t    if (self.x*self.dir <= (REF_WALL_WIDTH/2+self.r) ):\n\t      self.vx = 0;\n\t      self.x = self.dir*(REF_WALL_WIDTH/2+self.r)\n\t    if (self.x*self.dir >= (REF_W/2-self.r) ):\n", "      self.vx = 0;\n\t      self.x = self.dir*(REF_W/2-self.r)\n\t  def updateState(self, ball, opponent):\n\t    \"\"\" normalized to side, appears different for each agent's perspective\"\"\"\n\t    # agent's self\n\t    self.state.x = self.x*self.dir\n\t    self.state.y = self.y\n\t    self.state.vx = self.vx*self.dir\n\t    self.state.vy = self.vy\n\t    # ball\n", "    self.state.bx = ball.x*self.dir\n\t    self.state.by = ball.y\n\t    self.state.bvx = ball.vx*self.dir\n\t    self.state.bvy = ball.vy\n\t    # opponent\n\t    self.state.ox = opponent.x*(-self.dir)\n\t    self.state.oy = opponent.y\n\t    self.state.ovx = opponent.vx*(-self.dir)\n\t    self.state.ovy = opponent.vy\n\t  def getObservation(self):\n", "    return self.state.getObservation()\n\t  def display(self, canvas, bx, by):\n\t    x = self.x\n\t    y = self.y\n\t    r = self.r\n\t    angle = math.pi * 60 / 180\n\t    if self.dir == 1:\n\t      angle = math.pi * 120 / 180\n\t    eyeX = 0\n\t    eyeY = 0\n", "    canvas = half_circle(canvas, toX(x), toY(y), toP(r), color=self.c)\n\t    # track ball with eyes (replace with observed info later):\n\t    c = math.cos(angle)\n\t    s = math.sin(angle)\n\t    ballX = bx-(x+(0.6)*r*c);\n\t    ballY = by-(y+(0.6)*r*s);\n\t    if (self.emotion == \"sad\"):\n\t      ballX = -self.dir\n\t      ballY = -3\n\t    dist = math.sqrt(ballX*ballX+ballY*ballY)\n", "    eyeX = ballX/dist\n\t    eyeY = ballY/dist\n\t    canvas = circle(canvas, toX(x+(0.6)*r*c), toY(y+(0.6)*r*s), toP(r)*0.3, color=(255, 255, 255))\n\t    canvas = circle(canvas, toX(x+(0.6)*r*c+eyeX*0.15*r), toY(y+(0.6)*r*s+eyeY*0.15*r), toP(r)*0.1, color=(0, 0, 0))\n\t    # draw coins (lives) left\n\t    for i in range(1, self.life):\n\t      canvas = circle(canvas, toX(self.dir*(REF_W/2+0.5-i*2.)), WINDOW_HEIGHT-toY(1.5), toP(0.5), color=COIN_COLOR)\n\t    return canvas\n\tclass BaselinePolicy:\n\t  \"\"\" Tiny RNN policy with only 120 parameters of otoro.net/slimevolley agent \"\"\"\n", "  def __init__(self):\n\t    self.nGameInput = 8 # 8 states for agent\n\t    self.nGameOutput = 3 # 3 buttons (forward, backward, jump)\n\t    self.nRecurrentState = 4 # extra recurrent states for feedback.\n\t    self.nOutput = self.nGameOutput+self.nRecurrentState\n\t    self.nInput = self.nGameInput+self.nOutput\n\t    # store current inputs and outputs\n\t    self.inputState = np.zeros(self.nInput)\n\t    self.outputState = np.zeros(self.nOutput)\n\t    self.prevOutputState = np.zeros(self.nOutput)\n", "    \"\"\"See training details: https://blog.otoro.net/2015/03/28/neural-slime-volleyball/ \"\"\"\n\t    self.weight = np.array(\n\t      [7.5719, 4.4285, 2.2716, -0.3598, -7.8189, -2.5422, -3.2034, 0.3935, 1.2202, -0.49, -0.0316, 0.5221, 0.7026, 0.4179, -2.1689,\n\t       1.646, -13.3639, 1.5151, 1.1175, -5.3561, 5.0442, 0.8451, 0.3987, -2.9501, -3.7811, -5.8994, 6.4167, 2.5014, 7.338, -2.9887,\n\t       2.4586, 13.4191, 2.7395, -3.9708, 1.6548, -2.7554, -1.5345, -6.4708, 9.2426, -0.7392, 0.4452, 1.8828, -2.6277, -10.851, -3.2353,\n\t       -4.4653, -3.1153, -1.3707, 7.318, 16.0902, 1.4686, 7.0391, 1.7765, -1.155, 2.6697, -8.8877, 1.1958, -3.2839, -5.4425, 1.6809,\n\t       7.6812, -2.4732, 1.738, 0.3781, 0.8718, 2.5886, 1.6911, 1.2953, -9.0052, -4.6038, -6.7447, -2.5528, 0.4391, -4.9278, -3.6695,\n\t       -4.8673, -1.6035, 1.5011, -5.6124, 4.9747, 1.8998, 3.0359, 6.2983, -4.8568, -2.1888, -4.1143, -3.9874, -0.0459, 4.7134, 2.8952,\n\t       -9.3627, -4.685, 0.3601, -1.3699, 9.7294, 11.5596, 0.1918, 3.0783, 0.0329, -0.1362, -0.1188, -0.7579, 0.3278, -0.977, -0.9377])\n\t    self.bias = np.array([2.2935,-2.0353,-1.7786,5.4567,-3.6368,3.4996,-0.0685])\n", "    # unflatten weight, convert it into 7x15 matrix.\n\t    self.weight = self.weight.reshape(self.nGameOutput+self.nRecurrentState,\n\t      self.nGameInput+self.nGameOutput+self.nRecurrentState)\n\t  def reset(self):\n\t    self.inputState = np.zeros(self.nInput)\n\t    self.outputState = np.zeros(self.nOutput)\n\t    self.prevOutputState = np.zeros(self.nOutput)\n\t  def _forward(self):\n\t    self.prevOutputState = self.outputState\n\t    self.outputState = np.tanh(np.dot(self.weight, self.inputState)+self.bias)\n", "  def _setInputState(self, obs):\n\t    # obs is: (op is opponent). obs is also from perspective of the agent (x values negated for other agent)\n\t    [x, y, vx, vy, ball_x, ball_y, ball_vx, ball_vy, op_x, op_y, op_vx, op_vy] = obs\n\t    self.inputState[0:self.nGameInput] = np.array([x, y, vx, vy, ball_x, ball_y, ball_vx, ball_vy])\n\t    self.inputState[self.nGameInput:] = self.outputState\n\t  def _getAction(self):\n\t    forward = 0\n\t    backward = 0\n\t    jump = 0\n\t    if (self.outputState[0] > 0.75):\n", "      forward = 1\n\t    if (self.outputState[1] > 0.75):\n\t      backward = 1\n\t    if (self.outputState[2] > 0.75):\n\t      jump = 1\n\t    return [forward, backward, jump]\n\t  def predict(self, obs):\n\t    \"\"\" take obs, update rnn state, return action \"\"\"\n\t    self._setInputState(obs)\n\t    self._forward()\n", "    return self._getAction()\n\tclass Game:\n\t  \"\"\"\n\t  the main slime volley game.\n\t  can be used in various settings, such as ai vs ai, ai vs human, human vs human\n\t  \"\"\"\n\t  def __init__(self, np_random=np.random):\n\t    self.ball = None\n\t    self.ground = None\n\t    self.fence = None\n", "    self.fenceStub = None\n\t    self.agent_left = None\n\t    self.agent_right = None\n\t    self.delayScreen = None\n\t    self.np_random = np_random\n\t    self.reset()\n\t  def reset(self):\n\t    self.ground = Wall(0, 0.75, REF_W, REF_U, c=GROUND_COLOR)\n\t    self.fence = Wall(0, 0.75 + REF_WALL_HEIGHT/2, REF_WALL_WIDTH, (REF_WALL_HEIGHT-1.5), c=FENCE_COLOR)\n\t    self.fenceStub = Particle(0, REF_WALL_HEIGHT, 0, 0, REF_WALL_WIDTH/2, c=FENCE_COLOR);\n", "    ball_vx = self.np_random.uniform(low=-20, high=20)\n\t    ball_vy = self.np_random.uniform(low=10, high=25)\n\t    self.ball = Particle(0, REF_W/4, ball_vx, ball_vy, 0.5, c=BALL_COLOR);\n\t    self.agent_left = Agent(-1, -REF_W/4, 1.5, c=AGENT_LEFT_COLOR)\n\t    self.agent_right = Agent(1, REF_W/4, 1.5, c=AGENT_RIGHT_COLOR)\n\t    self.agent_left.updateState(self.ball, self.agent_right)\n\t    self.agent_right.updateState(self.ball, self.agent_left)\n\t    self.delayScreen = DelayScreen()\n\t  def newMatch(self):\n\t    ball_vx = self.np_random.uniform(low=-20, high=20)\n", "    ball_vy = self.np_random.uniform(low=10, high=25)\n\t    self.ball = Particle(0, REF_W/4, ball_vx, ball_vy, 0.5, c=BALL_COLOR);\n\t    self.delayScreen.reset()\n\t  def step(self):\n\t    \"\"\" main game loop \"\"\"\n\t    self.betweenGameControl()\n\t    self.agent_left.update()\n\t    self.agent_right.update()\n\t    if self.delayScreen.status():\n\t      self.ball.applyAcceleration(0, GRAVITY)\n", "      self.ball.limitSpeed(0, MAX_BALL_SPEED)\n\t      self.ball.move()\n\t    if (self.ball.isColliding(self.agent_left)):\n\t      self.ball.bounce(self.agent_left)\n\t    if (self.ball.isColliding(self.agent_right)):\n\t      self.ball.bounce(self.agent_right)\n\t    if (self.ball.isColliding(self.fenceStub)):\n\t      self.ball.bounce(self.fenceStub)\n\t    # negated, since we want reward to be from the persepctive of right agent being trained.\n\t    result = -self.ball.checkEdges()\n", "    if (result != 0):\n\t      self.newMatch() # not reset, but after a point is scored\n\t      if result < 0: # baseline agent won\n\t        self.agent_left.emotion = \"happy\"\n\t        self.agent_right.emotion = \"sad\"\n\t        self.agent_right.life -= 1\n\t      else:\n\t        self.agent_left.emotion = \"sad\"\n\t        self.agent_right.emotion = \"happy\"\n\t        self.agent_left.life -= 1\n", "      return result\n\t    # update internal states (the last thing to do)\n\t    self.agent_left.updateState(self.ball, self.agent_right)\n\t    self.agent_right.updateState(self.ball, self.agent_left)\n\t    return result\n\t  def display(self, canvas):\n\t    # background color\n\t    # if PIXEL_MODE is True, canvas is an RGB array.\n\t    # if PIXEL_MODE is False, canvas is viewer object\n\t    canvas = create_canvas(canvas, c=BACKGROUND_COLOR)\n", "    canvas = self.fence.display(canvas)\n\t    canvas = self.fenceStub.display(canvas)\n\t    canvas = self.agent_left.display(canvas, self.ball.x, self.ball.y)\n\t    canvas = self.agent_right.display(canvas, self.ball.x, self.ball.y)\n\t    canvas = self.ball.display(canvas)\n\t    canvas = self.ground.display(canvas)\n\t    return canvas\n\t  def betweenGameControl(self):\n\t    agent = [self.agent_left, self.agent_right]\n\t    if (self.delayScreen.life > 0):\n", "      pass\n\t      '''\n\t      for i in range(2):\n\t        if (agent[i].emotion == \"sad\"):\n\t          agent[i].setAction([0, 0, 0]) # nothing\n\t      '''\n\t    else:\n\t      agent[0].emotion = \"happy\"\n\t      agent[1].emotion = \"happy\"\n\tclass SlimeVolleyEnv(gym.Env):\n", "  \"\"\"\n\t  Gym wrapper for Slime Volley game.\n\t  By default, the agent you are training controls the right agent\n\t  on the right. The agent on the left is controlled by the baseline\n\t  RNN policy.\n\t  Game ends when an agent loses 5 matches (or at t=3000 timesteps).\n\t  Note: Optional mode for MARL experiments, like self-play which\n\t  deviates from Gym env. Can be enabled via supplying optional action\n\t  to override the default baseline agent's policy:\n\t  obs1, reward, done, info = env.step(action1, action2)\n", "  the next obs for the right agent is returned in the optional\n\t  fourth item from the step() method.\n\t  reward is in the perspective of the right agent so the reward\n\t  for the left agent is the negative of this number.\n\t  \"\"\"\n\t  metadata = {\n\t    'render.modes': ['human', 'rgb_array', 'state'],\n\t    'video.frames_per_second' : 50\n\t  }\n\t  # for compatibility with typical atari wrappers\n", "  atari_action_meaning = {\n\t    0: \"NOOP\",\n\t    1: \"FIRE\",\n\t    2: \"UP\",\n\t    3: \"RIGHT\",\n\t    4: \"LEFT\",\n\t    5: \"DOWN\",\n\t    6: \"UPRIGHT\",\n\t    7: \"UPLEFT\",\n\t    8: \"DOWNRIGHT\",\n", "    9: \"DOWNLEFT\",\n\t    10: \"UPFIRE\",\n\t    11: \"RIGHTFIRE\",\n\t    12: \"LEFTFIRE\",\n\t    13: \"DOWNFIRE\",\n\t    14: \"UPRIGHTFIRE\",\n\t    15: \"UPLEFTFIRE\",\n\t    16: \"DOWNRIGHTFIRE\",\n\t    17: \"DOWNLEFTFIRE\",\n\t  }\n", "  atari_action_set = {\n\t    0, # NOOP\n\t    4, # LEFT\n\t    7, # UPLEFT\n\t    2, # UP\n\t    6, # UPRIGHT\n\t    3, # RIGHT\n\t  }\n\t  action_table = [[0, 0, 0], # NOOP\n\t                  [1, 0, 0], # LEFT (forward)\n", "                  [1, 0, 1], # UPLEFT (forward jump)\n\t                  [0, 0, 1], # UP (jump)\n\t                  [0, 1, 1], # UPRIGHT (backward jump)\n\t                  [0, 1, 0]] # RIGHT (backward)\n\t  from_pixels = False\n\t  atari_mode = True\n\t  survival_bonus = False # Depreciated: augment reward, easier to train\n\t  multiagent = True # optional args anyways\n\t  def __init__(self):\n\t    \"\"\"\n", "    Reward modes:\n\t    net score = right agent wins minus left agent wins\n\t    0: returns net score (basic reward)\n\t    1: returns 0.01 x number of timesteps (max 3000) (survival reward)\n\t    2: sum of basic reward and survival reward\n\t    0 is suitable for evaluation, while 1 and 2 may be good for training\n\t    Setting multiagent to True puts in info (4th thing returned in stop)\n\t    the otherObs, the observation for the other agent. See multiagent.py\n\t    Setting self.from_pixels to True makes the observation with multiples\n\t    of 84, since usual atari wrappers downsample to 84x84\n", "    \"\"\"\n\t    self.t = 0\n\t    self.t_limit = 3000\n\t    #self.action_space = spaces.Box(0, 1.0, shape=(3,))\n\t    if self.atari_mode:\n\t      self.action_space = spaces.Discrete(6)\n\t    else:\n\t      self.action_space = spaces.MultiBinary(3)\n\t    if self.from_pixels:\n\t      setPixelObsMode()\n", "      self.observation_space = spaces.Box(low=0, high=255,\n\t        shape=(PIXEL_HEIGHT, PIXEL_WIDTH, 3), dtype=np.uint8)\n\t    else:\n\t      high = np.array([np.finfo(np.float32).max] * 12)\n\t      self.observation_space = spaces.Box(-high, high)\n\t    self.canvas = None\n\t    self.previous_rgbarray = None\n\t    self.game = Game()\n\t    self.ale = self.game.agent_right # for compatibility for some models that need the self.ale.lives() function\n\t    self.policy = BaselinePolicy() # the bad guy\n", "    self.viewer = None\n\t    # another avenue to override the built-in AI's action, going past many env wraps:\n\t    self.otherAction = None\n\t  def seed(self, seed=None):\n\t    self.np_random, seed = seeding.np_random(seed)\n\t    self.game = Game(np_random=self.np_random)\n\t    self.ale = self.game.agent_right # for compatibility for some models that need the self.ale.lives() function\n\t    return [seed]\n\t  def getObs(self):\n\t    if self.from_pixels:\n", "      obs = self.render(mode='state')\n\t      self.canvas = obs\n\t    else:\n\t      obs = self.game.agent_right.getObservation()\n\t    return obs\n\t  def discreteToBox(self, n):\n\t    # convert discrete action n into the actual triplet action\n\t    if isinstance(n, (list, tuple, np.ndarray)): # original input for some reason, just leave it:\n\t      if len(n) == 3:\n\t        return n\n", "    assert (int(n) == n) and (n >= 0) and (n < 6)\n\t    return self.action_table[n]\n\t  def step(self, action, otherAction=None):\n\t    \"\"\"\n\t    baseAction is only used if multiagent mode is True\n\t    note: although the action space is multi-binary, float vectors\n\t    are fine (refer to setAction() to see how they get interpreted)\n\t    \"\"\"\n\t    done = False\n\t    self.t += 1\n", "    if self.otherAction is not None:\n\t      otherAction = self.otherAction\n\t    if otherAction is None: # override baseline policy\n\t      obs = self.game.agent_left.getObservation()\n\t      otherAction = self.policy.predict(obs)\n\t    if self.atari_mode:\n\t      action = self.discreteToBox(action)\n\t      otherAction = self.discreteToBox(otherAction)\n\t    self.game.agent_left.setAction(otherAction)\n\t    self.game.agent_right.setAction(action) # external agent is agent_right\n", "    reward = self.game.step()\n\t    obs = self.getObs()\n\t    if self.t >= self.t_limit:\n\t      done = True\n\t    if self.game.agent_left.life <= 0 or self.game.agent_right.life <= 0:\n\t      done = True\n\t    otherObs = None\n\t    if self.multiagent:\n\t      if self.from_pixels:\n\t        otherObs = cv2.flip(obs, 1) # horizontal flip\n", "      else:\n\t        otherObs = self.game.agent_left.getObservation()\n\t    info = {\n\t      'ale.lives': self.game.agent_right.lives(),\n\t      'ale.otherLives': self.game.agent_left.lives(),\n\t      'otherObs': otherObs,\n\t      'state': self.game.agent_right.getObservation(),\n\t      'otherState': self.game.agent_left.getObservation(),\n\t    }\n\t    if self.survival_bonus:\n", "      return obs, reward+0.01, done, info\n\t    return obs, reward, done, info\n\t  def init_game_state(self):\n\t    self.t = 0\n\t    self.game.reset()\n\t  def reset(self):\n\t    self.init_game_state()\n\t    return self.getObs()\n\t  def checkViewer(self):\n\t    # for opengl viewer\n", "    if self.viewer is None:\n\t      checkRendering()\n\t      self.viewer = rendering.SimpleImageViewer(maxwidth=2160) # macbook pro resolution\n\t  def render(self, mode='human', close=False):\n\t    if PIXEL_MODE:\n\t      if self.canvas is not None: # already rendered\n\t        rgb_array = self.canvas\n\t        self.canvas = None\n\t        if mode == 'rgb_array' or mode == 'human':\n\t          self.checkViewer()\n", "          larger_canvas = upsize_image(rgb_array)\n\t          self.viewer.imshow(larger_canvas)\n\t          if (mode=='rgb_array'):\n\t            return larger_canvas\n\t          else:\n\t            return\n\t      self.canvas = self.game.display(self.canvas)\n\t      # scale down to original res (looks better than rendering directly to lower res)\n\t      self.canvas = downsize_image(self.canvas)\n\t      if mode=='state':\n", "        return np.copy(self.canvas)\n\t      # upsampling w/ nearest interp method gives a retro \"pixel\" effect look\n\t      larger_canvas = upsize_image(self.canvas)\n\t      self.checkViewer()\n\t      self.viewer.imshow(larger_canvas)\n\t      if (mode=='rgb_array'):\n\t        return larger_canvas\n\t    else: # pyglet renderer\n\t      if self.viewer is None:\n\t        checkRendering()\n", "        self.viewer = rendering.Viewer(WINDOW_WIDTH, WINDOW_HEIGHT)\n\t      self.game.display(self.viewer)\n\t      return self.viewer.render(return_rgb_array = mode=='rgb_array')\n\t  def close(self):\n\t    if self.viewer:\n\t      self.viewer.close()\n\t  def get_action_meanings(self):\n\t    return [self.atari_action_meaning[i] for i in self.atari_action_set]\n\tclass SlimeVolleyPixelEnv(SlimeVolleyEnv):\n\t  from_pixels = True\n", "class SlimeVolleyAtariEnv(SlimeVolleyEnv):\n\t  from_pixels = True\n\t  atari_mode = True\n\tclass SlimeVolleySurvivalAtariEnv(SlimeVolleyEnv):\n\t  from_pixels = True\n\t  atari_mode = True\n\t  survival_bonus = True\n\tclass SurvivalRewardEnv(gym.RewardWrapper):\n\t  def __init__(self, env):\n\t    \"\"\"\n", "    adds 0.01 to the reward for every timestep agent survives\n\t    :param env: (Gym Environment) the environment\n\t    \"\"\"\n\t    gym.RewardWrapper.__init__(self, env)\n\t  def reward(self, reward):\n\t    \"\"\"\n\t    adds that extra survival bonus for living a bit longer!\n\t    :param reward: (float)\n\t    \"\"\"\n\t    return reward + 0.01\n", "class FrameStack(gym.Wrapper):\n\t  def __init__(self, env, n_frames):\n\t    \"\"\"Stack n_frames last frames.\n\t    (don't use lazy frames)\n\t    modified from:\n\t    stable_baselines.common.atari_wrappers\n\t    :param env: (Gym Environment) the environment\n\t    :param n_frames: (int) the number of frames to stack\n\t    \"\"\"\n\t    gym.Wrapper.__init__(self, env)\n", "    self.n_frames = n_frames\n\t    self.frames = deque([], maxlen=n_frames)\n\t    shp = env.observation_space.shape\n\t    self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * n_frames),\n\t                                        dtype=env.observation_space.dtype)\n\t  def reset(self):\n\t    obs = self.env.reset()\n\t    for _ in range(self.n_frames):\n\t        self.frames.append(obs)\n\t    return self._get_ob()\n", "  def step(self, action):\n\t    obs, reward, done, info = self.env.step(action)\n\t    self.frames.append(obs)\n\t    return self._get_ob(), reward, done, info\n\t  def _get_ob(self):\n\t    assert len(self.frames) == self.n_frames\n\t    return np.concatenate(list(self.frames), axis=2)\n\t#####################\n\t# helper functions: #\n\t#####################\n", "def multiagent_rollout(env, policy_right, policy_left, render_mode=False):\n\t  \"\"\"\n\t  play one agent vs the other in modified gym-style loop.\n\t  important: returns the score from perspective of policy_right.\n\t  \"\"\"\n\t  obs_right = env.reset()\n\t  obs_left = obs_right # same observation at the very beginning for the other agent\n\t  done = False\n\t  total_reward = 0\n\t  t = 0\n", "  while not done:\n\t    action_right = policy_right.predict(obs_right)\n\t    action_left = policy_left.predict(obs_left)\n\t    # uses a 2nd (optional) parameter for step to put in the other action\n\t    # and returns the other observation in the 4th optional \"info\" param in gym's step()\n\t    obs_right, reward, done, info = env.step(action_right, action_left)\n\t    obs_left = info['otherObs']\n\t    total_reward += reward\n\t    t += 1\n\t    if render_mode:\n", "      env.render()\n\t  return total_reward, t\n\tdef render_atari(obs):\n\t  \"\"\"\n\t  Helper function that takes in a processed obs (84,84,4)\n\t  Useful for visualizing what an Atari agent actually *sees*\n\t  Outputs in Atari visual format (Top: resized to orig dimensions, buttom: 4 frames)\n\t  \"\"\"\n\t  tempObs = []\n\t  obs = np.copy(obs)\n", "  for i in range(4):\n\t    if i == 3:\n\t      latest = np.copy(obs[:, :, i])\n\t    if i > 0: # insert vertical lines\n\t      obs[:, 0, i] = 141\n\t    tempObs.append(obs[:, :, i])\n\t  latest = np.expand_dims(latest, axis=2)\n\t  latest = np.concatenate([latest*255.0] * 3, axis=2).astype(np.uint8)\n\t  latest = cv2.resize(latest, (84 * 8, 84 * 4), interpolation=cv2.INTER_NEAREST)\n\t  tempObs = np.concatenate(tempObs, axis=1)\n", "  tempObs = np.expand_dims(tempObs, axis=2)\n\t  tempObs = np.concatenate([tempObs*255.0] * 3, axis=2).astype(np.uint8)\n\t  tempObs = cv2.resize(tempObs, (84 * 8, 84 * 2), interpolation=cv2.INTER_NEAREST)\n\t  return np.concatenate([latest, tempObs], axis=0)\n\t####################\n\t# Reg envs for gym #\n\t####################\n\t# register(\n\t#     id='SlimeVolley-v0',\n\t#     entry_point='slimevolleygym.slimevolley:SlimeVolleyEnv'\n", "# )\n\t# register(\n\t#     id='SlimeVolleyPixel-v0',\n\t#     entry_point='slimevolleygym.slimevolley:SlimeVolleyPixelEnv'\n\t# )\n\t# register(\n\t#     id='SlimeVolleyNoFrameskip-v0',\n\t#     entry_point='slimevolleygym.slimevolley:SlimeVolleyAtariEnv'\n\t# )\n\t# register(\n", "#     id='SlimeVolleySurvivalNoFrameskip-v0',\n\t#     entry_point='slimevolleygym.slimevolley:SlimeVolleySurvivalAtariEnv'\n\t# )\n\tif __name__==\"__main__\":\n\t  \"\"\"\n\t  Example of how to use Gym env, in single or multiplayer setting\n\t  Humans can override controls:\n\t  left Agent:\n\t  W - Jump\n\t  A - Left\n", "  D - Right\n\t  right Agent:\n\t  Up Arrow, Left Arrow, Right Arrow\n\t  \"\"\"\n\t  if RENDER_MODE:\n\t    from pyglet.window import key\n\t    from time import sleep\n\t  manualAction = [0, 0, 0] # forward, backward, jump\n\t  otherManualAction = [0, 0, 0]\n\t  manualMode = False\n", "  otherManualMode = False\n\t  # taken from https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py\n\t  def key_press(k, mod):\n\t    global manualMode, manualAction, otherManualMode, otherManualAction\n\t    if k == key.LEFT:  manualAction[0] = 1\n\t    if k == key.RIGHT: manualAction[1] = 1\n\t    if k == key.UP:    manualAction[2] = 1\n\t    if (k == key.LEFT or k == key.RIGHT or k == key.UP): manualMode = True\n\t    if k == key.D:     otherManualAction[0] = 1\n\t    if k == key.A:     otherManualAction[1] = 1\n", "    if k == key.W:     otherManualAction[2] = 1\n\t    if (k == key.D or k == key.A or k == key.W): otherManualMode = True\n\t  def key_release(k, mod):\n\t    global manualMode, manualAction, otherManualMode, otherManualAction\n\t    if k == key.LEFT:  manualAction[0] = 0\n\t    if k == key.RIGHT: manualAction[1] = 0\n\t    if k == key.UP:    manualAction[2] = 0\n\t    if k == key.D:     otherManualAction[0] = 0\n\t    if k == key.A:     otherManualAction[1] = 0\n\t    if k == key.W:     otherManualAction[2] = 0\n", "  policy = BaselinePolicy() # defaults to use RNN Baseline for player\n\t  env = SlimeVolleyEnv()\n\t  env.seed(np.random.randint(0, 10000))\n\t  #env.seed(721)\n\t  if RENDER_MODE:\n\t    env.render()\n\t    env.viewer.window.on_key_press = key_press\n\t    env.viewer.window.on_key_release = key_release\n\t  obs = env.reset()\n\t  steps = 0\n", "  total_reward = 0\n\t  action = np.array([0, 0, 0])\n\t  done = False\n\t  while not done:\n\t    if manualMode: # override with keyboard\n\t      action = manualAction\n\t    else:\n\t      action = policy.predict(obs)\n\t    if otherManualMode:\n\t      otherAction = otherManualAction\n", "      obs, reward, done, _ = env.step(action, otherAction)\n\t    else:\n\t      obs, reward, done, _ = env.step(action)\n\t    if reward > 0 or reward < 0:\n\t      print(\"reward\", reward)\n\t      manualMode = False\n\t      otherManualMode = False\n\t    total_reward += reward\n\t    if RENDER_MODE:\n\t      env.render()\n", "      sleep(0.01)\n\t    # make the game go slower for human players to be fair to humans.\n\t    if (manualMode or otherManualMode):\n\t      if PIXEL_MODE:\n\t        sleep(0.01)\n\t      else:\n\t        sleep(0.02)\n\t  env.close()\n\t  print(\"cumulative score\", total_reward)\n"]}
{"filename": "envs/robosumo/envs/sumo.py", "chunked_list": ["\"\"\"\n\tMulti-agent sumo environment.\n\t\"\"\"\n\timport os\n\timport tempfile\n\timport numpy as np\n\timport gym\n\tfrom gym.spaces import Tuple\n\tfrom gym.utils import EzPickle\n\t# MuJoCo 1.5+\n", "from mujoco_py import MjViewer\n\tfrom envs.robosumo.envs import MujocoEnv\n\tfrom . import agents\n\tfrom .utils import construct_scene\n\t_AGENTS = {\n\t    'ant': os.path.join(os.path.dirname(__file__), \"assets\", \"ant.xml\"),\n\t    'bug': os.path.join(os.path.dirname(__file__), \"assets\", \"bug.xml\"),\n\t    'spider': os.path.join(os.path.dirname(__file__), \"assets\", \"spider.xml\"),\n\t}\n\tclass SumoEnv(MujocoEnv, EzPickle):\n", "    \"\"\"\n\t    Multi-agent sumo environment.\n\t    The goal of each agent is to push the other agent outside the tatami area.\n\t    The reward is shaped such that agents learn to prefer staying in the center\n\t    and pushing each other further away from the center. If any of the agents\n\t    gets outside of the tatami (even accidentially), it gets -WIN_REWARD_COEF\n\t    and the opponent gets +WIN_REWARD_COEF.\n\t    \"\"\"\n\t    STAY_IN_CENTER_COEF = 0.1\n\t    DRAW_PENALTY = -1000.\n", "    # MOVE_TO_CENTER_COEF = 0.1\n\t    MOVE_TO_OPP_COEF = 0.1\n\t    PUSH_OUT_COEF = 10.0\n\t    def __init__(self, agent_names,\n\t                 xml_path=None,\n\t                 init_pos_noise=.1,\n\t                 init_vel_noise=.1,\n\t                 agent_kwargs=None,\n\t                 frame_skip=5,\n\t                 tatami_size=2.0,\n", "                 timestep_limit=500,\n\t                 reward_shape=1.,\n\t                 **kwargs):\n\t        EzPickle.__init__(self)\n\t        self._tatami_size = tatami_size + 0.1\n\t        self._timestep_limit = timestep_limit\n\t        self._init_pos_noise = init_pos_noise\n\t        self._init_vel_noise = init_vel_noise\n\t        self._n_agents = len(agent_names)\n\t        self._mujoco_init = False\n", "        self._num_steps = 0\n\t        self._spec = None\n\t        self.WIN_REWARD = 2000 * reward_shape\n\t        self.LOSE_REWARD = - 2000 / reward_shape\n\t        # Resolve agent scopes\n\t        agent_scopes = [\n\t            \"%s%d\" % (name, i)\n\t            for i, name in enumerate(agent_names)\n\t        ]\n\t        # Consturct scene XML\n", "        scene_xml_path = os.path.join(os.path.dirname(__file__),\n\t                                      \"assets\", \"tatami.xml\")\n\t        agent_xml_paths = [_AGENTS[name] for name in agent_names]\n\t        scene = construct_scene(scene_xml_path, agent_xml_paths,\n\t                                agent_scopes=agent_scopes,\n\t                                tatami_size=tatami_size,\n\t                                **kwargs)\n\t        self.tatami_height = 0.5\n\t        # Init MuJoCo\n\t        if xml_path is None:\n", "            with tempfile.TemporaryDirectory() as tmpdir_name:\n\t                scene_filepath = os.path.join(tmpdir_name, \"scene.xml\")\n\t                scene.write(scene_filepath)\n\t                MujocoEnv.__init__(self, scene_filepath, frame_skip)\n\t        else:\n\t            with open(xml_path, 'w') as fp:\n\t                scene.write(fp.name)\n\t            MujocoEnv.__init__(self, fp.name, frame_skip)\n\t        self._mujoco_init = True\n\t        # Construct agents\n", "        agent_kwargs = agent_kwargs or {}\n\t        self.agents = [\n\t            agents.get(name, env=self, scope=agent_scopes[i], **agent_kwargs)\n\t            for i, name in enumerate(agent_names)\n\t        ]\n\t        # Set opponents\n\t        for i, agent in enumerate(self.agents):\n\t            agent.set_opponents([\n\t                agent for j, agent in enumerate(self.agents) if j != i\n\t            ])\n", "        # Setup agents\n\t        for i, agent in enumerate(self.agents):\n\t            agent.setup_spaces()\n\t        # Set observation and action spaces\n\t        # self.observation_space = Tuple([\n\t        #     agent.observation_space for agent in self.agents\n\t        # ])\n\t        # self.action_space = Tuple([\n\t        #     agent.action_space for agent in self.agents\n\t        # ])\n", "        self.observation_space = self.agents[0].observation_space\n\t        self.action_space = self.agents[0].action_space\n\t        self.num_agents = 2\n\t        self.is_vector_env = True\n\t    def simulate(self, actions):\n\t        a = np.concatenate(actions, axis=0)\n\t        self.do_simulation(a, self.frame_skip)\n\t    def step(self, actions):\n\t        if not self._mujoco_init:\n\t            return self._get_obs(), 0, False, None\n", "        dones = [False for _ in range(self._n_agents)]\n\t        rewards = [0. for _ in range(self._n_agents)]\n\t        infos = [{} for _ in range(self._n_agents)]\n\t        # Call `before_step` on the agents\n\t        for i in range(self._n_agents):\n\t            self.agents[i].before_step()\n\t        # Do simulation\n\t        self.simulate(actions)\n\t        # Call `after_step` on the agents\n\t        for i in range(self._n_agents):\n", "            infos[i]['ctrl_reward'] = self.agents[i].after_step(actions[i])\n\t        # Get obs\n\t        obs = self._get_obs()\n\t        self._num_steps += 1\n\t        # Compute rewards and dones\n\t        for i, agent in enumerate(self.agents):\n\t            self_xyz = agent.get_qpos()[:3]\n\t            # Loose penalty\n\t            infos[i]['lose_penalty'] = 0.\n\t            if (self_xyz[2] < 0.29 + self.tatami_height or\n", "                    np.max(np.abs(self_xyz[:2])) >= self._tatami_size):\n\t                infos[i]['lose_penalty'] += self.LOSE_REWARD\n\t                dones[i] = True\n\t            # Win reward\n\t            infos[i]['win_reward'] = 0.\n\t            for opp in agent._opponents:\n\t                opp_xyz = opp.get_qpos()[:3]\n\t                if (opp_xyz[2] < 0.29 + self.tatami_height or\n\t                        np.max(np.abs(opp_xyz[:2])) >= self._tatami_size):\n\t                    infos[i]['win_reward'] += self.WIN_REWARD\n", "                    infos[i]['winner'] = True\n\t                    dones[i] = True\n\t            infos[i]['main_reward'] = \\\n\t                infos[i]['win_reward'] + infos[i]['lose_penalty']\n\t            # Draw penalty\n\t            if self._num_steps > self._timestep_limit:\n\t                infos[i]['main_reward'] += self.DRAW_PENALTY\n\t                dones[i] = True\n\t            # Move to opponent(s) and push them out of center\n\t            infos[i]['move_to_opp_reward'] = 0.\n", "            infos[i]['push_opp_reward'] = 0.\n\t            for opp in agent._opponents:\n\t                infos[i]['move_to_opp_reward'] += \\\n\t                    self._comp_move_reward(agent, opp.posafter)\n\t                infos[i]['push_opp_reward'] += \\\n\t                    self._comp_push_reward(agent, opp.posafter)\n\t            # Stay in center reward (unused)\n\t            # infos[i]['stay_in_center'] = self._comp_stay_in_center_reward(agent)\n\t            # Contact rewards and penalties (unused)\n\t            infos[i]['contact_reward'] = self._comp_contact_reward(agent)\n", "            # Reward shaping\n\t            infos[i]['shaping_reward'] = \\\n\t                infos[i]['ctrl_reward'] + \\\n\t                infos[i]['push_opp_reward'] + \\\n\t                infos[i]['move_to_opp_reward'] + \\\n\t                infos[i]['contact_reward']\n\t            # Add up rewards\n\t            rewards[i] = infos[i]['main_reward'] + infos[i]['shaping_reward']\n\t        rewards = tuple(rewards) # normlize\n\t        dones = tuple(dones)\n", "        if np.all(dones):\n\t            if 'winner' in infos[0]:\n\t                infos[0]['score'] = 1.\n\t            elif 'winner' in infos[1]:\n\t                infos[0]['score'] = 0.\n\t            else:\n\t                infos[0]['score'] = 0.5\n\t        infos = tuple(infos)\n\t        return np.array(obs).reshape(self.num_agents, -1), np.array(rewards).reshape(self.num_agents, -1), np.array(dones).reshape(self.num_agents, -1), infos\n\t    def _comp_move_reward(self, agent, target):\n", "        move_vec = (agent.posafter - agent.posbefore) / self.dt\n\t        direction = target - agent.posbefore\n\t        direction /= np.linalg.norm(direction)\n\t        return max(np.sum(move_vec * direction), 0.) * self.MOVE_TO_OPP_COEF\n\t    def _comp_push_reward(self, agent, target):\n\t        dist_to_center = np.linalg.norm(target)\n\t        return - self.PUSH_OUT_COEF * np.exp(-dist_to_center)\n\t    def _comp_stay_in_center_reward(self, agent):\n\t        dist_to_center = np.linalg.norm(agent.posafter)\n\t        return self.STAY_IN_CENTER_COEF * np.exp(-dist_to_center)\n", "    def _comp_contact_reward(self, agent):\n\t        # Penalty for pain\n\t        body_ids = [\n\t            agent.body_name_idx[name]\n\t            for name in ['head', 'torso'] if name in agent.body_name_idx\n\t        ]\n\t        forces = np.clip(agent.get_cfrc_ext(body_ids), -100., 100.)\n\t        pain = agent.COST_COEFS['pain'] * np.sum(np.abs(forces))\n\t        # Reward for attacking opponents\n\t        attack = 0.\n", "        for other in agent._opponents:\n\t            body_ids = [\n\t                other.body_name_idx[name]\n\t                for name in ['head', 'torso'] if name in other.body_name_idx\n\t            ]\n\t            forces = np.clip(other.get_cfrc_ext(body_ids), -100., 100.)\n\t            attack += agent.COST_COEFS['attack'] * np.sum(np.abs(forces))\n\t        return attack - pain\n\t    def _get_obs(self):\n\t        if not self._mujoco_init:\n", "            return np.concatenate([self.data.qpos.flat, self.data.qvel.flat])\n\t        return np.array([agent.get_obs() for agent in self.agents])\n\t    def reset_model(self):\n\t        self._num_steps = 0\n\t        # Randomize agent positions\n\t        r, z = 1.15, 1.25\n\t        delta = (2. * np.pi) / self._n_agents\n\t        phi = self.np_random.uniform(0., 2. * np.pi)\n\t        for i, agent in enumerate(self.agents):\n\t            angle = phi + i * delta\n", "            x, y = r * np.cos(angle), r * np.sin(angle)\n\t            agent.set_xyz((x, y, z))\n\t        # Add noise to all qpos and qvel elements\n\t        pos_noise = self.np_random.uniform(\n\t            size=self.model.nq,\n\t            low=-self._init_pos_noise,\n\t            high=self._init_pos_noise)\n\t        vel_noise = self._init_vel_noise * \\\n\t                    self.np_random.randn(self.model.nv)\n\t        qpos = self.data.qpos.ravel() + pos_noise\n", "        qvel = self.data.qvel.ravel() + vel_noise\n\t        self.init_qpos, self.init_qvel = qpos, qvel\n\t        self.set_state(qpos, qvel)\n\t        return self._get_obs()\n\t    def viewer_setup(self):\n\t        if self.viewer is not None:\n\t            self.viewer._run_speed = 0.5\n\t            self.viewer.cam.trackbodyid = 0\n\t            # self.viewer.cam.lookat[2] += .8\n\t            self.viewer.cam.elevation = -25\n", "            self.viewer.cam.type = 1\n\t            self.sim.forward()\n\t            self.viewer.cam.distance = self.model.stat.extent * 1.0\n\t        # Make sure that the offscreen context has the same camera setup\n\t        if self.sim._render_context_offscreen is not None:\n\t            self.sim._render_context_offscreen.cam.trackbodyid = 0\n\t            # self.sim._render_context_offscreen.cam.lookat[2] += .8\n\t            self.sim._render_context_offscreen.cam.elevation = -25\n\t            self.sim._render_context_offscreen.cam.type = 1\n\t            self.sim._render_context_offscreen.cam.distance = \\\n", "                self.model.stat.extent * 1.0\n\t        self.buffer_size = (1280, 800)\n\t    def render(self, mode='human'):\n\t        return super(SumoEnv, self).render(mode=mode) # just raise an exception"]}
{"filename": "envs/robosumo/envs/__init__.py", "chunked_list": ["from .mujoco_env import MujocoEnv\n\tfrom .sumo import SumoEnv\n"]}
{"filename": "envs/robosumo/envs/utils.py", "chunked_list": ["import colorsys\n\timport numpy as np\n\timport os\n\timport xml.etree.ElementTree as ET\n\tdef cart2pol(vec):\n\t    \"\"\"Convert a cartesian 2D vector to polar coordinates.\"\"\"\n\t    r = np.sqrt(vec[0]**2 + vec[1]**2)\n\t    theta = np.arctan2(vec[1], vec[0])\n\t    return np.asarray([r, theta])\n\tdef get_distinct_colors(n=2):\n", "    \"\"\"Source: https://stackoverflow.com/a/876872.\"\"\"\n\t    HSV_tuples = [(x * 1. / n, .5, .5) for x in range(n)]\n\t    RGB_tuples = map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)\n\t    return RGB_tuples\n\tdef _set_class(root, prop, name):\n\t    if root is None:\n\t        return\n\t    if root.tag == prop:\n\t        root.set('class', name)\n\t    for child in list(root):\n", "        _set_class(child, prop, name)\n\tdef _add_prefix(root, prop, prefix, force_set=False):\n\t    if root is None:\n\t        return\n\t    root_prop_val = root.get(prop)\n\t    if root_prop_val is not None:\n\t        root.set(prop, prefix + '/' + root_prop_val)\n\t    elif force_set:\n\t        root.set(prop, prefix + '/' + 'anon' + str(np.random.randint(1, 1e10)))\n\t    for child in list(root):\n", "        _add_prefix(child, prop, prefix, force_set)\n\tdef _tuple_to_str(tp):\n\t    return \" \".join(map(str, tp))\n\tdef construct_scene(scene_xml_path, agent_xml_paths,\n\t                    agent_densities=None,\n\t                    agent_scopes=None,\n\t                    init_poses=None,\n\t                    rgb=None,\n\t                    tatami_size=None):\n\t    \"\"\"Construct an XML that represents a MuJoCo scene for sumo.\"\"\"\n", "    n_agents = len(agent_xml_paths)\n\t    assert n_agents == 2, \"Only 2-agent sumo is currently supported.\"\n\t    scene = ET.parse(scene_xml_path)\n\t    scene_root = scene.getroot()\n\t    scene_default = scene_root.find('default')\n\t    scene_body = scene_root.find('worldbody')\n\t    scene_actuator = None\n\t    scene_sensors = None\n\t    # Set tatami size if specified\n\t    if tatami_size is not None:\n", "        for geom in scene_body.findall('geom'):\n\t            if geom.get('name') == 'tatami':\n\t                size = tatami_size + 0.3\n\t                geom.set('size', \"{size:.2f} {size:.2f} 0.25\".format(size=size))\n\t            if geom.get('name') == 'topborder':\n\t                fromto = \\\n\t                    \"-{size:.2f} {size:.2f} 0.5  {size:.2f} {size:.2f} 0.5\" \\\n\t                    .format(size=tatami_size)\n\t                geom.set('fromto', fromto)\n\t            if geom.get('name') == 'rightborder':\n", "                fromto = \\\n\t                    \"{size:.2f} -{size:.2f} 0.5  {size:.2f} {size:.2f} 0.5\" \\\n\t                    .format(size=tatami_size)\n\t                geom.set('fromto', fromto)\n\t            if geom.get('name') == 'bottomborder':\n\t                fromto = \\\n\t                    \"-{size:.2f} -{size:.2f} 0.5  {size:.2f} -{size:.2f} 0.5\" \\\n\t                    .format(size=tatami_size)\n\t                geom.set('fromto', fromto)\n\t            if geom.get('name') == 'leftborder':\n", "                fromto = \\\n\t                    \"-{size:.2f} -{size:.2f} 0.5  -{size:.2f} {size:.2f} 0.5\" \\\n\t                    .format(size=tatami_size)\n\t                geom.set('fromto', fromto)\n\t    # Resolve colors\n\t    if rgb is None:\n\t        rgb = get_distinct_colors(n_agents)\n\t    else:\n\t        assert len(rgb) == n_agents, \"Each agent must have a color.\"\n\t    RGBA_tuples = list(map(lambda x: _tuple_to_str(x + (1,)), rgb))\n", "    # Resolve densities\n\t    if agent_densities is None:\n\t        agent_densities = [10.0] * n_agents\n\t    # Resolve scopes\n\t    if agent_scopes is None:\n\t        agent_scopes = ['agent' + str(i) for i in range(n_agents)]\n\t    else:\n\t        assert len(agent_scopes) == n_agents, \"Each agent must have a scope.\"\n\t    # Resolve initial positions\n\t    if init_poses is None:\n", "        r, phi, z = 1.5, 0., .75\n\t        delta = (2. * np.pi) / n_agents\n\t        init_poses = []\n\t        for i in range(n_agents):\n\t            angle = phi + i * delta\n\t            x, y = r * np.cos(angle), r * np.sin(angle)\n\t            init_poses.append((x, y, z))\n\t    # Build agent XMLs\n\t    for i in range(n_agents):\n\t        agent_xml = ET.parse(agent_xml_paths[i])\n", "        agent_default = ET.SubElement(\n\t            scene_default, 'default',\n\t            attrib={'class': agent_scopes[i]}\n\t        )\n\t        # Set defaults\n\t        rgba = RGBA_tuples[i]\n\t        density = str(agent_densities[i])\n\t        default_set = False\n\t        for child in list(agent_xml.find('default')):\n\t            if child.tag == 'geom':\n", "                child.set('rgba', rgba)\n\t                child.set('density', density)\n\t                default_set = True\n\t            agent_default.append(child)\n\t        if not default_set:\n\t            agent_geom = ET.SubElement(\n\t                agent_default, 'geom',\n\t                attrib={\n\t                    'density': density,\n\t                    'contype': '1',\n", "                    'conaffinity': '1',\n\t                    'rgba': rgba,\n\t                }\n\t            )\n\t        # Build agent body\n\t        agent_body = agent_xml.find('body')\n\t        # set initial position\n\t        agent_body.set('pos', _tuple_to_str(init_poses[i]))\n\t        # add class to all geoms\n\t        _set_class(agent_body, 'geom', agent_scopes[i])\n", "        # add prefix to all names, important to map joints\n\t        _add_prefix(agent_body, 'name', agent_scopes[i], force_set=True)\n\t        # add aggent body to xml\n\t        scene_body.append(agent_body)\n\t        # Build agent actuators\n\t        agent_actuator = agent_xml.find('actuator')\n\t        # add class and prefix to all motor joints\n\t        _add_prefix(agent_actuator, 'joint', agent_scopes[i])\n\t        _add_prefix(agent_actuator, 'name', agent_scopes[i])\n\t        _set_class(agent_actuator, 'motor', agent_scopes[i])\n", "        # add actuator\n\t        if scene_actuator is None:\n\t            scene_root.append(agent_actuator)\n\t            scene_actuator = scene_root.find('actuator')\n\t        else:\n\t            for motor in list(agent_actuator):\n\t                scene_actuator.append(motor)\n\t        # Build agent sensors\n\t        agent_sensors = agent_xml.find('sensor')\n\t        # add same prefix to all sensors\n", "        _add_prefix(agent_sensors, 'joint', agent_scopes[i])\n\t        _add_prefix(agent_sensors, 'name', agent_scopes[i])\n\t        if scene_sensors is None:\n\t            scene_root.append(agent_sensors)\n\t            scene_sensors = scene_root.find('sensor')\n\t        else:\n\t            for sensor in list(agent_sensors):\n\t                scene_sensors.append(sensor)\n\t    return scene\n"]}
{"filename": "envs/robosumo/envs/mujoco_env.py", "chunked_list": ["\"\"\"\n\tThe base class for environments based on MuJoCo 1.5.\n\t\"\"\"\n\timport os\n\timport sys\n\timport numpy as np\n\timport gym\n\tfrom gym import error, spaces\n\tfrom gym.utils import seeding\n\ttry:\n", "    import mujoco_py\n\t    from mujoco_py import load_model_from_path, MjSim, MjViewer\n\t    import glfw\n\texcept ImportError as e:\n\t    raise error.DependencyNotInstalled(\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\".format(e))\n\tfrom pkg_resources import parse_version\n\tif parse_version(mujoco_py.__version__) < parse_version('1.5'):\n\t    raise error.DependencyNotInstalled(\n\t        \"RoboSumo requires mujoco_py of version 1.5 or higher. \"\n\t        \"The installed version is {}. Please upgrade mujoco_py.\"\n", "        .format(mujoco_py.__version__))\n\tdef _read_pixels(sim, width=None, height=None, camera_name=None):\n\t    \"\"\"Reads pixels w/o markers and overlay from the same camera as screen.\"\"\"\n\t    if width is None or height is None:\n\t        resolution = glfw.get_framebuffer_size(\n\t            sim._render_context_window.window)\n\t        resolution = np.array(resolution)\n\t        resolution = resolution * min(1000 / np.min(resolution), 1)\n\t        resolution = resolution.astype(np.int32)\n\t        resolution -= resolution % 16\n", "        width, height = resolution\n\t    img = sim.render(width, height, camera_name=camera_name)\n\t    img = img[::-1, :, :] # Rendered images are upside-down.\n\t    return img\n\tclass MujocoEnv(gym.Env):\n\t    \"\"\"Superclass for all MuJoCo environments.\n\t    \"\"\"\n\t    def __init__(self, model_path, frame_skip):\n\t        if model_path.startswith(\"/\"):\n\t            fullpath = model_path\n", "        else:\n\t            fullpath = os.path.join(os.path.dirname(__file__), \"assets\", model_path)\n\t        if not os.path.exists(fullpath):\n\t            raise IOError(\"File %s does not exist\" % fullpath)\n\t        self.frame_skip = frame_skip\n\t        self.model = load_model_from_path(fullpath)\n\t        self.sim = MjSim(self.model)\n\t        self.data = self.sim.data\n\t        self.viewer = None\n\t        self.buffer_size = (1600, 1280)\n", "        self.metadata = {\n\t            'render.modes': ['human', 'rgb_array'],\n\t            'video.frames_per_second': 60,\n\t        }\n\t        self.init_qpos = self.data.qpos.ravel().copy()\n\t        self.init_qvel = self.data.qvel.ravel().copy()\n\t        observation, _reward, done, _info = self.step(np.zeros(self.model.nu))\n\t        assert not done\n\t        self.obs_dim = np.sum([o.size for o in observation]) if (\n\t            type(observation) is tuple) else observation.size\n", "        bounds = self.model.actuator_ctrlrange.copy().astype(np.float32)\n\t        low, high = bounds[:, 0], bounds[:, 1]\n\t        self.action_space = spaces.Box(low, high)\n\t        high = np.inf * np.ones(self.obs_dim, dtype=np.float32)\n\t        self.observation_space = spaces.Box(-high, high)\n\t        self._seed()\n\t    def _seed(self, seed=None):\n\t        self.np_random, seed = seeding.np_random(seed)\n\t        return [seed]\n\t    # methods to override:\n", "    # ------------------------------------------------------------------------\n\t    def reset_model(self):\n\t        \"\"\"Reset the robot degrees of freedom (qpos and qvel).\n\t        Implement this in each subclass.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    def viewer_setup(self):\n\t        \"\"\"Called when the viewer is initialized and after every reset.\n\t        Optionally implement this method, if you need to tinker with camera\n\t        position and so forth.\n", "        \"\"\"\n\t        pass\n\t    # ------------------------------------------------------------------------\n\t    def reset(self):\n\t        self.sim.reset()\n\t        self.sim.forward()\n\t        ob = self.reset_model()\n\t        if ob is None: # zihan: added, fix None observation at reset()\n\t            ob = tuple([np.zeros(self.obs_dim) for _ in self.agents])\n\t        return ob\n", "    def set_state(self, qpos, qvel):\n\t        assert qpos.shape == (self.model.nq,)\n\t        assert qvel.shape == (self.model.nv,)\n\t        state = self.sim.get_state()\n\t        for i in range(self.model.nq):\n\t            state.qpos[i] = qpos[i]\n\t        for i in range(self.model.nv):\n\t            state.qvel[i] = qvel[i]\n\t        self.sim.set_state(state)\n\t        self.sim.forward()\n", "    @property\n\t    def dt(self):\n\t        return self.model.opt.timestep * self.frame_skip\n\t    def do_simulation(self, ctrl, n_frames):\n\t        for i in range(self.model.nu):\n\t            self.sim.data.ctrl[i] = ctrl[i]\n\t        for _ in range(n_frames):\n\t            self.sim.step()\n\t    def render(self, mode='human', close=False):\n\t        if close:\n", "            if self.viewer is not None:\n\t                self.viewer = None\n\t            return\n\t        if mode == 'rgb_array':\n\t            self.viewer_setup()\n\t            return _read_pixels(self.sim, *self.buffer_size)\n\t        elif mode == 'human':\n\t            return self._get_viewer().render()\n\t    def _get_viewer(self, mode='human'):\n\t        if self.viewer is None and mode == 'human':\n", "            self.viewer = MjViewer(self.sim)\n\t            self.viewer_setup()\n\t        return self.viewer\n\t    def state_vector(self):\n\t        state = self.sim.get_state()\n\t        return np.concatenate([state.qpos.flat, state.qvel.flat])\n"]}
{"filename": "envs/robosumo/envs/agents.py", "chunked_list": ["import os\n\timport numpy as np\n\timport xml.etree.ElementTree as ET\n\timport gym\n\tclass Agent(object):\n\t    \"\"\"\n\t    Superclass for all agents in sumo MuJoCo environment.\n\t    \"\"\"\n\t    CFRC_CLIP = 100.\n\t    COST_COEFS = {\n", "        'ctrl': 1e-1,\n\t        'pain': 1e-4,\n\t        'attack': 1e-1,\n\t    }\n\t    JNT_NPOS = {\n\t        0: 7,\n\t        1: 4,\n\t        2: 1,\n\t        3: 1,\n\t    }\n", "    def __init__(self, env, scope, xml_path, adjust_z=0.):\n\t        self._env = env\n\t        self._scope = scope\n\t        self._xml_path = xml_path\n\t        self._xml = ET.parse(xml_path)\n\t        self._adjust_z = adjust_z\n\t        self._set_body()\n\t        self._set_joint()\n\t    def setup_spaces(self):\n\t        self._set_observation_space()\n", "        self._set_action_space()\n\t    def _in_scope(self, name):\n\t        return name.startswith(self._scope)\n\t    def _set_body(self):\n\t        self.body_names = list(filter(\n\t            lambda x: self._in_scope(x), self._env.model.body_names\n\t        ))\n\t        self.body_ids = [\n\t            self._env.model.body_names.index(name) for name in self.body_names\n\t        ]\n", "        self.body_name_idx = {\n\t            name.split('/')[-1]: idx\n\t            for name, idx in zip(self.body_names, self.body_ids)\n\t        }\n\t        # Determine body params\n\t        self.body_dofnum = self._env.model.body_dofnum[self.body_ids]\n\t        self.body_dofadr = self._env.model.body_dofadr[self.body_ids]\n\t        self.nv = self.body_dofnum.sum()\n\t        # Determine qvel_start_idx and qvel_end_idx\n\t        dof = list(filter(lambda x: x >= 0, self.body_dofadr))\n", "        self.qvel_start_idx = int(dof[0])\n\t        last_dof_body_id = self.body_dofnum.shape[0] - 1\n\t        while self.body_dofnum[last_dof_body_id] == 0:\n\t            last_dof_body_id -= 1\n\t        self.qvel_end_idx = int(dof[-1] + self.body_dofnum[last_dof_body_id])\n\t    def _set_joint(self):\n\t        self.joint_names = list(filter(\n\t            lambda x: self._in_scope(x), self._env.model.joint_names\n\t        ))\n\t        self.joint_ids = [\n", "            self._env.model.joint_names.index(name) for name in self.joint_names\n\t        ]\n\t        # Determine joint params\n\t        self.jnt_qposadr = self._env.model.jnt_qposadr[self.joint_ids]\n\t        self.jnt_type = self._env.model.jnt_type[self.joint_ids]\n\t        self.jnt_nqpos = [self.JNT_NPOS[int(j)] for j in self.jnt_type]\n\t        self.nq = sum(self.jnt_nqpos)\n\t        # Determine qpos_start_idx and qpos_end_idx\n\t        self.qpos_start_idx = int(self.jnt_qposadr[0])\n\t        self.qpos_end_idx = int(self.jnt_qposadr[-1] + self.jnt_nqpos[-1])\n", "    def _set_observation_space(self):\n\t        obs = self.get_obs()\n\t        self.obs_dim = obs.size\n\t        low = -np.inf * np.ones(self.obs_dim, dtype=np.float32)\n\t        high = np.inf * np.ones(self.obs_dim, dtype=np.float32)\n\t        self.observation_space = gym.spaces.Box(low, high)\n\t    def _set_action_space(self):\n\t        acts = self._xml.find('actuator')\n\t        self.action_dim = len(list(acts))\n\t        default = self._xml.find('default')\n", "        range_set = False\n\t        if default is not None:\n\t            motor = default.find('motor')\n\t            if motor is not None:\n\t                ctrl = motor.get('ctrlrange')\n\t                if ctrl:\n\t                    clow, chigh = list(map(float, ctrl.split()))\n\t                    high = chigh * np.ones(self.action_dim, np.float32)\n\t                    low = clow * np.ones(self.action_dim, np.float32)\n\t                    range_set = True\n", "        if not range_set:\n\t            high =  np.ones(self.action_dim, dtype=np.float32)\n\t            low = - np.ones(self.action_dim, dtype=np.float32)\n\t        for i, motor in enumerate(list(acts)):\n\t            ctrl = motor.get('ctrlrange')\n\t            if ctrl:\n\t                clow, chigh = list(map(float, ctrl.split()))\n\t                low[i], high[i] = clow, chigh\n\t        self._low, self._high = low, high\n\t        self.action_space = gym.spaces.Box(low, high)\n", "    def set_xyz(self, xyz):\n\t        \"\"\"Set (x, y, z) position of the agent; any element can be None.\"\"\"\n\t        qpos = self._env.data.qpos.ravel().copy()\n\t        start = self.qpos_start_idx\n\t        if xyz[0]: qpos[start]     = xyz[0]\n\t        if xyz[1]: qpos[start + 1] = xyz[1]\n\t        if xyz[2]: qpos[start + 2] = xyz[2]\n\t        qvel = self._env.data.qvel.ravel()\n\t        self._env.set_state(qpos, qvel)\n\t    def set_euler(self, euler):\n", "        \"\"\"Set euler angles the agent; any element can be None.\"\"\"\n\t        qpos = self._env.data.qpos.ravel().copy()\n\t        start = self.qpos_start_idx\n\t        if euler[0]: qpos[start + 4] = euler[0]\n\t        if euler[1]: qpos[start + 5] = euler[1]\n\t        if euler[2]: qpos[start + 6] = euler[2]\n\t        qvel = self._env.data.qvel.ravel()\n\t        self._env.set_state(qpos, qvel)\n\t    def set_opponents(self, opponents):\n\t        self._opponents = opponents\n", "    def reset(self):\n\t        pass\n\t    # --------------------------------------------------------------------------\n\t    # Various getters\n\t    # --------------------------------------------------------------------------\n\t    def get_body_com(self, body_name):\n\t        idx = self.body_names.index(self._scope + '/' + body_name)\n\t        return self._env.data.subtree_com[self.body_ids[idx]]\n\t    def get_cfrc_ext(self, body_ids=None):\n\t        if body_ids is None:\n", "            body_ids = self.body_ids\n\t        return self._env.data.cfrc_ext[body_ids]\n\t    def get_qpos(self):\n\t        \"\"\"Note: relies on the qpos for one agent being contiguously located.\n\t        \"\"\"\n\t        qpos = self._env.data.qpos[self.qpos_start_idx:self.qpos_end_idx].copy()\n\t        qpos[2] += self._adjust_z\n\t        return qpos\n\t    def get_qvel(self):\n\t        \"\"\"Note: relies on the qvel for one agent being contiguously located.\n", "        \"\"\"\n\t        qvel = self._env.data.qvel[self.qvel_start_idx:self.qvel_end_idx]\n\t        return qvel\n\t    def get_qfrc_actuator(self):\n\t        start, end = self.qvel_start_idx, self.qvel_end_idx\n\t        qfrc = self._env.data.qfrc_actuator[start:end]\n\t        return qfrc\n\t    def get_cvel(self):\n\t        cvel = self._env.data.cvel[self.body_ids]\n\t        return cvel\n", "    def get_body_mass(self):\n\t        body_mass = self._env.model.body_mass[self.body_ids]\n\t        return body_mass\n\t    def get_xipos(self):\n\t        xipos = self._env.data.xipos[self.body_ids]\n\t        return xipos\n\t    def get_cinert(self):\n\t        cinert = self._env.data.cinert[self.body_ids]\n\t        return cinert\n\t    def get_obs(self):\n", "        # Observe self\n\t        self_forces = np.abs(np.clip(\n\t            self.get_cfrc_ext(), -self.CFRC_CLIP, self.CFRC_CLIP))\n\t        obs  = [\n\t            self.get_qpos().flat,           # self all positions\n\t            self.get_qvel().flat,           # self all velocities\n\t            self_forces.flat,               # self all forces\n\t        ]\n\t        # Observe opponents\n\t        for opp in self._opponents:\n", "            body_ids = [\n\t                opp.body_name_idx[name]\n\t                for name in ['torso']\n\t                if name in opp.body_name_idx\n\t            ]\n\t            opp_forces = np.abs(np.clip(\n\t                opp.get_cfrc_ext(body_ids), -self.CFRC_CLIP, self.CFRC_CLIP))\n\t            obs.extend([\n\t                opp.get_qpos()[:7].flat,    # opponent torso position\n\t                opp_forces.flat,            # opponent torso forces\n", "            ])\n\t        return np.concatenate(obs)\n\t    def before_step(self):\n\t        self.posbefore = self.get_qpos()[:2].copy()\n\t    def after_step(self, action):\n\t        self.posafter = self.get_qpos()[:2].copy()\n\t        # Control cost\n\t        reward = - self.COST_COEFS['ctrl'] * np.square(action).sum()\n\t        return reward\n\t# ------------------------------------------------------------------------------\n", "# Beasts\n\t# ------------------------------------------------------------------------------\n\tclass Ant(Agent):\n\t    \"\"\"\n\t    The 4-leg agent.\n\t    \"\"\"\n\t    def __init__(self, env, scope=\"ant\", **kwargs):\n\t        xml_path = os.path.join(os.path.dirname(__file__),\n\t                                \"assets\", \"ant.xml\")\n\t        super(Ant, self).__init__(env, scope, xml_path, **kwargs)\n", "class Bug(Agent):\n\t    \"\"\"\n\t    The 6-leg agent.\n\t    \"\"\"\n\t    def __init__(self, env, scope=\"bug\", **kwargs):\n\t        xml_path = os.path.join(os.path.dirname(__file__),\n\t                                \"assets\", \"bug.xml\")\n\t        super(Bug, self).__init__(env, scope, xml_path, **kwargs)\n\tclass Spider(Agent):\n\t    \"\"\"\n", "    The 8-leg agent.\n\t    \"\"\"\n\t    def __init__(self, env, scope=\"spider\", **kwargs):\n\t        xml_path = os.path.join(os.path.dirname(__file__),\n\t                                \"assets\", \"spider.xml\")\n\t        super(Spider, self).__init__(env, scope, xml_path, **kwargs)\n\t# ------------------------------------------------------------------------------\n\t_available_agents = {\n\t    'ant': Ant,\n\t    'bug': Bug,\n", "    'spider': Spider,\n\t}\n\tdef get(name, *args, **kwargs):\n\t    if name not in _available_agents:\n\t        raise ValueError(\"Class %s is not available.\" % name)\n\t    return _available_agents[name](*args, **kwargs)\n"]}
{"filename": "toyexample/rppo.py", "chunked_list": ["import argparse\n\timport os\n\timport random\n\timport time\n\tfrom distutils.util import strtobool\n\timport sys\n\tsys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n\timport gym\n\timport numpy as np\n\timport torch\n", "import torch.nn as nn\n\timport torch.optim as optim\n\tfrom torch.distributions.categorical import Categorical\n\timport envs\n\tdef parse_args():\n\t    # fmt: off\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n\t        help=\"the name of this experiment\")\n\t    parser.add_argument(\"--seed\", type=int, default=1,\n", "        help=\"seed of the experiment\")\n\t    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n\t        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n\t    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n\t        help=\"if toggled, cuda will be enabled by default\")\n\t    # Algorithm specific arguments\n\t    parser.add_argument(\"--env-id\", type=str, default=\"ToyEnv-v0\",\n\t        help=\"the id of the environment\")\n\t    parser.add_argument(\"--total-timesteps\", type=int, default=1e6,\n\t        help=\"total timesteps of the experiments\")\n", "    parser.add_argument(\"--learning-rate\", type=float, default=1e-4,\n\t        help=\"the learning rate of the optimizer\")\n\t    parser.add_argument(\"--num-envs\", type=int, default=1,\n\t        help=\"the number of parallel game environments\")\n\t    parser.add_argument(\"--num-steps\", type=int, default=200,\n\t        help=\"the number of steps to run in each environment per policy rollout\")\n\t    parser.add_argument(\"--gamma\", type=float, default=0.95,\n\t        help=\"the discount factor gamma\")\n\t    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n\t        help=\"the number of mini-batches\")\n", "    parser.add_argument(\"--update-epochs\", type=int, default=4,\n\t        help=\"the K epochs to update the policy\")\n\t    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=False,\n\t        help=\"Toggles advantages normalization\")\n\t    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n\t        help=\"the surrogate clipping coefficient\")\n\t    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n\t        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n\t    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n\t        help=\"coefficient of the entropy\")\n", "    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n\t        help=\"coefficient of the value function\")\n\t    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n\t        help=\"the maximum norm for the gradient clipping\")\n\t    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n\t        help=\"the lambda for the general advantage estimation\")\n\t    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n\t        help=\"Use GAE for advantage computation\")\n\t    parser.add_argument(\"--risk\", type=lambda x: bool(strtobool(x)), default=True,\n\t        help=\"the maximum norm for the gradient clipping\")\n", "    parser.add_argument(\"--tau\", type=float, default=0.5)\n\t    parser.add_argument(\"--onehot-state\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True)\n\t    args = parser.parse_args()\n\t    args.batch_size = int(args.num_steps)\n\t    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n\t    return args\n\tdef make_env(args):\n\t    env = gym.make(args.env_id)\n\t    env.seed(args.seed)\n\t    env.action_space.seed(args.seed)\n", "    env.observation_space.seed(args.seed)\n\t    return env\n\tdef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n\t    torch.nn.init.orthogonal_(layer.weight, std)\n\t    torch.nn.init.constant_(layer.bias, bias_const)\n\t    return layer\n\tclass Agent(nn.Module):\n\t    def __init__(self, obs_space, act_space):\n\t        super().__init__()\n\t        obs_dim = int(np.prod(obs_space.shape))\n", "        act_dim = act_space.n\n\t        hidden_dim = 128\n\t        self.critic = nn.Sequential(\n\t            layer_init(nn.Linear(obs_dim, hidden_dim)),\n\t            nn.ReLU(),\n\t            layer_init(nn.Linear(hidden_dim, 1), std=0.01),\n\t        )\n\t        self.actor = nn.Sequential(\n\t            layer_init(nn.Linear(obs_dim, hidden_dim)),\n\t            nn.ReLU(),\n", "            layer_init(nn.Linear(hidden_dim, act_dim), std=0.01),\n\t        )\n\t    def get_value(self, x):\n\t        if x.ndim == 1:\n\t            x = x.view((-1, 1))\n\t        return self.critic(x)\n\t    def get_action(self, x):\n\t        if x.ndim == 1:\n\t            x = x.view((-1, 1))\n\t        logits = self.actor(x)\n", "        dist = Categorical(logits=logits)\n\t        action = dist.probs.argmax(dim=-1)\n\t        return action\n\t    def get_action_and_value(self, x, action=None):\n\t        if x.ndim == 1:\n\t            x = x.view((-1, 1))\n\t        logits = self.actor(x)\n\t        probs = Categorical(logits=logits)\n\t        if action is None:\n\t            action = probs.sample()\n", "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n\t    def save(self, run_dir):\n\t        torch.save(self.actor.state_dict(), run_dir + \"/actor.pt\")\n\t        torch.save(self.critic.state_dict(), run_dir + \"/critic.pt\")\n\t    def restore(self, run_dir):\n\t        actor_state_dict = torch.load(run_dir + '/actor.pt')\n\t        # critic_state_dict = torch.load(run_dir + '/critic.pt')\n\t        # actor_state_dict = torch.load(run_dir)\n\t        self.actor.load_state_dict(actor_state_dict)\n\t        # self.critic.load_state_dict(critic_state_dict)\n", "if __name__ == \"__main__\":\n\t    args = parse_args()\n\t    run_dir = f\"runs/{args.env_id}/{args.exp_name}/{args.tau}/\" + time.strftime(\"%b%d-%H%M%S\", time.localtime())\n\t    if not os.path.exists(run_dir):\n\t        os.makedirs(run_dir)\n\t    args.run_dir = run_dir\n\t    random.seed(args.seed)\n\t    np.random.seed(args.seed)\n\t    torch.manual_seed(args.seed)\n\t    torch.backends.cudnn.deterministic = args.torch_deterministic\n", "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n\t    # env setup\n\t    envs = make_env(args)\n\t    assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n\t    agent = Agent(envs.observation_space, envs.action_space).to(device)\n\t    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n\t    # ALGO Logic: Storage setup\n\t    obs = torch.zeros((args.num_steps, 1) + envs.observation_space.shape).to(device)\n\t    actions = torch.zeros((args.num_steps, 1) + envs.action_space.shape).to(device)\n\t    logprobs = torch.zeros((args.num_steps, 1)).to(device)\n", "    rewards = torch.zeros((args.num_steps, 1)).to(device)\n\t    dones = torch.zeros((args.num_steps+1, 1)).to(device)\n\t    values = torch.zeros((args.num_steps+1, 1)).to(device)\n\t    # TRY NOT TO MODIFY: start the game\n\t    global_step = 0\n\t    start_time = time.time()\n\t    next_obs = torch.Tensor(envs.reset()).to(device).unsqueeze(0)\n\t    next_done = False\n\t    num_updates = int(args.total_timesteps // args.batch_size)\n\t    value_loss = []\n", "    for update in range(1, num_updates + 1):\n\t        for step in range(0, args.num_steps):\n\t            global_step += 1\n\t            obs[step] = next_obs\n\t            dones[step] = next_done\n\t            # ALGO LOGIC: action logic\n\t            with torch.no_grad():\n\t                action, logprob, _, value = agent.get_action_and_value(next_obs)\n\t                values[step] = value.flatten()\n\t            actions[step] = action\n", "            logprobs[step] = logprob\n\t            # TRY NOT TO MODIFY: execute the game and log data.\n\t            next_obs, reward, done, info = envs.step(action.cpu().numpy().item())\n\t            rewards[step] = torch.tensor(reward).to(device).view(-1)\n\t            if done:\n\t                next_obs = envs.reset()\n\t            next_obs = torch.Tensor(next_obs).to(device).unsqueeze(0)\n\t            next_done = torch.tensor(done).to(device).view(-1)\n\t        # bootstrap value if not done\n\t        with torch.no_grad():\n", "            next_value = agent.get_value(next_obs).view(-1)\n\t            values[args.num_steps] = next_value\n\t            dones[args.num_steps] = next_done\n\t            max_depth = 25 # in toy example, max episode length is 25.\n\t            advantages = torch.zeros_like(rewards).to(device)\n\t            returns = torch.zeros_like(rewards).to(device)\n\t            depths = torch.zeros(args.num_steps+1,)\n\t            masks = 1- dones\n\t            for i in reversed(range(args.num_steps)):\n\t                depths[i] = min(depths[i+1]+1, max_depth)\n", "                if masks[i+1]==0:\n\t                    depths[i] = 1\n\t            depths = depths.cpu().numpy().astype(np.int32)\n\t            values_table = torch.zeros(max_depth, args.num_steps+1).to(device)\n\t            values_table[0, args.num_steps] = next_value\n\t            def operator(reward, value, nextvalue, mask):\n\t                delta = reward + args.gamma * nextvalue * mask - value\n\t                delta = args.tau * torch.maximum(delta, torch.zeros_like(delta)) + \\\n\t                        (1-args.tau) * torch.minimum(delta, torch.zeros_like(delta))\n\t                alpha = 1 / (2 * max(args.tau, (1-args.tau)))\n", "                delta = 2 * alpha * delta\n\t                return value + delta\n\t            if args.risk:\n\t                if args.gae:\n\t                    for step in reversed(range(args.num_steps)):\n\t                        values_table[0, step] = operator(rewards[step], values[step], values[step+1], masks[step+1])\n\t                        for d in range(depths[step]):\n\t                            values_table[d][step] = operator(rewards[step], values_table[d-1][step], values_table[d-1][step+1], masks[step+1])\n\t                        for d in reversed(range(depths[step])):\n\t                            returns[step] = values_table[d][step] + args.gae_lambda * returns[step]\n", "                        returns[step] *= (1-args.gae_lambda) / (1-args.gae_lambda**(depths[step]))\n\t                else:\n\t                    for t in reversed(range(args.num_steps)):\n\t                        returns[t] = operator(rewards[t], values[t], values[t+1], masks[t+1])\n\t                advantages = returns - values[:-1,...]\n\t            else:\n\t                if args.gae:\n\t                    gae = 0\n\t                    for t in reversed(range(args.num_steps)):\n\t                        delta = rewards[t] + args.gamma * values[t+1] * masks[t+1] - values[t]\n", "                        advantages[t] = gae = delta + args.gamma * args.gae_lambda * masks[t+1] * gae\n\t                else:\n\t                    raise NotImplementedError\n\t                returns = advantages + values[:-1, ...]\n\t        # flatten the batch\n\t        b_obs = obs.reshape((-1,) + envs.observation_space.shape)\n\t        b_logprobs = logprobs.reshape(-1)\n\t        b_actions = actions.reshape((-1,) + envs.action_space.shape)\n\t        b_advantages = advantages.reshape(-1)\n\t        b_returns = returns.reshape(-1)\n", "        b_values = values[:-1, ...].reshape(-1)\n\t        # Optimizing the policy and value network\n\t        b_inds = np.arange(args.batch_size)\n\t        clipfracs = []\n\t        for epoch in range(args.update_epochs):\n\t            np.random.shuffle(b_inds)\n\t            for start in range(0, args.batch_size, args.minibatch_size):\n\t                end = start + args.minibatch_size\n\t                mb_inds = b_inds[start:end]\n\t                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n", "                logratio = newlogprob - b_logprobs[mb_inds]\n\t                ratio = logratio.exp()\n\t                mb_advantages = b_advantages[mb_inds]\n\t                if args.norm_adv:\n\t                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n\t                # Policy loss\n\t                pg_loss1 = -mb_advantages * ratio\n\t                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n\t                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n\t                # pg_loss = pg_loss1.mean()\n", "                # Value loss\n\t                newvalue = newvalue.view(-1)\n\t                if args.clip_vloss:\n\t                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n\t                    v_clipped = b_values[mb_inds] + torch.clamp(\n\t                        newvalue - b_values[mb_inds],\n\t                        -args.clip_coef,\n\t                        args.clip_coef,\n\t                    )\n\t                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n", "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n\t                    v_loss = 0.5 * v_loss_max.mean()\n\t                else:\n\t                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n\t                entropy_loss = entropy.mean()\n\t                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n\t                optimizer.zero_grad()\n\t                loss.backward()\n\t                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n\t                optimizer.step()\n", "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n\t        var_y = np.var(y_true)\n\t        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n\t        value_loss.append(v_loss)\n\t        if update % 1 == 0:\n\t            print(f\"Updates: {update}/{num_updates}, Returns:{(rewards.sum()/dones.sum()).item()}\")\n\t        agent.save(run_dir)\n\t    envs.close()\n\t    from toyexample.eval import eval\n\t    print(eval(args))"]}
{"filename": "toyexample/eval.py", "chunked_list": ["import argparse\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.distributions.categorical import Categorical\n\timport os\n\timport sys\n\tsys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n\timport gym\n\timport numpy as np\n\tfrom envs import toy\n", "import random\n\timport seaborn as sns\n\timport matplotlib.pyplot as plt\n\timport matplotlib as mpl\n\tmpl.use('Agg')\n\tfrom rppo import Agent\n\tdef eval(args):\n\t    ## config\n\t    seed = 2021\n\t    num_rollouts = 1000\n", "    device = torch.device('cuda:0')\n\t    tpdv = dict(dtype=torch.float32, device=device)\n\t    env = toy.ToyEnv(wind=True, onehot_state=args.onehot_state)\n\t    env.seed(seed)\n\t    policy = Agent(env.observation_space, env.action_space).to(device)\n\t    policy.restore(args.run_dir)\n\t    ### state frequency\n\t    state_freq = np.zeros(env.shape)\n\t    episode_rewards = []\n\t    violations = []\n", "    for _ in range(num_rollouts):\n\t        state = env.reset()\n\t        episode_reward = 0\n\t        water_times = 0\n\t        step = 0\n\t        while True:\n\t            act = policy.get_action(torch.from_numpy(np.expand_dims(state, axis=0)).to(**tpdv))\n\t            state, reward, done, info = env.step(act.item())\n\t            episode_reward += reward\n\t            water_times += 1 if reward == -1 else 0\n", "            step += 1\n\t            if done:\n\t                state_freq += env.visited\n\t                episode_rewards.append(episode_reward)\n\t                violations.append(water_times / step)\n\t                break\n\t    np.save(args.run_dir + \"/episode_rewards.npy\", np.array(episode_rewards))\n\t    np.save(args.run_dir + \"/state_freq.npy\", state_freq)\n\t    state_freq = state_freq / np.sum(state_freq)\n\t    print(\"state frequency: \", state_freq)\n", "    plt.figure(figsize=(6,4))\n\t    sns.heatmap(state_freq, cmap=\"Reds\")\n\t    plt.savefig(args.run_dir + \"/state_frequency.pdf\")\n\t    # determinstic optimal path for no wind\n\t    env.use_wind = False\n\t    state = env.reset()\n\t    while True:\n\t        act = policy.get_action(torch.from_numpy(np.expand_dims(state, axis=0)).to(**tpdv))\n\t        state, reward, done, info = env.step(act.item())\n\t        if done:\n", "            break\n\t    print(\"policy path for no wind: \", env.visited)\n\t    plt.figure(figsize=(6,4))\n\t    sns.heatmap(env.visited, cmap=\"Reds\")\n\t    plt.savefig(args.run_dir + \"/determinstic_path.pdf\")\n\t    return np.mean(episode_rewards), np.mean(violations)\n\tif __name__ == \"__main__\":\n\t    def parse_args():\n\t        parser = argparse.ArgumentParser()\n\t        parser.add_argument(\"--run-dir\", type=str, default=\"runs/ToyEnv-v0/0.5/Apr07-121744\")\n", "        args = parser.parse_args()\n\t        return args\n\t    args = parse_args()\n\t    args.onehot_state = True\n\t    print(eval(args))\n\t    # returns = []\n\t    # violations = []\n\t    # for tau in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n\t    #     args.run_dir = f\"toyexample/runs/risks/tau{tau}.pt\"\n\t    #     r, v = eval(args)\n", "    #     returns.append(r)\n\t    #     violations.append(v)\n\t    # returns = np.array(returns)\n\t    # violations = np.array(violations)\n\t    # np.save(\"toy_returns.npy\", returns)\n\t    # np.save(\"toy_violations.npy\", violations)"]}
{"filename": "toyexample/rqlearning.py", "chunked_list": ["from math import gamma\n\timport os\n\timport sys\n\tsys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n\timport numpy as np\n\timport envs\n\timport gym\n\timport matplotlib.pyplot as plt\n\timport seaborn as sns\n\tenv = gym.make('ToyEnv-v0', onehot_state=False)\n", "num_epis_train = int(1e4)\n\tinit_lr = 0.01\n\tgamma = 0.95\n\teps = 0.3\n\tlength = 100\n\tnum_iter = 4 * length\n\tnS = env.nS\n\tnA = env.nA\n\tQtable = np.zeros((nS, nA))\n\trender = False\n", "visualize_learning_result = False\n\tlimited = True\n\t# tau = 0.5\n\tdef qlearning(tau=0.5):\n\t    for epis in range(num_epis_train):\n\t        s = env.reset()\n\t        # print(f\"--------episode:{epis+1}---------\")\n\t        for iter in range(1, num_iter+1):\n\t            if np.random.uniform(0, 1) < eps:\n\t                a = np.random.choice(nA)\n", "            else:\n\t                a = np.argmax(Qtable[s,:])\n\t            n_s, r, done,_ = env.step(a)\n\t            # Qtable[s,a] = Qtable[s, a] + lr * (r + gamma*np.max(Qtable[n_s,:]) - Qtable[s, a])\n\t            delta = r + gamma*np.max(Qtable[n_s,:]) - Qtable[s, a]\n\t            alpha = 1 / (2 * max(tau,(1-tau)))\n\t            if epis >= 1e3:\n\t                frac = 1 - (epis - 1e3) / (num_epis_train - 1e3)\n\t                lr = init_lr * frac\n\t            else:\n", "                lr = init_lr\n\t            Qtable[s,a] = Qtable[s, a] + lr * 2 * alpha * (tau * np.maximum(delta, 0) + (1 - tau) * np.minimum(delta, 0))\n\t            s = n_s\n\t            if done: \n\t                break\n\tfor tau in range(11):\n\t    tau = tau / 10\n\t    env.seed(2000)\n\t    qlearning(tau)\n\t    ### state frequency\n", "    env.use_wind = False\n\t    num_rollouts = 10000\n\t    state_freq = np.zeros(env.shape)\n\t    episode_rewards = []\n\t    for _ in range(num_rollouts):\n\t        s = env.reset()\n\t        episode_reward = 0\n\t        while True:\n\t            a  = np.argmax(Qtable[s,:])\n\t            s, reward, done, info = env.step(a)\n", "            episode_reward += reward\n\t            if done:\n\t                state_freq += env.visited\n\t                episode_rewards.append(episode_reward)\n\t                break\n\t    state_freq = state_freq / np.sum(state_freq)\n\t    print(\"state frequency: \", state_freq)\n\t    plt.figure(figsize=(6,4))\n\t    sns.heatmap(state_freq, cmap=\"Reds\")\n\t    plt.savefig(f\"tau{tau}_state_frequency.png\")"]}
