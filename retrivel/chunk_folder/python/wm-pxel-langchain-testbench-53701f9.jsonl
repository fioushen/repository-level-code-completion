{"filename": "setup.py", "chunked_list": ["from setuptools import setup\n\tsetup(\n\t    name='testbench',\n\t    version='0.1.0',\n\t    py_modules=['testbench'],\n\t    install_requires=[\n\t        'Click',\n\t    ],\n\t    entry_points={\n\t        'console_scripts': [\n", "            'testbench = cli:cli',\n\t        ],        \n\t    },\n\t)"]}
{"filename": "cli.py", "chunked_list": ["import click\n\tfrom lib.model.chain_revision import ChainRevision, find_ancestor_ids\n\tfrom lib.model.chain import Chain\n\tfrom lib.model.chain_spec import LLMSpec\n\tfrom lib.model.lang_chain_context import LangChainContext\n\tfrom lib.db import chain_revision_repository, chain_repository, result_repository\n\tfrom bson import ObjectId\n\tfrom collections import defaultdict\n\tfrom typing import Optional, Dict, Any\n\timport lib.chain_service as chain_service\n", "import csv\n\timport dotenv\n\timport json\n\timport re\n\timport sys\n\tdotenv.load_dotenv()\n\t@click.group()\n\tdef cli():\n\t  pass\n\t#####################\n", "# Chain revisions\n\t#####################\n\t@click.group()\n\tdef revision():\n\t  \"\"\"Save and load chain revisions.\"\"\"\n\t  pass\n\tcli.add_command(revision)\n\t@click.command()\n\t@click.argument('chain-name')\n\t@click.argument(\"file\", default=\"-\", type=click.File(\"rb\"))\n", "def save(chain_name, file):\n\t    \"\"\"Save a new chain revision.\n\t    The revision is read as json from stdin or from the given file.\n\t    If chain-name is unknown, a new chain is created. Otherwise,\n\t    This revision will be saved with the chain's current revision as\n\t    its parent, and the chain's reference will be updated to this\n\t    revision.\n\t    \"\"\"\n\t    buffer = b\"\"\n\t    for f in file:\n", "      buffer += f\n\t    revision_json = buffer.decode('utf-8')\n\t    revision = ChainRevision.parse_raw(revision_json)\n\t    revision_id = chain_service.save_revision(chain_name, revision)\n\t    print(\"Saved revision\", revision.id, \"for chain\", chain_name)\n\trevision.add_command(save)\n\t@click.command()\n\t@click.argument('chain-name')\n\tdef load(chain_name):\n\t  \"\"\"Load a chain revision from the database.\n", "  The revision will be loaded and then output as json to stdout.\n\t  \"\"\"\n\t  revision = chain_service.load_by_chain_name(chain_name)\n\t  print(revision.json(indent=2))\n\trevision.add_command(load)\n\t@click.command()\n\t@click.argument('chain-name')\n\tdef history(chain_name):\n\t  \"\"\"Output the ids of all revisions of the chain.\n\t  The ids will be output to stdout.\n", "  \"\"\"\n\t  ancestor_ids = chain_service.history_by_chain_name(chain_name)\n\t  for id in ancestor_ids:\n\t    print(id)\n\trevision.add_command(history)\n\t@click.command()\n\t@click.argument('chain-name')\n\tdef patch(chain_name):\n\t  \"\"\"Replace a single chain spec component to create a new chain.\n\t  The patch must have the same chain_id as the chain spec to be replaced.\n", "  The new chain will be saved as a new revision and the chain name will be\n\t  updated to refrence it.\n\t  \"\"\"\n\t  buffer = b\"\"\n\t  for f in sys.stdin:\n\t    buffer += f.encode('utf-8')\n\t  patch_dict = json.loads(buffer.decode('utf-8'))\n\t  # junk revision lets us correctly deserialize the patch\n\t  junk_revision = ChainRevision(**{'chain': patch_dict, 'llms':{}})\n\t  patch = junk_revision.chain\n", "  revision_id = chain_service.save_patch(chain_name, patch)\n\t  print(\"Saved revision\", revision_id, \"for chain\", chain_name)\n\trevision.add_command(patch)\n\t@click.command()\n\t@click.argument('chain-name')\n\t@click.argument('chain-id')\n\tdef patch_prompt(chain_name, chain_id):\n\t  \"\"\"Replace the prompt text for an LLMSpec of the given revision.\n\t  The prompt text will be provided on stdin.\n\t  The new chain will be saved as a new revision and the chain name will be\n", "  updated to refrence it.\n\t  \"\"\"\n\t  revision = chain_service.load_by_chain_name(chain_name)\n\t  patch = revision.chain.find_by_chain_id(int(chain_id)).copy(deep=True)\n\t  # error if spec is not an LLMSpec\n\t  if not isinstance(patch, LLMSpec):\n\t    print(f\"Spec {chain_id} is not an LLMSpec\")\n\t    sys.exit(1)\n\t  buffer = b\"\"\n\t  for f in sys.stdin:\n", "    buffer += f.encode('utf-8')\n\t  patch.prompt = buffer.decode('utf-8')\n\t  revision_id = chain_service.save_patch(chain_name, patch)\n\t  print(\"Saved revision\", revision_id, \"for chain\", chain_name)\n\trevision.add_command(patch_prompt)\n\t#####################\n\t# Chain names\n\t#####################\n\t@click.group()\n\tdef chain():\n", "  \"\"\"Branch and reset chain names.\"\"\"\n\t  pass\n\tcli.add_command(chain)\n\t@click.command()\n\t@click.argument('chain-name')\n\t@click.argument(\"branch-name\")\n\tdef branch(chain_name, branch_name):\n\t  \"\"\"Branch a chain revision.\n\t  This will create a new chain that points to the same revision\n\t  as the provided chain. The new chain may be then revised independently.\n", "  \"\"\"\n\t  new_chain_id = chain_service.branch(chain_name, branch_name)\n\t  print(f\"Created branch {branch_name} at {new_chain_id}\")\n\tchain.add_command(branch)\n\t@click.command()\n\t@click.argument('chain-name')\n\t@click.argument(\"revision\")\n\tdef reset(chain_name, revision):\n\t  \"\"\"Reset a chain to a another revision.\n\t  This will update the chain to point to the given revision.\n", "  \"\"\"\n\t  chain_service.reset(chain_name, revision)\n\t  print(f\"Reset {chain_name} to revision {revision}\")\n\tchain.add_command(reset)\n\t@click.command()\n\tdef list():\n\t  \"\"\"List all chains.\"\"\"\n\t  for chain in chain_service.list_chains().items():\n\t    print(f\"{chain[0]}: {chain[1]}\")\n\tchain.add_command(list)\n", "#####################\n\t# Running\n\t#####################\n\t@click.group()\n\tdef run():\n\t  \"\"\"Run chains once or interactively.\"\"\"\n\t  pass\n\tcli.add_command(run)\n\tdef save_results(ctx: LangChainContext, revision_id: str):\n\t  results = ctx.results(revision_id)\n", "  for result in results:\n\t    result_repository.save(result)\n\t  ctx.reset_results()\n\t@click.command()\n\t@click.argument('chain-name')\n\t@click.argument(\"input\", default=\"-\", type=click.File(\"rb\"))\n\t@click.option(\"--record\", is_flag=True, help = \"Record the inputs and outputs of LLM chains to the database.\")\n\tdef once(chain_name, input, record):\n\t  \"\"\"Run a chain revision.\n\t  The revision is read from the database and fed input from stdin or the given file.\n", "  The results are ouput as json to stdout.\n\t  \"\"\"\n\t  buffer = b\"\"\n\t  for f in input:\n\t    buffer += f\n\t  input_json = buffer.decode('utf-8')\n\t  input = json.loads(input_json)\n\t  chain_service.initialize_services()\n\t  output = chain_service.run_once(chain_name, input, record)\n\t  print(json.dumps(output, indent=2))\n", "run.add_command(once)\n\t@click.command()\n\t@click.argument('chain-name')\n\t@click.option(\"--record\", is_flag=True, help = \"Record the inputs and outputs of LLM chains to the database.\")\n\tdef interactive(chain_name, record):\n\t  \"\"\"Interact with a chain from the database on the command line.  \n\t  The chain must either have an input key called 'input' or have exactly one \n\t  input key that is not also an output key. The user's input will be fed to this key.\n\t  The chain must either have an output key called 'output' or have exactly one\n\t  output key that is not also an input key. The output of this key will be\n", "  displayed to the user for each iteration.\n\t  Outputs can be fed as inputs to subsequent iterations by adding '_in' to the\n\t  input name and '_out' to the output name. \n\t  For example, if the inputs are 'subject' and 'memory_in', and the outputs are\n\t  'output' and 'memory_out', then the user will be prompted for 'subject',\n\t  the output of 'output' will be displayed, and the output of 'memory_out' will\n\t  be fed to 'memory_in' in the next iteration.\n\t  All other inputs will get the empty string as input, and all other outputs\n\t  will be ignored.\n\t  \"\"\"\n", "  revision = chain_service.load_by_chain_name(chain_name)\n\t  ctx = LangChainContext(llms=revision.llms, recording=True)\n\t  lang_chain = revision.chain.to_lang_chain(ctx)\n\t  input_keys = set(lang_chain.input_keys)\n\t  output_keys = set(lang_chain.output_keys)\n\t  output_mapping = {re.sub(r\"_out$\", \"_in\", key): key\n\t    for key in output_keys\n\t    if re.search(r\"_out$\", key) and re.sub(r\"_out$\", \"_in\", key) in input_keys}\n\t  if \"input\" in input_keys:\n\t    user_input_key = \"input\"\n", "  elif len(input_keys - output_keys) == 1:\n\t    keys = input_keys - output_keys\n\t    user_input_key = next(iter(keys))\n\t  else:\n\t    print(\"error inp key\", input_keys)\n\t    raise Exception(\"Chain must have exactly one input key that is not also an output key or an input key called 'input'\")\n\t  if \"output\" in output_keys:\n\t    user_output_key = \"output\"\n\t  elif len(output_keys - input_keys) == 1:\n\t    user_output_key = list(output_keys - input_keys)[0]\n", "  else:\n\t    raise Exception(\"Chain must have exactly one output key that is not also an input key or an output key called 'output'\")\n\t  chain_service.initialize_services()\n\t  inputs = {key: \"\" for key in input_keys if key != user_input_key}\n\t  while True:\n\t    inputs[user_input_key] = input(f\"{user_input_key}> \")\n\t    outputs = chain_service.run_once(chain_name, inputs, record)\n\t    print(outputs[user_output_key])\n\t    print()\n\t    inputs = {key: outputs[output_mapping[key]] if key in output_mapping else \"\" for key in input_keys}\n", "run.add_command(interactive)\n\t#####################\n\t# Results\n\t#####################\n\t@click.group()\n\tdef results():\n\t  \"\"\"Show results.\"\"\"\n\t  pass\n\tcli.add_command(results)\n\tdef dict_to_csv_column(d: dict):\n", "  return \"\\n\".join([f\"{key}: {value}\" for key, value in d.items()])\n\t@click.command()\n\t@click.option(\"--chain-name\", default=None, help=\"Find results for the current revision of named chain.\")\n\t@click.option(\"--revision\", default=None, help=\"Find results for the given revision id.\")\n\t@click.option(\"--ancestors\", is_flag=True, help=\"Include results for ancestors of specified revision.\")\n\t@click.option(\"--csv-format\", is_flag=True, help=\"Return results as csv instead of json.\")\n\t@click.argument(\"chain-id\")\n\tdef show(chain_name: str, revision: str, ancestors: bool, csv_format: bool, chain_id: str):\n\t  \"\"\"Find results for a prompt in for one or more chain revisions.\n\t  One of chain-name or revision must be specified.\n", "  If the ancestors flag is set, results for all ancestors of the specified revision are included.\n\t  Results are output as json or csv (depending on the --csv-format flag) to stdout.\n\t  \"\"\"\n\t  if chain_name is not None:\n\t    revision = chain_service.load_by_chain_name(chain_name)\n\t  elif revision is not None:\n\t    revision = chain_service.load_by_id(revision)\n\t  else:\n\t    raise Exception(\"Must specify chain name, revision id, or chain id\")  \n\t  results = chain_service.results(revision.id, ancestors, chain_id)\n", "  if csv_format:\n\t    csv_out = csv.writer(sys.stdout)\n\t    csv_out.writerow([\"chain_id\", \"revision\", \"input\", \"output\"])\n\t    for result in results:\n\t      csv_out.writerow([\n\t        result.chain_id, \n\t        result.revision, \n\t        dict_to_csv_column(result.input), \n\t        result.output,\n\t      ])\n", "  else:\n\t    print('[')\n\t    for result in results:\n\t      print(json.dumps(result.dict(), indent=2, default=lambda o: str(o)), end=',\\n')\n\t    print(']')\n\tresults.add_command(show)\n\tdef add_result(results: Dict, key: str, value: Any):\n\t  if key not in results:\n\t    results[key] = []\n\t  results[key].append(value)\n", "@click.command()\n\t@click.argument('chain-name1')\n\t@click.argument('chain-name2')\n\t@click.option(\"-c\", \"--chain-id\", help=\"limit diff to a specific chain id\", required=False, type=int)\n\tdef diff(chain_name1, chain_name2, chain_id: Optional[int] = None):\n\t  revision1 = chain_service.load_by_chain_name(chain_name1)\n\t  revision2 = chain_service.load_by_chain_name(chain_name2)\n\t  query = {'chain_id': chain_id} if chain_id is not None else {}\n\t  grouped_results = {}\n\t  results1 = chain_service.results(revision1.id, False, chain_id)\n", "  for result in results1:\n\t    add_result(grouped_results, json.dumps(result.input), result)\n\t  results2 = chain_service.results(revision2.id, False, chain_id)\n\t  for result in results2:\n\t    add_result(grouped_results, json.dumps(result.input), result)\n\t  csv_out = csv.writer(sys.stdout)\n\t  csv_out.writerow([\"input\", \"revision 1\", \"revision 2\"])\n\t  for input, results in grouped_results.items():\n\t    formatted_input = dict_to_csv_column(json.loads(input))\n\t    if len(results) == 1:\n", "      if results[0].revision == revision1.id:\n\t        csv_out.writerow([formatted_input, results[0].output, \"\"])\n\t      else:\n\t        csv_out.writerow([formatted_input, \"\", results[0].output])\n\t    else:\n\t      csv_out.writerow([formatted_input, results[0].output, results[1].output])\n\tresults.add_command(diff)"]}
{"filename": "scripts/pickle_to_json.py", "chunked_list": ["import pandas as pd\n\tfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\tparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"-k\", \"--api_key\", type=str, help=\"API Key for Pinecone\")\n\tparser.add_argument(\"-emb\", \"--embedding_file\", help=\"File location for embeddings (must be .pkl file)\")\n\targs = vars(parser.parse_args())\n\t## Parse pickle file to JSON for further processing\n\tdef main(args):\n\t    embedding_filepath = args[\"embedding_file\"]\n\t    df_emb = pd.read_pickle(embedding_filepath)\n", "    print(df_emb.to_json())\n\tmain(args)"]}
{"filename": "scripts/generate_openai_embeddings.py", "chunked_list": ["import uuid\n\timport dotenv\n\timport sys\n\timport json\n\tfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\tdotenv.load_dotenv()\n\tparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"-m\", \"--model\", default=\"text-embedding-ada-002\", help=\"OpenAI Model for embedding\")\n\targs = vars(parser.parse_args())\n\tdef createUUID():\n", "    return str(uuid.uuid4())\n\t## Main Function ##\n\tdef main(args):\n\t    from langchain.embeddings import OpenAIEmbeddings\n\t    documents = json.load(sys.stdin)\n\t    embeddings_model = OpenAIEmbeddings(model=args['model'])\n\t    embeddings = embeddings_model.embed_documents([doc['text'] for doc in documents])\n\t    docs = [{\n\t        'text': doc['text'],\n\t        'meta': doc['meta'],\n", "        'embedding': embedding,\n\t    } for doc, embedding in zip(documents, embeddings)]\n\t    json.dump(docs, sys.stdout, indent=2)\n\tmain(args)"]}
{"filename": "scripts/run_vector_search_chain.py", "chunked_list": ["from lib.chains.vector_search_chain import VectorSearchChain\n\tchain = VectorSearchChain(\n\t    query=\"{input}\",\n\t    embedding_engine='huggingface',\n\t    embedding_params={},\n\t    database = 'pinecone',\n\t    database_params = {\n\t      'index': 'test-mpnet-v2',\n\t      'namespace': 'test1',\n\t    },\n", "    num_results=10,\n\t    min_score=0.0,\n\t    input_variables=['input'],\n\t    output_key='results',\n\t)\n\tchain._call({'input': 'How do I open a can of paint?'})\n\tchain._call({'input': 'What is the capitol of the US?'})\n\tchain._call({'input': 'Which country is Kingston in?'})\n\tchain._call({'input': 'Which country is Kingston the capitol of?'})\n\tchain._call({'input': 'Kingston is the capitol of Jamaica.'})\n", "chain._call({'input': \"Which city is Jamaica's capitol?\"})\n\tchain._call({'input': \"Which cities are capitols of Caribbean countries?\"})\n\tchain._call({'input': \"Which cities are capitols of European countries?\"})\n\tchain._call({'input': \"Which cities are capitols of Asian countries?\"})\n\tchain._call({'input': \"Which cities are capitols of South American countries?\"})\n\tchain._call({'input': \"Which cities are capitols of North American countries?\"})\n\tchain._call({'input': \"Which cities are capitols of North African countries?\"})\n\tchain._call({'input': \"blah blah blah\"})"]}
{"filename": "scripts/generate_sentencetransformers_embeddings.py", "chunked_list": ["import uuid\n\timport dotenv\n\timport sys\n\timport json\n\tfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\tdotenv.load_dotenv()\n\tparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"-m\", \"--model\", default=\"all-mpnet-base-v2\", help=\"Huggingface Hub model for embedding\")\n\targs = vars(parser.parse_args())\n\tdef createUUID():\n", "    return str(uuid.uuid4())\n\t## Main Function ##\n\tdef main(args):\n\t    from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n\t    documents = json.load(sys.stdin)\n\t    embeddings_model = HuggingFaceEmbeddings(model_name=args['model'])\n\t    embeddings = embeddings_model.embed_documents([doc['text'] for doc in documents])\n\t    docs = [{**doc,'embedding': embedding} for doc, embedding in zip(documents, embeddings)]\n\t    json.dump(docs, sys.stdout, indent=2)\n\tmain(args)"]}
{"filename": "scripts/coverage.py", "chunked_list": ["import sys\n\timport requests\n\timport json\n\timport re\n\tfrom langchain.prompts import PromptTemplate\n\tfrom langchain.llms import OpenAI\n\tfrom langchain.chains import LLMChain, SequentialChain, TransformChain\n\tfrom langchain.chains.base import Chain\n\tfrom langchain.input import get_colored_text\n\tfrom typing import Any, Callable, Dict, List, Mapping, Optional, Union\n", "from pydantic import root_validator\n\tllm_decider = OpenAI(temperature=0.0)\n\tllm_creative = OpenAI(temperature=0.7)\n\tproduct_prompt = \"What is a good name for a company that makes {product}?\"\n\tclass LazyPrompt(PromptTemplate):\n\t  def format(self, **kwargs: Any) -> str:\n\t    prompt_template = kwargs[self.template]\n\t    template_kwargs = dict(kwargs)\n\t    template_kwargs.pop(self.template)\n\t    return prompt_template.format(**template_kwargs)\n", "  @root_validator()\n\t  def template_is_valid(cls, values: Dict) -> Dict:\n\t    return values\n\tclass CategorizationConditional(Chain):\n\t  categorization_input: str\n\t  subchains: Dict[str, Chain]\n\t  default_chain: Chain\n\t  output_variables: List[str] = [\"text\"]\n\t  # TODO: validator requires the union of subchain inputs\n\t  # TODO: validator requires all subchains have all output keys that are not also input keys\n", "  @property\n\t  def input_keys(self) -> List[str]:    \n\t    return self.default_chain.input_keys + [key for subchain in self.subchains.values() for key in subchain.input_keys]\n\t  @property\n\t  def output_keys(self) -> List[str]:\n\t    return self.output_variables\n\t  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n\t    categorization = inputs[self.categorization_input].strip()\n\t    _colored_text = get_colored_text(categorization, \"yellow\")\n\t    _text = \"Categorization input:\\n\" + _colored_text\n", "    self.callback_manager.on_text(_text, end=\"\\n\", verbose=self.verbose)\n\t    subchain = self.subchains[categorization] if categorization in self.subchains else self.default_chain\n\t    known_values = inputs.copy()\n\t    outputs = subchain(known_values, return_only_outputs=True)\n\t    known_values.update(outputs)\n\t    return {k: known_values[k] for k in self.output_variables}    \n\tclass ESearchChain(Chain):\n\t  output_variable: str = \"text\"\n\t  input_variable: str = \"text\"\n\t  @property\n", "  def input_keys(self) -> List[str]:    \n\t    return [self.input_variable]\n\t  @property\n\t  def output_keys(self) -> List[str]:\n\t    return [self.output_variable]\n\t  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n\t    search = inputs[self.input_variable]\n\t    search = 'x-ray'\n\t    url = f\"http://localhost:9200/coverages/_search?q={requests.utils.quote(search)}\"\n\t    response = requests.get(url)\n", "    decoded_response = json.loads(response.content)\n\t    return {self.output_variable: decoded_response['hits']['hits']}\n\tformat_hit = lambda i, h: f\"{i+1}: ${h['rate']} for {h['name']} {h['description']} (CPT code: {h['billing_code']}).\"\n\tsearch_format_chain = TransformChain(\n\t  input_variables=['hits'],\n\t  output_variables=['search_results'],\n\t  transform = lambda inputs: {'search_results': '\\n'.join([format_hit(i, h['_source']) for (i, h) in enumerate(inputs['hits'])])}\n\t)\n\tcoverage_question_chain = SequentialChain(\n\t  input_variables=[\"disambiguation_prompt\", \"response_prompt\", \"response_prompt\", \"identity\", \"context\", \"response_prefix\", \"user_input\"],\n", "  chains=[\n\t    LLMChain(llm=llm_decider, prompt=LazyPrompt(input_variables=[\"disambiguation_prompt\", \"identity\", \"context\", \"user_input\"], template=\"disambiguation_prompt\"), output_key=\"search\", verbose=True),\n\t    ESearchChain(input_variable=\"search\", output_variable=\"hits\"),\n\t    search_format_chain,\n\t    LLMChain(llm=llm_creative, prompt=LazyPrompt(input_variables=[\"search\", \"response_prompt\", \"search_results\", \"context\", \"identity\", \"response_prefix\"], template=\"response_prompt\"), output_key=\"response\", verbose=True),\n\t    TransformChain(\n\t      input_variables=[\"response_prefix\", \"response\"], output_variables=[\"text\"], \n\t      transform = lambda inputs: {'text': inputs['response_prefix'] + inputs['response']}\n\t    )\n\t  ]\n", ")\n\tchain = SequentialChain(\n\t  input_variables=[\"categorization_prompt\", \"disambiguation_prompt\", \"response_prompt\", \"default_prompt\", \"response_prefix\", \"identity\", \"context\", \"user_input\"],\n\t  chains=[\n\t    LLMChain(llm=llm_decider, prompt=LazyPrompt(input_variables=[\"categorization_prompt\", \"context\", \"user_input\"], template=\"categorization_prompt\"), output_key=\"categorization\", verbose=True),\n\t    CategorizationConditional(\n\t      input_variables=[\"disambiguation_prompt\", \"response_prompt\", \"default_prompt\", \"response_prefix\", \"identity\", \"context\", \"user_input\"],\n\t      categorization_input=\"categorization\",\n\t      verbose=True,\n\t      subchains={\n", "        \"Questions about whether insurance will cover a medical procedure or service\": coverage_question_chain,\n\t        \"Questions about how much insurance will cover for a medical procedure or service\": coverage_question_chain,\n\t      },\n\t      default_chain=LLMChain(llm=llm_creative, prompt=LazyPrompt(input_variables=[\"default_prompt\", \"identity\", \"context\", \"user_input\"], template=\"default_prompt\"), verbose=True),\n\t    ),\n\t  ]\n\t)\n\tidentity_txt = '''You are a very friendly, positive and helpful representative of a health insurance company that is a good listener. \n\tThe customer is enrolled in the company's insurance plan.'''\n\tcategorization_template = '''Context: ${context}\\n The following is a list of categories\n", "that insurance customer questions fall into:\n\tQuestions about whether insurance will cover a medical procedure or service, Questions about how much insurance will cover for a medical procedure or service, Other statements or questions.\n\t{user_input}\n\tCategory:'''\n\tdisambiguation_template = '''{identity}\\n{context}\\nThe customer asks:\\n\\n{user_input}\\n\n\tWhat medical service, procedure or device is the customer asking about? Be concise.\\n\\nThe customer is asking about'''\n\tresponse_template = '''Context: {context} Customer has insurance from a company that covers medical services at the following rates:\\n\n\t{search_results}\n\t{identity} You will only answer about the services listed above.\\n\n\tRespond with normal capitalization. Use full words rather than abbrieviations. Do not provide CPT or medical codes in your responses. \n", "Do not respond with different coverage rates. \n\tAsk the customer to be more specific.\n\t\\n##\\nCustomer: Tell me how much money my insurance covers for {search} and describe that service.\\nAgent: {response_prefix}'''\n\tdefault_template = '''{identity}\\n{context}\\n\n\tYou will only answer questions about health insurance.\n\t\\n##\\nCustomer: ${user_input}\\nAgent:'''\n\tdef prompt_from_template(template) -> PromptTemplate:\n\t    keys = re.findall(r'{([^{}]*)}', template)\n\t    return PromptTemplate(\n\t      input_variables=keys,\n", "      template = template\n\t    )\n\tsystem_inputs = {\n\t  \"identity\": identity_txt,\n\t  \"context\": \"\",\n\t  \"response_prefix\": \"Your insurance will\",\n\t  \"categorization_prompt\": prompt_from_template(categorization_template),\n\t  \"disambiguation_prompt\": prompt_from_template(disambiguation_template),\n\t  \"response_prompt\": prompt_from_template(response_template),\n\t  \"default_prompt\": prompt_from_template(default_template),\n", "}\n\tuser_inputs = {\n\t  \"user_input\": sys.argv[1],\n\t}\n\t# Run the chain only specifying the input variable.\n\tprint(chain.run(dict(**system_inputs, **user_inputs)))"]}
{"filename": "scripts/print_chain_spec.py", "chunked_list": ["from langchain.llms import OpenAI\n\tfrom lib.model.chain_revision import ChainRevision\n\tfrom lib.model.chain_spec import LLMSpec, SequentialSpec\n\timport dotenv\n\tdotenv.load_dotenv()\n\tllm = OpenAI(temperature=0.8)\n\tchain = SequentialSpec(\n\t  chain_id = 0,\n\t  input_keys = [\"input\", \"memory_in\"],\n\t  output_keys = [\"output\", \"memory_out\"],\n", "  chain_type = \"sequential_spec\",\n\t  chains = [\n\t    LLMSpec(\n\t      chain_id = 1,\n\t      input_keys = [\"input\", \"memory_in\"],\n\t      output_key = \"output\",\n\t      prompt = \"Context: {memory_in}\\n\\nYou are a witty but kind professor. Respond in a single paragraph to the student's question or statement.\\n\\nStudent: {input}\\n\\nResponse:\",\n\t      llm_key = \"llm\",\n\t      chain_type = \"llm_spec\",\n\t    ),\n", "    LLMSpec(\n\t      chain_id = 2,\n\t      input_keys = [\"input\", \"output\", \"memory_in\"],\n\t      output_key = \"memory_out\",\n\t      prompt = \"You are a witty but kind professor. Summarize in a single paragraph the conversation up to this point including the context.\\n\\nContext: {memory_in}\\n\\nStudent: {input}\\n\\nProfessor: {output}\\n\\nSummary:\",\n\t      llm_key = \"llm\",\n\t      chain_type = \"llm_spec\",\n\t    ),\n\t  ]\n\t)\n", "chain_revision = ChainRevision(\n\t  llms = {\"llm\": llm},\n\t  chain = chain\n\t)\n\tprint(chain_revision.json())"]}
{"filename": "scripts/send_to_pinecone.py", "chunked_list": ["import pinecone\n\timport uuid\n\timport json\n\timport os\n\timport sys\n\timport dotenv\n\tfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\tdotenv.load_dotenv()\n\tBATCH_SIZE = 200\n\tparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n", "parser.add_argument(\"-i\", \"--index\", help=\"Name of Pinecone index to insert into\")\n\tparser.add_argument(\"-n\", \"--namespace\", help=\"Namespace for Pinecone index\")\n\targs = vars(parser.parse_args())\n\tdef createUUID():\n\t    return str(uuid.uuid4())\n\tdef main(args):\n\t    pinecone.init(api_key=os.environ['PINECONE_API_KEY'], environment=os.environ['PINECONE_ENVIRONMENT'])\n\t    embeddings = json.load(sys.stdin)\n\t    pinecone_embeddings = [(createUUID(), doc['embedding'], {**doc['meta'], 'text': doc['text']}) for doc in embeddings]\n\t    index = pinecone.Index(args['index'])\n", "    batch = pinecone_embeddings[:BATCH_SIZE]\n\t    pinecone_embeddings = pinecone_embeddings[BATCH_SIZE:]\n\t    while len(batch) > 0:\n\t      index.upsert(vectors=batch, namespace=args['namespace'])\n\t      batch = pinecone_embeddings[:BATCH_SIZE]\n\t      pinecone_embeddings = pinecone_embeddings[BATCH_SIZE:]\n\t    print(json.dumps(index.describe_index_stats().to_dict(), indent=2))\n\tmain(args)"]}
{"filename": "scripts/test.py", "chunked_list": ["from pprint import pprint\n\tfrom lib.model.chain_revision import ChainRevision\n\tfrom lib.db import chain_repository, chain_revision_repository\n\tfrom langchain.prompts import PromptTemplate\n\tfrom langchain.llms import OpenAI\n\tfrom lib.model.chain_spec import LLMSpec, SequentialSpec\n\tfrom lib.model.chain import Chain\n\tdef pretty_print(clas, indent=0):\n\t  print(' ' * indent +  type(clas).__name__ +  ':')\n\t  indent += 4\n", "  for k,v in clas.__dict__.items():\n\t    if '__dict__' in dir(v):\n\t      pretty_print(v,indent)\n\t    else:\n\t      print(' ' * indent +  k + ': ' + str(v))\n\tcategorization_template = '''Context: ${context}\\n The following is a list of categories\n\tthat insurance customer questions fall into:\n\tQuestions about whether insurance will cover a medical procedure or service, Questions about how much insurance will cover for a medical procedure or service, Other statements or questions.\n\t{user_input}\n\tCategory:'''\n", "template2 = \"Sumarize this response:\\n\\n{category}\\n\\nSummary:\"\n\t# llm = OpenAI(temperature=0.7)\n\tchain1 = LLMSpec(prompt=categorization_template, chain_id=0, llm_key=\"llm\", input_keys=[], output_key=\"out\", chain_type='llm_spec')\n\tchain2 = LLMSpec(prompt=template2, chain_id=0, llm_key=\"llm\", input_keys=[], output_key=\"out\", chain_type='llm_spec')\n\tchain = SequentialSpec(\n\t  chains=[chain1, chain2],\n\t  chain_id=1,\n\t  input_keys=[],\n\t  output_keys=[\"out\"],\n\t  chain_type='sequential_spec'\n", ")\n\trevision = ChainRevision(name=\"main\", major=0, minor=0, chain=chain, llms={\"llm\": OpenAI(temperature=0.7)})\n\tchain_revision_repository.save(revision)\n\ttest_chain = Chain(name=\"test\", revision=revision.id)\n\tchain_repository.save(test_chain)\n\trecords = chain_revision_repository.find_by({})\n\trevisions = [x for x in records]\n\tpretty_print(revisions[0], indent=2)\n\tfor revision in revisions:\n\t  print(chain_revision_repository.delete(revision))\n", "records = chain_repository.find_by({})\n\tchains = [x for x in records]\n\tprint(chains[0])\n\tfor c in chains:\n\t  print(chain_repository.delete(c))"]}
{"filename": "scripts/csv_to_json.py", "chunked_list": ["import csv\n\timport sys\n\timport json\n\tfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\tparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n\targs = vars(parser.parse_args())\n\t# Parse CSV to JSON\n\t# Input: CSV file with header containing 'sentences' and 'section_time_stamp'\n\t# Output: JSON file with sentences in 'text' field and timestamp in 'meta.section' field.\n\tdef main(args):\n", "  data = csv.DictReader(sys.stdin)\n\t  records = [{\n\t    'text': row['sentences'],\n\t    'meta': {\n\t      'section': row['section_time_stamp'],\n\t    }\n\t  } for row in data]\n\t  json.dump(records, sys.stdout)\n\tmain(args)"]}
{"filename": "lib/db_migration.py", "chunked_list": ["import os\n\tfrom dotenv import load_dotenv\n\tfrom pymongo import MongoClient\n\tchain_spec_conversion = {\n\t      \"llm_spec\": \"llm_chain_spec\",\n\t      \"sequential_spec\": \"sequential_chain_spec\",\n\t      \"case_spec\": \"case_chain_spec\",\n\t      \"reformat_spec\": \"reformat_chain_spec\",\n\t      \"transform_spec\": \"transform_chain_spec\",\n\t      \"api_spec\": \"api_chain_spec\",\n", "      \"vector_search_spec\": \"vector_search_chain_spec\"\n\t}\n\tdef updateChainType(chain, prefix, update_fields):\n\t  # make sure chain_type is properly formatted\n\t  try:\n\t    chain_type = chain.get(\"chain_type\")\n\t    if chain_type and chain_type in chain_spec_conversion:\n\t      update_fields[f\"{prefix}.chain_type\"] = chain_spec_conversion[chain_type]\n\t  except Exception as e:\n\t    print(e, chain, flush=True)\n", "  # recursively handle sequential chains\n\t  if \"chains\" in chain:\n\t    if type(chain[\"chains\"]) is list:\n\t      for index, child_chain in enumerate(chain[\"chains\"]):\n\t        updateChainType(child_chain, f\"{prefix}.chains.{index}\", update_fields)\n\t    else:\n\t      for key, child_chain in chain[\"chains\"].items():\n\t        updateChainType(child_chain, f\"{prefix}.chains.{key}\", update_fields)\n\t  # recursively handle case chains\n\t  if \"cases\" in chain:\n", "    for key, case in chain[\"cases\"].items():\n\t      updateChainType(case, f\"{prefix}.cases.{key}\", update_fields)\n\t    updateChainType(chain[\"default_case\"], f\"{prefix}.default_case\", update_fields)\n\tload_dotenv()\n\tclient = MongoClient(host=[\"localhost:27017\"],\n\t                     username=\"mongoadmin\",\n\t                     password=os.environ[\"MONGO_ROOT_PASSWORD\"])\n\tdatabase = client[os.environ[\"MONGODB_DATABASE\"]]\n\tchain_revisions = database[\"chain_revisions\"]\n\tfor revision in chain_revisions.find({}):\n", "  update_fields = {}\n\t  unset_fields = {}\n\t  # Handle the recursive case when chains have children\n\t  updateChainType(revision[\"chain\"], \"chain\", update_fields)\n\t  for key, llm in revision.get(\"llms\", {}).items():\n\t    if \"_type\" in llm and llm[\"_type\"] is not None:\n\t      llm_type = llm[\"_type\"].replace(\"_llm\", \"\")\n\t      update_fields[f\"llms.{key}.llm_type\"] = llm_type\n\t      unset_fields[f\"llms.{key}._type\"] = \"\"\n\t  if update_fields or unset_fields:\n", "    try:\n\t      updates = {}\n\t      if update_fields:\n\t        updates[\"$set\"] = update_fields\n\t      if unset_fields:\n\t        updates[\"$unset\"] = unset_fields\n\t      chain_revisions.update_one({\"_id\": revision[\"_id\"]}, updates)\n\t    except Exception as e:\n\t      print(f\"Error updating document with _id={revision['_id']}: {e}\")\n\tclient.close()\n"]}
{"filename": "lib/db.py", "chunked_list": ["import os\n\tfrom dotenv import load_dotenv\n\tfrom pymongo import MongoClient\n\tfrom lib.model.chain_revision import ChainRevisionRepository\n\tfrom lib.model.chain import ChainRepository\n\tfrom lib.model.result import ResultRepository\n\tload_dotenv()\n\tclient = MongoClient(os.environ[\"MONGODB_URL\"])\n\tdatabase = client[os.environ[\"MONGODB_DATABASE\"]]\n\tchain_revision_repository = ChainRevisionRepository(database=database)\n", "chain_repository = ChainRepository(database=database)\n\tresult_repository = ResultRepository(database=database)\n"]}
{"filename": "lib/chain_service.py", "chunked_list": ["import logging\n\timport traceback\n\tfrom types import MappingProxyType\n\tfrom typing import Optional, Dict\n\timport json\n\tfrom huggingface_hub.inference_api import InferenceApi\n\tfrom langchain import HuggingFaceHub, OpenAI\n\tfrom lib.model.chain_revision import ChainRevision, find_ancestor_ids\n\tfrom lib.model.chain import Chain\n\tfrom lib.model.lang_chain_context import LangChainContext\n", "from lib.db import chain_revision_repository, chain_repository, result_repository\n\tfrom bson import ObjectId\n\timport dotenv\n\timport os\n\timport pinecone\n\tlogging.basicConfig(level=logging.INFO)\n\tdotenv.load_dotenv()\n\tdef initialize_services():\n\t  if os.getenv(\"PINECONE_API_KEY\") is not None:\n\t    pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENVIRONMENT\"))\n", "def save_revision(chain_name, revision) -> str:\n\t  chain = chain_repository.find_one_by({\"name\": chain_name})\n\t  if chain is not None:\n\t    revision.parent = chain.revision\n\t    chain.revision = revision.id\n\t    chain_revision_repository.save(revision)\n\t    chain.revision = revision.id\n\t    chain_repository.save(chain)\n\t  else:\n\t    chain_revision_repository.save(revision)\n", "    chain = Chain(name=chain_name, revision=revision.id)\n\t    chain_repository.save(chain)\n\t  return revision.id\n\tdef load_by_chain_name(chain_name):\n\t  chain = chain_repository.find_one_by({\"name\": chain_name})\n\t  return chain_revision_repository.find_one_by_id(chain.revision)\n\tdef load_by_id(revision_id):\n\t  return chain_revision_repository.find_one_by_id(revision_id)\n\tdef history_by_chain_name(chain_name):\n\t  \"\"\"return the ids of all revisions of the chain.\n", "  \"\"\"\n\t  chain = chain_repository.find_one_by({\"name\": chain_name})\n\t  revision = chain_revision_repository.find_one_by_id(chain.revision)\n\t  return [revision.id] + find_ancestor_ids(revision.id, chain_revision_repository)\n\tdef save_patch(chain_name, patch) -> str:\n\t  chain = chain_repository.find_one_by({\"name\": chain_name})\n\t  revision = chain_revision_repository.find_one_by_id(chain.revision)\n\t  new_chain = revision.chain.copy_replace(lambda spec: patch if spec.chain_id == patch.chain_id else spec)\n\t  try:\n\t    ctx = LangChainContext(llms=revision.llms)\n", "    revision.chain.to_lang_chain(ctx)\n\t  except Exception as e:\n\t    raise ValueError(\"Could not realize patched chain:\", e)\n\t  new_revision = revision.copy()\n\t  new_revision.id = None\n\t  new_revision.chain = new_chain\n\t  new_revision.parent = revision.id\n\t  chain_revision_repository.save(new_revision)\n\t  chain.revision = new_revision.id\n\t  chain_repository.save(chain)\n", "  return revision.id\n\tdef branch(chain_name, branch_name):\n\t  \"\"\"Branch a chain revision.\n\t  This will create a new chain that points to the same revision\n\t  as the provided chain. The new chain may be then revised independently.\n\t  \"\"\"\n\t  chain = chain_repository.find_one_by({\"name\": chain_name})\n\t  new_chain = Chain(name=branch_name, revision=chain.revision)\n\t  chain_repository.save(new_chain)\n\tdef reset(chain_name, revision):\n", "  \"\"\"Reset a chain to a another revision.\n\t  This will update the chain to point to the given revision.\n\t  \"\"\"\n\t  new_revision = chain_revision_repository.find_one_by({\"id\": ObjectId(revision)})\n\t  if new_revision is None:\n\t    raise ValueError(f\"Revision {revision} not found\")\n\t  chain = chain_repository.find_one_by({\"name\": chain_name})\n\t  chain.revision = new_revision.id\n\t  chain_repository.save(chain)\n\tdef list_chains():\n", "  \"\"\"List all chains.\"\"\"\n\t  return {chain_name.name: str(chain_name.revision) for chain_name in chain_repository.find_by({})}\n\tdef save_results(ctx: LangChainContext, revision_id: str):\n\t  results = ctx.results(revision_id)\n\t  for result in results:\n\t    result_repository.save(result)\n\t  ctx.reset_results()\n\tdef run_once(chain_name, input, record):\n\t  \"\"\"Run a chain revision.\n\t  The revision is read from the database and fed input from stdin or the given file.\n", "  The results are ouput as json to stdout.\n\t  \"\"\"\n\t  chain = chain_repository.find_one_by({\"name\": chain_name})\n\t  revision = chain_revision_repository.find_one_by_id(chain.revision)\n\t  filledLLMs = {key: llm.to_llm() for key, llm in revision.llms.items()}\n\t  ctx = LangChainContext(llms=filledLLMs, recording=True)\n\t  lang_chain = revision.chain.to_lang_chain(ctx)\n\t  output = lang_chain._call(input)\n\t  if (record):\n\t    save_results(ctx, revision.id)\n", "  return output\n\tdef results(revision_id: str, ancestors: bool, chain_id: Optional[str] = None):\n\t  \"\"\"Find results for a prompt in for one or more chain revisions.\n\t  One of chain-name or revision must be specified.\n\t  If the ancestors flag is set, results for all ancestors of the specified revision are included.\n\t  \"\"\"\n\t  revision_ids = [ObjectId(revision_id)]\n\t  # if ancestors are requested, find all ancestors of the specified revision\n\t  if ancestors:\n\t    ancestors_id = find_ancestor_ids(revision_id, chain_revision_repository)\n", "    revision_ids += [ObjectId(i) for i in ancestors_id]\n\t  return result_repository.find_by({\"revision\": {\"$in\": revision_ids}, \"chain_id\": int(chain_id)})\n\tdef export_chain(chain_name: str) -> str:\n\t    export_ids = history_by_chain_name(chain_name)\n\t    chain_revisions = [chain_revision_repository.find_one_by_id(id) for id in export_ids]\n\t    if not all(chain_revisions):\n\t        missing_id = next((id for id, revision in zip(export_ids, chain_revisions) if not revision), None)\n\t        raise Exception(\"Could not find revision with id: \" + missing_id)\n\t    return chain_revisions[::-1]\n\tdef import_chain(chain_name, chain_details):\n", "    root_revision = None\n\t    for revision in chain_details:\n\t        try:\n\t          if not revision.parent:\n\t            root_revision = revision\n\t          chain_revision_repository.save(revision)\n\t        except Exception as e:\n\t          traceback.print_exc()\n\t    leaf_revision = find_leaf_revision(chain_details, root_revision)\n\t    chain = Chain(name=chain_name, revision=leaf_revision)\n", "    chain_repository.save(chain)\n\tdef find_leaf_revision(revisions, current_revision):\n\t    if (current_revision is None):\n\t       return current_revision.id\n\t    id = current_revision.id\n\t    for revision in revisions:\n\t        if revision.parent == current_revision.id:\n\t            return find_leaf_revision(revisions, revision)\n\t    return current_revision.id\n"]}
{"filename": "lib/chains/reformat_chain.py", "chunked_list": ["import json\n\tfrom typing import Dict, List\n\tfrom langchain.chains.base import Chain\n\tfrom lib.formatters.extended_formatter import ExtendedFormatter\n\tclass ReformatChain(Chain):\n\t  input_variables: List[str]\n\t  formatters: Dict[str, str]\n\t  @property\n\t  def input_keys(self) -> List[str]:    \n\t    return self.input_variables\n", "  @property\n\t  def output_keys(self) -> List[str]:\n\t    return list(self.formatters.keys())\n\t  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n\t    formatter = ExtendedFormatter()\n\t    return {k: formatter.format(v, **inputs) for k, v in self.formatters.items()}"]}
{"filename": "lib/chains/api_chain.py", "chunked_list": ["import os\n\tfrom typing import Dict, List, Optional\n\tfrom langchain.chains.base import Chain\n\timport requests\n\tclass APIChain(Chain):\n\t  url: str\n\t  method: str\n\t  headers: Optional[Dict[str, str]]\n\t  body: Optional[str]\n\t  output_variable: str\n", "  input_variables: List[str]\n\t  @property\n\t  def input_keys(self) -> List[str]:    \n\t    return self.input_variables\n\t  @property\n\t  def output_keys(self) -> List[str]:\n\t    return [self.output_variable]\n\t  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n\t    vars = {**os.environ, **inputs}\n\t    f_url = self.url.format(**vars)\n", "    f_headers = {}\n\t    if self.headers is not None:\n\t      f_headers = {k: v.format(**vars) for k, v in self.headers.items()}\n\t    if self.method.lower() == 'get':\n\t      res = requests.get(f_url, headers=f_headers)\n\t    elif self.method.lower() == 'post':\n\t      f_body = self.body.format(**vars)\n\t      res = requests.post(f_url, data=f_body)\n\t    return {self.output_variable: res.text}"]}
{"filename": "lib/chains/case_chain.py", "chunked_list": ["from typing import Dict, List\n\tfrom langchain.chains.base import Chain\n\tfrom pydantic import root_validator\n\tclass CaseChain(Chain):\n\t  categorization_input: str\n\t  subchains: Dict[str, Chain]\n\t  default_chain: Chain\n\t  output_variables: List[str] = [\"text\"]\n\t  input_keys = []\n\t  output_keys = []\n", "  @property\n\t  def input_keys(self) -> List[str]:\n\t    keys = list(set(self.default_chain.input_keys \\\n\t      + [key for subchain in self.subchains.values() for key in subchain.input_keys] \\\n\t      + [self.categorization_input]))\n\t    keys.sort()   \n\t    return keys\n\t  @property\n\t  def output_keys(self) -> List[str]:\n\t    keys_set = set(self.default_chain.output_keys)\n", "    for subchain in self.subchains.values():\n\t      keys_set = keys_set.intersection(subchain.output_keys)\n\t    keys = list(keys_set)\n\t    keys.sort()\n\t    return keys\n\t  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n\t    categorization = inputs[self.categorization_input].strip()\n\t    subchain = self.subchains[categorization] if categorization in self.subchains else self.default_chain\n\t    known_values = inputs.copy()\n\t    outputs = subchain(known_values, return_only_outputs=True)\n", "    known_values.update(outputs)\n\t    return {k: known_values[k] for k in self.output_keys}  "]}
{"filename": "lib/chains/__init__.py", "chunked_list": []}
{"filename": "lib/chains/vector_search_chain.py", "chunked_list": ["import json\n\timport os\n\tfrom typing import Dict, List, Any\n\tfrom langchain.chains.base import Chain\n\tfrom langchain.embeddings.openai import OpenAIEmbeddings\n\tfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\tfrom langchain.vectorstores.base import VectorStore\n\tclass VectorSearchChain(Chain):\n\t  query: str\n\t  embedding_engine: str\n", "  embedding_params: Dict[str, Any]\n\t  database: str\n\t  database_params: Dict[str, Any]\n\t  num_results: int\n\t  min_score: float = 0.0\n\t  input_variables: List[str] = []\n\t  output_key: str = 'results'\n\t  @property\n\t  def input_keys(self) -> List[str]:\n\t    return self.input_variables\n", "  @property\n\t  def output_keys(self) -> List[str]:\n\t    return [self.output_key]\n\t  def vector_store(self, query) -> VectorStore:    \n\t    if self.database == 'pinecone':\n\t      if 'index' not in self.database_params:\n\t        raise ValueError('Missing index parameter for Pinecone database')\n\t      # import just-in-time so auth will happen after env vars are loaded\n\t      import pinecone\n\t      from langchain.vectorstores.pinecone import Pinecone\n", "      index = pinecone.Index(self.database_params['index'])\n\t      return Pinecone(\n\t        index,\n\t        query,\n\t        self.database_params.get('text_key') or 'text',\n\t        namespace=self.database_params.get('namespace') or ''\n\t      )\n\t    else:\n\t      raise ValueError(f'Unknown database: {self.database}')\n\t  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n", "    if self.embedding_engine == 'openai':\n\t      self.embedding_params['openai_api_key'] = os.environ.get(\"OPENAI_API_KEY\")\n\t      embeddings = OpenAIEmbeddings(**self.embedding_params)\n\t    elif self.embedding_engine == 'huggingface':\n\t      model = self.embedding_params.get('model_name') or 'all-mpnet-base-v2'\n\t      embeddings = HuggingFaceEmbeddings(**self.embedding_params, model_name=model)\n\t    else:\n\t      raise ValueError(f'Unknown embedding engine: {self.embedding_engine}')\n\t    vector_store = self.vector_store(embeddings.embed_query)\n\t    formatted_query = self.query.format(**inputs)\n", "    items = vector_store.similarity_search_with_score(formatted_query, self.num_results, self.database_params.get('filter'), self.database_params.get('namespace'))\n\t    return {self.output_key: json.dumps([{'text': item[0].page_content, 'meta': item[0].metadata, 'score': item[1]} for item in items if item[1] >= self.min_score])}"]}
{"filename": "lib/chains/llm_recording_chain.py", "chunked_list": ["from typing import Dict, List\n\tfrom pydantic import Field\n\tfrom langchain.chains import LLMChain\n\tclass LLMRecordingChain(LLMChain):\n\t  recorded_calls: List[tuple[Dict[str, str], Dict[str,str]]]\n\t  chain_spec_id: int\n\t  def __init__(self, llm: object, prompt: str, output_key: str, chain_spec_id: int):\n\t    super().__init__(llm=llm, prompt=prompt, output_key=output_key, chain_spec_id=chain_spec_id, recorded_calls=[])\n\t  def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n\t    output = super()._call(inputs)\n", "    self.recorded_calls.append((dict(inputs), output[self.output_key]))\n\t    return output\n\t  @property\n\t  def calls(self) -> List[tuple[Dict[str, str], Dict[str, str]]]:\n\t    return self.recorded_calls\n\t  def reset(self):\n\t    self.recorded_calls.clear()\n"]}
{"filename": "lib/chains/tests/test_vector_search_chain.py", "chunked_list": ["import json\n\timport os\n\tfrom typing import Any, Iterable, Optional, List, Tuple\n\tfrom unittest.mock import patch\n\tfrom langchain.vectorstores.base import VectorStore\n\tfrom langchain.docstore.document import Document\n\tfrom chains.vector_search_chain import VectorSearchChain\n\tclass MockVectorStore(VectorStore):\n\t  def similarity_search_with_score(self, query: str, k: int = 5, filter: Optional[dict] = None, namespace: Optional[str] = None) -> List[Tuple[Document, float]]:\n\t    assert query == \"How do I open a can of soup?\"\n", "    return [\n\t      (Document(page_content=\"Opening cans of soup.\", metadata={}), 0.5),\n\t      (Document(page_content=\"Opening cans of paint.\", metadata={}), 0.4),\n\t    ]\n\t  def add_texts(self, texts: Iterable[str], metadatas: List[dict] | None = None, **kwargs: Any) -> List[str]:\n\t    return super().add_texts(texts, metadatas, **kwargs)\n\t  def similarity_search(self, query: str, k: int = 4, **kwargs: Any) -> List[Document]:\n\t    return super().similarity_search(query, k, **kwargs)\n\t  def from_texts(self, texts: Iterable[str], metadatas: List[dict] | None = None, **kwargs: Any) -> List[str]:\n\t    return super().from_texts(texts, metadatas, **kwargs)  \n", "def test_openai_pinecone_search():\n\t  os.environ.setdefault(\"OPENAI_API_KEY\", \"test\")\n\t  chain = VectorSearchChain(\n\t    query=\"How do I open a can of {can_type}?\",\n\t    embedding_engine=\"openai\",\n\t    embedding_params={\"openai_api_key\": \"test\"},\n\t    database=\"pinecone\",\n\t    database_params={\"index\": \"test\", \"text_key\": \"text\"},\n\t    input_variables=[],\n\t    num_results=10,\n", "  )\n\t  with patch.object(VectorSearchChain, 'vector_store', return_value=MockVectorStore()):\n\t    response = chain._call({\"can_type\": \"soup\"})\n\t    results = json.loads(response['results'])\n\t    assert len(results) == 2\n\t    assert results[0]['text'] == \"Opening cans of soup.\""]}
{"filename": "lib/chains/tests/test_case_chain.py", "chunked_list": ["from pytest import fixture\n\tfrom chains.case_chain import CaseChain\n\tfrom langchain.chains import LLMChain\n\tfrom langchain.prompts import PromptTemplate\n\tfrom langchain.llms.fake import FakeListLLM\n\tdef simple_case_chain():\n\t  return CaseChain(\n\t    categorization_input=\"categorization\",\n\t    subchains={\n\t      \"a\": LLMChain(\n", "        prompt=PromptTemplate(input_variables=[\"input2\", \"input3\"], template=\"prompt1 {input2} {input3}\"),\n\t        llm=FakeListLLM(responses=[\"fake_response1\"]),\n\t        output_key=\"output1\",\n\t      ),\n\t      \"b\": LLMChain(\n\t        prompt=PromptTemplate(input_variables=[\"input3\"], template=\"prompt2 {input3}\"),\n\t        llm=FakeListLLM(responses=[\"fake_response2\"]),\n\t        output_key=\"output1\",\n\t      ),\n\t    },\n", "    default_chain=LLMChain(\n\t      prompt=PromptTemplate(input_variables=[\"input1\", \"input2\"], template=\"prompt3 {input1} {input2}\"),\n\t      llm=FakeListLLM(responses=[\"fake_response3\"]),\n\t      output_key=\"output1\",\n\t    ),\n\t  )\n\tdef test_input_keys():\n\t  chain = simple_case_chain()\n\t  assert chain.input_keys == [\"categorization\", \"input1\", \"input2\", \"input3\"]\n\tdef test_output_keys():\n", "  chain = simple_case_chain()\n\t  assert chain.output_keys == [\"output1\"]\n\tdef test_run():\n\t  chain = simple_case_chain()\n\t  inputs = {\"input1\": \"input1\", \"input2\": \"input2\", \"input3\": \"input3\"}\n\t  assert chain.run({\"categorization\": \"a\", **inputs}) == \"fake_response1\"\n\t  assert chain.run({\"categorization\": \"b\", **inputs}) == \"fake_response2\"\n\t  assert chain.run({\"categorization\": \"garbage\", **inputs}) == \"fake_response3\""]}
{"filename": "lib/chains/tests/__init__.py", "chunked_list": []}
{"filename": "lib/chains/tests/test_reformat_chain.py", "chunked_list": ["from chains.reformat_chain import ReformatChain\n\tdef test_reformat_chain_formats_inputs():\n\t  chain = ReformatChain(\n\t      formatters={\"output1\": \"{input1}-{input2}\", \"output2\": \"{input2}\"},\n\t      input_variables=[\"input1\", \"input2\"],\n\t  )\n\t  inputs = {\"input1\": \"foo\", \"input2\": \"bar\"}\n\t  output = chain._call(inputs)\n\t  assert output == {\"output1\": \"foo-bar\", \"output2\": \"bar\"}\n\tdef test_reformat_extended_formatting():\n", "  inputs = {\"result\": \"[34,93,94]\", \"weight\": '.6'}\n\t  formatters = {\n\t    \"output\": \"{float:weight:w}{parse_json:result:values}{join:values:,}{expr:item*w:x}{x:.2f}\",\n\t  }\n\t  chain = ReformatChain(\n\t      formatters=formatters,\n\t      input_variables=[\"result\", \"weight\"],\n\t  )\n\t  output = chain._call(inputs)\n\t  assert output == {\"output\": \"20.40,55.80,56.40\"}"]}
{"filename": "lib/chains/tests/test_api_chain.py", "chunked_list": ["import requests_mock\n\tfrom chains.api_chain import APIChain\n\tdef test_api_chain_get():\n\t  chain = APIChain(\n\t    url='http://localhost:9200/test?q={query}',\n\t    method='get',\n\t    headers={'Authorization': 'Bearer {token}'},\n\t    output_variable='response',\n\t    input_variables=['query', 'token']\n\t  )\n", "  with requests_mock.Mocker() as m:\n\t    response = '{\"hits\": {\"hits\": [{\"_source\": {\"text\": \"x-ray\"}}]}}'\n\t    m.get('http://localhost:9200/test?q=x-ray',\n\t      text=response,      \n\t      headers={'Authorization': 'Bearer 1234'}\n\t    )\n\t    inputs = {\"query\": \"x-ray\", \"token\": \"1234\"}\n\t    output = chain.run(inputs)\n\t    assert output == response\n\tdef test_api_chain_post():\n", "  chain = APIChain(\n\t    url='http://localhost:9200/test',\n\t    method='post',\n\t    headers={'Authorization': 'Bearer {token}'},\n\t    body='{{\"query\": \"{query}\"}}',\n\t    output_variable='response',\n\t    input_variables=['query', 'token']\n\t  )\n\t  with requests_mock.Mocker() as m:\n\t    response = '{\"hits\": {\"hits\": [{\"_source\": {\"text\": \"x-ray\"}}]}}'\n", "    m.post('http://localhost:9200/test', \n\t      text=response,\n\t      headers={'Authorization': 'Bearer 1234'}\n\t    )\n\t    inputs = {\"query\": \"x-ray\", \"token\": \"1234\"}\n\t    output = chain.run(inputs)\n\t    assert m.last_request.json() == {\"query\": \"x-ray\"}\n\t    assert output == response\n"]}
{"filename": "lib/formatters/__init__.py", "chunked_list": []}
{"filename": "lib/formatters/extended_formatter.py", "chunked_list": ["import string\n\timport re\n\timport json\n\timport numexpr\n\tclass ExtendedFormatter(string.Formatter):\n\t  \"\"\"\n\t  This class extends the standard Python string formatter to add some extra\n\t  features that are useful for formatting data for the LangChain.\n\t  The following expressions are added:\n\t  - `{join:[varexpr]:[separator]}`: Join an array found at varexpr with the separator. For each\n", "    item in the array, the rest of the format string is formatted with the item as the `item` variable\n\t    and the index of the item as the `index` variable. If no separator is provided, the items are\n\t    joined without a separator. Example:\n\t    format('{join:data[colors]:, }{index}:{item}', data={'colors': ['r','g','b']})) -> '0:r, 1:g, 2:b'\n\t  - `{parse_json:[varexpr]:[varname]}`: Parse a JSON string found at varexpr and store the result\n\t    in a variable with the name varname which will be available in the rest of the expression.\n\t    If no varname is provided, the variable is named `data`.\n\t    Example: format('{parse_json:data[json]:person}{person[name]}', data={'json': '{\"name\": \"John\"}'}) -> 'John'\n\t  - `{let:[varexpr]:[varname]}`: Store the value found at varexpr in a variable with the name varname\n\t    which will be available in the rest of the expression. If no varname is provided, the variable\n", "    is named `data`.\n\t    Example: format('{let:data[name]:person}{person}', data={'name': 'John'}) -> 'John'\n\t  - `{expr:[expr]:[varname]}`: Evaluate the math expression expr using numexpr and store the result\n\t    in a variable with the name varname which will be available in the rest of the expression.\n\t    Note that variables used in [expr] must be ints or floats and cannot be nested format expressions\n\t    (eg. a[b] or a.b will not work). Use the int or float expressions to prepare data to use in expressions.\n\t    If no varname is provided, the variable is named `data`.\n\t    Example: format('{expr:a*b:result}{result}', a=2, b=3) -> '6'\n\t  - `{int:[varexpr]:[varname]}`: Parse varexpr as an integer in a variable with the name varname which\n\t    will be available in the rest of the expression. If no varname is provided, the variable is named `data`.\n", "    Example: format('{int:result[age]:age}{expr:age+4:result}{result}', result={'age': '2.0001'}) -> '6'\n\t  - `{float:[varexpr]:[varname]}`: Parse varexpr as a float in a variable with the name varname which\n\t    will be available in the rest of the expression. If no varname is provided, the variable is named `data`.\n\t    Example: format('{float:result[age]:age}{expr:age+4:result}{result}', result={'age': '2.5'}) -> '6.5'\n\t  \"\"\"\n\t  def format(self, format_string, *args, **kwargs):\n\t    regex = r\"\\{(join|parse_json|let|expr|int|float):([^:\\}]+)(:([^:\\}]+))?\\}(.*)\"\n\t    match = re.fullmatch(regex, format_string, flags=re.DOTALL)\n\t    if not match:      \n\t      return super().format(format_string, *args, **kwargs)\n", "    args = match.groups()\n\t    value = args[1]\n\t    arg = args[3]\n\t    rest = args[4]\n\t    if args[0] == \"join\":\n\t      return self.join(value, arg, rest, args, kwargs)\n\t    elif args[0] == \"parse_json\":\n\t      return self.parse_json(value, arg, rest, args, kwargs)\n\t    elif args[0] == \"let\":\n\t      return self.let(value, arg, rest, args, kwargs)\n", "    elif args[0] == \"expr\":\n\t      return self.expr(value, arg, rest, args, kwargs)\n\t    elif args[0] == \"int\":\n\t      return self.int(value, arg, rest, args, kwargs)\n\t    elif args[0] == \"float\":     \n\t      return self.float(value, arg, rest, args, kwargs)\n\t    return super().format(format_string, *args, **kwargs)\n\t  def join(self, iterator_exp, separator, inner_format, args, kwargs):\n\t    separator = '' if separator is None else separator\n\t    iterator = self.get_field(iterator_exp, args, kwargs)[0]\n", "    return separator.join(self.format(inner_format, *args, **{**kwargs, \"item\": item, \"index\": index})\n\t      for index, item in enumerate(iterator))\n\t  def parse_json(self, json_string_exp, variable_name, rest, args, kwargs):\n\t    variable_name = 'data' if variable_name is None else variable_name\n\t    json_string = self.get_field(json_string_exp, args, kwargs)[0]\n\t    parsed = json.loads(json_string)\n\t    return self.format(rest, *args, **{**kwargs, variable_name: parsed})\n\t  def let(self, value_exp, variable_name, rest, args, kwargs):\n\t    variable_name = 'data' if variable_name is None else variable_name    \n\t    value = self.get_field(value_exp, args, kwargs)[0]\n", "    return self.format(rest, *args, **{**kwargs, variable_name: value})\n\t  def int(self, value_exp, variable_name, rest, args, kwargs):\n\t    variable_name = 'data' if variable_name is None else variable_name    \n\t    value = self.get_field(value_exp, args, kwargs)[0]\n\t    return self.format(rest, *args, **{**kwargs, variable_name: int(value)})\n\t  def float(self, value_exp, variable_name, rest, args, kwargs):\n\t    variable_name = 'data' if variable_name is None else variable_name    \n\t    value = self.get_field(value_exp, args, kwargs)[0]\n\t    return self.format(rest, *args, **{**kwargs, variable_name: float(value)})\n\t  def expr(self, expression, variable_name, rest, args, kwargs):\n", "    variable_name = 'data' if variable_name is None else variable_name\n\t    result = numexpr.evaluate(expression, global_dict={}, local_dict=kwargs)\n\t    return self.format(rest, *args, **{**kwargs, variable_name: result})\n"]}
{"filename": "lib/formatters/tests/test_extended_formatter.py", "chunked_list": ["from formatters.extended_formatter import ExtendedFormatter\n\tdef test_extended_formatter_normal_formatting():  \n\t  formatter = ExtendedFormatter()\n\t  assert formatter.format(\"Price is ${0:.2f} today\", 42) == \"Price is $42.00 today\"\n\t  assert formatter.format(\"Price: ${price:.2f}\", price=42) == \"Price: $42.00\"\n\t  assert formatter.format(\"{name} is {age} years old and likes {animal}\", \n\t    name='John',\n\t    age='32',\n\t    animal='birds') == \"John is 32 years old and likes birds\"\n\t  nested = formatter.format('Complex number {0} has real {0.real:.2f} and imaginary {0.imag:.2f} parts.', 3-5j)\n", "  assert nested == 'Complex number (3-5j) has real 3.00 and imaginary -5.00 parts.'\n\t  assert formatter.format('{:*^30}', 'centered') == '***********centered***********'\n\tdef test_extended_formatter_join():\n\t  data = {'colors': ['red', 'green', 'blue', 'yellow'], 'name': 'John'}\n\t  formatter = ExtendedFormatter()\n\t  assert formatter.format('{join:data[colors]:, }{item}', data=data) == 'red, green, blue, yellow'\n\t  assert formatter.format('{join:data[colors]}{item}', data=data) == 'redgreenblueyellow'\n\t  result = \"John likes red, and John likes green, and John likes blue, and John likes yellow\"\n\t  assert formatter.format('{join:data[colors]:, and }{data[name]} likes {item}', data=data) == result\n\tdef test_extended_formatter_parse_json():\n", "  data = {'json': '{\"a\": [42, 36], \"name\": \"John\"}'}\n\t  formatter = ExtendedFormatter()\n\t  assert formatter.format('{parse_json:data[json]:person}{person[a][0]}', data=data) == '42'\n\t  assert formatter.format('{parse_json:data[json]:person}{person[a][1]}', data=data) == '36'\n\t  result = \"John: 42:36\"\n\t  assert formatter.format('{parse_json:data[json]:person}{person[name]}: {person[a][0]}:{person[a][1]}', data=data) == result\n\tdef test_extended_formatter_let():\n\t  data = {'name': 'John'}\n\t  formatter = ExtendedFormatter()\n\t  assert formatter.format('{let:data[name]:person}{person}', data=data) == 'John'\n", "def test_extended_formatter_expr():\n\t  data = {'a': 42, 'b': 36, 'c': 2}\n\t  formatter = ExtendedFormatter()\n\t  assert formatter.format('{expr:c*(a + b):result}{result}', **data) == '156'\n\t  assert formatter.format('{let:data[a]:a}{let:data[b]:b}{let:data[c]:c}{expr:c*(a + b):result}{result}', data=data) == '156'\n\tdef test_extended_formatter_int():\n\t  data = {'age': '32'}\n\t  formatter = ExtendedFormatter()\n\t  assert formatter.format('{int:data[age]:age}{expr:age+4:x}{x}', data=data) == '36'\n\tdef test_extended_formatter_float():\n", "  data = {'weight': '.493'}\n\t  formatter = ExtendedFormatter()\n\t  assert formatter.format('{float:data[weight]:weight}{expr:10*weight:x}{x:.2f}', data=data) == '4.93'\n\tdef test_extended_formatter_complex_expr():\n\t  data = {\"result\": '{\"colors\": [\"red\", \"green\", \"blue\", \"yellow\"], \"name\": \"John\"}'}\n\t  parse = '{parse_json:data[result]:person}'\n\t  let = '{let:person[name]:name}'\n\t  iterate = '{join:person[colors]:\\n}'\n\t  expr = '{expr:index + 1:i}'\n\t  format_str = '{i}. {name} likes {item}'\n", "  formatter = ExtendedFormatter()\n\t  result = \"1. John likes red\\n2. John likes green\\n3. John likes blue\\n4. John likes yellow\"\n\t  assert formatter.format(f\"{parse}{let}{iterate}{expr}{format_str}\", data=data) == result\n"]}
{"filename": "lib/formatters/tests/__init__.py", "chunked_list": []}
{"filename": "lib/model/chain_spec.py", "chunked_list": ["from typing import Annotated, Callable, Dict, List, Literal, Optional, Union, Any\n\tfrom pydantic import BaseModel, Field\n\tfrom langchain.chains.base import Chain\n\tfrom langchain.chains import LLMChain, SequentialChain, TransformChain\n\tfrom langchain.prompts import PromptTemplate\n\tfrom lib.model.lang_chain_context import LangChainContext\n\tfrom lib.chains.case_chain import CaseChain\n\tfrom lib.chains.api_chain import APIChain\n\tfrom lib.chains.reformat_chain import ReformatChain\n\tfrom lib.chains.llm_recording_chain import LLMRecordingChain\n", "from lib.chains.vector_search_chain import VectorSearchChain\n\tChainSpec = Annotated[Union[\n\t  \"APIChainSpec\",\n\t  \"SequentialChainSpec\",\n\t  \"LLMChainSpec\",\n\t  \"CaseChainSpec\",\n\t  \"ReformatChainSpec\",\n\t  \"TransformChainSpec\",\n\t  \"VectorSearchChainSpec\"\n\t  ], Field(discriminator='chain_type')]\n", "class BaseChainSpec(BaseModel):\n\t  chain_id: int\n\t  @property\n\t  def children(self) -> List[ChainSpec]:\n\t    return []\n\t  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n\t    raise NotImplementedError\n\t  def traverse(self, fn: Callable[[ChainSpec], None]) -> None:\n\t    fn(self)\n\t    for child in self.children:\n", "      child.traverse(fn)\n\t  def find_by_chain_id(self, chain_id: int) -> Optional[ChainSpec]:\n\t    if self.chain_id == chain_id:\n\t      return self\n\t    for child in self.children:\n\t      result = child.find_by_chain_id(chain_id)\n\t      if result is not None:\n\t        return result\n\t    return None\n\t  def copy_replace(self, replace: Callable[[ChainSpec], ChainSpec]):\n", "    return replace(self).copy(deep=True)\n\tclass LLMChainSpec(BaseChainSpec):\n\t  input_keys: List[str]\n\t  output_key: str\n\t  chain_type: Literal[\"llm_chain_spec\"] = \"llm_chain_spec\"\n\t  prompt: str\n\t  llm_key: str\n\t  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n\t    llm = ctx.llms.get(self.llm_key)\n\t    if llm is None:\n", "      raise ValueError(f\"LLM with key {self.llm_key} not found in context\")\n\t    promptTemplate = PromptTemplate(template=self.prompt, input_variables=self.input_keys)\n\t    if ctx.recording:\n\t      chain = LLMRecordingChain(llm=llm, prompt=promptTemplate, output_key=self.output_key, chain_spec_id=self.chain_id)\n\t      ctx.prompts.append(chain)\n\t      return chain\n\t    return LLMChain(llm=llm, prompt=promptTemplate, output_key=self.output_key)\n\tclass SequentialChainSpec(BaseChainSpec):\n\t  input_keys: List[str]\n\t  output_keys: List[str]\n", "  chain_type: Literal[\"sequential_chain_spec\"] = \"sequential_chain_spec\"\n\t  chains: List[ChainSpec]\n\t  @property\n\t  def children(self) -> List[ChainSpec]:\n\t    return list(self.chains)\n\t  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n\t    chains = [chain.to_lang_chain(ctx) for chain in self.chains]\n\t    return SequentialChain(chains=chains, input_variables=self.input_keys, output_variables=self.output_keys)\n\t  def copy_replace(self, replace: Callable[[ChainSpec], ChainSpec]):\n\t    sequential = replace(self).copy(deep=True, exclude={\"chains\"})\n", "    sequential.chains = [chain.copy_replace(replace) for chain in self.chains]\n\t    return sequential\n\tclass CaseChainSpec(BaseChainSpec):\n\t  chain_type: Literal[\"case_chain_spec\"] = \"case_chain_spec\"\n\t  cases: Dict[str, ChainSpec]\n\t  categorization_key: str\n\t  default_case: ChainSpec\n\t  @property\n\t  def children(self) -> List[ChainSpec]:\n\t    return list(self.cases.values()) + [self.default_case]\n", "  def to_lang_chain(self, ctx: LangChainContext) -> CaseChain:\n\t    subchains = {key: chain.to_lang_chain(ctx) for key, chain in self.cases.items()}\n\t    case_chain = CaseChain(\n\t      subchains=subchains, \n\t      categorization_input=self.categorization_key,\n\t      default_chain=self.default_case.to_lang_chain(ctx),\n\t    )\n\t    return case_chain\n\t  def copy_replace(self, replace: Callable[[ChainSpec], ChainSpec]):\n\t    case_chain = replace(self).copy(deep=True, exclude={\"cases\"})\n", "    case_chain.cases = {key: chain.copy_replace(replace) for key, chain in self.cases.items()}\n\t    return case_chain\n\tclass ReformatChainSpec(BaseChainSpec):\n\t  chain_type: Literal[\"reformat_chain_spec\"] = \"reformat_chain_spec\"\n\t  formatters: Dict[str, str]\n\t  input_keys: List[str]\n\t  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n\t    return ReformatChain(formatters=self.formatters, input_variables=self.input_keys)\n\tclass TransformChainSpec(BaseChainSpec):\n\t  chain_type: Literal[\"transform_chain_spec\"] = \"transform_chain_spec\"\n", "  transform_func: str\n\t  input_keys: List[str]\n\t  output_keys: List[str]\n\t  def create_function(self, body):\n\t    scope = {}\n\t    indented = body.replace(\"\\n\", \"\\n  \")\n\t    code = f\"def f(inputs: dict):\\n  {indented}\"\n\t    exec(code , scope)\n\t    return scope[\"f\"]\n\t  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n", "    return TransformChain(input_variables=self.input_keys, output_variables=self.output_keys, transform=self.create_function(self.transform_func))\n\tclass APIChainSpec(BaseChainSpec):\n\t  chain_type: Literal[\"api_chain_spec\"] = \"api_chain_spec\"\n\t  url: str\n\t  method: str\n\t  headers: Optional[Dict[str, str]]\n\t  body: Optional[str]\n\t  input_keys: List[str]\n\t  output_key: str\n\t  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n", "    return APIChain(\n\t      url=self.url,\n\t      method=self.method,\n\t      headers=self.headers,\n\t      body=self.body,\n\t      input_variables=self.input_keys,\n\t      output_variable=self.output_key,\n\t    )\n\tclass VectorSearchChainSpec(BaseChainSpec):\n\t  chain_type: Literal[\"vector_search_chain_spec\"] = \"vector_search_chain_spec\"\n", "  query: str\n\t  embedding_engine: str\n\t  embedding_params: Dict[str, Any]\n\t  database: str\n\t  database_params: Dict[str, Any]\n\t  num_results: int\n\t  min_score: float = 0.0\n\t  input_variables: List[str] = []\n\t  output_key: str = 'results'\n\t  def to_lang_chain(self, ctx: LangChainContext) -> Chain:\n", "    return VectorSearchChain(\n\t      query=self.query,\n\t      embedding_engine=self.embedding_engine,\n\t      embedding_params=self.embedding_params,\n\t      database=self.database,\n\t      database_params=self.database_params,\n\t      input_variables=self.input_variables,\n\t      num_results=self.num_results,\n\t      output_key=self.output_key,\n\t      min_score=self.min_score,\n", "    )\n\tSequentialChainSpec.update_forward_refs()\n\tCaseChainSpec.update_forward_refs()\n"]}
{"filename": "lib/model/chain.py", "chunked_list": ["from pydantic import BaseModel\n\tfrom pydantic_mongo import AbstractRepository, ObjectIdField\n\tclass Chain(BaseModel):\n\t  id: ObjectIdField = None\n\t  name: str\n\t  revision: ObjectIdField\n\t  class Config:\n\t    json_encoders = {ObjectIdField: str}\n\tclass ChainRepository(AbstractRepository[Chain]):\n\t  class Meta:\n", "    collection_name = 'chains'\n"]}
{"filename": "lib/model/__init__.py", "chunked_list": []}
{"filename": "lib/model/result.py", "chunked_list": ["from typing import Dict\n\tfrom pydantic import BaseModel\n\tfrom pydantic_mongo import AbstractRepository, ObjectIdField\n\tfrom datetime import datetime\n\tclass Result(BaseModel):\n\t  id: ObjectIdField = None\n\t  revision: ObjectIdField\n\t  chain_id: int\n\t  input: Dict[str, str]\n\t  output: str\n", "  recorded: datetime\n\t  class Config:\n\t    json_encoders = {ObjectIdField: str}\n\tclass ResultRepository(AbstractRepository[Result]):\n\t  class Meta:\n\t    collection_name = 'results'\n"]}
{"filename": "lib/model/chain_revision.py", "chunked_list": ["import json\n\tfrom typing import Dict, Optional\n\tfrom pydantic import BaseModel\n\tfrom pydantic_mongo import AbstractRepository, ObjectIdField\n\tfrom lib.model.chain_spec import ChainSpec, APIChainSpec, SequentialChainSpec, LLMChainSpec, CaseChainSpec, ReformatChainSpec, TransformChainSpec, VectorSearchChainSpec\n\tfrom lib.model.llm_spec import LLMSpec, OpenAILLMSpec, HuggingFaceHubLLMSpec, ChatOpenAILLMSpec\n\tdef dump_json(obj: object, **kwargs):\n\t  obj['id'] = str(obj['id']) if obj['id'] is not None else None\n\t  obj['parent'] = str(obj['parent']) if obj['parent'] is not None else None\n\t  return json.dumps(obj, **kwargs)\n", "class ChainRevision(BaseModel):\n\t  id: ObjectIdField = None\n\t  parent: Optional[ObjectIdField]\n\t  chain: ChainSpec\n\t  llms: Dict[str, LLMSpec]\n\t  class Config:\n\t    json_encoders = {\n\t      ObjectIdField: str,\n\t    }\n\t    json_dumps = dump_json\n", "ChainRevision.update_forward_refs()\n\tclass ChainRevisionRepository(AbstractRepository[ChainRevision]):\n\t  class Meta:\n\t    collection_name = 'chain_revisions'\n\tdef find_ancestor_ids(revision_id: ObjectIdField, repository: ChainRevisionRepository) -> list[ObjectIdField]:\n\t  revision = repository.find_one_by_id(revision_id)\n\t  if revision is None:\n\t    raise ValueError(f\"Revision with id {revision_id} not found\")\n\t  if revision.parent is None:\n\t    return []\n", "  return [revision.parent] + find_ancestor_ids(revision.parent, repository)\n"]}
{"filename": "lib/model/llm_spec.py", "chunked_list": ["from typing import Annotated, Callable, Optional, Dict, Literal, Union, TypedDict\n\tfrom pydantic import BaseModel, Field\n\tfrom langchain.llms.base import LLM\n\tfrom langchain.llms.openai import OpenAI\n\tfrom langchain.llms.huggingface_hub import HuggingFaceHub\n\tfrom langchain.chat_models.openai import ChatOpenAI\n\tLLMSpec = Annotated[Union[\n\t  \"OpenAILLMSpec\",\n\t  \"HuggingFaceHubLLMSpec\",\n\t  \"ChatOpenAILLMSpec\",\n", "  ], Field(discriminator='llm_type')]\n\tclass BaseLLMSpec(BaseModel):\n\t  def to_llm(self) -> LLM:\n\t    raise NotImplementedError\n\t  def copy_replace(self, replace: Callable[[LLMSpec], LLMSpec]):\n\t    return replace(self).copy(deep=True)\n\tclass OpenAILLMSpec(BaseLLMSpec):\n\t  llm_type: Literal[\"openai\"] = \"openai\"\n\t  model_name: str\n\t  temperature: float\n", "  max_tokens: int\n\t  top_p: float\n\t  frequency_penalty: float\n\t  presence_penalty: float\n\t  n: int\n\t  request_timeout: Optional[int]\n\t  logit_bias: Optional[Dict[int, int]]\n\t  def to_llm(self) -> LLM:\n\t    return OpenAI(model_name=self.model_name, temperature=self.temperature,\n\t                  max_tokens=self.max_tokens, top_p=self.top_p, frequency_penalty=self.frequency_penalty,\n", "                  presence_penalty=self.presence_penalty, n=self.n,\n\t                  request_timeout=self.request_timeout, logit_bias=self.logit_bias)\n\tclass HuggingFaceHubLLMSpec(BaseLLMSpec):\n\t  class ModelKwargs(TypedDict):\n\t    temperature: float\n\t    max_length: int\n\t  llm_type: Literal[\"huggingface_hub\"] = \"huggingface_hub\"\n\t  repo_id: str\n\t  task: Optional[str]\n\t  model_kwargs: Optional[ModelKwargs]\n", "  def to_llm(self) -> LLM:\n\t    return HuggingFaceHub(model_kwargs=self.model_kwargs, repo_id=self.repo_id, task=self.task)\n\tclass ChatOpenAILLMSpec(BaseLLMSpec):\n\t  llm_type: Literal[\"chat_openai\"] = \"chat_openai\"\n\t  model_name: str\n\t  temperature: float\n\t  max_tokens: int\n\t  n: int\n\t  request_timeout: Optional[int]\n\t  def to_llm(self) -> LLM:\n", "    return ChatOpenAI(model_name=self.model_name, temperature=self.temperature,\n\t                      max_tokens=self.max_tokens, n=self.n, request_timeout=self.request_timeout)\n"]}
{"filename": "lib/model/lang_chain_context.py", "chunked_list": ["from typing import Dict\n\tfrom pydantic import BaseModel\n\tfrom langchain.schema.language_model import BaseLanguageModel\n\tfrom lib.model.result import Result\n\tfrom datetime import datetime\n\t# provides context for generating lang chains including llms and prompts\n\tclass LangChainContext(BaseModel):\n\t  llms: Dict[str, BaseLanguageModel]\n\t  verbose: bool = False\n\t  recording: bool = False\n", "  prompts: Dict[str, object] = []\n\t  def results(self, revision_id: str):\n\t    return [Result(\n\t        revision=revision_id,\n\t        chain_id=chain.chain_spec_id,\n\t        input=call[0],\n\t        output=call[1],\n\t        recorded=datetime.now()\n\t      ) for chain in self.prompts for call in chain.calls]\n\t  def reset_results(self):\n", "    for chain in self.prompts:\n\t      chain.reset()\n"]}
{"filename": "lib/model/tests/test_llm_spec.py", "chunked_list": ["import os\n\tfrom model.llm_spec import OpenAILLMSpec\n\tdef test_llm_spec_serialization():\n\t  llm = OpenAILLMSpec(model_name=\"davinci\", temperature=0.5, max_tokens=10, top_p=1.0, frequency_penalty=0.0,\n\t                      presence_penalty=0.0, n=1, request_timeout=10, logit_bias=None)\n\t  serialized = llm.json()\n\t  deserialized = OpenAILLMSpec.parse_raw(serialized)\n\t  assert deserialized == llm\n"]}
{"filename": "lib/model/tests/test_chain_spec.py", "chunked_list": ["from model.chain_spec import ChainSpec, LLMChainSpec, SequentialChainSpec, CaseChainSpec, APIChainSpec, ReformatChainSpec, TransformChainSpec, VectorSearchChainSpec\n\tfrom model.chain_revision import ChainRevision\n\tfrom model.lang_chain_context import LangChainContext\n\tfrom langchain.llms.fake import FakeListLLM\n\tfrom model.tests.factory import SpecFactoryContext, llm_factory, sequential_factory, case_factory\n\tdef test_llm_chain_spec_serialization():\n\t    llm_chain_spec = LLMChainSpec(\n\t        chain_id=1,\n\t        input_keys=[\"input1\", \"input2\"],\n\t        output_key=\"output1\",\n", "        prompt=\"prompt\",\n\t        llm_key=\"llm_key\",\n\t        chain_type=\"llm_chain_spec\",\n\t    )\n\t    serialized = llm_chain_spec.json()\n\t    assert serialized == '{\"chain_id\": 1, \"input_keys\": [\"input1\", \"input2\"], \"output_key\": \"output1\", \"chain_type\": \"llm_chain_spec\", \"prompt\": \"prompt\", \"llm_key\": \"llm_key\"}'\n\t    revision = ChainRevision(chain=llm_chain_spec, llms={})\n\t    serialized_revision = revision.json()\n\t    deserialized = ChainRevision.parse_raw(serialized_revision).chain\n\t    assert deserialized == llm_chain_spec\n", "def test_llm_chain_spec_to_lang_chain_creates_valid_chain():\n\t    prompt_template = \"the prompt {input1} {input2}\"\n\t    llm_chain_spec = LLMChainSpec(\n\t        chain_id=1,\n\t        input_keys=[\"input1\", \"input2\"],\n\t        output_key=\"output1\",\n\t        prompt=prompt_template,\n\t        llm_key=\"test\",\n\t        chain_type=\"llm_chain_spec\",\n\t    )\n", "    llms = llms={\"test\": FakeListLLM(responses=[\"response1\"])}\n\t    ctx = LangChainContext(llms=llms)\n\t    llm_chain = llm_chain_spec.to_lang_chain(ctx)\n\t    assert llm_chain.input_keys == [\"input1\", \"input2\"]\n\t    assert llm_chain.output_key == \"output1\"\n\t    assert llm_chain.prompt.template == prompt_template\n\t    assert llm_chain.llm == llms[\"test\"]\n\t    output = llm_chain._call({\"input1\": \"input1\", \"input2\": \"input2\"})\n\t    assert output == {\"output1\": \"response1\"}\n\tdef test_llm_chain_spec_to_lang_chain_creates_recording_chain():\n", "    prompt_template = \"the prompt {input1} {input2}\"\n\t    llm_chain_spec = LLMChainSpec(\n\t        chain_id=1,\n\t        input_keys=[\"input1\", \"input2\"],\n\t        output_key=\"output1\",\n\t        prompt=prompt_template,\n\t        llm_key=\"test\",\n\t        chain_type=\"llm_chain_spec\",\n\t    )\n\t    llms = llms={\"test\": FakeListLLM(responses=[\"response1\"])}\n", "    ctx = LangChainContext(llms=llms, recording=True)\n\t    llm_chain = llm_chain_spec.to_lang_chain(ctx)\n\t    assert len(ctx.prompts) == 1\n\t    output = llm_chain._call({\"input1\": \"input1\", \"input2\": \"input2\"})\n\t    assert output == {\"output1\": \"response1\"}\n\t    assert ctx.prompts[0].calls == [\n\t        ({\"input1\": \"input1\", \"input2\": \"input2\"}, \"response1\")\n\t    ]\n\tdef test_sequential_chain_spec_serialization():\n\t    sequential_chain_spec = SequentialChainSpec(\n", "        chain_id=1,\n\t        input_keys=[\"input1\", \"input2\"],\n\t        output_keys=[\"output1\", \"output2\"],\n\t        chain_type=\"sequential_chain_spec\",\n\t        chains=[\n\t            LLMChainSpec(\n\t                chain_id=1,\n\t                input_keys=[\"input1\", \"input2\"],\n\t                output_key=\"output1\",\n\t                prompt=\"prompt\",\n", "                llm_key=\"llm_key\",\n\t                chain_type=\"llm_chain_spec\"\n\t            )\n\t        ],\n\t    )\n\t    serialized = sequential_chain_spec.json()\n\t    deserialized = SequentialChainSpec.parse_raw(serialized)\n\t    assert deserialized == sequential_chain_spec \n\tdef test_sequential_chain_spec_to_lang_chain_creates_valid_chain():\n\t    llm_chain_spec1 = LLMChainSpec(\n", "                chain_id=1,\n\t                input_keys=[\"input1\", \"input2\"],\n\t                output_key=\"llm1_output\",\n\t                prompt=\"the prompt {input1} {input2}\",\n\t                llm_key=\"test\",\n\t                chain_type=\"llm_chain_spec\",\n\t            )\n\t    llm_chain_spec2 = LLMChainSpec(\n\t                chain_id=2,\n\t                input_keys=[\"llm1_output\", \"input3\"],\n", "                output_key=\"llm2_output\",\n\t                prompt=\"the prompt {llm1_output} {input3}\",\n\t                llm_key=\"test\",\n\t                chain_type=\"llm_chain_spec\",\n\t            )\n\t    sequential_chain_spec = SequentialChainSpec(\n\t        chain_id=3,\n\t        input_keys=[\"input1\", \"input2\", \"input3\"],\n\t        output_keys=[\"llm2_output\"],\n\t        chain_type=\"sequential_chain_spec\",\n", "        chains=[llm_chain_spec1, llm_chain_spec2],\n\t    )\n\t    llms = llms={\"test\": FakeListLLM(responses=[\"fake_response1\", \"fake_response2\"])}\n\t    ctx = LangChainContext(llms=llms)\n\t    sequential_chain = sequential_chain_spec.to_lang_chain(ctx)\n\t    assert sequential_chain.input_keys == [\"input1\", \"input2\", \"input3\"]\n\t    assert sequential_chain.output_keys == [\"llm2_output\"]\n\t    assert sequential_chain.chains[0].input_keys == [\"input1\", \"input2\"]\n\t    assert sequential_chain.chains[0].output_keys == [\"llm1_output\"]\n\t    assert sequential_chain.chains[0].prompt.template == llm_chain_spec1.prompt\n", "    assert sequential_chain.chains[0].llm == llms[\"test\"]\n\t    assert sequential_chain.chains[1].input_keys == [\"llm1_output\", \"input3\"]\n\t    assert sequential_chain.chains[1].output_keys == [\"llm2_output\"]\n\t    assert sequential_chain.chains[1].prompt.template == llm_chain_spec2.prompt\n\t    assert sequential_chain.chains[1].llm == llms[\"test\"]\n\t    output = sequential_chain.run({\"input1\": \"input1\", \"input2\": \"input2\", \"input3\": \"input3\"})\n\t    assert output == \"fake_response2\"\n\tdef test_case_chain_spec_serialization():\n\t    case_chain_spec = CaseChainSpec(\n\t        chain_id=3,\n", "        chain_type=\"case_chain_spec\",\n\t        categorization_key=\"categorization_input\",\n\t        cases={\n\t            \"response1\": LLMChainSpec(\n\t                chain_id=1,\n\t                input_keys=[\"input1\", \"input2\"],\n\t                output_key=\"output\",\n\t                prompt=\"prompt\",\n\t                llm_key=\"test\",\n\t                chain_type=\"llm_chain_spec\"\n", "            ),\n\t            \"response2\": LLMChainSpec(\n\t                chain_id=2,\n\t                input_keys=[\"input3\", \"input4\"],\n\t                output_key=\"output\",\n\t                prompt=\"prompt\",\n\t                llm_key=\"test\",\n\t                chain_type=\"llm_chain_spec\"\n\t            )\n\t        },\n", "        default_case=LLMChainSpec(\n\t            chain_id=4,\n\t            input_keys=[\"input1\", \"input2\"],\n\t            output_key=\"output\",\n\t            prompt=\"prompt\",\n\t            llm_key=\"test\",\n\t            chain_type=\"llm_chain_spec\"\n\t        ),\n\t    )\n\t    serialized = case_chain_spec.json()\n", "    deserialized = CaseChainSpec.parse_raw(serialized)\n\t    assert deserialized == case_chain_spec\n\tdef test_case_chain_spec_to_lang_chain_creates_valid_chain():\n\t    llms = {\"test1\": FakeListLLM(responses=[\"fake_response1\"]), \"test2\": FakeListLLM(responses=[\"fake_response2\"])}\n\t    llm_chain_spec1 = LLMChainSpec(\n\t                chain_id=1,\n\t                input_keys=[\"input1\", \"input2\"],\n\t                output_key=\"output\",\n\t                prompt=\"ll1 prompt {input1} {input2}\",\n\t                llm_key=\"test1\",\n", "                chain_type=\"llm_chain_spec\",\n\t            )\n\t    llm_chain_spec2 = LLMChainSpec(\n\t                chain_id=2,\n\t                input_keys=[\"input3\", \"input4\"],\n\t                output_key=\"output\",\n\t                prompt=\"llm2 prompt {input3} {input4}\",\n\t                llm_key=\"test2\",\n\t                chain_type=\"llm_chain_spec\",\n\t            )\n", "    llm_chain_spec3 = LLMChainSpec(\n\t                chain_id=4,\n\t                input_keys=[\"input1\", \"input2\"],\n\t                output_key=\"output\",\n\t                prompt=\"default prompt {input1} {input2}\",\n\t                llm_key=\"test1\",\n\t                chain_type=\"llm_chain_spec\"\n\t            )\n\t    case_chain_spec = CaseChainSpec(\n\t        chain_id=3,\n", "        chain_type=\"case_chain_spec\",  \n\t        categorization_key=\"categorization_input\",      \n\t        cases={\n\t            \"response1\": llm_chain_spec1,\n\t            \"response2\": llm_chain_spec2,\n\t        },\n\t        default_case=llm_chain_spec3,\n\t    )\n\t    ctx = LangChainContext(llms=llms)\n\t    case_chain = case_chain_spec.to_lang_chain(ctx)\n", "    assert case_chain.input_keys == [\"categorization_input\", \"input1\", \"input2\", \"input3\", \"input4\"]\n\t    assert case_chain.output_keys == [\"output\"]\n\t    assert case_chain.subchains[\"response1\"].input_keys == [\"input1\", \"input2\"]\n\t    assert case_chain.subchains[\"response1\"].output_keys == [\"output\"]\n\t    assert case_chain.subchains[\"response1\"].prompt.template == llm_chain_spec1.prompt\n\t    assert case_chain.subchains[\"response1\"].llm == llms[\"test1\"]\n\t    assert case_chain.subchains[\"response2\"].input_keys == [\"input3\", \"input4\"]\n\t    assert case_chain.subchains[\"response2\"].output_keys == [\"output\"]\n\t    assert case_chain.subchains[\"response2\"].prompt.template == llm_chain_spec2.prompt\n\t    assert case_chain.subchains[\"response2\"].llm == llms[\"test2\"]\n", "    assert case_chain.default_chain.input_keys == [\"input1\", \"input2\"]\n\t    assert case_chain.default_chain.output_keys == [\"output\"]\n\t    assert case_chain.default_chain.prompt.template == llm_chain_spec3.prompt\n\t    assert case_chain.default_chain.llm == llms[\"test1\"]\n\t    output = case_chain.run({\n\t        \"categorization_input\": \"response2\",\n\t        \"input1\": \"input1\", \n\t        \"input2\": \"input2\",\n\t        \"input3\": \"input3\",\n\t        \"input4\": \"input4\"\n", "    })\n\t    assert output == \"fake_response2\"\n\tdef test_api_chain_spec_serialization():\n\t    api_chain_spec = APIChainSpec(\n\t        chain_id=3,\n\t        chain_type=\"api_chain_spec\",\n\t        input_keys=[\"input1\", \"input2\"],\n\t        output_key=\"output\",\n\t        url=\"http://test.com\",\n\t        method=\"POST\",\n", "        body=\"body {input1} {input2}\",\n\t        headers={\"header1\": \"header1\"},\n\t    )\n\t    revision = ChainRevision(chain=api_chain_spec, llms={})\n\t    serialized_revision = revision.json()\n\t    deserialized = ChainRevision.parse_raw(serialized_revision).chain\n\t    assert deserialized == api_chain_spec\n\tdef test_api_chain_spec_to_lang_chain_creates_valid_chain():\n\t    api_chain_spec = APIChainSpec(\n\t        chain_id=3,\n", "        chain_type=\"api_chain_spec\",\n\t        input_keys=[\"input1\", \"input2\"],\n\t        output_key=\"output\",\n\t        url=\"http://test.com\",\n\t        method=\"POST\",\n\t        body=\"body {input1} {input2}\",\n\t        headers={\"header1\": \"header1\"},\n\t    )\n\t    ctx = LangChainContext(llms={})\n\t    api_chain = api_chain_spec.to_lang_chain(ctx)\n", "    assert api_chain.input_keys == [\"input1\", \"input2\"]\n\t    assert api_chain.output_keys == [\"output\"]\n\t    assert api_chain.url == \"http://test.com\"\n\t    assert api_chain.method == \"POST\"\n\t    assert api_chain.body == \"body {input1} {input2}\"\n\t    assert api_chain.headers == {\"header1\": \"header1\"}\n\tdef test_reformat_chain_spec_serialization():\n\t    reformat_chain_spec = ReformatChainSpec(\n\t        chain_id=3,\n\t        chain_type=\"reformat_chain_spec\",\n", "        input_keys=[\"input1\", \"input2\"],\n\t        formatters={\"output1\": \"formatter1\", \"output2\": \"formatter2\"},\n\t    )\n\t    serialized = reformat_chain_spec.json()\n\t    deserialized = ReformatChainSpec.parse_raw(serialized)\n\t    assert deserialized == reformat_chain_spec\n\tdef test_reformat_chain_spec_to_lang_chain_creates_valid_chain():\n\t    reformat_chain_spec = ReformatChainSpec(\n\t        chain_id=3,\n\t        chain_type=\"reformat_chain_spec\",\n", "        input_keys=[\"input1\", \"input2\"],\n\t        formatters={\"output1\": \"formatter1\", \"output2\": \"formatter2\"},\n\t    )\n\t    ctx = LangChainContext(llms={})\n\t    reformat_chain = reformat_chain_spec.to_lang_chain(ctx)\n\t    assert reformat_chain.input_keys == [\"input1\", \"input2\"]\n\t    assert reformat_chain.output_keys == [\"output1\", \"output2\"]\n\t    assert reformat_chain.formatters == {\"output1\": \"formatter1\", \"output2\": \"formatter2\"}\n\tdef test_transform_chain_spec_to_lang_chain_creates_valid_chain():\n\t    transform_chain_spec = TransformChainSpec(\n", "        chain_id=4,\n\t        chain_type=\"transform_chain_spec\",\n\t        input_keys=[\"x\", \"y\"],\n\t        output_keys=[\"z\", \"w\"],\n\t        transform_func=\"\"\"\n\tz = int(inputs['x']) + int(inputs['y'])\n\tw = int(inputs['x']) - int(inputs['y'])\n\treturn {'z': z, 'w': w}\"\"\"\n\t    )\n\t    ctx = LangChainContext(llms={})\n", "    transform_chain = transform_chain_spec.to_lang_chain(ctx)\n\t    assert transform_chain.input_keys == [\"x\", \"y\"]\n\t    assert transform_chain.output_keys == [\"z\", \"w\"]\n\t    assert transform_chain.transform({'x': 4, 'y': 3}) == {'z': 7, 'w': 1}\n\tdef test_vector_search_chain_spec_serialization():\n\t    vector_search_chain_spec = VectorSearchChainSpec(\n\t        chain_id=3,\n\t        chain_type=\"vector_search_chain_spec\",\n\t        input_keys=[\"input1\", \"input2\"],\n\t        output_key=\"output\",\n", "        embedding_engine=\"openai\",\n\t        embedding_params={\"openai_api_key\": \"test\"},\n\t        database=\"pinecone\",\n\t        database_params={\"index\": \"test\", \"text_key\": \"text\"},\n\t        num_results=10,\n\t        query=\"How does {input}?\"\n\t    )\n\t    serialized = vector_search_chain_spec.json()\n\t    deserialized = VectorSearchChainSpec.parse_raw(serialized)\n\t    assert deserialized == vector_search_chain_spec\n", "class ChainDict:\n\t    def __init__(self):\n\t        self.chains = {}\n\t    def add_chain(self, chain):\n\t        self.chains[chain.chain_id] = chain\n\tdef test_chain_spec_copy_replace():\n\t    ctx = SpecFactoryContext()\n\t    chain = sequential_factory(ctx, chains=[\n\t        llm_factory(ctx),\n\t        case_factory(ctx, cases={\n", "            \"case1\": llm_factory(ctx),\n\t            \"case2\": sequential_factory(ctx, chains=[llm_factory(ctx), llm_factory(ctx)]),\n\t        }, default_case=llm_factory(ctx)),\n\t    ])\n\t    original_specs = ChainDict()\n\t    chain.traverse(original_specs.add_chain)\n\t    copied_specs = {}\n\t    copied_chain = chain.copy_replace(lambda spec: spec)\n\t    copied_specs = ChainDict()\n\t    copied_chain.traverse(copied_specs.add_chain)\n", "    assert len(original_specs.chains) == len(copied_specs.chains)\n\t    for chain_id, spec in original_specs.chains.items():\n\t        copied_spec = copied_specs.chains.get(chain_id)\n\t        assert copied_spec is not None\n\t        assert copied_spec == spec\n\t        assert copied_spec is not spec\n\t    replacement = copied_specs.chains[3]\n\t    replacement.prompt = \"replaced\"\n\t    replace_chain = chain.copy_replace(lambda spec: spec if spec.chain_id != 3 else replacement)\n\t    replace_specs = ChainDict()\n", "    replace_chain.traverse(replace_specs.add_chain)\n\t    assert len(original_specs.chains) == len(replace_specs.chains)\n\t    assert replace_specs.chains[3] == replacement\n\t    assert replace_specs.chains[3].prompt == \"replaced\"\n\tdef test_base_spec_find_by_chain_id():\n\t    ctx = SpecFactoryContext()\n\t    deep_llm = llm_factory(ctx)\n\t    chain = sequential_factory(ctx, chains=[\n\t        llm_factory(ctx),\n\t        case_factory(ctx, cases={\n", "            \"case1\": llm_factory(ctx),\n\t            \"case2\": sequential_factory(ctx, chains=[llm_factory(ctx), deep_llm]),\n\t        }, default_case=llm_factory(ctx)),\n\t    ])\n\t    assert chain.find_by_chain_id(deep_llm.chain_id) == deep_llm"]}
{"filename": "lib/model/tests/__init__.py", "chunked_list": []}
{"filename": "lib/model/tests/test_chain_revision.py", "chunked_list": ["import os\n\tfrom model.chain_revision import ChainRevision\n\tfrom model.chain_spec import LLMChainSpec\n\tfrom model.llm_spec import OpenAILLMSpec\n\tdef test_chain_revision_serialization():\n\t  if os.environ.get(\"OPENAI_API_KEY\") is None:\n\t    os.environ[\"OPENAI_API_KEY\"] = \"set me!\"\n\t  llm = OpenAILLMSpec(model_name=\"davinci\", temperature=0.5, max_tokens=10, top_p=1.0, frequency_penalty=0.0,\n\t                      presence_penalty=0.0, n=1, request_timeout=10, logit_bias=None)\n\t  chain_revision = ChainRevision(\n", "      chain_id=1,\n\t      chain=LLMChainSpec(\n\t          chain_id=1,\n\t          input_keys=[\"input1\", \"input2\"],\n\t          output_key=\"output1\",\n\t          prompt=\"prompt\",\n\t          llm_key=\"llm_key\",\n\t          chain_type=\"llm_chain_spec\",\n\t      ),\n\t      llms={\"llm\": llm},\n", "  )\n\t  serialized = chain_revision.json()\n\t  deserialized = ChainRevision.parse_raw(serialized)\n\t  assert deserialized == chain_revision\n"]}
{"filename": "lib/model/tests/factory.py", "chunked_list": ["from model.chain_spec import LLMChainSpec, SequentialChainSpec, CaseChainSpec, APIChainSpec, ReformatChainSpec\n\tclass SpecFactoryContext:\n\t  def __init__(self):\n\t    self._next_id = 0\n\t  def next_id(self):\n\t    self._next_id += 1\n\t    return self._next_id\n\tdef llm_factory(ctx, **kwargs) -> LLMChainSpec:\n\t  return LLMChainSpec(\n\t    chain_id=ctx.next_id(),\n", "    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n\t    output_key=kwargs.get(\"output_key\", \"output1\"),\n\t    prompt=kwargs.get(\"prompt\", \"prompt {input1} {input2}\"),\n\t    llm_key=kwargs.get(\"llm_key\", \"llm_key\"),\n\t    chain_type=\"llm_chain_spec\",\n\t  )\n\tdef sequential_factory(ctx, **kwargs) -> SequentialChainSpec:\n\t  return SequentialChainSpec(\n\t    chain_id=ctx.next_id(),\n\t    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n", "    output_keys=kwargs.get(\"output_keys\", [\"output1\", \"output2\"]),\n\t    chain_type=\"sequential_chain_spec\",\n\t    chains=kwargs.get(\"chains\", []),\n\t  )\n\tdef case_factory(ctx, **kwargs) -> CaseChainSpec:\n\t  return CaseChainSpec(\n\t    chain_id=ctx.next_id(),\n\t    chain_type=\"case_chain_spec\",\n\t    categorization_key=kwargs.get(\"categorization_key\", \"categorization_input\"),\n\t    cases=kwargs.get(\"cases\", {}),\n", "    default_case=kwargs.get(\"default_case\", None),\n\t  )\n\tdef api_factory(ctx, **kwargs) -> APIChainSpec:\n\t  return APIChainSpec(\n\t    chain_id=ctx.next_id(),\n\t    chain_type=\"api_chain_spec\",\n\t    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n\t    output_key=kwargs.get(\"output_key\", \"output1\"),\n\t    url=kwargs.get(\"url\", \"http://test.com\"),\n\t    method=kwargs.get(\"method\", \"POST\"),\n", "    body=kwargs.get(\"body\", \"body {input1} {input2}\"),\n\t    headers=kwargs.get(\"headers\", {\"header1\": \"header1\"}),\n\t  )\n\tdef reformat_factory(ctx, **kwargs) -> ReformatChainSpec:\n\t  return ReformatChainSpec(\n\t    chain_id=ctx.next_id(),\n\t    chain_type=\"reformat_chain_spec\",\n\t    input_keys=kwargs.get(\"input_keys\", [\"input1\", \"input2\"]),\n\t    formatters=kwargs.get(\"formatters\", {\"output1\": \"formatter1\", \"output2\": \"formatter2\"}),\n\t  )\n"]}
{"filename": "server/__init__.py", "chunked_list": []}
{"filename": "server/testbench.py", "chunked_list": ["import logging\n\timport json\n\tfrom flask import Flask, Response, request\n\tfrom bson.json_util import dumps\n\tfrom flask_cors import CORS\n\tfrom lib.model.chain_revision import ChainRevision\n\tfrom werkzeug.exceptions import BadRequest\n\tfrom pydantic.json import pydantic_encoder\n\tfrom pydantic import parse_obj_as\n\tfrom typing import List\n", "import lib.chain_service as chain_service\n\tlogging.basicConfig(level=logging.INFO)\n\tapp = Flask(__name__)\n\tCORS(app)\n\t@app.route(\"/\")\n\tdef check():\n\t  return Response('\"ok\"', mimetype=\"application/json\")\n\t# Create a REST API for the chain service\n\t@app.route(\"/chains\", methods=[\"GET\"])\n\tdef list_chains():\n", "  chains = chain_service.list_chains()\n\t  return Response(dumps(chains), mimetype=\"application/json\")\n\t@app.route(\"/chain/<chain_name>/revision\", methods=[\"POST\"])\n\tdef save_revision(chain_name):\n\t  try:\n\t    next_revision = ChainRevision.parse_raw(request.data)\n\t  except Exception as e:\n\t    print(\"ERROR parsing revision:\", e)\n\t    return {\"error\": str(e)}, 400\n\t  revision_id = chain_service.save_revision(chain_name, next_revision)\n", "  return Response(dumps({\"revision_id\": str(revision_id)}), mimetype=\"application/json\")\n\t@app.route(\"/chain/<chain_name>/revision\", methods=[\"GET\"])\n\tdef load_by_chain_name(chain_name):\n\t  revision = chain_service.load_by_chain_name(chain_name)\n\t  return Response(revision.json(), mimetype=\"application/json\")\n\t@app.route(\"/chain/<chain_name>/history\", methods=[\"GET\"])\n\tdef history_by_chain_name(chain_name):\n\t  history = chain_service.history_by_chain_name(chain_name)\n\t  return Response(dumps(history), mimetype=\"application/json\")\n\t@app.route(\"/chain/<chain_name>/patch\", methods=[\"POST\"])\n", "def save_patch(chain_name):\n\t  revision_id = chain_service.save_patch(chain_name, request.json)\n\t  return Response(dumps({\"revision_id\": str(revision_id)}), mimetype=\"application/json\")\n\t@app.route(\"/chain/<revision_id>\", methods=[\"GET\"])\n\tdef load_by_id(revision_id):\n\t  revision_id = chain_service.load_by_id(revision_id)\n\t  return Response(dumps({\"revision_id\": str(revision_id)}), mimetype=\"application/json\")\n\t@app.route(\"/chain/<chain_name>/results\", methods=[\"GET\"])\n\tdef load_results_by_chain_name(chain_name):\n\t  results = chain_service.load_results_by_chain_name(chain_name)\n", "  return Response(dumps(results), mimetype=\"application/json\")\n\t@app.route(\"/chain/<chain_name>/run\", methods=[\"POST\"])\n\tdef run_chain(chain_name):\n\t  input = request.json\n\t  result = chain_service.run_once(chain_name, input, False)\n\t  return Response(dumps(result), mimetype=\"application/json\")\n\tif __name__ == \"__main__\":\n\t  app.run(debug=True)\n\t@app.route(\"/chain/<chain_name>/export\", methods=[\"GET\"])\n\tdef export_chain(chain_name):\n", "    exported_chain = chain_service.export_chain(chain_name)\n\t    if not exported_chain:\n\t      return Response(\n\t          dumps({\"error\": f\"Chain '{chain_name}' not found.\"}),\n\t          mimetype=\"application/json\",\n\t          status=404\n\t      )\n\t    chain_json = '[' + ','.join(revision.json() for revision in exported_chain) + ']'\n\t    return Response(\n\t        chain_json,\n", "        mimetype=\"application/json\"\n\t    )\n\t@app.route(\"/chain/<chain_name>/import\", methods=[\"POST\"])\n\tdef import_chain_route(chain_name):\n\t    logging.info(f\"Importing chain '{chain_name}'\")\n\t    try:\n\t        json_data = request.get_json()\n\t        if json_data is None:\n\t            raise BadRequest(\"JSON data not present in request\")\n\t        revisions = parse_obj_as(List[ChainRevision], json_data) \n", "        chain_service.import_chain(chain_name, revisions)\n\t        return Response(\n\t            dumps({\"success\": f\"Import of '{chain_name}' successful.\"}),\n\t            mimetype=\"application/json\",\n\t            status=200\n\t        )\n\t    except Exception as e:\n\t        logging.error(f\"Import of '{chain_name}' failed. Reason: {str(e)}\")\n\t        return Response(\n\t            dumps({\"error\": f\"Import of '{chain_name}' failed. Reason: {str(e)}\"}),\n", "            mimetype=\"application/json\",\n\t            status=400\n\t        )\n\tdef allowed_file(filename):\n\t    ALLOWED_EXTENSIONS = {'json'}\n\t    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS"]}
