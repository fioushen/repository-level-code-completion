{"filename": "engine_for_finetuning.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport math\n\timport os\n\timport sys\n", "from multiprocessing import Pool\n\tfrom typing import Iterable, Optional\n\timport numpy as np\n\timport torch\n\tfrom scipy.special import softmax\n\tfrom timm.data import Mixup\n\tfrom timm.utils import ModelEma, accuracy\n\timport utils\n\tdef train_class_batch(model, samples, target, criterion):\n\t    outputs = model(samples)\n", "    loss = criterion(outputs, target)\n\t    return loss, outputs\n\tdef get_loss_scale_for_deepspeed(model):\n\t    optimizer = model.optimizer\n\t    return optimizer.loss_scale if hasattr(\n\t        optimizer, \"loss_scale\") else optimizer.cur_scale\n\tdef train_one_epoch(model: torch.nn.Module,\n\t                    criterion: torch.nn.Module,\n\t                    data_loader: Iterable,\n\t                    optimizer: torch.optim.Optimizer,\n", "                    device: torch.device,\n\t                    epoch: int,\n\t                    loss_scaler,\n\t                    max_norm: float = 0,\n\t                    model_ema: Optional[ModelEma] = None,\n\t                    mixup_fn: Optional[Mixup] = None,\n\t                    log_writer=None,\n\t                    start_steps=None,\n\t                    lr_schedule_values=None,\n\t                    wd_schedule_values=None,\n", "                    num_training_steps_per_epoch=None,\n\t                    update_freq=None):\n\t    model.train(True)\n\t    metric_logger = utils.MetricLogger(delimiter=\"  \")\n\t    metric_logger.add_meter(\n\t        'lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n\t    metric_logger.add_meter(\n\t        'min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n\t    header = 'Epoch: [{}]'.format(epoch)\n\t    print_freq = 20\n", "    if loss_scaler is None:\n\t        model.zero_grad()\n\t        model.micro_steps = 0\n\t    else:\n\t        optimizer.zero_grad()\n\t    for data_iter_step, (samples, targets, _, _) in enumerate(\n\t            metric_logger.log_every(data_loader, print_freq, header)):\n\t        step = data_iter_step // update_freq\n\t        if step >= num_training_steps_per_epoch:\n\t            continue\n", "        it = start_steps + step  # global training iteration\n\t        # Update LR & WD for the first acc\n\t        if lr_schedule_values is not None or wd_schedule_values is not None and data_iter_step % update_freq == 0:\n\t            for i, param_group in enumerate(optimizer.param_groups):\n\t                if lr_schedule_values is not None:\n\t                    param_group[\"lr\"] = lr_schedule_values[it] * param_group[\n\t                        \"lr_scale\"]\n\t                if wd_schedule_values is not None and param_group[\n\t                        \"weight_decay\"] > 0:\n\t                    param_group[\"weight_decay\"] = wd_schedule_values[it]\n", "        samples = samples.to(device, non_blocking=True)\n\t        targets = targets.to(device, non_blocking=True)\n\t        if mixup_fn is not None:\n\t            # mixup handle 3th & 4th dimension\n\t            B, C, T, H, W = samples.shape\n\t            samples = samples.view(B, C * T, H, W)\n\t            samples, targets = mixup_fn(samples, targets)\n\t            samples = samples.view(B, C, T, H, W)\n\t        if loss_scaler is None:\n\t            samples = samples.half()\n", "            loss, output = train_class_batch(model, samples, targets,\n\t                                             criterion)\n\t        else:\n\t            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n\t                loss, output = train_class_batch(model, samples, targets,\n\t                                                 criterion)\n\t        loss_value = loss.item()\n\t        if not math.isfinite(loss_value):\n\t            print(\"Loss is {}, stopping training\".format(loss_value))\n\t            sys.exit(1)\n", "        if loss_scaler is None:\n\t            loss /= update_freq\n\t            model.backward(loss)\n\t            grad_norm = model.get_global_grad_norm()\n\t            model.step()\n\t            if (data_iter_step + 1) % update_freq == 0:\n\t                # Deepspeed will call step() & model.zero_grad() automatic\n\t                if model_ema is not None:\n\t                    model_ema.update(model)\n\t            loss_scale_value = get_loss_scale_for_deepspeed(model)\n", "        else:\n\t            # this attribute is added by timm on one optimizer (adahessian)\n\t            is_second_order = hasattr(\n\t                optimizer, 'is_second_order') and optimizer.is_second_order\n\t            loss /= update_freq\n\t            grad_norm = loss_scaler(\n\t                loss,\n\t                optimizer,\n\t                clip_grad=max_norm,\n\t                parameters=model.parameters(),\n", "                create_graph=is_second_order,\n\t                update_grad=(data_iter_step + 1) % update_freq == 0)\n\t            if (data_iter_step + 1) % update_freq == 0:\n\t                optimizer.zero_grad()\n\t                if model_ema is not None:\n\t                    model_ema.update(model)\n\t            loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n\t        torch.cuda.synchronize()\n\t        if mixup_fn is None:\n\t            class_acc = (output.max(-1)[-1] == targets).float().mean()\n", "        else:\n\t            class_acc = None\n\t        metric_logger.update(loss=loss_value)\n\t        metric_logger.update(class_acc=class_acc)\n\t        metric_logger.update(loss_scale=loss_scale_value)\n\t        min_lr = 10.\n\t        max_lr = 0.\n\t        for group in optimizer.param_groups:\n\t            min_lr = min(min_lr, group[\"lr\"])\n\t            max_lr = max(max_lr, group[\"lr\"])\n", "        metric_logger.update(lr=max_lr)\n\t        metric_logger.update(min_lr=min_lr)\n\t        weight_decay_value = None\n\t        for group in optimizer.param_groups:\n\t            if group[\"weight_decay\"] > 0:\n\t                weight_decay_value = group[\"weight_decay\"]\n\t        metric_logger.update(weight_decay=weight_decay_value)\n\t        metric_logger.update(grad_norm=grad_norm)\n\t        if log_writer is not None:\n\t            log_writer.update(loss=loss_value, head=\"loss\")\n", "            log_writer.update(class_acc=class_acc, head=\"loss\")\n\t            log_writer.update(loss_scale=loss_scale_value, head=\"opt\")\n\t            log_writer.update(lr=max_lr, head=\"opt\")\n\t            log_writer.update(min_lr=min_lr, head=\"opt\")\n\t            log_writer.update(weight_decay=weight_decay_value, head=\"opt\")\n\t            log_writer.update(grad_norm=grad_norm, head=\"opt\")\n\t            log_writer.set_step()\n\t    # gather the stats from all processes\n\t    metric_logger.synchronize_between_processes()\n\t    print(\"Averaged stats:\", metric_logger)\n", "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n\t@torch.no_grad()\n\tdef validation_one_epoch(data_loader, model, device):\n\t    criterion = torch.nn.CrossEntropyLoss()\n\t    metric_logger = utils.MetricLogger(delimiter=\"  \")\n\t    header = 'Val:'\n\t    # switch to evaluation mode\n\t    model.eval()\n\t    for batch in metric_logger.log_every(data_loader, 10, header):\n\t        images = batch[0]\n", "        target = batch[1]\n\t        images = images.to(device, non_blocking=True)\n\t        target = target.to(device, non_blocking=True)\n\t        # compute output\n\t        with torch.cuda.amp.autocast():\n\t            output = model(images)\n\t            loss = criterion(output, target)\n\t        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\t        batch_size = images.shape[0]\n\t        metric_logger.update(loss=loss.item())\n", "        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n\t        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n\t    # gather the stats from all processes\n\t    metric_logger.synchronize_between_processes()\n\t    print(\n\t        '* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n\t        .format(\n\t            top1=metric_logger.acc1,\n\t            top5=metric_logger.acc5,\n\t            losses=metric_logger.loss))\n", "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n\t@torch.no_grad()\n\tdef final_test(data_loader, model, device, file):\n\t    criterion = torch.nn.CrossEntropyLoss()\n\t    metric_logger = utils.MetricLogger(delimiter=\"  \")\n\t    header = 'Test:'\n\t    # switch to evaluation mode\n\t    model.eval()\n\t    final_result = []\n\t    for batch in metric_logger.log_every(data_loader, 10, header):\n", "        images = batch[0]\n\t        target = batch[1]\n\t        ids = batch[2]\n\t        chunk_nb = batch[3]\n\t        split_nb = batch[4]\n\t        images = images.to(device, non_blocking=True)\n\t        target = target.to(device, non_blocking=True)\n\t        # compute output\n\t        with torch.cuda.amp.autocast():\n\t            output = model(images)\n", "            loss = criterion(output, target)\n\t        for i in range(output.size(0)):\n\t            string = \"{} {} {} {} {}\\n\".format(\n\t                ids[i], str(output.data[i].cpu().numpy().tolist()),\n\t                str(int(target[i].cpu().numpy())),\n\t                str(int(chunk_nb[i].cpu().numpy())),\n\t                str(int(split_nb[i].cpu().numpy())))\n\t            final_result.append(string)\n\t        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\t        batch_size = images.shape[0]\n", "        metric_logger.update(loss=loss.item())\n\t        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n\t        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n\t    if not os.path.exists(file):\n\t        os.mknod(file)\n\t    with open(file, 'w') as f:\n\t        f.write(\"{}, {}\\n\".format(acc1, acc5))\n\t        for line in final_result:\n\t            f.write(line)\n\t    # gather the stats from all processes\n", "    metric_logger.synchronize_between_processes()\n\t    print(\n\t        '* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n\t        .format(\n\t            top1=metric_logger.acc1,\n\t            top5=metric_logger.acc5,\n\t            losses=metric_logger.loss))\n\t    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n\tdef merge(eval_path, num_tasks, method='prob'):\n\t    assert method in ['prob', 'score']\n", "    dict_feats = {}\n\t    dict_label = {}\n\t    dict_pos = {}\n\t    print(\"Reading individual output files\")\n\t    for x in range(num_tasks):\n\t        file = os.path.join(eval_path, str(x) + '.txt')\n\t        lines = open(file, 'r').readlines()[1:]\n\t        for line in lines:\n\t            line = line.strip()\n\t            name = line.split('[')[0]\n", "            label = line.split(']')[1].split(' ')[1]\n\t            chunk_nb = line.split(']')[1].split(' ')[2]\n\t            split_nb = line.split(']')[1].split(' ')[3]\n\t            data = np.fromstring(\n\t                line.split('[')[1].split(']')[0], dtype=float, sep=',')\n\t            if name not in dict_feats:\n\t                dict_feats[name] = []\n\t                dict_label[name] = 0\n\t                dict_pos[name] = []\n\t            if chunk_nb + split_nb in dict_pos[name]:\n", "                continue\n\t            if method == 'prob':\n\t                dict_feats[name].append(softmax(data))\n\t            else:\n\t                dict_feats[name].append(data)\n\t            dict_pos[name].append(chunk_nb + split_nb)\n\t            dict_label[name] = label\n\t    print(\"Computing final results\")\n\t    input_lst = []\n\t    for i, item in enumerate(dict_feats):\n", "        input_lst.append([i, item, dict_feats[item], dict_label[item]])\n\t    p = Pool(64)\n\t    # [pred, top1, top5, label]\n\t    ans = p.map(compute_video, input_lst)\n\t    top1 = [x[1] for x in ans]\n\t    top5 = [x[2] for x in ans]\n\t    label = [x[3] for x in ans]\n\t    final_top1, final_top5 = np.mean(top1), np.mean(top5)\n\t    return final_top1 * 100, final_top5 * 100\n\tdef compute_video(lst):\n", "    i, video_id, data, label = lst\n\t    feat = [x for x in data]\n\t    feat = np.mean(feat, axis=0)\n\t    pred = np.argmax(feat)\n\t    top1 = (int(pred) == int(label)) * 1.0\n\t    top5 = (int(label) in np.argsort(-feat)[:5]) * 1.0\n\t    return [pred, top1, top5, int(label)]\n"]}
{"filename": "optim_factory.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport json\n\timport torch\n\tfrom timm.optim.adafactor import Adafactor\n", "from timm.optim.adahessian import Adahessian\n\tfrom timm.optim.adamp import AdamP\n\tfrom timm.optim.lookahead import Lookahead\n\tfrom timm.optim.nadam import Nadam\n\tfrom timm.optim.novograd import NovoGrad\n\tfrom timm.optim.nvnovograd import NvNovoGrad\n\tfrom timm.optim.radam import RAdam\n\tfrom timm.optim.rmsprop_tf import RMSpropTF\n\tfrom timm.optim.sgdp import SGDP\n\tfrom torch import optim as optim\n", "try:\n\t    from apex.optimizers import FusedAdam, FusedLAMB, FusedNovoGrad, FusedSGD\n\t    has_apex = True\n\texcept ImportError:\n\t    has_apex = False\n\tdef get_num_layer_for_vit(var_name, num_max_layer):\n\t    if var_name in (\"cls_token\", \"mask_token\", \"pos_embed\"):\n\t        return 0\n\t    elif var_name.startswith(\"patch_embed\"):\n\t        return 0\n", "    elif var_name.startswith(\"rel_pos_bias\"):\n\t        return num_max_layer - 1\n\t    elif var_name.startswith(\"blocks\"):\n\t        layer_id = int(var_name.split('.')[1])\n\t        return layer_id + 1\n\t    else:\n\t        return num_max_layer - 1\n\tclass LayerDecayValueAssigner(object):\n\t    def __init__(self, values):\n\t        self.values = values\n", "    def get_scale(self, layer_id):\n\t        return self.values[layer_id]\n\t    def get_layer_id(self, var_name):\n\t        return get_num_layer_for_vit(var_name, len(self.values))\n\tdef get_parameter_groups(model,\n\t                         weight_decay=1e-5,\n\t                         skip_list=(),\n\t                         get_num_layer=None,\n\t                         get_layer_scale=None):\n\t    parameter_group_names = {}\n", "    parameter_group_vars = {}\n\t    for name, param in model.named_parameters():\n\t        if not param.requires_grad:\n\t            continue  # frozen weights\n\t        if len(param.shape) == 1 or name.endswith(\".bias\") or name.endswith(\n\t                \".scale\") or name in skip_list:\n\t            group_name = \"no_decay\"\n\t            this_weight_decay = 0.\n\t        else:\n\t            group_name = \"decay\"\n", "            this_weight_decay = weight_decay\n\t        if get_num_layer is not None:\n\t            layer_id = get_num_layer(name)\n\t            group_name = \"layer_%d_%s\" % (layer_id, group_name)\n\t        else:\n\t            layer_id = None\n\t        if group_name not in parameter_group_names:\n\t            if get_layer_scale is not None:\n\t                scale = get_layer_scale(layer_id)\n\t            else:\n", "                scale = 1.\n\t            parameter_group_names[group_name] = {\n\t                \"weight_decay\": this_weight_decay,\n\t                \"params\": [],\n\t                \"lr_scale\": scale\n\t            }\n\t            parameter_group_vars[group_name] = {\n\t                \"weight_decay\": this_weight_decay,\n\t                \"params\": [],\n\t                \"lr_scale\": scale\n", "            }\n\t        parameter_group_vars[group_name][\"params\"].append(param)\n\t        parameter_group_names[group_name][\"params\"].append(name)\n\t    print(\"Param groups = %s\" % json.dumps(parameter_group_names, indent=2))\n\t    return list(parameter_group_vars.values())\n\tdef create_optimizer(args,\n\t                     model,\n\t                     get_num_layer=None,\n\t                     get_layer_scale=None,\n\t                     filter_bias_and_bn=True,\n", "                     skip_list=None):\n\t    opt_lower = args.opt.lower()\n\t    weight_decay = args.weight_decay\n\t    if weight_decay and filter_bias_and_bn:\n\t        skip = {}\n\t        if skip_list is not None:\n\t            skip = skip_list\n\t        elif hasattr(model, 'no_weight_decay'):\n\t            skip = model.no_weight_decay()\n\t        parameters = get_parameter_groups(model, weight_decay, skip,\n", "                                          get_num_layer, get_layer_scale)\n\t        weight_decay = 0.\n\t    else:\n\t        parameters = model.parameters()\n\t    if 'fused' in opt_lower:\n\t        assert has_apex and torch.cuda.is_available(\n\t        ), 'APEX and CUDA required for fused optimizers'\n\t    opt_args = dict(lr=args.lr, weight_decay=weight_decay)\n\t    if hasattr(args, 'opt_eps') and args.opt_eps is not None:\n\t        opt_args['eps'] = args.opt_eps\n", "    if hasattr(args, 'opt_betas') and args.opt_betas is not None:\n\t        opt_args['betas'] = args.opt_betas\n\t    print(\"optimizer settings:\", opt_args)\n\t    opt_split = opt_lower.split('_')\n\t    opt_lower = opt_split[-1]\n\t    if opt_lower == 'sgd' or opt_lower == 'nesterov':\n\t        opt_args.pop('eps', None)\n\t        optimizer = optim.SGD(\n\t            parameters, momentum=args.momentum, nesterov=True, **opt_args)\n\t    elif opt_lower == 'momentum':\n", "        opt_args.pop('eps', None)\n\t        optimizer = optim.SGD(\n\t            parameters, momentum=args.momentum, nesterov=False, **opt_args)\n\t    elif opt_lower == 'adam':\n\t        optimizer = optim.Adam(parameters, **opt_args)\n\t    elif opt_lower == 'adamw':\n\t        optimizer = optim.AdamW(parameters, **opt_args)\n\t    elif opt_lower == 'nadam':\n\t        optimizer = Nadam(parameters, **opt_args)\n\t    elif opt_lower == 'radam':\n", "        optimizer = RAdam(parameters, **opt_args)\n\t    elif opt_lower == 'adamp':\n\t        optimizer = AdamP(parameters, wd_ratio=0.01, nesterov=True, **opt_args)\n\t    elif opt_lower == 'sgdp':\n\t        optimizer = SGDP(\n\t            parameters, momentum=args.momentum, nesterov=True, **opt_args)\n\t    elif opt_lower == 'adadelta':\n\t        optimizer = optim.Adadelta(parameters, **opt_args)\n\t    elif opt_lower == 'adafactor':\n\t        if not args.lr:\n", "            opt_args['lr'] = None\n\t        optimizer = Adafactor(parameters, **opt_args)\n\t    elif opt_lower == 'adahessian':\n\t        optimizer = Adahessian(parameters, **opt_args)\n\t    elif opt_lower == 'rmsprop':\n\t        optimizer = optim.RMSprop(\n\t            parameters, alpha=0.9, momentum=args.momentum, **opt_args)\n\t    elif opt_lower == 'rmsproptf':\n\t        optimizer = RMSpropTF(\n\t            parameters, alpha=0.9, momentum=args.momentum, **opt_args)\n", "    elif opt_lower == 'novograd':\n\t        optimizer = NovoGrad(parameters, **opt_args)\n\t    elif opt_lower == 'nvnovograd':\n\t        optimizer = NvNovoGrad(parameters, **opt_args)\n\t    elif opt_lower == 'fusedsgd':\n\t        opt_args.pop('eps', None)\n\t        optimizer = FusedSGD(\n\t            parameters, momentum=args.momentum, nesterov=True, **opt_args)\n\t    elif opt_lower == 'fusedmomentum':\n\t        opt_args.pop('eps', None)\n", "        optimizer = FusedSGD(\n\t            parameters, momentum=args.momentum, nesterov=False, **opt_args)\n\t    elif opt_lower == 'fusedadam':\n\t        optimizer = FusedAdam(parameters, adam_w_mode=False, **opt_args)\n\t    elif opt_lower == 'fusedadamw':\n\t        optimizer = FusedAdam(parameters, adam_w_mode=True, **opt_args)\n\t    elif opt_lower == 'fusedlamb':\n\t        optimizer = FusedLAMB(parameters, **opt_args)\n\t    elif opt_lower == 'fusednovograd':\n\t        opt_args.setdefault('betas', (0.95, 0.98))\n", "        optimizer = FusedNovoGrad(parameters, **opt_args)\n\t    else:\n\t        assert False and \"Invalid optimizer\"\n\t        raise ValueError\n\t    if len(opt_split) > 1:\n\t        if opt_split[0] == 'lookahead':\n\t            optimizer = Lookahead(optimizer)\n\t    return optimizer\n"]}
{"filename": "run_class_finetuning.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport argparse\n\timport datetime\n\timport json\n", "import os\n\timport random\n\timport time\n\tfrom collections import OrderedDict\n\tfrom functools import partial\n\tfrom pathlib import Path\n\timport deepspeed\n\timport numpy as np\n\timport torch\n\timport torch.backends.cudnn as cudnn\n", "from timm.data.mixup import Mixup\n\tfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n\tfrom timm.models import create_model\n\tfrom timm.utils import ModelEma\n\t# NOTE: Do not comment `import models`, it is used to register models\n\timport models  # noqa: F401\n\timport utils\n\tfrom dataset import build_dataset\n\tfrom engine_for_finetuning import (\n\t    final_test,\n", "    merge,\n\t    train_one_epoch,\n\t    validation_one_epoch,\n\t)\n\tfrom optim_factory import (\n\t    LayerDecayValueAssigner,\n\t    create_optimizer,\n\t    get_parameter_groups,\n\t)\n\tfrom utils import NativeScalerWithGradNormCount as NativeScaler\n", "from utils import multiple_samples_collate\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(\n\t        'VideoMAE fine-tuning and evaluation script for action classification',\n\t        add_help=False)\n\t    parser.add_argument('--batch_size', default=64, type=int)\n\t    parser.add_argument('--epochs', default=30, type=int)\n\t    parser.add_argument('--update_freq', default=1, type=int)\n\t    parser.add_argument('--save_ckpt_freq', default=100, type=int)\n\t    # Model parameters\n", "    parser.add_argument(\n\t        '--model',\n\t        default='vit_base_patch16_224',\n\t        type=str,\n\t        metavar='MODEL',\n\t        help='Name of model to train')\n\t    parser.add_argument('--tubelet_size', type=int, default=2)\n\t    parser.add_argument(\n\t        '--input_size', default=224, type=int, help='images input size')\n\t    parser.add_argument(\n", "        '--with_checkpoint', action='store_true', default=False)\n\t    parser.add_argument(\n\t        '--drop',\n\t        type=float,\n\t        default=0.0,\n\t        metavar='PCT',\n\t        help='Dropout rate (default: 0.)')\n\t    parser.add_argument(\n\t        '--attn_drop_rate',\n\t        type=float,\n", "        default=0.0,\n\t        metavar='PCT',\n\t        help='Attention dropout rate (default: 0.)')\n\t    parser.add_argument(\n\t        '--drop_path',\n\t        type=float,\n\t        default=0.1,\n\t        metavar='PCT',\n\t        help='Drop path rate (default: 0.1)')\n\t    parser.add_argument(\n", "        '--head_drop_rate',\n\t        type=float,\n\t        default=0.0,\n\t        metavar='PCT',\n\t        help='cls head dropout rate (default: 0.)')\n\t    parser.add_argument(\n\t        '--disable_eval_during_finetuning', action='store_true', default=False)\n\t    parser.add_argument('--model_ema', action='store_true', default=False)\n\t    parser.add_argument(\n\t        '--model_ema_decay', type=float, default=0.9999, help='')\n", "    parser.add_argument(\n\t        '--model_ema_force_cpu', action='store_true', default=False, help='')\n\t    # Optimizer parameters\n\t    parser.add_argument(\n\t        '--opt',\n\t        default='adamw',\n\t        type=str,\n\t        metavar='OPTIMIZER',\n\t        help='Optimizer (default: \"adamw\"')\n\t    parser.add_argument(\n", "        '--opt_eps',\n\t        default=1e-8,\n\t        type=float,\n\t        metavar='EPSILON',\n\t        help='Optimizer Epsilon (default: 1e-8)')\n\t    parser.add_argument(\n\t        '--opt_betas',\n\t        default=None,\n\t        type=float,\n\t        nargs='+',\n", "        metavar='BETA',\n\t        help='Optimizer Betas (default: None, use opt default)')\n\t    parser.add_argument(\n\t        '--clip_grad',\n\t        type=float,\n\t        default=None,\n\t        metavar='NORM',\n\t        help='Clip gradient norm (default: None, no clipping)')\n\t    parser.add_argument(\n\t        '--momentum',\n", "        type=float,\n\t        default=0.9,\n\t        metavar='M',\n\t        help='SGD momentum (default: 0.9)')\n\t    parser.add_argument(\n\t        '--weight_decay',\n\t        type=float,\n\t        default=0.05,\n\t        help='weight decay (default: 0.05)')\n\t    parser.add_argument(\n", "        '--weight_decay_end',\n\t        type=float,\n\t        default=None,\n\t        help=\"\"\"Final value of the\n\t        weight decay. We use a cosine schedule for WD and using a larger decay by\n\t        the end of training improves performance for ViTs.\"\"\")\n\t    parser.add_argument(\n\t        '--lr',\n\t        type=float,\n\t        default=1e-3,\n", "        metavar='LR',\n\t        help='learning rate (default: 1e-3)')\n\t    parser.add_argument('--layer_decay', type=float, default=0.75)\n\t    parser.add_argument(\n\t        '--warmup_lr',\n\t        type=float,\n\t        default=1e-8,\n\t        metavar='LR',\n\t        help='warmup learning rate (default: 1e-6)')\n\t    parser.add_argument(\n", "        '--min_lr',\n\t        type=float,\n\t        default=1e-6,\n\t        metavar='LR',\n\t        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n\t    parser.add_argument(\n\t        '--warmup_epochs',\n\t        type=int,\n\t        default=5,\n\t        metavar='N',\n", "        help='epochs to warmup LR, if scheduler supports')\n\t    parser.add_argument(\n\t        '--warmup_steps',\n\t        type=int,\n\t        default=-1,\n\t        metavar='N',\n\t        help='num of steps to warmup LR, will overload warmup_epochs if set > 0'\n\t    )\n\t    # Augmentation parameters\n\t    parser.add_argument(\n", "        '--color_jitter',\n\t        type=float,\n\t        default=0.4,\n\t        metavar='PCT',\n\t        help='Color jitter factor (default: 0.4)')\n\t    parser.add_argument(\n\t        '--num_sample', type=int, default=2, help='Repeated_aug (default: 2)')\n\t    parser.add_argument(\n\t        '--aa',\n\t        type=str,\n", "        default='rand-m7-n4-mstd0.5-inc1',\n\t        metavar='NAME',\n\t        help=\n\t        'Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m7-n4-mstd0.5-inc1)'\n\t    ),\n\t    parser.add_argument(\n\t        '--smoothing',\n\t        type=float,\n\t        default=0.1,\n\t        help='Label smoothing (default: 0.1)')\n", "    parser.add_argument(\n\t        '--train_interpolation',\n\t        type=str,\n\t        default='bicubic',\n\t        help=\n\t        'Training interpolation (random, bilinear, bicubic default: \"bicubic\")'\n\t    )\n\t    # Evaluation parameters\n\t    parser.add_argument('--crop_pct', type=float, default=None)\n\t    parser.add_argument('--short_side_size', type=int, default=224)\n", "    parser.add_argument('--test_num_segment', type=int, default=10)\n\t    parser.add_argument('--test_num_crop', type=int, default=3)\n\t    # * Random Erase params\n\t    parser.add_argument(\n\t        '--reprob',\n\t        type=float,\n\t        default=0.25,\n\t        metavar='PCT',\n\t        help='Random erase prob (default: 0.25)')\n\t    parser.add_argument(\n", "        '--remode',\n\t        type=str,\n\t        default='pixel',\n\t        help='Random erase mode (default: \"pixel\")')\n\t    parser.add_argument(\n\t        '--recount',\n\t        type=int,\n\t        default=1,\n\t        help='Random erase count (default: 1)')\n\t    parser.add_argument(\n", "        '--resplit',\n\t        action='store_true',\n\t        default=False,\n\t        help='Do not random erase first (clean) augmentation split')\n\t    # * Mixup params\n\t    parser.add_argument(\n\t        '--mixup',\n\t        type=float,\n\t        default=0.8,\n\t        help='mixup alpha, mixup enabled if > 0.')\n", "    parser.add_argument(\n\t        '--cutmix',\n\t        type=float,\n\t        default=1.0,\n\t        help='cutmix alpha, cutmix enabled if > 0.')\n\t    parser.add_argument(\n\t        '--cutmix_minmax',\n\t        type=float,\n\t        nargs='+',\n\t        default=None,\n", "        help='cutmix min/max ratio, overrides alpha and enables cutmix if set')\n\t    parser.add_argument(\n\t        '--mixup_prob',\n\t        type=float,\n\t        default=1.0,\n\t        help=\n\t        'Probability of performing mixup or cutmix when either/both is enabled'\n\t    )\n\t    parser.add_argument(\n\t        '--mixup_switch_prob',\n", "        type=float,\n\t        default=0.5,\n\t        help=\n\t        'Probability of switching to cutmix when both mixup and cutmix enabled'\n\t    )\n\t    parser.add_argument(\n\t        '--mixup_mode',\n\t        type=str,\n\t        default='batch',\n\t        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"'\n", "    )\n\t    # * Finetuning params\n\t    parser.add_argument(\n\t        '--finetune', default='', help='finetune from checkpoint')\n\t    parser.add_argument('--model_key', default='model|module', type=str)\n\t    parser.add_argument('--model_prefix', default='', type=str)\n\t    parser.add_argument('--init_scale', default=0.001, type=float)\n\t    parser.add_argument('--use_mean_pooling', action='store_true')\n\t    parser.set_defaults(use_mean_pooling=True)\n\t    parser.add_argument(\n", "        '--use_cls', action='store_false', dest='use_mean_pooling')\n\t    # Dataset parameters\n\t    parser.add_argument(\n\t        '--data_path',\n\t        default='/your/data/path/',\n\t        type=str,\n\t        help='dataset path')\n\t    parser.add_argument(\n\t        '--data_root', default='', type=str, help='dataset path root')\n\t    parser.add_argument(\n", "        '--eval_data_path',\n\t        default=None,\n\t        type=str,\n\t        help='dataset path for evaluation')\n\t    parser.add_argument(\n\t        '--nb_classes',\n\t        default=400,\n\t        type=int,\n\t        help='number of the classification types')\n\t    parser.add_argument(\n", "        '--imagenet_default_mean_and_std', default=True, action='store_true')\n\t    parser.add_argument('--num_segments', type=int, default=1)\n\t    parser.add_argument('--num_frames', type=int, default=16)\n\t    parser.add_argument('--sampling_rate', type=int, default=4)\n\t    parser.add_argument('--sparse_sample', default=False, action='store_true')\n\t    parser.add_argument(\n\t        '--data_set',\n\t        default='Kinetics-400',\n\t        choices=[\n\t            'Kinetics-400', 'Kinetics-600', 'Kinetics-700', 'SSV2', 'UCF101',\n", "            'HMDB51', 'Diving48', 'Kinetics-710', 'MIT'\n\t        ],\n\t        type=str,\n\t        help='dataset')\n\t    parser.add_argument(\n\t        '--fname_tmpl',\n\t        default='img_{:05}.jpg',\n\t        type=str,\n\t        help='filename_tmpl for rawframe dataset')\n\t    parser.add_argument(\n", "        '--start_idx',\n\t        default=1,\n\t        type=int,\n\t        help='start_idx for rwaframe dataset')\n\t    parser.add_argument(\n\t        '--output_dir',\n\t        default='',\n\t        help='path where to save, empty for no saving')\n\t    parser.add_argument(\n\t        '--log_dir', default=None, help='path where to tensorboard log')\n", "    parser.add_argument(\n\t        '--device',\n\t        default='cuda',\n\t        help='device to use for training / testing')\n\t    parser.add_argument('--seed', default=0, type=int)\n\t    parser.add_argument('--resume', default='', help='resume from checkpoint')\n\t    parser.add_argument('--auto_resume', action='store_true')\n\t    parser.add_argument(\n\t        '--no_auto_resume', action='store_false', dest='auto_resume')\n\t    parser.set_defaults(auto_resume=True)\n", "    parser.add_argument('--save_ckpt', action='store_true')\n\t    parser.add_argument(\n\t        '--no_save_ckpt', action='store_false', dest='save_ckpt')\n\t    parser.set_defaults(save_ckpt=True)\n\t    parser.add_argument(\n\t        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')\n\t    parser.add_argument(\n\t        '--eval', action='store_true', help='Perform evaluation only')\n\t    parser.add_argument(\n\t        '--validation', action='store_true', help='Perform validation only')\n", "    parser.add_argument(\n\t        '--dist_eval',\n\t        action='store_true',\n\t        default=False,\n\t        help='Enabling distributed evaluation')\n\t    parser.add_argument('--num_workers', default=10, type=int)\n\t    parser.add_argument(\n\t        '--pin_mem',\n\t        action='store_true',\n\t        help=\n", "        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'\n\t    )\n\t    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n\t    parser.set_defaults(pin_mem=True)\n\t    # distributed training parameters\n\t    parser.add_argument(\n\t        '--world_size',\n\t        default=1,\n\t        type=int,\n\t        help='number of distributed processes')\n", "    parser.add_argument('--local_rank', default=-1, type=int)\n\t    parser.add_argument('--dist_on_itp', action='store_true')\n\t    parser.add_argument(\n\t        '--dist_url',\n\t        default='env://',\n\t        help='url used to set up distributed training')\n\t    parser.add_argument(\n\t        '--enable_deepspeed', action='store_true', default=False)\n\t    known_args, _ = parser.parse_known_args()\n\t    if known_args.enable_deepspeed:\n", "        parser = deepspeed.add_config_arguments(parser)\n\t        ds_init = deepspeed.initialize\n\t    else:\n\t        ds_init = None\n\t    return parser.parse_args(), ds_init\n\tdef main(args, ds_init):\n\t    utils.init_distributed_mode(args)\n\t    if ds_init is not None:\n\t        utils.create_ds_config(args)\n\t    print(args)\n", "    device = torch.device(args.device)\n\t    # fix the seed for reproducibility\n\t    seed = args.seed + utils.get_rank()\n\t    torch.manual_seed(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    cudnn.benchmark = True\n\t    dataset_train, args.nb_classes = build_dataset(\n\t        is_train=True, test_mode=False, args=args)\n\t    if args.disable_eval_during_finetuning:\n", "        dataset_val = None\n\t    else:\n\t        dataset_val, _ = build_dataset(\n\t            is_train=False, test_mode=False, args=args)\n\t    dataset_test, _ = build_dataset(is_train=False, test_mode=True, args=args)\n\t    num_tasks = utils.get_world_size()\n\t    global_rank = utils.get_rank()\n\t    sampler_train = torch.utils.data.DistributedSampler(\n\t        dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True)\n\t    print(\"Sampler_train = %s\" % str(sampler_train))\n", "    if args.dist_eval:\n\t        if len(dataset_val) % num_tasks != 0:\n\t            print(\n\t                'Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n\t                'This will slightly alter validation results as extra duplicate entries are added to achieve '\n\t                'equal num of samples per-process.')\n\t        sampler_val = torch.utils.data.DistributedSampler(\n\t            dataset_val,\n\t            num_replicas=num_tasks,\n\t            rank=global_rank,\n", "            shuffle=False)\n\t        sampler_test = torch.utils.data.DistributedSampler(\n\t            dataset_test,\n\t            num_replicas=num_tasks,\n\t            rank=global_rank,\n\t            shuffle=False)\n\t    else:\n\t        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n\t    if global_rank == 0 and args.log_dir is not None:\n\t        os.makedirs(args.log_dir, exist_ok=True)\n", "        log_writer = utils.TensorboardLogger(log_dir=args.log_dir)\n\t    else:\n\t        log_writer = None\n\t    if args.num_sample > 1:\n\t        collate_func = partial(multiple_samples_collate, fold=False)\n\t    else:\n\t        collate_func = None\n\t    data_loader_train = torch.utils.data.DataLoader(\n\t        dataset_train,\n\t        sampler=sampler_train,\n", "        batch_size=args.batch_size,\n\t        num_workers=args.num_workers,\n\t        pin_memory=args.pin_mem,\n\t        drop_last=True,\n\t        collate_fn=collate_func,\n\t        persistent_workers=True)\n\t    if dataset_val is not None:\n\t        data_loader_val = torch.utils.data.DataLoader(\n\t            dataset_val,\n\t            sampler=sampler_val,\n", "            batch_size=int(1.5 * args.batch_size),\n\t            num_workers=args.num_workers,\n\t            pin_memory=args.pin_mem,\n\t            drop_last=False,\n\t            persistent_workers=True)\n\t    else:\n\t        data_loader_val = None\n\t    if dataset_test is not None:\n\t        data_loader_test = torch.utils.data.DataLoader(\n\t            dataset_test,\n", "            sampler=sampler_test,\n\t            batch_size=args.batch_size,\n\t            num_workers=args.num_workers,\n\t            pin_memory=args.pin_mem,\n\t            drop_last=False,\n\t            persistent_workers=True)\n\t    else:\n\t        data_loader_test = None\n\t    mixup_fn = None\n\t    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n", "    if mixup_active:\n\t        print(\"Mixup is activated!\")\n\t        mixup_fn = Mixup(\n\t            mixup_alpha=args.mixup,\n\t            cutmix_alpha=args.cutmix,\n\t            cutmix_minmax=args.cutmix_minmax,\n\t            prob=args.mixup_prob,\n\t            switch_prob=args.mixup_switch_prob,\n\t            mode=args.mixup_mode,\n\t            label_smoothing=args.smoothing,\n", "            num_classes=args.nb_classes)\n\t    model = create_model(\n\t        args.model,\n\t        img_size=args.input_size,\n\t        pretrained=False,\n\t        num_classes=args.nb_classes,\n\t        all_frames=args.num_frames * args.num_segments,\n\t        tubelet_size=args.tubelet_size,\n\t        drop_rate=args.drop,\n\t        drop_path_rate=args.drop_path,\n", "        attn_drop_rate=args.attn_drop_rate,\n\t        head_drop_rate=args.head_drop_rate,\n\t        drop_block_rate=None,\n\t        use_mean_pooling=args.use_mean_pooling,\n\t        init_scale=args.init_scale,\n\t        with_cp=args.with_checkpoint,\n\t    )\n\t    patch_size = model.patch_embed.patch_size\n\t    print(\"Patch size = %s\" % str(patch_size))\n\t    args.window_size = (args.num_frames // args.tubelet_size,\n", "                        args.input_size // patch_size[0],\n\t                        args.input_size // patch_size[1])\n\t    args.patch_size = patch_size\n\t    if args.finetune:\n\t        if args.finetune.startswith('https'):\n\t            checkpoint = torch.hub.load_state_dict_from_url(\n\t                args.finetune, map_location='cpu', check_hash=True)\n\t        else:\n\t            checkpoint = torch.load(args.finetune, map_location='cpu')\n\t        print(\"Load ckpt from %s\" % args.finetune)\n", "        checkpoint_model = None\n\t        for model_key in args.model_key.split('|'):\n\t            if model_key in checkpoint:\n\t                checkpoint_model = checkpoint[model_key]\n\t                print(\"Load state_dict by model_key = %s\" % model_key)\n\t                break\n\t        if checkpoint_model is None:\n\t            checkpoint_model = checkpoint\n\t        for old_key in list(checkpoint_model.keys()):\n\t            if old_key.startswith('_orig_mod.'):\n", "                new_key = old_key[10:]\n\t                checkpoint_model[new_key] = checkpoint_model.pop(old_key)\n\t        state_dict = model.state_dict()\n\t        for k in ['head.weight', 'head.bias']:\n\t            if k in checkpoint_model and checkpoint_model[\n\t                    k].shape != state_dict[k].shape:\n\t                if checkpoint_model[k].shape[\n\t                        0] == 710 and args.data_set.startswith('Kinetics'):\n\t                    print(f'Convert K710 head to {args.data_set} head')\n\t                    if args.data_set == 'Kinetics-400':\n", "                        label_map_path = 'misc/label_710to400.json'\n\t                    elif args.data_set == 'Kinetics-600':\n\t                        label_map_path = 'misc/label_710to600.json'\n\t                    elif args.data_set == 'Kinetics-700':\n\t                        label_map_path = 'misc/label_710to700.json'\n\t                    label_map = json.load(open(label_map_path))\n\t                    checkpoint_model[k] = checkpoint_model[k][label_map]\n\t                else:\n\t                    print(f\"Removing key {k} from pretrained checkpoint\")\n\t                    del checkpoint_model[k]\n", "        all_keys = list(checkpoint_model.keys())\n\t        new_dict = OrderedDict()\n\t        for key in all_keys:\n\t            if key.startswith('backbone.'):\n\t                new_dict[key[9:]] = checkpoint_model[key]\n\t            elif key.startswith('encoder.'):\n\t                new_dict[key[8:]] = checkpoint_model[key]\n\t            else:\n\t                new_dict[key] = checkpoint_model[key]\n\t        checkpoint_model = new_dict\n", "        # interpolate position embedding\n\t        if 'pos_embed' in checkpoint_model:\n\t            pos_embed_checkpoint = checkpoint_model['pos_embed']\n\t            embedding_size = pos_embed_checkpoint.shape[-1]  # channel dim\n\t            num_patches = model.patch_embed.num_patches  #\n\t            num_extra_tokens = model.pos_embed.shape[-2] - num_patches  # 0/1\n\t            # height (== width) for the checkpoint position embedding\n\t            orig_size = int(\n\t                ((pos_embed_checkpoint.shape[-2] - num_extra_tokens) //\n\t                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)\n", "            # height (== width) for the new position embedding\n\t            new_size = int(\n\t                (num_patches //\n\t                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)\n\t            # class_token and dist_token are kept unchanged\n\t            if orig_size != new_size:\n\t                print(\"Position interpolate from %dx%d to %dx%d\" %\n\t                      (orig_size, orig_size, new_size, new_size))\n\t                extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n\t                # only the position tokens are interpolated\n", "                pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n\t                # B, L, C -> BT, H, W, C -> BT, C, H, W\n\t                pos_tokens = pos_tokens.reshape(\n\t                    -1, args.num_frames // model.patch_embed.tubelet_size,\n\t                    orig_size, orig_size, embedding_size)\n\t                pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size,\n\t                                                embedding_size).permute(\n\t                                                    0, 3, 1, 2)\n\t                pos_tokens = torch.nn.functional.interpolate(\n\t                    pos_tokens,\n", "                    size=(new_size, new_size),\n\t                    mode='bicubic',\n\t                    align_corners=False)\n\t                # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C\n\t                pos_tokens = pos_tokens.permute(0, 2, 3, 1).reshape(\n\t                    -1, args.num_frames // model.patch_embed.tubelet_size,\n\t                    new_size, new_size, embedding_size)\n\t                pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C\n\t                new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n\t                checkpoint_model['pos_embed'] = new_pos_embed\n", "        elif args.input_size != 224:\n\t            pos_tokens = model.pos_embed\n\t            org_num_frames = 16\n\t            T = org_num_frames // args.tubelet_size\n\t            P = int((pos_tokens.shape[1] // T)**0.5)\n\t            C = pos_tokens.shape[2]\n\t            new_P = args.input_size // patch_size[0]\n\t            # B, L, C -> BT, H, W, C -> BT, C, H, W\n\t            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)\n\t            pos_tokens = pos_tokens.reshape(-1, P, P, C).permute(0, 3, 1, 2)\n", "            pos_tokens = torch.nn.functional.interpolate(\n\t                pos_tokens,\n\t                size=(new_P, new_P),\n\t                mode='bicubic',\n\t                align_corners=False)\n\t            # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C\n\t            pos_tokens = pos_tokens.permute(0, 2, 3,\n\t                                            1).reshape(-1, T, new_P, new_P, C)\n\t            pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C\n\t            model.pos_embed = pos_tokens  # update\n", "        if args.num_frames != 16:\n\t            org_num_frames = 16\n\t            T = org_num_frames // args.tubelet_size\n\t            pos_tokens = model.pos_embed\n\t            new_T = args.num_frames // args.tubelet_size\n\t            P = int((pos_tokens.shape[1] // T)**0.5)\n\t            C = pos_tokens.shape[2]\n\t            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)\n\t            pos_tokens = pos_tokens.permute(0, 2, 3, 4,\n\t                                            1).reshape(-1, C, T)  # BHW,C,T\n", "            pos_tokens = torch.nn.functional.interpolate(\n\t                pos_tokens, size=new_T, mode='linear')\n\t            pos_tokens = pos_tokens.reshape(1, P, P, C,\n\t                                            new_T).permute(0, 4, 1, 2, 3)\n\t            pos_tokens = pos_tokens.flatten(1, 3)\n\t            model.pos_embed = pos_tokens  # update\n\t        utils.load_state_dict(\n\t            model, checkpoint_model, prefix=args.model_prefix)\n\t    model.to(device)\n\t    model_ema = None\n", "    if args.model_ema:\n\t        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n\t        model_ema = ModelEma(\n\t            model,\n\t            decay=args.model_ema_decay,\n\t            device='cpu' if args.model_ema_force_cpu else '',\n\t            resume='')\n\t        print(\"Using EMA with decay = %.8f\" % args.model_ema_decay)\n\t    model_without_ddp = model\n\t    n_parameters = sum(p.numel() for p in model.parameters()\n", "                       if p.requires_grad)\n\t    print(\"Model = %s\" % str(model_without_ddp))\n\t    print('number of params:', n_parameters)\n\t    total_batch_size = args.batch_size * args.update_freq * num_tasks\n\t    num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n\t    args.lr = args.lr * total_batch_size / 256\n\t    #########scale the lr#############\n\t    args.min_lr = args.min_lr * total_batch_size / 256\n\t    args.warmup_lr = args.warmup_lr * total_batch_size / 256\n\t    #########scale the lr#############\n", "    print(\"LR = %.8f\" % args.lr)\n\t    print(\"Batch size = %d\" % total_batch_size)\n\t    print(\"Update frequent = %d\" % args.update_freq)\n\t    print(\"Number of training examples = %d\" % len(dataset_train))\n\t    print(\"Number of training training per epoch = %d\" %\n\t          num_training_steps_per_epoch)\n\t    num_layers = model_without_ddp.get_num_layers()\n\t    if args.layer_decay < 1.0:\n\t        assigner = LayerDecayValueAssigner(\n\t            list(args.layer_decay**(num_layers + 1 - i)\n", "                 for i in range(num_layers + 2)))\n\t    else:\n\t        assigner = None\n\t    if assigner is not None:\n\t        print(\"Assigned values = %s\" % str(assigner.values))\n\t    skip_weight_decay_list = model.no_weight_decay()\n\t    print(\"Skip weight decay list: \", skip_weight_decay_list)\n\t    if args.enable_deepspeed:\n\t        loss_scaler = None\n\t        optimizer_params = get_parameter_groups(\n", "            model, args.weight_decay, skip_weight_decay_list,\n\t            assigner.get_layer_id if assigner is not None else None,\n\t            assigner.get_scale if assigner is not None else None)\n\t        model, optimizer, _, _ = ds_init(\n\t            args=args,\n\t            model=model,\n\t            model_parameters=optimizer_params,\n\t            dist_init_required=not args.distributed,\n\t        )\n\t        print(\"model.gradient_accumulation_steps() = %d\" %\n", "              model.gradient_accumulation_steps())\n\t        assert model.gradient_accumulation_steps() == args.update_freq\n\t    else:\n\t        if args.distributed:\n\t            model = torch.nn.parallel.DistributedDataParallel(\n\t                model, device_ids=[args.gpu], find_unused_parameters=False)\n\t            model_without_ddp = model.module\n\t        optimizer = create_optimizer(\n\t            args,\n\t            model_without_ddp,\n", "            skip_list=skip_weight_decay_list,\n\t            get_num_layer=assigner.get_layer_id\n\t            if assigner is not None else None,\n\t            get_layer_scale=assigner.get_scale\n\t            if assigner is not None else None)\n\t        loss_scaler = NativeScaler()\n\t    print(\"Use step level LR scheduler!\")\n\t    lr_schedule_values = utils.cosine_scheduler(\n\t        args.lr,\n\t        args.min_lr,\n", "        args.epochs,\n\t        num_training_steps_per_epoch,\n\t        warmup_epochs=args.warmup_epochs,\n\t        warmup_steps=args.warmup_steps,\n\t    )\n\t    if args.weight_decay_end is None:\n\t        args.weight_decay_end = args.weight_decay\n\t    wd_schedule_values = utils.cosine_scheduler(args.weight_decay,\n\t                                                args.weight_decay_end,\n\t                                                args.epochs,\n", "                                                num_training_steps_per_epoch)\n\t    print(\"Max WD = %.7f, Min WD = %.7f\" %\n\t          (max(wd_schedule_values), min(wd_schedule_values)))\n\t    if mixup_fn is not None:\n\t        # smoothing is handled with mixup label transform\n\t        criterion = SoftTargetCrossEntropy()\n\t    elif args.smoothing > 0.:\n\t        criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n\t    else:\n\t        criterion = torch.nn.CrossEntropyLoss()\n", "    print(\"criterion = %s\" % str(criterion))\n\t    utils.auto_load_model(\n\t        args=args,\n\t        model=model,\n\t        model_without_ddp=model_without_ddp,\n\t        optimizer=optimizer,\n\t        loss_scaler=loss_scaler,\n\t        model_ema=model_ema)\n\t    if args.validation:\n\t        test_stats = validation_one_epoch(data_loader_val, model, device)\n", "        print(\n\t            f\"{len(dataset_val)} val images: Top-1 {test_stats['acc1']:.2f}%, Top-5 {test_stats['acc5']:.2f}%, loss {test_stats['loss']:.4f}\"\n\t        )\n\t        exit(0)\n\t    if args.eval:\n\t        preds_file = os.path.join(args.output_dir, str(global_rank) + '.txt')\n\t        test_stats = final_test(data_loader_test, model, device, preds_file)\n\t        torch.distributed.barrier()\n\t        if global_rank == 0:\n\t            print(\"Start merging results...\")\n", "            final_top1, final_top5 = merge(args.output_dir, num_tasks)\n\t            print(\n\t                f\"Accuracy of the network on the {len(dataset_test)} test videos: Top-1: {final_top1:.2f}%, Top-5: {final_top5:.2f}%\"\n\t            )\n\t            log_stats = {'Final top-1': final_top1, 'Final Top-5': final_top5}\n\t            if args.output_dir and utils.is_main_process():\n\t                with open(\n\t                        os.path.join(args.output_dir, \"log.txt\"),\n\t                        mode=\"a\",\n\t                        encoding=\"utf-8\") as f:\n", "                    f.write(json.dumps(log_stats) + \"\\n\")\n\t        exit(0)\n\t    print(f\"Start training for {args.epochs} epochs\")\n\t    start_time = time.time()\n\t    max_accuracy = 0.0\n\t    for epoch in range(args.start_epoch, args.epochs):\n\t        if args.distributed:\n\t            data_loader_train.sampler.set_epoch(epoch)\n\t        if log_writer is not None:\n\t            log_writer.set_step(epoch * num_training_steps_per_epoch *\n", "                                args.update_freq)\n\t        train_stats = train_one_epoch(\n\t            model,\n\t            criterion,\n\t            data_loader_train,\n\t            optimizer,\n\t            device,\n\t            epoch,\n\t            loss_scaler,\n\t            args.clip_grad,\n", "            model_ema,\n\t            mixup_fn,\n\t            log_writer=log_writer,\n\t            start_steps=epoch * num_training_steps_per_epoch,\n\t            lr_schedule_values=lr_schedule_values,\n\t            wd_schedule_values=wd_schedule_values,\n\t            num_training_steps_per_epoch=num_training_steps_per_epoch,\n\t            update_freq=args.update_freq,\n\t        )\n\t        if args.output_dir and args.save_ckpt:\n", "            _epoch = epoch + 1\n\t            if _epoch % args.save_ckpt_freq == 0 or _epoch == args.epochs:\n\t                utils.save_model(\n\t                    args=args,\n\t                    model=model,\n\t                    model_without_ddp=model_without_ddp,\n\t                    optimizer=optimizer,\n\t                    loss_scaler=loss_scaler,\n\t                    epoch=epoch,\n\t                    model_ema=model_ema)\n", "        if data_loader_val is not None:\n\t            test_stats = validation_one_epoch(data_loader_val, model, device)\n\t            print(\n\t                f\"Accuracy of the network on the {len(dataset_val)} val images: {test_stats['acc1']:.2f}%\"\n\t            )\n\t            if max_accuracy < test_stats[\"acc1\"]:\n\t                max_accuracy = test_stats[\"acc1\"]\n\t                if args.output_dir and args.save_ckpt:\n\t                    utils.save_model(\n\t                        args=args,\n", "                        model=model,\n\t                        model_without_ddp=model_without_ddp,\n\t                        optimizer=optimizer,\n\t                        loss_scaler=loss_scaler,\n\t                        epoch=\"best\",\n\t                        model_ema=model_ema)\n\t            print(f'Max accuracy: {max_accuracy:.2f}%')\n\t            if log_writer is not None:\n\t                log_writer.update(\n\t                    val_acc1=test_stats['acc1'], head=\"perf\", step=epoch)\n", "                log_writer.update(\n\t                    val_acc5=test_stats['acc5'], head=\"perf\", step=epoch)\n\t                log_writer.update(\n\t                    val_loss=test_stats['loss'], head=\"perf\", step=epoch)\n\t            log_stats = {\n\t                **{f'train_{k}': v\n\t                   for k, v in train_stats.items()},\n\t                **{f'val_{k}': v\n\t                   for k, v in test_stats.items()}, 'epoch': epoch,\n\t                'n_parameters': n_parameters\n", "            }\n\t        else:\n\t            log_stats = {\n\t                **{f'train_{k}': v\n\t                   for k, v in train_stats.items()}, 'epoch': epoch,\n\t                'n_parameters': n_parameters\n\t            }\n\t        if args.output_dir and utils.is_main_process():\n\t            if log_writer is not None:\n\t                log_writer.flush()\n", "            with open(\n\t                    os.path.join(args.output_dir, \"log.txt\"),\n\t                    mode=\"a\",\n\t                    encoding=\"utf-8\") as f:\n\t                f.write(json.dumps(log_stats) + \"\\n\")\n\t    preds_file = os.path.join(args.output_dir, str(global_rank) + '.txt')\n\t    test_stats = final_test(data_loader_test, model, device, preds_file)\n\t    torch.distributed.barrier()\n\t    if global_rank == 0:\n\t        print(\"Start merging results...\")\n", "        final_top1, final_top5 = merge(args.output_dir, num_tasks)\n\t        print(\n\t            f\"Accuracy of the network on the {len(dataset_test)} test videos: Top-1: {final_top1:.2f}%, Top-5: {final_top5:.2f}%\"\n\t        )\n\t        log_stats = {'Final top-1': final_top1, 'Final Top-5': final_top5}\n\t        if args.output_dir and utils.is_main_process():\n\t            with open(\n\t                    os.path.join(args.output_dir, \"log.txt\"),\n\t                    mode=\"a\",\n\t                    encoding=\"utf-8\") as f:\n", "                f.write(json.dumps(log_stats) + \"\\n\")\n\t    total_time = time.time() - start_time\n\t    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t    print('Training time {}'.format(total_time_str))\n\tif __name__ == '__main__':\n\t    opts, ds_init = get_args()\n\t    if opts.output_dir:\n\t        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)\n\t    main(opts, ds_init)\n"]}
{"filename": "engine_for_pretraining.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport math\n\timport sys\n\tfrom typing import Iterable\n", "import torch\n\tfrom einops import rearrange\n\tfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\timport utils\n\tdef train_one_epoch(model: torch.nn.Module,\n\t                    data_loader: Iterable,\n\t                    optimizer: torch.optim.Optimizer,\n\t                    device: torch.device,\n\t                    epoch: int,\n\t                    loss_scaler,\n", "                    max_norm: float = 0,\n\t                    patch_size: int = 16,\n\t                    normlize_target: bool = True,\n\t                    log_writer=None,\n\t                    lr_scheduler=None,\n\t                    start_steps=None,\n\t                    lr_schedule_values=None,\n\t                    wd_schedule_values=None):\n\t    model.train()\n\t    metric_logger = utils.MetricLogger(delimiter=\"  \")\n", "    metric_logger.add_meter(\n\t        'lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n\t    metric_logger.add_meter(\n\t        'min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n\t    header = 'Epoch: [{}]'.format(epoch)\n\t    print_freq = 20\n\t    for step, batch in enumerate(\n\t            metric_logger.log_every(data_loader, print_freq, header)):\n\t        # assign learning rate & weight decay for each step\n\t        it = start_steps + step  # global training iteration\n", "        if lr_schedule_values is not None or wd_schedule_values is not None:\n\t            for i, param_group in enumerate(optimizer.param_groups):\n\t                if lr_schedule_values is not None:\n\t                    param_group[\"lr\"] = lr_schedule_values[it] * param_group[\n\t                        \"lr_scale\"]\n\t                if wd_schedule_values is not None and param_group[\n\t                        \"weight_decay\"] > 0:\n\t                    param_group[\"weight_decay\"] = wd_schedule_values[it]\n\t        # NOTE: When the decoder mask ratio is 0,\n\t        # in other words, when decoder masking is not used,\n", "        # decode_masked_pos = ~bool_masked_pos\n\t        images, bool_masked_pos, decode_masked_pos = batch\n\t        images = images.to(device, non_blocking=True)\n\t        bool_masked_pos = bool_masked_pos.to(\n\t            device, non_blocking=True).flatten(1).to(torch.bool)\n\t        decode_masked_pos = decode_masked_pos.to(\n\t            device, non_blocking=True).flatten(1).to(torch.bool)\n\t        with torch.no_grad():\n\t            # calculate the predict label\n\t            mean = torch.as_tensor(IMAGENET_DEFAULT_MEAN).to(device)[None, :,\n", "                                                                     None,\n\t                                                                     None,\n\t                                                                     None]\n\t            std = torch.as_tensor(IMAGENET_DEFAULT_STD).to(device)[None, :,\n\t                                                                   None, None,\n\t                                                                   None]\n\t            unnorm_images = images * std + mean  # in [0, 1]\n\t            if normlize_target:\n\t                images_squeeze = rearrange(\n\t                    unnorm_images,\n", "                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2) c',\n\t                    p0=2,\n\t                    p1=patch_size,\n\t                    p2=patch_size)\n\t                images_norm = (images_squeeze - images_squeeze.mean(\n\t                    dim=-2, keepdim=True)) / (\n\t                        images_squeeze.var(\n\t                            dim=-2, unbiased=True, keepdim=True).sqrt() + 1e-6)\n\t                images_patch = rearrange(images_norm, 'b n p c -> b n (p c)')\n\t            else:\n", "                images_patch = rearrange(\n\t                    unnorm_images,\n\t                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)',\n\t                    p0=2,\n\t                    p1=patch_size,\n\t                    p2=patch_size)\n\t            B, N, C = images_patch.shape\n\t            labels = images_patch[~decode_masked_pos].reshape(B, -1, C)\n\t        if loss_scaler is None:\n\t            outputs = model(images, bool_masked_pos, decode_masked_pos)\n", "            loss = (outputs - labels)**2\n\t            loss = loss.mean(dim=-1)\n\t            cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(B, -1)\n\t            loss = (loss * cal_loss_mask).sum() / cal_loss_mask.sum()\n\t        else:\n\t            with torch.cuda.amp.autocast():\n\t                outputs = model(images, bool_masked_pos, decode_masked_pos)\n\t                loss = (outputs - labels)**2\n\t                loss = loss.mean(dim=-1)\n\t                cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(\n", "                    B, -1)\n\t                loss = (loss * cal_loss_mask).sum() / cal_loss_mask.sum()\n\t        loss_value = loss.item()\n\t        if not math.isfinite(loss_value):\n\t            print(\"Loss is {}, stopping training\".format(loss_value))\n\t            sys.exit(2)\n\t        optimizer.zero_grad()\n\t        if loss_scaler is None:\n\t            loss.backward()\n\t            if max_norm is None:\n", "                grad_norm = utils.get_grad_norm_(model.parameters())\n\t            else:\n\t                grad_norm = torch.nn.utils.clip_grad_norm_(\n\t                    model.parameters(), max_norm)\n\t            optimizer.step()\n\t            loss_scale_value = 0\n\t        else:\n\t            # this attribute is added by timm on one optimizer (adahessian)\n\t            is_second_order = hasattr(\n\t                optimizer, 'is_second_order') and optimizer.is_second_order\n", "            grad_norm = loss_scaler(\n\t                loss,\n\t                optimizer,\n\t                clip_grad=max_norm,\n\t                parameters=model.parameters(),\n\t                create_graph=is_second_order)\n\t            loss_scale_value = loss_scaler.state_dict()[\"scale\"]\n\t        torch.cuda.synchronize()\n\t        metric_logger.update(loss=loss_value)\n\t        metric_logger.update(loss_scale=loss_scale_value)\n", "        min_lr = 10.\n\t        max_lr = 0.\n\t        for group in optimizer.param_groups:\n\t            min_lr = min(min_lr, group[\"lr\"])\n\t            max_lr = max(max_lr, group[\"lr\"])\n\t        metric_logger.update(lr=max_lr)\n\t        metric_logger.update(min_lr=min_lr)\n\t        weight_decay_value = None\n\t        for group in optimizer.param_groups:\n\t            if group[\"weight_decay\"] > 0:\n", "                weight_decay_value = group[\"weight_decay\"]\n\t        metric_logger.update(weight_decay=weight_decay_value)\n\t        metric_logger.update(grad_norm=grad_norm)\n\t        if log_writer is not None:\n\t            log_writer.update(loss=loss_value, head=\"loss\")\n\t            log_writer.update(loss_scale=loss_scale_value, head=\"opt\")\n\t            log_writer.update(lr=max_lr, head=\"opt\")\n\t            log_writer.update(min_lr=min_lr, head=\"opt\")\n\t            log_writer.update(weight_decay=weight_decay_value, head=\"opt\")\n\t            log_writer.update(grad_norm=grad_norm, head=\"opt\")\n", "            log_writer.set_step()\n\t        if lr_scheduler is not None:\n\t            lr_scheduler.step_update(start_steps + step)\n\t    # gather the stats from all processes\n\t    metric_logger.synchronize_between_processes()\n\t    print(\"Averaged stats:\", metric_logger)\n\t    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n"]}
{"filename": "utils.py", "chunked_list": ["# --------------------------------------------------------\n\t# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport datetime\n\timport io\n", "import json\n\timport math\n\timport os\n\timport random\n\timport subprocess\n\timport time\n\tfrom collections import defaultdict, deque\n\tfrom pathlib import Path\n\timport numpy as np\n\timport torch\n", "import torch.distributed as dist\n\tfrom tensorboardX import SummaryWriter\n\tfrom timm.utils import get_state_dict\n\tfrom torch import inf\n\tfrom torch.utils.data._utils.collate import default_collate\n\tclass SmoothedValue(object):\n\t    \"\"\"Track a series of values and provide access to smoothed values over a\n\t    window or the global series average.\n\t    \"\"\"\n\t    def __init__(self, window_size=20, fmt=None):\n", "        if fmt is None:\n\t            fmt = \"{median:.4f} ({global_avg:.4f})\"\n\t        self.deque = deque(maxlen=window_size)\n\t        self.total = 0.0\n\t        self.count = 0\n\t        self.fmt = fmt\n\t    def update(self, value, n=1):\n\t        self.deque.append(value)\n\t        self.count += n\n\t        self.total += value * n\n", "    def synchronize_between_processes(self):\n\t        \"\"\"\n\t        Warning: does not synchronize the deque!\n\t        \"\"\"\n\t        if not is_dist_avail_and_initialized():\n\t            return\n\t        t = torch.tensor([self.count, self.total],\n\t                         dtype=torch.float64,\n\t                         device='cuda')\n\t        dist.barrier()\n", "        dist.all_reduce(t)\n\t        t = t.tolist()\n\t        self.count = int(t[0])\n\t        self.total = t[1]\n\t    @property\n\t    def median(self):\n\t        d = torch.tensor(list(self.deque))\n\t        return d.median().item()\n\t    @property\n\t    def avg(self):\n", "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n\t        return d.mean().item()\n\t    @property\n\t    def global_avg(self):\n\t        return self.total / self.count\n\t    @property\n\t    def max(self):\n\t        return max(self.deque)\n\t    @property\n\t    def min(self):\n", "        return min(self.deque)\n\t    @property\n\t    def value(self):\n\t        return self.deque[-1]\n\t    def __str__(self):\n\t        return self.fmt.format(\n\t            median=self.median,\n\t            avg=self.avg,\n\t            global_avg=self.global_avg,\n\t            max=self.max,\n", "            min=self.min,\n\t            value=self.value)\n\tclass MetricLogger(object):\n\t    def __init__(self, delimiter=\"\\t\"):\n\t        self.meters = defaultdict(SmoothedValue)\n\t        self.delimiter = delimiter\n\t    def update(self, **kwargs):\n\t        for k, v in kwargs.items():\n\t            if v is None:\n\t                continue\n", "            if isinstance(v, torch.Tensor):\n\t                v = v.item()\n\t            assert isinstance(v, (float, int))\n\t            self.meters[k].update(v)\n\t    def __getattr__(self, attr):\n\t        if attr in self.meters:\n\t            return self.meters[attr]\n\t        if attr in self.__dict__:\n\t            return self.__dict__[attr]\n\t        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n", "            type(self).__name__, attr))\n\t    def __str__(self):\n\t        loss_str = []\n\t        for name, meter in self.meters.items():\n\t            loss_str.append(\"{}: {}\".format(name, str(meter)))\n\t        return self.delimiter.join(loss_str)\n\t    def synchronize_between_processes(self):\n\t        for meter in self.meters.values():\n\t            meter.synchronize_between_processes()\n\t    def add_meter(self, name, meter):\n", "        self.meters[name] = meter\n\t    def log_every(self, iterable, print_freq, header=None):\n\t        i = 0\n\t        if not header:\n\t            header = ''\n\t        start_time = time.time()\n\t        end = time.time()\n\t        iter_time = SmoothedValue(fmt='{avg:.4f} ({min:.4f} -- {max:.4f})')\n\t        data_time = SmoothedValue(fmt='{avg:.4f} ({min:.4f} -- {max:.4f})')\n\t        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n", "        log_msg = [\n\t            header, '[{0' + space_fmt + '}/{1}]', 'eta: {eta}', '{meters}',\n\t            'time: {time}', 'data: {data}'\n\t        ]\n\t        if torch.cuda.is_available():\n\t            log_msg.append('max mem: {memory:.0f}')\n\t        log_msg = self.delimiter.join(log_msg)\n\t        MB = 1024.0 * 1024.0\n\t        for obj in iterable:\n\t            data_time.update(time.time() - end)\n", "            yield obj\n\t            iter_time.update(time.time() - end)\n\t            if i % print_freq == 0 or i == len(iterable) - 1:\n\t                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n\t                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n\t                if torch.cuda.is_available():\n\t                    print(\n\t                        log_msg.format(\n\t                            i,\n\t                            len(iterable),\n", "                            eta=eta_string,\n\t                            meters=str(self),\n\t                            time=str(iter_time),\n\t                            data=str(data_time),\n\t                            memory=torch.cuda.max_memory_allocated() / MB))\n\t                else:\n\t                    print(\n\t                        log_msg.format(\n\t                            i,\n\t                            len(iterable),\n", "                            eta=eta_string,\n\t                            meters=str(self),\n\t                            time=str(iter_time),\n\t                            data=str(data_time)))\n\t            i += 1\n\t            end = time.time()\n\t        total_time = time.time() - start_time\n\t        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t        print('{} Total time: {} ({:.4f} s / it)'.format(\n\t            header, total_time_str, total_time / len(iterable)))\n", "class TensorboardLogger(object):\n\t    def __init__(self, log_dir):\n\t        self.writer = SummaryWriter(logdir=log_dir)\n\t        self.step = 0\n\t    def set_step(self, step=None):\n\t        if step is not None:\n\t            self.step = step\n\t        else:\n\t            self.step += 1\n\t    def update(self, head='scalar', step=None, **kwargs):\n", "        for k, v in kwargs.items():\n\t            if v is None:\n\t                continue\n\t            if isinstance(v, torch.Tensor):\n\t                v = v.item()\n\t            assert isinstance(v, (float, int))\n\t            self.writer.add_scalar(head + \"/\" + k, v,\n\t                                   self.step if step is None else step)\n\t    def flush(self):\n\t        self.writer.flush()\n", "def seed_worker(worker_id):\n\t    worker_seed = torch.initial_seed() % 2**32\n\t    np.random.seed(worker_seed)\n\t    random.seed(worker_seed)\n\tdef _load_checkpoint_for_ema(model_ema, checkpoint):\n\t    \"\"\"\n\t    Workaround for ModelEma._load_checkpoint to accept an already-loaded object\n\t    \"\"\"\n\t    mem_file = io.BytesIO()\n\t    torch.save(checkpoint, mem_file)\n", "    mem_file.seek(0)\n\t    model_ema._load_checkpoint(mem_file)\n\tdef setup_for_distributed(is_master):\n\t    \"\"\"\n\t    This function disables printing when not in master process\n\t    \"\"\"\n\t    import builtins as __builtin__\n\t    builtin_print = __builtin__.print\n\t    def print(*args, **kwargs):\n\t        force = kwargs.pop('force', False)\n", "        if is_master or force:\n\t            builtin_print(*args, **kwargs)\n\t    __builtin__.print = print\n\tdef is_dist_avail_and_initialized():\n\t    if not dist.is_available():\n\t        return False\n\t    if not dist.is_initialized():\n\t        return False\n\t    return True\n\tdef get_world_size():\n", "    if not is_dist_avail_and_initialized():\n\t        return 1\n\t    return dist.get_world_size()\n\tdef get_rank():\n\t    if not is_dist_avail_and_initialized():\n\t        return 0\n\t    return dist.get_rank()\n\tdef is_main_process():\n\t    return get_rank() == 0\n\tdef save_on_master(*args, **kwargs):\n", "    if is_main_process():\n\t        torch.save(*args, **kwargs)\n\tdef init_distributed_mode(args):\n\t    if args.dist_on_itp:\n\t        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n\t        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n\t        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n\t        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'],\n\t                                         os.environ['MASTER_PORT'])\n\t        os.environ['LOCAL_RANK'] = str(args.gpu)\n", "        os.environ['RANK'] = str(args.rank)\n\t        os.environ['WORLD_SIZE'] = str(args.world_size)\n\t        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n\t    elif 'SLURM_PROCID' in os.environ:\n\t        args.rank = int(os.environ['SLURM_PROCID'])\n\t        args.gpu = int(os.environ['SLURM_LOCALID'])\n\t        args.world_size = int(os.environ['SLURM_NTASKS'])\n\t        os.environ['RANK'] = str(args.rank)\n\t        os.environ['LOCAL_RANK'] = str(args.gpu)\n\t        os.environ['WORLD_SIZE'] = str(args.world_size)\n", "        node_list = os.environ['SLURM_NODELIST']\n\t        addr = subprocess.getoutput(\n\t            f'scontrol show hostname {node_list} | head -n1')\n\t        if 'MASTER_ADDR' not in os.environ:\n\t            os.environ['MASTER_ADDR'] = addr\n\t    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n\t        args.rank = int(os.environ[\"RANK\"])\n\t        args.world_size = int(os.environ['WORLD_SIZE'])\n\t        args.gpu = int(os.environ['LOCAL_RANK'])\n\t    else:\n", "        print('Not using distributed mode')\n\t        args.distributed = False\n\t        return\n\t    args.distributed = True\n\t    torch.cuda.set_device(args.gpu)\n\t    args.dist_backend = 'nccl'\n\t    print(\n\t        '| distributed init (rank {}): {}, gpu {}'.format(\n\t            args.rank, args.dist_url, args.gpu),\n\t        flush=True)\n", "    torch.distributed.init_process_group(\n\t        backend=args.dist_backend,\n\t        init_method=args.dist_url,\n\t        world_size=args.world_size,\n\t        rank=args.rank)\n\t    torch.cuda.empty_cache()\n\t    torch.distributed.barrier()\n\t    assert torch.distributed.is_initialized()\n\t    setup_for_distributed(args.rank == 0)\n\tdef load_state_dict(model,\n", "                    state_dict,\n\t                    prefix='',\n\t                    ignore_missing=\"relative_position_index\"):\n\t    missing_keys = []\n\t    unexpected_keys = []\n\t    error_msgs = []\n\t    # copy state_dict so _load_from_state_dict can modify it\n\t    metadata = getattr(state_dict, '_metadata', None)\n\t    state_dict = state_dict.copy()\n\t    if metadata is not None:\n", "        state_dict._metadata = metadata\n\t    def load(module, prefix=''):\n\t        local_metadata = {} if metadata is None else metadata.get(\n\t            prefix[:-1], {})\n\t        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n\t                                     missing_keys, unexpected_keys, error_msgs)\n\t        for name, child in module._modules.items():\n\t            if child is not None:\n\t                load(child, prefix + name + '.')\n\t    load(model, prefix=prefix)\n", "    warn_missing_keys = []\n\t    ignore_missing_keys = []\n\t    for key in missing_keys:\n\t        keep_flag = True\n\t        for ignore_key in ignore_missing.split('|'):\n\t            if ignore_key in key:\n\t                keep_flag = False\n\t                break\n\t        if keep_flag:\n\t            warn_missing_keys.append(key)\n", "        else:\n\t            ignore_missing_keys.append(key)\n\t    missing_keys = warn_missing_keys\n\t    if len(missing_keys) > 0:\n\t        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n\t            model.__class__.__name__, missing_keys))\n\t    if len(unexpected_keys) > 0:\n\t        print(\"Weights from pretrained model not used in {}: {}\".format(\n\t            model.__class__.__name__, unexpected_keys))\n\t    if len(ignore_missing_keys) > 0:\n", "        print(\n\t            \"Ignored weights of {} not initialized from pretrained model: {}\".\n\t            format(model.__class__.__name__, ignore_missing_keys))\n\t    if len(error_msgs) > 0:\n\t        print('\\n'.join(error_msgs))\n\tclass NativeScalerWithGradNormCount:\n\t    state_dict_key = \"amp_scaler\"\n\t    def __init__(self):\n\t        self._scaler = torch.cuda.amp.GradScaler()\n\t    def __call__(self,\n", "                 loss,\n\t                 optimizer,\n\t                 clip_grad=None,\n\t                 parameters=None,\n\t                 create_graph=False,\n\t                 update_grad=True):\n\t        self._scaler.scale(loss).backward(create_graph=create_graph)\n\t        if update_grad:\n\t            if clip_grad is not None:\n\t                assert parameters is not None\n", "                self._scaler.unscale_(\n\t                    optimizer\n\t                )  # unscale the gradients of optimizer's assigned params in-place\n\t                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n\t            else:\n\t                self._scaler.unscale_(optimizer)\n\t                norm = get_grad_norm_(parameters)\n\t            self._scaler.step(optimizer)\n\t            self._scaler.update()\n\t        else:\n", "            norm = None\n\t        return norm\n\t    def state_dict(self):\n\t        return self._scaler.state_dict()\n\t    def load_state_dict(self, state_dict):\n\t        self._scaler.load_state_dict(state_dict)\n\tdef get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n\t    if isinstance(parameters, torch.Tensor):\n\t        parameters = [parameters]\n\t    parameters = [p for p in parameters if p.grad is not None]\n", "    norm_type = float(norm_type)\n\t    if len(parameters) == 0:\n\t        return torch.tensor(0.)\n\t    device = parameters[0].grad.device\n\t    if norm_type == inf:\n\t        total_norm = max(p.grad.detach().abs().max().to(device)\n\t                         for p in parameters)\n\t    else:\n\t        total_norm = torch.norm(\n\t            torch.stack([\n", "                torch.norm(p.grad.detach(), norm_type).to(device)\n\t                for p in parameters\n\t            ]), norm_type)\n\t    return total_norm\n\tdef cosine_scheduler(base_value,\n\t                     final_value,\n\t                     epochs,\n\t                     niter_per_ep,\n\t                     warmup_epochs=0,\n\t                     start_warmup_value=0,\n", "                     warmup_steps=-1):\n\t    warmup_schedule = np.array([])\n\t    warmup_iters = warmup_epochs * niter_per_ep\n\t    if warmup_steps > 0:\n\t        warmup_iters = warmup_steps\n\t    print(\"Set warmup steps = %d\" % warmup_iters)\n\t    if warmup_epochs > 0:\n\t        warmup_schedule = np.linspace(start_warmup_value, base_value,\n\t                                      warmup_iters)\n\t    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n", "    schedule = np.array([\n\t        final_value + 0.5 * (base_value - final_value) *\n\t        (1 + math.cos(math.pi * i / (len(iters)))) for i in iters\n\t    ])\n\t    schedule = np.concatenate((warmup_schedule, schedule))\n\t    assert len(schedule) == epochs * niter_per_ep\n\t    return schedule\n\tdef save_model(args,\n\t               epoch,\n\t               model,\n", "               model_without_ddp,\n\t               optimizer,\n\t               loss_scaler,\n\t               model_ema=None):\n\t    output_dir = Path(args.output_dir)\n\t    epoch_name = str(epoch)\n\t    if loss_scaler is not None:\n\t        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n\t        for checkpoint_path in checkpoint_paths:\n\t            to_save = {\n", "                'model': model_without_ddp.state_dict(),\n\t                'optimizer': optimizer.state_dict(),\n\t                'epoch': epoch,\n\t                'scaler': loss_scaler.state_dict(),\n\t                'args': args,\n\t            }\n\t            if model_ema is not None:\n\t                to_save['model_ema'] = get_state_dict(model_ema)\n\t            save_on_master(to_save, checkpoint_path)\n\t    else:\n", "        client_state = {'epoch': epoch}\n\t        if model_ema is not None:\n\t            client_state['model_ema'] = get_state_dict(model_ema)\n\t        model.save_checkpoint(\n\t            save_dir=args.output_dir,\n\t            tag=\"checkpoint-%s\" % epoch_name,\n\t            client_state=client_state)\n\tdef auto_load_model(args,\n\t                    model,\n\t                    model_without_ddp,\n", "                    optimizer,\n\t                    loss_scaler,\n\t                    model_ema=None):\n\t    output_dir = Path(args.output_dir)\n\t    if loss_scaler is not None:\n\t        # torch.amp\n\t        if args.auto_resume and len(args.resume) == 0:\n\t            import glob\n\t            all_checkpoints = glob.glob(\n\t                os.path.join(output_dir, 'checkpoint-*.pth'))\n", "            latest_ckpt = -1\n\t            for ckpt in all_checkpoints:\n\t                t = ckpt.split('-')[-1].split('.')[0]\n\t                if t.isdigit():\n\t                    latest_ckpt = max(int(t), latest_ckpt)\n\t            if latest_ckpt >= 0:\n\t                args.resume = os.path.join(output_dir,\n\t                                           'checkpoint-%d.pth' % latest_ckpt)\n\t            print(\"Auto resume checkpoint: %s\" % args.resume)\n\t        if args.resume:\n", "            if args.resume.startswith('https'):\n\t                checkpoint = torch.hub.load_state_dict_from_url(\n\t                    args.resume, map_location='cpu', check_hash=True)\n\t            else:\n\t                checkpoint = torch.load(args.resume, map_location='cpu')\n\t            model_without_ddp.load_state_dict(checkpoint['model'])\n\t            print(\"Resume checkpoint %s\" % args.resume)\n\t            if 'optimizer' in checkpoint and 'epoch' in checkpoint:\n\t                optimizer.load_state_dict(checkpoint['optimizer'])\n\t                args.start_epoch = checkpoint['epoch'] + 1\n", "                if hasattr(args, 'model_ema') and args.model_ema:\n\t                    _load_checkpoint_for_ema(model_ema,\n\t                                             checkpoint['model_ema'])\n\t                if 'scaler' in checkpoint:\n\t                    loss_scaler.load_state_dict(checkpoint['scaler'])\n\t                print(\"With optim & sched!\")\n\t    else:\n\t        # deepspeed, only support '--auto_resume'.\n\t        if args.auto_resume:\n\t            import glob\n", "            all_checkpoints = glob.glob(\n\t                os.path.join(output_dir, 'checkpoint-*'))\n\t            latest_ckpt = -1\n\t            for ckpt in all_checkpoints:\n\t                t = ckpt.split('-')[-1].split('.')[0]\n\t                if t.isdigit():\n\t                    latest_ckpt = max(int(t), latest_ckpt)\n\t            if latest_ckpt >= 0:\n\t                args.resume = os.path.join(output_dir,\n\t                                           'checkpoint-%d' % latest_ckpt)\n", "                print(\"Auto resume checkpoint: %d\" % latest_ckpt)\n\t                _, client_states = model.load_checkpoint(\n\t                    args.output_dir, tag='checkpoint-%d' % latest_ckpt)\n\t                if 'epoch' in client_states:\n\t                    args.start_epoch = client_states['epoch'] + 1\n\t                if model_ema is not None:\n\t                    if args.model_ema:\n\t                        _load_checkpoint_for_ema(model_ema,\n\t                                                 client_states['model_ema'])\n\tdef create_ds_config(args):\n", "    args.deepspeed_config = os.path.join(args.output_dir,\n\t                                         \"deepspeed_config.json\")\n\t    with open(args.deepspeed_config, mode=\"w\") as writer:\n\t        ds_config = {\n\t            \"train_batch_size\":\n\t            args.batch_size * args.update_freq * get_world_size(),\n\t            \"train_micro_batch_size_per_gpu\":\n\t            args.batch_size,\n\t            \"steps_per_print\":\n\t            1000,\n", "            \"gradient_clipping\":\n\t            0.0 if args.clip_grad is None else args.clip_grad,\n\t            \"optimizer\": {\n\t                \"type\": \"Adam\",\n\t                \"adam_w_mode\": True,\n\t                \"params\": {\n\t                    \"lr\": args.lr,\n\t                    \"weight_decay\": args.weight_decay,\n\t                    \"bias_correction\": True,\n\t                    \"betas\": [0.9, 0.999],\n", "                    \"eps\": 1e-8\n\t                }\n\t            },\n\t            \"fp16\": {\n\t                \"enabled\": True,\n\t                \"loss_scale\": 0,\n\t                \"initial_scale_power\": 7,\n\t                \"loss_scale_window\": 128\n\t            }\n\t        }\n", "        writer.write(json.dumps(ds_config, indent=2))\n\tdef multiple_samples_collate(batch, fold=False):\n\t    \"\"\"\n\t    Collate function for repeated augmentation. Each instance in the batch has\n\t    more than one sample.\n\t    Args:\n\t        batch (tuple or list): data batch to collate.\n\t    Returns:\n\t        (tuple): collated data batch.\n\t    \"\"\"\n", "    inputs, labels, video_idx, extra_data = zip(*batch)\n\t    inputs = [item for sublist in inputs for item in sublist]\n\t    labels = [item for sublist in labels for item in sublist]\n\t    video_idx = [item for sublist in video_idx for item in sublist]\n\t    inputs, labels, video_idx, extra_data = (\n\t        default_collate(inputs),\n\t        default_collate(labels),\n\t        default_collate(video_idx),\n\t        default_collate(extra_data),\n\t    )\n", "    if fold:\n\t        return [inputs], labels, video_idx, extra_data\n\t    else:\n\t        return inputs, labels, video_idx, extra_data\n\tdef multiple_pretrain_samples_collate(batch, fold=False):\n\t    \"\"\"\n\t    Collate function for repeated augmentation. Each instance in the batch has\n\t    more than one sample.\n\t    Args:\n\t        batch (tuple or list): data batch to collate.\n", "    Returns:\n\t        (tuple): collated data batch.\n\t    \"\"\"\n\t    process_data, encoder_mask, decoder_mask = zip(*batch)\n\t    process_data = [item for sublist in process_data for item in sublist]\n\t    encoder_mask = [item for sublist in encoder_mask for item in sublist]\n\t    decoder_mask = [item for sublist in decoder_mask for item in sublist]\n\t    process_data, encoder_mask, decoder_mask = (\n\t        default_collate(process_data),\n\t        default_collate(encoder_mask),\n", "        default_collate(decoder_mask),\n\t    )\n\t    if fold:\n\t        return [process_data], encoder_mask, decoder_mask\n\t    else:\n\t        return process_data, encoder_mask, decoder_mask\n"]}
{"filename": "run_mae_pretraining.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport argparse\n\timport datetime\n\timport json\n", "import os\n\timport random\n\timport time\n\tfrom functools import partial\n\tfrom pathlib import Path\n\timport numpy as np\n\timport torch\n\timport torch.backends.cudnn as cudnn\n\tfrom packaging import version\n\tfrom timm.models import create_model\n", "# NOTE: Do not comment `import models`, it is used to register models\n\timport models  # noqa: F401\n\timport utils\n\tfrom dataset import build_pretraining_dataset\n\tfrom engine_for_pretraining import train_one_epoch\n\tfrom optim_factory import create_optimizer\n\tfrom utils import NativeScalerWithGradNormCount as NativeScaler\n\tfrom utils import multiple_pretrain_samples_collate\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(\n", "        'VideoMAE v2 pre-training script', add_help=False)\n\t    parser.add_argument('--batch_size', default=64, type=int)\n\t    parser.add_argument('--epochs', default=300, type=int)\n\t    parser.add_argument('--save_ckpt_freq', default=50, type=int)\n\t    # Model parameters\n\t    parser.add_argument(\n\t        '--model',\n\t        default='pretrain_videomae_base_patch16_224',\n\t        type=str,\n\t        metavar='MODEL',\n", "        help='Name of model to train')\n\t    parser.add_argument('--tubelet_size', type=int, default=2)\n\t    parser.add_argument(\n\t        '--with_checkpoint', action='store_true', default=False)\n\t    parser.add_argument(\n\t        '--decoder_depth', default=4, type=int, help='depth of decoder')\n\t    parser.add_argument(\n\t        '--mask_type',\n\t        default='tube',\n\t        choices=['random', 'tube'],\n", "        type=str,\n\t        help='encoder masked strategy')\n\t    parser.add_argument(\n\t        '--decoder_mask_type',\n\t        default='run_cell',\n\t        choices=['random', 'run_cell'],\n\t        type=str,\n\t        help='decoder masked strategy')\n\t    parser.add_argument(\n\t        '--mask_ratio', default=0.9, type=float, help='mask ratio of encoder')\n", "    parser.add_argument(\n\t        '--decoder_mask_ratio',\n\t        default=0.0,\n\t        type=float,\n\t        help='mask ratio of decoder')\n\t    parser.add_argument(\n\t        '--input_size',\n\t        default=224,\n\t        type=int,\n\t        help='images input size for backbone')\n", "    parser.add_argument(\n\t        '--drop_path',\n\t        type=float,\n\t        default=0.0,\n\t        metavar='PCT',\n\t        help='Drop path rate (default: 0.1)')\n\t    parser.add_argument(\n\t        '--normlize_target',\n\t        default=True,\n\t        type=bool,\n", "        help='normalized the target patch pixels')\n\t    # Optimizer parameters\n\t    parser.add_argument(\n\t        '--opt',\n\t        default='adamw',\n\t        type=str,\n\t        metavar='OPTIMIZER',\n\t        help='Optimizer (default: \"adamw\"')\n\t    parser.add_argument(\n\t        '--opt_eps',\n", "        default=1e-8,\n\t        type=float,\n\t        metavar='EPSILON',\n\t        help='Optimizer Epsilon (default: 1e-8)')\n\t    parser.add_argument(\n\t        '--opt_betas',\n\t        default=None,\n\t        type=float,\n\t        nargs='+',\n\t        metavar='BETA',\n", "        help='Optimizer Betas (default: None, use opt default)')\n\t    parser.add_argument(\n\t        '--clip_grad',\n\t        type=float,\n\t        default=None,\n\t        metavar='NORM',\n\t        help='Clip gradient norm (default: None, no clipping)')\n\t    parser.add_argument(\n\t        '--momentum',\n\t        type=float,\n", "        default=0.9,\n\t        metavar='M',\n\t        help='SGD momentum (default: 0.9)')\n\t    parser.add_argument(\n\t        '--weight_decay',\n\t        type=float,\n\t        default=0.05,\n\t        help='weight decay (default: 0.05)')\n\t    parser.add_argument(\n\t        '--weight_decay_end',\n", "        type=float,\n\t        default=None,\n\t        help=\"\"\"Final value of the\n\t        weight decay. We use a cosine schedule for WD. \n\t        (Set the same value with args.weight_decay to keep weight decay no change)\"\"\"\n\t    )\n\t    parser.add_argument(\n\t        '--lr',\n\t        type=float,\n\t        default=1.5e-4,\n", "        metavar='LR',\n\t        help='learning rate (default: 1.5e-4)')\n\t    parser.add_argument(\n\t        '--warmup_lr',\n\t        type=float,\n\t        default=1e-6,\n\t        metavar='LR',\n\t        help='warmup learning rate (default: 1e-6)')\n\t    parser.add_argument(\n\t        '--min_lr',\n", "        type=float,\n\t        default=1e-5,\n\t        metavar='LR',\n\t        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')\n\t    parser.add_argument(\n\t        '--warmup_epochs',\n\t        type=int,\n\t        default=40,\n\t        metavar='N',\n\t        help='epochs to warmup LR, if scheduler supports')\n", "    parser.add_argument(\n\t        '--warmup_steps',\n\t        type=int,\n\t        default=-1,\n\t        metavar='N',\n\t        help='epochs to warmup LR, if scheduler supports')\n\t    # Augmentation parameters\n\t    parser.add_argument(\n\t        '--color_jitter',\n\t        type=float,\n", "        default=0.0,\n\t        metavar='PCT',\n\t        help='Color jitter factor (default: 0.4)')\n\t    parser.add_argument(\n\t        '--train_interpolation',\n\t        type=str,\n\t        default='bicubic',\n\t        choices=['random', 'bilinear', 'bicubic'],\n\t        help='Training interpolation')\n\t    # * Finetuning params\n", "    parser.add_argument(\n\t        '--finetune', default='', help='finetune from checkpoint')\n\t    # Dataset parameters\n\t    parser.add_argument(\n\t        '--data_path',\n\t        default='/your/data/annotation/path',\n\t        type=str,\n\t        help='dataset path')\n\t    parser.add_argument(\n\t        '--data_root', default='', type=str, help='dataset path root')\n", "    parser.add_argument(\n\t        '--fname_tmpl',\n\t        default='img_{:05}.jpg',\n\t        type=str,\n\t        help='filename_tmpl for rawframe data')\n\t    parser.add_argument(\n\t        '--imagenet_default_mean_and_std', default=True, action='store_true')\n\t    parser.add_argument('--num_frames', type=int, default=16)\n\t    parser.add_argument('--sampling_rate', type=int, default=4)\n\t    parser.add_argument('--num_sample', type=int, default=1)\n", "    parser.add_argument(\n\t        '--output_dir',\n\t        default='',\n\t        help='path where to save, empty for no saving')\n\t    parser.add_argument(\n\t        '--log_dir', default=None, help='path where to tensorboard log')\n\t    parser.add_argument(\n\t        '--device',\n\t        default='cuda',\n\t        help='device to use for training / testing')\n", "    parser.add_argument('--seed', default=0, type=int)\n\t    parser.add_argument('--resume', default='', help='resume from checkpoint')\n\t    parser.add_argument('--auto_resume', action='store_true')\n\t    parser.add_argument(\n\t        '--no_auto_resume', action='store_false', dest='auto_resume')\n\t    parser.set_defaults(auto_resume=True)\n\t    parser.add_argument(\n\t        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')\n\t    parser.add_argument('--num_workers', default=10, type=int)\n\t    parser.add_argument(\n", "        '--pin_mem',\n\t        action='store_true',\n\t        help=\n\t        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'\n\t    )\n\t    parser.add_argument(\n\t        '--no_pin_mem', action='store_false', dest='pin_mem', help='')\n\t    parser.set_defaults(pin_mem=True)\n\t    # distributed training parameters\n\t    parser.add_argument(\n", "        '--world_size',\n\t        default=1,\n\t        type=int,\n\t        help='number of distributed processes')\n\t    parser.add_argument('--local_rank', default=-1, type=int)\n\t    parser.add_argument('--dist_on_itp', action='store_true')\n\t    parser.add_argument(\n\t        '--dist_url',\n\t        default='env://',\n\t        help='url used to set up distributed training')\n", "    return parser.parse_args()\n\tdef get_model(args):\n\t    print(f\"Creating model: {args.model}\")\n\t    model = create_model(\n\t        args.model,\n\t        pretrained=False,\n\t        drop_path_rate=args.drop_path,\n\t        drop_block_rate=None,\n\t        all_frames=args.num_frames,\n\t        tubelet_size=args.tubelet_size,\n", "        decoder_depth=args.decoder_depth,\n\t        with_cp=args.with_checkpoint)\n\t    if version.parse(torch.__version__) > version.parse('1.13.1'):\n\t        torch.set_float32_matmul_precision('high')\n\t        model = torch.compile(model)\n\t    return model\n\tdef main(args):\n\t    utils.init_distributed_mode(args)\n\t    print(args)\n\t    device = torch.device(args.device)\n", "    # fix the seed for reproducibility\n\t    seed = args.seed + utils.get_rank()\n\t    torch.manual_seed(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    cudnn.benchmark = True\n\t    model = get_model(args)\n\t    patch_size = model.encoder.patch_embed.patch_size\n\t    print(\"Patch size = %s\" % str(patch_size))\n\t    args.window_size = (args.num_frames // args.tubelet_size,\n", "                        args.input_size // patch_size[0],\n\t                        args.input_size // patch_size[1])\n\t    args.patch_size = patch_size\n\t    # get dataset\n\t    dataset_train = build_pretraining_dataset(args)\n\t    num_tasks = utils.get_world_size()\n\t    global_rank = utils.get_rank()\n\t    sampler_rank = global_rank\n\t    total_batch_size = args.batch_size * num_tasks\n\t    num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n", "    sampler_train = torch.utils.data.DistributedSampler(\n\t        dataset_train, num_replicas=num_tasks, rank=sampler_rank, shuffle=True)\n\t    print(\"Sampler_train = %s\" % str(sampler_train))\n\t    if global_rank == 0 and args.log_dir is not None:\n\t        os.makedirs(args.log_dir, exist_ok=True)\n\t        log_writer = utils.TensorboardLogger(log_dir=args.log_dir)\n\t    else:\n\t        log_writer = None\n\t    if args.num_sample > 1:\n\t        collate_func = partial(multiple_pretrain_samples_collate, fold=False)\n", "    else:\n\t        collate_func = None\n\t    data_loader_train = torch.utils.data.DataLoader(\n\t        dataset_train,\n\t        sampler=sampler_train,\n\t        batch_size=args.batch_size,\n\t        num_workers=args.num_workers,\n\t        pin_memory=args.pin_mem,\n\t        drop_last=True,\n\t        collate_fn=collate_func,\n", "        worker_init_fn=utils.seed_worker,\n\t        persistent_workers=True)\n\t    if args.finetune:\n\t        checkpoint = torch.load(args.finetune, map_location='cpu')\n\t        print(\"Load ckpt from %s\" % args.finetune)\n\t        checkpoint_model = None\n\t        for model_key in ['model', 'module']:\n\t            if model_key in checkpoint:\n\t                checkpoint_model = checkpoint[model_key]\n\t                print(\"Load state_dict by model_key = %s\" % model_key)\n", "                break\n\t        if checkpoint_model is None:\n\t            checkpoint_model = checkpoint\n\t        utils.load_state_dict(model, checkpoint_model)\n\t    model.to(device)\n\t    model_without_ddp = model\n\t    n_parameters = sum(p.numel() for p in model.parameters()\n\t                       if p.requires_grad)\n\t    print(\"Model = %s\" % str(model_without_ddp))\n\t    print('number of params: {} M'.format(n_parameters / 1e6))\n", "    # scale the lr\n\t    args.lr = args.lr * total_batch_size / 256\n\t    args.min_lr = args.min_lr * total_batch_size / 256\n\t    args.warmup_lr = args.warmup_lr * total_batch_size / 256\n\t    print(\"LR = %.8f\" % args.lr)\n\t    print(\"Batch size = %d\" % total_batch_size)\n\t    print(\"Number of training steps = %d\" % num_training_steps_per_epoch)\n\t    print(\"Number of training examples per epoch = %d\" %\n\t          (total_batch_size * num_training_steps_per_epoch))\n\t    if args.distributed:\n", "        model = torch.nn.parallel.DistributedDataParallel(\n\t            model, device_ids=[args.gpu], find_unused_parameters=False)\n\t        model_without_ddp = model.module\n\t    optimizer = create_optimizer(args, model_without_ddp)\n\t    loss_scaler = NativeScaler()\n\t    print(\"Use step level LR & WD scheduler!\")\n\t    lr_schedule_values = utils.cosine_scheduler(\n\t        args.lr,\n\t        args.min_lr,\n\t        args.epochs,\n", "        num_training_steps_per_epoch,\n\t        warmup_epochs=args.warmup_epochs,\n\t        warmup_steps=args.warmup_steps,\n\t    )\n\t    if args.weight_decay_end is None:\n\t        args.weight_decay_end = args.weight_decay\n\t    wd_schedule_values = utils.cosine_scheduler(args.weight_decay,\n\t                                                args.weight_decay_end,\n\t                                                args.epochs,\n\t                                                num_training_steps_per_epoch)\n", "    print(\"Max WD = %.7f, Min WD = %.7f\" %\n\t          (max(wd_schedule_values), min(wd_schedule_values)))\n\t    utils.auto_load_model(\n\t        args=args,\n\t        model=model,\n\t        model_without_ddp=model_without_ddp,\n\t        optimizer=optimizer,\n\t        loss_scaler=loss_scaler)\n\t    torch.cuda.empty_cache()\n\t    print(f\"Start training for {args.epochs} epochs\")\n", "    start_time = time.time()\n\t    for epoch in range(args.start_epoch, args.epochs):\n\t        if args.distributed:\n\t            data_loader_train.sampler.set_epoch(epoch)\n\t        if log_writer is not None:\n\t            log_writer.set_step(epoch * num_training_steps_per_epoch)\n\t        train_stats = train_one_epoch(\n\t            model,\n\t            data_loader_train,\n\t            optimizer,\n", "            device,\n\t            epoch,\n\t            loss_scaler,\n\t            args.clip_grad,\n\t            log_writer=log_writer,\n\t            start_steps=epoch * num_training_steps_per_epoch,\n\t            lr_schedule_values=lr_schedule_values,\n\t            wd_schedule_values=wd_schedule_values,\n\t            patch_size=patch_size[0],\n\t            normlize_target=args.normlize_target)\n", "        if args.output_dir:\n\t            _epoch = epoch + 1\n\t            if _epoch % args.save_ckpt_freq == 0 or _epoch == args.epochs:\n\t                utils.save_model(\n\t                    args=args,\n\t                    model=model,\n\t                    model_without_ddp=model_without_ddp,\n\t                    optimizer=optimizer,\n\t                    loss_scaler=loss_scaler,\n\t                    epoch=epoch)\n", "        log_stats = {\n\t            **{f'train_{k}': v\n\t               for k, v in train_stats.items()}, 'epoch': epoch,\n\t            'n_parameters': n_parameters\n\t        }\n\t        if args.output_dir and utils.is_main_process():\n\t            if log_writer is not None:\n\t                log_writer.flush()\n\t            with open(\n\t                    os.path.join(args.output_dir, \"log.txt\"),\n", "                    mode=\"a\",\n\t                    encoding=\"utf-8\") as f:\n\t                f.write(json.dumps(log_stats) + \"\\n\")\n\t    total_time = time.time() - start_time\n\t    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t    print('Training time {}'.format(total_time_str))\n\tif __name__ == '__main__':\n\t    opts = get_args()\n\t    if opts.output_dir:\n\t        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)\n", "    main(opts)\n"]}
{"filename": "extract_tad_feature.py", "chunked_list": ["\"\"\"Extract features for temporal action detection datasets\"\"\"\n\timport argparse\n\timport os\n\timport random\n\timport numpy as np\n\timport torch\n\tfrom timm.models import create_model\n\tfrom torchvision import transforms\n\t# NOTE: Do not comment `import models`, it is used to register models\n\timport models  # noqa: F401\n", "from dataset.loader import get_video_loader\n\tdef to_normalized_float_tensor(vid):\n\t    return vid.permute(3, 0, 1, 2).to(torch.float32) / 255\n\t# NOTE: for those functions, which generally expect mini-batches, we keep them\n\t# as non-minibatch so that they are applied as if they were 4d (thus image).\n\t# this way, we only apply the transformation in the spatial domain\n\tdef resize(vid, size, interpolation='bilinear'):\n\t    # NOTE: using bilinear interpolation because we don't work on minibatches\n\t    # at this level\n\t    scale = None\n", "    if isinstance(size, int):\n\t        scale = float(size) / min(vid.shape[-2:])\n\t        size = None\n\t    return torch.nn.functional.interpolate(\n\t        vid,\n\t        size=size,\n\t        scale_factor=scale,\n\t        mode=interpolation,\n\t        align_corners=False)\n\tclass ToFloatTensorInZeroOne(object):\n", "    def __call__(self, vid):\n\t        return to_normalized_float_tensor(vid)\n\tclass Resize(object):\n\t    def __init__(self, size):\n\t        self.size = size\n\t    def __call__(self, vid):\n\t        return resize(vid, self.size)\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(\n\t        'Extract TAD features using the videomae model', add_help=False)\n", "    parser.add_argument(\n\t        '--data_set',\n\t        default='THUMOS14',\n\t        choices=['THUMOS14', 'FINEACTION'],\n\t        type=str,\n\t        help='dataset')\n\t    parser.add_argument(\n\t        '--data_path',\n\t        default='YOUR_PATH/thumos14_video',\n\t        type=str,\n", "        help='dataset path')\n\t    parser.add_argument(\n\t        '--save_path',\n\t        default='YOUR_PATH/thumos14_video/th14_vit_g_16_4',\n\t        type=str,\n\t        help='path for saving features')\n\t    parser.add_argument(\n\t        '--model',\n\t        default='vit_giant_patch14_224',\n\t        type=str,\n", "        metavar='MODEL',\n\t        help='Name of model')\n\t    parser.add_argument(\n\t        '--ckpt_path',\n\t        default='YOUR_PATH/vit_g_hyrbid_pt_1200e_k710_ft.pth',\n\t        help='load from checkpoint')\n\t    return parser.parse_args()\n\tdef get_start_idx_range(data_set):\n\t    def thumos14_range(num_frames):\n\t        return range(0, num_frames - 15, 4)\n", "    def fineaction_range(num_frames):\n\t        return range(0, num_frames - 15, 16)\n\t    if data_set == 'THUMOS14':\n\t        return thumos14_range\n\t    elif data_set == 'FINEACTION':\n\t        return fineaction_range\n\t    else:\n\t        raise NotImplementedError()\n\tdef extract_feature(args):\n\t    # preparation\n", "    if not os.path.exists(args.save_path):\n\t        os.makedirs(args.save_path)\n\t    video_loader = get_video_loader()\n\t    start_idx_range = get_start_idx_range(args.data_set)\n\t    transform = transforms.Compose(\n\t        [ToFloatTensorInZeroOne(),\n\t         Resize((224, 224))])\n\t    # get video path\n\t    vid_list = os.listdir(args.data_path)\n\t    random.shuffle(vid_list)\n", "    # get model & load ckpt\n\t    model = create_model(\n\t        args.model,\n\t        img_size=224,\n\t        pretrained=False,\n\t        num_classes=710,\n\t        all_frames=16,\n\t        tubelet_size=2,\n\t        drop_path_rate=0.3,\n\t        use_mean_pooling=True)\n", "    ckpt = torch.load(args.ckpt_path, map_location='cpu')\n\t    for model_key in ['model', 'module']:\n\t        if model_key in ckpt:\n\t            ckpt = ckpt[model_key]\n\t            break\n\t    model.load_state_dict(ckpt)\n\t    model.eval()\n\t    model.cuda()\n\t    # extract feature\n\t    num_videos = len(vid_list)\n", "    for idx, vid_name in enumerate(vid_list):\n\t        url = os.path.join(args.save_path, vid_name.split('.')[0] + '.npy')\n\t        if os.path.exists(url):\n\t            continue\n\t        video_path = os.path.join(args.data_path, vid_name)\n\t        vr = video_loader(video_path)\n\t        feature_list = []\n\t        for start_idx in start_idx_range(len(vr)):\n\t            data = vr.get_batch(np.arange(start_idx, start_idx + 16)).asnumpy()\n\t            frame = torch.from_numpy(data)  # torch.Size([16, 566, 320, 3])\n", "            frame_q = transform(frame)  # torch.Size([3, 16, 224, 224])\n\t            input_data = frame_q.unsqueeze(0).cuda()\n\t            with torch.no_grad():\n\t                feature = model.forward_features(input_data)\n\t                feature_list.append(feature.cpu().numpy())\n\t        # [N, C]\n\t        np.save(url, np.vstack(feature_list))\n\t        print(f'[{idx} / {num_videos}]: save feature on {url}')\n\tif __name__ == '__main__':\n\t    args = get_args()\n", "    extract_feature(args)\n"]}
{"filename": "dataset/masking_generator.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport numpy as np\n\tclass Cell():\n\t    def __init__(self, num_masks, num_patches):\n", "        self.num_masks = num_masks\n\t        self.num_patches = num_patches\n\t        self.size = num_masks + num_patches\n\t        self.queue = np.hstack([np.ones(num_masks), np.zeros(num_patches)])\n\t        self.queue_ptr = 0\n\t    def set_ptr(self, pos=-1):\n\t        self.queue_ptr = np.random.randint(self.size) if pos < 0 else pos\n\t    def get_cell(self):\n\t        cell_idx = (np.arange(self.size) + self.queue_ptr) % self.size\n\t        return self.queue[cell_idx]\n", "    def run_cell(self):\n\t        self.queue_ptr += 1\n\tclass RandomMaskingGenerator:\n\t    def __init__(self, input_size, mask_ratio):\n\t        if not isinstance(input_size, tuple):\n\t            input_size = (input_size, ) * 3\n\t        self.frames, self.height, self.width = input_size\n\t        self.num_patches = self.frames * self.height * self.width  # 8x14x14\n\t        self.num_mask = int(mask_ratio * self.num_patches)\n\t    def __repr__(self):\n", "        repr_str = \"Mask: total patches {}, mask patches {}\".format(\n\t            self.num_patches, self.num_mask)\n\t        return repr_str\n\t    def __call__(self):\n\t        mask = np.hstack([\n\t            np.zeros(self.num_patches - self.num_mask),\n\t            np.ones(self.num_mask),\n\t        ])\n\t        np.random.shuffle(mask)\n\t        return mask  # [196*8]\n", "class TubeMaskingGenerator:\n\t    def __init__(self, input_size, mask_ratio):\n\t        self.frames, self.height, self.width = input_size\n\t        self.num_patches_per_frame = self.height * self.width  # 14x14\n\t        self.total_patches = self.frames * self.num_patches_per_frame\n\t        self.num_masks_per_frame = int(mask_ratio * self.num_patches_per_frame)\n\t        self.total_masks = self.frames * self.num_masks_per_frame\n\t    def __repr__(self):\n\t        repr_str = \"Tube Masking: total patches {}, mask patches {}\".format(\n\t            self.total_patches, self.total_masks)\n", "        return repr_str\n\t    def __call__(self):\n\t        mask_per_frame = np.hstack([\n\t            np.zeros(self.num_patches_per_frame - self.num_masks_per_frame),\n\t            np.ones(self.num_masks_per_frame),\n\t        ])\n\t        np.random.shuffle(mask_per_frame)\n\t        mask = np.tile(mask_per_frame, (self.frames, 1))\n\t        return mask  # [196*8]\n\tclass RunningCellMaskingGenerator:\n", "    def __init__(self, input_size, mask_ratio=0.5):\n\t        self.frames, self.height, self.width = input_size\n\t        self.mask_ratio = mask_ratio\n\t        num_masks_per_cell = int(4 * self.mask_ratio)\n\t        assert 0 < num_masks_per_cell < 4\n\t        num_patches_per_cell = 4 - num_masks_per_cell\n\t        self.cell = Cell(num_masks_per_cell, num_patches_per_cell)\n\t        self.cell_size = self.cell.size\n\t        mask_list = []\n\t        for ptr_pos in range(self.cell_size):\n", "            self.cell.set_ptr(ptr_pos)\n\t            mask = []\n\t            for _ in range(self.frames):\n\t                self.cell.run_cell()\n\t                mask_unit = self.cell.get_cell().reshape(2, 2)\n\t                mask_map = np.tile(mask_unit,\n\t                                   [self.height // 2, self.width // 2])\n\t                mask.append(mask_map.flatten())\n\t            mask = np.stack(mask, axis=0)\n\t            mask_list.append(mask)\n", "        self.all_mask_maps = np.stack(mask_list, axis=0)\n\t    def __repr__(self):\n\t        repr_str = f\"Running Cell Masking with mask ratio {self.mask_ratio}\"\n\t        return repr_str\n\t    def __call__(self):\n\t        mask = self.all_mask_maps[np.random.randint(self.cell_size)]\n\t        return np.copy(mask)\n"]}
{"filename": "dataset/loader.py", "chunked_list": ["import io\n\timport cv2\n\timport numpy as np\n\tfrom decord import VideoReader, cpu\n\ttry:\n\t    from petrel_client.client import Client\n\t    petrel_backend_imported = True\n\texcept (ImportError, ModuleNotFoundError):\n\t    petrel_backend_imported = False\n\tdef get_video_loader(use_petrel_backend: bool = True,\n", "                     enable_mc: bool = True,\n\t                     conf_path: str = None):\n\t    if petrel_backend_imported and use_petrel_backend:\n\t        _client = Client(conf_path=conf_path, enable_mc=enable_mc)\n\t    else:\n\t        _client = None\n\t    def _loader(video_path):\n\t        if _client is not None and 's3:' in video_path:\n\t            video_path = io.BytesIO(_client.get(video_path))\n\t        vr = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n", "        return vr\n\t    return _loader\n\tdef get_image_loader(use_petrel_backend: bool = True,\n\t                     enable_mc: bool = True,\n\t                     conf_path: str = None):\n\t    if petrel_backend_imported and use_petrel_backend:\n\t        _client = Client(conf_path=conf_path, enable_mc=enable_mc)\n\t    else:\n\t        _client = None\n\t    def _loader(frame_path):\n", "        if _client is not None and 's3:' in frame_path:\n\t            img_bytes = _client.get(frame_path)\n\t        else:\n\t            with open(frame_path, 'rb') as f:\n\t                img_bytes = f.read()\n\t        img_np = np.frombuffer(img_bytes, np.uint8)\n\t        img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)\n\t        cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n\t        return img\n\t    return _loader\n"]}
{"filename": "dataset/transforms.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport math\n\timport numbers\n\timport random\n", "import warnings\n\timport numpy as np\n\timport torch\n\timport torchvision\n\timport torchvision.transforms.functional as F\n\tfrom PIL import Image, ImageOps\n\tclass ToNumpy:\n\t    def __call__(self, pil_img):\n\t        np_img = np.array(pil_img, dtype=np.uint8)\n\t        if np_img.ndim < 3:\n", "            np_img = np.expand_dims(np_img, axis=-1)\n\t        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n\t        return np_img\n\tclass ToTensor:\n\t    def __init__(self, dtype=torch.float32):\n\t        self.dtype = dtype\n\t    def __call__(self, pil_img):\n\t        np_img = np.array(pil_img, dtype=np.uint8)\n\t        if np_img.ndim < 3:\n\t            np_img = np.expand_dims(np_img, axis=-1)\n", "        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n\t        return torch.from_numpy(np_img).to(dtype=self.dtype)\n\t_pil_interpolation_to_str = {\n\t    Image.NEAREST: 'PIL.Image.NEAREST',\n\t    Image.BILINEAR: 'PIL.Image.BILINEAR',\n\t    Image.BICUBIC: 'PIL.Image.BICUBIC',\n\t    Image.LANCZOS: 'PIL.Image.LANCZOS',\n\t    Image.HAMMING: 'PIL.Image.HAMMING',\n\t    Image.BOX: 'PIL.Image.BOX',\n\t}\n", "def _pil_interp(method):\n\t    if method == 'bicubic':\n\t        return Image.BICUBIC\n\t    elif method == 'lanczos':\n\t        return Image.LANCZOS\n\t    elif method == 'hamming':\n\t        return Image.HAMMING\n\t    else:\n\t        # default bilinear, do we want to allow nearest?\n\t        return Image.BILINEAR\n", "_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\tclass RandomResizedCropAndInterpolationWithTwoPic:\n\t    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n\t    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n\t    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n\t    is finally resized to given size.\n\t    This is popularly used to train the Inception networks.\n\t    Args:\n\t        size: expected output size of each edge\n\t        scale: range of size of the origin size cropped\n", "        ratio: range of aspect ratio of the origin aspect ratio cropped\n\t        interpolation: Default: PIL.Image.BILINEAR\n\t    \"\"\"\n\t    def __init__(self,\n\t                 size,\n\t                 second_size=None,\n\t                 scale=(0.08, 1.0),\n\t                 ratio=(3. / 4., 4. / 3.),\n\t                 interpolation='bilinear',\n\t                 second_interpolation='lanczos'):\n", "        if isinstance(size, tuple):\n\t            self.size = size\n\t        else:\n\t            self.size = (size, size)\n\t        if second_size is not None:\n\t            if isinstance(second_size, tuple):\n\t                self.second_size = second_size\n\t            else:\n\t                self.second_size = (second_size, second_size)\n\t        else:\n", "            self.second_size = None\n\t        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n\t            warnings.warn(\"range should be of kind (min, max)\")\n\t        if interpolation == 'random':\n\t            self.interpolation = _RANDOM_INTERPOLATION\n\t        else:\n\t            self.interpolation = _pil_interp(interpolation)\n\t        self.second_interpolation = _pil_interp(second_interpolation)\n\t        self.scale = scale\n\t        self.ratio = ratio\n", "    @staticmethod\n\t    def get_params(img, scale, ratio):\n\t        \"\"\"Get parameters for ``crop`` for a random sized crop.\n\t        Args:\n\t            img (PIL Image): Image to be cropped.\n\t            scale (tuple): range of size of the origin size cropped\n\t            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\t        Returns:\n\t            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n\t                sized crop.\n", "        \"\"\"\n\t        area = img.size[0] * img.size[1]\n\t        for attempt in range(10):\n\t            target_area = random.uniform(*scale) * area\n\t            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n\t            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\t            w = int(round(math.sqrt(target_area * aspect_ratio)))\n\t            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\t            if w <= img.size[0] and h <= img.size[1]:\n\t                i = random.randint(0, img.size[1] - h)\n", "                j = random.randint(0, img.size[0] - w)\n\t                return i, j, h, w\n\t        # Fallback to central crop\n\t        in_ratio = img.size[0] / img.size[1]\n\t        if in_ratio < min(ratio):\n\t            w = img.size[0]\n\t            h = int(round(w / min(ratio)))\n\t        elif in_ratio > max(ratio):\n\t            h = img.size[1]\n\t            w = int(round(h * max(ratio)))\n", "        else:  # whole image\n\t            w = img.size[0]\n\t            h = img.size[1]\n\t        i = (img.size[1] - h) // 2\n\t        j = (img.size[0] - w) // 2\n\t        return i, j, h, w\n\t    def __call__(self, img):\n\t        \"\"\"\n\t        Args:\n\t            img (PIL Image): Image to be cropped and resized.\n", "        Returns:\n\t            PIL Image: Randomly cropped and resized image.\n\t        \"\"\"\n\t        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n\t        if isinstance(self.interpolation, (tuple, list)):\n\t            interpolation = random.choice(self.interpolation)\n\t        else:\n\t            interpolation = self.interpolation\n\t        if self.second_size is None:\n\t            return F.resized_crop(img, i, j, h, w, self.size, interpolation)\n", "        else:\n\t            return F.resized_crop(img, i, j, h, w, self.size,\n\t                                  interpolation), F.resized_crop(\n\t                                      img, i, j, h, w, self.second_size,\n\t                                      self.second_interpolation)\n\t    def __repr__(self):\n\t        if isinstance(self.interpolation, (tuple, list)):\n\t            interpolate_str = ' '.join(\n\t                [_pil_interpolation_to_str[x] for x in self.interpolation])\n\t        else:\n", "            interpolate_str = _pil_interpolation_to_str[self.interpolation]\n\t        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n\t        format_string += ', scale={0}'.format(\n\t            tuple(round(s, 4) for s in self.scale))\n\t        format_string += ', ratio={0}'.format(\n\t            tuple(round(r, 4) for r in self.ratio))\n\t        format_string += ', interpolation={0}'.format(interpolate_str)\n\t        if self.second_size is not None:\n\t            format_string += ', second_size={0}'.format(self.second_size)\n\t            format_string += ', second_interpolation={0}'.format(\n", "                _pil_interpolation_to_str[self.second_interpolation])\n\t        format_string += ')'\n\t        return format_string\n\tclass GroupRandomCrop(object):\n\t    def __init__(self, size):\n\t        if isinstance(size, numbers.Number):\n\t            self.size = (int(size), int(size))\n\t        else:\n\t            self.size = size\n\t    def __call__(self, img_tuple):\n", "        img_group, label = img_tuple\n\t        w, h = img_group[0].size\n\t        th, tw = self.size\n\t        out_images = list()\n\t        x1 = random.randint(0, w - tw)\n\t        y1 = random.randint(0, h - th)\n\t        for img in img_group:\n\t            assert (img.size[0] == w and img.size[1] == h)\n\t            if w == tw and h == th:\n\t                out_images.append(img)\n", "            else:\n\t                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n\t        return (out_images, label)\n\tclass GroupCenterCrop(object):\n\t    def __init__(self, size):\n\t        self.worker = torchvision.transforms.CenterCrop(size)\n\t    def __call__(self, img_tuple):\n\t        img_group, label = img_tuple\n\t        return ([self.worker(img) for img in img_group], label)\n\tclass GroupRandomHorizontalFlip(object):\n", "    \"\"\"Randomly horizontally flips the given PIL.Image with a probability of 0.5\n\t    \"\"\"\n\t    def __init__(self, selective_flip=True, is_flow=False):\n\t        self.is_flow = is_flow\n\t        self.class_LeftRight = [86, 87, 93, 94, 166, 167\n\t                                ] if selective_flip else []\n\t    def __call__(self, img_tuple, is_flow=False):\n\t        img_group, label = img_tuple\n\t        v = random.random()\n\t        if (label not in self.class_LeftRight) and v < 0.5:\n", "            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n\t            if self.is_flow:\n\t                for i in range(0, len(ret), 2):\n\t                    ret[i] = ImageOps.invert(\n\t                        ret[i])  # invert flow pixel values when flipping\n\t            return (ret, label)\n\t        else:\n\t            return img_tuple\n\tclass GroupNormalize(object):\n\t    def __init__(self, mean, std):\n", "        self.mean = mean\n\t        self.std = std\n\t    def __call__(self, tensor_tuple):\n\t        tensor, label = tensor_tuple\n\t        rep_mean = self.mean * (tensor.size()[0] // len(self.mean))\n\t        rep_std = self.std * (tensor.size()[0] // len(self.std))\n\t        # TODO: make efficient\n\t        for t, m, s in zip(tensor, rep_mean, rep_std):\n\t            t.sub_(m).div_(s)\n\t        return (tensor, label)\n", "class GroupGrayScale(object):\n\t    def __init__(self, size):\n\t        self.worker = torchvision.transforms.Grayscale(size)\n\t    def __call__(self, img_tuple):\n\t        img_group, label = img_tuple\n\t        return ([self.worker(img) for img in img_group], label)\n\tclass GroupScale(object):\n\t    \"\"\" Rescales the input PIL.Image to the given 'size'.\n\t    'size' will be the size of the smaller edge.\n\t    For example, if height > width, then image will be\n", "    rescaled to (size * height / width, size)\n\t    size: size of the smaller edge\n\t    interpolation: Default: PIL.Image.BILINEAR\n\t    \"\"\"\n\t    def __init__(self, size, interpolation=Image.BILINEAR):\n\t        self.worker = torchvision.transforms.Resize(size, interpolation)\n\t    def __call__(self, img_tuple):\n\t        img_group, label = img_tuple\n\t        return ([self.worker(img) for img in img_group], label)\n\tclass GroupOverSample(object):\n", "    def __init__(self, crop_size, scale_size=None):\n\t        self.crop_size = crop_size if not isinstance(crop_size, int) else (\n\t            crop_size, crop_size)\n\t        if scale_size is not None:\n\t            self.scale_worker = GroupScale(scale_size)\n\t        else:\n\t            self.scale_worker = None\n\t    def __call__(self, img_tuple):\n\t        if self.scale_worker is not None:\n\t            img_tuple = self.scale_worker(img_tuple)\n", "        img_group, label = img_tuple\n\t        image_w, image_h = img_group[0].size\n\t        crop_w, crop_h = self.crop_size\n\t        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h,\n\t                                                      crop_w, crop_h)\n\t        oversample_group = list()\n\t        for o_w, o_h in offsets:\n\t            normal_group = list()\n\t            flip_group = list()\n\t            for i, img in enumerate(img_group):\n", "                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n\t                normal_group.append(crop)\n\t                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n\t                if img.mode == 'L' and i % 2 == 0:\n\t                    flip_group.append(ImageOps.invert(flip_crop))\n\t                else:\n\t                    flip_group.append(flip_crop)\n\t            oversample_group.extend(normal_group)\n\t            oversample_group.extend(flip_group)\n\t        return (oversample_group, label)\n", "class GroupFullResSample(object):\n\t    def __init__(self, crop_size, scale_size=None, flip=True):\n\t        self.crop_size = crop_size if not isinstance(crop_size, int) else (\n\t            crop_size, crop_size)\n\t        if scale_size is not None:\n\t            self.scale_worker = GroupScale(scale_size)\n\t        else:\n\t            self.scale_worker = None\n\t        self.flip = flip\n\t    def __call__(self, img_tuple):\n", "        if self.scale_worker is not None:\n\t            img_tuple = self.scale_worker(img_tuple)\n\t        img_group, label = img_tuple\n\t        image_w, image_h = img_group[0].size\n\t        crop_w, crop_h = self.crop_size\n\t        w_step = (image_w - crop_w) // 4\n\t        h_step = (image_h - crop_h) // 4\n\t        offsets = list()\n\t        offsets.append((0 * w_step, 2 * h_step))  # left\n\t        offsets.append((4 * w_step, 2 * h_step))  # right\n", "        offsets.append((2 * w_step, 2 * h_step))  # center\n\t        oversample_group = list()\n\t        for o_w, o_h in offsets:\n\t            normal_group = list()\n\t            flip_group = list()\n\t            for i, img in enumerate(img_group):\n\t                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n\t                normal_group.append(crop)\n\t                if self.flip:\n\t                    flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n", "                    if img.mode == 'L' and i % 2 == 0:\n\t                        flip_group.append(ImageOps.invert(flip_crop))\n\t                    else:\n\t                        flip_group.append(flip_crop)\n\t            oversample_group.extend(normal_group)\n\t            oversample_group.extend(flip_group)\n\t        return (oversample_group, label)\n\tclass GroupMultiScaleCrop(object):\n\t    def __init__(self,\n\t                 input_size,\n", "                 scales=None,\n\t                 max_distort=1,\n\t                 fix_crop=True,\n\t                 more_fix_crop=True):\n\t        self.scales = scales if scales is not None else [1, .875, .75, .66]\n\t        self.max_distort = max_distort\n\t        self.fix_crop = fix_crop\n\t        self.more_fix_crop = more_fix_crop\n\t        self.input_size = input_size if not isinstance(input_size, int) else [\n\t            input_size, input_size\n", "        ]\n\t        self.interpolation = Image.BILINEAR\n\t    def __call__(self, img_tuple):\n\t        img_group, label = img_tuple\n\t        im_size = img_group[0].size\n\t        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n\t        crop_img_group = [\n\t            img.crop(\n\t                (offset_w, offset_h, offset_w + crop_w, offset_h + crop_h))\n\t            for img in img_group\n", "        ]\n\t        ret_img_group = [\n\t            img.resize((self.input_size[0], self.input_size[1]),\n\t                       self.interpolation) for img in crop_img_group\n\t        ]\n\t        return (ret_img_group, label)\n\t    def _sample_crop_size(self, im_size):\n\t        image_w, image_h = im_size[0], im_size[1]\n\t        # find a crop size\n\t        base_size = min(image_w, image_h)\n", "        crop_sizes = [int(base_size * x) for x in self.scales]\n\t        crop_h = [\n\t            self.input_size[1] if abs(x - self.input_size[1]) < 3 else x\n\t            for x in crop_sizes\n\t        ]\n\t        crop_w = [\n\t            self.input_size[0] if abs(x - self.input_size[0]) < 3 else x\n\t            for x in crop_sizes\n\t        ]\n\t        pairs = []\n", "        for i, h in enumerate(crop_h):\n\t            for j, w in enumerate(crop_w):\n\t                if abs(i - j) <= self.max_distort:\n\t                    pairs.append((w, h))\n\t        crop_pair = random.choice(pairs)\n\t        if not self.fix_crop:\n\t            w_offset = random.randint(0, image_w - crop_pair[0])\n\t            h_offset = random.randint(0, image_h - crop_pair[1])\n\t        else:\n\t            w_offset, h_offset = self._sample_fix_offset(\n", "                image_w, image_h, crop_pair[0], crop_pair[1])\n\t        return crop_pair[0], crop_pair[1], w_offset, h_offset\n\t    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n\t        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h,\n\t                                       crop_w, crop_h)\n\t        return random.choice(offsets)\n\t    @staticmethod\n\t    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n\t        w_step = (image_w - crop_w) // 4\n\t        h_step = (image_h - crop_h) // 4\n", "        ret = list()\n\t        ret.append((0, 0))  # upper left\n\t        ret.append((4 * w_step, 0))  # upper right\n\t        ret.append((0, 4 * h_step))  # lower left\n\t        ret.append((4 * w_step, 4 * h_step))  # lower right\n\t        ret.append((2 * w_step, 2 * h_step))  # center\n\t        if more_fix_crop:\n\t            ret.append((0, 2 * h_step))  # center left\n\t            ret.append((4 * w_step, 2 * h_step))  # center right\n\t            ret.append((2 * w_step, 4 * h_step))  # lower center\n", "            ret.append((2 * w_step, 0 * h_step))  # upper center\n\t            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n\t            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n\t            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n\t            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n\t        return ret\n\tclass GroupRandomSizedCrop(object):\n\t    \"\"\"Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n\t    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n\t    This is popularly used to train the Inception networks\n", "    size: size of the smaller edge\n\t    interpolation: Default: PIL.Image.BILINEAR\n\t    \"\"\"\n\t    def __init__(self, size, interpolation=Image.BILINEAR):\n\t        self.size = size\n\t        self.interpolation = interpolation\n\t    def __call__(self, img_tuple):\n\t        img_group, label = img_tuple\n\t        for attempt in range(10):\n\t            area = img_group[0].size[0] * img_group[0].size[1]\n", "            target_area = random.uniform(0.08, 1.0) * area\n\t            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\t            w = int(round(math.sqrt(target_area * aspect_ratio)))\n\t            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\t            if random.random() < 0.5:\n\t                w, h = h, w\n\t            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n\t                x1 = random.randint(0, img_group[0].size[0] - w)\n\t                y1 = random.randint(0, img_group[0].size[1] - h)\n\t                found = True\n", "                break\n\t        else:\n\t            found = False\n\t            x1 = 0\n\t            y1 = 0\n\t        if found:\n\t            out_group = list()\n\t            for img in img_group:\n\t                img = img.crop((x1, y1, x1 + w, y1 + h))\n\t                assert (img.size == (w, h))\n", "                out_group.append(\n\t                    img.resize((self.size, self.size), self.interpolation))\n\t            return out_group\n\t        else:\n\t            # Fallback\n\t            scale = GroupScale(self.size, interpolation=self.interpolation)\n\t            crop = GroupRandomCrop(self.size)\n\t            return crop(scale(img_group))\n\tclass Stack(object):\n\t    def __init__(self, roll=False):\n", "        self.roll = roll\n\t    def __call__(self, img_tuple):\n\t        img_group, label = img_tuple\n\t        if img_group[0].mode == 'L':\n\t            return (np.concatenate([np.expand_dims(x, 2) for x in img_group],\n\t                                   axis=2), label)\n\t        elif img_group[0].mode == 'RGB':\n\t            if self.roll:\n\t                return (np.concatenate(\n\t                    [np.array(x)[:, :, ::-1] for x in img_group],\n", "                    axis=2), label)\n\t            else:\n\t                return (np.concatenate(img_group, axis=2), label)\n\tclass ToTorchFormatTensor(object):\n\t    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n\t    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n\t    def __init__(self, div=True):\n\t        self.div = div\n\t    def __call__(self, pic_tuple):\n\t        pic, label = pic_tuple\n", "        if isinstance(pic, np.ndarray):\n\t            # handle numpy array\n\t            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n\t        else:\n\t            # handle PIL Image\n\t            img = torch.as_tensor(pic.tobytes(), dtype=torch.uint8)\n\t            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n\t            # put it from HWC to CHW format\n\t            # yikes, this transpose takes 80% of the loading time/CPU\n\t            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n", "        return (img.float().div(255.) if self.div else img.float(), label)\n\tclass IdentityTransform(object):\n\t    def __call__(self, data):\n\t        return data\n"]}
{"filename": "dataset/video_transforms.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\timport math\n\timport numbers\n\timport random\n\timport numpy as np\n\timport PIL\n\timport torch\n\timport torchvision\n\timport torchvision.transforms.functional as F\n", "from PIL import Image\n\tfrom torchvision import transforms\n\tfrom . import functional as FF\n\tfrom .rand_augment import rand_augment_transform\n\tfrom .random_erasing import RandomErasing\n\t_pil_interpolation_to_str = {\n\t    Image.NEAREST: \"PIL.Image.NEAREST\",\n\t    Image.BILINEAR: \"PIL.Image.BILINEAR\",\n\t    Image.BICUBIC: \"PIL.Image.BICUBIC\",\n\t    Image.LANCZOS: \"PIL.Image.LANCZOS\",\n", "    Image.HAMMING: \"PIL.Image.HAMMING\",\n\t    Image.BOX: \"PIL.Image.BOX\",\n\t}\n\t_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\tdef _pil_interp(method):\n\t    if method == \"bicubic\":\n\t        return Image.BICUBIC\n\t    elif method == \"lanczos\":\n\t        return Image.LANCZOS\n\t    elif method == \"hamming\":\n", "        return Image.HAMMING\n\t    else:\n\t        return Image.BILINEAR\n\tdef random_short_side_scale_jitter(images,\n\t                                   min_size,\n\t                                   max_size,\n\t                                   boxes=None,\n\t                                   inverse_uniform_sampling=False):\n\t    \"\"\"\n\t    Perform a spatial short scale jittering on the given images and\n", "    corresponding boxes.\n\t    Args:\n\t        images (tensor): images to perform scale jitter. Dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t        min_size (int): the minimal size to scale the frames.\n\t        max_size (int): the maximal size to scale the frames.\n\t        boxes (ndarray): optional. Corresponding boxes to images.\n\t            Dimension is `num boxes` x 4.\n\t        inverse_uniform_sampling (bool): if True, sample uniformly in\n\t            [1 / max_scale, 1 / min_scale] and take a reciprocal to get the\n", "            scale. If False, take a uniform sample from [min_scale, max_scale].\n\t    Returns:\n\t        (tensor): the scaled images with dimension of\n\t            `num frames` x `channel` x `new height` x `new width`.\n\t        (ndarray or None): the scaled boxes with dimension of\n\t            `num boxes` x 4.\n\t    \"\"\"\n\t    if inverse_uniform_sampling:\n\t        size = int(\n\t            round(1.0 / np.random.uniform(1.0 / max_size, 1.0 / min_size)))\n", "    else:\n\t        size = int(round(np.random.uniform(min_size, max_size)))\n\t    height = images.shape[2]\n\t    width = images.shape[3]\n\t    if (width <= height and width == size) or (height <= width\n\t                                               and height == size):\n\t        return images, boxes\n\t    new_width = size\n\t    new_height = size\n\t    if width < height:\n", "        new_height = int(math.floor((float(height) / width) * size))\n\t        if boxes is not None:\n\t            boxes = boxes * float(new_height) / height\n\t    else:\n\t        new_width = int(math.floor((float(width) / height) * size))\n\t        if boxes is not None:\n\t            boxes = boxes * float(new_width) / width\n\t    return (\n\t        torch.nn.functional.interpolate(\n\t            images,\n", "            size=(new_height, new_width),\n\t            mode=\"bilinear\",\n\t            align_corners=False,\n\t        ),\n\t        boxes,\n\t    )\n\tdef crop_boxes(boxes, x_offset, y_offset):\n\t    \"\"\"\n\t    Peform crop on the bounding boxes given the offsets.\n\t    Args:\n", "        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n\t            is `num boxes` x 4.\n\t        x_offset (int): cropping offset in the x axis.\n\t        y_offset (int): cropping offset in the y axis.\n\t    Returns:\n\t        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n\t            `num boxes` x 4.\n\t    \"\"\"\n\t    cropped_boxes = boxes.copy()\n\t    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n", "    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset\n\t    return cropped_boxes\n\tdef random_crop(images, size, boxes=None):\n\t    \"\"\"\n\t    Perform random spatial crop on the given images and corresponding boxes.\n\t    Args:\n\t        images (tensor): images to perform random crop. The dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t        size (int): the size of height and width to crop on the image.\n\t        boxes (ndarray or None): optional. Corresponding boxes to images.\n", "            Dimension is `num boxes` x 4.\n\t    Returns:\n\t        cropped (tensor): cropped images with dimension of\n\t            `num frames` x `channel` x `size` x `size`.\n\t        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n\t            `num boxes` x 4.\n\t    \"\"\"\n\t    if images.shape[2] == size and images.shape[3] == size:\n\t        return images\n\t    height = images.shape[2]\n", "    width = images.shape[3]\n\t    y_offset = 0\n\t    if height > size:\n\t        y_offset = int(np.random.randint(0, height - size))\n\t    x_offset = 0\n\t    if width > size:\n\t        x_offset = int(np.random.randint(0, width - size))\n\t    cropped = images[:, :, y_offset:y_offset + size, x_offset:x_offset + size]\n\t    cropped_boxes = (\n\t        crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None)\n", "    return cropped, cropped_boxes\n\tdef horizontal_flip(prob, images, boxes=None):\n\t    \"\"\"\n\t    Perform horizontal flip on the given images and corresponding boxes.\n\t    Args:\n\t        prob (float): probility to flip the images.\n\t        images (tensor): images to perform horizontal flip, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t        boxes (ndarray or None): optional. Corresponding boxes to images.\n\t            Dimension is `num boxes` x 4.\n", "    Returns:\n\t        images (tensor): images with dimension of\n\t            `num frames` x `channel` x `height` x `width`.\n\t        flipped_boxes (ndarray or None): the flipped boxes with dimension of\n\t            `num boxes` x 4.\n\t    \"\"\"\n\t    if boxes is None:\n\t        flipped_boxes = None\n\t    else:\n\t        flipped_boxes = boxes.copy()\n", "    if np.random.uniform() < prob:\n\t        images = images.flip((-1))\n\t        if len(images.shape) == 3:\n\t            width = images.shape[2]\n\t        elif len(images.shape) == 4:\n\t            width = images.shape[3]\n\t        else:\n\t            raise NotImplementedError(\"Dimension does not supported\")\n\t        if boxes is not None:\n\t            flipped_boxes[:, [0, 2]] = width - boxes[:, [2, 0]] - 1\n", "    return images, flipped_boxes\n\tdef uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n\t    \"\"\"\n\t    Perform uniform spatial sampling on the images and corresponding boxes.\n\t    Args:\n\t        images (tensor): images to perform uniform crop. The dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t        size (int): size of height and weight to crop the images.\n\t        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n\t            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n", "            crop if height is larger than width.\n\t        boxes (ndarray or None): optional. Corresponding boxes to images.\n\t            Dimension is `num boxes` x 4.\n\t        scale_size (int): optinal. If not None, resize the images to scale_size before\n\t            performing any crop.\n\t    Returns:\n\t        cropped (tensor): images with dimension of\n\t            `num frames` x `channel` x `size` x `size`.\n\t        cropped_boxes (ndarray or None): the cropped boxes with dimension of\n\t            `num boxes` x 4.\n", "    \"\"\"\n\t    assert spatial_idx in [0, 1, 2]\n\t    ndim = len(images.shape)\n\t    if ndim == 3:\n\t        images = images.unsqueeze(0)\n\t    height = images.shape[2]\n\t    width = images.shape[3]\n\t    if scale_size is not None:\n\t        if width <= height:\n\t            width, height = scale_size, int(height / width * scale_size)\n", "        else:\n\t            width, height = int(width / height * scale_size), scale_size\n\t        images = torch.nn.functional.interpolate(\n\t            images,\n\t            size=(height, width),\n\t            mode=\"bilinear\",\n\t            align_corners=False,\n\t        )\n\t    y_offset = int(math.ceil((height - size) / 2))\n\t    x_offset = int(math.ceil((width - size) / 2))\n", "    if height > width:\n\t        if spatial_idx == 0:\n\t            y_offset = 0\n\t        elif spatial_idx == 2:\n\t            y_offset = height - size\n\t    else:\n\t        if spatial_idx == 0:\n\t            x_offset = 0\n\t        elif spatial_idx == 2:\n\t            x_offset = width - size\n", "    cropped = images[:, :, y_offset:y_offset + size, x_offset:x_offset + size]\n\t    cropped_boxes = (\n\t        crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None)\n\t    if ndim == 3:\n\t        cropped = cropped.squeeze(0)\n\t    return cropped, cropped_boxes\n\tdef clip_boxes_to_image(boxes, height, width):\n\t    \"\"\"\n\t    Clip an array of boxes to an image with the given height and width.\n\t    Args:\n", "        boxes (ndarray): bounding boxes to perform clipping.\n\t            Dimension is `num boxes` x 4.\n\t        height (int): given image height.\n\t        width (int): given image width.\n\t    Returns:\n\t        clipped_boxes (ndarray): the clipped boxes with dimension of\n\t            `num boxes` x 4.\n\t    \"\"\"\n\t    clipped_boxes = boxes.copy()\n\t    clipped_boxes[:, [0, 2]] = np.minimum(width - 1.0,\n", "                                          np.maximum(0.0, boxes[:, [0, 2]]))\n\t    clipped_boxes[:, [1, 3]] = np.minimum(height - 1.0,\n\t                                          np.maximum(0.0, boxes[:, [1, 3]]))\n\t    return clipped_boxes\n\tdef blend(images1, images2, alpha):\n\t    \"\"\"\n\t    Blend two images with a given weight alpha.\n\t    Args:\n\t        images1 (tensor): the first images to be blended, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n", "        images2 (tensor): the second images to be blended, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t        alpha (float): the blending weight.\n\t    Returns:\n\t        (tensor): blended images, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    \"\"\"\n\t    return images1 * alpha + images2 * (1 - alpha)\n\tdef grayscale(images):\n\t    \"\"\"\n", "    Get the grayscale for the input images. The channels of images should be\n\t    in order BGR.\n\t    Args:\n\t        images (tensor): the input images for getting grayscale. Dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    Returns:\n\t        img_gray (tensor): blended images, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    \"\"\"\n\t    # R -> 0.299, G -> 0.587, B -> 0.114.\n", "    img_gray = torch.tensor(images)\n\t    gray_channel = (0.299 * images[:, 2] + 0.587 * images[:, 1] +\n\t                    0.114 * images[:, 0])\n\t    img_gray[:, 0] = gray_channel\n\t    img_gray[:, 1] = gray_channel\n\t    img_gray[:, 2] = gray_channel\n\t    return img_gray\n\tdef color_jitter(images, img_brightness=0, img_contrast=0, img_saturation=0):\n\t    \"\"\"\n\t    Perfrom a color jittering on the input images. The channels of images\n", "    should be in order BGR.\n\t    Args:\n\t        images (tensor): images to perform color jitter. Dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t        img_brightness (float): jitter ratio for brightness.\n\t        img_contrast (float): jitter ratio for contrast.\n\t        img_saturation (float): jitter ratio for saturation.\n\t    Returns:\n\t        images (tensor): the jittered images, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n", "    \"\"\"\n\t    jitter = []\n\t    if img_brightness != 0:\n\t        jitter.append(\"brightness\")\n\t    if img_contrast != 0:\n\t        jitter.append(\"contrast\")\n\t    if img_saturation != 0:\n\t        jitter.append(\"saturation\")\n\t    if len(jitter) > 0:\n\t        order = np.random.permutation(np.arange(len(jitter)))\n", "        for idx in range(0, len(jitter)):\n\t            if jitter[order[idx]] == \"brightness\":\n\t                images = brightness_jitter(img_brightness, images)\n\t            elif jitter[order[idx]] == \"contrast\":\n\t                images = contrast_jitter(img_contrast, images)\n\t            elif jitter[order[idx]] == \"saturation\":\n\t                images = saturation_jitter(img_saturation, images)\n\t    return images\n\tdef brightness_jitter(var, images):\n\t    \"\"\"\n", "    Perfrom brightness jittering on the input images. The channels of images\n\t    should be in order BGR.\n\t    Args:\n\t        var (float): jitter ratio for brightness.\n\t        images (tensor): images to perform color jitter. Dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    Returns:\n\t        images (tensor): the jittered images, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    \"\"\"\n", "    alpha = 1.0 + np.random.uniform(-var, var)\n\t    img_bright = torch.zeros(images.shape)\n\t    images = blend(images, img_bright, alpha)\n\t    return images\n\tdef contrast_jitter(var, images):\n\t    \"\"\"\n\t    Perfrom contrast jittering on the input images. The channels of images\n\t    should be in order BGR.\n\t    Args:\n\t        var (float): jitter ratio for contrast.\n", "        images (tensor): images to perform color jitter. Dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    Returns:\n\t        images (tensor): the jittered images, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    \"\"\"\n\t    alpha = 1.0 + np.random.uniform(-var, var)\n\t    img_gray = grayscale(images)\n\t    img_gray[:] = torch.mean(img_gray, dim=(1, 2, 3), keepdim=True)\n\t    images = blend(images, img_gray, alpha)\n", "    return images\n\tdef saturation_jitter(var, images):\n\t    \"\"\"\n\t    Perfrom saturation jittering on the input images. The channels of images\n\t    should be in order BGR.\n\t    Args:\n\t        var (float): jitter ratio for saturation.\n\t        images (tensor): images to perform color jitter. Dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    Returns:\n", "        images (tensor): the jittered images, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    \"\"\"\n\t    alpha = 1.0 + np.random.uniform(-var, var)\n\t    img_gray = grayscale(images)\n\t    images = blend(images, img_gray, alpha)\n\t    return images\n\tdef lighting_jitter(images, alphastd, eigval, eigvec):\n\t    \"\"\"\n\t    Perform AlexNet-style PCA jitter on the given images.\n", "    Args:\n\t        images (tensor): images to perform lighting jitter. Dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t        alphastd (float): jitter ratio for PCA jitter.\n\t        eigval (list): eigenvalues for PCA jitter.\n\t        eigvec (list[list]): eigenvectors for PCA jitter.\n\t    Returns:\n\t        out_images (tensor): the jittered images, the dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t    \"\"\"\n", "    if alphastd == 0:\n\t        return images\n\t    # generate alpha1, alpha2, alpha3.\n\t    alpha = np.random.normal(0, alphastd, size=(1, 3))\n\t    eig_vec = np.array(eigvec)\n\t    eig_val = np.reshape(eigval, (1, 3))\n\t    rgb = np.sum(\n\t        eig_vec * np.repeat(alpha, 3, axis=0) * np.repeat(eig_val, 3, axis=0),\n\t        axis=1,\n\t    )\n", "    out_images = torch.zeros_like(images)\n\t    if len(images.shape) == 3:\n\t        # C H W\n\t        channel_dim = 0\n\t    elif len(images.shape) == 4:\n\t        # T C H W\n\t        channel_dim = 1\n\t    else:\n\t        raise NotImplementedError(f\"Unsupported dimension {len(images.shape)}\")\n\t    for idx in range(images.shape[channel_dim]):\n", "        # C H W\n\t        if len(images.shape) == 3:\n\t            out_images[idx] = images[idx] + rgb[2 - idx]\n\t        # T C H W\n\t        elif len(images.shape) == 4:\n\t            out_images[:, idx] = images[:, idx] + rgb[2 - idx]\n\t        else:\n\t            raise NotImplementedError(\n\t                f\"Unsupported dimension {len(images.shape)}\")\n\t    return out_images\n", "def color_normalization(images, mean, stddev):\n\t    \"\"\"\n\t    Perform color nomration on the given images.\n\t    Args:\n\t        images (tensor): images to perform color normalization. Dimension is\n\t            `num frames` x `channel` x `height` x `width`.\n\t        mean (list): mean values for normalization.\n\t        stddev (list): standard deviations for normalization.\n\t    Returns:\n\t        out_images (tensor): the noramlized images, the dimension is\n", "            `num frames` x `channel` x `height` x `width`.\n\t    \"\"\"\n\t    if len(images.shape) == 3:\n\t        assert (\n\t            len(mean) == images.shape[0]), \"channel mean not computed properly\"\n\t        assert (len(stddev) == images.shape[0]\n\t                ), \"channel stddev not computed properly\"\n\t    elif len(images.shape) == 4:\n\t        assert (\n\t            len(mean) == images.shape[1]), \"channel mean not computed properly\"\n", "        assert (len(stddev) == images.shape[1]\n\t                ), \"channel stddev not computed properly\"\n\t    else:\n\t        raise NotImplementedError(f\"Unsupported dimension {len(images.shape)}\")\n\t    out_images = torch.zeros_like(images)\n\t    for idx in range(len(mean)):\n\t        # C H W\n\t        if len(images.shape) == 3:\n\t            out_images[idx] = (images[idx] - mean[idx]) / stddev[idx]\n\t        elif len(images.shape) == 4:\n", "            out_images[:, idx] = (images[:, idx] - mean[idx]) / stddev[idx]\n\t        else:\n\t            raise NotImplementedError(\n\t                f\"Unsupported dimension {len(images.shape)}\")\n\t    return out_images\n\tdef _get_param_spatial_crop(scale,\n\t                            ratio,\n\t                            height,\n\t                            width,\n\t                            num_repeat=10,\n", "                            log_scale=True,\n\t                            switch_hw=False):\n\t    \"\"\"\n\t    Given scale, ratio, height and width, return sampled coordinates of the videos.\n\t    \"\"\"\n\t    for _ in range(num_repeat):\n\t        area = height * width\n\t        target_area = random.uniform(*scale) * area\n\t        if log_scale:\n\t            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n", "            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\t        else:\n\t            aspect_ratio = random.uniform(*ratio)\n\t        w = int(round(math.sqrt(target_area * aspect_ratio)))\n\t        h = int(round(math.sqrt(target_area / aspect_ratio)))\n\t        if np.random.uniform() < 0.5 and switch_hw:\n\t            w, h = h, w\n\t        if 0 < w <= width and 0 < h <= height:\n\t            i = random.randint(0, height - h)\n\t            j = random.randint(0, width - w)\n", "            return i, j, h, w\n\t    # Fallback to central crop\n\t    in_ratio = float(width) / float(height)\n\t    if in_ratio < min(ratio):\n\t        w = width\n\t        h = int(round(w / min(ratio)))\n\t    elif in_ratio > max(ratio):\n\t        h = height\n\t        w = int(round(h * max(ratio)))\n\t    else:  # whole image\n", "        w = width\n\t        h = height\n\t    i = (height - h) // 2\n\t    j = (width - w) // 2\n\t    return i, j, h, w\n\tdef random_resized_crop(\n\t        images,\n\t        target_height,\n\t        target_width,\n\t        scale=(0.8, 1.0),\n", "        ratio=(3.0 / 4.0, 4.0 / 3.0),\n\t):\n\t    \"\"\"\n\t    Crop the given images to random size and aspect ratio. A crop of random\n\t    size (default: of 0.08 to 1.0) of the original size and a random aspect\n\t    ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This\n\t    crop is finally resized to given size. This is popularly used to train the\n\t    Inception networks.\n\t    Args:\n\t        images: Images to perform resizing and cropping.\n", "        target_height: Desired height after cropping.\n\t        target_width: Desired width after cropping.\n\t        scale: Scale range of Inception-style area based random resizing.\n\t        ratio: Aspect ratio range of Inception-style area based random resizing.\n\t    \"\"\"\n\t    height = images.shape[2]\n\t    width = images.shape[3]\n\t    i, j, h, w = _get_param_spatial_crop(scale, ratio, height, width)\n\t    cropped = images[:, :, i:i + h, j:j + w]\n\t    return torch.nn.functional.interpolate(\n", "        cropped,\n\t        size=(target_height, target_width),\n\t        mode=\"bilinear\",\n\t        align_corners=False,\n\t    )\n\tdef random_resized_crop_with_shift(\n\t        images,\n\t        target_height,\n\t        target_width,\n\t        scale=(0.8, 1.0),\n", "        ratio=(3.0 / 4.0, 4.0 / 3.0),\n\t):\n\t    \"\"\"\n\t    This is similar to random_resized_crop. However, it samples two different\n\t    boxes (for cropping) for the first and last frame. It then linearly\n\t    interpolates the two boxes for other frames.\n\t    Args:\n\t        images: Images to perform resizing and cropping.\n\t        target_height: Desired height after cropping.\n\t        target_width: Desired width after cropping.\n", "        scale: Scale range of Inception-style area based random resizing.\n\t        ratio: Aspect ratio range of Inception-style area based random resizing.\n\t    \"\"\"\n\t    t = images.shape[1]\n\t    height = images.shape[2]\n\t    width = images.shape[3]\n\t    i, j, h, w = _get_param_spatial_crop(scale, ratio, height, width)\n\t    i_, j_, h_, w_ = _get_param_spatial_crop(scale, ratio, height, width)\n\t    i_s = [int(i) for i in torch.linspace(i, i_, steps=t).tolist()]\n\t    j_s = [int(i) for i in torch.linspace(j, j_, steps=t).tolist()]\n", "    h_s = [int(i) for i in torch.linspace(h, h_, steps=t).tolist()]\n\t    w_s = [int(i) for i in torch.linspace(w, w_, steps=t).tolist()]\n\t    out = torch.zeros((3, t, target_height, target_width))\n\t    for ind in range(t):\n\t        out[:, ind:ind + 1, :, :] = torch.nn.functional.interpolate(\n\t            images[:, ind:ind + 1, i_s[ind]:i_s[ind] + h_s[ind],\n\t                   j_s[ind]:j_s[ind] + w_s[ind], ],\n\t            size=(target_height, target_width),\n\t            mode=\"bilinear\",\n\t            align_corners=False,\n", "        )\n\t    return out\n\tdef create_random_augment(\n\t    input_size,\n\t    auto_augment=None,\n\t    interpolation=\"bilinear\",\n\t):\n\t    \"\"\"\n\t    Get video randaug transform.\n\t    Args:\n", "        input_size: The size of the input video in tuple.\n\t        auto_augment: Parameters for randaug. An example:\n\t            \"rand-m7-n4-mstd0.5-inc1\" (m is the magnitude and n is the number\n\t            of operations to apply).\n\t        interpolation: Interpolation method.\n\t    \"\"\"\n\t    if isinstance(input_size, tuple):\n\t        img_size = input_size[-2:]\n\t    else:\n\t        img_size = input_size\n", "    if auto_augment:\n\t        assert isinstance(auto_augment, str)\n\t        if isinstance(img_size, tuple):\n\t            img_size_min = min(img_size)\n\t        else:\n\t            img_size_min = img_size\n\t        aa_params = {\"translate_const\": int(img_size_min * 0.45)}\n\t        if interpolation and interpolation != \"random\":\n\t            aa_params[\"interpolation\"] = _pil_interp(interpolation)\n\t        if auto_augment.startswith(\"rand\"):\n", "            return transforms.Compose(\n\t                [rand_augment_transform(auto_augment, aa_params)])\n\t    raise NotImplementedError\n\tdef random_sized_crop_img(\n\t        im,\n\t        size,\n\t        jitter_scale=(0.08, 1.0),\n\t        jitter_aspect=(3.0 / 4.0, 4.0 / 3.0),\n\t        max_iter=10,\n\t):\n", "    \"\"\"\n\t    Performs Inception-style cropping (used for training).\n\t    \"\"\"\n\t    assert (len(\n\t        im.shape) == 3), \"Currently only support image for random_sized_crop\"\n\t    h, w = im.shape[1:3]\n\t    i, j, h, w = _get_param_spatial_crop(\n\t        scale=jitter_scale,\n\t        ratio=jitter_aspect,\n\t        height=h,\n", "        width=w,\n\t        num_repeat=max_iter,\n\t        log_scale=False,\n\t        switch_hw=True,\n\t    )\n\t    cropped = im[:, i:i + h, j:j + w]\n\t    return torch.nn.functional.interpolate(\n\t        cropped.unsqueeze(0),\n\t        size=(size, size),\n\t        mode=\"bilinear\",\n", "        align_corners=False,\n\t    ).squeeze(0)\n\t# The following code are modified based on timm lib, we will replace the following\n\t# contents with dependency from PyTorchVideo.\n\t# https://github.com/facebookresearch/pytorchvideo\n\tclass RandomResizedCropAndInterpolation:\n\t    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n\t    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n\t    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n\t    is finally resized to given size.\n", "    This is popularly used to train the Inception networks.\n\t    Args:\n\t        size: expected output size of each edge\n\t        scale: range of size of the origin size cropped\n\t        ratio: range of aspect ratio of the origin aspect ratio cropped\n\t        interpolation: Default: PIL.Image.BILINEAR\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n\t            size,\n", "            scale=(0.08, 1.0),\n\t            ratio=(3.0 / 4.0, 4.0 / 3.0),\n\t            interpolation=\"bilinear\",\n\t    ):\n\t        if isinstance(size, tuple):\n\t            self.size = size\n\t        else:\n\t            self.size = (size, size)\n\t        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n\t            print(\"range should be of kind (min, max)\")\n", "        if interpolation == \"random\":\n\t            self.interpolation = _RANDOM_INTERPOLATION\n\t        else:\n\t            self.interpolation = _pil_interp(interpolation)\n\t        self.scale = scale\n\t        self.ratio = ratio\n\t    @staticmethod\n\t    def get_params(img, scale, ratio):\n\t        \"\"\"Get parameters for ``crop`` for a random sized crop.\n\t        Args:\n", "            img (PIL Image): Image to be cropped.\n\t            scale (tuple): range of size of the origin size cropped\n\t            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\t        Returns:\n\t            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n\t                sized crop.\n\t        \"\"\"\n\t        area = img.size[0] * img.size[1]\n\t        for _ in range(10):\n\t            target_area = random.uniform(*scale) * area\n", "            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n\t            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\t            w = int(round(math.sqrt(target_area * aspect_ratio)))\n\t            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\t            if w <= img.size[0] and h <= img.size[1]:\n\t                i = random.randint(0, img.size[1] - h)\n\t                j = random.randint(0, img.size[0] - w)\n\t                return i, j, h, w\n\t        # Fallback to central crop\n\t        in_ratio = img.size[0] / img.size[1]\n", "        if in_ratio < min(ratio):\n\t            w = img.size[0]\n\t            h = int(round(w / min(ratio)))\n\t        elif in_ratio > max(ratio):\n\t            h = img.size[1]\n\t            w = int(round(h * max(ratio)))\n\t        else:  # whole image\n\t            w = img.size[0]\n\t            h = img.size[1]\n\t        i = (img.size[1] - h) // 2\n", "        j = (img.size[0] - w) // 2\n\t        return i, j, h, w\n\t    def __call__(self, img):\n\t        \"\"\"\n\t        Args:\n\t            img (PIL Image): Image to be cropped and resized.\n\t        Returns:\n\t            PIL Image: Randomly cropped and resized image.\n\t        \"\"\"\n\t        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n", "        if isinstance(self.interpolation, (tuple, list)):\n\t            interpolation = random.choice(self.interpolation)\n\t        else:\n\t            interpolation = self.interpolation\n\t        return F.resized_crop(img, i, j, h, w, self.size, interpolation)\n\t    def __repr__(self):\n\t        if isinstance(self.interpolation, (tuple, list)):\n\t            interpolate_str = \" \".join(\n\t                [_pil_interpolation_to_str[x] for x in self.interpolation])\n\t        else:\n", "            interpolate_str = _pil_interpolation_to_str[self.interpolation]\n\t        format_string = self.__class__.__name__ + \"(size={0}\".format(self.size)\n\t        format_string += \", scale={0}\".format(\n\t            tuple(round(s, 4) for s in self.scale))\n\t        format_string += \", ratio={0}\".format(\n\t            tuple(round(r, 4) for r in self.ratio))\n\t        format_string += \", interpolation={0})\".format(interpolate_str)\n\t        return format_string\n\tdef transforms_imagenet_train(\n\t    img_size=224,\n", "    scale=None,\n\t    ratio=None,\n\t    hflip=0.5,\n\t    vflip=0.0,\n\t    color_jitter=0.4,\n\t    auto_augment=None,\n\t    interpolation=\"random\",\n\t    use_prefetcher=False,\n\t    mean=(0.485, 0.456, 0.406),\n\t    std=(0.229, 0.224, 0.225),\n", "    re_prob=0.0,\n\t    re_mode=\"const\",\n\t    re_count=1,\n\t    re_num_splits=0,\n\t    separate=False,\n\t):\n\t    \"\"\"\n\t    If separate==True, the transforms are returned as a tuple of 3 separate transforms\n\t    for use in a mixing dataset that passes\n\t     * all data through the first (primary) transform, called the 'clean' data\n", "     * a portion of the data through the secondary transform\n\t     * normalizes and converts the branches above with the third, final transform\n\t    \"\"\"\n\t    if isinstance(img_size, tuple):\n\t        img_size = img_size[-2:]\n\t    else:\n\t        img_size = img_size\n\t    scale = tuple(scale or (0.08, 1.0))  # default imagenet scale range\n\t    ratio = tuple(ratio\n\t                  or (3.0 / 4.0, 4.0 / 3.0))  # default imagenet ratio range\n", "    primary_tfl = [\n\t        RandomResizedCropAndInterpolation(\n\t            img_size, scale=scale, ratio=ratio, interpolation=interpolation)\n\t    ]\n\t    if hflip > 0.0:\n\t        primary_tfl += [transforms.RandomHorizontalFlip(p=hflip)]\n\t    if vflip > 0.0:\n\t        primary_tfl += [transforms.RandomVerticalFlip(p=vflip)]\n\t    secondary_tfl = []\n\t    if auto_augment:\n", "        assert isinstance(auto_augment, str)\n\t        if isinstance(img_size, tuple):\n\t            img_size_min = min(img_size)\n\t        else:\n\t            img_size_min = img_size\n\t        aa_params = dict(\n\t            translate_const=int(img_size_min * 0.45),\n\t            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n\t        )\n\t        if interpolation and interpolation != \"random\":\n", "            aa_params[\"interpolation\"] = _pil_interp(interpolation)\n\t        if auto_augment.startswith(\"rand\"):\n\t            secondary_tfl += [rand_augment_transform(auto_augment, aa_params)]\n\t        elif auto_augment.startswith(\"augmix\"):\n\t            raise NotImplementedError(\"Augmix not implemented\")\n\t        else:\n\t            raise NotImplementedError(\"Auto aug not implemented\")\n\t    elif color_jitter is not None:\n\t        # color jitter is enabled when not using AA\n\t        if isinstance(color_jitter, (list, tuple)):\n", "            # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation\n\t            # or 4 if also augmenting hue\n\t            assert len(color_jitter) in (3, 4)\n\t        else:\n\t            # if it's a scalar, duplicate for brightness, contrast, and saturation, no hue\n\t            color_jitter = (float(color_jitter), ) * 3\n\t        secondary_tfl += [transforms.ColorJitter(*color_jitter)]\n\t    final_tfl = []\n\t    final_tfl += [\n\t        transforms.ToTensor(),\n", "        transforms.Normalize(mean=torch.tensor(mean), std=torch.tensor(std)),\n\t    ]\n\t    if re_prob > 0.0:\n\t        final_tfl.append(\n\t            RandomErasing(\n\t                re_prob,\n\t                mode=re_mode,\n\t                max_count=re_count,\n\t                num_splits=re_num_splits,\n\t                device=\"cpu\",\n", "                cube=False,\n\t            ))\n\t    if separate:\n\t        return (\n\t            transforms.Compose(primary_tfl),\n\t            transforms.Compose(secondary_tfl),\n\t            transforms.Compose(final_tfl),\n\t        )\n\t    else:\n\t        return transforms.Compose(primary_tfl + secondary_tfl + final_tfl)\n", "############################################################################################################\n\t############################################################################################################\n\tclass Compose(object):\n\t    \"\"\"Composes several transforms\n\t    Args:\n\t    transforms (list of ``Transform`` objects): list of transforms\n\t    to compose\n\t    \"\"\"\n\t    def __init__(self, transforms):\n\t        self.transforms = transforms\n", "    def __call__(self, clip):\n\t        for t in self.transforms:\n\t            clip = t(clip)\n\t        return clip\n\tclass RandomHorizontalFlip(object):\n\t    \"\"\"Horizontally flip the list of given images randomly\n\t    with a probability 0.5\n\t    \"\"\"\n\t    def __call__(self, clip):\n\t        \"\"\"\n", "        Args:\n\t        img (PIL.Image or numpy.ndarray): List of images to be cropped\n\t        in format (h, w, c) in numpy.ndarray\n\t        Returns:\n\t        PIL.Image or numpy.ndarray: Randomly flipped clip\n\t        \"\"\"\n\t        if random.random() < 0.5:\n\t            if isinstance(clip[0], np.ndarray):\n\t                return [np.fliplr(img) for img in clip]\n\t            elif isinstance(clip[0], PIL.Image.Image):\n", "                return [\n\t                    img.transpose(PIL.Image.FLIP_LEFT_RIGHT) for img in clip\n\t                ]\n\t            else:\n\t                raise TypeError('Expected numpy.ndarray or PIL.Image' +\n\t                                ' but got list of {0}'.format(type(clip[0])))\n\t        return clip\n\tclass RandomResize(object):\n\t    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n\t    The larger the original image is, the more times it takes to\n", "    interpolate\n\t    Args:\n\t    interpolation (str): Can be one of 'nearest', 'bilinear'\n\t    defaults to nearest\n\t    size (tuple): (widht, height)\n\t    \"\"\"\n\t    def __init__(self, ratio=(3. / 4., 4. / 3.), interpolation='nearest'):\n\t        self.ratio = ratio\n\t        self.interpolation = interpolation\n\t    def __call__(self, clip):\n", "        scaling_factor = random.uniform(self.ratio[0], self.ratio[1])\n\t        if isinstance(clip[0], np.ndarray):\n\t            im_h, im_w, im_c = clip[0].shape\n\t        elif isinstance(clip[0], PIL.Image.Image):\n\t            im_w, im_h = clip[0].size\n\t        new_w = int(im_w * scaling_factor)\n\t        new_h = int(im_h * scaling_factor)\n\t        new_size = (new_w, new_h)\n\t        resized = FF.resize_clip(\n\t            clip, new_size, interpolation=self.interpolation)\n", "        return resized\n\tclass Resize(object):\n\t    \"\"\"Resizes a list of (H x W x C) numpy.ndarray to the final size\n\t    The larger the original image is, the more times it takes to\n\t    interpolate\n\t    Args:\n\t    interpolation (str): Can be one of 'nearest', 'bilinear'\n\t    defaults to nearest\n\t    size (tuple): (widht, height)\n\t    \"\"\"\n", "    def __init__(self, size, interpolation='nearest'):\n\t        self.size = size\n\t        self.interpolation = interpolation\n\t    def __call__(self, clip):\n\t        resized = FF.resize_clip(\n\t            clip, self.size, interpolation=self.interpolation)\n\t        return resized\n\tclass RandomCrop(object):\n\t    \"\"\"Extract random crop at the same location for a list of images\n\t    Args:\n", "    size (sequence or int): Desired output size for the\n\t    crop in format (h, w)\n\t    \"\"\"\n\t    def __init__(self, size):\n\t        if isinstance(size, numbers.Number):\n\t            size = (size, size)\n\t        self.size = size\n\t    def __call__(self, clip):\n\t        \"\"\"\n\t        Args:\n", "        img (PIL.Image or numpy.ndarray): List of images to be cropped\n\t        in format (h, w, c) in numpy.ndarray\n\t        Returns:\n\t        PIL.Image or numpy.ndarray: Cropped list of images\n\t        \"\"\"\n\t        h, w = self.size\n\t        if isinstance(clip[0], np.ndarray):\n\t            im_h, im_w, im_c = clip[0].shape\n\t        elif isinstance(clip[0], PIL.Image.Image):\n\t            im_w, im_h = clip[0].size\n", "        else:\n\t            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n\t                            'but got list of {0}'.format(type(clip[0])))\n\t        if w > im_w or h > im_h:\n\t            error_msg = (\n\t                'Initial image size should be larger then '\n\t                'cropped size but got cropped sizes : ({w}, {h}) while '\n\t                'initial image is ({im_w}, {im_h})'.format(\n\t                    im_w=im_w, im_h=im_h, w=w, h=h))\n\t            raise ValueError(error_msg)\n", "        x1 = random.randint(0, im_w - w)\n\t        y1 = random.randint(0, im_h - h)\n\t        cropped = FF.crop_clip(clip, y1, x1, h, w)\n\t        return cropped\n\tclass ThreeCrop(object):\n\t    \"\"\"Extract random crop at the same location for a list of images\n\t    Args:\n\t    size (sequence or int): Desired output size for the\n\t    crop in format (h, w)\n\t    \"\"\"\n", "    def __init__(self, size):\n\t        if isinstance(size, numbers.Number):\n\t            size = (size, size)\n\t        self.size = size\n\t    def __call__(self, clip):\n\t        \"\"\"\n\t        Args:\n\t        img (PIL.Image or numpy.ndarray): List of images to be cropped\n\t        in format (h, w, c) in numpy.ndarray\n\t        Returns:\n", "        PIL.Image or numpy.ndarray: Cropped list of images\n\t        \"\"\"\n\t        h, w = self.size\n\t        if isinstance(clip[0], np.ndarray):\n\t            im_h, im_w, im_c = clip[0].shape\n\t        elif isinstance(clip[0], PIL.Image.Image):\n\t            im_w, im_h = clip[0].size\n\t        else:\n\t            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n\t                            'but got list of {0}'.format(type(clip[0])))\n", "        if w != im_w and h != im_h:\n\t            clip = FF.resize_clip(clip, self.size, interpolation=\"bilinear\")\n\t            im_h, im_w, im_c = clip[0].shape\n\t        step = np.max((np.max((im_w, im_h)) - self.size[0]) // 2, 0)\n\t        cropped = []\n\t        for i in range(3):\n\t            if (im_h > self.size[0]):\n\t                x1 = 0\n\t                y1 = i * step\n\t                cropped.extend(FF.crop_clip(clip, y1, x1, h, w))\n", "            else:\n\t                x1 = i * step\n\t                y1 = 0\n\t                cropped.extend(FF.crop_clip(clip, y1, x1, h, w))\n\t        return cropped\n\tclass RandomRotation(object):\n\t    \"\"\"Rotate entire clip randomly by a random angle within\n\t    given bounds\n\t    Args:\n\t    degrees (sequence or int): Range of degrees to select from\n", "    If degrees is a number instead of sequence like (min, max),\n\t    the range of degrees, will be (-degrees, +degrees).\n\t    \"\"\"\n\t    def __init__(self, degrees):\n\t        if isinstance(degrees, numbers.Number):\n\t            if degrees < 0:\n\t                raise ValueError('If degrees is a single number,'\n\t                                 'must be positive')\n\t            degrees = (-degrees, degrees)\n\t        else:\n", "            if len(degrees) != 2:\n\t                raise ValueError('If degrees is a sequence,'\n\t                                 'it must be of len 2.')\n\t        self.degrees = degrees\n\t    def __call__(self, clip):\n\t        \"\"\"\n\t        Args:\n\t        img (PIL.Image or numpy.ndarray): List of images to be cropped\n\t        in format (h, w, c) in numpy.ndarray\n\t        Returns:\n", "        PIL.Image or numpy.ndarray: Cropped list of images\n\t        \"\"\"\n\t        import skimage\n\t        angle = random.uniform(self.degrees[0], self.degrees[1])\n\t        if isinstance(clip[0], np.ndarray):\n\t            rotated = [skimage.transform.rotate(img, angle) for img in clip]\n\t        elif isinstance(clip[0], PIL.Image.Image):\n\t            rotated = [img.rotate(angle) for img in clip]\n\t        else:\n\t            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n", "                            'but got list of {0}'.format(type(clip[0])))\n\t        return rotated\n\tclass CenterCrop(object):\n\t    \"\"\"Extract center crop at the same location for a list of images\n\t    Args:\n\t    size (sequence or int): Desired output size for the\n\t    crop in format (h, w)\n\t    \"\"\"\n\t    def __init__(self, size):\n\t        if isinstance(size, numbers.Number):\n", "            size = (size, size)\n\t        self.size = size\n\t    def __call__(self, clip):\n\t        \"\"\"\n\t        Args:\n\t        img (PIL.Image or numpy.ndarray): List of images to be cropped\n\t        in format (h, w, c) in numpy.ndarray\n\t        Returns:\n\t        PIL.Image or numpy.ndarray: Cropped list of images\n\t        \"\"\"\n", "        h, w = self.size\n\t        if isinstance(clip[0], np.ndarray):\n\t            im_h, im_w, im_c = clip[0].shape\n\t        elif isinstance(clip[0], PIL.Image.Image):\n\t            im_w, im_h = clip[0].size\n\t        else:\n\t            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n\t                            'but got list of {0}'.format(type(clip[0])))\n\t        if w > im_w or h > im_h:\n\t            error_msg = (\n", "                'Initial image size should be larger then '\n\t                'cropped size but got cropped sizes : ({w}, {h}) while '\n\t                'initial image is ({im_w}, {im_h})'.format(\n\t                    im_w=im_w, im_h=im_h, w=w, h=h))\n\t            raise ValueError(error_msg)\n\t        x1 = int(round((im_w - w) / 2.))\n\t        y1 = int(round((im_h - h) / 2.))\n\t        cropped = FF.crop_clip(clip, y1, x1, h, w)\n\t        return cropped\n\tclass ColorJitter(object):\n", "    \"\"\"Randomly change the brightness, contrast and saturation and hue of the clip\n\t    Args:\n\t    brightness (float): How much to jitter brightness. brightness_factor\n\t    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n\t    contrast (float): How much to jitter contrast. contrast_factor\n\t    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n\t    saturation (float): How much to jitter saturation. saturation_factor\n\t    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n\t    hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n\t    [-hue, hue]. Should be >=0 and <= 0.5.\n", "    \"\"\"\n\t    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n\t        self.brightness = brightness\n\t        self.contrast = contrast\n\t        self.saturation = saturation\n\t        self.hue = hue\n\t    def get_params(self, brightness, contrast, saturation, hue):\n\t        if brightness > 0:\n\t            brightness_factor = random.uniform(\n\t                max(0, 1 - brightness), 1 + brightness)\n", "        else:\n\t            brightness_factor = None\n\t        if contrast > 0:\n\t            contrast_factor = random.uniform(\n\t                max(0, 1 - contrast), 1 + contrast)\n\t        else:\n\t            contrast_factor = None\n\t        if saturation > 0:\n\t            saturation_factor = random.uniform(\n\t                max(0, 1 - saturation), 1 + saturation)\n", "        else:\n\t            saturation_factor = None\n\t        if hue > 0:\n\t            hue_factor = random.uniform(-hue, hue)\n\t        else:\n\t            hue_factor = None\n\t        return brightness_factor, contrast_factor, saturation_factor, hue_factor\n\t    def __call__(self, clip):\n\t        \"\"\"\n\t        Args:\n", "        clip (list): list of PIL.Image\n\t        Returns:\n\t        list PIL.Image : list of transformed PIL.Image\n\t        \"\"\"\n\t        if isinstance(clip[0], np.ndarray):\n\t            raise TypeError(\n\t                'Color jitter not yet implemented for numpy arrays')\n\t        elif isinstance(clip[0], PIL.Image.Image):\n\t            brightness, contrast, saturation, hue = self.get_params(\n\t                self.brightness, self.contrast, self.saturation, self.hue)\n", "            # Create img transform function sequence\n\t            img_transforms = []\n\t            if brightness is not None:\n\t                img_transforms.append(\n\t                    lambda img: torchvision.transforms.functional.\n\t                    adjust_brightness(img, brightness))\n\t            if saturation is not None:\n\t                img_transforms.append(\n\t                    lambda img: torchvision.transforms.functional.\n\t                    adjust_saturation(img, saturation))\n", "            if hue is not None:\n\t                img_transforms.append(lambda img: torchvision.transforms.\n\t                                      functional.adjust_hue(img, hue))\n\t            if contrast is not None:\n\t                img_transforms.append(\n\t                    lambda img: torchvision.transforms.functional.\n\t                    adjust_contrast(img, contrast))\n\t            random.shuffle(img_transforms)\n\t            # Apply to all images\n\t            jittered_clip = []\n", "            for img in clip:\n\t                for func in img_transforms:\n\t                    jittered_img = func(img)\n\t                jittered_clip.append(jittered_img)\n\t        else:\n\t            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n\t                            'but got list of {0}'.format(type(clip[0])))\n\t        return jittered_clip\n\tclass Normalize(object):\n\t    \"\"\"Normalize a clip with mean and standard deviation.\n", "    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n\t    will normalize each channel of the input ``torch.*Tensor`` i.e.\n\t    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n\t    .. note::\n\t        This transform acts out of place, i.e., it does not mutates the input tensor.\n\t    Args:\n\t        mean (sequence): Sequence of means for each channel.\n\t        std (sequence): Sequence of standard deviations for each channel.\n\t    \"\"\"\n\t    def __init__(self, mean, std):\n", "        self.mean = mean\n\t        self.std = std\n\t    def __call__(self, clip):\n\t        \"\"\"\n\t        Args:\n\t            clip (Tensor): Tensor clip of size (T, C, H, W) to be normalized.\n\t        Returns:\n\t            Tensor: Normalized Tensor clip.\n\t        \"\"\"\n\t        return FF.normalize(clip, self.mean, self.std)\n", "    def __repr__(self):\n\t        return self.__class__.__name__ + '(mean={0}, std={1})'.format(\n\t            self.mean, self.std)\n"]}
{"filename": "dataset/rand_augment.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\t\"\"\"\n\tThis implementation is based on\n\thttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py\n\tpulished under an Apache License 2.0.\n\tCOMMENT FROM ORIGINAL:\n\tAutoAugment, RandAugment, and AugMix for PyTorch\n\tThis code implements the searched ImageNet policies with various tweaks and\n\timprovements and does not include any of the search code. AA and RA\n\tImplementation adapted from:\n", "    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\n\tAugMix adapted from:\n\t    https://github.com/google-research/augmix\n\tPapers:\n\t    AutoAugment: Learning Augmentation Policies from Data\n\t    https://arxiv.org/abs/1805.09501\n\t    Learning Data Augmentation Strategies for Object Detection\n\t    https://arxiv.org/abs/1906.11172\n\t    RandAugment: Practical automated data augmentation...\n\t    https://arxiv.org/abs/1909.13719\n", "    AugMix: A Simple Data Processing Method to Improve Robustness and\n\t    Uncertainty https://arxiv.org/abs/1912.02781\n\tHacked together by / Copyright 2020 Ross Wightman\n\t\"\"\"\n\timport math\n\timport random\n\timport re\n\timport numpy as np\n\timport PIL\n\tfrom PIL import Image, ImageEnhance, ImageOps\n", "_PIL_VER = tuple([int(x) for x in PIL.__version__.split(\".\")[:2]])\n\t_FILL = (128, 128, 128)\n\t# This signifies the max integer that the controller RNN could predict for the\n\t# augmentation scheme.\n\t_MAX_LEVEL = 10.0\n\t_HPARAMS_DEFAULT = {\n\t    \"translate_const\": 250,\n\t    \"img_mean\": _FILL,\n\t}\n\t_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n", "def _interpolation(kwargs):\n\t    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n\t    if isinstance(interpolation, (list, tuple)):\n\t        return random.choice(interpolation)\n\t    else:\n\t        return interpolation\n\tdef _check_args_tf(kwargs):\n\t    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n\t        kwargs.pop(\"fillcolor\")\n\t    kwargs[\"resample\"] = _interpolation(kwargs)\n", "def shear_x(img, factor, **kwargs):\n\t    _check_args_tf(kwargs)\n\t    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0),\n\t                         **kwargs)\n\tdef shear_y(img, factor, **kwargs):\n\t    _check_args_tf(kwargs)\n\t    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0),\n\t                         **kwargs)\n\tdef translate_x_rel(img, pct, **kwargs):\n\t    pixels = pct * img.size[0]\n", "    _check_args_tf(kwargs)\n\t    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0),\n\t                         **kwargs)\n\tdef translate_y_rel(img, pct, **kwargs):\n\t    pixels = pct * img.size[1]\n\t    _check_args_tf(kwargs)\n\t    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels),\n\t                         **kwargs)\n\tdef translate_x_abs(img, pixels, **kwargs):\n\t    _check_args_tf(kwargs)\n", "    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0),\n\t                         **kwargs)\n\tdef translate_y_abs(img, pixels, **kwargs):\n\t    _check_args_tf(kwargs)\n\t    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels),\n\t                         **kwargs)\n\tdef rotate(img, degrees, **kwargs):\n\t    _check_args_tf(kwargs)\n\t    if _PIL_VER >= (5, 2):\n\t        return img.rotate(degrees, **kwargs)\n", "    elif _PIL_VER >= (5, 0):\n\t        w, h = img.size\n\t        post_trans = (0, 0)\n\t        rotn_center = (w / 2.0, h / 2.0)\n\t        angle = -math.radians(degrees)\n\t        matrix = [\n\t            round(math.cos(angle), 15),\n\t            round(math.sin(angle), 15),\n\t            0.0,\n\t            round(-math.sin(angle), 15),\n", "            round(math.cos(angle), 15),\n\t            0.0,\n\t        ]\n\t        def transform(x, y, matrix):\n\t            (a, b, c, d, e, f) = matrix\n\t            return a * x + b * y + c, d * x + e * y + f\n\t        matrix[2], matrix[5] = transform(\n\t            -rotn_center[0] - post_trans[0],\n\t            -rotn_center[1] - post_trans[1],\n\t            matrix,\n", "        )\n\t        matrix[2] += rotn_center[0]\n\t        matrix[5] += rotn_center[1]\n\t        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n\t    else:\n\t        return img.rotate(degrees, resample=kwargs[\"resample\"])\n\tdef auto_contrast(img, **__):\n\t    return ImageOps.autocontrast(img)\n\tdef invert(img, **__):\n\t    return ImageOps.invert(img)\n", "def equalize(img, **__):\n\t    return ImageOps.equalize(img)\n\tdef solarize(img, thresh, **__):\n\t    return ImageOps.solarize(img, thresh)\n\tdef solarize_add(img, add, thresh=128, **__):\n\t    lut = []\n\t    for i in range(256):\n\t        if i < thresh:\n\t            lut.append(min(255, i + add))\n\t        else:\n", "            lut.append(i)\n\t    if img.mode in (\"L\", \"RGB\"):\n\t        if img.mode == \"RGB\" and len(lut) == 256:\n\t            lut = lut + lut + lut\n\t        return img.point(lut)\n\t    else:\n\t        return img\n\tdef posterize(img, bits_to_keep, **__):\n\t    if bits_to_keep >= 8:\n\t        return img\n", "    return ImageOps.posterize(img, bits_to_keep)\n\tdef contrast(img, factor, **__):\n\t    return ImageEnhance.Contrast(img).enhance(factor)\n\tdef color(img, factor, **__):\n\t    return ImageEnhance.Color(img).enhance(factor)\n\tdef brightness(img, factor, **__):\n\t    return ImageEnhance.Brightness(img).enhance(factor)\n\tdef sharpness(img, factor, **__):\n\t    return ImageEnhance.Sharpness(img).enhance(factor)\n\tdef _randomly_negate(v):\n", "    \"\"\"With 50% prob, negate the value\"\"\"\n\t    return -v if random.random() > 0.5 else v\n\tdef _rotate_level_to_arg(level, _hparams):\n\t    # range [-30, 30]\n\t    level = (level / _MAX_LEVEL) * 30.0\n\t    level = _randomly_negate(level)\n\t    return (level, )\n\tdef _enhance_level_to_arg(level, _hparams):\n\t    # range [0.1, 1.9]\n\t    return ((level / _MAX_LEVEL) * 1.8 + 0.1, )\n", "def _enhance_increasing_level_to_arg(level, _hparams):\n\t    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0 increases the enhancement blend\n\t    # range [0.1, 1.9]\n\t    level = (level / _MAX_LEVEL) * 0.9\n\t    level = 1.0 + _randomly_negate(level)\n\t    return (level, )\n\tdef _shear_level_to_arg(level, _hparams):\n\t    # range [-0.3, 0.3]\n\t    level = (level / _MAX_LEVEL) * 0.3\n\t    level = _randomly_negate(level)\n", "    return (level, )\n\tdef _translate_abs_level_to_arg(level, hparams):\n\t    translate_const = hparams[\"translate_const\"]\n\t    level = (level / _MAX_LEVEL) * float(translate_const)\n\t    level = _randomly_negate(level)\n\t    return (level, )\n\tdef _translate_rel_level_to_arg(level, hparams):\n\t    # default range [-0.45, 0.45]\n\t    translate_pct = hparams.get(\"translate_pct\", 0.45)\n\t    level = (level / _MAX_LEVEL) * translate_pct\n", "    level = _randomly_negate(level)\n\t    return (level, )\n\tdef _posterize_level_to_arg(level, _hparams):\n\t    # As per Tensorflow TPU EfficientNet impl\n\t    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n\t    # intensity/severity of augmentation decreases with level\n\t    return (int((level / _MAX_LEVEL) * 4), )\n\tdef _posterize_increasing_level_to_arg(level, hparams):\n\t    # As per Tensorflow models research and UDA impl\n\t    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n", "    # intensity/severity of augmentation increases with level\n\t    return (4 - _posterize_level_to_arg(level, hparams)[0], )\n\tdef _posterize_original_level_to_arg(level, _hparams):\n\t    # As per original AutoAugment paper description\n\t    # range [4, 8], 'keep 4 up to 8 MSB of image'\n\t    # intensity/severity of augmentation decreases with level\n\t    return (int((level / _MAX_LEVEL) * 4) + 4, )\n\tdef _solarize_level_to_arg(level, _hparams):\n\t    # range [0, 256]\n\t    # intensity/severity of augmentation decreases with level\n", "    return (int((level / _MAX_LEVEL) * 256), )\n\tdef _solarize_increasing_level_to_arg(level, _hparams):\n\t    # range [0, 256]\n\t    # intensity/severity of augmentation increases with level\n\t    return (256 - _solarize_level_to_arg(level, _hparams)[0], )\n\tdef _solarize_add_level_to_arg(level, _hparams):\n\t    # range [0, 110]\n\t    return (int((level / _MAX_LEVEL) * 110), )\n\tLEVEL_TO_ARG = {\n\t    \"AutoContrast\": None,\n", "    \"Equalize\": None,\n\t    \"Invert\": None,\n\t    \"Rotate\": _rotate_level_to_arg,\n\t    # There are several variations of the posterize level scaling in various Tensorflow/Google repositories/papers\n\t    \"Posterize\": _posterize_level_to_arg,\n\t    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,\n\t    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n\t    \"Solarize\": _solarize_level_to_arg,\n\t    \"SolarizeIncreasing\": _solarize_increasing_level_to_arg,\n\t    \"SolarizeAdd\": _solarize_add_level_to_arg,\n", "    \"Color\": _enhance_level_to_arg,\n\t    \"ColorIncreasing\": _enhance_increasing_level_to_arg,\n\t    \"Contrast\": _enhance_level_to_arg,\n\t    \"ContrastIncreasing\": _enhance_increasing_level_to_arg,\n\t    \"Brightness\": _enhance_level_to_arg,\n\t    \"BrightnessIncreasing\": _enhance_increasing_level_to_arg,\n\t    \"Sharpness\": _enhance_level_to_arg,\n\t    \"SharpnessIncreasing\": _enhance_increasing_level_to_arg,\n\t    \"ShearX\": _shear_level_to_arg,\n\t    \"ShearY\": _shear_level_to_arg,\n", "    \"TranslateX\": _translate_abs_level_to_arg,\n\t    \"TranslateY\": _translate_abs_level_to_arg,\n\t    \"TranslateXRel\": _translate_rel_level_to_arg,\n\t    \"TranslateYRel\": _translate_rel_level_to_arg,\n\t}\n\tNAME_TO_OP = {\n\t    \"AutoContrast\": auto_contrast,\n\t    \"Equalize\": equalize,\n\t    \"Invert\": invert,\n\t    \"Rotate\": rotate,\n", "    \"Posterize\": posterize,\n\t    \"PosterizeIncreasing\": posterize,\n\t    \"PosterizeOriginal\": posterize,\n\t    \"Solarize\": solarize,\n\t    \"SolarizeIncreasing\": solarize,\n\t    \"SolarizeAdd\": solarize_add,\n\t    \"Color\": color,\n\t    \"ColorIncreasing\": color,\n\t    \"Contrast\": contrast,\n\t    \"ContrastIncreasing\": contrast,\n", "    \"Brightness\": brightness,\n\t    \"BrightnessIncreasing\": brightness,\n\t    \"Sharpness\": sharpness,\n\t    \"SharpnessIncreasing\": sharpness,\n\t    \"ShearX\": shear_x,\n\t    \"ShearY\": shear_y,\n\t    \"TranslateX\": translate_x_abs,\n\t    \"TranslateY\": translate_y_abs,\n\t    \"TranslateXRel\": translate_x_rel,\n\t    \"TranslateYRel\": translate_y_rel,\n", "}\n\tclass AugmentOp:\n\t    \"\"\"\n\t    Apply for video.\n\t    \"\"\"\n\t    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n\t        hparams = hparams or _HPARAMS_DEFAULT\n\t        self.aug_fn = NAME_TO_OP[name]\n\t        self.level_fn = LEVEL_TO_ARG[name]\n\t        self.prob = prob\n", "        self.magnitude = magnitude\n\t        self.hparams = hparams.copy()\n\t        self.kwargs = {\n\t            \"fillcolor\":\n\t            hparams[\"img_mean\"] if \"img_mean\" in hparams else _FILL,\n\t            \"resample\":\n\t            hparams[\"interpolation\"]\n\t            if \"interpolation\" in hparams else _RANDOM_INTERPOLATION,\n\t        }\n\t        # If magnitude_std is > 0, we introduce some randomness\n", "        # in the usually fixed policy and sample magnitude from a normal distribution\n\t        # with mean `magnitude` and std-dev of `magnitude_std`.\n\t        # NOTE This is my own hack, being tested, not in papers or reference impls.\n\t        self.magnitude_std = self.hparams.get(\"magnitude_std\", 0)\n\t    def __call__(self, img_list):\n\t        if self.prob < 1.0 and random.random() > self.prob:\n\t            return img_list\n\t        magnitude = self.magnitude\n\t        if self.magnitude_std and self.magnitude_std > 0:\n\t            magnitude = random.gauss(magnitude, self.magnitude_std)\n", "        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range\n\t        level_args = (\n\t            self.level_fn(magnitude, self.hparams)\n\t            if self.level_fn is not None else ())\n\t        if isinstance(img_list, list):\n\t            return [\n\t                self.aug_fn(img, *level_args, **self.kwargs)\n\t                for img in img_list\n\t            ]\n\t        else:\n", "            return self.aug_fn(img_list, *level_args, **self.kwargs)\n\t_RAND_TRANSFORMS = [\n\t    \"AutoContrast\",\n\t    \"Equalize\",\n\t    \"Invert\",\n\t    \"Rotate\",\n\t    \"Posterize\",\n\t    \"Solarize\",\n\t    \"SolarizeAdd\",\n\t    \"Color\",\n", "    \"Contrast\",\n\t    \"Brightness\",\n\t    \"Sharpness\",\n\t    \"ShearX\",\n\t    \"ShearY\",\n\t    \"TranslateXRel\",\n\t    \"TranslateYRel\",\n\t]\n\t_RAND_INCREASING_TRANSFORMS = [\n\t    \"AutoContrast\",\n", "    \"Equalize\",\n\t    \"Invert\",\n\t    \"Rotate\",\n\t    \"PosterizeIncreasing\",\n\t    \"SolarizeIncreasing\",\n\t    \"SolarizeAdd\",\n\t    \"ColorIncreasing\",\n\t    \"ContrastIncreasing\",\n\t    \"BrightnessIncreasing\",\n\t    \"SharpnessIncreasing\",\n", "    \"ShearX\",\n\t    \"ShearY\",\n\t    \"TranslateXRel\",\n\t    \"TranslateYRel\",\n\t]\n\t# These experimental weights are based loosely on the relative improvements mentioned in paper.\n\t# They may not result in increased performance, but could likely be tuned to so.\n\t_RAND_CHOICE_WEIGHTS_0 = {\n\t    \"Rotate\": 0.3,\n\t    \"ShearX\": 0.2,\n", "    \"ShearY\": 0.2,\n\t    \"TranslateXRel\": 0.1,\n\t    \"TranslateYRel\": 0.1,\n\t    \"Color\": 0.025,\n\t    \"Sharpness\": 0.025,\n\t    \"AutoContrast\": 0.025,\n\t    \"Solarize\": 0.005,\n\t    \"SolarizeAdd\": 0.005,\n\t    \"Contrast\": 0.005,\n\t    \"Brightness\": 0.005,\n", "    \"Equalize\": 0.005,\n\t    \"Posterize\": 0,\n\t    \"Invert\": 0,\n\t}\n\tdef _select_rand_weights(weight_idx=0, transforms=None):\n\t    transforms = transforms or _RAND_TRANSFORMS\n\t    assert weight_idx == 0  # only one set of weights currently\n\t    rand_weights = _RAND_CHOICE_WEIGHTS_0\n\t    probs = [rand_weights[k] for k in transforms]\n\t    probs /= np.sum(probs)\n", "    return probs\n\tdef rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n\t    hparams = hparams or _HPARAMS_DEFAULT\n\t    transforms = transforms or _RAND_TRANSFORMS\n\t    return [\n\t        AugmentOp(name, prob=0.5, magnitude=magnitude, hparams=hparams)\n\t        for name in transforms\n\t    ]\n\tclass RandAugment:\n\t    def __init__(self, ops, num_layers=2, choice_weights=None):\n", "        self.ops = ops\n\t        self.num_layers = num_layers\n\t        self.choice_weights = choice_weights\n\t    def __call__(self, img):\n\t        # no replacement when using weighted choice\n\t        ops = np.random.choice(\n\t            self.ops,\n\t            self.num_layers,\n\t            replace=self.choice_weights is None,\n\t            p=self.choice_weights,\n", "        )\n\t        for op in ops:\n\t            img = op(img)\n\t        return img\n\tdef rand_augment_transform(config_str, hparams):\n\t    \"\"\"\n\t    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719\n\t    Create a RandAugment transform\n\t    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n\t    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n", "    sections, not order sepecific determine\n\t        'm' - integer magnitude of rand augment\n\t        'n' - integer num layers (number of transform ops selected per image)\n\t        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)\n\t        'mstd' -  float std deviation of magnitude noise applied\n\t        'inc' - integer (bool), use augmentations that increase in severity with magnitude (default: 0)\n\t    Ex 'rand-m9-n3-mstd0.5' results in RandAugment with magnitude 9, num_layers 3, magnitude_std 0.5\n\t    'rand-mstd1-w0' results in magnitude_std 1.0, weights 0, default magnitude of 10 and num_layers 2\n\t    :param hparams: Other hparams (kwargs) for the RandAugmentation scheme\n\t    :return: A PyTorch compatible Transform\n", "    \"\"\"\n\t    magnitude = _MAX_LEVEL  # default to _MAX_LEVEL for magnitude (currently 10)\n\t    num_layers = 2  # default to 2 ops per image\n\t    weight_idx = None  # default to no probability weights for op choice\n\t    transforms = _RAND_TRANSFORMS\n\t    config = config_str.split(\"-\")\n\t    assert config[0] == \"rand\"\n\t    config = config[1:]\n\t    for c in config:\n\t        cs = re.split(r\"(\\d.*)\", c)\n", "        if len(cs) < 2:\n\t            continue\n\t        key, val = cs[:2]\n\t        if key == \"mstd\":\n\t            # noise param injected via hparams for now\n\t            hparams.setdefault(\"magnitude_std\", float(val))\n\t        elif key == \"inc\":\n\t            if bool(val):\n\t                transforms = _RAND_INCREASING_TRANSFORMS\n\t        elif key == \"m\":\n", "            magnitude = int(val)\n\t        elif key == \"n\":\n\t            num_layers = int(val)\n\t        elif key == \"w\":\n\t            weight_idx = int(val)\n\t        else:\n\t            assert NotImplementedError\n\t    ra_ops = rand_augment_ops(\n\t        magnitude=magnitude, hparams=hparams, transforms=transforms)\n\t    choice_weights = (None if weight_idx is None else\n", "                      _select_rand_weights(weight_idx))\n\t    return RandAugment(ra_ops, num_layers, choice_weights=choice_weights)\n"]}
{"filename": "dataset/random_erasing.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\t\"\"\"\n\tThis implementation is based on\n\thttps://github.com/rwightman/pytorch-image-models/blob/master/timm/data/random_erasing.py\n\tpulished under an Apache License 2.0.\n\tCOMMENT FROM ORIGINAL:\n\tOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\n\tCopyright Zhun Zhong & Liang Zheng\n\tHacked together by / Copyright 2020 Ross Wightman\n\t\"\"\"\n", "import math\n\timport random\n\timport torch\n\tdef _get_pixels(per_pixel,\n\t                rand_color,\n\t                patch_size,\n\t                dtype=torch.float32,\n\t                device=\"cuda\"):\n\t    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n\t    # paths, flip the order so normal is run on CPU if this becomes a problem\n", "    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n\t    if per_pixel:\n\t        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n\t    elif rand_color:\n\t        return torch.empty((patch_size[0], 1, 1), dtype=dtype,\n\t                           device=device).normal_()\n\t    else:\n\t        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\tclass RandomErasing:\n\t    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n", "        'Random Erasing Data Augmentation' by Zhong et al.\n\t        See https://arxiv.org/pdf/1708.04896.pdf\n\t        This variant of RandomErasing is intended to be applied to either a batch\n\t        or single image tensor after it has been normalized by dataset mean and std.\n\t    Args:\n\t         probability: Probability that the Random Erasing operation will be performed.\n\t         min_area: Minimum percentage of erased area wrt input image area.\n\t         max_area: Maximum percentage of erased area wrt input image area.\n\t         min_aspect: Minimum aspect ratio of erased area.\n\t         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n", "            'const' - erase block is constant color of 0 for all channels\n\t            'rand'  - erase block is same per-channel random (normal) color\n\t            'pixel' - erase block is per-pixel random (normal) color\n\t        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n\t            per-image count is randomly chosen between 1 and this value.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        probability=0.5,\n\t        min_area=0.02,\n", "        max_area=1 / 3,\n\t        min_aspect=0.3,\n\t        max_aspect=None,\n\t        mode=\"const\",\n\t        min_count=1,\n\t        max_count=None,\n\t        num_splits=0,\n\t        device=\"cuda\",\n\t        cube=True,\n\t    ):\n", "        self.probability = probability\n\t        self.min_area = min_area\n\t        self.max_area = max_area\n\t        max_aspect = max_aspect or 1 / min_aspect\n\t        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n\t        self.min_count = min_count\n\t        self.max_count = max_count or min_count\n\t        self.num_splits = num_splits\n\t        mode = mode.lower()\n\t        self.rand_color = False\n", "        self.per_pixel = False\n\t        self.cube = cube\n\t        if mode == \"rand\":\n\t            self.rand_color = True  # per block random normal\n\t        elif mode == \"pixel\":\n\t            self.per_pixel = True  # per pixel random normal\n\t        else:\n\t            assert not mode or mode == \"const\"\n\t        self.device = device\n\t    def _erase(self, img, chan, img_h, img_w, dtype):\n", "        if random.random() > self.probability:\n\t            return\n\t        area = img_h * img_w\n\t        count = (\n\t            self.min_count if self.min_count == self.max_count else\n\t            random.randint(self.min_count, self.max_count))\n\t        for _ in range(count):\n\t            for _ in range(10):\n\t                target_area = (\n\t                    random.uniform(self.min_area, self.max_area) * area /\n", "                    count)\n\t                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n\t                h = int(round(math.sqrt(target_area * aspect_ratio)))\n\t                w = int(round(math.sqrt(target_area / aspect_ratio)))\n\t                if w < img_w and h < img_h:\n\t                    top = random.randint(0, img_h - h)\n\t                    left = random.randint(0, img_w - w)\n\t                    img[:, top:top + h, left:left + w] = _get_pixels(\n\t                        self.per_pixel,\n\t                        self.rand_color,\n", "                        (chan, h, w),\n\t                        dtype=dtype,\n\t                        device=self.device,\n\t                    )\n\t                    break\n\t    def _erase_cube(\n\t        self,\n\t        img,\n\t        batch_start,\n\t        batch_size,\n", "        chan,\n\t        img_h,\n\t        img_w,\n\t        dtype,\n\t    ):\n\t        if random.random() > self.probability:\n\t            return\n\t        area = img_h * img_w\n\t        count = (\n\t            self.min_count if self.min_count == self.max_count else\n", "            random.randint(self.min_count, self.max_count))\n\t        for _ in range(count):\n\t            for _ in range(100):\n\t                target_area = (\n\t                    random.uniform(self.min_area, self.max_area) * area /\n\t                    count)\n\t                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n\t                h = int(round(math.sqrt(target_area * aspect_ratio)))\n\t                w = int(round(math.sqrt(target_area / aspect_ratio)))\n\t                if w < img_w and h < img_h:\n", "                    top = random.randint(0, img_h - h)\n\t                    left = random.randint(0, img_w - w)\n\t                    for i in range(batch_start, batch_size):\n\t                        img_instance = img[i]\n\t                        img_instance[:, top:top + h,\n\t                                     left:left + w] = _get_pixels(\n\t                                         self.per_pixel,\n\t                                         self.rand_color,\n\t                                         (chan, h, w),\n\t                                         dtype=dtype,\n", "                                         device=self.device,\n\t                                     )\n\t                    break\n\t    def __call__(self, input):\n\t        if len(input.size()) == 3:\n\t            self._erase(input, *input.size(), input.dtype)\n\t        else:\n\t            batch_size, chan, img_h, img_w = input.size()\n\t            # skip first slice of batch if num_splits is set (for clean portion of samples)\n\t            batch_start = (\n", "                batch_size // self.num_splits if self.num_splits > 1 else 0)\n\t            if self.cube:\n\t                self._erase_cube(\n\t                    input,\n\t                    batch_start,\n\t                    batch_size,\n\t                    chan,\n\t                    img_h,\n\t                    img_w,\n\t                    input.dtype,\n", "                )\n\t            else:\n\t                for i in range(batch_start, batch_size):\n\t                    self._erase(input[i], chan, img_h, img_w, input.dtype)\n\t        return input\n"]}
{"filename": "dataset/pretrain_datasets.py", "chunked_list": ["import os\n\timport random\n\timport numpy as np\n\timport torch\n\tfrom PIL import Image\n\tfrom torchvision import transforms\n\tfrom .loader import get_image_loader, get_video_loader\n\tfrom .masking_generator import (\n\t    RunningCellMaskingGenerator,\n\t    TubeMaskingGenerator,\n", ")\n\tfrom .transforms import (\n\t    GroupMultiScaleCrop,\n\t    GroupNormalize,\n\t    Stack,\n\t    ToTorchFormatTensor,\n\t)\n\tclass DataAugmentationForVideoMAEv2(object):\n\t    def __init__(self, args):\n\t        self.input_mean = [0.485, 0.456, 0.406]\n", "        self.input_std = [0.229, 0.224, 0.225]\n\t        div = True\n\t        roll = False\n\t        normalize = GroupNormalize(self.input_mean, self.input_std)\n\t        self.train_augmentation = GroupMultiScaleCrop(args.input_size,\n\t                                                      [1, .875, .75, .66])\n\t        self.transform = transforms.Compose([\n\t            self.train_augmentation,\n\t            Stack(roll=roll),\n\t            ToTorchFormatTensor(div=div),\n", "            normalize,\n\t        ])\n\t        if args.mask_type == 'tube':\n\t            self.encoder_mask_map_generator = TubeMaskingGenerator(\n\t                args.window_size, args.mask_ratio)\n\t        else:\n\t            raise NotImplementedError(\n\t                'Unsupported encoder masking strategy type.')\n\t        if args.decoder_mask_ratio > 0.:\n\t            if args.decoder_mask_type == 'run_cell':\n", "                self.decoder_mask_map_generator = RunningCellMaskingGenerator(\n\t                    args.window_size, args.decoder_mask_ratio)\n\t            else:\n\t                raise NotImplementedError(\n\t                    'Unsupported decoder masking strategy type.')\n\t    def __call__(self, images):\n\t        process_data, _ = self.transform(images)\n\t        encoder_mask_map = self.encoder_mask_map_generator()\n\t        if hasattr(self, 'decoder_mask_map_generator'):\n\t            decoder_mask_map = self.decoder_mask_map_generator()\n", "        else:\n\t            decoder_mask_map = 1 - encoder_mask_map\n\t        return process_data, encoder_mask_map, decoder_mask_map\n\t    def __repr__(self):\n\t        repr = \"(DataAugmentationForVideoMAEv2,\\n\"\n\t        repr += \"  transform = %s,\\n\" % str(self.transform)\n\t        repr += \"  Encoder Masking Generator = %s,\\n\" % str(\n\t            self.encoder_mask_map_generator)\n\t        if hasattr(self, 'decoder_mask_map_generator'):\n\t            repr += \"  Decoder Masking Generator = %s,\\n\" % str(\n", "                self.decoder_mask_map_generator)\n\t        else:\n\t            repr += \"  Do not use decoder masking,\\n\"\n\t        repr += \")\"\n\t        return repr\n\tclass HybridVideoMAE(torch.utils.data.Dataset):\n\t    \"\"\"Load your own videomae pretraining dataset.\n\t    Parameters\n\t    ----------\n\t    root : str, required.\n", "        Path to the root folder storing the dataset.\n\t    setting : str, required.\n\t        A text file describing the dataset, each line per video sample.\n\t        There are four items in each line:\n\t        (1) video path; (2) start_idx, (3) total frames and (4) video label.\n\t        for pre-train video data\n\t            total frames < 0, start_idx and video label meaningless\n\t        for pre-train rawframe data\n\t            video label meaningless\n\t    train : bool, default True.\n", "        Whether to load the training or validation set.\n\t    test_mode : bool, default False.\n\t        Whether to perform evaluation on the test set.\n\t        Usually there is three-crop or ten-crop evaluation strategy involved.\n\t    name_pattern : str, default 'img_{:05}.jpg'.\n\t        The naming pattern of the decoded video frames.\n\t        For example, img_00012.jpg.\n\t    video_ext : str, default 'mp4'.\n\t        If video_loader is set to True, please specify the video format accordinly.\n\t    is_color : bool, default True.\n", "        Whether the loaded image is color or grayscale.\n\t    modality : str, default 'rgb'.\n\t        Input modalities, we support only rgb video frames for now.\n\t        Will add support for rgb difference image and optical flow image later.\n\t    num_segments : int, default 1.\n\t        Number of segments to evenly divide the video into clips.\n\t        A useful technique to obtain global video-level information.\n\t        Limin Wang, etal, Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, ECCV 2016.\n\t    num_crop : int, default 1.\n\t        Number of crops for each image. default is 1.\n", "        Common choices are three crops and ten crops during evaluation.\n\t    new_length : int, default 1.\n\t        The length of input video clip. Default is a single image, but it can be multiple video frames.\n\t        For example, new_length=16 means we will extract a video clip of consecutive 16 frames.\n\t    new_step : int, default 1.\n\t        Temporal sampling rate. For example, new_step=1 means we will extract a video clip of consecutive frames.\n\t        new_step=2 means we will extract a video clip of every other frame.\n\t    transform : function, default None.\n\t        A function that takes data and label and transforms them.\n\t    temporal_jitter : bool, default False.\n", "        Whether to temporally jitter if new_step > 1.\n\t    lazy_init : bool, default False.\n\t        If set to True, build a dataset instance without loading any dataset.\n\t    num_sample : int, default 1.\n\t        Number of sampled views for Repeated Augmentation.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 root,\n\t                 setting,\n\t                 train=True,\n", "                 test_mode=False,\n\t                 name_pattern='img_{:05}.jpg',\n\t                 video_ext='mp4',\n\t                 is_color=True,\n\t                 modality='rgb',\n\t                 num_segments=1,\n\t                 num_crop=1,\n\t                 new_length=1,\n\t                 new_step=1,\n\t                 transform=None,\n", "                 temporal_jitter=False,\n\t                 lazy_init=False,\n\t                 num_sample=1):\n\t        super(HybridVideoMAE, self).__init__()\n\t        self.root = root\n\t        self.setting = setting\n\t        self.train = train\n\t        self.test_mode = test_mode\n\t        self.is_color = is_color\n\t        self.modality = modality\n", "        self.num_segments = num_segments\n\t        self.num_crop = num_crop\n\t        self.new_length = new_length\n\t        self.new_step = new_step\n\t        self.skip_length = self.new_length * self.new_step\n\t        self.temporal_jitter = temporal_jitter\n\t        self.name_pattern = name_pattern\n\t        self.video_ext = video_ext\n\t        self.transform = transform\n\t        self.lazy_init = lazy_init\n", "        self.num_sample = num_sample\n\t        # NOTE:\n\t        # for hybrid train\n\t        # different frame naming formats are used for different datasets\n\t        # should MODIFY the fname_tmpl to your own situation\n\t        self.ava_fname_tmpl = 'image_{:06}.jpg'\n\t        self.ssv2_fname_tmpl = 'img_{:05}.jpg'\n\t        # NOTE:\n\t        # we set sampling_rate = 2 for ssv2\n\t        # thus being consistent with the fine-tuning stage\n", "        # Note that the ssv2 we use is decoded to frames at 12 fps;\n\t        # if decoded at 24 fps, the sample interval should be 4.\n\t        self.ssv2_skip_length = self.new_length * 2\n\t        self.orig_skip_length = self.skip_length\n\t        self.video_loader = get_video_loader()\n\t        self.image_loader = get_image_loader()\n\t        if not self.lazy_init:\n\t            self.clips = self._make_dataset(root, setting)\n\t            if len(self.clips) == 0:\n\t                raise (\n", "                    RuntimeError(\"Found 0 video clips in subfolders of: \" +\n\t                                 root + \"\\n\"\n\t                                 \"Check your data directory (opt.data-dir).\"))\n\t    def __getitem__(self, index):\n\t        try:\n\t            video_name, start_idx, total_frame = self.clips[index]\n\t            self.skip_length = self.orig_skip_length\n\t            if total_frame < 0:\n\t                decord_vr = self.video_loader(video_name)\n\t                duration = len(decord_vr)\n", "                segment_indices, skip_offsets = self._sample_train_indices(\n\t                    duration)\n\t                frame_id_list = self.get_frame_id_list(duration,\n\t                                                       segment_indices,\n\t                                                       skip_offsets)\n\t                video_data = decord_vr.get_batch(frame_id_list).asnumpy()\n\t                images = [\n\t                    Image.fromarray(video_data[vid, :, :, :]).convert('RGB')\n\t                    for vid, _ in enumerate(frame_id_list)\n\t                ]\n", "            else:\n\t                # ssv2 & ava & other rawframe dataset\n\t                if 'SomethingV2' in video_name:\n\t                    self.skip_length = self.ssv2_skip_length\n\t                    fname_tmpl = self.ssv2_fname_tmpl\n\t                elif 'AVA2.2' in video_name:\n\t                    fname_tmpl = self.ava_fname_tmpl\n\t                else:\n\t                    fname_tmpl = self.name_pattern\n\t                segment_indices, skip_offsets = self._sample_train_indices(\n", "                    total_frame)\n\t                frame_id_list = self.get_frame_id_list(total_frame,\n\t                                                       segment_indices,\n\t                                                       skip_offsets)\n\t                images = []\n\t                for idx in frame_id_list:\n\t                    frame_fname = os.path.join(\n\t                        video_name, fname_tmpl.format(idx + start_idx))\n\t                    img = self.image_loader(frame_fname)\n\t                    img = Image.fromarray(img)\n", "                    images.append(img)\n\t        except Exception as e:\n\t            print(\"Failed to load video from {} with error {}\".format(\n\t                video_name, e))\n\t            index = random.randint(0, len(self.clips) - 1)\n\t            return self.__getitem__(index)\n\t        if self.num_sample > 1:\n\t            process_data_list = []\n\t            encoder_mask_list = []\n\t            decoder_mask_list = []\n", "            for _ in range(self.num_sample):\n\t                process_data, encoder_mask, decoder_mask = self.transform(\n\t                    (images, None))\n\t                process_data = process_data.view(\n\t                    (self.new_length, 3) + process_data.size()[-2:]).transpose(\n\t                        0, 1)\n\t                process_data_list.append(process_data)\n\t                encoder_mask_list.append(encoder_mask)\n\t                decoder_mask_list.append(decoder_mask)\n\t            return process_data_list, encoder_mask_list, decoder_mask_list\n", "        else:\n\t            process_data, encoder_mask, decoder_mask = self.transform(\n\t                (images, None))\n\t            # T*C,H,W -> T,C,H,W -> C,T,H,W\n\t            process_data = process_data.view(\n\t                (self.new_length, 3) + process_data.size()[-2:]).transpose(\n\t                    0, 1)\n\t            return process_data, encoder_mask, decoder_mask\n\t    def __len__(self):\n\t        return len(self.clips)\n", "    def _make_dataset(self, root, setting):\n\t        if not os.path.exists(setting):\n\t            raise (RuntimeError(\n\t                \"Setting file %s doesn't exist. Check opt.train-list and opt.val-list. \"\n\t                % (setting)))\n\t        clips = []\n\t        with open(setting) as split_f:\n\t            data = split_f.readlines()\n\t            for line in data:\n\t                line_info = line.split(' ')\n", "                # line format: video_path, video_duration, video_label\n\t                if len(line_info) < 2:\n\t                    raise (RuntimeError(\n\t                        'Video input format is not correct, missing one or more element. %s'\n\t                        % line))\n\t                clip_path = os.path.join(root, line_info[0])\n\t                start_idx = int(line_info[1])\n\t                total_frame = int(line_info[2])\n\t                item = (clip_path, start_idx, total_frame)\n\t                clips.append(item)\n", "        return clips\n\t    def _sample_train_indices(self, num_frames):\n\t        average_duration = (num_frames - self.skip_length +\n\t                            1) // self.num_segments\n\t        if average_duration > 0:\n\t            offsets = np.multiply(\n\t                list(range(self.num_segments)), average_duration)\n\t            offsets = offsets + np.random.randint(\n\t                average_duration, size=self.num_segments)\n\t        elif num_frames > max(self.num_segments, self.skip_length):\n", "            offsets = np.sort(\n\t                np.random.randint(\n\t                    num_frames - self.skip_length + 1, size=self.num_segments))\n\t        else:\n\t            offsets = np.zeros((self.num_segments, ))\n\t        if self.temporal_jitter:\n\t            skip_offsets = np.random.randint(\n\t                self.new_step, size=self.skip_length // self.new_step)\n\t        else:\n\t            skip_offsets = np.zeros(\n", "                self.skip_length // self.new_step, dtype=int)\n\t        return offsets + 1, skip_offsets\n\t    def get_frame_id_list(self, duration, indices, skip_offsets):\n\t        frame_id_list = []\n\t        for seg_ind in indices:\n\t            offset = int(seg_ind)\n\t            for i, _ in enumerate(range(0, self.skip_length, self.new_step)):\n\t                if offset + skip_offsets[i] <= duration:\n\t                    frame_id = offset + skip_offsets[i] - 1\n\t                else:\n", "                    frame_id = offset - 1\n\t                frame_id_list.append(frame_id)\n\t                if offset + self.new_step < duration:\n\t                    offset += self.new_step\n\t        return frame_id_list\n\tclass VideoMAE(torch.utils.data.Dataset):\n\t    \"\"\"Load your own videomae pretraining dataset.\n\t    Parameters\n\t    ----------\n\t    root : str, required.\n", "        Path to the root folder storing the dataset.\n\t    setting : str, required.\n\t        A text file describing the dataset, each line per video sample.\n\t        There are four items in each line:\n\t        (1) video path; (2) start_idx, (3) total frames and (4) video label.\n\t        for pre-train video data\n\t            total frames < 0, start_idx and video label meaningless\n\t        for pre-train rawframe data\n\t            video label meaningless\n\t    train : bool, default True.\n", "        Whether to load the training or validation set.\n\t    test_mode : bool, default False.\n\t        Whether to perform evaluation on the test set.\n\t        Usually there is three-crop or ten-crop evaluation strategy involved.\n\t    name_pattern : str, default 'img_{:05}.jpg'.\n\t        The naming pattern of the decoded video frames.\n\t        For example, img_00012.jpg.\n\t    video_ext : str, default 'mp4'.\n\t        If video_loader is set to True, please specify the video format accordinly.\n\t    is_color : bool, default True.\n", "        Whether the loaded image is color or grayscale.\n\t    modality : str, default 'rgb'.\n\t        Input modalities, we support only rgb video frames for now.\n\t        Will add support for rgb difference image and optical flow image later.\n\t    num_segments : int, default 1.\n\t        Number of segments to evenly divide the video into clips.\n\t        A useful technique to obtain global video-level information.\n\t        Limin Wang, etal, Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, ECCV 2016.\n\t    num_crop : int, default 1.\n\t        Number of crops for each image. default is 1.\n", "        Common choices are three crops and ten crops during evaluation.\n\t    new_length : int, default 1.\n\t        The length of input video clip. Default is a single image, but it can be multiple video frames.\n\t        For example, new_length=16 means we will extract a video clip of consecutive 16 frames.\n\t    new_step : int, default 1.\n\t        Temporal sampling rate. For example, new_step=1 means we will extract a video clip of consecutive frames.\n\t        new_step=2 means we will extract a video clip of every other frame.\n\t    transform : function, default None.\n\t        A function that takes data and label and transforms them.\n\t    temporal_jitter : bool, default False.\n", "        Whether to temporally jitter if new_step > 1.\n\t    lazy_init : bool, default False.\n\t        If set to True, build a dataset instance without loading any dataset.\n\t    num_sample : int, default 1.\n\t        Number of sampled views for Repeated Augmentation.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 root,\n\t                 setting,\n\t                 train=True,\n", "                 test_mode=False,\n\t                 name_pattern='img_{:05}.jpg',\n\t                 video_ext='mp4',\n\t                 is_color=True,\n\t                 modality='rgb',\n\t                 num_segments=1,\n\t                 num_crop=1,\n\t                 new_length=1,\n\t                 new_step=1,\n\t                 transform=None,\n", "                 temporal_jitter=False,\n\t                 lazy_init=False,\n\t                 num_sample=1):\n\t        super(VideoMAE, self).__init__()\n\t        self.root = root\n\t        self.setting = setting\n\t        self.train = train\n\t        self.test_mode = test_mode\n\t        self.is_color = is_color\n\t        self.modality = modality\n", "        self.num_segments = num_segments\n\t        self.num_crop = num_crop\n\t        self.new_length = new_length\n\t        self.new_step = new_step\n\t        self.skip_length = self.new_length * self.new_step\n\t        self.temporal_jitter = temporal_jitter\n\t        self.name_pattern = name_pattern\n\t        self.video_ext = video_ext\n\t        self.transform = transform\n\t        self.lazy_init = lazy_init\n", "        self.num_sample = num_sample\n\t        self.video_loader = get_video_loader()\n\t        self.image_loader = get_image_loader()\n\t        if not self.lazy_init:\n\t            self.clips = self._make_dataset(root, setting)\n\t            if len(self.clips) == 0:\n\t                raise (\n\t                    RuntimeError(\"Found 0 video clips in subfolders of: \" +\n\t                                 root + \"\\n\"\n\t                                 \"Check your data directory (opt.data-dir).\"))\n", "    def __getitem__(self, index):\n\t        try:\n\t            video_name, start_idx, total_frame = self.clips[index]\n\t            if total_frame < 0:  # load video\n\t                decord_vr = self.video_loader(video_name)\n\t                duration = len(decord_vr)\n\t                segment_indices, skip_offsets = self._sample_train_indices(\n\t                    duration)\n\t                frame_id_list = self.get_frame_id_list(duration,\n\t                                                       segment_indices,\n", "                                                       skip_offsets)\n\t                video_data = decord_vr.get_batch(frame_id_list).asnumpy()\n\t                images = [\n\t                    Image.fromarray(video_data[vid, :, :, :]).convert('RGB')\n\t                    for vid, _ in enumerate(frame_id_list)\n\t                ]\n\t            else:  # load frames\n\t                segment_indices, skip_offsets = self._sample_train_indices(\n\t                    total_frame)\n\t                frame_id_list = self.get_frame_id_list(total_frame,\n", "                                                       segment_indices,\n\t                                                       skip_offsets)\n\t                images = []\n\t                for idx in frame_id_list:\n\t                    frame_fname = os.path.join(\n\t                        video_name, self.name_pattern.format(idx + start_idx))\n\t                    img = self.image_loader(frame_fname)\n\t                    img = Image.fromarray(img)\n\t                    images.append(img)\n\t        except Exception as e:\n", "            print(\"Failed to load video from {} with error {}\".format(\n\t                video_name, e))\n\t            index = random.randint(0, len(self.clips) - 1)\n\t            return self.__getitem__(index)\n\t        if self.num_sample > 1:\n\t            process_data_list = []\n\t            encoder_mask_list = []\n\t            decoder_mask_list = []\n\t            for _ in range(self.num_sample):\n\t                process_data, encoder_mask, decoder_mask = self.transform(\n", "                    (images, None))\n\t                process_data = process_data.view(\n\t                    (self.new_length, 3) + process_data.size()[-2:]).transpose(\n\t                        0, 1)\n\t                process_data_list.append(process_data)\n\t                encoder_mask_list.append(encoder_mask)\n\t                decoder_mask_list.append(decoder_mask)\n\t            return process_data_list, encoder_mask_list, decoder_mask_list\n\t        else:\n\t            process_data, encoder_mask, decoder_mask = self.transform(\n", "                (images, None))\n\t            # T*C,H,W -> T,C,H,W -> C,T,H,W\n\t            process_data = process_data.view(\n\t                (self.new_length, 3) + process_data.size()[-2:]).transpose(\n\t                    0, 1)\n\t            return process_data, encoder_mask, decoder_mask\n\t    def __len__(self):\n\t        return len(self.clips)\n\t    def _make_dataset(self, root, setting):\n\t        if not os.path.exists(setting):\n", "            raise (RuntimeError(\n\t                \"Setting file %s doesn't exist. Check opt.train-list and opt.val-list. \"\n\t                % (setting)))\n\t        clips = []\n\t        with open(setting) as split_f:\n\t            data = split_f.readlines()\n\t            for line in data:\n\t                line_info = line.split(' ')\n\t                # line format: video_path, start_idx, total_frames\n\t                if len(line_info) < 3:\n", "                    raise (RuntimeError(\n\t                        'Video input format is not correct, missing one or more element. %s'\n\t                        % line))\n\t                clip_path = os.path.join(root, line_info[0])\n\t                start_idx = int(line_info[1])\n\t                total_frame = int(line_info[2])\n\t                item = (clip_path, start_idx, total_frame)\n\t                clips.append(item)\n\t        return clips\n\t    def _sample_train_indices(self, num_frames):\n", "        average_duration = (num_frames - self.skip_length +\n\t                            1) // self.num_segments\n\t        if average_duration > 0:\n\t            offsets = np.multiply(\n\t                list(range(self.num_segments)), average_duration)\n\t            offsets = offsets + np.random.randint(\n\t                average_duration, size=self.num_segments)\n\t        elif num_frames > max(self.num_segments, self.skip_length):\n\t            offsets = np.sort(\n\t                np.random.randint(\n", "                    num_frames - self.skip_length + 1, size=self.num_segments))\n\t        else:\n\t            offsets = np.zeros((self.num_segments, ))\n\t        if self.temporal_jitter:\n\t            skip_offsets = np.random.randint(\n\t                self.new_step, size=self.skip_length // self.new_step)\n\t        else:\n\t            skip_offsets = np.zeros(\n\t                self.skip_length // self.new_step, dtype=int)\n\t        return offsets + 1, skip_offsets\n", "    def get_frame_id_list(self, duration, indices, skip_offsets):\n\t        frame_id_list = []\n\t        for seg_ind in indices:\n\t            offset = int(seg_ind)\n\t            for i, _ in enumerate(range(0, self.skip_length, self.new_step)):\n\t                if offset + skip_offsets[i] <= duration:\n\t                    frame_id = offset + skip_offsets[i] - 1\n\t                else:\n\t                    frame_id = offset - 1\n\t                frame_id_list.append(frame_id)\n", "                if offset + self.new_step < duration:\n\t                    offset += self.new_step\n\t        return frame_id_list\n"]}
{"filename": "dataset/__init__.py", "chunked_list": ["from .build import build_dataset, build_pretraining_dataset\n\t__all__ = ['build_dataset', 'build_pretraining_dataset']\n"]}
{"filename": "dataset/volume_transforms.py", "chunked_list": ["import numpy as np\n\timport torch\n\tfrom PIL import Image\n\tdef convert_img(img):\n\t    \"\"\"Converts (H, W, C) numpy.ndarray to (C, W, H) format\n\t    \"\"\"\n\t    if len(img.shape) == 3:\n\t        img = img.transpose(2, 0, 1)\n\t    if len(img.shape) == 2:\n\t        img = np.expand_dims(img, 0)\n", "    return img\n\tclass ClipToTensor(object):\n\t    \"\"\"Convert a list of m (H x W x C) numpy.ndarrays in the range [0, 255]\n\t    to a torch.FloatTensor of shape (C x m x H x W) in the range [0, 1.0]\n\t    \"\"\"\n\t    def __init__(self, channel_nb=3, div_255=True, numpy=False):\n\t        self.channel_nb = channel_nb\n\t        self.div_255 = div_255\n\t        self.numpy = numpy\n\t    def __call__(self, clip):\n", "        \"\"\"\n\t        Args: clip (list of numpy.ndarray): clip (list of images)\n\t        to be converted to tensor.\n\t        \"\"\"\n\t        # Retrieve shape\n\t        if isinstance(clip[0], np.ndarray):\n\t            h, w, ch = clip[0].shape\n\t            assert ch == self.channel_nb, 'Got {0} instead of 3 channels'.format(\n\t                ch)\n\t        elif isinstance(clip[0], Image.Image):\n", "            w, h = clip[0].size\n\t        else:\n\t            raise TypeError('Expected numpy.ndarray or PIL.Image\\\n\t            but got list of {0}'.format(type(clip[0])))\n\t        np_clip = np.zeros([self.channel_nb, len(clip), int(h), int(w)])\n\t        # Convert\n\t        for img_idx, img in enumerate(clip):\n\t            if isinstance(img, np.ndarray):\n\t                pass\n\t            elif isinstance(img, Image.Image):\n", "                img = np.array(img, copy=False)\n\t            else:\n\t                raise TypeError('Expected numpy.ndarray or PIL.Image\\\n\t                but got list of {0}'.format(type(clip[0])))\n\t            img = convert_img(img)\n\t            np_clip[:, img_idx, :, :] = img\n\t        if self.numpy:\n\t            if self.div_255:\n\t                np_clip = np_clip / 255.0\n\t            return np_clip\n", "        else:\n\t            tensor_clip = torch.from_numpy(np_clip)\n\t            if not isinstance(tensor_clip, torch.FloatTensor):\n\t                tensor_clip = tensor_clip.float()\n\t            if self.div_255:\n\t                tensor_clip = torch.div(tensor_clip, 255)\n\t            return tensor_clip\n\t# Note this norms data to -1/1\n\tclass ClipToTensor_K(object):\n\t    \"\"\"Convert a list of m (H x W x C) numpy.ndarrays in the range [0, 255]\n", "    to a torch.FloatTensor of shape (C x m x H x W) in the range [0, 1.0]\n\t    \"\"\"\n\t    def __init__(self, channel_nb=3, div_255=True, numpy=False):\n\t        self.channel_nb = channel_nb\n\t        self.div_255 = div_255\n\t        self.numpy = numpy\n\t    def __call__(self, clip):\n\t        \"\"\"\n\t        Args: clip (list of numpy.ndarray): clip (list of images)\n\t        to be converted to tensor.\n", "        \"\"\"\n\t        # Retrieve shape\n\t        if isinstance(clip[0], np.ndarray):\n\t            h, w, ch = clip[0].shape\n\t            assert ch == self.channel_nb, 'Got {0} instead of 3 channels'.format(\n\t                ch)\n\t        elif isinstance(clip[0], Image.Image):\n\t            w, h = clip[0].size\n\t        else:\n\t            raise TypeError('Expected numpy.ndarray or PIL.Image\\\n", "            but got list of {0}'.format(type(clip[0])))\n\t        np_clip = np.zeros([self.channel_nb, len(clip), int(h), int(w)])\n\t        # Convert\n\t        for img_idx, img in enumerate(clip):\n\t            if isinstance(img, np.ndarray):\n\t                pass\n\t            elif isinstance(img, Image.Image):\n\t                img = np.array(img, copy=False)\n\t            else:\n\t                raise TypeError('Expected numpy.ndarray or PIL.Image\\\n", "                but got list of {0}'.format(type(clip[0])))\n\t            img = convert_img(img)\n\t            np_clip[:, img_idx, :, :] = img\n\t        if self.numpy:\n\t            if self.div_255:\n\t                np_clip = (np_clip - 127.5) / 127.5\n\t            return np_clip\n\t        else:\n\t            tensor_clip = torch.from_numpy(np_clip)\n\t            if not isinstance(tensor_clip, torch.FloatTensor):\n", "                tensor_clip = tensor_clip.float()\n\t            if self.div_255:\n\t                tensor_clip = torch.div(torch.sub(tensor_clip, 127.5), 127.5)\n\t            return tensor_clip\n\tclass ToTensor(object):\n\t    \"\"\"Converts numpy array to tensor\n\t    \"\"\"\n\t    def __call__(self, array):\n\t        tensor = torch.from_numpy(array)\n\t        return tensor\n"]}
{"filename": "dataset/build.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\timport os\n\tfrom .datasets import RawFrameClsDataset, VideoClsDataset\n\tfrom .pretrain_datasets import (  # noqa: F401\n", "    DataAugmentationForVideoMAEv2, HybridVideoMAE, VideoMAE,\n\t)\n\tdef build_pretraining_dataset(args):\n\t    transform = DataAugmentationForVideoMAEv2(args)\n\t    dataset = VideoMAE(\n\t        root=args.data_root,\n\t        setting=args.data_path,\n\t        train=True,\n\t        test_mode=False,\n\t        name_pattern=args.fname_tmpl,\n", "        video_ext='mp4',\n\t        is_color=True,\n\t        modality='rgb',\n\t        num_segments=1,\n\t        num_crop=1,\n\t        new_length=args.num_frames,\n\t        new_step=args.sampling_rate,\n\t        transform=transform,\n\t        temporal_jitter=False,\n\t        lazy_init=False,\n", "        num_sample=args.num_sample)\n\t    print(\"Data Aug = %s\" % str(transform))\n\t    return dataset\n\tdef build_dataset(is_train, test_mode, args):\n\t    if is_train:\n\t        mode = 'train'\n\t        anno_path = os.path.join(args.data_path, 'train.csv')\n\t    elif test_mode:\n\t        mode = 'test'\n\t        anno_path = os.path.join(args.data_path, 'val.csv')\n", "    else:\n\t        mode = 'validation'\n\t        anno_path = os.path.join(args.data_path, 'val.csv')\n\t    if args.data_set == 'Kinetics-400':\n\t        if not args.sparse_sample:\n\t            dataset = VideoClsDataset(\n\t                anno_path=anno_path,\n\t                data_root=args.data_root,\n\t                mode=mode,\n\t                clip_len=args.num_frames,\n", "                frame_sample_rate=args.sampling_rate,\n\t                num_segment=1,\n\t                test_num_segment=args.test_num_segment,\n\t                test_num_crop=args.test_num_crop,\n\t                num_crop=1 if not test_mode else 3,\n\t                keep_aspect_ratio=True,\n\t                crop_size=args.input_size,\n\t                short_side_size=args.short_side_size,\n\t                new_height=256,\n\t                new_width=320,\n", "                sparse_sample=False,\n\t                args=args)\n\t        else:\n\t            dataset = VideoClsDataset(\n\t                anno_path=anno_path,\n\t                data_root=args.data_root,\n\t                mode=mode,\n\t                clip_len=1,\n\t                frame_sample_rate=1,\n\t                num_segment=args.num_frames,\n", "                test_num_segment=args.test_num_segment,\n\t                test_num_crop=args.test_num_crop,\n\t                num_crop=1 if not test_mode else 3,\n\t                keep_aspect_ratio=True,\n\t                crop_size=args.input_size,\n\t                short_side_size=args.short_side_size,\n\t                new_height=256,\n\t                new_width=320,\n\t                sparse_sample=True,\n\t                args=args)\n", "        nb_classes = 400\n\t    elif args.data_set == 'Kinetics-600':\n\t        dataset = VideoClsDataset(\n\t            anno_path=anno_path,\n\t            data_root=args.data_root,\n\t            mode=mode,\n\t            clip_len=args.num_frames,\n\t            frame_sample_rate=args.sampling_rate,\n\t            num_segment=1,\n\t            test_num_segment=args.test_num_segment,\n", "            test_num_crop=args.test_num_crop,\n\t            num_crop=1 if not test_mode else 3,\n\t            keep_aspect_ratio=True,\n\t            crop_size=args.input_size,\n\t            short_side_size=args.short_side_size,\n\t            new_height=256,\n\t            new_width=320,\n\t            args=args)\n\t        nb_classes = 600\n\t    elif args.data_set == 'Kinetics-700':\n", "        dataset = VideoClsDataset(\n\t            anno_path=anno_path,\n\t            data_root=args.data_root,\n\t            mode=mode,\n\t            clip_len=args.num_frames,\n\t            frame_sample_rate=args.sampling_rate,\n\t            num_segment=1,\n\t            test_num_segment=args.test_num_segment,\n\t            test_num_crop=args.test_num_crop,\n\t            num_crop=1 if not test_mode else 3,\n", "            keep_aspect_ratio=True,\n\t            crop_size=args.input_size,\n\t            short_side_size=args.short_side_size,\n\t            new_height=256,\n\t            new_width=320,\n\t            args=args)\n\t        nb_classes = 700\n\t    elif args.data_set == 'Kinetics-710':\n\t        dataset = VideoClsDataset(\n\t            anno_path=anno_path,\n", "            data_root=args.data_root,\n\t            mode=mode,\n\t            clip_len=args.num_frames,\n\t            frame_sample_rate=args.sampling_rate,\n\t            num_segment=1,\n\t            test_num_segment=args.test_num_segment,\n\t            test_num_crop=args.test_num_crop,\n\t            num_crop=1 if not test_mode else 3,\n\t            keep_aspect_ratio=True,\n\t            crop_size=args.input_size,\n", "            short_side_size=args.short_side_size,\n\t            new_height=256,\n\t            new_width=320,\n\t            args=args)\n\t        nb_classes = 710\n\t    elif args.data_set == 'SSV2':\n\t        dataset = RawFrameClsDataset(\n\t            anno_path=anno_path,\n\t            data_root=args.data_root,\n\t            mode=mode,\n", "            clip_len=1,\n\t            num_segment=args.num_frames,\n\t            test_num_segment=args.test_num_segment,\n\t            test_num_crop=args.test_num_crop,\n\t            num_crop=1 if not test_mode else 3,\n\t            keep_aspect_ratio=True,\n\t            crop_size=args.input_size,\n\t            short_side_size=args.short_side_size,\n\t            new_height=256,\n\t            new_width=320,\n", "            filename_tmpl=args.fname_tmpl,\n\t            start_idx=args.start_idx,\n\t            args=args)\n\t        nb_classes = 174\n\t    elif args.data_set == 'UCF101':\n\t        dataset = VideoClsDataset(\n\t            anno_path=anno_path,\n\t            data_root=args.data_root,\n\t            mode=mode,\n\t            clip_len=args.num_frames,\n", "            frame_sample_rate=args.sampling_rate,\n\t            num_segment=1,\n\t            test_num_segment=args.test_num_segment,\n\t            test_num_crop=args.test_num_crop,\n\t            num_crop=1 if not test_mode else 3,\n\t            keep_aspect_ratio=True,\n\t            crop_size=args.input_size,\n\t            short_side_size=args.short_side_size,\n\t            new_height=256,\n\t            new_width=320,\n", "            args=args)\n\t        nb_classes = 101\n\t    elif args.data_set == 'HMDB51':\n\t        dataset = VideoClsDataset(\n\t            anno_path=anno_path,\n\t            data_root=args.data_root,\n\t            mode=mode,\n\t            clip_len=args.num_frames,\n\t            frame_sample_rate=args.sampling_rate,\n\t            num_segment=1,\n", "            test_num_segment=args.test_num_segment,\n\t            test_num_crop=args.test_num_crop,\n\t            num_crop=1 if not test_mode else 3,\n\t            keep_aspect_ratio=True,\n\t            crop_size=args.input_size,\n\t            short_side_size=args.short_side_size,\n\t            new_height=256,\n\t            new_width=320,\n\t            args=args)\n\t        nb_classes = 51\n", "    elif args.data_set == 'Diving48':\n\t        dataset = VideoClsDataset(\n\t            anno_path=anno_path,\n\t            data_root=args.data_root,\n\t            mode=mode,\n\t            clip_len=args.num_frames,\n\t            frame_sample_rate=args.sampling_rate,\n\t            num_segment=1,\n\t            test_num_segment=args.test_num_segment,\n\t            test_num_crop=args.test_num_crop,\n", "            num_crop=1 if not test_mode else 3,\n\t            keep_aspect_ratio=True,\n\t            crop_size=args.input_size,\n\t            short_side_size=args.short_side_size,\n\t            new_height=256,\n\t            new_width=320,\n\t            args=args)\n\t        nb_classes = 48\n\t    elif args.data_set == 'MIT':\n\t        if not args.sparse_sample:\n", "            dataset = VideoClsDataset(\n\t                anno_path=anno_path,\n\t                data_root=args.data_root,\n\t                mode=mode,\n\t                clip_len=args.num_frames,\n\t                frame_sample_rate=args.sampling_rate,\n\t                num_segment=1,\n\t                test_num_segment=args.test_num_segment,\n\t                test_num_crop=args.test_num_crop,\n\t                num_crop=1 if not test_mode else 3,\n", "                keep_aspect_ratio=True,\n\t                crop_size=args.input_size,\n\t                short_side_size=args.short_side_size,\n\t                new_height=256,\n\t                new_width=320,\n\t                sparse_sample=False,\n\t                args=args)\n\t        else:\n\t            dataset = VideoClsDataset(\n\t                anno_path=anno_path,\n", "                data_root=args.data_root,\n\t                mode=mode,\n\t                clip_len=1,\n\t                frame_sample_rate=1,\n\t                num_segment=args.num_frames,\n\t                test_num_segment=args.test_num_segment,\n\t                test_num_crop=args.test_num_crop,\n\t                num_crop=1 if not test_mode else 3,\n\t                keep_aspect_ratio=True,\n\t                crop_size=args.input_size,\n", "                short_side_size=args.short_side_size,\n\t                new_height=256,\n\t                new_width=320,\n\t                sparse_sample=True,\n\t                args=args)\n\t        nb_classes = 339\n\t    else:\n\t        raise NotImplementedError('Unsupported Dataset')\n\t    assert nb_classes == args.nb_classes\n\t    print(\"Number of the class = %d\" % args.nb_classes)\n", "    return dataset, nb_classes\n"]}
{"filename": "dataset/datasets.py", "chunked_list": ["# pylint: disable=line-too-long,too-many-lines,missing-docstring\n\timport os\n\timport warnings\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision import transforms\n\tfrom . import video_transforms, volume_transforms\n\tfrom .loader import get_image_loader, get_video_loader\n", "from .random_erasing import RandomErasing\n\tclass VideoClsDataset(Dataset):\n\t    \"\"\"Load your own video classification dataset.\"\"\"\n\t    def __init__(self,\n\t                 anno_path,\n\t                 data_root='',\n\t                 mode='train',\n\t                 clip_len=8,\n\t                 frame_sample_rate=2,\n\t                 crop_size=224,\n", "                 short_side_size=256,\n\t                 new_height=256,\n\t                 new_width=340,\n\t                 keep_aspect_ratio=True,\n\t                 num_segment=1,\n\t                 num_crop=1,\n\t                 test_num_segment=10,\n\t                 test_num_crop=3,\n\t                 sparse_sample=False,\n\t                 args=None):\n", "        self.anno_path = anno_path\n\t        self.data_root = data_root\n\t        self.mode = mode\n\t        self.clip_len = clip_len\n\t        self.frame_sample_rate = frame_sample_rate\n\t        self.crop_size = crop_size\n\t        self.short_side_size = short_side_size\n\t        self.new_height = new_height\n\t        self.new_width = new_width\n\t        self.keep_aspect_ratio = keep_aspect_ratio\n", "        self.num_segment = num_segment\n\t        self.test_num_segment = test_num_segment\n\t        self.num_crop = num_crop\n\t        self.test_num_crop = test_num_crop\n\t        self.sparse_sample = sparse_sample\n\t        self.args = args\n\t        self.aug = False\n\t        self.rand_erase = False\n\t        if self.mode in ['train']:\n\t            self.aug = True\n", "            if self.args.reprob > 0:\n\t                self.rand_erase = True\n\t        self.video_loader = get_video_loader()\n\t        cleaned = pd.read_csv(self.anno_path, header=None, delimiter=' ')\n\t        self.dataset_samples = list(\n\t            cleaned[0].apply(lambda row: os.path.join(self.data_root, row)))\n\t        self.label_array = list(cleaned.values[:, 1])\n\t        if (mode == 'train'):\n\t            pass\n\t        elif (mode == 'validation'):\n", "            self.data_transform = video_transforms.Compose([\n\t                video_transforms.Resize(\n\t                    self.short_side_size, interpolation='bilinear'),\n\t                video_transforms.CenterCrop(\n\t                    size=(self.crop_size, self.crop_size)),\n\t                volume_transforms.ClipToTensor(),\n\t                video_transforms.Normalize(\n\t                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t            ])\n\t        elif mode == 'test':\n", "            self.data_resize = video_transforms.Compose([\n\t                video_transforms.Resize(\n\t                    size=(short_side_size), interpolation='bilinear')\n\t            ])\n\t            self.data_transform = video_transforms.Compose([\n\t                volume_transforms.ClipToTensor(),\n\t                video_transforms.Normalize(\n\t                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t            ])\n\t            self.test_seg = []\n", "            self.test_dataset = []\n\t            self.test_label_array = []\n\t            for ck in range(self.test_num_segment):\n\t                for cp in range(self.test_num_crop):\n\t                    for idx in range(len(self.label_array)):\n\t                        sample_label = self.label_array[idx]\n\t                        self.test_label_array.append(sample_label)\n\t                        self.test_dataset.append(self.dataset_samples[idx])\n\t                        self.test_seg.append((ck, cp))\n\t    def __getitem__(self, index):\n", "        if self.mode == 'train':\n\t            args = self.args\n\t            scale_t = 1\n\t            sample = self.dataset_samples[index]\n\t            # T H W C\n\t            buffer = self.load_video(sample, sample_rate_scale=scale_t)\n\t            if len(buffer) == 0:\n\t                while len(buffer) == 0:\n\t                    warnings.warn(\n\t                        \"video {} not correctly loaded during training\".format(\n", "                            sample))\n\t                    index = np.random.randint(self.__len__())\n\t                    sample = self.dataset_samples[index]\n\t                    buffer = self.load_video(sample, sample_rate_scale=scale_t)\n\t            if args.num_sample > 1:\n\t                frame_list = []\n\t                label_list = []\n\t                index_list = []\n\t                for _ in range(args.num_sample):\n\t                    new_frames = self._aug_frame(buffer, args)\n", "                    label = self.label_array[index]\n\t                    frame_list.append(new_frames)\n\t                    label_list.append(label)\n\t                    index_list.append(index)\n\t                return frame_list, label_list, index_list, {}\n\t            else:\n\t                buffer = self._aug_frame(buffer, args)\n\t            return buffer, self.label_array[index], index, {}\n\t        elif self.mode == 'validation':\n\t            sample = self.dataset_samples[index]\n", "            buffer = self.load_video(sample)\n\t            if len(buffer) == 0:\n\t                while len(buffer) == 0:\n\t                    warnings.warn(\n\t                        \"video {} not correctly loaded during validation\".\n\t                        format(sample))\n\t                    index = np.random.randint(self.__len__())\n\t                    sample = self.dataset_samples[index]\n\t                    buffer = self.load_video(sample)\n\t            buffer = self.data_transform(buffer)\n", "            return buffer, self.label_array[index], sample.split(\n\t                \"/\")[-1].split(\".\")[0]\n\t        elif self.mode == 'test':\n\t            sample = self.test_dataset[index]\n\t            chunk_nb, split_nb = self.test_seg[index]\n\t            buffer = self.load_video(sample)\n\t            while len(buffer) == 0:\n\t                warnings.warn(\n\t                    \"video {}, temporal {}, spatial {} not found during testing\"\n\t                    .format(str(self.test_dataset[index]), chunk_nb, split_nb))\n", "                index = np.random.randint(self.__len__())\n\t                sample = self.test_dataset[index]\n\t                chunk_nb, split_nb = self.test_seg[index]\n\t                buffer = self.load_video(sample)\n\t            buffer = self.data_resize(buffer)\n\t            if isinstance(buffer, list):\n\t                buffer = np.stack(buffer, 0)\n\t            if self.sparse_sample:\n\t                spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -\n\t                                      self.short_side_size) / (\n", "                                          self.test_num_crop - 1)\n\t                temporal_start = chunk_nb\n\t                spatial_start = int(split_nb * spatial_step)\n\t                if buffer.shape[1] >= buffer.shape[2]:\n\t                    buffer = buffer[temporal_start::self.test_num_segment,\n\t                                    spatial_start:spatial_start +\n\t                                    self.short_side_size, :, :]\n\t                else:\n\t                    buffer = buffer[temporal_start::self.test_num_segment, :,\n\t                                    spatial_start:spatial_start +\n", "                                    self.short_side_size, :]\n\t            else:\n\t                spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -\n\t                                      self.short_side_size) / (\n\t                                          self.test_num_crop - 1)\n\t                temporal_step = max(\n\t                    1.0 * (buffer.shape[0] - self.clip_len) /\n\t                    (self.test_num_segment - 1), 0)\n\t                temporal_start = int(chunk_nb * temporal_step)\n\t                spatial_start = int(split_nb * spatial_step)\n", "                if buffer.shape[1] >= buffer.shape[2]:\n\t                    buffer = buffer[temporal_start:temporal_start +\n\t                                    self.clip_len,\n\t                                    spatial_start:spatial_start +\n\t                                    self.short_side_size, :, :]\n\t                else:\n\t                    buffer = buffer[temporal_start:temporal_start +\n\t                                    self.clip_len, :,\n\t                                    spatial_start:spatial_start +\n\t                                    self.short_side_size, :]\n", "            buffer = self.data_transform(buffer)\n\t            return buffer, self.test_label_array[index], sample.split(\n\t                \"/\")[-1].split(\".\")[0], chunk_nb, split_nb\n\t        else:\n\t            raise NameError('mode {} unkown'.format(self.mode))\n\t    def _aug_frame(self, buffer, args):\n\t        aug_transform = video_transforms.create_random_augment(\n\t            input_size=(self.crop_size, self.crop_size),\n\t            auto_augment=args.aa,\n\t            interpolation=args.train_interpolation,\n", "        )\n\t        buffer = [transforms.ToPILImage()(frame) for frame in buffer]\n\t        buffer = aug_transform(buffer)\n\t        buffer = [transforms.ToTensor()(img) for img in buffer]\n\t        buffer = torch.stack(buffer)  # T C H W\n\t        buffer = buffer.permute(0, 2, 3, 1)  # T H W C\n\t        # T H W C\n\t        buffer = tensor_normalize(buffer, [0.485, 0.456, 0.406],\n\t                                  [0.229, 0.224, 0.225])\n\t        # T H W C -> C T H W.\n", "        buffer = buffer.permute(3, 0, 1, 2)\n\t        # Perform data augmentation.\n\t        scl, asp = (\n\t            [0.08, 1.0],\n\t            [0.75, 1.3333],\n\t        )\n\t        buffer = spatial_sampling(\n\t            buffer,\n\t            spatial_idx=-1,\n\t            min_scale=256,\n", "            max_scale=320,\n\t            # crop_size=224,\n\t            crop_size=args.input_size,\n\t            random_horizontal_flip=False if args.data_set == 'SSV2' else True,\n\t            inverse_uniform_sampling=False,\n\t            aspect_ratio=asp,\n\t            scale=scl,\n\t            motion_shift=False)\n\t        if self.rand_erase:\n\t            erase_transform = RandomErasing(\n", "                args.reprob,\n\t                mode=args.remode,\n\t                max_count=args.recount,\n\t                num_splits=args.recount,\n\t                device=\"cpu\",\n\t            )\n\t            buffer = buffer.permute(1, 0, 2, 3)  # C T H W -> T C H W\n\t            buffer = erase_transform(buffer)\n\t            buffer = buffer.permute(1, 0, 2, 3)  # T C H W -> C T H W\n\t        return buffer\n", "    def load_video(self, sample, sample_rate_scale=1):\n\t        fname = sample\n\t        try:\n\t            vr = self.video_loader(fname)\n\t        except Exception as e:\n\t            print(f\"Failed to load video from {fname} with error {e}!\")\n\t            return []\n\t        length = len(vr)\n\t        if self.mode == 'test':\n\t            if self.sparse_sample:\n", "                tick = length / float(self.num_segment)\n\t                all_index = []\n\t                for t_seg in range(self.test_num_segment):\n\t                    tmp_index = [\n\t                        int(t_seg * tick / self.test_num_segment + tick * x)\n\t                        for x in range(self.num_segment)\n\t                    ]\n\t                    all_index.extend(tmp_index)\n\t                all_index = list(np.sort(np.array(all_index)))\n\t            else:\n", "                all_index = [\n\t                    x for x in range(0, length, self.frame_sample_rate)\n\t                ]\n\t                while len(all_index) < self.clip_len:\n\t                    all_index.append(all_index[-1])\n\t            vr.seek(0)\n\t            buffer = vr.get_batch(all_index).asnumpy()\n\t            return buffer\n\t        # handle temporal segments\n\t        converted_len = int(self.clip_len * self.frame_sample_rate)\n", "        seg_len = length // self.num_segment\n\t        all_index = []\n\t        for i in range(self.num_segment):\n\t            if seg_len <= converted_len:\n\t                index = np.linspace(\n\t                    0, seg_len, num=seg_len // self.frame_sample_rate)\n\t                index = np.concatenate(\n\t                    (index,\n\t                     np.ones(self.clip_len - seg_len // self.frame_sample_rate)\n\t                     * seg_len))\n", "                index = np.clip(index, 0, seg_len - 1).astype(np.int64)\n\t            else:\n\t                if self.mode == 'validation':\n\t                    end_idx = (converted_len + seg_len) // 2\n\t                else:\n\t                    end_idx = np.random.randint(converted_len, seg_len)\n\t                str_idx = end_idx - converted_len\n\t                index = np.linspace(str_idx, end_idx, num=self.clip_len)\n\t                index = np.clip(index, str_idx, end_idx - 1).astype(np.int64)\n\t            index = index + i * seg_len\n", "            all_index.extend(list(index))\n\t        all_index = all_index[::int(sample_rate_scale)]\n\t        vr.seek(0)\n\t        buffer = vr.get_batch(all_index).asnumpy()\n\t        return buffer\n\t    def __len__(self):\n\t        if self.mode != 'test':\n\t            return len(self.dataset_samples)\n\t        else:\n\t            return len(self.test_dataset)\n", "class RawFrameClsDataset(Dataset):\n\t    \"\"\"Load your own raw frame classification dataset.\"\"\"\n\t    def __init__(self,\n\t                 anno_path,\n\t                 data_root,\n\t                 mode='train',\n\t                 clip_len=8,\n\t                 crop_size=224,\n\t                 short_side_size=256,\n\t                 new_height=256,\n", "                 new_width=340,\n\t                 keep_aspect_ratio=True,\n\t                 num_segment=1,\n\t                 num_crop=1,\n\t                 test_num_segment=10,\n\t                 test_num_crop=3,\n\t                 filename_tmpl='img_{:05}.jpg',\n\t                 start_idx=1,\n\t                 args=None):\n\t        self.anno_path = anno_path\n", "        self.data_root = data_root\n\t        self.mode = mode\n\t        self.clip_len = clip_len\n\t        self.crop_size = crop_size\n\t        self.short_side_size = short_side_size\n\t        self.new_height = new_height\n\t        self.new_width = new_width\n\t        self.keep_aspect_ratio = keep_aspect_ratio\n\t        self.num_segment = num_segment\n\t        self.test_num_segment = test_num_segment\n", "        self.num_crop = num_crop\n\t        self.test_num_crop = test_num_crop\n\t        self.filename_tmpl = filename_tmpl\n\t        self.start_idx = start_idx\n\t        self.args = args\n\t        self.aug = False\n\t        self.rand_erase = False\n\t        if self.mode in ['train']:\n\t            self.aug = True\n\t            if self.args.reprob > 0:\n", "                self.rand_erase = True\n\t        self.image_loader = get_image_loader()\n\t        cleaned = pd.read_csv(self.anno_path, header=None, delimiter=' ')\n\t        self.dataset_samples = list(\n\t            cleaned[0].apply(lambda row: os.path.join(self.data_root, row)))\n\t        self.total_frames = list(cleaned.values[:, 1])\n\t        self.label_array = list(cleaned.values[:, -1])\n\t        if (mode == 'train'):\n\t            pass\n\t        elif (mode == 'validation'):\n", "            self.data_transform = video_transforms.Compose([\n\t                video_transforms.Resize(\n\t                    self.short_side_size, interpolation='bilinear'),\n\t                video_transforms.CenterCrop(\n\t                    size=(self.crop_size, self.crop_size)),\n\t                volume_transforms.ClipToTensor(),\n\t                video_transforms.Normalize(\n\t                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t            ])\n\t        elif mode == 'test':\n", "            self.data_resize = video_transforms.Compose([\n\t                video_transforms.Resize(\n\t                    size=(short_side_size), interpolation='bilinear')\n\t            ])\n\t            self.data_transform = video_transforms.Compose([\n\t                volume_transforms.ClipToTensor(),\n\t                video_transforms.Normalize(\n\t                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t            ])\n\t            self.test_seg = []\n", "            self.test_dataset = []\n\t            self.test_total_frames = []\n\t            self.test_label_array = []\n\t            for ck in range(self.test_num_segment):\n\t                for cp in range(self.test_num_crop):\n\t                    for idx in range(len(self.label_array)):\n\t                        self.test_seg.append((ck, cp))\n\t                        self.test_dataset.append(self.dataset_samples[idx])\n\t                        self.test_total_frames.append(self.total_frames[idx])\n\t                        self.test_label_array.append(self.label_array[idx])\n", "    def __getitem__(self, index):\n\t        if self.mode == 'train':\n\t            args = self.args\n\t            scale_t = 1\n\t            sample = self.dataset_samples[index]\n\t            total_frame = self.total_frames[index]\n\t            buffer = self.load_frame(\n\t                sample, total_frame, sample_rate_scale=scale_t)  # T H W C\n\t            if len(buffer) == 0:\n\t                while len(buffer) == 0:\n", "                    warnings.warn(\n\t                        \"video {} not correctly loaded during training\".format(\n\t                            sample))\n\t                    index = np.random.randint(self.__len__())\n\t                    sample = self.dataset_samples[index]\n\t                    total_frame = self.total_frames[index]\n\t                    buffer = self.load_frame(\n\t                        sample, total_frame, sample_rate_scale=scale_t)\n\t            if args.num_sample > 1:\n\t                frame_list = []\n", "                label_list = []\n\t                index_list = []\n\t                for _ in range(args.num_sample):\n\t                    new_frames = self._aug_frame(buffer, args)\n\t                    label = self.label_array[index]\n\t                    frame_list.append(new_frames)\n\t                    label_list.append(label)\n\t                    index_list.append(index)\n\t                return frame_list, label_list, index_list, {}\n\t            else:\n", "                buffer = self._aug_frame(buffer, args)\n\t            return buffer, self.label_array[index], index, {}\n\t        elif self.mode == 'validation':\n\t            sample = self.dataset_samples[index]\n\t            total_frame = self.total_frames[index]\n\t            buffer = self.load_frame(sample, total_frame)\n\t            if len(buffer) == 0:\n\t                while len(buffer) == 0:\n\t                    warnings.warn(\n\t                        \"video {} not correctly loaded during validation\".\n", "                        format(sample))\n\t                    index = np.random.randint(self.__len__())\n\t                    sample = self.dataset_samples[index]\n\t                    buffer = self.load_frame(sample, total_frame)\n\t            buffer = self.data_transform(buffer)\n\t            return buffer, self.label_array[index], sample.split(\n\t                \"/\")[-1].split(\".\")[0]\n\t        elif self.mode == 'test':\n\t            sample = self.test_dataset[index]\n\t            total_frame = self.test_total_frames[index]\n", "            chunk_nb, split_nb = self.test_seg[index]\n\t            buffer = self.load_frame(sample, total_frame)\n\t            while len(buffer) == 0:\n\t                warnings.warn(\n\t                    \"video {}, temporal {}, spatial {} not found during testing\"\n\t                    .format(str(self.test_dataset[index]), chunk_nb, split_nb))\n\t                index = np.random.randint(self.__len__())\n\t                sample = self.test_dataset[index]\n\t                total_frame = self.test_total_frames[index]\n\t                chunk_nb, split_nb = self.test_seg[index]\n", "                buffer = self.load_frame(sample, total_frame)\n\t            buffer = self.data_resize(buffer)\n\t            if isinstance(buffer, list):\n\t                buffer = np.stack(buffer, 0)\n\t            spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -\n\t                                  self.short_side_size) / (\n\t                                      self.test_num_crop - 1)\n\t            temporal_start = chunk_nb\n\t            spatial_start = int(split_nb * spatial_step)\n\t            if buffer.shape[1] >= buffer.shape[2]:\n", "                buffer = buffer[temporal_start::self.test_num_segment,\n\t                                spatial_start:spatial_start +\n\t                                self.short_side_size, :, :]\n\t            else:\n\t                buffer = buffer[temporal_start::self.test_num_segment, :,\n\t                                spatial_start:spatial_start +\n\t                                self.short_side_size, :]\n\t            buffer = self.data_transform(buffer)\n\t            return buffer, self.test_label_array[index], sample.split(\n\t                \"/\")[-1].split(\".\")[0], chunk_nb, split_nb\n", "        else:\n\t            raise NameError('mode {} unkown'.format(self.mode))\n\t    def _aug_frame(self, buffer, args):\n\t        aug_transform = video_transforms.create_random_augment(\n\t            input_size=(self.crop_size, self.crop_size),\n\t            auto_augment=args.aa,\n\t            interpolation=args.train_interpolation,\n\t        )\n\t        buffer = [transforms.ToPILImage()(frame) for frame in buffer]\n\t        buffer = aug_transform(buffer)\n", "        buffer = [transforms.ToTensor()(img) for img in buffer]\n\t        buffer = torch.stack(buffer)  # T C H W\n\t        buffer = buffer.permute(0, 2, 3, 1)  # T H W C\n\t        # T H W C\n\t        buffer = tensor_normalize(buffer, [0.485, 0.456, 0.406],\n\t                                  [0.229, 0.224, 0.225])\n\t        # T H W C -> C T H W.\n\t        buffer = buffer.permute(3, 0, 1, 2)\n\t        # Perform data augmentation.\n\t        scl, asp = (\n", "            [0.08, 1.0],\n\t            [0.75, 1.3333],\n\t        )\n\t        buffer = spatial_sampling(\n\t            buffer,\n\t            spatial_idx=-1,\n\t            min_scale=256,\n\t            max_scale=320,\n\t            crop_size=self.crop_size,\n\t            random_horizontal_flip=False if args.data_set == 'SSV2' else True,\n", "            inverse_uniform_sampling=False,\n\t            aspect_ratio=asp,\n\t            scale=scl,\n\t            motion_shift=False)\n\t        if self.rand_erase:\n\t            erase_transform = RandomErasing(\n\t                args.reprob,\n\t                mode=args.remode,\n\t                max_count=args.recount,\n\t                num_splits=args.recount,\n", "                device=\"cpu\",\n\t            )\n\t            buffer = buffer.permute(1, 0, 2, 3)\n\t            buffer = erase_transform(buffer)\n\t            buffer = buffer.permute(1, 0, 2, 3)\n\t        return buffer\n\t    def load_frame(self, sample, num_frames, sample_rate_scale=1):\n\t        \"\"\"Load video content using Decord\"\"\"\n\t        fname = sample\n\t        if self.mode == 'test':\n", "            tick = num_frames / float(self.num_segment)\n\t            all_index = []\n\t            for t_seg in range(self.test_num_segment):\n\t                tmp_index = [\n\t                    int(t_seg * tick / self.test_num_segment + tick * x)\n\t                    for x in range(self.num_segment)\n\t                ]\n\t                all_index.extend(tmp_index)\n\t            all_index = list(np.sort(np.array(all_index) + self.start_idx))\n\t            imgs = []\n", "            for idx in all_index:\n\t                frame_fname = os.path.join(fname,\n\t                                           self.filename_tmpl.format(idx))\n\t                img = self.image_loader(frame_fname)\n\t                imgs.append(img)\n\t            buffer = np.array(imgs)\n\t            return buffer\n\t        # handle temporal segments\n\t        average_duration = num_frames // self.num_segment\n\t        all_index = []\n", "        if average_duration > 0:\n\t            if self.mode == 'validation':\n\t                all_index = list(\n\t                    np.multiply(\n\t                        list(range(self.num_segment)), average_duration) +\n\t                    np.ones(self.num_segment, dtype=int) *\n\t                    (average_duration // 2))\n\t            else:\n\t                all_index = list(\n\t                    np.multiply(\n", "                        list(range(self.num_segment)), average_duration) +\n\t                    np.random.randint(average_duration, size=self.num_segment))\n\t        elif num_frames > self.num_segment:\n\t            if self.mode == 'validation':\n\t                all_index = list(range(self.num_segment))\n\t            else:\n\t                all_index = list(\n\t                    np.sort(\n\t                        np.random.randint(num_frames, size=self.num_segment)))\n\t        else:\n", "            all_index = [0] * (self.num_segment - num_frames) + list(\n\t                range(num_frames))\n\t        all_index = list(np.array(all_index) + self.start_idx)\n\t        imgs = []\n\t        for idx in all_index:\n\t            frame_fname = os.path.join(fname, self.filename_tmpl.format(idx))\n\t            img = self.image_loader(frame_fname)\n\t            imgs.append(img)\n\t        buffer = np.array(imgs)\n\t        return buffer\n", "    def __len__(self):\n\t        if self.mode != 'test':\n\t            return len(self.dataset_samples)\n\t        else:\n\t            return len(self.test_dataset)\n\tdef spatial_sampling(\n\t    frames,\n\t    spatial_idx=-1,\n\t    min_scale=256,\n\t    max_scale=320,\n", "    crop_size=224,\n\t    random_horizontal_flip=True,\n\t    inverse_uniform_sampling=False,\n\t    aspect_ratio=None,\n\t    scale=None,\n\t    motion_shift=False,\n\t):\n\t    \"\"\"\n\t    Perform spatial sampling on the given video frames. If spatial_idx is\n\t    -1, perform random scale, random crop, and random flip on the given\n", "    frames. If spatial_idx is 0, 1, or 2, perform spatial uniform sampling\n\t    with the given spatial_idx.\n\t    Args:\n\t        frames (tensor): frames of images sampled from the video. The\n\t            dimension is `num frames` x `height` x `width` x `channel`.\n\t        spatial_idx (int): if -1, perform random spatial sampling. If 0, 1,\n\t            or 2, perform left, center, right crop if width is larger than\n\t            height, and perform top, center, buttom crop if height is larger\n\t            than width.\n\t        min_scale (int): the minimal size of scaling.\n", "        max_scale (int): the maximal size of scaling.\n\t        crop_size (int): the size of height and width used to crop the\n\t            frames.\n\t        inverse_uniform_sampling (bool): if True, sample uniformly in\n\t            [1 / max_scale, 1 / min_scale] and take a reciprocal to get the\n\t            scale. If False, take a uniform sample from [min_scale,\n\t            max_scale].\n\t        aspect_ratio (list): Aspect ratio range for resizing.\n\t        scale (list): Scale range for resizing.\n\t        motion_shift (bool): Whether to apply motion shift for resizing.\n", "    Returns:\n\t        frames (tensor): spatially sampled frames.\n\t    \"\"\"\n\t    assert spatial_idx in [-1, 0, 1, 2]\n\t    if spatial_idx == -1:\n\t        if aspect_ratio is None and scale is None:\n\t            frames, _ = video_transforms.random_short_side_scale_jitter(\n\t                images=frames,\n\t                min_size=min_scale,\n\t                max_size=max_scale,\n", "                inverse_uniform_sampling=inverse_uniform_sampling,\n\t            )\n\t            frames, _ = video_transforms.random_crop(frames, crop_size)\n\t        else:\n\t            transform_func = (\n\t                video_transforms.random_resized_crop_with_shift\n\t                if motion_shift else video_transforms.random_resized_crop)\n\t            frames = transform_func(\n\t                images=frames,\n\t                target_height=crop_size,\n", "                target_width=crop_size,\n\t                scale=scale,\n\t                ratio=aspect_ratio,\n\t            )\n\t        if random_horizontal_flip:\n\t            frames, _ = video_transforms.horizontal_flip(0.5, frames)\n\t    else:\n\t        # The testing is deterministic and no jitter should be performed.\n\t        # min_scale, max_scale, and crop_size are expect to be the same.\n\t        assert len({min_scale, max_scale, crop_size}) == 1\n", "        frames, _ = video_transforms.random_short_side_scale_jitter(\n\t            frames, min_scale, max_scale)\n\t        frames, _ = video_transforms.uniform_crop(frames, crop_size,\n\t                                                  spatial_idx)\n\t    return frames\n\tdef tensor_normalize(tensor, mean, std):\n\t    \"\"\"\n\t    Normalize a given tensor by subtracting the mean and dividing the std.\n\t    Args:\n\t        tensor (tensor): tensor to normalize.\n", "        mean (tensor or list): mean value to subtract.\n\t        std (tensor or list): std to divide.\n\t    \"\"\"\n\t    if tensor.dtype == torch.uint8:\n\t        tensor = tensor.float()\n\t        tensor = tensor / 255.0\n\t    if type(mean) == list:\n\t        mean = torch.tensor(mean)\n\t    if type(std) == list:\n\t        std = torch.tensor(std)\n", "    tensor = tensor - mean\n\t    tensor = tensor / std\n\t    return tensor\n"]}
{"filename": "dataset/functional.py", "chunked_list": ["import numbers\n\timport cv2\n\timport numpy as np\n\timport PIL\n\timport torch\n\tdef _is_tensor_clip(clip):\n\t    return torch.is_tensor(clip) and clip.ndimension() == 4\n\tdef crop_clip(clip, min_h, min_w, h, w):\n\t    if isinstance(clip[0], np.ndarray):\n\t        cropped = [img[min_h:min_h + h, min_w:min_w + w, :] for img in clip]\n", "    elif isinstance(clip[0], PIL.Image.Image):\n\t        cropped = [\n\t            img.crop((min_w, min_h, min_w + w, min_h + h)) for img in clip\n\t        ]\n\t    else:\n\t        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n\t                        'but got list of {0}'.format(type(clip[0])))\n\t    return cropped\n\tdef resize_clip(clip, size, interpolation='bilinear'):\n\t    if isinstance(clip[0], np.ndarray):\n", "        if isinstance(size, numbers.Number):\n\t            im_h, im_w, im_c = clip[0].shape\n\t            # Min spatial dim already matches minimal size\n\t            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n\t                                                   and im_h == size):\n\t                return clip\n\t            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n\t            size = (new_w, new_h)\n\t        else:\n\t            size = size[0], size[1]\n", "        if interpolation == 'bilinear':\n\t            np_inter = cv2.INTER_LINEAR\n\t        else:\n\t            np_inter = cv2.INTER_NEAREST\n\t        scaled = [\n\t            cv2.resize(img, size, interpolation=np_inter) for img in clip\n\t        ]\n\t    elif isinstance(clip[0], PIL.Image.Image):\n\t        if isinstance(size, numbers.Number):\n\t            im_w, im_h = clip[0].size\n", "            # Min spatial dim already matches minimal size\n\t            if (im_w <= im_h and im_w == size) or (im_h <= im_w\n\t                                                   and im_h == size):\n\t                return clip\n\t            new_h, new_w = get_resize_sizes(im_h, im_w, size)\n\t            size = (new_w, new_h)\n\t        else:\n\t            size = size[1], size[0]\n\t        if interpolation == 'bilinear':\n\t            pil_inter = PIL.Image.BILINEAR\n", "        else:\n\t            pil_inter = PIL.Image.NEAREST\n\t        scaled = [img.resize(size, pil_inter) for img in clip]\n\t    else:\n\t        raise TypeError('Expected numpy.ndarray or PIL.Image' +\n\t                        'but got list of {0}'.format(type(clip[0])))\n\t    return scaled\n\tdef get_resize_sizes(im_h, im_w, size):\n\t    if im_w < im_h:\n\t        ow = size\n", "        oh = int(size * im_h / im_w)\n\t    else:\n\t        oh = size\n\t        ow = int(size * im_w / im_h)\n\t    return oh, ow\n\tdef normalize(clip, mean, std, inplace=False):\n\t    if not _is_tensor_clip(clip):\n\t        raise TypeError('tensor is not a torch clip.')\n\t    if not inplace:\n\t        clip = clip.clone()\n", "    dtype = clip.dtype\n\t    mean = torch.as_tensor(mean, dtype=dtype, device=clip.device)\n\t    std = torch.as_tensor(std, dtype=dtype, device=clip.device)\n\t    clip.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])\n\t    return clip\n"]}
{"filename": "models/modeling_finetune.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\tfrom functools import partial\n\timport numpy as np\n\timport torch\n", "import torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch.utils.checkpoint as cp\n\tfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\n\tfrom timm.models.registry import register_model\n\tdef _cfg(url='', **kwargs):\n\t    return {\n\t        'url': url,\n\t        'num_classes': 400,\n\t        'input_size': (3, 224, 224),\n", "        'pool_size': None,\n\t        'crop_pct': .9,\n\t        'interpolation': 'bicubic',\n\t        'mean': (0.5, 0.5, 0.5),\n\t        'std': (0.5, 0.5, 0.5),\n\t        **kwargs\n\t    }\n\tclass DropPath(nn.Module):\n\t    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n\t    \"\"\"\n", "    def __init__(self, drop_prob=None):\n\t        super(DropPath, self).__init__()\n\t        self.drop_prob = drop_prob\n\t    def forward(self, x):\n\t        return drop_path(x, self.drop_prob, self.training)\n\t    def extra_repr(self) -> str:\n\t        return 'p={}'.format(self.drop_prob)\n\tclass Mlp(nn.Module):\n\t    def __init__(self,\n\t                 in_features,\n", "                 hidden_features=None,\n\t                 out_features=None,\n\t                 act_layer=nn.GELU,\n\t                 drop=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        self.fc1 = nn.Linear(in_features, hidden_features)\n\t        self.act = act_layer()\n\t        self.fc2 = nn.Linear(hidden_features, out_features)\n", "        self.drop = nn.Dropout(drop)\n\t    def forward(self, x):\n\t        x = self.fc1(x)\n\t        x = self.act(x)\n\t        # x = self.drop(x)\n\t        # commit this for the orignal BERT implement\n\t        x = self.fc2(x)\n\t        x = self.drop(x)\n\t        return x\n\tclass CosAttention(nn.Module):\n", "    def __init__(self,\n\t                 dim,\n\t                 num_heads=8,\n\t                 qkv_bias=False,\n\t                 qk_scale=None,\n\t                 attn_drop=0.,\n\t                 proj_drop=0.,\n\t                 attn_head_dim=None):\n\t        super().__init__()\n\t        self.num_heads = num_heads\n", "        head_dim = dim // num_heads\n\t        if attn_head_dim is not None:\n\t            head_dim = attn_head_dim\n\t        all_head_dim = head_dim * self.num_heads\n\t        # self.scale = qk_scale or head_dim**-0.5\n\t        # DO NOT RENAME [self.scale] (for no weight decay)\n\t        if qk_scale is None:\n\t            self.scale = nn.Parameter(\n\t                torch.log(10 * torch.ones((num_heads, 1, 1))),\n\t                requires_grad=True)\n", "        else:\n\t            self.scale = qk_scale\n\t        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\t        if qkv_bias:\n\t            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n\t            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n\t        else:\n\t            self.q_bias = None\n\t            self.v_bias = None\n\t        self.attn_drop = nn.Dropout(attn_drop)\n", "        self.proj = nn.Linear(all_head_dim, dim)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n\t    def forward(self, x):\n\t        B, N, C = x.shape\n\t        qkv_bias = None\n\t        if self.q_bias is not None:\n\t            qkv_bias = torch.cat(\n\t                (self.q_bias,\n\t                 torch.zeros_like(self.v_bias,\n\t                                  requires_grad=False), self.v_bias))\n", "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n\t        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv[0], qkv[1], qkv[\n\t            2]  # make torchscript happy (cannot use tensor as tuple)\n\t        attn = (\n\t            F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n\t        # torch.log(torch.tensor(1. / 0.01)) = 4.6052\n\t        logit_scale = torch.clamp(self.scale, max=4.6052).exp()\n\t        attn = attn * logit_scale\n\t        attn = attn.softmax(dim=-1)\n", "        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x\n\tclass Attention(nn.Module):\n\t    def __init__(self,\n\t                 dim,\n\t                 num_heads=8,\n\t                 qkv_bias=False,\n", "                 qk_scale=None,\n\t                 attn_drop=0.,\n\t                 proj_drop=0.,\n\t                 attn_head_dim=None):\n\t        super().__init__()\n\t        self.num_heads = num_heads\n\t        head_dim = dim // num_heads\n\t        if attn_head_dim is not None:\n\t            head_dim = attn_head_dim\n\t        all_head_dim = head_dim * self.num_heads\n", "        self.scale = qk_scale or head_dim**-0.5\n\t        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\t        if qkv_bias:\n\t            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n\t            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n\t        else:\n\t            self.q_bias = None\n\t            self.v_bias = None\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(all_head_dim, dim)\n", "        self.proj_drop = nn.Dropout(proj_drop)\n\t    def forward(self, x):\n\t        B, N, C = x.shape\n\t        qkv_bias = None\n\t        if self.q_bias is not None:\n\t            qkv_bias = torch.cat(\n\t                (self.q_bias,\n\t                 torch.zeros_like(self.v_bias,\n\t                                  requires_grad=False), self.v_bias))\n\t        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n", "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv[0], qkv[1], qkv[\n\t            2]  # make torchscript happy (cannot use tensor as tuple)\n\t        q = q * self.scale\n\t        attn = (q @ k.transpose(-2, -1))\n\t        attn = attn.softmax(dim=-1)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n", "        return x\n\tclass Block(nn.Module):\n\t    def __init__(self,\n\t                 dim,\n\t                 num_heads,\n\t                 mlp_ratio=4.,\n\t                 qkv_bias=False,\n\t                 qk_scale=None,\n\t                 drop=0.,\n\t                 attn_drop=0.,\n", "                 drop_path=0.,\n\t                 init_values=None,\n\t                 act_layer=nn.GELU,\n\t                 norm_layer=nn.LayerNorm,\n\t                 attn_head_dim=None,\n\t                 cos_attn=False):\n\t        super().__init__()\n\t        self.norm1 = norm_layer(dim)\n\t        if cos_attn:\n\t            self.attn = CosAttention(\n", "                dim,\n\t                num_heads=num_heads,\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                attn_drop=attn_drop,\n\t                proj_drop=drop,\n\t                attn_head_dim=attn_head_dim)\n\t        else:\n\t            self.attn = Attention(\n\t                dim,\n", "                num_heads=num_heads,\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                attn_drop=attn_drop,\n\t                proj_drop=drop,\n\t                attn_head_dim=attn_head_dim)\n\t        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n\t        self.drop_path = DropPath(\n\t            drop_path) if drop_path > 0. else nn.Identity()\n\t        self.norm2 = norm_layer(dim)\n", "        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(\n\t            in_features=dim,\n\t            hidden_features=mlp_hidden_dim,\n\t            act_layer=act_layer,\n\t            drop=drop)\n\t        if init_values > 0:\n\t            self.gamma_1 = nn.Parameter(\n\t                init_values * torch.ones((dim)), requires_grad=True)\n\t            self.gamma_2 = nn.Parameter(\n", "                init_values * torch.ones((dim)), requires_grad=True)\n\t        else:\n\t            self.gamma_1, self.gamma_2 = None, None\n\t    def forward(self, x):\n\t        if self.gamma_1 is None:\n\t            x = x + self.drop_path(self.attn(self.norm1(x)))\n\t            x = x + self.drop_path(self.mlp(self.norm2(x)))\n\t        else:\n\t            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n\t            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n", "        return x\n\tclass PatchEmbed(nn.Module):\n\t    \"\"\" Image to Patch Embedding\n\t    \"\"\"\n\t    def __init__(self,\n\t                 img_size=224,\n\t                 patch_size=16,\n\t                 in_chans=3,\n\t                 embed_dim=768,\n\t                 num_frames=16,\n", "                 tubelet_size=2):\n\t        super().__init__()\n\t        img_size = to_2tuple(img_size)\n\t        patch_size = to_2tuple(patch_size)\n\t        num_spatial_patches = (img_size[0] // patch_size[0]) * (\n\t            img_size[1] // patch_size[1])\n\t        num_patches = num_spatial_patches * (num_frames // tubelet_size)\n\t        self.img_size = img_size\n\t        self.tubelet_size = tubelet_size\n\t        self.patch_size = patch_size\n", "        self.num_patches = num_patches\n\t        self.proj = nn.Conv3d(\n\t            in_channels=in_chans,\n\t            out_channels=embed_dim,\n\t            kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),\n\t            stride=(self.tubelet_size, patch_size[0], patch_size[1]))\n\t    def forward(self, x, **kwargs):\n\t        B, C, T, H, W = x.shape\n\t        assert H == self.img_size[0] and W == self.img_size[\n\t            1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n", "        # b, c, l -> b, l, c\n\t        x = self.proj(x).flatten(2).transpose(1, 2)\n\t        return x\n\t# sin-cos position encoding\n\t# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n\tdef get_sinusoid_encoding_table(n_position, d_hid):\n\t    ''' Sinusoid position encoding table '''\n\t    # TODO: make it with torch instead of numpy\n\t    def get_position_angle_vec(position):\n\t        return [\n", "            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n\t            for hid_j in range(d_hid)\n\t        ]\n\t    sinusoid_table = np.array(\n\t        [get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n\t    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n\t    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\t    return torch.tensor(\n\t        sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)\n\tclass VisionTransformer(nn.Module):\n", "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n\t    \"\"\"\n\t    def __init__(self,\n\t                 img_size=224,\n\t                 patch_size=16,\n\t                 in_chans=3,\n\t                 num_classes=1000,\n\t                 embed_dim=768,\n\t                 depth=12,\n\t                 num_heads=12,\n", "                 mlp_ratio=4.,\n\t                 qkv_bias=False,\n\t                 qk_scale=None,\n\t                 drop_rate=0.,\n\t                 attn_drop_rate=0.,\n\t                 drop_path_rate=0.,\n\t                 head_drop_rate=0.,\n\t                 norm_layer=nn.LayerNorm,\n\t                 init_values=0.,\n\t                 use_learnable_pos_emb=False,\n", "                 init_scale=0.,\n\t                 all_frames=16,\n\t                 tubelet_size=2,\n\t                 use_mean_pooling=True,\n\t                 with_cp=False,\n\t                 cos_attn=False):\n\t        super().__init__()\n\t        self.num_classes = num_classes\n\t        # num_features for consistency with other models\n\t        self.num_features = self.embed_dim = embed_dim\n", "        self.tubelet_size = tubelet_size\n\t        self.patch_embed = PatchEmbed(\n\t            img_size=img_size,\n\t            patch_size=patch_size,\n\t            in_chans=in_chans,\n\t            embed_dim=embed_dim,\n\t            num_frames=all_frames,\n\t            tubelet_size=tubelet_size)\n\t        num_patches = self.patch_embed.num_patches\n\t        self.with_cp = with_cp\n", "        if use_learnable_pos_emb:\n\t            self.pos_embed = nn.Parameter(\n\t                torch.zeros(1, num_patches, embed_dim))\n\t        else:\n\t            # sine-cosine positional embeddings is on the way\n\t            self.pos_embed = get_sinusoid_encoding_table(\n\t                num_patches, embed_dim)\n\t        self.pos_drop = nn.Dropout(p=drop_rate)\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)\n\t               ]  # stochastic depth decay rule\n", "        self.blocks = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dim,\n\t                num_heads=num_heads,\n\t                mlp_ratio=mlp_ratio,\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                drop=drop_rate,\n\t                attn_drop=attn_drop_rate,\n\t                drop_path=dpr[i],\n", "                norm_layer=norm_layer,\n\t                init_values=init_values,\n\t                cos_attn=cos_attn) for i in range(depth)\n\t        ])\n\t        self.norm = nn.Identity() if use_mean_pooling else norm_layer(\n\t            embed_dim)\n\t        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n\t        self.head_dropout = nn.Dropout(head_drop_rate)\n\t        self.head = nn.Linear(\n\t            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n", "        if use_learnable_pos_emb:\n\t            trunc_normal_(self.pos_embed, std=.02)\n\t        self.apply(self._init_weights)\n\t        self.head.weight.data.mul_(init_scale)\n\t        self.head.bias.data.mul_(init_scale)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n", "        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def get_num_layers(self):\n\t        return len(self.blocks)\n\t    @torch.jit.ignore\n\t    def no_weight_decay(self):\n\t        return {'pos_embed', 'cls_token'}\n\t    def get_classifier(self):\n\t        return self.head\n", "    def reset_classifier(self, num_classes, global_pool=''):\n\t        self.num_classes = num_classes\n\t        self.head = nn.Linear(\n\t            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t    def forward_features(self, x):\n\t        B = x.size(0)\n\t        x = self.patch_embed(x)\n\t        if self.pos_embed is not None:\n\t            x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(\n\t                x.device).clone().detach()\n", "        x = self.pos_drop(x)\n\t        for blk in self.blocks:\n\t            if self.with_cp:\n\t                x = cp.checkpoint(blk, x)\n\t            else:\n\t                x = blk(x)\n\t        if self.fc_norm is not None:\n\t            return self.fc_norm(x.mean(1))\n\t        else:\n\t            return self.norm(x[:, 0])\n", "    def forward(self, x):\n\t        x = self.forward_features(x)\n\t        x = self.head_dropout(x)\n\t        x = self.head(x)\n\t        return x\n\t@register_model\n\tdef vit_small_patch16_224(pretrained=False, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=16,\n\t        embed_dim=384,\n", "        depth=12,\n\t        num_heads=6,\n\t        mlp_ratio=4,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n\t    return model\n\t@register_model\n\tdef vit_base_patch16_224(pretrained=False, **kwargs):\n", "    model = VisionTransformer(\n\t        patch_size=16,\n\t        embed_dim=768,\n\t        depth=12,\n\t        num_heads=12,\n\t        mlp_ratio=4,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n", "    return model\n\t@register_model\n\tdef vit_large_patch16_224(pretrained=False, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=16,\n\t        embed_dim=1024,\n\t        depth=24,\n\t        num_heads=16,\n\t        mlp_ratio=4,\n\t        qkv_bias=True,\n", "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n\t    return model\n\t@register_model\n\tdef vit_huge_patch16_224(pretrained=False, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=16,\n\t        embed_dim=1280,\n\t        depth=32,\n", "        num_heads=16,\n\t        mlp_ratio=4,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n\t    return model\n\t@register_model\n\tdef vit_giant_patch14_224(pretrained=False, **kwargs):\n\t    model = VisionTransformer(\n", "        patch_size=14,\n\t        embed_dim=1408,\n\t        depth=40,\n\t        num_heads=16,\n\t        mlp_ratio=48 / 11,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n\t    return model\n"]}
{"filename": "models/__init__.py", "chunked_list": ["from .modeling_finetune import (\n\t    vit_base_patch16_224,\n\t    vit_giant_patch14_224,\n\t    vit_huge_patch16_224,\n\t    vit_large_patch16_224,\n\t    vit_small_patch16_224,\n\t)\n\tfrom .modeling_pretrain import (\n\t    pretrain_videomae_base_patch16_224,\n\t    pretrain_videomae_giant_patch14_224,\n", "    pretrain_videomae_huge_patch16_224,\n\t    pretrain_videomae_large_patch16_224,\n\t    pretrain_videomae_small_patch16_224,\n\t)\n\t__all__ = [\n\t    'pretrain_videomae_small_patch16_224',\n\t    'pretrain_videomae_base_patch16_224',\n\t    'pretrain_videomae_large_patch16_224',\n\t    'pretrain_videomae_huge_patch16_224',\n\t    'pretrain_videomae_giant_patch14_224',\n", "    'vit_small_patch16_224',\n\t    'vit_base_patch16_224',\n\t    'vit_large_patch16_224',\n\t    'vit_huge_patch16_224',\n\t    'vit_giant_patch14_224',\n\t]"]}
{"filename": "models/modeling_pretrain.py", "chunked_list": ["# --------------------------------------------------------\n\t# Based on BEiT, timm, DINO and DeiT code bases\n\t# https://github.com/microsoft/unilm/tree/master/beit\n\t# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n\t# https://github.com/facebookresearch/deit\n\t# https://github.com/facebookresearch/dino\n\t# --------------------------------------------------------'\n\tfrom functools import partial\n\timport torch\n\timport torch.nn as nn\n", "import torch.utils.checkpoint as cp\n\tfrom timm.models.layers import trunc_normal_ as __call_trunc_normal_\n\tfrom timm.models.registry import register_model\n\tfrom .modeling_finetune import (\n\t    Block,\n\t    PatchEmbed,\n\t    _cfg,\n\t    get_sinusoid_encoding_table,\n\t)\n\tdef trunc_normal_(tensor, mean=0., std=1.):\n", "    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n\tclass PretrainVisionTransformerEncoder(nn.Module):\n\t    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n\t    \"\"\"\n\t    def __init__(self,\n\t                 img_size=224,\n\t                 patch_size=16,\n\t                 in_chans=3,\n\t                 num_classes=0,\n\t                 embed_dim=768,\n", "                 depth=12,\n\t                 num_heads=12,\n\t                 mlp_ratio=4.,\n\t                 qkv_bias=False,\n\t                 qk_scale=None,\n\t                 drop_rate=0.,\n\t                 attn_drop_rate=0.,\n\t                 drop_path_rate=0.,\n\t                 norm_layer=nn.LayerNorm,\n\t                 init_values=None,\n", "                 tubelet_size=2,\n\t                 use_learnable_pos_emb=False,\n\t                 with_cp=False,\n\t                 all_frames=16,\n\t                 cos_attn=False):\n\t        super().__init__()\n\t        self.num_classes = num_classes\n\t        # num_features for consistency with other models\n\t        self.num_features = self.embed_dim = embed_dim\n\t        self.patch_embed = PatchEmbed(\n", "            img_size=img_size,\n\t            patch_size=patch_size,\n\t            in_chans=in_chans,\n\t            embed_dim=embed_dim,\n\t            num_frames=all_frames,\n\t            tubelet_size=tubelet_size)\n\t        num_patches = self.patch_embed.num_patches\n\t        self.with_cp = with_cp\n\t        if use_learnable_pos_emb:\n\t            self.pos_embed = nn.Parameter(\n", "                torch.zeros(1, num_patches + 1, embed_dim))\n\t        else:\n\t            # sine-cosine positional embeddings\n\t            self.pos_embed = get_sinusoid_encoding_table(\n\t                num_patches, embed_dim)\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)\n\t               ]  # stochastic depth decay rule\n\t        self.blocks = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dim,\n", "                num_heads=num_heads,\n\t                mlp_ratio=mlp_ratio,\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                drop=drop_rate,\n\t                attn_drop=attn_drop_rate,\n\t                drop_path=dpr[i],\n\t                norm_layer=norm_layer,\n\t                init_values=init_values,\n\t                cos_attn=cos_attn) for i in range(depth)\n", "        ])\n\t        self.norm = norm_layer(embed_dim)\n\t        self.head = nn.Linear(\n\t            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t        if use_learnable_pos_emb:\n\t            trunc_normal_(self.pos_embed, std=.02)\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            nn.init.xavier_uniform_(m.weight)\n", "            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def get_num_layers(self):\n\t        return len(self.blocks)\n\t    @torch.jit.ignore\n\t    def no_weight_decay(self):\n\t        return {'pos_embed', 'cls_token'}\n", "    def get_classifier(self):\n\t        return self.head\n\t    def reset_classifier(self, num_classes, global_pool=''):\n\t        self.num_classes = num_classes\n\t        self.head = nn.Linear(\n\t            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t    def forward_features(self, x, mask):\n\t        x = self.patch_embed(x)\n\t        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n\t        B, _, C = x.shape\n", "        x_vis = x[~mask].reshape(B, -1, C)  # ~mask means visible\n\t        for blk in self.blocks:\n\t            if self.with_cp:\n\t                x_vis = cp.checkpoint(blk, x_vis)\n\t            else:\n\t                x_vis = blk(x_vis)\n\t        x_vis = self.norm(x_vis)\n\t        return x_vis\n\t    def forward(self, x, mask):\n\t        x = self.forward_features(x, mask)\n", "        x = self.head(x)\n\t        return x\n\tclass PretrainVisionTransformerDecoder(nn.Module):\n\t    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n\t    \"\"\"\n\t    def __init__(self,\n\t                 patch_size=16,\n\t                 num_classes=768,\n\t                 embed_dim=768,\n\t                 depth=12,\n", "                 num_heads=12,\n\t                 mlp_ratio=4.,\n\t                 qkv_bias=False,\n\t                 qk_scale=None,\n\t                 drop_rate=0.,\n\t                 attn_drop_rate=0.,\n\t                 drop_path_rate=0.,\n\t                 norm_layer=nn.LayerNorm,\n\t                 init_values=None,\n\t                 num_patches=196,\n", "                 tubelet_size=2,\n\t                 with_cp=False,\n\t                 cos_attn=False):\n\t        super().__init__()\n\t        self.num_classes = num_classes\n\t        assert num_classes == 3 * tubelet_size * patch_size**2\n\t        # num_features for consistency with other models\n\t        self.num_features = self.embed_dim = embed_dim\n\t        self.patch_size = patch_size\n\t        self.with_cp = with_cp\n", "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)\n\t               ]  # stochastic depth decay rule\n\t        self.blocks = nn.ModuleList([\n\t            Block(\n\t                dim=embed_dim,\n\t                num_heads=num_heads,\n\t                mlp_ratio=mlp_ratio,\n\t                qkv_bias=qkv_bias,\n\t                qk_scale=qk_scale,\n\t                drop=drop_rate,\n", "                attn_drop=attn_drop_rate,\n\t                drop_path=dpr[i],\n\t                norm_layer=norm_layer,\n\t                init_values=init_values,\n\t                cos_attn=cos_attn) for i in range(depth)\n\t        ])\n\t        self.norm = norm_layer(embed_dim)\n\t        self.head = nn.Linear(\n\t            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t        self.apply(self._init_weights)\n", "    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            nn.init.xavier_uniform_(m.weight)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def get_num_layers(self):\n\t        return len(self.blocks)\n", "    @torch.jit.ignore\n\t    def no_weight_decay(self):\n\t        return {'pos_embed', 'cls_token'}\n\t    def get_classifier(self):\n\t        return self.head\n\t    def reset_classifier(self, num_classes, global_pool=''):\n\t        self.num_classes = num_classes\n\t        self.head = nn.Linear(\n\t            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t    def forward(self, x, return_token_num):\n", "        for blk in self.blocks:\n\t            if self.with_cp:\n\t                x = cp.checkpoint(blk, x)\n\t            else:\n\t                x = blk(x)\n\t        if return_token_num > 0:\n\t            # only return the mask tokens predict pixels\n\t            x = self.head(self.norm(x[:, -return_token_num:]))\n\t        else:\n\t            # [B, N, 3*16^2]\n", "            x = self.head(self.norm(x))\n\t        return x\n\tclass PretrainVisionTransformer(nn.Module):\n\t    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        img_size=224,\n\t        patch_size=16,\n\t        encoder_in_chans=3,\n", "        encoder_num_classes=0,\n\t        encoder_embed_dim=768,\n\t        encoder_depth=12,\n\t        encoder_num_heads=12,\n\t        decoder_num_classes=1536,  # decoder_num_classes=768\n\t        decoder_embed_dim=512,\n\t        decoder_depth=8,\n\t        decoder_num_heads=8,\n\t        mlp_ratio=4.,\n\t        qkv_bias=False,\n", "        qk_scale=None,\n\t        drop_rate=0.,\n\t        attn_drop_rate=0.,\n\t        drop_path_rate=0.,\n\t        norm_layer=nn.LayerNorm,\n\t        init_values=0.,\n\t        use_learnable_pos_emb=False,\n\t        tubelet_size=2,\n\t        num_classes=0,  # avoid the error from create_fn in timm\n\t        in_chans=0,  # avoid the error from create_fn in timm\n", "        with_cp=False,\n\t        all_frames=16,\n\t        cos_attn=False,\n\t    ):\n\t        super().__init__()\n\t        self.encoder = PretrainVisionTransformerEncoder(\n\t            img_size=img_size,\n\t            patch_size=patch_size,\n\t            in_chans=encoder_in_chans,\n\t            num_classes=encoder_num_classes,\n", "            embed_dim=encoder_embed_dim,\n\t            depth=encoder_depth,\n\t            num_heads=encoder_num_heads,\n\t            mlp_ratio=mlp_ratio,\n\t            qkv_bias=qkv_bias,\n\t            qk_scale=qk_scale,\n\t            drop_rate=drop_rate,\n\t            attn_drop_rate=attn_drop_rate,\n\t            drop_path_rate=drop_path_rate,\n\t            norm_layer=norm_layer,\n", "            init_values=init_values,\n\t            tubelet_size=tubelet_size,\n\t            use_learnable_pos_emb=use_learnable_pos_emb,\n\t            with_cp=with_cp,\n\t            all_frames=all_frames,\n\t            cos_attn=cos_attn)\n\t        self.decoder = PretrainVisionTransformerDecoder(\n\t            patch_size=patch_size,\n\t            num_patches=self.encoder.patch_embed.num_patches,\n\t            num_classes=decoder_num_classes,\n", "            embed_dim=decoder_embed_dim,\n\t            depth=decoder_depth,\n\t            num_heads=decoder_num_heads,\n\t            mlp_ratio=mlp_ratio,\n\t            qkv_bias=qkv_bias,\n\t            qk_scale=qk_scale,\n\t            drop_rate=drop_rate,\n\t            attn_drop_rate=attn_drop_rate,\n\t            drop_path_rate=drop_path_rate,\n\t            norm_layer=norm_layer,\n", "            init_values=init_values,\n\t            tubelet_size=tubelet_size,\n\t            with_cp=with_cp,\n\t            cos_attn=cos_attn)\n\t        self.encoder_to_decoder = nn.Linear(\n\t            encoder_embed_dim, decoder_embed_dim, bias=False)\n\t        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n\t        self.pos_embed = get_sinusoid_encoding_table(\n\t            self.encoder.patch_embed.num_patches, decoder_embed_dim)\n\t        trunc_normal_(self.mask_token, std=.02)\n", "    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            nn.init.xavier_uniform_(m.weight)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def get_num_layers(self):\n\t        return len(self.blocks)\n", "    @torch.jit.ignore\n\t    def no_weight_decay(self):\n\t        return {'pos_embed', 'cls_token', 'mask_token'}\n\t    def forward(self, x, mask, decode_mask=None):\n\t        decode_vis = mask if decode_mask is None else ~decode_mask\n\t        x_vis = self.encoder(x, mask)  # [B, N_vis, C_e]\n\t        x_vis = self.encoder_to_decoder(x_vis)  # [B, N_vis, C_d]\n\t        B, N_vis, C = x_vis.shape\n\t        # we don't unshuffle the correct visible token order,\n\t        # but shuffle the pos embedding accorddingly.\n", "        expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(\n\t            x.device).clone().detach()\n\t        pos_emd_vis = expand_pos_embed[~mask].reshape(B, -1, C)\n\t        pos_emd_mask = expand_pos_embed[decode_vis].reshape(B, -1, C)\n\t        # [B, N, C_d]\n\t        x_full = torch.cat(\n\t            [x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1)\n\t        # NOTE: if N_mask==0, the shape of x is [B, N_mask, 3 * 16 * 16]\n\t        x = self.decoder(x_full, pos_emd_mask.shape[1])\n\t        return x\n", "@register_model\n\tdef pretrain_videomae_small_patch16_224(pretrained=False, **kwargs):\n\t    model = PretrainVisionTransformer(\n\t        img_size=224,\n\t        patch_size=16,\n\t        encoder_embed_dim=384,\n\t        encoder_depth=12,\n\t        encoder_num_heads=6,\n\t        encoder_num_classes=0,\n\t        decoder_num_classes=1536,  # 16 * 16 * 3 * 2\n", "        decoder_embed_dim=192,\n\t        decoder_num_heads=3,\n\t        mlp_ratio=4,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n\t    if pretrained:\n\t        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n\t        model.load_state_dict(checkpoint[\"model\"])\n", "    return model\n\t@register_model\n\tdef pretrain_videomae_base_patch16_224(pretrained=False, **kwargs):\n\t    model = PretrainVisionTransformer(\n\t        img_size=224,\n\t        patch_size=16,\n\t        encoder_embed_dim=768,\n\t        encoder_depth=12,\n\t        encoder_num_heads=12,\n\t        encoder_num_classes=0,\n", "        decoder_num_classes=1536,  # 16 * 16 * 3 * 2\n\t        decoder_embed_dim=384,\n\t        decoder_num_heads=6,\n\t        mlp_ratio=4,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n\t    if pretrained:\n\t        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n", "        model.load_state_dict(checkpoint[\"model\"])\n\t    return model\n\t@register_model\n\tdef pretrain_videomae_large_patch16_224(pretrained=False, **kwargs):\n\t    model = PretrainVisionTransformer(\n\t        img_size=224,\n\t        patch_size=16,\n\t        encoder_embed_dim=1024,\n\t        encoder_depth=24,\n\t        encoder_num_heads=16,\n", "        encoder_num_classes=0,\n\t        decoder_num_classes=1536,  # 16 * 16 * 3 * 2\n\t        decoder_embed_dim=512,\n\t        decoder_num_heads=8,\n\t        mlp_ratio=4,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n\t    if pretrained:\n", "        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n\t        model.load_state_dict(checkpoint[\"model\"])\n\t    return model\n\t@register_model\n\tdef pretrain_videomae_huge_patch16_224(pretrained=False, **kwargs):\n\t    model = PretrainVisionTransformer(\n\t        img_size=224,\n\t        patch_size=16,\n\t        encoder_embed_dim=1280,\n\t        encoder_depth=32,\n", "        encoder_num_heads=16,\n\t        encoder_num_classes=0,\n\t        decoder_num_classes=1536,  # 16 * 16 * 3 * 2\n\t        decoder_embed_dim=512,\n\t        decoder_num_heads=8,\n\t        mlp_ratio=4,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n\t    model.default_cfg = _cfg()\n", "    if pretrained:\n\t        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n\t        model.load_state_dict(checkpoint[\"model\"])\n\t    return model\n\t@register_model\n\tdef pretrain_videomae_giant_patch14_224(pretrained=False, **kwargs):\n\t    model = PretrainVisionTransformer(\n\t        img_size=224,\n\t        patch_size=14,\n\t        encoder_embed_dim=1408,\n", "        encoder_depth=40,\n\t        encoder_num_heads=16,\n\t        encoder_num_classes=0,\n\t        decoder_num_classes=1176,  # 14 * 14 * 3 * 2,\n\t        decoder_embed_dim=512,\n\t        decoder_num_heads=8,\n\t        mlp_ratio=48 / 11,\n\t        qkv_bias=True,\n\t        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n\t        **kwargs)\n", "    model.default_cfg = _cfg()\n\t    if pretrained:\n\t        checkpoint = torch.load(kwargs[\"init_ckpt\"], map_location=\"cpu\")\n\t        model.load_state_dict(checkpoint[\"model\"])\n\t    return model\n"]}
