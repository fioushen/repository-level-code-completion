{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tsetup(\n\t  name = 'blockwise-parallel-transformer',\n\t  packages = find_packages(exclude=[]),\n\t  version = '0.1.2',\n\t  license='MIT',\n\t  description = '32x Faster Attentionn',\n\t  author = 'Kye Gomez',\n\t  author_email = 'kye@apac.ai',\n\t  long_description_content_type = 'text/markdown',\n", "  url = 'https://github.com/kyegomez/Blockwise-Parallel-Transformer',\n\t  keywords = [\n\t    'artificial intelligence',\n\t    'deep learning',\n\t    'optimizers',\n\t    \"Prompt Engineering\"\n\t  ],\n\t  install_requires=[\n\t    'jax',\n\t    'torch'\n", "  ],\n\t  classifiers=[\n\t    'Development Status :: 4 - Beta',\n\t    'Intended Audience :: Developers',\n\t    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n\t    'License :: OSI Approved :: MIT License',\n\t    'Programming Language :: Python :: 3.6',\n\t  ],\n\t)"]}
{"filename": "example.py", "chunked_list": ["from jax import random\n\tfrom blockwise_parallel import BlockwiseParallelTransformerAttention\n\tfrom torch.nn import Embedding\n\t#hyperparams\n\tinput_size = 512\n\tnum_heads = 8\n\thidden_size = 512\n\tnum_layers = 6\n\tmax_seq_len = 1024\n\tblock_size = 64\n", "#create random input sequence\n\tkey = random.PRNGKey(0)\n\tx = random.normal(key, (1, max_seq_len, input_size))\n\t#create instance\n\tattention = BlockwiseParallelTransformerAttention(input_size,\n\t                                                  num_heads,\n\t                                                  hidden_size,\n\t                                                  num_layers,\n\t                                                  max_seq_len,\n\t                                                  block_size)\n", "##compute the output of the attention\n\toutput = attention(x)\n\t#print the shape of the output\n\tprint(output.shape)"]}
{"filename": "blockwise_parallel/test1.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tclass BlockwiseParallelTransformer(nn.Module):\n\t    def __init__(self, input_dim, output_dim, head_dim, num_heads, num_query_blocks, num_kv_blocks):\n\t        super(BlockwiseParallelTransformer, self).__init__()\n\t        self.query_blocks = num_query_blocks\n\t        self.kv_blocks = num_kv_blocks\n\t        self.input_dim = input_dim\n\t        self.output_dim = output_dim\n\t        self.head_dim = head_dim\n", "        self.num_heads = num_heads\n\t        self.query_layer = nn.Linear(input_dim, num_heads * head_dim)\n\t        self.key_layer = nn.Linear(input_dim, num_heads * head_dim)\n\t        self.value_layer = nn.Linear(input_dim, num_heads * head_dim)\n\t        self.ffn = nn.Sequential(\n\t            nn.Linear(input_dim, output_dim),\n\t            nn.ReLU(),\n\t            nn.Linear(output_dim, input_dim),\n\t        )\n\t    def forward(self, x):\n", "        b, n, _ = x.shape\n\t        q_chunk_size = n // self.query_blocks\n\t        kv_chunk_size = n // self.kv_blocks\n\t        outputs = torch.zeros_like(x)\n\t        for q_idx in range(self.query_blocks):\n\t            q_chunk_start = q_idx * q_chunk_size\n\t            q_chunk_end = (q_idx + 1) * q_chunk_size\n\t            q_ = self.query_layer(x[:, q_chunk_start:q_chunk_end])\n\t            q = q_.view(b, q_chunk_size, self.num_heads, self.head_dim)\n\t            q = q / torch.sqrt(torch.tensor(self.head_dim).float())\n", "            attn_numerator = torch.zeros_like(q)\n\t            attn_denominator = torch.zeros_like(q)\n\t            for kv_idx in range(self.kv_blocks):\n\t                kv_chunk_start = kv_idx * kv_chunk_size\n\t                kv_chunk_end = (kv_idx + 1) * kv_chunk_size\n\t                k_ = self.key_layer(x[:, kv_chunk_start:kv_chunk_end])\n\t                v_ = self.value_layer(x[:, kv_chunk_start:kv_chunk_end])\n\t                k = k_.view(b, kv_chunk_size, self.num_heads, self.head_dim)\n\t                v = v_.view(b, kv_chunk_size, self.num_heads, self.head_dim)\n\t                attn_weight = torch.einsum('bhqd,bkhd->bhqk', q, k)\n", "                max_score, _ = torch.max(attn_weight, dim=-1, keepdim=True)\n\t                exp_weight = torch.exp(attn_weight - max_score)\n\t                attn_numerator += torch.einsum('bhqv,bvhf->bhqf', exp_weight, v)\n\t                attn_denominator += exp_weight.sum(dim=-1, keepdim=True)\n\t            attn_out = (attn_numerator / attn_denominator)\n\t            attn_out = attn_out.contiguous().view(-1, self.num_heads * self.head_dim)\n\t            ffn_out = self.ffn(attn_out + x[:, q_chunk_start:q_chunk_end])\n\t            outputs[:, q_chunk_start:q_chunk_end] = ffn_out + attn_out + x[:, q_chunk_start:q_chunk_end]\n\t        return outputs\n\t#inpout sequence\n", "batch_size = 2\n\tseq_len = 1024\n\tinput_size = 512\n\tx = torch.randn(batch_size, seq_len, input_size)\n\t#define params\n\tnum_heads = 8\n\thidden_size = 512\n\tnum_layers = 6\n\tmax_seq_len = 1024\n\tblock_size = 64\n", "#crete an instance of blockwise paralel\n\tmodel = BlockwiseParallelTransformer(input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size)\n\t#pass the input sequence to the module to get the output\n\toutput = model(x)\n\tprint(output.shape)"]}
{"filename": "blockwise_parallel/blockwise_parallel_jax.py", "chunked_list": ["# import jax\n\t# import jax.numpy as jnp\n\t# from jax import nn, lax\n\t# from jax.experimental.stax import Dense\n\t# class BlockwiseParallelTransformerAttention:\n\t#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n\t#         self.input_size = input_size\n\t#         self.num_heads = num_heads\n\t#         self.hidden_size = hidden_size\n\t#         self.num_layers = num_layers\n", "#         self.max_seq_len = max_seq_len\n\t#         self.block_size = block_size\n\t#         self.dim_per_head = hidden_size // num_heads\n\t#         self.query_chunk_size = max_seq_len // block_size\n\t#         self.key_value_chunk_size = max_seq_len // block_size\n\t#         self.num_query_chunks = (max_seq_len + self.query_chunk_size - 1) // self.query_chunk_size\n\t#         self.num_key_value_chunks = (max_seq_len + self.key_value_chunk_size - 1) // self.key_value_chunk_size\n\t#         self.query_position_ids = jnp.arange(max_seq_len)\n\t#         self.key_value_position_ids = jnp.arange(max_seq_len)\n\t#         self.query_blocks = Dense(hidden_size, name='query')\n", "#         self.key_blocks = Dense(hidden_size, name='key')\n\t#         self.value_blocks = Dense(hidden_size, name='value')\n\t#         self.feedforward = Dense(hidden_size, name='feedforward')\n\t#     def _chunk_bias_fn(self, query_chunk_idx, key_chunk_idx):\n\t#         start = key_chunk_idx * self.key_value_chunk_size\n\t#         end = (key_chunk_idx + 1) * self.key_value_chunk_size\n\t#         bias_chunk = jnp.zeros((self.num_heads, self.query_chunk_size, self.key_value_chunk_size))\n\t#         bias_chunk = lax.dynamic_update_slice(bias_chunk, jnp.ones((self.num_heads, self.query_chunk_size, end - start)), (slice(None), slice(None), slice(start, end)))\n\t#         bias_chunk = jnp.expand_dims(bias_chunk, axis=0)\n\t#         bias_chunk = jnp.tile(bias_chunk, (query_chunk_idx.shape[0], 1, 1, 1))\n", "#         return bias_chunk\n\t#     def _query_block(self, input_chunk, query_chunk_idx):\n\t#         query_chunk = self.query_blocks(input_chunk)\n\t#         query_chunk = query_chunk / jnp.sqrt(query_chunk.shape[-1])\n\t#         return query_chunk\n\t#     def _key_value_blocks(self, carry, args):\n\t#         kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n\t#         query_chunk, query_chunk_idx = carry\n\t#         key_chunk = self.key_blocks(kv_chunk)\n\t#         value_chunk = self.value_blocks(kv_chunk)\n", "#         attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk)\n\t#         bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n\t#         bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n\t#         attn_weights = attn_weights + bias_chunk\n\t#         max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n\t#         exp_weights = jnp.exp(attn_weights - max_score)\n\t#         exp_values = jnp.einsum('bqhv,bvhf->bqhf', exp_weights, value_chunk)\n\t#         numerator = jax.lax.dynamic_update_slice(query_chunk, exp_values, (slice(None), key_chunk_idx, slice(None), slice(None)))\n\t#         denominator = jax.lax.dynamic_update_slice(query_chunk, exp_weights.sum(axis=-1, keepdims=True), (slice(None), key_chunk_idx, slice(None), slice(None)))\n\t#         return (numerator, denominator), None\n", "#     def __call__(self, x, deterministic=True):\n\t#         batch_size, seq_len, input_size = x.shape\n\t#         assert input_size == self.input_size, f\"Input size must be {self.input_size} but got {input_size}\"\n\t#         query_chunks = x.reshape(batch_size, self.num_query_chunks, self.query_chunk_size, input_size)\n\t#         query_chunks = self.query_blocks(query_chunks)\n\t#         query_chunks = query_chunks / jnp.sqrt(query_chunks.shape[-1])\n\t#         query_position_ids = jnp.tile(self.query_position_ids, (batch_size, 1))\n\t#         query_position_ids = query_position_ids.reshape(batch_size, self.num_query_chunks, self.query_chunk_size)\n\t#         query_position_ids = jax.lax.dynamic_slide(query_position_ids, (0, 0, 0), (batch_size, self.num_query_chunks, self.query_chunk_size - 1))\n\t#         query_position_ids = jnp.concatenate([query_position_ids, jnp.ones((batch_size, self.num_query_chunks, 1)) * (self.max_seq_len - 1)], axis=-1)\n", "#         query_position_ids = query_position_ids.astype(jnp.int32)\n\t#         key_value_chunks = x.reshape(batch_size, self.num_key_value_chinks, self.key_value_chunk_size, input_size)\n\t#         key_value_chunks = jax.lax.stop_gradient(key_value_chunks) if deterministic else key_value_chunks\n\t#         key_value_position_ids = jnp.tile(self.key_value_position_ids, (batch_size, 1))\n\t#         key_value_position_ids = key_value_position_ids.reshape(batch_size, self.num_value_chunks, self.key_value_chunk_size)\n\t#         key_value_position_ids = jax.lax.dynamic_slice(key_value_position_ids, (0, 0, 0), (batch_size, self.num_key_value_chunks, self.key_value_chunk_size - 1))\n\t#         key_value_position_ids = jnp.concatenate([key_value_position_ids, jnp.ones((batch_size, self.num_key_value_chunks, 1)) * (self.max_seq_len - 1)], axis=-1)\n\t#         key_value_position_ids = key_value_position_ids.astype(jnp.int32)\n\t#         query_blocks = jax.lax.map(self._query_block, query_chunks, jnp.arange(self.num_query_chunks))\n\t#         query_blocks = query_blocks.reshape(batch_size, self.num_query_chunks, self.num_heads, self.query_chunk_size, self.dim_per_head)\n", "#         query_blocks = jnp.moveaxis(query_blocks, 2, 3)\n\t#         key_value_blocks = key_value_chunks.reshape(batch_size, self.num_key_value_chunks, self.num_heads, self.key_value_chunk_size, self.dim_per_head)\n\t#         key_value_blocks = jnp.moveaxis(key_value_blocks, 2, 3)\n\t#         carry = (query_blocks, None)\n\t#         key_value_blocks = jax.lax.scan(self._key_value_blocks, carry, (key_value_blocks, jnp.arange(self.num_key_value_chunks), key_value_position_ids))[0][0]\n\t#         key_value_blocks = jnp.moveaxis(key_value_blocks, 2, 3)\n\t#         key_value_blocks = key_value_blocks.reshape(batch_size, self.num_key_value_chunks, self.key_value_chunk_size, self.hidden_size)\n\t#         output = jax.lax.map(lambda x: self.feedforward(x.reshape(-1, self.hidden_size)), key_value_blocks)\n\t#         output = output.reshape(batch_size, seq_len, self.hidden_size)\n\t#         return output\n", "#==================================== v2\n\t# import jax\n\t# import jax.numpy as jnp\n\t# from jax.experimental import stax\n\t# class BlockwiseParallelTransformerAttention(nn.Module):\n\t#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n\t#         super(BlockwiseParallelTransformerAttention, self).__init__()\n\t#         self.input_size = input_size\n\t#         self.num_heads = num_heads\n\t#         self.hidden_size = hidden_size\n", "#         self.num_layers = num_layers\n\t#         self.max_seq_len = max_seq_len\n\t#         self.block_size = block_size\n\t#         self.query_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n\t#         self.key_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n\t#         self.value_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n\t#         self.feedforward = nn.Sequential(\n\t#             stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal()),\n\t#             nn.ReLU(),\n\t#             stax.Dense(num_heads * hidden_size, W_init=jax.nn.initializers.glorot_normal())\n", "#         )\n\t#         self.layer_norm1 = nn.LayerNorm(input_size)\n\t#         self.layer_norm2 = nn.LayerNorm(num_heads * hidden_size)\n\t#     def forward(self, x):\n\t#         batch_size, seq_len, input_size = x.shape\n\t#         num_blocks = seq_len // self.block_size\n\t#         query_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n\t#         key_value_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n\t#         for i in range(self.num_layers):\n\t#             query = self.query_blocks(query_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n", "#             key = self.key_blocks(key_value_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n\t#             value = self.value_blocks(key_value_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n\t#             query = query.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n\t#             key = key.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n\t#             value = value.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n\t#             attention_scores = jnp.matmul(query, key.transpose((0, 1, 2, 4, 3))) / jnp.sqrt(jnp.array(self.hidden_size, dtype=jnp.float32))\n\t#             attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n\t#             attention_output = jnp.matmul(attention_weights, value)\n\t#             attention_output = attention_output.transpose((0, 2, 3, 1, 4)).reshape(batch_size*num_blocks, self.block_size, self.num_heads*self.hidden_size)\n\t#             attention_output = self.feedforward(attention_output)\n", "#             attention_output = attention_output.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 2, 1, 3, 4)).reshape(batch_size, seq_len, self.num_heads*self.hidden_size)\n\t#             attention_output = self.layer_norm1(query_blocks + attention_output)\n\t#             attention_output = self.layer_norm2(attention_output)\n\t#         return attention_output\n\t    # def __call__(self, x, deterministic=True):\n\t    #     batch_size, seq_len, input_size = x.shape\n\t    #     assert input_size == self.input_size, f'Input size must be {self.input_size}, but got {input_size}'\n\t    #     query_chunks = x.reshape(batch_size, self.num_query_chunks, self.query_chunk_size, input_size)\n\t    #     query_chunks = self.query_blocks(query_chunks)\n\t    #     query_chunks = query_chunks / jnp.sqrt(query_chunks.shape[-1])\n", "    #     kv_chunks = x.reshape(batch_size, self.num_key_value_chunks, self.key_value_chunk_size, input_size)\n\t    #     kv_chunks = self.key_blocks(kv_chunks), self.value_blocks(kv_chunks)\n\t    #     init_carry = (jnp.zeros((batch_size, self.query_chunk_size, self.num_heads, self.dim_per_head)),\n\t    #                   jnp.zeros((batch_size, self.query_chunk_size, self.num_heads, self.dim_per_head)),\n\t    #                   (-jnp.inf) * jnp.ones((batch_size, self.query_chunk_size, self.num_heads, 1)))\n\t    #     def attention_block(carry, args):\n\t    #         query_chunk, query_chunk_idx = carry\n\t    #         kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n\t    #         key_chunk, value_chunk = kv_chunk\n\t    #         attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk)\n", "    #         bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n\t    #         bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n\t    #         attn_weights = attn_weights + bias_chunk\n\t    #         max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n\t    #         exp_weights = jnp.exp(attn_weights - max_score)\n\t    #         exp_values = jnp.einsum('bqhv,bvhf->bqhf', exp_weights, value_chunk)\n\t    #         numerator = jax.lax.dynamic_update_slice(query_chunk, exp_values, (slice(None), query_chunk_idx, slice(None), slice(None)))\n\t    #         denominator = jax.lax.dynamic_update_slice(query_chunk, exp_weights.sum(axis=-1, keepdims=True), (slice(None), query_chunk_idx, slice(None), slice(None)))\n\t    #         return (numerator, denominator), None\n\t    #     def combine_blocks(carry, args):\n", "    #         query_chunk, query_chunk_idx = carry\n\t    #         numerator, denominator = args\n\t    #         numerator = jnp.concatenate([query_chunk, numerator], axis=2)\n\t    #         denominator = jnp.concatenate([jnp.ones_like(query_chunk), denominator], axis=2)\n\t    #         attn_output = jnp.sum(numerator / denominator, axis=2)\n\t    #         attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)\n\t    #         attn_output = attn_output + x\n\t    #         return (attn_output, query_chunk_idx + 1), None\n\t    #     def feedforward_block(x):\n\t    #         hidden = self.feedforward(x)\n", "    #         hidden = nn.gelu(hidden)\n\t    #         return hidden + x\n\t    #     for layer_idx in range(self.num_layers):\n\t    #         query_chunk_idx = 0\n\t    #         carry = (query_chunks[:, query_chunk_idx], query_chunk_idx)\n\t    #         for key_chunk_idx in range(self.num_key_value_chunks):\n\t    #             kv_chunk = kv_chunks[:, key_chunk_idx]\n\t    #             kv_position_ids_chunk = self.key_value_position_ids[key_chunk_idx * self.key_value_chunk_size:(key_chunk_idx + 1) * self.key_value_chunk_size]\n\t    #             carry, _ = BlockParallel(self.num_heads)(attention_block, carry, (kv_chunk, key_chunk_idx, kv_position_ids_chunk))\n\t    #         attn_output, _ = BlockParallel()(combine_blocks, carry, None)\n", "    #         x = attn_output\n\t    #         x = BlockParallel()(feedforward_block, x)\n\t    #     return x\n\t    #     # for key_chunk_idx in range(self.num_key_value_chunks):\n\t    #     #     for key_chunk_idx in range(self.num_key_value_chunks):\n\t    #     #         key_value_chunk = kv_chunks[:, key_chunk_idx]\n\t    #     #         key_value_position_ids_chunk = self.key_value_position_ids[key_chunk_idx * self.key_value_chunk_size:(key_chunk_idx + 1) * self.key_value_chunk_size]\n\t    #     #         carry, _ = lax.scan(self._key_value_blocks, carry, (key_value_chunk, key_chunk_idx, key_value_position_ids_chunk))\n\t    #     #     numerator, denominator, bias = carry\n\t    #     #     attn_weights = numerator / denominator\n", "    #     #     attn_weights = jax.lax.dynamic_update_slice(attn_weights, bias, (slice(None), slice(None), slice(None), 0))\n\t    #     #     attn_weights = nn.softmax(attn_weights, axis=-2)\n\t    #     #     attn_weights = jax.lax.dynamic_update_slice(attn_weights, jnp.zeros_like(bias), (slice(None), slice(None), slice(None), 0))\n\t    #     #     value_chunk = jnp.einsum('bqhv,bvhf->bqhf', attn_weights, kv_chunks)\n\t    #     #     value_chunk = value_chunk.reshape(batch_size, self.num_heads * self.query_chunk_size, self.dim_per_head)\n\t    #     #     value_chunk = self.feedforward(value_chunk)\n\t    #     #     value_chunk = value_chunk.reshape(batch_size, self.num_heads, self.query_chunk_size, self.dim_per_head)\n\t    #     #     value_chunk = jnp.moveaxis(value_chunk, 1, 2)\n\t    #     #     if query_chunk_idx == 0:\n\t    #     #         output = value_chunk\n", "    #     #     else:\n\t    #     #         output = jnp.concatenate([output, value_chunk], axis=2)\n\t    #     # output = output.reshape(batch_size, seq_len, self.hidden_size)\n\t    #     # return output\n\t    #     # # def _key_value_blocks(cell, carry, args):\n\t    #     # #     kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n\t    #     # #     query_chunk, query_chunk_idx = carry\n\t    #     # #     key_chunk = self.key_blocks(kv_chunk)\n\t    #     # #     value_chunk = self.value_blocks(kv_chunk)\n\t    #     # #     attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk)\n", "    #     # #     bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n\t    #     # #     bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n\t    #     # #     attn_weights = attn_weights + bias_chunk\n\t    #     # #     max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n\t    #     # #     exp_weights = jnp.exp(attn_weights - max_score)\n\t    #     # #     exp_values = jnp.einsum('bqhv,bvhf->bqhf', exp_weights, value_chunk)\n\t    #     # #     numerator = jax.lax.dynamic_update_slice(query_chunk, exp_values, (slice(None), key_chunk_idx, slice(None), slice(None)))\n\t    #     # #     denominator = jax.lax.dynamic_update_slice(query_chunk, exp_weights.sum(axis=-1, keepdims=True), (slice(None), key_chunk_idx, slice(None), slice(None)))\n\t    #     # #     return (numerator, denominator), None\n\t    #     # # for query_chunk_idx in range(self.num_query_chunks):\n", "    #     # #     query_chunk = self._query_block(query_chunks[:, query_chunk_idx], query_chunk_idx)\n\t    #     # #     for key_value_chunk_idx in range(self.num_key_value_chunks):\n\t    #     # #         kv_chunk = kv_chunks[:, key_value_chunk_idx, :, :]\n\t    #     # #         init_carry = (query_chunk, query_chunk_idx)\n\t    #     # #         (numerator, denominator), _ = lax.scan(_key_value_blocks, init_carry, (kv_chunk, key_value_chunk_idx))\n\t    #     # #     attention_output_chunk = numerator / denominator \n\t    #     # #     attention_output_chunk = self.feedforward(attention_output_chunk)\n\t    #     # #     query_chunk = query_chunks[:, query_chunk_idx]\n\t    #     # #     attention_output_chunk = attention_output_chunk + query_chunk\n\t    #     # #     attention_output_chunk = nn.LayerNorm(attention_output_chunk)\n", "    #     # #     query_chunks = jax.lax.dynamic_update_slice(query_chunks, attention_output_chunk, (slice(None), query_chunk_idx, slice(None), slice(None)))\n\t    #     # # attention_output = query_chunks.reshape(batch_size, seq_len, self.hidden_size)\n\t    #     # # return attention_output\n\t    # def BlockParallel(num_blocks=None):\n\t    #     def decorator(f):\n\t    #         def wrapper(*args, **kwargs):\n\t    #             if num_blocks is None:\n\t    #                 num_blocks = jax.local_device_count()\n\t    #             block_size = args[0].shape[0] // num_blocks\n\t    #             blocks = [jax.lax.dynamic_slice_in_dim(args[0], i * block_size, block_size, axis=0) for i in range(num_blocks)]\n", "    #             args = [(block,) + args[1:] for block in blocks]\n\t    #             outputs = jax.pmap(f)(*args, **kwargs)\n\t    #             return jnp.concatenate(outputs, axis=0)\n\t    #         return wrapper\n\t#     #     return decorator\n\t#     import jax\n\t# import jax.numpy as jnp\n\t# from jax.experimental import stax\n\t# class BlockwiseParallelTransformerAttention(nn.Module):\n\t#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n", "#         super(BlockwiseParallelTransformerAttention, self).__init__()\n\t#         self.input_size = input_size\n\t#         self.num_heads = num_heads\n\t#         self.hidden_size = hidden_size\n\t#         self.num_layers = num_layers\n\t#         self.max_seq_len = max_seq_len\n\t#         self.block_size = block_size\n\t#         self.query_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n\t#         self.key_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n\t#         self.value_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n", "#         self.feedforward = nn.Sequential(\n\t#             stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal()),\n\t#             nn.ReLU(),\n\t#             stax.Dense(num_heads * hidden_size, W_init=jax.nn.initializers.glorot_normal())\n\t#         )\n\t#         self.layer_norm1 = nn.LayerNorm(input_size)\n\t#         self.layer_norm2 = nn.LayerNorm(num_heads * hidden_size)\n\t#     def forward(self, x):\n\t#         batch_size, seq_len, input_size = x.shape\n\t#         num_blocks = seq_len // self.block_size\n", "#         query_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n\t#         key_value_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n\t#         for i in range(self.num_layers):\n\t#             query = self.query_blocks(query_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n\t#             key = self.key_blocks(key_value_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n\t#             value = self.value_blocks(key_value_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n\t#             query = query.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n\t#             key = key.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n\t#             value = value.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n\t#             attention_scores = jnp.matmul(query, key.transpose((0, 1, 2, 4, 3))) / jnp.sqrt(jnp.array(self.hidden_size, dtype=jnp.float32))\n", "#             attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n\t#             attention_output = jnp.matmul(attention_weights, value)\n\t#             attention_output = attention_output.transpose((0, 2, 3, 1, 4)).reshape(batch_size*num_blocks, self.block_size, self.num_heads*self.hidden_size)\n\t#             attention_output = self.feedforward(attention_output)\n\t#             attention_output = attention_output.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 2, 1, 3, 4)).reshape(batch_size, seq_len, self.num_heads*self.hidden_size)\n\t#             attention_output = self.layer_norm1(query_blocks + attention_output)\n\t#             attention_output = self.layer_norm2(attention_output)\n\t#         return attention_output\n\t#==================================== v3\n\timport functools\n", "import json\n\timport math\n\tfrom functools import partial\n\tfrom typing import Callable, NamedTuple, Optional\n\timport flax.linen as nn\n\timport jax\n\timport jax.numpy as jnp\n\timport numpy as np\n\tfrom einops import rearrange\n\tfrom flax.linen import combine_masks, make_causal_mask\n", "from jax import lax\n\tfrom jax import numpy as jnp\n\tdef quick_gelu(x):\n\t    return x * jax.nn.sigmoid(1.702 * x)\n\tACT2FN = {\n\t    \"gelu\": partial(nn.gelu, approximate=False),\n\t    \"relu\": nn.relu,\n\t    \"silu\": nn.swish,\n\t    \"swish\": nn.swish,\n\t    \"gelu_new\": partial(nn.gelu, approximate=True),\n", "    \"quick_gelu\": quick_gelu,\n\t}\n\tdef get_gradient_checkpoint_policy(name):\n\t    return {\n\t        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n\t        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n\t        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n\t        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n\t    }[name]\n\tMASK_VALUE = -1e10\n", "Q_CHUNK_SIZE = 1024\n\tK_CHUNK_SIZE = 1024\n\tdef create_sinusoidal_positions(num_pos, dim):\n\t    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n\t    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n\t    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\t    sentinel = dim // 2 + dim % 2\n\t    out = np.zeros((num_pos, dim))\n\t    out[:, 0:sentinel] = sin\n\t    out[:, sentinel:] = cos\n", "    return jnp.array(out)\n\tdef rotate_every_two(tensor):\n\t    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n\t    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n\t    return rotate_half_tensor\n\tdef apply_rotary_pos_emb(tensor, sincos):\n\t    sin_pos, cos_pos = sincos\n\t    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n\t    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n\t    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n", "class _AttentionBlock(nn.Module):\n\t    hidden_size: int\n\t    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n\t    activation_function: str = \"gelu\"\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n\t    dtype: jnp.dtype = jnp.float32\n", "    causal: bool = True\n\t    float32_logits: bool = False\n\t    def setup(self):\n\t        self.embed_dim = self.hidden_size\n\t        self.head_dim = self.embed_dim // self.num_heads\n\t        dense = partial(\n\t            nn.Dense,\n\t            self.embed_dim,\n\t            use_bias=False,\n\t            dtype=self.dtype,\n", "            kernel_init=jax.nn.initializers.variance_scaling(\n\t                scale=1.0, mode='fan_in',\n\t                distribution='normal',\n\t            )\n\t        )\n\t        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n\t        self.out_proj = dense()\n\t        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\t        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\t        self.fc_in = nn.Dense(self.intermediate_size,\n", "                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n\t                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n\t        self.fc_out = nn.Dense(self.embed_dim,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n\t                            scale=1.0, mode='fan_in',\n", "                            distribution='normal',\n\t            )\n\t        )\n\t        self.act = ACT2FN[self.activation_function]\n\t        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            pos_embd_dim = self.rotary_dim\n\t        else:\n\t            pos_embd_dim = self.embed_dim // self.num_heads\n\t        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n", "    def _split_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\t    def _merge_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\t    def attn_out_proj(self, attn_output, deterministic):\n\t        attn_output = self._merge_heads(attn_output)\n\t        attn_output = self.out_proj(attn_output)\n\t        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n\t        return attn_output\n\t    def forward_qkv(\n", "        self,\n\t        hidden_states,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_1(hidden_states)\n\t        query = self.q_proj(hidden_states)\n\t        key = self.k_proj(hidden_states)\n\t        value = self.v_proj(hidden_states)\n\t        query = self._split_heads(query)\n", "        key = self._split_heads(key)\n\t        value = self._split_heads(value)\n\t        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n\t        sincos = jnp.split(sincos, 2, axis=-1)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            k_rot = key[:, :, :, : self.rotary_dim]\n\t            k_pass = key[:, :, :, self.rotary_dim :]\n\t            q_rot = query[:, :, :, : self.rotary_dim]\n\t            q_pass = query[:, :, :, self.rotary_dim :]\n\t            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n", "            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\t            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n\t            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n\t        else:\n\t            key = apply_rotary_pos_emb(key, sincos)\n\t            query = apply_rotary_pos_emb(query, sincos)\n\t        if self.float32_logits:\n\t            query = query.astype(jnp.float32)\n\t            key = key.astype(jnp.float32)\n\t        return query, key, value\n", "    def forward_ffn(\n\t        self,\n\t        hidden_states,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_2(hidden_states)\n\t        hidden_states = self.fc_in(hidden_states)\n\t        hidden_states = self.act(hidden_states)\n\t        hidden_states = self.fc_out(hidden_states)\n\t        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n", "        return hidden_states\n\tclass AttentionBlock(nn.Module):\n\t    q_chunk_size: int\n\t    k_chunk_size: int\n\t    hidden_size: int\n\t    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n\t    activation_function: str = \"gelu\"\n", "    attn_pdrop: float = 0.0\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n\t    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n\t    policy: str = 'nothing_saveable'\n\t    prevent_cse: bool = False\n\t    float32_logits: bool = False\n\t    def setup(self):\n\t        self.attn = _AttentionBlock(\n", "            self.hidden_size,\n\t            self.num_heads,\n\t            self.rotary_dim,\n\t            self.intermediate_size,\n\t            self.layer_norm_epsilon,\n\t            self.activation_function,\n\t            self.resid_pdrop,\n\t            self.max_position_embeddings,\n\t            self.dtype,\n\t            self.causal,\n", "            self.float32_logits,\n\t        )\n\t    @nn.compact\n\t    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\t        \"\"\"\n\t        This function takes projected key, value states from a single input token and concatenates the states to cached\n\t        states from previous steps. This function is slighly adapted from the official Flax repository:\n\t        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n\t        \"\"\"\n\t        # detect if we're initializing by absence of existing cache data.\n", "        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n\t        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n\t        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n\t        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\t        if is_initialized:\n\t            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n\t            # update key, value caches with our new 1d spatial slices\n\t            cur_index = cache_index.value\n\t            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n\t            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n", "            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n\t            cached_key.value = key\n\t            cached_value.value = value\n\t            num_updated_cache_vectors = query.shape[1]\n\t            cache_index.value = cache_index.value + num_updated_cache_vectors\n\t            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n\t            pad_mask = jnp.broadcast_to(\n\t                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n\t                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n\t            )\n", "            attention_mask = combine_masks(pad_mask, attention_mask)\n\t        return key, value, attention_mask\n\t    def __call__(\n\t        self,\n\t        hidden_states,\n\t        attention_mask,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t        init_cache: bool = False,\n\t    ):\n", "        query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n\t        query = query / jnp.sqrt(query.shape[-1])\n\t        dropout_rng = None\n\t        if not deterministic and self.attn_pdrop > 0.0:\n\t            dropout_rng = self.make_rng(\"dropout\")\n\t        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\t        attention_bias = lax.select(\n\t            attention_mask > 0,\n\t            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n\t            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n", "        )\n\t        # During fast autoregressive decoding, we feed one position at a time,\n\t        # and cache the keys and values step by step.\n\t        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n\t            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n\t            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n\t            # use standard dot product attention since query length is 1\n\t            attn_weights = nn.dot_product_attention_weights(\n\t                query,\n\t                key,\n", "                bias=attention_bias,\n\t                dropout_rng=dropout_rng,\n\t                dropout_rate=self.config.attn_pdrop,\n\t                deterministic=deterministic,\n\t                dtype=self.dtype,\n\t                precision=None,\n\t            )\n\t            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n\t            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n\t            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n", "            outputs = attn_output + ffn_output + hidden_states\n\t        else:\n\t            attn_output = blockwise_compute_attn(\n\t                query,\n\t                key,\n\t                value,\n\t                bias=attention_bias,\n\t                deterministic=not deterministic,\n\t                dropout_rng=dropout_rng,\n\t                attn_pdrop=self.attn_pdrop,\n", "                causal_mask=self.causal,\n\t                query_chunk_size=self.q_chunk_size,\n\t                key_chunk_size=self.k_chunk_size,\n\t                dtype=self.dtype,\n\t                policy=self.policy,\n\t                precision=None,\n\t                prevent_cse=self.prevent_cse,\n\t            )\n\t            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n\t            ffn_output = blockwise_compute_ffn(\n", "                self.attn,\n\t                hidden_states + attn_output,\n\t                chunk_size=self.q_chunk_size,\n\t                deterministic=deterministic,\n\t                policy=self.policy,\n\t                prevent_cse=self.prevent_cse,\n\t            )\n\t            outputs = ffn_output + hidden_states + attn_output\n\t        return outputs\n\tdef _chunk_attention_bias(query_chunk_size, key_chunk_size,\n", "            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n\t            query_chunk_idx, key_chunk_idx):\n\t    query_offset = query_chunk_idx * query_chunk_size\n\t    key_offset = key_chunk_idx * key_chunk_size\n\t    chunk_bias = jnp.zeros((1, 1, 1, 1))\n\t    if bias is not None:\n\t        chunk_bias = lax.dynamic_slice(\n\t            bias,\n\t            start_indices=(0, 0, query_offset, key_offset),\n\t            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n", "        )\n\t    if causal_mask:\n\t        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n\t        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n\t        offset = query_offset - key_offset\n\t        query_idx += offset\n\t        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n\t        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\t    if not deterministic and attn_pdrop > 0.0:\n\t        attn_dropout_slice = lax.dynamic_slice(\n", "            attn_dropout,\n\t            start_indices=(0, 0, query_offset, key_offset),\n\t            slice_sizes=(\n\t                *attn_dropout.shape[:2],\n\t                min(attn_dropout.shape[-2], query_chunk_size),\n\t                min(attn_dropout.shape[-1], key_chunk_size),\n\t            ),\n\t        )\n\t        chunk_bias -= attn_dropout_slice * 1e6\n\t    return chunk_bias\n", "class Carry(NamedTuple):\n\t    numerator: jax.Array\n\t    denominator: jax.Array\n\t    max_so_far: jax.Array\n\tdef blockwise_compute_attn(query, key, value,\n\t        bias=None,\n\t        deterministic=False,\n\t        dropout_rng=None,\n\t        attn_pdrop=0.0,\n\t        causal_mask=True,\n", "        query_chunk_size=None,\n\t        key_chunk_size=None,\n\t        dtype=jnp.float32,\n\t        policy='nothing_saveable',\n\t        precision=lax.Precision.HIGHEST,\n\t        prevent_cse=False,):\n\t    q_len = query.shape[1]\n\t    kv_len = key.shape[1]\n\t    query = rearrange(query, 'b (n c) h q -> b n c h q', c=query_chunk_size)\n\t    key, value = map(lambda t: rearrange(t, 'b (n c) h v -> b n c h v', c=key_chunk_size), (key, value))\n", "    query, key, value = map(lambda t: rearrange(t, 'b n c h d -> n b c h d'), (query, key, value))\n\t    num_q, batch, _, num_heads, dim_per_head = query.shape\n\t    num_kv, _, _, _, _ = key.shape\n\t    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n\t        assert bias_dim == 1 or bias_dim == broadcast_dim\n\t    if not deterministic and attn_pdrop > 0.0:\n\t        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n\t        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n\t    else:\n\t        attn_dropout = None\n", "    _chunk_bias_fn = functools.partial(\n\t        _chunk_attention_bias,\n\t        query_chunk_size, key_chunk_size,\n\t        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\t    def _query_chunk_attention(args):\n\t        query_chunk, query_chunk_idx = args\n\t        @functools.partial(jax.checkpoint, prevent_cse=prevent_cse,\n\t                           policy=get_gradient_checkpoint_policy(policy))\n\t        def summarize_chunk(carry, args):\n\t            key_chunk, value_chunk, key_chunk_idx = args\n", "            (numerator, denominator, prev_max_score) = carry\n\t            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n\t            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n\t            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n\t            attn_weights = attn_weights + bias_chunk\n\t            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n\t            max_score = jnp.maximum(prev_max_score, max_score)\n\t            max_score = jax.lax.stop_gradient(max_score)\n\t            exp_weights = jnp.exp(attn_weights - max_score)\n\t            exp_values = jnp.einsum(\n", "                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n\t            )\n\t            correction = jnp.exp(prev_max_score - max_score)\n\t            numerator = numerator * correction + exp_values\n\t            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n\t            return Carry(numerator, denominator, max_score), None\n\t        init_carry = Carry(\n\t            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n\t            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n\t            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n", "        )\n\t        (numerator, denominator, max_score), _ = lax.scan(\n\t            summarize_chunk, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n\t        )\n\t        outputs = (numerator / denominator).astype(dtype)\n\t        return outputs\n\t    _, res = lax.scan(\n\t        lambda _, x: ((), _query_chunk_attention(x)),\n\t        (), xs=(query, jnp.arange(0, num_q))\n\t    )\n", "    res = rearrange(res, 'n b c h d -> b (n c) h d')\n\t    return res\n\tdef blockwise_compute_ffn(cell, inputs, chunk_size, deterministic, policy, prevent_cse):\n\t    inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=chunk_size)\n\t    inputs = rearrange(inputs, 'b n c d -> n b c d')\n\t    num_q, _, _, _ = inputs.shape\n\t    def ffn(cell, _, hidden_states):\n\t        outputs = cell.forward_ffn(hidden_states, deterministic=deterministic)\n\t        return _, outputs\n\t    ffn_remat = nn.remat(\n", "        ffn,\n\t        variables=\"params\",\n\t        rngs={\"params\" : False},\n\t        prevent_cse=prevent_cse,\n\t        policy=get_gradient_checkpoint_policy(policy),\n\t    )\n\t    _, res = nn.scan(\n\t        ffn_remat,\n\t        variable_broadcast=\"params\",\n\t        split_rngs={\"params\": False},\n", "        in_axes=0,\n\t        out_axes=0,\n\t        length=num_q,\n\t    )(cell, None, inputs)\n\t    res = rearrange(res, 'n b c d -> b (n c) d')\n\t    return res\n\tclass Blockwise_LM_Head(nn.Module):\n\t    vocab_size: int\n\t    chunk_size: int\n\t    policy: str = 'nothing_saveable'\n", "    dtype: jnp.dtype = jnp.float32\n\t    prevent_cse: bool = False\n\t    def setup(self):\n\t        self.lm_head = nn.Dense(\n\t            self.vocab_size,\n\t            dtype=self.dtype,\n\t            kernel_init=jax.nn.initializers.variance_scaling(\n\t                scale=1.0, mode='fan_in',\n\t                distribution='normal',\n\t            )\n", "        )\n\t    def __call__(self, inputs):\n\t        inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=self.chunk_size)\n\t        inputs = rearrange(inputs, 'b n c d -> n b c d')\n\t        num_q, _, _, _ = inputs.shape\n\t        def lm_head(cell, _, hidden_states):\n\t            outputs = cell(hidden_states)\n\t            return _, outputs\n\t        lm_head_remat = nn.remat(\n\t            lm_head,\n", "            variables=\"params\",\n\t            rngs={\"params\" : False},\n\t            prevent_cse=self.prevent_cse,\n\t            policy=get_gradient_checkpoint_policy(self.policy),\n\t        )\n\t        _, res = nn.scan(\n\t            lm_head_remat,\n\t            variable_broadcast=\"params\",\n\t            split_rngs={\"params\": False},\n\t            in_axes=0,\n", "            out_axes=0,\n\t            length=num_q,\n\t        )(self.lm_head, None, inputs)\n\t        res = rearrange(res, 'n b c d -> b (n c) d')\n\t        return res\n\tdef blockwise_cross_entropy(logits, tokens, valid=None,\n\t                            chunk_size=None, policy=None, prevent_cse=None):\n\t    if valid is None:\n\t        valid = jnp.ones(tokens.shape[:2])\n\t    valid = valid.astype(jnp.float32)\n", "    logits = jnp.reshape(logits, (-1, logits.shape[-1]))\n\t    tokens = jnp.reshape(tokens, (-1,))\n\t    valid = jnp.reshape(valid, (-1,))\n\t    def _cross_entropy_loss_and_accuracy(logits, tokens, valid):\n\t        valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n\t        token_log_prob = jnp.squeeze(\n\t            jnp.take_along_axis(\n\t                jax.nn.log_softmax(logits, axis=-1),\n\t                jnp.expand_dims(tokens, -1),\n\t                axis=-1,\n", "            ),\n\t            -1,\n\t        )\n\t        token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n\t        correct = jnp.where(\n\t            valid > 0.0,\n\t            jnp.argmax(logits, axis=-1) == tokens,\n\t            jnp.array(False)\n\t        )\n\t        return token_log_prob, correct, valid_text_length\n", "    @partial(jax.checkpoint, prevent_cse=prevent_cse,\n\t             policy=get_gradient_checkpoint_policy(policy))\n\t    def _loss_and_accuracy(carry, args):\n\t        loss, accuracy, num = carry\n\t        logits, tokens, valid = args\n\t        token_log_prob, correct, valid_text_length = \\\n\t            _cross_entropy_loss_and_accuracy(logits, tokens, valid)\n\t        loss = loss + jnp.sum(token_log_prob, axis=-1) / valid_text_length\n\t        accuracy = accuracy + jnp.sum(correct, axis=-1) / valid_text_length\n\t        num = num + 1\n", "        return (loss, accuracy, num), None\n\t    num_chunk = logits.shape[0] // chunk_size\n\t    logits = rearrange(logits, '(n c) d -> n c d', c=chunk_size)\n\t    tokens = rearrange(tokens, '(n c) -> n c', c=chunk_size)\n\t    valid = rearrange(valid, '(n c) -> n c', c=chunk_size)\n\t    (loss, accuracy, num), _ = jax.lax.scan(\n\t        _loss_and_accuracy, (0.0, 0.0, 0), xs=(logits, tokens, valid),\n\t        length=num_chunk,\n\t    )\n\t    loss = - loss / num\n", "    accuracy = accuracy / num\n\t    return loss, accuracy\n\tif __name__ == '__main__':\n\t    with jax.profiler.trace('/tmp/prof/blockwise_parallel_simplified'):\n\t        class Model(nn.Module):\n\t            def setup(self):\n\t                self.blocks = [\n\t                    AttentionBlock(\n\t                        q_chunk_size=256,\n\t                        k_chunk_size=256,\n", "                        hidden_size=2048,\n\t                        num_heads=16,\n\t                        rotary_dim=128,\n\t                        intermediate_size=8192,\n\t                        layer_norm_epsilon=1e-5,\n\t                        activation_function=\"gelu\",\n\t                        resid_pdrop=0.0,\n\t                        max_position_embeddings=2048,\n\t                        dtype=jnp.float32,\n\t                        causal=True,\n", "                )\n\t                for _ in range(2)\n\t                ]\n\t            def __call__(self, hidden_states, attention_mask, position_ids):\n\t                for block in self.blocks:\n\t                    hidden_states = block(hidden_states, attention_mask, position_ids)\n\t                return hidden_states\n\t        hidden_states = jnp.zeros((2, 1024, 2048))\n\t        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n\t        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n", "        model = Model()\n\t        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n\t        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n\t        output = output.block_until_ready()\n"]}
{"filename": "blockwise_parallel/__init__.py", "chunked_list": ["# from blockwise_parallel.blockwise_paralle import BlockwiseParallelTransformerAttention\n\t# from blockwise_parallel.test1 import BlockwiseParallelTransformer/\n\tfrom blockwise_parallel.blockwise_parallel_jax import BlockwiseParallelTransformerAttention"]}
{"filename": "blockwise_parallel/blockwise_torch.py", "chunked_list": ["import torch \n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport numpy as np\n\tfrom einops import rearrange\n\tfrom types import partial\n\tdef quick_gelu(x):\n\t    return x * torch.sigmoid(1.702 * x)\n\tACT2FN = {\n\t    \"gelu\": F.gelu,\n", "    \"relu\": F.relu,\n\t    \"silu\": F.silu,\n\t    \"swish\": F.swish,\n\t    \"gelu_new\": quick_gelu,\n\t    \"quick_gelu\": quick_gelu,\n\t}\n\tMASK_VALUE = -1e10\n\tQ_CHUNK_SIZE = 1024\n\tK_CHUNK_SIZE = 1024\n\tdef create_sinusoidal_positions(num_pos, dim):\n", "    inv_freq = 1.0 / (10000 * (np.arange(0, dim, 2) / dim))\n\t    sinusoid_inp = np.einsum(\"i, j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n\t    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\t    sentinel = dim // 2 + dim % 2\n\t    out = np.zeros((num_pos, dim))\n\t    out[:, 0:sentinel] = sin\n\t    out[:, 0:sentinel] = cos\n\t    return torch.tensor(out)\n\tdef rotate_every_two(tensor):\n\t    rotate_half_tensor = torch.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), dim=-1)\n", "    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n\t    return rotate_half_tensor\n\tdef apply_rotary_pos_emb(tensor, sincos):\n\t    sin_pos, cos_pos = sincos\n\t    sin_pos = sin_pos[:, :, None, :].repeat(1, 1, 2, 1)\n\t    cos_pos = cos_pos[:, :, None, :].repeat(1, 1, 2, 1)\n\t    return (torch * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\tclass BlockwiseParallel(nn.Module):\n\t    def __init__(self, hidden_size, num_heads, rotary_dim, intermediate_size, layer_norm_epsilon=1e-5,\n\t                 activation_function=\"gelu\", resid_pdrop=0.0, max_position_embeddings=1024, dtype=torch.float32,\n", "                 casual=True, float32_logits=False):\n\t        super().__init__()\n\t        self.hidden_size = hidden_size\n\t        self.num_heads = num_heads\n\t        self.rotary_dim = rotary_dim\n\t        self.intermediate_size = intermediate_size\n\t        self.layer_norm_epsilon = layer_norm_epsilon\n\t        self.activation_function = activation_function\n\t        self.resid_pdrop = resid_pdrop\n\t        self.max_position_embeddings = max_position_embeddings\n", "        self.dtype = dtype\n\t        self.casual = casual\n\t        self.float32_logits = float32_logits\n\t        self.embed_dim = self.hidden_size\n\t        self.head_dim = self.embed_dim // self.num_heads\n\t        dense = partial(\n\t            nn.Linear,\n\t            self.embed_dim,\n\t            bias=False,\n\t            dtype=self.dtype\n", "        )\n\t        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n\t        self.out_proj = dense()\n\t        self.ln_1 = nn.LayerNorm(self.hideen_size, eps=self.layer_norm_epsilon, elementwise_affine=True)\n\t        self.ln_2 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_epsilon, elementwise_affine=True)\n\t        self.fc_in = nn.Linear(self.hidden_size, self.intermediate_size, dtype=self.dtype)\n\t        self.fc_out = nn.Linear(self.intermediate_size, self.hidden_size, dtype=self.dtype)\n\t        self.act = ACT2FN(self.activation_function)\n\t        self.resid_pdrop = nn.Dropout(p=self.resid_pdrop)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n", "            pos_embd_dim = self.rotary_dim\n\t        else:\n\t            pos_embd_dim = self.embed_dim // self.num_heads\n\t        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\t    def _split_heads(self, hidden_states):\n\t        return hidden_states.view(hidden_states.shape[-2] + (self.num_heads, self.head_dim))\n\t    def _merge_heads(self, hidden_states):\n\t        return hidden_states.view(hidden_states.shape[:-2] + (self.embed_dim,))\n\t    def attn_out_proj(self, attn_output, deterministic):\n\t        attn_output = self._merge_heads(attn_output)\n", "        attn_output = self.out_proj(attn_output)\n\t        attn_output = self.resid_pdrop(attn_output)\n\t        return attn_output\n\t    def forward_qkv(self, hidden_states, position_ids, deterministic=True):\n\t        hidden_states = self.ln_1(hidden_states)\n\t        q = self.q_proj(hidden_states)\n\t        k = self.k_proj(hidden_states)\n\t        v = self.v_proj(hidden_states)\n\t        q = self._split_heads(q)\n\t        k = self._split_heads(k)\n", "        v = self._split_heads(v)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            sincos = self.embed_positions[position_ids].unsqueeze(1)\n\t            q, k = apply_rotary_pos_emb(q, sincos), apply_rotary_pos_emb(k, sincos)\n\t        return q, k, v\n\t    def forward(self, hidden_states, position_ids, attention_mask=None, deterministic=True):\n\t        q, k, v = self.forward_qkv(hidden_states, position_ids, deterministic)\n\t        attn_output, attn_weights = self._attn(q, k, v, attention_mask, deterministic)\n\t        attn_output = self.attn_out_proj(attn_output, deterministic)\n\t        hidden_states = hidden_states + attn_output\n", "        hidden_states = self.ln_2(hidden_states)\n\t        ffn_output = self.fc_in(hidden_states)\n\t        ffn_output = self.act(ffn_output)\n\t        ffn_output = self.fc_out(ffn_output)\n\t        ffn_output = self.resid_pdrop(ffn_output)\n\t        hidden_states = hidden_states + ffn_output\n\t        return hidden_states, attn_weights\n\t    def _attn(self, q, k, v, attention_mask=None, deterministic=True):\n\t        attn_weights = torch.matmul(q, k.transpose(-1, -2))\n\t        if attention_mask is None:\n", "            attn_weights = attn_weights + attention_mask\n\t        attn_weights = F.softmax(attn_weights, dim=-1)\n\t        if not deterministic:\n\t            attn_weights = self.resid_pdrop(attn_weights)\n\t        attn_output = torch.matmul(attn_weights, v)\n\t        return attn_output, attn_weights\n"]}
{"filename": "blockwise_parallel/blockwise_parallel_torch.py", "chunked_list": ["import torch \n\timport torch.nn as nn\n\tclass BlockwiseParallelTransformerAttention(nn.Module):\n\t    def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n\t        super(BlockwiseParallelTransformerAttention, self).__init__()\n\t        self.input_size = input_size\n\t        self.num_heads = num_heads\n\t        self.hidden_size = hidden_size\n\t        self.num_layers = num_layers\n\t        self.max_seq_len = max_seq_len\n", "        self.block_size = block_size\n\t        self.dim_per_head = hidden_size // num_heads\n\t        self.query_chunk_size = max_seq_len // block_size\n\t        self.key_value_chunk_size = max_seq_len // block_size\n\t        self.num_query_chunks = (max_seq_len + self.query_chunk_size - 1) // self.query_chunk_size\n\t        self.num_key_value_chunks = (max_seq_len + self.key_value_chunk_size - 1) // self.key_value_chunk_size\n\t        self.query_position_ids = torch.arange(max_seq_len)\n\t        self.key_value_position_ids = torch.arange(max_seq_len)\n\t        self.query_blocks = nn.Linear(input_size, hidden_size, bias=False)\n\t        self.key_blocks = nn.Linear(input_size, hidden_size, bias=False)\n", "        self.value_blocks = nn.Linear(input_size, hidden_size, bias=False)\n\t        self.feedforward = nn.Linear(hidden_size, hidden_size)\n\t    def _chunk_bias_fn(self, query_chunk_idx, key_chunk_idx):\n\t        start = key_chunk_idx * self.key_value_chunk_size\n\t        end = (key_chunk_idx + 1) * self.key_value_chunk_size\n\t        bias_chunk = torch.zeros((self.num_heads, self.query_chunk_size, self.key_value_chunk_size))\n\t        bias_chunk[:, :, start:end] = 1\n\t        bias_chunk = bias_chunk.unsqueeze(0)\n\t        bias_chunk = bias_chunk.repeat(query_chunk_idx.shape[0], 1, 1, 1)\n\t        return bias_chunk\n", "    def _query_block(self, input_chunk, query_chunk_idx):\n\t        query_chunk = self.query_blocks(input_chunk)\n\t        query_chunk = query_chunk / torch.sqrt(query_chunk.shape[-1])\n\t        return query_chunk\n\t    def _key_value_blocks(self, carry, args):\n\t        kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n\t        query_chunk, query_chunk_idx = carry\n\t        key_chunk = self.key_blocks(kv_chunk)\n\t        value_chunk = self.value_blocks(kv_chunk)\n\t        attn_weights = torch.einsum('bqhd, bkhd->bqhk', query_chunk. key_chunk)\n", "        bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n\t        bias_chunk = bias_chunk.permute(0, 1, 3, 2)\n\t        attn_weights = attn_weights + bias_chunk\n\t        max_score = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n\t        exp_weights = torch.exp(attn_weights - max_score)\n\t        exp_values = torch.einsum('bqhv, bvhf->bqhf', exp_weights, value_chunk)\n\t        numerator = query_chunk.clone()\n\t        numerator[:, key_chunk_idx, :, :] = exp_values\n\t        denominator = query_chunk.clone()\n\t        denominator[:, key_chunk_idx, :, :] = exp_weights.sum(dim=-1, keepdim=True)\n", "        return (numerator, denominator), None\n\t    def forward(self, x, deterministic=None):\n\t        batch_size, seq_len, input_size = x.shape\n\t        assert input_size == self.input_size, f\"Input size must be {self.input_size} but got {input_size}\"\n\t        query_chunks = x.reshape(batch_size, self.num_query_chunks, self.query_chunk_size, input_size)\n\t        query_chunks = self.query_blocks(query_chunks)\n\t        query_chunks = query_chunks / torch.sqrt(query_chunks.shape[-1])\n\t        query_position_ids = self.query_position_ids.repeat(batch_size, 1)\n\t        query_position_ids = query_position_ids.reshape(batch_size, self.num_query_chunks, self.query_chunk_size)\n\t        query_position_ids = query_position_ids.roll(shift=-1, dims=-1)\n", "        query_position_ids[:, :, -1] = self.max_seq_len - 1\n\t        key_value_chunks = x.reshape(batch_size, self.num_key_value_chunks, self.key_value_chunk_size, input_size)\n\t        key_value_chunks = key_value_chunks.detach() if deterministic else key_value_chunks\n\t        key_value_position_ids = self.key_value_chunk_position_ids.repeat(batch_size, 1)\n\t        key_value_position_ids = key_value_position_ids[:, :-1, :]\n\t        key_value_position_ids = torch.cat([key_value_position_ids, torch.ones((batch_size, 1, self.key_value_chunk_size)) * (self.max_seq_len -1)], dim=1)\n\t        carry = (query_chunks, None)\n\t        for key_chunk_idx in range(self.num_key_value_chunks):\n\t            kv_chunk = key_value_chunks[:, key_chunk_idx, :, :]\n\t            kv_position_ids_chunk = key_value_position_ids[:, key_chunk_idx, :]\n", "            carry, _ = self._key_value_blocks(carry, (kv_chunk, key_chunk_idx, kv_position_ids_chunk))\n\t        attn_output = carry[0]\n\t        attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)\n\t        attn_output = self.feedforward(attn_output)\n\t        return attn_output\n\t#inpout sequence\n\tbatch_size = 2\n\tseq_len = 1024\n\tinput_size = 512\n\tx = torch.randn(batch_size, seq_len, input_size)\n", "#define params\n\tnum_heads = 8\n\thidden_size = 512\n\tnum_layers = 6\n\tmax_seq_len = 1024\n\tblock_size = 64\n\t#crete an instance of blockwise paralel\n\tmodel = BlockwiseParallelTransformerAttention(input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size)\n\t#pass the input sequence to the module to get the output\n\toutput = model(x)\n", "print(output.shape)\n\t# import torch \n\t# from torch import nn\n\t# class BlockwiseParallelTransformerAttention(nn.Module):\n\t#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n\t#         super(BlockwiseParallelTransformerAttention, self).__init__()\n\t#         self.input_size = input_size\n\t#         self.num_heads = num_heads\n\t#         self.hidden_size = hidden_size\n\t#         self.num_layers = num_layers\n", "#         self.max_seq_len = max_seq_len\n\t#         self.block_size = block_size\n\t#         self.query_projection = nn.Linear(input_size, num_heads * hidden_size)\n\t#         self.key_projection = nn.Linear(input_size, num_heads * hidden_size)\n\t#         self.value_projection = nn.Linear(input_size, num_heads * hidden_size)\n\t#         self.feedforward = nn.Sequential(\n\t#             nn.Linear(num_heads * hidden_size, hidden_size),\n\t#             nn.ReLU(),\n\t#             nn.Linear(hidden_size, num_heads * hidden_size)\n\t#         )\n", "#         self.layer_norm1 = nn.LayerNorm(input_size)\n\t#         self.layer_norm2 = nn.LayerNorm(num_heads * hidden_size)\n\t#     def forward(self, x):\n\t#         batch_size, seq_len, input_size = x.size()\n\t#         num_blocks = seq_len // self.block_size\n\t#         query_blocks = x[:, :num_blocks*self.block_size, :].view(batch_size, num_blocks, self.block_size, input_size)\n\t#         key_value_blocks = x[:, :num_blocks*self.block_size, :].view(batch_size, num_blocks, self.block_size, input_size)\n\t#         for i in range(self.num_layers):\n\t#             for outer in range(num_blocks):\n\t#                 query = self.query_projection(query_blocks[:, outer, :, :])\n", "#                 for inner in range(num_blocks):\n\t#                     key = self.key_projection(key_value_blocks[:, inner, :, :])\n\t#                     value = self.value_projection(key_value_blocks[:, inner, :, :])\n\t#                     attention_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float32))\n\t#                     attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n\t#                     attention_output = torch.matmul(attention_weights, value)\n\t#                     if inner == 0:\n\t#                         blockwise_attention_output = attention_output\n\t#                     else:\n\t#                         blockwise_attention_output = torch.cat((blockwise_attention_output, attention_output), dim=2)\n", "#                 blockwise_attention_output = blockwise_attention_output / torch.sqrt(torch.tensor(blockwise_attention_output.size(-1), dtype=torch.float32))\n\t#                 feedforward_output = self.feedforward(blockwise_attention_output)\n\t#                 residual_output = query_blocks[:, outer, :, :] + feedforward_output\n\t#                 query_blocks[:, outer, :, :] = self.layer_norm1(residual_output)\n\t#             query_blocks = self.layer_norm2(query_blocks.view(batch_size, num_blocks*self.block_size, self.num_heads*self.hidden_size)).view(batch_size, num_blocks, self.block_size, self.num_heads*self.hidden_size)\n\t#         return query_blocks.view(batch_size, seq_len, self.num_heads*self.hidden_size)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/model.py", "chunked_list": ["# coding=utf-8\n\t# Copyright 2021 The EleutherAI and The HuggingFace Inc. team.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom functools import partial\n\tfrom typing import Optional, Tuple\n\timport json\n\timport numpy as np\n\timport flax.linen as nn\n\timport jax\n\timport jax.numpy as jnp\n", "from flax.core.frozen_dict import FrozenDict, freeze, unfreeze\n\tfrom flax.linen import combine_masks, make_causal_mask\n\tfrom flax.linen.attention import dot_product_attention_weights\n\tfrom flax.traverse_util import flatten_dict, unflatten_dict\n\tfrom jax import lax\n\tfrom flax.linen import partitioning as nn_partitioning\n\tfrom transformers.configuration_utils import PretrainedConfig\n\tfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput\n\tfrom transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel, append_call_sample_docstring\n\tfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n", "from transformers.generation.flax_logits_process import FlaxLogitsProcessorList\n\tfrom transformers import AutoTokenizer\n\tfrom jax.sharding import PartitionSpec as PS\n\tfrom ml_collections import ConfigDict\n\tfrom ml_collections.config_dict import config_dict\n\tfrom bpt.tools.utils import function_args_to_config, load_pickle, open_file\n\tfrom bpt.tools.jax_utils import (\n\t    with_sharding_constraint, get_jax_mesh, get_gradient_checkpoint_policy\n\t)\n\tfrom bpt.blocks.memeff import AttentionBlock as MemEffAttentionBlock\n", "from bpt.blocks.blockwise_parallel_v1 import AttentionBlock as BPAttentionBlock_v1\n\tfrom bpt.blocks.blockwise_parallel import AttentionBlock as BPAttentionBlock, Blockwise_LM_Head\n\tfrom bpt.blocks.vanilla import AttentionBlock as VanillaAttentionBlock\n\tGPT_STANDARD_CONFIGS = {\n\t    # 1.3B\n\t    '1b': {\n\t        'vocab_size': 50432,\n\t        'n_embd': 2048,\n\t        'n_inner': 8192,\n\t        'n_layer': 24,\n", "        'n_head': 16,\n\t        'n_positions': 16384,\n\t        'initializer_range': 0.02,\n\t        'layer_norm_epsilon': 1e-5,\n\t        'use_cache': True,\n\t        'tie_word_embeddings': False,\n\t        'rotary_dim': 128,\n\t        'bos_token_id': 50256,\n\t        'eos_token_id': 50256,\n\t        'n_real_tokens': 50257,\n", "    },\n\t    # 2.7B\n\t    '3b': {\n\t        'vocab_size': 50432,\n\t        'n_embd': 2560,\n\t        'n_inner': 10240,\n\t        'n_layer': 32,\n\t        'n_head': 32,\n\t        'n_positions': 16384,\n\t        'initializer_range': 0.02,\n", "        'layer_norm_epsilon': 1e-5,\n\t        'use_cache': True,\n\t        'tie_word_embeddings': False,\n\t        'rotary_dim': 80,\n\t        'bos_token_id': 50256,\n\t        'eos_token_id': 50256,\n\t        'n_real_tokens': 50257,\n\t    },\n\t    # 6.7B\n\t    '7b': {\n", "        'vocab_size': 50432,\n\t        'n_embd': 4096,\n\t        'n_inner': 16384,\n\t        'n_layer': 32,\n\t        'n_head': 32,\n\t        'n_positions': 16384,\n\t        'initializer_range': 0.02,\n\t        'layer_norm_epsilon': 1e-5,\n\t        'use_cache': True,\n\t        'tie_word_embeddings': False,\n", "        'rotary_dim': 128,\n\t        'bos_token_id': 50256,\n\t        'eos_token_id': 50256,\n\t        'n_real_tokens': 50257,\n\t    },\n\t    # 13B\n\t    '13b': {\n\t        'vocab_size': 50432,\n\t        'n_embd': 5120,\n\t        'n_inner': 20480,\n", "        'n_layer': 40,\n\t        'n_head': 40,\n\t        'n_positions': 16384,\n\t        'initializer_range': 0.02,\n\t        'layer_norm_epsilon': 1e-5,\n\t        'use_cache': True,\n\t        'tie_word_embeddings': False,\n\t        'rotary_dim': 128,\n\t        'bos_token_id': 50256,\n\t        'eos_token_id': 50256,\n", "        'n_real_tokens': 50257,\n\t    },\n\t    # 30B\n\t    '30b': {\n\t        'vocab_size': 50432,\n\t        'n_embd': 7168,\n\t        'n_inner': 28672,\n\t        'n_layer': 48,\n\t        'n_head': 56,\n\t        'n_positions': 16384,\n", "        'initializer_range': 0.02,\n\t        'layer_norm_epsilon': 1e-5,\n\t        'use_cache': True,\n\t        'tie_word_embeddings': False,\n\t        'rotary_dim': 128,\n\t        'bos_token_id': 50256,\n\t        'eos_token_id': 50256,\n\t        'n_real_tokens': 50257,\n\t    },\n\t    # 70B\n", "    '70b': {\n\t        'vocab_size': 50432,\n\t        'n_embd': 8192,\n\t        'n_inner': 32768,\n\t        'n_layer': 80,\n\t        'n_head': 64,\n\t        'n_positions': 16384,\n\t        'initializer_range': 0.02,\n\t        'layer_norm_epsilon': 1e-5,\n\t        'use_cache': True,\n", "        'tie_word_embeddings': False,\n\t        'rotary_dim': 128,\n\t        'bos_token_id': 50256,\n\t        'eos_token_id': 50256,\n\t        'n_real_tokens': 50257,\n\t    },\n\t    'debug': { # A small model for debugging\n\t        'vocab_size': 50432,\n\t        'n_embd': 128,\n\t        'n_inner': 256,\n", "        'n_layer': 2,\n\t        'n_head': 4,\n\t        'n_positions': 16384,\n\t        'initializer_range': 0.02,\n\t        'layer_norm_epsilon': 1e-5,\n\t        'use_cache': True,\n\t        'tie_word_embeddings': False,\n\t        'rotary_dim': 32,\n\t        'bos_token_id': 50256,\n\t        'eos_token_id': 50256,\n", "        'n_real_tokens': 50257,\n\t    },\n\t}\n\tclass GPTConfig(PretrainedConfig):\n\t    r\"\"\"\n\t    This is the configuration class to store the configuration of a [`GPTModel`]. It is used to instantiate a GPT-J\n\t    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n\t    defaults will yield a similar configuration to that of the GPT-J\n\t    [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B) architecture. Configuration objects inherit from\n\t    [`PretrainedConfig`] and can be used to control the model outputs. Read the documentation from [`PretrainedConfig`]\n", "    for more information.\n\t    Args:\n\t        vocab_size (`int`, *optional*, defaults to 50432):\n\t            Vocabulary size of the GPT-J model. Defines the number of different tokens that can be represented by the\n\t            `inputs_ids` passed when calling [`GPTModel`].\n\t        n_positions (`int`, *optional*, defaults to 2048):\n\t            The maximum sequence length that this model might ever be used with. Typically set this to something large\n\t            just in case (e.g., 512 or 1024 or 2048).\n\t        n_embd (`int`, *optional*, defaults to 4096):\n\t            Dimensionality of the embeddings and hidden states.\n", "        n_layer (`int`, *optional*, defaults to 28):\n\t            Number of hidden layers in the Transformer encoder.\n\t        n_head (`int`, *optional*, defaults to 16):\n\t            Number of attention heads for each attention layer in the Transformer encoder.\n\t        rotary_dim (`int`, *optional*, defaults to 64):\n\t            Number of dimensions in the embedding that Rotary Position Embedding is applied to.\n\t        n_inner (`int`, *optional*, defaults to 0):\n\t            Dimensionality of the inner feed-forward layers. 0 will set it to 4 times n_embd\n\t        activation_function (`str`, *optional*, defaults to `\"gelu_new\"`):\n\t            Activation function, to be selected in the list `[\"relu\", \"silu\", \"gelu\", \"tanh\", \"gelu_new\"]`.\n", "        resid_pdrop (`float`, *optional*, defaults to 0.1):\n\t            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n\t        embd_pdrop (`int`, *optional*, defaults to 0.1):\n\t            The dropout ratio for the embeddings.\n\t        attn_pdrop (`float`, *optional*, defaults to 0.1):\n\t            The dropout ratio for the attention.\n\t        layer_norm_epsilon (`float`, *optional*, defaults to 1e-5):\n\t            The epsilon to use in the layer normalization layers.\n\t        initializer_range (`float`, *optional*, defaults to 0.02):\n\t            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n", "        scale_attn_weights (`bool`, *optional*, defaults to `True`):\n\t            Scale attention weights by dividing by sqrt(hidden_size).\n\t        use_cache (`bool`, *optional*, defaults to `True`):\n\t            Whether or not the model should return the last key/values attentions (not used by all models).\n\t    Example:\n\t    ```python\n\t    >>> from transformers import GPTModel, GPTConfig\n\t    >>> # Initializing a GPT-J 6B configuration\n\t    >>> configuration = GPTConfig()\n\t    >>> # Initializing a model from the configuration\n", "    >>> model = GPTModel(configuration)\n\t    >>> # Accessing the model configuration\n\t    >>> configuration = model.config\n\t    ```\"\"\"\n\t    model_type = \"gpt\"\n\t    attribute_map = {\n\t        \"max_position_embeddings\": \"n_positions\",\n\t        \"hidden_size\": \"n_embd\",\n\t        \"num_attention_heads\": \"n_head\",\n\t        \"num_hidden_layers\": \"n_layer\",\n", "    }\n\t    def __init__(\n\t        self,\n\t        vocab_size=50432,\n\t        n_positions=2048,\n\t        n_embd=4096,\n\t        n_layer=28,\n\t        n_head=16,\n\t        rotary_dim=64,\n\t        n_inner=None,\n", "        activation_function=\"gelu_new\",\n\t        resid_pdrop=0.0,\n\t        embd_pdrop=0.0,\n\t        attn_pdrop=0.0,\n\t        layer_norm_epsilon=1e-5,\n\t        initializer_range=0.02,\n\t        scale_attn_weights=True,\n\t        use_cache=True,\n\t        bos_token_id=50256,\n\t        eos_token_id=50256,\n", "        tie_word_embeddings=False,\n\t        gradient_checkpointing='nothing_saveable',\n\t        n_real_tokens=50257,\n\t        fcm_min_ratio=0.0,\n\t        fcm_max_ratio=0.0,\n\t        causal=True,\n\t        attn_type='dot',\n\t        q_chunk_size=1024,\n\t        k_chunk_size=2048,\n\t        scan_layers=True,\n", "        param_scan_axis=0,\n\t        float32_logits=False,\n\t        **kwargs\n\t    ):\n\t        self.vocab_size = vocab_size\n\t        self.n_positions = n_positions\n\t        self.n_embd = n_embd\n\t        self.n_layer = n_layer\n\t        self.n_head = n_head\n\t        self.n_inner = n_inner\n", "        self.rotary_dim = rotary_dim\n\t        self.activation_function = activation_function\n\t        self.resid_pdrop = resid_pdrop\n\t        self.embd_pdrop = embd_pdrop\n\t        self.attn_pdrop = attn_pdrop\n\t        self.layer_norm_epsilon = layer_norm_epsilon\n\t        self.initializer_range = initializer_range\n\t        self.scale_attn_weights = scale_attn_weights\n\t        self.use_cache = use_cache\n\t        self.gradient_checkpointing = gradient_checkpointing\n", "        self.n_real_tokens = n_real_tokens\n\t        self.fcm_min_ratio = fcm_min_ratio\n\t        self.fcm_max_ratio = fcm_max_ratio\n\t        self.causal = causal\n\t        self.attn_type = attn_type\n\t        self.q_chunk_size = q_chunk_size\n\t        self.k_chunk_size = k_chunk_size\n\t        self.scan_layers = scan_layers\n\t        self.param_scan_axis = param_scan_axis\n\t        self.float32_logits = float32_logits\n", "        if self.n_real_tokens is None:\n\t            self.n_real_tokens = self.vocab_size\n\t        self.bos_token_id = bos_token_id\n\t        self.eos_token_id = eos_token_id\n\t        super().__init__(\n\t            bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n\t        )\n\t    @classmethod\n\t    def get_default_config(cls, updates=None):\n\t        none_arg_types = dict(\n", "            n_inner=int,\n\t            rotary_dim=int,\n\t        )\n\t        config = function_args_to_config(cls.__init__, none_arg_types=none_arg_types)\n\t        if updates is not None:\n\t            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    @staticmethod\n\t    def get_jax_mesh(axis_dims):\n\t        return get_jax_mesh(axis_dims, ('dp', 'fsdp', 'mp'))\n", "    @staticmethod\n\t    def get_partition_rules(scan_layers=False):\n\t        \"\"\" Parition rules for GPT. Note that these rules are orderd, so that\n\t            the beginning rules match first. It is important to use\n\t            PartitionSpec() instead of None here because JAX does not treat\n\t            None as a pytree leaf.\n\t        \"\"\"\n\t        if scan_layers:\n\t            return (\n\t                ('transformer/wte/embedding', PS('mp', 'fsdp')),\n", "                ('attn/(k_proj|q_proj|v_proj)/kernel', PS(None, 'fsdp', 'mp')),\n\t                ('attn/out_proj/kernel', PS(None, 'mp', 'fsdp')),\n\t                ('attn/fc_in/kernel', PS(None, 'fsdp', 'mp')),\n\t                ('attn/fc_in/bias', PS(None, 'mp')),\n\t                ('attn/fc_out/kernel', PS(None, 'mp', 'fsdp')),\n\t                ('attn/fc_out/bias', PS(None, None)),\n\t                ('ln_[0-9]+/bias', PS(None, None)),\n\t                ('[0-9]+/ln_[0-9]+/scale', PS(None, None)),\n\t                ('ln_f/bias', PS(None)),\n\t                ('ln_f/scale', PS(None)),\n", "                ('lm_head/kernel', PS('fsdp', 'mp')),\n\t                ('lm_head/bias', PS('mp')),\n\t                ('.*', PS(None)),\n\t            )\n\t        else:\n\t            return (\n\t                ('transformer/wte/embedding', PS('mp', 'fsdp')),\n\t                ('attn/(k_proj|q_proj|v_proj)/kernel', PS('fsdp', 'mp')),\n\t                ('attn/out_proj/kernel', PS('mp', 'fsdp')),\n\t                ('attn/fc_in/kernel', PS('fsdp', 'mp')),\n", "                ('attn/fc_in/bias', PS('mp')),\n\t                ('attn/fc_out/kernel', PS('mp', 'fsdp')),\n\t                ('attn/fc_out/bias', PS(None)),\n\t                ('ln_[0-9]+/bias', PS(None)),\n\t                ('[0-9]+/ln_[0-9]+/scale', PS(None)),\n\t                ('ln_f/bias', PS(None)),\n\t                ('ln_f/scale', PS(None)),\n\t                ('lm_head/kernel', PS('fsdp', 'mp')),\n\t                ('lm_head/bias', PS('mp')),\n\t                ('.*', PS(None)),\n", "            )\n\t    @staticmethod\n\t    def get_weight_decay_exclusions():\n\t        return (\n\t            'ln_[0-9]+/bias', 'ln_[0-9]+/scale', 'ln_f/bias', 'ln_f/scale',\n\t            'bias'\n\t        )\n\t    @staticmethod\n\t    def rng_keys():\n\t        return ('params', 'dropout', 'fcm')\n", "    @staticmethod\n\t    def get_tokenizer_config(updates=None):\n\t        config = ConfigDict()\n\t        config.name = 'EleutherAI/gpt-j-6B'\n\t        config.bos_token = '<|endoftext|>'\n\t        config.eos_token = '<|endoftext|>'\n\t        config.pad_token = '<|extratoken_40|>'\n\t        config.cls_token = '<|extratoken_41|>'\n\t        config.mask_token = '<|extratoken_42|>'\n\t        if updates is not None:\n", "            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    @classmethod\n\t    def get_tokenizer(cls, config, padding_side='left', truncation_side='right'):\n\t        config = cls.get_tokenizer_config(config)\n\t        return AutoTokenizer.from_pretrained(\n\t            config.name,\n\t            bos_token=config.bos_token,\n\t            eos_token=config.eos_token,\n\t            pad_token=config.pad_token,\n", "            cls_token=config.cls_token,\n\t            mask_token=config.mask_token,\n\t            padding_side=padding_side,\n\t            truncation_side=truncation_side,\n\t        )\n\t    @staticmethod\n\t    def load_pretrained(name, dtype=jnp.float32):\n\t        with jax.default_device(jax.devices(\"cpu\")[0]):\n\t            params = FlaxGPTForCausalLM.from_pretrained(\n\t                name, _do_init=False, dtype=dtype\n", "            )[1]\n\t            params = freeze({'params': params})\n\t        return jax.device_get(params)\n\t    @classmethod\n\t    def load_config(cls, path):\n\t        if path in GPT_STANDARD_CONFIGS:\n\t            return cls.from_dict(GPT_STANDARD_CONFIGS[path])\n\t        load_type, load_path = path.split('::', 1)\n\t        if load_type == 'pickle':\n\t            return cls.from_dict(load_pickle(load_path)['gpt_config'])\n", "        elif load_type == 'json':\n\t            with open_file(load_path, 'r') as fin:\n\t                raw_config = fin.read()\n\t            return cls.from_dict(json.loads(raw_config))\n\t        elif load_type == 'huggingface':\n\t            return cls.from_pretrained(load_path)\n\t        else:\n\t            raise ValueError(f'Unsupported load config type: {load_type}')\n\tlogger = logging.get_logger(__name__)\n\t_CHECKPOINT_FOR_DOC = \"gpt\"\n", "_CONFIG_FOR_DOC = \"GPTConfig\"\n\tGPT_START_DOCSTRING = r\"\"\"\n\t    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the\n\t    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n\t    etc.)\n\t    This model is also a Flax Linen\n\t    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a\n\t    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.\n\t    Finally, this model supports inherent JAX features such as:\n\t    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n", "    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n\t    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n\t    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n\t    Parameters:\n\t        config ([`GPTConfig`]): Model configuration class with all the parameters of the model.\n\t            Initializing with a config file does not load the weights associated with the model, only the\n\t            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.\n\t        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n\t            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n\t            `jax.numpy.bfloat16` (on TPUs).\n", "            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n\t            specified all the computation will be performed with the given `dtype`.\n\t            **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n\t            parameters.**\n\t            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and\n\t            [`~FlaxPreTrainedModel.to_bf16`].\n\t\"\"\"\n\tGPT_INPUTS_DOCSTRING = r\"\"\"\n\t    Args:\n\t        input_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`):\n", "            `input_ids_length` = `sequence_length`. Indices of input sequence tokens in the vocabulary.\n\t            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n\t            [`PreTrainedTokenizer.__call__`] for details.\n\t            [What are input IDs?](../glossary#input-ids)\n\t        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n\t            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\t            - 1 for tokens that are **not masked**,\n\t            - 0 for tokens that are **masked**.\n\t            [What are attention masks?](../glossary#attention-mask)\n\t        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n", "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n\t            config.max_position_embeddings - 1]`.\n\t        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n\t            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n\t            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n\t        output_attentions (`bool`, *optional*):\n\t            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n\t            tensors for more detail.\n\t        output_hidden_states (`bool`, *optional*):\n\t            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n", "            more detail.\n\t        return_dict (`bool`, *optional*):\n\t            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\t\"\"\"\n\tdef create_sinusoidal_positions(num_pos, dim):\n\t    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n\t    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n\t    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\t    sentinel = dim // 2 + dim % 2\n\t    out = np.zeros((num_pos, dim))\n", "    out[:, 0:sentinel] = sin\n\t    out[:, sentinel:] = cos\n\t    return jnp.array(out)\n\tdef rotate_every_two(tensor):\n\t    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n\t    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n\t    return rotate_half_tensor\n\tdef apply_rotary_pos_emb(tensor, sincos):\n\t    sin_pos, cos_pos = sincos\n\t    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n", "    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n\t    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\tclass FlaxGPTBlock(nn.Module):\n\t    config: GPTConfig\n\t    dtype: jnp.dtype = jnp.float32\n\t    def setup(self):\n\t        hidden_size = self.config.hidden_size\n\t        inner_dim = self.config.n_inner if self.config.n_inner is not None else 4 * hidden_size\n\t        attention_blocks = {\n\t            # default vanilla transformer (Vaswani et al).\n", "            'vanilla': VanillaAttentionBlock,\n\t            # default memory efficient transformer (Rabe et al and Dao et al).\n\t            'memeff': MemEffAttentionBlock,\n\t            # default blockwise parallel transformer (Liu et al).\n\t            'blockwise_parallel': BPAttentionBlock,\n\t            # less cleaner blockwise parallel transformer used in the paper.\n\t            'blockwise_parallel_v1': BPAttentionBlock_v1,\n\t        }\n\t        if self.config.attn_type in attention_blocks:\n\t            Block = attention_blocks[self.config.attn_type]\n", "        else:\n\t            raise ValueError(f\"Unknown attention type {self.config.attn_type}\")\n\t        self.attn = Block(\n\t            self.config.q_chunk_size,\n\t            self.config.k_chunk_size,\n\t            self.config.hidden_size,\n\t            self.config.num_attention_heads,\n\t            self.config.rotary_dim,\n\t            inner_dim,\n\t            self.config.layer_norm_epsilon,\n", "            self.config.activation_function,\n\t            self.config.attn_pdrop,\n\t            self.config.resid_pdrop,\n\t            self.config.max_position_embeddings,\n\t            self.dtype,\n\t            self.config.causal,\n\t            policy=self.config.gradient_checkpointing,\n\t            prevent_cse=not self.config.scan_layers,\n\t            float32_logits=self.config.float32_logits,\n\t        )\n", "    def __call__(\n\t        self,\n\t        hidden_states,\n\t        attention_mask=None,\n\t        position_ids=None,\n\t        deterministic: bool = True,\n\t        init_cache: bool = False,\n\t        output_attentions: bool = False,\n\t        fcm_mask=None,\n\t    ):\n", "        attn_outputs = self.attn(\n\t            hidden_states,\n\t            attention_mask=attention_mask,\n\t            position_ids=position_ids,\n\t            deterministic=deterministic,\n\t            init_cache=init_cache,\n\t        )\n\t        attn_weights = None\n\t        if self.config.scan_layers: # NOTE: this is a hack to work with scan_layers\n\t            outputs = attn_outputs, None\n", "        else:\n\t            outputs = (attn_outputs, attn_weights) if output_attentions else (attn_outputs,)\n\t        return outputs\n\tclass FlaxGPTPreTrainedModel(FlaxPreTrainedModel):\n\t    \"\"\"\n\t    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n\t    models.\n\t    \"\"\"\n\t    config_class = GPTConfig\n\t    base_model_prefix = \"transformer\"\n", "    module_class: nn.Module = None\n\t    def __init__(\n\t        self,\n\t        config: GPTConfig,\n\t        input_shape: Tuple = (1, 1),\n\t        seed: int = 0,\n\t        dtype: jnp.dtype = jnp.float32,\n\t        _do_init: bool = True,\n\t        **kwargs,\n\t    ):\n", "        module = self.module_class(config=config, dtype=dtype, **kwargs)\n\t        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\t    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n\t        # init input tensors\n\t        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n\t        attention_mask = jnp.ones_like(input_ids)\n\t        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n\t        params_rng, dropout_rng = jax.random.split(rng)\n\t        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\t        if self.config.add_cross_attention:\n", "            encoder_hidden_states = jnp.zeros(input_shape + (self.config.n_embd,))\n\t            encoder_attention_mask = attention_mask\n\t            module_init_outputs = self.module.init(\n\t                rngs,\n\t                input_ids,\n\t                attention_mask,\n\t                position_ids,\n\t                encoder_hidden_states,\n\t                encoder_attention_mask,\n\t                return_dict=False,\n", "            )\n\t        else:\n\t            module_init_outputs = self.module.init(rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\t        random_params = module_init_outputs[\"params\"]\n\t        if params is not None:\n\t            random_params = flatten_dict(unfreeze(random_params))\n\t            params = flatten_dict(unfreeze(params))\n\t            for missing_key in self._missing_keys:\n\t                params[missing_key] = random_params[missing_key]\n\t            self._missing_keys = set()\n", "            return freeze(unflatten_dict(params))\n\t        else:\n\t            return random_params\n\t    def init_cache(self, batch_size, max_length):\n\t        r\"\"\"\n\t        Args:\n\t            batch_size (`int`):\n\t                batch_size used for fast auto-regressive decoding. Defines the batch size of the initialized cache.\n\t            max_length (`int`):\n\t                maximum possible length for auto-regressive decoding. Defines the sequence length of the initialized\n", "                cache.\n\t        \"\"\"\n\t        # init input variables to retrieve cache\n\t        input_ids = jnp.ones((batch_size, max_length))\n\t        attention_mask = jnp.ones_like(input_ids)\n\t        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\t        init_variables = self.module.init(\n\t            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n\t        )\n\t        return init_variables[\"cache\"]\n", "    def _get_logits_processor(self,*args, **kwargs) -> FlaxLogitsProcessorList:\n\t        processors = super()._get_logits_processor(*args, **kwargs)\n\t        def squash_extra_tokens(input_ids, scores, cur_len):\n\t            return scores.at[:, self.config.n_real_tokens:].set(-float('inf'))\n\t        processors.append(squash_extra_tokens)\n\t        return processors\n\t    @add_start_docstrings_to_model_forward(GPT_INPUTS_DOCSTRING)\n\t    def __call__(\n\t        self,\n\t        input_ids,\n", "        attention_mask=None,\n\t        position_ids=None,\n\t        params: dict = None,\n\t        past_key_values: dict = None,\n\t        dropout_rng: jax.random.PRNGKey = None,\n\t        train: bool = False,\n\t        output_attentions: Optional[bool] = None,\n\t        output_hidden_states: Optional[bool] = None,\n\t        return_dict: Optional[bool] = None,\n\t    ):\n", "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n\t        output_hidden_states = (\n\t            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n\t        )\n\t        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\t        batch_size, sequence_length = input_ids.shape\n\t        if position_ids is None:\n\t            if past_key_values is not None:\n\t                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\t            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n", "        if attention_mask is None:\n\t            attention_mask = jnp.ones((batch_size, sequence_length))\n\t        # Handle any PRNG if needed\n\t        rngs = {}\n\t        if dropout_rng is not None:\n\t            rngs[\"dropout\"] = dropout_rng\n\t        inputs = {\"params\": params or self.params}\n\t        # if past_key_values are passed then cache is already initialized a private flag init_cache has to be passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that it can be changed by FlaxGPTAttention module\n\t        if past_key_values:\n\t            inputs[\"cache\"] = past_key_values\n", "            mutable = [\"cache\"]\n\t        else:\n\t            mutable = False\n\t        outputs = self.module.apply(\n\t            inputs,\n\t            jnp.array(input_ids, dtype=\"i4\"),\n\t            jnp.array(attention_mask, dtype=\"i4\"),\n\t            jnp.array(position_ids, dtype=\"i4\"),\n\t            not train,\n\t            False,\n", "            output_attentions,\n\t            output_hidden_states,\n\t            return_dict,\n\t            rngs=rngs,\n\t            mutable=mutable,\n\t        )\n\t        # add updated cache to model output\n\t        if past_key_values is not None and return_dict:\n\t            outputs, past_key_values = outputs\n\t            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n", "            return outputs\n\t        elif past_key_values is not None and not return_dict:\n\t            outputs, past_key_values = outputs\n\t            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\t        return outputs\n\tclass FlaxGPTBlockCollection(nn.Module):\n\t    config: GPTConfig\n\t    dtype: jnp.dtype = jnp.float32\n\t    @nn.compact\n\t    def __call__(\n", "        self,\n\t        hidden_states,\n\t        attention_mask=None,\n\t        position_ids=None,\n\t        deterministic: bool = True,\n\t        init_cache: bool = False,\n\t        output_attentions: bool = False,\n\t        output_hidden_states: bool = False,\n\t        return_dict: bool = True,\n\t    ):\n", "        all_attentions = () if output_attentions else None\n\t        all_hidden_states = () if output_hidden_states else None\n\t        if not deterministic and self.config.fcm_max_ratio > 0:\n\t            # Apply forgetful causal mask\n\t            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n\t            fcm_ratio = jax.random.uniform(\n\t                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n\t                minval=self.config.fcm_min_ratio,\n\t                maxval=self.config.fcm_max_ratio\n\t            )\n", "            fcm_mask = jax.random.uniform(\n\t                self.make_rng('fcm'),\n\t                shape=(batch_size, 1, seq_length, seq_length)\n\t            ) > fcm_ratio\n\t            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n\t            fcm_mask = fcm_mask.astype('bool')\n\t        else:\n\t            fcm_mask = None\n\t        block = FlaxGPTBlock\n\t        if self.config.gradient_checkpointing != '':\n", "            FlaxGPT2CheckpointBlock = nn.remat(\n\t                block, static_argnums=(3, 4, 5, 6),\n\t                prevent_cse=not self.config.scan_layers,\n\t                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing)\n\t            )\n\t            block = FlaxGPT2CheckpointBlock\n\t        if self.config.scan_layers:\n\t            initializing = self.is_mutable_collection('params')\n\t            params_spec = (\n\t                self.config.param_scan_axis if initializing else\n", "                nn_partitioning.ScanIn(self.config.param_scan_axis))\n\t            cache_spec = 0\n\t            hidden_states, _ = nn.scan(\n\t                block,\n\t                variable_axes={\n\t                    'params': params_spec,\n\t                    'cache': cache_spec,\n\t                    'intermediates': 0\n\t                },\n\t                split_rngs={\n", "                    'params': True,\n\t                    'dropout': True\n\t                },\n\t                in_axes=(nn.broadcast, nn.broadcast, nn.broadcast, nn.broadcast, nn.broadcast, nn.broadcast),\n\t                length=self.config.num_hidden_layers,\n\t                metadata_params={nn.PARTITION_NAME: 'scan_decoder_layer'},\n\t                )(config=self.config, name='scan_decoder', dtype=self.dtype)(\n\t                    hidden_states,\n\t                    attention_mask,\n\t                    position_ids,\n", "                    deterministic,\n\t                    init_cache,\n\t                    output_attentions,\n\t                    fcm_mask,\n\t                )\n\t        else:\n\t            blocks = [\n\t                block(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.num_hidden_layers)\n\t            ]\n\t            for block in blocks:\n", "                if output_hidden_states:\n\t                    all_hidden_states += (hidden_states,)\n\t                layer_outputs = block(\n\t                    hidden_states,\n\t                    attention_mask,\n\t                    position_ids,\n\t                    deterministic,\n\t                    init_cache,\n\t                    output_attentions,\n\t                    fcm_mask,\n", "                )\n\t                hidden_states = layer_outputs[0]\n\t                if output_attentions:\n\t                    all_attentions += (layer_outputs[1],)\n\t        # this contains possible `None` values - `FlaxGPTModule` will filter them out\n\t        outputs = (hidden_states, all_hidden_states, all_attentions)\n\t        return outputs\n\tclass FlaxGPTModule(nn.Module):\n\t    config: GPTConfig\n\t    dtype: jnp.dtype = jnp.float32\n", "    def setup(self):\n\t        self.embed_dim = self.config.hidden_size\n\t        self.wte = nn.Embed(\n\t            self.config.vocab_size,\n\t            self.config.hidden_size,\n\t            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n\t        )\n\t        self.dropout = nn.Dropout(rate=self.config.embd_pdrop)\n\t        self.h = FlaxGPTBlockCollection(self.config, dtype=self.dtype)\n\t        self.ln_f = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n", "    def __call__(\n\t        self,\n\t        input_ids,\n\t        attention_mask,\n\t        position_ids,\n\t        deterministic=True,\n\t        init_cache: bool = False,\n\t        output_attentions: bool = False,\n\t        output_hidden_states: bool = False,\n\t        return_dict: bool = True,\n", "    ):\n\t        input_embeds = self.wte(input_ids.astype(\"i4\"))\n\t        hidden_states = self.dropout(input_embeds, deterministic=deterministic)\n\t        outputs = self.h(\n\t            hidden_states,\n\t            attention_mask,\n\t            position_ids=position_ids,\n\t            deterministic=deterministic,\n\t            init_cache=init_cache,\n\t            output_attentions=output_attentions,\n", "            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n\t        )\n\t        hidden_states = outputs[0]\n\t        hidden_states = self.ln_f(hidden_states)\n\t        if output_hidden_states:\n\t            all_hidden_states = outputs[1] + (hidden_states,)\n\t            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n\t        else:\n\t            outputs = (hidden_states,) + outputs[1:]\n", "        if not return_dict:\n\t            return tuple(v for v in outputs if v is not None)\n\t        return FlaxBaseModelOutput(\n\t            last_hidden_state=hidden_states,\n\t            hidden_states=outputs[1],\n\t            attentions=outputs[-1],\n\t        )\n\t@add_start_docstrings(\n\t    \"The bare GPT Model transformer outputting raw hidden-states without any specific head on top.\",\n\t    GPT_START_DOCSTRING,\n", ")\n\tclass FlaxGPTModel(FlaxGPTPreTrainedModel):\n\t    module_class = FlaxGPTModule\n\tappend_call_sample_docstring(\n\t    FlaxGPTModel,\n\t    _CHECKPOINT_FOR_DOC,\n\t    FlaxCausalLMOutput,\n\t    _CONFIG_FOR_DOC,\n\t)\n\tclass FlaxGPTForCausalLMModule(nn.Module):\n", "    config: GPTConfig\n\t    dtype: jnp.dtype = jnp.float32\n\t    def setup(self):\n\t        self.transformer = FlaxGPTModule(self.config, dtype=self.dtype)\n\t        if self.config.attn_type == 'blockwise_parallel' or self.config.attn_type == 'blockwise_parallel_v1':\n\t            self.lm_head = Blockwise_LM_Head(self.config.vocab_size,\n\t                                      self.config.q_chunk_size, dtype=self.dtype,\n\t                                      prevent_cse=not self.config.scan_layers)\n\t        else:\n\t            self.lm_head = nn.Dense(\n", "                self.config.vocab_size,\n\t                dtype=self.dtype,\n\t                kernel_init=jax.nn.initializers.variance_scaling(\n\t                    scale=1.0, mode='fan_in',\n\t                    distribution='normal',\n\t                )\n\t            )\n\t    def __call__(\n\t        self,\n\t        input_ids,\n", "        attention_mask=None,\n\t        position_ids=None,\n\t        deterministic: bool = True,\n\t        init_cache: bool = False,\n\t        output_attentions: bool = False,\n\t        output_hidden_states: bool = False,\n\t        return_dict: bool = True,\n\t    ):\n\t        batch_size, seq_length = input_ids.shape\n\t        if attention_mask is None:\n", "            attention_mask = jnp.ones_like(input_ids)\n\t        if position_ids is None:\n\t            position_ids = jnp.broadcast_to(\n\t                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n\t                (batch_size, seq_length)\n\t            )\n\t        outputs = self.transformer(\n\t            input_ids,\n\t            attention_mask,\n\t            position_ids,\n", "            deterministic=deterministic,\n\t            init_cache=init_cache,\n\t            output_attentions=output_attentions,\n\t            output_hidden_states=output_hidden_states,\n\t            return_dict=return_dict,\n\t        )\n\t        hidden_states = outputs[0]\n\t        if self.config.tie_word_embeddings:\n\t            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n\t            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n", "        else:\n\t            lm_logits = self.lm_head(hidden_states)\n\t        if not return_dict:\n\t            return (lm_logits,) + outputs[1:]\n\t        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n\t@add_start_docstrings(\n\t    \"\"\"\n\t    The GPT Model transformer with a language modeling head on top.\n\t    \"\"\",\n\t    GPT_START_DOCSTRING,\n", ")\n\tclass FlaxGPTForCausalLM(FlaxGPTPreTrainedModel):\n\t    module_class = FlaxGPTForCausalLMModule\n\t    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jnp.DeviceArray] = None):\n\t        # initializing the cache\n\t        batch_size, seq_length = input_ids.shape\n\t        past_key_values = self.init_cache(batch_size, max_length)\n\t        # Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.\n\t        # But since GPT uses a causal mask, those positions are masked anyways.\n\t        # Thus we can create a single static attention_mask here, which is more efficient for compilation\n", "        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n\t        if attention_mask is not None:\n\t            position_ids = attention_mask.cumsum(axis=-1) - 1\n\t            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n\t        else:\n\t            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\t        return {\n\t            \"past_key_values\": past_key_values,\n\t            \"attention_mask\": extended_attention_mask,\n\t            \"position_ids\": position_ids,\n", "        }\n\t    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n\t        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n\t        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n\t        return model_kwargs\n\tappend_call_sample_docstring(\n\t    FlaxGPTForCausalLM,\n\t    _CHECKPOINT_FOR_DOC,\n\t    FlaxCausalLMOutput,\n\t    _CONFIG_FOR_DOC,\n", ")\n"]}
{"filename": "blockwise-parallel-transformer/bpt/train.py", "chunked_list": ["import dataclasses\n\timport pprint\n\tfrom functools import partial\n\timport re\n\tfrom tqdm import tqdm, trange\n\timport numpy as np\n\timport bpt.tools.utils as utils\n\timport jax\n\timport jax.numpy as jnp\n\tfrom jax.experimental.pjit import pjit\n", "from jax.sharding import PartitionSpec as PS\n\timport flax\n\tfrom flax import linen as nn\n\tfrom flax.jax_utils import prefetch_to_device\n\tfrom flax.training.train_state import TrainState\n\timport optax\n\tfrom bpt.data import Dataset, TextProcessor\n\tfrom bpt.tools.checkpoint import StreamingCheckpointer\n\tfrom bpt.tools.optimizers import OptimizerFactory\n\tfrom bpt.tools.jax_utils import (\n", "    JaxRNG, next_rng, match_partition_rules,\n\t    cross_entropy_loss_and_accuracy, named_tree_map, global_norm,\n\t    set_random_seed, average_metrics, get_weight_decay_mask,\n\t    make_shard_and_gather_fns, with_sharding_constraint, tree_apply, get_metrics,\n\t)\n\tfrom bpt.model import GPTConfig, FlaxGPTForCausalLMModule\n\tfrom bpt.blocks.blockwise_parallel import blockwise_cross_entropy\n\tFLAGS, FLAGS_DEF = utils.define_flags_with_default(\n\t    seed=42,\n\t    initialize_jax_distributed=False,\n", "    mesh_dim='1,-1,1',\n\t    total_steps=10000,\n\t    load_gpt_config='',\n\t    update_gpt_config='',\n\t    load_checkpoint='',\n\t    load_dataset_state='',\n\t    log_freq=50,\n\t    save_model_freq=0,\n\t    save_milestone_freq=0,\n\t    eval_steps=0,\n", "    tokenizer=GPTConfig.get_tokenizer_config(),\n\t    text_processor=TextProcessor.get_default_config(),\n\t    train_dataset=Dataset.get_default_config(),\n\t    eval_dataset=Dataset.get_default_config(),\n\t    optimizer=OptimizerFactory.get_default_config(),\n\t    checkpointer=StreamingCheckpointer.get_default_config(),\n\t    gpt=GPTConfig.get_default_config(),\n\t    logger=utils.WandBLogger.get_default_config(),\n\t    log_all_worker=False,\n\t    profile_steps=0,\n", "    stop_after_profile=True,\n\t)\n\tdef main(argv):\n\t    if FLAGS.initialize_jax_distributed:\n\t        jax.distributed.initialize()\n\t    variant = utils.get_user_flags(FLAGS, FLAGS_DEF)\n\t    flags_config_dict = utils.user_flags_to_config_dict(FLAGS, FLAGS_DEF)\n\t    logger = utils.WandBLogger(\n\t        config=FLAGS.logger,\n\t        variant=variant,\n", "        enable=FLAGS.log_all_worker or (jax.process_index() == 0),\n\t    )\n\t    set_random_seed(FLAGS.seed)\n\t    if FLAGS.load_dataset_state != '':\n\t        dataset = utils.load_pickle(FLAGS.load_dataset_state)\n\t    else:\n\t        tokenizer = GPTConfig.get_tokenizer(FLAGS.tokenizer)\n\t        text_processor = TextProcessor(FLAGS.text_processor, tokenizer)\n\t        dataset = Dataset(FLAGS.train_dataset, tokenizer, text_processor)\n\t    if FLAGS.eval_steps > 0:\n", "        eval_dataset = Dataset(\n\t            FLAGS.eval_dataset, dataset.tokenizer, dataset.text_processor,\n\t        )\n\t        eval_iterator = iter(eval_dataset.val_iter())\n\t    seq_length = dataset.seq_length\n\t    if FLAGS.load_gpt_config != '':\n\t        gpt_config = GPTConfig.load_config(FLAGS.load_gpt_config)\n\t        update_gpt_config = GPTConfig(**FLAGS.gpt)\n\t        gpt_config.update(dict(\n\t            q_chunk_size=update_gpt_config.q_chunk_size,\n", "            k_chunk_size=update_gpt_config.k_chunk_size,\n\t            attn_type=update_gpt_config.attn_type,\n\t            n_positions=update_gpt_config.n_positions,\n\t            gradient_checkpointing=update_gpt_config.gradient_checkpointing,\n\t            scan_layers=update_gpt_config.scan_layers,\n\t            param_scan_axis=update_gpt_config.param_scan_axis,\n\t        ))\n\t    else:\n\t        gpt_config = GPTConfig(**FLAGS.gpt)\n\t    if FLAGS.update_gpt_config != '':\n", "        gpt_config.update(dict(eval(FLAGS.update_gpt_config)))\n\t    gpt_config.update(dict(\n\t        bos_token_id=dataset.tokenizer.bos_token_id,\n\t        eos_token_id=dataset.tokenizer.eos_token_id,\n\t    ))\n\t    if gpt_config.vocab_size < dataset.vocab_size:\n\t        gpt_config.update(dict(vocab_size=dataset.vocab_size))\n\t    model = FlaxGPTForCausalLMModule(gpt_config)\n\t    optimizer, optimizer_info = OptimizerFactory.get_optimizer(\n\t        FLAGS.optimizer,\n", "        get_weight_decay_mask(GPTConfig.get_weight_decay_exclusions()),\n\t    )\n\t    def create_trainstate_from_params(params):\n\t        return TrainState.create(params=params, tx=optimizer, apply_fn=None)\n\t    def init_fn(rng):\n\t        rng_generator = JaxRNG(rng)\n\t        params = model.init(\n\t            input_ids=jnp.zeros((4, seq_length), dtype=jnp.int32),\n\t            position_ids=jnp.zeros((4, seq_length), dtype=jnp.int32),\n\t            attention_mask=jnp.ones((4, seq_length), dtype=jnp.int32),\n", "            rngs=rng_generator(gpt_config.rng_keys()),\n\t        )\n\t        return TrainState.create(params=params, tx=optimizer, apply_fn=None)\n\t    if FLAGS.gpt.attn_type == 'blockwise_parallel' or FLAGS.gpt.attn_type == 'blockwise_parallel_v1':\n\t        cross_entropy_loss_and_accuracy_fn = partial(blockwise_cross_entropy,\n\t                                                     policy=FLAGS.gpt.gradient_checkpointing,\n\t                                                     chunk_size=FLAGS.gpt.q_chunk_size,\n\t                                                     prevent_cse=not FLAGS.gpt.scan_layers,)\n\t    else:\n\t        cross_entropy_loss_and_accuracy_fn = cross_entropy_loss_and_accuracy\n", "    def train_step(train_state, rng, batch):\n\t        rng_generator = JaxRNG(rng)\n\t        input_tokens = with_sharding_constraint(batch['input_tokens'], PS(('dp', 'fsdp')))\n\t        output_tokens = with_sharding_constraint(batch['output_tokens'], PS(('dp', 'fsdp')))\n\t        loss_masks = with_sharding_constraint(batch['loss_masks'], PS(('dp', 'fsdp')))\n\t        def loss_and_accuracy(params):\n\t            logits = model.apply(\n\t                params,\n\t                input_tokens,\n\t                deterministic=False,\n", "                rngs=rng_generator(gpt_config.rng_keys()),\n\t            ).logits\n\t            return cross_entropy_loss_and_accuracy_fn(logits, output_tokens, loss_masks)\n\t        grad_fn = jax.value_and_grad(loss_and_accuracy, has_aux=True)\n\t        (loss, accuracy), grads = grad_fn(train_state.params)\n\t        train_state = train_state.apply_gradients(grads=grads)\n\t        metrics = dict(\n\t            loss=loss,\n\t            accuracy=accuracy,\n\t            learning_rate=optimizer_info['learning_rate_schedule'](train_state.step),\n", "            gradient_norm=global_norm(grads),\n\t            param_norm=global_norm(train_state.params),\n\t        )\n\t        return train_state, rng_generator(), metrics\n\t    def eval_step(train_state, rng, batch):\n\t        rng_generator = JaxRNG(rng)\n\t        input_tokens = with_sharding_constraint(batch['input_tokens'], PS(('dp', 'fsdp')))\n\t        output_tokens = with_sharding_constraint(batch['output_tokens'], PS(('dp', 'fsdp')))\n\t        loss_masks = with_sharding_constraint(batch['loss_masks'], PS(('dp', 'fsdp')))\n\t        logits = model.apply(\n", "            train_state.params,\n\t            input_tokens,\n\t            deterministic=True,\n\t            rngs=rng_generator(gpt_config.rng_keys()),\n\t        ).logits\n\t        loss, accuracy = cross_entropy_loss_and_accuracy_fn(logits, output_tokens, loss_masks)\n\t        metrics = dict(\n\t            loss=loss,\n\t            accuracy=accuracy,\n\t        )\n", "        return rng_generator(), metrics\n\t    train_state_shapes = jax.eval_shape(init_fn, next_rng())\n\t    train_state_partition = match_partition_rules(\n\t        GPTConfig.get_partition_rules(FLAGS.gpt.scan_layers), train_state_shapes\n\t    )\n\t    num_params = sum(x.size for x in jax.tree_leaves(train_state_shapes.params))\n\t    num_nonembed_params = num_params - gpt_config.vocab_size * gpt_config.n_embd\n\t    param_stats = {\"num_params\": num_params,\"num_nonembed_params\": num_nonembed_params}\n\t    logger.log(param_stats)\n\t    tqdm.write(\"\\n\" + pprint.pformat(param_stats) + \"\\n\")\n", "    shard_fns, gather_fns = make_shard_and_gather_fns(\n\t        train_state_partition, train_state_shapes\n\t    )\n\t    checkpointer = StreamingCheckpointer(\n\t        FLAGS.checkpointer, logger.output_dir,\n\t        enable=jax.process_index() == 0,\n\t    )\n\t    sharded_init_fn = pjit(\n\t        init_fn,\n\t        in_shardings=PS(),\n", "        out_shardings=train_state_partition\n\t    )\n\t    sharded_create_trainstate_from_params = pjit(\n\t        create_trainstate_from_params,\n\t        in_shardings=(train_state_partition.params, ),\n\t        out_shardings=train_state_partition,\n\t        donate_argnums=(0, ),\n\t    )\n\t    sharded_train_step = pjit(\n\t        train_step,\n", "        in_shardings=(train_state_partition, PS(), PS()),\n\t        out_shardings=(train_state_partition, PS(), PS()),\n\t        donate_argnums=(0, 1),\n\t    )\n\t    sharded_eval_step = pjit(\n\t        eval_step,\n\t        in_shardings=(train_state_partition, PS(), PS()),\n\t        out_shardings=(PS(), PS()),\n\t        donate_argnums=(1,),\n\t    )\n", "    def save_checkpoint(train_state, milestone=False):\n\t        step = int(jax.device_get(train_state.step))\n\t        metadata = dict(\n\t            step=step,\n\t            variant=variant,\n\t            flags=flags_config_dict,\n\t            gpt_config=gpt_config.to_dict(),\n\t        )\n\t        checkpointer.save_all(\n\t            train_state=train_state,\n", "            gather_fns=gather_fns,\n\t            metadata=metadata,\n\t            dataset=dataset.get_state_dict(),\n\t            milestone=milestone,\n\t        )\n\t    if FLAGS.profile_steps > 0:\n\t        import os\n\t        os.makedirs(logger.profile_dir, exist_ok=True)\n\t        mesh = GPTConfig.get_jax_mesh(FLAGS.mesh_dim)\n\t        with mesh:\n", "            train_state, restored_params = None, None\n\t            if train_state is None and restored_params is None:\n\t                # Initialize from scratch\n\t                train_state = sharded_init_fn(next_rng())\n\t            elif train_state is None and restored_params is not None:\n\t                # Restore from params but initialize train_state\n\t                train_state = sharded_create_trainstate_from_params(restored_params)\n\t                del restored_params\n\t            sharded_rng = next_rng()\n\t            # warmup\n", "            for batch, dataset_metrics in dataset:\n\t                train_state, sharded_rng, metrics = sharded_train_step(\n\t                    train_state, sharded_rng, batch\n\t                )\n\t                break\n\t            # profile\n\t            jax.profiler.start_trace(logger.profile_dir)\n\t            for step, (batch, dataset_metrics) in zip(trange(FLAGS.profile_steps), dataset):\n\t                train_state, sharded_rng, metrics = sharded_train_step(\n\t                    train_state, sharded_rng, batch\n", "                )\n\t                jax.block_until_ready(train_state)\n\t                jax.profiler.save_device_memory_profile(f'{logger.profile_dir}/memory{step}.prof')\n\t            jax.profiler.stop_trace()\n\t        if FLAGS.stop_after_profile:\n\t            exit()\n\t    mesh = GPTConfig.get_jax_mesh(FLAGS.mesh_dim)\n\t    with mesh:\n\t        train_state, restored_params = None, None\n\t        if FLAGS.load_checkpoint != '':\n", "            load_type, load_path = FLAGS.load_checkpoint.split('::', 1)\n\t            if load_type == 'huggingface':\n\t                restored_params = tree_apply(\n\t                    shard_fns.params, gpt_config.load_pretrained(load_path)\n\t                )\n\t                train_state = None\n\t            else:\n\t                train_state, restored_params = checkpointer.load_trainstate_checkpoint(\n\t                    FLAGS.load_checkpoint, train_state_shapes, shard_fns\n\t                )\n", "        if train_state is None and restored_params is None:\n\t            # Initialize from scratch\n\t            train_state = sharded_init_fn(next_rng())\n\t        elif train_state is None and restored_params is not None:\n\t            # Restore from params but initialize train_state\n\t            train_state = sharded_create_trainstate_from_params(restored_params)\n\t            del restored_params\n\t        start_step = int(jax.device_get(train_state.step))\n\t        if FLAGS.save_model_freq > 0:\n\t            save_checkpoint(train_state)\n", "        sharded_rng = next_rng()\n\t        step_counter = trange(start_step, FLAGS.total_steps, ncols=0)\n\t        def run_eval(sharded_rng, eval_fn, batch, eval_steps, eval_name):\n\t            eval_metric_list = []\n\t            for _ in range(eval_steps):\n\t                sharded_rng, eval_metrics = eval_fn(\n\t                    train_state, sharded_rng, batch\n\t                )\n\t                eval_metric_list.append(eval_metrics)\n\t            log_metrics = get_metrics(eval_metric_list, stack=True)\n", "            mean_metrics = {\n\t                f\"{eval_name}/{k}\": np.mean(v)\n\t                for k, v in log_metrics.items()\n\t            }\n\t            mean_metrics[\"step\"] = step\n\t            logger.log(mean_metrics)\n\t            tqdm.write(\"\\n\" + pprint.pformat(mean_metrics) + \"\\n\")\n\t            return sharded_rng\n\t        for step, (batch, dataset_metrics) in zip(step_counter, dataset):\n\t            train_state, sharded_rng, metrics = sharded_train_step(\n", "                train_state, sharded_rng, batch\n\t            )\n\t            if step % FLAGS.log_freq == 0:\n\t                if FLAGS.eval_steps > 0:\n\t                    batch, _ = next(eval_iterator)\n\t                    sharded_rng = run_eval(sharded_rng, sharded_eval_step,\n\t                                           batch, FLAGS.eval_steps, \"val\")\n\t                log_metrics = {\"step\": step}\n\t                log_metrics.update(metrics)\n\t                log_metrics.update(dataset_metrics)\n", "                log_metrics = jax.device_get(log_metrics)\n\t                logger.log(log_metrics)\n\t                tqdm.write(\"\\n\" + pprint.pformat(log_metrics) + \"\\n\")\n\t            if FLAGS.save_milestone_freq > 0 and (step + 1) % FLAGS.save_milestone_freq == 0:\n\t                save_checkpoint(train_state, milestone=True)\n\t            elif FLAGS.save_model_freq > 0 and (step + 1) % FLAGS.save_model_freq == 0:\n\t                save_checkpoint(train_state)\n\t        if FLAGS.save_model_freq > 0:\n\t            save_checkpoint(train_state)\n\tif __name__ == \"__main__\":\n", "    utils.run(main)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/__init__.py", "chunked_list": []}
{"filename": "blockwise-parallel-transformer/bpt/data.py", "chunked_list": ["import dataclasses\n\timport pprint\n\timport time\n\tfrom functools import partial\n\timport json\n\tfrom multiprocessing import Pool\n\timport h5py\n\timport bpt.tools.utils as utils\n\tfrom ml_collections.config_dict import config_dict\n\tfrom ml_collections import ConfigDict\n", "from tqdm import tqdm, trange\n\timport numpy as np\n\tfrom datasets import load_dataset\n\tclass TextProcessor(object):\n\t    \"\"\" Example processor that converts a dictionary of texts into tokens. \"\"\"\n\t    @staticmethod\n\t    def get_default_config(updates=None):\n\t        config = ConfigDict()\n\t        config.fields_from_example = ''\n\t        config.fields = ''\n", "        config.subfield_separator = ' '\n\t        config.add_eos_token = True\n\t        config.prepend_text = ''\n\t        if updates is not None:\n\t            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    def __init__(self, config, tokenizer):\n\t        self.config = self.get_default_config(config)\n\t        assert self.config.fields != '' or self.config.fields_from_example != '', (\n\t            'Either fields or fields_from_example must be specified.'\n", "        )\n\t        self.tokenizer = tokenizer\n\t    def __call__(self, example, has_aux=False):\n\t        if has_aux:\n\t            example, *aux = example\n\t        else:\n\t            aux = tuple()\n\t        token_buffer = []\n\t        loss_mask_buffer = []\n\t        if self.config.fields_from_example != '':\n", "            fields = example[self.config.fields_from_example].split(',')\n\t        else:\n\t            fields = self.config.fields.split(',')\n\t        for i, field in enumerate(fields):\n\t            if field.startswith('[') and field.endswith(']'):\n\t                # No loss for this field.\n\t                field = field[1:-1]\n\t                mask = 0.0\n\t            else:\n\t                mask = 1.0\n", "            if field == '<|bos|>':\n\t                token_buffer.append(self.tokenizer.bos_token_id)\n\t                loss_mask_buffer.append(mask)\n\t            elif field == '<|eos|>':\n\t                token_buffer.append(self.tokenizer.eos_token_id)\n\t                loss_mask_buffer.append(mask)\n\t            else:\n\t                subfields = field.split('+')\n\t                text = self.config.subfield_separator.join(\n\t                    [example[subfield] for subfield in subfields]\n", "                )\n\t                if i == 0:\n\t                    text = self.config.prepend_text + text\n\t                tokens = self.tokenizer.encode(text)\n\t                token_buffer.extend(tokens)\n\t                loss_mask_buffer.extend([mask for _ in range(len(tokens))])\n\t        if self.config.add_eos_token:\n\t            token_buffer.append(self.tokenizer.eos_token_id)\n\t            loss_mask_buffer.append(1.0)\n\t        return token_buffer, loss_mask_buffer, *aux\n", "class Dataset(object):\n\t    @staticmethod\n\t    def get_default_config(updates=None):\n\t        config = ConfigDict()\n\t        config.path = ''\n\t        config.seq_length = 1024\n\t        config.batch_size = 8\n\t        config.start_seek_loc = 0\n\t        config.index_at_start = 0\n\t        config.tokenizer_processes = 1\n", "        config.tokenizer_parallel_chunk_size = 32\n\t        config.tokenizer_parallel_batch_size = 1024\n\t        if updates is not None:\n\t            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    def __init__(self, config, tokenizer, text_processor):\n\t        self.config = self.get_default_config(config)\n\t        assert self.config.path != ''\n\t        self._tokenizer = tokenizer\n\t        self._text_processor = text_processor\n", "        self._index = self.config.index_at_start\n\t        self._file_loc = self.config.start_seek_loc\n\t        self._n_batch = 0\n\t    def parse_json(self, line):\n\t        if not line or line == '\\n':\n\t            return None\n\t        try:\n\t            data = json.loads(line)\n\t        except json.decoder.JSONDecodeError:\n\t            print(f'Error parsing json line:\\n{line}')\n", "            return None\n\t        return data\n\t    def json_iterator(self):\n\t        with utils.open_file(self.config.path, 'r') as fin:\n\t            fin.seek(self._file_loc)\n\t            while True:\n\t                line = fin.readline()\n\t                self._file_loc = fin.tell()\n\t                if not line:   # Reached EOF\n\t                    self._index = 0\n", "                    fin.seek(0)\n\t                    continue\n\t                data = self.parse_json(line)\n\t                if data is not None:\n\t                    # JSON parsing succeeded\n\t                    yield data, self._file_loc, self._index\n\t                self._index += 1\n\t    def batched(self, iterator, batch_size):\n\t        batch = []\n\t        for example in iterator:\n", "            batch.append(example)\n\t            if len(batch) == batch_size:\n\t                yield batch\n\t                batch = []\n\t        if len(batch) > 0:\n\t            yield batch\n\t    def parallel_example_iterator(self):\n\t        if self.config.tokenizer_processes == 1:\n\t            for example, loc, index in self.json_iterator():\n\t                yield self.text_processor((example, loc, index), has_aux=True)\n", "        else:\n\t            process_pool = Pool(self.config.tokenizer_processes)\n\t            batched_iterator = self.batched(\n\t                self.json_iterator(), self.config.tokenizer_parallel_batch_size\n\t            )\n\t            with process_pool as pool:\n\t                map_fn = partial(self.text_processor, has_aux=True)\n\t                next_batch = pool.map_async(\n\t                    map_fn, next(batched_iterator),\n\t                    chunksize=self.config.tokenizer_parallel_chunk_size\n", "                )\n\t                while True:\n\t                    current_batch = next_batch\n\t                    next_batch = pool.map_async(\n\t                        map_fn, next(batched_iterator),\n\t                        chunksize=self.config.tokenizer_parallel_chunk_size\n\t                    )\n\t                    for example in current_batch.get():\n\t                        yield example\n\t    def __iter__(self):\n", "        chunk_size = self.config.batch_size * self.config.seq_length\n\t        token_buffer = []\n\t        loss_mask_buffer = []\n\t        total_tokens = 0\n\t        last_time = 0.0\n\t        for tokens, loss_masks, loc, index in self.parallel_example_iterator():\n\t            token_buffer.extend(tokens)\n\t            loss_mask_buffer.extend(loss_masks)\n\t            while len(token_buffer) > chunk_size + 1:\n\t                total_tokens += chunk_size\n", "                metrics = {\n\t                    'dataset_file_loc': loc,\n\t                    'dataset_example_index': index,\n\t                    'dataset_total_tokens': total_tokens,\n\t                    'dataset_throughput_tps': chunk_size / (time.time() - last_time),\n\t                }\n\t                last_time = time.time()\n\t                input_tokens = np.array(token_buffer[:chunk_size], dtype=np.int32)\n\t                output_tokens = np.array(token_buffer[1:chunk_size+1], dtype=np.int32)\n\t                # reshape to batch_size x seq_length\n", "                input_tokens = input_tokens.reshape(self.config.batch_size, -1)\n\t                output_tokens = output_tokens.reshape(self.config.batch_size, -1)\n\t                loss_masks = np.array(loss_mask_buffer[:chunk_size], dtype=np.float32).reshape(self.config.batch_size, -1)\n\t                yield {\n\t                    \"input_tokens\": input_tokens,\n\t                    \"output_tokens\": output_tokens,\n\t                    \"loss_masks\": loss_masks,\n\t                }, metrics\n\t                token_buffer = token_buffer[chunk_size:]\n\t                loss_mask_buffer = loss_mask_buffer[chunk_size:]\n", "    def val_iter(self):\n\t        chunk_size = self.config.batch_size * self.config.seq_length\n\t        token_buffer = []\n\t        loss_mask_buffer = []\n\t        total_tokens = 0\n\t        last_time = 0.0\n\t        for tokens, loss_masks, loc, index in self.parallel_example_iterator():\n\t            token_buffer.extend(tokens)\n\t            loss_mask_buffer.extend(loss_masks)\n\t            while len(token_buffer) > chunk_size + 1:\n", "                total_tokens += chunk_size\n\t                metrics = {\n\t                    'dataset_file_loc': loc,\n\t                    'dataset_example_index': index,\n\t                    'dataset_total_tokens': total_tokens,\n\t                    'dataset_throughput_tps': chunk_size / (time.time() - last_time),\n\t                }\n\t                last_time = time.time()\n\t                input_tokens = np.array(token_buffer[:chunk_size], dtype=np.int32)\n\t                output_tokens = np.array(token_buffer[1:chunk_size+1], dtype=np.int32)\n", "                # reshape to batch_size x seq_length\n\t                input_tokens = input_tokens.reshape(self.config.batch_size, -1)\n\t                output_tokens = output_tokens.reshape(self.config.batch_size, -1)\n\t                loss_masks = np.array(loss_mask_buffer[:chunk_size], dtype=np.float32).reshape(self.config.batch_size, -1)\n\t                yield {\n\t                    \"input_tokens\": input_tokens,\n\t                    \"output_tokens\": output_tokens,\n\t                    \"loss_masks\": loss_masks,\n\t                }, metrics\n\t                token_buffer = token_buffer[chunk_size:]\n", "                loss_mask_buffer = loss_mask_buffer[chunk_size:]\n\t    def get_state_dict(self):\n\t        return dict(\n\t            config=self.config,\n\t            index=self._index,\n\t            file_loc=self._file_loc,\n\t        )\n\t    def load_state_dict(self, state_dict):\n\t        self.config = state_dict.get('config', self.config)\n\t        self._index = state_dict.get('index', self.config.index_at_start)\n", "        self._file_loc = state_dict.get('file_loc', self.config.start_seek_loc)\n\t    @property\n\t    def seq_length(self):\n\t        return self.config.seq_length\n\t    @property\n\t    def tokenizer(self):\n\t        return self._tokenizer\n\t    @property\n\t    def text_processor(self):\n\t        return self._text_processor\n", "    @property\n\t    def vocab_size(self):\n\t        return len(self.tokenizer)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/tools/checkpoint.py", "chunked_list": ["import os\n\timport numpy as np\n\tfrom ml_collections import ConfigDict\n\timport bpt.tools.utils as utils\n\timport jax\n\timport jax.numpy as jnp\n\timport flax\n\tfrom flax.serialization import (\n\t    from_bytes, to_bytes, to_state_dict, from_state_dict\n\t)\n", "from flax.traverse_util import flatten_dict, unflatten_dict, empty_node\n\timport msgpack\n\tfrom bpt.tools.jax_utils import tree_apply, float_tensor_to_dtype\n\tclass StreamingCheckpointer(object):\n\t    \"\"\" Custom msgpack checkpointer that saves large train states by serializing\n\t        and saving tensors one by one in a streaming fashion. Avoids running\n\t        out of memory or local TPU disk with default flax checkpointer.\n\t    \"\"\"\n\t    @staticmethod\n\t    def get_default_config(updates=None):\n", "        config = ConfigDict()\n\t        config.float_dtype = 'bf16'\n\t        config.save_optimizer_state = False\n\t        if updates is not None:\n\t            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    def __init__(self, config, checkpoint_dir, enable=True):\n\t        self.config = self.get_default_config(config)\n\t        self.checkpoint_dir = checkpoint_dir\n\t        self.enable = enable\n", "    def save_checkpoint(self, train_state, filename, gather_fns=None):\n\t        if self.enable:\n\t            path = os.path.join(self.checkpoint_dir, filename)\n\t        else:\n\t            path = '/dev/null'\n\t        self.save_train_state_to_file(\n\t            train_state, path, gather_fns, self.config.float_dtype\n\t        )\n\t    @staticmethod\n\t    def save_train_state_to_file(train_state, path, gather_fns=None, float_dtype=None):\n", "        train_state = to_state_dict(train_state)\n\t        packer = msgpack.Packer()\n\t        flattend_train_state = flatten_dict(train_state)\n\t        if gather_fns is not None:\n\t            gather_fns = flatten_dict(to_state_dict(gather_fns))\n\t        with utils.open_file(path, \"wb\") as fout:\n\t            for key, value in flattend_train_state.items():\n\t                if gather_fns is not None:\n\t                    value = gather_fns[key](value)\n\t                value = float_tensor_to_dtype(value, float_dtype)\n", "                fout.write(packer.pack((key, to_bytes(value))))\n\t    def save_pickle(self, obj, filename):\n\t        if self.enable:\n\t            path = os.path.join(self.checkpoint_dir, filename)\n\t        else:\n\t            path = '/dev/null'\n\t        utils.save_pickle(obj, path)\n\t    def save_all(self, train_state, gather_fns, metadata=None, dataset=None, milestone=False):\n\t        step = int(jax.device_get(train_state.step))\n\t        if self.config.save_optimizer_state:\n", "            checkpoint_state = train_state\n\t            checkpoint_name = 'streaming_train_state'\n\t            checkpoint_gather_fns = gather_fns\n\t        else:\n\t            checkpoint_state = train_state.params['params']\n\t            checkpoint_name = 'streaming_params'\n\t            checkpoint_gather_fns = gather_fns.params['params']\n\t        if milestone:\n\t            # Save a milestone checkpoint that will not be overwritten\n\t            self.save_pickle(metadata, f'metadata_{step}.pkl')\n", "            self.save_pickle(dataset, f'dataset_{step}.pkl')\n\t            self.save_checkpoint(\n\t                checkpoint_state, f'{checkpoint_name}_{step}', checkpoint_gather_fns\n\t            )\n\t        else:\n\t            # Save a normal checkpoint that can be overwritten\n\t            self.save_pickle(metadata, 'metadata.pkl')\n\t            self.save_pickle(dataset, 'dataset.pkl')\n\t            self.save_checkpoint(\n\t                checkpoint_state, f'{checkpoint_name}', checkpoint_gather_fns\n", "            )\n\t    @staticmethod\n\t    def load_checkpoint(path, target=None, shard_fns=None, remove_dict_prefix=None):\n\t        if shard_fns is not None:\n\t            shard_fns = flatten_dict(\n\t                to_state_dict(shard_fns)\n\t            )\n\t        if remove_dict_prefix is not None:\n\t            remove_dict_prefix = tuple(remove_dict_prefix)\n\t        flattend_train_state = {}\n", "        with utils.open_file(path) as fin:\n\t            # 83886080 bytes = 80 MB, which is 16 blocks on GCS\n\t            unpacker = msgpack.Unpacker(fin, read_size=83886080, max_buffer_size=0)\n\t            for key, value in unpacker:\n\t                key = tuple(key)\n\t                if remove_dict_prefix is not None:\n\t                    if key[:len(remove_dict_prefix)] == remove_dict_prefix:\n\t                        key = key[len(remove_dict_prefix):]\n\t                    else:\n\t                        continue\n", "                tensor = from_bytes(None, value)\n\t                if shard_fns is not None:\n\t                    tensor = shard_fns[key](tensor)\n\t                flattend_train_state[key] = tensor\n\t        if target is not None:\n\t            flattened_target = flatten_dict(\n\t                to_state_dict(target), keep_empty_nodes=True\n\t            )\n\t            for key, value in flattened_target.items():\n\t                if key not in flattend_train_state and value == empty_node:\n", "                    flattend_train_state[key] = value\n\t        train_state = unflatten_dict(flattend_train_state)\n\t        if target is None:\n\t            return train_state\n\t        return from_state_dict(target, train_state)\n\t    @staticmethod\n\t    def load_flax_checkpoint(path, target=None, shard_fns=None):\n\t        \"\"\" Load a standard flax checkpoint that's not saved with the\n\t            msgpack streaming format.\n\t        \"\"\"\n", "        with utils.open_file(path, \"rb\") as fin:\n\t            encoded_bytes = fin.read()\n\t        state_dict = flax.serialization.msgpack_restore(encoded_bytes)\n\t        if shard_fns is not None:\n\t            shard_fns = to_state_dict(shard_fns)\n\t            state_dict = tree_apply(shard_fns, state_dict)\n\t        if target is None:\n\t            return state_dict\n\t        return from_state_dict(target, state_dict)\n\t    @classmethod\n", "    def load_trainstate_checkpoint(cls, load_from, trainstate_target=None,\n\t                                   trainstate_shard_fns=None,\n\t                                   disallow_trainstate=False):\n\t        if trainstate_target is not None:\n\t            params_target = trainstate_target.params['params']\n\t        else:\n\t            params_target = None\n\t        if trainstate_shard_fns is not None:\n\t            params_shard_fns = trainstate_shard_fns.params['params']\n\t        else:\n", "            params_shard_fns = None\n\t        load_type, load_path = load_from.split('::', 1)\n\t        if disallow_trainstate:\n\t            assert load_type != 'trainstate', 'Loading full trainstate is not allowed!'\n\t        train_state = None\n\t        restored_params = None\n\t        if load_type == 'trainstate':\n\t            # Load the entire train state in the streaming format\n\t            train_state = cls.load_checkpoint(\n\t                path=load_path,\n", "                target=trainstate_target,\n\t                shard_fns=trainstate_shard_fns,\n\t            )\n\t        elif load_type == 'trainstate_params':\n\t            # Load the params part of the train state in the streaming format\n\t            restored_params = cls.load_checkpoint(\n\t                path=load_path,\n\t                target=params_target,\n\t                shard_fns=params_shard_fns,\n\t                remove_dict_prefix=('params', 'params'),\n", "            )\n\t            restored_params = flax.core.frozen_dict.freeze(\n\t                {'params': restored_params}\n\t            )\n\t        elif load_type == 'params':\n\t            # Load the params in the streaming format\n\t            restored_params = cls.load_checkpoint(\n\t                path=load_path,\n\t                target=params_target,\n\t                shard_fns=params_shard_fns,\n", "            )\n\t            restored_params = flax.core.frozen_dict.freeze(\n\t                {'params': restored_params}\n\t            )\n\t        elif load_type == 'flax_params':\n\t            # Load the params in the standard flax format (non-streaming)\n\t            # This requires the entire params to fit in memory\n\t            restored_params = cls.load_flax_checkpoint(\n\t                path=load_path,\n\t                target=params_target,\n", "                shard_fns=params_shard_fns\n\t            )\n\t            restored_params = flax.core.frozen_dict.freeze(\n\t                {'params': restored_params}\n\t            )\n\t        else:\n\t            raise ValueError(f'Invalid load_from type: {load_type}')\n\t        return train_state, restored_params\n"]}
{"filename": "blockwise-parallel-transformer/bpt/tools/__init__.py", "chunked_list": []}
{"filename": "blockwise-parallel-transformer/bpt/tools/utils.py", "chunked_list": ["import inspect\n\timport logging\n\timport os\n\timport pprint\n\timport random\n\timport tempfile\n\timport time\n\timport uuid\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom copy import copy\n", "from io import BytesIO\n\tfrom socket import gethostname\n\timport dataclasses\n\timport absl.flags\n\timport absl.logging\n\timport cloudpickle as pickle\n\timport flax\n\timport gcsfs\n\timport jax\n\timport jax.numpy as jnp\n", "import msgpack\n\timport numpy as np\n\timport wandb\n\tfrom flax.serialization import from_bytes, to_bytes\n\tfrom ml_collections import ConfigDict\n\tfrom ml_collections.config_dict.config_dict import placeholder\n\tfrom ml_collections.config_flags import config_flags\n\tfrom flax.training.train_state import TrainState\n\tfrom flax.core import FrozenDict\n\tfrom absl.app import run\n", "class WandBLogger(object):\n\t    @staticmethod\n\t    def get_default_config(updates=None):\n\t        config = ConfigDict()\n\t        config.project_id = \"\"\n\t        config.project_entity = placeholder(str)\n\t        config.experiment_id = placeholder(str)\n\t        config.append_uuid = True\n\t        config.experiment_note = placeholder(str)\n\t        config.output_dir = \"/tmp/\"\n", "        config.wandb_dir = \"\"\n\t        config.profile_dir = \"\"\n\t        config.online = False\n\t        if updates is not None:\n\t            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    def __init__(self, config, variant, enable=True):\n\t        self.enable = enable\n\t        self.config = self.get_default_config(config)\n\t        if self.config.experiment_id is None or self.config.experiment_id == \"\":\n", "            self.config.experiment_id = uuid.uuid4().hex\n\t        else:\n\t            if self.config.append_uuid:\n\t                self.config.experiment_id = (\n\t                    str(self.config.experiment_id) + \"_\" + uuid.uuid4().hex\n\t                )\n\t            else:\n\t                self.config.experiment_id = str(self.config.experiment_id)\n\t        if self.enable:\n\t            if self.config.output_dir == \"\":\n", "                self.config.output_dir = tempfile.mkdtemp()\n\t            else:\n\t                self.config.output_dir = os.path.join(\n\t                    self.config.output_dir, self.config.experiment_id\n\t                )\n\t                if not self.config.output_dir.startswith(\"gs://\"):\n\t                    os.makedirs(self.config.output_dir, exist_ok=True)\n\t            if self.config.wandb_dir == \"\":\n\t                if not self.config.output_dir.startswith(\"gs://\"):\n\t                    # Use the same directory as output_dir if it is not a GCS path.\n", "                    self.config.wandb_dir = self.config.output_dir\n\t                else:\n\t                    # Otherwise, use a temporary directory.\n\t                    self.config.wandb_dir = tempfile.mkdtemp()\n\t            else:\n\t                # Join the wandb_dir with the experiment_id.\n\t                self.config.wandb_dir = os.path.join(\n\t                    self.config.wandb_dir, self.config.experiment_id\n\t                )\n\t                os.makedirs(self.config.wandb_dir, exist_ok=True)\n", "            if self.config.profile_dir == \"\":\n\t                if not self.config.output_dir.startswith(\"gs://\"):\n\t                    # Use the same directory as output_dir if it is not a GCS path.\n\t                    self.config.profile_dir = self.config.output_dir\n\t                else:\n\t                    # Otherwise, use a temporary directory.\n\t                    self.config.profile_dir = tempfile.mkdtemp()\n\t            else:\n\t                # Join the profile_dir with the experiment_id.\n\t                self.config.profile_dir = os.path.join(\n", "                    self.config.profile_dir, self.config.experiment_id\n\t                )\n\t                os.makedirs(self.config.profile_dir, exist_ok=True)\n\t        self._variant = flatten_config_dict(variant)\n\t        if \"hostname\" not in self._variant:\n\t            self._variant[\"hostname\"] = gethostname()\n\t        if self.enable:\n\t            self.run = wandb.init(\n\t                reinit=True,\n\t                config=self._variant,\n", "                project=self.config.project_id,\n\t                dir=self.config.wandb_dir,\n\t                id=self.config.experiment_id,\n\t                resume=\"allow\",\n\t                notes=self.config.experiment_note,\n\t                entity=self.config.project_entity,\n\t                settings=wandb.Settings(\n\t                    start_method=\"thread\",\n\t                    _disable_stats=True,\n\t                ),\n", "                mode=\"online\" if self.config.online else \"offline\",\n\t            )\n\t        else:\n\t            self.run = None\n\t    def log(self, *args, **kwargs):\n\t        if self.enable:\n\t            self.run.log(*args, **kwargs)\n\t    def save_pickle(self, obj, filename):\n\t        if self.enable:\n\t            save_pickle(obj, os.path.join(self.config.output_dir, filename))\n", "    @property\n\t    def experiment_id(self):\n\t        return self.config.experiment_id\n\t    @property\n\t    def variant(self):\n\t        return self.config.variant\n\t    @property\n\t    def output_dir(self):\n\t        return self.config.output_dir\n\t    @property\n", "    def wandb_dir(self):\n\t        return self.config.wandb_dir\n\t    @property\n\t    def profile_dir(self):\n\t        return self.config.profile_dir\n\tdef config_dict(*args, **kwargs):\n\t    return ConfigDict(dict(*args, **kwargs))\n\tdef define_flags_with_default(**kwargs):\n\t    for key, val in kwargs.items():\n\t        if isinstance(val, tuple):\n", "            val, help_str = val\n\t        else:\n\t            help_str = \"\"\n\t        if isinstance(val, ConfigDict):\n\t            config_flags.DEFINE_config_dict(key, val)\n\t        elif isinstance(val, bool):\n\t            # Note that True and False are instances of int.\n\t            absl.flags.DEFINE_bool(key, val, help_str)\n\t        elif isinstance(val, int):\n\t            absl.flags.DEFINE_integer(key, val, help_str)\n", "        elif isinstance(val, float):\n\t            absl.flags.DEFINE_float(key, val, help_str)\n\t        elif isinstance(val, str):\n\t            absl.flags.DEFINE_string(key, val, help_str)\n\t        else:\n\t            raise ValueError(\"Incorrect value type\")\n\t    return absl.flags.FLAGS, kwargs\n\tdef print_flags(flags, flags_def):\n\t    flag_srings = [\n\t        \"{}: {}\".format(key, val)\n", "        for key, val in get_user_flags(flags, flags_def).items()\n\t    ]\n\t    logging.info(\n\t        \"Hyperparameter configs: \\n{}\".format(\n\t            pprint.pformat(flag_srings)\n\t        )\n\t    )\n\tdef get_user_flags(flags, flags_def):\n\t    output = {}\n\t    for key in flags_def:\n", "        val = getattr(flags, key)\n\t        if isinstance(val, ConfigDict):\n\t            output.update(flatten_config_dict(val, prefix=key))\n\t        else:\n\t            output[key] = val\n\t    return output\n\tdef user_flags_to_config_dict(flags, flags_def):\n\t    output = ConfigDict()\n\t    for key in flags_def:\n\t        output[key] = getattr(flags, key)\n", "    return output\n\tdef flatten_config_dict(config, prefix=None):\n\t    output = {}\n\t    for key, val in config.items():\n\t        if isinstance(val, ConfigDict) or isinstance(val, dict):\n\t            output.update(flatten_config_dict(val, prefix=key))\n\t        else:\n\t            if prefix is not None:\n\t                output[\"{}.{}\".format(prefix, key)] = val\n\t            else:\n", "                output[key] = val\n\t    return output\n\tdef function_args_to_config(fn, none_arg_types=None, exclude_args=None, override_args=None):\n\t    config = ConfigDict()\n\t    arg_spec = inspect.getargspec(fn)\n\t    n_args = len(arg_spec.defaults)\n\t    arg_names = arg_spec.args[-n_args:]\n\t    default_values = arg_spec.defaults\n\t    for name, value in zip(arg_names, default_values):\n\t        if exclude_args is not None and name in exclude_args:\n", "            continue\n\t        elif override_args is not None and name in override_args:\n\t            config[name] = override_args[name]\n\t        elif none_arg_types is not None and value is None and name in none_arg_types:\n\t            config[name] = placeholder(none_arg_types[name])\n\t        else:\n\t            config[name] = value\n\t    return config\n\tdef prefix_metrics(metrics, prefix):\n\t    return {\"{}/{}\".format(prefix, key): value for key, value in metrics.items()}\n", "def open_file(path, mode='rb', cache_type='readahead'):\n\t    if path.startswith(\"gs://\"):\n\t        logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n\t        return gcsfs.GCSFileSystem().open(path, mode, cache_type=cache_type)\n\t    else:\n\t        return open(path, mode)\n\tdef save_pickle(obj, path):\n\t    with open_file(path, \"wb\") as fout:\n\t        pickle.dump(obj, fout)\n\tdef load_pickle(path):\n", "    with open_file(path, \"rb\") as fin:\n\t        data = pickle.load(fin)\n\t    return data\n\tdef text_to_array(text, encoding=\"utf-8\"):\n\t    return np.frombuffer(text.encode(encoding), dtype=\"uint8\")\n\tdef array_to_text(array, encoding=\"utf-8\"):\n\t    with BytesIO(array) as fin:\n\t        text = fin.read().decode(encoding)\n\t    return text\n\tclass JaxRNG(object):\n", "    \"\"\" A convenient stateful Jax RNG wrapper. Can be used to wrap RNG inside\n\t        pure function.\n\t    \"\"\"\n\t    @classmethod\n\t    def from_seed(cls, seed):\n\t        return cls(jax.random.PRNGKey(seed))\n\t    def __init__(self, rng):\n\t        self.rng = rng\n\t    def __call__(self, keys=None):\n\t        if keys is None:\n", "            self.rng, split_rng = jax.random.split(self.rng)\n\t            return split_rng\n\t        elif isinstance(keys, int):\n\t            split_rngs = jax.random.split(self.rng, num=keys + 1)\n\t            self.rng = split_rngs[0]\n\t            return tuple(split_rngs[1:])\n\t        else:\n\t            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n\t            self.rng = split_rngs[0]\n\t            return {key: val for key, val in zip(keys, split_rngs[1:])}\n", "def wrap_function_with_rng(rng):\n\t    \"\"\" To be used as decorator, automatically bookkeep a RNG for the wrapped function. \"\"\"\n\t    def wrap_function(function):\n\t        def wrapped(*args, **kwargs):\n\t            nonlocal rng\n\t            rng, split_rng = jax.random.split(rng)\n\t            return function(split_rng, *args, **kwargs)\n\t        return wrapped\n\t    return wrap_function\n\tdef init_rng(seed):\n", "    global jax_utils_rng\n\t    jax_utils_rng = JaxRNG.from_seed(seed)\n\tdef next_rng(*args, **kwargs):\n\t    global jax_utils_rng\n\t    return jax_utils_rng(*args, **kwargs)\n\tdef flatten_tree(xs, is_leaf=None, sep=None):\n\t    \"\"\" A stronger version of flax.traverse_util.flatten_dict, supports\n\t        dict, tuple, list and TrainState. Tuple and list indices will be\n\t        converted to strings.\n\t    \"\"\"\n", "    tree_node_classes = (FrozenDict, dict, tuple, list, TrainState)\n\t    if not isinstance(xs, tree_node_classes):\n\t        ValueError('fUnsupported node type: {type(xs)}')\n\t    def _is_leaf(prefix, fx):\n\t        if is_leaf is not None:\n\t            return is_leaf(prefix, xs)\n\t        return False\n\t    def _key(path):\n\t        if sep is None:\n\t            return path\n", "        return sep.join(path)\n\t    def _convert_to_dict(xs):\n\t        if isinstance(xs, (FrozenDict, dict)):\n\t            return xs\n\t        elif isinstance(xs, (tuple, list)):\n\t            return {f'{i}': v for i, v in enumerate(xs)}\n\t        elif isinstance(xs, TrainState):\n\t            output = {}\n\t            for field in dataclasses.fields(xs):\n\t                if 'pytree_node' not in field.metadata or field.metadata['pytree_node']:\n", "                    output[field.name] = getattr(xs, field.name)\n\t            return output\n\t        else:\n\t            raise ValueError('fUnsupported node type: {type(xs)}')\n\t    def _flatten(xs, prefix):\n\t        if not isinstance(xs, tree_node_classes) or _is_leaf(prefix, xs):\n\t            return {_key(prefix): xs}\n\t        result = {}\n\t        is_empty = True\n\t        for (key, value) in _convert_to_dict(xs).items():\n", "            is_empty = False\n\t            path = prefix + (key, )\n\t            result.update(_flatten(value, path))\n\t        return result\n\t    return _flatten(xs, ())\n\tdef named_tree_map(f, tree, is_leaf=None, sep=None):\n\t    \"\"\" An extended version of jax.tree_util.tree_map, where the mapped function\n\t        f takes both the name (path) and the tree leaf as input.\n\t    \"\"\"\n\t    flattened_tree = flatten_tree(tree, is_leaf=is_leaf, sep=sep)\n", "    id_to_name = {id(val): key for key, val in flattened_tree.items()}\n\t    def map_fn(leaf):\n\t        name = id_to_name[id(leaf)]\n\t        return f(name, leaf)\n\t    return jax.tree_util.tree_map(map_fn, tree)\n\tdef get_pytree_shape_info(tree):\n\t    flattend_tree = flatten_tree(tree, sep='/')\n\t    shapes = []\n\t    for key in sorted(list(flattend_tree.keys())):\n\t        val = flattend_tree[key]\n", "        shapes.append(f'{key}: {val.dtype}, {val.shape}')\n\t    return '\\n'.join(shapes)\n\tdef collect_metrics(metrics, names, prefix=None):\n\t    collected = {}\n\t    for name in names:\n\t        if name in metrics:\n\t            collected[name] = jnp.mean(metrics[name])\n\t    if prefix is not None:\n\t        collected = {\n\t            '{}/{}'.format(prefix, key): value for key, value in collected.items()\n", "        }\n\t    return collected\n\tdef set_random_seed(seed):\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    init_rng(seed)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/tools/jax_utils.py", "chunked_list": ["import os\n\timport math\n\tfrom typing import Any, Mapping, Text, Tuple, Union, NamedTuple\n\tfrom functools import partial\n\timport re\n\timport dataclasses\n\timport random\n\timport dill\n\timport flax\n\timport jax\n", "import jax.numpy as jnp\n\tfrom jax.sharding import PartitionSpec as PS\n\tfrom jax.sharding import Mesh\n\tfrom jax.experimental.pjit import with_sharding_constraint as _with_sharding_constraint\n\tfrom jax.experimental.pjit import pjit\n\tfrom jax.interpreters import pxla\n\timport numpy as np\n\tfrom absl import logging\n\tfrom flax import jax_utils\n\tfrom flax.training.train_state import TrainState\n", "from flax.core import FrozenDict\n\timport optax\n\tfrom transformers import FlaxLogitsWarper\n\tclass JaxRNG(object):\n\t    \"\"\" A convenient stateful Jax RNG wrapper. Can be used to wrap RNG inside\n\t        pure function.\n\t    \"\"\"\n\t    @classmethod\n\t    def from_seed(cls, seed):\n\t        return cls(jax.random.PRNGKey(seed))\n", "    def __init__(self, rng):\n\t        self.rng = rng\n\t    def __call__(self, keys=None):\n\t        if keys is None:\n\t            self.rng, split_rng = jax.random.split(self.rng)\n\t            return split_rng\n\t        elif isinstance(keys, int):\n\t            split_rngs = jax.random.split(self.rng, num=keys + 1)\n\t            self.rng = split_rngs[0]\n\t            return tuple(split_rngs[1:])\n", "        else:\n\t            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n\t            self.rng = split_rngs[0]\n\t            return {key: val for key, val in zip(keys, split_rngs[1:])}\n\tclass FlaxTemperatureLogitsWarper(FlaxLogitsWarper):\n\t    \"\"\" JIT traceable version of FlaxLogitsWarper that performs temperature scaling.\"\"\"\n\t    def __init__(self, temperature):\n\t        self.temperature = temperature\n\t    def __call__(self, input_ids, scores, cur_len):\n\t        return scores / jnp.clip(self.temperature, a_min=1e-8)\n", "def make_shard_and_gather_fns(partition_specs, dtype_specs=None):\n\t    \"\"\" Create pytree of sharding and gathering functions from pytree of\n\t        partition specs.\n\t    \"\"\"\n\t    float_dtypes = (jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64)\n\t    def make_to_dtype_fn(dtype_spec):\n\t        def to_dtype(tensor):\n\t            if dtype_specs in float_dtypes and getattr(tensor, 'dtype', None) in float_dtypes:\n\t                # Convert all float tensors to the same dtype\n\t                return tensor.astype(dtype_specs)\n", "            elif hasattr(dtype_spec, 'dtype') and hasattr(tensor, 'dtype'):\n\t                return tensor.astype(dtype_spec.dtype)\n\t            return tensor\n\t        return to_dtype\n\t    def make_shard_fn(partition_spec, dtype_spec=None):\n\t        jax_shard_function = pjit(\n\t            make_to_dtype_fn(dtype_spec),\n\t            in_shardings=None,\n\t            out_shardings=partition_spec\n\t        )\n", "        def shard_fn(tensor):\n\t            return jax_shard_function(tensor).block_until_ready()\n\t        return shard_fn\n\t    def make_gather_fn(partition_spec, dtype_spec=None):\n\t        jax_gather_fn = pjit(\n\t            make_to_dtype_fn(dtype_spec),\n\t            in_shardings=partition_spec,\n\t            out_shardings=None\n\t        )\n\t        def gather_fn(tensor):\n", "            return jax.device_get(jax_gather_fn(tensor))\n\t        return gather_fn\n\t    if dtype_specs is None or dtype_specs in float_dtypes:\n\t        shard_fns = jax.tree_util.tree_map(make_shard_fn, partition_specs)\n\t        gather_fns = jax.tree_util.tree_map(make_gather_fn, partition_specs)\n\t    else:\n\t        shard_fns = jax.tree_util.tree_map(\n\t            make_shard_fn, partition_specs, dtype_specs\n\t        )\n\t        gather_fns = jax.tree_util.tree_map(\n", "            make_gather_fn, partition_specs, dtype_specs\n\t        )\n\t    return shard_fns, gather_fns\n\tdef set_random_seed(seed):\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    init_rng(seed)\n\tdef get_jax_mesh(axis_dims, names):\n\t    if ':' in axis_dims:\n\t        dims = []\n", "        dim_names = []\n\t        for axis in axis_dims.split(','):\n\t            name, dim = axis.split(':')\n\t            assert name in names\n\t            dims.append(int(dim))\n\t            dim_names.append(name)\n\t        assert(set(dim_names) == set(names))\n\t    else:\n\t        dims = [int(x) for x in axis_dims.split(',')]\n\t        dim_names = names\n", "    assert len(dims) == len(names)\n\t    return Mesh(np.array(jax.devices()).reshape(dims), dim_names)\n\tdef names_in_current_mesh(*names):\n\t    \"\"\" Check if current mesh axes contain these names. \"\"\"\n\t    mesh_axis_names = pxla.thread_resources.env.physical_mesh.axis_names\n\t    return set(names) <= set(mesh_axis_names)\n\tdef get_names_from_parition_spec(partition_specs):\n\t    \"\"\" Return axis names from partition specs. \"\"\"\n\t    names = set()\n\t    if isinstance(partition_specs, dict):\n", "        partition_specs = partition_specs.values()\n\t    for item in partition_specs:\n\t        if item is None:\n\t            continue\n\t        elif isinstance(item, str):\n\t            names.add(item)\n\t        else:\n\t            names.update(get_names_from_parition_spec(item))\n\t    return list(names)\n\tdef with_sharding_constraint(x, partition_specs):\n", "    \"\"\" A smarter version of with_sharding_constraint that only applies the\n\t        constraint if the current mesh contains the axes in the partition specs.\n\t    \"\"\"\n\t    axis_names = get_names_from_parition_spec(partition_specs)\n\t    if names_in_current_mesh(*axis_names):\n\t        x = _with_sharding_constraint(x, partition_specs)\n\t    return x\n\tdef wrap_function_with_rng(rng):\n\t    \"\"\" To be used as decorator, automatically bookkeep a RNG for the wrapped function. \"\"\"\n\t    def wrap_function(function):\n", "        def wrapped(*args, **kwargs):\n\t            nonlocal rng\n\t            rng, split_rng = jax.random.split(rng)\n\t            return function(split_rng, *args, **kwargs)\n\t        return wrapped\n\t    return wrap_function\n\tdef init_rng(seed):\n\t    global jax_utils_rng\n\t    jax_utils_rng = JaxRNG.from_seed(seed)\n\tdef next_rng(*args, **kwargs):\n", "    global jax_utils_rng\n\t    return jax_utils_rng(*args, **kwargs)\n\tdef get_metrics(metrics, unreplicate=False, stack=False):\n\t    if unreplicate:\n\t        metrics = flax.jax_utils.unreplicate(metrics)\n\t    metrics = jax.device_get(metrics)\n\t    if stack:\n\t        return jax.tree_map(lambda *args: np.stack(args), *metrics)\n\t    else:\n\t        return {key: float(val) for key, val in metrics.items()}\n", "def mse_loss(val, target, valid=None):\n\t    if valid is None:\n\t        valid = jnp.ones((*target.shape[:2], 1))\n\t    valid = valid.astype(jnp.float32)\n\t    loss = jnp.mean(\n\t        jnp.where(\n\t            valid > 0.0,\n\t            jnp.square(val - target),\n\t            0.0\n\t        )\n", "    )\n\t    return loss\n\tdef cross_entropy_loss(logits, labels, smoothing_factor=0.):\n\t    num_classes = logits.shape[-1]\n\t    if labels.dtype == jnp.int32 or labels.dtype == jnp.int64:\n\t        labels = jax.nn.one_hot(labels, num_classes)\n\t    if smoothing_factor > 0.:\n\t        labels = labels * (1. - smoothing_factor) + smoothing_factor / num_classes\n\t    logp = jax.nn.log_softmax(logits, axis=-1)\n\t    return -jnp.mean(jnp.sum(logp * labels, axis=-1))\n", "def cross_entropy_loss_and_accuracy(logits, tokens, valid=None):\n\t    if valid is None:\n\t        valid = jnp.ones(tokens.shape[:2])\n\t    valid = valid.astype(jnp.float32)\n\t    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n\t    token_log_prob = jnp.squeeze(\n\t        jnp.take_along_axis(\n\t            jax.nn.log_softmax(logits, axis=-1),\n\t            jnp.expand_dims(tokens, -1),\n\t            axis=-1,\n", "        ),\n\t        -1,\n\t    )\n\t    token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n\t    loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n\t    correct = jnp.where(\n\t        valid > 0.0,\n\t        jnp.argmax(logits, axis=-1) == tokens,\n\t        jnp.array(False)\n\t    )\n", "    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n\t    return loss, accuracy\n\tdef global_norm(tree):\n\t    \"\"\" Return the global L2 norm of a pytree. \"\"\"\n\t    squared = jax.tree_util.tree_map(lambda x: jnp.sum(jnp.square(x)), tree)\n\t    flattened, _ = jax.flatten_util.ravel_pytree(squared)\n\t    return jnp.sqrt(jnp.sum(flattened))\n\tdef average_metrics(metrics):\n\t    return jax.tree_map(\n\t        lambda *args: jnp.mean(jnp.stack(args)),\n", "        *metrics\n\t    )\n\tdef get_float_dtype_by_name(dtype):\n\t    return {\n\t        'bf16': jnp.bfloat16,\n\t        'fp16': jnp.float16,\n\t        'fp32': jnp.float32,\n\t        'fp64': jnp.float64,\n\t    }[dtype]\n\tdef float_tensor_to_dtype(tensor, dtype):\n", "    if dtype is None or dtype == '':\n\t        return tensor\n\t    if isinstance(dtype, str):\n\t        dtype = get_float_dtype_by_name(dtype)\n\t    float_dtypes = (jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64)\n\t    if getattr(tensor, 'dtype', None) in float_dtypes:\n\t        tensor = tensor.astype(dtype)\n\t    return tensor\n\tdef float_to_dtype(tree, dtype):\n\t    return jax.tree_util.tree_map(\n", "        partial(float_tensor_to_dtype, dtype=dtype), tree\n\t    )\n\tdef get_gradient_checkpoint_policy(name):\n\t    return {\n\t        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n\t        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n\t        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n\t        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n\t    }[name]\n\tdef tree_path_to_string(path, sep=None):\n", "    keys = []\n\t    for key in path:\n\t        if isinstance(key, jax.tree_util.SequenceKey):\n\t            keys.append(str(key.idx))\n\t        elif isinstance(key, jax.tree_util.DictKey):\n\t            keys.append(str(key.key))\n\t        elif isinstance(key, jax.tree_util.GetAttrKey):\n\t            keys.append(str(key.name))\n\t        elif isinstance(key, jax.tree_util.FlattenedIndexKey):\n\t            keys.append(str(key.key))\n", "        else:\n\t            keys.append(str(key))\n\t    if sep is None:\n\t        return tuple(keys)\n\t    return sep.join(keys)\n\tdef flatten_tree(xs, is_leaf=None, sep=None):\n\t    flattened, _ = jax.tree_util.tree_flatten_with_path(xs, is_leaf=is_leaf)\n\t    output = {}\n\t    for key, val in flattened:\n\t        output[tree_path_to_string(key, sep=sep)] = val\n", "    return output\n\tdef named_tree_map(f, tree, *rest, is_leaf=None, sep=None):\n\t    \"\"\" An extended version of jax.tree_util.tree_map, where the mapped function\n\t        f takes both the name (path) and the tree leaf as input.\n\t    \"\"\"\n\t    return jax.tree_util.tree_map_with_path(\n\t        lambda path, x, *r: f(tree_path_to_string(path, sep=sep), x, *r),\n\t        tree, *rest,\n\t        is_leaf=is_leaf\n\t    )\n", "def match_partition_rules(rules, params):\n\t    \"\"\" Returns a pytree of PartitionSpec according to rules. Supports handling\n\t        Flax TrainState and Optax optimizer state.\n\t    \"\"\"\n\t    def get_partition_spec(name, leaf):\n\t        if len(leaf.shape) == 0 or np.prod(leaf.shape) == 1:\n\t            \"\"\" Don't partition scalar values. \"\"\"\n\t            return PS()\n\t        for rule, ps in rules:\n\t            if re.search(rule, name) is not None:\n", "                return ps\n\t        raise ValueError(f'Partition rule not found for param: {name}')\n\t    return named_tree_map(get_partition_spec, params, sep='/')\n\tdef get_weight_decay_mask(exclusions):\n\t    \"\"\" Return a weight decay mask function that computes the pytree masks\n\t        according to the given exclusion rules.\n\t    \"\"\"\n\t    def decay(name, _):\n\t        for rule in exclusions:\n\t            if re.search(rule, name) is not None:\n", "                return False\n\t        return True\n\t    def weight_decay_mask(params):\n\t        return named_tree_map(decay, params, sep='/')\n\t    return weight_decay_mask\n\tdef tree_apply(fns, tree):\n\t    \"\"\" Apply a pytree of functions to the pytree. \"\"\"\n\t    return jax.tree_util.tree_map(lambda fn, x: fn(x), fns, tree)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/tools/optimizers.py", "chunked_list": ["import os\n\timport time\n\tfrom typing import Any, Mapping, Text, Tuple, Union, NamedTuple\n\tfrom functools import partial\n\timport re\n\timport dataclasses\n\timport random\n\tfrom ml_collections.config_dict import config_dict\n\tfrom ml_collections import ConfigDict\n\timport jax\n", "import jax.numpy as jnp\n\timport numpy as np\n\tfrom absl import logging\n\timport optax\n\tfrom bpt.tools.jax_utils import float_to_dtype\n\tclass OptimizerFactory(object):\n\t    \"\"\" Configurable optax optimizer factory. \"\"\"\n\t    def __init__(self):\n\t        raise NotImplementedError\n\t    @staticmethod\n", "    def get_default_config(updates=None):\n\t        config = ConfigDict()\n\t        config.accumulate_gradient_steps = 1\n\t        config.type = 'adamw'\n\t        config.palm_optimizer = PalmOptimizerFactory.get_default_config()\n\t        config.adamw_optimizer = AdamWOptimizerFactory.get_default_config()\n\t        if updates is not None:\n\t            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    @classmethod\n", "    def get_optimizer(cls, config, weight_decay_mask=None):\n\t        config = cls.get_default_config(config)\n\t        if config.type == 'palm':\n\t            optimizer, optimizer_info = PalmOptimizerFactory.get_optimizer(\n\t                config.palm_optimizer, weight_decay_mask\n\t            )\n\t        elif config.type == 'adamw':\n\t            optimizer, optimizer_info = AdamWOptimizerFactory.get_optimizer(\n\t                config.adamw_optimizer, weight_decay_mask\n\t            )\n", "        else:\n\t            raise ValueError(f'Unknown optimizer type: {config.type}')\n\t        if config.accumulate_gradient_steps > 1:\n\t            optimizer = optax.MultiSteps(\n\t                optimizer, config.accumulate_gradient_steps\n\t            )\n\t        return optimizer, optimizer_info\n\tclass PalmOptimizerFactory(object):\n\t    \"\"\" PaLM optimizer factory. This optimizer implements the optimizer\n\t        described in the PaLM paper: https://arxiv.org/abs/2204.02311\n", "    \"\"\"\n\t    def __init__(self):\n\t        raise NotImplementedError\n\t    @staticmethod\n\t    def get_default_config(updates=None):\n\t        config = ConfigDict()\n\t        config.lr = 0.01\n\t        config.lr_warmup_steps = 10000\n\t        config.b1 = 0.9\n\t        config.b2 = 0.99\n", "        config.clip_gradient = 1.0\n\t        config.weight_decay = 1e-4\n\t        config.bf16_momentum = True\n\t        if updates is not None:\n\t            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    @classmethod\n\t    def get_optimizer(cls, config, weight_decay_mask=None):\n\t        config = cls.get_default_config(config)\n\t        def learning_rate_schedule(step):\n", "            multiplier = config.lr / 0.01\n\t            return multiplier / jnp.sqrt(jnp.maximum(step, config.lr_warmup_steps))\n\t        def weight_decay_schedule(step):\n\t            multiplier = config.weight_decay / 1e-4\n\t            return -multiplier * jnp.square(learning_rate_schedule(step))\n\t        optimizer_info = dict(\n\t            learning_rate_schedule=learning_rate_schedule,\n\t            weight_decay_schedule=weight_decay_schedule,\n\t        )\n\t        optimizer = optax.chain(\n", "            optax.clip_by_global_norm(config.clip_gradient),\n\t            optax.adafactor(\n\t                learning_rate=learning_rate_schedule,\n\t                multiply_by_parameter_scale=True,\n\t                momentum=config.b1,\n\t                decay_rate=config.b2,\n\t                factored=False,\n\t                clipping_threshold=None,\n\t                dtype_momentum=jnp.bfloat16 if config.bf16_momentum else jnp.float32,\n\t            ),\n", "            optax_add_scheduled_weight_decay(\n\t                weight_decay_schedule, weight_decay_mask\n\t            )\n\t        )\n\t        return optimizer, optimizer_info\n\tclass AdamWOptimizerFactory(object):\n\t    \"\"\" AdamW optimizer with cosine schedule. \"\"\"\n\t    def __init__(self):\n\t        raise NotImplementedError\n\t    @staticmethod\n", "    def get_default_config(updates=None):\n\t        config = ConfigDict()\n\t        config.init_lr = 0.0\n\t        config.end_lr = 0.001\n\t        config.lr = 0.01\n\t        config.lr_warmup_steps = 2000\n\t        config.lr_decay_steps = 500000\n\t        config.b1 = 0.9\n\t        config.b2 = 0.95\n\t        config.clip_gradient = 1.0\n", "        config.weight_decay = 1e-4\n\t        config.bf16_momentum = True\n\t        config.multiply_by_parameter_scale = True\n\t        if updates is not None:\n\t            config.update(ConfigDict(updates).copy_and_resolve_references())\n\t        return config\n\t    @classmethod\n\t    def get_optimizer(cls, config, weight_decay_mask=None):\n\t        config = cls.get_default_config(config)\n\t        learning_rate_schedule = optax.warmup_cosine_decay_schedule(\n", "            init_value=config.init_lr,\n\t            peak_value=config.lr,\n\t            warmup_steps=config.lr_warmup_steps,\n\t            decay_steps=config.lr_decay_steps,\n\t            end_value=config.end_lr,\n\t        )\n\t        optimizer_info = dict(\n\t            learning_rate_schedule=learning_rate_schedule,\n\t        )\n\t        if config.multiply_by_parameter_scale:\n", "            optimizer = optax.chain(\n\t                optax.clip_by_global_norm(config.clip_gradient),\n\t                optax.adafactor(\n\t                    learning_rate=learning_rate_schedule,\n\t                    multiply_by_parameter_scale=True,\n\t                    momentum=config.b1,\n\t                    decay_rate=config.b2,\n\t                    factored=False,\n\t                    clipping_threshold=None,\n\t                    dtype_momentum=jnp.bfloat16 if config.bf16_momentum else jnp.float32,\n", "                ),\n\t                optax_add_scheduled_weight_decay(\n\t                    lambda step: -learning_rate_schedule(step) * config.weight_decay,\n\t                    weight_decay_mask\n\t                )\n\t            )\n\t        else:\n\t            optimizer = optax.chain(\n\t                optax.clip_by_global_norm(config.clip_gradient),\n\t                optax.adamw(\n", "                    learning_rate=learning_rate_schedule,\n\t                    weight_decay=config.weight_decay,\n\t                    b1=0.9,\n\t                    b2=0.95,\n\t                    mask=weight_decay_mask,\n\t                    mu_dtype=jnp.bfloat16 if config.bf16_momentum else jnp.float32,\n\t                ),\n\t            )\n\t        return optimizer, optimizer_info\n\tclass OptaxScheduledWeightDecayState(NamedTuple):\n", "    count: jnp.DeviceArray\n\tdef optax_add_scheduled_weight_decay(schedule_fn, mask=None):\n\t    \"\"\" Apply weight decay with schedule. \"\"\"\n\t    def init_fn(params):\n\t        del params\n\t        return OptaxScheduledWeightDecayState(count=jnp.zeros([], jnp.int32))\n\t    def update_fn(updates, state, params):\n\t        if params is None:\n\t            raise ValueError('Params cannot be None for weight decay!')\n\t        weight_decay = schedule_fn(state.count)\n", "        updates = jax.tree_util.tree_map(\n\t            lambda g, p: g + weight_decay * p, updates, params\n\t        )\n\t        return updates, OptaxScheduledWeightDecayState(\n\t            count=optax.safe_int32_increment(state.count)\n\t        )\n\t    if mask is not None:\n\t        return optax.masked(optax.GradientTransformation(init_fn, update_fn), mask)\n\t    return optax.GradientTransformation(init_fn, update_fn)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/vanilla.py", "chunked_list": ["import functools\n\timport json\n\timport math\n\tfrom functools import partial\n\tfrom typing import Optional, Tuple\n\timport flax.linen as nn\n\timport jax\n\timport jax.numpy as jnp\n\timport numpy as np\n\tfrom einops import rearrange\n", "from flax.linen import combine_masks, make_causal_mask\n\tfrom jax import lax\n\tfrom jax import numpy as jnp\n\tdef quick_gelu(x):\n\t    return x * jax.nn.sigmoid(1.702 * x)\n\tACT2FN = {\n\t    \"gelu\": partial(nn.gelu, approximate=False),\n\t    \"relu\": nn.relu,\n\t    \"silu\": nn.swish,\n\t    \"swish\": nn.swish,\n", "    \"gelu_new\": partial(nn.gelu, approximate=True),\n\t    \"quick_gelu\": quick_gelu,\n\t}\n\tMASK_VALUE = -1e10\n\tQ_CHUNK_SIZE = 1024\n\tK_CHUNK_SIZE = 1024\n\tdef create_sinusoidal_positions(num_pos, dim):\n\t    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n\t    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n\t    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n", "    sentinel = dim // 2 + dim % 2\n\t    out = np.zeros((num_pos, dim))\n\t    out[:, 0:sentinel] = sin\n\t    out[:, sentinel:] = cos\n\t    return jnp.array(out)\n\tdef rotate_every_two(tensor):\n\t    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n\t    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n\t    return rotate_half_tensor\n\tdef apply_rotary_pos_emb(tensor, sincos):\n", "    sin_pos, cos_pos = sincos\n\t    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n\t    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n\t    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\tclass _AttentionBlock(nn.Module):\n\t    hidden_size: int\n\t    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n", "    activation_function: str = \"gelu\"\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n\t    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n\t    float32_logits: bool = False\n\t    def setup(self):\n\t        self.embed_dim = self.hidden_size\n\t        self.head_dim = self.embed_dim // self.num_heads\n\t        dense = partial(\n", "            nn.Dense,\n\t            self.embed_dim,\n\t            use_bias=False,\n\t            dtype=self.dtype,\n\t            kernel_init=jax.nn.initializers.variance_scaling(\n\t                scale=1.0, mode='fan_in',\n\t                distribution='normal',\n\t            )\n\t        )\n\t        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n", "        self.out_proj = dense()\n\t        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\t        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\t        self.fc_in = nn.Dense(self.intermediate_size,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n\t                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n", "        self.fc_out = nn.Dense(self.embed_dim,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n\t                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n\t        self.act = ACT2FN[self.activation_function]\n\t        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n", "            pos_embd_dim = self.rotary_dim\n\t        else:\n\t            pos_embd_dim = self.embed_dim // self.num_heads\n\t        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\t    def _split_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\t    def _merge_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\t    def attn_out_proj(self, attn_output, deterministic):\n\t        attn_output = self._merge_heads(attn_output)\n", "        attn_output = self.out_proj(attn_output)\n\t        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n\t        return attn_output\n\t    def forward_qkv(\n\t        self,\n\t        hidden_states,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_1(hidden_states)\n", "        query = self.q_proj(hidden_states)\n\t        key = self.k_proj(hidden_states)\n\t        value = self.v_proj(hidden_states)\n\t        query = self._split_heads(query)\n\t        key = self._split_heads(key)\n\t        value = self._split_heads(value)\n\t        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n\t        sincos = jnp.split(sincos, 2, axis=-1)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            k_rot = key[:, :, :, : self.rotary_dim]\n", "            k_pass = key[:, :, :, self.rotary_dim :]\n\t            q_rot = query[:, :, :, : self.rotary_dim]\n\t            q_pass = query[:, :, :, self.rotary_dim :]\n\t            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n\t            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\t            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n\t            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n\t        else:\n\t            key = apply_rotary_pos_emb(key, sincos)\n\t            query = apply_rotary_pos_emb(query, sincos)\n", "        if self.float32_logits:\n\t            query = query.astype(jnp.float32)\n\t            key = key.astype(jnp.float32)\n\t        return query, key, value\n\t    def forward_ffn(\n\t        self,\n\t        hidden_states,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_2(hidden_states)\n", "        hidden_states = self.fc_in(hidden_states)\n\t        hidden_states = self.act(hidden_states)\n\t        hidden_states = self.fc_out(hidden_states)\n\t        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\t        return hidden_states\n\tclass AttentionBlock(nn.Module):\n\t    q_chunk_size: int # not used\n\t    k_chunk_size: int # not used\n\t    hidden_size: int\n\t    num_heads: int\n", "    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n\t    activation_function: str = \"gelu\"\n\t    attn_pdrop: float = 0.0\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n\t    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n\t    policy: str = None # not used\n", "    prevent_cse: bool = False # not used\n\t    float32_logits: bool = False\n\t    def setup(self):\n\t        self.attn = _AttentionBlock(\n\t            self.hidden_size,\n\t            self.num_heads,\n\t            self.rotary_dim,\n\t            self.intermediate_size,\n\t            self.layer_norm_epsilon,\n\t            self.activation_function,\n", "            self.resid_pdrop,\n\t            self.max_position_embeddings,\n\t            self.dtype,\n\t            self.causal,\n\t            self.float32_logits,\n\t        )\n\t        self.causal_mask = make_causal_mask(jnp.ones((1, self.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\")\n\t    @nn.compact\n\t    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\t        \"\"\"\n", "        This function takes projected key, value states from a single input token and concatenates the states to cached\n\t        states from previous steps. This function is slighly adapted from the official Flax repository:\n\t        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n\t        \"\"\"\n\t        # detect if we're initializing by absence of existing cache data.\n\t        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n\t        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n\t        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n\t        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\t        if is_initialized:\n", "            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n\t            # update key, value caches with our new 1d spatial slices\n\t            cur_index = cache_index.value\n\t            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n\t            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n\t            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n\t            cached_key.value = key\n\t            cached_value.value = value\n\t            num_updated_cache_vectors = query.shape[1]\n\t            cache_index.value = cache_index.value + num_updated_cache_vectors\n", "            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n\t            pad_mask = jnp.broadcast_to(\n\t                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n\t                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n\t            )\n\t            attention_mask = combine_masks(pad_mask, attention_mask)\n\t        return key, value, attention_mask\n\t    def __call__(\n\t        self,\n\t        hidden_states,\n", "        attention_mask,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t        init_cache: bool = False,\n\t    ):\n\t        query, key, value = self.attn.forward_qkv(\n\t            hidden_states,\n\t            position_ids,\n\t            deterministic=deterministic,\n\t        )\n", "        query_length, key_length = query.shape[1], key.shape[1]\n\t        if self.has_variable(\"cache\", \"cached_key\"):\n\t            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n\t            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n\t            causal_mask = lax.dynamic_slice(\n\t                self.causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n\t            )\n\t        else:\n\t            causal_mask = self.causal_mask[:, :, :query_length, :key_length]\n\t        batch_size = hidden_states.shape[0]\n", "        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n\t        attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n\t        if self.causal:\n\t            attention_mask = combine_masks(attention_mask, causal_mask)\n\t        else:\n\t            attention_mask = attention_mask\n\t        dropout_rng = None\n\t        if not deterministic and self.attn_pdrop > 0.0:\n\t            dropout_rng = self.make_rng(\"dropout\")\n\t        # During fast autoregressive decoding, we feed one position at a time,\n", "        # and cache the keys and values step by step.\n\t        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n\t            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n\t        attention_bias = lax.select(\n\t            attention_mask > 0,\n\t            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n\t            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n\t        )\n\t        attn_weights = nn.dot_product_attention_weights(\n\t            query,\n", "            key,\n\t            bias=attention_bias,\n\t            dropout_rng=dropout_rng,\n\t            dropout_rate=self.attn_pdrop,\n\t            deterministic=deterministic,\n\t            dtype=self.dtype,\n\t            precision=None,\n\t        )\n\t        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n\t        attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n", "        ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n\t        outputs = attn_output + ffn_output + hidden_states\n\t        return outputs\n\tif __name__ == '__main__':\n\t    with jax.profiler.trace('/tmp/prof/vanilla'):\n\t        class Model(nn.Module):\n\t            def setup(self):\n\t                self.blocks = [\n\t                    AttentionBlock(\n\t                        q_chunk_size=256,\n", "                        k_chunk_size=256,\n\t                        hidden_size=2048,\n\t                        num_heads=16,\n\t                        rotary_dim=128,\n\t                        intermediate_size=8192,\n\t                        layer_norm_epsilon=1e-5,\n\t                        activation_function=\"gelu\",\n\t                        resid_pdrop=0.0,\n\t                        max_position_embeddings=2048,\n\t                        dtype=jnp.float32,\n", "                        causal=True,\n\t                )\n\t                for _ in range(2)\n\t                ]\n\t            def __call__(self, hidden_states, attention_mask, position_ids):\n\t                for block in self.blocks:\n\t                    hidden_states = block(hidden_states, attention_mask, position_ids)\n\t                return hidden_states\n\t        hidden_states = jnp.zeros((2, 1024, 2048))\n\t        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n", "        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n\t        model = Model()\n\t        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n\t        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n\t        output = output.block_until_ready()\n"]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/memeff.py", "chunked_list": ["import functools\n\timport json\n\timport math\n\tfrom functools import partial\n\tfrom typing import Callable, NamedTuple, Optional\n\timport flax.linen as nn\n\timport jax\n\timport jax.numpy as jnp\n\timport numpy as np\n\tfrom einops import rearrange\n", "from flax.linen import combine_masks, make_causal_mask\n\tfrom jax import lax\n\tfrom jax import numpy as jnp\n\tdef quick_gelu(x):\n\t    return x * jax.nn.sigmoid(1.702 * x)\n\tACT2FN = {\n\t    \"gelu\": partial(nn.gelu, approximate=False),\n\t    \"relu\": nn.relu,\n\t    \"silu\": nn.swish,\n\t    \"swish\": nn.swish,\n", "    \"gelu_new\": partial(nn.gelu, approximate=True),\n\t    \"quick_gelu\": quick_gelu,\n\t}\n\tdef get_gradient_checkpoint_policy(name):\n\t    return {\n\t        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n\t        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n\t        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n\t        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n\t    }[name]\n", "MASK_VALUE = -1e10\n\tQ_CHUNK_SIZE = 1024\n\tK_CHUNK_SIZE = 1024\n\tdef create_sinusoidal_positions(num_pos, dim):\n\t    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n\t    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n\t    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\t    sentinel = dim // 2 + dim % 2\n\t    out = np.zeros((num_pos, dim))\n\t    out[:, 0:sentinel] = sin\n", "    out[:, sentinel:] = cos\n\t    return jnp.array(out)\n\tdef rotate_every_two(tensor):\n\t    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n\t    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n\t    return rotate_half_tensor\n\tdef apply_rotary_pos_emb(tensor, sincos):\n\t    sin_pos, cos_pos = sincos\n\t    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n\t    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n", "    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\tclass _AttentionBlock(nn.Module):\n\t    hidden_size: int\n\t    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n\t    activation_function: str = \"gelu\"\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n", "    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n\t    float32_logits: bool = False\n\t    def setup(self):\n\t        self.embed_dim = self.hidden_size\n\t        self.head_dim = self.embed_dim // self.num_heads\n\t        dense = partial(\n\t            nn.Dense,\n\t            self.embed_dim,\n\t            use_bias=False,\n", "            dtype=self.dtype,\n\t            kernel_init=jax.nn.initializers.variance_scaling(\n\t                scale=1.0, mode='fan_in',\n\t                distribution='normal',\n\t            )\n\t        )\n\t        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n\t        self.out_proj = dense()\n\t        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\t        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n", "        self.fc_in = nn.Dense(self.intermediate_size,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n\t                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n\t        self.fc_out = nn.Dense(self.embed_dim,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n", "                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n\t        self.act = ACT2FN[self.activation_function]\n\t        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            pos_embd_dim = self.rotary_dim\n\t        else:\n\t            pos_embd_dim = self.embed_dim // self.num_heads\n", "        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\t    def _split_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\t    def _merge_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\t    def attn_out_proj(self, attn_output, deterministic):\n\t        attn_output = self._merge_heads(attn_output)\n\t        attn_output = self.out_proj(attn_output)\n\t        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n\t        return attn_output\n", "    def forward_qkv(\n\t        self,\n\t        hidden_states,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_1(hidden_states)\n\t        query = self.q_proj(hidden_states)\n\t        key = self.k_proj(hidden_states)\n\t        value = self.v_proj(hidden_states)\n", "        query = self._split_heads(query)\n\t        key = self._split_heads(key)\n\t        value = self._split_heads(value)\n\t        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n\t        sincos = jnp.split(sincos, 2, axis=-1)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            k_rot = key[:, :, :, : self.rotary_dim]\n\t            k_pass = key[:, :, :, self.rotary_dim :]\n\t            q_rot = query[:, :, :, : self.rotary_dim]\n\t            q_pass = query[:, :, :, self.rotary_dim :]\n", "            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n\t            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\t            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n\t            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n\t        else:\n\t            key = apply_rotary_pos_emb(key, sincos)\n\t            query = apply_rotary_pos_emb(query, sincos)\n\t        if self.float32_logits:\n\t            query = query.astype(jnp.float32)\n\t            key = key.astype(jnp.float32)\n", "        return query, key, value\n\t    def forward_ffn(\n\t        self,\n\t        hidden_states,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_2(hidden_states)\n\t        hidden_states = self.fc_in(hidden_states)\n\t        hidden_states = self.act(hidden_states)\n\t        hidden_states = self.fc_out(hidden_states)\n", "        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\t        return hidden_states\n\tclass AttentionBlock(nn.Module):\n\t    q_chunk_size: int\n\t    k_chunk_size: int\n\t    hidden_size: int\n\t    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n", "    activation_function: str = \"gelu\"\n\t    attn_pdrop: float = 0.0\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n\t    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n\t    policy: str = 'nothing_saveable'\n\t    prevent_cse: bool = False\n\t    float32_logits: bool = False\n\t    def setup(self):\n", "        self.attn = _AttentionBlock(\n\t            self.hidden_size,\n\t            self.num_heads,\n\t            self.rotary_dim,\n\t            self.intermediate_size,\n\t            self.layer_norm_epsilon,\n\t            self.activation_function,\n\t            self.resid_pdrop,\n\t            self.max_position_embeddings,\n\t            self.dtype,\n", "            self.causal,\n\t            self.float32_logits,\n\t        )\n\t    @nn.compact\n\t    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\t        \"\"\"\n\t        This function takes projected key, value states from a single input token and concatenates the states to cached\n\t        states from previous steps. This function is slighly adapted from the official Flax repository:\n\t        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n\t        \"\"\"\n", "        # detect if we're initializing by absence of existing cache data.\n\t        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n\t        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n\t        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n\t        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\t        if is_initialized:\n\t            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n\t            # update key, value caches with our new 1d spatial slices\n\t            cur_index = cache_index.value\n\t            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n", "            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n\t            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n\t            cached_key.value = key\n\t            cached_value.value = value\n\t            num_updated_cache_vectors = query.shape[1]\n\t            cache_index.value = cache_index.value + num_updated_cache_vectors\n\t            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n\t            pad_mask = jnp.broadcast_to(\n\t                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n\t                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n", "            )\n\t            attention_mask = combine_masks(pad_mask, attention_mask)\n\t        return key, value, attention_mask\n\t    def __call__(\n\t        self,\n\t        hidden_states,\n\t        attention_mask,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t        init_cache: bool = False,\n", "    ):\n\t        query, key, value = self.attn.forward_qkv(\n\t            hidden_states,\n\t            position_ids,\n\t            deterministic=deterministic,\n\t        )\n\t        query = query / jnp.sqrt(query.shape[-1])\n\t        dropout_rng = None\n\t        if not deterministic and self.attn_pdrop > 0.0:\n\t            dropout_rng = self.make_rng(\"dropout\")\n", "        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\t        attention_bias = lax.select(\n\t            attention_mask > 0,\n\t            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n\t            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n\t        )\n\t        # During fast autoregressive decoding, we feed one position at a time,\n\t        # and cache the keys and values step by step.\n\t        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n\t            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n", "            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n\t            # use standard dot product attention since query length is 1\n\t            attn_weights = nn.dot_product_attention_weights(\n\t                query,\n\t                key,\n\t                bias=attention_bias,\n\t                dropout_rng=dropout_rng,\n\t                dropout_rate=self.config.attn_pdrop,\n\t                deterministic=deterministic,\n\t                dtype=self.dtype,\n", "                precision=None,\n\t            )\n\t            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n\t            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n\t            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n\t            outputs = attn_output + ffn_output + hidden_states\n\t        else:\n\t            attn_output = blockwise_compute_attn(\n\t                query,\n\t                key,\n", "                value,\n\t                bias=attention_bias,\n\t                deterministic=not deterministic,\n\t                dropout_rng=dropout_rng,\n\t                attn_pdrop=self.attn_pdrop,\n\t                causal_mask=self.causal,\n\t                query_chunk_size=self.q_chunk_size,\n\t                key_chunk_size=self.k_chunk_size,\n\t                dtype=self.dtype,\n\t                policy=self.policy,\n", "                precision=None,\n\t                prevent_cse=self.prevent_cse,\n\t            )\n\t            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n\t            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n\t            outputs = ffn_output + hidden_states + attn_output\n\t        return outputs\n\tdef _chunk_attention_bias(query_chunk_size, key_chunk_size,\n\t            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n\t            query_chunk_idx, key_chunk_idx):\n", "    query_offset = query_chunk_idx * query_chunk_size\n\t    key_offset = key_chunk_idx * key_chunk_size\n\t    chunk_bias = jnp.zeros((1, 1, 1, 1))\n\t    if bias is not None:\n\t        chunk_bias = lax.dynamic_slice(\n\t            bias,\n\t            start_indices=(0, 0, query_offset, key_offset),\n\t            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n\t        )\n\t    if causal_mask:\n", "        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n\t        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n\t        offset = query_offset - key_offset\n\t        query_idx += offset\n\t        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n\t        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\t    if not deterministic and attn_pdrop > 0.0:\n\t        attn_dropout_slice = lax.dynamic_slice(\n\t            attn_dropout,\n\t            start_indices=(0, 0, query_offset, key_offset),\n", "            slice_sizes=(\n\t                *attn_dropout.shape[:2],\n\t                min(attn_dropout.shape[-2], query_chunk_size),\n\t                min(attn_dropout.shape[-1], key_chunk_size),\n\t            ),\n\t        )\n\t        chunk_bias -= attn_dropout_slice * 1e6\n\t    return chunk_bias\n\tclass Carry(NamedTuple):\n\t    numerator: jax.Array\n", "    denominator: jax.Array\n\t    max_so_far: jax.Array\n\tdef blockwise_compute_attn(query, key, value,\n\t        bias=None,\n\t        deterministic=False,\n\t        dropout_rng=None,\n\t        attn_pdrop=0.0,\n\t        causal_mask=True,\n\t        query_chunk_size=None,\n\t        key_chunk_size=None,\n", "        dtype=jnp.float32,\n\t        policy='nothing_saveable',\n\t        precision=lax.Precision.HIGHEST,\n\t        prevent_cse=False,):\n\t    q_len = query.shape[1]\n\t    kv_len = key.shape[1]\n\t    query = rearrange(query, 'b (n c) h q -> b n c h q', c=query_chunk_size)\n\t    key, value = map(lambda t: rearrange(t, 'b (n c) h v -> b n c h v', c=key_chunk_size), (key, value))\n\t    query, key, value = map(lambda t: rearrange(t, 'b n c h d -> n b c h d'), (query, key, value))\n\t    num_q, batch, _, num_heads, dim_per_head = query.shape\n", "    num_kv, _, _, _, _ = key.shape\n\t    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n\t        assert bias_dim == 1 or bias_dim == broadcast_dim\n\t    if not deterministic and attn_pdrop > 0.0:\n\t        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n\t        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n\t    else:\n\t        attn_dropout = None\n\t    _chunk_bias_fn = functools.partial(\n\t        _chunk_attention_bias,\n", "        query_chunk_size, key_chunk_size,\n\t        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\t    def _query_chunk_attention(args):\n\t        query_chunk, query_chunk_idx = args\n\t        @functools.partial(jax.checkpoint, prevent_cse=prevent_cse,\n\t                           policy=get_gradient_checkpoint_policy(policy))\n\t        def summarize_chunk(carry, args):\n\t            key_chunk, value_chunk, key_chunk_idx = args\n\t            (numerator, denominator, prev_max_score) = carry\n\t            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n", "            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n\t            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n\t            attn_weights = attn_weights + bias_chunk\n\t            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n\t            max_score = jnp.maximum(prev_max_score, max_score)\n\t            max_score = jax.lax.stop_gradient(max_score)\n\t            exp_weights = jnp.exp(attn_weights - max_score)\n\t            exp_values = jnp.einsum(\n\t                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n\t            )\n", "            correction = jnp.exp(prev_max_score - max_score)\n\t            numerator = numerator * correction + exp_values\n\t            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n\t            return Carry(numerator, denominator, max_score), None\n\t        init_carry = Carry(\n\t            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n\t            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n\t            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n\t        )\n\t        (numerator, denominator, max_score), _ = lax.scan(\n", "            summarize_chunk, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n\t        )\n\t        outputs = (numerator / denominator).astype(dtype)\n\t        return outputs\n\t    _, res = lax.scan(\n\t        lambda _, x: ((), _query_chunk_attention(x)),\n\t        (), xs=(query, jnp.arange(0, num_q))\n\t    )\n\t    res = rearrange(res, 'n b c h d -> b (n c) h d')\n\t    return res\n", "if __name__ == '__main__':\n\t    with jax.profiler.trace('/tmp/prof/memeff'):\n\t        class Model(nn.Module):\n\t            def setup(self):\n\t                self.blocks = [\n\t                    AttentionBlock(\n\t                        q_chunk_size=256,\n\t                        k_chunk_size=256,\n\t                        hidden_size=2048,\n\t                        num_heads=16,\n", "                        rotary_dim=128,\n\t                        intermediate_size=8192,\n\t                        layer_norm_epsilon=1e-5,\n\t                        activation_function=\"gelu\",\n\t                        resid_pdrop=0.0,\n\t                        max_position_embeddings=2048,\n\t                        dtype=jnp.float32,\n\t                        causal=True,\n\t                )\n\t                for _ in range(2)\n", "                ]\n\t            def __call__(self, hidden_states, attention_mask, position_ids):\n\t                for block in self.blocks:\n\t                    hidden_states = block(hidden_states, attention_mask, position_ids)\n\t                return hidden_states\n\t        hidden_states = jnp.zeros((2, 1024, 2048))\n\t        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n\t        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n\t        model = Model()\n\t        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n", "        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n\t        output = output.block_until_ready()\n"]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/__init__.py", "chunked_list": []}
{"filename": "blockwise-parallel-transformer/bpt/blocks/blockwise_parallel_v1.py", "chunked_list": ["import functools\n\timport json\n\timport math\n\tfrom functools import partial\n\tfrom typing import Callable, NamedTuple, Optional\n\timport flax.linen as nn\n\timport jax\n\timport jax.numpy as jnp\n\timport numpy as np\n\tfrom einops import rearrange\n", "from flax.linen import combine_masks, make_causal_mask\n\tfrom jax import lax\n\tfrom jax import numpy as jnp\n\tdef quick_gelu(x):\n\t    return x * jax.nn.sigmoid(1.702 * x)\n\tACT2FN = {\n\t    \"gelu\": partial(nn.gelu, approximate=False),\n\t    \"relu\": nn.relu,\n\t    \"silu\": nn.swish,\n\t    \"swish\": nn.swish,\n", "    \"gelu_new\": partial(nn.gelu, approximate=True),\n\t    \"quick_gelu\": quick_gelu,\n\t}\n\tdef get_gradient_checkpoint_policy(name):\n\t    return {\n\t        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n\t        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n\t        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n\t        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n\t    }[name]\n", "MASK_VALUE = -1e10\n\tQ_CHUNK_SIZE = 1024\n\tK_CHUNK_SIZE = 1024\n\tdef create_sinusoidal_positions(num_pos, dim):\n\t    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n\t    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n\t    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\t    sentinel = dim // 2 + dim % 2\n\t    out = np.zeros((num_pos, dim))\n\t    out[:, 0:sentinel] = sin\n", "    out[:, sentinel:] = cos\n\t    return jnp.array(out)\n\tdef rotate_every_two(tensor):\n\t    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n\t    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n\t    return rotate_half_tensor\n\tdef apply_rotary_pos_emb(tensor, sincos):\n\t    sin_pos, cos_pos = sincos\n\t    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n\t    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n", "    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\tclass _AttentionBlock(nn.Module):\n\t    hidden_size: int\n\t    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n\t    activation_function: str = \"gelu\"\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n", "    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n\t    float32_logits: bool = False\n\t    def setup(self):\n\t        self.embed_dim = self.hidden_size\n\t        self.head_dim = self.embed_dim // self.num_heads\n\t        dense = partial(\n\t            nn.Dense,\n\t            self.embed_dim,\n\t            use_bias=False,\n", "            dtype=self.dtype,\n\t            kernel_init=jax.nn.initializers.variance_scaling(\n\t                scale=1.0, mode='fan_in',\n\t                distribution='normal',\n\t            )\n\t        )\n\t        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n\t        self.out_proj = dense()\n\t        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\t        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n", "        self.fc_in = nn.Dense(self.intermediate_size,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n\t                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n\t        self.fc_out = nn.Dense(self.embed_dim,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n", "                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n\t        self.act = ACT2FN[self.activation_function]\n\t        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            pos_embd_dim = self.rotary_dim\n\t        else:\n\t            pos_embd_dim = self.embed_dim // self.num_heads\n", "        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\t    def _split_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\t    def _merge_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\t    def attn_out_proj(self, attn_output, deterministic):\n\t        attn_output = self._merge_heads(attn_output)\n\t        attn_output = self.out_proj(attn_output)\n\t        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n\t        return attn_output\n", "    def forward_qkv(\n\t        self,\n\t        hidden_states,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_1(hidden_states)\n\t        query = self.q_proj(hidden_states)\n\t        key = self.k_proj(hidden_states)\n\t        value = self.v_proj(hidden_states)\n", "        query = self._split_heads(query)\n\t        key = self._split_heads(key)\n\t        value = self._split_heads(value)\n\t        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n\t        sincos = jnp.split(sincos, 2, axis=-1)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            k_rot = key[:, :, :, : self.rotary_dim]\n\t            k_pass = key[:, :, :, self.rotary_dim :]\n\t            q_rot = query[:, :, :, : self.rotary_dim]\n\t            q_pass = query[:, :, :, self.rotary_dim :]\n", "            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n\t            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\t            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n\t            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n\t        else:\n\t            key = apply_rotary_pos_emb(key, sincos)\n\t            query = apply_rotary_pos_emb(query, sincos)\n\t        if self.float32_logits:\n\t            query = query.astype(jnp.float32)\n\t            key = key.astype(jnp.float32)\n", "        return query, key, value\n\t    def forward_ffn(\n\t        self,\n\t        hidden_states,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_2(hidden_states)\n\t        hidden_states = self.fc_in(hidden_states)\n\t        hidden_states = self.act(hidden_states)\n\t        hidden_states = self.fc_out(hidden_states)\n", "        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\t        return hidden_states\n\t    def forward_query(\n\t        self,\n\t        hidden_states,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_1(hidden_states)\n\t        query = self.q_proj(hidden_states)\n", "        query = self._split_heads(query)\n\t        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n\t        sincos = jnp.split(sincos, 2, axis=-1)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            q_rot = query[:, :, :, : self.rotary_dim]\n\t            q_pass = query[:, :, :, self.rotary_dim :]\n\t            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\t            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n\t        else:\n\t            query = apply_rotary_pos_emb(query, sincos)\n", "        if self.float32_logits:\n\t            query = query.astype(jnp.float32)\n\t        return query\n\t    def forward_key_value(\n\t        self,\n\t        hidden_states,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_1(hidden_states)\n", "        key = self.k_proj(hidden_states)\n\t        value = self.v_proj(hidden_states)\n\t        key = self._split_heads(key)\n\t        value = self._split_heads(value)\n\t        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n\t        sincos = jnp.split(sincos, 2, axis=-1)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            k_rot = key[:, :, :, : self.rotary_dim]\n\t            k_pass = key[:, :, :, self.rotary_dim :]\n\t            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n", "            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n\t        else:\n\t            key = apply_rotary_pos_emb(key, sincos)\n\t        if self.float32_logits:\n\t            key = key.astype(jnp.float32)\n\t        return key, value\n\tclass AttentionBlock(nn.Module):\n\t    q_chunk_size: int\n\t    k_chunk_size: int\n\t    hidden_size: int\n", "    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n\t    activation_function: str = \"gelu\"\n\t    attn_pdrop: float = 0.0\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n\t    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n", "    policy: str = 'nothing_saveable'\n\t    prevent_cse: bool = False\n\t    float32_logits: bool = False\n\t    def setup(self):\n\t        self.attn = _AttentionBlock(\n\t            self.hidden_size,\n\t            self.num_heads,\n\t            self.rotary_dim,\n\t            self.intermediate_size,\n\t            self.layer_norm_epsilon,\n", "            self.activation_function,\n\t            self.resid_pdrop,\n\t            self.max_position_embeddings,\n\t            self.dtype,\n\t            self.causal,\n\t            self.float32_logits,\n\t        )\n\t    @nn.compact\n\t    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\t        \"\"\"\n", "        This function takes projected key, value states from a single input token and concatenates the states to cached\n\t        states from previous steps. This function is slighly adapted from the official Flax repository:\n\t        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n\t        \"\"\"\n\t        # detect if we're initializing by absence of existing cache data.\n\t        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n\t        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n\t        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n\t        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\t        if is_initialized:\n", "            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n\t            # update key, value caches with our new 1d spatial slices\n\t            cur_index = cache_index.value\n\t            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n\t            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n\t            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n\t            cached_key.value = key\n\t            cached_value.value = value\n\t            num_updated_cache_vectors = query.shape[1]\n\t            cache_index.value = cache_index.value + num_updated_cache_vectors\n", "            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n\t            pad_mask = jnp.broadcast_to(\n\t                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n\t                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n\t            )\n\t            attention_mask = combine_masks(pad_mask, attention_mask)\n\t        return key, value, attention_mask\n\t    def __call__(\n\t        self,\n\t        hidden_states,\n", "        attention_mask,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t        init_cache: bool = False,\n\t    ):\n\t        dropout_rng = None\n\t        if not deterministic and self.attn_pdrop > 0.0:\n\t            dropout_rng = self.make_rng(\"dropout\")\n\t        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\t        attention_bias = lax.select(\n", "            attention_mask > 0,\n\t            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n\t            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n\t        )\n\t        # During fast autoregressive decoding, we feed one position at a time,\n\t        # and cache the keys and values step by step.\n\t        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n\t            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n\t            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n\t            # use standard dot product attention since query length is 1\n", "            attn_weights = nn.dot_product_attention_weights(\n\t                query,\n\t                key,\n\t                bias=attention_bias,\n\t                dropout_rng=dropout_rng,\n\t                dropout_rate=self.config.attn_pdrop,\n\t                deterministic=deterministic,\n\t                dtype=self.dtype,\n\t                precision=None,\n\t            )\n", "            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n\t            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n\t            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n\t            outputs = attn_output + ffn_output + hidden_states\n\t        else:\n\t            outputs = blockwise_compute(\n\t                self.attn,\n\t                hidden_states,\n\t                hidden_states,\n\t                position_ids,\n", "                num_heads=self.num_heads,\n\t                bias=attention_bias,\n\t                deterministic=deterministic,\n\t                dropout_rng=dropout_rng,\n\t                attn_pdrop=self.attn_pdrop,\n\t                causal_mask=self.causal,\n\t                query_chunk_size=self.q_chunk_size,\n\t                key_chunk_size=self.k_chunk_size,\n\t                dtype=self.dtype,\n\t                policy=self.policy,\n", "                precision=None,\n\t                prevent_cse=self.prevent_cse,\n\t            )\n\t        return outputs\n\tdef _chunk_attention_bias(query_chunk_size, key_chunk_size,\n\t            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n\t            query_chunk_idx, key_chunk_idx):\n\t    query_offset = query_chunk_idx * query_chunk_size\n\t    key_offset = key_chunk_idx * key_chunk_size\n\t    chunk_bias = jnp.zeros((1, 1, 1, 1))\n", "    if bias is not None:\n\t        chunk_bias = lax.dynamic_slice(\n\t            bias,\n\t            start_indices=(0, 0, query_offset, key_offset),\n\t            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n\t        )\n\t    if causal_mask:\n\t        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n\t        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n\t        offset = query_offset - key_offset\n", "        query_idx += offset\n\t        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n\t        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\t    if not deterministic and attn_pdrop > 0.0:\n\t        attn_dropout_slice = lax.dynamic_slice(\n\t            attn_dropout,\n\t            start_indices=(0, 0, query_offset, key_offset),\n\t            slice_sizes=(\n\t                *attn_dropout.shape[:2],\n\t                min(attn_dropout.shape[-2], query_chunk_size),\n", "                min(attn_dropout.shape[-1], key_chunk_size),\n\t            ),\n\t        )\n\t        chunk_bias -= attn_dropout_slice * 1e6\n\t    return chunk_bias\n\tclass Carry(NamedTuple):\n\t    numerator: jax.Array\n\t    denominator: jax.Array\n\t    max_so_far: jax.Array\n\tdef blockwise_compute(cell,\n", "        q_inputs,\n\t        kv_inputs,\n\t        position_ids,\n\t        num_heads,\n\t        bias=None,\n\t        deterministic=False,\n\t        dropout_rng=None,\n\t        attn_pdrop=0.0,\n\t        causal_mask=True,\n\t        query_chunk_size=None,\n", "        key_chunk_size=None,\n\t        dtype=jnp.float32,\n\t        policy='nothing_saveable',\n\t        precision=lax.Precision.HIGHEST,\n\t        prevent_cse=False,):\n\t    q_len = q_inputs.shape[1]\n\t    kv_len = kv_inputs.shape[1]\n\t    q_inputs = rearrange(q_inputs, 'b (n c) d -> b n c d', c=query_chunk_size)\n\t    kv_inputs = rearrange(kv_inputs, 'b (n c) d -> b n c d', c=key_chunk_size)\n\t    q_inputs, kv_inputs = map(lambda t: rearrange(t, 'b n c d -> n b c d'), (q_inputs, kv_inputs))\n", "    num_q, batch, _, _ = q_inputs.shape\n\t    num_kv, _, _, _ = kv_inputs.shape\n\t    q_position_ids = rearrange(position_ids, 'b (n c) -> n b c', c=query_chunk_size)\n\t    kv_position_ids = rearrange(position_ids, 'b (n c) -> n b c', c=key_chunk_size)\n\t    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n\t        assert bias_dim == 1 or bias_dim == broadcast_dim\n\t    if not deterministic and attn_pdrop > 0.0:\n\t        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n\t        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n\t    else:\n", "        attn_dropout = None\n\t    _chunk_bias_fn = functools.partial(\n\t        _chunk_attention_bias,\n\t        query_chunk_size, key_chunk_size,\n\t        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\t    def _query_chunk_attention(cell, _, args):\n\t        input_chunk, query_chunk_idx, query_position_ids_chunk = args\n\t        query_chunk = cell.forward_query(input_chunk, query_position_ids_chunk)\n\t        query_chunk = query_chunk / jnp.sqrt(query_chunk.shape[-1])\n\t        dim_per_head = query_chunk.shape[-1]\n", "        def summarize_chunk(cell, carry, args):\n\t            kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n\t            (numerator, denominator, prev_max_score) = carry\n\t            key_chunk, value_chunk = cell.forward_key_value(kv_chunk, kv_position_ids_chunk)\n\t            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n\t            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n\t            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n\t            attn_weights = attn_weights + bias_chunk\n\t            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n\t            max_score = jnp.maximum(prev_max_score, max_score)\n", "            max_score = jax.lax.stop_gradient(max_score)\n\t            exp_weights = jnp.exp(attn_weights - max_score)\n\t            exp_values = jnp.einsum(\n\t                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n\t            )\n\t            correction = jnp.exp(prev_max_score - max_score)\n\t            numerator = numerator * correction + exp_values\n\t            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n\t            return Carry(numerator, denominator, max_score), None\n\t        init_carry = Carry(\n", "            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n\t            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n\t            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n\t        )\n\t        summarize_chunk = nn.remat(\n\t            summarize_chunk,\n\t            variables=\"params\",\n\t            rngs={\"params\" : False, \"dropout\": False},\n\t            prevent_cse=prevent_cse,\n\t            policy=get_gradient_checkpoint_policy(policy),\n", "        )\n\t        (numerator, denominator, max_score), _ = nn.scan(\n\t            summarize_chunk,\n\t            variable_broadcast=\"params\",\n\t            split_rngs={\"params\" : False, \"dropout\": False},\n\t            in_axes=0,\n\t            out_axes=0,\n\t            length=num_kv,\n\t        )(cell, init_carry, (kv_inputs, jnp.arange(0, num_kv), kv_position_ids))\n\t        attn_chunk = (numerator / denominator).astype(dtype)\n", "        attn_chunk = cell.attn_out_proj(attn_chunk, deterministic)\n\t        ffn_chunk = cell.forward_ffn(attn_chunk + input_chunk, deterministic)\n\t        outputs = ffn_chunk + attn_chunk + input_chunk\n\t        return _, outputs\n\t    _query_chunk_attention = nn.remat(\n\t        _query_chunk_attention,\n\t        variables=\"params\",\n\t        rngs={\"params\" : False, \"dropout\": False},\n\t        prevent_cse=prevent_cse,\n\t        policy=get_gradient_checkpoint_policy(policy),\n", "    )\n\t    _, res = nn.scan(\n\t        _query_chunk_attention,\n\t        variable_broadcast=\"params\",\n\t        split_rngs={\"params\" : False, \"dropout\": False},\n\t        in_axes=0,\n\t        out_axes=0,\n\t        length=num_q,\n\t    )(cell, None, (q_inputs, jnp.arange(0, num_q), q_position_ids))\n\t    res = rearrange(res, 'n b c d -> b (n c) d')\n", "    return res\n\tif __name__ == '__main__':\n\t    with jax.profiler.trace('/tmp/prof/blockwise_parallel_v1'):\n\t        class Model(nn.Module):\n\t            def setup(self):\n\t                self.blocks = [\n\t                    AttentionBlock(\n\t                        q_chunk_size=256,\n\t                        k_chunk_size=256,\n\t                        hidden_size=2048,\n", "                        num_heads=16,\n\t                        rotary_dim=128,\n\t                        intermediate_size=8192,\n\t                        layer_norm_epsilon=1e-5,\n\t                        activation_function=\"gelu\",\n\t                        resid_pdrop=0.0,\n\t                        max_position_embeddings=2048,\n\t                        dtype=jnp.float32,\n\t                        causal=True,\n\t                )\n", "                for _ in range(2)\n\t                ]\n\t            def __call__(self, hidden_states, attention_mask, position_ids):\n\t                for block in self.blocks:\n\t                    hidden_states = block(hidden_states, attention_mask, position_ids)\n\t                return hidden_states\n\t        hidden_states = jnp.zeros((2, 1024, 2048))\n\t        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n\t        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n\t        model = Model()\n", "        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n\t        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n\t        output = output.block_until_ready()\n"]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/blockwise_parallel.py", "chunked_list": ["import functools\n\timport json\n\timport math\n\tfrom functools import partial\n\tfrom typing import Callable, NamedTuple, Optional\n\timport flax.linen as nn\n\timport jax\n\timport jax.numpy as jnp\n\timport numpy as np\n\tfrom einops import rearrange\n", "from flax.linen import combine_masks, make_causal_mask\n\tfrom jax import lax\n\tfrom jax import numpy as jnp\n\tdef quick_gelu(x):\n\t    return x * jax.nn.sigmoid(1.702 * x)\n\tACT2FN = {\n\t    \"gelu\": partial(nn.gelu, approximate=False),\n\t    \"relu\": nn.relu,\n\t    \"silu\": nn.swish,\n\t    \"swish\": nn.swish,\n", "    \"gelu_new\": partial(nn.gelu, approximate=True),\n\t    \"quick_gelu\": quick_gelu,\n\t}\n\tdef get_gradient_checkpoint_policy(name):\n\t    return {\n\t        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n\t        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n\t        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n\t        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n\t    }[name]\n", "MASK_VALUE = -1e10\n\tQ_CHUNK_SIZE = 1024\n\tK_CHUNK_SIZE = 1024\n\tdef create_sinusoidal_positions(num_pos, dim):\n\t    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n\t    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n\t    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\t    sentinel = dim // 2 + dim % 2\n\t    out = np.zeros((num_pos, dim))\n\t    out[:, 0:sentinel] = sin\n", "    out[:, sentinel:] = cos\n\t    return jnp.array(out)\n\tdef rotate_every_two(tensor):\n\t    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n\t    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n\t    return rotate_half_tensor\n\tdef apply_rotary_pos_emb(tensor, sincos):\n\t    sin_pos, cos_pos = sincos\n\t    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n\t    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n", "    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\tclass _AttentionBlock(nn.Module):\n\t    hidden_size: int\n\t    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n\t    activation_function: str = \"gelu\"\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n", "    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n\t    float32_logits: bool = False\n\t    def setup(self):\n\t        self.embed_dim = self.hidden_size\n\t        self.head_dim = self.embed_dim // self.num_heads\n\t        dense = partial(\n\t            nn.Dense,\n\t            self.embed_dim,\n\t            use_bias=False,\n", "            dtype=self.dtype,\n\t            kernel_init=jax.nn.initializers.variance_scaling(\n\t                scale=1.0, mode='fan_in',\n\t                distribution='normal',\n\t            )\n\t        )\n\t        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n\t        self.out_proj = dense()\n\t        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\t        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n", "        self.fc_in = nn.Dense(self.intermediate_size,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n\t                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n\t        self.fc_out = nn.Dense(self.embed_dim,\n\t                            dtype=self.dtype,\n\t                            kernel_init=jax.nn.initializers.variance_scaling(\n", "                            scale=1.0, mode='fan_in',\n\t                            distribution='normal',\n\t            )\n\t        )\n\t        self.act = ACT2FN[self.activation_function]\n\t        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            pos_embd_dim = self.rotary_dim\n\t        else:\n\t            pos_embd_dim = self.embed_dim // self.num_heads\n", "        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\t    def _split_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\t    def _merge_heads(self, hidden_states):\n\t        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\t    def attn_out_proj(self, attn_output, deterministic):\n\t        attn_output = self._merge_heads(attn_output)\n\t        attn_output = self.out_proj(attn_output)\n\t        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n\t        return attn_output\n", "    def forward_qkv(\n\t        self,\n\t        hidden_states,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_1(hidden_states)\n\t        query = self.q_proj(hidden_states)\n\t        key = self.k_proj(hidden_states)\n\t        value = self.v_proj(hidden_states)\n", "        query = self._split_heads(query)\n\t        key = self._split_heads(key)\n\t        value = self._split_heads(value)\n\t        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n\t        sincos = jnp.split(sincos, 2, axis=-1)\n\t        if self.rotary_dim is not None and self.rotary_dim > 0:\n\t            k_rot = key[:, :, :, : self.rotary_dim]\n\t            k_pass = key[:, :, :, self.rotary_dim :]\n\t            q_rot = query[:, :, :, : self.rotary_dim]\n\t            q_pass = query[:, :, :, self.rotary_dim :]\n", "            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n\t            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\t            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n\t            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n\t        else:\n\t            key = apply_rotary_pos_emb(key, sincos)\n\t            query = apply_rotary_pos_emb(query, sincos)\n\t        if self.float32_logits:\n\t            query = query.astype(jnp.float32)\n\t            key = key.astype(jnp.float32)\n", "        return query, key, value\n\t    def forward_ffn(\n\t        self,\n\t        hidden_states,\n\t        deterministic: bool = True,\n\t    ):\n\t        hidden_states = self.ln_2(hidden_states)\n\t        hidden_states = self.fc_in(hidden_states)\n\t        hidden_states = self.act(hidden_states)\n\t        hidden_states = self.fc_out(hidden_states)\n", "        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\t        return hidden_states\n\tclass AttentionBlock(nn.Module):\n\t    q_chunk_size: int\n\t    k_chunk_size: int\n\t    hidden_size: int\n\t    num_heads: int\n\t    rotary_dim: Optional[int]\n\t    intermediate_size: int\n\t    layer_norm_epsilon: float = 1e-5\n", "    activation_function: str = \"gelu\"\n\t    attn_pdrop: float = 0.0\n\t    resid_pdrop: float = 0.0\n\t    max_position_embeddings: int = 1024\n\t    dtype: jnp.dtype = jnp.float32\n\t    causal: bool = True\n\t    policy: str = 'nothing_saveable'\n\t    prevent_cse: bool = False\n\t    float32_logits: bool = False\n\t    def setup(self):\n", "        self.attn = _AttentionBlock(\n\t            self.hidden_size,\n\t            self.num_heads,\n\t            self.rotary_dim,\n\t            self.intermediate_size,\n\t            self.layer_norm_epsilon,\n\t            self.activation_function,\n\t            self.resid_pdrop,\n\t            self.max_position_embeddings,\n\t            self.dtype,\n", "            self.causal,\n\t            self.float32_logits,\n\t        )\n\t    @nn.compact\n\t    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\t        \"\"\"\n\t        This function takes projected key, value states from a single input token and concatenates the states to cached\n\t        states from previous steps. This function is slighly adapted from the official Flax repository:\n\t        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n\t        \"\"\"\n", "        # detect if we're initializing by absence of existing cache data.\n\t        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n\t        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n\t        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n\t        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\t        if is_initialized:\n\t            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n\t            # update key, value caches with our new 1d spatial slices\n\t            cur_index = cache_index.value\n\t            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n", "            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n\t            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n\t            cached_key.value = key\n\t            cached_value.value = value\n\t            num_updated_cache_vectors = query.shape[1]\n\t            cache_index.value = cache_index.value + num_updated_cache_vectors\n\t            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n\t            pad_mask = jnp.broadcast_to(\n\t                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n\t                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n", "            )\n\t            attention_mask = combine_masks(pad_mask, attention_mask)\n\t        return key, value, attention_mask\n\t    def __call__(\n\t        self,\n\t        hidden_states,\n\t        attention_mask,\n\t        position_ids,\n\t        deterministic: bool = True,\n\t        init_cache: bool = False,\n", "    ):\n\t        query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n\t        query = query / jnp.sqrt(query.shape[-1])\n\t        dropout_rng = None\n\t        if not deterministic and self.attn_pdrop > 0.0:\n\t            dropout_rng = self.make_rng(\"dropout\")\n\t        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\t        attention_bias = lax.select(\n\t            attention_mask > 0,\n\t            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n", "            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n\t        )\n\t        # During fast autoregressive decoding, we feed one position at a time,\n\t        # and cache the keys and values step by step.\n\t        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n\t            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n\t            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n\t            # use standard dot product attention since query length is 1\n\t            attn_weights = nn.dot_product_attention_weights(\n\t                query,\n", "                key,\n\t                bias=attention_bias,\n\t                dropout_rng=dropout_rng,\n\t                dropout_rate=self.config.attn_pdrop,\n\t                deterministic=deterministic,\n\t                dtype=self.dtype,\n\t                precision=None,\n\t            )\n\t            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n\t            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n", "            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n\t            outputs = attn_output + ffn_output + hidden_states\n\t        else:\n\t            attn_output = blockwise_compute_attn(\n\t                query,\n\t                key,\n\t                value,\n\t                bias=attention_bias,\n\t                deterministic=not deterministic,\n\t                dropout_rng=dropout_rng,\n", "                attn_pdrop=self.attn_pdrop,\n\t                causal_mask=self.causal,\n\t                query_chunk_size=self.q_chunk_size,\n\t                key_chunk_size=self.k_chunk_size,\n\t                dtype=self.dtype,\n\t                policy=self.policy,\n\t                precision=None,\n\t                prevent_cse=self.prevent_cse,\n\t            )\n\t            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n", "            ffn_output = blockwise_compute_ffn(\n\t                self.attn,\n\t                hidden_states + attn_output,\n\t                chunk_size=self.q_chunk_size,\n\t                deterministic=deterministic,\n\t                policy=self.policy,\n\t                prevent_cse=self.prevent_cse,\n\t            )\n\t            outputs = ffn_output + hidden_states + attn_output\n\t        return outputs\n", "def _chunk_attention_bias(query_chunk_size, key_chunk_size,\n\t            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n\t            query_chunk_idx, key_chunk_idx):\n\t    query_offset = query_chunk_idx * query_chunk_size\n\t    key_offset = key_chunk_idx * key_chunk_size\n\t    chunk_bias = jnp.zeros((1, 1, 1, 1))\n\t    if bias is not None:\n\t        chunk_bias = lax.dynamic_slice(\n\t            bias,\n\t            start_indices=(0, 0, query_offset, key_offset),\n", "            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n\t        )\n\t    if causal_mask:\n\t        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n\t        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n\t        offset = query_offset - key_offset\n\t        query_idx += offset\n\t        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n\t        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\t    if not deterministic and attn_pdrop > 0.0:\n", "        attn_dropout_slice = lax.dynamic_slice(\n\t            attn_dropout,\n\t            start_indices=(0, 0, query_offset, key_offset),\n\t            slice_sizes=(\n\t                *attn_dropout.shape[:2],\n\t                min(attn_dropout.shape[-2], query_chunk_size),\n\t                min(attn_dropout.shape[-1], key_chunk_size),\n\t            ),\n\t        )\n\t        chunk_bias -= attn_dropout_slice * 1e6\n", "    return chunk_bias\n\tclass Carry(NamedTuple):\n\t    numerator: jax.Array\n\t    denominator: jax.Array\n\t    max_so_far: jax.Array\n\tdef blockwise_compute_attn(query, key, value,\n\t        bias=None,\n\t        deterministic=False,\n\t        dropout_rng=None,\n\t        attn_pdrop=0.0,\n", "        causal_mask=True,\n\t        query_chunk_size=None,\n\t        key_chunk_size=None,\n\t        dtype=jnp.float32,\n\t        policy='nothing_saveable',\n\t        precision=lax.Precision.HIGHEST,\n\t        prevent_cse=False,):\n\t    q_len = query.shape[1]\n\t    kv_len = key.shape[1]\n\t    query = rearrange(query, 'b (n c) h q -> b n c h q', c=query_chunk_size)\n", "    key, value = map(lambda t: rearrange(t, 'b (n c) h v -> b n c h v', c=key_chunk_size), (key, value))\n\t    query, key, value = map(lambda t: rearrange(t, 'b n c h d -> n b c h d'), (query, key, value))\n\t    num_q, batch, _, num_heads, dim_per_head = query.shape\n\t    num_kv, _, _, _, _ = key.shape\n\t    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n\t        assert bias_dim == 1 or bias_dim == broadcast_dim\n\t    if not deterministic and attn_pdrop > 0.0:\n\t        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n\t        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n\t    else:\n", "        attn_dropout = None\n\t    _chunk_bias_fn = functools.partial(\n\t        _chunk_attention_bias,\n\t        query_chunk_size, key_chunk_size,\n\t        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\t    def _query_chunk_attention(args):\n\t        query_chunk, query_chunk_idx = args\n\t        @functools.partial(jax.checkpoint, prevent_cse=prevent_cse,\n\t                           policy=get_gradient_checkpoint_policy(policy))\n\t        def summarize_chunk(carry, args):\n", "            key_chunk, value_chunk, key_chunk_idx = args\n\t            (numerator, denominator, prev_max_score) = carry\n\t            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n\t            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n\t            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n\t            attn_weights = attn_weights + bias_chunk\n\t            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n\t            max_score = jnp.maximum(prev_max_score, max_score)\n\t            max_score = jax.lax.stop_gradient(max_score)\n\t            exp_weights = jnp.exp(attn_weights - max_score)\n", "            exp_values = jnp.einsum(\n\t                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n\t            )\n\t            correction = jnp.exp(prev_max_score - max_score)\n\t            numerator = numerator * correction + exp_values\n\t            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n\t            return Carry(numerator, denominator, max_score), None\n\t        init_carry = Carry(\n\t            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n\t            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n", "            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n\t        )\n\t        (numerator, denominator, max_score), _ = lax.scan(\n\t            summarize_chunk, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n\t        )\n\t        outputs = (numerator / denominator).astype(dtype)\n\t        return outputs\n\t    _, res = lax.scan(\n\t        lambda _, x: ((), _query_chunk_attention(x)),\n\t        (), xs=(query, jnp.arange(0, num_q))\n", "    )\n\t    res = rearrange(res, 'n b c h d -> b (n c) h d')\n\t    return res\n\tdef blockwise_compute_ffn(cell, inputs, chunk_size, deterministic, policy, prevent_cse):\n\t    inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=chunk_size)\n\t    inputs = rearrange(inputs, 'b n c d -> n b c d')\n\t    num_q, _, _, _ = inputs.shape\n\t    def ffn(cell, _, hidden_states):\n\t        outputs = cell.forward_ffn(hidden_states, deterministic=deterministic)\n\t        return _, outputs\n", "    ffn_remat = nn.remat(\n\t        ffn,\n\t        variables=\"params\",\n\t        rngs={\"params\" : False},\n\t        prevent_cse=prevent_cse,\n\t        policy=get_gradient_checkpoint_policy(policy),\n\t    )\n\t    _, res = nn.scan(\n\t        ffn_remat,\n\t        variable_broadcast=\"params\",\n", "        split_rngs={\"params\": False},\n\t        in_axes=0,\n\t        out_axes=0,\n\t        length=num_q,\n\t    )(cell, None, inputs)\n\t    res = rearrange(res, 'n b c d -> b (n c) d')\n\t    return res\n\tclass Blockwise_LM_Head(nn.Module):\n\t    vocab_size: int\n\t    chunk_size: int\n", "    policy: str = 'nothing_saveable'\n\t    dtype: jnp.dtype = jnp.float32\n\t    prevent_cse: bool = False\n\t    def setup(self):\n\t        self.lm_head = nn.Dense(\n\t            self.vocab_size,\n\t            dtype=self.dtype,\n\t            kernel_init=jax.nn.initializers.variance_scaling(\n\t                scale=1.0, mode='fan_in',\n\t                distribution='normal',\n", "            )\n\t        )\n\t    def __call__(self, inputs):\n\t        inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=self.chunk_size)\n\t        inputs = rearrange(inputs, 'b n c d -> n b c d')\n\t        num_q, _, _, _ = inputs.shape\n\t        def lm_head(cell, _, hidden_states):\n\t            outputs = cell(hidden_states)\n\t            return _, outputs\n\t        lm_head_remat = nn.remat(\n", "            lm_head,\n\t            variables=\"params\",\n\t            rngs={\"params\" : False},\n\t            prevent_cse=self.prevent_cse,\n\t            policy=get_gradient_checkpoint_policy(self.policy),\n\t        )\n\t        _, res = nn.scan(\n\t            lm_head_remat,\n\t            variable_broadcast=\"params\",\n\t            split_rngs={\"params\": False},\n", "            in_axes=0,\n\t            out_axes=0,\n\t            length=num_q,\n\t        )(self.lm_head, None, inputs)\n\t        res = rearrange(res, 'n b c d -> b (n c) d')\n\t        return res\n\tdef blockwise_cross_entropy(logits, tokens, valid=None,\n\t                            chunk_size=None, policy=None, prevent_cse=None):\n\t    if valid is None:\n\t        valid = jnp.ones(tokens.shape[:2])\n", "    valid = valid.astype(jnp.float32)\n\t    logits = jnp.reshape(logits, (-1, logits.shape[-1]))\n\t    tokens = jnp.reshape(tokens, (-1,))\n\t    valid = jnp.reshape(valid, (-1,))\n\t    def _cross_entropy_loss_and_accuracy(logits, tokens, valid):\n\t        valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n\t        token_log_prob = jnp.squeeze(\n\t            jnp.take_along_axis(\n\t                jax.nn.log_softmax(logits, axis=-1),\n\t                jnp.expand_dims(tokens, -1),\n", "                axis=-1,\n\t            ),\n\t            -1,\n\t        )\n\t        token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n\t        correct = jnp.where(\n\t            valid > 0.0,\n\t            jnp.argmax(logits, axis=-1) == tokens,\n\t            jnp.array(False)\n\t        )\n", "        return token_log_prob, correct, valid_text_length\n\t    @partial(jax.checkpoint, prevent_cse=prevent_cse,\n\t             policy=get_gradient_checkpoint_policy(policy))\n\t    def _loss_and_accuracy(carry, args):\n\t        loss, accuracy, num = carry\n\t        logits, tokens, valid = args\n\t        token_log_prob, correct, valid_text_length = \\\n\t            _cross_entropy_loss_and_accuracy(logits, tokens, valid)\n\t        loss = loss + jnp.sum(token_log_prob, axis=-1) / valid_text_length\n\t        accuracy = accuracy + jnp.sum(correct, axis=-1) / valid_text_length\n", "        num = num + 1\n\t        return (loss, accuracy, num), None\n\t    num_chunk = logits.shape[0] // chunk_size\n\t    logits = rearrange(logits, '(n c) d -> n c d', c=chunk_size)\n\t    tokens = rearrange(tokens, '(n c) -> n c', c=chunk_size)\n\t    valid = rearrange(valid, '(n c) -> n c', c=chunk_size)\n\t    (loss, accuracy, num), _ = jax.lax.scan(\n\t        _loss_and_accuracy, (0.0, 0.0, 0), xs=(logits, tokens, valid),\n\t        length=num_chunk,\n\t    )\n", "    loss = - loss / num\n\t    accuracy = accuracy / num\n\t    return loss, accuracy\n\tif __name__ == '__main__':\n\t    with jax.profiler.trace('/tmp/prof/blockwise_parallel_simplified'):\n\t        class Model(nn.Module):\n\t            def setup(self):\n\t                self.blocks = [\n\t                    AttentionBlock(\n\t                        q_chunk_size=256,\n", "                        k_chunk_size=256,\n\t                        hidden_size=2048,\n\t                        num_heads=16,\n\t                        rotary_dim=128,\n\t                        intermediate_size=8192,\n\t                        layer_norm_epsilon=1e-5,\n\t                        activation_function=\"gelu\",\n\t                        resid_pdrop=0.0,\n\t                        max_position_embeddings=2048,\n\t                        dtype=jnp.float32,\n", "                        causal=True,\n\t                )\n\t                for _ in range(2)\n\t                ]\n\t            def __call__(self, hidden_states, attention_mask, position_ids):\n\t                for block in self.blocks:\n\t                    hidden_states = block(hidden_states, attention_mask, position_ids)\n\t                return hidden_states\n\t        hidden_states = jnp.zeros((2, 1024, 2048))\n\t        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n", "        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n\t        model = Model()\n\t        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n\t        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n\t        output = output.block_until_ready()\n"]}
