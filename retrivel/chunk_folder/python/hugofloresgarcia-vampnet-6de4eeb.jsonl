{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages\n\tfrom setuptools import setup\n\twith open(\"README.md\") as f:\n\t    long_description = f.read()\n\tsetup(\n\t    name=\"vampnet\",\n\t    version=\"0.0.1\",\n\t    classifiers=[\n\t        \"Intended Audience :: Developers\",\n\t        \"Natural Language :: English\",\n", "        \"Programming Language :: Python :: 3.7\",\n\t        \"Topic :: Artistic Software\",\n\t        \"Topic :: Multimedia\",\n\t        \"Topic :: Multimedia :: Sound/Audio\",\n\t        \"Topic :: Multimedia :: Sound/Audio :: Editors\",\n\t        \"Topic :: Software Development :: Libraries\",\n\t    ],\n\t    description=\"Generative Music Modeling.\",\n\t    long_description=long_description,\n\t    long_description_content_type=\"text/markdown\",\n", "    author=\"Hugo Flores GarcÃ­a, Prem Seetharaman\",\n\t    author_email=\"hfgacrcia@descript.com\",\n\t    url=\"https://github.com/hugofloresgarcia/vampnet\",\n\t    license=\"MIT\",\n\t    packages=find_packages(),\n\t    install_requires=[\n\t        \"torch\",\n\t        \"argbind>=0.3.2\",\n\t        \"numpy==1.23\",\n\t        \"wavebeat @ git+https://github.com/hugofloresgarcia/wavebeat\",\n", "        \"lac @ git+https://github.com/hugofloresgarcia/lac.git\",\n\t        \"descript-audiotools @ git+https://github.com/descriptinc/audiotools.git@0.7.2\",\n\t        \"gradio\", \n\t        \"loralib\",\n\t        \"torch_pitch_shift\",\n\t        \"madmom\",\n\t    ],\n\t)\n"]}
{"filename": "app.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Tuple\n\timport yaml\n\timport tempfile\n\timport uuid\n\tfrom dataclasses import dataclass, asdict\n\timport numpy as np\n\timport audiotools as at\n\timport argbind\n\timport gradio as gr\n", "from vampnet.interface import Interface\n\tfrom vampnet import mask as pmask\n\tInterface = argbind.bind(Interface)\n\t# AudioLoader = argbind.bind(at.data.datasets.AudioLoader)\n\tconf = argbind.parse_args()\n\tfrom torch_pitch_shift import pitch_shift, get_fast_shifts\n\tdef shift_pitch(signal, interval: int):\n\t    signal.samples = pitch_shift(\n\t        signal.samples, \n\t        shift=interval, \n", "        sample_rate=signal.sample_rate\n\t    )\n\t    return signal\n\tdef load_interface():\n\t    with argbind.scope(conf):\n\t        interface = Interface()\n\t        # loader = AudioLoader()\n\t        print(f\"interface device is {interface.device}\")\n\t        return interface\n\tinterface = load_interface()\n", "# dataset = at.data.datasets.AudioDataset(\n\t#     loader,\n\t#     sample_rate=interface.codec.sample_rate,\n\t#     duration=interface.coarse.chunk_size_s,\n\t#     n_examples=5000,\n\t#     without_replacement=True,\n\t# )\n\tOUT_DIR = Path(\"gradio-outputs\")\n\tOUT_DIR.mkdir(exist_ok=True, parents=True)\n\tdef load_audio(file):\n", "    print(file)\n\t    filepath = file.name\n\t    sig = at.AudioSignal.salient_excerpt(\n\t        filepath, \n\t        duration=interface.coarse.chunk_size_s\n\t    )\n\t    sig = interface.preprocess(sig)\n\t    out_dir = OUT_DIR / \"tmp\" / str(uuid.uuid4())\n\t    out_dir.mkdir(parents=True, exist_ok=True)\n\t    sig.write(out_dir / \"input.wav\")\n", "    return sig.path_to_file\n\tdef load_example_audio():\n\t    return \"./assets/example.wav\"\n\tdef _vamp(data, return_mask=False):\n\t    out_dir = OUT_DIR / str(uuid.uuid4())\n\t    out_dir.mkdir()\n\t    sig = at.AudioSignal(data[input_audio])\n\t    sig = interface.preprocess(sig)\n\t    if data[pitch_shift_amt] != 0:\n\t        sig = shift_pitch(sig, data[pitch_shift_amt])\n", "    z = interface.encode(sig)\n\t    ncc = data[n_conditioning_codebooks]\n\t    # build the mask\n\t    mask = pmask.linear_random(z, data[rand_mask_intensity])\n\t    mask = pmask.mask_and(\n\t        mask, pmask.inpaint(\n\t            z,\n\t            interface.s2t(data[prefix_s]),\n\t            interface.s2t(data[suffix_s])\n\t        )\n", "    )\n\t    mask = pmask.mask_and(\n\t        mask, pmask.periodic_mask(\n\t            z,\n\t            data[periodic_p],\n\t            data[periodic_w],\n\t            random_roll=True\n\t        )\n\t    )\n\t    if data[onset_mask_width] > 0:\n", "        mask = pmask.mask_or(\n\t            mask, pmask.onset_mask(sig, z, interface, width=data[onset_mask_width])\n\t        )\n\t    if data[beat_mask_width] > 0:\n\t        beat_mask = interface.make_beat_mask(\n\t            sig,\n\t            after_beat_s=(data[beat_mask_width]/1000), \n\t            mask_upbeats=not data[beat_mask_downbeats],\n\t        )\n\t        mask = pmask.mask_and(mask, beat_mask)\n", "    # these should be the last two mask ops\n\t    mask = pmask.dropout(mask, data[dropout])\n\t    mask = pmask.codebook_unmask(mask, ncc)\n\t    print(f\"dropout {data[dropout]}\")\n\t    print(f\"masktemp {data[masktemp]}\")\n\t    print(f\"sampletemp {data[sampletemp]}\")\n\t    print(f\"top_p {data[top_p]}\")\n\t    print(f\"prefix_s {data[prefix_s]}\")\n\t    print(f\"suffix_s {data[suffix_s]}\")\n\t    print(f\"rand_mask_intensity {data[rand_mask_intensity]}\")\n", "    print(f\"num_steps {data[num_steps]}\")\n\t    print(f\"periodic_p {data[periodic_p]}\")\n\t    print(f\"periodic_w {data[periodic_w]}\")\n\t    print(f\"n_conditioning_codebooks {data[n_conditioning_codebooks]}\")\n\t    print(f\"use_coarse2fine {data[use_coarse2fine]}\")\n\t    print(f\"onset_mask_width {data[onset_mask_width]}\")\n\t    print(f\"beat_mask_width {data[beat_mask_width]}\")\n\t    print(f\"beat_mask_downbeats {data[beat_mask_downbeats]}\")\n\t    print(f\"stretch_factor {data[stretch_factor]}\")\n\t    print(f\"seed {data[seed]}\")\n", "    print(f\"pitch_shift_amt {data[pitch_shift_amt]}\")\n\t    print(f\"sample_cutoff {data[sample_cutoff]}\")\n\t    _top_p = data[top_p] if data[top_p] > 0 else None\n\t    # save the mask as a txt file\n\t    np.savetxt(out_dir / \"mask.txt\", mask[:,0,:].long().cpu().numpy())\n\t    _seed = data[seed] if data[seed] > 0 else None\n\t    zv, mask_z = interface.coarse_vamp(\n\t        z, \n\t        mask=mask,\n\t        sampling_steps=data[num_steps],\n", "        mask_temperature=data[masktemp]*10,\n\t        sampling_temperature=data[sampletemp],\n\t        return_mask=True, \n\t        typical_filtering=data[typical_filtering], \n\t        typical_mass=data[typical_mass], \n\t        typical_min_tokens=data[typical_min_tokens], \n\t        top_p=_top_p,\n\t        gen_fn=interface.coarse.generate,\n\t        seed=_seed,\n\t        sample_cutoff=data[sample_cutoff],\n", "    )\n\t    if use_coarse2fine: \n\t        zv = interface.coarse_to_fine(\n\t            zv, \n\t            mask_temperature=data[masktemp]*10, \n\t            sampling_temperature=data[sampletemp],\n\t            mask=mask,\n\t            sampling_steps=data[num_steps],\n\t            sample_cutoff=data[sample_cutoff], \n\t            seed=_seed,\n", "        )\n\t    sig = interface.to_signal(zv).cpu()\n\t    print(\"done\")\n\t    sig.write(out_dir / \"output.wav\")\n\t    if return_mask:\n\t        mask = interface.to_signal(mask_z).cpu()\n\t        mask.write(out_dir / \"mask.wav\")\n\t        return sig.path_to_file, mask.path_to_file\n\t    else:\n\t        return sig.path_to_file\n", "def vamp(data):\n\t    return _vamp(data, return_mask=True)\n\tdef api_vamp(data):\n\t    return _vamp(data, return_mask=False)\n\tdef save_vamp(data):\n\t    out_dir = OUT_DIR / \"saved\" / str(uuid.uuid4())\n\t    out_dir.mkdir(parents=True, exist_ok=True)\n\t    sig_in = at.AudioSignal(data[input_audio])\n\t    sig_out = at.AudioSignal(data[output_audio])\n\t    sig_in.write(out_dir / \"input.wav\")\n", "    sig_out.write(out_dir / \"output.wav\")\n\t    _data = {\n\t        \"masktemp\": data[masktemp],\n\t        \"sampletemp\": data[sampletemp],\n\t        \"top_p\": data[top_p],\n\t        \"prefix_s\": data[prefix_s],\n\t        \"suffix_s\": data[suffix_s],\n\t        \"rand_mask_intensity\": data[rand_mask_intensity],\n\t        \"num_steps\": data[num_steps],\n\t        \"notes\": data[notes_text],\n", "        \"periodic_period\": data[periodic_p],\n\t        \"periodic_width\": data[periodic_w],\n\t        \"n_conditioning_codebooks\": data[n_conditioning_codebooks], \n\t        \"use_coarse2fine\": data[use_coarse2fine],\n\t        \"stretch_factor\": data[stretch_factor],\n\t        \"seed\": data[seed],\n\t        \"samplecutoff\": data[sample_cutoff],\n\t    }\n\t    # save with yaml\n\t    with open(out_dir / \"data.yaml\", \"w\") as f:\n", "        yaml.dump(_data, f)\n\t    import zipfile\n\t    zip_path = out_dir.with_suffix(\".zip\")\n\t    with zipfile.ZipFile(zip_path, \"w\") as zf:\n\t        for file in out_dir.iterdir():\n\t            zf.write(file, file.name)\n\t    return f\"saved! your save code is {out_dir.stem}\", zip_path\n\twith gr.Blocks() as demo:\n\t    with gr.Row():\n\t        with gr.Column():\n", "            gr.Markdown(\"# VampNet Audio Vamping\")\n\t            gr.Markdown(\"\"\"## Description:\n\t            This is a demo of the VampNet, a generative audio model that transforms the input audio based on the chosen settings. \n\t            You can control the extent and nature of variation with a set of manual controls and presets. \n\t            Use this interface to experiment with different mask settings and explore the audio outputs.\n\t            \"\"\")\n\t            gr.Markdown(\"\"\"\n\t            ## Instructions:\n\t            1. You can start by uploading some audio, or by loading the example audio. \n\t            2. Choose a preset for the vamp operation, or manually adjust the controls to customize the mask settings. \n", "            3. Click the \"generate (vamp)!!!\" button to apply the vamp operation. Listen to the output audio.\n\t            4. Optionally, you can add some notes and save the result. \n\t            5. You can also use the output as the new input and continue experimenting!\n\t            \"\"\")\n\t    with gr.Row():\n\t        with gr.Column():\n\t            manual_audio_upload = gr.File(\n\t                label=f\"upload some audio (will be randomly trimmed to max of {interface.coarse.chunk_size_s:.2f}s)\",\n\t                file_types=[\"audio\"]\n\t            )\n", "            load_example_audio_button = gr.Button(\"or load example audio\")\n\t            input_audio = gr.Audio(\n\t                label=\"input audio\",\n\t                interactive=False, \n\t                type=\"filepath\",\n\t            )\n\t            audio_mask = gr.Audio(\n\t                label=\"audio mask (listen to this to hear the mask hints)\",\n\t                interactive=False, \n\t                type=\"filepath\",\n", "            )\n\t            # connect widgets\n\t            load_example_audio_button.click(\n\t                fn=load_example_audio,\n\t                inputs=[],\n\t                outputs=[ input_audio]\n\t            )\n\t            manual_audio_upload.change(\n\t                fn=load_audio,\n\t                inputs=[manual_audio_upload],\n", "                outputs=[ input_audio]\n\t            )\n\t        # mask settings\n\t        with gr.Column():\n\t            presets = {\n\t                    \"unconditional\": {\n\t                        \"periodic_p\": 0,\n\t                        \"onset_mask_width\": 0,\n\t                        \"beat_mask_width\": 0,\n\t                        \"beat_mask_downbeats\": False,\n", "                    }, \n\t                    \"slight periodic variation\": {\n\t                        \"periodic_p\": 5,\n\t                        \"onset_mask_width\": 5,\n\t                        \"beat_mask_width\": 0,\n\t                        \"beat_mask_downbeats\": False,\n\t                    },\n\t                    \"moderate periodic variation\": {\n\t                        \"periodic_p\": 13,\n\t                        \"onset_mask_width\": 5,\n", "                        \"beat_mask_width\": 0,\n\t                        \"beat_mask_downbeats\": False,\n\t                    },\n\t                    \"strong periodic variation\": {\n\t                        \"periodic_p\": 17,\n\t                        \"onset_mask_width\": 5,\n\t                        \"beat_mask_width\": 0,\n\t                        \"beat_mask_downbeats\": False,\n\t                    },\n\t                    \"very strong periodic variation\": {\n", "                        \"periodic_p\": 21,\n\t                        \"onset_mask_width\": 5,\n\t                        \"beat_mask_width\": 0,\n\t                        \"beat_mask_downbeats\": False,\n\t                    },\n\t                    \"beat-driven variation\": {\n\t                        \"periodic_p\": 0,\n\t                        \"onset_mask_width\": 0,\n\t                        \"beat_mask_width\": 50,\n\t                        \"beat_mask_downbeats\": False,\n", "                    },\n\t                    \"beat-driven variation (downbeats only)\": {\n\t                        \"periodic_p\": 0,\n\t                        \"onset_mask_width\": 0,\n\t                        \"beat_mask_width\": 50,\n\t                        \"beat_mask_downbeats\": True,\n\t                    },\n\t                    \"beat-driven variation (downbeats only, strong)\": {\n\t                        \"periodic_p\": 0,\n\t                        \"onset_mask_width\": 0,\n", "                        \"beat_mask_width\": 20,\n\t                        \"beat_mask_downbeats\": True,\n\t                    },\n\t                }\n\t            preset = gr.Dropdown(\n\t                label=\"preset\", \n\t                choices=list(presets.keys()),\n\t                value=\"strong periodic variation\",\n\t            )\n\t            load_preset_button = gr.Button(\"load_preset\")\n", "            with gr.Accordion(\"manual controls\", open=True):\n\t                periodic_p = gr.Slider(\n\t                    label=\"periodic prompt  (0 - unconditional, 2 - lots of hints, 8 - a couple of hints, 16 - occasional hint, 32 - very occasional hint, etc)\",\n\t                    minimum=0,\n\t                    maximum=128, \n\t                    step=1,\n\t                    value=3, \n\t                )\n\t                onset_mask_width = gr.Slider(\n\t                    label=\"onset mask width (multiplies with the periodic mask, 1 step ~= 10milliseconds) \",\n", "                    minimum=0,\n\t                    maximum=100,\n\t                    step=1,\n\t                    value=5,\n\t                )\n\t                beat_mask_width = gr.Slider(\n\t                    label=\"beat mask width (in milliseconds)\",\n\t                    minimum=0,\n\t                    maximum=200,\n\t                    value=0,\n", "                )\n\t                beat_mask_downbeats = gr.Checkbox(\n\t                    label=\"beat mask downbeats only?\", \n\t                    value=False\n\t                )\n\t                with gr.Accordion(\"extras \", open=False):\n\t                    pitch_shift_amt = gr.Slider(\n\t                        label=\"pitch shift amount (semitones)\",\n\t                        minimum=-12,\n\t                        maximum=12,\n", "                        step=1,\n\t                        value=0,\n\t                    )\n\t                    rand_mask_intensity = gr.Slider(\n\t                        label=\"random mask intensity. (If this is less than 1, scatters prompts throughout the audio, should be between 0.9 and 1.0)\",\n\t                        minimum=0.0,\n\t                        maximum=1.0,\n\t                        value=1.0\n\t                    )\n\t                    periodic_w = gr.Slider(\n", "                        label=\"periodic prompt width (steps, 1 step ~= 10milliseconds)\",\n\t                        minimum=1,\n\t                        maximum=20,\n\t                        step=1,\n\t                        value=1,\n\t                    )\n\t                    n_conditioning_codebooks = gr.Number(\n\t                        label=\"number of conditioning codebooks. probably 0\", \n\t                        value=0,\n\t                        precision=0,\n", "                    )\n\t                    stretch_factor = gr.Slider(\n\t                        label=\"time stretch factor\",\n\t                        minimum=0,\n\t                        maximum=64, \n\t                        step=1,\n\t                        value=1, \n\t                    )\n\t            preset_outputs = {\n\t                periodic_p, \n", "                onset_mask_width, \n\t                beat_mask_width,\n\t                beat_mask_downbeats,\n\t            }\n\t            def load_preset(_preset):\n\t                return tuple(presets[_preset].values())\n\t            load_preset_button.click(\n\t                fn=load_preset,\n\t                inputs=[preset],\n\t                outputs=preset_outputs\n", "            )\n\t            with gr.Accordion(\"prefix/suffix prompts\", open=False):\n\t                prefix_s = gr.Slider(\n\t                    label=\"prefix hint length (seconds)\",\n\t                    minimum=0.0,\n\t                    maximum=10.0,\n\t                    value=0.0\n\t                )\n\t                suffix_s = gr.Slider(\n\t                    label=\"suffix hint length (seconds)\",\n", "                    minimum=0.0,\n\t                    maximum=10.0,\n\t                    value=0.0\n\t                )\n\t            masktemp = gr.Slider(\n\t                label=\"mask temperature\",\n\t                minimum=0.0,\n\t                maximum=100.0,\n\t                value=1.5\n\t            )\n", "            sampletemp = gr.Slider(\n\t                label=\"sample temperature\",\n\t                minimum=0.1,\n\t                maximum=10.0,\n\t                value=1.0, \n\t                step=0.001\n\t            )\n\t            with gr.Accordion(\"sampling settings\", open=False):\n\t                top_p = gr.Slider(\n\t                    label=\"top p (0.0 = off)\",\n", "                    minimum=0.0,\n\t                    maximum=1.0,\n\t                    value=0.0\n\t                )\n\t                typical_filtering = gr.Checkbox(\n\t                    label=\"typical filtering \",\n\t                    value=False\n\t                )\n\t                typical_mass = gr.Slider( \n\t                    label=\"typical mass (should probably stay between 0.1 and 0.5)\",\n", "                    minimum=0.01,\n\t                    maximum=0.99,\n\t                    value=0.15\n\t                )\n\t                typical_min_tokens = gr.Slider(\n\t                    label=\"typical min tokens (should probably stay between 1 and 256)\",\n\t                    minimum=1,\n\t                    maximum=256,\n\t                    step=1,\n\t                    value=64\n", "                )\n\t                sample_cutoff = gr.Slider(\n\t                    label=\"sample cutoff\",\n\t                    minimum=0.0,\n\t                    maximum=1.0,\n\t                    value=0.5, \n\t                    step=0.01\n\t                )\n\t            use_coarse2fine = gr.Checkbox(\n\t                label=\"use coarse2fine\",\n", "                value=True, \n\t                visible=False\n\t            )\n\t            num_steps = gr.Slider(\n\t                label=\"number of steps (should normally be between 12 and 36)\",\n\t                minimum=1,\n\t                maximum=128,\n\t                step=1,\n\t                value=36\n\t            )\n", "            dropout = gr.Slider(\n\t                label=\"mask dropout\",\n\t                minimum=0.0,\n\t                maximum=1.0,\n\t                step=0.01,\n\t                value=0.0\n\t            )\n\t            seed = gr.Number(\n\t                label=\"seed (0 for random)\",\n\t                value=0,\n", "                precision=0,\n\t            )\n\t        # mask settings\n\t        with gr.Column():\n\t            # lora_choice = gr.Dropdown(\n\t            #     label=\"lora choice\", \n\t            #     choices=list(loras.keys()),\n\t            #     value=LORA_NONE, \n\t            #     visible=False\n\t            # )\n", "            vamp_button = gr.Button(\"generate (vamp)!!!\")\n\t            output_audio = gr.Audio(\n\t                label=\"output audio\",\n\t                interactive=False,\n\t                type=\"filepath\"\n\t            )\n\t            notes_text = gr.Textbox(\n\t                label=\"type any notes about the generated audio here\", \n\t                value=\"\",\n\t                interactive=True\n", "            )\n\t            save_button = gr.Button(\"save vamp\")\n\t            download_file = gr.File(\n\t                label=\"vamp to download will appear here\",\n\t                interactive=False\n\t            )\n\t            use_as_input_button = gr.Button(\"use output as input\")\n\t            thank_you = gr.Markdown(\"\")\n\t    _inputs = {\n\t            input_audio, \n", "            num_steps,\n\t            masktemp,\n\t            sampletemp,\n\t            top_p,\n\t            prefix_s, suffix_s, \n\t            rand_mask_intensity, \n\t            periodic_p, periodic_w,\n\t            n_conditioning_codebooks, \n\t            dropout,\n\t            use_coarse2fine, \n", "            stretch_factor, \n\t            onset_mask_width, \n\t            typical_filtering,\n\t            typical_mass,\n\t            typical_min_tokens,\n\t            beat_mask_width,\n\t            beat_mask_downbeats,\n\t            seed, \n\t            # lora_choice,\n\t            pitch_shift_amt, \n", "            sample_cutoff\n\t        }\n\t    # connect widgets\n\t    vamp_button.click(\n\t        fn=vamp,\n\t        inputs=_inputs,\n\t        outputs=[output_audio, audio_mask], \n\t    )\n\t    api_vamp_button = gr.Button(\"api vamp\", visible=False)\n\t    api_vamp_button.click(\n", "        fn=api_vamp,\n\t        inputs=_inputs, \n\t        outputs=[output_audio], \n\t        api_name=\"vamp\"\n\t    )\n\t    use_as_input_button.click(\n\t        fn=lambda x: x,\n\t        inputs=[output_audio],\n\t        outputs=[input_audio]\n\t    )\n", "    save_button.click(\n\t        fn=save_vamp,\n\t        inputs=_inputs | {notes_text, output_audio},\n\t        outputs=[thank_you, download_file]\n\t    )\n\tdemo.launch(share=True, enable_queue=True, debug=True)\n"]}
{"filename": "scripts/exp/experiment.py", "chunked_list": ["from pathlib import Path\n\timport random\n\tfrom typing import List\n\timport tempfile\n\timport subprocess\n\timport argbind\n\tfrom tqdm import tqdm\n\timport torch\n\tfrom vampnet.interface import Interface\n\tfrom vampnet import mask as pmask\n", "import audiotools as at\n\tInterface: Interface = argbind.bind(Interface)\n\tdef calculate_bitrate(\n\t        interface, num_codebooks, \n\t        downsample_factor\n\t    ):\n\t    bit_width = 10\n\t    sr = interface.codec.sample_rate\n\t    hop = interface.codec.hop_size\n\t    rate = (sr / hop) * ((bit_width * num_codebooks) / downsample_factor)\n", "    return rate\n\tdef baseline(sig, interface):\n\t    return interface.preprocess(sig)\n\tdef reconstructed(sig, interface):\n\t    return interface.to_signal(\n\t        interface.encode(sig)\n\t    )\n\tdef coarse2fine(sig, interface):\n\t    z = interface.encode(sig)\n\t    z = z[:, :interface.c2f.n_conditioning_codebooks, :]\n", "    z = interface.coarse_to_fine(z)\n\t    return interface.to_signal(z)\n\tclass CoarseCond:\n\t    def __init__(self, num_conditioning_codebooks, downsample_factor):\n\t        self.num_conditioning_codebooks = num_conditioning_codebooks\n\t        self.downsample_factor = downsample_factor\n\t    def __call__(self, sig, interface):\n\t        z = interface.encode(sig)\n\t        mask = pmask.full_mask(z)\n\t        mask = pmask.codebook_unmask(mask, self.num_conditioning_codebooks)\n", "        mask = pmask.periodic_mask(mask, self.downsample_factor)\n\t        zv = interface.coarse_vamp(z, mask)\n\t        zv = interface.coarse_to_fine(zv)\n\t        return interface.to_signal(zv)\n\tdef opus(sig, interface, bitrate=128):\n\t    sig = interface.preprocess(sig)\n\t    with tempfile.NamedTemporaryFile(suffix=\".wav\") as f:\n\t        sig.write(f.name)\n\t        opus_name = Path(f.name).with_suffix(\".opus\")\n\t        # convert to opus\n", "        cmd = [\n\t            \"ffmpeg\", \"-y\", \"-i\", f.name, \n\t            \"-c:a\", \"libopus\", \n\t            \"-b:a\", f\"{bitrate}\", \n\t           opus_name\n\t        ]\n\t        subprocess.run(cmd, check=True)\n\t        # convert back to wav\n\t        output_name = Path(f\"{f.name}-opus\").with_suffix(\".wav\")\n\t        cmd = [\n", "            \"ffmpeg\", \"-y\", \"-i\", opus_name, \n\t            output_name\n\t        ]\n\t        subprocess.run(cmd, check=True)\n\t        sig = at.AudioSignal(\n\t            output_name, \n\t            sample_rate=sig.sample_rate\n\t        )\n\t    return sig\n\tdef mask_ratio_1_step(ratio=1.0):\n", "    def wrapper(sig, interface):\n\t        z = interface.encode(sig)\n\t        mask = pmask.linear_random(z, ratio)\n\t        zv = interface.coarse_vamp(\n\t            z, \n\t            mask,\n\t            sampling_steps=1, \n\t        )\n\t        return interface.to_signal(zv)\n\t    return wrapper\n", "def num_sampling_steps(num_steps=1):\n\t    def wrapper(sig, interface: Interface):\n\t        z = interface.encode(sig)\n\t        mask = pmask.periodic_mask(z, 16)\n\t        zv = interface.coarse_vamp(\n\t            z, \n\t            mask,\n\t            sampling_steps=num_steps, \n\t        )\n\t        zv = interface.coarse_to_fine(zv)\n", "        return interface.to_signal(zv)\n\t    return wrapper\n\tdef beat_mask(ctx_time):\n\t    def wrapper(sig, interface):\n\t        beat_mask = interface.make_beat_mask(\n\t            sig,\n\t            before_beat_s=ctx_time/2,\n\t            after_beat_s=ctx_time/2,\n\t            invert=True\n\t        )\n", "        z = interface.encode(sig)\n\t        zv = interface.coarse_vamp(\n\t            z, beat_mask\n\t        )\n\t        zv = interface.coarse_to_fine(zv)\n\t        return interface.to_signal(zv)\n\t    return wrapper\n\tdef inpaint(ctx_time):\n\t    def wrapper(sig, interface: Interface):\n\t        z = interface.encode(sig)\n", "        mask = pmask.inpaint(z, interface.s2t(ctx_time), interface.s2t(ctx_time))\n\t        zv = interface.coarse_vamp(z, mask)\n\t        zv = interface.coarse_to_fine(zv)\n\t        return interface.to_signal(zv)\n\t    return wrapper\n\tdef token_noise(noise_amt):\n\t    def wrapper(sig, interface: Interface):\n\t        z = interface.encode(sig)\n\t        mask = pmask.random(z, noise_amt)\n\t        z = torch.where(\n", "            mask, \n\t            torch.randint_like(z, 0, interface.coarse.vocab_size), \n\t            z\n\t        )\n\t        return interface.to_signal(z)\n\t    return wrapper\n\tEXP_REGISTRY = {}\n\tEXP_REGISTRY[\"gen-compression\"] = {\n\t    \"baseline\": baseline,\n\t    \"reconstructed\": reconstructed,\n", "    \"coarse2fine\": coarse2fine,\n\t    **{\n\t        f\"{n}_codebooks_downsampled_{x}x\": CoarseCond(num_conditioning_codebooks=n, downsample_factor=x)\n\t            for (n, x) in (\n\t                (1, 1), # 1 codebook, no downsampling\n\t                (4, 4), # 4 codebooks, downsampled 4x\n\t                (4, 16), # 4 codebooks, downsampled 16x\n\t                (4, 32), # 4 codebooks, downsampled 16x\n\t            )\n\t    }, \n", "    **{\n\t        f\"token_noise_{x}\": mask_ratio_1_step(ratio=x)\n\t            for x in [0.25, 0.5, 0.75]\n\t    },\n\t}\n\tEXP_REGISTRY[\"sampling-steps\"] = {\n\t    # \"codec\": reconstructed,\n\t    **{f\"steps_{n}\": num_sampling_steps(n)  for n in [1, 4, 12, 36, 64, 72]},\n\t}\n\tEXP_REGISTRY[\"musical-sampling\"] = {\n", "    **{f\"beat_mask_{t}\": beat_mask(t) for t in [0.075]}, \n\t    **{f\"inpaint_{t}\": inpaint(t) for t in [0.5, 1.0,]}, # multiply these by 2 (they go left and right)\n\t}\n\t@argbind.bind(without_prefix=True)\n\tdef main(\n\t        sources=[\n\t            \"/media/CHONK/hugo/spotdl/val\",\n\t        ], \n\t        output_dir: str = \"./samples\",\n\t        max_excerpts: int = 2000,\n", "        exp_type: str = \"gen-compression\", \n\t        seed: int = 0,\n\t        ext: str = [\".mp3\"],\n\t    ):\n\t    at.util.seed(seed)\n\t    interface = Interface()\n\t    output_dir = Path(output_dir) \n\t    output_dir.mkdir(exist_ok=True, parents=True)\n\t    from audiotools.data.datasets import AudioLoader, AudioDataset\n\t    loader = AudioLoader(sources=sources, shuffle_state=seed, ext=ext)\n", "    dataset = AudioDataset(loader, \n\t        sample_rate=interface.codec.sample_rate, \n\t        duration=interface.coarse.chunk_size_s, \n\t        n_examples=max_excerpts, \n\t        without_replacement=True,\n\t    )\n\t    if exp_type in EXP_REGISTRY:\n\t        SAMPLE_CONDS = EXP_REGISTRY[exp_type]\n\t    else:\n\t        raise ValueError(f\"Unknown exp_type {exp_type}\")\n", "    indices = list(range(max_excerpts))\n\t    random.shuffle(indices)\n\t    for i in tqdm(indices):\n\t        # if all our files are already there, skip\n\t        done = []\n\t        for name in SAMPLE_CONDS:\n\t            o_dir = Path(output_dir) / name\n\t            done.append((o_dir / f\"{i}.wav\").exists())\n\t        if all(done):\n\t            continue\n", "        sig = dataset[i][\"signal\"]\n\t        results = {\n\t            name: cond(sig, interface).cpu()\n\t            for name, cond in SAMPLE_CONDS.items()\n\t        }\n\t        for name, sig in results.items():\n\t            o_dir = Path(output_dir) / name\n\t            o_dir.mkdir(exist_ok=True, parents=True)\n\t            sig.write(o_dir / f\"{i}.wav\")\n\tif __name__ == \"__main__\":\n", "    args = argbind.parse_args()\n\t    with argbind.scope(args):\n\t        main()\n"]}
{"filename": "scripts/exp/fine_tune.py", "chunked_list": ["import argbind\n\tfrom pathlib import Path\n\timport yaml\n\tfrom typing import List\n\t\"\"\"example output: (yaml)\n\t\"\"\"\n\t@argbind.bind(without_prefix=True, positional=True)\n\tdef fine_tune(audio_files_or_folders: List[str], name: str):\n\t    conf_dir = Path(\"conf\")\n\t    assert conf_dir.exists(), \"conf directory not found. are you in the vampnet directory?\"\n", "    conf_dir = conf_dir / \"generated\"\n\t    conf_dir.mkdir(exist_ok=True)\n\t    finetune_dir = conf_dir / name\n\t    finetune_dir.mkdir(exist_ok=True)\n\t    finetune_c2f_conf = {\n\t        \"$include\": [\"conf/lora/lora.yml\"],\n\t        \"fine_tune\": True,\n\t        \"train/AudioLoader.sources\": audio_files_or_folders,\n\t        \"val/AudioLoader.sources\": audio_files_or_folders,\n\t        \"VampNet.n_codebooks\": 14,\n", "        \"VampNet.n_conditioning_codebooks\": 4,\n\t        \"VampNet.embedding_dim\": 1280,\n\t        \"VampNet.n_layers\": 16,\n\t        \"VampNet.n_heads\": 20,\n\t        \"AudioDataset.duration\": 3.0,\n\t        \"AudioDataset.loudness_cutoff\": -40.0,\n\t        \"save_path\": f\"./runs/{name}/c2f\",\n\t        \"fine_tune_checkpoint\": \"./models/vampnet/c2f.pth\"\n\t    }\n\t    finetune_coarse_conf = {\n", "        \"$include\": [\"conf/lora/lora.yml\"],\n\t        \"fine_tune\": True,\n\t        \"train/AudioLoader.sources\": audio_files_or_folders,\n\t        \"val/AudioLoader.sources\": audio_files_or_folders,\n\t        \"save_path\": f\"./runs/{name}/coarse\",\n\t        \"fine_tune_checkpoint\": \"./models/vampnet/coarse.pth\"\n\t    }\n\t    interface_conf = {\n\t        \"Interface.coarse_ckpt\": f\"./runs/{name}/coarse/latest/vampnet/weights.pth\",\n\t        \"Interface.coarse2fine_ckpt\": f\"./runs/{name}/c2f/latest/vampnet/weights.pth\",\n", "        \"Interface.wavebeat_ckpt\": \"./models/wavebeat.pth\",\n\t        \"Interface.codec_ckpt\": \"./models/vampnet/codec.pth\",\n\t        \"AudioLoader.sources\": [audio_files_or_folders],\n\t    }\n\t    # save the confs\n\t    with open(finetune_dir / \"c2f.yml\", \"w\") as f:\n\t        yaml.dump(finetune_c2f_conf, f)\n\t    with open(finetune_dir / \"coarse.yml\", \"w\") as f:\n\t        yaml.dump(finetune_coarse_conf, f)\n\t    with open(finetune_dir / \"interface.yml\", \"w\") as f: \n", "        yaml.dump(interface_conf, f)\n\t    print(f\"generated confs in {finetune_dir}. run training jobs with `python scripts/exp/train.py --args.load {finetune_dir}/<c2f/coarse>.yml` \")\n\tif __name__ == \"__main__\":\n\t    args = argbind.parse_args()\n\t    with argbind.scope(args):\n\t        fine_tune()\n"]}
{"filename": "scripts/exp/train.py", "chunked_list": ["import os\n\timport sys\n\timport warnings\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\tfrom dataclasses import dataclass\n\timport argbind\n\timport audiotools as at\n\timport torch\n\timport torch.nn as nn\n", "from audiotools import AudioSignal\n\tfrom audiotools.data import transforms\n\tfrom einops import rearrange\n\tfrom rich import pretty\n\tfrom rich.traceback import install\n\tfrom torch.utils.tensorboard import SummaryWriter\n\timport vampnet\n\tfrom vampnet.modules.transformer import VampNet\n\tfrom vampnet.util import codebook_unflatten, codebook_flatten\n\tfrom vampnet import mask as pmask\n", "# from dac.model.dac import DAC\n\tfrom lac.model.lac import LAC as DAC\n\tfrom audiotools.ml.decorators import (\n\t    timer, Tracker, when\n\t)\n\timport loralib as lora\n\timport torch._dynamo\n\ttorch._dynamo.config.verbose=True\n\t# Enable cudnn autotuner to speed up training\n\t# (can be altered by the funcs.seed function)\n", "torch.backends.cudnn.benchmark = bool(int(os.getenv(\"CUDNN_BENCHMARK\", 1)))\n\t# Uncomment to trade memory for speed.\n\t# Install to make things look nice\n\twarnings.filterwarnings(\"ignore\", category=UserWarning)\n\tpretty.install()\n\tinstall()\n\t# optim\n\tAccelerator = argbind.bind(at.ml.Accelerator, without_prefix=True)\n\tCrossEntropyLoss = argbind.bind(nn.CrossEntropyLoss)\n\tAdamW = argbind.bind(torch.optim.AdamW)\n", "NoamScheduler = argbind.bind(vampnet.scheduler.NoamScheduler)\n\t# transforms\n\tfilter_fn = lambda fn: hasattr(fn, \"transform\") and fn.__qualname__ not in [\n\t    \"BaseTransform\",\n\t    \"Compose\",\n\t    \"Choose\",\n\t]\n\ttfm = argbind.bind_module(transforms, \"train\", \"val\", filter_fn=filter_fn)\n\t# model\n\tVampNet = argbind.bind(VampNet)\n", "# data\n\tAudioLoader = argbind.bind(at.datasets.AudioLoader)\n\tAudioDataset = argbind.bind(at.datasets.AudioDataset, \"train\", \"val\")\n\tIGNORE_INDEX = -100\n\t@argbind.bind(\"train\", \"val\", without_prefix=True)\n\tdef build_transform():\n\t    transform = transforms.Compose(\n\t        tfm.VolumeNorm((\"const\", -24)),\n\t        # tfm.PitchShift(),\n\t        tfm.RescaleAudio(),\n", "    )\n\t    return transform\n\t@torch.no_grad()\n\tdef apply_transform(transform_fn, batch):\n\t    sig: AudioSignal = batch[\"signal\"]\n\t    kwargs = batch[\"transform_args\"]\n\t    sig: AudioSignal = transform_fn(sig.clone(), **kwargs)\n\t    return sig\n\tdef build_datasets(args, sample_rate: int):\n\t    with argbind.scope(args, \"train\"):\n", "        train_data = AudioDataset(\n\t            AudioLoader(), sample_rate, transform=build_transform()\n\t        )\n\t    with argbind.scope(args, \"val\"):\n\t        val_data = AudioDataset(AudioLoader(), sample_rate, transform=build_transform())\n\t    return train_data, val_data\n\tdef rand_float(shape, low, high, rng):\n\t    return rng.draw(shape)[:, 0] * (high - low) + low\n\tdef flip_coin(shape, p, rng):\n\t    return rng.draw(shape)[:, 0] < p\n", "def num_params_hook(o, p):\n\t    return o + f\" {p/1e6:<.3f}M params.\"\n\tdef add_num_params_repr_hook(model):\n\t    import numpy as np\n\t    from functools import partial\n\t    for n, m in model.named_modules():\n\t        o = m.extra_repr()\n\t        p = sum([np.prod(p.size()) for p in m.parameters()])\n\t        setattr(m, \"extra_repr\", partial(num_params_hook, o=o, p=p))\n\tdef accuracy(\n", "    preds: torch.Tensor,\n\t    target: torch.Tensor,\n\t    top_k: int = 1,\n\t    ignore_index: Optional[int] = None,\n\t) -> torch.Tensor:\n\t    # Flatten the predictions and targets to be of shape (batch_size * sequence_length, n_class)\n\t    preds = rearrange(preds, \"b p s -> (b s) p\")\n\t    target = rearrange(target, \"b s -> (b s)\")\n\t    # return torchmetrics.functional.accuracy(preds, target, task='multiclass', top_k=topk, num_classes=preds.shape[-1], ignore_index=ignore_index)\n\t    if ignore_index is not None:\n", "        # Create a mask for the ignored index\n\t        mask = target != ignore_index\n\t        # Apply the mask to the target and predictions\n\t        preds = preds[mask]\n\t        target = target[mask]\n\t    # Get the top-k predicted classes and their indices\n\t    _, pred_indices = torch.topk(preds, k=top_k, dim=-1)\n\t    # Determine if the true target is in the top-k predicted classes\n\t    correct = torch.sum(torch.eq(pred_indices, target.unsqueeze(1)), dim=1)\n\t    # Calculate the accuracy\n", "    accuracy = torch.mean(correct.float())\n\t    return accuracy\n\tdef _metrics(z_hat, r, target, flat_mask, output):\n\t    for r_range in [(0, 0.5), (0.5, 1.0)]:\n\t        unmasked_target = target.masked_fill(flat_mask.bool(), IGNORE_INDEX)\n\t        masked_target = target.masked_fill(~flat_mask.bool(), IGNORE_INDEX)\n\t        assert target.shape[0] == r.shape[0]\n\t        # grab the indices of the r values that are in the range\n\t        r_idx = (r >= r_range[0]) & (r < r_range[1])\n\t        # grab the target and z_hat values that are in the range\n", "        r_unmasked_target = unmasked_target[r_idx]\n\t        r_masked_target = masked_target[r_idx]\n\t        r_z_hat = z_hat[r_idx]\n\t        for topk in (1, 25):\n\t            s, e = r_range\n\t            tag = f\"accuracy-{s}-{e}/top{topk}\"\n\t            output[f\"{tag}/unmasked\"] = accuracy(\n\t                preds=r_z_hat,\n\t                target=r_unmasked_target,\n\t                ignore_index=IGNORE_INDEX,\n", "                top_k=topk,\n\t            )\n\t            output[f\"{tag}/masked\"] = accuracy(\n\t                preds=r_z_hat,\n\t                target=r_masked_target,\n\t                ignore_index=IGNORE_INDEX,\n\t                top_k=topk,\n\t            )\n\t@dataclass\n\tclass State:\n", "    model: VampNet\n\t    codec: DAC\n\t    optimizer: AdamW\n\t    scheduler: NoamScheduler\n\t    criterion: CrossEntropyLoss\n\t    grad_clip_val: float\n\t    rng: torch.quasirandom.SobolEngine\n\t    train_data: AudioDataset\n\t    val_data: AudioDataset\n\t    tracker: Tracker\n", "@timer()\n\tdef train_loop(state: State, batch: dict, accel: Accelerator):\n\t    state.model.train()\n\t    batch = at.util.prepare_batch(batch, accel.device)\n\t    signal = apply_transform(state.train_data.transform, batch)\n\t    output = {}\n\t    vn = accel.unwrap(state.model)\n\t    with accel.autocast():\n\t        with torch.inference_mode():\n\t            state.codec.to(accel.device)\n", "            z = state.codec.encode(signal.samples, signal.sample_rate)[\"codes\"]\n\t            z = z[:, : vn.n_codebooks, :]\n\t        n_batch = z.shape[0]\n\t        r = state.rng.draw(n_batch)[:, 0].to(accel.device)\n\t        mask = pmask.random(z, r)\n\t        mask = pmask.codebook_unmask(mask, vn.n_conditioning_codebooks)\n\t        z_mask, mask = pmask.apply_mask(z, mask, vn.mask_token)\n\t        z_mask_latent = vn.embedding.from_codes(z_mask, state.codec)\n\t        dtype = torch.bfloat16 if accel.amp else None\n\t        with accel.autocast(dtype=dtype):\n", "            z_hat = state.model(z_mask_latent)\n\t        target = codebook_flatten(\n\t            z[:, vn.n_conditioning_codebooks :, :],\n\t        )\n\t        flat_mask = codebook_flatten(\n\t            mask[:, vn.n_conditioning_codebooks :, :],\n\t        )\n\t        # replace target with ignore index for masked tokens\n\t        t_masked = target.masked_fill(~flat_mask.bool(), IGNORE_INDEX)\n\t        output[\"loss\"] = state.criterion(z_hat, t_masked)\n", "        _metrics(\n\t            r=r,\n\t            z_hat=z_hat,\n\t            target=target,\n\t            flat_mask=flat_mask,\n\t            output=output,\n\t        )\n\t    accel.backward(output[\"loss\"])\n\t    output[\"other/learning_rate\"] = state.optimizer.param_groups[0][\"lr\"]\n\t    output[\"other/batch_size\"] = z.shape[0]\n", "    accel.scaler.unscale_(state.optimizer)\n\t    output[\"other/grad_norm\"] = torch.nn.utils.clip_grad_norm_(\n\t        state.model.parameters(), state.grad_clip_val\n\t    )\n\t    accel.step(state.optimizer)\n\t    state.optimizer.zero_grad()\n\t    state.scheduler.step()\n\t    accel.update()\n\t    return {k: v for k, v in sorted(output.items())}\n\t@timer()\n", "@torch.no_grad()\n\tdef val_loop(state: State, batch: dict, accel: Accelerator):\n\t    state.model.eval()\n\t    state.codec.eval()\n\t    batch = at.util.prepare_batch(batch, accel.device)\n\t    signal = apply_transform(state.val_data.transform, batch)\n\t    vn = accel.unwrap(state.model)\n\t    z = state.codec.encode(signal.samples, signal.sample_rate)[\"codes\"]\n\t    z = z[:, : vn.n_codebooks, :]\n\t    n_batch = z.shape[0]\n", "    r = state.rng.draw(n_batch)[:, 0].to(accel.device)\n\t    mask = pmask.random(z, r)\n\t    mask = pmask.codebook_unmask(mask, vn.n_conditioning_codebooks)\n\t    z_mask, mask = pmask.apply_mask(z, mask, vn.mask_token)\n\t    z_mask_latent = vn.embedding.from_codes(z_mask, state.codec)\n\t    z_hat = state.model(z_mask_latent)\n\t    target = codebook_flatten(\n\t        z[:, vn.n_conditioning_codebooks :, :],\n\t    )\n\t    flat_mask = codebook_flatten(\n", "        mask[:, vn.n_conditioning_codebooks :, :]\n\t    )\n\t    output = {}\n\t    # replace target with ignore index for masked tokens\n\t    t_masked = target.masked_fill(~flat_mask.bool(), IGNORE_INDEX)\n\t    output[\"loss\"] = state.criterion(z_hat, t_masked)\n\t    _metrics(\n\t        r=r,\n\t        z_hat=z_hat,\n\t        target=target,\n", "        flat_mask=flat_mask,\n\t        output=output,\n\t    )\n\t    return output\n\tdef validate(state, val_dataloader, accel):\n\t    for batch in val_dataloader:\n\t        output = val_loop(state, batch, accel)\n\t    # Consolidate state dicts if using ZeroRedundancyOptimizer\n\t    if hasattr(state.optimizer, \"consolidate_state_dict\"):\n\t        state.optimizer.consolidate_state_dict()\n", "    return output\n\tdef checkpoint(state, save_iters, save_path, fine_tune):\n\t    if accel.local_rank != 0:\n\t        state.tracker.print(f\"ERROR:Skipping checkpoint on rank {accel.local_rank}\")\n\t        return\n\t    metadata = {\"logs\": dict(state.tracker.history)}\n\t    tags = [\"latest\"]\n\t    state.tracker.print(f\"Saving to {str(Path('.').absolute())}\")\n\t    if state.tracker.step in save_iters:\n\t        tags.append(f\"{state.tracker.step // 1000}k\")\n", "    if state.tracker.is_best(\"val\", \"loss\"):\n\t        state.tracker.print(f\"Best model so far\")\n\t        tags.append(\"best\")\n\t    if fine_tune:\n\t        for tag in tags: \n\t            # save the lora model \n\t            (Path(save_path) / tag).mkdir(parents=True, exist_ok=True)\n\t            torch.save(\n\t                lora.lora_state_dict(accel.unwrap(state.model)), \n\t                f\"{save_path}/{tag}/lora.pth\"\n", "            )\n\t    for tag in tags:\n\t        model_extra = {\n\t            \"optimizer.pth\": state.optimizer.state_dict(),\n\t            \"scheduler.pth\": state.scheduler.state_dict(),\n\t            \"tracker.pth\": state.tracker.state_dict(),\n\t            \"metadata.pth\": metadata,\n\t        }\n\t        accel.unwrap(state.model).metadata = metadata\n\t        accel.unwrap(state.model).save_to_folder(\n", "            f\"{save_path}/{tag}\", model_extra, package=False\n\t        )\n\tdef save_sampled(state, z, writer):\n\t    num_samples = z.shape[0]\n\t    for i in range(num_samples):\n\t        sampled = accel.unwrap(state.model).generate(\n\t            codec=state.codec,\n\t            time_steps=z.shape[-1],\n\t            start_tokens=z[i : i + 1],\n\t        )\n", "        sampled.cpu().write_audio_to_tb(\n\t            f\"sampled/{i}\",\n\t            writer,\n\t            step=state.tracker.step,\n\t            plot_fn=None,\n\t        )\n\tdef save_imputation(state, z, val_idx, writer):\n\t    n_prefix = int(z.shape[-1] * 0.25)\n\t    n_suffix = int(z.shape[-1] *  0.25)\n\t    vn = accel.unwrap(state.model)\n", "    mask = pmask.inpaint(z, n_prefix, n_suffix)\n\t    mask = pmask.codebook_unmask(mask, vn.n_conditioning_codebooks)\n\t    z_mask, mask = pmask.apply_mask(z, mask, vn.mask_token)\n\t    imputed_noisy = vn.to_signal(z_mask, state.codec)\n\t    imputed_true = vn.to_signal(z, state.codec)\n\t    imputed = []\n\t    for i in range(len(z)):\n\t        imputed.append(\n\t            vn.generate(\n\t                codec=state.codec,\n", "                time_steps=z.shape[-1],\n\t                start_tokens=z[i][None, ...],\n\t                mask=mask[i][None, ...],\n\t            )   \n\t        )   \n\t    imputed = AudioSignal.batch(imputed)\n\t    for i in range(len(val_idx)):\n\t        imputed_noisy[i].cpu().write_audio_to_tb(\n\t            f\"imputed_noisy/{i}\",\n\t            writer,\n", "            step=state.tracker.step,\n\t            plot_fn=None,\n\t        )\n\t        imputed[i].cpu().write_audio_to_tb(\n\t            f\"imputed/{i}\",\n\t            writer,\n\t            step=state.tracker.step,\n\t            plot_fn=None,\n\t        )\n\t        imputed_true[i].cpu().write_audio_to_tb(\n", "            f\"imputed_true/{i}\",\n\t            writer,\n\t            step=state.tracker.step,\n\t            plot_fn=None,\n\t        )\n\t@torch.no_grad()\n\tdef save_samples(state: State, val_idx: int, writer: SummaryWriter):\n\t    state.model.eval()\n\t    state.codec.eval()\n\t    vn = accel.unwrap(state.model)\n", "    batch = [state.val_data[i] for i in val_idx]\n\t    batch = at.util.prepare_batch(state.val_data.collate(batch), accel.device)\n\t    signal = apply_transform(state.val_data.transform, batch)\n\t    z = state.codec.encode(signal.samples, signal.sample_rate)[\"codes\"]\n\t    z = z[:, : vn.n_codebooks, :]\n\t    r = torch.linspace(0.1, 0.95, len(val_idx)).to(accel.device)\n\t    mask = pmask.random(z, r)\n\t    mask = pmask.codebook_unmask(mask, vn.n_conditioning_codebooks)\n\t    z_mask, mask = pmask.apply_mask(z, mask, vn.mask_token)\n\t    z_mask_latent = vn.embedding.from_codes(z_mask, state.codec)\n", "    z_hat = state.model(z_mask_latent)\n\t    z_pred = torch.softmax(z_hat, dim=1).argmax(dim=1)\n\t    z_pred = codebook_unflatten(z_pred, n_c=vn.n_predict_codebooks)\n\t    z_pred = torch.cat([z[:, : vn.n_conditioning_codebooks, :], z_pred], dim=1)\n\t    generated = vn.to_signal(z_pred, state.codec)\n\t    reconstructed = vn.to_signal(z, state.codec)\n\t    masked = vn.to_signal(z_mask.squeeze(1), state.codec)\n\t    for i in range(generated.batch_size):\n\t        audio_dict = {\n\t            \"original\": signal[i],\n", "            \"masked\": masked[i],\n\t            \"generated\": generated[i],\n\t            \"reconstructed\": reconstructed[i],\n\t        }\n\t        for k, v in audio_dict.items():\n\t            v.cpu().write_audio_to_tb(\n\t                f\"samples/_{i}.r={r[i]:0.2f}/{k}\",\n\t                writer,\n\t                step=state.tracker.step,\n\t                plot_fn=None,\n", "            )\n\t    save_sampled(state=state, z=z, writer=writer)\n\t    save_imputation(state=state, z=z, val_idx=val_idx, writer=writer)\n\t@argbind.bind(without_prefix=True)\n\tdef load(\n\t    args,\n\t    accel: at.ml.Accelerator,\n\t    tracker: Tracker,\n\t    save_path: str,\n\t    resume: bool = False,\n", "    tag: str = \"latest\",\n\t    fine_tune_checkpoint: Optional[str] = None,\n\t    grad_clip_val: float = 5.0,\n\t) -> State:\n\t    codec = DAC.load(args[\"codec_ckpt\"], map_location=\"cpu\")\n\t    codec.eval()\n\t    model, v_extra = None, {}\n\t    if resume:\n\t        kwargs = {\n\t            \"folder\": f\"{save_path}/{tag}\",\n", "            \"map_location\": \"cpu\",\n\t            \"package\": False,\n\t        }\n\t        tracker.print(f\"Loading checkpoint from {kwargs['folder']}\")\n\t        if (Path(kwargs[\"folder\"]) / \"vampnet\").exists():\n\t            model, v_extra = VampNet.load_from_folder(**kwargs)\n\t        else:\n\t            raise ValueError(\n\t                f\"Could not find a VampNet checkpoint in {kwargs['folder']}\"\n\t            )\n", "    if args[\"fine_tune\"]:\n\t        assert fine_tune_checkpoint is not None, \"Must provide a fine-tune checkpoint\"\n\t        model = torch.compile(\n\t            VampNet.load(location=Path(fine_tune_checkpoint), \n\t                         map_location=\"cpu\", \n\t            )\n\t        )\n\t    model = torch.compile(VampNet()) if model is None else model\n\t    model = accel.prepare_model(model)\n\t    # assert accel.unwrap(model).n_codebooks == codec.quantizer.n_codebooks\n", "    assert (\n\t        accel.unwrap(model).vocab_size == codec.quantizer.quantizers[0].codebook_size\n\t    )\n\t    optimizer = AdamW(model.parameters(), use_zero=accel.use_ddp)\n\t    scheduler = NoamScheduler(optimizer, d_model=accel.unwrap(model).embedding_dim)\n\t    scheduler.step()\n\t    if \"optimizer.pth\" in v_extra:\n\t        optimizer.load_state_dict(v_extra[\"optimizer.pth\"])\n\t        scheduler.load_state_dict(v_extra[\"scheduler.pth\"])\n\t    if \"tracker.pth\" in v_extra:\n", "        tracker.load_state_dict(v_extra[\"tracker.pth\"])\n\t    criterion = CrossEntropyLoss()\n\t    sample_rate = codec.sample_rate\n\t    # a better rng for sampling from our schedule\n\t    rng = torch.quasirandom.SobolEngine(1, scramble=True, seed=args[\"seed\"])  \n\t    # log a model summary w/ num params\n\t    if accel.local_rank == 0:\n\t        add_num_params_repr_hook(accel.unwrap(model))\n\t        with open(f\"{save_path}/model.txt\", \"w\") as f:\n\t            f.write(repr(accel.unwrap(model)))\n", "    # load the datasets\n\t    train_data, val_data = build_datasets(args, sample_rate)\n\t    return State(\n\t        tracker=tracker,\n\t        model=model,\n\t        codec=codec,\n\t        optimizer=optimizer,\n\t        scheduler=scheduler,\n\t        criterion=criterion,\n\t        rng=rng,\n", "        train_data=train_data,\n\t        val_data=val_data,\n\t        grad_clip_val=grad_clip_val,\n\t    )\n\t@argbind.bind(without_prefix=True)\n\tdef train(\n\t    args,\n\t    accel: at.ml.Accelerator,\n\t    seed: int = 0,\n\t    codec_ckpt: str = None,\n", "    save_path: str = \"ckpt\",\n\t    num_iters: int = int(1000e6),\n\t    save_iters: list = [10000, 50000, 100000, 300000, 500000,],\n\t    sample_freq: int = 10000, \n\t    val_freq: int = 1000,\n\t    batch_size: int = 12,\n\t    val_idx: list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n\t    num_workers: int = 10,\n\t    fine_tune: bool = False, \n\t):\n", "    assert codec_ckpt is not None, \"codec_ckpt is required\"\n\t    seed = seed + accel.local_rank\n\t    at.util.seed(seed)\n\t    writer = None\n\t    if accel.local_rank == 0:\n\t        writer = SummaryWriter(log_dir=f\"{save_path}/logs/\")\n\t        argbind.dump_args(args, f\"{save_path}/args.yml\")\n\t        tracker = Tracker(\n\t            writer=writer, log_file=f\"{save_path}/log.txt\", rank=accel.local_rank\n\t        )\n", "    # load the codec model\n\t    state: State = load(\n\t        args=args, \n\t        accel=accel, \n\t        tracker=tracker, \n\t        save_path=save_path)\n\t    print(\"initialized state.\")\n\t    train_dataloader = accel.prepare_dataloader(\n\t        state.train_data,\n\t        start_idx=state.tracker.step * batch_size,\n", "        num_workers=num_workers,\n\t        batch_size=batch_size,\n\t        collate_fn=state.train_data.collate,\n\t    )\n\t    val_dataloader = accel.prepare_dataloader(\n\t        state.val_data,\n\t        start_idx=0,\n\t        num_workers=num_workers,\n\t        batch_size=batch_size,\n\t        collate_fn=state.val_data.collate,\n", "        persistent_workers=num_workers > 0,\n\t    )\n\t    print(\"initialized dataloader.\")\n\t    if fine_tune:\n\t        lora.mark_only_lora_as_trainable(state.model)\n\t        print(\"marked only lora as trainable.\")\n\t    # Wrap the functions so that they neatly track in TensorBoard + progress bars\n\t    # and only run when specific conditions are met.\n\t    global train_loop, val_loop, validate, save_samples, checkpoint\n\t    train_loop = tracker.log(\"train\", \"value\", history=False)(\n", "        tracker.track(\"train\", num_iters, completed=state.tracker.step)(train_loop)\n\t    )\n\t    val_loop = tracker.track(\"val\", len(val_dataloader))(val_loop)\n\t    validate = tracker.log(\"val\", \"mean\")(validate)\n\t    save_samples = when(lambda: accel.local_rank == 0)(save_samples)\n\t    checkpoint = when(lambda: accel.local_rank == 0)(checkpoint)\n\t    print(\"starting training loop.\")\n\t    with tracker.live:\n\t        for tracker.step, batch in enumerate(train_dataloader, start=tracker.step):\n\t            train_loop(state, batch, accel)\n", "            last_iter = (\n\t                tracker.step == num_iters - 1 if num_iters is not None else False\n\t            )\n\t            if tracker.step % sample_freq == 0 or last_iter:\n\t                save_samples(state, val_idx, writer)\n\t            if tracker.step % val_freq == 0 or last_iter:\n\t                validate(state, val_dataloader, accel)\n\t                checkpoint(\n\t                    state=state, \n\t                    save_iters=save_iters, \n", "                    save_path=save_path, \n\t                    fine_tune=fine_tune)\n\t                # Reset validation progress bar, print summary since last validation.\n\t                tracker.done(\"val\", f\"Iteration {tracker.step}\")\n\t            if last_iter:\n\t                break\n\tif __name__ == \"__main__\":\n\t    args = argbind.parse_args()\n\t    args[\"args.debug\"] = int(os.getenv(\"LOCAL_RANK\", 0)) == 0\n\t    with argbind.scope(args):\n", "        with Accelerator() as accel:\n\t            if accel.local_rank != 0:\n\t                sys.tracebacklimit = 0\n\t            train(args, accel)\n"]}
{"filename": "scripts/exp/eval.py", "chunked_list": ["from pathlib import Path\n\timport os\n\tfrom functools import partial\n\tfrom frechet_audio_distance import FrechetAudioDistance\n\timport pandas\n\timport argbind\n\timport torch\n\tfrom tqdm import tqdm\n\timport audiotools\n\tfrom audiotools import AudioSignal\n", "@argbind.bind(without_prefix=True)\n\tdef eval(\n\t    exp_dir: str = None,\n\t    baseline_key: str = \"baseline\", \n\t    audio_ext: str = \".wav\",\n\t):\n\t    assert exp_dir is not None\n\t    exp_dir = Path(exp_dir)\n\t    assert exp_dir.exists(), f\"exp_dir {exp_dir} does not exist\"\n\t    # set up our metrics\n", "    # sisdr_loss = audiotools.metrics.distance.SISDRLoss()\n\t    # stft_loss = audiotools.metrics.spectral.MultiScaleSTFTLoss()\n\t    mel_loss = audiotools.metrics.spectral.MelSpectrogramLoss()\n\t    frechet = FrechetAudioDistance(\n\t        use_pca=False, \n\t        use_activation=False,\n\t        verbose=True, \n\t        audio_load_worker=4,\n\t    )\n\t    frechet.model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "    # figure out what conditions we have\n\t    conditions = [d.name for d in exp_dir.iterdir() if d.is_dir()]\n\t    assert baseline_key in conditions, f\"baseline_key {baseline_key} not found in {exp_dir}\"\n\t    conditions.remove(baseline_key)\n\t    print(f\"Found {len(conditions)} conditions in {exp_dir}\")\n\t    print(f\"conditions: {conditions}\")\n\t    baseline_dir = exp_dir / baseline_key \n\t    baseline_files = sorted(list(baseline_dir.glob(f\"*{audio_ext}\")), key=lambda x: int(x.stem))\n\t    metrics = []\n\t    for condition in tqdm(conditions):\n", "        cond_dir = exp_dir / condition\n\t        cond_files = sorted(list(cond_dir.glob(f\"*{audio_ext}\")), key=lambda x: int(x.stem))\n\t        print(f\"computing fad for {baseline_dir} and {cond_dir}\")\n\t        frechet_score = frechet.score(baseline_dir, cond_dir)\n\t        # make sure we have the same number of files\n\t        num_files = min(len(baseline_files), len(cond_files))\n\t        baseline_files = baseline_files[:num_files]\n\t        cond_files = cond_files[:num_files]\n\t        assert len(list(baseline_files)) == len(list(cond_files)), f\"number of files in {baseline_dir} and {cond_dir} do not match. {len(list(baseline_files))} vs {len(list(cond_files))}\"\n\t        def process(baseline_file, cond_file):\n", "            # make sure the files match (same name)\n\t            assert baseline_file.stem == cond_file.stem, f\"baseline file {baseline_file} and cond file {cond_file} do not match\"\n\t            # load the files\n\t            baseline_sig = AudioSignal(str(baseline_file))\n\t            cond_sig = AudioSignal(str(cond_file))\n\t            cond_sig.resample(baseline_sig.sample_rate)\n\t            cond_sig.truncate_samples(baseline_sig.length)\n\t            # if our condition is inpainting, we need to trim the conditioning off\n\t            if \"inpaint\" in condition:\n\t                ctx_amt = float(condition.split(\"_\")[-1])\n", "                ctx_samples = int(ctx_amt * baseline_sig.sample_rate)\n\t                print(f\"found inpainting condition. trimming off {ctx_samples} samples from {cond_file} and {baseline_file}\")\n\t                cond_sig.trim(ctx_samples, ctx_samples)\n\t                baseline_sig.trim(ctx_samples, ctx_samples)\n\t            return {\n\t                # \"sisdr\": -sisdr_loss(baseline_sig, cond_sig).item(),\n\t                # \"stft\": stft_loss(baseline_sig, cond_sig).item(),\n\t                \"mel\": mel_loss(baseline_sig, cond_sig).item(),\n\t                \"frechet\": frechet_score,\n\t                # \"visqol\": vsq,\n", "                \"condition\": condition,\n\t                \"file\": baseline_file.stem,\n\t            }\n\t        print(f\"processing {len(baseline_files)} files in {baseline_dir} and {cond_dir}\")\n\t        metrics.extend(tqdm(map(process, baseline_files, cond_files), total=len(baseline_files)))\n\t    metric_keys = [k for k in metrics[0].keys() if k not in (\"condition\", \"file\")]\n\t    for mk in metric_keys:\n\t        stat = pandas.DataFrame(metrics)\n\t        stat = stat.groupby(['condition'])[mk].agg(['mean', 'count', 'std'])\n\t        stat.to_csv(exp_dir / f\"stats-{mk}.csv\")\n", "    df = pandas.DataFrame(metrics)\n\t    df.to_csv(exp_dir / \"metrics-all.csv\", index=False)\n\tif __name__ == \"__main__\":\n\t    args = argbind.parse_args()\n\t    with argbind.scope(args):\n\t        eval()"]}
{"filename": "scripts/utils/remove_quiet_files.py", "chunked_list": ["# removes files with loudness below 24db\n\tfrom pathlib import Path \n\timport shutil\n\timport audiotools as at\n\timport argbind\n\t@argbind.bind(without_prefix=True)\n\tdef remove_quiet_files(\n\t    src_dir: Path = None,\n\t    dest_dir: Path = None,\n\t    min_loudness: float = -30,\n", "):\n\t    # copy src to dest\n\t    dest_dir.mkdir(parents=True, exist_ok=True)\n\t    shutil.copytree(src_dir, dest_dir, dirs_exist_ok=True)\n\t    audio_files = at.util.find_audio(dest_dir)\n\t    for audio_file in audio_files:\n\t        sig = at.AudioSignal(audio_file)\n\t        if sig.loudness() < min_loudness:\n\t            audio_file.unlink()\n\t            print(f\"removed {audio_file}\")\n", "if __name__ == \"__main__\":\n\t    args = argbind.parse_args()\n\t    with argbind.scope(args):\n\t        remove_quiet_files()"]}
{"filename": "scripts/utils/stage.py", "chunked_list": ["import os\n\timport subprocess\n\tfrom pathlib import Path\n\timport argbind\n\timport rich\n\tfrom audiotools.ml import Experiment\n\t@argbind.bind(without_prefix=True)\n\tdef run(\n\t    run_dir: str = os.getenv(\"PATH_TO_RUNS\", \"runs\"),\n\t    name: str = None,\n", "    recent: bool = False,\n\t):\n\t    if recent:\n\t        paths = sorted(Path(run_dir).iterdir(), key=os.path.getmtime)\n\t        paths = [p.name for p in paths if p.is_dir()]\n\t        if paths:\n\t            name = paths[-1]\n\t    with Experiment(run_dir, name) as exp:\n\t        exp.snapshot()\n\t        rich.print(f\"Created a snapshot of {exp.parent_directory} at {exp.exp_dir}\")\n", "if __name__ == \"__main__\":\n\t    args = argbind.parse_args()\n\t    with argbind.scope(args):\n\t        run()\n"]}
{"filename": "scripts/utils/xeno-canto-dl.py", "chunked_list": ["from xenopy import Query\n\tSPECIES = [\n\t    \"American Robin\",\n\t    \"Northern Cardinal\",\n\t    \"Mourning Dove\",\n\t    \"American Crow\",\n\t    \"Baltimore Oriole\",\n\t    \"Blue Jay\",\n\t    \"Eastern Bluebird\",\n\t    \"House Finch\",\n", "    \"American Goldfinch\",\n\t    \"House Sparrow\",\n\t    \"Song Sparrow\",\n\t    \"Tufted Titmouse\",\n\t    \"White-breasted Nuthatch\",\n\t    \"European Starling\",\n\t    \"American Redstart\",\n\t    \"Red-winged Blackbird\",\n\t    \"Brown-headed Cowbird\",\n\t    \"Common Grackle\",\n", "    \"Boat-tailed Grackle\",\n\t    \"Common Yellowthroat\",\n\t    \"Northern Mockingbird\",\n\t    \"Carolina Wren\",\n\t    \"Eastern Meadowlark\",\n\t    \"Chipping Sparrow\",\n\t    \"Tree Swallow\",\n\t    \"Barn Swallow\",\n\t    \"Cliff Swallow\",\n\t    \"Pine Siskin\",\n", "    \"Indigo Bunting\",\n\t    \"Eastern Towhee\",\n\t    \"Carolina Chickadee\",\n\t    \"Great Crested Flycatcher\",\n\t    \"Eastern Wood-Pewee\",\n\t    \"Ovenbird\",\n\t    \"Northern Flicker\",\n\t    \"Red-eyed Vireo\",\n\t    \"American Woodcock\",\n\t    \"Eastern Phoebe\",\n", "    \"Downy Woodpecker\",\n\t    \"Scarlet Tanager\",\n\t    \"Yellow Warbler\",\n\t    \"White-eyed Vireo\",\n\t    \"Common Loon\",\n\t    \"White-throated Sparrow\",\n\t    \"Yellow-throated Vireo\",\n\t    \"Great Blue Heron\",\n\t    \"Belted Kingfisher\",\n\t    \"Pied-billed Grebe\",\n", "    \"Wild Turkey\",\n\t    \"Wood Thrush\",\n\t    \"Rose-breasted Grosbeak\",\n\t    \"Field Sparrow\",\n\t    \"Hooded Warbler\",\n\t    \"Northern Parula\",\n\t    \"Chestnut-sided Warbler\",\n\t    \"Blue-winged Warbler\",\n\t    \"Red-bellied Woodpecker\",\n\t    \"Yellow-billed Cuckoo\",\n", "    \"Gray Catbird\",\n\t    \"Northern Saw-whet Owl\",\n\t    \"Osprey\",\n\t    \"Common Nighthawk\",\n\t    \"Broad-winged Hawk\",\n\t    \"Black-throated Green Warbler\",\n\t    \"Great Horned Owl\",\n\t    \"Common Raven\",\n\t    \"Barred Owl\",\n\t    \"Canada Warbler\",\n", "    \"Magnolia Warbler\",\n\t    \"Black-and-white Warbler\",\n\t    \"Eastern Kingbird\",\n\t    \"Swainson's Thrush\",\n\t    \"Worm-eating Warbler\",\n\t    \"Prairie Warbler\",\n\t    \"Baltimore Oriole\",\n\t    \"Black-throated Blue Warbler\",\n\t    \"Louisiana Waterthrush\",\n\t    \"Blackburnian Warbler\",\n", "    \"Black-capped Chickadee\",\n\t    \"Cerulean Warbler\",\n\t    \"Red-shouldered Hawk\",\n\t    \"Cooper's Hawk\",\n\t    \"Yellow-throated Warbler\",\n\t    \"Blue-headed Vireo\",\n\t    \"Blackpoll Warbler\",\n\t    \"Ruffed Grouse\",\n\t    \"Kentucky Warbler\",\n\t    \"Hermit Thrush\",\n", "    \"Cedar Waxwing\",\n\t    \"Eastern Screech-Owl\",\n\t    \"Northern Goshawk\",\n\t    \"Green Heron\",\n\t    \"Red-tailed Hawk\",\n\t    \"Black Vulture\",\n\t    \"Hairy Woodpecker\",\n\t    \"Golden-crowned Kinglet\",\n\t    \"Ruby-crowned Kinglet\",\n\t    \"Bicknell's Thrush\",\n", "    \"Blue-gray Gnatcatcher\",\n\t    \"Veery\",\n\t    \"Pileated Woodpecker\",\n\t    \"Purple Finch\",\n\t    \"White-crowned Sparrow\",\n\t    \"Snow Bunting\",\n\t    \"Pine Grosbeak\",\n\t    \"American Tree Sparrow\",\n\t    \"Dark-eyed Junco\",\n\t    \"Snowy Owl\",\n", "    \"White-winged Crossbill\",\n\t    \"Red Crossbill\",\n\t    \"Common Redpoll\",\n\t    \"Northern Shrike\",\n\t    \"Northern Harrier\",\n\t    \"Rough-legged Hawk\",\n\t    \"Long-eared Owl\",\n\t    \"Evening Grosbeak\",\n\t    \"Northern Pintail\",\n\t    \"American Black Duck\",\n", "    \"Mallard\",\n\t    \"Canvasback\",\n\t    \"Redhead\",\n\t    \"Ring-necked Duck\",\n\t    \"Greater Scaup\",\n\t    \"Lesser Scaup\",\n\t    \"Bufflehead\",\n\t    \"Common Goldeneye\",\n\t    \"Hooded Merganser\",\n\t    \"Common Merganser\",\n", "    \"Red-breasted Merganser\",\n\t    \"Ruddy Duck\",\n\t    \"Wood Duck\",\n\t    \"Gadwall\",\n\t    \"American Wigeon\",\n\t    \"Northern Shoveler\",\n\t    \"Green-winged Teal\",\n\t    \"Blue-winged Teal\",\n\t    \"Cinnamon Teal\",\n\t    \"Ringed Teal\",\n", "    \"Cape Teal\",\n\t    \"Northern Fulmar\",\n\t    \"Yellow-billed Loon\",\n\t    \"Red-throated Loon\",\n\t    \"Arctic Loon\",\n\t    \"Pacific Loon\",\n\t    \"Horned Grebe\",\n\t    \"Red-necked Grebe\",\n\t    \"Eared Grebe\",\n\t    \"Western Grebe\",\n", "    \"Clark's Grebe\",\n\t    \"Double-crested Cormorant\",\n\t    \"Pelagic Cormorant\",\n\t    \"Great Cormorant\",\n\t    \"American White Pelican\",\n\t    \"Brown Pelican\",\n\t    \"Brandt's Cormorant\",\n\t    \"Least Bittern\",\n\t    \"Great Egret\",\n\t    \"Snowy Egret\",\n", "    \"Little Blue Heron\",\n\t    \"Tricolored Heron\",\n\t    \"Reddish Egret\",\n\t    \"Black-crowned Night-Heron\",\n\t    \"Yellow-crowned Night-Heron\",\n\t    \"White Ibis\",\n\t    \"Glossy Ibis\",\n\t    \"Roseate Spoonbill\",\n\t    \"Wood Stork\",\n\t    \"Black-bellied Whistling-Duck\",\n", "    \"Fulvous Whistling-Duck\",\n\t    \"Greater White-fronted Goose\",\n\t    \"Snow Goose\",\n\t    \"Ross's Goose\",\n\t    \"Canada Goose\",\n\t    \"Brant\",\n\t    \"Mute Swan\",\n\t    \"Tundra Swan\",\n\t    \"Whooper Swan\",\n\t    \"Sandhill Crane\",\n", "    \"Black-necked Stilt\",\n\t    \"American Avocet\",\n\t    \"Northern Jacana\",\n\t    \"Greater Yellowlegs\",\n\t    \"Lesser Yellowlegs\",\n\t    \"Willet\",\n\t    \"Spotted Sandpiper\",\n\t    \"Upland Sandpiper\",\n\t    \"Whimbrel\",\n\t    \"Long-billed Curlew\",\n", "    \"Marbled Godwit\",\n\t    \"Ruddy Turnstone\",\n\t    \"Red Knot\",\n\t    \"Sanderling\",\n\t    \"Semipalmated Sandpiper\",\n\t    \"Western Sandpiper\",\n\t    \"Least Sandpiper\",\n\t    \"White-rumped Sandpiper\",\n\t    \"Baird's Sandpiper\",\n\t    \"Pectoral Sandpiper\",\n", "    \"Dunlin\",\n\t    \"Buff-breasted Sandpiper\",\n\t    \"Short-billed Dowitcher\",\n\t    \"Long-billed Dowitcher\",\n\t    \"Common Snipe\",\n\t    \"American Woodcock\",\n\t    \"Wilson's Phalarope\",\n\t    \"Red-necked Phalarope\",\n\t    \"Red Phalarope\"\n\t]\n", "from pathlib import Path\n\tdef remove_spaces(s):\n\t    return s.replace(\" \", \"\")\n\tfor species in SPECIES: \n\t    if Path(\"/media/CHONK/hugo/xeno-canto-full/\" + remove_spaces(species)).exists():\n\t        continue\n\t    try:\n\t        q = Query(\n\t            name=species, q=\"A\", length=\"10-30\", \n\t            )\n", "        # retrieve metadata\n\t        metafiles = q.retrieve_meta(verbose=True)\n\t        # retrieve recordings\n\t        q.retrieve_recordings(multiprocess=True, nproc=10, attempts=10, outdir=\"/media/CHONK/hugo/xeno-canto-full/\")\n\t    except:\n\t        print(\"Failed to download \" + species)\n\t        continue"]}
{"filename": "scripts/utils/split.py", "chunked_list": ["from pathlib import Path\n\timport random\n\timport shutil\n\timport os\n\timport json \n\timport argbind\n\tfrom tqdm import tqdm\n\tfrom tqdm.contrib.concurrent import thread_map\n\tfrom audiotools.core import util\n\t@argbind.bind(without_prefix=True)\n", "def train_test_split(\n\t    audio_folder: str = \".\", \n\t    test_size: float = 0.2,\n\t    seed: int = 42,\n\t    pattern: str = \"**/*.mp3\",\n\t):\n\t    print(f\"finding audio\")\n\t    audio_folder = Path(audio_folder)\n\t    audio_files = list(tqdm(audio_folder.glob(pattern)))\n\t    print(f\"found {len(audio_files)} audio files\")\n", "    # split according to test_size\n\t    n_test = int(len(audio_files) * test_size)\n\t    n_train = len(audio_files) - n_test\n\t    # shuffle\n\t    random.seed(seed)\n\t    random.shuffle(audio_files)\n\t    train_files = audio_files[:n_train]\n\t    test_files = audio_files[n_train:]\n\t    print(f\"Train files: {len(train_files)}\")\n\t    print(f\"Test files: {len(test_files)}\")\n", "    continue_ = input(\"Continue [yn]? \") or \"n\"\n\t    if continue_ != \"y\":\n\t        return\n\t    for split, files in (\n\t        (\"train\", train_files), (\"test\", test_files)\n\t    ):\n\t        for file in tqdm(files):\n\t            out_file = audio_folder.parent / f\"{audio_folder.name}-{split}\" / Path(file).name\n\t            out_file.parent.mkdir(exist_ok=True, parents=True)\n\t            os.symlink(file, out_file)\n", "        # save split as json\n\t        with open(Path(audio_folder) / f\"{split}.json\", \"w\") as f:\n\t            json.dump([str(f) for f in files], f)\n\tif __name__ == \"__main__\":\n\t    args  = argbind.parse_args()\n\t    with argbind.scope(args):\n\t        train_test_split()"]}
{"filename": "scripts/utils/plots.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport seaborn as sns\n\tfrom pandas.api.types import CategoricalDtype\n\tdef plot_metrics(metrics, condition_to_latex, title, color_palette):\n\t    # Add a new column to your dataframe with the latex representation\n\t    metrics['condition_latex'] = metrics['condition'].map(condition_to_latex)\n\t    # Order condition_latex as per the condition_to_latex dictionary\n\t    cat_type = CategoricalDtype(categories=condition_to_latex.values(), ordered=True)\n\t    metrics['condition_latex'] = metrics['condition_latex'].astype(cat_type)\n\t    # Compute mean and std for each condition for each metric\n", "    grouped = metrics.groupby('condition_latex')[['mel', 'frechet']].agg(['mean', 'std'])\n\t    fig, axs = plt.subplots(2, 1, figsize=(7, 5.25))\n\t    # Set the main title for the figure\n\t    fig.suptitle(title, fontsize=16)\n\t    # Get color for each bar in the plot\n\t    bar_colors = [color_palette[condition] for condition in grouped.index]\n\t    # Plot mel\n\t    sns.boxplot(x='condition_latex', y='mel', data=metrics, ax=axs[0], palette=color_palette, showfliers=False)\n\t    axs[0].set_ylabel('Mel Spectrogram Loss \\u2190')\n\t    axs[0].set_xlabel('') # Remove x-axis label\n", "    axs[0].set_xticklabels(grouped.index, rotation=0, ha='center')\n\t    # Plot frechet\n\t    axs[1].bar(grouped.index, grouped['frechet']['mean'], yerr=grouped['frechet']['std'], color=bar_colors)\n\t    axs[1].set_ylabel('FAD \\u2190')\n\t    axs[1].set_xlabel('') # Remove x-axis label\n\t    axs[1].set_xticklabels(grouped.index, rotation=0, ha='center')\n\t    # Adjust the space between plots\n\t    plt.subplots_adjust(hspace=0.1)\n\t    # Remove any unnecessary space around the plot\n\t    plt.tight_layout(rect=[0, 0, 1, 0.96])\n", "    # Reduce the space between suptitle and the plot\n\t    plt.subplots_adjust(top=0.92)"]}
{"filename": "scripts/utils/augment.py", "chunked_list": ["from pathlib import Path\n\timport audiotools as at\n\tfrom audiotools import AudioSignal\n\timport argbind\n\timport tqdm\n\timport torch\n\tfrom torch_pitch_shift import pitch_shift, get_fast_shifts\n\tfrom torch_time_stretch import time_stretch, get_fast_stretches\n\tfrom audiotools.core.util import sample_from_dist\n\t@argbind.bind(without_prefix=True)\n", "def augment(\n\t    audio_folder: Path = None,\n\t    dest_folder: Path = None,\n\t    n_augmentations: int = 10,\n\t):\n\t    \"\"\" \n\t        Augment a folder of audio files by applying audiotools and pedalboard transforms. \n\t        The dest foler will contain a folder for each of the clean dataset's files. \n\t        Under each of these folders, there will be a clean file and many augmented files.\n\t    \"\"\"\n", "    assert audio_folder is not None\n\t    assert dest_folder is not None\n\t    audio_files = at.util.find_audio(audio_folder)\n\t    for audio_file in tqdm.tqdm(audio_files):\n\t        subtree = dest_folder / audio_file.relative_to(audio_folder).parent\n\t        subdir = subtree / audio_file.stem\n\t        subdir.mkdir(parents=True, exist_ok=True)\n\t        src = AudioSignal(audio_file).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t        for i, chunk in tqdm.tqdm(enumerate(src.windows(10, 10))):\n\t            # apply pedalboard transforms\n", "            for j in range(n_augmentations):\n\t                # pitch shift between -7 and 7 semitones\n\t                import random\n\t                dst = chunk.clone()\n\t                dst.samples = pitch_shift(\n\t                    dst.samples, \n\t                    shift=random.choice(get_fast_shifts(src.sample_rate, \n\t                            condition=lambda x: x >= 0.25 and x <= 1.0)), \n\t                    sample_rate=src.sample_rate\n\t                )\n", "                dst.samples = time_stretch(\n\t                    dst.samples,\n\t                    stretch=random.choice(get_fast_stretches(src.sample_rate, \n\t                                          condition=lambda x: x >= 0.667 and x <= 1.5, )),\n\t                    sample_rate=src.sample_rate, \n\t                )\n\t                dst.cpu().write(subdir / f\"{i}-{j}.wav\")\n\tif __name__ == \"__main__\":\n\t    args = argbind.parse_args()\n\t    with argbind.scope(args):\n", "        augment()"]}
{"filename": "scripts/utils/split_long_audio_file.py", "chunked_list": ["from pathlib import Path\n\timport argbind\n\timport audiotools as at\n\timport tqdm\n\t@argbind.bind(without_prefix=True)\n\tdef split_long_audio_file(\n\t    file: str = None, \n\t    max_chunk_size_s: int = 60*10\n\t):\n\t    file = Path(file)\n", "    output_dir = file.parent / file.stem\n\t    output_dir.mkdir()\n\t    sig = at.AudioSignal(file)\n\t    # split into chunks\n\t    for i, sig in tqdm.tqdm(enumerate(sig.windows(\n\t        window_duration=max_chunk_size_s, hop_duration=max_chunk_size_s/2, \n\t        preprocess=True))\n\t    ):\n\t        sig.write(output_dir / f\"{i}.wav\")\n\t    print(f\"wrote {len(list(output_dir.glob('*.wav')))} files to {output_dir}\")\n", "    return output_dir\n\tif __name__ == \"__main__\":\n\t    args = argbind.parse_args()\n\t    with argbind.scope(args):\n\t        split_long_audio_file()"]}
{"filename": "scripts/utils/maestro-reorg.py", "chunked_list": ["from pathlib import Path\n\timport json\n\timport os\n\tmaestro_path = Path(\"/media/CHONK/hugo/maestro-v3.0.0\")\n\toutput_path = Path(\"/media/CHONK/hugo/maestro-v3.0.0-split\")\n\t# split\n\twith open(maestro_path / \"maestro-v3.0.0.json\") as f:\n\t    maestro = json.load(f)\n\tbreakpoint()\n\ttrain = []\n", "validation = []\n\ttest = []\n\tfor key, split in maestro[\"split\"].items():\n\t    audio_filename = maestro['audio_filename'][key]\n\t    if split == \"train\":\n\t        train.append(audio_filename)\n\t    elif split == \"test\":\n\t        test.append(audio_filename)\n\t    elif split == \"validation\":\n\t        validation.append(audio_filename)\n", "    else:\n\t        raise ValueError(f\"Unknown split {split}\")\n\t# symlink all files\n\tfor audio_filename in train:\n\t    p = output_path / \"train\" / audio_filename\n\t    p.parent.mkdir(parents=True, exist_ok=True)\n\t    os.symlink(maestro_path / audio_filename, p)\n\tfor audio_filename in validation:\n\t    p = output_path / \"validation\" / audio_filename\n\t    p.parent.mkdir(parents=True, exist_ok=True)\n", "    os.symlink(maestro_path / audio_filename, p)\n\tfor audio_filename in test:\n\t    p = output_path / \"test\" / audio_filename\n\t    p.parent.mkdir(parents=True, exist_ok=True)\n\t    os.symlink(maestro_path / audio_filename, p)"]}
{"filename": "vampnet/scheduler.py", "chunked_list": ["import copy\n\tfrom typing import List\n\timport torch\n\tclass NoamScheduler:\n\t    \"\"\"OG scheduler from transformer paper: https://arxiv.org/pdf/1706.03762.pdf\n\t    Implementation from Annotated Transformer: https://nlp.seas.harvard.edu/2018/04/03/attention.html\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        optimizer: torch.optim.Optimizer,\n", "        d_model: int = 512,\n\t        factor: float = 1.0,\n\t        warmup: int = 4000,\n\t    ):\n\t        # Store hparams\n\t        self.warmup = warmup\n\t        self.factor = factor\n\t        self.d_model = d_model\n\t        # Initialize variables `lr` and `steps`\n\t        self.lr = None\n", "        self.steps = 0\n\t        # Store the optimizer\n\t        self.optimizer = optimizer\n\t    def state_dict(self):\n\t        return {\n\t            key: value for key, value in self.__dict__.items() if key != \"optimizer\"\n\t        }\n\t    def load_state_dict(self, state_dict):\n\t        self.__dict__.update(state_dict)\n\t    def step(self):\n", "        self.steps += 1\n\t        self.lr = self.factor * (\n\t            self.d_model ** (-0.5)\n\t            * min(self.steps ** (-0.5), self.steps * self.warmup ** (-1.5))\n\t        )\n\t        for p in self.optimizer.param_groups:\n\t            p[\"lr\"] = self.lr\n"]}
{"filename": "vampnet/__init__.py", "chunked_list": ["from . import modules\n\tfrom . import scheduler\n\tfrom .interface import Interface\n\t__version__ = \"0.0.1\"\n"]}
{"filename": "vampnet/mask.py", "chunked_list": ["from typing import Optional\n\timport torch\n\tfrom audiotools import AudioSignal\n\tfrom .util import scalar_to_batch_tensor\n\tdef _gamma(r):\n\t    return (r * torch.pi / 2).cos().clamp(1e-10, 1.0)\n\tdef _invgamma(y):\n\t    if not torch.is_tensor(y):\n\t        y = torch.tensor(y)[None]\n\t    return 2 * y.acos() / torch.pi\n", "def full_mask(x: torch.Tensor):\n\t    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n\t    return torch.ones_like(x).long()\n\tdef empty_mask(x: torch.Tensor):\n\t    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n\t    return torch.zeros_like(x).long()\n\tdef apply_mask(\n\t        x: torch.Tensor, \n\t        mask: torch.Tensor, \n\t        mask_token: int\n", "    ):\n\t    assert mask.ndim == 3, \"mask must be (batch, n_codebooks, seq), but got {mask.ndim}\"\n\t    assert mask.shape == x.shape, f\"mask must be same shape as x, but got {mask.shape} and {x.shape}\" \n\t    assert mask.dtype == torch.long, \"mask must be long dtype, but got {mask.dtype}\"\n\t    assert ~torch.any(mask > 1), \"mask must be binary\"\n\t    assert ~torch.any(mask < 0), \"mask must be binary\"\n\t    fill_x = torch.full_like(x, mask_token)\n\t    x = x * (1 - mask) + fill_x * mask\n\t    return x, mask\n\tdef random(\n", "    x: torch.Tensor,\n\t    r: torch.Tensor\n\t):\n\t    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n\t    if not isinstance(r, torch.Tensor):\n\t        r = scalar_to_batch_tensor(r, x.shape[0]).to(x.device)\n\t    r = _gamma(r)[:, None, None]\n\t    probs = torch.ones_like(x) * r\n\t    mask = torch.bernoulli(probs)\n\t    mask = mask.round().long()\n", "    return mask\n\tdef linear_random(\n\t    x: torch.Tensor,\n\t    r: torch.Tensor,\n\t):\n\t    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n\t    if not isinstance(r, torch.Tensor):\n\t        r = scalar_to_batch_tensor(r, x.shape[0]).to(x.device).float()\n\t    probs = torch.ones_like(x).to(x.device).float()\n\t    # expand to batch and codebook dims\n", "    probs = probs.expand(x.shape[0], x.shape[1], -1)\n\t    probs = probs * r\n\t    mask = torch.bernoulli(probs)\n\t    mask = mask.round().long()\n\t    return mask\n\tdef inpaint(x: torch.Tensor, \n\t    n_prefix,\n\t    n_suffix,\n\t):\n\t    assert n_prefix is not None\n", "    assert n_suffix is not None\n\t    mask = full_mask(x)\n\t    # if we have a prefix or suffix, set their mask prob to 0\n\t    if n_prefix > 0:\n\t        if not isinstance(n_prefix, torch.Tensor):\n\t            n_prefix = scalar_to_batch_tensor(n_prefix, x.shape[0]).to(x.device) \n\t        for i, n in enumerate(n_prefix):\n\t            if n > 0:\n\t                mask[i, :, :n] = 0.0\n\t    if n_suffix > 0:\n", "        if not isinstance(n_suffix, torch.Tensor):\n\t            n_suffix = scalar_to_batch_tensor(n_suffix, x.shape[0]).to(x.device)\n\t        for i, n in enumerate(n_suffix):\n\t            if n > 0:\n\t                mask[i, :, -n:] = 0.0\n\t    return mask\n\tdef periodic_mask(x: torch.Tensor, \n\t                period: int, width: int = 1, \n\t                random_roll=False,\n\t    ):\n", "    mask = full_mask(x)\n\t    if period == 0:\n\t        return mask\n\t    if not isinstance(period, torch.Tensor):\n\t        period = scalar_to_batch_tensor(period, x.shape[0])\n\t    for i, factor in enumerate(period):\n\t        if factor == 0:\n\t            continue\n\t        for j in range(mask.shape[-1]):\n\t            if j % factor == 0:\n", "                # figure out how wide the mask should be\n\t                j_start = max(0, j - width // 2  )\n\t                j_end = min(mask.shape[-1] - 1, j + width // 2 ) + 1 \n\t                # flip a coin for each position in the mask\n\t                j_mask = torch.bernoulli(torch.ones(j_end - j_start))\n\t                assert torch.all(j_mask == 1)\n\t                j_fill = torch.ones_like(j_mask) * (1 - j_mask)\n\t                assert torch.all(j_fill == 0)\n\t                # fill\n\t                mask[i, :, j_start:j_end] = j_fill\n", "    if random_roll:\n\t        # add a random offset to the mask\n\t        offset = torch.randint(0, period[0], (1,))\n\t        mask = torch.roll(mask, offset.item(), dims=-1)\n\t    return mask\n\tdef codebook_unmask(\n\t    mask: torch.Tensor, \n\t    n_conditioning_codebooks: int\n\t):\n\t    if n_conditioning_codebooks == None:\n", "        return mask\n\t    # if we have any conditioning codebooks, set their mask  to 0\n\t    mask = mask.clone()\n\t    mask[:, :n_conditioning_codebooks, :] = 0\n\t    return mask\n\tdef mask_and(\n\t    mask1: torch.Tensor, \n\t    mask2: torch.Tensor\n\t):\n\t    assert mask1.shape == mask2.shape, \"masks must be same shape\"\n", "    return torch.min(mask1, mask2)\n\tdef dropout(\n\t    mask: torch.Tensor,\n\t    p: float,\n\t):\n\t    assert 0 <= p <= 1, \"p must be between 0 and 1\"\n\t    assert mask.max() <= 1, \"mask must be binary\"\n\t    assert mask.min() >= 0, \"mask must be binary\"\n\t    mask = (~mask.bool()).float()\n\t    mask = torch.bernoulli(mask * (1 - p))\n", "    mask = ~mask.round().bool()\n\t    return mask.long()\n\tdef mask_or(\n\t    mask1: torch.Tensor, \n\t    mask2: torch.Tensor\n\t):\n\t    assert mask1.shape == mask2.shape, f\"masks must be same shape, but got {mask1.shape} and {mask2.shape}\"\n\t    assert mask1.max() <= 1, \"mask1 must be binary\"\n\t    assert mask2.max() <= 1, \"mask2 must be binary\"\n\t    assert mask1.min() >= 0, \"mask1 must be binary\"\n", "    assert mask2.min() >= 0, \"mask2 must be binary\"\n\t    return (mask1 + mask2).clamp(0, 1)\n\tdef time_stretch_mask(\n\t    x: torch.Tensor, \n\t    stretch_factor: int,\n\t):\n\t    assert stretch_factor >= 1, \"stretch factor must be >= 1\"\n\t    c_seq_len = x.shape[-1]\n\t    x = x.repeat_interleave(stretch_factor, dim=-1)\n\t    # trim cz to the original length\n", "    x = x[:, :, :c_seq_len]\n\t    mask = periodic_mask(x, stretch_factor, width=1)\n\t    return mask\n\tdef onset_mask(\n\t    sig: AudioSignal, \n\t    z: torch.Tensor,\n\t    interface,\n\t    width: int = 1\n\t):\n\t    import librosa\n", "    import madmom\n\t    from madmom.features.onsets import RNNOnsetProcessor, OnsetPeakPickingProcessor\n\t    import tempfile\n\t    import numpy as np \n\t    with tempfile.NamedTemporaryFile(suffix='.wav') as f:\n\t        sig = sig.clone()\n\t        sig.write(f.name)\n\t        proc = RNNOnsetProcessor(online=False)\n\t        onsetproc = OnsetPeakPickingProcessor(threshold=0.3,\n\t                                              fps=sig.sample_rate/interface.codec.hop_length)\n", "        act = proc(f.name)\n\t        onset_times = onsetproc(act)\n\t        # convert to indices for z array\n\t        onset_indices = librosa.time_to_frames(onset_times, sr=sig.sample_rate, hop_length=interface.codec.hop_length)\n\t        if onset_indices.shape[0] == 0:\n\t            mask = empty_mask(z)   \n\t            print(f\"no onsets found, returning empty mask\")\n\t        else: \n\t            torch.set_printoptions(threshold=1000)\n\t            print(\"onset indices: \", onset_indices)\n", "            print(\"onset times: \", onset_times)\n\t            # create a mask, set onset \n\t            mask = torch.ones_like(z)\n\t            n_timesteps = z.shape[-1]\n\t            for onset_index in onset_indices:\n\t                onset_index = min(onset_index, n_timesteps - 1)\n\t                onset_index = max(onset_index, 0)\n\t                mask[:, :, onset_index - width:onset_index + width] = 0.0\n\t            print(mask)\n\t    return mask\n", "if __name__ == \"__main__\":\n\t    pass\n"]}
{"filename": "vampnet/beats.py", "chunked_list": ["import json\n\timport logging\n\timport warnings\n\tfrom dataclasses import dataclass\n\tfrom pathlib import Path\n\tfrom typing import Any\n\tfrom typing import List\n\tfrom typing import Tuple\n\tfrom typing import Union\n\timport librosa\n", "import torch\n\timport numpy as np\n\tfrom audiotools import AudioSignal\n\tlogging.basicConfig(level=logging.INFO)\n\t###################\n\t# beat sync utils #\n\t###################\n\tAGGREGATOR_REGISTRY = {\n\t    \"mean\": np.mean,\n\t    \"median\": np.median,\n", "    \"max\": np.max,\n\t    \"min\": np.min,\n\t}\n\tdef list_aggregators() -> list:\n\t    return list(AGGREGATOR_REGISTRY.keys())\n\t@dataclass\n\tclass TimeSegment:\n\t    start: float\n\t    end: float\n\t    @property\n", "    def duration(self):\n\t        return self.end - self.start\n\t    def __str__(self) -> str:\n\t        return f\"{self.start} - {self.end}\"\n\t    def find_overlapping_segment(\n\t        self, segments: List[\"TimeSegment\"]\n\t    ) -> Union[\"TimeSegment\", None]:\n\t        \"\"\"Find the first segment that overlaps with this segment, or None if no segment overlaps\"\"\"\n\t        for s in segments:\n\t            if s.start <= self.start and s.end >= self.end:\n", "                return s\n\t        return None\n\tdef mkdir(path: Union[Path, str]) -> Path:\n\t    p = Path(path)\n\t    p.mkdir(parents=True, exist_ok=True)\n\t    return p\n\t###################\n\t#    beat data    #\n\t###################\n\t@dataclass\n", "class BeatSegment(TimeSegment):\n\t    downbeat: bool = False  # if there's a downbeat on the start_time\n\tclass Beats:\n\t    def __init__(self, beat_times, downbeat_times):\n\t        if isinstance(beat_times, np.ndarray):\n\t            beat_times = beat_times.tolist()\n\t        if isinstance(downbeat_times, np.ndarray):\n\t            downbeat_times = downbeat_times.tolist()\n\t        self._beat_times = beat_times\n\t        self._downbeat_times = downbeat_times\n", "        self._use_downbeats = False\n\t    def use_downbeats(self, use_downbeats: bool = True):\n\t        \"\"\"use downbeats instead of beats when calling beat_times\"\"\"\n\t        self._use_downbeats = use_downbeats\n\t    def beat_segments(self, signal: AudioSignal) -> List[BeatSegment]:\n\t        \"\"\"\n\t        segments a song into time segments corresponding to beats.\n\t        the first segment starts at 0 and ends at the first beat time.\n\t        the last segment starts at the last beat time and ends at the end of the song.\n\t        \"\"\"\n", "        beat_times = self._beat_times.copy()\n\t        downbeat_times = self._downbeat_times\n\t        beat_times.insert(0, 0)\n\t        beat_times.append(signal.signal_duration)\n\t        downbeat_ids = np.intersect1d(beat_times, downbeat_times, return_indices=True)[\n\t            1\n\t        ]\n\t        is_downbeat = [\n\t            True if i in downbeat_ids else False for i in range(len(beat_times))\n\t        ]\n", "        segments = [\n\t            BeatSegment(start_time, end_time, downbeat)\n\t            for start_time, end_time, downbeat in zip(\n\t                beat_times[:-1], beat_times[1:], is_downbeat\n\t            )\n\t        ]\n\t        return segments\n\t    def get_beats(self) -> np.ndarray:\n\t        \"\"\"returns an array of beat times, in seconds\n\t        if downbeats is True, returns an array of downbeat times, in seconds\n", "        \"\"\"\n\t        return np.array(\n\t            self._downbeat_times if self._use_downbeats else self._beat_times\n\t        )\n\t    @property\n\t    def beat_times(self) -> np.ndarray:\n\t        \"\"\"return beat times\"\"\"\n\t        return np.array(self._beat_times)\n\t    @property\n\t    def downbeat_times(self) -> np.ndarray:\n", "        \"\"\"return downbeat times\"\"\"\n\t        return np.array(self._downbeat_times)\n\t    def beat_times_to_feature_frames(\n\t        self, signal: AudioSignal, features: np.ndarray\n\t    ) -> np.ndarray:\n\t        \"\"\"convert beat times to frames, given an array of time-varying features\"\"\"\n\t        beat_times = self.get_beats()\n\t        beat_frames = (\n\t            beat_times * signal.sample_rate / signal.signal_length * features.shape[-1]\n\t        ).astype(np.int64)\n", "        return beat_frames\n\t    def sync_features(\n\t        self, feature_frames: np.ndarray, features: np.ndarray, aggregate=\"median\"\n\t    ) -> np.ndarray:\n\t        \"\"\"sync features to beats\"\"\"\n\t        if aggregate not in AGGREGATOR_REGISTRY:\n\t            raise ValueError(f\"unknown aggregation method {aggregate}\")\n\t        return librosa.util.sync(\n\t            features, feature_frames, aggregate=AGGREGATOR_REGISTRY[aggregate]\n\t        )\n", "    def to_json(self) -> dict:\n\t        \"\"\"return beats and downbeats as json\"\"\"\n\t        return {\n\t            \"beats\": self._beat_times,\n\t            \"downbeats\": self._downbeat_times,\n\t            \"use_downbeats\": self._use_downbeats,\n\t        }\n\t    @classmethod\n\t    def from_dict(cls, data: dict):\n\t        \"\"\"load beats and downbeats from json\"\"\"\n", "        inst = cls(data[\"beats\"], data[\"downbeats\"])\n\t        inst.use_downbeats(data[\"use_downbeats\"])\n\t        return inst\n\t    def save(self, output_dir: Path):\n\t        \"\"\"save beats and downbeats to json\"\"\"\n\t        mkdir(output_dir)\n\t        with open(output_dir / \"beats.json\", \"w\") as f:\n\t            json.dump(self.to_json(), f)\n\t    @classmethod\n\t    def load(cls, input_dir: Path):\n", "        \"\"\"load beats and downbeats from json\"\"\"\n\t        beats_file = Path(input_dir) / \"beats.json\"\n\t        with open(beats_file, \"r\") as f:\n\t            data = json.load(f)\n\t        return cls.from_dict(data)\n\t###################\n\t#  beat tracking  #\n\t###################\n\tclass BeatTracker:\n\t    def extract_beats(self, signal: AudioSignal) -> Tuple[np.ndarray, np.ndarray]:\n", "        \"\"\"extract beats from an audio signal\"\"\"\n\t        raise NotImplementedError\n\t    def __call__(self, signal: AudioSignal) -> Beats:\n\t        \"\"\"extract beats from an audio signal\n\t        NOTE: if the first beat (and/or downbeat) is detected within the first 100ms of the audio,\n\t        it is discarded. This is to avoid empty bins with no beat synced features in the first beat.\n\t        Args:\n\t            signal (AudioSignal): signal to beat track\n\t        Returns:\n\t            Tuple[np.ndarray, np.ndarray]: beats and downbeats\n", "        \"\"\"\n\t        beats, downbeats = self.extract_beats(signal)\n\t        return Beats(beats, downbeats)\n\tclass WaveBeat(BeatTracker):\n\t    def __init__(self, ckpt_path: str = \"checkpoints/wavebeat\", device: str = \"cpu\"):\n\t        from wavebeat.dstcn import dsTCNModel\n\t        model = dsTCNModel.load_from_checkpoint(ckpt_path, map_location=torch.device(device))\n\t        model.eval()\n\t        self.device = device\n\t        self.model = model\n", "    def extract_beats(self, signal: AudioSignal) -> Tuple[np.ndarray, np.ndarray]:\n\t        \"\"\"returns beat and downbeat times, in  seconds\"\"\"\n\t        # extract beats\n\t        beats, downbeats = self.model.predict_beats_from_array(\n\t            audio=signal.audio_data.squeeze(0),\n\t            sr=signal.sample_rate,\n\t            use_gpu=self.device != \"cpu\",\n\t        )\n\t        return beats, downbeats\n\tclass MadmomBeats(BeatTracker):\n", "    def __init__(self):\n\t        raise NotImplementedError\n\t    def extract_beats(self, signal: AudioSignal) -> Tuple[np.ndarray, np.ndarray]:\n\t        \"\"\"returns beat and downbeat times, in  seconds\"\"\"\n\t        pass\n\tBEAT_TRACKER_REGISTRY = {\n\t    \"wavebeat\": WaveBeat,\n\t    \"madmom\": MadmomBeats,\n\t}\n\tdef list_beat_trackers() -> list:\n", "    return list(BEAT_TRACKER_REGISTRY.keys())\n\tdef load_beat_tracker(beat_tracker: str, **kwargs) -> BeatTracker:\n\t    if beat_tracker not in BEAT_TRACKER_REGISTRY:\n\t        raise ValueError(\n\t            f\"Unknown beat tracker {beat_tracker}. Available: {list_beat_trackers()}\"\n\t        )\n\t    return BEAT_TRACKER_REGISTRY[beat_tracker](**kwargs)"]}
{"filename": "vampnet/util.py", "chunked_list": ["import tqdm\n\timport torch\n\tfrom einops import rearrange\n\tdef scalar_to_batch_tensor(x, batch_size):\n\t    return torch.tensor(x).repeat(batch_size)\n\tdef parallelize(\n\t        fn, \n\t        *iterables,\n\t        parallel: str = \"thread_map\",\n\t        **kwargs\n", "    ):\n\t    if parallel == \"thread_map\":\n\t        from tqdm.contrib.concurrent import thread_map\n\t        return thread_map(\n\t            fn, \n\t            *iterables, \n\t            **kwargs\n\t        )\n\t    elif parallel == \"process_map\":\n\t        from tqdm.contrib.concurrent import process_map\n", "        return process_map(\n\t            fn, \n\t            *iterables, \n\t            **kwargs\n\t        )\n\t    elif parallel == \"single\":\n\t        return [fn(x) for x in tqdm.tqdm(*iterables)]\n\t    else:\n\t        raise ValueError(f\"parallel must be one of 'thread_map', 'process_map', 'single', but got {parallel}\")\n\tdef codebook_flatten(tokens: torch.Tensor):\n", "    \"\"\" \n\t    flatten a sequence of tokens from (batch, codebook, time) to (batch, codebook * time)\n\t    \"\"\"\n\t    return rearrange(tokens, \"b c t -> b (t c)\")\n\tdef codebook_unflatten(flat_tokens: torch.Tensor, n_c: int = None):\n\t    \"\"\"\n\t    unflatten a sequence of tokens from (batch, codebook * time) to (batch, codebook, time)\n\t    \"\"\"\n\t    tokens = rearrange(flat_tokens, \"b (t c) -> b c t\", c=n_c)\n\t    return tokens\n"]}
{"filename": "vampnet/interface.py", "chunked_list": ["import os\n\tfrom pathlib import Path\n\timport math\n\timport torch\n\timport numpy as np\n\tfrom audiotools import AudioSignal\n\timport tqdm\n\tfrom .modules.transformer import VampNet\n\tfrom .beats import WaveBeat\n\tfrom .mask import *\n", "# from dac.model.dac import DAC\n\tfrom lac.model.lac import LAC as DAC\n\tdef signal_concat(\n\t    audio_signals: list,\n\t):\n\t    audio_data = torch.cat([x.audio_data for x in audio_signals], dim=-1)\n\t    return AudioSignal(audio_data, sample_rate=audio_signals[0].sample_rate)\n\tdef _load_model(\n\t    ckpt: str, \n\t    lora_ckpt: str = None,\n", "    device: str = \"cpu\",\n\t    chunk_size_s: int = 10,\n\t):\n\t    # we need to set strict to False if the model has lora weights to add later\n\t    model = VampNet.load(location=Path(ckpt), map_location=\"cpu\", strict=False)\n\t    # load lora weights if needed\n\t    if lora_ckpt is not None:\n\t        if not Path(lora_ckpt).exists():\n\t            should_cont = input(\n\t                f\"lora checkpoint {lora_ckpt} does not exist. continue? (y/n) \"\n", "            )\n\t            if should_cont != \"y\":\n\t                raise Exception(\"aborting\")\n\t        else:\n\t            model.load_state_dict(torch.load(lora_ckpt, map_location=\"cpu\"), strict=False)\n\t    model.to(device)\n\t    model.eval()\n\t    model.chunk_size_s = chunk_size_s\n\t    return model\n\tclass Interface(torch.nn.Module):\n", "    def __init__(\n\t        self,\n\t        coarse_ckpt: str = None,\n\t        coarse_lora_ckpt: str = None,\n\t        coarse2fine_ckpt: str = None,\n\t        coarse2fine_lora_ckpt: str = None,\n\t        codec_ckpt: str = None,\n\t        wavebeat_ckpt: str = None,\n\t        device: str = \"cpu\",\n\t        coarse_chunk_size_s: int =  10, \n", "        coarse2fine_chunk_size_s: int =  3,\n\t    ):\n\t        super().__init__()\n\t        assert codec_ckpt is not None, \"must provide a codec checkpoint\"\n\t        self.codec = DAC.load(Path(codec_ckpt))\n\t        self.codec.eval()\n\t        self.codec.to(device)\n\t        assert coarse_ckpt is not None, \"must provide a coarse checkpoint\"\n\t        self.coarse = _load_model(\n\t            ckpt=coarse_ckpt,\n", "            lora_ckpt=coarse_lora_ckpt,\n\t            device=device,\n\t            chunk_size_s=coarse_chunk_size_s,\n\t        )\n\t        # check if we have a coarse2fine ckpt\n\t        if coarse2fine_ckpt is not None:\n\t            self.c2f = _load_model(\n\t                ckpt=coarse2fine_ckpt,\n\t                lora_ckpt=coarse2fine_lora_ckpt,\n\t                device=device,\n", "                chunk_size_s=coarse2fine_chunk_size_s,\n\t            )\n\t        else:\n\t            self.c2f = None\n\t        if wavebeat_ckpt is not None:\n\t            print(f\"loading wavebeat from {wavebeat_ckpt}\")\n\t            self.beat_tracker = WaveBeat(wavebeat_ckpt)\n\t            self.beat_tracker.model.to(device)\n\t        else:\n\t            self.beat_tracker = None\n", "        self.device = device\n\t    def lora_load(\n\t        self, \n\t        coarse_ckpt: str = None,\n\t        c2f_ckpt: str = None,\n\t        full_ckpts: bool = False,\n\t    ):\n\t        if full_ckpts:\n\t            if coarse_ckpt is not None:\n\t                self.coarse = _load_model(\n", "                    ckpt=coarse_ckpt,  \n\t                    device=self.device,\n\t                    chunk_size_s=self.coarse.chunk_size_s,\n\t                )\n\t            if c2f_ckpt is not None:\n\t                self.c2f = _load_model(\n\t                    ckpt=c2f_ckpt,\n\t                    device=self.device,\n\t                    chunk_size_s=self.c2f.chunk_size_s,\n\t                )\n", "        else:\n\t            if coarse_ckpt is not None:\n\t                self.coarse.to(\"cpu\")\n\t                state_dict = torch.load(coarse_ckpt, map_location=\"cpu\")\n\t                print(f\"loading coarse from {coarse_ckpt}\")\n\t                self.coarse.load_state_dict(state_dict, strict=False)\n\t                self.coarse.to(self.device)\n\t            if c2f_ckpt is not None:\n\t                self.c2f.to(\"cpu\")\n\t                state_dict = torch.load(c2f_ckpt, map_location=\"cpu\")\n", "                print(f\"loading c2f from {c2f_ckpt}\")\n\t                self.c2f.load_state_dict(state_dict, strict=False)\n\t                self.c2f.to(self.device)\n\t    def s2t(self, seconds: float):\n\t        \"\"\"seconds to tokens\"\"\"\n\t        if isinstance(seconds, np.ndarray):\n\t            return np.ceil(seconds * self.codec.sample_rate / self.codec.hop_length)\n\t        else:\n\t            return math.ceil(seconds * self.codec.sample_rate / self.codec.hop_length)\n\t    def s2t2s(self, seconds: float):\n", "        \"\"\"seconds to tokens to seconds\"\"\"\n\t        return self.t2s(self.s2t(seconds))\n\t    def t2s(self, tokens: int):\n\t        \"\"\"tokens to seconds\"\"\"\n\t        return tokens * self.codec.hop_length / self.codec.sample_rate\n\t    def to(self, device):\n\t        self.device = device\n\t        self.coarse.to(device)\n\t        self.codec.to(device)\n\t        if self.c2f is not None:\n", "            self.c2f.to(device)\n\t        if self.beat_tracker is not None:\n\t            self.beat_tracker.model.to(device)\n\t        return self\n\t    def to_signal(self, z: torch.Tensor):\n\t        return self.coarse.to_signal(z, self.codec)\n\t    def preprocess(self, signal: AudioSignal):\n\t        signal = (\n\t            signal.clone()\n\t            .resample(self.codec.sample_rate)\n", "            .to_mono()\n\t            .normalize(-24)\n\t            .ensure_max_of_audio(1.0)\n\t        )\n\t        return signal\n\t    @torch.inference_mode()\n\t    def encode(self, signal: AudioSignal):\n\t        signal = self.preprocess(signal).to(self.device)\n\t        z = self.codec.encode(signal.samples, signal.sample_rate)[\"codes\"]\n\t        return z\n", "    def snap_to_beats(\n\t        self, \n\t        signal: AudioSignal\n\t    ):\n\t        assert hasattr(self, \"beat_tracker\"), \"No beat tracker loaded\"\n\t        beats, downbeats = self.beat_tracker.extract_beats(signal)\n\t        # trim the signa around the first beat time\n\t        samples_begin = int(beats[0] * signal.sample_rate )\n\t        samples_end = int(beats[-1] * signal.sample_rate)\n\t        print(beats[0])\n", "        signal = signal.clone().trim(samples_begin, signal.length - samples_end)\n\t        return signal\n\t    def make_beat_mask(self, \n\t            signal: AudioSignal, \n\t            before_beat_s: float = 0.0,\n\t            after_beat_s: float = 0.02,\n\t            mask_downbeats: bool = True,\n\t            mask_upbeats: bool = True,\n\t            downbeat_downsample_factor: int = None,\n\t            beat_downsample_factor: int = None,\n", "            dropout: float = 0.0,\n\t            invert: bool = True,\n\t    ):\n\t        \"\"\"make a beat synced mask. that is, make a mask that \n\t        places 1s at and around the beat, and 0s everywhere else. \n\t        \"\"\"\n\t        assert self.beat_tracker is not None, \"No beat tracker loaded\"\n\t        # get the beat times\n\t        beats, downbeats = self.beat_tracker.extract_beats(signal)\n\t        # get the beat indices in z\n", "        beats_z, downbeats_z = self.s2t(beats), self.s2t(downbeats)\n\t        # remove downbeats from beats\n\t        beats_z = torch.tensor(beats_z)[~torch.isin(torch.tensor(beats_z), torch.tensor(downbeats_z))]\n\t        beats_z = beats_z.tolist()\n\t        downbeats_z = downbeats_z.tolist()\n\t        # make the mask \n\t        seq_len = self.s2t(signal.duration)\n\t        mask = torch.zeros(seq_len, device=self.device)\n\t        mask_b4 = self.s2t(before_beat_s)\n\t        mask_after = self.s2t(after_beat_s)\n", "        if beat_downsample_factor is not None:\n\t            if beat_downsample_factor < 1:\n\t                raise ValueError(\"mask_beat_downsample_factor must be >= 1 or None\")\n\t        else:\n\t            beat_downsample_factor = 1\n\t        if downbeat_downsample_factor is not None:\n\t            if downbeat_downsample_factor < 1:\n\t                raise ValueError(\"mask_beat_downsample_factor must be >= 1 or None\")\n\t        else:\n\t            downbeat_downsample_factor = 1\n", "        beats_z = beats_z[::beat_downsample_factor]\n\t        downbeats_z = downbeats_z[::downbeat_downsample_factor]\n\t        print(f\"beats_z: {len(beats_z)}\")\n\t        print(f\"downbeats_z: {len(downbeats_z)}\")\n\t        if mask_upbeats:\n\t            for beat_idx in beats_z:\n\t                _slice = int(beat_idx - mask_b4), int(beat_idx + mask_after)\n\t                num_steps = mask[_slice[0]:_slice[1]].shape[0]\n\t                _m = torch.ones(num_steps, device=self.device)\n\t                _m_mask = torch.bernoulli(_m * (1 - dropout))\n", "                _m = _m * _m_mask.long()\n\t                mask[_slice[0]:_slice[1]] = _m\n\t        if mask_downbeats:\n\t            for downbeat_idx in downbeats_z:\n\t                _slice = int(downbeat_idx - mask_b4), int(downbeat_idx + mask_after)\n\t                num_steps = mask[_slice[0]:_slice[1]].shape[0]\n\t                _m = torch.ones(num_steps, device=self.device)\n\t                _m_mask = torch.bernoulli(_m * (1 - dropout))\n\t                _m = _m * _m_mask.long()\n\t                mask[_slice[0]:_slice[1]] = _m\n", "        mask = mask.clamp(0, 1)\n\t        if invert:\n\t            mask = 1 - mask\n\t        mask = mask[None, None, :].bool().long()\n\t        if self.c2f is not None:\n\t            mask = mask.repeat(1, self.c2f.n_codebooks, 1)\n\t        else:\n\t            mask = mask.repeat(1, self.coarse.n_codebooks, 1)\n\t        return mask\n\t    def coarse_to_fine(\n", "        self, \n\t        z: torch.Tensor,\n\t        mask: torch.Tensor = None,\n\t        **kwargs\n\t    ):\n\t        assert self.c2f is not None, \"No coarse2fine model loaded\"\n\t        length = z.shape[-1]\n\t        chunk_len = self.s2t(self.c2f.chunk_size_s)\n\t        n_chunks = math.ceil(z.shape[-1] / chunk_len)\n\t        # zero pad to chunk_len\n", "        if length % chunk_len != 0:\n\t            pad_len = chunk_len - (length % chunk_len)\n\t            z = torch.nn.functional.pad(z, (0, pad_len))\n\t            mask = torch.nn.functional.pad(mask, (0, pad_len)) if mask is not None else None\n\t        n_codebooks_to_append = self.c2f.n_codebooks - z.shape[1]\n\t        if n_codebooks_to_append > 0:\n\t            z = torch.cat([\n\t                z,\n\t                torch.zeros(z.shape[0], n_codebooks_to_append, z.shape[-1]).long().to(self.device)\n\t            ], dim=1)\n", "        # set the mask to 0 for all conditioning codebooks\n\t        if mask is not None:\n\t            mask = mask.clone()\n\t            mask[:, :self.c2f.n_conditioning_codebooks, :] = 0\n\t        fine_z = []\n\t        for i in range(n_chunks):\n\t            chunk = z[:, :, i * chunk_len : (i + 1) * chunk_len]\n\t            mask_chunk = mask[:, :, i * chunk_len : (i + 1) * chunk_len] if mask is not None else None\n\t            chunk = self.c2f.generate(\n\t                codec=self.codec,\n", "                time_steps=chunk_len,\n\t                start_tokens=chunk,\n\t                return_signal=False,\n\t                mask=mask_chunk,\n\t                **kwargs\n\t            )\n\t            fine_z.append(chunk)\n\t        fine_z = torch.cat(fine_z, dim=-1)\n\t        return fine_z[:, :, :length].clone()\n\t    def coarse_vamp(\n", "        self, \n\t        z, \n\t        mask,\n\t        return_mask=False,\n\t        gen_fn=None,\n\t        **kwargs\n\t    ):\n\t        # coarse z\n\t        cz = z[:, : self.coarse.n_codebooks, :].clone()\n\t        assert cz.shape[-1] <= self.s2t(self.coarse.chunk_size_s), f\"the sequence of tokens provided must match the one specified in the coarse chunk size, but got {cz.shape[-1]} and {self.s2t(self.coarse.chunk_size_s)}\"\n", "        mask = mask[:, : self.coarse.n_codebooks, :]\n\t        cz_masked, mask = apply_mask(cz, mask, self.coarse.mask_token)\n\t        cz_masked = cz_masked[:, : self.coarse.n_codebooks, :]\n\t        gen_fn = gen_fn or self.coarse.generate\n\t        c_vamp = gen_fn(\n\t            codec=self.codec,\n\t            time_steps=cz.shape[-1],\n\t            start_tokens=cz,\n\t            mask=mask, \n\t            return_signal=False,\n", "            **kwargs\n\t        )\n\t        # add the fine codes back in\n\t        c_vamp = torch.cat(\n\t            [c_vamp, z[:, self.coarse.n_codebooks :, :]], \n\t            dim=1\n\t        )\n\t        if return_mask:\n\t            return c_vamp, cz_masked\n\t        return c_vamp\n", "if __name__ == \"__main__\":\n\t    import audiotools as at\n\t    import logging\n\t    logger = logging.getLogger()\n\t    logger.setLevel(logging.INFO)\n\t    torch.set_printoptions(threshold=10000)\n\t    at.util.seed(42)\n\t    interface = Interface(\n\t        coarse_ckpt=\"./models/vampnet/coarse.pth\", \n\t        coarse2fine_ckpt=\"./models/vampnet/c2f.pth\", \n", "        codec_ckpt=\"./models/vampnet/codec.pth\",\n\t        device=\"cuda\", \n\t        wavebeat_ckpt=\"./models/wavebeat.pth\"\n\t    )\n\t    sig = at.AudioSignal('assets/example.wav')\n\t    z = interface.encode(sig)\n\t    breakpoint()\n\t    # mask = linear_random(z, 1.0)\n\t    # mask = mask_and(\n\t    #     mask, periodic_mask(\n", "    #         z,\n\t    #         32,\n\t    #         1,\n\t    #         random_roll=True\n\t    #     )\n\t    # )\n\t    # mask = interface.make_beat_mask(\n\t    #     sig, 0.0, 0.075\n\t    # )\n\t    # mask = dropout(mask, 0.0)\n", "    # mask = codebook_unmask(mask, 0)\n\t    mask = inpaint(z, n_prefix=100, n_suffix=100)\n\t    zv, mask_z = interface.coarse_vamp(\n\t        z, \n\t        mask=mask,\n\t        sampling_steps=36,\n\t        temperature=8.0,\n\t        return_mask=True, \n\t        gen_fn=interface.coarse.generate\n\t    )\n", "    use_coarse2fine = True\n\t    if use_coarse2fine: \n\t        zv = interface.coarse_to_fine(zv, temperature=0.8, mask=mask)\n\t        breakpoint()\n\t    mask = interface.to_signal(mask_z).cpu()\n\t    sig = interface.to_signal(zv).cpu()\n\t    print(\"done\")\n"]}
{"filename": "vampnet/modules/layers.py", "chunked_list": ["import time\n\tfrom typing import Optional\n\tfrom typing import Tuple\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom einops import rearrange\n\tfrom torch.nn.utils import weight_norm\n\t# Scripting this brings model speed up 1.4x\n\t@torch.jit.script\n", "def snake(x, alpha):\n\t    shape = x.shape\n\t    x = x.reshape(shape[0], shape[1], -1)\n\t    x = x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2)\n\t    x = x.reshape(shape)\n\t    return x\n\tclass Snake1d(nn.Module):\n\t    def __init__(self, channels):\n\t        super().__init__()\n\t        self.alpha = nn.Parameter(torch.ones(1, channels, 1))\n", "    def forward(self, x):\n\t        return snake(x, self.alpha)\n\tdef num_params(model):\n\t    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\tdef recurse_children(module, fn):\n\t    for child in module.children():\n\t        if isinstance(child, nn.ModuleList):\n\t            for c in child:\n\t                yield recurse_children(c, fn)\n\t        if isinstance(child, nn.ModuleDict):\n", "            for c in child.values():\n\t                yield recurse_children(c, fn)\n\t        yield recurse_children(child, fn)\n\t        yield fn(child)\n\tdef WNConv1d(*args, **kwargs):\n\t    return weight_norm(nn.Conv1d(*args, **kwargs))\n\tdef WNConvTranspose1d(*args, **kwargs):\n\t    return weight_norm(nn.ConvTranspose1d(*args, **kwargs))\n\tclass SequentialWithFiLM(nn.Module):\n\t    \"\"\"\n", "    handy wrapper for nn.Sequential that allows FiLM layers to be\n\t    inserted in between other layers.\n\t    \"\"\"\n\t    def __init__(self, *layers):\n\t        super().__init__()\n\t        self.layers = nn.ModuleList(layers)\n\t    @staticmethod\n\t    def has_film(module):\n\t        mod_has_film = any(\n\t            [res for res in recurse_children(module, lambda c: isinstance(c, FiLM))]\n", "        )\n\t        return mod_has_film\n\t    def forward(self, x, cond):\n\t        for layer in self.layers:\n\t            if self.has_film(layer):\n\t                x = layer(x, cond)\n\t            else:\n\t                x = layer(x)\n\t        return x\n\tclass FiLM(nn.Module):\n", "    def __init__(self, input_dim: int, output_dim: int):\n\t        super().__init__()\n\t        self.input_dim = input_dim\n\t        self.output_dim = output_dim\n\t        if input_dim > 0:\n\t            self.beta = nn.Linear(input_dim, output_dim)\n\t            self.gamma = nn.Linear(input_dim, output_dim)\n\t    def forward(self, x, r):\n\t        if self.input_dim == 0:\n\t            return x\n", "        else:\n\t            beta, gamma = self.beta(r), self.gamma(r)\n\t            beta, gamma = (\n\t                beta.view(x.size(0), self.output_dim, 1),\n\t                gamma.view(x.size(0), self.output_dim, 1),\n\t            )\n\t            x = x * (gamma + 1) + beta\n\t        return x\n\tclass CodebookEmbedding(nn.Module):\n\t    def __init__(\n", "        self,\n\t        vocab_size: int,\n\t        latent_dim: int,\n\t        n_codebooks: int,\n\t        emb_dim: int,\n\t        special_tokens: Optional[Tuple[str]] = None,\n\t    ):\n\t        super().__init__()\n\t        self.n_codebooks = n_codebooks\n\t        self.emb_dim = emb_dim\n", "        self.latent_dim = latent_dim\n\t        self.vocab_size = vocab_size\n\t        if special_tokens is not None:\n\t            for tkn in special_tokens:\n\t                self.special = nn.ParameterDict(\n\t                    {\n\t                        tkn: nn.Parameter(torch.randn(n_codebooks, self.latent_dim))\n\t                        for tkn in special_tokens\n\t                    }\n\t                )\n", "                self.special_idxs = {\n\t                    tkn: i + vocab_size for i, tkn in enumerate(special_tokens)\n\t                }\n\t        self.out_proj = nn.Conv1d(n_codebooks * self.latent_dim, self.emb_dim, 1)\n\t    def from_codes(self, codes: torch.Tensor, codec):\n\t        \"\"\" \n\t        get a sequence of continuous embeddings from a sequence of discrete codes. \n\t        unlike it's counterpart in the original VQ-VAE, this function adds for any special tokens\n\t        necessary for the language model, like <MASK>. \n\t        \"\"\"\n", "        n_codebooks = codes.shape[1]\n\t        latent = []\n\t        for i in range(n_codebooks):\n\t            c = codes[:, i, :]\n\t            lookup_table = codec.quantizer.quantizers[i].codebook.weight\n\t            if hasattr(self, \"special\"):\n\t                special_lookup = torch.cat(\n\t                    [self.special[tkn][i : i + 1] for tkn in self.special], dim=0\n\t                )\n\t                lookup_table = torch.cat([lookup_table, special_lookup], dim=0)\n", "            l = F.embedding(c, lookup_table).transpose(1, 2)\n\t            latent.append(l)\n\t        latent = torch.cat(latent, dim=1)\n\t        return latent\n\t    def forward(self, latents: torch.Tensor):\n\t        \"\"\"\n\t        project a sequence of latents to a sequence of embeddings\n\t        \"\"\"\n\t        x = self.out_proj(latents)\n\t        return x\n"]}
{"filename": "vampnet/modules/transformer.py", "chunked_list": ["import math\n\timport logging\n\tfrom typing import Optional, Tuple, Union\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom einops import rearrange\n\timport loralib as lora\n\timport audiotools as at\n", "from .activations import get_activation\n\tfrom .layers import CodebookEmbedding\n\tfrom .layers import FiLM\n\tfrom .layers import SequentialWithFiLM\n\tfrom .layers import WNConv1d\n\tfrom ..util import scalar_to_batch_tensor, codebook_flatten, codebook_unflatten\n\tfrom ..mask import _gamma\n\tLORA_R = 8\n\t# def log(t, eps=1e-20):\n\t#     return torch.log(t + eps)\n", "def gumbel_noise_like(t):\n\t    noise = torch.zeros_like(t).uniform_(1e-20, 1)\n\t    return -torch.log(-torch.log(noise))\n\tdef gumbel_sample(t, temperature=1.0, dim=-1):\n\t    return ((t / max(temperature, 1e-10)) + gumbel_noise_like(t)).argmax(dim=dim)\n\tclass RMSNorm(nn.Module):\n\t    def __init__(self, hidden_size: int, eps=1e-6):\n\t        super().__init__()\n\t        self.weight = nn.Parameter(torch.ones(hidden_size))\n\t        self.var_eps = eps\n", "    def forward(self, x):\n\t        \"\"\"Returns root mean square normalized version of input `x`\n\t        # T5 uses a layer_norm which only scales and doesn't shift, which is also known\n\t        # as Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467\n\t        # thus varience is calculated w/o mean and there is no bias\n\t        Parameters\n\t        ----------\n\t        x : Tensor[B x T x D]\n\t        Returns\n\t        -------\n", "        Tensor[B x T x D]\n\t        \"\"\"\n\t        var = x.pow(2).mean(-1, keepdim=True)\n\t        x = x * torch.rsqrt(var + self.var_eps)\n\t        return self.weight * x\n\tclass FeedForward(nn.Module):\n\t    def __init__(\n\t        self, d_model: int = 512, dropout: float = 0.1, activation: str = \"geglu\"\n\t    ):\n\t        super().__init__()\n", "        factor = 2 if activation == \"geglu\" else 1\n\t        self.w_1 = lora.Linear(d_model, d_model * 4, bias=False, r=LORA_R)\n\t        self.w_2 = lora.Linear(d_model * 4 // factor, d_model, bias=False, r=LORA_R)\n\t        self.drop = nn.Dropout(dropout)\n\t        self.act = get_activation(activation)()\n\t    def forward(self, x):\n\t        \"\"\"Computes position-wise feed-forward layer\n\t        Parameters\n\t        ----------\n\t        x : Tensor[B x T x D]\n", "        Returns\n\t        -------\n\t        Tensor[B x T x D]\n\t        \"\"\"\n\t        x = self.w_1(x)\n\t        x = self.act(x)\n\t        x = self.drop(x)\n\t        x = self.w_2(x)\n\t        return x\n\tclass MultiHeadRelativeAttention(nn.Module):\n", "    def __init__(\n\t        self,\n\t        n_head: int = 8,\n\t        d_model: int = 512,\n\t        dropout: float = 0.1,\n\t        bidirectional: bool = True,\n\t        has_relative_attention_bias: bool = True,\n\t        attention_num_buckets: int = 32,\n\t        attention_max_distance: int = 128,\n\t    ):\n", "        super().__init__()\n\t        d_head = d_model // n_head\n\t        self.n_head = n_head\n\t        self.d_head = d_head\n\t        self.bidirectional = bidirectional\n\t        self.has_relative_attention_bias = has_relative_attention_bias\n\t        self.attention_num_buckets = attention_num_buckets\n\t        self.attention_max_distance = attention_max_distance\n\t        # Create linear query, key, value projections\n\t        self.w_qs = lora.Linear(d_model, d_model, bias=False, r=LORA_R)\n", "        self.w_ks = nn.Linear(d_model, d_model, bias=False)\n\t        self.w_vs = lora.Linear(d_model, d_model, bias=False, r=LORA_R)\n\t        # Create linear final output projection\n\t        self.fc = lora.Linear(d_model, d_model, bias=False, r=LORA_R)\n\t        # Dropout for attention output weights\n\t        self.dropout = nn.Dropout(dropout)\n\t        # Create relative positional embeddings (if turned on)\n\t        if has_relative_attention_bias:\n\t            self.relative_attention_bias = nn.Embedding(attention_num_buckets, n_head)\n\t    def _relative_position_bucket(self, relative_position):\n", "        \"\"\"Converts unbounded relative position into bounded set of buckets\n\t        with half \"exact\" buckets (1 position = 1 bucket) and half \"log-spaced\"\n\t        buckets\n\t        Parameters\n\t        ----------\n\t        relative_position : Tensor[T_q x T_kv]\n\t            Relative positions between queries and key_value items\n\t        Returns\n\t        -------\n\t        Tensor[T_q x T_kv]\n", "            Input relative positions converted into buckets\n\t        \"\"\"\n\t        relative_buckets = 0\n\t        num_buckets = self.attention_num_buckets\n\t        max_distance = self.attention_max_distance\n\t        # Convert relative position for (-inf, inf) to [0, inf]\n\t        # Negative relative positions correspond to past\n\t        # Positive relative positions correspond to future\n\t        if self.bidirectional:\n\t            # use half buckets for each side (past / future)\n", "            num_buckets //= 2\n\t            # Shift the position positions by `num_buckets` to wrap around\n\t            # negative positions\n\t            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n\t            relative_position = torch.abs(relative_position)\n\t        else:\n\t            # If not bidirectional, ignore positive positions and wrap\n\t            # negative positions to positive\n\t            relative_position = -torch.min(\n\t                relative_position, torch.zeros_like(relative_position)\n", "            )\n\t        # Allocate half of the buckets are for exact increments in positions\n\t        max_exact = num_buckets // 2\n\t        is_small = relative_position < max_exact\n\t        # The other half of the buckets are for logarithmically bigger bins in\n\t        # positions up to `max_distance`\n\t        relative_postion_if_large = max_exact + (\n\t            torch.log(relative_position.float() / max_exact)\n\t            / math.log(max_distance / max_exact)\n\t            * (num_buckets - max_exact)\n", "        ).to(torch.long)\n\t        # Clip the max relative position to `num_buckets - 1`\n\t        relative_postion_if_large = torch.min(\n\t            relative_postion_if_large,\n\t            torch.full_like(relative_postion_if_large, num_buckets - 1),\n\t        )\n\t        # Choose relative buckets based on small or large positions\n\t        relative_buckets += torch.where(\n\t            is_small, relative_position, relative_postion_if_large\n\t        )\n", "        return relative_buckets\n\t    def compute_bias(self, query_length, key_length):\n\t        \"\"\"Computes a position bias scalar for each index in query_length x key_length\n\t        Parameters\n\t        ----------\n\t        query_length : int\n\t        key_length : int\n\t        Returns\n\t        -------\n\t        Tensor[heads x 1 x T_q x T_kv]\n", "            Position bias to be applied on attention logits\n\t        \"\"\"\n\t        query_position = torch.arange(query_length, dtype=torch.long)[:, None]\n\t        key_position = torch.arange(key_length, dtype=torch.long)[None, :]\n\t        relative_position = key_position - query_position\n\t        # Convert relative position to buckets\n\t        relative_position_bucket = self._relative_position_bucket(relative_position)\n\t        relative_position_bucket = relative_position_bucket.to(\n\t            self.relative_attention_bias.weight.device\n\t        )\n", "        # Index attention bias values\n\t        values = self.relative_attention_bias(relative_position_bucket)\n\t        values = rearrange(values, \"q k h -> h 1 q k\")\n\t        return values\n\t    def forward(self, q, k, v, mask=None, position_bias=None):\n\t        \"\"\"Computes attention over (keys, values) for every timestep in query\n\t        Parameters\n\t        ----------\n\t        q : Tensor[B x T_q x d_model]\n\t            Query vectors\n", "        k : Tensor[B x T_kv x d_model]\n\t            Key vectors to compute attention over\n\t        v : Tensor[B x T_kv x d_model]\n\t            Value vectors corresponding to the keys\n\t        mask : Tensor[B x T_q x T_kv], optional\n\t        position_bias: Tensor[head x 1 x T_q x T_kv]\n\t        Returns\n\t        -------\n\t        Tensor[B x T_q x d_model]\n\t            Outputs after attending (key, value) using queries\n", "        \"\"\"\n\t        # Compute query, key, value projections\n\t        q = rearrange(self.w_qs(q), \"b l (head k) -> head b l k\", head=self.n_head)\n\t        k = rearrange(self.w_ks(k), \"b t (head k) -> head b t k\", head=self.n_head)\n\t        v = rearrange(self.w_vs(v), \"b t (head k) -> head b t k\", head=self.n_head)\n\t        # Compute attention matrix\n\t        attn = torch.einsum(\"hblk,hbtk->hblt\", [q, k]) / np.sqrt(q.shape[-1])\n\t        # Add relative position bias to attention scores\n\t        if position_bias is None:\n\t            if self.has_relative_attention_bias:\n", "                position_bias = self.compute_bias(q.size(-2), k.size(-2))\n\t            else:\n\t                position_bias = torch.zeros_like(attn)\n\t        attn += position_bias\n\t        # Apply mask to attention scores to prevent looking up invalid locations\n\t        if mask is not None:\n\t            attn = attn.masked_fill(mask[None] == 0, -1e9)\n\t        # Normalize attention scores and add dropout\n\t        attn = torch.softmax(attn, dim=3)\n\t        attn = self.dropout(attn)\n", "        # Compute attended outputs (product of attention matrix and values)\n\t        output = torch.einsum(\"hblt,hbtv->hblv\", [attn, v])\n\t        output = rearrange(output, \"head b l v -> b l (head v)\")\n\t        output = self.fc(output)\n\t        return output, position_bias\n\tclass TransformerLayer(nn.Module):\n\t    def __init__(\n\t        self,\n\t        d_model: int = 512,\n\t        d_cond: int = 64,\n", "        n_heads: int = 8,\n\t        bidirectional: bool = True,\n\t        is_decoder: bool = False,\n\t        has_relative_attention_bias: bool = False,\n\t        flash_attn: bool = False,\n\t        dropout: float = 0.1,\n\t    ):\n\t        super().__init__()\n\t        # Store args\n\t        self.is_decoder = is_decoder\n", "        # Create self-attention layer\n\t        self.norm_1 = RMSNorm(d_model)\n\t        self.film_1 = FiLM(d_cond, d_model)\n\t        self.flash_attn = flash_attn\n\t        if flash_attn:\n\t            from flash_attn.flash_attention import FlashMHA\n\t            self.self_attn = FlashMHA(\n\t                embed_dim=d_model,\n\t                num_heads=n_heads,\n\t                attention_dropout=dropout,\n", "                causal=False,\n\t            )\n\t        else:\n\t            self.self_attn = MultiHeadRelativeAttention(\n\t                n_heads, d_model, dropout, bidirectional, has_relative_attention_bias\n\t            )\n\t        # (Optional) Create cross-attention layer\n\t        if is_decoder:\n\t            self.norm_2 = RMSNorm(d_model)\n\t            self.film_2 = FiLM(d_cond, d_model)\n", "            self.cross_attn = MultiHeadRelativeAttention(\n\t                n_heads,\n\t                d_model,\n\t                dropout,\n\t                bidirectional=True,\n\t                has_relative_attention_bias=False,\n\t            )\n\t        # Create last feed-forward layer\n\t        self.norm_3 = RMSNorm(d_model)\n\t        self.film_3 = FiLM(d_cond, d_model)\n", "        self.feed_forward = FeedForward(d_model=d_model, dropout=dropout)\n\t        # Create dropout\n\t        self.dropout = nn.Dropout(dropout)\n\t    def forward(\n\t        self,\n\t        x,\n\t        x_mask,\n\t        cond,\n\t        src=None,\n\t        src_mask=None,\n", "        position_bias=None,\n\t        encoder_decoder_position_bias=None,\n\t    ):\n\t        \"\"\"Computes one transformer layer consisting of self attention, (op) cross attention\n\t        and feedforward layer\n\t        Parameters\n\t        ----------\n\t        x : Tensor[B x T_q x D]\n\t        x_mask : Tensor[B x T_q]\n\t        src : Tensor[B x T_kv x D], optional\n", "        src_mask : Tensor[B x T_kv x D], optional\n\t        position_bias : Tensor[heads x B x T_q x T_q], optional\n\t            Relative position bias for self attention layer\n\t        encoder_decoder_position_bias : Tensor[heads x B x T_q x T_kv], optional\n\t            Relative position bias for cross attention layer\n\t        Returns\n\t        -------\n\t        Tensor[B x T_q x D]\n\t        \"\"\"\n\t        y = self.norm_1(x)\n", "        y = self.film_1(y.permute(0, 2, 1), cond).permute(0, 2, 1)\n\t        if self.flash_attn:\n\t            with torch.autocast(y.device.type, dtype=torch.bfloat16):\n\t                y = self.self_attn(y)[0]\n\t        else:\n\t            y, position_bias = self.self_attn(y, y, y, x_mask, position_bias)\n\t        x = x + self.dropout(y)\n\t        if self.is_decoder:\n\t            y = self.norm_2(x)\n\t            y = self.film_2(y.permute(0, 2, 1), cond).permute(0, 2, 1)\n", "            y, encoder_decoder_position_bias = self.cross_attn(\n\t                y, src, src, src_mask, encoder_decoder_position_bias\n\t            )\n\t            x = x + self.dropout(y)\n\t        y = self.norm_3(x)\n\t        y = self.film_3(\n\t            y.permute(\n\t                0,\n\t                2,\n\t                1,\n", "            ),\n\t            cond,\n\t        ).permute(0, 2, 1)\n\t        y = self.feed_forward(y)\n\t        x = x + self.dropout(y)\n\t        return x, position_bias, encoder_decoder_position_bias\n\tclass TransformerStack(nn.Module):\n\t    def __init__(\n\t        self,\n\t        d_model: int = 512,\n", "        d_cond: int = 64,\n\t        n_heads: int = 8,\n\t        n_layers: int = 8,\n\t        last_layer: bool = True,\n\t        bidirectional: bool = True,\n\t        flash_attn: bool = False,\n\t        is_decoder: bool = False,\n\t        dropout: float = 0.1,\n\t    ):\n\t        super().__init__()\n", "        # Store args\n\t        self.bidirectional = bidirectional\n\t        self.is_decoder = is_decoder\n\t        # Create transformer layers\n\t        # In T5, relative attention bias is shared by all layers in the stack\n\t        self.layers = nn.ModuleList(\n\t            [\n\t                TransformerLayer(\n\t                    d_model,\n\t                    d_cond,\n", "                    n_heads,\n\t                    bidirectional,\n\t                    is_decoder,\n\t                    has_relative_attention_bias=True if (i == 0) else False,\n\t                    flash_attn=flash_attn,\n\t                    dropout=dropout,\n\t                )\n\t                for i in range(n_layers)\n\t            ]\n\t        )\n", "        # Perform last normalization\n\t        self.norm = RMSNorm(d_model) if last_layer else None\n\t    def subsequent_mask(self, size):\n\t        return torch.ones(1, size, size).tril().bool()\n\t    def forward(self, x, x_mask, cond=None, src=None, src_mask=None):\n\t        \"\"\"Computes a full transformer stack\n\t        Parameters\n\t        ----------\n\t        x : Tensor[B x T_q x D]\n\t        x_mask : Tensor[B x T_q]\n", "        src : Tensor[B x T_kv x D], optional\n\t        src_mask : Tensor[B x T_kv], optional\n\t        Returns\n\t        -------\n\t        Tensor[B x T_q x D]\n\t        \"\"\"\n\t        # Convert `src_mask` to (B x T_q x T_kv) shape for cross attention masking\n\t        if self.is_decoder:\n\t            src_mask = x_mask.unsqueeze(-1) * src_mask.unsqueeze(-2)\n\t        # Convert `x_mask` to (B x T_q x T_q) shape for self attention masking\n", "        x_mask = x_mask.unsqueeze(-2)\n\t        if not self.bidirectional:\n\t            x_mask = x_mask * self.subsequent_mask(x.size(1)).to(x_mask.device)\n\t        # Initialize position biases\n\t        position_bias = None\n\t        encoder_decoder_position_bias = None\n\t        # Compute transformer layers\n\t        for layer in self.layers:\n\t            x, position_bias, encoder_decoder_position_bias = layer(\n\t                x=x,\n", "                x_mask=x_mask,\n\t                cond=cond,\n\t                src=src,\n\t                src_mask=src_mask,\n\t                position_bias=position_bias,\n\t                encoder_decoder_position_bias=encoder_decoder_position_bias,\n\t            )\n\t        return self.norm(x) if self.norm is not None else x\n\tclass VampNet(at.ml.BaseModel):\n\t    def __init__(\n", "        self,\n\t        n_heads: int = 20,\n\t        n_layers: int = 16,\n\t        r_cond_dim: int = 0,\n\t        n_codebooks: int = 9,\n\t        n_conditioning_codebooks: int = 0,\n\t        latent_dim: int = 8,\n\t        embedding_dim: int = 1280,\n\t        vocab_size: int = 1024,\n\t        flash_attn: bool = True,\n", "        noise_mode: str = \"mask\",\n\t        dropout: float = 0.1\n\t    ):\n\t        super().__init__()\n\t        assert r_cond_dim == 0, f\"r_cond_dim must be 0 (not supported), but got {r_cond_dim}\"\n\t        self.n_heads = n_heads\n\t        self.n_layers = n_layers\n\t        self.r_cond_dim = r_cond_dim\n\t        self.n_codebooks = n_codebooks\n\t        self.n_conditioning_codebooks = n_conditioning_codebooks\n", "        self.embedding_dim = embedding_dim\n\t        self.vocab_size = vocab_size\n\t        self.latent_dim = latent_dim\n\t        self.flash_attn = flash_attn\n\t        self.noise_mode = noise_mode\n\t        assert self.noise_mode == \"mask\", \"deprecated\"\n\t        self.embedding = CodebookEmbedding(\n\t            latent_dim=latent_dim,\n\t            n_codebooks=n_codebooks,\n\t            vocab_size=vocab_size,\n", "            emb_dim=embedding_dim,\n\t            special_tokens=[\"MASK\"],\n\t        )\n\t        self.mask_token = self.embedding.special_idxs[\"MASK\"]\n\t        self.transformer = TransformerStack(\n\t            d_model=embedding_dim,\n\t            d_cond=r_cond_dim,\n\t            n_heads=n_heads,\n\t            n_layers=n_layers,\n\t            last_layer=True,\n", "            bidirectional=True,\n\t            flash_attn=flash_attn,\n\t            is_decoder=False,\n\t            dropout=dropout,\n\t        )\n\t        # Add final conv layer\n\t        self.n_predict_codebooks = n_codebooks - n_conditioning_codebooks\n\t        self.classifier = SequentialWithFiLM(\n\t            WNConv1d(\n\t                embedding_dim,\n", "                vocab_size * self.n_predict_codebooks,\n\t                kernel_size=1,\n\t                padding=\"same\",\n\t                # groups=self.n_predict_codebooks,\n\t            ),\n\t        )\n\t    def forward(self, x):\n\t        x = self.embedding(x)\n\t        x_mask = torch.ones_like(x, dtype=torch.bool)[:, :1, :].squeeze(1)\n\t        x = rearrange(x, \"b d n -> b n d\")\n", "        out = self.transformer(x=x, x_mask=x_mask)\n\t        out = rearrange(out, \"b n d -> b d n\")\n\t        out = self.classifier(out, None) # no cond here!\n\t        out = rearrange(out, \"b (p c) t -> b p (t c)\", c=self.n_predict_codebooks)\n\t        return out\n\t    def r_embed(self, r, max_positions=10000):\n\t        if self.r_cond_dim > 0:\n\t            dtype = r.dtype\n\t            r = _gamma(r) * max_positions\n\t            half_dim = self.r_cond_dim // 2\n", "            emb = math.log(max_positions) / (half_dim - 1)\n\t            emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n\t            emb = r[:, None] * emb[None, :]\n\t            emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n\t            if self.r_cond_dim % 2 == 1:  # zero pad\n\t                emb = nn.functional.pad(emb, (0, 1), mode=\"constant\")\n\t            return emb.to(dtype)\n\t        else:\n\t            return r\n\t    @torch.no_grad()\n", "    def to_signal(self, z, codec):\n\t        \"\"\"\n\t        convert a sequence of latents to a signal. \n\t        \"\"\"\n\t        assert z.ndim == 3\n\t        signal = at.AudioSignal(\n\t            codec.decode(\n\t                codec.quantizer.from_latents(self.embedding.from_codes(z, codec))[0]\n\t            )[\"audio\"],\n\t            codec.sample_rate,\n", "        )\n\t        # find where the mask token is and replace it with silence in the audio\n\t        for tstep in range(z.shape[-1]):\n\t            if torch.any(z[:, :, tstep] == self.mask_token):\n\t                sample_idx_0 = tstep * codec.hop_length\n\t                sample_idx_1 = sample_idx_0 + codec.hop_length\n\t                signal.samples[:, :, sample_idx_0:sample_idx_1] = 0.0\n\t        return signal\n\t    @torch.no_grad()\n\t    def generate(\n", "        self,\n\t        codec,\n\t        time_steps: int = 300,\n\t        sampling_steps: int = 36,\n\t        start_tokens: Optional[torch.Tensor] = None,\n\t        sampling_temperature: float = 1.0,\n\t        mask: Optional[torch.Tensor] = None,\n\t        mask_temperature: float = 10.5,\n\t        typical_filtering=False,\n\t        typical_mass=0.2,\n", "        typical_min_tokens=1,\n\t        top_p=None,\n\t        return_signal=True,\n\t        seed: int = None, \n\t        sample_cutoff: float = 0.5,\n\t    ):\n\t        if seed is not None:\n\t            at.util.seed(seed)\n\t        logging.debug(f\"beginning generation with {sampling_steps} steps\")\n\t        ##################### \n", "        # resolve initial z #\n\t        #####################\n\t        z = start_tokens\n\t        if z is None:\n\t            z = torch.full((1, self.n_codebooks, time_steps), self.mask_token).to(\n\t                self.device\n\t            )\n\t        logging.debug(f\"created z with shape {z.shape}\")\n\t        #################\n\t        # resolve mask #\n", "        #################\n\t        if mask is None:\n\t            mask = torch.ones_like(z).to(self.device).int()\n\t            mask[:, : self.n_conditioning_codebooks, :] = 0.0\n\t        if mask.ndim == 2:\n\t            mask = mask[:, None, :].repeat(1, z.shape[1], 1)\n\t        # init_mask = mask.clone()\n\t        logging.debug(f\"created mask with shape {mask.shape}\")\n\t        ###########\n\t        # set up #\n", "        ##########\n\t        # apply the mask to z\n\t        z_masked = z.masked_fill(mask.bool(), self.mask_token)\n\t        # logging.debug(f\"z_masked: {z_masked}\")\n\t        # how many mask tokens to begin with?\n\t        num_mask_tokens_at_start = (z_masked == self.mask_token).sum()\n\t        logging.debug(f\"num mask tokens at start: {num_mask_tokens_at_start}\")\n\t        # how many codebooks are we inferring vs conditioning on?\n\t        n_infer_codebooks = self.n_codebooks - self.n_conditioning_codebooks\n\t        logging.debug(f\"n infer codebooks: {n_infer_codebooks}\")\n", "        #################\n\t        # begin sampling #\n\t        #################\n\t        for i in range(sampling_steps):\n\t            logging.debug(f\"step {i} of {sampling_steps}\")\n\t            # our current schedule step\n\t            r = scalar_to_batch_tensor(\n\t                (i + 1) / sampling_steps, \n\t                z.shape[0]\n\t            ).to(z.device)\n", "            logging.debug(f\"r: {r}\")\n\t            # get latents\n\t            latents = self.embedding.from_codes(z_masked, codec)\n\t            logging.debug(f\"computed latents with shape: {latents.shape}\")\n\t            # infer from latents\n\t            # NOTE: this collapses the codebook dimension into the sequence dimension\n\t            logits = self.forward(latents) # b, prob, seq\n\t            logits = logits.permute(0, 2, 1)  # b, seq, prob\n\t            b = logits.shape[0]\n\t            logging.debug(f\"permuted logits with shape: {logits.shape}\")\n", "            sampled_z, selected_probs = sample_from_logits(\n\t                logits, sample=(\n\t                   (i / sampling_steps) <= sample_cutoff\n\t                ), \n\t                temperature=sampling_temperature,\n\t                typical_filtering=typical_filtering, typical_mass=typical_mass,\n\t                typical_min_tokens=typical_min_tokens,\n\t                top_k=None, top_p=top_p, return_probs=True,\n\t            )\n\t            logging.debug(f\"sampled z with shape: {sampled_z.shape}\")\n", "            # flatten z_masked and mask, so we can deal with the sampling logic\n\t            # we'll unflatten them at the end of the loop for the next forward pass\n\t            # remove conditioning codebooks, we'll add them back at the end\n\t            z_masked = codebook_flatten(z_masked[:, self.n_conditioning_codebooks:, :])           \n\t            mask = (z_masked == self.mask_token).int()\n\t            # update the mask, remove conditioning codebooks from the mask\n\t            logging.debug(f\"updated mask with shape: {mask.shape}\")\n\t            # add z back into sampled z where the mask was false\n\t            sampled_z = torch.where(\n\t                mask.bool(), sampled_z, z_masked\n", "            )\n\t            logging.debug(f\"added z back into sampled z with shape: {sampled_z.shape}\")\n\t            # ignore any tokens that weren't masked\n\t            selected_probs = torch.where(\n\t                mask.bool(), selected_probs, torch.inf\n\t            )\n\t            # get the num tokens to mask, according to the schedule\n\t            num_to_mask = torch.floor(_gamma(r) * num_mask_tokens_at_start).unsqueeze(1).long()\n\t            logging.debug(f\"num to mask: {num_to_mask}\")\n\t            if i != (sampling_steps - 1):\n", "                num_to_mask = torch.maximum(\n\t                    torch.tensor(1),\n\t                    torch.minimum(\n\t                        mask.sum(dim=-1, keepdim=True) - 1,\n\t                        num_to_mask\n\t                    )\n\t                )\n\t            # get our new mask\n\t            mask = mask_by_random_topk(\n\t                num_to_mask, selected_probs, mask_temperature * (1-r)\n", "            )  \n\t            # update the mask\n\t            z_masked = torch.where(\n\t                mask.bool(), self.mask_token, sampled_z\n\t            )\n\t            logging.debug(f\"updated z_masked with shape: {z_masked.shape}\")\n\t            z_masked = codebook_unflatten(z_masked, n_infer_codebooks)\n\t            mask = codebook_unflatten(mask, n_infer_codebooks)\n\t            logging.debug(f\"unflattened z_masked with shape: {z_masked.shape}\")\n\t            # add conditioning codebooks back to z_masked\n", "            z_masked = torch.cat(\n\t                (z[:, :self.n_conditioning_codebooks, :], z_masked), dim=1\n\t            )\n\t            logging.debug(f\"added conditioning codebooks back to z_masked with shape: {z_masked.shape}\")\n\t        # add conditioning codebooks back to sampled_z\n\t        sampled_z = codebook_unflatten(sampled_z, n_infer_codebooks)\n\t        sampled_z = torch.cat(\n\t            (z[:, :self.n_conditioning_codebooks, :], sampled_z), dim=1\n\t        )\n\t        logging.debug(f\"finished sampling\")\n", "        if return_signal:\n\t            return self.to_signal(sampled_z, codec)\n\t        else:\n\t            return sampled_z\n\tdef sample_from_logits(\n\t        logits, \n\t        sample: bool = True,\n\t        temperature: float = 1.0,\n\t        top_k: int = None,\n\t        top_p: float = None,\n", "        typical_filtering: bool = False,\n\t        typical_mass: float = 0.2,\n\t        typical_min_tokens: int = 1,\n\t        return_probs: bool = False\n\t    ):\n\t    \"\"\"Convenience function to sample from a categorial distribution with input as\n\t    unnormalized logits.\n\t    Parameters\n\t    ----------\n\t    logits : Tensor[..., vocab_size]\n", "    config: SamplingConfig\n\t        The set of hyperparameters to be used for sampling\n\t        sample : bool, optional\n\t            Whether to perform multinomial sampling, by default True\n\t        temperature : float, optional\n\t            Scaling parameter when multinomial samping, by default 1.0\n\t        top_k : int, optional\n\t            Restricts sampling to only `top_k` values acc. to probability,\n\t            by default None\n\t        top_p : float, optional\n", "            Restricts sampling to only those values with cumulative\n\t            probability = `top_p`, by default None\n\t    Returns\n\t    -------\n\t    Tensor[...]\n\t        Sampled tokens\n\t    \"\"\"\n\t    shp = logits.shape[:-1]\n\t    if typical_filtering:\n\t        typical_filter(logits, \n", "                        typical_mass=typical_mass, \n\t                        typical_min_tokens=typical_min_tokens\n\t        )\n\t    # Apply top_k sampling\n\t    if top_k is not None:\n\t        v, _ = logits.topk(top_k)\n\t        logits[logits < v[..., [-1]]] = -float(\"inf\")\n\t    # Apply top_p (nucleus) sampling\n\t    if top_p is not None and top_p < 1.0:\n\t        v, sorted_indices = logits.sort(descending=True)\n", "        cumulative_probs = v.softmax(dim=-1).cumsum(dim=-1)\n\t        sorted_indices_to_remove = cumulative_probs > top_p\n\t        # Right shift indices_to_remove to keep 1st token over threshold\n\t        sorted_indices_to_remove = F.pad(sorted_indices_to_remove, (1, 0), value=False)[\n\t            ..., :-1\n\t        ]\n\t        # Compute indices_to_remove in unsorted array\n\t        indices_to_remove = sorted_indices_to_remove.scatter(\n\t            -1, sorted_indices, sorted_indices_to_remove\n\t        )\n", "        logits[indices_to_remove] = -float(\"inf\")\n\t    # Perform multinomial sampling after normalizing logits\n\t    probs = (\n\t        F.softmax(logits / temperature, dim=-1)\n\t        if temperature > 0\n\t        else logits.softmax(dim=-1)\n\t    )\n\t    token = (\n\t        probs.view(-1, probs.size(-1)).multinomial(1).squeeze(1).view(*shp)\n\t        if sample\n", "        else logits.argmax(-1)\n\t    )\n\t    if return_probs:\n\t        token_probs = probs.take_along_dim(token.unsqueeze(-1), dim=-1).squeeze(-1)\n\t        return token, token_probs\n\t    else:\n\t        return token\n\tdef mask_by_random_topk(\n\t        num_to_mask: int, \n\t        probs: torch.Tensor, \n", "        temperature: float = 1.0, \n\t    ):\n\t    \"\"\"\n\t    Args:\n\t        num_to_mask (int): number of tokens to mask\n\t        probs (torch.Tensor): probabilities for each sampled event, shape (batch, seq)\n\t        temperature (float, optional): temperature. Defaults to 1.0.\n\t    \"\"\"\n\t    logging.debug(f\"masking by random topk\")\n\t    logging.debug(f\"num to mask: {num_to_mask}\")\n", "    logging.debug(f\"probs shape: {probs.shape}\")\n\t    logging.debug(f\"temperature: {temperature}\")\n\t    logging.debug(\"\")\n\t    noise = gumbel_noise_like(probs)\n\t    confidence = torch.log(probs) + temperature * noise\n\t    logging.debug(f\"confidence shape: {confidence.shape}\")\n\t    sorted_confidence, sorted_idx = confidence.sort(dim=-1)\n\t    logging.debug(f\"sorted confidence shape: {sorted_confidence.shape}\")\n\t    logging.debug(f\"sorted idx shape: {sorted_idx.shape}\")\n\t    # get the cut off threshold, given the mask length\n", "    cut_off = torch.take_along_dim(\n\t        sorted_confidence, num_to_mask, axis=-1\n\t    )\n\t    logging.debug(f\"cut off shape: {cut_off.shape}\")\n\t    # mask out the tokens\n\t    mask = confidence < cut_off\n\t    logging.debug(f\"mask shape: {mask.shape}\")\n\t    return mask\n\tdef typical_filter(\n\t        logits, \n", "        typical_mass: float = 0.95,\n\t        typical_min_tokens: int = 1,):\n\t    nb, nt, _ = logits.shape\n\t    x_flat = rearrange(logits, \"b t l -> (b t ) l\")\n\t    x_flat_norm = torch.nn.functional.log_softmax(x_flat, dim=-1)\n\t    x_flat_norm_p = torch.exp(x_flat_norm)\n\t    entropy = -(x_flat_norm * x_flat_norm_p).nansum(-1, keepdim=True)\n\t    c_flat_shifted = torch.abs((-x_flat_norm) - entropy)\n\t    c_flat_sorted, x_flat_indices = torch.sort(c_flat_shifted, descending=False)\n\t    x_flat_cumsum = (\n", "        x_flat.gather(-1, x_flat_indices).softmax(dim=-1).cumsum(dim=-1)\n\t    )\n\t    last_ind = (x_flat_cumsum < typical_mass).sum(dim=-1)\n\t    sorted_indices_to_remove = c_flat_sorted > c_flat_sorted.gather(\n\t        1, last_ind.view(-1, 1)\n\t    )\n\t    if typical_min_tokens > 1:\n\t        sorted_indices_to_remove[..., :typical_min_tokens] = 0\n\t    indices_to_remove = sorted_indices_to_remove.scatter(\n\t        1, x_flat_indices, sorted_indices_to_remove\n", "    )\n\t    x_flat = x_flat.masked_fill(indices_to_remove, -float(\"Inf\"))\n\t    logits = rearrange(x_flat, \"(b t) l -> b t l\", t=nt)\n\t    return logits\n\tif __name__ == \"__main__\":\n\t    # import argbind\n\t    from .layers import num_params\n\t    VampNet = argbind.bind(VampNet)\n\t    @argbind.bind(without_prefix=True)\n\t    def try_model(device: str = \"cuda\", batch_size: int = 2, seq_len_s: float = 10.0):\n", "        seq_len = int(32000 / 512 * seq_len_s)\n\t        model = VampNet().to(device)\n\t        z = torch.randint(\n\t            0, model.vocab_size, size=(batch_size, model.n_codebooks, seq_len)\n\t        ).to(device)\n\t        r = torch.zeros(batch_size).to(device)\n\t        z_mask_latent = torch.rand(\n\t            batch_size, model.latent_dim * model.n_codebooks, seq_len\n\t        ).to(device)\n\t        z_hat = model(z_mask_latent)\n", "        pred = z_hat.argmax(dim=1)\n\t        pred = model.embedding.unflatten(pred, n_codebooks=model.n_predict_codebooks)\n\t        print(f\"model has {num_params(model)/1e6:<.3f}M parameters\")\n\t        print(f\"prediction has shape {pred.shape}\")\n\t        breakpoint()\n\t    args = argbind.parse_args()\n\t    with argbind.scope(args):\n\t        try_model()\n"]}
{"filename": "vampnet/modules/__init__.py", "chunked_list": ["import audiotools\n\taudiotools.ml.BaseModel.INTERN += [\"vampnet.modules.**\"]\n\taudiotools.ml.BaseModel.EXTERN += [\"einops\", \"flash_attn.flash_attention\", \"loralib\"]\n\tfrom .transformer import VampNet"]}
{"filename": "vampnet/modules/activations.py", "chunked_list": ["import math\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom einops import rearrange\n\tclass NewGELU(nn.Module):\n\t    \"\"\"\n\t    Implementation of the GELU activation function currently in Google BERT repo\n\t    (identical to OpenAI GPT). Also see the Gaussian Error Linear Units\n", "    paper: https://arxiv.org/abs/1606.08415\n\t    \"\"\"\n\t    def forward(self, x):\n\t        return (\n\t            0.5\n\t            * x\n\t            * (\n\t                1.0\n\t                + torch.tanh(\n\t                    math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))\n", "                )\n\t            )\n\t        )\n\tclass GatedGELU(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.gelu = NewGELU()\n\t    def forward(self, x, dim: int = -1):\n\t        p1, p2 = x.chunk(2, dim=dim)\n\t        return p1 * self.gelu(p2)\n", "class Snake1d(nn.Module):\n\t    def __init__(self, channels):\n\t        super().__init__()\n\t        self.alpha = nn.Parameter(torch.ones(channels))\n\t    def forward(self, x):\n\t        return x + (self.alpha + 1e-9).reciprocal() * torch.sin(self.alpha * x).pow(2)\n\tdef get_activation(name: str = \"relu\"):\n\t    if name == \"relu\":\n\t        return nn.ReLU\n\t    elif name == \"gelu\":\n", "        return NewGELU\n\t    elif name == \"geglu\":\n\t        return GatedGELU\n\t    elif name == \"snake\":\n\t        return Snake1d\n\t    else:\n\t        raise ValueError(f\"Unrecognized activation {name}\")"]}
