{"filename": "main.py", "chunked_list": ["import json\n\tfrom cute_assistant.core import client\n\tsettings_file = 'datastore/settings.json'\n\tdef main():\n\t    with open(settings_file) as f:\n\t        settings = json.load(f)\n\t    client.run_bot(settings['kuro_api_key'])\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/scripts/process_zip/process_zip.py", "chunked_list": ["import uuid\n\timport zipfile\n\timport os\n\timport json\n\timport argparse\n\timport asyncio\n\tfrom models.models import Document, DocumentMetadata, Source\n\tfrom datastore.datastore import DataStore\n\tfrom datastore.factory import get_datastore\n\tfrom services.extract_metadata import extract_metadata_from_document\n", "from services.file import extract_text_from_filepath\n\tfrom services.pii_detection import screen_text_for_pii\n\tDOCUMENT_UPSERT_BATCH_SIZE = 50\n\tasync def process_file_dump(\n\t    filepath: str,\n\t    datastore: DataStore,\n\t    custom_metadata: dict,\n\t    screen_for_pii: bool,\n\t    extract_metadata: bool,\n\t):\n", "    # create a ZipFile object and extract all the files into a directory named 'dump'\n\t    with zipfile.ZipFile(filepath) as zip_file:\n\t        zip_file.extractall(\"dump\")\n\t    documents = []\n\t    skipped_files = []\n\t    # use os.walk to traverse the dump directory and its subdirectories\n\t    for root, dirs, files in os.walk(\"dump\"):\n\t        for filename in files:\n\t            if len(documents) % 20 == 0:\n\t                print(f\"Processed {len(documents)} documents\")\n", "            filepath = os.path.join(root, filename)\n\t            try:\n\t                extracted_text = extract_text_from_filepath(filepath)\n\t                print(f\"extracted_text from {filepath}\")\n\t                # create a metadata object with the source and source_id fields\n\t                metadata = DocumentMetadata(\n\t                    source=Source.file,\n\t                    source_id=filename,\n\t                )\n\t                # update metadata with custom values\n", "                for key, value in custom_metadata.items():\n\t                    if hasattr(metadata, key):\n\t                        setattr(metadata, key, value)\n\t                # screen for pii if requested\n\t                if screen_for_pii:\n\t                    pii_detected = screen_text_for_pii(extracted_text)\n\t                    # if pii detected, print a warning and skip the document\n\t                    if pii_detected:\n\t                        print(\"PII detected in document, skipping\")\n\t                        skipped_files.append(\n", "                            filepath\n\t                        )  # add the skipped file to the list\n\t                        continue\n\t                # extract metadata if requested\n\t                if extract_metadata:\n\t                    # extract metadata from the document text\n\t                    extracted_metadata = extract_metadata_from_document(\n\t                        f\"Text: {extracted_text}; Metadata: {str(metadata)}\"\n\t                    )\n\t                    # get a Metadata object from the extracted metadata\n", "                    metadata = DocumentMetadata(**extracted_metadata)\n\t                # create a document object with a random id, text and metadata\n\t                document = Document(\n\t                    id=str(uuid.uuid4()),\n\t                    text=extracted_text,\n\t                    metadata=metadata,\n\t                )\n\t                documents.append(document)\n\t            except Exception as e:\n\t                # log the error and continue with the next file\n", "                print(f\"Error processing {filepath}: {e}\")\n\t                skipped_files.append(filepath)  # add the skipped file to the list\n\t    # do this in batches, the upsert method already batches documents but this allows\n\t    # us to add more descriptive logging\n\t    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):\n\t        # Get the text of the chunks in the current batch\n\t        batch_documents = [doc for doc in documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]]\n\t        print(f\"Upserting batch of {len(batch_documents)} documents, batch {i}\")\n\t        print(\"documents: \", documents)\n\t        await datastore.upsert(batch_documents)\n", "    # delete all files in the dump directory\n\t    for root, dirs, files in os.walk(\"dump\", topdown=False):\n\t        for filename in files:\n\t            filepath = os.path.join(root, filename)\n\t            os.remove(filepath)\n\t        for dirname in dirs:\n\t            dirpath = os.path.join(root, dirname)\n\t            os.rmdir(dirpath)\n\t    # delete the dump directory\n\t    os.rmdir(\"dump\")\n", "    # print the skipped files\n\t    print(f\"Skipped {len(skipped_files)} files due to errors or PII detection\")\n\t    for file in skipped_files:\n\t        print(file)\n\tasync def main():\n\t    # parse the command-line arguments\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--filepath\", required=True, help=\"The path to the file dump\")\n\t    parser.add_argument(\n\t        \"--custom_metadata\",\n", "        default=\"{}\",\n\t        help=\"A JSON string of key-value pairs to update the metadata of the documents\",\n\t    )\n\t    parser.add_argument(\n\t        \"--screen_for_pii\",\n\t        default=False,\n\t        type=bool,\n\t        help=\"A boolean flag to indicate whether to try the PII detection function (using a language model)\",\n\t    )\n\t    parser.add_argument(\n", "        \"--extract_metadata\",\n\t        default=False,\n\t        type=bool,\n\t        help=\"A boolean flag to indicate whether to try to extract metadata from the document (using a language model)\",\n\t    )\n\t    args = parser.parse_args()\n\t    # get the arguments\n\t    filepath = args.filepath\n\t    custom_metadata = json.loads(args.custom_metadata)\n\t    screen_for_pii = args.screen_for_pii\n", "    extract_metadata = args.extract_metadata\n\t    # initialize the db instance once as a global variable\n\t    datastore = await get_datastore()\n\t    # process the file dump\n\t    await process_file_dump(\n\t        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata\n\t    )\n\tif __name__ == \"__main__\":\n\t    asyncio.run(main())\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/scripts/process_jsonl/process_jsonl.py", "chunked_list": ["import uuid\n\timport json\n\timport argparse\n\timport asyncio\n\tfrom models.models import Document, DocumentMetadata\n\tfrom datastore.datastore import DataStore\n\tfrom datastore.factory import get_datastore\n\tfrom services.extract_metadata import extract_metadata_from_document\n\tfrom services.pii_detection import screen_text_for_pii\n\tDOCUMENT_UPSERT_BATCH_SIZE = 50\n", "async def process_jsonl_dump(\n\t    filepath: str,\n\t    datastore: DataStore,\n\t    custom_metadata: dict,\n\t    screen_for_pii: bool,\n\t    extract_metadata: bool,\n\t):\n\t    # open the jsonl file as a generator of dictionaries\n\t    with open(filepath) as jsonl_file:\n\t        data = [json.loads(line) for line in jsonl_file]\n", "    documents = []\n\t    skipped_items = []\n\t    # iterate over the data and create document objects\n\t    for item in data:\n\t        if len(documents) % 20 == 0:\n\t            print(f\"Processed {len(documents)} documents\")\n\t        try:\n\t            # get the id, text, source, source_id, url, created_at and author from the item\n\t            # use default values if not specified\n\t            id = item.get(\"id\", None)\n", "            text = item.get(\"text\", None)\n\t            source = item.get(\"source\", None)\n\t            source_id = item.get(\"source_id\", None)\n\t            url = item.get(\"url\", None)\n\t            created_at = item.get(\"created_at\", None)\n\t            author = item.get(\"author\", None)\n\t            if not text:\n\t                print(\"No document text, skipping...\")\n\t                continue\n\t            # create a metadata object with the source, source_id, url, created_at and author\n", "            metadata = DocumentMetadata(\n\t                source=source,\n\t                source_id=source_id,\n\t                url=url,\n\t                created_at=created_at,\n\t                author=author,\n\t            )\n\t            # update metadata with custom values\n\t            for key, value in custom_metadata.items():\n\t                if hasattr(metadata, key):\n", "                    setattr(metadata, key, value)\n\t            # screen for pii if requested\n\t            if screen_for_pii:\n\t                pii_detected = screen_text_for_pii(text)\n\t                # if pii detected, print a warning and skip the document\n\t                if pii_detected:\n\t                    print(\"PII detected in document, skipping\")\n\t                    skipped_items.append(item)  # add the skipped item to the list\n\t                    continue\n\t            # extract metadata if requested\n", "            if extract_metadata:\n\t                # extract metadata from the document text\n\t                extracted_metadata = extract_metadata_from_document(\n\t                    f\"Text: {text}; Metadata: {str(metadata)}\"\n\t                )\n\t                # get a Metadata object from the extracted metadata\n\t                metadata = DocumentMetadata(**extracted_metadata)\n\t            # create a document object with the id, text and metadata\n\t            document = Document(\n\t                id=id,\n", "                text=text,\n\t                metadata=metadata,\n\t            )\n\t            documents.append(document)\n\t        except Exception as e:\n\t            # log the error and continue with the next item\n\t            print(f\"Error processing {item}: {e}\")\n\t            skipped_items.append(item)  # add the skipped item to the list\n\t    # do this in batches, the upsert method already batches documents but this allows\n\t    # us to add more descriptive logging\n", "    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):\n\t        # Get the text of the chunks in the current batch\n\t        batch_documents = documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]\n\t        print(f\"Upserting batch of {len(batch_documents)} documents, batch {i}\")\n\t        await datastore.upsert(batch_documents)\n\t    # print the skipped items\n\t    print(f\"Skipped {len(skipped_items)} items due to errors or PII detection\")\n\t    for item in skipped_items:\n\t        print(item)\n\tasync def main():\n", "    # parse the command-line arguments\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--filepath\", required=True, help=\"The path to the jsonl dump\")\n\t    parser.add_argument(\n\t        \"--custom_metadata\",\n\t        default=\"{}\",\n\t        help=\"A JSON string of key-value pairs to update the metadata of the documents\",\n\t    )\n\t    parser.add_argument(\n\t        \"--screen_for_pii\",\n", "        default=False,\n\t        type=bool,\n\t        help=\"A boolean flag to indicate whether to try the PII detection function (using a language model)\",\n\t    )\n\t    parser.add_argument(\n\t        \"--extract_metadata\",\n\t        default=False,\n\t        type=bool,\n\t        help=\"A boolean flag to indicate whether to try to extract metadata from the document (using a language model)\",\n\t    )\n", "    args = parser.parse_args()\n\t    # get the arguments\n\t    filepath = args.filepath\n\t    custom_metadata = json.loads(args.custom_metadata)\n\t    screen_for_pii = args.screen_for_pii\n\t    extract_metadata = args.extract_metadata\n\t    # initialize the db instance once as a global variable\n\t    datastore = await get_datastore()\n\t    # process the jsonl dump\n\t    await process_jsonl_dump(\n", "        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata\n\t    )\n\tif __name__ == \"__main__\":\n\t    asyncio.run(main())\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/scripts/process_json/process_json.py", "chunked_list": ["import uuid\n\timport json\n\timport argparse\n\timport asyncio\n\tfrom models.models import Document, DocumentMetadata\n\tfrom datastore.datastore import DataStore\n\tfrom datastore.factory import get_datastore\n\tfrom services.extract_metadata import extract_metadata_from_document\n\tfrom services.pii_detection import screen_text_for_pii\n\tDOCUMENT_UPSERT_BATCH_SIZE = 50\n", "async def process_json_dump(\n\t    filepath: str,\n\t    datastore: DataStore,\n\t    custom_metadata: dict,\n\t    screen_for_pii: bool,\n\t    extract_metadata: bool,\n\t):\n\t    # load the json file as a list of dictionaries\n\t    with open(filepath) as json_file:\n\t        data = json.load(json_file)\n", "    documents = []\n\t    skipped_items = []\n\t    # iterate over the data and create document objects\n\t    for item in data:\n\t        if len(documents) % 20 == 0:\n\t            print(f\"Processed {len(documents)} documents\")\n\t        try:\n\t            # get the id, text, source, source_id, url, created_at and author from the item\n\t            # use default values if not specified\n\t            id = item.get(\"id\", None)\n", "            text = item.get(\"text\", None)\n\t            source = item.get(\"source\", None)\n\t            source_id = item.get(\"source_id\", None)\n\t            url = item.get(\"url\", None)\n\t            created_at = item.get(\"created_at\", None)\n\t            author = item.get(\"author\", None)\n\t            if not text:\n\t                print(\"No document text, skipping...\")\n\t                continue\n\t            # create a metadata object with the source, source_id, url, created_at and author\n", "            metadata = DocumentMetadata(\n\t                source=source,\n\t                source_id=source_id,\n\t                url=url,\n\t                created_at=created_at,\n\t                author=author,\n\t            )\n\t            print(\"metadata: \", str(metadata))\n\t            # update metadata with custom values\n\t            for key, value in custom_metadata.items():\n", "                if hasattr(metadata, key):\n\t                    setattr(metadata, key, value)\n\t            # screen for pii if requested\n\t            if screen_for_pii:\n\t                pii_detected = screen_text_for_pii(text)\n\t                # if pii detected, print a warning and skip the document\n\t                if pii_detected:\n\t                    print(\"PII detected in document, skipping\")\n\t                    skipped_items.append(item)  # add the skipped item to the list\n\t                    continue\n", "            # extract metadata if requested\n\t            if extract_metadata:\n\t                # extract metadata from the document text\n\t                extracted_metadata = extract_metadata_from_document(\n\t                    f\"Text: {text}; Metadata: {str(metadata)}\"\n\t                )\n\t                # get a Metadata object from the extracted metadata\n\t                metadata = DocumentMetadata(**extracted_metadata)\n\t            # create a document object with the id or a random id, text and metadata\n\t            document = Document(\n", "                id=id or str(uuid.uuid4()),\n\t                text=text,\n\t                metadata=metadata,\n\t            )\n\t            documents.append(document)\n\t        except Exception as e:\n\t            # log the error and continue with the next item\n\t            print(f\"Error processing {item}: {e}\")\n\t            skipped_items.append(item)  # add the skipped item to the list\n\t    # do this in batches, the upsert method already batches documents but this allows\n", "    # us to add more descriptive logging\n\t    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):\n\t        # Get the text of the chunks in the current batch\n\t        batch_documents = documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]\n\t        print(f\"Upserting batch of {len(batch_documents)} documents, batch {i}\")\n\t        print(\"documents: \", documents)\n\t        await datastore.upsert(batch_documents)\n\t    # print the skipped items\n\t    print(f\"Skipped {len(skipped_items)} items due to errors or PII detection\")\n\t    for item in skipped_items:\n", "        print(item)\n\tasync def main():\n\t    # parse the command-line arguments\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--filepath\", required=True, help=\"The path to the json dump\")\n\t    parser.add_argument(\n\t        \"--custom_metadata\",\n\t        default=\"{}\",\n\t        help=\"A JSON string of key-value pairs to update the metadata of the documents\",\n\t    )\n", "    parser.add_argument(\n\t        \"--screen_for_pii\",\n\t        default=False,\n\t        type=bool,\n\t        help=\"A boolean flag to indicate whether to try the PII detection function (using a language model)\",\n\t    )\n\t    parser.add_argument(\n\t        \"--extract_metadata\",\n\t        default=False,\n\t        type=bool,\n", "        help=\"A boolean flag to indicate whether to try to extract metadata from the document (using a language model)\",\n\t    )\n\t    args = parser.parse_args()\n\t    # get the arguments\n\t    filepath = args.filepath\n\t    custom_metadata = json.loads(args.custom_metadata)\n\t    screen_for_pii = args.screen_for_pii\n\t    extract_metadata = args.extract_metadata\n\t    # initialize the db instance once as a global variable\n\t    datastore = await get_datastore()\n", "    # process the json dump\n\t    await process_json_dump(\n\t        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata\n\t    )\n\tif __name__ == \"__main__\":\n\t    asyncio.run(main())\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/tests/__init__.py", "chunked_list": []}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/tests/datastore/providers/redis/test_redis_datastore.py", "chunked_list": ["from datastore.providers.redis_datastore import RedisDataStore\n\timport datastore.providers.redis_datastore as static_redis\n\tfrom models.models import DocumentChunk, DocumentChunkMetadata, QueryWithEmbedding, Source\n\timport pytest\n\timport redis.asyncio as redis\n\timport numpy as np\n\t@pytest.fixture\n\tasync def redis_datastore():\n\t    return await RedisDataStore.init(dim=5)\n\tdef create_embedding(i, dim):\n", "    vec = np.array([0.1] * dim).astype(np.float64).tolist()\n\t    vec[dim-1] = i+1/10\n\t    return vec\n\tdef create_document_chunk(i, dim):\n\t    return DocumentChunk(\n\t        id=f\"first-doc_{i}\",\n\t        text=f\"Lorem ipsum {i}\",\n\t        embedding=create_embedding(i, dim),\n\t        metadata=DocumentChunkMetadata(\n\t            source=Source.file, created_at=\"1970-01-01\", document_id=f\"doc-{i}\"\n", "        ),\n\t    )\n\tdef create_document_chunks(n, dim):\n\t    docs =  [create_document_chunk(i, dim) for i in range(n)]\n\t    return {\"docs\": docs}\n\t@pytest.mark.asyncio\n\tasync def test_redis_upsert_query(redis_datastore):\n\t    docs = create_document_chunks(10, 5)\n\t    await redis_datastore._upsert(docs)\n\t    query = QueryWithEmbedding(\n", "        query=\"Lorem ipsum 0\",\n\t        top_k=5,\n\t        embedding= create_embedding(0, 5),\n\t    )\n\t    query_results = await redis_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    for i in range(5):\n\t        assert f\"Lorem ipsum {i}\" == query_results[0].results[i].text\n\t        assert f\"doc-{i}\" == query_results[0].results[i].id\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/tests/datastore/providers/weaviate/test_weaviate_datastore.py", "chunked_list": ["import pytest\n\tfrom fastapi.testclient import TestClient\n\tfrom weaviate import Client\n\timport weaviate\n\timport os\n\tfrom models.models import DocumentMetadataFilter, Source\n\tfrom server.main import app\n\tfrom datastore.providers.weaviate_datastore import (\n\t    SCHEMA,\n\t    WeaviateDataStore,\n", "    extract_schema_properties,\n\t)\n\timport logging\n\tfrom loguru import logger\n\tfrom _pytest.logging import LogCaptureFixture\n\tBEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\n\tclient = TestClient(app)\n\tclient.headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n\t@pytest.fixture\n\tdef weaviate_client():\n", "    host = os.getenv(\"WEAVIATE_HOST\", \"http://localhost\")\n\t    port = os.getenv(\"WEAVIATE_PORT\", \"8080\")\n\t    client = Client(f\"{host}:{port}\")\n\t    yield client\n\t    client.schema.delete_all()\n\t@pytest.fixture\n\tdef test_db(weaviate_client, documents):\n\t    weaviate_client.schema.delete_all()\n\t    weaviate_client.schema.create_class(SCHEMA)\n\t    response = client.post(\"/upsert\", json={\"documents\": documents})\n", "    if response.status_code != 200:\n\t        raise Exception(\n\t            f\"Could not upsert to test client.\\nStatus Code: {response.status_code}\\nResponse:\\n{response.json()}\"\n\t        )\n\t    yield client\n\t@pytest.fixture\n\tdef documents():\n\t    documents = []\n\t    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n\t    texts = [\n", "        \"lorem ipsum dolor sit amet\",\n\t        \"consectetur adipiscing elit\",\n\t        \"sed do eiusmod tempor incididunt\",\n\t    ]\n\t    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n\t    sources = [\"chat\", \"email\", \"email\"]\n\t    created_at = [\n\t        \"1929-10-28T09:30:00-05:00\",\n\t        \"2009-01-03T16:39:57-08:00\",\n\t        \"2021-01-21T10:00:00-02:00\",\n", "    ]\n\t    for i in range(3):\n\t        documents.append(\n\t            {\n\t                \"id\": ids[i],\n\t                \"text\": texts[i],\n\t                \"metadata\": {\n\t                    \"source\": sources[i],\n\t                    \"source_id\": \"5325\",\n\t                    \"url\": \"http://example.com\",\n", "                    \"created_at\": created_at[i],\n\t                    \"author\": authors[i],\n\t                },\n\t            }\n\t        )\n\t    no_metadata_doc = {\n\t        \"id\": \"jkl_012\",\n\t        \"text\": \"no metadata\",\n\t    }\n\t    documents.append(no_metadata_doc)\n", "    partial_metadata_doc = {\n\t        \"id\": \"mno_345\",\n\t        \"text\": \"partial metadata\",\n\t        \"metadata\": {\n\t            \"source\": \"file\",\n\t        },\n\t    }\n\t    documents.append(partial_metadata_doc)\n\t    yield documents\n\t@pytest.fixture\n", "def mock_env_public_access(monkeypatch):\n\t    monkeypatch.setattr(\n\t        \"datastore.providers.weaviate_datastore.WEAVIATE_USERNAME\", None\n\t    )\n\t    monkeypatch.setattr(\n\t        \"datastore.providers.weaviate_datastore.WEAVIATE_PASSWORD\", None\n\t    )\n\t@pytest.fixture\n\tdef mock_env_resource_owner_password_flow(monkeypatch):\n\t    monkeypatch.setattr(\n", "        \"datastore.providers.weaviate_datastore.WEAVIATE_SCOPES\",\n\t        [\"schema:read\", \"schema:write\"],\n\t    )\n\t    monkeypatch.setattr(\n\t        \"datastore.providers.weaviate_datastore.WEAVIATE_USERNAME\", \"admin\"\n\t    )\n\t    monkeypatch.setattr(\n\t        \"datastore.providers.weaviate_datastore.WEAVIATE_PASSWORD\", \"abc123\"\n\t    )\n\t@pytest.fixture\n", "def caplog(caplog: LogCaptureFixture):\n\t    handler_id = logger.add(caplog.handler, format=\"{message}\")\n\t    yield caplog\n\t    logger.remove(handler_id)\n\t@pytest.mark.parametrize(\n\t    \"document_id\", [(\"abc_123\"), (\"9a253e0b-d2df-5c2e-be6d-8e9b1f4ae345\")]\n\t)\n\tdef test_upsert(weaviate_client, document_id):\n\t    weaviate_client.schema.delete_all()\n\t    weaviate_client.schema.create_class(SCHEMA)\n", "    text = \"\"\"\n\t    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce in ipsum eget dolor malesuada fermentum at ac massa. \n\t    Aliquam erat volutpat. Sed eu velit est. Morbi semper quam id urna fringilla lacinia. Vivamus sit amet velit id lorem \n\t    pretium molestie. Nulla tincidunt sapien eu nulla consequat, a lacinia justo facilisis. Maecenas euismod urna sapien, \n\t    sit amet tincidunt est dapibus ac. Sed in lorem in nunc tincidunt bibendum. Nullam vel urna vitae nulla iaculis rutrum. \n\t    Suspendisse varius, massa a dignissim vehicula, urna ligula tincidunt orci, id fringilla velit tellus eu metus. Sed \n\t    vestibulum, nisl in malesuada tempor, nisi turpis facilisis nibh, nec dictum velit velit vel ex. Donec euismod, \n\t    leo ut sollicitudin tempor, dolor augue blandit nunc, eu lacinia ipsum turpis vitae nulla. Aenean bibendum \n\t    tincidunt magna in pulvinar. Sed tincidunt vel nisi ac maximus.\n\t    \"\"\"\n", "    source = \"email\"\n\t    source_id = \"5325\"\n\t    url = \"http://example.com\"\n\t    created_at = \"2022-12-16T08:00:00+01:00\"\n\t    author = \"Max Mustermann\"\n\t    documents = {\n\t        \"documents\": [\n\t            {\n\t                \"id\": document_id,\n\t                \"text\": text,\n", "                \"metadata\": {\n\t                    \"source\": source,\n\t                    \"source_id\": source_id,\n\t                    \"url\": url,\n\t                    \"created_at\": created_at,\n\t                    \"author\": author,\n\t                },\n\t            }\n\t        ]\n\t    }\n", "    response = client.post(\"/upsert\", json=documents)\n\t    assert response.status_code == 200\n\t    assert response.json() == {\"ids\": [document_id]}\n\t    properties = [\n\t        \"chunk_id\",\n\t        \"document_id\",\n\t        \"source\",\n\t        \"source_id\",\n\t        \"url\",\n\t        \"created_at\",\n", "        \"author\",\n\t    ]\n\t    where_filter = {\n\t        \"path\": [\"document_id\"],\n\t        \"operator\": \"Equal\",\n\t        \"valueString\": document_id,\n\t    }\n\t    weaviate_doc = (\n\t        weaviate_client.query.get(\"OpenAIDocument\", properties)\n\t        .with_additional(\"vector\")\n", "        .with_where(where_filter)\n\t        .with_sort({\"path\": [\"chunk_id\"], \"order\": \"asc\"})\n\t        .do()\n\t    )\n\t    weaviate_docs = weaviate_doc[\"data\"][\"Get\"][\"OpenAIDocument\"]\n\t    assert len(weaviate_docs) == 2\n\t    for i, weaviate_doc in enumerate(weaviate_docs):\n\t        assert weaviate_doc[\"chunk_id\"] == f\"{document_id}_{i}\"\n\t        assert weaviate_doc[\"document_id\"] == document_id\n\t        assert weaviate_doc[\"source\"] == source\n", "        assert weaviate_doc[\"source_id\"] == source_id\n\t        assert weaviate_doc[\"url\"] == url\n\t        assert weaviate_doc[\"created_at\"] == created_at\n\t        assert weaviate_doc[\"author\"] == author\n\t        assert weaviate_doc[\"_additional\"][\"vector\"]\n\tdef test_upsert_no_metadata(weaviate_client):\n\t    weaviate_client.schema.delete_all()\n\t    weaviate_client.schema.create_class(SCHEMA)\n\t    no_metadata_doc = {\n\t        \"id\": \"jkl_012\",\n", "        \"text\": \"no metadata\",\n\t    }\n\t    metadata_properties = [\n\t        \"source\",\n\t        \"source_id\",\n\t        \"url\",\n\t        \"created_at\",\n\t        \"author\",\n\t    ]\n\t    response = client.post(\"/upsert\", json={\"documents\": [no_metadata_doc]})\n", "    assert response.status_code == 200\n\t    weaviate_doc = weaviate_client.query.get(\"OpenAIDocument\", metadata_properties).do()\n\t    weaviate_doc = weaviate_doc[\"data\"][\"Get\"][\"OpenAIDocument\"][0]\n\t    for _, metadata_value in weaviate_doc.items():\n\t        assert metadata_value is None\n\t@pytest.mark.parametrize(\n\t    \"test_document, expected_status_code\",\n\t    [\n\t        ({\"id\": \"abc_123\", \"text\": \"some text\"}, 200),\n\t        ({\"id\": \"abc_123\"}, 422),\n", "        ({\"text\": \"some text\"}, 200),\n\t    ],\n\t)\n\tdef test_upsert_invalid_documents(weaviate_client, test_document, expected_status_code):\n\t    weaviate_client.schema.delete_all()\n\t    weaviate_client.schema.create_class(SCHEMA)\n\t    response = client.post(\"/upsert\", json={\"documents\": [test_document]})\n\t    assert response.status_code == expected_status_code\n\t@pytest.mark.parametrize(\n\t    \"query, expected_num_results\",\n", "    [\n\t        ({\"query\": \"consectetur adipiscing\", \"top_k\": 3}, 3),\n\t        ({\"query\": \"consectetur adipiscing elit\", \"filter\": {\"source\": \"email\"}}, 2),\n\t        (\n\t            {\n\t                \"query\": \"sed do eiusmod tempor\",\n\t                \"filter\": {\n\t                    \"start_date\": \"2020-01-01T00:00:00Z\",\n\t                    \"end_date\": \"2022-12-31T00:00:00Z\",\n\t                },\n", "            },\n\t            1,\n\t        ),\n\t        (\n\t            {\n\t                \"query\": \"some random query\",\n\t                \"filter\": {\"start_date\": \"2009-01-01T00:00:00Z\"},\n\t                \"top_k\": 3,\n\t            },\n\t            2,\n", "        ),\n\t        (\n\t            {\n\t                \"query\": \"another random query\",\n\t                \"filter\": {\"end_date\": \"1929-12-31T00:00:00Z\"},\n\t                \"top_k\": 3,\n\t            },\n\t            1,\n\t        ),\n\t    ],\n", ")\n\tdef test_query(test_db, query, expected_num_results):\n\t    queries = {\"queries\": [query]}\n\t    response = client.post(\"/query\", json=queries)\n\t    assert response.status_code == 200\n\t    num_docs = response.json()[\"results\"][0][\"results\"]\n\t    assert len(num_docs) == expected_num_results\n\tdef test_delete(test_db, weaviate_client, caplog):\n\t    caplog.set_level(logging.DEBUG)\n\t    delete_request = {\"ids\": [\"def_456\"]}\n", "    response = client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n\t    assert response.status_code == 200\n\t    assert response.json()[\"success\"]\n\t    assert weaviate_client.data_object.get()[\"totalResults\"] == 4\n\t    client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n\t    assert \"Failed to delete\" in caplog.text\n\t    caplog.clear()\n\t    delete_request = {\"filter\": {\"source\": \"email\"}}\n\t    response = client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n\t    assert response.status_code == 200\n", "    assert response.json()[\"success\"]\n\t    assert weaviate_client.data_object.get()[\"totalResults\"] == 3\n\t    client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n\t    assert \"Failed to delete\" in caplog.text\n\t    delete_request = {\"delete_all\": True}\n\t    response = client.request(method=\"delete\", url=\"/delete\", json=delete_request)\n\t    assert response.status_code == 200\n\t    assert response.json()[\"success\"]\n\t    assert not weaviate_client.data_object.get()[\"objects\"]\n\tdef test_access_with_username_password(mock_env_resource_owner_password_flow):\n", "    auth_credentials = WeaviateDataStore._build_auth_credentials()\n\t    assert isinstance(auth_credentials, weaviate.auth.AuthClientPassword)\n\tdef test_public_access(mock_env_public_access):\n\t    auth_credentials = WeaviateDataStore._build_auth_credentials()\n\t    assert auth_credentials is None\n\tdef test_extract_schema_properties():\n\t    class_schema = {\n\t        \"class\": \"Question\",\n\t        \"description\": \"Information from a Jeopardy! question\",\n\t        \"properties\": [\n", "            {\n\t                \"dataType\": [\"text\"],\n\t                \"description\": \"The question\",\n\t                \"name\": \"question\",\n\t            },\n\t            {\n\t                \"dataType\": [\"text\"],\n\t                \"description\": \"The answer\",\n\t                \"name\": \"answer\",\n\t            },\n", "            {\n\t                \"dataType\": [\"text\"],\n\t                \"description\": \"The category\",\n\t                \"name\": \"category\",\n\t            },\n\t        ],\n\t        \"vectorizer\": \"text2vec-openai\",\n\t    }\n\t    results = extract_schema_properties(class_schema)\n\t    assert results == {\"question\", \"answer\", \"category\"}\n", "def test_reuse_schema(weaviate_client, caplog):\n\t    caplog.set_level(logging.DEBUG)\n\t    weaviate_client.schema.delete_all()\n\t    WeaviateDataStore()\n\t    assert \"Creating index\" in caplog.text\n\t    WeaviateDataStore()\n\t    assert \"Will reuse this schema\" in caplog.text\n\tdef test_build_date_filters():\n\t    filter = DocumentMetadataFilter(\n\t        document_id=None,\n", "        source=None,\n\t        source_id=None,\n\t        author=None,\n\t        start_date=\"2020-01-01T00:00:00Z\",\n\t        end_date=\"2022-12-31T00:00:00Z\",\n\t    )\n\t    actual_result = WeaviateDataStore.build_filters(filter)\n\t    expected_result = {\n\t        \"operator\": \"And\",\n\t        \"operands\": [\n", "            {\n\t                \"path\": [\"created_at\"],\n\t                \"operator\": \"GreaterThanEqual\",\n\t                \"valueDate\": \"2020-01-01T00:00:00Z\",\n\t            },\n\t            {\n\t                \"path\": [\"created_at\"],\n\t                \"operator\": \"LessThanEqual\",\n\t                \"valueDate\": \"2022-12-31T00:00:00Z\",\n\t            },\n", "        ],\n\t    }\n\t    assert actual_result == expected_result\n\t@pytest.mark.parametrize(\n\t    \"test_input, expected_result\",\n\t    [\n\t        (\"abc_123\", False),\n\t        (\"b2e4133c-c956-5684-bbf5-584e50ec3647\", True),  # version 5\n\t        (\"f6179953-11d8-4ee0-9af8-e51e00dbf727\", True),  # version 4\n\t        (\"16fe8165-3c08-348f-a015-a8bb31e26b5c\", True),  # version 3\n", "        (\"bda85f97-be72-11ed-9291-00000000000a\", False),  # version 1\n\t    ],\n\t)\n\tdef test_is_valid_weaviate_id(test_input, expected_result):\n\t    actual_result = WeaviateDataStore._is_valid_weaviate_id(test_input)\n\t    assert actual_result == expected_result\n\tdef test_upsert_same_docid(test_db, weaviate_client):\n\t    def get_doc_by_document_id(document_id):\n\t        properties = [\n\t            \"chunk_id\",\n", "            \"document_id\",\n\t            \"source\",\n\t            \"source_id\",\n\t            \"url\",\n\t            \"created_at\",\n\t            \"author\",\n\t        ]\n\t        where_filter = {\n\t            \"path\": [\"document_id\"],\n\t            \"operator\": \"Equal\",\n", "            \"valueString\": document_id,\n\t        }\n\t        results = (\n\t            weaviate_client.query.get(\"OpenAIDocument\", properties)\n\t            .with_additional(\"id\")\n\t            .with_where(where_filter)\n\t            .with_sort({\"path\": [\"chunk_id\"], \"order\": \"asc\"})\n\t            .do()\n\t        )\n\t        return results[\"data\"][\"Get\"][\"OpenAIDocument\"]\n", "    def build_upsert_payload(document):\n\t        return {\"documents\": [document]}\n\t    # upsert a new document\n\t    # this is a document that has 2 chunks and\n\t    # the source is email\n\t    doc_id = \"abc_123\"\n\t    text = \"\"\"\n\t    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce in ipsum eget dolor malesuada fermentum at ac massa. \n\t    Aliquam erat volutpat. Sed eu velit est. Morbi semper quam id urna fringilla lacinia. Vivamus sit amet velit id lorem \n\t    pretium molestie. Nulla tincidunt sapien eu nulla consequat, a lacinia justo facilisis. Maecenas euismod urna sapien, \n", "    sit amet tincidunt est dapibus ac. Sed in lorem in nunc tincidunt bibendum. Nullam vel urna vitae nulla iaculis rutrum. \n\t    Suspendisse varius, massa a dignissim vehicula, urna ligula tincidunt orci, id fringilla velit tellus eu metus. Sed \n\t    vestibulum, nisl in malesuada tempor, nisi turpis facilisis nibh, nec dictum velit velit vel ex. Donec euismod, \n\t    leo ut sollicitudin tempor, dolor augue blandit nunc, eu lacinia ipsum turpis vitae nulla. Aenean bibendum \n\t    tincidunt magna in pulvinar. Sed tincidunt vel nisi ac maximus.\n\t    \"\"\"\n\t    document = {\n\t        \"id\": doc_id,\n\t        \"text\": text,\n\t        \"metadata\": {\"source\": Source.email},\n", "    }\n\t    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n\t    assert response.status_code == 200\n\t    weaviate_doc = get_doc_by_document_id(doc_id)\n\t    assert len(weaviate_doc) == 2\n\t    for chunk in weaviate_doc:\n\t        assert chunk[\"source\"] == Source.email\n\t    # now update the source to file\n\t    # user still has to specify the text\n\t    # because test is a required field\n", "    document[\"metadata\"][\"source\"] = Source.file\n\t    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n\t    assert response.status_code == 200\n\t    weaviate_doc = get_doc_by_document_id(doc_id)\n\t    assert len(weaviate_doc) == 2\n\t    for chunk in weaviate_doc:\n\t        assert chunk[\"source\"] == \"file\"\n\t    # now update the text so that it is only 1 chunk\n\t    # user does not need to specify metadata\n\t    # since it is optional\n", "    document[\"text\"] = \"This is a short text\"\n\t    document.pop(\"metadata\")\n\t    response = client.post(\"/upsert\", json=build_upsert_payload(document))\n\t    assert response.status_code == 200\n\t    weaviate_doc = get_doc_by_document_id(doc_id)\n\t    assert len(weaviate_doc) == 1\n\t    # TODO: Implement update function\n\t    # but the source should still be file\n\t    # but it is None right now because an\n\t    # update function is out of scope\n", "    assert weaviate_doc[0][\"source\"] is None\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/tests/datastore/providers/zilliz/test_zilliz_datastore.py", "chunked_list": ["# from pathlib import Path\n\t# from dotenv import find_dotenv, load_dotenv\n\t# env_path = Path(\".\") / \"zilliz.env\"\n\t# load_dotenv(dotenv_path=env_path, verbose=True)\n\timport pytest\n\tfrom datastore.providers.zilliz_datastore import (\n\t    ZillizDataStore,\n\t)\n\tfrom datastore.providers.milvus_datastore import (\n\t    EMBEDDING_FIELD,\n", ")\n\t# Note: Only do basic test here, the ZillizDataStore is derived from MilvusDataStore.\n\t@pytest.fixture\n\tdef zilliz_datastore():\n\t    return ZillizDataStore()\n\t@pytest.mark.asyncio\n\tasync def test_zilliz(zilliz_datastore):\n\t    assert True == zilliz_datastore.col.has_index()\n\t    index_list = [x.to_dict() for x in zilliz_datastore.col.indexes]\n\t    for index in index_list:\n", "        if index['index_name'] == EMBEDDING_FIELD:\n\t            assert 'AUTOINDEX' == index['index_param']['index_type']"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/tests/datastore/providers/milvus/test_milvus_datastore.py", "chunked_list": ["# from pathlib import Path\n\t# from dotenv import find_dotenv, load_dotenv\n\t# env_path = Path(\".\") / \"milvus.env\"\n\t# load_dotenv(dotenv_path=env_path, verbose=True)\n\timport pytest\n\tfrom models.models import (\n\t    DocumentChunkMetadata,\n\t    DocumentMetadataFilter,\n\t    DocumentChunk,\n\t    QueryWithEmbedding,\n", "    Source,\n\t)\n\tfrom datastore.providers.milvus_datastore import (\n\t    OUTPUT_DIM,\n\t    MilvusDataStore,\n\t)\n\t@pytest.fixture\n\tdef milvus_datastore():\n\t    return MilvusDataStore(consistency_level = \"Strong\")\n\tdef sample_embedding(one_element_poz: int):\n", "    embedding = [0] * OUTPUT_DIM\n\t    embedding[one_element_poz % OUTPUT_DIM] = 1\n\t    return embedding\n\tdef sample_embeddings(num: int, one_element_start: int = 0):\n\t    # since metric type is consine, we create vector contains only one element 1, others 0\n\t    embeddings = []\n\t    for x in range(num):\n\t        embedding = [0] * OUTPUT_DIM\n\t        embedding[(x + one_element_start) % OUTPUT_DIM] = 1\n\t        embeddings.append(embedding)\n", "    return embeddings\n\t@pytest.fixture\n\tdef document_chunk_one():\n\t    doc_id = \"zerp\"\n\t    doc_chunks = []\n\t    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n\t    texts = [\n\t        \"lorem ipsum dolor sit amet\",\n\t        \"consectetur adipiscing elit\",\n\t        \"sed do eiusmod tempor incididunt\",\n", "    ]\n\t    sources = [Source.email, Source.file, Source.chat]\n\t    source_ids = [\"foo\", \"bar\", \"baz\"]\n\t    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n\t    created_ats = [\n\t        \"1929-10-28T09:30:00-05:00\",\n\t        \"2009-01-03T16:39:57-08:00\",\n\t        \"2021-01-21T10:00:00-02:00\",\n\t    ]\n\t    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n", "    embeddings = sample_embeddings(len(texts))\n\t    for i in range(3):\n\t        chunk = DocumentChunk(\n\t            id=ids[i],\n\t            text=texts[i],\n\t            metadata=DocumentChunkMetadata(\n\t                document_id=doc_id,\n\t                source=sources[i],\n\t                source_id=source_ids[i],\n\t                url=urls[i],\n", "                created_at=created_ats[i],\n\t                author=authors[i],\n\t            ),\n\t            embedding=embeddings[i],  # type: ignore\n\t        )\n\t        doc_chunks.append(chunk)\n\t    return {doc_id: doc_chunks}\n\t@pytest.fixture\n\tdef document_chunk_two():\n\t    doc_id_1 = \"zerp\"\n", "    doc_chunks_1 = []\n\t    ids = [\"abc_123\", \"def_456\", \"ghi_789\"]\n\t    texts = [\n\t        \"1lorem ipsum dolor sit amet\",\n\t        \"2consectetur adipiscing elit\",\n\t        \"3sed do eiusmod tempor incididunt\",\n\t    ]\n\t    sources = [Source.email, Source.file, Source.chat]\n\t    source_ids = [\"foo\", \"bar\", \"baz\"]\n\t    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n", "    created_ats = [\n\t        \"1929-10-28T09:30:00-05:00\",\n\t        \"2009-01-03T16:39:57-08:00\",\n\t        \"3021-01-21T10:00:00-02:00\",\n\t    ]\n\t    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n\t    embeddings = sample_embeddings(len(texts))\n\t    for i in range(3):\n\t        chunk = DocumentChunk(\n\t            id=ids[i],\n", "            text=texts[i],\n\t            metadata=DocumentChunkMetadata(\n\t                document_id=doc_id_1,\n\t                source=sources[i],\n\t                source_id=source_ids[i],\n\t                url=urls[i],\n\t                created_at=created_ats[i],\n\t                author=authors[i],\n\t            ),\n\t            embedding=embeddings[i],  # type: ignore\n", "        )\n\t        doc_chunks_1.append(chunk)\n\t    doc_id_2 = \"merp\"\n\t    doc_chunks_2 = []\n\t    ids = [\"jkl_123\", \"lmn_456\", \"opq_789\"]\n\t    texts = [\n\t        \"3sdsc efac feas sit qweas\",\n\t        \"4wert sdfas fdsc\",\n\t        \"52dsc fdsf eiusmod asdasd incididunt\",\n\t    ]\n", "    sources = [Source.email, Source.file, Source.chat]\n\t    source_ids = [\"foo\", \"bar\", \"baz\"]\n\t    urls = [\"foo.com\", \"bar.net\", \"baz.org\"]\n\t    created_ats = [\n\t        \"4929-10-28T09:30:00-05:00\",\n\t        \"5009-01-03T16:39:57-08:00\",\n\t        \"6021-01-21T10:00:00-02:00\",\n\t    ]\n\t    authors = [\"Max Mustermann\", \"John Doe\", \"Jane Doe\"]\n\t    embeddings = sample_embeddings(len(texts), 3)\n", "    for i in range(3):\n\t        chunk = DocumentChunk(\n\t            id=ids[i],\n\t            text=texts[i],\n\t            metadata=DocumentChunkMetadata(\n\t                document_id=doc_id_2,\n\t                source=sources[i],\n\t                source_id=source_ids[i],\n\t                url=urls[i],\n\t                created_at=created_ats[i],\n", "                author=authors[i],\n\t            ),\n\t            embedding=embeddings[i],  # type: ignore\n\t        )\n\t        doc_chunks_2.append(chunk)\n\t    return {doc_id_1: doc_chunks_1, doc_id_2: doc_chunks_2}\n\t@pytest.mark.asyncio\n\tasync def test_upsert(milvus_datastore, document_chunk_one):\n\t    await milvus_datastore.delete(delete_all=True)\n\t    res = await milvus_datastore._upsert(document_chunk_one)\n", "    assert res == list(document_chunk_one.keys())\n\t    milvus_datastore.col.flush()\n\t    assert 3 == milvus_datastore.col.num_entities\n\t    milvus_datastore.col.drop()\n\t@pytest.mark.asyncio\n\tasync def test_reload(milvus_datastore, document_chunk_one, document_chunk_two):\n\t    await milvus_datastore.delete(delete_all=True)\n\t    res = await milvus_datastore._upsert(document_chunk_one)\n\t    assert res == list(document_chunk_one.keys())\n\t    milvus_datastore.col.flush()\n", "    assert 3 == milvus_datastore.col.num_entities\n\t    new_store = MilvusDataStore()\n\t    another_in = {i: document_chunk_two[i] for i in document_chunk_two if i != res[0]}\n\t    res = await new_store._upsert(another_in)\n\t    new_store.col.flush()\n\t    assert 6 == new_store.col.num_entities\n\t    query = QueryWithEmbedding(\n\t        query=\"lorem\",\n\t        top_k=10,\n\t        embedding=sample_embedding(0),\n", "    )\n\t    query_results = await milvus_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    new_store.col.drop()\n\t@pytest.mark.asyncio\n\tasync def test_upsert_query_all(milvus_datastore, document_chunk_two):\n\t    await milvus_datastore.delete(delete_all=True)\n\t    res = await milvus_datastore._upsert(document_chunk_two)\n\t    assert res == list(document_chunk_two.keys())\n\t    milvus_datastore.col.flush()\n", "    # Num entities currently doesn't track deletes\n\t    query = QueryWithEmbedding(\n\t        query=\"lorem\",\n\t        top_k=10,\n\t        embedding=sample_embedding(0),\n\t    )\n\t    query_results = await milvus_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 6 == len(query_results[0].results)\n\t    milvus_datastore.col.drop()\n", "@pytest.mark.asyncio\n\tasync def test_query_accuracy(milvus_datastore, document_chunk_one):\n\t    await milvus_datastore.delete(delete_all=True)\n\t    res = await milvus_datastore._upsert(document_chunk_one)\n\t    assert res == list(document_chunk_one.keys())\n\t    milvus_datastore.col.flush()\n\t    query = QueryWithEmbedding(\n\t        query=\"lorem\",\n\t        top_k=1,\n\t        embedding=sample_embedding(0),\n", "    )\n\t    query_results = await milvus_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 1 == len(query_results[0].results)\n\t    assert 1.0 == query_results[0].results[0].score\n\t    assert \"abc_123\" == query_results[0].results[0].id\n\t    milvus_datastore.col.drop()\n\t@pytest.mark.asyncio\n\tasync def test_query_filter(milvus_datastore, document_chunk_one):\n\t    await milvus_datastore.delete(delete_all=True)\n", "    res = await milvus_datastore._upsert(document_chunk_one)\n\t    assert res == list(document_chunk_one.keys())\n\t    milvus_datastore.col.flush()\n\t    query = QueryWithEmbedding(\n\t        query=\"lorem\",\n\t        top_k=1,\n\t        embedding=sample_embedding(0),\n\t        filter=DocumentMetadataFilter(\n\t            start_date=\"2000-01-03T16:39:57-08:00\", end_date=\"2010-01-03T16:39:57-08:00\"\n\t        ),\n", "    )\n\t    query_results = await milvus_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 1 == len(query_results[0].results)\n\t    assert 1.0 != query_results[0].results[0].score\n\t    assert \"def_456\" == query_results[0].results[0].id\n\t    milvus_datastore.col.drop()\n\t@pytest.mark.asyncio\n\tasync def test_delete_with_date_filter(milvus_datastore, document_chunk_one):\n\t    await milvus_datastore.delete(delete_all=True)\n", "    res = await milvus_datastore._upsert(document_chunk_one)\n\t    assert res == list(document_chunk_one.keys())\n\t    milvus_datastore.col.flush()\n\t    await milvus_datastore.delete(\n\t        filter=DocumentMetadataFilter(\n\t            end_date=\"2009-01-03T16:39:57-08:00\",\n\t        )\n\t    )\n\t    query = QueryWithEmbedding(\n\t        query=\"lorem\",\n", "        top_k=9,\n\t        embedding=sample_embedding(0),\n\t    )\n\t    query_results = await milvus_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 1 == len(query_results[0].results)\n\t    assert \"ghi_789\" == query_results[0].results[0].id\n\t    milvus_datastore.col.drop()\n\t@pytest.mark.asyncio\n\tasync def test_delete_with_source_filter(milvus_datastore, document_chunk_one):\n", "    await milvus_datastore.delete(delete_all=True)\n\t    res = await milvus_datastore._upsert(document_chunk_one)\n\t    assert res == list(document_chunk_one.keys())\n\t    milvus_datastore.col.flush()\n\t    await milvus_datastore.delete(\n\t        filter=DocumentMetadataFilter(\n\t            source=Source.email,\n\t        )\n\t    )\n\t    query = QueryWithEmbedding(\n", "        query=\"lorem\",\n\t        top_k=9,\n\t        embedding=sample_embedding(0),\n\t    )\n\t    query_results = await milvus_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 2 == len(query_results[0].results)\n\t    assert \"def_456\" == query_results[0].results[0].id\n\t    milvus_datastore.col.drop()\n\t@pytest.mark.asyncio\n", "async def test_delete_with_document_id_filter(milvus_datastore, document_chunk_one):\n\t    await milvus_datastore.delete(delete_all=True)\n\t    res = await milvus_datastore._upsert(document_chunk_one)\n\t    assert res == list(document_chunk_one.keys())\n\t    milvus_datastore.col.flush()\n\t    await milvus_datastore.delete(\n\t        filter=DocumentMetadataFilter(\n\t            document_id=res[0],\n\t        )\n\t    )\n", "    query = QueryWithEmbedding(\n\t        query=\"lorem\",\n\t        top_k=9,\n\t        embedding=sample_embedding(0),\n\t    )\n\t    query_results = await milvus_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 0 == len(query_results[0].results)\n\t    milvus_datastore.col.drop()\n\t@pytest.mark.asyncio\n", "async def test_delete_with_document_id(milvus_datastore, document_chunk_one):\n\t    await milvus_datastore.delete(delete_all=True)\n\t    res = await milvus_datastore._upsert(document_chunk_one)\n\t    assert res == list(document_chunk_one.keys())\n\t    milvus_datastore.col.flush()\n\t    await milvus_datastore.delete([res[0]])\n\t    query = QueryWithEmbedding(\n\t        query=\"lorem\",\n\t        top_k=9,\n\t        embedding=sample_embedding(0),\n", "    )\n\t    query_results = await milvus_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 0 == len(query_results[0].results)\n\t    milvus_datastore.col.drop()\n\t# if __name__ == '__main__':\n\t#     import sys\n\t#     import pytest\n\t#     pytest.main(sys.argv)\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/tests/datastore/providers/llama/test_llama_datastore.py", "chunked_list": ["from typing import Dict, List\n\timport pytest\n\tfrom datastore.providers.llama_datastore import LlamaDataStore\n\tfrom models.models import DocumentChunk, DocumentChunkMetadata, QueryWithEmbedding\n\tdef create_embedding(non_zero_pos: int, size: int) -> List[float]:\n\t    vector = [0.0] * size\n\t    vector[non_zero_pos % size] = 1.0\n\t    return vector\n\t@pytest.fixture\n\tdef initial_document_chunks() -> Dict[str, List[DocumentChunk]]:\n", "    first_doc_chunks = [\n\t        DocumentChunk(\n\t            id=f\"first-doc-{i}\",\n\t            text=f\"Lorem ipsum {i}\",\n\t            metadata=DocumentChunkMetadata(),\n\t            embedding=create_embedding(i, 5),\n\t        )\n\t        for i in range(4, 7)\n\t    ]\n\t    return {\n", "        \"first-doc\": first_doc_chunks,\n\t    }\n\t@pytest.fixture\n\tdef queries() -> List[QueryWithEmbedding]:\n\t    queries = [\n\t        QueryWithEmbedding(\n\t            query='Query 1',\n\t            top_k=1,\n\t            embedding=create_embedding(4, 5),\n\t        ),\n", "        QueryWithEmbedding(\n\t            query='Query 2',\n\t            top_k=2,\n\t            embedding=create_embedding(5, 5),\n\t        ),\n\t    ]\n\t    return queries\n\t@pytest.fixture\n\tdef llama_datastore() -> LlamaDataStore:\n\t    return LlamaDataStore()\n", "@pytest.mark.asyncio\n\tasync def test_upsert(\n\t    llama_datastore: LlamaDataStore, \n\t    initial_document_chunks: Dict[str, List[DocumentChunk]]\n\t) -> None:\n\t    \"\"\"Test basic upsert.\"\"\"\n\t    doc_ids = await llama_datastore._upsert(initial_document_chunks)\n\t    assert doc_ids == [doc_id for doc_id in initial_document_chunks]\n\t@pytest.mark.asyncio\n\tasync def test_query(\n", "    llama_datastore: LlamaDataStore, \n\t    initial_document_chunks: Dict[str, List[DocumentChunk]],\n\t    queries: List[QueryWithEmbedding],\n\t) -> None:\n\t    \"\"\"Test basic query.\"\"\"\n\t    # insert to prepare for test\n\t    await llama_datastore._upsert(initial_document_chunks)\n\t    query_results = await llama_datastore._query(queries)\n\t    assert len(query_results) == len(queries)\n\t    query_0_results = query_results[0].results \n", "    query_1_results = query_results[1].results\n\t    assert len(query_0_results) == 1\n\t    assert len(query_1_results) == 2\n\t    # NOTE: this is the correct behavior\n\t    assert query_0_results[0].id == 'first-doc-4'\n\t    assert query_1_results[0].id == 'first-doc-5'\n\t    assert query_1_results[1].id == 'first-doc-4'\n\t@pytest.mark.asyncio\n\tasync def test_delete(\n\t    llama_datastore: LlamaDataStore, \n", "    initial_document_chunks: Dict[str, List[DocumentChunk]],\n\t) -> None:\n\t    # insert to prepare for test\n\t    await llama_datastore._upsert(initial_document_chunks)\n\t    is_success = llama_datastore.delete(['first-doc'])\n\t    assert is_success\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/tests/datastore/providers/qdrant/test_qdrant_datastore.py", "chunked_list": ["from typing import Dict, List\n\timport pytest\n\timport qdrant_client\n\tfrom qdrant_client.http.models import PayloadSchemaType\n\tfrom datastore.providers.qdrant_datastore import QdrantDataStore\n\tfrom models.models import (\n\t    DocumentChunk,\n\t    DocumentChunkMetadata,\n\t    QueryWithEmbedding,\n\t    DocumentMetadataFilter,\n", "    Source,\n\t)\n\tdef create_embedding(non_zero_pos: int, size: int) -> List[float]:\n\t    vector = [0.0] * size\n\t    vector[non_zero_pos % size] = 1.0\n\t    return vector\n\t@pytest.fixture\n\tdef qdrant_datastore() -> QdrantDataStore:\n\t    return QdrantDataStore(\n\t        collection_name=\"documents\", vector_size=5, recreate_collection=True\n", "    )\n\t@pytest.fixture\n\tdef client() -> qdrant_client.QdrantClient:\n\t    return qdrant_client.QdrantClient()\n\t@pytest.fixture\n\tdef initial_document_chunks() -> Dict[str, List[DocumentChunk]]:\n\t    first_doc_chunks = [\n\t        DocumentChunk(\n\t            id=f\"first-doc-{i}\",\n\t            text=f\"Lorem ipsum {i}\",\n", "            metadata=DocumentChunkMetadata(),\n\t            embedding=create_embedding(i, 5),\n\t        )\n\t        for i in range(4, 7)\n\t    ]\n\t    return {\n\t        \"first-doc\": first_doc_chunks,\n\t    }\n\t@pytest.fixture\n\tdef document_chunks() -> Dict[str, List[DocumentChunk]]:\n", "    first_doc_chunks = [\n\t        DocumentChunk(\n\t            id=f\"first-doc_{i}\",\n\t            text=f\"Lorem ipsum {i}\",\n\t            metadata=DocumentChunkMetadata(\n\t                source=Source.email, created_at=\"2023-03-05\", document_id=\"first-doc\"\n\t            ),\n\t            embedding=create_embedding(i, 5),\n\t        )\n\t        for i in range(3)\n", "    ]\n\t    second_doc_chunks = [\n\t        DocumentChunk(\n\t            id=f\"second-doc_{i}\",\n\t            text=f\"Dolor sit amet {i}\",\n\t            metadata=DocumentChunkMetadata(\n\t                created_at=\"2023-03-04\", document_id=\"second-doc\"\n\t            ),\n\t            embedding=create_embedding(i + len(first_doc_chunks), 5),\n\t        )\n", "        for i in range(2)\n\t    ]\n\t    return {\n\t        \"first-doc\": first_doc_chunks,\n\t        \"second-doc\": second_doc_chunks,\n\t    }\n\t@pytest.mark.asyncio\n\tasync def test_datastore_creates_payload_indexes(\n\t    qdrant_datastore,\n\t    client,\n", "):\n\t    collection_info = client.get_collection(collection_name=\"documents\")\n\t    assert 2 == len(collection_info.payload_schema)\n\t    assert \"created_at\" in collection_info.payload_schema\n\t    created_at = collection_info.payload_schema[\"created_at\"]\n\t    assert PayloadSchemaType.INTEGER == created_at.data_type\n\t    assert \"metadata.document_id\" in collection_info.payload_schema\n\t    document_id = collection_info.payload_schema[\"metadata.document_id\"]\n\t    assert PayloadSchemaType.KEYWORD == document_id.data_type\n\t@pytest.mark.asyncio\n", "async def test_upsert_creates_all_points(\n\t    qdrant_datastore,\n\t    client,\n\t    document_chunks,\n\t):\n\t    document_ids = await qdrant_datastore._upsert(document_chunks)\n\t    assert 2 == len(document_ids)\n\t    assert 5 == client.count(collection_name=\"documents\").count\n\t@pytest.mark.asyncio\n\tasync def test_upsert_does_not_remove_existing_documents_but_store_new(\n", "    qdrant_datastore,\n\t    client,\n\t    initial_document_chunks,\n\t    document_chunks,\n\t):\n\t    \"\"\"\n\t    This test ensures calling ._upsert no longer removes the existing document chunks,\n\t    as they are currently removed in the .upsert method directly.\n\t    \"\"\"\n\t    # Fill the database with document chunks before running the actual test\n", "    await qdrant_datastore._upsert(initial_document_chunks)\n\t    await qdrant_datastore._upsert(document_chunks)\n\t    assert 8 == client.count(collection_name=\"documents\").count\n\t@pytest.mark.asyncio\n\tasync def test_query_returns_all_on_single_query(qdrant_datastore, document_chunks):\n\t    # Fill the database with document chunks before running the actual test\n\t    await qdrant_datastore._upsert(document_chunks)\n\t    query = QueryWithEmbedding(\n\t        query=\"lorem\",\n\t        top_k=5,\n", "        embedding=[0.5, 0.5, 0.5, 0.5, 0.5],\n\t    )\n\t    query_results = await qdrant_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert \"lorem\" == query_results[0].query\n\t    assert 5 == len(query_results[0].results)\n\t@pytest.mark.asyncio\n\tasync def test_query_returns_closest_entry(qdrant_datastore, document_chunks):\n\t    # Fill the database with document chunks before running the actual test\n\t    await qdrant_datastore._upsert(document_chunks)\n", "    query = QueryWithEmbedding(\n\t        query=\"ipsum\",\n\t        top_k=1,\n\t        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n\t    )\n\t    query_results = await qdrant_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert \"ipsum\" == query_results[0].query\n\t    assert 1 == len(query_results[0].results)\n\t    first_document_chunk = query_results[0].results[0]\n", "    assert 0.0 <= first_document_chunk.score <= 1.0\n\t    assert Source.email == first_document_chunk.metadata.source\n\t    assert \"2023-03-05\" == first_document_chunk.metadata.created_at\n\t    assert \"first-doc\" == first_document_chunk.metadata.document_id\n\t@pytest.mark.asyncio\n\tasync def test_query_filter_by_document_id_returns_this_document_chunks(\n\t    qdrant_datastore, document_chunks\n\t):\n\t    # Fill the database with document chunks before running the actual test\n\t    await qdrant_datastore._upsert(document_chunks)\n", "    first_query = QueryWithEmbedding(\n\t        query=\"dolor\",\n\t        filter=DocumentMetadataFilter(document_id=\"first-doc\"),\n\t        top_k=5,\n\t        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n\t    )\n\t    second_query = QueryWithEmbedding(\n\t        query=\"dolor\",\n\t        filter=DocumentMetadataFilter(document_id=\"second-doc\"),\n\t        top_k=5,\n", "        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n\t    )\n\t    query_results = await qdrant_datastore._query(queries=[first_query, second_query])\n\t    assert 2 == len(query_results)\n\t    assert \"dolor\" == query_results[0].query\n\t    assert \"dolor\" == query_results[1].query\n\t    assert 3 == len(query_results[0].results)\n\t    assert 2 == len(query_results[1].results)\n\t@pytest.mark.asyncio\n\t@pytest.mark.parametrize(\"start_date\", [\"2023-03-05T00:00:00\", \"2023-03-05\"])\n", "async def test_query_start_date_converts_datestring(\n\t    qdrant_datastore,\n\t    document_chunks,\n\t    start_date,\n\t):\n\t    # Fill the database with document chunks before running the actual test\n\t    await qdrant_datastore._upsert(document_chunks)\n\t    query = QueryWithEmbedding(\n\t        query=\"sit amet\",\n\t        filter=DocumentMetadataFilter(start_date=start_date),\n", "        top_k=5,\n\t        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n\t    )\n\t    query_results = await qdrant_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 3 == len(query_results[0].results)\n\t@pytest.mark.asyncio\n\t@pytest.mark.parametrize(\"end_date\", [\"2023-03-04T00:00:00\", \"2023-03-04\"])\n\tasync def test_query_end_date_converts_datestring(\n\t    qdrant_datastore,\n", "    document_chunks,\n\t    end_date,\n\t):\n\t    # Fill the database with document chunks before running the actual test\n\t    await qdrant_datastore._upsert(document_chunks)\n\t    query = QueryWithEmbedding(\n\t        query=\"sit amet\",\n\t        filter=DocumentMetadataFilter(end_date=end_date),\n\t        top_k=5,\n\t        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],\n", "    )\n\t    query_results = await qdrant_datastore._query(queries=[query])\n\t    assert 1 == len(query_results)\n\t    assert 2 == len(query_results[0].results)\n\t@pytest.mark.asyncio\n\tasync def test_delete_removes_by_ids(\n\t    qdrant_datastore,\n\t    client,\n\t    document_chunks,\n\t):\n", "    # Fill the database with document chunks before running the actual test\n\t    await qdrant_datastore._upsert(document_chunks)\n\t    await qdrant_datastore.delete(ids=[\"first-doc\"])\n\t    assert 2 == client.count(collection_name=\"documents\").count\n\t@pytest.mark.asyncio\n\tasync def test_delete_removes_by_document_id_filter(\n\t    qdrant_datastore,\n\t    client,\n\t    document_chunks,\n\t):\n", "    # Fill the database with document chunks before running the actual test\n\t    await qdrant_datastore._upsert(document_chunks)\n\t    await qdrant_datastore.delete(\n\t        filter=DocumentMetadataFilter(document_id=\"first-doc\")\n\t    )\n\t    assert 2 == client.count(collection_name=\"documents\").count\n\t@pytest.mark.asyncio\n\tasync def test_delete_removes_all(\n\t    qdrant_datastore,\n\t    client,\n", "    document_chunks,\n\t):\n\t    # Fill the database with document chunks before running the actual test\n\t    await qdrant_datastore._upsert(document_chunks)\n\t    await qdrant_datastore.delete(delete_all=True)\n\t    assert 0 == client.count(collection_name=\"documents\").count\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/server/main.py", "chunked_list": ["import os\n\tfrom typing import Optional\n\timport uvicorn\n\tfrom fastapi import FastAPI, File, Form, HTTPException, Depends, Body, UploadFile\n\tfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\tfrom fastapi.staticfiles import StaticFiles\n\tfrom models.api import (\n\t    DeleteRequest,\n\t    DeleteResponse,\n\t    QueryRequest,\n", "    QueryResponse,\n\t    UpsertRequest,\n\t    UpsertResponse,\n\t)\n\tfrom datastore.factory import get_datastore\n\tfrom services.file import get_document_from_file\n\tfrom models.models import DocumentMetadata, Source\n\tbearer_scheme = HTTPBearer()\n\tBEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\")\n\tassert BEARER_TOKEN is not None\n", "def validate_token(credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)):\n\t    if credentials.scheme != \"Bearer\" or credentials.credentials != BEARER_TOKEN:\n\t        raise HTTPException(status_code=401, detail=\"Invalid or missing token\")\n\t    return credentials\n\tapp = FastAPI(dependencies=[Depends(validate_token)])\n\tapp.mount(\"/.well-known\", StaticFiles(directory=\".well-known\"), name=\"static\")\n\t# Create a sub-application, in order to access just the query endpoint in an OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally\n\tsub_app = FastAPI(\n\t    title=\"Retrieval Plugin API\",\n\t    description=\"A retrieval API for querying and filtering documents based on natural language queries and metadata\",\n", "    version=\"1.0.0\",\n\t    servers=[{\"url\": \"https://your-app-url.com\"}],\n\t    dependencies=[Depends(validate_token)],\n\t)\n\tapp.mount(\"/sub\", sub_app)\n\t@app.post(\n\t    \"/upsert-file\",\n\t    response_model=UpsertResponse,\n\t)\n\tasync def upsert_file(\n", "    file: UploadFile = File(...),\n\t    metadata: Optional[str] = Form(None),\n\t):\n\t    try:\n\t        metadata_obj = (\n\t            DocumentMetadata.parse_raw(metadata)\n\t            if metadata\n\t            else DocumentMetadata(source=Source.file)\n\t        )\n\t    except:\n", "        metadata_obj = DocumentMetadata(source=Source.file)\n\t    document = await get_document_from_file(file, metadata_obj)\n\t    try:\n\t        ids = await datastore.upsert([document])\n\t        return UpsertResponse(ids=ids)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=f\"str({e})\")\n\t@app.post(\n\t    \"/upsert\",\n", "    response_model=UpsertResponse,\n\t)\n\tasync def upsert(\n\t    request: UpsertRequest = Body(...),\n\t):\n\t    try:\n\t        ids = await datastore.upsert(request.documents)\n\t        return UpsertResponse(ids=ids)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n", "        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.post(\n\t    \"/query\",\n\t    response_model=QueryResponse,\n\t)\n\tasync def query_main(\n\t    request: QueryRequest = Body(...),\n\t):\n\t    try:\n\t        results = await datastore.query(\n", "            request.queries,\n\t        )\n\t        return QueryResponse(results=results)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@sub_app.post(\n\t    \"/query\",\n\t    response_model=QueryResponse,\n\t    # NOTE: We are describing the shape of the API endpoint input due to a current limitation in parsing arrays of objects from OpenAPI schemas. This will not be necessary in the future.\n", "    description=\"Accepts search query objects array each with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.\",\n\t)\n\tasync def query(\n\t    request: QueryRequest = Body(...),\n\t):\n\t    try:\n\t        results = await datastore.query(\n\t            request.queries,\n\t        )\n\t        return QueryResponse(results=results)\n", "    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.delete(\n\t    \"/delete\",\n\t    response_model=DeleteResponse,\n\t)\n\tasync def delete(\n\t    request: DeleteRequest = Body(...),\n\t):\n", "    if not (request.ids or request.filter or request.delete_all):\n\t        raise HTTPException(\n\t            status_code=400,\n\t            detail=\"One of ids, filter, or delete_all is required\",\n\t        )\n\t    try:\n\t        success = await datastore.delete(\n\t            ids=request.ids,\n\t            filter=request.filter,\n\t            delete_all=request.delete_all,\n", "        )\n\t        return DeleteResponse(success=success)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.on_event(\"startup\")\n\tasync def startup():\n\t    global datastore\n\t    datastore = await get_datastore()\n\tdef start():\n", "    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/local-server/main.py", "chunked_list": ["# This is a version of the main.py file found in ../../../server/main.py for testing the plugin locally.\n\t# Use the command `poetry run dev` to run this.\n\tfrom typing import Optional\n\timport uvicorn\n\tfrom fastapi import FastAPI, File, Form, HTTPException, Body, UploadFile\n\tfrom models.api import (\n\t    DeleteRequest,\n\t    DeleteResponse,\n\t    QueryRequest,\n\t    QueryResponse,\n", "    UpsertRequest,\n\t    UpsertResponse,\n\t)\n\tfrom datastore.factory import get_datastore\n\tfrom services.file import get_document_from_file\n\tfrom starlette.responses import FileResponse\n\tfrom models.models import DocumentMetadata, Source\n\tfrom fastapi.middleware.cors import CORSMiddleware\n\tapp = FastAPI()\n\tPORT = 3333\n", "origins = [\n\t    f\"http://localhost:{PORT}\",\n\t    \"https://chat.openai.com\",\n\t]\n\tapp.add_middleware(\n\t    CORSMiddleware,\n\t    allow_origins=origins,\n\t    allow_credentials=True,\n\t    allow_methods=[\"*\"],\n\t    allow_headers=[\"*\"],\n", ")\n\t@app.route(\"/.well-known/ai-plugin.json\")\n\tasync def get_manifest(request):\n\t    file_path = \"./local-server/ai-plugin.json\"\n\t    return FileResponse(file_path, media_type=\"text/json\")\n\t@app.route(\"/.well-known/logo.png\")\n\tasync def get_logo(request):\n\t    file_path = \"./local-server/logo.png\"\n\t    return FileResponse(file_path, media_type=\"text/json\")\n\t@app.route(\"/.well-known/openapi.yaml\")\n", "async def get_openapi(request):\n\t    file_path = \"./local-server/openapi.yaml\"\n\t    return FileResponse(file_path, media_type=\"text/json\")\n\t@app.post(\n\t    \"/upsert-file\",\n\t    response_model=UpsertResponse,\n\t)\n\tasync def upsert_file(\n\t    file: UploadFile = File(...),\n\t    metadata: Optional[str] = Form(None),\n", "):\n\t    try:\n\t        metadata_obj = (\n\t            DocumentMetadata.parse_raw(metadata)\n\t            if metadata\n\t            else DocumentMetadata(source=Source.file)\n\t        )\n\t    except:\n\t        metadata_obj = DocumentMetadata(source=Source.file)\n\t    document = await get_document_from_file(file, metadata_obj)\n", "    try:\n\t        ids = await datastore.upsert([document])\n\t        return UpsertResponse(ids=ids)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=f\"str({e})\")\n\t@app.post(\n\t    \"/upsert\",\n\t    response_model=UpsertResponse,\n\t)\n", "async def upsert(\n\t    request: UpsertRequest = Body(...),\n\t):\n\t    try:\n\t        ids = await datastore.upsert(request.documents)\n\t        return UpsertResponse(ids=ids)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.post(\"/query\", response_model=QueryResponse)\n", "async def query_main(request: QueryRequest = Body(...)):\n\t    try:\n\t        results = await datastore.query(\n\t            request.queries,\n\t        )\n\t        return QueryResponse(results=results)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.delete(\n", "    \"/delete\",\n\t    response_model=DeleteResponse,\n\t)\n\tasync def delete(\n\t    request: DeleteRequest = Body(...),\n\t):\n\t    if not (request.ids or request.filter or request.delete_all):\n\t        raise HTTPException(\n\t            status_code=400,\n\t            detail=\"One of ids, filter, or delete_all is required\",\n", "        )\n\t    try:\n\t        success = await datastore.delete(\n\t            ids=request.ids,\n\t            filter=request.filter,\n\t            delete_all=request.delete_all,\n\t        )\n\t        return DeleteResponse(success=success)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n", "        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.on_event(\"startup\")\n\tasync def startup():\n\t    global datastore\n\t    datastore = await get_datastore()\n\tdef start():\n\t    uvicorn.run(\"local-server.main:app\", host=\"localhost\", port=PORT, reload=True)\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/__init__.py", "chunked_list": []}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/datastore.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import Dict, List, Optional\n\timport asyncio\n\tfrom models.models import (\n\t    Document,\n\t    DocumentChunk,\n\t    DocumentMetadataFilter,\n\t    Query,\n\t    QueryResult,\n\t    QueryWithEmbedding,\n", ")\n\tfrom services.chunks import get_document_chunks\n\tfrom services.openai import get_embeddings\n\tclass DataStore(ABC):\n\t    async def upsert(\n\t        self, documents: List[Document], chunk_token_size: Optional[int] = None\n\t    ) -> List[str]:\n\t        \"\"\"\n\t        Takes in a list of documents and inserts them into the database.\n\t        First deletes all the existing vectors with the document id (if necessary, depends on the vector db), then inserts the new ones.\n", "        Return a list of document ids.\n\t        \"\"\"\n\t        # Delete any existing vectors for documents with the input document ids\n\t        await asyncio.gather(\n\t            *[\n\t                self.delete(\n\t                    filter=DocumentMetadataFilter(\n\t                        document_id=document.id,\n\t                    ),\n\t                    delete_all=False,\n", "                )\n\t                for document in documents\n\t                if document.id\n\t            ]\n\t        )\n\t        chunks = get_document_chunks(documents, chunk_token_size)\n\t        return await self._upsert(chunks)\n\t    @abstractmethod\n\t    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n\t        \"\"\"\n", "        Takes in a list of list of document chunks and inserts them into the database.\n\t        Return a list of document ids.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    async def query(self, queries: List[Query]) -> List[QueryResult]:\n\t        \"\"\"\n\t        Takes in a list of queries and filters and returns a list of query results with matching document chunks and scores.\n\t        \"\"\"\n\t        # get a list of of just the queries from the Query list\n\t        query_texts = [query.query for query in queries]\n", "        query_embeddings = get_embeddings(query_texts)\n\t        # hydrate the queries with embeddings\n\t        queries_with_embeddings = [\n\t            QueryWithEmbedding(**query.dict(), embedding=embedding)\n\t            for query, embedding in zip(queries, query_embeddings)\n\t        ]\n\t        return await self._query(queries_with_embeddings)\n\t    @abstractmethod\n\t    async def _query(self, queries: List[QueryWithEmbedding]) -> List[QueryResult]:\n\t        \"\"\"\n", "        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    async def delete(\n\t        self,\n\t        ids: Optional[List[str]] = None,\n\t        filter: Optional[DocumentMetadataFilter] = None,\n\t        delete_all: Optional[bool] = None,\n\t    ) -> bool:\n", "        \"\"\"\n\t        Removes vectors by ids, filter, or everything in the datastore.\n\t        Multiple parameters can be used at once.\n\t        Returns whether the operation was successful.\n\t        \"\"\"\n\t        raise NotImplementedError\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/factory.py", "chunked_list": ["from datastore.datastore import DataStore\n\timport os\n\tasync def get_datastore() -> DataStore:\n\t    datastore = os.environ.get(\"DATASTORE\")\n\t    assert datastore is not None\n\t    match datastore:\n\t        case \"llama\":\n\t            from datastore.providers.llama_datastore import LlamaDataStore\n\t            return LlamaDataStore()\n\t        case \"pinecone\":\n", "            from datastore.providers.pinecone_datastore import PineconeDataStore\n\t            return PineconeDataStore()\n\t        case \"weaviate\":\n\t            from datastore.providers.weaviate_datastore import WeaviateDataStore\n\t            return WeaviateDataStore()\n\t        case \"milvus\":\n\t            from datastore.providers.milvus_datastore import MilvusDataStore\n\t            return MilvusDataStore()\n\t        case \"zilliz\":\n\t            from datastore.providers.zilliz_datastore import ZillizDataStore\n", "            return ZillizDataStore()\n\t        case \"redis\":\n\t            from datastore.providers.redis_datastore import RedisDataStore\n\t            return await RedisDataStore.init()\n\t        case \"qdrant\":\n\t            from datastore.providers.qdrant_datastore import QdrantDataStore\n\t            return QdrantDataStore()\n\t        case _:\n\t            raise ValueError(f\"Unsupported vector database: {datastore}\")\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/providers/pinecone_datastore.py", "chunked_list": ["import os\n\tfrom typing import Any, Dict, List, Optional\n\timport pinecone\n\tfrom tenacity import retry, wait_random_exponential, stop_after_attempt\n\timport asyncio\n\tfrom datastore.datastore import DataStore\n\tfrom models.models import (\n\t    DocumentChunk,\n\t    DocumentChunkMetadata,\n\t    DocumentChunkWithScore,\n", "    DocumentMetadataFilter,\n\t    QueryResult,\n\t    QueryWithEmbedding,\n\t    Source,\n\t)\n\tfrom services.date import to_unix_timestamp\n\t# Read environment variables for Pinecone configuration\n\tPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n\tPINECONE_ENVIRONMENT = os.environ.get(\"PINECONE_ENVIRONMENT\")\n\tPINECONE_INDEX = os.environ.get(\"PINECONE_INDEX\")\n", "assert PINECONE_API_KEY is not None\n\tassert PINECONE_ENVIRONMENT is not None\n\tassert PINECONE_INDEX is not None\n\t# Initialize Pinecone with the API key and environment\n\tpinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n\t# Set the batch size for upserting vectors to Pinecone\n\tUPSERT_BATCH_SIZE = 100\n\tclass PineconeDataStore(DataStore):\n\t    def __init__(self):\n\t        # Check if the index name is specified and exists in Pinecone\n", "        if PINECONE_INDEX and PINECONE_INDEX not in pinecone.list_indexes():\n\t            # Get all fields in the metadata object in a list\n\t            fields_to_index = list(DocumentChunkMetadata.__fields__.keys())\n\t            # Create a new index with the specified name, dimension, and metadata configuration\n\t            try:\n\t                print(\n\t                    f\"Creating index {PINECONE_INDEX} with metadata config {fields_to_index}\"\n\t                )\n\t                pinecone.create_index(\n\t                    PINECONE_INDEX,\n", "                    dimension=1536,  # dimensionality of OpenAI ada v2 embeddings\n\t                    metadata_config={\"indexed\": fields_to_index},\n\t                )\n\t                self.index = pinecone.Index(PINECONE_INDEX)\n\t                print(f\"Index {PINECONE_INDEX} created successfully\")\n\t            except Exception as e:\n\t                print(f\"Error creating index {PINECONE_INDEX}: {e}\")\n\t                raise e\n\t        elif PINECONE_INDEX and PINECONE_INDEX in pinecone.list_indexes():\n\t            # Connect to an existing index with the specified name\n", "            try:\n\t                print(f\"Connecting to existing index {PINECONE_INDEX}\")\n\t                self.index = pinecone.Index(PINECONE_INDEX)\n\t                print(f\"Connected to index {PINECONE_INDEX} successfully\")\n\t            except Exception as e:\n\t                print(f\"Error connecting to index {PINECONE_INDEX}: {e}\")\n\t                raise e\n\t    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n\t    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n\t        \"\"\"\n", "        Takes in a dict from document id to list of document chunks and inserts them into the index.\n\t        Return a list of document ids.\n\t        \"\"\"\n\t        # Initialize a list of ids to return\n\t        doc_ids: List[str] = []\n\t        # Initialize a list of vectors to upsert\n\t        vectors = []\n\t        # Loop through the dict items\n\t        for doc_id, chunk_list in chunks.items():\n\t            # Append the id to the ids list\n", "            doc_ids.append(doc_id)\n\t            print(f\"Upserting document_id: {doc_id}\")\n\t            for chunk in chunk_list:\n\t                # Create a vector tuple of (id, embedding, metadata)\n\t                # Convert the metadata object to a dict with unix timestamps for dates\n\t                pinecone_metadata = self._get_pinecone_metadata(chunk.metadata)\n\t                # Add the text and document id to the metadata dict\n\t                pinecone_metadata[\"text\"] = chunk.text\n\t                pinecone_metadata[\"document_id\"] = doc_id\n\t                vector = (chunk.id, chunk.embedding, pinecone_metadata)\n", "                vectors.append(vector)\n\t        # Split the vectors list into batches of the specified size\n\t        batches = [\n\t            vectors[i : i + UPSERT_BATCH_SIZE]\n\t            for i in range(0, len(vectors), UPSERT_BATCH_SIZE)\n\t        ]\n\t        # Upsert each batch to Pinecone\n\t        for batch in batches:\n\t            try:\n\t                print(f\"Upserting batch of size {len(batch)}\")\n", "                self.index.upsert(vectors=batch)\n\t                print(f\"Upserted batch successfully\")\n\t            except Exception as e:\n\t                print(f\"Error upserting batch: {e}\")\n\t                raise e\n\t        return doc_ids\n\t    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n\t    async def _query(\n\t        self,\n\t        queries: List[QueryWithEmbedding],\n", "    ) -> List[QueryResult]:\n\t        \"\"\"\n\t        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n\t        \"\"\"\n\t        # Define a helper coroutine that performs a single query and returns a QueryResult\n\t        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n\t            print(f\"Query: {query.query}\")\n\t            # Convert the metadata filter object to a dict with pinecone filter expressions\n\t            pinecone_filter = self._get_pinecone_filter(query.filter)\n\t            try:\n", "                # Query the index with the query embedding, filter, and top_k\n\t                query_response = self.index.query(\n\t                    # namespace=namespace,\n\t                    top_k=query.top_k,\n\t                    vector=query.embedding,\n\t                    filter=pinecone_filter,\n\t                    include_metadata=True,\n\t                )\n\t            except Exception as e:\n\t                print(f\"Error querying index: {e}\")\n", "                raise e\n\t            query_results: List[DocumentChunkWithScore] = []\n\t            for result in query_response.matches:\n\t                score = result.score\n\t                metadata = result.metadata\n\t                # Remove document id and text from metadata and store it in a new variable\n\t                metadata_without_text = (\n\t                    {key: value for key, value in metadata.items() if key != \"text\"}\n\t                    if metadata\n\t                    else None\n", "                )\n\t                # If the source is not a valid Source in the Source enum, set it to None\n\t                if (\n\t                    metadata_without_text\n\t                    and \"source\" in metadata_without_text\n\t                    and metadata_without_text[\"source\"] not in Source.__members__\n\t                ):\n\t                    metadata_without_text[\"source\"] = None\n\t                # Create a document chunk with score object with the result data\n\t                result = DocumentChunkWithScore(\n", "                    id=result.id,\n\t                    score=score,\n\t                    text=metadata[\"text\"] if metadata and \"text\" in metadata else None,\n\t                    metadata=metadata_without_text,\n\t                )\n\t                query_results.append(result)\n\t            return QueryResult(query=query.query, results=query_results)\n\t        # Use asyncio.gather to run multiple _single_query coroutines concurrently and collect their results\n\t        results: List[QueryResult] = await asyncio.gather(\n\t            *[_single_query(query) for query in queries]\n", "        )\n\t        return results\n\t    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n\t    async def delete(\n\t        self,\n\t        ids: Optional[List[str]] = None,\n\t        filter: Optional[DocumentMetadataFilter] = None,\n\t        delete_all: Optional[bool] = None,\n\t    ) -> bool:\n\t        \"\"\"\n", "        Removes vectors by ids, filter, or everything from the index.\n\t        \"\"\"\n\t        # Delete all vectors from the index if delete_all is True\n\t        if delete_all:\n\t            try:\n\t                print(f\"Deleting all vectors from index\")\n\t                self.index.delete(delete_all=True)\n\t                print(f\"Deleted all vectors successfully\")\n\t                return True\n\t            except Exception as e:\n", "                print(f\"Error deleting all vectors: {e}\")\n\t                raise e\n\t        # Convert the metadata filter object to a dict with pinecone filter expressions\n\t        pinecone_filter = self._get_pinecone_filter(filter)\n\t        # Delete vectors that match the filter from the index if the filter is not empty\n\t        if pinecone_filter != {}:\n\t            try:\n\t                print(f\"Deleting vectors with filter {pinecone_filter}\")\n\t                self.index.delete(filter=pinecone_filter)\n\t                print(f\"Deleted vectors with filter successfully\")\n", "            except Exception as e:\n\t                print(f\"Error deleting vectors with filter: {e}\")\n\t                raise e\n\t        # Delete vectors that match the document ids from the index if the ids list is not empty\n\t        if ids is not None and len(ids) > 0:\n\t            try:\n\t                print(f\"Deleting vectors with ids {ids}\")\n\t                pinecone_filter = {\"document_id\": {\"$in\": ids}}\n\t                self.index.delete(filter=pinecone_filter)  # type: ignore\n\t                print(f\"Deleted vectors with ids successfully\")\n", "            except Exception as e:\n\t                print(f\"Error deleting vectors with ids: {e}\")\n\t                raise e\n\t        return True\n\t    def _get_pinecone_filter(\n\t        self, filter: Optional[DocumentMetadataFilter] = None\n\t    ) -> Dict[str, Any]:\n\t        if filter is None:\n\t            return {}\n\t        pinecone_filter = {}\n", "        # For each field in the MetadataFilter, check if it has a value and add the corresponding pinecone filter expression\n\t        # For start_date and end_date, uses the $gte and $lte operators respectively\n\t        # For other fields, uses the $eq operator\n\t        for field, value in filter.dict().items():\n\t            if value is not None:\n\t                if field == \"start_date\":\n\t                    pinecone_filter[\"date\"] = pinecone_filter.get(\"date\", {})\n\t                    pinecone_filter[\"date\"][\"$gte\"] = to_unix_timestamp(value)\n\t                elif field == \"end_date\":\n\t                    pinecone_filter[\"date\"] = pinecone_filter.get(\"date\", {})\n", "                    pinecone_filter[\"date\"][\"$lte\"] = to_unix_timestamp(value)\n\t                else:\n\t                    pinecone_filter[field] = value\n\t        return pinecone_filter\n\t    def _get_pinecone_metadata(\n\t        self, metadata: Optional[DocumentChunkMetadata] = None\n\t    ) -> Dict[str, Any]:\n\t        if metadata is None:\n\t            return {}\n\t        pinecone_metadata = {}\n", "        # For each field in the Metadata, check if it has a value and add it to the pinecone metadata dict\n\t        # For fields that are dates, convert them to unix timestamps\n\t        for field, value in metadata.dict().items():\n\t            if value is not None:\n\t                if field in [\"created_at\"]:\n\t                    pinecone_metadata[field] = to_unix_timestamp(value)\n\t                else:\n\t                    pinecone_metadata[field] = value\n\t        return pinecone_metadata\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/providers/redis_datastore.py", "chunked_list": ["import asyncio\n\timport logging\n\timport os\n\timport re\n\timport json\n\timport redis.asyncio as redis\n\timport numpy as np\n\tfrom redis.commands.search.query import Query as RediSearchQuery\n\tfrom redis.commands.search.indexDefinition import IndexDefinition, IndexType\n\tfrom redis.commands.search.field import (\n", "    TagField,\n\t    TextField,\n\t    NumericField,\n\t    VectorField,\n\t)\n\tfrom typing import Dict, List, Optional\n\tfrom datastore.datastore import DataStore\n\tfrom models.models import (\n\t    DocumentChunk,\n\t    DocumentMetadataFilter,\n", "    DocumentChunkWithScore,\n\t    DocumentMetadataFilter,\n\t    QueryResult,\n\t    QueryWithEmbedding,\n\t)\n\tfrom services.date import to_unix_timestamp\n\t# Read environment variables for Redis\n\tREDIS_HOST = os.environ.get(\"REDIS_HOST\", \"localhost\")\n\tREDIS_PORT = int(os.environ.get(\"REDIS_PORT\", 6379))\n\tREDIS_PASSWORD = os.environ.get(\"REDIS_PASSWORD\")\n", "REDIS_INDEX_NAME = os.environ.get(\"REDIS_INDEX_NAME\", \"index\")\n\tREDIS_DOC_PREFIX = os.environ.get(\"REDIS_DOC_PREFIX\", \"doc\")\n\tREDIS_DISTANCE_METRIC = os.environ.get(\"REDIS_DISTANCE_METRIC\", \"COSINE\")\n\tREDIS_INDEX_TYPE = os.environ.get(\"REDIS_INDEX_TYPE\", \"FLAT\")\n\tassert REDIS_INDEX_TYPE in (\"FLAT\", \"HNSW\")\n\t# OpenAI Ada Embeddings Dimension\n\tVECTOR_DIMENSION = 1536\n\t# RediSearch constants\n\tREDIS_REQUIRED_MODULES = [\n\t    {\"name\": \"search\", \"ver\": 20600},\n", "    {\"name\": \"ReJSON\", \"ver\": 20404}\n\t]\n\tREDIS_DEFAULT_ESCAPED_CHARS = re.compile(r\"[,.<>{}\\[\\]\\\\\\\"\\':;!@#$%^&*()\\-+=~\\/ ]\")\n\t# Helper functions\n\tdef unpack_schema(d: dict):\n\t    for v in d.values():\n\t        if isinstance(v, dict):\n\t            yield from unpack_schema(v)\n\t        else:\n\t            yield v\n", "async def _check_redis_module_exist(client: redis.Redis, modules: List[dict]):\n\t    installed_modules = (await client.info()).get(\"modules\", [])\n\t    installed_modules = {module[\"name\"]: module for module in installed_modules}\n\t    for module in modules:\n\t        if module[\"name\"] not in installed_modules or int(installed_modules[module[\"name\"]][\"ver\"]) < int(module[\"ver\"]):\n\t            error_message =  \"You must add the RediSearch (>= 2.6) and ReJSON (>= 2.4) modules from Redis Stack. \" \\\n\t                \"Please refer to Redis Stack docs: https://redis.io/docs/stack/\"\n\t            logging.error(error_message)\n\t            raise AttributeError(error_message)\n\tclass RedisDataStore(DataStore):\n", "    def __init__(self, client: redis.Redis, redisearch_schema):\n\t        self.client = client\n\t        self._schema = redisearch_schema\n\t        # Init default metadata with sentinel values in case the document written has no metadata\n\t        self._default_metadata = {\n\t            field: \"_null_\" for field in redisearch_schema[\"metadata\"]\n\t        }\n\t    ### Redis Helper Methods ###\n\t    @classmethod\n\t    async def init(cls, **kwargs):\n", "        \"\"\"\n\t        Setup the index if it does not exist.\n\t        \"\"\"\n\t        try:\n\t            # Connect to the Redis Client\n\t            logging.info(\"Connecting to Redis\")\n\t            client = redis.Redis(\n\t                host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD\n\t            )\n\t        except Exception as e:\n", "            logging.error(f\"Error setting up Redis: {e}\")\n\t            raise e\n\t        await _check_redis_module_exist(client, modules=REDIS_REQUIRED_MODULES)\n\t        dim = kwargs.get(\"dim\", VECTOR_DIMENSION)\n\t        redisearch_schema = {\n\t            \"document_id\": TagField(\"$.document_id\", as_name=\"document_id\"),\n\t            \"metadata\": {\n\t                \"source_id\": TagField(\"$.metadata.source_id\", as_name=\"source_id\"),\n\t                \"source\": TagField(\"$.metadata.source\", as_name=\"source\"),\n\t                \"author\": TextField(\"$.metadata.author\", as_name=\"author\"),\n", "                \"created_at\": NumericField(\"$.metadata.created_at\", as_name=\"created_at\"),\n\t            },\n\t            \"embedding\": VectorField(\n\t                \"$.embedding\",\n\t                REDIS_INDEX_TYPE,\n\t                {\n\t                    \"TYPE\": \"FLOAT64\",\n\t                    \"DIM\": dim,\n\t                    \"DISTANCE_METRIC\": REDIS_DISTANCE_METRIC,\n\t                },\n", "                as_name=\"embedding\",\n\t            ),\n\t        }\n\t        try:\n\t            # Check for existence of RediSearch Index\n\t            await client.ft(REDIS_INDEX_NAME).info()\n\t            logging.info(f\"RediSearch index {REDIS_INDEX_NAME} already exists\")\n\t        except:\n\t            # Create the RediSearch Index\n\t            logging.info(f\"Creating new RediSearch index {REDIS_INDEX_NAME}\")\n", "            definition = IndexDefinition(\n\t                prefix=[REDIS_DOC_PREFIX], index_type=IndexType.JSON\n\t            )\n\t            fields = list(unpack_schema(redisearch_schema))\n\t            logging.info(f\"Creating index with fields: {fields}\")\n\t            await client.ft(REDIS_INDEX_NAME).create_index(\n\t                fields=fields, definition=definition\n\t            )\n\t        return cls(client, redisearch_schema)\n\t    @staticmethod\n", "    def _redis_key(document_id: str, chunk_id: str) -> str:\n\t        \"\"\"\n\t        Create the JSON key for document chunks in Redis.\n\t        Args:\n\t            document_id (str): Document Identifier\n\t            chunk_id (str): Chunk Identifier\n\t        Returns:\n\t            str: JSON key string.\n\t        \"\"\"\n\t        return f\"doc:{document_id}:chunk:{chunk_id}\"\n", "    @staticmethod\n\t    def _escape(value: str) -> str:\n\t        \"\"\"\n\t        Escape filter value.\n\t        Args:\n\t            value (str): Value to escape.\n\t        Returns:\n\t            str: Escaped filter value for RediSearch.\n\t        \"\"\"\n\t        def escape_symbol(match) -> str:\n", "            value = match.group(0)\n\t            return f\"\\\\{value}\"\n\t        return REDIS_DEFAULT_ESCAPED_CHARS.sub(escape_symbol, value)\n\t    def _get_redis_chunk(self, chunk: DocumentChunk) -> dict:\n\t        \"\"\"\n\t        Convert DocumentChunk into a JSON object for storage\n\t        in Redis.\n\t        Args:\n\t            chunk (DocumentChunk): Chunk of a Document.\n\t        Returns:\n", "            dict: JSON object for storage in Redis.\n\t        \"\"\"\n\t        # Convert chunk -> dict\n\t        data = chunk.__dict__\n\t        metadata = chunk.metadata.__dict__\n\t        data[\"chunk_id\"] = data.pop(\"id\")\n\t        # Prep Redis Metadata\n\t        redis_metadata = dict(self._default_metadata)\n\t        if metadata:\n\t            for field, value in metadata.items():\n", "                if value:\n\t                    if field == \"created_at\":\n\t                        redis_metadata[field] = to_unix_timestamp(value)  # type: ignore\n\t                    else:\n\t                        redis_metadata[field] = value\n\t        data[\"metadata\"] = redis_metadata\n\t        return data\n\t    def _get_redis_query(self, query: QueryWithEmbedding) -> RediSearchQuery:\n\t        \"\"\"\n\t        Convert a QueryWithEmbedding into a RediSearchQuery.\n", "        Args:\n\t            query (QueryWithEmbedding): Search query.\n\t        Returns:\n\t            RediSearchQuery: Query for RediSearch.\n\t        \"\"\"\n\t        filter_str: str = \"\"\n\t        # RediSearch field type to query string\n\t        def _typ_to_str(typ, field, value) -> str:  # type: ignore\n\t            if isinstance(typ, TagField):\n\t                return f\"@{field}:{{{self._escape(value)}}} \"\n", "            elif isinstance(typ, TextField):\n\t                return f\"@{field}:{self._escape(value)} \"\n\t            elif isinstance(typ, NumericField):\n\t                num = to_unix_timestamp(value)\n\t                match field:\n\t                    case \"start_date\":\n\t                        return f\"@{field}:[{num} +inf] \"\n\t                    case \"end_date\":\n\t                        return f\"@{field}:[-inf {num}] \"\n\t        # Build filter\n", "        if query.filter:\n\t            redisearch_schema = self._schema\n\t            for field, value in query.filter.__dict__.items():\n\t                if not value:\n\t                    continue\n\t                if field in redisearch_schema:\n\t                    filter_str += _typ_to_str(redisearch_schema[field], field, value)\n\t                elif field in redisearch_schema[\"metadata\"]:\n\t                    if field == \"source\":  # handle the enum\n\t                        value = value.value\n", "                    filter_str += _typ_to_str(\n\t                        redisearch_schema[\"metadata\"][field], field, value\n\t                    )\n\t                elif field in [\"start_date\", \"end_date\"]:\n\t                    filter_str += _typ_to_str(\n\t                        redisearch_schema[\"metadata\"][\"created_at\"], field, value\n\t                    )\n\t        # Postprocess filter string\n\t        filter_str = filter_str.strip()\n\t        filter_str = filter_str if filter_str else \"*\"\n", "        # Prepare query string\n\t        query_str = (\n\t            f\"({filter_str})=>[KNN {query.top_k} @embedding $embedding as score]\"\n\t        )\n\t        return (\n\t            RediSearchQuery(query_str)\n\t            .sort_by(\"score\")\n\t            .paging(0, query.top_k)\n\t            .dialect(2)\n\t        )\n", "    async def _redis_delete(self, keys: List[str]):\n\t        \"\"\"\n\t        Delete a list of keys from Redis.\n\t        Args:\n\t            keys (List[str]): List of keys to delete.\n\t        \"\"\"\n\t        # Delete the keys\n\t        await asyncio.gather(*[self.client.delete(key) for key in keys])\n\t    #######\n\t    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n", "        \"\"\"\n\t        Takes in a list of list of document chunks and inserts them into the database.\n\t        Return a list of document ids.\n\t        \"\"\"\n\t        # Initialize a list of ids to return\n\t        doc_ids: List[str] = []\n\t        # Loop through the dict items\n\t        for doc_id, chunk_list in chunks.items():\n\t            # Append the id to the ids list\n\t            doc_ids.append(doc_id)\n", "            # Write chunks in a pipelines\n\t            async with self.client.pipeline(transaction=False) as pipe:\n\t                for chunk in chunk_list:\n\t                    key = self._redis_key(doc_id, chunk.id)\n\t                    data = self._get_redis_chunk(chunk)\n\t                    await pipe.json().set(key, \"$\", data)\n\t                await pipe.execute()\n\t        return doc_ids\n\t    async def _query(\n\t        self,\n", "        queries: List[QueryWithEmbedding],\n\t    ) -> List[QueryResult]:\n\t        \"\"\"\n\t        Takes in a list of queries with embeddings and filters and\n\t        returns a list of query results with matching document chunks and scores.\n\t        \"\"\"\n\t        # Prepare query responses and results object\n\t        results: List[QueryResult] = []\n\t        # Gather query results in a pipeline\n\t        logging.info(f\"Gathering {len(queries)} query results\", flush=True)\n", "        for query in queries:\n\t            logging.info(f\"Query: {query.query}\")\n\t            query_results: List[DocumentChunkWithScore] = []\n\t            # Extract Redis query\n\t            redis_query: RediSearchQuery = self._get_redis_query(query)\n\t            embedding = np.array(query.embedding, dtype=np.float64).tobytes()\n\t            # Perform vector search\n\t            query_response = await self.client.ft(REDIS_INDEX_NAME).search(\n\t                redis_query, {\"embedding\": embedding}\n\t            )\n", "            # Iterate through the most similar documents\n\t            for doc in query_response.docs:\n\t                # Load JSON data\n\t                doc_json = json.loads(doc.json)\n\t                # Create document chunk object with score\n\t                result = DocumentChunkWithScore(\n\t                    id=doc_json[\"metadata\"][\"document_id\"],\n\t                    score=doc.score,\n\t                    text=doc_json[\"text\"],\n\t                    metadata=doc_json[\"metadata\"]\n", "                )\n\t                query_results.append(result)\n\t            # Add to overall results\n\t            results.append(QueryResult(query=query.query, results=query_results))\n\t        return results\n\t    async def _find_keys(self, pattern: str) -> List[str]:\n\t        return [key async for key in self.client.scan_iter(pattern)]\n\t    async def delete(\n\t        self,\n\t        ids: Optional[List[str]] = None,\n", "        filter: Optional[DocumentMetadataFilter] = None,\n\t        delete_all: Optional[bool] = None,\n\t    ) -> bool:\n\t        \"\"\"\n\t        Removes vectors by ids, filter, or everything in the datastore.\n\t        Returns whether the operation was successful.\n\t        \"\"\"\n\t        # Delete all vectors from the index if delete_all is True\n\t        if delete_all:\n\t            try:\n", "                logging.info(f\"Deleting all documents from index\")\n\t                await self.client.ft(REDIS_INDEX_NAME).dropindex(True)\n\t                logging.info(f\"Deleted all documents successfully\")\n\t                return True\n\t            except Exception as e:\n\t                logging.info(f\"Error deleting all documents: {e}\")\n\t                raise e\n\t        # Delete by filter\n\t        if filter:\n\t            # TODO - extend this to work with other metadata filters?\n", "            if filter.document_id:\n\t                try:\n\t                    keys = await self._find_keys(\n\t                        f\"{REDIS_DOC_PREFIX}:{filter.document_id}:*\"\n\t                    )\n\t                    await self._redis_delete(keys)\n\t                    logging.info(f\"Deleted document {filter.document_id} successfully\")\n\t                except Exception as e:\n\t                    logging.info(f\"Error deleting document {filter.document_id}: {e}\")\n\t                    raise e\n", "        # Delete by explicit ids (Redis keys)\n\t        if ids:\n\t            try:\n\t                logging.info(f\"Deleting document ids {ids}\")\n\t                keys = []\n\t                # find all keys associated with the document ids\n\t                for document_id in ids:\n\t                    doc_keys = await self._find_keys(\n\t                        pattern=f\"{REDIS_DOC_PREFIX}:{document_id}:*\"\n\t                    )\n", "                    keys.extend(doc_keys)\n\t                # delete all keys\n\t                logging.info(f\"Deleting {len(keys)} keys from Redis\")\n\t                await self._redis_delete(keys)\n\t            except Exception as e:\n\t                logging.info(f\"Error deleting ids: {e}\")\n\t                raise e\n\t        return True\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/providers/zilliz_datastore.py", "chunked_list": ["import os\n\tfrom typing import Optional\n\tfrom pymilvus import (\n\t    connections,\n\t)\n\tfrom uuid import uuid4\n\tfrom datastore.providers.milvus_datastore import (\n\t    MilvusDataStore,\n\t)\n\tZILLIZ_COLLECTION = os.environ.get(\"ZILLIZ_COLLECTION\") or \"c\" + uuid4().hex\n", "ZILLIZ_URI = os.environ.get(\"ZILLIZ_URI\")\n\tZILLIZ_USER = os.environ.get(\"ZILLIZ_USER\")\n\tZILLIZ_PASSWORD = os.environ.get(\"ZILLIZ_PASSWORD\")\n\tZILLIZ_USE_SECURITY = False if ZILLIZ_PASSWORD is None else True\n\tZILLIZ_CONSISTENCY_LEVEL = os.environ.get(\"ZILLIZ_CONSISTENCY_LEVEL\")\n\tclass ZillizDataStore(MilvusDataStore):\n\t    def __init__(self, create_new: Optional[bool] = False):\n\t        \"\"\"Create a Zilliz DataStore.\n\t        The Zilliz Datastore allows for storing your indexes and metadata within a Zilliz Cloud instance.\n\t        Args:\n", "            create_new (Optional[bool], optional): Whether to overwrite if collection already exists. Defaults to True.\n\t        \"\"\"\n\t        # Overwrite the default consistency level by MILVUS_CONSISTENCY_LEVEL\n\t        self._consistency_level = ZILLIZ_CONSISTENCY_LEVEL or \"Bounded\"\n\t        self._create_connection()\n\t        self._create_collection(ZILLIZ_COLLECTION, create_new)  # type: ignore\n\t        self._create_index()\n\t    def _create_connection(self):\n\t        # Check if the connection already exists\n\t        try:\n", "            i = [\n\t                connections.get_connection_addr(x[0])\n\t                for x in connections.list_connections()\n\t            ].index({\"address\": ZILLIZ_URI, \"user\": ZILLIZ_USER})\n\t            self.alias = connections.list_connections()[i][0]\n\t        except ValueError:\n\t            # Connect to the Zilliz instance using the passed in Environment variables\n\t            self.alias = uuid4().hex\n\t            connections.connect(alias=self.alias, uri=ZILLIZ_URI, user=ZILLIZ_USER, password=ZILLIZ_PASSWORD, secure=ZILLIZ_USE_SECURITY)  # type: ignore\n\t            self._print_info(\"Connect to zilliz cloud server\")\n", "    def _create_index(self):\n\t        try:\n\t            # If no index on the collection, create one\n\t            if len(self.col.indexes) == 0:\n\t                self.index_params = {\"metric_type\": \"IP\", \"index_type\": \"AUTOINDEX\", \"params\": {}}\n\t                self.col.create_index(\"embedding\", index_params=self.index_params)\n\t            self.col.load()\n\t            self.search_params = {\"metric_type\": \"IP\", \"params\": {}}\n\t        except Exception as e:\n\t            self._print_err(\"Failed to create index, error: {}\".format(e))\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/providers/llama_datastore.py", "chunked_list": ["import json\n\timport os\n\tfrom typing import Dict, List, Optional, Type\n\tfrom loguru import logger\n\tfrom datastore.datastore import DataStore\n\tfrom models.models import DocumentChunk, DocumentChunkMetadata, DocumentChunkWithScore, DocumentMetadataFilter, Query, QueryResult, QueryWithEmbedding\n\tfrom llama_index.indices.base import BaseGPTIndex\n\tfrom llama_index.indices.vector_store.base import GPTVectorStoreIndex\n\tfrom llama_index.indices.query.schema import QueryBundle\n\tfrom llama_index.response.schema import Response\n", "from llama_index.data_structs.node_v2 import Node, DocumentRelationship, NodeWithScore\n\tfrom llama_index.indices.registry import INDEX_STRUCT_TYPE_TO_INDEX_CLASS\n\tfrom llama_index.data_structs.struct_type import IndexStructType\n\tfrom llama_index.indices.response.builder import ResponseMode\n\tINDEX_STRUCT_TYPE_STR = os.environ.get('LLAMA_INDEX_TYPE', IndexStructType.SIMPLE_DICT.value)\n\tINDEX_JSON_PATH = os.environ.get('LLAMA_INDEX_JSON_PATH', None)\n\tQUERY_KWARGS_JSON_PATH = os.environ.get('LLAMA_QUERY_KWARGS_JSON_PATH', None)\n\tRESPONSE_MODE = os.environ.get('LLAMA_RESPONSE_MODE', ResponseMode.NO_TEXT.value)\n\tEXTERNAL_VECTOR_STORE_INDEX_STRUCT_TYPES = [\n\t    IndexStructType.DICT,\n", "    IndexStructType.WEAVIATE,\n\t    IndexStructType.PINECONE,\n\t    IndexStructType.QDRANT,\n\t    IndexStructType.CHROMA,\n\t    IndexStructType.VECTOR_STORE,\n\t]\n\tdef _create_or_load_index(\n\t    index_type_str: Optional[str] = None,\n\t    index_json_path: Optional[str] = None,\n\t    index_type_to_index_cls: Optional[dict[str, Type[BaseGPTIndex]]] = None,\n", ") -> BaseGPTIndex:\n\t    \"\"\"Create or load index from json path.\"\"\"\n\t    index_json_path = index_json_path or INDEX_JSON_PATH\n\t    index_type_to_index_cls = index_type_to_index_cls or INDEX_STRUCT_TYPE_TO_INDEX_CLASS\n\t    index_type_str = index_type_str or INDEX_STRUCT_TYPE_STR\n\t    index_type = IndexStructType(index_type_str)\n\t    if index_type not in index_type_to_index_cls:\n\t        raise ValueError(f'Unknown index type: {index_type}')\n\t    if index_type in EXTERNAL_VECTOR_STORE_INDEX_STRUCT_TYPES:\n\t        raise ValueError('Please use vector store directly.')\n", "    index_cls = index_type_to_index_cls[index_type]\n\t    if index_json_path is None:\n\t        return index_cls(nodes=[])  # Create empty index\n\t    else:\n\t        return index_cls.load_from_disk(index_json_path) # Load index from disk\n\tdef _create_or_load_query_kwargs(query_kwargs_json_path: Optional[str] = None) -> Optional[dict]:\n\t    \"\"\"Create or load query kwargs from json path.\"\"\"\n\t    query_kwargs_json_path= query_kwargs_json_path or QUERY_KWARGS_JSON_PATH\n\t    query_kargs: Optional[dict] = None\n\t    if  query_kwargs_json_path is not None:\n", "        with open(INDEX_JSON_PATH, 'r') as f:\n\t            query_kargs = json.load(f)\n\t    return query_kargs\n\tdef _doc_chunk_to_node(doc_chunk: DocumentChunk, source_doc_id: str) -> Node:\n\t    \"\"\"Convert document chunk to Node\"\"\"\n\t    return Node(\n\t        doc_id=doc_chunk.id,\n\t        text=doc_chunk.text,\n\t        embedding=doc_chunk.embedding,\n\t        extra_info=doc_chunk.metadata.dict(),\n", "        relationships={\n\t            DocumentRelationship.SOURCE: source_doc_id\n\t        }\n\t    )\n\tdef _query_with_embedding_to_query_bundle(query: QueryWithEmbedding) -> QueryBundle:\n\t    return QueryBundle(\n\t        query_str = query.query,\n\t        embedding=query.embedding,\n\t    )\n\tdef _source_node_to_doc_chunk_with_score(node_with_score: NodeWithScore) -> DocumentChunkWithScore:\n", "    node = node_with_score.node\n\t    if node.extra_info is not None:\n\t        metadata = DocumentChunkMetadata(**node.extra_info)\n\t    else:\n\t        metadata = DocumentChunkMetadata()\n\t    return DocumentChunkWithScore(\n\t        id=node.doc_id,\n\t        text=node.text,\n\t        score=node_with_score.score if node_with_score.score is not None else 1.,\n\t        metadata=metadata,\n", "    )\n\tdef _response_to_query_result(response: Response, query: QueryWithEmbedding) -> QueryResult:\n\t    results = [_source_node_to_doc_chunk_with_score(node) for node in response.source_nodes]\n\t    return QueryResult(query=query.query, results=results,)\n\tclass LlamaDataStore(DataStore):\n\t    def __init__(self, index: Optional[BaseGPTIndex] = None, query_kwargs: Optional[dict] = None):\n\t        self._index = index or _create_or_load_index()\n\t        self._query_kwargs = query_kwargs or _create_or_load_query_kwargs()\n\t    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n\t        \"\"\"\n", "        Takes in a list of list of document chunks and inserts them into the database.\n\t        Return a list of document ids.\n\t        \"\"\"\n\t        doc_ids = []\n\t        for doc_id, doc_chunks in chunks.items():\n\t            logger.debug(f\"Upserting {doc_id} with {len(doc_chunks)} chunks\")\n\t            nodes = [\n\t                _doc_chunk_to_node(doc_chunk=doc_chunk, source_doc_id=doc_id)\n\t                for doc_chunk in doc_chunks\n\t            ]\n", "            self._index.insert_nodes(nodes)\n\t            doc_ids.append(doc_id)\n\t        return doc_ids\n\t    async def _query(\n\t        self,\n\t        queries: List[QueryWithEmbedding],\n\t    ) -> List[QueryResult]:\n\t        \"\"\"\n\t        Takes in a list of queries with embeddings and filters and\n\t        returns a list of query results with matching document chunks and scores.\n", "        \"\"\"\n\t        query_result_all = []\n\t        for query in queries:\n\t            if query.filter is not None:\n\t                logger.warning('Filters are not supported yet, ignoring for now.')\n\t            query_bundle = _query_with_embedding_to_query_bundle(query)\n\t            # Setup query kwargs\n\t            if self._query_kwargs is not None:\n\t                query_kwargs = self._query_kwargs\n\t            else:\n", "                query_kwargs = {}\n\t            # TODO: support top_k for other indices\n\t            if isinstance(self._index, GPTVectorStoreIndex):\n\t                query_kwargs['similarity_top_k'] = query.top_k\n\t            response = await self._index.aquery(query_bundle, response_mode=RESPONSE_MODE, **query_kwargs)\n\t            query_result = _response_to_query_result(response, query)\n\t            query_result_all.append(query_result)\n\t        return query_result_all\n\t    async def delete(\n\t        self,\n", "        ids: Optional[List[str]] = None,\n\t        filter: Optional[DocumentMetadataFilter] = None,\n\t        delete_all: Optional[bool] = None,\n\t    ) -> bool:\n\t        \"\"\"\n\t        Removes vectors by ids, filter, or everything in the datastore.\n\t        Returns whether the operation was successful.\n\t        \"\"\"\n\t        if delete_all:\n\t            logger.warning('Delete all not supported yet.')\n", "            return False\n\t        if filter is not None:\n\t            logger.warning('Filters are not supported yet.')\n\t            return False\n\t        if ids is not None:\n\t            for id_ in ids:\n\t                try:\n\t                    self._index.delete(id_)\n\t                except NotImplementedError:\n\t                    # NOTE: some indices does not support delete yet.\n", "                    logger.warning(f'{type(self._index)} does not support delete yet.')\n\t                    return False\n\t        return True"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/providers/weaviate_datastore.py", "chunked_list": ["# TODO\n\timport asyncio\n\tfrom typing import Dict, List, Optional\n\tfrom loguru import logger\n\tfrom weaviate import Client\n\timport weaviate\n\timport os\n\timport uuid\n\tfrom weaviate.util import generate_uuid5\n\tfrom datastore.datastore import DataStore\n", "from models.models import (\n\t    DocumentChunk,\n\t    DocumentChunkMetadata,\n\t    DocumentMetadataFilter,\n\t    QueryResult,\n\t    QueryWithEmbedding,\n\t    DocumentChunkWithScore,\n\t    Source,\n\t)\n\tWEAVIATE_HOST = os.environ.get(\"WEAVIATE_HOST\", \"http://127.0.0.1\")\n", "WEAVIATE_PORT = os.environ.get(\"WEAVIATE_PORT\", \"8080\")\n\tWEAVIATE_USERNAME = os.environ.get(\"WEAVIATE_USERNAME\", None)\n\tWEAVIATE_PASSWORD = os.environ.get(\"WEAVIATE_PASSWORD\", None)\n\tWEAVIATE_SCOPES = os.environ.get(\"WEAVIATE_SCOPES\", \"offline_access\")\n\tWEAVIATE_CLASS = os.environ.get(\"WEAVIATE_CLASS\", \"OpenAIDocument\")\n\tWEAVIATE_BATCH_SIZE = int(os.environ.get(\"WEAVIATE_BATCH_SIZE\", 20))\n\tWEAVIATE_BATCH_DYNAMIC = os.environ.get(\"WEAVIATE_BATCH_DYNAMIC\", False)\n\tWEAVIATE_BATCH_TIMEOUT_RETRIES = int(os.environ.get(\"WEAVIATE_TIMEOUT_RETRIES\", 3))\n\tWEAVIATE_BATCH_NUM_WORKERS = int(os.environ.get(\"WEAVIATE_BATCH_NUM_WORKERS\", 1))\n\tSCHEMA = {\n", "    \"class\": WEAVIATE_CLASS,\n\t    \"description\": \"The main class\",\n\t    \"properties\": [\n\t        {\n\t            \"name\": \"chunk_id\",\n\t            \"dataType\": [\"string\"],\n\t            \"description\": \"The chunk id\",\n\t        },\n\t        {\n\t            \"name\": \"document_id\",\n", "            \"dataType\": [\"string\"],\n\t            \"description\": \"The document id\",\n\t        },\n\t        {\n\t            \"name\": \"text\",\n\t            \"dataType\": [\"text\"],\n\t            \"description\": \"The chunk's text\",\n\t        },\n\t        {\n\t            \"name\": \"source\",\n", "            \"dataType\": [\"string\"],\n\t            \"description\": \"The source of the data\",\n\t        },\n\t        {\n\t            \"name\": \"source_id\",\n\t            \"dataType\": [\"string\"],\n\t            \"description\": \"The source id\",\n\t        },\n\t        {\n\t            \"name\": \"url\",\n", "            \"dataType\": [\"string\"],\n\t            \"description\": \"The source url\",\n\t        },\n\t        {\n\t            \"name\": \"created_at\",\n\t            \"dataType\": [\"date\"],\n\t            \"description\": \"Creation date of document\",\n\t        },\n\t        {\n\t            \"name\": \"author\",\n", "            \"dataType\": [\"string\"],\n\t            \"description\": \"Document author\",\n\t        },\n\t    ],\n\t}\n\tdef extract_schema_properties(schema):\n\t    properties = schema[\"properties\"]\n\t    return {property[\"name\"] for property in properties}\n\tclass WeaviateDataStore(DataStore):\n\t    def handle_errors(self, results: Optional[List[dict]]) -> List[str]:\n", "        if not self or not results:\n\t            return []\n\t        error_messages = []\n\t        for result in results:\n\t            if (\n\t                \"result\" not in result\n\t                or \"errors\" not in result[\"result\"]\n\t                or \"error\" not in result[\"result\"][\"errors\"]\n\t            ):\n\t                continue\n", "            for message in result[\"result\"][\"errors\"][\"error\"]:\n\t                error_messages.append(message[\"message\"])\n\t                logger.exception(message[\"message\"])\n\t        return error_messages\n\t    def __init__(self):\n\t        auth_credentials = self._build_auth_credentials()\n\t        url = f\"{WEAVIATE_HOST}:{WEAVIATE_PORT}\"\n\t        logger.debug(\n\t            f\"Connecting to weaviate instance at {url} with credential type {type(auth_credentials).__name__}\"\n\t        )\n", "        self.client = Client(url, auth_client_secret=auth_credentials)\n\t        self.client.batch.configure(\n\t            batch_size=WEAVIATE_BATCH_SIZE,\n\t            dynamic=WEAVIATE_BATCH_DYNAMIC,  # type: ignore\n\t            callback=self.handle_errors,  # type: ignore\n\t            timeout_retries=WEAVIATE_BATCH_TIMEOUT_RETRIES,\n\t            num_workers=WEAVIATE_BATCH_NUM_WORKERS,\n\t        )\n\t        if self.client.schema.contains(SCHEMA):\n\t            current_schema = self.client.schema.get(WEAVIATE_CLASS)\n", "            current_schema_properties = extract_schema_properties(current_schema)\n\t            logger.debug(\n\t                f\"Found index {WEAVIATE_CLASS} with properties {current_schema_properties}\"\n\t            )\n\t            logger.debug(\"Will reuse this schema\")\n\t        else:\n\t            new_schema_properties = extract_schema_properties(SCHEMA)\n\t            logger.debug(\n\t                f\"Creating collection {WEAVIATE_CLASS} with properties {new_schema_properties}\"\n\t            )\n", "            self.client.schema.create_class(SCHEMA)\n\t    @staticmethod\n\t    def _build_auth_credentials():\n\t        if WEAVIATE_USERNAME and WEAVIATE_PASSWORD:\n\t            return weaviate.auth.AuthClientPassword(\n\t                WEAVIATE_USERNAME, WEAVIATE_PASSWORD, WEAVIATE_SCOPES\n\t            )\n\t        else:\n\t            return None\n\t    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n", "        \"\"\"\n\t        Takes in a list of list of document chunks and inserts them into the database.\n\t        Return a list of document ids.\n\t        \"\"\"\n\t        doc_ids = []\n\t        with self.client.batch as batch:\n\t            for doc_id, doc_chunks in chunks.items():\n\t                logger.debug(f\"Upserting {doc_id} with {len(doc_chunks)} chunks\")\n\t                for doc_chunk in doc_chunks:\n\t                    # we generate a uuid regardless of the format of the document_id because\n", "                    # weaviate needs a uuid to store each document chunk and\n\t                    # a document chunk cannot share the same uuid\n\t                    doc_uuid = generate_uuid5(doc_chunk, WEAVIATE_CLASS)\n\t                    metadata = doc_chunk.metadata\n\t                    doc_chunk_dict = doc_chunk.dict()\n\t                    doc_chunk_dict.pop(\"metadata\")\n\t                    for key, value in metadata.dict().items():\n\t                        doc_chunk_dict[key] = value\n\t                    doc_chunk_dict[\"chunk_id\"] = doc_chunk_dict.pop(\"id\")\n\t                    doc_chunk_dict[\"source\"] = (\n", "                        doc_chunk_dict.pop(\"source\").value\n\t                        if doc_chunk_dict[\"source\"]\n\t                        else None\n\t                    )\n\t                    embedding = doc_chunk_dict.pop(\"embedding\")\n\t                    batch.add_data_object(\n\t                        uuid=doc_uuid,\n\t                        data_object=doc_chunk_dict,\n\t                        class_name=WEAVIATE_CLASS,\n\t                        vector=embedding,\n", "                    )\n\t                doc_ids.append(doc_id)\n\t            batch.flush()\n\t        return doc_ids\n\t    async def _query(\n\t        self,\n\t        queries: List[QueryWithEmbedding],\n\t    ) -> List[QueryResult]:\n\t        \"\"\"\n\t        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n", "        \"\"\"\n\t        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n\t            logger.debug(f\"Query: {query.query}\")\n\t            if not hasattr(query, \"filter\") or not query.filter:\n\t                result = (\n\t                    self.client.query.get(\n\t                        WEAVIATE_CLASS,\n\t                        [\n\t                            \"chunk_id\",\n\t                            \"document_id\",\n", "                            \"text\",\n\t                            \"source\",\n\t                            \"source_id\",\n\t                            \"url\",\n\t                            \"created_at\",\n\t                            \"author\",\n\t                        ],\n\t                    )\n\t                    .with_hybrid(query=query.query, alpha=0.5, vector=query.embedding)\n\t                    .with_limit(query.top_k)  # type: ignore\n", "                    .with_additional([\"score\", \"vector\"])\n\t                    .do()\n\t                )\n\t            else:\n\t                filters_ = self.build_filters(query.filter)\n\t                result = (\n\t                    self.client.query.get(\n\t                        WEAVIATE_CLASS,\n\t                        [\n\t                            \"chunk_id\",\n", "                            \"document_id\",\n\t                            \"text\",\n\t                            \"source\",\n\t                            \"source_id\",\n\t                            \"url\",\n\t                            \"created_at\",\n\t                            \"author\",\n\t                        ],\n\t                    )\n\t                    .with_hybrid(query=query.query, alpha=0.5, vector=query.embedding)\n", "                    .with_where(filters_)\n\t                    .with_limit(query.top_k)  # type: ignore\n\t                    .with_additional([\"score\", \"vector\"])\n\t                    .do()\n\t                )\n\t            query_results: List[DocumentChunkWithScore] = []\n\t            response = result[\"data\"][\"Get\"][WEAVIATE_CLASS]\n\t            for resp in response:\n\t                result = DocumentChunkWithScore(\n\t                    id=resp[\"chunk_id\"],\n", "                    text=resp[\"text\"],\n\t                    embedding=resp[\"_additional\"][\"vector\"],\n\t                    score=resp[\"_additional\"][\"score\"],\n\t                    metadata=DocumentChunkMetadata(\n\t                        document_id=resp[\"document_id\"] if resp[\"document_id\"] else \"\",\n\t                        source=Source(resp[\"source\"]),\n\t                        source_id=resp[\"source_id\"],\n\t                        url=resp[\"url\"],\n\t                        created_at=resp[\"created_at\"],\n\t                        author=resp[\"author\"],\n", "                    ),\n\t                )\n\t                query_results.append(result)\n\t            return QueryResult(query=query.query, results=query_results)\n\t        return await asyncio.gather(*[_single_query(query) for query in queries])\n\t    async def delete(\n\t        self,\n\t        ids: Optional[List[str]] = None,\n\t        filter: Optional[DocumentMetadataFilter] = None,\n\t        delete_all: Optional[bool] = None,\n", "    ) -> bool:\n\t        # TODO\n\t        \"\"\"\n\t        Removes vectors by ids, filter, or everything in the datastore.\n\t        Returns whether the operation was successful.\n\t        \"\"\"\n\t        if delete_all:\n\t            logger.debug(f\"Deleting all vectors in index {WEAVIATE_CLASS}\")\n\t            self.client.schema.delete_all()\n\t            return True\n", "        if ids:\n\t            operands = [\n\t                {\"path\": [\"document_id\"], \"operator\": \"Equal\", \"valueString\": id}\n\t                for id in ids\n\t            ]\n\t            where_clause = {\"operator\": \"Or\", \"operands\": operands}\n\t            logger.debug(f\"Deleting vectors from index {WEAVIATE_CLASS} with ids {ids}\")\n\t            result = self.client.batch.delete_objects(\n\t                class_name=WEAVIATE_CLASS, where=where_clause, output=\"verbose\"\n\t            )\n", "            if not bool(result[\"results\"][\"successful\"]):\n\t                logger.debug(\n\t                    f\"Failed to delete the following objects: {result['results']['objects']}\"\n\t                )\n\t        if filter:\n\t            where_clause = self.build_filters(filter)\n\t            logger.debug(\n\t                f\"Deleting vectors from index {WEAVIATE_CLASS} with filter {where_clause}\"\n\t            )\n\t            result = self.client.batch.delete_objects(\n", "                class_name=WEAVIATE_CLASS, where=where_clause\n\t            )\n\t            if not bool(result[\"results\"][\"successful\"]):\n\t                logger.debug(\n\t                    f\"Failed to delete the following objects: {result['results']['objects']}\"\n\t                )\n\t        return True\n\t    @staticmethod\n\t    def build_filters(filter):\n\t        if filter.source:\n", "            filter.source = filter.source.value\n\t        operands = []\n\t        filter_conditions = {\n\t            \"source\": {\n\t                \"operator\": \"Equal\",\n\t                \"value\": \"query.filter.source.value\",\n\t                \"value_key\": \"valueString\",\n\t            },\n\t            \"start_date\": {\"operator\": \"GreaterThanEqual\", \"value_key\": \"valueDate\"},\n\t            \"end_date\": {\"operator\": \"LessThanEqual\", \"value_key\": \"valueDate\"},\n", "            \"default\": {\"operator\": \"Equal\", \"value_key\": \"valueString\"},\n\t        }\n\t        for attr, value in filter.__dict__.items():\n\t            if value is not None:\n\t                filter_condition = filter_conditions.get(\n\t                    attr, filter_conditions[\"default\"]\n\t                )\n\t                value_key = filter_condition[\"value_key\"]\n\t                operand = {\n\t                    \"path\": [\n", "                        attr\n\t                        if not (attr == \"start_date\" or attr == \"end_date\")\n\t                        else \"created_at\"\n\t                    ],\n\t                    \"operator\": filter_condition[\"operator\"],\n\t                    value_key: value,\n\t                }\n\t                operands.append(operand)\n\t        return {\"operator\": \"And\", \"operands\": operands}\n\t    @staticmethod\n", "    def _is_valid_weaviate_id(candidate_id: str) -> bool:\n\t        \"\"\"\n\t        Check if candidate_id is a valid UUID for weaviate's use\n\t        Weaviate supports UUIDs of version 3, 4 and 5. This function checks if the candidate_id is a valid UUID of one of these versions.\n\t        See https://weaviate.io/developers/weaviate/more-resources/faq#q-are-there-restrictions-on-uuid-formatting-do-i-have-to-adhere-to-any-standards\n\t        for more information.\n\t        \"\"\"\n\t        acceptable_version = [3, 4, 5]\n\t        try:\n\t            result = uuid.UUID(candidate_id)\n", "            if result.version not in acceptable_version:\n\t                return False\n\t            else:\n\t                return True\n\t        except ValueError:\n\t            return False\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/providers/milvus_datastore.py", "chunked_list": ["import json\n\timport os\n\timport asyncio\n\tfrom typing import Dict, List, Optional\n\tfrom pymilvus import (\n\t    Collection,\n\t    connections,\n\t    utility,\n\t    FieldSchema,\n\t    DataType,\n", "    CollectionSchema,\n\t    MilvusException,\n\t)\n\tfrom uuid import uuid4\n\tfrom services.date import to_unix_timestamp\n\tfrom datastore.datastore import DataStore\n\tfrom models.models import (\n\t    DocumentChunk,\n\t    DocumentChunkMetadata,\n\t    Source,\n", "    DocumentMetadataFilter,\n\t    QueryResult,\n\t    QueryWithEmbedding,\n\t    DocumentChunkWithScore,\n\t)\n\tMILVUS_COLLECTION = os.environ.get(\"MILVUS_COLLECTION\") or \"c\" + uuid4().hex\n\tMILVUS_HOST = os.environ.get(\"MILVUS_HOST\") or \"localhost\"\n\tMILVUS_PORT = os.environ.get(\"MILVUS_PORT\") or 19530\n\tMILVUS_USER = os.environ.get(\"MILVUS_USER\")\n\tMILVUS_PASSWORD = os.environ.get(\"MILVUS_PASSWORD\")\n", "MILVUS_USE_SECURITY = False if MILVUS_PASSWORD is None else True\n\tMILVUS_INDEX_PARAMS = os.environ.get(\"MILVUS_INDEX_PARAMS\")\n\tMILVUS_SEARCH_PARAMS = os.environ.get(\"MILVUS_SEARCH_PARAMS\")\n\tMILVUS_CONSISTENCY_LEVEL = os.environ.get(\"MILVUS_CONSISTENCY_LEVEL\")\n\tUPSERT_BATCH_SIZE = 100\n\tOUTPUT_DIM = 1536\n\tEMBEDDING_FIELD = \"embedding\"\n\tclass Required:\n\t    pass\n\t# The fields names that we are going to be storing within Milvus, the field declaration for schema creation, and the default value\n", "SCHEMA_V1 = [\n\t    (\n\t        \"pk\",\n\t        FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n\t        Required,\n\t    ),\n\t    (\n\t        EMBEDDING_FIELD,\n\t        FieldSchema(name=EMBEDDING_FIELD, dtype=DataType.FLOAT_VECTOR, dim=OUTPUT_DIM),\n\t        Required,\n", "    ),\n\t    (\n\t        \"text\",\n\t        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n\t        Required,\n\t    ),\n\t    (\n\t        \"document_id\",\n\t        FieldSchema(name=\"document_id\", dtype=DataType.VARCHAR, max_length=65535),\n\t        \"\",\n", "    ),\n\t    (\n\t        \"source_id\",\n\t        FieldSchema(name=\"source_id\", dtype=DataType.VARCHAR, max_length=65535),\n\t        \"\",\n\t    ),\n\t    (\n\t        \"id\",\n\t        FieldSchema(\n\t            name=\"id\",\n", "            dtype=DataType.VARCHAR,\n\t            max_length=65535,\n\t        ),\n\t        \"\",\n\t    ),\n\t    (\n\t        \"source\",\n\t        FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=65535),\n\t        \"\",\n\t    ),\n", "    (\"url\", FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=65535), \"\"),\n\t    (\"created_at\", FieldSchema(name=\"created_at\", dtype=DataType.INT64), -1),\n\t    (\n\t        \"author\",\n\t        FieldSchema(name=\"author\", dtype=DataType.VARCHAR, max_length=65535),\n\t        \"\",\n\t    ),\n\t]\n\t# V2 schema, remomve the \"pk\" field\n\tSCHEMA_V2 = SCHEMA_V1[1:]\n", "SCHEMA_V2[4][1].is_primary = True\n\tclass MilvusDataStore(DataStore):\n\t    def __init__(\n\t        self,\n\t        create_new: Optional[bool] = False,\n\t        consistency_level: str = \"Bounded\",\n\t    ):\n\t        \"\"\"Create a Milvus DataStore.\n\t        The Milvus Datastore allows for storing your indexes and metadata within a Milvus instance.\n\t        Args:\n", "            create_new (Optional[bool], optional): Whether to overwrite if collection already exists. Defaults to True.\n\t            consistency_level(str, optional): Specify the collection consistency level.\n\t                                                Defaults to \"Bounded\" for search performance.\n\t                                                Set to \"Strong\" in test cases for result validation.\n\t        \"\"\"\n\t        # Overwrite the default consistency level by MILVUS_CONSISTENCY_LEVEL\n\t        self._consistency_level = MILVUS_CONSISTENCY_LEVEL or consistency_level\n\t        self._create_connection()\n\t        self._create_collection(MILVUS_COLLECTION, create_new)  # type: ignore\n\t        self._create_index()\n", "    def _print_info(self, msg):\n\t        # TODO: logger\n\t        print(msg)\n\t    def _print_err(self, msg):\n\t        # TODO: logger\n\t        print(msg)\n\t    def _get_schema(self):\n\t        return SCHEMA_V1 if self._schema_ver == \"V1\" else SCHEMA_V2\n\t    def _create_connection(self):\n\t        try:\n", "            self.alias = \"\"\n\t            # Check if the connection already exists\n\t            for x in connections.list_connections():\n\t                addr = connections.get_connection_addr(x[0])\n\t                if x[1] and ('address' in addr) and (addr['address'] == \"{}:{}\".format(MILVUS_HOST, MILVUS_PORT)):\n\t                    self.alias = x[0]\n\t                    self._print_info(\"Reuse connection to Milvus server '{}:{}' with alias '{:s}'\"\n\t                                     .format(MILVUS_HOST, MILVUS_PORT, self.alias))\n\t                    break\n\t            # Connect to the Milvus instance using the passed in Environment variables\n", "            if len(self.alias) == 0:\n\t                self.alias = uuid4().hex\n\t                connections.connect(\n\t                    alias=self.alias,\n\t                    host=MILVUS_HOST,\n\t                    port=MILVUS_PORT,\n\t                    user=MILVUS_USER,  # type: ignore\n\t                    password=MILVUS_PASSWORD,  # type: ignore\n\t                    secure=MILVUS_USE_SECURITY,\n\t                )\n", "                self._print_info(\"Create connection to Milvus server '{}:{}' with alias '{:s}'\"\n\t                                 .format(MILVUS_HOST, MILVUS_PORT, self.alias))\n\t        except Exception as e:\n\t            self._print_err(\"Failed to create connection to Milvus server '{}:{}', error: {}\"\n\t                            .format(MILVUS_HOST, MILVUS_PORT, e))\n\t    def _create_collection(self, collection_name, create_new: bool) -> None:\n\t        \"\"\"Create a collection based on environment and passed in variables.\n\t        Args:\n\t            create_new (bool): Whether to overwrite if collection already exists.\n\t        \"\"\"\n", "        try:\n\t            self._schema_ver = \"V1\"\n\t            # If the collection exists and create_new is True, drop the existing collection\n\t            if utility.has_collection(collection_name, using=self.alias) and create_new:\n\t                utility.drop_collection(collection_name, using=self.alias)\n\t            # Check if the collection doesnt exist\n\t            if utility.has_collection(collection_name, using=self.alias) is False:\n\t                # If it doesnt exist use the field params from init to create a new schem\n\t                schema = [field[1] for field in SCHEMA_V2]\n\t                schema = CollectionSchema(schema)\n", "                # Use the schema to create a new collection\n\t                self.col = Collection(\n\t                    collection_name,\n\t                    schema=schema,\n\t                    using=self.alias,\n\t                    consistency_level=self._consistency_level,\n\t                )\n\t                self._schema_ver = \"V2\"\n\t                self._print_info(\"Create Milvus collection '{}' with schema {} and consistency level {}\"\n\t                                 .format(collection_name, self._schema_ver, self._consistency_level))\n", "            else:\n\t                # If the collection exists, point to it\n\t                self.col = Collection(\n\t                    collection_name, using=self.alias\n\t                )  # type: ignore\n\t                # Which sechma is used\n\t                for field in self.col.schema.fields:\n\t                    if field.name == \"id\" and field.is_primary:\n\t                        self._schema_ver = \"V2\"\n\t                        break\n", "                self._print_info(\"Milvus collection '{}' already exists with schema {}\"\n\t                                 .format(collection_name, self._schema_ver))\n\t        except Exception as e:\n\t            self._print_err(\"Failed to create collection '{}', error: {}\".format(collection_name, e))\n\t    def _create_index(self):\n\t        # TODO: verify index/search params passed by os.environ\n\t        self.index_params = MILVUS_INDEX_PARAMS or None\n\t        self.search_params = MILVUS_SEARCH_PARAMS or None\n\t        try:\n\t            # If no index on the collection, create one\n", "            if len(self.col.indexes) == 0:\n\t                if self.index_params is not None:\n\t                    # Convert the string format to JSON format parameters passed by MILVUS_INDEX_PARAMS\n\t                    self.index_params = json.loads(self.index_params)\n\t                    self._print_info(\"Create Milvus index: {}\".format(self.index_params))\n\t                    # Create an index on the 'embedding' field with the index params found in init\n\t                    self.col.create_index(EMBEDDING_FIELD, index_params=self.index_params)\n\t                else:\n\t                    # If no index param supplied, to first create an HNSW index for Milvus\n\t                    try:\n", "                        i_p = {\n\t                            \"metric_type\": \"IP\",\n\t                            \"index_type\": \"HNSW\",\n\t                            \"params\": {\"M\": 8, \"efConstruction\": 64},\n\t                        }\n\t                        self._print_info(\"Attempting creation of Milvus '{}' index\".format(i_p[\"index_type\"]))\n\t                        self.col.create_index(EMBEDDING_FIELD, index_params=i_p)\n\t                        self.index_params = i_p\n\t                        self._print_info(\"Creation of Milvus '{}' index successful\".format(i_p[\"index_type\"]))\n\t                    # If create fails, most likely due to being Zilliz Cloud instance, try to create an AutoIndex\n", "                    except MilvusException:\n\t                        self._print_info(\"Attempting creation of Milvus default index\")\n\t                        i_p = {\"metric_type\": \"IP\", \"index_type\": \"AUTOINDEX\", \"params\": {}}\n\t                        self.col.create_index(EMBEDDING_FIELD, index_params=i_p)\n\t                        self.index_params = i_p\n\t                        self._print_info(\"Creation of Milvus default index successful\")\n\t            # If an index already exists, grab its params\n\t            else:\n\t                # How about if the first index is not vector index?\n\t                for index in self.col.indexes:\n", "                    idx = index.to_dict()\n\t                    if idx[\"field\"] == EMBEDDING_FIELD:\n\t                        self._print_info(\"Index already exists: {}\".format(idx))\n\t                        self.index_params = idx['index_param']\n\t                        break\n\t            self.col.load()\n\t            if self.search_params is not None:\n\t                # Convert the string format to JSON format parameters passed by MILVUS_SEARCH_PARAMS\n\t                self.search_params = json.loads(self.search_params)\n\t            else:\n", "                # The default search params\n\t                metric_type = \"IP\"\n\t                if \"metric_type\" in self.index_params:\n\t                    metric_type = self.index_params[\"metric_type\"]\n\t                default_search_params = {\n\t                    \"IVF_FLAT\": {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n\t                    \"IVF_SQ8\": {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n\t                    \"IVF_PQ\": {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10}},\n\t                    \"HNSW\": {\"metric_type\": metric_type, \"params\": {\"ef\": 10}},\n\t                    \"RHNSW_FLAT\": {\"metric_type\": metric_type, \"params\": {\"ef\": 10}},\n", "                    \"RHNSW_SQ\": {\"metric_type\": metric_type, \"params\": {\"ef\": 10}},\n\t                    \"RHNSW_PQ\": {\"metric_type\": metric_type, \"params\": {\"ef\": 10}},\n\t                    \"IVF_HNSW\": {\"metric_type\": metric_type, \"params\": {\"nprobe\": 10, \"ef\": 10}},\n\t                    \"ANNOY\": {\"metric_type\": metric_type, \"params\": {\"search_k\": 10}},\n\t                    \"AUTOINDEX\": {\"metric_type\": metric_type, \"params\": {}},\n\t                }\n\t                # Set the search params\n\t                self.search_params = default_search_params[self.index_params[\"index_type\"]]\n\t            self._print_info(\"Milvus search parameters: {}\".format(self.search_params))\n\t        except Exception as e:\n", "            self._print_err(\"Failed to create index, error: {}\".format(e))\n\t    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n\t        \"\"\"Upsert chunks into the datastore.\n\t        Args:\n\t            chunks (Dict[str, List[DocumentChunk]]): A list of DocumentChunks to insert\n\t        Raises:\n\t            e: Error in upserting data.\n\t        Returns:\n\t            List[str]: The document_id's that were inserted.\n\t        \"\"\"\n", "        try:\n\t            # The doc id's to return for the upsert\n\t            doc_ids: List[str] = []\n\t            # List to collect all the insert data, skip the \"pk\" for schema V1\n\t            offset = 1 if self._schema_ver == \"V1\" else 0\n\t            insert_data = [[] for _ in range(len(self._get_schema()) - offset)]\n\t            # Go through each document chunklist and grab the data\n\t            for doc_id, chunk_list in chunks.items():\n\t                # Append the doc_id to the list we are returning\n\t                doc_ids.append(doc_id)\n", "                # Examine each chunk in the chunklist\n\t                for chunk in chunk_list:\n\t                    # Extract data from the chunk\n\t                    list_of_data = self._get_values(chunk)\n\t                    # Check if the data is valid\n\t                    if list_of_data is not None:\n\t                        # Append each field to the insert_data\n\t                        for x in range(len(insert_data)):\n\t                            insert_data[x].append(list_of_data[x])\n\t            # Slice up our insert data into batches\n", "            batches = [\n\t                insert_data[i : i + UPSERT_BATCH_SIZE]\n\t                for i in range(0, len(insert_data), UPSERT_BATCH_SIZE)\n\t            ]\n\t            # Attempt to insert each batch into our collection\n\t            # batch data can work with both V1 and V2 schema\n\t            for batch in batches:\n\t                if len(batch[0]) != 0:\n\t                    try:\n\t                        self._print_info(f\"Upserting batch of size {len(batch[0])}\")\n", "                        self.col.insert(batch)\n\t                        self._print_info(f\"Upserted batch successfully\")\n\t                    except Exception as e:\n\t                        self._print_err(f\"Failed to insert batch records, error: {e}\")\n\t                        raise e\n\t            # This setting perfoms flushes after insert. Small insert == bad to use\n\t            # self.col.flush()\n\t            return doc_ids\n\t        except Exception as e:\n\t            self._print_err(\"Failed to insert records, error: {}\".format(e))\n", "            return []\n\t    def _get_values(self, chunk: DocumentChunk) -> List[any] | None:  # type: ignore\n\t        \"\"\"Convert the chunk into a list of values to insert whose indexes align with fields.\n\t        Args:\n\t            chunk (DocumentChunk): The chunk to convert.\n\t        Returns:\n\t            List (any): The values to insert.\n\t        \"\"\"\n\t        # Convert DocumentChunk and its sub models to dict\n\t        values = chunk.dict()\n", "        # Unpack the metadata into the same dict\n\t        meta = values.pop(\"metadata\")\n\t        values.update(meta)\n\t        # Convert date to int timestamp form\n\t        if values[\"created_at\"]:\n\t            values[\"created_at\"] = to_unix_timestamp(values[\"created_at\"])\n\t        # If source exists, change from Source object to the string value it holds\n\t        if values[\"source\"]:\n\t            values[\"source\"] = values[\"source\"].value\n\t        # List to collect data we will return\n", "        ret = []\n\t        # Grab data responding to each field, excluding the hidden auto pk field for schema V1\n\t        offset = 1 if self._schema_ver == \"V1\" else 0\n\t        for key, _, default in self._get_schema()[offset:]:\n\t            # Grab the data at the key and default to our defaults set in init\n\t            x = values.get(key) or default\n\t            # If one of our required fields is missing, ignore the entire entry\n\t            if x is Required:\n\t                self._print_info(\"Chunk \" + values[\"id\"] + \" missing \" + key + \" skipping\")\n\t                return None\n", "            # Add the corresponding value if it passes the tests\n\t            ret.append(x)\n\t        return ret\n\t    async def _query(\n\t        self,\n\t        queries: List[QueryWithEmbedding],\n\t    ) -> List[QueryResult]:\n\t        \"\"\"Query the QueryWithEmbedding against the MilvusDocumentSearch\n\t        Search the embedding and its filter in the collection.\n\t        Args:\n", "            queries (List[QueryWithEmbedding]): The list of searches to perform.\n\t        Returns:\n\t            List[QueryResult]: Results for each search.\n\t        \"\"\"\n\t        # Async to perform the query, adapted from pinecone implementation\n\t        async def _single_query(query: QueryWithEmbedding) -> QueryResult:\n\t            try:\n\t                filter = None\n\t                # Set the filter to expression that is valid for Milvus\n\t                if query.filter is not None:\n", "                    # Either a valid filter or None will be returned\n\t                    filter = self._get_filter(query.filter)\n\t                # Perform our search\n\t                return_from = 2 if self._schema_ver == \"V1\" else 1\n\t                res = self.col.search(\n\t                    data=[query.embedding],\n\t                    anns_field=EMBEDDING_FIELD,\n\t                    param=self.search_params,\n\t                    limit=query.top_k,\n\t                    expr=filter,\n", "                    output_fields=[\n\t                        field[0] for field in self._get_schema()[return_from:]\n\t                    ],  # Ignoring pk, embedding\n\t                )\n\t                # Results that will hold our DocumentChunkWithScores\n\t                results = []\n\t                # Parse every result for our search\n\t                for hit in res[0]:  # type: ignore\n\t                    # The distance score for the search result, falls under DocumentChunkWithScore\n\t                    score = hit.score\n", "                    # Our metadata info, falls under DocumentChunkMetadata\n\t                    metadata = {}\n\t                    # Grab the values that correspond to our fields, ignore pk and embedding.\n\t                    for x in [field[0] for field in self._get_schema()[return_from:]]:\n\t                        metadata[x] = hit.entity.get(x)\n\t                    # If the source isn't valid, convert to None\n\t                    if metadata[\"source\"] not in Source.__members__:\n\t                        metadata[\"source\"] = None\n\t                    # Text falls under the DocumentChunk\n\t                    text = metadata.pop(\"text\")\n", "                    # Id falls under the DocumentChunk\n\t                    ids = metadata.pop(\"id\")\n\t                    chunk = DocumentChunkWithScore(\n\t                        id=ids,\n\t                        score=score,\n\t                        text=text,\n\t                        metadata=DocumentChunkMetadata(**metadata),\n\t                    )\n\t                    results.append(chunk)\n\t                # TODO: decide on doing queries to grab the embedding itself, slows down performance as double query occurs\n", "                return QueryResult(query=query.query, results=results)\n\t            except Exception as e:\n\t                self._print_err(\"Failed to query, error: {}\".format(e))\n\t                return QueryResult(query=query.query, results=[])\n\t        results: List[QueryResult] = await asyncio.gather(\n\t            *[_single_query(query) for query in queries]\n\t        )\n\t        return results\n\t    async def delete(\n\t        self,\n", "        ids: Optional[List[str]] = None,\n\t        filter: Optional[DocumentMetadataFilter] = None,\n\t        delete_all: Optional[bool] = None,\n\t    ) -> bool:\n\t        \"\"\"Delete the entities based either on the chunk_id of the vector,\n\t        Args:\n\t            ids (Optional[List[str]], optional): The document_ids to delete. Defaults to None.\n\t            filter (Optional[DocumentMetadataFilter], optional): The filter to delete by. Defaults to None.\n\t            delete_all (Optional[bool], optional): Whether to drop the collection and recreate it. Defaults to None.\n\t        \"\"\"\n", "        # If deleting all, drop and create the new collection\n\t        if delete_all:\n\t            coll_name = self.col.name\n\t            self._print_info(\"Delete the entire collection {} and create new one\".format(coll_name))\n\t            # Release the collection from memory\n\t            self.col.release()\n\t            # Drop the collection\n\t            self.col.drop()\n\t            # Recreate the new collection\n\t            self._create_collection(coll_name, True)\n", "            self._create_index()\n\t            return True\n\t        # Keep track of how many we have deleted for later printing\n\t        delete_count = 0\n\t        batch_size = 100\n\t        pk_name = \"pk\" if self._schema_ver == \"V1\" else \"id\"\n\t        try:\n\t            # According to the api design, the ids is a list of document_id,\n\t            # document_id is not primary key, use query+delete to workaround,\n\t            # in future version we can delete by expression\n", "            if (ids is not None) and len(ids) > 0:\n\t                # Add quotation marks around the string format id\n\t                ids = ['\"' + str(id) + '\"' for id in ids]\n\t                # Query for the pk's of entries that match id's\n\t                ids = self.col.query(f\"document_id in [{','.join(ids)}]\")\n\t                # Convert to list of pks\n\t                pks = [str(entry[pk_name]) for entry in ids]  # type: ignore\n\t                # for schema V2, the \"id\" is varchar, rewrite the expression\n\t                if self._schema_ver != \"V1\":\n\t                    pks = ['\"' + pk + '\"' for pk in pks]\n", "                # Delete by ids batch by batch(avoid too long expression)\n\t                self._print_info(\"Apply {:d} deletions to schema {:s}\".format(len(pks), self._schema_ver))\n\t                while len(pks) > 0:\n\t                    batch_pks = pks[:batch_size]\n\t                    pks = pks[batch_size:]\n\t                    # Delete the entries batch by batch\n\t                    res = self.col.delete(f\"{pk_name} in [{','.join(batch_pks)}]\")\n\t                    # Increment our deleted count\n\t                    delete_count += int(res.delete_count)  # type: ignore\n\t        except Exception as e:\n", "            self._print_err(\"Failed to delete by ids, error: {}\".format(e))\n\t        try:\n\t            # Check if empty filter\n\t            if filter is not None:\n\t                # Convert filter to milvus expression\n\t                filter = self._get_filter(filter)  # type: ignore\n\t                # Check if there is anything to filter\n\t                if len(filter) != 0:  # type: ignore\n\t                    # Query for the pk's of entries that match filter\n\t                    res = self.col.query(filter)  # type: ignore\n", "                    # Convert to list of pks\n\t                    pks = [str(entry[pk_name]) for entry in res]  # type: ignore\n\t                    # for schema V2, the \"id\" is varchar, rewrite the expression\n\t                    if self._schema_ver != \"V1\":\n\t                        pks = ['\"' + pk + '\"' for pk in pks]\n\t                    # Check to see if there are valid pk's to delete, delete batch by batch(avoid too long expression)\n\t                    while len(pks) > 0:  # type: ignore\n\t                        batch_pks = pks[:batch_size]\n\t                        pks = pks[batch_size:]\n\t                        # Delete the entries batch by batch\n", "                        res = self.col.delete(f\"{pk_name} in [{','.join(batch_pks)}]\")  # type: ignore\n\t                        # Increment our delete count\n\t                        delete_count += int(res.delete_count)  # type: ignore\n\t        except Exception as e:\n\t            self._print_err(\"Failed to delete by filter, error: {}\".format(e))\n\t        self._print_info(\"{:d} records deleted\".format(delete_count))\n\t        # This setting performs flushes after delete. Small delete == bad to use\n\t        # self.col.flush()\n\t        return True\n\t    def _get_filter(self, filter: DocumentMetadataFilter) -> Optional[str]:\n", "        \"\"\"Converts a DocumentMetdataFilter to the expression that Milvus takes.\n\t        Args:\n\t            filter (DocumentMetadataFilter): The Filter to convert to Milvus expression.\n\t        Returns:\n\t            Optional[str]: The filter if valid, otherwise None.\n\t        \"\"\"\n\t        filters = []\n\t        # Go through all the fields and their values\n\t        for field, value in filter.dict().items():\n\t            # Check if the Value is empty\n", "            if value is not None:\n\t                # Convert start_date to int and add greater than or equal logic\n\t                if field == \"start_date\":\n\t                    filters.append(\n\t                        \"(created_at >= \" + str(to_unix_timestamp(value)) + \")\"\n\t                    )\n\t                # Convert end_date to int and add less than or equal logic\n\t                elif field == \"end_date\":\n\t                    filters.append(\n\t                        \"(created_at <= \" + str(to_unix_timestamp(value)) + \")\"\n", "                    )\n\t                # Convert Source to its string value and check equivalency\n\t                elif field == \"source\":\n\t                    filters.append(\"(\" + field + ' == \"' + str(value.value) + '\")')\n\t                # Check equivalency of rest of string fields\n\t                else:\n\t                    filters.append(\"(\" + field + ' == \"' + str(value) + '\")')\n\t        # Join all our expressions with `and``\n\t        return \" and \".join(filters)\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/datastore/providers/qdrant_datastore.py", "chunked_list": ["import os\n\timport uuid\n\tfrom typing import Dict, List, Optional\n\tfrom grpc._channel import _InactiveRpcError\n\tfrom qdrant_client.http.exceptions import UnexpectedResponse\n\tfrom qdrant_client.http.models import PayloadSchemaType\n\tfrom datastore.datastore import DataStore\n\tfrom models.models import (\n\t    DocumentChunk,\n\t    DocumentMetadataFilter,\n", "    QueryResult,\n\t    QueryWithEmbedding,\n\t    DocumentChunkWithScore,\n\t)\n\tfrom qdrant_client.http import models as rest\n\timport qdrant_client\n\tfrom services.date import to_unix_timestamp\n\tQDRANT_URL = os.environ.get(\"QDRANT_URL\", \"http://localhost\")\n\tQDRANT_PORT = os.environ.get(\"QDRANT_PORT\", \"6333\")\n\tQDRANT_GRPC_PORT = os.environ.get(\"QDRANT_GRPC_PORT\", \"6334\")\n", "QDRANT_API_KEY = os.environ.get(\"QDRANT_API_KEY\")\n\tQDRANT_COLLECTION = os.environ.get(\"QDRANT_COLLECTION\", \"document_chunks\")\n\tclass QdrantDataStore(DataStore):\n\t    UUID_NAMESPACE = uuid.UUID(\"3896d314-1e95-4a3a-b45a-945f9f0b541d\")\n\t    def __init__(\n\t        self,\n\t        collection_name: Optional[str] = None,\n\t        vector_size: int = 1536,\n\t        distance: str = \"Cosine\",\n\t        recreate_collection: bool = False,\n", "    ):\n\t        \"\"\"\n\t        Args:\n\t            collection_name: Name of the collection to be used\n\t            vector_size: Size of the embedding stored in a collection\n\t            distance:\n\t                Any of \"Cosine\" / \"Euclid\" / \"Dot\". Distance function to measure\n\t                similarity\n\t        \"\"\"\n\t        self.client = qdrant_client.QdrantClient(\n", "            url=QDRANT_URL,\n\t            port=int(QDRANT_PORT),\n\t            grpc_port=int(QDRANT_GRPC_PORT),\n\t            api_key=QDRANT_API_KEY,\n\t            prefer_grpc=True,\n\t            timeout=10,\n\t        )\n\t        self.collection_name = collection_name or QDRANT_COLLECTION\n\t        # Set up the collection so the points might be inserted or queried\n\t        self._set_up_collection(vector_size, distance, recreate_collection)\n", "    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:\n\t        \"\"\"\n\t        Takes in a list of document chunks and inserts them into the database.\n\t        Return a list of document ids.\n\t        \"\"\"\n\t        points = [\n\t            self._convert_document_chunk_to_point(chunk)\n\t            for _, chunks in chunks.items()\n\t            for chunk in chunks\n\t        ]\n", "        self.client.upsert(\n\t            collection_name=self.collection_name,\n\t            points=points,  # type: ignore\n\t            wait=True,\n\t        )\n\t        return list(chunks.keys())\n\t    async def _query(\n\t        self,\n\t        queries: List[QueryWithEmbedding],\n\t    ) -> List[QueryResult]:\n", "        \"\"\"\n\t        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.\n\t        \"\"\"\n\t        search_requests = [\n\t            self._convert_query_to_search_request(query) for query in queries\n\t        ]\n\t        results = self.client.search_batch(\n\t            collection_name=self.collection_name,\n\t            requests=search_requests,\n\t        )\n", "        return [\n\t            QueryResult(\n\t                query=query.query,\n\t                results=[\n\t                    self._convert_scored_point_to_document_chunk_with_score(point)\n\t                    for point in result\n\t                ],\n\t            )\n\t            for query, result in zip(queries, results)\n\t        ]\n", "    async def delete(\n\t        self,\n\t        ids: Optional[List[str]] = None,\n\t        filter: Optional[DocumentMetadataFilter] = None,\n\t        delete_all: Optional[bool] = None,\n\t    ) -> bool:\n\t        \"\"\"\n\t        Removes vectors by ids, filter, or everything in the datastore.\n\t        Returns whether the operation was successful.\n\t        \"\"\"\n", "        if ids is None and filter is None and delete_all is None:\n\t            raise ValueError(\n\t                \"Please provide one of the parameters: ids, filter or delete_all.\"\n\t            )\n\t        if delete_all:\n\t            points_selector = rest.Filter()\n\t        else:\n\t            points_selector = self._convert_metadata_filter_to_qdrant_filter(\n\t                filter, ids\n\t            )\n", "        response = self.client.delete(\n\t            collection_name=self.collection_name,\n\t            points_selector=points_selector,  # type: ignore\n\t        )\n\t        return \"COMPLETED\" == response.status\n\t    def _convert_document_chunk_to_point(\n\t        self, document_chunk: DocumentChunk\n\t    ) -> rest.PointStruct:\n\t        created_at = (\n\t            to_unix_timestamp(document_chunk.metadata.created_at)\n", "            if document_chunk.metadata.created_at is not None\n\t            else None\n\t        )\n\t        return rest.PointStruct(\n\t            id=self._create_document_chunk_id(document_chunk.id),\n\t            vector=document_chunk.embedding,  # type: ignore\n\t            payload={\n\t                \"id\": document_chunk.id,\n\t                \"text\": document_chunk.text,\n\t                \"metadata\": document_chunk.metadata.dict(),\n", "                \"created_at\": created_at,\n\t            },\n\t        )\n\t    def _create_document_chunk_id(self, external_id: Optional[str]) -> str:\n\t        if external_id is None:\n\t            return uuid.uuid4().hex\n\t        return uuid.uuid5(self.UUID_NAMESPACE, external_id).hex\n\t    def _convert_query_to_search_request(\n\t        self, query: QueryWithEmbedding\n\t    ) -> rest.SearchRequest:\n", "        return rest.SearchRequest(\n\t            vector=query.embedding,\n\t            filter=self._convert_metadata_filter_to_qdrant_filter(query.filter),\n\t            limit=query.top_k,  # type: ignore\n\t            with_payload=True,\n\t            with_vector=False,\n\t        )\n\t    def _convert_metadata_filter_to_qdrant_filter(\n\t        self,\n\t        metadata_filter: Optional[DocumentMetadataFilter] = None,\n", "        ids: Optional[List[str]] = None,\n\t    ) -> Optional[rest.Filter]:\n\t        if metadata_filter is None and ids is None:\n\t            return None\n\t        must_conditions, should_conditions = [], []\n\t        # Filtering by document ids\n\t        if ids and len(ids) > 0:\n\t            for document_id in ids:\n\t                should_conditions.append(\n\t                    rest.FieldCondition(\n", "                        key=\"metadata.document_id\",\n\t                        match=rest.MatchValue(value=document_id),\n\t                    )\n\t                )\n\t        # Equality filters for the payload attributes\n\t        if metadata_filter:\n\t            meta_attributes_keys = {\n\t                \"document_id\": \"metadata.document_id\",\n\t                \"source\": \"metadata.source\",\n\t                \"source_id\": \"metadata.source_id\",\n", "                \"author\": \"metadata.author\",\n\t            }\n\t            for meta_attr_name, payload_key in meta_attributes_keys.items():\n\t                attr_value = getattr(metadata_filter, meta_attr_name)\n\t                if attr_value is None:\n\t                    continue\n\t                must_conditions.append(\n\t                    rest.FieldCondition(\n\t                        key=payload_key, match=rest.MatchValue(value=attr_value)\n\t                    )\n", "                )\n\t            # Date filters use range filtering\n\t            start_date = metadata_filter.start_date\n\t            end_date = metadata_filter.end_date\n\t            if start_date or end_date:\n\t                gte_filter = (\n\t                    to_unix_timestamp(start_date) if start_date is not None else None\n\t                )\n\t                lte_filter = (\n\t                    to_unix_timestamp(end_date) if end_date is not None else None\n", "                )\n\t                must_conditions.append(\n\t                    rest.FieldCondition(\n\t                        key=\"created_at\",\n\t                        range=rest.Range(\n\t                            gte=gte_filter,\n\t                            lte=lte_filter,\n\t                        ),\n\t                    )\n\t                )\n", "        if 0 == len(must_conditions) and 0 == len(should_conditions):\n\t            return None\n\t        return rest.Filter(must=must_conditions, should=should_conditions)\n\t    def _convert_scored_point_to_document_chunk_with_score(\n\t        self, scored_point: rest.ScoredPoint\n\t    ) -> DocumentChunkWithScore:\n\t        payload = scored_point.payload or {}\n\t        return DocumentChunkWithScore(\n\t            id=payload.get(\"id\"),\n\t            text=scored_point.payload.get(\"text\"),  # type: ignore\n", "            metadata=scored_point.payload.get(\"metadata\"),  # type: ignore\n\t            embedding=scored_point.vector,  # type: ignore\n\t            score=scored_point.score,\n\t        )\n\t    def _set_up_collection(\n\t        self, vector_size: int, distance: str, recreate_collection: bool\n\t    ):\n\t        distance = rest.Distance[distance.upper()]\n\t        if recreate_collection:\n\t            self._recreate_collection(distance, vector_size)\n", "        try:\n\t            collection_info = self.client.get_collection(self.collection_name)\n\t            current_distance = collection_info.config.params.vectors.distance  # type: ignore\n\t            current_vector_size = collection_info.config.params.vectors.size  # type: ignore\n\t            if current_distance != distance:\n\t                raise ValueError(\n\t                    f\"Collection '{self.collection_name}' already exists in Qdrant, \"\n\t                    f\"but it is configured with a similarity '{current_distance.name}'. \"\n\t                    f\"If you want to use that collection, but with a different \"\n\t                    f\"similarity, please set `recreate_collection=True` argument.\"\n", "                )\n\t            if current_vector_size != vector_size:\n\t                raise ValueError(\n\t                    f\"Collection '{self.collection_name}' already exists in Qdrant, \"\n\t                    f\"but it is configured with a vector size '{current_vector_size}'. \"\n\t                    f\"If you want to use that collection, but with a different \"\n\t                    f\"vector size, please set `recreate_collection=True` argument.\"\n\t                )\n\t        except (UnexpectedResponse, _InactiveRpcError):\n\t            self._recreate_collection(distance, vector_size)\n", "    def _recreate_collection(self, distance: rest.Distance, vector_size: int):\n\t        self.client.recreate_collection(\n\t            self.collection_name,\n\t            vectors_config=rest.VectorParams(\n\t                size=vector_size,\n\t                distance=distance,\n\t            ),\n\t        )\n\t        # Create the payload index for the document_id metadata attribute, as it is\n\t        # used to delete the document related entries\n", "        self.client.create_payload_index(\n\t            self.collection_name,\n\t            field_name=\"metadata.document_id\",\n\t            field_type=PayloadSchemaType.KEYWORD,\n\t        )\n\t        # Create the payload index for the created_at attribute, to make the lookup\n\t        # by range filters faster\n\t        self.client.create_payload_index(\n\t            self.collection_name,\n\t            field_name=\"created_at\",\n", "            field_schema=PayloadSchemaType.INTEGER,\n\t        )\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/models/models.py", "chunked_list": ["from pydantic import BaseModel\n\tfrom typing import List, Optional\n\tfrom enum import Enum\n\tclass Source(str, Enum):\n\t    email = \"email\"\n\t    file = \"file\"\n\t    chat = \"chat\"\n\tclass DocumentMetadata(BaseModel):\n\t    source: Optional[Source] = None\n\t    source_id: Optional[str] = None\n", "    url: Optional[str] = None\n\t    created_at: Optional[str] = None\n\t    author: Optional[str] = None\n\tclass DocumentChunkMetadata(DocumentMetadata):\n\t    document_id: Optional[str] = None\n\tclass DocumentChunk(BaseModel):\n\t    id: Optional[str] = None\n\t    text: str\n\t    metadata: DocumentChunkMetadata\n\t    embedding: Optional[List[float]] = None\n", "class DocumentChunkWithScore(DocumentChunk):\n\t    score: float\n\tclass Document(BaseModel):\n\t    id: Optional[str] = None\n\t    text: str\n\t    metadata: Optional[DocumentMetadata] = None\n\tclass DocumentWithChunks(Document):\n\t    chunks: List[DocumentChunk]\n\tclass DocumentMetadataFilter(BaseModel):\n\t    document_id: Optional[str] = None\n", "    source: Optional[Source] = None\n\t    source_id: Optional[str] = None\n\t    author: Optional[str] = None\n\t    start_date: Optional[str] = None  # any date string format\n\t    end_date: Optional[str] = None  # any date string format\n\tclass Query(BaseModel):\n\t    query: str\n\t    filter: Optional[DocumentMetadataFilter] = None\n\t    top_k: Optional[int] = 3\n\tclass QueryWithEmbedding(Query):\n", "    embedding: List[float]\n\tclass QueryResult(BaseModel):\n\t    query: str\n\t    results: List[DocumentChunkWithScore]\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/models/api.py", "chunked_list": ["from models.models import (\n\t    Document,\n\t    DocumentMetadataFilter,\n\t    Query,\n\t    QueryResult,\n\t)\n\tfrom pydantic import BaseModel\n\tfrom typing import List, Optional\n\tclass UpsertRequest(BaseModel):\n\t    documents: List[Document]\n", "class UpsertResponse(BaseModel):\n\t    ids: List[str]\n\tclass QueryRequest(BaseModel):\n\t    queries: List[Query]\n\tclass QueryResponse(BaseModel):\n\t    results: List[QueryResult]\n\tclass DeleteRequest(BaseModel):\n\t    ids: Optional[List[str]] = None\n\t    filter: Optional[DocumentMetadataFilter] = None\n\t    delete_all: Optional[bool] = False\n", "class DeleteResponse(BaseModel):\n\t    success: bool\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/services/chunks.py", "chunked_list": ["from typing import Dict, List, Optional, Tuple\n\timport uuid\n\tfrom models.models import Document, DocumentChunk, DocumentChunkMetadata\n\timport tiktoken\n\tfrom services.openai import get_embeddings\n\t# Global variables\n\ttokenizer = tiktoken.get_encoding(\n\t    \"cl100k_base\"\n\t)  # The encoding scheme to use for tokenization\n\t# Constants\n", "CHUNK_SIZE = 128  # The target size of each text chunk in tokens\n\tMIN_CHUNK_SIZE_CHARS = 350  # The minimum size of each text chunk in characters\n\tMIN_CHUNK_LENGTH_TO_EMBED = 5  # Discard chunks shorter than this\n\tEMBEDDINGS_BATCH_SIZE = 128  # The number of embeddings to request at a time\n\tMAX_NUM_CHUNKS = 10000  # The maximum number of chunks to generate from a text\n\tdef get_text_chunks(text: str, chunk_token_size: Optional[int]) -> List[str]:\n\t    \"\"\"\n\t    Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries.\n\t    Args:\n\t        text: The text to split into chunks.\n", "        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.\n\t    Returns:\n\t        A list of text chunks, each of which is a string of ~CHUNK_SIZE tokens.\n\t    \"\"\"\n\t    # Return an empty list if the text is empty or whitespace\n\t    if not text or text.isspace():\n\t        return []\n\t    # Tokenize the text\n\t    tokens = tokenizer.encode(text, disallowed_special=())\n\t    # Initialize an empty list of chunks\n", "    chunks = []\n\t    # Use the provided chunk token size or the default one\n\t    chunk_size = chunk_token_size or CHUNK_SIZE\n\t    # Initialize a counter for the number of chunks\n\t    num_chunks = 0\n\t    # Loop until all tokens are consumed\n\t    while tokens and num_chunks < MAX_NUM_CHUNKS:\n\t        # Take the first chunk_size tokens as a chunk\n\t        chunk = tokens[:chunk_size]\n\t        # Decode the chunk into text\n", "        chunk_text = tokenizer.decode(chunk)\n\t        # Skip the chunk if it is empty or whitespace\n\t        if not chunk_text or chunk_text.isspace():\n\t            # Remove the tokens corresponding to the chunk text from the remaining tokens\n\t            tokens = tokens[len(chunk) :]\n\t            # Continue to the next iteration of the loop\n\t            continue\n\t        # Find the last period or punctuation mark in the chunk\n\t        last_punctuation = max(\n\t            chunk_text.rfind(\".\"),\n", "            chunk_text.rfind(\"?\"),\n\t            chunk_text.rfind(\"!\"),\n\t            chunk_text.rfind(\"\\n\"),\n\t        )\n\t        # If there is a punctuation mark, and the last punctuation index is before MIN_CHUNK_SIZE_CHARS\n\t        if last_punctuation != -1 and last_punctuation > MIN_CHUNK_SIZE_CHARS:\n\t            # Truncate the chunk text at the punctuation mark\n\t            chunk_text = chunk_text[: last_punctuation + 1]\n\t        # Remove any newline characters and strip any leading or trailing whitespace\n\t        chunk_text_to_append = chunk_text.replace(\"\\n\", \" \").strip()\n", "        if len(chunk_text_to_append) > MIN_CHUNK_LENGTH_TO_EMBED:\n\t            # Append the chunk text to the list of chunks\n\t            chunks.append(chunk_text_to_append)\n\t        # Remove the tokens corresponding to the chunk text from the remaining tokens\n\t        tokens = tokens[len(tokenizer.encode(chunk_text, disallowed_special=())) :]\n\t        # Increment the number of chunks\n\t        num_chunks += 1\n\t    # Handle the remaining tokens\n\t    if tokens:\n\t        remaining_text = tokenizer.decode(tokens).replace(\"\\n\", \" \").strip()\n", "        if len(remaining_text) > MIN_CHUNK_LENGTH_TO_EMBED:\n\t            chunks.append(remaining_text)\n\t    return chunks\n\tdef create_document_chunks(\n\t    doc: Document, chunk_token_size: Optional[int]\n\t) -> Tuple[List[DocumentChunk], str]:\n\t    \"\"\"\n\t    Create a list of document chunks from a document object and return the document id.\n\t    Args:\n\t        doc: The document object to create chunks from. It should have a text attribute and optionally an id and a metadata attribute.\n", "        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.\n\t    Returns:\n\t        A tuple of (doc_chunks, doc_id), where doc_chunks is a list of document chunks, each of which is a DocumentChunk object with an id, a document_id, a text, and a metadata attribute,\n\t        and doc_id is the id of the document object, generated if not provided. The id of each chunk is generated from the document id and a sequential number, and the metadata is copied from the document object.\n\t    \"\"\"\n\t    # Check if the document text is empty or whitespace\n\t    if not doc.text or doc.text.isspace():\n\t        return [], doc.id or str(uuid.uuid4())\n\t    # Generate a document id if not provided\n\t    doc_id = doc.id or str(uuid.uuid4())\n", "    # Split the document text into chunks\n\t    text_chunks = get_text_chunks(doc.text, chunk_token_size)\n\t    metadata = (\n\t        DocumentChunkMetadata(**doc.metadata.__dict__)\n\t        if doc.metadata is not None\n\t        else DocumentChunkMetadata()\n\t    )\n\t    metadata.document_id = doc_id\n\t    # Initialize an empty list of chunks for this document\n\t    doc_chunks = []\n", "    # Assign each chunk a sequential number and create a DocumentChunk object\n\t    for i, text_chunk in enumerate(text_chunks):\n\t        chunk_id = f\"{doc_id}_{i}\"\n\t        doc_chunk = DocumentChunk(\n\t            id=chunk_id,\n\t            text=text_chunk,\n\t            metadata=metadata,\n\t        )\n\t        # Append the chunk object to the list of chunks for this document\n\t        doc_chunks.append(doc_chunk)\n", "    # Return the list of chunks and the document id\n\t    return doc_chunks, doc_id\n\tdef get_document_chunks(\n\t    documents: List[Document], chunk_token_size: Optional[int]\n\t) -> Dict[str, List[DocumentChunk]]:\n\t    \"\"\"\n\t    Convert a list of documents into a dictionary from document id to list of document chunks.\n\t    Args:\n\t        documents: The list of documents to convert.\n\t        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.\n", "    Returns:\n\t        A dictionary mapping each document id to a list of document chunks, each of which is a DocumentChunk object\n\t        with text, metadata, and embedding attributes.\n\t    \"\"\"\n\t    # Initialize an empty dictionary of lists of chunks\n\t    chunks: Dict[str, List[DocumentChunk]] = {}\n\t    # Initialize an empty list of all chunks\n\t    all_chunks: List[DocumentChunk] = []\n\t    # Loop over each document and create chunks\n\t    for doc in documents:\n", "        doc_chunks, doc_id = create_document_chunks(doc, chunk_token_size)\n\t        # Append the chunks for this document to the list of all chunks\n\t        all_chunks.extend(doc_chunks)\n\t        # Add the list of chunks for this document to the dictionary with the document id as the key\n\t        chunks[doc_id] = doc_chunks\n\t    # Check if there are no chunks\n\t    if not all_chunks:\n\t        return {}\n\t    # Get all the embeddings for the document chunks in batches, using get_embeddings\n\t    embeddings: List[List[float]] = []\n", "    for i in range(0, len(all_chunks), EMBEDDINGS_BATCH_SIZE):\n\t        # Get the text of the chunks in the current batch\n\t        batch_texts = [\n\t            chunk.text for chunk in all_chunks[i : i + EMBEDDINGS_BATCH_SIZE]\n\t        ]\n\t        # Get the embeddings for the batch texts\n\t        batch_embeddings = get_embeddings(batch_texts)\n\t        # Append the batch embeddings to the embeddings list\n\t        embeddings.extend(batch_embeddings)\n\t    # Update the document chunk objects with the embeddings\n", "    for i, chunk in enumerate(all_chunks):\n\t        # Assign the embedding from the embeddings list to the chunk object\n\t        chunk.embedding = embeddings[i]\n\t    return chunks\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/services/extract_metadata.py", "chunked_list": ["from models.models import Source\n\tfrom services.openai import get_chat_completion\n\timport json\n\tfrom typing import Dict\n\tdef extract_metadata_from_document(text: str) -> Dict[str, str]:\n\t    sources = Source.__members__.keys()\n\t    sources_string = \", \".join(sources)\n\t    # This prompt is just an example, change it to fit your use case\n\t    messages = [\n\t        {\n", "            \"role\": \"system\",\n\t            \"content\": f\"\"\"\n\t            Given a document from a user, try to extract the following metadata:\n\t            - source: string, one of {sources_string}\n\t            - url: string or don't specify\n\t            - created_at: string or don't specify\n\t            - author: string or don't specify\n\t            Respond with a JSON containing the extracted metadata in key value pairs. If you don't find a metadata field, don't specify it.\n\t            \"\"\",\n\t        },\n", "        {\"role\": \"user\", \"content\": text},\n\t    ]\n\t    completion = get_chat_completion(\n\t        messages, \"gpt-4\"\n\t    )  # TODO: change to your preferred model name\n\t    print(f\"completion: {completion}\")\n\t    try:\n\t        metadata = json.loads(completion)\n\t    except:\n\t        metadata = {}\n", "    return metadata\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/services/openai.py", "chunked_list": ["from typing import List\n\timport openai\n\tfrom tenacity import retry, wait_random_exponential, stop_after_attempt\n\t@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n\tdef get_embeddings(texts: List[str]) -> List[List[float]]:\n\t    \"\"\"\n\t    Embed texts using OpenAI's ada model.\n\t    Args:\n\t        texts: The list of texts to embed.\n\t    Returns:\n", "        A list of embeddings, each of which is a list of floats.\n\t    Raises:\n\t        Exception: If the OpenAI API call fails.\n\t    \"\"\"\n\t    # Call the OpenAI API to get the embeddings\n\t    response = openai.Embedding.create(input=texts, model=\"text-embedding-ada-002\")\n\t    # Extract the embedding data from the response\n\t    data = response[\"data\"]  # type: ignore\n\t    # Return the embeddings as a list of lists of floats\n\t    return [result[\"embedding\"] for result in data]\n", "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n\tdef get_chat_completion(\n\t    messages,\n\t    model=\"gpt-3.5-turbo\",  # use \"gpt-4\" for better results\n\t):\n\t    \"\"\"\n\t    Generate a chat completion using OpenAI's chat completion API.\n\t    Args:\n\t        messages: The list of messages in the chat history.\n\t        model: The name of the model to use for the completion. Default is gpt-3.5-turbo, which is a fast, cheap and versatile model. Use gpt-4 for higher quality but slower results.\n", "    Returns:\n\t        A string containing the chat completion.\n\t    Raises:\n\t        Exception: If the OpenAI API call fails.\n\t    \"\"\"\n\t    # call the OpenAI chat completion API with the given messages\n\t    response = openai.ChatCompletion.create(\n\t        model=model,\n\t        messages=messages,\n\t    )\n", "    choices = response[\"choices\"]  # type: ignore\n\t    completion = choices[0].message.content.strip()\n\t    print(f\"Completion: {completion}\")\n\t    return completion\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/services/pii_detection.py", "chunked_list": ["from services.openai import get_chat_completion\n\tdef screen_text_for_pii(text: str) -> bool:\n\t    # This prompt is just an example, change it to fit your use case\n\t    messages = [\n\t        {\n\t            \"role\": \"system\",\n\t            \"content\": f\"\"\"\n\t            You can only respond with the word \"True\" or \"False\", where your answer indicates whether the text in the user's message contains PII.\n\t            Do not explain your answer, and do not use punctuation.\n\t            Your task is to identify whether the text extracted from your company files\n", "            contains sensitive PII information that should not be shared with the broader company. Here are some things to look out for:\n\t            - An email address that identifies a specific person in either the local-part or the domain\n\t            - The postal address of a private residence (must include at least a street name)\n\t            - The postal address of a public place (must include either a street name or business name)\n\t            - Notes about hiring decisions with mentioned names of candidates. The user will send a document for you to analyze.\n\t            \"\"\",\n\t        },\n\t        {\"role\": \"user\", \"content\": text},\n\t    ]\n\t    completion = get_chat_completion(\n", "        messages,\n\t    )\n\t    if completion.startswith(\"True\"):\n\t        return True\n\t    return False\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/services/date.py", "chunked_list": ["import arrow\n\tdef to_unix_timestamp(date_str: str) -> int:\n\t    \"\"\"\n\t    Convert a date string to a unix timestamp (seconds since epoch).\n\t    Args:\n\t        date_str: The date string to convert.\n\t    Returns:\n\t        The unix timestamp corresponding to the date string.\n\t    If the date string cannot be parsed as a valid date format, returns the current unix timestamp and prints a warning.\n\t    \"\"\"\n", "    # Try to parse the date string using arrow, which supports many common date formats\n\t    try:\n\t        date_obj = arrow.get(date_str)\n\t        return int(date_obj.timestamp())\n\t    except arrow.parser.ParserError:\n\t        # If the parsing fails, return the current unix timestamp and print a warning\n\t        print(f\"Invalid date format: {date_str}\")\n\t        return int(arrow.now().timestamp())\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/services/file.py", "chunked_list": ["import os\n\tfrom io import BufferedReader\n\tfrom typing import Optional\n\tfrom fastapi import UploadFile\n\timport mimetypes\n\tfrom PyPDF2 import PdfReader\n\timport docx2txt\n\timport csv\n\timport pptx\n\timport tempfile\n", "from models.models import Document, DocumentMetadata\n\tasync def get_document_from_file(\n\t    file: UploadFile, metadata: DocumentMetadata\n\t) -> Document:\n\t    extracted_text = await extract_text_from_form_file(file)\n\t    doc = Document(text=extracted_text, metadata=metadata)\n\t    return doc\n\tdef extract_text_from_filepath(filepath: str, mimetype: Optional[str] = None) -> str:\n\t    \"\"\"Return the text content of a file given its filepath.\"\"\"\n\t    if mimetype is None:\n", "        # Get the mimetype of the file based on its extension\n\t        mimetype, _ = mimetypes.guess_type(filepath)\n\t    if not mimetype:\n\t        if filepath.endswith(\".md\"):\n\t            mimetype = \"text/markdown\"\n\t        else:\n\t            raise Exception(\"Unsupported file type\")\n\t    try:\n\t        with open(filepath, \"rb\") as file:\n\t            extracted_text = extract_text_from_file(file, mimetype)\n", "    except Exception as e:\n\t        print(f\"Error: {e}\")\n\t        raise e\n\t    return extracted_text\n\tdef extract_text_from_file(file: BufferedReader, mimetype: str) -> str:\n\t    if mimetype == \"application/pdf\":\n\t        # Extract text from pdf using PyPDF2\n\t        reader = PdfReader(file)\n\t        extracted_text = \" \".join([page.extract_text() for page in reader.pages])\n\t    elif mimetype == \"text/plain\" or mimetype == \"text/markdown\":\n", "        # Read text from plain text file\n\t        extracted_text = file.read().decode(\"utf-8\")\n\t    elif (\n\t        mimetype\n\t        == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n\t    ):\n\t        # Extract text from docx using docx2txt\n\t        extracted_text = docx2txt.process(file)\n\t    elif mimetype == \"text/csv\":\n\t        # Extract text from csv using csv module\n", "        extracted_text = \"\"\n\t        decoded_buffer = (line.decode(\"utf-8\") for line in file)\n\t        reader = csv.reader(decoded_buffer)\n\t        for row in reader:\n\t            extracted_text += \" \".join(row) + \"\\n\"\n\t    elif (\n\t        mimetype\n\t        == \"application/vnd.openxmlformats-officedocument.presentationml.presentation\"\n\t    ):\n\t        # Extract text from pptx using python-pptx\n", "        extracted_text = \"\"\n\t        presentation = pptx.Presentation(file)\n\t        for slide in presentation.slides:\n\t            for shape in slide.shapes:\n\t                if shape.has_text_frame:\n\t                    for paragraph in shape.text_frame.paragraphs:\n\t                        for run in paragraph.runs:\n\t                            extracted_text += run.text + \" \"\n\t                    extracted_text += \"\\n\"\n\t    else:\n", "        # Unsupported file type\n\t        raise ValueError(\"Unsupported file type: {}\".format(mimetype))\n\t    return extracted_text\n\t# Extract text from a file based on its mimetype\n\tasync def extract_text_from_form_file(file):\n\t    # create a temporary directory to store the uploaded file\n\t    temp_dir = tempfile.mkdtemp()\n\t    temp_file_path = os.path.join(temp_dir, file.filename)\n\t    mimetype = file.content_type\n\t    print(f\"mimetype: {mimetype}\")\n", "    print(f\"file.file: {file.file}\")\n\t    print(\"file: \", file)   \n\t    # write the file to a temporary location\n\t    with open(temp_file_path, \"wb\") as f:\n\t        f.write(await file.read())\n\t    # extract the text from the file\n\t    try:\n\t        extracted_text = extract_text_from_filepath(temp_file_path, mimetype)\n\t    except Exception as e:\n\t        print(f\"Error: {e}\")\n", "        os.remove(temp_file_path)\n\t        raise e\n\t    # remove file from temp location\n\t    os.remove(temp_file_path)\n\t    return extracted_text"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/examples/authentication-methods/no-auth/main.py", "chunked_list": ["# This is a version of the main.py file found in ../../../server/main.py without authentication.\n\t# Copy and paste this into the main file at ../../../server/main.py if you choose to use no authentication for your retrieval plugin.\n\tfrom typing import Optional\n\timport uvicorn\n\tfrom fastapi import FastAPI, File, Form, HTTPException, Body, UploadFile\n\tfrom fastapi.staticfiles import StaticFiles\n\tfrom models.api import (\n\t    DeleteRequest,\n\t    DeleteResponse,\n\t    QueryRequest,\n", "    QueryResponse,\n\t    UpsertRequest,\n\t    UpsertResponse,\n\t)\n\tfrom datastore.factory import get_datastore\n\tfrom services.file import get_document_from_file\n\tfrom models.models import DocumentMetadata, Source\n\tapp = FastAPI()\n\tapp.mount(\"/.well-known\", StaticFiles(directory=\".well-known\"), name=\"static\")\n\t# Create a sub-application, in order to access just the query endpoints in the OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally\n", "sub_app = FastAPI(\n\t    title=\"Retrieval Plugin API\",\n\t    description=\"A retrieval API for querying and filtering documents based on natural language queries and metadata\",\n\t    version=\"1.0.0\",\n\t    servers=[{\"url\": \"https://your-app-url.com\"}],\n\t)\n\tapp.mount(\"/sub\", sub_app)\n\t@app.post(\n\t    \"/upsert-file\",\n\t    response_model=UpsertResponse,\n", ")\n\tasync def upsert_file(\n\t    file: UploadFile = File(...),\n\t    metadata: Optional[str] = Form(None),\n\t):\n\t    try:\n\t        metadata_obj = (\n\t            DocumentMetadata.parse_raw(metadata)\n\t            if metadata\n\t            else DocumentMetadata(source=Source.file)\n", "        )\n\t    except:\n\t        metadata_obj = DocumentMetadata(source=Source.file)\n\t    document = await get_document_from_file(file, metadata_obj)\n\t    try:\n\t        ids = await datastore.upsert([document])\n\t        return UpsertResponse(ids=ids)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=f\"str({e})\")\n", "@app.post(\n\t    \"/upsert\",\n\t    response_model=UpsertResponse,\n\t)\n\tasync def upsert(\n\t    request: UpsertRequest = Body(...),\n\t):\n\t    try:\n\t        ids = await datastore.upsert(request.documents)\n\t        return UpsertResponse(ids=ids)\n", "    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.post(\n\t    \"/query\",\n\t    response_model=QueryResponse,\n\t)\n\tasync def query_main(\n\t    request: QueryRequest = Body(...),\n\t):\n", "    try:\n\t        results = await datastore.query(\n\t            request.queries,\n\t        )\n\t        return QueryResponse(results=results)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@sub_app.post(\n\t    \"/query\",\n", "    response_model=QueryResponse,\n\t    description=\"Accepts search query objects with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.\",\n\t)\n\tasync def query(\n\t    request: QueryRequest = Body(...),\n\t):\n\t    try:\n\t        results = await datastore.query(\n\t            request.queries,\n\t        )\n", "        return QueryResponse(results=results)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.delete(\n\t    \"/delete\",\n\t    response_model=DeleteResponse,\n\t)\n\tasync def delete(\n\t    request: DeleteRequest = Body(...),\n", "):\n\t    if not (request.ids or request.filter or request.delete_all):\n\t        raise HTTPException(\n\t            status_code=400,\n\t            detail=\"One of ids, filter, or delete_all is required\",\n\t        )\n\t    try:\n\t        success = await datastore.delete(\n\t            ids=request.ids,\n\t            filter=request.filter,\n", "            delete_all=request.delete_all,\n\t        )\n\t        return DeleteResponse(success=success)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.on_event(\"startup\")\n\tasync def startup():\n\t    global datastore\n\t    datastore = await get_datastore()\n", "def start():\n\t    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"]}
{"filename": "cute_assistant/chatgpt-retrieval-plugin/examples/memory/main.py", "chunked_list": ["# This is a version of the main.py file found in ../../server/main.py that also gives ChatGPT access to the upsert endpoint\n\t# (allowing it to save information from the chat back to the vector) database.\n\t# Copy and paste this into the main file at ../../server/main.py if you choose to give the model access to the upsert endpoint\n\t# and want to access the openapi.json when you run the app locally at http://0.0.0.0:8000/sub/openapi.json.\n\timport os\n\tfrom typing import Optional\n\timport uvicorn\n\tfrom fastapi import FastAPI, File, Form, HTTPException, Depends, Body, UploadFile\n\tfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\tfrom fastapi.staticfiles import StaticFiles\n", "from models.api import (\n\t    DeleteRequest,\n\t    DeleteResponse,\n\t    QueryRequest,\n\t    QueryResponse,\n\t    UpsertRequest,\n\t    UpsertResponse,\n\t)\n\tfrom datastore.factory import get_datastore\n\tfrom services.file import get_document_from_file\n", "from models.models import DocumentMetadata, Source\n\tbearer_scheme = HTTPBearer()\n\tBEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\")\n\tassert BEARER_TOKEN is not None\n\tdef validate_token(credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)):\n\t    if credentials.scheme != \"Bearer\" or credentials.credentials != BEARER_TOKEN:\n\t        raise HTTPException(status_code=401, detail=\"Invalid or missing token\")\n\t    return credentials\n\tapp = FastAPI()\n\tapp.mount(\"/.well-known\", StaticFiles(directory=\".well-known\"), name=\"static\")\n", "# Create a sub-application, in order to access just the upsert and query endpoints in the OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally\n\tsub_app = FastAPI(\n\t    title=\"Retrieval Plugin API\",\n\t    description=\"A retrieval API for querying and filtering documents based on natural language queries and metadata\",\n\t    version=\"1.0.0\",\n\t    servers=[{\"url\": \"https://your-app-url.com\"}],\n\t    dependencies=[Depends(validate_token)],\n\t)\n\tapp.mount(\"/sub\", sub_app)\n\t@app.post(\n", "    \"/upsert-file\",\n\t    response_model=UpsertResponse,\n\t)\n\tasync def upsert_file(\n\t    file: UploadFile = File(...),\n\t    metadata: Optional[str] = Form(None),\n\t):\n\t    try:\n\t        metadata_obj = (\n\t            DocumentMetadata.parse_raw(metadata)\n", "            if metadata\n\t            else DocumentMetadata(source=Source.file)\n\t        )\n\t    except:\n\t        metadata_obj = DocumentMetadata(source=Source.file)\n\t    document = await get_document_from_file(file, metadata_obj)\n\t    try:\n\t        ids = await datastore.upsert([document])\n\t        return UpsertResponse(ids=ids)\n\t    except Exception as e:\n", "        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=f\"str({e})\")\n\t@app.post(\n\t    \"/upsert\",\n\t    response_model=UpsertResponse,\n\t)\n\tasync def upsert_main(\n\t    request: UpsertRequest = Body(...),\n\t    token: HTTPAuthorizationCredentials = Depends(validate_token),\n\t):\n", "    try:\n\t        ids = await datastore.upsert(request.documents)\n\t        return UpsertResponse(ids=ids)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@sub_app.post(\n\t    \"/upsert\",\n\t    response_model=UpsertResponse,\n\t    # NOTE: We are describing the shape of the API endpoint input due to a current limitation in parsing arrays of objects from OpenAPI schemas. This will not be necessary in the future.\n", "    description=\"Save chat information. Accepts an array of documents with text (potential questions + conversation text), metadata (source 'chat' and timestamp, no ID as this will be generated). Confirm with the user before saving, ask for more details/context.\",\n\t)\n\tasync def upsert(\n\t    request: UpsertRequest = Body(...),\n\t    token: HTTPAuthorizationCredentials = Depends(validate_token),\n\t):\n\t    try:\n\t        ids = await datastore.upsert(request.documents)\n\t        return UpsertResponse(ids=ids)\n\t    except Exception as e:\n", "        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.post(\n\t    \"/query\",\n\t    response_model=QueryResponse,\n\t)\n\tasync def query_main(\n\t    request: QueryRequest = Body(...),\n\t    token: HTTPAuthorizationCredentials = Depends(validate_token),\n\t):\n", "    try:\n\t        results = await datastore.query(\n\t            request.queries,\n\t        )\n\t        return QueryResponse(results=results)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@sub_app.post(\n\t    \"/query\",\n", "    response_model=QueryResponse,\n\t    # NOTE: We are describing the shape of the API endpoint input due to a current limitation in parsing arrays of objects from OpenAPI schemas. This will not be necessary in the future.\n\t    description=\"Accepts search query objects array each with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.\",\n\t)\n\tasync def query(\n\t    request: QueryRequest = Body(...),\n\t    token: HTTPAuthorizationCredentials = Depends(validate_token),\n\t):\n\t    try:\n\t        results = await datastore.query(\n", "            request.queries,\n\t        )\n\t        return QueryResponse(results=results)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.delete(\n\t    \"/delete\",\n\t    response_model=DeleteResponse,\n\t)\n", "async def delete(\n\t    request: DeleteRequest = Body(...),\n\t    token: HTTPAuthorizationCredentials = Depends(validate_token),\n\t):\n\t    if not (request.ids or request.filter or request.delete_all):\n\t        raise HTTPException(\n\t            status_code=400,\n\t            detail=\"One of ids, filter, or delete_all is required\",\n\t        )\n\t    try:\n", "        success = await datastore.delete(\n\t            ids=request.ids,\n\t            filter=request.filter,\n\t            delete_all=request.delete_all,\n\t        )\n\t        return DeleteResponse(success=success)\n\t    except Exception as e:\n\t        print(\"Error:\", e)\n\t        raise HTTPException(status_code=500, detail=\"Internal Service Error\")\n\t@app.on_event(\"startup\")\n", "async def startup():\n\t    global datastore\n\t    datastore = await get_datastore()\n\tdef start():\n\t    uvicorn.run(\"server.main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n"]}
{"filename": "cute_assistant/utils/utils.py", "chunked_list": ["import re\n\tdef format_memory(role, message):\n\t    user = \"Memory\"\n\t    msg = {\n\t        \"role\" : role,\n\t        \"content\" : f\"{user} : {str(message)}\"\n\t    }\n\t    return msg\n\tdef format_discord_tag(user):\n\t    formatted_tag = f\"{user.display_name} ({str(user)})\"\n", "    return formatted_tag\n\tdef format_message(message, role, user, preprompt=None):\n\t    if preprompt:\n\t        content = f\"{preprompt} : {format_discord_tag(user)} : {str(message)}\"\n\t    else:\n\t        content = f\"{format_discord_tag(user)} : {str(message)}\"\n\t    msg = {\n\t        \"role\" : role,\n\t        \"content\" : content\n\t    }\n", "    return msg\n\tdef remove_links(s: str) -> str:\n\t    # Remove markdown image links\n\t    s = re.sub(r\"!\\[.*?\\]\", \"\", s)\n\t    # Remove wikilinks\n\t    s = re.sub(r\"\\[\\[.*?\\]\\]\", \"\", s)\n\t    # Remove markdown links\n\t    s = re.sub(r\"\\[.*?\\]\\(.*?\\)\", \"\", s)\n\t    # Remove markdown heading tags\n\t    s = re.sub(r\"#+\", \"\", s)\n", "    # Remove markdown line break formatting\n\t    s = re.sub(r\"\\*{3,4}|-{3,4}\", \"\", s)\n\t    # Remove URLs\n\t    s = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', s)\n\t    return s\n\tdef remove_phrases_from_string(phrases, text):\n\t    for phrase in phrases:\n\t        text = text.replace(phrase, '')\n\t    return text\n\tasync def check_privilege(user_id: int, level: str, settings: dict) -> bool:\n", "    level_users = settings.get(level, [])\n\t    return user_id in level_users"]}
{"filename": "cute_assistant/utils/pdf2md.py", "chunked_list": ["import pdfplumber\n\timport argparse\n\timport os\n\tdef pdf_to_text(input_pdf, output_txt):\n\t    with pdfplumber.open(input_pdf) as pdf:\n\t        text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n\t    with open(output_txt, 'w', encoding='utf-8') as f:\n\t        f.write(text)\n\tdef main():\n\t    parser = argparse.ArgumentParser(description='Convert PDF to TXT')\n", "    parser.add_argument('input_pdf', help='Input PDF file')\n\t    parser.add_argument('-txt', '--text_output', help='Output TXT file', default=None)\n\t    args = parser.parse_args()\n\t    input_pdf = args.input_pdf\n\t    if args.text_output:\n\t        output_txt = args.text_output\n\t    else:\n\t        output_txt = os.path.splitext(input_pdf)[0] + '.txt'\n\t    pdf_to_text(input_pdf, output_txt)\n\tif __name__ == '__main__':\n", "    main()\n"]}
{"filename": "cute_assistant/extensions/config_cog.py", "chunked_list": ["# A cog extension for channel and configuration-related commands\n\timport discord\n\tfrom discord.ext import commands\n\tfrom discord.commands import option\n\timport cute_assistant.core.nosql_module as db\n\tfrom cute_assistant.utils.utils import check_privilege\n\tfrom cute_assistant.core.log import cute_logger as logger\n\timport json\n\twith open(\"datastore/settings.json\", \"r\") as f:\n\t    settings = json.load(f)\n", "with open(\"datastore/config.json\", \"r\") as f:\n\t    config = json.load(f)\n\twith open(\"datastore/responses.json\", \"r\") as f:\n\t    responses = json.load(f)\n\tasync def update_config(updates: dict):\n\t    with open(\"datastore/config.json\", \"r\") as f:\n\t        config = json.load(f)\n\t    config.update(updates)\n\t    with open(\"datastore/config.json\", \"w\") as f:\n\t        json.dump(config, f, indent=4)\n", "    with open(\"datastore/config.json\", \"r\") as f:\n\t        config = json.load(f)\n\tclass Configuration(commands.Cog):\n\t    def __init__(self, bot):\n\t        self.bot = bot\n\t    # Ping-pong\n\t    @commands.slash_command(description=f\"Say Hello\", guild_ids=config['guilds'])\n\t    async def hello(self, ctx: discord.ApplicationContext, name: str = None):\n\t        name = name or ctx.author.name\n\t        await ctx.respond(f\"Hello {name}!\")\n", "    # Set chat settings - not limited to admins\n\t    @commands.slash_command(description=\"Set the Temperature\", guild_ids=config['guilds'])  # Replace 1234567890 with your actual guild ID\n\t    @option(\"value\", description=\"Temperature range 0-2, higher for more creative results\")\n\t    async def set_temp(self, ctx: discord.ApplicationContext, value: float):\n\t        before = db.get_channel_setting(ctx.channel.id, \"config_temp\", default=config['default_temp'])\n\t        db.set_channel_setting(ctx.channel.id, \"config_temp\", value)\n\t        await ctx.respond(f\"Config_temp for channel `{ctx.channel.id}` has been set from **{before}** to: **{value}**\")\n\t    @commands.slash_command(description=\"Set the Frequency Penalty\", guild_ids=config['guilds'])  # Replace 1234567890 with your actual guild ID\n\t    @option(\"value\", description=\"Frequency 0-2 \")\n\t    async def set_freq(self, ctx: discord.ApplicationContext, value: float):\n", "        before = db.get_channel_setting(ctx.channel.id, \"config_freq\", default=config['default_freq'])\n\t        db.set_channel_setting(ctx.channel.id, \"config_freq\", value)\n\t        await ctx.respond(f\"Config_freq for channel `{ctx.channel.id}` has been set from **{before}** to: **{value}**\")\n\t    @commands.slash_command(description=\"Set the Presence Penalty\", guild_ids=config['guilds'])  # Replace 1234567890 with your actual guild ID\n\t    @option(\"value\", description=\"Presence 0-2\")\n\t    async def set_pres(self, ctx: discord.ApplicationContext, value: float):\n\t        before = db.get_channel_setting(ctx.channel.id, \"config_pres\", default=config['default_pres'])\n\t        db.set_channel_setting(ctx.channel.id, \"config_pres\", value)\n\t        await ctx.respond(f\"Config_pres for channel `{ctx.channel.id}` has been set from **{before}** to: **{value}**\")\n\t    # Dangerous! Drops tables!!! (Not the vector tables though)\n", "    @commands.slash_command(description=f\"Clear conversations database\", guild_ids=config['guilds'])\n\t    async def clear_convo(self, ctx: discord.ApplicationContext):\n\t        if not await check_privilege(ctx.user.id, 'admins', config):\n\t            await ctx.respond('You do not have sufficient user permissions to use this command.')\n\t            logger(\"discord\").info(f\"{ctx.user}: User does not have permissions\")\n\t            return\n\t        db.drop_tables()\n\t        await ctx.respond(f\"Conversations cleared.\")\n\t    @commands.slash_command(description=\"Start a new conversation in this Channel\", guild_ids=config['guilds'])\n\t    async def new_convo(self, ctx: discord.ApplicationContext):\n", "        db.add_conversation(\"Title for now\", ctx.channel.id)    \n\t        await ctx.respond(f\"Conversation restarted!\")\n\t    @commands.slash_command(description=\"Allow bot to use this channel or another channel\", guild_ids=config['guilds'])\n\t    @option(\"channel\", description=\"The Channel ID\")\n\t    @option(\"allowed\", description=\"True/False\")\n\t    async def allow_channel(self, ctx: discord.ApplicationContext, channel: str = None, allowed: bool = True):\n\t        # Check for permissions\n\t        if not await check_privilege(ctx.user.id, 'admins', config):\n\t            await ctx.respond('You do not have sufficient user permissions to use this command.')\n\t            logger(\"discord\").info(f\"{ctx.user}: User does not have permissions\")\n", "            return\n\t        # Get the channel ID, assume it is the current channel if None\n\t        if channel is None:\n\t            channel = ctx.channel.id\n\t        else:\n\t            if commands.get_channel(channel) is None:\n\t                await ctx.respond(f\"Channel `{channel}` was not found.\")\n\t                logger(\"discord\").info(f\"{ctx.user}: Channel `{channel}` was not found.\")\n\t                return\n\t        # Find the channel in the database. if not, add, else, update.\n", "        db_channel = db.read_channel(channel)\n\t        if not db_channel:\n\t            db.create_channel(channel, allowed)\n\t        else:\n\t            # Channel exists, set the value to be allowed\n\t            db.update_channel(channel, allowed)\n\t        await ctx.respond(f\"Channel `{channel}` permissions have been set to **{allowed}**.\")\n\t    @commands.slash_command(description=\"Allow bot to output memories to a set logging channel\", guild_ids=config['guilds'])\n\t    @option(\"channel\", description=\"The Channel ID\")\n\t    @option(\"allowed\", description=\"True/False\")\n", "    async def allow_memory_log(self, ctx: discord.ApplicationContext, channel: str = None, allowed: bool = True):\n\t        # Check for permissions\n\t        if not await check_privilege(ctx.user.id, 'admins', config):\n\t            await ctx.respond('You do not have sufficient user permissions to use this command.')\n\t            logger(\"discord\").info(f\"{ctx.user}: User does not have permissions\")\n\t            return\n\t        memlog = list(config['memory_log'])\n\t        channel_id = channel or ctx.channel.id\n\t        if allowed:\n\t            if not channel_id in memlog:\n", "                memlog.append(channel_id)\n\t            response = f\"Channel `{channel_id}` has been set as a Memory Log.\"\n\t        else: # Want to remove from list\n\t            memlog.remove(channel_id)\n\t            response = f\"Channel `{channel_id}` has been removed from the memory log list\"\n\t        await update_config(updates = {\"memory_log\" : memlog})\n\t        await ctx.respond(response)\n\t    @commands.slash_command(description=\"Allow bot to echo memories when answering\", guild_ids=config['guilds'])\n\t    @option(\"allowed\", description=\"True/False\")\n\t    async def enable_memory_echo(self, ctx: discord.ApplicationContext, allowed: bool = False):\n", "        # Check for permissions\n\t        if not await check_privilege(ctx.user.id, 'admins', config):\n\t            await ctx.respond('You do not have sufficient user permissions to use this command.')\n\t            logger(\"discord\").info(f\"{ctx.user}: User does not have permissions\")\n\t            return\n\t        response = f\"Memory echo has been set to **{allowed}**\"\n\t        await update_config(updates = {\"enable_memory_echo\" : allowed})\n\t        await ctx.respond(response)\n\t    @commands.slash_command(description=\"Set this channel type\", guild_ids=config['guilds'])\n\t    async def set_channel_type(self, ctx: discord.ApplicationContext, channel: str = None, type: str = \"None\"):\n", "        # Check for permissions\n\t        if not await check_privilege(ctx.user.id, 'admins', config):\n\t            await ctx.respond('You do not have sufficient user permissions to use this command.')\n\t            logger(\"discord\").info(f\"{ctx.user}: User does not have permissions\")\n\t            return\n\t        # Get the channel ID, assume it is the current channel if None\n\t        if channel is None:\n\t            channel = ctx.channel.id\n\t        else:\n\t            if commands.get_channel(channel) is None:\n", "                await ctx.respond(f\"Channel `{channel}` was not found.\")\n\t                logger(\"discord\").info(f\"{ctx.user}: Channel `{channel}` was not found.\")\n\t                return\n\t        # Find the channel in the database. if not, add\n\t        response = \"\"\n\t        db_channel = db.read_channel(channel)\n\t        if not db_channel:\n\t            db.create_channel(channel, False)\n\t            response = f\"Channel `{channel}` permissions have been set to **False**. \"\n\t            db_channel = db.read_channel(channel)\n", "        # Set the channel type\n\t        db.set_channel_type(channel, type )\n\t        response += f\"Channel `{channel}` has been set to type `{type}`\"\n\t        await ctx.respond(response)\n\t    @commands.slash_command(description=\"Get the channel type\", guild_ids=config['guilds'])\n\t    async def get_channel_type(self, ctx: discord.ApplicationContext, channel: str = None):\n\t        # Check for permissions\n\t        if not await check_privilege(ctx.user.id, 'admins', config):\n\t            await ctx.respond('You do not have sufficient user permissions to use this command.')\n\t            logger(\"discord\").info(f\"{ctx.user}: User does not have permissions\")\n", "            return\n\t        # Get the channel ID, assume it is the current channel if None\n\t        if channel is None:\n\t            channel = ctx.channel.id\n\t        else:\n\t            if commands.get_channel(channel) is None:\n\t                await ctx.respond(f\"Channel `{channel}` was not found.\")\n\t                logger(\"discord\").info(f\"{ctx.user}: Channel `{channel}` was not found.\")\n\t                return\n\t        # Find the channel in the database\n", "        type = db.get_channel_type(channel)\n\t        response = f\"Channel `{channel}` is of type `{type}`\"\n\t        await ctx.respond(response)\n\tdef setup(bot):\n\t    bot.add_cog(Configuration(bot))"]}
{"filename": "cute_assistant/core/channel.py", "chunked_list": ["import os\n\tfrom discord.ext import commands\n\tbot = commands.Bot(command_prefix=\"!\")\n\t@bot.command()\n\tasync def list_users(ctx, channel_id: int):\n\t    channel = bot.get_channel(channel_id)\n\t    if isinstance(channel, discord.VoiceChannel):\n\t        users = [member.display_name for member in channel.members]\n\t        await ctx.send(f\"Users in the channel {channel.name}: {', '.join(users)}\")\n\t    else:\n", "        await ctx.send(\"Invalid voice channel ID.\")\n\t@bot.command()\n\tasync def server_info(ctx):\n\t    server_name = ctx.guild.name\n\t    server_created_at = ctx.guild.created_at\n\t    await ctx.send(f\"Server name: {server_name}\\nServer created at: {server_created_at}\")\n\t@bot.event\n\tasync def on_message(message):\n\t    if message.author == bot.user:\n\t        return\n", "    # Replace mentions with Discord tag and nickname\n\t    content = message.content\n\t    for mention in message.mentions:\n\t        content = content.replace(f\"<@!{mention.id}>\", f\"{mention.name}#{mention.discriminator} ({mention.display_name})\")\n\t    # Reply to the message\n\t    reply = f\"Message received: {content}\"\n\t    await message.reply(reply)\n\t    # Process bot commands\n\t    await bot.process_commands(message)\n\t@bot.event\n", "async def on_ready():\n\t    print(f\"{bot.user.name} has connected to Discord!\")\n\tif __name__ == \"__main__\":\n\t    bot.run(os.environ[\"DISCORD_TOKEN\"])\n"]}
{"filename": "cute_assistant/core/log.py", "chunked_list": ["import logging\n\timport os\n\tdef cute_logger(name):\n\t    # Create the logs directory if it doesn't exist\n\t    os.makedirs(\"logs\", exist_ok=True)\n\t    # Set up the logger\n\t    logger = logging.getLogger(name)\n\t    logger.setLevel(logging.INFO)\n\t    # Set up a FileHandler to save logs to disk\n\t    file_handler = logging.FileHandler(f\"logs/cute_{name}.log\")\n", "    logger.addHandler(file_handler)\n\t    # Set up a formatter for the log messages\n\t    formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n\t    file_handler.setFormatter(formatter)\n\t    # Check if a file handler is already added\n\t    if not any(isinstance(handler, logging.FileHandler) for handler in logger.handlers):\n\t        # Set up a FileHandler to save logs to disk\n\t        file_handler = logging.FileHandler(f\"logs/{name}.log\")\n\t        logger.addHandler(file_handler)\n\t        # Set up a formatter for the log messages\n", "        formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n\t        file_handler.setFormatter(formatter)\n\t    return logger\n"]}
{"filename": "cute_assistant/core/client.py", "chunked_list": ["import discord\n\tfrom discord.ext import commands\n\tfrom discord.commands import option\n\tfrom datetime import datetime\n\timport cute_assistant.core.nosql_module as db\n\timport cute_assistant.core.database_utils as vdb\n\timport cute_assistant.core.tokens as tk\n\timport openai\n\timport pprint\n\timport secrets\n", "import random\n\timport re\n\timport json\n\tfrom cute_assistant.core.log import cute_logger as logger\n\tfrom cute_assistant.utils.utils import remove_links, remove_phrases_from_string, format_message, format_discord_tag, format_memory\n\timport asyncio\n\twith open(\"datastore/settings.json\", \"r\") as f:\n\t    settings = json.load(f)\n\twith open(\"datastore/config.json\", \"r\") as f:\n\t    config = json.load(f)\n", "with open(\"datastore/responses.json\", \"r\") as f:\n\t    responses = json.load(f)\n\topenai.api_key = settings[\"openai_api_key\"]\n\tvdb.db_bearer_token = settings[\"db_int_bearer_token\"]\n\t# Set up bot\n\tintents = discord.Intents.default()\n\tintents.message_content = True\n\tintents.members = True\n\tclient = commands.Bot(command_prefix=\"!\", intents=intents)\n\tclient.load_extension(\"cute_assistant.extensions.config_cog\")\n", "status_ch = None\n\tremove_phrases = config['removed_phrases']\n\t@client.event\n\tasync def on_ready():\n\t    global status_ch\n\t    logger(\"discord\").info(f\"Logged in as {client.user} (ID: {client.user.id})\")\n\t    on_ready_msg = f\"Logged in as {client.user} (ID: {client.user.id})\"\n\t    print(on_ready_msg)\n\t    print(\"------\")\n\t    # Log status to dedicated announcements channel\n", "    status_ch = client.get_channel(1102061260671045652)\n\t    await status_ch.send(on_ready_msg)\n\tasync def handle_upload(message) -> str:\n\t    ctx = await client.get_context(message)\n\t    user = message.author\n\t    async with ctx.typing():\n\t        if message.attachments:\n\t            for attachment in message.attachments:\n\t                if attachment.filename.endswith('.txt') or attachment.filename.endswith('.md'):\n\t                    content = await attachment.read()\n", "                    content_str = remove_links(content.decode('utf-8'))\n\t                    db.save_file_content(attachment.filename, content_str)\n\t                    response_code = vdb.upsert(secrets.token_hex(64 // 2), str({\"content\" : format_discord_tag(user) + \" : \" + content_str, \"time\" : str(datetime.now().isoformat())}))\n\t            msg_response = \" > \" + random.choice(responses['memory_plural'])\n\t        if str(message.content) != \"\":\n\t            response_code = vdb.upsert(secrets.token_hex(64 // 2), str({\"content\" : format_discord_tag(user) + \" : \" + str(message.content), \"time\" : str(datetime.now().isoformat())}))\n\t            msg_response = \" > \" + random.choice(responses['memory_single'])\n\t        if response_code == 200: msg_response += f\"\\n {random.choice(responses['memory_success'])}\" \n\t        else: msg_response += f\"\\n {random.choice(responses['memory_failure'])}\" \n\t    return msg_response\n", "@client.event\n\tasync def on_message(message):\n\t    ctx = await client.get_context(message)\n\t    user = message.author\n\t    if message.author.bot:\n\t        return\n\t    if not db.is_channel_allowed(message.channel.id):\n\t        return\n\t    if db.get_channel_type(message.channel.id) == \"memory\":\n\t        msg_response = await handle_upload(message)\n", "        await message.reply(msg_response)\n\t        return\n\t    if db.get_channel_type(message.channel.id) == \"test\":\n\t        # Test area\n\t        return\n\t    conversation_id = db.get_most_recent_conversation(message.channel.id)['conversation_id']\n\t    system_prompt = config[\"system_prompt\"]\n\t    gateway_prompts = config[\"gateway_prompts\"]\n\t    pre_prompt = f\"{str(datetime.now().isoformat())} : {config['pre_prompt']}\"\n\t    new_message = format_message(str(message.content), \"user\",  client.get_user(int(user.id)), preprompt=pre_prompt)\n", "    # Get long term memory from vdb\n\t    chunks_response = vdb.query_database(message.content)\n\t    selected_memories, distant_memories = tk.get_memory_until_token_limit(chunks_response, 1024)\n\t    for memory in selected_memories:\n\t        db.delete_memory(memory['id'])\n\t        db.add_memory(memory['id'], memory['text'])\n\t    print(\" --- SELECTED MEMORY --- \")\n\t    pprint.pprint(selected_memories)\n\t    print(\" --- END --- \")\n\t    memory_shards = []\n", "    if config['memory_log']:\n\t        for ch in config['memory_log']:\n\t            send_ch = client.get_channel(ch)\n\t            for memory in selected_memories:\n\t                await send_ch.send('> ' + memory['text'])\n\t                #react to delete these heheh\n\t    for result in selected_memories:\n\t        memory_shards.append(result[\"text\"])\n\t    for result in selected_memories:\n\t        memory_shards.append(result[\"text\"])\n", "    distant_shards = []\n\t    for result in distant_memories:\n\t        distant_shards.append(result[\"text\"])\n\t    mem_messages = [format_memory(\"user\", _msg) for _msg in memory_shards]\n\t    dist_messages = [format_memory(\"user\", _msg) for _msg in distant_shards]\n\t    memory_tokens = tk.get_num_tokens(mem_messages)\n\t    distant_tokens = tk.get_num_tokens(dist_messages)\n\t    print(f\"Memory Tokens: {memory_tokens}, Distant Tokens: {distant_tokens}\")\n\t    print(\" --- MEMORY --- \")\n\t    pprint.pprint(mem_messages)\n", "    print(\" --- END --- \")\n\t    system_prompt_tokens = tk.get_num_tokens([system_prompt])\n\t    gateway_prompts_tokens = tk.get_num_tokens(gateway_prompts)\n\t    memory_tokens = tk.get_num_tokens(mem_messages)\n\t    query_tokens = tk.get_num_tokens([new_message])\n\t    max_tokens = 4096 - 1200 - system_prompt_tokens - gateway_prompts_tokens - memory_tokens - query_tokens\n\t    last_messages = tk.get_messages_until_token_limit(conversation_id, max_tokens)\n\t    prev_messages = [format_message(_msg['message'], _msg['role'], client.get_user(int(_msg['user_id']))) for _msg in last_messages]\n\t    last_messages_tokens = tk.get_num_tokens(prev_messages)\n\t    # Message format\n", "    if prev_messages: \n\t        messages = [system_prompt] + gateway_prompts + mem_messages + prev_messages + [new_message]\n\t    else:\n\t        messages = [system_prompt] + gateway_prompts + mem_messages  + [new_message]\n\t    memory_prompt = {\n\t        \"role\" : \"user\",\n\t        \"content\" : f\"You are to shorten {len(distant_shards)} pieces of information encapsulated in <> into {max(len(distant_shards)-1,1)} or less pieces also encapsulated in <> organised by ideas. Split and re-word if ideas are combined such that the output pieces contain related ideas.\"\n\t    }\n\t    # Memory Format\n\t    for shard in distant_shards:\n", "        filtered_shard = shard.replace('<', '').replace('>', '')\n\t        memory_prompt['content'] = f\"{memory_prompt['content']} <{filtered_shard}>\" \n\t    memory_messages = [\n\t            {\n\t            \"role\" : \"system\",\n\t            \"content\" : \"You are a text shortening AI, using your understanding of natural language to ensure summaries are faithful to the original content and written in Australian English. The shortened text contains wording from the original where possible. If the piece is an encoded string that is not natural language, please remove the piece entirely.\"\n\t        },\n\t        memory_prompt\n\t    ]\n\t    max_mem_tokens = 4096 - tk.get_num_tokens(memory_messages)\n", "    print(\" --- MESSAGES --- \")\n\t    pprint.pprint(messages)\n\t    print(\" --- END --- \")\n\t    max_tokens = max_tokens - last_messages_tokens - 1\n\t    config_temp = db.get_channel_setting(message.channel.id, \"config_temp\", default=config['default_temp'])\n\t    config_freq = db.get_channel_setting(message.channel.id, \"config_freq\", default=config['default_freq'])\n\t    config_pres = db.get_channel_setting(message.channel.id, \"config_pres\", default=config['default_pres'])\n\t    print(f\"Total tokens: 4096 - {system_prompt_tokens} SP - {gateway_prompts_tokens}GPT - {memory_tokens} MT - {query_tokens} QT - {last_messages_tokens} LMT = {max_tokens} Tokens left. Used tokens: {4096 - max_tokens}\")\n\t    async with ctx.typing():\n\t        response = openai.ChatCompletion.create(\n", "            model=\"gpt-3.5-turbo\",\n\t            messages=messages,\n\t            max_tokens=max_tokens + 1200, # we took 96 off just before\n\t            temperature=config_temp,  # High temperature leads to a more creative response.\n\t            frequency_penalty=config_freq,  # High temperature leads to a more creative response.\n\t            presence_penalty=config_pres,  # High temperature leads to a more creative response.\n\t        )\n\t        msg_response = response[\"choices\"][0][\"message\"][\"content\"]\n\t        msg_response = remove_phrases_from_string(remove_phrases, msg_response)\n\t        db.add_message(conversation_id, str(user.id), 'user', str(message.content))\n", "        db.add_message(conversation_id, str(client.user.id), 'assistant', str(msg_response))\n\t        status_msg = f\"{message.author.name}: {message.content}\"\n\t        # We should focus on user experience next and include user descriptions, pre config, etc.\n\t        vdb.upsert(secrets.token_hex(64 // 2), str({\"query\" : format_discord_tag(user) + \" : \" + str(message.content), \"response\": format_discord_tag(client.user) + \" : \" +str(msg_response), \"time\" : str(datetime.now().isoformat())}))\n\t    await message.channel.send(msg_response)\n\t    if (len(distant_shards) >=1):\n\t        memory_response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo\",\n\t            messages=memory_messages,\n\t            max_tokens=max_mem_tokens - 1, # we took 96 off just before\n", "            temperature=config_temp,  # High temperature leads to a more creative response.\n\t            frequency_penalty=config_freq,  # High temperature leads to a more creative response.\n\t            presence_penalty=config_pres,  # High temperature leads to a more creative response.\n\t        )\n\t        mem_response = memory_response[\"choices\"][0][\"message\"][\"content\"]\n\t        new_distant_shards = re.findall(r'<(.*?)>', mem_response)\n\t        new_distant_shards = [remove_links(s) for s in new_distant_shards]\n\t        print(\" --- MEM PROMPT --- \")\n\t        pprint.pprint(memory_messages)\n\t        print(\" --- END --- \")\n", "        print(\" --- MEM RESPONSE --- \")\n\t        pprint.pprint(new_distant_shards)\n\t        print(\" --- END --- \")\n\t        print(\" --- UPDATING DISTANT MEMORIES --- \")\n\t        ids = []\n\t        for memory in distant_memories:\n\t            ids.append(str(memory['id']))\n\t            db.delete_memory(memory['id'])\n\t            #maybe get IDs back to update database.\n\t        print(\"Deleting: \" + str(ids))\n", "        vdb.delete_vectors(ids)\n\t        vdb.upsert_texts(new_distant_shards)\n\t        print(\" --- END --- \")\n\t    else: \n\t        print(\" --- NO DISTANT MEMORIES RETRIEVED --- \")\n\tdef run_bot(token):\n\t    client.run(token)"]}
{"filename": "cute_assistant/core/tokens.py", "chunked_list": ["import tiktoken\n\tfrom tinydb import TinyDB, Query\n\t# Calculations based on the ChatGPT Wrapper https://github.com/mmabrouk/chatgpt-wrapper\n\t#TODO: Move this to the Nosql module\n\tdb_location = 'datastore/data.json'\n\tdb = TinyDB(db_location, indent=4, separators=(',', ': '))\n\tmessages_table = db.table('messages')\n\tdef format_memory(role, message):\n\t    user = \"Memory\"\n\t    msg = {\n", "        \"role\" : role,\n\t        \"content\" : f\"{user} : {str(message)}\"\n\t    }\n\t    return msg\n\t# From the wrapper\n\tdef get_num_tokens(messages):\n\t    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\t    num_tokens = 0\n\t    for message in messages:\n\t        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n", "        for key, value in message.items():\n\t            num_tokens += len(encoding.encode(value))\n\t            if key == \"name\":  # if there's a name, the role is omitted\n\t                num_tokens += -1  # role is always required and always 1 token\n\t    num_tokens += 2  # every reply is primed with <im_start>assistant\n\t    return num_tokens\n\tdef get_conversation_token_count(self, conversation_id=None):\n\t    conversation_id = conversation_id or self.conversation_id\n\t    success, old_messages, user_message = self.message.get_messages(conversation_id)\n\t    if not success:\n", "        raise Exception(user_message)\n\t    token_messages = self.prepare_prompt_messsage_context(old_messages)\n\t    tokens = self.get_num_tokens(token_messages)\n\t    return tokens\n\tdef get_messages_until_token_limit(conversation_id, max_tokens):\n\t    Message = Query()\n\t    messages_in_convo = messages_table.search(Message.conversation_id == conversation_id)\n\t    most_recent_messages = sorted(messages_in_convo, key=lambda convo: convo['created_time'])\n\t    tokens = 0\n\t    selected_messages = []\n", "    print(f\"Getting messages until token: {tokens}/{max_tokens}\")\n\t    for msg in reversed(most_recent_messages):\n\t        msg_tokens = get_num_tokens([{\"role\": str(msg[\"role\"]), \"content\": msg[\"message\"]}])\n\t        print(f\"Getting messages until token: {tokens}/{max_tokens}\")\n\t        if tokens + msg_tokens <= max_tokens:\n\t            tokens += msg_tokens\n\t            selected_messages.insert(0, msg)\n\t        else:\n\t            break\n\t    return selected_messages\n", "def get_memory_until_token_limit(chunk_memories, max_tokens, max_mem_tokens=3192):\n\t    relevant_memories = sorted(chunk_memories['results'][0]['results'], key=lambda memory: memory['score'])\n\t    tokens = 0\n\t    distant_tokens = 0\n\t    selected_memories = []\n\t    distant_memories = []\n\t    for mem in reversed(relevant_memories):\n\t        memory_message = [format_memory(\"user\", mem['text'])]\n\t        msg_tokens = get_num_tokens(memory_message)\n\t        if tokens + msg_tokens <= max_tokens:\n", "            tokens += msg_tokens\n\t            selected_memories.insert(0, mem)\n\t        elif distant_tokens + msg_tokens <= max_mem_tokens:\n\t            distant_tokens += msg_tokens\n\t            distant_memories.insert(0, mem)\n\t        else:\n\t            break\n\t    return selected_memories, distant_memories\n"]}
{"filename": "cute_assistant/core/discord_logging.py", "chunked_list": ["import logging\n\tfrom discord import Webhook\n\timport aiohttp\n\tclass DiscordLoggingHandler(logging.Handler):\n\t    def __init__(self, webhook_url):\n\t        super().__init__()\n\t        self.webhook_url = webhook_url\n\t    def emit(self, record):\n\t        try:\n\t            with aiohttp.ClientSession() as session:\n", "                webhook = Webhook.from_url(self.webhook_url, session=session)\n\t                webhook.send('Hello World')\n\t            webhook = Webhook.from_url(self.webhook_url)\n\t            log_entry = self.format(record)\n\t            webhook.send(log_entry)\n\t        except Exception as e:\n\t            print(f\"Error sending log to Discord: {e}\")"]}
{"filename": "cute_assistant/core/elevenlabs.py", "chunked_list": ["import os\n\timport requests\n\tfrom pydub import AudioSegment\n\tfrom pydub.playback import play\n\tfrom discord.ext import commands\n\tfrom google.cloud import speech_v1p1beta1 as speech\n\tfrom google.cloud.speech_v1p1beta1 import types\n\t# Set up the Google Cloud Speech-to-Text client\n\tos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/your/google-credentials.json\"\n\tspeech_client = speech.SpeechClient()\n", "# Initialize the Discord bot\n\tbot = commands.Bot(command_prefix=\"!\")\n\t@bot.command()\n\tasync def play_wav(ctx):\n\t    # Query the Elevenlabs API and save the WAV file\n\t    url = \"https://api.elevenlabs.com/path/to/your/endpoint\"\n\t    response = requests.get(url)\n\t    with open(\"audio.wav\", \"wb\") as f:\n\t        f.write(response.content)\n\t    # Play the WAV file in the voice channel\n", "    channel = ctx.author.voice.channel\n\t    if channel:\n\t        voice_client = await channel.connect()\n\t        source = discord.FFmpegPCMAudio(\"audio.wav\")\n\t        voice_client.play(source, after=lambda e: print(\"Finished playing\"))\n\t        while voice_client.is_playing():\n\t            pass\n\t        await voice_client.disconnect()\n\t    # Record the microphone input for speech-to-text\n\t    # The following code should be executed on the user's side and not within the bot's script\n", "    # Please refer to the note below for more details\n\t    \"\"\"\n\t    with open(\"microphone_input.wav\", \"rb\") as f:\n\t        content = f.read()\n\t    audio = types.RecognitionAudio(content=content)\n\t    config = types.RecognitionConfig(\n\t        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n\t        sample_rate_hertz=16000,\n\t        language_code=\"en-US\",\n\t    )\n", "    response = speech_client.recognize(config=config, audio=audio)\n\t    for result in response.results:\n\t        print(\"Transcript: {}\".format(result.alternatives[0].transcript))\n\t    \"\"\"\n\t@bot.event\n\tasync def on_ready():\n\t    print(f\"{bot.user.name} has connected to Discord!\")\n\t# Run the bot\n\tif __name__ == \"__main__\":\n\t    bot.run(os.environ[\"DISCORD_TOKEN\"])\n"]}
{"filename": "cute_assistant/core/save_file.py", "chunked_list": ["import os\n\timport json\n\tfrom discord.ext import commands\n\t# Initialize the Discord bot\n\tbot = commands.Bot(command_prefix=\"!\")\n\t# Function to save question-prompts to a JSON file\n\tdef save_to_json(question_prompts, filename=\"question_prompts.json\"):\n\t    with open(filename, \"w\") as f:\n\t        json.dump(question_prompts, f, indent=2)\n\t# Discord bot command to save question-prompts\n", "@bot.command()\n\tasync def save_questions(ctx, *questions):\n\t    question_prompts = {\"questions\": list(questions)}\n\t    save_to_json(question_prompts)\n\t    await ctx.send(f\"Questions saved to JSON file: {', '.join(questions)}\")\n\t@bot.event\n\tasync def on_ready():\n\t    print(f\"{bot.user.name} has connected to Discord!\")\n\t# Run the bot\n\tif __name__ == \"__main__\":\n", "    bot.run(os.environ[\"DISCORD_TOKEN\"])\n"]}
{"filename": "cute_assistant/core/reminder.py", "chunked_list": ["import os\n\timport sqlite3\n\timport asyncio\n\tfrom datetime import datetime\n\tfrom discord.ext import commands\n\t# Set up the SQLite database\n\tconn = sqlite3.connect(\"reminders.db\")\n\tcursor = conn.cursor()\n\tcursor.execute('''CREATE TABLE IF NOT EXISTS reminders (\n\t                    id INTEGER PRIMARY KEY,\n", "                    user_id INTEGER NOT NULL,\n\t                    reminder_text TEXT NOT NULL,\n\t                    reminder_time INTEGER NOT NULL\n\t                  );''')\n\tconn.commit()\n\t# Initialize the Discord bot\n\tbot = commands.Bot(command_prefix=\"!\")\n\tasync def create_reminder(user_id, reminder_text, reminder_time):\n\t    cursor.execute(\"INSERT INTO reminders (user_id, reminder_text, reminder_time) VALUES (?, ?, ?)\",\n\t                   (user_id, reminder_text, reminder_time))\n", "    conn.commit()\n\tasync def send_reminder(user_id, reminder_text):\n\t    user = await bot.fetch_user(user_id)\n\t    await user.send(f\"Reminder: {reminder_text}\")\n\t@bot.command()\n\tasync def remindme(ctx, unix_time: int, *, reminder_text: str):\n\t    user_id = ctx.author.id\n\t    reminder_time = unix_time\n\t    current_time = int(datetime.utcnow().timestamp())\n\t    if reminder_time < current_time:\n", "        await ctx.send(\"The specified time is in the past. Please provide a valid Unix timestamp.\")\n\t        return\n\t    await create_reminder(user_id, reminder_text, reminder_time)\n\t    await ctx.send(f\"Reminder set for {reminder_text} at Unix timestamp {reminder_time}\")\n\t    delay = reminder_time - current_time\n\t    await asyncio.sleep(delay)\n\t    await send_reminder(user_id, reminder_text)\n\t@bot.event\n\tasync def on_ready():\n\t    print(f\"{bot.user.name} has connected to Discord!\")\n", "# Run the bot\n\tif __name__ == \"__main__\":\n\t    bot.run(os.environ[\"DISCORD_TOKEN\"])\n"]}
{"filename": "cute_assistant/core/nosql_module.py", "chunked_list": ["from tinydb import TinyDB, Query\n\tfrom datetime import datetime\n\tdb_location = 'datastore/data.json'\n\tdb = None\n\tconversations_table = None\n\tmessages_table = None\n\tusers_table = None\n\tvdb_location = 'datastore/memories.json'\n\tvdb = None\n\t# Initialize the database\n", "db = TinyDB(db_location, indent=4, separators=(',', ': '))\n\tvdb = TinyDB(vdb_location, indent=4, separators=(',', ': '))\n\t# Tables for Conversations, Messages, and Users\n\tconversations_table = db.table('conversations')\n\tmessages_table = db.table('messages')\n\tusers_table = db.table('users')\n\tchannels_table = db.table(\"channel\")\n\tfiles_table = db.table('files')\n\t# Table for vdb information\n\tmemory_table = vdb.table('memories')\n", "User = Query()\n\tConversation = Query()\n\tMessage = Query()\n\tChannel = Query()\n\tMemory = Query()\n\tdef drop_tables():\n\t    messages_table.truncate()\n\t    conversations_table.truncate()\n\tdef create_channel(channel_id: int, allowed: bool = True):\n\t    channels_table.insert({\"channel_id\": channel_id, \"allowed\": allowed})\n", "def read_channel(channel_id: int):\n\t    return channels_table.search(Channel.channel_id == channel_id)\n\tdef update_channel(channel_id: int, allowed: bool):\n\t    channels_table.update({\"allowed\": allowed}, Channel.channel_id == channel_id)\n\tdef delete_channel(channel_id: int):\n\t    channels_table.remove(Channel.channel_id == channel_id)\n\tdef add_memory(memory_id: str, content: str):\n\t    memory_table.insert({\"memory_id\": memory_id, \"content\": content})\n\tdef delete_memory(memory_id: str):\n\t    memory_table.remove(Memory.memory_id == memory_id)\n", "def get_channel_type(channel_id: int):\n\t    channel = channels_table.search(Channel.channel_id == channel_id)\n\t    if not channel:\n\t        return \"None\"\n\t    return channel[0].get(\"type\", \"None\")\n\tdef set_channel_type(channel_id: int, type: str):\n\t    channels_table.update({\"type\": type}, Channel.channel_id == channel_id)\n\tdef get_channel_setting(channel_id: int, setting: str, default=\"None\"):\n\t    channel = channels_table.search(Channel.channel_id == channel_id)\n\t    if not channel:\n", "        return default\n\t    return channel[0].get(setting, default)\n\tdef set_channel_setting(channel_id: int, setting: str, value: str):\n\t    channels_table.update({setting: value}, Channel.channel_id == channel_id)\n\tdef save_file_content(file_name, content):\n\t    file_entry = {\n\t        'file_name': file_name,\n\t        'content': content\n\t    }\n\t    files_table.insert(file_entry)\n", "def get_all_channels():\n\t    return channels_table.all()\n\tdef is_channel_allowed(channel_id: int):\n\t    channel = channels_table.search(Channel.channel_id == channel_id)\n\t    if not channel:\n\t        return False\n\t    return channel[0][\"allowed\"]\n\tdef set_db_location(location):\n\t    global db_location, db, conversations_table, messages_table, users_table\n\t    db_location = location\n", "    db = TinyDB(db_location)\n\t    conversations_table = db.table('conversations')\n\t    messages_table = db.table('messages')\n\t    users_table = db.table('users')\n\tdef add_conversation(title :str, channel_id: str) -> int:\n\t    # Get the highest conversation ID (if exists) and increment it by 1\n\t    highest_convo_id = max(conversations_table.all(), key=lambda x: x['conversation_id'], default={'conversation_id': 0})['conversation_id']\n\t    new_convo_id = highest_convo_id + 1\n\t    conversations_table.insert({\n\t        'conversation_id': new_convo_id, \n", "        'title': title, \n\t        'created_time': datetime.now().isoformat(),\n\t        'channel_id': channel_id\n\t        })\n\t    return new_convo_id\n\tdef get_most_recent_conversation(channel_id: int):\n\t    # Define a query to filter conversations by channel_id\n\t    Convo = Query()\n\t    matching_conversations = conversations_table.search(Convo.channel_id == channel_id)\n\t    if not matching_conversations:\n", "        # no convos make a new one and return id\n\t        id = add_conversation(\"Convo name for now\", channel_id)\n\t        matching_conversations = conversations_table.search(Convo.channel_id == channel_id)\n\t    # Sort the conversations by created_time and return the most recent one\n\t    most_recent_convo = sorted(matching_conversations, key=lambda convo: convo['created_time'], reverse=True)[0]\n\t    return most_recent_convo\n\tdef add_user(name, uid, preference, nickname, discord_tag):\n\t    user = {\n\t        'name': name,\n\t        'uid': uid,\n", "        'preference': preference,\n\t        'nickname': nickname,\n\t        'discord_tag': discord_tag,\n\t    }\n\t    return users_table.insert(user)\n\tdef add_message(conversation_id, user_id, role, message):\n\t    message = {\n\t        'conversation_id': conversation_id,\n\t        'user_id': user_id,\n\t        'role': role,\n", "        'message': message,\n\t        'created_time': datetime.now().isoformat()\n\t    }\n\t    return messages_table.insert(message)\n\tdef get_messages_by_role(conversation_id, role):\n\t    Message = Query()\n\t    result = messages_table.search(\n\t        (Message.conversation_id == conversation_id) & (Message.role == role)\n\t    )\n\t    return result\n", "def get_last_messages_from_convo(conversation_id, numMessages):\n\t    Message = Query()\n\t    messages_in_convo = messages_table.search(Message.conversation_id == conversation_id)\n\t    most_recent_messages = sorted(messages_in_convo, key=lambda convo: convo['created_time'])\n\t    last_messages = most_recent_messages[-numMessages:] if len(most_recent_messages) > numMessages else most_recent_messages\n\t    return last_messages\n"]}
{"filename": "cute_assistant/core/database_utils.py", "chunked_list": ["from typing import Any, Dict\n\timport requests\n\timport os\n\timport shutil\n\timport pprint\n\timport secrets\n\t# Source: https://betterprogramming.pub/build-a-question-answering-app-using-pinecone-and-python-1d624c5818bf\n\tSEARCH_TOP_K = 10\n\tdb_bearer_token = None\n\tdef upsert_file(directory: str):\n", "    \"\"\"\n\t    Upload all files under a directory to the vector database.\n\t    \"\"\"\n\t    url = \"http://localhost:8000/upsert-file\"\n\t    headers = {\"Authorization\": \"Bearer \" + db_bearer_token}\n\t    files = []\n\t    for filename in os.listdir(directory):\n\t        if os.path.isfile(os.path.join(directory, filename)):\n\t            file_path = os.path.join(directory, filename)\n\t            with open(file_path, \"rb\") as f:\n", "                file_content = f.read()\n\t                files.append((\"file\", (filename, file_content, \"text/plain\")))\n\t            response = requests.post(url,\n\t                                     headers=headers,\n\t                                     files=files,\n\t                                     timeout=600)\n\t            if response.status_code == 200:\n\t                print(filename + \" uploaded successfully.\")\n\t            else:\n\t                print(\n", "                    f\"Error: {response.status_code} {response.content} for uploading \"\n\t                    + filename)\n\tdef upsert_texts(content: list):\n\t    \"\"\"\n\t    Upload one piece of text to the database.\n\t    \"\"\"\n\t    url = \"http://localhost:8000/upsert\"\n\t    headers = {\n\t        \"accept\": \"application/json\",\n\t        \"Content-Type\": \"application/json\",\n", "        \"Authorization\": \"Bearer \" + db_bearer_token,\n\t    }\n\t    documents = []\n\t    for itm in content:\n\t        documents.append(\n\t                {\n\t                \"id\": secrets.token_hex(64 // 2),\n\t                \"text\": itm,\n\t            }\n\t        )\n", "    data = {\n\t        \"documents\": documents\n\t    }\n\t    response = requests.post(url, json=data, headers=headers, timeout=600)\n\t    if response.status_code == 200:\n\t        print(\" --- UPLOAD --- \")\n\t        pprint.pprint(content)\n\t        print(\" --- END --- \")\n\t    else:\n\t        print(f\"Error: {response.status_code} {response.content}\")\n", "    return response.status_code\n\tdef upsert(id: str, content: str):\n\t    \"\"\"\n\t    Upload one piece of text to the database.\n\t    \"\"\"\n\t    url = \"http://localhost:8000/upsert\"\n\t    headers = {\n\t        \"accept\": \"application/json\",\n\t        \"Content-Type\": \"application/json\",\n\t        \"Authorization\": \"Bearer \" + db_bearer_token,\n", "    }\n\t    data = {\n\t        \"documents\": [{\n\t            \"id\": id,\n\t            \"text\": content,\n\t        }]\n\t    }\n\t    response = requests.post(url, json=data, headers=headers, timeout=600)\n\t    if response.status_code == 200:\n\t        print(\" --- UPLOAD --- \")\n", "        pprint.pprint(content)\n\t        print(\" --- END --- \")\n\t    else:\n\t        print(f\"Error: {response.status_code} {response.content}\")\n\t    return response.status_code\n\tdef query_database(query_prompt: str) -> Dict[str, Any]:\n\t    url = \"http://localhost:8000/query\"\n\t    headers = {\n\t        \"Content-Type\": \"application/json\",\n\t        \"accept\": \"application/json\",\n", "        \"Authorization\": f\"Bearer {db_bearer_token}\",\n\t    }\n\t    data = {\"queries\": [{\"query\": query_prompt, \"top_k\": SEARCH_TOP_K}]}\n\t    response = requests.post(url, json=data, headers=headers, timeout=600)\n\t    if response.status_code == 200:\n\t        result = response.json()\n\t        # process the result\n\t        return result\n\t    else:\n\t        raise ValueError(f\"Error: {response.status_code} : {response.content}\")\n", "def delete_vectors(ids):\n\t    url = \"http://localhost:8000/delete\"\n\t    headers = {\n\t        \"accept\": \"application/json\",\n\t        \"Content-Type\": \"application/json\",\n\t        \"Authorization\": f\"Bearer {db_bearer_token}\",\n\t    }\n\t    data = {\n\t        \"ids\" : ids,\n\t        \"deleteAll\": \"false\"\n", "        }\n\t    response = requests.delete(url, json=data, headers=headers, timeout=600)\n\t    if response.status_code == 200:\n\t        result = response.json()\n\t        # process the result\n\t        return result\n\t    else:\n\t        raise ValueError(f\"Error: {response.status_code} : {response.content}\")\n\tif __name__ == \"__main__\":\n\t    upsert_file(\"datastore/to_be_upserted\")\n", "    source_dir = 'datastore/to_be_upserted'\n\t    destination_dir = 'datastore/upserted_files'\n\t    for filename in os.listdir(source_dir):\n\t        source_file = os.path.join(source_dir, filename)\n\t        destination_file = os.path.join(destination_dir, filename)\n\t        shutil.move(source_file, destination_file)\n"]}
{"filename": "cute_assistant/core/chat.py", "chunked_list": ["from typing import Any, List, Dict\n\timport openai\n\tfrom cute_assistant.core.log import cute_logger as logger\n\tfrom cute_assistant.core.database_utils import query_database\n\t# Convert this into gated and pre-prompts\n\tdef apply_prompt_template(question: str) -> str:\n\t    prompt = f\"\"\"\n\t        By considering above input from me, answer the question: {question}\n\t    \"\"\"\n\t    return prompt\n", "def call_chatgpt_api(user_question: str, chunks: List[str]) -> Dict[str, Any]:\n\t    # Send a request to the GPT-3 API\n\t    messages = list(\n\t        map(lambda chunk: {\n\t            \"role\": \"user\",\n\t            \"content\": chunk\n\t        }, chunks))\n\t    question = apply_prompt_template(user_question)\n\t    messages.append({\"role\": \"user\", \"content\": question})\n\t    response = openai.ChatCompletion.create(\n", "        model=\"gpt-3.5-turbo\",\n\t        messages=messages,\n\t        max_tokens=512,\n\t        temperature=0.7,  # High temperature leads to a more creative response.\n\t    )\n\t    return response\n\tdef ask(user_question: str) -> Dict[str, Any]:\n\t    # Get chunks from database.\n\t    chunks_response = query_database(user_question)\n\t    chunks = []\n", "    for result in chunks_response[\"results\"]:\n\t        for inner_result in result[\"results\"]:\n\t            chunks.append(inner_result[\"text\"])\n\t    logger(\"chat\").info(\"User's questions: %s\", user_question)\n\t    logger(\"chat\").info(\"Retrieved chunks: %s\", chunks)\n\t    response = call_chatgpt_api(user_question, chunks)\n\t    logger(\"chat\").info(\"Response: %s\", response)\n\t    return response[\"choices\"][0][\"message\"][\"content\"]"]}
{"filename": "cute_assistant/core/request.py", "chunked_list": ["def assemble_openai_request(user_messages, system_prompt, additional_prompts, i):\n\t    # Insert the system prompt and additional prompts after the ith most recent message\n\t    assembled_messages = user_messages.copy()\n\t    assembled_messages.insert(-i, system_prompt)\n\t    assembled_messages.insert(-i, additional_prompts)\n\t    # Assemble the messages into a single string to send to the OpenAI API\n\t    assembled_request = \" \".join(assembled_messages)\n\t    return assembled_request\n\t# Example usage\n\tuser_messages = [\n", "    \"User: Hi\",\n\t    \"User: How are you?\",\n\t    \"User: Can you help me?\",\n\t    \"User: Thanks!\",\n\t]\n\tsystem_prompt = \"System: I'm here to help!\"\n\tadditional_prompts = \"System: What do you need assistance with?\"\n\t# Insert the system and additional prompts after the 2nd most recent message\n\tassembled_request = assemble_openai_request(user_messages, system_prompt, additional_prompts, 2)\n\tprint(assembled_request)\n"]}
