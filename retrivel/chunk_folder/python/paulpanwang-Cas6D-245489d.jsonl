{"filename": "predict.py", "chunked_list": ["import argparse\n\timport subprocess\n\tfrom pathlib import Path\n\timport numpy as np\n\tfrom skimage.io import imsave, imread\n\tfrom tqdm import tqdm\n\tfrom dataset.database import parse_database_name, get_ref_point_cloud\n\tfrom estimator import name2estimator\n\tfrom eval import visualize_intermediate_results\n\tfrom prepare import video2image\n", "from utils.base_utils import load_cfg, project_points\n\tfrom utils.draw_utils import pts_range_to_bbox_pts, draw_bbox_3d\n\tfrom utils.pose_utils import pnp\n\tdef weighted_pts(pts_list, weight_num=10, std_inv=10):\n\t    weights=np.exp(-(np.arange(weight_num)/std_inv)**2)[::-1] # wn\n\t    pose_num=len(pts_list)\n\t    if pose_num<weight_num:\n\t        weights = weights[-pose_num:]\n\t    else:\n\t        pts_list = pts_list[-weight_num:]\n", "    pts = np.sum(np.asarray(pts_list) * weights[:,None,None],0)/np.sum(weights)\n\t    return pts\n\tdef main(args):\n\t    cfg = load_cfg(args.cfg)\n\t    ref_database = parse_database_name(args.database)\n\t    estimator = name2estimator[cfg['type']](cfg)\n\t    estimator.build(ref_database, split_type='all')\n\t    object_pts = get_ref_point_cloud(ref_database)\n\t    object_bbox_3d = pts_range_to_bbox_pts(np.max(object_pts,0), np.min(object_pts,0))\n\t    output_dir = Path(args.output)\n", "    output_dir.mkdir(exist_ok=True, parents=True)\n\t    (output_dir / 'images_raw').mkdir(exist_ok=True, parents=True)\n\t    (output_dir / 'images_out').mkdir(exist_ok=True, parents=True)\n\t    (output_dir / 'images_inter').mkdir(exist_ok=True, parents=True)\n\t    (output_dir / 'images_out_smooth').mkdir(exist_ok=True, parents=True)\n\t    que_num = video2image(args.video, output_dir/'images_raw', 1, args.resolution, args.transpose)\n\t    pose_init = None\n\t    hist_pts = []\n\t    for que_id in tqdm(range(que_num)):\n\t        img = imread(str(output_dir/'images_raw'/f'frame{que_id}.jpg'))\n", "        # generate a pseudo K\n\t        h, w, _ = img.shape\n\t        f=np.sqrt(h**2+w**2)\n\t        K = np.asarray([[f,0,w/2],[0,f,h/2],[0,0,1]],np.float32)\n\t        if pose_init is not None:\n\t            estimator.cfg['refine_iter'] = 1 # we only refine one time after initialization\n\t        pose_pr, inter_results = estimator.predict(img, K, pose_init=pose_init)\n\t        pose_init = pose_pr\n\t        pts, _ = project_points(object_bbox_3d, pose_pr, K)\n\t        bbox_img = draw_bbox_3d(img, pts, (0,0,255))\n", "        imsave(f'{str(output_dir)}/images_out/{que_id}-bbox.jpg', bbox_img)\n\t        np.save(f'{str(output_dir)}/images_out/{que_id}-pose.npy', pose_pr)\n\t        imsave(f'{str(output_dir)}/images_inter/{que_id}.jpg', visualize_intermediate_results(img, K, inter_results, estimator.ref_info, object_bbox_3d))\n\t        hist_pts.append(pts)\n\t        pts_ = weighted_pts(hist_pts, weight_num=args.num, std_inv=args.std)\n\t        pose_ = pnp(object_bbox_3d, pts_, K)\n\t        pts__, _ = project_points(object_bbox_3d, pose_, K)\n\t        bbox_img_ = draw_bbox_3d(img, pts__, (0,0,255))\n\t        imsave(f'{str(output_dir)}/images_out_smooth/{que_id}-bbox.jpg', bbox_img_)\n\t    cmd=[args.ffmpeg, '-y', '-framerate','30', '-r', '30',\n", "         '-i', f'{output_dir}/images_out_smooth/%d-bbox.jpg',\n\t         '-c:v', 'libx264','-pix_fmt','yuv420p', f'{output_dir}/video.mp4']\n\t    subprocess.run(cmd)\n\tif __name__==\"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--cfg', type=str, default='configs/gen6d_pretrain.yaml')\n\t    parser.add_argument('--database', type=str, default=\"custom/mouse\")\n\t    parser.add_argument('--output', type=str, default=\"data/custom/mouse/test\")\n\t    # input video process\n\t    parser.add_argument('--video', type=str, default=\"data/custom/video/mouse-test.mp4\")\n", "    parser.add_argument('--resolution', type=int, default=960)\n\t    parser.add_argument('--transpose', action='store_true', dest='transpose', default=False)\n\t    # smooth poses\n\t    parser.add_argument('--num', type=int, default=5)\n\t    parser.add_argument('--std', type=float, default=2.5)\n\t    parser.add_argument('--ffmpeg', type=str, default='ffmpeg')\n\t    args = parser.parse_args()\n\t    main(args)"]}
{"filename": "colmap_script.py", "chunked_list": ["import logging\n\timport subprocess\n\timport os\n\tfrom pathlib import Path\n\timport numpy as np\n\tfrom skimage.io import imsave\n\tfrom dataset.database import BaseDatabase, get_database_split\n\tfrom utils.colmap_database import COLMAPDatabase\n\tfrom utils.read_write_model import CAMERA_MODEL_NAMES\n\tdef run_sfm(colmap_path, model_path, database_path, image_dir):\n", "    logging.info('Running the triangulation...')\n\t    model_path.mkdir(exist_ok=True, parents=True)\n\t    cmd = [\n\t        str(colmap_path), 'mapper',\n\t        '--database_path', str(database_path),\n\t        '--image_path', str(image_dir),\n\t        '--output_path', str(model_path),\n\t    ]\n\t    logging.info(' '.join(cmd))\n\t    subprocess.run(cmd, check=True)\n", "def run_patch_match(colmap_path, sparse_model: Path, image_dir: Path, dense_model: Path):\n\t    logging.info('Running patch match...')\n\t    assert sparse_model.exists()\n\t    dense_model.mkdir(parents=True, exist_ok=True)\n\t    cmd = [str(colmap_path), 'image_undistorter', '--input_path', str(sparse_model), '--image_path', str(image_dir), '--output_path', str(dense_model),]\n\t    logging.info(' '.join(cmd))\n\t    subprocess.run(cmd, check=True)\n\t    cmd = [str(colmap_path), 'patch_match_stereo','--workspace_path', str(dense_model),]\n\t    logging.info(' '.join(cmd))\n\t    subprocess.run(cmd, check=True)\n", "def run_depth_fusion(colmap_path, dense_model: Path, ply_path: Path):\n\t    logging.info('Running patch match...')\n\t    dense_model.mkdir(parents=True, exist_ok=True)\n\t    cmd = [str(colmap_path), 'stereo_fusion',\n\t           '--workspace_path', str(dense_model),\n\t           '--workspace_format', 'COLMAP',\n\t           '--input_type', 'geometric',\n\t           '--output_path', str(ply_path),]\n\t    logging.info(' '.join(cmd))\n\t    subprocess.run(cmd, check=True)\n", "def dump_images(database, ref_ids, image_path: Path):\n\t    image_path.mkdir(parents=True, exist_ok=True)\n\t    for ref_id in ref_ids:\n\t        if (image_path / f'{ref_id}.jpg').exists():\n\t            continue\n\t        else:\n\t            imsave(str(image_path / f'{ref_id}.jpg'),database.get_image(ref_id))\n\tdef extract_and_match_sift(colmap_path, database_path, image_dir):\n\t    cmd = [\n\t        str(colmap_path), 'feature_extractor',\n", "        '--database_path', str(database_path),\n\t        '--image_path', str(image_dir),\n\t    ]\n\t    logging.info(' '.join(cmd))\n\t    subprocess.run(cmd, check=True)\n\t    cmd = [\n\t        str(colmap_path), 'exhaustive_matcher',\n\t        '--database_path', str(database_path),\n\t    ]\n\t    logging.info(' '.join(cmd))\n", "    subprocess.run(cmd, check=True)\n\tdef create_db_from_database(database, ref_ids, database_path: Path):\n\t    if database_path.exists():\n\t        logging.warning('Database already exists. we will skip db creation.')\n\t        return\n\t    db = COLMAPDatabase.connect(database_path)\n\t    db.create_tables()\n\t    for ri, ref_id in enumerate(ref_ids):\n\t        img = database.get_image(ref_id)\n\t        h, w = img.shape[:2]\n", "        model_id = CAMERA_MODEL_NAMES[\"SIMPLE_RADIAL\"].model_id\n\t        db.add_camera(model_id, float(w), float(h), np.asarray([np.sqrt(h**2+w**2), w/2.0, h/2.0, 0.0],np.float64), camera_id=ri+1)\n\t        db.add_image(f'{ref_id}.jpg', ri+1, image_id=ri+1)\n\t    db.commit()\n\t    db.close()\n\tdef build_colmap_model_no_pose(database: BaseDatabase, colmap_path='colmap'):\n\t    colmap_root = Path('data') / database.database_name / 'colmap'\n\t    colmap_root.mkdir(exist_ok=True, parents=True)\n\t    image_path = colmap_root / 'images'\n\t    database_path = colmap_root / 'database.db'\n", "    ref_ids, _ = get_database_split(database, 'all')\n\t    dump_images(database, ref_ids, image_path)\n\t    create_db_from_database(database, ref_ids, database_path)\n\t    extract_and_match_sift(colmap_path, database_path, image_path)\n\t    sparse_model_path = colmap_root / f'sparse'\n\t    dense_model_path = colmap_root / f'dense'\n\t    ply_path = colmap_root / f'pointcloud.ply'\n\t    run_sfm(colmap_path, sparse_model_path, database_path, image_path)\n\t    run_patch_match(colmap_path, sparse_model_path / '0', image_path, dense_model_path)\n\t    run_depth_fusion(colmap_path, dense_model_path, ply_path)\n", "def clean_colmap_project(database, split_name):\n\t    extractor_name = 'colmap_default'\n\t    matcher_name = 'colmap_default'\n\t    colmap_root = Path('data/colmap_projects') / database.database_name / f'colmap-{split_name}' / f'{extractor_name}-{matcher_name}'\n\t    image_path = colmap_root / 'images'\n\t    database_path = colmap_root / 'database.db'\n\t    empty_model_path = colmap_root / 'empty'\n\t    sparse_model_path = colmap_root / f'sparse'\n\t    dense_model_path = colmap_root / f'dense'\n\t    os.system(f'rm {str(sparse_model_path)} -r')\n", "    os.system(f'rm {str(database_path)} -r')\n\t    os.system(f'rm {str(image_path)} -r')\n\t    os.system(f'rm {str(empty_model_path)} -r')\n\t    os.system(f'rm {str(dense_model_path / \"images\")} -r')\n\t    os.system(f'rm {str(dense_model_path / \"sparse\")} -r')\n\t    os.system(f'rm {str(dense_model_path / \"stereo\" / \"normal_maps\")} -r')\n\t    os.system(f'rm {str(dense_model_path / \"stereo\" / \"depth_maps\")}/*.photometric.bin')\n"]}
{"filename": "prepare.py", "chunked_list": ["import argparse\n\tfrom pathlib import Path\n\timport cv2\n\timport torch\n\tfrom skimage.io import imsave\n\tfrom tqdm import tqdm\n\tfrom colmap_script import build_colmap_model_no_pose\n\tfrom dataset.database import parse_database_name, get_database_split\n\tfrom estimator import Gen6DEstimator\n\tfrom network import name2network\n", "from utils.base_utils import load_cfg, save_pickle\n\tdef video2image(input_video, output_dir, interval=30, image_size = 640, transpose=False):\n\t    print(f'split video {input_video} into images ...')\n\t    Path(output_dir).mkdir(parents=True, exist_ok=True)\n\t    vidcap = cv2.VideoCapture(input_video)\n\t    success, image = vidcap.read()\n\t    count = 0\n\t    while success:\n\t        if count % interval==0:\n\t            h, w = image.shape[:2]\n", "            ratio = image_size/max(h,w)\n\t            ht, wt = int(ratio*h), int(ratio*w)\n\t            image = cv2.resize(image,(wt,ht),interpolation=cv2.INTER_LINEAR)\n\t            if transpose:\n\t                v0 = cv2.getVersionMajor()\n\t                v1 = cv2.getVersionMinor()\n\t                if v0>=4 and v1>=5:\n\t                    image = cv2.flip(image, 0)\n\t                    image = cv2.flip(image, 1)\n\t                else:\n", "                    image = cv2.transpose(image)\n\t                    image = cv2.flip(image, 1)\n\t            image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n\t            imsave(f\"{output_dir}/frame%d.jpg\" % count, image)  # save frame as JPEG file\n\t        success, image = vidcap.read()\n\t        count += 1\n\t    return count\n\tdef prepare_validation_set(ref_database_name, que_database_name, ref_split, que_split, estimator_cfg):\n\t    ref_database = parse_database_name(ref_database_name)\n\t    que_database = parse_database_name(que_database_name)\n", "    _, que_ids = get_database_split(que_database, que_split)\n\t    estimator_cfg = load_cfg(estimator_cfg)\n\t    estimator_cfg['refiner']=None\n\t    estimator = Gen6DEstimator(estimator_cfg)\n\t    estimator.build(ref_database, split_type=ref_split)\n\t    img_id2det_info, img_id2sel_info = {}, {}\n\t    for que_id in tqdm(que_ids):\n\t        # estimate pose\n\t        img = que_database.get_image(que_id)\n\t        K = que_database.get_K(que_id)\n", "        _, inter_results = estimator.predict(img, K)\n\t        det_scale_r2q = inter_results['det_scale_r2q']\n\t        det_position = inter_results['det_position']\n\t        self_angle_r2q = inter_results['sel_angle_r2q']\n\t        ref_idx = inter_results['sel_ref_idx']\n\t        ref_pose = estimator.ref_info['poses'][ref_idx]\n\t        ref_K = estimator.ref_info['Ks'][ref_idx]\n\t        img_id2det_info[que_id]=(det_position, det_scale_r2q, 0)\n\t        img_id2sel_info[que_id]=(self_angle_r2q, ref_pose, ref_K)\n\t    save_pickle(img_id2det_info,f'data/val/det/{que_database_name}/{estimator.detector.cfg[\"name\"]}.pkl')\n", "    save_pickle(img_id2sel_info,f'data/val/sel/{que_database_name}/{estimator.detector.cfg[\"name\"]}-{estimator.selector.cfg[\"name\"]}.pkl')\n\tif __name__==\"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--action', type=str, required=True)\n\t    # for video2image\n\t    parser.add_argument('--input', type=str, default='example/video/mouse-ref.mp4')\n\t    parser.add_argument('--output', type=str, default='example/mouse/images')\n\t    parser.add_argument('--frame_inter', type=int, default=10)\n\t    parser.add_argument('--image_size', type=int, default=960)\n\t    parser.add_argument('--transpose', action='store_true', dest='transpose', default=False)\n", "    # for sfm\n\t    parser.add_argument('--database_name', type=str, default='example/mouse')\n\t    parser.add_argument('--colmap_path', type=str, default='colmap')\n\t    # for sfm\n\t    parser.add_argument('--que_database', type=str, default='linemod/cat')\n\t    parser.add_argument('--que_split', type=str, default='linemod_test')\n\t    parser.add_argument('--ref_database', type=str, default='linemod/cat')\n\t    parser.add_argument('--ref_split', type=str, default='linemod_test')\n\t    parser.add_argument('--estimator_cfg', type=str, default='configs/gen6d_train.yaml')\n\t    args = parser.parse_args()\n", "    if args.action == 'video2image':\n\t        video2image(args.input,args.output,args.frame_inter,args.image_size, args.transpose)\n\t    elif args.action=='sfm':\n\t        build_colmap_model_no_pose(parse_database_name(args.database_name),args.colmap_path)\n\t    elif args.action=='gen_val_set':\n\t        prepare_validation_set(args.ref_database,args.que_database,args.ref_split,args.que_split,args.estimator_cfg)\n\t    else:\n\t        raise NotImplementedError"]}
{"filename": "modelsize_estimate.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport numpy as np\n\tdef modelsize(model, input, type_size=4):\n\t    para = sum([np.prod(list(p.size())) for p in model.parameters()])\n\t    # print('Model {} : Number of params: {}'.format(model._get_name(), para))\n\t    print('Model {} : params: {:4f}M'.format(model._get_name(), para * type_size / 1000 / 1000))\n\t    input_ = input.clone()\n\t    input_.requires_grad_(requires_grad=False)\n\t    mods = list(model.modules())\n", "    out_sizes = []\n\t    for i in range(1, len(mods)):\n\t        m = mods[i]\n\t        if isinstance(m, nn.ReLU):\n\t            if m.inplace:\n\t                continue\n\t        out = m(input_)\n\t        out_sizes.append(np.array(out.size()))\n\t        input_ = out\n\t    total_nums = 0\n", "    for i in range(len(out_sizes)):\n\t        s = out_sizes[i]\n\t        nums = np.prod(np.array(s))\n\t        total_nums += nums\n\t    print('Model {} : intermedite variables: {:3f} M (without backward)'\n\t          .format(model._get_name(), total_nums * type_size / 1000 / 1000))\n\t    print('Model {} : intermedite variables: {:3f} M (with backward)'\n\t          .format(model._get_name(), total_nums * type_size*2 / 1000 / 1000))\n"]}
{"filename": "eval.py", "chunked_list": ["# run --type v100-32g --cpu 30 --memory 150 -- python3 eval.py\n\timport argparse\n\tfrom copy import copy\n\tfrom pathlib import Path\n\timport numpy as np\n\tfrom skimage.io import imsave\n\timport cv2\n\tfrom tqdm import tqdm\n\tfrom dataset.database import parse_database_name, get_database_split, get_ref_point_cloud, get_diameter, get_object_center\n\tfrom estimator import name2estimator\n", "from utils.base_utils import load_cfg, save_pickle, read_pickle, project_points, transformation_crop\n\tfrom utils.database_utils import compute_normalized_view_correlation\n\tfrom utils.draw_utils import draw_bbox, concat_images_list, draw_bbox_3d, pts_range_to_bbox_pts\n\tfrom utils.pose_utils import compute_metrics_impl, scale_rotation_difference_from_cameras\n\tfrom utils.pose_utils import compute_pose_errors\n\tfrom tabulate import tabulate\n\tfrom loguru import logger\n\tdef get_gt_info(que_pose, que_K, render_poses, render_Ks, object_center):\n\t    gt_corr = compute_normalized_view_correlation(que_pose[None], render_poses, object_center, False)\n\t    gt_ref_idx = np.argmax(gt_corr[0])\n", "    gt_scale_r2q, gt_angle_r2q = scale_rotation_difference_from_cameras(\n\t        render_poses[gt_ref_idx][None], que_pose[None], render_Ks[gt_ref_idx][None], que_K[None], object_center)\n\t    gt_scale_r2q, gt_angle_r2q = gt_scale_r2q[0], gt_angle_r2q[0]\n\t    gt_position = project_points(object_center[None], que_pose, que_K)[0][0]\n\t    size = 128\n\t    gt_bbox = np.concatenate([gt_position - size / 2 * gt_scale_r2q, np.full(2, size) * gt_scale_r2q])\n\t    return gt_position, gt_scale_r2q, gt_angle_r2q, gt_ref_idx, gt_bbox, gt_corr[0]\n\tdef visualize_intermediate_results(object_pts, object_diameter , img, K, inter_results, ref_info, object_bbox_3d, \\\n\t                                    object_center=None, pose_gt=None, est_name=\"\",object_name=\"\", que_id=-1 ):\n\t    ref_imgs = ref_info['ref_imgs']  \n", "    if pose_gt is not None:\n\t        gt_position, gt_scale_r2q, gt_angle_r2q, gt_ref_idx, gt_bbox, gt_scores = \\\n\t            get_gt_info(pose_gt, K, ref_info['poses'], ref_info['Ks'], object_center)\n\t    img_h , img_w , _ = img.shape\n\t    output_imgs = []\n\t    pts2d_gt, _ = project_points(object_pts, pose_gt, K)\n\t    x0, y0, w, h = cv2.boundingRect(pts2d_gt.astype(np.int32))\n\t    img_h, img_w, c = img.shape\n\t    max_r = max(w,h)\n\t    x1, y1 = min(x0 + 1.5*max_r,img_w), min(y0 + 1.5*max_r,img_h)\n", "    x0, y0 = max(x0 - 0.5*max_r, 0), max(y0 - 0.5*max_r, 0)\n\t    if 'det_scale_r2q' in inter_results and 'sel_angle_r2q' in inter_results:\n\t        det_scale_r2q = inter_results['det_scale_r2q']\n\t        det_position = inter_results['det_position']\n\t        det_que_img = inter_results['det_que_img']\n\t        size = det_que_img.shape[0]\n\t        pr_bbox = np.concatenate([det_position - size / 2 * det_scale_r2q, np.full(2, size) * det_scale_r2q])\n\t        pr_bbox[0] =  int(pr_bbox[0] )\n\t        pr_bbox[1] =  int(pr_bbox[1] )\n\t        pr_bbox[2] =  int(pr_bbox[2])\n", "        pr_bbox[3] =  int(pr_bbox[3])\n\t        max_r = max(pr_bbox[2], pr_bbox[3] )\n\t        bbox_img = img\n\t        bbox_img = draw_bbox(bbox_img, pr_bbox, color=(0, 0, 255))\n\t        x0,y0,x1,y1 = pr_bbox[0], pr_bbox[1],  pr_bbox[0] + pr_bbox[2],  pr_bbox[1] +  pr_bbox[3]\n\t        x1, y1 = int(min(x0 + 1.5*max_r,img_w)), int(min(y0 + 1.5*max_r,img_h))\n\t        x0, y0 = int(max(x0 - 0.5*max_r, 0)), int(max(y0 - 0.5*max_r, 0))\n\t        if pose_gt is not None: bbox_img = draw_bbox(bbox_img, gt_bbox, color=(0, 255, 0))\n\t        crop_img = bbox_img[y0:y1, x0:x1,:]\n\t        imsave(f'data/vis_final/{est_name}/{object_name}/{que_id}-bbox2d.jpg',bbox_img )\n", "        imsave(f'data/vis_final/{est_name}/{object_name}/{que_id}-bbox2d-crop.jpg', cv2.resize(crop_img, (512, 512)) )   \n\t        output_imgs.append(bbox_img)\n\t        # visualize selection\n\t        sel_angle_r2q = inter_results['sel_angle_r2q']  #\n\t        sel_scores = inter_results['sel_scores']  #\n\t        h, w, _ = det_que_img.shape\n\t        sel_img_rot, _ = transformation_crop(det_que_img, np.asarray([w / 2, h / 2], np.float32), 1.0, -sel_angle_r2q, h)\n\t        an = ref_imgs.shape[0]\n\t        sel_img = concat_images_list(det_que_img, sel_img_rot, *[ref_imgs[an // 2, score_idx] for score_idx in np.argsort(-sel_scores)[:5]], vert=True)\n\t        if pose_gt is not None:\n", "            sel_img_rot_gt, _ = transformation_crop(det_que_img, np.asarray([w/2, h/2], np.float32), 1.0, -gt_angle_r2q, h)\n\t            sel_img_gt = concat_images_list(det_que_img, sel_img_rot_gt, *[ref_imgs[an // 2, score_idx] for score_idx in np.argsort(-gt_scores)[:5]], vert=True)\n\t            sel_img = concat_images_list(sel_img, sel_img_gt)\n\t        output_imgs.append(sel_img)\n\t    # visualize refinements\n\t    refine_poses = inter_results['refine_poses'] if 'refine_poses' in inter_results else []\n\t    refine_imgs = []\n\t    # refine pose 打印出来\n\t    for k in range(1,len(refine_poses)):\n\t        pose_in, pose_out = refine_poses[k-1], refine_poses[k]\n", "        bbox_pts_in, _ = project_points(object_bbox_3d, pose_in, K)\n\t        bbox_pts_out, _ = project_points(object_bbox_3d, pose_out, K)\n\t        prj_err, obj_err, pose_err = compute_pose_errors(object_pts, pose_out, pose_gt, K)\n\t        is_add01 = obj_err>0.1*object_diameter\n\t        img_render = img.copy()\n\t        if is_add01: # bgr\n\t            img_render = draw_bbox_3d(img_render, bbox_pts_out, (0, 0, 255) )\n\t        else:\n\t            img_render = draw_bbox_3d(img_render.copy(), bbox_pts_out, (0, 0, 255))\n\t        if pose_gt is not None:\n", "            bbox_pts_gt, _ = project_points(object_bbox_3d, pose_gt, K)\n\t            img_render = draw_bbox_3d(img_render, bbox_pts_gt, (0, 255, 0))\n\t        crop_img = img_render[y0:y1, x0:x1,:]\n\t        imsave(f'data/vis_final/{est_name}/{object_name}/{que_id}-refiner-{k}-crop.jpg', cv2.resize(crop_img, (512, 512)) )   \n\t        output_imgs.append(bbox_img)\n\t        refine_imgs.append(bbox_img)\n\t    if len(refine_poses)!=0:\n\t        output_imgs.append(concat_images_list(*refine_imgs))\n\t        # cv2.putText(output_imgs[-1], str(is_add01) , (0, 0), cv2.FONT_HERSHEY_SIMPLEX,1, (255, 255, 255), 2, cv2.LINE_AA)\n\t    return concat_images_list(*output_imgs)\n", "def visualize_final_poses(object_pts, object_diameter , img, K, object_bbox_3d, pose_pr, pose_gt=None):\n\t    bbox_pts_pr, _ = project_points(object_bbox_3d, pose_pr, K)\n\t    pts2d_pr, _ = project_points(object_pts, pose_pr, K)\n\t    prj_err, obj_err, pose_err = compute_pose_errors(object_pts, pose_pr, pose_gt, K)\n\t    bbox_img = img\n\t    point_size = 1\n\t    thickness = 1\n\t    pts2d_gt, _ = project_points(object_pts, pose_gt, K)\n\t    x0, y0, w, h = cv2.boundingRect(pts2d_gt.astype(np.int))\n\t    img_h, img_w, c = img.shape\n", "    max_r = max(w,h)\n\t    x1, y1 = min(x0 + 1.5*max_r,img_w), min(y0 + 1.5*max_r,img_h)\n\t    x0, y0 = max(x0 - 0.5*max_r, 0), max(y0 - 0.5*max_r, 0)\n\t    alpha = 0.5\n\t    if pose_gt is not None:\n\t        bbox_pts_gt, _ = project_points(object_bbox_3d, pose_gt, K)\n\t        bbox_img = draw_bbox_3d(bbox_img, bbox_pts_gt)\n\t    if obj_err>0.1*object_diameter:\n\t        bbox_img = draw_bbox_3d(bbox_img, bbox_pts_pr, (0, 0, 255))\n\t        return bbox_img,[x0,y0,x1,y1]\n", "    else:\n\t        bbox_img = draw_bbox_3d(bbox_img, bbox_pts_pr, (0, 0, 255))\n\t        return bbox_img,[x0,y0,x1,y1]\n\t    return None, None\n\t    return bbox_img\n\timport time\n\tdef main(args):\n\t    # estimator\n\t    cfg = load_cfg(args.cfg)\n\t    estimator = name2estimator[cfg['type']](cfg)\n", "    # object_name_list = [  \"linemod/cat\", \"linemod/duck\", \"linemod/benchvise\", \\\n\t    #                       \"linemod/cam\",  \"linemod/driller\", \\\n\t    #                       \"linemod/lamp\",  \"linemod/eggbox\", \"linemod/glue\"]\n\t    # object_name_list = [ \"linemod/cat\", \"linemod/duck\" ]\n\t    object_name_list = [\"genmop/chair\", \"genmop/plug_en\", \"genmop/piggy\",  \\\n\t                        \"genmop/scissors\", \"genmop/tformer\"]\n\t    t1 = time.time()\n\t    logger.debug(f\"use_gt_box2d:{ args.use_gt_box } , use_refiner:{ args.use_refiner }\")\n\t    metric_list = list()\n\t    add_list, proj5_list = list() , list()\n", "    for object_name in object_name_list:\n\t        if \"eggbox\" in object_name or  \"glue\" in object_name:\n\t            symmetric = True\n\t        else:\n\t            symmetric = False\n\t        if object_name.startswith('linemod'):\n\t            ref_database_name = que_database_name = object_name\n\t            que_split = 'linemod_test'\n\t        elif object_name.startswith('genmop'):\n\t            ref_database_name = object_name+'-ref'\n", "            que_database_name = object_name+'-test'\n\t            que_split = 'all'\n\t        else:\n\t            raise NotImplementedError\n\t        ref_database = parse_database_name(ref_database_name)\n\t        ref_split = que_split if args.split_type is None else args.split_type\n\t        estimator.build(ref_database, split_type=ref_split)\n\t        que_database = parse_database_name(que_database_name)\n\t        _, que_ids = get_database_split(que_database, que_split)\n\t        object_pts = get_ref_point_cloud(ref_database)\n", "        object_center = get_object_center(ref_database)\n\t        object_bbox_3d = pts_range_to_bbox_pts(np.max(object_pts,0), np.min(object_pts,0))\n\t        est_name = estimator.cfg[\"name\"] # + f'-{args.render_pose_name}'\n\t        est_name = est_name + args.split_type if args.split_type is not None else est_name\n\t        est_name = \"DEBUG\"\n\t        Path(f'data/eval/poses/{object_name}').mkdir(exist_ok=True,parents=True)\n\t        Path(f'data/vis_inter/{est_name}/{object_name}').mkdir(exist_ok=True,parents=True)\n\t        Path(f'data/vis_final/{est_name}/{object_name}').mkdir(exist_ok=True,parents=True)\n\t        # evaluation metrics\n\t        object_diameter = get_diameter(que_database)\n", "        if not args.eval_only:\n\t            pose_pr_list = []\n\t            new_que_ids = []\n\t            print(f\"obj number =  {len(que_ids)}\")\n\t            for idx, que_id in enumerate(tqdm(que_ids)):\n\t                new_que_ids.append(que_id)\n\t                # estimate pose\n\t                img = que_database.get_image(que_id)\n\t                K = que_database.get_K(que_id)\n\t                pose_gt = que_database.get_pose(que_id)\n", "                if args.use_gt_box:\n\t                    gt_position, gt_scale_r2q, gt_angle_r2q, gt_ref_idx, gt_bbox, gt_scores = \\\n\t                    get_gt_info(pose_gt, K, estimator.ref_info['poses'], estimator.ref_info['Ks'], object_center)\n\t                    pose_pr, inter_results = estimator.predict(img, K, position = gt_position, \\\n\t                                                                       scale_r2q = gt_scale_r2q, \\\n\t                                                                       need_refiner = args.use_refiner)\n\t                else:\n\t                    pose_pr, inter_results = estimator.predict(img, K, need_refiner = args.use_refiner)        \n\t                    pose_pr_list.append(pose_pr)\n\t                final_img, bbox2d = visualize_final_poses(object_pts , object_diameter, img, K, object_bbox_3d, pose_pr, pose_gt)                \n", "                if final_img is not None and visualize_final_poses is not None:\n\t                    x0, y0, x1, y1 = [int(x) for  x in bbox2d]\n\t                    crop_img = final_img[y0:y1, x0:x1,:]\n\t                    imsave(f'data/vis_final/{est_name}/{object_name}/{idx}-bbox3d.jpg', final_img)\n\t                    imsave(f'data/vis_final/{est_name}/{object_name}/{idx}-bbox3d-crop.jpg', \\\n\t                           cv2.resize(crop_img, (512, 512)) )   \n\t        pose_gt_list = [que_database.get_pose(que_id) for que_id in new_que_ids]\n\t        que_Ks = [que_database.get_K(que_id) for que_id  in new_que_ids]\n\t        def get_eval_msg(pose_in_list,msg_in,scale=1.0):\n\t            msg_in = copy(msg_in)\n", "            results = compute_metrics_impl(object_pts, object_diameter, pose_gt_list, pose_in_list, \\\n\t                                           que_Ks, scale, symmetric = symmetric)\n\t            for k, v in results.items(): msg_in+=f'{k} {v:.4f} '\n\t            return msg_in + '\\n', results\n\t        msg_pr = f'{object_name:10} {est_name:20} '\n\t        msg_pr, results = get_eval_msg(pose_pr_list, msg_pr)\n\t        add , prj5 = results['add-0.1d'], results['prj-5']\n\t        if symmetric:\n\t            add = results['add-0.1d-sym']\n\t        add_list.append(add), proj5_list.append(prj5)\n", "        print(object_name +  \": ,  add0.1:\", add , \" ,proj5:\", prj5 )\n\t        metric_list.append(  (object_name, add , prj5 )  )\n\t        with open('data/performance.log','a') as f: f.write(msg_pr)\n\t    print(\"avg add0.1:\", sum(add_list)/len(add_list) , \" , avg proj5:\", sum(proj5_list)/len(proj5_list))\n\t    metric_list.append(  (\"avg\", sum(add_list)/len(add_list) , sum(proj5_list)/len(proj5_list) )  )\n\t    print(tabulate(metric_list, headers=['objname', 'add0.1', 'proj5'],tablefmt='fancy_grid'))\n\t    t2 = time.time()\n\t    print(f\"[debug]: the exp costs {t2-t1} seconds\")\n\tif __name__==\"__main__\":\n\t    parser = argparse.ArgumentParser()\n", "    parser.add_argument('--cfg', type=str, default=\"configs/cas6d_train.yaml\")\n\t    parser.add_argument('--object_name', type=str, default='warrior')\n\t    parser.add_argument('--eval_only', action='store_true', dest='eval_only', default=False)\n\t    parser.add_argument('--symmetric', action='store_true', dest='symmetric', default=False)\n\t    parser.add_argument('--split_type', type=str, default=None)\n\t    parser.add_argument('--use_gt_box',action='store_true', dest='use_gt_box', default=False)\n\t    parser.add_argument('--use_refiner',action='store_true', dest='use_refiner', default=True)\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "estimator.py", "chunked_list": ["import cv2\n\timport numpy as np\n\timport torch\n\tfrom dataset.database import BaseDatabase, get_database_split, get_object_vert, get_object_center\n\tfrom network import name2network\n\tfrom utils.base_utils import load_cfg, transformation_offset_2d, transformation_scale_2d, \\\n\t    transformation_compose_2d, transformation_crop, transformation_rotation_2d\n\tfrom utils.database_utils import select_reference_img_ids_fps, normalize_reference_views\n\tfrom utils.pose_utils import estimate_pose_from_similarity_transform_compose\n\tfrom gpu_mem_track import MemTracker\n", "# gpu_tracker = MemTracker()\n\tstart = torch.cuda.Event(enable_timing=True)\n\tend = torch.cuda.Event(enable_timing=True)\n\timport time\n\tdef compute_similarity_transform(pts0, pts1):\n\t    \"\"\"\n\t    @param pts0:\n\t    @param pts1:\n\t    @return: sR @ p0 + t = p1\n\t    \"\"\"\n", "    ref_c = np.mean(pts0, 0)\n\t    que_c = np.mean(pts1, 0)\n\t    ref_d = pts0 - ref_c[None, :]\n\t    que_d = pts1 - que_c[None, :]\n\t    scale = np.mean(np.linalg.norm(que_d,2,1)) / np.mean(np.linalg.norm(ref_d,2,1))\n\t    ref_d_ = ref_d * scale\n\t    U, S, VT = np.linalg.svd(ref_d_.T @ que_d)\n\t    rotation = VT.T @ U.T\n\t    offset = - scale * (rotation @ ref_c) + que_c\n\t    return scale, rotation, offset\n", "def compute_similarity_transform_batch(pts0, pts1):\n\t    \"\"\"\n\t    @param pts0:\n\t    @param pts1:\n\t    @return: sR @ p0 + t = p1\n\t    \"\"\"\n\t    c0 = np.mean(pts0, 1) # n, 2\n\t    c1 = np.mean(pts1, 1) # n, 2\n\t    d0 = pts0 - c0[:, None, :]\n\t    d1 = pts1 - c1[:, None, :]\n", "    scale = np.mean(np.linalg.norm(d1,2,2,keepdims=True),1,keepdims=True) / \\\n\t            np.mean(np.linalg.norm(d0,2,2,keepdims=True),1,keepdims=True) # n,1,1\n\t    d0_ = d0 * scale # n,k,2\n\t    U, S, VT = np.linalg.svd(d0_.transpose([0,2,1]) @ d1) # n,2,2\n\t    rotation = VT.transpose([0,2,1]) @ U.transpose([0,2,1]) # n,2,2\n\t    offset = - scale * (rotation @ c0[:,:,None]) + c1[:,:,None]\n\t    return scale, rotation, offset # [n,1,1] [n,2,2] [n,2,1]\n\tdef compute_inlier_mask(scale, rotation, offset, corr, thresh):\n\t    x0=corr[None, :, :2] # [1,k,2]\n\t    x1=corr[None, :, 2:] # [1,k,2]\n", "    x1_ = scale * (x0 @ rotation.transpose([0,2,1])) + offset.transpose([0,2,1])\n\t    mask = np.linalg.norm(x1-x1_,2,2) < thresh\n\t    return mask\n\tdef ransac_similarity_transform(corr):\n\t    n, _ = corr.shape\n\t    batch_size=4096\n\t    bad_seed_thresh=4\n\t    inlier_thresh=5\n\t    best_inlier, best_mask = 0, None\n\t    iter_num = 0\n", "    confidence = 0.99\n\t    while True:\n\t        idx = np.random.randint(0,n,[batch_size,2])\n\t        seed0 = corr[idx[:,0]] # b,4\n\t        seed1 = corr[idx[:,1]] # b,4\n\t        bad_mask = np.linalg.norm(seed0 - seed1, 2, 1) < bad_seed_thresh\n\t        seed0 = seed0[~bad_mask]\n\t        seed1 = seed1[~bad_mask]\n\t        seed = np.stack([seed0,seed1],1)\n\t        scale, rotation, offset = compute_similarity_transform_batch(seed[:,:,:2],seed[:,:,2:]) #\n", "        mask = compute_inlier_mask(scale,rotation,offset,corr,inlier_thresh) # b,n\n\t        inlier_num = np.sum(mask,1)\n\t        if np.max(inlier_num) >= best_inlier:\n\t            best_mask = mask[np.argmax(inlier_num)]\n\t        iter_num += seed.shape[0]\n\t        inlier_ratio = np.mean(best_mask)\n\t        if 1-(1-inlier_ratio**2)**iter_num > confidence:\n\t            break\n\t    inlier_corr=corr[best_mask]\n\t    scale, rotation, offset = compute_similarity_transform_batch(inlier_corr[None,:,:2],inlier_corr[None,:,2:])\n", "    scale, rotation, offset = scale[0,0,0], rotation[0], offset[0,:,0]\n\t    return scale, rotation, offset, best_mask\n\tdef compose_similarity_transform(scale, rotation, offset):\n\t    M = transformation_scale_2d(scale)\n\t    M = transformation_compose_2d(M, np.concatenate([rotation, np.zeros([2, 1])], 1).astype(np.float32))\n\t    M = transformation_compose_2d(M, transformation_offset_2d(offset[0], offset[1]))\n\t    return M\n\tfrom loguru import logger\n\tclass Gen6DEstimator:\n\t    default_cfg={\n", "        'ref_resolution': 128,\n\t        \"ref_view_num\": 64,\n\t        \"det_ref_view_num\": 32,\n\t        'selector': None,\n\t        'detector': None,\n\t        'refiner': None,\n\t        'refine_iter': 3,\n\t    }\n\t    def __init__(self,cfg):\n\t        self.cfg = {**self.default_cfg,**cfg}\n", "        self.ref_info = {}\n\t        self.use_multi_pose = self.cfg.get(\"use_multi_pose\", False)\n\t        self.use_multi_pose_num = self.cfg.get(\"use_multi_pose_num\", 3)\n\t        logger.debug(f'use_multi_pose {self.use_multi_pose}')\n\t        if  \"pretrain\" in  self.cfg['detector']:\n\t            self.detector = self._load_module(self.cfg['detector'])\n\t        else:\n\t            self.detector = self._load_detector_dino_module(self.cfg['detector'])\n\t        if \"pretrain\" in  self.cfg['selector']:\n\t            self.selector = self._load_module(self.cfg['selector'])\n", "        else:\n\t            self.selector = self._load_selector_dino_module(self.cfg['selector'])\n\t        if self.cfg['refiner'] is not None:\n\t            if  \"pretrain\" in  self.cfg['refiner']:\n\t                self.refiner = self._load_module(self.cfg['refiner']).cuda()\n\t            else:\n\t                self.refiner = self._load_refiner_dino_module(self.cfg['refiner']).cuda()\n\t        else:\n\t            self.refiner = None\n\t    @staticmethod\n", "    def _load_module(cfg):\n\t        refiner_cfg = load_cfg(cfg)\n\t        refiner = name2network[refiner_cfg['network']](refiner_cfg)\n\t        state_dict = torch.load(f'data/model/{refiner_cfg[\"name\"]}/model_best.pth')\n\t        refiner.load_state_dict(state_dict['network_state_dict'], strict = False  )\n\t        logger.debug(f'load from {refiner_cfg[\"name\"]}/model_best.pth step {state_dict[\"step\"]}')\n\t        # gpu_tracker.track()\n\t        refiner.cuda().eval()\n\t        # gpu_tracker.track()\n\t        return refiner\n", "    @staticmethod\n\t    def _load_detector_dino_module(cfg):\n\t        detector_cfg = load_cfg(cfg)\n\t        detector = name2network[detector_cfg['network']](detector_cfg)\n\t        name = detector_cfg[\"name\"]\n\t        # state_dict = torch.load(f'data/model/{name}/model_best.pth')\n\t        state_dict = torch.load(f'data/model/{name}/model.pth')\n\t        detector.load_state_dict(state_dict['network_state_dict'], strict = True  )\n\t        logger.debug(f'load from data/model/{name}/model_best.pth step {state_dict[\"step\"]}')\n\t        detector.cuda().eval()\n", "        return detector\n\t    @staticmethod\n\t    def _load_selector_dino_module(cfg):\n\t        selector_cfg = load_cfg(cfg)\n\t        selector = name2network[selector_cfg['network']](selector_cfg)\n\t        state_dict = torch.load(f'data/model/fpn_dino_selector/model_best.pth')\n\t        selector.load_state_dict(state_dict['network_state_dict'], strict = True  )\n\t        logger.debug(f'load from data/model/fpn_dino_selector/model_best.pth step {state_dict[\"step\"]}')\n\t        selector.cuda().eval()\n\t        return selector\n", "    @staticmethod\n\t    def _load_refiner_dino_module(cfg):\n\t        refiner_cfg = load_cfg(cfg)\n\t        refiner = name2network[refiner_cfg['network']](refiner_cfg)\n\t        state_dict = torch.load(f'data/model/{refiner_cfg[\"name\"]}/model.pth')\n\t        # refiner.load_state_dict(state_dict['network_state_dict'], strict = True  )\n\t        refiner.load_state_dict(state_dict['network_state_dict'], strict = False  )\n\t        logger.debug(f'load from data/model/{refiner_cfg[\"name\"]}/model.pth step {state_dict[\"step\"]}')\n\t        # gpu_tracker.track()\n\t        refiner.cuda().eval()\n", "        # gpu_tracker.track()\n\t        return refiner\n\t    def build(self, database: BaseDatabase, split_type: str):\n\t        object_center = get_object_center(database)\n\t        object_vert = get_object_vert(database)\n\t        ref_ids_all, _ = get_database_split(database, split_type)\n\t        # use fps to select reference images for detection and selection\n\t        ref_ids = select_reference_img_ids_fps(database, ref_ids_all, self.cfg['ref_view_num'])\n\t        ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs = \\\n\t            normalize_reference_views(database, ref_ids, self.cfg['ref_resolution'], 0.05)\n", "        # in-plane rotation for viewpoint selection\n\t        rfn, h, w, _ = ref_imgs.shape\n\t        ref_imgs_rots = []\n\t        angles = [-np.pi/2, -np.pi/4, 0, np.pi/4, np.pi/2]\n\t        for angle in angles:\n\t            M = transformation_offset_2d(-w/2,-h/2)\n\t            M = transformation_compose_2d(M, transformation_rotation_2d(angle))\n\t            M = transformation_compose_2d(M, transformation_offset_2d(w/2,h/2))\n\t            H_ = np.identity(3).astype(np.float32)\n\t            H_[:2,:3] = M\n", "            ref_imgs_rot = []\n\t            for rfi in range(rfn):\n\t                H_new = H_ @ ref_Hs[rfi]\n\t                ref_imgs_rot.append(cv2.warpPerspective(database.get_image(ref_ids[rfi]), H_new, (w,h), flags=cv2.INTER_LINEAR))\n\t            ref_imgs_rots.append(np.stack(ref_imgs_rot, 0))\n\t        ref_imgs_rots = np.stack(ref_imgs_rots, 0) # an,rfn,h,w,3\n\t        self.detector.load_ref_imgs(ref_imgs[:self.cfg['det_ref_view_num']])\n\t        self.selector.load_ref_imgs(ref_imgs_rots, ref_poses, object_center, object_vert)\n\t        self.ref_info={'imgs': ref_imgs, 'ref_imgs': ref_imgs_rots, 'masks': ref_masks, 'Ks': ref_Ks, 'poses': ref_poses, 'center': object_center}\n\t        if self.refiner is not None:\n", "            logger.info(f\"the number of ref_ids is:{len(ref_ids)}\")\n\t            if len(ref_ids)>=64:\n\t                self.refiner.load_ref_imgs(database, ref_ids_all) # 1000\n\t            else:\n\t                self.refiner.load_ref_imgs(database, ref_ids)\n\t        self.count = 0\n\t        self.total_time = 0\n\t    def predict(self, que_img, \\\n\t                      que_K, \\\n\t                      pose_init=None, \\\n", "                      position=None, \\\n\t                      scale_r2q=None, \\\n\t                      angle_r2q=None,  \\\n\t                      need_refiner=True ):\n\t        \"\"\"\n\t        que_img: 查询图片 原始的输入图片\n\t        que_K: 查询的图片的内参\n\t        pose_init: 初始位姿\n\t        position: 目标的中心点位置\n\t        scale_r2q: 缩放比例\n", "        need_refiner: 是否需要refiner\n\t        \"\"\"\n\t        inter_results = {}\n\t        if pose_init is None:\n\t            # stage 1: detection\n\t            with torch.no_grad():\n\t                detection_outputs = self.detector.detect_que_imgs(que_img[None])\n\t                position = detection_outputs['positions'][0] if position is None else position\n\t                scale_r2q = detection_outputs['scales'][0]  if scale_r2q is None else scale_r2q\n\t            # crop the image according to the detected scale and the detected position\n", "            que_img_, _ = transformation_crop(que_img, position, 1/scale_r2q, 0, self.cfg['ref_resolution'])  # h,w,3\n\t            inter_results['det_position'] = position\n\t            inter_results['det_scale_r2q'] = scale_r2q\n\t            inter_results['det_que_img'] = que_img_\n\t            # stage 2: viewpoint selection\n\t            with torch.no_grad():\n\t                selection_results = self.selector.select_que_imgs(que_img_[None])\n\t            if self.use_multi_pose:\n\t                ref_idx = selection_results['ref_idx'][:self.use_multi_pose_num]\n\t                angle_r2q = selection_results['angles'][:self.use_multi_pose_num] if angle_r2q is None else angle_r2q\n", "                scores = selection_results['scores'][:self.use_multi_pose_num]\n\t                inter_results['sel_angle_r2q'] = angle_r2q\n\t                inter_results['sel_scores'] = scores\n\t                inter_results['sel_ref_idx'] = ref_idx\n\t                # stage 3: solve for pose from detection/selected viewpoint/in-plane rotation\n\t                # find most closest image to query\n\t                ref_pose = self.ref_info['poses'][ref_idx]\n\t                ref_K = self.ref_info['Ks'][ref_idx]\n\t                pose_pr = estimate_pose_from_similarity_transform_compose(\n\t                    position, scale_r2q, angle_r2q, ref_pose, ref_K, que_K, self.ref_info['center'])\n", "            else:\n\t                ref_idx = selection_results['ref_idx'][0]\n\t                angle_r2q = selection_results['angles'][0] if angle_r2q is None else angle_r2q\n\t                scores = selection_results['scores'][0]\n\t                inter_results['sel_angle_r2q'] = angle_r2q\n\t                inter_results['sel_scores'] = scores\n\t                inter_results['sel_ref_idx'] = ref_idx\n\t                # stage 3: solve for pose from detection/selected viewpoint/in-plane rotation\n\t                # find most closest image to query\n\t                ref_pose = self.ref_info['poses'][ref_idx]\n", "                ref_K = self.ref_info['Ks'][ref_idx]\n\t                pose_pr = estimate_pose_from_similarity_transform_compose(\n\t                    position, scale_r2q, angle_r2q, ref_pose, ref_K, que_K, self.ref_info['center'])\n\t        else:\n\t            pose_pr = pose_init\n\t        # stage 4: refine pose\n\t        if self.refiner is not None :\n\t            refine_poses = [pose_pr]\n\t            # gpu_tracker.track()\n\t            t1 = time.time()\n", "            for k in range(self.cfg['refine_iter']):\n\t                if need_refiner:\n\t                    pose_pr = self.refiner.refine_que_imgs(que_img, que_K, pose_pr, size=128, \\\n\t                                                           ref_num=6, ref_even=True)\n\t                    refine_poses.append(pose_pr)\n\t            t2 = time.time()\n\t            # gpu_tracker.track()\n\t            # torch.cuda.empty_cache()\n\t            self.total_time += 1000*(t2-t1)\n\t            if self.count==50:\n", "                # print(f\"average cost: t2-t1 {self.total_time/50.} ms\" )\n\t                self.count = 0\n\t            else:\n\t                self.count +=1\n\t            # print(f\"network cost: t2-t1 {1000*(t2-t1)} ms\" )\n\t            inter_results['refine_poses'] = refine_poses\n\t        return pose_pr, inter_results\n\tname2estimator={\n\t    'gen6d': Gen6DEstimator,\n\t}"]}
{"filename": "train_model.py", "chunked_list": ["import argparse\n\tfrom train.trainer import Trainer\n\tfrom utils.base_utils import load_cfg\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--cfg', type=str, default='configs/train/')\n\tflags = parser.parse_args()\n\ttrainer = Trainer(load_cfg(flags.cfg),)\n\ttrainer.run()"]}
{"filename": "gpu_mem_track.py", "chunked_list": ["import gc\n\timport datetime\n\timport inspect\n\timport torch\n\timport numpy as np\n\tdtype_memory_size_dict = {\n\t    torch.float64: 64/8,\n\t    torch.double: 64/8,\n\t    torch.float32: 32/8,\n\t    torch.float: 32/8,\n", "    torch.float16: 16/8,\n\t    torch.half: 16/8,\n\t    torch.int64: 64/8,\n\t    torch.long: 64/8,\n\t    torch.int32: 32/8,\n\t    torch.int: 32/8,\n\t    torch.int16: 16/8,\n\t    torch.short: 16/6,\n\t    torch.uint8: 8/8,\n\t    torch.int8: 8/8,\n", "}\n\t# compatibility of torch1.0\n\tif getattr(torch, \"bfloat16\", None) is not None:\n\t    dtype_memory_size_dict[torch.bfloat16] = 16/8\n\tif getattr(torch, \"bool\", None) is not None:\n\t    dtype_memory_size_dict[torch.bool] = 8/8 # pytorch use 1 byte for a bool, see https://github.com/pytorch/pytorch/issues/41571\n\tdef get_mem_space(x):\n\t    try:\n\t        ret = dtype_memory_size_dict[x]\n\t    except KeyError:\n", "        print(f\"dtype {x} is not supported!\")\n\t    return ret\n\tclass MemTracker(object):\n\t    \"\"\"\n\t    Class used to track pytorch memory usage\n\t    Arguments:\n\t        detail(bool, default True): whether the function shows the detail gpu memory usage\n\t        path(str): where to save log file\n\t        verbose(bool, default False): whether show the trivial exception\n\t        device(int): GPU number, default is 0\n", "    \"\"\"\n\t    def __init__(self, detail=True, path='', verbose=False, device=0):\n\t        self.print_detail = detail\n\t        self.last_tensor_sizes = set()\n\t        self.gpu_profile_fn = path + f'{datetime.datetime.now():%d-%b-%y-%H:%M:%S}-gpu_mem_track.txt'\n\t        self.verbose = verbose\n\t        self.begin = True\n\t        self.device = device\n\t    def get_tensors(self):\n\t        for obj in gc.get_objects():\n", "            try:\n\t                if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n\t                    tensor = obj\n\t                else:\n\t                    continue\n\t                if tensor.is_cuda:\n\t                    yield tensor\n\t            except Exception as e:\n\t                if self.verbose:\n\t                    print('A trivial exception occured: {}'.format(e))\n", "    def get_tensor_usage(self):\n\t        sizes = [np.prod(np.array(tensor.size())) * get_mem_space(tensor.dtype) for tensor in self.get_tensors()]\n\t        return np.sum(sizes) / 1024**2\n\t    def get_allocate_usage(self):\n\t        return torch.cuda.memory_allocated() / 1024**2\n\t    def clear_cache(self):\n\t        gc.collect()\n\t        torch.cuda.empty_cache()\n\t    def print_all_gpu_tensor(self, file=None):\n\t        for x in self.get_tensors():\n", "            print(x.size(), x.dtype, np.prod(np.array(x.size()))*get_mem_space(x.dtype)/1024**2, file=file)\n\t    def track(self):\n\t        \"\"\"\n\t        Track the GPU memory usage\n\t        \"\"\"\n\t        frameinfo = inspect.stack()[1]\n\t        where_str = frameinfo.filename + ' line ' + str(frameinfo.lineno) + ': ' + frameinfo.function\n\t        with open(self.gpu_profile_fn, 'a+') as f:\n\t            if self.begin:\n\t                f.write(f\"GPU Memory Track | {datetime.datetime.now():%d-%b-%y-%H:%M:%S} |\"\n", "                        f\" Total Tensor Used Memory:{self.get_tensor_usage():<7.1f}Mb\"\n\t                        f\" Total Allocated Memory:{self.get_allocate_usage():<7.1f}Mb\\n\\n\")\n\t                self.begin = False\n\t            if self.print_detail is True:\n\t                ts_list = [(tensor.size(), tensor.dtype) for tensor in self.get_tensors()]\n\t                new_tensor_sizes = {(type(x),\n\t                                    tuple(x.size()),\n\t                                    ts_list.count((x.size(), x.dtype)),\n\t                                    np.prod(np.array(x.size()))*get_mem_space(x.dtype)/1024**2,\n\t                                    x.dtype) for x in self.get_tensors()}\n", "                for t, s, n, m, data_type in new_tensor_sizes - self.last_tensor_sizes:\n\t                    f.write(f'+ | {str(n)} * Size:{str(s):<20} | Memory: {str(m*n)[:6]} M | {str(t):<20} | {data_type}\\n')\n\t                for t, s, n, m, data_type in self.last_tensor_sizes - new_tensor_sizes:\n\t                    f.write(f'- | {str(n)} * Size:{str(s):<20} | Memory: {str(m*n)[:6]} M | {str(t):<20} | {data_type}\\n')\n\t                self.last_tensor_sizes = new_tensor_sizes\n\t            f.write(f\"\\nAt {where_str:<50}\"\n\t                    f\" Total Tensor Used Memory:{self.get_tensor_usage():<7.1f}Mb\"\n\t                    f\" Total Allocated Memory:{self.get_allocate_usage():<7.1f}Mb\\n\\n\")\n"]}
{"filename": "utils/base_utils.py", "chunked_list": ["import math\n\timport os\n\timport cv2\n\timport plyfile\n\timport torch\n\timport numpy as np\n\timport pickle\n\timport yaml\n\tfrom numpy import ndarray\n\tfrom plyfile import PlyData\n", "from skimage.io import imread\n\tfrom torch import Tensor\n\tfrom transforms3d.euler import euler2mat\n\t#########################IO#####################################\n\tdef load_point_cloud(pcl_path):\n\t    with open(pcl_path, \"rb\") as f:\n\t        plydata = plyfile.PlyData.read(f)\n\t        xyz = np.stack([np.array(plydata[\"vertex\"][c]).astype(float) for c in (\"x\", \"y\", \"z\")], axis=1)\n\t    return xyz\n\tdef read_pickle(pkl_path):\n", "    with open(pkl_path, 'rb') as f:\n\t        return pickle.load(f)\n\tdef save_pickle(data, pkl_path):\n\t    os.system('mkdir -p {}'.format(os.path.dirname(pkl_path)))\n\t    with open(pkl_path, 'wb') as f:\n\t        pickle.dump(data, f)\n\tdef compute_precision_recall_np(pr,gt,eps=1e-5):\n\t    tp=np.sum(gt & pr)\n\t    fp=np.sum((~gt) & pr)\n\t    fn=np.sum(gt & (~pr))\n", "    precision=(tp+eps)/(fp+tp+eps)\n\t    recall=(tp+eps)/(tp+fn+eps)\n\t    if precision<1e-3 or recall<1e-3:\n\t        f1=0.0\n\t    else:\n\t        f1=(2*precision*recall+eps)/(precision+recall+eps)\n\t    return precision, recall, f1\n\tdef load_cfg(path):\n\t    with open(path, 'r') as f:\n\t        return yaml.load(f, Loader=yaml.FullLoader)\n", "def get_stem(path,suffix_len=5):\n\t    return os.path.basename(path)[:-suffix_len]\n\tdef load_component(component_func,component_cfg_fn):\n\t    component_cfg=load_cfg(component_cfg_fn)\n\t    return component_func[component_cfg['type']](component_cfg)\n\tdef load_ply_model(model_path):\n\t    ply = PlyData.read(model_path)\n\t    data = ply.elements[0].data\n\t    x = data['x']\n\t    y = data['y']\n", "    z = data['z']\n\t    return np.stack([x, y, z], axis=-1)\n\tdef save_depth(fn,depth,max_val=1000):\n\t    import png\n\t    depth = np.clip(depth,a_min=0,a_max=max_val)/max_val*65535\n\t    depth = depth.astype(np.uint16)\n\t    with open(fn, 'wb') as f:\n\t        writer = png.Writer(width=depth.shape[1], height=depth.shape[0], bitdepth=16, greyscale=True)\n\t        zgray2list = depth.tolist()\n\t        writer.write(f, zgray2list)\n", "#####################depth and image###############################\n\tdef mask_zbuffer_to_pts(mask, zbuffer, K):\n\t    ys,xs=np.nonzero(mask)\n\t    zbuffer=zbuffer[ys, xs]\n\t    u,v,f=K[0,2],K[1,2],K[0,0]\n\t    depth = zbuffer / np.sqrt((xs - u + 0.5) ** 2 + (ys - v + 0.5) ** 2 + f ** 2) * f\n\t    pts=np.asarray([xs, ys, depth], np.float32).transpose()\n\t    pts[:,:2]*=pts[:,2:]\n\t    return np.dot(pts,np.linalg.inv(K).transpose())\n\tdef mask_depth_to_pts(mask,depth,K,rgb=None):\n", "    hs,ws=np.nonzero(mask)\n\t    depth=depth[hs,ws]\n\t    pts=np.asarray([ws,hs,depth],np.float32).transpose()\n\t    pts[:,:2]*=pts[:,2:]\n\t    if rgb is not None:\n\t        return np.dot(pts, np.linalg.inv(K).transpose()), rgb[hs,ws]\n\t    else:\n\t        return np.dot(pts, np.linalg.inv(K).transpose())\n\tdef read_render_zbuffer(dpt_pth,max_depth,min_depth):\n\t    zbuffer = imread(dpt_pth)\n", "    mask = (zbuffer>0) & (zbuffer<5000)\n\t    zbuffer=zbuffer.astype(np.float64)/2**16*(max_depth-min_depth)+min_depth\n\t    return mask, zbuffer\n\tdef zbuffer_to_depth(zbuffer,K):\n\t    u,v,f=K[0,2],K[1,2],K[0,0]\n\t    x=np.arange(zbuffer.shape[1])\n\t    y=np.arange(zbuffer.shape[0])\n\t    x,y=np.meshgrid(x,y)\n\t    x=np.reshape(x,[-1,1])\n\t    y=np.reshape(y,[-1,1])\n", "    depth = np.reshape(zbuffer,[-1,1])\n\t    depth = depth / np.sqrt((x - u + 0.5) ** 2 + (y - v + 0.5) ** 2 + f ** 2) * f\n\t    return np.reshape(depth,zbuffer.shape)\n\tdef color_map_forward(rgb):\n\t    return rgb.astype(np.float32)/255\n\tdef color_map_backward(rgb):\n\t    rgb=rgb*255\n\t    rgb=np.clip(rgb,a_min=0,a_max=255).astype(np.uint8)\n\t    return rgb\n\tdef rotate_image(rot, pose, K, img, mask):\n", "    if isinstance(rot, np.ndarray):\n\t        R = rot\n\t    else:\n\t        R=np.array([[np.cos(rot), -np.sin(rot), 0.0],\n\t                    [np.sin(rot), np.cos(rot), 0.0],\n\t                    [0,0,1]],dtype=np.float32)\n\t    # adjust pose\n\t    pose_adj=np.copy(pose)\n\t    pose_adj[:, :3] = R @ pose_adj[:, :3]\n\t    pose_adj[:, 3:] = R @ pose_adj[:, 3:]\n", "    # adjust image\n\t    transform = K @ R @ np.linalg.inv(K) # transform original\n\t    h, w, _ = img.shape\n\t    ys, xs = np.nonzero(mask)\n\t    coords = np.stack([xs,ys],-1).astype(np.float32)\n\t    coords_new = cv2.perspectiveTransform(coords[:,None,:],transform)[:,0,:]\n\t    x_min, y_min = np.floor(np.min(coords_new,0)).astype(np.int32)\n\t    x_max, y_max = np.ceil(np.max(coords_new,0)).astype(np.int32)\n\t    th, tw = y_max - y_min, x_max - x_min\n\t    translation = np.identity(3)\n", "    translation[0,2]=-x_min\n\t    translation[1,2]=-y_min\n\t    K = translation @ K\n\t    transform = translation @ transform\n\t    img = cv2.warpPerspective(img, transform, (tw, th), flags=cv2.INTER_LINEAR)\n\t    return img, pose_adj, K\n\tdef resize_img(img, ratio):\n\t    # if ratio>=1.0: return img\n\t    h, w, _ = img.shape\n\t    hn, wn = int(np.round(h * ratio)), int(np.round(w * ratio))\n", "    img_out = cv2.resize(downsample_gaussian_blur(img, ratio), (wn, hn), cv2.INTER_LINEAR)\n\t    return img_out\n\tdef pad_img(img,padding_interval=8):\n\t    h, w = img.shape[:2]\n\t    hp = (padding_interval - (h % padding_interval)) % padding_interval\n\t    wp = (padding_interval - (w % padding_interval)) % padding_interval\n\t    if hp != 0 or wp != 0:\n\t        img = np.pad(img, ((0, hp), (0, wp), (0, 0)), 'edge')\n\t    return img\n\tdef pad_img_end(img,th,tw,padding_mode='edge',constant_values=0):\n", "    h, w = img.shape[:2]\n\t    hp = th-h\n\t    wp = tw-w\n\t    if hp != 0 or wp != 0:\n\t        if padding_mode=='constant':\n\t            img = np.pad(img, ((0, hp), (0, wp), (0, 0)), padding_mode, constant_values=constant_values)\n\t        else:\n\t            img = np.pad(img, ((0, hp), (0, wp), (0, 0)), padding_mode)\n\t    return img\n\tdef pad_img_target(img, th, tw, K=np.eye(3), background_color=0):\n", "    h, w = img.shape[:2]\n\t    hp = th - h\n\t    wp = tw - w\n\t    if hp != 0 or wp != 0:\n\t        if len(img.shape) == 3:\n\t            img = np.pad(img, ((hp//2, hp-hp//2), (wp//2, wp - wp//2), (0, 0)), 'constant', constant_values=background_color)\n\t        elif len(img.shape) == 2:\n\t            img = np.pad(img, ((hp // 2, hp - hp // 2), (wp // 2, wp - wp // 2)), 'constant', constant_values=background_color)\n\t        else:\n\t            print(f'image shape unknown {img.shape}')\n", "            raise NotImplementedError\n\t        translation = np.identity(3)\n\t        translation[0,2]=wp//2\n\t        translation[1,2]=hp//2\n\t        K = translation @ K\n\t    return img, K\n\t#######################image processing#############################\n\tdef grey_repeats(img_raw):\n\t    if len(img_raw.shape) == 2: img_raw = np.repeat(img_raw[:, :, None], 3, axis=2)\n\t    if img_raw.shape[2] > 3: img_raw = img_raw[:, :, :3]\n", "    return img_raw\n\tdef normalize_image(img,mask=None):\n\t    if mask is not None: img[np.logical_not(mask.astype(np.bool))]=127\n\t    img=(img.transpose([2,0,1]).astype(np.float32)-127.0)/128.0\n\t    return torch.tensor(img,dtype=torch.float32)\n\tdef tensor_to_image(tensor):\n\t    return (tensor * 128 + 127).astype(np.uint8).transpose(1,2,0)\n\tdef equal_hist(img):\n\t    if len(img.shape)==3:\n\t        img0=cv2.equalizeHist(img[:,:,0])\n", "        img1=cv2.equalizeHist(img[:,:,1])\n\t        img2=cv2.equalizeHist(img[:,:,2])\n\t        img=np.concatenate([img0[...,None],img1[...,None],img2[...,None]],2)\n\t    else:\n\t        img=cv2.equalizeHist(img)\n\t    return img\n\tdef resize_large_image(img,resize_max):\n\t    h,w=img.shape[:2]\n\t    max_side = max(h, w)\n\t    if max_side > resize_max:\n", "        ratio = resize_max / max_side\n\t        if ratio <= 0.5: img = cv2.GaussianBlur(img, (5, 5), 1.5)\n\t        img = cv2.resize(img, (int(round(ratio * w)), int(round(ratio * h))), interpolation=cv2.INTER_LINEAR)\n\t        return img, ratio\n\t    else:\n\t        return img, 1.0\n\tdef downsample_gaussian_blur(img,ratio):\n\t    sigma=(1/ratio)/3\n\t    # ksize=np.ceil(2*sigma)\n\t    ksize=int(np.ceil(((sigma-0.8)/0.3+1)*2+1))\n", "    ksize=ksize+1 if ksize%2==0 else ksize\n\t    img=cv2.GaussianBlur(img,(ksize,ksize),sigma,borderType=cv2.BORDER_REFLECT101)\n\t    return img\n\tdef resize_small_image(img,resize_min):\n\t    h,w=img.shape[:2]\n\t    min_side = min(h, w)\n\t    if min_side < resize_min:\n\t        ratio = resize_min / min_side\n\t        img = cv2.resize(img, (int(round(ratio * w)), int(round(ratio * h))), interpolation=cv2.INTER_LINEAR)\n\t        return img, ratio\n", "    else:\n\t        return img, 1.0\n\t############################geometry######################################\n\tdef project_points(pts,RT,K):\n\t    pts = np.matmul(pts,RT[:,:3].transpose())+RT[:,3:].transpose()\n\t    pts = np.matmul(pts,K.transpose())\n\t    dpt = pts[:,2]\n\t    mask0 = (np.abs(dpt)<1e-4) & (np.abs(dpt)>0)\n\t    if np.sum(mask0)>0: dpt[mask0]=1e-4\n\t    mask1=(np.abs(dpt) > -1e-4) & (np.abs(dpt) < 0)\n", "    if np.sum(mask1)>0: dpt[mask1]=-1e-4\n\t    pts2d = pts[:,:2]/dpt[:,None]\n\t    return pts2d, dpt\n\tdef round_coordinates(coord,h,w):\n\t    coord=np.round(coord).astype(np.int32)\n\t    coord[coord[:,0]<0,0]=0\n\t    coord[coord[:,0]>=w,0]=w-1\n\t    coord[coord[:,1]<0,1]=0\n\t    coord[coord[:,1]>=h,1]=h-1\n\t    return coord\n", "def perspective_transform(pts, H):\n\t    tpts = np.concatenate([pts, np.ones([pts.shape[0], 1])], 1) @ H.transpose()\n\t    tpts = tpts[:, :2] / np.abs(tpts[:, 2:]) # todo: why only abs? this one is correct\n\t    return tpts\n\tdef get_rot_m(angle):\n\t    return np.asarray([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]], np.float32) # rn+1,3,3\n\tdef get_rot_m_batch(angle):\n\t    return np.asarray([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]], np.float32).transpose([2,0,1])\n\tdef compute_F(K1, K2, R, t):\n\t    \"\"\"\n", "    :param K1: [3,3]\n\t    :param K2: [3,3]\n\t    :param R:  [3,3]\n\t    :param t:  [3,1]\n\t    :return:\n\t    \"\"\"\n\t    A = K1 @ R.T @ t # [3,1]\n\t    C = np.asarray([[0,-A[2,0],A[1,0]],\n\t                    [A[2,0],0,-A[0,0]],\n\t                    [-A[1,0],A[0,0],0]])\n", "    F = (np.linalg.inv(K2)).T @ R @ K1.T @ C\n\t    return F\n\tdef compute_relative_transformation(Rt0,Rt1):\n\t    \"\"\"\n\t    x1=Rx0+t\n\t    :param Rt0: x0=R0x+t0\n\t    :param Rt1: x1=R1x+t1\n\t    :return:\n\t        R1R0.T(x0-t0)+t1\n\t    \"\"\"\n", "    R=Rt1[:,:3] @ Rt0[:,:3].T\n\t    t=Rt1[:,3] - R @ Rt0[:,3]\n\t    return np.concatenate([R,t[:,None]],1)\n\tdef pts_to_hpts(pts):\n\t    return np.concatenate([pts,np.ones([pts.shape[0],1])],1)\n\tdef hpts_to_pts(hpts):\n\t    return hpts[:,:-1]/hpts[:,-1:]\n\tdef np_skew_symmetric(v):\n\t    M = np.asarray([\n\t        [0, -v[2], v[1],],\n", "        [v[2], 0, -v[0],],\n\t        [-v[1], v[0], 0,],\n\t    ])\n\t    return M\n\tdef point_line_dist(hpts,lines):\n\t    \"\"\"\n\t    :param hpts: n,3 or n,2\n\t    :param lines: n,3\n\t    :return:\n\t    \"\"\"\n", "    if hpts.shape[1]==2:\n\t        hpts=np.concatenate([hpts,np.ones([hpts.shape[0],1])],1)\n\t    return np.abs(np.sum(hpts*lines,1))/np.linalg.norm(lines[:,:2],2,1)\n\tdef epipolar_distance(x0, x1, F):\n\t    \"\"\"\n\t    :param x0: [n,2]\n\t    :param x1: [n,2]\n\t    :param F:  [3,3]\n\t    :return:\n\t    \"\"\"\n", "    hkps0 = np.concatenate([x0, np.ones([x0.shape[0], 1])], 1)\n\t    hkps1 = np.concatenate([x1, np.ones([x1.shape[0], 1])], 1)\n\t    lines1 = hkps0 @ F.T\n\t    lines0 = hkps1 @ F\n\t    dist10 = point_line_dist(hkps0, lines0)\n\t    dist01 = point_line_dist(hkps1, lines1)\n\t    return dist10, dist01\n\tdef epipolar_distance_mean(x0, x1, F):\n\t    return np.mean(np.stack(epipolar_distance(x0,x1,F),1),1)\n\tdef compute_dR_dt(R0, t0, R1, t1):\n", "    # Compute dR, dt\n\t    dR = np.dot(R1, R0.T)\n\t    dt = t1 - np.dot(dR, t0)\n\t    return dR, dt\n\tdef interpolate_image_points(img, pts, interpolation=cv2.INTER_LINEAR):\n\t    # img [h,w,k] pts [n,2]\n\t    if len(pts)<32767:\n\t        pts=pts.astype(np.float32)\n\t        return cv2.remap(img,pts[:,None,0],pts[:,None,1],borderMode=cv2.BORDER_CONSTANT,borderValue=0,interpolation=interpolation)[:,0]\n\t        # pn=len(pts)\n", "        # sl=int(np.ceil(np.sqrt(pn)))\n\t        # tmp_img=np.zeros([sl*sl,2],np.float32)\n\t        # tmp_img[:pn]=pts\n\t        # tmp_img=tmp_img.reshape([sl,sl,2])\n\t        # tmp_img=cv2.remap(img,tmp_img[:,:,0],tmp_img[:,:,1],borderMode=cv2.BORDER_CONSTANT,borderValue=0,interpolation=interpolation)\n\t        # return tmp_img.flatten()[:pn]\n\t    else:\n\t        results=[]\n\t        for k in range(0,len(pts),30000):\n\t            results.append(interpolate_image_points(img, pts[k:k + 30000], interpolation))\n", "        return np.concatenate(results,0)\n\tdef transform_points_Rt(pts, R, t):\n\t    t = t.flatten()\n\t    return pts @ R.T + t[None,:]\n\tdef transform_points_pose(pts, pose):\n\t    R, t = pose[:, :3], pose[:, 3]\n\t    if len(pts.shape)==1:\n\t        return (R @ pts[:,None] + t[:,None])[:,0]\n\t    return pts @ R.T + t[None,:]\n\tdef quaternion_from_matrix(matrix, isprecise=False):\n", "    '''Return quaternion from rotation matrix.\n\t    If isprecise is True, the input matrix is assumed to be a precise rotation\n\t    matrix and a faster algorithm is used.\n\t    >>> q = quaternion_from_matrix(numpy.identity(4), True)\n\t    >>> numpy.allclose(q, [1, 0, 0, 0])\n\t    True\n\t    >>> q = quaternion_from_matrix(numpy.diag([1, -1, -1, 1]))\n\t    >>> numpy.allclose(q, [0, 1, 0, 0]) or numpy.allclose(q, [0, -1, 0, 0])\n\t    True\n\t    >>> R = rotation_matrix(0.123, (1, 2, 3))\n", "    >>> q = quaternion_from_matrix(R, True)\n\t    >>> numpy.allclose(q, [0.9981095, 0.0164262, 0.0328524, 0.0492786])\n\t    True\n\t    >>> R = [[-0.545, 0.797, 0.260, 0], [0.733, 0.603, -0.313, 0],\n\t    ...      [-0.407, 0.021, -0.913, 0], [0, 0, 0, 1]]\n\t    >>> q = quaternion_from_matrix(R)\n\t    >>> numpy.allclose(q, [0.19069, 0.43736, 0.87485, -0.083611])\n\t    True\n\t    >>> R = [[0.395, 0.362, 0.843, 0], [-0.626, 0.796, -0.056, 0],\n\t    ...      [-0.677, -0.498, 0.529, 0], [0, 0, 0, 1]]\n", "    >>> q = quaternion_from_matrix(R)\n\t    >>> numpy.allclose(q, [0.82336615, -0.13610694, 0.46344705, -0.29792603])\n\t    True\n\t    >>> R = random_rotation_matrix()\n\t    >>> q = quaternion_from_matrix(R)\n\t    >>> is_same_transform(R, quaternion_matrix(q))\n\t    True\n\t    >>> R = euler_matrix(0.0, 0.0, numpy.pi/2.0)\n\t    >>> numpy.allclose(quaternion_from_matrix(R, isprecise=False),\n\t    ...                quaternion_from_matrix(R, isprecise=True))\n", "    True\n\t    '''\n\t    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n\t    if isprecise:\n\t        q = np.empty((4, ))\n\t        t = np.trace(M)\n\t        if t > M[3, 3]:\n\t            q[0] = t\n\t            q[3] = M[1, 0] - M[0, 1]\n\t            q[2] = M[0, 2] - M[2, 0]\n", "            q[1] = M[2, 1] - M[1, 2]\n\t        else:\n\t            i, j, k = 1, 2, 3\n\t            if M[1, 1] > M[0, 0]:\n\t                i, j, k = 2, 3, 1\n\t            if M[2, 2] > M[i, i]:\n\t                i, j, k = 3, 1, 2\n\t            t = M[i, i] - (M[j, j] + M[k, k]) + M[3, 3]\n\t            q[i] = t\n\t            q[j] = M[i, j] + M[j, i]\n", "            q[k] = M[k, i] + M[i, k]\n\t            q[3] = M[k, j] - M[j, k]\n\t        q *= 0.5 / math.sqrt(t * M[3, 3])\n\t    else:\n\t        m00 = M[0, 0]\n\t        m01 = M[0, 1]\n\t        m02 = M[0, 2]\n\t        m10 = M[1, 0]\n\t        m11 = M[1, 1]\n\t        m12 = M[1, 2]\n", "        m20 = M[2, 0]\n\t        m21 = M[2, 1]\n\t        m22 = M[2, 2]\n\t        # symmetric matrix K\n\t        K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n\t                      [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n\t                      [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n\t                      [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n\t        K /= 3.0\n\t        # quaternion is eigenvector of K that corresponds to largest eigenvalue\n", "        w, V = np.linalg.eigh(K)\n\t        q = V[[3, 0, 1, 2], np.argmax(w)]\n\t    if q[0] < 0.0:\n\t        np.negative(q, q)\n\t    return q\n\tdef compute_rotation_angle_diff(R_gt,R):\n\t    eps = 1e-15\n\t    q_gt = quaternion_from_matrix(R_gt)\n\t    q = quaternion_from_matrix(R)\n\t    q = q / (np.linalg.norm(q) + eps)\n", "    q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\t    loss_q = np.maximum(eps, (1.0 - np.sum(q * q_gt)**2))\n\t    err_q = np.arccos(1 - 2 * loss_q)\n\t    return np.rad2deg(np.abs(err_q))\n\tdef compute_translation_angle_diff(t_gt,t):\n\t    eps=1e-15\n\t    t = t / (np.linalg.norm(t) + eps)\n\t    t_gt = t_gt / (np.linalg.norm(t_gt) + eps)\n\t    loss_t = np.maximum(eps, (1.0 - np.sum(t * t_gt)**2))\n\t    err_t = np.arccos(np.sqrt(1 - loss_t))\n", "    return np.rad2deg(np.abs(err_t))\n\tdef get_identity_pose():\n\t    return np.concatenate([np.identity(3),np.zeros([3,1])],1).astype(np.float32)\n\tdef pose_inverse(pose):\n\t    R = pose[:,:3].T\n\t    t = - R @ pose[:,3:]\n\t    return np.concatenate([R,t],-1)\n\tdef similarity_pose_inverse(pose):\n\t    A = np.linalg.inv(pose[:,:3])\n\t    t = - A @ pose[:,3:]\n", "    return np.concatenate([A,t],-1)\n\tdef pose_compose(pose0, pose1):\n\t    \"\"\"\n\t    apply pose0 first, then pose1\n\t    :param pose0:\n\t    :param pose1:\n\t    :return:\n\t    \"\"\"\n\t    t = pose1[:,:3] @ pose0[:,3:] + pose1[:,3:]\n\t    R = pose1[:,:3] @ pose0[:,:3]\n", "    return np.concatenate([R,t], 1)\n\tdef pose_apply(pose,pts):\n\t    return transform_points_pose(pts, pose)\n\tdef to_cuda(data):\n\t    if type(data)==list:\n\t        results = []\n\t        for i, item in enumerate(data):\n\t            results.append(to_cuda(item))\n\t        return results\n\t    elif type(data)==dict:\n", "        results={}\n\t        for k,v in data.items():\n\t            results[k]=to_cuda(v)\n\t        return results\n\t    elif type(data).__name__ == \"Tensor\":\n\t        return data.cuda()\n\t    else:\n\t        return data\n\tdef to_cpu_numpy(data):\n\t    if type(data)==list:\n", "        results = []\n\t        for i, item in enumerate(data):\n\t            results.append(to_cpu_numpy(item))\n\t        return results\n\t    elif type(data)==dict:\n\t        results={}\n\t        for k,v in data.items():\n\t            results[k]=to_cpu_numpy(v)\n\t        return results\n\t    elif type(data).__name__ == \"Tensor\":\n", "        return data.detach().cpu().numpy()\n\t    else:\n\t        return data\n\tdef sample_fps_points(points, sample_num, init_center=True, index_model=False, init_first=False, init_first_index=0, init_point=None):\n\t    sample_num = min(points.shape[0],sample_num)\n\t    output_index=[]\n\t    if init_point is None:\n\t        if init_center:\n\t            init_point=np.mean(points,0)\n\t        else:\n", "            if init_first:\n\t                init_index=init_first_index\n\t            else:\n\t                init_index=np.random.randint(0, points.shape[0])\n\t            init_point=points[init_index]\n\t            output_index.append(init_index)\n\t    output_points=[init_point]\n\t    cur_point=init_point\n\t    distance=np.full(points.shape[0],1e8)\n\t    for k in range(min(sample_num-1, points.shape[0]-1)):\n", "        cur_distance=np.linalg.norm(cur_point[None,:]-points,2,1)\n\t        distance=np.min(np.stack([cur_distance,distance],1),1)\n\t        cur_index=np.argmax(distance)\n\t        cur_point=points[cur_index]\n\t        output_points.append(cur_point)\n\t        output_index.append(cur_index)\n\t    if index_model:\n\t        return np.asarray(output_index)\n\t    else:\n\t        return np.asarray(output_points)\n", "def triangulate(kps0,kps1,pose0,pose1,K0,K1):\n\t    kps0_ = hpts_to_pts(pts_to_hpts(kps0) @ np.linalg.inv(K0).T)\n\t    kps1_ = hpts_to_pts(pts_to_hpts(kps1) @ np.linalg.inv(K1).T)\n\t    pts3d = cv2.triangulatePoints(pose0.astype(np.float64),pose1.astype(np.float64),\n\t                                  kps0_.T.astype(np.float64),kps1_.T.astype(np.float64)).T\n\t    pts3d = pts3d[:,:3]/pts3d[:,3:]\n\t    return pts3d\n\tdef transformation_inverse_2d(trans):\n\t    A=trans[:2,:2] # 2,2\n\t    t=trans[:,2:]  # 2,1\n", "    if isinstance(trans, Tensor):\n\t        A_ = np.linalg.inv(A)\n\t        t_ = -A_ @ t\n\t        return torch.cat([A_, t_], 1)\n\t    elif isinstance(trans, ndarray):\n\t        A_ = np.linalg.inv(A)\n\t        t_ = -A_ @ t\n\t        return np.concatenate([A_, t_], 1)\n\t    else:\n\t        raise NotImplementedError\n", "def transformation_compose_2d(trans0, trans1):\n\t    \"\"\"\n\t    @param trans0: [2,3]\n\t    @param trans1: [2,3]\n\t    @return: apply trans0 then trans1\n\t    \"\"\"\n\t    t1 = trans1[:, 2]\n\t    t0 = trans0[:, 2]\n\t    R1 = trans1[:, :2]\n\t    R0 = trans0[:, :2]\n", "    R = R1 @ R0\n\t    t = R1 @ t0 + t1\n\t    return np.concatenate([R, t[:, None]], 1)\n\tdef transformation_apply_2d(trans,points):\n\t    return points @ trans[:,:2].T + trans[:,2:].T\n\tdef angle_to_rotation_2d(angle):\n\t    return np.asarray([[np.cos(angle),-np.sin(angle)],\n\t                       [np.sin(angle),np.cos(angle)]])\n\tdef transformation_offset_2d(x,y):\n\t    return np.concatenate([np.eye(2),np.asarray([x,y])[:,None]],1).astype(np.float32)\n", "def transformation_scale_2d(scale):\n\t    return np.concatenate([np.diag([scale,scale]),np.zeros([2,1])],1).astype(np.float32)\n\tdef transformation_rotation_2d(ang):\n\t    return np.concatenate([angle_to_rotation_2d(ang),np.zeros([2,1])],1).astype(np.float32)\n\tdef transformation_decompose_2d(M):\n\t    scale = np.sqrt(np.linalg.det(M[:2, :2]))\n\t    rotation = np.arctan2(M[1, 0], M[0, 0])\n\t    offset = M[:2,2]\n\t    return scale, rotation, offset\n\tdef transformation_crop(img, position, scale, angle, size, new_position=None):\n", "    M = transformation_offset_2d(-position[0], -position[1])\n\t    M = transformation_compose_2d(M, transformation_scale_2d(scale))\n\t    M = transformation_compose_2d(M, transformation_rotation_2d(angle))\n\t    if new_position is None:\n\t        M = transformation_compose_2d(M, transformation_offset_2d(size / 2, size / 2))\n\t    else:\n\t        M = transformation_compose_2d(M, transformation_offset_2d(new_position[0], new_position[1]))\n\t    img_region = cv2.warpAffine(img, M, (size, size), flags=cv2.INTER_LINEAR)\n\t    return img_region, M\n\tdef look_at_rotation(point):\n", "    \"\"\"\n\t    @param point: point in normalized image coordinate not in pixels\n\t    @return: R\n\t    R @ x_raw -> x_lookat\n\t    \"\"\"\n\t    x, y = point\n\t    R1 = euler2mat(-np.arctan2(x, 1),0,0,'syxz')\n\t    R2 = euler2mat(np.arctan2(y, 1),0,0,'sxyz')\n\t    return R2 @ R1\n"]}
{"filename": "utils/dataset_utils.py", "chunked_list": ["import numpy as np\n\timport time\n\timport random\n\timport torch\n\tdef dummy_collate_fn(data_list):\n\t    return data_list[0]\n\tdef simple_collate_fn(data_list):\n\t    ks=data_list[0].keys()\n\t    outputs={k:[] for k in ks}\n\t    for k in ks:\n", "        if isinstance(data_list[0][k], dict):\n\t            outputs[k] = {k_: [] for k_ in data_list[0][k].keys()}\n\t            for k_ in data_list[0][k].keys():\n\t                for data in data_list:\n\t                    outputs[k][k_].append(data[k][k_])\n\t                outputs[k][k_]=torch.stack(outputs[k][k_], 0)\n\t        else:\n\t            for data in data_list:\n\t                outputs[k].append(data[k])\n\t            if isinstance(data_list[0][k], torch.Tensor):\n", "                outputs[k]=torch.stack(outputs[k],0)\n\t    return outputs\n\tdef set_seed(index,is_train):\n\t    if is_train:\n\t        np.random.seed((index+int(time.time()))%(2**16))\n\t        random.seed((index+int(time.time()))%(2**16)+1)\n\t        torch.random.manual_seed((index+int(time.time()))%(2**16)+1)\n\t    else:\n\t        np.random.seed(index % (2 ** 16))\n\t        random.seed(index % (2 ** 16) + 1)\n", "        torch.random.manual_seed(index % (2 ** 16) + 1)"]}
{"filename": "utils/read_write_model.py", "chunked_list": ["import os\n\timport collections\n\timport numpy as np\n\timport struct\n\timport argparse\n\timport logging\n\tCameraModel = collections.namedtuple(\n\t    \"CameraModel\", [\"model_id\", \"model_name\", \"num_params\"])\n\tCamera = collections.namedtuple(\n\t    \"Camera\", [\"id\", \"model\", \"width\", \"height\", \"params\"])\n", "BaseImage = collections.namedtuple(\n\t    \"Image\", [\"id\", \"qvec\", \"tvec\", \"camera_id\", \"name\", \"xys\", \"point3D_ids\"])\n\tPoint3D = collections.namedtuple(\n\t    \"Point3D\", [\"id\", \"xyz\", \"rgb\", \"error\", \"image_ids\", \"point2D_idxs\"])\n\tclass Image(BaseImage):\n\t    def qvec2rotmat(self):\n\t        return qvec2rotmat(self.qvec)\n\tCAMERA_MODELS = {\n\t    CameraModel(model_id=0, model_name=\"SIMPLE_PINHOLE\", num_params=3),\n\t    CameraModel(model_id=1, model_name=\"PINHOLE\", num_params=4),\n", "    CameraModel(model_id=2, model_name=\"SIMPLE_RADIAL\", num_params=4),\n\t    CameraModel(model_id=3, model_name=\"RADIAL\", num_params=5),\n\t    CameraModel(model_id=4, model_name=\"OPENCV\", num_params=8),\n\t    CameraModel(model_id=5, model_name=\"OPENCV_FISHEYE\", num_params=8),\n\t    CameraModel(model_id=6, model_name=\"FULL_OPENCV\", num_params=12),\n\t    CameraModel(model_id=7, model_name=\"FOV\", num_params=5),\n\t    CameraModel(model_id=8, model_name=\"SIMPLE_RADIAL_FISHEYE\", num_params=4),\n\t    CameraModel(model_id=9, model_name=\"RADIAL_FISHEYE\", num_params=5),\n\t    CameraModel(model_id=10, model_name=\"THIN_PRISM_FISHEYE\", num_params=12)\n\t}\n", "CAMERA_MODEL_IDS = dict([(camera_model.model_id, camera_model)\n\t                         for camera_model in CAMERA_MODELS])\n\tCAMERA_MODEL_NAMES = dict([(camera_model.model_name, camera_model)\n\t                           for camera_model in CAMERA_MODELS])\n\tdef read_next_bytes(fid, num_bytes, format_char_sequence, endian_character=\"<\"):\n\t    \"\"\"Read and unpack the next bytes from a binary file.\n\t    :param fid:\n\t    :param num_bytes: Sum of combination of {2, 4, 8}, e.g. 2, 6, 16, 30, etc.\n\t    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.\n\t    :param endian_character: Any of {@, =, <, >, !}\n", "    :return: Tuple of read and unpacked values.\n\t    \"\"\"\n\t    data = fid.read(num_bytes)\n\t    return struct.unpack(endian_character + format_char_sequence, data)\n\tdef write_next_bytes(fid, data, format_char_sequence, endian_character=\"<\"):\n\t    \"\"\"pack and write to a binary file.\n\t    :param fid:\n\t    :param data: data to send, if multiple elements are sent at the same time,\n\t    they should be encapsuled either in a list or a tuple\n\t    :param format_char_sequence: List of {c, e, f, d, h, H, i, I, l, L, q, Q}.\n", "    should be the same length as the data list or tuple\n\t    :param endian_character: Any of {@, =, <, >, !}\n\t    \"\"\"\n\t    if isinstance(data, (list, tuple)):\n\t        bytes = struct.pack(endian_character + format_char_sequence, *data)\n\t    else:\n\t        bytes = struct.pack(endian_character + format_char_sequence, data)\n\t    fid.write(bytes)\n\tdef read_cameras_text(path):\n\t    \"\"\"\n", "    see: src/base/reconstruction.cc\n\t        void Reconstruction::WriteCamerasText(const std::string& path)\n\t        void Reconstruction::ReadCamerasText(const std::string& path)\n\t    \"\"\"\n\t    cameras = {}\n\t    with open(path, \"r\") as fid:\n\t        while True:\n\t            line = fid.readline()\n\t            if not line:\n\t                break\n", "            line = line.strip()\n\t            if len(line) > 0 and line[0] != \"#\":\n\t                elems = line.split()\n\t                camera_id = int(elems[0])\n\t                model = elems[1]\n\t                width = int(elems[2])\n\t                height = int(elems[3])\n\t                params = np.array(tuple(map(float, elems[4:])))\n\t                cameras[camera_id] = Camera(id=camera_id, model=model,\n\t                                            width=width, height=height,\n", "                                            params=params)\n\t    return cameras\n\tdef read_cameras_binary(path_to_model_file):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::WriteCamerasBinary(const std::string& path)\n\t        void Reconstruction::ReadCamerasBinary(const std::string& path)\n\t    \"\"\"\n\t    cameras = {}\n\t    with open(path_to_model_file, \"rb\") as fid:\n", "        num_cameras = read_next_bytes(fid, 8, \"Q\")[0]\n\t        for _ in range(num_cameras):\n\t            camera_properties = read_next_bytes(\n\t                fid, num_bytes=24, format_char_sequence=\"iiQQ\")\n\t            camera_id = camera_properties[0]\n\t            model_id = camera_properties[1]\n\t            model_name = CAMERA_MODEL_IDS[camera_properties[1]].model_name\n\t            width = camera_properties[2]\n\t            height = camera_properties[3]\n\t            num_params = CAMERA_MODEL_IDS[model_id].num_params\n", "            params = read_next_bytes(fid, num_bytes=8*num_params,\n\t                                     format_char_sequence=\"d\"*num_params)\n\t            cameras[camera_id] = Camera(id=camera_id,\n\t                                        model=model_name,\n\t                                        width=width,\n\t                                        height=height,\n\t                                        params=np.array(params))\n\t        assert len(cameras) == num_cameras\n\t    return cameras\n\tdef write_cameras_text(cameras, path):\n", "    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::WriteCamerasText(const std::string& path)\n\t        void Reconstruction::ReadCamerasText(const std::string& path)\n\t    \"\"\"\n\t    HEADER = \"# Camera list with one line of data per camera:\\n\" + \\\n\t             \"#   CAMERA_ID, MODEL, WIDTH, HEIGHT, PARAMS[]\\n\" + \\\n\t             \"# Number of cameras: {}\\n\".format(len(cameras))\n\t    with open(path, \"w\") as fid:\n\t        fid.write(HEADER)\n", "        for _, cam in cameras.items():\n\t            to_write = [cam.id, cam.model, cam.width, cam.height, *cam.params]\n\t            line = \" \".join([str(elem) for elem in to_write])\n\t            fid.write(line + \"\\n\")\n\tdef write_cameras_binary(cameras, path_to_model_file):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::WriteCamerasBinary(const std::string& path)\n\t        void Reconstruction::ReadCamerasBinary(const std::string& path)\n\t    \"\"\"\n", "    with open(path_to_model_file, \"wb\") as fid:\n\t        write_next_bytes(fid, len(cameras), \"Q\")\n\t        for _, cam in cameras.items():\n\t            model_id = CAMERA_MODEL_NAMES[cam.model].model_id\n\t            camera_properties = [cam.id,\n\t                                 model_id,\n\t                                 cam.width,\n\t                                 cam.height]\n\t            write_next_bytes(fid, camera_properties, \"iiQQ\")\n\t            for p in cam.params:\n", "                write_next_bytes(fid, float(p), \"d\")\n\t    return cameras\n\tdef read_images_text(path):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::ReadImagesText(const std::string& path)\n\t        void Reconstruction::WriteImagesText(const std::string& path)\n\t    \"\"\"\n\t    images = {}\n\t    with open(path, \"r\") as fid:\n", "        while True:\n\t            line = fid.readline()\n\t            if not line:\n\t                break\n\t            line = line.strip()\n\t            if len(line) > 0 and line[0] != \"#\":\n\t                elems = line.split()\n\t                image_id = int(elems[0])\n\t                qvec = np.array(tuple(map(float, elems[1:5])))\n\t                tvec = np.array(tuple(map(float, elems[5:8])))\n", "                camera_id = int(elems[8])\n\t                image_name = elems[9]\n\t                elems = fid.readline().split()\n\t                xys = np.column_stack([tuple(map(float, elems[0::3])),\n\t                                       tuple(map(float, elems[1::3]))])\n\t                point3D_ids = np.array(tuple(map(int, elems[2::3])))\n\t                images[image_id] = Image(\n\t                    id=image_id, qvec=qvec, tvec=tvec,\n\t                    camera_id=camera_id, name=image_name,\n\t                    xys=xys, point3D_ids=point3D_ids)\n", "    return images\n\tdef read_images_binary(path_to_model_file):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::ReadImagesBinary(const std::string& path)\n\t        void Reconstruction::WriteImagesBinary(const std::string& path)\n\t    \"\"\"\n\t    images = {}\n\t    with open(path_to_model_file, \"rb\") as fid:\n\t        num_reg_images = read_next_bytes(fid, 8, \"Q\")[0]\n", "        for _ in range(num_reg_images):\n\t            binary_image_properties = read_next_bytes(\n\t                fid, num_bytes=64, format_char_sequence=\"idddddddi\")\n\t            image_id = binary_image_properties[0]\n\t            qvec = np.array(binary_image_properties[1:5])\n\t            tvec = np.array(binary_image_properties[5:8])\n\t            camera_id = binary_image_properties[8]\n\t            image_name = \"\"\n\t            current_char = read_next_bytes(fid, 1, \"c\")[0]\n\t            while current_char != b\"\\x00\":   # look for the ASCII 0 entry\n", "                image_name += current_char.decode(\"utf-8\")\n\t                current_char = read_next_bytes(fid, 1, \"c\")[0]\n\t            num_points2D = read_next_bytes(fid, num_bytes=8,\n\t                                           format_char_sequence=\"Q\")[0]\n\t            x_y_id_s = read_next_bytes(fid, num_bytes=24*num_points2D,\n\t                                       format_char_sequence=\"ddq\"*num_points2D)\n\t            xys = np.column_stack([tuple(map(float, x_y_id_s[0::3])),\n\t                                   tuple(map(float, x_y_id_s[1::3]))])\n\t            point3D_ids = np.array(tuple(map(int, x_y_id_s[2::3])))\n\t            images[image_id] = Image(\n", "                id=image_id, qvec=qvec, tvec=tvec,\n\t                camera_id=camera_id, name=image_name,\n\t                xys=xys, point3D_ids=point3D_ids)\n\t    return images\n\tdef write_images_text(images, path):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::ReadImagesText(const std::string& path)\n\t        void Reconstruction::WriteImagesText(const std::string& path)\n\t    \"\"\"\n", "    if len(images) == 0:\n\t        mean_observations = 0\n\t    else:\n\t        mean_observations = sum((len(img.point3D_ids) for _, img in images.items()))/len(images)\n\t    HEADER = \"# Image list with two lines of data per image:\\n\" + \\\n\t             \"#   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\\n\" + \\\n\t             \"#   POINTS2D[] as (X, Y, POINT3D_ID)\\n\" + \\\n\t             \"# Number of images: {}, mean observations per image: {}\\n\".format(len(images), mean_observations)\n\t    with open(path, \"w\") as fid:\n\t        fid.write(HEADER)\n", "        for _, img in images.items():\n\t            image_header = [img.id, *img.qvec, *img.tvec, img.camera_id, img.name]\n\t            first_line = \" \".join(map(str, image_header))\n\t            fid.write(first_line + \"\\n\")\n\t            points_strings = []\n\t            for xy, point3D_id in zip(img.xys, img.point3D_ids):\n\t                points_strings.append(\" \".join(map(str, [*xy, point3D_id])))\n\t            fid.write(\" \".join(points_strings) + \"\\n\")\n\tdef write_images_binary(images, path_to_model_file):\n\t    \"\"\"\n", "    see: src/base/reconstruction.cc\n\t        void Reconstruction::ReadImagesBinary(const std::string& path)\n\t        void Reconstruction::WriteImagesBinary(const std::string& path)\n\t    \"\"\"\n\t    with open(path_to_model_file, \"wb\") as fid:\n\t        write_next_bytes(fid, len(images), \"Q\")\n\t        for _, img in images.items():\n\t            write_next_bytes(fid, img.id, \"i\")\n\t            write_next_bytes(fid, img.qvec.tolist(), \"dddd\")\n\t            write_next_bytes(fid, img.tvec.tolist(), \"ddd\")\n", "            write_next_bytes(fid, img.camera_id, \"i\")\n\t            for char in img.name:\n\t                write_next_bytes(fid, char.encode(\"utf-8\"), \"c\")\n\t            write_next_bytes(fid, b\"\\x00\", \"c\")\n\t            write_next_bytes(fid, len(img.point3D_ids), \"Q\")\n\t            for xy, p3d_id in zip(img.xys, img.point3D_ids):\n\t                write_next_bytes(fid, [*xy, p3d_id], \"ddq\")\n\tdef read_points3D_text(path):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n", "        void Reconstruction::ReadPoints3DText(const std::string& path)\n\t        void Reconstruction::WritePoints3DText(const std::string& path)\n\t    \"\"\"\n\t    points3D = {}\n\t    with open(path, \"r\") as fid:\n\t        while True:\n\t            line = fid.readline()\n\t            if not line:\n\t                break\n\t            line = line.strip()\n", "            if len(line) > 0 and line[0] != \"#\":\n\t                elems = line.split()\n\t                point3D_id = int(elems[0])\n\t                xyz = np.array(tuple(map(float, elems[1:4])))\n\t                rgb = np.array(tuple(map(int, elems[4:7])))\n\t                error = float(elems[7])\n\t                image_ids = np.array(tuple(map(int, elems[8::2])))\n\t                point2D_idxs = np.array(tuple(map(int, elems[9::2])))\n\t                points3D[point3D_id] = Point3D(id=point3D_id, xyz=xyz, rgb=rgb,\n\t                                               error=error, image_ids=image_ids,\n", "                                               point2D_idxs=point2D_idxs)\n\t    return points3D\n\tdef read_points3D_binary(path_to_model_file):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::ReadPoints3DBinary(const std::string& path)\n\t        void Reconstruction::WritePoints3DBinary(const std::string& path)\n\t    \"\"\"\n\t    points3D = {}\n\t    with open(path_to_model_file, \"rb\") as fid:\n", "        num_points = read_next_bytes(fid, 8, \"Q\")[0]\n\t        for _ in range(num_points):\n\t            binary_point_line_properties = read_next_bytes(\n\t                fid, num_bytes=43, format_char_sequence=\"QdddBBBd\")\n\t            point3D_id = binary_point_line_properties[0]\n\t            xyz = np.array(binary_point_line_properties[1:4])\n\t            rgb = np.array(binary_point_line_properties[4:7])\n\t            error = np.array(binary_point_line_properties[7])\n\t            track_length = read_next_bytes(\n\t                fid, num_bytes=8, format_char_sequence=\"Q\")[0]\n", "            track_elems = read_next_bytes(\n\t                fid, num_bytes=8*track_length,\n\t                format_char_sequence=\"ii\"*track_length)\n\t            image_ids = np.array(tuple(map(int, track_elems[0::2])))\n\t            point2D_idxs = np.array(tuple(map(int, track_elems[1::2])))\n\t            points3D[point3D_id] = Point3D(\n\t                id=point3D_id, xyz=xyz, rgb=rgb,\n\t                error=error, image_ids=image_ids,\n\t                point2D_idxs=point2D_idxs)\n\t    return points3D\n", "def write_points3D_text(points3D, path):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::ReadPoints3DText(const std::string& path)\n\t        void Reconstruction::WritePoints3DText(const std::string& path)\n\t    \"\"\"\n\t    if len(points3D) == 0:\n\t        mean_track_length = 0\n\t    else:\n\t        mean_track_length = sum((len(pt.image_ids) for _, pt in points3D.items()))/len(points3D)\n", "    HEADER = \"# 3D point list with one line of data per point:\\n\" + \\\n\t             \"#   POINT3D_ID, X, Y, Z, R, G, B, ERROR, TRACK[] as (IMAGE_ID, POINT2D_IDX)\\n\" + \\\n\t             \"# Number of points: {}, mean track length: {}\\n\".format(len(points3D), mean_track_length)\n\t    with open(path, \"w\") as fid:\n\t        fid.write(HEADER)\n\t        for _, pt in points3D.items():\n\t            point_header = [pt.id, *pt.xyz, *pt.rgb, pt.error]\n\t            fid.write(\" \".join(map(str, point_header)) + \" \")\n\t            track_strings = []\n\t            for image_id, point2D in zip(pt.image_ids, pt.point2D_idxs):\n", "                track_strings.append(\" \".join(map(str, [image_id, point2D])))\n\t            fid.write(\" \".join(track_strings) + \"\\n\")\n\tdef write_points3D_binary(points3D, path_to_model_file):\n\t    \"\"\"\n\t    see: src/base/reconstruction.cc\n\t        void Reconstruction::ReadPoints3DBinary(const std::string& path)\n\t        void Reconstruction::WritePoints3DBinary(const std::string& path)\n\t    \"\"\"\n\t    with open(path_to_model_file, \"wb\") as fid:\n\t        write_next_bytes(fid, len(points3D), \"Q\")\n", "        for _, pt in points3D.items():\n\t            write_next_bytes(fid, pt.id, \"Q\")\n\t            write_next_bytes(fid, pt.xyz.tolist(), \"ddd\")\n\t            write_next_bytes(fid, pt.rgb.tolist(), \"BBB\")\n\t            write_next_bytes(fid, pt.error, \"d\")\n\t            track_length = pt.image_ids.shape[0]\n\t            write_next_bytes(fid, track_length, \"Q\")\n\t            for image_id, point2D_id in zip(pt.image_ids, pt.point2D_idxs):\n\t                write_next_bytes(fid, [image_id, point2D_id], \"ii\")\n\tdef detect_model_format(path, ext):\n", "    if os.path.isfile(os.path.join(path, \"cameras\"  + ext)) and \\\n\t       os.path.isfile(os.path.join(path, \"images\"   + ext)) and \\\n\t       os.path.isfile(os.path.join(path, \"points3D\" + ext)):\n\t        return True\n\t    return False\n\tdef read_model(path, ext=\"\"):\n\t    # try to detect the extension automatically\n\t    if ext == \"\":\n\t        if detect_model_format(path, \".bin\"):\n\t            ext = \".bin\"\n", "        elif detect_model_format(path, \".txt\"):\n\t            ext = \".txt\"\n\t        else:\n\t            try:\n\t                cameras, images, points3D = read_model(os.path.join(path, \"model/\"))\n\t                logging.warning(\n\t                    \"This SfM file structure was deprecated in hloc v1.1\")\n\t                return cameras, images, points3D\n\t            except FileNotFoundError:\n\t                raise FileNotFoundError(\n", "                    f\"Could not find binary or text COLMAP model at {path}\")\n\t    if ext == \".txt\":\n\t        cameras = read_cameras_text(os.path.join(path, \"cameras\" + ext))\n\t        images = read_images_text(os.path.join(path, \"images\" + ext))\n\t        points3D = read_points3D_text(os.path.join(path, \"points3D\") + ext)\n\t    else:\n\t        cameras = read_cameras_binary(os.path.join(path, \"cameras\" + ext))\n\t        images = read_images_binary(os.path.join(path, \"images\" + ext))\n\t        points3D = read_points3D_binary(os.path.join(path, \"points3D\") + ext)\n\t    return cameras, images, points3D\n", "def write_model(cameras, images, points3D, path, ext=\".bin\"):\n\t    if ext == \".txt\":\n\t        write_cameras_text(cameras, os.path.join(path, \"cameras\" + ext))\n\t        write_images_text(images, os.path.join(path, \"images\" + ext))\n\t        write_points3D_text(points3D, os.path.join(path, \"points3D\") + ext)\n\t    else:\n\t        write_cameras_binary(cameras, os.path.join(path, \"cameras\" + ext))\n\t        write_images_binary(images, os.path.join(path, \"images\" + ext))\n\t        write_points3D_binary(points3D, os.path.join(path, \"points3D\") + ext)\n\t    return cameras, images, points3D\n", "def qvec2rotmat(qvec):\n\t    return np.array([\n\t        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n\t         2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n\t         2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],\n\t        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n\t         1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n\t         2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],\n\t        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n\t         2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n", "         1 - 2 * qvec[1]**2 - 2 * qvec[2]**2]])\n\tdef rotmat2qvec(R):\n\t    Rxx, Ryx, Rzx, Rxy, Ryy, Rzy, Rxz, Ryz, Rzz = R.flat\n\t    K = np.array([\n\t        [Rxx - Ryy - Rzz, 0, 0, 0],\n\t        [Ryx + Rxy, Ryy - Rxx - Rzz, 0, 0],\n\t        [Rzx + Rxz, Rzy + Ryz, Rzz - Rxx - Ryy, 0],\n\t        [Ryz - Rzy, Rzx - Rxz, Rxy - Ryx, Rxx + Ryy + Rzz]]) / 3.0\n\t    eigvals, eigvecs = np.linalg.eigh(K)\n\t    qvec = eigvecs[[3, 0, 1, 2], np.argmax(eigvals)]\n", "    if qvec[0] < 0:\n\t        qvec *= -1\n\t    return qvec\n\tdef main():\n\t    parser = argparse.ArgumentParser(description=\"Read and write COLMAP binary and text models\")\n\t    parser.add_argument(\"--input_model\", help=\"path to input model folder\")\n\t    parser.add_argument(\"--input_format\", choices=[\".bin\", \".txt\"],\n\t                        help=\"input model format\", default=\"\")\n\t    parser.add_argument(\"--output_model\",\n\t                        help=\"path to output model folder\")\n", "    parser.add_argument(\"--output_format\", choices=[\".bin\", \".txt\"],\n\t                        help=\"outut model format\", default=\".txt\")\n\t    args = parser.parse_args()\n\t    cameras, images, points3D = read_model(path=args.input_model, ext=args.input_format)\n\t    print(\"num_cameras:\", len(cameras))\n\t    print(\"num_images:\", len(images))\n\t    print(\"num_points3D:\", len(points3D))\n\t    if args.output_model is not None:\n\t        write_model(cameras, images, points3D, path=args.output_model, ext=args.output_format)\n\tif __name__ == \"__main__\":\n", "    main()\n"]}
{"filename": "utils/draw_utils.py", "chunked_list": ["import matplotlib\n\tmatplotlib.use('Agg')\n\tfrom utils.base_utils import compute_relative_transformation, compute_F\n\timport numpy as np\n\timport cv2\n\timport matplotlib.pyplot as plt\n\timport matplotlib.lines as mlines\n\tfrom matplotlib import cm\n\tdef newline(p1, p2):\n\t    ax = plt.gca()\n", "    xmin, xmax = ax.get_xbound()\n\t    if p2[0] == p1[0]:\n\t        xmin = xmax = p1[0]\n\t        ymin, ymax = ax.get_ybound()\n\t    else:\n\t        ymax = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmax-p1[0])\n\t        ymin = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmin-p1[0])\n\t    l = mlines.Line2D([xmin,xmax], [ymin,ymax])\n\t    ax.add_line(l)\n\t    return l\n", "def draw_correspondence(img0, img1, kps0, kps1, matches=None, colors=None, max_draw_line_num=None, kps_color=(0,0,255),vert=False):\n\t    if len(img0.shape)==2:\n\t        img0=np.repeat(img0[:,:,None],3,2)\n\t    if len(img1.shape)==2:\n\t        img1=np.repeat(img1[:,:,None],3,2)\n\t    h0, w0 = img0.shape[:2]\n\t    h1, w1 = img1.shape[:2]\n\t    if matches is None:\n\t        assert(kps0.shape[0]==kps1.shape[0])\n\t        matches=np.repeat(np.arange(kps0.shape[0])[:,None],2,1)\n", "    if vert:\n\t        w = max(w0, w1)\n\t        h = h0 + h1\n\t        out_img = np.zeros([h, w, 3], np.uint8)\n\t        out_img[:h0, :w0] = img0\n\t        out_img[h0:, :w1] = img1\n\t    else:\n\t        h = max(h0, h1)\n\t        w = w0 + w1\n\t        out_img = np.zeros([h, w, 3], np.uint8)\n", "        out_img[:h0, :w0] = img0\n\t        out_img[:h1, w0:] = img1\n\t    for pt in kps0:\n\t        pt = np.round(pt).astype(np.int32)\n\t        cv2.circle(out_img, tuple(pt), 1, kps_color, -1)\n\t    for pt in kps1:\n\t        pt = np.round(pt).astype(np.int32)\n\t        pt = pt.copy()\n\t        if vert:\n\t            pt[1] += h0\n", "        else:\n\t            pt[0] += w0\n\t        cv2.circle(out_img, tuple(pt), 1, kps_color, -1)\n\t    if max_draw_line_num is not None and matches.shape[0]>max_draw_line_num:\n\t        np.random.seed(6033)\n\t        idxs=np.arange(matches.shape[0])\n\t        np.random.shuffle(idxs)\n\t        idxs=idxs[:max_draw_line_num]\n\t        matches= matches[idxs]\n\t        if colors is not None and (type(colors)==list or type(colors)==np.ndarray):\n", "            colors=np.asarray(colors)\n\t            colors= colors[idxs]\n\t    for mi,m in enumerate(matches):\n\t        pt = np.round(kps0[m[0]]).astype(np.int32)\n\t        pr_pt = np.round(kps1[m[1]]).astype(np.int32)\n\t        if vert:\n\t            pr_pt[1] += h0\n\t        else:\n\t            pr_pt[0] += w0\n\t        if colors is None:\n", "            cv2.line(out_img, tuple(pt), tuple(pr_pt), (0, 255, 0), 1)\n\t        elif type(colors)==list or type(colors)==np.ndarray:\n\t            color=(int(c) for c in colors[mi])\n\t            cv2.line(out_img, tuple(pt), tuple(pr_pt), tuple(color), 1)\n\t        else:\n\t            color=(int(c) for c in colors)\n\t            cv2.line(out_img, tuple(pt), tuple(pr_pt), tuple(color), 1)\n\t    return out_img\n\tdef draw_keypoints(img, kps, colors=None, radius=2):\n\t    out_img=img.copy()\n", "    for pi, pt in enumerate(kps):\n\t        pt = np.round(pt).astype(np.int32)\n\t        if colors is not None:\n\t            color=[int(c) for c in colors[pi]]\n\t            cv2.circle(out_img, tuple(pt), radius, color, -1)\n\t        else:\n\t            cv2.circle(out_img, tuple(pt), radius, (0,255,0), -1)\n\t    return out_img\n\tdef draw_epipolar_line(F, img0, img1, pt0, color):\n\t    h1,w1=img1.shape[:2]\n", "    hpt = np.asarray([pt0[0], pt0[1], 1], dtype=np.float32)[:, None]\n\t    l = F @ hpt\n\t    l = l[:, 0]\n\t    a, b, c = l[0], l[1], l[2]\n\t    pt1 = np.asarray([0, -c / b]).astype(np.int32)\n\t    pt2 = np.asarray([w1, (-a * w1 - c) / b]).astype(np.int32)\n\t    img0 = cv2.circle(img0, tuple(pt0.astype(np.int32)), 5, color, 2)\n\t    img1 = cv2.line(img1, tuple(pt1), tuple(pt2), color, 2)\n\t    return img0, img1\n\tdef draw_epipolar_lines(F, img0, img1,num=20):\n", "    img0,img1=img0.copy(),img1.copy()\n\t    h0, w0, _ = img0.shape\n\t    h1, w1, _ = img1.shape\n\t    for k in range(num):\n\t        color = np.random.randint(0, 255, [3], dtype=np.int32)\n\t        color = [int(c) for c in color]\n\t        pt = np.random.uniform(0, 1, 2)\n\t        pt[0] *= w0\n\t        pt[1] *= h0\n\t        pt = pt.astype(np.int32)\n", "        img0, img1 = draw_epipolar_line(F, img0, img1, pt, color)\n\t    return img0, img1\n\tdef gen_color_map(error, clip_max=12.0, clip_min=2.0):\n\t    rectified_error=(error-clip_min)/(clip_max-clip_min)\n\t    rectified_error[rectified_error<0]=0\n\t    rectified_error[rectified_error>=1.0]=1.0\n\t    viridis=cm.get_cmap('viridis',256)\n\t    colors=[viridis(e) for e in rectified_error]\n\t    return np.asarray(np.asarray(colors)[:,:3]*255,np.uint8)\n\tdef scale_float_image(image):\n", "    max_val, min_val = np.max(image), np.min(image)\n\t    image = (image - min_val) / (max_val - min_val) * 255\n\t    return image.astype(np.uint8)\n\tdef concat_images(img0,img1,vert=False):\n\t    if not vert:\n\t        h0,h1=img0.shape[0],img1.shape[0],\n\t        if h0<h1: img0=cv2.copyMakeBorder(img0,0,h1-h0,0,0,borderType=cv2.BORDER_CONSTANT,value=0)\n\t        if h1<h0: img1=cv2.copyMakeBorder(img1,0,h0-h1,0,0,borderType=cv2.BORDER_CONSTANT,value=0)\n\t        img = np.concatenate([img0, img1], axis=1)\n\t    else:\n", "        w0,w1=img0.shape[1],img1.shape[1]\n\t        if w0<w1: img0=cv2.copyMakeBorder(img0,0,0,0,w1-w0,borderType=cv2.BORDER_CONSTANT,value=0)\n\t        if w1<w0: img1=cv2.copyMakeBorder(img1,0,0,0,w0-w1,borderType=cv2.BORDER_CONSTANT,value=0)\n\t        img = np.concatenate([img0, img1], axis=0)\n\t    return img\n\tdef concat_images_list(*args,vert=False):\n\t    if len(args)==1: return args[0]\n\t    img_out=args[0]\n\t    for img in args[1:]:\n\t        img_out=concat_images(img_out,img,vert)\n", "    return img_out\n\tdef get_colors_gt_pr(gt,pr=None):\n\t    if pr is None:\n\t        pr=np.ones_like(gt)\n\t    colors=np.zeros([gt.shape[0],3],np.uint8)\n\t    colors[gt & pr]=np.asarray([0,255,0])[None,:]     # tp\n\t    colors[ (~gt) & pr]=np.asarray([255,0,0])[None,:] # fp\n\t    colors[ gt & (~pr)]=np.asarray([0,0,255])[None,:] # fn\n\t    return colors\n\tdef draw_hist(fn,vals,bins=100,hist_range=None,names=None):\n", "    if type(vals)==list:\n\t        val_num=len(vals)\n\t        if hist_range is None:\n\t            hist_range = (np.min(vals),np.max(vals))\n\t        if names is None:\n\t            names=[str(k) for k in range(val_num)]\n\t        for k in range(val_num):\n\t            plt.hist(vals[k], bins=bins, range=hist_range, alpha=0.5, label=names[k])\n\t        plt.legend()\n\t    else:\n", "        if hist_range is None:\n\t            hist_range = (np.min(vals),np.max(vals))\n\t        plt.hist(vals,bins=bins,range=hist_range)\n\t    plt.savefig(fn)\n\t    plt.close()\n\tdef draw_pr_curve(fn,gt_sort):\n\t    pos_num_all=np.sum(gt_sort)\n\t    pos_nums=np.cumsum(gt_sort)\n\t    sample_nums=np.arange(gt_sort.shape[0])+1\n\t    precisions=pos_nums.astype(np.float64)/sample_nums\n", "    recalls=pos_nums/pos_num_all\n\t    precisions=precisions[np.arange(0,gt_sort.shape[0],gt_sort.shape[0]//40)]\n\t    recalls=recalls[np.arange(0,gt_sort.shape[0],gt_sort.shape[0]//40)]\n\t    plt.plot(recalls,precisions,'r-')\n\t    plt.xlim(0,1)\n\t    plt.ylim(0,1)\n\t    plt.savefig(fn)\n\t    plt.close()\n\tdef draw_points(img,points):\n\t    pts=np.round(points).astype(np.int32)\n", "    h,w,_=img.shape\n\t    pts[:,0]=np.clip(pts[:,0],a_min=0,a_max=w-1)\n\t    pts[:,1]=np.clip(pts[:,1],a_min=0,a_max=h-1)\n\t    img=img.copy()\n\t    img[pts[:,1],pts[:,0]]=255\n\t    # img[pts[:,1],pts[:,0]]+=np.asarray([127,0,0],np.uint8)[None,:]\n\t    return img\n\tdef draw_bbox(img,bbox,color=None,thickness=2):\n\t    img=np.copy(img)\n\t    if color is not None:\n", "        color=[int(c) for c in color]\n\t    else:\n\t        color=(0,255,0)\n\t    left = int(round(bbox[0]))\n\t    top = int(round(bbox[1]))\n\t    width = int(round(bbox[2]))\n\t    height = int(round(bbox[3]))\n\t    img=cv2.rectangle(img,(left,top),(left+width,top+height),color,thickness=thickness)\n\t    return img\n\tdef output_points(fn,pts,colors=None):\n", "    with open(fn, 'w') as f:\n\t        for pi, pt in enumerate(pts):\n\t            f.write(f'{pt[0]:.6f} {pt[1]:.6f} {pt[2]:.6f} ')\n\t            if colors is not None:\n\t                f.write(f'{int(colors[pi,0])} {int(colors[pi,1])} {int(colors[pi,2])}')\n\t            f.write('\\n')\n\tdef compute_axis_points(pose):\n\t    R=pose[:,:3] # 3,3\n\t    t=pose[:,3:] # 3,1\n\t    pts = np.concatenate([np.identity(3),np.zeros([3,1])],1) # 3,4\n", "    pts = R.T @ (pts - t)\n\t    colors = np.asarray([[255,0,0],[0,255,0,],[0,0,255],[0,0,0]],np.uint8)\n\t    return pts.T, colors\n\tdef draw_epipolar_lines_func(img0,img1,Rt0,Rt1,K0,K1):\n\t    Rt=compute_relative_transformation(Rt0,Rt1)\n\t    F=compute_F(K0,K1,Rt[:,:3],Rt[:,3:])\n\t    return concat_images_list(*draw_epipolar_lines(F,img0,img1))\n\tdef pts_range_to_bbox_pts(max_pt,min_pt):\n\t    maxx,maxy,maxz = max_pt\n\t    minx,miny,minz = min_pt\n", "    pts=[\n\t        [minx,miny,minz],\n\t        [minx,maxy,minz],\n\t        [maxx,maxy,minz],\n\t        [maxx,miny,minz],\n\t        [minx,miny,maxz],\n\t        [minx,maxy,maxz],\n\t        [maxx,maxy,maxz],\n\t        [maxx,miny,maxz],\n\t    ]\n", "    return np.asarray(pts,np.float32)\n\tdef draw_bbox_3d(img,pts2d,color=(0,255,0)):\n\t    red_colors=np.zeros([8,3],np.uint8)\n\t    red_colors[:,0] = 255\n\t    # img = draw_keypoints(img, pts2d, colors=red_colors)\n\t    pts2d = np.round(pts2d).astype(np.int32)\n\t    img = cv2.line(img,tuple(pts2d[0]),tuple(pts2d[1]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[1]),tuple(pts2d[2]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[2]),tuple(pts2d[3]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[3]),tuple(pts2d[0]),color,2,cv2.LINE_AA)\n", "    img = cv2.line(img,tuple(pts2d[4]),tuple(pts2d[5]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[5]),tuple(pts2d[6]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[6]),tuple(pts2d[7]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[7]),tuple(pts2d[4]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[0]),tuple(pts2d[4]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[1]),tuple(pts2d[5]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[2]),tuple(pts2d[6]),color,2,cv2.LINE_AA)\n\t    img = cv2.line(img,tuple(pts2d[3]),tuple(pts2d[7]),color,2,cv2.LINE_AA)\n\t    return img"]}
{"filename": "utils/pose_utils.py", "chunked_list": ["import cv2\n\timport numpy as np\n\tfrom transforms3d.axangles import mat2axangle\n\tfrom transforms3d.euler import mat2euler\n\tfrom transforms3d.quaternions import quat2mat\n\tfrom utils.base_utils import transformation_inverse_2d, project_points, transformation_apply_2d, hpts_to_pts, \\\n\t    pts_to_hpts, transformation_decompose_2d, angle_to_rotation_2d, look_at_rotation, transformation_offset_2d, \\\n\t    transformation_compose_2d, transformation_scale_2d, transformation_rotation_2d, pose_inverse, transform_points_pose, pose_apply\n\tdef estimate_pose_from_similarity_transform(ref_pose, ref_K, que_K, M_que_to_ref, object_center):\n\t    # todo: here we assume the scale is approximately correct, even for the rectified version\n", "    M_ref_to_que = transformation_inverse_2d(M_que_to_ref) # from reference to query\n\t    ref_cam = (-ref_pose[:,:3].T @ ref_pose[:,3:])[...,0]\n\t    ref_obj_center, _ = project_points(object_center[None,:],ref_pose,ref_K)\n\t    # ref_obj_center = ref_obj_center[0]\n\t    que_obj_center = transformation_apply_2d(M_ref_to_que, ref_obj_center)[0]\n\t    que_obj_center_ = hpts_to_pts(pts_to_hpts(que_obj_center[None]) @ np.linalg.inv(que_K).T)[0]  # normalized\n\t    scale, rotation, _ = transformation_decompose_2d(M_ref_to_que)\n\t    # approximate depth\n\t    que_f = (que_K[0,0]+que_K[1,1])/2\n\t    ref_f = (ref_K[0,0]+ref_K[1,1])/2\n", "    que_obj_center__ = que_obj_center_ * que_f\n\t    que_f_ = np.sqrt(que_f ** 2 + np.linalg.norm(que_obj_center__,2)**2)\n\t    ref_dist = np.linalg.norm(ref_cam - object_center)\n\t    que_dist = ref_dist * que_f_ / ref_f / scale\n\t    # que_cam = object_center + (ref_cam - object_center) / ref_dist * que_dist\n\t    que_obj_center___ = pts_to_hpts(que_obj_center_[None])[0]\n\t    que_cen3d = que_obj_center___ / np.linalg.norm(que_obj_center___)  * que_dist\n\t    # que_cen3d = R @ object_center + t\n\t    ref_rot = ref_pose[:,:3]\n\t    R0 = np.eye(3)\n", "    R0[:2,:2] = angle_to_rotation_2d(rotation)\n\t    # x_, y_ = que_obj_center_\n\t    # R1 = euler2mat(-np.arctan2(x_, 1),0,0,'syxz')\n\t    # R2 = euler2mat(np.arctan2(y_, 1),0,0,'sxyz')\n\t    R = look_at_rotation(que_obj_center_)\n\t    # print(R2 @ R1 @ pts_to_hpts(que_obj_center_[None])[0])\n\t    # que_rot = R1.T @ R2.T @ (R0 @ ref_rot)\n\t    que_rot = R.T @ (R0 @ ref_rot)\n\t    que_trans = que_cen3d - que_rot @ object_center\n\t    return np.concatenate([que_rot, que_trans[:,None]], 1)\n", "def let_me_look_at(pose, K, obj_center):\n\t    image_center, _ = project_points(obj_center[None, :], pose, K)\n\t    return let_me_look_at_2d(image_center[0], K)\n\tdef let_me_look_at_2d(image_center, K):\n\t    f_raw = (K[0, 0] + K[1, 1]) / 2\n\t    image_center = image_center - K[:2, 2]\n\t    f_new = np.sqrt(np.linalg.norm(image_center, 2, 0) ** 2 + f_raw ** 2)\n\t    image_center_ = image_center / f_raw\n\t    R_new = look_at_rotation(image_center_)\n\t    return R_new, f_new\n", "def scale_rotation_difference_from_cameras(ref_poses, que_poses, ref_Ks, que_Ks, center):\n\t    \"\"\"\n\t    relative scale and rotation from ref to que (apply M on ref to get que)\n\t    @param ref_poses:\n\t    @param que_poses:\n\t    @param ref_Ks:\n\t    @param que_Ks:\n\t    @param center:\n\t    @return:\n\t    \"\"\"\n", "    que_rot, que_f = [], []\n\t    for qi in range(que_poses.shape[0]):\n\t        R, f = let_me_look_at(que_poses[qi],que_Ks[qi],center)\n\t        que_rot.append(R @ que_poses[qi,:,:3])\n\t        que_f.append(f)\n\t    que_rot = np.stack(que_rot,0)\n\t    que_f = np.asarray(que_f)\n\t    ref_rot, ref_f = [], []\n\t    for qi in range(ref_poses.shape[0]):\n\t        R, f = let_me_look_at(ref_poses[qi],ref_Ks[qi],center)\n", "        ref_rot.append(R @ ref_poses[qi,:,:3])\n\t        ref_f.append(f)\n\t    ref_rot = np.stack(ref_rot,0)\n\t    ref_f = np.asarray(ref_f)\n\t    ref_cam = (-ref_poses[:, :, :3].transpose([0, 2, 1]) @ ref_poses[:, :, 3:])[..., 0]  # rfn,3\n\t    que_cam = (-que_poses[:, :, :3].transpose([0, 2, 1]) @ que_poses[:, :, 3:])[..., 0]  # qn,3\n\t    ref_dist = np.linalg.norm(ref_cam - center[None, :], 2, 1)  # rfn\n\t    que_dist = np.linalg.norm(que_cam - center[None, :], 2, 1)  # qn\n\t    scale_diff = ref_dist / que_dist * que_f / ref_f\n\t    # compute relative rotation\n", "    # from ref to que\n\t    rel_rot = que_rot @ ref_rot.transpose([0, 2, 1])  # qn, 3, 3\n\t    angle_diff = []\n\t    for qi in range(rel_rot.shape[0]):\n\t        angle, _, _ = mat2euler(rel_rot[qi], 'szyx')\n\t        angle_diff.append(angle)\n\t    angle_diff = np.asarray(angle_diff)\n\t    return scale_diff, angle_diff\n\tdef estimate_pose_from_similarity_transform_compose(position, scale_r2q, angle_r2q, ref_pose, ref_K, que_K, object_center):\n\t    ref_cen = project_points(object_center[None],ref_pose,ref_K)[0][0]\n", "    M_q2r = transformation_offset_2d(-position[0], -position[1])\n\t    M_q2r = transformation_compose_2d(M_q2r, transformation_scale_2d(1 / scale_r2q))\n\t    M_q2r = transformation_compose_2d(M_q2r, transformation_rotation_2d(-angle_r2q))\n\t    M_q2r = transformation_compose_2d(M_q2r, transformation_offset_2d(ref_cen[0], ref_cen[1]))\n\t    pose_pr = estimate_pose_from_similarity_transform(ref_pose, ref_K, que_K, M_q2r, object_center)\n\t    return pose_pr\n\tdef estimate_pose_from_refinement(context_info, refine_info, ref_pose, ref_K, que_K, object_center):\n\t    context_position, context_scale_r2q, context_angle_r2q, warp_M = \\\n\t        context_info['position'], context_info['scale_r2q'], context_info['angle_r2q'], context_info['warp_M']\n\t    offset_r2c, scale_r2c, rot_r2c = refine_info['offset_r2q'], refine_info['scale_r2q'], refine_info['rot_r2q']\n", "    ref_cen = project_points(object_center[None], ref_pose, ref_K)[0][0]\n\t    # find the corrected center\n\t    cen_pr = ref_cen + offset_r2c\n\t    cen_pr = transformation_apply_2d(transformation_inverse_2d(warp_M), cen_pr[None, :])[0]  # coordinate on original image\n\t    rect_R, rect_f = let_me_look_at_2d(cen_pr, que_K)\n\t    scale_r2q = context_scale_r2q * scale_r2c\n\t    # compute the camera from scale\n\t    ref_f = (ref_K[0, 0] + ref_K[1, 1]) / 2\n\t    que_f = rect_f\n\t    ref_cam = pose_inverse(ref_pose)[:, 3]\n", "    ref_dist = np.linalg.norm(ref_cam - object_center)\n\t    que_dist = ref_dist * que_f / ref_f / scale_r2q\n\t    obejct_dir = pts_to_hpts(cen_pr[None]) @ np.linalg.inv(que_K).T\n\t    obejct_dir /= np.linalg.norm(obejct_dir,2,1,True)\n\t    que_cam_ = (obejct_dir * que_dist)[0]\n\t    # compute the rotation\n\t    ref_R = ref_pose[:, :3]\n\t    rot_r2c = quat2mat(rot_r2c)\n\t    rot_sel = np.asarray([[np.cos(context_angle_r2q), -np.sin(context_angle_r2q), 0],\n\t                          [np.sin(context_angle_r2q), np.cos(context_angle_r2q), 0], [0, 0, 1]], np.float32)\n", "    que_R = rect_R.T @ rot_sel @ rot_r2c @ ref_R\n\t    # compute the translation\n\t    # que_t = -que_R @ que_cam\n\t    que_t = que_cam_[:,None] - que_R @ object_center[:,None]\n\t    pose_pr = np.concatenate([que_R, que_t], 1)\n\t    return pose_pr\n\tdef compute_pose_errors(object_pts, pose_pr, pose_gt, K):\n\t    # eval projection errors\n\t    pts2d_pr, _ = project_points(object_pts, pose_pr, K)\n\t    pts2d_gt, _ = project_points(object_pts, pose_gt, K)\n", "    prj_err = np.mean(np.linalg.norm(pts2d_pr - pts2d_gt, 2, 1))\n\t    # eval 3D pts errors\n\t    pts3d_pr = transform_points_pose(object_pts, pose_pr)\n\t    pts3d_gt = transform_points_pose(object_pts, pose_gt)\n\t    obj_err = np.mean(np.linalg.norm(pts3d_pr - pts3d_gt, 2, 1))\n\t    # eval pose errors\n\t    dr = pose_pr[:3,:3] @ pose_gt[:3,:3].T\n\t    try:\n\t        _, dr = mat2axangle(dr)\n\t    except ValueError:\n", "        print(dr)\n\t        dr = np.pi\n\t    cam_pr = -pose_pr[:3,:3].T @ pose_pr[:3,3:]\n\t    cam_gt = -pose_gt[:3,:3].T @ pose_gt[:3,3:]\n\t    dt = np.linalg.norm(cam_pr - cam_gt)\n\t    pose_err = np.asarray([np.abs(dr),dt])\n\t    return prj_err, obj_err, pose_err\n\tdef compute_auc(errors, thresholds):\n\t    sort_idx = np.argsort(errors)\n\t    errors = np.array(errors.copy())[sort_idx]\n", "    recall = (np.arange(len(errors)) + 1) / len(errors)\n\t    errors = np.r_[0., errors]\n\t    recall = np.r_[0., recall]\n\t    aucs = []\n\t    for t in thresholds:\n\t        last_index = np.searchsorted(errors, t)\n\t        r = np.r_[recall[:last_index], recall[last_index-1]]\n\t        e = np.r_[errors[:last_index], t]\n\t        aucs.append(np.trapz(r, x=e)/t)\n\t    return aucs\n", "def compute_metrics_impl(object_pts, diameter, pose_gt_list, pose_pr_list, Ks, scale=1.0, symmetric=False):\n\t    prj_errs, obj_errs, pose_errs, obj_errs_sym = [], [], [], []\n\t    for pose_gt, pose_pr, K in zip(pose_gt_list, pose_pr_list, Ks):\n\t        prj_err, obj_err, pose_err = compute_pose_errors(object_pts, pose_pr, pose_gt, K)\n\t        if symmetric:\n\t            obj_pts_pr = transform_points_pose(object_pts, pose_pr)\n\t            obj_pts_gt = transform_points_pose(object_pts, pose_gt)\n\t            obj_err_sym = np.min(np.linalg.norm(obj_pts_pr[:,None]-obj_pts_gt[None,:],2,2),1)\n\t            obj_err_sym = np.mean(obj_err_sym)\n\t            obj_err_sym *= scale\n", "            obj_errs_sym.append(obj_err_sym)\n\t        obj_err*=scale\n\t        pose_err[1]*=scale\n\t        prj_errs.append(prj_err)\n\t        obj_errs.append(obj_err)\n\t        pose_errs.append(pose_err)\n\t    prj_errs = np.asarray(prj_errs)\n\t    obj_errs = np.asarray(obj_errs)\n\t    pose_errs = np.asarray(pose_errs)\n\t    results = {\n", "        'add-0.1d': np.mean(obj_errs<(diameter*0.1)),\n\t        'prj-5':np.mean(prj_errs<5),\n\t    }\n\t    if symmetric:\n\t        obj_errs_sym = np.asarray(obj_errs_sym)\n\t        results['add-0.1d-sym']=np.mean(obj_errs_sym<(diameter*0.1))\n\t    return results\n\tdef pose_sim_to_pose_rigid(pose_sim_in_to_que, pose_in, K_que, K_in, center):\n\t    f_que = np.mean(np.diag(K_que)[:2])\n\t    f_in = np.mean(np.diag(K_in)[:2])\n", "    center_in = pose_apply(pose_in, center)\n\t    depth_in = center_in[2]\n\t    U, S, V = np.linalg.svd(pose_sim_in_to_que[:3,:3])\n\t    R = U @ V\n\t    scale = np.mean(np.abs(S))\n\t    depth_que = depth_in / scale * f_que / f_in\n\t    center_sim = pose_apply(pose_sim_in_to_que, center_in)\n\t    center_que = center_sim / center_sim[2] * depth_que\n\t    rotation = R @ pose_in[:3,:3]\n\t    offset = center_que - rotation @ center\n", "    pose_que = np.concatenate([rotation, offset[:,None]], 1)\n\t    return pose_que\n\tdef compose_sim_pose(scale, quat, offset, in_pose, object_center):\n\t    offset = np.concatenate([offset, np.zeros(1)])\n\t    rotation = quat2mat(quat)\n\t    center_in = pose_apply(in_pose, object_center)\n\t    center_que = center_in + offset\n\t    offset = center_que - (scale * rotation @ center_in)\n\t    pose_sim_in_to_que = np.concatenate([scale * rotation, offset[:, None]], 1)\n\t    return pose_sim_in_to_que\n", "def pnp(points_3d, points_2d, camera_matrix,method=cv2.SOLVEPNP_ITERATIVE):\n\t    try:\n\t        dist_coeffs = pnp.dist_coeffs\n\t    except:\n\t        dist_coeffs = np.zeros(shape=[8, 1], dtype='float64')\n\t    assert points_3d.shape[0] == points_2d.shape[0], 'points 3D and points 2D must have same number of vertices'\n\t    if method==cv2.SOLVEPNP_EPNP:\n\t        points_3d=np.expand_dims(points_3d, 0)\n\t        points_2d=np.expand_dims(points_2d, 0)\n\t    points_2d = np.ascontiguousarray(points_2d.astype(np.float64))\n", "    points_3d = np.ascontiguousarray(points_3d.astype(np.float64))\n\t    camera_matrix = camera_matrix.astype(np.float64)\n\t    _, R_exp, t = cv2.solvePnP(points_3d,\n\t                               points_2d,\n\t                               camera_matrix,\n\t                               dist_coeffs,\n\t                               flags=method)\n\t                              # , None, None, False, cv2.SOLVEPNP_UPNP)\n\t    # R_exp, t, _ = cv2.solvePnPRansac(points_3D,\n\t    #                            points_2D,\n", "    #                            cameraMatrix,\n\t    #                            distCoeffs,\n\t    #                            reprojectionError=12.0)\n\t    R, _ = cv2.Rodrigues(R_exp)\n\t    # trans_3d=np.matmul(points_3d,R.transpose())+t.transpose()\n\t    # if np.max(trans_3d[:,2]<0):\n\t    #     R=-R\n\t    #     t=-t\n\t    return np.concatenate([R, t], axis=-1)\n\tdef ransac_pnp(points_3d, points_2d, camera_matrix, method=cv2.SOLVEPNP_ITERATIVE, iter_num=100, rep_error=1.0):\n", "    dist_coeffs = np.zeros(shape=[8, 1], dtype='float64')\n\t    assert points_3d.shape[0] == points_2d.shape[0], 'points 3D and points 2D must have same number of vertices'\n\t    if method==cv2.SOLVEPNP_EPNP:\n\t        points_3d=np.expand_dims(points_3d, 0)\n\t        points_2d=np.expand_dims(points_2d, 0)\n\t    points_2d = np.ascontiguousarray(points_2d.astype(np.float64))\n\t    points_3d = np.ascontiguousarray(points_3d.astype(np.float64))\n\t    camera_matrix = camera_matrix.astype(np.float64)\n\t    state, R_exp, t, inliers = cv2.solvePnPRansac(points_3d, points_2d, camera_matrix, dist_coeffs, flags=method,\n\t                                                  iterationsCount=iter_num, reprojectionError=rep_error, confidence=0.999)\n", "    mask = np.zeros([points_3d.shape[0]], np.bool)\n\t    if state:\n\t        R, _ = cv2.Rodrigues(R_exp)\n\t        mask[inliers[:,0]] = True\n\t        return np.concatenate([R, t], axis=-1), mask\n\t    else:\n\t        return np.concatenate([np.eye(3),np.zeros([3,1])],1).astype(np.float32), mask\n"]}
{"filename": "utils/bbox_utils.py", "chunked_list": ["import torch\n\timport numpy as np\n\tdef bboxes_lthw_squared(bboxes):\n\t    \"\"\"\n\t    @param bboxes: b,4 in lthw\n\t    @return:  b,4\n\t    \"\"\"\n\t    bboxes_len = bboxes[:, 2:]\n\t    bboxes_cen = bboxes[:, :2] + bboxes_len/2\n\t    bboxes_max_len = torch.max(bboxes_len,1,keepdim=True)[0] # b,1\n", "    bboxes_len = bboxes_max_len.repeat(1,2)\n\t    bboxes_left_top = bboxes_cen - bboxes_len/2\n\t    return torch.cat([bboxes_left_top,bboxes_len],1)\n\tdef bboxes_area(bboxes):\n\t    return (bboxes[...,2]-bboxes[...,0])*(bboxes[...,3]-bboxes[...,1])\n\tdef bboxes_iou(bboxes0, bboxes1,th=True):\n\t    \"\"\"\n\t    @param bboxes0: ...,4\n\t    @param bboxes1: ...,4\n\t    @return: ...\n", "    \"\"\"\n\t    if th:\n\t        x0 = torch.max(torch.stack([bboxes0[..., 0], bboxes1[..., 0]], -1), -1)[0]\n\t        y0 = torch.max(torch.stack([bboxes0[..., 1], bboxes1[..., 1]], -1), -1)[0]\n\t        x1 = torch.min(torch.stack([bboxes0[..., 2], bboxes1[..., 2]], -1), -1)[0]\n\t        y1 = torch.min(torch.stack([bboxes0[..., 3], bboxes1[..., 3]], -1), -1)[0]\n\t        inter = torch.clip(x1 - x0, min=0) * torch.clip(y1 - y0, min=0)\n\t    else:\n\t        x0 = np.max(np.stack([bboxes0[..., 0], bboxes1[..., 0]], -1), -1)[0]\n\t        y0 = np.max(np.stack([bboxes0[..., 1], bboxes1[..., 1]], -1), -1)[0]\n", "        x1 = np.min(np.stack([bboxes0[..., 2], bboxes1[..., 2]], -1), -1)[0]\n\t        y1 = np.min(np.stack([bboxes0[..., 3], bboxes1[..., 3]], -1), -1)[0]\n\t        inter = np.clip(x1 - x0, a_min=0, a_max=999999) * np.clip(y1 - y0, a_min=0, a_max=999999)\n\t    union = bboxes_area(bboxes0) + bboxes_area(bboxes1) - inter\n\t    iou = inter / union\n\t    return iou\n\tdef lthw_to_ltrb(bboxes,th=True):\n\t    if th:\n\t        return torch.cat([bboxes[...,:2],bboxes[...,:2]+bboxes[...,2:]],-1)\n\t    else:\n", "        return np.concatenate([bboxes[..., :2], bboxes[..., :2] + bboxes[..., 2:]], -1)\n\tdef cl_to_ltrb(bboxes_cl):\n\t    bboxes_cen = bboxes_cl[...,:2]\n\t    bboxes_len = bboxes_cl[...,2:]\n\t    return torch.cat([bboxes_cen-bboxes_len/2,bboxes_cen+bboxes_len/2],-1)\n\tdef ltrb_to_cl(bboxes_ltrb):\n\t    bboxes_cen = (bboxes_ltrb[...,:2]+bboxes_ltrb[...,2:])/2\n\t    bboxes_len = bboxes_ltrb[..., 2:]-bboxes_ltrb[...,:2]\n\t    return torch.cat([bboxes_cen,bboxes_len],-1)\n\tdef ltrb_to_lthw(bboxes,th=True):\n", "    if th:\n\t        raise NotImplementedError\n\t    else:\n\t        lt = bboxes[...,:2]\n\t        hw = bboxes[...,2:] - lt\n\t        return np.concatenate([lt,hw],-1)\n\tdef cl_to_lthw(bboxes_cl,th=True):\n\t    if th:\n\t        lt = bboxes_cl[..., :2] - bboxes_cl[..., 2:] / 2\n\t        return torch.cat([lt, bboxes_cl[..., 2:]], -1)\n", "    else:\n\t        lt = bboxes_cl[..., :2] - bboxes_cl[..., 2:] / 2\n\t        return np.concatenate([lt,bboxes_cl[...,2:]],-1)\n\tdef parse_bbox_from_scale_offset(que_select_id, scale_pr, select_offset, pool_ratio, ref_shape):\n\t    \"\"\"\n\t    @param que_select_id:  [2] x,y\n\t    @param scale_pr:       [hq,wq]\n\t    @param select_offset:  [2,hq,wq]\n\t    @param pool_ratio:     int\n\t    @param ref_shape:      [2] h,w\n", "    @return:\n\t    \"\"\"\n\t    hr, wr = ref_shape\n\t    select_x, select_y = que_select_id\n\t    scale_pr = scale_pr\n\t    offset_pr = select_offset\n\t    scale_pr = scale_pr[select_y,select_x]\n\t    scale_pr = 2**scale_pr\n\t    pool_ratio = pool_ratio\n\t    offset_x, offset_y = offset_pr[:,select_y,select_x]\n", "    center_x, center_y = select_x+offset_x, select_y+offset_y\n\t    center_x = (center_x + 0.5) * pool_ratio - 0.5\n\t    center_y = (center_y + 0.5) * pool_ratio - 0.5\n\t    h_pr, w_pr = hr * scale_pr, wr * scale_pr\n\t    bbox_pr = np.asarray([center_x - w_pr/2, center_y-h_pr/2, w_pr, h_pr])\n\t    return bbox_pr"]}
{"filename": "utils/database_utils.py", "chunked_list": ["import torch.nn.functional as F\n\tfrom dataset.database import get_object_center, get_diameter, get_object_vert\n\tfrom utils.base_utils import *\n\tfrom utils.pose_utils import scale_rotation_difference_from_cameras, let_me_look_at, let_me_look_at_2d\n\tdef look_at_crop(img, K, pose, position, angle, scale, h, w):\n\t    \"\"\"rotate the image with \"angle\" and resize it with \"scale\", then crop the image on \"position\" with (h,w)\"\"\"\n\t    # this function will return\n\t    # 1) the resulted pose (pose_new) and intrinsic (K_new);\n\t    # 2) pose_new = pose_compose(pose, pose_rect): \"pose_rect\" is the difference between the \"pose_new\" and the \"pose\"\n\t    # 3) H is the homography that transform the \"img\" to \"img_new\"\n", "    R_new, f_new = let_me_look_at_2d(position, K)\n\t    R_z = np.asarray([[np.cos(angle), -np.sin(angle), 0], [np.sin(angle), np.cos(angle), 0], [0, 0, 1]], np.float32)\n\t    R_new = R_z @ R_new\n\t    f_new = f_new * scale\n\t    K_new = np.asarray([[f_new, 0, w / 2], [0, f_new, h / 2], [0, 0, 1]], np.float32)\n\t    H = K_new @ R_new @ np.linalg.inv(K)\n\t    img_new = cv2.warpPerspective(img, H, (w, h), flags=cv2.INTER_LINEAR)\n\t    pose_rect = np.concatenate([R_new, np.zeros([3, 1])], 1).astype(np.float32)\n\t    pose_new = pose_compose(pose, pose_rect)\n\t    return img_new, K_new, pose_new, pose_rect, H\n", "def compute_normalized_view_correlation(que_poses, ref_poses, center, th=True):\n\t    \"\"\"\n\t    @param que_poses: [qn,3,4]\n\t    @param ref_poses: [rfn,3,4]\n\t    @param center:    [3]\n\t    @param th:\n\t    @return:\n\t    \"\"\"\n\t    if th:\n\t        que_cams = (que_poses[:,:,:3].permute(0,2,1) @ -que_poses[:,:,3:])[...,0] # qn,3\n", "        ref_cams = (ref_poses[:,:,:3].permute(0,2,1) @ -ref_poses[:,:,3:])[...,0] # rfn,3\n\t        que_diff = que_cams - center[None]\n\t        ref_diff = ref_cams - center[None]\n\t        que_diff = F.normalize(que_diff, dim=1)\n\t        ref_diff = F.normalize(ref_diff, dim=1)\n\t        corr = torch.sum(que_diff[:,None] * ref_diff[None,:], 2)\n\t    else:\n\t        que_cams = (que_poses[:,:,:3].transpose([0,2,1]) @ -que_poses[:,:,3:])[...,0] # qn,3\n\t        ref_cams = (ref_poses[:,:,:3].transpose([0,2,1]) @ -ref_poses[:,:,3:])[...,0] # rfn,3\n\t        # normalize to the same sphere\n", "        que_cams = que_cams - center[None,:]\n\t        ref_cams = ref_cams - center[None,:]\n\t        que_cams = que_cams / np.linalg.norm(que_cams,2,1,keepdims=True)\n\t        ref_cams = ref_cams / np.linalg.norm(ref_cams,2,1,keepdims=True)\n\t        corr = np.sum(que_cams[:,None,:]*ref_cams[None,:,:], 2) # qn,rfn\n\t    return corr\n\tdef normalize_reference_views(database, ref_ids, size, margin,\n\t                              rectify_rot=True, input_pose=None, input_K=None,\n\t                              add_rots=False, rots_list=None):\n\t    object_center = get_object_center(database)\n", "    object_diameter = get_diameter(database)\n\t    ref_poses = np.asarray([database.get_pose(ref_id) for ref_id in ref_ids]) # rfn,3,3\n\t    ref_Ks = np.asarray([database.get_K(ref_id) for ref_id in ref_ids]) # rfn,3,3\n\t    ref_cens = np.asarray([project_points(object_center[None],pose, K)[0][0] for pose,K in zip(ref_poses, ref_Ks)]) # rfn,2\n\t    ref_cams = np.stack([pose_inverse(pose)[:,3] for pose in ref_poses], 0) # rfn, 3\n\t    # ensure that the output reference images have the same scale\n\t    ref_dist = np.linalg.norm(ref_cams - object_center[None,], 2, 1) # rfn\n\t    ref_focal_look = np.asarray([let_me_look_at(pose, K, object_center)[1] for pose, K in zip(ref_poses, ref_Ks)]) # rfn\n\t    ref_focal_new = size * (1 - margin) / object_diameter * ref_dist\n\t    ref_scales = ref_focal_new / ref_focal_look\n", "    # ref_vert_angle will rotate the reference image to ensure the \"up\" direction approximate the Y- of the image\n\t    if rectify_rot:\n\t        if input_K is not None and input_pose is not None:\n\t            # optionally, we may want to rotate the image with respect to a given pose so that they will be aligned.\n\t            rfn = len(ref_poses)\n\t            input_pose = np.repeat(input_pose[None, :], rfn, 0)\n\t            input_K = np.repeat(input_K[None, :], rfn, 0)\n\t            _, ref_vert_angle = scale_rotation_difference_from_cameras(ref_poses, input_pose, ref_Ks, input_K, object_center)  # rfn\n\t        else:\n\t            object_vert = get_object_vert(database)\n", "            ref_vert_2d = np.asarray([(pose[:,:3] @ object_vert)[:2] for pose in ref_poses])\n\t            mask = np.linalg.norm(ref_vert_2d,2,1)<1e-5\n\t            ref_vert_2d[mask] += 1e-5 * np.sign(ref_vert_2d[mask]) # avoid 0 vector\n\t            ref_vert_angle = -np.arctan2(ref_vert_2d[:,1],ref_vert_2d[:,0])-np.pi/2\n\t    else:\n\t        ref_vert_angle = np.zeros(len(ref_ids),np.float32)\n\t    ref_imgs_new, ref_Ks_new, ref_poses_new, ref_Hs, ref_masks_new, ref_imgs_rots = [], [], [], [], [], []\n\t    for k in range(len(ref_ids)):\n\t        ref_img = database.get_image(ref_ids[k])\n\t        if add_rots:\n", "            ref_img_rot = np.stack([look_at_crop(ref_img, ref_Ks[k], ref_poses[k], ref_cens[k], ref_vert_angle[k]+rot, ref_scales[k], size, size)[0] for rot in rots_list],0)\n\t            ref_imgs_rots.append(ref_img_rot)\n\t        ref_img_new, ref_K_new, ref_pose_new, ref_pose_rect, ref_H = look_at_crop(\n\t            ref_img, ref_Ks[k], ref_poses[k], ref_cens[k], ref_vert_angle[k], ref_scales[k], size, size)\n\t        ref_imgs_new.append(ref_img_new)\n\t        ref_Ks_new.append(ref_K_new)\n\t        ref_poses_new.append(ref_pose_new)\n\t        ref_Hs.append(ref_H)\n\t        ref_mask = database.get_mask(ref_ids[k]).astype(np.float32)\n\t        ref_masks_new.append(cv2.warpPerspective(ref_mask, ref_H, (size, size), flags=cv2.INTER_LINEAR))\n", "    ref_imgs_new, ref_Ks_new, ref_poses_new, ref_Hs, ref_masks_new = \\\n\t        np.stack(ref_imgs_new, 0), np.stack(ref_Ks_new,0), np.stack(ref_poses_new,0), np.stack(ref_Hs,0), np.stack(ref_masks_new,0)\n\t    if add_rots:\n\t        ref_imgs_rots = np.stack(ref_imgs_rots,1)\n\t        return ref_imgs_new, ref_masks_new, ref_Ks_new, ref_poses_new, ref_Hs, ref_imgs_rots\n\t    return ref_imgs_new, ref_masks_new, ref_Ks_new, ref_poses_new, ref_Hs\n\tdef select_reference_img_ids_fps(database, ref_ids_all, ref_num, random_fps=False):\n\t    object_center = get_object_center(database)\n\t    # select ref ids\n\t    poses = [database.get_pose(ref_id) for ref_id in ref_ids_all]\n", "    cam_pts = np.asarray([pose_inverse(pose)[:, 3] - object_center for pose in poses])\n\t    if random_fps:\n\t        idxs = sample_fps_points(cam_pts, ref_num, False, index_model=True)\n\t    else:\n\t        idxs = sample_fps_points(cam_pts, ref_num + 1, True, index_model=True)\n\t    ref_ids = np.asarray(ref_ids_all)[idxs]  # rfn\n\t    return ref_ids\n\tdef select_reference_img_ids_refinement(ref_database, object_center, ref_ids, sel_pose, refine_ref_num=6, refine_even_ref_views=False, refine_even_num=128):\n\t    ref_ids = np.asarray(ref_ids)\n\t    ref_poses_all = np.asarray([ref_database.get_pose(ref_id) for ref_id in ref_ids])\n", "    if refine_even_ref_views:\n\t        # use fps to resample the reference images to make them distribute more evenly\n\t        ref_cams_all = np.asarray([pose_inverse(pose)[:, 3] for pose in ref_poses_all])\n\t        idx = sample_fps_points(ref_cams_all, refine_even_num + 1, True, index_model=True)\n\t        ref_ids = ref_ids[idx]\n\t        ref_poses_all = ref_poses_all[idx]\n\t    corr = compute_normalized_view_correlation(sel_pose[None], ref_poses_all, object_center, False)\n\t    ref_idxs = np.argsort(-corr[0])\n\t    ref_idxs = ref_idxs[:refine_ref_num]\n\t    ref_ids = ref_ids[ref_idxs]\n", "    return ref_ids\n\t# def normalize_reference_views(database: BaseDatabase, ref_ids_all, ref_num=32, size=128, margin=0.05):\n\t#     ref_ids = select_reference_img_ids_fps(database, ref_ids_all, ref_num)\n\t#     ref_imgs_new, ref_masks_new, ref_Ks_new, ref_poses_new, ref_Hs = construct_reference_views(database, ref_ids, size, margin)\n\t#     return ref_imgs_new, ref_masks_new, ref_Ks_new, ref_poses_new, ref_Hs, ref_ids\n"]}
{"filename": "utils/imgs_info.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom utils.base_utils import color_map_forward\n\tdef imgs_info_to_torch(imgs_info):\n\t    for k, v in imgs_info.items():\n\t        if isinstance(v,np.ndarray):\n\t            imgs_info[k] = torch.from_numpy(v)\n\t    return imgs_info\n\tdef build_imgs_info(database, ref_ids, has_mask=True):\n\t    ref_Ks = np.asarray([database.get_K(ref_id) for ref_id in ref_ids], dtype=np.float32)\n", "    ref_imgs = [database.get_image(ref_id) for ref_id in ref_ids]\n\t    if has_mask: ref_masks =  [database.get_mask(ref_id) for ref_id in ref_ids]\n\t    else: ref_masks = None\n\t    ref_imgs = (np.stack(ref_imgs, 0)).transpose([0, 3, 1, 2])\n\t    ref_imgs = color_map_forward(ref_imgs)\n\t    if has_mask: ref_masks = np.stack(ref_masks, 0)[:, None, :, :]\n\t    ref_poses = np.asarray([database.get_pose(ref_id) for ref_id in ref_ids], dtype=np.float32)\n\t    ref_imgs_info = {'imgs': ref_imgs, 'poses': ref_poses, 'Ks': ref_Ks}\n\t    if has_mask: ref_imgs_info['masks'] = ref_masks\n\t    return ref_imgs_info\n"]}
{"filename": "utils/colmap_database.py", "chunked_list": ["import sys\n\timport sqlite3\n\timport numpy as np\n\tIS_PYTHON3 = sys.version_info[0] >= 3\n\tMAX_IMAGE_ID = 2**31 - 1\n\tCREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n\t    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n\t    model INTEGER NOT NULL,\n\t    width INTEGER NOT NULL,\n\t    height INTEGER NOT NULL,\n", "    params BLOB,\n\t    prior_focal_length INTEGER NOT NULL)\"\"\"\n\tCREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n\t    image_id INTEGER PRIMARY KEY NOT NULL,\n\t    rows INTEGER NOT NULL,\n\t    cols INTEGER NOT NULL,\n\t    data BLOB,\n\t    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n\tCREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n\t    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n", "    name TEXT NOT NULL UNIQUE,\n\t    camera_id INTEGER NOT NULL,\n\t    prior_qw REAL,\n\t    prior_qx REAL,\n\t    prior_qy REAL,\n\t    prior_qz REAL,\n\t    prior_tx REAL,\n\t    prior_ty REAL,\n\t    prior_tz REAL,\n\t    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n", "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n\t\"\"\".format(MAX_IMAGE_ID)\n\tCREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n\tCREATE TABLE IF NOT EXISTS two_view_geometries (\n\t    pair_id INTEGER PRIMARY KEY NOT NULL,\n\t    rows INTEGER NOT NULL,\n\t    cols INTEGER NOT NULL,\n\t    data BLOB,\n\t    config INTEGER NOT NULL,\n\t    F BLOB,\n", "    E BLOB,\n\t    H BLOB)\n\t\"\"\"\n\tCREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n\t    image_id INTEGER PRIMARY KEY NOT NULL,\n\t    rows INTEGER NOT NULL,\n\t    cols INTEGER NOT NULL,\n\t    data BLOB,\n\t    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n\t\"\"\"\n", "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n\t    pair_id INTEGER PRIMARY KEY NOT NULL,\n\t    rows INTEGER NOT NULL,\n\t    cols INTEGER NOT NULL,\n\t    data BLOB)\"\"\"\n\tCREATE_NAME_INDEX = \\\n\t    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n\tCREATE_ALL = \"; \".join([\n\t    CREATE_CAMERAS_TABLE,\n\t    CREATE_IMAGES_TABLE,\n", "    CREATE_KEYPOINTS_TABLE,\n\t    CREATE_DESCRIPTORS_TABLE,\n\t    CREATE_MATCHES_TABLE,\n\t    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n\t    CREATE_NAME_INDEX\n\t])\n\tdef image_ids_to_pair_id(image_id1, image_id2):\n\t    if image_id1 > image_id2:\n\t        image_id1, image_id2 = image_id2, image_id1\n\t    return image_id1 * MAX_IMAGE_ID + image_id2\n", "def pair_id_to_image_ids(pair_id):\n\t    image_id2 = pair_id % MAX_IMAGE_ID\n\t    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n\t    return image_id1, image_id2\n\tdef array_to_blob(array):\n\t    if IS_PYTHON3:\n\t        return array.tobytes()\n\t    else:\n\t        return np.getbuffer(array)\n\tdef blob_to_array(blob, dtype, shape=(-1,)):\n", "    if IS_PYTHON3:\n\t        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n\t    else:\n\t        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n\tclass COLMAPDatabase(sqlite3.Connection):\n\t    @staticmethod\n\t    def connect(database_path):\n\t        return sqlite3.connect(str(database_path), factory=COLMAPDatabase)\n\t    def __init__(self, *args, **kwargs):\n\t        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n", "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n\t        self.create_cameras_table = \\\n\t            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n\t        self.create_descriptors_table = \\\n\t            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n\t        self.create_images_table = \\\n\t            lambda: self.executescript(CREATE_IMAGES_TABLE)\n\t        self.create_two_view_geometries_table = \\\n\t            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n\t        self.create_keypoints_table = \\\n", "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n\t        self.create_matches_table = \\\n\t            lambda: self.executescript(CREATE_MATCHES_TABLE)\n\t        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n\t    def add_camera(self, model, width, height, params,\n\t                   prior_focal_length=False, camera_id=None):\n\t        params = np.asarray(params, np.float64)\n\t        cursor = self.execute(\n\t            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n\t            (camera_id, model, width, height, array_to_blob(params),\n", "             prior_focal_length))\n\t        return cursor.lastrowid\n\t    def add_image(self, name, camera_id,\n\t                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n\t        cursor = self.execute(\n\t            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n\t            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n\t             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n\t        return cursor.lastrowid\n\t    def add_keypoints(self, image_id, keypoints):\n", "        assert(len(keypoints.shape) == 2)\n\t        assert(keypoints.shape[1] in [2, 4, 6])\n\t        keypoints = np.asarray(keypoints, np.float32)\n\t        self.execute(\n\t            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n\t            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n\t    def add_descriptors(self, image_id, descriptors):\n\t        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n\t        self.execute(\n\t            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n", "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n\t    def add_matches(self, image_id1, image_id2, matches):\n\t        assert(len(matches.shape) == 2)\n\t        assert(matches.shape[1] == 2)\n\t        if image_id1 > image_id2:\n\t            matches = matches[:,::-1]\n\t        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n\t        matches = np.asarray(matches, np.uint32)\n\t        self.execute(\n\t            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n", "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n\t    def add_two_view_geometry(self, image_id1, image_id2, matches,\n\t                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n\t        assert(len(matches.shape) == 2)\n\t        assert(matches.shape[1] == 2)\n\t        if image_id1 > image_id2:\n\t            matches = matches[:,::-1]\n\t        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n\t        matches = np.asarray(matches, np.uint32)\n\t        F = np.asarray(F, dtype=np.float64)\n", "        E = np.asarray(E, dtype=np.float64)\n\t        H = np.asarray(H, dtype=np.float64)\n\t        self.execute(\n\t            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n\t            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n\t             array_to_blob(F), array_to_blob(E), array_to_blob(H)))\n\tdef example_usage():\n\t    import os\n\t    import argparse\n\t    parser = argparse.ArgumentParser()\n", "    parser.add_argument(\"--database_path\", default=\"database.db\")\n\t    args = parser.parse_args()\n\t    if os.path.exists(args.database_path):\n\t        print(\"ERROR: database path already exists -- will not modify it.\")\n\t        return\n\t    # Open the database.\n\t    db = COLMAPDatabase.connect(args.database_path)\n\t    # For convenience, try creating all the tables upfront.\n\t    db.create_tables()\n\t    # Create dummy cameras.\n", "    model1, width1, height1, params1 = \\\n\t        0, 1024, 768, np.array((1024., 512., 384.))\n\t    model2, width2, height2, params2 = \\\n\t        2, 1024, 768, np.array((1024., 512., 384., 0.1))\n\t    camera_id1 = db.add_camera(model1, width1, height1, params1)\n\t    camera_id2 = db.add_camera(model2, width2, height2, params2)\n\t    # Create dummy images.\n\t    image_id1 = db.add_image(\"image1.png\", camera_id1)\n\t    image_id2 = db.add_image(\"image2.png\", camera_id1)\n\t    image_id3 = db.add_image(\"image3.png\", camera_id2)\n", "    image_id4 = db.add_image(\"image4.png\", camera_id2)\n\t    # Create dummy keypoints.\n\t    #\n\t    # Note that COLMAP supports:\n\t    #      - 2D keypoints: (x, y)\n\t    #      - 4D keypoints: (x, y, theta, scale)\n\t    #      - 6D affine keypoints: (x, y, a_11, a_12, a_21, a_22)\n\t    num_keypoints = 1000\n\t    keypoints1 = np.random.rand(num_keypoints, 2) * (width1, height1)\n\t    keypoints2 = np.random.rand(num_keypoints, 2) * (width1, height1)\n", "    keypoints3 = np.random.rand(num_keypoints, 2) * (width2, height2)\n\t    keypoints4 = np.random.rand(num_keypoints, 2) * (width2, height2)\n\t    db.add_keypoints(image_id1, keypoints1)\n\t    db.add_keypoints(image_id2, keypoints2)\n\t    db.add_keypoints(image_id3, keypoints3)\n\t    db.add_keypoints(image_id4, keypoints4)\n\t    # Create dummy matches.\n\t    M = 50\n\t    matches12 = np.random.randint(num_keypoints, size=(M, 2))\n\t    matches23 = np.random.randint(num_keypoints, size=(M, 2))\n", "    matches34 = np.random.randint(num_keypoints, size=(M, 2))\n\t    db.add_matches(image_id1, image_id2, matches12)\n\t    db.add_matches(image_id2, image_id3, matches23)\n\t    db.add_matches(image_id3, image_id4, matches34)\n\t    # Commit the data to the file.\n\t    db.commit()\n\t    # Read and check cameras.\n\t    rows = db.execute(\"SELECT * FROM cameras\")\n\t    camera_id, model, width, height, params, prior = next(rows)\n\t    params = blob_to_array(params, np.float64)\n", "    assert camera_id == camera_id1\n\t    assert model == model1 and width == width1 and height == height1\n\t    assert np.allclose(params, params1)\n\t    camera_id, model, width, height, params, prior = next(rows)\n\t    params = blob_to_array(params, np.float64)\n\t    assert camera_id == camera_id2\n\t    assert model == model2 and width == width2 and height == height2\n\t    assert np.allclose(params, params2)\n\t    # Read and check keypoints.\n\t    keypoints = dict(\n", "        (image_id, blob_to_array(data, np.float32, (-1, 2)))\n\t        for image_id, data in db.execute(\n\t            \"SELECT image_id, data FROM keypoints\"))\n\t    assert np.allclose(keypoints[image_id1], keypoints1)\n\t    assert np.allclose(keypoints[image_id2], keypoints2)\n\t    assert np.allclose(keypoints[image_id3], keypoints3)\n\t    assert np.allclose(keypoints[image_id4], keypoints4)\n\t    # Read and check matches.\n\t    pair_ids = [image_ids_to_pair_id(*pair) for pair in\n\t                ((image_id1, image_id2),\n", "                 (image_id2, image_id3),\n\t                 (image_id3, image_id4))]\n\t    matches = dict(\n\t        (pair_id_to_image_ids(pair_id),\n\t         blob_to_array(data, np.uint32, (-1, 2)))\n\t        for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\")\n\t    )\n\t    assert np.all(matches[(image_id1, image_id2)] == matches12)\n\t    assert np.all(matches[(image_id2, image_id3)] == matches23)\n\t    assert np.all(matches[(image_id3, image_id4)] == matches34)\n", "    # Clean up.\n\t    db.close()\n\t    if os.path.exists(args.database_path):\n\t        os.remove(args.database_path)\n\tif __name__ == \"__main__\":\n\t    example_usage()\n"]}
{"filename": "network/detector.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchvision\n\tfrom network.pretrain_models import VGGBNPretrain\n\tfrom utils.base_utils import color_map_forward, transformation_crop, to_cpu_numpy\n\tfrom utils.bbox_utils import parse_bbox_from_scale_offset\n\tfrom network.vis_dino_encoder import VitExtractor\n\tfrom loguru import logger\n", "import skimage\n\timport cv2\n\t# count = 0\n\tdef show_heatmap(feature, output_jpg_name):\n\t    data = feature\n\t    heatmap = data.sum(0)/data.shape[0]\n\t    heatmap = np.maximum(heatmap, 0)\n\t    # heatmap /= np.max(heatmap)\n\t    # heatmap = 1.0 - heatmap\n\t    heatmap = np.uint8(255*heatmap)\n", "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\t    # skimage.io.imsave(output_jpg_name, heatmap)\n\t    cv2.imwrite(output_jpg_name, heatmap)\n\tclass BaseDetector(nn.Module):\n\t    def load_impl(self, ref_imgs):\n\t        raise NotImplementedError\n\t    def detect_impl(self, que_imgs):\n\t        raise NotImplementedError\n\t    def load(self, ref_imgs):\n\t        ref_imgs = torch.from_numpy(color_map_forward(ref_imgs)).permute(0, 3, 1, 2).cuda()\n", "        self.load_impl(ref_imgs)\n\t    def detect(self, que_imgs):\n\t        que_imgs = torch.from_numpy(color_map_forward(que_imgs)).permute(0, 3, 1, 2).cuda()\n\t        return self.detect_impl(que_imgs) # 'scores' 'select_pr_offset' 'select_pr_scale'\n\t    @staticmethod\n\t    def parse_detect_results(results):\n\t        \"\"\"\n\t        @param results: dict\n\t            pool_ratio: int -- pn\n\t            scores: qn,1,h/pn,w/pn\n", "            select_pr_offset: qn,2,h/pn,w/pn\n\t            select_pr_scale:  qn,1,h/pn,w/pn\n\t            select_pr_angle:  qn,2,h/pn,w/pn # optional\n\t        @return: all numpy ndarray\n\t        \"\"\"\n\t        qn = results['scores'].shape[0]\n\t        pool_ratio = results['pool_ratio']\n\t        # max scores\n\t        _, score_x, score_y = BaseDetector.get_select_index(results['scores']) # qn\n\t        position = torch.stack([score_x, score_y], -1)  # qn,2\n", "        # offset\n\t        offset = results['select_pr_offset'][torch.arange(qn),:,score_y,score_x] # qn,2\n\t        position = position + offset\n\t        # to original coordinate\n\t        position = (position + 0.5) * pool_ratio - 0.5 # qn,2\n\t        # scale\n\t        scale_r2q = results['select_pr_scale'][torch.arange(qn),0,score_y,score_x] # qn\n\t        scale_r2q = 2**scale_r2q\n\t        outputs = {'position': position.detach().cpu().numpy(), 'scale_r2q': scale_r2q.detach().cpu().numpy()}\n\t        # rotation\n", "        if 'select_pr_angle' in results:\n\t            angle_r2q = results['select_pr_angle'][torch.arange(qn),:,score_y,score_x] # qn,2\n\t            angle = torch.atan2(angle_r2q[:,1],angle_r2q[:,0])\n\t            outputs['angle_r2q'] = angle.cpu().numpy() # qn\n\t        return outputs\n\t    @staticmethod\n\t    def detect_results_to_bbox(dets, length):\n\t        pos = dets['position'] # qn,2\n\t        length = dets['scale_r2q'] * length # qn,\n\t        length = length[:,None]\n", "        begin = pos - length/2\n\t        return np.concatenate([begin,length,length],1)\n\t    @staticmethod\n\t    def detect_results_to_image_region(imgs, dets, region_len):\n\t        qn = len(imgs)\n\t        img_regions = []\n\t        for qi in range(qn):\n\t            pos = dets['position'][qi]; scl_r2q = dets['scale_r2q'][qi]\n\t            ang_r2q = dets['angle_r2q'][qi] if 'anlge_r2q' in dets else 0\n\t            img = imgs[qi]\n", "            img_region, _ = transformation_crop(img, pos, 1/scl_r2q, -ang_r2q, region_len)\n\t            img_regions.append(img_region)\n\t        return img_regions\n\t    @staticmethod\n\t    def get_select_index(scores):\n\t        \"\"\"\n\t        @param scores: qn,rfn or 1,hq,wq\n\t        @return: qn\n\t        \"\"\"\n\t        qn, rfn, hq, wq = scores.shape\n", "        # 取最大值\n\t        select_id = torch.argmax(scores.flatten(1), 1)\n\t        select_ref_id = select_id // (hq * wq)\n\t        select_h_id = (select_id - select_ref_id * hq * wq) // wq\n\t        select_w_id = select_id - select_ref_id * hq * wq - select_h_id * wq\n\t        return select_ref_id, select_w_id, select_h_id\n\t    @staticmethod\n\t    def parse_detection(scores, scales, offsets, pool_ratio):\n\t        \"\"\"\n\t        @param scores:    qn,1,h/8,w/8\n", "        @param scales:    qn,1,h/8,w/8\n\t        @param offsets:   qn,2,h/8,w/8\n\t        @param pool_ratio:int\n\t        @return: position in x_cur\n\t        \"\"\"\n\t        qn, _, _, _ = offsets.shape\n\t        _, score_x, score_y = BaseDetector.get_select_index(scores) # qn\n\t        positions = torch.stack([score_x, score_y], -1)  # qn,2\n\t        offset = offsets[torch.arange(qn),:,score_y,score_x] # qn,2\n\t        positions = positions + offset\n", "        # to original coordinate\n\t        positions = (positions + 0.5) * pool_ratio - 0.5 # qn,2\n\t        # scale\n\t        scales = scales[torch.arange(qn),0,score_y,score_x] # qn\n\t        scales = 2**scales\n\t        return positions, scales # [qn,2] [qn]\n\tdef disable_bn_grad(input_module):\n\t    for module in input_module.modules():\n\t        if isinstance(module, nn.BatchNorm2d):\n\t            if hasattr(module, 'weight'):\n", "                module.weight.requires_grad_(False)\n\t            if hasattr(module, 'bias'):\n\t                module.bias.requires_grad_(False)\n\tdef disable_bn_track(input_module):\n\t    for module in input_module.modules():\n\t        if isinstance(module, nn.BatchNorm2d):\n\t            module.eval()\n\tclass Detector(BaseDetector):\n\t    default_cfg={\n\t        \"vgg_score_stats\": [[36.264317,13.151907],[13910.291,5345.965],[829.70807,387.98788]],\n", "        \"vgg_score_max\": 10,\n\t        \"detection_scales\": [-1.0,-0.5,0.0,0.5],\n\t        \"train_feats\": False,\n\t    }\n\t    def __init__(self, cfg):\n\t        self.cfg={**self.default_cfg,**cfg}\n\t        super().__init__()\n\t        self.backbone = VGGBNPretrain()\n\t        self.use_dino = self.cfg.get(\"use_dino\",False)\n\t        logger.info(f\"Detector use_dino: {self.use_dino}\")\n", "        if self.use_dino:\n\t            self.fea_ext =  VitExtractor(model_name='dino_vits8').eval()\n\t            for para in self.fea_ext.parameters():\n\t                para.requires_grad = False\n\t            # self.input_zero_conv = nn.Conv2d(in_channels=3, \\\n\t            #                             out_channels=3, \\\n\t            #                             kernel_size=1,\\\n\t            #                             stride=1,\\\n\t            #                             padding=0, \\\n\t            #                             bias=True)\n", "            # self.input_zero_conv.weight.data.fill_(0)\n\t            # self.zero_conv1 = nn.Conv2d(in_channels=512, \\\n\t            #                             out_channels=384, \\\n\t            #                             kernel_size=1,\\\n\t            #                             stride=1,\\\n\t            #                             padding=0, \\\n\t            #                             bias=True)\n\t            # self.zero_conv1_re = nn.Conv2d(in_channels=384, \\\n\t            #                             out_channels=512, \\\n\t            #                             kernel_size=1,\\\n", "            #                             stride=1,\\\n\t            #                             padding=0, \\\n\t            #                             bias=True)\n\t            # self.zero_conv1.weight.data.fill_(0)\n\t            # self.zero_conv1_re.weight.data.fill_(0)\n\t            # self.zero_conv2 = nn.Conv2d(in_channels=512, \\\n\t            #                             out_channels=384, \\\n\t            #                             kernel_size=1,\\\n\t            #                             stride=1,\\\n\t            #                             padding=0, \\\n", "            #                             bias=True)\n\t            # self.zero_conv2_re = nn.Conv2d(in_channels=384, \\\n\t            #                             out_channels=512, \\\n\t            #                             kernel_size=1,\\\n\t            #                             stride=1,\\\n\t            #                             padding=0, \\\n\t            #                             bias=True)\n\t            # self.zero_conv2.weight.data.fill_(0)\n\t            # self.zero_conv2_re.weight.data.fill_(0)\n\t            # self.zero_conv3 = nn.Conv2d(in_channels=512, \\\n", "            #                             out_channels=384, \\\n\t            #                             kernel_size=1,\\\n\t            #                             stride=1,\\\n\t            #                             padding=0, \\\n\t            #                             bias=True)\n\t            # self.zero_conv3_re = nn.Conv2d(in_channels=384, \\\n\t            #                             out_channels=512, \\\n\t            #                             kernel_size=1,\\\n\t            #                             stride=1,\\\n\t            #                             padding=0, \\\n", "            #                             bias=True)\n\t            # self.zero_conv3.weight.data.fill_(0)\n\t            # self.zero_conv3_re.weight.data.fill_(0)\n\t            self.fuse_conv1 = nn.Conv2d(in_channels=512+384, \\\n\t                                        out_channels=512, \\\n\t                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True)\n\t            self.fuse_conv2 = nn.Conv2d(in_channels=512+384, \\\n", "                                        out_channels=512, \\\n\t                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True)\n\t            self.fuse_conv3 = nn.Conv2d(in_channels=512+384, \\\n\t                                        out_channels=512, \\\n\t                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n", "                                        bias=True)\n\t        if self.cfg[\"train_feats\"]:\n\t            # disable BN training only\n\t            disable_bn_grad(self.backbone)\n\t        else:\n\t            for para in self.backbone.parameters():\n\t                para.requires_grad = False\n\t        self.pool_ratio = 8\n\t        self.img_norm = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t        d = 64\n", "        self.score_conv = nn.Sequential(\n\t            nn.Conv3d(3*len(self.cfg['detection_scales']),d,1,1),\n\t            nn.ReLU(),\n\t            nn.Conv3d(d,d,1,1),\n\t        )\n\t        self.score_predict = nn.Sequential(\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n", "            nn.Conv2d(d,1,3,1,1),\n\t        )\n\t        self.scale_predict = nn.Sequential(\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,1,3,1,1),\n\t        )\n\t        self.offset_predict = nn.Sequential(\n", "            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,2,3,1,1),\n\t        )\n\t        self.ref_center_feats=None\n\t        self.ref_shape=None\n\t    def extract_feats(self, imgs):\n\t        n,c,h,w = imgs.shape\n", "        if self.use_dino:\n\t            with torch.no_grad():\n\t                # dino_ret =  self.fea_ext.get_vit_attn_feat(self.input_zero_conv(F.interpolate(imgs,size=(224,224))))\n\t                dino_ret =  self.fea_ext.get_vit_attn_feat(F.interpolate(imgs,size=(224,224)))\n\t                attn, cls_, feat = dino_ret['attn'], dino_ret['cls_'], dino_ret['feat']\n\t                dino_fea = feat.permute(0,2,1).reshape(-1,384,224//8,224//8)\n\t                dino_fea = F.interpolate(dino_fea,size=(h//8,w//8))\n\t                dino_fea = F.normalize(dino_fea, dim=1)\n\t        imgs = self.img_norm(imgs)\n\t        if self.cfg['train_feats']:\n", "            disable_bn_track(self.backbone)\n\t            x0, x1, x2 = self.backbone(imgs)\n\t        else:\n\t            self.backbone.eval()\n\t            with torch.no_grad():\n\t                x0, x1, x2 = self.backbone(imgs)\n\t        if self.use_dino:\n\t            with torch.no_grad():\n\t                # fuse v1\n\t                fused_fea1 = torch.cat( (x0, dino_fea.clone() ), dim = 1)\n", "                x0 = self.fuse_conv1(fused_fea1)\n\t                dino_fea2 = F.interpolate(dino_fea.clone(), size=(h//16, w//16))\n\t                fused_fea2 = torch.cat( (x1, dino_fea2 ), dim = 1)\n\t                x1 = self.fuse_conv2(fused_fea2)\n\t                dino_fea3 = F.interpolate(dino_fea.clone(), size=(h//32, w//32))\n\t                fused_fea3 = torch.cat( (x2, dino_fea3 ), dim = 1)\n\t                x2 = self.fuse_conv3(fused_fea3)\n\t                # fuse v2\n\t                # dino_fea_0 = self.zero_conv1(dino_fea)\n\t                # x0 = x0 + dino_fea_0\n", "                # dino_fea_1 = F.interpolate(dino_fea.clone(), size=(h//16, w//16))\n\t                # x1 = x1 + dino_fea_1\n\t                # dino_fea_2 = F.interpolate(dino_fea.clone(), size=(h//32, w//32))\n\t                # dino_fea_2 = torch.cat( (x2, dino_fea_2 ), dim = 1)\n\t                # x2 = self.fuse_conv3(dino_fea_2)\n\t        return x0, x1, x2\n\t    def load_impl(self, ref_imgs):\n\t        # resize to 120,120\n\t        ref_imgs = F.interpolate(ref_imgs,size=(120,120))\n\t        # 15, 7, 3\n", "        self.ref_center_feats = self.extract_feats(ref_imgs)\n\t        rfn, _, h, w = ref_imgs.shape\n\t        self.ref_shape = [h, w]\n\t    def normalize_scores(self,scores0,scores1,scores2):\n\t        stats = self.cfg['vgg_score_stats']\n\t        scores0 = (scores0 - stats[0][0])/stats[0][1]\n\t        scores1 = (scores1 - stats[1][0])/stats[1][1]\n\t        scores2 = (scores2 - stats[2][0])/stats[2][1]\n\t        scores0 = torch.clip(scores0,max=self.cfg['vgg_score_max'],min=-self.cfg['vgg_score_max'])\n\t        scores1 = torch.clip(scores1,max=self.cfg['vgg_score_max'],min=-self.cfg['vgg_score_max'])\n", "        scores2 = torch.clip(scores2,max=self.cfg['vgg_score_max'],min=-self.cfg['vgg_score_max'])\n\t        return scores0, scores1, scores2\n\t    def get_scores(self, que_imgs):\n\t        que_x0, que_x1, que_x2 = self.extract_feats(que_imgs)\n\t        ref_x0, ref_x1, ref_x2 = self.ref_center_feats # rfn,f,hr,wr\n\t        scores2 = F.conv2d(que_x2, ref_x2, padding=1)\n\t        scores1 = F.conv2d(que_x1, ref_x1, padding=3)\n\t        scores0 = F.conv2d(que_x0, ref_x0, padding=7)\n\t        scores2 = F.interpolate(scores2, scale_factor=4)\n\t        scores1 = F.interpolate(scores1, scale_factor=2)\n", "        scores0, scores1, scores2 = self.normalize_scores(scores0, scores1, scores2)\n\t        scores = torch.stack([scores0, scores1, scores2],1) # qn,3,rfn,hq/8,wq/8\n\t        return scores\n\t    def detect_impl(self, que_imgs):\n\t        global count\n\t        qn, _, hq, wq = que_imgs.shape\n\t        hs, ws = hq // 8, wq // 8\n\t        scores = []\n\t        for scale in self.cfg['detection_scales']:\n\t            ht, wt = int(np.round(hq*2**scale)), int(np.round(wq*2**scale))\n", "            if ht%32!=0: ht=(ht//32+1)*32\n\t            if wt%32!=0: wt=(wt//32+1)*32\n\t            que_imgs_cur = F.interpolate(que_imgs,size=(ht,wt),mode='bilinear')\n\t            scores_cur = self.get_scores(que_imgs_cur)\n\t            qn, _, rfn, hcs, wcs = scores_cur.shape\n\t            scores.append(F.interpolate(scores_cur.reshape(qn,3*rfn,hcs,wcs),size=(hs,ws),mode='bilinear').reshape(qn,3,rfn,hs,ws))\n\t        scores = torch.cat(scores, 1) # qn,sn*3,rfn,hq/8,wq/8\n\t        scores = self.score_conv(scores)\n\t        scores_feats = torch.max(scores,2)[0] # qn,f,hq/8,wq/8\n\t        scores = self.score_predict(scores_feats) # qn,1,hq/8,wq/8\n", "        # predict offset and bbox\n\t        _, select_w_id, select_h_id = self.get_select_index(scores)\n\t        que_select_id = torch.stack([select_w_id, select_h_id],1) # qn, 2\n\t        select_offset = self.offset_predict(scores_feats)  # qn,1,hq/8,wq/8\n\t        select_scale = self.scale_predict(scores_feats) # qn,1,hq/8,wq/8\n\t        # count +=1\n\t        outputs = {\n\t                   'scores': scores,\\\n\t                   'que_select_id': que_select_id, \\\n\t                   'pool_ratio': self.pool_ratio, \\\n", "                   'select_pr_offset': select_offset,\\\n\t                   'select_pr_scale': select_scale\n\t        }\n\t        return outputs\n\t    def forward(self, data):\n\t        ref_imgs_info = data['ref_imgs_info'].copy()\n\t        que_imgs_info = data['que_imgs_info'].copy()\n\t        ref_imgs = ref_imgs_info['imgs']\n\t        self.load_impl(ref_imgs)\n\t        outputs = self.detect_impl(que_imgs_info['imgs'])\n", "        return outputs\n\t    def load_ref_imgs(self, ref_imgs):\n\t        \"\"\"\n\t        @param ref_imgs: [an,rfn,h,w,3] in numpy\n\t        @return:\n\t        \"\"\"\n\t        ref_imgs = torch.from_numpy(color_map_forward(ref_imgs)).permute(0,3,1,2).contiguous() # rfn,3,h,w\n\t        ref_imgs = ref_imgs.cuda()\n\t        rfn, _, h, w = ref_imgs.shape\n\t        self.load_impl(ref_imgs)\n", "    def detect_que_imgs(self, que_imgs):\n\t        \"\"\"\n\t        @param que_imgs: [qn,h,w,3]\n\t        @return:\n\t        \"\"\"\n\t        que_imgs = torch.from_numpy(color_map_forward(que_imgs)).permute(0,3,1,2).contiguous().cuda()\n\t        qn, _, h, w = que_imgs.shape\n\t        outputs = self.detect_impl(que_imgs)\n\t        positions, scales = self.parse_detection(\n\t                            outputs['scores'].detach(),\n", "                            outputs['select_pr_scale'].detach(),\n\t                            outputs['select_pr_offset'].detach(),\n\t                            self.pool_ratio)\n\t        detection_results = {'positions': positions, 'scales': scales}\n\t        detection_results = to_cpu_numpy(detection_results)\n\t        return detection_results\n\tif __name__ == \"__main__\":\n\t    # mock_data = torch.randn(6,3,128,128)\n\t    # default_cfg = {\n\t    #     'selector_angle_num': 5,\n", "    # }\n\t    # net = Detector(default_cfg)\n\t    # out =  net.extract_feats(mock_data)\n\t    # print(len(out), out[0].shape, out[1].shape, out[2].shape  )\n\t    from gpu_mem_track import MemTracker\n\t    gpu_tracker = MemTracker()\n\t    dino = VitExtractor(model_name='dino_vits8').eval().cuda()\n\t    data = torch.Tensor(1,3,224,224).cuda()\n\t    import time\n\t    t1 = time.time()\n", "    gpu_tracker.track()\n\t    for i in range(100):\n\t        dino.get_vit_attn_feat(data)\n\t    t2 = time.time()\n\t    gpu_tracker.track()\n\t    print(\"t2-t1:\", (t2-t1)/100 *1000)\n"]}
{"filename": "network/loss.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom network.operator import generate_coords, pose_apply_th\n\tfrom pytorch3d.transforms import quaternion_apply\n\tclass Loss:\n\t    def __init__(self, keys):\n\t        \"\"\"\n\t        keys are used in multi-gpu model, DummyLoss in train_tools.py\n", "        :param keys: the output keys of the dict\n\t        \"\"\"\n\t        self.keys=keys\n\t    def __call__(self, data_pr, data_gt, step, **kwargs):\n\t        pass\n\tclass DetectionSoftmaxLoss(Loss):\n\t    default_cfg={\n\t        'score_diff_thresh': 1.5,\n\t    }\n\t    def __init__(self, cfg):\n", "        self.cfg={**self.default_cfg,**cfg}\n\t        super().__init__(['loss_cls','acc_loc'])\n\t        self.loss_op = nn.BCEWithLogitsLoss(reduction='none')\n\t    def __call__(self, data_pr, data_gt, step, **kwargs):\n\t        center = data_gt['que_imgs_info']['cens'] # qn,2\n\t        pool_ratio = data_pr['pool_ratio']\n\t        center = (center + 0.5) / pool_ratio - 0.5 # qn,2\n\t        # generate label\n\t        scores = data_pr['scores']\n\t        qn,_, h, w = scores.shape\n", "        coords = generate_coords(h, w, scores.device) # h,w,2\n\t        coords = coords.unsqueeze(0).repeat(qn,1,1,1).permute(0,3,1,2) # qn,2,h,w\n\t        center = center[:,:,None,None] # qn,2,h,w\n\t        labels = (torch.norm(coords-center,dim=1)<self.cfg['score_diff_thresh']).float() # qn,h,w\n\t        scores, labels = scores.flatten(1), labels.flatten(1) # [qn,h*w] [qn,h*w]\n\t        loss = self.loss_op(scores, labels)\n\t        loss_pos = torch.sum(loss * labels, 1)/ (torch.sum(labels,1)+1e-3)\n\t        loss_neg = torch.sum(loss * (1-labels), 1)/ (torch.sum(1-labels,1)+1e-3)\n\t        loss = (loss_pos+loss_neg) / 2.0\n\t        return {'loss_cls': loss}\n", "class DetectionOffsetAndScaleLoss(Loss):\n\t    default_cfg={\n\t        'diff_thresh': 1.5,\n\t        'scale_ratio': 1.0,\n\t        'use_offset_loss': True,\n\t        'use_angle_loss': False,\n\t    }\n\t    def __init__(self, cfg):\n\t        self.cfg={**self.default_cfg,**cfg}\n\t        super(DetectionOffsetAndScaleLoss, self).__init__(['loss_scale','loss_offset'])\n", "    # @staticmethod\n\t    # def _check_offset(offset_diff,name):\n\t    #     from utils.base_utils import color_map_backward\n\t    #     from skimage.io import imsave\n\t    #     offset_diff = offset_diff.detach().cpu().numpy()\n\t    #     offset_diff = 1/(1+offset_diff)\n\t    #     imsave(name, color_map_backward(offset_diff[0]))\n\t    #     print('check mode is on !!!')\n\t    def _loss(self, offset_pr, scale_pr, center, scale_gt):\n\t        \"\"\"\n", "        @param offset_pr: [qn,2,h,w]\n\t        @param scale_pr:  [qn,1,h,w]\n\t        @param center:    [qn,2]\n\t        @param scale_gt:  [qn]\n\t        @return:\n\t        \"\"\"\n\t        qn, _, h, w = offset_pr.shape\n\t        coords = generate_coords(h, w, offset_pr.device) # h,w,2\n\t        coords = coords.unsqueeze(0).repeat(qn,1,1,1).permute(0,3,1,2) # qn,2,h,w\n\t        center = center[:,:,None,None].repeat(1,1,h,w) # qn,2,h,w\n", "        diff = center - coords # qn,2,h,w\n\t        mask = torch.norm(diff,2,1)<self.cfg['diff_thresh'] # qn, h, w\n\t        mask = mask.float()\n\t        scale_gt = torch.log2(scale_gt)\n\t        scale_diff = (scale_pr - scale_gt[:, None, None, None]) ** 2\n\t        loss_scale = torch.sum(scale_diff.flatten(1)*mask.flatten(1),1) / (torch.sum(mask.flatten(1),1)+1e-4)\n\t        if self.cfg['use_offset_loss']:\n\t            offset_diff = torch.sum((offset_pr - diff) ** 2, 1) # qn, h, w\n\t            loss_offset = torch.sum(offset_diff.flatten(1)*mask.flatten(1),1) / (torch.sum(mask.flatten(1),1)+1e-4)\n\t        else:\n", "            loss_offset = torch.zeros_like(loss_scale)\n\t        return loss_offset, loss_scale\n\t    def __call__(self, data_pr, data_gt, step, **kwargs):\n\t        center = data_gt['que_imgs_info']['cens']\n\t        pool_ratio = data_pr['pool_ratio']\n\t        center = (center + 0.5) / pool_ratio - 0.5 # qn,2\n\t        loss_offset, loss_scale = self._loss(data_pr['select_pr_offset'], data_pr['select_pr_scale'], center, data_gt['scale_diff'])  # qn\n\t        loss_scale = self.cfg['scale_ratio'] * loss_scale\n\t        return {'loss_scale': loss_scale, 'loss_offset': loss_offset}\n\tclass SelectionLoss(Loss):\n", "    default_cfg={\n\t        \"normalize_gt_score\": True,\n\t    }\n\t    def __init__(self,cfg):\n\t        self.cfg={**self.default_cfg,**cfg}\n\t        super().__init__([])\n\t        self.bce_loss=nn.BCEWithLogitsLoss(reduction='none')\n\t    def __call__(self, data_pr, data_gt, step, **kwargs):\n\t        logits_pr = data_pr['ref_vp_logits'] # qn,rfn\n\t        scores_gt = data_gt['ref_vp_scores'] # qn,rfn\n", "        # scale scores_gt to [0,1]\n\t        # todo: maybe we can scale scores to softmax, normalize scores_gt to sum = 1.0.\n\t        if self.cfg['normalize_gt_score']:\n\t            scores_gt_min = torch.min(scores_gt,1,keepdim=True)[0]\n\t            scores_gt_max = torch.max(scores_gt,1,keepdim=True)[0]\n\t            scores_gt = (scores_gt - scores_gt_min) / torch.clamp(scores_gt_max - scores_gt_min, min=1e-4)\n\t        else:\n\t            scores_gt = (scores_gt + 1) / 2\n\t        loss_score = self.bce_loss(logits_pr, scores_gt)\n\t        loss_score = torch.mean(loss_score,1)\n", "        # angle loss\n\t        angles_pr = data_pr['angles_pr'] # qn, rfn\n\t        angles_gt = data_gt['angles_r2q'] # qn,\n\t        ref_ids_gt = data_gt['gt_ref_ids'] # qn\n\t        qn, rfn = angles_pr.shape\n\t        angles_pr = angles_pr[torch.arange(qn), ref_ids_gt]\n\t        angles_gt = angles_gt*2/np.pi # scale [-90,90] to [-1,1]\n\t        loss_angle = (angles_pr - angles_gt)**2\n\t        return {'loss_score': loss_score, 'loss_angle': loss_angle}\n\tclass RefinerLoss(Loss):\n", "    default_cfg={\n\t        \"scale_log_base\": 2,\n\t        \"loss_space\": 'sim',\n\t    }\n\t    def __init__(self, cfg):\n\t        self.cfg={**self.default_cfg,**cfg}\n\t        super().__init__([])\n\t    @staticmethod\n\t    def apply_rigid_transformation(grids, center, scale, offset, quaternion):\n\t        \"\"\"\n", "        @param grids:       [qn,pn,3]\n\t        @param center:      [qn,1,3]\n\t        @param scale:       [qn,1]\n\t        @param offset:      [qn,2]\n\t        @param quaternion:  [qn,4]\n\t        @return:\n\t        \"\"\"\n\t        pn = grids.shape[1]\n\t        grids_ = quaternion_apply(quaternion[:, None].repeat(1, pn, 1), grids - center) # rotate\n\t        center[:, :, :2] += offset[:, None, :2] # 2D offset\n", "        center[:, :, 2:] *= scale[:, None, :] # scale\n\t        grids_ = grids_ + center\n\t        return grids_\n\t    def __call__(self, data_pr, data_gt, step, **kwargs):\n\t        quaternion_pr = data_pr['rotation'] # qn,4\n\t        offset_pr = data_pr['offset'] # qn,2\n\t        scale_pr = data_pr['scale'] # qn,1\n\t        center = data_gt['object_center'] # qn,3\n\t        poses_in = data_gt['que_imgs_info']['poses_in'] # qn,3,4\n\t        center_in = pose_apply_th(poses_in, center[:,None,:]) # qn,1,3\n", "        grids = data_pr['grids'] # qn,pn,3\n\t        pn = grids.shape[1]\n\t        if self.cfg['loss_space'] == 'sim':\n\t            grids_pr = (self.cfg['scale_log_base'] ** scale_pr[:,None]) * quaternion_apply(quaternion_pr[:,None].repeat(1,pn,1), grids - center_in) + center_in\n\t            grids_pr[...,:2] = grids_pr[...,:2] + offset_pr[:,None,:2]\n\t            grids_gt = pose_apply_th(data_gt['que_imgs_info']['poses_sim_in_to_que'], grids)\n\t        elif self.cfg['loss_space'] == 'raw':\n\t            scale_gt, offset_gt, quaternion_gt = data_gt['scale'].unsqueeze(1), data_gt['offset'], data_gt['rotation']\n\t            grids_gt = self.apply_rigid_transformation(grids, center_in, scale_gt, offset_gt, quaternion_gt)\n\t            scale_pr = self.cfg['scale_log_base'] ** scale_pr\n", "            grids_pr = self.apply_rigid_transformation(grids, center_in, scale_pr, offset_pr, quaternion_pr)\n\t        else:\n\t            raise NotImplementedError\n\t        loss = torch.mean(torch.sum((grids_gt - grids_pr)**2,-1), 1)\n\t        return {'loss_pose': loss}\n\tname2loss={\n\t    'detection_softmax': DetectionSoftmaxLoss,\n\t    'detection_offset_scale': DetectionOffsetAndScaleLoss,\n\t    'selection_loss': SelectionLoss,\n\t    'refiner_loss': RefinerLoss,\n", "}"]}
{"filename": "network/metrics.py", "chunked_list": ["import torch\n\tfrom pathlib import Path\n\timport numpy as np\n\tfrom skimage.io import imsave\n\tfrom transforms3d.axangles import mat2axangle\n\tfrom transforms3d.quaternions import quat2mat\n\tfrom network.loss import Loss\n\tfrom utils.base_utils import color_map_backward, transformation_crop, pose_apply, pose_compose, pose_inverse, \\\n\t    project_points\n\tfrom utils.bbox_utils import parse_bbox_from_scale_offset, bboxes_iou, lthw_to_ltrb\n", "from utils.draw_utils import draw_bbox, concat_images_list, pts_range_to_bbox_pts, draw_bbox_3d\n\tfrom utils.pose_utils import pose_sim_to_pose_rigid, compute_pose_errors\n\tclass VisualizeBBoxScale(Loss):\n\t    default_cfg={\n\t        'output_interval': 250,\n\t    }\n\t    def __init__(self, cfg):\n\t        self.cfg={**self.default_cfg,**cfg}\n\t        super().__init__([])\n\t        self.count_num=0\n", "    def __call__(self, data_pr, data_gt, step, **kwargs):\n\t        data_index=kwargs['data_index']\n\t        model_name=kwargs['model_name']\n\t        output_root = kwargs['output_root'] if 'output_root' in kwargs else 'data/vis'\n\t        b, _, hr, wr = data_gt['ref_imgs_info']['imgs'].shape\n\t        que_select_id = data_pr['que_select_id'][0].cpu().numpy() # 3\n\t        scale_pr = data_pr['select_pr_scale'].detach().cpu().numpy()[0,0]\n\t        offset_pr = data_pr['select_pr_offset'].detach().cpu().numpy()[0]\n\t        pool_ratio = data_pr['pool_ratio']\n\t        ref_shape=(hr,wr)\n", "        bbox_pr = parse_bbox_from_scale_offset(que_select_id, scale_pr, offset_pr, pool_ratio, ref_shape)\n\t        center = data_gt['que_imgs_info']['cens'][0].cpu().numpy()\n\t        scale_gt = data_gt['scale_diff'].cpu().numpy()[0]\n\t        h_gt, w_gt = hr * scale_gt, wr * scale_gt\n\t        # center = bbox[:2] + bbox[2:] / 2\n\t        bbox_gt = np.asarray([center[0] - w_gt / 2, center[1] - h_gt / 2, w_gt, h_gt])\n\t        iou = bboxes_iou(lthw_to_ltrb(bbox_gt[None],False),lthw_to_ltrb(bbox_pr[None],False),False)\n\t        if data_index % self.cfg['output_interval'] != 0:\n\t            return {'iou': iou}\n\t        que_imgs = data_gt['que_imgs_info']['imgs']\n", "        que_imgs = color_map_backward(que_imgs.permute(0, 2, 3, 1).cpu().numpy())\n\t        que_img = que_imgs[0]\n\t        ref_imgs = data_gt['ref_imgs_info']['imgs']\n\t        ref_imgs = color_map_backward(ref_imgs.permute(0, 2, 3, 1).cpu().numpy())\n\t        rfn, hr, wr, _ = ref_imgs.shape\n\t        que_img = draw_bbox(que_img, bbox_pr, color=(0,0,255))\n\t        que_img = draw_bbox(que_img, bbox_gt)\n\t        Path(f'{output_root}/{model_name}').mkdir(exist_ok=True,parents=True)\n\t        imsave(f'{output_root}/{model_name}/{step}-{data_index}-bbox.jpg', que_img)\n\t        return {'iou': iou}\n", "class VisualizeSelector(Loss):\n\t    default_cfg={}\n\t    def __init__(self,cfg):\n\t        self.cfg={**self.default_cfg,**cfg}\n\t        super().__init__([])\n\t    def __call__(self, data_pr, data_gt, step, **kwargs):\n\t        data_index=kwargs['data_index']\n\t        model_name=kwargs['model_name']\n\t        output_root = kwargs['output_root'] if 'output_root' in kwargs else 'data/vis'\n\t        outputs = {}\n", "        logits = data_pr['ref_vp_logits'] # qn,rfn\n\t        order_pr = torch.argsort(-logits, 1) # qn,rfn\n\t        scores_gt = data_gt['ref_vp_scores']\n\t        order_gt = torch.argsort(-scores_gt, 1)  # qn,rfn\n\t        order_pr, order_gt = order_pr.cpu().numpy(), order_gt.cpu().numpy()\n\t        order_pr_min = order_pr[:,:1]\n\t        mask1 = np.sum(order_pr_min == order_gt[:,:1], 1).astype(np.float32)\n\t        mask3 = np.sum(order_pr_min == order_gt[:,:3], 1).astype(np.float32)\n\t        mask5 = np.sum(order_pr_min == order_gt[:,:5], 1).astype(np.float32)\n\t        outputs['sel_acc_1'] = mask1\n", "        outputs['sel_acc_3'] = mask3\n\t        outputs['sel_acc_5'] = mask5\n\t        angles_pr = data_pr['angles_pr'].cpu().numpy()*np.pi/2 # qn,rfn\n\t        angles_gt = data_gt['angles_r2q'].cpu().numpy() # qn,\n\t        gt_ref_ids = data_gt['gt_ref_ids'].cpu().numpy() # qn\n\t        angles_pr_ = angles_pr[np.arange(gt_ref_ids.shape[0]),gt_ref_ids]\n\t        angles_diff = angles_pr_ - angles_gt # qn\n\t        angles_diff = np.abs(np.rad2deg(angles_diff))\n\t        angle5 = (angles_diff < 5).astype(np.float32)\n\t        angle15 = (angles_diff < 15).astype(np.float32)\n", "        angle30 = (angles_diff < 30).astype(np.float32)\n\t        outputs['sel_ang_5'] = angle5\n\t        outputs['sel_ang_15'] = angle15\n\t        outputs['sel_ang_30'] = angle30\n\t        outputs['angles_diff'] = angles_diff\n\t        if data_index % self.cfg['output_interval']!=0:\n\t            return outputs\n\t        # visualize selected viewpoints and regressed rotations\n\t        ref_imgs = data_gt['ref_imgs'] # an,rfn,3,h,w\n\t        que_imgs = data_gt['que_imgs_info']['imgs'] # qn,3,h,w\n", "        ref_imgs = color_map_backward(ref_imgs.cpu().numpy()).transpose([0,1,3,4,2]) # an,rfn,h,w,3\n\t        que_imgs = color_map_backward(que_imgs.cpu().numpy()).transpose([0,2,3,1]) # qn,h,w,3\n\t        imgs_out=[]\n\t        for qi in range(que_imgs.shape[0]):\n\t            que_img = que_imgs[qi] # h,w,3\n\t            h, w, _ = que_img.shape\n\t            gt_rot_img_gt, _ = transformation_crop(que_img,np.asarray([w/2,h/2],np.float32),1.0,-angles_gt[qi],h)\n\t            rot_img_gt, _ = transformation_crop(que_img,np.asarray([w/2,h/2],np.float32),1.0,-angles_pr[qi,order_gt[qi,0]],h)\n\t            rot_img_pr, _ = transformation_crop(que_img,np.asarray([w/2,h/2],np.float32),1.0,-angles_pr[qi,order_pr[qi,0]],h)\n\t            gt_imgs = [que_img, gt_rot_img_gt]\n", "            pr_imgs = [rot_img_gt, rot_img_pr]\n\t            gt_imgs += [ref_imgs[2,k] for k in order_gt[qi,:5]]\n\t            pr_imgs += [ref_imgs[2,k] for k in order_pr[qi,:5]]\n\t            imgs_out.append(concat_images_list(concat_images_list(*gt_imgs),concat_images_list(*pr_imgs),vert=True))\n\t        Path(f'{output_root}/{model_name}').mkdir(exist_ok=True, parents=True)\n\t        imsave(f'{output_root}/{model_name}/{step}-{data_index}-region.jpg',concat_images_list(*imgs_out,vert=True))\n\t        return outputs\n\tclass RefinerMetrics(Loss):\n\t    default_cfg={\n\t        \"output_interval\": 15,\n", "        \"scale_log_base\": 2,\n\t    }\n\t    def __init__(self, cfg):\n\t        self.cfg={**self.default_cfg,**cfg}\n\t        super().__init__([])\n\t    def __call__(self, data_pr, data_gt, step, **kwargs):\n\t        # 'quaternion': quaternion, 'offset': offset, 'scale': scale\n\t        quat_pr = data_pr['rotation'].cpu().numpy() # b,4\n\t        offset_pr = data_pr['offset'].cpu().numpy() # b,2\n\t        scale_pr = data_pr['scale'].cpu().numpy() # b,1\n", "        quat_gt = data_gt['rotation'].cpu().numpy() # b,4\n\t        offset_gt = data_gt['offset'].cpu().numpy()[:,:2] # b,2\n\t        scale_gt = data_gt['scale'].cpu().numpy() # b,\n\t        outputs = {}\n\t        offset_err = np.linalg.norm(offset_pr - offset_gt,2,1) # b\n\t        offset_acc_01 = (offset_err < 0.1).astype(np.float32)\n\t        offset_acc_02 = (offset_err < 0.2).astype(np.float32)\n\t        offset_acc_03 = (offset_err < 0.3).astype(np.float32)\n\t        outputs.update({'off_acc_01': offset_acc_01, 'off_acc_02': offset_acc_02, 'off_acc_03': offset_acc_03,})\n\t        rot_gt = [quat2mat(quat) for quat in quat_gt]\n", "        rot_pr = [quat2mat(quat) for quat in quat_pr]\n\t        rot_err = [mat2axangle(gt.T @ pr)[1] for gt, pr in zip(rot_gt, rot_pr)]\n\t        rot_err = np.abs(np.rad2deg(rot_err))\n\t        rot_acc_5 = (rot_err<5).astype(np.float32)\n\t        rot_acc_10 = (rot_err<10).astype(np.float32)\n\t        rot_acc_15 = (rot_err<15).astype(np.float32)\n\t        outputs.update({'rot_acc_5': rot_acc_5, 'rot_acc_10': rot_acc_10, 'rot_acc_15': rot_acc_15,})\n\t        scale_pr = self.cfg['scale_log_base'] ** scale_pr[...,0]\n\t        scale_err = np.abs(np.log2(scale_pr/scale_gt))\n\t        scale_acc_005 = (scale_err<0.05).astype(np.float32)\n", "        scale_acc_003 = (scale_err<0.03).astype(np.float32)\n\t        scale_acc_001 = (scale_err<0.01).astype(np.float32)\n\t        outputs.update({'sc_acc_001': scale_acc_001, 'sc_acc_003': scale_acc_003, 'sc_acc_005': scale_acc_005})\n\t        # estimate pose\n\t        que_imgs_info = data_gt['que_imgs_info']\n\t        poses_raw_gt = que_imgs_info['poses_raw'].cpu().numpy()\n\t        Ks_raw = que_imgs_info['Ks_raw'].cpu().numpy()\n\t        Ks_que = que_imgs_info['Ks'].cpu().numpy()\n\t        Ks_in = que_imgs_info['Ks_in'].cpu().numpy()\n\t        poses_rect = que_imgs_info['poses_rect'].cpu().numpy()\n", "        poses_in = que_imgs_info['poses_in'].cpu().numpy()\n\t        # poses_sim_in_to_que = que_imgs_info['poses_sim_in_to_que'].cpu().numpy()\n\t        object_points = data_gt['object_points'].cpu().numpy()\n\t        object_diameter = data_gt['object_diameter'].cpu().numpy()\n\t        object_center = data_gt['object_center'].cpu().numpy()\n\t        qn = object_center.shape[0]\n\t        prj_errs, obj_errs, pose_errs, pose_pr_list = [], [], [], []\n\t        for qi in range(qn):\n\t            offset = np.concatenate([offset_pr[qi],np.zeros(1)])\n\t            scale = scale_pr[qi]\n", "            rotation = quat2mat(quat_pr[qi])\n\t            center_in = pose_apply(poses_in[qi], object_center[qi])\n\t            center_que = center_in + offset\n\t            offset = center_que - (scale * rotation @ center_in)\n\t            pose_sim_in_to_que = np.concatenate([scale * rotation, offset[:,None]],1)\n\t            pose_in = poses_in[qi]\n\t            pose_que = pose_sim_to_pose_rigid(pose_sim_in_to_que, pose_in, Ks_que[qi], Ks_in[qi], object_center[qi]) # obj to que\n\t            pose_rect = poses_rect[qi]\n\t            que_pose_pr = pose_compose(pose_que, pose_inverse(pose_rect)) # obj to raw\n\t            pose_pr_list.append(que_pose_pr)\n", "            que_pose_gt = poses_raw_gt[qi]\n\t            prj_err, obj_err, pose_err = compute_pose_errors(object_points[qi],que_pose_pr,que_pose_gt,Ks_raw[qi])\n\t            prj_errs.append(prj_err)\n\t            obj_errs.append(obj_err)\n\t            pose_errs.append(pose_err)\n\t        prj_errs = np.stack(prj_errs, 0)\n\t        obj_errs = np.stack(obj_errs, 0)\n\t        pose_errs = np.stack(pose_errs, 0)\n\t        add_01 = np.asarray(obj_errs<object_diameter*0.1,np.float32)\n\t        prj_5 = np.asarray(prj_errs<5,np.float32)\n", "        outputs.update({'prj_errs': prj_errs, 'obj_errs': obj_errs, 'R_errs': pose_errs[:,0], 't_errs': pose_errs[:,1], 'add_01': add_01, 'prj_5': prj_5})\n\t        data_index=kwargs['data_index']\n\t        if data_index % self.cfg['output_interval']!=0:\n\t            return outputs\n\t        model_name=kwargs['model_name']\n\t        output_root = kwargs['output_root'] if 'output_root' in kwargs else 'data/vis'\n\t        que_imgs = color_map_backward(data_gt['que_imgs_info']['imgs'].cpu().numpy()).transpose([0,2,3,1])\n\t        ref_imgs = color_map_backward(data_gt['ref_imgs_info']['imgs'].cpu().numpy()).transpose([0,1,3,4,2])\n\t        qn, h_, w_, _ = que_imgs.shape\n\t        # visualize bbox\n", "        que_img_raw = color_map_backward(que_imgs_info['imgs_raw'].cpu().numpy()).transpose([0,2,3,1])\n\t        bbox_imgs = []\n\t        for qi in range(min(qn,4)):\n\t            # compute the initial pose\n\t            object_bbox_3d = pts_range_to_bbox_pts(np.max(object_points[qi], 0), np.min(object_points[qi], 0))\n\t            pose_pr = pose_pr_list[qi]\n\t            pose_gt = poses_raw_gt[qi]\n\t            pose_in = que_imgs_info['pose_in_raw'][qi].cpu().numpy()\n\t            bbox_pts_pr, _ = project_points(object_bbox_3d,pose_pr,Ks_raw[qi])\n\t            bbox_pts_gt, _ = project_points(object_bbox_3d,pose_gt,Ks_raw[qi])\n", "            bbox_pts_in, _ = project_points(object_bbox_3d,pose_in,Ks_raw[qi])\n\t            bbox_img = que_img_raw[qi]\n\t            bbox_img = concat_images_list(bbox_img, concat_images_list(que_imgs[qi],*(ref_imgs[qi]), vert=True))\n\t            bbox_imgs.append(bbox_img)\n\t        Path(f'{output_root}/{model_name}').mkdir(exist_ok=True, parents=True)\n\t        imsave(f'{output_root}/{model_name}/{step}-{data_index}-bbox.jpg',concat_images_list(*bbox_imgs))\n\t        return outputs\n\tname2metrics={\n\t    'vis_bbox_scale': VisualizeBBoxScale,\n\t    'vis_sel': VisualizeSelector,\n", "    'ref_metrics': RefinerMetrics\n\t}\n\tdef selector_ang_acc(results):\n\t    return np.mean(results['sel_acc_3'])+np.mean(results['sel_ang_5'])\n\tdef mean_iou(results):\n\t    return np.mean(results['iou'])\n\tdef pose_add(results):\n\t    return np.mean(results['add_01'])\n\tname2key_metrics={\n\t    'mean_iou': mean_iou,\n", "    'sel_ang_acc': selector_ang_acc,\n\t    'pose_add': pose_add,\n\t}"]}
{"filename": "network/dino_transformer.py", "chunked_list": ["\"\"\"\n\tMostly copy-paste from timm library.\n\thttps://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n\t\"\"\"\n\timport math\n\tfrom functools import partial\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tdef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n", "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n\t    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n\t    def norm_cdf(x):\n\t        # Computes standard normal cumulative distribution function\n\t        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\t    if (mean < a - 2 * std) or (mean > b + 2 * std):\n\t        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n\t                      \"The distribution of values may be incorrect.\",\n\t                      stacklevel=2)\n\t    with torch.no_grad():\n", "        # Values are generated by using a truncated uniform distribution and\n\t        # then using the inverse CDF for the normal distribution.\n\t        # Get upper and lower cdf values\n\t        l = norm_cdf((a - mean) / std)\n\t        u = norm_cdf((b - mean) / std)\n\t        # Uniformly fill tensor with values from [l, u], then translate to\n\t        # [2l-1, 2u-1].\n\t        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\t        # Use inverse cdf transform for normal distribution to get truncated\n\t        # standard normal\n", "        tensor.erfinv_()\n\t        # Transform to proper mean, std\n\t        tensor.mul_(std * math.sqrt(2.))\n\t        tensor.add_(mean)\n\t        # Clamp to ensure it's in the proper range\n\t        tensor.clamp_(min=a, max=b)\n\t        return tensor\n\tdef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n\t    # type: (Tensor, float, float, float, float) -> Tensor\n\t    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n", "def drop_path(x, drop_prob: float = 0., training: bool = False):\n\t    if drop_prob == 0. or not training:\n\t        return x\n\t    keep_prob = 1 - drop_prob\n\t    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n\t    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n\t    random_tensor.floor_()  # binarize\n\t    output = x.div(keep_prob) * random_tensor\n\t    return output\n\tclass PositionEmbeddingSine(nn.Module):\n", "    \"\"\"\n\t    This is a more standard version of the position embedding, very similar to the one\n\t    used by the Attention is all you need paper, generalized to work on images.\n\t    \"\"\"\n\t    def __init__(self, num_pos_feats=384, temperature=10000, scale=None):\n\t        super().__init__()\n\t        self.num_pos_feats = num_pos_feats\n\t        self.temperature = temperature\n\t        if scale is None:\n\t            scale = 2 * math.pi\n", "        self.scale = scale\n\t    def forward(self, u, v):  # [B,L]\n\t        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=u.device)\n\t        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n\t        pos_x = u[:, :, None] / dim_t * self.scale\n\t        pos_y = v[:, :, None] / dim_t * self.scale\n\t        pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n\t        pos_y = torch.stack((pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()), dim=3).flatten(2)\n\t        pos = torch.cat((pos_y, pos_x), dim=2)\n\t        return pos\n", "class SinglePositionEmbeddingSine(nn.Module):\n\t    \"\"\"\n\t    This is a more standard version of the position embedding, very similar to the one\n\t    used by the Attention is all you need paper, generalized to work on images.\n\t    \"\"\"\n\t    def __init__(self, num_pos_feats=384, temperature=10000, scale=None):\n\t        super().__init__()\n\t        self.num_pos_feats = num_pos_feats\n\t        self.temperature = temperature\n\t        if scale is None:\n", "            scale = 2 * math.pi\n\t        self.scale = scale\n\t    def forward(self, x):  # [B,L]\n\t        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n\t        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n\t        pos_x = x[:, :, None] / dim_t * self.scale\n\t        pos_x = torch.stack((pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()), dim=3).flatten(2)\n\t        return pos_x\n\tclass DropPath(nn.Module):\n\t    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n", "    \"\"\"\n\t    def __init__(self, drop_prob=None):\n\t        super(DropPath, self).__init__()\n\t        self.drop_prob = drop_prob\n\t    def forward(self, x):\n\t        return drop_path(x, self.drop_prob, self.training)\n\tclass Mlp(nn.Module):\n\t    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n", "        hidden_features = hidden_features or in_features\n\t        self.fc1 = nn.Linear(in_features, hidden_features)\n\t        self.act = act_layer()\n\t        self.fc2 = nn.Linear(hidden_features, out_features)\n\t        self.drop = nn.Dropout(drop)\n\t    def forward(self, x):\n\t        x = self.fc1(x)\n\t        x = self.act(x)\n\t        x = self.drop(x)\n\t        x = self.fc2(x)\n", "        x = self.drop(x)\n\t        return x\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n\t        super().__init__()\n\t        self.num_heads = num_heads\n\t        head_dim = dim // num_heads\n\t        # self.scale = qk_scale or head_dim ** -0.5\n\t        self.qk_scale = qk_scale  # 224 (14**2)\n\t        self.scale = head_dim ** -0.5\n", "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(dim, dim)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n\t    def forward(self, x):\n\t        B, N, C = x.shape\n\t        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv[0], qkv[1], qkv[2]\n\t        if self.qk_scale == 'default':\n\t            scale = self.scale\n", "        else:\n\t            scale = math.log(N, self.qk_scale ** 2 + 1) * self.scale\n\t        attn = (q @ k.transpose(-2, -1)) * scale  # * self.scale\n\t        attn = attn.softmax(dim=-1)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x, attn\n\tclass CrossAttention(nn.Module):\n", "    def __init__(self, dim, num_heads=8, qkv_bias=False, nview=5):\n\t        super().__init__()\n\t        self.num_heads = num_heads\n\t        self.nview = nview\n\t        self.eps = 1e-6\n\t        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n\t        self.proj = nn.Linear(dim, dim)\n\t    def forward(self, x):  # x:[BV,HW,C]\n\t        BV, HW, C = x.shape\n\t        V = self.nview\n", "        B = BV // V\n\t        x = x.reshape(B, V, HW, C).reshape(B, V * HW, C)\n\t        qkv = self.qkv(x).reshape(B, V * HW, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv[0], qkv[1], qkv[2]  # [B,nh,VHW,C]\n\t        q = q.permute(0, 2, 1, 3).contiguous()  # [B,VHW,nh,C2]\n\t        k = k.permute(0, 2, 1, 3).contiguous()\n\t        v = v.permute(0, 2, 1, 3).contiguous()\n\t        k = F.elu(k) + 1.0\n\t        q = F.elu(q) + 1.0\n\t        q = q.to(torch.float32)\n", "        k = k.to(torch.float32)\n\t        v = v.to(torch.float32)\n\t        with torch.cuda.amp.autocast(enabled=False):\n\t            kv = torch.einsum(\"nlhd,nlhm->nhmd\", k, v)  # [B,nh,C2,C2]\n\t            # Compute the normalizer\n\t            z = 1 / (torch.einsum(\"nlhd,nhd->nlh\", q, k.sum(dim=1)) + self.eps)\n\t            # Finally compute and return the new values\n\t            y = torch.einsum(\"nlhd,nhmd,nlh->nlhm\", q, kv, z)  # [B,VHW,nh,C2]\n\t        y = y.reshape(B, V, HW, C).reshape(BV, HW, C)\n\t        y = self.proj(y)\n", "        return y\n\tclass Block(nn.Module):\n\t    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n\t                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n\t        super().__init__()\n\t        self.norm1 = norm_layer(dim)\n\t        self.attn = Attention(\n\t            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\t        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\t        self.norm2 = norm_layer(dim)\n", "        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\t    def forward(self, x, return_attention=False):\n\t        y, attn = self.attn(self.norm1(x))\n\t        x = x + self.drop_path(y)\n\t        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\t        if return_attention:\n\t            return x, attn\n\t        else:\n\t            return x\n", "class CrossBlock(nn.Module):\n\t    def __init__(self, dim, num_heads, mlp_ratio=2., qkv_bias=False, drop=0.,\n\t                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, nview=5):\n\t        super().__init__()\n\t        self.nview = nview\n\t        self.attn = CrossAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, nview=nview)\n\t        # self.norm1 = norm_layer(dim)\n\t        # self.norm2 = norm_layer(dim)\n\t        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n", "        self.alpha1 = nn.Parameter(torch.tensor(0, dtype=torch.float32), requires_grad=True)\n\t        self.alpha2 = nn.Parameter(torch.tensor(0, dtype=torch.float32), requires_grad=True)\n\t        self.tok_embeddings = nn.Embedding(2, dim)\n\t        # self.epipole_embeddings = PositionEmbeddingSine(dim // 2)\n\t        self.rel_epipole_emb = PositionEmbeddingSine(dim // 4, scale=32 * math.pi)  # 每个patch相对方向大致变化在0.01~0.05\n\t        self.abs_epipole_dis_emb = SinglePositionEmbeddingSine(dim // 4, scale=2 * math.pi)  # 每个极点距离，大致在100~500\n\t        self.abs_epipole_dir_emb = PositionEmbeddingSine(dim // 8, scale=2 * math.pi)  # 每个极点方向\n\t    def forward(self, x, epipole, height, width):\n\t        # x:[BV,1+HW,C], src_epipoles:[B,V-1,2]\n\t        BV, HW, C = x.shape\n", "        B = BV // self.nview\n\t        if epipole is None:\n\t            ref_ids = torch.zeros((B, 1, HW), dtype=torch.long, device=x.device)\n\t            src_ids = torch.ones((B, self.nview - 1, HW), dtype=torch.long, device=x.device)\n\t            tok_ids = torch.cat([ref_ids, src_ids], dim=1)  # [B,V,1+HW]\n\t            tok_ids = tok_ids.reshape(BV, HW)\n\t            tok_emb = self.tok_embeddings(tok_ids)  # [BV,1+HW,C]\n\t        else:\n\t            # 方案3\n\t            y_, x_ = torch.meshgrid([torch.arange(0, height, dtype=torch.float32, device=x.device),\n", "                                     torch.arange(0, width, dtype=torch.float32, device=x.device)])\n\t            x_, y_ = x_.contiguous(), y_.contiguous()\n\t            x_ = x_.reshape(1, 1, height, width)\n\t            y_ = y_.reshape(1, 1, height, width)\n\t            epipole_map = epipole.reshape(B, self.nview - 1, 2, 1, 1)  # [B, V-1, 2, 1, 1]\n\t            rel_u = x_ - epipole_map[:, :, 0, :, :]  # [B, V-1, H, W]\n\t            rel_v = y_ - epipole_map[:, :, 1, :, :]  # [B, V-1, H, W]\n\t            normed_uv = torch.sqrt(rel_u ** 2 + rel_v ** 2)\n\t            rel_u, rel_v = rel_u / (normed_uv + 1e-6), rel_v / (normed_uv + 1e-6)\n\t            rel_u = rel_u.reshape(B * (self.nview - 1), HW - 1)  # (-1~1)\n", "            rel_v = rel_v.reshape(B * (self.nview - 1), HW - 1)  # (-1~1)\n\t            rel_epi_emb = self.rel_epipole_emb(rel_u, rel_v)  # [B(V-1),HW,C//2]\n\t            epipole_map = F.normalize(epipole_map, dim=2)\n\t            epipole_map = epipole_map.repeat(1, 1, 1, height, width)\n\t            abs_u, abs_v = epipole_map[:, :, 0], epipole_map[:, :, 1]  # [B,V-1,H,W]\n\t            abs_u = abs_u.reshape(B * (self.nview - 1), HW - 1)  # (-1~1)\n\t            abs_v = abs_v.reshape(B * (self.nview - 1), HW - 1)  # (-1~1)\n\t            abs_epi_dir_emb = self.abs_epipole_dir_emb(abs_u, abs_v)  # [B(V-1),HW,C//4]\n\t            epipole_dis = torch.sqrt(epipole[:, :, 0] ** 2 + epipole[:, :, 1] ** 2) / 512\n\t            epipole_dis = torch.clamp(epipole_dis, 0, 1.0)\n", "            epipole_dis = epipole_dis.reshape(B, self.nview - 1, 1, 1).repeat(1, 1, height, width)  # [B,V-1,H,W]\n\t            epipole_dis = epipole_dis.reshape(B * (self.nview - 1), HW - 1)  # [B(V-1),HW](0~1)\n\t            abs_epi_dis_emb = self.abs_epipole_dis_emb(epipole_dis)  # [B(V-1),HW,C//4]\n\t            abs_epi_emb = torch.cat([abs_epi_dir_emb, abs_epi_dis_emb], dim=-1)\n\t            epi_emb = torch.cat([abs_epi_emb, rel_epi_emb], dim=2)  # [B(V-1),HW,C]\n\t            epi_emb = epi_emb.reshape(B, self.nview - 1, HW - 1, C)\n\t            ref_ids = torch.zeros((B, HW), dtype=torch.long, device=x.device)  # [B,1+HW]\n\t            sep_ids = torch.ones((B, self.nview - 1), dtype=torch.long, device=x.device)  # [B,V-1]\n\t            ref_emb = self.tok_embeddings(ref_ids).unsqueeze(1)  # [B,1,1+HW,C]\n\t            sep_emb = self.tok_embeddings(sep_ids).unsqueeze(2)  # [B,V-1,1,C]\n", "            src_emb = torch.cat([sep_emb, epi_emb], dim=2)  # [B,V-1,1+HW,C]\n\t            tok_emb = torch.cat([ref_emb, src_emb], dim=1)  # [B,V,1+HW,C]\n\t            tok_emb = tok_emb.reshape(BV, HW, C)\n\t        x1 = x + tok_emb\n\t        x2 = x + self.alpha1 * self.attn(x1)\n\t        out = x2 + self.alpha2 * self.mlp(x2)\n\t        return out\n\tclass PatchEmbed(nn.Module):\n\t    \"\"\" Image to Patch Embedding\n\t    \"\"\"\n", "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n\t        super().__init__()\n\t        num_patches = (img_size // patch_size) * (img_size // patch_size)\n\t        self.img_size = img_size\n\t        self.patch_size = patch_size\n\t        self.num_patches = num_patches\n\t        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\t    def forward(self, x):\n\t        B, C, H, W = x.shape\n\t        x = self.proj(x).flatten(2).transpose(1, 2)\n", "        return x\n\tclass VisionTransformer(nn.Module):\n\t    \"\"\" Vision Transformer \"\"\"\n\t    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n\t                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale='default', drop_rate=0., attn_drop_rate=0.,\n\t                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n\t        super().__init__()\n\t        self.num_features = self.embed_dim = embed_dim\n\t        self.nview = kwargs.get('nview', 5)\n\t        self.cross_att = kwargs.get('cross_att', False)\n", "        self.patch_size = patch_size\n\t        self.height = -1\n\t        self.width = -1\n\t        self.patch_embed = PatchEmbed(\n\t            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n\t        num_patches = self.patch_embed.num_patches\n\t        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\t        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n\t        self.pos_drop = nn.Dropout(p=drop_rate)\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n", "        if qk_scale != 'default':\n\t            qk_scale = (224 / patch_size) ** 2\n\t        self.blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n\t                                           qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n\t                                           norm_layer=norm_layer) for i in range(depth)])\n\t        if self.cross_att:\n\t            self.cross_num = kwargs.get('cross_num', 4)\n\t            self.cross_inter = depth // self.cross_num\n\t            self.cross_blocks = nn.ModuleList([CrossBlock(embed_dim, num_heads, mlp_ratio=2., qkv_bias=qkv_bias, drop=0.,\n\t                                                          act_layer=nn.GELU, norm_layer=nn.LayerNorm, nview=5) for _ in range(self.cross_num)])\n", "        self.norm = norm_layer(embed_dim)\n\t        # Classifier head\n\t        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t        trunc_normal_(self.pos_embed, std=.02)\n\t        trunc_normal_(self.cls_token, std=.02)\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n", "                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def interpolate_pos_encoding(self, x, w, h):\n\t        npatch = x.shape[1] - 1\n\t        N = self.pos_embed.shape[1] - 1\n\t        if npatch == N and w == h:\n\t            return self.pos_embed\n\t        class_pos_embed = self.pos_embed[:, 0]\n", "        patch_pos_embed = self.pos_embed[:, 1:]\n\t        dim = x.shape[-1]\n\t        w0 = w // self.patch_embed.patch_size\n\t        h0 = h // self.patch_embed.patch_size\n\t        # we add a small number to avoid floating point error in the interpolation\n\t        # see discussion at https://github.com/facebookresearch/dino/issues/8\n\t        w0, h0 = w0 + 0.1, h0 + 0.1\n\t        patch_pos_embed = F.interpolate(\n\t            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n\t            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n", "            mode='bicubic', align_corners=False\n\t        )\n\t        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n\t        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n\t        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n\t    def prepare_tokens(self, x):\n\t        B, nc, h, w = x.shape\n\t        self.height = h // self.patch_size\n\t        self.width = w // self.patch_size\n\t        x = self.patch_embed(x)  # patch linear embedding\n", "        # add the [CLS] token to the embed patch tokens\n\t        cls_tokens = self.cls_token.expand(B, -1, -1)\n\t        x = torch.cat((cls_tokens, x), dim=1)\n\t        # add positional encoding to each token\n\t        x = x + self.interpolate_pos_encoding(x, h, w)\n\t        return self.pos_drop(x)\n\t    def forward(self, x, src_epipoles=None):\n\t        x = self.prepare_tokens(x)\n\t        for i, blk in enumerate(self.blocks):\n\t            x = blk(x)\n", "            if self.cross_att and (i + 1) % self.cross_inter == 0:\n\t                x = self.cross_blocks[(i + 1) // self.cross_inter - 1](x, src_epipoles, self.height, self.width)\n\t        x = self.norm(x)\n\t        return x\n\t    def forward_with_last_att(self, x):\n\t        x = self.prepare_tokens(x)\n\t        att = None\n\t        for i, blk in enumerate(self.blocks):\n\t            if i < len(self.blocks) - 1:\n\t                x = blk(x)\n", "            else:\n\t                x, att = blk(x, return_attention=True)\n\t        x = self.norm(x)\n\t        return x, att\n\t    def get_last_selfattention(self, x):\n\t        x = self.prepare_tokens(x)\n\t        for i, blk in enumerate(self.blocks):\n\t            if i < len(self.blocks) - 1:\n\t                x = blk(x)\n\t            else:\n", "                # return attention of the last block\n\t                return blk(x, return_attention=True)\n\t    def get_intermediate_layers(self, x, n=1):\n\t        x = self.prepare_tokens(x)\n\t        # we return the output tokens from the `n` last blocks\n\t        output = []\n\t        for i, blk in enumerate(self.blocks):\n\t            x = blk(x)\n\t            if len(self.blocks) - i <= n:\n\t                output.append(self.norm(x))\n", "        return output\n\tclass HRVisionTransformer(nn.Module):\n\t    \"\"\" Vision Transformer \"\"\"\n\t    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n\t                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale='default', drop_rate=0., attn_drop_rate=0.,\n\t                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n\t        super().__init__()\n\t        self.num_features = self.embed_dim = embed_dim\n\t        self.nview = kwargs.get('nview', 5)\n\t        self.cross_att = kwargs.get('cross_att', False)\n", "        self.patch_size = patch_size\n\t        self.height = -1\n\t        self.width = -1\n\t        self.patch_embed = PatchEmbed(\n\t            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n\t        num_patches = self.patch_embed.num_patches\n\t        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\t        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n\t        self.pos_drop = nn.Dropout(p=drop_rate)\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n", "        if qk_scale != 'default':\n\t            qk_scale = (224 / patch_size) ** 2\n\t        self.blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n\t                                           qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n\t                                           norm_layer=norm_layer) for i in range(depth)])\n\t        if self.cross_att:\n\t            self.cross_num = kwargs.get('cross_num', 4)\n\t            self.cross_inter = depth // self.cross_num\n\t            self.cross_blocks = nn.ModuleList([CrossBlock(embed_dim, num_heads, mlp_ratio=2., qkv_bias=qkv_bias, drop=0.,\n\t                                                          act_layer=nn.GELU, norm_layer=nn.LayerNorm, nview=5) for _ in range(self.cross_num)])\n", "        self.norm = norm_layer(embed_dim)\n\t        # Classifier head\n\t        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t        trunc_normal_(self.pos_embed, std=.02)\n\t        trunc_normal_(self.cls_token, std=.02)\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n", "                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def interpolate_pos_encoding(self, x, w, h):\n\t        npatch = x.shape[1] - 1\n\t        N = self.pos_embed.shape[1] - 1\n\t        if npatch == N and w == h:\n\t            return self.pos_embed\n\t        class_pos_embed = self.pos_embed[:, 0]\n", "        patch_pos_embed = self.pos_embed[:, 1:]\n\t        dim = x.shape[-1]\n\t        w0 = w // self.patch_embed.patch_size\n\t        h0 = h // self.patch_embed.patch_size\n\t        # we add a small number to avoid floating point error in the interpolation\n\t        # see discussion at https://github.com/facebookresearch/dino/issues/8\n\t        w0, h0 = w0 + 0.1, h0 + 0.1\n\t        patch_pos_embed = F.interpolate(\n\t            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n\t            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n", "            mode='bicubic', align_corners=False\n\t        )\n\t        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n\t        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n\t        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n\t    def prepare_tokens(self, x):\n\t        B, nc, h, w = x.shape\n\t        self.height = h // self.patch_size\n\t        self.width = w // self.patch_size\n\t        x = self.patch_embed(x)  # patch linear embedding\n", "        # add the [CLS] token to the embed patch tokens\n\t        cls_tokens = self.cls_token.expand(B, -1, -1)\n\t        x = torch.cat((cls_tokens, x), dim=1)\n\t        # add positional encoding to each token\n\t        x = x + self.interpolate_pos_encoding(x, h, w)\n\t        return self.pos_drop(x)\n\t    def forward(self, x, src_epipoles=None):\n\t        x = self.prepare_tokens(x)\n\t        for i, blk in enumerate(self.blocks):\n\t            x = blk(x)\n", "            if self.cross_att and (i + 1) % self.cross_inter == 0:\n\t                x = self.cross_blocks[(i + 1) // self.cross_inter - 1](x, src_epipoles, self.height, self.width)\n\t        x = self.norm(x)\n\t        return x\n\t    def forward_with_last_att(self, x):\n\t        x = self.prepare_tokens(x)\n\t        att = None\n\t        for i, blk in enumerate(self.blocks):\n\t            if i < len(self.blocks) - 1:\n\t                x = blk(x)\n", "            else:\n\t                x, att = blk(x, return_attention=True)\n\t        x = self.norm(x)\n\t        return x, att\n\t    def get_last_selfattention(self, x):\n\t        x = self.prepare_tokens(x)\n\t        for i, blk in enumerate(self.blocks):\n\t            if i < len(self.blocks) - 1:\n\t                x = blk(x)\n\t            else:\n", "                # return attention of the last block\n\t                return blk(x, return_attention=True)\n\t    def get_intermediate_layers(self, x, n=1):\n\t        x = self.prepare_tokens(x)\n\t        # we return the output tokens from the `n` last blocks\n\t        output = []\n\t        for i, blk in enumerate(self.blocks):\n\t            x = blk(x)\n\t            if len(self.blocks) - i <= n:\n\t                output.append(self.norm(x))\n", "        return output\n\tdef vit_tiny(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n\t        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\t    return model\n\tdef vit_small(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n\t        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n", "    return model\n\tdef vit_base(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n\t        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\t    return model\n\tclass DINOHead(nn.Module):\n\t    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n\t        super().__init__()\n\t        nlayers = max(nlayers, 1)\n", "        if nlayers == 1:\n\t            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n\t        else:\n\t            layers = [nn.Linear(in_dim, hidden_dim)]\n\t            if use_bn:\n\t                layers.append(nn.BatchNorm1d(hidden_dim))\n\t            layers.append(nn.GELU())\n\t            for _ in range(nlayers - 2):\n\t                layers.append(nn.Linear(hidden_dim, hidden_dim))\n\t                if use_bn:\n", "                    layers.append(nn.BatchNorm1d(hidden_dim))\n\t                layers.append(nn.GELU())\n\t            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n\t            self.mlp = nn.Sequential(*layers)\n\t        self.apply(self._init_weights)\n\t        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n\t        self.last_layer.weight_g.data.fill_(1)\n\t        if norm_last_layer:\n\t            self.last_layer.weight_g.requires_grad = False\n\t    def _init_weights(self, m):\n", "        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t    def forward(self, x):\n\t        x = self.mlp(x)\n\t        x = F.normalize(x, dim=-1, p=2)\n\t        x = self.last_layer(x)\n\t        return x\n"]}
{"filename": "network/refiner.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchvision\n\timport numpy as np\n\tfrom network.refiner_ablation import FPN\n\tfrom dataset.database import NormalizedDatabase, normalize_pose, get_object_center, get_diameter, denormalize_pose\n\tfrom network.operator import pose_apply_th, normalize_coords\n\tfrom network.pretrain_models import VGGBNPretrainV3\n\tfrom utils.base_utils import pose_inverse, project_points, color_map_forward, to_cuda, pose_compose\n", "from utils.database_utils import look_at_crop, select_reference_img_ids_refinement, normalize_reference_views\n\tfrom utils.pose_utils import let_me_look_at, compose_sim_pose, pose_sim_to_pose_rigid\n\tfrom utils.imgs_info import imgs_info_to_torch\n\tfrom network.vis_dino_encoder import VitExtractor\n\t# sin-cose embedding module\n\tclass Embedder(nn.Module):\n\t    def __init__(self, **kwargs):\n\t        super(Embedder, self).__init__()\n\t        self.kwargs = kwargs\n\t        self.create_embedding_fn()\n", "    def create_embedding_fn(self):\n\t        embed_fns = []\n\t        d = self.kwargs[\"input_dims\"]\n\t        out_dim = 0\n\t        if self.kwargs[\"include_input\"]:\n\t            embed_fns.append(lambda x: x)\n\t            out_dim += d\n\t        max_freq = self.kwargs[\"max_freq_log2\"]\n\t        N_freqs = self.kwargs[\"num_freqs\"]\n\t        if self.kwargs[\"log_sampling\"]:\n", "            freq_bands = 2.0 ** torch.linspace(0.0, max_freq, steps=N_freqs)\n\t        else:\n\t            freq_bands = torch.linspace(2.0**0.0, 2.0**max_freq, steps=N_freqs)\n\t        for freq in freq_bands:\n\t            for p_fn in self.kwargs[\"periodic_fns\"]:\n\t                embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))\n\t                out_dim += d\n\t        self.embed_fns = embed_fns\n\t        self.out_dim = out_dim\n\t    def forward(self, inputs):\n", "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n\tclass FeedForward(nn.Module):\n\t    def __init__(self, dim, hid_dim, dp_rate):\n\t        super(FeedForward, self).__init__()\n\t        self.fc1 = nn.Linear(dim, hid_dim)\n\t        self.fc2 = nn.Linear(hid_dim, dim)\n\t        self.dp = nn.Dropout(dp_rate)\n\t        self.activ = nn.ReLU()\n\t    def forward(self, x):\n\t        x = self.dp(self.activ(self.fc1(x)))\n", "        x = self.dp(self.fc2(x))\n\t        return x\n\t# Subtraction-based efficient attention\n\tclass Attention2D(nn.Module):\n\t    def __init__(self, dim, dp_rate):\n\t        super(Attention2D, self).__init__()\n\t        self.q_fc = nn.Linear(dim, dim, bias=False)\n\t        self.k_fc = nn.Linear(dim, dim, bias=False)\n\t        self.v_fc = nn.Linear(dim, dim, bias=False)\n\t        self.pos_fc = nn.Sequential(\n", "            nn.Linear(4, dim // 8),\n\t            nn.ReLU(),\n\t            nn.Linear(dim // 8, dim),\n\t        )\n\t        self.attn_fc = nn.Sequential(\n\t            nn.Linear(dim, dim // 8),\n\t            nn.ReLU(),\n\t            nn.Linear(dim // 8, dim),\n\t        )\n\t        self.out_fc = nn.Linear(dim, dim)\n", "        self.dp = nn.Dropout(dp_rate)\n\t    def forward(self, q, k, pos, mask=None):\n\t        q = self.q_fc(q)\n\t        k = self.k_fc(k)\n\t        v = self.v_fc(k)\n\t        pos = self.pos_fc(pos)\n\t        attn = k - q[:, :, None, :] + pos\n\t        attn = self.attn_fc(attn)\n\t        if mask is not None:\n\t            attn = attn.masked_fill(mask == 0, -1e9)\n", "        attn = torch.softmax(attn, dim=-2)\n\t        attn = self.dp(attn)\n\t        x = ((v + pos) * attn).sum(dim=2)\n\t        x = self.dp(self.out_fc(x))\n\t        return x\n\t# View Transformer\n\tclass Transformer2D(nn.Module):\n\t    def __init__(self, dim, ff_hid_dim, ff_dp_rate, attn_dp_rate):\n\t        super(Transformer2D, self).__init__()\n\t        self.attn_norm = nn.LayerNorm(dim, eps=1e-6)\n", "        self.ff_norm = nn.LayerNorm(dim, eps=1e-6)\n\t        self.ff = FeedForward(dim, ff_hid_dim, ff_dp_rate)\n\t        self.attn = Attention2D(dim, attn_dp_rate)\n\t    def forward(self, q, k, pos, mask=None):\n\t        residue = q\n\t        x = self.attn_norm(q)\n\t        x = self.attn(x, k, pos, mask)\n\t        x = x + residue\n\t        residue = x\n\t        x = self.ff_norm(x)\n", "        x = self.ff(x)\n\t        x = x + residue\n\t        return x\n\tclass RefineFeatureNet(nn.Module):\n\t    def __init__(self, \\\n\t                 norm_layer='instance',\\\n\t                 use_dino=False,\\\n\t                 upsample=False):\n\t        super().__init__()\n\t        if norm_layer == 'instance':\n", "            norm=nn.InstanceNorm2d\n\t        else:\n\t            raise NotImplementedError\n\t        self.conv0 = nn.Sequential(\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(64, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n", "        self.conv1 = nn.Sequential(\n\t            nn.Conv2d(512, 256, 3, 1, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n\t        self.conv2 = nn.Sequential(\n\t            nn.Conv2d(512, 256, 3, 1, 1),\n\t            norm(256),\n", "            nn.ReLU(True),\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n\t        self.conv_out = nn.Sequential(\n\t            nn.Conv2d(64*3, 128, 3, 1, 1),\n\t            norm(128),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(128, 128, 3, 1, 1),\n\t            norm(128),\n", "        )\n\t        self.upsample = upsample\n\t        self.use_dino = use_dino\n\t        if self.upsample:\n\t            self.down_sample = nn.Conv2d(in_channels=128, \\\n\t                                        out_channels=64, \\\n\t                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True)  \n", "        if self.use_dino:\n\t            self.fuse_conv = nn.Conv2d(in_channels=512, \\\n\t                                       out_channels=128, \\\n\t                                       kernel_size=1,\\\n\t                                       stride=1,\\\n\t                                       padding=0, \\\n\t                                       bias=True)\n\t            self.fuse_conv1 = nn.Conv2d(in_channels=256+384, \\\n\t                            out_channels=256, \\\n\t                            kernel_size=1,\\\n", "                            stride=1,\\\n\t                            padding=0, \\\n\t                            bias=True)  \n\t            self.fuse_conv2 = nn.Conv2d(in_channels=512+384, \\\n\t                                        out_channels=512, \\\n\t                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True) \n\t            self.fuse_conv3 = nn.Conv2d(in_channels=512+384, \\\n", "                            out_channels=512, \\\n\t                            kernel_size=1,\\\n\t                            stride=1,\\\n\t                            padding=0, \\\n\t                            bias=True) \n\t        self.fpn = FPN([512,512,128],128)  \n\t        self.use_fpn = False     \n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n\t                nn.init.kaiming_normal(m.weight.data, mode='fan_in')\n", "                if m.bias is not None:\n\t                    m.bias.data.zero_()\n\t        if self.use_dino:\n\t            self.fea_ext =  VitExtractor(model_name='dino_vits8').eval()\n\t            for para in self.fea_ext.parameters():\n\t                para.requires_grad = False\n\t            self.fea_ext.requires_grad_(False) \n\t        self.backbone = VGGBNPretrainV3().eval()\n\t        for para in self.backbone.parameters():\n\t            para.requires_grad = False\n", "        self.img_norm = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t    def forward(self, imgs):\n\t        _,_, h,w = imgs.shape\n\t        if self.upsample:\n\t            imgs = F.interpolate(imgs, size=(int(1.5*h), int(1.5*h)))\n\t        if self.use_dino:\n\t            dino_imgs = imgs.clone()\n\t        imgs = self.img_norm(imgs)\n\t        self.backbone.eval()\n\t        with torch.no_grad():\n", "            x0, x1, x2 = self.backbone(imgs)\n\t            x0 = F.normalize(x0, dim=1)\n\t            x1 = F.normalize(x1, dim=1)\n\t            x2 = F.normalize(x2, dim=1)\n\t        x0 = self.conv0(x0)\n\t        x1 = F.interpolate(self.conv1(x1),scale_factor=2,mode='bilinear')\n\t        x2 = F.interpolate(self.conv2(x2),scale_factor=4,mode='bilinear')\n\t        x = torch.cat([x0,x1,x2],1)\n\t        if self.use_fpn:\n\t            x = self.fpn([x0,x1,x])\n", "        else:\n\t            x = self.conv_out(x)  \n\t        if self.use_dino:\n\t            # -------------------------------------------------------- \n\t            dino_imgs = F.interpolate(dino_imgs, size=(256, 256)) \n\t            dino_ret =  self.fea_ext.get_vit_attn_feat(dino_imgs)\n\t            attn, cls_, feat = dino_ret['attn'], dino_ret['cls_'], dino_ret['feat']\n\t            dino_fea = feat.permute(0,2,1).reshape(-1,384,32,32)    \n\t            fused_fea = torch.cat( (x,dino_fea), dim = 1)\n\t            x = self.fuse_conv(fused_fea)\n", "            # --------------------------------------------------------\n\t        return x\n\tclass RefineVolumeEncodingNet(nn.Module):\n\t    def __init__(self,norm_layer='no_norm'):\n\t        super().__init__()\n\t        if norm_layer == 'instance':\n\t            norm=nn.InstanceNorm3d\n\t        else:\n\t            raise NotImplementedError\n\t        self.mean_embed = nn.Sequential(\n", "            nn.Conv3d(128 * 2, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(64, 64, 3, 1, 1)\n\t        )\n\t        self.var_embed = nn.Sequential(\n\t            nn.Conv3d(128, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(64, 64, 3, 1, 1)\n", "        )\n\t        self.conv0 = nn.Sequential(\n\t            nn.Conv3d(64*2, 64, 3, 1, 1), # 32\n\t            norm(64),\n\t            nn.ReLU(True),\n\t        ) # 32\n\t        self.conv1 = nn.Sequential(\n\t            nn.Conv3d(64, 128, 3, 2, 1),\n\t            norm(128),\n\t            nn.ReLU(True),\n", "        )\n\t        self.conv2 = nn.Sequential(\n\t            nn.Conv3d(128, 128, 3, 1, 1),\n\t            norm(128),\n\t            nn.ReLU(True),\n\t        ) # 16\n\t        self.conv3 = nn.Sequential(\n\t            nn.Conv3d(128, 256, 3, 2, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n", "        )\n\t        self.conv4 = nn.Sequential(\n\t            nn.Conv3d(256, 256, 3, 1, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n\t        )  #8\n\t        self.conv5 = nn.Sequential(\n\t            nn.Conv3d(256, 512, 3, 2, 1),\n\t            norm(512),\n\t            nn.ReLU(True),\n", "            nn.Conv3d(512, 512, 3, 1, 1)\n\t        )\n\t    def forward(self, mean, var):\n\t        x = torch.cat([self.mean_embed(mean),self.var_embed(var)],1)\n\t        x = self.conv0(x)\n\t        x = self.conv2(self.conv1(x))\n\t        x = self.conv4(self.conv3(x))\n\t        x = self.conv5(x)\n\t        return x\n\tdef fc(in_planes, out_planes, relu=True):\n", "    if relu:\n\t        return nn.Sequential(\n\t            nn.Linear(in_planes, out_planes),\n\t            nn.LeakyReLU(0.1, inplace=True))\n\t    else:\n\t        return nn.Linear(in_planes, out_planes)\n\tclass RefineRegressor(nn.Module):\n\t    def __init__(self, upsample=False):\n\t        super().__init__()\n\t        if upsample:\n", "            self.fc = nn.Sequential(   fc( int((1.5)**3*512 * 4**3) , 512), nn.Dropout(p=0.15), fc(512, 512))\n\t        else:\n\t            self.fc = nn.Sequential(fc(512 * 4**3, 512), fc(512, 512))\n\t        self.fcr = nn.Linear(512,4)\n\t        self.fct = nn.Linear(512,2)\n\t        self.fcs = nn.Linear(512,1)\n\t    def forward(self, x):\n\t        x = self.fc(x)\n\t        r = F.normalize(self.fcr(x),dim=1)\n\t        t = self.fct(x)\n", "        s = self.fcs(x)\n\t        return r, t, s\n\tclass Transformer(nn.Module):\n\t    def __init__(self, input_size, output_size, hidden_size, num_layer, nhead=8, dropout=0.1):\n\t        super(Transformer, self).__init__()\n\t        self.linear1 = nn.Linear(input_size, hidden_size)\n\t        self.transformer_encoder = nn.Sequential(*[nn.TransformerEncoderLayer(hidden_size, \\\n\t            nhead=nhead, dropout=dropout, batch_first=True) for _ in range(num_layer)])\n\t        self.linear2 = nn.Linear(hidden_size, output_size)\n\t        self.dropout = nn.Dropout(dropout)\n", "    def forward(self, x, state=None):\n\t        x = self.linear1(self.dropout(x))\n\t        output = self.transformer_encoder(x)\n\t        output = self.linear2(output)\n\t        return output, state\n\tfrom loguru import logger\n\tclass VolumeRefiner(nn.Module):\n\t    default_cfg = {\n\t        \"refiner_sample_num\": 32,\n\t    }\n", "    def __init__(self, cfg, upsample=False):\n\t        self.cfg={**self.default_cfg, **cfg}\n\t        super().__init__()\n\t        self.use_dino = self.cfg.get(\"use_dino\", False)  \n\t        self.use_transformer = self.cfg.get(\"use_transformer\", False) \n\t        logger.debug( f\"VolumeRefiner use_dino:{self.use_dino}, use_transformer:{self.use_transformer}\" )\n\t        self.upsample = upsample\n\t        self.feature_net = RefineFeatureNet('instance', self.use_dino, upsample)\n\t        self.volume_net = RefineVolumeEncodingNet('instance')\n\t        self.regressor = RefineRegressor(upsample)\n", "        # used in inference\n\t        self.ref_database = None\n\t        self.ref_ids = None\n\t        if self.use_transformer:\n\t            self.view_trans = Transformer(\n\t                input_size=32768, \n\t                output_size=32768, \n\t                hidden_size=64,\n\t                num_layer=1,\n\t                nhead=8, \n", "                dropout=0.1\n\t            )\n\t    @staticmethod\n\t    def interpolate_volume_feats(feats, verts, projs, h_in, w_in):\n\t        \"\"\"\n\t        @param feats: b,f,h,w \n\t        @param verts: b,sx,sy,sz,3\n\t        @param projs: b,3,4 : project matric\n\t        @param h_in:  int\n\t        @param w_in:  int\n", "        @return:\n\t        \"\"\"\n\t        b, sx, sy, sz, _ = verts.shape\n\t        b, f, h, w = feats.shape\n\t        R, t = projs[:,:3,:3], projs[:,:3,3:] # b,3,3  b,3,1\n\t        verts = verts.reshape(b,sx*sy*sz,3)\n\t        verts = verts @ R.permute(0, 2, 1) + t.permute(0, 2, 1) #\n\t        depth = verts[:, :, -1:]\n\t        depth[depth < 1e-4] = 1e-4\n\t        verts = verts[:, :, :2] / depth  # [b,sx*sy*sz,2]\n", "        verts = normalize_coords(verts, h_in, w_in) # b,sx*sy*sz,2]\n\t        verts = verts.reshape([b, sx, sy*sz, 2])\n\t        volume_feats = F.grid_sample(feats, verts, mode='bilinear', align_corners=False) # b,f,sx,sy*sz\n\t        return volume_feats.reshape(b, f, sx, sy, sz)\n\t    def construct_feature_volume(self, que_imgs_info, ref_imgs_info, feature_extractor, sample_num):\n\t        \"\"\"_summary_\n\t        Args:\n\t            que_imgs_info (_type_): _description_\n\t            ref_imgs_info (_type_): _description_\n\t            feature_extractor (_type_): 特征提取器\n", "            sample_num (_type_): 采样图片的个数\n\t        Returns:\n\t            _type_: _description_\n\t        \"\"\"\n\t        # build a volume on the unit cube\n\t        sn = sample_num\n\t        device = que_imgs_info['imgs'].device\n\t        vol_coords = torch.linspace(-1, 1, sample_num, dtype=torch.float32, device=device)\n\t        vol_coords = torch.stack(torch.meshgrid(vol_coords,vol_coords,vol_coords),-1) # sn,sn,sn,3\n\t        vol_coords = vol_coords.reshape(1,sn**3,3)\n", "        # rotate volume to align with the input pose, but still in the object coordinate\n\t        poses_in = que_imgs_info['poses_in'] # qn,3,4\n\t        rotation = poses_in[:,:3,:3] # qn,3,3\n\t        vol_coords = vol_coords @ rotation # qn,sn**3,3\n\t        qn = poses_in.shape[0]\n\t        vol_coords = vol_coords.reshape(qn, sn, sn, sn, 3)\n\t        # project onto every reference view\n\t        ref_poses = ref_imgs_info['poses'] # qn,rfn,3,4\n\t        ref_Ks = ref_imgs_info['Ks'] # qn,rfn,3,3\n\t        ref_proj = ref_Ks @ ref_poses # qn,rfn,3,4\n", "        vol_feats_mean, vol_feats_std = [], []\n\t        h_in, w_in = ref_imgs_info['imgs'].shape[-2:]\n\t        for qi in range(qn):\n\t            ref_feats = feature_extractor(ref_imgs_info['imgs'][qi]) # rfn,f,h,w\n\t            rfn = ref_feats.shape[0]\n\t            vol_coords_cur = vol_coords[qi:qi+1].repeat(rfn,1,1,1,1) # rfn,sx,sy,sz,3\n\t            vol_feats = VolumeRefiner.interpolate_volume_feats(ref_feats, vol_coords_cur, ref_proj[qi], h_in, w_in)\n\t            if self.use_transformer:\n\t                x = vol_feats.view(rfn,128,sn*sn*sn)\n\t                x  = self.view_trans(x)\n", "                vol_feats = x[0].view(rfn,128,sn,sn,sn)\n\t            vol_feats_mean.append(torch.mean(vol_feats, 0))\n\t            vol_feats_std.append(torch.std(vol_feats, 0))\n\t        vol_feats_mean = torch.stack(vol_feats_mean, 0)\n\t        vol_feats_std = torch.stack(vol_feats_std, 0)\n\t        # project onto query view\n\t        h_in, w_in = que_imgs_info['imgs'].shape[-2:]\n\t        que_feats = feature_extractor(que_imgs_info['imgs']) # qn,f,h,w\n\t        que_proj = que_imgs_info['Ks_in'] @ que_imgs_info['poses_in']\n\t        vol_feats_in = VolumeRefiner.interpolate_volume_feats(que_feats, vol_coords, que_proj, h_in, w_in) # qn,f,sx,sy,sz\n", "        return vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords\n\t    def forward(self, data):\n\t        is_inference = data['inference'] if 'inference' in data else False\n\t        que_imgs_info = data['que_imgs_info'].copy()\n\t        ref_imgs_info = data['ref_imgs_info'].copy()\n\t        if self.upsample:\n\t            refiner_sample_num = int(self.cfg['refiner_sample_num']*1.5) \n\t        else:\n\t            refiner_sample_num = self.cfg['refiner_sample_num']\n\t        vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords = self.construct_feature_volume(\n", "            que_imgs_info, ref_imgs_info, self.feature_net, refiner_sample_num) # qn,f,dn,h,w   qn,dn\n\t        vol_feats = torch.cat([vol_feats_mean, vol_feats_in], 1)\n\t        vol_feats = self.volume_net(vol_feats, vol_feats_std)\n\t        vol_feats = vol_feats.flatten(1) # qn, f* 4**3\n\t        rotation, offset, scale = self.regressor(vol_feats)\n\t        outputs={'rotation': rotation, 'offset': offset, 'scale': scale}\n\t        if not is_inference:\n\t            # used in training not inference\n\t            qn, sx, sy, sz, _ = vol_coords.shape\n\t            grids = pose_apply_th(que_imgs_info['poses_in'], vol_coords.reshape(qn, sx * sy * sz, 3))\n", "            outputs['grids'] = grids\n\t        return outputs\n\t    def load_ref_imgs(self,ref_database,ref_ids):\n\t        self.ref_database = ref_database\n\t        self.ref_ids = ref_ids\n\t    def refine_que_imgs(self, que_img, que_K, in_pose, size=128, ref_num=6, ref_even=False):\n\t        \"\"\"\n\t        @param que_img:  [h,w,3]\n\t        @param que_K:    [3,3]\n\t        @param in_pose:  [3,4]\n", "        @param size:     int\n\t        @param ref_num:  int\n\t        @param ref_even: bool\n\t        @return:\n\t        \"\"\"\n\t        margin = 0.05\n\t        ref_even_num = min(128,len(self.ref_ids))\n\t        # normalize database and input pose\n\t        ref_database = NormalizedDatabase(self.ref_database) # wrapper: object is in the unit sphere at origin\n\t        in_pose = normalize_pose(in_pose, ref_database.scale, ref_database.offset)\n", "        object_center = get_object_center(ref_database)\n\t        object_diameter = get_diameter(ref_database)\n\t        # warp the query image to look at the object w.r.t input pose\n\t        _, new_f = let_me_look_at(in_pose, que_K, object_center)\n\t        in_dist = np.linalg.norm(pose_inverse(in_pose)[:,3] - object_center)\n\t        in_f = size * (1 - margin) / object_diameter * in_dist\n\t        scale = in_f / new_f\n\t        position = project_points(object_center[None], in_pose, que_K)[0][0]\n\t        que_img_warp, que_K_warp, in_pose_warp, que_pose_rect, H = look_at_crop(\n\t            que_img, que_K, in_pose, position, 0, scale, size, size)\n", "        que_imgs_info = {\n\t            'imgs': color_map_forward(que_img_warp).transpose([2,0,1]),  # 3,h,w\n\t            'Ks_in': que_K_warp.astype(np.float32), # 3,3\n\t            'poses_in': in_pose_warp.astype(np.float32), # 3,4\n\t        }\n\t        # print( que_imgs_info['imgs'].shape ,  que_imgs_info['Ks_in'].shape  , que_imgs_info['poses_in'].shape )\n\t        # select reference views for refinement\n\t        ref_ids = select_reference_img_ids_refinement(ref_database, object_center, self.ref_ids, \\\n\t                                                      in_pose_warp, ref_num, ref_even, ref_even_num)\n\t        # normalize the reference images and align the in-plane orientation w.r.t input pose.\n", "        ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs = normalize_reference_views(\n\t            ref_database, ref_ids, size, margin, True, in_pose_warp, que_K_warp)\n\t        ref_imgs_info = {\n\t            'imgs': color_map_forward(np.stack(ref_imgs, 0)).transpose([0, 3, 1, 2]),  # rfn,3,h,w\n\t            'poses': np.stack(ref_poses, 0).astype(np.float32),\n\t            'Ks': np.stack(ref_Ks, 0).astype(np.float32),\n\t        }\n\t        # print( ref_imgs_info['imgs'].shape ,  ref_imgs_info['poses'].shape  , ref_imgs_info['Ks'].shape )\n\t        que_imgs_info = to_cuda(imgs_info_to_torch(que_imgs_info))\n\t        ref_imgs_info = to_cuda(imgs_info_to_torch(ref_imgs_info))\n", "        for k,v in que_imgs_info.items(): que_imgs_info[k] = v.unsqueeze(0)\n\t        for k,v in ref_imgs_info.items(): ref_imgs_info[k] = v.unsqueeze(0)\n\t        with torch.no_grad():\n\t            outputs = self.forward({'que_imgs_info': que_imgs_info, 'ref_imgs_info': ref_imgs_info, 'inference': True})\n\t            quat = outputs['rotation'].detach().cpu().numpy()[0] # 4\n\t            scale = 2**outputs['scale'].detach().cpu().numpy()[0] # 1\n\t            offset = outputs['offset'].detach().cpu().numpy()[0] # 2\n\t            # print(\"scale:\", scale , \"quat:\", quat, \"offset:\", offset )\n\t        # compose rotation/scale/offset into a similarity transformation matrix\n\t        pose_sim = compose_sim_pose(scale, quat, offset, in_pose_warp, object_center)\n", "        # convert the similarity transformation to the rigid transformation\n\t        pose_pr = pose_sim_to_pose_rigid(pose_sim, in_pose_warp, que_K_warp, que_K_warp, object_center)\n\t        # apply the pose residual\n\t        pose_pr = pose_compose(pose_pr, pose_inverse(que_pose_rect))\n\t        # convert back to original coordinate system (because we use NormalizedDatabase to wrap the input)\n\t        pose_pr = denormalize_pose(pose_pr, ref_database.scale, ref_database.offset)\n\t        return pose_pr\n\tif __name__ == \"__main__\":\n\t    from utils.base_utils import load_cfg\n\t    cfg = \"configs/refiner/refiner_pretrain.yaml\"\n", "    refiner_cfg = load_cfg(cfg)\n\t    refiner = VolumeRefiner(refiner_cfg)\n\t    refiner_sample_num = 32\n\t    ref_imgs_info = {\n\t        'imgs': torch.randn(6,3,128,128) , # rfn,3,h,w\n\t        'poses': torch.randn(6, 3, 4),\n\t        'Ks': torch.randn(6,3,3),\n\t    }\n\t    que_imgs_info = {\n\t        'imgs': torch.randn(3,128,128),  # 3,h,w\n", "        'Ks_in': torch.randn(3, 3), # 3,3\n\t        'poses_in':  torch.randn(3, 4), # 3,4\n\t    }\n\t    for k,v in que_imgs_info.items(): que_imgs_info[k] = v.unsqueeze(0)\n\t    for k,v in ref_imgs_info.items(): ref_imgs_info[k] = v.unsqueeze(0)\n\t    # pose_pr = refiner.refine_que_imgs(que_img, que_K, pose_pr, size=128, ref_num=6, ref_even=True)\n\t    vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords = refiner.construct_feature_volume(\n\t            que_imgs_info, ref_imgs_info, refiner.feature_net, refiner_sample_num)\n\t    mock_data = torch.randn(6,3,128,128)\n\t    net = RefineFeatureNet()\n", "    out =  net(mock_data)\n\t    print(out.shape)"]}
{"filename": "network/__init__.py", "chunked_list": ["from network.detector import Detector\n\tfrom network.refiner import VolumeRefiner\n\tfrom network.selector import ViewpointSelector\n\tfrom network.dino_detector import Detector as DinoDetector\n\tfrom network.cascade_refiner import VolumeRefiner as cascadeVolumeRefiner\n\tname2network={\n\t    'refiner': VolumeRefiner,\n\t    'detector': Detector,\n\t    'selector': ViewpointSelector,\n\t    'dino_detector':DinoDetector,\n", "    'cascade_refiner':cascadeVolumeRefiner\n\t}\n"]}
{"filename": "network/module.py", "chunked_list": ["import sys\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass identity_with(object):\n\t    def __init__(self, enabled=True):\n\t        self._enabled = enabled\n\t    def __enter__(self):\n\t        pass\n\t    def __exit__(self, *args):\n", "        pass\n\tdef torch_init_model(model, total_dict, key, rank=0):\n\t    if key in total_dict:\n\t        state_dict = total_dict[key]\n\t    else:\n\t        state_dict = total_dict\n\t    missing_keys = []\n\t    unexpected_keys = []\n\t    error_msgs = []\n\t    # copy state_dict so _load_from_state_dict can modify it\n", "    metadata = getattr(state_dict, '_metadata', None)\n\t    state_dict = state_dict.copy()\n\t    if metadata is not None:\n\t        state_dict._metadata = metadata\n\t    def load(module, prefix=''):\n\t        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n\t        module._load_from_state_dict(state_dict=state_dict, prefix=prefix, local_metadata=local_metadata, strict=True,\n\t                                     missing_keys=missing_keys, unexpected_keys=unexpected_keys, error_msgs=error_msgs)\n\t        for name, child in module._modules.items():\n\t            if child is not None:\n", "                load(child, prefix + name + '.')\n\t    load(model, prefix='')\n\t    if rank == 0:\n\t        print(\"missing keys:{}\".format(missing_keys))\n\t        print('unexpected keys:{}'.format(unexpected_keys))\n\t        print('error msgs:{}'.format(error_msgs))\n\tautocast = torch.cuda.amp.autocast if torch.__version__ >= '1.6.0' else identity_with\n\tdef init_bn(module):\n\t    if module.weight is not None:\n\t        nn.init.ones_(module.weight)\n", "    if module.bias is not None:\n\t        nn.init.zeros_(module.bias)\n\t    return\n\tdef init_uniform(module, init_method):\n\t    if module.weight is not None:\n\t        if init_method == \"kaiming\":\n\t            nn.init.kaiming_uniform_(module.weight)\n\t        elif init_method == \"xavier\":\n\t            nn.init.xavier_uniform_(module.weight)\n\t    return\n", "class Conv2d(nn.Module):\n\t    \"\"\"Applies a 2D convolution (optionally with batch normalization and relu activation)\n\t    over an input signal composed of several input planes.\n\t    Attributes:\n\t        conv (nn.Module): convolution module\n\t        bn (nn.Module): batch normalization module\n\t        relu (bool): whether to activate by relu\n\t    Notes:\n\t        Default momentum for batch normalization is set to be 0.01,\n\t    \"\"\"\n", "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n\t                 relu=True, bn=True, bn_momentum=0.1, norm_type='IN', **kwargs):\n\t        super(Conv2d, self).__init__()\n\t        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, bias=(not bn), **kwargs)\n\t        self.kernel_size = kernel_size\n\t        self.stride = stride\n\t        if norm_type == 'IN':\n\t            self.bn = nn.InstanceNorm2d(out_channels, momentum=bn_momentum) if bn else None\n\t        elif norm_type == 'BN':\n\t            self.bn = nn.BatchNorm2d(out_channels, momentum=bn_momentum) if bn else None\n", "        self.relu = relu\n\t    def forward(self, x):\n\t        y = self.conv(x)\n\t        if self.bn is not None:\n\t            y = self.bn(y)\n\t        if self.relu:\n\t            y = F.leaky_relu(y, 0.1, inplace=True)\n\t        return y\n\t    def init_weights(self, init_method):\n\t        \"\"\"default initialization\"\"\"\n", "        init_uniform(self.conv, init_method)\n\t        if self.bn is not None:\n\t            init_bn(self.bn)\n\tclass Conv3d(nn.Module):\n\t    \"\"\"Applies a 3D convolution (optionally with batch normalization and relu activation)\n\t    over an input signal composed of several input planes.\n\t    Attributes:\n\t        conv (nn.Module): convolution module\n\t        bn (nn.Module): batch normalization module\n\t        relu (bool): whether to activate by relu\n", "    Notes:\n\t        Default momentum for batch normalization is set to be 0.01,\n\t    \"\"\"\n\t    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n\t                 relu=True, bn=True, bn_momentum=0.1, init_method=\"xavier\", **kwargs):\n\t        super(Conv3d, self).__init__()\n\t        self.out_channels = out_channels\n\t        self.kernel_size = kernel_size\n\t        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride,\n\t                              bias=(not bn), **kwargs)\n", "        self.bn = nn.BatchNorm3d(out_channels, momentum=bn_momentum) if bn else None\n\t        self.relu = relu\n\t        # assert init_method in [\"kaiming\", \"xavier\"]\n\t        # self.init_weights(init_method)\n\t    def forward(self, x):\n\t        x = self.conv(x)\n\t        if self.bn is not None:\n\t            x = self.bn(x)\n\t        if self.relu:\n\t            x = F.relu(x, inplace=True)\n", "        return x\n\t    def init_weights(self, init_method):\n\t        \"\"\"default initialization\"\"\"\n\t        init_uniform(self.conv, init_method)\n\t        if self.bn is not None:\n\t            init_bn(self.bn)\n\tclass Deconv3d(nn.Module):\n\t    \"\"\"Applies a 3D deconvolution (optionally with batch normalization and relu activation)\n\t       over an input signal composed of several input planes.\n\t       Attributes:\n", "           conv (nn.Module): convolution module\n\t           bn (nn.Module): batch normalization module\n\t           relu (bool): whether to activate by relu\n\t       Notes:\n\t           Default momentum for batch normalization is set to be 0.01,\n\t       \"\"\"\n\t    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n\t                 relu=True, bn=True, bn_momentum=0.1, init_method=\"xavier\", **kwargs):\n\t        super(Deconv3d, self).__init__()\n\t        self.out_channels = out_channels\n", "        self.conv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride,\n\t                                       bias=(not bn), **kwargs)\n\t        self.bn = nn.BatchNorm3d(out_channels, momentum=bn_momentum) if bn else None\n\t        self.relu = relu\n\t        # assert init_method in [\"kaiming\", \"xavier\"]\n\t        # self.init_weights(init_method)\n\t    def forward(self, x):\n\t        x = self.conv(x)\n\t        if self.bn is not None:\n\t            x = self.bn(x)\n", "        if self.relu:\n\t            x = F.relu(x, inplace=True)\n\t        return x\n\t    def init_weights(self, init_method):\n\t        \"\"\"default initialization\"\"\"\n\t        init_uniform(self.conv, init_method)\n\t        if self.bn is not None:\n\t            init_bn(self.bn)\n\tclass ConvBnReLU(nn.Module):\n\t    \"\"\"Implements 2d Convolution + batch normalization + ReLU\"\"\"\n", "    def __init__(\n\t            self,\n\t            in_channels: int,\n\t            out_channels: int,\n\t            kernel_size: int = 3,\n\t            stride: int = 1,\n\t            pad: int = 1,\n\t            dilation: int = 1,\n\t    ) -> None:\n\t        \"\"\"initialization method for convolution2D + batch normalization + relu module\n", "        Args:\n\t            in_channels: input channel number of convolution layer\n\t            out_channels: output channel number of convolution layer\n\t            kernel_size: kernel size of convolution layer\n\t            stride: stride of convolution layer\n\t            pad: pad of convolution layer\n\t            dilation: dilation of convolution layer\n\t        \"\"\"\n\t        super(ConvBnReLU, self).__init__()\n\t        self.conv = nn.Conv2d(\n", "            in_channels, out_channels, kernel_size, stride=stride, padding=pad, dilation=dilation, bias=False\n\t        )\n\t        self.bn = nn.BatchNorm2d(out_channels)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        \"\"\"forward method\"\"\"\n\t        return F.relu(self.bn(self.conv(x)), inplace=True)\n\tclass Swish(nn.Module):\n\t    def __init__(self):\n\t        super(Swish, self).__init__()\n\t    def forward(self, x):\n", "        return x * torch.sigmoid(x)\n\tclass FPNDecoderV2(nn.Module):\n\t    def __init__(self, feat_chs):\n\t        super(FPNDecoderV2, self).__init__()\n\t        # feat_chs=[0->8,1->16,2->32,3->64,4->128,5->256,6->512]\n\t        self.out1 = nn.Sequential(nn.Conv2d(768, 512, kernel_size=1), nn.BatchNorm2d(feat_chs[6]), Swish())\n\t        self.out2 = nn.Sequential(nn.Conv2d(640 , 512, kernel_size=1), nn.BatchNorm2d(feat_chs[6]), Swish())\n\t        self.out3 = nn.Sequential(nn.Conv2d(576, 512, kernel_size=1), nn.BatchNorm2d(feat_chs[6]), Swish())\n\t    #                   1/8     1/16     1/32  |  1/32  1/16 1/8\n\t    def forward(self, conv21, conv31, conv41, vit1, vit2, vit3):\n", "        # print(conv21.shape, conv31.shape, conv41.shape )\n\t        # print(vit1.shape, vit2.shape, vit3.shape)\n\t        out1 = conv41\n\t        # drop out \n\t        vit1 = F.interpolate(vit1,size=(out1.shape[-2],out1.shape[-1]))\n\t        out1 = self.out1( torch.cat([out1, vit1], dim=1) ) + conv41\n\t        out2 = conv31\n\t        vit2 = F.interpolate(vit2,size=(out2.shape[-2],out2.shape[-1]))\n\t        out2 = self.out2(torch.cat([out2, vit2], dim=1))  + conv31\n\t        out3 = conv21\n", "        vit3 = F.interpolate(vit3,size=(out3.shape[-2],out3.shape[-1]))\n\t        out3 = self.out3(torch.cat([out3, vit3], dim=1)) + conv21\n\t        return [out3,out2,out1]\n\tclass VITDecoderStage4(nn.Module):\n\t    def __init__(self, args):\n\t        super(VITDecoderStage4, self).__init__()\n\t        ch, vit_ch = args['out_ch'], args['vit_ch']\n\t        self.multi_scale_decoder = args.get('multi_scale_decoder', True)\n\t        assert args['att_fusion'] is True\n\t        self.attn = AttentionFusionSimple(vit_ch, ch * 4, args['nhead'])\n", "        self.decoder1 = nn.Sequential(nn.ConvTranspose2d(ch * 4, ch * 2, 4, stride=2, padding=1),\n\t                                      nn.BatchNorm2d(ch * 2), nn.GELU())\n\t        self.decoder2 = nn.Sequential(nn.ConvTranspose2d(ch * 2, ch, 4, stride=2, padding=1),\n\t                                        nn.BatchNorm2d(ch), nn.GELU(),)\n\t    def forward(self, x, att):\n\t        out1 = self.attn(x, att)\n\t        out2 = self.decoder1(out1)\n\t        out3 = self.decoder2(out2)\n\t        return out1, out2, out3\n\tclass VITDecoderStage4Single(nn.Module):\n", "    def __init__(self, args):\n\t        super(VITDecoderStage4Single, self).__init__()\n\t        ch, vit_ch = args['out_ch'], args['vit_ch']\n\t        assert args['att_fusion'] is True\n\t        self.attn = AttentionFusionSimple(vit_ch, ch * 4, args['nhead'])\n\t        self.decoder = nn.Sequential(nn.ConvTranspose2d(ch * 4, ch * 2, 4, stride=2, padding=1),\n\t                                     nn.BatchNorm2d(ch * 2), nn.GELU(),\n\t                                     nn.ConvTranspose2d(ch * 2, ch, 4, stride=2, padding=1),\n\t                                     nn.BatchNorm2d(ch), nn.GELU())\n\t    def forward(self, x, att):\n", "        x = self.attn(x, att)\n\t        x = self.decoder(x)\n\t        return x\n\tclass TwinDecoderStage4(nn.Module):\n\t    def __init__(self, args):\n\t        super(TwinDecoderStage4, self).__init__()\n\t        ch, vit_chs = args['out_ch'], args['vit_ch']\n\t        ch = ch * 4  # 256\n\t        self.upsampler0 = nn.Sequential(nn.ConvTranspose2d(vit_chs[-1], ch, 4, stride=2, padding=1),\n\t                                        nn.BatchNorm2d(ch), nn.GELU())  # 256\n", "        self.inner1 = nn.Conv2d(vit_chs[-2], ch, kernel_size=1, stride=1, padding=0)\n\t        self.smooth1 = nn.Sequential(nn.Conv2d(ch, ch // 2, kernel_size=3, stride=1, padding=1),\n\t                                     nn.BatchNorm2d(ch // 2), nn.ReLU(True))  # 256->128\n\t        self.inner2 = nn.Conv2d(vit_chs[-3], ch // 2, kernel_size=1, stride=1, padding=0)\n\t        self.smooth2 = nn.Sequential(nn.Conv2d(ch // 2, ch // 4, kernel_size=3, stride=1, padding=1),\n\t                                     nn.BatchNorm2d(ch // 4), nn.ReLU(True))  # 128->64\n\t        self.inner3 = nn.Conv2d(vit_chs[-4], ch // 4, kernel_size=1, stride=1, padding=0)\n\t        self.smooth3 = nn.Sequential(nn.Conv2d(ch // 4, ch // 4, kernel_size=3, stride=1, padding=1),\n\t                                     nn.BatchNorm2d(ch // 4), Swish())  # 64->64\n\t    def forward(self, x1, x2, x3, x4):  # in:[1/8 ~ 1/64] out:[1/2,1/4,1/8]\n", "        x = self.smooth1(self.upsampler0(x4) + self.inner1(x3))  # 1/64->1/32\n\t        x = self.smooth2(F.upsample(x, scale_factor=2, mode='bilinear', align_corners=False) + self.inner2(x2))  # 1/32->1/16\n\t        x = self.smooth3(F.upsample(x, scale_factor=2, mode='bilinear', align_corners=False) + self.inner3(x1))  # 1/16->1/8\n\t        return x\n\tclass TwinDecoderStage4V2(nn.Module):\n\t    def __init__(self, args):\n\t        super(TwinDecoderStage4V2, self).__init__()\n\t        ch, vit_chs = args['out_ch'], args['vit_ch']\n\t        ch = ch * 4  # 256\n\t        self.upsampler0 = nn.Sequential(nn.ConvTranspose2d(vit_chs[-1], ch, 4, stride=2, padding=1),\n", "                                        nn.BatchNorm2d(ch), nn.GELU())  # 256\n\t        self.inner1 = nn.Conv2d(vit_chs[-2], ch, kernel_size=1, stride=1, padding=0)\n\t        self.smooth1 = nn.Sequential(nn.Conv2d(ch, ch // 2, kernel_size=3, stride=1, padding=1),\n\t                                     nn.BatchNorm2d(ch // 2), nn.GELU())  # 256->128\n\t        self.inner2 = nn.Conv2d(vit_chs[-3], ch // 2, kernel_size=1, stride=1, padding=0)\n\t        self.smooth2 = nn.Sequential(nn.Conv2d(ch // 2, ch // 4, kernel_size=3, stride=1, padding=1),\n\t                                     nn.BatchNorm2d(ch // 4), nn.GELU())  # 128->64\n\t        self.inner3 = nn.Conv2d(vit_chs[-4], ch // 4, kernel_size=1, stride=1, padding=0)\n\t        self.smooth3 = nn.Sequential(nn.Conv2d(ch // 4, ch // 4, kernel_size=3, stride=1, padding=1),\n\t                                     nn.BatchNorm2d(ch // 4), nn.GELU())  # 64->64\n", "        self.decoder1 = nn.Sequential(nn.ConvTranspose2d(ch // 4, ch // 8, 4, stride=2, padding=1),\n\t                                      nn.BatchNorm2d(ch // 8), nn.GELU())\n\t        self.decoder2 = nn.Sequential(nn.ConvTranspose2d(ch // 8, ch // 16, 4, stride=2, padding=1),\n\t                                      nn.BatchNorm2d(ch // 16), nn.GELU())\n\t    def forward(self, x1, x2, x3, x4):  # in:[1/8 ~ 1/64] out:[1/2,1/4,1/8]\n\t        x = self.smooth1(self.upsampler0(x4) + self.inner1(x3))  # 1/64->1/32\n\t        x = self.smooth2(F.upsample(x, scale_factor=2, mode='bilinear', align_corners=False) + self.inner2(x2))  # 1/32->1/16\n\t        out1 = self.smooth3(F.upsample(x, scale_factor=2, mode='bilinear', align_corners=False) + self.inner3(x1))  # 1/16->1/8\n\t        out2 = self.decoder1(out1)\n\t        out3 = self.decoder2(out2)\n", "        return out1, out2, out3\n\tclass AttentionFusionSimple(nn.Module):\n\t    def __init__(self, vit_ch, out_ch, nhead):\n\t        super(AttentionFusionSimple, self).__init__()\n\t        self.conv_l = nn.Sequential(nn.Conv2d(vit_ch + nhead, vit_ch, kernel_size=3, padding=1),\n\t                                    nn.BatchNorm2d(vit_ch))\n\t        self.conv_r = nn.Sequential(nn.Conv2d(vit_ch, vit_ch, kernel_size=3, padding=1),\n\t                                    nn.BatchNorm2d(vit_ch))\n\t        self.act = Swish()\n\t        self.proj = nn.Conv2d(vit_ch, out_ch, kernel_size=1)\n", "    def forward(self, x, att):\n\t        # x:[B,C,H,W]; att:[B,nh,H,W]\n\t        x1 = self.act(self.conv_l(torch.cat([x, att], dim=1)))\n\t        att = torch.mean(att, dim=1, keepdim=True)\n\t        x2 = self.act(self.conv_r(x * att))\n\t        x = self.proj(x1 * x2)\n\t        return x\n\tclass CostRegNet(nn.Module):\n\t    def __init__(self, in_channels, base_channels, last_layer=True):\n\t        super(CostRegNet, self).__init__()\n", "        self.last_layer = last_layer\n\t        self.conv1 = Conv3d(in_channels, base_channels * 2, stride=2, padding=1)\n\t        self.conv2 = Conv3d(base_channels * 2, base_channels * 2, padding=1)\n\t        self.conv3 = Conv3d(base_channels * 2, base_channels * 4, stride=2, padding=1)\n\t        self.conv4 = Conv3d(base_channels * 4, base_channels * 4, padding=1)\n\t        self.conv5 = Conv3d(base_channels * 4, base_channels * 8, stride=2, padding=1)\n\t        self.conv6 = Conv3d(base_channels * 8, base_channels * 8, padding=1)\n\t        self.conv7 = Deconv3d(base_channels * 8, base_channels * 4, stride=2, padding=1, output_padding=1)\n\t        self.conv9 = Deconv3d(base_channels * 4, base_channels * 2, stride=2, padding=1, output_padding=1)\n\t        self.conv11 = Deconv3d(base_channels * 2, base_channels * 1, stride=2, padding=1, output_padding=1)\n", "        if in_channels != base_channels:\n\t            self.inner = nn.Conv3d(in_channels, base_channels, 1, 1)\n\t        else:\n\t            self.inner = nn.Identity()\n\t        if self.last_layer:\n\t            self.prob = nn.Conv3d(base_channels, 1, 3, stride=1, padding=1, bias=False)\n\t    def forward(self, x):\n\t        conv0 = x\n\t        conv2 = self.conv2(self.conv1(conv0))\n\t        conv4 = self.conv4(self.conv3(conv2))\n", "        x = self.conv6(self.conv5(conv4))\n\t        x = conv4 + self.conv7(x)\n\t        x = conv2 + self.conv9(x)\n\t        x = self.inner(conv0) + self.conv11(x)\n\t        if self.last_layer:\n\t            x = self.prob(x)\n\t        return x\n\tclass CostRegNet2D(nn.Module):\n\t    def __init__(self, in_channels, base_channel=8):\n\t        super(CostRegNet2D, self).__init__()\n", "        self.conv1 = Conv3d(in_channels, base_channel * 2, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n\t        self.conv2 = Conv3d(base_channel * 2, base_channel * 2, padding=1)\n\t        self.conv3 = Conv3d(base_channel * 2, base_channel * 4, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n\t        self.conv4 = Conv3d(base_channel * 4, base_channel * 4, padding=1)\n\t        self.conv5 = Conv3d(base_channel * 4, base_channel * 8, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))\n\t        self.conv6 = Conv3d(base_channel * 8, base_channel * 8, padding=1)\n\t        self.conv7 = nn.Sequential(\n\t            nn.ConvTranspose3d(base_channel * 8, base_channel * 4, kernel_size=(1, 3, 3), padding=(0, 1, 1), output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False),\n\t            nn.BatchNorm3d(base_channel * 4),\n\t            nn.ReLU(inplace=True))\n", "        self.conv9 = nn.Sequential(\n\t            nn.ConvTranspose3d(base_channel * 4, base_channel * 2, kernel_size=(1, 3, 3), padding=(0, 1, 1), output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False),\n\t            nn.BatchNorm3d(base_channel * 2),\n\t            nn.ReLU(inplace=True))\n\t        self.conv11 = nn.Sequential(\n\t            nn.ConvTranspose3d(base_channel * 2, base_channel, kernel_size=(1, 3, 3), padding=(0, 1, 1), output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False),\n\t            nn.BatchNorm3d(base_channel),\n\t            nn.ReLU(inplace=True))\n\t        self.prob = nn.Conv3d(base_channel, 1, 1, stride=1, padding=0)\n\t    def forward(self, x):\n", "        conv0 = x\n\t        conv2 = self.conv2(self.conv1(conv0))\n\t        conv4 = self.conv4(self.conv3(conv2))\n\t        x = self.conv6(self.conv5(conv4))\n\t        x = conv4 + self.conv7(x)\n\t        x = conv2 + self.conv9(x)\n\t        x = conv0 + self.conv11(x)\n\t        x = self.prob(x)\n\t        return x\n\tclass CostRegNet3D(nn.Module):\n", "    def __init__(self, in_channels, base_channel=8):\n\t        super(CostRegNet3D, self).__init__()\n\t        self.conv1 = Conv3d(in_channels, base_channel * 2, kernel_size=3, stride=(1, 2, 2), padding=1)\n\t        self.conv2 = Conv3d(base_channel * 2, base_channel * 2, padding=1)\n\t        self.conv3 = Conv3d(base_channel * 2, base_channel * 4, kernel_size=3, stride=(1, 2, 2), padding=1)\n\t        self.conv4 = Conv3d(base_channel * 4, base_channel * 4, padding=1)\n\t        self.conv5 = Conv3d(base_channel * 4, base_channel * 8, kernel_size=3, stride=(1, 2, 2), padding=1)\n\t        self.conv6 = Conv3d(base_channel * 8, base_channel * 8, padding=1)\n\t        self.conv7 = nn.Sequential(\n\t            nn.ConvTranspose3d(base_channel * 8, base_channel * 4, kernel_size=3, padding=1, output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False),\n", "            nn.BatchNorm3d(base_channel * 4),\n\t            nn.ReLU(inplace=True))\n\t        self.conv9 = nn.Sequential(\n\t            nn.ConvTranspose3d(base_channel * 4, base_channel * 2, kernel_size=3, padding=1, output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False),\n\t            nn.BatchNorm3d(base_channel * 2),\n\t            nn.ReLU(inplace=True))\n\t        self.conv11 = nn.Sequential(\n\t            nn.ConvTranspose3d(base_channel * 2, base_channel, kernel_size=3, padding=1, output_padding=(0, 1, 1), stride=(1, 2, 2), bias=False),\n\t            nn.BatchNorm3d(base_channel),\n\t            nn.ReLU(inplace=True))\n", "        if in_channels != base_channel:\n\t            self.inner = nn.Conv3d(in_channels, base_channel, 1, 1)\n\t        else:\n\t            self.inner = nn.Identity()\n\t        self.prob = nn.Conv3d(base_channel, 1, 1, stride=1, padding=0)\n\t    def forward(self, x):\n\t        conv0 = x\n\t        conv2 = self.conv2(self.conv1(conv0))\n\t        conv4 = self.conv4(self.conv3(conv2))\n\t        x = self.conv6(self.conv5(conv4))\n", "        x = conv4 + self.conv7(x)\n\t        x = conv2 + self.conv9(x)\n\t        x = self.inner(conv0) + self.conv11(x)\n\t        x = self.prob(x)\n\t        return x\n\tdef depth_regression(p, depth_values):\n\t    if depth_values.dim() <= 2:\n\t        # print(\"regression dim <= 2\")\n\t        depth_values = depth_values.view(*depth_values.shape, 1, 1)\n\t    depth = torch.sum(p * depth_values, 1)\n", "    return depth\n\tdef conf_regression(p, n=4):\n\t    ndepths = p.size(1)\n\t    with torch.no_grad():\n\t        # photometric confidence\n\t        if n % 2 == 1:\n\t            prob_volume_sum4 = n * F.avg_pool3d(F.pad(p.unsqueeze(1), pad=[0, 0, 0, 0, n // 2, n // 2]),\n\t                                                (n, 1, 1), stride=1, padding=0).squeeze(1)\n\t        else:\n\t            prob_volume_sum4 = n * F.avg_pool3d(F.pad(p.unsqueeze(1), pad=[0, 0, 0, 0, n // 2 - 1, n // 2]),\n", "                                                (n, 1, 1), stride=1, padding=0).squeeze(1)\n\t        depth_index = depth_regression(p.detach(), depth_values=torch.arange(ndepths, device=p.device, dtype=torch.float)).long()\n\t        depth_index = depth_index.clamp(min=0, max=ndepths - 1)\n\t        conf = torch.gather(prob_volume_sum4, 1, depth_index.unsqueeze(1))\n\t    return conf.squeeze(1)\n\tdef init_range(cur_depth, ndepths, device, dtype, H, W):\n\t    cur_depth_min = cur_depth[:, 0]  # (B,)\n\t    cur_depth_max = cur_depth[:, -1]\n\t    new_interval = (cur_depth_max - cur_depth_min) / (ndepths - 1)  # (B, )\n\t    new_interval = new_interval[:, None, None]  # B H W\n", "    depth_range_samples = cur_depth_min.unsqueeze(1) + (torch.arange(0, ndepths, device=device, dtype=dtype,\n\t                                                                     requires_grad=False).reshape(1, -1) * new_interval.squeeze(1))  # (B, D)\n\t    depth_range_samples = depth_range_samples.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, H, W)  # (B, D, H, W)\n\t    return depth_range_samples\n\tdef init_inverse_range(cur_depth, ndepths, device, dtype, H, W):\n\t    inverse_depth_min = 1. / cur_depth[:, 0]  # (B,)\n\t    inverse_depth_max = 1. / cur_depth[:, -1]\n\t    itv = torch.arange(0, ndepths, device=device, dtype=dtype, requires_grad=False).reshape(1, -1, 1, 1).repeat(1, 1, H, W) / (ndepths - 1)  # 1 D H W\n\t    inverse_depth_hypo = inverse_depth_max[:, None, None, None] + (inverse_depth_min - inverse_depth_max)[:, None, None, None] * itv\n\t    return 1. / inverse_depth_hypo\n", "def schedule_inverse_range(depth, depth_hypo, ndepths, split_itv, H, W):\n\t    last_depth_itv = 1. / depth_hypo[:, 2, :, :] - 1. / depth_hypo[:, 1, :, :]\n\t    inverse_min_depth = 1 / depth + split_itv * last_depth_itv  # B H W\n\t    inverse_max_depth = 1 / depth - split_itv * last_depth_itv  # B H W\n\t    # cur_depth_min, (B, H, W)\n\t    # cur_depth_max: (B, H, W)\n\t    itv = torch.arange(0, ndepths, device=inverse_min_depth.device, dtype=inverse_min_depth.dtype,\n\t                       requires_grad=False).reshape(1, -1, 1, 1).repeat(1, 1, H // 2, W // 2) / (ndepths - 1)  # 1 D H W\n\t    inverse_depth_hypo = inverse_max_depth[:, None, :, :] + (inverse_min_depth - inverse_max_depth)[:, None, :, :] * itv  # B D H W\n\t    inverse_depth_hypo = F.interpolate(inverse_depth_hypo.unsqueeze(1), [ndepths, H, W], mode='trilinear', align_corners=True).squeeze(1)\n", "    return 1. / inverse_depth_hypo\n\tdef init_inverse_range_eth3d(cur_depth, ndepths, device, dtype, H, W):\n\t    cur_depth = torch.clamp(cur_depth, min=0.01, max=50)\n\t    inverse_depth_min = 1. / cur_depth[:, 0]  # (B,)\n\t    inverse_depth_max = 1. / cur_depth[:, -1]\n\t    itv = torch.arange(0, ndepths, device=device, dtype=dtype, requires_grad=False).reshape(1, -1, 1, 1).repeat(1, 1, H, W) / (ndepths - 1)  # 1 D H W\n\t    inverse_depth_hypo = inverse_depth_max[:, None, None, None] + (inverse_depth_min - inverse_depth_max)[:, None, None, None] * itv\n\t    return 1. / inverse_depth_hypo\n\tdef schedule_inverse_range_eth3d(depth, depth_hypo, ndepths, split_itv, H, W):\n\t    last_depth_itv = 1. / depth_hypo[:, 2, :, :] - 1. / depth_hypo[:, 1, :, :]\n", "    inverse_min_depth = 1 / depth + split_itv * last_depth_itv  # B H W\n\t    inverse_max_depth = 1 / depth - split_itv * last_depth_itv  # B H W 只有他可能是负数！\n\t    is_neg = (inverse_max_depth < 0.02).float()\n\t    inverse_max_depth = inverse_max_depth - (inverse_max_depth - 0.02) * is_neg\n\t    inverse_min_depth = inverse_min_depth - (inverse_max_depth - 0.02) * is_neg\n\t    # cur_depth_min, (B, H, W)\n\t    # cur_depth_max: (B, H, W)\n\t    itv = torch.arange(0, ndepths, device=inverse_min_depth.device, dtype=inverse_min_depth.dtype,\n\t                       requires_grad=False).reshape(1, -1, 1, 1).repeat(1, 1, H // 2, W // 2) / (ndepths - 1)  # 1 D H W\n\t    inverse_depth_hypo = inverse_max_depth[:, None, :, :] + (inverse_min_depth - inverse_max_depth)[:, None, :, :] * itv  # B D H W\n", "    inverse_depth_hypo = F.interpolate(inverse_depth_hypo.unsqueeze(1), [ndepths, H, W], mode='trilinear', align_corners=True).squeeze(1)\n\t    return 1. / inverse_depth_hypo\n\tdef schedule_range(cur_depth, ndepth, depth_inteval_pixel, H, W):\n\t    # shape, (B, H, W)\n\t    # cur_depth: (B, H, W)\n\t    # return depth_range_values: (B, D, H, W)\n\t    cur_depth_min = (cur_depth - ndepth / 2 * depth_inteval_pixel[:, None, None])  # (B, H, W)\n\t    cur_depth_min = torch.clamp_min(cur_depth_min, 0.01)\n\t    cur_depth_max = (cur_depth + ndepth / 2 * depth_inteval_pixel[:, None, None])\n\t    new_interval = (cur_depth_max - cur_depth_min) / (ndepth - 1)  # (B, H, W)\n", "    depth_range_samples = cur_depth_min.unsqueeze(1) + (torch.arange(0, ndepth, device=cur_depth.device, dtype=cur_depth.dtype,\n\t                                                                     requires_grad=False).reshape(1, -1, 1, 1) * new_interval.unsqueeze(1))\n\t    depth_range_samples = F.interpolate(depth_range_samples.unsqueeze(1), [ndepth, H, W], mode='trilinear', align_corners=True).squeeze(1)\n\t    return depth_range_samples\n"]}
{"filename": "network/pretrain_models.py", "chunked_list": ["import torch.nn as nn\n\tfrom typing import Union, List, cast, Type\n\tfrom collections import OrderedDict\n\tfrom torch import Tensor\n\tfrom torchvision import models\n\tfrom torchvision.models.resnet import Bottleneck, BasicBlock, conv1x1\n\tclass VGGBNPretrain(nn.Module):\n\t    def __init__(self, output_index = None):\n\t        super().__init__()\n\t        self.features = _make_vgg_layers(vgg_cfgs['A'], True)\n", "        self.splits=vgg_split['A']\n\t        self._initialize_weights()\n\t        self.output_index = output_index\n\t    def forward(self, x):\n\t        x = self.features[self.splits[0][0]:self.splits[0][1]](x) # 1\n\t        x = self.features[self.splits[1][0]:self.splits[1][1]](x) # 1/2\n\t        x = self.features[self.splits[2][0]:self.splits[2][1]](x) # 1/4\n\t        x0 = self.features[self.splits[3][0]:self.splits[3][1]](x) # 1/8\n\t        x1 = self.features[self.splits[4][0]:self.splits[4][1]](x0) # 1/16\n\t        x2 = self.features[-1](x1) # 1/32\n", "        if self.output_index is None:\n\t            return x0, x1, x2\n\t        elif isinstance(self.output_index, int):\n\t            return [x0,x1,x2][self.output_index]\n\t        elif isinstance(self.output_index, list):\n\t            return [[x0,x1,x2][index] for index in self.output_index]\n\t        else:\n\t            raise NotImplementedError\n\t    def _initialize_weights(self):\n\t        pretrain_model = models.vgg11_bn(True)\n", "        state_dict = pretrain_model.state_dict()\n\t        new_state_dict = OrderedDict()\n\t        for k,v in state_dict.items():\n\t            if k.startswith('features'):\n\t                new_state_dict[k] = v\n\t        self.load_state_dict(new_state_dict)\n\tclass VGGBNPretrainV2(VGGBNPretrain):\n\t    def __init__(self, output_index=None):\n\t        super().__init__(output_index)\n\t        self.output_index=output_index\n", "    def forward(self, x):\n\t        x = self.features[self.splits[0][0]:self.splits[0][1]](x) # 1\n\t        if self.output_index == 0: return x\n\t        x = self.features[self.splits[1][0]:self.splits[1][1]](x) # 1/2\n\t        if self.output_index == 1: return x\n\t        x = self.features[self.splits[2][0]:self.splits[2][1]](x) # 1/4\n\t        if self.output_index == 2: return x\n\t        x = self.features[self.splits[3][0]:self.splits[3][1]](x) # 1/8\n\t        if self.output_index == 3: return x\n\t        x = self.features[self.splits[4][0]:self.splits[4][1]](x) # 1/16\n", "        if self.output_index == 4: return x\n\t        x = self.features[-1](x)\n\t        if self.output_index == 5: return x\n\tclass VGGBNPretrainV3(VGGBNPretrain):\n\t    def __init__(self, output_index=None):\n\t        super().__init__(output_index)\n\t        self.output_index=output_index\n\t    def forward(self, x):\n\t        x0 = self.features[self.splits[0][0]:self.splits[0][1]](x) # 1\n\t        x1 = self.features[self.splits[1][0]:self.splits[1][1]](x0) # 1/2\n", "        x2 = self.features[self.splits[2][0]:self.splits[2][1]](x1) # 1/4\n\t        x3 = self.features[self.splits[3][0]:self.splits[3][1]](x2) # 1/8\n\t        x4 = self.features[self.splits[4][0]:self.splits[4][1]](x3) # 1/16\n\t        return x2, x3, x4\n\tclass VGGBNPretrainV4(VGGBNPretrain):\n\t    def __init__(self, output_index=None):\n\t        super().__init__(output_index)\n\t        self.output_index=output_index\n\t    def forward(self, x):\n\t        x0 = self.features[self.splits[0][0]:self.splits[0][1]](x) # 1\n", "        x1 = self.features[self.splits[1][0]:self.splits[1][1]](x0) # 1/2\n\t        x2 = self.features[self.splits[2][0]:self.splits[2][1]](x1) # 1/4\n\t        x3 = self.features[self.splits[3][0]:self.splits[3][1]](x2) # 1/8\n\t        return x0, x1, x2, x3\n\tdef _make_vgg_layers(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n\t    layers: List[nn.Module] = []\n\t    in_channels = 3\n\t    for v in cfg:\n\t        if v == 'M':\n\t            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n", "        else:\n\t            v = cast(int, v)\n\t            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n\t            if batch_norm:\n\t                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n\t            else:\n\t                layers += [conv2d, nn.ReLU(inplace=True)]\n\t            in_channels = v\n\t    return nn.Sequential(*layers)\n\tvgg_cfgs = {\n", "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n\t    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n\t    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n\t    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n\t}\n\tvgg_split={\n\t    'A': [(0,3), (3,7), (7,14), (14,21), (21,27)]\n\t}\n\tclass ResNet18Pretrain(nn.Module):\n\t    def __init__(self):\n", "        super().__init__()\n\t        replace_stride_with_dilation=None\n\t        block=BasicBlock\n\t        layers=[2,2,2,2]\n\t        norm_layer = nn.BatchNorm2d\n\t        self._norm_layer = norm_layer\n\t        self.inplanes = 64\n\t        self.dilation = 1\n\t        if replace_stride_with_dilation is None:\n\t            # each element in the tuple indicates if we should replace\n", "            # the 2x2 stride with a dilated convolution instead\n\t            replace_stride_with_dilation = [False, False, False]\n\t        if len(replace_stride_with_dilation) != 3:\n\t            raise ValueError(\"replace_stride_with_dilation should be None \"\n\t                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n\t        self.groups = 1\n\t        self.base_width = 64\n\t        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n\t                               bias=False)\n\t        self.bn1 = norm_layer(self.inplanes)\n", "        self.relu = nn.ReLU(inplace=True)\n\t        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\t        self.layer1 = self._make_layer(block, 64, layers[0])\n\t        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n\t                                       dilate=replace_stride_with_dilation[0])\n\t        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n\t                                       dilate=replace_stride_with_dilation[1])\n\t        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n\t                                       dilate=replace_stride_with_dilation[2])\n\t    def _init_pretrain(self):\n", "        pretrain_model = models.resnet18(pretrained=True)\n\t        state_dict = pretrain_model.state_dict()\n\t        new_state_dict = OrderedDict()\n\t        for k,v in state_dict.items():\n\t            if k.startswith('conv1') or k.startswith('bn1') or k.startswith('layer'):\n\t                new_state_dict[k]=v\n\t        self.load_state_dict(new_state_dict)\n\t    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n\t                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n\t        norm_layer = self._norm_layer\n", "        downsample = None\n\t        previous_dilation = self.dilation\n\t        if dilate:\n\t            self.dilation *= stride\n\t            stride = 1\n\t        if stride != 1 or self.inplanes != planes * block.expansion:\n\t            downsample = nn.Sequential(\n\t                conv1x1(self.inplanes, planes * block.expansion, stride),\n\t                norm_layer(planes * block.expansion),\n\t            )\n", "        layers = []\n\t        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n\t                            self.base_width, previous_dilation, norm_layer))\n\t        self.inplanes = planes * block.expansion\n\t        for _ in range(1, blocks):\n\t            layers.append(block(self.inplanes, planes, groups=self.groups,\n\t                                base_width=self.base_width, dilation=self.dilation,\n\t                                norm_layer=norm_layer))\n\t        return nn.Sequential(*layers)\n\t    def _forward_impl(self, x: Tensor) -> Tensor:\n", "        # See note [TorchScript super()]\n\t        x = self.conv1(x)\n\t        x = self.bn1(x)\n\t        x = self.relu(x)\n\t        x = self.maxpool(x)\n\t        x = self.layer1(x)\n\t        x = self.layer2(x)\n\t        x = self.layer3(x)\n\t        x = self.layer4(x)\n\t        return x\n", "    def forward(self, x: Tensor) -> Tensor:\n\t        return self._forward_impl(x)"]}
{"filename": "network/operator.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef normalize_coords(coords: torch.Tensor, h, w):\n\t    \"\"\"\n\t    normalzie coords to [-1,1]\n\t    @param coords:\n\t    @param h:\n\t    @param w:\n\t    @return:\n\t    \"\"\"\n", "    coords = torch.clone(coords)\n\t    coords = coords + 0.5\n\t    coords[...,0] = coords[...,0]/w\n\t    coords[...,1] = coords[...,1]/h\n\t    coords = (coords - 0.5)*2\n\t    return coords\n\tdef pose_apply_th(poses,pts):\n\t    return pts @ poses[:,:,:3].permute(0,2,1) + poses[:,:,3:].permute(0,2,1)\n\tdef generate_coords(h,w,device):\n\t    coords=torch.stack(torch.meshgrid(torch.arange(h,device=device),torch.arange(w,device=device)),-1)\n", "    return coords[...,(1,0)]"]}
{"filename": "network/refiner_ablation.py", "chunked_list": ["import time\n\timport torch\n\timport torch.nn as nn\n\timport torchvision.models._utils as _utils\n\timport torchvision.models as models\n\timport torch.nn.functional as F\n\tfrom torch.autograd import Variable\n\tdef conv_bn(inp, oup, stride = 1, leaky = 0):\n\t    return nn.Sequential(\n\t        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n", "        nn.BatchNorm2d(oup),\n\t        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n\t    )\n\tdef conv_bn1X1(inp, oup, stride, leaky=0):\n\t    return nn.Sequential(\n\t        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),\n\t        nn.BatchNorm2d(oup),\n\t        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n\t    )\n\tclass FPN(nn.Module):\n", "    def __init__(self,in_channels_list,out_channels):\n\t        super(FPN,self).__init__()\n\t        leaky = 0\n\t        if (out_channels <= 64):\n\t            leaky = 0.1\n\t        self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride = 1, leaky = leaky)\n\t        self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride = 1, leaky = leaky)\n\t        self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride = 1, leaky = leaky)\n\t        self.merge1 = conv_bn(out_channels, out_channels, leaky = leaky)\n\t        self.merge2 = conv_bn(out_channels, out_channels, leaky = leaky)\n", "    def forward(self, input):\n\t        # names = list(input.keys())\n\t        input = list(input.values())\n\t        output1 = self.output1(input[0])\n\t        output2 = self.output2(input[1])\n\t        output3 = self.output3(input[2])\n\t        up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\"nearest\")\n\t        output2 = output2 + up3\n\t        output2 = self.merge2(output2)\n\t        up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\"nearest\")\n", "        output1 = output1 + up2\n\t        output1 = self.merge1(output1)\n\t        out = [output1, output2, output3]\n\t        return out\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchvision\n\timport numpy as np\n\tfrom network.refiner_ablation import FPN\n", "from dataset.database import NormalizedDatabase, normalize_pose, get_object_center, get_diameter, denormalize_pose\n\tfrom network.operator import pose_apply_th, normalize_coords\n\tfrom network.pretrain_models import VGGBNPretrainV3\n\tfrom utils.base_utils import pose_inverse, project_points, color_map_forward, to_cuda, pose_compose\n\tfrom utils.database_utils import look_at_crop, select_reference_img_ids_refinement, normalize_reference_views\n\tfrom utils.pose_utils import let_me_look_at, compose_sim_pose, pose_sim_to_pose_rigid\n\tfrom utils.imgs_info import imgs_info_to_torch\n\tfrom network.vis_dino_encoder import VitExtractor\n\t# sin-cose embedding module\n\tclass CasEmbedder(nn.Module):\n", "    def __init__(self, **kwargs):\n\t        super(CasEmbedder, self).__init__()\n\t        self.kwargs = kwargs\n\t        self.create_embedding_fn()\n\t    def create_embedding_fn(self):\n\t        embed_fns = []\n\t        d = self.kwargs[\"input_dims\"]\n\t        out_dim = 0\n\t        if self.kwargs[\"include_input\"]:\n\t            embed_fns.append(lambda x: x)\n", "            out_dim += d\n\t        max_freq = self.kwargs[\"max_freq_log2\"]\n\t        N_freqs = self.kwargs[\"num_freqs\"]\n\t        if self.kwargs[\"log_sampling\"]:\n\t            freq_bands = 2.0 ** torch.linspace(0.0, max_freq, steps=N_freqs)\n\t        else:\n\t            freq_bands = torch.linspace(2.0**0.0, 2.0**max_freq, steps=N_freqs)\n\t        for freq in freq_bands:\n\t            for p_fn in self.kwargs[\"periodic_fns\"]:\n\t                embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))\n", "                out_dim += d\n\t        self.embed_fns = embed_fns\n\t        self.out_dim = out_dim\n\t    def forward(self, inputs):\n\t        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n\tclass CasFeedForward(nn.Module):\n\t    def __init__(self, dim, hid_dim, dp_rate):\n\t        super(CasFeedForward, self).__init__()\n\t        self.fc1 = nn.Linear(dim, hid_dim)\n\t        self.fc2 = nn.Linear(hid_dim, dim)\n", "        self.dp = nn.Dropout(dp_rate)\n\t        self.activ = nn.ReLU()\n\t    def forward(self, x):\n\t        x = self.dp(self.activ(self.fc1(x)))\n\t        x = self.dp(self.fc2(x))\n\t        return x\n\t# Subtraction-based efficient attention\n\tclass CasAttention2D(nn.Module):\n\t    def __init__(self, dim, dp_rate):\n\t        super(CasAttention2D, self).__init__()\n", "        self.q_fc = nn.Linear(dim, dim, bias=False)\n\t        self.k_fc = nn.Linear(dim, dim, bias=False)\n\t        self.v_fc = nn.Linear(dim, dim, bias=False)\n\t        self.pos_fc = nn.Sequential(\n\t            nn.Linear(4, dim // 8),\n\t            nn.ReLU(),\n\t            nn.Linear(dim // 8, dim),\n\t        )\n\t        self.attn_fc = nn.Sequential(\n\t            nn.Linear(dim, dim // 8),\n", "            nn.ReLU(),\n\t            nn.Linear(dim // 8, dim),\n\t        )\n\t        self.out_fc = nn.Linear(dim, dim)\n\t        self.dp = nn.Dropout(dp_rate)\n\t    def forward(self, q, k, pos, mask=None):\n\t        q = self.q_fc(q)\n\t        k = self.k_fc(k)\n\t        v = self.v_fc(k)\n\t        pos = self.pos_fc(pos)\n", "        attn = k - q[:, :, None, :] + pos\n\t        attn = self.attn_fc(attn)\n\t        if mask is not None:\n\t            attn = attn.masked_fill(mask == 0, -1e9)\n\t        attn = torch.softmax(attn, dim=-2)\n\t        attn = self.dp(attn)\n\t        x = ((v + pos) * attn).sum(dim=2)\n\t        x = self.dp(self.out_fc(x))\n\t        return x\n\t# View Transformer\n", "class CasTransformer2D(nn.Module):\n\t    def __init__(self, dim, ff_hid_dim, ff_dp_rate, attn_dp_rate):\n\t        super(CasTransformer2D, self).__init__()\n\t        self.attn_norm = nn.LayerNorm(dim, eps=1e-6)\n\t        self.ff_norm = nn.LayerNorm(dim, eps=1e-6)\n\t        self.ff = CasFeedForward(dim, ff_hid_dim, ff_dp_rate)\n\t        self.attn = CasAttention2D(dim, attn_dp_rate)\n\t    def forward(self, q, k, pos, mask=None):\n\t        residue = q\n\t        x = self.attn_norm(q)\n", "        x = self.attn(x, k, pos, mask)\n\t        x = x + residue\n\t        residue = x\n\t        x = self.ff_norm(x)\n\t        x = self.ff(x)\n\t        x = x + residue\n\t        return x\n\tclass CasRefineFeatureNet(nn.Module):\n\t    def __init__(self, \\\n\t                 norm_layer='instance',\\\n", "                 use_dino=False,\\\n\t                 upsample=False):\n\t        super().__init__()\n\t        if norm_layer == 'instance':\n\t            norm=nn.InstanceNorm2d\n\t        else:\n\t            raise NotImplementedError\n\t        self.conv0 = nn.Sequential(\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n", "            nn.ReLU(True),\n\t            nn.Conv2d(64, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n\t        self.conv1 = nn.Sequential(\n\t            nn.Conv2d(512, 256, 3, 1, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n", "        )\n\t        self.conv2 = nn.Sequential(\n\t            nn.Conv2d(512, 256, 3, 1, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n\t        self.conv_out = nn.Sequential(\n\t            nn.Conv2d(64*3, 128, 3, 1, 1),\n", "            norm(128),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(128, 128, 3, 1, 1),\n\t            norm(128),\n\t        )\n\t        self.upsample = upsample\n\t        self.use_dino = use_dino\n\t        if self.upsample:\n\t            self.down_sample = nn.Conv2d(in_channels=128, \\\n\t                                        out_channels=64, \\\n", "                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True)  \n\t        if self.use_dino:\n\t            self.fuse_conv = nn.Conv2d(in_channels=512, \\\n\t                                       out_channels=128, \\\n\t                                       kernel_size=1,\\\n\t                                       stride=1,\\\n\t                                       padding=0, \\\n", "                                       bias=True)\n\t            self.fuse_conv1 = nn.Conv2d(in_channels=256+384, \\\n\t                            out_channels=256, \\\n\t                            kernel_size=1,\\\n\t                            stride=1,\\\n\t                            padding=0, \\\n\t                            bias=True)  \n\t            self.fuse_conv2 = nn.Conv2d(in_channels=512+384, \\\n\t                                        out_channels=512, \\\n\t                                        kernel_size=1,\\\n", "                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True) \n\t            self.fuse_conv3 = nn.Conv2d(in_channels=512+384, \\\n\t                            out_channels=512, \\\n\t                            kernel_size=1,\\\n\t                            stride=1,\\\n\t                            padding=0, \\\n\t                            bias=True) \n\t        self.fpn = FPN([512,512,128],128)  \n", "        self.use_fpn = False     \n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n\t                nn.init.kaiming_normal(m.weight.data, mode='fan_in')\n\t                if m.bias is not None:\n\t                    m.bias.data.zero_()\n\t        if self.use_dino:\n\t            self.fea_ext =  VitExtractor(model_name='dino_vits8').eval()\n\t            for para in self.fea_ext.parameters():\n\t                para.requires_grad = False\n", "            self.fea_ext.requires_grad_(False) \n\t        self.backbone = VGGBNPretrainV3().eval()\n\t        for para in self.backbone.parameters():\n\t            para.requires_grad = False\n\t        self.img_norm = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t    def forward(self, imgs):\n\t        _,_, h,w = imgs.shape\n\t        if self.upsample:\n\t            imgs = F.interpolate(imgs, size=(int(1.5*h), int(1.5*h)))\n\t        if self.use_dino:\n", "            dino_imgs = imgs.clone()\n\t        imgs = self.img_norm(imgs)\n\t        self.backbone.eval()\n\t        with torch.no_grad():\n\t            x0, x1, x2 = self.backbone(imgs)\n\t            x0 = F.normalize(x0, dim=1)\n\t            x1 = F.normalize(x1, dim=1)\n\t            x2 = F.normalize(x2, dim=1)\n\t        x0 = self.conv0(x0)\n\t        x1 = F.interpolate(self.conv1(x1),scale_factor=2,mode='bilinear')\n", "        x2 = F.interpolate(self.conv2(x2),scale_factor=4,mode='bilinear')\n\t        x = torch.cat([x0,x1,x2],1)\n\t        if self.use_fpn:\n\t            x = self.fpn([x0,x1,x])\n\t        else:\n\t            x = self.conv_out(x)  \n\t        if self.use_dino:\n\t            # -------------------------------------------------------- \n\t            dino_imgs = F.interpolate(dino_imgs, size=(256, 256)) \n\t            dino_ret =  self.fea_ext.get_vit_attn_feat(dino_imgs)\n", "            attn, cls_, feat = dino_ret['attn'], dino_ret['cls_'], dino_ret['feat']\n\t            dino_fea = feat.permute(0,2,1).reshape(-1,384,32,32)    \n\t            fused_fea = torch.cat( (x,dino_fea), dim = 1)\n\t            x = self.fuse_conv(fused_fea)\n\t            # --------------------------------------------------------\n\t        return x\n\tclass CasRefineVolumeEncodingNet(nn.Module):\n\t    def __init__(self,norm_layer='no_norm'):\n\t        super().__init__()\n\t        if norm_layer == 'instance':\n", "            norm=nn.InstanceNorm3d\n\t        else:\n\t            raise NotImplementedError\n\t        self.mean_embed = nn.Sequential(\n\t            nn.Conv3d(128 * 2, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(64, 64, 3, 1, 1)\n\t        )\n\t        self.var_embed = nn.Sequential(\n", "            nn.Conv3d(128, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(64, 64, 3, 1, 1)\n\t        )\n\t        self.conv0 = nn.Sequential(\n\t            nn.Conv3d(64*2, 64, 3, 1, 1), # 32\n\t            norm(64),\n\t            nn.ReLU(True),\n\t        ) # 32\n", "        self.conv1 = nn.Sequential(\n\t            nn.Conv3d(64, 128, 3, 2, 1),\n\t            norm(128),\n\t            nn.ReLU(True),\n\t        )\n\t        self.conv2 = nn.Sequential(\n\t            nn.Conv3d(128, 128, 3, 1, 1),\n\t            norm(128),\n\t            nn.ReLU(True),\n\t        ) # 16\n", "        self.conv3 = nn.Sequential(\n\t            nn.Conv3d(128, 256, 3, 2, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n\t        )\n\t        self.conv4 = nn.Sequential(\n\t            nn.Conv3d(256, 256, 3, 1, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n\t        )  #8\n", "        self.conv5 = nn.Sequential(\n\t            nn.Conv3d(256, 512, 3, 2, 1),\n\t            norm(512),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(512, 512, 3, 1, 1)\n\t        )\n\t    def forward(self, mean, var):\n\t        x = torch.cat([self.mean_embed(mean),self.var_embed(var)],1)\n\t        x = self.conv0(x)\n\t        x = self.conv2(self.conv1(x))\n", "        x = self.conv4(self.conv3(x))\n\t        x = self.conv5(x)\n\t        return x\n\tdef fc(in_planes, out_planes, relu=True):\n\t    if relu:\n\t        return nn.Sequential(\n\t            nn.Linear(in_planes, out_planes),\n\t            nn.LeakyReLU(0.1, inplace=True))\n\t    else:\n\t        return nn.Linear(in_planes, out_planes)\n", "class CasRefineRegressor(nn.Module):\n\t    def __init__(self, upsample=False):\n\t        super().__init__()\n\t        if upsample:\n\t            self.fc = nn.Sequential(   fc( int((1.5)**3*512 * 4**3) , 512), nn.Dropout(p=0.15), fc(512, 512))\n\t        else:\n\t            self.fc = nn.Sequential(fc(512 * 4**3, 512), fc(512, 512))\n\t        self.fcr = nn.Linear(512,4)\n\t        self.fct = nn.Linear(512,2)\n\t        self.fcs = nn.Linear(512,1)\n", "    def forward(self, x):\n\t        x = self.fc(x)\n\t        r = F.normalize(self.fcr(x),dim=1)\n\t        t = self.fct(x)\n\t        s = self.fcs(x)\n\t        return r, t, s\n\tclass CasTransformer(nn.Module):\n\t    def __init__(self, input_size, output_size, hidden_size, num_layer, nhead=8, dropout=0.1):\n\t        super(CasTransformer, self).__init__()\n\t        self.linear1 = nn.Linear(input_size, hidden_size)\n", "        self.transformer_encoder = nn.Sequential(*[nn.TransformerEncoderLayer(hidden_size, \\\n\t            nhead=nhead, dropout=dropout, batch_first=True) for _ in range(num_layer)])\n\t        self.linear2 = nn.Linear(hidden_size, output_size)\n\t        self.dropout = nn.Dropout(dropout)\n\t    def forward(self, x, state=None):\n\t        x = self.linear1(self.dropout(x))\n\t        output = self.transformer_encoder(x)\n\t        output = self.linear2(output)\n\t        return output, state\n\tfrom loguru import logger\n", "class CasVolumeRefiner(nn.Module):\n\t    default_cfg = {\n\t        \"refiner_sample_num\": 32,\n\t    }\n\t    def __init__(self, cfg, upsample=False):\n\t        self.cfg={**self.default_cfg, **cfg}\n\t        super().__init__()\n\t        self.use_dino = self.cfg.get(\"use_dino\", False)  \n\t        self.use_transformer = self.cfg.get(\"use_transformer\", False) \n\t        logger.debug( f\"VolumeRefiner use_dino:{self.use_dino}, use_transformer:{self.use_transformer}\" )\n", "        self.upsample = upsample\n\t        self.feature_net = CasRefineFeatureNet('instance', self.use_dino, upsample)\n\t        self.volume_net = CasRefineVolumeEncodingNet('instance')\n\t        self.regressor = CasRefineRegressor(upsample)\n\t        # used in inference\n\t        self.ref_database = None\n\t        self.ref_ids = None\n\t        if self.use_transformer:\n\t            self.view_trans = CasTransformer(\n\t                input_size=32768, \n", "                output_size=32768, \n\t                hidden_size=64,\n\t                num_layer=1,\n\t                nhead=8, \n\t                dropout=0.1\n\t            )\n\t    @staticmethod\n\t    def interpolate_volume_feats(feats, verts, projs, h_in, w_in):\n\t        \"\"\"\n\t        @param feats: b,f,h,w \n", "        @param verts: b,sx,sy,sz,3\n\t        @param projs: b,3,4 : project matric\n\t        @param h_in:  int\n\t        @param w_in:  int\n\t        @return:\n\t        \"\"\"\n\t        b, sx, sy, sz, _ = verts.shape\n\t        b, f, h, w = feats.shape\n\t        R, t = projs[:,:3,:3], projs[:,:3,3:] # b,3,3  b,3,1\n\t        verts = verts.reshape(b,sx*sy*sz,3)\n", "        verts = verts @ R.permute(0, 2, 1) + t.permute(0, 2, 1) #\n\t        depth = verts[:, :, -1:]\n\t        depth[depth < 1e-4] = 1e-4\n\t        verts = verts[:, :, :2] / depth  # [b,sx*sy*sz,2]\n\t        verts = normalize_coords(verts, h_in, w_in) # b,sx*sy*sz,2]\n\t        verts = verts.reshape([b, sx, sy*sz, 2])\n\t        volume_feats = F.grid_sample(feats, verts, mode='bilinear', align_corners=False) # b,f,sx,sy*sz\n\t        return volume_feats.reshape(b, f, sx, sy, sz)\n\t    def construct_feature_volume(self, que_imgs_info, ref_imgs_info, feature_extractor, sample_num):\n\t        \"\"\"_summary_\n", "        Args:\n\t            que_imgs_info (_type_): _description_\n\t            ref_imgs_info (_type_): _description_\n\t            feature_extractor (_type_): 特征提取器\n\t            sample_num (_type_): 采样图片的个数\n\t        Returns:\n\t            _type_: _description_\n\t        \"\"\"\n\t        # build a volume on the unit cube\n\t        sn = sample_num\n", "        device = que_imgs_info['imgs'].device\n\t        vol_coords = torch.linspace(-1, 1, sample_num, dtype=torch.float32, device=device)\n\t        vol_coords = torch.stack(torch.meshgrid(vol_coords,vol_coords,vol_coords),-1) # sn,sn,sn,3\n\t        vol_coords = vol_coords.reshape(1,sn**3,3)\n\t        # rotate volume to align with the input pose, but still in the object coordinate\n\t        poses_in = que_imgs_info['poses_in'] # qn,3,4\n\t        rotation = poses_in[:,:3,:3] # qn,3,3\n\t        vol_coords = vol_coords @ rotation # qn,sn**3,3\n\t        qn = poses_in.shape[0]\n\t        vol_coords = vol_coords.reshape(qn, sn, sn, sn, 3)\n", "        # project onto every reference view\n\t        ref_poses = ref_imgs_info['poses'] # qn,rfn,3,4\n\t        ref_Ks = ref_imgs_info['Ks'] # qn,rfn,3,3\n\t        ref_proj = ref_Ks @ ref_poses # qn,rfn,3,4\n\t        vol_feats_mean, vol_feats_std = [], []\n\t        h_in, w_in = ref_imgs_info['imgs'].shape[-2:]\n\t        for qi in range(qn):\n\t            ref_feats = feature_extractor(ref_imgs_info['imgs'][qi]) # rfn,f,h,w\n\t            rfn = ref_feats.shape[0]\n\t            vol_coords_cur = vol_coords[qi:qi+1].repeat(rfn,1,1,1,1) # rfn,sx,sy,sz,3\n", "            vol_feats = CasVolumeRefiner.interpolate_volume_feats(ref_feats, vol_coords_cur, ref_proj[qi], h_in, w_in)\n\t            if self.use_transformer:\n\t                x = vol_feats.view(rfn,128,sn*sn*sn)\n\t                x  = self.view_trans(x)\n\t                vol_feats = x[0].view(rfn,128,sn,sn,sn)\n\t            vol_feats_mean.append(torch.mean(vol_feats, 0))\n\t            vol_feats_std.append(torch.std(vol_feats, 0))\n\t        vol_feats_mean = torch.stack(vol_feats_mean, 0)\n\t        vol_feats_std = torch.stack(vol_feats_std, 0)\n\t        # project onto query view\n", "        h_in, w_in = que_imgs_info['imgs'].shape[-2:]\n\t        que_feats = feature_extractor(que_imgs_info['imgs']) # qn,f,h,w\n\t        que_proj = que_imgs_info['Ks_in'] @ que_imgs_info['poses_in']\n\t        vol_feats_in = CasVolumeRefiner.interpolate_volume_feats(que_feats, vol_coords, que_proj, h_in, w_in) # qn,f,sx,sy,sz\n\t        return vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords\n\t    def forward(self, data):\n\t        is_inference = data['inference'] if 'inference' in data else False\n\t        que_imgs_info = data['que_imgs_info'].copy()\n\t        ref_imgs_info = data['ref_imgs_info'].copy()\n\t        if self.upsample:\n", "            refiner_sample_num = int(self.cfg['refiner_sample_num']*1.5) \n\t        else:\n\t            refiner_sample_num = self.cfg['refiner_sample_num']\n\t        vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords = self.construct_feature_volume(\n\t            que_imgs_info, ref_imgs_info, self.feature_net, refiner_sample_num) # qn,f,dn,h,w   qn,dn\n\t        vol_feats = torch.cat([vol_feats_mean, vol_feats_in], 1)\n\t        vol_feats = self.volume_net(vol_feats, vol_feats_std)\n\t        vol_feats = vol_feats.flatten(1) # qn, f* 4**3\n\t        rotation, offset, scale = self.regressor(vol_feats)\n\t        outputs={'rotation': rotation, 'offset': offset, 'scale': scale}\n", "        if not is_inference:\n\t            # used in training not inference\n\t            qn, sx, sy, sz, _ = vol_coords.shape\n\t            grids = pose_apply_th(que_imgs_info['poses_in'], vol_coords.reshape(qn, sx * sy * sz, 3))\n\t            outputs['grids'] = grids\n\t        return outputs\n\t    def load_ref_imgs(self,ref_database,ref_ids):\n\t        self.ref_database = ref_database\n\t        self.ref_ids = ref_ids\n\t    def refine_que_imgs(self, que_img, que_K, in_pose, size=128, ref_num=6, ref_even=False):\n", "        \"\"\"\n\t        @param que_img:  [h,w,3]\n\t        @param que_K:    [3,3]\n\t        @param in_pose:  [3,4]\n\t        @param size:     int\n\t        @param ref_num:  int\n\t        @param ref_even: bool\n\t        @return:\n\t        \"\"\"\n\t        margin = 0.05\n", "        ref_even_num = min(128,len(self.ref_ids))\n\t        # normalize database and input pose\n\t        ref_database = NormalizedDatabase(self.ref_database) # wrapper: object is in the unit sphere at origin\n\t        in_pose = normalize_pose(in_pose, ref_database.scale, ref_database.offset)\n\t        object_center = get_object_center(ref_database)\n\t        object_diameter = get_diameter(ref_database)\n\t        # warp the query image to look at the object w.r.t input pose\n\t        _, new_f = let_me_look_at(in_pose, que_K, object_center)\n\t        in_dist = np.linalg.norm(pose_inverse(in_pose)[:,3] - object_center)\n\t        in_f = size * (1 - margin) / object_diameter * in_dist\n", "        scale = in_f / new_f\n\t        position = project_points(object_center[None], in_pose, que_K)[0][0]\n\t        que_img_warp, que_K_warp, in_pose_warp, que_pose_rect, H = look_at_crop(\n\t            que_img, que_K, in_pose, position, 0, scale, size, size)\n\t        que_imgs_info = {\n\t            'imgs': color_map_forward(que_img_warp).transpose([2,0,1]),  # 3,h,w\n\t            'Ks_in': que_K_warp.astype(np.float32), # 3,3\n\t            'poses_in': in_pose_warp.astype(np.float32), # 3,4\n\t        }\n\t        ref_ids = select_reference_img_ids_refinement(ref_database, object_center, self.ref_ids, \\\n", "                                                      in_pose_warp, ref_num, ref_even, ref_even_num)\n\t        # normalize the reference images and align the in-plane orientation w.r.t input pose.\n\t        ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs = normalize_reference_views(\n\t            ref_database, ref_ids, size, margin, True, in_pose_warp, que_K_warp)\n\t        ref_imgs_info = {\n\t            'imgs': color_map_forward(np.stack(ref_imgs, 0)).transpose([0, 3, 1, 2]),  # rfn,3,h,w\n\t            'poses': np.stack(ref_poses, 0).astype(np.float32),\n\t            'Ks': np.stack(ref_Ks, 0).astype(np.float32),\n\t        }\n\t        que_imgs_info = to_cuda(imgs_info_to_torch(que_imgs_info))\n", "        ref_imgs_info = to_cuda(imgs_info_to_torch(ref_imgs_info))\n\t        for k,v in que_imgs_info.items(): que_imgs_info[k] = v.unsqueeze(0)\n\t        for k,v in ref_imgs_info.items(): ref_imgs_info[k] = v.unsqueeze(0)\n\t        with torch.no_grad():\n\t            outputs = self.forward({'que_imgs_info': que_imgs_info, 'ref_imgs_info': ref_imgs_info, 'inference': True})\n\t            quat = outputs['rotation'].detach().cpu().numpy()[0] # 4\n\t            scale = 2**outputs['scale'].detach().cpu().numpy()[0] # 1\n\t            offset = outputs['offset'].detach().cpu().numpy()[0] # 2\n\t            # print(\"scale:\", scale , \"quat:\", quat, \"offset:\", offset )\n\t        # compose rotation/scale/offset into a similarity transformation matrix\n", "        pose_sim = compose_sim_pose(scale, quat, offset, in_pose_warp, object_center)\n\t        # convert the similarity transformation to the rigid transformation\n\t        pose_pr = pose_sim_to_pose_rigid(pose_sim, in_pose_warp, que_K_warp, que_K_warp, object_center)\n\t        # apply the pose residual\n\t        pose_pr = pose_compose(pose_pr, pose_inverse(que_pose_rect))\n\t        # convert back to original coordinate system (because we use NormalizedDatabase to wrap the input)\n\t        pose_pr = denormalize_pose(pose_pr, ref_database.scale, ref_database.offset)\n\t        return pose_pr\n\tif __name__ == \"__main__\":\n\t    from utils.base_utils import load_cfg\n", "    cfg = \"configs/refiner/refiner_pretrain.yaml\"\n\t    refiner_cfg = load_cfg(cfg)\n\t    refiner = CasVolumeRefiner(refiner_cfg)\n\t    refiner_sample_num = 32\n\t    ref_imgs_info = {\n\t        'imgs': torch.randn(6,3,128,128) , # rfn,3,h,w\n\t        'poses': torch.randn(6, 3, 4),\n\t        'Ks': torch.randn(6,3,3),\n\t    }\n\t    que_imgs_info = {\n", "        'imgs': torch.randn(3,128,128),  # 3,h,w\n\t        'Ks_in': torch.randn(3, 3), # 3,3\n\t        'poses_in':  torch.randn(3, 4), # 3,4\n\t    }\n\t    for k,v in que_imgs_info.items(): que_imgs_info[k] = v.unsqueeze(0)\n\t    for k,v in ref_imgs_info.items(): ref_imgs_info[k] = v.unsqueeze(0)\n\t    # pose_pr = refiner.refine_que_imgs(que_img, que_K, pose_pr, size=128, ref_num=6, ref_even=True)\n\t    vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords = refiner.construct_feature_volume(\n\t            que_imgs_info, ref_imgs_info, refiner.feature_net, refiner_sample_num)\n\t    mock_data = torch.randn(6,3,128,128)\n", "    net = CasRefineFeatureNet()\n\t    out =  net(mock_data)\n\t    print(out.shape)"]}
{"filename": "network/mvs2d_refiner.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchvision\n\timport numpy as np\n\tfrom network.refiner_ablation import FPN\n\tfrom dataset.database import NormalizedDatabase, normalize_pose, get_object_center, get_diameter, denormalize_pose\n\tfrom network.operator import pose_apply_th, normalize_coords\n\tfrom network.pretrain_models import VGGBNPretrainV3\n\tfrom utils.base_utils import pose_inverse, project_points, color_map_forward, to_cuda, pose_compose\n", "from utils.database_utils import look_at_crop, select_reference_img_ids_refinement, normalize_reference_views\n\tfrom utils.pose_utils import let_me_look_at, compose_sim_pose, pose_sim_to_pose_rigid\n\tfrom utils.imgs_info import imgs_info_to_torch\n\tfrom network.vis_dino_encoder import VitExtractor\n\t# sin-cose embedding module\n\tclass Embedder(nn.Module):\n\t    def __init__(self, **kwargs):\n\t        super(Embedder, self).__init__()\n\t        self.kwargs = kwargs\n\t        self.create_embedding_fn()\n", "    def create_embedding_fn(self):\n\t        embed_fns = []\n\t        d = self.kwargs[\"input_dims\"]\n\t        out_dim = 0\n\t        if self.kwargs[\"include_input\"]:\n\t            embed_fns.append(lambda x: x)\n\t            out_dim += d\n\t        max_freq = self.kwargs[\"max_freq_log2\"]\n\t        N_freqs = self.kwargs[\"num_freqs\"]\n\t        if self.kwargs[\"log_sampling\"]:\n", "            freq_bands = 2.0 ** torch.linspace(0.0, max_freq, steps=N_freqs)\n\t        else:\n\t            freq_bands = torch.linspace(2.0**0.0, 2.0**max_freq, steps=N_freqs)\n\t        for freq in freq_bands:\n\t            for p_fn in self.kwargs[\"periodic_fns\"]:\n\t                embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))\n\t                out_dim += d\n\t        self.embed_fns = embed_fns\n\t        self.out_dim = out_dim\n\t    def forward(self, inputs):\n", "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n\tclass FeedForward(nn.Module):\n\t    def __init__(self, dim, hid_dim, dp_rate):\n\t        super(FeedForward, self).__init__()\n\t        self.fc1 = nn.Linear(dim, hid_dim)\n\t        self.fc2 = nn.Linear(hid_dim, dim)\n\t        self.dp = nn.Dropout(dp_rate)\n\t        self.activ = nn.ReLU()\n\t    def forward(self, x):\n\t        x = self.dp(self.activ(self.fc1(x)))\n", "        x = self.dp(self.fc2(x))\n\t        return x\n\t# Subtraction-based efficient attention\n\tclass Attention2D(nn.Module):\n\t    def __init__(self, dim, dp_rate):\n\t        super(Attention2D, self).__init__()\n\t        self.q_fc = nn.Linear(dim, dim, bias=False)\n\t        self.k_fc = nn.Linear(dim, dim, bias=False)\n\t        self.v_fc = nn.Linear(dim, dim, bias=False)\n\t        self.pos_fc = nn.Sequential(\n", "            nn.Linear(4, dim // 8),\n\t            nn.ReLU(),\n\t            nn.Linear(dim // 8, dim),\n\t        )\n\t        self.attn_fc = nn.Sequential(\n\t            nn.Linear(dim, dim // 8),\n\t            nn.ReLU(),\n\t            nn.Linear(dim // 8, dim),\n\t        )\n\t        self.out_fc = nn.Linear(dim, dim)\n", "        self.dp = nn.Dropout(dp_rate)\n\t    def forward(self, q, k, pos, mask=None):\n\t        q = self.q_fc(q)\n\t        k = self.k_fc(k)\n\t        v = self.v_fc(k)\n\t        pos = self.pos_fc(pos)\n\t        attn = k - q[:, :, None, :] + pos\n\t        attn = self.attn_fc(attn)\n\t        if mask is not None:\n\t            attn = attn.masked_fill(mask == 0, -1e9)\n", "        attn = torch.softmax(attn, dim=-2)\n\t        attn = self.dp(attn)\n\t        x = ((v + pos) * attn).sum(dim=2)\n\t        x = self.dp(self.out_fc(x))\n\t        return x\n\t# View Transformer\n\tclass Transformer2D(nn.Module):\n\t    def __init__(self, dim, ff_hid_dim, ff_dp_rate, attn_dp_rate):\n\t        super(Transformer2D, self).__init__()\n\t        self.attn_norm = nn.LayerNorm(dim, eps=1e-6)\n", "        self.ff_norm = nn.LayerNorm(dim, eps=1e-6)\n\t        self.ff = FeedForward(dim, ff_hid_dim, ff_dp_rate)\n\t        self.attn = Attention2D(dim, attn_dp_rate)\n\t    def forward(self, q, k, pos, mask=None):\n\t        residue = q\n\t        x = self.attn_norm(q)\n\t        x = self.attn(x, k, pos, mask)\n\t        x = x + residue\n\t        residue = x\n\t        x = self.ff_norm(x)\n", "        x = self.ff(x)\n\t        x = x + residue\n\t        return x\n\tclass RefineFeatureNet(nn.Module):\n\t    def __init__(self, \\\n\t                 norm_layer='instance',\\\n\t                 use_dino=False,\\\n\t                 upsample=False):\n\t        super().__init__()\n\t        if norm_layer == 'instance':\n", "            norm=nn.InstanceNorm2d\n\t        else:\n\t            raise NotImplementedError\n\t        self.conv0 = nn.Sequential(\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(64, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n", "        self.conv1 = nn.Sequential(\n\t            nn.Conv2d(512, 256, 3, 1, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n\t        self.conv2 = nn.Sequential(\n\t            nn.Conv2d(512, 256, 3, 1, 1),\n\t            norm(256),\n", "            nn.ReLU(True),\n\t            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n\t        self.conv_out = nn.Sequential(\n\t            nn.Conv2d(64*3, 128, 3, 1, 1),\n\t            norm(128),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(128, 128, 3, 1, 1),\n\t            norm(128),\n", "        )\n\t        self.upsample = upsample\n\t        self.use_dino = use_dino\n\t        if self.upsample:\n\t            self.down_sample = nn.Conv2d(in_channels=128, \\\n\t                                        out_channels=64, \\\n\t                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True)  \n", "        if self.use_dino:\n\t            self.fuse_conv = nn.Conv2d(in_channels=512, \\\n\t                                       out_channels=128, \\\n\t                                       kernel_size=1,\\\n\t                                       stride=1,\\\n\t                                       padding=0, \\\n\t                                       bias=True)\n\t            self.fuse_conv1 = nn.Conv2d(in_channels=256+384, \\\n\t                            out_channels=256, \\\n\t                            kernel_size=1,\\\n", "                            stride=1,\\\n\t                            padding=0, \\\n\t                            bias=True)  \n\t            self.fuse_conv2 = nn.Conv2d(in_channels=512+384, \\\n\t                                        out_channels=512, \\\n\t                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True) \n\t            self.fuse_conv3 = nn.Conv2d(in_channels=512+384, \\\n", "                            out_channels=512, \\\n\t                            kernel_size=1,\\\n\t                            stride=1,\\\n\t                            padding=0, \\\n\t                            bias=True) \n\t        self.fpn = FPN([512,512,128],128)  \n\t        self.use_fpn = False     \n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n\t                nn.init.kaiming_normal(m.weight.data, mode='fan_in')\n", "                if m.bias is not None:\n\t                    m.bias.data.zero_()\n\t        if self.use_dino:\n\t            self.fea_ext =  VitExtractor(model_name='dino_vits8').eval()\n\t            for para in self.fea_ext.parameters():\n\t                para.requires_grad = False\n\t            self.fea_ext.requires_grad_(False) \n\t        self.backbone = VGGBNPretrainV3().eval()\n\t        for para in self.backbone.parameters():\n\t            para.requires_grad = False\n", "        self.img_norm = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t    def forward(self, imgs):\n\t        _,_, h,w = imgs.shape\n\t        if self.upsample:\n\t            imgs = F.interpolate(imgs, size=(int(1.5*h), int(1.5*h)))\n\t        if self.use_dino:\n\t            dino_imgs = imgs.clone()\n\t        imgs = self.img_norm(imgs)\n\t        self.backbone.eval()\n\t        with torch.no_grad():\n", "            x0, x1, x2 = self.backbone(imgs)\n\t            x0 = F.normalize(x0, dim=1)\n\t            x1 = F.normalize(x1, dim=1)\n\t            x2 = F.normalize(x2, dim=1)\n\t        x0 = self.conv0(x0)\n\t        x1 = F.interpolate(self.conv1(x1),scale_factor=2,mode='bilinear')\n\t        x2 = F.interpolate(self.conv2(x2),scale_factor=4,mode='bilinear')\n\t        x = torch.cat([x0,x1,x2],1)\n\t        if self.use_fpn:\n\t            x = self.fpn([x0,x1,x])\n", "        else:\n\t            x = self.conv_out(x)  \n\t        if self.use_dino:\n\t            # -------------------------------------------------------- \n\t            dino_imgs = F.interpolate(dino_imgs, size=(256, 256)) \n\t            dino_ret =  self.fea_ext.get_vit_attn_feat(dino_imgs)\n\t            attn, cls_, feat = dino_ret['attn'], dino_ret['cls_'], dino_ret['feat']\n\t            dino_fea = feat.permute(0,2,1).reshape(-1,384,32,32)    \n\t            fused_fea = torch.cat( (x,dino_fea), dim = 1)\n\t            x = self.fuse_conv(fused_fea)\n", "            # --------------------------------------------------------\n\t        return x\n\tclass RefineVolumeEncodingNet(nn.Module):\n\t    def __init__(self,norm_layer='no_norm'):\n\t        super().__init__()\n\t        if norm_layer == 'instance':\n\t            norm=nn.InstanceNorm3d\n\t        else:\n\t            raise NotImplementedError\n\t        self.mean_embed = nn.Sequential(\n", "            nn.Conv3d(128 * 2, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(64, 64, 3, 1, 1)\n\t        )\n\t        self.var_embed = nn.Sequential(\n\t            nn.Conv3d(128, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(64, 64, 3, 1, 1)\n", "        )\n\t        self.conv0 = nn.Sequential(\n\t            nn.Conv3d(64*2, 64, 3, 1, 1), # 32\n\t            norm(64),\n\t            nn.ReLU(True),\n\t        ) # 32\n\t        self.conv1 = nn.Sequential(\n\t            nn.Conv3d(64, 128, 3, 2, 1),\n\t            norm(128),\n\t            nn.ReLU(True),\n", "        )\n\t        self.conv2 = nn.Sequential(\n\t            nn.Conv3d(128, 128, 3, 1, 1),\n\t            norm(128),\n\t            nn.ReLU(True),\n\t        ) # 16\n\t        self.conv3 = nn.Sequential(\n\t            nn.Conv3d(128, 256, 3, 2, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n", "        )\n\t        self.conv4 = nn.Sequential(\n\t            nn.Conv3d(256, 256, 3, 1, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n\t        )  #8\n\t        self.conv5 = nn.Sequential(\n\t            nn.Conv3d(256, 512, 3, 2, 1),\n\t            norm(512),\n\t            nn.ReLU(True),\n", "            nn.Conv3d(512, 512, 3, 1, 1)\n\t        )\n\t    def forward(self, mean, var):\n\t        x = torch.cat([self.mean_embed(mean),self.var_embed(var)],1)\n\t        x = self.conv0(x)\n\t        x = self.conv2(self.conv1(x))\n\t        x = self.conv4(self.conv3(x))\n\t        x = self.conv5(x)\n\t        return x\n\tdef fc(in_planes, out_planes, relu=True):\n", "    if relu:\n\t        return nn.Sequential(\n\t            nn.Linear(in_planes, out_planes),\n\t            nn.LeakyReLU(0.1, inplace=True))\n\t    else:\n\t        return nn.Linear(in_planes, out_planes)\n\tclass RefineRegressor(nn.Module):\n\t    def __init__(self, upsample=False):\n\t        super().__init__()\n\t        if upsample:\n", "            self.fc = nn.Sequential(   fc( int((1.5)**3*512 * 4**3) , 512), nn.Dropout(p=0.15), fc(512, 512))\n\t        else:\n\t            self.fc = nn.Sequential(fc(512 * 4**3, 512), fc(512, 512))\n\t        self.fcr = nn.Linear(512,4)\n\t        self.fct = nn.Linear(512,2)\n\t        self.fcs = nn.Linear(512,1)\n\t    def forward(self, x):\n\t        x = self.fc(x)\n\t        r = F.normalize(self.fcr(x),dim=1)\n\t        t = self.fct(x)\n", "        s = self.fcs(x)\n\t        return r, t, s\n\tclass Transformer(nn.Module):\n\t    def __init__(self, input_size, output_size, hidden_size, num_layer, nhead=8, dropout=0.1):\n\t        super(Transformer, self).__init__()\n\t        self.linear1 = nn.Linear(input_size, hidden_size)\n\t        self.transformer_encoder = nn.Sequential(*[nn.TransformerEncoderLayer(hidden_size, \\\n\t            nhead=nhead, dropout=dropout, batch_first=True) for _ in range(num_layer)])\n\t        self.linear2 = nn.Linear(hidden_size, output_size)\n\t        self.dropout = nn.Dropout(dropout)\n", "    def forward(self, x, state=None):\n\t        x = self.linear1(self.dropout(x))\n\t        output = self.transformer_encoder(x)\n\t        output = self.linear2(output)\n\t        return output, state\n\tfrom loguru import logger\n\tclass VolumeRefiner(nn.Module):\n\t    default_cfg = {\n\t        \"refiner_sample_num\": 32,\n\t    }\n", "    def __init__(self, cfg, upsample=False):\n\t        self.cfg={**self.default_cfg, **cfg}\n\t        super().__init__()\n\t        self.use_dino = self.cfg.get(\"use_dino\", False)  \n\t        self.use_transformer = self.cfg.get(\"use_transformer\", False) \n\t        logger.debug( f\"VolumeRefiner use_dino:{self.use_dino}, use_transformer:{self.use_transformer}\" )\n\t        self.upsample = upsample\n\t        self.feature_net = RefineFeatureNet('instance', self.use_dino, upsample)\n\t        self.volume_net = RefineVolumeEncodingNet('instance')\n\t        self.regressor = RefineRegressor(upsample)\n", "        # used in inference\n\t        self.ref_database = None\n\t        self.ref_ids = None\n\t        if self.use_transformer:\n\t            self.view_trans = Transformer(\n\t                input_size=32768, \n\t                output_size=32768, \n\t                hidden_size=64,\n\t                num_layer=1,\n\t                nhead=8, \n", "                dropout=0.1\n\t            )\n\t    @staticmethod\n\t    def interpolate_volume_feats(feats, verts, projs, h_in, w_in):\n\t        \"\"\"\n\t        @param feats: b,f,h,w \n\t        @param verts: b,sx,sy,sz,3\n\t        @param projs: b,3,4 : project matric\n\t        @param h_in:  int\n\t        @param w_in:  int\n", "        @return:\n\t        \"\"\"\n\t        b, sx, sy, sz, _ = verts.shape\n\t        b, f, h, w = feats.shape\n\t        R, t = projs[:,:3,:3], projs[:,:3,3:] # b,3,3  b,3,1\n\t        verts = verts.reshape(b,sx*sy*sz,3)\n\t        verts = verts @ R.permute(0, 2, 1) + t.permute(0, 2, 1) #\n\t        depth = verts[:, :, -1:]\n\t        depth[depth < 1e-4] = 1e-4\n\t        verts = verts[:, :, :2] / depth  # [b,sx*sy*sz,2]\n", "        verts = normalize_coords(verts, h_in, w_in) # b,sx*sy*sz,2]\n\t        verts = verts.reshape([b, sx, sy*sz, 2])\n\t        volume_feats = F.grid_sample(feats, verts, mode='bilinear', align_corners=False) # b,f,sx,sy*sz\n\t        return volume_feats.reshape(b, f, sx, sy, sz)\n\t    def construct_feature_volume(self, que_imgs_info, ref_imgs_info, feature_extractor, sample_num):\n\t        \"\"\"_summary_\n\t        Args:\n\t            que_imgs_info (_type_): _description_\n\t            ref_imgs_info (_type_): _description_\n\t            feature_extractor (_type_): 特征提取器\n", "            sample_num (_type_): 采样图片的个数\n\t        Returns:\n\t            _type_: _description_\n\t        \"\"\"\n\t        # build a volume on the unit cube\n\t        sn = sample_num\n\t        device = que_imgs_info['imgs'].device\n\t        vol_coords = torch.linspace(-1, 1, sample_num, dtype=torch.float32, device=device)\n\t        vol_coords = torch.stack(torch.meshgrid(vol_coords,vol_coords,vol_coords),-1) # sn,sn,sn,3\n\t        vol_coords = vol_coords.reshape(1,sn**3,3)\n", "        # rotate volume to align with the input pose, but still in the object coordinate\n\t        poses_in = que_imgs_info['poses_in'] # qn,3,4\n\t        rotation = poses_in[:,:3,:3] # qn,3,3\n\t        vol_coords = vol_coords @ rotation # qn,sn**3,3\n\t        qn = poses_in.shape[0]\n\t        vol_coords = vol_coords.reshape(qn, sn, sn, sn, 3)\n\t        # project onto every reference view\n\t        ref_poses = ref_imgs_info['poses'] # qn,rfn,3,4\n\t        ref_Ks = ref_imgs_info['Ks'] # qn,rfn,3,3\n\t        ref_proj = ref_Ks @ ref_poses # qn,rfn,3,4\n", "        vol_feats_mean, vol_feats_std = [], []\n\t        h_in, w_in = ref_imgs_info['imgs'].shape[-2:]\n\t        for qi in range(qn):\n\t            ref_feats = feature_extractor(ref_imgs_info['imgs'][qi]) # rfn,f,h,w\n\t            rfn = ref_feats.shape[0]\n\t            vol_coords_cur = vol_coords[qi:qi+1].repeat(rfn,1,1,1,1) # rfn,sx,sy,sz,3\n\t            vol_feats = VolumeRefiner.interpolate_volume_feats(ref_feats, vol_coords_cur, ref_proj[qi], h_in, w_in)\n\t            if self.use_transformer:\n\t                x = vol_feats.view(rfn,128,sn*sn*sn)\n\t                x  = self.view_trans(x)\n", "                vol_feats = x[0].view(rfn,128,sn,sn,sn)\n\t            vol_feats_mean.append(torch.mean(vol_feats, 0))\n\t            vol_feats_std.append(torch.std(vol_feats, 0))\n\t        vol_feats_mean = torch.stack(vol_feats_mean, 0)\n\t        vol_feats_std = torch.stack(vol_feats_std, 0)\n\t        # project onto query view\n\t        h_in, w_in = que_imgs_info['imgs'].shape[-2:]\n\t        que_feats = feature_extractor(que_imgs_info['imgs']) # qn,f,h,w\n\t        que_proj = que_imgs_info['Ks_in'] @ que_imgs_info['poses_in']\n\t        vol_feats_in = VolumeRefiner.interpolate_volume_feats(que_feats, vol_coords, que_proj, h_in, w_in) # qn,f,sx,sy,sz\n", "        return vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords\n\t    def forward(self, data):\n\t        is_inference = data['inference'] if 'inference' in data else False\n\t        que_imgs_info = data['que_imgs_info'].copy()\n\t        ref_imgs_info = data['ref_imgs_info'].copy()\n\t        if self.upsample:\n\t            refiner_sample_num = int(self.cfg['refiner_sample_num']*1.5) \n\t        else:\n\t            refiner_sample_num = self.cfg['refiner_sample_num']\n\t        vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords = self.construct_feature_volume(\n", "            que_imgs_info, ref_imgs_info, self.feature_net, refiner_sample_num) # qn,f,dn,h,w   qn,dn\n\t        vol_feats = torch.cat([vol_feats_mean, vol_feats_in], 1)\n\t        vol_feats = self.volume_net(vol_feats, vol_feats_std)\n\t        vol_feats = vol_feats.flatten(1) # qn, f* 4**3\n\t        rotation, offset, scale = self.regressor(vol_feats)\n\t        outputs={'rotation': rotation, 'offset': offset, 'scale': scale}\n\t        if not is_inference:\n\t            # used in training not inference\n\t            qn, sx, sy, sz, _ = vol_coords.shape\n\t            grids = pose_apply_th(que_imgs_info['poses_in'], vol_coords.reshape(qn, sx * sy * sz, 3))\n", "            outputs['grids'] = grids\n\t        return outputs\n\t    def load_ref_imgs(self,ref_database,ref_ids):\n\t        self.ref_database = ref_database\n\t        self.ref_ids = ref_ids\n\t    def refine_que_imgs(self, que_img, que_K, in_pose, size=128, ref_num=6, ref_even=False):\n\t        \"\"\"\n\t        @param que_img:  [h,w,3]\n\t        @param que_K:    [3,3]\n\t        @param in_pose:  [3,4]\n", "        @param size:     int\n\t        @param ref_num:  int\n\t        @param ref_even: bool\n\t        @return:\n\t        \"\"\"\n\t        margin = 0.05\n\t        ref_even_num = min(128,len(self.ref_ids))\n\t        # normalize database and input pose\n\t        ref_database = NormalizedDatabase(self.ref_database) # wrapper: object is in the unit sphere at origin\n\t        in_pose = normalize_pose(in_pose, ref_database.scale, ref_database.offset)\n", "        object_center = get_object_center(ref_database)\n\t        object_diameter = get_diameter(ref_database)\n\t        # warp the query image to look at the object w.r.t input pose\n\t        _, new_f = let_me_look_at(in_pose, que_K, object_center)\n\t        in_dist = np.linalg.norm(pose_inverse(in_pose)[:,3] - object_center)\n\t        in_f = size * (1 - margin) / object_diameter * in_dist\n\t        scale = in_f / new_f\n\t        position = project_points(object_center[None], in_pose, que_K)[0][0]\n\t        que_img_warp, que_K_warp, in_pose_warp, que_pose_rect, H = look_at_crop(\n\t            que_img, que_K, in_pose, position, 0, scale, size, size)\n", "        que_imgs_info = {\n\t            'imgs': color_map_forward(que_img_warp).transpose([2,0,1]),  # 3,h,w\n\t            'Ks_in': que_K_warp.astype(np.float32), # 3,3\n\t            'poses_in': in_pose_warp.astype(np.float32), # 3,4\n\t        }\n\t        # print( que_imgs_info['imgs'].shape ,  que_imgs_info['Ks_in'].shape  , que_imgs_info['poses_in'].shape )\n\t        # select reference views for refinement\n\t        ref_ids = select_reference_img_ids_refinement(ref_database, object_center, self.ref_ids, \\\n\t                                                      in_pose_warp, ref_num, ref_even, ref_even_num)\n\t        # normalize the reference images and align the in-plane orientation w.r.t input pose.\n", "        ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs = normalize_reference_views(\n\t            ref_database, ref_ids, size, margin, True, in_pose_warp, que_K_warp)\n\t        ref_imgs_info = {\n\t            'imgs': color_map_forward(np.stack(ref_imgs, 0)).transpose([0, 3, 1, 2]),  # rfn,3,h,w\n\t            'poses': np.stack(ref_poses, 0).astype(np.float32),\n\t            'Ks': np.stack(ref_Ks, 0).astype(np.float32),\n\t        }\n\t        # print( ref_imgs_info['imgs'].shape ,  ref_imgs_info['poses'].shape  , ref_imgs_info['Ks'].shape )\n\t        que_imgs_info = to_cuda(imgs_info_to_torch(que_imgs_info))\n\t        ref_imgs_info = to_cuda(imgs_info_to_torch(ref_imgs_info))\n", "        for k,v in que_imgs_info.items(): que_imgs_info[k] = v.unsqueeze(0)\n\t        for k,v in ref_imgs_info.items(): ref_imgs_info[k] = v.unsqueeze(0)\n\t        with torch.no_grad():\n\t            outputs = self.forward({'que_imgs_info': que_imgs_info, 'ref_imgs_info': ref_imgs_info, 'inference': True})\n\t            quat = outputs['rotation'].detach().cpu().numpy()[0] # 4\n\t            scale = 2**outputs['scale'].detach().cpu().numpy()[0] # 1\n\t            offset = outputs['offset'].detach().cpu().numpy()[0] # 2\n\t            # print(\"scale:\", scale , \"quat:\", quat, \"offset:\", offset )\n\t        # compose rotation/scale/offset into a similarity transformation matrix\n\t        pose_sim = compose_sim_pose(scale, quat, offset, in_pose_warp, object_center)\n", "        # convert the similarity transformation to the rigid transformation\n\t        pose_pr = pose_sim_to_pose_rigid(pose_sim, in_pose_warp, que_K_warp, que_K_warp, object_center)\n\t        # apply the pose residual\n\t        pose_pr = pose_compose(pose_pr, pose_inverse(que_pose_rect))\n\t        # convert back to original coordinate system (because we use NormalizedDatabase to wrap the input)\n\t        pose_pr = denormalize_pose(pose_pr, ref_database.scale, ref_database.offset)\n\t        return pose_pr\n\tif __name__ == \"__main__\":\n\t    from utils.base_utils import load_cfg\n\t    cfg = \"configs/refiner/refiner_pretrain.yaml\"\n", "    refiner_cfg = load_cfg(cfg)\n\t    refiner = VolumeRefiner(refiner_cfg)\n\t    refiner_sample_num = 32\n\t    ref_imgs_info = {\n\t        'imgs': torch.randn(6,3,128,128) , # rfn,3,h,w\n\t        'poses': torch.randn(6, 3, 4),\n\t        'Ks': torch.randn(6,3,3),\n\t    }\n\t    que_imgs_info = {\n\t        'imgs': torch.randn(3,128,128),  # 3,h,w\n", "        'Ks_in': torch.randn(3, 3), # 3,3\n\t        'poses_in':  torch.randn(3, 4), # 3,4\n\t    }\n\t    for k,v in que_imgs_info.items(): que_imgs_info[k] = v.unsqueeze(0)\n\t    for k,v in ref_imgs_info.items(): ref_imgs_info[k] = v.unsqueeze(0)\n\t    # pose_pr = refiner.refine_que_imgs(que_img, que_K, pose_pr, size=128, ref_num=6, ref_even=True)\n\t    vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords = refiner.construct_feature_volume(\n\t            que_imgs_info, ref_imgs_info, refiner.feature_net, refiner_sample_num)\n\t    mock_data = torch.randn(6,3,128,128)\n\t    net = RefineFeatureNet()\n", "    out =  net(mock_data)\n\t    print(out.shape)"]}
{"filename": "network/vis_dino_encoder.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch\n\timport network.vision_transformer as vits\n\tfrom network.vision_transformer import Attention, dino_vits16, dino_vits8 \n\tfrom network.vision_transformer import dino_vitb16, dino_vitb8\n\tfrom network.vision_transformer import dino_xcit_small_12_p16\n\tfrom network.vision_transformer import dino_xcit_medium_24_p8\n\tfrom  network.vision_transformer  import vit_small\n", "from torchvision import transforms as pth_transforms\n\tfrom PIL import Image\n\t\"\"\"\n\tAPI:\n\tmodel =  VitExtractor(\"model_name\")\n\t\"\"\"\n\tname2network={\n\t    'vit_small': vit_small,\n\t    'dino_vits16':dino_vits16,\n\t    'dino_vits8':dino_vits8,\n", "    'dino_vitb16': dino_vitb16,\n\t    'dino_vitb8': dino_vitb8,\n\t    'dino_xcit_small_12_p16': dino_xcit_small_12_p16,\n\t    'dino_xcit_medium_24_p8': dino_xcit_medium_24_p8,\n\t}\n\tclass VitExtractor(nn.Module):\n\t    BLOCK_KEY = 'block'\n\t    ATTN_KEY = 'attn'\n\t    PATCH_IMD_KEY = 'patch_imd'\n\t    QKV_KEY = 'qkv'\n", "    KEY_LIST = [BLOCK_KEY, ATTN_KEY, PATCH_IMD_KEY, QKV_KEY]\n\t    def __init__(self, model_name):\n\t        super().__init__()\n\t        self.model = name2network[model_name](pretrained=True)\n\t        self.model.eval()\n\t        self.model_name = model_name\n\t        self.hook_handlers = []\n\t        self.layers_dict = {}\n\t        self.outputs_dict = {}\n\t        for key in VitExtractor.KEY_LIST:\n", "            self.layers_dict[key] = []\n\t            self.outputs_dict[key] = []\n\t        self._init_hooks_data()\n\t    def get_feat_attn_from_input(self, input_img):\n\t        self._register_hooks()\n\t        self.model(input_img)\n\t        attn = self.outputs_dict[VitExtractor.ATTN_KEY]\n\t        cls_ = self.outputs_dict[VitExtractor.BLOCK_KEY]\n\t        self._clear_hooks()\n\t        self._init_hooks_data()\n", "        return  {'attn': attn, 'cls_': cls_}\n\t    def get_vit_feature(self, x):\n\t        mean = torch.tensor([0.485, 0.456, 0.406],\n\t                            device=x.device).reshape(1, 3, 1, 1)\n\t        std = torch.tensor([0.229, 0.224, 0.225],\n\t                           device=x.device).reshape(1, 3, 1, 1)\n\t        x = F.interpolate(x, size=(224, 224))\n\t        x = (x - mean) / std\n\t        return self.get_feature_from_input(x)[-1][0, 0, :]\n\t    def get_vit_attn_feat(self, x):\n", "        mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).reshape(1, 3, 1, 1)\n\t        std = torch.tensor([0.229, 0.224, 0.225], device=x.device).reshape(1, 3, 1, 1)\n\t        x = (x - mean) / std\n\t        ret = self.get_feat_attn_from_input(x)\n\t        attn = ret['attn'][-1].mean(1).unsqueeze(1)[:, :, 0, 1:]\n\t        cls_ = ret['cls_'][-1][:, 0, :]\n\t        feat = ret['cls_'][-1][:, 1:, :]\n\t        return {'attn': attn, 'cls_': cls_, 'feat':feat}\n\t    def get_vit_att_map(self, img):\n\t        img = Image.fromarray(img, mode='RGB')\n", "        transform = pth_transforms.Compose([\n\t            # pth_transforms.Resize((224,224)),\n\t            pth_transforms.ToTensor(),\n\t            pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n\t        ])\n\t        img = transform(img)\n\t        patch_size = 8\n\t        w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n\t        img = img[:, :w, :h].unsqueeze(0)\n\t        w_featmap = img.shape[-2] // patch_size\n", "        h_featmap = img.shape[-1] // patch_size\n\t        attentions = self.model.get_last_selfattention(img)\n\t        nh = attentions.shape[1]\n\t        attentions = attentions[0, :, 0, 1:].reshape(nh,  w_featmap, h_featmap)\n\t        # attentions = attentions.reshape(nh,)\n\t        attentions = nn.functional.interpolate(attentions.unsqueeze(0),  \\\n\t                                                scale_factor=patch_size,\\\n\t                                                 mode=\"nearest\")[0]\n\t        return attentions\n\t    def attn_cosine_sim(self, x, eps=1e-08):\n", "        x = x[0]  # TEMP: getting rid of redundant dimension, TBF\n\t        norm1 = x.norm(dim=2, keepdim=True)\n\t        factor = torch.clamp(norm1 @ norm1.permute(0, 2, 1), min=eps)\n\t        sim_matrix = (x @ x.permute(0, 2, 1)) / factor\n\t        return sim_matrix\n\t    def _init_hooks_data(self):\n\t        self.layers_dict[VitExtractor.BLOCK_KEY] = [\n\t            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\t        self.layers_dict[VitExtractor.ATTN_KEY] = [\n\t            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n", "        self.layers_dict[VitExtractor.QKV_KEY] = [\n\t            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\t        self.layers_dict[VitExtractor.PATCH_IMD_KEY] = [\n\t            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\t        for key in VitExtractor.KEY_LIST:\n\t            # self.layers_dict[key] = kwargs[key] if key in kwargs.keys() else []\n\t            self.outputs_dict[key] = []\n\t    def _register_hooks(self, **kwargs):\n\t        for block_idx, block in enumerate(self.model.blocks):\n\t            if block_idx in self.layers_dict[VitExtractor.BLOCK_KEY]:\n", "                self.hook_handlers.append(\n\t                    block.register_forward_hook(self._get_block_hook()))\n\t            if block_idx in self.layers_dict[VitExtractor.ATTN_KEY]:\n\t                self.hook_handlers.append(\n\t                    block.attn.attn_drop.register_forward_hook(self._get_attn_hook()))\n\t            if block_idx in self.layers_dict[VitExtractor.QKV_KEY]:\n\t                self.hook_handlers.append(\n\t                    block.attn.qkv.register_forward_hook(self._get_qkv_hook()))\n\t            if block_idx in self.layers_dict[VitExtractor.PATCH_IMD_KEY]:\n\t                self.hook_handlers.append(\n", "                    block.attn.register_forward_hook(self._get_patch_imd_hook()))\n\t    def _clear_hooks(self):\n\t        for handler in self.hook_handlers:\n\t            handler.remove()\n\t        self.hook_handlers = []\n\t    def _get_block_hook(self):\n\t        def _get_block_output(model, input, output):\n\t            self.outputs_dict[VitExtractor.BLOCK_KEY].append(output)\n\t        return _get_block_output\n\t    def _get_attn_hook(self):\n", "        def _get_attn_output(model, inp, output):\n\t            self.outputs_dict[VitExtractor.ATTN_KEY].append(output)\n\t        return _get_attn_output\n\t    def _get_qkv_hook(self):\n\t        def _get_qkv_output(model, inp, output):\n\t            self.outputs_dict[VitExtractor.QKV_KEY].append(output)\n\t        return _get_qkv_output\n\t    # TODO: CHECK ATTN OUTPUT TUPLE\n\t    def _get_patch_imd_hook(self):\n\t        def _get_attn_output(model, inp, output):\n", "            self.outputs_dict[VitExtractor.PATCH_IMD_KEY].append(output[0])\n\t        return _get_attn_output\n\t    def get_att_from_input(self, input_img):  # List([B, N, D])\n\t        self._register_hooks()\n\t        self.model.to(input_img.device)\n\t        self.model(input_img)\n\t        feature = self.outputs_dict[VitExtractor.PATCH_IMD_KEY]\n\t        self._clear_hooks()\n\t        self._init_hooks_data()\n\t        return feature\n", "    def get_feature_from_input(self, input_img):  # List([B, N, D])\n\t        self._register_hooks()\n\t        self.model.to(input_img.device)\n\t        self.model(input_img)\n\t        feature = self.outputs_dict[VitExtractor.BLOCK_KEY]\n\t        self._clear_hooks()\n\t        self._init_hooks_data()\n\t        return feature\n\t    def get_qkv_feature_from_input(self, input_img):\n\t        self._register_hooks()\n", "        self.model(input_img)\n\t        feature = self.outputs_dict[VitExtractor.QKV_KEY]\n\t        self._clear_hooks()\n\t        self._init_hooks_data()\n\t        return feature\n\t    def get_attn_feature_from_input(self, input_img):\n\t        self._register_hooks()\n\t        self.model(input_img)\n\t        feature = self.outputs_dict[VitExtractor.ATTN_KEY]\n\t        self._clear_hooks()\n", "        self._init_hooks_data()\n\t        return feature\n\t    def get_patch_size(self):\n\t        return 8 if \"8\" in self.model_name else 16\n\t    def get_width_patch_num(self, input_img_shape):\n\t        b, c, h, w = input_img_shape\n\t        patch_size = self.get_patch_size()\n\t        return w // patch_size\n\t    def get_height_patch_num(self, input_img_shape):\n\t        b, c, h, w = input_img_shape\n", "        patch_size = self.get_patch_size()\n\t        return h // patch_size\n\t    def get_patch_num(self, input_img_shape):\n\t        patch_num = 1 + (self.get_height_patch_num(input_img_shape)\n\t                         * self.get_width_patch_num(input_img_shape))\n\t        return patch_num\n\t    def get_head_num(self):\n\t        if \"dino\" in self.model_name:\n\t            return 6 if \"s\" in self.model_name else 12\n\t        return 6 if \"small\" in self.model_name else 12\n", "    def get_embedding_dim(self):\n\t        if \"dino\" in self.model_name:\n\t            return 384 if \"s\" in self.model_name else 768\n\t        return 384 if \"small\" in self.model_name else 768\n\t    def get_queries_from_qkv(self, qkv, input_img_shape):\n\t        patch_num = self.get_patch_num(input_img_shape)\n\t        head_num = self.get_head_num()\n\t        embedding_dim = self.get_embedding_dim()\n\t        q = qkv.reshape(patch_num, 3, head_num, embedding_dim //\n\t                        head_num).permute(1, 2, 0, 3)[0]\n", "        return q\n\t    def get_keys_from_qkv(self, qkv, input_img_shape):\n\t        patch_num = self.get_patch_num(input_img_shape)\n\t        head_num = self.get_head_num()\n\t        embedding_dim = self.get_embedding_dim()\n\t        k = qkv.reshape(patch_num, 3, head_num, embedding_dim //\n\t                        head_num).permute(1, 2, 0, 3)[1]\n\t        return k\n\t    def get_values_from_qkv(self, qkv, input_img_shape):\n\t        patch_num = self.get_patch_num(input_img_shape)\n", "        head_num = self.get_head_num()\n\t        embedding_dim = self.get_embedding_dim()\n\t        v = qkv.reshape(patch_num, 3, head_num, embedding_dim //\n\t                        head_num).permute(1, 2, 0, 3)[2]\n\t        return v\n\t    def get_keys_from_input(self, input_img, layer_num):\n\t        qkv_features = self.get_qkv_feature_from_input(input_img)[layer_num]\n\t        keys = self.get_keys_from_qkv(qkv_features, input_img.shape)\n\t        return keys\n\t    def get_keys_self_sim_from_input(self, input_img, layer_num):\n", "        keys = self.get_keys_from_input(input_img, layer_num=layer_num)\n\t        h, t, d = keys.shape\n\t        concatenated_keys = keys.transpose(0, 1).reshape(t, h * d)\n\t        ssim_map = self.attn_cosine_sim(concatenated_keys[None, None, ...])\n\t        return ssim_map\n\tif __name__ == \"__main__\":\n\t    fea_ext =  VitExtractor(model_name='dino_vits8')\n\t    dino_in = torch.randn(1,3,224,224)\n\t    dino_ret = fea_ext.get_vit_attn_feat(dino_in)\n\t    attn, cls_, feat = dino_ret['attn'], dino_ret['cls_'], dino_ret['feat']\n", "    feat = feat.reshape(1,28,28,384)\n\t    print(attn.shape,cls_.shape,feat.shape)\n\t    print( (0.7308+0.5980+0.6139+0.6931+0.4019+0.7009+0.4519+0.8667) /8 )\n\t    print( (0.8173+0.9216+0.9505+0.7723+0.8037+1+0.9615+0.8952) /8 )\n"]}
{"filename": "network/attention.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tdef attention(query, key, value, key_mask=None, temperature=1.0):\n\t    \"\"\"\n\t    @param query:        b,d,h,n\n\t    @param key:          b,d,h,m\n\t    @param value:        b,d,h,m\n\t    @param key_mask:     b,1,1,m\n\t    @param temperature:\n\t    @return:\n", "    \"\"\"\n\t    dim = query.shape[1]\n\t    scores = torch.einsum('bdhn,bdhm->bhnm', query / temperature, key) / dim ** .5 # b,head,n0,n1\n\t    if key_mask is not None: scores = scores.masked_fill(key_mask == 0, -1e7)\n\t    prob = torch.nn.functional.softmax(scores, dim=-1)\n\t    return torch.einsum('bhnm,bdhm->bdhn', prob, value), prob\n\tclass SpecialLayerNorm(nn.Module):\n\t    def __init__(self, in_dim):\n\t        super().__init__()\n\t        self.norm=nn.LayerNorm(in_dim)\n", "    def forward(self,x):\n\t        x = self.norm(x.permute(0,2,1))\n\t        return x.permute(0,2,1)\n\tclass AttentionBlock(nn.Module):\n\t    def __init__(self,in_dim,att_dim,out_dim,head_num=4,temperature=1.0,bias=True,skip_connect=True,norm='layer'):\n\t        super().__init__()\n\t        self.conv_key=nn.Conv1d(in_dim,att_dim,1,bias=bias)\n\t        self.conv_query=nn.Conv1d(in_dim,att_dim,1,bias=bias)\n\t        self.conv_feats=nn.Conv1d(in_dim,out_dim,1,bias=bias)\n\t        self.conv_merge=nn.Conv1d(out_dim,out_dim,1,bias=bias)\n", "        self.head_att_dim=att_dim//head_num\n\t        self.head_out_dim=out_dim//head_num\n\t        self.head_num=head_num\n\t        self.temperature=temperature\n\t        if norm=='layer':\n\t            self.norm=SpecialLayerNorm(out_dim)\n\t        elif norm=='instance':\n\t            self.norm = nn.InstanceNorm1d(out_dim)\n\t        else:\n\t            raise NotImplementedError\n", "        self.skip_connect = skip_connect\n\t        if skip_connect:\n\t            assert(in_dim==out_dim)\n\t    def forward(self, feats_query, feats_key, key_mask=None):\n\t        '''\n\t        :param feats_query: b,f,n0\n\t        :param feats_key: b,f,n1\n\t        :param key_mask: b,1,n1\n\t        :return: b,f,n0\n\t        '''\n", "        b,f,n0=feats_query.shape\n\t        b,f,n1=feats_key.shape\n\t        query=self.conv_query(feats_query).reshape(b, self.head_att_dim, self.head_num, n0)  # b,had,hn,n0\n\t        key=self.conv_key(feats_key).reshape(b, self.head_att_dim, self.head_num, n1)        # b,had,hn,n1\n\t        feats=self.conv_feats(feats_key).reshape(b, self.head_out_dim, self.head_num, n1)    # b,hod,hn,n1\n\t        if key_mask is not None: key_mask = key_mask.reshape(b, 1, 1, n1) # b,1,1,n1\n\t        feats_out, weights = attention(query, key, feats, key_mask, self.temperature)\n\t        feats_out = feats_out.reshape(b,self.head_out_dim*self.head_num,n0) # b,hod*hn,n0\n\t        feats_out = self.conv_merge(feats_out)\n\t        if self.skip_connect: feats_out=feats_out+feats_query\n", "        feats_out = self.norm(feats_out)\n\t        return feats_out"]}
{"filename": "network/selector.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torchvision\n\timport numpy as np\n\timport torch.nn.functional as F\n\tfrom loguru import logger\n\tfrom network.attention import AttentionBlock\n\tfrom network.pretrain_models import VGGBNPretrain\n\tfrom utils.base_utils import color_map_forward\n\tfrom network.vis_dino_encoder import VitExtractor\n", "class ViewpointSelector(nn.Module):\n\t    default_cfg = {\n\t        'selector_angle_num': 5,\n\t    }\n\t    def __init__(self, cfg):\n\t        self.cfg = {**self.default_cfg, **cfg}\n\t        super().__init__()\n\t        self.backbone = VGGBNPretrain([0,1,2])\n\t        for para in self.backbone.parameters():\n\t            para.requires_grad = False\n", "        self.use_dino = self.cfg[\"use_dino\"]\n\t        logger.info(f\"ViewpointSelector use_dino: {self.use_dino}\")\n\t        if self.use_dino:\n\t            self.fea_ext =  VitExtractor(model_name='dino_vits8').eval()\n\t            for para in self.fea_ext.parameters():\n\t                para.requires_grad = False\n\t            self.fea_ext.requires_grad_(False) \n\t            self.fuse_conv1 = nn.Conv2d(in_channels=512+384, \\\n\t                                       out_channels=512, \\\n\t                                       kernel_size=1,\\\n", "                                       stride=1,\\\n\t                                       padding=0, \\\n\t                                       bias=True)\n\t            self.fuse_conv2 = nn.Conv2d(in_channels=512+384, \\\n\t                                       out_channels=512, \\\n\t                                       kernel_size=1,\\\n\t                                       stride=1,\\\n\t                                       padding=0, \\\n\t                                       bias=True)\n\t            self.fuse_conv3 = nn.Conv2d(in_channels=512+384, \\\n", "                                       out_channels=512, \\\n\t                                       kernel_size=1,\\\n\t                                       stride=1,\\\n\t                                       padding=0, \\\n\t                                       bias=True)    \n\t        self.img_norm = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t        self.ref_feats_cache = None\n\t        self.ref_pose_embed = None # rfn,f\n\t        self.ref_angle_embed = None # an,f\n\t        corr_conv0 = nn.Sequential(\n", "            nn.InstanceNorm3d(512),\n\t            nn.Conv3d(512,64,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(64),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(64,64,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(64),\n\t            nn.MaxPool3d((1,2,2),(1,2,2)),\n\t            nn.Conv3d(64,128,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(128),\n\t            nn.ReLU(True),\n", "            nn.Conv3d(128,128,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(128),\n\t            nn.MaxPool3d((1,2,2),(1,2,2)),\n\t            nn.Conv3d(128,256,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(256),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(256,256,(1,3,3),padding=(0,1,1)),\n\t        )\n\t        corr_conv1 = nn.Sequential(\n\t            nn.InstanceNorm3d(512),\n", "            nn.Conv3d(512,128,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(128),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(128,128,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(128),\n\t            nn.MaxPool3d((1,2,2),(1,2,2)),\n\t            nn.Conv3d(128,256,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(256),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(256,256,(1,3,3),padding=(0,1,1)),\n", "        )\n\t        corr_conv2 = nn.Sequential(\n\t            nn.InstanceNorm3d(512),\n\t            nn.Conv3d(512,256,(1,3,3),padding=(0,1,1)),\n\t            nn.InstanceNorm3d(256),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(256,256,(1,3,3),padding=(0,1,1)),\n\t        )\n\t        self.corr_conv_list = nn.ModuleList([corr_conv0,corr_conv1,corr_conv2])\n\t        self.corr_feats_conv = nn.Sequential(\n", "            nn.Conv3d(256*3,512,1,1),\n\t            nn.InstanceNorm3d(512),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(512,512,1,1),\n\t            nn.AvgPool3d((1,4,4))\n\t        )\n\t        self.vp_norm=nn.InstanceNorm2d(3)\n\t        self.score_process = nn.Sequential(\n\t            nn.Conv2d(3+512, 512, 1, 1),\n\t            nn.ReLU(True),\n", "            nn.Conv2d(512, 512, 1, 1),\n\t        )\n\t        self.atts = [AttentionBlock(512, 512, 512, 8, skip_connect=False) for _ in range(2)]\n\t        self.mlps = [nn.Sequential(nn.Conv1d(512*2,512,1,1),nn.InstanceNorm1d(512),nn.ReLU(True),\n\t                                   nn.Conv1d(512,512,1,1),nn.InstanceNorm1d(512),nn.ReLU(True)) for _ in range(2)]\n\t        self.mlps = nn.ModuleList(self.mlps)\n\t        self.atts = nn.ModuleList(self.atts)\n\t        self.score_predict = nn.Sequential(\n\t            nn.Conv1d(512, 512, 1, 1),\n\t            nn.ReLU(True),\n", "            nn.Conv1d(512, 1, 1, 1),\n\t        )\n\t        an = self.cfg['selector_angle_num']\n\t        self.angle_predict = nn.Sequential(\n\t            nn.Conv1d((3+512) * an, 512, 1, 1),\n\t            nn.ReLU(True),\n\t            nn.Conv1d(512, 512, 1, 1),\n\t            nn.ReLU(True),\n\t            nn.Conv1d(512, 1, 1, 1)\n\t        )\n", "        self.view_point_encoder = nn.Sequential(\n\t            nn.Linear(3, 128),\n\t            nn.ReLU(True),\n\t            nn.Linear(128, 256),\n\t            nn.ReLU(True),\n\t            nn.Linear(256, 512),\n\t        )\n\t    def get_feats(self, imgs):\n\t        self.backbone.eval()\n\t        if self.use_dino:\n", "            with torch.no_grad():\n\t                dino_imgs = imgs.clone()\n\t                dino_ret =  self.fea_ext.get_vit_attn_feat(dino_imgs)\n\t            attn, cls_, feat = dino_ret['attn'], dino_ret['cls_'], dino_ret['feat']\n\t            dino_fea = feat.permute(0,2,1).reshape(-1,384,16,16)\n\t            dino_fea = F.normalize(dino_fea, dim=1)\n\t        imgs = self.img_norm(imgs)\n\t        with torch.no_grad():\n\t            feats_list = self.backbone(imgs)\n\t            feats_list = [F.normalize(feats, dim=1) for feats in feats_list]\n", "        if self.use_dino:\n\t            with torch.no_grad():\n\t                dino_fea1 = dino_fea.clone()\n\t                fused_fea1 = torch.cat( (feats_list[0], dino_fea1 ), dim = 1)\n\t                feats_list[0] =  self.fuse_conv1(fused_fea1)\n\t                dino_fea2 = F.interpolate(dino_fea.clone(), size=(8, 8))\n\t                fused_fea2 = torch.cat( (feats_list[1],dino_fea2), dim = 1)\n\t                feats_list[1] =  self.fuse_conv2(fused_fea2)\n\t                dino_fea3 = F.interpolate(dino_fea.clone(), size=(4, 4))\n\t                fused_fea3 = torch.cat( (feats_list[2],dino_fea3), dim = 1)\n", "                feats_list[2] =  self.fuse_conv3(fused_fea3)\n\t        return feats_list\n\t    def extract_ref_feats(self, ref_imgs, ref_poses, object_center, object_vert, is_train=False):\n\t        # get features\n\t        an, rfn, _, h, w = ref_imgs.shape\n\t        ref_feats = self.get_feats(ref_imgs.reshape(an * rfn, 3, h, w))\n\t        ref_feats_out = []\n\t        for feats in ref_feats:\n\t            _, f, h, w = feats.shape\n\t            ref_feats_out.append(feats.reshape(an, rfn, f, h, w))\n", "        self.ref_feats_cache = ref_feats_out\n\t        # get pose embedding\n\t        ref_cam_pts = -ref_poses[:,:3,:3].permute(0,2,1) @ ref_poses[:,:3,3:] # rfn,3,3 @ rfn,3,1\n\t        ref_cam_pts = ref_cam_pts[...,0] - object_center[None] # rfn,3\n\t        if is_train:\n\t            object_forward = ref_cam_pts[np.random.randint(0,ref_cam_pts.shape[0])]\n\t        else:\n\t            object_forward = ref_cam_pts[0]\n\t        # get rotation\n\t        y = torch.cross(object_vert, object_forward)\n", "        x = torch.cross(y, object_vert)\n\t        object_vert = F.normalize(object_vert,dim=0)\n\t        x = F.normalize(x,dim=0)\n\t        y = F.normalize(y,dim=0)\n\t        R = torch.stack([x, y, object_vert], 0)\n\t        ref_cam_pts = ref_cam_pts @ R.T # rfn,3 @ 3,3\n\t        ref_cam_pts = F.normalize(ref_cam_pts,dim=1) # rfn, 3 --> normalized viewpoints here\n\t        self.ref_pose_embed = self.view_point_encoder(ref_cam_pts) # rfn,512\n\t    def load_ref_imgs(self, ref_imgs, ref_poses, object_center, object_vert):\n\t        \"\"\"\n", "        @param ref_imgs: [an,rfn,h,w,3]\n\t        @param ref_poses: [rfn,3,4]\n\t        @param object_center: [3]\n\t        @param object_vert: [3]\n\t        @return:\n\t        \"\"\"\n\t        an,rfn,h,w,_=ref_imgs.shape\n\t        ref_imgs = torch.from_numpy(color_map_forward(ref_imgs).transpose([0, 1, 4, 2, 3])).cuda()  # an,rfn,3,h,w\n\t        ref_poses, object_center, object_vert = torch.from_numpy(ref_poses.astype(np.float32)).cuda(), \\\n\t                                                torch.from_numpy(object_center.astype(np.float32)).cuda(), \\\n", "                                                torch.from_numpy(object_vert.astype(np.float32)).cuda()\n\t        self.extract_ref_feats(ref_imgs, ref_poses, object_center, object_vert)\n\t    def select_que_imgs(self, que_imgs):\n\t        \"\"\"\n\t        @param que_imgs: [qn,h,w,3]\n\t        @return:\n\t        \"\"\"\n\t        que_imgs = torch.from_numpy(color_map_forward(que_imgs).transpose([0, 3, 1, 2])).cuda()  # qn,3,h,w\n\t        logits, angles = self.compute_view_point_feats(que_imgs) # qn,rfn\n\t        ref_idx = torch.argmax(logits,1) # qn,\n", "        angles = angles[torch.arange(ref_idx.shape[0]), ref_idx] # qn,\n\t        # debug set angle to zero\n\t        # angles = torch.zeros_like(angles)\n\t        # qn, qn, [qn,rfn]\n\t        return {'ref_idx': ref_idx.cpu().numpy(), 'angles': angles.cpu().numpy(), 'scores': logits.cpu().numpy()}\n\t    def compute_view_point_feats(self, que_imgs):\n\t        que_feats_list = self.get_feats(que_imgs) # qn,f,h,w\n\t        ref_feats_list = self.ref_feats_cache # an,rfn,f,h,w\n\t        vps_feats, corr_feats = [], []\n\t        for ref_feats, que_feats, corr_conv in zip(ref_feats_list, que_feats_list, self.corr_conv_list):\n", "            ref_feats = ref_feats.permute(1,0,2,3,4) # rfn,an,f,h,w\n\t            feats_corr = que_feats[:, None,None] * ref_feats[None] # qn,rfn,an,f,h,w\n\t            qn, rfn, an, f, h, w = feats_corr.shape\n\t            feats_corr = feats_corr.permute(0,3,1,2,4,5).reshape(qn,f,rfn*an,h,w)\n\t            feats_corr_ = corr_conv(feats_corr)\n\t            _, f_, _, h_, w_ = feats_corr_.shape\n\t            corr_feats.append(feats_corr_.reshape(qn,f_, rfn, an, h_, w_)) # qn,f_,rfn,an,h_,w_\n\t            # vps score feats\n\t            score_maps = torch.sum(feats_corr, 1) # qn,rfn*an,h,w\n\t            score_maps_ = score_maps/(torch.max(score_maps.flatten(2),2)[0][...,None,None]) # qn,rfn*an,h,w\n", "            score_vps = torch.sum(score_maps.flatten(2)*score_maps_.flatten(2),2) # qn,rfn*an\n\t            vps_feats.append(score_vps.reshape(qn,rfn,an))\n\t        corr_feats = torch.cat(corr_feats, 1)  # qn,f_*3,rfn,an,h_,w_\n\t        qn, f, rfn, an, h, w = corr_feats.shape\n\t        corr_feats = self.corr_feats_conv(corr_feats.reshape(qn, f, rfn*an, h, w))[...,0,0] # qn,f,rfn,an\n\t        corr_feats = corr_feats.reshape(qn,corr_feats.shape[1],rfn,an)\n\t        vps_feats = self.vp_norm(torch.stack(vps_feats, 1),) # qn,3,rfn,an\n\t        feats = torch.cat([corr_feats, vps_feats],1) # qn,f+3,rfn,an\n\t        scores_feats = torch.max(self.score_process(feats),3)[0] # qn,512,rfn\n\t        scores_feats = scores_feats + self.ref_pose_embed.T.unsqueeze(0) # qn,512,rfn\n", "        for att, mlp in zip(self.atts, self.mlps):\n\t            msg = att(scores_feats, scores_feats) #\n\t            scores_feats = mlp(torch.cat([scores_feats, msg], 1)) + scores_feats\n\t        logits = self.score_predict(scores_feats)[:,0,:] # qn,rfn\n\t        qn, f, rfn, an = feats.shape\n\t        feats = feats.permute(0,1,3,2).reshape(qn,f*an,rfn)\n\t        angles = self.angle_predict(feats)[:,0,:] # qn,rfn\n\t        return logits, angles\n\t    def forward(self, data):\n\t        ref_imgs = data['ref_imgs'] # an,rfn,3,h,w\n", "        ref_poses = data['ref_imgs_info']['poses']\n\t        object_center = data['object_center']\n\t        object_vert = data['object_vert']\n\t        que_imgs = data['que_imgs_info']['imgs'] # qn,3,h,w\n\t        is_train = 'eval' not in data\n\t        self.extract_ref_feats(ref_imgs, ref_poses, object_center, object_vert, is_train)\n\t        logits, angles = self.compute_view_point_feats(que_imgs)\n\t        return {'ref_vp_logits': logits, 'angles_pr': angles}\n\tif __name__ == \"__main__\":\n\t    mock_data = torch.randn(6,3,128,128)\n", "    default_cfg = {\n\t        'selector_angle_num': 5,\n\t    }\n\t    net = ViewpointSelector(default_cfg)\n\t    out =  net.get_feats(mock_data)\n\t    print(len(out), out[0].shape, out[1].shape, out[2].shape  )"]}
{"filename": "network/cascade_refiner.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchvision\n\timport numpy as np\n\tfrom dataset.database import NormalizedDatabase, normalize_pose, get_object_center, get_diameter, denormalize_pose\n\tfrom network.operator import pose_apply_th, normalize_coords\n\tfrom network.pretrain_models import VGGBNPretrainV3\n\tfrom utils.base_utils import pose_inverse, project_points, color_map_forward, to_cuda, pose_compose\n\tfrom utils.database_utils import look_at_crop, select_reference_img_ids_refinement, normalize_reference_views\n", "from utils.pose_utils import let_me_look_at, compose_sim_pose, pose_sim_to_pose_rigid\n\tfrom utils.imgs_info import imgs_info_to_torch\n\tfrom network.vis_dino_encoder import VitExtractor\n\tfrom loguru import logger\n\tfrom gpu_mem_track import MemTracker\n\tgpu_tracker = MemTracker()\n\tstart = torch.cuda.Event(enable_timing=True)\n\tend = torch.cuda.Event(enable_timing=True)\n\timport time\n\tclass RefineFeatureNet(nn.Module):\n", "    def __init__(self, \\\n\t                 norm_layer='instance',\\\n\t                 use_dino=False,\\\n\t                 use_fpn=True):\n\t        super().__init__()\n\t        if norm_layer == 'instance':\n\t            norm=nn.InstanceNorm2d\n\t        else:\n\t            raise NotImplementedError\n\t        self.conv0 = nn.Sequential(\n", "            nn.Conv2d(256, 64, 3, 1, 1),\n\t            norm(64),\n\t            nn.ReLU(True),\n\t            nn.Conv2d(64, 64, 3, 1, 1),\n\t            norm(64),\n\t        )\n\t        self.conv1 = nn.Sequential(\n\t            nn.Conv2d(512, 256, 3, 1, 1),\n\t            norm(256),\n\t            nn.ReLU(True),\n", "            nn.Conv2d(256, 128, 3, 1, 1),\n\t            norm(128),\n\t        )\n\t        self.use_fpn = use_fpn\n\t        self.use_dino = use_dino\n\t        if self.use_dino:\n\t            self.fuse_conv = nn.Conv2d(in_channels=512, \\\n\t                                       out_channels=128, \\\n\t                                       kernel_size=1,\\\n\t                                       stride=1,\\\n", "                                       padding=0, \\\n\t                                       bias=True)\n\t            self.fuse_conv1 = nn.Conv2d(in_channels=256+384, \\\n\t                            out_channels=256, \\\n\t                            kernel_size=1,\\\n\t                            stride=1,\\\n\t                            padding=0, \\\n\t                            bias=True)  \n\t            self.fuse_conv2 = nn.Conv2d(in_channels=512+384, \\\n\t                                        out_channels=512, \\\n", "                                        kernel_size=1,\\\n\t                                        stride=1,\\\n\t                                        padding=0, \\\n\t                                        bias=True) \n\t            self.fuse_conv3 = nn.Conv2d(in_channels=512+384, \\\n\t                            out_channels=512, \\\n\t                            kernel_size=1,\\\n\t                            stride=1,\\\n\t                            padding=0, \\\n\t                            bias=True) \n", "        for m in self.modules():\n\t            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n\t                nn.init.kaiming_normal(m.weight.data, mode='fan_in')\n\t                if m.bias is not None:\n\t                    m.bias.data.zero_()\n\t        if self.use_dino:\n\t            self.fea_ext =  VitExtractor(model_name='dino_vits8').eval()\n\t            for para in self.fea_ext.parameters():\n\t                para.requires_grad = False\n\t            self.fea_ext.requires_grad_(False) \n", "        self.backbone = VGGBNPretrainV3().eval()\n\t        for para in self.backbone.parameters():\n\t            para.requires_grad = False\n\t        self.img_norm = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t    def forward(self, imgs):\n\t        _,_, h,w = imgs.shape\n\t        imgs = self.img_norm(imgs)\n\t        self.backbone.eval()\n\t        with torch.no_grad():\n\t            x0, x1, x2 = self.backbone(imgs)\n", "            x0 = F.normalize(x0, dim=1)\n\t            x1 = F.normalize(x1, dim=1)\n\t            x2 = F.normalize(x2, dim=1) \n\t        x0 = self.conv0(x0)\n\t        x1 = self.conv1(x1)\n\t        return [x1,x0]\n\tclass RefineVolumeEncodingNet(nn.Module):\n\t    def __init__(self, fea_ch = 128, norm_layer='no_norm'):\n\t        super().__init__()\n\t        if norm_layer == 'instance':\n", "            norm=nn.InstanceNorm3d\n\t        else:\n\t            raise NotImplementedError\n\t        self.mean_embed = nn.Sequential(\n\t            nn.Conv3d(fea_ch*2, fea_ch//2, 3, 1, 1),\n\t            norm(fea_ch//2,),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(fea_ch//2, fea_ch//2, 3, 1, 1)\n\t        )\n\t        self.var_embed = nn.Sequential(\n", "            nn.Conv3d(fea_ch, fea_ch//2, 3, 1, 1),\n\t            norm(fea_ch//2),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(fea_ch//2, fea_ch//2, 3, 1, 1)\n\t        )\n\t        self.conv0 = nn.Sequential(\n\t            nn.Conv3d(fea_ch, fea_ch//2, 3, 1, 1), # 32\n\t            norm(fea_ch//2),\n\t            nn.ReLU(True),\n\t        ) # 32\n", "        self.conv1 = nn.Sequential(\n\t            nn.Conv3d(fea_ch//2, fea_ch, 3, 2, 1),\n\t            norm(fea_ch),\n\t            nn.ReLU(True),\n\t        )\n\t        self.conv2 = nn.Sequential(\n\t            nn.Conv3d(fea_ch, fea_ch, 3, 1, 1),\n\t            norm(fea_ch),\n\t            nn.ReLU(True),\n\t        ) # 16\n", "        self.conv3 = nn.Sequential(\n\t            nn.Conv3d(fea_ch, 2*fea_ch, 3, 2, 1),\n\t            norm(2*fea_ch),\n\t            nn.ReLU(True),\n\t        )\n\t        self.conv4 = nn.Sequential(\n\t            nn.Conv3d(2*fea_ch, 2*fea_ch, 3, 1, 1),\n\t            norm(2*fea_ch),\n\t            nn.ReLU(True),\n\t        )  #8\n", "        self.conv5 = nn.Sequential(\n\t            nn.Conv3d(2*fea_ch, 4*fea_ch, 3, 2, 1),\n\t            norm(4*fea_ch),\n\t            nn.ReLU(True),\n\t            nn.Conv3d(4*fea_ch, 4*fea_ch, 3, 1, 1)\n\t        )\n\t    def forward(self, mean, var):\n\t        x = torch.cat([self.mean_embed(mean),self.var_embed(var)],1)\n\t        x = self.conv0(x)\n\t        x = self.conv2(self.conv1(x))\n", "        x = self.conv4(self.conv3(x))\n\t        x = self.conv5(x)\n\t        return x\n\tdef fc(in_planes, out_planes, relu=True):\n\t    if relu:\n\t        return nn.Sequential(\n\t            nn.Linear(in_planes, out_planes),\n\t            nn.LeakyReLU(0.1, inplace=True))\n\t    else:\n\t        return nn.Linear(in_planes, out_planes)\n", "class RefineRegressor(nn.Module):\n\t    def __init__(self, flatten_len = 4096):\n\t        super().__init__()\n\t        self.fc = nn.Sequential(fc(flatten_len, 512), fc(512, 512))\n\t        self.fcr = nn.Linear(512,4)\n\t        self.fct = nn.Linear(512,2)\n\t        self.fcs = nn.Linear(512,1)\n\t    def forward(self, x):\n\t        x = self.fc(x)\n\t        r = F.normalize(self.fcr(x),dim=1)\n", "        t = self.fct(x)\n\t        s = self.fcs(x)\n\t        return r, t, s\n\tclass Transformer(nn.Module):\n\t    def __init__(self, input_size, output_size, hidden_size, num_layer, nhead=8, dropout=0.1):\n\t        super(Transformer, self).__init__()\n\t        self.linear1 = nn.Linear(input_size, hidden_size)\n\t        self.transformer_encoder = nn.Sequential(*[nn.TransformerEncoderLayer(hidden_size, \\\n\t            nhead=nhead, dropout=dropout, batch_first=True) for _ in range(num_layer)])\n\t        self.linear2 = nn.Linear(hidden_size, output_size)\n", "        self.dropout = nn.Dropout(dropout)\n\t    def forward(self, x, state=None):\n\t        x = self.linear1(self.dropout(x))\n\t        output = self.transformer_encoder(x)\n\t        output = self.linear2(output)\n\t        return output, state\n\tclass VolumeRefiner(nn.Module):\n\t    default_cfg = {\n\t        \"refiner_sample_num\": 32,\n\t    }\n", "    def __init__(self, cfg, upsample=False):\n\t        self.cfg={**self.default_cfg, **cfg}\n\t        super().__init__()\n\t        self.use_dino = self.cfg.get(\"use_dino\", False) \n\t        self.use_fpn = self.cfg.get(\"use_fpn\", False)  \n\t        self.upsample = upsample\n\t        self.feature_net = RefineFeatureNet('instance', self.use_dino, self.use_fpn )\n\t        self.training = self.cfg.get(\"trainng\", False)\n\t        # self.feature_channel = [[128,64], [64,32,16], [64,16,4] ]\n\t        self.feature_channel = [ 128,64 ]\n", "        self.scale_rmin, self.scale_rmax = nn.Parameter(torch.Tensor(1)), nn.Parameter(torch.Tensor(1)) \n\t        self.translation_rmin, self.translation_rmax = nn.Parameter(torch.Tensor(1)), nn.Parameter(torch.Tensor(1))\n\t        self.quan_rmin , self.quan_rmax =  nn.Parameter(torch.Tensor(1)), nn.Parameter(torch.Tensor(1))\n\t        self.stage = len(self.feature_channel)\n\t        self.volume_net = list()\n\t        for idx,f_c in enumerate(self.feature_channel):\n\t            self.volume_net.append(RefineVolumeEncodingNet(self.feature_channel[idx],'instance').cuda())\n\t        self.regressor = [RefineRegressor(flatten_len=4096).cuda(),\\\n\t                          RefineRegressor(flatten_len=16384).cuda(),  ]\n\t        self.ref_database = None\n", "        self.ref_ids = None\n\t        # self.stage = 3\n\t        self.stage = 2\n\t        logger.debug( f\"VolumeRefiner use_dino:{self.use_dino} ,  stage:{self.stage}\" )\n\t    @staticmethod\n\t    def update_state(scale, tranlation, quan):\n\t        self.scale_rmin = min(self.scale_rmin, scale.min() )\n\t        self.scale_rmax = max(self.scale_max, scale.max() )\n\t        self.translation_rmin = min( tranlation.min(),  self.translation_rmin   )\n\t        self.translation_rmax  = max(tranlation.max(), self.translation_rmax  )\n", "        self.quan_rmin = min( self.quan_rmin, quan.min()  )\n\t        self.quan_rmax = max(self.quan_rmax,  quan.max() )\n\t    @staticmethod\n\t    def interpolate_volume_feats(feats, verts, projs, h_in, w_in):\n\t        \"\"\"\n\t        @param feats: b,f,h,w \n\t        @param verts: b,sx,sy,sz,3\n\t        @param projs: b,3,4 : project matric\n\t        @param h_in:  int\n\t        @param w_in:  int\n", "        @return:\n\t        \"\"\"\n\t        b, sx, sy, sz, _ = verts.shape\n\t        b, f, h, w = feats.shape\n\t        R, t = projs[:,:3,:3], projs[:,:3,3:] # b,3,3  b,3,1\n\t        verts = verts.reshape(b,sx*sy*sz,3)\n\t        verts = verts @ R.permute(0, 2, 1) + t.permute(0, 2, 1) #\n\t        depth = verts[:, :, -1:]\n\t        depth[depth < 1e-4] = 1e-4\n\t        verts = verts[:, :, :2] / depth  # [b,sx*sy*sz,2]\n", "        verts = normalize_coords(verts, h_in, w_in) # b,sx*sy*sz,2]\n\t        verts = verts.reshape([b, sx, sy*sz, 2])\n\t        volume_feats = F.grid_sample(feats, verts, mode='nearest', align_corners=False) # b,f,sx,sy*sz\n\t        return volume_feats.reshape(b, f, sx, sy, sz)\n\t    def construct_feature_volume(self, que_imgs_info, ref_imgs_info, \\\n\t                                 poses_in, query_image_feature,\\\n\t                                 ref_image_featrues_list, \\\n\t                                 sample_num = 32, \\\n\t                                 stage_idx = 0):\n\t        \"\"\"_summary_\n", "        Args:\n\t            que_imgs_info (_type_): _description_\n\t            ref_imgs_info (_type_): _description_\n\t            poses_in (_type_): 输入的姿态\n\t            query_image_feature (_type_): feature of query image\n\t            ref_image_featrues: feature of reference images\n\t            sample_num: feature volume sample number\n\t            stage_idx: the index of the cacaded network stage\n\t        Returns:\n\t            _type_: _description_\n", "        \"\"\"\n\t        # build a volume on the unit cube\n\t        sn = (sample_num//2) *  (2**stage_idx)\n\t        device = que_imgs_info['imgs'].device\n\t        vol_coords = torch.linspace(-1, 1, sn, dtype=torch.float32, device=device)\n\t        vol_coords = torch.stack(torch.meshgrid(vol_coords,vol_coords,vol_coords),-1) # sn,sn,sn,3\n\t        vol_coords = vol_coords.reshape(1,sn**3,3)\n\t        rotation = poses_in[:,:3,:3] # qn,3,3\n\t        vol_coords = vol_coords @ rotation # qn,sn**3,3\n\t        qn = poses_in.shape[0]\n", "        vol_coords = vol_coords.reshape(qn, sn, sn, sn, 3)\n\t        # project onto every reference view\n\t        ref_poses = ref_imgs_info['poses'] # qn,rfn,3,4\n\t        ref_Ks = ref_imgs_info['Ks'] # qn,rfn,3,3\n\t        ref_proj = ref_Ks @ ref_poses # qn,rfn,3,4\n\t        vol_feats_mean, vol_feats_std = [], []\n\t        h_in, w_in = ref_imgs_info['imgs'].shape[-2:]\n\t        for qi in range(qn):\n\t            ref_feats = ref_image_featrues_list[qi][stage_idx]    # rfn,f,h,w\n\t            rfn = ref_feats.shape[0]\n", "            vol_coords_cur = vol_coords[qi:qi+1].repeat(rfn,1,1,1,1) # rfn,sx,sy,sz,3\n\t            vol_feats = VolumeRefiner.interpolate_volume_feats(ref_feats, vol_coords_cur, ref_proj[qi], h_in, w_in)\n\t            vol_feats_mean.append(torch.mean(vol_feats, 0))\n\t            vol_feats_std.append(torch.std(vol_feats, 0))\n\t        vol_feats_mean = torch.stack(vol_feats_mean, 0)\n\t        vol_feats_std = torch.stack(vol_feats_std, 0)\n\t        # project onto query view\n\t        h_in, w_in = que_imgs_info['imgs'].shape[-2:]\n\t        que_feats = query_image_feature[stage_idx]\n\t        que_proj = que_imgs_info['Ks_in'] @ que_imgs_info['poses_in']\n", "        # qn,f,sx,sy,sz\n\t        vol_feats_in = VolumeRefiner.interpolate_volume_feats(que_feats, vol_coords, que_proj, h_in, w_in) \n\t        return vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords\n\t    def forward(self, data):\n\t        is_inference = data['inference'] if 'inference' in data else False\n\t        que_imgs_info = data['que_imgs_info'].copy()\n\t        ref_imgs_info = data['ref_imgs_info'].copy()\n\t        if self.upsample:\n\t            refiner_sample_num = int(self.cfg['refiner_sample_num']*1.5) \n\t        else:\n", "            refiner_sample_num = self.cfg['refiner_sample_num']\n\t        # qn,f,dn,h,w   qn,dn\n\t        query_image_feature = self.feature_net(que_imgs_info['imgs']) # qn,f,h,w\n\t        # rotate volume to align with the input pose, but still in the object coordinate\n\t        poses_in = que_imgs_info['poses_in'] # qn,3,4\n\t        qn = poses_in.shape[0]\n\t        ref_image_featrues_list = list(list())\n\t        # gpu_tracker.track()\n\t        for qi in range(qn):\n\t            ref_feats = self.feature_net(ref_imgs_info['imgs'][qi])\n", "            ref_image_featrues_list.append(ref_feats)\n\t        # gpu_tracker.track()\n\t        all_stage_outputs = list()\n\t        for stage_idx in range(self.stage):\n\t            # gpu_tracker.track()\n\t            vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords = \\\n\t            self.construct_feature_volume(\n\t                que_imgs_info,\\\n\t                ref_imgs_info, \\\n\t                poses_in, \\\n", "                query_image_feature, \\\n\t                ref_image_featrues_list , \\\n\t                sample_num = refiner_sample_num, \\\n\t                stage_idx = stage_idx\n\t            ) \n\t            # gpu_tracker.track()\n\t            vol_feats = torch.cat([vol_feats_mean, vol_feats_in ], 1)\n\t            vol_feats = self.volume_net[stage_idx](vol_feats, vol_feats_std)\n\t            vol_feats = vol_feats.flatten(1) # qn, f* 4**3\n\t            #traing the regression model, different levels of vol feature\n", "            rotation, offset, scale = self.regressor[stage_idx](vol_feats)\n\t            outputs = {'rotation': rotation, 'offset': offset, 'scale': scale}\n\t            if not is_inference:\n\t                qn, sx, sy, sz, _ = vol_coords.shape\n\t                grids = pose_apply_th(que_imgs_info['poses_in'],\\\n\t                                      vol_coords.reshape(qn, sx * sy * sz, 3))\n\t                outputs['grids'] = grids\n\t                self.update_state(scale, tranlation, quan)\n\t            all_stage_outputs.append(outputs)\n\t        if not is_inference:\n", "            return all_stage_outputs[-1]\n\t        else:\n\t            return all_stage_outputs[-1]\n\t    def load_ref_imgs(self,ref_database,ref_ids):\n\t        self.ref_database = ref_database\n\t        self.ref_ids = ref_ids\n\t    def refine_que_imgs(self, que_img, que_K, in_pose, size=128, ref_num=6, ref_even=False):\n\t        \"\"\"\n\t        @param que_img:  [h,w,3]\n\t        @param que_K:    [3,3]\n", "        @param in_pose:  [3,4]\n\t        @param size:     int\n\t        @param ref_num:  int\n\t        @param ref_even: bool\n\t        @return:\n\t        \"\"\"\n\t        margin = 0.05\n\t        ref_even_num = min(128, len(self.ref_ids))\n\t        # Normlization: normalize database and input pose,\n\t        ref_database = NormalizedDatabase(self.ref_database) \n", "        # Wrapper: object is in the unit sphere at origin \n\t        in_pose = normalize_pose(in_pose, ref_database.scale, ref_database.offset)\n\t        object_center = get_object_center(ref_database)\n\t        object_diameter = get_diameter(ref_database)\n\t        # warp the query image to look at the object w.r.t input pose\n\t        _, new_f = let_me_look_at(in_pose, que_K, object_center)\n\t        in_dist = np.linalg.norm(pose_inverse(in_pose)[:,3] - object_center)\n\t        in_f = size * (1 - margin) / object_diameter * in_dist\n\t        scale = in_f / new_f\n\t        position = project_points(object_center[None], in_pose, que_K)[0][0]\n", "        que_img_warp, que_K_warp, in_pose_warp, que_pose_rect, H = look_at_crop(\n\t            que_img, que_K, in_pose, position, 0, scale, size, size)\n\t        que_imgs_info = {\n\t            'imgs': color_map_forward(que_img_warp).transpose([2,0,1]),  # 3,h,w\n\t            'Ks_in': que_K_warp.astype(np.float32), # 3,3\n\t            'poses_in': in_pose_warp.astype(np.float32), # 3,4\n\t        }\n\t        # import pdb\n\t        # pdb.set_trace()\n\t        # print( que_imgs_info['imgs'].shape ,  que_imgs_info['Ks_in'].shape  , que_imgs_info['poses_in'].shape )\n", "        # select reference views for refinement\n\t        ref_ids = select_reference_img_ids_refinement(ref_database, object_center, self.ref_ids, \\\n\t                                                      in_pose_warp, ref_num, ref_even, ref_even_num)\n\t        # normalize the reference images and align the in-plane orientation w.r.t input pose.\n\t        ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs = normalize_reference_views(\n\t            ref_database, ref_ids, size, margin, True, in_pose_warp, que_K_warp)\n\t        ref_imgs_info = {\n\t            'imgs': color_map_forward(np.stack(ref_imgs, 0)).transpose([0, 3, 1, 2]),  # rfn,3,h,w\n\t            'poses': np.stack(ref_poses, 0).astype(np.float32),\n\t            'Ks': np.stack(ref_Ks, 0).astype(np.float32),\n", "        }\n\t        que_imgs_info = to_cuda(imgs_info_to_torch(que_imgs_info))\n\t        ref_imgs_info = to_cuda(imgs_info_to_torch(ref_imgs_info))\n\t        for k,v in que_imgs_info.items(): que_imgs_info[k] = v.unsqueeze(0)\n\t        for k,v in ref_imgs_info.items(): ref_imgs_info[k] = v.unsqueeze(0)\n\t        with torch.no_grad():\n\t            outputs = self.forward({'que_imgs_info': que_imgs_info, \\\n\t                                    'ref_imgs_info': ref_imgs_info, \\\n\t                                    'inference': True})\n\t            quat = outputs['rotation'].detach().cpu().numpy()[0] # 4\n", "            scale = 2**outputs['scale'].detach().cpu().numpy()[0] # 1\n\t            offset = outputs['offset'].detach().cpu().numpy()[0] # 2\n\t        # compose rotation/scale/offset into a similarity transformation matrix\n\t        pose_sim = compose_sim_pose(scale, quat, offset, in_pose_warp, object_center)\n\t        # convert the similarity transformation to the rigid transformation\n\t        pose_pr = pose_sim_to_pose_rigid(pose_sim, in_pose_warp, que_K_warp, que_K_warp, object_center)\n\t        # apply the pose residual\n\t        pose_pr = pose_compose(pose_pr, pose_inverse(que_pose_rect))\n\t        # convert back to original coordinate system (because we use NormalizedDatabase to wrap the input)\n\t        pose_pr = denormalize_pose(pose_pr, ref_database.scale, ref_database.offset)\n", "        return pose_pr\n\tif __name__ == \"__main__\":\n\t    from utils.base_utils import load_cfg\n\t    cfg = \"configs/refiner/refiner_pretrain.yaml\"\n\t    refiner_cfg = load_cfg(cfg)\n\t    refiner = VolumeRefiner(refiner_cfg)\n\t    refiner_sample_num = 32\n\t    ref_imgs_info = {\n\t        'imgs': torch.randn(6,3,128,128) , # rfn,3,h,w\n\t        'poses': torch.randn(6, 3, 4),\n", "        'Ks': torch.randn(6,3,3),\n\t    }\n\t    que_imgs_info = {\n\t        'imgs': torch.randn(3,128,128),  # 3,h,w\n\t        'Ks_in': torch.randn(3, 3), # 3,3\n\t        'poses_in':  torch.randn(3, 4), # 3,4\n\t    }\n\t    for k,v in que_imgs_info.items(): que_imgs_info[k] = v.unsqueeze(0)\n\t    for k,v in ref_imgs_info.items(): ref_imgs_info[k] = v.unsqueeze(0)\n\t    # unit test1 : refine_que_images\n", "    # pose_pr = refiner.refine_que_imgs(que_img, que_K, pose_pr, size=128, ref_num=6, ref_even=True)\n\t    # vol_feats_mean, vol_feats_std, vol_feats_in, vol_coords = refiner.construct_feature_volume(\n\t    #         que_imgs_info, ref_imgs_info, refiner.feature_net, refiner_sample_num, stage=1)\n\t    # print(vol_feats_mean.shape, vol_feats_std.shape, vol_feats_in.shape, vol_coords.shape)\n\t    data = dict()\n\t    data['que_imgs_info'] = que_imgs_info\n\t    data['ref_imgs_info'] = ref_imgs_info\n\t    refiner(data)"]}
{"filename": "network/vision_transformer.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n\t# \n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t# \n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# \n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport math\n\tfrom functools import partial\n\timport torch\n\timport torch.nn as nn\n\tdef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n\t    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n\t    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n", "    def norm_cdf(x):\n\t        # Computes standard normal cumulative distribution function\n\t        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\t    if (mean < a - 2 * std) or (mean > b + 2 * std):\n\t        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n\t                      \"The distribution of values may be incorrect.\",\n\t                      stacklevel=2)\n\t    with torch.no_grad():\n\t        # Values are generated by using a truncated uniform distribution and\n\t        # then using the inverse CDF for the normal distribution.\n", "        # Get upper and lower cdf values\n\t        l = norm_cdf((a - mean) / std)\n\t        u = norm_cdf((b - mean) / std)\n\t        # Uniformly fill tensor with values from [l, u], then translate to\n\t        # [2l-1, 2u-1].\n\t        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\t        # Use inverse cdf transform for normal distribution to get truncated\n\t        # standard normal\n\t        tensor.erfinv_()\n\t        # Transform to proper mean, std\n", "        tensor.mul_(std * math.sqrt(2.))\n\t        tensor.add_(mean)\n\t        # Clamp to ensure it's in the proper range\n\t        tensor.clamp_(min=a, max=b)\n\t        return tensor\n\tdef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n\t    # type: (Tensor, float, float, float, float) -> Tensor\n\t    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\tdef drop_path(x, drop_prob: float = 0., training: bool = False):\n\t    if drop_prob == 0. or not training:\n", "        return x\n\t    keep_prob = 1 - drop_prob\n\t    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n\t    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n\t    random_tensor.floor_()  # binarize\n\t    output = x.div(keep_prob) * random_tensor\n\t    return output\n\tclass DropPath(nn.Module):\n\t    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n\t    \"\"\"\n", "    def __init__(self, drop_prob=None):\n\t        super(DropPath, self).__init__()\n\t        self.drop_prob = drop_prob\n\t    def forward(self, x):\n\t        return drop_path(x, self.drop_prob, self.training)\n\tclass Mlp(nn.Module):\n\t    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n", "        self.fc1 = nn.Linear(in_features, hidden_features)\n\t        self.act = act_layer()\n\t        self.fc2 = nn.Linear(hidden_features, out_features)\n\t        self.drop = nn.Dropout(drop)\n\t    def forward(self, x):\n\t        x = self.fc1(x)\n\t        x = self.act(x)\n\t        x = self.drop(x)\n\t        x = self.fc2(x)\n\t        x = self.drop(x)\n", "        return x\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n\t        super().__init__()\n\t        self.num_heads = num_heads\n\t        head_dim = dim // num_heads\n\t        self.scale = qk_scale or head_dim ** -0.5\n\t        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(dim, dim)\n", "        self.proj_drop = nn.Dropout(proj_drop)\n\t    def forward(self, x):\n\t        B, N, C = x.shape\n\t        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv[0], qkv[1], qkv[2]\n\t        attn = (q @ k.transpose(-2, -1)) * self.scale\n\t        attn = attn.softmax(dim=-1)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n\t        x = self.proj(x)\n", "        x = self.proj_drop(x)\n\t        return x, attn\n\tclass Block(nn.Module):\n\t    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n\t                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n\t        super().__init__()\n\t        self.norm1 = norm_layer(dim)\n\t        self.attn = Attention(\n\t            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\t        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n", "        self.norm2 = norm_layer(dim)\n\t        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\t    def forward(self, x, return_attention=False):\n\t        y, attn = self.attn(self.norm1(x))\n\t        if return_attention:\n\t            return attn\n\t        x = x + self.drop_path(y)\n\t        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\t        return x\n", "class PatchEmbed(nn.Module):\n\t    \"\"\" Image to Patch Embedding\n\t    \"\"\"\n\t    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n\t        super().__init__()\n\t        num_patches = (img_size // patch_size) * (img_size // patch_size)\n\t        self.img_size = img_size\n\t        self.patch_size = patch_size\n\t        self.num_patches = num_patches\n\t        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n", "    def forward(self, x):\n\t        B, C, H, W = x.shape\n\t        x = self.proj(x).flatten(2).transpose(1, 2)\n\t        return x\n\tclass VisionTransformer(nn.Module):\n\t    \"\"\" Vision Transformer \"\"\"\n\t    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n\t                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n\t                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n\t        super().__init__()\n", "        self.num_features = self.embed_dim = embed_dim\n\t        self.patch_embed = PatchEmbed(\n\t            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n\t        num_patches = self.patch_embed.num_patches\n\t        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\t        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n\t        self.pos_drop = nn.Dropout(p=drop_rate)\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n\t        self.blocks = nn.ModuleList([\n\t            Block(\n", "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n\t            for i in range(depth)])\n\t        self.norm = norm_layer(embed_dim)\n\t        # Classifier head\n\t        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\t        trunc_normal_(self.pos_embed, std=.02)\n\t        trunc_normal_(self.cls_token, std=.02)\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n", "        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def interpolate_pos_encoding(self, x, w, h):\n\t        npatch = x.shape[1] - 1\n\t        N = self.pos_embed.shape[1] - 1\n", "        if npatch == N and w == h:\n\t            return self.pos_embed\n\t        class_pos_embed = self.pos_embed[:, 0]\n\t        patch_pos_embed = self.pos_embed[:, 1:]\n\t        dim = x.shape[-1]\n\t        w0 = w // self.patch_embed.patch_size\n\t        h0 = h // self.patch_embed.patch_size\n\t        # we add a small number to avoid floating point error in the interpolation\n\t        # see discussion at https://github.com/facebookresearch/dino/issues/8\n\t        w0, h0 = w0 + 0.1, h0 + 0.1\n", "        patch_pos_embed = nn.functional.interpolate(\n\t            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n\t            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n\t            mode='bicubic',\n\t        )\n\t        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n\t        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n\t        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n\t    def prepare_tokens(self, x):\n\t        B, nc, w, h = x.shape\n", "        x = self.patch_embed(x)  # patch linear embedding\n\t        # add the [CLS] token to the embed patch tokens\n\t        cls_tokens = self.cls_token.expand(B, -1, -1)\n\t        x = torch.cat((cls_tokens, x), dim=1)\n\t        # add positional encoding to each token\n\t        x = x + self.interpolate_pos_encoding(x, w, h)\n\t        return self.pos_drop(x)\n\t    def forward(self, x):\n\t        x = self.prepare_tokens(x)\n\t        for blk in self.blocks:\n", "            x = blk(x)\n\t        x = self.norm(x)\n\t        return x[:, 0]\n\t    def get_last_selfattention(self, x):\n\t        x = self.prepare_tokens(x)\n\t        for i, blk in enumerate(self.blocks):\n\t            if i < len(self.blocks) - 1:\n\t                x = blk(x)\n\t            else:\n\t                # return attention of the last block\n", "                return blk(x, return_attention=True)\n\t    def get_intermediate_layers(self, x, n=1):\n\t        x = self.prepare_tokens(x)\n\t        # we return the output tokens from the `n` last blocks\n\t        output = []\n\t        for i, blk in enumerate(self.blocks):\n\t            x = blk(x)\n\t            if len(self.blocks) - i <= n:\n\t                output.append(self.norm(x))\n\t        return output\n", "def vit_tiny(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n\t        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\t    return model\n\tdef vit_small(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n\t        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\t    return model\n", "def vit_base(patch_size=16, **kwargs):\n\t    model = VisionTransformer(\n\t        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n\t        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n\t    return model\n\tclass DINOHead(nn.Module):\n\t    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n\t        super().__init__()\n\t        nlayers = max(nlayers, 1)\n\t        if nlayers == 1:\n", "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n\t        else:\n\t            layers = [nn.Linear(in_dim, hidden_dim)]\n\t            if use_bn:\n\t                layers.append(nn.BatchNorm1d(hidden_dim))\n\t            layers.append(nn.GELU())\n\t            for _ in range(nlayers - 2):\n\t                layers.append(nn.Linear(hidden_dim, hidden_dim))\n\t                if use_bn:\n\t                    layers.append(nn.BatchNorm1d(hidden_dim))\n", "                layers.append(nn.GELU())\n\t            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n\t            self.mlp = nn.Sequential(*layers)\n\t        self.apply(self._init_weights)\n\t        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n\t        self.last_layer.weight_g.data.fill_(1)\n\t        if norm_last_layer:\n\t            self.last_layer.weight_g.requires_grad = False\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n", "            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t    def forward(self, x):\n\t        x = self.mlp(x)\n\t        x = nn.functional.normalize(x, dim=-1, p=2)\n\t        x = self.last_layer(x)\n\t        return x\n\tfrom torchvision.models.resnet import resnet50\n\tdef dino_vits16(pretrained=True, **kwargs):\n", "    \"\"\"\n\t    ViT-Small/16x16 pre-trained with DINO.\n\t    Achieves 74.5% top-1 accuracy on ImageNet with k-NN classification.\n\t    \"\"\"\n\t    model = vit_small(patch_size=16, num_classes=0, **kwargs)\n\t    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n\t            url=\"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\",\n\t            map_location=\"cpu\",\n\t        )\n", "        model.load_state_dict(state_dict, strict=True)\n\t    return model\n\tdef dino_vits8(pretrained=True, **kwargs):\n\t    \"\"\"\n\t    ViT-Small/8x8 pre-trained with DINO.\n\t    Achieves 78.3% top-1 accuracy on ImageNet with k-NN classification.\n\t    \"\"\"\n\t    model = vit_small(patch_size=8, num_classes=0, **kwargs)\n\t    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n", "            url=\"https://dl.fbaipublicfiles.com/dino/dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth\",\n\t            map_location=\"cpu\",\n\t        )\n\t        model.load_state_dict(state_dict, strict=True)\n\t    return model\n\tdef dino_vitb16(pretrained=True, **kwargs):\n\t    \"\"\"\n\t    ViT-Base/16x16 pre-trained with DINO.\n\t    Achieves 76.1% top-1 accuracy on ImageNet with k-NN classification.\n\t    \"\"\"\n", "    model = vit_base(patch_size=16, num_classes=0, **kwargs)\n\t    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n\t            url=\"https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\",\n\t            map_location=\"cpu\",\n\t        )\n\t        model.load_state_dict(state_dict, strict=True)\n\t    return model\n\tdef dino_vitb8(pretrained=True, **kwargs):\n\t    \"\"\"\n", "    ViT-Base/8x8 pre-trained with DINO.\n\t    Achieves 77.4% top-1 accuracy on ImageNet with k-NN classification.\n\t    \"\"\"\n\t    model = vit_base(patch_size=8, num_classes=0, **kwargs)\n\t    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n\t            url=\"https://dl.fbaipublicfiles.com/dino/dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\",\n\t            map_location=\"cpu\",\n\t        )\n\t        model.load_state_dict(state_dict, strict=True)\n", "    return model\n\tdef dino_resnet50(pretrained=True, **kwargs):\n\t    \"\"\"\n\t    ResNet-50 pre-trained with DINO.\n\t    Achieves 75.3% top-1 accuracy on ImageNet linear evaluation benchmark (requires to train `fc`).\n\t    \"\"\"\n\t    model = resnet50(pretrained=False, **kwargs)\n\t    model.fc = torch.nn.Identity()\n\t    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n", "            url=\"https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/dino_resnet50_pretrain.pth\",\n\t            map_location=\"cpu\",\n\t        )\n\t        model.load_state_dict(state_dict, strict=False)\n\t    return model\n\tdef dino_xcit_small_12_p16(pretrained=True, **kwargs):\n\t    \"\"\"\n\t    XCiT-Small-12/16 pre-trained with DINO.\n\t    \"\"\"\n\t    model = torch.hub.load('facebookresearch/xcit', \"xcit_small_12_p16\", num_classes=0, **kwargs)\n", "    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n\t            url=\"https://dl.fbaipublicfiles.com/dino/dino_xcit_small_12_p16_pretrain/dino_xcit_small_12_p16_pretrain.pth\",\n\t            map_location=\"cpu\",\n\t        )\n\t        model.load_state_dict(state_dict, strict=True)\n\t    return model\n\tdef dino_xcit_small_12_p8(pretrained=True, **kwargs):\n\t    \"\"\"\n\t    XCiT-Small-12/8 pre-trained with DINO.\n", "    \"\"\"\n\t    model = torch.hub.load('facebookresearch/xcit', \"xcit_small_12_p8\", num_classes=0, **kwargs)\n\t    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n\t            url=\"https://dl.fbaipublicfiles.com/dino/dino_xcit_small_12_p8_pretrain/dino_xcit_small_12_p8_pretrain.pth\",\n\t            map_location=\"cpu\",\n\t        )\n\t        model.load_state_dict(state_dict, strict=True)\n\t    return model\n\tdef dino_xcit_medium_24_p16(pretrained=True, **kwargs):\n", "    \"\"\"\n\t    XCiT-Medium-24/16 pre-trained with DINO.\n\t    \"\"\"\n\t    model = torch.hub.load('facebookresearch/xcit', \"xcit_medium_24_p16\", num_classes=0, **kwargs)\n\t    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n\t            url=\"https://dl.fbaipublicfiles.com/dino/dino_xcit_medium_24_p16_pretrain/dino_xcit_medium_24_p16_pretrain.pth\",\n\t            map_location=\"cpu\",\n\t        )\n\t        model.load_state_dict(state_dict, strict=True)\n", "    return model\n\tdef dino_xcit_medium_24_p8(pretrained=True, **kwargs):\n\t    \"\"\"\n\t    XCiT-Medium-24/8 pre-trained with DINO.\n\t    \"\"\"\n\t    model = torch.hub.load('facebookresearch/xcit', \"xcit_medium_24_p8\", num_classes=0, **kwargs)\n\t    if pretrained:\n\t        state_dict = torch.hub.load_state_dict_from_url(\n\t            url=\"https://dl.fbaipublicfiles.com/dino/dino_xcit_medium_24_p8_pretrain/dino_xcit_medium_24_p8_pretrain.pth\",\n\t            map_location=\"cpu\",\n", "        )\n\t        model.load_state_dict(state_dict, strict=True)\n\t    return model"]}
{"filename": "network/dino_detector.py", "chunked_list": ["import os\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchvision\n\tfrom utils.base_utils import color_map_forward, transformation_crop, to_cpu_numpy\n\tfrom utils.bbox_utils import parse_bbox_from_scale_offset\n\tfrom network.module import (FPNDecoderV2,VITDecoderStage4,torch_init_model)\n\timport network.dino_transformer as vits\n", "from loguru import logger\n\tfrom math import ceil\n\tfrom network.pretrain_models import VGGBNPretrain\n\tdef disable_bn_grad(input_module):\n\t    for module in input_module.modules():\n\t        if isinstance(module, nn.BatchNorm2d):\n\t            if hasattr(module, 'weight'):\n\t                module.weight.requires_grad_(False)\n\t            if hasattr(module, 'bias'):\n\t                module.bias.requires_grad_(False)\n", "def disable_bn_track(input_module):\n\t    for module in input_module.modules():\n\t        if isinstance(module, nn.BatchNorm2d):\n\t            module.eval()\n\tclass BaseDetector(nn.Module):\n\t    def load_impl(self, ref_imgs):\n\t        raise NotImplementedError\n\t    def detect_impl(self, que_imgs):\n\t        raise NotImplementedError\n\t    def load(self, ref_imgs):\n", "        ref_imgs = torch.from_numpy(color_map_forward(ref_imgs)).permute(0, 3, 1, 2).cuda()\n\t        self.load_impl(ref_imgs)\n\t    def detect(self, que_imgs):\n\t        que_imgs = torch.from_numpy(color_map_forward(que_imgs)).permute(0, 3, 1, 2).cuda()\n\t        return self.detect_impl(que_imgs) # 'scores' 'select_pr_offset' 'select_pr_scale'\n\t    @staticmethod\n\t    def parse_detect_results(results):\n\t        \"\"\"\n\t        @param results: dict\n\t            pool_ratio: int -- pn\n", "            scores: qn,1,h/pn,w/pn\n\t            select_pr_offset: qn,2,h/pn,w/pn\n\t            select_pr_scale:  qn,1,h/pn,w/pn\n\t            select_pr_angle:  qn,2,h/pn,w/pn # optional\n\t        @return: all numpy ndarray\n\t        \"\"\"\n\t        qn = results['scores'].shape[0]\n\t        pool_ratio = results['pool_ratio']\n\t        # max scores\n\t        _, score_x, score_y = BaseDetector.get_select_index(results['scores']) # qn\n", "        position = torch.stack([score_x, score_y], -1)  # qn,2\n\t        # offset\n\t        offset = results['select_pr_offset'][torch.arange(qn),:,score_y,score_x] # qn,2\n\t        position = position + offset\n\t        # to original coordinate\n\t        position = (position + 0.5) * pool_ratio - 0.5 # qn,2\n\t        # scale\n\t        scale_r2q = results['select_pr_scale'][torch.arange(qn),0,score_y,score_x] # qn\n\t        scale_r2q = 2**scale_r2q\n\t        outputs = {'position': position.detach().cpu().numpy(), 'scale_r2q': scale_r2q.detach().cpu().numpy()}\n", "        # rotation\n\t        if 'select_pr_angle' in results:\n\t            angle_r2q = results['select_pr_angle'][torch.arange(qn),:,score_y,score_x] # qn,2\n\t            angle = torch.atan2(angle_r2q[:,1],angle_r2q[:,0])\n\t            outputs['angle_r2q'] = angle.cpu().numpy() # qn\n\t        return outputs\n\t    @staticmethod\n\t    def detect_results_to_bbox(dets, length):\n\t        pos = dets['position'] # qn,2\n\t        length = dets['scale_r2q'] * length # qn,\n", "        length = length[:,None]\n\t        begin = pos - length/2\n\t        return np.concatenate([begin,length,length],1)\n\t    @staticmethod\n\t    def detect_results_to_image_region(imgs, dets, region_len):\n\t        qn = len(imgs)\n\t        img_regions = []\n\t        for qi in range(qn):\n\t            pos = dets['position'][qi]; scl_r2q = dets['scale_r2q'][qi]\n\t            ang_r2q = dets['angle_r2q'][qi] if 'anlge_r2q' in dets else 0\n", "            img = imgs[qi]\n\t            img_region, _ = transformation_crop(img, pos, 1/scl_r2q, -ang_r2q, region_len)\n\t            img_regions.append(img_region)\n\t        return img_regions\n\t    @staticmethod\n\t    def get_select_index(scores):\n\t        \"\"\"\n\t        @param scores: qn,rfn or 1,hq,wq\n\t        @return: qn\n\t        \"\"\"\n", "        qn, rfn, hq, wq = scores.shape\n\t        # 取最大值\n\t        select_id = torch.argmax(scores.flatten(1), 1)\n\t        select_ref_id = select_id // (hq * wq)\n\t        select_h_id = (select_id - select_ref_id * hq * wq) // wq\n\t        select_w_id = select_id - select_ref_id * hq * wq - select_h_id * wq\n\t        return select_ref_id, select_w_id, select_h_id\n\t    @staticmethod\n\t    def parse_detection(scores, scales, offsets, pool_ratio):\n\t        \"\"\"\n", "        @param scores:    qn,1,h/8,w/8\n\t        @param scales:    qn,1,h/8,w/8\n\t        @param offsets:   qn,2,h/8,w/8\n\t        @param pool_ratio:int\n\t        @return: position in x_cur\n\t        \"\"\"\n\t        qn, _, _, _ = offsets.shape\n\t        _, score_x, score_y = BaseDetector.get_select_index(scores) # qn\n\t        positions = torch.stack([score_x, score_y], -1)  # qn,2\n\t        offset = offsets[torch.arange(qn),:,score_y,score_x] # qn,2\n", "        positions = positions + offset\n\t        # to original coordinate\n\t        positions = (positions + 0.5) * pool_ratio - 0.5 # qn,2\n\t        # scale\n\t        scales = scales[torch.arange(qn),0,score_y,score_x] # qn\n\t        scales = 2**scales\n\t        return positions, scales # [qn,2] [qn]\n\tclass Detector(BaseDetector):\n\t    default_cfg={\n\t        \"score_stats\": [[5000,30000],[5000,2000],[700,400]],\n", "        \"vgg_score_max\": 20,\n\t        \"detection_scales\": [-1.0,-0.5,0.0,0.5],\n\t        \"train_feats\": False,\n\t    }\n\t    def __init__(self, cfg):\n\t        self.cfg={**self.default_cfg,**cfg}\n\t        super().__init__()\n\t        self.use_dino = self.cfg.get(\"use_dino\", True)\n\t        logger.info(f\"Detector use_dino: {self.use_dino}\")\n\t        self.backbone = VGGBNPretrain()\n", "        if self.use_dino:\n\t            self.vit_args =  {\n\t                \"twin\": False,\n\t                \"rescale\": 0.5,\n\t                \"do_vit\": True,\n\t                \"patch_size\": 16,\n\t                \"qk_scale\": \"default\",\n\t                \"vit_arch\": \"vit_small\",\n\t                \"vit_path\": \"checkpoints/dino_deitsmall16_pretrain.pth\",\n\t                \"vit_ch\": 384,\n", "                \"out_ch\": 64,\n\t                \"att_fusion\": True,\n\t                \"nhead\": 6\n\t            }\n\t            self.vit = vits.__dict__[self.vit_args['vit_arch']](patch_size=self.vit_args['patch_size'],\n\t                                                                qk_scale=self.vit_args['qk_scale'])\n\t            if os.path.exists(self.vit_args['vit_path']):\n\t                state_dict = torch.load(self.vit_args['vit_path'], map_location='cpu')\n\t                if self.vit_args['vit_path'].split('/')[-1] == 'model_best.pth' and 'state_dict' in state_dict:\n\t                    state_dict_ = state_dict['state_dict']\n", "                    state_dict = {}\n\t                    for k in state_dict_:\n\t                        if k.startswith('vit.'):\n\t                            state_dict[k.replace('vit.', '')] = state_dict_[k]\n\t                torch_init_model(self.vit, state_dict, key='model')\n\t            self.decoder_vit = VITDecoderStage4(self.vit_args)\n\t        self.pool_ratio = 8\n\t        self.img_norm = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t        self.decoder = FPNDecoderV2(feat_chs=[8,16,32,64,128,256,512])\n\t        d = 64\n", "        self.score_conv = nn.Sequential(\n\t            nn.Conv3d(3*len(self.cfg['detection_scales']),d,1,1),\n\t            nn.ReLU(),\n\t            nn.Conv3d(d,d,1,1),\n\t        )\n\t        self.score_predict = nn.Sequential(\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n", "            nn.Conv2d(d,1,3,1,1),\n\t        )\n\t        self.scale_predict = nn.Sequential(\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,1,3,1,1),\n\t        )\n\t        self.offset_predict = nn.Sequential(\n", "            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,d,3,1,1),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(d,2,3,1,1),\n\t        )\n\t        self.ref_center_feats=None\n\t        self.ref_shape = None\n\t    def extract_feats(self, imgs):\n\t        n,c,h,w = imgs.shape\n", "        imgs = self.img_norm(imgs)\n\t        if self.cfg['train_feats']:\n\t            disable_bn_track(self.backbone)\n\t            conv21, conv31, conv41 = self.backbone(imgs)\n\t        else:\n\t            self.backbone.eval()\n\t            with torch.no_grad():\n\t                conv21, conv31, conv41 = self.backbone(imgs)   \n\t        vit_h, vit_w = int(h * self.vit_args['rescale']), int(w * self.vit_args['rescale'])\n\t        vit_imgs = F.interpolate(imgs, (vit_h, vit_w), mode='bicubic', align_corners=False)\n", "        with torch.no_grad():\n\t            vit_feat, vit_att = self.vit.forward_with_last_att(vit_imgs)\n\t        vit_feat = vit_feat[:, 1:].reshape(n * 1, vit_h // self.vit_args['patch_size'],\\\n\t                                                  vit_w // self.vit_args['patch_size'], \\\n\t                                                  self.vit_args['vit_ch']).permute(0, 3, 1, 2).contiguous()  \n\t        vit_att = vit_att[:, :, 0, 1:].reshape(n * 1, -1, \\\n\t                                              vit_h // self.vit_args['patch_size'], \\\n\t                                              vit_w // self.vit_args['patch_size'])\n\t        # 1/32  1/16 1/8\n\t        vit_out1, vit_out2, vit_out3 = self.decoder_vit.forward(vit_feat, vit_att)\n", "        feat1, feat2, feat3 = self.decoder.forward(conv21, conv31, conv41, vit_out1, vit_out2, vit_out3)\n\t        return feat1, feat2, feat3\n\t    def load_impl(self, ref_imgs):\n\t        ref_imgs = F.interpolate(ref_imgs,size=(120,120))\n\t        self.ref_center_feats = self.extract_feats(ref_imgs)\n\t        rfn, _, h, w = ref_imgs.shape\n\t        self.ref_shape = [h, w]\n\t    def normalize_scores(self,scores0,scores1,scores2):\n\t        stats = self.cfg['score_stats']\n\t        scores0 = (scores0 - stats[0][0])/stats[0][1]\n", "        scores1 = (scores1 - stats[1][0])/stats[1][1]\n\t        scores2 = (scores2 - stats[2][0])/stats[2][1]\n\t        return scores0, scores1, scores2\n\t    def get_scores(self, que_imgs):\n\t        que_x0, que_x1, que_x2 = self.extract_feats(que_imgs)\n\t        ref_x0, ref_x1, ref_x2 = self.ref_center_feats # rfn,f,hr,wr\n\t        scores2 = F.conv2d(que_x2, ref_x2, padding=1)\n\t        scores1 = F.conv2d(que_x1, ref_x1, padding=3)\n\t        scores0 = F.conv2d(que_x0, ref_x0, padding=7)\n\t        scores2 = F.interpolate(scores2, scale_factor=4)\n", "        scores1 = F.interpolate(scores1, scale_factor=2) \n\t        scores0, scores1, scores2 = self.normalize_scores(scores0, scores1, scores2)\n\t        scores = torch.stack([scores0, scores1, scores2],1)\n\t        return scores\n\t    def pad_tensor(self, x, block_size=32):\n\t        b, c, h, w = x.size()\n\t        if h % block_size == 0:\n\t            min_height = h\n\t        else:\n\t            min_height = (h // block_size + 1) * block_size\n", "        if w % block_size == 0:\n\t            min_width = w\n\t        else:\n\t            min_width = (w // block_size + 1) * block_size\n\t        padding = (\n\t            0, min_width - w, \n\t            0, min_height - h,\n\t            0, 0,\n\t            0, 0,\n\t        )\n", "        x = F.pad(x, padding)\n\t        return x\n\t    def detect_impl(self, que_imgs):\n\t        qn, _, hq, wq = que_imgs.shape\n\t        hs, ws = hq // 8, wq // 8\n\t        scores = []\n\t        for scale in self.cfg['detection_scales']:\n\t            ht, wt = int(np.round(hq*2**scale)), int(np.round(wq*2**scale))\n\t            if ht%32!=0: ht=(ht//32+1)*32\n\t            if wt%32!=0: wt=(wt//32+1)*32\n", "            que_imgs_cur = F.interpolate(que_imgs,size=(ht,wt),mode='bilinear')\n\t            scores_cur = self.get_scores(que_imgs_cur)\n\t            qn, _, rfn, hcs, wcs = scores_cur.shape\n\t            scores.append(F.interpolate(scores_cur.reshape(qn,3*rfn,hcs,wcs),size=(hs,ws),mode='bilinear').reshape(qn,3,rfn,hs,ws))\n\t        scores = torch.cat(scores, 1) # qn,sn*3,rfn,hq/8,wq/8\n\t        scores = self.score_conv(scores)\n\t        scores_feats = torch.max(scores,2)[0] # qn,f,hq/8,wq/8\n\t        scores = self.score_predict(scores_feats) # qn,1,hq/8,wq/8\n\t        # predict offset and bbox\n\t        _, select_w_id, select_h_id = self.get_select_index(scores)\n", "        que_select_id = torch.stack([select_w_id, select_h_id],1) # qn, 2\n\t        select_offset = self.offset_predict(scores_feats)  # qn,1,hq/8,wq/8\n\t        select_scale = self.scale_predict(scores_feats) # qn,1,hq/8,wq/8\n\t        outputs = {\n\t                   'scores': scores,\\\n\t                   'que_select_id': que_select_id, \\\n\t                   'pool_ratio': self.pool_ratio, \\\n\t                   'select_pr_offset': select_offset,\\\n\t                   'select_pr_scale': select_scale\n\t                }\n", "        return outputs\n\t    def forward(self, data):\n\t        ref_imgs_info = data['ref_imgs_info'].copy()\n\t        que_imgs_info = data['que_imgs_info'].copy()\n\t        ref_imgs = ref_imgs_info['imgs']\n\t        self.load_impl(ref_imgs)\n\t        outputs = self.detect_impl(que_imgs_info['imgs'])\n\t        return outputs\n\t    def load_ref_imgs(self, ref_imgs):\n\t        \"\"\"\n", "        @param ref_imgs: [an,rfn,h,w,3] in numpy\n\t        @return:\n\t        \"\"\"\n\t        ref_imgs = torch.from_numpy(color_map_forward(ref_imgs)).permute(0,3,1,2) # rfn,3,h,w\n\t        ref_imgs = ref_imgs.cuda()\n\t        rfn, _, h, w = ref_imgs.shape\n\t        self.load_impl(ref_imgs)\n\t    def detect_que_imgs(self, que_imgs):\n\t        \"\"\"\n\t        @param que_imgs: [qn,h,w,3]\n", "        @return:\n\t        \"\"\"\n\t        que_imgs = torch.from_numpy(color_map_forward(que_imgs)).permute(0,3,1,2).cuda()\n\t        qn, _, h, w = que_imgs.shape\n\t        outputs = self.detect_impl(que_imgs)\n\t        positions, scales = self.parse_detection(\n\t                            outputs['scores'].detach(), \n\t                            outputs['select_pr_scale'].detach(),\n\t                            outputs['select_pr_offset'].detach(), \n\t                            self.pool_ratio)\n", "        detection_results = {'positions': positions, 'scales': scales}\n\t        detection_results = to_cpu_numpy(detection_results)\n\t        return detection_results\n\tif __name__ == \"__main__\":\n\t    mock_data = torch.randn(6,3,128,128)\n\t    default_cfg = {\n\t        'selector_angle_num': 5,\n\t    }\n\t    net = Detector(default_cfg)\n\t    out =  net.extract_feats(mock_data)\n", "    print(len(out), out[0].shape, out[1].shape, out[2].shape  )"]}
{"filename": "dataset/database.py", "chunked_list": ["import abc\n\timport glob\n\tfrom pathlib import Path\n\timport os\n\timport cv2\n\timport numpy as np\n\timport plyfile\n\tfrom PIL import Image\n\tfrom skimage.io import imread, imsave\n\tfrom utils.base_utils import read_pickle, save_pickle, pose_compose, load_point_cloud, pose_inverse, resize_img, \\\n", "    mask_depth_to_pts, transform_points_pose\n\tfrom utils.read_write_model import read_model\n\tSUN_IMAGE_ROOT = 'data/SUN2012pascalformat/JPEGImages'\n\tSUN_IMAGE_ROOT_128 = 'data/SUN2012pascalformat/JPEGImages_128'\n\tSUN_IMAGE_ROOT_256 = 'data/SUN2012pascalformat/JPEGImages_256'\n\tSUN_IMAGE_ROOT_512 = 'data/SUN2012pascalformat/JPEGImages_512'\n\tSUN_IMAGE_ROOT_32 = 'data/SUN2012pascalformat/JPEGImages_64'\n\tdef get_SUN397_image_fn_list():\n\t    if Path('data/SUN397_list.pkl').exists():\n\t        return read_pickle('data/SUN397_list.pkl')\n", "    img_list = os.listdir(SUN_IMAGE_ROOT)\n\t    img_list = [img for img in img_list if img.endswith('.jpg')]\n\t    save_pickle(img_list, 'data/SUN397_list.pkl')\n\t    return img_list\n\tclass BaseDatabase(abc.ABC):\n\t    def __init__(self, database_name):\n\t        self.database_name = database_name\n\t    @abc.abstractmethod\n\t    def get_image(self, img_id):\n\t        pass\n", "    @abc.abstractmethod\n\t    def get_K(self, img_id):\n\t        pass\n\t    @abc.abstractmethod\n\t    def get_pose(self, img_id):\n\t        pass\n\t    @abc.abstractmethod\n\t    def get_img_ids(self):\n\t        pass\n\t    def get_mask(self,img_id):\n", "        # dummy mask\n\t        img = self.get_image(img_id)\n\t        h, w = img.shape[:2]\n\t        return np.ones([h,w],np.bool)\n\tLINEMOD_ROOT='data/LINEMOD'\n\tclass LINEMODDatabase(BaseDatabase):\n\t    K=np.array([[572.4114, 0., 325.2611],\n\t               [0., 573.57043, 242.04899],\n\t               [0., 0., 1.]], dtype=np.float32)\n\t    def __init__(self, database_name):\n", "        super().__init__(database_name)\n\t        _, self.model_name = database_name.split('/')\n\t        # 首先是图片的images\n\t        self.img_ids = [str(k) for k in range(len(os.listdir(f'{LINEMOD_ROOT}/{self.model_name}/JPEGImages')))]\n\t        self.model = self.get_ply_model().astype(np.float32)\n\t        self.object_center = np.zeros(3,dtype=np.float32)\n\t        self.object_vert = np.asarray([0,0,1],np.float32)\n\t        self.img_id2depth_range = {}\n\t        self.img_id2pose = {}\n\t    def get_ply_model(self):\n", "        fn = Path(f'{LINEMOD_ROOT}/{self.model_name}/{self.model_name}.pkl')\n\t        if fn.exists(): return read_pickle(str(fn))\n\t        ply = plyfile.PlyData.read(f'{LINEMOD_ROOT}/{self.model_name}/{self.model_name}.ply')\n\t        data = ply.elements[0].data\n\t        x = data['x']\n\t        y = data['y']\n\t        z = data['z']\n\t        model = np.stack([x, y, z], axis=-1)\n\t        # 从里面获取4096个点\n\t        if model.shape[0]>4096:\n", "            idxs = np.arange(model.shape[0])\n\t            np.random.shuffle(idxs)\n\t            model = model[idxs[:4096]]\n\t        save_pickle(model, str(fn))\n\t        return model\n\t    def get_image(self, img_id):\n\t        return imread(f'{LINEMOD_ROOT}/{self.model_name}/JPEGImages/{int(img_id):06}.jpg')\n\t    def get_K(self, img_id):\n\t        return np.copy(self.K)\n\t    def get_pose(self, img_id):\n", "        if img_id in self.img_id2pose:\n\t            return self.img_id2pose[img_id]\n\t        else:\n\t            pose = np.load(f'{LINEMOD_ROOT}/{self.model_name}/pose/pose{int(img_id)}.npy')\n\t            self.img_id2pose[img_id] = pose\n\t            return pose\n\t    def get_img_ids(self):\n\t        return self.img_ids.copy()\n\t    def get_mask(self, img_id):\n\t        return np.sum(imread(f'{LINEMOD_ROOT}/{self.model_name}/mask/{int(img_id):04}.png'),-1)>0\n", "GenMOP_ROOT='data/GenMOP'\n\tgenmop_meta_info={\n\t    'cup': {'gravity': np.asarray([-0.0893124,-0.399691,-0.912288]), 'forward': np.asarray([-0.009871,0.693020,-0.308549],np.float32)},\n\t    'tformer': {'gravity': np.asarray([-0.0734401,-0.633415,-0.77032]), 'forward': np.asarray([-0.121561, -0.249061, 0.211048],np.float32)},\n\t    'chair': {'gravity': np.asarray((0.111445, -0.373825, -0.920779),np.float32), 'forward': np.asarray([0.788313,-0.139603,0.156288],np.float32)},\n\t    'knife': {'gravity': np.asarray((-0.0768299, -0.257446, -0.963234),np.float32), 'forward': np.asarray([0.954157,0.401808,-0.285027],np.float32)},\n\t    'love': {'gravity': np.asarray((0.131457, -0.328559, -0.93529),np.float32), 'forward': np.asarray([-0.045739,-1.437427,0.497225],np.float32)},\n\t    'plug_cn': {'gravity': np.asarray((-0.0267497, -0.406514, -0.913253),np.float32), 'forward': np.asarray([-0.172773,-0.441210,0.216283],np.float32)},\n\t    'plug_en': {'gravity': np.asarray((0.0668682, -0.296538, -0.952677),np.float32), 'forward': np.asarray([0.229183,-0.923874,0.296636],np.float32)},\n\t    'miffy': {'gravity': np.asarray((-0.153506, -0.35346, -0.922769),np.float32), 'forward': np.asarray([-0.584448,-1.111544,0.490026],np.float32)},\n", "    'scissors': {'gravity': np.asarray((-0.129767, -0.433414, -0.891803),np.float32), 'forward': np.asarray([1.899760,0.418542,-0.473156],np.float32)},\n\t    'piggy': {'gravity': np.asarray((-0.122392, -0.344009, -0.930955), np.float32), 'forward': np.asarray([0.079012,1.441836,-0.524981], np.float32)},\n\t}\n\tclass GenMOPMetaInfoWrapper:\n\t    def __init__(self, object_name):\n\t        self.object_name = object_name\n\t        self.gravity = genmop_meta_info[self.object_name]['gravity']\n\t        self.forward = genmop_meta_info[self.object_name]['forward']\n\t        self.object_point_cloud = load_point_cloud(f'{GenMOP_ROOT}/{self.object_name}-ref/object_point_cloud.ply')\n\t        # rotate\n", "        self.rotation = self.compute_rotation(self.gravity, self.forward)\n\t        self.object_point_cloud = (self.object_point_cloud @ self.rotation.T)\n\t        # scale\n\t        self.scale_ratio = self.compute_normalized_ratio(self.object_point_cloud)\n\t        self.object_point_cloud = self.object_point_cloud * self.scale_ratio\n\t        min_pt = np.min(self.object_point_cloud,0)\n\t        max_pt = np.max(self.object_point_cloud,0)\n\t        self.center = (max_pt + min_pt)/2\n\t        test_fn = f'{GenMOP_ROOT}/{self.object_name}-ref/test-object_point_cloud.ply'\n\t        if Path(test_fn).exists():\n", "            self.test_object_point_cloud = load_point_cloud(test_fn)\n\t    @staticmethod\n\t    def compute_normalized_ratio(pc):\n\t        min_pt = np.min(pc,0)\n\t        max_pt = np.max(pc,0)\n\t        dist = np.linalg.norm(max_pt - min_pt)\n\t        scale_ratio = 2.0 / dist\n\t        return scale_ratio\n\t    def normalize_pose(self, pose):\n\t        R = pose[:3,:3]\n", "        t = pose[:3,3:]\n\t        R = R @ self.rotation.T\n\t        t = self.scale_ratio * t\n\t        return np.concatenate([R,t], 1).astype(np.float32)\n\t    @staticmethod\n\t    def compute_rotation(vert, forward):\n\t        y = np.cross(vert, forward)\n\t        x = np.cross(y, vert)\n\t        vert = vert/np.linalg.norm(vert)\n\t        x = x/np.linalg.norm(x)\n", "        y = y/np.linalg.norm(y)\n\t        R = np.stack([x, y, vert], 0)\n\t        return R\n\tdef parse_colmap_project(cameras, images, img_fns):\n\t    img_id2db_id = {v.name[:-4]:k for k, v in images.items()}\n\t    poses, Ks = {}, {}\n\t    img_ids = [str(k) for k in range(len(img_fns))]\n\t    for img_id in img_ids:\n\t        if img_id not in img_id2db_id: continue\n\t        db_id = img_id2db_id[img_id]\n", "        R = images[db_id].qvec2rotmat()\n\t        t = images[db_id].tvec\n\t        pose = np.concatenate([R,t[:,None]],1).astype(np.float32)\n\t        poses[img_id]=pose\n\t        cam_id = images[db_id].camera_id\n\t        f, cx, cy, _ = cameras[cam_id].params\n\t        Ks[img_id] = np.asarray([[f,0,cx], [0,f,cy], [0,0,1]],np.float32)\n\t    return poses, Ks, img_ids\n\tclass GenMOPDatabase(BaseDatabase):\n\t    def __init__(self, database_name):\n", "        super().__init__(database_name)\n\t        _, seq_name = database_name.split('/') # genmop/object_name-test or genmop/object_name-ref\n\t        # get image filenames\n\t        self.seq_name = seq_name\n\t        self.root = Path(GenMOP_ROOT) / self.seq_name\n\t        img_fns_cache = self.root / 'images_fn_cache.pkl'\n\t        try:\n\t            self.img_fns = read_pickle(str(img_fns_cache))\n\t            # parse colmap project\n\t            cameras, images, points3d = read_model(f'{GenMOP_ROOT}/{seq_name}/colmap-all/colmap_default-colmap_default/sparse/0')\n", "            self.poses, self.Ks, self.img_ids = parse_colmap_project(cameras, images, self.img_fns)\n\t        except:\n\t            print(str(img_fns_cache) + \" not exists\")\n\t        # align test sequence to the reference sequence\n\t        object_name, database_type = seq_name.split('-')\n\t        if database_type=='test':\n\t            scale_ratio, transfer_pose = read_pickle(f'{GenMOP_ROOT}/{seq_name}/align.pkl')\n\t            for img_id in self.get_img_ids():\n\t                pose = self.poses[img_id]\n\t                pose_new = pose_compose(transfer_pose, pose)\n", "                pose_new[:, 3:] *= scale_ratio\n\t                self.poses[img_id] = pose_new\n\t        # normalize object poses by meta info: rotate and scale not offset\n\t        self.meta_info = GenMOPMetaInfoWrapper(object_name)\n\t        self.poses = {img_id: self.meta_info.normalize_pose(self.poses[img_id]) for img_id in self.get_img_ids()}\n\t    def get_image(self, img_id):\n\t        return imread(str(self.root / 'images' / self.img_fns[int(img_id)]))\n\t    def get_K(self, img_id):\n\t        return self.Ks[img_id].copy()\n\t    def get_pose(self, img_id):\n", "        return self.poses[img_id].copy()\n\t    def get_img_ids(self):\n\t        return self.img_ids\n\tclass CustomDatabase(BaseDatabase):\n\t    def __init__(self, database_name):\n\t        super().__init__(database_name)\n\t        self.root = Path(os.path.join('data',database_name))\n\t        self.img_dir = self.root / 'images'\n\t        if (self.root/'img_fns.pkl').exists():\n\t            self.img_fns = read_pickle(str(self.root/'img_fns.pkl'))\n", "        else:\n\t            self.img_fns = [Path(fn).name for fn in glob.glob(str(self.img_dir)+'/*.jpg')]\n\t            save_pickle(self.img_fns, str(self.root/'img_fns.pkl'))\n\t        self.colmap_root = self.root / 'colmap'\n\t        if (self.colmap_root / 'sparse' / '0').exists():\n\t            cameras, images, points3d = read_model(str(self.colmap_root / 'sparse' / '0'))\n\t            self.poses, self.Ks, self.img_ids = parse_colmap_project(cameras, images, self.img_fns)\n\t        else:\n\t            self.img_ids = [str(k) for k in range(len(self.img_fns))]\n\t            self.poses, self.Ks = {}, {}\n", "        if len(self.poses.keys())>0:\n\t            # read meta information to scale and rotate\n\t            directions = np.loadtxt(str(self.root/'meta_info.txt'))\n\t            x = directions[0]\n\t            z = directions[1]\n\t            self.object_point_cloud = load_point_cloud(f'{self.root}/object_point_cloud.ply')\n\t            # rotate\n\t            self.rotation = GenMOPMetaInfoWrapper.compute_rotation(z, x)\n\t            self.object_point_cloud = (self.object_point_cloud @ self.rotation.T)\n\t            # scale\n", "            self.scale_ratio = GenMOPMetaInfoWrapper.compute_normalized_ratio(self.object_point_cloud)\n\t            self.object_point_cloud = self.object_point_cloud * self.scale_ratio\n\t            min_pt = np.min(self.object_point_cloud, 0)\n\t            max_pt = np.max(self.object_point_cloud, 0)\n\t            self.center = (max_pt + min_pt) / 2\n\t            # modify poses\n\t            for k, pose in self.poses.items():\n\t                R = pose[:3, :3]\n\t                t = pose[:3, 3:]\n\t                R = R @ self.rotation.T\n", "                t = self.scale_ratio * t\n\t                self.poses[k] = np.concatenate([R,t], 1).astype(np.float32)\n\t    def get_image(self, img_id):\n\t        return imread(str(self.img_dir/self.img_fns[int(img_id)]))\n\t    def get_K(self, img_id):\n\t        return self.Ks[img_id].copy()\n\t    def get_pose(self, img_id):\n\t        return self.poses[img_id].copy()\n\t    def get_img_ids(self):\n\t        return self.img_ids\n", "def parse_database_name(database_name:str)->BaseDatabase:\n\t    name2database={\n\t        'linemod': LINEMODDatabase,\n\t        'genmop': GenMOPDatabase,\n\t        'custom': CustomDatabase,\n\t        'co3d_resize': Co3DResizeDatabase,\n\t        'shapenet': ShapeNetRenderDatabase,\n\t        'gso': GoogleScannedObjectDatabase,\n\t    }\n\t    database_type = database_name.split('/')[0]\n", "    if database_type in name2database:\n\t        return name2database[database_type](database_name)\n\t    else:\n\t        raise NotImplementedError\n\tdef get_database_split(database, split_name):\n\t    if split_name.startswith('linemod'): # linemod_test or linemod_val\n\t        assert(database.database_name.startswith('linemod'))\n\t        model_name = database.database_name.split('/')[1]\n\t        lines = np.loadtxt(f\"{LINEMOD_ROOT}/{model_name}/test.txt\",dtype=np.str).tolist()\n\t        que_ids, ref_ids = [], []\n", "        for line in lines: que_ids.append(str(int(line.split('/')[-1].split('.')[0])))\n\t        if split_name=='linemod_val': que_ids = que_ids[::10]\n\t        lines = np.loadtxt(f\"{LINEMOD_ROOT}/{model_name}/train.txt\", dtype=np.str).tolist()\n\t        for line in lines: ref_ids.append(str(int(line.split('/')[-1].split('.')[0])))\n\t    elif split_name=='all':\n\t        ref_ids = que_ids = database.get_img_ids()\n\t    else:\n\t        raise NotImplementedError\n\t    return ref_ids, que_ids\n\tdef get_ref_point_cloud(database):\n", "    if isinstance(database, LINEMODDatabase):\n\t        ref_point_cloud = database.model\n\t    elif isinstance(database, GenMOPDatabase):\n\t        ref_point_cloud = database.meta_info.object_point_cloud\n\t    elif isinstance(database, Co3DResizeDatabase) or isinstance(database, GoogleScannedObjectDatabase):\n\t        raise NotImplementedError\n\t    elif isinstance(database, ShapeNetRenderDatabase):\n\t        return database.model_verts\n\t    elif isinstance(database, CustomDatabase):\n\t        ref_point_cloud = database.object_point_cloud\n", "    elif isinstance(database, NormalizedDatabase):\n\t        pc = get_ref_point_cloud(database.database)\n\t        pc = pc * database.scale + database.offset\n\t        return pc\n\t    else:\n\t        raise NotImplementedError\n\t    return ref_point_cloud\n\tdef get_diameter(database):\n\t    if isinstance(database, LINEMODDatabase):\n\t        model_name = database.database_name.split('/')[-1]\n", "        return np.loadtxt(f\"{LINEMOD_ROOT}/{model_name}/distance.txt\") / 100\n\t    elif isinstance(database, GenMOPDatabase):\n\t        return 2.0 # we already align and scale it\n\t    elif isinstance(database, GoogleScannedObjectDatabase):\n\t        return database.object_diameter\n\t    elif isinstance(database, Co3DResizeDatabase):\n\t        raise NotImplementedError\n\t    elif isinstance(database, ShapeNetRenderDatabase):\n\t        return database.object_diameter\n\t    elif isinstance(database, NormalizedDatabase):\n", "        return 2.0\n\t    elif isinstance(database, CustomDatabase):\n\t        return 2.0\n\t    else:\n\t        raise NotImplementedError\n\tdef get_object_center(database):\n\t    if isinstance(database, LINEMODDatabase):\n\t        return database.object_center\n\t    elif isinstance(database, GenMOPDatabase):\n\t        return database.meta_info.center\n", "    elif isinstance(database, GoogleScannedObjectDatabase):\n\t        return database.object_center\n\t    elif isinstance(database, Co3DResizeDatabase):\n\t        raise NotImplementedError\n\t    elif isinstance(database, ShapeNetRenderDatabase):\n\t        return database.object_center\n\t    elif isinstance(database, CustomDatabase):\n\t        return database.center\n\t    elif isinstance(database, NormalizedDatabase):\n\t        return np.zeros(3,dtype=np.float32)\n", "    else:\n\t        raise NotImplementedError\n\tdef get_object_vert(database):\n\t    if isinstance(database, LINEMODDatabase):\n\t        return database.object_vert\n\t    elif isinstance(database, GenMOPDatabase):\n\t        return np.asarray([0,0,1], np.float32)\n\t    elif isinstance(database, GoogleScannedObjectDatabase):\n\t        return database.object_vert\n\t    elif isinstance(database, Co3DResizeDatabase):\n", "        raise NotImplementedError\n\t    elif isinstance(database, ShapeNetRenderDatabase):\n\t        return database.object_vert\n\t    elif isinstance(database, CustomDatabase):\n\t        return np.asarray([0,0,1], np.float32)\n\t    else:\n\t        raise NotImplementedError\n\tdef normalize_pose(pose, scale, offset):\n\t    # x_obj_new = x_obj * scale + offset\n\t    R = pose[:3, :3]\n", "    t = pose[:3, 3]\n\t    t_ = R @ -offset + scale * t\n\t    return np.concatenate([R, t_[:,None]], -1).astype(np.float32)\n\tdef denormalize_pose(pose, scale, offset):\n\t    R = pose[:3,:3]\n\t    t = pose[:3, 3]\n\t    t = R @ offset / scale + t/scale\n\t    return np.concatenate([R, t[:, None]], -1).astype(np.float32)\n\tclass GoogleScannedObjectDatabase(BaseDatabase):\n\t    def __init__(self, database_name):\n", "        super().__init__(database_name)\n\t        _, model_name, background_resolution = database_name.split('/')\n\t        background, resolution = background_resolution.split('_')\n\t        assert(background in ['black','white'])\n\t        self.resolution = resolution\n\t        self.background = background\n\t        self.prefix=f'data/google_scanned_objects/{model_name}'\n\t        if self.resolution!='raw':\n\t            resolution = int(self.resolution)\n\t            # cache images\n", "            self.img_cache_prefix = f'data/google_scanned_objects/{model_name}/rgb_{resolution}'\n\t            Path(self.img_cache_prefix).mkdir(exist_ok=True,parents=True)\n\t            for img_id in self.get_img_ids():\n\t                fn = Path(self.img_cache_prefix) / f'{int(img_id):06}.jpg'\n\t                if fn.exists(): continue\n\t                img = imread(f'{self.prefix}/rgb/{int(img_id):06}.png')[:, :, :3]\n\t                img = resize_img(img, resolution / 512)\n\t                imsave(str(fn), img)\n\t            # cache masks\n\t            self.mask_cache_prefix = f'data/google_scanned_objects/{model_name}/mask_{resolution}'\n", "            Path(self.mask_cache_prefix).mkdir(exist_ok=True, parents=True)\n\t            for img_id in self.get_img_ids():\n\t                fn = Path(self.mask_cache_prefix) / f'{int(img_id):06}.png'\n\t                if fn.exists(): continue\n\t                mask = imread(f'{self.prefix}/mask/{int(img_id):06}.png')>0\n\t                mask = mask.astype(np.uint8)\n\t                mask = cv2.resize(mask, (resolution,resolution), interpolation=cv2.INTER_NEAREST)\n\t                cv2.imwrite(str(fn), mask, [cv2.IMWRITE_PNG_COMPRESSION, 9])\n\t        #################compute object center###################\n\t        self.model_name = model_name\n", "        object_center_fn = f'data/google_scanned_objects/{model_name}/object_center.pkl'\n\t        if os.path.exists(object_center_fn):\n\t            self.object_center = read_pickle(object_center_fn)\n\t        else:\n\t            obj_pts = self.get_object_points()\n\t            max_pt, min_pt = np.max(obj_pts,0), np.min(obj_pts,0)\n\t            self.object_center = (max_pt+min_pt)/2\n\t            save_pickle(self.object_center, object_center_fn)\n\t        self.img_id2pose={}\n\t        ################compute object vertical direction############\n", "        vert_dir_fn = f'data/google_scanned_objects/{model_name}/object_vert.pkl'\n\t        if os.path.exists(vert_dir_fn):\n\t            self.object_vert = read_pickle(vert_dir_fn)\n\t        else:\n\t            poses = [self.get_pose(img_id) for img_id in self.get_img_ids()]\n\t            cam_pts = [pose_inverse(pose)[:3, 3] for pose in poses]\n\t            cam_pts_diff = np.asarray(cam_pts) - self.object_center[None,]\n\t            self.object_vert = np.mean(cam_pts_diff, 0)\n\t            save_pickle(self.object_vert, vert_dir_fn)\n\t        #################compute object diameter###################\n", "        object_diameter_fn = f'data/google_scanned_objects/{model_name}/object_diameter.pkl'\n\t        if os.path.exists(object_diameter_fn):\n\t            self.object_diameter = read_pickle(object_diameter_fn)\n\t        else:\n\t            self.object_diameter = self._get_diameter()\n\t            save_pickle(self.object_diameter, object_diameter_fn)\n\t    def get_raw_depth(self, img_id):\n\t        img = Image.open(f'{self.prefix}/depth/{int(img_id):06}.png')\n\t        depth = np.asarray(img, dtype=np.float32) / 1000.0\n\t        mask = imread(f'{self.prefix}/mask/{int(img_id):06}.png')>0\n", "        depth[~mask] = 0\n\t        return depth\n\t    def get_object_points(self):\n\t        fn = f'data/gso_cache/{self.model_name}-pts.pkl'\n\t        if os.path.exists(fn): return read_pickle(fn)\n\t        obj_pts = []\n\t        for img_id in self.get_img_ids():\n\t            pose = self.get_pose(img_id)\n\t            mask = self.get_mask(img_id)\n\t            K = self.get_K(img_id)\n", "            pose_inv = pose_inverse(pose)\n\t            depth = self.get_raw_depth(img_id)\n\t            pts = mask_depth_to_pts(mask, depth, K)\n\t            pts = transform_points_pose(pts, pose_inv)\n\t            idx = np.arange(pts.shape[0])\n\t            np.random.shuffle(idx)\n\t            idx = idx[:1024]\n\t            pts = pts[idx]\n\t            obj_pts.append(pts)\n\t        obj_pts = np.concatenate(obj_pts, 0)\n", "        save_pickle(obj_pts, fn)\n\t        return obj_pts\n\t    def _get_diameter(self):\n\t        obj_pts = self.get_object_points()\n\t        max_pt, min_pt = np.max(obj_pts, 0), np.min(obj_pts, 0)\n\t        return np.linalg.norm(max_pt - min_pt)\n\t    def get_image(self, img_id, ref_mode=False):\n\t        if self.resolution!='raw':\n\t            img = imread(f'{self.img_cache_prefix}/{int(img_id):06}.jpg')[:,:,:3]\n\t            if self.background == 'black':\n", "                mask = self.get_mask(img_id)\n\t                img[~mask]=0\n\t        else:\n\t            img = imread(f'{self.prefix}/rgb/{int(img_id):06}.png')[:,:,:3]\n\t            if self.background=='black':\n\t                mask = imread(f'{self.prefix}/mask/{int(img_id):06}.png')>0\n\t                img[~mask] = 0\n\t        return img\n\t    def get_K(self, img_id):\n\t        K=np.loadtxt(f'{self.prefix}/intrinsics/{int(img_id):06}.txt').reshape([4,4])[:3,:3]\n", "        if self.resolution!='raw':\n\t            ratio = int(self.resolution) / 512\n\t            K = np.diag([ratio,ratio,1.0]) @ K\n\t        return np.copy(K.astype(np.float32))\n\t    def get_pose(self, img_id):\n\t        if img_id in self.img_id2pose:\n\t            return self.img_id2pose[img_id].copy()\n\t        else:\n\t            pose_name = f'{self.prefix}/pose/{int(img_id):06}.txt'\n\t            # print(\"pose_name:\", pose_name)\n", "            try:\n\t                pose = np.loadtxt(pose_name).reshape([4,4])[:3,:]\n\t            except:\n\t                pose = np.ones((4,4))[:3,:]\n\t            R = pose[:3, :3].T\n\t            t = R @ -pose[:3, 3:]\n\t            pose = np.concatenate([R,t],-1)\n\t            self.img_id2pose[img_id] = pose\n\t            return np.copy(pose)\n\t    def get_img_ids(self):\n", "        return [str(img_id) for img_id in range(250)]\n\t    def get_mask(self, img_id):\n\t        if self.resolution!='raw':\n\t            mask = imread(f'{self.mask_cache_prefix}/{int(img_id):06}.png')>0\n\t        else:\n\t            mask=imread(f'{self.prefix}/mask/{int(img_id):06}.png')>0\n\t        return mask\n\tCo3D_ROOT = 'data/co3d'\n\tdef mask2bbox(mask):\n\t    if np.sum(mask)==0:\n", "        return np.asarray([0, 0, 0, 0],np.float32)\n\t    ys, xs = np.nonzero(mask)\n\t    x_min = np.min(xs)\n\t    y_min = np.min(ys)\n\t    x_max = np.max(xs)\n\t    y_max = np.max(ys)\n\t    return np.asarray([x_min, y_min, x_max - x_min, y_max - y_min], np.int32)\n\tclass Co3DResizeDatabase(BaseDatabase):\n\t    def __init__(self, database_name):\n\t        super(Co3DResizeDatabase, self).__init__(database_name)\n", "        _, self.category, self.sequence, sizes = database_name.split('/')\n\t        self.fg_size, self.bg_size = [int(item) for item in sizes.split('_')]\n\t        self._build_resize_database()\n\t    def _build_resize_database(self):\n\t        annotation_fn = Path(f'{Co3D_ROOT}_{self.fg_size}_{self.bg_size}/{self.category}/{self.sequence}/info.pkl')\n\t        root_dir = annotation_fn.parent\n\t        self.image_root = (root_dir / 'images')\n\t        self.mask_root = (root_dir / 'masks')\n\t        if annotation_fn.exists():\n\t            self.Ks, self.poses, self.img_ids, self.ratios = read_pickle(str(annotation_fn))\n", "        else:\n\t            raise NotImplementedError\n\t    def get_image(self, img_id, ref_mode=False):\n\t        return imread(str(self.image_root / f'{img_id}.jpg'))\n\t    def get_K(self, img_id):\n\t        return self.Ks[img_id].copy()\n\t    def get_pose(self, img_id):\n\t        return self.poses[img_id].copy()\n\t    def get_img_ids(self):\n\t        return self.img_ids\n", "    def get_bbox(self, img_id):\n\t        return mask2bbox(self.get_mask(img_id))\n\t    def get_mask(self, img_id):\n\t        return imread(str(self.mask_root / f'{img_id}.png')) > 0\n\tSHAPENET_RENDER_ROOT='data/shapenet/shapenet_render'\n\tclass ShapeNetRenderDatabase(BaseDatabase):\n\t    def __init__(self, database_name):\n\t        super(ShapeNetRenderDatabase, self).__init__(database_name)\n\t        # shapenet/02691156/1ba18539803c12aae75e6a02e772bcee/evenly-32-128\n\t        _, self.category, self.model_name, self.render_setting = database_name.split('/')\n", "        self.render_num = int(self.render_setting.split('-')[1])\n\t        self.object_vert = np.asarray([0,1,0],np.float32)\n\t        self.img_id2camera={}\n\t        cache_fn=Path(f'data/shapenet/shapenet_cache/{self.category}-{self.model_name}-{self.render_setting}.pkl')\n\t        if cache_fn.exists():\n\t            self.img_id2camera=read_pickle(str(cache_fn))\n\t        else:\n\t            for img_id in self.get_img_ids():\n\t                self.get_K(img_id)\n\t            cache_fn.parent.mkdir(parents=True,exist_ok=True)\n", "            save_pickle(self.img_id2camera,str(cache_fn))\n\t        self.model_verts=None\n\t        cache_verts_fn = Path(f'data/shapenet/shapenet_cache/{self.category}-{self.model_name}-{self.render_setting}-verts.pkl')\n\t        if cache_verts_fn.exists():\n\t            self.model_verts, self.object_center, self.object_diameter = read_pickle(str(cache_verts_fn))\n\t        else:\n\t            self.model_verts = self.parse_model_verts()\n\t            min_pt = np.min(self.model_verts, 0)\n\t            max_pt = np.max(self.model_verts, 0)\n\t            self.object_center = (max_pt + min_pt) / 2\n", "            self.object_diameter = np.linalg.norm(max_pt - min_pt)\n\t            save_pickle([self.model_verts, self.object_center, self.object_diameter], str(cache_verts_fn))\n\t    def parse_model_verts(self):\n\t        raise NotImplementedError\n\t        import open3d\n\t        SHAPENET_ROOT='/home/liuyuan/data/ShapeNetCore.v2'\n\t        mesh = open3d.io.read_triangle_mesh(f'{SHAPENET_ROOT}/{self.category}/{self.model_name}/models/model_normalized.obj')\n\t        return np.asarray(mesh.vertices,np.float32)\n\t    def get_image(self, img_id, ref_mode=False):\n\t        try:\n", "            return imread(f'{SHAPENET_RENDER_ROOT}/{self.render_setting}/{self.category}/{self.model_name}/{img_id}.png')[:,:,:3]\n\t        except ValueError:\n\t            print(f'{SHAPENET_RENDER_ROOT}/{self.render_setting}/{self.category}/{self.model_name}/{img_id}.png')\n\t            import ipdb; ipdb.set_trace()\n\t    def get_K(self, img_id):\n\t        if img_id in self.img_id2camera:\n\t            pose, K = self.img_id2camera[img_id]\n\t        else:\n\t            pose, K = read_pickle(f'{SHAPENET_RENDER_ROOT}/{self.render_setting}/{self.category}/{self.model_name}/{img_id}-camera.pkl')\n\t            self.img_id2camera[img_id] = (pose, K)\n", "        return np.copy(K)\n\t    def get_pose(self, img_id):\n\t        if img_id in self.img_id2camera:\n\t            pose, K = self.img_id2camera[img_id]\n\t        else:\n\t            pose, K = read_pickle(f'{SHAPENET_RENDER_ROOT}/{self.render_setting}/{self.category}/{self.model_name}/{img_id}-camera.pkl')\n\t            self.img_id2camera[img_id] = (pose, K)\n\t        return np.copy(pose)\n\t    def get_img_ids(self):\n\t        return [str(k) for k in range(self.render_num)]\n", "    def get_mask(self, img_id):\n\t        mask = imread(f'{SHAPENET_RENDER_ROOT}/{self.render_setting}/{self.category}/{self.model_name}/{img_id}.png')[:,:,3]\n\t        return (mask>0).astype(np.float32)\n\tclass NormalizedDatabase(BaseDatabase):\n\t    def get_image(self, img_id):\n\t        return self.database.get_image(img_id)\n\t    def get_K(self, img_id):\n\t        return self.database.get_K(img_id)\n\t    def get_pose(self, img_id):\n\t        pose = self.database.get_pose(img_id)\n", "        return normalize_pose(pose, self.scale, self.offset)\n\t    def get_img_ids(self):\n\t        return self.database.get_img_ids()\n\t    def get_mask(self, img_id):\n\t        return self.database.get_mask(img_id)\n\t    def __init__(self, database: BaseDatabase):\n\t        super().__init__(\"norm/\" + database.database_name)\n\t        self.database = database\n\t        center = get_object_center(self.database)\n\t        diameter = get_diameter(self.database)\n", "        self.scale = 2/diameter\n\t        self.offset = - self.scale * center"]}
{"filename": "dataset/train_meta_info.py", "chunked_list": ["import random\n\timport os\n\timport numpy as np\n\tfrom dataset.database import Co3D_ROOT\n\tfrom utils.base_utils import read_pickle, save_pickle\n\t####################google scan objects##########################\n\tdef get_gso_split(resolution=128):\n\t    gso_split_pkl = 'data/gso_split.pkl'\n\t    if os.path.exists(gso_split_pkl):\n\t        train_fns, val_fns, test_fns = read_pickle(gso_split_pkl)\n", "    else:\n\t        if os.path.exists('data/google_scanned_objects'):\n\t            sym_fns = np.loadtxt('assets/gso_sym.txt',dtype=np.str).tolist()\n\t            gso_fns = []\n\t            for fn in os.listdir('data/google_scanned_objects'):\n\t                if os.path.isdir(os.path.join('data/google_scanned_objects',fn)) and fn not in sym_fns:\n\t                    gso_fns.append(fn)\n\t            random.seed(1234)\n\t            random.shuffle(gso_fns)\n\t            val_fns, test_fns, train_fns = gso_fns[:5], gso_fns[5:20], gso_fns[20:]\n", "            save_pickle([train_fns, val_fns, test_fns], gso_split_pkl)\n\t        else:\n\t            val_fns, test_fns, train_fns = [], [], []\n\t    gso_train_names = [f'gso/{fn}/white_{resolution}' for fn in train_fns]\n\t    gso_val_names = [f'gso/{fn}/white_{resolution}' for fn in val_fns]\n\t    gso_test_names = [f'gso/{fn}/white_{resolution}' for fn in test_fns]\n\t    return gso_train_names, gso_val_names, gso_test_names\n\tgso_train_names_128, gso_val_names_128, gso_test_names_128 = get_gso_split(128)\n\t###############################Co3D###############################\n\t# elevation >= 30 degree\n", "# 32 sample points, max angle difference is 20 degree\n\t# 1024 sample points, max angle difference is 3.5 degree\n\tdef get_co3d_category_split(category):\n\t    seq_names_fn = f'{Co3D_ROOT}_256_512/{category}/valid_seq_names.pkl'\n\t    seq_names = read_pickle(seq_names_fn)\n\t    random.seed(1234)\n\t    random.shuffle(seq_names)\n\t    seq_names = [f'co3d_resize/{category}/{name}/256_512' for name in seq_names]\n\t    train_names, val_names = seq_names[2:], seq_names[:2]\n\t    return train_names, val_names\n", "co3d_categories = np.loadtxt('assets/co3d_names.txt',dtype=np.str).tolist()\n\tdef get_co3d_split(category_num=None):\n\t    if not os.path.exists(Co3D_ROOT) and not os.path.exists(f'{Co3D_ROOT}_256_512'): return [], []\n\t    train_names, val_names = [], []\n\t    cur_co3d_categories = [item for item in co3d_categories]\n\t    for c in cur_co3d_categories:\n\t        ts, vs = get_co3d_category_split(c)\n\t        if category_num is None:\n\t            train_names += ts\n\t        else:\n", "            train_names += ts[:category_num]\n\t        val_names += vs\n\t    random.seed(1234)\n\t    random.shuffle(val_names)\n\t    return train_names, val_names[:10]\n\tco3d_train_names, co3d_val_names = get_co3d_split()\n\t###########################ShapeNet###############################\n\tshapenet_excluded_clasees=[\n\t    '02747177',\n\t    '02876657',\n", "    '02880940',\n\t    '02808440',\n\t    '04225987',\n\t]\n\tshapenet_excluded_instance=np.loadtxt('assets/shapenet_sym_objects.txt', dtype=np.str).tolist()\n\tshapenet_train_names = read_pickle(f'data/shapenet/shapenet_render_v1.pkl')\n\t# 'co3d_train', 'gso_train_128', 'shapenet_train_v1', 'linemod_train', 'gen6d_train'\n\tname2database_names={\n\t    'gso_train_128': gso_train_names_128,\n\t    'co3d_train': co3d_train_names,\n", "    'shapenet_train': shapenet_train_names,\n\t    'linemod_train': [f'linemod/{obj}' for obj in ['ape','can','holepuncher','iron','phone' ]],\n\t    'genmop_train': [f'genmop/{name}-test' for name in ['cup','knife','love','plug_cn','miffy']],\n\t    'gso_train_128_exp': gso_train_names_128[:10],\n\t    'co3d_train_exp': co3d_train_names[:10],\n\t    'shapenet_train_exp': shapenet_train_names[:10],\n\t}"]}
{"filename": "dataset/train_dataset.py", "chunked_list": ["import os\n\timport random\n\timport cv2\n\timport numpy as np\n\timport torchvision.transforms as T\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom path import Path\n\tfrom skimage.io import imread\n", "from torch.utils.data import Dataset\n\tfrom tqdm import tqdm\n\tfrom transforms3d.quaternions import mat2quat\n\tfrom dataset.database import parse_database_name, get_object_center, Co3DResizeDatabase, get_database_split, \\\n\t    get_object_vert, get_diameter, NormalizedDatabase, normalize_pose, get_ref_point_cloud\n\tfrom dataset.train_meta_info import name2database_names\n\tfrom utils.base_utils import read_pickle, save_pickle, color_map_forward, project_points, transformation_compose_2d, \\\n\t    transformation_offset_2d, transformation_scale_2d, transformation_rotation_2d, transformation_apply_2d, \\\n\t    color_map_backward, sample_fps_points, transformation_crop, pose_inverse, pose_compose, transform_points_pose, \\\n\t    pose_apply\n", "from utils.database_utils import select_reference_img_ids_fps, normalize_reference_views, \\\n\t    compute_normalized_view_correlation, look_at_crop, select_reference_img_ids_refinement\n\tfrom utils.dataset_utils import set_seed\n\tfrom utils.imgs_info import build_imgs_info, imgs_info_to_torch\n\tfrom utils.pose_utils import scale_rotation_difference_from_cameras, let_me_look_at, let_me_look_at_2d, \\\n\t    estimate_pose_from_similarity_transform_compose\n\tclass MotionBlur(nn.Module):\n\t    def __init__(self,max_ksize=10):\n\t        super().__init__()\n\t        self.max_ksize=max_ksize\n", "    def forward(self, image):\n\t        \"\"\"\n\t        @param image: [b,3,h,w] torch.float32 [0,1]\n\t        @return:\n\t        \"\"\"\n\t        mode = np.random.choice(['h', 'v', 'diag_down', 'diag_up'])\n\t        ksize = np.random.randint(0, (self.max_ksize+1)/2)*2 + 1  # make sure is odd\n\t        center = int((ksize-1)/2)\n\t        kernel = np.zeros((ksize, ksize))\n\t        if mode == 'h':\n", "            kernel[center, :] = 1.\n\t        elif mode == 'v':\n\t            kernel[:, center] = 1.\n\t        elif mode == 'diag_down':\n\t            kernel = np.eye(ksize)\n\t        elif mode == 'diag_up':\n\t            kernel = np.flip(np.eye(ksize), 0)\n\t        var = ksize * ksize / 16.\n\t        grid = np.repeat(np.arange(ksize)[:, np.newaxis], ksize, axis=-1)\n\t        gaussian = np.exp(-(np.square(grid-center)+np.square(grid.T-center))/(2.*var))\n", "        kernel *= gaussian\n\t        kernel /= np.sum(kernel)\n\t        kernel = torch.from_numpy(kernel.astype(np.float32)) # [k,k]\n\t        kernel = kernel.unsqueeze(0).unsqueeze(1) # [1,1,k,k]\n\t        zeros = torch.zeros([1,1,ksize,ksize],dtype=torch.float32)\n\t        kernel = torch.cat([torch.cat([kernel,zeros,zeros],0),torch.cat([zeros,kernel,zeros],0),torch.cat([zeros,zeros,kernel],0)],1)\n\t        # kernel = kernel.repeat(1,1,1,1) # [3,3,k,k]\n\t        # import ipdb; ipdb.set_trace()\n\t        image = F.conv2d(image,kernel,padding=ksize//2)\n\t        image = torch.clip(image,min=0.0,max=1.0)\n", "        return image\n\tclass AdditiveShade(nn.Module):\n\t    def __init__(self, nb_ellipses=5, transparency_range=(0.3, 0.5), kernel_size_range=(20, 50)):\n\t        super().__init__()\n\t        self.nb_ellipses=nb_ellipses\n\t        self.transparency_range=transparency_range\n\t        self.kernel_size_range=kernel_size_range\n\t    def forward(self,image):\n\t        h, w = image.shape[2:]\n\t        min_dim = min(h, w) / 4\n", "        mask = np.zeros([h, w], np.uint8)\n\t        for i in range(self.nb_ellipses):\n\t            ax = int(max(np.random.rand() * min_dim, min_dim / 5))\n\t            ay = int(max(np.random.rand() * min_dim, min_dim / 5))\n\t            max_rad = max(ax, ay)\n\t            x = np.random.randint(max_rad, w - max_rad)  # center\n\t            y = np.random.randint(max_rad, h - max_rad)\n\t            angle = np.random.rand() * 90\n\t            cv2.ellipse(mask, (x, y), (ax, ay), angle, 0, 360, 255, -1)\n\t        transparency = np.random.uniform(*self.transparency_range)\n", "        kernel_size = np.random.randint(*self.kernel_size_range)\n\t        if np.random.random()<0.5: transparency = -transparency\n\t        if (kernel_size % 2) == 0:  kernel_size += 1\n\t        mask = cv2.GaussianBlur(mask.astype(np.float32), (kernel_size, kernel_size), 0)\n\t        mask = torch.from_numpy((1 - transparency * mask / 255.).astype(np.float32))\n\t        image = image * mask[None,None,:,:]\n\t        image = torch.clip(image,min=0.0,max=1.0)\n\t        return image\n\tCOCO_IMAGE_ROOT = 'data/coco/train2017'\n\tdef get_coco_image_fn_list():\n", "    if Path('data/COCO_list.pkl').exists():\n\t        return read_pickle('data/COCO_list.pkl')\n\t    img_list = os.listdir(COCO_IMAGE_ROOT)\n\t    img_list = [img for img in img_list if img.endswith('.jpg')]\n\t    save_pickle(img_list, 'data/COCO_list.pkl')\n\t    return img_list\n\tdef get_background_image_coco(fn, h, w):\n\t    back_img = imread(f'{COCO_IMAGE_ROOT}/{fn}')\n\t    h1, w1 = back_img.shape[:2]\n\t    if h1 > h and w1 > w:\n", "        hb = np.random.randint(0, h1 - h)\n\t        wb = np.random.randint(0, w1 - w)\n\t        back_img = back_img[hb:hb + h, wb:wb + w]\n\t    else:\n\t        back_img = cv2.resize(back_img,(w,h),interpolation=cv2.INTER_LINEAR)\n\t    if len(back_img.shape)==2:\n\t        back_img = np.repeat(back_img[:,:,None],3,2)\n\t    return back_img[:,:,:3]\n\tdef crop_bbox_resize_to_target_size(img, mask, bbox, target_size, margin_ratio,\n\t                                    scale_aug=None, rotation_aug=None, offset_aug=None):\n", "    \"\"\"\n\t    scale -> rotation -> offset\n\t    @param img:\n\t    @param mask:\n\t    @param bbox:\n\t    @param target_size:\n\t    @param margin_ratio:\n\t    @param scale_aug:\n\t    @param rotation_aug:\n\t    @param offset_aug:\n", "    @return:\n\t    \"\"\"\n\t    center = bbox[:2] + bbox[2:] / 2\n\t    bw, bh = bbox[2:]\n\t    rotation_val = 0.0\n\t    if bw==0 or bh==0:\n\t        scale_val = 1.0\n\t    else:\n\t        scale_val = target_size / (max(bw, bh) * (1 + margin_ratio))\n\t    M = transformation_offset_2d(-center[0], -center[1])\n", "    M = transformation_compose_2d(M, transformation_scale_2d(scale_val))\n\t    if scale_aug is not None:\n\t        M = transformation_compose_2d(M, transformation_scale_2d(scale_aug))\n\t        scale_val *= scale_aug\n\t    if rotation_aug is not None:\n\t        M = transformation_compose_2d(M, transformation_rotation_2d(rotation_aug))\n\t        rotation_val += rotation_aug\n\t    offset_val = np.asarray([0,0],np.float32)\n\t    if offset_aug is not None:\n\t        M = transformation_compose_2d(M, transformation_offset_2d(offset_aug[0], offset_aug[1]))\n", "        offset_val += offset_aug\n\t    offset1 = transformation_offset_2d(target_size / 2, target_size / 2)\n\t    M = transformation_compose_2d(M, offset1)\n\t    img = cv2.warpAffine(img, M, (target_size, target_size), flags=cv2.INTER_LINEAR)\n\t    mask = cv2.warpAffine(mask.astype(np.float32), M, (target_size, target_size), flags=cv2.INTER_LINEAR)\n\t    return img, mask, M\n\tdef _check_detection(que_imgs_info, ref_imgs_info, gt_ref_idx, scale_diff, rotation_diff, name):\n\t    from utils.draw_utils import concat_images_list\n\t    from skimage.io import imsave\n\t    qn = que_imgs_info['imgs'].shape[0]\n", "    que_imgs = color_map_backward(que_imgs_info['imgs'].numpy()).transpose([0,2,3,1])\n\t    ref_imgs = color_map_backward(ref_imgs_info['imgs'].numpy()).transpose([0,2,3,1])\n\t    _, hr, wr, _ = ref_imgs.shape\n\t    imgs, whole_imgs = [], []\n\t    for qi in range(qn):\n\t        # ref to que\n\t        scale = 1/scale_diff.numpy()[qi]\n\t        rotation = -rotation_diff.numpy()[qi]\n\t        center = que_imgs_info['cens'][qi]\n\t        M = transformation_offset_2d(-center[0], -center[1])\n", "        M = transformation_compose_2d(M, transformation_rotation_2d(rotation))\n\t        M = transformation_compose_2d(M, transformation_scale_2d(scale))\n\t        M = transformation_compose_2d(M, transformation_offset_2d(wr/2,hr/2))\n\t        warp_img = cv2.warpAffine(que_imgs[qi],M,(wr,hr))\n\t        M = transformation_offset_2d(-center[0], -center[1])\n\t        M = transformation_compose_2d(M, transformation_offset_2d(wr/2,hr/2))\n\t        warp_img2 = cv2.warpAffine(que_imgs[qi],M,(wr,hr))\n\t        ref_img = ref_imgs[gt_ref_idx[qi]]\n\t        imgs.append(concat_images_list(ref_img, warp_img, warp_img2, vert=True))\n\t        whole_imgs.append(que_imgs[qi])\n", "    imsave(name,concat_images_list(concat_images_list(*imgs),concat_images_list(*ref_imgs),concat_images_list(*whole_imgs),vert=True))\n\t    print('check mode is on!!')\n\tclass Gen6DTrainDataset(Dataset):\n\t    default_cfg={\n\t        'batch_size': 8,\n\t        \"use_database_sample_prob\": False,\n\t        \"database_sample_prob\": [100, 10, 30, 10, 10],\n\t        'database_names': ['co3d_train', \\\n\t                           'gso_train_128', \\\n\t                            'shapenet_train', \\\n", "                            'linemod_train', \\\n\t                            'genmop_train'],\n\t        \"resolution\": 128,\n\t        \"reference_num\": 32,\n\t        \"co3d_margin_ratio\": 0.3,\n\t    }\n\t    def __init__(self, cfg, is_train):\n\t        self.cfg = {**self.default_cfg, **cfg}\n\t        self.is_train = is_train\n\t        self.database_names = []\n", "        self.database_set_names = []\n\t        self.database_set_name2names = {}\n\t        for name in self.cfg['database_names']:\n\t            self.database_names += name2database_names[name]\n\t            self.database_set_names.append(name)\n\t            self.database_set_name2names[name] = name2database_names[name]\n\t        self.name2database = {}\n\t        for name in tqdm(self.database_names):\n\t            self.name2database[name] = parse_database_name(name)\n\t            if name.startswith('genmop'):\n", "                test_name = name.replace('test','ref')\n\t                self.name2database[test_name] = parse_database_name(test_name)\n\t        self.cum_que_num = np.cumsum([len(self.name2database[name].get_img_ids()) for name in self.database_names])\n\t        self.background_img_list = get_coco_image_fn_list() # get_SUN397_image_fn_list()\n\t        self.photometric_augment_modules = [\n\t            T.GaussianBlur(3),\n\t            T.ColorJitter(brightness=0.3),\n\t            T.ColorJitter(contrast=0.2),\n\t            T.ColorJitter(hue=0.05),\n\t            T.ColorJitter(saturation=0.3),\n", "            MotionBlur(5),\n\t            AdditiveShade(),\n\t        ]\n\t    def __len__(self):\n\t        if self.is_train:\n\t            return 999999\n\t        else:\n\t            return self.cum_que_num[-1]\n\t    def _select_query(self,index):\n\t        if self.is_train:\n", "            if self.cfg['use_database_sample_prob']:\n\t                probs = np.asarray(self.cfg['database_sample_prob'])\n\t                probs = probs/np.sum(probs)\n\t                database_set_name = np.random.choice(self.database_set_names,p=probs)\n\t                names = self.database_set_name2names[database_set_name]\n\t                database = self.name2database[np.random.choice(names)]\n\t            else:\n\t                database = self.name2database[self.database_names[np.random.randint(0,len(self.database_names))]]\n\t            img_ids = database.get_img_ids()\n\t            random.shuffle(img_ids)\n", "            que_ids = img_ids[:self.cfg['batch_size']]\n\t        else:\n\t            data_id = np.searchsorted(self.cum_que_num, index, 'right')\n\t            database = self.name2database[self.database_names[data_id]]\n\t            image_id_back = self.cum_que_num[data_id] - index\n\t            que_ids = [database.get_img_ids()[-image_id_back]] # only use single image in testing\n\t        return database, que_ids\n\t    def _add_background(self, imgs, masks, same_background_prob):\n\t        \"\"\"\n\t        @param imgs:   [b,3,h,w] in [0,1] torch.tensor\n", "        @param masks:\n\t        @param same_background_prob:\n\t        @return:\n\t        \"\"\"\n\t        # imgs = imgs_info['imgs']\n\t        # masks = imgs_info['masks']\n\t        qn, _, h, w = imgs.shape\n\t        back_imgs = []\n\t        if np.random.random() < same_background_prob:\n\t            fn = self.background_img_list[np.random.randint(0, len(self.background_img_list))]\n", "            back_img_global = get_background_image_coco(fn, h, w)\n\t        else:\n\t            back_img_global = None\n\t        for qi in range(qn):\n\t            if back_img_global is None:\n\t                fn = self.background_img_list[np.random.randint(0, len(self.background_img_list))]\n\t                back_img = get_background_image_coco(fn, h, w)\n\t            else:\n\t                back_img = back_img_global\n\t            back_img = color_map_forward(back_img)\n", "            if len(back_img.shape)==2:\n\t                back_img = np.repeat(back_img[:,:,None],3,2)\n\t            back_img = torch.from_numpy(back_img).permute(2,0,1)\n\t            back_imgs.append(back_img)\n\t        back_imgs = torch.stack(back_imgs,0)\n\t        masks = masks.float()\n\t        imgs = imgs*masks + (1 - masks)*back_imgs\n\t        return imgs\n\t    def _build_ref_imgs_info(self, database, ref_ids):\n\t        if database.database_name.startswith('gso') or database.database_name.startswith('shapenet'):\n", "            ref_imgs_info = build_imgs_info(database, ref_ids)\n\t            rfn = len(ref_ids)\n\t            M = np.concatenate([np.eye(2),np.zeros([2,1])],1)\n\t            ref_imgs_info['Ms'] = np.repeat(M[None,:],rfn,0)\n\t            ref_imgs_info['ref_ids'] = np.asarray(ref_ids)\n\t            object_center = get_object_center(database)\n\t            # add object center to imgs_info\n\t            ref_imgs_info['cens'] = [project_points(object_center[None],pose,K)[0][0] for pose,K in zip(ref_imgs_info['poses'],ref_imgs_info['Ks'])] # object center\n\t            ref_imgs_info['cens'] = np.asarray(ref_imgs_info['cens'])\n\t        elif database.database_name.startswith('co3d'):\n", "            t = self.cfg['resolution']\n\t            m = self.cfg['co3d_margin_ratio']\n\t            imgs, masks, Ms = [], [], []\n\t            for ref_id in ref_ids:\n\t                assert(isinstance(database, Co3DResizeDatabase))\n\t                img = database.get_image(ref_id)\n\t                mask = database.get_mask(ref_id)\n\t                bbox = database.get_bbox(ref_id)\n\t                img, mask, M = crop_bbox_resize_to_target_size(img, mask, bbox, t, m)\n\t                imgs.append(img)\n", "                masks.append(mask)\n\t                Ms.append(M)\n\t            imgs = np.stack(imgs, 0)\n\t            imgs = color_map_forward(imgs)\n\t            masks = np.stack(masks, 0)\n\t            Ms = np.stack(Ms,0)\n\t            poses = np.asarray([database.get_pose(img_id) for img_id in ref_ids])\n\t            Ks = np.asarray([database.get_K(img_id) for img_id in ref_ids])\n\t            cens = np.repeat(np.asarray([t/2,t/2])[None,:],len(ref_ids),0)  # add object center to imgs_info\n\t            ref_imgs_info = {'ref_ids': np.asarray(ref_ids), 'imgs': imgs.transpose([0,3,1,2]), 'masks': masks[:,None,:,:], 'Ms': Ms, 'poses': poses, 'Ks': Ks, 'cens': cens}\n", "        elif database.database_name.startswith('linemod') or database.database_name.startswith('genmop'):\n\t            ref_ids_all = database.get_img_ids()\n\t            res = self.cfg['resolution']\n\t            # ref_num = self.cfg['reference_num']\n\t            ref_num = np.random.randint(16,32)\n\t            ref_ids = select_reference_img_ids_fps(database, ref_ids_all, ref_num, random_fps=self.is_train)\n\t            imgs, masks, Ks, poses, _ = normalize_reference_views(database, ref_ids, res, 0.05)\n\t            M = np.concatenate([np.eye(2), np.zeros([2, 1])], 1)\n\t            rfn, h, w, _ = imgs.shape\n\t            object_center = get_object_center(database)\n", "            cens = np.asarray([project_points(object_center[None], pose, K)[0][0] for pose, K in zip(poses, Ks)],np.float32)\n\t            ref_imgs_info = {\n\t                'imgs': color_map_forward(imgs).transpose([0,3,1,2]),\n\t                'masks': np.ones([rfn,1,h,w],dtype=np.float32),\n\t                'ref_ids': [None for _ in range(rfn)],\n\t                'Ms': np.repeat(M[None,:], rfn, 0),\n\t                'poses': poses.astype(np.float32),\n\t                'Ks': Ks.astype(np.float32),\n\t                'cens': cens, # rfn, 2\n\t            }\n", "        else:\n\t            raise NotImplementedError\n\t        return ref_imgs_info\n\t    def _photometric_augment(self, imgs_info, aug_prob):\n\t        if len(imgs_info['imgs'].shape)==3:\n\t            if np.random.random() < aug_prob:\n\t                ids = np.random.choice(np.arange(len(self.photometric_augment_modules)), np.random.randint(1, 4), False)\n\t                for idx in ids: imgs_info['imgs'] = self.photometric_augment_modules[idx](imgs_info['imgs'][None])[0]\n\t        else:\n\t            qn = imgs_info['imgs'].shape[0]\n", "            for qi in range(qn):\n\t                if np.random.random()<aug_prob:\n\t                    ids = np.random.choice(np.arange(len(self.photometric_augment_modules)), np.random.randint(1, 4), False)\n\t                    for idx in ids: imgs_info['imgs'][qi:qi + 1] = self.photometric_augment_modules[idx](imgs_info['imgs'][qi:qi + 1])\n\t    def _photometric_augment_imgs(self, imgs, aug_prob):\n\t        qn = imgs.shape[0]\n\t        for qi in range(qn):\n\t            if np.random.random()<aug_prob:\n\t                ids = np.random.choice(np.arange(len(self.photometric_augment_modules)), np.random.randint(1, 4), False)\n\t                for idx in ids: imgs[qi:qi+1] = self.photometric_augment_modules[idx](imgs[qi:qi + 1])\n", "        return imgs\n\t    def __getitem__(self, index):\n\t        raise NotImplementedError\n\tdef add_object_to_background(img, mask, back_img, max_obj_ratio=0.5):\n\t    \"\"\"\n\t    @param img: [0,1] in float32\n\t    @param mask:\n\t    @param back_img:\n\t    @param max_obj_ratio:\n\t    @return:\n", "    \"\"\"\n\t    img_out = np.copy(back_img)\n\t    h1, w1 = img_out.shape[:2]\n\t    # get compact region\n\t    ys, xs = np.nonzero(mask.astype(np.bool))\n\t    min_x, max_x, min_y, max_y = np.min(xs), np.max(xs), np.min(ys), np.max(ys)\n\t    img = img[min_y:max_y, min_x:max_x]\n\t    mask = mask[min_y:max_y, min_x:max_x]\n\t    h, w = img.shape[:2]\n\t    # if too large, we resize it\n", "    if max(h, w)/max(h1, w1)>max_obj_ratio:\n\t        ratio = max(h1, w1) * np.random.uniform(0.1, max_obj_ratio) / max(h, w)\n\t        h, w = int(round(ratio*h)),int(round(ratio*w))\n\t        mask = cv2.resize(mask.astype(np.uint8),(w, h), interpolation=cv2.INTER_LINEAR)>0\n\t        img = cv2.resize(img, (w, h), interpolation=cv2.INTER_LINEAR)\n\t    h0, w0 = np.random.randint(0, h1 - h), np.random.randint(0, w1 - w)\n\t    raw = img_out[h0:h + h0, w0:w + w0]\n\t    img_out[h0:h + h0, w0:w + w0] = img * mask.astype(np.float32)[:, :, None] + \\\n\t                                    raw * (1 - mask[:, :, None].astype(np.float32))\n\t    mask_out = np.zeros([h1, w1], dtype=np.bool)\n", "    mask_out[h0:h + h0, w0:w + w0] = mask.astype(np.bool)\n\t    bbox_out = np.asarray([w0, h0, w, h], np.float32)\n\t    return img_out, mask_out, bbox_out\n\tdef transformation_decompose_scale(M):\n\t    return np.sqrt(np.linalg.det(M[:2,:2]))\n\tdef transformation_decompose_rotation(M):\n\t    return np.arctan2(M[1,0],M[0,0])\n\tdef get_ref_ids(database, ref_view_type):\n\t    if database.database_name.startswith('linemod') or database.database_name.startswith('genmop'):\n\t        return []\n", "    # sample farthest points for these datasets\n\t    if ref_view_type.startswith('fps'):\n\t        anchor_num = int(ref_view_type.split('_')[-1])\n\t        img_ids = database.get_img_ids()\n\t        poses = [database.get_pose(img_id) for img_id in img_ids]\n\t        cam_pts = np.asarray([(pose[:,:3].T @ pose[:,3:])[...,0] for pose in poses],np.float32)\n\t        indices = sample_fps_points(cam_pts, anchor_num, False, True)\n\t        ref_ids = np.asarray(img_ids)[indices]\n\t    else:\n\t        raise NotImplementedError\n", "    return ref_ids\n\tclass DetectionTrainDataset(Gen6DTrainDataset):\n\t    det_default_cfg={\n\t        'ref_type': 'fps_32',\n\t        \"detector_scale_range\": [-0.5,1.2],\n\t        \"detector_rotation_range\": [-22.5,22.5],\n\t        # only used in validation\n\t        'background_database_name': 'gso_train_128',\n\t        'background_database_num': 32,\n\t        # query image resolution\n", "        \"query_resolution\": 512,\n\t        # co3d settings of generate query iamge\n\t        'que_add_background_objects': True,\n\t        'que_background_objects_num': 2,\n\t        'que_background_objects_ratio': 0.3,\n\t        'offset_type': 'random',\n\t        \"detector_offset_std\": 3,\n\t        'detector_real_aug_rot': True,\n\t    }\n\t    def __init__(self, cfg, is_train):\n", "        cfg={**self.det_default_cfg, **cfg}\n\t        super(DetectionTrainDataset, self).__init__(cfg, is_train)\n\t        if is_train:\n\t            self.name2back_database = {k:v for k,v in self.name2database.items() if (not k.startswith('genmop')) and (not k.startswith('linemod'))}\n\t        else:\n\t            random.seed(1234)\n\t            background_names = name2database_names[self.cfg['background_database_name']]\n\t            random.shuffle(background_names)\n\t            background_names = background_names[:self.cfg['background_database_num']]\n\t            self.name2back_database = {name:parse_database_name(name) for name in background_names}\n", "            self.name2back_database.update(self.name2database)\n\t        self.back_names = [name for name in self.name2back_database.keys()]\n\t    def get_offset(self, output_resolution, M, mask):\n\t        if self.cfg['offset_type'] == 'random':\n\t            # add random transformations and put objects on random regions\n\t            ys, xs = np.nonzero(mask)\n\t            min_x, max_x, min_y, max_y = np.min(xs), np.max(xs), np.min(ys), np.max(ys)\n\t            corners = np.asarray([[min_x, min_y,],[min_x, max_y,],[max_x, max_y,],[max_x, min_y,]],np.float32)\n\t            corners_ = transformation_apply_2d(M, corners)\n\t            min_x_, min_y_ = np.min(corners_, 0)\n", "            max_x_, max_y_ = np.max(corners_, 0)\n\t            if (max_x_ - min_x_ >= output_resolution) or (max_y_ - min_y_ >= output_resolution):\n\t                offset_x = output_resolution / 2; offset_y = output_resolution / 2\n\t                # actually impossible here, because we already build ref imgs info to resize them to 128\n\t                raise NotImplementedError\n\t            else:\n\t                offset_x = np.random.uniform(-min_x_, output_resolution - max_x_)\n\t                offset_y = np.random.uniform(-min_y_, output_resolution - max_y_)\n\t            M = transformation_compose_2d(M, transformation_offset_2d(offset_x, offset_y))\n\t        elif self.cfg['offset_type'] == 'center':\n", "            # approximately keep the object in the center\n\t            # off_min, off_max = self.cfg['detector_offset_range']\n\t            # off_x, off_y = output_resolution * np.random.uniform(off_min,off_max,2)\n\t            off_x, off_y = np.random.normal(0,self.cfg['detector_offset_std'],2)\n\t            M = transformation_compose_2d(M, transformation_offset_2d(off_x, off_y))\n\t            M = transformation_compose_2d(M, transformation_offset_2d(output_resolution/2, output_resolution/2))\n\t        else:\n\t            raise NotImplementedError\n\t        return M\n\t    def _build_que_imgs_info(self, database, que_ids):\n", "        if database.database_name.startswith('linemod') or database.database_name.startswith('genmop'):\n\t            # todo: add random background or rotation for linemod dataset\n\t            que_imgs_info = build_imgs_info(database, que_ids, has_mask=False)\n\t            poses, Ks, imgs = que_imgs_info['poses'], que_imgs_info['Ks'], que_imgs_info['imgs']\n\t            qn, _, h, w = imgs.shape\n\t            M = np.concatenate([np.eye(2), np.zeros([2, 1])], 1)\n\t            Ms = np.repeat(M[None], qn, 0)\n\t            object_center = get_object_center(database)\n\t            cens = np.asarray([project_points(object_center[None], pose, K)[0][0] for pose, K in zip(poses, Ks)], np.float32)\n\t            que_imgs_info.update({\n", "                'Ms': Ms,\n\t                'cens': np.stack(cens, 0),        # [qn,2]\n\t                'que_ids': np.asarray(que_ids),\n\t            })\n\t        else:\n\t            que_imgs_info = self._build_ref_imgs_info(database, que_ids)\n\t            imgs, masks, Ms, cens = [], [], [], []\n\t            q = self.cfg['query_resolution']\n\t            for qi in range(len(que_ids)):\n\t                img = color_map_backward(que_imgs_info['imgs'][qi].transpose([1,2,0]))\n", "                mask = que_imgs_info['masks'][qi][0]\n\t                center = que_imgs_info['cens'][qi]\n\t                scale_aug = 2 ** np.random.uniform(*self.cfg['detector_scale_range'])\n\t                rotation_aug = np.random.uniform(*self.cfg['detector_rotation_range'])\n\t                rotation_aug = np.deg2rad(rotation_aug)\n\t                M = transformation_offset_2d(-center[0],-center[1]) # offset to object center\n\t                M = transformation_compose_2d(M, transformation_scale_2d(scale_aug))\n\t                M = transformation_compose_2d(M, transformation_rotation_2d(rotation_aug))\n\t                M = self.get_offset(q,M,mask) # get offset\n\t                if database.database_name.startswith('co3d'):\n", "                    # warp co3d image from original image\n\t                    M_init = que_imgs_info['Ms'][qi]\n\t                    img_init = database.get_image(que_ids[qi])\n\t                    mask_init = database.get_mask(que_ids[qi])\n\t                    M_ = transformation_compose_2d(M_init, M)\n\t                    img = cv2.warpAffine(img_init, M_, (q, q), flags=cv2.INTER_LINEAR)\n\t                    mask = cv2.warpAffine(mask_init.astype(np.uint8), M_, (q, q), flags=cv2.INTER_LINEAR)\n\t                else:\n\t                    img = cv2.warpAffine(img, M, (q, q), flags=cv2.INTER_LINEAR)\n\t                    mask = cv2.warpAffine(mask.astype(np.uint8), M, (q, q), flags=cv2.INTER_LINEAR)\n", "                # add random background if it is a synthetic data or 80% probability with co3d dataset\n\t                if database.database_name.startswith('gso') or database.database_name.startswith('shapenet') or np.random.random() < 0.8:\n\t                    fn = self.background_img_list[np.random.randint(0, len(self.background_img_list))]\n\t                    back_img = get_background_image_coco(fn, q, q).astype(np.float32)\n\t                    mask_ = mask[:,:,None]\n\t                    img = back_img * (1 - mask_) + img * mask_\n\t                # add random objects\n\t                img = color_map_forward(img)\n\t                mask = mask.astype(np.bool)\n\t                # add background objects\n", "                if self.cfg['que_add_background_objects']:\n\t                    img_ = self._add_background_objects(img, database, self.cfg['que_background_objects_num'], self.cfg['que_background_objects_ratio'])\n\t                    # prevent background object overwrite foreground object\n\t                    mask_ = mask.astype(np.float32)[:, :, None]\n\t                    img = img * mask_ + img_ * (1 - mask_)\n\t                imgs.append(img)\n\t                masks.append(mask)\n\t                cens.append(transformation_apply_2d(M, np.asarray([center]))[0]) # [2]\n\t                # apply initial transformations\n\t                M_in = que_imgs_info['Ms'][qi]\n", "                Ms.append(transformation_compose_2d(M_in, M))\n\t            que_imgs_info = {\n\t                'imgs': np.stack(imgs, 0).transpose([0, 3, 1, 2]),\n\t                'masks': np.stack(masks, 0)[:,None],\n\t                'Ms': np.stack(Ms, 0),     # [qn,2,3]\n\t                'cens': np.stack(cens, 0), # [qn,2]\n\t                'que_ids': np.asarray(que_ids),\n\t                'poses': que_imgs_info['poses'],\n\t                'Ks': que_imgs_info['Ks'],\n\t            }\n", "        return que_imgs_info\n\t    def _add_background_objects(self, que_img, database, object_num, max_background_object_size=0.5):\n\t        \"\"\"\n\t        @param que_img: [0,1] in float32\n\t        @param database:\n\t        @return:\n\t        \"\"\"\n\t        if object_num > 0:\n\t            for obj_id in range(object_num):\n\t                while True:\n", "                    random_database = self.name2back_database[self.back_names[np.random.randint(0, len(self.back_names))]]\n\t                    if random_database.database_name != database.database_name: break\n\t                img_id = np.random.choice(random_database.get_img_ids())\n\t                img = random_database.get_image(img_id)\n\t                img = color_map_forward(img)\n\t                mask = random_database.get_mask(img_id)\n\t                que_img, _, _ = add_object_to_background(img, mask, que_img, max_background_object_size)\n\t        return que_img\n\t    @staticmethod\n\t    def que_ref_scale_rotation_from_poses(center, ref_imgs_info, que_imgs_info):\n", "        \"\"\"\n\t        call this function before to torch\n\t        @param center: used in\n\t        @param ref_imgs_info:\n\t        @param que_imgs_info:\n\t        @return:\n\t            1. transformation from reference to query\n\t               rotate(scale(ref)) = que\n\t            2. gt_ref_ids [qn,] the ground truth reference view index for every query view\n\t        \"\"\"\n", "        ref_poses = ref_imgs_info['poses']\n\t        que_poses = que_imgs_info['poses']\n\t        ref_Ks = ref_imgs_info['Ks']\n\t        que_Ks = que_imgs_info['Ks']\n\t        # select nearest views\n\t        corr = compute_normalized_view_correlation(que_poses, ref_poses, center, False)\n\t        gt_ref_ids = np.argmax(corr, 1)  # qn\n\t        scale_diff, rotation_diff = scale_rotation_difference_from_cameras(\n\t            ref_poses[gt_ref_ids], que_poses, ref_Ks[gt_ref_ids], que_Ks, center) # from ref to que\n\t        ref_scales = np.asarray([transformation_decompose_scale(M) for M in ref_imgs_info['Ms'][gt_ref_ids]])\n", "        que_scales = np.asarray([transformation_decompose_scale(M) for M in que_imgs_info['Ms']])\n\t        ref_rotations = np.asarray([transformation_decompose_rotation(M) for M in ref_imgs_info['Ms'][gt_ref_ids]])\n\t        que_rotations = np.asarray([transformation_decompose_rotation(M) for M in que_imgs_info['Ms']])\n\t        scale = scale_diff * que_scales / ref_scales\n\t        rotation = -ref_rotations + rotation_diff + que_rotations\n\t        return scale, rotation, gt_ref_ids, corr\n\t    @staticmethod\n\t    def que_ref_scale_rotation_from_index(database, ref_imgs_info, que_imgs_info):\n\t        ref_ids = np.asarray(ref_imgs_info['ref_ids'])  # rfn\n\t        que_ids = np.asarray(que_imgs_info['que_ids'])  # qn\n", "        diff = np.abs(que_ids[:, None].astype(np.int32) - ref_ids[None, :].astype(np.int32))\n\t        gt_ref_ids = np.argmin(diff, 1)\n\t        ref_scales = np.asarray([transformation_decompose_scale(M) for M in ref_imgs_info['Ms'][gt_ref_ids]])\n\t        que_scales = np.asarray([transformation_decompose_scale(M) for M in que_imgs_info['Ms']])\n\t        ref_rotations = np.asarray([transformation_decompose_rotation(M) for M in ref_imgs_info['Ms'][gt_ref_ids]])\n\t        que_rotations = np.asarray([transformation_decompose_rotation(M) for M in que_imgs_info['Ms']])\n\t        for qi in range(len(gt_ref_ids)):\n\t            ref_id = ref_ids[gt_ref_ids[qi]]\n\t            que_id = que_ids[qi]\n\t            # record scale for the computation of scale difference\n", "            if isinstance(database, Co3DResizeDatabase):\n\t                # since we already resize co3d dataset when resizing\n\t                ref_scales[qi] *= database.ratios[ref_id]\n\t                que_scales[qi] *= database.ratios[que_id]\n\t        # ref to query\n\t        scale = que_scales / ref_scales\n\t        rotation = que_rotations - ref_rotations\n\t        return scale, rotation, gt_ref_ids\n\t    def _compute_scale_rotation_target(self, database, ref_imgs_info, que_imgs_info):\n\t        if database.database_name.startswith('gso') or \\\n", "            database.database_name.startswith('shapenet') or \\\n\t            database.database_name.startswith('linemod') or \\\n\t            database.database_name.startswith('genmop'):\n\t            center = get_object_center(database)\n\t            scale_diff, rotation_diff, gt_ref_ids, _ = \\\n\t                self.que_ref_scale_rotation_from_poses(center, ref_imgs_info, que_imgs_info)\n\t        elif database.database_name.startswith('co3d'):\n\t            scale_diff, rotation_diff, gt_ref_ids = \\\n\t                self.que_ref_scale_rotation_from_index(database, ref_imgs_info, que_imgs_info)\n\t        else:\n", "            raise NotImplementedError\n\t        return scale_diff, rotation_diff, gt_ref_ids\n\t    def add_background_or_not(self, que_database):\n\t        add_background=False\n\t        if que_database.database_name.startswith('shapenet') or que_database.database_name.startswith('gso'):\n\t            add_background=True\n\t        elif que_database.database_name.startswith('co3d'):\n\t            if np.random.random()<0.75:\n\t                add_background = True\n\t        elif que_database.database_name.startswith('linemod'):\n", "            add_background = False\n\t        elif que_database.database_name.startswith('genmop'):\n\t            add_background = False\n\t        else:\n\t            raise NotImplementedError\n\t        return add_background\n\t    def __getitem__(self, index):\n\t        set_seed(index,self.is_train)\n\t        que_database, que_ids = self._select_query(index)\n\t        if que_database.database_name.startswith('genmop'):\n", "            que_name = que_database.database_name\n\t            ref_database = self.name2database[que_name.replace('test', 'ref')]\n\t        else:\n\t            ref_database = que_database\n\t        ref_ids = get_ref_ids(ref_database, self.cfg['ref_type'])\n\t        ref_imgs_info = self._build_ref_imgs_info(ref_database, ref_ids)\n\t        que_imgs_info = self._build_que_imgs_info(que_database, que_ids)\n\t        # compute scale and rotation difference\n\t        scale_diff, rotation_diff, gt_ref_idx = \\\n\t            self._compute_scale_rotation_target(que_database,ref_imgs_info,que_imgs_info)\n", "        ref_imgs_info.pop('ref_ids'); que_imgs_info.pop('que_ids')\n\t        ref_imgs_info = imgs_info_to_torch(ref_imgs_info)\n\t        que_imgs_info = imgs_info_to_torch(que_imgs_info)\n\t        if self.is_train and self.add_background_or_not(que_database):\n\t            ref_imgs_info['imgs'] = self._add_background(ref_imgs_info['imgs'], ref_imgs_info['masks'], 0.5)\n\t        if self.is_train:\n\t            self._photometric_augment(que_imgs_info, 0.8)\n\t            self._photometric_augment(ref_imgs_info, 0.8)\n\t        scale_diff = torch.from_numpy(scale_diff.astype(np.float32))\n\t        rotation_diff = torch.from_numpy(rotation_diff.astype(np.float32))\n", "        gt_ref_idx = torch.from_numpy(gt_ref_idx.astype(np.int32))\n\t        # _check_detection(que_imgs_info, ref_imgs_info, gt_ref_idx, scale_diff, rotation_diff, f'data/vis_val/{index}.jpg')\n\t        return {'que_imgs_info': que_imgs_info, 'ref_imgs_info': ref_imgs_info,\n\t                'gt_ref_idx': gt_ref_idx, 'scale_diff': scale_diff, 'rotation_diff': rotation_diff}\n\tclass DetectionValDataset(Dataset):\n\t    default_cfg={\n\t        \"test_database_name\": 'linemod/cat',\n\t        \"ref_database_name\": 'linemod/cat',\n\t        \"test_split_type\": \"linemod_val\",\n\t        \"ref_split_type\": \"linemod_val\",\n", "        \"detector_ref_num\": 32,\n\t        \"detector_ref_res\": 128,\n\t    }\n\t    def __init__(self, cfg, is_train):\n\t        self.cfg = {**self.default_cfg, **cfg}\n\t        super().__init__()\n\t        assert(not is_train)\n\t        self.test_database = parse_database_name(self.cfg['test_database_name'])\n\t        self.ref_database = parse_database_name(self.cfg['ref_database_name'])\n\t        ref_ids, _ = get_database_split(self.ref_database,self.cfg['ref_split_type'])\n", "        _, self.test_ids = get_database_split(self.test_database,self.cfg['test_split_type'])\n\t        ref_ids = select_reference_img_ids_fps(self.ref_database, ref_ids, self.cfg['detector_ref_num'])\n\t        # ref_imgs_new, ref_masks_new, ref_Ks_new, ref_poses_new, ref_Hs\n\t        ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs = \\\n\t            normalize_reference_views(self.ref_database, ref_ids, self.cfg['detector_ref_res'], 0.05)\n\t        self.ref_info={\n\t            'poses': torch.from_numpy(ref_poses.astype(np.float32)),\n\t            'Ks': torch.from_numpy(ref_Ks.astype(np.float32)),\n\t            'imgs': torch.from_numpy(color_map_forward(ref_imgs)).permute(0,3,1,2)\n\t        }\n", "        self.center = get_object_center(self.ref_database).astype(np.float32)\n\t        self.res = self.cfg['detector_ref_res']\n\t    def __getitem__(self, index):\n\t        ref_imgs_info = self.ref_info.copy()\n\t        img_id = self.test_ids[index]\n\t        que_img = self.test_database.get_image(img_id)\n\t        center_np = self.center\n\t        que_poses = self.test_database.get_pose(img_id)[None,]\n\t        que_Ks = self.test_database.get_K(img_id)[None,]\n\t        que_cen = project_points(center_np[None], que_poses[0], que_Ks[0])[0][0]\n", "        ref_poses = ref_imgs_info['poses'].numpy()\n\t        ref_Ks = ref_imgs_info['Ks'].numpy()\n\t        corr = compute_normalized_view_correlation(que_poses, ref_poses, center_np, False)\n\t        gt_ref_ids = np.argmax(corr, 1)  # qn\n\t        scale_diff, angle_diff = scale_rotation_difference_from_cameras(ref_poses[gt_ref_ids], que_poses, ref_Ks[gt_ref_ids], que_Ks, center_np)\n\t        que_imgs_info = {\n\t            'imgs': torch.from_numpy(color_map_forward(que_img)[None]).permute(0,3,1,2),\n\t            'poses': torch.from_numpy(que_poses.astype(np.float32)),\n\t            'Ks': torch.from_numpy(que_Ks.astype(np.float32)),\n\t            'cens': torch.from_numpy(que_cen[None]),\n", "        }\n\t        scale_diff = torch.from_numpy(scale_diff.astype(np.float32))\n\t        angle_diff = torch.from_numpy(angle_diff.astype(np.float32))\n\t        gt_ref_ids = torch.from_numpy(gt_ref_ids.astype(np.int32))\n\t        return {'que_imgs_info': que_imgs_info, 'ref_imgs_info': ref_imgs_info, 'gt_ref_idx': gt_ref_ids, 'scale_diff': scale_diff, 'rotation_diff': angle_diff}\n\t    def __len__(self):\n\t        return len(self.test_ids)\n\tdef _check_selection(que_imgs_info,ref_imgs,ref_vp_scores,angles_r2q,index):\n\t    from utils.draw_utils import concat_images_list\n\t    from skimage.io import imsave\n", "    que_imgs = color_map_backward(que_imgs_info['imgs'].cpu().numpy()).transpose([0,2,3,1]) # qn,h,w,3\n\t    ref_imgs = color_map_backward(ref_imgs.cpu().numpy()).transpose([0,1,3,4,2]) # an,rfn,h,w,3\n\t    angles_r2q = angles_r2q.cpu().numpy()\n\t    ref_vp_scores = ref_vp_scores.cpu().numpy() # qn,rfn\n\t    ref_vp_idx = np.argsort(-ref_vp_scores,1) # qn,rfn\n\t    qn, h, w, _ = que_imgs.shape\n\t    for qi in range(1):\n\t        M = transformation_offset_2d(-w/2,-h/2)\n\t        M = transformation_compose_2d(M, transformation_rotation_2d(-angles_r2q[qi]))\n\t        M = transformation_compose_2d(M, transformation_offset_2d(w/2, h/2))\n", "        warp_img = cv2.warpAffine(que_imgs[qi], M, (w,h), cv2.INTER_LINEAR)\n\t        ori_img = que_imgs[qi]\n\t        cur_imgs = [concat_images_list(ori_img, warp_img)]\n\t        for k in range(5):\n\t            cur_imgs.append(concat_images_list(*[ref_imgs[ai, ref_vp_idx[qi,k]] for ai in range(5)]))\n\t        imsave(f'data/vis_val/{index}-{qi}.jpg',concat_images_list(*cur_imgs,vert=True))\n\t    print('check mode is on!')\n\tclass SelectionTrainDataset(Gen6DTrainDataset):\n\t    default_cfg_v2 = {\n\t        'ref_type': 'fps_32',\n", "        'selector_scale_range': [-0.1, 0.1],\n\t        'selector_angle_range': [-90, 90],\n\t        'selector_angles': [-90, -45, 0, 45, 90],\n\t        'selector_real_aug': False,\n\t    }\n\t    def __init__(self, cfg, is_train):\n\t        cfg = {**self.default_cfg_v2,**cfg}\n\t        super().__init__(cfg, is_train)\n\t    def geometric_augment_que(self, que_imgs_info):\n\t        qn, _, h, w = que_imgs_info['imgs'].shape\n", "        imgs = que_imgs_info['imgs'].transpose([0, 2, 3, 1])\n\t        masks = que_imgs_info['masks'][:,0,:,:]\n\t        Ms = que_imgs_info['Ms']\n\t        imgs_out, masks_out, Ms_out = [], [], []\n\t        for qi in range(qn):\n\t            scale_aug = 2 ** np.random.uniform(*self.cfg['selector_scale_range'])\n\t            rotation_aug = np.random.uniform(*self.cfg['selector_angle_range'])\n\t            rotation_aug = np.deg2rad(rotation_aug)\n\t            M = transformation_offset_2d(-w/2, -h/2)\n\t            M = transformation_compose_2d(M, transformation_rotation_2d(rotation_aug))\n", "            M = transformation_compose_2d(M, transformation_scale_2d(scale_aug))\n\t            M = transformation_compose_2d(M, transformation_offset_2d(w/2, h/2))\n\t            imgs_out.append(cv2.warpAffine(imgs[qi], M, (w,h), flags=cv2.INTER_LINEAR))\n\t            masks_out.append(cv2.warpAffine(masks[qi].astype(np.float32), M, (w,h), flags=cv2.INTER_LINEAR))\n\t            Ms_out.append(transformation_compose_2d(Ms[qi], M))\n\t        que_imgs_info['imgs'] = np.stack(imgs_out, 0).transpose([0,3,1,2])\n\t        que_imgs_info['masks'] = np.stack(masks_out, 0)[:,None]\n\t        que_imgs_info['Ms'] = np.stack(Ms_out, 0)\n\t        return que_imgs_info\n\t    @staticmethod\n", "    def geometric_augment_ref(ref_imgs_in, ref_mask_in, detection_angles):\n\t        rfn, _, h, w = ref_imgs_in.shape\n\t        assert(h==w)\n\t        imgs_out, masks_out = [], []\n\t        for rfi in range(rfn):\n\t            imgs, masks = [], []\n\t            for angle in detection_angles:\n\t                M = transformation_offset_2d(-h/2, -w/2)\n\t                M = transformation_compose_2d(M, transformation_rotation_2d(np.deg2rad(angle)))  # q2r\n\t                M = transformation_compose_2d(M, transformation_offset_2d(w/2, h/2))\n", "                img_rotation = cv2.warpAffine(ref_imgs_in[rfi].transpose([1,2,0]), M, (w, h), flags=cv2.INTER_LINEAR) # h,w,3\n\t                mask_rotation = cv2.warpAffine(ref_mask_in[rfi][0].astype(np.float32), M, (w, h), flags=cv2.INTER_LINEAR)\n\t                imgs.append(img_rotation) # h,w,3\n\t                masks.append(mask_rotation) # h,w\n\t            imgs_out.append(np.stack(imgs,0)) # an,shape,shape,3\n\t            masks_out.append(np.stack(masks,0)) # an,shape,shape\n\t        imgs_out = np.stack(imgs_out,1).transpose([0,1,4,2,3]) # an,rfn,3,h,w\n\t        masks_out = np.stack(masks_out,1)[:,:,None,:,:]        # an,rfn,1,h,w\n\t        return imgs_out, masks_out\n\t    @staticmethod\n", "    def que_ref_scale_rotation_from_poses(center, ref_imgs_info, que_imgs_info):\n\t        \"\"\"\n\t        call this function before to torch\n\t        @param center: used in\n\t        @param ref_imgs_info:\n\t        @param que_imgs_info:\n\t        @return:\n\t            1. transformation from reference to query\n\t               rotate(scale(ref)) = que\n\t            2. gt_ref_ids [qn,] the ground truth reference view index for every query view\n", "        \"\"\"\n\t        ref_poses = ref_imgs_info['poses']\n\t        que_poses = que_imgs_info['poses']\n\t        ref_Ks = ref_imgs_info['Ks']\n\t        que_Ks = que_imgs_info['Ks']\n\t        # select nearest views\n\t        corr = compute_normalized_view_correlation(que_poses, ref_poses, center, False)\n\t        gt_ref_ids = np.argmax(corr, 1)  # qn\n\t        scale_diff, rotation_diff = scale_rotation_difference_from_cameras(\n\t            ref_poses[gt_ref_ids], que_poses, ref_Ks[gt_ref_ids], que_Ks, center) # from ref to que\n", "        ref_scales = np.asarray([transformation_decompose_scale(M) for M in ref_imgs_info['Ms'][gt_ref_ids]])\n\t        que_scales = np.asarray([transformation_decompose_scale(M) for M in que_imgs_info['Ms']])\n\t        ref_rotations = np.asarray([transformation_decompose_rotation(M) for M in ref_imgs_info['Ms'][gt_ref_ids]])\n\t        que_rotations = np.asarray([transformation_decompose_rotation(M) for M in que_imgs_info['Ms']])\n\t        scale = scale_diff * que_scales / ref_scales\n\t        rotation = -ref_rotations + rotation_diff + que_rotations\n\t        return scale, rotation, gt_ref_ids, corr\n\t    @staticmethod\n\t    def get_back_imgs(fn_list, qn,h,w):\n\t        back_imgs = []\n", "        for qi in range(qn):\n\t            fn = fn_list[np.random.randint(0, len(fn_list))]\n\t            back_img = get_background_image_coco(fn, h, w)  # h,w,3\n\t            back_img = torch.from_numpy(color_map_forward(back_img)).permute(2, 0, 1)\n\t            back_imgs.append(back_img)\n\t        back_imgs = torch.stack(back_imgs, 0)  # qn,3,h,w\n\t        return back_imgs\n\t    def _build_real_ref_imgs_info(self, database):\n\t        # load ref img info\n\t        ref_ids_all = database.get_img_ids()\n", "        res = self.cfg['resolution']\n\t        ref_num = self.cfg['reference_num']\n\t        angles = np.deg2rad(np.asarray(self.cfg['selector_angles']))\n\t        ref_ids = select_reference_img_ids_fps(database, ref_ids_all, ref_num, self.is_train)\n\t        imgs, masks, Ks, poses, Hs, ref_imgs = \\\n\t            normalize_reference_views(database, ref_ids, res, 0.05, add_rots=True, rots_list=angles)\n\t        # imgs, masks, Ks, poses, Hs, ref_ids, ref_imgs = select_reference_views(\n\t        #     database, ref_ids_all, ref_num, res, 0.05, self.is_train, True, angles)\n\t        rfn = imgs.shape[0]\n\t        M = np.concatenate([np.eye(2), np.zeros([2, 1])], 1)\n", "        object_center = get_object_center(database)\n\t        cens = np.asarray([project_points(object_center[None], pose, K)[0][0] for pose, K in zip(poses, Ks)],np.float32)\n\t        ref_imgs_info = {\n\t            'imgs': color_map_forward(imgs).transpose([0,3,1,2]),\n\t            'masks': np.ones([rfn,1,128,128],dtype=np.float32),\n\t            'ref_ids': [None for _ in range(rfn)],\n\t            'Ms': np.repeat(M[None,:], rfn, 0),\n\t            'poses': poses.astype(np.float32),\n\t            'Ks': Ks.astype(np.float32),\n\t            'cens': cens, # rfn, 2\n", "        }\n\t        ref_masks = None\n\t        # an,rfn,h,w,3\n\t        ref_imgs = color_map_forward(ref_imgs).transpose([0,1,4,2,3]) # an,rfn,3,h,w\n\t        return ref_imgs_info, ref_imgs, ref_masks\n\t    def _build_real_que_imgs_info(self, database, que_ids, center_np, ref_poses, ref_Ks, size):\n\t        # load que imgs info\n\t        outputs = [[] for _ in range(8)]\n\t        for qi in range(len(que_ids)):\n\t            img_id = que_ids[qi]\n", "            que_img = database.get_image(img_id)\n\t            que_pose = database.get_pose(img_id)\n\t            que_K = database.get_K(img_id)\n\t            que_cen = project_points(center_np[None],que_pose, que_K)[0][0]\n\t            ref_vp_score = compute_normalized_view_correlation(que_pose[None], ref_poses, center_np, False)[0]\n\t            gt_ref_id = np.argmax(ref_vp_score)  # qn\n\t            scale_r2q, angle_r2q = scale_rotation_difference_from_cameras(ref_poses[gt_ref_id[None]], que_pose[None],\n\t                                                                          ref_Ks[gt_ref_id[None]], que_K[None], center_np)\n\t            scale_r2q, angle_r2q = scale_r2q[0], angle_r2q[0]\n\t            if self.cfg['selector_real_aug']:\n", "                scale_aug = 2 ** np.random.uniform(*self.cfg['selector_scale_range'])\n\t                rotation_aug = np.deg2rad(np.random.uniform(*self.cfg['selector_angle_range']))\n\t                que_img, M = transformation_crop(que_img, que_cen, 1/scale_r2q * scale_aug, -angle_r2q + rotation_aug, size)\n\t                scale_r2q, angle_r2q = scale_aug, rotation_aug\n\t            else:\n\t                que_img, M = transformation_crop(que_img, que_cen, 1/scale_r2q, 0, size)\n\t                scale_r2q = 1.0 # we only rescale here\n\t            que_cen = transformation_apply_2d(M, que_cen[None])[0]\n\t            # que_imgs, que_poses, que_Ks, que_cens, angles_r2q, scales_r2q, ref_vp_scores, gt_ref_ids\n\t            data = [que_img, que_pose, que_K, que_cen, angle_r2q, scale_r2q, ref_vp_score, gt_ref_id]\n", "            for output, item in zip(outputs, data):\n\t                output.append(np.asarray(item))\n\t        for k in range(len(outputs)):\n\t            outputs[k] = np.stack(outputs[k], 0)\n\t        que_imgs, que_poses, que_Ks, que_cens, angles_r2q, scales_r2q, ref_vp_scores, gt_ref_ids = outputs\n\t        que_imgs_info = {\n\t            'imgs': color_map_forward(que_imgs).transpose([0,3,1,2]), # qn,3,h,w\n\t            'poses': que_poses.astype(np.float32), # qn,3,4\n\t            'Ks': que_Ks.astype(np.float32), # qn,3,3\n\t            'cens': que_cens.astype(np.float32), # qn,2\n", "        }\n\t        ref_vp_scores = ref_vp_scores.astype(np.float32) # qn, rfn\n\t        angles_r2q = angles_r2q.astype(np.float32) # qn\n\t        scales_r2q = scales_r2q.astype(np.float32) # qn\n\t        gt_ref_ids = gt_ref_ids.astype(np.int64) # qn\n\t        return que_imgs_info, angles_r2q, scales_r2q, ref_vp_scores, gt_ref_ids\n\t    def __getitem__(self, index):\n\t        set_seed(index,self.is_train)\n\t        database, que_ids = self._select_query(index)\n\t        if database.database_name.startswith('linemod'):\n", "            object_center = get_object_center(database)\n\t            ref_imgs_info, ref_imgs, ref_masks = self._build_real_ref_imgs_info(database)\n\t            que_imgs_info, angles_r2q, scales_r2q, ref_vp_scores, gt_ref_ids = \\\n\t                self._build_real_que_imgs_info(database, que_ids, object_center, ref_imgs_info['poses'], ref_imgs_info['Ks'], 128)\n\t        elif database.database_name.startswith('genmop'):\n\t            ref_database = self.name2database[database.database_name.replace('test','ref')]\n\t            object_center = get_object_center(database)\n\t            ref_imgs_info, ref_imgs, ref_masks = self._build_real_ref_imgs_info(ref_database)\n\t            que_imgs_info, angles_r2q, scales_r2q, ref_vp_scores, gt_ref_ids = \\\n\t                self._build_real_que_imgs_info(database, que_ids, object_center, ref_imgs_info['poses'], ref_imgs_info['Ks'], 128)\n", "        else:\n\t            ref_ids = get_ref_ids(database, self.cfg['ref_type'])\n\t            ref_imgs_info = self._build_ref_imgs_info(database, ref_ids)\n\t            que_imgs_info = self._build_ref_imgs_info(database, que_ids)\n\t            ref_imgs_info.pop('ref_ids')\n\t            que_imgs_info.pop('ref_ids')\n\t            # add transformation for query images\n\t            que_imgs_info = self.geometric_augment_que(que_imgs_info)\n\t            # add transformation for reference images\n\t            ref_imgs, ref_masks = self.geometric_augment_ref(\n", "                ref_imgs_info['imgs'],ref_imgs_info['masks'],self.cfg['selector_angles']) # an,rfn,_,h,w\n\t            # compute scale and rotation difference\n\t            center = get_object_center(database)\n\t            scales_r2q, angles_r2q, gt_ref_ids, ref_vp_scores = self.\\\n\t                que_ref_scale_rotation_from_poses(center, ref_imgs_info, que_imgs_info)\n\t            ref_masks = torch.from_numpy(ref_masks.astype(np.float32))\n\t        # to torch tensor\n\t        ref_imgs = torch.from_numpy(ref_imgs.astype(np.float32))\n\t        scales_r2q = torch.from_numpy(scales_r2q.astype(np.float32))\n\t        angles_r2q = torch.from_numpy(angles_r2q.astype(np.float32))\n", "        ref_vp_scores = torch.from_numpy(ref_vp_scores.astype(np.float32))\n\t        gt_ref_ids = torch.from_numpy(gt_ref_ids.astype(np.int64))\n\t        ref_imgs_info = imgs_info_to_torch(ref_imgs_info)\n\t        que_imgs_info = imgs_info_to_torch(que_imgs_info)\n\t        if database.database_name.startswith('linemod') or \\\n\t                database.database_name.startswith('genmop'):\n\t            pass\n\t        else:\n\t            # add background to que imgs\n\t            qn, _, h, w = que_imgs_info['imgs'].shape\n", "            back_imgs = self.get_back_imgs(self.background_img_list, qn, h, w)\n\t            que_imgs_info['imgs'] = back_imgs * (1 - que_imgs_info['masks']) + que_imgs_info['imgs'] * que_imgs_info['masks']\n\t            # add background to ref imgs\n\t            an, rfn, _, h, w = ref_imgs.shape\n\t            if np.random.random() < 0.5:\n\t                back_imgs = self.get_back_imgs(self.background_img_list, 1, h, w).unsqueeze(0)\n\t            else:\n\t                back_imgs = self.get_back_imgs(self.background_img_list, rfn, h, w).unsqueeze(0)\n\t            ref_imgs = back_imgs * (1 - ref_masks) + ref_imgs * ref_masks\n\t        # add photometric augmentation\n", "        self._photometric_augment(que_imgs_info, 0.8)\n\t        an, rfn, _, h, w = ref_imgs.shape\n\t        ref_imgs = self._photometric_augment_imgs(ref_imgs.reshape(an*rfn, 3, h, w), 0.5)\n\t        ref_imgs = ref_imgs.reshape(an, rfn, 3, h, w)\n\t        object_center, object_vert = get_object_center(database), get_object_vert(database)\n\t        object_center, object_vert = torch.from_numpy(object_center.astype(np.float32)), torch.from_numpy(object_vert.astype(np.float32))\n\t        # self.check(que_imgs_info, ref_imgs, ref_vp_scores, angles_r2q, index)\n\t        return {'ref_imgs_info': ref_imgs_info, 'que_imgs_info': que_imgs_info, 'ref_imgs': ref_imgs,\n\t                'scales_r2q': scales_r2q, 'angles_r2q': angles_r2q, 'gt_ref_ids': gt_ref_ids, 'ref_vp_scores': ref_vp_scores,\n\t                'object_center': object_center, 'object_vert': object_vert}\n", "class SelectionValDataset(Dataset):\n\t    default_cfg={\n\t        \"test_database_name\": 'linemod/cat',\n\t        \"ref_database_name\": 'linemod/cat',\n\t        \"test_split_type\": \"linemod_val\",\n\t        \"ref_split_type\": \"linemod_val\",\n\t        \"selector_ref_num\": 32,\n\t        \"selector_ref_res\": 128,\n\t        'selector_angles': [-90, -45, 0, 45, 90],\n\t    }\n", "    def __init__(self, cfg, is_train):\n\t        self.cfg = {**self.default_cfg, **cfg}\n\t        super().__init__()\n\t        assert(not is_train)\n\t        self.test_database = parse_database_name(self.cfg['test_database_name'])\n\t        self.ref_database = parse_database_name(self.cfg['ref_database_name'])\n\t        ref_ids, _ = get_database_split(self.ref_database,self.cfg['ref_split_type'])\n\t        _, self.test_ids = get_database_split(self.test_database,self.cfg['test_split_type'])\n\t        rots = np.deg2rad(self.cfg['selector_angles'])\n\t        ref_ids = select_reference_img_ids_fps(self.ref_database, ref_ids, self.cfg['selector_ref_num'], False)\n", "        ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs, ref_imgs_rots = normalize_reference_views(\n\t            self.ref_database, ref_ids, self.cfg['selector_ref_res'], 0.05, add_rots=True, rots_list=rots)\n\t        self.ref_info={\n\t            'poses': torch.from_numpy(ref_poses.astype(np.float32)),\n\t            'Ks': torch.from_numpy(ref_Ks.astype(np.float32)),\n\t            'imgs': torch.from_numpy(color_map_forward(ref_imgs)).permute(0,3,1,2)\n\t        }\n\t        # self.ref_pc = get_ref_point_cloud(self.ref_database).astype(np.float32)\n\t        self.center = get_object_center(self.ref_database).astype(np.float32)\n\t        self.res = self.cfg['selector_ref_res']\n", "        self.ref_imgs_rots = torch.from_numpy(color_map_forward(ref_imgs_rots)).permute(0,1,4,2,3)\n\t    def __getitem__(self, index):\n\t        ref_imgs_info = self.ref_info.copy()\n\t        img_id = self.test_ids[index]\n\t        # get query information\n\t        que_img = self.test_database.get_image(img_id)\n\t        center_np = self.center\n\t        que_poses = self.test_database.get_pose(img_id)[None,]\n\t        que_Ks = self.test_database.get_K(img_id)[None,]\n\t        que_cen = project_points(center_np[None],que_poses[0],que_Ks[0])[0][0]\n", "        # reference information\n\t        ref_poses = ref_imgs_info['poses'].numpy()\n\t        ref_Ks = ref_imgs_info['Ks'].numpy()\n\t        ref_vp_scores = compute_normalized_view_correlation(que_poses, ref_poses, center_np, False)\n\t        gt_ref_ids = np.argmax(ref_vp_scores, 1)  # qn\n\t        scales_r2q, angles_r2q = scale_rotation_difference_from_cameras(ref_poses[gt_ref_ids], que_poses, ref_Ks[gt_ref_ids], que_Ks, center_np)\n\t        an, rfn, _, h, w = self.ref_imgs_rots.shape\n\t        que_img, _ = transformation_crop(que_img, que_cen, 1/scales_r2q[0], 0, h)\n\t        que_imgs_info = {\n\t            'imgs': torch.from_numpy(color_map_forward(que_img)[None]).permute(0,3,1,2),\n", "            # 'poses': torch.from_numpy(que_poses.astype(np.float32)),\n\t            # 'Ks': torch.from_numpy(que_Ks.astype(np.float32)),\n\t            # 'cens': torch.from_numpy(np.asarray([h/2,w/2],np.float32)[None]),\n\t        }\n\t        scales_r2q = torch.from_numpy(scales_r2q.astype(np.float32))\n\t        angles_r2q = torch.from_numpy(angles_r2q.astype(np.float32))\n\t        gt_ref_ids = torch.from_numpy(gt_ref_ids.astype(np.int64))\n\t        ref_vp_scores = torch.from_numpy(ref_vp_scores.astype(np.float32))\n\t        object_center, object_vert = get_object_center(self.test_database), get_object_vert(self.test_database)\n\t        object_center, object_vert = torch.from_numpy(object_center.astype(np.float32)), torch.from_numpy(object_vert.astype(np.float32))\n", "        # ViewSelectionGen6DDatasetV2.check(que_imgs_info,self.ref_imgs_rots,ref_vp_scores,angles_r2q,index)\n\t        return {'que_imgs_info': que_imgs_info, 'ref_imgs_info': ref_imgs_info, 'ref_imgs': self.ref_imgs_rots,\n\t                'gt_ref_ids': gt_ref_ids, 'scales_r2q': scales_r2q, 'angles_r2q': angles_r2q, 'ref_vp_scores': ref_vp_scores,\n\t                \"object_center\": object_center, \"object_vert\": object_vert}\n\t    def __len__(self):\n\t        return len(self.test_ids)\n\tclass RefinerTrainDataset(Gen6DTrainDataset):\n\t    refine_default_cfg={\n\t        \"batch_size\": 1,\n\t        \"refine_scale_range\": [-0.3, 0.3],\n", "        \"refine_rotation_range\": [-15, 15],\n\t        \"refine_offset_std\": 4,\n\t        \"refine_ref_num\": 6,\n\t        \"refine_resolution\": 128,\n\t        \"refine_view_cfg\": \"v0\",\n\t        \"refine_ref_ids_version\": \"all\",\n\t    }\n\t    def __init__(self, cfg, is_train):\n\t        cfg = {**self.refine_default_cfg, **cfg}\n\t        super().__init__(cfg, is_train)\n", "    def get_view_config(self, database_name):\n\t        gso_config={\n\t            'select_max': 16,\n\t            'ref_select_max': 24,\n\t        }\n\t        shapenet_config={\n\t            'select_max': 24,\n\t            'ref_select_max': 32,\n\t        }\n\t        linemod_config = {\n", "            \"select_max\": 16,\n\t            \"ref_select_max\": 32,\n\t        }\n\t        genmop_config = {\n\t            \"select_max\": 16,\n\t            \"ref_select_max\": 32,\n\t        }\n\t        if database_name.startswith('shapenet'):\n\t            return shapenet_config\n\t        elif database_name.startswith('gso'):\n", "            return gso_config\n\t        elif database_name.startswith('linemod'):\n\t            return linemod_config\n\t        elif database_name.startswith('genmop'):\n\t            return genmop_config\n\t        elif database_name.startswith('norm'):\n\t            return self.get_view_config('/'.join(database_name.split('/')[1:]))\n\t        else:\n\t            raise NotImplementedError\n\t    @staticmethod\n", "    def approximate_rigid_to_similarity(pose_src, pose_tgt, K_src, K_tgt, center):\n\t        f_tgt = (K_tgt[0, 0] + K_tgt[1, 1]) / 2\n\t        f_src = (K_src[0, 0] + K_src[1, 1]) / 2\n\t        cen_src = transform_points_pose(center[None], pose_src)[0] # [3]\n\t        cen_tgt = transform_points_pose(center[None], pose_tgt)[0]\n\t        # scale\n\t        scale = cen_src[2] / cen_tgt[2] *  f_tgt / f_src\n\t        # offset\n\t        offset = (cen_tgt - cen_src)[:,None]\n\t        offset[2,0] = 0 # note: we only consider 2D offset here\n", "        offset = scale * offset\n\t        # rotation\n\t        pose = pose_compose(pose_inverse(pose_src), pose_tgt)\n\t        rot = pose[:3,:3]\n\t        # combine\n\t        offset = offset + (cen_src[:,None] - scale * rot @ cen_src[:,None])\n\t        sim = np.concatenate([scale * rot, offset],1)\n\t        return sim\n\t    @staticmethod\n\t    def decomposed_transformations(pose_in, pose_sim, object_center):\n", "        cen0 = pose_apply(pose_in, object_center)\n\t        cen1 = pose_apply(pose_sim, cen0)\n\t        offset = cen1 - cen0\n\t        U, S, V = np.linalg.svd(pose_sim[:, :3])\n\t        rotation = mat2quat(U @ V)\n\t        scale = np.mean(np.abs(S))\n\t        return scale, rotation, offset\n\t    def _select_query_input_id(self, index):\n\t        # select\n\t        que_database, que_id = self._select_query(index)\n", "        que_id = que_id[0]\n\t        que_pose = que_database.get_pose(que_id)\n\t        view_cfg = self.get_view_config(que_database.database_name)\n\t        if que_database.database_name.startswith('gen6d'):\n\t            ref_database = self.name2database[que_database.database_name.replace('test','ref')]\n\t        else:\n\t            ref_database = que_database\n\t        input_ids = ref_database.get_img_ids()\n\t        input_ids = np.asarray(input_ids)\n\t        input_poses = np.stack([ref_database.get_pose(input_id) for input_id in input_ids],0).astype(np.float32)\n", "        object_center = get_object_center(que_database)\n\t        corr = compute_normalized_view_correlation(que_pose[None], input_poses, object_center, False)[0] # rfn\n\t        near_idx = np.argsort(-corr)\n\t        near_idx = near_idx[:view_cfg['select_max']]\n\t        near_idx = near_idx[np.random.randint(0,near_idx.shape[0])]\n\t        input_id = input_ids[near_idx]\n\t        return que_database, ref_database, que_id, input_id\n\t    def _get_que_imgs_info(self, que_database, ref_database, que_id, input_id, margin=0.05):\n\t        que_img = que_database.get_image(que_id)\n\t        que_mask = que_database.get_mask(que_id).astype(np.float32)\n", "        que_pose = que_database.get_pose(que_id)\n\t        que_K = que_database.get_K(que_id)\n\t        object_center = get_object_center(que_database)\n\t        object_diameter = get_diameter(que_database)\n\t        # augmentation parameters\n\t        scale_aug = 2 ** np.random.uniform(*self.cfg['refine_scale_range'])\n\t        angle_aug = np.deg2rad(np.random.uniform(*self.cfg['refine_rotation_range']))\n\t        offset_aug = np.random.normal(0, self.cfg['refine_offset_std'], 2).astype(np.float32)\n\t        # we need to rescale the input image to the target size\n\t        size = self.cfg['refine_resolution']\n", "        input_pose, input_K = ref_database.get_pose(input_id), ref_database.get_K(input_id)\n\t        is_synthetic = False\n\t        if not is_synthetic:\n\t            # compute the scale to correct input to a fixed size\n\t            # let the input pose and input K look at the obejct\n\t            input_dist = np.linalg.norm(pose_inverse(input_pose)[:,3] - object_center[None,], 2)\n\t            input_rot_look, input_focal_look = let_me_look_at(input_pose, input_K, object_center)\n\t            input_pose = pose_compose(input_pose, np.concatenate([input_rot_look,np.zeros([3,1])],1)) # new input pose\n\t            input_focal_new = size * (1 - margin) / object_diameter * input_dist\n\t            input_K = np.diag([input_focal_new,input_focal_new,1.0])\n", "            input_K[:,2] = np.asarray([size/2, size/2, 1.0]) # new input K\n\t            scale_diff, angle_diff = scale_rotation_difference_from_cameras(\n\t                input_pose[None], que_pose[None], input_K[None], que_K[None], object_center)\n\t            scale_diff, angle_diff = scale_diff[0], angle_diff[0] # input to query\n\t            # rotation\n\t            que_cen = project_points(object_center, que_pose, que_K)[0][0]\n\t            R_new, f_new = let_me_look_at_2d(que_cen + offset_aug, que_K)\n\t            angle = angle_aug - angle_diff\n\t            R_z = np.asarray([[np.cos(angle), -np.sin(angle), 0], [np.sin(angle), np.cos(angle), 0], [0, 0, 1]], np.float32)\n\t            R_new = R_z @ R_new\n", "            # scale\n\t            f_new = f_new * scale_aug / scale_diff\n\t            que_K_warp = np.asarray([[f_new,0,size/2],[0,f_new,size/2],[0,0,1]],np.float32)\n\t            # warp image\n\t            H = que_K_warp @ R_new @ np.linalg.inv(que_K)\n\t            que_img_warp = cv2.warpPerspective(que_img, H, (size, size), flags=cv2.INTER_LINEAR)\n\t            que_mask_warp = cv2.warpPerspective(que_mask.astype(np.float32), H, (size, size), flags=cv2.INTER_LINEAR)\n\t            # compute ground-truth pose of the warped image and similarity pose\n\t            pose_rect = np.concatenate([R_new,np.zeros([3,1])],1).astype(np.float32)\n\t            que_pose_warp = pose_compose(que_pose, pose_rect)\n", "            # todo: the approximation is not accurate, if the the object is close to the camera. so we use different codes for synthetic data\n\t            poses_sim_in_to_warp = self.approximate_rigid_to_similarity(input_pose, que_pose_warp, input_K, que_K_warp, object_center)\n\t        else:\n\t            raise NotImplementedError\n\t            # scale_diff, angle_diff = scale_rotation_difference_from_cameras(\n\t            #     input_pose[None], que_pose[None], input_K[None], que_K[None], object_center)\n\t            # scale_diff, angle_diff = scale_diff[0], angle_diff[0]\n\t            #\n\t            # que_cen, que_depth = project_points(object_center, que_pose, que_K)\n\t            # que_cen, que_depth = que_cen[0], que_depth[0]\n", "            # input_cen, input_depth = project_points(object_center, input_pose, input_K)\n\t            # input_cen, input_depth = input_cen[0], input_depth[0]\n\t            #\n\t            # M = transformation_offset_2d(-que_cen[0], -que_cen[1])\n\t            # M = transformation_compose_2d(M, transformation_scale_2d(1 / scale_diff))\n\t            # M = transformation_compose_2d(M, transformation_rotation_2d(-angle_diff))\n\t            # M = transformation_compose_2d(M, transformation_offset_2d(offset_aug[0], offset_aug[1]))\n\t            # M = transformation_compose_2d(M, transformation_scale_2d(scale_aug))\n\t            # M = transformation_compose_2d(M, transformation_rotation_2d(angle_aug))\n\t            # M = transformation_compose_2d(M, transformation_offset_2d(input_cen[0], input_cen[1]))\n", "            #\n\t            # que_img_warp = cv2.warpAffine(que_img, M, (size, size), flags=cv2.INTER_LINEAR)\n\t            # que_mask_warp = cv2.warpAffine(que_mask.astype(np.float32), M, (size, size), flags=cv2.INTER_LINEAR)\n\t            # H = np.identity(3)\n\t            # H[:2,:3] = M\n\t            #\n\t            # # compute the pose similarity transformation\n\t            # # rotation\n\t            # angle = angle_aug - angle_diff\n\t            # R_z = np.asarray([[np.cos(angle), -np.sin(angle), 0], [np.sin(angle), np.cos(angle), 0], [0, 0, 1]], np.float32)\n", "            # R0, R1 = que_pose[:, :3], input_pose[:, :3]\n\t            # rotation_i2q = R_z @ R0 @ R1.T\n\t            #\n\t            # # scale\n\t            # scale_i2q = scale_aug\n\t            #\n\t            # # offset\n\t            # que_cen_ = transformation_apply_2d(M, que_cen[None])[0]\n\t            # offset2d_i2q = que_cen_ - input_cen\n\t            # input_f = np.mean(np.diag(input_K)[:2])\n", "            # offset2d_i2q = offset2d_i2q / input_f * input_depth\n\t            # offset2d_i2q = np.append(offset2d_i2q, 0).astype(np.float32)\n\t            #\n\t            # ref_obj_cen = pose_apply(input_pose, object_center)\n\t            # offset2d_i2q = offset2d_i2q + ref_obj_cen - scale_i2q * rotation_i2q @ ref_obj_cen\n\t            # poses_sim_in_to_warp = np.concatenate([scale_i2q * rotation_i2q, offset2d_i2q[:, None]], 1).astype(np.float32)\n\t        que_imgs_info={\n\t            # warp pose info\n\t            'imgs': color_map_forward(que_img_warp).transpose([2, 0, 1]),  # h,w,3\n\t            'masks': que_mask_warp[None].astype(np.float32),  # 1,h,w\n", "            \"Ks\": que_K_warp.astype(np.float32),\n\t            \"poses\": que_pose_warp.astype(np.float32),\n\t            # input pose info\n\t            'Hs': H.astype(np.float32), # 3,3\n\t            'Ks_in': input_K.astype(np.float32), # 3,3\n\t            'poses_in': input_pose.astype(np.float32), # 3,4\n\t            'poses_sim_in_to_que': np.asarray(poses_sim_in_to_warp,np.float32), # 3,4\n\t        }\n\t        # compute decomposed transformations\n\t        scale, rotation, offset = self.decomposed_transformations(input_pose, poses_sim_in_to_warp, object_center)\n", "        return que_imgs_info, scale, rotation, offset\n\t    def _get_ref_imgs_info(self, database, input_pose, input_K, is_synthetic, margin=0.05):\n\t        if self.cfg['refine_ref_ids_version']=='all':\n\t            img_ids = np.asarray(database.get_img_ids())\n\t        elif self.cfg['refine_ref_ids_version']=='fps':\n\t            img_ids = select_reference_img_ids_fps(database, database.get_img_ids(), 128, self.is_train)\n\t        else:\n\t            raise NotImplementedError\n\t        ref_poses_all = np.asarray([database.get_pose(ref_id) for ref_id in img_ids])\n\t        view_cfg = self.get_view_config(database.database_name)\n", "        # select reference ids from input pose\n\t        object_center = get_object_center(database)\n\t        corr = compute_normalized_view_correlation(input_pose[None], ref_poses_all, object_center, False)\n\t        ref_idxs = np.argsort(-corr[0])\n\t        ref_idxs = ref_idxs[:view_cfg['ref_select_max']]\n\t        np.random.shuffle(ref_idxs)\n\t        ref_idxs = ref_idxs[:self.cfg['refine_ref_num']]\n\t        ref_ids = img_ids[ref_idxs]\n\t        size = self.cfg['refine_resolution']\n\t        if is_synthetic:\n", "            raise NotImplementedError\n\t            # ref_imgs = [database.get_image(ref_id) for ref_id in ref_ids]\n\t            # ref_masks = [database.get_mask(ref_id) for ref_id in ref_ids]\n\t            # ref_Ks = [database.get_K(ref_id) for ref_id in ref_ids]\n\t            # ref_poses = [database.get_pose(ref_id) for ref_id in ref_ids]\n\t            # ref_poses, ref_Ks, ref_imgs, ref_masks = CostVolumeRefineEvalDataset.rectify_inplane_rotation(\n\t            #     input_pose, input_K, object_center, ref_poses, ref_Ks, ref_imgs, ref_masks)\n\t        else:\n\t            # if it is a real database, we need to re-scale them to the same size.\n\t            ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs = normalize_reference_views(\n", "                database, ref_ids, size, margin, True, input_pose, input_K)\n\t        ref_imgs_info={\n\t            'imgs': color_map_forward(np.stack(ref_imgs,0)).transpose([0,3,1,2]), # rfn,3,h,w\n\t            'masks': np.stack(ref_masks, 0).astype(np.float32)[:,None,:,:], # rfn,1,h,w\n\t            'poses': np.stack(ref_poses, 0).astype(np.float32),\n\t            'Ks': np.stack(ref_Ks, 0).astype(np.float32),\n\t        }\n\t        return ref_imgs_info\n\t    @staticmethod\n\t    def add_ref_background(ref_imgs, ref_masks, background_img_list):\n", "        \"\"\"\n\t        @param ref_imgs: rfn,3,h,w\n\t        @param ref_masks: rfn,1,h,w\n\t        @param background_img_list:\n\t        @return:\n\t        \"\"\"\n\t        same_background_prob = 0.4\n\t        if np.random.random()<0.95:\n\t            rfn, _, h, w = ref_imgs.shape\n\t            if np.random.random() < same_background_prob:\n", "                fn = background_img_list[np.random.randint(0, len(background_img_list))]\n\t                back_imgs = get_background_image_coco(fn, h, w)\n\t                back_imgs = color_map_forward(back_imgs).transpose([2,0,1])[None,:]\n\t            else:\n\t                rfn = ref_imgs.shape[0]\n\t                back_imgs = []\n\t                for rfi in range(rfn):\n\t                    fn = background_img_list[np.random.randint(0, len(background_img_list))]\n\t                    back_img = get_background_image_coco(fn, h, w)\n\t                    back_img = color_map_forward(back_img).transpose([2,0,1])\n", "                    back_imgs.append(back_img)\n\t                back_imgs = np.stack(back_imgs, 0)\n\t            ref_imgs = ref_imgs * ref_masks + back_imgs * (1 - ref_masks)\n\t        return ref_imgs\n\t    @staticmethod\n\t    def add_que_background(que_img, que_mask, background_img_list):\n\t        \"\"\"\n\t        @param que_img:  [3,h,w]\n\t        @param que_mask: [1,h,w]\n\t        @param background_img_list:\n", "        @return:\n\t        \"\"\"\n\t        _, h, w = que_img.shape\n\t        if np.random.random() < 0.95:\n\t            fn = background_img_list[np.random.randint(0, len(background_img_list))]\n\t            back_img = get_background_image_coco(fn, h, w)\n\t            back_img = color_map_forward(back_img).transpose([2,0,1])\n\t            que_img = que_img * que_mask + back_img * (1 - que_mask)\n\t        return que_img\n\t    def __getitem__(self, index):\n", "        set_seed(index, self.is_train)\n\t        que_database, ref_database, que_id, input_id = self._select_query_input_id(index)\n\t        is_synthetic = que_database.database_name.startswith('gso') or que_database.database_name.startswith('shapenet')\n\t        que_database = NormalizedDatabase(que_database)\n\t        ref_database = NormalizedDatabase(ref_database)\n\t        que_imgs_info, scale, rotation, offset = self._get_que_imgs_info(que_database, ref_database, que_id, input_id)\n\t        input_pose, input_K = que_imgs_info['poses_in'], que_imgs_info['Ks_in']\n\t        ref_imgs_info = self._get_ref_imgs_info(ref_database, input_pose, input_K, False)\n\t        object_center = get_object_center(que_database)\n\t        object_center = torch.from_numpy(object_center.astype(np.float32))\n", "        rotation = torch.from_numpy(np.asarray(rotation,np.float32))\n\t        scale = torch.from_numpy(np.asarray(scale,np.float32))\n\t        offset = torch.from_numpy(np.asarray(offset,np.float32))\n\t        # add background\n\t        if is_synthetic:\n\t            ref_imgs_info['imgs'] = self.add_ref_background(ref_imgs_info['imgs'],ref_imgs_info['masks'],self.background_img_list)\n\t            que_imgs_info['imgs'] = self.add_que_background(que_imgs_info['imgs'],que_imgs_info['masks'],self.background_img_list)\n\t        # pc = get_ref_point_cloud(que_database)\n\t        # CostVolumeRefineDataset.check(que_imgs_info, ref_imgs_info, pc, f'data/vis_val/{index}.jpg')\n\t        que_imgs_info = imgs_info_to_torch(que_imgs_info)\n", "        ref_imgs_info = imgs_info_to_torch(ref_imgs_info)\n\t        # add dataaugmentation\n\t        self._photometric_augment(que_imgs_info, 0.8)\n\t        self._photometric_augment(ref_imgs_info, 0.8)\n\t        return {'que_imgs_info': que_imgs_info, 'ref_imgs_info': ref_imgs_info, 'object_center': object_center,\n\t                'rotation': rotation, 'scale': scale, 'offset': offset}\n\tclass RefinerValDataset(Dataset):\n\t    default_cfg={\n\t        'ref_database_name': 'linemod/cat',\n\t        'ref_split_type': 'linemod_test',\n", "        'test_database_name': 'linemod/cat',\n\t        'test_split_type': 'linemod_test',\n\t        \"selector_name\": \"selector_train\",\n\t        \"detector_name\": \"detector_train\",\n\t        \"refine_ref_num\": 5,\n\t        \"refine_resolution\": 128,\n\t        \"refine_even_ref_views\": True,\n\t    }\n\t    def __init__(self, cfg, is_train):\n\t        self.cfg={**self.default_cfg, **cfg}\n", "        self.test_database = parse_database_name(self.cfg['test_database_name'])\n\t        self.ref_database = parse_database_name(self.cfg['ref_database_name'])\n\t        _, self.test_ids = get_database_split(self.test_database, self.cfg['test_split_type'])\n\t        self.ref_ids, _ = get_database_split(self.ref_database, self.cfg['ref_split_type'])\n\t        self.ref_ids, self.test_ids = np.asarray(self.ref_ids), np.asarray(self.test_ids)\n\t        self.img_id2det_info = read_pickle(f'data/val/det/{self.test_database.database_name}/{self.cfg[\"detector_name\"]}.pkl')\n\t        self.img_id2sel_info = read_pickle(f'data/val/sel/{self.test_database.database_name}/{self.cfg[\"detector_name\"]}-{self.cfg[\"selector_name\"]}.pkl')\n\t    def __getitem__(self, index):\n\t        que_id = self.test_ids[index]\n\t        test_database = NormalizedDatabase(self.test_database)\n", "        ref_database = NormalizedDatabase(self.ref_database)\n\t        que_img = test_database.get_image(que_id)\n\t        que_mask = test_database.get_mask(que_id)\n\t        que_pose = test_database.get_pose(que_id)\n\t        que_K = test_database.get_K(que_id)\n\t        center = get_object_center(ref_database)\n\t        res = self.cfg['refine_resolution']\n\t        det_position, det_scale_r2q, _ = self.img_id2det_info[que_id]\n\t        sel_angle_r2q, sel_pose, sel_K = self.img_id2sel_info[que_id]\n\t        # remember to normalize the pose !!!\n", "        sel_pose = normalize_pose(sel_pose, test_database.scale, test_database.offset)\n\t        que_img_warp, que_K_warp, que_pose_warp, que_pose_rect, H = look_at_crop(\n\t            que_img, que_K, que_pose, det_position, -sel_angle_r2q, 1/det_scale_r2q, res, res)\n\t        que_mask_warp = cv2.warpPerspective(que_mask.astype(np.float32), H, (res, res), flags=cv2.INTER_LINEAR)\n\t        poses_sim_in_to_warp = RefinerTrainDataset.approximate_rigid_to_similarity(\n\t            sel_pose, que_pose_warp, sel_K, que_K_warp, center)\n\t        pose_in_raw = estimate_pose_from_similarity_transform_compose(\n\t            det_position, det_scale_r2q, sel_angle_r2q, sel_pose, sel_K, que_K, center)\n\t        que_imgs_info={\n\t            # warp pose info\n", "            'imgs': color_map_forward(que_img_warp).transpose([2,0,1]),  # 3,h,w\n\t            'masks': que_mask_warp.astype(np.float32),  # 1,h,w\n\t            \"Ks\": que_K_warp.astype(np.float32),\n\t            \"poses\": que_pose_warp.astype(np.float32),\n\t            \"poses_rect\": np.asarray(que_pose_rect, np.float32),\n\t            # input pose info\n\t            'Hs': H.astype(np.float32), # 3,3\n\t            'Ks_in': sel_K.astype(np.float32), # 3,3\n\t            'poses_in': sel_pose.astype(np.float32), # 3,4\n\t            \"poses_sim_in_to_que\": poses_sim_in_to_warp.astype(np.float32),  # 3,4\n", "            # original image and pose info\n\t            'imgs_raw': color_map_forward(que_img).transpose([2,0,1]),  # 3,h,w\n\t            'masks_raw': que_mask[None].astype(np.float32),  # 1,h,w\n\t            'poses_raw': que_pose.astype(np.float32),  # 3,4\n\t            'Ks_raw': que_K.astype(np.float32),  # 3,3\n\t            'pose_in_raw': pose_in_raw.astype(np.float32)\n\t        }\n\t        scale, rotation, offset = RefinerTrainDataset.decomposed_transformations(sel_pose, poses_sim_in_to_warp, center)\n\t        rotation = torch.from_numpy(np.asarray(rotation,np.float32))\n\t        scale = torch.from_numpy(np.asarray(scale,np.float32))\n", "        offset = torch.from_numpy(np.asarray(offset,np.float32))\n\t        ref_ids = select_reference_img_ids_refinement(\n\t            ref_database, center, self.ref_ids, sel_pose, self.cfg['refine_ref_num'], self.cfg['refine_even_ref_views'])\n\t        size = self.cfg['refine_resolution']\n\t        margin = 0.05\n\t        ref_imgs, ref_masks, ref_Ks, ref_poses, ref_Hs = normalize_reference_views(ref_database, ref_ids, size, margin, True, sel_pose, sel_K)\n\t        # ref_poses, ref_Ks, ref_imgs, ref_masks = CostVolumeRefineEvalDataset.rectify_inplane_rotation(\n\t        #     sel_pose, sel_K, object_center, ref_poses, ref_Ks, ref_imgs, ref_masks)\n\t        ref_imgs_info={\n\t            'imgs': color_map_forward(np.stack(ref_imgs,0)).transpose([0,3,1,2]), # rfn,3,h,w\n", "            'masks': np.stack(ref_masks, 0).astype(np.float32)[:,None,:,:], # rfn,1,h,w\n\t            'poses': np.stack(ref_poses, 0).astype(np.float32),\n\t            'Ks': np.stack(ref_Ks, 0).astype(np.float32),\n\t        }\n\t        diameter = np.asarray(get_diameter(test_database),np.float32)\n\t        points = get_ref_point_cloud(test_database).astype(np.float32)\n\t        que_imgs_info = imgs_info_to_torch(que_imgs_info)\n\t        ref_imgs_info = imgs_info_to_torch(ref_imgs_info)\n\t        diameter = torch.from_numpy(diameter)\n\t        points = torch.from_numpy(points)\n", "        center = torch.from_numpy(center)\n\t        que_img_raw = test_database.get_image(que_id)\n\t        que_img_raw = torch.from_numpy(color_map_forward(que_img_raw)).permute(2,0,1).unsqueeze(0)\n\t        return {'que_imgs_info': que_imgs_info, 'ref_imgs_info':ref_imgs_info, 'object_diameter': diameter, 'object_points': points, 'object_center': center,\n\t                'que_img_raw': que_img_raw, 'que_id': que_id, 'database_name': self.test_database.database_name, 'rotation': rotation, 'scale': scale, 'offset': offset}\n\t    def __len__(self):\n\t        return len(self.test_ids)\n\tname2dataset={\n\t    'det_train': DetectionTrainDataset,\n\t    'det_val': DetectionValDataset,\n", "    'sel_train': SelectionTrainDataset,\n\t    'sel_val': SelectionValDataset,\n\t    'ref_train': RefinerTrainDataset,\n\t    'ref_val': RefinerValDataset,\n\t}"]}
