{"filename": "code_genie/_cache.py", "chunked_list": ["import json\n\timport logging\n\timport os\n\tfrom hashlib import blake2b\n\tfrom tempfile import mkdtemp\n\tfrom typing import Dict, List, Optional\n\tfrom pydantic import BaseModel\n\tlogger = logging.getLogger(__name__)\n\tclass _MetaValue(BaseModel):\n\t    id: str\n", "    instructions: List[str]\n\t    inputs: List[str]\n\t    class Config:\n\t        frozen = True\n\t    def toJson(self):\n\t        return json.dumps(self, default=lambda o: o.__dict__)\n\tclass _CacheValue(_MetaValue):\n\t    code: str\n\tclass _CacheManager:\n\t    DEFAULT_NAME = \"_genie_cache.json\"\n", "    META_NAME = \"_meta.json\"\n\t    DEFAULT_CACHE_DIR = mkdtemp()\n\t    def __init__(self, cache_dir: Optional[str] = None):\n\t        self.cache_dir, self.meta_path = self._check_cache_dir(cache_dir or self.DEFAULT_CACHE_DIR)\n\t    @classmethod\n\t    def _set_cache_dir(cls, cache_dir: str):\n\t        cls.DEFAULT_CACHE_DIR = cache_dir\n\t    @classmethod\n\t    def reload(cls, filepath: str):\n\t        return cls(filepath)\n", "    def _check_cache_dir(self, name: str):\n\t        # if dir doesn't exist, create it\n\t        if not os.path.exists(name):\n\t            os.makedirs(name)\n\t        return name, os.path.join(name, self.META_NAME)\n\t    @staticmethod\n\t    def _json_decoder(schema):\n\t        def decoder(obj):\n\t            try:\n\t                return schema.parse_obj(obj)\n", "            except:\n\t                return obj\n\t        return decoder\n\t    @staticmethod\n\t    def _json_encoder(obj):\n\t        try:\n\t            return obj.dict()\n\t        except:\n\t            return obj\n\t    def _load_meta(self) -> Dict[str, _MetaValue]:\n", "        if not os.path.exists(self.meta_path):\n\t            return {}\n\t        with open(self.meta_path, \"r\") as f:\n\t            meta_data: Dict[str, _MetaValue] = json.load(f, object_hook=self._json_decoder(_MetaValue))\n\t        return meta_data\n\t    def _load_code(self, id: str) -> str:\n\t        with open(self._get_filename(id), \"r\") as f:\n\t            return f.read()\n\t    @classmethod\n\t    def _consistent_hash(cls, value: str) -> str:\n", "        h = blake2b()\n\t        h.update(bytes(value, \"utf-8\"))\n\t        return h.hexdigest()\n\t    def _get_filename(self, id: str) -> str:\n\t        return os.path.join(self.cache_dir, f\"{id}.py\")\n\t    def update(self, key: str, value: _CacheValue):\n\t        _cache_meta = self._load_meta()\n\t        key_hash = self._consistent_hash(key)\n\t        # if key is present in, delete the file which is currently cashed\n\t        if key_hash in _cache_meta:\n", "            filename = self._get_filename(_cache_meta[key_hash].id)\n\t            if os.path.exists(filename):\n\t                os.remove(filename)\n\t        # add new cash entry\n\t        _cache_meta[key_hash] = value\n\t        # write to code file\n\t        with open(self._get_filename(value.id), \"w\") as f:\n\t            f.write(value.code)\n\t        # update metadata\n\t        with open(self.meta_path, \"w\") as f:\n", "            json.dump(_cache_meta, f, indent=4, default=self._json_encoder)\n\t    def get(self, key: str) -> Optional[_CacheValue]:\n\t        _cache_meta = self._load_meta()\n\t        meta = _cache_meta.get(self._consistent_hash(key), None)\n\t        if meta is not None:\n\t            try:\n\t                code = self._load_code(meta.id)\n\t                return _CacheValue(code=code, id=meta.id, instructions=meta.instructions, inputs=meta.inputs)\n\t            except FileNotFoundError:\n\t                return None\n", "        return None\n\t    def num_items(self):\n\t        return len(self._load_meta())\n\t    def get_all_code_segments(self) -> Dict[str, str]:\n\t        _cache_meta = self._load_meta()\n\t        code_segments = {}\n\t        for meta in _cache_meta.values():\n\t            code_segments[meta.id] = self._load_code(meta.id)\n\t        return code_segments\n"]}
{"filename": "code_genie/client.py", "chunked_list": ["import os\n\tfrom typing import Dict, List, Optional, Tuple, Union\n\tfrom uuid import UUID\n\timport requests\n\tfrom pydantic import BaseModel\n\tclass GetExecutableRequest(BaseModel):\n\t    instructions: Union[str, List[str]]\n\t    inputs: Dict[str, str]\n\t    allowed_imports: Optional[List[str]] = None\n\tclass GetPandasExecutableRequest(GetExecutableRequest):\n", "    inputs: Optional[Dict[str, str]] = None\n\t    columns: Optional[List[str]] = None\n\tclass GetExecutableResponse(BaseModel):\n\t    id: UUID\n\t    code: str\n\t    fn_name: str\n\tclass Client:\n\t    TOKEN_ENV_VAR = \"CODE_GENIE_TOKEN\"\n\t    URL = \"https://code-scribe-pzj44qvhfa-el.a.run.app\"\n\t    ENDPOINT = \"get-executable/generic\"\n", "    def __init__(self, token: Optional[str] = None):\n\t        self._token = token or os.environ[self.TOKEN_ENV_VAR]\n\t    def _get_response(self, endpoint, data):\n\t        headers = {\"token\": self._token, \"Content-Type\": \"application/json\"}\n\t        response = requests.post(url=f\"{self.URL}/{endpoint}\", data=data.json(), headers=headers)\n\t        # if error found, raise the error\n\t        response.raise_for_status()\n\t        return GetExecutableResponse.parse_obj(response.json())\n\t    def get(self, instructions: List[str], inputs: Dict[str, str]) -> str:\n\t        request = GetExecutableRequest(instructions=instructions, inputs=inputs, allowed_imports=[])\n", "        # send a request with given data\n\t        response = self._get_response(self.ENDPOINT, request)\n\t        return response.code\n"]}
{"filename": "code_genie/pipeline.py", "chunked_list": ["import json\n\timport os\n\timport time\n\tfrom typing import Any, Dict, List, Optional, TypeVar, Union\n\tfrom pydantic import BaseModel, validator\n\tfrom code_genie.genie import Genie, GenieResult\n\tfrom code_genie.io import (\n\t    BigQueryToDataframeSource,\n\t    BoolArg,\n\t    CsvToDataFrameSource,\n", "    DataFrameToCsvSink,\n\t    IntArg,\n\t    StringArg,\n\t)\n\tfrom code_genie.io.argument import GenieArgument\n\tfrom code_genie.io.base import GenieSource\n\tSource = TypeVar(\"Source\", CsvToDataFrameSource, BigQueryToDataframeSource)\n\tSink = TypeVar(\"Sink\", bound=DataFrameToCsvSink)\n\tArgument = TypeVar(\"Argument\", StringArg, IntArg, BoolArg)\n\tclass PipelineStep(BaseModel):\n", "    genie_result: GenieResult\n\t    \"\"\"Result of the genie which should be run in this step\"\"\"\n\t    data: Optional[Union[Source, GenieResult]] = None\n\t    \"\"\"Data to be passed to the genie for computation. This could be either a data source or a previous genie result\"\"\"\n\t    additional_inputs: Optional[Dict[str, Union[Source, GenieResult, Argument]]] = None\n\t    \"\"\"Set this value for each additional input to the genie. The dictionary key should be the name of the input\n\t    and the value could be one of the 3 things: \n\t    1. A genie data source\n\t    2. A genie result from a previous step\n\t    3. A constant value to be passed as an argument to the pipeline\"\"\"\n", "    sink: Optional[Sink] = None\n\t    \"\"\"If the output of this step needs to be exported, then a sink can be provided here\"\"\"\n\tclass GeniePipeline(BaseModel):\n\t    name: str\n\t    \"\"\"Name of the pipeline\"\"\"\n\t    version: str\n\t    \"\"\"Version of the pipeline\"\"\"\n\t    cache_dir: str\n\t    \"\"\"Directory where the genies being used in this pipeline are cached. The pipeline will also be cached at the \n\t    same location\"\"\"\n", "    steps: List[PipelineStep]\n\t    \"\"\"List of steps in the pipeline\"\"\"\n\t    def add(self, step: PipelineStep):\n\t        \"\"\"Add a step to the pipeline\"\"\"\n\t        return self.copy(update={\"steps\": self.steps + [step]})\n\t    @validator(\"steps\")\n\t    def _validate_steps(cls, v: List[PipelineStep]):\n\t        # base_input of the first step should be a genie source\n\t        if v[0].data is None:\n\t            raise ValueError(f\"base_input_source of the first step should be set, found None\")\n", "        # there should be atleast 1 sink\n\t        if all(step.sink is None for step in v):\n\t            raise ValueError(\"atleast one of the steps should have a sink; none found\")\n\t        return v\n\t    def _get_filepath(self, filename: str):\n\t        if not filename.endswith(\"json\"):\n\t            filename += \".json\"\n\t        return os.path.join(self.cache_dir, filename)\n\t    def export(self, filename: str):\n\t        \"\"\"Export the pipeline as a json file\"\"\"\n", "        with open(self._get_filepath(filename), \"w\") as f:\n\t            json.dump(self.dict(), f)\n\t    @classmethod\n\t    def load(cls, filepath: str):\n\t        \"\"\"Load the pipeline from a json file\"\"\"\n\t        with open(filepath, \"r\") as f:\n\t            return cls.parse_obj(json.load(f))\n\t    @classmethod\n\t    def _get_cached_genie_result(cls, step_id: str, genie_id: str, cached_results: Dict[str, GenieResult]):\n\t        if genie_id not in cached_results:\n", "            raise ValueError(\n\t                f\"Error in step id id: {step_id}; You are attempting to use the results of genie with id: {genie_id} \"\n\t                f\"in this step but the genie has not been run in a previous step. Add a step before this step in the \"\n\t                f\"pipeline to run genie id {genie_id}.\"\n\t            )\n\t        return cached_results[genie_id].result\n\t    def run(self, args: Dict[str, Any]):\n\t        \"\"\"Run the pipeline using the value of the arguments passed. Note that all arguments which do not have a\n\t        default value needs to be passed here for the pipeline to run.\"\"\"\n\t        cached_genie_results: Dict[str, GenieResult] = {}\n", "        for i, step in enumerate(self.steps):\n\t            print(f\"Running step {i+1}: {step.genie_result.id}\")\n\t            # initialize timer\n\t            start_time = time.time()\n\t            step_id = step.genie_result.id\n\t            # get the base input\n\t            if isinstance(step.data, GenieSource):\n\t                base_input = step.data.get(**args)\n\t            elif isinstance(step.data, GenieResult):\n\t                base_input = self._get_cached_genie_result(\n", "                    step_id=step_id, genie_id=step.data.id, cached_results=cached_genie_results\n\t                )\n\t            else:\n\t                raise ValueError(f\"Invalid type for base_input: {type(step.data)}\")\n\t            # get the additional inputs\n\t            additional_inputs = {}\n\t            for name, add_input in (step.additional_inputs or {}).items():\n\t                if isinstance(add_input, GenieSource):\n\t                    additional_inputs[name] = add_input.get(**args)\n\t                if isinstance(add_input, GenieResult):\n", "                    additional_inputs[name] = self._get_cached_genie_result(step_id, add_input.id, cached_genie_results)\n\t                if isinstance(add_input, GenieArgument):\n\t                    additional_inputs[name] = add_input.get(**args)\n\t            # run the genie\n\t            genie = Genie(data=base_input)\n\t            genie_result = genie.run(step.genie_result.code, additional_inputs)\n\t            cached_genie_results[step_id] = GenieResult(\n\t                id=step_id, result=genie_result, code=step.genie_result.code, cache_dir=self.cache_dir\n\t            )\n\t            # write the output\n", "            if step.sink is not None:\n\t                step.sink.put(genie_result, **args)\n\t            end_time = time.time()\n\t            print(f\"\\tCompleted in {end_time - start_time:.1f} seconds\")\n"]}
{"filename": "code_genie/genie.py", "chunked_list": ["import random\n\timport re\n\tfrom typing import Any, Callable, Dict, List, Optional, Union\n\timport pandas as pd\n\tfrom pydantic import BaseModel\n\tfrom code_genie._cache import _CacheManager, _CacheValue\n\tfrom code_genie.client import Client\n\tclass GenieResult(BaseModel):\n\t    \"\"\"The result of a genie execution\"\"\"\n\t    id: str\n", "    \"\"\"ID of the genie, this would also be the filename used for storing the generated code in the cache.\"\"\"\n\t    code: str\n\t    \"\"\"The code generated by the genie\"\"\"\n\t    cache_dir: str\n\t    \"\"\"The cache directory used by the genie\"\"\"\n\t    result: Any = None\n\t    \"\"\"The result of the execution; None if no result was returned\"\"\"\n\t    class Config:\n\t        # always exclude result from json export\n\t        fields = {\"result\": {\"exclude\": True}}\n", "        frozen = True\n\tclass Genie:\n\t    _hash_sep = \"::\"\n\t    def __init__(\n\t        self,\n\t        data: Optional[Any] = None,\n\t        client: Optional[Client] = None,\n\t        cache_dir: Optional[str] = None,\n\t        copy_data_before_use: bool = True,\n\t    ):\n", "        \"\"\"Initialize a genie instance\n\t        Args:\n\t            data: a base dataset whose attributes will be used to generate the code. the result will be determined\n\t                by running this data over the code\n\t            client: an instance of the client to use for making requests to the api. if not provided, a new instance\n\t                will be created.\n\t            cache_dir: if provided, the code generated by the genie will be cached in this directory. if not\n\t                provided, the global default is used. it is recommended to use set_cache_dir() method to set this.\n\t            copy_data_before_use: if True, the data will be copied before passing through generated code. this is to\n\t                prevent the data from being modified inplace by the code. the data passed should have a copy() method\n", "                implemented. if False, the data will be passed as is. this is faster but can lead to unexpected results\n\t        Returns:\n\t            A callable which can be used to execute the code generated by the genie.\n\t        \"\"\"\n\t        self.data = data\n\t        self._base_key = self._get_data_key(data)\n\t        self._cache = _CacheManager(cache_dir)\n\t        self.copy_data_before_use = copy_data_before_use\n\t        if copy_data_before_use:\n\t            # check data should have a copy method\n", "            if not hasattr(data, \"copy\"):\n\t                raise ValueError(\n\t                    \"data should have a copy method implemented if copy_data_before_use is True\",\n\t                    \"Set it to False if you want to continue using the genie\",\n\t                )\n\t        self._client = client or Client()\n\t    def plz(\n\t        self,\n\t        instructions: Optional[Union[str, List[str]]],\n\t        additional_inputs: Optional[Dict[str, Any]] = None,\n", "        override: bool = False,\n\t        update_base_input: bool = False,\n\t    ) -> GenieResult:\n\t        \"\"\"Generate code for a new task\n\t        Args:\n\t            instructions: text instructions on the task required to be performed. use the keywords in inputs argument\n\t                to refer to the inputs.\n\t            additional_inputs: a dictionary of inputs to the function. the keys are the names of the inputs and the\n\t                values are small description of the inputs.\n\t            override: if a genie has been generated before with the same args, then it will be loaded from cache be\n", "                default. set override to True to make a new API call and recreate the genie.\n\t            update_base_input: if True, the base data will be replaced by the result of executing the code. this is used\n\t                if we are making a permanent update to the input and want to use the updated input moving forward.\n\t        Returns:\n\t            A GenieResult instance which contains attributes:\n\t                - result: the result of executing the code\n\t                - id: the id of the genie\n\t                - code: the code generated by the genie\n\t                - cache_dir: the directory where the code is cached. the code will be cached in a file named\n\t                    \"cache_dir/<id>.py\n", "        \"\"\"\n\t        if isinstance(instructions, str):\n\t            instructions = [instructions]\n\t        # check cache\n\t        cache_key = self._get_hash_str(instructions, additional_inputs)\n\t        cache_value = self._cache.get(cache_key)\n\t        # case: reading from cache\n\t        if (not override) and (cache_value is not None):\n\t            code, id = cache_value.code, cache_value.id\n\t            print(f\"Loading cached genie id: {id}, set override = True to rerun\")\n", "        else:\n\t            # case: creating new genie\n\t            inputs = self._combine_inputs(additional_inputs)\n\t            code = self._get_code(instructions, inputs)\n\t            id = self._generate_id(code)\n\t            self._update_cache(code, cache_key, instructions, inputs, id)\n\t        return self._get_result(code, additional_inputs, update_base_input, id)\n\t    def _update_cache(\n\t        self, code: str, cache_key: str, instructions: Optional[Union[str, List[str]]], inputs: Dict[str, Any], id: str\n\t    ):\n", "        self._cache.update(\n\t            cache_key,\n\t            _CacheValue(code=code, id=id, instructions=instructions, inputs=list(inputs.keys())),\n\t        )\n\t        print(f\"Genie cached with id: {id}\")\n\t    def _get_result(self, code, additional_inputs, update_base_input, id: Optional[str] = None):\n\t        # create executor and return results\n\t        result = self.run(code, additional_inputs)\n\t        if update_base_input:\n\t            if result is None:\n", "                raise ValueError(f\"result of genie is None, cannot update base input\")\n\t            self.data = result\n\t        id = id or self._generate_id(code)\n\t        return GenieResult(id=id, code=code, cache_dir=self._cache.cache_dir, result=result)\n\t    def custom(\n\t        self, code: str, additional_inputs: Optional[Dict[str, Any]] = None, update_base_input: bool = False\n\t    ) -> GenieResult:\n\t        \"\"\"Define a custom genie with user defined code segment. The first argument of the function should be the\n\t        base input of the genie.\n\t        Note that this code should define a stand alone function, ie, it should not depend on any external variables\n", "        or functions or imports. If any additional packages are required, you need to import them in the code segment\n\t        itself.\n\t        Args:\n\t            code: the code segment defining a single function to be used to process data.\n\t            additional_inputs: a dictionary of inputs to the function. the keys are the names of the inputs and the\n\t                values are small description of the inputs.\n\t            update_base_input: if True, the base data will be replaced by the result of executing the code. this is used\n\t                if we are making a permanent update to the input and want to use the updated input moving forward.\n\t        Returns:\n\t            A GenieResult instance which contains attributes:\n", "                - result: the result of executing the code\n\t                - id: the id of the genie\n\t                - code: the code generated by the genie\n\t                - cache_dir: the directory where the code is cached. the code will be cached in a file named\n\t                    \"cache_dir/<id>.py\n\t        \"\"\"\n\t        # proxy instructions as the code entered\n\t        instructions = [code]\n\t        cache_key = self._get_hash_str(instructions, additional_inputs)\n\t        id = self._generate_id(code)\n", "        self._update_cache(code, cache_key, instructions, inputs=self._combine_inputs(additional_inputs), id=id)\n\t        return self._get_result(code, additional_inputs, update_base_input)\n\t    def run(self, code: str, additional_inputs: Dict[str, Any]):\n\t        executor = self._extract_executable(code)\n\t        try:\n\t            return executor(**self._combine_inputs(additional_inputs, copy_base_input=True))\n\t        except Exception as e:\n\t            raise RuntimeError(f\"Failed to execute code segment: \\n\\n{code}\") from e\n\t    @classmethod\n\t    def _extract_fn_name(cls, code: str):\n", "        # find function name from code block and substitute with function_name\n\t        match = re.search(\"def\\s+(.*)\\(.*\\).*:\", code)\n\t        if match is None:\n\t            raise RuntimeError(f\"Failed to extract function from code block: {code}\")\n\t        return match.group(1)\n\t    def _combine_inputs(\n\t        self, additional_inputs: Optional[Dict[str, Any]], copy_base_input: bool = False\n\t    ) -> Dict[str, Any]:\n\t        data = self.data.copy() if (copy_base_input and self.copy_data_before_use) else self.data\n\t        return {self._base_key: data, **(additional_inputs or {})}\n", "    @staticmethod\n\t    def _create_input_str(x):\n\t        if isinstance(x, pd.DataFrame):\n\t            return f\"pandas dataframes with columns: {x.columns}\"\n\t        return f\"{type(x)}\"\n\t    def _get_code(self, instructions: List[str], inputs: Dict[str, Any]) -> str:\n\t        input_str = {key: self._create_input_str(value) for key, value in inputs.items()}\n\t        return self._client.get(instructions=instructions, inputs=input_str)\n\t    @classmethod\n\t    def _extract_executable(cls, code: str) -> Callable:\n", "        # define function in memory\n\t        fn_name = cls._extract_fn_name(code)\n\t        mem = {}\n\t        exec(code, mem)\n\t        return mem[fn_name]\n\t    @classmethod\n\t    def _generate_id(cls, code: str) -> str:\n\t        fn_name = cls._extract_fn_name(code)\n\t        # use fn name with random 5 digit suffix\n\t        return f\"{fn_name}_{random.randint(10000, 99999)}\"\n", "    @classmethod\n\t    def _list_to_str(cls, l: List[str]) -> str:\n\t        return cls._hash_sep.join(l)\n\t    @classmethod\n\t    def _inputs_to_str(cls, d: Dict[str, Any]) -> str:\n\t        sorted_keys = sorted(d.keys())\n\t        return cls._hash_sep.join([f\"{k}={type(d[k])}\" for k in sorted_keys])\n\t    def _get_hash_str(self, instructions: List[str], additional_inputs: Optional[Dict[str, Any]]) -> str:\n\t        hash_strings = [\n\t            self._list_to_str(instructions),\n", "            self._inputs_to_str(self._combine_inputs(additional_inputs)),\n\t        ]\n\t        return self._hash_sep.join(hash_strings)\n\t    def read_cache(self) -> Dict[str, str]:\n\t        \"\"\"Read all the code segments in the cache directory set in the current genie instance\n\t        Returns:\n\t            A dictionary with keys as the genie ids and values as the code segments\n\t        \"\"\"\n\t        return self._cache.get_all_code_segments()\n\t    @staticmethod\n", "    def _get_data_key(data):\n\t        if isinstance(data, pd.DataFrame):\n\t            return \"df\"\n\t        return \"data\"\n"]}
{"filename": "code_genie/__init__.py", "chunked_list": ["__version__ = \"0.4.0\"\n\tfrom code_genie._cache import _CacheManager\n\tfrom code_genie.genie import Genie\n"]}
{"filename": "code_genie/io/base.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import Any, Dict, TypeVar\n\tfrom pydantic import BaseModel\n\tfrom code_genie.io.argument import GenieArgument\n\tARG = TypeVar(\"ARG\", bound=GenieArgument)\n\tclass GenieSource(ABC, BaseModel):\n\t    @abstractmethod\n\t    def get(self, **kwargs: Dict[str, GenieArgument]):\n\t        raise NotImplemented\n\tclass GenieSink(ABC, BaseModel):\n", "    @abstractmethod\n\t    def put(self, data: Any, **kwargs: Dict[str, GenieArgument]):\n\t        ...\n"]}
{"filename": "code_genie/io/local.py", "chunked_list": ["from typing import Dict\n\timport pandas as pd\n\tfrom code_genie.io.argument import BoolArg, GenieArgument, StringArg\n\tfrom code_genie.io.base import GenieSink, GenieSource\n\tclass DataFrameToCsvSink(GenieSink):\n\t    path: StringArg\n\t    \"\"\"Path to the local file where data is to be exported.\"\"\"\n\t    index: BoolArg = BoolArg(name=\"export-pd-index\", default_value=False)\n\t    \"\"\"Whether to export the index of the dataframe.\"\"\"\n\t    reset_index: BoolArg = BoolArg(name=\"reset-pd-index\", default_value=True)\n", "    \"\"\"Whether to reset the index of the dataframe before exporting.\"\"\"\n\t    def put(self, data: pd.DataFrame, **kwargs):\n\t        if self.reset_index.get(**kwargs):\n\t            data = data.reset_index()\n\t        data.to_csv(self.path.get(**kwargs), index=self.index.get(**kwargs))\n\tclass CsvToDataFrameSource(GenieSource):\n\t    path: StringArg\n\t    \"\"\"Path to the local file from where data is to be imported from.\"\"\"\n\t    kwargs: Dict[str, GenieArgument] = {}\n\t    \"\"\"Any arguments to be passed to the pandas read_csv method.\"\"\"\n", "    def get(self, **kwargs):\n\t        return pd.read_csv(self.path.get(**kwargs), **{k: v.get(**kwargs) for k, v in self.kwargs.items()})\n"]}
{"filename": "code_genie/io/gcp.py", "chunked_list": ["from abc import ABC\n\tfrom typing import Dict\n\tfrom google.cloud import bigquery\n\tfrom google.oauth2 import service_account\n\tfrom code_genie.io.argument import GenieArgument, StringArg\n\tfrom code_genie.io.base import GenieSource\n\tclass _WithCredentials(GenieSource, ABC):\n\t    _default_key_path_env_var: str = \"GOOGLE_APPLICATION_CREDENTIALS\"\n\t    key_path: StringArg = StringArg(env_var=_default_key_path_env_var)\n\t    \"\"\"Path to the GCP key file.\"\"\"\n", "    def _bq_client(self, **kwargs):\n\t        credentials = service_account.Credentials.from_service_account_file(\n\t            self.key_path.get(**kwargs),\n\t            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n\t        )\n\t        return bigquery.Client(credentials=credentials, project=credentials.project_id)\n\tclass BigQueryToDataframeSource(_WithCredentials):\n\t    query: str\n\t    \"\"\"Query to be used to read data from BQ\"\"\"\n\t    query_args: Dict[str, GenieArgument]\n", "    \"\"\"Any arguments to be used in the query. These will be passed to the query using the format method.\"\"\"\n\t    def get(self, **kwargs: Dict[str, GenieArgument]):\n\t        query = self.query\n\t        if self.query_args:\n\t            query = query.format(**{k: v.get(**kwargs) for k, v in self.query_args.items()})\n\t        return self._bq_client(**kwargs).query(query).result().to_dataframe()\n"]}
{"filename": "code_genie/io/argument.py", "chunked_list": ["import os\n\timport warnings\n\tfrom abc import ABC\n\tfrom typing import Any, Dict, Optional\n\tfrom pydantic import BaseModel, root_validator, validator\n\tclass GenieArgument(ABC, BaseModel):\n\t    \"\"\"Define an argument to the pipeline. This would be used to create the deployment setup\"\"\"\n\t    name: Optional[str] = None\n\t    \"\"\"Name of the argument, should only contain numbers, letters, dash and underscores; should start with a letter.\n\t    If a name is provided, then the value of this argument can be set when running the script.\n", "    If name is not provided, then default_value must be provided and the value of this argument cannot be set while \n\t    running the pipeline.\"\"\"\n\t    default_value: Optional[Any] = None  # set custom validator in the subclass\n\t    \"\"\"Default value of the argument if not provided\"\"\"\n\t    env_var: Optional[str] = None\n\t    \"\"\"Name of the environment variable from which this argument should be read if not provided\"\"\"\n\t    @validator(\"name\")\n\t    def name_valid(cls, v):\n\t        if v is None:\n\t            return v\n", "        if not isinstance(v, str):\n\t            raise ValueError(f\"name must be a string, found: {v}\")\n\t        # check v contains only letters, numbers and underscores\n\t        if not v.replace(\"_\", \"\").replace(\"-\", \"\").isalnum():\n\t            raise ValueError(f\"name must only contain letters, numbers, dash and underscores, found: {v}\")\n\t        if not v[0].isalpha():\n\t            raise ValueError(f\"name must start with a letter, found: {v}\")\n\t        return v\n\t    @root_validator\n\t    def name_or_default_value(cls, values):\n", "        if \"name\" not in values and \"default_value\" not in values:\n\t            raise ValueError(\"Either name or default_value must be provided, both are none\")\n\t        return values\n\t    def get(self, **kwargs: Dict[str, Any]):\n\t        \"\"\"Resolve the value of the arg from the dictionary of arguments passed to the pipeline. Resolution is done\n\t        using the following precedence:\n\t        1. value set by the pipeline object\n\t        2. env_var\n\t        3. default_value\n\t        \"\"\"\n", "        value = kwargs.get(self.name, None)\n\t        if value is not None:\n\t            return value\n\t        if self.env_var is not None:\n\t            value = os.getenv(self.env_var)\n\t            if value is None:\n\t                warnings.warn(\n\t                    f\"No environment variable {self.env_var} found for argument: {self.name}\"\n\t                    f\" falling back to default value\"\n\t                )\n", "            else:\n\t                return value\n\t        if self.default_value is not None:\n\t            return self.default_value\n\t        raise ValueError(f\"None of value or default_value or env_var provided for argument: {self.name}\")\n\tclass StringArg(GenieArgument):\n\t    @validator(\"default_value\")\n\t    def must_be_str(cls, v):\n\t        if v is not None and not isinstance(v, str):\n\t            raise ValueError(f\"default_value must be a string, found: {v}\")\n", "        return v\n\tclass IntArg(GenieArgument):\n\t    @validator(\"default_value\")\n\t    def must_be_int(cls, v):\n\t        if v is not None and not isinstance(v, int):\n\t            raise ValueError(f\"default_value must be an int, found: {v}\")\n\t        return v\n\tclass BoolArg(GenieArgument):\n\t    @validator(\"default_value\")\n\t    def must_be_bool(cls, v):\n", "        if v is not None and not isinstance(v, bool):\n\t            raise ValueError(f\"default_value must be a bool, found: {v}\")\n\t        return v\n"]}
{"filename": "code_genie/io/__init__.py", "chunked_list": ["from code_genie.io.argument import BoolArg, IntArg, StringArg\n\tfrom code_genie.io.gcp import BigQueryToDataframeSource\n\tfrom code_genie.io.local import CsvToDataFrameSource, DataFrameToCsvSink\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_pipeline.py", "chunked_list": ["import os\n\timport pandas as pd\n\timport pytest\n\tfrom code_genie import Genie\n\tfrom code_genie.io import IntArg, StringArg\n\tfrom code_genie.io.local import CsvToDataFrameSource, DataFrameToCsvSink\n\tfrom code_genie.pipeline import GeniePipeline, PipelineStep\n\t@pytest.fixture(scope=\"module\")\n\tdef pipeline_cache_dir() -> str:\n\t    # path to _cache dir in same directory as this file\n", "    return os.path.join(os.path.dirname(__file__), \"_pipeline_cache\")\n\t@pytest.fixture(scope=\"module\")\n\tdef df() -> pd.DataFrame:\n\t    return pd.DataFrame({\"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [\"a\"] * 5 + [\"b\"] * 5})\n\t@pytest.fixture(scope=\"module\")\n\tdef df_eval() -> pd.DataFrame:\n\t    return pd.DataFrame({\"x\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100], \"y\": [\"c\"] * 5 + [\"d\"] * 5})\n\t@pytest.fixture(scope=\"module\")\n\tdef df_path(pipeline_cache_dir, df) -> str:\n\t    filepath = os.path.join(pipeline_cache_dir, \"data\", \"df.csv\")\n", "    df.to_csv(filepath, index=False)\n\t    return filepath\n\t@pytest.fixture(scope=\"module\")\n\tdef df_eval_path(pipeline_cache_dir, df_eval) -> str:\n\t    filepath = os.path.join(pipeline_cache_dir, \"data\", \"df_eval.csv\")\n\t    df_eval.to_csv(filepath, index=False)\n\t    return filepath\n\tdef test_pipeline(client, pipeline_cache_dir, df, df_path, df_eval_path):\n\t    # create a genie to be converted into a pipeline\n\t    genie = Genie(data=df, cache_dir=pipeline_cache_dir, client=client)\n", "    multiplier = 2\n\t    gr_grp = genie.plz(\"create a df with mean values of x grouped by y\")\n\t    genie = Genie(data=gr_grp.result, cache_dir=pipeline_cache_dir, client=client)\n\t    gr_mul = genie.plz(\"multiply values of x by multiplier\", additional_inputs={\"multiplier\": multiplier})\n\t    result_values = gr_mul.result.reset_index().set_index(\"y\")[\"x\"].to_dict()\n\t    assert result_values == {\"a\": 3.0 * multiplier, \"b\": 8.0 * multiplier}\n\t    # create a pipeline which takes the input as a df, runs the genie and then stores the result locally\n\t    source_path_key = \"df-source-path\"\n\t    sink_path_key = \"df-sink-path\"\n\t    multiplier_key = \"multiplier\"\n", "    source = CsvToDataFrameSource(path=StringArg(name=source_path_key))\n\t    sink = DataFrameToCsvSink(path=StringArg(name=sink_path_key))\n\t    steps = [\n\t        PipelineStep(\n\t            genie_result=gr_grp,\n\t            data=source,\n\t        ),\n\t        PipelineStep(\n\t            genie_result=gr_mul,\n\t            data=gr_grp,\n", "            additional_inputs={multiplier_key: IntArg(name=multiplier_key)},\n\t            sink=sink,\n\t        ),\n\t    ]\n\t    pipeline = GeniePipeline(name=\"test-pipe\", version=\"1\", cache_dir=pipeline_cache_dir, steps=steps)\n\t    pipeline.export(\"test-pipe.json\")\n\t    pipeline_imported = GeniePipeline.load(os.path.join(pipeline_cache_dir, \"test-pipe.json\"))\n\t    assert pipeline_imported == pipeline\n\t    # now we will run this pipeline with a new df and multiplier and check results\n\t    sink_path = os.path.join(pipeline_cache_dir, \"data\", \"df_eval_result.csv\")\n", "    pipeline_imported.run({source_path_key: df_eval_path, multiplier_key: 5, sink_path_key: sink_path})\n\t    # read eval df and make sure it is correct\n\t    df_eval_result = pd.read_csv(sink_path)\n\t    result_values_eval = df_eval_result.reset_index().set_index(\"y\")[\"x\"].to_dict()\n\t    assert result_values_eval == {\"c\": 150.0, \"d\": 400.0}\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\n\timport pytest\n\tfrom dotenv import load_dotenv\n\tfrom code_genie.client import Client\n\t@pytest.fixture(scope=\"module\")\n\tdef client():\n\t    # path to 2 directories above current file\n\t    path = os.path.join(os.path.dirname(os.path.dirname(__file__)), \".env\")\n\t    load_dotenv(path)\n\t    return Client()\n", "@pytest.fixture(scope=\"module\")\n\tdef cache_dir():\n\t    # path to _cache dir in same directory as this file\n\t    return os.path.join(os.path.dirname(__file__), \"_cache\")\n"]}
{"filename": "tests/test_genie.py", "chunked_list": ["import pandas as pd\n\timport pytest\n\tfrom code_genie import Genie\n\t@pytest.fixture(scope=\"session\")\n\tdef df():\n\t    return pd.DataFrame({\"x\": [1, 2, 3, 1, 2, 3], \"y\": [4, 5, 6, 4, 5, 6]})\n\tdef test_math(client):\n\t    # use genie to get a method to add 2 numbers\n\t    genie = Genie(data=[10, 20, 30], client=client)\n\t    # call the method\n", "    assert genie.plz(instructions=\"add all inputs\").result == 60\n\t    assert genie.plz(instructions=\"multiply all inputs\").result == 6000\n\tdef test_math_additional_inputs(client):\n\t    # use genie to get a method to add 2 numbers\n\t    genie = Genie(data=4, client=client, copy_data_before_use=False)\n\t    # call the method\n\t    additional_input = {\"y\": 2}\n\t    assert genie.plz(instructions=\"add data and y\", additional_inputs=additional_input).result == 6\n\t    assert genie.plz(instructions=\"multiply data and y\", additional_inputs=additional_input).result == 8\n\tdef test_pd_mean(client, df):\n", "    genie = Genie(data=df, client=client)\n\t    # call the method\n\t    assert genie.plz(instructions=\"sum of mean of x and y\").result == 7\n\t    assert set(genie.plz(instructions=\"distinct values of x\").result) == {1, 2, 3}\n\tdef test_custom(client, df):\n\t    genie = Genie(data=df, client=client)\n\t    # call the method\n\t    code = \"\"\"\n\tdef run(df):\n\t    return df.x.unique() \n", "    \"\"\"\n\t    assert set(genie.custom(code=code).result) == {1, 2, 3}\n\tdef test_cache(client):\n\t    # use genie to get a method to add 2 numbers\n\t    genie = Genie(data=[1, 2, 3, 4], client=client)\n\t    # call the method\n\t    gr_1 = genie.plz(instructions=\"add all elements of input\")\n\t    assert gr_1.result == 10\n\t    gr_2 = genie.plz(instructions=\"add all elements of input\")\n\t    assert gr_2.result == 10\n", "    assert gr_1.id == gr_2.id\n\t    gr_3 = genie.plz(instructions=\"add all elements of input\", override=True)\n\t    assert gr_3.result == 10\n\t    assert gr_3.id != gr_1.id\n\tdef test_pd_copy_before_use(client):\n\t    df = pd.DataFrame({\"x\": [1, 2, 3, 1, 2, 3], \"y\": [4, 5, 6, 4, 5, 6]})\n\t    genie = Genie(data=df, client=client)\n\t    # call the method\n\t    gr = genie.plz(instructions=\"drop column x\")\n\t    assert set(gr.result.columns) == {\"y\"}\n", "    assert set(df.columns) == {\"x\", \"y\"}\n"]}
{"filename": "tests/_pipeline_cache/multiply_df_98613.py", "chunked_list": ["import pandas as pd\n\tdef multiply_df(df: pd.DataFrame, multiplier: int) -> pd.DataFrame:\n\t    df[\"x\"] = df[\"x\"] * multiplier\n\t    return df\n"]}
{"filename": "tests/_pipeline_cache/create_means_df_73453.py", "chunked_list": ["import pandas as pd\n\tdef create_means_df(df):\n\t    means_df = df.groupby(\"y\").mean()\n\t    return means_df\n"]}
{"filename": "docs/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# For the full list of built-in configuration values, see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\timport os\n\timport sys\n\tsys.path.insert(0, os.path.abspath('..'))\n\tproject = 'code-genie'\n", "copyright = '2023, Aarshay Jain'\n\tauthor = 'Aarshay Jain'\n\t# -- General configuration ---------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\textensions = ['sphinx.ext.napoleon',\n\t              'sphinx.ext.autodoc',\n\t              'sphinx.ext.viewcode',\n\t              'sphinx.ext.autosummary',\n\t              \"nbsphinx\",\n\t              \"sphinx_gallery.load_style\",\n", "              \"sphinxcontrib.autodoc_pydantic\"]\n\ttemplates_path = ['_templates']\n\texclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\t# Napoleon settings\n\tnapoleon_google_docstring = True\n\tnapoleon_numpy_docstring = False\n\t# autodoc-pydantic settings\n\tautodoc_pydantic_model_show_json = False\n\tautodoc_pydantic_model_show_config = False\n\t# -- Options for HTML output -------------------------------------------------\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\thtml_theme = 'sphinx_rtd_theme'\n\thtml_static_path = ['_static']\n"]}
{"filename": "docs/notebooks/_cache_starter/remote_ratio_by_work_years_96078.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport pandas as pd\n\tdef remote_ratio_by_work_years(df):\n\t    \"\"\"\n\t    This function creates a bar chart of percentage counts of remote_ratio grouped by work_years.\n\t    Parameters:\n\t    df (pd.DataFrame): pandas dataframe with columns 'work_year', 'experience_level', 'employment_type',\n\t                        'job_title', 'salary', 'employee_residence', 'remote_ratio', 'company_location', \n\t                        'company_size'.\n\t    Returns:\n", "    None (displays a bar chart)\n\t    \"\"\"\n\t    # Group by work year and remote ratio to get counts\n\t    counts = df.groupby(['work_year', 'remote_ratio']).size().reset_index(name='count')\n\t    # Group by work year to get total count for each year\n\t    total_counts = counts.groupby('work_year').agg({'count': 'sum'}).reset_index()\n\t    total_counts.rename(columns={'count': 'total_count'}, inplace=True)\n\t    # Merge counts with total counts\n\t    counts = counts.merge(total_counts, on=['work_year'])\n\t    # Calculate percentage counts\n", "    counts['percentage_count'] = counts['count'] / counts['total_count'] * 100\n\t    # Pivot the table to create bar chart\n\t    counts_pivot = counts.pivot(index='work_year', columns='remote_ratio', values='percentage_count')\n\t    # Create the bar chart\n\t    ax = counts_pivot.plot(kind='bar', stacked=True, figsize=(10, 6))\n\t    ax.set_xlabel('Work Years')\n\t    ax.set_ylabel('Percentage Count')\n\t    ax.set_title('Remote Ratio by Work Years')\n\t    plt.show()\n"]}
{"filename": "docs/notebooks/_cache_starter/plot_salary_distribution_51861.py", "chunked_list": ["import seaborn as sns\n\tdef plot_salary_distribution(df):\n\t    sns.set(style=\"whitegrid\")\n\t    sns.displot(data=df, x=\"salary\", hue=\"work_year\", kind=\"kde\", fill=True)\n"]}
{"filename": "docs/notebooks/_cache_starter/pie_chart_47446.py", "chunked_list": ["import matplotlib.pyplot as plt\n\tdef pie_chart(df):\n\t    plt.pie(df['work_year'].value_counts(), labels=df['work_year'].unique())\n\t    plt.title('Distribution of work year')\n\t    plt.show()"]}
{"filename": "docs/notebooks/_cache_starter/unique_remote_ratio_counts_63145.py", "chunked_list": ["def unique_remote_ratio_counts(df):\n\t    print(df[\"remote_ratio\"].value_counts())\n"]}
{"filename": "docs/notebooks/_cache_starter/count_job_designations_83350.py", "chunked_list": ["def count_job_designations(df):\n\t    num_job_designations = df['job_title'].nunique()\n\t    return num_job_designations"]}
{"filename": "docs/notebooks/_cache_starter/process_dataframe_88292.py", "chunked_list": ["import pandas as pd\n\tdef process_dataframe(df):\n\t    df = df.drop(columns=['salary', 'salary_currency'])\n\t    df = df.rename(columns={\"salary_in_usd\": \"salary\"})\n\t    return df"]}
{"filename": "docs/notebooks/_cache_starter/remote_ratio_mapper_69067.py", "chunked_list": ["import pandas as pd\n\tdef remote_ratio_mapper(df):\n\t    df['remote_ratio'].replace({0: 'No Remote Work', 50: 'Partially Remote', 100: 'Fully Remote'}, inplace=True)\n\t    return df\n"]}
{"filename": "docs/notebooks/_cache_starter/plot_emp_type_experience_level_31165.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport numpy as np\n\tdef plot_emp_type_experience_level(df):\n\t    emp_types = df['employment_type'].unique()\n\t    fig, axes = plt.subplots(1, len(emp_types), figsize=(15,5))\n\t    for i, emp_type in enumerate(emp_types):\n\t        data = df[df['employment_type'] == emp_type]['experience_level'].value_counts()\n\t        axes[i].bar(data.index, data.values)\n\t        axes[i].set_title(emp_type)\n\t        axes[i].tick_params(axis='x', rotation=45)\n", "    plt.show()\n"]}
{"filename": "docs/notebooks/_cache_starter/map_experience_level_48687.py", "chunked_list": ["import pandas as pd\n\tdef map_experience_level(df):\n\t    mapping = {'EN': 'Entry-level / Junior', 'MI': 'Mid-level / Intermediate', \n\t               'SE': 'Senior-level / Expert', 'EX': 'Executive-level / Director'}\n\t    df['experience_level'] = df['experience_level'].map(mapping)\n\t    return df"]}
{"filename": "docs/notebooks/_cache_starter/count_missing_values_26226.py", "chunked_list": ["import pandas as pd\n\tdef count_missing_values(df):\n\t    return df.isnull().sum()\n"]}
{"filename": "docs/notebooks/_cache_starter/make_wordcloud_51022.py", "chunked_list": ["import matplotlib.pyplot as plt\n\tfrom wordcloud import WordCloud\n\tdef make_wordcloud(df):\n\t    # define the frequency of each job title\n\t    job_title_freq = df['job_title'].value_counts(normalize=True)\n\t    # create the wordcloud object\n\t    wc = WordCloud(width=800, height=600, background_color='white', max_words=50)\n\t    # generate the wordcloud\n\t    wc.generate_from_frequencies(job_title_freq)\n\t    # plot the wordcloud\n", "    plt.figure(figsize=(12, 8))\n\t    plt.imshow(wc, interpolation='bilinear')\n\t    plt.axis('off')\n\t    plt.show()\n"]}
{"filename": "docs/notebooks/_cache_starter/unique_experience_count_96626.py", "chunked_list": ["import pandas as pd\n\tdef unique_experience_count(df):\n\t    experience_counts = df['experience_level'].value_counts()\n\t    print(experience_counts)\n"]}
{"filename": "docs/notebooks/_cache_starter/top_jobs_41202.py", "chunked_list": ["import matplotlib.pyplot as plt\n\tdef top_jobs(df):\n\t    top_jobs = df['job_title'].value_counts()[:15] # Get top 15 most frequently occurring job designations\n\t    plt.bar(top_jobs.index, top_jobs.values) # Make bar chart\n\t    plt.xticks(rotation=90) # Rotate labels\n\t    for i, v in enumerate(top_jobs.values):\n\t        plt.text(i, v+5, str(v), ha='center') # Print count on top of each bar\n\t    plt.show() \n"]}
{"filename": "docs/notebooks/_cache_starter/create_boxplot_24032.py", "chunked_list": ["import matplotlib.pyplot as plt\n\tdef create_boxplot(df):\n\t    plt.boxplot(df['salary'])\n\t    plt.show()\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/aggregate_data_92888.py", "chunked_list": ["import pandas as pd\n\tdef aggregate_data(df):\n\t    df_filtered = df[df['hit_type']=='PAGE']\n\t    df_grouped = df_filtered.groupby('page_path').agg({'hit_type':'count','bounces':'mean'})\n\t    df_grouped.columns = ['views','exit_rate']\n\t    df_sorted = df_grouped.sort_values(by='views', ascending=False)\n\t    return df_sorted\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/group_action_type_27490.py", "chunked_list": ["import pandas as pd\n\tdef group_action_type(df):\n\t    # group data by action_type and count number of rows in each group\n\t    action_counts = df.groupby('action_type').size().reset_index(name='count')\n\t    # remove action_type other than 1, 2, 5, 6\n\t    action_counts = action_counts[action_counts['action_type'].isin([1, 2, 5, 6])]\n\t    # replace the action_type values as:\n\t    # 1: Click on product list page\n\t    # 2: Product details page\n\t    # 5: Checkout\n", "    # 6: Purchase Complete\n\t    action_counts['action_type'] = action_counts['action_type'].replace({1: 'Click on product list page', 2: 'Product details page', 5: 'Checkout', 6: 'Purchase Complete'})\n\t    return action_counts\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/get_page_bounce_rate_81411.py", "chunked_list": ["import pandas as pd\n\tdef get_page_bounce_rate(df):\n\t    hit1_page_df = df[(df[\"hit_number\"] == 1) & (df[\"hit_type\"] == \"PAGE\")]\n\t    page_agg_df = hit1_page_df.groupby(\"page_path\").agg(\n\t        views=pd.NamedAgg(column=\"page_path\", aggfunc=\"count\"),\n\t        bounce_rate=pd.NamedAgg(column=\"bounces\", aggfunc=lambda x: sum(x)/len(x))\n\t    ).sort_values(\"views\", ascending=False)\n\t    return page_agg_df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/extract_hit_type_49083.py", "chunked_list": ["import pandas as pd\n\tdef extract_hit_type(df):\n\t    # Loop through the hits column and extract the hit type from the dictionary\n\t    hit_types = []\n\t    for row in df['hits']:\n\t        hit_type = row['type']\n\t        hit_types.append(hit_type)\n\t    # Create a new column called hit_type with the extracted hit types\n\t    df['hit_type'] = hit_types\n\t    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/extract_hit_number_77980.py", "chunked_list": ["import pandas as pd\n\tdef extract_hit_number(df):\n\t    df['hit_number'] = df['hits'].apply(lambda x: x.get('hitNumber'))\n\t    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/run_51794.py", "chunked_list": ["def run(df):\n\t    df[\"hit_type\"] = df[\"hits\"].apply(lambda x: x[\"type\"])\n\t    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/extract_action_type_93153.py", "chunked_list": ["import pandas as pd\n\tdef extract_action_type(df):\n\t    hits_list = []\n\t    for row in df.itertuples(index=False):\n\t        hits_dict = row.hits\n\t        if 'eCommerceAction' in hits_dict.keys():\n\t            ecommerce_dict = hits_dict['eCommerceAction']\n\t            if 'action_type' in ecommerce_dict.keys():\n\t                hits_dict.update({'action_type': ecommerce_dict['action_type']})\n\t            else:\n", "                hits_dict.update({'action_type': None})\n\t        else:\n\t            hits_dict.update({'action_type': None})\n\t        hits_list.append(hits_dict)\n\t    hits_df = pd.DataFrame(hits_list)\n\t    df = pd.concat([df, hits_df['action_type']], axis=1)\n\t    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/run_20093.py", "chunked_list": ["def run(df):\n\t    df[\"action_type\"] = df[\"hits\"].apply(lambda x: int(x[\"eCommerceAction\"][\"action_type\"]))\n\t    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/extract_page_path_78866.py", "chunked_list": ["import pandas as pd\n\tdef extract_page_path(df):\n\t    # extract page path\n\t    df['page_path'] = df['hits'].apply(lambda x: x['page']['pagePath'])\n\t    return df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/separate_hits_70651.py", "chunked_list": ["import pandas as pd\n\tdef separate_hits(df):\n\t    \"\"\"\n\t    Input:\n\t    df: pandas dataframe with columns: Index(['visitorId', 'visitNumber', 'visitId', 'visitStartTime', 'date',\n\t       'totals', 'trafficSource', 'device', 'geoNetwork', 'customDimensions',\n\t       'hits', 'fullVisitorId', 'userId', 'clientId', 'channelGrouping',\n\t       'socialEngagementType'],\n\t      dtype='object')\n\t    Output:\n", "    new_df: pandas dataframe with separate rows for each item in the hits column and the same value for each item in the totals column\n\t    \"\"\"\n\t    # create new dataframe with separate row for each item in hits column\n\t    new_df = df.explode('hits')\n\t    # drop columns except hits and totals\n\t    new_df = new_df[['hits', 'totals']]\n\t    return new_df\n"]}
{"filename": "docs/notebooks/_cache_google_analytics/add_bounces_column_93508.py", "chunked_list": ["import pandas as pd\n\tdef add_bounces_column(df):\n\t    df['bounces'] = df['totals'].apply(lambda x: True if 'bounces' in x and x['bounces'] == 1 else False)\n\t    return df\n"]}
