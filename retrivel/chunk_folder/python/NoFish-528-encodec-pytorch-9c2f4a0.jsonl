{"filename": "model.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"EnCodec model implementation.\"\"\"\n\timport math\n\tfrom pathlib import Path\n\timport typing as tp\n\timport numpy as np\n", "import torch\n\tfrom torch import nn\n\timport quantization as qt\n\timport modules as m\n\tfrom utils import _check_checksum, _linear_overlap_add, _get_checkpoint_url\n\timport random\n\tROOT_URL = 'https://dl.fbaipublicfiles.com/encodec/v0/'\n\tEncodedFrame = tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]\n\tclass LMModel(nn.Module):\n\t    \"\"\"Language Model to estimate probabilities of each codebook entry.\n", "    We predict all codebooks in parallel for a given time step.\n\t    Args:\n\t        n_q (int): number of codebooks.\n\t        card (int): codebook cardinality.\n\t        dim (int): transformer dimension.\n\t        **kwargs: passed to `encodec.modules.transformer.StreamingTransformerEncoder`.\n\t    \"\"\"\n\t    def __init__(self, n_q: int = 32, card: int = 1024, dim: int = 200, **kwargs):\n\t        super().__init__()\n\t        self.card = card\n", "        self.n_q = n_q\n\t        self.dim = dim\n\t        self.transformer = m.StreamingTransformerEncoder(dim=dim, **kwargs)\n\t        self.emb = nn.ModuleList([nn.Embedding(card + 1, dim) for _ in range(n_q)])\n\t        self.linears = nn.ModuleList([nn.Linear(dim, card) for _ in range(n_q)])\n\t    def forward(self, indices: torch.Tensor,\n\t                states: tp.Optional[tp.List[torch.Tensor]] = None, offset: int = 0):\n\t        \"\"\"\n\t        Args:\n\t            indices (torch.Tensor): indices from the previous time step. Indices\n", "                should be 1 + actual index in the codebook. The value 0 is reserved for\n\t                when the index is missing (i.e. first time step). Shape should be\n\t                `[B, n_q, T]`.\n\t            states: state for the streaming decoding.\n\t            offset: offset of the current time step.\n\t        Returns a 3-tuple `(probabilities, new_states, new_offset)` with probabilities\n\t        with a shape `[B, card, n_q, T]`.\n\t        \"\"\"\n\t        B, K, T = indices.shape\n\t        input_ = sum([self.emb[k](indices[:, k]) for k in range(K)])\n", "        out, states, offset = self.transformer(input_, states, offset)\n\t        logits = torch.stack([self.linears[k](out) for k in range(K)], dim=1).permute(0, 3, 1, 2)\n\t        return torch.softmax(logits, dim=1), states, offset\n\tclass EncodecModel(nn.Module):\n\t    \"\"\"EnCodec model operating on the raw waveform.\n\t    Args:\n\t        target_bandwidths (list of float): Target bandwidths.\n\t        encoder (nn.Module): Encoder network.\n\t        decoder (nn.Module): Decoder network.\n\t        sample_rate (int): Audio sample rate.\n", "        channels (int): Number of audio channels.\n\t        normalize (bool): Whether to apply audio normalization.\n\t        segment (float or None): segment duration in sec. when doing overlap-add.\n\t        overlap (float): overlap between segment, given as a fraction of the segment duration.\n\t        name (str): name of the model, used as metadata when compressing audio.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 encoder: m.SEANetEncoder,\n\t                 decoder: m.SEANetDecoder,\n\t                 quantizer: qt.ResidualVectorQuantizer,\n", "                 target_bandwidths: tp.List[float],\n\t                 sample_rate: int,\n\t                 channels: int,\n\t                 normalize: bool = False,\n\t                 segment: tp.Optional[float] = None,\n\t                 overlap: float = 0.01,\n\t                 name: str = 'unset'):\n\t        super().__init__()\n\t        self.bandwidth: tp.Optional[float] = None\n\t        self.target_bandwidths = target_bandwidths\n", "        self.encoder = encoder\n\t        self.quantizer = quantizer\n\t        self.decoder = decoder\n\t        self.sample_rate = sample_rate\n\t        self.channels = channels\n\t        self.normalize = normalize\n\t        self.segment = segment\n\t        self.overlap = overlap\n\t        self.frame_rate = math.ceil(self.sample_rate / np.prod(self.encoder.ratios)) #75\n\t        self.name = name\n", "        self.bits_per_codebook = int(math.log2(self.quantizer.bins))\n\t        assert 2 ** self.bits_per_codebook == self.quantizer.bins, \\\n\t            \"quantizer bins must be a power of 2.\"\n\t    @property\n\t    def segment_length(self) -> tp.Optional[int]:\n\t        if self.segment is None:\n\t            return None\n\t        return int(self.segment * self.sample_rate)\n\t    @property\n\t    def segment_stride(self) -> tp.Optional[int]:\n", "        segment_length = self.segment_length\n\t        if segment_length is None:\n\t            return None\n\t        return max(1, int((1 - self.overlap) * segment_length))\n\t    def encode(self, x: torch.Tensor) -> tp.List[EncodedFrame]:\n\t        \"\"\"Given a tensor `x`, returns a list of frames containing\n\t        the discrete encoded codes for `x`, along with rescaling factors\n\t        for each segment, when `self.normalize` is True.\n\t        Each frames is a tuple `(codebook, scale)`, with `codebook` of\n\t        shape `[B, K, T]`, with `K` the number of codebooks.\n", "        \"\"\"\n\t        assert x.dim() == 3\n\t        _, channels, length = x.shape\n\t        assert channels > 0 and channels <= 2\n\t        segment_length = self.segment_length \n\t        if segment_length is None: #segment_length = 1*sample_rate\n\t            segment_length = length\n\t            stride = length\n\t        else:\n\t            stride = self.segment_stride  # type: ignore\n", "            assert stride is not None\n\t        encoded_frames: tp.List[EncodedFrame] = []\n\t        for offset in range(0, length, stride): # shift windows to choose data\n\t            frame = x[:, :, offset: offset + segment_length]\n\t            encoded_frames.append(self._encode_frame(frame))\n\t        return encoded_frames\n\t    def _encode_frame(self, x: torch.Tensor) -> EncodedFrame:\n\t        length = x.shape[-1] # tensor_cut or original\n\t        duration = length / self.sample_rate\n\t        assert self.segment is None or duration <= 1e-5 + self.segment\n", "        if self.normalize:\n\t            mono = x.mean(dim=1, keepdim=True)\n\t            volume = mono.pow(2).mean(dim=2, keepdim=True).sqrt()\n\t            scale = 1e-8 + volume\n\t            x = x / scale\n\t            scale = scale.view(-1, 1)\n\t        else:\n\t            scale = None\n\t        emb = self.encoder(x) # [2,1,10000] -> [2,128,32]\n\t        #TODO: Encodec Trainer的training\n", "        if self.training:\n\t            return emb,scale\n\t        codes = self.quantizer.encode(emb, self.frame_rate, self.bandwidth)\n\t        codes = codes.transpose(0, 1)\n\t        # codes is [B, K, T], with T frames, K nb of codebooks.\n\t        return codes, scale\n\t    def decode(self, encoded_frames: tp.List[EncodedFrame]) -> torch.Tensor:\n\t        \"\"\"Decode the given frames into a waveform.\n\t        Note that the output might be a bit bigger than the input. In that case,\n\t        any extra steps at the end can be trimmed.\n", "        \"\"\"\n\t        segment_length = self.segment_length\n\t        if segment_length is None:\n\t            assert len(encoded_frames) == 1\n\t            return self._decode_frame(encoded_frames[0])\n\t        frames = [self._decode_frame(frame) for frame in encoded_frames]\n\t        return _linear_overlap_add(frames, self.segment_stride or 1)\n\t    def _decode_frame(self, encoded_frame: EncodedFrame) -> torch.Tensor:\n\t        codes, scale = encoded_frame\n\t        if self.training:\n", "            emb = codes\n\t        else:\n\t            codes = codes.transpose(0, 1)\n\t            emb = self.quantizer.decode(codes)\n\t        out = self.decoder(emb)\n\t        if scale is not None:\n\t            out = out * scale.view(-1, 1, 1)\n\t        return out\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        frames = self.encode(x) # input_wav -> encoder , x.shape = [BatchSize,channel,tensor_cut or original length] 2,1,10000\n", "        if self.training:\n\t            # if encodec is training, input_wav -> encoder -> quantizer forward -> decode\n\t            loss_w = torch.tensor([0.0], device=x.device, requires_grad=True)\n\t            codes = []\n\t            # self.quantizer.train(self.training)\n\t            index = torch.tensor(random.randint(0,len(self.target_bandwidths)-1),device=x.device)\n\t            if torch.distributed.is_initialized():\n\t                torch.distributed.broadcast(index, src=0)\n\t            bw = self.target_bandwidths[index.item()]# fixme: variable bandwidth training, if you broadcast bd, the broadcast will encounter error\n\t            for emb,scale in frames:\n", "                qv = self.quantizer(emb,self.frame_rate,bw)\n\t                loss_w = loss_w + qv.penalty # loss_w is the sum of all quantizer forward loss (RVQ commitment loss :l_w)\n\t                codes.append((qv.quantized,scale))\n\t            return self.decode(codes)[:,:,:x.shape[-1]],loss_w,frames\n\t        else:\n\t            # if encodec is not training, input_wav -> encoder -> quantizer encode -> decode\n\t            return self.decode(frames)[:, :, :x.shape[-1]]\n\t    def set_target_bandwidth(self, bandwidth: float):\n\t        if bandwidth not in self.target_bandwidths:\n\t            raise ValueError(f\"This model doesn't support the bandwidth {bandwidth}. \"\n", "                             f\"Select one of {self.target_bandwidths}.\")\n\t        self.bandwidth = bandwidth\n\t    def get_lm_model(self) -> LMModel:\n\t        \"\"\"Return the associated LM model to improve the compression rate.\n\t        \"\"\"\n\t        device = next(self.parameters()).device\n\t        lm = LMModel(self.quantizer.n_q, self.quantizer.bins, num_layers=5, dim=200,\n\t                     past_context=int(3.5 * self.frame_rate)).to(device)\n\t        checkpoints = {\n\t            'encodec_24khz': 'encodec_lm_24khz-1608e3c0.th',\n", "            'encodec_48khz': 'encodec_lm_48khz-7add9fc3.th',\n\t        }\n\t        try:\n\t            checkpoint_name = checkpoints[self.name]\n\t        except KeyError:\n\t            raise RuntimeError(\"No LM pre-trained for the current Encodec model.\")\n\t        url = _get_checkpoint_url(ROOT_URL, checkpoint_name)\n\t        state = torch.hub.load_state_dict_from_url(\n\t            url, map_location='cpu', check_hash=True)  # type: ignore\n\t        lm.load_state_dict(state)\n", "        lm.eval()\n\t        return lm\n\t    @staticmethod\n\t    def _get_model(target_bandwidths: tp.List[float],\n\t                   sample_rate: int = 24_000,\n\t                   channels: int = 1,\n\t                   causal: bool = True,\n\t                   model_norm: str = 'weight_norm',\n\t                   audio_normalize: bool = False,\n\t                   segment: tp.Optional[float] = None,\n", "                   name: str = 'unset'):\n\t        encoder = m.SEANetEncoder(channels=channels, norm=model_norm, causal=causal)\n\t        decoder = m.SEANetDecoder(channels=channels, norm=model_norm, causal=causal)\n\t        n_q = int(1000 * target_bandwidths[-1] // (math.ceil(sample_rate / encoder.hop_length) * 10)) # int(1000*24//(math.ceil(24000/320)*10))\n\t        quantizer = qt.ResidualVectorQuantizer(\n\t            dimension=encoder.dimension,\n\t            n_q=n_q,\n\t            bins=1024,\n\t        )\n\t        model = EncodecModel(\n", "            encoder,\n\t            decoder,\n\t            quantizer,\n\t            target_bandwidths,\n\t            sample_rate,\n\t            channels,\n\t            normalize=audio_normalize,\n\t            segment=segment,\n\t            name=name,\n\t        )\n", "        return model\n\t    @staticmethod\n\t    def _get_pretrained(checkpoint_name: str, repository: tp.Optional[Path] = None):\n\t        if repository is not None:\n\t            if not repository.is_dir():\n\t                raise ValueError(f\"{repository} must exist and be a directory.\")\n\t            file = repository / checkpoint_name\n\t            checksum = file.stem.split('-')[1]\n\t            _check_checksum(file, checksum)\n\t            return torch.load(file)\n", "        else:\n\t            url = _get_checkpoint_url(ROOT_URL, checkpoint_name)\n\t            return torch.hub.load_state_dict_from_url(url, map_location='cpu', check_hash=True)  # type:ignore\n\t    @staticmethod\n\t    def encodec_model_24khz(pretrained: bool = True, repository: tp.Optional[Path] = None):\n\t        \"\"\"Return the pretrained causal 24khz model.\n\t        \"\"\"\n\t        if repository:\n\t            assert pretrained\n\t        target_bandwidths = [1.5, 3., 6, 12., 24.]\n", "        checkpoint_name = 'encodec_24khz-d7cc33bc.th'\n\t        sample_rate = 24_000\n\t        channels = 1\n\t        model = EncodecModel._get_model(\n\t            target_bandwidths, sample_rate, channels,\n\t            causal=True, model_norm='weight_norm', audio_normalize=False,\n\t            name='encodec_24khz' if pretrained else 'unset')\n\t        if pretrained:\n\t            state_dict = EncodecModel._get_pretrained(checkpoint_name, repository)\n\t            model.load_state_dict(state_dict)\n", "        model.eval()\n\t        return model\n\t    @staticmethod\n\t    def encodec_model_48khz(pretrained: bool = True, repository: tp.Optional[Path] = None):\n\t        \"\"\"Return the pretrained 48khz model.\n\t        \"\"\"\n\t        if repository:\n\t            assert pretrained\n\t        target_bandwidths = [3., 6., 12., 24.]\n\t        checkpoint_name = 'encodec_48khz-7e698e3e.th'\n", "        sample_rate = 48_000\n\t        channels = 2\n\t        model = EncodecModel._get_model(\n\t            target_bandwidths, sample_rate, channels,\n\t            causal=False, model_norm='time_group_norm', audio_normalize=True,\n\t            segment=1., name='encodec_48khz' if pretrained else 'unset')\n\t        if pretrained:\n\t            state_dict = EncodecModel._get_pretrained(checkpoint_name, repository)\n\t            model.load_state_dict(state_dict)\n\t        model.eval()\n", "        return model\n\t    #TODO: 自己实现一个encodec的model\n\t    @staticmethod\n\t    def my_encodec_model(checkpoint: str):\n\t        \"\"\"Return the pretrained 24khz model.\n\t        \"\"\"\n\t        import os\n\t        assert os.path.exists(checkpoint), \"checkpoint not exists\"\n\t        print(\"loading model from: \",checkpoint)\n\t        target_bandwidths = [1.5, 3., 6, 12., 24.]\n", "        sample_rate = 24_000\n\t        channels = 1\n\t        model = EncodecModel._get_model(\n\t                target_bandwidths, sample_rate, channels,\n\t                causal=False, model_norm='time_group_norm', audio_normalize=True,\n\t                segment=1., name='my_encodec')\n\t        pre_dic = torch.load(checkpoint)['model_state_dict']\n\t        model.load_state_dict({k.replace('quantizer.model','quantizer.vq'):v for k,v in pre_dic.items()})\n\t        model.eval()\n\t        return model\n", "    @staticmethod\n\t    def encodec_model_bw(checkpoint: str, bandwidth: float):\n\t        \"\"\"Return target bw model, if you train a model in a single bandwidth\n\t        \"\"\"\n\t        import os\n\t        assert os.path.exists(checkpoint), \"checkpoint not exists\"\n\t        print(\"loading model from: \",checkpoint)\n\t        target_bandwidths = bandwidth\n\t        sample_rate = 24_000\n\t        channels = 1\n", "        model = EncodecModel._get_model(\n\t                target_bandwidths, sample_rate, channels,\n\t                causal=False, model_norm='time_group_norm', audio_normalize=True,\n\t                segment=1., name='my_encodec')\n\t        pre_dic = torch.load(checkpoint)['model_state_dict']\n\t        model.load_state_dict({k.replace('quantizer.model','quantizer.vq'):v for k,v in pre_dic.items()})\n\t        model.eval()\n\t        return model\n\tdef test():\n\t    from itertools import product\n", "    import torchaudio\n\t    bandwidths = [3, 6, 12, 24]\n\t    models = {\n\t        'encodec_24khz': EncodecModel.encodec_model_24khz,\n\t        'encodec_48khz': EncodecModel.encodec_model_48khz,\n\t        \"my_encodec\": EncodecModel.my_encodec_model,\n\t        \"encodec_bw\": EncodecModel.encodec_model_bw,\n\t    }\n\t    for model_name, bw in product(models.keys(), bandwidths):\n\t        model = models[model_name]()\n", "        model.set_target_bandwidth(bw)\n\t        audio_suffix = model_name.split('_')[1][:3]\n\t        wav, sr = torchaudio.load(f\"test_{audio_suffix}.wav\")\n\t        wav = wav[:, :model.sample_rate * 2]\n\t        wav_in = wav.unsqueeze(0)\n\t        wav_dec = model(wav_in)[0]\n\t        assert wav.shape == wav_dec.shape, (wav.shape, wav_dec.shape)\n\tif __name__ == '__main__':\n\t    test()\n"]}
{"filename": "losses.py", "chunked_list": ["import torch\n\tfrom audio_to_mel import Audio2Mel\n\tdef total_loss(fmap_real, logits_fake, fmap_fake, input_wav, output_wav, sample_rate=24000):\n\t    \"\"\"This function is used to compute the total loss of the encodec generator.\n\t        Loss = \\lambda_t * L_t + \\lambda_f * L_f + \\lambda_g * L_g + \\lambda_feat * L_feat\n\t        L_t: time domain loss | L_f: frequency domain loss | L_g: generator loss | L_feat: feature loss\n\t        \\lambda_t = 0.1       | \\lambda_f = 1              | \\lambda_g = 3       | \\lambda_feat = 3\n\t    Args:\n\t        fmap_real (list): fmap_real is the output of the discriminator when the input is the real audio. \n\t            len(fmap_real) = len(fmap_fake) = disc.num_discriminators = 3\n", "        logits_fake (_type_): logits_fake is the list of every sub discriminator output of the Multi discriminator \n\t            logits_fake, _ = disc_model(model(input_wav)[0].detach())\n\t        fmap_fake (_type_): fmap_fake is the output of the discriminator when the input is the fake audio.\n\t            fmap_fake = disc_model(model(input_wav)[0]) = disc_model(reconstructed_audio)\n\t        input_wav (tensor): input_wav is the input audio of the generator (GT audio)\n\t        output_wav (tensor): output_wav is the output of the generator (output = model(input_wav)[0])\n\t        sample_rate (int, optional): Defaults to 24000.\n\t    Returns:\n\t        loss: total loss\n\t    \"\"\"\n", "    relu = torch.nn.ReLU()\n\t    l1Loss = torch.nn.L1Loss(reduction='mean')\n\t    l2Loss = torch.nn.MSELoss(reduction='mean')\n\t    loss = torch.tensor([0.0], device='cuda', requires_grad=True)\n\t    l_t = torch.tensor([0.0], device='cuda', requires_grad=True)\n\t    l_f = torch.tensor([0.0], device='cuda', requires_grad=True)\n\t    l_g = torch.tensor([0.0], device='cuda', requires_grad=True)\n\t    l_feat = torch.tensor([0.0], device='cuda', requires_grad=True)\n\t    #time domain loss, output_wav is the output of the generator\n\t    l_t = l1Loss(input_wav, output_wav) \n", "    #frequency domain loss, window length is 2^i, hop length is 2^i/4, i \\in [5,11]. combine l1 and l2 loss\n\t    for i in range(5, 11):\n\t        fft = Audio2Mel(win_length=2 ** i, hop_length=2 ** i // 4, n_mel_channels=64, sampling_rate=sample_rate)\n\t        l_f = l_f+l1Loss(fft(input_wav), fft(output_wav)) + l2Loss(fft(input_wav), fft(output_wav))\n\t    #generator loss and feat loss, D_k(\\hat x) = logits_fake[k], D_k^l(x) = fmap_real[k][l], D_k^l(\\hat x) = fmap_fake[k][l]\n\t    # l_g = \\sum max(0, 1 - D_k(\\hat x)) / K, K = disc.num_discriminators = len(fmap_real) = len(fmap_fake) = len(logits_fake) = 3\n\t    # l_feat = \\sum |D_k^l(x) - D_k^l(\\hat x)| / |D_k^l(x)| / KL, KL = len(fmap_real[0])*len(fmap_real)=3 * 5\n\t    for tt1 in range(len(fmap_real)): # len(fmap_real) = 3\n\t        l_g = l_g + torch.mean(relu(1 - logits_fake[tt1])) / len(logits_fake)\n\t        for tt2 in range(len(fmap_real[tt1])): # len(fmap_real[tt1]) = 5\n", "            # l_feat = l_feat + l1Loss(fmap_real[tt1][tt2].detach(), fmap_fake[tt1][tt2]) / torch.mean(torch.abs(fmap_real[tt1][tt2].detach()))\n\t            l_feat = l_feat + l1Loss(fmap_real[tt1][tt2], fmap_fake[tt1][tt2]) / torch.mean(torch.abs(fmap_real[tt1][tt2]))\n\t    KL_scale = len(fmap_real)*len(fmap_real[0]) # len(fmap_real) == len(fmap_fake) == len(logits_real) == len(logits_fake) == disc.num_discriminators == K\n\t    K_scale = len(fmap_real) # len(fmap_real[0]) = len(fmap_fake[0]) == L\n\t    loss = 3*l_g/K_scale + 3*l_feat/KL_scale + (l_t / 10) + l_f\n\t    return loss\n\tdef disc_loss(logits_real, logits_fake):\n\t    \"\"\"This function is used to compute the loss of the discriminator.\n\t        l_d = \\sum max(0, 1 - D_k(x)) + max(0, 1 + D_k(\\hat x)) / K, K = disc.num_discriminators = len(logits_real) = len(logits_fake) = 3\n\t    Args:\n", "        logits_real (List[torch.Tensor]): logits_real = disc_model(input_wav)[0]\n\t        logits_fake (List[torch.Tensor]): logits_fake = disc_model(model(input_wav)[0])[0]\n\t    Returns:\n\t        lossd: discriminator loss\n\t    \"\"\"\n\t    relu = torch.nn.ReLU()\n\t    lossd = torch.tensor([0.0], device='cuda', requires_grad=True)\n\t    for tt1 in range(len(logits_real)):\n\t        lossd = lossd + torch.mean(relu(1-logits_real[tt1])) + torch.mean(relu(1+logits_fake[tt1]))\n\t    lossd = lossd / len(logits_real)\n", "    return lossd\n"]}
{"filename": "balancer.py", "chunked_list": ["from collections import defaultdict\n\timport typing as tp\n\timport torch\n\tfrom torch import autograd\n\tfrom .distrib import average_metrics\n\tdef averager(beta: float = 1):\n\t    \"\"\"\n\t    Exponential Moving Average callback.\n\t    Returns a single function that can be called to repeatidly update the EMA\n\t    with a dict of metrics. The callback will return\n", "    the new averaged dict of metrics.\n\t    Note that for `beta=1`, this is just plain averaging.\n\t    \"\"\"\n\t    fix: tp.Dict[str, float] = defaultdict(float)\n\t    total: tp.Dict[str, float] = defaultdict(float)\n\t    def _update(metrics: tp.Dict[str, tp.Any], weight: float = 1) -> tp.Dict[str, float]:\n\t        nonlocal total, fix\n\t        for key, value in metrics.items():\n\t            total[key] = total[key] * beta + weight * float(value)\n\t            fix[key] = fix[key] * beta + weight\n", "        return {key: tot / fix[key] for key, tot in total.items()}\n\t    return _update\n\tclass Balancer:\n\t    \"\"\"Loss balancer.\n\t    The loss balancer combines losses together to compute gradients for the backward.\n\t    A call to the balancer will weight the losses according the specified weight coefficients.\n\t    A call to the backward method of the balancer will compute the gradients, combining all the losses and\n\t    potentially rescaling the gradients, which can help stabilize the training and reasonate\n\t    about multiple losses with varying scales.\n\t    Expected usage:\n", "        weights = {'loss_a': 1, 'loss_b': 4}\n\t        balancer = Balancer(weights, ...)\n\t        losses: dict = {}\n\t        losses['loss_a'] = compute_loss_a(x, y)\n\t        losses['loss_b'] = compute_loss_b(x, y)\n\t        if model.training():\n\t            balancer.backward(losses, x)\n\t    ..Warning:: It is unclear how this will interact with DistributedDataParallel,\n\t        in particular if you have some losses not handled by the balancer. In that case\n\t        you can use `encodec.distrib.sync_grad(model.parameters())` and\n", "        `encodec.distrib.sync_buffwers(model.buffers())` as a safe alternative.\n\t    Args:\n\t        weights (Dict[str, float]): Weight coefficient for each loss. The balancer expect the losses keys\n\t            from the backward method to match the weights keys to assign weight to each of the provided loss.\n\t        rescale_grads (bool): Whether to rescale gradients or not, without. If False, this is just\n\t            a regular weighted sum of losses.\n\t        total_norm (float): Reference norm when rescaling gradients, ignored otherwise.\n\t        emay_decay (float): EMA decay for averaging the norms when `rescale_grads` is True.\n\t        per_batch_item (bool): Whether to compute the averaged norm per batch item or not. This only holds\n\t            when rescaling the gradients.\n", "        epsilon (float): Epsilon value for numerical stability.\n\t        monitor (bool): Whether to store additional ratio for each loss key in metrics.\n\t    \"\"\"\n\t    def __init__(self, weights: tp.Dict[str, float], rescale_grads: bool = True, total_norm: float = 1.,\n\t                 ema_decay: float = 0.999, per_batch_item: bool = True, epsilon: float = 1e-12,\n\t                 monitor: bool = False):\n\t        self.weights = weights\n\t        self.per_batch_item = per_batch_item\n\t        self.total_norm = total_norm\n\t        self.averager = averager(ema_decay)\n", "        self.epsilon = epsilon\n\t        self.monitor = monitor\n\t        self.rescale_grads = rescale_grads\n\t        self._metrics: tp.Dict[str, tp.Any] = {}\n\t    @property\n\t    def metrics(self):\n\t        return self._metrics\n\t    def backward(self, losses: tp.Dict[str, torch.Tensor], input: torch.Tensor):\n\t        norms = {}\n\t        grads = {}\n", "        for name, loss in losses.items():\n\t            grad, = autograd.grad(loss, [input], retain_graph=True)\n\t            if self.per_batch_item:\n\t                dims = tuple(range(1, grad.dim()))\n\t                norm = grad.norm(dim=dims).mean()\n\t            else:\n\t                norm = grad.norm()\n\t            norms[name] = norm\n\t            grads[name] = grad\n\t        count = 1\n", "        if self.per_batch_item:\n\t            count = len(grad)\n\t        avg_norms = average_metrics(self.averager(norms), count)\n\t        total = sum(avg_norms.values())\n\t        self._metrics = {}\n\t        if self.monitor:\n\t            for k, v in avg_norms.items():\n\t                self._metrics[f'ratio_{k}'] = v / total\n\t        total_weights = sum([self.weights[k] for k in avg_norms])\n\t        ratios = {k: w / total_weights for k, w in self.weights.items()}\n", "        out_grad: tp.Any = 0\n\t        for name, avg_norm in avg_norms.items():\n\t            if self.rescale_grads:\n\t                scale = ratios[name] * self.total_norm / (self.epsilon + avg_norm)\n\t                grad = grads[name] * scale\n\t            else:\n\t                grad = self.weights[name] * grads[name]\n\t            out_grad += grad\n\t        input.backward(out_grad)\n\tdef test():\n", "    from torch.nn import functional as F\n\t    x = torch.zeros(1, requires_grad=True)\n\t    one = torch.ones_like(x)\n\t    loss_1 = F.l1_loss(x, one)\n\t    loss_2 = 100 * F.l1_loss(x, -one)\n\t    losses = {'1': loss_1, '2': loss_2}\n\t    balancer = Balancer(weights={'1': 1, '2': 1}, rescale_grads=False)\n\t    balancer.backward(losses, x)\n\t    assert torch.allclose(x.grad, torch.tensor(99.)), x.grad\n\t    loss_1 = F.l1_loss(x, one)\n", "    loss_2 = 100 * F.l1_loss(x, -one)\n\t    losses = {'1': loss_1, '2': loss_2}\n\t    x.grad = None\n\t    balancer = Balancer(weights={'1': 1, '2': 1}, rescale_grads=True)\n\t    balancer.backward({'1': loss_1, '2': loss_2}, x)\n\t    assert torch.allclose(x.grad, torch.tensor(0.)), x.grad\n\tif __name__ == '__main__':\n\t    test()\n"]}
{"filename": "main.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Command-line for audio compression.\"\"\"\n\timport argparse\n\tfrom pathlib import Path\n\timport sys\n\timport torchaudio\n", "from compress import compress, decompress, MODELS\n\tfrom utils import save_audio, convert_audio\n\tSUFFIX = '.ecdc'\n\tdef get_parser():\n\t    parser = argparse.ArgumentParser(\n\t        'encodec',\n\t        description='High fidelity neural audio codec. '\n\t                    'If input is a .ecdc, decompresses it. '\n\t                    'If input is .wav, compresses it. If output is also wav, '\n\t                    'do a compression/decompression cycle.')\n", "    parser.add_argument(\n\t        'input', type=Path,\n\t        help='Input file, whatever is supported by torchaudio on your system.')\n\t    parser.add_argument(\n\t        'output', type=Path, nargs='?',\n\t        help='Output file, otherwise inferred from input file.')\n\t    parser.add_argument(\n\t        '-b', '--bandwidth', type=float, default=6, choices=[1.5, 3., 6., 12., 24.],\n\t        help='Target bandwidth (1.5, 3, 6, 12 or 24). 1.5 is not supported with --hq.')\n\t    parser.add_argument(\n", "        '-q', '--hq', action='store_true',\n\t        help='Use HQ stereo model operating on 48 kHz sampled audio.')\n\t    parser.add_argument(\n\t        '-l', '--lm', action='store_true',\n\t        help='Use a language model to reduce the model size (5x slower though).')\n\t    parser.add_argument(\n\t        '-f', '--force', action='store_true',\n\t        help='Overwrite output file if it exists.')\n\t    parser.add_argument(\n\t        '-s', '--decompress_suffix', type=str, default='_decompressed',\n", "        help='Suffix for the decompressed output file (if no output path specified)')\n\t    parser.add_argument(\n\t        '-r', '--rescale', action='store_true',\n\t        help='Automatically rescale the output to avoid clipping.')\n\t    parser.add_argument(\n\t        '-m','--model_name', type=str, default='encodec_24khz',\n\t        help='support encodec_24khz,encodec_48khz,my_encodec')\n\t    parser.add_argument(\n\t        '-c','--checkpoint', type=str, \n\t        help='if use my_encodec, please input checkpoint')\n", "    return parser\n\tdef fatal(*args):\n\t    print(*args, file=sys.stderr)\n\t    sys.exit(1)\n\tdef check_output_exists(args):\n\t    if not args.output.parent.exists():\n\t        fatal(f\"Output folder for {args.output} does not exist.\")\n\t    if args.output.exists() and not args.force:\n\t        fatal(f\"Output file {args.output} exist. Use -f / --force to overwrite.\")\n\tdef check_clipping(wav, args):\n", "    if args.rescale:\n\t        return\n\t    mx = wav.abs().max()\n\t    limit = 0.99\n\t    if mx > limit:\n\t        print(\n\t            f\"Clipping!! max scale {mx}, limit is {limit}. \"\n\t            \"To avoid clipping, use the `-r` option to rescale the output.\",\n\t            file=sys.stderr)\n\tdef main(args):\n", "    if args.input.suffix.lower() == SUFFIX:\n\t        # Decompression\n\t        if args.output is None:\n\t            args.output = args.input.with_name(args.input.stem + args.decompress_suffix).with_suffix('.wav')\n\t        elif args.output.suffix.lower() != '.wav':\n\t            fatal(\"Output extension must be .wav\")\n\t        check_output_exists(args)\n\t        out, out_sample_rate = decompress(args.input.read_bytes())\n\t        check_clipping(out, args)\n\t        save_audio(out, args.output, out_sample_rate, rescale=args.rescale)\n", "    else:\n\t        # Compression\n\t        if args.output is None:\n\t            args.output = args.input.with_suffix(SUFFIX)\n\t        elif args.output.suffix.lower() not in [SUFFIX, '.wav']:\n\t            fatal(f\"Output extension must be .wav or {SUFFIX}\")\n\t        check_output_exists(args)\n\t        if args.hq:\n\t            model_name = 'encodec_48khz'\n\t        else:\n", "            model_name = args.model_name\n\t        if model_name == 'my_encodec':\n\t            model = MODELS[model_name](args.checkpoint)\n\t        elif model_name == 'encodec_bw':\n\t            model = MODELS[model_name](args.checkpoint,[args.bandwidth])\n\t        else:\n\t            model = MODELS[model_name]()\n\t        print(f\"-------------USE {model_name} MODEL-------------\")\n\t        if args.bandwidth not in model.target_bandwidths:\n\t            fatal(f\"Bandwidth {args.bandwidth} is not supported by the model {model_name}\")\n", "        model.set_target_bandwidth(args.bandwidth)\n\t        wav, sr = torchaudio.load(args.input)\n\t        wav = convert_audio(wav, sr, model.sample_rate, model.channels)\n\t        compressed = compress(model, wav, use_lm=args.lm)\n\t        if args.output.suffix.lower() == SUFFIX:\n\t            args.output.write_bytes(compressed)\n\t        else:\n\t            # Directly run decompression stage\n\t            assert args.output.suffix.lower() == '.wav'\n\t            out, out_sample_rate = decompress(model,compressed)\n", "            check_clipping(out, args)\n\t            save_audio(out, args.output, out_sample_rate, rescale=args.rescale)\n\tdef test():\n\t    args = get_parser().parse_args()\n\t    if not args.input.exists():\n\t        fatal(f\"Input file {args.input} does not exist.\")\n\t    if args.input.is_dir():\n\t        output_root = args.output\n\t        if not output_root.exists():\n\t            output_root.mkdir(parents=True)\n", "        for wav in args.input.glob('**/*.wav'):\n\t            args.input = wav\n\t            print(f\"Processing {wav}\")\n\t            args.output = output_root.joinpath(wav.stem+f\"_bw{args.bandwidth}.wav\")\n\t            main(args)\n\t    elif args.input.is_file():\n\t        main(args)\n\tif __name__ == '__main__':\n\t    args = get_parser().parse_args()\n\t    if not args.input.exists():\n", "        fatal(f\"Input file {args.input} does not exist.\")\n\t    main(args) # if you want to test batch wav in a folder, please use test() instead of main(args)\n"]}
{"filename": "scheduler.py", "chunked_list": ["import torch\n\timport math\n\tfrom bisect import bisect_right\n\tfrom torch.optim.lr_scheduler import _LRScheduler\n\tclass WarmUpLR(_LRScheduler):\n\t    \"\"\"warmup_training learning rate scheduler\n\t    Args:\n\t        optimizer: optimzier(e.g. SGD)\n\t        total_iters: totoal_iters of warmup phase\n\t    \"\"\"\n", "    def __init__(self, optimizer, iter_per_epoch, warmup_epoch,last_epoch=-1):\n\t        self.total_iters = iter_per_epoch * warmup_epoch\n\t        self.iter_per_epoch = iter_per_epoch\n\t        super().__init__(optimizer, last_epoch)\n\t    def get_lr(self):\n\t        \"\"\"we will use the first m batches, and set the learning\n\t        rate to base_lr * m / total_iters\n\t        \"\"\"\n\t        return [base_lr * self.last_epoch/ (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n\tclass WarmupLrScheduler(_LRScheduler):\n", "    def __init__(\n\t            self,\n\t            optimizer,\n\t            warmup_iter=500,\n\t            warmup_ratio=5e-4,\n\t            warmup='exp',\n\t            last_epoch=-1,\n\t    ):\n\t        self.warmup_iter = warmup_iter\n\t        self.warmup_ratio = warmup_ratio\n", "        self.warmup = warmup\n\t        super(WarmupLrScheduler, self).__init__(optimizer, last_epoch)\n\t    def get_lr(self):\n\t        ratio = self.get_lr_ratio()\n\t        lrs = [ratio * lr for lr in self.base_lrs]\n\t        return lrs\n\t    def get_lr_ratio(self):\n\t        if self.last_epoch < self.warmup_iter:\n\t            ratio = self.get_warmup_ratio()\n\t        else:\n", "            ratio = self.get_main_ratio()\n\t        return ratio\n\t    def get_main_ratio(self):\n\t        raise NotImplementedError\n\t    def get_warmup_ratio(self):\n\t        assert self.warmup in ('linear', 'exp')\n\t        alpha = self.last_epoch / self.warmup_iter\n\t        if self.warmup == 'linear':\n\t            ratio = self.warmup_ratio + (1 - self.warmup_ratio) * alpha\n\t        elif self.warmup == 'exp':\n", "            ratio = self.warmup_ratio ** (1. - alpha)\n\t        return ratio\n\tclass WarmupPolyLrScheduler(WarmupLrScheduler):\n\t    def __init__(\n\t            self,\n\t            optimizer,\n\t            power,\n\t            max_iter,\n\t            warmup_iter=500,\n\t            warmup_ratio=5e-4,\n", "            warmup='exp',\n\t            last_epoch=-1,\n\t    ):\n\t        self.power = power\n\t        self.max_iter = max_iter\n\t        super(WarmupPolyLrScheduler, self).__init__(\n\t            optimizer, warmup_iter, warmup_ratio, warmup, last_epoch)\n\t    def get_main_ratio(self):\n\t        real_iter = self.last_epoch - self.warmup_iter\n\t        real_max_iter = self.max_iter - self.warmup_iter\n", "        alpha = real_iter / real_max_iter\n\t        ratio = (1 - alpha) ** self.power\n\t        return ratio\n\tclass WarmupExpLrScheduler(WarmupLrScheduler):\n\t    def __init__(\n\t            self,\n\t            optimizer,\n\t            gamma,\n\t            interval=1,\n\t            warmup_iter=500,\n", "            warmup_ratio=5e-4,\n\t            warmup='exp',\n\t            last_epoch=-1,\n\t    ):\n\t        self.gamma = gamma\n\t        self.interval = interval\n\t        super(WarmupExpLrScheduler, self).__init__(\n\t            optimizer, warmup_iter, warmup_ratio, warmup, last_epoch)\n\t    def get_main_ratio(self):\n\t        real_iter = self.last_epoch - self.warmup_iter\n", "        ratio = self.gamma ** (real_iter // self.interval)\n\t        return ratio\n\tclass WarmupCosineLrScheduler(WarmupLrScheduler):\n\t    def __init__(\n\t            self,\n\t            optimizer,\n\t            max_iter,\n\t            eta_ratio=0,\n\t            warmup_iter=500,\n\t            warmup_ratio=5e-4,\n", "            warmup='exp',\n\t            last_epoch=-1,\n\t    ):\n\t        self.eta_ratio = eta_ratio\n\t        self.max_iter = max_iter\n\t        super(WarmupCosineLrScheduler, self).__init__(\n\t            optimizer, warmup_iter, warmup_ratio, warmup, last_epoch)\n\t    def get_main_ratio(self):\n\t        real_iter = self.last_epoch - self.warmup_iter\n\t        real_max_iter = self.max_iter - self.warmup_iter\n", "        return self.eta_ratio + (1 - self.eta_ratio) * (\n\t                1 + math.cos(math.pi * self.last_epoch / real_max_iter)) / 2\n\tclass WarmupStepLrScheduler(WarmupLrScheduler):\n\t    def __init__(\n\t            self,\n\t            optimizer,\n\t            milestones: list,\n\t            gamma=0.1,\n\t            warmup_iter=500,\n\t            warmup_ratio=5e-4,\n", "            warmup='exp',\n\t            last_epoch=-1,\n\t    ):\n\t        self.milestones = milestones\n\t        self.gamma = gamma\n\t        super(WarmupStepLrScheduler, self).__init__(\n\t            optimizer, warmup_iter, warmup_ratio, warmup, last_epoch)\n\t    def get_main_ratio(self):\n\t        real_iter = self.last_epoch - self.warmup_iter\n\t        ratio = self.gamma ** bisect_right(self.milestones, real_iter)\n", "        return ratio\n\tif __name__ == \"__main__\":\n\t    model = torch.nn.Conv2d(3, 16, 3, 1, 1)\n\t    optim = torch.optim.SGD(model.parameters(), lr=1e-3)\n\t    max_iter = 20000\n\t    lr_scheduler = WarmupCosineLrScheduler(optim,max_iter=max_iter, warmup_iter=1000, warmup_ratio=1e-3)\n\t    lrs = []\n\t    for _ in range(max_iter):\n\t        lr = lr_scheduler.get_lr()[0]\n\t        print(lr)\n", "        lrs.append(lr)\n\t        lr_scheduler.step()\n\t    import matplotlib\n\t    import matplotlib.pyplot as plt\n\t    import numpy as np\n\t    lrs = np.array(lrs)\n\t    n_lrs = len(lrs)\n\t    plt.plot(np.arange(n_lrs), lrs)\n\t    plt.grid()\n\t    plt.show()\n"]}
{"filename": "msstftd.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"MS-STFT discriminator, provided here for reference.\"\"\"\n\timport typing as tp\n\timport torchaudio\n\timport torch\n\tfrom torch import nn\n", "from einops import rearrange\n\tfrom modules import NormConv2d\n\tFeatureMapType = tp.List[torch.Tensor]\n\tLogitsType = torch.Tensor\n\tDiscriminatorOutput = tp.Tuple[tp.List[LogitsType], tp.List[FeatureMapType]]\n\tdef get_2d_padding(kernel_size: tp.Tuple[int, int], dilation: tp.Tuple[int, int] = (1, 1)):\n\t    return (((kernel_size[0] - 1) * dilation[0]) // 2, ((kernel_size[1] - 1) * dilation[1]) // 2)\n\tclass DiscriminatorSTFT(nn.Module):\n\t    \"\"\"STFT sub-discriminator.\n\t    Args:\n", "        filters (int): Number of filters in convolutions\n\t        in_channels (int): Number of input channels. Default: 1\n\t        out_channels (int): Number of output channels. Default: 1\n\t        n_fft (int): Size of FFT for each scale. Default: 1024\n\t        hop_length (int): Length of hop between STFT windows for each scale. Default: 256\n\t        kernel_size (tuple of int): Inner Conv2d kernel sizes. Default: ``(3, 9)``\n\t        stride (tuple of int): Inner Conv2d strides. Default: ``(1, 2)``\n\t        dilations (list of int): Inner Conv2d dilation on the time dimension. Default: ``[1, 2, 4]``\n\t        win_length (int): Window size for each scale. Default: 1024\n\t        normalized (bool): Whether to normalize by magnitude after stft. Default: True\n", "        norm (str): Normalization method. Default: `'weight_norm'`\n\t        activation (str): Activation function. Default: `'LeakyReLU'`\n\t        activation_params (dict): Parameters to provide to the activation function.\n\t        growth (int): Growth factor for the filters. Default: 1\n\t    \"\"\"\n\t    def __init__(self, filters: int, in_channels: int = 1, out_channels: int = 1,\n\t                 n_fft: int = 1024, hop_length: int = 256, win_length: int = 1024, max_filters: int = 1024,\n\t                 filters_scale: int = 1, kernel_size: tp.Tuple[int, int] = (3, 9), dilations: tp.List = [1, 2, 4],\n\t                 stride: tp.Tuple[int, int] = (1, 2), normalized: bool = True, norm: str = 'weight_norm',\n\t                 activation: str = 'LeakyReLU', activation_params: dict = {'negative_slope': 0.2}):\n", "        super().__init__()\n\t        assert len(kernel_size) == 2\n\t        assert len(stride) == 2\n\t        self.filters = filters\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.n_fft = n_fft\n\t        self.hop_length = hop_length\n\t        self.win_length = win_length\n\t        self.normalized = normalized\n", "        self.activation = getattr(torch.nn, activation)(**activation_params)\n\t        self.spec_transform = torchaudio.transforms.Spectrogram(\n\t            n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.win_length, window_fn=torch.hann_window,\n\t            normalized=self.normalized, center=False, pad_mode=None, power=None)\n\t        spec_channels = 2 * self.in_channels\n\t        self.convs = nn.ModuleList()\n\t        self.convs.append(\n\t            NormConv2d(spec_channels, self.filters, kernel_size=kernel_size, padding=get_2d_padding(kernel_size))\n\t        )\n\t        in_chs = min(filters_scale * self.filters, max_filters)\n", "        for i, dilation in enumerate(dilations):\n\t            out_chs = min((filters_scale ** (i + 1)) * self.filters, max_filters)\n\t            self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride,\n\t                                         dilation=(dilation, 1), padding=get_2d_padding(kernel_size, (dilation, 1)),\n\t                                         norm=norm))\n\t            in_chs = out_chs\n\t        out_chs = min((filters_scale ** (len(dilations) + 1)) * self.filters, max_filters)\n\t        self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=(kernel_size[0], kernel_size[0]),\n\t                                     padding=get_2d_padding((kernel_size[0], kernel_size[0])),\n\t                                     norm=norm))\n", "        self.conv_post = NormConv2d(out_chs, self.out_channels,\n\t                                    kernel_size=(kernel_size[0], kernel_size[0]),\n\t                                    padding=get_2d_padding((kernel_size[0], kernel_size[0])),\n\t                                    norm=norm)\n\t    def forward(self, x: torch.Tensor):\n\t        \"\"\"Discriminator STFT Module is the sub module of MultiScaleSTFTDiscriminator.\n\t        Args:\n\t            x (torch.Tensor): input tensor of shape [B, 1, Time]\n\t        Returns:\n\t            z: z is the output of the last convolutional layer of shape\n", "            fmap: fmap is the list of feature maps of every convolutional layer of shape\n\t        \"\"\"\n\t        fmap = []\n\t        z = self.spec_transform(x)  # [B, 2, Freq, Frames, 2]\n\t        z = torch.cat([z.real, z.imag], dim=1)\n\t        z = rearrange(z, 'b c w t -> b c t w')\n\t        for i, layer in enumerate(self.convs):\n\t            z = layer(z)\n\t            z = self.activation(z)\n\t            fmap.append(z)\n", "        z = self.conv_post(z)\n\t        return z, fmap\n\tclass MultiScaleSTFTDiscriminator(nn.Module):\n\t    \"\"\"Multi-Scale STFT (MS-STFT) discriminator.\n\t    Args:\n\t        filters (int): Number of filters in convolutions\n\t        in_channels (int): Number of input channels. Default: 1\n\t        out_channels (int): Number of output channels. Default: 1\n\t        n_ffts (Sequence[int]): Size of FFT for each scale\n\t        hop_lengths (Sequence[int]): Length of hop between STFT windows for each scale\n", "        win_lengths (Sequence[int]): Window size for each scale\n\t        **kwargs: additional args for STFTDiscriminator\n\t    \"\"\"\n\t    def __init__(self, filters: int, in_channels: int = 1, out_channels: int = 1,\n\t                 n_ffts: tp.List[int] = [1024, 2048, 512], hop_lengths: tp.List[int] = [256, 512, 128],\n\t                 win_lengths: tp.List[int] = [1024, 2048, 512], **kwargs):\n\t        super().__init__()\n\t        assert len(n_ffts) == len(hop_lengths) == len(win_lengths)\n\t        self.discriminators = nn.ModuleList([\n\t            DiscriminatorSTFT(filters, in_channels=in_channels, out_channels=out_channels,\n", "                              n_fft=n_ffts[i], win_length=win_lengths[i], hop_length=hop_lengths[i], **kwargs)\n\t            for i in range(len(n_ffts))\n\t        ])\n\t        self.num_discriminators = len(self.discriminators)\n\t    def forward(self, x: torch.Tensor) -> DiscriminatorOutput:\n\t        \"\"\"Multi-Scale STFT (MS-STFT) discriminator.\n\t        Args:\n\t            x (torch.Tensor): input waveform\n\t        Returns:\n\t            logits: list of every discriminator's output\n", "            fmaps: list of every discriminator's feature maps, \n\t                each feature maps is a list of Discriminator STFT's every layer\n\t        \"\"\"\n\t        logits = []\n\t        fmaps = []\n\t        for disc in self.discriminators:\n\t            logit, fmap = disc(x) #\n\t            #TODO: logits 是否需要downsample + scale是否对齐\n\t            logits.append(logit)\n\t            fmaps.append(fmap)\n", "        return logits, fmaps\n\tdef test():\n\t    disc = MultiScaleSTFTDiscriminator(filters=32)\n\t    y = torch.randn(1, 1, 24000)\n\t    y_hat = torch.randn(1, 1, 24000)\n\t    y_disc_r, fmap_r = disc(y)\n\t    y_disc_gen, fmap_gen = disc(y_hat)\n\t    assert len(y_disc_r) == len(y_disc_gen) == len(fmap_r) == len(fmap_gen) == disc.num_discriminators\n\t    assert all([len(fm) == 5 for fm in fmap_r + fmap_gen])\n\t    assert all([list(f.shape)[:2] == [1, 32] for fm in fmap_r + fmap_gen for f in fm])\n", "    assert all([len(logits.shape) == 4 for logits in y_disc_r + y_disc_gen])\n\t    ##################Zhikang Niu Test######################\n\t    print(type(y_disc_r))\n\t    print(type(fmap_r))\n\t    print(type(y_disc_gen))\n\t    print(type(fmap_gen))\n\t    # for logits in y_disc_gen:\n\t        # print(logits.shape) # [1, 1, ?, ?,]\n\t        # print(logits)\n\t    # print(len(y_disc_gen)) # len = 3\n", "    for fmap in fmap_gen:\n\t        # print(len(fmap))  # len = 5\n\t        # print(fmap)\n\t        for f in fmap:\n\t            print(f.shape)\n\t            print(\"---------------\")\n\t        print(\"+++++++++++++++++++++\")\n\t    # print(len(fmap_gen)) # len = 3\n\t    print(disc)\n\t    print(len(fmap_gen))\n", "    print(len(fmap_gen[0]))\n\t    print(len(y_disc_gen))\n\t    print(disc)\n\t    # norm2d = NormConv2d()\n\tif __name__ == '__main__':\n\t    test()\n"]}
{"filename": "distrib.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Torch distributed utilities.\"\"\"\n\timport typing as tp\n\timport torch\n\tdef rank():\n\t    if torch.distributed.is_initialized():\n", "        return torch.distributed.get_rank()\n\t    else:\n\t        return 0\n\tdef world_size():\n\t    if torch.distributed.is_initialized():\n\t        return torch.distributed.get_world_size()\n\t    else:\n\t        return 1\n\tdef is_distributed():\n\t    return world_size() > 1\n", "def all_reduce(tensor: torch.Tensor, op=torch.distributed.ReduceOp.SUM):\n\t    if is_distributed():\n\t        return torch.distributed.all_reduce(tensor, op)\n\tdef _is_complex_or_float(tensor):\n\t    return torch.is_floating_point(tensor) or torch.is_complex(tensor)\n\tdef _check_number_of_params(params: tp.List[torch.Tensor]):\n\t    # utility function to check that the number of params in all workers is the same,\n\t    # and thus avoid a deadlock with distributed all reduce.\n\t    if not is_distributed() or not params:\n\t        return\n", "    tensor = torch.tensor([len(params)], device=params[0].device, dtype=torch.long)\n\t    all_reduce(tensor)\n\t    if tensor.item() != len(params) * world_size():\n\t        # If not all the workers have the same number, for at least one of them,\n\t        # this inequality will be verified.\n\t        raise RuntimeError(f\"Mismatch in number of params: ours is {len(params)}, \"\n\t                           \"at least one worker has a different one.\")\n\tdef broadcast_tensors(tensors: tp.Iterable[torch.Tensor], src: int = 0):\n\t    \"\"\"Broadcast the tensors from the given parameters to all workers.\n\t    This can be used to ensure that all workers have the same model to start with.\n", "    \"\"\"\n\t    if not is_distributed():\n\t        return\n\t    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]\n\t    _check_number_of_params(tensors)\n\t    handles = []\n\t    for tensor in tensors:\n\t        handle = torch.distributed.broadcast(tensor.data, src=src, async_op=True)\n\t        handles.append(handle)\n\t    for handle in handles:\n", "        handle.wait()\n\tdef sync_buffer(buffers, average=True):\n\t    \"\"\"\n\t    Sync grad for buffers. If average is False, broadcast instead of averaging.\n\t    \"\"\"\n\t    if not is_distributed():\n\t        return\n\t    handles = []\n\t    for buffer in buffers:\n\t        if torch.is_floating_point(buffer.data):\n", "            if average:\n\t                handle = torch.distributed.all_reduce(\n\t                    buffer.data, op=torch.distributed.ReduceOp.SUM, async_op=True)\n\t            else:\n\t                handle = torch.distributed.broadcast(\n\t                    buffer.data, src=0, async_op=True)\n\t            handles.append((buffer, handle))\n\t    for buffer, handle in handles:\n\t        handle.wait()\n\t        if average:\n", "            buffer.data /= world_size\n\tdef sync_grad(params):\n\t    \"\"\"\n\t    Simpler alternative to DistributedDataParallel, that doesn't rely\n\t    on any black magic. For simple models it can also be as fast.\n\t    Just call this on your model parameters after the call to backward!\n\t    \"\"\"\n\t    if not is_distributed():\n\t        return\n\t    handles = []\n", "    for p in params:\n\t        if p.grad is not None:\n\t            handle = torch.distributed.all_reduce(\n\t                p.grad.data, op=torch.distributed.ReduceOp.SUM, async_op=True)\n\t            handles.append((p, handle))\n\t    for p, handle in handles:\n\t        handle.wait()\n\t        p.grad.data /= world_size()\n\tdef average_metrics(metrics: tp.Dict[str, float], count=1.):\n\t    \"\"\"Average a dictionary of metrics across all workers, using the optional\n", "    `count` as unnormalized weight.\n\t    \"\"\"\n\t    if not is_distributed():\n\t        return metrics\n\t    keys, values = zip(*metrics.items())\n\t    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\t    tensor = torch.tensor(list(values) + [1], device=device, dtype=torch.float32)\n\t    tensor *= count\n\t    all_reduce(tensor)\n\t    averaged = (tensor[:-1] / tensor[-1]).cpu().tolist()\n", "    return dict(zip(keys, averaged))\n"]}
{"filename": "utils.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Various utilities.\"\"\"\n\tfrom hashlib import sha256\n\tfrom pathlib import Path\n\timport typing as tp\n\timport numpy as np\n", "import random\n\timport torch\n\timport torchaudio\n\tdef _linear_overlap_add(frames: tp.List[torch.Tensor], stride: int):\n\t    # Generic overlap add, with linear fade-in/fade-out, supporting complex scenario\n\t    # e.g., more than 2 frames per position.\n\t    # The core idea is to use a weight function that is a triangle,\n\t    # with a maximum value at the middle of the segment.\n\t    # We use this weighting when summing the frames, and divide by the sum of weights\n\t    # for each positions at the end. Thus:\n", "    #   - if a frame is the only one to cover a position, the weighting is a no-op.\n\t    #   - if 2 frames cover a position:\n\t    #          ...  ...\n\t    #         /   \\/   \\\n\t    #        /    /\\    \\\n\t    #            S  T       , i.e. S offset of second frame starts, T end of first frame.\n\t    # Then the weight function for each one is: (t - S), (T - t), with `t` a given offset.\n\t    # After the final normalization, the weight of the second frame at position `t` is\n\t    # (t - S) / (t - S + (T - t)) = (t - S) / (T - S), which is exactly what we want.\n\t    #\n", "    #   - if more than 2 frames overlap at a given point, we hope that by induction\n\t    #      something sensible happens.\n\t    assert len(frames)\n\t    device = frames[0].device\n\t    dtype = frames[0].dtype\n\t    shape = frames[0].shape[:-1]\n\t    total_size = stride * (len(frames) - 1) + frames[-1].shape[-1]\n\t    frame_length = frames[0].shape[-1]\n\t    t = torch.linspace(0, 1, frame_length + 2, device=device, dtype=dtype)[1: -1]\n\t    weight = 0.5 - (t - 0.5).abs()\n", "    sum_weight = torch.zeros(total_size, device=device, dtype=dtype)\n\t    out = torch.zeros(*shape, total_size, device=device, dtype=dtype)\n\t    offset: int = 0\n\t    for frame in frames:\n\t        frame_length = frame.shape[-1]\n\t        out[..., offset:offset + frame_length] += weight[:frame_length] * frame\n\t        sum_weight[offset:offset + frame_length] += weight[:frame_length]\n\t        offset += stride\n\t    assert sum_weight.min() > 0\n\t    return out / sum_weight\n", "def _get_checkpoint_url(root_url: str, checkpoint: str):\n\t    if not root_url.endswith('/'):\n\t        root_url += '/'\n\t    return root_url + checkpoint\n\tdef _check_checksum(path: Path, checksum: str):\n\t    sha = sha256()\n\t    with open(path, 'rb') as file:\n\t        while True:\n\t            buf = file.read(2**20)\n\t            if not buf:\n", "                break\n\t            sha.update(buf)\n\t    actual_checksum = sha.hexdigest()[:len(checksum)]\n\t    if actual_checksum != checksum:\n\t        raise RuntimeError(f'Invalid checksum for file {path}, '\n\t                           f'expected {checksum} but got {actual_checksum}')\n\tdef convert_audio(wav: torch.Tensor, sr: int, target_sr: int, target_channels: int):\n\t    assert wav.dim() >= 2, \"Audio tensor must have at least 2 dimensions\"\n\t    assert wav.shape[-2] in [1, 2], \"Audio must be mono or stereo.\"\n\t    *shape, channels, length = wav.shape\n", "    if target_channels == 1:\n\t        wav = wav.mean(-2, keepdim=True)\n\t    elif target_channels == 2:\n\t        wav = wav.expand(*shape, target_channels, length)\n\t    elif channels == 1:\n\t        wav = wav.expand(target_channels, -1)\n\t    else:\n\t        raise RuntimeError(f\"Impossible to convert from {channels} to {target_channels}\")\n\t    wav = torchaudio.transforms.Resample(sr, target_sr)(wav)\n\t    return wav\n", "def save_audio(wav: torch.Tensor, path: tp.Union[Path, str],\n\t               sample_rate: int, rescale: bool = False):\n\t    limit = 0.99\n\t    mx = wav.abs().max()\n\t    if rescale:\n\t        wav = wav * min(limit / mx, 1)\n\t    else:\n\t        wav = wav.clamp(-limit, limit)\n\t    torchaudio.save(str(path), wav, sample_rate=sample_rate, encoding='PCM_S', bits_per_sample=16)\n\tdef set_seed(seed):\n", "    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)\n\t    torch.backends.cudnn.deterministic = True\n\t    torch.backends.cudnn.benchmark = False\n\t    np.random.seed(seed)\n\t    random.seed(seed)"]}
{"filename": "customAudioDataset.py", "chunked_list": ["import os\n\timport pandas as pd\n\timport torch\n\timport torchaudio\n\timport random\n\tfrom utils import convert_audio\n\tclass CustomAudioDataset(torch.utils.data.Dataset):\n\t    def __init__(self, config, transform=None,mode='train'):\n\t        assert mode in ['train', 'test'], 'dataset mode must be train or test'\n\t        if mode == 'train':\n", "            self.audio_files = pd.read_csv(config.datasets.train_csv_path,sep=\"/n\",on_bad_lines='skip')\n\t        elif mode == 'test':\n\t            self.audio_files = pd.read_csv(config.datasets.test_csv_path,sep=\"/n\",on_bad_lines='skip',)\n\t        self.transform = transform\n\t        self.fixed_length = config.datasets.fixed_length\n\t        self.tensor_cut = config.datasets.tensor_cut\n\t        self.sample_rate = config.model.sample_rate\n\t        self.channels = config.model.channels\n\t    def __len__(self):\n\t        if self.fixed_length:\n", "            return self.fixed_length\n\t        return len(self.audio_files)\n\t    def __getitem__(self, idx):\n\t        waveform, sample_rate = torchaudio.load(self.audio_files.iloc[idx, :].values[0])\n\t        \"\"\"you can preprocess the waveform's sample rate to save time and memory\"\"\"\n\t        if sample_rate != self.sample_rate:\n\t            waveform = convert_audio(waveform, sample_rate, self.sample_rate, self.channels)\n\t        if self.transform:\n\t            waveform = self.transform(waveform)\n\t        if self.tensor_cut > 0:\n", "            if waveform.size()[1] > self.tensor_cut:\n\t                start = random.randint(0, waveform.size()[1]-self.tensor_cut-1) # random start point\n\t                waveform = waveform[:, start:start+self.tensor_cut] # cut tensor\n\t                return waveform, self.sample_rate\n\t            else:\n\t                return waveform, self.sample_rate\n\tdef pad_sequence(batch):\n\t    # Make all tensor in a batch the same length by padding with zeros\n\t    batch = [item.permute(1, 0) for item in batch]\n\t    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n", "    batch = batch.permute(0, 2, 1)\n\t    return batch\n\tdef collate_fn(batch):\n\t    tensors = []\n\t    for waveform, _ in batch:\n\t        tensors += [waveform]\n\t    # Group the list of tensors into a batched tensor\n\t    tensors = pad_sequence(tensors)\n\t    return tensors"]}
{"filename": "audio_to_mel.py", "chunked_list": ["import torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch\n\tfrom librosa.filters import mel as librosa_mel_fn\n\tfrom torch.nn.utils import weight_norm\n\timport numpy as np\n\tclass Audio2Mel(nn.Module):\n\t    def __init__(\n\t        self,\n\t        n_fft=1024,\n", "        hop_length=256,\n\t        win_length=1024,\n\t        sampling_rate=22050,\n\t        n_mel_channels=80,\n\t        mel_fmin=0.0,\n\t        mel_fmax=None,\n\t        device='cuda'\n\t    ):\n\t        super().__init__()\n\t        ##############################################\n", "        # FFT Parameters                              #\n\t        ##############################################\n\t        window = torch.hann_window(win_length, device=device).float()\n\t        mel_basis = librosa_mel_fn(sr=sampling_rate,n_fft=n_fft,n_mels=n_mel_channels,fmin=mel_fmin,fmax=mel_fmax)\n\t        mel_basis = torch.from_numpy(mel_basis).cuda().float()\n\t        self.register_buffer(\"mel_basis\", mel_basis)\n\t        self.register_buffer(\"window\", window)\n\t        self.n_fft = n_fft\n\t        self.hop_length = hop_length\n\t        self.win_length = win_length\n", "        self.sampling_rate = sampling_rate\n\t        self.n_mel_channels = n_mel_channels\n\t    def forward(self, audioin):\n\t        p = (self.n_fft - self.hop_length) // 2\n\t        audio = F.pad(audioin, (p, p), \"reflect\").squeeze(1)\n\t        fft = torch.stft(\n\t            audio,\n\t            n_fft=self.n_fft,\n\t            hop_length=self.hop_length,\n\t            win_length=self.win_length,\n", "            window=self.window,\n\t            center=False,\n\t            return_complex=False,\n\t        )\n\t        mel_output = torch.matmul(self.mel_basis, torch.sum(torch.pow(fft, 2), dim=[-1]))\n\t        log_mel_spec = torch.log10(torch.clamp(mel_output, min=1e-5))\n\t        return log_mel_spec"]}
{"filename": "cal_metrics.py", "chunked_list": ["# core codes are copy from https://github.com/yangdongchao/AcademiCodec/tree/master/evaluation_metric/calculate_voc_obj_metrics/metrics\n\timport argparse\n\tfrom pesq import pesq,cypesq\n\tfrom pystoi import stoi\n\tfrom pathlib import Path\n\timport librosa\n\timport numpy as np\n\tfrom tqdm import tqdm\n\tfrom audiotools.metrics.quality import visqol\n\tdef get_parser():\n", "    parser = argparse.ArgumentParser(description=\"Compute STOI and PESQ measure\")\n\t    parser.add_argument(\n\t        '-r',\n\t        '--ref_dir',\n\t        required=True,\n\t        help=\"Reference wave folder.\"\n\t    )\n\t    parser.add_argument(\n\t        '-d',\n\t        '--deg_dir',\n", "        required=True,\n\t        help=\"Degraded wave folder.\"\n\t    )\n\t    parser.add_argument(\n\t        '-s',\n\t        '--sr',\n\t        type=int,\n\t        default=24000,\n\t        help=\"encodec sample rate.\"\n\t    )\n", "    parser.add_argument(\n\t        '-b',\n\t        '--bandwidth',\n\t        type=float,\n\t        default=6.0,\n\t        help=\"encodec bandwidth.\",\n\t        choices=[1.5, 3.0, 6.0, 12.0, 24.0, 48.0]\n\t    )\n\t    return parser\n\tdef calculate_stoi(ref_wav, deg_wav, sr):\n", "    \"\"\"Calculate STOI score between ref_wav and deg_wav\"\"\"\n\t    min_len = min(len(ref_wav), len(deg_wav))\n\t    ref_wav = ref_wav[:min_len]\n\t    deg_wav = deg_wav[:min_len]\n\t    stoi_score = stoi(ref_wav, deg_wav, sr, extended=False)\n\t    return stoi_score\n\tdef calculate_pesq(ref_wav, deg_wav, sr):\n\t    \"\"\"Calculate PESQ score between ref_wav and deg_wav, we need to resample to 16000Hz first\"\"\"\n\t    min_len = min(len(ref_wav), len(deg_wav))\n\t    ref_wav = ref_wav[:min_len]\n", "    deg_wav = deg_wav[:min_len]\n\t    nb_pesq_score = pesq(sr, ref_wav, deg_wav, 'nb')\n\t    wb_pesq_score = pesq(sr, ref_wav, deg_wav, 'wb')\n\t    return nb_pesq_score, wb_pesq_score\n\tdef calculate_visqol(ref_wav,deg_wav,mode='audio'):\n\t    pass\n\tdef main():\n\t    args = get_parser().parse_args()\n\t    stoi_scores = []\n\t    nb_pesq_scores = []\n", "    wb_pesq_scores = []\n\t    for deg_wav_path in tqdm(list(Path(args.deg_dir).rglob('*.wav'))):\n\t        relative_path = deg_wav_path.relative_to(args.deg_dir)\n\t        ref_wav_path = Path(args.ref_dir) / relative_path.parents[0] /deg_wav_path.name.replace(f'_bw{args.bandwidth}', '')\n\t        ref_wav,_ = librosa.load(ref_wav_path, sr=args.sr)\n\t        deg_wav,_ = librosa.load(deg_wav_path, sr=args.sr)\n\t        stoi_score = calculate_stoi(ref_wav, deg_wav, sr=args.sr)\n\t        if args.sr != 16000:\n\t            ref_wav = librosa.resample(y=ref_wav, orig_sr=args.sr, target_sr=16000)\n\t            deg_wav = librosa.resample(y=deg_wav, orig_sr=args.sr, target_sr=16000)\n", "        try:\n\t            nb_pesq_score, wb_pesq_score = calculate_pesq(ref_wav, deg_wav, 16000)\n\t            nb_pesq_scores.append(nb_pesq_score)\n\t            wb_pesq_scores.append(wb_pesq_score)\n\t        except cypesq.NoUtterancesError:\n\t            print(ref_wav_path)\n\t            print(deg_wav_path)\n\t            nb_pesq_score, wb_pesq_score = 0, 0\n\t        if stoi_score!=1e-5:\n\t            stoi_scores.append(stoi_score)\n", "    return np.mean(stoi_scores), np.mean(nb_pesq_scores), np.mean(wb_pesq_scores)\n\tif __name__ == '__main__':\n\t    mean_stoi, mean_nb_pesq, mean_wb_pesq = main()\n\t    print(f\"STOI: {mean_stoi}\")\n\t    print(f\"NB PESQ: {mean_nb_pesq}\")\n\t    print(f\"WB PESQ: {mean_wb_pesq}\")\n"]}
{"filename": "train_multi_gpu.py", "chunked_list": ["import os\n\timport random\n\timport torch\n\timport torch.optim as optim\n\timport customAudioDataset as data\n\tfrom customAudioDataset import collate_fn\n\tfrom utils import set_seed\n\tfrom tqdm import tqdm\n\tfrom model import EncodecModel \n\tfrom msstftd import MultiScaleSTFTDiscriminator\n", "from losses import total_loss, disc_loss\n\tfrom scheduler import WarmupCosineLrScheduler\n\timport torch.distributed as dist\n\timport torch.multiprocessing as mp\n\timport hydra\n\timport logging\n\timport warnings\n\twarnings.filterwarnings(\"ignore\")\n\tlogger = logging.getLogger()\n\tlogger.setLevel(logging.DEBUG)\n", "# Define train one step function\n\tdef train_one_step(epoch,optimizer,optimizer_disc, model, disc_model, trainloader,config,scheduler,disc_scheduler):\n\t    \"\"\"train one step function\n\t    Args:\n\t        epoch (int): current epoch\n\t        optimizer (_type_) : generator optimizer\n\t        optimizer_disc (_type_): discriminator optimizer\n\t        model (_type_): generator model\n\t        disc_model (_type_): discriminator model\n\t        trainloader (_type_): train dataloader\n", "        config (_type_): hydra config file\n\t        scheduler (_type_): adjust generate model learning rate\n\t        disc_scheduler (_type_): adjust discriminator model learning rate\n\t        warmup_scheduler (_type_): warmup learning rate\n\t    \"\"\"\n\t    model.train()\n\t    disc_model.train()\n\t    for input_wav in tqdm(trainloader):\n\t        # warmup learning rate, warmup_epoch is defined in config file,default is 5\n\t        input_wav = input_wav.cuda() #[B, 1, T]: eg. [2, 1, 203760]\n", "        optimizer.zero_grad()\n\t        optimizer_disc.zero_grad()\n\t        output, loss_w, _ = model(input_wav) #output: [B, 1, T]: eg. [2, 1, 203760] | loss_w: [1] \n\t        logits_real, fmap_real = disc_model(input_wav)\n\t        logits_fake, fmap_fake = disc_model(output)\n\t        loss_g = total_loss(fmap_real, logits_fake, fmap_fake, input_wav, output) \n\t        loss = loss_g + loss_w\n\t        loss.backward()\n\t        optimizer.step()\n\t        scheduler.step()\n", "        # train discriminator when epoch > warmup_epoch and train_discriminator is True\n\t        if config.model.train_discriminator and epoch > config.lr_scheduler.warmup_epoch:\n\t            logits_fake, _ = disc_model(output.detach()) # detach to avoid backpropagation to model\n\t            loss_disc = disc_loss([logit_real.detach() for logit_real in logits_real], logits_fake) # compute discriminator loss\n\t            loss_disc.backward() \n\t            optimizer_disc.step()\n\t            disc_scheduler.step()\n\t    if not config.distributed.data_parallel or dist.get_rank()==0:\n\t        logger.info(f'| epoch: {epoch} | loss: {loss.item()} | loss_g: {loss_g.item()} | loss_w: {loss_w.item()} | lr: {optimizer.param_groups[0][\"lr\"]} | disc_lr: {optimizer_disc.param_groups[0][\"lr\"]}')\n\t        if config.model.train_discriminator and epoch > config.lr_scheduler.warmup_epoch:\n", "            logger.info(f'| loss_disc: {loss_disc.item()}')\n\t@torch.no_grad()\n\tdef test(epoch,model, disc_model, testloader,config):\n\t    model.eval()\n\t    for input_wav in tqdm(testloader):\n\t        input_wav = input_wav.cuda() #[B, 1, T]: eg. [2, 1, 203760]\n\t        output = model(input_wav) #output: [B, 1, T]: eg. [2, 1, 203760] | loss_w: [1] \n\t        logits_real, fmap_real = disc_model(input_wav)\n\t        logits_fake, fmap_fake = disc_model(output)\n\t        loss_disc = disc_loss(logits_real, logits_fake) # compute discriminator loss\n", "        loss_g = total_loss(fmap_real, logits_fake, fmap_fake, input_wav, output) \n\t    if not config.distributed.data_parallel or dist.get_rank()==0:\n\t        logger.info(f'| TEST | epoch: {epoch} | loss_g: {loss_g.item()} | loss_disc: {loss_disc.item()}')\n\tdef train(local_rank,world_size,config):\n\t    \"\"\"train main function.\"\"\"\n\t    # set logger\n\t    file_handler = logging.FileHandler(f\"train_encodec_bs{config.datasets.batch_size}_lr{config.optimization.lr}.log\")\n\t    formatter = logging.Formatter('%(asctime)s: %(levelname)s: [%(filename)s: %(lineno)d]: %(message)s')\n\t    file_handler.setFormatter(formatter)\n\t    # print to screen\n", "    stream_handler = logging.StreamHandler()\n\t    stream_handler.setLevel(logging.ERROR)\n\t    # add handlers to logger\n\t    logger.addHandler(file_handler)\n\t    logger.addHandler(stream_handler)\n\t    # set seed\n\t    if config.common.seed is not None:\n\t        set_seed(config.common.seed)\n\t    # set train dataset\n\t    trainset = data.CustomAudioDataset(config=config)\n", "    testset = data.CustomAudioDataset(config=config,mode='test')\n\t    # set encodec model and discriminator model\n\t    model = EncodecModel._get_model(\n\t                config.model.target_bandwidths, \n\t                config.model.sample_rate, \n\t                config.model.channels,\n\t                causal=False, model_norm='time_group_norm', \n\t                audio_normalize=config.model.audio_normalize,\n\t                segment=1., name='my_encodec')\n\t    disc_model = MultiScaleSTFTDiscriminator(filters=config.model.filters)\n", "    # log model, disc model parameters and train mode\n\t    logger.info(config)\n\t    logger.info(f\"Encodec Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\t    logger.info(f\"Disc Model Parameters: {sum(p.numel() for p in disc_model.parameters() if p.requires_grad)}\")\n\t    logger.info(f\"model train mode :{model.training} | quantizer train mode :{model.quantizer.training} \")\n\t    # resume training\n\t    resume_epoch = 1\n\t    if config.checkpoint.resume:\n\t        # check the checkpoint_path\n\t        assert config.checkpoint.checkpoint_path != '', \"resume path is empty\"\n", "        assert config.checkpoint.disc_checkpoint_path != '', \"disc resume path is empty\"\n\t        model_checkpoint = torch.load(config.checkpoint.checkpoint_path, map_location='cpu')\n\t        disc_model_checkpoint = torch.load(config.checkpoint.disc_checkpoint_path, map_location='cpu')\n\t        model.load_state_dict(model_checkpoint['model_state_dict'])\n\t        disc_model.load_state_dict(disc_model_checkpoint['model_state_dict'])\n\t        resume_epoch = model_checkpoint['epoch']\n\t        if resume_epoch > config.common.max_epoch:\n\t            raise ValueError(f\"resume epoch {resume_epoch} is larger than total epochs {config.common.epochs}\")\n\t    train_sampler = None\n\t    test_sampler = None\n", "    if config.distributed.data_parallel:\n\t        # distributed init\n\t        os.environ['MASTER_ADDR'] = 'localhost'\n\t        os.environ['MASTER_PORT'] = '12455'\n\t        torch.distributed.init_process_group(backend='nccl',rank=local_rank,world_size=world_size)\n\t        torch.cuda.set_device(local_rank) \n\t        torch.cuda.empty_cache()\n\t        # set distributed sampler\n\t        train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n\t        test_sampler = torch.utils.data.distributed.DistributedSampler(testset)\n", "    model.cuda()\n\t    disc_model.cuda()\n\t    trainloader = torch.utils.data.DataLoader(\n\t        trainset,\n\t        batch_size=config.datasets.batch_size,\n\t        sampler=train_sampler, \n\t        shuffle=(train_sampler is None), collate_fn=collate_fn,\n\t        pin_memory=config.datasets.pin_memory)\n\t    testloader = torch.utils.data.DataLoader(\n\t        testset,\n", "        batch_size=config.datasets.batch_size,\n\t        sampler=test_sampler, \n\t        shuffle=False, collate_fn=collate_fn,\n\t        pin_memory=config.datasets.pin_memory)\n\t    # set optimizer and scheduler, warmup scheduler\n\t    params = [p for p in model.parameters() if p.requires_grad]\n\t    disc_params = [p for p in disc_model.parameters() if p.requires_grad]\n\t    optimizer = optim.Adam([{'params': params, 'lr': config.optimization.lr}], betas=(0.5, 0.9))\n\t    optimizer_disc = optim.Adam([{'params':disc_params, 'lr': config.optimization.disc_lr}], betas=(0.5, 0.9))\n\t    scheduler = WarmupCosineLrScheduler(optimizer, max_iter=config.common.max_epoch*len(trainloader), eta_ratio=0.1, warmup_iter=config.lr_scheduler.warmup_epoch*len(trainloader), warmup_ratio=1e-4)\n", "    disc_scheduler = WarmupCosineLrScheduler(optimizer_disc, max_iter=config.common.max_epoch*len(trainloader), eta_ratio=0.1, warmup_iter=config.lr_scheduler.warmup_epoch*len(trainloader), warmup_ratio=1e-4)\n\t    # scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0)\n\t    # disc_scheduler = CosineAnnealingLR(optimizer_disc, T_max=100, eta_min=0)\n\t    # iter_per_epoch = len(trainloader)\n\t    # warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch,config.lr_scheduler.warmup_epoch)\n\t    if config.checkpoint.resume and 'scheduler_state_dict' in model_checkpoint.keys() and 'scheduler_state_dict' in disc_model_checkpoint.keys(): \n\t        optimizer.load_state_dict(model_checkpoint['optimizer_state_dict'])\n\t        scheduler.load_state_dict(model_checkpoint['scheduler_state_dict'])\n\t        optimizer_disc.load_state_dict(disc_model_checkpoint['optimizer_state_dict'])\n\t        disc_scheduler.load_state_dict(disc_model_checkpoint['scheduler_state_dict'])\n", "    if config.distributed.data_parallel:\n\t        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n\t        disc_model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(disc_model)\n\t        # wrap the model by using DDP\n\t        model = torch.nn.parallel.DistributedDataParallel(\n\t            model,\n\t            device_ids=[local_rank],\n\t            output_device=local_rank,\n\t            broadcast_buffers=False,\n\t            find_unused_parameters=config.distributed.find_unused_parameters)\n", "        disc_model = torch.nn.parallel.DistributedDataParallel(\n\t            disc_model,\n\t            device_ids=[local_rank],\n\t            output_device=local_rank,\n\t            broadcast_buffers=False,\n\t            find_unused_parameters=config.distributed.find_unused_parameters)\n\t    start_epoch = max(1,resume_epoch) # start epoch is 1 if not resume\n\t    for epoch in range(start_epoch, config.common.max_epoch+1):\n\t        train_one_step(\n\t            epoch, optimizer, optimizer_disc, \n", "            model, disc_model, trainloader,config,\n\t            scheduler,disc_scheduler)\n\t        if epoch % config.common.test_interval == 0:\n\t            test(epoch,model,disc_model,testloader,config)\n\t        # save checkpoint and epoch\n\t        if epoch % config.common.save_interval == 0:\n\t            if config.distributed.data_parallel and dist.get_rank()==0:\n\t                torch.save({\n\t                    'epoch': epoch,\n\t                    'model_state_dict': model.module.state_dict(),\n", "                    'optimizer_state_dict': optimizer.state_dict(),\n\t                    'scheduler_state_dict': scheduler.state_dict(),\n\t                }, f'{config.checkpoint.save_location}epoch{epoch}_lr{config.optimization.lr}.pt')\n\t                torch.save({\n\t                    'epoch': epoch,\n\t                    'model_state_dict': disc_model.module.state_dict(),\n\t                    'optimizer_state_dict': optimizer_disc.state_dict(),\n\t                    'scheduler_state_dict': disc_scheduler.state_dict(),\n\t                },f'{config.checkpoint.save_location}epoch{epoch}_disc_lr{config.optimization.lr}.pt')\n\t            elif not config.distributed.data_parallel:\n", "                torch.save({\n\t                    'epoch': epoch,\n\t                    'model_state_dict': model.state_dict(),\n\t                    'optimizer_state_dict': optimizer.state_dict(),\n\t                    'scheduler_state_dict': scheduler.state_dict(),\n\t                }, f'{config.checkpoint.save_location}epoch{epoch}_lr{config.optimization.lr}.pt')\n\t                torch.save({\n\t                    'epoch': epoch,\n\t                    'model_state_dict': disc_model.state_dict(),\n\t                    'optimizer_state_dict': optimizer_disc.state_dict(),\n", "                    'scheduler_state_dict': disc_scheduler.state_dict(),\n\t                },f'{config.checkpoint.save_location}epoch{epoch}_disc_lr{config.optimization.lr}.pt')\n\t    if config.distributed.data_parallel:\n\t        dist.destroy_process_group()\n\t@hydra.main(config_path='config', config_name='config')\n\tdef main(config):\n\t    if config.distributed.torch_distributed_debug: # set distributed debug, if you encouter some multi gpu bug, please set torch_distributed_debug=True\n\t        os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n\t        os.environ[\"TORCH_DISTRIBUTED_DEBUG\"]=\"DETAIL\"\n\t    if not os.path.exists(config.checkpoint.save_folder):\n", "        os.makedirs(config.checkpoint.save_folder)\n\t    # set distributed\n\t    if config.distributed.data_parallel:\n\t        world_size=config.distributed.world_size\n\t        torch.multiprocessing.set_start_method('spawn')\n\t        mp.spawn(\n\t            train,\n\t            args=(world_size,config,),\n\t            nprocs=world_size,\n\t            join=True\n", "        )\n\t    else:\n\t        train(1,1,config) # set single gpu train\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "binary.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Raw binary format for Encodec compressed audio. Actual compression API is in `encodec.compress`.\"\"\"\n\timport io\n\timport json\n\timport struct\n\timport typing as tp\n", "# format is `ECDC` magic code, followed by the header size as uint32.\n\t# Then an uint8 indicates the protocol version (0.)\n\t# The header is then provided as json and should contain all required\n\t# informations for decoding. A raw stream of bytes is then provided\n\t# and should be interpretable using the json header.\n\t_encodec_header_struct = struct.Struct('!4sBI')\n\t_ENCODEC_MAGIC = b'ECDC'\n\tdef write_ecdc_header(fo: tp.IO[bytes], metadata: tp.Any):\n\t    meta_dumped = json.dumps(metadata).encode('utf-8')\n\t    version = 0\n", "    header = _encodec_header_struct.pack(_ENCODEC_MAGIC, version, len(meta_dumped))\n\t    fo.write(header)\n\t    fo.write(meta_dumped)\n\t    fo.flush()\n\tdef _read_exactly(fo: tp.IO[bytes], size: int) -> bytes:\n\t    buf = b\"\"\n\t    while len(buf) < size:\n\t        new_buf = fo.read(size)\n\t        if not new_buf:\n\t            raise EOFError(\"Impossible to read enough data from the stream, \"\n", "                           f\"{size} bytes remaining.\")\n\t        buf += new_buf\n\t        size -= len(new_buf)\n\t    return buf\n\tdef read_ecdc_header(fo: tp.IO[bytes]):\n\t    header_bytes = _read_exactly(fo, _encodec_header_struct.size)\n\t    magic, version, meta_size = _encodec_header_struct.unpack(header_bytes)\n\t    if magic != _ENCODEC_MAGIC:\n\t        raise ValueError(\"File is not in ECDC format.\")\n\t    if version != 0:\n", "        raise ValueError(\"Version not supported.\")\n\t    meta_bytes = _read_exactly(fo, meta_size)\n\t    return json.loads(meta_bytes.decode('utf-8'))\n\tclass BitPacker:\n\t    \"\"\"Simple bit packer to handle ints with a non standard width, e.g. 10 bits.\n\t    Note that for some bandwidth (1.5, 3), the codebook representation\n\t    will not cover an integer number of bytes.\n\t    Args:\n\t        bits (int): number of bits per value that will be pushed.\n\t        fo (IO[bytes]): file-object to push the bytes to.\n", "    \"\"\"\n\t    def __init__(self, bits: int, fo: tp.IO[bytes]):\n\t        self._current_value = 0\n\t        self._current_bits = 0\n\t        self.bits = bits\n\t        self.fo = fo\n\t    def push(self, value: int):\n\t        \"\"\"Push a new value to the stream. This will immediately\n\t        write as many uint8 as possible to the underlying file-object.\"\"\"\n\t        self._current_value += (value << self._current_bits)\n", "        self._current_bits += self.bits\n\t        while self._current_bits >= 8:\n\t            lower_8bits = self._current_value & 0xff\n\t            self._current_bits -= 8\n\t            self._current_value >>= 8\n\t            self.fo.write(bytes([lower_8bits]))\n\t    def flush(self):\n\t        \"\"\"Flushes the remaining partial uint8, call this at the end\n\t        of the stream to encode.\"\"\"\n\t        if self._current_bits:\n", "            self.fo.write(bytes([self._current_value]))\n\t            self._current_value = 0\n\t            self._current_bits = 0\n\t        self.fo.flush()\n\tclass BitUnpacker:\n\t    #TODO: binary这部分需要重新修改下\n\t    \"\"\"BitUnpacker does the opposite of `BitPacker`.\n\t    Args:\n\t        bits (int): number of bits of the values to decode.\n\t        fo (IO[bytes]): file-object to push the bytes to.\n", "        \"\"\"\n\t    def __init__(self, bits: int, fo: tp.IO[bytes]):\n\t        self.bits = bits\n\t        self.fo = fo\n\t        self._mask = (1 << bits) - 1\n\t        self._current_value = 0\n\t        self._current_bits = 0\n\t    def pull(self) -> tp.Optional[int]:\n\t        \"\"\"\n\t        Pull a single value from the stream, potentially reading some\n", "        extra bytes from the underlying file-object.\n\t        Returns `None` when reaching the end of the stream.\n\t        \"\"\"\n\t        while self._current_bits < self.bits:\n\t            buf = self.fo.read(1)\n\t            if not buf:\n\t                return None\n\t            character = buf[0]\n\t            self._current_value += character << self._current_bits\n\t            self._current_bits += 8\n", "        out = self._current_value & self._mask\n\t        self._current_value >>= self.bits\n\t        self._current_bits -= self.bits\n\t        return out\n\tdef test():\n\t    import torch\n\t    torch.manual_seed(1234)\n\t    for rep in range(4):\n\t        length: int = torch.randint(10, 2_000, (1,)).item()\n\t        bits: int = torch.randint(1, 16, (1,)).item()\n", "        tokens: tp.List[int] = torch.randint(2 ** bits, (length,)).tolist()\n\t        rebuilt: tp.List[int] = []\n\t        buf = io.BytesIO()\n\t        packer = BitPacker(bits, buf)\n\t        for token in tokens:\n\t            packer.push(token)\n\t        packer.flush()\n\t        buf.seek(0)\n\t        unpacker = BitUnpacker(bits, buf)\n\t        while True:\n", "            value = unpacker.pull()\n\t            if value is None:\n\t                break\n\t            rebuilt.append(value)\n\t        assert len(rebuilt) >= len(tokens), (len(rebuilt), len(tokens))\n\t        # The flushing mechanism might lead to \"ghost\" values at the end of the stream.\n\t        assert len(rebuilt) <= len(tokens) + 8 // bits, (len(rebuilt), len(tokens), bits)\n\t        for idx, (a, b) in enumerate(zip(tokens, rebuilt)):\n\t            assert a == b, (idx, a, b)\n\tif __name__ == '__main__':\n", "    test()\n"]}
{"filename": "compress.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"API to compress/decompress audio to bytestreams.\"\"\"\n\timport io\n\timport math\n\timport struct\n\timport time\n", "import typing as tp\n\timport torch\n\timport binary\n\tfrom quantization.ac import ArithmeticCoder, ArithmeticDecoder, build_stable_quantized_cdf\n\tfrom model import EncodecModel, EncodedFrame\n\tMODELS = {\n\t    'encodec_24khz': EncodecModel.encodec_model_24khz,\n\t    'encodec_48khz': EncodecModel.encodec_model_48khz,\n\t    'my_encodec':EncodecModel.my_encodec_model,\n\t    'encodec_bw':EncodecModel.encodec_model_bw,\n", "}\n\tdef compress_to_file(model: EncodecModel, wav: torch.Tensor, fo: tp.IO[bytes],\n\t                     use_lm: bool = True):\n\t    \"\"\"Compress a waveform to a file-object using the given model.\n\t    Args:\n\t        model (EncodecModel): a pre-trained EncodecModel to use to compress the audio.\n\t        wav (torch.Tensor): waveform to compress, should have a shape `[C, T]`, with `C`\n\t            matching `model.channels`, and the proper sample rate (e.g. `model.sample_rate`).\n\t            Use `utils.convert_audio` if this is not the case.\n\t        fo (IO[bytes]): file-object to which the compressed bits will be written.\n", "            See `compress` if you want obtain a `bytes` object instead.\n\t        use_lm (bool): if True, use a pre-trained language model to further\n\t            compress the stream using Entropy Coding. This will slow down compression\n\t            quite a bit, expect between 20 to 30% of size reduction.\n\t    \"\"\"\n\t    assert wav.dim() == 2, \"Only single waveform can be encoded.\"\n\t    if model.name not in MODELS:\n\t        raise ValueError(f\"The provided model {model.name} is not supported.\")\n\t    if use_lm:\n\t        lm = model.get_lm_model()\n", "    with torch.no_grad():\n\t        frames = model.encode(wav[None])\n\t    metadata = {\n\t        'm': model.name,                 # model name\n\t        'al': wav.shape[-1],             # audio_length\n\t        'nc': frames[0][0].shape[1],     # num_codebooks\n\t        'lm': use_lm,                    # use lm?\n\t    }\n\t    binary.write_ecdc_header(fo, metadata)\n\t    for (frame, scale) in frames:\n", "        if scale is not None:\n\t            fo.write(struct.pack('!f', scale.cpu().item()))\n\t        _, K, T = frame.shape\n\t        if use_lm:\n\t            coder = ArithmeticCoder(fo)\n\t            states: tp.Any = None\n\t            offset = 0\n\t            input_ = torch.zeros(1, K, 1, dtype=torch.long, device=wav.device)\n\t        else:\n\t            packer = binary.BitPacker(model.bits_per_codebook, fo)\n", "        for t in range(T):\n\t            if use_lm:\n\t                with torch.no_grad():\n\t                    probas, states, offset = lm(input_, states, offset)\n\t                # We emulate a streaming scenario even though we do not provide an API for it.\n\t                # This gives us a more accurate benchmark.\n\t                input_ = 1 + frame[:, :, t: t + 1]\n\t            for k, value in enumerate(frame[0, :, t].tolist()):\n\t                if use_lm:\n\t                    q_cdf = build_stable_quantized_cdf(\n", "                        probas[0, :, k, 0], coder.total_range_bits, check=False)\n\t                    coder.push(value, q_cdf)\n\t                else:\n\t                    packer.push(value)\n\t        if use_lm:\n\t            coder.flush()\n\t        else:\n\t            packer.flush()\n\tdef decompress_from_file(model:EncodecModel,fo: tp.IO[bytes], device='cpu') -> tp.Tuple[torch.Tensor, int]:\n\t    \"\"\"Decompress from a file-object.\n", "    Returns a tuple `(wav, sample_rate)`.\n\t    Args:\n\t        fo (IO[bytes]): file-object from which to read. If you want to decompress\n\t            from `bytes` instead, see `decompress`.\n\t        device: device to use to perform the computations.\n\t    \"\"\"\n\t    metadata = binary.read_ecdc_header(fo)\n\t    model_name = metadata['m']\n\t    audio_length = metadata['al']\n\t    num_codebooks = metadata['nc']\n", "    use_lm = metadata['lm']\n\t    assert isinstance(audio_length, int)\n\t    assert isinstance(num_codebooks, int)\n\t    if model.name not in MODELS:\n\t        raise ValueError(f\"The audio was compressed with an unsupported model {model_name}.\")\n\t    if use_lm:\n\t        lm = model.get_lm_model()\n\t    frames: tp.List[EncodedFrame] = []\n\t    segment_length = model.segment_length or audio_length\n\t    segment_stride = model.segment_stride or audio_length\n", "    for offset in range(0, audio_length, segment_stride):\n\t        this_segment_length = min(audio_length - offset, segment_length)\n\t        frame_length = int(math.ceil(this_segment_length * model.frame_rate / model.sample_rate))\n\t        if model.normalize:\n\t            scale_f, = struct.unpack('!f', binary._read_exactly(fo, struct.calcsize('!f')))\n\t            scale = torch.tensor(scale_f, device=device).view(1)\n\t        else:\n\t            scale = None\n\t        if use_lm:\n\t            decoder = ArithmeticDecoder(fo)\n", "            states: tp.Any = None\n\t            offset = 0\n\t            input_ = torch.zeros(1, num_codebooks, 1, dtype=torch.long, device=device)\n\t        else:\n\t            unpacker = binary.BitUnpacker(model.bits_per_codebook, fo)\n\t        frame = torch.zeros(1, num_codebooks, frame_length, dtype=torch.long, device=device)\n\t        for t in range(frame_length):\n\t            if use_lm:\n\t                with torch.no_grad():\n\t                    probas, states, offset = lm(input_, states, offset)\n", "            code_list: tp.List[int] = []\n\t            for k in range(num_codebooks):\n\t                if use_lm:\n\t                    q_cdf = build_stable_quantized_cdf(\n\t                        probas[0, :, k, 0], decoder.total_range_bits, check=False)\n\t                    code = decoder.pull(q_cdf)\n\t                else:\n\t                    code = unpacker.pull()\n\t                if code is None:\n\t                    raise EOFError(\"The stream ended sooner than expected.\")\n", "                code_list.append(code)\n\t            codes = torch.tensor(code_list, dtype=torch.long, device=device)\n\t            frame[0, :, t] = codes\n\t            if use_lm:\n\t                input_ = 1 + frame[:, :, t: t + 1]\n\t        frames.append((frame, scale))\n\t    with torch.no_grad():\n\t        wav = model.decode(frames)\n\t    return wav[0, :, :audio_length], model.sample_rate\n\tdef compress(model: EncodecModel, wav: torch.Tensor, use_lm: bool = False) -> bytes:\n", "    \"\"\"Compress a waveform using the given model. Returns the compressed bytes.\n\t    Args:\n\t        model (EncodecModel): a pre-trained EncodecModel to use to compress the audio.\n\t        wav (torch.Tensor): waveform to compress, should have a shape `[C, T]`, with `C`\n\t            matching `model.channels`, and the proper sample rate (e.g. `model.sample_rate`).\n\t            Use `utils.convert_audio` if this is not the case.\n\t        use_lm (bool): if True, use a pre-trained language model to further\n\t            compress the stream using Entropy Coding. This will slow down compression\n\t            quite a bit, expect between 20 to 30% of size reduction.\n\t    \"\"\"\n", "    fo = io.BytesIO()\n\t    compress_to_file(model, wav, fo, use_lm=use_lm)\n\t    return fo.getvalue()\n\tdef decompress(model:EncodecModel,compressed: bytes, device='cpu') -> tp.Tuple[torch.Tensor, int]:\n\t    \"\"\"Decompress from a file-object.\n\t    Returns a tuple `(wav, sample_rate)`.\n\t    Args:\n\t        compressed (bytes): compressed bytes.\n\t        device: device to use to perform the computations.\n\t    \"\"\"\n", "    fo = io.BytesIO(compressed)\n\t    return decompress_from_file(model,fo, device=device)\n\tdef test():\n\t    import torchaudio\n\t    torch.set_num_threads(1)\n\t    for name in MODELS.keys():\n\t        model = MODELS[name]()\n\t        sr = model.sample_rate // 1000\n\t        x, _ = torchaudio.load(f'test_{sr}k.wav')\n\t        x = x[:, :model.sample_rate * 5]\n", "        model.set_target_bandwidth(12)\n\t        for use_lm in [False, True]:\n\t            print(f\"Doing {name}, use_lm={use_lm}\")\n\t            begin = time.time()\n\t            res = compress(model, x, use_lm=use_lm)\n\t            t_comp = time.time() - begin\n\t            x_dec, _ = decompress(res)\n\t            t_decomp = time.time() - begin - t_comp\n\t            kbps = 8 * len(res) / 1000 / (x.shape[-1] / model.sample_rate)\n\t            print(f\"kbps: {kbps:.1f}, time comp: {t_comp:.1f} sec. \"\n", "                  f\"time decomp:{t_decomp:.1f}.\")\n\t            assert x_dec.shape == x.shape\n\tif __name__ == '__main__':\n\t    test()\n"]}
{"filename": "modules/conv.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Convolutional layers wrappers and utilities.\"\"\"\n\timport math\n\timport typing as tp\n\timport warnings\n\timport torch\n", "from torch import nn\n\tfrom torch.nn import functional as F\n\tfrom torch.nn.utils import spectral_norm, weight_norm\n\tfrom .norm import ConvLayerNorm\n\tCONV_NORMALIZATIONS = frozenset(['none', 'weight_norm', 'spectral_norm',\n\t                                 'time_layer_norm', 'layer_norm', 'time_group_norm'])\n\tdef apply_parametrization_norm(module: nn.Module, norm: str = 'none') -> nn.Module:\n\t    assert norm in CONV_NORMALIZATIONS\n\t    if norm == 'weight_norm':\n\t        return weight_norm(module)\n", "    elif norm == 'spectral_norm':\n\t        return spectral_norm(module)\n\t    else:\n\t        # We already check was in CONV_NORMALIZATION, so any other choice\n\t        # doesn't need reparametrization.\n\t        return module\n\tdef get_norm_module(module: nn.Module, causal: bool = False, norm: str = 'none', **norm_kwargs) -> nn.Module:\n\t    \"\"\"Return the proper normalization module. If causal is True, this will ensure the returned\n\t    module is causal, or return an error if the normalization doesn't support causal evaluation.\n\t    \"\"\"\n", "    assert norm in CONV_NORMALIZATIONS\n\t    if norm == 'layer_norm':\n\t        assert isinstance(module, nn.modules.conv._ConvNd)\n\t        return ConvLayerNorm(module.out_channels, **norm_kwargs)\n\t    elif norm == 'time_group_norm':\n\t        if causal:\n\t            raise ValueError(\"GroupNorm doesn't support causal evaluation.\")\n\t        assert isinstance(module, nn.modules.conv._ConvNd)\n\t        return nn.GroupNorm(1, module.out_channels, **norm_kwargs)\n\t    else:\n", "        return nn.Identity()\n\tdef get_extra_padding_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int,\n\t                                 padding_total: int = 0) -> int:\n\t    \"\"\"See `pad_for_conv1d`.\n\t    \"\"\"\n\t    length = x.shape[-1]\n\t    n_frames = (length - kernel_size + padding_total) / stride + 1\n\t    ideal_length = (math.ceil(n_frames) - 1) * stride + (kernel_size - padding_total)\n\t    return ideal_length - length\n\tdef pad_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int, padding_total: int = 0):\n", "    \"\"\"Pad for a convolution to make sure that the last window is full.\n\t    Extra padding is added at the end. This is required to ensure that we can rebuild\n\t    an output of the same length, as otherwise, even with padding, some time steps\n\t    might get removed.\n\t    For instance, with total padding = 4, kernel size = 4, stride = 2:\n\t        0 0 1 2 3 4 5 0 0   # (0s are padding)\n\t        1   2   3           # (output frames of a convolution, last 0 is never used)\n\t        0 0 1 2 3 4 5 0     # (output of tr. conv., but pos. 5 is going to get removed as padding)\n\t            1 2 3 4         # once you removed padding, we are missing one time step !\n\t    \"\"\"\n", "    extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)\n\t    return F.pad(x, (0, extra_padding))\n\tdef pad1d(x: torch.Tensor, paddings: tp.Tuple[int, int], mode: str = 'zero', value: float = 0.):\n\t    \"\"\"Tiny wrapper around F.pad, just to allow for reflect padding on small input.\n\t    If this is the case, we insert extra 0 padding to the right before the reflection happen.\n\t    \"\"\"\n\t    length = x.shape[-1]\n\t    padding_left, padding_right = paddings\n\t    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)\n\t    if mode == 'reflect':\n", "        max_pad = max(padding_left, padding_right)\n\t        extra_pad = 0\n\t        if length <= max_pad:\n\t            extra_pad = max_pad - length + 1\n\t            x = F.pad(x, (0, extra_pad))\n\t        padded = F.pad(x, paddings, mode, value)\n\t        end = padded.shape[-1] - extra_pad\n\t        return padded[..., :end]\n\t    else:\n\t        return F.pad(x, paddings, mode, value)\n", "def unpad1d(x: torch.Tensor, paddings: tp.Tuple[int, int]):\n\t    \"\"\"Remove padding from x, handling properly zero padding. Only for 1d!\"\"\"\n\t    padding_left, padding_right = paddings\n\t    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)\n\t    assert (padding_left + padding_right) <= x.shape[-1]\n\t    end = x.shape[-1] - padding_right\n\t    return x[..., padding_left: end]\n\tclass NormConv1d(nn.Module):\n\t    \"\"\"Wrapper around Conv1d and normalization applied to this conv\n\t    to provide a uniform interface across normalization approaches.\n", "    \"\"\"\n\t    def __init__(self, *args, causal: bool = False, norm: str = 'none',\n\t                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):\n\t        super().__init__()\n\t        self.conv = apply_parametrization_norm(nn.Conv1d(*args, **kwargs), norm)\n\t        self.norm = get_norm_module(self.conv, causal, norm, **norm_kwargs)\n\t        self.norm_type = norm\n\t    def forward(self, x):\n\t        x = self.conv(x)\n\t        x = self.norm(x)\n", "        return x\n\tclass NormConv2d(nn.Module):\n\t    \"\"\"Wrapper around Conv2d and normalization applied to this conv\n\t    to provide a uniform interface across normalization approaches.\n\t    \"\"\"\n\t    def __init__(self, *args, norm: str = 'none',\n\t                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):\n\t        super().__init__()\n\t        self.conv = apply_parametrization_norm(nn.Conv2d(*args, **kwargs), norm)\n\t        self.norm = get_norm_module(self.conv, causal=False, norm=norm, **norm_kwargs)\n", "        self.norm_type = norm\n\t    def forward(self, x):\n\t        x = self.conv(x)\n\t        x = self.norm(x)\n\t        return x\n\tclass NormConvTranspose1d(nn.Module):\n\t    \"\"\"Wrapper around ConvTranspose1d and normalization applied to this conv\n\t    to provide a uniform interface across normalization approaches.\n\t    \"\"\"\n\t    def __init__(self, *args, causal: bool = False, norm: str = 'none',\n", "                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):\n\t        super().__init__()\n\t        self.convtr = apply_parametrization_norm(nn.ConvTranspose1d(*args, **kwargs), norm)\n\t        self.norm = get_norm_module(self.convtr, causal, norm, **norm_kwargs)\n\t        self.norm_type = norm\n\t    def forward(self, x):\n\t        x = self.convtr(x)\n\t        x = self.norm(x)\n\t        return x\n\tclass NormConvTranspose2d(nn.Module):\n", "    \"\"\"Wrapper around ConvTranspose2d and normalization applied to this conv\n\t    to provide a uniform interface across normalization approaches.\n\t    \"\"\"\n\t    def __init__(self, *args, norm: str = 'none',\n\t                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):\n\t        super().__init__()\n\t        self.convtr = apply_parametrization_norm(nn.ConvTranspose2d(*args, **kwargs), norm)\n\t        self.norm = get_norm_module(self.convtr, causal=False, norm=norm, **norm_kwargs)\n\t    def forward(self, x):\n\t        x = self.convtr(x)\n", "        x = self.norm(x)\n\t        return x\n\tclass SConv1d(nn.Module):\n\t    \"\"\"Conv1d with some builtin handling of asymmetric or causal padding\n\t    and normalization.\n\t    \"\"\"\n\t    def __init__(self, in_channels: int, out_channels: int,\n\t                 kernel_size: int, stride: int = 1, dilation: int = 1,\n\t                 groups: int = 1, bias: bool = True, causal: bool = False,\n\t                 norm: str = 'none', norm_kwargs: tp.Dict[str, tp.Any] = {},\n", "                 pad_mode: str = 'reflect'):\n\t        super().__init__()\n\t        # warn user on unusual setup between dilation and stride\n\t        if stride > 1 and dilation > 1:\n\t            warnings.warn('SConv1d has been initialized with stride > 1 and dilation > 1'\n\t                          f' (kernel_size={kernel_size} stride={stride}, dilation={dilation}).')\n\t        self.conv = NormConv1d(in_channels, out_channels, kernel_size, stride,\n\t                               dilation=dilation, groups=groups, bias=bias, causal=causal,\n\t                               norm=norm, norm_kwargs=norm_kwargs)\n\t        self.causal = causal\n", "        self.pad_mode = pad_mode\n\t    def forward(self, x):\n\t        B, C, T = x.shape\n\t        kernel_size = self.conv.conv.kernel_size[0]\n\t        stride = self.conv.conv.stride[0]\n\t        dilation = self.conv.conv.dilation[0]\n\t        padding_total = (kernel_size - 1) * dilation - (stride - 1)\n\t        extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)\n\t        if self.causal:\n\t            # Left padding for causal\n", "            x = pad1d(x, (padding_total, extra_padding), mode=self.pad_mode)\n\t        else:\n\t            # Asymmetric padding required for odd strides\n\t            padding_right = padding_total // 2\n\t            padding_left = padding_total - padding_right\n\t            x = pad1d(x, (padding_left, padding_right + extra_padding), mode=self.pad_mode)\n\t        return self.conv(x)\n\tclass SConvTranspose1d(nn.Module):\n\t    \"\"\"ConvTranspose1d with some builtin handling of asymmetric or causal padding\n\t    and normalization.\n", "    \"\"\"\n\t    def __init__(self, in_channels: int, out_channels: int,\n\t                 kernel_size: int, stride: int = 1, causal: bool = False,\n\t                 norm: str = 'none', trim_right_ratio: float = 1.,\n\t                 norm_kwargs: tp.Dict[str, tp.Any] = {}):\n\t        super().__init__()\n\t        self.convtr = NormConvTranspose1d(in_channels, out_channels, kernel_size, stride,\n\t                                          causal=causal, norm=norm, norm_kwargs=norm_kwargs)\n\t        self.causal = causal\n\t        self.trim_right_ratio = trim_right_ratio\n", "        assert self.causal or self.trim_right_ratio == 1., \\\n\t            \"`trim_right_ratio` != 1.0 only makes sense for causal convolutions\"\n\t        assert self.trim_right_ratio >= 0. and self.trim_right_ratio <= 1.\n\t    def forward(self, x):\n\t        kernel_size = self.convtr.convtr.kernel_size[0]\n\t        stride = self.convtr.convtr.stride[0]\n\t        padding_total = kernel_size - stride\n\t        y = self.convtr(x)\n\t        # We will only trim fixed padding. Extra padding from `pad_for_conv1d` would be\n\t        # removed at the very end, when keeping only the right length for the output,\n", "        # as removing it here would require also passing the length at the matching layer\n\t        # in the encoder.\n\t        if self.causal:\n\t            # Trim the padding on the right according to the specified ratio\n\t            # if trim_right_ratio = 1.0, trim everything from right\n\t            padding_right = math.ceil(padding_total * self.trim_right_ratio)\n\t            padding_left = padding_total - padding_right\n\t            y = unpad1d(y, (padding_left, padding_right))\n\t        else:\n\t            # Asymmetric padding required for odd strides\n", "            padding_right = padding_total // 2\n\t            padding_left = padding_total - padding_right\n\t            y = unpad1d(y, (padding_left, padding_right))\n\t        return y\n"]}
{"filename": "modules/transformer.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"A streamable transformer.\"\"\"\n\timport typing as tp\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n", "def create_sin_embedding(positions: torch.Tensor, dim: int, max_period: float = 10000):\n\t    \"\"\"Create time embedding for the given positions, target dimension `dim`.\n\t    \"\"\"\n\t    # We aim for BTC format\n\t    assert dim % 2 == 0\n\t    half_dim = dim // 2\n\t    adim = torch.arange(half_dim, device=positions.device).view(1, 1, -1)\n\t    phase = positions / (max_period ** (adim / (half_dim - 1)))\n\t    return torch.cat([\n\t        torch.cos(phase),\n", "        torch.sin(phase),\n\t    ], dim=-1)\n\tclass StreamingTransformerEncoderLayer(nn.TransformerEncoderLayer):\n\t    def forward(self, x: torch.Tensor, x_past: torch.Tensor, past_context: int):  # type: ignore\n\t        if self.norm_first:\n\t            sa_input = self.norm1(x)\n\t            x = x + self._sa_block(sa_input, x_past, past_context)\n\t            x = x + self._ff_block(self.norm2(x))\n\t        else:\n\t            sa_input = x\n", "            x = self.norm1(x + self._sa_block(sa_input, x_past, past_context))\n\t            x = self.norm2(x + self._ff_block(x))\n\t        return x, sa_input\n\t    # self-attention block\n\t    def _sa_block(self, x: torch.Tensor, x_past: torch.Tensor, past_context: int):  # type: ignore\n\t        _, T, _ = x.shape\n\t        _, H, _ = x_past.shape\n\t        queries = x\n\t        keys = torch.cat([x_past, x], dim=1)\n\t        values = keys\n", "        queries_pos = torch.arange(H, T + H, device=x.device).view(-1, 1)\n\t        keys_pos = torch.arange(T + H, device=x.device).view(1, -1)\n\t        delta = queries_pos - keys_pos\n\t        valid_access = (delta >= 0) & (delta <= past_context)\n\t        x = self.self_attn(queries, keys, values,\n\t                           attn_mask=~valid_access,\n\t                           need_weights=False)[0]\n\t        return self.dropout1(x)\n\tclass StreamingTransformerEncoder(nn.Module):\n\t    \"\"\"TransformerEncoder with streaming support.\n", "    Args:\n\t        dim (int): dimension of the data.\n\t        hidden_scale (int): intermediate dimension of FF module is this times the dimension.\n\t        num_heads (int): number of heads.\n\t        num_layers (int): number of layers.\n\t        max_period (float): maxium period of cosines in the positional embedding.\n\t        past_context (int or None): receptive field for the causal mask, infinite if None.\n\t        gelu (bool): if true uses GeLUs, otherwise use ReLUs.\n\t        norm_in (bool): normalize the input.\n\t        dropout (float): dropout probability.\n", "        **kwargs: See `nn.TransformerEncoderLayer`.\n\t    \"\"\"\n\t    def __init__(self, dim, hidden_scale: float = 4., num_heads: int = 8, num_layers: int = 5,\n\t                 max_period: float = 10000, past_context: int = 1000, gelu: bool = True,\n\t                 norm_in: bool = True, dropout: float = 0., **kwargs):\n\t        super().__init__()\n\t        assert dim % num_heads == 0\n\t        hidden_dim = int(dim * hidden_scale)\n\t        self.max_period = max_period\n\t        self.past_context = past_context\n", "        activation: tp.Any = F.gelu if gelu else F.relu\n\t        self.norm_in: nn.Module\n\t        if norm_in:\n\t            self.norm_in = nn.LayerNorm(dim)\n\t        else:\n\t            self.norm_in = nn.Identity()\n\t        self.layers = nn.ModuleList()\n\t        for idx in range(num_layers):\n\t            self.layers.append(\n\t                StreamingTransformerEncoderLayer(\n", "                    dim, num_heads, hidden_dim,\n\t                    activation=activation, batch_first=True, dropout=dropout, **kwargs))\n\t    def forward(self, x: torch.Tensor,\n\t                states: tp.Optional[tp.List[torch.Tensor]] = None,\n\t                offset: tp.Union[int, torch.Tensor] = 0):\n\t        B, T, C = x.shape\n\t        if states is None:\n\t            states = [torch.zeros_like(x[:, :1]) for _ in range(1 + len(self.layers))]\n\t        positions = torch.arange(T, device=x.device).view(1, -1, 1) + offset\n\t        pos_emb = create_sin_embedding(positions, C, max_period=self.max_period)\n", "        new_state: tp.List[torch.Tensor] = []\n\t        x = self.norm_in(x)\n\t        x = x + pos_emb\n\t        for layer_state, layer in zip(states, self.layers):\n\t            x, new_layer_state = layer(x, layer_state, self.past_context)\n\t            new_layer_state = torch.cat([layer_state, new_layer_state], dim=1)\n\t            new_state.append(new_layer_state[:, -self.past_context:, :])\n\t        return x, new_state, offset + T\n"]}
{"filename": "modules/lstm.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"LSTM layers module.\"\"\"\n\tfrom torch import nn\n\tclass SLSTM(nn.Module):\n\t    \"\"\"\n\t    LSTM without worrying about the hidden state, nor the layout of the data.\n", "    Expects input as convolutional layout.\n\t    \"\"\"\n\t    def __init__(self, dimension: int, num_layers: int = 2, skip: bool = True):\n\t        super().__init__()\n\t        self.skip = skip\n\t        self.lstm = nn.LSTM(dimension, dimension, num_layers)\n\t    def forward(self, x):\n\t        x = x.permute(2, 0, 1)\n\t        y, _ = self.lstm(x)\n\t        if self.skip:\n", "            y = y + x\n\t        y = y.permute(1, 2, 0)\n\t        return y\n"]}
{"filename": "modules/__init__.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Torch modules.\"\"\"\n\t# flake8: noqa\n\tfrom .conv import (\n\t    pad1d,\n\t    unpad1d,\n", "    NormConv1d,\n\t    NormConvTranspose1d,\n\t    NormConv2d,\n\t    NormConvTranspose2d,\n\t    SConv1d,\n\t    SConvTranspose1d,\n\t)\n\tfrom .lstm import SLSTM\n\tfrom .seanet import SEANetEncoder, SEANetDecoder\n\tfrom .transformer import StreamingTransformerEncoder\n"]}
{"filename": "modules/norm.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Normalization modules.\"\"\"\n\timport typing as tp\n\timport einops\n\timport torch\n\tfrom torch import nn\n", "class ConvLayerNorm(nn.LayerNorm):\n\t    \"\"\"\n\t    Convolution-friendly LayerNorm that moves channels to last dimensions\n\t    before running the normalization and moves them back to original position right after.\n\t    \"\"\"\n\t    def __init__(self, normalized_shape: tp.Union[int, tp.List[int], torch.Size], **kwargs):\n\t        super().__init__(normalized_shape, **kwargs)\n\t    def forward(self, x):\n\t        x = einops.rearrange(x, 'b ... t -> b t ...')\n\t        x = super().forward(x)\n", "        x = einops.rearrange(x, 'b t ... -> b ... t')\n\t        return\n"]}
{"filename": "modules/seanet.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Encodec SEANet-based encoder and decoder implementation.\"\"\"\n\timport typing as tp\n\timport numpy as np\n\timport torch.nn as nn\n\tfrom . import (\n", "    SConv1d,\n\t    SConvTranspose1d,\n\t    SLSTM\n\t)\n\tclass SEANetResnetBlock(nn.Module):\n\t    \"\"\"Residual block from SEANet model.\n\t    Args:\n\t        dim (int): Dimension of the input/output\n\t        kernel_sizes (list): List of kernel sizes for the convolutions.\n\t        dilations (list): List of dilations for the convolutions.\n", "        activation (str): Activation function.\n\t        activation_params (dict): Parameters to provide to the activation function\n\t        norm (str): Normalization method.\n\t        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.\n\t        causal (bool): Whether to use fully causal convolution.\n\t        pad_mode (str): Padding mode for the convolutions.\n\t        compress (int): Reduced dimensionality in residual branches (from Demucs v3)\n\t        true_skip (bool): Whether to use true skip connection or a simple convolution as the skip connection.\n\t    \"\"\"\n\t    def __init__(self, dim: int, kernel_sizes: tp.List[int] = [3, 1], dilations: tp.List[int] = [1, 1],\n", "                 activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},\n\t                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, causal: bool = False,\n\t                 pad_mode: str = 'reflect', compress: int = 2, true_skip: bool = True):\n\t        super().__init__()\n\t        assert len(kernel_sizes) == len(dilations), 'Number of kernel sizes should match number of dilations'\n\t        act = getattr(nn, activation)\n\t        hidden = dim // compress\n\t        block = []\n\t        for i, (kernel_size, dilation) in enumerate(zip(kernel_sizes, dilations)):\n\t            in_chs = dim if i == 0 else hidden\n", "            out_chs = dim if i == len(kernel_sizes) - 1 else hidden\n\t            block += [\n\t                act(**activation_params),\n\t                SConv1d(in_chs, out_chs, kernel_size=kernel_size, dilation=dilation,\n\t                        norm=norm, norm_kwargs=norm_params,\n\t                        causal=causal, pad_mode=pad_mode),\n\t            ]\n\t        self.block = nn.Sequential(*block)\n\t        self.shortcut: nn.Module\n\t        if true_skip:\n", "            self.shortcut = nn.Identity()\n\t        else:\n\t            self.shortcut = SConv1d(dim, dim, kernel_size=1, norm=norm, norm_kwargs=norm_params,\n\t                                    causal=causal, pad_mode=pad_mode)\n\t    def forward(self, x):\n\t        return self.shortcut(x) + self.block(x)\n\tclass SEANetEncoder(nn.Module):\n\t    \"\"\"SEANet encoder.\n\t    Args:\n\t        channels (int): Audio channels.\n", "        dimension (int): Intermediate representation dimension.\n\t        n_filters (int): Base width for the model.\n\t        n_residual_layers (int): nb of residual layers.\n\t        ratios (Sequence[int]): kernel size and stride ratios. The encoder uses downsampling ratios instead of\n\t            upsampling ratios, hence it will use the ratios in the reverse order to the ones specified here\n\t            that must match the decoder order\n\t        activation (str): Activation function.\n\t        activation_params (dict): Parameters to provide to the activation function\n\t        norm (str): Normalization method.\n\t        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.\n", "        kernel_size (int): Kernel size for the initial convolution.\n\t        last_kernel_size (int): Kernel size for the initial convolution.\n\t        residual_kernel_size (int): Kernel size for the residual layers.\n\t        dilation_base (int): How much to increase the dilation with each layer.\n\t        causal (bool): Whether to use fully causal convolution.\n\t        pad_mode (str): Padding mode for the convolutions.\n\t        true_skip (bool): Whether to use true skip connection or a simple\n\t            (streamable) convolution as the skip connection in the residual network blocks.\n\t        compress (int): Reduced dimensionality in residual branches (from Demucs v3).\n\t        lstm (int): Number of LSTM layers at the end of the encoder.\n", "    \"\"\"\n\t    def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1,\n\t                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},\n\t                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,\n\t                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,\n\t                 pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2):\n\t        super().__init__()\n\t        self.channels = channels\n\t        self.dimension = dimension\n\t        self.n_filters = n_filters\n", "        self.ratios = list(reversed(ratios))\n\t        del ratios\n\t        self.n_residual_layers = n_residual_layers\n\t        self.hop_length = np.prod(self.ratios)\n\t        act = getattr(nn, activation)\n\t        mult = 1\n\t        model: tp.List[nn.Module] = [\n\t            SConv1d(channels, mult * n_filters, kernel_size, norm=norm, norm_kwargs=norm_params,\n\t                    causal=causal, pad_mode=pad_mode)\n\t        ]\n", "        # Downsample to raw audio scale\n\t        for i, ratio in enumerate(self.ratios):\n\t            # Add residual layers\n\t            for j in range(n_residual_layers):\n\t                model += [\n\t                    SEANetResnetBlock(mult * n_filters, kernel_sizes=[residual_kernel_size, 1],\n\t                                      dilations=[dilation_base ** j, 1],\n\t                                      norm=norm, norm_params=norm_params,\n\t                                      activation=activation, activation_params=activation_params,\n\t                                      causal=causal, pad_mode=pad_mode, compress=compress, true_skip=true_skip)]\n", "            # Add downsampling layers\n\t            model += [\n\t                act(**activation_params),\n\t                SConv1d(mult * n_filters, mult * n_filters * 2,\n\t                        kernel_size=ratio * 2, stride=ratio,\n\t                        norm=norm, norm_kwargs=norm_params,\n\t                        causal=causal, pad_mode=pad_mode),\n\t            ]\n\t            mult *= 2\n\t        if lstm:\n", "            model += [SLSTM(mult * n_filters, num_layers=lstm)]\n\t        model += [\n\t            act(**activation_params),\n\t            SConv1d(mult * n_filters, dimension, last_kernel_size, norm=norm, norm_kwargs=norm_params,\n\t                    causal=causal, pad_mode=pad_mode)\n\t        ]\n\t        self.model = nn.Sequential(*model)\n\t    def forward(self, x):\n\t        return self.model(x)\n\tclass SEANetDecoder(nn.Module):\n", "    \"\"\"SEANet decoder.\n\t    Args:\n\t        channels (int): Audio channels.\n\t        dimension (int): Intermediate representation dimension.\n\t        n_filters (int): Base width for the model.\n\t        n_residual_layers (int): nb of residual layers.\n\t        ratios (Sequence[int]): kernel size and stride ratios\n\t        activation (str): Activation function.\n\t        activation_params (dict): Parameters to provide to the activation function\n\t        final_activation (str): Final activation function after all convolutions.\n", "        final_activation_params (dict): Parameters to provide to the activation function\n\t        norm (str): Normalization method.\n\t        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.\n\t        kernel_size (int): Kernel size for the initial convolution.\n\t        last_kernel_size (int): Kernel size for the initial convolution.\n\t        residual_kernel_size (int): Kernel size for the residual layers.\n\t        dilation_base (int): How much to increase the dilation with each layer.\n\t        causal (bool): Whether to use fully causal convolution.\n\t        pad_mode (str): Padding mode for the convolutions.\n\t        true_skip (bool): Whether to use true skip connection or a simple\n", "            (streamable) convolution as the skip connection in the residual network blocks.\n\t        compress (int): Reduced dimensionality in residual branches (from Demucs v3).\n\t        lstm (int): Number of LSTM layers at the end of the encoder.\n\t        trim_right_ratio (float): Ratio for trimming at the right of the transposed convolution under the causal setup.\n\t            If equal to 1.0, it means that all the trimming is done at the right.\n\t    \"\"\"\n\t    def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1,\n\t                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},\n\t                 final_activation: tp.Optional[str] = None, final_activation_params: tp.Optional[dict] = None,\n\t                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,\n", "                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,\n\t                 pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2,\n\t                 trim_right_ratio: float = 1.0):\n\t        super().__init__()\n\t        self.dimension = dimension\n\t        self.channels = channels\n\t        self.n_filters = n_filters\n\t        self.ratios = ratios\n\t        del ratios\n\t        self.n_residual_layers = n_residual_layers\n", "        self.hop_length = np.prod(self.ratios)\n\t        act = getattr(nn, activation)\n\t        mult = int(2 ** len(self.ratios))\n\t        model: tp.List[nn.Module] = [\n\t            SConv1d(dimension, mult * n_filters, kernel_size, norm=norm, norm_kwargs=norm_params,\n\t                    causal=causal, pad_mode=pad_mode)\n\t        ]\n\t        if lstm:\n\t            model += [SLSTM(mult * n_filters, num_layers=lstm)]\n\t        # Upsample to raw audio scale\n", "        for i, ratio in enumerate(self.ratios):\n\t            # Add upsampling layers\n\t            model += [\n\t                act(**activation_params),\n\t                SConvTranspose1d(mult * n_filters, mult * n_filters // 2,\n\t                                 kernel_size=ratio * 2, stride=ratio,\n\t                                 norm=norm, norm_kwargs=norm_params,\n\t                                 causal=causal, trim_right_ratio=trim_right_ratio),\n\t            ]\n\t            # Add residual layers\n", "            for j in range(n_residual_layers):\n\t                model += [\n\t                    SEANetResnetBlock(mult * n_filters // 2, kernel_sizes=[residual_kernel_size, 1],\n\t                                      dilations=[dilation_base ** j, 1],\n\t                                      activation=activation, activation_params=activation_params,\n\t                                      norm=norm, norm_params=norm_params, causal=causal,\n\t                                      pad_mode=pad_mode, compress=compress, true_skip=true_skip)]\n\t            mult //= 2\n\t        # Add final layers\n\t        model += [\n", "            act(**activation_params),\n\t            SConv1d(n_filters, channels, last_kernel_size, norm=norm, norm_kwargs=norm_params,\n\t                    causal=causal, pad_mode=pad_mode)\n\t        ]\n\t        # Add optional final activation to decoder (eg. tanh)\n\t        if final_activation is not None:\n\t            final_act = getattr(nn, final_activation)\n\t            final_activation_params = final_activation_params or {}\n\t            model += [\n\t                final_act(**final_activation_params)\n", "            ]\n\t        self.model = nn.Sequential(*model)\n\t    def forward(self, z):\n\t        y = self.model(z)\n\t        return y\n\tdef test():\n\t    import torch\n\t    encoder = SEANetEncoder()\n\t    decoder = SEANetDecoder()\n\t    x = torch.randn(1, 1, 24000)\n", "    z = encoder(x)\n\t    assert list(z.shape) == [1, 128, 75], z.shape\n\t    y = decoder(z)\n\t    assert y.shape == x.shape, (x.shape, y.shape)\n\tif __name__ == '__main__':\n\t    test()\n"]}
{"filename": "datasets/generate_train_file.py", "chunked_list": ["import os\n\timport argparse\n\tdef generate_csv(file_dir, csv_path,mode='train'):\n\t    # 生成file_dir下所有文件的路径\n\t    file_list = []\n\t    for root, dirs, files in os.walk(file_dir):\n\t        for file in files:\n\t            if file.endswith('.flac') or file.endswith('.wav') and mode in root:\n\t                file_list.append(os.path.join(root, file))\n\t    # 生成csv文件\n", "    with open(csv_path, 'w') as f:\n\t        for file in file_list:\n\t            f.write(file + '\\n')\n\tif __name__ == '__main__':\n\t    arg = argparse.ArgumentParser()\n\t    arg.add_argument('-i','--input_file_dir', type=str, default='./LibriSpeech/train-clean-100')\n\t    arg.add_argument('-o','--output_path', type=str, default='./librispeech_train100h.csv')\n\t    arg.add_argument('-m','--mode', type=str, default='train',help='train,test-clean/other or dev-clean/other')\n\t    args = arg.parse_args()\n\t    generate_csv(args.input_file_dir, args.output_path,args.mode)"]}
{"filename": "datasets/resample_audio.py", "chunked_list": ["from pathlib import Path\n\tfrom tqdm import tqdm\n\timport torch\n\timport torchaudio\n\timport argparse\n\tdef get_parser():\n\t    parser = argparse.ArgumentParser(description=\"Convert sample rate of all audio files in source_dir and saves to target_dir\")\n\t    parser.add_argument(\n\t        '-s',\n\t        '--source_dir',\n", "        required=True,\n\t        help=\"Source wave folder.\"\n\t    )\n\t    parser.add_argument(\n\t        '-t',\n\t        '--target_sr',\n\t        type=int,\n\t        default=24000,\n\t        help=\"Target sample rate.\"\n\t    )\n", "    parser.add_argument(\n\t        '-c',\n\t        '--target_channels',\n\t        type=int,\n\t        default=1,\n\t        help=\"Target channels.\"\n\t    )\n\t    parser.add_argument(\n\t        '-e',\n\t        '--file_extension',\n", "        type=str,\n\t        default='flac',\n\t        help=\"File extension.\",\n\t        choices=['flac','wav']\n\t    )\n\t    return parser\n\tdef convert_audio(wav: torch.Tensor, sr: int, target_sr: int, target_channels: int):\n\t    assert wav.dim() >= 2, \"Audio tensor must have at least 2 dimensions\"\n\t    assert wav.shape[-2] in [1, 2], \"Audio must be mono or stereo.\"\n\t    *shape, channels, length = wav.shape\n", "    if target_channels == 1:\n\t        wav = wav.mean(-2, keepdim=True)\n\t    elif target_channels == 2:\n\t        wav = wav.expand(*shape, target_channels, length)\n\t    elif channels == 1:\n\t        wav = wav.expand(target_channels, -1)\n\t    else:\n\t        raise RuntimeError(f\"Impossible to convert from {channels} to {target_channels}\")\n\t    wav = torchaudio.transforms.Resample(sr, target_sr)(wav)\n\t    return wav\n", "def convert_sample_rate(source_dir,target_sr=24000,target_channels=1,file_extension='.flac'):\n\t    \"\"\"Converts sample rate of all audio files in source_dir and saves to target_dir\"\"\"\n\t    source_dir = Path(source_dir)\n\t    target_dir = source_dir.parent / f\"{source_dir.name}_{int(target_sr/1000)}khz\"\n\t    for wav_path in tqdm(list(source_dir.rglob(f'*.{file_extension}'))):\n\t        relative_path = wav_path.relative_to(source_dir)\n\t        wav,sr = torchaudio.load(wav_path)  # Load audio\n\t        resample_wav = convert_audio(wav,sr,target_sr,target_channels)\n\t        save_path = target_dir / relative_path\n\t        if not save_path.parent.exists():\n", "            save_path.parent.mkdir(parents=True)\n\t        torchaudio.save(save_path, resample_wav, sample_rate=target_sr)\n\tdef main():\n\t    parser = get_parser()\n\t    args = parser.parse_args()\n\t    print(args)\n\t    convert_sample_rate(args.source_dir,args.target_sr,args.target_channels,args.file_extension)\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "quantization/__init__.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t# flake8: noqa\n\tfrom .vq import QuantizedResult, ResidualVectorQuantizer\n"]}
{"filename": "quantization/ac.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Arithmetic coder.\"\"\"\n\timport io\n\timport math\n\timport random\n\timport typing as tp\n", "import torch\n\tfrom binary import BitPacker, BitUnpacker\n\tdef build_stable_quantized_cdf(pdf: torch.Tensor, total_range_bits: int,\n\t                               roundoff: float = 1e-8, min_range: int = 2,\n\t                               check: bool = True) -> torch.Tensor:\n\t    \"\"\"Turn the given PDF into a quantized CDF that splits\n\t    [0, 2 ** self.total_range_bits - 1] into chunks of size roughly proportional\n\t    to the PDF.\n\t    Args:\n\t        pdf (torch.Tensor): probability distribution, shape should be `[N]`.\n", "        total_range_bits (int): see `ArithmeticCoder`, the typical range we expect\n\t            during the coding process is `[0, 2 ** total_range_bits - 1]`.\n\t        roundoff (float): will round the pdf up to that level to remove difference coming\n\t        from e.g. evaluating the Language Model on different architectures.\n\t        min_range (int): minimum range width. Should always be at least 2 for numerical\n\t            stability. Use this to avoid pathological behavior is a value\n\t            that is expected to be rare actually happens in real life.\n\t        check (bool): if True, checks that nothing bad happened, can be deactivated for speed.\n\t    \"\"\"\n\t    pdf = pdf.detach()\n", "    if roundoff:\n\t        pdf = (pdf / roundoff).floor() * roundoff\n\t    # interpolate with uniform distribution to achieve desired minimum probability.\n\t    total_range = 2 ** total_range_bits\n\t    cardinality = len(pdf)\n\t    alpha = min_range * cardinality / total_range\n\t    assert alpha <= 1, \"you must reduce min_range\"\n\t    ranges = (((1 - alpha) * total_range) * pdf).floor().long()\n\t    ranges += min_range\n\t    quantized_cdf = torch.cumsum(ranges, dim=-1)\n", "    if min_range < 2:\n\t        raise ValueError(\"min_range must be at least 2.\")\n\t    if check:\n\t        assert quantized_cdf[-1] <= 2 ** total_range_bits, quantized_cdf[-1]\n\t        if ((quantized_cdf[1:] - quantized_cdf[:-1]) < min_range).any() or quantized_cdf[0] < min_range:\n\t            raise ValueError(\"You must increase your total_range_bits.\")\n\t    return quantized_cdf\n\tclass ArithmeticCoder:\n\t    \"\"\"ArithmeticCoder,\n\t    Let us take a distribution `p` over `N` symbols, and assume we have a stream\n", "    of random variables `s_t` sampled from `p`. Let us assume that we have a budget\n\t    of `B` bits that we can afford to write on device. There are `2**B` possible numbers,\n\t    corresponding to the range `[0, 2 ** B - 1]`. We can map each of those number to a single\n\t    sequence `(s_t)` by doing the following:\n\t    1) Initialize the current range to` [0 ** 2 B - 1]`.\n\t    2) For each time step t, split the current range into contiguous chunks,\n\t        one for each possible outcome, with size roughly proportional to `p`.\n\t        For instance, if `p = [0.75, 0.25]`, and the range is `[0, 3]`, the chunks\n\t        would be `{[0, 2], [3, 3]}`.\n\t    3) Select the chunk corresponding to `s_t`, and replace the current range with this.\n", "    4) When done encoding all the values, just select any value remaining in the range.\n\t    You will notice that this procedure can fail: for instance if at any point in time\n\t    the range is smaller than `N`, then we can no longer assign a non-empty chunk to each\n\t    possible outcome. Intuitively, the more likely a value is, the less the range width\n\t    will reduce, and the longer we can go on encoding values. This makes sense: for any efficient\n\t    coding scheme, likely outcomes would take less bits, and more of them can be coded\n\t    with a fixed budget.\n\t    In practice, we do not know `B` ahead of time, but we have a way to inject new bits\n\t    when the current range decreases below a given limit (given by `total_range_bits`), without\n\t    having to redo all the computations. If we encode mostly likely values, we will seldom\n", "    need to inject new bits, but a single rare value can deplete our stock of entropy!\n\t    In this explanation, we assumed that the distribution `p` was constant. In fact, the present\n\t    code works for any sequence `(p_t)` possibly different for each timestep.\n\t    We also assume that `s_t ~ p_t`, but that doesn't need to be true, although the smaller\n\t    the KL between the true distribution and `p_t`, the most efficient the coding will be.\n\t    Args:\n\t        fo (IO[bytes]): file-like object to which the bytes will be written to.\n\t        total_range_bits (int): the range `M` described above is `2 ** total_range_bits.\n\t            Any time the current range width fall under this limit, new bits will\n\t            be injected to rescale the initial range.\n", "    \"\"\"\n\t    def __init__(self, fo: tp.IO[bytes], total_range_bits: int = 24):\n\t        assert total_range_bits <= 30\n\t        self.total_range_bits = total_range_bits\n\t        self.packer = BitPacker(bits=1, fo=fo)  # we push single bits at a time.\n\t        self.low: int = 0\n\t        self.high: int = 0\n\t        self.max_bit: int = -1\n\t        self._dbg: tp.List[tp.Any] = []\n\t        self._dbg2: tp.List[tp.Any] = []\n", "    @property\n\t    def delta(self) -> int:\n\t        \"\"\"Return the current range width.\"\"\"\n\t        return self.high - self.low + 1\n\t    def _flush_common_prefix(self):\n\t        # If self.low and self.high start with the sames bits,\n\t        # those won't change anymore as we always just increase the range\n\t        # by powers of 2, and we can flush them out to the bit stream.\n\t        assert self.high >= self.low, (self.low, self.high)\n\t        assert self.high < 2 ** (self.max_bit + 1)\n", "        while self.max_bit >= 0:\n\t            b1 = self.low >> self.max_bit\n\t            b2 = self.high >> self.max_bit\n\t            if b1 == b2:\n\t                self.low -= (b1 << self.max_bit)\n\t                self.high -= (b1 << self.max_bit)\n\t                assert self.high >= self.low, (self.high, self.low, self.max_bit)\n\t                assert self.low >= 0\n\t                self.max_bit -= 1\n\t                self.packer.push(b1)\n", "            else:\n\t                break\n\t    def push(self, symbol: int, quantized_cdf: torch.Tensor):\n\t        \"\"\"Push the given symbol on the stream, flushing out bits\n\t        if possible.\n\t        Args:\n\t            symbol (int): symbol to encode with the AC.\n\t            quantized_cdf (torch.Tensor): use `build_stable_quantized_cdf`\n\t                to build this from your pdf estimate.\n\t        \"\"\"\n", "        while self.delta < 2 ** self.total_range_bits:\n\t            self.low *= 2\n\t            self.high = self.high * 2 + 1\n\t            self.max_bit += 1\n\t        range_low = 0 if symbol == 0 else quantized_cdf[symbol - 1].item()\n\t        range_high = quantized_cdf[symbol].item() - 1\n\t        effective_low = int(math.ceil(range_low * (self.delta / (2 ** self.total_range_bits))))\n\t        effective_high = int(math.floor(range_high * (self.delta / (2 ** self.total_range_bits))))\n\t        assert self.low <= self.high\n\t        self.high = self.low + effective_high\n", "        self.low = self.low + effective_low\n\t        assert self.low <= self.high, (effective_low, effective_high, range_low, range_high)\n\t        self._dbg.append((self.low, self.high))\n\t        self._dbg2.append((self.low, self.high))\n\t        outs = self._flush_common_prefix()\n\t        assert self.low <= self.high\n\t        assert self.max_bit >= -1\n\t        assert self.max_bit <= 61, self.max_bit\n\t        return outs\n\t    def flush(self):\n", "        \"\"\"Flush the remaining information to the stream.\n\t        \"\"\"\n\t        while self.max_bit >= 0:\n\t            b1 = (self.low >> self.max_bit) & 1\n\t            self.packer.push(b1)\n\t            self.max_bit -= 1\n\t        self.packer.flush()\n\tclass ArithmeticDecoder:\n\t    \"\"\"ArithmeticDecoder, see `ArithmeticCoder` for a detailed explanation.\n\t    Note that this must be called with **exactly** the same parameters and sequence\n", "    of quantized cdf as the arithmetic encoder or the wrong values will be decoded.\n\t    If the AC encoder current range is [L, H], with `L` and `H` having the some common\n\t    prefix (i.e. the same most significant bits), then this prefix will be flushed to the stream.\n\t    For instances, having read 3 bits `b1 b2 b3`, we know that `[L, H]` is contained inside\n\t    `[b1 b2 b3 0 ... 0 b1 b3 b3 1 ... 1]`. Now this specific sub-range can only be obtained\n\t    for a specific sequence of symbols and a binary-search allows us to decode those symbols.\n\t    At some point, the prefix `b1 b2 b3` will no longer be sufficient to decode new symbols,\n\t    and we will need to read new bits from the stream and repeat the process.\n\t    \"\"\"\n\t    def __init__(self, fo: tp.IO[bytes], total_range_bits: int = 24):\n", "        self.total_range_bits = total_range_bits\n\t        self.low: int = 0\n\t        self.high: int = 0\n\t        self.current: int = 0\n\t        self.max_bit: int = -1\n\t        self.unpacker = BitUnpacker(bits=1, fo=fo)  # we pull single bits at a time.\n\t        # Following is for debugging\n\t        self._dbg: tp.List[tp.Any] = []\n\t        self._dbg2: tp.List[tp.Any] = []\n\t        self._last: tp.Any = None\n", "    @property\n\t    def delta(self) -> int:\n\t        return self.high - self.low + 1\n\t    def _flush_common_prefix(self):\n\t        # Given the current range [L, H], if both have a common prefix,\n\t        # we know we can remove it from our representation to avoid handling large numbers.\n\t        while self.max_bit >= 0:\n\t            b1 = self.low >> self.max_bit\n\t            b2 = self.high >> self.max_bit\n\t            if b1 == b2:\n", "                self.low -= (b1 << self.max_bit)\n\t                self.high -= (b1 << self.max_bit)\n\t                self.current -= (b1 << self.max_bit)\n\t                assert self.high >= self.low\n\t                assert self.low >= 0\n\t                self.max_bit -= 1\n\t            else:\n\t                break\n\t    def pull(self, quantized_cdf: torch.Tensor) -> tp.Optional[int]:\n\t        \"\"\"Pull a symbol, reading as many bits from the stream as required.\n", "        This returns `None` when the stream has been exhausted.\n\t        Args:\n\t            quantized_cdf (torch.Tensor): use `build_stable_quantized_cdf`\n\t                to build this from your pdf estimate. This must be **exatly**\n\t                the same cdf as the one used at encoding time.\n\t        \"\"\"\n\t        while self.delta < 2 ** self.total_range_bits:\n\t            bit = self.unpacker.pull()\n\t            if bit is None:\n\t                return None\n", "            self.low *= 2\n\t            self.high = self.high * 2 + 1\n\t            self.current = self.current * 2 + bit\n\t            self.max_bit += 1\n\t        def bin_search(low_idx: int, high_idx: int):\n\t            # Binary search is not just for coding interviews :)\n\t            if high_idx < low_idx:\n\t                raise RuntimeError(\"Binary search failed\")\n\t            mid = (low_idx + high_idx) // 2\n\t            range_low = quantized_cdf[mid - 1].item() if mid > 0 else 0\n", "            range_high = quantized_cdf[mid].item() - 1\n\t            effective_low = int(math.ceil(range_low * (self.delta / (2 ** self.total_range_bits))))\n\t            effective_high = int(math.floor(range_high * (self.delta / (2 ** self.total_range_bits))))\n\t            low = effective_low + self.low\n\t            high = effective_high + self.low\n\t            if self.current >= low:\n\t                if self.current <= high:\n\t                    return (mid, low, high, self.current)\n\t                else:\n\t                    return bin_search(mid + 1, high_idx)\n", "            else:\n\t                return bin_search(low_idx, mid - 1)\n\t        self._last = (self.low, self.high, self.current, self.max_bit)\n\t        sym, self.low, self.high, self.current = bin_search(0, len(quantized_cdf) - 1)\n\t        self._dbg.append((self.low, self.high, self.current))\n\t        self._flush_common_prefix()\n\t        self._dbg2.append((self.low, self.high, self.current))\n\t        return sym\n\tdef test():\n\t    torch.manual_seed(1234)\n", "    random.seed(1234)\n\t    for _ in range(4):\n\t        pdfs = []\n\t        cardinality = random.randrange(4000)\n\t        steps = random.randrange(100, 500)\n\t        fo = io.BytesIO()\n\t        encoder = ArithmeticCoder(fo)\n\t        symbols = []\n\t        for step in range(steps):\n\t            pdf = torch.softmax(torch.randn(cardinality), dim=0)\n", "            pdfs.append(pdf)\n\t            q_cdf = build_stable_quantized_cdf(pdf, encoder.total_range_bits)\n\t            symbol = torch.multinomial(pdf, 1).item()\n\t            symbols.append(symbol)\n\t            encoder.push(symbol, q_cdf)\n\t        encoder.flush()\n\t        fo.seek(0)\n\t        decoder = ArithmeticDecoder(fo)\n\t        for idx, (pdf, symbol) in enumerate(zip(pdfs, symbols)):\n\t            q_cdf = build_stable_quantized_cdf(pdf, encoder.total_range_bits)\n", "            decoded_symbol = decoder.pull(q_cdf)\n\t            assert decoded_symbol == symbol, idx\n\t        assert decoder.pull(torch.zeros(1)) is None\n\tif __name__ == \"__main__\":\n\t    test()\n"]}
{"filename": "quantization/vq.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Residual vector quantizer implementation.\"\"\"\n\tfrom dataclasses import dataclass, field\n\timport math\n\timport typing as tp\n\timport torch\n", "from torch import nn\n\tfrom .core_vq import ResidualVectorQuantization\n\t@dataclass\n\tclass QuantizedResult:\n\t    quantized: torch.Tensor\n\t    codes: torch.Tensor\n\t    bandwidth: torch.Tensor  # bandwidth in kb/s used, per batch item.\n\t    penalty: tp.Optional[torch.Tensor] = None\n\t    metrics: dict = field(default_factory=dict)\n\tclass ResidualVectorQuantizer(nn.Module):\n", "    \"\"\"Residual Vector Quantizer.\n\t        if you want to know more information about RVQ, you can read the soundstream paper (https://arxiv.org/abs/2107.03312)\n\t        Residual vector quantizer cascades N_q layers of VQ. \n\t        the algorithm is described as follows:\n\t        **********************************************************************************\n\t        Input: y = enc(x) the output of the encoder, vector quantizers Q_i for i = 1...N_q\n\t        Output: the quantized y^hat\n\t        y^hat <- 0 \n\t        residual <- y\n\t        for i=1 to N_q do\n", "            y^hat += Q_i(residual)\n\t            residual -= Q_i(residual)\n\t        return y^hat\n\t        **********************************************************************************\n\t    Args:\n\t        dimension (int): Dimension of the codebooks.\n\t        n_q (int): Number of residual vector quantizers used.\n\t        bins (int): Codebook size.\n\t        decay (float): Decay for exponential moving average over the codebooks.\n\t        kmeans_init (bool): Whether to use kmeans to initialize the codebooks.\n", "        kmeans_iters (int): Number of iterations used for kmeans initialization.\n\t        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes\n\t            that have an exponential moving average cluster size less than the specified threshold with\n\t            randomly selected vector from the current batch.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        dimension: int = 256,\n\t        n_q: int = 8,\n\t        bins: int = 1024,\n", "        decay: float = 0.99,\n\t        kmeans_init: bool = True,\n\t        kmeans_iters: int = 50,\n\t        threshold_ema_dead_code: int = 2,\n\t    ):\n\t        super().__init__()\n\t        self.n_q = n_q\n\t        self.dimension = dimension\n\t        self.bins = bins\n\t        self.decay = decay\n", "        self.kmeans_init = kmeans_init\n\t        self.kmeans_iters = kmeans_iters\n\t        self.threshold_ema_dead_code = threshold_ema_dead_code\n\t        self.vq = ResidualVectorQuantization(\n\t            dim=self.dimension,\n\t            codebook_size=self.bins,\n\t            num_quantizers=self.n_q,\n\t            decay=self.decay,\n\t            kmeans_init=self.kmeans_init,\n\t            kmeans_iters=self.kmeans_iters,\n", "            threshold_ema_dead_code=self.threshold_ema_dead_code,\n\t        )\n\t    def forward(self, x: torch.Tensor, sample_rate: int, bandwidth: tp.Optional[float] = None) -> QuantizedResult:\n\t        \"\"\"Residual vector quantization on the given input tensor.\n\t        Args:\n\t            x (torch.Tensor): Input tensor.\n\t            sample_rate (int): Sample rate of the input tensor.\n\t            bandwidth (float): Target bandwidth.\n\t        Returns:\n\t            QuantizedResult:\n", "                The quantized (or approximately quantized) representation with\n\t                the associated bandwidth and any penalty term for the loss.\n\t        \"\"\"\n\t        bw_per_q = self.get_bandwidth_per_quantizer(sample_rate)\n\t        n_q = self.get_num_quantizers_for_bandwidth(sample_rate, bandwidth)\n\t        quantized, codes, commit_loss = self.vq(x, n_q=n_q)\n\t        bw = torch.tensor(n_q * bw_per_q).to(x)\n\t        return QuantizedResult(quantized, codes, bw, penalty=torch.mean(commit_loss))\n\t    def get_num_quantizers_for_bandwidth(self, sample_rate: int, bandwidth: tp.Optional[float] = None) -> int:\n\t        \"\"\"Return n_q based on specified target bandwidth.\n", "        \"\"\"\n\t        bw_per_q = self.get_bandwidth_per_quantizer(sample_rate)\n\t        n_q = self.n_q\n\t        if bandwidth and bandwidth > 0.:\n\t            n_q = int(max(1, math.floor(bandwidth / bw_per_q)))\n\t        return n_q\n\t    def get_bandwidth_per_quantizer(self, sample_rate: int):\n\t        \"\"\"Return bandwidth per quantizer for a given input sample rate.\n\t        \"\"\"\n\t        return math.log2(self.bins) * sample_rate / 1000\n", "    def encode(self, x: torch.Tensor, sample_rate: int, bandwidth: tp.Optional[float] = None) -> torch.Tensor:\n\t        \"\"\"Encode a given input tensor with the specified sample rate at the given bandwidth.\n\t        The RVQ encode method sets the appropriate number of quantizer to use\n\t        and returns indices for each quantizer.\n\t        \"\"\"\n\t        n_q = self.get_num_quantizers_for_bandwidth(sample_rate, bandwidth)\n\t        codes = self.vq.encode(x, n_q=n_q) # vq.encode output -> out_indices\n\t        return codes\n\t    def decode(self, codes: torch.Tensor) -> torch.Tensor:\n\t        \"\"\"Decode the given codes to the quantized representation.\n", "        \"\"\"\n\t        quantized = self.vq.decode(codes) # vq.decode output -> quantized_out\n\t        return quantized\n"]}
{"filename": "quantization/core_vq.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t# This implementation is inspired from\n\t# https://github.com/lucidrains/vector-quantize-pytorch\n\t# which is released under MIT License. Hereafter, the original license:\n\t# MIT License\n", "#\n\t# Copyright (c) 2020 Phil Wang\n\t#\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy\n\t# of this software and associated documentation files (the \"Software\"), to deal\n\t# in the Software without restriction, including without limitation the rights\n\t# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\t# copies of the Software, and to permit persons to whom the Software is\n\t# furnished to do so, subject to the following conditions:\n\t#\n", "# The above copyright notice and this permission notice shall be included in all\n\t# copies or substantial portions of the Software.\n\t#\n\t# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\t# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\t# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\t# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\t# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\t# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\t# SOFTWARE.\n", "\"\"\"Core vector quantization implementation.\"\"\"\n\timport typing as tp\n\timport warnings\n\tfrom einops import rearrange, repeat\n\timport torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\timport distrib\n\tdef default(val: tp.Any, d: tp.Any) -> tp.Any:\n\t    return val if val is not None else d\n", "def ema_inplace(moving_avg, new, decay: float):\n\t    \"\"\"ema update parameter. moving_avg = moving_avg + (1-decay) * new\n\t    Args:\n\t        moving_avg (_type_): \n\t        new (_type_): update parameter\n\t        decay (float): update rate\n\t    \"\"\"\n\t    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))\n\tdef laplace_smoothing(x, n_categories: int, epsilon: float = 1e-5):\n\t    return (x + epsilon) / (x.sum() + n_categories * epsilon)\n", "def uniform_init(*shape: int):\n\t    t = torch.empty(shape)\n\t    nn.init.kaiming_uniform_(t)\n\t    return t\n\tdef sample_vectors(samples, num: int):\n\t    num_samples, device = samples.shape[0], samples.device\n\t    if num_samples >= num:\n\t        indices = torch.randperm(num_samples, device=device)[:num]\n\t    else:\n\t        indices = torch.randint(0, num_samples, (num,), device=device)\n", "    return samples[indices]\n\tdef kmeans(samples, num_clusters: int, num_iters: int = 10):\n\t    dim, dtype = samples.shape[-1], samples.dtype\n\t    means = sample_vectors(samples, num_clusters)\n\t    for _ in range(num_iters):\n\t        diffs = rearrange(samples, \"n d -> n () d\") - rearrange(\n\t            means, \"c d -> () c d\"\n\t        )\n\t        dists = -(diffs ** 2).sum(dim=-1)\n\t        buckets = dists.max(dim=-1).indices\n", "        bins = torch.bincount(buckets, minlength=num_clusters)\n\t        zero_mask = bins == 0\n\t        bins_min_clamped = bins.masked_fill(zero_mask, 1)\n\t        new_means = buckets.new_zeros(num_clusters, dim, dtype=dtype)\n\t        new_means.scatter_add_(0, repeat(buckets, \"n -> n d\", d=dim), samples)\n\t        new_means = new_means / bins_min_clamped[..., None]\n\t        means = torch.where(zero_mask[..., None], means, new_means)\n\t    return means, bins\n\tclass EuclideanCodebook(nn.Module):\n\t    \"\"\"Codebook with Euclidean distance.\n", "    Args:\n\t        dim (int): Dimension.\n\t        codebook_size (int): Codebook size.\n\t        kmeans_init (bool): Whether to use k-means to initialize the codebooks.\n\t            If set to true, run the k-means algorithm on the first training batch and use\n\t            the learned centroids as initialization.\n\t        kmeans_iters (int): Number of iterations used for k-means algorithm at initialization.\n\t        decay (float): Decay for exponential moving average over the codebooks.\n\t        epsilon (float): Epsilon value for numerical stability.\n\t        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes\n", "            that have an exponential moving average cluster size less than the specified threshold with\n\t            randomly selected vector from the current batch.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        dim: int,\n\t        codebook_size: int,\n\t        kmeans_init: int = False,\n\t        kmeans_iters: int = 10,\n\t        decay: float = 0.99,\n", "        epsilon: float = 1e-5,\n\t        threshold_ema_dead_code: int = 2,\n\t    ):\n\t        super().__init__()\n\t        self.decay = decay\n\t        init_fn: tp.Union[tp.Callable[..., torch.Tensor], tp.Any] = uniform_init if not kmeans_init else torch.zeros\n\t        embed = init_fn(codebook_size, dim)\n\t        self.codebook_size = codebook_size\n\t        self.kmeans_iters = kmeans_iters\n\t        self.epsilon = epsilon\n", "        self.threshold_ema_dead_code = threshold_ema_dead_code\n\t        self.register_buffer(\"inited\", torch.Tensor([not kmeans_init]))\n\t        self.register_buffer(\"cluster_size\", torch.zeros(codebook_size))\n\t        self.register_buffer(\"embed\", embed)\n\t        self.register_buffer(\"embed_avg\", embed.clone())\n\t    @torch.jit.ignore\n\t    def init_embed_(self, data):\n\t        if self.inited:\n\t            return\n\t        embed, cluster_size = kmeans(data, self.codebook_size, self.kmeans_iters)\n", "        self.embed.data.copy_(embed)\n\t        self.embed_avg.data.copy_(embed.clone())\n\t        self.cluster_size.data.copy_(cluster_size)\n\t        self.inited.data.copy_(torch.Tensor([True]))\n\t        # Make sure all buffers across workers are in sync after initialization\n\t        # distrib.broadcast_tensors(self.buffers()) # FIXME: this is not working for some reason\n\t    def replace_(self, samples, mask):\n\t        modified_codebook = torch.where(\n\t            mask[..., None], sample_vectors(samples, self.codebook_size), self.embed\n\t        )\n", "        self.embed.data.copy_(modified_codebook)\n\t    def expire_codes_(self, batch_samples):\n\t        if self.threshold_ema_dead_code == 0:\n\t            return\n\t        expired_codes = self.cluster_size < self.threshold_ema_dead_code\n\t        if not torch.any(expired_codes):\n\t            return\n\t        batch_samples = rearrange(batch_samples, \"... d -> (...) d\")\n\t        self.replace_(batch_samples, mask=expired_codes)\n\t        # distrib.broadcast_tensors(self.buffers()) # FIXME: this is not working for some reason\n", "    def preprocess(self, x):\n\t        x = rearrange(x, \"... d -> (...) d\")\n\t        return x\n\t    def quantize(self, x):\n\t        embed = self.embed.t()\n\t        dist = -(\n\t            x.pow(2).sum(1, keepdim=True)\n\t            - 2 * x @ embed\n\t            + embed.pow(2).sum(0, keepdim=True)\n\t        ) # get the distance between x and embed\n", "        embed_ind = dist.max(dim=-1).indices # get the index of the closest embed\n\t        return embed_ind\n\t    def postprocess_emb(self, embed_ind, shape):\n\t        return embed_ind.view(*shape[:-1])\n\t    def dequantize(self, embed_ind):\n\t        quantize = F.embedding(embed_ind, self.embed)\n\t        return quantize\n\t    def encode(self, x):\n\t        shape = x.shape\n\t        # pre-process\n", "        x = self.preprocess(x)\n\t        # quantize\n\t        embed_ind = self.quantize(x)\n\t        # post-process\n\t        embed_ind = self.postprocess_emb(embed_ind, shape)\n\t        return embed_ind\n\t    def decode(self, embed_ind):\n\t        quantize = self.dequantize(embed_ind)\n\t        return quantize\n\t    def forward(self, x):\n", "        shape, dtype = x.shape, x.dtype\n\t        x = self.preprocess(x) # [2,32,128] -> [64,128]\n\t        self.init_embed_(x) # to better initialize the codebook\n\t        embed_ind = self.quantize(x) # get the index of the closest embed\n\t        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)\n\t        embed_ind = self.postprocess_emb(embed_ind, shape)\n\t        quantize = self.dequantize(embed_ind)\n\t        if self.training: # update the codebook\n\t            # We do the expiry of code at that point as buffers are in sync\n\t            # and all the workers will take the same decision.\n", "            self.expire_codes_(x)\n\t            ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)\n\t            embed_sum = x.t() @ embed_onehot\n\t            ema_inplace(self.embed_avg, embed_sum.t(), self.decay)\n\t            cluster_size = (\n\t                laplace_smoothing(self.cluster_size, self.codebook_size, self.epsilon)\n\t                * self.cluster_size.sum()\n\t            )\n\t            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n\t            self.embed.data.copy_(embed_normalized)\n", "        return quantize, embed_ind\n\tclass VectorQuantization(nn.Module):\n\t    \"\"\"Vector quantization implementation.\n\t    Currently supports only euclidean distance.\n\t    Args:\n\t        dim (int): Dimension\n\t        codebook_size (int): Codebook size, the number of vectors in the codebook\n\t        codebook_dim (int): Codebook dimension. If not defined, uses the specified dimension in dim.\n\t                            the dimension of each vector in the codebook\n\t        decay (float): Decay for exponential moving average over the codebooks.\n", "        epsilon (float): Epsilon value for numerical stability.\n\t        kmeans_init (bool): Whether to use kmeans to initialize the codebooks.\n\t        kmeans_iters (int): Number of iterations used for kmeans initialization.\n\t        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes\n\t            that have an exponential moving average cluster size less than the specified threshold with\n\t            randomly selected vector from the current batch.\n\t        commitment_weight (float): Weight for commitment loss.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        dim: int,\n\t        codebook_size: int,\n\t        codebook_dim: tp.Optional[int] = None,\n\t        decay: float = 0.99,\n\t        epsilon: float = 1e-5,\n\t        kmeans_init: bool = True,\n\t        kmeans_iters: int = 50,\n\t        threshold_ema_dead_code: int = 2,\n\t        commitment_weight: float = 1.,\n\t    ):\n", "        super().__init__()\n\t        _codebook_dim: int = default(codebook_dim, dim)\n\t        requires_projection = _codebook_dim != dim\n\t        self.project_in = (nn.Linear(dim, _codebook_dim) if requires_projection else nn.Identity())\n\t        self.project_out = (nn.Linear(_codebook_dim, dim) if requires_projection else nn.Identity())\n\t        self.epsilon = epsilon\n\t        self.commitment_weight = commitment_weight\n\t        self._codebook = EuclideanCodebook(dim=_codebook_dim, codebook_size=codebook_size,\n\t                                           kmeans_init=kmeans_init, kmeans_iters=kmeans_iters,\n\t                                           decay=decay, epsilon=epsilon,\n", "                                           threshold_ema_dead_code=threshold_ema_dead_code)\n\t        self.codebook_size = codebook_size\n\t    @property\n\t    def codebook(self):\n\t        return self._codebook.embed\n\t    def encode(self, x):\n\t        x = rearrange(x, \"b d n -> b n d\")\n\t        x = self.project_in(x)\n\t        embed_in = self._codebook.encode(x)\n\t        return embed_in\n", "    def decode(self, embed_ind):\n\t        quantize = self._codebook.decode(embed_ind)\n\t        quantize = self.project_out(quantize)\n\t        quantize = rearrange(quantize, \"b n d -> b d n\")\n\t        return quantize\n\t    def forward(self, x):\n\t        device = x.device\n\t        x = rearrange(x, \"b d n -> b n d\") # [2,128,32] -> [2,32,128]\n\t        x = self.project_in(x)\n\t        quantize, embed_ind = self._codebook(x)\n", "        if self.training:\n\t            quantize = x + (quantize - x).detach()\n\t        loss = torch.tensor([0.0], device=device, requires_grad=self.training)\n\t        if self.training:\n\t            warnings.warn('When using RVQ in training model, first check '\n\t                          'https://github.com/facebookresearch/encodec/issues/25 . '\n\t                          'The bug wasn\\'t fixed here for reproducibility.')\n\t            if self.commitment_weight > 0:\n\t                commit_loss = F.mse_loss(quantize.detach(), x)\n\t                loss = loss + commit_loss * self.commitment_weight\n", "        quantize = self.project_out(quantize)\n\t        quantize = rearrange(quantize, \"b n d -> b d n\")\n\t        return quantize, embed_ind, loss\n\tclass ResidualVectorQuantization(nn.Module):\n\t    \"\"\"Residual vector quantization implementation.\n\t    Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf\n\t    \"\"\"\n\t    def __init__(self, *, num_quantizers, **kwargs):\n\t        super().__init__()\n\t        self.layers = nn.ModuleList(\n", "            [VectorQuantization(**kwargs) for _ in range(num_quantizers)]\n\t        )\n\t    def forward(self, x, n_q: tp.Optional[int] = None):\n\t        quantized_out = 0.0\n\t        residual = x # x is encoder output emb\n\t        all_losses = []\n\t        all_indices = []\n\t        n_q = n_q or len(self.layers)\n\t        for layer in self.layers[:n_q]:\n\t            quantized, indices, loss = layer(residual)\n", "            residual = residual - quantized.detach()\n\t            quantized_out = quantized_out + quantized # y^hat\n\t            all_indices.append(indices)\n\t            all_losses.append(loss)\n\t        out_losses, out_indices = map(torch.stack, (all_losses, all_indices))\n\t        return quantized_out, out_indices, out_losses\n\t    def encode(self, x: torch.Tensor, n_q: tp.Optional[int] = None) -> torch.Tensor:\n\t        residual = x\n\t        all_indices = []\n\t        n_q = n_q or len(self.layers)\n", "        for layer in self.layers[:n_q]:\n\t            indices = layer.encode(residual)\n\t            quantized = layer.decode(indices)\n\t            residual = residual - quantized\n\t            all_indices.append(indices)\n\t        out_indices = torch.stack(all_indices)\n\t        return out_indices\n\t    def decode(self, q_indices: torch.Tensor) -> torch.Tensor:\n\t        quantized_out = torch.tensor(0.0, device=q_indices.device)\n\t        for i, indices in enumerate(q_indices):\n", "            layer = self.layers[i]\n\t            quantized = layer.decode(indices)\n\t            quantized_out = quantized_out + quantized\n\t        return quantized_out\n"]}
