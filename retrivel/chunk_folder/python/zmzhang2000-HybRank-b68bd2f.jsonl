{"filename": "evaluation.py", "chunked_list": ["import logging\n\tfrom tqdm import tqdm\n\timport torch\n\tfrom beir.retrieval.evaluation import EvaluateRetrieval\n\tfrom models import compute_loss\n\tfrom utils import Metrics\n\tlogger = logging.getLogger()\n\tdef eval_datasets(args, dev_loader, test_loader, model=None):\n\t    if dev_loader:\n\t        logger.info(f\"{'*' * 10} Evaluating on dev set {'*' * 10}\")\n", "        dev_metric = evaluate(args, dev_loader, model)\n\t    else:\n\t        dev_metric = None\n\t    if test_loader:\n\t        logger.info(f\"{'*' * 10} Evaluating on test set {'*' * 10}\")\n\t        test_metric = evaluate(args, test_loader, model)\n\t    else:\n\t        test_metric = None\n\t    return dev_metric, test_metric\n\t@torch.no_grad()\n", "def evaluate(args, dataloader, model=None):\n\t    if model:\n\t        model.eval()\n\t    metrics = Metrics()\n\t    dataloader.dataset.data['pytrec_results'] = dict()\n\t    dataloader.dataset.data['pytrec_qrels'] = dict()\n\t    for idx, (qids, sim_matrix, targets) in tqdm(enumerate(dataloader), desc=\"Evaluating\"):\n\t        sim_matrix, targets = sim_matrix.to(args.device), targets.to(args.device)\n\t        if model:\n\t            scores = model(sim_matrix)\n", "            loss = compute_loss(scores, targets)\n\t            metrics.update(loss=(loss.item(), len(sim_matrix)))\n\t        else:\n\t            scores = torch.arange(sim_matrix.shape[-1], 0, -1, \n\t                                  device=sim_matrix.device)[None, :].expand(sim_matrix.shape[0], -1)\n\t        for qid, score_list in zip(qids, scores):\n\t            qdata = dataloader.dataset.data[qid]\n\t            pytrec_results = {pid:score.item() for pid, score in zip(qdata['retrieved_ctxs'], score_list)}\n\t            dataloader.dataset.data['pytrec_results'][qid] = pytrec_results\n\t            pytrec_qrels = {pid:1 for pid in qdata['positive_ctxs']}\n", "            if 'has_answer' in qdata:\n\t                pytrec_qrels.update({pid:1 for pid, has_answer in zip(qdata['retrieved_ctxs'], qdata['has_answer']) if has_answer})\n\t            dataloader.dataset.data['pytrec_qrels'][qid] = pytrec_qrels\n\t    if model:\n\t        logger.info(\"loss: \" + str(metrics.meters['loss']))\n\t    ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(dataloader.dataset.data['pytrec_qrels'], dataloader.dataset.data['pytrec_results'], [1, 5, 10, 20, 50, 100], ignore_identical_ids=False)\n\t    mrr = EvaluateRetrieval.evaluate_custom(dataloader.dataset.data['pytrec_qrels'], dataloader.dataset.data['pytrec_results'], [1, 5, 10, 20, 50, 100], metric=\"mrr@k\")\n\t    accuracy = EvaluateRetrieval.evaluate_custom(dataloader.dataset.data['pytrec_qrels'], dataloader.dataset.data['pytrec_results'], [1, 5, 10, 20, 50, 100], metric=\"accuracy@k\")\n\t    metrics.update(**ndcg, **mrr, **accuracy)\n\t    logger.info(\"\\n\")\n", "    return metrics\n"]}
{"filename": "main.py", "chunked_list": ["import os\n\timport logging\n\timport torch\n\tfrom torch import optim\n\tfrom torch.cuda.amp.grad_scaler import GradScaler\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom transformers import get_cosine_schedule_with_warmup\n\tfrom config import get_args, print_args\n\tfrom dataset import RetrievalSim\n", "from models import Hybrank, compute_loss\n\tfrom evaluation import eval_datasets\n\tfrom utils import Metrics, setup_logger\n\tlogger = logging.getLogger()\n\ttorch.set_num_threads(1)\n\tdef main():\n\t    args = get_args()\n\t    if args.device == 'cuda':\n\t        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\t    else:\n", "        device = torch.device('cpu')\n\t    # make directories\n\t    args.output_dir = os.path.join('experiments', args.exp_name)\n\t    if not os.path.exists(args.output_dir):\n\t        os.makedirs(args.output_dir, exist_ok=True)\n\t    setup_logger(logger, args)\n\t    print_args(args)\n\t    logger.info(f\"{'*' * 30} Loading datasets and model {'*' * 30}\")\n\t    # load datasets\n\t    assert any([x in args.data_path for x in [\"NQ\", \"MSMARCO\", \"TRECDL2019\", \"TRECDL2020\"]]), \"Dataset unknown.\"\n", "    data_files = os.listdir(args.data_path)\n\t    if not args.only_eval:\n\t        train_dataset = RetrievalSim(root=args.data_path, split='train', \n\t                                    list_len=args.list_len, num_anchors=args.num_anchors)\n\t        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, \n\t                                num_workers=args.num_workers, drop_last=False)\n\t    if any([\"dev\" in x for x in data_files]):\n\t        dev_dataset = RetrievalSim(root=args.data_path, split=\"dev\", \n\t                                list_len=args.list_len, num_anchors=args.num_anchors)\n\t        dev_loader = DataLoader(dev_dataset, batch_size=args.batch_size, shuffle=False, \n", "                                num_workers=args.num_workers, drop_last=False)\n\t    else:\n\t        dev_dataset, dev_loader = None, None\n\t    if any([\"test\" in x for x in data_files]):\n\t        test_dataset = RetrievalSim(root=args.data_path, split=\"test\", \n\t                                    list_len=args.list_len, num_anchors=args.num_anchors)\n\t        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, \n\t                                 num_workers=args.num_workers, drop_last=False)\n\t    else:\n\t        test_dataset, test_loader = None, None\n", "    if \"NQ\" in args.data_path:\n\t        select_criterion_key = \"Accuracy@1\"\n\t    elif \"MSMARCO\" in args.data_path:\n\t        select_criterion_key = \"MRR@10\"\n\t    else:\n\t        select_criterion_key = \"NDCG@10\"\n\t    # define model\n\t    model = Hybrank(in_dim=args.num_sim_type, embed_dim=args.embed_dim,\n\t                    depth=args.depth, num_heads=args.num_heads).to(device)\n\t    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "    logger.info('Number of params: {:.2f}M'.format(n_parameters / 1e6))\n\t    start_epoch = 0\n\t    if args.resume is not None:\n\t        assert os.path.isfile(args.resume), \"No checkpoint found at '{}'\".format(args.resume)\n\t        checkpoint = torch.load(args.resume, map_location='cpu')\n\t        start_epoch = checkpoint['epoch'] + 1\n\t        model.load_state_dict(checkpoint['state_dict'], strict=False)\n\t        logger.info(\"Loaded checkpoint: >>>> '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n\t    logger.info(f\"{'*' * 30} Initial retrieval list {'*' * 30}\")\n\t    eval_datasets(args, dev_loader, test_loader)\n", "    logger.info(f\"{'*' * 30} Initialized model performance {'*' * 30}\")\n\t    eval_datasets(args, dev_loader, test_loader, model)\n\t    if not args.only_eval:\n\t        no_decay = ['bias', 'LayerNorm.weight', 'norm1.weight', 'norm2.weight']\n\t        optimizer_grouped_parameters = [\n\t            {'params': [p for n, p in model.named_parameters() if\n\t                        p.requires_grad and not any(nd in n for nd in no_decay)],\n\t            'weight_decay': args.weight_decay},\n\t            {'params': [p for n, p in model.named_parameters() if\n\t                        p.requires_grad and any(nd in n for nd in no_decay)],\n", "            'weight_decay': 0.0}\n\t        ]\n\t        optimizer = optim.Adam(optimizer_grouped_parameters, lr=args.lr, weight_decay=args.weight_decay)\n\t        lr_scheduler = get_cosine_schedule_with_warmup(\n\t            optimizer=optimizer,\n\t            num_warmup_steps=args.warmup_epochs * (len(train_loader) // args.gradient_accumulation_steps),\n\t            num_training_steps=args.num_epochs * (len(train_loader) // args.gradient_accumulation_steps),\n\t        )\n\t        # Start training\n\t        logger.info(f\"{'*' * 30} Start training {'*' * 30}\")\n", "        metrics = Metrics()\n\t        scaler = GradScaler()\n\t        tb_writer = SummaryWriter(log_dir=os.path.join(args.output_dir, 'tensorboard'))\n\t        best_select_criterion, best_epoch, best_epoch_metric = 0, -1, Metrics()\n\t        for epoch in range(start_epoch, args.num_epochs):\n\t            logger.info(f\"{'*' * 30} Epoch {epoch} {'*' * 30}\")\n\t            metrics.reset_meters()\n\t            optimizer.zero_grad()\n\t            model.train()\n\t            for batch_idx, (_, sim_matrix, targets) in enumerate(train_loader):\n", "                sim_matrix, targets = sim_matrix.to(device), targets.to(device)\n\t                scores = model(sim_matrix)\n\t                loss = compute_loss(scores, targets)\n\t                scaler.scale(loss).backward()\n\t                metrics.meters['loss'].update(loss.item(), n=len(sim_matrix))\n\t                metrics.meters['lr'].reset()\n\t                metrics.meters['lr'].update(lr_scheduler.get_last_lr()[0])\n\t                n_iter = epoch * len(train_loader) + batch_idx\n\t                tb_writer.add_scalar('train/batch_loss', loss.item(), n_iter)\n\t                tb_writer.add_scalar('train/learning_rate', metrics.meters['lr'].avg, n_iter)\n", "                if (batch_idx + 1) % args.gradient_accumulation_steps == 0:\n\t                    if args.max_grad_norm > 0:\n\t                        scaler.unscale_(optimizer)\n\t                        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\t                    scaler.step(optimizer)\n\t                    scaler.update()\n\t                    lr_scheduler.step()\n\t                    optimizer.zero_grad()\n\t                if batch_idx % args.log_per_steps == 0:\n\t                    batch_num = len(train_loader)\n", "                    logger.info(f\"Epoch: {epoch}, Batch: {batch_idx:{len(str(batch_num))}d}/{batch_num}, {metrics}\")\n\t            # Save checkpoint\n\t            model_path = os.path.join(args.output_dir, 'checkpoint-epoch{}.pth'.format(epoch))\n\t            torch.save({'epoch': epoch, 'state_dict': model.state_dict()}, model_path)\n\t            logger.info(f\"Evaluting model performance for epoch {epoch}\")\n\t            n_iter = (epoch + 1) * len(train_loader)\n\t            epoch_dev_metric, epoch_test_metric = eval_datasets(args, dev_loader, test_loader, model)\n\t            if epoch_dev_metric:\n\t                for key, meters in epoch_dev_metric.meters.items():\n\t                    tb_writer.add_scalar(f'dev/{key}', meters.avg, n_iter)\n", "            elif epoch_test_metric:\n\t                for key, meters in epoch_test_metric.meters.items():\n\t                    tb_writer.add_scalar(f'test/{key}', meters.avg, n_iter)\n\t            if epoch_dev_metric is not None:\n\t                select_criterion = epoch_dev_metric.meters[select_criterion_key].avg\n\t            else:\n\t                select_criterion = epoch_test_metric.meters[select_criterion_key].avg\n\t            if select_criterion > best_select_criterion:\n\t                best_select_criterion = select_criterion\n\t                best_epoch_metric = epoch_test_metric if test_loader is not None else epoch_dev_metric\n", "                best_epoch = epoch\n\t            logger.info(f\">>>>>> Best Epoch: {best_epoch} {best_epoch_metric} <<<<<<\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "config.py", "chunked_list": ["import argparse\n\timport random\n\timport logging\n\timport torch\n\tfrom torch import cuda\n\timport numpy as np\n\tlogger = logging.getLogger()\n\tdef get_args():\n\t    parser = argparse.ArgumentParser()\n\t    # training experiment name\n", "    parser.add_argument('--exp_name', type=str, default='test')\n\t    parser.add_argument('--data_path', type=str, default=None)\n\t    # model related params\n\t    parser.add_argument('--num_sim_type', type=int, default=2)\n\t    parser.add_argument('--embed_dim', type=int, default=64)\n\t    parser.add_argument('--depth', type=int, default=2)\n\t    parser.add_argument('--num_heads', type=int, default=8)\n\t    # training specific args\n\t    parser.add_argument('--only_eval', action='store_true', default=False)\n\t    parser.add_argument('--resume', type=str, default=None)\n", "    parser.add_argument('--list_len', type=int, default=100)\n\t    parser.add_argument('--num_anchors', type=int, default=100)\n\t    parser.add_argument('--batch_size', type=int, default=32)\n\t    parser.add_argument('--num_workers', type=int, default=0)\n\t    parser.add_argument('--device', type=str, default='cuda' if cuda.is_available() else 'cpu')\n\t    parser.add_argument('--num_epochs', type=int, default=100)\n\t    parser.add_argument('--warmup_epochs', type=int, default=10)\n\t    parser.add_argument('--lr', type=float, default=1e-3)\n\t    parser.add_argument('--weight_decay', type=float, default=1e-6)\n\t    parser.add_argument('--max_grad_norm', type=float, default=2.0)\n", "    parser.add_argument('--seed', type=int, default=12345)\n\t    parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n\t    parser.add_argument('--log_per_steps', type=int, default=100)\n\t    args = parser.parse_args()\n\t    if args.seed is not None:\n\t        random.seed(args.seed)\n\t        np.random.seed(args.seed)\n\t        torch.manual_seed(args.seed)\n\t        torch.cuda.manual_seed_all(args.seed)\n\t        torch.backends.cudnn.benchmark = False\n", "        torch.backends.cudnn.deterministic = True\n\t    return args\n\tdef print_args(args):\n\t    logger.info(f\"{'*' * 30} CONFIGURATION {'*' * 30}\")\n\t    for key, val in sorted(vars(args).items()):\n\t        keystr = \"{}\".format(key) + (\" \" * (30 - len(key)))\n\t        logger.info(\"%s -->   %s\", keystr, val)\n\t    logger.info(f\"{'*' * 30} CONFIGURATION {'*' * 30}\")\n"]}
{"filename": "models.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom transformers import BertConfig, BertModel\n\tclass Hybrank(nn.Module):\n\t    def __init__(self, in_dim=2, embed_dim=64, depth=2, num_heads=8):\n\t        super().__init__()\n\t        self.cnn = nn.Conv2d(in_dim, embed_dim, 1)\n\t        self.embed_dim = embed_dim\n\t        config = BertConfig(vocab_size=1, hidden_size=embed_dim, num_hidden_layers=depth,\n", "                            num_attention_heads=num_heads, intermediate_size=int(embed_dim * 4))\n\t        self.col_transformer = BertModel(config=config)\n\t        self.out_cls_token = nn.Parameter(torch.empty(1, 1, embed_dim))\n\t        nn.init.kaiming_uniform_(self.out_cls_token)\n\t        config = BertConfig(vocab_size=1, hidden_size=embed_dim, num_hidden_layers=1,\n\t                            num_attention_heads=num_heads, intermediate_size=int(embed_dim * 4))\n\t        self.out_transformer = BertModel(config=config)\n\t        self.t = 0.07\n\t    def forward(self, x):\n\t        x = self.cnn(x)\n", "        x = x.permute(0, 2, 3, 1)\n\t        B, Nrow, Ncol, D = x.shape\n\t        assert D == self.embed_dim\n\t        x = x.transpose(1, 2).reshape(-1, Nrow, self.embed_dim)\n\t        x = self.col_transformer(inputs_embeds=x)[0]\n\t        x = x.reshape(B, Ncol, Nrow, self.embed_dim).transpose(1, 2)\n\t        x = x.reshape(-1, Ncol, self.embed_dim)\n\t        x = torch.cat((self.out_cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n\t        x = self.out_transformer(inputs_embeds=x)[0]\n\t        x = x[:, 0, :]\n", "        x = x.reshape(B, Nrow, self.embed_dim)\n\t        x = F.normalize(x, dim=-1)\n\t        q, p = x[:, :1], x[:, 1:]\n\t        out = (q @ p.transpose(1, 2)).squeeze(1) / self.t\n\t        return out\n\tdef compute_loss(input, target):\n\t    \"\"\"\n\t    :param input: Float tensor of size (B, L)\n\t    :param target: Bool tensor of size (B, L)\n\t    :return: loss: Scalar tensor\n", "    \"\"\"\n\t    # remove samples without positive\n\t    mask = target.sum(dim=-1).bool()\n\t    input, target = input[mask], target[mask]\n\t    target = target.float()\n\t    log_softmax = input.log_softmax(dim=-1)\n\t    loss = -((log_softmax * target).sum(-1) / target.sum(-1)).mean()\n\t    return loss\n"]}
{"filename": "dataset.py", "chunked_list": ["import os\n\timport h5py\n\timport pickle\n\timport logging\n\timport numpy as np\n\timport torch\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import Dataset\n\tlogger = logging.getLogger()\n\tclass RetrievalSim(Dataset):\n", "    r\"\"\"\n\t    :returns\n\t        sim_matrix: FloatTensor with size [C, 1 + list_len, num_anchors]\n\t        label: BoolTensor with size [list_len], relevant or not\n\t    \"\"\"\n\t    def __init__(self, root='data', split='train', list_len=100, num_anchors=100,\n\t                 shuffle_retrieval_list=False, perm_feat_dim=False):\n\t        self.root = root\n\t        self.split = split\n\t        self.list_len = int(list_len)\n", "        self.num_anchors = int(num_anchors)\n\t        self.shuffle_retrieval_list = shuffle_retrieval_list\n\t        self.perm_feat_dim = perm_feat_dim\n\t        logger.info(f'Loading {split} dataset...')\n\t        with open(os.path.join(root, split + '_ground_truth.pkl'), 'rb') as f:\n\t            self.data = pickle.load(f)\n\t        logger.info('Preparing features...')\n\t        self.prepare_features()\n\t        logger.info('Preparing labels...')\n\t        self.prepare_labels()\n", "        self.filter_data()\n\t        self.qs = sorted(list(self.data.keys()))  # sort to guarantee reproducibility\n\t        logger.info(f'Total number of questions: {len(self.qs)}')\n\t        pos_num = []\n\t        for q in self.data:\n\t            pos_num.append(self.data[q]['label'].sum().item())\n\t        logger.info(f'Average positive number: {sum(pos_num) / len(pos_num)}')\n\t    def filter_data(self):\n\t        # remove samples without positive in training\n\t        if self.split == 'train':\n", "            unlabeled_qs = [q for q in self.data if self.data[q]['label'].sum() == 0]\n\t            for q in unlabeled_qs:\n\t                self.data.pop(q)\n\t    def prepare_features(self):\n\t        with h5py.File(os.path.join(self.root, self.split + '_scores.hdf5'), 'r') as f:\n\t            for q in tqdm(self.data):\n\t                bm25_matrix = torch.FloatTensor(np.concatenate((f[q]['query_ctx_bm25_score'][()],\n\t                                                                f[q]['ctx_ctx_bm25_score'][()]), axis=0))\n\t                bm25_matrix = bm25_matrix[:int(1 + self.list_len), :int(self.num_anchors)]\n\t                bm25_matrix = self.norm_feature(bm25_matrix, 100)\n", "                dense_matrix = torch.FloatTensor(np.concatenate((f[q]['query_ctx_dense_score'][()],\n\t                                                                 f[q]['ctx_ctx_dense_score'][()]), axis=0))\n\t                dense_matrix = dense_matrix[:int(1 + self.list_len), :int(self.num_anchors)]\n\t                dense_matrix = self.norm_feature(dense_matrix, 10)\n\t                self.data[q]['feature'] = torch.stack([bm25_matrix, dense_matrix], dim=0)\n\t    @staticmethod\n\t    def norm_feature(x, norm_temperature=None):\n\t        if norm_temperature is not None:\n\t            x = (x / norm_temperature).softmax(dim=-1)\n\t        # max-min normalization\n", "        norm_min = x.min(dim=-1, keepdim=True).values\n\t        norm_max = x.max(dim=-1, keepdim=True).values\n\t        x = (x - norm_min) / (norm_max - norm_min + 1e-10)\n\t        x = x * 2.0 - 1\n\t        return x\n\t    def prepare_labels(self):\n\t        for q, d in self.data.items():\n\t            if 'has_answer' in d and self.split != 'train':\n\t                # for NQ evaluation\n\t                has_answer = d['has_answer']\n", "                label = [hit for hit in has_answer]\n\t            else:\n\t                positive_ctxs = d['positive_ctxs']\n\t                retrieved_ctxs = d['retrieved_ctxs']\n\t                label = [pid in positive_ctxs for pid in retrieved_ctxs]\n\t            self.data[q]['label'] = torch.BoolTensor(label)[:int(self.list_len)]\n\t    def __getitem__(self, index):\n\t        q = self.qs[index]\n\t        sim_matrix = self.data[q]['feature']\n\t        label = self.data[q]['label']\n", "        if self.shuffle_retrieval_list:\n\t            label_perm_idx = torch.randperm(label.shape[0])\n\t            label = label[label_perm_idx]\n\t            matrix_perm_idx = torch.cat((torch.zeros(1, dtype=torch.long),\n\t                                         label_perm_idx + torch.scalar_tensor(1, dtype=torch.long)))\n\t            sim_matrix = sim_matrix[matrix_perm_idx]\n\t            if self.perm_feat_dim:\n\t                sim_matrix = sim_matrix[:, matrix_perm_idx]\n\t        return q, sim_matrix, label\n\t    def __len__(self):\n", "        return len(self.qs)\n"]}
{"filename": "utils.py", "chunked_list": ["import os\n\timport math\n\tfrom collections import defaultdict\n\timport logging\n\timport torch\n\tclass AverageMeter:\n\t    def __init__(self):\n\t        self.reset()\n\t    def reset(self):\n\t        self.val = 0.0\n", "        self.avg = 0.0\n\t        self.sum = 0.0\n\t        self.count = 0\n\t    def update(self, val, n=1):\n\t        if not math.isfinite(val):\n\t            val = 10000.0\n\t        self.val = val\n\t        self.sum += val * n\n\t        self.count += n\n\t        self.avg = self.sum / self.count\n", "    def __str__(self):\n\t        num = self.avg\n\t        if abs(num) > 1e-4:\n\t            return f\"{num:.4f}\"\n\t        else:\n\t            return f\"{num:.4e}\"\n\tclass Metrics:\n\t    def __init__(self, delimiter=\" \"):\n\t        self.meters = defaultdict(AverageMeter)\n\t        self.delimiter = delimiter\n", "    def update(self, **kwargs):\n\t        for k, v in kwargs.items():\n\t            if isinstance(v, list) or isinstance(v, tuple):\n\t                v, n = v\n\t            else:\n\t                v, n = v, 1\n\t            if isinstance(v, torch.Tensor):\n\t                v = v.item()\n\t            if not math.isfinite(v):\n\t                v = 10000.0\n", "            assert isinstance(v, (float, int))\n\t            self.meters[k].update(v, n)\n\t    def reset_meters(self):\n\t        for k in self.meters:\n\t            if isinstance(self.meters[k], AverageMeter):\n\t                self.meters[k].reset()\n\t    def __str__(self):\n\t        meters_str = [name + \": \" + str(meter) for name, meter in self.meters.items()]\n\t        return self.delimiter.join(meters_str)\n\tdef setup_logger(logger, args, filename='log'):\n", "    logger.setLevel(logging.INFO)\n\t    if logger.hasHandlers():\n\t        logger.handlers.clear()\n\t    log_formatter = logging.Formatter(fmt=\"[%(asctime)s][%(levelname)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\t    console = logging.StreamHandler()\n\t    console.setFormatter(log_formatter)\n\t    file_handler = logging.FileHandler(os.path.join(args.output_dir, filename))\n\t    file_handler.setFormatter(log_formatter)\n\t    logger.addHandler(console)\n\t    logger.addHandler(file_handler)\n"]}
{"filename": "data/generate_MSMARCO_data.py", "chunked_list": ["import os\n\timport pickle\n\timport h5py\n\timport argparse\n\tfrom tqdm import tqdm\n\tfrom pyserini.search import FaissSearcher\n\tfrom pyserini.search.faiss.__main__ import init_query_encoder\n\tfrom utils import BM25SimComputerCached, DenseSimComputer\n\t\"\"\"\n\t******************************************************************\n", "ground_truth.pkl\n\t{\n\t    \"qid\":\n\t    {\n\t        \"question\": raw text of question\n\t        \"positive_ctxs\": [pid1, pid2, pid3, ...], string list, variable length\n\t        \"retrieved_ctxs\": [pid1, pid2, pid3, ...], string list, length of K\n\t    }\n\t}\n\t******************************************************************\n", "score.hdf5\n\t{\n\t    \"qid\":\n\t    {\n\t        \"query_ctx_dense_score\": float array, size of [1, K]\n\t        \"ctx_ctx_dense_score\": float array, size of [K, K]\n\t        \"query_ctx_bm25_score\": float array, size of [1, K]\n\t        \"ctx_ctx_bm25_score\": float array, size of [K, K]\n\t    }\n\t}\n", "******************************************************************\n\t\"\"\"\n\tdef generate_positive_list(input_path, output_path):\n\t    # qid, question and positive_ctxs\n\t    for split in ('train', 'dev'):\n\t        data = dict()\n\t        qid2q = dict()\n\t        file_name = 'queries.train.tsv' if split == 'train' else 'queries.dev.small.tsv'\n\t        with open(os.path.join(input_path, file_name), 'r') as f:\n\t            for line in f:\n", "                qid, q = line.rstrip().split('\\t')\n\t                qid2q[qid] = q\n\t        file_name = 'qrels.train.tsv' if split == 'train' else 'qrels.dev.small.tsv'\n\t        with open(os.path.join(input_path, file_name), 'r') as f:\n\t            for line in tqdm(f, desc='generating relevance list'):\n\t                qid, _, pid, rel = line.rstrip().split('\\t')\n\t                assert rel == '1'\n\t                question = qid2q[qid]\n\t                if qid not in data:\n\t                    data[qid] = {'question': question, 'positive_ctxs': []}\n", "                data[qid]['positive_ctxs'].append(pid)\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n\t            pickle.dump(data, f)\n\tdef generate_retrieval_list(index, query_encoder, output_path, results_path_prefix=None):\n\t    # retrieved_ctxs\n\t    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n\t    searcher = FaissSearcher.from_prebuilt_index(index, query_encoder)\n\t    for split in ('train', 'dev'):\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n", "        if results_path_prefix is not None:\n\t            with open(results_path_prefix + split, 'r') as f:\n\t                for i, line in enumerate(tqdm(f, desc='generating retrieval list')):\n\t                    qid, pid, rank, score = line.rstrip().split('\\t')\n\t                    assert qid in data\n\t                    if 'retrieved_ctxs' in data[qid]:\n\t                        data[qid]['retrieved_ctxs'].append(pid)\n\t                    else:\n\t                        data[qid]['retrieved_ctxs'] = [pid]\n\t        else:\n", "            batch_size, threads = 36, 30\n\t            qids = list(data.keys())\n\t            qs = [data[qid]['question'] for qid in qids]\n\t            batch_qids, batch_qs = list(), list()\n\t            for index, (qid, q) in enumerate(tqdm(zip(qids, qs), desc='generating retrieval list')):\n\t                batch_qids.append(qid)\n\t                batch_qs.append(q)\n\t                if (index + 1) % batch_size == 0 or index == len(qids) - 1:\n\t                    batch_hits = searcher.batch_search(batch_qs, batch_qids, k=100, threads=threads)\n\t                    assert len(batch_qids) == len(batch_hits)\n", "                    for qid_in_batch, hits in batch_hits.items():\n\t                        data[qid_in_batch]['retrieved_ctxs'] = [hit.docid for hit in hits]\n\t                    batch_qids.clear()\n\t                    batch_qs.clear()\n\t                else:\n\t                    continue\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n\t            pickle.dump(data, f)\n\tdef generate_dense_score(index, query_encoder, output_path):\n\t    # query_ctx_dense_score and ctx_ctx_dense_score\n", "    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n\t    simcomputer = DenseSimComputer.from_prebuilt_index(index, query_encoder)\n\t    for split in ('train', 'dev'):\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n\t        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n\t            for qid in tqdm(data, desc='generating dense scores'):\n\t                question = data[qid]['question']\n\t                retrieved_pids = data[qid]['retrieved_ctxs']\n\t                query_ctx_dense_score, ctx_ctx_dense_score = simcomputer.compute(question, retrieved_pids)\n", "                if qid not in f.keys():\n\t                    f.create_group(qid)\n\t                f[qid]['query_ctx_dense_score'] = query_ctx_dense_score.astype(float)\n\t                f[qid]['ctx_ctx_dense_score'] = ctx_ctx_dense_score.astype(float)\n\tdef generate_bm25_score(lucene_index_path, output_path):\n\t    # query_ctx_bm25score and ctx_ctx_bm25score\n\t    simcomputer = BM25SimComputerCached(lucene_index_path)\n\t    for split in ('train', 'dev'):\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n", "        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n\t            for qid in tqdm(data, desc='generating BM25 scores'):\n\t                question = data[qid]['question']\n\t                retrieved_pids = data[qid]['retrieved_ctxs']\n\t                query_ctx_bm25_score, ctx_ctx_bm25_score = simcomputer.compute(question, retrieved_pids)\n\t                if qid not in f.keys():\n\t                    f.create_group(qid)\n\t                f[qid]['query_ctx_bm25_score'] = query_ctx_bm25_score.astype(float)\n\t                f[qid]['ctx_ctx_bm25_score'] = ctx_ctx_bm25_score.astype(float)\n\tretriever_dense_index_map = {\n", "    'ANCE': 'msmarco-passage-ance-bf',\n\t    'DistilBERT-KD': 'msmarco-passage-distilbert-dot-margin_mse-T2-bf',\n\t    'TAS-B': 'msmarco-passage-distilbert-dot-tas_b-b256-bf',\n\t    'TCT-ColBERT-v1': 'msmarco-passage-tct_colbert-bf',\n\t    'TCT-ColBERT-v2': 'msmarco-passage-tct_colbert-v2-hnp-bf',\n\t    'RocketQA-retriever': 'msmarco-passage-ance-bf',\n\t    'RocketQAv2-retriever': 'msmarco-passage-ance-bf',\n\t    'RocketQA-reranker': 'msmarco-passage-ance-bf',\n\t    'RocketQAv2-reranker': 'msmarco-passage-ance-bf',\n\t}\n", "retriever_query_encoder_map = {\n\t    'ANCE': 'castorini/ance-msmarco-passage',\n\t    'DistilBERT-KD': 'sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco',\n\t    'TAS-B': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',\n\t    'TCT-ColBERT-v1': 'castorini/tct_colbert-msmarco',\n\t    'TCT-ColBERT-v2': 'castorini/tct_colbert-v2-hnp-msmarco',\n\t    'RocketQA-retriever': 'castorini/ance-msmarco-passage',\n\t    'RocketQAv2-retriever': 'castorini/ance-msmarco-passage',\n\t    'RocketQA-reranker': 'castorini/ance-msmarco-passage',\n\t    'RocketQAv2-reranker': 'castorini/ance-msmarco-passage',\n", "}\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description='Generate MSMARCO dataset.')\n\t    parser.add_argument('--msmarco_data_path', type=str, required=False, default='raw/MSMARCO',\n\t                        help='Original MSMARCO retriever dataset path.')\n\t    parser.add_argument('--retriever', type=str, required=True,\n\t                        choices=[\"ANCE\", \"DistilBERT-KD\", \"TAS-B\", \"TCT-ColBERT-v1\", \"TCT-ColBERT-v2\",\n\t                                 \"RocketQA-retriever\", \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n\t                        default=None, help='Which retriever method to use.')\n\t    parser.add_argument('--lucene_index_path', type=str, required=False,\n", "                        default='pyserini/indexes/lucene-index-msmarco-passage',\n\t                        help='Lucene index path.')\n\t    parser.add_argument('--output_path', type=str, required=False, default=None,\n\t                        help='Generated dataset path.')\n\t    args = parser.parse_args()\n\t    msmarco_data_path = args.msmarco_data_path\n\t    dense_index_name = retriever_dense_index_map[args.retriever]\n\t    query_encoder = retriever_query_encoder_map[args.retriever]\n\t    lucene_index_path = args.lucene_index_path\n\t    if args.output_path is None:\n", "        output_path = 'MSMARCO_' + args.retriever\n\t    else:\n\t        output_path = args.output_path\n\t    if 'RocketQA' in args.retriever:\n\t        results_path_prefix = f'RocketQA_baselines/MSMARCO/{args.retriever.split(\"-\")[0]}/res.top100.'\n\t        if 'reranker' in args.retriever:\n\t            results_path = results_path_prefix.replace('res.top100', 'rerank.res.top100')\n\t    else:\n\t        results_path_prefix = None\n\t    os.makedirs(output_path, exist_ok=True)\n", "    generate_positive_list(msmarco_data_path, output_path)\n\t    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path_prefix)  # 5.5h\n\t    generate_dense_score(dense_index_name, query_encoder, output_path)  # 2h\n\t    generate_bm25_score(lucene_index_path, output_path)  # 15h\n"]}
{"filename": "data/generate_TRECDL_data.py", "chunked_list": ["import os\n\timport pickle\n\timport h5py\n\timport argparse\n\tfrom tqdm import tqdm\n\tfrom pyserini.search import FaissSearcher\n\tfrom pyserini.search.faiss.__main__ import init_query_encoder\n\tfrom utils import BM25SimComputerCached, DenseSimComputer\n\t\"\"\"\n\t******************************************************************\n", "ground_truth.pkl\n\t{\n\t    \"qid\":\n\t    {\n\t        \"question\": raw text of question\n\t        \"positive_ctxs\": [pid1, pid2, pid3, ...], string list, variable length\n\t        \"retrieved_ctxs\": [pid1, pid2, pid3, ...], string list, length of K\n\t    }\n\t}\n\t******************************************************************\n", "score.hdf5\n\t{\n\t    \"qid\":\n\t    {\n\t        \"query_ctx_dense_score\": float array, size of [1, K]\n\t        \"ctx_ctx_dense_score\": float array, size of [K, K]\n\t        \"query_ctx_bm25_score\": float array, size of [1, K]\n\t        \"ctx_ctx_bm25_score\": float array, size of [K, K]\n\t    }\n\t}\n", "******************************************************************\n\t\"\"\"\n\tdef generate_positive_list(input_path, split_year, output_path):\n\t    # qid, question and positive_ctxs\n\t    msmarco_path = output_path.replace(f\"TRECDL{split_year}\", \"MSMARCO\")\n\t    for split in ('test',):\n\t        data = dict()\n\t        qid2q = dict()\n\t        file_name = f'msmarco-test{split_year}-queries.tsv'\n\t        with open(os.path.join(input_path, file_name), 'r') as f:\n", "            for line in f:\n\t                qid, q = line.rstrip().split('\\t')\n\t                qid2q[qid] = q\n\t        file_name = f'{split_year}qrels-pass.txt'\n\t        with open(os.path.join(input_path, file_name), 'r') as f:\n\t            for line in tqdm(f, desc='generating relevance list'):\n\t                qid, _, pid, rel = line.rstrip().split(' ')\n\t                if rel == '0' or rel == '1':\n\t                    continue\n\t                question = qid2q[qid]\n", "                if qid not in data:\n\t                    data[qid] = {'question': question, 'positive_ctxs': []}\n\t                data[qid]['positive_ctxs'].append(pid)\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n\t            pickle.dump(data, f)\n\tdef generate_retrieval_list(index, query_encoder, output_path, results_path=None):\n\t    # retrieved_ctxs\n\t    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n\t    searcher = FaissSearcher.from_prebuilt_index(index, query_encoder)\n\t    for split in ('test',):\n", "        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n\t        if results_path is not None:\n\t            with open(results_path, 'r') as f:\n\t                for i, line in enumerate(tqdm(f, desc='generating retrieval list')):\n\t                    qid, pid, rank, score = line.rstrip().split('\\t')\n\t                    assert qid in data\n\t                    if 'retrieved_ctxs' in data[qid]:\n\t                        data[qid]['retrieved_ctxs'].append(pid)\n\t                    else:\n", "                        data[qid]['retrieved_ctxs'] = [pid]\n\t        else:\n\t            batch_size, threads = 36, 30\n\t            qids = list(data.keys())\n\t            qs = [data[qid]['question'] for qid in qids]\n\t            batch_qids, batch_qs = list(), list()\n\t            for index, (qid, q) in enumerate(tqdm(zip(qids, qs), desc='generating retrieval list')):\n\t                batch_qids.append(qid)\n\t                batch_qs.append(q)\n\t                if (index + 1) % batch_size == 0 or index == len(qids) - 1:\n", "                    batch_hits = searcher.batch_search(batch_qs, batch_qids, k=100, threads=threads)\n\t                    assert len(batch_qids) == len(batch_hits)\n\t                    for qid_in_batch, hits in batch_hits.items():\n\t                        data[qid_in_batch]['retrieved_ctxs'] = [hit.docid for hit in hits]\n\t                    batch_qids.clear()\n\t                    batch_qs.clear()\n\t                else:\n\t                    continue\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n\t            pickle.dump(data, f)\n", "def generate_dense_score(index, query_encoder, output_path):\n\t    # query_ctx_dense_score and ctx_ctx_dense_score\n\t    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n\t    simcomputer = DenseSimComputer.from_prebuilt_index(index, query_encoder)\n\t    for split in ('test',):\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n\t        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n\t            for qid in tqdm(data, desc='generating dense scores'):\n\t                question = data[qid]['question']\n", "                retrieved_pids = data[qid]['retrieved_ctxs']\n\t                query_ctx_dense_score, ctx_ctx_dense_score = simcomputer.compute(question, retrieved_pids)\n\t                if qid not in f.keys():\n\t                    f.create_group(qid)\n\t                f[qid]['query_ctx_dense_score'] = query_ctx_dense_score.astype(float)\n\t                f[qid]['ctx_ctx_dense_score'] = ctx_ctx_dense_score.astype(float)\n\tdef generate_bm25_score(lucene_index_path, output_path):\n\t    # query_ctx_bm25score and ctx_ctx_bm25score\n\t    simcomputer = BM25SimComputerCached(lucene_index_path)\n\t    for split in ('test',):\n", "        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n\t        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n\t            for qid in tqdm(data, desc='generating BM25 scores'):\n\t                question = data[qid]['question']\n\t                retrieved_pids = data[qid]['retrieved_ctxs']\n\t                query_ctx_bm25_score, ctx_ctx_bm25_score = simcomputer.compute(question, retrieved_pids)\n\t                if qid not in f.keys():\n\t                    f.create_group(qid)\n\t                f[qid]['query_ctx_bm25_score'] = query_ctx_bm25_score.astype(float)\n", "                f[qid]['ctx_ctx_bm25_score'] = ctx_ctx_bm25_score.astype(float)\n\tretriever_dense_index_map = {\n\t    'ANCE': 'msmarco-passage-ance-bf',\n\t    'DistilBERT-KD': 'msmarco-passage-distilbert-dot-margin_mse-T2-bf',\n\t    'TAS-B': 'msmarco-passage-distilbert-dot-tas_b-b256-bf',\n\t    'TCT-ColBERT-v1': 'msmarco-passage-tct_colbert-bf',\n\t    'TCT-ColBERT-v2': 'msmarco-passage-tct_colbert-v2-hnp-bf',\n\t    'RocketQA-retriever': 'msmarco-passage-ance-bf',\n\t    'RocketQAv2-retriever': 'msmarco-passage-ance-bf',\n\t    'RocketQA-reranker': 'msmarco-passage-ance-bf',\n", "    'RocketQAv2-reranker': 'msmarco-passage-ance-bf',\n\t}\n\tretriever_query_encoder_map = {\n\t    'ANCE': 'castorini/ance-msmarco-passage',\n\t    'DistilBERT-KD': 'sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco',\n\t    'TAS-B': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',\n\t    'TCT-ColBERT-v1': 'castorini/tct_colbert-msmarco',\n\t    'TCT-ColBERT-v2': 'castorini/tct_colbert-v2-hnp-msmarco',\n\t    'RocketQA-retriever': 'castorini/ance-msmarco-passage',\n\t    'RocketQAv2-retriever': 'castorini/ance-msmarco-passage',\n", "    'RocketQA-reranker': 'castorini/ance-msmarco-passage',\n\t    'RocketQAv2-reranker': 'castorini/ance-msmarco-passage',\n\t}\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description='Generate TREC-DL dataset.')\n\t    parser.add_argument('--trecdl_data_path', type=str, required=False, default='raw/TRECDL',\n\t                        help='Original TREC-DL retriever dataset path.')\n\t    parser.add_argument('--split', type=str, required=True, choices=['2019', '2020'], default=None,\n\t                        help=\"Which split of TREC-DL to use.\")\n\t    parser.add_argument('--retriever', type=str, required=True,\n", "                        choices=[\"ANCE\", \"DistilBERT-KD\", \"TAS-B\", \"TCT-ColBERT-v1\", \"TCT-ColBERT-v2\",\n\t                                 \"RocketQA-retriever\", \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n\t                        default=None, help='Which retriever method to use.')\n\t    parser.add_argument('--lucene_index_path', type=str, required=False,\n\t                        default='pyserini/indexes/lucene-index-msmarco-passage',\n\t                        help='Lucene index path.')\n\t    parser.add_argument('--output_path', type=str, required=False, default=None,\n\t                        help='Generated dataset path.')\n\t    args = parser.parse_args()\n\t    trecdl_data_path = args.trecdl_data_path\n", "    dense_index_name = retriever_dense_index_map[args.retriever]\n\t    query_encoder = retriever_query_encoder_map[args.retriever]\n\t    lucene_index_path = args.lucene_index_path\n\t    if args.output_path is None:\n\t        output_path = f'TRECDL{args.split}_{args.retriever}'\n\t    else:\n\t        output_path = args.output_path\n\t    if 'RocketQA' in args.retriever:\n\t        results_path = f'RocketQA_baselines/TRECDL/{args.retriever.split(\"-\")[0]}/res.top100.{args.split}'\n\t        if 'reranker' in args.retriever:\n", "            results_path = results_path.replace('res.top100', 'rerank.res.top100')\n\t    else:\n\t        results_path = None\n\t    os.makedirs(output_path, exist_ok=True)\n\t    generate_positive_list(trecdl_data_path, args.split, output_path)\n\t    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path)  # 5.5h\n\t    generate_dense_score(dense_index_name, query_encoder, output_path)  # 2h\n\t    generate_bm25_score(lucene_index_path, output_path)  # 15h\n"]}
{"filename": "data/utils.py", "chunked_list": ["import numpy as np\n\tfrom typing import List, Optional, Union\n\tfrom sklearn.preprocessing import normalize\n\tfrom scipy.sparse import csr_matrix, vstack\n\timport transformers\n\ttransformers.logging.set_verbosity_error()\n\tfrom pyserini.index.lucene import IndexReader\n\tfrom pyserini.vectorizer import BM25Vectorizer\n\tfrom pyserini.search import FaissSearcher, QueryEncoder\n\tclass BM25SimComputerCached(BM25Vectorizer):\n", "    \"\"\"Faster than BM25SimComputer when involve the same doc multiple times\"\"\"\n\t    def __init__(self, lucene_index_path: str, min_df: int = 1, verbose: bool = False):\n\t        super().__init__(lucene_index_path, min_df, verbose)\n\t        self.index_reader = IndexReader(lucene_index_path)\n\t        self.tf_vector_cache = dict()\n\t        self.bm25_vector_cache = dict()\n\t    def compute(self, q: str, docids: List[str], norm: Optional[str] = None):\n\t        query_doc_matrix = np.array([[self.index_reader.compute_query_document_score(docid, q) for docid in docids]])\n\t        doc_tf_vectors = self.get_tf_vectors(docids, norm)\n\t        doc_bm25_vectors = self.get_bm25_vectors(docids, norm)\n", "        doc_doc_matrix = doc_tf_vectors.dot(doc_bm25_vectors.T).toarray()\n\t        return query_doc_matrix, doc_doc_matrix\n\t    def get_tf_vectors(self, docids: List[str], norm: Optional[str] = None):\n\t        \"\"\"Get the tf vectors given a list of docids\n\t        Parameters\n\t        ----------\n\t        norm : str\n\t            Normalize the sparse matrix\n\t        docids : List[str]\n\t            The piece of text to analyze.\n", "        Returns\n\t        -------\n\t        csr_matrix\n\t            Sparse matrix representation of tf vectors\n\t        \"\"\"\n\t        num_docs = len(docids)\n\t        vectors = []\n\t        for doc_id in docids:\n\t            if doc_id in self.tf_vector_cache:\n\t                vector = self.tf_vector_cache[doc_id]\n", "            else:\n\t                matrix_col, matrix_data = [], []\n\t                # Term Frequency\n\t                tf = self.index_reader.get_document_vector(doc_id)\n\t                if tf is None:\n\t                    vector = csr_matrix((1, self.vocabulary_size))\n\t                else:\n\t                    # Filter out in-eligible terms\n\t                    tf = {t: tf[t] for t in tf if t in self.term_to_index}\n\t                    # Convert from dict to sparse matrix\n", "                    for term in tf:\n\t                        tfidf = tf[term]\n\t                        matrix_col.append(self.term_to_index[term])\n\t                        matrix_data.append(tfidf)\n\t                    matrix_row = [0] * len(matrix_col)\n\t                    vector = csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(1, self.vocabulary_size))\n\t                self.tf_vector_cache[doc_id] = vector\n\t            vectors.append(vector)\n\t        assert num_docs == len(vectors)\n\t        vectors = vstack(vectors)\n", "        if norm:\n\t            return normalize(vectors, norm=norm)\n\t        return vectors\n\t    def get_bm25_vectors(self, docids: List[str], norm: Optional[str] = None):\n\t        \"\"\"Get the BM25 vectors given a list of docids\n\t        Parameters\n\t        ----------\n\t        norm : str\n\t            Normalize the sparse matrix\n\t        docids : List[str]\n", "            The piece of text to analyze.\n\t        Returns\n\t        -------\n\t        csr_matrix\n\t            Sparse matrix representation of BM25 vectors\n\t        \"\"\"\n\t        num_docs = len(docids)\n\t        vectors = []\n\t        for doc_id in docids:\n\t            if doc_id in self.bm25_vector_cache:\n", "                vector = self.bm25_vector_cache[doc_id]\n\t            else:\n\t                matrix_col, matrix_data = [], []\n\t                # Term Frequency\n\t                tf = self.index_reader.get_document_vector(doc_id)\n\t                if tf is None:\n\t                    vector = csr_matrix((1, self.vocabulary_size))\n\t                else:\n\t                    # Filter out in-eligible terms\n\t                    tf = {t: tf[t] for t in tf if t in self.term_to_index}\n", "                    # Convert from dict to sparse matrix\n\t                    for term in tf:\n\t                        bm25_weight = self.index_reader.compute_bm25_term_weight(doc_id, term, analyzer=None)\n\t                        matrix_col.append(self.term_to_index[term])\n\t                        matrix_data.append(bm25_weight)\n\t                    matrix_row = [0] * len(matrix_col)\n\t                    vector = csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(1, self.vocabulary_size))\n\t                self.bm25_vector_cache[doc_id] = vector\n\t            vectors.append(vector)\n\t        assert num_docs == len(vectors)\n", "        vectors = vstack(vectors)\n\t        if norm:\n\t            return normalize(vectors, norm=norm)\n\t        return vectors\n\tclass BM25SimComputer(BM25Vectorizer):\n\t    def __init__(self, lucene_index_path: str, min_df: int = 1, verbose: bool = False):\n\t        super().__init__(lucene_index_path, min_df, verbose)\n\t        self.index_reader = IndexReader(lucene_index_path)\n\t    def compute(self, q: str, docids: List[str], norm: Optional[str] = None):\n\t        query_doc_matrix = np.array([[self.index_reader.compute_query_document_score(docid, q) for docid in docids]])\n", "        doc_tf_vectors = self.get_tf_vectors(docids, norm)\n\t        doc_bm25_vectors = self.get_bm25_vectors(docids, norm)\n\t        doc_doc_matrix = doc_tf_vectors.dot(doc_bm25_vectors.T).toarray()\n\t        return query_doc_matrix, doc_doc_matrix\n\t    def get_tf_vectors(self, docids: List[str], norm: Optional[str] = None):\n\t        \"\"\"Get the tf vectors given a list of docids\n\t        Parameters\n\t        ----------\n\t        norm : str\n\t            Normalize the sparse matrix\n", "        docids : List[str]\n\t            The piece of text to analyze.\n\t        Returns\n\t        -------\n\t        csr_matrix\n\t            Sparse matrix representation of tf vectors\n\t        \"\"\"\n\t        matrix_row, matrix_col, matrix_data = [], [], []\n\t        num_docs = len(docids)\n\t        for index, doc_id in enumerate(docids):\n", "            # Term Frequency\n\t            tf = self.index_reader.get_document_vector(doc_id)\n\t            if tf is None:\n\t                continue\n\t            # Filter out in-eligible terms\n\t            tf = {t: tf[t] for t in tf if t in self.term_to_index}\n\t            # Convert from dict to sparse matrix\n\t            for term in tf:\n\t                tfidf = tf[term]\n\t                matrix_row.append(index)\n", "                matrix_col.append(self.term_to_index[term])\n\t                matrix_data.append(tfidf)\n\t        vectors = csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(num_docs, self.vocabulary_size))\n\t        if norm:\n\t            return normalize(vectors, norm=norm)\n\t        return vectors\n\t    def get_bm25_vectors(self, docids: List[str], norm: Optional[str] = None):\n\t        \"\"\"Get the BM25 vectors given a list of docids\n\t        Parameters\n\t        ----------\n", "        norm : str\n\t            Normalize the sparse matrix\n\t        docids : List[str]\n\t            The piece of text to analyze.\n\t        Returns\n\t        -------\n\t        csr_matrix\n\t            Sparse matrix representation of BM25 vectors\n\t        \"\"\"\n\t        matrix_row, matrix_col, matrix_data = [], [], []\n", "        num_docs = len(docids)\n\t        for index, doc_id in enumerate(docids):\n\t            # Term Frequency\n\t            tf = self.index_reader.get_document_vector(doc_id)\n\t            if tf is None:\n\t                continue\n\t            # Filter out in-eligible terms\n\t            tf = {t: tf[t] for t in tf if t in self.term_to_index}\n\t            # Convert from dict to sparse matrix\n\t            for term in tf:\n", "                bm25_weight = self.index_reader.compute_bm25_term_weight(doc_id, term, analyzer=None)\n\t                matrix_row.append(index)\n\t                matrix_col.append(self.term_to_index[term])\n\t                matrix_data.append(bm25_weight)\n\t        vectors = csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(num_docs, self.vocabulary_size))\n\t        if norm:\n\t            return normalize(vectors, norm=norm)\n\t        return vectors\n\tclass DenseSimComputer(FaissSearcher):\n\t    def __init__(self, index_dir: str, query_encoder: Union[QueryEncoder, str],\n", "                 prebuilt_index_name: Optional[str] = None):\n\t        super().__init__(index_dir, query_encoder, prebuilt_index_name)\n\t        self.docid2idx = {docid: i for i, docid in enumerate(self.docids)}\n\t    def compute(self, q: str, docids: List[str], norm: Optional[str] = None):\n\t        query_vectors = self.get_query_vectors(q, norm)\n\t        doc_vectors = self.get_doc_vectors(docids, norm)\n\t        query_doc_matrix = query_vectors @ doc_vectors.T\n\t        doc_doc_matrix = doc_vectors @ doc_vectors.T\n\t        return query_doc_matrix, doc_doc_matrix\n\t    def get_query_vectors(self, q: str, norm: Optional[str] = None):\n", "        \"\"\"Get the dense vectors given a query text\n\t        Parameters\n\t        ----------\n\t        norm : str\n\t            Normalize the matrix\n\t        q : str\n\t            The raw query text to analyze.\n\t        Returns\n\t        -------\n\t        vectors\n", "            Numpy matrix representation of dense vectors\n\t        \"\"\"\n\t        vectors = self.query_encoder.encode(q)\n\t        vectors = vectors.reshape((1, -1))\n\t        if norm:\n\t            return normalize(vectors, norm=norm)\n\t        return vectors\n\t    def get_doc_vectors(self, docids: List[str], norm: Optional[str] = None):\n\t        \"\"\"Get the dense vectors given a list of docids\n\t        Parameters\n", "        ----------\n\t        norm : str\n\t            Normalize the matrix\n\t        docids : List[str]\n\t            The piece of text to analyze.\n\t        Returns\n\t        -------\n\t        vectors\n\t            Numpy matrix representation of dense vectors\n\t        \"\"\"\n", "        indices = [self.docid2idx[docid] for docid in docids]\n\t        vectors = [self.index.reconstruct(i) for i in indices]\n\t        vectors = np.vstack(vectors)\n\t        if norm:\n\t            return normalize(vectors, norm=norm)\n\t        return vectors\n"]}
{"filename": "data/download_DPR_data.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Copyright (c) Facebook, Inc. and its affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"\n\t Command line tool to download various preprocessed data sources & checkpoints for DPR\n\t\"\"\"\n\timport argparse\n", "import gzip\n\timport logging\n\timport os\n\timport pathlib\n\timport wget\n\tfrom typing import Tuple\n\tlogger = logging.getLogger(__name__)\n\t# TODO: move to hydra config group\n\tNQ_LICENSE_FILES = [\n\t    \"https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\",\n", "    \"https://dl.fbaipublicfiles.com/dpr/nq_license/README\",\n\t]\n\tRESOURCES_MAP = {\n\t    \"data.wikipedia_split.psgs_w100\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\",\n\t        \"original_ext\": \".tsv\",\n\t        \"compressed\": True,\n\t        \"desc\": \"Entire wikipedia passages set obtain by splitting all pages into 100-word segments (no overlap)\",\n\t    },\n\t    \"data.retriever.nq-dev\": {\n", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"NQ dev subset with passages pools for the Retriever train time validation\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.retriever.nq-train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n", "        \"desc\": \"NQ train subset with passages pools for the Retriever training\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.retriever.nq-adv-hn-train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-adv-hn-train.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"NQ train subset with hard negative passages mined using the baseline DPR NQ encoders & wikipedia index\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n", "    \"data.retriever.trivia-dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-trivia-dev.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"TriviaQA dev subset with passages pools for the Retriever train time validation\",\n\t    },\n\t    \"data.retriever.trivia-train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-trivia-train.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n", "        \"desc\": \"TriviaQA train subset with passages pools for the Retriever training\",\n\t    },\n\t    \"data.retriever.squad1-train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-squad1-train.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"SQUAD 1.1 train subset with passages pools for the Retriever training\",\n\t    },\n\t    \"data.retriever.squad1-dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-squad1-dev.json.gz\",\n", "        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"SQUAD 1.1 dev subset with passages pools for the Retriever train time validation\",\n\t    },\n\t    \"data.retriever.webq-train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-webquestions-train.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"WebQuestions dev subset with passages pools for the Retriever train time validation\",\n\t    },\n", "    \"data.retriever.webq-dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-webquestions-dev.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"WebQuestions dev subset with passages pools for the Retriever train time validation\",\n\t    },\n\t    \"data.retriever.curatedtrec-train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-curatedtrec-train.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n", "        \"desc\": \"CuratedTrec dev subset with passages pools for the Retriever train time validation\",\n\t    },\n\t    \"data.retriever.curatedtrec-dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-curatedtrec-dev.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"CuratedTrec dev subset with passages pools for the Retriever train time validation\",\n\t    },\n\t    \"data.retriever.qas.nq-dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-dev.qa.csv\",\n", "        \"original_ext\": \".csv\",\n\t        \"compressed\": False,\n\t        \"desc\": \"NQ dev subset for Retriever validation and IR results generation\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.retriever.qas.nq-test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-test.qa.csv\",\n\t        \"original_ext\": \".csv\",\n\t        \"compressed\": False,\n\t        \"desc\": \"NQ test subset for Retriever validation and IR results generation\",\n", "        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.retriever.qas.nq-train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-train.qa.csv\",\n\t        \"original_ext\": \".csv\",\n\t        \"compressed\": False,\n\t        \"desc\": \"NQ train subset for Retriever validation and IR results generation\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    #\n", "    \"data.retriever.qas.trivia-dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/trivia-dev.qa.csv.gz\",\n\t        \"original_ext\": \".csv\",\n\t        \"compressed\": True,\n\t        \"desc\": \"Trivia dev subset for Retriever validation and IR results generation\",\n\t    },\n\t    \"data.retriever.qas.trivia-test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/trivia-test.qa.csv.gz\",\n\t        \"original_ext\": \".csv\",\n\t        \"compressed\": True,\n", "        \"desc\": \"Trivia test subset for Retriever validation and IR results generation\",\n\t    },\n\t    \"data.retriever.qas.trivia-train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/trivia-train.qa.csv.gz\",\n\t        \"original_ext\": \".csv\",\n\t        \"compressed\": True,\n\t        \"desc\": \"Trivia train subset for Retriever validation and IR results generation\",\n\t    },\n\t    \"data.retriever.qas.squad1-test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/squad1-test.qa.csv\",\n", "        \"original_ext\": \".csv\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Trivia test subset for Retriever validation and IR results generation\",\n\t    },\n\t    \"data.retriever.qas.webq-test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/webquestions-test.qa.csv\",\n\t        \"original_ext\": \".csv\",\n\t        \"compressed\": False,\n\t        \"desc\": \"WebQuestions test subset for Retriever validation and IR results generation\",\n\t    },\n", "    \"data.retriever.qas.curatedtrec-test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/curatedtrec-test.qa.csv\",\n\t        \"original_ext\": \".csv\",\n\t        \"compressed\": False,\n\t        \"desc\": \"CuratedTrec test subset for Retriever validation and IR results generation\",\n\t    },\n\t    \"data.gold_passages_info.nq_train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/nq_gold_info/nq-train_gold_info.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n", "        \"desc\": \"Original NQ (our train subset) gold positive passages and alternative question tokenization\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.gold_passages_info.nq_dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/nq_gold_info/nq-dev_gold_info.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"Original NQ (our dev subset) gold positive passages and alternative question tokenization\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n", "    \"data.gold_passages_info.nq_test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/nq_gold_info/nq-test_gold_info.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"Original NQ (our test, original dev subset) gold positive passages and alternative question \"\n\t        \"tokenization\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"pretrained.fairseq.roberta-base.dict\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/pretrained/fairseq/roberta/dict.txt\",\n", "        \"original_ext\": \".txt\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Dictionary for pretrained fairseq roberta model\",\n\t    },\n\t    \"pretrained.fairseq.roberta-base.model\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/pretrained/fairseq/roberta/model.pt\",\n\t        \"original_ext\": \".pt\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Weights for pretrained fairseq roberta base model\",\n\t    },\n", "    \"pretrained.pytext.bert-base.model\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/pretrained/pytext/bert/bert-base-uncased.pt\",\n\t        \"original_ext\": \".pt\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Weights for pretrained pytext bert base model\",\n\t    },\n\t    \"data.retriever_results.nq.single.wikipedia_passages\": {\n\t        \"s3_url\": [\n\t            \"https://dl.fbaipublicfiles.com/dpr/data/wiki_encoded/single/nq/wiki_passages_{}\".format(i)\n\t            for i in range(50)\n", "        ],\n\t        \"original_ext\": \".pkl\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Encoded wikipedia files using a biencoder checkpoint(\"\n\t        \"checkpoint.retriever.single.nq.bert-base-encoder) trained on NQ dataset \",\n\t    },\n\t    \"data.retriever_results.nq.single-adv-hn.wikipedia_passages\": {\n\t        \"s3_url\": [\n\t            \"https://dl.fbaipublicfiles.com/dpr/data/wiki_encoded/single-adv-hn/nq/wiki_passages_{}\".format(i)\n\t            for i in range(50)\n", "        ],\n\t        \"original_ext\": \".pkl\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Encoded wikipedia files using a biencoder checkpoint(\"\n\t        \"checkpoint.retriever.single-adv-hn.nq.bert-base-encoder) trained on NQ dataset + adversarial hard negatives\",\n\t    },\n\t    \"data.retriever_results.nq.single.test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single/nq-test.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n", "        \"desc\": \"Retrieval results of NQ test dataset for the encoder trained on NQ\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.retriever_results.nq.single.dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single/nq-dev.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"Retrieval results of NQ dev dataset for the encoder trained on NQ\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n", "    \"data.retriever_results.nq.single.train\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single/nq-train.json.gz\",\n\t        \"original_ext\": \".json\",\n\t        \"compressed\": True,\n\t        \"desc\": \"Retrieval results of NQ train dataset for the encoder trained on NQ\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.retriever_results.nq.single-adv-hn.test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single-adv-hn/nq-test.json.gz\",\n\t        \"original_ext\": \".json\",\n", "        \"compressed\": True,\n\t        \"desc\": \"Retrieval results of NQ test dataset for the encoder trained on NQ + adversarial hard negatives\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"checkpoint.retriever.single.nq.bert-base-encoder\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/retriever/single/nq/hf_bert_base.cp\",\n\t        \"original_ext\": \".cp\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Biencoder weights trained on NQ data and HF bert-base-uncased model\",\n\t    },\n", "    \"checkpoint.retriever.multiset.bert-base-encoder\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/retriver/multiset/hf_bert_base.cp\",\n\t        \"original_ext\": \".cp\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Biencoder weights trained on multi set data and HF bert-base-uncased model\",\n\t    },\n\t    \"checkpoint.retriever.single-adv-hn.nq.bert-base-encoder\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/retriver/single-adv-hn/nq/hf_bert_base.cp\",\n\t        \"original_ext\": \".cp\",\n\t        \"compressed\": False,\n", "        \"desc\": \"Biencoder weights trained on the original DPR NQ data combined with adversarial hard negatives (See data.retriever.nq-adv-hn-train resource). \"\n\t        \"The model is HF bert-base-uncased\",\n\t    },\n\t    \"data.reader.nq.single.train\": {\n\t        \"s3_url\": [\"https://dl.fbaipublicfiles.com/dpr/data/reader/nq/single/train.{}.pkl\".format(i) for i in range(8)],\n\t        \"original_ext\": \".pkl\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Reader model NQ train dataset input data preprocessed from retriever results (also trained on NQ)\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n", "    \"data.reader.nq.single.dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/reader/nq/single/dev.0.pkl\",\n\t        \"original_ext\": \".pkl\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Reader model NQ dev dataset input data preprocessed from retriever results (also trained on NQ)\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.reader.nq.single.test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/reader/nq/single/test.0.pkl\",\n\t        \"original_ext\": \".pkl\",\n", "        \"compressed\": False,\n\t        \"desc\": \"Reader model NQ test dataset input data preprocessed from retriever results (also trained on NQ)\",\n\t        \"license_files\": NQ_LICENSE_FILES,\n\t    },\n\t    \"data.reader.trivia.multi-hybrid.train\": {\n\t        \"s3_url\": [\n\t            \"https://dl.fbaipublicfiles.com/dpr/data/reader/trivia/multi-hybrid/train.{}.pkl\".format(i)\n\t            for i in range(8)\n\t        ],\n\t        \"original_ext\": \".pkl\",\n", "        \"compressed\": False,\n\t        \"desc\": \"Reader model Trivia train dataset input data preprocessed from hybrid retriever results \"\n\t        \"(where dense part is trained on multiset)\",\n\t    },\n\t    \"data.reader.trivia.multi-hybrid.dev\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/reader/trivia/multi-hybrid/dev.0.pkl\",\n\t        \"original_ext\": \".pkl\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Reader model Trivia dev dataset input data preprocessed from hybrid retriever results \"\n\t        \"(where dense part is trained on multiset)\",\n", "    },\n\t    \"data.reader.trivia.multi-hybrid.test\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/reader/trivia/multi-hybrid/test.0.pkl\",\n\t        \"original_ext\": \".pkl\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Reader model Trivia test dataset input data preprocessed from hybrid retriever results \"\n\t        \"(where dense part is trained on multiset)\",\n\t    },\n\t    \"checkpoint.reader.nq-single.hf-bert-base\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-single/hf_bert_base.cp\",\n", "        \"original_ext\": \".cp\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Reader weights trained on NQ-single retriever results and HF bert-base-uncased model\",\n\t    },\n\t    \"checkpoint.reader.nq-trivia-hybrid.hf-bert-base\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-trivia-hybrid/hf_bert_base.cp\",\n\t        \"original_ext\": \".cp\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Reader weights trained on Trivia multi hybrid retriever results and HF bert-base-uncased model\",\n\t    },\n", "    # extra checkpoints for EfficientQA competition\n\t    \"checkpoint.reader.nq-single-subset.hf-bert-base\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-single-seen_only/hf_bert_base.cp\",\n\t        \"original_ext\": \".cp\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Reader weights trained on NQ-single retriever results and HF bert-base-uncased model, when only Wikipedia pages seen during training are considered\",\n\t    },\n\t    \"checkpoint.reader.nq-tfidf.hf-bert-base\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-drqa/hf_bert_base.cp\",\n\t        \"original_ext\": \".cp\",\n", "        \"compressed\": False,\n\t        \"desc\": \"Reader weights trained on TFIDF results and HF bert-base-uncased model\",\n\t    },\n\t    \"checkpoint.reader.nq-tfidf-subset.hf-bert-base\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-drqa-seen_only/hf_bert_base.cp\",\n\t        \"original_ext\": \".cp\",\n\t        \"compressed\": False,\n\t        \"desc\": \"Reader weights trained on TFIDF results and HF bert-base-uncased model, when only Wikipedia pages seen during training are considered\",\n\t    },\n\t    # retrieval indexes\n", "    \"indexes.single.nq.full.index\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/full.index.dpr\",\n\t        \"original_ext\": \".dpr\",\n\t        \"compressed\": False,\n\t        \"desc\": \"DPR index on NQ-single retriever\",\n\t    },\n\t    \"indexes.single.nq.full.index_meta\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/full.index_meta.dpr\",\n\t        \"original_ext\": \".dpr\",\n\t        \"compressed\": False,\n", "        \"desc\": \"DPR index on NQ-single retriever (metadata)\",\n\t    },\n\t    \"indexes.single.nq.subset.index\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/seen_only.index.dpr\",\n\t        \"original_ext\": \".dpr\",\n\t        \"compressed\": False,\n\t        \"desc\": \"DPR index on NQ-single retriever when only Wikipedia pages seen during training are considered\",\n\t    },\n\t    \"indexes.single.nq.subset.index_meta\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/seen_only.index_meta.dpr\",\n", "        \"original_ext\": \".dpr\",\n\t        \"compressed\": False,\n\t        \"desc\": \"DPR index on NQ-single retriever when only Wikipedia pages seen during training are considered (metadata)\",\n\t    },\n\t    \"indexes.tfidf.nq.full\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/drqa/nq/full-tfidf.npz\",\n\t        \"original_ext\": \".npz\",\n\t        \"compressed\": False,\n\t        \"desc\": \"TFIDF index\",\n\t    },\n", "    \"indexes.tfidf.nq.subset\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/drqa/nq/seen_only-tfidf.npz\",\n\t        \"original_ext\": \".npz\",\n\t        \"compressed\": False,\n\t        \"desc\": \"TFIDF index when only Wikipedia pages seen during training are considered\",\n\t    },\n\t    # Universal retriever project data\n\t    \"data.wikipedia_split.psgs_w100\": {\n\t        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\",\n\t        \"original_ext\": \".tsv\",\n", "        \"compressed\": True,\n\t        \"desc\": \"Entire wikipedia passages set obtain by splitting all pages into 100-word segments (no overlap)\",\n\t    },\n\t}\n\tdef unpack(gzip_file: str, out_file: str):\n\t    logger.info(\"Uncompressing %s\", gzip_file)\n\t    input = gzip.GzipFile(gzip_file, \"rb\")\n\t    s = input.read()\n\t    input.close()\n\t    output = open(out_file, \"wb\")\n", "    output.write(s)\n\t    output.close()\n\t    logger.info(\" Saved to %s\", out_file)\n\tdef download_resource(\n\t    s3_url: str, original_ext: str, compressed: bool, resource_key: str, out_dir: str\n\t) -> Tuple[str, str]:\n\t    logger.info(\"Requested resource from %s\", s3_url)\n\t    path_names = resource_key.split(\".\")\n\t    if out_dir:\n\t        root_dir = out_dir\n", "    else:\n\t        # since hydra overrides the location for the 'current dir' for every run and we don't want to duplicate\n\t        # resources multiple times, remove the current folder's volatile part\n\t        root_dir = os.path.abspath(\"./\")\n\t        if \"/outputs/\" in root_dir:\n\t            root_dir = root_dir[: root_dir.index(\"/outputs/\")]\n\t    logger.info(\"Download root_dir %s\", root_dir)\n\t    save_root = os.path.join(root_dir, \"downloads\", *path_names[:-1])  # last segment is for file name\n\t    pathlib.Path(save_root).mkdir(parents=True, exist_ok=True)\n\t    local_file_uncompressed = os.path.abspath(os.path.join(save_root, path_names[-1] + original_ext))\n", "    logger.info(\"File to be downloaded as %s\", local_file_uncompressed)\n\t    if os.path.exists(local_file_uncompressed):\n\t        logger.info(\"File already exist %s\", local_file_uncompressed)\n\t        return save_root, local_file_uncompressed\n\t    local_file = os.path.abspath(os.path.join(save_root, path_names[-1] + (\".tmp\" if compressed else original_ext)))\n\t    wget.download(s3_url, out=local_file)\n\t    logger.info(\"Downloaded to %s\", local_file)\n\t    if compressed:\n\t        uncompressed_file = os.path.join(save_root, path_names[-1] + original_ext)\n\t        unpack(local_file, uncompressed_file)\n", "        os.remove(local_file)\n\t        local_file = uncompressed_file\n\t    return save_root, local_file\n\tdef download_file(s3_url: str, out_dir: str, file_name: str):\n\t    logger.info(\"Loading from %s\", s3_url)\n\t    local_file = os.path.join(out_dir, file_name)\n\t    if os.path.exists(local_file):\n\t        logger.info(\"File already exist %s\", local_file)\n\t        return\n\t    wget.download(s3_url, out=local_file)\n", "    logger.info(\"Downloaded to %s\", local_file)\n\tdef download(resource_key: str, out_dir: str = None):\n\t    if resource_key not in RESOURCES_MAP:\n\t        # match by prefix\n\t        resources = [k for k in RESOURCES_MAP.keys() if k.startswith(resource_key)]\n\t        logger.info(\"matched by prefix resources: %s\", resources)\n\t        if resources:\n\t            for key in resources:\n\t                download(key, out_dir)\n\t        else:\n", "            logger.info(\"no resources found for specified key\")\n\t        return []\n\t    download_info = RESOURCES_MAP[resource_key]\n\t    s3_url = download_info[\"s3_url\"]\n\t    save_root_dir = None\n\t    data_files = []\n\t    if isinstance(s3_url, list):\n\t        for i, url in enumerate(s3_url):\n\t            save_root_dir, local_file = download_resource(\n\t                url,\n", "                download_info[\"original_ext\"],\n\t                download_info[\"compressed\"],\n\t                \"{}_{}\".format(resource_key, i),\n\t                out_dir,\n\t            )\n\t            data_files.append(local_file)\n\t    else:\n\t        save_root_dir, local_file = download_resource(\n\t            s3_url,\n\t            download_info[\"original_ext\"],\n", "            download_info[\"compressed\"],\n\t            resource_key,\n\t            out_dir,\n\t        )\n\t        data_files.append(local_file)\n\t    license_files = download_info.get(\"license_files\", None)\n\t    if license_files:\n\t        download_file(license_files[0], save_root_dir, \"LICENSE\")\n\t        download_file(license_files[1], save_root_dir, \"README\")\n\t    return data_files\n", "def main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--output_dir\",\n\t        default=\"./\",\n\t        type=str,\n\t        help=\"The output directory to download file\",\n\t    )\n\t    parser.add_argument(\n\t        \"--resource\",\n", "        type=str,\n\t        help=\"Resource name. See RESOURCES_MAP for all possible values\",\n\t    )\n\t    args = parser.parse_args()\n\t    if args.resource:\n\t        download(args.resource, args.output_dir)\n\t    else:\n\t        logger.warning(\"Please specify resource value. Possible options are:\")\n\t        for k, v in RESOURCES_MAP.items():\n\t            logger.warning(\"Resource key=%s  :  %s\", k, v[\"desc\"])\n", "if __name__ == \"__main__\":\n\t    main()"]}
{"filename": "data/generate_NQ_data.py", "chunked_list": ["import os\n\timport json\n\timport pickle\n\timport h5py\n\timport argparse\n\tfrom tqdm import tqdm\n\tfrom pyserini.search import FaissSearcher\n\tfrom pyserini.search.faiss.__main__ import init_query_encoder\n\tfrom pyserini.eval.evaluate_dpr_retrieval import has_answers, SimpleTokenizer\n\tfrom utils import BM25SimComputerCached, DenseSimComputer\n", "\"\"\"\n\t******************************************************************\n\tground_truth.pkl\n\t{\n\t    \"qid\":\n\t    {\n\t        \"question\": raw text of question\n\t        \"answers\": [answer1, answer2, ...], string list, variable length\n\t        \"positive_ctxs\": [pid1, pid2, pid3, ...], string list, variable length\n\t        \"retrieved_ctxs\": [pid1, pid2, pid3, ...], string list, length of K\n", "        \"has_answer\": [True, False, True, ...], bool list, length of K\n\t    }\n\t}\n\t******************************************************************\n\tscore.hdf5\n\t{\n\t    \"qid\":\n\t    {\n\t        \"query_ctx_dense_score\": float array, size of [1, K]\n\t        \"ctx_ctx_dense_score\": float array, size of [K, K]\n", "        \"query_ctx_bm25_score\": float array, size of [1, K]\n\t        \"ctx_ctx_bm25_score\": float array, size of [K, K]\n\t    }\n\t}\n\t******************************************************************\n\t\"\"\"\n\tdef generate_positive_list(input_path, output_path):\n\t    # qid, question, answers and positive_ctxs\n\t    for split in ('train', 'dev', 'test'):\n\t        data = dict()\n", "        q2id = dict()\n\t        with open(os.path.join(input_path, 'qas', 'nq-' + split + '.csv'), 'r') as f:\n\t            for i, line in enumerate(tqdm(f, desc='reading questions')):\n\t                question, answer_str = line.rstrip().split('\\t')\n\t                data[str(i)] = {'question': question, 'answers': eval(answer_str), 'positive_ctxs': []}\n\t                q2id[question] = str(i)\n\t        if split != 'test':\n\t            with open(os.path.join(input_path, 'nq-' + split + '.json'), 'r') as f:\n\t                anno = json.load(f)\n\t                for sample in tqdm(anno, desc='generating relevance list'):\n", "                    qid = q2id[sample['question']]\n\t                    data[qid]['positive_ctxs'] = [x['passage_id'] for x in sample['positive_ctxs']]\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n\t            pickle.dump(data, f)\n\tdef generate_retrieval_list(index, query_encoder, output_path, results_path_prefix=None):\n\t    # retrieved_ctxs and has_answer\n\t    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n\t    searcher = FaissSearcher.from_prebuilt_index(index, query_encoder)\n\t    tokenizer = SimpleTokenizer()\n\t    for split in ('train', 'dev', 'test'):\n", "        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n\t        if results_path_prefix is not None:\n\t            with open(results_path_prefix + split, 'r') as f:\n\t                for i, line in enumerate(tqdm(f, desc='generating retrieval list')):\n\t                    qid, pid, rank, score = line.rstrip().split('\\t')\n\t                    assert qid in data\n\t                    if 'retrieved_ctxs' in data[qid]:\n\t                        data[qid]['retrieved_ctxs'].append(pid)\n\t                    else:\n", "                        data[qid]['retrieved_ctxs'] = [pid]\n\t        else:\n\t            batch_size, threads = 36, 30\n\t            qids = list(data.keys())\n\t            qs = [data[qid]['question'] for qid in qids]\n\t            batch_qids, batch_qs = list(), list()\n\t            for index, (qid, q) in enumerate(tqdm(zip(qids, qs), desc='generating retrieval list')):\n\t                batch_qids.append(qid)\n\t                batch_qs.append(q)\n\t                if (index + 1) % batch_size == 0 or index == len(qids) - 1:\n", "                    batch_hits = searcher.batch_search(batch_qs, batch_qids, k=100, threads=threads)\n\t                    assert len(batch_qids) == len(batch_hits)\n\t                    for qid_in_batch, hits in batch_hits.items():\n\t                        data[qid_in_batch]['retrieved_ctxs'] = [hit.docid for hit in hits]\n\t                    batch_qids.clear()\n\t                    batch_qs.clear()\n\t                else:\n\t                    continue\n\t        for qid in tqdm(data, desc='computing has_answer'):\n\t            answers = data[qid]['answers']\n", "            data[qid]['has_answer'] = []\n\t            for docid in data[qid]['retrieved_ctxs']:\n\t                text = json.loads(searcher.doc(docid).raw())['contents'].split('\\n')[1]\n\t                has_answer = has_answers(text, answers, tokenizer)\n\t                data[qid]['has_answer'].append(has_answer)\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n\t            pickle.dump(data, f)\n\tdef generate_dense_score(index, query_encoder, output_path):\n\t    # query_ctx_dense_score and ctx_ctx_dense_score\n\t    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n", "    simcomputer = DenseSimComputer.from_prebuilt_index(index, query_encoder)\n\t    for split in ('train', 'dev', 'test'):\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n\t        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n\t            for qid in tqdm(data, desc='generating dense scores'):\n\t                question = data[qid]['question']\n\t                retrieved_pids = data[qid]['retrieved_ctxs']\n\t                query_ctx_dense_score, ctx_ctx_dense_score = simcomputer.compute(question, retrieved_pids)\n\t                if qid not in f.keys():\n", "                    f.create_group(qid)\n\t                f[qid]['query_ctx_dense_score'] = query_ctx_dense_score.astype(float)\n\t                f[qid]['ctx_ctx_dense_score'] = ctx_ctx_dense_score.astype(float)\n\tdef generate_bm25_score(lucene_index_path, output_path):\n\t    # query_ctx_bm25score and ctx_ctx_bm25score\n\t    simcomputer = BM25SimComputerCached(lucene_index_path)\n\t    for split in ('train', 'dev', 'test'):\n\t        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n\t            data = pickle.load(f)\n\t        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n", "            for qid in tqdm(data, desc='generating BM25 scores'):\n\t                question = data[qid]['question']\n\t                retrieved_pids = data[qid]['retrieved_ctxs']\n\t                query_ctx_bm25_score, ctx_ctx_bm25_score = simcomputer.compute(question, retrieved_pids)\n\t                if qid not in f.keys():\n\t                    f.create_group(qid)\n\t                f[qid]['query_ctx_bm25_score'] = query_ctx_bm25_score.astype(float)\n\t                f[qid]['ctx_ctx_bm25_score'] = ctx_ctx_bm25_score.astype(float)\n\tretriever_dense_index_map = {\n\t    'DPR-Multi': 'wikipedia-dpr-multi-bf',\n", "    'DPR-Single': 'wikipedia-dpr-single-nq-bf',\n\t    'ANCE': 'wikipedia-ance-multi-bf',\n\t    'FiD-KD': 'wikipedia-dpr-dkrr-nq',\n\t    'RocketQA-retriever': 'wikipedia-dpr-dkrr-nq',\n\t    'RocketQAv2-retriever': 'wikipedia-dpr-dkrr-nq',\n\t    'RocketQA-reranker': 'wikipedia-dpr-dkrr-nq',\n\t    'RocketQAv2-reranker': 'wikipedia-dpr-dkrr-nq',\n\t}\n\tretriever_query_encoder_map = {\n\t    'DPR-Multi': 'facebook/dpr-question_encoder-multiset-base',\n", "    'DPR-Single': 'facebook/dpr-question_encoder-single-nq-base',\n\t    'ANCE': 'castorini/ance-dpr-question-multi',\n\t    'FiD-KD': 'castorini/dkrr-dpr-nq-retriever',\n\t    'RocketQA-retriever': 'castorini/dkrr-dpr-nq-retriever',\n\t    'RocketQAv2-retriever': 'castorini/dkrr-dpr-nq-retriever',\n\t    'RocketQA-reranker': 'castorini/dkrr-dpr-nq-retriever',\n\t    'RocketQAv2-reranker': 'castorini/dkrr-dpr-nq-retriever',\n\t}\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description='Generate NQ dataset.')\n", "    parser.add_argument('--nq_data_path', type=str, required=False, default='raw/NQ/data/retriever',\n\t                        help='Original NQ retriever dataset path.')\n\t    parser.add_argument('--retriever', type=str, required=True,\n\t                        choices=[\"DPR-Multi\", \"DPR-Single\", \"ANCE\", \"FiD-KD\", \"RocketQA-retriever\", \n\t                                 \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n\t                        default=None, help='Which retriever method to use.')\n\t    parser.add_argument('--lucene_index_path', type=str, required=False,\n\t                        default='pyserini/indexes/wikipedia_doc',\n\t                        help='Lucene index path.')\n\t    parser.add_argument('--output_path', type=str, required=False, default=None,\n", "                        help='Generated dataset path.')\n\t    args = parser.parse_args()\n\t    nq_data_path = args.nq_data_path\n\t    dense_index_name = retriever_dense_index_map[args.retriever]\n\t    query_encoder = retriever_query_encoder_map[args.retriever]\n\t    lucene_index_path = args.lucene_index_path\n\t    if args.output_path is None:\n\t        output_path = 'NQ_' + args.retriever\n\t    else:\n\t        output_path = args.output_path\n", "    if 'RocketQA' in args.retriever:\n\t        results_path_prefix = f'RocketQA_baselines/NQ/{args.retriever.split(\"-\")[0]}/res.top100.{args.split}'\n\t        if 'reranker' in args.retriever:\n\t            results_path_prefix = results_path_prefix.replace('res.top100', 'rerank.res.top100')\n\t    else:\n\t        results_path_prefix = None\n\t    os.makedirs(output_path, exist_ok=True)\n\t    generate_positive_list(nq_data_path, output_path)\n\t    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path_prefix)  # 4.5h\n\t    generate_dense_score(dense_index_name, query_encoder, output_path)  # 1h\n", "    generate_bm25_score(lucene_index_path, output_path)  # 4.5h\n"]}
{"filename": "data/convert_NQ_collection_to_jsonl.py", "chunked_list": ["import os\n\timport json\n\timport argparse\n\tfrom tqdm import tqdm\n\tdef convert_collection(args):\n\t    print('Converting collection...')\n\t    with open(args.collection_path, 'r') as f:\n\t        with open(os.path.join(args.output_folder, 'DPR_wikipedia_corpus.jsonl'), 'w') as fw:\n\t            next(f)\n\t            for line in tqdm(f):\n", "                pid, text, title = line.split('\\t')\n\t                contents = title + text[1:-1]\n\t                doc = json.dumps({\"id\": pid, \"contents\": contents})\n\t                fw.write(doc + '\\n')\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser(description='Convert NQ wikipedia tsv passage collection into jsonl files for Anserini.')\n\t    parser.add_argument('--collection-path', required=True, help='Path to NQ wikipedia tsv collection.')\n\t    parser.add_argument('--output-folder', required=True, help='Output folder.')\n\t    args = parser.parse_args()\n\t    if not os.path.exists(args.output_folder):\n", "        os.makedirs(args.output_folder)\n\t    convert_collection(args)\n\t    print('Done!')\n"]}
{"filename": "data/convert_MSMARCO_collection_to_jsonl.py", "chunked_list": ["#\n\t# Pyserini: Python interface to the Anserini IR toolkit built on Lucene\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t# http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t#\n\timport json\n\timport os\n\timport argparse\n\tdef convert_collection(args):\n\t    print('Converting collection...')\n", "    file_index = 0\n\t    with open(args.collection_path, encoding='utf-8') as f:\n\t        for i, line in enumerate(f):\n\t            doc_id, doc_text = line.rstrip().split('\\t')\n\t            if i % args.max_docs_per_file == 0:\n\t                if i > 0:\n\t                    output_jsonl_file.close()\n\t                output_path = os.path.join(args.output_folder, 'docs{:02d}.json'.format(file_index))\n\t                output_jsonl_file = open(output_path, 'w', encoding='utf-8', newline='\\n')\n\t                file_index += 1\n", "            output_dict = {'id': doc_id, 'contents': doc_text}\n\t            output_jsonl_file.write(json.dumps(output_dict) + '\\n')\n\t            if i % 100000 == 0:\n\t                print(f'Converted {i:,} docs, writing into file {file_index}')\n\t    output_jsonl_file.close()\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser(description='Convert MSMARCO tsv passage collection into jsonl files for Anserini.')\n\t    parser.add_argument('--collection-path', required=True, help='Path to MS MARCO tsv collection.')\n\t    parser.add_argument('--output-folder', required=True, help='Output folder.')\n\t    parser.add_argument('--max-docs-per-file', default=1000000, type=int,\n", "                        help='Maximum number of documents in each jsonl file.')\n\t    args = parser.parse_args()\n\t    if not os.path.exists(args.output_folder):\n\t        os.makedirs(args.output_folder)\n\t    convert_collection(args)\n\t    print('Done!')\n"]}
{"filename": "data/RocketQA_baselines/TRECDL/generate_embeddings.py", "chunked_list": ["import os\n\timport sys\n\timport logging\n\timport pickle\n\timport rocketqa\n\tlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\tshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])\n\tshard_num, device_id, batch_size = [int(x) for x in sys.argv[2:5]]\n\tmodel_name, output_folder = sys.argv[5:7]\n\tif sys.argv[1] == 'q':\n", "    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\t    for split in ['2019', '2020']:\n\t        qrels_file = f'../../raw/TRECDL/{split}qrels-pass.txt'\n\t        queries_file = f'../../raw/TRECDL/msmarco-test{split}-queries.tsv'\n\t        qid2q = dict()\n\t        with open(queries_file, 'r') as f:\n\t            for line in f:\n\t                qid, q = line.strip().split('\\t')\n\t                qid2q[qid] = q\n\t        results = []\n", "        with open(qrels_file, 'r') as f:\n\t            lines = [line.strip().split() for line in f.readlines()]\n\t            qids = sorted(list(set([x[0] for x in lines])))\n\t            texts = [qid2q[qid] for qid in qids]\n\t            q_num = len(qids)\n\t            batch_qids, batch_texts = [], []\n\t            for idx, (qid, text) in enumerate(zip(qids, texts)):                \n\t                batch_qids.append(qid)\n\t                batch_texts.append(text)\n\t                if len(batch_texts) == batch_size or idx == q_num - 1:\n", "                    batch_outs = list(model.encode_query(query=batch_texts))\n\t                    assert len(batch_outs) == len(batch_qids)\n\t                    results.extend([(qid, out) for qid, out in zip(batch_qids, batch_outs)])\n\t                    batch_qids, batch_texts = [], []\n\t                    logging.info(\"%d query encoded.\" % idx)\n\t        os.makedirs(output_folder, exist_ok=True)\n\t        with open(f'{output_folder}/q_embed_' + split + '.pkl', 'wb') as f:\n\t            pickle.dump(results, f)\n\telse:\n\t    logging.info(\"Passage already encoded in MSMARCO.\")"]}
{"filename": "data/RocketQA_baselines/TRECDL/rerank.py", "chunked_list": ["import os\n\timport sys\n\tfrom tqdm import tqdm\n\timport rocketqa\n\tmodel_name, output_folder = sys.argv[1], sys.argv[2]\n\tmodel = rocketqa.load_model(model_name, use_cuda=True, device_id=0, batch_size=100)\n\tpid2p = dict()\n\twith open('../../raw/MSMARCO/collection.tsv', 'r') as f:\n\t    next(f)\n\t    for line in tqdm(f, desc='loading passages'):\n", "        pid, text = line.strip().split('\\t')\n\t        pid2p[pid] = text\n\tfor split in ['2019', '2020']:\n\t    queries_file = f'../../raw/TRECDL/msmarco-test{split}-queries.tsv'\n\t    qid2q = dict()\n\t    with open(queries_file, 'r') as f:\n\t        for line in f:\n\t            qid, q = line.strip().split('\\t')\n\t            qid2q[qid] = q\n\t    qid2pids = dict()\n", "    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n\t        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n\t            qid, pid, rank, score = line.strip().split()\n\t            if qid in qid2pids:\n\t                qid2pids[qid].append(pid)\n\t            else:\n\t                qid2pids[qid] = [pid]\n\t    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n\t        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n\t            if len(pids) != 100:\n", "                pids = pids[:100]\n\t                # print(f'retrieval list for question {qid} truncated.')\n\t            query = [qid2q[qid]] * 100\n\t            para = [pid2p[pid] for pid in pids]\n\t            scores = list(model.matching(query=query, para=para))\n\t            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n\t            sorted_pids = [pids[idx] for idx in sorted_index]\n\t            sorted_scores = [scores[idx] for idx in sorted_index]\n\t            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n\t                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))\n"]}
{"filename": "data/RocketQA_baselines/TRECDL/retrieval.py", "chunked_list": ["import os\n\timport sys\n\timport glob\n\timport pickle\n\timport logging\n\timport numpy as np\n\timport faiss\n\tlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\ttopk = 100\n\tmodel_name, output_folder = sys.argv[1], sys.argv[2]\n", "os.makedirs(output_folder, exist_ok=True)\n\tindex_path = f'../MSMARCO/{output_folder}/index'\n\tindex_meta_path = f'../MSMARCO/{output_folder}/index_meta'\n\tq_embed_pattern = f'{output_folder}/q_embed_*.pkl'\n\tp_embed_pattern = f'../MSMARCO/{output_folder}/p_embed_*.pkl'\n\toutput_path = f'{output_folder}/res.top100'\n\tif os.path.exists(index_path) and os.path.exists(index_meta_path):\n\t    logging.info('reading index ' + index_path)\n\t    with open(index_meta_path, 'rb') as f:\n\t        ids = pickle.load(f)\n", "    index = faiss.read_index(index_path)\n\t    assert len(ids) == index.ntotal\n\telse:\n\t    ids = []\n\t    index = faiss.IndexFlatIP(768)\n\t    for p_embed_file in glob.glob(p_embed_pattern):\n\t        logging.info('indexing ' + p_embed_file)\n\t        with open(p_embed_file, \"rb\") as f:\n\t            data = pickle.load(f)\n\t        buffer = []\n", "        buffer_size = 50000\n\t        for db_id, doc_vector in data:\n\t            buffer.append((db_id, doc_vector))\n\t            if buffer_size == len(buffer):\n\t                ids.extend([x[0] for x in buffer])\n\t                index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n\t                buffer = []\n\t        if len(buffer) > 0:\n\t            ids.extend([x[0] for x in buffer])\n\t            index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n", "            buffer = []\n\t        logging.info('done. ntotal = ' + str(index.ntotal))\n\t    faiss.write_index(index, index_path)\n\t    with open(index_meta_path, mode='wb') as f:\n\t        pickle.dump(ids, f)\n\tlogging.info(\"indexing done.\")\n\tbuffer_size = 32\n\tfaiss.omp_set_num_threads(30)\n\tfor split in ['2019', '2020']:\n\t    with open(q_embed_pattern.replace('*', split), 'rb') as f:\n", "        data = pickle.load(f)\n\t    new_output_path = output_path + '.' + split\n\t    if os.path.exists(new_output_path):\n\t        os.remove(new_output_path)\n\t    for start_idx in range(0, len(data), buffer_size):\n\t        qids = [x[0] for x in data[start_idx:(start_idx + buffer_size)]]\n\t        qvecs = np.stack([x[1] for x in data[start_idx:(start_idx + buffer_size)]], axis=0)\n\t        scores, indexes = index.search(qvecs, topk)\n\t        retrieved_lists = [[ids[i] for i in query_top_idxs] for query_top_idxs in indexes]\n\t        assert len(scores) == len(retrieved_lists) == len(qids)\n", "        with open(new_output_path, 'a') as f:\n\t            for qid, pid_list, score_list in zip(qids, retrieved_lists, scores):\n\t                for rank, (pid, score) in enumerate(zip(pid_list, score_list), 1):\n\t                    f.write(\"%s\\t%s\\t%d\\t%f\\n\" % (qid, pid, rank, score))\n\t        logging.info(f\"{start_idx} queries searched.\")\n"]}
{"filename": "data/RocketQA_baselines/NQ/generate_embeddings.py", "chunked_list": ["import os\n\timport sys\n\timport logging\n\timport pickle\n\timport rocketqa\n\tlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\tshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])\n\tshard_num, device_id, batch_size = [int(x) for x in sys.argv[2:5]]\n\tmodel_name, output_folder = sys.argv[5:7]\n\tif sys.argv[1] == 'q':\n", "    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\t    for split in ['train', 'dev', 'test']:\n\t        query_file = '../../raw/NQ/data/retriever/qas/nq-' + split + '.csv'\n\t        results = []\n\t        with open(query_file, 'r') as f:\n\t            batch_qids, batch_texts = [], []\n\t            lines = f.readlines()\n\t            line_num = len(lines)\n\t            for idx, line in enumerate(lines):\n\t                query, answers = line.strip().split('\\t')\n", "                batch_qids.append(str(idx))\n\t                batch_texts.append(query)\n\t                if len(batch_texts) == batch_size or idx == line_num - 1:\n\t                    logging.info(\"encoding query %s to %s ...\" % (batch_qids[0], batch_qids[-1]))\n\t                    batch_outs = list(model.encode_query(query=batch_texts))\n\t                    assert len(batch_outs) == len(batch_qids)\n\t                    results.extend([(qid, out) for qid, out in zip(batch_qids, batch_outs)])\n\t                    batch_qids, batch_texts = [], []\n\t        os.makedirs(output_folder, exist_ok=True)\n\t        with open(f'{output_folder}/q_embed_' + split + '.pkl', 'wb') as f:\n", "            pickle.dump(results, f)\n\telse:\n\t    logging.info((shard_id, shard_num, device_id, batch_size))\n\t    assert shard_id < shard_num\n\t    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\t    para_num = 21015324\n\t    shard_size = para_num // shard_num\n\t    start_idx = shard_id * shard_size\n\t    end_idx = para_num + 1 if shard_id == shard_num - 1 else start_idx + shard_size\n\t    corpus_file = '../../raw/NQ/data/wikipedia_split/psgs_w100.tsv'\n", "    results = []\n\t    with open(corpus_file, 'r') as f:\n\t        next(f)\n\t        batch_pids, batch_texts, batch_titles = [], [], []\n\t        for idx, line in enumerate(f):\n\t            if idx >= start_idx and idx < end_idx:\n\t                pid, text, title = line.strip().split('\\t')\n\t                text = text[1:-1]\n\t                batch_pids.append(pid)\n\t                batch_texts.append(text)\n", "                batch_titles.append(title)\n\t                if len(batch_texts) == batch_size or idx == end_idx - 1:\n\t                    logging.info(\"encoding passage %s to %s ...\" % (batch_pids[0], batch_pids[-1]))\n\t                    batch_outs = list(model.encode_para(para=batch_texts, title=batch_titles))\n\t                    assert len(batch_outs) == len(batch_pids)\n\t                    results.extend([(pid, out) for pid, out in zip(batch_pids, batch_outs)])\n\t                    batch_pids, batch_texts, batch_titles = [], [], []\n\t    os.makedirs(output_folder, exist_ok=True)\n\t    with open(f'{output_folder}/p_embed_' + str(shard_id) + '.pkl', 'wb') as f:\n\t        pickle.dump(results, f)"]}
{"filename": "data/RocketQA_baselines/NQ/rerank.py", "chunked_list": ["import os\n\timport sys\n\tfrom tqdm import tqdm\n\timport rocketqa\n\tif sys.argv[1] == 'v1_nq_ce':\n\t    model_name = 'v1_nq_ce'\n\telif sys.argv[1] == 'v2_nq_ce':\n\t    model_name = 'v2_nq_ce/config.json'\n\telse:\n\t    raise NotImplementedError\n", "output_folder = sys.argv[2]\n\tmodel = rocketqa.load_model(model_name, use_cuda=True, device_id=0, batch_size=100)\n\tpid2p = dict()\n\twith open('../../raw/NQ/data/wikipedia_split/psgs_w100.tsv', 'r') as f:\n\t    for line in tqdm(f, desc='loading passages'):\n\t        pid, text, title = line.strip().split('\\t')\n\t        pid2p[pid] = text[1:-1]\n\tfor split in ['train', 'dev', 'test']:\n\t    qid2q = dict()\n\t    with open(os.path.join('../../raw/NQ/data/retriever/qas', 'nq-' + split + '.csv'), 'r') as f:\n", "        for qid, line in enumerate(tqdm(f, desc='loading ' + split + ' queries')):\n\t            query, answers = line.strip().split('\\t')\n\t            qid2q[str(qid)] = query\n\t    qid2pids = dict()\n\t    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n\t        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n\t            qid, pid, rank, score = line.strip().split()\n\t            if qid in qid2pids:\n\t                qid2pids[qid].append(pid)\n\t            else:\n", "                qid2pids[qid] = [pid]\n\t    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n\t        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n\t            assert len(pids) == 100\n\t            query = [qid2q[qid]] * 100\n\t            para = [pid2p[pid] for pid in pids]\n\t            scores = list(model.matching(query=query, para=para))\n\t            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n\t            sorted_pids = [pids[idx] for idx in sorted_index]\n\t            sorted_scores = [scores[idx] for idx in sorted_index]\n", "            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n\t                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))\n"]}
{"filename": "data/RocketQA_baselines/NQ/retrieval.py", "chunked_list": ["import os\n\timport sys\n\timport glob\n\timport pickle\n\timport logging\n\timport numpy as np\n\timport faiss\n\tlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\ttopk = 100\n\tmodel_name, output_folder = sys.argv[1], sys.argv[2]\n", "os.makedirs(output_folder, exist_ok=True)\n\tindex_path = f'{output_folder}/index'\n\tindex_meta_path = f'{output_folder}/index_meta'\n\tq_embed_pattern = f'{output_folder}/q_embed_*.pkl'\n\tp_embed_pattern = f'{output_folder}/p_embed_*.pkl'\n\toutput_path = f'{output_folder}/res.top100'\n\tif os.path.exists(index_path) and os.path.exists(index_meta_path):\n\t    logging.info('reading index ' + index_path)\n\t    with open(index_meta_path, 'rb') as f:\n\t        ids = pickle.load(f)\n", "    index = faiss.read_index(index_path)\n\t    assert len(ids) == index.ntotal\n\telse:\n\t    ids = []\n\t    index = faiss.IndexFlatIP(768)\n\t    for p_embed_file in glob.glob(p_embed_pattern):\n\t        logging.info('indexing ' + p_embed_file)\n\t        with open(p_embed_file, \"rb\") as f:\n\t            data = pickle.load(f)\n\t        buffer = []\n", "        buffer_size = 50000\n\t        for db_id, doc_vector in data:\n\t            buffer.append((db_id, doc_vector))\n\t            if buffer_size == len(buffer):\n\t                ids.extend([x[0] for x in buffer])\n\t                index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n\t                buffer = []\n\t        if len(buffer) > 0:\n\t            ids.extend([x[0] for x in buffer])\n\t            index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n", "            buffer = []\n\t        logging.info('done. ntotal = ' + str(index.ntotal))\n\t    faiss.write_index(index, index_path)\n\t    with open(index_meta_path, mode='wb') as f:\n\t        pickle.dump(ids, f)\n\tlogging.info(\"indexing done.\")\n\tbuffer_size = 32\n\tfaiss.omp_set_num_threads(30)\n\tfor split in ['train', 'dev', 'test']:\n\t    with open(q_embed_pattern.replace('*', split), 'rb') as f:\n", "        data = pickle.load(f)\n\t    new_output_path = output_path + '.' + split\n\t    if os.path.exists(new_output_path):\n\t        os.remove(new_output_path)\n\t    for start_idx in range(0, len(data), buffer_size):\n\t        qids = [x[0] for x in data[start_idx:(start_idx + buffer_size)]]\n\t        qvecs = np.stack([x[1] for x in data[start_idx:(start_idx + buffer_size)]], axis=0)\n\t        scores, indexes = index.search(qvecs, topk)\n\t        retrieved_lists = [[ids[i] for i in query_top_idxs] for query_top_idxs in indexes]\n\t        assert len(scores) == len(retrieved_lists) == len(qids)\n", "        with open(new_output_path, 'a') as f:\n\t            for qid, pid_list, score_list in zip(qids, retrieved_lists, scores):\n\t                for rank, (pid, score) in enumerate(zip(pid_list, score_list), 1):\n\t                    f.write(\"%s\\t%s\\t%d\\t%f\\n\" % (qid, pid, rank, score))\n\t        logging.info(f\"{start_idx} queries searched.\")\n"]}
{"filename": "data/RocketQA_baselines/MSMARCO/generate_embeddings.py", "chunked_list": ["import os\n\timport sys\n\timport logging\n\timport pickle\n\timport rocketqa\n\tlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\tshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])\n\tshard_num, device_id, batch_size = [int(x) for x in sys.argv[2:5]]\n\tmodel_name, output_folder = sys.argv[5:7]\n\tif sys.argv[1] == 'q':\n", "    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\t    for split in ['train', 'dev']:\n\t        if split == 'train':\n\t            qrels_file = '../../raw/MSMARCO/qrels.train.tsv'\n\t            queries_file = '../../raw/MSMARCO/queries.train.tsv'\n\t        else:\n\t            qrels_file = '../../raw/MSMARCO/qrels.dev.small.tsv'\n\t            queries_file = '../../raw/MSMARCO/queries.dev.small.tsv'\n\t        qid2q = dict()\n\t        with open(queries_file, 'r') as f:\n", "            for line in f:\n\t                qid, q = line.strip().split('\\t')\n\t                qid2q[qid] = q\n\t        results = []\n\t        with open(qrels_file, 'r') as f:\n\t            lines = [line.strip().split('\\t') for line in f.readlines()]\n\t            qids = sorted(list(set([x[0] for x in lines])))\n\t            texts = [qid2q[qid] for qid in qids]\n\t            q_num = len(qids)\n\t            batch_qids, batch_texts = [], []\n", "            for idx, (qid, text) in enumerate(zip(qids, texts)):                \n\t                batch_qids.append(qid)\n\t                batch_texts.append(text)\n\t                if len(batch_texts) == batch_size or idx == q_num - 1:\n\t                    batch_outs = list(model.encode_query(query=batch_texts))\n\t                    assert len(batch_outs) == len(batch_qids)\n\t                    results.extend([(qid, out) for qid, out in zip(batch_qids, batch_outs)])\n\t                    batch_qids, batch_texts = [], []\n\t                    logging.info(\"%d query encoded.\" % idx)\n\t        os.makedirs(output_folder, exist_ok=True)\n", "        with open(f'{output_folder}/q_embed_' + split + '.pkl', 'wb') as f:\n\t            pickle.dump(results, f)\n\telse:\n\t    logging.info((shard_id, shard_num, device_id, batch_size))\n\t    assert shard_id < shard_num\n\t    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\t    corpus_file = '../../raw/MSMARCO/collection.tsv'\n\t    with open(corpus_file) as f:\n\t        para_num = len([idx for idx, _ in enumerate(f)])\n\t    shard_size = para_num // shard_num\n", "    start_idx = shard_id * shard_size\n\t    end_idx = para_num + 1 if shard_id == shard_num - 1 else start_idx + shard_size\n\t    results = []\n\t    with open(corpus_file, 'r') as f:\n\t        next(f)\n\t        batch_pids, batch_texts = [], []\n\t        for idx, line in enumerate(f):\n\t            if idx >= start_idx and idx < end_idx:\n\t                pid, text = line.strip().split('\\t')\n\t                batch_pids.append(pid)\n", "                batch_texts.append(text)\n\t                if len(batch_texts) == batch_size or idx == end_idx - 1:\n\t                    logging.info(\"encoding passage %s to %s ...\" % (batch_pids[0], batch_pids[-1]))\n\t                    batch_outs = list(model.encode_para(para=batch_texts))\n\t                    assert len(batch_outs) == len(batch_pids)\n\t                    results.extend([(pid, out) for pid, out in zip(batch_pids, batch_outs)])\n\t                    batch_pids, batch_texts = [], []\n\t    os.makedirs(output_folder, exist_ok=True)\n\t    with open(f'{output_folder}/p_embed_' + str(shard_id) + '.pkl', 'wb') as f:\n\t        pickle.dump(results, f)"]}
{"filename": "data/RocketQA_baselines/MSMARCO/rerank.py", "chunked_list": ["import os\n\timport sys\n\tfrom tqdm import tqdm\n\timport rocketqa\n\tmodel_name, output_folder = sys.argv[1], sys.argv[2]\n\tmodel = rocketqa.load_model(model_name, use_cuda=True, device_id=0, batch_size=100)\n\tpid2p = dict()\n\twith open('../../raw/MSMARCO/collection.tsv', 'r') as f:\n\t    next(f)\n\t    for line in tqdm(f, desc='loading passages'):\n", "        pid, text = line.strip().split('\\t')\n\t        pid2p[pid] = text\n\tfor split in ['train', 'dev']:\n\t    if split == 'train':\n\t        queries_file = '../../raw/MSMARCO/queries.train.tsv'\n\t    else:\n\t        queries_file = '../../raw/MSMARCO/queries.dev.small.tsv'\n\t    qid2q = dict()\n\t    with open(queries_file, 'r') as f:\n\t        for line in f:\n", "            qid, q = line.strip().split('\\t')\n\t            qid2q[qid] = q\n\t    qid2pids = dict()\n\t    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n\t        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n\t            qid, pid, rank, score = line.strip().split()\n\t            if qid in qid2pids:\n\t                qid2pids[qid].append(pid)\n\t            else:\n\t                qid2pids[qid] = [pid]\n", "    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n\t        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n\t            if len(pids) != 100:\n\t                pids = pids[:100]\n\t                # print(f'retrieval list for question {qid} truncated.')\n\t            query = [qid2q[qid]] * 100\n\t            para = [pid2p[pid] for pid in pids]\n\t            scores = list(model.matching(query=query, para=para))\n\t            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n\t            sorted_pids = [pids[idx] for idx in sorted_index]\n", "            sorted_scores = [scores[idx] for idx in sorted_index]\n\t            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n\t                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))\n"]}
{"filename": "data/RocketQA_baselines/MSMARCO/retrieval.py", "chunked_list": ["import os\n\timport sys\n\timport glob\n\timport pickle\n\timport logging\n\timport numpy as np\n\timport faiss\n\tlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\ttopk = 100\n\tmodel_name, output_folder = sys.argv[1], sys.argv[2]\n", "os.makedirs(output_folder, exist_ok=True)\n\tindex_path = f'{output_folder}/index'\n\tindex_meta_path = f'{output_folder}/index_meta'\n\tq_embed_pattern = f'{output_folder}/q_embed_*.pkl'\n\tp_embed_pattern = f'{output_folder}/p_embed_*.pkl'\n\toutput_path = f'{output_folder}/res.top100'\n\tif os.path.exists(index_path) and os.path.exists(index_meta_path):\n\t    logging.info('reading index ' + index_path)\n\t    with open(index_meta_path, 'rb') as f:\n\t        ids = pickle.load(f)\n", "    index = faiss.read_index(index_path)\n\t    assert len(ids) == index.ntotal\n\telse:\n\t    ids = []\n\t    index = faiss.IndexFlatIP(768)\n\t    for p_embed_file in glob.glob(p_embed_pattern):\n\t        logging.info('indexing ' + p_embed_file)\n\t        with open(p_embed_file, \"rb\") as f:\n\t            data = pickle.load(f)\n\t        buffer = []\n", "        buffer_size = 50000\n\t        for db_id, doc_vector in data:\n\t            buffer.append((db_id, doc_vector))\n\t            if buffer_size == len(buffer):\n\t                ids.extend([x[0] for x in buffer])\n\t                index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n\t                buffer = []\n\t        if len(buffer) > 0:\n\t            ids.extend([x[0] for x in buffer])\n\t            index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n", "            buffer = []\n\t        logging.info('done. ntotal = ' + str(index.ntotal))\n\t    faiss.write_index(index, index_path)\n\t    with open(index_meta_path, mode='wb') as f:\n\t        pickle.dump(ids, f)\n\tlogging.info(\"indexing done.\")\n\tbuffer_size = 32\n\tfaiss.omp_set_num_threads(30)\n\tfor split in ['train', 'dev']:\n\t    with open(q_embed_pattern.replace('*', split), 'rb') as f:\n", "        data = pickle.load(f)\n\t    new_output_path = output_path + '.' + split\n\t    if os.path.exists(new_output_path):\n\t        os.remove(new_output_path)\n\t    for start_idx in range(0, len(data), buffer_size):\n\t        qids = [x[0] for x in data[start_idx:(start_idx + buffer_size)]]\n\t        qvecs = np.stack([x[1] for x in data[start_idx:(start_idx + buffer_size)]], axis=0)\n\t        scores, indexes = index.search(qvecs, topk)\n\t        retrieved_lists = [[ids[i] for i in query_top_idxs] for query_top_idxs in indexes]\n\t        assert len(scores) == len(retrieved_lists) == len(qids)\n", "        with open(new_output_path, 'a') as f:\n\t            for qid, pid_list, score_list in zip(qids, retrieved_lists, scores):\n\t                for rank, (pid, score) in enumerate(zip(pid_list, score_list), 1):\n\t                    f.write(\"%s\\t%s\\t%d\\t%f\\n\" % (qid, pid, rank, score))\n\t        logging.info(f\"{start_idx} queries searched.\")\n"]}
