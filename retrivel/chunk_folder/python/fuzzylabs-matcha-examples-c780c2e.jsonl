{"filename": "llm/run.py", "chunked_list": ["\"\"\"Run the LLM finetuning and deployment pipeline.\"\"\"\n\timport click\n\tfrom zenml.logger import get_logger\n\tfrom steps.finetune_model import finetune_model\n\tfrom steps.get_hg_model import get_huggingface_model\n\tfrom steps.download_data_step import download_dataset\n\tfrom steps.convert_to_hg_dataset_step import convert_to_hg_dataset\n\tfrom steps.preprocess_hg_dataset_step import preprocess_dataset\n\tfrom steps.deploy_model_step import seldon_llm_custom_deployment\n\tfrom steps.fetch_trained_model_step import fetch_model\n", "from pipelines.llm_deployment_pipeline import llm_deployment_pipeline\n\tfrom pipelines.llm_pipeline import llm_pipeline\n\tlogger = get_logger(__name__)\n\tdef run_llm_pipeline():\n\t    \"\"\"Run all steps in the llm finetuning pipeline.\"\"\"\n\t    pipeline = llm_pipeline(\n\t        download_dataset(),\n\t        convert_to_hg_dataset(),\n\t        get_huggingface_model(),\n\t        preprocess_dataset(),\n", "        finetune_model(),\n\t    )\n\t    pipeline.run(config_path=\"pipelines/config_llm_pipeline.yaml\")\n\tdef run_llm_deploy_pipeline():\n\t    \"\"\"Run all steps in llm deploy pipeline.\"\"\"\n\t    pipeline = llm_deployment_pipeline(fetch_model(), seldon_llm_custom_deployment)\n\t    pipeline.run(config_path=\"pipelines/config_llm_deployment_pipeline.yaml\")\n\t@click.command()\n\t@click.option(\"--train\", \"-t\", is_flag=True, help=\"Run training pipeline\")\n\t@click.option(\"--deploy\", \"-d\", is_flag=True, help=\"Run the deployment pipeline\")\n", "def main(train: bool, deploy: bool):\n\t    \"\"\"Run all pipelines.\n\t    args:\n\t        train (bool): Flag for running the training pipeline.\n\t        deploy (bool): Flag for running the deployment pipeline.\n\t    \"\"\"\n\t    if train:\n\t        \"\"\"Run all pipelines.\"\"\"\n\t        logger.info(\"Running LLM fine-tuning pipeline.\")\n\t        run_llm_pipeline()\n", "    if deploy:\n\t        logger.info(\"Running LLM deployment pipeline.\")\n\t        run_llm_deploy_pipeline()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "llm/steps/finetune_model.py", "chunked_list": ["\"\"\"LLM finetuning step.\"\"\"\n\tfrom typing import Tuple\n\timport torch\n\tfrom zenml.steps import BaseParameters, step, Output\n\tfrom transformers import (\n\t    DataCollatorForSeq2Seq,\n\t    Seq2SeqTrainingArguments,\n\t    Seq2SeqTrainer,\n\t    PreTrainedTokenizerBase,\n\t    PreTrainedModel,\n", ")\n\tfrom datasets import Dataset\n\tclass TuningParameters(BaseParameters):\n\t    \"\"\"Fine-tuning step parameters.\"\"\"\n\t    # Learning rate\n\t    learning_rate: float\n\t    # Weight decay\n\t    weight_decay: float\n\t    # Use CUDA for training\n\t    use_cuda: bool\n", "    # Batch size per device for training\n\t    per_device_train_batch_size: int\n\t    # Batch size per device for evaluation\n\t    per_device_eval_batch_size: int\n\t    # Number of epochs to run tuning for\n\t    epochs: int\n\t    # Load the best checkpoint at the end flag\n\t    load_best_model_at_end: bool\n\tdef prepare_training_args(params: TuningParameters) -> Seq2SeqTrainingArguments:\n\t    \"\"\"Prepare training arguments.\n", "    Args:\n\t        params (TuningParameters): tuning step parameters\n\t    Returns:\n\t        Seq2SeqTrainingArguments: training arguments\n\t    \"\"\"\n\t    use_cuda = torch.cuda.is_available() if params.use_cuda else False\n\t    training_args = Seq2SeqTrainingArguments(\n\t        output_dir=\"model\",  # can be hardcoded to anything, since we are not using this directory afterwards\n\t        learning_rate=params.learning_rate,\n\t        evaluation_strategy=\"epoch\",\n", "        per_device_train_batch_size=params.per_device_train_batch_size,\n\t        per_device_eval_batch_size=params.per_device_eval_batch_size,\n\t        weight_decay=params.weight_decay,\n\t        save_total_limit=1,\n\t        save_strategy=\"epoch\",\n\t        num_train_epochs=params.epochs,\n\t        predict_with_generate=True,\n\t        no_cuda=not use_cuda,\n\t        fp16=use_cuda,\n\t        load_best_model_at_end=params.load_best_model_at_end,\n", "    )\n\t    return training_args\n\tdef train(\n\t    tokenizer: PreTrainedTokenizerBase,\n\t    model: PreTrainedModel,\n\t    data: Dataset,\n\t    training_args: Seq2SeqTrainingArguments,\n\t) -> Tuple[PreTrainedTokenizerBase, PreTrainedModel]:\n\t    \"\"\"Perform sequence to sequence training.\n\t    Args:\n", "        tokenizer (PreTrainedTokenizerBase): Huggingface tokenizer to be used in fine-tuning.\n\t        model (PreTrainedModel): Huggingface pre-trained model to fine-tune\n\t        data (Dataset): dataset to fine-tune with\n\t        training_args (Seq2SeqTrainingArguments): training arguments\n\t    Returns:\n\t        Tuple[PreTrainedTokenizerBase, PreTrainedModel]: fine-tuned tokenizer and model\n\t    Raises:\n\t        Exception: when tokenizer or model is missing from the trainer\n\t    \"\"\"\n\t    data_collator = DataCollatorForSeq2Seq(\n", "        tokenizer=tokenizer, model=model.name_or_path\n\t    )\n\t    trainer = Seq2SeqTrainer(\n\t        model=model,\n\t        args=training_args,\n\t        train_dataset=data[\"train\"],\n\t        eval_dataset=data[\"test\"],\n\t        tokenizer=tokenizer,\n\t        data_collator=data_collator,\n\t    )\n", "    trainer.train()\n\t    if trainer.tokenizer is None:\n\t        raise Exception(\"Trainer's tokenizer is None\")\n\t    if trainer.model is None:\n\t        raise Exception(\"Trainer's model is None\")\n\t    return trainer.tokenizer, trainer.model\n\t@step\n\tdef finetune_model(\n\t    params: TuningParameters,\n\t    tokenizer: PreTrainedTokenizerBase,\n", "    model: PreTrainedModel,\n\t    data: Dataset,\n\t) -> Output(tokenizer=PreTrainedTokenizerBase, model=PreTrainedModel):\n\t    \"\"\"A step to fine-tune a pre-trained model.\n\t    Args:\n\t        params (TuningParameters): Tuning parameters\n\t        tokenizer (PreTrainedTokenizerBase): A pre-trained tokenizer\n\t        model (PreTrainedModel): A pre-trained model\n\t        data (Dataset): A dataset to fine-tune with\n\t    Returns:\n", "        PreTrainedTokenizerBase: a tuned tokenizer\n\t        PreTrainedModel: a tuned model\n\t    \"\"\"\n\t    training_args = prepare_training_args(params)\n\t    tuned_tokenizer, tuned_model = train(tokenizer, model, data, training_args)\n\t    return tuned_tokenizer, tuned_model\n"]}
{"filename": "llm/steps/preprocess_hg_dataset_step.py", "chunked_list": ["\"\"\"Preprocess, tokenize and split the huggingface dataset into train/test set.\"\"\"\n\tfrom functools import partial\n\tfrom zenml.logger import get_logger\n\tfrom zenml.steps import step, BaseParameters\n\tfrom datasets import Dataset, DatasetDict\n\tfrom transformers import BatchEncoding\n\tfrom transformers import PreTrainedTokenizerBase\n\tlogger = get_logger(__name__)\n\tclass PreprocessParameters(BaseParameters):\n\t    \"\"\"Parameters for preprocessing the Huggingface dataset.\"\"\"\n", "    # Prefix to be added to the input (required for T5 LLM family)\n\t    prefix: str = \"summarize: \"\n\t    # Max length of the input text\n\t    input_max_length: int = 4096\n\t    # Max length of the target summary\n\t    target_max_length: int = 512\n\t    # Split ratio for train/test\n\t    test_size: float = 0.2\n\tdef preprocess_function(\n\t    dataset: Dataset,\n", "    tokenizer: PreTrainedTokenizerBase,\n\t    prefix: str,\n\t    input_max_length: int,\n\t    target_max_length: int,\n\t) -> BatchEncoding:\n\t    \"\"\"Preprocess and tokenize the huggingface dataset.\n\t    Args:\n\t        dataset (Dataset): Dataset to preprocess and tokenize.\n\t        tokenizer (str): Huggingface tokenizer.\n\t        prefix (str): Prefix to add so that T5 model knows this is a summarization task.\n", "        input_max_length (int): Max length of the input text. Truncate sequences to be no longer than this length.\n\t        target_max_length (int): Max length of the target summary. Truncate sequences to be no longer than this length.\n\t    Returns:\n\t        BatchEncoding: Tokenized input and targets.\n\t    \"\"\"\n\t    # Preprocess input by adding the prefix so T5 knows this is a summarization task.\n\t    inputs = [prefix + doc for doc in dataset[\"text\"]]\n\t    # Tokenize input and target\n\t    model_inputs = tokenizer(inputs, max_length=input_max_length, truncation=True)\n\t    labels = tokenizer(\n", "        text_target=dataset[\"summary\"], max_length=target_max_length, truncation=True\n\t    )\n\t    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\t    return model_inputs\n\t@step\n\tdef preprocess_dataset(\n\t    dataset: Dataset, tokenizer: PreTrainedTokenizerBase, params: PreprocessParameters\n\t) -> DatasetDict:\n\t    \"\"\"Preprocess the huggingface dataset.\n\t    Args:\n", "        dataset (Dataset): Dataset to preprocess, tokenize and split.\n\t        tokenizer (str): Huggingface tokenizer.\n\t        params (PreprocessParameters): Parameters for preprocessing the dataset.\n\t    Returns:\n\t        DatasetDict: Tokenized dataset split into train and test.\n\t    \"\"\"\n\t    # Tokenize and preprocess dataset\n\t    tokenized_data = dataset.map(\n\t        partial(\n\t            preprocess_function,\n", "            tokenizer=tokenizer,\n\t            prefix=params.prefix,\n\t            input_max_length=params.input_max_length,\n\t            target_max_length=params.target_max_length,\n\t        ),\n\t        batched=True,\n\t    )\n\t    # Split into train and test\n\t    tokenized_data = tokenized_data.train_test_split(test_size=params.test_size)\n\t    logger.info(f\"Number of examples in training set: {len(tokenized_data['train'])}\")\n", "    logger.info(f\"Number of examples in test set: {len(tokenized_data['test'])}\")\n\t    return tokenized_data\n"]}
{"filename": "llm/steps/zenml_llm_custom_model.py", "chunked_list": ["# Derived from zenml seldon integration; source : https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/seldon/custom_deployer/zenml_custom_model.py\n\t\"\"\"Zenml Custom LLM Class\"\"\"\n\tfrom typing import Any, Dict, List, Union, Optional\n\timport subprocess\n\timport numpy as np\n\timport click\n\timport os\n\tfrom zenml.logger import get_logger\n\tfrom zenml.utils.source_utils import import_class_by_path\n\tfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n", "DEFAULT_MODEL_NAME = \"models\"\n\tDEFAULT_LOCAL_MODEL_DIR = \"/mnt/models\"\n\tlogger = get_logger(__name__)\n\tArray_Like = Union[np.ndarray, List[Any], str, bytes, Dict[str, Any]]\n\tDEFAULT_PT_MODEL_DIR = \"hf_pt_model\"\n\tDEFAULT_TOKENIZER_DIR = \"hf_tokenizer\"\n\tclass ZenMLCustomLLMModel:\n\t    \"\"\"Custom model class for ZenML and Seldon for LLM.\n\t    This class is used to implement a custom model for the Seldon Core integration,\n\t    which is used as the main entry point for custom code execution.\n", "    Attributes:\n\t        model_uri: The URI of the model.\n\t        tokenizer_uri: The URI of the tokenizer.\n\t        model_name: The name of the model.\n\t        predict_func: The predict function of the model.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        model_uri: str,\n\t        tokenizer_uri: str,\n", "        model_name: str,\n\t        predict_func: str,\n\t    ):\n\t        \"\"\"Initializes a ZenMLCustomModel object.\n\t        Args:\n\t            model_uri (str): The URI of the model.\n\t            tokenizer_uri (str): The URI of tokenizer.\n\t            model_name (str): The name of the model.\n\t            predict_func (str): The predict function of the model.\n\t        \"\"\"\n", "        self.name = model_name\n\t        self.model_uri = model_uri\n\t        self.tokenizer_uri = tokenizer_uri\n\t        self.predict_func = import_class_by_path(predict_func)\n\t        self.model = None\n\t        self.tokenizer = None\n\t        self.ready = False\n\t    def load(self) -> bool:\n\t        \"\"\"Load the model.\n\t        This function loads the model into memory and sets the ready flag to True.\n", "        The model is loaded using the materializer, by saving the information of\n\t        the artifact to a file at the preparing time and loading it again at the\n\t        prediction time by the materializer.\n\t        Returns:\n\t            bool: True if the model was loaded successfully, False otherwise.\n\t        \"\"\"\n\t        try:\n\t            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n\t                os.path.join(self.model_uri, DEFAULT_PT_MODEL_DIR)\n\t            )\n", "            self.tokenizer = AutoTokenizer.from_pretrained(\n\t                os.path.join(self.tokenizer_uri, DEFAULT_TOKENIZER_DIR)\n\t            )\n\t        except Exception as e:\n\t            logger.error(\"Failed to load model: {}\".format(e))\n\t            return False\n\t        self.ready = True\n\t        return self.ready\n\t    def predict(\n\t        self,\n", "        X: Array_Like,\n\t        features_names: Optional[List[str]],\n\t        **kwargs: Any,\n\t    ) -> Array_Like:\n\t        \"\"\"Predict the given request.\n\t        The main predict function of the model. This function is called by the\n\t        Seldon Core server when a request is received. Then inside this function,\n\t        the user-defined predict function is called.\n\t        Args:\n\t            X (Array_Like): The request to predict in a dictionary.\n", "            features_names (Optional[List[str]]): The names of the features.\n\t            **kwargs (Any): Additional arguments.\n\t        Returns:\n\t            Array_Like: The prediction dictionary.\n\t        Raises:\n\t            Exception: If function could not be called.\n\t            NotImplementedError: If the model is not ready.\n\t            TypeError: If the request is not a dictionary.\n\t        \"\"\"\n\t        if self.predict_func is not None:\n", "            try:\n\t                prediction = {\n\t                    \"predictions\": self.predict_func(self.model, self.tokenizer, X)\n\t                }\n\t            except Exception as e:\n\t                raise Exception(\"Failed to predict: {}\".format(e))\n\t            if isinstance(prediction, dict):\n\t                return prediction\n\t            else:\n\t                raise TypeError(\n", "                    f\"Prediction is not a dictionary. Expected dict type but got {type(prediction)}\"\n\t                )\n\t        else:\n\t            raise NotImplementedError(\"Predict function is not implemented\")\n\t@click.command()\n\t@click.option(\n\t    \"--model_uri\",\n\t    default=DEFAULT_LOCAL_MODEL_DIR,\n\t    type=click.STRING,\n\t    help=\"The directory where the model is stored locally.\",\n", ")\n\t@click.option(\n\t    \"--tokenizer_uri\",\n\t    default=DEFAULT_LOCAL_MODEL_DIR,\n\t    type=click.STRING,\n\t    help=\"The directory where the model is stored locally.\",\n\t)\n\t@click.option(\n\t    \"--model_name\",\n\t    default=DEFAULT_MODEL_NAME,\n", "    required=True,\n\t    type=click.STRING,\n\t    help=\"The name of the model to deploy.\",\n\t)\n\t@click.option(\n\t    \"--predict_func\",\n\t    required=True,\n\t    type=click.STRING,\n\t    help=\"The path to the custom predict function defined by the user.\",\n\t)\n", "def main(\n\t    model_name: str, model_uri: str, tokenizer_uri: str, predict_func: str\n\t) -> None:\n\t    \"\"\"Main function for the custom model.\n\t    Within the deployment process, the built-in custom deployment step is used to\n\t    to prepare the Seldon Core deployment with an entry point that calls this script,\n\t    which then starts a subprocess to start the Seldon server and waits for requests.\n\t    The following is an example of the entry point:\n\t    ```\n\t    entrypoint_command = [\n", "        \"python\",\n\t        \"-m\",\n\t        \"zenml.integrations.seldon.custom_deployer.zenml_custom_model\",\n\t        \"--model_name\",\n\t        config.service_config.model_name,\n\t        \"--predict_func\",\n\t        config.custom_deploy_parameters.predict_function,\n\t    ]\n\t    ```\n\t    Args:\n", "        model_name: The name of the model.\n\t        model_uri: The URI of the model.\n\t        tokenizer_uri: The URI of tokenizer.\n\t        predict_func: The path to the predict function.\n\t    \"\"\"\n\t    command = [\n\t        \"seldon-core-microservice\",\n\t        \"steps.zenml_llm_custom_model.ZenMLCustomLLMModel\",\n\t        \"--service-type\",\n\t        \"MODEL\",\n", "        \"--parameters\",\n\t        (\n\t            f'[{{\"name\":\"model_uri\",\"value\":\"{model_uri}\",\"type\":\"STRING\"}},'\n\t            f'{{\"name\":\"tokenizer_uri\",\"value\":\"{tokenizer_uri}\",\"type\":\"STRING\"}},'\n\t            f'{{\"name\":\"model_name\",\"value\":\"{model_name}\",\"type\":\"STRING\"}},'\n\t            f'{{\"name\":\"predict_func\",\"value\":\"{predict_func}\",\"type\":\"STRING\"}}]'\n\t        ),\n\t    ]\n\t    try:\n\t        subprocess.check_call(command)\n", "    except subprocess.CalledProcessError as ProcessError:\n\t        logger.error(\n\t            f\"Failed to start the seldon-core-microservice process. {ProcessError}\"\n\t        )\n\t        return\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "llm/steps/get_hg_model.py", "chunked_list": ["\"\"\"Get Huggingface model.\"\"\"\n\tfrom zenml.steps import step, Output, BaseParameters\n\tfrom transformers import (\n\t    AutoTokenizer,\n\t    AutoModelForSeq2SeqLM,\n\t    PreTrainedTokenizerBase,\n\t    PreTrainedModel,\n\t)\n\tfrom zenml.logger import get_logger\n\tlogger = get_logger(__name__)\n", "class GetHuggingfaceModelParameters(BaseParameters):\n\t    \"\"\"Parameters for downloading the huggingface model.\"\"\"\n\t    # Huggingface model name\n\t    model_name: str\n\t@step\n\tdef get_huggingface_model(\n\t    params: GetHuggingfaceModelParameters,\n\t) -> Output(tokenizer=PreTrainedTokenizerBase, model=PreTrainedModel):\n\t    \"\"\"A step to get Huggingface model from the hub.\n\t    Args:\n", "        params: step parameters\n\t    Returns:\n\t        PreTrainedTokenizerBase: a pre-trained tokenizer\n\t        PreTrainedModel: a pre-trained model\n\t    \"\"\"\n\t    logger.info(\n\t        f\"Loading model and tokenizer from HuggingFace hub with model_name = {params.model_name}\"\n\t    )\n\t    tokenizer = AutoTokenizer.from_pretrained(params.model_name)\n\t    model = AutoModelForSeq2SeqLM.from_pretrained(params.model_name)\n", "    return tokenizer, model\n"]}
{"filename": "llm/steps/llm_custom_predict.py", "chunked_list": ["from typing import Any, Dict, List, Union\n\timport numpy as np\n\tArray_Like = Union[np.ndarray, List[Any], str, bytes, Dict[str, Any]]\n\tdef custom_predict(\n\t    model: Any,\n\t    tokenizer: Any,\n\t    request: Array_Like,\n\t) -> Array_Like:\n\t    \"\"\"Custom Prediction function for LLM models.\n\t    Request input is in the format: [{\"text\": \"I am a text!\"}]\n", "    where each dictionary is a sample of text to be summarized.\n\t    The custom predict function is the core of the custom deployment, the\n\t    function is called by the custom deployment class defined for the serving\n\t    tool. The current implementation requires the function to get the model\n\t    loaded in the memory and a request with the data to predict.\n\t    Args:\n\t        model (Any): The model to use for prediction.\n\t        tokenizer (Any): The tokenizer to use for prediction.\n\t        request (Array_Like): The request response is an array-like format.\n\t    Returns:\n", "        Array_Like: The prediction in an array-like format.\n\t                    (e.g: np.ndarray, List[Any], str, bytes, Dict[str, Any])\n\t    \"\"\"\n\t    inputs = []\n\t    for instance in request:\n\t        text = instance[\"text\"]\n\t        inp = tokenizer(text, return_tensors=\"pt\").input_ids\n\t        outputs = model.generate(\n\t            inp,\n\t            max_length=300,\n", "            min_length=30,\n\t            length_penalty=2.0,\n\t            num_beams=4,\n\t            no_repeat_ngram_size=3,\n\t            early_stopping=True,\n\t        )\n\t        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\t        inputs.append(pred)\n\t    return inputs\n"]}
{"filename": "llm/steps/__init__.py", "chunked_list": []}
{"filename": "llm/steps/convert_to_hg_dataset_step.py", "chunked_list": ["\"\"\"Convert a dataset to the huggingface dataset format.\"\"\"\n\timport pandas as pd\n\tfrom zenml.logger import get_logger\n\tfrom zenml.steps import step\n\tfrom datasets import Dataset\n\tlogger = get_logger(__name__)\n\t@step\n\tdef convert_to_hg_dataset(data: dict) -> Dataset:\n\t    \"\"\"Convert a dataset to the huggingface dataset format.\n\t    Args:\n", "        data (dict): dataset in `dict` format.\n\t    Returns:\n\t        Dataset: dataset in huggingface `Dataset` format.\n\t    \"\"\"\n\t    # Extract text and summary from dataset\n\t    texts, summaries = [], []\n\t    for _, v in data.items():\n\t        texts.append(v[\"original_text\"])\n\t        summaries.append(v[\"reference_summary\"])\n\t    df = pd.DataFrame({\"text\": texts, \"summary\": summaries})\n", "    dataset = Dataset.from_pandas(df)\n\t    logger.info(\"Total number of examples in dataset: {}\".format(len(dataset)))\n\t    return dataset\n"]}
{"filename": "llm/steps/download_data_step.py", "chunked_list": ["\"\"\"Download summarization dataset for fine-tuning LLM.\"\"\"\n\timport os\n\timport requests\n\timport json\n\tfrom requests.exceptions import HTTPError\n\tfrom zenml.logger import get_logger\n\tfrom zenml.steps import step, BaseParameters\n\tlogger = get_logger(__name__)\n\tDATASET_URL = \"https://raw.githubusercontent.com/lauramanor/legal_summarization/master/all_v1.json\"\n\tclass DownloadDataParams(BaseParameters):\n", "    \"\"\"Parameters for downloading dataset.\"\"\"\n\t    # Path to directory where dataset will be downloaded\n\t    data_dir: str = \"data\"\n\t@step\n\tdef download_dataset(params: DownloadDataParams) -> dict:\n\t    \"\"\"Zenml step to download summarization dataset.\n\t    Args:\n\t        params (DownloadDataParams): Parameters for downloading dataset.\n\t    Returns:\n\t        dict: Dataset in dictionary format.\n", "    Raises:\n\t        Exception: If dataset cannot be downloaded.\n\t    \"\"\"\n\t    data_path = os.path.join(params.data_dir, \"summarization_dataset.json\")\n\t    if os.path.exists(data_path):\n\t        logger.info(f\"Dataset already exists at {data_path}\")\n\t        json_data = open(data_path).read()\n\t        data = json.loads(json_data)\n\t        return data\n\t    else:\n", "        logger.info(\"Downloading dataset\")\n\t        os.makedirs(params.data_dir, exist_ok=True)\n\t        try:\n\t            response = requests.get(DATASET_URL)\n\t            response.raise_for_status()\n\t        except HTTPError as http_err:\n\t            err_msg = f\"HTTP Error: {http_err}\"\n\t            logger.error(err_msg)\n\t            raise Exception(err_msg)\n\t        except Exception as err:\n", "            err_msg = f\"An error occurred: {err}\"\n\t            logger.error(err_msg)\n\t            raise Exception(err_msg)\n\t        else:\n\t            data = response.json()\n\t            with open(data_path, \"w\", encoding=\"utf-8\") as f:\n\t                json.dump(data, f, ensure_ascii=False, indent=4)\n\t            logger.info(f\"Dataset downloaded to {params.data_dir}\")\n\t            return data\n"]}
{"filename": "llm/steps/deploy_model_step.py", "chunked_list": ["\"\"\"Deploy LLM model server using seldon.\"\"\"\n\tfrom zenml.integrations.seldon.seldon_client import SeldonResourceRequirements\n\tfrom zenml.integrations.seldon.services.seldon_deployment import (\n\t    SeldonDeploymentConfig,\n\t)\n\tfrom zenml.integrations.seldon.steps.seldon_deployer import (\n\t    CustomDeployParameters,\n\t    SeldonDeployerStepParameters,\n\t)\n\tfrom steps.seldon_llm_custom_deployer_step import seldon_llm_model_deployer_step\n", "seldon_llm_custom_deployment = seldon_llm_model_deployer_step(\n\t    params=SeldonDeployerStepParameters(\n\t        service_config=SeldonDeploymentConfig(\n\t            model_name=\"seldon-llm-custom-model\",\n\t            replicas=1,\n\t            implementation=\"custom\",\n\t            resources=SeldonResourceRequirements(\n\t                limits={\"cpu\": \"500m\", \"memory\": \"900Mi\"}\n\t            ),\n\t        ),\n", "        timeout=300,\n\t        custom_deploy_parameters=CustomDeployParameters(\n\t            predict_function=\"steps.llm_custom_predict.custom_predict\"\n\t        ),\n\t    )\n\t)\n"]}
{"filename": "llm/steps/fetch_trained_model_step.py", "chunked_list": ["\"\"\"Fetch trained model for deploying.\"\"\"\n\tfrom typing import Optional\n\tfrom zenml.post_execution import PipelineView, get_pipeline\n\tfrom zenml.post_execution.artifact import ArtifactView\n\tfrom zenml.steps import step, BaseParameters, Output\n\tfrom zenml.logger import get_logger\n\tlogger = get_logger(__name__)\n\tclass FetchModelParameters(BaseParameters):\n\t    \"\"\"Parameters for fetch model step.\"\"\"\n\t    # Name of the pipeline to fetch from\n", "    pipeline_name: str\n\t    # Step name to fetch artifact from.\n\t    step_name: str\n\t    # Optional pipeline version\n\t    pipeline_version: Optional[int] = None\n\tdef get_output_from_step(pipeline: PipelineView, step_name: str) -> ArtifactView:\n\t    \"\"\"Fetch output from a step with last completed run in a pipeline.\n\t    Args:\n\t        pipeline (PipelineView): Post-execution pipeline class object.\n\t        step_name (str): Name of step to fetch output from\n", "    Returns:\n\t        ArtifactView: Artifact data\n\t    Raises:\n\t        KeyError: If no step found with given name\n\t        ValueError: If no output found for step\n\t    \"\"\"\n\t    # Get the step with the given name from the last run\n\t    try:\n\t        # Get last completed run of the pipeline\n\t        fetch_last_completed_run = pipeline.get_run_for_completed_step(step_name)\n", "        logger.info(f\"Run used: {fetch_last_completed_run}\")\n\t        # Get the output of the step from last completed run\n\t        fetch_step = fetch_last_completed_run.get_step(step_name)\n\t        logger.info(f\"Step used: {fetch_step}\")\n\t    except KeyError as e:\n\t        logger.error(f\"No step found with name '{step_name}': {e}\")\n\t        raise e\n\t    # Get the model artifacts from the step\n\t    outputs = fetch_step.outputs\n\t    if outputs is None:\n", "        logger.error(f\"No output found for step '{step_name}'\")\n\t        raise ValueError(f\"No output found for step '{step_name}'\")\n\t    return outputs\n\t@step\n\tdef fetch_model(\n\t    params: FetchModelParameters,\n\t) -> Output(model=str, tokenizer=str, decision=bool):\n\t    \"\"\"Step to fetch model artifacts from last run of nft_embedding pipeline.\n\t    Args:\n\t        params (FetchModelParameters): Parameters for fetch model step.\n", "    Returns:\n\t        str: Location of model artifact.\n\t        str: Location of tokenizer artifact.\n\t        bool: Decision to deploy the model.\n\t    Raises:\n\t        ValueError: if the pipeline does not exist\n\t    \"\"\"\n\t    # Fetch pipeline by name\n\t    pipeline: PipelineView = get_pipeline(\n\t        params.pipeline_name, version=params.pipeline_version\n", "    )\n\t    if pipeline is None:\n\t        logger.error(f\"Pipeline '{params.pipeline_name}' does not exist\")\n\t        raise ValueError(f\"Pipeline '{params.pipeline_name}' does not exist\")\n\t    logger.info(f\"Pipeline: {pipeline}\")\n\t    # Fetch the output for step from the pipeline\n\t    outputs = get_output_from_step(pipeline, params.step_name)\n\t    model, tokenizer = outputs[\"model\"], outputs[\"tokenizer\"]\n\t    return model.uri, tokenizer.uri, True\n"]}
{"filename": "llm/steps/seldon_llm_custom_deployer_step.py", "chunked_list": ["# Derived from zenml seldon integration; source : https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/seldon/steps/seldon_deployer.py\n\t\"\"\"Custom zenml deployer step for Seldon LLM.\"\"\"\n\timport os\n\tfrom typing import cast\n\tfrom zenml.environment import Environment\n\tfrom zenml.exceptions import DoesNotExistException\n\tfrom zenml.integrations.seldon.constants import (\n\t    SELDON_CUSTOM_DEPLOYMENT,\n\t    SELDON_DOCKER_IMAGE_KEY,\n\t)\n", "from zenml.integrations.seldon.model_deployers.seldon_model_deployer import (\n\t    SeldonModelDeployer,\n\t)\n\tfrom zenml.integrations.seldon.seldon_client import (\n\t    create_seldon_core_custom_spec,\n\t)\n\tfrom zenml.integrations.seldon.steps.seldon_deployer import SeldonDeployerStepParameters\n\tfrom zenml.integrations.seldon.services.seldon_deployment import (\n\t    SeldonDeploymentService,\n\t)\n", "from zenml.io import fileio\n\tfrom zenml.logger import get_logger\n\tfrom zenml.steps import (\n\t    STEP_ENVIRONMENT_NAME,\n\t    StepEnvironment,\n\t    step,\n\t)\n\tfrom zenml.steps.step_context import StepContext\n\tfrom zenml.utils import io_utils\n\tlogger = get_logger(__name__)\n", "DEFAULT_PT_MODEL_DIR = \"hf_pt_model\"\n\tDEFAULT_TOKENIZER_DIR = \"hf_tokenizer\"\n\tdef copy_artifact(uri: str, filename: str, context: StepContext) -> str:\n\t    \"\"\"Copy an artifact to the output location of the current step.\n\t    Args:\n\t        uri (str): URI of the artifact to copy\n\t        filename (str): filename for the output artifact\n\t        context (StepContext): ZenML step context\n\t    Returns:\n\t        str: URI of the output location\n", "    Raises:\n\t        RuntimeError: if the artifact is not found\n\t    \"\"\"\n\t    served_artifact_uri = os.path.join(context.get_output_artifact_uri(), \"seldon\")\n\t    fileio.makedirs(served_artifact_uri)\n\t    if not fileio.exists(uri):\n\t        raise RuntimeError(f\"Expected artifact was not found at \" f\"{uri}\")\n\t    if io_utils.isdir(uri):\n\t        io_utils.copy_dir(uri, os.path.join(served_artifact_uri, filename))\n\t    else:\n", "        fileio.copy(uri, os.path.join(served_artifact_uri, filename))\n\t    return served_artifact_uri\n\t@step(enable_cache=False, extra={SELDON_CUSTOM_DEPLOYMENT: True})\n\tdef seldon_llm_model_deployer_step(\n\t    deploy_decision: bool,\n\t    model_uri: str,\n\t    tokenizer_uri: str,\n\t    params: SeldonDeployerStepParameters,\n\t    context: StepContext,\n\t) -> SeldonDeploymentService:\n", "    \"\"\"Seldon Core custom model deployer pipeline step for LLM models.\n\t    This step can be used in a pipeline to implement the\n\t    the process required to deploy a custom model with Seldon Core.\n\t    Args:\n\t        deploy_decision (bool): whether to deploy the model or not\n\t        model_uri (str): The URI of huggingface model\n\t        tokenizer_uri (str): The URI of huggingface tokenizer\n\t        params (SeldonDeployerStepParameters): parameters for the deployer step\n\t        context (StepContext): the step context\n\t    Raises:\n", "        ValueError: if the custom deployer is not defined\n\t        DoesNotExistException: if an entity does not exist raise an exception\n\t    Returns:\n\t        SeldonDeploymentService: Seldon Core deployment service\n\t    \"\"\"\n\t    # verify that a custom deployer is defined\n\t    if not params.custom_deploy_parameters:\n\t        raise ValueError(\n\t            \"Custom deploy parameter is required as part of the step configuration this parameter is\",\n\t            \"the path of the custom predict function\",\n", "        )\n\t    # get the active model deployer\n\t    model_deployer = cast(\n\t        SeldonModelDeployer, SeldonModelDeployer.get_active_model_deployer()\n\t    )\n\t    # get pipeline name, step name, run id\n\t    step_env = cast(StepEnvironment, Environment()[STEP_ENVIRONMENT_NAME])\n\t    pipeline_name = step_env.pipeline_name\n\t    run_name = step_env.run_name\n\t    step_name = step_env.step_name\n", "    # update the step configuration with the real pipeline runtime information\n\t    params.service_config.pipeline_name = pipeline_name\n\t    params.service_config.pipeline_run_id = run_name\n\t    params.service_config.pipeline_step_name = step_name\n\t    params.service_config.is_custom_deployment = True\n\t    # fetch existing services with the same pipeline name, step name and\n\t    # model name\n\t    existing_services = model_deployer.find_model_server(\n\t        pipeline_name=pipeline_name,\n\t        pipeline_step_name=step_name,\n", "        model_name=params.service_config.model_name,\n\t    )\n\t    # even when the deploy decision is negative if an existing model server\n\t    # is not running for this pipeline/step, we still have to serve the\n\t    # current model, to ensure that a model server is available at all times\n\t    if not deploy_decision and existing_services:\n\t        logger.info(\n\t            f\"Skipping model deployment because the model quality does not\"\n\t            f\" meet the criteria. Reusing the last model server deployed by step \"\n\t            f\"'{step_name}' and pipeline '{pipeline_name}' for model \"\n", "            f\"'{params.service_config.model_name}'...\"\n\t        )\n\t        service = cast(SeldonDeploymentService, existing_services[0])\n\t        # even when the deployment decision is negative, we still need to start\n\t        # the previous model server if it is no longer running, to ensure that\n\t        # a model server is available at all times\n\t        if not service.is_running:\n\t            service.start(timeout=params.timeout)\n\t        return service\n\t    # entrypoint for starting Seldon microservice deployment for custom model\n", "    entrypoint_command = [\n\t        \"python\",\n\t        \"-m\",\n\t        \"steps.zenml_llm_custom_model\",\n\t        \"--model_name\",\n\t        params.service_config.model_name,\n\t        \"--predict_func\",\n\t        params.custom_deploy_parameters.predict_function,\n\t    ]\n\t    # verify if there is an active stack before starting the service\n", "    if not context.stack:\n\t        raise DoesNotExistException(\n\t            \"No active stack is available. \"\n\t            \"Please make sure that you have registered and set a stack.\"\n\t        )\n\t    image_name = step_env.step_run_info.get_image(key=SELDON_DOCKER_IMAGE_KEY)\n\t    # Copy artifacts\n\t    model_path = os.path.join(model_uri, DEFAULT_PT_MODEL_DIR)\n\t    served_model_uri = copy_artifact(model_path, DEFAULT_PT_MODEL_DIR, context)\n\t    tokenizer_path = os.path.join(tokenizer_uri, DEFAULT_TOKENIZER_DIR)\n", "    copy_artifact(tokenizer_path, DEFAULT_TOKENIZER_DIR, context)\n\t    # prepare the service configuration for the deployment\n\t    service_config = params.service_config.copy()\n\t    service_config.model_uri = served_model_uri\n\t    # create the specification for the custom deployment\n\t    service_config.spec = create_seldon_core_custom_spec(\n\t        model_uri=service_config.model_uri,\n\t        custom_docker_image=image_name,\n\t        secret_name=model_deployer.kubernetes_secret_name,\n\t        command=entrypoint_command,\n", "    )\n\t    # deploy the service\n\t    service = cast(\n\t        SeldonDeploymentService,\n\t        model_deployer.deploy_model(\n\t            service_config, replace=True, timeout=params.timeout\n\t        ),\n\t    )\n\t    logger.info(\n\t        f\"Seldon Core deployment service started and reachable at:\\n\"\n", "        f\"    {service.prediction_url}\\n\"\n\t    )\n\t    return service\n"]}
{"filename": "llm/pipelines/__init__.py", "chunked_list": []}
{"filename": "llm/pipelines/llm_deployment_pipeline.py", "chunked_list": ["\"\"\"Deployment pipeline for the LLM example.\"\"\"\n\tfrom zenml.pipelines import pipeline\n\t@pipeline\n\tdef llm_deployment_pipeline(fetch_trained_model, deploy_model):\n\t    \"\"\"Pipeline to deploy fine-tuned LLM model.\n\t    Args:\n\t        fetch_trained_model : A step to fetch path to trained model and decision to deploy the model.\n\t        deploy_model: A step to deploy LLM model using Seldon.\n\t    \"\"\"\n\t    # Fetch the trained model path, tokenizer path and decision\n", "    model_uri, tokenizer_uri, decision = fetch_trained_model()\n\t    # Deploy the model\n\t    deploy_model(decision, model_uri, tokenizer_uri)\n"]}
{"filename": "llm/pipelines/llm_pipeline.py", "chunked_list": ["\"\"\"Pipeline for the llm finetuning model.\"\"\"\n\tfrom zenml.pipelines import pipeline\n\t@pipeline\n\tdef llm_pipeline(\n\t    download_dataset,\n\t    convert_to_hg_dataset,\n\t    get_huggingface_model,\n\t    preprocess_dataset,\n\t    tune_model,\n\t):\n", "    \"\"\"Pipeline for llm fine-tuning on summarization dataset.\n\t    Args:\n\t        download_dataset: A step to download the summarization dataset.\n\t        convert_to_hg_dataset: A step to convert summarization dataset into\n\t                            huggingface dataset format.\n\t        get_huggingface_model: A step to get pre-trained model from huggingface.\n\t        preprocess_dataset: A step to preprocess, tokenize and split the summarization dataset.\n\t        tune_model: A step to fine-tune a huggingface pre-trained model.\n\t    \"\"\"\n\t    # Download the summarization dataset\n", "    data = download_dataset()\n\t    # Convert dataset into huggingface dataset format\n\t    dataset = convert_to_hg_dataset(data)\n\t    # Get the pre-trained model\n\t    tokenizer, model = get_huggingface_model()\n\t    # Preprocess, tokenize and split dataset\n\t    tokenized_data = preprocess_dataset(dataset, tokenizer)\n\t    # Fine-tune\n\t    tuned_tokenizer, tuned_model = tune_model(tokenizer, model, tokenized_data)\n"]}
{"filename": "llm/tests/__init__.py", "chunked_list": ["\"\"\"Test functions to be imported.\"\"\""]}
{"filename": "llm/tests/test_steps/test_convert_to_hg_dataset.py", "chunked_list": ["\"\"\"Test suite to test the convert_to_hg_dataset step.\"\"\"\n\timport pytest\n\timport datasets\n\tfrom steps.convert_to_hg_dataset_step import convert_to_hg_dataset\n\t@pytest.fixture\n\tdef mock_data() -> dict:\n\t    \"\"\"Mock input dictionary data for step.\n\t    Returns:\n\t        dict: Dictionary with mocked input data\n\t    \"\"\"\n", "    mock_data = {\"data\": {\"original_text\": \"I am a text!\",\n\t                          \"reference_summary\": \"I am a summary!\"}}\n\t    return mock_data\n\tdef test_convert_to_hg_dataset_step(mock_data: dict):\n\t    \"\"\"Test the convert_to_hg_dataset step.\n\t    Args:\n\t        mock_data (dict): Fixture to mock input data\n\t    \"\"\"\n\t    hg_dataset = convert_to_hg_dataset.entrypoint(mock_data)\n\t    expected_features = [\"text\", \"summary\"]\n", "    # Check if the output is a huggingface `Dataset` object\n\t    assert isinstance(hg_dataset, datasets.Dataset)\n\t    # Check if the output has the expected number of features\n\t    assert len(hg_dataset.features) == len(expected_features)\n\t    # Check if the output has the expected features\n\t    assert all([feat in hg_dataset.features for feat in expected_features])\n\t    # Check if the all features are a `tf.Tensor` object\n\t    assert all([isinstance(hg_dataset[feat][0], str) for feat in expected_features])\n"]}
{"filename": "llm/tests/test_steps/test_get_hg_model.py", "chunked_list": ["\"\"\"Tests for get_huggingface_model step.\"\"\"\n\tfrom unittest import mock\n\tfrom transformers import PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n\tfrom steps.get_hg_model import get_huggingface_model, GetHuggingfaceModelParameters\n\timport pytest\n\t@pytest.fixture\n\tdef params() -> GetHuggingfaceModelParameters:\n\t    \"\"\"Mock parameters required for step.\n\t    Returns:\n\t        GetHuggingfaceModelParameters: Parameters for step.\n", "    \"\"\"\n\t    return GetHuggingfaceModelParameters(\n\t        model_name=\"test_model\"\n\t    )\n\tdef test_get_huggingface_model(params: GetHuggingfaceModelParameters):\n\t    \"\"\"Test get_huggingface_model gets pre-trained tokenizer and model\n\t    Args:\n\t        params (GetHuggingfaceModelParameters): test parameters\n\t    \"\"\"\n\t    with mock.patch(\"steps.get_hg_model.AutoTokenizer\") as mock_tokenizer, \\\n", "            mock.patch(\"steps.get_hg_model.AutoModelForSeq2SeqLM\") as mock_model:\n\t        mock_tokenizer.from_pretrained.return_value = PreTrainedTokenizerBase()\n\t        mock_model.from_pretrained.return_value = PreTrainedModel(PretrainedConfig())\n\t        tokenizer, model = get_huggingface_model.entrypoint(params)\n\t        assert isinstance(tokenizer, PreTrainedTokenizerBase)\n\t        assert isinstance(model, PreTrainedModel)\n"]}
{"filename": "llm/tests/test_steps/test_finetune_model.py", "chunked_list": ["\"\"\"Test finetune_model step.\"\"\"\n\tfrom unittest import mock\n\tfrom steps.finetune_model import finetune_model, TuningParameters\n\timport pytest\n\tfrom datasets import Dataset\n\tfrom transformers import PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n\t@pytest.fixture\n\tdef params() -> TuningParameters:\n\t    \"\"\"Mock parameters required for step.\n\t    Returns:\n", "        GetHuggingfaceModelParameters: Parameters for step.\n\t    \"\"\"\n\t    return TuningParameters(\n\t        learning_rate=2e-5,\n\t        weight_decay=0.01,\n\t        use_cuda=False,\n\t        per_device_train_batch_size=2,\n\t        per_device_eval_batch_size=2,\n\t        epochs=5,\n\t        load_best_model_at_end=True\n", "    )\n\t@pytest.fixture\n\tdef test_model() -> PreTrainedModel:\n\t    \"\"\"Get test dummy huggingface model.\n\t    Returns:\n\t        PreTrainedModel: test model\n\t    \"\"\"\n\t    return PreTrainedModel(PretrainedConfig())\n\t@pytest.fixture\n\tdef test_tokenizer() -> PreTrainedTokenizerBase:\n", "    \"\"\"Get test dummy huggingface tokenizer.\n\t    Returns:\n\t        PreTrainedTokenizerBase: test tokenizer\n\t    \"\"\"\n\t    return PreTrainedTokenizerBase()\n\t@pytest.fixture\n\tdef test_dataset() -> Dataset:\n\t    \"\"\"Get empty dataset.\n\t    Returns:\n\t        Dataset: empty dataset\n", "    \"\"\"\n\t    return Dataset.from_dict({\"train\": [], \"test\": []})\n\t@pytest.fixture\n\tdef expected_training_args() -> dict:\n\t    \"\"\"Get expected training arguments.\n\t    Returns:\n\t        dict: expected training arguments\n\t    \"\"\"\n\t    return {\n\t        \"output_dir\": \"model\",\n", "        \"learning_rate\": 2e-5,\n\t        \"evaluation_strategy\": 'epoch',\n\t        \"per_device_train_batch_size\": 2,\n\t        \"per_device_eval_batch_size\": 2,\n\t        \"weight_decay\": 0.01,\n\t        \"save_total_limit\": 1,\n\t        \"save_strategy\": 'epoch',\n\t        \"num_train_epochs\": 5,\n\t        \"predict_with_generate\": True,\n\t        \"no_cuda\": True,\n", "        \"fp16\": False,\n\t        \"load_best_model_at_end\": True,\n\t    }\n\tdef test_finetune_model(\n\t    params: TuningParameters,\n\t    test_tokenizer: PreTrainedTokenizerBase,\n\t    test_model: PreTrainedModel,\n\t    test_dataset: Dataset,\n\t    expected_training_args: dict\n\t):\n", "    \"\"\"Test finetune_model step fine-tunes a provided model.\n\t    Args:\n\t        params (TuningParameters): step parameters\n\t        test_tokenizer (PreTrainedTokenizerBase): test tokenizer\n\t        test_model (PreTrainedModel): test model\n\t        test_dataset (Dataset): test empty dataset\n\t        expected_training_args (dict): dictionary of expected training arguments\n\t    \"\"\"\n\t    with mock.patch(\"steps.finetune_model.Seq2SeqTrainer\") as mock_trainer, \\\n\t            mock.patch(\"steps.finetune_model.Seq2SeqTrainingArguments\") as mock_trainer_args, \\\n", "            mock.patch(\"steps.finetune_model.DataCollatorForSeq2Seq\") as mock_data_collator:\n\t        mock_trainer_instance = mock_trainer.return_value\n\t        mock_trainer_instance.tokenizer = test_tokenizer\n\t        mock_trainer_instance.model = test_model\n\t        tuned_tokenizer, tuned_model = finetune_model.entrypoint(params, test_tokenizer, test_model, test_dataset)\n\t        mock_trainer_args.assert_called_with(**expected_training_args)\n\t        mock_trainer.assert_called_with(\n\t            model=test_model,\n\t            args=mock_trainer_args.return_value,\n\t            train_dataset=test_dataset[\"train\"],\n", "            eval_dataset=test_dataset[\"test\"],\n\t            tokenizer=test_tokenizer,\n\t            data_collator=mock_data_collator.return_value\n\t        )\n\t        assert isinstance(tuned_tokenizer, PreTrainedTokenizerBase)\n\t        assert isinstance(tuned_model, PreTrainedModel)\n"]}
{"filename": "llm/tests/test_steps/test_preprocess_hg_dataset_setp.py", "chunked_list": ["\"\"\"Test suite to test the preprocess_hg_dataset step.\"\"\"\n\timport pytest\n\tfrom types import SimpleNamespace\n\tfrom datasets import Dataset, DatasetDict\n\tfrom transformers import AutoTokenizer, BatchEncoding, PreTrainedTokenizerBase\n\tfrom steps.convert_to_hg_dataset_step import convert_to_hg_dataset\n\tfrom steps.preprocess_hg_dataset_step import preprocess_dataset, preprocess_function\n\t@pytest.fixture\n\tdef get_params() -> dict:\n\t    \"\"\"Mock parameters required for step.\n", "    Returns:\n\t        dict: Parameters for step.\n\t    \"\"\"\n\t    params = SimpleNamespace()\n\t    params.prefix = \"summarize: \"\n\t    params.input_max_length = 128\n\t    params.target_max_length = 128\n\t    params.test_size = 0.1\n\t    return params\n\t@pytest.fixture\n", "def test_tokenizer() -> PreTrainedTokenizerBase:\n\t    return AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\t@pytest.fixture\n\tdef mock_data() -> dict:\n\t    \"\"\"Mock input dictionary data for step.\n\t    Returns:\n\t        dict: Dictionary with mocked input data\n\t    \"\"\"\n\t    mock_data = {\"data\": {\"original_text\": \"I am a text!\",\n\t                          \"reference_summary\": \"I am a summary!\"\n", "                          },\n\t                \"data1\": {\"original_text\": \"I am another text!\",\n\t                         \"reference_summary\": \"I am another summary!\"\n\t                         }          \n\t                }\n\t    return mock_data\n\t@pytest.fixture\n\tdef mock_hf_dataset(mock_data: dict) -> Dataset:\n\t    \"\"\"Fixture to create a mock hugging face dataset.\n\t    This uses convert_to_hg_dataset step to convert dict to Dataset.\n", "    Args:\n\t        mock_data (dict): Mocked input dict.\n\t    Returns:\n\t        Dataset: Mocked huggingface dataset using mocked input dict.\n\t    \"\"\"\n\t    hg_dataset = convert_to_hg_dataset.entrypoint(mock_data)\n\t    return hg_dataset\n\tdef test_preprocess_function(mock_hf_dataset: Dataset, get_params: dict, test_tokenizer: PreTrainedTokenizerBase):\n\t    \"\"\"Test the preprocess_function function.\n\t    Args:\n", "        mock_hf_dataset (Dataset): Fixture to mock huggingface dataset.\n\t        get_params (dict): Parameters required for step.\n\t        test_tokenizer (PreTrainedTokenizerBase): Test tokenizer to use.\n\t    \"\"\"\n\t    tokenized_dataset = preprocess_function(mock_hf_dataset,\n\t                                            test_tokenizer,\n\t                                            get_params.prefix,\n\t                                            get_params.input_max_length,\n\t                                            get_params.target_max_length)\n\t    expected_features = ['input_ids', 'attention_mask', 'labels']\n", "    expected_labels = [27, 183, 3, 9, 9251, 55, 1]\n\t    expected_input_ids = [21603, 10, 27, 183, 3, 9, 1499, 55, 1]\n\t    # Check if the output is a huggingface `BatchEncoding` object\n\t    assert isinstance(tokenized_dataset, BatchEncoding)\n\t    # Check if the output contains the expected features\n\t    assert all([feat in tokenized_dataset.keys() for feat in expected_features])\n\t    # Check if length of each feature is correct\n\t    assert all([len(v) == len(mock_hf_dataset) for _, v in tokenized_dataset.items()])\n\t    # Check if input and labels are correct\n\t    assert tokenized_dataset['input_ids'][0] == expected_input_ids\n", "    assert tokenized_dataset['labels'][0] == expected_labels\n\tdef test_preprocess_dataset_step(mock_hf_dataset: Dataset, get_params: dict, test_tokenizer: PreTrainedTokenizerBase):\n\t    \"\"\"Test the preprocess_dataset step.\n\t    Args:\n\t        mock_hf_dataset (Dataset): Fixture to mock huggingface dataset.\n\t        get_params (dict): Parameters required for step.\n\t        test_tokenizer (PreTrainedTokenizerBase): Test tokenizer to use.\n\t    \"\"\"\n\t    tokenized_dataset = preprocess_dataset.entrypoint(mock_hf_dataset, test_tokenizer, get_params)\n\t    expected_features = ['text', 'summary', 'input_ids', 'attention_mask', 'labels']\n", "    # Check if the output is a huggingface `DatasetDict` object\n\t    assert isinstance(tokenized_dataset, DatasetDict)\n\t    # Check if two sets: train and test are created\n\t    assert sorted(list(tokenized_dataset.keys())) == sorted([\"train\", \"test\"])\n\t    # Check length of train and test set\n\t    assert len(tokenized_dataset[\"train\"]) == 1\n\t    assert len(tokenized_dataset[\"test\"]) == 1\n\t    # Check if the output has the expected number of features for both train and test set\n\t    assert len(tokenized_dataset[\"train\"].features) == len(expected_features)\n\t    assert len(tokenized_dataset[\"test\"].features) == len(expected_features)\n", "    # Check if the output has the expected features for both train and test set\n\t    assert all([feat in tokenized_dataset[\"train\"].features for feat in expected_features])\n\t    assert all([feat in tokenized_dataset[\"test\"].features for feat in expected_features])\n"]}
{"filename": "llm/tests/test_steps/test_download_data_step.py", "chunked_list": ["\"\"\"Test suite to test the download data step.\"\"\"\n\timport pytest\n\tfrom typing import Iterator\n\tfrom types import SimpleNamespace\n\timport tempfile\n\timport os\n\tfrom unittest import mock\n\tfrom requests import HTTPError\n\tfrom steps.download_data_step import download_dataset\n\t@pytest.fixture\n", "def temp_testing_directory() -> Iterator[str]:\n\t    \"\"\"A fixture for creating and removing temporary test directory for storing and moving files.\n\t    Yields:\n\t        str: a path to temporary directory for storing and moving files from tests.\n\t    \"\"\"\n\t    temp_dir = tempfile.TemporaryDirectory()\n\t    # tests are executed at this point\n\t    yield temp_dir.name\n\t    # delete temp folder\n\t    temp_dir.cleanup()\n", "@pytest.fixture\n\tdef get_params(temp_testing_directory: str) -> dict:\n\t    \"\"\"Mock parameters required for step.\n\t    Args:\n\t        temp_testing_directory (str): Path to temporary directory\n\t    Returns:\n\t        dict: Parameters for step.\n\t    \"\"\"\n\t    params = SimpleNamespace()\n\t    params.data_dir = temp_testing_directory\n", "    return params\n\tdef test_download_data_step(get_params: dict):\n\t    \"\"\"Test the download data step.\n\t    Args:\n\t        get_params (dict): Fixture containing paramters for step.\n\t    \"\"\"\n\t    dummy_dict = {'text': 'summary'}\n\t    with mock.patch(\"requests.get\") as mockresponse:\n\t        mockresponse.return_value.status_code = 200\n\t        mockresponse.return_value.json.return_value = dummy_dict\n", "        data = download_dataset.entrypoint(get_params)\n\t        # Check if returned data matches expected data\n\t        assert data == dummy_dict\n\t        # Check if folder is created\n\t        assert os.path.exists(get_params.data_dir)\n\t        # Check if file is created inside folder\n\t        file_path = os.path.join(get_params.data_dir, \"summarization_dataset.json\")\n\t        assert os.path.exists(file_path)\n\tdef test_download_data_step_invalid_url(get_params: dict):\n\t    \"\"\"Test the download data step when invalid url is passed.\n", "    Args:\n\t        get_params (dict): Fixture containing paramters for step.\n\t    \"\"\"\n\t    dummy_dict = {'text': 'summary'}\n\t    with mock.patch(\"requests.get\") as mockresponse:\n\t        mock_req_instance = mockresponse.return_value\n\t        mock_req_instance.status_code = 404\n\t        mock_req_instance.url = \"http://invalid_url\"\n\t        mock_req_instance.json.return_value = dummy_dict\n\t        mock_req_instance.raise_for_status.side_effect = HTTPError()\n", "        with pytest.raises(Exception) as exc_info:\n\t            _ = download_dataset.entrypoint(get_params)\n\t    assert (str(exc_info.value) == \"HTTP Error: \")\n"]}
{"filename": "llm/app/llm_demo.py", "chunked_list": ["\"\"\"Streamlit application.\"\"\"\n\timport os\n\timport requests\n\timport streamlit as st\n\timport json\n\tfrom zenml.integrations.seldon.model_deployers.seldon_model_deployer import (\n\t    SeldonModelDeployer,\n\t)\n\tst.title(\"LLM Legal Text Summarization Demo\")\n\tPIPELINE_NAME = \"llm_deployment_pipeline\"\n", "PIPELINE_STEP = \"deploy_model\"\n\tMODEL_NAME = \"seldon-llm-custom-model\"\n\tBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\t@st.cache_data\n\tdef _get_prediction_endpoint() -> str:\n\t    \"\"\"Get the endpoint for the currently deployed LLM model.\n\t    Returns:\n\t        str: the url endpoint.\n\t    \"\"\"\n\t    try:\n", "        model_deployer = SeldonModelDeployer.get_active_model_deployer()\n\t        deployed_services = model_deployer.find_model_server(\n\t            pipeline_name=PIPELINE_NAME,\n\t            pipeline_step_name=PIPELINE_STEP,\n\t            model_name=MODEL_NAME,\n\t        )\n\t        return deployed_services[0].prediction_url\n\t    except Exception:\n\t        return None\n\tdef _create_payload(input_text: str) -> dict:\n", "    \"\"\"Create a payload from the user input to send to the LLM model.\n\t    Args:\n\t        input_text (str): Input text to summarize.\n\t    Returns:\n\t        dict: the payload to send in the correct format.\n\t    \"\"\"\n\t    return {\"data\": {\"ndarray\": [{\"text\": str(input_text)}]}}\n\tdef _get_predictions(prediction_endpoint: str, payload: dict) -> dict:\n\t    \"\"\"Using the prediction endpont and payload, make a prediction request to the deployed model.\n\t    Args:\n", "        prediction_endpoint (str): the url endpoint.\n\t        payload (dict): the payload to send to the model.\n\t    Returns:\n\t        dict: the predictions from the model.\n\t    \"\"\"\n\t    response = requests.post(\n\t        url=prediction_endpoint,\n\t        data=json.dumps(payload),\n\t        headers={\"Content-Type\": \"application/json\"},\n\t    )\n", "    return json.loads(response.text)[\"jsonData\"][\"predictions\"][0]\n\tdef fetch_summary(seldon_url: str, txt: str) -> str:\n\t    \"\"\"Query seldon endpoint to fetch the summary.\n\t    Args:\n\t        seldon_url (str): Seldon endpoint\n\t        txt (str): Input text to summarize\n\t    Returns:\n\t        str: Summarized text\n\t    \"\"\"\n\t    with st.spinner(\"Applying LLM Magic...\"):\n", "        payload = _create_payload(txt)\n\t        summary_txt = _get_predictions(seldon_url, payload)\n\t    return summary_txt\n\tdef read_examples(file_path: str = \"example.json\") -> dict:\n\t    \"\"\"Read sample examples for LLM summarization demo.\n\t    Args:\n\t        file_path (str, optional): Path to json file. Defaults to 'example.json'.\n\t    Returns:\n\t        dict: Dictionary containing examples.\n\t    \"\"\"\n", "    with open(file_path, \"r\") as myfile:\n\t        data = myfile.read()\n\t    return json.loads(data)\n\tdef switch_examples(data: dict) -> str:\n\t    \"\"\"Switch between different examples.\n\t    Args:\n\t        data (dict): Dictionary containing examples.\n\t    Returns:\n\t        str: Input text to summarize.\n\t    \"\"\"\n", "    pages = [\"Fuzzy Labs\", \"Stack Overflow\", \"ZenML\", \"Your own example\"]\n\t    page = st.radio(\"Test Examples\", pages)\n\t    text = data.get(page, \"\")\n\t    input_text = st.text_area(label=\"Text to summarize\", value=text, height=400)\n\t    return input_text\n\tdef main():\n\t    example_file_path = os.path.join(BASE_DIR, \"example.json\")\n\t    data = read_examples(file_path=example_file_path)\n\t    txt = switch_examples(data)\n\t    result = st.button(label=\"Ready\")\n", "    if result:\n\t        seldon_url = _get_prediction_endpoint()\n\t        if seldon_url is None:\n\t            st.write(\"Hmm, seldon endpoint is not provisioned yet!\")\n\t        else:\n\t            if len(txt) > 0:\n\t                summarized_text = st.text_area(\n\t                    label=\"Summarized Text\",\n\t                    value=fetch_summary(seldon_url, txt),\n\t                    height=200,\n", "                )\n\t            else:\n\t                st.write(\"Hmm, input text is empty.\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "recommendation/run.py", "chunked_list": ["\"\"\"Run the recommendation example pipeline.\"\"\"\n\timport click\n\tfrom steps.load_data_step import load_data\n\tfrom steps.train_step import train\n\tfrom steps.evaluate_step import evaluate\n\tfrom pipelines.recommendation_pipeline import recommendation_pipeline\n\tfrom steps.fetch_model import fetch_model\n\tfrom steps.deployer import seldon_surprise_custom_deployment\n\tfrom steps.deployment_trigger import deployment_trigger\n\tfrom pipelines.deploy_recommendation_pipeline import recommendation_deployment_pipeline\n", "from materializer import SurpriseMaterializer\n\tfrom zenml.logger import get_logger\n\tfrom zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n\tlogger = get_logger(__name__)\n\tdef run_recommendation_pipeline():\n\t    \"\"\"Run all steps in the example pipeline.\"\"\"\n\t    pipeline = recommendation_pipeline(\n\t        load_data().configure(output_materializers=SurpriseMaterializer),\n\t        train().configure(output_materializers=SurpriseMaterializer),\n\t        evaluate(),\n", "    )\n\t    pipeline.run(config_path=\"pipelines/config_recommendation_pipeline.yaml\")\n\t    logger.info(\n\t        f\"Visit: {get_tracking_uri()}\\n \"\n\t        \"To inspect your experiment runs within the mlflow UI.\\n\"\n\t    )\n\tdef run_deployment_pipeline():\n\t    \"\"\"Run all steps in deployment pipeline.\"\"\"\n\t    deploy_pipeline = recommendation_deployment_pipeline(\n\t        fetch_model().configure(output_materializers=SurpriseMaterializer),\n", "        deployment_trigger(),\n\t        deploy_model=seldon_surprise_custom_deployment,\n\t    )\n\t    deploy_pipeline.run(config_path=\"pipelines/config_deploy_recommendation_pipeline.yaml\")\n\t@click.command()\n\t@click.option(\"--train\", \"-t\", is_flag=True, help=\"Run training pipeline\")\n\t@click.option(\"--deploy\", \"-d\", is_flag=True, help=\"Run the deployment pipeline\")\n\tdef main(train: bool, deploy: bool):\n\t    \"\"\"Run all pipelines.\n\t    args:\n", "        train (bool): Flag for running the training pipeline.\n\t        deploy (bool): Flag for running the deployment pipeline.\n\t    \"\"\"\n\t    if train:\n\t        logger.info(\"Running recommendation training pipeline.\")\n\t        run_recommendation_pipeline()\n\t    if deploy:\n\t        logger.info(\"Running deployment pipeline.\")\n\t        run_deployment_pipeline()\n\t    if (not train) and (not deploy):\n", "        logger.info(\"Running recommendation training pipeline.\")\n\t        run_recommendation_pipeline()\n\t        logger.info(\"Running deployment pipeline.\")\n\t        run_deployment_pipeline()\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "recommendation/inference.py", "chunked_list": ["\"\"\"A inference script to query the deployed recommendation model.\"\"\"\n\timport requests\n\timport json \n\timport click\n\tfrom zenml.integrations.seldon.model_deployers.seldon_model_deployer import SeldonModelDeployer\n\tPIPELINE_NAME = 'recommendation_deployment_pipeline'\n\tPIPELINE_STEP = 'deploy_model'\n\tMODEL_NAME = 'seldon-svd-custom-model'\n\tdef _get_prediction_endpoint() -> str:\n\t    \"\"\"Get the endpoint for the currently deployed recommendation model.\n", "    Returns:\n\t        str: the url endpoint.\n\t    \"\"\"\n\t    model_deployer = SeldonModelDeployer.get_active_model_deployer()\n\t    deployed_services = model_deployer.find_model_server(\n\t        pipeline_name=PIPELINE_NAME,\n\t        pipeline_step_name=PIPELINE_STEP,\n\t        model_name=MODEL_NAME\n\t    )\n\t    return deployed_services[0].prediction_url\n", "def _create_payload(user: str, movie: str) -> dict:\n\t    \"\"\"Create a payload from the user input to send to the recommendation model.\n\t    Args:\n\t        user (str): the id of the user.\n\t        movie (str): the id of the movie.\n\t    Returns:\n\t        dict: the payload to send in the correct format.\n\t    \"\"\"\n\t    return {\n\t        \"data\": {\n", "            \"ndarray\": [{\"uid\": str(user), \"iid\": str(movie)}]\n\t        }\n\t    }\n\tdef _get_predictions(prediction_endpoint: str, payload: dict) -> dict:\n\t    \"\"\"Using the prediction endpont and payload, make a prediction request to the deployed model.\n\t    Args:\n\t        prediction_endpoint (str): the url endpoint.\n\t        payload (dict): the payload to send to the model.\n\t    Returns:\n\t        dict: the predictions from the model.\n", "    \"\"\"\n\t    response = requests.post(\n\t        url=prediction_endpoint,\n\t        data=json.dumps(payload),\n\t        headers={'Content-Type': 'application/json'}\n\t    )\n\t    return json.loads(response.text)['jsonData']['predictions']\n\tdef _output(predictions: dict):\n\t    \"\"\"Output the results to the terminal.\n\t    Args:\n", "        predictions (dict): the predictions produced by the deployed model.\n\t    \"\"\"\n\t    for pred in predictions:\n\t        rating = round(pred['est'], 2)\n\t        print(f\"User {pred['uid']} is predicted to give the movie ({pred['iid']}) a rating of: {rating} out of 5.\")\n\t@click.command()\n\t@click.option('--user', default=1, help='the user id.', type=click.IntRange(1, 943))\n\t@click.option('--movie', default=1, help='the movie id.', type=click.IntRange(1, 1682))\n\tdef main(user: int, movie: int):\n\t    \"\"\"The main runner function.\n", "    Args:\n\t        user (int): the user inputted user id.\n\t        movie (int): the user inputted movie id.\n\t    \"\"\"\n\t    endpoint = _get_prediction_endpoint()\n\t    payload = _create_payload(user, movie)\n\t    predictions = _get_predictions(endpoint, payload)\n\t    _output(predictions)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "recommendation/steps/deployment_trigger.py", "chunked_list": ["\"\"\"A step to decide whether to deploy the model or not.\"\"\"\n\tfrom zenml.logger import get_logger\n\tfrom zenml.post_execution import PipelineView, get_pipeline\n\tfrom zenml.post_execution.artifact import ArtifactView\n\tfrom zenml.steps import BaseParameters, Output, step\n\tfrom surprise import SVD\n\tlogger = get_logger(__name__)\n\t@step()\n\tdef deployment_trigger(\n\t    model: SVD,\n", ") -> Output(decision=bool):\n\t    \"\"\"Step to decide whether to deploy the model or not.\n\t    Args:\n\t        model (SVD): Model to check deployment.\n\t    Returns:\n\t        bool: Decision to deploy the model.\n\t    \"\"\"\n\t    return True"]}
{"filename": "recommendation/steps/load_data_step.py", "chunked_list": ["\"\"\"A step to load a movie ratings dataset.\"\"\"\n\timport mlflow\n\tfrom zenml.steps import step, Output, BaseParameters\n\tfrom surprise import Dataset\n\tfrom surprise.model_selection import train_test_split\n\tfrom surprise.trainset import Trainset\n\tclass DataParameters(BaseParameters):\n\t    \"\"\"Load data parameters.\"\"\"\n\t    # The size of test set\n\t    test_size = 0.25\n", "@step()\n\tdef load_data(params: DataParameters) -> Output(trainset=Trainset, testset=list):\n\t    \"\"\"Load the movie len 100k dataset.\n\t    Args:\n\t        params (DataParameters): Parameters for loading data\n\t    Returns:\n\t        trainset: data for training\n\t        testset: data for testing\n\t    \"\"\"\n\t    data = Dataset.load_builtin(\"ml-100k\", prompt=False)\n", "    trainset, testset = train_test_split(data, test_size=params.test_size)\n\t    if mlflow.active_run():\n\t        mlflow.log_param(\"test_size\", params.test_size)\n\t    return trainset, testset"]}
{"filename": "recommendation/steps/deployer.py", "chunked_list": ["\"\"\"Custom Seldon deployer step.\"\"\"\n\tfrom zenml.integrations.seldon.seldon_client import SeldonResourceRequirements\n\tfrom zenml.integrations.seldon.services.seldon_deployment import (\n\t    SeldonDeploymentConfig,\n\t)\n\tfrom zenml.integrations.seldon.steps.seldon_deployer import (\n\t    CustomDeployParameters,\n\t    SeldonDeployerStepParameters,\n\t    seldon_custom_model_deployer_step,\n\t)\n", "seldon_surprise_custom_deployment = seldon_custom_model_deployer_step(\n\t    params=SeldonDeployerStepParameters(\n\t        service_config=SeldonDeploymentConfig(\n\t            model_name=\"seldon-svd-custom-model\",\n\t            replicas=1,\n\t            implementation=\"custom\",\n\t            resources=SeldonResourceRequirements(\n\t                limits={\"cpu\": \"100m\", \"memory\": \"250Mi\"}\n\t            ),\n\t        ),\n", "        timeout=240,\n\t        custom_deploy_parameters=CustomDeployParameters(\n\t            predict_function=\"steps.svd_custom_deploy.custom_predict\"\n\t        ),\n\t    )\n\t)"]}
{"filename": "recommendation/steps/train_step.py", "chunked_list": ["\"\"\"A step to train a Singular value decomposition (SVD) model.\"\"\"\n\tfrom zenml.steps import step, Output\n\tfrom surprise import SVD\n\tfrom surprise.trainset import Trainset\n\t@step\n\tdef train(trainset: Trainset) -> Output(model=SVD):\n\t    \"\"\"Train and return a SVD model.\n\t    Args:\n\t        trainset (Trainset): the data for model training.\n\t    Returns:\n", "        SVD: the trained SVD model\n\t    \"\"\"\n\t    model = SVD()\n\t    model.fit(trainset)\n\t    return model"]}
{"filename": "recommendation/steps/evaluate_step.py", "chunked_list": ["\"\"\"A step to evaluate the train model Root Mean Squared Error (rmse).\"\"\"\n\timport mlflow\n\tfrom zenml.steps import step\n\tfrom surprise import accuracy\n\tfrom surprise import SVD\n\t@step()\n\tdef evaluate(model: SVD, testset: list) -> float:\n\t    \"\"\"Make predictions with the testset and compute for the rmse.\n\t    Args:\n\t        model (SVD): the trained model\n", "        testset (list): the test dataset\n\t    Returns:\n\t        float: rmse\n\t    \"\"\"\n\t    predictions = model.test(testset)\n\t    rmse = accuracy.rmse(predictions)\n\t    if mlflow.active_run():\n\t        mlflow.log_metric(\"rmse\", rmse)\n\t    return rmse\n"]}
{"filename": "recommendation/steps/fetch_model.py", "chunked_list": ["\"\"\"A step to fetch model artifacts for recommendation deployment pipeline.\"\"\"\n\tfrom typing import Optional\n\tfrom zenml.logger import get_logger\n\tfrom zenml.post_execution import PipelineView, get_pipeline\n\tfrom zenml.post_execution.artifact import ArtifactView\n\tfrom zenml.steps import BaseParameters, Output, step\n\tfrom surprise import SVD\n\tlogger = get_logger(__name__)\n\tclass FetchModelParameters(BaseParameters):\n\t    \"\"\"Parameters for fetch model step.\"\"\"\n", "    # Name of the pipeline to fetch from\n\t    pipeline_name: str\n\t    # Step name to fetch model from.\n\t    step_name: str\n\t    # Optional pipeline version\n\t    pipeline_version: Optional[int] = None\n\tdef get_output_from_step(pipeline: PipelineView, step_name: str) -> ArtifactView:\n\t    \"\"\"Fetch output from a step with last completed run in a pipeline.\n\t    Args:\n\t        pipeline (PipelineView): Post-execution pipeline class object.\n", "        step_name (str): Name of step to fetch output from\n\t    Returns:\n\t        ArtifactView: Artifact data\n\t    Raises:\n\t        KeyError: If no step found with given name\n\t        ValueError: If no output found for step\n\t    \"\"\"\n\t    # Get the step with the given name from the last run\n\t    try:\n\t        # Get last completed run of the pipeline\n", "        fetch_last_completed_run = pipeline.get_run_for_completed_step(step_name)\n\t        logger.info(f\"Run used: {fetch_last_completed_run}\")\n\t        # Get the output of the step from last completed run\n\t        fetch_step = fetch_last_completed_run.get_step(step_name)\n\t        logger.info(f\"Step used: {fetch_step}\")\n\t    except KeyError as e:\n\t        logger.error(f\"No step found with name '{step_name}': {e}\")\n\t        raise e\n\t    # Get the model artifacts from the step\n\t    output = fetch_step.output\n", "    if output is None:\n\t        logger.error(f\"No output found for step '{step_name}'\")\n\t        raise ValueError(f\"No output found for step '{step_name}'\")\n\t    return output\n\tdef get_model_from_step(pipeline: PipelineView, fetch_model_step_name: str) -> SVD:\n\t    \"\"\"Fetch recommender model from specified pipeline and step name.\n\t    Args:\n\t        pipeline (PipelineView): Post-execution pipeline class object.\n\t        fetch_model_step_name (str): Name of step to fetch model from.\n\t    Returns:\n", "        SVD: surprise SVD recommender model.\n\t    \"\"\"\n\t    # Get the output from the step\n\t    output = get_output_from_step(pipeline, fetch_model_step_name)\n\t    # Read the model artifact from the output\n\t    model = output.read()\n\t    return model\n\t@step()\n\tdef fetch_model(\n\t    params: FetchModelParameters,\n", ") -> Output(model=SVD):\n\t    \"\"\"Step to fetch model artifacts from last run of the recommendation pipeline.\n\t    Args:\n\t        params (FetchModelParameters): Parameters for fetch model step.\n\t    Returns:\n\t        bool: Decision to deploy the model.\n\t        SVD: Model artifacts.\n\t    Raises:\n\t        ValueError: if the pipeline does not exist\n\t        KeyError: if step name parameter is not correct.\n", "    \"\"\"\n\t    # Fetch pipeline by name\n\t    pipeline: PipelineView | None = get_pipeline(\n\t        params.pipeline_name, version=params.pipeline_version\n\t    )\n\t    if pipeline is None:\n\t        logger.error(f\"Pipeline '{params.pipeline_name}' does not exist\")\n\t        raise ValueError(f\"Pipeline '{params.pipeline_name}' does not exist\")\n\t    logger.info(f\"Pipeline: {pipeline}\")\n\t    # Fetch the model artifacts from the pipeline\n", "    model = get_model_from_step(pipeline, params.step_name)\n\t    return model\n"]}
{"filename": "recommendation/steps/svd_custom_deploy.py", "chunked_list": ["\"\"\"Functions required by ZenML and Seldon to deploy a custom SVD recommender model to Seldon Core.\"\"\"\n\timport numpy as np\n\tfrom typing import Any, Dict, List, Union\n\tfrom zenml.logger import get_logger\n\tlogger = get_logger(__name__)\n\tArray_Like = Union[np.ndarray, List[Any], str, bytes, Dict[str, Any]]\n\tdef custom_predict(\n\t    model: Any,\n\t    request: Array_Like,\n\t) -> Array_Like:\n", "    \"\"\"Custom Prediction function for SVD models.\n\t    Request input is in the format: [{\"iid\": \"2\", \"uid\": \"26\"}, {\"iid\": \"11\", \"uid\": \"7\"}] \n\t    where each dictionary is a sample containing a user ID and a item ID to get an expected rating for.\n\t    The custom predict function is the core of the custom deployment, the \n\t    function is called by the custom deployment class defined for the serving \n\t    tool. The current implementation requires the function to get the model \n\t    loaded in the memory and a request with the data to predict.\n\t    Args:\n\t        model (Any): The model to use for prediction.\n\t        request: The prediction response of the model is an array-like format.\n", "    Returns:\n\t        The prediction in an array-like format. (e.g: np.ndarray, \n\t        List[Any], str, bytes, Dict[str, Any])\n\t    \"\"\"\n\t    inputs = []\n\t    for instance in request:\n\t        pred = model.predict(instance['uid'], instance['iid'])\n\t        inputs.append(pred._asdict())\n\t    return inputs\n"]}
{"filename": "recommendation/pipelines/__init__.py", "chunked_list": []}
{"filename": "recommendation/pipelines/deploy_recommendation_pipeline.py", "chunked_list": ["\"\"\"A pipeline to deploy recommendation model.\"\"\"\n\tfrom zenml.pipelines import pipeline\n\tfrom zenml.config import DockerSettings\n\t# This fixes an environment related issue with the Surprise module, see here: https://github.com/NicolasHug/Surprise/issues/364\n\tdocker_settings = DockerSettings(apt_packages=[\"gcc\", \"build-essential\"], \n\t                                 environment={\"SURPRISE_DATA_FOLDER\": \"/tmp\"})\n\t@pipeline(settings={\"docker\": docker_settings})\n\tdef recommendation_deployment_pipeline(\n\t    fetch_model, deployment_trigger, deploy_model\n\t):\n", "    \"\"\"Recommendation deployment pipeline.\n\t    Args:\n\t        fetch_model: This step fetches a model artifact from a training pipeline run\n\t        deploy_model: This step deploys the model with Seldon Core\n\t    \"\"\"\n\t    model = fetch_model()\n\t    decision = deployment_trigger(model)\n\t    deploy_model(decision, model)\n"]}
{"filename": "recommendation/pipelines/recommendation_pipeline.py", "chunked_list": ["\"\"\"A pipeline to load, train and evaluate a simple recommendation model.\"\"\"\n\tfrom zenml.pipelines import pipeline\n\tfrom zenml.config import DockerSettings\n\tdocker_settings = DockerSettings(apt_packages=[\"gcc\", \"build-essential\"])\n\t@pipeline(settings={\"docker\": docker_settings})\n\tdef recommendation_pipeline(\n\t    load_data,\n\t    train,\n\t    evaluate,\n\t):\n", "    \"\"\"Recommendation example pipeline.\n\t    Steps\n\t    1. load_data: This step load a built in dataset from the Surprise library and splits into train and test set.\n\t    2. train: This step creates and trains a SVD model with the train set.\n\t    3. evaluate: This step evaluate the trained model's rmse.\n\t    Args:\n\t        load_data: This step load a built in dataset from the Surprise library and splits into train and test set.\n\t        train: This step creates and trains a SVD model with the train set.\n\t        evaluate: This step evaluate the trained model's rmse.\n\t    \"\"\"\n", "    trainset, testset = load_data()\n\t    model = train(trainset)\n\t    score = evaluate(model, testset)\n"]}
{"filename": "recommendation/tests/__init__.py", "chunked_list": ["\"\"\"Test functions to be imported.\"\"\""]}
{"filename": "recommendation/tests/conftest.py", "chunked_list": ["\"\"\"Fixtures to be used and shared among tests.\"\"\"\n\timport pytest \n\tfrom types import SimpleNamespace\n\t@pytest.fixture\n\tdef data_parameters() -> dict:\n\t    \"\"\"Create a dictionary for parameters used in load_data step.\n\t    Returns:\n\t        dict: dictionary containing parameters for the step\n\t    \"\"\"\n\t    parameters = SimpleNamespace()\n", "    parameters.test_size = 0.25\n\t    return parameters"]}
{"filename": "recommendation/tests/test_steps/test_step_train.py", "chunked_list": ["\"\"\"Test suite for the train step.\"\"\"\n\timport pytest\n\tfrom steps.load_data_step import load_data\n\tfrom steps.train_step import train\n\tfrom surprise import SVD\n\tfrom surprise.trainset import Trainset\n\t@pytest.fixture\n\tdef data(data_parameters: dict) -> Trainset:\n\t    \"\"\"A fixture to get the data used in training the model.\n\t    Args:\n", "        data_parameters (dict): parameters for train test split\n\t    Returns:\n\t        Trainset: training data\n\t    \"\"\"\n\t    trainset, _ = load_data.entrypoint(data_parameters)\n\t    return trainset\n\tdef test_correct_type(data: Trainset):\n\t    \"\"\"Test whether the trained model has correct type.\n\t    Args:\n\t        data (Trainset): training data\n", "    \"\"\"\n\t    trainset = data\n\t    assert isinstance(train.entrypoint(trainset), SVD)"]}
{"filename": "recommendation/tests/test_steps/test_step_load_data_step.py", "chunked_list": ["\"\"\"Test suite for the load_data step.\"\"\"\n\tfrom steps.load_data_step import load_data\n\tfrom surprise.trainset import Trainset\n\tEXPECTED_DATA_LENGTH = 100000\n\tdef test_load_data_step_expected_output_types(data_parameters: dict):\n\t    \"\"\"Test whether the load_data step output the expected types.\n\t    Args:\n\t        data_parameters (dict): parameters for train test split\n\t    \"\"\"\n\t    trainset, testset = load_data.entrypoint(data_parameters)\n", "    assert isinstance(trainset, Trainset)\n\t    assert isinstance(testset, list)\n\tdef test_load_data_step_expected_data_amount(data_parameters: dict):\n\t    \"\"\"Test whether the load_data step split train and test data as expected.\n\t    Args:\n\t        data_parameters (dict): parameters for train test split\n\t    \"\"\"\n\t    trainset, testset = load_data.entrypoint(data_parameters)\n\t    expected_size_train = int(EXPECTED_DATA_LENGTH * (1 - data_parameters.test_size))\n\t    expected_size_test = int(EXPECTED_DATA_LENGTH * data_parameters.test_size)\n", "    assert trainset.n_ratings + len(testset) == EXPECTED_DATA_LENGTH\n\t    assert trainset.n_ratings == expected_size_train\n\t    assert len(testset) == expected_size_test\n"]}
{"filename": "recommendation/tests/test_steps/test_step_evaluate_step.py", "chunked_list": ["\"\"\"Test suite for the evaluate step.\"\"\"\n\timport pytest\n\tfrom typing import Tuple\n\tfrom steps.load_data_step import load_data\n\tfrom steps.train_step import train\n\tfrom steps.evaluate_step import evaluate\n\tfrom surprise.trainset import Trainset\n\tfrom surprise import Dataset\n\tfrom surprise import SVD\n\tBENCHMARK_SVD_SCORE = 0.93\n", "@pytest.fixture\n\tdef data(data_parameters: dict) -> Tuple[Trainset, list]:\n\t    \"\"\"Pytest fixtures for getting train and test data.\n\t    Args:\n\t        data_parameters (dict): parameters for train test split\n\t    Returns:\n\t        Tuple[Trainset, list]: train and test data\n\t    \"\"\"\n\t    trainset, testset = load_data.entrypoint(data_parameters)\n\t    return trainset, testset\n", "@pytest.fixture\n\tdef model(data: Dataset) -> SVD:\n\t    \"\"\"Pytest fixture for getting a trained model.\n\t    Args:\n\t        data (Dataset): training data\n\t    Returns:\n\t        SVD: the trained model\n\t    \"\"\"\n\t    trainset, _ = data\n\t    return train.entrypoint(trainset)\n", "def test_rmse_equals_benchmarks(model: SVD, data: Dataset):\n\t    \"\"\"Test whether the trained model rmse is roughly equals to the benchmark score.\n\t    Args:\n\t        model (SVD): model for recommendation\n\t        data (Dataset): dataset for testing\n\t    \"\"\"\n\t    _, testset = data\n\t    rmse = evaluate.entrypoint(model, testset)\n\t    # assert that the accuracy is 0.93 +/- 0.1\n\t    assert rmse == pytest.approx(BENCHMARK_SVD_SCORE, rel=0.1)\n"]}
{"filename": "recommendation/tests/test_pipelines/test_pipeline.py", "chunked_list": ["\"\"\"Integration tests for the pipeline as a whole.\"\"\"\n\timport pytest\n\timport os\n\timport logging\n\tfrom steps.load_data_step import load_data\n\tfrom steps.train_step import train\n\tfrom steps.evaluate_step import evaluate\n\tfrom zenml.logger import disable_logging\n\tfrom zenml.post_execution import get_unlisted_runs\n\tfrom zenml.post_execution.pipeline_run import PipelineRunView\n", "from pipelines.recommendation_pipeline import recommendation_pipeline\n\tfrom materializer import SurpriseMaterializer\n\tfrom surprise import SVD\n\tBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\tEXPECTED_DATA_LENGTH = 100000\n\tBENCHMARK_SVD_SCORE = 0.93\n\t@pytest.fixture(scope=\"class\", autouse=True)\n\tdef pipeline_run():\n\t    \"\"\"Set up fixture for running the pipeline.\"\"\"\n\t    pipeline = recommendation_pipeline(\n", "        load_data().configure(output_materializers=SurpriseMaterializer),\n\t        train().configure(output_materializers=SurpriseMaterializer),\n\t        evaluate(),\n\t    )\n\t    with disable_logging(log_level=logging.INFO):\n\t        pipeline.run(config_path=BASE_DIR + \"/test_pipeline_config.yaml\", unlisted=True)\n\t@pytest.fixture()\n\tdef get_pipeline_run() -> PipelineRunView:\n\t    \"\"\"Get the most recent pipeline run.\n\t    Args:\n", "        pipeline_run: the pipeline run fixture which is executed once.\n\t    Returns:\n\t        PipelineRunView: the test run\n\t    \"\"\"\n\t    return get_unlisted_runs()[0]\n\tdef test_pipeline_executes(get_pipeline_run: PipelineRunView):\n\t    \"\"\"Test the model training result from the the pipeline run.\n\t    Args:\n\t        get_pipeline_run (PipelineRunView): pipeline run.\n\t    \"\"\"\n", "    rmse = get_pipeline_run.get_step(step=\"evaluate\").output.read()\n\t    assert rmse == pytest.approx(BENCHMARK_SVD_SCORE, rel=0.1)\n\tdef test_pipeline_loads_and_splits_correctly(get_pipeline_run: PipelineRunView, data_parameters: dict):\n\t    \"\"\"Test whether the pipeline splits data into train and test correctly.\n\t    Args:\n\t        get_pipeline_run (PipelineRunView): the pipeline run\n\t        data_parameters (dict): parameters for train test split\n\t    \"\"\"\n\t    step_outputs = get_pipeline_run.get_step(step=\"load_data\").outputs\n\t    trainset = step_outputs[\"trainset\"].read()\n", "    testset = step_outputs[\"testset\"].read()\n\t    expected_size_train = int(EXPECTED_DATA_LENGTH * (1 - data_parameters.test_size))\n\t    expected_size_test = int(EXPECTED_DATA_LENGTH * data_parameters.test_size)\n\t    assert trainset.n_ratings + len(testset) == EXPECTED_DATA_LENGTH\n\t    assert trainset.n_ratings == expected_size_train\n\t    assert len(testset) == expected_size_test\n\tdef test_correct_model_type(get_pipeline_run: PipelineRunView):\n\t    \"\"\"Test whether the pipeline return the correct model type.\n\t    Args:\n\t        get_pipeline_run (PipelineRunView): the pipeline run\n", "    \"\"\"\n\t    step_output = get_pipeline_run.get_step(step='train').output.read()\n\t    assert isinstance(step_output, SVD)\n"]}
{"filename": "recommendation/materializer/__init__.py", "chunked_list": ["\"\"\"Materializer to be imported.\"\"\"\n\tfrom .surprise_materializer import SurpriseMaterializer"]}
{"filename": "recommendation/materializer/surprise_materializer.py", "chunked_list": ["\"\"\"Custom materializer for Surprise data and model.\"\"\"\n\timport os\n\timport pickle\n\tfrom zenml.io import fileio\n\tfrom zenml.materializers.base_materializer import BaseMaterializer\n\tfrom typing import Any, Type, Union\n\tfrom surprise import AlgoBase\n\tfrom surprise.trainset import Trainset\n\tDEFAULT_FILENAME = \"surprise_output.pickle\"\n\tclass SurpriseMaterializer(BaseMaterializer):\n", "    \"\"\"Custom materializer for handling object created using Surprise such as dataset and models.\"\"\"\n\t    ASSOCIATED_TYPES = (AlgoBase, Trainset, list)\n\t    def __init__(self, uri: str):\n\t        \"\"\"Initializes a materializer with the given URI.\"\"\"\n\t        self.uri = uri\n\t    def load(self, data_type: Type[Any]) -> Union[AlgoBase, Trainset]:\n\t        \"\"\"This function loads the input from the artifact store and returns it.\n\t        Args:\n\t            data_type (Type[Any]): The type of the artifact.\n\t        Returns:\n", "            Union[AlgoBase, Trainset]: the output of the artifact.\n\t        \"\"\"\n\t        # read from self.uri\n\t        super().load(data_type)\n\t        filepath = os.path.join(self.uri, DEFAULT_FILENAME)\n\t        with fileio.open(filepath, \"rb\") as fid:\n\t            obj = pickle.load(fid)\n\t        return obj\n\t    def save(self, obj: Any) -> None:\n\t        \"\"\"This function saves the artifact to the artifact store.\n", "        Args:\n\t            obj (any): The input artifact to be saved\n\t        \"\"\"\n\t        # write `data` to self.uri\n\t        super().save(obj)\n\t        model_filepath = os.path.join(self.uri, DEFAULT_FILENAME)\n\t        with fileio.open(model_filepath, \"wb\") as fid:\n\t            pickle.dump(obj, fid)"]}
