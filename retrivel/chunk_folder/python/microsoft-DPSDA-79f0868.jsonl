{"filename": "main.py", "chunked_list": ["import argparse\n\timport logging\n\timport os\n\timport numpy as np\n\timport imageio\n\tfrom torchvision.utils import make_grid\n\timport torch\n\tfrom dpsda.logging import setup_logging\n\tfrom dpsda.data_loader import load_data\n\tfrom dpsda.feature_extractor import extract_features\n", "from dpsda.metrics import make_fid_stats\n\tfrom dpsda.metrics import compute_fid\n\tfrom dpsda.dp_counter import dp_nn_histogram\n\tfrom dpsda.arg_utils import str2bool\n\tfrom apis import get_api_class_from_name\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        '--api',\n\t        type=str,\n", "        required=True,\n\t        choices=['DALLE', 'stable_diffusion', 'improved_diffusion'],\n\t        help='Which foundation model API to use')\n\t    parser.add_argument(\n\t        '--plot_images',\n\t        type=str2bool,\n\t        default=True,\n\t        help='Whether to save generated images in PNG files')\n\t    parser.add_argument(\n\t        '--data_checkpoint_path',\n", "        type=str,\n\t        default=\"\",\n\t        help='Path to the data checkpoint')\n\t    parser.add_argument(\n\t        '--data_checkpoint_step',\n\t        type=int,\n\t        default=-1,\n\t        help='Iteration of the data checkpoint')\n\t    parser.add_argument(\n\t        '--num_samples_schedule',\n", "        type=str,\n\t        default='50000,'*9 + '50000',\n\t        help='Number of samples to generate at each iteration')\n\t    parser.add_argument(\n\t        '--variation_degree_schedule',\n\t        type=str,\n\t        default='0,'*9 + '0',\n\t        help='Variation degree at each iteration')\n\t    parser.add_argument(\n\t        '--num_fid_samples',\n", "        type=int,\n\t        default=50000,\n\t        help='Number of generated samples to compute FID')\n\t    parser.add_argument(\n\t        '--num_private_samples',\n\t        type=int,\n\t        default=50000,\n\t        help='Number of private samples to load')\n\t    parser.add_argument(\n\t        '--noise_multiplier',\n", "        type=float,\n\t        default=0.0,\n\t        help='Noise multiplier for DP NN histogram')\n\t    parser.add_argument(\n\t        '--lookahead_degree',\n\t        type=int,\n\t        default=0,\n\t        help=('Lookahead degree for computing distances between private and '\n\t              'generated images'))\n\t    parser.add_argument(\n", "        '--feature_extractor',\n\t        type=str,\n\t        default='clip_vit_b_32',\n\t        choices=['inception_v3', 'clip_vit_b_32', 'original'],\n\t        help='Which image feature extractor to use')\n\t    parser.add_argument(\n\t        '--num_nearest_neighbor',\n\t        type=int,\n\t        default=1,\n\t        help='Number of nearest neighbors to find in DP NN histogram')\n", "    parser.add_argument(\n\t        '--nn_mode',\n\t        type=str,\n\t        default='L2',\n\t        choices=['L2', 'IP'],\n\t        help='Which distance metric to use in DP NN histogram')\n\t    parser.add_argument(\n\t        '--private_image_size',\n\t        type=int,\n\t        default=1024,\n", "        help='Size of private images')\n\t    parser.add_argument(\n\t        '--tmp_folder',\n\t        type=str,\n\t        default='result/tmp',\n\t        help='Temporary folder for storing intermediate results')\n\t    parser.add_argument(\n\t        '--result_folder',\n\t        type=str,\n\t        default='result',\n", "        help='Folder for storing results')\n\t    parser.add_argument(\n\t        '--data_folder',\n\t        type=str,\n\t        required=True,\n\t        help='Folder that contains the private images')\n\t    parser.add_argument(\n\t        '--count_threshold',\n\t        type=float,\n\t        default=0.0,\n", "        help='Threshold for DP NN histogram')\n\t    parser.add_argument(\n\t        '--compute_fid',\n\t        type=str2bool,\n\t        default=True,\n\t        help='Whether to compute FID')\n\t    parser.add_argument(\n\t        '--fid_dataset_name',\n\t        type=str,\n\t        default='customized_dataset',\n", "        help=('Name of the dataset for computing FID against. If '\n\t              'fid_dataset_name and fid_dataset_split in combination are one '\n\t              'of the precomputed datasets in '\n\t              'https://github.com/GaParmar/clean-fid and make_fid_stats=False,'\n\t              ' then the precomputed statistics will be used. Otherwise, the '\n\t              'statistics will be computed using the private samples and saved'\n\t              ' with fid_dataset_name and fid_dataset_split for future use.'))\n\t    parser.add_argument(\n\t        '--fid_dataset_split',\n\t        type=str,\n", "        default='train',\n\t        help=('Split of the dataset for computing FID against. If '\n\t              'fid_dataset_name and fid_dataset_split in combination are one '\n\t              'of the precomputed datasets in '\n\t              'https://github.com/GaParmar/clean-fid and make_fid_stats=False,'\n\t              ' then the precomputed statistics will be used. Otherwise, the '\n\t              'statistics will be computed using the private samples and saved'\n\t              ' with fid_dataset_name and fid_dataset_split for future use.'))\n\t    parser.add_argument(\n\t        '--fid_model_name',\n", "        type=str,\n\t        default='inception_v3',\n\t        choices=['inception_v3', 'clip_vit_b_32'],\n\t        help='Which embedding network to use for computing FID')\n\t    parser.add_argument(\n\t        '--make_fid_stats',\n\t        type=str2bool,\n\t        default=True,\n\t        help='Whether to compute FID stats for the private samples')\n\t    parser.add_argument(\n", "        '--data_loading_batch_size',\n\t        type=int,\n\t        default=100,\n\t        help='Batch size for loading private samples')\n\t    parser.add_argument(\n\t        '--feature_extractor_batch_size',\n\t        type=int,\n\t        default=500,\n\t        help='Batch size for feature extraction')\n\t    parser.add_argument(\n", "        '--fid_batch_size',\n\t        type=int,\n\t        default=500,\n\t        help='Batch size for computing FID')\n\t    parser.add_argument(\n\t        '--gen_class_cond',\n\t        type=str2bool,\n\t        default=False,\n\t        help='Whether to generate class labels')\n\t    parser.add_argument(\n", "        '--initial_prompt',\n\t        action='append',\n\t        type=str,\n\t        help='Initial prompt for image generation. It can be specified '\n\t             'multiple times to provide a list of prompts. If the API accepts '\n\t             'prompts, the initial samples will be generated with these '\n\t             'prompts')\n\t    parser.add_argument(\n\t        '--image_size',\n\t        type=str,\n", "        default='1024x1024',\n\t        help='Size of generated images in the format of HxW')\n\t    args, api_args = parser.parse_known_args()\n\t    args.num_samples_schedule = list(map(\n\t        int, args.num_samples_schedule.split(',')))\n\t    variation_degree_type = (float if '.' in args.variation_degree_schedule\n\t                             else int)\n\t    args.variation_degree_schedule = list(map(\n\t        variation_degree_type, args.variation_degree_schedule.split(',')))\n\t    if len(args.num_samples_schedule) != len(args.variation_degree_schedule):\n", "        raise ValueError('The length of num_samples_schedule and '\n\t                         'variation_degree_schedule should be the same')\n\t    api_class = get_api_class_from_name(args.api)\n\t    api = api_class.from_command_line_args(api_args)\n\t    return args, api\n\tdef log_samples(samples, additional_info, folder, plot_images):\n\t    if not os.path.exists(folder):\n\t        os.makedirs(folder)\n\t    np.savez(\n\t        os.path.join(folder, 'samples.npz'),\n", "        samples=samples,\n\t        additional_info=additional_info)\n\t    if plot_images:\n\t        for i in range(samples.shape[0]):\n\t            imageio.imwrite(os.path.join(folder, f'{i}.png'), samples[i])\n\tdef load_samples(path):\n\t    data = np.load(path)\n\t    samples = data['samples']\n\t    additional_info = data['additional_info']\n\t    return samples, additional_info\n", "def log_count(count, clean_count, path):\n\t    dirname = os.path.dirname(path)\n\t    if not os.path.exists(dirname):\n\t        os.makedirs(dirname)\n\t    np.savez(path, count=count, clean_count=clean_count)\n\tdef round_to_uint8(image):\n\t    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)\n\tdef visualize(samples, packed_samples, count, folder, suffix=''):\n\t    if not os.path.exists(folder):\n\t        os.makedirs(folder)\n", "    samples = samples.transpose((0, 3, 1, 2))\n\t    packed_samples = packed_samples.transpose((0, 1, 4, 2, 3))\n\t    ids = np.argsort(count)[::-1][:5]\n\t    print(count[ids])\n\t    vis_samples = []\n\t    for i in range(len(ids)):\n\t        vis_samples.append(samples[ids[i]])\n\t        for j in range(packed_samples.shape[1]):\n\t            vis_samples.append(packed_samples[ids[i]][j])\n\t    vis_samples = np.stack(vis_samples)\n", "    vis_samples = make_grid(\n\t        torch.Tensor(vis_samples),\n\t        nrow=packed_samples.shape[1] + 1).numpy().transpose((1, 2, 0))\n\t    vis_samples = round_to_uint8(vis_samples)\n\t    imageio.imsave(\n\t        os.path.join(folder, f'visualize_top_{suffix}.png'), vis_samples)\n\t    ids = np.argsort(count)[:5]\n\t    print(count[ids])\n\t    vis_samples = []\n\t    for i in range(len(ids)):\n", "        vis_samples.append(samples[ids[i]])\n\t        for j in range(packed_samples.shape[1]):\n\t            vis_samples.append(packed_samples[ids[i]][j])\n\t    vis_samples = np.stack(vis_samples)\n\t    vis_samples = make_grid(\n\t        torch.Tensor(vis_samples),\n\t        nrow=packed_samples.shape[1] + 1).numpy().transpose((1, 2, 0))\n\t    vis_samples = round_to_uint8(vis_samples)\n\t    imageio.imsave(\n\t        os.path.join(folder, f'visualize_bottom_{suffix}.png'), vis_samples)\n", "def log_fid(folder, fid, t):\n\t    with open(os.path.join(folder, 'fid.csv'), 'a') as f:\n\t        f.write(f'{t} {fid}\\n')\n\tdef main():\n\t    args, api = parse_args()\n\t    if os.path.exists(args.result_folder):\n\t        raise RuntimeError(f'{args.result_folder} exists')\n\t    os.makedirs(args.result_folder)\n\t    setup_logging(os.path.join(args.result_folder, 'log.log'))\n\t    logging.info(f'config: {args}')\n", "    logging.info(f'API config: {api.args}')\n\t    all_private_samples, all_private_labels = load_data(\n\t        data_dir=args.data_folder,\n\t        batch_size=args.data_loading_batch_size,\n\t        image_size=args.private_image_size,\n\t        class_cond=args.gen_class_cond,\n\t        num_private_samples=args.num_private_samples)\n\t    private_classes = list(sorted(set(list(all_private_labels))))\n\t    private_num_classes = len(private_classes)\n\t    logging.info(f'Private_num_classes: {private_num_classes}')\n", "    logging.info('Extracting features')\n\t    all_private_features = extract_features(\n\t        data=all_private_samples,\n\t        tmp_folder=args.tmp_folder,\n\t        model_name=args.feature_extractor,\n\t        res=args.private_image_size,\n\t        batch_size=args.feature_extractor_batch_size)\n\t    logging.info(f'all_private_features.shape: {all_private_features.shape}')\n\t    if args.make_fid_stats:\n\t        logging.info('Computing FID stats')\n", "        make_fid_stats(\n\t            samples=all_private_samples,\n\t            dataset=args.fid_dataset_name,\n\t            dataset_res=args.private_image_size,\n\t            dataset_split=args.fid_dataset_split,\n\t            tmp_folder=args.tmp_folder,\n\t            model_name=args.fid_model_name,\n\t            batch_size=args.fid_batch_size)\n\t    # Generating initial samples.\n\t    if args.data_checkpoint_path != '':\n", "        logging.info(\n\t            f'Loading data checkpoint from {args.data_checkpoint_path}')\n\t        samples, additional_info = load_samples(args.data_checkpoint_path)\n\t        if args.data_checkpoint_step < 0:\n\t            raise ValueError('data_checkpoint_step should be >= 0')\n\t        start_t = args.data_checkpoint_step + 1\n\t    else:\n\t        logging.info('Generating initial samples')\n\t        samples, additional_info = api.image_random_sampling(\n\t            prompts=args.initial_prompt,\n", "            num_samples=args.num_samples_schedule[0],\n\t            size=args.image_size)\n\t        log_samples(\n\t            samples=samples,\n\t            additional_info=additional_info,\n\t            folder=f'{args.result_folder}/{0}',\n\t            plot_images=args.plot_images)\n\t        if args.data_checkpoint_step >= 0:\n\t            logging.info('Ignoring data_checkpoint_step')\n\t        start_t = 1\n", "    if args.compute_fid:\n\t        logging.info('Computing FID')\n\t        fid = compute_fid(\n\t            samples=samples,\n\t            tmp_folder=args.tmp_folder,\n\t            num_fid_samples=args.num_fid_samples,\n\t            dataset_res=args.private_image_size,\n\t            dataset=args.fid_dataset_name,\n\t            dataset_split=args.fid_dataset_split,\n\t            model_name=args.fid_model_name,\n", "            batch_size=args.fid_batch_size)\n\t        logging.info(f'fid={fid}')\n\t        log_fid(args.result_folder, fid, 0)\n\t    for t in range(start_t, len(args.num_samples_schedule)):\n\t        logging.info(f't={t}')\n\t        assert samples.shape[0] % private_num_classes == 0\n\t        num_samples_per_class = samples.shape[0] // private_num_classes\n\t        if args.lookahead_degree == 0:\n\t            packed_samples = np.expand_dims(samples, axis=1)\n\t        else:\n", "            logging.info('Running image variation')\n\t            packed_samples = api.image_variation(\n\t                images=samples,\n\t                additional_info=additional_info,\n\t                num_variations_per_image=args.lookahead_degree,\n\t                size=args.image_size,\n\t                variation_degree=args.variation_degree_schedule[t])\n\t        packed_features = []\n\t        logging.info('Running feature extraction')\n\t        for i in range(packed_samples.shape[1]):\n", "            sub_packed_features = extract_features(\n\t                data=packed_samples[:, i],\n\t                tmp_folder=args.tmp_folder,\n\t                model_name=args.feature_extractor,\n\t                res=args.private_image_size,\n\t                batch_size=args.feature_extractor_batch_size)\n\t            logging.info(\n\t                f'sub_packed_features.shape: {sub_packed_features.shape}')\n\t            packed_features.append(sub_packed_features)\n\t        packed_features = np.mean(packed_features, axis=0)\n", "        logging.info('Computing histogram')\n\t        count = []\n\t        for class_i, class_ in enumerate(private_classes):\n\t            sub_count, sub_clean_count = dp_nn_histogram(\n\t                public_features=packed_features[\n\t                    num_samples_per_class * class_i:\n\t                    num_samples_per_class * (class_i + 1)],\n\t                private_features=all_private_features[\n\t                    all_private_labels == class_],\n\t                noise_multiplier=args.noise_multiplier,\n", "                num_nearest_neighbor=args.num_nearest_neighbor,\n\t                mode=args.nn_mode,\n\t                threshold=args.count_threshold)\n\t            log_count(\n\t                sub_count,\n\t                sub_clean_count,\n\t                f'{args.result_folder}/{t}/count_class{class_}.npz')\n\t            count.append(sub_count)\n\t        count = np.concatenate(count)\n\t        for class_i, class_ in enumerate(private_classes):\n", "            visualize(\n\t                samples=samples[\n\t                    num_samples_per_class * class_i:\n\t                    num_samples_per_class * (class_i + 1)],\n\t                packed_samples=packed_samples[\n\t                    num_samples_per_class * class_i:\n\t                    num_samples_per_class * (class_i + 1)],\n\t                count=count[\n\t                    num_samples_per_class * class_i:\n\t                    num_samples_per_class * (class_i + 1)],\n", "                folder=f'{args.result_folder}/{t}',\n\t                suffix=f'class{class_}')\n\t        logging.info('Generating new indices')\n\t        assert args.num_samples_schedule[t] % private_num_classes == 0\n\t        new_num_samples_per_class = (\n\t            args.num_samples_schedule[t] // private_num_classes)\n\t        new_indices = []\n\t        for class_i in private_classes:\n\t            sub_count = count[\n\t                num_samples_per_class * class_i:\n", "                num_samples_per_class * (class_i + 1)]\n\t            sub_new_indices = np.random.choice(\n\t                np.arange(num_samples_per_class * class_i,\n\t                          num_samples_per_class * (class_i + 1)),\n\t                size=new_num_samples_per_class,\n\t                p=sub_count / np.sum(sub_count))\n\t            new_indices.append(sub_new_indices)\n\t        new_indices = np.concatenate(new_indices)\n\t        new_samples = samples[new_indices]\n\t        new_additional_info = additional_info[new_indices]\n", "        logging.info('Generating new samples')\n\t        new_new_samples = api.image_variation(\n\t            images=new_samples,\n\t            additional_info=new_additional_info,\n\t            num_variations_per_image=1,\n\t            size=args.image_size,\n\t            variation_degree=args.variation_degree_schedule[t])\n\t        new_new_samples = np.squeeze(new_new_samples, axis=1)\n\t        new_new_additional_info = new_additional_info\n\t        if args.compute_fid:\n", "            logging.info('Computing FID')\n\t            new_new_fid = compute_fid(\n\t                new_new_samples,\n\t                tmp_folder=args.tmp_folder,\n\t                num_fid_samples=args.num_fid_samples,\n\t                dataset_res=args.private_image_size,\n\t                dataset=args.fid_dataset_name,\n\t                dataset_split=args.fid_dataset_split,\n\t                model_name=args.fid_model_name,\n\t                batch_size=args.fid_batch_size)\n", "            logging.info(f'fid={new_new_fid}')\n\t            log_fid(args.result_folder, new_new_fid, t)\n\t        samples = new_new_samples\n\t        additional_info = new_new_additional_info\n\t        log_samples(\n\t            samples=samples,\n\t            additional_info=additional_info,\n\t            folder=f'{args.result_folder}/{t}',\n\t            plot_images=args.plot_images)\n\tif __name__ == '__main__':\n", "    main()\n"]}
{"filename": "apis/stable_diffusion_api.py", "chunked_list": ["import torch\n\timport torchvision.transforms as T\n\tfrom PIL import Image\n\timport numpy as np\n\tfrom tqdm import tqdm\n\tfrom diffusers import StableDiffusionPipeline\n\tfrom diffusers import StableDiffusionImg2ImgPipeline\n\tfrom .api import API\n\tfrom dpsda.pytorch_utils import dev\n\tdef _round_to_uint8(image):\n", "    return np.around(np.clip(image * 255, a_min=0, a_max=255)).astype(np.uint8)\n\tclass StableDiffusionAPI(API):\n\t    def __init__(self, random_sampling_checkpoint,\n\t                 random_sampling_guidance_scale,\n\t                 random_sampling_num_inference_steps,\n\t                 random_sampling_batch_size,\n\t                 variation_checkpoint,\n\t                 variation_guidance_scale,\n\t                 variation_num_inference_steps,\n\t                 variation_batch_size,\n", "                 *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self._random_sampling_checkpoint = random_sampling_checkpoint\n\t        self._random_sampling_guidance_scale = random_sampling_guidance_scale\n\t        self._random_sampling_num_inference_steps = \\\n\t            random_sampling_num_inference_steps\n\t        self._random_sampling_batch_size = random_sampling_batch_size\n\t        self._random_sampling_pipe = StableDiffusionPipeline.from_pretrained(\n\t            self._random_sampling_checkpoint, torch_dtype=torch.float16)\n\t        self._random_sampling_pipe.safety_checker = None\n", "        self._random_sampling_pipe = self._random_sampling_pipe.to(dev())\n\t        self._variation_checkpoint = variation_checkpoint\n\t        self._variation_guidance_scale = variation_guidance_scale\n\t        self._variation_num_inference_steps = variation_num_inference_steps\n\t        self._variation_batch_size = variation_batch_size\n\t        self._variation_pipe = \\\n\t            StableDiffusionImg2ImgPipeline.from_pretrained(\n\t                self._variation_checkpoint,\n\t                torch_dtype=torch.float16)\n\t        self._variation_pipe.safety_checker = None\n", "        self._variation_pipe = self._variation_pipe.to(dev())\n\t    @staticmethod\n\t    def command_line_parser():\n\t        parser = super(\n\t            StableDiffusionAPI, StableDiffusionAPI).command_line_parser()\n\t        parser.add_argument(\n\t            '--random_sampling_checkpoint',\n\t            type=str,\n\t            required=True,\n\t            help='The path to the checkpoint for random sampling API')\n", "        parser.add_argument(\n\t            '--random_sampling_guidance_scale',\n\t            type=float,\n\t            default=7.5,\n\t            help='The guidance scale for random sampling API')\n\t        parser.add_argument(\n\t            '--random_sampling_num_inference_steps',\n\t            type=int,\n\t            default=50,\n\t            help='The number of diffusion steps for random sampling API')\n", "        parser.add_argument(\n\t            '--random_sampling_batch_size',\n\t            type=int,\n\t            default=10,\n\t            help='The batch size for random sampling API')\n\t        parser.add_argument(\n\t            '--variation_checkpoint',\n\t            type=str,\n\t            required=True,\n\t            help='The path to the checkpoint for variation API')\n", "        parser.add_argument(\n\t            '--variation_guidance_scale',\n\t            type=float,\n\t            default=7.5,\n\t            help='The guidance scale for variation API')\n\t        parser.add_argument(\n\t            '--variation_num_inference_steps',\n\t            type=int,\n\t            default=50,\n\t            help='The number of diffusion steps for variation API')\n", "        parser.add_argument(\n\t            '--variation_batch_size',\n\t            type=int,\n\t            default=10,\n\t            help='The batch size for variation API')\n\t        return parser\n\t    def image_random_sampling(self, num_samples, size, prompts):\n\t        \"\"\"\n\t        Generates a specified number of random image samples based on a given\n\t        prompt and size using OpenAI's Image API.\n", "        Args:\n\t            num_samples (int):\n\t                The number of image samples to generate.\n\t            size (str, optional):\n\t                The size of the generated images in the format\n\t                \"widthxheight\". Options include \"256x256\", \"512x512\", and\n\t                \"1024x1024\".\n\t            prompts (List[str]):\n\t                The text prompts to generate images from. Each promot will be\n\t                used to generate num_samples/len(prompts) number of samples.\n", "        Returns:\n\t            numpy.ndarray:\n\t                A numpy array of shape [num_samples x width x height x\n\t                channels] with type np.uint8 containing the generated image\n\t                samples as numpy arrays.\n\t            numpy.ndarray:\n\t                A numpy array with length num_samples containing prompts for\n\t                each image.\n\t        \"\"\"\n\t        max_batch_size = self._random_sampling_batch_size\n", "        images = []\n\t        return_prompts = []\n\t        width, height = list(map(int, size.split('x')))\n\t        for prompt_i, prompt in enumerate(prompts):\n\t            num_samples_for_prompt = (num_samples + prompt_i) // len(prompts)\n\t            num_iterations = int(np.ceil(\n\t                float(num_samples_for_prompt) / max_batch_size))\n\t            for iteration in tqdm(range(num_iterations)):\n\t                batch_size = min(\n\t                    max_batch_size,\n", "                    num_samples_for_prompt - iteration * max_batch_size)\n\t                images.append(_round_to_uint8(self._random_sampling_pipe(\n\t                    prompt=prompt,\n\t                    width=width,\n\t                    height=height,\n\t                    num_inference_steps=(\n\t                        self._random_sampling_num_inference_steps),\n\t                    guidance_scale=self._random_sampling_guidance_scale,\n\t                    num_images_per_prompt=batch_size,\n\t                    output_type='np').images))\n", "            return_prompts.extend([prompt] * num_samples_for_prompt)\n\t        return np.concatenate(images, axis=0), np.array(return_prompts)\n\t    def image_variation(self, images, additional_info,\n\t                        num_variations_per_image, size, variation_degree):\n\t        \"\"\"\n\t        Generates a specified number of variations for each image in the input\n\t        array using OpenAI's Image Variation API.\n\t        Args:\n\t            images (numpy.ndarray):\n\t                A numpy array of shape [num_samples x width x height\n", "                x channels] containing the input images as numpy arrays of type\n\t                uint8.\n\t            additional_info (numpy.ndarray):\n\t                A numpy array with the first dimension equaling to\n\t                num_samples containing prompts provided by\n\t                image_random_sampling.\n\t            num_variations_per_image (int):\n\t                The number of variations to generate for each input image.\n\t            size (str):\n\t                The size of the generated image variations in the\n", "                format \"widthxheight\". Options include \"256x256\", \"512x512\",\n\t                and \"1024x1024\".\n\t            variation_degree (float):\n\t                The image variation degree, between 0~1. A larger value means\n\t                more variation.\n\t        Returns:\n\t            numpy.ndarray:\n\t                A numpy array of shape [num_samples x num_variations_per_image\n\t                x width x height x channels] containing the generated image\n\t                variations as numpy arrays of type uint8.\n", "        \"\"\"\n\t        if not (0 <= variation_degree <= 1):\n\t            raise ValueError('variation_degree should be between 0 and 1')\n\t        variations = []\n\t        for _ in tqdm(range(num_variations_per_image)):\n\t            sub_variations = self._image_variation(\n\t                images=images,\n\t                prompts=list(additional_info),\n\t                size=size,\n\t                variation_degree=variation_degree)\n", "            variations.append(sub_variations)\n\t        return np.stack(variations, axis=1)\n\t    def _image_variation(self, images, prompts, size, variation_degree):\n\t        width, height = list(map(int, size.split('x')))\n\t        variation_transform = T.Compose([\n\t            T.Resize(\n\t                (width, height),\n\t                interpolation=T.InterpolationMode.BICUBIC),\n\t            T.ToTensor(),\n\t            T.Normalize(\n", "                [0.5, 0.5, 0.5],\n\t                [0.5, 0.5, 0.5])])\n\t        images = [variation_transform(Image.fromarray(im))\n\t                  for im in images]\n\t        images = torch.stack(images).to(dev())\n\t        max_batch_size = self._variation_batch_size\n\t        variations = []\n\t        num_iterations = int(np.ceil(\n\t            float(images.shape[0]) / max_batch_size))\n\t        for iteration in tqdm(range(num_iterations), leave=False):\n", "            variations.append(self._variation_pipe(\n\t                prompt=prompts[iteration * max_batch_size:\n\t                               (iteration + 1) * max_batch_size],\n\t                image=images[iteration * max_batch_size:\n\t                             (iteration + 1) * max_batch_size],\n\t                num_inference_steps=self._variation_num_inference_steps,\n\t                strength=variation_degree,\n\t                guidance_scale=self._variation_guidance_scale,\n\t                num_images_per_prompt=1,\n\t                output_type='np').images)\n", "        variations = _round_to_uint8(np.concatenate(variations, axis=0))\n\t        return variations\n"]}
{"filename": "apis/api.py", "chunked_list": ["from abc import ABC, abstractmethod\n\timport argparse\n\tclass API(ABC):\n\t    def __init__(self, args=None):\n\t        self.args = args\n\t    @staticmethod\n\t    def command_line_parser():\n\t        parser = argparse.ArgumentParser()\n\t        parser.add_argument(\n\t            '--api_help',\n", "            action='help')\n\t        return parser\n\t    @classmethod\n\t    def from_command_line_args(cls, args):\n\t        \"\"\"\n\t        Creating the API from command line arguments.\n\t        Args:\n\t            args: (List[str]):\n\t            The command line arguments\n\t        Returns:\n", "            API:\n\t                The API object.\n\t        \"\"\"\n\t        args = cls.command_line_parser().parse_args(args)\n\t        return cls(**vars(args), args=args)\n\t    @abstractmethod\n\t    def image_random_sampling(self, num_samples, size, prompts=None):\n\t        \"\"\"\n\t        Generates a specified number of random image samples based on a given\n\t        prompt and size.\n", "        Args:\n\t            num_samples (int, optional):\n\t                The number of image samples to generate.\n\t            size (str, optional):\n\t                The size of the generated images in the format\n\t                \"widthxheight\", e.g., \"1024x1024\".\n\t            prompts (List[str], optional):\n\t                The text prompts to generate images from. Each promot will be\n\t                used to generate num_samples/len(prompts) number of samples.\n\t        Returns:\n", "            numpy.ndarray:\n\t                A numpy array of shape [num_samples x width x height x\n\t                channels] with type np.uint8 containing the generated image\n\t                samples as numpy arrays.\n\t            numpy.ndarray:\n\t                A numpy array with the first dimension equaling to\n\t                num_samples containing additional information such as labels.\n\t        \"\"\"\n\t        pass\n\t    @abstractmethod\n", "    def image_variation(self, images, additional_info,\n\t                        num_variations_per_image, size, variation_degree=None):\n\t        \"\"\"\n\t        Generates a specified number of variations for each image in the input\n\t        array.\n\t        Args:\n\t            images (numpy.ndarray):\n\t                A numpy array of shape [num_samples x width x height\n\t                x channels] containing the input images as numpy arrays of type\n\t                uint8.\n", "            additional_info (numpy.ndarray):\n\t                A numpy array with the first dimension equaling to\n\t                num_samples containing additional information such as labels or\n\t                prompts provided by image_random_sampling.\n\t            num_variations_per_image (int):\n\t                The number of variations to generate for each input image.\n\t            size (str):\n\t                The size of the generated image variations in the\n\t                format \"widthxheight\", e.g., \"1024x1024\".\n\t            variation_degree (int or float, optional):\n", "                The degree of image variation.\n\t        Returns:\n\t            numpy.ndarray:\n\t                A numpy array of shape [num_samples x num_variations_per_image\n\t                x width x height x channels] containing the generated image\n\t                variations as numpy arrays of type uint8.\n\t        \"\"\"\n\t        pass\n"]}
{"filename": "apis/__init__.py", "chunked_list": ["from .api import API\n\tdef get_api_class_from_name(name):\n\t    # Lazy import to improve loading speed and reduce libary dependency.\n\t    if name == 'DALLE':\n\t        from .dalle_api import DALLEAPI\n\t        return DALLEAPI\n\t    elif name == 'stable_diffusion':\n\t        from .stable_diffusion_api import StableDiffusionAPI\n\t        return StableDiffusionAPI\n\t    elif name == 'improved_diffusion':\n", "        from .improved_diffusion_api import ImprovedDiffusionAPI\n\t        return ImprovedDiffusionAPI\n\t    else:\n\t        raise ValueError(f'Unknown API name {name}')\n\t__all__ = ['get_api_class_from_name', 'API']\n"]}
{"filename": "apis/dalle_api.py", "chunked_list": ["import openai\n\timport numpy as np\n\tfrom imageio.v2 import imread\n\timport requests\n\tfrom io import BytesIO\n\tfrom PIL import Image\n\tfrom tqdm import tqdm\n\timport os\n\timport logging\n\tfrom .api import API\n", "from tenacity import (\n\t    retry,\n\t    retry_if_not_exception_type,\n\t    stop_after_attempt,\n\t    wait_random_exponential,\n\t)\n\tclass DALLEAPI(API):\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self._openai_api_key = os.environ['OPENAI_API_KEY']\n", "        openai.api_key = self._openai_api_key\n\t    @staticmethod\n\t    def command_line_parser():\n\t        return super(DALLEAPI, DALLEAPI).command_line_parser()\n\t    def image_random_sampling(self, num_samples, size, prompts):\n\t        \"\"\"\n\t        Generates a specified number of random image samples based on a given\n\t        prompt and size using OpenAI's Image API.\n\t        Args:\n\t            num_samples (int):\n", "                The number of image samples to generate.\n\t            size (str, optional):\n\t                The size of the generated images in the format\n\t                \"widthxheight\". Options include \"256x256\", \"512x512\", and\n\t                \"1024x1024\".\n\t            prompts (List[str]):\n\t                The text prompts to generate images from. Each promot will be\n\t                used to generate num_samples/len(prompts) number of samples.\n\t        Returns:\n\t            numpy.ndarray:\n", "                A numpy array of shape [num_samples x width x height x\n\t                channels] with type np.uint8 containing the generated image\n\t                samples as numpy arrays.\n\t            numpy.ndarray:\n\t                A numpy array with length num_samples containing prompts for\n\t                each image.\n\t        \"\"\"\n\t        max_batch_size = 10\n\t        images = []\n\t        return_prompts = []\n", "        for prompt_i, prompt in enumerate(prompts):\n\t            num_samples_for_prompt = (num_samples + prompt_i) // len(prompts)\n\t            num_iterations = int(np.ceil(\n\t                float(num_samples_for_prompt) / max_batch_size))\n\t            for iteration in tqdm(range(num_iterations)):\n\t                batch_size = min(\n\t                    max_batch_size,\n\t                    num_samples_for_prompt - iteration * max_batch_size)\n\t                images.append(_dalle2_random_sampling(\n\t                    prompt=prompt, num_samples=batch_size, size=size))\n", "            return_prompts.extend([prompt] * num_samples_for_prompt)\n\t        return np.concatenate(images, axis=0), np.array(return_prompts)\n\t    def image_variation(self, images, additional_info,\n\t                        num_variations_per_image, size, variation_degree=None):\n\t        \"\"\"\n\t        Generates a specified number of variations for each image in the input\n\t        array using OpenAI's Image Variation API.\n\t        Args:\n\t            images (numpy.ndarray):\n\t                A numpy array of shape [num_samples x width x height\n", "                x channels] containing the input images as numpy arrays of type\n\t                uint8.\n\t            additional_info (numpy.ndarray):\n\t                A numpy array with the first dimension equaling to\n\t                num_samples containing prompts provided by\n\t                image_random_sampling.\n\t            num_variations_per_image (int):\n\t                The number of variations to generate for each input image.\n\t            size (str):\n\t                The size of the generated image variations in the\n", "                format \"widthxheight\". Options include \"256x256\", \"512x512\",\n\t                and \"1024x1024\".\n\t        Returns:\n\t            numpy.ndarray:\n\t                A numpy array of shape [num_samples x num_variations_per_image\n\t                x width x height x channels] containing the generated image\n\t                variations as numpy arrays of type uint8.\n\t        \"\"\"\n\t        if variation_degree is not None:\n\t            logging.info(f'Ignoring variation degree {variation_degree}')\n", "        if additional_info is not None:\n\t            logging.info('Ignoring additional info')\n\t        max_batch_size = 10\n\t        variations = []\n\t        for iteration in tqdm(range(int(np.ceil(\n\t                float(num_variations_per_image) / max_batch_size)))):\n\t            batch_size = min(\n\t                max_batch_size,\n\t                num_variations_per_image - iteration * max_batch_size)\n\t            sub_variations = []\n", "            for image in tqdm(images, leave=False):\n\t                sub_variations.append(_dalle2_image_variation(\n\t                    image=image,\n\t                    num_variations_per_image=batch_size,\n\t                    size=size))\n\t            sub_variations = np.array(sub_variations, dtype=np.uint8)\n\t            variations.append(sub_variations)\n\t        return np.concatenate(variations, axis=1)\n\t# Decorator is retry logic to get around rate limits. COMMENT OUT WHEN\n\t# DEBUGGING! Otherwise it will constantly retry on errors.\n", "@retry(\n\t    retry=retry_if_not_exception_type(openai.error.InvalidRequestError),\n\t    wait=wait_random_exponential(min=1, max=60),\n\t    stop=stop_after_attempt(15),\n\t)\n\tdef _dalle2_random_sampling(prompt, num_samples=10, size=\"1024x1024\"):\n\t    \"\"\"\n\t    Generates a specified number of random image samples based on a given\n\t    prompt and size using OpenAI's Image API.\n\t    Args:\n", "        prompt (str): The text prompt to generate images from.\n\t        num_samples (int, optional): The number of image samples to generate.\n\t            Default is 10. Max of 10.\n\t        size (str, optional): The size of the generated images in the format\n\t            \"widthxheight\". Default is \"1024x1024\". Options include \"256x256\",\n\t            \"512x512\", and \"1024x1024\".\n\t    Returns:\n\t        numpy.ndarray: A numpy array of shape [num_samples x image size x \n\t            image size x channels] containing the generated image samples as\n\t            numpy arrays.\n", "    \"\"\"\n\t    response = openai.Image.create(prompt=prompt, n=num_samples, size=size)\n\t    image_urls = [image[\"url\"] for image in response[\"data\"]]\n\t    images = [\n\t        imread(BytesIO(requests.get(url).content)) for url in image_urls\n\t    ]  # Store as np array\n\t    return np.array(images)\n\t@retry(\n\t    retry=retry_if_not_exception_type(openai.error.InvalidRequestError),\n\t    wait=wait_random_exponential(min=1, max=60),\n", "    stop=stop_after_attempt(15),\n\t)\n\tdef _dalle2_image_variation(image, num_variations_per_image,\n\t                            size=\"1024x1024\"):\n\t    \"\"\"\n\t    Generates a specified number of variations for one image in the input\n\t    array using OpenAI's Image Variation API.\n\t    Args:\n\t        images (numpy.ndarray): A numpy array of shape [channels\n\t            x image size x image size] containing the input images as numpy\n", "            arrays of type uint8.\n\t        num_variations_per_image (int): The number of variations to generate\n\t            for each input image.\n\t        size (str, optional): The size of the generated image variations in the\n\t            format \"widthxheight\". Default is \"1024x1024\". Options include\n\t            \"256x256\", \"512x512\", and \"1024x1024\".\n\t    Returns:\n\t        numpy.ndarray: A numpy array of shape [\n\t            num_variations_per_image x image size x image size x channels]\n\t            containing the generated image variations as numpy arrays of type\n", "            uint8.\n\t    \"\"\"\n\t    im = Image.fromarray(image)\n\t    with BytesIO() as buffer:\n\t        im.save(buffer, format=\"PNG\")\n\t        buffer.seek(0)\n\t        response = openai.Image.create_variation(\n\t            image=buffer, n=num_variations_per_image, size=size\n\t        )\n\t    image_urls = [image[\"url\"] for image in response[\"data\"]]\n", "    image_variations = [\n\t        imread(BytesIO(requests.get(url).content)) for url in image_urls\n\t    ]\n\t    return np.array(image_variations, dtype=np.uint8)\n"]}
{"filename": "apis/improved_diffusion_api.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom tqdm import tqdm\n\timport logging\n\tfrom .api import API\n\tfrom dpsda.arg_utils import str2bool\n\tfrom .improved_diffusion.unet import create_model\n\tfrom improved_diffusion import dist_util\n\tfrom improved_diffusion.script_util import NUM_CLASSES\n\tfrom .improved_diffusion.gaussian_diffusion import create_gaussian_diffusion\n", "def _round_to_uint8(image):\n\t    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)\n\tclass ImprovedDiffusionAPI(API):\n\t    def __init__(self, model_image_size, num_channels, num_res_blocks,\n\t                 learn_sigma, class_cond, use_checkpoint,\n\t                 attention_resolutions, num_heads, num_heads_upsample,\n\t                 use_scale_shift_norm, dropout, diffusion_steps, sigma_small,\n\t                 noise_schedule, use_kl, predict_xstart, rescale_timesteps,\n\t                 rescale_learned_sigmas, timestep_respacing, model_path,\n\t                 batch_size, use_ddim, clip_denoised, use_data_parallel,\n", "                 *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self._model = create_model(\n\t            image_size=model_image_size,\n\t            num_channels=num_channels,\n\t            num_res_blocks=num_res_blocks,\n\t            learn_sigma=learn_sigma,\n\t            class_cond=class_cond,\n\t            use_checkpoint=use_checkpoint,\n\t            attention_resolutions=attention_resolutions,\n", "            num_heads=num_heads,\n\t            num_heads_upsample=num_heads_upsample,\n\t            use_scale_shift_norm=use_scale_shift_norm,\n\t            dropout=dropout)\n\t        self._diffusion = create_gaussian_diffusion(\n\t            steps=diffusion_steps,\n\t            learn_sigma=learn_sigma,\n\t            sigma_small=sigma_small,\n\t            noise_schedule=noise_schedule,\n\t            use_kl=use_kl,\n", "            predict_xstart=predict_xstart,\n\t            rescale_timesteps=rescale_timesteps,\n\t            rescale_learned_sigmas=rescale_learned_sigmas,\n\t            timestep_respacing=timestep_respacing)\n\t        self._model.load_state_dict(\n\t            dist_util.load_state_dict(model_path, map_location=\"cpu\"))\n\t        self._model.to(dist_util.dev())\n\t        self._model.eval()\n\t        self._sampler = Sampler(model=self._model, diffusion=self._diffusion)\n\t        if use_data_parallel:\n", "            self._sampler = torch.nn.DataParallel(self._sampler)\n\t        self._batch_size = batch_size\n\t        self._use_ddim = use_ddim\n\t        self._image_size = model_image_size\n\t        self._clip_denoised = clip_denoised\n\t        self._class_cond = class_cond\n\t    @staticmethod\n\t    def command_line_parser():\n\t        parser = super(\n\t            ImprovedDiffusionAPI, ImprovedDiffusionAPI).command_line_parser()\n", "        parser.description = (\n\t            'See https://github.com/openai/improved-diffusion for the details'\n\t            ' of the arguments.')\n\t        parser.add_argument(\n\t            '--model_image_size',\n\t            type=int,\n\t            default=64)\n\t        parser.add_argument(\n\t            '--num_channels',\n\t            type=int,\n", "            default=128)\n\t        parser.add_argument(\n\t            '--num_res_blocks',\n\t            type=int,\n\t            default=2)\n\t        parser.add_argument(\n\t            '--learn_sigma',\n\t            type=str2bool,\n\t            default=False)\n\t        parser.add_argument(\n", "            '--class_cond',\n\t            type=str2bool,\n\t            default=False)\n\t        parser.add_argument(\n\t            '--use_checkpoint',\n\t            type=str2bool,\n\t            default=False)\n\t        parser.add_argument(\n\t            '--attention_resolutions',\n\t            type=str,\n", "            default='16,8')\n\t        parser.add_argument(\n\t            '--num_heads',\n\t            type=int,\n\t            default=4)\n\t        parser.add_argument(\n\t            '--num_heads_upsample',\n\t            type=int,\n\t            default=-1)\n\t        parser.add_argument(\n", "            '--use_scale_shift_norm',\n\t            type=str2bool,\n\t            default=True)\n\t        parser.add_argument(\n\t            '--dropout',\n\t            type=float,\n\t            default=0.0)\n\t        parser.add_argument(\n\t            '--diffusion_steps',\n\t            type=int,\n", "            default=1000)\n\t        parser.add_argument(\n\t            '--sigma_small',\n\t            type=str2bool,\n\t            default=False)\n\t        parser.add_argument(\n\t            '--noise_schedule',\n\t            type=str,\n\t            default='linear')\n\t        parser.add_argument(\n", "            '--use_kl',\n\t            type=str2bool,\n\t            default=False)\n\t        parser.add_argument(\n\t            '--predict_xstart',\n\t            type=str2bool,\n\t            default=False)\n\t        parser.add_argument(\n\t            '--rescale_timesteps',\n\t            type=str2bool,\n", "            default=True)\n\t        parser.add_argument(\n\t            '--rescale_learned_sigmas',\n\t            type=str2bool,\n\t            default=True)\n\t        parser.add_argument(\n\t            '--timestep_respacing',\n\t            type=str,\n\t            default='')\n\t        parser.add_argument(\n", "            '--model_path',\n\t            type=str,\n\t            required=True)\n\t        parser.add_argument(\n\t            '--batch_size',\n\t            type=int,\n\t            default=100)\n\t        parser.add_argument(\n\t            '--use_ddim',\n\t            type=str2bool,\n", "            default=False)\n\t        parser.add_argument(\n\t            '--clip_denoised',\n\t            type=str2bool,\n\t            default=True)\n\t        parser.add_argument(\n\t            '--use_data_parallel',\n\t            type=str2bool,\n\t            default=True,\n\t            help='Whether to use DataParallel to speed up sampling')\n", "        return parser\n\t    def image_random_sampling(self, num_samples, size, prompts):\n\t        \"\"\"\n\t        Generates a specified number of random image samples based on a given\n\t        prompt and size using OpenAI's Image API.\n\t        Args:\n\t            num_samples (int):\n\t                The number of image samples to generate.\n\t            size (str, optional):\n\t                The size of the generated images in the format\n", "                \"widthxheight\". Options include \"256x256\", \"512x512\", and\n\t                \"1024x1024\".\n\t            prompts (List[str]):\n\t                The text prompts to generate images from. Each promot will be\n\t                used to generate num_samples/len(prompts) number of samples.\n\t        Returns:\n\t            numpy.ndarray:\n\t                A numpy array of shape [num_samples x width x height x\n\t                channels] with type np.uint8 containing the generated image\n\t                samples as numpy arrays.\n", "            numpy.ndarray:\n\t                A numpy array with length num_samples containing labels for\n\t                each image.\n\t        \"\"\"\n\t        width, height = list(map(int, size.split('x')))\n\t        if width != self._image_size or height != self._image_size:\n\t            raise ValueError(\n\t                f'width and height must be equal to {self._image_size}')\n\t        samples, labels = sample(\n\t            sampler=self._sampler,\n", "            start_t=0,\n\t            num_samples=num_samples,\n\t            batch_size=self._batch_size,\n\t            use_ddim=self._use_ddim,\n\t            image_size=self._image_size,\n\t            clip_denoised=self._clip_denoised,\n\t            class_cond=self._class_cond)\n\t        samples = _round_to_uint8((samples + 1.0) * 127.5)\n\t        samples = samples.transpose(0, 2, 3, 1)\n\t        torch.cuda.empty_cache()\n", "        return samples, labels\n\t    def image_variation(self, images, additional_info,\n\t                        num_variations_per_image, size, variation_degree):\n\t        \"\"\"\n\t        Generates a specified number of variations for each image in the input\n\t        array using OpenAI's Image Variation API.\n\t        Args:\n\t            images (numpy.ndarray):\n\t                A numpy array of shape [num_samples x width x height\n\t                x channels] containing the input images as numpy arrays of type\n", "                uint8.\n\t            additional_info (numpy.ndarray):\n\t                A numpy array with the first dimension equaling to\n\t                num_samples containing labels provided by\n\t                image_random_sampling.\n\t            num_variations_per_image (int):\n\t                The number of variations to generate for each input image.\n\t            size (str):\n\t                The size of the generated image variations in the\n\t                format \"widthxheight\". Options include \"256x256\", \"512x512\",\n", "                and \"1024x1024\".\n\t            variation_degree (int):\n\t                The diffusion step to add noise to the images to before running\n\t                the denoising steps. The value should between 0 and\n\t                timestep_respacing-1. 0 means the step that is closest to\n\t                noise. timestep_respacing-1 means the step that is closest to\n\t                clean image. A smaller value will result in more variation.\n\t        Returns:\n\t            numpy.ndarray:\n\t                A numpy array of shape [num_samples x num_variations_per_image\n", "                x width x height x channels] containing the generated image\n\t                variations as numpy arrays of type uint8.\n\t        \"\"\"\n\t        width, height = list(map(int, size.split('x')))\n\t        if width != self._image_size or height != self._image_size:\n\t            raise ValueError(\n\t                f'width and height must be equal to {self._image_size}')\n\t        images = images.astype(np.float32) / 127.5 - 1.0\n\t        images = images.transpose(0, 3, 1, 2)\n\t        variations = []\n", "        for _ in tqdm(range(num_variations_per_image)):\n\t            sub_variations = self._image_variation(\n\t                images=images,\n\t                labels=additional_info,\n\t                variation_degree=variation_degree)\n\t            variations.append(sub_variations)\n\t        variations = np.stack(variations, axis=1)\n\t        variations = _round_to_uint8((variations + 1.0) * 127.5)\n\t        variations = variations.transpose(0, 1, 3, 4, 2)\n\t        torch.cuda.empty_cache()\n", "        return variations\n\t    def _image_variation(self, images, labels, variation_degree):\n\t        samples, _ = sample(\n\t            sampler=self._sampler,\n\t            start_t=variation_degree,\n\t            start_image=torch.Tensor(images).to(dist_util.dev()),\n\t            labels=(None if not self._class_cond\n\t                    else torch.LongTensor(labels).to(dist_util.dev())),\n\t            num_samples=images.shape[0],\n\t            batch_size=self._batch_size,\n", "            use_ddim=self._use_ddim,\n\t            image_size=self._image_size,\n\t            clip_denoised=self._clip_denoised,\n\t            class_cond=self._class_cond)\n\t        return samples\n\tdef sample(sampler, num_samples, start_t, batch_size, use_ddim,\n\t           image_size, clip_denoised, class_cond,\n\t           start_image=None, labels=None):\n\t    all_images = []\n\t    all_labels = []\n", "    batch_cnt = 0\n\t    cnt = 0\n\t    while cnt < num_samples:\n\t        current_batch_size = \\\n\t            (batch_size if start_image is None\n\t             else min(batch_size,\n\t                      start_image.shape[0] - batch_cnt * batch_size))\n\t        shape = (current_batch_size, 3, image_size, image_size)\n\t        model_kwargs = {}\n\t        if class_cond:\n", "            if labels is None:\n\t                classes = torch.randint(\n\t                    low=0, high=NUM_CLASSES, size=(current_batch_size,),\n\t                    device=dist_util.dev()\n\t                )\n\t            else:\n\t                classes = labels[batch_cnt * batch_size:\n\t                                 (batch_cnt + 1) * batch_size]\n\t            model_kwargs[\"y\"] = classes\n\t        sample = sampler(\n", "            clip_denoised=clip_denoised,\n\t            model_kwargs=model_kwargs,\n\t            start_t=max(start_t, 0),\n\t            start_image=(None if start_image is None\n\t                         else start_image[batch_cnt * batch_size:\n\t                                          (batch_cnt + 1) * batch_size]),\n\t            use_ddim=use_ddim,\n\t            noise=torch.randn(*shape, device=dist_util.dev()),\n\t            image_size=image_size)\n\t        batch_cnt += 1\n", "        all_images.append(sample.detach().cpu().numpy())\n\t        if class_cond:\n\t            all_labels.append(classes.detach().cpu().numpy())\n\t        cnt += sample.shape[0]\n\t        logging.info(f\"Created {cnt} samples\")\n\t    all_images = np.concatenate(all_images, axis=0)\n\t    all_images = all_images[: num_samples]\n\t    if class_cond:\n\t        all_labels = np.concatenate(all_labels, axis=0)\n\t        all_labels = all_labels[: num_samples]\n", "    else:\n\t        all_labels = np.zeros(shape=(num_samples,))\n\t    return all_images, all_labels\n\tclass Sampler(torch.nn.Module):\n\t    \"\"\"\n\t    A wrapper around the model and diffusion modules that handles the entire\n\t    sampling process, so as to reduce the communiation rounds between GPUs when\n\t    using DataParallel.\n\t    \"\"\"\n\t    def __init__(self, model, diffusion):\n", "        super().__init__()\n\t        self._model = model\n\t        self._diffusion = diffusion\n\t    def forward(self, clip_denoised, model_kwargs, start_t, start_image,\n\t                use_ddim, noise, image_size):\n\t        sample_fn = (\n\t            self._diffusion.p_sample_loop if not use_ddim\n\t            else self._diffusion.ddim_sample_loop)\n\t        sample = sample_fn(\n\t            self._model,\n", "            (noise.shape[0], 3, image_size, image_size),\n\t            clip_denoised=clip_denoised,\n\t            model_kwargs=model_kwargs,\n\t            start_t=max(start_t, 0),\n\t            start_image=start_image,\n\t            noise=noise,\n\t            device=noise.device)\n\t        return sample\n"]}
{"filename": "apis/improved_diffusion/gaussian_diffusion.py", "chunked_list": ["\"\"\"\n\tThis code contains minor edits from the original code at\n\thttps://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n\tand\n\thttps://github.com/openai/improved-diffusion/blob/main/improved_diffusion/script_util.py\n\tto support sampling from the middle of the diffusion process with start_t and\n\tstart_image arguments.\n\t\"\"\"\n\timport torch as th\n\tfrom improved_diffusion.respace import SpacedDiffusion\n", "from improved_diffusion.respace import space_timesteps\n\tfrom improved_diffusion.gaussian_diffusion import _extract_into_tensor\n\tfrom improved_diffusion import gaussian_diffusion as gd\n\tclass SkippedSpacedDiffusion(SpacedDiffusion):\n\t    def p_sample_loop(\n\t        self,\n\t        model,\n\t        shape,\n\t        noise=None,\n\t        clip_denoised=True,\n", "        denoised_fn=None,\n\t        model_kwargs=None,\n\t        device=None,\n\t        progress=False,\n\t        start_t=0,\n\t        start_image=None,\n\t    ):\n\t        \"\"\"\n\t        Generate samples from the model.\n\t        :param model: the model module.\n", "        :param shape: the shape of the samples, (N, C, H, W).\n\t        :param noise: if specified, the noise from the encoder to sample.\n\t                      Should be of the same shape as `shape`.\n\t        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n\t        :param denoised_fn: if not None, a function which applies to the\n\t            x_start prediction before it is used to sample.\n\t        :param model_kwargs: if not None, a dict of extra keyword arguments to\n\t            pass to the model. This can be used for conditioning.\n\t        :param device: if specified, the device to create the samples on.\n\t                       If not specified, use a model parameter's device.\n", "        :param progress: if True, show a tqdm progress bar.\n\t        :return: a non-differentiable batch of samples.\n\t        \"\"\"\n\t        final = None\n\t        for sample in self.p_sample_loop_progressive(\n\t            model,\n\t            shape,\n\t            noise=noise,\n\t            clip_denoised=clip_denoised,\n\t            denoised_fn=denoised_fn,\n", "            model_kwargs=model_kwargs,\n\t            device=device,\n\t            progress=progress,\n\t            start_t=start_t,\n\t            start_image=start_image,\n\t        ):\n\t            final = sample\n\t        return final[\"sample\"]\n\t    def p_sample_loop_progressive(\n\t        self,\n", "        model,\n\t        shape,\n\t        noise=None,\n\t        clip_denoised=True,\n\t        denoised_fn=None,\n\t        model_kwargs=None,\n\t        device=None,\n\t        progress=False,\n\t        start_t=0,\n\t        start_image=None,\n", "    ):\n\t        \"\"\"\n\t        Generate samples from the model and yield intermediate samples from\n\t        each timestep of diffusion.\n\t        Arguments are the same as p_sample_loop().\n\t        Returns a generator over dicts, where each dict is the return value of\n\t        p_sample().\n\t        \"\"\"\n\t        if device is None:\n\t            device = next(model.parameters()).device\n", "        assert isinstance(shape, (tuple, list))\n\t        if noise is not None:\n\t            img = noise\n\t        else:\n\t            img = th.randn(*shape, device=device)\n\t        indices = list(range(self.num_timesteps))[::-1]\n\t        indices = indices[start_t:]\n\t        if start_image is not None:\n\t            t_batch = th.tensor([indices[0]] * img.shape[0], device=device)\n\t            img = self.q_sample(start_image, t=t_batch, noise=img)\n", "        if progress:\n\t            # Lazy import so that we don't depend on tqdm.\n\t            from tqdm.auto import tqdm\n\t            indices = tqdm(indices)\n\t        for i in indices:\n\t            t = th.tensor([i] * shape[0], device=device)\n\t            with th.no_grad():\n\t                out = self.p_sample(\n\t                    model,\n\t                    img,\n", "                    t,\n\t                    clip_denoised=clip_denoised,\n\t                    denoised_fn=denoised_fn,\n\t                    model_kwargs=model_kwargs,\n\t                )\n\t                yield out\n\t                img = out[\"sample\"]\n\t    def ddim_sample(\n\t        self,\n\t        model,\n", "        x,\n\t        t,\n\t        clip_denoised=True,\n\t        denoised_fn=None,\n\t        model_kwargs=None,\n\t        eta=0.0,\n\t    ):\n\t        \"\"\"\n\t        Sample x_{t-1} from the model using DDIM.\n\t        Same usage as p_sample().\n", "        \"\"\"\n\t        out = self.p_mean_variance(\n\t            model,\n\t            x,\n\t            t,\n\t            clip_denoised=clip_denoised,\n\t            denoised_fn=denoised_fn,\n\t            model_kwargs=model_kwargs,\n\t        )\n\t        # Usually our model outputs epsilon, but we re-derive it\n", "        # in case we used x_start or x_prev prediction.\n\t        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n\t        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n\t        alpha_bar_prev = _extract_into_tensor(\n\t            self.alphas_cumprod_prev, t, x.shape)\n\t        sigma = (\n\t            eta\n\t            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n\t            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n\t        )\n", "        # Equation 12.\n\t        noise = th.randn_like(x)\n\t        mean_pred = (\n\t            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n\t            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n\t        )\n\t        nonzero_mask = (\n\t            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n\t        )  # no noise when t == 0\n\t        sample = mean_pred + nonzero_mask * sigma * noise\n", "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\t    def ddim_reverse_sample(\n\t        self,\n\t        model,\n\t        x,\n\t        t,\n\t        clip_denoised=True,\n\t        denoised_fn=None,\n\t        model_kwargs=None,\n\t        eta=0.0,\n", "    ):\n\t        \"\"\"\n\t        Sample x_{t+1} from the model using DDIM reverse ODE.\n\t        \"\"\"\n\t        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n\t        out = self.p_mean_variance(\n\t            model,\n\t            x,\n\t            t,\n\t            clip_denoised=clip_denoised,\n", "            denoised_fn=denoised_fn,\n\t            model_kwargs=model_kwargs,\n\t        )\n\t        # Usually our model outputs epsilon, but we re-derive it\n\t        # in case we used x_start or x_prev prediction.\n\t        eps = (\n\t            _extract_into_tensor(\n\t                self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n\t            - out[\"pred_xstart\"]\n\t        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n", "        alpha_bar_next = _extract_into_tensor(\n\t            self.alphas_cumprod_next, t, x.shape)\n\t        # Equation 12. reversed\n\t        mean_pred = (\n\t            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n\t            + th.sqrt(1 - alpha_bar_next) * eps\n\t        )\n\t        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n\t    def ddim_sample_loop(\n\t        self,\n", "        model,\n\t        shape,\n\t        noise=None,\n\t        clip_denoised=True,\n\t        denoised_fn=None,\n\t        model_kwargs=None,\n\t        device=None,\n\t        progress=False,\n\t        eta=0.0,\n\t        start_t=0,\n", "        start_image=None,\n\t    ):\n\t        \"\"\"\n\t        Generate samples from the model using DDIM.\n\t        Same usage as p_sample_loop().\n\t        \"\"\"\n\t        final = None\n\t        for sample in self.ddim_sample_loop_progressive(\n\t            model,\n\t            shape,\n", "            noise=noise,\n\t            clip_denoised=clip_denoised,\n\t            denoised_fn=denoised_fn,\n\t            model_kwargs=model_kwargs,\n\t            device=device,\n\t            progress=progress,\n\t            eta=eta,\n\t            start_t=start_t,\n\t            start_image=start_image,\n\t        ):\n", "            final = sample\n\t        return final[\"sample\"]\n\t    def ddim_sample_loop_progressive(\n\t        self,\n\t        model,\n\t        shape,\n\t        noise=None,\n\t        clip_denoised=True,\n\t        denoised_fn=None,\n\t        model_kwargs=None,\n", "        device=None,\n\t        progress=False,\n\t        eta=0.0,\n\t        start_t=0,\n\t        start_image=None,\n\t    ):\n\t        \"\"\"\n\t        Use DDIM to sample from the model and yield intermediate samples from\n\t        each timestep of DDIM.\n\t        Same usage as p_sample_loop_progressive().\n", "        \"\"\"\n\t        if device is None:\n\t            device = next(model.parameters()).device\n\t        assert isinstance(shape, (tuple, list))\n\t        if noise is not None:\n\t            img = noise\n\t        else:\n\t            img = th.randn(*shape, device=device)\n\t        indices = list(range(self.num_timesteps))[::-1]\n\t        indices = indices[start_t:]\n", "        if start_image is not None:\n\t            t_batch = th.tensor([indices[0]] * img.shape[0], device=device)\n\t            img = self.q_sample(start_image, t=t_batch, noise=img)\n\t        if progress:\n\t            # Lazy import so that we don't depend on tqdm.\n\t            from tqdm.auto import tqdm\n\t            indices = tqdm(indices)\n\t        for i in indices:\n\t            t = th.tensor([i] * shape[0], device=device)\n\t            with th.no_grad():\n", "                out = self.ddim_sample(\n\t                    model,\n\t                    img,\n\t                    t,\n\t                    clip_denoised=clip_denoised,\n\t                    denoised_fn=denoised_fn,\n\t                    model_kwargs=model_kwargs,\n\t                    eta=eta,\n\t                )\n\t                yield out\n", "                img = out[\"sample\"]\n\tdef create_gaussian_diffusion(\n\t    *,\n\t    steps=1000,\n\t    learn_sigma=False,\n\t    sigma_small=False,\n\t    noise_schedule=\"linear\",\n\t    use_kl=False,\n\t    predict_xstart=False,\n\t    rescale_timesteps=False,\n", "    rescale_learned_sigmas=False,\n\t    timestep_respacing=\"\",\n\t):\n\t    betas = gd.get_named_beta_schedule(noise_schedule, steps)\n\t    if use_kl:\n\t        loss_type = gd.LossType.RESCALED_KL\n\t    elif rescale_learned_sigmas:\n\t        loss_type = gd.LossType.RESCALED_MSE\n\t    else:\n\t        loss_type = gd.LossType.MSE\n", "    if not timestep_respacing:\n\t        timestep_respacing = [steps]\n\t    return SkippedSpacedDiffusion(\n\t        use_timesteps=space_timesteps(steps, timestep_respacing),\n\t        betas=betas,\n\t        model_mean_type=(\n\t            gd.ModelMeanType.EPSILON if not predict_xstart\n\t            else gd.ModelMeanType.START_X\n\t        ),\n\t        model_var_type=(\n", "            (\n\t                gd.ModelVarType.FIXED_LARGE\n\t                if not sigma_small\n\t                else gd.ModelVarType.FIXED_SMALL\n\t            )\n\t            if not learn_sigma\n\t            else gd.ModelVarType.LEARNED_RANGE\n\t        ),\n\t        loss_type=loss_type,\n\t        rescale_timesteps=rescale_timesteps,\n", "    )\n"]}
{"filename": "apis/improved_diffusion/unet.py", "chunked_list": ["\"\"\"\n\tThis code contains minor edits from the original code at\n\thttps://github.com/openai/improved-diffusion/blob/main/improved_diffusion/unet.py\n\tand\n\thttps://github.com/openai/improved-diffusion/blob/main/improved_diffusion/script_util.py\n\tto avoid calling self.input_blocks.parameters() in the original code, which is\n\tnot supported by DataParallel.\n\t\"\"\"\n\timport torch\n\tfrom improved_diffusion.unet import UNetModel\n", "from improved_diffusion.script_util import NUM_CLASSES\n\tclass FP32UNetModel(UNetModel):\n\t    @property\n\t    def inner_dtype(self):\n\t        return torch.float32\n\tdef create_model(\n\t    image_size,\n\t    num_channels,\n\t    num_res_blocks,\n\t    learn_sigma,\n", "    class_cond,\n\t    use_checkpoint,\n\t    attention_resolutions,\n\t    num_heads,\n\t    num_heads_upsample,\n\t    use_scale_shift_norm,\n\t    dropout,\n\t):\n\t    if image_size == 256:\n\t        channel_mult = (1, 1, 2, 2, 4, 4)\n", "    elif image_size == 64:\n\t        channel_mult = (1, 2, 3, 4)\n\t    elif image_size == 32:\n\t        channel_mult = (1, 2, 2, 2)\n\t    else:\n\t        raise ValueError(f\"unsupported image size: {image_size}\")\n\t    attention_ds = []\n\t    for res in attention_resolutions.split(\",\"):\n\t        attention_ds.append(image_size // int(res))\n\t    return FP32UNetModel(\n", "        in_channels=3,\n\t        model_channels=num_channels,\n\t        out_channels=(3 if not learn_sigma else 6),\n\t        num_res_blocks=num_res_blocks,\n\t        attention_resolutions=tuple(attention_ds),\n\t        dropout=dropout,\n\t        channel_mult=channel_mult,\n\t        num_classes=(NUM_CLASSES if class_cond else None),\n\t        use_checkpoint=use_checkpoint,\n\t        num_heads=num_heads,\n", "        num_heads_upsample=num_heads_upsample,\n\t        use_scale_shift_norm=use_scale_shift_norm,\n\t    )\n"]}
{"filename": "apis/improved_diffusion/__init__.py", "chunked_list": []}
{"filename": "data/get_camelyon17.py", "chunked_list": ["from wilds import get_dataset\n\tfrom tqdm import tqdm\n\timport os\n\tdef save(dataset, path):\n\t    if not os.path.exists(path):\n\t        os.makedirs(path)\n\t    for i in tqdm(range(len(dataset))):\n\t        image, label, _ = dataset[i]\n\t        image.save(f'{path}/{label.item()}_{i}.png')\n\tif __name__ == '__main__':\n", "    dataset = get_dataset(dataset=\"camelyon17\", download=True)\n\t    train_data = dataset.get_subset(\"train\")\n\t    val_data = dataset.get_subset(\"val\")\n\t    test_data = dataset.get_subset(\"test\")\n\t    save(train_data, 'camelyon17_train')\n\t    save(val_data, 'camelyon17_test')\n\t    save(test_data, 'camelyon17_val')\n"]}
{"filename": "data/get_cifar10.py", "chunked_list": ["import os\n\timport tempfile\n\timport torchvision\n\tfrom tqdm.auto import tqdm\n\tCLASSES = (\n\t    \"plane\",\n\t    \"car\",\n\t    \"bird\",\n\t    \"cat\",\n\t    \"deer\",\n", "    \"dog\",\n\t    \"frog\",\n\t    \"horse\",\n\t    \"ship\",\n\t    \"truck\",\n\t)\n\tdef main():\n\t    for split in [\"train\", \"test\"]:\n\t        out_dir = f\"cifar10_{split}\"\n\t        if os.path.exists(out_dir):\n", "            print(f\"skipping split {split} since {out_dir} already exists.\")\n\t            continue\n\t        print(\"downloading...\")\n\t        with tempfile.TemporaryDirectory() as tmp_dir:\n\t            dataset = torchvision.datasets.CIFAR10(\n\t                root=tmp_dir, train=split == \"train\", download=True\n\t            )\n\t        print(\"dumping images...\")\n\t        os.mkdir(out_dir)\n\t        for i in tqdm(range(len(dataset))):\n", "            image, label = dataset[i]\n\t            filename = os.path.join(out_dir, f\"{CLASSES[label]}_{i:05d}.png\")\n\t            image.save(filename)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "dpsda/feature_extractor.py", "chunked_list": ["import imageio\n\timport cleanfid\n\timport os\n\timport shutil\n\timport numpy as np\n\tfrom tqdm import tqdm\n\timport torch\n\tfrom cleanfid.resize import make_resizer\n\tdef round_to_uint8(image):\n\t    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)\n", "def extract_features(\n\t        data, mode='clean', device=torch.device('cuda'), use_dataparallel=True,\n\t        num_workers=12, batch_size=5000, custom_fn_resize=None, description='',\n\t        verbose=True, custom_image_tranform=None, tmp_folder='tmp_feature',\n\t        model_name=\"inception_v3\", res=32):\n\t    if model_name == 'original':\n\t        return data.reshape((data.shape[0], -1)).astype(np.float32)\n\t    elif model_name == \"inception_v3\":\n\t        feat_model = cleanfid.features.build_feature_extractor(\n\t            mode=mode,\n", "            device=device,\n\t            use_dataparallel=use_dataparallel\n\t        )\n\t    elif model_name == \"clip_vit_b_32\":\n\t        from cleanfid.clip_features import CLIP_fx, img_preprocess_clip\n\t        clip_fx = CLIP_fx(\"ViT-B/32\", device=device)\n\t        feat_model = clip_fx\n\t        custom_fn_resize = img_preprocess_clip\n\t    else:\n\t        raise Exception(f'Unknown model_name {model_name}')\n", "    if not os.path.exists(tmp_folder):\n\t        os.makedirs(tmp_folder)\n\t    else:\n\t        shutil.rmtree(tmp_folder, ignore_errors=False, onerror=None)\n\t        os.makedirs(tmp_folder)\n\t    assert data.dtype == np.uint8\n\t    resizer = make_resizer(\n\t        library='PIL', quantize_after=False, filter='bicubic',\n\t        output_size=(res, res))\n\t    # TODO: in memory processing for computing features.\n", "    for i in tqdm(range(data.shape[0])):\n\t        image = round_to_uint8(resizer(data[i]))\n\t        imageio.imsave(os.path.join(tmp_folder, f'{i}.png'), image)\n\t    files = [os.path.join(tmp_folder, f'{i}.png')\n\t             for i in range(data.shape[0])]\n\t    np_feats = cleanfid.fid.get_files_features(\n\t        l_files=files,\n\t        model=feat_model,\n\t        num_workers=num_workers,\n\t        batch_size=batch_size,\n", "        device=device,\n\t        mode=mode,\n\t        custom_fn_resize=custom_fn_resize,\n\t        custom_image_tranform=custom_image_tranform,\n\t        description=description,\n\t        fdir=tmp_folder,\n\t        verbose=verbose)\n\t    return np_feats.astype(np.float32)\n"]}
{"filename": "dpsda/metrics.py", "chunked_list": ["import os\n\timport shutil\n\timport numpy as np\n\timport imageio\n\tfrom cleanfid import fid\n\tfrom cleanfid.resize import make_resizer\n\timport torch\n\timport cleanfid\n\tfrom cleanfid.features import build_feature_extractor\n\tfrom cleanfid.fid import get_folder_features\n", "from tqdm import tqdm\n\tdef round_to_uint8(image):\n\t    return np.around(np.clip(image, a_min=0, a_max=255)).astype(np.uint8)\n\tdef cleanfid_make_custom_stats(\n\t        name, fdir, split, res, num=None, mode=\"clean\",\n\t        model_name=\"inception_v3\", num_workers=0, batch_size=64,\n\t        device=torch.device(\"cuda\"), verbose=True):\n\t    stats_folder = os.path.join(os.path.dirname(cleanfid.__file__), \"stats\")\n\t    os.makedirs(stats_folder, exist_ok=True)\n\t    if model_name == \"inception_v3\":\n", "        model_modifier = \"\"\n\t    else:\n\t        model_modifier = \"_\" + model_name\n\t    outf = os.path.join(\n\t        stats_folder,\n\t        f\"{name}_{mode}{model_modifier}_{split}_{res}.npz\".lower())\n\t    # if the custom stat file already exists\n\t    if os.path.exists(outf):\n\t        msg = f\"The statistics file {name} already exists. \"\n\t        msg += \"Use remove_custom_stats function to delete it first.\"\n", "        print(msg)\n\t        return\n\t    if model_name == \"inception_v3\":\n\t        feat_model = build_feature_extractor(mode, device)\n\t        custom_fn_resize = None\n\t        custom_image_tranform = None\n\t    elif model_name == \"clip_vit_b_32\":\n\t        from cleanfid.clip_features import CLIP_fx, img_preprocess_clip\n\t        clip_fx = CLIP_fx(\"ViT-B/32\")\n\t        feat_model = clip_fx\n", "        custom_fn_resize = img_preprocess_clip\n\t        custom_image_tranform = None\n\t    else:\n\t        raise ValueError(\n\t            f\"The entered model name - {model_name} was not recognized.\")\n\t    # get all inception features for folder images\n\t    np_feats = get_folder_features(\n\t        fdir, feat_model, num_workers=num_workers, num=num,\n\t        batch_size=batch_size, device=device, verbose=verbose,\n\t        mode=mode, description=f\"custom stats: {os.path.basename(fdir)} : \",\n", "        custom_image_tranform=custom_image_tranform,\n\t        custom_fn_resize=custom_fn_resize)\n\t    mu = np.mean(np_feats, axis=0)\n\t    sigma = np.cov(np_feats, rowvar=False)\n\t    print(f\"saving custom FID stats to {outf}\")\n\t    np.savez_compressed(outf, mu=mu, sigma=sigma)\n\tdef make_fid_stats(samples, dataset, dataset_res, dataset_split,\n\t                   tmp_folder='tmp_fid', batch_size=5000,\n\t                   model_name='inception_v3'):\n\t    if not os.path.exists(tmp_folder):\n", "        os.makedirs(tmp_folder)\n\t    else:\n\t        shutil.rmtree(tmp_folder, ignore_errors=False, onerror=None)\n\t        os.makedirs(tmp_folder)\n\t    assert samples.dtype == np.uint8\n\t    resizer = make_resizer(\n\t        library='PIL', quantize_after=False, filter='bicubic',\n\t        output_size=(dataset_res, dataset_res))\n\t    # TODO: in memory processing for computing stats.\n\t    for i in tqdm(range(samples.shape[0])):\n", "        image = round_to_uint8(resizer(samples[i]))\n\t        imageio.imsave(os.path.join(tmp_folder, f'{i}.png'), image)\n\t    cleanfid_make_custom_stats(\n\t        name=dataset, fdir=tmp_folder, split=dataset_split, res=dataset_res,\n\t        batch_size=batch_size, model_name=model_name)\n\tdef compute_fid(samples, tmp_folder='tmp_fid', dataset='cifar10',\n\t                dataset_res=32, dataset_split='train', batch_size=5000,\n\t                num_fid_samples=10000, model_name='inception_v3'):\n\t    if not os.path.exists(tmp_folder):\n\t        os.makedirs(tmp_folder)\n", "    else:\n\t        shutil.rmtree(tmp_folder, ignore_errors=False, onerror=None)\n\t        os.makedirs(tmp_folder)\n\t    if num_fid_samples == samples.shape[0]:\n\t        ids = np.arange(samples.shape[0])\n\t    elif num_fid_samples < samples.shape[0]:\n\t        ids = np.random.choice(\n\t            samples.shape[0], size=num_fid_samples, replace=False)\n\t    else:\n\t        ids = np.random.choice(\n", "            samples.shape[0], size=num_fid_samples, replace=True)\n\t    samples = samples[ids]\n\t    assert samples.dtype == np.uint8\n\t    resizer = make_resizer(\n\t        library='PIL', quantize_after=False, filter='bicubic',\n\t        output_size=(dataset_res, dataset_res))\n\t    # TODO: in memory processing for computing FID.\n\t    for i in range(samples.shape[0]):\n\t        image = round_to_uint8(resizer(samples[i]))\n\t        imageio.imsave(os.path.join(tmp_folder, f'{i}.png'), image)\n", "    score = fid.compute_fid(\n\t        tmp_folder, dataset_name=dataset, dataset_split=dataset_split,\n\t        dataset_res=dataset_res, batch_size=batch_size, model_name=model_name)\n\t    return score\n"]}
{"filename": "dpsda/arg_utils.py", "chunked_list": ["import argparse\n\tdef str2bool(v):\n\t    # From:\n\t    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n\t    if isinstance(v, bool):\n\t        return v\n\t    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n\t        return True\n\t    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n\t        return False\n", "    else:\n\t        raise argparse.ArgumentTypeError('Boolean value expected.')"]}
{"filename": "dpsda/dataset.py", "chunked_list": ["from PIL import Image\n\timport blobfile as bf\n\tfrom torch.utils.data import Dataset\n\tdef _list_image_files_recursively(data_dir):\n\t    results = []\n\t    for entry in sorted(bf.listdir(data_dir)):\n\t        full_path = bf.join(data_dir, entry)\n\t        ext = entry.split('.')[-1]\n\t        if \".\" in entry and ext.lower() in ['jpg', 'jpeg', 'png', 'gif']:\n\t            results.append(full_path)\n", "        elif bf.isdir(full_path):\n\t            results.extend(_list_image_files_recursively(full_path))\n\t    return results\n\tclass ImageDataset(Dataset):\n\t    def __init__(self, folder, transform):\n\t        super().__init__()\n\t        self.folder = folder\n\t        self.transform = transform\n\t        self.local_images = _list_image_files_recursively(folder)\n\t        class_names = [bf.basename(path).split('_')[0]\n", "                       for path in self.local_images]\n\t        sorted_classes = {x: i for i, x in enumerate(sorted(set(class_names)))}\n\t        self.local_classes = [sorted_classes[x] for x in class_names]\n\t    def __len__(self):\n\t        return len(self.local_images)\n\t    def __getitem__(self, idx):\n\t        path = self.local_images[idx]\n\t        with bf.BlobFile(path, 'rb') as f:\n\t            pil_image = Image.open(f)\n\t            pil_image.load()\n", "        arr = self.transform(pil_image)\n\t        label = self.local_classes[idx]\n\t        return arr, label\n"]}
{"filename": "dpsda/__init__.py", "chunked_list": []}
{"filename": "dpsda/data_loader.py", "chunked_list": ["import torch\n\timport torchvision.transforms as T\n\tfrom torch.utils.data import DataLoader\n\timport numpy as np\n\timport logging\n\tfrom .dataset import ImageDataset\n\tdef load_data(data_dir, batch_size, image_size, class_cond,\n\t              num_private_samples):\n\t    transform = T.Compose([\n\t        T.Resize(image_size),\n", "        T.CenterCrop(image_size),\n\t        T.ToTensor()\n\t    ])\n\t    dataset = ImageDataset(folder=data_dir, transform=transform)\n\t    loader = DataLoader(dataset, batch_size, shuffle=False, num_workers=10,\n\t                        pin_memory=torch.cuda.is_available(), drop_last=False)\n\t    all_samples = []\n\t    all_labels = []\n\t    cnt = 0\n\t    for batch, cond in loader:\n", "        all_samples.append(batch.cpu().numpy())\n\t        if class_cond:\n\t            all_labels.append(cond.cpu().numpy())\n\t        cnt += batch.shape[0]\n\t        logging.info(f'loaded {cnt} samples')\n\t        if batch.shape[0] < batch_size:\n\t            logging.info('WARNING: containing incomplete batch. Please check'\n\t                         'num_private_samples')\n\t        if cnt >= num_private_samples:\n\t            break\n", "    all_samples = np.concatenate(all_samples, axis=0)\n\t    all_samples = all_samples[:num_private_samples]\n\t    all_samples = np.around(np.clip(\n\t        all_samples * 255, a_min=0, a_max=255)).astype(np.uint8)\n\t    all_samples = np.transpose(all_samples, (0, 2, 3, 1))\n\t    if class_cond:\n\t        all_labels = np.concatenate(all_labels, axis=0)\n\t        all_labels = all_labels[:num_private_samples]\n\t    else:\n\t        all_labels = np.zeros(shape=all_samples.shape[0], dtype=np.int64)\n", "    return all_samples, all_labels\n"]}
{"filename": "dpsda/pytorch_utils.py", "chunked_list": ["import torch\n\tdef dev():\n\t    \"\"\"\n\t    Get the device to use for torch.distributed.\n\t    \"\"\"\n\t    if torch.cuda.is_available():\n\t        return torch.device('cuda')\n\t    return torch.device('cpu')\n"]}
{"filename": "dpsda/logging.py", "chunked_list": ["import logging\n\tdef setup_logging(log_file):\n\t    log_formatter = logging.Formatter(\n\t        fmt=('%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  '\n\t             '%(message)s'),\n\t        datefmt='%m/%d/%Y %H:%M:%S %p')\n\t    root_logger = logging.getLogger()\n\t    root_logger.setLevel(logging.DEBUG)\n\t    console_handler = logging.StreamHandler()\n\t    console_handler.setFormatter(log_formatter)\n", "    root_logger.addHandler(console_handler)\n\t    file_handler = logging.FileHandler(log_file)\n\t    file_handler.setFormatter(log_formatter)\n\t    root_logger.addHandler(file_handler)\n\t    pil_logger = logging.getLogger('PIL')\n\t    pil_logger.setLevel(logging.INFO)\n"]}
{"filename": "dpsda/dp_counter.py", "chunked_list": ["import faiss\n\timport logging\n\timport numpy as np\n\tfrom collections import Counter\n\timport torch\n\tdef dp_nn_histogram(public_features, private_features, noise_multiplier,\n\t                    num_packing=1, num_nearest_neighbor=1, mode='L2',\n\t                    threshold=0.0):\n\t    assert public_features.shape[0] % num_packing == 0\n\t    num_true_public_features = public_features.shape[0] // num_packing\n", "    faiss_res = faiss.StandardGpuResources()\n\t    if mode == 'L2':\n\t        index = faiss.IndexFlatL2(public_features.shape[1])\n\t    elif mode == 'IP':\n\t        index = faiss.IndexFlatIP(public_features.shape[1])\n\t    else:\n\t        raise Exception(f'Unknown mode {mode}')\n\t    if torch.cuda.is_available():\n\t        index = faiss.index_cpu_to_gpu(faiss_res, 0, index)\n\t    index.add(public_features)\n", "    logging.info(f'Number of samples in index: {index.ntotal}')\n\t    _, ids = index.search(private_features, k=num_nearest_neighbor)\n\t    logging.info('Finished search')\n\t    counter = Counter(list(ids.flatten()))\n\t    count = np.zeros(shape=num_true_public_features)\n\t    for k in counter:\n\t        count[k % num_true_public_features] += counter[k]\n\t    logging.info(f'Clean count sum: {np.sum(count)}')\n\t    logging.info(f'Clean count num>0: {np.sum(count > 0)}')\n\t    logging.info(f'Largest clean counters: {sorted(count)[::-1][:50]}')\n", "    count = np.asarray(count)\n\t    clean_count = count.copy()\n\t    count += (np.random.normal(size=len(count)) * np.sqrt(num_nearest_neighbor)\n\t              * noise_multiplier)\n\t    logging.info(f'Noisy count sum: {np.sum(count)}')\n\t    logging.info(f'Noisy count num>0: {np.sum(count > 0)}')\n\t    logging.info(f'Largest noisy counters: {sorted(count)[::-1][:50]}')\n\t    count = np.clip(count, a_min=threshold, a_max=None)\n\t    count = count - threshold\n\t    logging.info(f'Clipped noisy count sum: {np.sum(count)}')\n", "    logging.info(f'Clipped noisy count num>0: {np.sum(count > 0)}')\n\t    logging.info(f'Clipped largest noisy counters: {sorted(count)[::-1][:50]}')\n\t    return count, clean_count\n"]}
