{"filename": "tests/test_permutations.py", "chunked_list": ["import numpy as np\n\timport unittest\n\tfrom hypothesis import given\n\tfrom tests.strategies import objects, adapted_function, finite_functions, permutations, parallel_permutations, parallel_arrows\n\tfrom yarrow.numpy import FiniteFunction\n\tfrom yarrow.finite_function import argsort\n\tfrom tests.util import sorts\n\t# Invert a permutation\n\tdef invert(p):\n\t    return argsort(p)\n", "# Ensure the invert function works(!)\n\t@given(p=permutations())\n\tdef test_invert(p):\n\t    assert invert(p) >> p == FiniteFunction.identity(p.source)\n\t    assert p >> invert(p) == FiniteFunction.identity(p.source)\n\t# Definition A.2 \"Sorting\"\n\t@given(f=finite_functions())\n\tdef test_argsort_matches_definition(f):\n\t    p = f.argsort()\n\t    y = p >> f\n", "    if len(y.table) <= 1:\n\t        return None\n\t    assert sorts(p, f)\n\t# Proposition A.3\n\t# we test something slightly weaker; instead of a general monomorphism we just\n\t# use a permutation.\n\t# TODO: generate a monomorphism by just `spreading out' values of the identity\n\t# function, then permuting?\n\t@given(p=permutations())\n\tdef test_argsort_monomorphism_strictly_increasing(p):\n", "    q = p.argsort()\n\t    y = q >> p\n\t    if len(y.table) <= 1:\n\t        return None\n\t    assert sorts(q, p, strict=True)\n\t# TODO: test uniqueness A.4 (?)\n\t# Proposition A.5\n\t@given(fpq=adapted_function(source=None, target=None))\n\tdef test_sort_by_permuted_key(fpq):\n\t    f, p, q = fpq\n", "    s = f.argsort()\n\t    assert sorts(s >> invert(p), p >> f)\n\t# Proposition A.6\n\t# Again using permutations instead of monomorphisms;\n\t# see test_argsort_monomorphism_strictly_increasing\n\t@given(fp=parallel_permutations())\n\tdef test_sort_pf_equals_sortf_p(fp):\n\t    f, p = fp\n\t    assert (p >> f).argsort() == (f.argsort() >> invert(p))\n\t# interleave and its inverse cancel on both sides\n", "@given(n=objects)\n\tdef test_interleave_inverse(n: int):\n\t    a = FiniteFunction.interleave(n)\n\t    b = FiniteFunction.cointerleave(n)\n\t    i = FiniteFunction.identity(2*n)\n\t    assert a >> b == i\n\t    assert b >> a == i\n\t# Cointerleaving is the opposite of interleaving, and has a more meaningful\n\t# interpretation which we can test easily.\n\t@given(fg=parallel_arrows())\n", "def test_cointerleave(fg):\n\t    f, g = fg\n\t    N = f.source\n\t    assert N == g.source # should be true because parallel_arrows\n\t    h = (f @ g)\n\t    a = FiniteFunction.cointerleave(N)\n\t    r = a >> h\n\t    Array = type(f)._Array\n\t    assert Array.all(r.table[0::2] == h.table[0:N])\n\t    assert Array.all(r.table[1::2] == h.table[N:])\n"]}
{"filename": "tests/test_optic.py", "chunked_list": ["from hypothesis import given\n\tfrom dataclasses import dataclass\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\t# testing against the numpy backend\n\tfrom yarrow.numpy import *\n\tfrom yarrow.functor.functor import *\n\tfrom yarrow.functor.optic import *\n\t# Assume a finite signature Σ.\n\t#\n", "# For each operation Σ₁\n\t#\n\t#   f : Ai → Aj\n\t#\n\t# Define residuals\n\t#\n\t#   M_f\n\t#\n\t# And two operations in Ω₁ = Σ₁*2:\n\t#\n", "#   fwd(f) : fwd(Ai) ● M_f → fwd(Aj)\n\t#   rev(f) : M_f ● rev(Ai) → rev(Aj)\n\t# \n\t# with fwd(f) = 2*f\n\t#      rev(f) = 2*f+1\n\t# \n\t# The types are determined by the action of fwd/rev on *objects*.\n\t@dataclass\n\tclass FiniteOpticFunctor(FrobeniusOpticFunctor):\n\t    \"\"\" Optic Functor whose action on generating objects is given, and whose action on operations is ... \"\"\"\n", "    # Assume categories C presented by Σ and D presented by Ω.\n\t    # objects in Σ₀ are mapped to lists Ω₀*\n\t    wn_fwd: IndexedCoproduct # Σ₀ → Ω₀*\n\t    wn_rev: IndexedCoproduct # Σ₀ → Ω₀*\n\t    # Σ₀ → Ω₁ where Ω₁ = 2 * Σ₀\n\t    # Residuals map generating operations into some choice of object in D\n\t    # residuals : Σ₁ → Ω₀*\n\t    #   sources : Σ₁ → Ks\n\t    #   targets : Σ₁ → Kt\n\t    #   values  : Σ₁ → Ω₀\n", "    _residuals: IndexedCoproduct\n\t    def __post_init__(self):\n\t        assert len(self.wn_fwd) == len(self.wn_rev)\n\t        assert self.wn_fwd.values.target == self._residuals.values.target\n\t        assert self.wn_rev.values.target == self._residuals.values.target\n\t    def map_fwd_objects(self, objects) -> IndexedCoproduct:\n\t        # wn_fwd.sources associates to each object i ∈ Σ₀ a size k(i).\n\t        # Thus to get the sizes of the coproducts, we just compose objects >> self.wn_fwd.sources\n\t        assert objects.target == len(self.wn_fwd)\n\t        result = IndexedCoproduct(\n", "                sources=objects >> self.wn_fwd.sources,\n\t                values = FiniteFunction(self.wn_fwd.values.target, self.wn_fwd.coproduct(objects).table))\n\t        return result\n\t    def map_rev_objects(self, objects) -> IndexedCoproduct:\n\t        assert objects.target == len(self.wn_rev)\n\t        return IndexedCoproduct(\n\t                sources=objects >> self.wn_rev.sources,\n\t                values = FiniteFunction(self.wn_rev.values.target, self.wn_rev.coproduct(objects).table))\n\t    def residuals(self, ops: Operations) -> IndexedCoproduct:\n\t        assert ops.xn.target == len(self._residuals)\n", "        return IndexedCoproduct(\n\t                sources=FiniteFunction(None, (ops.xn >> self._residuals.sources).table),\n\t                values=FiniteFunction(self.wn_fwd.values.target, self._residuals.coproduct(ops.xn).table))\n\t    def map_fwd_operations(self, ops: Operations) -> Diagram:\n\t        # Each operation f maps to the singleton diagram \n\t        #\n\t        #   2*f : F(A) → F(B) ● M_f\n\t        #\n\t        # We could parallelize this, but it's easier to just use a loop for testing!\n\t        omega = ops.xn.target * 2\n", "        diagrams = []\n\t        for (x, a, b), M in zip(ops, self.residuals(ops)):\n\t            FA = self.map_fwd_objects(a).values\n\t            FB = self.map_fwd_objects(b).values\n\t            xn = FiniteFunction.singleton(2*x, omega)\n\t            diagrams.append(Diagram.singleton(FA, FB + M, xn))\n\t        wn = None \n\t        xn = None\n\t        if len(diagrams) == 0:\n\t            wn = FiniteFunction(self.wn_fwd.values.target, [])\n", "            xn = FiniteFunction(ops.xn.target, [])\n\t        return Diagram.tensor_list(diagrams, wn=wn, xn=xn), FiniteFunction(None, [ len(d.type[1]) for d in diagrams ])\n\t    def map_rev_operations(self, ops: Operations) -> Diagram:\n\t        omega = ops.xn.target * 2\n\t        diagrams = []\n\t        for (x, a, b), M in zip(ops, self.residuals(ops)):\n\t            FA = self.map_rev_objects(a).values\n\t            FB = self.map_rev_objects(b).values\n\t            xn = FiniteFunction.singleton(2*x+1, omega)\n\t            diagrams.append(Diagram.singleton(M + FB, FA, xn))\n", "        wn = None \n\t        xn = None\n\t        if len(diagrams) == 0:\n\t            wn = FiniteFunction(self.wn_rev.values.target, [])\n\t            xn = FiniteFunction(ops.xn.target, [])\n\t        # return Diagram.tensor_list(diagrams, wn=wn, xn=xn)\n\t        return Diagram.tensor_list(diagrams, wn=wn, xn=xn), FiniteFunction(None, [ len(d.type[0]) for d in diagrams ])\n\t@st.composite\n\tdef finite_optic_functor(draw):\n\t    sigma_0, omega_0 = draw(arrow_type())\n", "    wn_fwd  = draw(indexed_coproducts(N=sigma_0, Obj=omega_0))\n\t    wn_fwd.sources.target = None # bit of a hack\n\t    wn_rev  = draw(indexed_coproducts(N=sigma_0, Obj=omega_0))\n\t    wn_rev.sources.target = None # bit of a hack\n\t    sigma_1, _ = draw(arrow_type(target=omega_0))\n\t    residuals = draw(segmented_finite_functions(N=sigma_1, Obj=omega_0))\n\t    return FiniteOpticFunctor(wn_fwd, wn_rev, residuals)\n\t@st.composite\n\tdef finite_optic_functor_and_diagram(draw):\n\t    # F : Free_Σ → Free_Ω\n", "    #   wn_fwd    : Σ₀ → Ω₀*\n\t    #   wn_rev    : Σ₀ → Ω₀*\n\t    #   residuals : Σ₁ → Ω₀*\n\t    F = draw(finite_optic_functor())\n\t    sigma_0 = len(F.wn_fwd)\n\t    sigma_1 = len(F._residuals)\n\t    omega_0 = F.wn_fwd.values.target\n\t    assert omega_0 == F.wn_rev.values.target\n\t    assert omega_0 == F._residuals.values.target\n\t    d = draw(diagrams(Obj=sigma_0, Arr=sigma_1))\n", "    return F, d\n\t@given(finite_optic_functor_and_diagram())\n\tdef test_optic_functor(Fd):\n\t    F, d = Fd\n\t    F.map_arrow(d)\n"]}
{"filename": "tests/test_bipartite_multigraph.py", "chunked_list": ["import pytest\n\timport numpy as np\n\tfrom yarrow.numpy import *\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\t################################################################################\n\t# Applying coequalizers to bipartite multigraphs\n\t@given(fg=parallel_arrows())\n\tdef test_universal_identity(fg):\n", "    f, g = fg\n\t    q = f.coequalizer(g)\n\t    u = universal(q, q)\n\t    assert u == FiniteFunction.identity(q.target)\n\t# A custom strategy for the test_universal_permutation test.\n\t@st.composite\n\tdef coequalizer_and_permutation(draw, source=None, target=None):\n\t    f, g = draw(parallel_arrows(source, target))\n\t    q = f.coequalizer(g)\n\t    p = draw(permutations(n=q.target))\n", "    return f, g, q, p\n\t@given(fgqp=coequalizer_and_permutation())\n\tdef test_universal_permutation(fgqp):\n\t    f, g, q1, p = fgqp\n\t    q2 = q1 >> p # a permuted coequalizer still coequalizes!\n\t    u = universal(q1, q2)\n\t    assert q1 >> u == q2\n\t################################################################################\n\t# Discrete BPMGs\n\t@given(wn=finite_functions(source=0), xn=finite_functions(source=0))\n", "def test_empty(wn, xn):\n\t    # constructor should run with no errors.\n\t    BipartiteMultigraph.empty(wn, xn)\n\t@given(wn=finite_functions(), xn=finite_functions(source=0))\n\tdef test_discrete(wn, xn):\n\t    # constructor should run with no errors.\n\t    BipartiteMultigraph.discrete(wn, xn)\n\t# Custom strategy for test_discrete_coequalize_wires\n\t@st.composite\n\tdef coequalizer_and_permutation(draw, source=None, target=None, ob=None):\n", "    f, g = draw(parallel_arrows(source, target))\n\t    W = f.target\n\t    wn   = draw(finite_functions(source=W, target=ob))\n\t    print(f'target: {W}')\n\t    print(f'wn    : {wn}')\n\t    xn   = draw(finite_functions(source=0))\n\t    return f, g, wn, xn\n\t# Given:\n\t#   f, g : A → W\n\t#   wn : W → 1\n", "#   xn : 0 → Σ₁\n\t#   D: discrete(wn, xn)\n\t#   q = FF.coequalizer(f, g) : wn.source → Q\n\t# Ensure that wires can be coequalized.\n\t# NOTE: This only handles the PROP case when Σ₀ = 1\n\t@given(cap=coequalizer_and_permutation(ob=1))\n\tdef test_discrete_coequalize_unityped_wires(cap):\n\t    f, g, wn, xn = cap\n\t    q = f.coequalizer(g)\n\t    D = BipartiteMultigraph.discrete(wn, xn)\n", "    E = D.coequalize_wires(q)\n\t    assert E.wn.source == q.target\n\t# TODO: FIXME: Need to test coequalize_wires with genuine label-preserving maps!\n"]}
{"filename": "tests/test_diagram.py", "chunked_list": ["import pytest\n\timport unittest\n\timport numpy as np\n\tfrom yarrow.numpy import *\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\t################################################################################\n\t# Primitives\n\t@given(wn=finite_functions(source=0), xn=finite_functions(source=0))\n", "def test_empty(wn, xn):\n\t    # Should run without errors\n\t    e = Diagram.empty(wn, xn)\n\t    (A, B) = e.type\n\t    assert A == FiniteFunction.initial(wn.target)\n\t    assert B == FiniteFunction.initial(wn.target)\n\t    assert e.s == FiniteFunction.initial(0)\n\t    assert e.t == FiniteFunction.initial(0)\n\t@given(wn=finite_functions(), xn=finite_functions(source=0))\n\tdef test_identity(wn: FiniteFunction, xn: FiniteFunction):\n", "    # Should run without errors\n\t    d = Diagram.identity(wn, xn)\n\t    (A, B) = d.type\n\t    assert A == wn\n\t    assert B == wn\n\t@given(ab=parallel_arrows(), xn=finite_functions(source=0))\n\tdef test_twist(ab, xn: FiniteFunction):\n\t    wn_A, wn_B = ab\n\t    d = Diagram.twist(wn_A, wn_B, xn)\n\t    (S, T) = d.type\n", "    assert S == wn_A + wn_B\n\t    assert T == wn_B + wn_A\n\t# TODO!\n\t# @unittest.skip\n\t# @given(ab=parallel_arrows(), xn=finite_functions(source=0))\n\t# def test_twist(ab, xn: FiniteFunction):\n\t    # wn_A, wn_B = ab\n\t    # d = Diagram.twist(wn, xn)\n\t    # # TODO: can't implement this as below; no guarantee we'll not get something like (p,\n\t    # # p^{-1}, G) instead!\n", "    # d >> d == Diagram.identity(d.wires)\n\t    # (S, T) = d.type\n\t    # assert S == wn_A + wn_B\n\t    # assert T == wn_B + wn_A\n\t@given(stw=labeled_cospans(), x=finite_functions(source=0))\n\tdef test_spider(stw, x):\n\t    \"\"\" Given a random cospan\n\t          s   t\n\t        A → W ← B\n\t    And a labeling\n", "        w : W → Σ₀\n\t    Generates a random spider.\n\t    \"\"\"\n\t    s, t, w = stw\n\t    Diagram.spider(s, t, w, x)\n\t@given(d=spiders())\n\tdef test_dagger_spider(d: Diagram):\n\t    e = d.dagger()\n\t    assert e.s == d.t\n\t    assert e.t == d.s\n", "    X = e.type\n\t    Y = d.type\n\t    assert X[0] == Y[1]\n\t    assert X[1] == Y[0]\n\t    assert e.G == d.G\n\t@given(abx=generator_and_typing())\n\tdef test_singleton(abx):\n\t    a, b, xn = abx\n\t    # test constructor works\n\t    d = Diagram.singleton(a, b, xn)\n", "    (A, B) = d.type\n\t    # Type of the diagram should be the same as the chosen typing of the generator.\n\t    assert a == A\n\t    assert b == B\n\t    # The number of internal wires should be equal to the number of ports on the\n\t    # generator.\n\t    assert d.wires == a.source + b.source\n\t################################################################################\n\t# Tensor\n\t@given(ds=many_diagrams(n=2))\n", "def test_tensor_type(ds):\n\t    \"\"\" Check that the tensor of two diagrams has the correct type and preserves\n\t    the number of wires and edges \"\"\"\n\t    d1, d2 = ds\n\t    d = d1.tensor(d2)\n\t    S, T = d.type\n\t    S1, T1 = d1.type\n\t    S2, T2 = d2.type\n\t    # NOTE: types are maps  N → Σ₀, so we take their COPRODUCT!\n\t    assert S == S1 + S2\n", "    assert T == T1 + T2\n\t    assert d.wires == d1.wires + d2.wires\n\t    assert d.G.Ei == d1.G.Ei + d2.G.Ei\n\t    assert d.G.Eo == d1.G.Eo + d2.G.Eo\n\t################################################################################\n\t# Composition\n\t@given(fg=composite_diagrams())\n\tdef test_compose_type(fg):\n\t    f, g = fg\n\t    h = f >> g\n", "    A, B = f.type\n\t    B2, C = g.type\n\t    assert B == B2\n\t    assert h.type == (A, C)\n\t@given(fg=composite_diagrams())\n\tdef test_compose_nonfinite_signature(fg):\n\t    # TODO: FIXME: this test should not exist. Instead, diagrams with non-finite\n\t    # signatures should be generated by the hypothesis strategies.\n\t    f, g = fg\n\t    def objectify(f: Diagram):\n", "        # modify the wn and xn arrays of a diagram f in place\n\t        # so they have the dtype object.\n\t        # It doesn't matter too much what the actual objects are.\n\t        wn = np.empty_like(f.G.wn.table, dtype='object')\n\t        wn[:] = [(x,x) for x in f.G.wn.table] # put tuples in.\n\t        xn = np.empty_like(f.G.xn.table, dtype='object')\n\t        xn[:] = [(x,x) for x in f.G.xn.table]\n\t        f.G.wn = FiniteFunction(None, wn)\n\t        f.G.xn = FiniteFunction(None, xn)\n\t    objectify(f)\n", "    objectify(g)\n\t    h = f >> g\n\t    A, B = f.type\n\t    B2, C = g.type\n\t    assert B == B2\n\t    assert h.type == (A, C)\n\t@given(fg=composite_diagrams())\n\tdef test_compose_wire_count(fg):\n\t    \"\"\" Check that the number of wires in a composite is within a certain range \"\"\"\n\t    f, g = fg\n", "    h = f >> g\n\t    # If f has M wires in the boundary, and g has N,\n\t    # we might quotient M+N → 0.\n\t    # So wires in the composite f >> g is no more than the composite f @ g,\n\t    # but greater than or equal to f.W + g.W - (M + N)\n\t    M = f.t.target\n\t    N = g.s.target\n\t    assert h.wires <= (f.wires + g.wires) and \\\n\t           h.wires >= (f.wires + g.wires - (M + N))\n\t@given(f=diagrams())\n", "def test_compose_dagger(f):\n\t    A, B = f.type\n\t    g = f.dagger()\n\t    h = f >> g\n\t    X, Y = h.type\n\t    assert X == A\n\t    assert Y == A\n\t@given(f=singletons())\n\tdef test_compose_singleton_dagger(f):\n\t    A, B = f.type\n", "    g = f.dagger()\n\t    h = f >> g\n\t    X, Y = h.type\n\t    assert X == A\n\t    assert Y == A\n\t    # Check that the total number of wires in the result is equal to those of f\n\t    # and g minus the shared boundary.\n\t    assert (f.wires + g.wires - (B.source)) == h.wires\n\t    # Since the result is a composition of singletons, we should also expect\n\t    # that the set of nodes appearing in the image of s is completely disjoint\n", "    # from t.\n\t    assert set(h.s.table).isdisjoint(set(h.t.table))\n\t@given(fg=many_singletons(n=2))\n\tdef test_tensor_singletons(fg):\n\t    f, g = fg\n\t    A0, B0 = f.type\n\t    A1, B1 = g.type\n\t    h = f @ g\n\t    assert h.type == (A0 + A1, B0 + B1)\n\t@given(fg=composite_singletons())\n", "def test_compose_singletons(fg):\n\t    \"\"\" Explicitly test that singletons can be composed \"\"\"\n\t    f, g = fg\n\t    A, B = f.type\n\t    B_, C = g.type\n\t    assert B == B_\n\t    h = f >> g\n\t    A_, C_ = h.type\n\t    assert A == A_\n\t    assert C == C_\n", "################################################################################\n\t# N-fold tensor of a list of diagrams\n\t@given(d=diagrams())\n\tdef test_tensor_list_empty(d):\n\t    # generate a random diagram and use its signature, but not the actual\n\t    # diagram content.\n\t    wn = d._Fun.initial(d.G.wn.target)\n\t    xn = d._Fun.initial(d.G.xn.target)\n\t    assert Diagram.tensor_list([], wn, xn) == Diagram.empty(wn, xn)\n\t@given(ds=many_diagrams(min_n=1))\n", "def test_tensor_list(ds):\n\t    \"\"\" Check that the tensor of two diagrams has the correct type and preserves\n\t    the number of wires and edges \"\"\"\n\t    assert len(ds) > 0\n\t    # slowly compute the tensor product in O(n²) time\n\t    expected = ds[0]\n\t    for d in ds[1:]:\n\t        expected = expected @ d\n\t    actual = Diagram.tensor_list(ds)\n\t    assert expected == actual\n"]}
{"filename": "tests/test_decompose.py", "chunked_list": ["import pytest\n\tfrom yarrow.decompose.frobenius import frobenius_decomposition\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\tfrom tests.util import monotonic, is_segmented_arange\n\t@given(c=diagrams())\n\tdef test_sort_x_p(c):\n\t    pass\n\t@given(c=diagrams())\n", "def test_frobenius_decomposition(c):\n\t    # Check the function doesn't crash!\n\t    frobenius_decomposition(c)\n\t@given(c=diagrams())\n\tdef test_frobenius_decomposition_type(c):\n\t    # Check that a frobenius decomposition has the same type as the diagram being decomposed\n\t    d = frobenius_decomposition(c)\n\t    A1, B1 = c.type\n\t    A2, B2 = d.type\n\t    assert A1 == A2\n", "    assert B1 == B2\n\t@given(c=diagrams())\n\tdef test_frobenius_decomposition_monotonic(c):\n\t    # Check that components of a Frobenius decomposition are in\n\t    # (generator, port) order.\n\t    d = frobenius_decomposition(c)\n\t    # (generator, port) order means xi and xo should be arrays of the form\n\t    # 0 0 0 ... | 1 1 1 ... | 2 2 2 ... |\n\t    assert monotonic(d.G.xi.table)\n\t    assert monotonic(d.G.xo.table)\n", "    # similarly, ports should be increasing \"runs\" (i.e., segmented aranges) of\n\t    # the form\n\t    # 0 1 2 ... | 0 1 2 ... | ...\n\t    assert is_segmented_arange(d.G.pi.table)\n\t    assert is_segmented_arange(d.G.po.table)\n"]}
{"filename": "tests/test_strategies.py", "chunked_list": ["\"\"\" Tests for the test strategies! Test all the things! \"\"\"\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\t@given(f=finite_functions(source=2))\n\tdef test_healthcheck_finite_functions(f):\n\t    assert (f.target != 0 if f.source != 0 else True)\n\t@given(AB=arrow_type(target=0))\n\tdef test_arrow_type_target_zero(AB):\n\t    A, B = AB\n", "    assert B == 0\n\t    assert A == 0\n\t@given(f=finite_functions(target=0))\n\tdef test_finite_functions_target_zero(f):\n\t    assert f.source == 0\n\t@given(fg=composite_diagrams())\n\tdef test_composite_diagrams(fg):\n\t    f, g = fg\n\t    assert f.type[1] == g.type[0]\n\t@given(fg=composite_singletons())\n", "def test_composite_singletons(fg):\n\t    f, g = fg\n\t    assert f.operations == 1\n\t    assert g.operations == 1\n\t    assert f.type[1] == g.type[0]\n\t@given(fg=many_singletons(n=2))\n\tdef test_many_singletons(fg):\n\t    f, g = fg\n\t    assert f.G.wn.target == g.G.wn.target\n\t    assert f.G.xn.target == g.G.xn.target\n", "@given(sff=segmented_finite_functions())\n\tdef test_segmented_finite_function(sff):\n\t    assert sff.sources.source == sff.targets.source\n\t    assert np.sum(sff.sources.table) == sff.values.source\n\t    # values are in the range [0, N)\n\t    # targets are all exactly N, so targets has codomain N+1.\n\t    assert sff.targets.target == sff.values.target +1\n\t@given(ops=operations())\n\tdef test_operations(ops):\n\t    xn, s_type, t_type = ops.xn, ops.s_type, ops.t_type\n", "    N = xn.source\n\t    assert len(s_type.sources) == N\n\t    assert len(t_type.sources) == N\n\t    assert np.sum(s_type.sources.table) == s_type.values.source\n\t    assert np.sum(t_type.sources.table) == t_type.values.source\n\t@given(sff_f_wn=object_map_and_half_spider())\n\tdef test_object_map_and_half_spider(sff_f_wn):\n\t    sff, f, wn = sff_f_wn\n\t    assert sff.sources.source == wn.target\n\t    assert f.target == wn.source\n"]}
{"filename": "tests/test_segmented.py", "chunked_list": ["from hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom yarrow.numpy import FiniteFunction, SegmentedFiniteFunction\n\tfrom tests.strategies import *\n\t@given(fsx=common_targets())\n\tdef test_indexed_coproduct(fsx):\n\t    \"\"\" Test the sequential indexed coproduct against parallel/vectorised one \"\"\"\n\t    fs, x = fsx\n\t    target = 0 if len(fs) == 0 else fs[0].target\n\t    expected = \\\n", "        FiniteFunction.coproduct_list([ fs[x(i)] for i in range(0, x.source) ], target)\n\t    actual = SegmentedFiniteFunction.from_list(fs).coproduct(x)\n\t    assert actual == expected\n\t    assert actual.source == sum(fs[x(i)].source for i in range(0, x.source))\n\t    assert actual.target == target\n\t@given(fsx=finite_function_lists())\n\tdef test_indexed_tensor(fsx):\n\t    \"\"\" Test the sequential indexed *tensor* against parallel/vectorised one \"\"\"\n\t    fs, x = fsx\n\t    expected = FiniteFunction.tensor_list([fs[x(i)] for i in range(0, x.source) ])\n", "    actual   = SegmentedFiniteFunction.from_list(fs).tensor(x)\n\t    assert actual == expected\n\t    assert actual.source == sum(fs[x(i)].source for i in range(0, x.source))\n\t    assert actual.target == sum(fs[x(i)].target for i in range(0, x.source))\n\t@given(segmented_finite_functions())\n\tdef test_segmented_finite_functions_roundtrip(sff):\n\t    # Given a list of functions, we can convert to a segmented finite function and back losslessly.\n\t    # NOTE: this is not true the other way, because there are multiple valid target values for the 'sources' and 'targets' fields.\n\t    fs = list(sff)\n\t    roundtrip = list(type(sff).from_list(fs))\n", "    assert fs == roundtrip\n\t@given(segmented_finite_functions())\n\tdef test_segmented_finite_functions_roundtrip_op(sff):\n\t    # Roundtrip from SFF → list → SFF\n\t    # NOTE: this is not lossless, since sff.sources.target and\n\t    # sff.targets.target values might be lost.\n\t    roundtrip = type(sff).from_list(list(sff))\n\t    # sources must match exactly, but we ignore the domains of finite functions\n\t    # because there are multiple valid choices.\n\t    assert np.all(sff.sources.table == roundtrip.sources.table)\n", "    assert np.all(sff.targets.table == roundtrip.targets.table)\n\t    assert np.all(sff.values.table  == roundtrip.values.table)\n\t################################################################################\n\t# Segmented operations\n\t@given(ops=operations())\n\tdef test_tensor_operations_type(ops):\n\t    d = Diagram.tensor_operations(ops)\n\t    A, B = d.type\n\t    assert A == ops.s_type.values\n\t    assert B == ops.t_type.values\n", "    # check number of edges and wires in the diagram.\n\t    # Ei should be equal to total input arity\n\t    # Eo equal to total output arity\n\t    # wires = Ei + Eo.\n\t    Ki = len(ops.s_type.values)\n\t    Ko = len(ops.t_type.values)\n\t    assert d.G.W  == Ki + Ko\n\t    assert d.G.Ei == Ki\n\t    assert d.G.Eo == Ko\n\t    assert d.G.X  == ops.xn.source\n", "@given(ops=operations())\n\tdef test_operations_iter(ops):\n\t    ops_list = list(ops)\n\t    assert len(ops_list) == len(ops)\n\t    assert np.all(np.array([ x[0] for x in ops_list ], dtype=ops.xn.table.dtype) == ops.xn.table)\n\t    # check types have correct codomain (i.e. Σ₀)\n\t    assert np.all(np.array([ x[1].target for x in ops_list ], dtype=int) == ops.s_type.values.target)\n\t    assert np.all(np.array([ x[2].target for x in ops_list ], dtype=int) == ops.t_type.values.target)\n"]}
{"filename": "tests/test_indexed_coproduct.py", "chunked_list": ["from yarrow import *\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\t@given(c=indexed_coproducts())\n\tdef test_indexed_coproduct_roundtrip(c):\n\t    assert c == IndexedCoproduct.from_list(c.target, list(c))\n\t@given(fsx=common_targets())\n\tdef test_indexed_coproduct_roundtrip_functions(fsx):\n\t    fs, x = fsx\n", "    target = 0 if len(fs) == 0 else fs[0].target\n\t    assert fs == list(IndexedCoproduct.from_list(target, fs))\n\t@given(fsx=common_targets())\n\tdef test_indexed_coproduct_map(fsx):\n\t    # a bunch of finite functions \"fs\" with common target.\n\t    fs, x = fsx\n\t    target = 0 if len(fs) == 0 else fs[0].target\n\t    expected_sources = FiniteFunction(None, [len(fs[x(i)]) for i in range(0, x.source) ])\n\t    expected_values = FiniteFunction.coproduct_list([ fs[x(i)] for i in range(0, x.source) ], target)\n\t    c = IndexedCoproduct.from_list(target, fs)\n", "    assert len(c) == len(fs)\n\t    d = c.map(x)\n\t    assert len(d) == x.source\n\t    assert d.sources == expected_sources\n\t    assert d.values == expected_values\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_vector.py", "chunked_list": ["# Vector program tests\n\timport numpy as np\n\tfrom yarrow.array import numpy\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\t# A \"run\" of length N being e.g. 0 1 2 3 4 ... N\n\t_MAX_RUN_LENGTH = 128\n\t_MAX_RUNS = 128\n\t# A non-vectorised implementation of segmented_arange\n", "def _slow_segmented_arange(x):\n\t    x = np.array(x)\n\t    N = np.sum(x) # how many values to make?\n\t    r = np.zeros(N, dtype=x.dtype) # allocate\n\t    k = 0\n\t    # for each size s,\n\t    for i in range(0, len(x)):\n\t        size = x[i]\n\t        # fill result with a run 0, 1, ..., s\n\t        for j in range(0, size):\n", "            r[k] = j\n\t            k += 1\n\t    return r\n\t@given(\n\t    x=st.lists(st.integers(min_value=0, max_value=_MAX_RUN_LENGTH), min_size=0, max_size=_MAX_RUNS)\n\t)\n\tdef test_segmented_arange(x):\n\t    \"\"\" Ensure the 'segmented_arange' vector program outputs runs like 0, 1, 2, 0, 1, 2, 3, 4, ... \"\"\"\n\t    # We're returning an array of size MAX_VALUE * MAX_SIZE, so keep it smallish!\n\t    x = np.array(x, dtype=int)\n", "    N = np.sum(x)\n\t    a = numpy.segmented_arange(x)\n\t    # Check we got the expected number of elements\n\t    assert len(a) == N\n\t    assert np.all(_slow_segmented_arange(x) == a)\n"]}
{"filename": "tests/strategies.py", "chunked_list": ["\"\"\" Hypothesis strategies for FiniteFunctions \"\"\"\n\timport numpy as np\n\tfrom yarrow.array import numpy\n\tfrom yarrow import FiniteFunction, BipartiteMultigraph, Diagram, SegmentedFiniteFunction, IndexedCoproduct\n\tfrom yarrow.segmented.operations import Operations\n\timport hypothesis.strategies as st\n\t# these constants are completely arbitrary, I picked a smallish number I like.\n\t_MAX_SEGMENT_SIZE = 32\n\t_MAX_SIGMA_1 = 32\n\t_MAX_OBJECTS = 32\n", "# generator for arity/coarity of operations\n\tsegment_sizes = st.integers(min_value=0, max_value=_MAX_SEGMENT_SIZE)\n\t# generator for finite sets Σ₁\n\tsigma_1 = st.integers(min_value=0, max_value=_MAX_SIGMA_1)\n\tnonzero_sigma_1 = st.integers(min_value=1, max_value=_MAX_SIGMA_1)\n\t# a generator for objects of FinFun\n\tobjects = st.integers(min_value=0, max_value=_MAX_OBJECTS)\n\tnonzero_objects = st.integers(min_value=1, max_value=32)\n\tdef _is_valid_arrow_type(s, t):\n\t    if t == 0:\n", "        return s == 0\n\t    return True\n\t@st.composite\n\tdef arrow_type(draw, source=None, target=None):\n\t    \"\"\" Generate a random type of finite function.\n\t    For example, a type of n → 0 is forbidden.\n\t    \"\"\"\n\t    # User specified both target and source\n\t    if target is not None and source is not None:\n\t        if target == 0 and source != 0:\n", "            raise ValueError(\"No arrows exist of type n → 0 for n != 0.\")\n\t        return source, target\n\t    elif source is None:\n\t        # any target\n\t        target = draw(objects) if target is None else target\n\t        if target == 0:\n\t            source = 0\n\t        else:\n\t            source = draw(objects)\n\t        return source, target\n", "    # target is None, but source is not\n\t    target = draw(nonzero_objects) if source > 0 else draw(objects)\n\t    return source, target\n\t# generate a random FiniteFunction\n\t@st.composite\n\tdef finite_functions(draw, source=None, target=None):\n\t    source, target = draw(arrow_type(source=source, target=target))\n\t    assert _is_valid_arrow_type(source, target)\n\t    # generate a random array of elements in {0, ..., target - 1}\n\t    if target == 0:\n", "        # FIXME: remove np hardcoding for other backends.\n\t        table = np.zeros(0, dtype=int)\n\t    else:\n\t        table = np.random.randint(0, high=target, size=source)\n\t    return FiniteFunction(target, table)\n\t@st.composite\n\tdef finite_function_lists(draw, n=None, source=None, target=None):\n\t    \"\"\" Draw a small-ish list of N finite functions, and an indexer x : X → N\"\"\"\n\t    n = n if n is not None else draw(objects)\n\t    fs = [ draw(finite_functions(source=source, target=target)) ]\n", "    x = draw(finite_functions(target=len(fs)))\n\t    return fs, x\n\t# Generate exactly n composite functions.\n\t@st.composite\n\tdef composite_functions(draw, n):\n\t    # NOTE: n is the number of *arrows*, so we need n+1 *objects*.\n\t    #\n\t    #    f₁   f₂ ... fn\n\t    # A₀ → A₁ →  ... → An\n\t    if n == 0:\n", "        return []\n\t    # for each function f : A → B, if B = 0, then A = 0.\n\t    # This is because there can be no functions n → 0 with n != 0\n\t    obj = draw(st.lists(objects, min_size=n+1, max_size=n+1))\n\t    for i in range(0, n+1):\n\t        if obj[-i] == 0:\n\t            obj[-i-1] = 0\n\t    fs = [ draw(finite_functions(source=a, target=b)) for a, b in zip(obj, obj[1:]) ]\n\t    return fs\n\t# generate h : X → B,\n", "# and from this generate f : A₁ → B, g : A₂ → B\n\t# such that A₁ + A₂ = X\n\t# and f + g = h\n\t@st.composite\n\tdef composite_coproduct(draw, source=None, target=None):\n\t    source, target = draw(arrow_type(source, target))\n\t    assert _is_valid_arrow_type(source, target)\n\t    a1 = draw(st.integers(min_value=0, max_value=source))\n\t    a2 = source - a1\n\t    f = draw(finite_functions(source=a1, target=target))\n", "    g = draw(finite_functions(source=a2, target=target))\n\t    return f, g\n\t@st.composite\n\tdef common_targets(draw, n=None, source=None, target=None):\n\t    \"\"\" Draw a list of random functions with the same target,\n\t        fs[i] : A_i → B\n\t    For i ∈ N, and an indexing function\n\t        x : X → N\n\t    \"\"\"\n\t    n = draw(objects) if n is None else n\n", "    target = draw(objects) if target is None else target\n\t    fs = []\n\t    for i in range(0, n):\n\t        source, _ = draw(arrow_type(target=target))\n\t        fs.append(draw(finite_functions(source=source, target=target)))\n\t    x = draw(finite_functions(target=n))\n\t    return fs, x\n\t@st.composite\n\tdef parallel_arrows(draw, source=None, target=None):\n\t    source, target = draw(arrow_type(source, target))\n", "    assert _is_valid_arrow_type(source, target)\n\t    f = draw(finite_functions(source=source, target=target))\n\t    g = draw(finite_functions(source=source, target=target))\n\t    return f, g\n\t@st.composite\n\tdef parallel_permutations(draw, source=None, target=None):\n\t    n = draw(objects)\n\t    assert _is_valid_arrow_type(n, n)\n\t    p = draw(permutations(n))\n\t    q = draw(permutations(n))\n", "    return p, q\n\t@st.composite\n\tdef permutations(draw, n=None):\n\t    if n is None:\n\t        n = draw(objects)\n\t    x = np.arange(0, n, dtype=int)\n\t    np.random.shuffle(x)\n\t    return FiniteFunction(n, x)\n\t@st.composite\n\tdef adapted_function(draw, source=None, target=None):\n", "    source, target = draw(arrow_type(source, target))\n\t    assert _is_valid_arrow_type(source, target)\n\t    f = draw(finite_functions(source=source, target=target))\n\t    p = draw(permutations(n=source))\n\t    q = draw(permutations(n=target))\n\t    return f, p, q\n\t################################################################################\n\t# Diagrams\n\t# Draw a cospan\n\t#   s : A → W\n", "#   t : B → W\n\t#   w : W → Σ₀\n\t@st.composite\n\tdef labeled_cospans(draw, W=None, Ob=None, A=None, B=None):\n\t    w = draw(finite_functions(source=W, target=Ob))\n\t    s = draw(finite_functions(source=A, target=w.source))\n\t    t = draw(finite_functions(source=B, target=w.source))\n\t    return (s, t, w)\n\t@st.composite\n\tdef spiders(draw, W=None, Ob=None, A=None, B=None, Arr=None):\n", "    \"\"\" Given a random cospan\n\t          s   t\n\t        A → W ← B\n\t    And a labeling\n\t        w : W → Σ₀\n\t    Generate a random spider.\n\t    \"\"\"\n\t    s, t, w = draw(labeled_cospans(W=W, Ob=Ob, A=A, B=B))\n\t    x = draw(finite_functions(source=0, target=Arr))\n\t    return Diagram.spider(s, t, w, x)\n", "@st.composite\n\tdef generator_and_typing(draw, Obj=None, Arr=None):\n\t    \"\"\" Generate a random generator\n\t        x : 1 → Σ₁\n\t    and its type\n\t        a : A → Σ₀\n\t        b : B → Σ₀\n\t    \"\"\"\n\t    # Σ₁ > 0, Σ₀ > 0\n\t    Arr = draw(nonzero_objects) if Arr is None else Arr\n", "    Obj = draw(nonzero_objects) if Obj is None else Obj\n\t    # xn : 1 → Σ₁\n\t    xn = draw(finite_functions(source=1, target=Arr))\n\t    # Typing\n\t    a = draw(finite_functions(target=Obj))\n\t    b = draw(finite_functions(target=Obj))\n\t    return a, b, xn\n\t@st.composite\n\tdef singletons(draw, Obj=None, Arr=None):\n\t    a, b, xn = draw(generator_and_typing(Obj=Obj, Arr=Arr))\n", "    return Diagram.singleton(a, b, xn)\n\t@st.composite\n\tdef diagrams(draw, Obj=None, Arr=None):\n\t    \"\"\" Generate a random diagram.\n\t    Since we're also generating a random signature,\n\t    we only need to ensure that ports are correct.\n\t    \"\"\"\n\t    Array = numpy\n\t    # Σ₀ > 0    Σ₁ ≥ 0\n\t    Obj = Obj if Obj is not None else draw(objects)\n", "    Arr = Arr if Arr is not None else draw(objects)\n\t    # max arity, coarity of generators.\n\t    # These get set to 0 if there are no wires in the diagram.\n\t    MAX_ARITY = None\n\t    MAX_COARITY = None\n\t    # Start with the number of wires in the diagram\n\t    # NOTE: This probably biases generation somehow.\n\t    wn = draw(finite_functions(target=Obj))\n\t    if wn.source == 0 or Arr == 0:\n\t        MAX_ARITY = 0\n", "        MAX_COARITY = 0\n\t        # return Diagram.empty(wn)\n\t    # 'arities' maps each generator xn(i) to its arity\n\t    arities   = draw(finite_functions(target=MAX_ARITY))\n\t    Ei = np.sum(arities.table)\n\t    coarities = draw(finite_functions(source=arities.source, target=MAX_COARITY))\n\t    Eo = np.sum(coarities.table)\n\t    # Now choose the number of generators and their arity/coarity.\n\t    xn = draw(finite_functions(source=arities.source, target=Arr))\n\t    # wi : Ei → W\n", "    # NOTE: Hypothesis builtin strategies really don't like numpy's int64s!\n\t    wi = draw(finite_functions(source=int(Ei), target=wn.source))\n\t    wo = draw(finite_functions(source=int(Eo), target=wn.source))\n\t    # pi and po are a 'segmented arange' of the arities and coarities\n\t    # e.g., [ 3 2 0 5 ] → [ 0 1 2 0 1 0 1 2 3 4 ]\n\t    pi = FiniteFunction(None, Array.segmented_arange(arities.table))\n\t    po = FiniteFunction(None, Array.segmented_arange(coarities.table))\n\t    # relatedly, xi and xo are just a repeat:\n\t    # (TODO: we could inline segmented_arange here and save recomputation of e.g., repeats)\n\t    # e.g., [ 3 2 0 5 ] → [ 0 0 0 1 1 2 2 2 2 2 ]\n", "    i = Array.arange(xn.source, dtype=int)\n\t    xi = FiniteFunction(xn.source, Array.repeat(i, arities.table))\n\t    xo = FiniteFunction(xn.source, Array.repeat(i, coarities.table))\n\t    G = BipartiteMultigraph(\n\t            wi=wi,\n\t            wo=wo,\n\t            xi=xi,\n\t            xo=xo,\n\t            wn=wn,\n\t            pi=pi,\n", "            po=po,\n\t            xn=xn)\n\t    s = draw(finite_functions(target=wn.source))\n\t    t = draw(finite_functions(target=wn.source))\n\t    return Diagram(s, t, G)\n\t@st.composite\n\tdef many_diagrams(draw, n=None, min_n=0, max_n=32):\n\t    \"\"\" Generate several diagrams from the same signature \"\"\"\n\t    # TODO: allow Obj = 0? Then we can only ever generate the empty diagram, or\n\t    # maybe only diagrams with generating morphisms of type 0 → 0?\n", "    if n is None:\n\t        n = draw(st.integers(min_value=min_n, max_value=max_n))\n\t    Obj = draw(nonzero_objects)\n\t    Arr = draw(sigma_1)\n\t    return [ draw(diagrams(Obj=Obj, Arr=Arr)) for _ in range(0, n) ]\n\t@st.composite\n\tdef many_singletons(draw, n):\n\t    \"\"\" Generate several singleton diagrams from the same signature \"\"\"\n\t    Obj = draw(objects)\n\t    Arr = draw(nonzero_sigma_1)\n", "    return [ draw(singletons(Obj=Obj, Arr=Arr)) for _ in range(0, n) ]\n\t@st.composite\n\tdef composite_diagrams(draw, max_boundary_size=128):\n\t    \"\"\"\n\t    Generate a composite diagram with a random signature.\n\t          f ; g\n\t        A → B → C\n\t    \"\"\"\n\t    # Obj = draw(nonzero_objects)\n\t    Obj = 1 # Only handle the PROP case for now.\n", "    Arr = draw(sigma_1)\n\t    # Draw two diagrams with Σ₀ = 1, then change sources + targets to have a\n\t    # common boundary.\n\t    f = draw(diagrams(Obj=Obj, Arr=Arr))\n\t    g = draw(diagrams(Obj=Obj, Arr=Arr))\n\t    if f.wires == 0 or g.wires == 0:\n\t        B = 0\n\t    else:\n\t        B = draw(st.integers(min_value=0, max_value=max_boundary_size))\n\t    f.t = draw(finite_functions(source=B, target=f.wires))\n", "    g.s = draw(finite_functions(source=B, target=g.wires))\n\t    return f, g\n\t@st.composite\n\tdef composite_singletons(draw, max_boundary_size=128):\n\t    \"\"\"\n\t    Generate a composite diagram with a random signature.\n\t          f ; g\n\t        A → B → C\n\t    where f, g are singleton diagrams.\n\t    \"\"\"\n", "    Σ_0 = draw(objects)\n\t    # types of f, g\n\t    a = draw(finite_functions(target=Σ_0))\n\t    b = draw(finite_functions(target=Σ_0))\n\t    c = draw(finite_functions(target=Σ_0))\n\t    # generator labels\n\t    _, Σ_1 = draw(arrow_type(source=1))\n\t    xn_f = draw(finite_functions(source=1, target=Σ_1))\n\t    xn_g = draw(finite_functions(source=1, target=Σ_1))\n\t    f = Diagram.singleton(a, b, xn_f)\n", "    g = Diagram.singleton(b, c, xn_g)\n\t    return f, g\n\t################################################################################\n\t# Segmented finite functions\n\t# Generate a segmented finite function like the one below\n\t#   sff\n\t#       sources: N            → K₀\n\t#       values : sum(sources) → Σ₀      (= max(targets))\n\t#       targets: N            → Σ₀      (= const Σ₀+1)\n\t@st.composite\n", "def segmented_finite_functions(draw, N=None, Obj=None):\n\t    N, Obj = draw(arrow_type(source=N, target=Obj))\n\t    sources = draw(finite_functions(source=N))\n\t    values  = draw(finite_functions(source=np.sum(sources.table), target=Obj))\n\t    # make an array [Σ₀, Σ₀, ... ]\n\t    targets = FiniteFunction.terminal(N).inject1(Obj)\n\t    return SegmentedFiniteFunction(\n\t        sources=sources,\n\t        targets=targets,\n\t        values=values)\n", "# Generate a coproduct of finite functions like the one below\n\t#   sff\n\t#       sources: N            → K₀\n\t#       values : sum(sources) → Σ₀      (= max(targets))\n\t@st.composite\n\tdef indexed_coproducts(draw, N=None, Obj=None):\n\t    N, Obj = draw(arrow_type(source=N, target=Obj))\n\t    sources = FiniteFunction(None, draw(finite_functions(source=N)).table)\n\t    values  = draw(finite_functions(source=np.sum(sources.table), target=Obj))\n\t    return IndexedCoproduct(sources=sources, values=values)\n", "# Generate a tensoring of operations with the following types.\n\t#   xn         : N            → Σ₁\n\t#\n\t#   s_type\n\t#       sources: N            → K₀\n\t#       values : sum(sources) → Σ₀      (= max(targets))\n\t#       targets: N            → Σ₀      (= const Σ₀+1)\n\t#   t_type\n\t#       sources: N            → K₁\n\t#       values : sum(sources) → Σ₀      (= max(targets))\n", "#       targets: N            → Σ₀      (= const Σ₀+1)\n\t@st.composite\n\tdef operations(draw):\n\t    Obj = draw(objects)\n\t    s_type = draw(indexed_coproducts(Obj=Obj))\n\t    t_type = draw(indexed_coproducts(\n\t        N=len(s_type.sources),\n\t        Obj=s_type.values.target))\n\t    N = len(s_type.sources)\n\t    xn = draw(finite_functions(source=N))\n", "    return Operations(xn, s_type, t_type)\n\t################################################################################\n\t# Half spiders\n\t@st.composite\n\tdef half_spider(draw, Obj=None):\n\t    Obj = draw(objects) if Obj is None else Obj\n\t    wn = draw(finite_functions(target=Obj))\n\t    f  = draw(finite_functions(target=wn.source))\n\t    return f, wn\n\t# The functions\n", "#   f  : A → B\n\t#   wn : B → Σ₀\n\t# together give a half-spider, and\n\t#   F₀~ : Σ₀ → Ω₀*\n\t#   F₀  : sum(s) → Ω₀\n\t#   s   : Σ₀ → Nat\n\t# a segmented array encoding the object map of a (finite) functor.\n\t@st.composite\n\tdef object_map_and_half_spider(draw):\n\t    c = draw(indexed_coproducts())\n", "    f, wn = draw(half_spider(Obj=c.sources.source))\n\t    return c, f, wn\n\t################################################################################\n\t# Functions of Finite Domain\n\t# TODO: these generators are quite hacky and not nice. Refactor!\n\t# A FinFun target is either:\n\t#   (target: int, dtype=int64)\n\t#   (None,        dtype=[int64 | object])\n\t#\n\t@st.composite\n", "def finite_function_target(draw, target=None, is_inf=None, inf_dtype=None):\n\t    if target is not None:\n\t        return False, None\n\t    is_inf = draw(st.booleans()) if is_inf == None else is_inf\n\t    inf_dtype = draw(st.sampled_from(['int', 'object'])) if inf_dtype is None else inf_dtype\n\t    return is_inf, inf_dtype\n\t@st.composite\n\tdef finite_domain_functions(draw, source=None, target=None, is_inf=None, inf_dtype=None):\n\t    \"\"\" Generate functions of finite domain (possibly infinite codomain!) \"\"\"\n\t    is_inf, inf_dtype = draw(finite_function_target(target, is_inf, inf_dtype))\n", "    if is_inf:\n\t        f = draw(finite_functions(source=source))\n\t        target = None\n\t        dtype = inf_dtype # if inf_dtype is not None else draw(st.sampled_from(['int', 'object']))\n\t        if dtype == 'int':\n\t            table = f.table\n\t        else:\n\t            # TODO: Generate more varied object data\n\t            table = np.empty(len(f.table), dtype='object')\n\t            table[:] = [(x,x) for x in f.table]\n", "    else:\n\t        f = draw(finite_functions(source=source, target=target))\n\t        target = f.target\n\t        table = f.table\n\t    return FiniteFunction(target, table, dtype=table.dtype)\n\t@st.composite\n\tdef composite_coproduct_finite_domain(draw, source=None, target=None):\n\t    source, target = draw(arrow_type(source, target))\n\t    assert _is_valid_arrow_type(source, target)\n\t    a1 = draw(st.integers(min_value=0, max_value=source))\n", "    a2 = source - a1\n\t    is_inf, inf_dtype = draw(finite_function_target(target=target))\n\t    f = draw(finite_domain_functions(source=a1, target=target, is_inf=is_inf, inf_dtype=inf_dtype))\n\t    g = draw(finite_domain_functions(source=a2, target=target, is_inf=is_inf, inf_dtype=inf_dtype))\n\t    return f, g\n\t@st.composite\n\tdef composite_nonfinite_codomain(draw, source=None, middle=None):\n\t    \"\"\" Draw a composite function with a possibly non-finite codomain \"\"\"\n\t    A, B = draw(arrow_type(source, middle))\n\t    f = draw(finite_functions(source=A, target=B))\n", "    g = draw(finite_domain_functions(source=B))\n\t    return f, g\n"]}
{"filename": "tests/util.py", "chunked_list": ["import numpy as np\n\tdef monotonic(x, strict=False):\n\t    if len(x) <= 1:\n\t        return True # arrays of length <= 1 are trivially sorted\n\t    if strict:\n\t        return np.all(x[:-1] < x[1:])\n\t    return np.all(x[:-1] <= x[1:])\n\t# return true if s sorts by f.\n\tdef sorts(s, f, strict=False):\n\t    y = s >> f\n", "    return monotonic(y.table)\n\t# check if an array is of the form\n\t#   [ 0, 1, 2, ..., N₀ | 0 1 2 ... N₁ | ... ]\n\tdef is_segmented_arange(x):\n\t    # empty array is trivially segmented\n\t    if len(x) == 0:\n\t        return True\n\t    # all differences should be exactly 1, except where x = 0.\n\t    d = x[1:] - x[:-1]\n\t    z = (x == 0)[1:]\n", "    # either difference is 1, or it's <= 0 at the start of a run.\n\t    return np.all((d == 1) | (z & (d <= 0)))\n"]}
{"filename": "tests/test_layering.py", "chunked_list": ["import scipy.sparse as sp\n\timport numpy as np\n\t# SUT\n\tfrom yarrow.finite_function import bincount\n\tfrom yarrow.numpy import Diagram\n\tfrom yarrow.numpy.layer import layer, kahn, operation_adjacency\n\tfrom tests.strategies import diagrams\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\t# TODO:\n", "# Generators:\n\t#   - monogamous acyclic diagrams \n\t# Tests:\n\t#   - Check that layer(ma_diagram) visits all operations\n\t#   - Check that for MA diagrams d₁ and d₂ and layer(d₁ ; d₂) ...\n\t#       - d₁ layerings should be unchanged\n\t#       - d₂ layerings should all be larger or equal to layer(d₂)\n\t#           - NOTE: relies on operations not being reordered after composition\n\t# Generate a random acyclic adjacency matrix\n\t_MAX_MATRIX_SIZE = 512\n", "@st.composite\n\tdef acyclic_adjacency_matrix(draw):\n\t    N = draw(st.integers(min_value=0, max_value=_MAX_MATRIX_SIZE))\n\t    # generate a random matrix, zero below diagonal\n\t    # Density set to 1/N, so memory usage is O(N).\n\t    density = 1 if N == 0 else 1/N\n\t    # NOTE: use LIL format so we can call setdiag efficiently below\n\t    M = sp.random(N, N, density=density, format='lil', dtype=bool)\n\t    # Set diagonal to zero otherwise we can get self-loops!\n\t    M.setdiag(0)\n", "    # convert to CSR for later.\n\t    M = sp.csr_array(M)\n\t    # Return a matrix which zeroes out the upper triangle\n\t    return sp.tril(M.astype(int))\n\t# Test kahn layering visits all nodes in an acyclic matrix\n\t@given(acyclic_adjacency_matrix())\n\tdef test_kahn_acyclic_all_visited(M):\n\t    \"\"\" Verify that kahn layering correctly orders nodes and visits all nodes in\n\t    an acyclic graph \"\"\"\n\t    order, visited = kahn(M)\n", "    # All nodes should be visited (the graph is acyclic)\n\t    assert np.all(visited)\n\t    # TODO: check that the ordering reflects the acyclic structure.\n\t    pass\n\t# Test kahn layering *doesn't* visit all nodes in a *cyclic* matrix.\n\t@given(acyclic_adjacency_matrix())\n\tdef test_kahn_cyclic_not_all_visited(M):\n\t    # TODO: this test only checks a small subset of cyclic graphs. Extend the\n\t    # generator to a larger class.\n\t    # put a self-cycle on all nodes\n", "    M.setdiag(1)\n\t    # All nodes should be in layer 0 because there are self-cycles on all nodes\n\t    order, visited = kahn(M)\n\t    # all nodes should have order 0, and none should be visited.\n\t    assert np.all(order == 0)\n\t    assert not np.any(visited)\n\t# Test operation_adjacency function does not crash :)\n\t@given(d=diagrams())\n\tdef test_operation_adjacency(d: Diagram):\n\t    M = operation_adjacency(d)\n", "    # NOTE: the (commented) test code below is wrong:\n\t    # Given an operation, we don't know how many others it connects to just by its arity/coarity, because of the Frobenius structure. We might have for each port:\n\t    #   - No other operations connected to (\"discard\")\n\t    #   - Every other operation connected to (\"copy\")\n\t    # check number of incoming edges is less than or equal to arity\n\t    # arity = bincount(d.G.xi)\n\t    # indegree = M.sum(axis=1)\n\t    # assert np.all(arity.table >= indegree)\n\t    # check number of outgoing edges is less than or equal to coarity\n\t    # coarity = bincount(d.G.xo)\n", "    # outdegree = M.sum(axis=0)\n\t    # assert np.all(coarity.table >= outdegree)\n\t    pass\n\t# Test the layer function does not crash\n\t@given(d=diagrams())\n\tdef test_layer_call(d: Diagram):\n\t    layer(d)\n"]}
{"filename": "tests/test_finite_function.py", "chunked_list": ["import pytest\n\timport numpy as np\n\tfrom yarrow import *\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\t################################################################################\n\t# Equality\n\t@given(f=finite_functions())\n\tdef test_equality_reflexive(f):\n", "    assert f == f\n\t@given(fg=parallel_arrows())\n\tdef test_inequality_table(fg):\n\t    \"\"\" Ensure that if the function tables of two FiniteFunctions are different,\n\t    then == returns false.\"\"\"\n\t    f, g = fg\n\t    if np.any(f.table != g.table):\n\t        assert f != g\n\t@given(f=finite_functions(), g=finite_functions())\n\tdef test_inequality_type(f, g):\n", "    \"\"\" Ensure that if the function tables of two FiniteFunctions are different,\n\t    then == returns false.\"\"\"\n\t    if f.source != g.source:\n\t        assert f != g\n\t    if f.target != g.target:\n\t        assert f != g\n\t################################################################################\n\t# Basic tests\n\t# only test small arrays, we don't need to OOM thank you very much\n\t@given(objects)\n", "def test_identity(n):\n\t    identity = FiniteFunction.identity(n)\n\t    assert np.all(identity.table == np.arange(0, n))\n\t    assert identity.table.shape == (n, )\n\t@given(st.integers(max_value=-1))\n\tdef test_identity_invalid_object(n):\n\t    with pytest.raises(AssertionError) as exc_info:\n\t        identity = FiniteFunction.identity(n)\n\t################################################################################\n\t# Category laws\n", "# left identities       id ; f = f\n\t@given(f=finite_functions(source=0))\n\tdef test_identity_composition_left(f):\n\t    \"\"\" id ; f = f \"\"\"\n\t    identity = FiniteFunction.identity(f.source)\n\t    assert (identity >> f) == f\n\t# right identities      f ; id = f\n\t@given(f=finite_functions())\n\tdef test_identity_composition_right(f):\n\t    \"\"\" f ; id = f \"\"\"\n", "    identity = FiniteFunction.identity(f.target)\n\t    assert (f >> identity) == f\n\t# Make sure composition doesn't crash\n\t@given(fns=composite_functions(n=2))\n\tdef test_composition(fns):\n\t    f, g = fns\n\t    x = f >> g\n\t# Check associativity of composition    (f ; g) ; h = f ; (g ; h)\n\t@given(fgh=composite_functions(n=3))\n\tdef test_composition_assoc(fgh):\n", "    f, g, h = fgh\n\t    assert ((f >> g) >> h) == (f >> (g >> h))\n\t################################################################################\n\t# Coproducts\n\t# Uniqueness of the initial map\n\t# any map f : 0 → B is equal to the initial map ? : 0 → B\n\t@given(f=finite_functions(source=0))\n\tdef test_initial_map_unique(f):\n\t    assert f == FiniteFunction.initial(f.target)\n\t# Coproducts!\n", "# given f : A₁ → B and g : A₂ → B, ensure f + g commutes with injections.\n\t# i.e.,  ι₀ ; (f + g) = f\n\t#        ι₁ ; (f + g) = g\n\t@given(fg=composite_coproduct())\n\tdef test_coproduct_commutes(fg):\n\t    f, g = fg\n\t    i0 = FiniteFunction.inj0(f.source, g.source)\n\t    i1 = FiniteFunction.inj1(f.source, g.source)\n\t    assert (i0 >> (f + g)) == f\n\t    assert (i1 >> (f + g)) == g\n", "@given(f=finite_functions(), b=objects)\n\tdef test_f_cp_inj0_equals_inject0(f, b):\n\t    assert f >> FiniteFunction.inj0(f.target, b) == f.inject0(b)\n\t@given(f=finite_functions(), a=objects)\n\tdef test_f_cp_inj1_equals_inject0(f, a):\n\t    assert f >> FiniteFunction.inj1(a, f.target) == f.inject1(a)\n\t################################################################################\n\t# (Strict) symmetric monoidal tests\n\t@given(f=finite_functions(), g=finite_functions())\n\tdef test_tensor_vs_injections(f, g):\n", "    \"\"\" Verify that the tensor product corresponds to its definition in terms of\n\t    coproducts and injections \"\"\"\n\t    i0 = FiniteFunction.inj0(f.target, g.target)\n\t    i1 = FiniteFunction.inj1(f.target, g.target)\n\t    f @ g == (f >> i0) + (g >> i1)\n\t@given(a=objects, b=objects)\n\tdef test_twist_inverse(a, b):\n\t    \"\"\" Check the law σ ; σ = id \"\"\"\n\t    f = FiniteFunction.twist(a, b)\n\t    g = FiniteFunction.twist(b, a)\n", "    identity = FiniteFunction.identity(a + b)\n\t    assert f >> g == identity\n\t    assert g >> f == identity\n\t@given(f=finite_functions(), g=finite_functions())\n\tdef test_twist_naturality(f, g):\n\t    \"\"\" Check naturality of σ, so that (f @ g) ; σ = σ ; (f @ g) \"\"\"\n\t    post_twist = FiniteFunction.twist(f.target, g.target)\n\t    pre_twist  = FiniteFunction.twist(f.source, g.source)\n\t    assert ((f @ g) >> post_twist) == (pre_twist >> (g @ f))\n\t################################################################################\n", "# Test coequalizers\n\t@given(fg=parallel_arrows())\n\tdef test_coequalizer_commutes(fg):\n\t    f, g = fg\n\t    c = f.coequalizer(g)\n\t    assert (f >> c) == (g >> c)\n\t################################################################################\n\t# Finite coproducts\n\t@given(s=finite_functions())\n\tdef test_injection_coproduct_identity(s: FiniteFunction):\n", "    \"\"\" Test that\n\t        ι_0 + ι_1 + ... + ι_N = identity(sum_{i ∈ N} s(i))\n\t    \"\"\"\n\t    i = FiniteFunction.identity(s.source)\n\t    assert s.injections(i) == FiniteFunction.identity(np.sum(s.table))\n\t@given(f=finite_functions())\n\tdef test_bincount(f: FiniteFunction):\n\t    b = bincount(f)\n\t    assert b.source == f.target\n\t    assert b.target == f.source+1\n", "    assert np.all(b.table < len(f)+1)\n\t    assert np.all(b.table >= 0)\n\t################################################################################\n\t# Test functions with None target\n\t@given(fg=composite_coproduct_finite_domain())\n\tdef test_finite_domain_coproduct(fg):\n\t    f, g = fg\n\t    A0, B = f.type\n\t    A1, B_ = g.type\n\t    assert B == B_\n", "    h = f + g\n\t    X, Y = h.type\n\t    assert X == A0 + A1\n\t    assert Y == B\n\t    assert h.table.dtype == f.table.dtype\n\t    assert h.table.dtype == g.table.dtype\n\t@given(fg=composite_nonfinite_codomain())\n\tdef test_compose_finite_domain(fg):\n\t    \"\"\" Test composition with functions of non-finite codomain \"\"\"\n\t    f, g = fg\n", "    A, B = f.type\n\t    B_, C = g.type\n\t    assert B == B_\n\t    h = f >> g\n\t    assert h.type == (A, C)\n\t@given(f=finite_functions())\n\tdef test_cumsum(f):\n\t    g = cumsum(f)\n\t    assert len(g) == len(f)\n\t    assert g.target == np.sum(f.table) + 1\n"]}
{"filename": "tests/test_functor.py", "chunked_list": ["import unittest\n\tfrom hypothesis import given\n\timport hypothesis.strategies as st\n\tfrom tests.strategies import *\n\tfrom yarrow.decompose.frobenius import frobenius_decomposition\n\t# SUT\n\tfrom yarrow.functor.functor import *\n\t@given(d=diagrams())\n\tdef test_decomposition_to_operations(d):\n\t    d = frobenius_decomposition(d)\n", "    ops = decomposition_to_operations(d)\n\t# Given an object L(B)\n\t#   Bwn : B → Σ₀\n\t# ... a half-spider\n\t#   f  : A → B\n\t# ... and segmented finite function representing the object map of a functor\n\t#   c\n\t#       sources : Σ₀ → Nat\n\t#       values  : sum(sources) → Ω₀\n\t# verify that applying the functor does not raise errors.\n", "@given(c_f_wn=object_map_and_half_spider())\n\tdef test_apply_finite_object_map(c_f_wn):\n\t    c, f, Bwn = c_f_wn\n\t    FBwn = apply_finite_object_map(c, Bwn)\n\t# Check that mapping half-spiders is natural\n\t#   f    :   A  →   B\n\t#   F(f) : F(A) → F(B)\n\t# Proposition 9.6, paper v1\n\t@given(c_f_wn=object_map_and_half_spider())\n\tdef test_map_half_spider(c_f_wn):\n", "    c, f, Bwn = c_f_wn\n\t    Awn = f >> Bwn # A(wn) = f ; B(wn) by naturality\n\t    # the object map of the functor we're applying.\n\t    # For the purposes of this test, it's finite.\n\t    def object_map(wn):\n\t        return apply_finite_object_map(c, wn)\n\t    # Compute F(B)(wn) and F(f)\n\t    FBwn = object_map(Bwn)\n\t    Ff = map_half_spider(FBwn, f)\n\t    # F(A)(wn)\n", "    FAwn = Ff >> FBwn.values\n\t    # Holds by Proposition 9.6\n\t    assert object_map(Awn).values == FAwn\n\t# Ensure that applying the identity object map to an object\n\t# L(A) (represented by wn) gives the array wn.\n\t@given(c=diagrams())\n\tdef test_identity_object_map(c):\n\t    wn = c.G.wn\n\t    Fwn = identity_object_map(wn)\n\t    assert Fwn.values == wn\n", "    assert np.all(Fwn.sources.table == 1)\n\t# Check that the identity functor implemented as a 'Frobenius Functor' gives the\n\t# input diagram back.\n\t# TODO: test against the Identity functor!\n\t@given(c=diagrams())\n\tdef test_identity_frobenius_functor(c):\n\t    F = FrobeniusIdentity()\n\t    d = F.map_arrow(c)\n\t    A, B = c.type \n\t    C, D = d.type\n", "    assert A == C\n\t    assert B == D\n\t    # for this particular functor, we expect exact equality of diagrams.\n\t    assert c == d\n\tclass DaggerDaggerFunctor(FrobeniusFunctor):\n\t    \"\"\" The DaggerDagger functor maps each generating morphism\n\t        f : A → B\n\t    to the composition\n\t        f ; f† ; f : A → B\n\t    and so is identity-on-objects.\n", "    \"\"\"\n\t    def map_objects(self, objects: AbstractFiniteFunction):\n\t        return identity_object_map(objects)\n\t    def map_operations(self, ops: Operations) -> Diagram:\n\t        d = Diagram.tensor_operations(ops)\n\t        return (d >> d.dagger() >> d)\n\t# A more complicated test for the FrobeniusFunctor class:\n\t# make sure that even diagrams with internal wiring are mapped correctly.\n\t# This checks that the diagram has the expected type and number of operations.\n\t@given(c=diagrams())\n", "def test_dagger_dagger_functor(c):\n\t    F = DaggerDaggerFunctor()\n\t    d = F.map_arrow(c)\n\t    A, B = c.type \n\t    C, D = d.type\n\t    assert A == C\n\t    assert B == D\n\t    # d has exactly 3x as many generating operations as c.\n\t    assert d.G.xn.source == c.G.xn.source * 3\n\t# TODO: replace this with FiniteFunction.interleave >> (x + y) ?\n", "def interleave(x, y):\n\t    \"\"\" Return the finite function whose table is the interleaving of x and y \"\"\"\n\t    h = x + y\n\t    h.table[0::2] = x.table\n\t    h.table[1::2] = y.table\n\t    return h\n\tclass DoublingFunctor(FrobeniusFunctor):\n\t    \"\"\" The functor which maps each generating object A to the tensor (A ● A),\n\t    and each generating operation f : A₀ ● A₁ ... An → B₀ ● B₁ ... Bn\n\t    to the operation f : A₀ A₀ ● A₁ A₁ ... An An → B₀ B₀ ● B₁ B₁ ... Bn Bn.\n", "    Note that we simply assume a signature in which this is well-typed.\n\t    \"\"\"\n\t    def map_objects(self, objects: AbstractFiniteFunction):\n\t        # TODO: generalise this to the application of two distinct functors interleaved (optics!)\n\t        N = len(objects.table) * 2\n\t        table = np.zeros_like(objects.table, shape=N)\n\t        table[0::2] = objects.table\n\t        table[1::2] = objects.table\n\t        sources = FiniteFunction(None, np.full(N//2, 2, dtype='int'))\n\t        values  = FiniteFunction(objects.target, table)\n", "        # return SegmentedFiniteFunction(sources, targets, values)\n\t        return IndexedCoproduct(sources, values)\n\t    def map_operations(self, ops: Operations) -> Diagram:\n\t        s_type = IndexedCoproduct(\n\t            sources = FiniteFunction(None, ops.s_type.sources.table * 2),\n\t            values  = interleave(ops.s_type.values, ops.s_type.values))\n\t        t_type = IndexedCoproduct(\n\t            sources = FiniteFunction(None, ops.t_type.sources.table * 2),\n\t            values  = interleave(ops.t_type.values, ops.t_type.values))\n\t        ops = Operations(ops.xn, s_type, t_type)\n", "        d = Diagram.tensor_operations(ops)\n\t        return d\n\t# We need to test the case of functors which are not identity-on-objects.\n\t@given(c=diagrams())\n\tdef test_doubling_functor(c):\n\t    F = DoublingFunctor()\n\t    d = F.map_arrow(c)\n\t    A, B = c.type\n\t    C, D = d.type\n\t    assert np.all(C.table == interleave(A, A).table)\n", "    assert np.all(D.table == interleave(B, B).table)\n\t    # Same number of operations (just with different types)\n\t    assert d.G.xn.source == c.G.xn.source\n"]}
{"filename": "yarrow/finite_function.py", "chunked_list": ["\"\"\"An implementation of finite functions as arrays.\n\tAll yarrow datastructures are ultimately built from finite functions.\n\tFor an overview, see :cite:t:`dpafsd`, Section 2.2.2.\n\tFinite functions can be thought of as a thin wrapper around integer arrays whose\n\telements are within a specified range.\n\tHere's an example of contructing a finite function:\n\t>>> print(FiniteFunction(3, [0, 1, 2, 0]))\n\t[0 1 2 0] : 4 → 3\n\tMathematically, this represents a function from the set of 4 elements to the set\n\tof 3 elements, and so its \"type\" is ``4 → 3``.\n", "There are several constructors for finite functions corresponding to useful\n\tmorphisms in category theory.\n\tFor example, the ``identity`` map is like numpy's ``arange``:\n\t>>> print(FiniteFunction.identity(5))\n\t[0 1 2 3 4] : 5 → 5\n\tand the ``terminal`` map is an array of zeroes:\n\t>>> print(FiniteFunction.terminal(5))\n\t[0 0 0 0 0] : 5 → 1\n\tFinite functions form a *symmetric monoidal category*.\n\tThey can be composed sequentially:\n", ">>> print(FiniteFunction.identity(5) >> FiniteFunction.terminal(5))\n\t[0 0 0 0 0] : 5 → 5\n\tAnd in parallel:\n\t>>> FiniteFunction.identity(5) @ FiniteFunction.terminal(5)\n\tFiniteFunction(6, [0 1 2 3 4 5 5 5 5 5])\n\t\"\"\"\n\tfrom typing import List\n\t# import yarrow.array.numpy as numpy\n\tDTYPE='int64'\n\tclass AbstractFiniteFunction:\n", "    \"\"\"\n\t    Finite functions parametrised over the underlying array type (the \"backend\").\n\t    This implementation assumes there is a cls._Array member implementing various primitives.\n\t    For example, cls._Array.sum() should compute the sum of an array.\n\t    \"\"\"\n\t    def __init__(self, target, table, dtype=DTYPE):\n\t        # TODO: this constructor is too complicated; it should be simplified.\n\t        # _Array is the \"array functions module\"\n\t        # It lets us parametrise AbstractFiniteFunction by a module like \"numpy\".\n\t        Array = type(self)._Array\n", "        if type(table) == Array.Type:\n\t           self.table = table\n\t        else:\n\t            self.table = Array.array(table, dtype=dtype)\n\t        self.target = target\n\t        assert len(self.table.shape) == 1 # ensure 1D array\n\t        assert self.source >= 0\n\t        if self.source > 0 and self.target is not None:\n\t            assert self.target >= 0\n\t            assert self.target > Array.max(table)\n", "    @property\n\t    def source(self):\n\t        \"\"\"The source (aka \"domain\") of this finite function\"\"\"\n\t        return len(self.table)\n\t    def __len__(self):\n\t        \"\"\"Same as self.source.\n\t        Sometimes this is clearer when thinking of a finite function as an array.\n\t        \"\"\"\n\t        return len(self.table)\n\t    def __str__(self):\n", "        return f'{self.table} : {self.source} → {self.target}'\n\t    def __repr__(self):\n\t        return f'FiniteFunction({self.target}, {self.table})'\n\t    def __call__(self, i: int):\n\t        if i >= self.source:\n\t            raise ValueError(\"Calling {self} with {i} >= source {self.source}\")\n\t        return self.table[i]\n\t    @property\n\t    def type(f):\n\t        \"\"\"Get the source and target of this finite function.\n", "        Returns:\n\t            tuple: (f.source, f.target)\n\t        \"\"\"\n\t        return f.source, f.target\n\t    ################################################################################\n\t    # FiniteFunction forms a category\n\t    @classmethod\n\t    def identity(cls, n: int):\n\t        \"\"\"Return the identity finite function of type n → n.\n\t        Args:\n", "            n(int): The object of which to return the identity map\n\t        Returns:\n\t            AbstractFiniteFunction: Identity map at n\n\t        \"\"\"\n\t        assert n >= 0\n\t        return cls(n, cls._Array.arange(0, n, dtype=DTYPE))\n\t    # Compute (f ; g), i.e., the function x → g(f(x))\n\t    def compose(f: 'AbstractFiniteFunction', g: 'AbstractFiniteFunction'):\n\t        \"\"\"Compose this finite function with another\n\t        Args:\n", "            g: A FiniteFunction for which self.target == g.source\n\t        Returns:\n\t            The composition f ; g.\n\t        Raises:\n\t            ValueError: if self.target != g.source\n\t        \"\"\"\n\t        if f.target != g.source:\n\t            raise ValueError(f\"Can't compose FiniteFunction {f} with {g}: f.target != g.source\")\n\t        source = f.source\n\t        target = g.target\n", "        # Use array indexing to compute composition in parallel (if applicable\n\t        # cls._Array backend is used)\n\t        table = g.table[f.table]\n\t        return type(f)(target, table)\n\t    def __rshift__(f, g):\n\t        return f.compose(g)\n\t    # We can compare functions for equality in a reasonable way: by just\n\t    # comparing elements.\n\t    # This is basically because FinFun is skeletal, so we don't need to check\n\t    # \"up to isomorphism\".\n", "    def __eq__(f, g):\n\t        return f.source == g.source \\\n\t           and f.target == g.target \\\n\t           and f._Array.all(f.table == g.table)\n\t    ################################################################################\n\t    # FiniteFunction has initial objects and coproducts\n\t    @classmethod\n\t    def initial(cls, b, dtype=DTYPE):\n\t        \"\"\"Compute the initial map ``? : 0 → b``\"\"\"\n\t        return cls(b, cls._Array.zeros(0, dtype=dtype))\n", "    @classmethod\n\t    def inj0(cls, a, b):\n\t        \"\"\"Compute the injection ``ι₀ : a → a + b``\"\"\"\n\t        table = cls._Array.arange(0, a, dtype=DTYPE)\n\t        return cls(a + b, table)\n\t    @classmethod\n\t    def inj1(cls, a, b):\n\t        \"\"\"Compute the injection ``ι₁ : b → a + b``\"\"\"\n\t        table = cls._Array.arange(a, a + b, dtype=DTYPE)\n\t        return cls(a + b, table)\n", "    def inject0(f, b):\n\t        \"\"\"\n\t        Directly compute (f ; ι₀) instead of by composition.\n\t        >>> f.inject0(b) == f >> ι₀\n\t        \"\"\"\n\t        return type(f)(f.target + b, f.table)\n\t    def inject1(f, a):\n\t        \"\"\"\n\t        Directly compute (f ; ι₁) instead of by composition.\n\t        >>> f.inject1(a) == f >> ι₁\n", "        \"\"\"\n\t        return type(f)(a + f.target, a + f.table)\n\t    def coproduct(f, g):\n\t        \"\"\" Given maps ``f : A₀ → B`` and ``g : A₁ → B``\n\t        compute the coproduct ``f.coproduct(g) : A₀ + A₁ → B``\"\"\"\n\t        assert f.target == g.target\n\t        assert f.table.dtype == g.table.dtype\n\t        target = f.target\n\t        table = type(f)._Array.concatenate([f.table, g.table])\n\t        return type(f)(target, table)\n", "    def __add__(f, g):\n\t        \"\"\" Inline coproduct \"\"\"\n\t        return f.coproduct(g)\n\t    ################################################################################\n\t    # FiniteFunction as a strict symmetric monoidal category\n\t    @staticmethod\n\t    def unit():\n\t        \"\"\" return the unit object of the category \"\"\"\n\t        return 0\n\t    def tensor(f, g):\n", "        \"\"\" Given maps\n\t        ``f : A₀ → B₀`` and\n\t        ``g : A₁ → B₁``\n\t        compute the *tensor* product\n\t        ``f.tensor(g) : A₀ + A₁ → B₀ + B₁``\"\"\"\n\t        # The tensor (f @ g) is the same as (f;ι₀) + (g;ι₁)\n\t        # however, we compute it directly for the sake of efficiency\n\t        T = type(f)\n\t        table = T._Array.concatenate([f.table, g.table + f.target])\n\t        return T(f.target + g.target, table)\n", "    def __matmul__(f, g):\n\t        return f.tensor(g)\n\t    @classmethod\n\t    def twist(cls, a, b):\n\t        # Read a permutation as the array whose ith position denotes \"where to send\" value i.\n\t        # e.g., twist_{2, 3} = [3 4 0 1 2]\n\t        #       twist_{2, 1} = [1 2 0]\n\t        #       twist_{0, 2} = [0 1]\n\t        table = cls._Array.concatenate([b + cls._Array.arange(0, a), cls._Array.arange(0, b)])\n\t        return cls(a + b, table)\n", "    ################################################################################\n\t    # Coequalizers for FiniteFunction\n\t    def coequalizer(f, g):\n\t        \"\"\"\n\t        Given finite functions    ``f, g : A → B``,\n\t        return the *coequalizer*  ``q    : B → Q``\n\t        which is the unique arrow such that  ``f >> q = g >> q``\n\t        having a unique arrow to any other such map.\n\t        \"\"\"\n\t        if f.type != g.type:\n", "            raise ValueError(\n\t                f\"cannot coequalize arrows {f} and {g} of different types: {f.type} != {g.type}\")\n\t        # connected_components returns:\n\t        #   Q:        number of components\n\t        #   q: B → Q  map assigning vertices to their component\n\t        # For the latter we have that\n\t        #   * if f.table[i] == g.table[i]\n\t        #   * then q[f.table[i]] == q[g.table[i]]\n\t        # NOTE: we pass f.target so the size of the sparse adjacency matrix\n\t        # representing the graph can be computed efficiently; otherwise we'd\n", "        # have to take a max() of each table.\n\t        # Q: number of connected components\n\t        T = type(f)\n\t        Q, q = T._Array.connected_components(f.table, g.table, f.target)\n\t        return T(Q, q)\n\t    ################################################################################\n\t    # FiniteFunction also has cartesian structure which is useful\n\t    @classmethod\n\t    def terminal(cls, a, dtype=DTYPE):\n\t        \"\"\" Compute the terminal map ``! : a → 1``. \"\"\"\n", "        return cls(1, cls._Array.zeros(a, dtype=DTYPE))\n\t    # TODO: rename this \"element\"?\n\t    @classmethod\n\t    def singleton(cls, x, b, dtype=DTYPE):\n\t        \"\"\" return the singleton array ``[x]`` whose domain is ``b``. \"\"\"\n\t        assert x < b\n\t        return cls(b, cls._Array.full(1, x, dtype=dtype))\n\t    ################################################################################\n\t    # Sorting morphisms\n\t    def argsort(f: 'AbstractFiniteFunction'):\n", "        \"\"\"\n\t        Given a finite function                     ``f : A → B``\n\t        Return the *stable* sorting permutation     ``p : A → A``\n\t        such that                                   ``p >> f``  is monotonic.\n\t        \"\"\"\n\t        return type(f)(f.source, f._Array.argsort(f.table))\n\t    ################################################################################\n\t    # Useful permutations\n\t    # Given generating objects A_i and B_i for i ∈ ord{n},\n\t    #   interleave : (A₀ ● A₁ ● ... ● An) ● (B₀ ● B₁ ● ... ● Bn) → (A₀ ● B₀) ● .. ● (An ● Bn)\n", "    @classmethod\n\t    def interleave(cls, N: int):\n\t        table = cls._Array.zeros(2*N, dtype=int)\n\t        table[0:N] = cls._Array.arange(N)*2\n\t        table[N:] = table[0:N] + 1\n\t        return cls(2*N, table)\n\t    # Given generating objects A_i and B_i for i ∈ ord{n},\n\t    #   cointerleave : (A₀ ● B₀) ● .. ● (An ● Bn) → (A₀ ● A₁ ● ... ● An) ● (B₀ ● B₁ ● ... ● Bn)\n\t    @classmethod\n\t    def cointerleave(cls, N):\n", "        table = cls._Array.zeros(2*N, dtype=int)\n\t        table[0::2] = cls._Array.arange(N)\n\t        table[1::2] = table[0::2] + N\n\t        return cls(2*N, table)\n\t    ################################################################################\n\t    # Sequential-only methods\n\t    @classmethod\n\t    def coproduct_list(cls, fs: List['AbstractFiniteFunction'], target=None):\n\t        \"\"\" Compute the coproduct of a list of finite functions. O(n) in size of the result.\n\t        .. warning::\n", "            Does not speed up to O(log n) in the parallel case.\n\t        \"\"\"\n\t        # NOTE: this function is not parallelized!\n\t        if len(fs) == 0:\n\t            return cls.initial(0 if target is None else target)\n\t        # all targets must be equal\n\t        assert all(f.target == g.target for f, g in zip(fs, fs[:1]))\n\t        return cls(fs[0].target, cls._Array.concatenate([f.table for f in fs]))\n\t    @classmethod\n\t    def tensor_list(cls, fs: List['AbstractFiniteFunction']):\n", "        \"\"\" Compute the tensor product of a list of finite functions. O(n) in size of the result.\n\t        .. warning::\n\t            Does not speed up to O(log n) in the parallel case.\n\t        \"\"\"\n\t        if len(fs) == 0:\n\t            return cls.initial(0)\n\t        targets = cls._Array.array([f.target for f in fs])\n\t        offsets = cls._Array.zeros(len(targets) + 1, dtype=type(fs[0].source))\n\t        offsets[1:] = cls._Array.cumsum(targets) # exclusive scan\n\t        table = cls._Array.concatenate([f.table + offset for f, offset in zip(fs, offsets[:-1])])\n", "        return cls(offsets[-1], table)\n\t    ################################################################################\n\t    # Finite coproducts\n\t    def injections(s: 'AbstractFiniteFunction', a: 'AbstractFiniteFunction'):\n\t        \"\"\"\n\t        Given a finite function ``s : N → K``\n\t        representing the objects of the coproduct\n\t        ``Σ_{n ∈ N} s(n)``\n\t        whose injections have the type\n\t        ``ι_x : s(x) → Σ_{n ∈ N} s(n)``,\n", "        and given a finite map\n\t        ``a : A → N``,\n\t        compute the coproduct of injections\n\t        .. code-block:: text\n\t            injections(s, a) : Σ_{x ∈ A} s(x) → Σ_{n ∈ N} s(n)\n\t            injections(s, a) = Σ_{x ∈ A} ι_a(x)\n\t        So that ``injections(s, id) == id``\n\t        Note that when a is a permutation,\n\t        injections(s, a) is a \"blockwise\" version of that permutation with block\n\t        sizes equal to s.\n", "        \"\"\"\n\t        # segment pointers\n\t        Array = a._Array\n\t        # cumsum is inclusive, we need exclusive so we just allocate 1 more space.\n\t        p = Array.zeros(s.source + 1, dtype=Array.DEFAULT_DTYPE)\n\t        p[1:] = Array.cumsum(s.table)\n\t        k = a >> s # avoid recomputation\n\t        r = Array.segmented_arange(k.table)\n\t        # NOTE: p[-1] is sum(s).\n\t        cls = type(s)\n", "        return cls(p[-1], r + cls._Array.repeat(p[a.table], k.table))\n\tdef argsort(f: AbstractFiniteFunction):\n\t    \"\"\" Applies a stable 'argsort' to the underlying array of a finite function.\n\t    When that finite function is a permutation, this inverts it.\n\t    \"\"\"\n\t    return type(f)(f.source, f._Array.argsort(f.table))\n\tdef bincount(f: AbstractFiniteFunction):\n\t    \"\"\" bincount the underlying array of a finite function\n\t    Args:\n\t        f: A finite function of type ``A → B``\n", "    Returns:\n\t        AbstractFiniteFunction: A finite function of type ``B → A+1``\n\t    \"\"\"\n\t    # the bincount of an array\n\t    #   f : A → B\n\t    # is a finite function\n\t    #   g : B → A+1\n\t    # where\n\t    #   g(b) = |{b . ∃a. f(a) = b}|\n\t    return type(f)(len(f)+1, f._Array.bincount(f.table, minlength=f.target))\n", "def cumsum(f: AbstractFiniteFunction) -> AbstractFiniteFunction:\n\t    Fun = type(f)\n\t    Array = Fun._Array\n\t    table = Array.zeros(len(f) + 1, dtype=f.table.dtype)\n\t    table[1:] = Array.cumsum(f.table)\n\t    return Fun(table[-1]+1, table[:-1])\n"]}
{"filename": "yarrow/cupy.py", "chunked_list": ["\"\"\" CuPy-backed finite functions, bipartite multigraphs, and diagrams.\n\t.. danger::\n\t   **Experimental Module**\n\t   This code is not thoroughly tested.\n\t   It's included here as a proof-of-concept for GPU acceleration.\n\t   The way backends are selected is also likely to change in the future.\n\t\"\"\"\n\t# Abstract implementations\n\tfrom yarrow.finite_function import *\n\tfrom yarrow.bipartite_multigraph import *\n", "from yarrow.diagram import *\n\tfrom yarrow.segmented.finite_function import AbstractIndexedCoproduct, AbstractSegmentedFiniteFunction\n\t# Array backend\n\timport yarrow.array.cupy as cupy\n\tclass FiniteFunction(AbstractFiniteFunction):\n\t    \"\"\" CuPy-backed finite functions \"\"\"\n\t    _Array = cupy\n\tclass IndexedCoproduct(AbstractIndexedCoproduct):\n\t    _Fun = FiniteFunction\n\tclass BipartiteMultigraph(AbstractBipartiteMultigraph):\n", "    \"\"\" CuPy-backed bipartite multigraphs \"\"\"\n\t    _Fun = FiniteFunction\n\tclass Diagram(AbstractDiagram):\n\t    \"\"\" CuPy-backed string diagrams \"\"\"\n\t    _Fun = FiniteFunction\n\t    _Graph = BipartiteMultigraph\n\tclass SegmentedFiniteFunction(AbstractSegmentedFiniteFunction):\n\t    _Array = cupy\n\t    _Fun = FiniteFunction\n\t# If we had types, this would be 'type-level function' giving us the\n", "# implementation of each of these classes in terms of the base (Numpy-backed\n\t# FiniteFunction)\n\tFiniteFunction.IndexedCoproduct = IndexedCoproduct\n\tFiniteFunction.BipartiteMultigraph = BipartiteMultigraph\n\tFiniteFunction.Diagram = Diagram\n"]}
{"filename": "yarrow/diagram.py", "chunked_list": ["\"\"\"Diagrams are the main datastructure of yarrow, and represent string diagrams.\n\tThe :py:class:`AbstractDiagram` is the main datastructure of yarrow.\n\tIt represents a string diagram as a *cospan of bipartite multigraphs*.\n\tFor example, the diagram below left is represented internally below right:\n\t.. image:: /string-diagram-side-by-side.svg\n\t   :scale: 150%\n\t   :align: center\n\t   :alt: a string diagram and its representation as a cospan\n\tThis representation (the :py:class:`AbstractDiagram` class) consists of three\n\tthings:\n", "1. An :py:class:`AbstractBipartiteMultigraph` ``G`` (center grey box)\n\t2. The *source map* ``s``: the dotted arrows from the *left* blue box to the center box\n\t3. The *target map* ``t``: the dotted arrows from the *right* blue box to the center box\n\tThe center box `G` encodes the internal wiring of the diagram, while ``s`` and\n\t``t`` encode the \"dangling wires\" on the left and right.\n\tThe :py:class:`AbstractDiagram` class is a backend-agnostic implementation.\n\tConcrete implementations choose a *backend*, which is an implementation of the classes\n\t:py:class:`AbstractFiniteFunction` and :py:class:`AbstractBipartiteMultigraph`.\n\tFor example, numpy-backed diagrams are implemented by the :py:class:`Diagram` class,\n\twhich inherits :py:class:`AbstractDiagram` and sets two class members:\n", "- `_Fun = yarrow.array.numpy`\n\t- `_Graph = yarrow.bipartite_multigraph.BipartiteMultigraph`\n\tFor more information on backends, see :ref:`backends`.\n\tSummary\n\t-------\n\t.. autosummary::\n\t    :template: class.rst\n\t    AbstractDiagram\n\t\"\"\"\n\tfrom dataclasses import astuple\n", "from yarrow.finite_function import AbstractFiniteFunction\n\tfrom yarrow.bipartite_multigraph import AbstractBipartiteMultigraph\n\t# for tensor_operations\n\tfrom yarrow.segmented.operations import Operations\n\tclass AbstractDiagram:\n\t    \"\"\" Implements diagrams parametrised by an underlying choice of backend.\n\t    To use this class, inherit from it and set class members:\n\t    - ``_Fun`` (finite functions)\n\t    - ``_Graph`` (bipartite multigraphs)\n\t    See for example the :py:class:`Diagram` class, which uses numpy-backed arrays.\n", "    \"\"\"\n\t    def __init__(self,\n\t                 s: AbstractFiniteFunction,\n\t                 t: AbstractFiniteFunction,\n\t                 G: AbstractBipartiteMultigraph):\n\t        \"\"\"Construct a :py:class:`AbstractDiagram` from a triple ``(s, t, G)``.\n\t        Description\n\t        Args:\n\t            s: Finite function of type `A → G.W`\n\t            t: Finite function of type `B → G.W`\n", "            G: An :py:class:`AbstractBipartiteMultigraph`\n\t        \"\"\"\n\t        self.s = s\n\t        self.t = t\n\t        self.G = G\n\t        # the cospan (s, t) is a pair of arrows\n\t        #     s   t\n\t        #   A → G(W) ← B\n\t        # so we need to verify types work out.\n\t        assert G.W == s.target\n", "        assert G.W == t.target\n\t        # Lastly, the underlying finite function type should be the same.\n\t        _Fun = type(self)._Fun\n\t        assert _Fun == type(s)\n\t        assert _Fun == type(t)\n\t        assert _Fun == G._Fun\n\t    @property\n\t    def wires(self):\n\t        \"\"\"\n\t        Return the number of 'wires' in the diagram.\n", "        A wire is a node in the graph corresponding to a wire of the string diagram.\n\t        \"\"\"\n\t        return self.G.W\n\t    @property\n\t    def operations(self):\n\t        \"\"\"Return the number of generating operations in the diagram.\"\"\"\n\t        return self.G.X\n\t    @property\n\t    def shape(self):\n\t        \"\"\" Return the arity and coarity of the diagram. \"\"\"\n", "        return self.s.source, self.t.source\n\t    @property\n\t    def type(self):\n\t        \"\"\" Return a pair of finite functions representing the type of the morphism.\n\t        Returns:\n\t            (tuple): tuple of:\n\t                source(AbstractFiniteFunction): typed `self.s.domain → Σ₀`\n\t                target(AbstractFiniteFunction): typed `self.t.domain → Σ₀`\n\t        \"\"\"\n\t        wire_labels = self.G.wn\n", "        return (self.s >> wire_labels, self.t >> wire_labels)\n\t    def __eq__(f, g):\n\t        return f.s == g.s and f.t == g.t and f.G == g.G\n\t    @classmethod\n\t    def empty(cls, wn : AbstractFiniteFunction, xn: AbstractFiniteFunction):\n\t        \"\"\"\n\t        Args:\n\t            wn: A FiniteFunction typed `0 → Σ₀`: giving the generating objects\n\t            xn: A FiniteFunction typed `0 → Σ₁`: giving the generating operations\n\t        Returns:\n", "            The empty diagram for the monoidal signature (Σ₀, Σ₁)\n\t        Note that for a non-finite signature, we allow the targets of ``wn`` and\n\t        ``xn`` to be ``None``.\n\t        \"\"\"\n\t        s = t = cls._Fun.initial(0)\n\t        return cls(s, t, cls._Graph.empty(wn, xn))\n\t    @classmethod\n\t    def identity(cls, wn: AbstractFiniteFunction, xn: AbstractFiniteFunction):\n\t        \"\"\"\n\t        Args:\n", "            wn: A FiniteFunction typed `W → Σ₀`: giving the generating objects\n\t            xn: A FiniteFunction typed `0 → Σ₁`: giving the generating operations\n\t        Returns:\n\t            AbstractDiagram: The identity diagram with `W` wires labeled `wn : W → Σ₀` whose empty set of generators is labeled in Σ₁\n\t        \"\"\"\n\t        assert xn.source == 0\n\t        s = cls._Fun.identity(wn.source)\n\t        t = cls._Fun.identity(wn.source)\n\t        G = cls._Graph.discrete(wn, xn)\n\t        return cls(s, t, G)\n", "    @classmethod\n\t    def twist(cls, wn_A: AbstractFiniteFunction, wn_B: AbstractFiniteFunction, xn: AbstractFiniteFunction):\n\t        \"\"\"\n\t        Args:\n\t            wn_A : typed `A → Σ₀`\n\t            wn_B : typed `B → Σ₀`\n\t            xn   : typed `0 → Σ₁`\n\t        Returns:\n\t            AbstractDiagram: The symmetry diagram `σ : A ● B → B ● A`.\n\t        \"\"\"\n", "        assert xn.source == 0\n\t        wn = wn_A + wn_B\n\t        s = cls._Fun.identity(wn.source)\n\t        t = cls._Fun.twist(wn_A.source, wn_B.source)\n\t        G = cls._Graph.discrete(wn, xn)\n\t        return cls(s, t, G)\n\t    @classmethod\n\t    def spider(cls,\n\t               s: AbstractFiniteFunction,\n\t               t: AbstractFiniteFunction,\n", "               w: AbstractFiniteFunction,\n\t               x: AbstractFiniteFunction):\n\t        \"\"\"Create a *Frobenius Spider* (see Definition 2.8, Proposition 4.7 of :cite:p:`dpafsd`).\n\t        Args:\n\t            s : source map typed `S → W`\n\t            t : target map typed `T → W`\n\t            w : wires typed `W → Σ₀`\n\t            x : empty set of operations `0 → Σ₁`\n\t        Returns:\n\t            AbstractDiagram: A frobenius spider with `S` inputs and `T` outputs.\n", "        \"\"\"\n\t        assert x.source == 0\n\t        assert w.source == s.target\n\t        assert w.source == t.target\n\t        G = cls._Graph.discrete(w, x)\n\t        return cls(s, t, G)\n\t    @classmethod\n\t    def half_spider(cls,\n\t                    s: AbstractFiniteFunction,\n\t                    w: AbstractFiniteFunction,\n", "                    x: AbstractFiniteFunction):\n\t        \"\"\" Create a *Frobenius Half-Spider*, which is a spider whose target map is the identity \"\"\"\n\t        # s : A → W\n\t        # w : W → Σ₀\n\t        assert s.target == w.source\n\t        t = cls._Fun.identity(w.source)\n\t        return cls.spider(s, t, w, x)\n\t    def dagger(self):\n\t        \"\"\"Swap the *source* and *target* maps of the diagram.\n\t        Returns:\n", "            AbstractDiagram: The dagger functor applied to this diagram.\n\t        \"\"\"\n\t        return type(self)(self.t, self.s, self.G)\n\t    @classmethod\n\t    def singleton(cls, a: AbstractFiniteFunction, b: AbstractFiniteFunction, xn: AbstractFiniteFunction):\n\t        \"\"\" Construct a diagram consisting of a single operation (Definition 4.9, :cite:p:`dpafsd`).\n\t        Args:\n\t            x: A single operation represented as an AbstractFiniteFunction of type `1 → Σ₁`\n\t            a: The input type of `x` as a finite function `A → Σ₀`\n\t            b: The output type of `x` as a finite function `B → Σ₀`\n", "        Returns:\n\t            AbstractDiagram: a diagram with a single generating operation.\n\t        \"\"\"\n\t        Array = cls._Fun._Array\n\t        F = cls._Fun\n\t        assert F == type(a)\n\t        assert F == type(b)\n\t        assert F == type(xn)\n\t        # x : 1 → Σ₁\n\t        assert xn.source == 1\n", "        # Must be able to take coproduct a + b because\n\t        #   wn : A + B → Σ₀\n\t        assert a.target == b.target\n\t        # wi : A → A + B     wo : B → A + B\n\t        wi = F.inj0(a.source, b.source)\n\t        wo = F.inj1(a.source, b.source)\n\t        G = cls._Graph(\n\t            wi=wi,\n\t            wo=wo,\n\t            # xi : A → 1         xo : B → 1\n", "            xi = F.terminal(a.source),\n\t            xo = F.terminal(b.source),\n\t            # wn : A + B → Σ₀\n\t            wn = a + b,\n\t            # pi' : A → Nat       po' : B → Nat\n\t            # pi = pi';ι          po  = po';ι\n\t            pi = F(None, Array.arange(a.source)),\n\t            po = F(None, Array.arange(b.source)),\n\t            xn = xn,\n\t        )\n", "        # Note: s=inj0, t=inj1, so we just reuse wi and wo.\n\t        return cls(s=wi, t=wo, G=G)\n\t    def tensor(f: 'AbstractDiagram', g: 'AbstractDiagram'):\n\t        \"\"\"Stack one diagram atop another, so `f.tensor(g)` is the diagram depicted by\n\t        .. image:: /tensor-f-g.svg\n\t           :scale: 150%\n\t           :align: center\n\t           :alt: a depiction of the tensor product of diagrams\n\t        Args:\n\t            g(AbstractDiagram): An arbitrary diagram\n", "        Returns:\n\t            AbstractDiagram: The tensor product of this diagram with `g`.\n\t        \"\"\"\n\t        return type(f)(\n\t            s = f.s @ g.s,\n\t            t = f.t @ g.t,\n\t            G = f.G @ g.G)\n\t    def __matmul__(f, g):\n\t        \"\"\" Shorthand for :py:meth:`yarrow.Diagram.tensor`.\n\t        f @ g == f.tensor(g)\n", "        \"\"\"\n\t        return f.tensor(g)\n\t    def compose(f: 'AbstractDiagram', g: 'AbstractDiagram'):\n\t        \"\"\"Compose this diagram with `g`, so `f.compose(g)` is the diagram\n\t        .. image:: /compose-f-g.svg\n\t           :scale: 150%\n\t           :align: center\n\t           :alt: a depiction of the tensor product of diagrams\n\t        Args:\n\t            g(AbstractDiagram): A diagram with `g.type[0] == self.type[1]`\n", "        Returns:\n\t            AbstractDiagram: The tensor product of this diagram with `g`.\n\t        Raises:\n\t            AssertionError: If `g.type[0] != f.type[1]`\n\t        \"\"\"\n\t        assert f.type[1] == g.type[0]\n\t        h = f @ g\n\t        q = f.t.inject0(g.G.W).coequalizer(g.s.inject1(f.G.W))\n\t        return type(f)(\n\t            s = f.s.inject0(g.G.W) >> q,\n", "            t = g.t.inject1(f.G.W) >> q,\n\t            G = h.G.coequalize_wires(q))\n\t    def __rshift__(f, g):\n\t        return f.compose(g)\n\t    @classmethod\n\t    def tensor_list(cls, ds: 'List[AbstractDiagram]', wn=None, xn=None):\n\t        \"\"\" Compute the tensor product of a list of diagrams. O(n) time in the size of the result.\n\t        .. warning::\n\t            Does not speed up to O(log n) in the parallel case\n\t        \"\"\"\n", "        if len(ds) == 0:\n\t            assert wn is not None\n\t            assert xn is not None\n\t            return cls.empty(wn, xn)\n\t        assert wn is None\n\t        assert xn is None\n\t        s = cls._Fun.tensor_list([d.s for d in ds])\n\t        t = cls._Fun.tensor_list([d.t for d in ds])\n\t        G = cls._Graph.coproduct_list([d.G for d in ds])\n\t        return cls(s, t, G)\n", "    @classmethod\n\t    def tensor_operations(cls, ops: Operations):\n\t        pass # hide the docstring for now\n\t        \"\"\" Compute the X-fold tensoring of operations\n\t        .. code-block:: text\n\t            xn : X → Σ₁\n\t        whose typings are given by the segmented finite functions\n\t        .. code-block:: text\n\t            s_type : sum_{i ∈ X} arity(xn(i))   → Σ₀\n\t            t_type : sum_{i ∈ X} coarity(xn(i)) → Σ₀\n", "        See Proposition 4.13 in :cite:t:`dpafsd`.\n\t        \"\"\"\n\t        Fun   = cls._Fun\n\t        Array = Fun._Array\n\t        xn, s_type, t_type = ops.xn, ops.s_type, ops.t_type\n\t        r = Array.arange(0, xn.source)\n\t        # TODO: FIXME: redundant computation.\n\t        # we do a sum of s_type/t_type sources, but we already do a cumsum in\n\t        # segmented_arange, so this is wasted effort!\n\t        Ki = Array.sum(s_type.sources.table)\n", "        Ko = Array.sum(t_type.sources.table)\n\t        i0 = Fun.inj0(Ki, Ko)\n\t        i1 = Fun.inj1(Ki, Ko)\n\t        return cls(\n\t            s = i0,\n\t            t = i1,\n\t            G = cls._Graph(\n\t                xn = xn,\n\t                # Tensor product of terminal maps\n\t                # e.g. 1 1 1 | 2 2 | 3 3 3 3 ...\n", "                xi = Fun(xn.source, Array.repeat(r, s_type.sources.table)),\n\t                xo = Fun(xn.source, Array.repeat(r, t_type.sources.table)),\n\t                # Coproduct of ι₀ maps\n\t                # e.g. 0 1 2 | 0 1 | 0 1 2 3 ...\n\t                pi = Fun(None, Array.segmented_arange(s_type.sources.table)),\n\t                po = Fun(None, Array.segmented_arange(t_type.sources.table)),\n\t                # wires: sources first, then targets\n\t                wi = i0,\n\t                wo = i1,\n\t                wn = s_type.values + t_type.values))\n"]}
{"filename": "yarrow/__init__.py", "chunked_list": ["# The default backend is numpy\n\tfrom yarrow.numpy import *\n"]}
{"filename": "yarrow/bipartite_multigraph.py", "chunked_list": ["\"\"\"A representation of the \"internal wiring\" of string diagrams.\n\tBipartite multigraphs have edge labels corresponding to the \"ports\" of\n\toperations, and node labels either generating objects or generating operations\n\tof a signature Σ.\n\tAs with other classes, these graphs are implemented with an abstract base class\n\t:py:class:`AbstractBipartiteMultigraph`,\n\twhose concrete instantiations choose a backend.\n\tFor example, :py:class:`BipartiteMultigraph` are backed by numpy arrays.\n\t\"\"\"\n\tfrom dataclasses import dataclass\n", "from yarrow.finite_function import AbstractFiniteFunction\n\tclass AbstractBipartiteMultigraph:\n\t    \"\"\" The type of bipartite multigraphs, parametrised by cls._Fun, the\n\t    underlying representation of finite functions \"\"\"\n\t    def __init__(self, wi, wo, xi, xo, wn, pi, po, xn):\n\t        \"\"\"Create a BipartiteMultigraph from its component finite functions.\n\t        For more details see :cite:p:`dpafsd`, Section 3.2.\n\t        \"\"\"\n\t        # Edge/Wire incidence\n\t        self.wi = wi\n", "        self.wo = wo\n\t        # Edge/Generator incidence\n\t        self.xi = xi\n\t        self.xo = xo\n\t        # Wire + generator labels\n\t        self.xn = xn\n\t        self.wn = wn\n\t        # Port labels\n\t        self.pi = pi\n\t        self.po = po\n", "        # Check schema of bipartite multigraphs is satisfied\n\t        if wi.target != wo.target:\n\t            raise ValueError(\"wi.target != wo.target\")\n\t        if xi.target != xo.target:\n\t            raise ValueError(\"xi.target != xo.target\")\n\t        if wi.source != xi.source:\n\t            raise ValueError(\"wi.source != xi.source\")\n\t        if wo.source != xo.source:\n\t            raise ValueError(\"wo.source != xo.source\")\n\t        if wn.source != wi.target:\n", "            raise ValueError(\"wn.source != wi.target\")\n\t        if wn.source != wi.target:\n\t            raise ValueError(\"wn.source != wi.target\")\n\t        if pi.source != xi.source:\n\t            raise ValueError(\"pi.source != xi.source\")\n\t        if po.source != xo.source:\n\t            raise ValueError(\"pi.source != xo.source\")\n\t        if xn.source != xi.target:\n\t            raise ValueError(\"xn.source != xi.target\")\n\t    @property\n", "    def W(self):\n\t        \"\"\"Test\n\t        Returns:\n\t            G(W)\n\t        \"\"\"\n\t        # wn : G(W) → Σ₀\n\t        return self.wn.source\n\t    @property\n\t    def Ei(self):\n\t        \"\"\"Test\n", "        Returns:\n\t            The number of *input edges* in the graph\n\t        \"\"\"\n\t        return self.wi.source\n\t    @property\n\t    def Eo(self):\n\t        \"\"\"\n\t        Returns:\n\t            The number of *output edges* in the graph\n\t        \"\"\"\n", "        return self.wo.source\n\t    @property\n\t    def X(self):\n\t        \"\"\"\n\t        Returns:\n\t            int: Corresponds to G(X), the number of generating operations in the diagram\"\"\"\n\t        # xn : G(X) → Σ₁\n\t        return self.xn.source\n\t    @classmethod\n\t    def empty(cls, wn, xn):\n", "        \"\"\"\n\t        Args:\n\t            wn: Finite function typed `0 → Σ₀`\n\t            xn: Finite function typed `0 → Σ₁`\n\t        Returns:\n\t            AbstractBipartiteMultigraph: The empty bipartite multigraph with no edges and no nodes.\n\t        \"\"\"\n\t        assert wn.source == 0\n\t        assert xn.source == 0\n\t        e = cls._Fun.initial(0)\n", "        pi = po = cls._Fun.initial(None)\n\t        return cls(e, e, e, e, wn, pi, po, xn)\n\t    @classmethod\n\t    def discrete(cls, wn: AbstractFiniteFunction, xn: AbstractFiniteFunction):\n\t        \"\"\"\n\t        Create the discrete graph of n wires for a given monoidal signature Σ\n\t        whose maps are all initial except for `wn`.\n\t        Args:\n\t            wn: An array of wire labels as a finite function typed `n → Σ₀`\n\t            xn: The type of operations as an empty finite function typed `0 → Σ₁`\n", "        \"\"\"\n\t        if xn.source != 0:\n\t            raise ValueError(\"xn.source != 0\")\n\t        return cls(\n\t            # There are no edges, so we make empty maps for all edge data\n\t            wi = cls._Fun.initial(wn.source),\n\t            wo = cls._Fun.initial(wn.source),\n\t            xi = cls._Fun.initial(0),\n\t            xo = cls._Fun.initial(0),\n\t            # TODO: dirty hack alert: None represents any \"non-finite\" codomain here.\n", "            # In this case, we need edge_data : E → Nat\n\t            # but this (obviously) cannot be represented by a finite function.\n\t            # This is justified because both these maps factor through some\n\t            # finite function: we just don't know what it is at this point in\n\t            # the code.\n\t            pi = cls._Fun.initial(None),\n\t            po = cls._Fun.initial(None),\n\t            wn = wn, # there are w_label.target wires\n\t            xn = xn, # and no operation nodes\n\t        )\n", "    def __eq__(a, b):\n\t        return \\\n\t            a.wi == b.wi and \\\n\t            a.wo == b.wo and \\\n\t            a.xi == b.xi and \\\n\t            a.xo == b.xo and \\\n\t            a.wn == b.wn and \\\n\t            a.pi == b.pi and \\\n\t            a.po == b.po and \\\n\t            a.xn == b.xn\n", "    def coproduct(f, g):\n\t        \"\"\"Compute the coproduct of bipartite multigraphs\n\t        Args:\n\t            g: an arbitrary AbstractBipartiteMultigraph over the same signature\n\t        Returns:\n\t            The coproduct ``self + g``.\n\t        \"\"\"\n\t        # check signatures match\n\t        assert f.wn.target == g.wn.target\n\t        assert f.xn.target == g.xn.target\n", "        return type(f)(\n\t            # Tensor product of data\n\t            wi=f.wi @ g.wi,\n\t            wo=f.wo @ g.wo,\n\t            xi=f.xi @ g.xi,\n\t            xo=f.xo @ g.xo,\n\t            # Coproduct of attributes\n\t            wn=f.wn + g.wn,\n\t            pi=f.pi + g.pi,\n\t            po=f.po + g.po,\n", "            xn=f.xn + g.xn,\n\t        )\n\t    def __matmul__(f, g):\n\t        return f.coproduct(g)\n\t    @classmethod\n\t    def coproduct_list(cls, Gs: 'List[AbstractBipartiteMultigraph]', wn=None, xn=None):\n\t        \"\"\" Compute the coproduct of a list of bipartite multigraphs. O(n) in the size of the result.\n\t        .. warning::\n\t            Does not speed up to O(log n) in the parallel case.\n\t        \"\"\"\n", "        if len(Gs) == 0:\n\t            assert wn is not None\n\t            assert xn is not None\n\t            return cls.empty(wn, xn)\n\t        # can't specify Σ if Gs is non-empty\n\t        assert wn is None\n\t        assert xn is None\n\t        return cls(\n\t            wi=cls._Fun.tensor_list([g.wi for g in Gs]),\n\t            wo=cls._Fun.tensor_list([g.wo for g in Gs]),\n", "            xi=cls._Fun.tensor_list([g.xi for g in Gs]),\n\t            xo=cls._Fun.tensor_list([g.xo for g in Gs]),\n\t            wn=cls._Fun.coproduct_list([g.wn for g in Gs]),\n\t            pi=cls._Fun.coproduct_list([g.pi for g in Gs]),\n\t            po=cls._Fun.coproduct_list([g.po for g in Gs]),\n\t            xn=cls._Fun.coproduct_list([g.xn for g in Gs]))\n\t    # Apply a morphism α of bipartite multigraphs whose only\n\t    # non-identity component is α_W = q.\n\t    def coequalize_wires(self, q : AbstractFiniteFunction):\n\t        \"\"\"\n", "        Apply a morphism α of bipartite multigraphs\n\t        whose only non-identity component α_W = q\n\t        for some coequalizer q.\n\t        Args:\n\t            q: An AbstractFiniteFunction which is a coequalizer.\n\t        Returns:\n\t            AbstractBipartiteMultigraph: The bipartite multigraph equal to the target of α.\n\t        \"\"\"\n\t        assert self.wn.source == q.source\n\t        u = universal(q, self.wn)\n", "        # Check that resulting diagram commutes\n\t        # TODO: this is unnecessary extra computation when the user knows that q is a coequalizer.\n\t        # Make a flag?\n\t        Array = type(q)._Array\n\t        if not (q >> u) == self.wn:\n\t            raise ValueError(f\"Universal morphism doesn't make {q};{u}, {self.wn} commute. Is q really a coequalizer?\")\n\t        return type(self)(\n\t                wi=self.wi >> q,\n\t                wo=self.wo >> q,\n\t                wn=u,\n", "                xi=self.xi,\n\t                xo=self.xo,\n\t                pi=self.pi,\n\t                po=self.po,\n\t                xn=self.xn)\n\t# Let G be a bipartite multigraph.\n\t# Given a coequalizer q : G(W) → Q of finite functions,\n\t# Define the map\n\t#   α : G → G'\n\t# with\n", "#   α_W = q\n\t#   α_Y = id_Y\n\t#\n\t# Then we need a map\n\t#   Q(W) → Σ₀\n\t# which we can get because\n\t#   G(wn) : G(W) → Σ₀\n\t# coequalizes, and so there exists some unique\n\t#   u : Q(W) → Σ₀\n\t# such that\n", "#   q ; u = G(wn)\n\t# And this can be computed as follows:\n\t#   u[q[i]] = G(wn)\n\tdef universal(q: AbstractFiniteFunction, f: AbstractFiniteFunction):\n\t    \"\"\"\n\t    Given a coequalizer q : B → Q of morphisms a, b : A → B\n\t    and some f : B → B' such that f(a) = f(b),\n\t    Compute the universal map u : Q → B'\n\t    such that q ; u = f.\n\t    \"\"\"\n", "    target = f.target\n\t    u = q._Array.zeros(q.target, dtype=f.table.dtype)\n\t    # TODO: in the below we assume the PRAM CRCW model: multiple writes to the\n\t    # same memory location in the 'u' array can happen in parallel, with an\n\t    # arbitrary write succeeding.\n\t    # Note that this doesn't affect correctness because q and f are co-forks,\n\t    # and q is a coequalizer.\n\t    # However, this won't perform well on e.g., GPU hardware. FIXME!\n\t    u[q.table] = f.table\n\t    return type(f)(target, u)\n"]}
{"filename": "yarrow/numpy/layer.py", "chunked_list": ["\"\"\" Layered decomposition for numpy-backed diagrams.\n\tNote that this (currently) uses SciPy sparse arrays, so it can't be used for diagrams backed\n\tby other array libraries (e.g., CuPy).\n\tUse the ``layer`` function to assign a *layering* to operations in the diagram.\n\tThis is like a topological sort, except multiple operations can be assigned to\n\tthe same layering.\n\t\"\"\"\n\timport numpy as np\n\timport scipy.sparse as sp\n\tfrom yarrow.numpy import *\n", "def make_sparse(s: FiniteFunction, t: FiniteFunction):\n\t    \"\"\"Given finite functions ``s : E → A`` and ``t : E → B``\n\t    representing a bipartite graph ``G : A → B``,\n\t    return the sparse ``B×A`` adjacency matrix representing ``G``.\n\t    \"\"\"\n\t    assert s.source == t.source\n\t    N = s.source\n\t    # (data, (row, col))\n\t    # rows are *outputs*, so row = t.table\n\t    # cols are *inputs* so col = s.table\n", "    return sp.csr_array((np.ones(N, dtype=bool), (t.table, s.table)), shape=(t.target, s.target))\n\tdef operation_adjacency(d: Diagram):\n\t    \"\"\" Construct the underlying graph of operation adjacency from a diagram.\n\t    An operation ``x`` is adjacent to an operation ``y`` if there is a directed\n\t    path from ``x`` to ``y`` going through a single ■-node.\n\t    \"\"\"\n\t    # construct the adjacency matrix for generators\n\t    Mi = make_sparse(d.G.wi, d.G.xi)\n\t    Mo = make_sparse(d.G.wo, d.G.xo)\n\t    return Mi @ Mo.T\n", "# Kahn's Algorithm, but vectorised a bit.\n\t# https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm\n\t# Attempts to 'parallelize' the layering where possible, e.g.:\n\t#\n\t#          ○--\\\n\t#              ○---○\n\t#          ○--/\n\t#\n\t#       ---○--------\n\t#\n", "#  layer   0   1   2\n\t#\n\tdef kahn(adjacency: sp.csr_array):\n\t    \"\"\" A version of Kahn's algorithm which assigns a *layering* to each ○-node,\n\t    but where multiple nodes can have the same layering.\n\t    Returns a pair of arrays ``(order, visited)``.\n\t    ``order[v]`` is a natural number indicating the computed ordering of node ``v``,\n\t    and ``visited[v]`` is 1 if and only if ``v`` was visited while traversing the graph.\n\t    If not all vertices were visited, the graph had a cycle.\n\t    \"\"\"\n", "    n, m = adjacency.shape\n\t    assert n == m\n\t    adjacency = adjacency.astype(int)\n\t    # NOTE: convert to numpy ndarray instead of matrix given by adjacency;\n\t    # this makes indexing behaviour a bit nicer!\n\t    # NOTE: we use reshape here because adjacency.sum() gives different dimensions when input is 0x0!\n\t    indegree = np.asarray(adjacency.sum(axis=1, dtype=int)).reshape((n,))\n\t    # return values\n\t    visited  = np.zeros(n, dtype=bool)\n\t    order    = np.zeros(n, dtype=int)\n", "    # start at nodes with no incoming edges\n\t    start = (indegree == 0).nonzero()[0]\n\t    # initialize the frontier at the requested start nodes\n\t    k = len(start)\n\t    frontier = sp.csr_array((np.ones(k, int), (start, np.zeros(k, int))), (n, 1))\n\t    # as long as the frontier contains some nodes, we'll keep going.\n\t    depth = 0\n\t    while frontier.nnz > 0:\n\t        # Mark nodes in the current frontier as visited,\n\t        # and set their layering value ('order') to the current depth.\n", "        frontier_ixs = frontier.nonzero()[0]\n\t        visited[frontier_ixs] = True\n\t        order[frontier_ixs] = depth\n\t        # Find \"reachable\", which is the set of nodes adjacent to the current frontier.\n\t        # Decrement the indegree of each adjacent node by the number of edges between it and the frontier.\n\t        # NOTE: nodes only remain in the frontier for ONE iteration, so this\n\t        # will only decrement once for each edge.\n\t        reachable = adjacency @ frontier\n\t        reachable_ix = reachable.nonzero()[0]\n\t        indegree[reachable_ix] -= reachable.data\n", "        # Compute the new frontier: the reachable nodes with indegree equal to zero (no more remaining edges!)\n\t        # NOTE: indegree is an (N,1) matrix, so we select out the first row.\n\t        new_frontier_ixs = reachable_ix[indegree[reachable_ix] == 0]\n\t        k = len(new_frontier_ixs)\n\t        frontier = sp.csr_array((np.ones(k, int), (new_frontier_ixs, np.zeros(k, int))), shape=(n, 1))\n\t        # increment depth so the new frontier will be layered correctly\n\t        depth += 1\n\t    # Return the layering (order) and whether each node was visited.\n\t    # Note that if not np.all(visited), then there must be a cycle.\n\t    return order, visited\n", "def layer(d: Diagram):\n\t    \"\"\" Assign a *layering* to a diagram ``d``.\n\t        This computes a FiniteFunction ``f : G(X) → K``,\n\t        mapping ○-nodes of ``d.G`` to a natural number in the ``range(0, K)``.\n\t        This mapping is 'dense' in the sense that for each ``i ∈ {0..K}``,\n\t        there is always some ○-node v for which ``f(v) = i``.\n\t    \"\"\"\n\t    # (Partially) topologically sort it using a kahn-ish algorithm\n\t    # NOTE: if completed is not all True, then some generators were not ordered.\n\t    # In this case, we leave their ordering as 0, since they contain cycles.\n", "    M = operation_adjacency(d)\n\t    ordering, completed = kahn(M)\n\t    return FiniteFunction(d.operations, ordering), completed\n"]}
{"filename": "yarrow/numpy/__init__.py", "chunked_list": ["\"\"\" NumPy-backed finite functions, bipartite multigraphs, and diagrams.\n\t**Additional NumPy-backend-only code:**\n\t.. autosummary::\n\t    :toctree: _autosummary\n\t    :recursive:\n\t    yarrow.numpy.layer\n\t**NumPy-backed datastructures**:\n\t\"\"\"\n\t# Abstract implementations\n\tfrom yarrow.finite_function import *\n", "from yarrow.bipartite_multigraph import *\n\tfrom yarrow.diagram import *\n\tfrom yarrow.segmented.finite_function import AbstractIndexedCoproduct, AbstractSegmentedFiniteFunction\n\t# Array backend\n\timport yarrow.array.numpy as numpy_backend\n\tclass FiniteFunction(AbstractFiniteFunction):\n\t    \"\"\" NumPy-backed finite functions \"\"\"\n\t    _Array = numpy_backend\n\tclass IndexedCoproduct(AbstractIndexedCoproduct):\n\t    _Fun = FiniteFunction\n", "class BipartiteMultigraph(AbstractBipartiteMultigraph):\n\t    \"\"\" NumPy-backed bipartite multigraphs \"\"\"\n\t    _Fun = FiniteFunction\n\tclass Diagram(AbstractDiagram):\n\t    \"\"\" NumPy-backed string diagrams \"\"\"\n\t    _Fun = FiniteFunction\n\t    _Graph = BipartiteMultigraph\n\tclass SegmentedFiniteFunction(AbstractSegmentedFiniteFunction):\n\t    _Array = numpy_backend\n\t    _Fun = FiniteFunction\n", "# If we had types, this would be 'type-level function' giving us the\n\t# implementation of each of these classes in terms of the base (Numpy-backed\n\t# FiniteFunction)\n\tFiniteFunction.IndexedCoproduct = IndexedCoproduct\n\tFiniteFunction.BipartiteMultigraph = BipartiteMultigraph\n\tFiniteFunction.Diagram = Diagram\n"]}
{"filename": "yarrow/functor/optic.py", "chunked_list": ["\"\"\" Functors to diagrams of optics.\n\tThis module contains a class :py:class:`FrobeniusOpticFunctor`, which can be\n\tused to implement a functor from diagrams into a category of *optics*.\n\t\"\"\"\n\tfrom yarrow.diagram import AbstractDiagram\n\tfrom yarrow.functor.functor import *\n\t# A generic base class for optics\n\tclass FrobeniusOpticFunctor(FrobeniusFunctor):\n\t    @abstractmethod\n\t    def map_fwd_objects(self, objects: AbstractFiniteFunction) -> AbstractIndexedCoproduct:\n", "        ...\n\t    @abstractmethod\n\t    def map_rev_objects(self, objects: AbstractFiniteFunction) -> AbstractIndexedCoproduct:\n\t        ...\n\t    @abstractmethod\n\t    def residuals(self, ops: Operations) -> AbstractIndexedCoproduct:\n\t        ...\n\t    @abstractmethod\n\t    def map_fwd_operations(self, ops: Operations) -> AbstractDiagram:\n\t        ...\n", "    @abstractmethod\n\t    def map_rev_operations(self, ops: Operations) -> AbstractDiagram:\n\t        ...\n\t    ############################################################################\n\t    # Implementation\n\t    def map_objects(self, objects: AbstractFiniteFunction) -> AbstractIndexedCoproduct:\n\t        \"\"\" Implements map_objects in terms of ``map_fwd_objects`` and ``map_rev_objects``. \"\"\"\n\t        # look up concrete impl. of IndexedCoproduct\n\t        IndexedCoproduct = type(objects).IndexedCoproduct\n\t        # interleave forward and reverse objects\n", "        fwd = self.map_fwd_objects(objects)\n\t        rev = self.map_rev_objects(objects)\n\t        # this must hold because len(fwd) == len(objects).\n\t        N = len(objects)\n\t        assert N == len(fwd)\n\t        assert N == len(rev)\n\t        Fun = type(objects)\n\t        # return the blockwise interleaving of fwd/rev\n\t        i = Fun.cointerleave(N)\n\t        sources = Fun(None, fwd.sources.table + rev.sources.table)\n", "        both = IndexedCoproduct(\n\t            sources = fwd.sources + rev.sources,\n\t            values  = fwd.values  + rev.values)\n\t        sigma_1 = fwd.values.target\n\t        result = IndexedCoproduct(\n\t            sources = sources,\n\t            values  = Fun(sigma_1, both.coproduct(i).table))\n\t        return result\n\t    def map_operations(self, ops: Operations) -> AbstractDiagram:\n\t        \"\"\" Implements ``map_operations`` using ``map_fwd_operations`` and ``map_rev_operations``. \"\"\"\n", "        # TODO: add diagram from notes 2023-06-12\n\t        fwds, fwd_coarity = self.map_fwd_operations(ops)\n\t        revs, rev_arity   = self.map_rev_operations(ops)\n\t        Diagram = type(fwds)\n\t        Fun = fwds._Fun\n\t        xn = Fun.initial(fwds.G.xn.source) >> fwds.G.xn\n\t        # We need the sizes of each of these types in order to compute\n\t        # both the internal and external interleavings.\n\t        Afwd = self.map_fwd_objects(ops.s_type.values)\n\t        Arev = self.map_rev_objects(ops.s_type.values)\n", "        Bfwd = self.map_fwd_objects(ops.t_type.values)\n\t        Brev = self.map_rev_objects(ops.t_type.values)\n\t        Na = len(Afwd.sources) # number of source wires before functor\n\t        Nb = len(Bfwd.sources) # number of target wires before functor\n\t        # Get the residuals for each operation\n\t        M = self.residuals(ops)\n\t        # 'Internal' interleaving maps which bundle together all the Bfwd / M values\n\t        # so we can pass all the M's to \"revs\".\n\t        wn1 = Bfwd.values + M.values\n\t        fwd_output_sizes = Fun(None, fwd_coarity.table - M.sources.table) + M.sources\n", "        i1 = fwd_output_sizes.injections(Fun.cointerleave(len(ops.xn)))\n\t        i1 = Diagram.half_spider(i1, wn1, xn)\n\t        wn2 = M.values + Brev.values\n\t        rev_input_sizes = M.sources + Fun(None, rev_arity.table - M.sources.table)\n\t        i2 = rev_input_sizes.injections(Fun.cointerleave(len(ops.xn)))\n\t        i2 = Diagram.half_spider(i2, wn2, xn).dagger()\n\t        id_Bfwd = Diagram.identity(Bfwd.values, xn)\n\t        id_Brev = Diagram.identity(Brev.values, xn)\n\t        x = (fwds >> i1) @ id_Brev\n\t        y = id_Bfwd @ (i2 >> revs)\n", "        c = x >> y\n\t        # Bend wires to make an optic.\n\t        # Sources: fwd sources and rev targets\n\t        # Targets: fwd targets and rev sources\n\t        s = (Fun.inj0(len(Afwd.values), len(Brev.values)) >> c.s) + (Fun.inj1(len(Bfwd.values), len(Arev.values)) >> c.t)\n\t        t = (Fun.inj0(len(Bfwd.values), len(Arev.values)) >> c.t) + (Fun.inj1(len(Afwd.values), len(Brev.values)) >> c.s)\n\t        d = Diagram(s, t, c.G)\n\t        # Finally, interleave Afwd/Arev and Bfwd/Brev so optics can be tensored.\n\t        lhs = (Afwd.sources + Arev.sources).injections(Fun.cointerleave(Na))\n\t        rhs = (Bfwd.sources + Brev.sources).injections(Fun.cointerleave(Nb))\n", "        lhs = Diagram.half_spider(lhs, d.type[0], xn)\n\t        rhs = Diagram.half_spider(rhs, d.type[1], xn).dagger()\n\t        return lhs >> d >> rhs\n\tdef lens_fwd(ops: Operations, copy_label) -> AbstractDiagram:\n\t    \"\"\" :meta hide-value: \"\"\"\n\t    # Given a tensoring\n\t    #       f₀ ● f₁ ● ... ● fn\n\t    # Return the diagram representing the forward part of a lens optic, i.e.,\n\t    #       Δ ; (f₀ ● id)  ● Δ ; (f₁ ● id) ... \n\t    # This is given by \n", "    raise NotImplementedError(\"TODO\")\n\tdef adapt_optic(optic: AbstractDiagram, Afwd, Arev, Bfwd, Brev):\n\t    \"\"\" :meta hide-value: \"\"\"\n\t    raise NotImplementedError(\"TODO\")\n"]}
{"filename": "yarrow/functor/functor.py", "chunked_list": ["\"\"\" Strict Symmetric Monoidal Hypergraph Functors of Diagrams.\n\tThis module consists of two classes.\n\t* :py:class:`Functor`\n\t* :py:class:`FrobeniusFunctor`\n\tThe former is the generic interface that all functors must implement.\n\tIf you wish to define a functor, it is strongly recommended to define\n\ta class which inherits from :py:class:`FrobeniusFunctor`.\n\tThat way, instead of mapping arbitrary diagrams, you can define your functor in\n\tterms of mapping *generating operations*.\n\t\"\"\"\n", "from abc import abstractmethod\n\tfrom yarrow.finite_function import AbstractFiniteFunction, bincount\n\tfrom yarrow.diagram import AbstractDiagram\n\tfrom yarrow.segmented.finite_function import AbstractIndexedCoproduct\n\tfrom yarrow.segmented.operations import Operations\n\tfrom yarrow.decompose.frobenius import frobenius_decomposition\n\tclass Functor:\n\t    \"\"\" A base class for implementing strict symmetric monoidal hypergraph functors.\n\t    A Functor must implement two things:\n\t    * A mapping on *objects* :py:meth:`yarrow.functor.functor.Functor.map_objects`\n", "    * A mapping on *arrows* :py:meth:`yarrow.functor.functor.Functor.map_arrow`\n\t    The latter is hard to write from scratch!\n\t    Generally you will want to implement\n\t    :py:class:`yarrow.functor.functor.FrobeniusFunctor`.\n\t    Then you only have to implement a mapping on *generating operations*, and the\n\t    `map_arrow` function will be implemented for you.\n\t    \"\"\"\n\t    # In the paper we denote\n\t    # - map_objects as F₀\n\t    # - map_arrow   as F₁\n", "    @abstractmethod\n\t    def map_objects(self, objects: AbstractFiniteFunction) -> AbstractIndexedCoproduct:\n\t        \"\"\" The object map ``F₀`` of a functor ``F : Diagram_Σ → Diagram_Ω`` maps between\n\t        lists of generating objects `F₀ : Σ₀* → Ω₀*`.\n\t        Given an array of generating objects encoded as a FiniteFunction::\n\t            objects : W → Σ₀\n\t        This function must return an *indexed coproduct* representing the lists\n\t        to which each object was mapped::\n\t            sources : W            → Nat\n\t            values  : sum(sources) → Ω₀\n", "        (An \"indexed coproduct\" is basically a categorical name for a segmented array.)\n\t        \"\"\"\n\t        ...\n\t    @abstractmethod\n\t    def map_arrow(self, d: AbstractDiagram) -> AbstractDiagram:\n\t        # \"\"\"The arrow map of a functor \"\"\"\n\t        \"\"\"F.map_arrow(d) should apply the functor F to diagram d.\"\"\"\n\t        ...\n\tdef apply_finite_object_map(\n\t        finite_object_map: AbstractIndexedCoproduct,\n", "        wn: AbstractFiniteFunction) -> AbstractIndexedCoproduct:\n\t    \"\"\"Given an AbstractIndexedCoproduct ``f`` representing a family of ``K`` functions::\n\t        f_i : N_i → Ω₀*, i ∈ K\n\t    where ``f_i`` is the action of a functor ``F`` on generating object ``i``,\n\t    and an object ``wn = L(A)`` in the image of ``L``,\n\t    ``apply_finite_object_map(f, wn)`` computes the object ``F(wn)``\n\t    as a segmented array.\n\t    \"\"\"\n\t    assert isinstance(finite_object_map, AbstractIndexedCoproduct)\n\t    return type(finite_object_map)(\n", "        values  = finite_object_map.sources.injections(wn) >> finite_object_map.values,\n\t        sources = wn >> finite_object_map.sources)\n\tdef map_half_spider(swn: AbstractIndexedCoproduct, f: AbstractFiniteFunction) -> AbstractFiniteFunction:\n\t    \"\"\"Let ``swn = F.map_objects(f.type[1])`` for some functor ``F``,\n\t    and suppose ``S(f)`` is a half-spider.\n\t    Then ``S(map_half_spider(swn, f)) == F(S(f))``.\n\t    \"\"\"\n\t    # NOTE: swn should be the result of applying an object map to wn.\n\t    # swn = object_map(wn)\n\t    return swn.sources.injections(f)\n", "def decomposition_to_operations(d: 'AbstractDiagram') -> Operations:\n\t    \"\"\" Get the array of operations (and their types) from a Frobenius\n\t    decomposition.  \"\"\"\n\t    # NOTE: it's *very* important that d is a frobenius decomposition, since we\n\t    # directly use the maps d.G.wi and d.G.wo in the result.\n\t    Fun = d._Fun\n\t    Array = Fun._Array\n\t    # A concrete Fun implementation knows what its IndexedCoproduct class is;\n\t    # see concrete modules yarrow.numpy and yarrow.cupy for details!\n\t    IndexedCoproduct = Fun.IndexedCoproduct\n", "    s_type = IndexedCoproduct(\n\t        sources = Fun(None, bincount(d.G.xi).table),\n\t        values  = d.G.wi >> d.G.wn)\n\t    t_type = IndexedCoproduct(\n\t        sources = Fun(None, bincount(d.G.xo).table),\n\t        values  = d.G.wo >> d.G.wn)\n\t    return Operations(d.G.xn, s_type, t_type)\n\tclass FrobeniusFunctor(Functor):\n\t    \"\"\" A functor defined in terms of Frobenius decompositions.\n\t    Instead of specifying `map_arrow`, the implementor can specify `map_operations`.\n", "    This should map a *tensoring* of generators to a :py:class:`AbstractDiagram`.\n\t    \"\"\"\n\t    @abstractmethod\n\t    def map_objects(self, objects: AbstractFiniteFunction) -> AbstractIndexedCoproduct:\n\t        ...\n\t    @abstractmethod\n\t    def map_operations(self, ops: Operations) -> AbstractDiagram:\n\t        \"\"\"Given an array of generating operations::\n\t            xn : X → Σ₁\n\t        and their types::\n", "            s_type : sum_{i ∈ X} arity(xn(i))   → Σ₀\n\t            t_type : sum_{i ∈ X} coarity(xn(i)) → Σ₀\n\t        You must return an :py:class:`yarrow.diagram.AbstractDiagram` representing the tensoring::\n\t            F₁(xn(0)) ● F₁(xn(1)) ... F₁(xn(X - 1))\n\t        \"\"\"\n\t        ...\n\t    def map_arrow(self, d: AbstractDiagram) -> AbstractDiagram:\n\t        \"\"\" Apply a functor to a diagram. NOTE: You do not need to implement this method \"\"\"\n\t        Diagram = type(d)\n\t        d = frobenius_decomposition(d)\n", "        ops = decomposition_to_operations(d)\n\t        # swn = F(G(wn))\n\t        # ... is the IndexedCoproduct resulting from applying the functor to the\n\t        # wire labeling d.G.wn\n\t        swn = self.map_objects(d.G.wn)\n\t        h = self.map_operations(ops)\n\t        Fun   = h._Fun\n\t        Graph = h._Graph\n\t        xn = d._Fun.initial(h.G.xn.target, dtype=h.G.xn.table.dtype)\n\t        # build the morphisms (s ; x) ; (id ● h) ; (y ; t) from Proposition B.1\n", "        i = Diagram.identity(swn.values, xn)\n\t        # note: we use the source/target maps of i in constructing those of sx, yt\n\t        # to avoid constructing another array with the same data.\n\t        sx = Diagram(map_half_spider(swn, d.s), i.t + map_half_spider(swn, d.G.wi), Graph.discrete(swn.values, xn))\n\t        yt = Diagram(i.s + map_half_spider(swn, d.G.wo), map_half_spider(swn, d.t), Graph.discrete(swn.values, xn))\n\t        return (sx >> (i @ h) >> yt)\n\t################################################################################\n\t# Built-in functors, supplied as examples.\n\tdef identity_object_map(objects: AbstractFiniteFunction) -> AbstractIndexedCoproduct:\n\t    \"\"\" The object map of the identity functor \"\"\"\n", "    Fun = type(objects)\n\t    Array = objects._Array\n\t    IndexedCoproduct = Fun.IndexedCoproduct\n\t    # TODO: write a test for this!\n\t    targets_codomain = None if objects.target is None else objects.target + 1\n\t    return IndexedCoproduct(\n\t        sources = Fun(None, Array.ones(len(objects))),\n\t        values  = objects)\n\tclass Identity(Functor):\n\t    \"\"\" The identity functor, whose ``map_arrow`` method is implemented by\n", "    actually just returning the same diagram \"\"\"\n\t    def map_objects(self, objects) -> AbstractIndexedCoproduct:\n\t        \"\"\" \"\"\"\n\t        return identity_object_map(objects)\n\t    def map_arrow(self, d: AbstractDiagram) -> AbstractDiagram:\n\t        \"\"\" \"\"\"\n\t        return d\n\tclass FrobeniusIdentity(FrobeniusFunctor):\n\t    \"\"\" The identity functor, implemented using Frobenius decompositions.\n\t    This is provided as a simple example of how to use the FrobeniusFunctor type:\n", "    instead of implementing ``map_arrow`` directly, one can instead write a mapping\n\t    on tensorings of operations.\n\t    This is typically much easier, since for a strict monoidal functor F we have\n\t    ``F(f₀ ● f₁ ● ... ● fn) = F(f₀) ● F(f₁) ● ... ● F(fn)``\n\t    \"\"\"\n\t    def map_objects(self, objects: AbstractFiniteFunction):\n\t        \"\"\" \"\"\"\n\t        return identity_object_map(objects)\n\t    def map_operations(self, ops: Operations) -> AbstractDiagram:\n\t        \"\"\" \"\"\"\n", "        # look up concrete Diagram type from the FiniteFunction\n\t        Diagram = type(ops.xn).Diagram\n\t        return Diagram.tensor_operations(ops)\n"]}
{"filename": "yarrow/functor/__init__.py", "chunked_list": ["\"\"\" (Strict Symmetric Monoidal Hypergraph) Functors of Diagrams, and diagrams of optics.\n\tThe \"functor\" module provides two things:\n\t* An API for defining functors of diagrams\n\t* An API for defining functors into *optics*.\n\t.. warning::\n\t   This API is not described in :cite:t:`dpafsd`.\n\t.. autosummary::\n\t    :toctree: _autosummary\n\t    :recursive:\n\t    yarrow.functor.functor\n", "    yarrow.functor.optic\n\t\"\"\"\n"]}
{"filename": "yarrow/decompose/frobenius.py", "chunked_list": ["from yarrow.diagram import AbstractDiagram\n\tdef frobenius_decomposition(d: AbstractDiagram) -> AbstractDiagram:\n\t    \"\"\" Given a Diagram, permute its xi, xo, pi, and po maps to be in\n\t    (generator, port) order.\n\t    \"\"\"\n\t    # A Frobenius Decomposition is really just diagram whose edges are put in\n\t    # \"generator, port\" order.\n\t    # Obtaining the half spiders and tensorings from such a diagram is trivial:\n\t    # - s, t  are the source, targets of the diagram\n\t    # - everything else is just a component of the bipartite multigraph\n", "    p = sort_x_p(d.G.xi, d.G.pi)\n\t    q = sort_x_p(d.G.xo, d.G.po)\n\t    return type(d)(\n\t        s  = d.s,\n\t        t  = d.t,\n\t        G  = type(d.G)(\n\t            wi = p >> d.G.wi, # e_s\n\t            wo = q >> d.G.wo, # e_t\n\t            wn = d.G.wn,\n\t            xi = p >> d.G.xi,\n", "            xo = q >> d.G.xo,\n\t            pi = p >> d.G.pi,\n\t            po = q >> d.G.po,\n\t            xn = d.G.xn)\n\t    )\n\tdef sort_x_p(x, port):\n\t    # Sort by the compound key <x, port>.\n\t    # First argsorts port (\"p\")\n\t    # then stably argsorts by x\n\t    Array = x._Array\n", "    assert Array == port._Array\n\t    # x, port must be equal length arrays\n\t    assert x.source == port.source\n\t    p = Array.argsort(port.table)\n\t    table = Array.argsort(x.table[p])\n\t    return type(x)(x.source, table[p])\n"]}
{"filename": "yarrow/array/cupy.py", "chunked_list": ["\"\"\"A CuPy array backend.\n\t.. danger::\n\t   **Experimental Module**\n\t   This code is not thoroughly tested.\n\t   It's included here as a proof-of-concept for GPU acceleration.\n\t\"\"\"\n\timport cupy as cp\n\timport cupyx.scipy.sparse as sparse\n\tfrom cupyx.scipy.sparse import csgraph\n\tDEFAULT_DTYPE='int64'\n", "Type = cp.ndarray\n\tdef array(*args, **kwargs):\n\t    return cp.array(*args, **kwargs)\n\tdef max(*args, **kwargs):\n\t    return cp.max(*args, **kwargs)\n\tdef arange(*args, **kwargs):\n\t    return cp.arange(*args, **kwargs)\n\tdef all(*args, **kwargs):\n\t    return cp.all(*args, **kwargs)\n\tdef zeros(*args, **kwargs):\n", "    return cp.zeros(*args, **kwargs)\n\tdef ones(*args, **kwargs):\n\t    return cp.ones(*args, **kwargs)\n\tdef cumsum(*args, **kwargs):\n\t    return cp.cumsum(*args, **kwargs)\n\tdef sum(*args, **kwargs):\n\t    return cp.sum(*args, **kwargs)\n\tdef repeat(*args, **kwargs):\n\t    return cp.repeat(*args, **kwargs)\n\tdef concatenate(*args, **kwargs):\n", "    return cp.concatenate(*args, **kwargs)\n\t# Compute the connected components of a graph.\n\t# connected components of a graph, encoded as a list of edges between points\n\t# so we have s, t arrays encoding edges (s[i], t[i]) of a square n×n matrix.\n\t# NOTE: we have to wrap libraries since we don't tend to get a consistent interface,\n\t# and don't want to expose e.g. sparse graphs in the main code.\n\tdef connected_components(source, target, n, dtype=DEFAULT_DTYPE):\n\t    \"\"\"Compute the connected components of a graph with ``N`` nodes,\n\t    whose edges are encoded as a pair of arrays ``(source, target)``\n\t    such that the edges of the graph are ``source[i] → target[i]``.\n", "    Args:\n\t        source(array): A length-N array with elements in the set ``{0 .. N - 1}``.\n\t        target(array): A length-N array with elements in the set ``{0 .. N - 1}``.\n\t    Returns:\n\t        (int, array):\n\t        A pair ``(c, cc_ix)`` of the number of connected components\n\t        ``c`` and a mapping from nodes to connected components ``cc_ix``.\n\t    \"\"\"\n\t    if len(source) != len(target):\n\t        raise ValueError(\"Expected a graph encoded as a pair of arrays (source, target) of the same length\")\n", "    assert len(source) == len(target)\n\t    # TODO: FIXME:\n\t    # Something seems broken with cupy's weakly connected components but I can't\n\t    # figure out what.\n\t    # The workaround is to make the graph symmetric and compute *strongly*\n\t    # connected components.\n\t    # This is obviously lame and needs to be fixed.\n\t    # make an n×n sparse matrix representing the graph with edges\n\t    # source[i] → target[i]\n\t    # NOTE: dtype of the values is float32 since integers aren't supported.\n", "    # This doesn't matter though, we actually don't care about this data.\n\t    ones = cp.ones(2*len(source), dtype='float32')\n\t    s = cp.concatenate([source, target])\n\t    t = cp.concatenate([target, source])\n\t    M = sparse.csr_matrix((ones, (s, t)), shape=(n, n))\n\t    # compute & return connected components\n\t    c, cc_ix = csgraph.connected_components(M, connection='strong')\n\t    return c, cc_ix\n\tdef argsort(x):\n\t    return cp.argsort(x, kind='stable')\n", "################################################################################\n\t# Non-primitive routines (i.e., vector routines built out of primitives)\n\t# TODO: implement an \"asbtract array library\" class, inherit faster impls for numpy etc.\n\t# e.g.,\n\t#   x       = [ 2 3 0 5 ]\n\t#   output  = [ 0 1 | 0 1 2 | | 0 1 2 3 4 ]\n\t# compute ptrs\n\t#   p       = [ 0 2 5 5 ]\n\t#   r       = [ 0 0 | 2 2 2 | | 5 5 5 5 5 ]\n\t#   i       = [ 0 1   2 3 4     5 6 7 8 9 ]\n", "#   i - r   = [ 0 1 | 0 1 2 | | 0 1 2 3 4 ]\n\t# Note: r is computed as repeat(p, n)\n\t#\n\t# Complexity\n\t#   O(n)     sequential\n\t#   O(log n) PRAM CREW (cumsum is log n)\n\tdef segmented_arange(x):\n\t    \"\"\"Given an array of *sizes*, ``[x₀, x₁, ...]``  output an array equal to the concatenation\n\t    ``concatenate([arange(x₀), arange(x₁), ...])``\n\t    >>> FiniteFunction._Array.segmented_arange([5, 2, 3, 1])\n", "    array([0, 1, 2, 3, 4, 0, 1, 0, 1, 2, 0])\n\t    Params:\n\t        x: An array of the sizes of each \"segment\" of the output\n\t    Returns:\n\t        array:\n\t        segmented array with segment ``i`` equal to ``arange(i)``.\n\t    \"\"\"\n\t    x = cp.array(x)\n\t    # create segment pointer array\n\t    ptr = cp.zeros(len(x) + 1, dtype=x.dtype) # O(1) PRAM\n", "    ptr[1:] = cp.cumsum(x)                    # O(log x) PRAM\n\t    N = ptr[-1] # total size\n\t    r = cp.repeat(ptr[:-1], x) # O(log x) PRAM\n\t    return cp.arange(0, N) - r # O(1)     PRAM\n\tdef bincount(x, *args, **kwargs):\n\t    return cp.bincount(x, *args, **kwargs)\n\tdef full(n, x, *args, **kwargs):\n\t    return cp.full(n, x, *args, **kwargs)\n"]}
{"filename": "yarrow/array/__init__.py", "chunked_list": ["\"\"\"Array backends for :ref:`yarrow.finite_function`.\n\tEach sub-module of :ref:`yarrow.array` is an \"array backend\".\n\tArray backends provide a small number of *primitive functions*\n\tlike :func:`yarrow.array.numpy.zeros` and :func:`yarrow.array.numpy.arange` .\n\tSee :ref:`yarrow.array.numpy` (the default backend) for a list.\n\t.. warning::\n\t   This part of the API is likely to change significantly in future releases.\n\t.. autosummary::\n\t    :toctree: _autosummary\n\t    :recursive:\n", "    yarrow.array.numpy\n\t    yarrow.array.cupy\n\t\"\"\"\n"]}
{"filename": "yarrow/array/numpy.py", "chunked_list": ["\"\"\"numpy- and scipy-backed arrays and algorithms.\n\tAlmost all exposed functions are thin wrappers around numpy functions.\n\tThe only exceptions are:\n\t* :func:`connected_components` -- wraps `scipy.sparse.csgraph.connected_components`\n\t* :func:`segmented_arange` -- a subroutine implemented in terms of the other primitives\n\tThis module is the default array backend.\n\tIt's used by :py:class:`FiniteFunction`.\n\t\"\"\"\n\timport numpy as np\n\timport scipy.sparse as sparse\n", "DEFAULT_DTYPE='int64'\n\tType = np.ndarray\n\t\"\"\" The underlying array type used by functions in the backend. For numpy this is ``np.ndarray``.\n\t   :meta hide-value:\n\t\"\"\"\n\t# NOTE: we use :meta hide-value: above because numpy is mocked, so sphinx will\n\t# have the incorrect value in documentation.\n\tdef array(*args, **kwargs):\n\t    kwargs.setdefault('dtype', DEFAULT_DTYPE)\n\t    return np.fromiter(*args, **kwargs)\n", "def max(*args, **kwargs):\n\t    return np.max(*args, **kwargs)\n\tdef arange(*args, **kwargs):\n\t    return np.arange(*args, **kwargs)\n\tdef all(*args, **kwargs):\n\t    return np.all(*args, **kwargs)\n\tdef zeros(*args, **kwargs):\n\t    kwargs.setdefault('dtype', DEFAULT_DTYPE)\n\t    return np.zeros(*args, **kwargs)\n\tdef ones(*args, **kwargs):\n", "    kwargs.setdefault('dtype', DEFAULT_DTYPE)\n\t    return np.ones(*args, **kwargs)\n\tdef cumsum(*args, **kwargs):\n\t    return np.cumsum(*args, **kwargs)\n\tdef sum(*args, **kwargs):\n\t    return np.sum(*args, **kwargs)\n\tdef repeat(*args, **kwargs):\n\t    return np.repeat(*args, **kwargs)\n\tdef concatenate(*args, **kwargs):\n\t    return np.concatenate(*args, **kwargs)\n", "# Compute the connected components of a graph.\n\t# connected components of a graph, encoded as a list of edges between points\n\t# so we have s, t arrays encoding edges (s[i], t[i]) of a square n×n matrix.\n\t# NOTE: we have to wrap libraries since we don't tend to get a consistent interface,\n\t# and don't want to expose e.g. sparse graphs in the main code.\n\tdef connected_components(source, target, n, dtype=DEFAULT_DTYPE):\n\t    \"\"\"Compute the connected components of a graph with ``N`` nodes,\n\t    whose edges are encoded as a pair of arrays ``(source, target)``\n\t    such that the edges of the graph are ``source[i] → target[i]``.\n\t    Args:\n", "        source(array): A length-N array with elements in the set ``{0 .. N - 1}``.\n\t        target(array): A length-N array with elements in the set ``{0 .. N - 1}``.\n\t    Returns:\n\t        (int, array):\n\t        A pair ``(c, cc_ix)`` of the number of connected components\n\t        ``c`` and a mapping from nodes to connected components ``cc_ix``.\n\t    \"\"\"\n\t    if len(source) != len(target):\n\t        raise ValueError(\"Expected a graph encoded as a pair of arrays (source, target) of the same length\")\n\t    assert len(source) == len(target)\n", "    # make an n×n sparse matrix representing the graph with edges\n\t    # source[i] → target[i]\n\t    ones = np.ones(len(source), dtype=DEFAULT_DTYPE)\n\t    M = sparse.csr_matrix((ones, (source, target)), shape=(n, n))\n\t    # compute & return connected components\n\t    c, cc_ix = sparse.csgraph.connected_components(M)\n\t    return c, cc_ix\n\tdef argsort(x):\n\t    return np.argsort(x, kind='stable')\n\t################################################################################\n", "# Non-primitive routines (i.e., vector routines built out of primitives)\n\t# TODO: implement an \"asbtract array library\" class, inherit faster impls for numpy etc.\n\t# e.g.,\n\t#   x       = [ 2 3 0 5 ]\n\t#   output  = [ 0 1 | 0 1 2 | | 0 1 2 3 4 ]\n\t# compute ptrs\n\t#   p       = [ 0 2 5 5 ]\n\t#   r       = [ 0 0 | 2 2 2 | | 5 5 5 5 5 ]\n\t#   i       = [ 0 1   2 3 4     5 6 7 8 9 ]\n\t#   i - r   = [ 0 1 | 0 1 2 | | 0 1 2 3 4 ]\n", "# Note: r is computed as repeat(p, n)\n\t#\n\t# Complexity\n\t#   O(n)     sequential\n\t#   O(log n) PRAM CREW (cumsum is log n)\n\tdef segmented_arange(x):\n\t    \"\"\"Given an array of *sizes*, ``[x₀, x₁, ...]``  output an array equal to the concatenation\n\t    ``concatenate([arange(x₀), arange(x₁), ...])``\n\t    >>> FiniteFunction._Array.segmented_arange([5, 2, 3, 1])\n\t    array([0, 1, 2, 3, 4, 0, 1, 0, 1, 2, 0])\n", "    Params:\n\t        x: An array of the sizes of each \"segment\" of the output\n\t    Returns:\n\t        array:\n\t        segmented array with segment ``i`` equal to ``arange(i)``.\n\t    \"\"\"\n\t    x = np.array(x, dtype=DEFAULT_DTYPE)\n\t    # create segment pointer array\n\t    ptr = np.zeros(len(x) + 1, dtype=x.dtype) # O(1) PRAM\n\t    ptr[1:] = np.cumsum(x)                    # O(log x) PRAM\n", "    N = ptr[-1] # total size\n\t    r = np.repeat(ptr[:-1], x) # O(log x) PRAM\n\t    return np.arange(0, N) - r # O(1)     PRAM\n\tdef bincount(x, *args, **kwargs):\n\t    return np.bincount(x, *args, **kwargs)\n\tdef full(n, x, *args, **kwargs):\n\t    kwargs.setdefault('dtype', DEFAULT_DTYPE)\n\t    return np.full(n, x, *args, **kwargs)\n"]}
{"filename": "yarrow/segmented/finite_function.py", "chunked_list": ["\"\"\" Finite coproducts for Finite Functions.\n\tThey're used to parallelize several operations like the\n\t:py:meth:`yarrow.functor.functor.Functor.map_objects` method.\n\t\"\"\"\n\tfrom yarrow.finite_function import *\n\tfrom dataclasses import dataclass\n\timport yarrow.array.numpy as numpy\n\t@dataclass\n\tclass AbstractIndexedCoproduct:\n\t    \"\"\" A finite coproduct of finite functions.\n", "    You can think of it simply as a segmented array.\n\t    Categorically, it represents a finite coproduct::\n\t        Σ_{i ∈ N} f_i : s(f_i) → Y\n\t    as a pair of maps::\n\t        sources: N            → Nat     (target is natural numbers)\n\t        values : sum(sources) → Σ₀\n\t    \"\"\"\n\t    # sources: an array of segment sizes (note: not ptrs)\n\t    sources: AbstractFiniteFunction\n\t    # values: the values of the coproduct\n", "    values: AbstractFiniteFunction\n\t    def __post_init__(self):\n\t        # TODO FIXME: make this type derivable from AbstractFiniteFunction so we\n\t        # don't need to have one version for each backend?\n\t        self._Fun = type(self.sources)\n\t        self._Array = self._Fun._Array\n\t        # we always ignore the target of sources; this ensures\n\t        # roundtrippability.\n\t        assert self.sources.target is None\n\t        assert type(self.values) == self._Fun\n", "        assert len(self.values) == self._Array.sum(self.sources.table)\n\t    @property\n\t    def target(self):\n\t        return self.values.target\n\t    def __len__(self):\n\t        \"\"\" return the number of finite functions in the coproduct \"\"\"\n\t        return len(self.sources)\n\t    @classmethod\n\t    def from_list(cls, target, fs: List['AbstractFiniteFunction']):\n\t        \"\"\" Create an `AbstractIndexedCoproduct` from a list of :py:class:`AbstractFiniteFunction` \"\"\"\n", "        assert all(target == f.target for f in fs)\n\t        return cls(\n\t            sources=cls._Fun(None, [len(f) for f in fs], dtype=int),\n\t            values=cls._Fun.coproduct_list(fs, target=target))\n\t    def __iter__(self):\n\t        \"\"\" Yield an iterator of the constituent finite functions\n\t        >>> list(AbstractIndexedCoproduct.from_list(fs)) == fs\n\t        True\n\t        \"\"\"\n\t        N     = len(self.sources)\n", "        # Compute source pointers\n\t        s_ptr = self._Array.zeros(N+1, dtype=self.sources.table.dtype)\n\t        s_ptr[1:] = self._Array.cumsum(self.sources.table)\n\t        for i in range(0, N):\n\t            yield self._Fun(self.target, self.values.table[s_ptr[i]:s_ptr[i+1]])\n\t    def map(self, x: AbstractFiniteFunction):\n\t        \"\"\" Given an :py:class:`AbstractIndexedCoproduct` of finite functions::\n\t            Σ_{i ∈ X} f_i : Σ_{i ∈ X} A_i → B\n\t        and a finite function::\n\t            x : W → X\n", "        return a new :py:class:`AbstractIndexedCoproduct` representing::\n\t            Σ_{i ∈ X} f_{x(i)} : Σ_{i ∈ W} A_{x(i)} → B\n\t        \"\"\"\n\t        return type(self)(\n\t            sources = x >> self.sources,\n\t            values = self.coproduct(x))\n\t    def coproduct(self, x: AbstractFiniteFunction) -> AbstractFiniteFunction:\n\t        \"\"\"Like ``map`` but only computes the ``values`` array of an AbstractIndexedCoproduct\"\"\"\n\t        assert x.target == len(self.sources)\n\t        return self.sources.injections(x) >> self.values\n", "@dataclass\n\tclass AbstractSegmentedFiniteFunction:\n\t    \"\"\" An AbstractSegmentedFiniteFunction encodes a *tensoring* of finite functions.\n\t    This means we have to include an array of *targets* as well.\n\t    ..warning::\n\t        Deprecated\n\t    \"\"\"\n\t    # sizes of each of the N segments\n\t    # sources : N → Nat\n\t    sources: AbstractFiniteFunction\n", "    # Targets of each finite function.\n\t    # This is required if we want to tensor\n\t    # targets : N → Nat\n\t    targets: AbstractFiniteFunction\n\t    # values of all segments, flattened\n\t    # value : Σ_{i ∈ N} size(i) → Nat\n\t    values: AbstractFiniteFunction\n\t    def __post_init__(self):\n\t        cls = type(self)\n\t        assert self.sources._Array == cls._Array\n", "        assert self.targets._Array == cls._Array\n\t        assert self.values._Array == cls._Array\n\t        # Check that values : Σ_{i ∈ N} n(i)\n\t        assert self._Array.sum(self.sources.table) == len(self.values)\n\t        # lengths of sources and targets arrays are the same\n\t        assert len(self.sources) == len(self.targets)\n\t        if len(self.targets) == 0:\n\t            self._is_coproduct = True\n\t        else:\n\t            self._is_coproduct = \\\n", "                    self._Array.all(self.targets.table[:-1] == self.targets.table[1:])\n\t    # return the number of segments\n\t    def __len__(self):\n\t        return len(self.sources)\n\t    @classmethod\n\t    def from_list(cls, fs: List['AbstractFiniteFunction']):\n\t        \"\"\" Create a SegmentedFiniteFunction from a list of morphisms \"\"\"\n\t        # TODO: tidy up. do 1 iteration instead of 3\n\t        sources = cls._Array.array([ f.source for f in fs ])\n\t        targets = cls._Array.array([ f.target for f in fs ])\n", "        if len(fs) == 0:\n\t            max_source = 0\n\t            max_target = 0\n\t            values = cls._Array.zeros(0)\n\t        else:\n\t            max_source = cls._Array.max(sources) + 1\n\t            max_target = cls._Array.max(targets) + 1\n\t            values  = cls._Array.concatenate([ f.table for f in fs])\n\t        return cls(\n\t            sources = cls._Fun(max_source, sources),\n", "            targets = cls._Fun(max_target, targets),\n\t            values  = cls._Fun(None, values))\n\t    def __iter__(self):\n\t        Fun   = type(self.sources)\n\t        Array = Fun._Array\n\t        N     = len(self.sources)\n\t        s_ptr = Array.zeros(N+1, dtype=self.sources.table.dtype)\n\t        s_ptr[1:] = Array.cumsum(self.sources.table)\n\t        for i in range(0, N):\n\t            yield Fun(self.targets(i), self.values.table[s_ptr[i]:s_ptr[i+1]])\n", "    @property\n\t    def N(self):\n\t        # number of segments in the array\n\t        return self.sources.source\n\t    def slice(self, x: AbstractFiniteFunction):\n\t        # check indexing function won't go out of bounds\n\t        assert x.target == self.N\n\t        return self.sources.injections(x) >> self.values\n\t    # Since values is the concatenation of\n\t    # finite functions F_i : size(i) → Nat,\n", "    # i.e.,\n\t    #   values = F_0 + F_1 + ... + F_{N-1}\n\t    # we have\n\t    #   ι_x ; value = F_i\n\t    def coproduct(self, x: AbstractFiniteFunction):\n\t        \"\"\" sff.coproduct(x) computes an x-indexed coproduct of sff \"\"\"\n\t        # check all targets are the same\n\t        assert self._is_coproduct\n\t        # TODO FIXME: this is a hack, and is totally broken for \"empty\" coproducts, which MUST have target specified!\n\t        target = 0 if self.targets.source == 0 else self.targets(0)\n", "        return type(x)(target, self.slice(x).table)\n\t    def tensor(self, x: AbstractFiniteFunction):\n\t        \"\"\" sff.coproduct(x) computes an x-indexed *tensor* product of sff \"\"\"\n\t        table = self.slice(x).table\n\t        p = self._Array.zeros(x.source + 1, dtype='int64')\n\t        # p[1:] = self._Array.cumsum(self.targets.table[x.table])\n\t        p[1:] = self._Array.cumsum(self.targets.table[x.table])\n\t        z = self._Array.repeat(p[:-1], self.sources.table[x.table])\n\t        return type(x)(p[-1], table + z)\n"]}
{"filename": "yarrow/segmented/operations.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom yarrow.finite_function import AbstractFiniteFunction\n\tfrom yarrow.segmented.finite_function import AbstractIndexedCoproduct\n\tdef _is_valid(ops: 'Operations'):\n\t    \"\"\" Check if a tensoring of operations has correct types \"\"\"\n\t    N = ops.xn.source\n\t    return len(ops.s_type) == N and \\\n\t           len(ops.t_type) == N\n\t@dataclass\n\tclass Operations:\n", "    \"\"\" A flat array representation of a sequence of (typed) operations.\n\t    Since polymorphic operations have variable types, in order to get a\n\t    completely flat representation, we need to store them in *segmented arrays*.\n\t    The Operations type is therefore a 3-tuple:\n\t    Operation labels::\n\t      xn         : N            → Σ₁\n\t    Source types (encoded as an :py:class:`AbstractIndexedCoproduct`)::\n\t      s_type\n\t          sources: N            → None\n\t          values : sum(sources) → Σ₀\n", "    Target types (encoded as an :py:class:`AbstractIndexedCoproduct`)::\n\t      t_type\n\t          sources: N            → None\n\t          values : sum(sources) → Σ₀\n\t    \"\"\"\n\t    xn: AbstractFiniteFunction\n\t    s_type: AbstractIndexedCoproduct\n\t    t_type: AbstractIndexedCoproduct\n\t    def __post_init__(self):\n\t        assert _is_valid(self)\n", "        # check types of finite functions, segmented finite functions are equal\n\t        assert self.s_type._Fun == type(self.xn)\n\t        assert type(self.s_type) == type(self.t_type)\n\t    # return the number of operations\n\t    def __len__(self):\n\t        return len(self.xn)\n\t    def __iter__(self):\n\t        yield from zip(self.xn.table, self.s_type, self.t_type, strict=True)\n"]}
{"filename": "yarrow/segmented/__init__.py", "chunked_list": ["\"\"\" Segmented arrays with a categorical flavour\n\t.. warning::\n\t   This API has diverged from its description in :cite:t:`dpafsd`.\n\t.. autosummary::\n\t    :toctree: _autosummary\n\t    :recursive:\n\t    yarrow.segmented.finite_function\n\t    yarrow.segmented.operations\n\t\"\"\"\n"]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# For the full list of built-in configuration values, see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\t# -- Setup -----------------------------------------------------\n\t# Ensure the yarrow module is in the path so autosummary can load it.\n\timport sys\n\tfrom pathlib import Path\n\troot = Path(__file__).parent.parent.parent\n\tsys.path.insert(0, str(root))\n", "# Mock numpy, scipy, and cupy so sphinx can build docs without installing them\n\t# as dependencies.\n\t# See https://blog.rtwilson.com/how-to-make-your-sphinx-documentation-compile-with-readthedocs-when-youre-using-numpy-and-scipy/\n\tfrom unittest import mock\n\tMOCK_MODULES = ['numpy', 'scipy', 'scipy.sparse', 'cupy', 'cupyx', 'cupyx.scipy', 'cupyx.scipy.sparse']\n\tfor mod_name in MOCK_MODULES:\n\t    sys.modules[mod_name] = mock.MagicMock()\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\t# Project name etc.\n", "project = 'yarrow'\n\tcopyright = '2023, Paul Wilson'\n\tauthor = 'Paul Wilson'\n\t# -- General configuration ---------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\textensions = [\n\t    \"sphinx_rtd_theme\", # readthedocs theme\n\t    \"sphinx.ext.duration\",\n\t    \"sphinx.ext.doctest\",\n\t    \"sphinx.ext.autodoc\",\n", "    \"sphinx.ext.autosummary\",\n\t    \"sphinxcontrib.bibtex\",\n\t    \"sphinx.ext.napoleon\",\n\t]\n\t# Include both __init__ docstrings and class docstrings\n\t# autoclass_content = \"both\" # the non-napoleon version of this option\n\tnapoleon_include_init_with_doc = True\n\t# Number figures\n\tnumfig = True\n\t# class members should be in the order they are written in the file\n", "autodoc_member_order = \"bysource\"\n\t# references\n\tbibtex_bibfiles = [\"refs.bib\"]\n\t# let autosummary recurse and generate all modules specified\n\t# https://stackoverflow.com/questions/2701998/\n\tautosummary_generate = True\n\ttemplates_path = ['_templates']\n\texclude_patterns = []\n\t# -- Options for HTML output -------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n", "# html_theme = 'alabaster'\n\thtml_theme = 'sphinx_rtd_theme'\n\thtml_static_path = []\n\t# html_static_path = ['_static']\n"]}
