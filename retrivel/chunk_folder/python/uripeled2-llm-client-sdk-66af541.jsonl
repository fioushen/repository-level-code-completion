{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_utils/__init__.py", "chunked_list": []}
{"filename": "tests/test_utils/load_json_resource.py", "chunked_list": ["import json\n\timport os\n\tfrom pathlib import Path\n\tdef load_json_resource(path: str) -> dict:\n\t    project_base_path = Path(__file__).parent.parent\n\t    with open(os.path.join(f'{project_base_path}/resources/', path), \"r\") as f:\n\t        return json.load(f)\n"]}
{"filename": "tests/llm_client/__init__.py", "chunked_list": []}
{"filename": "tests/llm_client/local_client/__init__.py", "chunked_list": []}
{"filename": "tests/llm_client/local_client/test_local_client.py", "chunked_list": ["from unittest.mock import call\n\timport pytest\n\tfrom llm_client.llm_client.local_client import LocalClient, LocalClientConfig\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__sanity(local_client, mock_model, mock_tokenizer, tensors_type, device):\n\t    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n\t    mock_model.generate.return_value = [1]\n\t    mock_tokenizer.decode.return_value = \"first completion\"\n\t    actual = await local_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == [\"first completion\"]\n", "    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type)\n\t    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n\t    mock_model.generate.assert_called_once_with([1, 2, 3])\n\t    mock_tokenizer.decode.assert_called_once_with(1)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__return_multiple_completions(local_client, mock_model, mock_tokenizer, tensors_type,\n\t                                                            device):\n\t    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n\t    mock_model.generate.return_value = [2, 3, 4]\n\t    mock_tokenizer.decode.side_effect = [\"first completion\", \"second completion\", \"third completion\"]\n", "    actual = await local_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == [\"first completion\", \"second completion\", \"third completion\"]\n\t    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type)\n\t    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n\t    mock_model.generate.assert_called_once_with([1, 2, 3])\n\t    assert mock_tokenizer.decode.call_args_list == [call(2), call(3), call(4)]\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__with_kwargs(local_client, mock_model, mock_tokenizer, tensors_type, device):\n\t    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n\t    mock_model.generate.return_value = [1]\n", "    mock_tokenizer.decode.return_value = \"first completion\"\n\t    actual = await local_client.text_completion(prompt=\"These are a few of my favorite\", max_length=100)\n\t    assert actual == [\"first completion\"]\n\t    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type)\n\t    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n\t    mock_model.generate.assert_called_once_with([1, 2, 3], max_length=100)\n\t    mock_tokenizer.decode.assert_called_once_with(1)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__with_encode_kwargs(mock_model, mock_tokenizer, tensors_type, device):\n\t    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n", "    mock_model.generate.return_value = [1]\n\t    mock_tokenizer.decode.return_value = \"first completion\"\n\t    encode_kwargs = {\"add_special_tokens\": False}\n\t    local_client = LocalClient(LocalClientConfig(mock_model, mock_tokenizer, tensors_type, device, encode_kwargs))\n\t    actual = await local_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == [\"first completion\"]\n\t    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type,\n\t                                                  add_special_tokens=False)\n\t    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n\t    mock_model.generate.assert_called_once_with([1, 2, 3])\n", "    mock_tokenizer.decode.assert_called_once_with(1)\n\t@pytest.mark.asyncio\n\tasync def test_get_tokens_count__sanity(local_client, mock_tokenizer, tensors_type, device):\n\t    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n\t    actual = await local_client.get_tokens_count(text=\"This is a test\")\n\t    assert actual == 3\n\t    mock_tokenizer.encode.assert_called_once_with(\"This is a test\", return_tensors=tensors_type)\n\t    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n\t@pytest.mark.asyncio\n\tasync def test_get_tokens_count__with_kwargs(mock_model, mock_tokenizer, tensors_type, device):\n", "    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n\t    encode_kwargs = {\"add_special_tokens\": False}\n\t    local_client = LocalClient(LocalClientConfig(mock_model, mock_tokenizer, tensors_type, device, encode_kwargs))\n\t    actual = await local_client.get_tokens_count(text=\"This is a test\")\n\t    assert actual == 3\n\t    mock_tokenizer.encode.assert_called_once_with(\"This is a test\", return_tensors=tensors_type,\n\t                                                  add_special_tokens=False)\n\t    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n"]}
{"filename": "tests/llm_client/local_client/conftest.py", "chunked_list": ["from unittest.mock import MagicMock\n\timport pytest\n\tfrom transformers import PreTrainedTokenizerBase\n\tfrom llm_client import LocalClientConfig, LocalClient\n\t@pytest.fixture\n\tdef mock_model():\n\t    return MagicMock()\n\t@pytest.fixture\n\tdef mock_tokenizer():\n\t    return MagicMock(PreTrainedTokenizerBase)\n", "@pytest.fixture\n\tdef tensors_type():\n\t    return \"pt\"\n\t@pytest.fixture\n\tdef device():\n\t    return \"cpu\"\n\t@pytest.fixture\n\tdef local_client(mock_model, mock_tokenizer, tensors_type, device):\n\t    return LocalClient(LocalClientConfig(mock_model, mock_tokenizer, tensors_type, device))\n"]}
{"filename": "tests/llm_api_client/__init__.py", "chunked_list": []}
{"filename": "tests/llm_api_client/test_llm_api_client_factory.py", "chunked_list": ["from unittest.mock import patch\n\timport pytest\n\tfrom llm_client import LLMAPIClientType, LLMAPIClientFactory, LLMAPIClientConfig\n\tfrom llm_client.llm_api_client.llm_api_client_factory import get_llm_api_client_class\n\tdef test_get_llm_api_client__without_context_manager():\n\t    llm_api_client_factory = LLMAPIClientFactory()\n\t    with pytest.raises(ValueError):\n\t        llm_api_client_factory.get_llm_api_client(LLMAPIClientType.OPEN_AI, api_key=\"super secret key\")\n\t@pytest.mark.asyncio\n\t@pytest.mark.parametrize(\"client_type,client_patch\",\n", "                         [(LLMAPIClientType.OPEN_AI, \"OpenAIClient\"), (LLMAPIClientType.AI21, \"AI21Client\"),\n\t                          (LLMAPIClientType.HUGGING_FACE, \"HuggingFaceClient\"),\n\t                          (LLMAPIClientType.ALEPH_ALPHA, \"AlephAlphaClient\"),\n\t                          (LLMAPIClientType.ANTHROPIC, \"AnthropicClient\"),\n\t                          (LLMAPIClientType.GOOGLE, \"GoogleClient\")])\n\tasync def test_get_llm_api_client__with_client_type(client_type, client_patch):\n\t    assert len(LLMAPIClientType) == 6\n\t    llm_api_client_factory = LLMAPIClientFactory()\n\t    async with llm_api_client_factory:\n\t        with patch(f\"llm_client.{client_patch}\") as mock_client:\n", "            actual = llm_api_client_factory.get_llm_api_client(client_type, api_key=\"super secret key\")\n\t            assert actual is mock_client.return_value\n\t            mock_client.assert_called_once_with(LLMAPIClientConfig(session=llm_api_client_factory._session,\n\t                                                                   api_key=\"super secret key\"))\n\t@pytest.mark.asyncio\n\tasync def test_get_llm_api_client__with_unknown_client_type():\n\t    llm_api_client_factory = LLMAPIClientFactory()\n\t    async with llm_api_client_factory:\n\t        with pytest.raises(ValueError):\n\t            llm_api_client_factory.get_llm_api_client(\"unknown-client-type\", api_key=\"super secret key\")\n", "@pytest.mark.parametrize(\"client_type,client_patch\",\n\t                         [(LLMAPIClientType.OPEN_AI, \"OpenAIClient\"), (LLMAPIClientType.AI21, \"AI21Client\"),\n\t                          (LLMAPIClientType.HUGGING_FACE, \"HuggingFaceClient\"),\n\t                          (LLMAPIClientType.ALEPH_ALPHA, \"AlephAlphaClient\"),\n\t                          (LLMAPIClientType.ANTHROPIC, \"AnthropicClient\"),\n\t                          (LLMAPIClientType.GOOGLE, \"GoogleClient\")])\n\tdef test_get_llm_api_client_class(client_type, client_patch):\n\t    assert len(LLMAPIClientType) == 6\n\t    with patch(f\"llm_client.{client_patch}\") as mock_client:\n\t        actual = get_llm_api_client_class(client_type)\n", "        assert actual is mock_client\n\t        mock_client.assert_not_called()\n\tdef test_get_llm_api_client_class__with_unknown_client_type():\n\t    with pytest.raises(ValueError):\n\t        get_llm_api_client_class(\"unknown-client-type\")\n"]}
{"filename": "tests/llm_api_client/conftest.py", "chunked_list": ["import pytest\n\timport pytest_asyncio\n\tfrom aiohttp import ClientSession\n\tfrom aioresponses import aioresponses\n\t@pytest.fixture\n\tdef mock_aioresponse():\n\t    with aioresponses() as m:\n\t        yield m\n\t@pytest_asyncio.fixture\n\tasync def client_session():\n", "    session = ClientSession()\n\t    yield session\n\t    await session.close()\n"]}
{"filename": "tests/llm_api_client/google_client/__init__.py", "chunked_list": []}
{"filename": "tests/llm_api_client/google_client/test_google_client.py", "chunked_list": ["import pytest\n\tfrom llm_client import LLMAPIClientType, LLMAPIClientFactory\n\tfrom llm_client.consts import PROMPT_KEY\n\tfrom llm_client.llm_api_client.google_client import TEXT_KEY, GoogleClient, COMPLETE_PATH, AUTH_PARAM, CHAT_PATH, \\\n\t    EMBEDDING_PATH, TOKENIZE_PATH, MESSAGES_KEY, MESSAGE_CONTENT_KEY, MAX_TOKENS_KEY\n\tfrom tests.llm_api_client.google_client.conftest import build_url\n\tfrom tests.test_utils.load_json_resource import load_json_resource\n\t@pytest.mark.asyncio\n\tasync def test_get_llm_api_client__with_google_client(config):\n\t    del config.session\n", "    async with LLMAPIClientFactory() as llm_api_client_factory:\n\t        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.GOOGLE, **config.__dict__)\n\t    assert isinstance(actual, GoogleClient)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__sanity(mock_aioresponse, llm_client, params):\n\t    url = build_url(llm_client, COMPLETE_PATH)\n\t    mock_aioresponse.post(\n\t        url + params,\n\t        payload=load_json_resource(\"google/text_completion.json\")\n\t    )\n", "    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == ['Once upon a time, there was a young girl named Lily...',\n\t                      'Once upon a time, there was a young boy named Billy...']\n\t    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n\t                                             json={PROMPT_KEY: {TEXT_KEY: 'These are a few of my favorite'},\n\t                                                   MAX_TOKENS_KEY: 64,\n\t                                                   'temperature': None},\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n", "@pytest.mark.asyncio\n\tasync def test_text_completion__override_model(mock_aioresponse, llm_client, params):\n\t    new_model_name = \"text-bison-002\"\n\t    url = build_url(llm_client, COMPLETE_PATH, new_model_name)\n\t    mock_aioresponse.post(\n\t        url + params,\n\t        payload=load_json_resource(\"google/text_completion.json\")\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name)\n\t    assert actual == ['Once upon a time, there was a young girl named Lily...',\n", "                      'Once upon a time, there was a young boy named Billy...']\n\t    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n\t                                             json={PROMPT_KEY: {TEXT_KEY: 'These are a few of my favorite'},\n\t                                                   MAX_TOKENS_KEY: 64,\n\t                                                   'temperature': None},\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__with_kwargs(mock_aioresponse, llm_client, params):\n", "    url = build_url(llm_client, COMPLETE_PATH)\n\t    mock_aioresponse.post(\n\t        url + params,\n\t        payload=load_json_resource(\"google/text_completion.json\")\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10, blabla=\"aaa\", top_p= 0.95)\n\t    assert actual == ['Once upon a time, there was a young girl named Lily...',\n\t                      'Once upon a time, there was a young boy named Billy...']\n\t    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n\t                                             json={PROMPT_KEY: {TEXT_KEY: 'These are a few of my favorite'},\n", "                                                   MAX_TOKENS_KEY: 10,\n\t                                                   'temperature': None,\n\t                                                   'blabla': 'aaa',\"topP\" : 0.95},\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n\t@pytest.mark.asyncio\n\tasync def test_embedding__sanity(mock_aioresponse, llm_client, params):\n\t    url = build_url(llm_client, EMBEDDING_PATH)\n\t    mock_aioresponse.post(\n", "        url + params,\n\t        payload=load_json_resource(\"google/embedding.json\")\n\t    )\n\t    actual = await llm_client.embedding(text=\"These are a few of my favorite\")\n\t    assert actual == [0.0011238843, -0.040586308, -0.013174802, 0.015497498]\n\t    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n\t                                             json={TEXT_KEY: 'These are a few of my favorite'},\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n", "@pytest.mark.asyncio\n\tasync def test_embedding__override_model(mock_aioresponse, llm_client, params):\n\t    new_model_name = \"text-bison-002\"\n\t    url = build_url(llm_client, EMBEDDING_PATH, new_model_name)\n\t    mock_aioresponse.post(\n\t        url + params,\n\t        payload=load_json_resource(\"google/embedding.json\")\n\t    )\n\t    actual = await llm_client.embedding(text=\"These are a few of my favorite\", model=new_model_name)\n\t    assert actual == [0.0011238843, -0.040586308, -0.013174802, 0.015497498]\n", "    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n\t                                             json={TEXT_KEY: 'These are a few of my favorite'},\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n\t@pytest.mark.asyncio\n\tasync def test_embedding__with_kwargs_not_pass_through(mock_aioresponse, llm_client, params):\n\t    url = build_url(llm_client, EMBEDDING_PATH)\n\t    mock_aioresponse.post(\n\t        url + params,\n", "        payload=load_json_resource(\"google/embedding.json\")\n\t    )\n\t    actual = await llm_client.embedding(text=\"These are a few of my favorite\", max_tokens=10)\n\t    assert actual == [0.0011238843, -0.040586308, -0.013174802, 0.015497498]\n\t    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n\t                                             json={TEXT_KEY: 'These are a few of my favorite'},\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n\t@pytest.mark.asyncio\n", "async def test_get_tokens_count__sanity(mock_aioresponse, llm_client, params):\n\t    url = build_url(llm_client, TOKENIZE_PATH)\n\t    mock_aioresponse.post(\n\t        url + params,\n\t        payload=load_json_resource(\"google/tokens_count.json\")\n\t    )\n\t    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite\")\n\t    assert actual == 23\n\t    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n\t                                             json={\n", "                                                 PROMPT_KEY: {\n\t                                                     MESSAGES_KEY: [\n\t                                                         {MESSAGE_CONTENT_KEY: \"These are a few of my favorite\"},\n\t                                                     ]\n\t                                                 }\n\t                                             },\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n\t@pytest.mark.asyncio\n", "async def test_get_tokens_count__override_model(mock_aioresponse, llm_client, params):\n\t    new_model_name = \"text-bison-002\"\n\t    url = build_url(llm_client, TOKENIZE_PATH, new_model_name)\n\t    mock_aioresponse.post(\n\t        url + params,\n\t        payload=load_json_resource(\"google/tokens_count.json\")\n\t    )\n\t    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite\", model=new_model_name)\n\t    assert actual == 23\n\t    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n", "                                             json={\n\t                                                 PROMPT_KEY: {\n\t                                                     MESSAGES_KEY: [\n\t                                                         {MESSAGE_CONTENT_KEY: \"These are a few of my favorite\"},\n\t                                                     ]\n\t                                                 }\n\t                                             },\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n", "@pytest.mark.asyncio\n\tasync def test_get_tokens_count__kwargs_not_pass_through(mock_aioresponse, llm_client, params):\n\t    url = build_url(llm_client, TOKENIZE_PATH)\n\t    mock_aioresponse.post(\n\t        url + params,\n\t        payload=load_json_resource(\"google/tokens_count.json\")\n\t    )\n\t    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite\", max_tokens=10)\n\t    assert actual == 23\n\t    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n", "                                             json={\n\t                                                 PROMPT_KEY: {\n\t                                                     MESSAGES_KEY: [\n\t                                                         {MESSAGE_CONTENT_KEY: \"These are a few of my favorite\"},\n\t                                                     ]\n\t                                                 }\n\t                                             },\n\t                                             headers=llm_client._headers,\n\t                                             raise_for_status=True,\n\t                                             )\n"]}
{"filename": "tests/llm_api_client/google_client/conftest.py", "chunked_list": ["from typing import Optional\n\timport pytest\n\tfrom llm_client.llm_api_client.google_client import GoogleClient, BASE_URL, AUTH_PARAM\n\tfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig, BaseLLMAPIClient\n\t@pytest.fixture\n\tdef model_name():\n\t    return \"text-bison-001\"\n\t@pytest.fixture\n\tdef config(client_session, model_name):\n\t    return LLMAPIClientConfig(\"top-secret-api-key\", client_session, default_model=model_name)\n", "@pytest.fixture\n\tdef llm_client(config):\n\t    return GoogleClient(config)\n\t@pytest.fixture\n\tdef params(llm_client):\n\t    return \"?\" + AUTH_PARAM + \"=\" + llm_client._api_key\n\tdef build_url(llm_client: BaseLLMAPIClient, path: str, model: Optional[str] = None) -> str:\n\t    model = model or llm_client._default_model\n\t    return BASE_URL + model + \":\" + path\n"]}
{"filename": "tests/llm_api_client/huggingface_client/__init__.py", "chunked_list": []}
{"filename": "tests/llm_api_client/huggingface_client/test_huggingface.py", "chunked_list": ["import pytest\n\tfrom llm_client import LLMAPIClientType, LLMAPIClientFactory\n\tfrom llm_client.llm_api_client.huggingface_client import AUTH_HEADER, \\\n\t    BEARER_TOKEN, HuggingFaceClient\n\tfrom tests.test_utils.load_json_resource import load_json_resource\n\t@pytest.mark.asyncio\n\tasync def test_get_llm_api_client__with_hugging_face(config):\n\t    del config.session\n\t    async with LLMAPIClientFactory() as llm_api_client_factory:\n\t        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.HUGGING_FACE, **config.__dict__)\n", "    assert isinstance(actual, HuggingFaceClient)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__sanity(mock_aioresponse, llm_client, url):\n\t    mock_aioresponse.post(\n\t        url,\n\t        payload=load_json_resource(\"huggingface/text_completion.json\")\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"who is kobe bryant\")\n\t    assert actual == ['Kobe Bryant is a retired professional basketball player who played for the Los Angeles Lakers of']\n\t    mock_aioresponse.assert_called_once_with(url, method='POST',\n", "                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n\t                                             json={'inputs': 'who is kobe bryant',\"max_length\": None, \"temperature\": 1.0, \"top_p\" : None},\n\t                                             raise_for_status=True)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__with_kwargs(mock_aioresponse, llm_client, url):\n\t    mock_aioresponse.post(\n\t        url,\n\t        payload=load_json_resource(\"huggingface/text_completion.json\")\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"who is kobe bryant\",max_tokens = 10)\n", "    assert actual == ['Kobe Bryant is a retired professional basketball player who played for the Los Angeles Lakers of']\n\t    mock_aioresponse.assert_called_once_with(url, method='POST',\n\t                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n\t                                             json={'inputs': 'who is kobe bryant',\"max_length\": 10, \"temperature\": 1.0, \"top_p\" : None},\n\t                                             raise_for_status=True)\n\t@pytest.mark.asyncio\n\tdef test_get_tokens_count__sanity(mock_aioresponse, llm_client, url):\n\t    actual = llm_client.get_tokens_count(text=\"is queen elisabeth alive?\")\n\t    assert actual == 7\n"]}
{"filename": "tests/llm_api_client/huggingface_client/conftest.py", "chunked_list": ["import pytest\n\tfrom llm_client.llm_api_client.huggingface_client import HuggingFaceClient, BASE_URL\n\tfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig\n\t@pytest.fixture\n\tdef model_name():\n\t    return \"oasst-sft-4-pythia-12b-epoch-3.5\"\n\t@pytest.fixture\n\tdef config(client_session, model_name):\n\t    return LLMAPIClientConfig(\"top-secret-api-key\", client_session, default_model=model_name)\n\t@pytest.fixture\n", "def llm_client(config):\n\t    return HuggingFaceClient(config)\n\t@pytest.fixture\n\tdef url(model_name):\n\t    return build_url(model_name)\n\tdef build_url(model: str) -> str:\n\t    return BASE_URL + model + \"/\"\n"]}
{"filename": "tests/llm_api_client/ai21_client/test_ai21.py", "chunked_list": ["import pytest\n\tfrom llm_client import LLMAPIClientType, LLMAPIClientFactory\n\tfrom llm_client.llm_api_client.ai21_client import BASE_URL, DATA_KEY, TEXT_KEY, TOKENIZE_PATH, AUTH_HEADER, \\\n\t    BEARER_TOKEN, AI21Client\n\tfrom tests.llm_api_client.ai21_client.conftest import build_url\n\tfrom tests.test_utils.load_json_resource import load_json_resource\n\t@pytest.mark.asyncio\n\tasync def test_get_llm_api_client__with_ai21(config):\n\t    del config.session\n\t    async with LLMAPIClientFactory() as llm_api_client_factory:\n", "        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.AI21, **config.__dict__)\n\t    assert isinstance(actual, AI21Client)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__sanity(mock_aioresponse, llm_client, url):\n\t    mock_aioresponse.post(\n\t        url,\n\t        payload=load_json_resource(\"ai21/text_completion.json\")\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == [\n", "        ' things!\\n\\nI love entertaining, entertaining and decorating my home, entertaining clients, entertaining '\n\t        'friends, entertaining family...you get the point! One of my favorite things to do is plan parties']\n\t    mock_aioresponse.assert_called_once_with(url, method='POST',\n\t                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key },\n\t                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 16, \"temperature\" : 0.7, \"topP\" : 1 },\n\t                                             raise_for_status=True)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__return_multiple_completions(mock_aioresponse, llm_client, url):\n\t    payload = load_json_resource(\"ai21/text_completion.json\")\n\t    payload[\"completions\"].append({DATA_KEY: {TEXT_KEY: \"second completion\"}})\n", "    mock_aioresponse.post(url, payload=payload)\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == [\n\t        ' things!\\n\\nI love entertaining, entertaining and decorating my home, entertaining clients, entertaining '\n\t        'friends, entertaining family...you get the point! One of my favorite things to do is plan parties',\n\t        \"second completion\"\n\t    ]\n\t    mock_aioresponse.assert_called_once_with(url, method='POST',\n\t                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n\t                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 16, \"temperature\" : 0.7, \"topP\" : 1  },\n", "                                             raise_for_status=True)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__override_model(mock_aioresponse, llm_client):\n\t    new_model_name = \"gpt3\"\n\t    url = build_url(new_model_name)\n\t    mock_aioresponse.post(\n\t        url,\n\t        payload=load_json_resource(\"ai21/text_completion.json\")\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name)\n", "    assert actual == [\n\t        ' things!\\n\\nI love entertaining, entertaining and decorating my home, entertaining clients, entertaining '\n\t        'friends, entertaining family...you get the point! One of my favorite things to do is plan parties']\n\t    mock_aioresponse.assert_called_once_with(url, method='POST',\n\t                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n\t                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 16, \"temperature\" : 0.7, \"topP\" : 1 },\n\t                                             raise_for_status=True)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__with_kwargs(mock_aioresponse, llm_client, url):\n\t    mock_aioresponse.post(\n", "        url,\n\t        payload=load_json_resource(\"ai21/text_completion.json\")\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10)\n\t    assert actual == [\n\t        ' things!\\n\\nI love entertaining, entertaining and decorating my home, entertaining clients, entertaining '\n\t        'friends, entertaining family...you get the point! One of my favorite things to do is plan parties']\n\t    mock_aioresponse.assert_called_once_with(url, method='POST',\n\t                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n\t                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 10, \"temperature\" : 0.7 ,\"topP\" : 1},\n", "                                             raise_for_status=True)\n\t@pytest.mark.asyncio\n\tasync def test_get_tokens_count__sanity(mock_aioresponse, llm_client, url):\n\t    mock_aioresponse.post(\n\t        BASE_URL + TOKENIZE_PATH,\n\t        payload=load_json_resource(\"ai21/tokenize.json\")\n\t    )\n\t    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite things!\")\n\t    assert actual == 3\n"]}
{"filename": "tests/llm_api_client/ai21_client/__init__.py", "chunked_list": []}
{"filename": "tests/llm_api_client/ai21_client/conftest.py", "chunked_list": ["import pytest\n\tfrom llm_client.llm_api_client.ai21_client import AI21Client, COMPLETE_PATH, BASE_URL\n\tfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig\n\t@pytest.fixture\n\tdef model_name():\n\t    return \"ada\"\n\t@pytest.fixture\n\tdef config(client_session, model_name):\n\t    return LLMAPIClientConfig(\"top-secret-api-key\", client_session, default_model=model_name)\n\t@pytest.fixture\n", "def llm_client(config):\n\t    return AI21Client(config)\n\t@pytest.fixture\n\tdef url(model_name):\n\t    return build_url(model_name)\n\tdef build_url(model: str) -> str:\n\t    return BASE_URL + model + \"/\" + COMPLETE_PATH\n"]}
{"filename": "tests/llm_api_client/openai_client/test_openai.py", "chunked_list": ["from unittest.mock import AsyncMock, MagicMock\n\timport pytest\n\tfrom aiohttp import ClientSession\n\tfrom openai.openai_object import OpenAIObject\n\tfrom llm_client import OpenAIClient, LLMAPIClientType, LLMAPIClientFactory\n\tfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig, Role\n\tfrom llm_client.llm_api_client.openai_client import ChatMessage\n\tfrom tests.test_utils.load_json_resource import load_json_resource\n\t@pytest.mark.asyncio\n\tasync def test_get_llm_api_client__with_open_ai(config):\n", "    del config.session\n\t    async with LLMAPIClientFactory() as llm_api_client_factory:\n\t        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.OPEN_AI, **config.__dict__)\n\t    assert isinstance(actual, OpenAIClient)\n\tdef test_init__sanity(openai_mock, client_session):\n\t    OpenAIClient(LLMAPIClientConfig(\"fake_api_key\", client_session))\n\t    assert openai_mock.api_key == \"fake_api_key\"\n\t    openai_mock.aiosession.set.assert_called_once()\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__sanity(openai_mock, open_ai_client, model_name):\n", "    openai_mock.Completion.acreate = AsyncMock(\n\t        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n\t    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == [\"\\n\\nThis is indeed a test\"]\n\t    openai_mock.Completion.acreate.assert_awaited_once_with(\n\t        model=model_name,\n\t        prompt=\"These are a few of my favorite\",\n\t        headers={}, temperature=0, max_tokens=16, top_p=1)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__return_multiple_completions(openai_mock, open_ai_client, model_name):\n", "    open_ai_object = OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\"))\n\t    open_ai_object.choices.append(OpenAIObject.construct_from({\"text\": \"second completion\"}))\n\t    openai_mock.Completion.acreate = AsyncMock(return_value=open_ai_object)\n\t    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == [\"\\n\\nThis is indeed a test\", \"second completion\"]\n\t    openai_mock.Completion.acreate.assert_awaited_once_with(\n\t        model=model_name,\n\t        prompt=\"These are a few of my favorite\",\n\t        headers={}, temperature=0, max_tokens=16, top_p=1)\n\t@pytest.mark.asyncio\n", "async def test_text_completion__override_model(openai_mock, open_ai_client, model_name):\n\t    new_model_name = \"gpt3\"\n\t    openai_mock.Completion.acreate = AsyncMock(\n\t        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n\t    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name)\n\t    assert actual == [\"\\n\\nThis is indeed a test\"]\n\t    openai_mock.Completion.acreate.assert_awaited_once_with(\n\t        model=new_model_name,\n\t        prompt=\"These are a few of my favorite\",\n\t        headers={}, temperature=0, max_tokens=16, top_p=1)\n", "@pytest.mark.asyncio\n\tasync def test_text_completion__with_kwargs(openai_mock, open_ai_client, model_name):\n\t    openai_mock.Completion.acreate = AsyncMock(\n\t        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n\t    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10)\n\t    assert actual == [\"\\n\\nThis is indeed a test\"]\n\t    openai_mock.Completion.acreate.assert_awaited_once_with(\n\t        model=model_name,\n\t        prompt=\"These are a few of my favorite\",\n\t        temperature=0, max_tokens=10, top_p=1,\n", "        headers={})\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__with_headers(openai_mock, model_name):\n\t    openai_mock.Completion.acreate = AsyncMock(\n\t        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n\t    open_ai_client = OpenAIClient(LLMAPIClientConfig(\"fake_api_key\", MagicMock(ClientSession), default_model=model_name,\n\t                                                     headers={\"header_name\": \"header_value\"}))\n\t    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\")\n\t    assert actual == [\"\\n\\nThis is indeed a test\"]\n\t    openai_mock.Completion.acreate.assert_awaited_once_with(\n", "        model=model_name,\n\t        prompt=\"These are a few of my favorite\",\n\t        headers={\"header_name\": \"header_value\"}, temperature=0, max_tokens=16, top_p=1)\n\t@pytest.mark.asyncio\n\tasync def test_chat_completion__sanity(openai_mock, open_ai_client, model_name):\n\t    openai_mock.ChatCompletion.acreate = AsyncMock(\n\t        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\")))\n\t    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")])\n\t    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n\t    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n", "        model=model_name,\n\t        messages=[{'content': 'Hello!', 'role': 'user'}],\n\t        headers={}, temperature=0, max_tokens=16, top_p=1)\n\t@pytest.mark.asyncio\n\tasync def test_chat_completion__return_multiple_completions(openai_mock, open_ai_client, model_name):\n\t    open_ai_object = OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\"))\n\t    open_ai_object.choices.append(OpenAIObject.construct_from({\"message\": {\"content\": \"second completion\"}}))\n\t    openai_mock.ChatCompletion.acreate = AsyncMock(return_value=open_ai_object)\n\t    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")])\n\t    assert actual == [\"\\n\\nHello there, how may I assist you today?\", \"second completion\"]\n", "    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n\t        model=model_name,\n\t        messages=[{'content': 'Hello!', 'role': 'user'}],\n\t        headers={}, temperature=0, max_tokens=16, top_p=1)\n\t@pytest.mark.asyncio\n\tasync def test_chat_completion__override_model(openai_mock, open_ai_client, model_name):\n\t    new_model_name = \"gpt3\"\n\t    openai_mock.ChatCompletion.acreate = AsyncMock(\n\t        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\")))\n\t    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")], model=new_model_name)\n", "    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n\t    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n\t        model=new_model_name,\n\t        messages=[{'content': 'Hello!', 'role': 'user'}],\n\t        headers={}, temperature=0, max_tokens=16, top_p=1)\n\t@pytest.mark.asyncio\n\tasync def test_chat_completion__with_kwargs(openai_mock, open_ai_client, model_name):\n\t    openai_mock.ChatCompletion.acreate = AsyncMock(\n\t        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\")))\n\t    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")], max_tokens=10, top_p=1)\n", "    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n\t    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n\t        model=model_name,\n\t        messages=[{'content': 'Hello!', 'role': 'user'}],\n\t        max_tokens=10,\n\t        headers={}, temperature=0, top_p=1)\n\t@pytest.mark.asyncio\n\tasync def test_chat_completion__with_headers(openai_mock, model_name):\n\t    openai_mock.ChatCompletion.acreate = AsyncMock(\n\t        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\")))\n", "    open_ai_client = OpenAIClient(LLMAPIClientConfig(\"fake_api_key\", MagicMock(ClientSession), default_model=model_name,\n\t                                                     headers={\"header_name\": \"header_value\"}))\n\t    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")])\n\t    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n\t    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n\t        model=model_name,\n\t        messages=[{'content': 'Hello!', 'role': 'user'}],\n\t        headers={\"header_name\": \"header_value\"}, temperature=0, max_tokens=16, top_p=1)\n\t@pytest.mark.asyncio\n\t@pytest.mark.parametrize(\"model_name,expected\", [(\"gpt-3.5-turbo-0301\", 127), (\"gpt-3.5-turbo-0613\", 129),\n", "                                                 (\"gpt-3.5-turbo\", 129), (\"gpt-4-0314\", 129), (\"gpt-4-0613\", 129),\n\t                                                 (\"gpt-4\", 129)])\n\tasync def test_get_chat_tokens_count__with_examples_from_openai_cookbook(model_name, expected, open_ai_client):\n\t    example_messages = [\n\t        ChatMessage(Role.SYSTEM,\n\t                    \"You are a helpful, pattern-following assistant that translates corporate jargon \"\n\t                    \"into plain English.\"),\n\t        ChatMessage(Role.SYSTEM, \"New synergies will help drive top-line growth.\", name=\"example_user\"),\n\t        ChatMessage(Role.SYSTEM, \"Things working well together will increase revenue.\", name=\"example_assistant\"),\n\t        ChatMessage(Role.SYSTEM,\n", "                    \"Let's circle back when we have more bandwidth to touch base on opportunities \"\n\t                    \"for increased leverage.\", name=\"example_user\"),\n\t        ChatMessage(Role.SYSTEM, \"Let's talk later when we're less busy about how to do better.\",\n\t                    name=\"example_assistant\"),\n\t        ChatMessage(Role.USER,\n\t                    \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"),\n\t    ]\n\t    actual = await open_ai_client.get_chat_tokens_count(example_messages, model=model_name)\n\t    assert actual == expected\n\t@pytest.mark.asyncio\n", "async def test_get_tokens_count__sanity(model_name, open_ai_client, tiktoken_mock):\n\t    tokeniser_mock = tiktoken_mock.encoding_for_model.return_value\n\t    tokeniser_mock.encode.return_value = [123, 456]\n\t    text = \"This is a test\"\n\t    actual = await open_ai_client.get_tokens_count(text=text)\n\t    assert actual == len(tokeniser_mock.encode.return_value)\n\t    tiktoken_mock.encoding_for_model.assert_called_once_with(model_name)\n\t    tokeniser_mock.encode.assert_called_once_with(text)\n\t@pytest.mark.asyncio\n\tasync def test_get_tokens_count__override_model(open_ai_client, tiktoken_mock):\n", "    tokeniser_mock = tiktoken_mock.encoding_for_model.return_value\n\t    tokeniser_mock.encode.return_value = [123, 456]\n\t    text = \"This is a test\"\n\t    model_name = \"gpt3\"\n\t    actual = await open_ai_client.get_tokens_count(text=text, model=model_name)\n\t    assert actual == len(tokeniser_mock.encode.return_value)\n\t    tiktoken_mock.encoding_for_model.assert_called_once_with(model_name)\n\t    tokeniser_mock.encode.assert_called_once_with(text)\n"]}
{"filename": "tests/llm_api_client/openai_client/__init__.py", "chunked_list": []}
{"filename": "tests/llm_api_client/openai_client/conftest.py", "chunked_list": ["from unittest.mock import MagicMock, patch\n\timport pytest\n\tfrom aiohttp import ClientSession\n\tfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig\n\tfrom llm_client.llm_api_client.openai_client import OpenAIClient\n\t@pytest.fixture\n\tdef model_name():\n\t    return \"ada\"\n\t@pytest.fixture\n\tdef openai_mock():\n", "    with patch(\"llm_client.llm_api_client.openai_client.openai\") as openai_mock:\n\t        yield openai_mock\n\t@pytest.fixture\n\tdef config(model_name):\n\t    return LLMAPIClientConfig(\"fake-api-key\", MagicMock(ClientSession), default_model=model_name)\n\t@pytest.fixture\n\tdef open_ai_client(config):\n\t    return OpenAIClient(config)\n\t@pytest.fixture\n\tdef tiktoken_mock():\n", "    with patch(\"llm_client.llm_api_client.openai_client.tiktoken\") as tiktoken_mock:\n\t        yield tiktoken_mock\n"]}
{"filename": "tests/llm_api_client/anthropic_client/__init__.py", "chunked_list": []}
{"filename": "tests/llm_api_client/anthropic_client/test_anthropic_client.py", "chunked_list": ["from unittest.mock import AsyncMock\n\timport pytest\n\tfrom llm_client import LLMAPIClientFactory, LLMAPIClientType, ChatMessage\n\tfrom llm_client.consts import PROMPT_KEY, MODEL_KEY\n\tfrom llm_client.llm_api_client.anthropic_client import AUTH_HEADER, COMPLETIONS_KEY, MAX_TOKENS_KEY, ACCEPT_HEADER, \\\n\t    ACCEPT_VALUE, VERSION_HEADER, AnthropicClient, USER_PREFIX, ASSISTANT_PREFIX, START_PREFIX, SYSTEM_START_PREFIX, \\\n\t    SYSTEM_END_PREFIX\n\tfrom llm_client.llm_api_client.base_llm_api_client import Role\n\t@pytest.mark.asyncio\n\tasync def test_get_llm_api_client__with_anthropic(config):\n", "    del config.session\n\t    async with LLMAPIClientFactory() as llm_api_client_factory:\n\t        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.ANTHROPIC, **config.__dict__)\n\t    assert isinstance(actual, AnthropicClient)\n\t@pytest.mark.asyncio\n\tasync def test_chat_completion_sanity(llm_client):\n\t    text_completion_mock = AsyncMock(return_value=[\"completion text\"])\n\t    llm_client.text_completion = text_completion_mock\n\t    actual = await llm_client.chat_completion(messages=[ChatMessage(Role.USER, \"Why is the sky blue?\")], max_tokens=10)\n\t    assert actual == [\"completion text\"]\n", "    text_completion_mock.assert_awaited_once_with(f\"{START_PREFIX}{USER_PREFIX} Why is the sky blue?\"\n\t                                                  f\"{START_PREFIX}{ASSISTANT_PREFIX}\", None, 10, 1)\n\t@pytest.mark.asyncio\n\tasync def test_chat_completion_with_assistant_in_the_end(llm_client):\n\t    text_completion_mock = AsyncMock(return_value=[\"completion text\"])\n\t    llm_client.text_completion = text_completion_mock\n\t    actual = await llm_client.chat_completion(messages=[ChatMessage(Role.USER, \"Why is the sky blue?\"),\n\t                                                        ChatMessage(Role.ASSISTANT, \"Answer - \")], temperature=10)\n\t    assert actual == [\"completion text\"]\n\t    text_completion_mock.assert_awaited_once_with(f\"{START_PREFIX}{USER_PREFIX} Why is the sky blue?\"\n", "                                                  f\"{START_PREFIX}{ASSISTANT_PREFIX} Answer -\", None, None,\n\t                                                  10)\n\t@pytest.mark.asyncio\n\tasync def test_chat_completion_with_system(llm_client):\n\t    text_completion_mock = AsyncMock(return_value=[\"completion text\"])\n\t    llm_client.text_completion = text_completion_mock\n\t    actual = await llm_client.chat_completion(messages=[ChatMessage(Role.SYSTEM, \"Be nice!\"),\n\t                                                        ChatMessage(Role.USER, \"Why is the sky blue?\")], max_tokens=10,\n\t                                              temperature=2)\n\t    assert actual == [\"completion text\"]\n", "    text_completion_mock.assert_awaited_once_with(f\"{START_PREFIX}{USER_PREFIX} \"\n\t                                                  f\"{SYSTEM_START_PREFIX}Be nice!{SYSTEM_END_PREFIX}{START_PREFIX}\"\n\t                                                  f\"{USER_PREFIX} Why is the sky blue?\"\n\t                                                  f\"{START_PREFIX}{ASSISTANT_PREFIX}\", None, 10, 2)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__sanity(mock_aioresponse, llm_client, complete_url, anthropic_version):\n\t    mock_aioresponse.post(\n\t        complete_url,\n\t        payload={COMPLETIONS_KEY: \"completion text\"}\n\t    )\n", "    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10,)\n\t    assert actual == [\"completion text\"]\n\t    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n\t                                             headers={AUTH_HEADER: llm_client._api_key,\n\t                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n\t                                                      VERSION_HEADER: anthropic_version},\n\t                                             json={PROMPT_KEY: 'These are a few of my favorite',\n\t                                                   MAX_TOKENS_KEY: 10, \"temperature\": 1,\n\t                                                   MODEL_KEY: llm_client._default_model},\n\t                                             raise_for_status=True)\n", "@pytest.mark.asyncio\n\tasync def test_text_completion__with_version_header(mock_aioresponse, config, complete_url):\n\t    mock_aioresponse.post(\n\t        complete_url,\n\t        payload={COMPLETIONS_KEY: \"completion text\"}\n\t    )\n\t    config.headers[VERSION_HEADER] = \"1.0.0\"\n\t    llm_client = AnthropicClient(config)\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10)\n\t    assert actual == [\"completion text\"]\n", "    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n\t                                             headers={AUTH_HEADER: llm_client._api_key,\n\t                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n\t                                                      VERSION_HEADER: \"1.0.0\"},\n\t                                             json={PROMPT_KEY: 'These are a few of my favorite',\n\t                                                   MAX_TOKENS_KEY: 10, \"temperature\": 1,\n\t                                                   MODEL_KEY: llm_client._default_model},\n\t                                             raise_for_status=True)\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__without_max_tokens_raise_value_error(mock_aioresponse, llm_client):\n", "    with pytest.raises(ValueError):\n\t        await llm_client.text_completion(prompt=\"These are a few of my favorite\")\n\t@pytest.mark.asyncio\n\tasync def test_text_completion__override_model(mock_aioresponse, llm_client, complete_url, anthropic_version):\n\t    new_model_name = \"claude-instant\"\n\t    mock_aioresponse.post(\n\t        complete_url,\n\t        payload={COMPLETIONS_KEY: \"completion text\"}\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name,\n", "                                              max_tokens=10)\n\t    assert actual == [\"completion text\"]\n\t    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n\t                                             headers={AUTH_HEADER: llm_client._api_key,\n\t                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n\t                                                      VERSION_HEADER: anthropic_version},\n\t                                             json={PROMPT_KEY: 'These are a few of my favorite',\n\t                                                   MAX_TOKENS_KEY: 10, \"temperature\": 1,\n\t                                                   MODEL_KEY: new_model_name},\n\t                                             raise_for_status=True)\n", "@pytest.mark.asyncio\n\tasync def test_text_completion__with_kwargs(mock_aioresponse, llm_client, complete_url, anthropic_version):\n\t    mock_aioresponse.post(\n\t        complete_url,\n\t        payload={COMPLETIONS_KEY: \"completion text\"}\n\t    )\n\t    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10, temperature=0.5,top_p=0.5)\n\t    assert actual == [\"completion text\"]\n\t    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n\t                                             headers={AUTH_HEADER: llm_client._api_key,\n", "                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n\t                                                      VERSION_HEADER: anthropic_version},\n\t                                             json={PROMPT_KEY: 'These are a few of my favorite',\n\t                                                   MAX_TOKENS_KEY: 10,\n\t                                                   MODEL_KEY: llm_client._default_model,\n\t                                                   \"temperature\": 0.5, \"top_p\" : 0.5},\n\t                                             raise_for_status=True)\n\t@pytest.mark.asyncio\n\tasync def test_get_tokens_count__sanity(llm_client, number_of_tokens, mock_anthropic):\n\t    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite things!\")\n", "    assert actual == 10\n\t    mock_anthropic.return_value.count_tokens.assert_awaited_once_with(\"These are a few of my favorite things!\")\n"]}
{"filename": "tests/llm_api_client/anthropic_client/conftest.py", "chunked_list": ["from unittest.mock import patch, AsyncMock\n\timport pytest\n\tfrom llm_client.llm_api_client.anthropic_client import BASE_URL, COMPLETE_PATH, VERSION_HEADER, AnthropicClient\n\tfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig\n\t@pytest.fixture\n\tdef model_name():\n\t    return \"claude-v1\"\n\t@pytest.fixture\n\tdef config(client_session, model_name):\n\t    return LLMAPIClientConfig(\"top-secret-api-key\", client_session, default_model=model_name)\n", "@pytest.fixture\n\tdef llm_client(config):\n\t    return AnthropicClient(config)\n\t@pytest.fixture\n\tdef complete_url():\n\t    return BASE_URL + COMPLETE_PATH\n\t@pytest.fixture\n\tdef number_of_tokens():\n\t    return 10\n\t@pytest.fixture\n", "def anthropic_version():\n\t    return \"2023-06-01\"\n\t@pytest.fixture(autouse=True)\n\tdef mock_anthropic(number_of_tokens, anthropic_version):\n\t    with patch(\"llm_client.llm_api_client.anthropic_client.AsyncAnthropic\") as mock_anthropic:\n\t        mock_anthropic.return_value.count_tokens = AsyncMock(return_value=number_of_tokens)\n\t        mock_anthropic.return_value.default_headers = {VERSION_HEADER: anthropic_version}\n\t        yield mock_anthropic\n"]}
{"filename": "llm_client/consts.py", "chunked_list": ["MODEL_KEY = \"model\"\n\tPROMPT_KEY = \"prompt\"\n"]}
{"filename": "llm_client/__init__.py", "chunked_list": ["__version__ = \"0.8.0\"\n\tfrom llm_client.base_llm_client import BaseLLMClient\n\t# load api clients\n\ttry:\n\t    from llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig, ChatMessage, Role\n\t    from llm_client.llm_api_client.llm_api_client_factory import LLMAPIClientFactory, LLMAPIClientType\n\t    # load base-api clients\n\t    try:\n\t        from llm_client.llm_api_client.ai21_client import AI21Client\n\t        from llm_client.llm_api_client.aleph_alpha_client import AlephAlphaClient\n", "        from llm_client.llm_api_client.google_client import GoogleClient, MessagePrompt\n\t    except ImportError:\n\t        pass\n\t    # load apis with different dependencies\n\t    try:\n\t        from llm_client.llm_api_client.openai_client import OpenAIClient\n\t    except ImportError:\n\t        pass\n\t    try:\n\t        from llm_client.llm_api_client.huggingface_client import HuggingFaceClient\n", "    except ImportError:\n\t        pass\n\t    try:\n\t        from llm_client.llm_api_client.anthropic_client import AnthropicClient\n\t    except ImportError:\n\t        pass\n\texcept ImportError:\n\t    pass\n\t# load local clients\n\ttry:\n", "    from llm_client.llm_client.local_client import LocalClient, LocalClientConfig\n\texcept ImportError:\n\t    pass\n\t# load sync support\n\ttry:\n\t    from llm_client.sync.sync_llm_api_client_factory import init_sync_llm_api_client\n\texcept ImportError:\n\t    pass\n"]}
{"filename": "llm_client/base_llm_client.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tclass BaseLLMClient(ABC):\n\t    @abstractmethod\n\t    async def text_completion(self, prompt: str, **kwargs) -> list[str]:\n\t        raise NotImplementedError()\n\t    async def get_tokens_count(self, text: str, **kwargs) -> int:\n\t        raise NotImplementedError()\n"]}
{"filename": "llm_client/llm_client/local_client.py", "chunked_list": ["from dataclasses import dataclass, field\n\tfrom typing import Any\n\ttry:\n\t    from transformers import PreTrainedModel, PreTrainedTokenizerBase\n\texcept ImportError:\n\t    PreTrainedModel = Any\n\t    PreTrainedTokenizerBase = Any\n\tfrom llm_client import BaseLLMClient\n\t@dataclass\n\tclass LocalClientConfig:\n", "    model: PreTrainedModel\n\t    tokenizer: PreTrainedTokenizerBase\n\t    tensors_type: str\n\t    device: str\n\t    encode_kwargs: dict[str, Any] = field(default_factory=dict)\n\tclass LocalClient(BaseLLMClient):\n\t    def __init__(self, llm_client_config: LocalClientConfig):\n\t        if not llm_client_config.model.can_generate():\n\t            raise TypeError(f\"{llm_client_config.model} is not a text generation model\")\n\t        self._model: PreTrainedModel = llm_client_config.model\n", "        self._tokenizer: PreTrainedTokenizerBase = llm_client_config.tokenizer\n\t        self._tensors_type: str = llm_client_config.tensors_type\n\t        self._device: str = llm_client_config.device\n\t        self._encode_kwargs: dict[str, Any] = llm_client_config.encode_kwargs\n\t        self._encode_kwargs[\"return_tensors\"] = llm_client_config.tensors_type\n\t    async def text_completion(self, prompt: str, **kwargs) -> list[str]:\n\t        input_ids = self._encode(prompt)\n\t        outputs = self._model.generate(input_ids, **kwargs)\n\t        return [self._tokenizer.decode(output) for output in outputs]\n\t    async def get_tokens_count(self, text: str, **kwargs) -> int:\n", "        return len(self._encode(text))\n\t    def _encode(self, prompt: str) -> list[int]:\n\t        return self._tokenizer.encode(prompt, **self._encode_kwargs).to(self._device)\n"]}
{"filename": "llm_client/llm_client/__init__.py", "chunked_list": []}
{"filename": "llm_client/sync/sync_llm_api_client_factory.py", "chunked_list": ["import inspect\n\tfrom functools import wraps\n\tfrom typing import Callable, Any\n\timport async_to_sync\n\tfrom aiohttp import ClientSession\n\tfrom llm_client import LLMAPIClientConfig\n\tfrom llm_client.llm_api_client.llm_api_client_factory import LLMAPIClientType, get_llm_api_client_class\n\tdef _create_new_session(f: Callable[..., Any]) -> Callable[..., Any]:\n\t    @wraps(f)\n\t    async def func(self, *args, **kwargs):\n", "        self._session = ClientSession()\n\t        try:\n\t            response = await f(self, *args, **kwargs)\n\t        except Exception as e:\n\t            await self._session.close()\n\t            raise e\n\t        await self._session.close()\n\t        return response\n\t    return func\n\tdef _decorate_all_methods_in_class(decorators):\n", "    def apply_decorator(cls: Any) -> Any:\n\t        for k, f in cls.__dict__.items():\n\t            if inspect.isfunction(f) and not k.startswith(\"_\"):\n\t                for decorator in decorators:\n\t                    setattr(cls, k, decorator(cls.__dict__[k]))\n\t        return cls\n\t    return apply_decorator\n\tdef init_sync_llm_api_client(llm_api_client_type: LLMAPIClientType, **config_kwargs) -> \"Sync BaseLLMAPIClient\":\n\t    llm_api_client_class = get_llm_api_client_class(llm_api_client_type)\n\t    return async_to_sync.methods(_decorate_all_methods_in_class([_create_new_session])(llm_api_client_class)(\n", "        LLMAPIClientConfig(session=None, **config_kwargs)))\n"]}
{"filename": "llm_client/sync/__init__.py", "chunked_list": []}
{"filename": "llm_client/llm_api_client/aleph_alpha_client.py", "chunked_list": ["from typing import Optional\n\tfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\n\tfrom llm_client.consts import PROMPT_KEY\n\tCOMPLETE_PATH = \"complete\"\n\tTOKENIZE_PATH = \"tokenize\"\n\tEMBEDDING_PATH = \"semantic_embed\"\n\tBASE_URL = \"https://api.aleph-alpha.com/\"\n\tCOMPLETIONS_KEY = \"completions\"\n\tTEXT_KEY = \"completion\"\n\tTOKENS_IDS_KEY = \"token_ids\"\n", "TOKENS_KEY = \"tokens\"\n\tREPRESENTATION_KEY = \"representation\"\n\tREPRESENTATION_DEFAULT_VALUE = \"symmetric\"\n\tEMBEDDING_KEY = \"embedding\"\n\tAUTH_HEADER = \"Authorization\"\n\tBEARER_TOKEN = \"Bearer \"\n\tMAX_TOKENS_KEY = \"maximum_tokens\"\n\tclass AlephAlphaClient(BaseLLMAPIClient):\n\t    def __init__(self, config: LLMAPIClientConfig):\n\t        super().__init__(config)\n", "        if self._base_url is None:\n\t            self._base_url = BASE_URL\n\t        self._headers[AUTH_HEADER] = BEARER_TOKEN + self._api_key\n\t    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: Optional[int] = None,\n\t                              temperature: float = 0,top_p: float = 0, **kwargs) -> \\\n\t            list[str]:\n\t        self._set_model_in_kwargs(kwargs, model)\n\t        if max_tokens is None:\n\t            raise ValueError(\"max_tokens must be specified\")\n\t        kwargs[PROMPT_KEY] = prompt\n", "        kwargs[\"top_p\"] = top_p\n\t        kwargs[\"maximum_tokens\"] = kwargs.pop(\"maximum_tokens\", max_tokens)\n\t        kwargs[\"temperature\"] = temperature\n\t        response = await self._session.post(self._base_url + COMPLETE_PATH,\n\t                                            json=kwargs,\n\t                                            headers=self._headers,\n\t                                            raise_for_status=True)\n\t        response_json = await response.json()\n\t        completions = response_json[COMPLETIONS_KEY]\n\t        return [completion[TEXT_KEY] for completion in completions]\n", "    async def embedding(self, text: str, model: Optional[str] = None,\n\t                        representation: str = REPRESENTATION_DEFAULT_VALUE,\n\t                        **kwargs) -> list[float]:\n\t        self._set_model_in_kwargs(kwargs, model)\n\t        kwargs[REPRESENTATION_KEY] = representation\n\t        kwargs[PROMPT_KEY] = text\n\t        response = await self._session.post(self._base_url + EMBEDDING_PATH,\n\t                                            json=kwargs,\n\t                                            headers=self._headers,\n\t                                            raise_for_status=True)\n", "        response_json = await response.json()\n\t        return response_json[EMBEDDING_KEY]\n\t    async def get_tokens_count(self, text: str, model: Optional[str] = None, **kwargs) -> int:\n\t        self._set_model_in_kwargs(kwargs, model)\n\t        kwargs[TOKENS_KEY] = False\n\t        kwargs[TOKENS_IDS_KEY] = True\n\t        kwargs[PROMPT_KEY] = text\n\t        response = await self._session.post(self._base_url + TOKENIZE_PATH,\n\t                                            json=kwargs,\n\t                                            headers=self._headers,\n", "                                            raise_for_status=True)\n\t        response_json = await response.json()\n\t        return len(response_json[TOKENS_IDS_KEY])\n"]}
{"filename": "llm_client/llm_api_client/anthropic_client.py", "chunked_list": ["from typing import Optional\n\tfrom anthropic import AsyncAnthropic\n\tfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig, ChatMessage, Role\n\tfrom llm_client.consts import PROMPT_KEY\n\tCOMPLETE_PATH = \"complete\"\n\tBASE_URL = \"https://api.anthropic.com/v1/\"\n\tCOMPLETIONS_KEY = \"completion\"\n\tAUTH_HEADER = \"x-api-key\"\n\tACCEPT_HEADER = \"Accept\"\n\tVERSION_HEADER = \"anthropic-version\"\n", "ACCEPT_VALUE = \"application/json\"\n\tMAX_TOKENS_KEY = \"max_tokens_to_sample\"\n\tUSER_PREFIX = \"Human:\"\n\tASSISTANT_PREFIX = \"Assistant:\"\n\tSTART_PREFIX = \"\\n\\n\"\n\tSYSTEM_START_PREFIX = \"<admin>\"\n\tSYSTEM_END_PREFIX = \"</admin>\"\n\tclass AnthropicClient(BaseLLMAPIClient):\n\t    def __init__(self, config: LLMAPIClientConfig):\n\t        super().__init__(config)\n", "        if self._base_url is None:\n\t            self._base_url = BASE_URL\n\t        self._anthropic = AsyncAnthropic()\n\t        if self._headers.get(VERSION_HEADER) is None:\n\t            self._headers[VERSION_HEADER] = self._anthropic.default_headers[VERSION_HEADER]\n\t        self._headers[ACCEPT_HEADER] = ACCEPT_VALUE\n\t        self._headers[AUTH_HEADER] = self._api_key\n\t    async def chat_completion(self, messages: list[ChatMessage], model: Optional[str] = None,\n\t                              max_tokens: Optional[int] = None, temperature: float = 1, **kwargs) -> list[str]:\n\t        return await self.text_completion(self.messages_to_text(messages), model, max_tokens, temperature, **kwargs)\n", "    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: Optional[int] = None,\n\t                              temperature: float = 1, top_p: Optional[float] = None,\n\t                              **kwargs) -> \\\n\t            list[str]:\n\t        if max_tokens is None and kwargs.get(MAX_TOKENS_KEY) is None:\n\t            raise ValueError(f\"max_tokens or {MAX_TOKENS_KEY} must be specified\")\n\t        if top_p:\n\t            kwargs[\"top_p\"] = top_p\n\t        self._set_model_in_kwargs(kwargs, model)\n\t        kwargs[PROMPT_KEY] = prompt\n", "        kwargs[MAX_TOKENS_KEY] = kwargs.pop(MAX_TOKENS_KEY, max_tokens)\n\t        kwargs[\"temperature\"] = temperature\n\t        response = await self._session.post(self._base_url + COMPLETE_PATH,\n\t                                            json=kwargs,\n\t                                            headers=self._headers,\n\t                                            raise_for_status=True)\n\t        response_json = await response.json()\n\t        return [response_json[COMPLETIONS_KEY]]\n\t    async def get_chat_tokens_count(self, messages: list[ChatMessage], **kwargs) -> int:\n\t        return await self.get_tokens_count(self.messages_to_text(messages), **kwargs)\n", "    async def get_tokens_count(self, text: str, **kwargs) -> int:\n\t        return await self._anthropic.count_tokens(text)\n\t    def messages_to_text(self, messages: list[ChatMessage]) -> str:\n\t        prompt = START_PREFIX\n\t        prompt += START_PREFIX.join(map(self._message_to_prompt, messages))\n\t        if messages[-1].role != Role.ASSISTANT:\n\t            prompt += START_PREFIX\n\t            prompt += self._message_to_prompt(ChatMessage(role=Role.ASSISTANT, content=\"\"))\n\t        return prompt.rstrip()\n\t    @staticmethod\n", "    def _message_to_prompt(message: ChatMessage) -> str:\n\t        if message.role == Role.USER:\n\t            return f\"{USER_PREFIX} {message.content}\"\n\t        if message.role == Role.ASSISTANT:\n\t            return f\"{ASSISTANT_PREFIX} {message.content}\"\n\t        if message.role == Role.SYSTEM:\n\t            return f\"{USER_PREFIX} {SYSTEM_START_PREFIX}{message.content}{SYSTEM_END_PREFIX}\"\n\t        raise ValueError(f\"Unknown role: {message.role}\")\n"]}
{"filename": "llm_client/llm_api_client/__init__.py", "chunked_list": []}
{"filename": "llm_client/llm_api_client/google_client.py", "chunked_list": ["from typing import Any, Optional\n\ttry:\n\t    from google.ai.generativelanguage_v1beta2 import MessagePrompt\n\texcept ImportError:\n\t    MessagePrompt = Any  # This only needed for runtime chat_completion and chat tokens count\n\tfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\n\tfrom llm_client.consts import PROMPT_KEY\n\tCOMPLETE_PATH = \"generateText\"\n\tCHAT_PATH = \"generateMessage\"\n\tEMBEDDING_PATH = \"embedText\"\n", "TOKENIZE_PATH = \"countMessageTokens\"\n\tBASE_URL = \"https://generativelanguage.googleapis.com/v1beta2/models/\"\n\tCOMPLETIONS_KEY = \"candidates\"\n\tTEXT_KEY = \"text\"\n\tMAX_TOKENS_KEY = \"maxOutputTokens\"\n\tCOMPLETIONS_OUTPUT_KEY = \"output\"\n\tTOKENS_KEY = \"tokenCount\"\n\tMESSAGES_KEY = \"messages\"\n\tMESSAGE_CONTENT_KEY = \"content\"\n\tEMBEDDING_KEY = \"embedding\"\n", "EMBEDDING_VALUE_KEY = \"value\"\n\tAUTH_PARAM = \"key\"\n\tclass GoogleClient(BaseLLMAPIClient):\n\t    def __init__(self, config: LLMAPIClientConfig):\n\t        super().__init__(config)\n\t        if self._base_url is None:\n\t            self._base_url = BASE_URL\n\t        self._params = {AUTH_PARAM: self._api_key}\n\t    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: Optional[int] = 64,\n\t                              temperature: Optional[float] = None,top_p: Optional[float] = None, **kwargs) -> list[str]:\n", "        model = model or self._default_model\n\t        kwargs[PROMPT_KEY] = {TEXT_KEY: prompt}\n\t        kwargs[MAX_TOKENS_KEY] = kwargs.pop(MAX_TOKENS_KEY, max_tokens)\n\t        if top_p:\n\t            kwargs[\"topP\"] = top_p\n\t        kwargs[\"temperature\"] = kwargs.pop(\"temperature\", temperature)\n\t        response = await self._session.post(self._base_url + model + \":\" + COMPLETE_PATH,\n\t                                            params=self._params,\n\t                                            json=kwargs,\n\t                                            headers=self._headers,\n", "                                            raise_for_status=True)\n\t        response_json = await response.json()\n\t        completions = response_json[COMPLETIONS_KEY]\n\t        return [completion[COMPLETIONS_OUTPUT_KEY] for completion in completions]\n\t    async def embedding(self, text: str, model: Optional[str] = None, **kwargs) -> list[float]:\n\t        model = model or self._default_model\n\t        response = await self._session.post(self._base_url + model + \":\" + EMBEDDING_PATH,\n\t                                            params=self._params,\n\t                                            json={TEXT_KEY: text},\n\t                                            headers=self._headers,\n", "                                            raise_for_status=True)\n\t        response_json = await response.json()\n\t        return response_json[EMBEDDING_KEY][EMBEDDING_VALUE_KEY]\n\t    async def get_tokens_count(self, text: str, model: Optional[str] = None,\n\t                               messages: Optional[MessagePrompt] = None, **kwargs) -> int:\n\t        \"\"\"\n\t        Retrieves the count of tokens in the given text using the specified model or the default_model.\n\t        :param text: (str) The input text to tokenize and count the tokens. If the keyword argument `${MESSAGES_KEY}` \\\n\t                       is provided, this parameter will be ignored.\n\t        :param model: (Optional[str], optional) The name of the model to use for tokenization. If not provided,\n", "                       the default model will be used. Defaults to `None`.\n\t        :param messages: (MessagePrompt | None, optional) The messages to tokenize and count the tokens. If provided,\n\t                            the `text` parameter will be ignored.\n\t        :param kwargs: Ignored.\n\t        :return: (int) The count of tokens in the given text.\n\t        \"\"\"\n\t        model = model or self._default_model\n\t        if not messages:\n\t            messages = {MESSAGES_KEY: [{MESSAGE_CONTENT_KEY: text}]}\n\t        response = await self._session.post(self._base_url + model + \":\" + TOKENIZE_PATH,\n", "                                            params=self._params,\n\t                                            json={PROMPT_KEY: messages},\n\t                                            headers=self._headers,\n\t                                            raise_for_status=True)\n\t        response_json = await response.json()\n\t        return response_json[TOKENS_KEY]\n"]}
{"filename": "llm_client/llm_api_client/ai21_client.py", "chunked_list": ["from typing import Optional\n\tfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\n\tfrom llm_client.consts import PROMPT_KEY\n\tCOMPLETE_PATH = \"complete\"\n\tTOKENIZE_PATH = \"tokenize\"\n\tBASE_URL = \"https://api.ai21.com/studio/v1/\"\n\tCOMPLETIONS_KEY = \"completions\"\n\tDATA_KEY = \"data\"\n\tTEXT_KEY = \"text\"\n\tTOKENS_KEY = \"tokens\"\n", "AUTH_HEADER = \"Authorization\"\n\tBEARER_TOKEN = \"Bearer \"\n\tclass AI21Client(BaseLLMAPIClient):\n\t    def __init__(self, config: LLMAPIClientConfig):\n\t        super().__init__(config)\n\t        if self._base_url is None:\n\t            self._base_url = BASE_URL\n\t        self._headers[AUTH_HEADER] = BEARER_TOKEN + self._api_key\n\t    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: int = 16,\n\t                              temperature: float = 0.7, top_p: float = 1,**kwargs) -> list[str]:\n", "        model = model or self._default_model\n\t        kwargs[PROMPT_KEY] = prompt\n\t        kwargs[\"topP\"] = kwargs.pop(\"topP\", top_p)\n\t        kwargs[\"maxTokens\"] = kwargs.pop(\"maxTokens\", max_tokens)\n\t        kwargs[\"temperature\"] = temperature\n\t        response = await self._session.post(self._base_url + model + \"/\" + COMPLETE_PATH,\n\t                                            json=kwargs,\n\t                                            headers=self._headers,\n\t                                            raise_for_status=True)\n\t        response_json = await response.json()\n", "        completions = response_json[COMPLETIONS_KEY]\n\t        return [completion[DATA_KEY][TEXT_KEY] for completion in completions]\n\t    async def get_tokens_count(self, text: str, **kwargs) -> int:\n\t        response = await self._session.post(self._base_url + TOKENIZE_PATH,\n\t                                            json={TEXT_KEY: text},\n\t                                            headers=self._headers,\n\t                                            raise_for_status=True)\n\t        response_json = await response.json()\n\t        return len(response_json[TOKENS_KEY])\n"]}
{"filename": "llm_client/llm_api_client/openai_client.py", "chunked_list": ["from functools import lru_cache\n\tfrom typing import Optional\n\timport openai\n\timport tiktoken\n\tfrom tiktoken import Encoding\n\tfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig, ChatMessage\n\tfrom llm_client.consts import PROMPT_KEY\n\tINPUT_KEY = \"input\"\n\tMODEL_NAME_TO_TOKENS_PER_MESSAGE_AND_TOKENS_PER_NAME = {\n\t    \"gpt-3.5-turbo-0613\": (3, 1),\n", "    \"gpt-3.5-turbo-16k-0613\": (3, 1),\n\t    \"gpt-4-0314\": (3, 1),\n\t    \"gpt-4-32k-0314\": (3, 1),\n\t    \"gpt-4-0613\": (3, 1),\n\t    \"gpt-4-32k-0613\": (3, 1),\n\t    # every message follows <|start|>{role/name}\\n{content}<|end|>\\n, if there's a name, the role is omitted\n\t    \"gpt-3.5-turbo-0301\": (4, -1),\n\t}\n\tclass OpenAIClient(BaseLLMAPIClient):\n\t    def __init__(self, config: LLMAPIClientConfig):\n", "        super().__init__(config)\n\t        openai.api_key = self._api_key\n\t        openai.aiosession.set(self._session)\n\t        self._client = openai\n\t    async def text_completion(self, prompt: str, model: Optional[str] = None, temperature: float = 0,\n\t                              max_tokens: int = 16, top_p: float = 1, **kwargs) -> list[str]:\n\t        self._set_model_in_kwargs(kwargs, model)\n\t        kwargs[PROMPT_KEY] = prompt\n\t        kwargs[\"top_p\"] = top_p\n\t        kwargs[\"temperature\"] = temperature\n", "        kwargs[\"max_tokens\"] = max_tokens\n\t        completions = await self._client.Completion.acreate(headers=self._headers, **kwargs)\n\t        return [choice.text for choice in completions.choices]\n\t    async def chat_completion(self, messages: list[ChatMessage], temperature: float = 0,\n\t                              max_tokens: int = 16, top_p: float = 1, model: Optional[str] = None, **kwargs) \\\n\t            -> list[str]:\n\t        self._set_model_in_kwargs(kwargs, model)\n\t        kwargs[\"messages\"] = [message.to_dict() for message in messages]\n\t        kwargs[\"temperature\"] = temperature\n\t        kwargs[\"top_p\"] = top_p\n", "        kwargs[\"max_tokens\"] = max_tokens\n\t        completions = await self._client.ChatCompletion.acreate(headers=self._headers, **kwargs)\n\t        return [choice.message.content for choice in completions.choices]\n\t    async def embedding(self, text: str, model: Optional[str] = None, **kwargs) -> list[float]:\n\t        self._set_model_in_kwargs(kwargs, model)\n\t        kwargs[INPUT_KEY] = text\n\t        embeddings = await openai.Embedding.acreate(**kwargs)\n\t        return embeddings.data[0].embedding\n\t    async def get_tokens_count(self, text: str, model: Optional[str] = None, **kwargs) -> int:\n\t        if model is None:\n", "            model = self._default_model\n\t        return len(self._get_relevant_tokeniser(model).encode(text))\n\t    async def get_chat_tokens_count(self, messages: list[ChatMessage], model: Optional[str] = None) -> int:\n\t        \"\"\"\n\t        This is based on:\n\t        https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\t        \"\"\"\n\t        model = self._get_model_name_for_tokeniser(model)\n\t        encoding = self._get_relevant_tokeniser(model)\n\t        tokens_per_message, tokens_per_name = MODEL_NAME_TO_TOKENS_PER_MESSAGE_AND_TOKENS_PER_NAME[model]\n", "        num_tokens = 0\n\t        for message in messages:\n\t            num_tokens += tokens_per_message\n\t            num_tokens += len(encoding.encode(message.content))\n\t            num_tokens += len(encoding.encode(message.role.value))\n\t            if message.name:\n\t                num_tokens += len(encoding.encode(message.name))\n\t                num_tokens += tokens_per_name\n\t        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n\t        return num_tokens\n", "    def _get_model_name_for_tokeniser(self, model: Optional[str] = None) -> str:\n\t        if model is None:\n\t            model = self._default_model\n\t        if model in {\n\t            \"gpt-3.5-turbo-0613\",\n\t            \"gpt-3.5-turbo-16k-0613\",\n\t            \"gpt-4-0314\",\n\t            \"gpt-4-32k-0314\",\n\t            \"gpt-4-0613\",\n\t            \"gpt-4-32k-0613\",\n", "        }:\n\t            return model\n\t        elif model == \"gpt-3.5-turbo-0301\":\n\t            return model\n\t        elif \"gpt-3.5-turbo\" in model:\n\t            print(\"Warning: gpt-3.5-turbo may update over time. Returning tokeniser assuming gpt-3.5-turbo-0613.\")\n\t            return \"gpt-3.5-turbo-0613\"\n\t        elif \"gpt-4\" in model:\n\t            print(\"Warning: gpt-4 may update over time. Returning tokeniser assuming gpt-4-0613.\")\n\t            return \"gpt-4-0613\"\n", "        else:\n\t            raise NotImplementedError(\n\t                f\"\"\"not implemented for model {model}. \n\t                See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n\t            )\n\t    @staticmethod\n\t    @lru_cache(maxsize=40)\n\t    def _get_relevant_tokeniser(model: str) -> Encoding:\n\t        return tiktoken.encoding_for_model(model)\n"]}
{"filename": "llm_client/llm_api_client/base_llm_api_client.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom dataclasses import dataclass, field\n\tfrom enum import Enum\n\tfrom typing import Any, Optional\n\tfrom dataclasses_json import dataclass_json, config\n\ttry:\n\t    from aiohttp import ClientSession\n\texcept ImportError:\n\t    ClientSession = Any\n\tfrom llm_client import BaseLLMClient\n", "from llm_client.consts import MODEL_KEY\n\tclass Role(Enum):\n\t    SYSTEM = \"system\"\n\t    USER = \"user\"\n\t    ASSISTANT = \"assistant\"\n\t@dataclass_json\n\t@dataclass\n\tclass ChatMessage:\n\t    role: Role = field(metadata=config(encoder=lambda role: role.value, decoder=Role))\n\t    content: str\n", "    name: Optional[str] = field(default=None, metadata=config(exclude=lambda name: name is None))\n\t    example: bool = field(default=False, metadata=config(exclude=lambda _: True))\n\t@dataclass\n\tclass LLMAPIClientConfig:\n\t    api_key: str\n\t    session: ClientSession\n\t    base_url: Optional[str] = None\n\t    default_model: Optional[str] = None\n\t    headers: dict[str, Any] = field(default_factory=dict)\n\tclass BaseLLMAPIClient(BaseLLMClient, ABC):\n", "    def __init__(self, config: LLMAPIClientConfig):\n\t        self._api_key: str = config.api_key\n\t        self._session: ClientSession = config.session\n\t        self._base_url: str = config.base_url\n\t        self._default_model: str = config.default_model\n\t        self._headers: dict[str, str] = config.headers\n\t    @abstractmethod\n\t    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: Optional[int] = None,\n\t                              temperature: Optional[float] = None, top_p: Optional[float] = None, **kwargs) -> list[str]:\n\t        raise NotImplementedError()\n", "    async def chat_completion(self, messages: list[ChatMessage], temperature: float = 0,\n\t                              max_tokens: int = 16, model: Optional[str] = None, **kwargs) -> list[str]:\n\t        raise NotImplementedError()\n\t    async def embedding(self, text: str, model: Optional[str] = None, **kwargs) -> list[float]:\n\t        raise NotImplementedError()\n\t    async def get_chat_tokens_count(self, messages: list[ChatMessage], **kwargs) -> int:\n\t        raise NotImplementedError()\n\t    def _set_model_in_kwargs(self, kwargs, model: Optional[str]) -> None:\n\t        if model is not None:\n\t            kwargs[MODEL_KEY] = model\n", "        kwargs.setdefault(MODEL_KEY, self._default_model)\n"]}
{"filename": "llm_client/llm_api_client/huggingface_client.py", "chunked_list": ["from typing import Optional\n\tfrom transformers import AutoTokenizer\n\tfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\n\tDEFAULT_DIR = \"OpenAssistant\"\n\tBASE_URL = f\"https://api-inference.huggingface.co/models/{DEFAULT_DIR}/\"\n\tCOMPLETIONS_KEY = 0\n\tINPUT_KEY = \"inputs\"\n\tTEXT_KEY = \"generated_text\"\n\tAUTH_HEADER = \"Authorization\"\n\tBEARER_TOKEN = \"Bearer \"\n", "DEFAULT_MODEL = \"oasst-sft-4-pythia-12b-epoch-3.5\"\n\tCONST_SLASH = '/'\n\tEMPTY_STR = ''\n\tNEWLINE = '\\n'\n\tTEMPERATURE_KEY = \"temperature\"\n\tTOKENS_KEY = \"max_length\"\n\tclass HuggingFaceClient(BaseLLMAPIClient):\n\t    def __init__(self, config: LLMAPIClientConfig):\n\t        super().__init__(config)\n\t        if self._base_url is None:\n", "            self._base_url = BASE_URL\n\t        if self._default_model is None:\n\t            self._default_model = DEFAULT_MODEL\n\t        self._headers[AUTH_HEADER] = BEARER_TOKEN + self._api_key\n\t    async def text_completion(self, prompt: str, max_tokens: Optional[int] = None, temperature: float = 1.0,\n\t                              model: Optional[str] = None, top_p: Optional[float] = None, **kwargs) -> list[str]:\n\t        model = model or self._default_model\n\t        kwargs[\"top_p\"] = top_p\n\t        kwargs[INPUT_KEY] = prompt\n\t        kwargs[TEMPERATURE_KEY] = temperature\n", "        kwargs[TOKENS_KEY] = kwargs.pop(TOKENS_KEY, max_tokens)\n\t        response = await self._session.post(self._base_url + model + CONST_SLASH,\n\t                                            json=kwargs,\n\t                                            headers=self._headers,\n\t                                            raise_for_status=True)\n\t        response_json = await response.json()\n\t        if isinstance(response_json, list):\n\t            completions = response_json[COMPLETIONS_KEY][TEXT_KEY]\n\t        else:\n\t            completions = response_json[TEXT_KEY]\n", "        return [completion for completion in completions.split(NEWLINE) if completion != EMPTY_STR][1:]\n\t    def get_tokens_count(self, text: str, **kwargs) -> int:\n\t        tokenizer = AutoTokenizer.from_pretrained(DEFAULT_DIR + CONST_SLASH + self._default_model)\n\t        return len(tokenizer.encode(text))\n"]}
{"filename": "llm_client/llm_api_client/llm_api_client_factory.py", "chunked_list": ["from enum import Enum\n\tfrom typing import Optional\n\tfrom aiohttp import ClientSession\n\tfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\n\tclass LLMAPIClientType(Enum):\n\t    OPEN_AI = \"OpenAI\"\n\t    AI21 = \"AI21\"\n\t    HUGGING_FACE = \"HUGGING_FACE\"\n\t    ALEPH_ALPHA = \"AlephAlpha\"\n\t    ANTHROPIC = \"ANTHROPIC\"\n", "    GOOGLE = \"GOOGLE\"\n\tclass LLMAPIClientFactory:\n\t    def __init__(self):\n\t        self._session: Optional[ClientSession] = None\n\t    async def __aenter__(self):\n\t        self._session = ClientSession()\n\t        return self\n\t    async def __aexit__(self, exc_type, exc_val, exc_tb):\n\t        await self._session.close()\n\t    def get_llm_api_client(self, llm_api_client_type: LLMAPIClientType, **config_kwargs) -> BaseLLMAPIClient:\n", "        if self._session is None:\n\t            raise ValueError(\"Must be used as an context manager\")\n\t        config = LLMAPIClientConfig(session=self._session, **config_kwargs)\n\t        llm_api_client_class = get_llm_api_client_class(llm_api_client_type)\n\t        return llm_api_client_class(config)\n\tdef get_llm_api_client_class(llm_api_client_type: LLMAPIClientType):\n\t    if llm_api_client_type == LLMAPIClientType.OPEN_AI:\n\t        from llm_client import OpenAIClient\n\t        return OpenAIClient\n\t    elif llm_api_client_type == LLMAPIClientType.AI21:\n", "        from llm_client import AI21Client\n\t        return AI21Client\n\t    elif llm_api_client_type == LLMAPIClientType.HUGGING_FACE:\n\t        from llm_client import HuggingFaceClient\n\t        return HuggingFaceClient\n\t    elif llm_api_client_type == LLMAPIClientType.ALEPH_ALPHA:\n\t        from llm_client import AlephAlphaClient\n\t        return AlephAlphaClient\n\t    elif llm_api_client_type == LLMAPIClientType.ANTHROPIC:\n\t        from llm_client import AnthropicClient\n", "        return AnthropicClient\n\t    elif llm_api_client_type == LLMAPIClientType.GOOGLE:\n\t        from llm_client import GoogleClient\n\t        return GoogleClient\n\t    else:\n\t        raise ValueError(\"Unknown LLM client type\")\n"]}
