{"filename": "setup.py", "chunked_list": ["import setuptools\n\twith open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\n\t    long_description = f.read()\n\t__version__ = \"0.0.0\"\n\tREPO_NAME = \"Text-Summarizer-Project\"\n\tAUTHOR_USER_NAME = \"entbappy\"\n\tSRC_REPO = \"textSummarizer\"\n\tAUTHOR_EMAIL = \"entbappy73@gmail.com\"\n\tsetuptools.setup(\n\t    name=SRC_REPO,\n", "    version=__version__,\n\t    author=AUTHOR_USER_NAME,\n\t    author_email=AUTHOR_EMAIL,\n\t    description=\"A small python package for NLP app\",\n\t    long_description=long_description,\n\t    long_description_content=\"text/markdown\",\n\t    url=f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}\",\n\t    project_urls={\n\t        \"Bug Tracker\": f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}/issues\",\n\t    },\n", "    package_dir={\"\": \"src\"},\n\t    packages=setuptools.find_packages(where=\"src\")\n\t)"]}
{"filename": "main.py", "chunked_list": ["from textSummarizer.pipeline.stage_01_data_ingestion import DataIngestionTrainingPipeline\n\tfrom textSummarizer.pipeline.stage_02_data_validation import DataValidationTrainingPipeline\n\tfrom textSummarizer.pipeline.stage_03_data_transformation import DataTransformationTrainingPipeline\n\tfrom textSummarizer.pipeline.stage_04_model_trainer import ModelTrainerTrainingPipeline\n\tfrom textSummarizer.pipeline.stage_05_model_evaluation import ModelEvaluationTrainingPipeline\n\tfrom textSummarizer.logging import logger\n\tSTAGE_NAME = \"Data Ingestion stage\"\n\ttry:\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \n\t   data_ingestion = DataIngestionTrainingPipeline()\n", "   data_ingestion.main()\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n\texcept Exception as e:\n\t        logger.exception(e)\n\t        raise e\n\tSTAGE_NAME = \"Data Validation stage\"\n\ttry:\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \n\t   data_validation = DataValidationTrainingPipeline()\n\t   data_validation.main()\n", "   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n\texcept Exception as e:\n\t        logger.exception(e)\n\t        raise e\n\tSTAGE_NAME = \"Data Transformation stage\"\n\ttry:\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \n\t   data_transformation = DataTransformationTrainingPipeline()\n\t   data_transformation.main()\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n", "except Exception as e:\n\t        logger.exception(e)\n\t        raise e\n\tSTAGE_NAME = \"Model Trainer stage\"\n\ttry: \n\t   logger.info(f\"*******************\")\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n\t   model_trainer = ModelTrainerTrainingPipeline()\n\t   model_trainer.main()\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n", "except Exception as e:\n\t        logger.exception(e)\n\t        raise e\n\tSTAGE_NAME = \"Model Evaluation stage\"\n\ttry: \n\t   logger.info(f\"*******************\")\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n\t   model_evaluation = ModelEvaluationTrainingPipeline()\n\t   model_evaluation.main()\n\t   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n", "except Exception as e:\n\t        logger.exception(e)\n\t        raise e\n"]}
{"filename": "app.py", "chunked_list": ["from fastapi import FastAPI\n\timport uvicorn\n\timport sys\n\timport os\n\tfrom fastapi.templating import Jinja2Templates\n\tfrom starlette.responses import RedirectResponse\n\tfrom fastapi.responses import Response\n\tfrom textSummarizer.pipeline.prediction import PredictionPipeline\n\ttext:str = \"What is Text Summarization?\"\n\tapp = FastAPI()\n", "@app.get(\"/\", tags=[\"authentication\"])\n\tasync def index():\n\t    return RedirectResponse(url=\"/docs\")\n\t@app.get(\"/train\")\n\tasync def training():\n\t    try:\n\t        os.system(\"python main.py\")\n\t        return Response(\"Training successful !!\")\n\t    except Exception as e:\n\t        return Response(f\"Error Occurred! {e}\")\n", "@app.post(\"/predict\")\n\tasync def predict_route(text):\n\t    try:\n\t        obj = PredictionPipeline()\n\t        text = obj.predict(text)\n\t        return text\n\t    except Exception as e:\n\t        raise e\n\tif __name__==\"__main__\":\n\t    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n"]}
{"filename": "template.py", "chunked_list": ["import os\n\tfrom pathlib import Path\n\timport logging\n\tlogging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n\tproject_name = \"textSummarizer\"\n\tlist_of_files = [\n\t    \".github/workflows/.gitkeep\",\n\t    f\"src/{project_name}/__init__.py\",\n\t    f\"src/{project_name}/conponents/__init__.py\",\n\t    f\"src/{project_name}/utils/__init__.py\",\n", "    f\"src/{project_name}/utils/common.py\",\n\t    f\"src/{project_name}/logging/__init__.py\",\n\t    f\"src/{project_name}/config/__init__.py\",\n\t    f\"src/{project_name}/config/configuration.py\",\n\t    f\"src/{project_name}/pipeline/__init__.py\",\n\t    f\"src/{project_name}/entity/__init__.py\",\n\t    f\"src/{project_name}/constants/__init__.py\",\n\t    \"config/config.yaml\",\n\t    \"params.yaml\",\n\t    \"app.py\",\n", "    \"main.py\",\n\t    \"Dockerfile\",\n\t    \"requirements.txt\",\n\t    \"setup.py\",\n\t    \"research/trials.ipynb\",\n\t]\n\tfor filepath in list_of_files:\n\t    filepath = Path(filepath)\n\t    filedir, filename = os.path.split(filepath)\n\t    if filedir != \"\":\n", "        os.makedirs(filedir, exist_ok=True)\n\t        logging.info(f\"Creating directory:{filedir} for the file {filename}\")\n\t    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n\t        with open(filepath,'w') as f:\n\t            pass\n\t            logging.info(f\"Creating empty file: {filepath}\")\n\t    else:\n\t        logging.info(f\"{filename} is already exists\")\n"]}
{"filename": "test.py", "chunked_list": []}
{"filename": "src/textSummarizer/__init__.py", "chunked_list": []}
{"filename": "src/textSummarizer/entity/__init__.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom pathlib import Path\n\t@dataclass(frozen=True)\n\tclass DataIngestionConfig:\n\t    root_dir: Path\n\t    source_URL: str\n\t    local_data_file: Path\n\t    unzip_dir: Path\n\t@dataclass(frozen=True)\n\tclass DataValidationConfig:\n", "    root_dir: Path\n\t    STATUS_FILE: str\n\t    ALL_REQUIRED_FILES: list\n\t@dataclass(frozen=True)\n\tclass DataTransformationConfig:\n\t    root_dir: Path\n\t    data_path: Path\n\t    tokenizer_name: Path\n\t@dataclass(frozen=True)\n\tclass ModelTrainerConfig:\n", "    root_dir: Path\n\t    data_path: Path\n\t    model_ckpt: Path\n\t    num_train_epochs: int\n\t    warmup_steps: int\n\t    per_device_train_batch_size: int\n\t    weight_decay: float\n\t    logging_steps: int\n\t    evaluation_strategy: str\n\t    eval_steps: int\n", "    save_steps: float\n\t    gradient_accumulation_steps: int\n\t@dataclass(frozen=True)\n\tclass ModelEvaluationConfig:\n\t    root_dir: Path\n\t    data_path: Path\n\t    model_path: Path\n\t    tokenizer_path: Path\n\t    metric_file_name: Path"]}
{"filename": "src/textSummarizer/utils/__init__.py", "chunked_list": []}
{"filename": "src/textSummarizer/utils/common.py", "chunked_list": ["import os\n\tfrom box.exceptions import BoxValueError\n\timport yaml\n\tfrom textSummarizer.logging import logger\n\tfrom ensure import ensure_annotations\n\tfrom box import ConfigBox\n\tfrom pathlib import Path\n\tfrom typing import Any\n\t@ensure_annotations\n\tdef read_yaml(path_to_yaml: Path) -> ConfigBox:\n", "    \"\"\"reads yaml file and returns\n\t    Args:\n\t        path_to_yaml (str): path like input\n\t    Raises:\n\t        ValueError: if yaml file is empty\n\t        e: empty file\n\t    Returns:\n\t        ConfigBox: ConfigBox type\n\t    \"\"\"\n\t    try:\n", "        with open(path_to_yaml) as yaml_file:\n\t            content = yaml.safe_load(yaml_file)\n\t            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\n\t            return ConfigBox(content)\n\t    except BoxValueError:\n\t        raise ValueError(\"yaml file is empty\")\n\t    except Exception as e:\n\t        raise e\n\t@ensure_annotations\n\tdef create_directories(path_to_directories: list, verbose=True):\n", "    \"\"\"create list of directories\n\t    Args:\n\t        path_to_directories (list): list of path of directories\n\t        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\n\t    \"\"\"\n\t    for path in path_to_directories:\n\t        os.makedirs(path, exist_ok=True)\n\t        if verbose:\n\t            logger.info(f\"created directory at: {path}\")\n\t@ensure_annotations\n", "def get_size(path: Path) -> str:\n\t    \"\"\"get size in KB\n\t    Args:\n\t        path (Path): path of the file\n\t    Returns:\n\t        str: size in KB\n\t    \"\"\"\n\t    size_in_kb = round(os.path.getsize(path)/1024)\n\t    return f\"~ {size_in_kb} KB\"\n"]}
{"filename": "src/textSummarizer/config/__init__.py", "chunked_list": []}
{"filename": "src/textSummarizer/config/configuration.py", "chunked_list": ["from textSummarizer.constants import *\n\tfrom textSummarizer.utils.common import read_yaml, create_directories\n\tfrom textSummarizer.entity import (DataIngestionConfig,\n\t                                   DataValidationConfig,\n\t                                   DataTransformationConfig,\n\t                                   ModelTrainerConfig,\n\t                                   ModelEvaluationConfig)\n\tclass ConfigurationManager:\n\t    def __init__(\n\t        self,\n", "        config_filepath = CONFIG_FILE_PATH,\n\t        params_filepath = PARAMS_FILE_PATH):\n\t        self.config = read_yaml(config_filepath)\n\t        self.params = read_yaml(params_filepath)\n\t        create_directories([self.config.artifacts_root])\n\t    def get_data_ingestion_config(self) -> DataIngestionConfig:\n\t        config = self.config.data_ingestion\n\t        create_directories([config.root_dir])\n\t        data_ingestion_config = DataIngestionConfig(\n\t            root_dir=config.root_dir,\n", "            source_URL=config.source_URL,\n\t            local_data_file=config.local_data_file,\n\t            unzip_dir=config.unzip_dir \n\t        )\n\t        return data_ingestion_config\n\t    def get_data_validation_config(self) -> DataValidationConfig:\n\t        config = self.config.data_validation\n\t        create_directories([config.root_dir])\n\t        data_validation_config = DataValidationConfig(\n\t            root_dir=config.root_dir,\n", "            STATUS_FILE=config.STATUS_FILE,\n\t            ALL_REQUIRED_FILES=config.ALL_REQUIRED_FILES,\n\t        )\n\t        return data_validation_config\n\t    def get_data_transformation_config(self) -> DataTransformationConfig:\n\t        config = self.config.data_transformation\n\t        create_directories([config.root_dir])\n\t        data_transformation_config = DataTransformationConfig(\n\t            root_dir=config.root_dir,\n\t            data_path=config.data_path,\n", "            tokenizer_name = config.tokenizer_name\n\t        )\n\t        return data_transformation_config\n\t    def get_model_trainer_config(self) -> ModelTrainerConfig:\n\t        config = self.config.model_trainer\n\t        params = self.params.TrainingArguments\n\t        create_directories([config.root_dir])\n\t        model_trainer_config = ModelTrainerConfig(\n\t            root_dir=config.root_dir,\n\t            data_path=config.data_path,\n", "            model_ckpt = config.model_ckpt,\n\t            num_train_epochs = params.num_train_epochs,\n\t            warmup_steps = params.warmup_steps,\n\t            per_device_train_batch_size = params.per_device_train_batch_size,\n\t            weight_decay = params.weight_decay,\n\t            logging_steps = params.logging_steps,\n\t            evaluation_strategy = params.evaluation_strategy,\n\t            eval_steps = params.evaluation_strategy,\n\t            save_steps = params.save_steps,\n\t            gradient_accumulation_steps = params.gradient_accumulation_steps\n", "        )\n\t        return model_trainer_config\n\t    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n\t        config = self.config.model_evaluation\n\t        create_directories([config.root_dir])\n\t        model_evaluation_config = ModelEvaluationConfig(\n\t            root_dir=config.root_dir,\n\t            data_path=config.data_path,\n\t            model_path = config.model_path,\n\t            tokenizer_path = config.tokenizer_path,\n", "            metric_file_name = config.metric_file_name\n\t        )\n\t        return model_evaluation_config\n"]}
{"filename": "src/textSummarizer/logging/__init__.py", "chunked_list": ["import os\n\timport sys\n\timport logging\n\tlogging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s]\"\n\tlog_dir = \"logs\"\n\tlog_filepath = os.path.join(log_dir,\"running_logs.log\")\n\tos.makedirs(log_dir, exist_ok=True)\n\tlogging.basicConfig(\n\t    level= logging.INFO,\n\t    format= logging_str,\n", "    handlers=[\n\t        logging.FileHandler(log_filepath),\n\t        logging.StreamHandler(sys.stdout)\n\t    ]\n\t)\n\tlogger = logging.getLogger(\"textSummarizerLogger\")"]}
{"filename": "src/textSummarizer/pipeline/stage_03_data_transformation.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\n\tfrom textSummarizer.conponents.data_transformation import DataTransformation\n\tfrom textSummarizer.logging import logger\n\tclass DataTransformationTrainingPipeline:\n\t    def __init__(self):\n\t        pass\n\t    def main(self):\n\t        config = ConfigurationManager()\n\t        data_transformation_config = config.get_data_transformation_config()\n\t        data_transformation = DataTransformation(config=data_transformation_config)\n", "        data_transformation.convert()"]}
{"filename": "src/textSummarizer/pipeline/stage_04_model_trainer.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\n\tfrom textSummarizer.conponents.model_trainer import ModelTrainer\n\tfrom textSummarizer.logging import logger\n\tclass ModelTrainerTrainingPipeline:\n\t    def __init__(self):\n\t        pass\n\t    def main(self):\n\t        config = ConfigurationManager()\n\t        model_trainer_config = config.get_model_trainer_config()\n\t        model_trainer_config = ModelTrainer(config=model_trainer_config)\n", "        model_trainer_config.train()"]}
{"filename": "src/textSummarizer/pipeline/prediction.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\n\tfrom transformers import AutoTokenizer\n\tfrom transformers import pipeline\n\tclass PredictionPipeline:\n\t    def __init__(self):\n\t        self.config = ConfigurationManager().get_model_evaluation_config()\n\t    def predict(self,text):\n\t        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n\t        gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n\t        pipe = pipeline(\"summarization\", model=self.config.model_path,tokenizer=tokenizer)\n", "        print(\"Dialogue:\")\n\t        print(text)\n\t        output = pipe(text, **gen_kwargs)[0][\"summary_text\"]\n\t        print(\"\\nModel Summary:\")\n\t        print(output)\n\t        return output"]}
{"filename": "src/textSummarizer/pipeline/__init__.py", "chunked_list": []}
{"filename": "src/textSummarizer/pipeline/stage_01_data_ingestion.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\n\tfrom textSummarizer.conponents.data_ingestion import DataIngestion\n\tfrom textSummarizer.logging import logger\n\tclass DataIngestionTrainingPipeline:\n\t    def __init__(self):\n\t        pass\n\t    def main(self):\n\t        config = ConfigurationManager()\n\t        data_ingestion_config = config.get_data_ingestion_config()\n\t        data_ingestion = DataIngestion(config=data_ingestion_config)\n", "        data_ingestion.download_file()\n\t        data_ingestion.extract_zip_file()\n"]}
{"filename": "src/textSummarizer/pipeline/stage_02_data_validation.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\n\tfrom textSummarizer.conponents.data_validation import DataValiadtion\n\tfrom textSummarizer.logging import logger\n\tclass DataValidationTrainingPipeline:\n\t    def __init__(self):\n\t        pass\n\t    def main(self):\n\t        config = ConfigurationManager()\n\t        data_validation_config = config.get_data_validation_config()\n\t        data_validation = DataValiadtion(config=data_validation_config)\n", "        data_validation.validate_all_files_exist()"]}
{"filename": "src/textSummarizer/pipeline/stage_05_model_evaluation.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\n\tfrom textSummarizer.conponents.model_evaluation import ModelEvaluation\n\tfrom textSummarizer.logging import logger\n\tclass ModelEvaluationTrainingPipeline:\n\t    def __init__(self):\n\t        pass\n\t    def main(self):\n\t        config = ConfigurationManager()\n\t        model_evaluation_config = config.get_model_evaluation_config()\n\t        model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n", "        model_evaluation_config.evaluate()"]}
{"filename": "src/textSummarizer/constants/__init__.py", "chunked_list": ["from pathlib import Path\n\tCONFIG_FILE_PATH = Path(\"config/config.yaml\")\n\tPARAMS_FILE_PATH = Path(\"params.yaml\")"]}
{"filename": "src/textSummarizer/conponents/data_validation.py", "chunked_list": ["import os\n\tfrom textSummarizer.logging import logger\n\tfrom textSummarizer.entity import DataValidationConfig\n\tclass DataValiadtion:\n\t    def __init__(self, config: DataValidationConfig):\n\t        self.config = config\n\t    def validate_all_files_exist(self)-> bool:\n\t        try:\n\t            validation_status = None\n\t            all_files = os.listdir(os.path.join(\"artifacts\",\"data_ingestion\",\"samsum_dataset\"))\n", "            for file in all_files:\n\t                if file not in self.config.ALL_REQUIRED_FILES:\n\t                    validation_status = False\n\t                    with open(self.config.STATUS_FILE, 'w') as f:\n\t                        f.write(f\"Validation status: {validation_status}\")\n\t                else:\n\t                    validation_status = True\n\t                    with open(self.config.STATUS_FILE, 'w') as f:\n\t                        f.write(f\"Validation status: {validation_status}\")\n\t            return validation_status\n", "        except Exception as e:\n\t            raise e\n"]}
{"filename": "src/textSummarizer/conponents/model_trainer.py", "chunked_list": ["from transformers import TrainingArguments, Trainer\n\tfrom transformers import DataCollatorForSeq2Seq\n\tfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\tfrom datasets import load_dataset, load_from_disk\n\tfrom textSummarizer.entity import ModelTrainerConfig\n\timport torch\n\timport os\n\tclass ModelTrainer:\n\t    def __init__(self, config: ModelTrainerConfig):\n\t        self.config = config\n", "    def train(self):\n\t        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\t        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n\t        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n\t        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n\t        #loading data \n\t        dataset_samsum_pt = load_from_disk(self.config.data_path)\n\t        # trainer_args = TrainingArguments(\n\t        #     output_dir=self.config.root_dir, num_train_epochs=self.config.num_train_epochs, warmup_steps=self.config.warmup_steps,\n\t        #     per_device_train_batch_size=self.config.per_device_train_batch_size, per_device_eval_batch_size=self.config.per_device_train_batch_size,\n", "        #     weight_decay=self.config.weight_decay, logging_steps=self.config.logging_steps,\n\t        #     evaluation_strategy=self.config.evaluation_strategy, eval_steps=self.config.eval_steps, save_steps=1e6,\n\t        #     gradient_accumulation_steps=self.config.gradient_accumulation_steps\n\t        # ) \n\t        trainer_args = TrainingArguments(\n\t            output_dir=self.config.root_dir, num_train_epochs=1, warmup_steps=500,\n\t            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n\t            weight_decay=0.01, logging_steps=10,\n\t            evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n\t            gradient_accumulation_steps=16\n", "        ) \n\t        trainer = Trainer(model=model_pegasus, args=trainer_args,\n\t                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n\t                  train_dataset=dataset_samsum_pt[\"train\"], \n\t                  eval_dataset=dataset_samsum_pt[\"validation\"])\n\t        trainer.train()\n\t        ## Save model\n\t        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n\t        ## Save tokenizer\n\t        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n"]}
{"filename": "src/textSummarizer/conponents/__init__.py", "chunked_list": []}
{"filename": "src/textSummarizer/conponents/model_evaluation.py", "chunked_list": ["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\tfrom datasets import load_dataset, load_from_disk, load_metric\n\timport torch\n\timport pandas as pd\n\tfrom tqdm import tqdm\n\tfrom textSummarizer.entity import ModelEvaluationConfig\n\tclass ModelEvaluation:\n\t    def __init__(self, config: ModelEvaluationConfig):\n\t        self.config = config\n\t    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\n", "        \"\"\"split the dataset into smaller batches that we can process simultaneously\n\t        Yield successive batch-sized chunks from list_of_elements.\"\"\"\n\t        for i in range(0, len(list_of_elements), batch_size):\n\t            yield list_of_elements[i : i + batch_size]\n\t    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \n\t                               batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n\t                               column_text=\"article\", \n\t                               column_summary=\"highlights\"):\n\t        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n\t        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n", "        for article_batch, target_batch in tqdm(\n\t            zip(article_batches, target_batches), total=len(article_batches)):\n\t            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n\t                            padding=\"max_length\", return_tensors=\"pt\")\n\t            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n\t                            attention_mask=inputs[\"attention_mask\"].to(device), \n\t                            length_penalty=0.8, num_beams=8, max_length=128)\n\t            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n\t            # Finally, we decode the generated texts, \n\t            # replace the  token, and add the decoded texts with the references to the metric.\n", "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n\t                                    clean_up_tokenization_spaces=True) \n\t                for s in summaries]      \n\t            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n\t            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n\t        #  Finally compute and return the ROUGE scores.\n\t        score = metric.compute()\n\t        return score\n\t    def evaluate(self):\n\t        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n\t        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n\t        #loading data \n\t        dataset_samsum_pt = load_from_disk(self.config.data_path)\n\t        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n\t        rouge_metric = load_metric('rouge')\n\t        score = self.calculate_metric_on_test_ds(\n\t        dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n\t            )\n\t        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n", "        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )\n\t        df.to_csv(self.config.metric_file_name, index=False)\n"]}
{"filename": "src/textSummarizer/conponents/data_transformation.py", "chunked_list": ["import os\n\tfrom textSummarizer.logging import logger\n\tfrom transformers import AutoTokenizer\n\tfrom datasets import load_dataset, load_from_disk\n\tfrom textSummarizer.entity import DataTransformationConfig\n\tclass DataTransformation:\n\t    def __init__(self, config: DataTransformationConfig):\n\t        self.config = config\n\t        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n\t    def convert_examples_to_features(self,example_batch):\n", "        input_encodings = self.tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n\t        with self.tokenizer.as_target_tokenizer():\n\t            target_encodings = self.tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n\t        return {\n\t            'input_ids' : input_encodings['input_ids'],\n\t            'attention_mask': input_encodings['attention_mask'],\n\t            'labels': target_encodings['input_ids']\n\t        }\n\t    def convert(self):\n\t        dataset_samsum = load_from_disk(self.config.data_path)\n", "        dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched = True)\n\t        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir,\"samsum_dataset\"))\n"]}
{"filename": "src/textSummarizer/conponents/data_ingestion.py", "chunked_list": ["import os\n\timport urllib.request as request\n\timport zipfile\n\tfrom textSummarizer.logging import logger\n\tfrom textSummarizer.utils.common import get_size\n\tfrom pathlib import Path\n\tfrom textSummarizer.entity import DataIngestionConfig\n\tclass DataIngestion:\n\t    def __init__(self, config: DataIngestionConfig):\n\t        self.config = config\n", "    def download_file(self):\n\t        if not os.path.exists(self.config.local_data_file):\n\t            filename, headers = request.urlretrieve(\n\t                url = self.config.source_URL,\n\t                filename = self.config.local_data_file\n\t            )\n\t            logger.info(f\"{filename} download! with following info: \\n{headers}\")\n\t        else:\n\t            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")  \n\t    def extract_zip_file(self):\n", "        \"\"\"\n\t        zip_file_path: str\n\t        Extracts the zip file into the data directory\n\t        Function returns None\n\t        \"\"\"\n\t        unzip_path = self.config.unzip_dir\n\t        os.makedirs(unzip_path, exist_ok=True)\n\t        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n\t            zip_ref.extractall(unzip_path)"]}
