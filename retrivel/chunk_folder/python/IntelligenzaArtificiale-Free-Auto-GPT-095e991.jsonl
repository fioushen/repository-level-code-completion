{"filename": "BABYAGI.py", "chunked_list": ["import os\n\timport json\n\tfrom dotenv import load_dotenv\n\tfrom pathlib import Path\n\tfrom json import JSONDecodeError\n\tfrom collections import deque\n\tfrom typing import Dict, List, Optional, Any\n\tfrom langchain.vectorstores import FAISS\n\tfrom langchain import HuggingFaceHub\n\tfrom langchain.docstore import InMemoryDocstore\n", "from langchain import LLMChain, PromptTemplate\n\tfrom langchain.llms import BaseLLM\n\tfrom FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\n\tfrom FreeLLM import ChatGPTAPI  # FREE CHATGPT API\n\tfrom FreeLLM import BingChatAPI  # FREE BINGCHAT API\n\tfrom FreeLLM import BardChatAPI  # FREE GOOGLE BARD API\n\tfrom langchain.vectorstores.base import VectorStore\n\tfrom pydantic import BaseModel, Field\n\tfrom langchain.chains.base import Chain\n\tfrom langchain.experimental import BabyAGI\n", "from BabyAgi import BabyAGIMod\n\timport faiss\n\tload_dotenv()\n\tselect_model = input(\n\t    \"Select the model you want to use (1, 2, 3 or 4) \\n \\\n\t1) ChatGPT \\n \\\n\t2) HuggingChat \\n \\\n\t3) BingChat (NOT GOOD RESULT)\\n \\\n\t4) BardChat \\n \\\n\t>>> \"\n", ")\n\tif select_model == \"1\":\n\t    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\t    if CG_TOKEN != \"your-chatgpt-token\":\n\t        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n\t        )\n\t    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n", "    if os.getenv(\"USE_GPT4\") == \"True\":\n\t        model = \"gpt-4\"\n\t    else:\n\t        model = \"default\"\n\t    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\telif select_model == \"2\":\n\t    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n\t    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n\t    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n\t        os.environ[\"emailHF\"] = emailHF\n", "        os.environ[\"pswHF\"] = pswHF\n\t    else:\n\t        raise ValueError(\n\t            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n\t        )\n\t    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\telif select_model == \"3\":\n\t    if not os.path.exists(\"cookiesBing.json\"):\n\t        raise ValueError(\n\t            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n", "        )\n\t    cookie_path = Path() / \"cookiesBing.json\"\n\t    with open(\"cookiesBing.json\", \"r\") as file:\n\t        try:\n\t            file_json = json.loads(file.read())\n\t        except JSONDecodeError:\n\t            raise ValueError(\n\t                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n\t            )\n\t    llm = BingChatAPI.BingChat(\n", "        cookiepath=str(cookie_path), conversation_style=\"creative\"\n\t    )\n\telif select_model == \"4\":\n\t    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\t    if GB_TOKEN != \"your-googlebard-token\":\n\t        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n\t        )\n", "    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n\t    llm = BardChatAPI.BardChat(cookie=cookie_path)\n\tHF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n\tif HF_TOKEN != \"your-huggingface-token\":\n\t    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n\telse:\n\t    raise ValueError(\n\t        \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n\t    )\n\tfrom Embedding import HuggingFaceEmbedding  # EMBEDDING FUNCTION\n", "# Define your embedding model\n\tembeddings_model = HuggingFaceEmbedding.newEmbeddingFunction\n\tembedding_size = 1536\n\tindex = faiss.IndexFlatL2(embedding_size)\n\tvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n\tprint(vectorstore)\n\t# DEFINE TOOL\n\tfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n\tfrom langchain import OpenAI, LLMChain\n\tfrom langchain.tools import BaseTool, DuckDuckGoSearchRun\n", "todo_prompt = PromptTemplate.from_template(\n\t    \"I need to create a plan for complete me GOAl. Can you help me to create a TODO list? Create only the todo list for this objective: '{objective}'.\"\n\t)\n\ttodo_chain = LLMChain(llm=llm, prompt=todo_prompt)\n\tsearch = DuckDuckGoSearchRun()\n\ttools = [\n\t    Tool(\n\t        name=\"Search\",\n\t        func=search.run,\n\t        description=\"useful for when you need to answer questions about current events\",\n", "    ),\n\t    Tool(\n\t        name=\"TODO\",\n\t        func=todo_chain.run,\n\t        description=\"useful for when you need to create a task list to complete a objective. You have to give an Input: a objective for which to create a to-do list. Output: just a list of tasks to do for that objective. It is important to give the target input 'objective' correctly!\",\n\t    ),\n\t]\n\tprefix = \"\"\"Can you help me to performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.\"\"\"\n\tsuffix = \"\"\"Question: {task}. \n\t{agent_scratchpad}\"\"\"\n", "prompt = ZeroShotAgent.create_prompt(\n\t    tools,\n\t    prefix=prefix,\n\t    suffix=suffix,\n\t    input_variables=[\"objective\", \"task\", \"context\", \"agent_scratchpad\"],\n\t)\n\tllm_chain = LLMChain(llm=llm, prompt=prompt)\n\ttool_names = [tool.name for tool in tools]\n\tagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\n\tagent_executor = AgentExecutor.from_agent_and_tools(\n", "    agent=agent, tools=tools, verbose=True\n\t)\n\t# START\n\t# Logging of LLMChains\n\tverbose = False\n\tint_max_iterations = input(\n\t    \"Enter the maximum number of iterations: (Suggest from 3 and 5) \"\n\t)\n\tmax_iterations = int(int_max_iterations)\n\tif input(\"Do you want to store the results? (y/n) \") == \"y\":\n", "    store_results = True\n\telse:\n\t    store_results = False\n\t# If None, will keep on going forever\n\tmax_iterations: Optional[int] = max_iterations\n\tbaby_agi = BabyAGIMod.BabyAGI.from_llm(\n\t    llm=llm,\n\t    vectorstore=vectorstore,\n\t    task_execution_chain=agent_executor,\n\t    verbose=verbose,\n", "    max_iterations=max_iterations,\n\t    store=store_results,\n\t)\n\t# DEFINE THE OBJECTIVE - MODIFY THIS\n\tOBJECTIVE = input(\"Enter the objective of the AI system: (Be realistic!) \")\n\tbaby_agi({\"objective\": OBJECTIVE})\n"]}
{"filename": "MetaPrompt.py", "chunked_list": ["import json\n\tfrom dotenv import load_dotenv\n\tfrom pathlib import Path\n\tfrom json import JSONDecodeError\n\tfrom langchain import LLMChain, PromptTemplate\n\tfrom FreeLLM import ChatGPTAPI  # FREE CHATGPT API\n\tfrom FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\n\tfrom FreeLLM import BingChatAPI  # FREE BINGCHAT API\n\tfrom FreeLLM import BardChatAPI  # FREE GOOGLE BARD API\n\tfrom langchain.memory import ConversationBufferWindowMemory\n", "import os\n\tload_dotenv()\n\t#### LOG IN FOR CHATGPT FREE LLM\n\tselect_model = input(\n\t    \"Select the model you want to use (1, 2, 3 or 4) \\n \\\n\t1) ChatGPT \\n \\\n\t2) HuggingChat \\n \\\n\t3) BingChat \\n \\\n\t4) Google Bard \\n \\\n\t>>> \"\n", ")\n\tif select_model == \"1\":\n\t    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\t    if CG_TOKEN != \"your-chatgpt-token\":\n\t        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n\t        )\n\t    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n", "    if os.getenv(\"USE_GPT4\") == \"True\":\n\t        model = \"gpt4\"\n\t    else:\n\t        model = \"default\"\n\t    if start_chat:\n\t        chat_id = os.getenv(\"CHAT_ID\")\n\t        if chat_id == None:\n\t            raise ValueError(\"You have to set up your chat-id in the .env file\")\n\t        llm = ChatGPTAPI.ChatGPT(\n\t            token=os.environ[\"CHATGPT_TOKEN\"], conversation=chat_id, model=model\n", "        )\n\t    else:\n\t        llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\telif select_model == \"2\":\n\t    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n\t    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n\t    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n\t        os.environ[\"emailHF\"] = emailHF\n\t        os.environ[\"pswHF\"] = pswHF\n\t    else:\n", "        raise ValueError(\n\t            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n\t        )\n\t    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\telif select_model == \"3\":\n\t    if not os.path.exists(\"cookiesBing.json\"):\n\t        raise ValueError(\n\t            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n\t        )\n\t    cookie_path = Path() / \"cookiesBing.json\"\n", "    with open(\"cookiesBing.json\", \"r\") as file:\n\t        try:\n\t            file_json = json.loads(file.read())\n\t        except JSONDecodeError:\n\t            raise ValueError(\n\t                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n\t            )\n\t    llm = BingChatAPI.BingChat(\n\t        cookiepath=str(cookie_path), conversation_style=\"creative\"\n\t    )\n", "elif select_model == \"4\":\n\t    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\t    if GB_TOKEN != \"your-googlebard-token\":\n\t        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n\t        )\n\t    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n\t    llm = BardChatAPI.BardChat(cookie=cookie_path)\n", "####\n\tdef initialize_chain(instructions, memory=None):\n\t    if memory is None:\n\t        memory = ConversationBufferWindowMemory()\n\t        memory.ai_prefix = \"Assistant\"\n\t    template = f\"\"\"\n\t    Instructions: {instructions}\n\t    {{{memory.memory_key}}}\n\t    Human: {{human_input}}\n\t    Assistant:\"\"\"\n", "    prompt = PromptTemplate(\n\t        input_variables=[\"history\", \"human_input\"], template=template\n\t    )\n\t    chain = LLMChain(\n\t        llm=llm,\n\t        prompt=prompt,\n\t        verbose=True,\n\t        memory=ConversationBufferWindowMemory(),\n\t    )\n\t    return chain\n", "def initialize_meta_chain():\n\t    meta_template = \"\"\"\n\t    Assistant has just had the below interactions with a User. Assistant followed their \"Instructions\" closely. Your job is to critique the Assistant's performance and then revise the Instructions so that Assistant would quickly and correctly respond in the future.\n\t    ####\n\t    {chat_history}\n\t    ####\n\t    Please reflect on these interactions.\n\t    You should first critique Assistant's performance. What could Assistant have done better? What should the Assistant remember about this user? Are there things this user always wants? Indicate this with \"Critique: ...\".\n\t    You should next revise the Instructions so that Assistant would quickly and correctly respond in the future. Assistant's goal is to satisfy the user in as few interactions as possible. Assistant will only see the new Instructions, not the interaction history, so anything important must be summarized in the Instructions. Don't forget any important details in the current Instructions! Indicate the new Instructions by \"Instructions: ...\".\n\t    \"\"\"\n", "    meta_prompt = PromptTemplate(\n\t        input_variables=[\"chat_history\"], template=meta_template\n\t    )\n\t    meta_chain = LLMChain(\n\t        llm=llm,\n\t        prompt=meta_prompt,\n\t        verbose=True,\n\t    )\n\t    return meta_chain\n\tdef get_chat_history(chain_memory):\n", "    memory_key = chain_memory.memory_key\n\t    chat_history = chain_memory.load_memory_variables(memory_key)[memory_key]\n\t    return chat_history\n\tdef get_new_instructions(meta_output):\n\t    delimiter = \"Instructions: \"\n\t    new_instructions = meta_output[meta_output.find(delimiter) + len(delimiter) :]\n\t    return new_instructions\n\tdef main(task, max_iters=3, max_meta_iters=5):\n\t    failed_phrase = \"task failed\"\n\t    success_phrase = \"task succeeded\"\n", "    key_phrases = [success_phrase, failed_phrase]\n\t    instructions = \"None\"\n\t    for i in range(max_meta_iters):\n\t        print(f\"[Episode {i+1}/{max_meta_iters}]\")\n\t        chain = initialize_chain(instructions, memory=None)\n\t        output = chain.predict(human_input=task)\n\t        for j in range(max_iters):\n\t            print(f\"(Step {j+1}/{max_iters})\")\n\t            print(f\"Assistant: {output}\")\n\t            print(f\"Human: \")\n", "            human_input = input()\n\t            if any(phrase in human_input.lower() for phrase in key_phrases):\n\t                break\n\t            output = chain.predict(human_input=human_input)\n\t        if success_phrase in human_input.lower():\n\t            print(f\"You succeeded! Thanks for playing!\")\n\t            return\n\t        meta_chain = initialize_meta_chain()\n\t        meta_output = meta_chain.predict(chat_history=get_chat_history(chain.memory))\n\t        print(f\"Feedback: {meta_output}\")\n", "        instructions = get_new_instructions(meta_output)\n\t        print(f\"New Instructions: {instructions}\")\n\t        print(\"\\n\" + \"#\" * 80 + \"\\n\")\n\t    print(f\"You failed! Thanks for playing!\")\n\ttask = input(\"Enter the objective of the AI system: (Be realistic!) \")\n\tmax_iters = int(input(\"Enter the maximum number of interactions per episode: \"))\n\tmax_meta_iters = int(input(\"Enter the maximum number of episodes: \"))\n\tmain(task, max_iters, max_meta_iters)\n"]}
{"filename": "AUTOGPT.py", "chunked_list": ["# !pip install bs4\n\t# !pip install nest_asyncio\n\t# General\n\timport os\n\timport json\n\timport pandas as pd\n\tfrom dotenv import load_dotenv\n\tfrom pathlib import Path\n\tfrom json import JSONDecodeError\n\tfrom langchain.experimental.autonomous_agents.autogpt.agent import AutoGPT\n", "from FreeLLM import ChatGPTAPI  # FREE CHATGPT API\n\tfrom FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\n\tfrom FreeLLM import BingChatAPI  # FREE BINGCHAT API\n\tfrom FreeLLM import BardChatAPI  # FREE GOOGLE BARD API\n\tfrom langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\n\tfrom langchain.docstore.document import Document\n\timport asyncio\n\timport nest_asyncio\n\t# Needed synce jupyter runs an async eventloop\n\tnest_asyncio.apply()\n", "# [Optional] Set the environment variable Tokenizers_PARALLELISM to false to get rid of the warning\n\t# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\tload_dotenv()\n\tselect_model = input(\n\t    \"Select the model you want to use (1, 2, 3 or 4) \\n \\\n\t1) ChatGPT \\n \\\n\t2) HuggingChat \\n \\\n\t3) BingChat \\n \\\n\t4) Google Bard \\n \\\n\t>>> \"\n", ")\n\tif select_model == \"1\":\n\t    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\t    if CG_TOKEN != \"your-chatgpt-token\":\n\t        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n\t        )\n\t    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n", "    if os.getenv(\"USE_GPT4\") == \"True\":\n\t        model = \"gpt-4\"\n\t    else:\n\t        model = \"default\"\n\t    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\telif select_model == \"2\":\n\t    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n\t    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n\t    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n\t        os.environ[\"emailHF\"] = emailHF\n", "        os.environ[\"pswHF\"] = pswHF\n\t    else:\n\t        raise ValueError(\n\t            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n\t        )\n\t    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\telif select_model == \"3\":\n\t    if not os.path.exists(\"cookiesBing.json\"):\n\t        raise ValueError(\n\t            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n", "        )\n\t    cookie_path = Path() / \"cookiesBing.json\"\n\t    with open(\"cookiesBing.json\", \"r\") as file:\n\t        try:\n\t            file_json = json.loads(file.read())\n\t        except JSONDecodeError:\n\t            raise ValueError(\n\t                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n\t            )\n\t    llm = BingChatAPI.BingChat(\n", "        cookiepath=str(cookie_path), conversation_style=\"creative\"\n\t    )\n\telif select_model == \"4\":\n\t    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\t    if GB_TOKEN != \"your-googlebard-token\":\n\t        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n\t        )\n", "    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n\t    llm = BardChatAPI.BardChat(cookie=cookie_path)\n\tHF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n\tif HF_TOKEN != \"your-huggingface-token\":\n\t    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n\telse:\n\t    raise ValueError(\n\t        \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n\t    )\n\t# Tools\n", "import os\n\tfrom contextlib import contextmanager\n\tfrom typing import Optional\n\tfrom langchain.agents import tool\n\tfrom langchain.tools.file_management.read import ReadFileTool\n\tfrom langchain.tools.file_management.write import WriteFileTool\n\tfrom tempfile import TemporaryDirectory\n\tROOT_DIR = TemporaryDirectory()\n\t@contextmanager\n\tdef pushd(new_dir):\n", "    \"\"\"Context manager for changing the current working directory.\"\"\"\n\t    prev_dir = os.getcwd()\n\t    os.chdir(new_dir)\n\t    try:\n\t        yield\n\t    finally:\n\t        os.chdir(prev_dir)\n\t@tool\n\tdef process_csv(\n\t    csv_file_path: str, instructions: str, output_path: Optional[str] = None\n", ") -> str:\n\t    \"\"\"Process a CSV by with pandas in a limited REPL.\\\n\t Only use this after writing data to disk as a csv file.\\\n\t Any figures must be saved to disk to be viewed by the human.\\\n\t Instructions should be written in natural language, not code. Assume the dataframe is already loaded.\"\"\"\n\t    with pushd(ROOT_DIR):\n\t        try:\n\t            df = pd.read_csv(csv_file_path)\n\t        except Exception as e:\n\t            return f\"Error: {e}\"\n", "        agent = create_pandas_dataframe_agent(llm, df, max_iterations=30, verbose=True)\n\t        if output_path is not None:\n\t            instructions += f\" Save output to disk at {output_path}\"\n\t        try:\n\t            result = agent.run(instructions)\n\t            return result\n\t        except Exception as e:\n\t            return f\"Error: {e}\"\n\t# !pip install playwright\n\t# !playwright install\n", "async def async_load_playwright(url: str) -> str:\n\t    \"\"\"Load the specified URLs using Playwright and parse using BeautifulSoup.\"\"\"\n\t    from bs4 import BeautifulSoup\n\t    from playwright.async_api import async_playwright\n\t    try:\n\t        print(\">>> WARNING <<<\")\n\t        print(\n\t            \"If you are running this for the first time, you nedd to install playwright\"\n\t        )\n\t        print(\">>> AUTO INSTALLING PLAYWRIGHT <<<\")\n", "        os.system(\"playwright install\")\n\t        print(\">>> PLAYWRIGHT INSTALLED <<<\")\n\t    except:\n\t        print(\">>> PLAYWRIGHT ALREADY INSTALLED <<<\")\n\t        pass\n\t    results = \"\"\n\t    async with async_playwright() as p:\n\t        browser = await p.chromium.launch(headless=True)\n\t        try:\n\t            page = await browser.new_page()\n", "            await page.goto(url)\n\t            page_source = await page.content()\n\t            soup = BeautifulSoup(page_source, \"html.parser\")\n\t            for script in soup([\"script\", \"style\"]):\n\t                script.extract()\n\t            text = soup.get_text()\n\t            lines = (line.strip() for line in text.splitlines())\n\t            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n\t            results = \"\\n\".join(chunk for chunk in chunks if chunk)\n\t        except Exception as e:\n", "            results = f\"Error: {e}\"\n\t        await browser.close()\n\t    return results\n\tdef run_async(coro):\n\t    event_loop = asyncio.get_event_loop()\n\t    return event_loop.run_until_complete(coro)\n\t@tool\n\tdef browse_web_page(url: str) -> str:\n\t    \"\"\"Verbose way to scrape a whole webpage. Likely to cause issues parsing.\"\"\"\n\t    return run_async(async_load_playwright(url))\n", "from langchain.tools import BaseTool, DuckDuckGoSearchRun\n\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\tfrom pydantic import Field\n\tfrom langchain.chains.qa_with_sources.loading import (\n\t    load_qa_with_sources_chain,\n\t    BaseCombineDocumentsChain,\n\t)\n\tdef _get_text_splitter():\n\t    return RecursiveCharacterTextSplitter(\n\t        # Set a really small chunk size, just to show.\n", "        chunk_size=500,\n\t        chunk_overlap=20,\n\t        length_function=len,\n\t    )\n\tclass WebpageQATool(BaseTool):\n\t    name = \"query_webpage\"\n\t    description = (\n\t        \"Browse a webpage and retrieve the information relevant to the question.\"\n\t    )\n\t    text_splitter: RecursiveCharacterTextSplitter = Field(\n", "        default_factory=_get_text_splitter\n\t    )\n\t    qa_chain: BaseCombineDocumentsChain\n\t    def _run(self, url: str, question: str) -> str:\n\t        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n\t        result = browse_web_page.run(url)\n\t        docs = [Document(page_content=result, metadata={\"source\": url})]\n\t        web_docs = self.text_splitter.split_documents(docs)\n\t        results = []\n\t        # TODO: Handle this with a MapReduceChain\n", "        for i in range(0, len(web_docs), 4):\n\t            input_docs = web_docs[i : i + 4]\n\t            window_result = self.qa_chain(\n\t                {\"input_documents\": input_docs, \"question\": question},\n\t                return_only_outputs=True,\n\t            )\n\t            results.append(f\"Response from window {i} - {window_result}\")\n\t        results_docs = [\n\t            Document(page_content=\"\\n\".join(results), metadata={\"source\": url})\n\t        ]\n", "        return self.qa_chain(\n\t            {\"input_documents\": results_docs, \"question\": question},\n\t            return_only_outputs=True,\n\t        )\n\t    async def _arun(self, url: str, question: str) -> str:\n\t        raise NotImplementedError\n\tquery_website_tool = WebpageQATool(qa_chain=load_qa_with_sources_chain(llm))\n\t# Memory\n\timport faiss\n\tfrom langchain.vectorstores import FAISS\n", "from langchain.docstore import InMemoryDocstore\n\tfrom Embedding import HuggingFaceEmbedding  # EMBEDDING FUNCTION\n\tfrom langchain.tools.human.tool import HumanInputRun\n\t# Define your embedding model\n\tembeddings_model = HuggingFaceEmbedding.newEmbeddingFunction\n\tembedding_size = 1536  # if you change this you need to change also in Embedding/HuggingFaceEmbedding.py\n\tindex = faiss.IndexFlatL2(embedding_size)\n\tvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n\t# !pip install duckduckgo_search\n\tweb_search = DuckDuckGoSearchRun()\n", "tools = [\n\t    web_search,\n\t    WriteFileTool(),\n\t    ReadFileTool(),\n\t    process_csv,\n\t    query_website_tool,\n\t    # HumanInputRun(), # Activate if you want the permit asking for help from the human\n\t]\n\tagent = AutoGPT.from_llm_and_tools(\n\t    ai_name=\"BingChat\",\n", "    ai_role=\"Assistant\",\n\t    tools=tools,\n\t    llm=llm,\n\t    memory=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n\t    # human_in_the_loop=True, # Set to True if you want to add feedback at each step.\n\t)\n\t# agent.chain.verbose = True\n\tagent.run([input(\"Enter the objective of the AI system: (Be realistic!) \")])\n"]}
{"filename": "TransformersAgent.py", "chunked_list": ["from hfAgent import agents\n\tfrom dotenv import load_dotenv\n\timport os\n\timport json\n\tfrom json import JSONDecodeError\n\tfrom pathlib import Path\n\timport huggingface_hub\n\tload_dotenv()\n\tselect_model = input(\n\t    \"Select the model you want to use (1, 2, 3, 4, 5, 6) \\n \\\n", "1) ChatGPT \\n \\\n\t2) HuggingChat (NOT GOOD RESULT)\\n \\\n\t3) BingChat (NOT GOOD RESULT)\\n \\\n\t4) BardChat \\n \\\n\t5) StarCoder\\n \\\n\t6) OpenAssistant\\n \\\n\t>>> \"\n\t)\n\tif select_model == \"1\":\n\t    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n", "    if CG_TOKEN != \"your-chatgpt-token\":\n\t        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n\t        )\n\t    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n\t    if os.getenv(\"USE_GPT4\") == \"True\":\n\t        model = \"gpt-4\"\n\t    else:\n", "        model = \"default\"\n\t    agent = agents.ChatGPTAgent(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\telif select_model == \"2\":\n\t    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n\t    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n\t    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n\t        os.environ[\"emailHF\"] = emailHF\n\t        os.environ[\"pswHF\"] = pswHF\n\t    else:\n\t        raise ValueError(\n", "            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n\t        )\n\t    agent = agents.HuggingChatAgent()\n\telif select_model == \"3\":\n\t    if not os.path.exists(\"cookiesBing.json\"):\n\t        raise ValueError(\n\t            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n\t        )\n\t    cookie_path = \"cookiesBing.json\"\n\t    with open(\"cookiesBing.json\", \"r\") as file:\n", "        try:\n\t            file_json = json.loads(file.read())\n\t        except JSONDecodeError:\n\t            raise ValueError(\n\t                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n\t            )\n\t    agent = agents.BingChatAgent(cookiepath=cookie_path, conversation=\"balanced\")\n\telif select_model == \"4\":\n\t    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\t    if GB_TOKEN != \"your-googlebard-token\":\n", "        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n\t        )\n\t    cookie = os.environ[\"BARDCHAT_TOKEN\"]\n\t    agent = agents.BardChatAgent(token=cookie)\n\telif select_model == \"5\":\n\t    HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n\t    if HF_TOKEN != \"your-huggingface-token\":\n", "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n\t        huggingface_hub.login(token=HF_TOKEN)\n\t    else:\n\t        raise ValueError(\n\t            \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n\t        )\n\t    from transformers.tools import HfAgent\n\t    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\telif select_model == \"6\":\n\t    HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n", "    if HF_TOKEN != \"your-huggingface-token\":\n\t        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n\t        huggingface_hub.login(token=HF_TOKEN)\n\t    else:\n\t        raise ValueError(\n\t            \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n\t        )\n\t    from transformers.tools import HfAgent\n\t    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\t    from transformers.tools import HfAgent\n", "    agent = HfAgent(\n\t        url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n\t    )\n\tprompt = input(\">>> Input prompt:\\n>\")\n\twhile prompt != \"exit\":\n\t    agent.run(prompt, remote=True)\n\t    prompt = input(\">>> Input prompt:\\n>\")\n"]}
{"filename": "Camel.py", "chunked_list": ["from langchain.prompts.chat import (\n\t    SystemMessagePromptTemplate,\n\t    HumanMessagePromptTemplate,\n\t)\n\tfrom langchain.schema import (\n\t    AIMessage,\n\t    HumanMessage,\n\t    SystemMessage,\n\t    BaseMessage,\n\t)\n", "import os\n\timport json\n\tfrom pathlib import Path\n\tfrom json import JSONDecodeError\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\tfrom FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\n\tfrom FreeLLM import ChatGPTAPI  # FREE CHATGPT API\n\tfrom FreeLLM import BingChatAPI  # FREE BINGCHAT API\n\timport streamlit as st\n", "from streamlit_chat_media import message\n\timport os\n\tst.set_page_config(\n\t    page_title=\"FREE AUTOGPT ðŸš€ by Intelligenza Artificiale Italia\",\n\t    page_icon=\"ðŸš€\",\n\t    layout=\"wide\",\n\t    menu_items={\n\t        \"Get help\": \"https://www.intelligenzaartificialeitalia.net/\",\n\t        \"Report a bug\": \"mailto:servizi@intelligenzaartificialeitalia.net\",\n\t        \"About\": \"# *ðŸš€  FREE AUTOGPT  ðŸš€* \",\n", "    },\n\t)\n\tst.markdown(\n\t    \"<style> iframe > div {    text-align: left;} </style>\", unsafe_allow_html=True\n\t)\n\tclass CAMELAgent:\n\t    def __init__(\n\t        self,\n\t        system_message: SystemMessage,\n\t        model: None,\n", "    ) -> None:\n\t        self.system_message = system_message.content\n\t        self.model = model\n\t        self.init_messages()\n\t    def reset(self) -> None:\n\t        self.init_messages()\n\t        return self.stored_messages\n\t    def init_messages(self) -> None:\n\t        self.stored_messages = [self.system_message]\n\t    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\n", "        self.stored_messages.append(message)\n\t        return self.stored_messages\n\t    def step(\n\t        self,\n\t        input_message: HumanMessage,\n\t    ) -> AIMessage:\n\t        messages = self.update_messages(input_message)\n\t        output_message = self.model(str(input_message.content))\n\t        self.update_messages(output_message)\n\t        print(f\"AI Assistant:\\n\\n{output_message}\\n\\n\")\n", "        return output_message\n\tcol1, col2 = st.columns(2)\n\tassistant_role_name = col1.text_input(\"Assistant Role Name\", \"Python Programmer\")\n\tuser_role_name = col2.text_input(\"User Role Name\", \"Stock Trader\")\n\ttask = st.text_area(\"Task\", \"Develop a trading bot for the stock market\")\n\tword_limit = st.number_input(\"Word Limit\", 10, 1500, 50)\n\tif not os.path.exists(\"cookiesHuggingChat.json\"):\n\t    raise ValueError(\n\t        \"File 'cookiesHuggingChat.json' not found! Create it and put your cookies in there in the JSON format.\"\n\t    )\n", "cookie_path = Path() / \"cookiesHuggingChat.json\"\n\twith open(\"cookiesHuggingChat.json\", \"r\") as file:\n\t    try:\n\t        file_json = json.loads(file.read())\n\t    except JSONDecodeError:\n\t        raise ValueError(\n\t            \"You did not put your cookies inside 'cookiesHuggingChat.json'! You can find the simple guide to get the cookie file here: https://github.com/IntelligenzaArtificiale/Free-Auto-GPT\"\n\t        )  \n\tllm = HuggingChatAPI.HuggingChat(cookiepath = str(cookie_path))\n\tif st.button(\"Start Autonomus AI AGENT\"):\n", "    task_specifier_sys_msg = SystemMessage(content=\"You can make a task more specific.\")\n\t    task_specifier_prompt = \"\"\"Here is a task that {assistant_role_name} will help {user_role_name} to complete: {task}.\n\t    Please make it more specific. Be creative and imaginative.\n\t    Please reply with the specified task in {word_limit} words or less. Do not add anything else.\"\"\"\n\t    task_specifier_template = HumanMessagePromptTemplate.from_template(\n\t        template=task_specifier_prompt\n\t    )\n\t    task_specify_agent = CAMELAgent(\n\t        task_specifier_sys_msg, llm\n\t    )\n", "    task_specifier_msg = task_specifier_template.format_messages(\n\t        assistant_role_name=assistant_role_name,\n\t        user_role_name=user_role_name,\n\t        task=task,\n\t        word_limit=word_limit,\n\t    )[0]\n\t    specified_task_msg = task_specify_agent.step(task_specifier_msg)\n\t    print(f\"Specified task: {specified_task_msg}\")\n\t    message(\n\t        f\"Specified task: {specified_task_msg}\",\n", "        allow_html=True,\n\t        key=\"specified_task\",\n\t        avatar_style=\"adventurer\",\n\t    )\n\t    specified_task = specified_task_msg\n\t    # messages.py\n\t    from langchain.prompts.chat import (\n\t        SystemMessagePromptTemplate,\n\t        HumanMessagePromptTemplate,\n\t    )\n", "    assistant_inception_prompt = \"\"\"Never forget you are a {assistant_role_name} and I am a {user_role_name}. Never flip roles! Never instruct me!\n\t    We share a common interest in collaborating to successfully complete a task.\n\t    You must help me to complete the task.\n\t    Here is the task: {task}. Never forget our task and to focus only to complete the task do not add anything else!\n\t    I must instruct you based on your expertise and my needs to complete the task.\n\t    I must give you one instruction at a time.\n\t    It is important that when the . \"{task}\" is completed, you need to tell {user_role_name} that the task has completed and to stop!\n\t    You must write a specific solution that appropriately completes the requested instruction.\n\t    Do not add anything else other than your solution to my instruction.\n\t    You are never supposed to ask me any questions you only answer questions.\n", "    REMEMBER NEVER INSTRUCT ME! \n\t    Your solution must be declarative sentences and simple present tense.\n\t    Unless I say the task is completed, you should always start with:\n\t    Solution: <YOUR_SOLUTION>\n\t    <YOUR_SOLUTION> should be specific and provide preferable implementations and examples for task-solving.\n\t    Always end <YOUR_SOLUTION> with: Next request.\"\"\"\n\t    user_inception_prompt = \"\"\"Never forget you are a {user_role_name} and I am a {assistant_role_name}. Never flip roles! You will always instruct me.\n\t    We share a common interest in collaborating to successfully complete a task.\n\t    I must help you to complete the task.\n\t    Here is the task: {task}. Never forget our task!\n", "    You must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n\t    1. Instruct with a necessary input:\n\t    Instruction: <YOUR_INSTRUCTION>\n\t    Input: <YOUR_INPUT>\n\t    2. Instruct without any input:\n\t    Instruction: <YOUR_INSTRUCTION>\n\t    Input: None\n\t    The \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n\t    You must give me one instruction at a time.\n\t    I must write a response that appropriately completes the requested instruction.\n", "    I must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\n\t    You should instruct me not ask me questions.\n\t    Now you must start to instruct me using the two ways described above.\n\t    Do not add anything else other than your instruction and the optional corresponding input!\n\t    Keep giving me instructions and necessary inputs until you think the task is completed.\n\t    It's Important wich when the task . \"{task}\" is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\n\t    Never say <CAMEL_TASK_DONE> unless my responses have solved your task!\n\t    It's Important wich when the task . \"{task}\" is completed, you must only reply with a single word <CAMEL_TASK_DONE>\"\"\"\n\t    def get_sys_msgs(assistant_role_name: str, user_role_name: str, task: str):\n\t        assistant_sys_template = SystemMessagePromptTemplate.from_template(\n", "            template=assistant_inception_prompt\n\t        )\n\t        assistant_sys_msg = assistant_sys_template.format_messages(\n\t            assistant_role_name=assistant_role_name,\n\t            user_role_name=user_role_name,\n\t            task=task,\n\t        )[0]\n\t        user_sys_template = SystemMessagePromptTemplate.from_template(\n\t            template=user_inception_prompt\n\t        )\n", "        user_sys_msg = user_sys_template.format_messages(\n\t            assistant_role_name=assistant_role_name,\n\t            user_role_name=user_role_name,\n\t            task=task,\n\t        )[0]\n\t        return assistant_sys_msg, user_sys_msg\n\t    # define the role system messages\n\t    assistant_sys_msg, user_sys_msg = get_sys_msgs(\n\t        assistant_role_name, user_role_name, specified_task\n\t    )\n", "    # AI ASSISTANT setup                           |-> add the agent LLM MODEL HERE <-|\n\t    assistant_agent = CAMELAgent(assistant_sys_msg, llm)\n\t    # AI USER setup                      |-> add the agent LLM MODEL HERE <-|\n\t    user_agent = CAMELAgent(user_sys_msg, llm)\n\t    # Reset agents\n\t    assistant_agent.reset()\n\t    user_agent.reset()\n\t    # Initialize chats\n\t    assistant_msg = HumanMessage(\n\t        content=(\n", "            f\"{user_sys_msg}. \"\n\t            \"Now start to give me introductions one by one. \"\n\t            \"Only reply with Instruction and Input.\"\n\t        )\n\t    )\n\t    user_msg = HumanMessage(content=f\"{assistant_sys_msg.content}\")\n\t    user_msg = assistant_agent.step(user_msg)\n\t    message(\n\t        f\"AI Assistant ({assistant_role_name}):\\n\\n{user_msg}\\n\\n\",\n\t        is_user=False,\n", "        allow_html=True,\n\t        key=\"0_assistant\",\n\t        avatar_style=\"pixel-art\",\n\t    )\n\t    print(f\"Original task prompt:\\n{task}\\n\")\n\t    print(f\"Specified task prompt:\\n{specified_task}\\n\")\n\t    chat_turn_limit, n = 30, 0\n\t    while n < chat_turn_limit:\n\t        n += 1\n\t        user_ai_msg = user_agent.step(assistant_msg)\n", "        user_msg = HumanMessage(content=user_ai_msg)\n\t        # print(f\"AI User ({user_role_name}):\\n\\n{user_msg}\\n\\n\")\n\t        message(\n\t            f\"AI User ({user_role_name}):\\n\\n{user_msg.content}\\n\\n\",\n\t            is_user=True,\n\t            allow_html=True,\n\t            key=str(n) + \"_user\",\n\t        )\n\t        assistant_ai_msg = assistant_agent.step(user_msg)\n\t        assistant_msg = HumanMessage(content=assistant_ai_msg)\n", "        # print(f\"AI Assistant ({assistant_role_name}):\\n\\n{assistant_msg}\\n\\n\")\n\t        message(\n\t            f\"AI Assistant ({assistant_role_name}):\\n\\n{assistant_msg.content}\\n\\n\",\n\t            is_user=False,\n\t            allow_html=True,\n\t            key=str(n) + \"_assistant\",\n\t            avatar_style=\"pixel-art\",\n\t        )\n\t        if (\n\t            \"<CAMEL_TASK_DONE>\" in user_msg.content\n", "            or \"task  completed\" in user_msg.content\n\t        ):\n\t            message(\"Task completed!\", allow_html=True, key=\"task_done\")\n\t            break\n\t        if \"Error\" in user_msg.content:\n\t            message(\"Task failed!\", allow_html=True, key=\"task_failed\")\n\t            break\n"]}
{"filename": "BabyAgi/task_creation.py", "chunked_list": ["from langchain import LLMChain, PromptTemplate\n\tfrom langchain.base_language import BaseLanguageModel\n\tclass TaskCreationChain(LLMChain):\n\t    \"\"\"Chain to generates tasks.\"\"\"\n\t    @classmethod\n\t    def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:\n\t        \"\"\"Get the response parser.\"\"\"\n\t        task_creation_template = (\n\t            \"Can you hel me to\"\n\t            \" to create new tasks with the following objective: {objective},\"\n", "            \" The last completed task has the result: {result}.\"\n\t            \" This result was based on this task description: {task_description}.\"\n\t            \" These are incomplete tasks: {incomplete_tasks}.\"\n\t            \" Based on the result, create new tasks to be completed\"\n\t            \" Return the task as an List without anything else.\"\n\t        )\n\t        prompt = PromptTemplate(\n\t            template=task_creation_template,\n\t            input_variables=[\n\t                \"result\",\n", "                \"task_description\",\n\t                \"incomplete_tasks\",\n\t                \"objective\",\n\t            ],\n\t        )\n\t        return cls(prompt=prompt, llm=llm, verbose=verbose)\n"]}
{"filename": "BabyAgi/BabyAGIMod.py", "chunked_list": ["\"\"\"BabyAGI agent.\"\"\"\n\tfrom collections import deque\n\tfrom typing import Any, Dict, List, Optional\n\tfrom pydantic import BaseModel, Field\n\tfrom langchain import LLMChain, PromptTemplate\n\tfrom langchain.base_language import BaseLanguageModel\n\tfrom langchain.callbacks.manager import CallbackManagerForChainRun\n\tfrom langchain.chains.base import Chain\n\tfrom langchain.vectorstores.base import VectorStore\n\timport os\n", "from .task_creation import TaskCreationChain\n\tfrom .task_execution import TaskExecutionChain\n\tfrom .task_prioritization import TaskPrioritizationChain\n\tclass BabyAGI(Chain, BaseModel):\n\t    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n\t    task_list: deque = Field(default_factory=deque)\n\t    task_creation_chain: Chain = Field(...)\n\t    task_prioritization_chain: Chain = Field(...)\n\t    execution_chain: Chain = Field(...)\n\t    task_id_counter: int = Field(1)\n", "    vectorstore: VectorStore = Field(init=False)\n\t    max_iterations: Optional[int] = None\n\t    store: Optional[bool] = False\n\t    write_step: Optional[int] = 0\n\t    class Config:\n\t        \"\"\"Configuration for this pydantic object.\"\"\"\n\t        arbitrary_types_allowed = True\n\t    def add_task(self, task: Dict) -> None:\n\t        self.task_list.append(task)\n\t    def print_task_list(self) -> None:\n", "        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n\t        for t in self.task_list:\n\t            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n\t    def print_next_task(self, task: Dict) -> None:\n\t        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n\t        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n\t    def print_task_result(self, result: str) -> None:\n\t        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n\t        print(result)\n\t    @property\n", "    def input_keys(self) -> List[str]:\n\t        return [\"objective\"]\n\t    @property\n\t    def output_keys(self) -> List[str]:\n\t        return []\n\t    def get_next_task(\n\t        self, result: str, task_description: str, objective: str\n\t    ) -> List[Dict]:\n\t        \"\"\"Get the next task.\"\"\"\n\t        task_names = [t[\"task_name\"] for t in self.task_list]\n", "        incomplete_tasks = \", \".join(task_names)\n\t        response = self.task_creation_chain.run(\n\t            result=result,\n\t            task_description=task_description,\n\t            incomplete_tasks=incomplete_tasks,\n\t            objective=objective,\n\t        )\n\t        new_tasks = response.split(\"\\n\")\n\t        return [\n\t            {\"task_name\": task_name} for task_name in new_tasks if task_name.strip()\n", "        ]\n\t    def prioritize_tasks(self, this_task_id: int, objective: str) -> List[Dict]:\n\t        \"\"\"Prioritize tasks.\"\"\"\n\t        task_names = [t[\"task_name\"] for t in list(self.task_list)]\n\t        next_task_id = int(this_task_id) + 1\n\t        response = self.task_prioritization_chain.run(\n\t            task_names=\", \".join(task_names),\n\t            next_task_id=str(next_task_id),\n\t            objective=objective,\n\t        )\n", "        new_tasks = response.split(\"\\n\")\n\t        prioritized_task_list = []\n\t        for task_string in new_tasks:\n\t            if not task_string.strip():\n\t                continue\n\t            task_parts = task_string.strip().split(\".\", 1)\n\t            if len(task_parts) == 2:\n\t                task_id = task_parts[0].strip()\n\t                task_name = task_parts[1].strip()\n\t                prioritized_task_list.append(\n", "                    {\"task_id\": task_id, \"task_name\": task_name}\n\t                )\n\t        return prioritized_task_list\n\t    def _get_top_tasks(self, query: str, k: int) -> List[str]:\n\t        \"\"\"Get the top k tasks based on the query.\"\"\"\n\t        results = self.vectorstore.similarity_search(query, k=k)\n\t        if not results:\n\t            return []\n\t        return [str(item.metadata[\"task\"]) for item in results]\n\t    def execute_task(self, objective: str, task: str, k: int = 5) -> str:\n", "        \"\"\"Execute a task.\"\"\"\n\t        context = self._get_top_tasks(query=objective, k=k)\n\t        return self.execution_chain.run(\n\t            objective=objective, context=\"\\n\".join(context), task=task\n\t        )\n\t    def _call(\n\t        self,\n\t        inputs: Dict[str, Any],\n\t        run_manager: Optional[CallbackManagerForChainRun] = None,\n\t    ) -> Dict[str, Any]:\n", "        \"\"\"Run the agent.\"\"\"\n\t        objective = inputs[\"objective\"]\n\t        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n\t        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n\t        num_iters = 0\n\t        dir_name=\"\"\n\t        if self.store:\n\t            try:    \n\t                # create a directory to store the results of evry task\n\t                os.mkdir(\"BABYAGI_RESULTS_FOR_\" + objective.replace(\" \", \"_\"))\n", "                dir_name = \"BABYAGI_RESULTS_FOR_\" + objective.replace(\" \", \"_\")\n\t                self.write_step = 0\n\t            except:\n\t                print(\"ATTENTION: directory already exists, Delete the directory to store the results of evry task\")\n\t                self.store = False\n\t        while True:\n\t            if self.task_list:\n\t                self.print_task_list()\n\t                # Step 1: Pull the first task\n\t                task = self.task_list.popleft()\n", "                self.print_next_task(task)\n\t                # Step 2: Execute the task\n\t                result = self.execute_task(objective, task[\"task_name\"])\n\t                this_task_id = int(task[\"task_id\"])  # THIS LINE GIVE ERROR  WOLUD BE FIXED\n\t                self.print_task_result(result)\n\t                if self.store:\n\t                    # save the result in a file\n\t                    self.write_step += 1\n\t                    with open(dir_name + \"/\" + str(self.write_step) + \".txt\", \"w\") as f:\n\t                        f.write(result)\n", "                    print(\"<<BABY AGI>> : result saved in \" + dir_name + \"/\" + str(self.write_step) +  \".txt\")\n\t                # Step 3: Store the result in Pinecone\n\t                result_id = f\"result_{task['task_id']}\"\n\t                self.vectorstore.add_texts(\n\t                    texts=[result],\n\t                    metadatas=[{\"task\": task[\"task_name\"]}],\n\t                    ids=[result_id],\n\t                )\n\t                # Step 4: Create new tasks and reprioritize task list\n\t                new_tasks = self.get_next_task(result, task[\"task_name\"], objective)\n", "                for new_task in new_tasks:\n\t                    self.task_id_counter += 1\n\t                    new_task.update({\"task_id\": self.task_id_counter})\n\t                    self.add_task(new_task)\n\t                self.task_list = deque(self.prioritize_tasks(this_task_id, objective))\n\t            num_iters += 1\n\t            if self.max_iterations is not None and num_iters == self.max_iterations:\n\t                print(\n\t                    \"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\"\n\t                )\n", "                if self.store:\n\t                    #create final file to append in order by write_step \n\t                    final_file = open(dir_name + \"/\" + \"final.txt\", \"w\")    \n\t                    all_step = os.listdir(dir_name)\n\t                    all_step.sort()\n\t                    for step in all_step:\n\t                        #append the result of each step in the final file\n\t                        with open(dir_name + \"/\" + step, \"r\") as f:\n\t                            final_file.write(f.read())\n\t                    final_file.close()\n", "                    print(\n\t                        \"\\033[91m\\033[1m\" + \"\\n*****RESULT STORED*****\\n\" + \"\\033[0m\\033[0m\"\n\t                    )\n\t                break\n\t        return {}\n\t    @classmethod\n\t    def from_llm(\n\t        cls,\n\t        llm: BaseLanguageModel,\n\t        vectorstore: VectorStore,\n", "        verbose: bool = False,\n\t        task_execution_chain: Optional[Chain] = None,\n\t        **kwargs: Dict[str, Any],\n\t    ) -> \"BabyAGI\":\n\t        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n\t        task_creation_chain = TaskCreationChain.from_llm(llm, verbose=verbose)\n\t        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n\t            llm, verbose=verbose\n\t        )\n\t        if task_execution_chain is None:\n", "            execution_chain: Chain = TaskExecutionChain.from_llm(llm, verbose=verbose)\n\t        else:\n\t            execution_chain = task_execution_chain\n\t        return cls(\n\t            task_creation_chain=task_creation_chain,\n\t            task_prioritization_chain=task_prioritization_chain,\n\t            execution_chain=execution_chain,\n\t            vectorstore=vectorstore,\n\t            **kwargs,\n\t        )\n"]}
{"filename": "BabyAgi/task_prioritization.py", "chunked_list": ["from langchain import LLMChain, PromptTemplate\n\tfrom langchain.base_language import BaseLanguageModel\n\tclass TaskPrioritizationChain(LLMChain):\n\t    \"\"\"Chain to prioritize tasks.\"\"\"\n\t    @classmethod\n\t    def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:\n\t        \"\"\"Get the response parser.\"\"\"\n\t        task_prioritization_template = (\n\t            \"Please help me to cleaning the formatting of \"\n\t            \"and reprioritizing the following tasks: {task_names}.\"\n", "            \"Consider the ultimate objective of your team: {objective}.\"\n\t            \"Do not remove any tasks. Return ONLY the result as a numbered list without anything else, like:\\n\"\n\t            \"1. First task\\n\"\n\t            \"2. Second task\\n\"\n\t            \"Start the task list with number {next_task_id}.\"\n\t        )\n\t        prompt = PromptTemplate(\n\t            template=task_prioritization_template,\n\t            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n\t        )\n", "        return cls(prompt=prompt, llm=llm, verbose=verbose)\n"]}
{"filename": "BabyAgi/task_execution.py", "chunked_list": ["from langchain import LLMChain, PromptTemplate\n\tfrom langchain.base_language import BaseLanguageModel\n\tclass TaskExecutionChain(LLMChain):\n\t    \"\"\"Chain to execute tasks.\"\"\"\n\t    @classmethod\n\t    def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:\n\t        \"\"\"Get the response parser.\"\"\"\n\t        execution_template = (\n\t            \"Can you help me to performs one task based on the following objective: \"\n\t            \"{objective}.\"\n", "            \"Take into account these previously completed tasks: {context}.\"\n\t            \"Can you perform this task? Your task: {task}. Response:\"\n\t        )\n\t        prompt = PromptTemplate(\n\t            template=execution_template,\n\t            input_variables=[\"objective\", \"context\", \"task\"],\n\t        )\n\t        return cls(prompt=prompt, llm=llm, verbose=verbose)"]}
{"filename": "FreeLLM/HuggingChatAPI.py", "chunked_list": ["from hugchat import hugchat\n\tfrom hugchat.login import Login\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\tfrom time import sleep\n\tclass HuggingChat(LLM):\n\t    history_data: Optional[List] = []\n\t    chatbot : Optional[hugchat.ChatBot] = None\n\t    conversation : Optional[str] = \"\"\n\t    email : Optional[str]\n", "    psw : Optional[str]\n\t    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n\t        #token is a must check\n", "        if self.chatbot is None:\n\t            if self.email is None and self.psw is None:\n\t                ValueError(\"Email and Password is required, pls check the documentation on github\")\n\t            else: \n\t                if self.conversation == \"\":\n\t                    sign = Login(self.email, self.psw)\n\t                    cookies = sign.login()\n\t                    # Save cookies to usercookies/<email>.json\n\t                    sign.saveCookies()\n\t                    # Create a ChatBot\n", "                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n\t                else:\n\t                    raise ValueError(\"Something went wrong\")\n\t        sleep(2)\n\t        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n\t        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n\t        return data\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n", "        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"HuggingCHAT\"}\n\t#llm = HuggingChat(email = \"YOUR-COOKIES-PATH\" , psw ) #for start new chat\n\t#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "FreeLLM/ChatGPTAPI.py", "chunked_list": ["from gpt4_openai import GPT4OpenAI\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\tfrom time import sleep\n\tclass ChatGPT(LLM):\n\t    history_data: Optional[List] = []\n\t    token : Optional[str]\n\t    chatbot : Optional[GPT4OpenAI] = None\n\t    call : int = 0\n\t    model : str = \"gpt-3\" # or gpt-4\n", "    plugin_id : Optional[List] = []\n\t    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n\t        #token is a must check\n", "        if self.chatbot is None:\n\t            if self.token is None:\n\t                raise ValueError(\"Need a token , check https://chat.openai.com/api/auth/session for get your token\")\n\t            else:\n\t                try:\n\t                    if self.plugin_id == []:\n\t                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model)\n\t                    else:\n\t                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model, plugin_ids=self.plugin_id)\n\t                except:\n", "                    raise ValueError(\"Error on create chatbot, check your token, or your model\")\n\t        response = \"\"\n\t        # OpenAI: 50 requests / hour for each account\n\t        if (self.call >= 45 and self.model == \"default\") or (self.call >= 23 and self.model == \"gpt4\"):\n\t            raise ValueError(\"You have reached the maximum number of requests per hour ! Help me to Improve. Abusing this tool is at your own risk\")\n\t        else:\n\t            sleep(2)\n\t            response = self.chatbot(prompt)\n\t            self.call += 1\n\t        #add to history\n", "        self.history_data.append({\"prompt\":prompt,\"response\":response})    \n\t        return response\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"ChatGPT\", \"token\": self.token, \"model\": self.model}\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\") #for start new chat\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\" , model=\"gpt4\") # REQUIRED CHATGPT PLUS subscription\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\", conversation = \"Add-XXXX-XXXX-Convesation-ID\") #for use a chat already started\n\t#print(llm(\"Hello, how are you?\"))\n", "#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "FreeLLM/BardChatAPI.py", "chunked_list": ["from Bard import Chatbot\n\timport asyncio\n\timport requests\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\timport pydantic\n\timport os\n\tfrom langchain import PromptTemplate, LLMChain\n\tfrom time import sleep\n\tclass BardChat(LLM):\n", "    history_data: Optional[List] = []\n\t    cookie : Optional[str]\n\t    chatbot : Optional[Chatbot] = None\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n", "        #cookie is a must check\n\t        if self.chatbot is None:\n\t            if self.cookie is None:\n\t                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n\t            else:\n\t                #if self.chatbot == None:\n\t                self.chatbot = Chatbot(self.cookie)\n\t        response = self.chatbot.ask(prompt)\n\t        #print(response)\n\t        response_text = response['content']\n", "        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n\t        return response_text\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        return asyncio.run(self.call(prompt=prompt))\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"BardCHAT\", \"cookie\": self.cookie}\n\t#llm = BardChat(cookie = \"YOURCOOKIE\") #for start new chat\n", "#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "FreeLLM/BingChatAPI.py", "chunked_list": ["from EdgeGPT import Chatbot, ConversationStyle\n\timport asyncio\n\timport requests\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\timport pydantic\n\timport os\n\tfrom langchain import PromptTemplate, LLMChain\n\tfrom time import sleep\n\tclass BingChat(LLM):\n", "    history_data: Optional[List] = []\n\t    cookiepath : Optional[str]\n\t    chatbot : Optional[Chatbot] = None\n\t    conversation_style : Optional[str] \n\t    conversation_style_on : Optional[ConversationStyle] = ConversationStyle.precise\n\t    search_result : Optional[bool] = False\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def select_conversation(self, conversation_style: str):\n", "        if conversation_style == \"precise\":\n\t            self.conversation_style_on = ConversationStyle.precise\n\t        elif conversation_style == \"creative\":\n\t            self.conversation_style_on = ConversationStyle.creative\n\t        elif conversation_style == \"balanced\":\n\t            self.conversation_style_on = ConversationStyle.balanced\n\t        else:\n\t            raise ValueError(\"conversation_style must be precise, creative or balaced\")\n\t        self.conversation_style = conversation_style\n\t    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n", "        if stop is not None:\n\t            raise ValueError(\"stop kwargs are not permitted.\")\n\t        #cookiepath is a must check\n\t        if self.chatbot is None:\n\t            if self.cookiepath is None:\n\t                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n\t            else:\n\t                #if self.chatbot == None:\n\t                self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n\t        if self.conversation_style is not None:\n", "            self.conversation_style_on = self.select_conversation(self.conversation_style)\n\t        response = await self.chatbot.ask(prompt=prompt, conversation_style=self.conversation_style, search_result=self.search_result)\n\t        \"\"\"\n\t        this is a sample response. \n\t        {'type': 2, 'invocationId': '0', \n\t        'item': {'messages': [{'text': 'Hello, how are you?', 'author': 'user', 'from': {'id': '985157152860707', 'name': None}, 'createdAt': '2023-05-03T19:51:39.5491558+00:00', 'timestamp': '2023-05-03T19:51:39.5455787+00:00', 'locale': 'en-us', 'market': 'en-us', 'region': 'us', 'messageId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'nlu': {'scoredClassification': {'classification': 'CHAT_GPT', 'score': None}, 'classificationRanking': [{'classification': 'CHAT_GPT', 'score': None}], 'qualifyingClassifications': None, 'ood': None, 'metaData': None, 'entities': None}, 'offense': 'None', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'cib', 'privacy': None, 'inputMethod': 'Keyboard'}, {'text': \"Hello! I'm doing well, thank you. How can I assist you today?\", 'author': 'bot', 'createdAt': '2023-05-03T19:51:41.5176164+00:00', 'timestamp': '2023-05-03T19:51:41.5176164+00:00', 'messageId': '1d013e71-408b-4031-a131-2f5c009fe938', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'offense': 'None', 'adaptiveCards': [{'type': 'AdaptiveCard', 'version': '1.0', 'body': [{'type': 'TextBlock', 'text': \"Hello! I'm doing well, thank you. How can I assist you today?\\n\", 'wrap': True}]}], \n\t        'sourceAttributions': [], \n\t        'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, \n\t        'contentOrigin': 'DeepLeo', \n\t        'privacy': None, \n", "        'suggestedResponses': [{'text': 'What is the weather like today?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502696+00:00', 'timestamp': '2023-05-03T19:51:42.7502696+00:00', 'messageId': 'cd7a84d3-f9bc-47ff-9897-077b2de12e21', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'What is the latest news?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502739+00:00', 'timestamp': '2023-05-03T19:51:42.7502739+00:00', 'messageId': 'b611632a-9a8e-42de-86eb-8eb3b7b8ddbb', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'Tell me a joke.', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502743+00:00', 'timestamp': '2023-05-03T19:51:42.7502743+00:00', 'messageId': '70232e45-d7e8-4d77-83fc-752b3cd3355c', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}], 'spokenText': 'How can I assist you today?'}], 'firstNewMessageIndex': 1, 'defaultChatName': None, 'conversationId': '51D|BingProd|3E1274E188350D7BE273FFE95E02DD2984DAB52F95260300D0A2937162F98FDA', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'conversationExpiryTime': '2023-05-04T01:51:42.8260286Z', 'shouldInitiateConversation': True, 'telemetry': {'metrics': None, 'startTime': '2023-05-03T19:51:39.5456555Z'}, 'throttling': {'maxNumUserMessagesInConversation': 20, 'numUserMessagesInConversation': 1}, 'result': {'value': 'Success', 'serviceVersion': '20230501.30'}}}\n\t        \"\"\"\n\t        response_messages = response.get(\"item\", {}).get(\"messages\", [])\n\t        response_text = response_messages[1].get(\"text\", \"\")\n\t        if response_text == \"\":\n\t            hidden_text = response_messages[1].get(\"hiddenText\", \"\")\n\t            print(\">>>> [DEBBUGGER] hidden_text = \" + str(hidden_text) + \" [DEBBUGGER] <<<<\")\n\t            print(\">>>> [DEBBUGGER] BING CHAT dont is open Like CHATGPT , BingCHAT have refused to respond. [DEBBUGGER] <<<<\")\n\t            response_text = hidden_text\n\t            \"\"\"\n", "            # reset the chatbot and remake the call\n\t            print(\"[DEBUGGER] Chatbot failed to respond. Resetting and trying again. [DEBUGGER]\")\n\t            print(\"[ INFO DEBUGGER ] \\n<Response>\\n\" + str(response) + \"\\n</Response>\\n\\n\")\n\t            sleep(10)\n\t            self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n\t            sleep(2)\n\t            response = await self.chatbot.ask(prompt=prompt)\n\t            response_messages = response.get(\"item\", {}).get(\"messages\", [])\n\t            response_text = response_messages[1].get(\"text\", \"\")\n\t            \"\"\"\n", "        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n\t        return response_text\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        return asyncio.run(self.call(prompt=prompt))\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"BingCHAT\", \"cookiepath\": self.cookiepath}\n\t#llm = BingChat(cookiepath = \"YOUR-COOKIE\") #for start new chat\n", "#llm = BingChat(cookiepath = \"YOUR-COOKIE\", conversation_style = \"precise\") #precise, creative or balaced\n\t#llm = BingChat(cookiepath = \"YOUR-COOKIE\" , conversation_style = \"precise\" , search_result=True) #with web access\n\t#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "OtherAgent/pythonAgent.py", "chunked_list": ["import json\n\tfrom pathlib import Path\n\tfrom json import JSONDecodeError\n\tfrom langchain.agents.agent_toolkits import create_python_agent\n\tfrom langchain.tools.python.tool import PythonREPLTool\n\tfrom FreeLLM import ChatGPTAPI # FREE CHATGPT API\n\tfrom FreeLLM import HuggingChatAPI # FREE HUGGINGCHAT API\n\tfrom FreeLLM import BingChatAPI # FREE BINGCHAT API\n\tfrom FreeLLM import BardChatAPI # FREE GOOGLE BARD API\n\timport os\n", "#### LOG IN FOR CHATGPT FREE LLM\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tselect_model = input(\"Select the model you want to use (1, 2, 3 or 4) \\n \\\n\t1) ChatGPT \\n \\\n\t2) HuggingChat \\n \\\n\t3) BingChat \\n \\\n\t4) Google Bard \\n \\\n\t>>> \")\n\tif select_model == \"1\":\n", "    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\t    if CG_TOKEN != \"your-chatgpt-token\":\n\t        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n\t        )\n\t    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n\t    if os.getenv(\"USE_GPT4\") == \"True\":\n\t        model = \"gpt-4\"\n", "    else:\n\t        model = \"default\"\n\t    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\telif select_model == \"2\":\n\t    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n\t    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n\t    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n\t        os.environ[\"emailHF\"] = emailHF\n\t        os.environ[\"pswHF\"] = pswHF\n\t    else:\n", "        raise ValueError(\n\t            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n\t        )\n\t    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\telif select_model == \"3\":\n\t    if not os.path.exists(\"cookiesBing.json\"):\n\t        raise ValueError(\"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\")\n\t    cookie_path = Path() / \"cookiesBing.json\"\n\t    with open(\"cookiesBing.json\", 'r') as file:\n\t        try:\n", "            file_json = json.loads(file.read())\n\t        except JSONDecodeError:\n\t            raise ValueError(\"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\")\n\t    llm=BingChatAPI.BingChat(cookiepath=str(cookie_path), conversation_style=\"creative\")\n\telif select_model == \"4\":\n\t    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\t    if GB_TOKEN != \"your-googlebard-token\":\n\t        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n\t    else:\n\t        raise ValueError(\"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\")\n", "    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n\t    llm=BardChatAPI.BardChat(cookie=cookie_path)\n\t####\n\tagent_executor = create_python_agent(\n\t    llm=llm,\n\t    tool=PythonREPLTool(),\n\t    verbose=True\n\t)\n\t#todo : ADD MEMORY\n\tprint(\">> START Python AGENT\")\n", "print(\"> Digit 'exit' for exit or 'your task or question' for start\\n\\n\")\n\tprompt = input(\"(Enter your task or question) >> \")\n\twhile prompt != \"exit\":\n\t    agent_executor.run(prompt)\n\t    prompt = input(\"(Enter your task or question) >> \")\n"]}
{"filename": "OtherAgent/customAgent.py", "chunked_list": ["import json\n\tfrom pathlib import Path\n\tfrom json import JSONDecodeError\n\tfrom langchain.agents import initialize_agent\n\tfrom langchain.utilities import PythonREPL\n\tfrom langchain.utilities import WikipediaAPIWrapper\n\tfrom langchain.tools import BaseTool, DuckDuckGoSearchRun\n\tfrom langchain.tools.human.tool import HumanInputRun\n\tfrom FreeLLM import ChatGPTAPI # FREE CHATGPT API\n\tfrom FreeLLM import HuggingChatAPI # FREE HUGGINGCHAT API\n", "from FreeLLM import BingChatAPI # FREE BINGCHAT API\n\tfrom FreeLLM import BardChatAPI # FREE Google BArd API\n\tfrom langchain.agents import initialize_agent, Tool\n\timport os\n\t#### LOG IN FOR CHATGPT FREE LLM\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tselect_model = input(\"Select the model you want to use (1, 2, 3 or 4) \\n \\\n\t1) ChatGPT \\n \\\n\t2) HuggingChat \\n \\\n", "3) BingChat \\n \\\n\t4) Google Bard \\n \\\n\t>>> \")\n\tif select_model == \"1\":\n\t    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\t    if CG_TOKEN != \"your-chatgpt-token\":\n\t        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n", "        )\n\t    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n\t    if os.getenv(\"USE_GPT4\") == \"True\":\n\t        model = \"gpt-4\"\n\t    else:\n\t        model = \"default\"\n\t    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\telif select_model == \"2\":\n\t    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n\t    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n", "    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n\t        os.environ[\"emailHF\"] = emailHF\n\t        os.environ[\"pswHF\"] = pswHF\n\t    else:\n\t        raise ValueError(\n\t            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n\t        )\n\t    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\telif select_model == \"3\":\n\t    if not os.path.exists(\"cookiesBing.json\"):\n", "        raise ValueError(\"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\")\n\t    cookie_path = Path() / \"cookiesBing.json\"\n\t    with open(\"cookiesBing.json\", 'r') as file:\n\t        try:\n\t            file_json = json.loads(file.read())\n\t        except JSONDecodeError:\n\t            raise ValueError(\"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\")\n\t    llm=BingChatAPI.BingChat(cookiepath=str(cookie_path), conversation_style=\"creative\")\n\telif select_model == \"4\":\n\t    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n", "    if GB_TOKEN != \"your-googlebard-token\":\n\t        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n\t    else:\n\t        raise ValueError(\"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\")\n\t    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n\t    llm=BardChatAPI.BardChat(cookie=cookie_path)\n\t####\n\twikipedia = WikipediaAPIWrapper()\n\tpython_repl = PythonREPL()\n\tsearch = DuckDuckGoSearchRun()\n", "#from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain, BaseCombineDocumentsChain\n\t#from Tool import browserQA\n\t#query_website_tool = browserQA.WebpageQATool(qa_chain=load_qa_with_sources_chain(llm))\n\t#define TOOLs\n\ttools = [\n\t    Tool(\n\t        name = \"python repl\",\n\t        func=python_repl.run,\n\t        description=\"useful for when you need to use python to answer a question. You should input python code\"\n\t    )\n", "]\n\twikipedia_tool = Tool(\n\t    name='wikipedia',\n\t    func= wikipedia.run,\n\t    description=\"Useful for when you need to look up a topic, country or person on wikipedia\"\n\t)\n\tduckduckgo_tool = Tool(\n\t    name='DuckDuckGo Search',\n\t    func= search.run,\n\t    description=\"Useful for when you need to do a search on the internet to find information that another tool can't find. be specific with your input.\"\n", ")\n\t\"\"\"\n\tqueryWebsite_tool = Tool(\n\t    name= query_website_tool.name,\n\t    func= query_website_tool.run,\n\t    description= query_website_tool.description\n\t)\n\t\"\"\"\n\t#human_input_tool = Tool(\n\t    #name='human input',\n", "    #func= HumanInputRun.run,\n\t    #description=\"Useful for when you need to ask a human a question. be specific with your input.\"\n\t#)\n\t#Add here your tools\n\t#custom_tool = Tool(\n\t    #name='custom tool',\n\t    #func= custom_tool.run,\n\t    #description=\"My fantasitc tool\"\n\t#)\n\ttools.append(duckduckgo_tool)\n", "#tools.append(queryWebsite_tool)\n\ttools.append(wikipedia_tool)\n\t#tools.append(human_input_tool)\n\t#tools.append(custom_tool)\n\t#Create the Agent\n\titeration = (int(input(\"Enter the number of iterations: \")) if input(\"Do you want to set the number of iterations? (y/n): \") == \"y\" else 3)\n\tzero_shot_agent = initialize_agent(\n\t    agent=\"zero-shot-react-description\", \n\t    tools=tools, \n\t    llm=llm,\n", "    verbose=True,\n\t    max_iterations=iteration,\n\t)\n\tprint(\">> START CUSTOM AGENT\")\n\tprint(\"> Digit 'exit' for exit or 'your task or question' for start\\n\\n\")\n\tprompt = input(\"(Enter your task or question) >> \")\n\twhile prompt != \"exit\":\n\t    zero_shot_agent.run(prompt)\n\t    prompt = input(\"(Enter your task or question) >> \")\n"]}
{"filename": "OtherAgent/csvAgent.py", "chunked_list": ["import json\n\tfrom pathlib import Path\n\tfrom json import JSONDecodeError\n\tfrom langchain.agents import create_csv_agent\n\tfrom FreeLLM import ChatGPTAPI # FREE CHATGPT API\n\tfrom FreeLLM import HuggingChatAPI # FREE HUGGINGCHAT API\n\tfrom FreeLLM import BingChatAPI # FREE BINGCHAT API\n\tfrom FreeLLM import BardChatAPI # FREE GOOGLE BARD API\n\tfrom langchain.utilities import PythonREPL\n\timport os\n", "#### LOG IN FOR CHATGPT FREE LLM\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tselect_model = input(\"Select the model you want to use (1, 2, 3 or 4) \\n \\\n\t1) ChatGPT \\n \\\n\t2) HuggingChat \\n \\\n\t3) BingChat \\n \\\n\t4) Google Bard \\n \\\n\t>>> \")\n\tif select_model == \"1\":\n", "    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\t    if CG_TOKEN != \"your-chatgpt-token\":\n\t        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n\t    else:\n\t        raise ValueError(\n\t            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n\t        )\n\t    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n\t    if os.getenv(\"USE_GPT4\") == \"True\":\n\t        model = \"gpt-4\"\n", "    else:\n\t        model = \"default\"\n\t    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\telif select_model == \"2\":\n\t    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n\t    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n\t    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n\t        os.environ[\"emailHF\"] = emailHF\n\t        os.environ[\"pswHF\"] = pswHF\n\t    else:\n", "        raise ValueError(\n\t            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n\t        )\n\t    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\telif select_model == \"3\":\n\t    if not os.path.exists(\"cookiesBing.json\"):\n\t        raise ValueError(\"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\")\n\t    cookie_path = Path() / \"cookiesBing.json\"\n\t    with open(\"cookiesBing.json\", 'r') as file:\n\t        try:\n", "            file_json = json.loads(file.read())\n\t        except JSONDecodeError:\n\t            raise ValueError(\"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\")\n\t    llm=BingChatAPI.BingChat(cookiepath=str(cookie_path), conversation_style=\"creative\")\n\telif select_model == \"4\":\n\t    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\t    if GB_TOKEN != \"your-googlebard-token\":\n\t        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n\t    else:\n\t        raise ValueError(\"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\")\n", "    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n\t    llm=BardChatAPI.BardChat(cookie=cookie_path)\n\t####\n\tpath_csv = input(\"Enter the path of the csv file: \") or \"OtherAgent/startup.csv\"\n\tagent = create_csv_agent(llm=llm, tool=PythonREPL(), path=path_csv, verbose=True)\n\t#todo : ADD MEMORY\n\tprint(\">> START CSV AGENT\")\n\tprint(\"> Digit 'exit' for exit or 'your task or question' for start\\n\\n\")\n\tprompt = input(\"(Enter your task or question) >> \")\n\twhile prompt != \"exit\":\n", "    agent.run(prompt)\n\t    prompt = input(\"(Enter your task or question) >> \")\n"]}
{"filename": "OtherAgent/FreeLLM/HuggingChatAPI.py", "chunked_list": ["from hugchat import hugchat\n\tfrom hugchat.login import Login\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\tfrom time import sleep\n\tclass HuggingChat(LLM):\n\t    history_data: Optional[List] = []\n\t    chatbot : Optional[hugchat.ChatBot] = None\n\t    conversation : Optional[str] = \"\"\n\t    email : Optional[str]\n", "    psw : Optional[str]\n\t    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n\t        #token is a must check\n", "        if self.chatbot is None:\n\t            if self.email is None and self.psw is None:\n\t                ValueError(\"Email and Password is required, pls check the documentation on github\")\n\t            else: \n\t                if self.conversation == \"\":\n\t                    sign = Login(self.email, self.psw)\n\t                    cookies = sign.login()\n\t                    # Save cookies to usercookies/<email>.json\n\t                    sign.saveCookies()\n\t                    # Create a ChatBot\n", "                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n\t                else:\n\t                    raise ValueError(\"Something went wrong\")\n\t        sleep(2)\n\t        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n\t        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n\t        return data\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n", "        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"HuggingCHAT\"}\n\t#llm = HuggingChat(cookiepath = \"YOUR-COOKIES-PATH\") #for start new chat\n\t#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "OtherAgent/FreeLLM/ChatGPTAPI.py", "chunked_list": ["from gpt4_openai import GPT4OpenAI\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\tfrom time import sleep\n\tclass ChatGPT(LLM):\n\t    history_data: Optional[List] = []\n\t    token : Optional[str]\n\t    chatbot : Optional[GPT4OpenAI] = None\n\t    call : int = 0\n\t    model : str = \"gpt-3\" # or gpt-4\n", "    plugin_id : Optional[List] = []\n\t    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n\t        #token is a must check\n", "        if self.chatbot is None:\n\t            if self.token is None:\n\t                raise ValueError(\"Need a token , check https://chat.openai.com/api/auth/session for get your token\")\n\t            else:\n\t                try:\n\t                    if self.plugin_id == []:\n\t                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model)\n\t                    else:\n\t                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model, plugin_ids=self.plugin_id)\n\t                except:\n", "                    raise ValueError(\"Error on create chatbot, check your token, or your model\")\n\t        response = \"\"\n\t        # OpenAI: 50 requests / hour for each account\n\t        if (self.call >= 45 and self.model == \"default\") or (self.call >= 23 and self.model == \"gpt4\"):\n\t            raise ValueError(\"You have reached the maximum number of requests per hour ! Help me to Improve. Abusing this tool is at your own risk\")\n\t        else:\n\t            sleep(2)\n\t            response = self.chatbot(prompt)\n\t            self.call += 1\n\t        #add to history\n", "        self.history_data.append({\"prompt\":prompt,\"response\":response})    \n\t        return response\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"ChatGPT\", \"token\": self.token, \"model\": self.model}\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\") #for start new chat\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\" , model=\"gpt4\") # REQUIRED CHATGPT PLUS subscription\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\", conversation = \"Add-XXXX-XXXX-Convesation-ID\") #for use a chat already started\n\t#print(llm(\"Hello, how are you?\"))\n", "#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "OtherAgent/FreeLLM/BardChatAPI.py", "chunked_list": ["from Bard import Chatbot\n\timport asyncio\n\timport requests\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\timport pydantic\n\timport os\n\tfrom langchain import PromptTemplate, LLMChain\n\tfrom time import sleep\n\tclass BardChat(LLM):\n", "    history_data: Optional[List] = []\n\t    cookie : Optional[str]\n\t    chatbot : Optional[Chatbot] = None\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n", "        #cookie is a must check\n\t        if self.chatbot is None:\n\t            if self.cookie is None:\n\t                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n\t            else:\n\t                #if self.chatbot == None:\n\t                self.chatbot = Chatbot(self.cookie)\n\t        response = self.chatbot.ask(prompt)\n\t        #print(response)\n\t        response_text = response['content']\n", "        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n\t        return response_text\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        return asyncio.run(self.call(prompt=prompt))\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"BardCHAT\", \"cookie\": self.cookie}\n\t#llm = BardChat(cookie = \"YOURCOOKIE\") #for start new chat\n", "#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "OtherAgent/FreeLLM/BingChatAPI.py", "chunked_list": ["from EdgeGPT import Chatbot, ConversationStyle\n\timport asyncio\n\timport requests\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\timport pydantic\n\timport os\n\tfrom langchain import PromptTemplate, LLMChain\n\tfrom time import sleep\n\tclass BingChat(LLM):\n", "    history_data: Optional[List] = []\n\t    cookiepath : Optional[str]\n\t    chatbot : Optional[Chatbot] = None\n\t    conversation_style : Optional[str] \n\t    conversation_style_on : Optional[ConversationStyle] = ConversationStyle.precise\n\t    search_result : Optional[bool] = False\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def select_conversation(self, conversation_style: str):\n", "        if conversation_style == \"precise\":\n\t            self.conversation_style_on = ConversationStyle.precise\n\t        elif conversation_style == \"creative\":\n\t            self.conversation_style_on = ConversationStyle.creative\n\t        elif conversation_style == \"balanced\":\n\t            self.conversation_style_on = ConversationStyle.balanced\n\t        else:\n\t            raise ValueError(\"conversation_style must be precise, creative or balaced\")\n\t        self.conversation_style = conversation_style\n\t    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n", "        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n\t        #cookiepath is a must check\n\t        if self.chatbot is None:\n\t            if self.cookiepath is None:\n\t                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n\t            else:\n\t                #if self.chatbot == None:\n\t                self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n", "        if self.conversation_style is not None:\n\t            self.conversation_style_on = self.select_conversation(self.conversation_style)\n\t        response = await self.chatbot.ask(prompt=prompt, conversation_style=self.conversation_style, search_result=self.search_result)\n\t        response_messages = response.get(\"item\", {}).get(\"messages\", [])\n\t        response_text = response_messages[1].get(\"text\", \"\")\n\t        if response_text == \"\":\n\t            hidden_text = response_messages[1].get(\"hiddenText\", \"\")\n\t            print(\">>>> [DEBBUGGER] hidden_text = \" + str(hidden_text) + \" [DEBBUGGER] <<<<\")\n\t            print(\">>>> [DEBBUGGER] BING CHAT dont is open Like CHATGPT , BingCHAT have refused to respond. [DEBBUGGER] <<<<\")\n\t            response_text = hidden_text\n", "            \"\"\"\n\t            # reset the chatbot and remake the call\n\t            print(\"[DEBUGGER] Chatbot failed to respond. Resetting and trying again. [DEBUGGER]\")\n\t            print(\"[ INFO DEBUGGER ] \\n<Response>\\n\" + str(response) + \"\\n</Response>\\n\\n\")\n\t            sleep(10)\n\t            self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n\t            sleep(2)\n\t            response = await self.chatbot.ask(prompt=prompt)\n\t            response_messages = response.get(\"item\", {}).get(\"messages\", [])\n\t            response_text = response_messages[1].get(\"text\", \"\")\n", "            \"\"\"\n\t        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n\t        return response_text\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        return asyncio.run(self.call(prompt=prompt))\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"BingCHAT\", \"cookiepath\": self.cookiepath}\n", "#llm = BingChat(cookiepath = \"YOUR-COOKIE\") #for start new chat\n\t#llm = BingChat(cookiepath = \"YOUR-COOKIE\", conversation_style = \"precise\") #precise, creative or balaced\n\t#llm = BingChat(cookiepath = \"YOUR-COOKIE\" , conversation_style = \"precise\" , search_result=True) #with web access\n\t#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "OtherAgent/Tool/browserQA.py", "chunked_list": ["# !pip install playwright\n\t# !playwright install\n\tasync def async_load_playwright(url: str) -> str:\n\t    \"\"\"Load the specified URLs using Playwright and parse using BeautifulSoup.\"\"\"\n\t    from bs4 import BeautifulSoup\n\t    from playwright.async_api import async_playwright\n\t    try:\n\t        print(\">>> WARNING <<<\")\n\t        print(\"If you are running this for the first time, you nedd to install playwright\")\n\t        print(\">>> AUTO INSTALLING PLAYWRIGHT <<<\")\n", "        os.system(\"playwright install\")\n\t        print(\">>> PLAYWRIGHT INSTALLED <<<\")\n\t    except:\n\t        print(\">>> PLAYWRIGHT ALREADY INSTALLED <<<\")\n\t        pass\n\t    results = \"\"\n\t    async with async_playwright() as p:\n\t        browser = await p.chromium.launch(headless=True)\n\t        try:\n\t            page = await browser.new_page()\n", "            await page.goto(url)\n\t            page_source = await page.content()\n\t            soup = BeautifulSoup(page_source, \"html.parser\")\n\t            for script in soup([\"script\", \"style\"]):\n\t                script.extract()\n\t            text = soup.get_text()\n\t            lines = (line.strip() for line in text.splitlines())\n\t            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n\t            results = \"\\n\".join(chunk for chunk in chunks if chunk)\n\t        except Exception as e:\n", "            results = f\"Error: {e}\"\n\t        await browser.close()\n\t    return results\n\tdef run_async(coro):\n\t    event_loop = asyncio.get_event_loop()\n\t    return event_loop.run_until_complete(coro)\n\tdef browse_web_page(url: str) -> str:\n\t    \"\"\"Verbose way to scrape a whole webpage. Likely to cause issues parsing.\"\"\"\n\t    return run_async(async_load_playwright(url))\n\tfrom langchain.tools import BaseTool\n", "from langchain.tools import DuckDuckGoSearchRun \n\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\tfrom pydantic import Field\n\tfrom langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain, BaseCombineDocumentsChain\n\tdef _get_text_splitter():\n\t    return RecursiveCharacterTextSplitter(\n\t        # Set a really small chunk size, just to show.\n\t        chunk_size = 500,\n\t        chunk_overlap  = 20,\n\t        length_function = len,\n", "    )\n\tclass WebpageQATool(BaseTool):\n\t    name = \"query_webpage\"\n\t    description = \"Browse a webpage and retrieve the information relevant to the question.\"\n\t    text_splitter: RecursiveCharacterTextSplitter = Field(default_factory=_get_text_splitter)\n\t    qa_chain: BaseCombineDocumentsChain\n\t    def _run(self, url: str, question: str) -> str:\n\t        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n\t        result = browse_web_page.run(url)\n\t        docs = [Document(page_content=result, metadata={\"source\": url})]\n", "        web_docs = self.text_splitter.split_documents(docs)\n\t        results = []\n\t        # TODO: Handle this with a MapReduceChain\n\t        for i in range(0, len(web_docs), 4):\n\t            input_docs = web_docs[i:i+4]\n\t            window_result = self.qa_chain({\"input_documents\": input_docs, \"question\": question}, return_only_outputs=True)\n\t            results.append(f\"Response from window {i} - {window_result}\")\n\t        results_docs = [Document(page_content=\"\\n\".join(results), metadata={\"source\": url})]\n\t        return self.qa_chain({\"input_documents\": results_docs, \"question\": question}, return_only_outputs=True)\n\t    async def _arun(self, url: str, question: str) -> str:\n", "        raise NotImplementedError\n"]}
{"filename": "Embedding/HuggingFaceEmbedding.py", "chunked_list": ["import requests\n\tfrom retry import retry\n\timport numpy as np\n\timport os\n\t#read from env the hf token\n\tif os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\") is not None:\n\t    hf_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n\telse:\n\t    raise Exception(\"You must provide the huggingface token\")\n\t# model_id = \"sentence-transformers/all-MiniLM-L6-v2\" NOT WORKING FROM 10/05/2023\n", "model_id = \"obrizum/all-MiniLM-L6-v2\"\n\tapi_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n\theaders = {\"Authorization\": f\"Bearer {hf_token}\"}\n\tdef reshape_array(arr):\n\t    # create an array of zeros with shape (1536)\n\t    new_arr = np.zeros((1536,))\n\t    # copy the original array into the new array\n\t    new_arr[:arr.shape[0]] = arr\n\t    # return the new array\n\t    return new_arr\n", "@retry(tries=3, delay=10)\n\tdef newEmbeddings(texts):\n\t    response = requests.post(api_url, headers=headers, json={\"inputs\": texts, \"options\":{\"wait_for_model\":True}})\n\t    result = response.json()\n\t    if isinstance(result, list):\n\t      return result\n\t    elif list(result.keys())[0] == \"error\":\n\t      raise RuntimeError(\n\t          \"The model is currently loading, please re-run the query.\"\n\t          )\n", "def newEmbeddingFunction(texts):\n\t    embeddings = newEmbeddings(texts)\n\t    embeddings = np.array(embeddings, dtype=np.float32)\n\t    shaped_embeddings = reshape_array(embeddings)\n\t    return shaped_embeddings\n"]}
{"filename": "hfAgent/agents.py", "chunked_list": ["#!/usr/bin/env python\n\t# coding=utf-8\n\t# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n", "# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport importlib.util\n\timport json\n\timport os\n\timport time\n\tfrom dataclasses import dataclass\n", "from typing import Dict\n\timport requests\n\tfrom huggingface_hub import HfFolder, hf_hub_download, list_spaces\n\tfrom transformers.utils import logging\n\tfrom transformers.tools.base import TASK_MAPPING, TOOL_CONFIG_FILE, Tool, load_tool, supports_remote\n\tfrom transformers.tools.prompts import CHAT_MESSAGE_PROMPT, CHAT_PROMPT_TEMPLATE, RUN_PROMPT_TEMPLATE\n\tfrom transformers.tools.python_interpreter import evaluate\n\tlogger = logging.get_logger(__name__)\n\t#if is_openai_available():\n\timport openai\n", "_tools_are_initialized = False\n\tBASE_PYTHON_TOOLS = {\n\t    \"print\": print,\n\t    \"float\": float,\n\t    \"int\": int,\n\t    \"bool\": bool,\n\t    \"str\": str,\n\t}\n\t@dataclass\n\tclass PreTool:\n", "    task: str\n\t    description: str\n\t    repo_id: str\n\tHUGGINGFACE_DEFAULT_TOOLS = {}\n\tHUGGINGFACE_DEFAULT_TOOLS_FROM_HUB = [\n\t    \"image-transformation\",\n\t    \"text-download\",\n\t    \"text-to-image\",\n\t    \"text-to-video\",\n\t]\n", "def get_remote_tools(organization=\"huggingface-tools\"):\n\t    spaces = list_spaces(author=organization)\n\t    tools = {}\n\t    for space_info in spaces:\n\t        repo_id = space_info.id\n\t        resolved_config_file = hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type=\"space\")\n\t        with open(resolved_config_file, encoding=\"utf-8\") as reader:\n\t            config = json.load(reader)\n\t        task = repo_id.split(\"/\")[-1]\n\t        tools[config[\"name\"]] = PreTool(task=task, description=config[\"description\"], repo_id=repo_id)\n", "    return tools\n\tdef _setup_default_tools():\n\t    global HUGGINGFACE_DEFAULT_TOOLS\n\t    global _tools_are_initialized\n\t    if _tools_are_initialized:\n\t        return\n\t    main_module = importlib.import_module(\"transformers\")\n\t    tools_module = main_module.tools\n\t    remote_tools = get_remote_tools()\n\t    for task_name in TASK_MAPPING:\n", "        tool_class_name = TASK_MAPPING.get(task_name)\n\t        tool_class = getattr(tools_module, tool_class_name)\n\t        description = tool_class.description\n\t        HUGGINGFACE_DEFAULT_TOOLS[tool_class.name] = PreTool(task=task_name, description=description, repo_id=None)\n\t    for task_name in HUGGINGFACE_DEFAULT_TOOLS_FROM_HUB:\n\t        found = False\n\t        for tool_name, tool in remote_tools.items():\n\t            if tool.task == task_name:\n\t                HUGGINGFACE_DEFAULT_TOOLS[tool_name] = tool\n\t                found = True\n", "                break\n\t        if not found:\n\t            raise ValueError(f\"{task_name} is not implemented on the Hub.\")\n\t    _tools_are_initialized = True\n\tdef resolve_tools(code, toolbox, remote=False, cached_tools=None):\n\t    if cached_tools is None:\n\t        resolved_tools = BASE_PYTHON_TOOLS.copy()\n\t    else:\n\t        resolved_tools = cached_tools\n\t    for name, tool in toolbox.items():\n", "        if name not in code or name in resolved_tools:\n\t            continue\n\t        if isinstance(tool, Tool):\n\t            resolved_tools[name] = tool\n\t        else:\n\t            task_or_repo_id = tool.task if tool.repo_id is None else tool.repo_id\n\t            _remote = remote and supports_remote(task_or_repo_id)\n\t            resolved_tools[name] = load_tool(task_or_repo_id, remote=_remote)\n\t    return resolved_tools\n\tdef get_tool_creation_code(code, toolbox, remote=False):\n", "    code_lines = [\"from transformers import load_tool\", \"\"]\n\t    for name, tool in toolbox.items():\n\t        if name not in code or isinstance(tool, Tool):\n\t            continue\n\t        task_or_repo_id = tool.task if tool.repo_id is None else tool.repo_id\n\t        line = f'{name} = load_tool(\"{task_or_repo_id}\"'\n\t        if remote:\n\t            line += \", remote=True\"\n\t        line += \")\"\n\t        code_lines.append(line)\n", "    return \"\\n\".join(code_lines) + \"\\n\"\n\tdef clean_code_for_chat(result):\n\t    lines = result.split(\"\\n\")\n\t    idx = 0\n\t    while idx < len(lines) and not lines[idx].lstrip().startswith(\"```\"):\n\t        idx += 1\n\t    explanation = \"\\n\".join(lines[:idx]).strip()\n\t    if idx == len(lines):\n\t        return explanation, None\n\t    idx += 1\n", "    start_idx = idx\n\t    while not lines[idx].lstrip().startswith(\"```\"):\n\t        idx += 1\n\t    code = \"\\n\".join(lines[start_idx:idx]).strip()\n\t     #if code start with \"py`\"  or \"python`\"\n\t    if code.startswith(\"py`\"):\n\t        code = code[3:]\n\t    elif code.startswith(\"python`\"):\n\t        code = code[7:]\n\t    elif code.startswith(\"`\"):\n", "        code = code[1:]\n\t    if code.endswith(\"`\"):\n\t        code = code[:-1]\n\t    return explanation, code\n\tdef clean_code_for_run(result):\n\t    result = f\"I will use the following {result}\"\n\t    try:\n\t        explanation, code = result.split(\"Answer:\")\n\t    except ValueError:\n\t        explanation = result\n", "        code = \"#Problem with the code\"\n\t    explanation = explanation.strip()\n\t    code = code.strip()\n\t    code_lines = code.split(\"\\n\")\n\t    if code_lines[0] in [\"```\", \"```py\", \"```python\", \"py`\", \"`\"]:\n\t        code_lines = code_lines[1:]\n\t    if code_lines[-1] == \"```\":\n\t        code_lines = code_lines[:-1]\n\t    code = \"\\n\".join(code_lines)\n\t    #if code start with \"py`\"  or \"python`\"\n", "    if code.startswith(\"py`\"):\n\t        code = code[3:]\n\t    elif code.startswith(\"python`\"):\n\t        code = code[7:]\n\t    elif code.startswith(\"`\"):\n\t        code = code[1:]\n\t    if code.endswith(\"`\"):\n\t        code = code[:-1]\n\t    return explanation, code\n\tclass Agent:\n", "    \"\"\"\n\t    Base class for all agents which contains the main API methods.\n\t    Args:\n\t        chat_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `chat` method.\n\t        run_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `run` method.\n\t        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n\t            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n\t            one of the default tools, that default tool will be overridden.\n", "    \"\"\"\n\t    def __init__(self, chat_prompt_template=None, run_prompt_template=None, additional_tools=None):\n\t        _setup_default_tools()\n\t        self.chat_prompt_template = CHAT_MESSAGE_PROMPT if chat_prompt_template is None else chat_prompt_template\n\t        self.run_prompt_template = RUN_PROMPT_TEMPLATE if run_prompt_template is None else run_prompt_template\n\t        self._toolbox = HUGGINGFACE_DEFAULT_TOOLS.copy()\n\t        if additional_tools is not None:\n\t            if isinstance(additional_tools, (list, tuple)):\n\t                additional_tools = {t.name: t for t in additional_tools}\n\t            elif not isinstance(additional_tools, dict):\n", "                additional_tools = {additional_tools.name: additional_tools}\n\t            replacements = {name: tool for name, tool in additional_tools.items() if name in HUGGINGFACE_DEFAULT_TOOLS}\n\t            self._toolbox.update(additional_tools)\n\t            if len(replacements) > 1:\n\t                names = \"\\n\".join([f\"- {n}: {t}\" for n, t in replacements.items()])\n\t                logger.warn(\n\t                    f\"The following tools have been replaced by the ones provided in `additional_tools`:\\n{names}.\"\n\t                )\n\t            elif len(replacements) == 1:\n\t                name = list(replacements.keys())[0]\n", "                logger.warn(f\"{name} has been replaced by {replacements[name]} as provided in `additional_tools`.\")\n\t        self.prepare_for_new_chat()\n\t    @property\n\t    def toolbox(self) -> Dict[str, Tool]:\n\t        \"\"\"Get all tool currently available to the agent\"\"\"\n\t        return self._toolbox\n\t    def format_prompt(self, task, chat_mode=False):\n\t        description = \"\\n\".join([f\"- {name}: {tool.description}\" for name, tool in self.toolbox.items()])\n\t        if chat_mode:\n\t            if self.chat_history is None:\n", "                prompt = CHAT_PROMPT_TEMPLATE.replace(\"<<all_tools>>\", description)\n\t            else:\n\t                prompt = self.chat_history\n\t            prompt += CHAT_MESSAGE_PROMPT.replace(\"<<task>>\", task)\n\t        else:\n\t            prompt = self.run_prompt_template.replace(\"<<all_tools>>\", description)\n\t            prompt = prompt.replace(\"<<prompt>>\", task)\n\t        return prompt\n\t    def chat(self, task, *, return_code=False, remote=False, **kwargs):\n\t        \"\"\"\n", "        Sends a new request to the agent in a chat. Will use the previous ones in its history.\n\t        Args:\n\t            task (`str`): The task to perform\n\t            return_code (`bool`, *optional*, defaults to `False`):\n\t                Whether to just return code and not evaluate it.\n\t            remote (`bool`, *optional*, defaults to `False`):\n\t                Whether or not to use remote tools (inference endpoints) instead of local ones.\n\t            kwargs:\n\t                Any keyword argument to send to the agent when evaluating the code.\n\t        Example:\n", "        ```py\n\t        from transformers import HfAgent\n\t        agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\t        agent.chat(\"Draw me a picture of rivers and lakes\")\n\t        agent.chat(\"Transform the picture so that there is a rock in there\")\n\t        ```\n\t        \"\"\"\n\t        prompt = self.format_prompt(task, chat_mode=True)\n\t        result = self.generate_one(prompt, stop=[\"Human:\", \"=====\"])\n\t        self.chat_history = prompt + result.strip() + \"\\n\"\n", "        explanation, code = clean_code_for_chat(result)\n\t        print(f\"==Explanation from the agent==\\n{explanation}\")\n\t        if code is not None:\n\t            print(f\"\\n\\n==Code generated by the agent==\\n{code}\")\n\t            if not return_code:\n\t                print(\"\\n\\n==Result==\")\n\t                self.cached_tools = resolve_tools(code, self.toolbox, remote=remote, cached_tools=self.cached_tools)\n\t                self.chat_state.update(kwargs)\n\t                return evaluate(code, self.cached_tools, self.chat_state, chat_mode=True)\n\t            else:\n", "                tool_code = get_tool_creation_code(code, self.toolbox, remote=remote)\n\t                return f\"{tool_code}\\n{code}\"\n\t    def prepare_for_new_chat(self):\n\t        \"\"\"\n\t        Clears the history of prior calls to [`~Agent.chat`].\n\t        \"\"\"\n\t        self.chat_history = None\n\t        self.chat_state = {}\n\t        self.cached_tools = None\n\t    def run(self, task, *, return_code=False, remote=False, **kwargs):\n", "        \"\"\"\n\t        Sends a request to the agent.\n\t        Args:\n\t            task (`str`): The task to perform\n\t            return_code (`bool`, *optional*, defaults to `False`):\n\t                Whether to just return code and not evaluate it.\n\t            remote (`bool`, *optional*, defaults to `False`):\n\t                Whether or not to use remote tools (inference endpoints) instead of local ones.\n\t            kwargs:\n\t                Any keyword argument to send to the agent when evaluating the code.\n", "        Example:\n\t        ```py\n\t        from transformers import HfAgent\n\t        agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\t        agent.run(\"Draw me a picture of rivers and lakes\")\n\t        ```\n\t        \"\"\"\n\t        prompt = self.format_prompt(task)\n\t        result = self.generate_one(prompt, stop=[\"Task:\"])\n\t        explanation, code = clean_code_for_run(result)\n", "        print(f\"==Explanation from the agent==\\n{explanation}\")\n\t        print(f\"\\n\\n==Code generated by the agent==\\n{code}\")\n\t        if not return_code:\n\t            print(\"\\n\\n==Result==\")\n\t            self.cached_tools = resolve_tools(code, self.toolbox, remote=remote, cached_tools=self.cached_tools)\n\t            return evaluate(code, self.cached_tools, state=kwargs.copy())\n\t        else:\n\t            tool_code = get_tool_creation_code(code, self.toolbox, remote=remote)\n\t            return f\"{tool_code}\\n{code}\"\n\t    def generate_one(self, prompt, stop):\n", "        # This is the method to implement in your custom agent.\n\t        raise NotImplementedError\n\t    def generate_many(self, prompts, stop):\n\t        # Override if you have a way to do batch generation faster than one by one\n\t        return [self.generate_one(prompt, stop) for prompt in prompts]\n\tclass OpenAiAgent(Agent):\n\t    \"\"\"\n\t    Agent that uses the openai API to generate code.\n\t    <Tip warning={true}>\n\t    The openAI models are used in generation mode, so even for the `chat()` API, it's better to use models like\n", "    `\"text-davinci-003\"` over the chat-GPT variant. Proper support for chat-GPT models will come in a next version.\n\t    </Tip>\n\t    Args:\n\t        model (`str`, *optional*, defaults to `\"text-davinci-003\"`):\n\t            The name of the OpenAI model to use.\n\t        api_key (`str`, *optional*):\n\t            The API key to use. If unset, will look for the environment variable `\"OPENAI_API_KEY\"`.\n\t        chat_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `chat` method.\n\t        run_prompt_template (`str`, *optional*):\n", "            Pass along your own prompt if you want to override the default template for the `run` method.\n\t        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n\t            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n\t            one of the default tools, that default tool will be overridden.\n\t    Example:\n\t    ```py\n\t    from transformers import OpenAiAgent\n\t    agent = OpenAiAgent(model=\"text-davinci-003\", api_key=xxx)\n\t    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n\t    ```\n", "    \"\"\"\n\t    def __init__(\n\t        self,\n\t        model=\"text-davinci-003\",\n\t        api_key=None,\n\t        chat_prompt_template=None,\n\t        run_prompt_template=None,\n\t        additional_tools=None,\n\t    ):\n\t        #if not is_openai_available():\n", "            #raise ImportError(\"Using `OpenAiAgent` requires `openai`: `pip install openai`.\")\n\t        if api_key is None:\n\t            api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n\t        if api_key is None:\n\t            raise ValueError(\n\t                \"You need an openai key to use `OpenAIAgent`. You can get one here: Get one here \"\n\t                \"https://openai.com/api/`. If you have one, set it in your env with `os.environ['OPENAI_API_KEY'] = \"\n\t                \"xxx.\"\n\t            )\n\t        else:\n", "            openai.api_key = api_key\n\t        self.model = model\n\t        super().__init__(\n\t            chat_prompt_template=chat_prompt_template,\n\t            run_prompt_template=run_prompt_template,\n\t            additional_tools=additional_tools,\n\t        )\n\t    def generate_many(self, prompts, stop):\n\t        if \"gpt\" in self.model:\n\t            return [self._chat_generate(prompt, stop) for prompt in prompts]\n", "        else:\n\t            return self._completion_generate(prompts, stop)\n\t    def generate_one(self, prompt, stop):\n\t        if \"gpt\" in self.model:\n\t            return self._chat_generate(prompt, stop)\n\t        else:\n\t            return self._completion_generate([prompt], stop)[0]\n\t    def _chat_generate(self, prompt, stop):\n\t        result = openai.ChatCompletion.create(\n\t            model=self.model,\n", "            messages=[{\"role\": \"user\", \"content\": prompt}],\n\t            temperature=0,\n\t            stop=stop,\n\t        )\n\t        return result[\"choices\"][0][\"message\"][\"content\"]\n\t    def _completion_generate(self, prompts, stop):\n\t        result = openai.Completion.create(\n\t            model=self.model,\n\t            prompt=prompts,\n\t            temperature=0,\n", "            stop=stop,\n\t            max_tokens=200,\n\t        )\n\t        return [answer[\"text\"] for answer in result[\"choices\"]]\n\tclass HfAgent(Agent):\n\t    \"\"\"\n\t    Agent that uses and inference endpoint to generate code.\n\t    Args:\n\t        url_endpoint (`str`):\n\t            The name of the url endpoint to use.\n", "        token (`str`, *optional*):\n\t            The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when\n\t            running `huggingface-cli login` (stored in `~/.huggingface`).\n\t        chat_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `chat` method.\n\t        run_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `run` method.\n\t        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n\t            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n\t            one of the default tools, that default tool will be overridden.\n", "    Example:\n\t    ```py\n\t    from transformers import HfAgent\n\t    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\t    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n\t    ```\n\t    \"\"\"\n\t    def __init__(\n\t        self, url_endpoint, token=None, chat_prompt_template=None, run_prompt_template=None, additional_tools=None\n\t    ):\n", "        self.url_endpoint = url_endpoint\n\t        if token is None:\n\t            self.token = f\"Bearer {HfFolder().get_token()}\"\n\t        elif token.startswith(\"Bearer\") or token.startswith(\"Basic\"):\n\t            self.token = token\n\t        else:\n\t            self.token = f\"Bearer {token}\"\n\t        super().__init__(\n\t            chat_prompt_template=chat_prompt_template,\n\t            run_prompt_template=run_prompt_template,\n", "            additional_tools=additional_tools,\n\t        )\n\t    def generate_one(self, prompt, stop):\n\t        headers = {\"Authorization\": self.token}\n\t        inputs = {\n\t            \"inputs\": prompt,\n\t            \"parameters\": {\"max_new_tokens\": 200, \"return_full_text\": False, \"stop\": stop},\n\t        }\n\t        response = requests.post(self.url_endpoint, json=inputs, headers=headers)\n\t        if response.status_code == 429:\n", "            print(\"Getting rate-limited, waiting a tiny bit before trying again.\")\n\t            time.sleep(1)\n\t            return self._generate_one(prompt)\n\t        elif response.status_code != 200:\n\t            raise ValueError(f\"Error {response.status_code}: {response.json()}\")\n\t        result = response.json()[0][\"generated_text\"]\n\t        # Inference API returns the stop sequence\n\t        for stop_seq in stop:\n\t            if result.endswith(stop_seq):\n\t                result = result[: -len(stop_seq)]\n", "        return result\n\tclass ChatGPTAgent(Agent):\n\t    \"\"\"\n\t    Agent that uses and inference endpoint of CHATGPT by IntelligenzaArtificialeItalia.net\n\t    Args:\n\t        token (`str`, *optional*):\n\t            The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when\n\t            running `huggingface-cli login` (stored in `~/.huggingface`).\n\t        chat_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `chat` method.\n", "        run_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `run` method.\n\t        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n\t            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n\t            one of the default tools, that default tool will be overridden.\n\t    Example:\n\t    ```py\n\t    from hfAgent import ChatGPTAgent\n\t    agent = ChatGPTAgent(\"TOKEN\")\n\t    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n", "    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n\t    ```\n\t    \"\"\"\n\t    def __init__(\n\t        self, token, chat_prompt_template=None, run_prompt_template=None, additional_tools=None, llm=None, model=None\n\t    ):\n\t        if token is None:\n\t            ValueError(\"You must provide a ChatGPT token\")\n\t        else:\n\t            from .FreeLLM import ChatGPTAPI\n", "            import asyncio\n\t            self.token = token\n\t            if model is not None:\n\t                self.llm = ChatGPTAPI.ChatGPT(token = self.token, model = \"gpt-4\")\n\t            else:\n\t                self.llm = ChatGPTAPI.ChatGPT(token = self.token)\n\t        super().__init__(\n\t            chat_prompt_template=chat_prompt_template,\n\t            run_prompt_template=run_prompt_template,\n\t            additional_tools=additional_tools,\n", "        )\n\t    def generate_one(self, prompt, stop):\n\t        result = self.llm(prompt + \"\\nRemember to use the following stop sequence: \" + str(stop))  \n\t        # Inference API returns the stop sequence\n\t        for stop_seq in stop:\n\t            if result.endswith(stop_seq):\n\t                result = result[: -len(stop_seq)]\n\t        return result\n\tclass HuggingChatAgent(Agent):\n\t    \"\"\"\n", "    Agent that uses and inference endpoint of HuggingCHAT by IntelligenzaArtificialeItalia.net\n\t    Args:\n\t        chat_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `chat` method.\n\t        run_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `run` method.\n\t        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n\t            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n\t            one of the default tools, that default tool will be overridden.\n\t    Example:\n", "    ```py\n\t    from hfAgent import HuggingChatAgent\n\t    agent = HuggingChatAgent()\n\t    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n\t    agent.chat(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n\t    ```\n\t    \"\"\"\n\t    def __init__(\n\t        self, chat_prompt_template=None, run_prompt_template=None, additional_tools=None, llm=None, model=None\n\t    ):\n", "        import json\n\t        from pathlib import Path\n\t        from json import JSONDecodeError\n\t        emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n\t        pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n\t        if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n\t            os.environ[\"emailHF\"] = emailHF\n\t            os.environ[\"pswHF\"] = pswHF\n\t        else:\n\t            raise ValueError(\n", "                \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n\t            )\n\t        from .FreeLLM import HuggingChatAPI\n\t        self.llm = HuggingChatAPI.HuggingChat(email=emailHF, psw=pswHF)\n\t        super().__init__(\n\t            chat_prompt_template=chat_prompt_template,\n\t            run_prompt_template=run_prompt_template,\n\t            additional_tools=additional_tools,\n\t        )\n\t    def generate_one(self, prompt, stop):\n", "        result = self.llm(prompt + \"\\nRemember to use the following stop sequence: \" + str(stop))  \n\t        # Inference API returns the stop sequence\n\t        for stop_seq in stop:\n\t            if result.endswith(stop_seq):\n\t                result = result[: -len(stop_seq)]\n\t        return result\n\tclass BingChatAgent(Agent):\n\t    \"\"\"\n\t    Agent that uses and inference endpoint of BingCHAT by IntelligenzaArtificialeItalia.net\n\t    Args:\n", "        cookiepath (`str`):\n\t        chat_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `chat` method.\n\t        run_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `run` method.\n\t        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n\t            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n\t            one of the default tools, that default tool will be overridden.\n\t    Example:\n\t    ```py\n", "    from hfAgent import BingChatAgent\n\t    agent = BingChatAgent(\"cookie-path\")\n\t    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n\t    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n\t    ```\n\t    \"\"\"\n\t    def __init__(\n\t        self, cookiepath, chat_prompt_template=None, run_prompt_template=None, additional_tools=None, llm=None, model=None , conversation = \"balanced\"\n\t    ):\n\t        from .FreeLLM import BingChatAPI\n", "        if cookiepath is None:\n\t            ValueError(\"You must provide a cookie path\")\n\t        else:\n\t            self.cookiepath = cookiepath\n\t            if conversation == \"balanced\":\n\t                self.llm = BingChatAPI.BingChat(cookiepath = self.cookiepath, conversation_style = \"balanced\")\n\t            elif conversation == \"creative\":\n\t                self.llm = BingChatAPI.BingChat(cookiepath = self.cookiepath, conversation_style = \"creative\")\n\t            elif conversation == \"precise\":\n\t                self.llm = BingChatAPI.BingChat(cookiepath = self.cookiepath, conversation_style = \"precise\")\n", "        super().__init__(\n\t            chat_prompt_template=chat_prompt_template,\n\t            run_prompt_template=run_prompt_template,\n\t            additional_tools=additional_tools,\n\t        )\n\t    def generate_one(self, prompt, stop):\n\t        result = self.llm(prompt + \"\\nRemember to use the following stop sequence: \" + str(stop))  \n\t        # Inference API returns the stop sequence\n\t        for stop_seq in stop:\n\t            if result.endswith(stop_seq):\n", "                result = result[: -len(stop_seq)]\n\t        return result\n\tclass BardChatAgent(Agent):\n\t    \"\"\"\n\t    Agent that uses and inference endpoint of Bard Chat by IntelligenzaArtificialeItalia.net\n\t    Args:\n\t        token (`str`):\n\t        chat_prompt_template (`str`, *optional*):\n\t            Pass along your own prompt if you want to override the default template for the `chat` method.\n\t        run_prompt_template (`str`, *optional*):\n", "            Pass along your own prompt if you want to override the default template for the `run` method.\n\t        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n\t            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n\t            one of the default tools, that default tool will be overridden.\n\t    Example:\n\t    ```py\n\t    from hfAgent import BardChatAgent\n\t    agent = BardChatAgent(\"token\")\n\t    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n\t    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"Â¡Este es un API muy agradable!\")\n", "    ```\n\t    \"\"\"\n\t    def __init__(\n\t        self, token ,chat_prompt_template=None, run_prompt_template=None, additional_tools=None, llm=None, model=None , conversation = \"balanced\"\n\t    ):\n\t        from .FreeLLM import BardChatAPI\n\t        if token is None:\n\t            ValueError(\"You must provide a cookie path\")\n\t        else:\n\t            self.token = token\n", "            self.llm = BardChatAPI.BardChat(cookie = self.token)\n\t        super().__init__(\n\t            chat_prompt_template=chat_prompt_template,\n\t            run_prompt_template=run_prompt_template,\n\t            additional_tools=additional_tools,\n\t        )\n\t    def generate_one(self, prompt, stop):\n\t        result = self.llm(prompt + \"\\nRemember to use the following stop sequence: \" + str(stop))  \n\t        # Inference API returns the stop sequence\n\t        for stop_seq in stop:\n", "            if result.endswith(stop_seq):\n\t                result = result[: -len(stop_seq)]\n\t        return result\n"]}
{"filename": "hfAgent/FreeLLM/HuggingChatAPI.py", "chunked_list": ["from hugchat import hugchat\n\tfrom hugchat.login import Login\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\tfrom time import sleep\n\tclass HuggingChat(LLM):\n\t    history_data: Optional[List] = []\n\t    chatbot : Optional[hugchat.ChatBot] = None\n\t    conversation : Optional[str] = \"\"\n\t    email : Optional[str]\n", "    psw : Optional[str]\n\t    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n\t        #token is a must check\n", "        if self.chatbot is None:\n\t            if self.email is None and self.psw is None:\n\t                ValueError(\"Email and Password is required, pls check the documentation on github\")\n\t            else: \n\t                if self.conversation == \"\":\n\t                    sign = Login(self.email, self.psw)\n\t                    cookies = sign.login()\n\t                    # Save cookies to usercookies/<email>.json\n\t                    sign.saveCookies()\n\t                    # Create a ChatBot\n", "                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n\t                else:\n\t                    raise ValueError(\"Something went wrong\")\n\t        sleep(2)\n\t        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n\t        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n\t        return data\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n", "        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"HuggingCHAT\"}\n\t#llm = HuggingChat(cookiepath = \"YOUR-COOKIES-PATH\") #for start new chat\n\t#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "hfAgent/FreeLLM/ChatGPTAPI.py", "chunked_list": ["from gpt4_openai import GPT4OpenAI\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\tfrom time import sleep\n\tclass ChatGPT(LLM):\n\t    history_data: Optional[List] = []\n\t    token : Optional[str]\n\t    chatbot : Optional[GPT4OpenAI] = None\n\t    call : int = 0\n\t    model : str = \"gpt-3\" # or gpt-4\n", "    plugin_id : Optional[List] = []\n\t    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n\t        #token is a must check\n", "        if self.chatbot is None:\n\t            if self.token is None:\n\t                raise ValueError(\"Need a token , check https://chat.openai.com/api/auth/session for get your token\")\n\t            else:\n\t                try:\n\t                    if self.plugin_id == []:\n\t                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model)\n\t                    else:\n\t                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model, plugin_ids=self.plugin_id)\n\t                except:\n", "                    raise ValueError(\"Error on create chatbot, check your token, or your model\")\n\t        response = \"\"\n\t        # OpenAI: 50 requests / hour for each account\n\t        if (self.call >= 45 and self.model == \"default\") or (self.call >= 23 and self.model == \"gpt4\"):\n\t            raise ValueError(\"You have reached the maximum number of requests per hour ! Help me to Improve. Abusing this tool is at your own risk\")\n\t        else:\n\t            sleep(2)\n\t            response = self.chatbot(prompt)\n\t            self.call += 1\n\t        #add to history\n", "        self.history_data.append({\"prompt\":prompt,\"response\":response})    \n\t        return response\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"ChatGPT\", \"token\": self.token, \"model\": self.model}\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\") #for start new chat\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\" , model=\"gpt4\") # REQUIRED CHATGPT PLUS subscription\n\t#llm = ChatGPT(token = \"YOUR-COOKIE\", conversation = \"Add-XXXX-XXXX-Convesation-ID\") #for use a chat already started\n\t#print(llm(\"Hello, how are you?\"))\n", "#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "hfAgent/FreeLLM/BardChatAPI.py", "chunked_list": ["from Bard import Chatbot\n\timport asyncio\n\timport requests\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\timport pydantic\n\timport os\n\tfrom langchain import PromptTemplate, LLMChain\n\tfrom time import sleep\n\tclass BardChat(LLM):\n", "    history_data: Optional[List] = []\n\t    cookie : Optional[str]\n\t    chatbot : Optional[Chatbot] = None\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        if stop is not None:\n\t            pass\n\t            #raise ValueError(\"stop kwargs are not permitted.\")\n", "        #cookie is a must check\n\t        if self.chatbot is None:\n\t            if self.cookie is None:\n\t                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n\t            else:\n\t                #if self.chatbot == None:\n\t                self.chatbot = Chatbot(self.cookie)\n\t        response = self.chatbot.ask(prompt)\n\t        #print(response)\n\t        response_text = response['content']\n", "        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n\t        return response_text\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        return asyncio.run(self.call(prompt=prompt))\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"BardCHAT\", \"cookie\": self.cookie}\n\t#llm = BardChat(cookie = \"YOURCOOKIE\") #for start new chat\n", "#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "hfAgent/FreeLLM/BingChatAPI.py", "chunked_list": ["from EdgeGPT import Chatbot, ConversationStyle\n\timport asyncio\n\timport requests\n\tfrom langchain.llms.base import LLM\n\tfrom typing import Optional, List, Mapping, Any\n\timport pydantic\n\timport os\n\tfrom langchain import PromptTemplate, LLMChain\n\tfrom time import sleep\n\tclass BingChat(LLM):\n", "    history_data: Optional[List] = []\n\t    cookiepath : Optional[str]\n\t    chatbot : Optional[Chatbot] = None\n\t    conversation_style : Optional[str] \n\t    conversation_style_on : Optional[ConversationStyle] = ConversationStyle.precise\n\t    search_result : Optional[bool] = False\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        return \"custom\"\n\t    def select_conversation(self, conversation_style: str):\n", "        if conversation_style == \"precise\":\n\t            self.conversation_style_on = ConversationStyle.precise\n\t        elif conversation_style == \"creative\":\n\t            self.conversation_style_on = ConversationStyle.creative\n\t        elif conversation_style == \"balanced\":\n\t            self.conversation_style_on = ConversationStyle.balanced\n\t        else:\n\t            raise ValueError(\"conversation_style must be precise, creative or balaced\")\n\t        self.conversation_style = conversation_style\n\t    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n", "        if stop is not None:\n\t            raise ValueError(\"stop kwargs are not permitted.\")\n\t        #cookiepath is a must check\n\t        if self.chatbot is None:\n\t            if self.cookiepath is None:\n\t                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n\t            else:\n\t                #if self.chatbot == None:\n\t                self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n\t        if self.conversation_style is not None:\n", "            self.conversation_style_on = self.select_conversation(self.conversation_style)\n\t        response = await self.chatbot.ask(prompt=prompt, conversation_style=self.conversation_style, search_result=self.search_result)\n\t        \"\"\"\n\t        this is a sample response. \n\t        {'type': 2, 'invocationId': '0', \n\t        'item': {'messages': [{'text': 'Hello, how are you?', 'author': 'user', 'from': {'id': '985157152860707', 'name': None}, 'createdAt': '2023-05-03T19:51:39.5491558+00:00', 'timestamp': '2023-05-03T19:51:39.5455787+00:00', 'locale': 'en-us', 'market': 'en-us', 'region': 'us', 'messageId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'nlu': {'scoredClassification': {'classification': 'CHAT_GPT', 'score': None}, 'classificationRanking': [{'classification': 'CHAT_GPT', 'score': None}], 'qualifyingClassifications': None, 'ood': None, 'metaData': None, 'entities': None}, 'offense': 'None', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'cib', 'privacy': None, 'inputMethod': 'Keyboard'}, {'text': \"Hello! I'm doing well, thank you. How can I assist you today?\", 'author': 'bot', 'createdAt': '2023-05-03T19:51:41.5176164+00:00', 'timestamp': '2023-05-03T19:51:41.5176164+00:00', 'messageId': '1d013e71-408b-4031-a131-2f5c009fe938', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'offense': 'None', 'adaptiveCards': [{'type': 'AdaptiveCard', 'version': '1.0', 'body': [{'type': 'TextBlock', 'text': \"Hello! I'm doing well, thank you. How can I assist you today?\\n\", 'wrap': True}]}], \n\t        'sourceAttributions': [], \n\t        'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, \n\t        'contentOrigin': 'DeepLeo', \n\t        'privacy': None, \n", "        'suggestedResponses': [{'text': 'What is the weather like today?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502696+00:00', 'timestamp': '2023-05-03T19:51:42.7502696+00:00', 'messageId': 'cd7a84d3-f9bc-47ff-9897-077b2de12e21', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'What is the latest news?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502739+00:00', 'timestamp': '2023-05-03T19:51:42.7502739+00:00', 'messageId': 'b611632a-9a8e-42de-86eb-8eb3b7b8ddbb', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'Tell me a joke.', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502743+00:00', 'timestamp': '2023-05-03T19:51:42.7502743+00:00', 'messageId': '70232e45-d7e8-4d77-83fc-752b3cd3355c', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}], 'spokenText': 'How can I assist you today?'}], 'firstNewMessageIndex': 1, 'defaultChatName': None, 'conversationId': '51D|BingProd|3E1274E188350D7BE273FFE95E02DD2984DAB52F95260300D0A2937162F98FDA', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'conversationExpiryTime': '2023-05-04T01:51:42.8260286Z', 'shouldInitiateConversation': True, 'telemetry': {'metrics': None, 'startTime': '2023-05-03T19:51:39.5456555Z'}, 'throttling': {'maxNumUserMessagesInConversation': 20, 'numUserMessagesInConversation': 1}, 'result': {'value': 'Success', 'serviceVersion': '20230501.30'}}}\n\t        \"\"\"\n\t        response_messages = response.get(\"item\", {}).get(\"messages\", [])\n\t        response_text = response_messages[1].get(\"text\", \"\")\n\t        if response_text == \"\":\n\t            hidden_text = response_messages[1].get(\"hiddenText\", \"\")\n\t            print(\">>>> [DEBBUGGER] hidden_text = \" + str(hidden_text) + \" [DEBBUGGER] <<<<\")\n\t            print(\">>>> [DEBBUGGER] BING CHAT dont is open Like CHATGPT , BingCHAT have refused to respond. [DEBBUGGER] <<<<\")\n\t            response_text = hidden_text\n\t            \"\"\"\n", "            # reset the chatbot and remake the call\n\t            print(\"[DEBUGGER] Chatbot failed to respond. Resetting and trying again. [DEBUGGER]\")\n\t            print(\"[ INFO DEBUGGER ] \\n<Response>\\n\" + str(response) + \"\\n</Response>\\n\\n\")\n\t            sleep(10)\n\t            self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n\t            sleep(2)\n\t            response = await self.chatbot.ask(prompt=prompt)\n\t            response_messages = response.get(\"item\", {}).get(\"messages\", [])\n\t            response_text = response_messages[1].get(\"text\", \"\")\n\t            \"\"\"\n", "        #add to history\n\t        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n\t        return response_text\n\t    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n\t        return asyncio.run(self.call(prompt=prompt))\n\t    @property\n\t    def _identifying_params(self) -> Mapping[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n\t        return {\"model\": \"BingCHAT\", \"cookiepath\": self.cookiepath}\n\t#llm = BingChat(cookiepath = \"YOUR-COOKIE\") #for start new chat\n", "#llm = BingChat(cookiepath = \"YOUR-COOKIE\", conversation_style = \"precise\") #precise, creative or balaced\n\t#llm = BingChat(cookiepath = \"YOUR-COOKIE\" , conversation_style = \"precise\" , search_result=True) #with web access\n\t#print(llm(\"Hello, how are you?\"))\n\t#print(llm(\"what is AI?\"))\n\t#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
