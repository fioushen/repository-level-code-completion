{"filename": "utils/measure.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport numpy as np\n\timport time\n\timport sys\n\ttry:\n\t    import torch_tensorrt\n\t    from torch_tensorrt.logging import set_reportable_log_level, Level\n\texcept:\n\t    print(\"TensorRT is not installed\")\n", "@torch.no_grad()\n\tdef get_time(\n\t    model, input_shape=(1, 3, 224, 224), name=\"\", rep=1000, warmup=2000, verb=False\n\t):\n\t    st = time.time()\n\t    device = torch.device(\"cuda\")\n\t    model.eval()\n\t    model.to(device)\n\t    dummy_input = torch.randn(input_shape).to(device)\n\t    # INIT LOGGERS\n", "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(\n\t        enable_timing=True\n\t    )\n\t    timings = np.zeros((rep, 1))\n\t    # GPU-WARM-UP\n\t    for _ in range(warmup):\n\t        _ = model(dummy_input)\n\t    # MEASURE PERFORMANCE\n\t    with torch.no_grad():\n\t        for rep_ in range(rep):\n", "            starter.record()\n\t            _ = model(dummy_input)\n\t            ender.record()\n\t            # WAIT FOR GPU SYNC\n\t            torch.cuda.synchronize()\n\t            curr_time = starter.elapsed_time(ender)\n\t            timings[rep_] = curr_time\n\t    mean_syn = np.sum(timings) / rep\n\t    std_syn = np.std(timings)\n\t    batch_size = input_shape[0]\n", "    thpts = 1 / (timings * 0.001) * batch_size\n\t    mean_thpt = np.sum(thpts) / rep\n\t    std_thpt = np.std(thpts)\n\t    if verb:\n\t        print(f\"Batch size is {batch_size}\")\n\t        print(f\"Measure time {time.time() - st:.2f} seconds\")\n\t        print(f\"[{name:>20}] THPT : {mean_thpt:.2f} || STD : {std_thpt:.2f}\")\n\t    print(f\"[{name:>20}] MEAN : {mean_syn:.2f}ms || STD : {std_syn:.2f}\")\n\t    return mean_syn, std_syn\n\tdef unroll_merged(module):\n", "    result = nn.Sequential()\n\t    for ind, blk in enumerate(module.m_features):\n\t        result.add_module(f\"blk{ind}\", blk)\n\t    pre_last = [nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten()]\n\t    for ind, blk in enumerate(pre_last):\n\t        result.add_module(f\"pre_last{ind}\", blk)\n\t    for ind, blk in enumerate(module.classifier):\n\t        result.add_module(f\"last{ind}\", blk)\n\t    return result\n\tdef unroll_merged_vgg(module):\n", "    result = nn.Sequential()\n\t    for ind, blk in enumerate(module.m_features):\n\t        result.add_module(f\"blk{ind}\", blk)\n\t    pre_last = [nn.AdaptiveAvgPool2d((7, 7)), nn.Flatten()]\n\t    for ind, blk in enumerate(pre_last):\n\t        result.add_module(f\"pre_last{ind}\", blk)\n\t    for ind, blk in enumerate(module.classifier):\n\t        result.add_module(f\"last{ind}\", blk)\n\t    return result\n\t@torch.no_grad()\n", "def compile_and_time(module, sz, txt, verb=False):\n\t    if not \"torch_tensorrt\" in sys.modules:\n\t        raise Exception(\"You should install TensorRT\")\n\t    set_reportable_log_level(Level.Error)\n\t    st = time.time()\n\t    module.eval()\n\t    model = torch_tensorrt.compile(\n\t        module,\n\t        inputs=[torch_tensorrt.Input(sz)],\n\t        enabled_precisions={torch_tensorrt.dtype.float},\n", "    )\n\t    print(f\"TensorRT compiling done ({time.time() - st:.2f} seconds)\")\n\t    result, std = get_time(\n\t        model, input_shape=sz, name=txt, rep=1000, warmup=2000, verb=verb\n\t    )\n\t    del module\n\t    return result, std\n\t@torch.no_grad()\n\tdef torch_time(module, sz, txt, verb=False, rep=1000, warmup=2000):\n\t    module.eval()\n", "    print(f\"Measuring time without TensorRT compiling...\")\n\t    result, std = get_time(\n\t        module, input_shape=sz, name=txt, rep=rep, warmup=warmup, verb=verb\n\t    )\n\t    del module\n\t    return result, std\n\t@torch.no_grad()\n\tdef get_cpu_time(\n\t    model, input_shape=(1, 3, 224, 224), name=\"\", rep=10, warmup=100, verb=False\n\t):\n", "    st = time.time()\n\t    device = torch.device(\"cpu\")\n\t    model.eval()\n\t    model.to(device)\n\t    dummy_input = torch.randn(input_shape).to(device)\n\t    # INIT LOGGERS\n\t    timings = np.zeros((rep, 1))\n\t    # CPU-WARM-UP\n\t    for i in range(warmup):\n\t        _ = model(dummy_input)\n", "        if i % 10 == 0:\n\t            print(f\"{i} / {warmup}\")\n\t    # MEASURE PERFORMANCE\n\t    with torch.no_grad():\n\t        for rep_ in range(rep):\n\t            start = time.time() * 1000\n\t            _ = model(dummy_input)\n\t            end = time.time() * 1000\n\t            curr_time = end - start\n\t            timings[rep_] = curr_time\n", "            if rep_ % 10 == 0:\n\t                print(f\"{rep_} / {rep}\")\n\t    mean_syn = np.sum(timings) / rep\n\t    std_syn = np.std(timings)\n\t    batch_size = input_shape[0]\n\t    thpts = 1 / (timings * 0.001) * batch_size\n\t    mean_thpt = np.sum(thpts) / rep\n\t    std_thpt = np.std(thpts)\n\t    if verb:\n\t        print(f\"Batch size is {batch_size}\")\n", "        print(f\"Measure time {time.time() - st:.2f} seconds\")\n\t        print(f\"[{name:>20}] THPT : {mean_thpt:.2f} || STD : {std_thpt:.2f}\")\n\t    print(f\"[{name:>20}] MEAN : {mean_syn:.2f}ms || STD : {std_syn:.2f}\")\n\t    return mean_syn, std_syn\n\t@torch.no_grad()\n\tdef torch_cpu_time(module, sz, txt, verb=False):\n\t    module.eval()\n\t    print(f\"Measuring time with cpu...\")\n\t    result, std = get_cpu_time(\n\t        module, input_shape=sz, name=txt, rep=100, warmup=10, verb=verb\n", "    )\n\t    del module\n\t    return result, std\n"]}
{"filename": "utils/dp.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\timport socket\n\timport torch\n\timport torch.nn as nn\n\timport pandas as pd\n\timport numpy as np\n\tfrom datetime import datetime\n\tfrom typing import List, Tuple\n", "from itertools import product\n\tfrom models.model_op import get_conv_lst, simulate_list_merge, valid_blks\n\tfrom utils.measure import compile_and_time, torch_time\n\tfrom utils.logger import Logger\n\tdef val_df(df, st, end, st_act=None, end_act=None, col=\"\"):\n\t    \"\"\"\n\t    Helper function for pandas DataFrame\n\t    \"\"\"\n\t    mask = (df[\"st\"] == st) & (df[\"end\"] == end)\n\t    if st_act != None:\n", "        mask = mask & (df[\"st_act\"] == st_act)\n\t    if end_act != None:\n\t        mask = mask & (df[\"end_act\"] == end_act)\n\t    filtered = df.loc[mask, :]\n\t    if len(filtered) == 0:\n\t        return None\n\t    assert len(filtered) == 1\n\t    return filtered[col].values[0]\n\tdef generate_time_table(\n\t    model,\n", "    blks: List[Tuple[int, int]],\n\t    arch: str,\n\t    blk_type,\n\t    dir: str,\n\t    tag: str = \"\",\n\t    logger: Logger = None,\n\t    trt: bool = True,\n\t):\n\t    \"\"\"\n\t    This function computes the $T(\\cdot, \\cdot)$ table in the paper\n", "    \"\"\"\n\t    cnt = torch.cuda.device_count()\n\t    env = f\"{socket.gethostname()}_gpu{cnt}\"\n\t    out_shape = model.out_shape\n\t    blks = valid_blks(model)\n\t    blks_dict = {\"id\": [], \"st\": [], \"end\": [], \"time\": [], \"stdev\": []}\n\t    for ind, (st, end) in enumerate(blks):\n\t        if logger:\n\t            now = datetime.now().strftime(\"%m/%d %H:%M:%S\")\n\t            logger.comment(\n", "                f\"ind : {ind:>3} | st : {st:>2} | end : {end:>2}    |     {now}\",\n\t                verb=True,\n\t            )\n\t        else:\n\t            print(f\"ind : {ind:>3} | st : {st:>2} | end : {end:>2}\")\n\t        blks_dict[\"id\"].append(ind)\n\t        blks_dict[\"st\"].append(st)\n\t        blks_dict[\"end\"].append(end)\n\t        if arch == \"learn_mobilenet_v2\":\n\t            conv_lst = get_conv_lst(model.features[1:-1], blk_type, st, end)\n", "        elif arch == \"learn_vgg19\":\n\t            conv_lst = get_conv_lst(model.features, blk_type, st, end)\n\t        else:\n\t            raise NotImplementedError(\"Not supported architecture\")\n\t        if logger:\n\t            logger.comment(str(conv_lst), verb=True)\n\t        else:\n\t            print(conv_lst)\n\t        merged_conv = simulate_list_merge(conv_lst)\n\t        if logger:\n", "            logger.comment(str(merged_conv), verb=True)\n\t        else:\n\t            print(merged_conv)\n\t        merged_model = nn.Sequential(merged_conv, nn.ReLU(inplace=True))\n\t        if trt:\n\t            time, std = compile_and_time(\n\t                merged_model, out_shape[st], f\"st : {st:>2} | end : {end:>2}\"\n\t            )\n\t        else:\n\t            time, std = torch_time(\n", "                merged_model,\n\t                out_shape[st],\n\t                f\"st : {st:>2} | end : {end:>2}\",\n\t                True,\n\t                rep=200,\n\t                warmup=300,\n\t            )\n\t        blks_dict[\"time\"].append(time)\n\t        blks_dict[\"stdev\"].append(std)\n\t    blks = pd.DataFrame(blks_dict)\n", "    if tag:\n\t        filename = f\"time_{tag}.csv\"\n\t    else:\n\t        filename = f\"time_{env}.csv\"\n\t    csv_path = os.path.join(dir, filename)\n\t    blks.to_csv(csv_path, sep=\",\", index=False)\n\tdef generate_optimal_time_table(\n\t    blks: List[Tuple[int, int]],\n\t    dir: str,\n\t    tag: str,\n", "    time_path=\"\",\n\t):\n\t    \"\"\"\n\t    This function computes the $T_{opt}(\\cdot, \\cdot)$ table in the paper\n\t    \"\"\"\n\t    df = pd.read_csv(time_path)\n\t    # In order to speed up the `isin` operation\n\t    blks_set = set(blks.copy())\n\t    df_time_dict = dict()\n\t    df_time_stdev_dict = dict()\n", "    for st, end in blks:\n\t        df_time_dict[f\"{st}_{end}\"] = val_df(df, st=st, end=end, col=\"time\")\n\t        df_time_stdev_dict[f\"{st}_{end}\"] = val_df(df, st=st, end=end, col=\"stdev\")\n\t    opt_tab = dict()\n\t    argmin_lst = dict()\n\t    stdev_tab = dict()\n\t    for st, end in blks:\n\t        g = df_time_dict[f\"{st}_{end}\"]\n\t        stdev = df_time_stdev_dict[f\"{st}_{end}\"]\n\t        assert g != None\n", "        min_val = g\n\t        amin = end\n\t        for j in range(st + 1, end):\n\t            if (st, j) in blks_set and (j, end) in blks_set:\n\t                g_j = df_time_dict[f\"{j}_{end}\"]\n\t                std_j = df_time_stdev_dict[f\"{j}_{end}\"]\n\t                assert g_j != None\n\t                if opt_tab[(st, j)] + g_j < min_val:\n\t                    min_val = opt_tab[(st, j)] + g_j\n\t                    stdev = stdev_tab[(st, j)] + std_j\n", "                    amin = j\n\t        opt_tab[(st, end)] = min_val\n\t        stdev_tab[(st, end)] = stdev\n\t        if amin != end:\n\t            argmin_lst[(st, end)] = argmin_lst[(st, amin)] + [end]\n\t        else:\n\t            argmin_lst[(st, end)] = [st, end]\n\t    blks_dict = {\"id\": [], \"st\": [], \"end\": [], \"time\": [], \"stdev\": [], \"breaks\": []}\n\t    for ind, (st, end) in enumerate(blks):\n\t        blks_dict[\"id\"].append(ind)\n", "        blks_dict[\"st\"].append(st)\n\t        blks_dict[\"end\"].append(end)\n\t        blks_dict[\"time\"].append(opt_tab[(st, end)])\n\t        blks_dict[\"stdev\"].append(stdev_tab[(st, end)])\n\t        blks_dict[\"breaks\"].append(\",\".join([str(i) for i in argmin_lst[(st, end)]]))\n\t    blks = pd.DataFrame(blks_dict)\n\t    if tag:\n\t        filename = f\"opt_time_{tag}.csv\"\n\t    else:\n\t        filename = f\"opt_time.csv\"\n", "    csv_path = os.path.join(dir, filename)\n\t    blks.to_csv(csv_path, sep=\",\", index=False)\n\tdef generate_ext_imp_table(\n\t    ext_blks: List[Tuple[int, int, int, int]],\n\t    act_num: int,\n\t    dir: str,\n\t    imp_path=\"\",\n\t    score: str = \"val_acc\",\n\t    default: set = {},\n\t    norm: str = \"default\",\n", "    alpha: float = 1.0,\n\t):\n\t    \"\"\"\n\t    This function computes the $I_{ext}(\\cdot, \\cdot, \\cdot, \\cdot)$ table in the paper\n\t    \"\"\"\n\t    df_imp = pd.read_csv(imp_path)\n\t    if norm == \"default\":\n\t        # Normalize with default\n\t        mask1 = df_imp[\"end\"] - df_imp[\"st\"] == 1\n\t        mask2 = (df_imp[\"st_act\"] == 1) == (df_imp[\"st\"].isin(default))\n", "        mask3 = (df_imp[\"end_act\"] == 1) == (df_imp[\"end\"].isin(default))\n\t        mask4 = (df_imp[\"st\"] == 0) & (df_imp[\"end\"] == 1)\n\t        mask = (mask1 & mask2 & mask3) | mask4\n\t        assert sum(mask) == act_num\n\t    elif norm == \"single\":\n\t        mask = df_imp[\"end\"] - df_imp[\"st\"] == 1\n\t    elif norm == \"all\":\n\t        mask = df_imp[\"st_act\"].isin([0, 1])\n\t    else:\n\t        raise NotImplementedError()\n", "    mean = df_imp.loc[mask, :].mean()[score]\n\t    df_imp[score] -= alpha * mean\n\t    print(mean)\n\t    print(alpha * mean)\n\t    df_imp_dict = dict()\n\t    for m in range(1, act_num + 1):\n\t        for i in range(0, m):\n\t            for j in range(2):\n\t                for a in range(2):\n\t                    df_imp_dict[f\"{i}_{m}_{j}_{a}\"] = val_df(\n", "                        df_imp, st=i, end=m, st_act=j, end_act=a, col=score\n\t                    )\n\t    # In order to speed up the `isin` operation\n\t    ext_blks_set = set(ext_blks.copy())\n\t    ext_tab = dict()\n\t    argmax_lst = dict()\n\t    \"\"\"\n\t    I_{ext}(st, end, a) \\leftarrow \\max_j(I_{ext}(st, j, 0) + h(j, end, 0, a))\n\t    \"\"\"\n\t    for (st, end, st_act, end_act) in ext_blks:\n", "        h = df_imp_dict[f\"{st}_{end}_{st_act}_{end_act}\"]\n\t        assert h != None\n\t        max_val = h\n\t        amax = end\n\t        for j in range(st + 1, end):\n\t            if (st, j, st_act, 0) in ext_tab and (j, end, 0, end_act) in ext_blks_set:\n\t                h_j = df_imp_dict[f\"{j}_{end}_{0}_{end_act}\"]\n\t                assert h_j != None\n\t                if ext_tab[(st, j, st_act, 0)] + h_j > max_val:\n\t                    max_val = ext_tab[(st, j, st_act, 0)] + h_j\n", "                    amax = j\n\t        ext_tab[(st, end, st_act, end_act)] = max_val\n\t        if amax != end:\n\t            sub = argmax_lst[(st, amax, st_act, 0)]\n\t            argmax_lst[(st, end, st_act, end_act)] = sub + [end]\n\t        else:\n\t            argmax_lst[(st, end, st_act, end_act)] = [st, end]\n\t    blks_dict = {\n\t        \"id\": [],\n\t        \"st\": [],\n", "        \"end\": [],\n\t        \"st_act\": [],\n\t        \"end_act\": [],\n\t        \"imp\": [],\n\t        \"breaks\": [],\n\t    }\n\t    for ind, (st, end, st_act, end_act) in enumerate(ext_blks):\n\t        blks_dict[\"id\"].append(ind)\n\t        blks_dict[\"st\"].append(st)\n\t        blks_dict[\"st_act\"].append(st_act)\n", "        blks_dict[\"end\"].append(end)\n\t        blks_dict[\"end_act\"].append(end_act)\n\t        blks_dict[\"imp\"].append(ext_tab[(st, end, st_act, end_act)])\n\t        blks_dict[\"breaks\"].append(\n\t            \",\".join([str(i) for i in argmax_lst[(st, end, st_act, end_act)]])\n\t        )\n\t    blks = pd.DataFrame(blks_dict)\n\t    filename = f\"ext_importance\"\n\t    filename += f\"_s_{score}\"\n\t    if norm != \"default\":\n", "        filename += f\"_n_{norm}\"\n\t    if alpha != 1.0:\n\t        filename += f\"_a_{alpha}\"\n\t    filename += \".csv\"\n\t    csv_path = os.path.join(dir, filename)\n\t    blks.to_csv(csv_path, sep=\",\", index=False)\n\tdef optimal_patterns(\n\t    flt_time_limit: float,\n\t    act_num: int,\n\t    opt_time_path: str,\n", "    ext_imp_path: str,\n\t    prec: float = 100,\n\t    verbose: bool = False,\n\t    logger: Logger = None,\n\t):\n\t    df_opt_time = pd.read_csv(opt_time_path)\n\t    df_ext_imp = pd.read_csv(ext_imp_path)\n\t    df_opt_time_dict = dict()\n\t    df_opt_time_brks_dict = dict()\n\t    df_ext_imp_dict = dict()\n", "    for m in range(1, act_num + 1):\n\t        for i in range(0, m):\n\t            df_opt_time_dict[f\"{i}_{m}\"] = val_df(df_opt_time, st=i, end=m, col=\"time\")\n\t            df_opt_time_brks_dict[f\"{i}_{m}\"] = val_df(\n\t                df_opt_time, st=i, end=m, col=\"breaks\"\n\t            )\n\t            for j in range(2):\n\t                for a in range(2):\n\t                    df_ext_imp_dict[f\"{i}_{m}_{j}_{a}\"] = val_df(\n\t                        df_ext_imp, st=i, end=m, st_act=j, end_act=a, col=\"imp\"\n", "                    )\n\t    time_limit = int(flt_time_limit * prec)\n\t    # Sum of maximum importance\n\t    dp_tab = dict()\n\t    # Sum of optimal time\n\t    dp_time_tab = dict()\n\t    # Initialization\n\t    for t in range(0, time_limit):\n\t        dp_tab[(t, 0, 1)] = 0\n\t        dp_time_tab[(t, 0, 1)] = 0\n", "    max_tab = dict()\n\t    mpos_tab = dict()\n\t    for m in range(1, act_num + 1):\n\t        t_min = float(\"Inf\")\n\t        for i in range(0, m):\n\t            g_flt = df_opt_time_dict[f\"{i}_{m}\"]\n\t            if g_flt == None:\n\t                continue\n\t            cand = int(g_flt * prec)\n\t            if t_min > cand:\n", "                t_min = cand\n\t        # Impossible time limit\n\t        if t_min > time_limit:\n\t            if logger:\n\t                logger.comment(f\"Impossible time limit : {time_limit}\", verb=True)\n\t            else:\n\t                print(f\"Impossible time limit : {time_limit}\")\n\t            exit()\n\t        for t, a in product(range(t_min, time_limit + 1), range(2)):\n\t            if verbose and (t - t_min) % 1000 == 0:\n", "                now = datetime.now().strftime(\"%m/%d %H:%M:%S\")\n\t                prg = f\"m = {m:>2} & a = {a:>2} : {t - t_min:>5} / {time_limit - t_min:>5}       ||      {now}\"\n\t                if logger:\n\t                    logger.comment(prg, verb=True)\n\t                else:\n\t                    print(prg)\n\t            max_imp = float(\"-Inf\")\n\t            for i, j in product(range(m), range(2)):\n\t                g_flt = df_opt_time_dict[f\"{i}_{m}\"]\n\t                h = df_ext_imp_dict[f\"{i}_{m}_{j}_{a}\"]\n", "                if g_flt == None or h == None:\n\t                    assert h == None\n\t                    continue\n\t                g = int(g_flt * prec)\n\t                # Skip for impossible combination\n\t                if not (t - g, i, j) in dp_tab:\n\t                    continue\n\t                cand = dp_tab[(t - g, i, j)] + h\n\t                cand_time = dp_time_tab[(t - g, i, j)] + g\n\t                # For numerical stability of importance value\n", "                # (we round the values before comparison between float)\n\t                r_imp, r_cand = round(max_imp, 7), round(cand, 7)\n\t                # Choose maximum imp (on the tie we choose the one with faster time)\n\t                if r_imp < r_cand or (r_imp == r_cand and cand_time < opt_time):\n\t                    max_imp = cand\n\t                    opt_time = cand_time\n\t                    argmax_imp = (t - g, i, j)\n\t                    brkstr = df_opt_time_brks_dict[f\"{i}_{m}\"]\n\t                    brks = set([int(pos) for pos in brkstr.split(\",\")])\n\t                    if i == 0:\n", "                        mpos = brks\n\t                    else:\n\t                        mpos = set.union(brks, mpos_tab[(t - g, i, j)])\n\t            # Skip for impossible combination (or combination that we do not take into account)\n\t            if max_imp != float(\"-Inf\"):\n\t                dp_tab[(t, m, a)] = max_imp\n\t                dp_time_tab[(t, m, a)] = opt_time\n\t                max_tab[(t, m, a)] = argmax_imp\n\t                mpos_tab[(t, m, a)] = mpos\n\t    t, m = time_limit, act_num\n", "    if all(not (t, m, a) in dp_tab for a in range(0, 2)):\n\t        if logger:\n\t            logger.comment(f\"Impossible time limit : {t}\", verb=True)\n\t        else:\n\t            print(f\"Impossible time limit : {t}\")\n\t        exit()\n\t    ends = []\n\t    for a in range(2):\n\t        if (t, m, a) in dp_tab:\n\t            ends.append(dp_tab[(t, m, a)])\n", "        else:\n\t            ends.append(float(\"-Inf\"))\n\t    sol_a = np.argmax(ends)\n\t    a = sol_a\n\t    opt_m_pos = {m}\n\t    blk_ends = {m}\n\t    act_pos = {m} if bool(sol_a) else set()\n\t    while m > 0:\n\t        opt_m_pos = set.union(opt_m_pos, mpos_tab[(t, m, a)])\n\t        blk_ends = set.union(blk_ends, {m})\n", "        act_pos = set.union(act_pos, {m}) if bool(a) else act_pos\n\t        t, m, a = max_tab[(t, m, a)]\n\t    assert dp_tab[(t, m, a)] == 0\n\t    opt_m_pos -= {0}\n\t    st, st_act = 0, 1\n\t    imp_sum, int_time_sum, flt_time_sum = 0, 0, 0\n\t    for pos in sorted(list(blk_ends)):\n\t        a = int(pos in act_pos)\n\t        if pos == 0:\n\t            continue\n", "        imp = df_ext_imp_dict[f\"{st}_{pos}_{st_act}_{a}\"]\n\t        flt_time = df_opt_time_dict[f\"{st}_{pos}\"]\n\t        assert imp != None\n\t        imp_sum += imp\n\t        int_time_sum += int(flt_time * prec)\n\t        flt_time_sum += flt_time\n\t        st, st_act = pos, a\n\t    assert imp_sum == dp_tab[(time_limit, act_num, sol_a)]\n\t    return (act_pos, opt_m_pos, imp_sum, int_time_sum, flt_time_sum)\n\tdef optimal_merge_pattern(act_pos, act_num, time_path):\n", "    df = pd.read_csv(time_path)\n\t    dp_tab = {0: 0}\n\t    min_tab = dict()\n\t    for m in range(1, act_num + 1):\n\t        starts = set.union(act_pos, {0})\n\t        t_max = max([x for x in starts if x < m])\n\t        min_time = float(\"Inf\")\n\t        for t in range(t_max, m):\n\t            g = val_df(df, st=t, end=m, col=\"time\")\n\t            if g == None:\n", "                continue\n\t            cand = dp_tab[t] + g\n\t            if min_time > cand:\n\t                min_time = cand\n\t                argmin_time = t\n\t        dp_tab[m] = min_time\n\t        min_tab[m] = argmin_time\n\t    m = act_num\n\t    m_pos = {m}\n\t    while m > 0:\n", "        m_pos = set.union(m_pos, {m})\n\t        m = min_tab[m]\n\t    st, stdev = 0, 0\n\t    for pos in sorted(list(m_pos)):\n\t        if pos == 0:\n\t            continue\n\t        std = val_df(df, st=st, end=pos, col=\"stdev\")\n\t        assert std != None\n\t        stdev += std\n\t        st = pos\n", "    assert act_pos.issubset(m_pos)\n\t    return (m_pos, dp_tab[act_num], stdev)\n"]}
{"filename": "utils/train.py", "chunked_list": ["# Code started from  https://github.com/d-li14/mobilenetv2.pytorch.git\n\timport os\n\timport time\n\tfrom math import cos, pi\n\timport torch\n\timport torch.nn.parallel\n\timport torch.optim\n\timport torch.utils.data\n\timport torch.utils.data.distributed\n\tfrom utils.logger import AverageMeter\n", "from utils.misc import KLLossSoft\n\t# progress bar\n\t# https://github.com/verigak/progress\n\tfrom progress.bar import Bar as Bar\n\t__all__ = [\"accuracy\", \"validate\", \"train\"]\n\tdef train(\n\t    train_loader,\n\t    model,\n\t    criterion,\n\t    optimizer,\n", "    epoch,\n\t    total_epochs,\n\t    lr_decay,\n\t    reg=None,\n\t    logger=None,\n\t    args=None,\n\t):\n\t    bar = Bar(\"Processing\", max=len(train_loader))\n\t    batch_time = AverageMeter()\n\t    data_time = AverageMeter()\n", "    losses = AverageMeter()\n\t    top1 = AverageMeter()\n\t    top5 = AverageMeter()\n\t    # switch to train mode\n\t    model.train()\n\t    end = time.time()\n\t    logger.comment(\"-----------------------------\")\n\t    for ind, (input, target) in enumerate(train_loader):\n\t        adjust_learning_rate(\n\t            optimizer,\n", "            epoch,\n\t            ind,\n\t            len(train_loader),\n\t            total_epochs,\n\t            args.lr,\n\t            lr_decay,\n\t            args.schedule,\n\t            args.gamma,\n\t        )\n\t        # measure data loading time\n", "        data_time.update(time.time() - end)\n\t        input = input.cuda(non_blocking=True)\n\t        target = target.cuda(non_blocking=True)\n\t        # compute output\n\t        output = model(input)\n\t        if isinstance(criterion, KLLossSoft):\n\t            with torch.no_grad():\n\t                soft_logits = criterion.teacher(input)\n\t            loss = criterion(output, soft_logits, target)\n\t        else:\n", "            loss = criterion(output, target)\n\t        if reg != None:\n\t            loss += args.lamb * model.module.regularizer(reg)\n\t        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n\t        top1.update(prec1.item(), input.size(0))\n\t        top5.update(prec5.item(), input.size(0))\n\t        losses.update(loss.item(), input.size(0))\n\t        # compute gradient and do SGD step\n\t        optimizer.zero_grad()\n\t        loss.backward()\n", "        if args.aug:\n\t            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n\t        optimizer.step()\n\t        # measure elapsed time\n\t        batch_time.update(time.time() - end)\n\t        end = time.time()\n\t        # plot progress\n\t        plot_progress = \"({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\".format(\n\t            batch=ind + 1,\n\t            size=len(train_loader),\n", "            data=data_time.avg,\n\t            bt=batch_time.avg,\n\t            total=bar.elapsed_td,\n\t            eta=bar.eta_td,\n\t            loss=losses.avg,\n\t            top1=top1.avg,\n\t            top5=top5.avg,\n\t        )\n\t        bar.suffix = plot_progress\n\t        bar.next()\n", "        # For logging, print newline\n\t        if (ind + 1) % args.print_freq == 0:\n\t            logger.comment(plot_progress)\n\t        # if i == len(train_loader.dataloader) - 1:\n\t        #     top1, _ = accuracy(output, target, topk=(1, 5))\n\t        if args.debug and ind > 5:\n\t            break\n\t    bar.finish()\n\t    logger.comment(\"-----------------------------\")\n\t    return (losses.avg, top1.avg)\n", "def validate(val_loader, model, criterion, args):\n\t    bar = Bar(\"Processing\", max=len(val_loader))\n\t    batch_time = AverageMeter()\n\t    data_time = AverageMeter()\n\t    losses = AverageMeter()\n\t    top1 = AverageMeter()\n\t    top5 = AverageMeter()\n\t    # switch to evaluate mode\n\t    model.eval()\n\t    end = time.time()\n", "    for i, (input, target) in enumerate(val_loader):\n\t        # measure data loading time\n\t        data_time.update(time.time() - end)\n\t        input = input.cuda(non_blocking=True)\n\t        target = target.cuda(non_blocking=True)\n\t        with torch.no_grad():\n\t            # compute output\n\t            output = model(input)\n\t            if isinstance(criterion, KLLossSoft):\n\t                soft_logits = criterion.teacher(input)\n", "                loss = criterion(output, soft_logits, target)\n\t            else:\n\t                loss = criterion(output, target)\n\t        # measure accuracy and record loss\n\t        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n\t        losses.update(loss.item(), input.size(0))\n\t        top1.update(prec1.item(), input.size(0))\n\t        top5.update(prec5.item(), input.size(0))\n\t        # measure elapsed time\n\t        batch_time.update(time.time() - end)\n", "        end = time.time()\n\t        # plot progress\n\t        bar.suffix = \"({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\".format(\n\t            batch=i + 1,\n\t            size=len(val_loader),\n\t            data=data_time.avg,\n\t            bt=batch_time.avg,\n\t            total=bar.elapsed_td,\n\t            eta=bar.eta_td,\n\t            loss=losses.avg,\n", "            top1=top1.avg,\n\t            top5=top5.avg,\n\t        )\n\t        bar.next()\n\t        if args.debug and i > 5:\n\t            break\n\t    bar.finish()\n\t    return (losses.avg, top1.avg)\n\tdef adjust_learning_rate(\n\t    optimizer, epoch, iteration, num_iter, epochs, lr_init, lr_decay, schedule, gamma\n", "):\n\t    lr = optimizer.param_groups[0][\"lr\"]\n\t    # # This warmup parameter is obsolete\n\t    # wu_epoch = 5 if args.warmup else 0\n\t    # wu_iter = wu_epoch * num_iter\n\t    wu_epoch, wu_iter = 0, 0\n\t    cur_iter = iteration + epoch * num_iter\n\t    max_iter = epochs * num_iter\n\t    if lr_decay == \"step\":\n\t        lr = lr_init * (gamma ** ((cur_iter - wu_iter) / (max_iter - wu_iter)))\n", "    elif lr_decay == \"cos\":\n\t        lr = lr_init * (1 + cos(pi * (cur_iter - wu_iter) / (max_iter - wu_iter))) / 2\n\t    elif lr_decay == \"linear\":\n\t        lr = lr_init * (1 - (cur_iter - wu_iter) / (max_iter - wu_iter))\n\t    elif lr_decay == \"schedule\":\n\t        count = sum([1 for s in schedule if s <= epoch])\n\t        lr = lr_init * pow(gamma, count)\n\t    elif lr_decay == \"const\":\n\t        lr = lr_init\n\t    else:\n", "        raise ValueError(\"Unknown lr mode {}\".format(lr_decay))\n\t    if epoch < wu_epoch:\n\t        lr = lr_init * cur_iter / wu_iter\n\t    for param_group in optimizer.param_groups:\n\t        param_group[\"lr\"] = lr\n\tdef accuracy(output, target, topk=(1,)):\n\t    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n\t    with torch.no_grad():\n\t        maxk = max(topk)\n\t        batch_size = target.size(0)\n", "        _, pred = output.topk(maxk, 1, True, True)\n\t        pred = pred.t()\n\t        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\t        res = []\n\t        for k in topk:\n\t            correct_k = correct[:k].reshape(-1).float().sum(0)\n\t            res.append(correct_k.mul_(100.0 / batch_size))\n\t        return res\n"]}
{"filename": "utils/logger.py", "chunked_list": ["# A simple torch style logger\n\t# (C) Wei YANG 2017\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\t__all__ = [\"Logger\", \"LoggerMonitor\", \"savefig\", \"AverageMeter\"]\n\tdef savefig(fname, dpi=None):\n\t    dpi = 150 if dpi == None else dpi\n\t    plt.savefig(fname, dpi=dpi)\n\tdef plot_overlap(logger, names=None):\n\t    names = logger.names if names == None else names\n", "    numbers = logger.numbers\n\t    for _, name in enumerate(names):\n\t        x = np.arange(len(numbers[name]))\n\t        plt.plot(x, np.asarray(numbers[name]))\n\t    return [logger.title + \"(\" + name + \")\" for name in names]\n\tclass Logger(object):\n\t    \"\"\"Save training process to log file with simple plot function.\"\"\"\n\t    def __init__(self, fpath, title=None, resume=False):\n\t        self.file = None\n\t        self.resume = resume\n", "        self.title = \"\" if title == None else title\n\t        if fpath is not None:\n\t            if resume:\n\t                self.file = open(fpath, \"a\")\n\t            else:\n\t                self.file = open(fpath, \"w\")\n\t    def set_names(self, names):\n\t        # initialize numbers as empty list\n\t        self.numbers = {}\n\t        self.names = names\n", "        for _, name in enumerate(self.names):\n\t            self.file.write(name)\n\t            self.file.write(\"\\t\")\n\t            self.numbers[name] = []\n\t        self.file.write(\"\\n\")\n\t        self.file.flush()\n\t    def append(self, numbers):\n\t        assert len(self.names) == len(numbers), \"Numbers do not match names\"\n\t        for index, num in enumerate(numbers):\n\t            if self.names[index] == \"Epoch\":\n", "                self.file.write(\"{0:3}\".format(int(num)))\n\t            else:\n\t                self.file.write(\"{0:.6f}\".format(num))\n\t            self.file.write(\"\\t\")\n\t            self.numbers[self.names[index]].append(num)\n\t        self.file.write(\"\\n\")\n\t        self.file.flush()\n\t    def comment(self, string, verb=False):\n\t        self.file.write(string)\n\t        self.file.write(\"\\n\")\n", "        self.file.flush()\n\t        if verb:\n\t            print(string)\n\t    def plot(self, names=None):\n\t        names = self.names if names == None else names\n\t        numbers = self.numbers\n\t        for _, name in enumerate(names):\n\t            x = np.arange(len(numbers[name]))\n\t            plt.plot(x, np.asarray(numbers[name]))\n\t        plt.legend([self.title + \"(\" + name + \")\" for name in names])\n", "        plt.grid(True)\n\t    def close(self):\n\t        if self.file is not None:\n\t            self.file.close()\n\tclass LoggerMonitor(object):\n\t    \"\"\"Load and visualize multiple logs.\"\"\"\n\t    def __init__(self, paths):\n\t        \"\"\"paths is a distionary with {name:filepath} pair\"\"\"\n\t        self.loggers = []\n\t        for title, path in paths.items():\n", "            logger = Logger(path, title=title, resume=True)\n\t            self.loggers.append(logger)\n\t    def plot(self, names=None):\n\t        plt.figure()\n\t        plt.subplot(121)\n\t        legend_text = []\n\t        for logger in self.loggers:\n\t            legend_text += plot_overlap(logger, names)\n\t        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n\t        plt.grid(True)\n", "class AverageMeter(object):\n\t    \"\"\"Computes and stores the average and current value\n\t    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n\t    \"\"\"\n\t    def __init__(self):\n\t        self.reset()\n\t    def reset(self):\n\t        self.val = 0\n\t        self.avg = 0\n\t        self.sum = 0\n", "        self.count = 0\n\t    def update(self, val, n=1):\n\t        self.val = val\n\t        self.sum += val * n\n\t        self.count += n\n\t        self.avg = self.sum / self.count\n"]}
{"filename": "utils/__init__.py", "chunked_list": ["\"\"\"Useful utils\n\t\"\"\"\n\tfrom .logger import *\n\tfrom .train import *\n"]}
{"filename": "utils/misc.py", "chunked_list": ["import os\n\timport shutil\n\timport torch\n\timport torch.nn.parallel\n\timport torch.optim\n\timport torch.utils.data\n\timport torch.utils.data.distributed\n\tdef save_checkpoint(state, is_best, checkpoint=\"checkpoint\", filename=\"checkpoint.pth\"):\n\t    filepath = os.path.join(checkpoint, filename)\n\t    torch.save(state, filepath)\n", "    if is_best:\n\t        best_filepath = filepath[:-4]\n\t        best_filepath += \"_best.pth\"\n\t        shutil.copyfile(filepath, best_filepath)\n\tdef load_checkpoint(\n\t    model,\n\t    arch,\n\t    path,\n\t    act_path=None,\n\t    ds_pat=None,\n", "    logger=None,\n\t):\n\t    if os.path.isfile(path):\n\t        source_state = torch.load(path)\n\t        log_tool(f\"=> loading pretrained weight '{path}'\", logger)\n\t        if \"epoch\" in source_state:\n\t            log_tool(f\"=> (epoch {source_state['epoch']})\", logger)\n\t    else:\n\t        log_tool(f\"=> no weight found at '{path}'\", logger)\n\t        exit()\n", "    if act_path == None:\n\t        act_path = path\n\t    if arch in [\"mobilenet_v2\", \"vgg19\"]:\n\t        log_tool(f\"=> loading on the architecture '{arch}'\", logger)\n\t        act_state = dict()\n\t    elif os.path.isfile(act_path):\n\t        act_state = torch.load(act_path)\n\t        log_tool(f\"=> loading activations from '{act_path}'\", logger)\n\t        if \"act_pos\" in act_state:\n\t            log_tool(f\"Number of activations : {len(act_state['act_pos'])}\", logger)\n", "            log_tool(f\"{act_state['act_pos']}\", logger)\n\t        else:\n\t            log_tool(f\"=> no activations found at '{path}'\", logger)\n\t    else:\n\t        log_tool(f\"=> not a valid act_path '{act_path}'\", logger)\n\t        exit()\n\t    if ds_pat != None:\n\t        ds_pattern, compress_k = ds_pat\n\t        # fmt: off\n\t        pat2cmp = {\n", "            \"A\": 11, \"B\": 8, \"C\": 8, \"D\": 6, \"E\": 6, \"F\": 0, \"A10\": 12, \"B10\": 12, \"C10\": 9, \"D10\": 7, \"AR\": 11, \"BR\": 8, \"CR\": 6, \"AR10\": 12, \"BR10\": 9, \"CR10\": 7, \"AR_AUG\": 11, \"BR_AUG\": 8, \"CR_AUG\": 6, \"AR10_AUG\": 12, \"BR10_AUG\": 9, \"CR10_AUG\": 7\n\t        }\n\t        # fmt: on\n\t        assert ds_pattern != \"none\" and arch == \"dep_shrink_mobilenet_v2\"\n\t        assert compress_k == pat2cmp[ds_pattern]\n\t        log_tool(f\"=> training from ds_pattern '{ds_pattern}'\", logger)\n\t        source_state[\"compress_k\"] = compress_k\n\t        model.module.load_pattern(ds_pattern)\n\t    if arch == \"dep_shrink_mobilenet_v2\":\n\t        model.module.compress_k = source_state[\"compress_k\"]\n", "    # `act_pos` for finetuned/merged network weights\n\t    if \"act_pos\" in act_state:\n\t        a_pos = act_state[\"act_pos\"]\n\t        source_state[\"act_pos\"] = a_pos\n\t        # `merge_pos` for merged network weights\n\t        if \"merge_pos\" in act_state:\n\t            log_tool(f\"=> loading optimal merge pattern from '{act_path}'\", logger)\n\t            log_tool(f\"{act_state['merge_pos']}\", logger)\n\t            m_pos = act_state[\"merge_pos\"]\n\t            source_state[\"merge_pos\"] = m_pos\n", "        else:\n\t            m_pos = None\n\t        if arch in [\"learn_mobilenet_v2\", \"learn_vgg19\"]:\n\t            model.module.fix_act(a_pos, m_pos)\n\t        else:\n\t            model.module.fix_act(a_pos)\n\t        if \"merged\" in act_state:\n\t            model.to(\"cpu\")\n\t            if m_pos:\n\t                assert arch in [\"learn_mobilenet_v2\", \"learn_vgg19\"]\n", "                model.module.merge(act_pos=act_state[\"act_pos\"], merge_pos=m_pos)\n\t            else:\n\t                model.module.merge(act_pos=act_state[\"act_pos\"])\n\t            model.to(\"cuda\")\n\t    model.load_state_dict(source_state[\"state_dict\"], strict=False)\n\t    del source_state[\"state_dict\"]\n\t    return source_state\n\tdef cp_state(new_state, source_state, name):\n\t    if name in source_state:\n\t        new_state[name] = source_state[name]\n", "def print_act_pos(module, source_state):\n\t    with torch.no_grad():\n\t        if \"act_pos\" in source_state:\n\t            act_pos = source_state[\"act_pos\"]\n\t            print(\"learned activation is below\")\n\t            print(sorted(list(act_pos)))\n\t            print(f\"{len(act_pos)} alive\")\n\tdef log_tool(string, logger, mode=\"print\"):\n\t    if mode == \"print\":\n\t        print(string)\n", "        if logger != None:\n\t            logger.comment(f\"{string}\")\n\t    elif mode == \"opt\":\n\t        logger.comment(\"\\n-----------------------------\\n\")\n\t        logger.comment(string)\n\t        logger.comment(\"\\n-----------------------------\\n\")\n\t    elif mode == \"ap\":\n\t        logger.comment(\"-----------------------------\")\n\t        logger.comment(\"learned activation is below\")\n\t        logger.comment(string)\n", "        logger.comment(\"-----------------------------\")\n\t# Implementation adapted from AlphaNet - https://github.com/facebookresearch/AlphaNet\n\tclass KLLossSoft(torch.nn.modules.loss._Loss):\n\t    \"\"\"inplace distillation for image classification\n\t    output: output logits of the student network\n\t    target: output logits of the teacher network\n\t    T: temperature\n\t    \"\"\"\n\t    def __init__(\n\t        self, alpha, teacher, size_average=None, reduce=None, reduction: str = \"mean\"\n", "    ) -> None:\n\t        super().__init__(size_average, reduce, reduction)\n\t        self.alpha = alpha\n\t        self.teacher = teacher\n\t    def forward(self, output, soft_logits, target, temperature=1.0):\n\t        output, soft_logits = output / temperature, soft_logits / temperature\n\t        soft_target_prob = torch.nn.functional.softmax(soft_logits, dim=1)\n\t        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)\n\t        kd_loss = -torch.sum(soft_target_prob * output_log_prob, dim=1)\n\t        if target is not None:\n", "            target = torch.zeros_like(output).scatter(1, target.view(-1, 1), 1)\n\t            target = target.unsqueeze(1)\n\t            output_log_prob = output_log_prob.unsqueeze(2)\n\t            ce_loss = -torch.bmm(target, output_log_prob).squeeze()\n\t            loss = (\n\t                self.alpha * temperature * temperature * kd_loss\n\t                + (1.0 - self.alpha) * ce_loss\n\t            )\n\t        else:\n\t            loss = kd_loss\n", "        if self.reduction == \"mean\":\n\t            return loss.mean()\n\t        elif self.reduction == \"sum\":\n\t            return loss.sum()\n\t        return loss\n"]}
{"filename": "utils/loaders.py", "chunked_list": ["import os\n\timport torch\n\timport torch.nn.parallel\n\timport torch.optim\n\timport torch.utils.data\n\timport torch.utils.data.distributed\n\timport torchvision.transforms as transforms\n\timport utils.datasets as datasets\n\tfrom timm.data import create_transform\n\tdef get_train_loader(\n", "    data_path,\n\t    batch_size,\n\t    workers=5,\n\t    _worker_init_fn=None,\n\t    input_size=224,\n\t    nclass=1000,\n\t    holdout=None,\n\t    timm_aug=False,\n\t):\n\t    traindir = os.path.join(data_path, \"train\")\n", "    normalize = transforms.Normalize(\n\t        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n\t    )\n\t    if holdout == \"val\":\n\t        aug = transforms.Compose(\n\t            [\n\t                transforms.Resize(256),\n\t                transforms.CenterCrop(input_size),\n\t                transforms.ToTensor(),\n\t                normalize,\n", "            ]\n\t        )\n\t    else:\n\t        if timm_aug:\n\t            aug = create_transform(\n\t                input_size=224,\n\t                is_training=True,\n\t                color_jitter=0.4,\n\t                auto_augment=\"rand-m5-mstd0.5-inc1\",\n\t                re_prob=0.25,\n", "                re_mode=\"pixel\",\n\t                re_count=1,\n\t                interpolation=\"bicubic\",\n\t            )\n\t        else:\n\t            aug = transforms.Compose(\n\t                [\n\t                    transforms.RandomResizedCrop(input_size),\n\t                    transforms.RandomHorizontalFlip(),\n\t                    transforms.ToTensor(),\n", "                    normalize,\n\t                ]\n\t            )\n\t    train_dataset = datasets.ImageFolder(traindir, aug, nclass=nclass, holdout=holdout)\n\t    if torch.distributed.is_initialized():\n\t        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n\t    else:\n\t        train_sampler = None\n\t    if holdout == \"val\":\n\t        shfl = False\n", "    else:\n\t        shfl = train_sampler is None\n\t    train_loader = torch.utils.data.DataLoader(\n\t        train_dataset,\n\t        sampler=train_sampler,\n\t        batch_size=batch_size,\n\t        shuffle=shfl,\n\t        num_workers=workers,\n\t        worker_init_fn=_worker_init_fn,\n\t        pin_memory=True,\n", "    )\n\t    return train_loader, len(train_loader)\n\tdef get_val_loader(\n\t    data_path, batch_size, workers=5, _worker_init_fn=None, input_size=224, nclass=1000\n\t):\n\t    valdir = os.path.join(data_path, \"val\")\n\t    normalize = transforms.Normalize(\n\t        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n\t    )\n\t    val_dataset = datasets.ImageFolder(\n", "        valdir,\n\t        transforms.Compose(\n\t            [\n\t                transforms.Resize(256),\n\t                transforms.CenterCrop(input_size),\n\t                transforms.ToTensor(),\n\t                normalize,\n\t            ]\n\t        ),\n\t        nclass=nclass,\n", "    )\n\t    if torch.distributed.is_initialized():\n\t        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n\t    else:\n\t        val_sampler = None\n\t    val_loader = torch.utils.data.DataLoader(\n\t        val_dataset,\n\t        sampler=val_sampler,\n\t        batch_size=batch_size,\n\t        shuffle=False,\n", "        num_workers=workers,\n\t        worker_init_fn=_worker_init_fn,\n\t        pin_memory=True,\n\t    )\n\t    return val_loader, len(val_loader)\n"]}
{"filename": "utils/datasets.py", "chunked_list": ["import os\n\timport os.path\n\tfrom typing import Any, Callable, cast, Dict, List, Optional, Tuple\n\tfrom typing import Union\n\tfrom PIL import Image\n\tfrom torchvision.datasets.vision import VisionDataset\n\tdef has_file_allowed_extension(\n\t    filename: str, extensions: Union[str, Tuple[str, ...]]\n\t) -> bool:\n\t    \"\"\"Checks if a file is an allowed extension.\n", "    Args:\n\t        filename (string): path to a file\n\t        extensions (tuple of strings): extensions to consider (lowercase)\n\t    Returns:\n\t        bool: True if the filename ends with one of given extensions\n\t    \"\"\"\n\t    return filename.lower().endswith(\n\t        extensions if isinstance(extensions, str) else tuple(extensions)\n\t    )\n\tdef is_image_file(filename: str) -> bool:\n", "    \"\"\"Checks if a file is an allowed image extension.\n\t    Args:\n\t        filename (string): path to a file\n\t    Returns:\n\t        bool: True if the filename ends with a known image extension\n\t    \"\"\"\n\t    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n\tdef find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n\t    \"\"\"Finds the class folders in a dataset.\n\t    See :class:`DatasetFolder` for details.\n", "    \"\"\"\n\t    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n\t    if not classes:\n\t        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n\t    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n\t    return classes, class_to_idx\n\tdef get_holdout():\n\t    \"\"\"Returns a list of filename of a holdout validation data\n\t    Written in \"utils/txt/holdout_val.txt\" file\n\t    \"\"\"\n", "    with open(\"utils/txt/holdout_val.txt\") as f:\n\t        fnames = f.read().splitlines()\n\t    return set(fnames)\n\tdef make_dataset(\n\t    directory: str,\n\t    class_to_idx: Optional[Dict[str, int]] = None,\n\t    extensions: Optional[Union[str, Tuple[str, ...]]] = None,\n\t    is_valid_file: Optional[Callable[[str], bool]] = None,\n\t    holdout: Optional[str] = None,\n\t) -> List[Tuple[str, int]]:\n", "    \"\"\"Generates a list of samples of a form (path_to_sample, class).\n\t    See :class:`DatasetFolder` for details.\n\t    Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function\n\t    by default.\n\t    \"\"\"\n\t    directory = os.path.expanduser(directory)\n\t    if class_to_idx is None:\n\t        _, class_to_idx = find_classes(directory)\n\t    elif not class_to_idx:\n\t        raise ValueError(\n", "            \"'class_to_index' must have at least one entry to collect any samples.\"\n\t        )\n\t    both_none = extensions is None and is_valid_file is None\n\t    both_something = extensions is not None and is_valid_file is not None\n\t    if both_none or both_something:\n\t        raise ValueError(\n\t            \"Both extensions and is_valid_file cannot be None or not None at the same time\"\n\t        )\n\t    if extensions is not None:\n\t        def is_valid_file(x: str) -> bool:\n", "            return has_file_allowed_extension(x, extensions)  # type: ignore[arg-type]\n\t    is_valid_file = cast(Callable[[str], bool], is_valid_file)\n\t    if holdout != None:\n\t        assert holdout in [\"train\", \"val\"]\n\t        holdout_fnames = get_holdout()\n\t    instances = []\n\t    available_classes = set()\n\t    for target_class in sorted(class_to_idx.keys()):\n\t        class_index = class_to_idx[target_class]\n\t        target_dir = os.path.join(directory, target_class)\n", "        if not os.path.isdir(target_dir):\n\t            continue\n\t        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n\t            for fname in sorted(fnames):\n\t                path = os.path.join(root, fname)\n\t                if is_valid_file(path):\n\t                    item = path, class_index\n\t                    if holdout == \"train\" and fname in holdout_fnames:\n\t                        continue\n\t                    elif holdout == \"val\" and not fname in holdout_fnames:\n", "                        continue\n\t                    instances.append(item)\n\t                    if target_class not in available_classes:\n\t                        available_classes.add(target_class)\n\t    empty_classes = set(class_to_idx.keys()) - available_classes\n\t    if empty_classes:\n\t        msg = (\n\t            f\"Found no valid file for the classes {', '.join(sorted(empty_classes))}. \"\n\t        )\n\t        if extensions is not None:\n", "            msg += f\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\n\t        raise FileNotFoundError(msg)\n\t    return instances\n\tclass DatasetFolder(VisionDataset):\n\t    \"\"\"A generic data loader.\n\t    This default directory structure can be customized by overriding the\n\t    :meth:`find_classes` method.\n\t    Args:\n\t        root (string): Root directory path.\n\t        loader (callable): A function to load a sample given its path.\n", "        extensions (tuple[string]): A list of allowed extensions.\n\t            both extensions and is_valid_file should not be passed.\n\t        transform (callable, optional): A function/transform that takes in\n\t            a sample and returns a transformed version.\n\t            E.g, ``transforms.RandomCrop`` for images.\n\t        target_transform (callable, optional): A function/transform that takes\n\t            in the target and transforms it.\n\t        is_valid_file (callable, optional): A function that takes path of a file\n\t            and check if the file is a valid file (used to check of corrupt files)\n\t            both extensions and is_valid_file should not be passed.\n", "     Attributes:\n\t        classes (list): List of the class names sorted alphabetically.\n\t        class_to_idx (dict): Dict with items (class_name, class_index).\n\t        samples (list): List of (sample path, class_index) tuples\n\t        targets (list): The class_index value for each image in the dataset\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        root: str,\n\t        loader: Callable[[str], Any],\n", "        extensions: Optional[Tuple[str, ...]] = None,\n\t        transform: Optional[Callable] = None,\n\t        target_transform: Optional[Callable] = None,\n\t        is_valid_file: Optional[Callable[[str], bool]] = None,\n\t        nclass: int = 1000,\n\t        holdout: Optional[str] = None,\n\t    ) -> None:\n\t        super().__init__(root, transform=transform, target_transform=target_transform)\n\t        if nclass < 1000:\n\t            self.classes, self.class_to_idx = self.find_subclasses(nclass=nclass)\n", "        else:\n\t            self.classes, self.class_to_idx = self.find_classes(self.root)\n\t        self.loader = loader\n\t        self.extensions = extensions\n\t        self.samples = self.make_dataset(\n\t            self.root, self.class_to_idx, self.extensions, is_valid_file, holdout\n\t        )\n\t        self.targets = [s[1] for s in self.samples]\n\t    @staticmethod\n\t    def make_dataset(\n", "        directory: str,\n\t        class_to_idx: Dict[str, int],\n\t        extensions: Optional[Tuple[str, ...]] = None,\n\t        is_valid_file: Optional[Callable[[str], bool]] = None,\n\t        holdout: Optional[str] = None,\n\t    ) -> List[Tuple[str, int]]:\n\t        \"\"\"Generates a list of samples of a form (path_to_sample, class).\n\t        This can be overridden to e.g. read files from a compressed zip file instead of from the disk.\n\t        Args:\n\t            directory (str): root dataset directory, corresponding to ``self.root``.\n", "            class_to_idx (Dict[str, int]): Dictionary mapping class name to class index.\n\t            extensions (optional): A list of allowed extensions.\n\t                Either extensions or is_valid_file should be passed. Defaults to None.\n\t            is_valid_file (optional): A function that takes path of a file\n\t                and checks if the file is a valid file\n\t                (used to check of corrupt files) both extensions and\n\t                is_valid_file should not be passed. Defaults to None.\n\t        Raises:\n\t            ValueError: In case ``class_to_idx`` is empty.\n\t            ValueError: In case ``extensions`` and ``is_valid_file`` are None or both are not None.\n", "            FileNotFoundError: In case no valid file was found for any class.\n\t        Returns:\n\t            List[Tuple[str, int]]: samples of a form (path_to_sample, class)\n\t        \"\"\"\n\t        if class_to_idx is None:\n\t            # prevent potential bug since make_dataset() would use the class_to_idx logic of the\n\t            # find_classes() function, instead of using that of the find_classes() method, which\n\t            # is potentially overridden and thus could have a different logic.\n\t            raise ValueError(\"The class_to_idx parameter cannot be None.\")\n\t        return make_dataset(\n", "            directory,\n\t            class_to_idx,\n\t            extensions=extensions,\n\t            is_valid_file=is_valid_file,\n\t            holdout=holdout,\n\t        )\n\t    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:\n\t        \"\"\"Find the class folders in a dataset structured as follows::\n\t            directory/\n\t             class_x\n", "                xxx.ext\n\t                xxy.ext\n\t                ...\n\t                    xxz.ext\n\t             class_y\n\t                 123.ext\n\t                 nsdf3.ext\n\t                 ...\n\t                 asd932_.ext\n\t        This method can be overridden to only consider\n", "        a subset of classes, or to adapt to a different dataset directory structure.\n\t        Args:\n\t            directory(str): Root directory path, corresponding to ``self.root``\n\t        Raises:\n\t            FileNotFoundError: If ``dir`` has no class folders.\n\t        Returns:\n\t            (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\n\t        \"\"\"\n\t        return find_classes(directory)\n\t    def find_subclasses(self, nclass=100):\n", "        \"\"\"Finds the class folders in a dataset.\"\"\"\n\t        with open(\"utils/txt/class100.txt\", \"r\") as f:\n\t            classes = f.read().splitlines()\n\t        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n\t        assert len(classes) == nclass\n\t        return classes, class_to_idx\n\t    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n\t        \"\"\"\n\t        Args:\n\t            index (int): Index\n", "        Returns:\n\t            tuple: (sample, target) where target is class_index of the target class.\n\t        \"\"\"\n\t        path, target = self.samples[index]\n\t        sample = self.loader(path)\n\t        if self.transform is not None:\n\t            sample = self.transform(sample)\n\t        if self.target_transform is not None:\n\t            target = self.target_transform(target)\n\t        return sample, target\n", "    def __len__(self) -> int:\n\t        return len(self.samples)\n\tIMG_EXTENSIONS = (\n\t    \".jpg\",\n\t    \".jpeg\",\n\t    \".png\",\n\t    \".ppm\",\n\t    \".bmp\",\n\t    \".pgm\",\n\t    \".tif\",\n", "    \".tiff\",\n\t    \".webp\",\n\t)\n\tdef pil_loader(path: str) -> Image.Image:\n\t    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n\t    with open(path, \"rb\") as f:\n\t        img = Image.open(f)\n\t        return img.convert(\"RGB\")\n\t# TODO: specify the return type\n\tdef accimage_loader(path: str) -> Any:\n", "    import accimage\n\t    try:\n\t        return accimage.Image(path)\n\t    except OSError:\n\t        # Potentially a decoding problem, fall back to PIL.Image\n\t        return pil_loader(path)\n\tdef default_loader(path: str) -> Any:\n\t    from torchvision import get_image_backend\n\t    if get_image_backend() == \"accimage\":\n\t        return accimage_loader(path)\n", "    else:\n\t        return pil_loader(path)\n\tclass ImageFolder(DatasetFolder):\n\t    \"\"\"A generic data loader where the images are arranged in this way by default: ::\n\t        root/dog/xxx.png\n\t        root/dog/xxy.png\n\t        root/dog/[...]/xxz.png\n\t        root/cat/123.png\n\t        root/cat/nsdf3.png\n\t        root/cat/[...]/asd932_.png\n", "    This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n\t    the same methods can be overridden to customize the dataset.\n\t    Args:\n\t        root (string): Root directory path.\n\t        transform (callable, optional): A function/transform that  takes in an PIL image\n\t            and returns a transformed version. E.g, ``transforms.RandomCrop``\n\t        target_transform (callable, optional): A function/transform that takes in the\n\t            target and transforms it.\n\t        loader (callable, optional): A function to load an image given its path.\n\t        is_valid_file (callable, optional): A function that takes path of an Image file\n", "            and check if the file is a valid file (used to check of corrupt files)\n\t     Attributes:\n\t        classes (list): List of the class names sorted alphabetically.\n\t        class_to_idx (dict): Dict with items (class_name, class_index).\n\t        imgs (list): List of (image path, class_index) tuples\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        root: str,\n\t        transform: Optional[Callable] = None,\n", "        target_transform: Optional[Callable] = None,\n\t        loader: Callable[[str], Any] = default_loader,\n\t        is_valid_file: Optional[Callable[[str], bool]] = None,\n\t        nclass: int = 1000,\n\t        holdout: Optional[str] = None,\n\t    ):\n\t        # ImageNet/ImageNet-100 is implemented\n\t        assert nclass in [100, 1000]\n\t        if holdout != None:\n\t            assert holdout in [\"train\", \"val\"]\n", "        super().__init__(\n\t            root,\n\t            loader,\n\t            IMG_EXTENSIONS if is_valid_file is None else None,\n\t            transform=transform,\n\t            target_transform=target_transform,\n\t            is_valid_file=is_valid_file,\n\t            nclass=nclass,\n\t            holdout=holdout,\n\t        )\n", "        self.imgs = self.samples\n"]}
{"filename": "config/arguments.py", "chunked_list": ["import os\n\timport argparse\n\tdef str2bool(v):\n\t    \"\"\"Cast string to boolean\"\"\"\n\t    if isinstance(v, bool):\n\t        return v\n\t    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n\t        return True\n\t    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n\t        return False\n", "    else:\n\t        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\tdef make_log_file(mode, checkpoint, filename, **kwargs):\n\t    if mode in [\"train\", \"finetune\", \"dp_imp\"]:\n\t        if not os.path.isdir(checkpoint):\n\t            os.makedirs(checkpoint, exist_ok=True)\n\t    if mode == \"train\":\n\t        write_filename = filename\n\t        log_filename = \"log.txt\"\n\t        logs_name = \"logs\"\n", "    elif mode == \"finetune\":\n\t        if args.filename.endswith(\".pth\"):\n\t            write_filename = filename[:-4]\n\t        write_filename = write_filename + f\"_ft_lr{kwargs['lr']}.pth\"\n\t        log_filename = f\"log_{write_filename[:-4]}.txt\"\n\t        logs_name = f\"logs_{write_filename[:-4]}\"\n\t    elif mode == \"dp_imp\":\n\t        write_filename = \"\"\n\t        log_filename = f\"log_f{args.from_blk}_t{args.to_blk}.txt\"\n\t        logs_name = f\"log_f{args.from_blk}_t{args.to_blk}\"\n", "    else:\n\t        write_filename = \"\"\n\t        log_filename = \"\"\n\t        logs_name = \"\"\n\t    log_header = [\"Epoch\", \"LR\", \"Train Loss\", \"Valid Loss\", \"Train Acc.\", \"Valid Acc.\"]\n\t    return write_filename, log_filename, logs_name, log_header\n\tparser = argparse.ArgumentParser(description=\"PyTorch ImageNet Training\")\n\tparser.add_argument(\n\t    \"--aug\", default=False, type=str2bool, help=\"Add augmentation to training script\"\n\t)\n", "parser.add_argument(\"--distill\", default=0.0, type=float, help=\"distillation ratio\")\n\t# data and models\n\tparser.add_argument(\n\t    \"-d\", \"--data\", metavar=\"DIR\", default=\"/ssd_data/imagenet\", help=\"path to dataset\"\n\t)\n\tparser.add_argument(\n\t    \"--nclass\",\n\t    default=1000,\n\t    type=int,\n\t    choices=[10, 100, 1000],\n", "    help=\"number of class to use\",\n\t)\n\tparser.add_argument(\n\t    \"--dataset\",\n\t    default=\"imagenet\",\n\t    type=str,\n\t    choices=[\"imagenet\"],\n\t    help=\"Type of dataset to use\",\n\t)\n\tparser.add_argument(\n", "    \"-a\",\n\t    \"--arch\",\n\t    metavar=\"ARCH\",\n\t    default=\"mobilenet_v2\",\n\t    help=\"model architecture: \",\n\t)\n\tparser.add_argument(\n\t    \"-j\",\n\t    \"--workers\",\n\t    default=4,\n", "    type=int,\n\t    metavar=\"N\",\n\t    help=\"number of data loading workers (default: 4)\",\n\t)\n\tparser.add_argument(\n\t    \"--epochs\", default=150, type=int, metavar=\"N\", help=\"number of total epochs to run\"\n\t)\n\tparser.add_argument(\n\t    \"--start-epoch\",\n\t    default=0,\n", "    type=int,\n\t    metavar=\"N\",\n\t    help=\"manual epoch number (useful on restarts)\",\n\t)\n\tparser.add_argument(\n\t    \"-b\",\n\t    \"--batch-size\",\n\t    default=256,\n\t    type=int,\n\t    metavar=\"N\",\n", "    help=\"mini-batch size (default: 256), this is the total \"\n\t    \"batch size of all GPUs on the current node when \"\n\t    \"using Data Parallel or Distributed Data Parallel\",\n\t)\n\tparser.add_argument(\n\t    \"-c\",\n\t    \"--checkpoint\",\n\t    default=\"checkpoints\",\n\t    type=str,\n\t    metavar=\"PATH\",\n", "    help=\"path to save checkpoint (default: checkpoints)\",\n\t)\n\tparser.add_argument(\n\t    \"-f\",\n\t    \"--filename\",\n\t    default=\"checkpoint.pth\",\n\t    type=str,\n\t    metavar=\"FILE\",\n\t    help=\"filename of the checkopint (default: checkpoint.pth)\",\n\t)\n", "parser.add_argument(\n\t    \"--width-mult\", type=float, default=1.0, help=\"MobileNet model width multiplier.\"\n\t)\n\tparser.add_argument(\n\t    \"--input-size\", type=int, default=224, help=\"MobileNet model input resolution\"\n\t)\n\t# Optimizer\n\tparser.add_argument(\n\t    \"--lr\",\n\t    \"--learning-rate\",\n", "    default=0.1,\n\t    type=float,\n\t    metavar=\"LR\",\n\t    help=\"initial learning rate\",\n\t    dest=\"lr\",\n\t)\n\tparser.add_argument(\"--momentum\", default=0.9, type=float, metavar=\"M\", help=\"momentum\")\n\tparser.add_argument(\n\t    \"--wd\",\n\t    \"--weight-decay\",\n", "    default=1e-5,\n\t    type=float,\n\t    metavar=\"W\",\n\t    help=\"weight decay (default: 4e-5)\",\n\t    dest=\"weight_decay\",\n\t)\n\tparser.add_argument(\"--nesterov\", default=True, type=str2bool, help=\"nesterov for sgd\")\n\tparser.add_argument(\n\t    \"--lr-decay\", type=str, default=\"cos\", help=\"mode for learning rate decay\"\n\t)\n", "parser.add_argument(\n\t    \"--schedule\",\n\t    type=int,\n\t    nargs=\"+\",\n\t    default=[45, 90, 135, 157],\n\t    help=\"decrease learning rate at these epochs.\",\n\t)\n\tparser.add_argument(\n\t    \"--gamma\", type=float, default=0.1, help=\"LR is multiplied by gamma on schedule.\"\n\t)\n", "# Options\n\tparser.add_argument(\n\t    \"-p\",\n\t    \"--print-freq\",\n\t    default=1000,\n\t    type=int,\n\t    metavar=\"N\",\n\t    help=\"print frequency (default: 1000)\",\n\t)\n\tparser.add_argument(\n", "    \"--resume\",\n\t    default=\"\",\n\t    type=str,\n\t    metavar=\"PATH\",\n\t    help=\"path to latest checkpoint (default: none)\",\n\t)\n\tparser.add_argument(\n\t    \"-m\",\n\t    \"--mode\",\n\t    default=\"train\",\n", "    type=str,\n\t    choices=[\"train\", \"finetune\", \"eval\", \"merge\", \"dp_imp\"],\n\t    help=\"script mode : choose among (train/finetune/eval/merge/dp_imp)\",\n\t)\n\tparser.add_argument(\n\t    \"--seed\", default=None, type=int, help=\"seed for initializing training. \"\n\t)\n\tparser.add_argument(\"--debug\", action=\"store_true\", help=\"only heavy ones\")\n\t# Hyperparams of main experiment/baseline\n\tparser.add_argument(\n", "    \"--pretrain\", default=\"\", type=str, help=\"path to the pretrained vanilla network\"\n\t)\n\tparser.add_argument(\"--act-path\", default=\"\", type=str, help=\"path to the act pos\")\n\tparser.add_argument(\n\t    \"--ft-holdout\",\n\t    default=False,\n\t    type=str2bool,\n\t    help=\"Whether to finetune with holdout validation set\",\n\t)\n\t# Merge & Finetune\n", "parser.add_argument(\n\t    \"--time-path\",\n\t    default=\"\",\n\t    type=str,\n\t    help=\"\"\"\n\t    Path to the time table. \n\t    Needed in finding optimal merging pattern using DP. \n\t    Will use act_pos for merging pattern if not specififed.\n\t    \"\"\",\n\t)\n", "# DP\n\tparser.add_argument(\n\t    \"--imp-epoch\",\n\t    default=1,\n\t    type=int,\n\t    help=\"\"\"\n\t    epoch per measuring the importance\n\t    \"\"\",\n\t)\n\tparser.add_argument(\n", "    \"--imp-lr-decay\",\n\t    default=\"cos\",\n\t    type=str,\n\t    help=\"\"\"\n\t    lr_decay when measuring the importance\n\t    \"\"\",\n\t)\n\tparser.add_argument(\n\t    \"--from-blk\",\n\t    default=None,\n", "    type=int,\n\t    help=\"\"\"\n\t    ID of the block to measure from\n\t    \"\"\",\n\t)\n\tparser.add_argument(\n\t    \"--to-blk\",\n\t    default=None,\n\t    type=int,\n\t    help=\"\"\"\n", "    ID of the block to measure until\n\t    \"\"\",\n\t)\n\tparser.add_argument(\n\t    \"--exclude-zeros\",\n\t    default=False,\n\t    type=str2bool,\n\t    help=\"\"\"\n\t    Exclude blocks starting with 0 activation\n\t    when both of the start and end indices are \n", "    end of the ResidualBlock\n\t    \"\"\",\n\t)\n\t# DepthShrinker\n\tparser.add_argument(\n\t    \"--lamb\",\n\t    default=1e-4,\n\t    type=float,\n\t    help=\"lambda value that weighs sparsity regularizer\",\n\t)\n", "parser.add_argument(\n\t    \"--reg\",\n\t    default=\"none\",\n\t    choices=[\"none\", \"soft\", \"w1.0\", \"w1.4\"],\n\t    type=str,\n\t    help=\"Type of regularizer to use\",\n\t)\n\tparser.add_argument(\n\t    \"--compress-k\",\n\t    default=8,\n", "    type=int,\n\t    help=\"The number of InvertedResidualBlock to compress (min 0, max 17)\",\n\t)\n\t# fmt: off\n\tparser.add_argument(\n\t    \"--ds-pattern\",\n\t    default=\"none\",\n\t    choices=[\"none\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A10\", \"B10\", \"C10\", \"D10\", \"AR\", \"BR\", \"CR\", \"AR10\", \"BR10\", \"CR10\", \"AR_AUG\", \"BR_AUG\", \"CR_AUG\", \"AR10_AUG\", \"BR10_AUG\", \"CR10_AUG\"],\n\t    type=str,\n\t    help=\"pre-defined pattern from the paper\",\n", ")\n\t# fmt: on\n\targs = parser.parse_args()\n\tif args.dataset == \"cifar\":\n\t    assert args.nclass == 10\n\telif args.dataset == \"imagenet\":\n\t    assert args.nclass in [100, 1000]\n\t# Some post-processing on args for each mode\n\tif args.mode in [\"train\", \"dp_imp\"]:\n\t    if args.arch in [\"mobilenet_v2\", \"learn_mobilenet_v2\", \"dep_shrink_mobilenet_v2\"]:\n", "        args.checkpoint += f\"_w{args.width_mult}\"\n\t    # Architecture specific config\n\t    if args.mode in [\"dp_imp\"]:\n\t        args.checkpoint += f\"_ie{args.imp_epoch}\"\n\t        args.checkpoint += f\"_ild_{args.imp_lr_decay}\"\n\t        if args.exclude_zeros:\n\t            args.checkpoint += f\"_ex\"\n\t        args.checkpoint = os.path.join(args.checkpoint, f\"par\")\n\t    elif args.mode == \"train\":\n\t        args.checkpoint += f\"_e{args.epochs}\"\n", "        if args.arch == \"dep_shrink_mobilenet_v2\":\n\t            args.checkpoint += f\"_cmp{args.compress_k}\"\n\tif args.aug:\n\t    args.checkpoint += \"_aug\"\n\tif args.distill > 0:\n\t    args.checkpoint += f\"_dt{args.distill}\"\n\tif args.seed:\n\t    args.checkpoint += f\"_s{args.seed}\"\n\tif args.mode != \"train\" or args.arch == \"mobilenet_v2\" or args.reg == \"none\":\n\t    args.reg = None\n", "if args.resume:\n\t    assert args.mode in [\"train\", \"finetune\"]\n\t    if not os.path.isfile(args.resume):\n\t        raise FileNotFoundError(\"=> no checkpoint found at '{}'\".format(args.resume))\n\t    args.checkpoint = os.path.dirname(args.resume)\n\tif args.ft_holdout:\n\t    assert args.mode == \"finetune\"\n\tif args.time_path:\n\t    assert args.mode in [\"finetune\", \"merge\"]\n\twrite_filename, log_filename, logs_name, log_header = make_log_file(\n", "    args.mode, args.checkpoint, args.filename, lr=args.lr\n\t)\n\tlog_header = [\"Epoch\", \"LR\", \"Train Loss\", \"Valid Loss\", \"Train Acc.\", \"Valid Acc.\"]\n"]}
{"filename": "exps/aggregate_imp.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\timport pandas as pd\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Inference Time with TensorRT\")\n\tparser.add_argument(\n\t    \"-d\",\n\t    \"--dir\",\n\t    type=str,\n", "    help=\"directory name\",\n\t)\n\tparser.add_argument(\n\t    \"-n\",\n\t    \"--num\",\n\t    type=int,\n\t    help=\"the number of blks\",\n\t)\n\timport re\n\tdef natural_key(string_):\n", "    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_)]\n\tif __name__ == \"__main__\":\n\t    args = parser.parse_args()\n\t    res = pd.DataFrame()\n\t    for currentpath, folders, files in os.walk(args.dir):\n\t        for f in sorted(files, key=natural_key):\n\t            if \".csv\" in f:\n\t                print(f)\n\t                tmp = pd.read_csv(os.path.join(currentpath, f))\n\t                res = pd.concat([res, tmp])\n", "    print(len(res))\n\t    assert len(res) == args.num\n\t    res.to_csv(os.path.join(args.dir, \"importance.csv\"))\n"]}
{"filename": "exps/main.py", "chunked_list": ["# Code started from  https://github.com/d-li14/mobilenetv2.pytorch.git\n\timport os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\timport random\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.parallel\n\timport torch.backends.cudnn as cudnn\n\timport torch.optim\n", "import torch.utils.data\n\timport torch.utils.data.distributed\n\timport pandas as pd\n\timport copy\n\tfrom models.imagenet import models, blocks\n\tfrom models.model_op import reset_layers, valid_blks\n\tfrom config.arguments import args, write_filename, log_filename, logs_name, log_header\n\tfrom utils import Logger, train, validate\n\tfrom utils.loaders import get_train_loader, get_val_loader\n\tfrom utils.misc import (\n", "    save_checkpoint,\n\t    load_checkpoint,\n\t    print_act_pos,\n\t    cp_state,\n\t    log_tool,\n\t    KLLossSoft,\n\t)\n\tfrom tensorboardX import SummaryWriter\n\tfrom itertools import product\n\tfrom timm.loss import LabelSmoothingCrossEntropy\n", "def random_seed(args):\n\t    cudnn.benchmark = True\n\t    if args.seed is not None:\n\t        random.seed(args.seed)\n\t        torch.manual_seed(args.seed)\n\t        torch.cuda.manual_seed(args.seed)\n\t        cudnn.deterministic = True\n\tdef select_model(args, arch):\n\t    print(os.path.join(args.checkpoint, args.filename))\n\t    print(\"=> creating model '{}'\".format(arch))\n", "    if arch in [\"learn_mobilenet_v2\"]:\n\t        model = models[arch](\n\t            num_classes=args.nclass, width_mult=args.width_mult, add_relu=True\n\t        )\n\t    elif arch in [\"mobilenet_v2\", \"dep_shrink_mobilenet_v2\"]:\n\t        model = models[arch](num_classes=args.nclass, width_mult=args.width_mult)\n\t    elif arch in [\"vgg19\", \"learn_vgg19\"]:\n\t        model = models[arch](num_classes=args.nclass)\n\t    else:\n\t        raise NotImplementedError(f\"Architecture {arch} not supported\")\n", "    model = torch.nn.DataParallel(model).cuda()\n\t    return model\n\tdef load_resume(args, model, optimizer, logger):\n\t    print(\"=> loading checkpoint '{}'\".format(args.resume))\n\t    checkpoint = torch.load(args.resume)\n\t    args.start_epoch = checkpoint[\"epoch\"]\n\t    best_prec1 = checkpoint[\"best_prec1\"]\n\t    source_state = load_checkpoint(model, args.arch, args.resume, logger=logger)\n\t    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\t    print(f\"=> loaded checkpoint '{args.resume}' (epoch {args.start_epoch})\")\n", "    args.checkpoint = os.path.dirname(args.resume)\n\t    return best_prec1, source_state\n\tdef init_training(args, model, optimizer):\n\t    title = \"ImageNet-\" + args.arch\n\t    if args.resume:\n\t        fname = os.path.join(os.path.dirname(args.resume), log_filename)\n\t    else:\n\t        fname = os.path.join(args.checkpoint, log_filename)\n\t    logger = Logger(\n\t        fpath=fname,\n", "        title=title,\n\t        resume=bool(args.resume),\n\t    )\n\t    if args.resume:\n\t        logger.comment(\"\")\n\t        logger.comment(\"=================\")\n\t        logger.comment(\"\")\n\t        logger.comment(\"Resuming...\")\n\t        logger.comment(\"\")\n\t        best_prec1, source_state = load_resume(args, model, optimizer, logger)\n", "    else:\n\t        best_prec1, source_state = 0, dict()\n\t        if args.ds_pattern != \"none\":\n\t            ds_pat = (args.ds_pattern, args.compress_k)\n\t            assert args.pretrain\n\t        else:\n\t            ds_pat = None\n\t        path = args.pretrain if args.pretrain else None\n\t        act_path = args.act_path if args.act_path else path\n\t        if args.mode != \"train\":\n", "            source_state = load_checkpoint(\n\t                model, args.arch, path, act_path, ds_pat, logger\n\t            )\n\t    logger.set_names(log_header)\n\t    log_tool(str(optimizer), logger, \"opt\")\n\t    return logger, best_prec1, source_state\n\tdef set_loader(args, holdout=False):\n\t    assert args.dataset in [\"imagenet\"]\n\t    train_loader, _ = get_train_loader(\n\t        args.data,\n", "        args.batch_size,\n\t        workers=args.workers,\n\t        input_size=args.input_size,\n\t        nclass=args.nclass,\n\t        holdout=\"train\" if holdout else None,\n\t        timm_aug=args.aug,\n\t    )\n\t    if holdout:\n\t        holdout_loader, _ = get_train_loader(\n\t            args.data,\n", "            100,\n\t            workers=args.workers,\n\t            input_size=args.input_size,\n\t            nclass=args.nclass,\n\t            holdout=\"val\",\n\t        )\n\t    else:\n\t        holdout_loader = None\n\t    val_loader, _ = get_val_loader(\n\t        args.data,\n", "        100,\n\t        workers=args.workers,\n\t        input_size=args.input_size,\n\t        nclass=args.nclass,\n\t    )\n\t    return train_loader, holdout_loader, val_loader\n\tdef action(model, source_state, mode, args, val_loader, criterion):\n\t    if mode == \"eval\":\n\t        print(model)\n\t        print_act_pos(model.module, source_state)\n", "        val_loss, prec1 = validate(val_loader, model, criterion, args)\n\t        print(f\"Validation loss : {val_loss:.2f} | Top 1 Accuracy : {prec1}\")\n\t    elif mode == \"merge\":\n\t        model.to(\"cpu\")\n\t        module = model.module\n\t        save_state = {}\n\t        for name in [\"act_pos\", \"merge_pos\", \"compress_k\"]:\n\t            cp_state(save_state, source_state, name)\n\t        if args.filename.endswith(\".pth\"):\n\t            filename = args.filename[:-4]\n", "        filename = f\"{filename}_merged.pth\"\n\t        print(model)\n\t        if \"merge_pos\" in save_state:\n\t            m_pos = save_state[\"merge_pos\"]\n\t        else:\n\t            m_pos = save_state[\"act_pos\"]\n\t        if args.arch in [\n\t            \"learn_mobilenet_v2\",\n\t            \"learn_vgg19\",\n\t        ]:\n", "            module.merge(save_state[\"act_pos\"], m_pos)\n\t            print(model)\n\t            print()\n\t            print(f\"act_pos             : {save_state['act_pos']}\")\n\t            if \"merge_pos\" in save_state:\n\t                print(f\"merge_pos           : {m_pos}\")\n\t            print()\n\t        elif args.arch == \"dep_shrink_mobilenet_v2\":\n\t            module.merge(save_state[\"act_pos\"])\n\t            print(model)\n", "        save_state.update({\"state_dict\": model.state_dict(), \"merged\": True})\n\t        save_checkpoint(\n\t            save_state,\n\t            False,\n\t            checkpoint=args.checkpoint,\n\t            filename=filename,\n\t        )\n\tdef prepare_finetune(model, args, source_state):\n\t    model.to(\"cpu\")\n\t    module = model.module\n", "    save_state = dict()\n\t    cp_state(save_state, source_state, \"act_pos\")\n\t    cp_state(save_state, source_state, \"merge_pos\")\n\t    cp_state(save_state, source_state, \"compress_k\")\n\t    # For `learn_mobilenet_v2`, see `utils.misc.load_checkpoint` ftn\n\t    if args.arch == \"dep_shrink_mobilenet_v2\":\n\t        act_pos, _ = module.get_act_info()\n\t        module.fix_act(act_pos)\n\t        save_state[\"act_pos\"] = act_pos\n\t    model.to(\"cuda\")\n", "    return save_state\n\tdef prepare_train(model, args):\n\t    module = model.module\n\t    save_state = dict()\n\t    if args.arch == \"dep_shrink_mobilenet_v2\":\n\t        module.compress_k = args.compress_k\n\t        save_state[\"compress_k\"] = args.compress_k\n\t        module.set_act_hats()\n\t    return save_state\n\tdef run_one_epoch(\n", "    args,\n\t    epoch,\n\t    model,\n\t    logger,\n\t    writer,\n\t    train_loader,\n\t    val_loader,\n\t    criterion,\n\t    optimizer,\n\t    best_prec1,\n", "    save_state,\n\t):\n\t    print(\"\\nEpoch: [%d | %d]\" % (epoch + 1, args.epochs))\n\t    if args.mode == \"train\":\n\t        if args.arch == \"dep_shrink_mobilenet_v2\":\n\t            with torch.no_grad():\n\t                str_act_param = str(torch.cat(model.module.get_arch_parameters()))\n\t            log_tool(str_act_param, logger, \"ap\")\n\t    # train for one epoch\n\t    train_loss, train_acc = train(\n", "        train_loader,\n\t        model,\n\t        criterion,\n\t        optimizer,\n\t        epoch,\n\t        args.epochs,\n\t        args.lr_decay,\n\t        args.reg,\n\t        logger,\n\t        args,\n", "    )\n\t    # evaluate on validation set\n\t    val_loss, prec1 = validate(val_loader, model, criterion, args)\n\t    lr = optimizer.param_groups[0][\"lr\"]\n\t    # append logger file\n\t    logger.append([epoch + 1, lr, train_loss, val_loss, train_acc, prec1])\n\t    # tensorboardX\n\t    writer.add_scalar(\"learning_rate\", lr, epoch + 1)\n\t    writer.add_scalars(\n\t        \"loss\", {\"train loss\": train_loss, \"validation loss\": val_loss}, epoch + 1\n", "    )\n\t    writer.add_scalars(\n\t        \"accuracy\",\n\t        {\"train accuracy\": train_acc, \"validation accuracy\": prec1},\n\t        epoch + 1,\n\t    )\n\t    is_best = prec1 > best_prec1\n\t    best_prec1 = max(prec1, best_prec1)\n\t    save_state.update(\n\t        {\n", "            \"epoch\": epoch + 1,\n\t            \"arch\": args.arch,\n\t            \"state_dict\": model.state_dict(),\n\t            \"best_prec1\": best_prec1,\n\t            \"optimizer\": optimizer.state_dict(),\n\t            \"train_loss\": train_loss,\n\t            \"val_loss\": val_loss,\n\t            \"train_acc\": train_acc,\n\t            \"val_acc\": prec1,\n\t        }\n", "    )\n\t    save_checkpoint(\n\t        save_state,\n\t        is_best,\n\t        checkpoint=args.checkpoint,\n\t        filename=write_filename,\n\t    )\n\t    return best_prec1\n\tdef train_finetune(model, criterion, optimizer):\n\t    # set logging file config\n", "    mode = args.mode\n\t    logger, best_prec1, source_state = init_training(args, model, optimizer)\n\t    if not args.ft_holdout:\n\t        train_loader, _, val_loader = set_loader(args)\n\t        vloader = val_loader\n\t    else:\n\t        train_loader, holdout_loader, test_loader = set_loader(args, holdout=True)\n\t        vloader = holdout_loader\n\t    # Mode dependant action of the script\n\t    if mode == \"train\":\n", "        save_state = prepare_train(model, args)\n\t    elif mode == \"finetune\":\n\t        save_state = prepare_finetune(model, args, source_state)\n\t        if \"act_pos\" in save_state:\n\t            logger.comment(str(save_state[\"act_pos\"]))\n\t    logger.comment(str(model))\n\t    # visualization\n\t    writer = SummaryWriter(os.path.join(args.checkpoint, logs_name))\n\t    for epoch in range(args.start_epoch, args.epochs):\n\t        best_prec1 = run_one_epoch(\n", "            args,\n\t            epoch,\n\t            model,\n\t            logger,\n\t            writer,\n\t            train_loader,\n\t            vloader,\n\t            criterion,\n\t            optimizer,\n\t            best_prec1,\n", "            save_state,\n\t        )\n\t        if args.ft_holdout:\n\t            test_loss, test_acc = validate(test_loader, model, criterion, args)\n\t            logger.comment(f\"Test Acc: {test_acc} | Test Loss : {test_loss}\")\n\t            logger.comment(\"-----------------------------\")\n\t    logger.close()\n\t    # logger.plot()\n\t    # savefig(os.path.join(args.checkpoint, \"log.eps\"))\n\t    writer.close()\n", "    print(\"Best accuracy:\")\n\t    print(best_prec1)\n\tdef eval_merge(model, criterion):\n\t    # set logging file config\n\t    mode = args.mode\n\t    path = os.path.join(args.checkpoint, args.filename)\n\t    if args.act_path:\n\t        act_path = args.act_path\n\t    else:\n\t        act_path = path\n", "    source_state = load_checkpoint(model, args.arch, path, act_path)\n\t    _, _, val_loader = set_loader(args)\n\t    # Mode dependant action of the script\n\t    action(model, source_state, mode, args, val_loader, criterion)\n\tdef measure_imp(model, criterion, optimizer):\n\t    logger, _, source_state = init_training(args, model, optimizer)\n\t    train_loader, holdout_loader, test_loader = set_loader(args, holdout=True)\n\t    module = model.module\n\t    act_num = module.get_act_info()[1]\n\t    epoch = 0\n", "    assert args.arch in [\"learn_mobilenet_v2\", \"learn_vgg19\"]\n\t    if args.arch in [\"learn_vgg19\"]:\n\t        default = set(range(1, act_num + 1))\n\t    elif args.arch == \"learn_mobilenet_v2\":\n\t        default = set(range(1, act_num + 1)) - set(range(2, act_num + 1, 3))\n\t    model.module.fix_act(default)\n\t    writer = SummaryWriter(os.path.join(args.checkpoint, logs_name))\n\t    if all([st in source_state for st in [\"holdout_train_loss\", \"holdout_train_acc\"]]):\n\t        train_loss, train_acc = (\n\t            source_state[\"holdout_train_loss\"],\n", "            source_state[\"holdout_train_acc\"],\n\t        )\n\t    else:\n\t        print(\"Measuring base performance in train-set\")\n\t        train_loss, train_acc = validate(train_loader, model, criterion, args)\n\t    if all(st in source_state for st in [\"holdout_val_loss\", \"holdout_val_acc\"]):\n\t        val_loss, val_acc = (\n\t            source_state[\"holdout_val_loss\"],\n\t            source_state[\"holdout_val_acc\"],\n\t        )\n", "    else:\n\t        print(\"Measuring base performance in holdout-val-set\")\n\t        val_loss, val_acc = validate(holdout_loader, model, criterion, args)\n\t    logger.append([epoch, args.lr, train_loss, val_loss, train_acc, val_acc])\n\t    logger.comment(\"\")\n\t    base_tl, base_ta, base_vl, base_va = train_loss, train_acc, val_loss, val_acc\n\t    blks = valid_blks(model.module)\n\t    blk_pos = list(model.module.get_blk_info()[0].values())\n\t    # Extended blocks have (st, end, st_act, end_act)\n\t    ext_blks = []\n", "    for st, end in blks:\n\t        # Exclude blk_pos blocks that ends with zero\n\t        # when both st and end is blk_pos\n\t        if args.arch == \"learn_mobilenet_v2\":\n\t            if args.exclude_zeros:\n\t                if st == 0:\n\t                    st_acts, end_acts = [1], [1]\n\t                elif st in blk_pos:\n\t                    st_acts, end_acts = [1, 0], [1]\n\t                elif end in blk_pos:\n", "                    st_acts, end_acts = [1], [1, 0]\n\t                else:\n\t                    st_acts, end_acts = [1], [1]\n\t            else:\n\t                st_acts = [1, 0] if st in blk_pos else [1]\n\t                end_acts = [1, 0] if end in blk_pos else [1]\n\t        elif args.arch == \"learn_vgg19\":\n\t            st_acts, end_acts = [1], [1]\n\t        else:\n\t            raise NotImplementedError()\n", "        for st_act, end_act in product(st_acts, end_acts):\n\t            ext_blks.append((st, end, st_act, end_act))\n\t    logger.comment(f\"Number of extended blocks are {len(ext_blks)}.\")\n\t    if args.from_blk != None and args.to_blk != None:\n\t        logger.comment(f\"Training blocks from {args.from_blk} to {args.to_blk}\")\n\t    logger.comment(\"\")\n\t    for ind, (st, end, st_act, end_act) in enumerate(ext_blks):\n\t        if args.from_blk != None and args.to_blk != None:\n\t            if ind < args.from_blk or args.to_blk <= ind:\n\t                continue\n", "        if os.path.exists(os.path.join(args.checkpoint, f\"importance{ind}.csv\")):\n\t            logger.comment(f\"Skipping block {ind:>3} since the csv file exists...\")\n\t            continue\n\t        tmp_model = copy.deepcopy(model)\n\t        logger.comment(\"\")\n\t        logger.comment(f\"blk  : {(st, end, st_act, end_act)}\")\n\t        if st + 1 == end:\n\t            if \"mobilenet_v2\" in args.arch:\n\t                reset_layers(\n\t                    tmp_model.module.features[1:-1], blocks[args.arch], st, end, logger\n", "                )\n\t            elif \"vgg19\" in args.arch:\n\t                reset_layers(\n\t                    tmp_model.module.features, blocks[args.arch], st, end, logger\n\t                )\n\t        new_acts = set()\n\t        for i in default:\n\t            if st < i and i < end:\n\t                continue\n\t            new_acts = set.union(new_acts, {i})\n", "        if st_act:\n\t            new_acts = set.union(new_acts, {st})\n\t        if end_act:\n\t            new_acts = set.union(new_acts, {end})\n\t        logger.comment(f\"acts : {new_acts}\")\n\t        tmp_model.module.fix_act(new_acts)\n\t        tmp_optimizer = torch.optim.SGD(\n\t            tmp_model.parameters(),\n\t            args.lr,\n\t            momentum=args.momentum,\n", "            weight_decay=args.weight_decay,\n\t            nesterov=args.nesterov,\n\t        )\n\t        n = args.imp_epoch\n\t        for i in range(n):\n\t            train_loss, train_acc = train(\n\t                train_loader,\n\t                tmp_model,\n\t                criterion,\n\t                tmp_optimizer,\n", "                i,\n\t                n,\n\t                args.imp_lr_decay,\n\t                args.reg,\n\t                logger,\n\t                args,\n\t            )\n\t            val_loss, val_acc = validate(holdout_loader, tmp_model, criterion, args)\n\t            lr = tmp_optimizer.param_groups[0][\"lr\"]\n\t            # append logger file\n", "            logger.append([epoch + 1, lr, train_loss, val_loss, train_acc, val_acc])\n\t        dtl, dta = train_loss - base_tl, train_acc - base_ta\n\t        dvl, dva = val_loss - base_vl, val_acc - base_va\n\t        del tmp_model\n\t        del tmp_optimizer\n\t        blks_dict = {\n\t            \"id\": [ind],\n\t            \"st\": [st],\n\t            \"end\": [end],\n\t            \"st_act\": [st_act],\n", "            \"end_act\": [end_act],\n\t            \"train_loss\": [dtl],\n\t            \"train_acc\": [dta],\n\t            \"val_loss\": [dvl],\n\t            \"val_acc\": [dva],\n\t        }\n\t        blks = pd.DataFrame(blks_dict)\n\t        csv_path = os.path.join(args.checkpoint, f\"importance{ind}.csv\")\n\t        blks.to_csv(csv_path, sep=\",\", index=False)\n\tdef main():\n", "    random_seed(args)\n\t    model = select_model(args, args.arch)\n\t    # define loss function (criterion) and optimizer\n\t    if args.distill > 0:\n\t        if args.arch in [\"learn_mobilenet_v2\", \"dep_shrink_mobilenet_v2\"]:\n\t            teacher_arch = \"mobilenet_v2\"\n\t        else:\n\t            raise NotImplementedError\n\t        teacher_model = models[teacher_arch](\n\t            num_classes=args.nclass, width_mult=args.width_mult\n", "        )\n\t        teacher_model = torch.nn.DataParallel(teacher_model).cuda()\n\t        pretrain_state = torch.load(args.pretrain)\n\t        teacher_model.load_state_dict(pretrain_state[\"state_dict\"])\n\t        del pretrain_state\n\t        criterion = KLLossSoft(alpha=args.distill, teacher=teacher_model)\n\t    elif args.aug:\n\t        criterion = LabelSmoothingCrossEntropy(smoothing=0.1).cuda()\n\t    else:\n\t        criterion = nn.CrossEntropyLoss().cuda()\n", "    optimizer = torch.optim.SGD(\n\t        model.parameters(),\n\t        args.lr,\n\t        momentum=args.momentum,\n\t        weight_decay=args.weight_decay,\n\t        nesterov=args.nesterov,\n\t    )\n\t    if args.mode in [\"train\", \"finetune\"]:\n\t        train_finetune(model, criterion, optimizer)\n\t    elif args.mode in [\"eval\", \"merge\"]:\n", "        eval_merge(model, criterion)\n\t    elif args.mode in [\"dp_imp\"]:\n\t        measure_imp(model, criterion, optimizer)\n\t    else:\n\t        raise NotImplementedError(\"Add mode\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "exps/inference_trt.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\timport torch\n\timport torch.nn as nn\n\timport numpy as np\n\timport time\n\timport argparse\n\timport torch.backends.cudnn as cudnn\n\tfrom models.imagenet import (\n", "    MobileNetV2,\n\t    LearnMobileNetV2,\n\t    DepShrinkMobileNetV2,\n\t    VGG,\n\t    LearnVGG,\n\t)\n\tfrom utils.misc import load_checkpoint\n\tfrom utils.logger import Logger\n\tvgg_cfgs = {\n\t    # (n, inp, oup, isize)\n", "    \"19\": [\n\t        (2, 3, 64, 224),\n\t        (2, 64, 128, 112),\n\t        (4, 128, 256, 56),\n\t        (4, 256, 512, 28),\n\t        (4, 512, 512, 14),\n\t    ]\n\t}\n\tdef str2bool(v):\n\t    \"\"\"Cast string to boolean\"\"\"\n", "    if isinstance(v, bool):\n\t        return v\n\t    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n\t        return True\n\t    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n\t        return False\n\t    else:\n\t        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\tdef load(path, arch, width, nclass, logger):\n\t    if arch == \"learn_mobilenet_v2\":\n", "        model = nn.DataParallel(\n\t            LearnMobileNetV2(num_classes=nclass, width_mult=width, add_relu=True)\n\t        )\n\t    elif arch == \"dep_shrink_mobilenet_v2\":\n\t        model = nn.DataParallel(\n\t            DepShrinkMobileNetV2(num_classes=nclass, width_mult=width)\n\t        )\n\t    elif arch == \"mobilenet_v2\":\n\t        model = nn.DataParallel(MobileNetV2(num_classes=nclass, width_mult=width))\n\t    # VGG\n", "    elif arch == \"vgg19\":\n\t        model = nn.DataParallel(VGG(cfg=vgg_cfgs[\"19\"], num_classes=nclass))\n\t    elif arch == \"learn_vgg19\":\n\t        model = nn.DataParallel(LearnVGG(cfg=vgg_cfgs[\"19\"], num_classes=nclass))\n\t    load_checkpoint(model, arch, path, logger=logger)\n\t    return model\n\tdef main():\n\t    print(torch.cuda.device_count())  # Check the num of gpu\n\t    cudnn.benchmark = True\n\t    parser = argparse.ArgumentParser(description=\"Inference Time with TensorRT\")\n", "    parser.add_argument(\n\t        \"-a\",\n\t        \"--arch\",\n\t        metavar=\"ARCH\",\n\t        default=\"mobilenet_v2\",\n\t        type=str,\n\t        choices=[\n\t            \"mobilenet_v2\",\n\t            \"learn_mobilenet_v2\",\n\t            \"dep_shrink_mobilenet_v2\",\n", "            \"vgg19\",\n\t            \"learn_vgg19\",\n\t        ],\n\t        help=\"model architecture\",\n\t    )\n\t    parser.add_argument(\n\t        \"-w\",\n\t        \"--width-mult\",\n\t        type=float,\n\t        default=1.0,\n", "        help=\"MobileNet model width multiplier.\",\n\t    )\n\t    parser.add_argument(\n\t        \"-c\",\n\t        \"--checkpoint\",\n\t        type=str,\n\t        help=\"dir to the checkpoint (default: checkpoints)\",\n\t    )\n\t    parser.add_argument(\n\t        \"-f\",\n", "        \"--filename\",\n\t        type=str,\n\t        metavar=\"FILE\",\n\t        help=\"filename of the checkopint (default: checkpoint.pth)\",\n\t    )\n\t    parser.add_argument(\n\t        \"--nclass\",\n\t        type=int,\n\t        default=1000,\n\t        choices=[10, 100, 1000],\n", "        help=\"number of classes\",\n\t    )\n\t    parser.add_argument(\n\t        \"-n\",\n\t        \"--name\",\n\t        type=str,\n\t        help=\"log name\",\n\t    )\n\t    parser.add_argument(\n\t        \"--trt\",\n", "        default=True,\n\t        type=str2bool,\n\t        help=\"Whether to measure in TensorRT or not\",\n\t    )\n\t    parser.add_argument(\n\t        \"--cpu\",\n\t        default=False,\n\t        type=str2bool,\n\t        help=\"Whether to measure with CPU\",\n\t    )\n", "    args = parser.parse_args()\n\t    if args.cpu:\n\t        assert not args.trt\n\t    logger = Logger(os.path.join(args.checkpoint, f\"time_{args.name}.log\"))\n\t    logger.comment(str(args))\n\t    logger.comment(\"\")\n\t    gpu_name = torch.cuda.get_device_name(0)\n\t    logger.comment(gpu_name)\n\t    ckpt = os.path.join(args.checkpoint, args.filename)\n\t    model = load(ckpt, args.arch, args.width_mult, args.nclass, logger)\n", "    model = model.to(\"cuda\")\n\t    model.eval()\n\t    # Fuse batchnorm layers...\n\t    if args.arch in [\n\t        \"mobilenet_v2\",\n\t        \"vgg19\",\n\t    ]:\n\t        model.module.merge()\n\t    print(model)\n\t    logger.comment(f\"TensorRT  :  {args.trt}\")\n", "    logger.comment(f\"CPU             :  {args.cpu}\")\n\t    if args.cpu:\n\t        time, std = model.module.cpu_time(txt=args.arch, verb=True)\n\t    else:\n\t        time, std = model.module.time(txt=args.arch, verb=True, trt=args.trt)\n\t    logger.comment(\"\")\n\t    logger.comment(\"TIME MEAN\")\n\t    logger.comment(str([time]))\n\t    logger.comment(\"STD\")\n\t    logger.comment(str([std]))\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "exps/solve_dp.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\timport argparse\n\tfrom utils.logger import Logger\n\tfrom utils.dp import optimal_patterns, optimal_merge_pattern\n\tfrom utils.misc import save_checkpoint\n\tdef make_log_file(checkpoint, filename, **kwargs):\n\t    if not os.path.isdir(checkpoint):\n\t        os.makedirs(checkpoint, exist_ok=True)\n", "    write_filename = filename\n\t    log_filename = \"log.txt\"\n\t    return write_filename, log_filename\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description=\"Script for solving DP\")\n\t    parser.add_argument(\n\t        \"-c\",\n\t        \"--checkpoint\",\n\t        default=\"checkpoints\",\n\t        type=str,\n", "        metavar=\"PATH\",\n\t        help=\"path to save checkpoint (default: checkpoints)\",\n\t    )\n\t    parser.add_argument(\n\t        \"-f\",\n\t        \"--filename\",\n\t        default=\"checkpoint.pth\",\n\t        type=str,\n\t        metavar=\"FILE\",\n\t        help=\"filename of the checkopint (default: checkpoint.pth)\",\n", "    )\n\t    parser.add_argument(\n\t        \"--time-limit\",\n\t        type=float,\n\t        help=\"Time limit to optimize on\",\n\t    )\n\t    parser.add_argument(\n\t        \"--act-num\",\n\t        type=int,\n\t        help=\"Total number of the full activations\",\n", "    )\n\t    parser.add_argument(\n\t        \"--time-path\", type=str, help=\"Path to the inference time table of each block\"\n\t    )\n\t    parser.add_argument(\n\t        \"--imp-path\", type=str, help=\"Path to the importance table of each block\"\n\t    )\n\t    parser.add_argument(\n\t        \"--prec\",\n\t        type=float,\n", "        help=\"Precision when converting float constriant into int constraint\",\n\t    )\n\t    parser.add_argument(\n\t        \"--chk-time-path\",\n\t        default=\"\",\n\t        type=str,\n\t        help=\"Path to the inference time table of each block\",\n\t    )\n\t    args = parser.parse_args()\n\t    args.checkpoint = os.path.join(args.checkpoint, f\"p{args.prec}_tl{args.time_limit}\")\n", "    title = \"Solving DP\"\n\t    write_filename, log_filename = make_log_file(args.checkpoint, args.filename)\n\t    logger = Logger(os.path.join(args.checkpoint, log_filename), title=title)\n\t    logger.comment(f\"IMP PATH      : {args.imp_path}\")\n\t    logger.comment(f\"TIME PATH     : {args.time_path}\")\n\t    logger.comment(f\"TIME LIMIT    : {args.time_limit}\")\n\t    logger.comment(f\"PRECISION     : {args.prec}\")\n\t    logger.comment(\"\\n---------------------------\\n\")\n\t    act_pos, opt_m_pos, imp_sum, int_time_sum, flt_time_sum = optimal_patterns(\n\t        flt_time_limit=args.time_limit,\n", "        act_num=args.act_num,\n\t        opt_time_path=args.time_path,\n\t        ext_imp_path=args.imp_path,\n\t        prec=args.prec,\n\t        verbose=True,\n\t        logger=logger,\n\t    )\n\t    if args.chk_time_path:\n\t        m_pos, time, _ = optimal_merge_pattern(\n\t            act_pos=act_pos, act_num=args.act_num, time_path=args.chk_time_path\n", "        )\n\t        assert opt_m_pos == m_pos\n\t        logger.comment(\"\")\n\t        logger.comment(\"OPTIMAL MERGE POS IS CORRECT\")\n\t    logger.comment(\"\\n---------------------------\\n\")\n\t    logger.comment(f\"ACT POS       : {act_pos}\")\n\t    logger.comment(f\"MERGE POS     : {opt_m_pos}\")\n\t    logger.comment(f\"IMP SUM       : {imp_sum}\")\n\t    logger.comment(f\"INT TIME SUM  : {int_time_sum}\")\n\t    logger.comment(f\"FLT TIME SUM  : {flt_time_sum}\")\n", "    save_checkpoint(\n\t        {\n\t            \"act_pos\": act_pos,\n\t            \"merge_pos\": opt_m_pos,\n\t            \"imp_sum\": imp_sum,\n\t            \"int_time_sum\": int_time_sum,\n\t            \"flt_time_sum\": flt_time_sum,\n\t        },\n\t        False,\n\t        checkpoint=args.checkpoint,\n", "        filename=args.filename,\n\t    )\n"]}
{"filename": "exps/generate_tables.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\timport argparse\n\tfrom itertools import product\n\tfrom models.imagenet import LearnMobileNetV2, LearnVGG, vgg_cfgs\n\tfrom models.imagenet import InvertedResidual, VGGBlock\n\tfrom models.model_op import valid_blks\n\tfrom utils.dp import (\n\t    generate_time_table,\n", "    generate_optimal_time_table,\n\t    generate_ext_imp_table,\n\t)\n\tfrom utils.logger import Logger\n\tdef str2bool(v):\n\t    \"\"\"Cast string to boolean\"\"\"\n\t    if isinstance(v, bool):\n\t        return v\n\t    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n\t        return True\n", "    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n\t        return False\n\t    else:\n\t        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\tdef construct_ext_blks(arch, blks, blk_pos, exclude_zeros=True):\n\t    ext_blks = []\n\t    for st, end in blks:\n\t        # Exclude blk_pos blocks that ends with zero\n\t        if arch == \"learn_mobilenet_v2\":\n\t            if exclude_zeros:\n", "                if st == 0:\n\t                    st_acts, end_acts = [1], [1]\n\t                elif st in blk_pos:\n\t                    st_acts, end_acts = [1, 0], [1]\n\t                elif end in blk_pos:\n\t                    st_acts, end_acts = [1], [1, 0]\n\t                else:\n\t                    st_acts, end_acts = [1], [1]\n\t            else:\n\t                st_acts = [1, 0] if st in blk_pos else [1]\n", "                end_acts = [1, 0] if end in blk_pos else [1]\n\t        elif arch == \"learn_vgg19\":\n\t            st_acts, end_acts = [1], [1]\n\t        else:\n\t            raise NotImplementedError()\n\t        for st_act, end_act in product(st_acts, end_acts):\n\t            ext_blks.append((st, end, st_act, end_act))\n\t    return ext_blks\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description=\"Script for Generating Time Table\")\n", "    parser.add_argument(\n\t        \"-d\",\n\t        \"--dir\",\n\t        type=str,\n\t        default=\"utils/table\",\n\t        help=\"directory name\",\n\t    )\n\t    parser.add_argument(\n\t        \"-a\",\n\t        \"--arch\",\n", "        type=str,\n\t        default=\"learn_mobilenet_v2\",\n\t        help=\"architecture of the network\",\n\t    )\n\t    parser.add_argument(\n\t        \"-w\",\n\t        \"--width-mult\",\n\t        type=float,\n\t        default=None,\n\t        help=\"width multiplier\",\n", "    )\n\t    parser.add_argument(\n\t        \"--nclass\",\n\t        type=int,\n\t        default=100,\n\t        choices=[10, 100, 1000],\n\t        help=\"number of classes\",\n\t    )\n\t    parser.add_argument(\n\t        \"-t\",\n", "        \"--tag\",\n\t        type=str,\n\t        default=\"\",\n\t        help=\"tag\",\n\t    )\n\t    parser.add_argument(\"--mode\", type=str, choices=[\"time\", \"opt-time\", \"ext-imp\"])\n\t    parser.add_argument(\n\t        \"--time-path\", type=str, default=\"\", help=\"Path to the normal time table\"\n\t    )\n\t    parser.add_argument(\n", "        \"--trt\", type=str2bool, default=True, help=\"Path to the normal time table\"\n\t    )\n\t    parser.add_argument(\n\t        \"--imp-path\", type=str, default=\"\", help=\"Path to the normal importance table\"\n\t    )\n\t    parser.add_argument(\n\t        \"--score\", type=str, default=\"\", help=\"Score for the importance\"\n\t    )\n\t    parser.add_argument(\n\t        \"--norm\", type=str, default=\"default\", help=\"Normalization method of score\"\n", "    )\n\t    parser.add_argument(\"--alph\", type=float, default=1.0, help=\"Alpha in normalizing\")\n\t    args = parser.parse_args()\n\t    if args.mode == \"time\":\n\t        logger = Logger(os.path.join(args.dir, f\"time_table_{args.tag}.log\"))\n\t    assert args.arch in [\n\t        \"learn_mobilenet_v2\",\n\t        \"learn_vgg19\",\n\t    ]\n\t    if args.arch == \"learn_mobilenet_v2\":\n", "        model = LearnMobileNetV2(\n\t            num_classes=args.nclass, width_mult=args.width_mult, add_relu=True\n\t        )\n\t        blk_type = InvertedResidual\n\t        act_num = model.get_act_info()[1]\n\t        default = set(range(1, act_num + 1)) - set(range(2, act_num + 1, 3))\n\t    elif args.arch == \"learn_vgg19\":\n\t        model = LearnVGG(vgg_cfgs[\"19\"], num_classes=args.nclass)\n\t        act_num = model.get_act_info()[1]\n\t        blk_type = VGGBlock\n", "        default = set(range(1, act_num + 1))\n\t    blks = valid_blks(model)\n\t    if args.mode == \"time\":\n\t        generate_time_table(\n\t            model, blks, args.arch, blk_type, args.dir, args.tag, logger, args.trt\n\t        )\n\t    elif args.mode == \"opt-time\":\n\t        assert args.time_path\n\t        generate_optimal_time_table(blks, args.dir, args.tag, args.time_path)\n\t    elif args.mode == \"ext-imp\":\n", "        assert str(args.width_mult) in args.imp_path or args.width_mult == None\n\t        blk_pos = list(model.get_blk_info()[0].values())\n\t        ext_blks = construct_ext_blks(args.arch, blks, blk_pos)\n\t        assert args.imp_path and args.score in [\"train_acc\", \"val_acc\"]\n\t        generate_ext_imp_table(\n\t            ext_blks,\n\t            act_num,\n\t            args.dir,\n\t            args.imp_path,\n\t            args.score,\n", "            default,\n\t            args.norm,\n\t            args.alph,\n\t        )\n\t    else:\n\t        raise NotImplementedError()\n"]}
{"filename": "models/modules_trt.py", "chunked_list": ["from collections import OrderedDict\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom collections import OrderedDict\n\tclass NaiveFeed(nn.Module):\n\t    def __init__(self, odict: OrderedDict) -> None:\n\t        super().__init__()\n\t        self.md = nn.Sequential(odict)\n\t    def forward(self, x):\n", "        return self.md(x)\n\tclass SkipFeed(nn.Module):\n\t    def __init__(self, odict: OrderedDict, last=nn.Identity) -> None:\n\t        super().__init__()\n\t        self.md = nn.Sequential(odict)\n\t        self.last = last()\n\t    def forward(self, x):\n\t        return self.last(self.md(x) + x)\n\tclass Downsample(nn.Module):\n\t    def __init__(self, planes) -> None:\n", "        super().__init__()\n\t        self.planes = planes\n\t    def forward(self, x):\n\t        sz = x.shape[3] // 2\n\t        ch = x.shape[1] // 2\n\t        out = x\n\t        out = F.interpolate(out, size=(sz, sz))\n\t        zeros = out.mul(0)\n\t        out = torch.cat((zeros[:, :ch, :, :], out), 1)\n\t        out = torch.cat((out, zeros[:, ch:, :, :]), 1)\n", "        return out\n\tclass SkipFeedDown(nn.Module):\n\t    def __init__(\n\t        self, odict: OrderedDict, last=nn.Identity, downsample=nn.Identity()\n\t    ) -> None:\n\t        super().__init__()\n\t        self.md = nn.Sequential(odict)\n\t        self.last = last()\n\t        self.downsample = downsample\n\t    def forward(self, x):\n", "        return self.last(self.md(x) + self.downsample(x))\n"]}
{"filename": "models/model_op.py", "chunked_list": ["from torch import nn\n\tfrom einops import rearrange, repeat\n\tfrom typing import List\n\tfrom functools import reduce\n\tfrom colorama import Fore, Style\n\tfrom torchvision.ops.misc import Conv2dNormActivation\n\tfrom collections import OrderedDict\n\timport warnings\n\timport torch\n\timport torch.nn.functional as F\n", "import re\n\tdef unroll_bn_params(bn):\n\t    bw = bn.weight.data.clone().detach()\n\t    bb = bn.bias.data.clone().detach()\n\t    brm = bn.running_mean.data.clone().detach()\n\t    brv = bn.running_var.data.clone().detach()\n\t    return bw, bb, brm, brv\n\tdef unroll_conv_params(conv, dw=False):\n\t    # params of conv (if dw then to dense)\n\t    if dw:\n", "        cw = conv.weight.data.clone().detach()\n\t        cw = repeat(cw, \"c one h w -> c (rep one) h w\", rep=cw.shape[0])\n\t        cw = torch.diagonal(cw, dim1=0, dim2=1)\n\t        cw = torch.diag_embed(cw, dim1=0, dim2=1)\n\t    else:\n\t        cw = conv.weight.data.clone().detach()\n\t    if conv.bias == None:\n\t        bw = None\n\t    else:\n\t        bw = conv.bias.data.clone().detach()\n", "    return cw, bw\n\tdef unroll_bias_params(bias):\n\t    biasw = bias.weight.data.clone().detach()\n\t    return biasw\n\tdef adjust_with_bn(conv_weight, bias, bn):\n\t    # Address bn\n\t    bw, bb, brm, brv = unroll_bn_params(bn)\n\t    for i in range(bb.size(0)):\n\t        bias[i] *= bw[i] / torch.sqrt(brv[i] + 1e-5)\n\t        conv_weight[i] = conv_weight[i] / torch.sqrt(brv[i] + 1e-5) * bw[i]\n", "        bb[i] -= brm[i] / torch.sqrt(brv[i] + 1e-5) * bw[i]\n\t    for i in range(bb.size(0)):\n\t        bias[i] += bb[i]\n\t    return conv_weight, bias\n\tdef conv_with_new(old_conv, old_bias, new_conv, stride, padding):\n\t    old_conv = rearrange(old_conv, \"oup inp h w -> inp oup h w\")\n\t    f_new_conv = torch.flip(new_conv, [2, 3])\n\t    old_conv = F.conv2d(\n\t        input=old_conv, weight=f_new_conv, stride=1, padding=f_new_conv.shape[2] - 1\n\t    )\n", "    old_conv = rearrange(old_conv, \"inp oup h w -> oup inp h w\")\n\t    old_bias = torch.einsum(\"jikl,i->j\", new_conv, old_bias)\n\t    return old_conv, old_bias\n\t# `stack` is called by reference; pop/push reflects outside the function\n\tdef push_layer(stack, new_name, weight, mode, stride=1, padding=0, bias=None):\n\t    if mode == \"cw\":\n\t        layer = nn.Conv2d(\n\t            weight.size(1),\n\t            weight.size(0),\n\t            kernel_size=weight.size(2),\n", "            stride=stride,\n\t            padding=padding,\n\t            bias=(bias != None),\n\t        )\n\t        layer.weight.data = weight.clone().detach()\n\t        if bias != None:\n\t            layer.bias.data = bias.clone().detach()\n\t    elif mode == \"dw\":\n\t        assert weight.size(1) == weight.size(0)\n\t        layer = nn.Conv2d(\n", "            weight.size(1),\n\t            weight.size(0),\n\t            kernel_size=weight.size(2),\n\t            stride=stride,\n\t            padding=padding,\n\t            bias=(bias != None),\n\t            groups=weight.size(1),\n\t        )\n\t        weight = torch.diagonal(weight, dim1=0, dim2=1)\n\t        weight = rearrange(weight, \"h w (c one) -> c one h w\", one=1)\n", "        layer.weight.data = weight.clone().detach()\n\t        if bias != None:\n\t            layer.bias.data = bias.clone().detach()\n\t    elif mode == \"relu6\":\n\t        layer = nn.ReLU6(inplace=True)\n\t    elif mode == \"relu\":\n\t        layer = nn.ReLU(inplace=True)\n\t    else:\n\t        raise NotImplementedError(\"Not implemented mode\")\n\t    if isinstance(stack, list):\n", "        stack.append((new_name, layer))\n\t    elif isinstance(stack, nn.Sequential):\n\t        stack.add_module(new_name, layer)\n\t    else:\n\t        raise NotImplementedError(\"Not implemented type of stack\")\n\tdef get_mid(tensor: torch.Tensor, reduced_axes: List[int]):\n\t    sz = list(tensor.shape)\n\t    new_sz = sz.copy()\n\t    result = tensor\n\t    ind = [slice(None)] * len(sz)\n", "    for i in reduced_axes:\n\t        new_sz[i] = 1\n\t        ind[i] = sz[i] // 2\n\t        result = result[ind].view(new_sz)\n\t        ind[i] = slice(None)\n\t    return result\n\tdef push_merged_layers(m_layers, pos, weights, cparam, relu=False, act_type=\"relu6\"):\n\t    assert act_type in [\"relu6\", \"relu\"]\n\t    _, m_cw, m_p, m_b = weights\n\t    ctype, cstr = cparam\n", "    push_layer(m_layers, f\"merged_conv{pos}\", m_cw, ctype, cstr, m_p, m_b)\n\t    if relu:\n\t        push_layer(m_layers, f\"relu{pos}\", None, act_type)\n\tdef update_m_weights(isize, m_cw, m_p, m_b, conv, bn, merged):\n\t    cw, bw = unroll_conv_params(conv, conv.in_channels == conv.groups)\n\t    # feature size after conv\n\t    isize = (isize - cw.size(2) + 2 * conv.padding[0]) // conv.stride[0] + 1\n\t    # Convolve cw (w/ flip) and add bw\n\t    if merged:\n\t        m_p += conv.padding[0]\n", "        m_cw, m_b = conv_with_new(m_cw, m_b, cw, conv.stride[0], conv.padding[0])\n\t    else:\n\t        m_p = conv.padding[0]\n\t        m_cw = cw\n\t        m_b = torch.zeros(cw.size(0))\n\t    if bw != None:\n\t        m_b += bw\n\t    assert isinstance(bn, nn.BatchNorm2d) or bn == None\n\t    if isinstance(bn, nn.BatchNorm2d):\n\t        m_cw, m_b = adjust_with_bn(m_cw, m_b, bn)\n", "    return isize, m_cw, m_p, m_b\n\tdef merge_or_new(m_layers, pos, weights, lyrs, merged, cparam, pop_k=2):\n\t    \"\"\"\n\t    - if `merged` == True :\n\t        - Pop the last (conv, bn) in `m_layers` (have same params with `weights`).\n\t        - Merge the `weights` with `lyrs` and return the merged params.\n\t    - if `merged` == False :\n\t        - Return the parameters of `lyrs`.\n\t    \"\"\"\n\t    isize, m_cw, m_p, m_b = weights\n", "    ctype, cstr = cparam\n\t    conv, bn = lyrs\n\t    pos += 1\n\t    isize, m_cw, m_p, m_b = update_m_weights(isize, m_cw, m_p, m_b, conv, bn, merged)\n\t    new_type = \"dw\" if conv.in_channels == conv.groups else \"cw\"\n\t    new_str = conv.stride[0]\n\t    if merged:\n\t        # pop k layers from the back\n\t        del m_layers[-pop_k:]\n\t        pos -= 1\n", "        ctype = \"dw\" if all(tp == \"dw\" for tp in [new_type, ctype]) else \"cw\"\n\t        cstr = cstr * new_str\n\t    else:\n\t        ctype = new_type\n\t        cstr = new_str\n\t    return pos, (isize, m_cw, m_p, m_b), (ctype, cstr)\n\tdef fuse_skip(cw):\n\t    # Fuse identity addition\n\t    mid = cw.size(2) // 2\n\t    for i in range(cw.size(1)):\n", "        cw[i][i][mid][mid] += 1\n\tdef get_skip_info(blocks, arch=\"mbv2\"):\n\t    assert arch in [\"mbv2\"]\n\t    node_pos = 0\n\t    skip_s2t, skip_t2s = dict(), dict()\n\t    str_pos = set()\n\t    for block in blocks:\n\t        if arch == \"mbv2\" and block.use_res_connect:\n\t            src = node_pos\n\t        for layer in block.conv.modules():\n", "            if isinstance(layer, nn.Conv2d):\n\t                node_pos += 1\n\t                # merging strided conv is not implemented yet\n\t                if layer.stride != (1, 1):\n\t                    if arch == \"mbv2\":\n\t                        str_pos.add(node_pos + 1)\n\t        if arch == \"mbv2\" and block.use_res_connect:\n\t            skip_s2t[src], skip_t2s[node_pos] = node_pos, src\n\t    return skip_s2t, skip_t2s, str_pos\n\tdef get_skip_bumps(act_pos, skip_s2t):\n", "    ind, acts = 0, sorted(list(act_pos))\n\t    bumps, bumps_s2t, l = set(), dict(), len(acts)\n\t    if l == 0:\n\t        return bumps, bumps_s2t\n\t    for src, tgt in skip_s2t.items():\n\t        while acts[ind] < tgt:\n\t            if acts[ind] > src:\n\t                bumps.update([src, tgt])\n\t                bumps_s2t[src] = tgt\n\t                break\n", "            ind += 1\n\t            if ind >= l:\n\t                return bumps, bumps_s2t\n\t    return bumps, bumps_s2t\n\tdef adjust_padding(blocks, bumps):\n\t    node_pos, pad, starting_layer = 0, 0, None\n\t    for block in blocks:\n\t        for layer in block.conv:\n\t            if isinstance(layer, nn.Conv2d):\n\t                if starting_layer == None:\n", "                    starting_layer = layer\n\t                if node_pos in bumps and not node_pos == 0:\n\t                    starting_layer.padding = (pad, pad)\n\t                    pad = 0\n\t                    starting_layer = layer\n\t                node_pos += 1\n\t                pad += layer.padding[0]\n\t                layer.padding = (0, 0)\n\t        starting_layer.padding = (pad, pad)\n\tdef adjust_isize(blocks):\n", "    for ind, block in enumerate(blocks):\n\t        if ind == 0:\n\t            cur_isize = block.isize\n\t        else:\n\t            block.isize = cur_isize\n\t        for layer in block.conv:\n\t            if isinstance(layer, nn.Conv2d):\n\t                k, pad, st = layer.kernel_size[0], layer.padding[0], layer.stride[0]\n\t                cur_isize = (cur_isize - k + 2 * pad) // st + 1\n\tdef add_nonlinear(blocks, add_pos):\n", "    node_pos = 0\n\t    for block in blocks:\n\t        for layer in block.conv:\n\t            if isinstance(layer, nn.Conv2d):\n\t                node_pos += 1\n\t        if node_pos in add_pos:\n\t            block.conv.add_module(\"relu3\", nn.ReLU6(inplace=True))\n\tdef fuse_bn(module):\n\t    module.to(\"cpu\")\n\t    # Input module is merged module\n", "    prev_lyr = None\n\t    remove_bns = []\n\t    for name, lyr in module.named_modules():\n\t        # Fuse batchnorm layers with previous conv layers\n\t        if \"md.bn\" in name:\n\t            assert isinstance(prev_lyr, nn.Conv2d) and isinstance(lyr, nn.BatchNorm2d)\n\t            cw, bw = unroll_conv_params(\n\t                prev_lyr, prev_lyr.in_channels == prev_lyr.groups\n\t            )\n\t            m_b = torch.zeros(cw.size(0))\n", "            if bw != None:\n\t                m_b += bw\n\t            m_cw, m_b = adjust_with_bn(cw, m_b, lyr)\n\t            new_conv = nn.Conv2d(\n\t                m_cw.size(1),\n\t                m_cw.size(0),\n\t                kernel_size=m_cw.size(2),\n\t                stride=prev_lyr.stride,\n\t                padding=prev_lyr.padding,\n\t                bias=True,\n", "                groups=prev_lyr.groups,\n\t            )\n\t            # depthwise conv\n\t            if prev_lyr.in_channels == prev_lyr.groups:\n\t                m_cw = torch.diagonal(m_cw, dim1=0, dim2=1)\n\t                m_cw = rearrange(m_cw, \"h w (c one) -> c one h w\", one=1)\n\t            new_conv.weight.data = m_cw.clone().detach()\n\t            new_conv.bias.data = m_b.clone().detach()\n\t            conv_names = prev_name.split(\".\")\n\t            seq = reduce(getattr, conv_names[:-1], module)\n", "            setattr(seq, conv_names[-1], new_conv)\n\t            remove_bns += [name]\n\t        prev_lyr = lyr\n\t        prev_name = name\n\t    module.to(\"cuda\")\n\t    for remove_bn in remove_bns:\n\t        bn_names = remove_bn.split(\".\")\n\t        seq = reduce(getattr, bn_names[:-1], module)\n\t        delattr(seq, bn_names[-1])\n\tdef simulate_merge(conv1: nn.Conv2d, conv2: nn.Conv2d):\n", "    assert all(isinstance(x, nn.Conv2d) for x in [conv1, conv2])\n\t    new_in_channels = conv1.in_channels\n\t    new_out_channels = conv2.out_channels\n\t    new_kernel = (conv1.kernel_size[0] // 2 + conv2.kernel_size[0] // 2) * 2 + 1\n\t    new_stride = conv1.stride[0] * conv2.stride[0]\n\t    new_pad = conv1.padding[0] + conv2.padding[0]\n\t    is_dw = all(x.groups == x.in_channels for x in [conv1, conv2])\n\t    new_conv = nn.Conv2d(\n\t        new_in_channels,\n\t        new_out_channels,\n", "        new_kernel,\n\t        new_stride,\n\t        new_pad,\n\t        groups=conv1.in_channels if is_dw else 1,\n\t    )\n\t    return new_conv\n\tdef simulate_list_merge(convs: List[nn.Conv2d]):\n\t    assert all(isinstance(x, nn.Conv2d) for x in convs)\n\t    new_in_channels = convs[0].in_channels\n\t    new_out_channels = convs[-1].out_channels\n", "    new_kernel = convs[0].kernel_size[0]\n\t    for conv in convs[1:]:\n\t        new_kernel = (new_kernel // 2 + conv.kernel_size[0] // 2) * 2 + 1\n\t    new_stride = convs[0].stride[0]\n\t    for conv in convs[1:]:\n\t        new_stride = new_stride * conv.stride[0]\n\t    new_pad = convs[0].padding[0]\n\t    for conv in convs[1:]:\n\t        new_pad = new_pad + conv.padding[0]\n\t    is_dw = all(x.groups == x.in_channels for x in convs)\n", "    new_conv = nn.Conv2d(\n\t        new_in_channels,\n\t        new_out_channels,\n\t        new_kernel,\n\t        new_stride,\n\t        new_pad,\n\t        groups=convs[0].in_channels if is_dw else 1,\n\t    )\n\t    return new_conv\n\tdef trace_feat_size(blks, inp):\n", "    pos, res = 0, dict()\n\t    out = inp\n\t    res[pos] = tuple(out.shape)\n\t    for blk in blks:\n\t        for lyr in blk.conv:\n\t            out = lyr(out)\n\t            if isinstance(lyr, nn.Conv2d):\n\t                pos += 1\n\t                res[pos] = tuple(out.shape)\n\t    return res\n", "def get_act(blocks, block_type):\n\t    node_pos, act_pos = 0, set()\n\t    for block in blocks:\n\t        if isinstance(block, block_type):\n\t            for layer in block.conv:\n\t                if isinstance(layer, nn.Conv2d):\n\t                    node_pos += 1\n\t                elif isinstance(layer, DepShrinkReLU6):\n\t                    if layer.act_hat:\n\t                        act_pos.add(node_pos)\n", "                elif isinstance(layer, (nn.ReLU, nn.ReLU6)):\n\t                    act_pos.add(node_pos)\n\t    act_num = node_pos\n\t    return act_pos, act_num\n\t# Reset the layers between `st_pos` and `end_pos`\n\tdef reset_layers(blocks, block_type, st_pos, end_pos, logger=None):\n\t    node_pos = 0\n\t    for block in blocks:\n\t        if isinstance(block, block_type):\n\t            for layer in block.conv:\n", "                if isinstance(layer, (nn.Conv2d, nn.BatchNorm2d)):\n\t                    if isinstance(layer, nn.Conv2d):\n\t                        node_pos += 1\n\t                    if st_pos < node_pos and node_pos <= end_pos:\n\t                        if logger:\n\t                            logger.comment(str(layer))\n\t                        layer.reset_parameters()\n\tdef get_blk(blocks, block_type):\n\t    node_pos, blk_pos = 0, set()\n\t    for block in blocks:\n", "        if isinstance(block, block_type):\n\t            for _, (_, layer) in enumerate(block.conv._modules.items()):\n\t                if isinstance(layer, nn.Conv2d):\n\t                    node_pos += 1\n\t        blk_pos.add(node_pos)\n\t    return blk_pos\n\tdef get_conv_lst(blocks, block_type, st, end):\n\t    node_pos, lst = 0, []\n\t    assert st < end\n\t    for block in blocks:\n", "        if isinstance(block, block_type):\n\t            for layer in block.conv:\n\t                if isinstance(layer, nn.Conv2d):\n\t                    if node_pos >= st and node_pos < end:\n\t                        lst.append(layer)\n\t                    if node_pos + 1 == end:\n\t                        return lst\n\t                    node_pos += 1\n\tdef fix_act_lyrs(blocks, block_type, act_pos, act_type=\"relu6\"):\n\t    assert act_type in [\"relu6\", \"relu\"]\n", "    node_pos = 0\n\t    for block in blocks:\n\t        if isinstance(block, block_type):\n\t            for _, (name, layer) in enumerate(block.conv._modules.items()):\n\t                if isinstance(layer, nn.Conv2d):\n\t                    node_pos += 1\n\t                elif isinstance(layer, (nn.ReLU, nn.ReLU6, nn.Identity)):\n\t                    if node_pos in act_pos:\n\t                        if act_type == \"relu6\":\n\t                            setattr(block.conv, name, nn.ReLU6(inplace=True))\n", "                        elif act_type == \"relu\":\n\t                            setattr(block.conv, name, nn.ReLU(inplace=True))\n\t                        else:\n\t                            raise NotImplementedError(\"Not right activation\")\n\t                    else:\n\t                        setattr(block.conv, name, nn.Identity())\n\tdef valid_blks(model):\n\t    skip_s2t = model.skip_s2t\n\t    str_pos = sorted(list(model.str_pos))\n\t    act_num = model.get_act_info()[1]\n", "    breaks = str_pos + [act_num]\n\t    phase = 0\n\t    b_pos = breaks[phase]\n\t    skip_pos = b_pos\n\t    blks = []\n\t    for st in range(0, act_num):\n\t        if st == b_pos:\n\t            if b_pos == breaks[phase]:\n\t                phase += 1\n\t            b_pos = breaks[phase]\n", "        if st == skip_pos:\n\t            skip_pos = b_pos\n\t        end = st + 1\n\t        while end <= min(b_pos, skip_pos):\n\t            blks.append((st, end))\n\t            if end in skip_s2t:\n\t                end = skip_s2t[end]\n\t            else:\n\t                end += 1\n\t        if st in skip_s2t:\n", "            skip_pos = skip_s2t[st]\n\t    return blks\n\t# necessary for backwards compatibility\n\tclass _DeprecatedConvBNAct(Conv2dNormActivation):\n\t    def __init__(self, *args, **kwargs):\n\t        warnings.warn(\n\t            \"The ConvBNReLU/ConvBNActivation classes are deprecated since 0.12 and will be removed in 0.14. \"\n\t            \"Use torchvision.ops.misc.Conv2dNormActivation instead.\",\n\t            FutureWarning,\n\t        )\n", "        if kwargs.get(\"norm_layer\", None) is None:\n\t            kwargs[\"norm_layer\"] = nn.BatchNorm2d\n\t        if kwargs.get(\"activation_layer\", None) is None:\n\t            kwargs[\"activation_layer\"] = nn.ReLU6\n\t        super().__init__(*args, **kwargs)\n\tConvBNReLU = _DeprecatedConvBNAct\n\tConvBNActivation = _DeprecatedConvBNAct\n\tclass DepShrinkReLU6(nn.ReLU6):\n\t    # Inplace op not supported\n\t    def __init__(self, inplace=False) -> None:\n", "        super().__init__()\n\t        self.act = nn.Parameter(torch.tensor([1.0]), requires_grad=True)\n\t        # act_hat is either 1 or 0; indicates if activation is alive\n\t        self.act_hat = 1.0\n\t    def forward(self, x):\n\t        # straight-forward estimator\n\t        act = torch.clamp(self.act, 0, 1)\n\t        act = self.act_hat + act - act.detach()\n\t        return act * F.relu6(x) + (1 - act) * x\n\t    def __repr__(self):\n", "        if self.act_hat == 1.0:\n\t            string = (\n\t                f\"{Fore.GREEN}DepShrinkReLU6() Enabled {Style.RESET_ALL}\"\n\t                + f\"[Act : {self.act.data.item():.2e}]\"\n\t                + f\"[Act Hat : {self.act_hat}]\"\n\t            )\n\t        else:\n\t            string = (\n\t                f\"{Fore.RED}DepShrinkReLU6() Disabled {Style.RESET_ALL}\"\n\t                + f\"[Act : {self.act.data.item():.2e}] \"\n", "                + f\"[Act Hat : {self.act_hat}]\"\n\t            )\n\t        return string\n"]}
{"filename": "models/imagenet/mobilenetv2.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\timport torch\n\timport torch.nn.functional as F\n\tfrom functools import partial\n\tfrom typing import Any, Optional, List, Tuple\n\tfrom torch import Tensor\n\tfrom torch import nn\n\tfrom collections import OrderedDict\n", "from torchvision.ops.misc import Conv2dNormActivation\n\tfrom torchvision.transforms._presets import ImageClassification\n\tfrom torchvision.utils import _log_api_usage_once\n\tfrom torchvision.models._api import WeightsEnum, Weights\n\tfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\tfrom torchvision.models._utils import (\n\t    handle_legacy_interface,\n\t    _ovewrite_named_param,\n\t    _make_divisible,\n\t)\n", "from models.modules_trt import SkipFeed, NaiveFeed\n\tfrom models.model_op import fuse_bn\n\tfrom utils.measure import compile_and_time, torch_time, unroll_merged, torch_cpu_time\n\tfrom fvcore.nn import FlopCountAnalysis\n\t__all__ = [\"InvertedResidual\", \"MobileNetV2\", \"MobileNet_V2_Weights\", \"mobilenet_v2\"]\n\tclass InvertedResidual(nn.Module):\n\t    def __init__(\n\t        self,\n\t        inp: int,\n\t        oup: int,\n", "        stride: int,\n\t        expand_ratio: int,\n\t        norm_layer=nn.BatchNorm2d,\n\t        activation_layer=partial(nn.ReLU6, inplace=True),\n\t        last_relu=False,\n\t    ) -> None:\n\t        super().__init__()\n\t        self.stride = stride\n\t        self.activation_layer = activation_layer\n\t        act_layers = self.make_act_layers(3 if last_relu else 2)\n", "        if stride not in [1, 2]:\n\t            raise ValueError(f\"stride should be 1 or 2 insted of {stride}\")\n\t        hidden_dim = int(round(inp * expand_ratio))\n\t        self.use_res_connect = self.stride == 1 and inp == oup\n\t        layers: List[Tuple[str, torch.nn.Module]] = []\n\t        ind = 1\n\t        if expand_ratio != 1:\n\t            layers += [\n\t                (f\"conv{ind}\", nn.Conv2d(inp, hidden_dim, kernel_size=1, bias=False)),\n\t                (f\"bn{ind}\", norm_layer(hidden_dim)),\n", "                (f\"relu{ind}\", act_layers[0]),\n\t            ]\n\t            ind += 1\n\t        layers += [\n\t            (\n\t                f\"conv{ind}\",\n\t                nn.Conv2d(\n\t                    hidden_dim,\n\t                    hidden_dim,\n\t                    kernel_size=3,\n", "                    stride=self.stride,\n\t                    padding=1,\n\t                    groups=hidden_dim,\n\t                    bias=False,\n\t                ),\n\t            ),\n\t            (f\"bn{ind}\", norm_layer(hidden_dim)),\n\t            (f\"relu{ind}\", act_layers[1]),\n\t            (f\"conv{ind+1}\", nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False)),\n\t            (f\"bn{ind+1}\", norm_layer(oup)),\n", "        ]\n\t        if last_relu:\n\t            layers += [(f\"relu{ind+1}\", act_layers[2])]\n\t        # Each indicating if i-th act is disappeared\n\t        self.is_merged = False\n\t        self.m_layers: List[nn.Module] = []\n\t        self.m_seq: nn.Sequential = None\n\t        self.conv = nn.Sequential(OrderedDict(layers))\n\t        self.in_channels = inp\n\t        self.out_channels = oup\n", "        self._is_cn = stride > 1\n\t    def make_act_layers(self, num=2):\n\t        return [self.activation_layer() for _ in range(num)]\n\t    def forward(self, x: Tensor) -> Tensor:\n\t        if self.is_merged:\n\t            out = self.m_seq(x)\n\t        else:\n\t            pre_skip = len(self.conv)\n\t            if isinstance(self.conv[-1], nn.ReLU6):\n\t                pre_skip -= 1\n", "            out = self.conv[:pre_skip](x)\n\t            if self.use_res_connect:\n\t                # In fine-tuning stage, the size might not match\n\t                diff = (out.shape[-1] - x.shape[-1]) // 2\n\t                x = F.pad(x, [diff] * 4)\n\t                out = out + x\n\t            out = self.conv[pre_skip:](out)\n\t        return out\n\tclass MobileNetV2(nn.Module):\n\t    def __init__(\n", "        self,\n\t        num_classes: int = 1000,\n\t        width_mult: float = 1.0,\n\t        inverted_residual_setting: Optional[List[List[int]]] = None,\n\t        round_nearest: int = 8,\n\t        block=InvertedResidual,\n\t        norm_layer=nn.BatchNorm2d,\n\t        dropout: float = 0.2,\n\t        add_relu: bool = False,\n\t    ) -> None:\n", "        \"\"\"\n\t        MobileNet V2 main class\n\t        Args:\n\t            num_classes (int): Number of classes\n\t            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n\t            inverted_residual_setting: Network structure\n\t            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n\t            Set to 1 to turn off rounding\n\t            block: Module specifying inverted residual building block for mobilenet\n\t            norm_layer: Module specifying the normalization layer to use\n", "            dropout (float): The droupout probability\n\t        \"\"\"\n\t        super().__init__()\n\t        _log_api_usage_once(self)\n\t        input_channel = 32\n\t        last_channel = 1280\n\t        if inverted_residual_setting is None:\n\t            inverted_residual_setting = [\n\t                # t, c, n, s, isize\n\t                [1, 16, 1, 1, 112],\n", "                [6, 24, 2, 2, 112],\n\t                [6, 32, 3, 2, 56],\n\t                [6, 64, 4, 2, 28],\n\t                [6, 96, 3, 1, 14],\n\t                [6, 160, 3, 2, 14],\n\t                [6, 320, 1, 1, 7],\n\t            ]\n\t        # only check the first element, assuming user knows t,c,n,s are required\n\t        if (\n\t            len(inverted_residual_setting) == 0\n", "            or len(inverted_residual_setting[0]) != 5\n\t        ):\n\t            raise ValueError(\n\t                f\"inverted_residual_setting should be non-empty or a 5-element list, got {inverted_residual_setting}\"\n\t            )\n\t        self.add_relu = add_relu\n\t        # building first layer\n\t        self.norm_layer = norm_layer\n\t        self.block = block\n\t        self.width_mult = width_mult\n", "        self.round_nearest = round_nearest\n\t        self.dropout = dropout\n\t        self.num_classes = num_classes\n\t        self.build(input_channel, last_channel, inverted_residual_setting)\n\t        self.initialize()\n\t        self.is_merged = False\n\t        self.is_fixed_act = False\n\t        self.m_blocks: List[nn.Module] = []\n\t        self.m_features = None  # nn.Sequential\n\t        self.compress_k: int = 0\n", "    def make_first_feature(self, input_channel):\n\t        return Conv2dNormActivation(\n\t            3,\n\t            input_channel,\n\t            stride=2,\n\t            norm_layer=self.norm_layer,\n\t            activation_layer=partial(nn.ReLU6, inplace=True),\n\t        )\n\t    def make_feature(self, input_channel, output_channel, stride, t):\n\t        return self.block(\n", "            input_channel,\n\t            output_channel,\n\t            stride,\n\t            expand_ratio=t,\n\t            norm_layer=self.norm_layer,\n\t            last_relu=self.add_relu,\n\t        )\n\t    def make_last_feature(self, input_channel):\n\t        return Conv2dNormActivation(\n\t            input_channel,\n", "            self.last_channel,\n\t            kernel_size=1,\n\t            norm_layer=self.norm_layer,\n\t            activation_layer=partial(nn.ReLU6, inplace=True),\n\t        )\n\t    def set_classifier(self):\n\t        self.classifier = nn.Sequential(\n\t            nn.Dropout(p=self.dropout),\n\t            nn.Linear(self.last_channel, self.num_classes),\n\t        )\n", "    def initialize(self):\n\t        # weight initialization\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n\t                if m.bias is not None:\n\t                    nn.init.zeros_(m.bias)\n\t            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n\t                nn.init.ones_(m.weight)\n\t                nn.init.zeros_(m.bias)\n", "            elif isinstance(m, nn.Linear):\n\t                nn.init.normal_(m.weight, 0, 0.01)\n\t                nn.init.zeros_(m.bias)\n\t    def build(self, input_channel, last_channel, inverted_residual_setting):\n\t        input_channel = _make_divisible(\n\t            input_channel * self.width_mult, self.round_nearest\n\t        )\n\t        self.last_channel = _make_divisible(\n\t            last_channel * max(1.0, self.width_mult), self.round_nearest\n\t        )\n", "        features: List[nn.Module] = []\n\t        features.append(self.make_first_feature(input_channel))\n\t        # building inverted residual blocks\n\t        for t, c, n, s, isize in inverted_residual_setting:\n\t            output_channel = _make_divisible(c * self.width_mult, self.round_nearest)\n\t            self.cur_isize = isize\n\t            for i in range(n):\n\t                stride = s if i == 0 else 1\n\t                features.append(\n\t                    self.make_feature(input_channel, output_channel, stride, t)\n", "                )\n\t                self.cur_isize = self.cur_isize // stride\n\t                input_channel = output_channel\n\t        features.append(self.make_last_feature(input_channel))\n\t        self.features = nn.Sequential(*features)\n\t        self.set_classifier()\n\t    def set_act_hats(self):\n\t        pass\n\t    def _forward_impl(self, x: Tensor) -> Tensor:\n\t        if self.is_merged:\n", "            x = self.m_features(x)\n\t        else:\n\t            if not self.is_fixed_act:\n\t                self.set_act_hats()\n\t            x = self.features(x)\n\t        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n\t        x = torch.flatten(x, 1)\n\t        x = self.classifier(x)\n\t        return x\n\t    def forward(self, x: Tensor) -> Tensor:\n", "        return self._forward_impl(x)\n\t    # Does not merge anything and fuse batchnorm layer\n\t    def merge(self):\n\t        self.is_merged = True\n\t        stack = []\n\t        for _, block in enumerate(self.features):\n\t            if isinstance(block, InvertedResidual):\n\t                if block.use_res_connect:\n\t                    stack.append(SkipFeed(block.conv._modules))\n\t                else:\n", "                    stack.append(NaiveFeed(block.conv._modules))\n\t            else:\n\t                stack.append(block)\n\t        self.m_features = nn.Sequential(*stack)\n\t        fuse_bn(self.m_features)\n\t        print(\"Fused batchnorm...\")\n\t        delattr(self, \"features\")\n\t    def time(self, txt=\"model\", verb=False, trt=True):\n\t        assert self.is_merged\n\t        unrolled_module = unroll_merged(self)\n", "        print(unrolled_module)\n\t        if trt:\n\t            print(\"Start compiling merged model...\")\n\t            result, std = compile_and_time(\n\t                unrolled_module, (128, 3, 224, 224), txt, verb\n\t            )\n\t        else:\n\t            result, std = torch_time(unrolled_module, (128, 3, 224, 224), txt, verb)\n\t        del unrolled_module\n\t        return result, std\n", "    def flops(self):\n\t        assert self.is_merged\n\t        unrolled_module = unroll_merged(self)\n\t        unrolled_module.eval()\n\t        print(unrolled_module)\n\t        inputs = torch.randn((1, 3, 224, 224)).cuda()\n\t        flops = FlopCountAnalysis(unrolled_module, inputs)\n\t        print(f\"number of MFLOPs: {flops.total() / 1e6}\")\n\t        del unrolled_module\n\t        return flops.total()\n", "    def mem(self):\n\t        assert self.is_merged\n\t        mem = torch.cuda.max_memory_allocated()\n\t        print(f\"Before : {mem / 1e6:>15} MB\")\n\t        unrolled_module = unroll_merged(self)\n\t        unrolled_module.eval()\n\t        params = sum(p.numel() for p in unrolled_module.parameters())\n\t        inputs = torch.randn((128, 3, 224, 224)).cuda()\n\t        print(unrolled_module)\n\t        for i in range(10):\n", "            torch.cuda.reset_peak_memory_stats()\n\t            unrolled_module(inputs)\n\t            mem = torch.cuda.max_memory_allocated()\n\t            print(f\"Iter {i}  : {mem / 1e6:>15} MB\")\n\t        return params, mem\n\t    def cpu_time(self, txt=\"model\", verb=False):\n\t        assert self.is_merged\n\t        unrolled_module = unroll_merged(self)\n\t        print(unrolled_module)\n\t        result, std = torch_cpu_time(unrolled_module, (128, 3, 224, 224), txt, verb)\n", "        del unrolled_module\n\t        return result, std\n\t_COMMON_META = {\n\t    \"num_params\": 3504872,\n\t    \"min_size\": (1, 1),\n\t    \"categories\": _IMAGENET_CATEGORIES,\n\t}\n\tclass MobileNet_V2_Weights(WeightsEnum):\n\t    IMAGENET1K_V1 = Weights(\n\t        url=\"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\",\n", "        transforms=partial(ImageClassification, crop_size=224),\n\t        meta={\n\t            **_COMMON_META,\n\t            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv2\",\n\t            \"_metrics\": {\n\t                \"ImageNet-1K\": {\n\t                    \"acc@1\": 71.878,\n\t                    \"acc@5\": 90.286,\n\t                }\n\t            },\n", "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n\t        },\n\t    )\n\t    IMAGENET1K_V2 = Weights(\n\t        url=\"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\",\n\t        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n\t        meta={\n\t            **_COMMON_META,\n\t            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning\",\n\t            \"_metrics\": {\n", "                \"ImageNet-1K\": {\n\t                    \"acc@1\": 72.154,\n\t                    \"acc@5\": 90.822,\n\t                }\n\t            },\n\t            \"_docs\": \"\"\"\n\t                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n\t                `new training recipe\n\t                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n\t            \"\"\",\n", "        },\n\t    )\n\t    DEFAULT = IMAGENET1K_V2\n\t@handle_legacy_interface(weights=(\"pretrained\", MobileNet_V2_Weights.IMAGENET1K_V1))\n\tdef mobilenet_v2(\n\t    *,\n\t    weights: Optional[MobileNet_V2_Weights] = None,\n\t    progress: bool = True,\n\t    **kwargs: Any,\n\t) -> MobileNetV2:\n", "    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n\t    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.\n\t    Args:\n\t        weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n\t            pretrained weights to use. See\n\t            :class:`~torchvision.models.MobileNet_V2_Weights` below for\n\t            more details, and possible values. By default, no pre-trained\n\t            weights are used.\n\t        progress (bool, optional): If True, displays a progress bar of the\n\t            download to stderr. Default is True.\n", "        **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2``\n\t            base class. Please refer to the `source code\n\t            <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_\n\t            for more details about this class.\n\t    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n\t        :members:\n\t    \"\"\"\n\t    weights = MobileNet_V2_Weights.verify(weights)\n\t    if weights is not None:\n\t        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n", "    model = MobileNetV2(**kwargs)\n\t    if weights is not None:\n\t        model.load_state_dict(weights.get_state_dict(progress=progress))\n\t    return model\n\t# The dictionary below is internal implementation detail and will be removed in v0.15\n\tfrom torchvision.models._utils import _ModelURLs\n\tmodel_urls = _ModelURLs(\n\t    {\n\t        \"mobilenet_v2\": MobileNet_V2_Weights.IMAGENET1K_V1.url,\n\t    }\n", ")\n"]}
{"filename": "models/imagenet/mobilenetv2_com.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\tfrom typing import Any, Optional, List\n\tfrom collections import OrderedDict\n\tfrom functools import partial\n\tfrom models.model_op import (\n\t    get_skip_info,\n\t    get_skip_bumps,\n\t    adjust_padding,\n", "    adjust_isize,\n\t    merge_or_new,\n\t    push_merged_layers,\n\t    fuse_skip,\n\t    trace_feat_size,\n\t    get_act,\n\t    get_blk,\n\t    fix_act_lyrs,\n\t)\n\tfrom models.imagenet.mobilenetv2 import InvertedResidual, MobileNetV2\n", "from models.modules_trt import NaiveFeed, SkipFeed\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import nn\n\tfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\t__all__ = [\"LearnMobileNetV2\", \"learn_mobilenet_v2\"]\n\tdef get_merged_list(module):\n\t    merged = []\n\t    if isinstance(module, LearnInvertedResidual):\n\t        for _, act_ind in enumerate(range(2, len(module.conv), 3)):\n", "            act_lyr = module.conv[act_ind]\n\t            if isinstance(act_lyr, (nn.Identity, nn.ReLU6)):\n\t                merged += [isinstance(act_lyr, nn.Identity)]\n\t    else:\n\t        raise NotImplementedError()\n\t    return merged\n\tclass LearnInvertedResidual(InvertedResidual):\n\t    def __init__(\n\t        self,\n\t        inp: int,\n", "        oup: int,\n\t        stride: int,\n\t        expand_ratio: int,\n\t        isize: int,\n\t        norm_layer=nn.BatchNorm2d,\n\t        last_relu=False,\n\t    ) -> None:\n\t        super().__init__(\n\t            inp=inp,\n\t            oup=oup,\n", "            stride=stride,\n\t            expand_ratio=expand_ratio,\n\t            norm_layer=norm_layer,\n\t            activation_layer=nn.ReLU6,\n\t            last_relu=last_relu,\n\t        )\n\t        self.isize = isize\n\t        self.merged = [False] * (len(self.conv) // 3)\n\t    def merge(self):\n\t        if self.is_merged:\n", "            print(\"Model already Merged\")\n\t            return\n\t        self.is_merged = True\n\t        # weights = (isize, m_cw, m_p, m_b), cparam = (ctype, cstr)\n\t        weights, cparam = (self.isize, None, None, None), (\"dw\", 1)\n\t        pos = 0\n\t        # Bring merged list from act_lyr\n\t        self.merged = get_merged_list(self)\n\t        for ind in range(0, len(self.conv), 3):\n\t            is_merged = False if ind == 0 else self.merged[(ind - 2) // 3]\n", "            is_last = ind >= len(self.conv) - 3\n\t            relu = not (ind == len(self.conv) - 2)\n\t            if (ind == len(self.conv) - 3) and self.merged[-1]:\n\t                relu = False\n\t            conv, bn = self.conv[ind], self.conv[ind + 1]\n\t            lyrs = (conv, bn)\n\t            pos, weights, cparam = merge_or_new(\n\t                self.m_layers, pos, weights, lyrs, is_merged, cparam\n\t            )\n\t            if all(self.merged + [is_last, self.use_res_connect]):\n", "                fuse_skip(weights[1])\n\t                self.use_res_connect = False\n\t            push_merged_layers(self.m_layers, pos, weights, cparam, relu=relu)\n\t        delattr(self, \"conv\")\n\t        if not self.use_res_connect:\n\t            self.m_seq = NaiveFeed(OrderedDict(self.m_layers))\n\t        elif relu:\n\t            self.m_seq = SkipFeed(OrderedDict(self.m_layers[:-1]), nn.ReLU6)\n\t        elif not relu:\n\t            self.m_seq = SkipFeed(OrderedDict(self.m_layers))\n", "    def fix_act(self):\n\t        self.conv.conv1.padding = (1, 1)\n\t        self.conv.conv2.padding = (0, 0)\n\t        if hasattr(self.conv, \"conv3\"):\n\t            self.conv.conv3.padding = (0, 0)\n\t    def extra_repr(self):\n\t        r\"\"\"Set the extra representation of the module\n\t        To print customized extra information, you should re-implement\n\t        this method in your own modules. Both single-line and multi-line\n\t        strings are acceptable.\n", "        \"\"\"\n\t        if self.use_res_connect:\n\t            out = f\"[Skip : Enabled]  [isize : {self.isize}]\"\n\t        else:\n\t            out = f\"[Skip : Disabled] [isize : {self.isize}]\"\n\t        return out\n\tclass LearnMobileNetV2(MobileNetV2):\n\t    def __init__(\n\t        self,\n\t        num_classes: int = 1000,\n", "        width_mult: float = 1.0,\n\t        inverted_residual_setting: Optional[List[List[int]]] = None,\n\t        round_nearest: int = 8,\n\t        block=LearnInvertedResidual,\n\t        norm_layer=nn.BatchNorm2d,\n\t        dropout: float = 0.2,\n\t        add_relu: bool = False,\n\t    ) -> None:\n\t        super().__init__(\n\t            num_classes,\n", "            width_mult,\n\t            inverted_residual_setting,\n\t            round_nearest,\n\t            block,\n\t            norm_layer,\n\t            dropout,\n\t            add_relu,\n\t        )\n\t        self.skip_s2t, self.skip_t2s, self.str_pos = get_skip_info(self.features[1:-1])\n\t        self.skip_bump, self.act_pos = set(), set()\n", "        inp = torch.randn(128, 3, 224, 224)\n\t        out = self.features[0](inp)\n\t        self.out_shape = trace_feat_size(self.features[1:-1], out)\n\t    def make_feature(self, input_channel, output_channel, stride, t):\n\t        return self.block(\n\t            input_channel,\n\t            output_channel,\n\t            stride,\n\t            expand_ratio=t,\n\t            isize=self.cur_isize,\n", "            norm_layer=self.norm_layer,\n\t            last_relu=self.add_relu,\n\t        )\n\t    def get_act_info(self):\n\t        act_pos, act_num = get_act(self.features[1:-1], InvertedResidual)\n\t        return act_pos, act_num\n\t    def get_blk_info(self):\n\t        blk_end = sorted(get_blk(self.features[1:-1], InvertedResidual))\n\t        blks = dict([(ind + 1, end) for ind, end in enumerate(blk_end)])\n\t        blk_num = len(blks)\n", "        return blks, blk_num\n\t    def fix_act(self, act_pos=None, merge_pos=None):\n\t        self.is_fixed_act = True\n\t        # position in nodes and bumps denote the number of come-acrossed conv\n\t        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n\t        self.merge_pos = merge_pos if merge_pos != None else self.act_pos\n\t        assert self.act_pos.issubset(self.merge_pos)\n\t        self.skip_bump, _ = get_skip_bumps(self.merge_pos, self.skip_s2t)\n\t        bumps = set.union(self.str_pos, self.merge_pos, self.skip_bump)\n\t        fix_act_lyrs(self.features[1:-1], InvertedResidual, self.act_pos)\n", "        # adjust padding and input size among the conv layers w.r.t. bumps\n\t        adjust_padding(self.features[1:-1], bumps)\n\t        adjust_isize(self.features[1:-1])\n\t    def merge(self, act_pos=None, merge_pos=None, keep_feat=False):\n\t        self.to(\"cpu\")\n\t        self.is_merged = True\n\t        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n\t        self.merge_pos = merge_pos if merge_pos != None else self.act_pos\n\t        assert self.act_pos.issubset(self.merge_pos)\n\t        self.skip_bump, skip_bump_s2t = get_skip_bumps(self.merge_pos, self.skip_s2t)\n", "        bumps = set.union(self.str_pos, self.merge_pos, self.skip_bump)\n\t        self.m_layers, stack = [], []\n\t        pos, bump_end, loc_end, last = 0, None, None, None\n\t        # weights = (isize, m_cw, m_p, m_b), cparam = (ctype, cstr)\n\t        weights, cparam = (112, None, None, None), (\"dw\", 1)\n\t        for block in self.features[1:-1]:\n\t            for ind in range(0, len(block.conv), 3):\n\t                if pos in skip_bump_s2t:\n\t                    if len(stack) > 0:\n\t                        self.m_layers += [NaiveFeed(OrderedDict(stack))]\n", "                        stack = []\n\t                    bump_end = skip_bump_s2t[pos]\n\t                relu = not (ind == len(block.conv) - 2)\n\t                if not pos + 1 in self.act_pos:\n\t                    relu = False\n\t                conv, bn = block.conv[ind], block.conv[ind + 1]\n\t                lyrs = (conv, bn)\n\t                is_merged = pos > 0 and not pos in bumps\n\t                if pos in self.skip_s2t and not pos in skip_bump_s2t:\n\t                    loc_end = self.skip_s2t[pos]\n", "                    saved = (stack, weights, cparam, last, is_merged)\n\t                    weights, cparam = (weights[0], None, None, None), (\"dw\", 1)\n\t                    stack, last, is_merged = [], None, False\n\t                if pos + 1 == loc_end:\n\t                    _, weights, cparam = merge_or_new(\n\t                        stack, pos, weights, lyrs, is_merged, cparam, last\n\t                    )\n\t                    fuse_skip(weights[1])\n\t                    push_merged_layers(stack, pos, weights, cparam, relu=relu)\n\t                    # Doesn't have bn (already fused), thus give `None` in the place of `bn`\n", "                    lyrs = stack[0][1], None\n\t                    relu = len(stack) > 1\n\t                    stack, weights, cparam, last, is_merged = saved\n\t                _, weights, cparam = merge_or_new(\n\t                    stack, pos, weights, lyrs, is_merged, cparam, last\n\t                )\n\t                push_merged_layers(stack, pos, weights, cparam, relu=relu)\n\t                if pos + 1 == bump_end:\n\t                    if relu:\n\t                        layer = partial(nn.ReLU6, inplace=True)\n", "                        skipfeed = SkipFeed(OrderedDict(stack[:-1]), layer)\n\t                    else:\n\t                        skipfeed = SkipFeed(OrderedDict(stack))\n\t                    self.m_layers += [skipfeed]\n\t                    stack = []\n\t                pos += 1\n\t                last = 2 if relu else 1\n\t        if len(stack) > 0:\n\t            self.m_layers += [NaiveFeed(OrderedDict(stack))]\n\t        self.m_layers = [self.features[0]] + self.m_layers + [self.features[-1]]\n", "        self.m_features = nn.Sequential(*self.m_layers)\n\t        if not keep_feat:\n\t            delattr(self, \"features\")\n\t        self.to(\"cuda\")\n\t    def unmerge(self):\n\t        assert hasattr(self, \"features\")\n\t        if self.is_merged:\n\t            self.is_merged = False\n\t            delattr(self, \"m_features\")\n\t_COMMON_META = {\n", "    \"num_params\": 3504872,\n\t    \"min_size\": (1, 1),\n\t    \"categories\": _IMAGENET_CATEGORIES,\n\t}\n\tdef learn_mobilenet_v2(\n\t    **kwargs: Any,\n\t) -> LearnMobileNetV2:\n\t    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n\t    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.\n\t    Args:\n", "        weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n\t            pretrained weights to use. See\n\t            :class:`~torchvision.models.MobileNet_V2_Weights` below for\n\t            more details, and possible values. By default, no pre-trained\n\t            weights are used.\n\t        progress (bool, optional): If True, displays a progress bar of the\n\t            download to stderr. Default is True.\n\t        **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2``\n\t            base class. Please refer to the `source code\n\t            <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_\n", "            for more details about this class.\n\t    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n\t        :members:\n\t    \"\"\"\n\t    model = LearnMobileNetV2(**kwargs)\n\t    return model\n"]}
{"filename": "models/imagenet/__init__.py", "chunked_list": ["from .mobilenetv2 import *\n\tfrom .mobilenetv2_com import *\n\tfrom .mobilenetv2_ds import *\n\tfrom .vgg import *\n\tfrom .vgg_com import *\n\tmodels = {\n\t    \"mobilenet_v2\": mobilenet_v2,\n\t    \"learn_mobilenet_v2\": learn_mobilenet_v2,\n\t    \"dep_shrink_mobilenet_v2\": dep_shrink_mobilenet_v2,\n\t    \"vgg19\": vgg19_bn,\n", "    \"learn_vgg19\": learn_vgg19_bn,\n\t}\n\tblocks = {\n\t    \"mobilenet_v2\": InvertedResidual,\n\t    \"learn_mobilenet_v2\": InvertedResidual,\n\t    \"dep_shrink_mobilenet_v2\": InvertedResidual,\n\t    \"vgg19\": VGGBlock,\n\t    \"learn_vgg19\": LearnVGGBlock,\n\t}\n"]}
{"filename": "models/imagenet/mobilenetv2_ds.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\tfrom functools import partial\n\tfrom collections import OrderedDict\n\tfrom models.imagenet.mobilenetv2 import InvertedResidual, MobileNetV2\n\tfrom typing import Any, Optional, List\n\tfrom models.model_op import (\n\t    get_act,\n\t    get_blk,\n", "    adjust_padding,\n\t    fix_act_lyrs,\n\t    add_nonlinear,\n\t    merge_or_new,\n\t    push_merged_layers,\n\t    fuse_skip,\n\t    DepShrinkReLU6,\n\t)\n\tfrom models.modules_trt import SkipFeed, NaiveFeed\n\timport torch\n", "from torch import nn\n\tfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\t__all__ = [\"DepShrinkMobileNetV2\", \"dep_shrink_mobilenet_v2\"]\n\tclass DepShrinkInvertedResidual(InvertedResidual):\n\t    def __init__(\n\t        self,\n\t        inp: int,\n\t        oup: int,\n\t        stride: int,\n\t        expand_ratio: int,\n", "        isize: int,\n\t        norm_layer=nn.BatchNorm2d,\n\t    ) -> None:\n\t        super().__init__(\n\t            inp=inp,\n\t            oup=oup,\n\t            stride=stride,\n\t            expand_ratio=expand_ratio,\n\t            norm_layer=norm_layer,\n\t            activation_layer=DepShrinkReLU6,\n", "        )\n\t        self.isize = isize\n\t    def make_act_layers(self, _):\n\t        self.act_layer = self.activation_layer()\n\t        return self.act_layer, self.act_layer\n\t    def set_act_hat(self, val):\n\t        self.act_layer.act_hat = val\n\t    def get_act_hat(self):\n\t        return self.act_layer.act_hat\n\t    def merge(self, merged):\n", "        if self.is_merged:\n\t            print(\"Model already Merged\")\n\t            return\n\t        self.is_merged = True\n\t        # weights = (isize, m_cw, m_p, m_b), cparam = (ctype, cstr)\n\t        weights, cparam = (self.isize, None, None, None), (\"dw\", 1)\n\t        pos = 0\n\t        last_relu = isinstance(self.conv[-1], nn.ReLU6)\n\t        for ind in range(0, len(self.conv), 3):\n\t            is_merged = False if ind == 0 else merged\n", "            is_last = ind >= len(self.conv) - 3\n\t            relu = not (ind == len(self.conv) - 2)\n\t            conv, bn = self.conv[ind], self.conv[ind + 1]\n\t            lyrs = (conv, bn)\n\t            pos, weights, cparam = merge_or_new(\n\t                self.m_layers, pos, weights, lyrs, is_merged, cparam\n\t            )\n\t            if all([merged, is_last, self.use_res_connect]):\n\t                fuse_skip(weights[1])\n\t                self.use_res_connect = False\n", "            push_merged_layers(self.m_layers, pos, weights, cparam, relu=relu)\n\t        delattr(self, \"conv\")\n\t        if self.use_res_connect and last_relu:\n\t            layer = partial(nn.ReLU6, inplace=True)\n\t            self.m_seq = SkipFeed(OrderedDict(self.m_layers[:-1]), layer)\n\t        elif self.use_res_connect and not last_relu:\n\t            self.m_seq = SkipFeed(OrderedDict(self.m_layers))\n\t        else:\n\t            self.m_seq = NaiveFeed(OrderedDict(self.m_layers))\n\t    def extra_repr(self):\n", "        r\"\"\"Set the extra representation of the module\n\t        To print customized extra information, you should re-implement\n\t        this method in your own modules. Both single-line and multi-line\n\t        strings are acceptable.\n\t        \"\"\"\n\t        if self.use_res_connect:\n\t            out = f\"[Skip : Enabled]\"\n\t        else:\n\t            out = f\"[Skip : Disabled]\"\n\t        return out\n", "class DepShrinkMobileNetV2(MobileNetV2):\n\t    def __init__(\n\t        self,\n\t        num_classes: int = 1000,\n\t        width_mult: float = 1.0,\n\t        inverted_residual_setting: Optional[List[List[int]]] = None,\n\t        round_nearest: int = 8,\n\t        block=DepShrinkInvertedResidual,\n\t        norm_layer=nn.BatchNorm2d,\n\t        dropout: float = 0.2,\n", "    ) -> None:\n\t        super().__init__(\n\t            num_classes,\n\t            width_mult,\n\t            inverted_residual_setting,\n\t            round_nearest,\n\t            block,\n\t            norm_layer,\n\t            dropout,\n\t        )\n", "        self.compress_k = -1\n\t        self.act_pos = set()\n\t    def make_feature(self, input_channel, output_channel, stride, t):\n\t        return self.block(\n\t            input_channel,\n\t            output_channel,\n\t            stride,\n\t            expand_ratio=t,\n\t            isize=self.cur_isize,\n\t            norm_layer=self.norm_layer,\n", "        )\n\t    def load_pattern(self, pat):\n\t        if pat == \"none\":\n\t            return\n\t        elif pat == \"A\":\n\t            lst = [0, 3, 4, 6, 8, 10, 11, 13, 14, 15, 16]\n\t        elif pat == \"B\":\n\t            lst = [3, 4, 10, 11, 13, 14, 15, 16]\n\t        elif pat == \"C\":\n\t            lst = [0, 3, 10, 11, 13, 14, 15, 16]\n", "        elif pat == \"D\":\n\t            lst = [3, 4, 13, 14, 15, 16]\n\t        elif pat == \"E\":\n\t            lst = [0, 3, 13, 14, 15, 16]\n\t        elif pat == \"F\":\n\t            lst = []\n\t        elif pat == \"A10\":\n\t            lst = [2, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16]\n\t        elif pat == \"B10\":\n\t            lst = [0, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16]\n", "        elif pat == \"C10\":\n\t            lst = [0, 3, 4, 6, 10, 13, 14, 15, 16]\n\t        elif pat == \"D10\":\n\t            lst = [0, 3, 10, 11, 13, 15, 16]\n\t        elif pat == \"AR\":\n\t            lst = [5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n\t        elif pat == \"BR\":\n\t            lst = [4, 10, 11, 12, 13, 14, 15, 16]\n\t        elif pat == \"CR\":\n\t            lst = [8, 9, 10, 11, 12, 16]\n", "        elif pat == \"AR10\":\n\t            lst = [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n\t        elif pat == \"BR10\":\n\t            lst = [4, 7, 8, 9, 12, 13, 14, 15, 16]\n\t        elif pat == \"CR10\":\n\t            lst = [8, 10, 12, 13, 14, 15, 16]\n\t        elif pat == \"AR_AUG\":\n\t            lst = [0, 3, 5, 7, 8, 9, 12, 13, 14, 15, 16]\n\t        elif pat == \"BR_AUG\":\n\t            lst = [5, 7, 8, 9, 12, 14, 15, 16]\n", "        elif pat == \"CR_AUG\":\n\t            lst = [9, 11, 12, 14, 15, 16]\n\t        elif pat == \"AR10_AUG\":\n\t            lst = [0, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n\t        elif pat == \"BR10_AUG\":\n\t            lst = [5, 7, 8, 9, 12, 13, 14, 15, 16]\n\t        elif pat == \"CR10_AUG\":\n\t            lst = [6, 7, 9, 13, 14, 15, 16]\n\t        else:\n\t            raise NotImplementedError()\n", "        for ind, block in enumerate(self.features[1:-1]):\n\t            if ind in lst:\n\t                block.act_layer.act.data = torch.tensor([1e5])\n\t                block.act_layer.act.requires_grad = False\n\t            else:\n\t                block.act_layer.act.data = torch.tensor([-1e5])\n\t                block.act_layer.act.requires_grad = False\n\t        print(f\"Loaded pattern {pat}\")\n\t    def set_act_hats(self):\n\t        assert self.compress_k >= 0\n", "        lst, k = [], self.compress_k\n\t        for block in self.features[1:-1]:\n\t            if isinstance(block, DepShrinkInvertedResidual):\n\t                lst.append(block.act_layer.act.data)\n\t            else:\n\t                lst.append(torch.tensor([float(\"-inf\")]))\n\t        acts = torch.cat(lst)\n\t        _, act_ind = torch.sort(acts, stable=True)\n\t        # set act_hats for top-k blocks\n\t        for ind, block in enumerate(self.features[1:-1]):\n", "            if isinstance(block, DepShrinkInvertedResidual):\n\t                if k == 0:\n\t                    block.set_act_hat(0.0)\n\t                elif ind in act_ind[-k:]:\n\t                    block.set_act_hat(1.0)\n\t                else:\n\t                    block.set_act_hat(0.0)\n\t    def get_act_info(self):\n\t        self.set_act_hats()\n\t        act_pos, act_num = get_act(self.features[1:-1], InvertedResidual)\n", "        # DepthShrinker appends activation to merged blocks\n\t        blk_pos = get_blk(self.features[1:-1], InvertedResidual)\n\t        add_pos, blk_empty = set(), True\n\t        for i in range(act_num + 1):\n\t            if i in act_pos:\n\t                blk_empty = False\n\t            if i in blk_pos:\n\t                if blk_empty:\n\t                    add_pos.add(i)\n\t                else:\n", "                    blk_empty = True\n\t        act_pos = set.union(act_pos, add_pos)\n\t        return act_pos, act_num\n\t    def get_arch_parameters(self):\n\t        return [w for name, w in self.named_parameters() if \"act\" in name]\n\t    def regularizer(self, mode=\"soft\"):\n\t        if mode == \"soft\":\n\t            arch_params = torch.cat(self.get_arch_parameters())\n\t            loss = torch.sum(arch_params)\n\t        elif mode == \"w1.4\":\n", "            # fmt: off\n\t            lat = torch.tensor([15.19, 133.34, 75.77, 43.8, 30.39, 30.39, 18.81, 15.19, 15.19, 15.19, 17.0, 26.23, 26.23, 13.23, 11.41, 11.41, 14.39])\n\t            # fmt: on\n\t            arch_params = torch.cat(self.get_arch_parameters())\n\t            assert len(lat) == len(arch_params)\n\t            loss = torch.sum(torch.mul(lat.to(\"cuda\"), arch_params))\n\t        elif mode == \"w1.0\":\n\t            # fmt: off\n\t            lat = torch.tensor([18.15, 85.52, 57.97, 35.78, 20.93, 20.93, 12.51, 11.45, 11.45, 11.45, 11.64, 17.85, 17.85, 9.49, 7.35, 7.35, 8.48])\n\t            # fmt: on\n", "            arch_params = torch.cat(self.get_arch_parameters())\n\t            assert len(lat) == len(arch_params)\n\t            loss = torch.sum(torch.mul(lat.to(\"cuda\"), arch_params))\n\t        else:\n\t            raise NotImplementedError()\n\t        return loss\n\t    def fix_act(self, act_pos=None):\n\t        if self.is_fixed_act:\n\t            print(\"Model already Fixed\")\n\t            return\n", "        self.is_fixed_act = True\n\t        # position in nodes and bumps denote the number of come-acrossed conv\n\t        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n\t        blk_end = sorted(get_blk(self.features[1:-1], InvertedResidual))\n\t        fix_act_lyrs(self.features[1:-1], InvertedResidual, self.act_pos)\n\t        add_nonlinear(self.features[1:-1], self.act_pos)\n\t        adjust_padding(self.features[1:-1], set.union(self.act_pos, blk_end))\n\t        for block in self.features[1:-1]:\n\t            delattr(block, \"act_layer\")\n\t    def merge(self, act_pos=None):\n", "        if self.is_merged:\n\t            print(\"Model already Merged\")\n\t            return\n\t        self.is_merged = True\n\t        # position in nodes and bumps denote the number of come-acrossed conv\n\t        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n\t        node_pos = 0\n\t        for _, block in enumerate(self.features[1:-1]):\n\t            for _, layer in block.conv._modules.items():\n\t                if isinstance(layer, nn.Conv2d):\n", "                    node_pos += 1\n\t            self.m_blocks.append(block)\n\t            is_merged = node_pos in self.act_pos\n\t            if isinstance(block, DepShrinkInvertedResidual):\n\t                block.merge(is_merged)\n\t        self.m_blocks = [self.features[0]] + self.m_blocks + [self.features[-1]]\n\t        stack = []\n\t        for _, block in enumerate(self.m_blocks):\n\t            if isinstance(block, DepShrinkInvertedResidual):\n\t                if block.use_res_connect:\n", "                    stack.append(\n\t                        SkipFeed(block.m_seq.md._modules, block.m_seq.last.__class__)\n\t                    )\n\t                else:\n\t                    stack.append(NaiveFeed(block.m_seq.md._modules))\n\t            else:\n\t                stack.append(block)\n\t        self.m_features = nn.Sequential(*stack)\n\t        delattr(self, \"features\")\n\t_COMMON_META = {\n", "    \"num_params\": 3504872,\n\t    \"min_size\": (1, 1),\n\t    \"categories\": _IMAGENET_CATEGORIES,\n\t}\n\tdef dep_shrink_mobilenet_v2(\n\t    **kwargs: Any,\n\t) -> DepShrinkMobileNetV2:\n\t    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n\t    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.\n\t    Args:\n", "        weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n\t            pretrained weights to use. See\n\t            :class:`~torchvision.models.MobileNet_V2_Weights` below for\n\t            more details, and possible values. By default, no pre-trained\n\t            weights are used.\n\t        progress (bool, optional): If True, displays a progress bar of the\n\t            download to stderr. Default is True.\n\t        **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2``\n\t            base class. Please refer to the `source code\n\t            <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_\n", "            for more details about this class.\n\t    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n\t        :members:\n\t    \"\"\"\n\t    model = DepShrinkMobileNetV2(**kwargs)\n\t    return model\n"]}
{"filename": "models/imagenet/vgg.py", "chunked_list": ["from functools import partial\n\tfrom typing import Union, List, Dict, Any, Optional, cast\n\tfrom collections import OrderedDict\n\timport torch\n\timport torch.nn as nn\n\tfrom torchvision.transforms._presets import ImageClassification\n\tfrom torchvision.utils import _log_api_usage_once\n\tfrom torchvision.models._api import WeightsEnum, Weights\n\tfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\tfrom torchvision.models._utils import handle_legacy_interface, _ovewrite_named_param\n", "from models.modules_trt import NaiveFeed\n\tfrom models.model_op import fuse_bn\n\tfrom utils.measure import compile_and_time, torch_time, unroll_merged_vgg\n\t__all__ = [\n\t    \"VGG\",\n\t    \"VGGBlock\",\n\t    \"vgg19_bn\",\n\t    \"vgg_cfgs\",\n\t]\n\tclass VGGBlock(nn.Module):\n", "    def __init__(self, inp: int, oup: int, num: int) -> None:\n\t        super().__init__()\n\t        layers = []\n\t        ind = 1\n\t        for _ in range(num):\n\t            if ind != 1:\n\t                inp = oup\n\t            layers += [\n\t                (f\"conv{ind}\", nn.Conv2d(inp, oup, kernel_size=3, padding=1)),\n\t                (f\"bn{ind}\", nn.BatchNorm2d(oup)),\n", "                (f\"relu{ind}\", nn.ReLU(inplace=True)),\n\t            ]\n\t            ind += 1\n\t        layers += [(f\"pool{ind}\", nn.MaxPool2d(kernel_size=2, stride=2))]\n\t        # Each indicating if i-th act is disappeared\n\t        self.conv = nn.Sequential(OrderedDict(layers))\n\t    def forward(self, x):\n\t        out = self.conv(x)\n\t        return out\n\tclass VGG(nn.Module):\n", "    def __init__(\n\t        self,\n\t        cfg,\n\t        num_classes: int = 1000,\n\t        dropout: float = 0.5,\n\t        block: nn.Module = VGGBlock,\n\t    ) -> None:\n\t        super().__init__()\n\t        _log_api_usage_once(self)\n\t        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n", "        self.dropout = dropout\n\t        self.num_classes = num_classes\n\t        self.block = block\n\t        self.build(cfg)\n\t        self.initialize()\n\t        self.is_merged = False\n\t    def set_classifier(self):\n\t        self.classifier = nn.Sequential(\n\t            nn.Linear(512 * 7 * 7, 4096),\n\t            nn.ReLU(True),\n", "            nn.Dropout(p=self.dropout),\n\t            nn.Linear(4096, 4096),\n\t            nn.ReLU(True),\n\t            nn.Dropout(p=self.dropout),\n\t            nn.Linear(4096, self.num_classes),\n\t        )\n\t    def make_feature(self, inp, oup, n):\n\t        return self.block(inp, oup, n)\n\t    def build(self, cfg):\n\t        features = []\n", "        for n, inp, oup, isize in cfg:\n\t            self.cur_isize = isize\n\t            features.append(self.make_feature(inp, oup, n))\n\t        self.features = nn.Sequential(*features)\n\t        self.set_classifier()\n\t    def initialize(self):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n\t                if m.bias is not None:\n", "                    nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.normal_(m.weight, 0, 0.01)\n\t                nn.init.constant_(m.bias, 0)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        if self.is_merged:\n\t            x = self.m_features(x)\n", "        else:\n\t            x = self.features(x)\n\t        x = self.avgpool(x)\n\t        x = torch.flatten(x, 1)\n\t        x = self.classifier(x)\n\t        return x\n\t    # Does not merge anything and fuse batchnorm layer\n\t    def merge(self):\n\t        self.is_merged = True\n\t        stack = []\n", "        for _, block in enumerate(self.features):\n\t            stack.append(NaiveFeed(block.conv._modules))\n\t        self.m_features = nn.Sequential(*stack)\n\t        fuse_bn(self.m_features)\n\t        print(\"Fused batchnorm...\")\n\t        delattr(self, \"features\")\n\t    def time(self, txt=\"model\", verb=False, trt=True):\n\t        assert self.is_merged\n\t        unrolled_module = unroll_merged_vgg(self)\n\t        print(unrolled_module)\n", "        if trt:\n\t            print(\"Start compiling merged model...\")\n\t            result, std = compile_and_time(\n\t                unrolled_module, (64, 3, 224, 224), txt, verb\n\t            )\n\t        else:\n\t            result, std = torch_time(\n\t                unrolled_module, (64, 3, 224, 224), txt, verb, rep=200, warmup=300\n\t            )\n\t        del unrolled_module\n", "        return result, std\n\t    def mem(self):\n\t        assert self.is_merged\n\t        mem = torch.cuda.max_memory_allocated()\n\t        print(f\"Before : {mem / 1e6:>15} MB\")\n\t        unrolled_module = unroll_merged_vgg(self)\n\t        unrolled_module.eval()\n\t        params = sum(p.numel() for p in unrolled_module.parameters())\n\t        inputs = torch.randn((64, 3, 224, 224)).cuda()\n\t        print(unrolled_module)\n", "        for i in range(10):\n\t            torch.cuda.reset_peak_memory_stats()\n\t            unrolled_module(inputs)\n\t            mem = torch.cuda.max_memory_allocated()\n\t            print(f\"Iter {i}  : {mem / 1e6:>15} MB\")\n\t        return params, mem\n\t# fmt: off\n\t# cfgs: Dict[str, List[Union[str, int]]] = {\n\t#     \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n\t#     \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n", "#     \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n\t#     \"E\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n\t# }\n\t# fmt: on\n\tvgg_cfgs: Dict[str, List[Union[str, int]]] = {\n\t    # (n, inp, oup, isize)\n\t    \"19\": [\n\t        (2, 3, 64, 224),\n\t        (2, 64, 128, 112),\n\t        (4, 128, 256, 56),\n", "        (4, 256, 512, 28),\n\t        (4, 512, 512, 14),\n\t    ]\n\t}\n\tdef _vgg(\n\t    cfg: str,\n\t    weights: Optional[WeightsEnum],\n\t    progress: bool,\n\t    **kwargs: Any,\n\t) -> VGG:\n", "    if weights is not None:\n\t        kwargs[\"init_weights\"] = False\n\t        if weights.meta[\"categories\"] is not None:\n\t            _ovewrite_named_param(\n\t                kwargs, \"num_classes\", len(weights.meta[\"categories\"])\n\t            )\n\t    model = VGG(vgg_cfgs[cfg], **kwargs)\n\t    if weights is not None:\n\t        model.load_state_dict(weights.get_state_dict(progress=progress))\n\t    return model\n", "def vgg19_bn(*, weights=None, progress=True, **kwargs: Any) -> VGG:\n\t    return _vgg(\"19\", weights, progress, **kwargs)\n"]}
{"filename": "models/imagenet/vgg_com.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\tfrom typing import Any, Optional, List\n\tfrom collections import OrderedDict\n\tfrom functools import partial\n\tfrom models.model_op import (\n\t    adjust_padding,\n\t    adjust_isize,\n\t    merge_or_new,\n", "    push_merged_layers,\n\t    trace_feat_size,\n\t    get_act,\n\t    get_blk,\n\t    fix_act_lyrs,\n\t)\n\tfrom models.imagenet.vgg import VGG, VGGBlock, vgg_cfgs\n\tfrom models.modules_trt import NaiveFeed, SkipFeed\n\tfrom utils.measure import compile_and_time\n\timport torch\n", "import torch.nn.functional as F\n\tfrom torch import nn\n\tfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\t__all__ = [\"LearnVGG\", \"LearnVGGBlock\", \"learn_vgg19_bn\"]\n\tdef push_mp_layer(stack, new_name):\n\t    layer = nn.MaxPool2d(kernel_size=2, stride=2)\n\t    if isinstance(stack, list):\n\t        stack.append((new_name, layer))\n\t    elif isinstance(stack, nn.Sequential):\n\t        stack.add_module(new_name, layer)\n", "    else:\n\t        raise NotImplementedError(\"Not implemented type of stack\")\n\tclass LearnVGGBlock(VGGBlock):\n\t    def __init__(\n\t        self,\n\t        inp: int,\n\t        oup: int,\n\t        num: int,\n\t        isize: int,\n\t    ) -> None:\n", "        super().__init__(inp, oup, num)\n\t        self.isize = isize\n\t    def extra_repr(self):\n\t        out = f\"[isize : {self.isize}]\"\n\t        return out\n\tclass LearnVGG(VGG):\n\t    def __init__(\n\t        self,\n\t        cfg,\n\t        num_classes: int = 1000,\n", "        dropout: float = 0.5,\n\t    ) -> None:\n\t        super().__init__(cfg, num_classes, dropout, LearnVGGBlock)\n\t        self.act_pos = set()\n\t        self.str_pos = {2, 4, 8, 12, 16}\n\t        # No skip-connections\n\t        self.skip_s2t = dict()\n\t        inp = torch.randn(64, 3, 224, 224)\n\t        self.out_shape = trace_feat_size(self.features, inp)\n\t    def get_act_info(self):\n", "        act_pos, act_num = get_act(self.features, VGGBlock)\n\t        return act_pos, act_num\n\t    def get_blk_info(self):\n\t        blk_end = sorted(get_blk(self.features, VGGBlock))\n\t        blks = dict([(ind + 1, end) for ind, end in enumerate(blk_end)])\n\t        blk_num = len(blks)\n\t        return blks, blk_num\n\t    def make_feature(self, inp, oup, n):\n\t        return self.block(inp, oup, n, self.cur_isize)\n\t    def fix_act(self, act_pos=None, merge_pos=None):\n", "        self.is_fixed_act = True\n\t        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n\t        self.merge_pos = merge_pos if merge_pos != None else self.act_pos\n\t        assert self.act_pos.issubset(self.merge_pos)\n\t        bumps = set.union(self.str_pos, self.merge_pos)\n\t        fix_act_lyrs(self.features, VGGBlock, self.act_pos, \"relu\")\n\t        adjust_padding(self.features, bumps)\n\t        adjust_isize(self.features)\n\t    def merge(self, act_pos=None, merge_pos=None, keep_feat=False):\n\t        self.to(\"cpu\")\n", "        self.is_merged = True\n\t        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n\t        self.merge_pos = merge_pos if merge_pos != None else self.act_pos\n\t        assert self.act_pos.issubset(self.merge_pos)\n\t        bumps = set.union(self.str_pos, self.merge_pos)\n\t        self.m_layers, stack = [], []\n\t        pos, bump_end, loc_end, last = 0, None, None, None\n\t        # weights = (isize, m_cw, m_p, m_b), cparam = (ctype, cstr)\n\t        weights, cparam = (112, None, None, None), (\"dw\", 1)\n\t        for block in self.features:\n", "            for ind in range(0, len(block.conv) - 1, 3):\n\t                relu = pos + 1 in self.act_pos\n\t                conv, bn = block.conv[ind], block.conv[ind + 1]\n\t                lyrs = (conv, bn)\n\t                is_merged = pos > 0 and not pos in bumps\n\t                _, weights, cparam = merge_or_new(\n\t                    stack, pos, weights, lyrs, is_merged, cparam, last\n\t                )\n\t                push_merged_layers(\n\t                    stack, pos, weights, cparam, relu=relu, act_type=\"relu\"\n", "                )\n\t                pos += 1\n\t                last = 2 if relu else 1\n\t            push_mp_layer(stack, f\"pool{pos-1}\")\n\t        if len(stack) > 0:\n\t            self.m_layers += [NaiveFeed(OrderedDict(stack))]\n\t        self.m_features = nn.Sequential(*self.m_layers)\n\t        if not keep_feat:\n\t            delattr(self, \"features\")\n\t        self.to(\"cuda\")\n", "def learn_vgg19_bn(num_classes):\n\t    return LearnVGG(vgg_cfgs[\"19\"], num_classes=num_classes)\n"]}
