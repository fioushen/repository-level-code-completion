{"filename": "train.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Train diffusion-based generative model using the techniques described in the\n\tpaper \"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\timport os\n\timport re\n", "import json\n\timport click\n\timport torch\n\timport dnnlib\n\tfrom torch_utils import distributed as dist\n\tfrom training import training_loop\n\timport warnings\n\twarnings.filterwarnings('ignore', 'Grad strides do not match bucket view strides') # False warning printed by PyTorch 1.12.\n\t#----------------------------------------------------------------------------\n\t# Parse a comma separated list of numbers or ranges and return a list of ints.\n", "# Example: '1,2,5-10' returns [1, 2, 5, 6, 7, 8, 9, 10]\n\tdef parse_int_list(s):\n\t    if isinstance(s, list): return s\n\t    ranges = []\n\t    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n\t    for p in s.split(','):\n\t        m = range_re.match(p)\n\t        if m:\n\t            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n\t        else:\n", "            ranges.append(int(p))\n\t    return ranges\n\t#----------------------------------------------------------------------------\n\t@click.command()\n\t#Patch options\n\t@click.option('--real_p',        help='Full size image ratio', metavar='INT',                       type=click.FloatRange(min=0, max=1), default=0.5, show_default=True)\n\t@click.option('--train_on_latents',      help='Training on latent embeddings', metavar='BOOL',      type=bool, default=False, show_default=True)\n\t@click.option('--progressive',      help='Training on latent embeddings', metavar='BOOL',           type=bool, default=False, show_default=True)\n\t# Main options.\n\t@click.option('--outdir',        help='Where to save the results', metavar='DIR',                   type=str, required=True)\n", "@click.option('--data',          help='Path to the dataset', metavar='ZIP|DIR',                     type=str, required=True)\n\t@click.option('--cond',          help='Train class-conditional model', metavar='BOOL',              type=bool, default=False, show_default=True)\n\t@click.option('--arch',          help='Network architecture', metavar='ddpmpp|ncsnpp|adm',          type=click.Choice(['ddpmpp', 'ncsnpp', 'adm']), default='ddpmpp', show_default=True)\n\t@click.option('--precond',       help='Preconditioning & loss function', metavar='vp|ve|edm',       type=click.Choice(['vp', 've', 'edm', 'pedm']), default='pedm', show_default=True)\n\t# Hyperparameters.\n\t@click.option('--duration',      help='Training duration', metavar='MIMG',                          type=click.FloatRange(min=0, min_open=True), default=200, show_default=True)\n\t@click.option('--batch',         help='Total batch size', metavar='INT',                            type=click.IntRange(min=1), default=512, show_default=True)\n\t@click.option('--batch-gpu',     help='Limit batch size per GPU', metavar='INT',                    type=click.IntRange(min=1))\n\t@click.option('--cbase',         help='Channel multiplier  [default: varies]', metavar='INT',       type=int)\n\t@click.option('--cres',          help='Channels per resolution  [default: varies]', metavar='LIST', type=parse_int_list)\n", "@click.option('--lr',            help='Learning rate', metavar='FLOAT',                             type=click.FloatRange(min=0, min_open=True), default=10e-4, show_default=True)\n\t@click.option('--ema',           help='EMA half-life', metavar='MIMG',                              type=click.FloatRange(min=0), default=0.5, show_default=True)\n\t@click.option('--dropout',       help='Dropout probability', metavar='FLOAT',                       type=click.FloatRange(min=0, max=1), default=0.13, show_default=True)\n\t@click.option('--augment',       help='Augment probability', metavar='FLOAT',                       type=click.FloatRange(min=0, max=1), default=0.12, show_default=True)\n\t@click.option('--xflip',         help='Enable dataset x-flips', metavar='BOOL',                     type=bool, default=False, show_default=True)\n\t@click.option('--implicit_mlp',  help='encoding coordbefore sending to the conv', metavar='BOOL',   type=bool, default=False, show_default=True)\n\t# Performance-related.\n\t@click.option('--fp16',          help='Enable mixed-precision training', metavar='BOOL',            type=bool, default=False, show_default=True)\n\t@click.option('--ls',            help='Loss scaling', metavar='FLOAT',                              type=click.FloatRange(min=0, min_open=True), default=1, show_default=True)\n\t@click.option('--bench',         help='Enable cuDNN benchmarking', metavar='BOOL',                  type=bool, default=True, show_default=True)\n", "@click.option('--cache',         help='Cache dataset in CPU memory', metavar='BOOL',                type=bool, default=True, show_default=True)\n\t@click.option('--workers',       help='DataLoader worker processes', metavar='INT',                 type=click.IntRange(min=1), default=1, show_default=True)\n\t# I/O-related.\n\t@click.option('--desc',          help='String to include in result dir name', metavar='STR',        type=str)\n\t@click.option('--nosubdir',      help='Do not create a subdirectory for results',                   is_flag=True)\n\t@click.option('--tick',          help='How often to print progress', metavar='KIMG',                type=click.IntRange(min=1), default=50, show_default=True)\n\t@click.option('--snap',          help='How often to save snapshots', metavar='TICKS',               type=click.IntRange(min=1), default=50, show_default=True)\n\t@click.option('--dump',          help='How often to dump state', metavar='TICKS',                   type=click.IntRange(min=1), default=500, show_default=True)\n\t@click.option('--seed',          help='Random seed  [default: random]', metavar='INT',              type=int)\n\t@click.option('--transfer',      help='Transfer learning from network pickle', metavar='PKL|URL',   type=str)\n", "@click.option('--resume',        help='Resume from previous training state', metavar='PT',          type=str)\n\t@click.option('-n', '--dry-run', help='Print training options and exit',                            is_flag=True)\n\tdef main(**kwargs):\n\t    \"\"\"Train diffusion-based generative model using the techniques described in the\n\t    paper \"Elucidating the Design Space of Diffusion-Based Generative Models\".\n\t    Examples:\n\t    \\b\n\t    # Train DDPM++ model for class-conditional CIFAR-10 using 8 GPUs\n\t    torchrun --standalone --nproc_per_node=8 train.py --outdir=training-runs \\\\\n\t        --data=datasets/cifar10-32x32.zip --cond=1 --arch=ddpmpp\n", "    \"\"\"\n\t    opts = dnnlib.EasyDict(kwargs)\n\t    torch.multiprocessing.set_start_method('spawn')\n\t    dist.init()\n\t    # Initialize config dict.\n\t    c = dnnlib.EasyDict()\n\t    c.dataset_kwargs = dnnlib.EasyDict(class_name='training.dataset.ImageFolderDataset', path=opts.data, use_labels=opts.cond, xflip=opts.xflip, cache=opts.cache)\n\t    c.data_loader_kwargs = dnnlib.EasyDict(pin_memory=True, num_workers=opts.workers, prefetch_factor=2)\n\t    c.network_kwargs = dnnlib.EasyDict()\n\t    c.loss_kwargs = dnnlib.EasyDict()\n", "    c.optimizer_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', lr=opts.lr, betas=[0.9,0.999], eps=1e-8)\n\t    c.real_p = opts.real_p\n\t    c.train_on_latents = opts.train_on_latents\n\t    c.progressive = opts.progressive\n\t    # Validate dataset options.\n\t    try:\n\t        dataset_obj = dnnlib.util.construct_class_by_name(**c.dataset_kwargs)\n\t        dataset_name = dataset_obj.name\n\t        c.dataset_kwargs.resolution = dataset_obj.resolution # be explicit about dataset resolution\n\t        c.dataset_kwargs.max_size = len(dataset_obj) # be explicit about dataset size\n", "        if opts.cond and not dataset_obj.has_labels:\n\t            raise click.ClickException('--cond=True requires labels specified in dataset.json')\n\t        del dataset_obj # conserve memory\n\t    except IOError as err:\n\t        raise click.ClickException(f'--data: {err}')\n\t    # Network architecture.\n\t    if opts.arch == 'ddpmpp':\n\t        c.network_kwargs.update(model_type='SongUNet', embedding_type='positional', encoder_type='standard', decoder_type='standard')\n\t        c.network_kwargs.update(channel_mult_noise=1, resample_filter=[1,1], model_channels=128, channel_mult=[2,2,2])\n\t    elif opts.arch == 'ncsnpp':\n", "        c.network_kwargs.update(model_type='SongUNet', embedding_type='fourier', encoder_type='residual', decoder_type='standard')\n\t        c.network_kwargs.update(channel_mult_noise=2, resample_filter=[1,3,3,1], model_channels=128, channel_mult=[2,2,2])\n\t    else:\n\t        assert opts.arch == 'adm'\n\t        c.network_kwargs.update(model_type='DhariwalUNet', model_channels=192, channel_mult=[1,2,3,4])\n\t    # Preconditioning & loss function.\n\t    if opts.precond == 'vp':\n\t        c.network_kwargs.class_name = 'training.networks.VPPrecond'\n\t        c.loss_kwargs.class_name = 'training.loss.VPLoss'\n\t    elif opts.precond == 've':\n", "        c.network_kwargs.class_name = 'training.networks.VEPrecond'\n\t        c.loss_kwargs.class_name = 'training.loss.VELoss'\n\t    elif opts.precond == 'pedm':\n\t        c.network_kwargs.class_name = 'training.networks.Patch_EDMPrecond'\n\t        c.loss_kwargs.class_name = 'training.patch_loss.Patch_EDMLoss'\n\t    else:\n\t        assert opts.precond == 'edm'\n\t        c.network_kwargs.class_name = 'training.networks.EDMPrecond'\n\t        c.loss_kwargs.class_name = 'training.loss.EDMLoss'\n\t    # Network options.\n", "    if opts.cbase is not None:\n\t        c.network_kwargs.model_channels = opts.cbase\n\t    if opts.cres is not None:\n\t        c.network_kwargs.channel_mult = opts.cres\n\t    if opts.augment:\n\t        c.augment_kwargs = dnnlib.EasyDict(class_name='training.augment.AugmentPipe', p=opts.augment)\n\t        c.augment_kwargs.update(xflip=1e8, yflip=1, scale=1, rotate_frac=1, aniso=1, translate_frac=1)\n\t        c.network_kwargs.augment_dim = 9\n\t        # c.augment_kwargs.update(brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1)\n\t        # c.network_kwargs.augment_dim = 6\n", "    if opts.implicit_mlp:\n\t        c.network_kwargs.implicit_mlp = True\n\t    c.network_kwargs.update(dropout=opts.dropout, use_fp16=opts.fp16)\n\t    # Training options.\n\t    c.total_kimg = max(int(opts.duration * 1000), 1)\n\t    c.ema_halflife_kimg = int(opts.ema * 1000)\n\t    c.update(batch_size=opts.batch, batch_gpu=opts.batch_gpu)\n\t    c.update(loss_scaling=opts.ls, cudnn_benchmark=opts.bench)\n\t    c.update(kimg_per_tick=opts.tick, snapshot_ticks=opts.snap, state_dump_ticks=opts.dump)\n\t    # Random seed.\n", "    if opts.seed is not None:\n\t        c.seed = opts.seed\n\t    else:\n\t        seed = torch.randint(1 << 31, size=[], device=torch.device('cuda'))\n\t        torch.distributed.broadcast(seed, src=0)\n\t        c.seed = int(seed)\n\t    # Transfer learning and resume.\n\t    if opts.transfer is not None:\n\t        if opts.resume is not None:\n\t            raise click.ClickException('--transfer and --resume cannot be specified at the same time')\n", "        c.resume_pkl = opts.transfer\n\t        c.ema_rampup_ratio = None\n\t    elif opts.resume is not None:\n\t        match = re.fullmatch(r'training-state-(\\d+).pt', os.path.basename(opts.resume))\n\t        if not match or not os.path.isfile(opts.resume):\n\t            raise click.ClickException('--resume must point to training-state-*.pt from a previous training run')\n\t        c.resume_pkl = os.path.join(os.path.dirname(opts.resume), f'network-snapshot-{match.group(1)}.pkl')\n\t        c.resume_kimg = int(match.group(1))\n\t        c.resume_state_dump = opts.resume\n\t    # Description string.\n", "    cond_str = 'cond' if c.dataset_kwargs.use_labels else 'uncond'\n\t    dtype_str = 'fp16' if c.network_kwargs.use_fp16 else 'fp32'\n\t    desc = f'{dataset_name:s}-{cond_str:s}-{opts.arch:s}-{opts.precond:s}-gpus{dist.get_world_size():d}-batch{c.batch_size:d}-{dtype_str:s}'\n\t    if opts.desc is not None:\n\t        desc += f'-{opts.desc}'\n\t    # Pick output directory.\n\t    if dist.get_rank() != 0:\n\t        c.run_dir = None\n\t    elif opts.nosubdir:\n\t        c.run_dir = opts.outdir\n", "    else:\n\t        prev_run_dirs = []\n\t        if os.path.isdir(opts.outdir):\n\t            prev_run_dirs = [x for x in os.listdir(opts.outdir) if os.path.isdir(os.path.join(opts.outdir, x))]\n\t        prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n\t        prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n\t        cur_run_id = max(prev_run_ids, default=-1) + 1\n\t        c.run_dir = os.path.join(opts.outdir, f'{cur_run_id:05d}-{desc}')\n\t        assert not os.path.exists(c.run_dir)\n\t    # Print options.\n", "    dist.print0()\n\t    dist.print0('Training options:')\n\t    dist.print0(json.dumps(c, indent=2))\n\t    dist.print0()\n\t    dist.print0(f'Output directory:        {c.run_dir}')\n\t    dist.print0(f'Dataset path:            {c.dataset_kwargs.path}')\n\t    dist.print0(f'Class-conditional:       {c.dataset_kwargs.use_labels}')\n\t    dist.print0(f'Network architecture:    {opts.arch}')\n\t    dist.print0(f'Preconditioning & loss:  {opts.precond}')\n\t    dist.print0(f'Number of GPUs:          {dist.get_world_size()}')\n", "    dist.print0(f'Batch size:              {c.batch_size}')\n\t    dist.print0(f'Mixed-precision:         {c.network_kwargs.use_fp16}')\n\t    dist.print0()\n\t    # Dry run?\n\t    if opts.dry_run:\n\t        dist.print0('Dry run; exiting.')\n\t        return\n\t    # Create output directory.\n\t    dist.print0('Creating output directory...')\n\t    if dist.get_rank() == 0:\n", "        os.makedirs(c.run_dir, exist_ok=True)\n\t        with open(os.path.join(c.run_dir, 'training_options.json'), 'wt') as f:\n\t            json.dump(c, f, indent=2)\n\t        dnnlib.util.Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n\t    # Train.\n\t    training_loop.training_loop(**c)\n\t#----------------------------------------------------------------------------\n\tif __name__ == \"__main__\":\n\t    main()\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "fid.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Script for calculating Frechet Inception Distance (FID).\"\"\"\n\timport os\n\timport click\n\timport tqdm\n", "import pickle\n\timport numpy as np\n\timport scipy.linalg\n\timport torch\n\timport dnnlib\n\tfrom torch_utils import distributed as dist\n\tfrom training import dataset\n\t#----------------------------------------------------------------------------\n\tdef calculate_inception_stats(\n\t    image_path, num_expected=None, seed=0, max_batch_size=64,\n", "    num_workers=3, prefetch_factor=2, device=torch.device('cuda'),\n\t):\n\t    # Rank 0 goes first.\n\t    if dist.get_rank() != 0:\n\t        torch.distributed.barrier()\n\t    # Load Inception-v3 model.\n\t    # This is a direct PyTorch translation of http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n\t    dist.print0('Loading Inception-v3 model...')\n\t    detector_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl'\n\t    detector_kwargs = dict(return_features=True)\n", "    feature_dim = 2048\n\t    with dnnlib.util.open_url(detector_url, verbose=(dist.get_rank() == 0)) as f:\n\t        detector_net = pickle.load(f).to(device)\n\t    # List images.\n\t    dist.print0(f'Loading images from \"{image_path}\"...')\n\t    dataset_obj = dataset.ImageFolderDataset(path=image_path, max_size=num_expected, random_seed=seed)\n\t    if num_expected is not None and len(dataset_obj) < num_expected:\n\t        raise click.ClickException(f'Found {len(dataset_obj)} images, but expected at least {num_expected}')\n\t    if len(dataset_obj) < 2:\n\t        raise click.ClickException(f'Found {len(dataset_obj)} images, but need at least 2 to compute statistics')\n", "    # Other ranks follow.\n\t    if dist.get_rank() == 0:\n\t        torch.distributed.barrier()\n\t    # Divide images into batches.\n\t    num_batches = ((len(dataset_obj) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n\t    all_batches = torch.arange(len(dataset_obj)).tensor_split(num_batches)\n\t    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n\t    data_loader = torch.utils.data.DataLoader(dataset_obj, batch_sampler=rank_batches, num_workers=num_workers, prefetch_factor=prefetch_factor)\n\t    # Accumulate statistics.\n\t    dist.print0(f'Calculating statistics for {len(dataset_obj)} images...')\n", "    mu = torch.zeros([feature_dim], dtype=torch.float64, device=device)\n\t    sigma = torch.zeros([feature_dim, feature_dim], dtype=torch.float64, device=device)\n\t    for images, _labels in tqdm.tqdm(data_loader, unit='batch', disable=(dist.get_rank() != 0)):\n\t        torch.distributed.barrier()\n\t        if images.shape[0] == 0:\n\t            continue\n\t        if images.shape[1] == 1:\n\t            images = images.repeat([1, 3, 1, 1])\n\t        features = detector_net(images.to(device), **detector_kwargs).to(torch.float64)\n\t        mu += features.sum(0)\n", "        sigma += features.T @ features\n\t    # Calculate grand totals.\n\t    torch.distributed.all_reduce(mu)\n\t    torch.distributed.all_reduce(sigma)\n\t    mu /= len(dataset_obj)\n\t    sigma -= mu.ger(mu) * len(dataset_obj)\n\t    sigma /= len(dataset_obj) - 1\n\t    return mu.cpu().numpy(), sigma.cpu().numpy()\n\t#----------------------------------------------------------------------------\n\tdef calculate_fid_from_inception_stats(mu, sigma, mu_ref, sigma_ref):\n", "    m = np.square(mu - mu_ref).sum()\n\t    s, _ = scipy.linalg.sqrtm(np.dot(sigma, sigma_ref), disp=False)\n\t    fid = m + np.trace(sigma + sigma_ref - s * 2)\n\t    return float(np.real(fid))\n\t#----------------------------------------------------------------------------\n\t@click.group()\n\tdef main():\n\t    \"\"\"Calculate Frechet Inception Distance (FID).\n\t    Examples:\n\t    \\b\n", "    # Generate 50000 images and save them as fid-tmp/*/*.png\n\t    torchrun --standalone --nproc_per_node=1 generate.py --outdir=fid-tmp --seeds=0-49999 --subdirs \\\\\n\t        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n\t    \\b\n\t    # Calculate FID\n\t    torchrun --standalone --nproc_per_node=1 fid.py calc --images=fid-tmp \\\\\n\t        --ref=https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\n\t    \\b\n\t    # Compute dataset reference statistics\n\t    python fid.py ref --data=datasets/my-dataset.zip --dest=fid-refs/my-dataset.npz\n", "    \"\"\"\n\t#----------------------------------------------------------------------------\n\t@main.command()\n\t@click.option('--images', 'image_path', help='Path to the images', metavar='PATH|ZIP',              type=str, required=True)\n\t@click.option('--ref', 'ref_path',      help='Dataset reference statistics ', metavar='NPZ|URL',    type=str, required=True)\n\t@click.option('--num', 'num_expected',  help='Number of images to use', metavar='INT',              type=click.IntRange(min=2), default=50000, show_default=True)\n\t@click.option('--seed',                 help='Random seed for selecting the images', metavar='INT', type=int, default=0, show_default=True)\n\t@click.option('--batch',                help='Maximum batch size', metavar='INT',                   type=click.IntRange(min=1), default=64, show_default=True)\n\tdef calc(image_path, ref_path, num_expected, seed, batch):\n\t    \"\"\"Calculate FID for a given set of images.\"\"\"\n", "    torch.multiprocessing.set_start_method('spawn')\n\t    dist.init()\n\t    dist.print0(f'Loading dataset reference statistics from \"{ref_path}\"...')\n\t    ref = None\n\t    if dist.get_rank() == 0:\n\t        with dnnlib.util.open_url(ref_path) as f:\n\t            ref = dict(np.load(f))\n\t    mu, sigma = calculate_inception_stats(image_path=image_path, num_expected=num_expected, seed=seed, max_batch_size=batch)\n\t    dist.print0('Calculating FID...')\n\t    if dist.get_rank() == 0:\n", "        fid = calculate_fid_from_inception_stats(mu, sigma, ref['mu'], ref['sigma'])\n\t        print(f'{fid:g}')\n\t    torch.distributed.barrier()\n\t#----------------------------------------------------------------------------\n\t@main.command()\n\t@click.option('--data', 'dataset_path', help='Path to the dataset', metavar='PATH|ZIP', type=str, required=True)\n\t@click.option('--dest', 'dest_path',    help='Destination .npz file', metavar='NPZ',    type=str, required=True)\n\t@click.option('--batch',                help='Maximum batch size', metavar='INT',       type=click.IntRange(min=1), default=64, show_default=True)\n\tdef ref(dataset_path, dest_path, batch):\n\t    \"\"\"Calculate dataset reference statistics needed by 'calc'.\"\"\"\n", "    torch.multiprocessing.set_start_method('spawn')\n\t    dist.init()\n\t    mu, sigma = calculate_inception_stats(image_path=dataset_path, max_batch_size=batch)\n\t    dist.print0(f'Saving dataset reference statistics to \"{dest_path}\"...')\n\t    if dist.get_rank() == 0:\n\t        if os.path.dirname(dest_path):\n\t            os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n\t        np.savez(dest_path, mu=mu, sigma=sigma)\n\t    torch.distributed.barrier()\n\t    dist.print0('Done.')\n", "#----------------------------------------------------------------------------\n\tif __name__ == \"__main__\":\n\t    main()\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "example.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Minimal standalone example to reproduce the main results from the paper\n\t\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\timport tqdm\n\timport pickle\n", "import numpy as np\n\timport torch\n\timport PIL.Image\n\timport dnnlib\n\t#----------------------------------------------------------------------------\n\tdef generate_image_grid(\n\t    network_pkl, dest_path,\n\t    seed=0, gridw=8, gridh=8, device=torch.device('cuda'),\n\t    num_steps=18, sigma_min=0.002, sigma_max=80, rho=7,\n\t    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n", "):\n\t    batch_size = gridw * gridh\n\t    torch.manual_seed(seed)\n\t    # Load network.\n\t    print(f'Loading network from \"{network_pkl}\"...')\n\t    with dnnlib.util.open_url(network_pkl) as f:\n\t        net = pickle.load(f)['ema'].to(device)\n\t    # Pick latents and labels.\n\t    print(f'Generating {batch_size} images...')\n\t    latents = torch.randn([batch_size, net.img_channels, net.img_resolution, net.img_resolution], device=device)\n", "    class_labels = None\n\t    if net.label_dim:\n\t        class_labels = torch.eye(net.label_dim, device=device)[torch.randint(net.label_dim, size=[batch_size], device=device)]\n\t    # Adjust noise levels based on what's supported by the network.\n\t    sigma_min = max(sigma_min, net.sigma_min)\n\t    sigma_max = min(sigma_max, net.sigma_max)\n\t    # Time step discretization.\n\t    step_indices = torch.arange(num_steps, dtype=torch.float64, device=device)\n\t    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n\t    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n", "    # Main sampling loop.\n\t    x_next = latents.to(torch.float64) * t_steps[0]\n\t    for i, (t_cur, t_next) in tqdm.tqdm(list(enumerate(zip(t_steps[:-1], t_steps[1:]))), unit='step'): # 0, ..., N-1\n\t        x_cur = x_next\n\t        # Increase noise temporarily.\n\t        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n\t        t_hat = net.round_sigma(t_cur + gamma * t_cur)\n\t        x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * torch.randn_like(x_cur)\n\t        # Euler step.\n\t        denoised = net(x_hat, t_hat, class_labels).to(torch.float64)\n", "        d_cur = (x_hat - denoised) / t_hat\n\t        x_next = x_hat + (t_next - t_hat) * d_cur\n\t        # Apply 2nd order correction.\n\t        if i < num_steps - 1:\n\t            denoised = net(x_next, t_next, class_labels).to(torch.float64)\n\t            d_prime = (x_next - denoised) / t_next\n\t            x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n\t    # Save image grid.\n\t    print(f'Saving image grid to \"{dest_path}\"...')\n\t    image = (x_next * 127.5 + 128).clip(0, 255).to(torch.uint8)\n", "    image = image.reshape(gridh, gridw, *image.shape[1:]).permute(0, 3, 1, 4, 2)\n\t    image = image.reshape(gridh * net.img_resolution, gridw * net.img_resolution, net.img_channels)\n\t    image = image.cpu().numpy()\n\t    PIL.Image.fromarray(image, 'RGB').save(dest_path)\n\t    print('Done.')\n\t#----------------------------------------------------------------------------\n\tdef main():\n\t    model_root = 'https://nvlabs-fi-cdn.nvidia.com/edm/pretrained'\n\t    generate_image_grid(f'{model_root}/edm-cifar10-32x32-cond-vp.pkl',   'cifar10-32x32.png',  num_steps=18) # FID = 1.79, NFE = 35\n\t    generate_image_grid(f'{model_root}/edm-ffhq-64x64-uncond-vp.pkl',    'ffhq-64x64.png',     num_steps=40) # FID = 1.97, NFE = 79\n", "    generate_image_grid(f'{model_root}/edm-afhqv2-64x64-uncond-vp.pkl',  'afhqv2-64x64.png',   num_steps=40) # FID = 1.96, NFE = 79\n\t    generate_image_grid(f'{model_root}/edm-imagenet-64x64-cond-adm.pkl', 'imagenet-64x64.png', num_steps=256, S_churn=40, S_min=0.05, S_max=50, S_noise=1.003) # FID = 1.36, NFE = 511\n\t#----------------------------------------------------------------------------\n\tif __name__ == \"__main__\":\n\t    main()\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "dataset_tool.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Tool for creating ZIP/PNG based datasets.\"\"\"\n\timport functools\n\timport gzip\n\timport io\n", "import json\n\timport os\n\timport pickle\n\timport re\n\timport sys\n\timport tarfile\n\timport zipfile\n\tfrom pathlib import Path\n\tfrom typing import Callable, Optional, Tuple, Union\n\timport click\n", "import numpy as np\n\timport PIL.Image\n\tfrom tqdm import tqdm\n\t#----------------------------------------------------------------------------\n\t# Parse a 'M,N' or 'MxN' integer tuple.\n\t# Example: '4x2' returns (4,2)\n\tdef parse_tuple(s: str) -> Tuple[int, int]:\n\t    m = re.match(r'^(\\d+)[x,](\\d+)$', s)\n\t    if m:\n\t        return int(m.group(1)), int(m.group(2))\n", "    raise click.ClickException(f'cannot parse tuple {s}')\n\t#----------------------------------------------------------------------------\n\tdef maybe_min(a: int, b: Optional[int]) -> int:\n\t    if b is not None:\n\t        return min(a, b)\n\t    return a\n\t#----------------------------------------------------------------------------\n\tdef file_ext(name: Union[str, Path]) -> str:\n\t    return str(name).split('.')[-1]\n\t#----------------------------------------------------------------------------\n", "def is_image_ext(fname: Union[str, Path]) -> bool:\n\t    ext = file_ext(fname).lower()\n\t    return f'.{ext}' in PIL.Image.EXTENSION\n\t#----------------------------------------------------------------------------\n\tdef open_image_folder(source_dir, *, max_images: Optional[int]):\n\t    input_images = [str(f) for f in sorted(Path(source_dir).rglob('*')) if is_image_ext(f) and os.path.isfile(f)]\n\t    arch_fnames = {fname: os.path.relpath(fname, source_dir).replace('\\\\', '/') for fname in input_images}\n\t    max_idx = maybe_min(len(input_images), max_images)\n\t    # Load labels.\n\t    labels = dict()\n", "    meta_fname = os.path.join(source_dir, 'dataset.json')\n\t    if os.path.isfile(meta_fname):\n\t        with open(meta_fname, 'r') as file:\n\t            data = json.load(file)['labels']\n\t            if data is not None:\n\t                labels = {x[0]: x[1] for x in data}\n\t    # No labels available => determine from top-level directory names.\n\t    if len(labels) == 0:\n\t        toplevel_names = {arch_fname: arch_fname.split('/')[0] if '/' in arch_fname else '' for arch_fname in arch_fnames.values()}\n\t        toplevel_indices = {toplevel_name: idx for idx, toplevel_name in enumerate(sorted(set(toplevel_names.values())))}\n", "        if len(toplevel_indices) > 1:\n\t            labels = {arch_fname: toplevel_indices[toplevel_name] for arch_fname, toplevel_name in toplevel_names.items()}\n\t    def iterate_images():\n\t        for idx, fname in enumerate(input_images):\n\t            img = np.array(PIL.Image.open(fname))\n\t            yield dict(img=img, label=labels.get(arch_fnames.get(fname)))\n\t            if idx >= max_idx - 1:\n\t                break\n\t    return max_idx, iterate_images()\n\t#----------------------------------------------------------------------------\n", "def open_image_zip(source, *, max_images: Optional[int]):\n\t    with zipfile.ZipFile(source, mode='r') as z:\n\t        input_images = [str(f) for f in sorted(z.namelist()) if is_image_ext(f)]\n\t        max_idx = maybe_min(len(input_images), max_images)\n\t        # Load labels.\n\t        labels = dict()\n\t        if 'dataset.json' in z.namelist():\n\t            with z.open('dataset.json', 'r') as file:\n\t                data = json.load(file)['labels']\n\t                if data is not None:\n", "                    labels = {x[0]: x[1] for x in data}\n\t    def iterate_images():\n\t        with zipfile.ZipFile(source, mode='r') as z:\n\t            for idx, fname in enumerate(input_images):\n\t                with z.open(fname, 'r') as file:\n\t                    img = np.array(PIL.Image.open(file))\n\t                yield dict(img=img, label=labels.get(fname))\n\t                if idx >= max_idx - 1:\n\t                    break\n\t    return max_idx, iterate_images()\n", "#----------------------------------------------------------------------------\n\tdef open_lmdb(lmdb_dir: str, *, max_images: Optional[int]):\n\t    import cv2  # pyright: ignore [reportMissingImports] # pip install opencv-python\n\t    import lmdb  # pyright: ignore [reportMissingImports] # pip install lmdb\n\t    with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:\n\t        max_idx = maybe_min(txn.stat()['entries'], max_images)\n\t    def iterate_images():\n\t        with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:\n\t            for idx, (_key, value) in enumerate(txn.cursor()):\n\t                try:\n", "                    try:\n\t                        img = cv2.imdecode(np.frombuffer(value, dtype=np.uint8), 1)\n\t                        if img is None:\n\t                            raise IOError('cv2.imdecode failed')\n\t                        img = img[:, :, ::-1] # BGR => RGB\n\t                    except IOError:\n\t                        img = np.array(PIL.Image.open(io.BytesIO(value)))\n\t                    yield dict(img=img, label=None)\n\t                    if idx >= max_idx - 1:\n\t                        break\n", "                except:\n\t                    print(sys.exc_info()[1])\n\t    return max_idx, iterate_images()\n\t#----------------------------------------------------------------------------\n\tdef open_cifar10(tarball: str, *, max_images: Optional[int]):\n\t    images = []\n\t    labels = []\n\t    with tarfile.open(tarball, 'r:gz') as tar:\n\t        for batch in range(1, 6):\n\t            member = tar.getmember(f'cifar-10-batches-py/data_batch_{batch}')\n", "            with tar.extractfile(member) as file:\n\t                data = pickle.load(file, encoding='latin1')\n\t            images.append(data['data'].reshape(-1, 3, 32, 32))\n\t            labels.append(data['labels'])\n\t    images = np.concatenate(images)\n\t    labels = np.concatenate(labels)\n\t    images = images.transpose([0, 2, 3, 1]) # NCHW -> NHWC\n\t    assert images.shape == (50000, 32, 32, 3) and images.dtype == np.uint8\n\t    assert labels.shape == (50000,) and labels.dtype in [np.int32, np.int64]\n\t    assert np.min(images) == 0 and np.max(images) == 255\n", "    assert np.min(labels) == 0 and np.max(labels) == 9\n\t    max_idx = maybe_min(len(images), max_images)\n\t    def iterate_images():\n\t        for idx, img in enumerate(images):\n\t            yield dict(img=img, label=int(labels[idx]))\n\t            if idx >= max_idx - 1:\n\t                break\n\t    return max_idx, iterate_images()\n\t#----------------------------------------------------------------------------\n\tdef open_mnist(images_gz: str, *, max_images: Optional[int]):\n", "    labels_gz = images_gz.replace('-images-idx3-ubyte.gz', '-labels-idx1-ubyte.gz')\n\t    assert labels_gz != images_gz\n\t    images = []\n\t    labels = []\n\t    with gzip.open(images_gz, 'rb') as f:\n\t        images = np.frombuffer(f.read(), np.uint8, offset=16)\n\t    with gzip.open(labels_gz, 'rb') as f:\n\t        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n\t    images = images.reshape(-1, 28, 28)\n\t    images = np.pad(images, [(0,0), (2,2), (2,2)], 'constant', constant_values=0)\n", "    assert images.shape == (60000, 32, 32) and images.dtype == np.uint8\n\t    assert labels.shape == (60000,) and labels.dtype == np.uint8\n\t    assert np.min(images) == 0 and np.max(images) == 255\n\t    assert np.min(labels) == 0 and np.max(labels) == 9\n\t    max_idx = maybe_min(len(images), max_images)\n\t    def iterate_images():\n\t        for idx, img in enumerate(images):\n\t            yield dict(img=img, label=int(labels[idx]))\n\t            if idx >= max_idx - 1:\n\t                break\n", "    return max_idx, iterate_images()\n\t#----------------------------------------------------------------------------\n\tdef make_transform(\n\t    transform: Optional[str],\n\t    output_width: Optional[int],\n\t    output_height: Optional[int]\n\t) -> Callable[[np.ndarray], Optional[np.ndarray]]:\n\t    def scale(width, height, img):\n\t        w = img.shape[1]\n\t        h = img.shape[0]\n", "        if width == w and height == h:\n\t            return img\n\t        img = PIL.Image.fromarray(img)\n\t        ww = width if width is not None else w\n\t        hh = height if height is not None else h\n\t        img = img.resize((ww, hh), PIL.Image.Resampling.LANCZOS)\n\t        return np.array(img)\n\t    def center_crop(width, height, img):\n\t        crop = np.min(img.shape[:2])\n\t        img = img[(img.shape[0] - crop) // 2 : (img.shape[0] + crop) // 2, (img.shape[1] - crop) // 2 : (img.shape[1] + crop) // 2]\n", "        if img.ndim == 2:\n\t            img = img[:, :, np.newaxis].repeat(3, axis=2)\n\t        img = PIL.Image.fromarray(img, 'RGB')\n\t        img = img.resize((width, height), PIL.Image.Resampling.LANCZOS)\n\t        return np.array(img)\n\t    def center_crop_wide(width, height, img):\n\t        ch = int(np.round(width * img.shape[0] / img.shape[1]))\n\t        if img.shape[1] < width or ch < height:\n\t            return None\n\t        img = img[(img.shape[0] - ch) // 2 : (img.shape[0] + ch) // 2]\n", "        if img.ndim == 2:\n\t            img = img[:, :, np.newaxis].repeat(3, axis=2)\n\t        img = PIL.Image.fromarray(img, 'RGB')\n\t        img = img.resize((width, height), PIL.Image.Resampling.LANCZOS)\n\t        img = np.array(img)\n\t        canvas = np.zeros([width, width, 3], dtype=np.uint8)\n\t        canvas[(width - height) // 2 : (width + height) // 2, :] = img\n\t        return canvas\n\t    if transform is None:\n\t        return functools.partial(scale, output_width, output_height)\n", "    if transform == 'center-crop':\n\t        if output_width is None or output_height is None:\n\t            raise click.ClickException('must specify --resolution=WxH when using ' + transform + 'transform')\n\t        return functools.partial(center_crop, output_width, output_height)\n\t    if transform == 'center-crop-wide':\n\t        if output_width is None or output_height is None:\n\t            raise click.ClickException('must specify --resolution=WxH when using ' + transform + ' transform')\n\t        return functools.partial(center_crop_wide, output_width, output_height)\n\t    assert False, 'unknown transform'\n\t#----------------------------------------------------------------------------\n", "def open_dataset(source, *, max_images: Optional[int]):\n\t    if os.path.isdir(source):\n\t        if source.rstrip('/').endswith('_lmdb'):\n\t            return open_lmdb(source, max_images=max_images)\n\t        else:\n\t            return open_image_folder(source, max_images=max_images)\n\t    elif os.path.isfile(source):\n\t        if os.path.basename(source) == 'cifar-10-python.tar.gz':\n\t            return open_cifar10(source, max_images=max_images)\n\t        elif os.path.basename(source) == 'train-images-idx3-ubyte.gz':\n", "            return open_mnist(source, max_images=max_images)\n\t        elif file_ext(source) == 'zip':\n\t            return open_image_zip(source, max_images=max_images)\n\t        else:\n\t            assert False, 'unknown archive type'\n\t    else:\n\t        raise click.ClickException(f'Missing input file or directory: {source}')\n\t#----------------------------------------------------------------------------\n\tdef open_dest(dest: str) -> Tuple[str, Callable[[str, Union[bytes, str]], None], Callable[[], None]]:\n\t    dest_ext = file_ext(dest)\n", "    if dest_ext == 'zip':\n\t        if os.path.dirname(dest) != '':\n\t            os.makedirs(os.path.dirname(dest), exist_ok=True)\n\t        zf = zipfile.ZipFile(file=dest, mode='w', compression=zipfile.ZIP_STORED)\n\t        def zip_write_bytes(fname: str, data: Union[bytes, str]):\n\t            zf.writestr(fname, data)\n\t        return '', zip_write_bytes, zf.close\n\t    else:\n\t        # If the output folder already exists, check that is is\n\t        # empty.\n", "        #\n\t        # Note: creating the output directory is not strictly\n\t        # necessary as folder_write_bytes() also mkdirs, but it's better\n\t        # to give an error message earlier in case the dest folder\n\t        # somehow cannot be created.\n\t        if os.path.isdir(dest) and len(os.listdir(dest)) != 0:\n\t            raise click.ClickException('--dest folder must be empty')\n\t        os.makedirs(dest, exist_ok=True)\n\t        def folder_write_bytes(fname: str, data: Union[bytes, str]):\n\t            os.makedirs(os.path.dirname(fname), exist_ok=True)\n", "            with open(fname, 'wb') as fout:\n\t                if isinstance(data, str):\n\t                    data = data.encode('utf8')\n\t                fout.write(data)\n\t        return dest, folder_write_bytes, lambda: None\n\t#----------------------------------------------------------------------------\n\t@click.command()\n\t@click.option('--source',     help='Input directory or archive name', metavar='PATH',   type=str, required=True)\n\t@click.option('--dest',       help='Output directory or archive name', metavar='PATH',  type=str, required=True)\n\t@click.option('--max-images', help='Maximum number of images to output', metavar='INT', type=int)\n", "@click.option('--transform',  help='Input crop/resize mode', metavar='MODE',            type=click.Choice(['center-crop', 'center-crop-wide']))\n\t@click.option('--resolution', help='Output resolution (e.g., 512x512)', metavar='WxH',  type=parse_tuple)\n\tdef main(\n\t    source: str,\n\t    dest: str,\n\t    max_images: Optional[int],\n\t    transform: Optional[str],\n\t    resolution: Optional[Tuple[int, int]]\n\t):\n\t    \"\"\"Convert an image dataset into a dataset archive usable with StyleGAN2 ADA PyTorch.\n", "    The input dataset format is guessed from the --source argument:\n\t    \\b\n\t    --source *_lmdb/                    Load LSUN dataset\n\t    --source cifar-10-python.tar.gz     Load CIFAR-10 dataset\n\t    --source train-images-idx3-ubyte.gz Load MNIST dataset\n\t    --source path/                      Recursively load all images from path/\n\t    --source dataset.zip                Recursively load all images from dataset.zip\n\t    Specifying the output format and path:\n\t    \\b\n\t    --dest /path/to/dir                 Save output files under /path/to/dir\n", "    --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip\n\t    The output dataset format can be either an image folder or an uncompressed zip archive.\n\t    Zip archives makes it easier to move datasets around file servers and clusters, and may\n\t    offer better training performance on network file systems.\n\t    Images within the dataset archive will be stored as uncompressed PNG.\n\t    Uncompresed PNGs can be efficiently decoded in the training loop.\n\t    Class labels are stored in a file called 'dataset.json' that is stored at the\n\t    dataset root folder.  This file has the following structure:\n\t    \\b\n\t    {\n", "        \"labels\": [\n\t            [\"00000/img00000000.png\",6],\n\t            [\"00000/img00000001.png\",9],\n\t            ... repeated for every image in the datase\n\t            [\"00049/img00049999.png\",1]\n\t        ]\n\t    }\n\t    If the 'dataset.json' file cannot be found, class labels are determined from\n\t    top-level directory names.\n\t    Image scale/crop and resolution requirements:\n", "    Output images must be square-shaped and they must all have the same power-of-two\n\t    dimensions.\n\t    To scale arbitrary input image size to a specific width and height, use the\n\t    --resolution option.  Output resolution will be either the original\n\t    input resolution (if resolution was not specified) or the one specified with\n\t    --resolution option.\n\t    Use the --transform=center-crop or --transform=center-crop-wide options to apply a\n\t    center crop transform on the input image.  These options should be used with the\n\t    --resolution option.  For example:\n\t    \\b\n", "    python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\\\\n\t        --transform=center-crop-wide --resolution=512x384\n\t    \"\"\"\n\t    PIL.Image.init()\n\t    if dest == '':\n\t        raise click.ClickException('--dest output filename or directory must not be an empty string')\n\t    num_files, input_iter = open_dataset(source, max_images=max_images)\n\t    archive_root_dir, save_bytes, close_dest = open_dest(dest)\n\t    if resolution is None: resolution = (None, None)\n\t    transform_image = make_transform(transform, *resolution)\n", "    dataset_attrs = None\n\t    labels = []\n\t    for idx, image in tqdm(enumerate(input_iter), total=num_files):\n\t        idx_str = f'{idx:08d}'\n\t        archive_fname = f'{idx_str[:5]}/img{idx_str}.png'\n\t        # Apply crop and resize.\n\t        img = transform_image(image['img'])\n\t        if img is None:\n\t            continue\n\t        # Error check to require uniform image attributes across\n", "        # the whole dataset.\n\t        channels = img.shape[2] if img.ndim == 3 else 1\n\t        cur_image_attrs = {'width': img.shape[1], 'height': img.shape[0], 'channels': channels}\n\t        if dataset_attrs is None:\n\t            dataset_attrs = cur_image_attrs\n\t            width = dataset_attrs['width']\n\t            height = dataset_attrs['height']\n\t            if width != height:\n\t                raise click.ClickException(f'Image dimensions after scale and crop are required to be square.  Got {width}x{height}')\n\t            if dataset_attrs['channels'] not in [1, 3]:\n", "                raise click.ClickException('Input images must be stored as RGB or grayscale')\n\t            if width != 2 ** int(np.floor(np.log2(width))):\n\t                raise click.ClickException('Image width/height after scale and crop are required to be power-of-two')\n\t        elif dataset_attrs != cur_image_attrs:\n\t            err = [f'  dataset {k}/cur image {k}: {dataset_attrs[k]}/{cur_image_attrs[k]}' for k in dataset_attrs.keys()]\n\t            raise click.ClickException(f'Image {archive_fname} attributes must be equal across all images of the dataset.  Got:\\n' + '\\n'.join(err))\n\t        # Save the image as an uncompressed PNG.\n\t        img = PIL.Image.fromarray(img, {1: 'L', 3: 'RGB'}[channels])\n\t        image_bits = io.BytesIO()\n\t        img.save(image_bits, format='png', compress_level=0, optimize=False)\n", "        save_bytes(os.path.join(archive_root_dir, archive_fname), image_bits.getbuffer())\n\t        labels.append([archive_fname, image['label']] if image['label'] is not None else None)\n\t    metadata = {'labels': labels if all(x is not None for x in labels) else None}\n\t    save_bytes(os.path.join(archive_root_dir, 'dataset.json'), json.dumps(metadata))\n\t    close_dest()\n\t#----------------------------------------------------------------------------\n\tif __name__ == \"__main__\":\n\t    main()\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "generate.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Generate random images using the techniques described in the paper\n\t\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\timport os\n\timport re\n", "import click\n\timport tqdm\n\timport pickle\n\timport numpy as np\n\timport torch\n\timport PIL.Image\n\timport dnnlib\n\tfrom torch_utils import distributed as dist\n\tfrom training.pos_embedding import Pos_Embedding\n\tfrom diffusers import AutoencoderKL\n", "def random_patch(images, patch_size, resolution):\n\t    device = images.device\n\t    pos_shape = (images.shape[0], 1, patch_size, patch_size)\n\t    x_pos = torch.ones(pos_shape)\n\t    y_pos = torch.ones(pos_shape)\n\t    x_start = np.random.randint(resolution - patch_size)\n\t    y_start = np.random.randint(resolution - patch_size)\n\t    x_pos = x_pos * x_start + torch.arange(patch_size).view(1, -1)\n\t    y_pos = y_pos * y_start + torch.arange(patch_size).view(-1, 1)\n\t    x_pos = (x_pos / resolution - 0.5) * 2.\n", "    y_pos = (y_pos / resolution - 0.5) * 2.\n\t    # Add x and y additional position channels\n\t    images_patch = images[:, :, x_start:x_start + patch_size, y_start:y_start + patch_size]\n\t    images_pos = torch.cat([x_pos.to(device), y_pos.to(device)], dim=1)\n\t    return images_patch, images_pos\n\t#----------------------------------------------------------------------------\n\t# Proposed EDM sampler (Algorithm 2).\n\tdef edm_sampler(\n\t    net, latents, latents_pos, mask_pos, class_labels=None, randn_like=torch.randn_like,\n\t    num_steps=18, sigma_min=0.002, sigma_max=80, rho=7,\n", "    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n\t):\n\t    # img_channel = latents.shape[1]\n\t    # Adjust noise levels based on what's supported by the network.\n\t    sigma_min = max(sigma_min, net.sigma_min)\n\t    sigma_max = min(sigma_max, net.sigma_max)\n\t    # Time step discretization.\n\t    step_indices = torch.arange(num_steps, dtype=torch.float64, device=latents.device)\n\t    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n\t    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n", "    # Main sampling loop.\n\t    x_next = latents.to(torch.float64) * t_steps[0]\n\t    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n\t        x_cur = x_next\n\t        # Increase noise temporarily.\n\t        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n\t        t_hat = net.round_sigma(t_cur + gamma * t_cur)\n\t        x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * randn_like(x_cur)\n\t        # Euler step.\n\t        if mask_pos:\n", "            denoised = net(x_hat, t_hat, class_labels).to(torch.float64)\n\t        else:\n\t            # x_hat_w_pos = torch.cat([x_hat, latents_pos], dim=1)\n\t            denoised = net(x_hat, t_hat, latents_pos, class_labels).to(torch.float64)\n\t            # denoised = denoised[:, :img_channel]\n\t        d_cur = (x_hat - denoised) / t_hat\n\t        x_next = x_hat + (t_next - t_hat) * d_cur\n\t        # Apply 2nd order correction.\n\t        if i < num_steps - 1:\n\t            if mask_pos:\n", "                denoised = net(x_next, t_next, class_labels).to(torch.float64)\n\t            else:\n\t                # x_next_w_pos = torch.cat([x_next, latents_pos], dim=1)\n\t                denoised = net(x_next, t_next, latents_pos, class_labels).to(torch.float64)\n\t                # denoised = denoised[:, :img_channel]\n\t            d_prime = (x_next - denoised) / t_next\n\t            x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n\t    return x_next\n\t#----------------------------------------------------------------------------\n\t# Generalized ablation sampler, representing the superset of all sampling\n", "# methods discussed in the paper.\n\tdef ablation_sampler(\n\t    net, latents, class_labels=None, randn_like=torch.randn_like,\n\t    num_steps=18, sigma_min=None, sigma_max=None, rho=7,\n\t    solver='heun', discretization='edm', schedule='linear', scaling='none',\n\t    epsilon_s=1e-3, C_1=0.001, C_2=0.008, M=1000, alpha=1,\n\t    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n\t):\n\t    assert solver in ['euler', 'heun']\n\t    assert discretization in ['vp', 've', 'iddpm', 'edm']\n", "    assert schedule in ['vp', 've', 'linear']\n\t    assert scaling in ['vp', 'none']\n\t    # Helper functions for VP & VE noise level schedules.\n\t    vp_sigma = lambda beta_d, beta_min: lambda t: (np.e ** (0.5 * beta_d * (t ** 2) + beta_min * t) - 1) ** 0.5\n\t    vp_sigma_deriv = lambda beta_d, beta_min: lambda t: 0.5 * (beta_min + beta_d * t) * (sigma(t) + 1 / sigma(t))\n\t    vp_sigma_inv = lambda beta_d, beta_min: lambda sigma: ((beta_min ** 2 + 2 * beta_d * (sigma ** 2 + 1).log()).sqrt() - beta_min) / beta_d\n\t    ve_sigma = lambda t: t.sqrt()\n\t    ve_sigma_deriv = lambda t: 0.5 / t.sqrt()\n\t    ve_sigma_inv = lambda sigma: sigma ** 2\n\t    # Select default noise level range based on the specified time step discretization.\n", "    if sigma_min is None:\n\t        vp_def = vp_sigma(beta_d=19.1, beta_min=0.1)(t=epsilon_s)\n\t        sigma_min = {'vp': vp_def, 've': 0.02, 'iddpm': 0.002, 'edm': 0.002}[discretization]\n\t    if sigma_max is None:\n\t        vp_def = vp_sigma(beta_d=19.1, beta_min=0.1)(t=1)\n\t        sigma_max = {'vp': vp_def, 've': 100, 'iddpm': 81, 'edm': 80}[discretization]\n\t    # Adjust noise levels based on what's supported by the network.\n\t    sigma_min = max(sigma_min, net.sigma_min)\n\t    sigma_max = min(sigma_max, net.sigma_max)\n\t    # Compute corresponding betas for VP.\n", "    vp_beta_d = 2 * (np.log(sigma_min ** 2 + 1) / epsilon_s - np.log(sigma_max ** 2 + 1)) / (epsilon_s - 1)\n\t    vp_beta_min = np.log(sigma_max ** 2 + 1) - 0.5 * vp_beta_d\n\t    # Define time steps in terms of noise level.\n\t    step_indices = torch.arange(num_steps, dtype=torch.float64, device=latents.device)\n\t    if discretization == 'vp':\n\t        orig_t_steps = 1 + step_indices / (num_steps - 1) * (epsilon_s - 1)\n\t        sigma_steps = vp_sigma(vp_beta_d, vp_beta_min)(orig_t_steps)\n\t    elif discretization == 've':\n\t        orig_t_steps = (sigma_max ** 2) * ((sigma_min ** 2 / sigma_max ** 2) ** (step_indices / (num_steps - 1)))\n\t        sigma_steps = ve_sigma(orig_t_steps)\n", "    elif discretization == 'iddpm':\n\t        u = torch.zeros(M + 1, dtype=torch.float64, device=latents.device)\n\t        alpha_bar = lambda j: (0.5 * np.pi * j / M / (C_2 + 1)).sin() ** 2\n\t        for j in torch.arange(M, 0, -1, device=latents.device): # M, ..., 1\n\t            u[j - 1] = ((u[j] ** 2 + 1) / (alpha_bar(j - 1) / alpha_bar(j)).clip(min=C_1) - 1).sqrt()\n\t        u_filtered = u[torch.logical_and(u >= sigma_min, u <= sigma_max)]\n\t        sigma_steps = u_filtered[((len(u_filtered) - 1) / (num_steps - 1) * step_indices).round().to(torch.int64)]\n\t    else:\n\t        assert discretization == 'edm'\n\t        sigma_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n", "    # Define noise level schedule.\n\t    if schedule == 'vp':\n\t        sigma = vp_sigma(vp_beta_d, vp_beta_min)\n\t        sigma_deriv = vp_sigma_deriv(vp_beta_d, vp_beta_min)\n\t        sigma_inv = vp_sigma_inv(vp_beta_d, vp_beta_min)\n\t    elif schedule == 've':\n\t        sigma = ve_sigma\n\t        sigma_deriv = ve_sigma_deriv\n\t        sigma_inv = ve_sigma_inv\n\t    else:\n", "        assert schedule == 'linear'\n\t        sigma = lambda t: t\n\t        sigma_deriv = lambda t: 1\n\t        sigma_inv = lambda sigma: sigma\n\t    # Define scaling schedule.\n\t    if scaling == 'vp':\n\t        s = lambda t: 1 / (1 + sigma(t) ** 2).sqrt()\n\t        s_deriv = lambda t: -sigma(t) * sigma_deriv(t) * (s(t) ** 3)\n\t    else:\n\t        assert scaling == 'none'\n", "        s = lambda t: 1\n\t        s_deriv = lambda t: 0\n\t    # Compute final time steps based on the corresponding noise levels.\n\t    t_steps = sigma_inv(net.round_sigma(sigma_steps))\n\t    t_steps = torch.cat([t_steps, torch.zeros_like(t_steps[:1])]) # t_N = 0\n\t    # Main sampling loop.\n\t    t_next = t_steps[0]\n\t    x_next = latents.to(torch.float64) * (sigma(t_next) * s(t_next))\n\t    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n\t        x_cur = x_next\n", "        # Increase noise temporarily.\n\t        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= sigma(t_cur) <= S_max else 0\n\t        t_hat = sigma_inv(net.round_sigma(sigma(t_cur) + gamma * sigma(t_cur)))\n\t        x_hat = s(t_hat) / s(t_cur) * x_cur + (sigma(t_hat) ** 2 - sigma(t_cur) ** 2).clip(min=0).sqrt() * s(t_hat) * S_noise * randn_like(x_cur)\n\t        # Euler step.\n\t        h = t_next - t_hat\n\t        denoised = net(x_hat / s(t_hat), sigma(t_hat), class_labels).to(torch.float64)\n\t        d_cur = (sigma_deriv(t_hat) / sigma(t_hat) + s_deriv(t_hat) / s(t_hat)) * x_hat - sigma_deriv(t_hat) * s(t_hat) / sigma(t_hat) * denoised\n\t        x_prime = x_hat + alpha * h * d_cur\n\t        t_prime = t_hat + alpha * h\n", "        # Apply 2nd order correction.\n\t        if solver == 'euler' or i == num_steps - 1:\n\t            x_next = x_hat + h * d_cur\n\t        else:\n\t            assert solver == 'heun'\n\t            denoised = net(x_prime / s(t_prime), sigma(t_prime), class_labels).to(torch.float64)\n\t            d_prime = (sigma_deriv(t_prime) / sigma(t_prime) + s_deriv(t_prime) / s(t_prime)) * x_prime - sigma_deriv(t_prime) * s(t_prime) / sigma(t_prime) * denoised\n\t            x_next = x_hat + h * ((1 - 1 / (2 * alpha)) * d_cur + 1 / (2 * alpha) * d_prime)\n\t    return x_next\n\t#----------------------------------------------------------------------------\n", "# Wrapper for torch.Generator that allows specifying a different random seed\n\t# for each sample in a minibatch.\n\tclass StackedRandomGenerator:\n\t    def __init__(self, device, seeds):\n\t        super().__init__()\n\t        self.generators = [torch.Generator(device).manual_seed(int(seed) % (1 << 32)) for seed in seeds]\n\t    def randn(self, size, **kwargs):\n\t        assert size[0] == len(self.generators)\n\t        return torch.stack([torch.randn(size[1:], generator=gen, **kwargs) for gen in self.generators])\n\t    def randn_like(self, input):\n", "        return self.randn(input.shape, dtype=input.dtype, layout=input.layout, device=input.device)\n\t    def randint(self, *args, size, **kwargs):\n\t        assert size[0] == len(self.generators)\n\t        return torch.stack([torch.randint(*args, size=size[1:], generator=gen, **kwargs) for gen in self.generators])\n\t#----------------------------------------------------------------------------\n\t# Parse a comma separated list of numbers or ranges and return a list of ints.\n\t# Example: '1,2,5-10' returns [1, 2, 5, 6, 7, 8, 9, 10]\n\tdef parse_int_list(s):\n\t    if isinstance(s, list): return s\n\t    ranges = []\n", "    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n\t    for p in s.split(','):\n\t        m = range_re.match(p)\n\t        if m:\n\t            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n\t        else:\n\t            ranges.append(int(p))\n\t    return ranges\n\t#----------------------------------------------------------------------------\n\tdef set_requires_grad(model, value):\n", "    for param in model.parameters():\n\t        param.requires_grad = value\n\t#----------------------------------------------------------------------------\n\t@click.command()\n\t@click.option('--network', 'network_pkl',  help='Network pickle filename', metavar='PATH|URL',                      type=str, required=True)\n\t@click.option('--resolution',              help='Sample resolution', metavar='INT',                                 type=int, default=64)\n\t@click.option('--embed_fq',                help='Positional embedding frequency', metavar='INT',                    type=int, default=0)\n\t@click.option('--mask_pos',                help='Mask out pos channels', metavar='BOOL',                            type=bool, default=False, show_default=True)\n\t@click.option('--on_latents',              help='Generate with latent vae', metavar='BOOL',                            type=bool, default=False, show_default=True)\n\t@click.option('--outdir',                  help='Where to save the output images', metavar='DIR',                   type=str, required=True)\n", "# patch options\n\t@click.option('--x_start',                 help='Sample resolution', metavar='INT',                                 type=int, default=0)\n\t@click.option('--y_start',                 help='Sample resolution', metavar='INT',                                 type=int, default=0)\n\t@click.option('--image_size',                help='Sample resolution', metavar='INT',                                 type=int, default=None)\n\t@click.option('--seeds',                   help='Random seeds (e.g. 1,2,5-10)', metavar='LIST',                     type=parse_int_list, default='0-63', show_default=True)\n\t@click.option('--subdirs',                 help='Create subdirectory for every 1000 seeds',                         is_flag=True)\n\t@click.option('--class', 'class_idx',      help='Class label  [default: random]', metavar='INT',                    type=click.IntRange(min=0), default=None)\n\t@click.option('--batch', 'max_batch_size', help='Maximum batch size', metavar='INT',                                type=click.IntRange(min=1), default=64, show_default=True)\n\t@click.option('--steps', 'num_steps',      help='Number of sampling steps', metavar='INT',                          type=click.IntRange(min=1), default=18, show_default=True)\n\t@click.option('--sigma_min',               help='Lowest noise level  [default: varies]', metavar='FLOAT',           type=click.FloatRange(min=0, min_open=True))\n", "@click.option('--sigma_max',               help='Highest noise level  [default: varies]', metavar='FLOAT',          type=click.FloatRange(min=0, min_open=True))\n\t@click.option('--rho',                     help='Time step exponent', metavar='FLOAT',                              type=click.FloatRange(min=0, min_open=True), default=7, show_default=True)\n\t@click.option('--S_churn', 'S_churn',      help='Stochasticity strength', metavar='FLOAT',                          type=click.FloatRange(min=0), default=0, show_default=True)\n\t@click.option('--S_min', 'S_min',          help='Stoch. min noise level', metavar='FLOAT',                          type=click.FloatRange(min=0), default=0, show_default=True)\n\t@click.option('--S_max', 'S_max',          help='Stoch. max noise level', metavar='FLOAT',                          type=click.FloatRange(min=0), default='inf', show_default=True)\n\t@click.option('--S_noise', 'S_noise',      help='Stoch. noise inflation', metavar='FLOAT',                          type=float, default=1, show_default=True)\n\t@click.option('--solver',                  help='Ablate ODE solver', metavar='euler|heun',                          type=click.Choice(['euler', 'heun']))\n\t@click.option('--disc', 'discretization',  help='Ablate time step discretization {t_i}', metavar='vp|ve|iddpm|edm', type=click.Choice(['vp', 've', 'iddpm', 'edm']))\n\t@click.option('--schedule',                help='Ablate noise schedule sigma(t)', metavar='vp|ve|linear',           type=click.Choice(['vp', 've', 'linear']))\n\t@click.option('--scaling',                 help='Ablate signal scaling s(t)', metavar='vp|none',                    type=click.Choice(['vp', 'none']))\n", "def main(network_pkl, resolution, on_latents, embed_fq, mask_pos, x_start, y_start, image_size, outdir, subdirs, seeds, class_idx, max_batch_size, device=torch.device('cuda'), **sampler_kwargs):\n\t    \"\"\"Generate random images using the techniques described in the paper\n\t    \"Elucidating the Design Space of Diffusion-Based Generative Models\".\n\t    Examples:\n\t    \\b\n\t    # Generate 64 images and save them as out/*.png\n\t    python generate.py --outdir=out --seeds=0-63 --batch=64 \\\\\n\t        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n\t    \\b\n\t    # Generate 1024 images using 2 GPUs\n", "    torchrun --standalone --nproc_per_node=2 generate.py --outdir=out --seeds=0-999 --batch=64 \\\\\n\t        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n\t    \"\"\"\n\t    dist.init()\n\t    num_batches = ((len(seeds) - 1) // (max_batch_size * dist.get_world_size()) + 1) * dist.get_world_size()\n\t    all_batches = torch.as_tensor(seeds).tensor_split(num_batches)\n\t    rank_batches = all_batches[dist.get_rank() :: dist.get_world_size()]\n\t    # Rank 0 goes first.\n\t    if dist.get_rank() != 0:\n\t        torch.distributed.barrier()\n", "    # Load network.\n\t    dist.print0(f'Loading network from \"{network_pkl}\"...')\n\t    with dnnlib.util.open_url(network_pkl, verbose=(dist.get_rank() == 0)) as f:\n\t        net = pickle.load(f)['ema'].to(device)\n\t    # Other ranks follow.\n\t    if dist.get_rank() == 0:\n\t        torch.distributed.barrier()\n\t    if on_latents:\n\t        # img_vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(device)\n\t        img_vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device)\n", "        img_vae.eval()\n\t        set_requires_grad(img_vae, False)\n\t        latent_scale_factor = 0.18215\n\t    # Loop over batches.\n\t    dist.print0(f'Generating {len(seeds)} images to \"{outdir}\"...')\n\t    for batch_seeds in tqdm.tqdm(rank_batches, unit='batch', disable=(dist.get_rank() != 0)):\n\t        torch.distributed.barrier()\n\t        batch_size = len(batch_seeds)\n\t        if batch_size == 0:\n\t            continue\n", "        # Pick latents and labels.\n\t        rnd = StackedRandomGenerator(device, batch_seeds)\n\t        image_channel = 3\n\t        if image_size is None: image_size = resolution\n\t        if on_latents:\n\t            x_start, image_size, resolution, image_channel = 0, 32, 32, 4\n\t        x_pos = torch.arange(x_start, x_start+image_size).view(1, -1).repeat(image_size, 1)\n\t        y_pos = torch.arange(y_start, y_start+image_size).view(-1, 1).repeat(1, image_size)\n\t        x_pos = (x_pos / (resolution - 1) - 0.5) * 2.\n\t        y_pos = (y_pos / (resolution - 1) - 0.5) * 2.\n", "        latents_pos = torch.stack([x_pos, y_pos], dim=0).to(device)\n\t        latents_pos = latents_pos.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\t        if mask_pos: latents_pos = torch.zeros_like(latents_pos)\n\t        if embed_fq > 0:\n\t            pos_embed = Pos_Embedding(num_freqs=embed_fq)\n\t            latents_pos = pos_embed(latents_pos)\n\t        latents = rnd.randn([batch_size, image_channel, image_size, image_size], device=device)\n\t        # rnd = StackedRandomGenerator(device, batch_seeds)\n\t        # latents = rnd.randn([batch_size, 3, 64, 64], device=device)\n\t        # latents, latents_pos = random_patch(latents, 16, 64)\n", "        class_labels = None\n\t        if net.label_dim:\n\t            class_labels = torch.eye(net.label_dim, device=device)[rnd.randint(net.label_dim, size=[batch_size], device=device)]\n\t        if class_idx is not None:\n\t            class_labels[:, :] = 0\n\t            class_labels[:, class_idx] = 1\n\t        # Generate images.\n\t        sampler_kwargs = {key: value for key, value in sampler_kwargs.items() if value is not None}\n\t        have_ablation_kwargs = any(x in sampler_kwargs for x in ['solver', 'discretization', 'schedule', 'scaling'])\n\t        sampler_fn = ablation_sampler if have_ablation_kwargs else edm_sampler\n", "        images = sampler_fn(net, latents, latents_pos, mask_pos, class_labels, randn_like=rnd.randn_like, **sampler_kwargs)\n\t        if on_latents:\n\t            images = 1 / 0.18215 * images\n\t            images = img_vae.decode(images.float()).sample\n\t        # Save images.\n\t        images_np = (images * 127.5 + 128).clip(0, 255).to(torch.uint8).permute(0, 2, 3, 1).cpu().numpy()\n\t        for seed, image_np in zip(batch_seeds, images_np):\n\t            image_dir = os.path.join(outdir, f'{seed-seed%1000:06d}') if subdirs else outdir\n\t            os.makedirs(image_dir, exist_ok=True)\n\t            image_path = os.path.join(image_dir, f'{seed:06d}.png')\n", "            if image_np.shape[2] == 1:\n\t                PIL.Image.fromarray(image_np[:, :, 0], 'L').save(image_path)\n\t            else:\n\t                PIL.Image.fromarray(image_np, 'RGB').save(image_path)\n\t    # Done.\n\t    torch.distributed.barrier()\n\t    dist.print0('Done.')\n\t#----------------------------------------------------------------------------\n\tif __name__ == \"__main__\":\n\t    main()\n", "#----------------------------------------------------------------------------\n"]}
{"filename": "training/networks.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Model architectures and preconditioning schemes used in the paper\n\t\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\timport numpy as np\n\timport torch\n", "from torch_utils import persistence\n\tfrom torch.nn.functional import silu\n\t#----------------------------------------------------------------------------\n\t# Unified routine for initializing weights and biases.\n\tdef weight_init(shape, mode, fan_in, fan_out):\n\t    if mode == 'xavier_uniform': return np.sqrt(6 / (fan_in + fan_out)) * (torch.rand(*shape) * 2 - 1)\n\t    if mode == 'xavier_normal':  return np.sqrt(2 / (fan_in + fan_out)) * torch.randn(*shape)\n\t    if mode == 'kaiming_uniform': return np.sqrt(3 / fan_in) * (torch.rand(*shape) * 2 - 1)\n\t    if mode == 'kaiming_normal':  return np.sqrt(1 / fan_in) * torch.randn(*shape)\n\t    raise ValueError(f'Invalid init mode \"{mode}\"')\n", "#----------------------------------------------------------------------------\n\t# Fully-connected layer.\n\t@persistence.persistent_class\n\tclass Linear(torch.nn.Module):\n\t    def __init__(self, in_features, out_features, bias=True, init_mode='kaiming_normal', init_weight=1, init_bias=0):\n\t        super().__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        init_kwargs = dict(mode=init_mode, fan_in=in_features, fan_out=out_features)\n\t        self.weight = torch.nn.Parameter(weight_init([out_features, in_features], **init_kwargs) * init_weight)\n", "        self.bias = torch.nn.Parameter(weight_init([out_features], **init_kwargs) * init_bias) if bias else None\n\t    def forward(self, x):\n\t        x = x @ self.weight.to(x.dtype).t()\n\t        if self.bias is not None:\n\t            x = x.add_(self.bias.to(x.dtype))\n\t        return x\n\t#----------------------------------------------------------------------------\n\t# Convolutional layer with optional up/downsampling.\n\t@persistence.persistent_class\n\tclass Conv2d(torch.nn.Module):\n", "    def __init__(self,\n\t        in_channels, out_channels, kernel, bias=True, up=False, down=False,\n\t        resample_filter=[1,1], fused_resample=False, init_mode='kaiming_normal', init_weight=1, init_bias=0,\n\t    ):\n\t        assert not (up and down)\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.up = up\n\t        self.down = down\n", "        self.fused_resample = fused_resample\n\t        init_kwargs = dict(mode=init_mode, fan_in=in_channels*kernel*kernel, fan_out=out_channels*kernel*kernel)\n\t        self.weight = torch.nn.Parameter(weight_init([out_channels, in_channels, kernel, kernel], **init_kwargs) * init_weight) if kernel else None\n\t        self.bias = torch.nn.Parameter(weight_init([out_channels], **init_kwargs) * init_bias) if kernel and bias else None\n\t        f = torch.as_tensor(resample_filter, dtype=torch.float32)\n\t        f = f.ger(f).unsqueeze(0).unsqueeze(1) / f.sum().square()\n\t        self.register_buffer('resample_filter', f if up or down else None)\n\t    def forward(self, x):\n\t        w = self.weight.to(x.dtype) if self.weight is not None else None\n\t        b = self.bias.to(x.dtype) if self.bias is not None else None\n", "        f = self.resample_filter.to(x.dtype) if self.resample_filter is not None else None\n\t        w_pad = w.shape[-1] // 2 if w is not None else 0\n\t        f_pad = (f.shape[-1] - 1) // 2 if f is not None else 0\n\t        if self.fused_resample and self.up and w is not None:\n\t            x = torch.nn.functional.conv_transpose2d(x, f.mul(4).tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=max(f_pad - w_pad, 0))\n\t            x = torch.nn.functional.conv2d(x, w, padding=max(w_pad - f_pad, 0))\n\t        elif self.fused_resample and self.down and w is not None:\n\t            x = torch.nn.functional.conv2d(x, w, padding=w_pad+f_pad)\n\t            x = torch.nn.functional.conv2d(x, f.tile([self.out_channels, 1, 1, 1]), groups=self.out_channels, stride=2)\n\t        else:\n", "            if self.up:\n\t                x = torch.nn.functional.conv_transpose2d(x, f.mul(4).tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=f_pad)\n\t            if self.down:\n\t                x = torch.nn.functional.conv2d(x, f.tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=f_pad)\n\t            if w is not None:\n\t                x = torch.nn.functional.conv2d(x, w, padding=w_pad)\n\t        if b is not None:\n\t            x = x.add_(b.reshape(1, -1, 1, 1))\n\t        return x\n\t#----------------------------------------------------------------------------\n", "# Group normalization.\n\t@persistence.persistent_class\n\tclass GroupNorm(torch.nn.Module):\n\t    def __init__(self, num_channels, num_groups=32, min_channels_per_group=4, eps=1e-5):\n\t        super().__init__()\n\t        self.num_groups = min(num_groups, num_channels // min_channels_per_group)\n\t        self.eps = eps\n\t        self.weight = torch.nn.Parameter(torch.ones(num_channels))\n\t        self.bias = torch.nn.Parameter(torch.zeros(num_channels))\n\t    def forward(self, x):\n", "        x = torch.nn.functional.group_norm(x, num_groups=self.num_groups, weight=self.weight.to(x.dtype), bias=self.bias.to(x.dtype), eps=self.eps)\n\t        return x\n\t#----------------------------------------------------------------------------\n\t# Attention weight computation, i.e., softmax(Q^T * K).\n\t# Performs all computation using FP32, but uses the original datatype for\n\t# inputs/outputs/gradients to conserve memory.\n\tclass AttentionOp(torch.autograd.Function):\n\t    @staticmethod\n\t    def forward(ctx, q, k):\n\t        w = torch.einsum('ncq,nck->nqk', q.to(torch.float32), (k / np.sqrt(k.shape[1])).to(torch.float32)).softmax(dim=2).to(q.dtype)\n", "        ctx.save_for_backward(q, k, w)\n\t        return w\n\t    @staticmethod\n\t    def backward(ctx, dw):\n\t        q, k, w = ctx.saved_tensors\n\t        db = torch._softmax_backward_data(grad_output=dw.to(torch.float32), output=w.to(torch.float32), dim=2, input_dtype=torch.float32)\n\t        dq = torch.einsum('nck,nqk->ncq', k.to(torch.float32), db).to(q.dtype) / np.sqrt(k.shape[1])\n\t        dk = torch.einsum('ncq,nqk->nck', q.to(torch.float32), db).to(k.dtype) / np.sqrt(k.shape[1])\n\t        return dq, dk\n\t#----------------------------------------------------------------------------\n", "# Unified U-Net block with optional up/downsampling and self-attention.\n\t# Represents the union of all features employed by the DDPM++, NCSN++, and\n\t# ADM architectures.\n\t@persistence.persistent_class\n\tclass UNetBlock(torch.nn.Module):\n\t    def __init__(self,\n\t        in_channels, out_channels, emb_channels, up=False, down=False, attention=False,\n\t        num_heads=None, channels_per_head=64, dropout=0, skip_scale=1, eps=1e-5,\n\t        resample_filter=[1,1], resample_proj=False, adaptive_scale=True,\n\t        init=dict(), init_zero=dict(init_weight=0), init_attn=None,\n", "    ):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.emb_channels = emb_channels\n\t        self.num_heads = 0 if not attention else num_heads if num_heads is not None else out_channels // channels_per_head\n\t        self.dropout = dropout\n\t        self.skip_scale = skip_scale\n\t        self.adaptive_scale = adaptive_scale\n\t        self.norm0 = GroupNorm(num_channels=in_channels, eps=eps)\n", "        self.conv0 = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel=3, up=up, down=down, resample_filter=resample_filter, **init)\n\t        self.affine = Linear(in_features=emb_channels, out_features=out_channels*(2 if adaptive_scale else 1), **init)\n\t        self.norm1 = GroupNorm(num_channels=out_channels, eps=eps)\n\t        self.conv1 = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=3, **init_zero)\n\t        self.skip = None\n\t        if out_channels != in_channels or up or down:\n\t            kernel = 1 if resample_proj or out_channels!= in_channels else 0\n\t            self.skip = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel=kernel, up=up, down=down, resample_filter=resample_filter, **init)\n\t        if self.num_heads:\n\t            self.norm2 = GroupNorm(num_channels=out_channels, eps=eps)\n", "            self.qkv = Conv2d(in_channels=out_channels, out_channels=out_channels*3, kernel=1, **(init_attn if init_attn is not None else init))\n\t            self.proj = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=1, **init_zero)\n\t    def forward(self, x, emb):\n\t        orig = x\n\t        x = self.conv0(silu(self.norm0(x)))\n\t        params = self.affine(emb).unsqueeze(2).unsqueeze(3).to(x.dtype)\n\t        if self.adaptive_scale:\n\t            scale, shift = params.chunk(chunks=2, dim=1)\n\t            x = silu(torch.addcmul(shift, self.norm1(x), scale + 1))\n\t        else:\n", "            x = silu(self.norm1(x.add_(params)))\n\t        x = self.conv1(torch.nn.functional.dropout(x, p=self.dropout, training=self.training))\n\t        x = x.add_(self.skip(orig) if self.skip is not None else orig)\n\t        x = x * self.skip_scale\n\t        if self.num_heads:\n\t            q, k, v = self.qkv(self.norm2(x)).reshape(x.shape[0] * self.num_heads, x.shape[1] // self.num_heads, 3, -1).unbind(2)\n\t            w = AttentionOp.apply(q, k)\n\t            a = torch.einsum('nqk,nck->ncq', w, v)\n\t            x = self.proj(a.reshape(*x.shape)).add_(x)\n\t            x = x * self.skip_scale\n", "        return x\n\t#----------------------------------------------------------------------------\n\t# Timestep embedding used in the DDPM++ and ADM architectures.\n\t@persistence.persistent_class\n\tclass PositionalEmbedding(torch.nn.Module):\n\t    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n\t        super().__init__()\n\t        self.num_channels = num_channels\n\t        self.max_positions = max_positions\n\t        self.endpoint = endpoint\n", "    def forward(self, x):\n\t        freqs = torch.arange(start=0, end=self.num_channels//2, dtype=torch.float32, device=x.device)\n\t        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n\t        freqs = (1 / self.max_positions) ** freqs\n\t        x = x.ger(freqs.to(x.dtype))\n\t        x = torch.cat([x.cos(), x.sin()], dim=1)\n\t        return x\n\t#----------------------------------------------------------------------------\n\t# Timestep embedding used in the NCSN++ architecture.\n\t@persistence.persistent_class\n", "class FourierEmbedding(torch.nn.Module):\n\t    def __init__(self, num_channels, scale=16):\n\t        super().__init__()\n\t        self.register_buffer('freqs', torch.randn(num_channels // 2) * scale)\n\t    def forward(self, x):\n\t        x = x.ger((2 * np.pi * self.freqs).to(x.dtype))\n\t        x = torch.cat([x.cos(), x.sin()], dim=1)\n\t        return x\n\t#----------------------------------------------------------------------------\n\t# Reimplementation of the DDPM++ and NCSN++ architectures from the paper\n", "# \"Score-Based Generative Modeling through Stochastic Differential\n\t# Equations\". Equivalent to the original implementation by Song et al.,\n\t# available at https://github.com/yang-song/score_sde_pytorch\n\t@persistence.persistent_class\n\tclass SongUNet(torch.nn.Module):\n\t    def __init__(self,\n\t        img_resolution,                     # Image resolution at input/output.\n\t        in_channels,                        # Number of color channels at input.\n\t        out_channels,                       # Number of color channels at output.\n\t        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n", "        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n\t        model_channels      = 128,          # Base multiplier for the number of channels.\n\t        channel_mult        = [1,2,2,2],    # Per-resolution multipliers for the number of channels.\n\t        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n\t        num_blocks          = 4,            # Number of residual blocks per resolution.\n\t        attn_resolutions    = [16],         # List of resolutions with self-attention.\n\t        dropout             = 0.10,         # Dropout probability of intermediate activations.\n\t        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n\t        embedding_type      = 'positional', # Timestep embedding type: 'positional' for DDPM++, 'fourier' for NCSN++.\n\t        channel_mult_noise  = 1,            # Timestep embedding size: 1 for DDPM++, 2 for NCSN++.\n", "        encoder_type        = 'standard',   # Encoder architecture: 'standard' for DDPM++, 'residual' for NCSN++.\n\t        decoder_type        = 'standard',   # Decoder architecture: 'standard' for both DDPM++ and NCSN++.\n\t        resample_filter     = [1,1],        # Resampling filter: [1,1] for DDPM++, [1,3,3,1] for NCSN++.\n\t        implicit_mlp        = False,        # enable implicit coordinate encoding\n\t    ):\n\t        assert embedding_type in ['fourier', 'positional']\n\t        assert encoder_type in ['standard', 'skip', 'residual']\n\t        assert decoder_type in ['standard', 'skip']\n\t        super().__init__()\n\t        self.label_dropout = label_dropout\n", "        emb_channels = model_channels * channel_mult_emb\n\t        noise_channels = model_channels * channel_mult_noise\n\t        init = dict(init_mode='xavier_uniform')\n\t        init_zero = dict(init_mode='xavier_uniform', init_weight=1e-5)\n\t        init_attn = dict(init_mode='xavier_uniform', init_weight=np.sqrt(0.2))\n\t        block_kwargs = dict(\n\t            emb_channels=emb_channels, num_heads=1, dropout=dropout, skip_scale=np.sqrt(0.5), eps=1e-6,\n\t            resample_filter=resample_filter, resample_proj=True, adaptive_scale=False,\n\t            init=init, init_zero=init_zero, init_attn=init_attn,\n\t        )\n", "        # Mapping.\n\t        self.map_noise = PositionalEmbedding(num_channels=noise_channels, endpoint=True) if embedding_type == 'positional' else FourierEmbedding(num_channels=noise_channels)\n\t        self.map_label = Linear(in_features=label_dim, out_features=noise_channels, **init) if label_dim else None\n\t        self.map_augment = Linear(in_features=augment_dim, out_features=noise_channels, bias=False, **init) if augment_dim else None\n\t        self.map_layer0 = Linear(in_features=noise_channels, out_features=emb_channels, **init)\n\t        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n\t        # Encoder.\n\t        self.enc = torch.nn.ModuleDict()\n\t        cout = in_channels\n\t        caux = in_channels\n", "        for level, mult in enumerate(channel_mult):\n\t            res = img_resolution >> level\n\t            if level == 0:\n\t                cin = cout\n\t                cout = model_channels\n\t                if implicit_mlp:\n\t                    self.enc[f'{res}x{res}_conv'] = torch.nn.Sequential(\n\t                                                        Conv2d(in_channels=cin, out_channels=cout, kernel=1, **init),\n\t                                                        torch.nn.SiLU(),\n\t                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=1, **init),\n", "                                                        torch.nn.SiLU(),\n\t                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=1, **init),\n\t                                                        torch.nn.SiLU(),\n\t                                                        Conv2d(in_channels=cout, out_channels=cout, kernel=3, **init),\n\t                                                    )\n\t                    self.enc[f'{res}x{res}_conv'].out_channels = cout\n\t                else:\n\t                    self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n\t            else:\n\t                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n", "                if encoder_type == 'skip':\n\t                    self.enc[f'{res}x{res}_aux_down'] = Conv2d(in_channels=caux, out_channels=caux, kernel=0, down=True, resample_filter=resample_filter)\n\t                    self.enc[f'{res}x{res}_aux_skip'] = Conv2d(in_channels=caux, out_channels=cout, kernel=1, **init)\n\t                if encoder_type == 'residual':\n\t                    self.enc[f'{res}x{res}_aux_residual'] = Conv2d(in_channels=caux, out_channels=cout, kernel=3, down=True, resample_filter=resample_filter, fused_resample=True, **init)\n\t                    caux = cout\n\t            for idx in range(num_blocks):\n\t                cin = cout\n\t                cout = model_channels * mult\n\t                attn = (res in attn_resolutions)\n", "                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=attn, **block_kwargs)\n\t        skips = [block.out_channels for name, block in self.enc.items() if 'aux' not in name]\n\t        # Decoder.\n\t        self.dec = torch.nn.ModuleDict()\n\t        for level, mult in reversed(list(enumerate(channel_mult))):\n\t            res = img_resolution >> level\n\t            if level == len(channel_mult) - 1:\n\t                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n\t                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n\t            else:\n", "                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n\t            for idx in range(num_blocks + 1):\n\t                cin = cout + skips.pop()\n\t                cout = model_channels * mult\n\t                attn = (idx == num_blocks and res in attn_resolutions)\n\t                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=attn, **block_kwargs)\n\t            if decoder_type == 'skip' or level == 0:\n\t                if decoder_type == 'skip' and level < len(channel_mult) - 1:\n\t                    self.dec[f'{res}x{res}_aux_up'] = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=0, up=True, resample_filter=resample_filter)\n\t                self.dec[f'{res}x{res}_aux_norm'] = GroupNorm(num_channels=cout, eps=1e-6)\n", "                self.dec[f'{res}x{res}_aux_conv'] = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n\t    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n\t        # Mapping.\n\t        emb = self.map_noise(noise_labels)\n\t        emb = emb.reshape(emb.shape[0], 2, -1).flip(1).reshape(*emb.shape) # swap sin/cos\n\t        if self.map_label is not None:\n\t            tmp = class_labels\n\t            if self.training and self.label_dropout:\n\t                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n\t            emb = emb + self.map_label(tmp * np.sqrt(self.map_label.in_features))\n", "        if self.map_augment is not None and augment_labels is not None:\n\t            emb = emb + self.map_augment(augment_labels)\n\t        emb = silu(self.map_layer0(emb))\n\t        emb = silu(self.map_layer1(emb))\n\t        # Encoder.\n\t        skips = []\n\t        aux = x\n\t        for name, block in self.enc.items():\n\t            if 'aux_down' in name:\n\t                aux = block(aux)\n", "            elif 'aux_skip' in name:\n\t                x = skips[-1] = x + block(aux)\n\t            elif 'aux_residual' in name:\n\t                x = skips[-1] = aux = (x + block(aux)) / np.sqrt(2)\n\t            else:\n\t                x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n\t                skips.append(x)\n\t        # Decoder.\n\t        aux = None\n\t        tmp = None\n", "        for name, block in self.dec.items():\n\t            if 'aux_up' in name:\n\t                aux = block(aux)\n\t            elif 'aux_norm' in name:\n\t                tmp = block(x)\n\t            elif 'aux_conv' in name:\n\t                tmp = block(silu(tmp))\n\t                aux = tmp if aux is None else tmp + aux\n\t            else:\n\t                if x.shape[1] != block.in_channels:\n", "                    x = torch.cat([x, skips.pop()], dim=1)\n\t                x = block(x, emb)\n\t        return aux\n\t#----------------------------------------------------------------------------\n\t# Reimplementation of the ADM architecture from the paper\n\t# \"Diffusion Models Beat GANS on Image Synthesis\". Equivalent to the\n\t# original implementation by Dhariwal and Nichol, available at\n\t# https://github.com/openai/guided-diffusion\n\t@persistence.persistent_class\n\tclass DhariwalUNet(torch.nn.Module):\n", "    def __init__(self,\n\t        img_resolution,                     # Image resolution at input/output.\n\t        in_channels,                        # Number of color channels at input.\n\t        out_channels,                       # Number of color channels at output.\n\t        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n\t        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n\t        model_channels      = 192,          # Base multiplier for the number of channels.\n\t        channel_mult        = [1,2,3,4],    # Per-resolution multipliers for the number of channels.\n\t        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n\t        num_blocks          = 3,            # Number of residual blocks per resolution.\n", "        attn_resolutions    = [32,16,8],    # List of resolutions with self-attention.\n\t        dropout             = 0.10,         # List of resolutions with self-attention.\n\t        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n\t    ):\n\t        super().__init__()\n\t        self.label_dropout = label_dropout\n\t        emb_channels = model_channels * channel_mult_emb\n\t        init = dict(init_mode='kaiming_uniform', init_weight=np.sqrt(1/3), init_bias=np.sqrt(1/3))\n\t        init_zero = dict(init_mode='kaiming_uniform', init_weight=0, init_bias=0)\n\t        block_kwargs = dict(emb_channels=emb_channels, channels_per_head=64, dropout=dropout, init=init, init_zero=init_zero)\n", "        # Mapping.\n\t        self.map_noise = PositionalEmbedding(num_channels=model_channels)\n\t        self.map_augment = Linear(in_features=augment_dim, out_features=model_channels, bias=False, **init_zero) if augment_dim else None\n\t        self.map_layer0 = Linear(in_features=model_channels, out_features=emb_channels, **init)\n\t        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n\t        self.map_label = Linear(in_features=label_dim, out_features=emb_channels, bias=False, init_mode='kaiming_normal', init_weight=np.sqrt(label_dim)) if label_dim else None\n\t        # Encoder.\n\t        self.enc = torch.nn.ModuleDict()\n\t        cout = in_channels\n\t        for level, mult in enumerate(channel_mult):\n", "            res = img_resolution >> level\n\t            if level == 0:\n\t                cin = cout\n\t                cout = model_channels * mult\n\t                self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n\t            else:\n\t                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n\t            for idx in range(num_blocks):\n\t                cin = cout\n\t                cout = model_channels * mult\n", "                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n\t        skips = [block.out_channels for block in self.enc.values()]\n\t        # Decoder.\n\t        self.dec = torch.nn.ModuleDict()\n\t        for level, mult in reversed(list(enumerate(channel_mult))):\n\t            res = img_resolution >> level\n\t            if level == len(channel_mult) - 1:\n\t                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n\t                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n\t            else:\n", "                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n\t            for idx in range(num_blocks + 1):\n\t                cin = cout + skips.pop()\n\t                cout = model_channels * mult\n\t                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n\t        self.out_norm = GroupNorm(num_channels=cout)\n\t        self.out_conv = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n\t    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n\t        # Mapping.\n\t        emb = self.map_noise(noise_labels)\n", "        if self.map_augment is not None and augment_labels is not None:\n\t            emb = emb + self.map_augment(augment_labels)\n\t        emb = silu(self.map_layer0(emb))\n\t        emb = self.map_layer1(emb)\n\t        if self.map_label is not None:\n\t            tmp = class_labels\n\t            if self.training and self.label_dropout:\n\t                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n\t            emb = emb + self.map_label(tmp)\n\t        emb = silu(emb)\n", "        # Encoder.\n\t        skips = []\n\t        for block in self.enc.values():\n\t            x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n\t            skips.append(x)\n\t        # Decoder.\n\t        for block in self.dec.values():\n\t            if x.shape[1] != block.in_channels:\n\t                x = torch.cat([x, skips.pop()], dim=1)\n\t            x = block(x, emb)\n", "        x = self.out_conv(silu(self.out_norm(x)))\n\t        return x\n\t#----------------------------------------------------------------------------\n\t# Preconditioning corresponding to the variance preserving (VP) formulation\n\t# from the paper \"Score-Based Generative Modeling through Stochastic\n\t# Differential Equations\".\n\t@persistence.persistent_class\n\tclass VPPrecond(torch.nn.Module):\n\t    def __init__(self,\n\t        img_resolution,                 # Image resolution.\n", "        img_channels,                   # Number of color channels.\n\t        label_dim       = 0,            # Number of class labels, 0 = unconditional.\n\t        use_fp16        = False,        # Execute the underlying model at FP16 precision?\n\t        beta_d          = 19.9,         # Extent of the noise level schedule.\n\t        beta_min        = 0.1,          # Initial slope of the noise level schedule.\n\t        M               = 1000,         # Original number of timesteps in the DDPM formulation.\n\t        epsilon_t       = 1e-5,         # Minimum t-value used during training.\n\t        model_type      = 'SongUNet',   # Class name of the underlying model.\n\t        **model_kwargs,                 # Keyword arguments for the underlying model.\n\t    ):\n", "        super().__init__()\n\t        self.img_resolution = img_resolution\n\t        self.img_channels = img_channels\n\t        self.label_dim = label_dim\n\t        self.use_fp16 = use_fp16\n\t        self.beta_d = beta_d\n\t        self.beta_min = beta_min\n\t        self.M = M\n\t        self.epsilon_t = epsilon_t\n\t        self.sigma_min = float(self.sigma(epsilon_t))\n", "        self.sigma_max = float(self.sigma(1))\n\t        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n\t    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n\t        x = x.to(torch.float32)\n\t        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n\t        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n\t        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\t        c_skip = 1\n\t        c_out = -sigma\n\t        c_in = 1 / (sigma ** 2 + 1).sqrt()\n", "        c_noise = (self.M - 1) * self.sigma_inv(sigma)\n\t        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n\t        assert F_x.dtype == dtype\n\t        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n\t        return D_x\n\t    def sigma(self, t):\n\t        t = torch.as_tensor(t)\n\t        return ((0.5 * self.beta_d * (t ** 2) + self.beta_min * t).exp() - 1).sqrt()\n\t    def sigma_inv(self, sigma):\n\t        sigma = torch.as_tensor(sigma)\n", "        return ((self.beta_min ** 2 + 2 * self.beta_d * (1 + sigma ** 2).log()).sqrt() - self.beta_min) / self.beta_d\n\t    def round_sigma(self, sigma):\n\t        return torch.as_tensor(sigma)\n\t#----------------------------------------------------------------------------\n\t# Preconditioning corresponding to the variance exploding (VE) formulation\n\t# from the paper \"Score-Based Generative Modeling through Stochastic\n\t# Differential Equations\".\n\t@persistence.persistent_class\n\tclass VEPrecond(torch.nn.Module):\n\t    def __init__(self,\n", "        img_resolution,                 # Image resolution.\n\t        img_channels,                   # Number of color channels.\n\t        label_dim       = 0,            # Number of class labels, 0 = unconditional.\n\t        use_fp16        = False,        # Execute the underlying model at FP16 precision?\n\t        sigma_min       = 0.02,         # Minimum supported noise level.\n\t        sigma_max       = 100,          # Maximum supported noise level.\n\t        model_type      = 'SongUNet',   # Class name of the underlying model.\n\t        **model_kwargs,                 # Keyword arguments for the underlying model.\n\t    ):\n\t        super().__init__()\n", "        self.img_resolution = img_resolution\n\t        self.img_channels = img_channels\n\t        self.label_dim = label_dim\n\t        self.use_fp16 = use_fp16\n\t        self.sigma_min = sigma_min\n\t        self.sigma_max = sigma_max\n\t        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n\t    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n\t        x = x.to(torch.float32)\n\t        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n", "        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n\t        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\t        c_skip = 1\n\t        c_out = sigma\n\t        c_in = 1\n\t        c_noise = (0.5 * sigma).log()\n\t        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n\t        assert F_x.dtype == dtype\n\t        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n\t        return D_x\n", "    def round_sigma(self, sigma):\n\t        return torch.as_tensor(sigma)\n\t#----------------------------------------------------------------------------\n\t# Preconditioning corresponding to improved DDPM (iDDPM) formulation from\n\t# the paper \"Improved Denoising Diffusion Probabilistic Models\".\n\t@persistence.persistent_class\n\tclass iDDPMPrecond(torch.nn.Module):\n\t    def __init__(self,\n\t        img_resolution,                     # Image resolution.\n\t        img_channels,                       # Number of color channels.\n", "        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n\t        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n\t        C_1             = 0.001,            # Timestep adjustment at low noise levels.\n\t        C_2             = 0.008,            # Timestep adjustment at high noise levels.\n\t        M               = 1000,             # Original number of timesteps in the DDPM formulation.\n\t        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n\t        **model_kwargs,                     # Keyword arguments for the underlying model.\n\t    ):\n\t        super().__init__()\n\t        self.img_resolution = img_resolution\n", "        self.img_channels = img_channels\n\t        self.label_dim = label_dim\n\t        self.use_fp16 = use_fp16\n\t        self.C_1 = C_1\n\t        self.C_2 = C_2\n\t        self.M = M\n\t        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels*2, label_dim=label_dim, **model_kwargs)\n\t        u = torch.zeros(M + 1)\n\t        for j in range(M, 0, -1): # M, ..., 1\n\t            u[j - 1] = ((u[j] ** 2 + 1) / (self.alpha_bar(j - 1) / self.alpha_bar(j)).clip(min=C_1) - 1).sqrt()\n", "        self.register_buffer('u', u)\n\t        self.sigma_min = float(u[M - 1])\n\t        self.sigma_max = float(u[0])\n\t    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n\t        x = x.to(torch.float32)\n\t        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n\t        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n\t        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\t        c_skip = 1\n\t        c_out = -sigma\n", "        c_in = 1 / (sigma ** 2 + 1).sqrt()\n\t        c_noise = self.M - 1 - self.round_sigma(sigma, return_index=True).to(torch.float32)\n\t        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n\t        assert F_x.dtype == dtype\n\t        D_x = c_skip * x + c_out * F_x[:, :self.img_channels].to(torch.float32)\n\t        return D_x\n\t    def alpha_bar(self, j):\n\t        j = torch.as_tensor(j)\n\t        return (0.5 * np.pi * j / self.M / (self.C_2 + 1)).sin() ** 2\n\t    def round_sigma(self, sigma, return_index=False):\n", "        sigma = torch.as_tensor(sigma)\n\t        index = torch.cdist(sigma.to(self.u.device).to(torch.float32).reshape(1, -1, 1), self.u.reshape(1, -1, 1)).argmin(2)\n\t        result = index if return_index else self.u[index.flatten()].to(sigma.dtype)\n\t        return result.reshape(sigma.shape).to(sigma.device)\n\t#----------------------------------------------------------------------------\n\t# Improved preconditioning proposed in the paper \"Elucidating the Design\n\t# Space of Diffusion-Based Generative Models\" (EDM).\n\t@persistence.persistent_class\n\tclass EDMPrecond(torch.nn.Module):\n\t    def __init__(self,\n", "        img_resolution,                     # Image resolution.\n\t        img_channels,                       # Number of color channels.\n\t        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n\t        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n\t        sigma_min       = 0,                # Minimum supported noise level.\n\t        sigma_max       = float('inf'),     # Maximum supported noise level.\n\t        sigma_data      = 0.5,              # Expected standard deviation of the training data.\n\t        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n\t        **model_kwargs,                     # Keyword arguments for the underlying model.\n\t    ):\n", "        super().__init__()\n\t        self.img_resolution = img_resolution\n\t        self.img_channels = img_channels\n\t        self.label_dim = label_dim\n\t        self.use_fp16 = use_fp16\n\t        self.sigma_min = sigma_min\n\t        self.sigma_max = sigma_max\n\t        self.sigma_data = sigma_data\n\t        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n\t    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n", "        x = x.to(torch.float32)\n\t        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n\t        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n\t        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\t        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n\t        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n\t        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n\t        c_noise = sigma.log() / 4\n\t        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n\t        assert F_x.dtype == dtype\n", "        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n\t        return D_x\n\t    def round_sigma(self, sigma):\n\t        return torch.as_tensor(sigma)\n\t#----------------------------------------------------------------------------\n\t# Patch Version of EDMPrecond.\n\t@persistence.persistent_class\n\tclass Patch_EDMPrecond(torch.nn.Module):\n\t    def __init__(self,\n\t        img_resolution,                     # Image resolution.\n", "        img_channels,                       # Number of color channels.\n\t        out_channels    = None,\n\t        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n\t        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n\t        sigma_min       = 0,                # Minimum supported noise level.\n\t        sigma_max       = float('inf'),     # Maximum supported noise level.\n\t        sigma_data      = 0.5,              # Expected standard deviation of the training data.\n\t        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n\t        **model_kwargs,                     # Keyword arguments for the underlying model.\n\t    ):\n", "        super().__init__()\n\t        self.img_resolution = img_resolution\n\t        self.img_channels = img_channels\n\t        self.label_dim = label_dim\n\t        self.use_fp16 = use_fp16\n\t        self.sigma_min = sigma_min\n\t        self.sigma_max = sigma_max\n\t        self.sigma_data = sigma_data\n\t        self.out_channels = img_channels if out_channels is None else out_channels\n\t        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=self.out_channels, label_dim=label_dim, **model_kwargs)\n", "    def forward(self, x, sigma, x_pos=None, class_labels=None, force_fp32=False, **model_kwargs):\n\t        x = x.to(torch.float32)\n\t        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n\t        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n\t        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n\t        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n\t        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n\t        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n\t        c_noise = sigma.log() / 4\n\t        x_in = torch.cat([c_in * x, x_pos], dim=1) if x_pos is not None else c_in * x\n", "        F_x = self.model((x_in).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n\t        assert F_x.dtype == dtype\n\t        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n\t        return D_x\n\t    def round_sigma(self, sigma):\n\t        return torch.as_tensor(sigma)\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "training/loss.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Loss functions used in the paper\n\t\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\timport numpy as np\n\timport torch\n", "from torch_utils import persistence\n\t#----------------------------------------------------------------------------\n\t# Loss function corresponding to the variance preserving (VP) formulation\n\t# from the paper \"Score-Based Generative Modeling through Stochastic\n\t# Differential Equations\".\n\t@persistence.persistent_class\n\tclass VPLoss:\n\t    def __init__(self, beta_d=19.9, beta_min=0.1, epsilon_t=1e-5):\n\t        self.beta_d = beta_d\n\t        self.beta_min = beta_min\n", "        self.epsilon_t = epsilon_t\n\t    def __call__(self, net, images, labels, augment_pipe=None):\n\t        rnd_uniform = torch.rand([images.shape[0], 1, 1, 1], device=images.device)\n\t        sigma = self.sigma(1 + rnd_uniform * (self.epsilon_t - 1))\n\t        weight = 1 / sigma ** 2\n\t        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n\t        n = torch.randn_like(y) * sigma\n\t        D_yn = net(y + n, sigma, labels, augment_labels=augment_labels)\n\t        loss = weight * ((D_yn - y) ** 2)\n\t        return loss\n", "    def sigma(self, t):\n\t        t = torch.as_tensor(t)\n\t        return ((0.5 * self.beta_d * (t ** 2) + self.beta_min * t).exp() - 1).sqrt()\n\t#----------------------------------------------------------------------------\n\t# Loss function corresponding to the variance exploding (VE) formulation\n\t# from the paper \"Score-Based Generative Modeling through Stochastic\n\t# Differential Equations\".\n\t@persistence.persistent_class\n\tclass VELoss:\n\t    def __init__(self, sigma_min=0.02, sigma_max=100):\n", "        self.sigma_min = sigma_min\n\t        self.sigma_max = sigma_max\n\t    def __call__(self, net, images, labels, augment_pipe=None):\n\t        rnd_uniform = torch.rand([images.shape[0], 1, 1, 1], device=images.device)\n\t        sigma = self.sigma_min * ((self.sigma_max / self.sigma_min) ** rnd_uniform)\n\t        weight = 1 / sigma ** 2\n\t        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n\t        n = torch.randn_like(y) * sigma\n\t        D_yn = net(y + n, sigma, labels, augment_labels=augment_labels)\n\t        loss = weight * ((D_yn - y) ** 2)\n", "        return loss\n\t#----------------------------------------------------------------------------\n\t# Improved loss function proposed in the paper \"Elucidating the Design Space\n\t# of Diffusion-Based Generative Models\" (EDM).\n\t@persistence.persistent_class\n\tclass EDMLoss:\n\t    def __init__(self, P_mean=-1.2, P_std=1.2, sigma_data=0.5):\n\t        self.P_mean = P_mean\n\t        self.P_std = P_std\n\t        self.sigma_data = sigma_data\n", "    def __call__(self, net, images, labels=None, augment_pipe=None):\n\t        rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n\t        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n\t        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n\t        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n\t        n = torch.randn_like(y) * sigma\n\t        D_yn = net(y + n, sigma, labels, augment_labels=augment_labels)\n\t        loss = weight * ((D_yn - y) ** 2)\n\t        return loss\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "training/patch_loss.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Loss functions used in the paper\n\t\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n\timport numpy as np\n\timport torch\n", "from torch_utils import persistence\n\t#----------------------------------------------------------------------------\n\t# Loss function corresponding to the variance preserving (VP) formulation\n\t# from the paper \"Score-Based Generative Modeling through Stochastic\n\t# Differential Equations\".\n\t@persistence.persistent_class\n\tclass Patch_EDMLoss:\n\t    def __init__(self, P_mean=-1.2, P_std=1.2, sigma_data=0.5):\n\t        self.P_mean = P_mean\n\t        self.P_std = P_std\n", "        self.sigma_data = sigma_data\n\t    def pachify(self, images, patch_size, padding=None):\n\t        device = images.device\n\t        batch_size, resolution = images.size(0), images.size(2)\n\t        if padding is not None:\n\t            padded = torch.zeros((images.size(0), images.size(1), images.size(2) + padding * 2,\n\t                                  images.size(3) + padding * 2), dtype=images.dtype, device=device)\n\t            padded[:, :, padding:-padding, padding:-padding] = images\n\t        else:\n\t            padded = images\n", "        h, w = padded.size(2), padded.size(3)\n\t        th, tw = patch_size, patch_size\n\t        if w == tw and h == th:\n\t            i = torch.zeros((batch_size,), device=device).long()\n\t            j = torch.zeros((batch_size,), device=device).long()\n\t        else:\n\t            i = torch.randint(0, h - th + 1, (batch_size,), device=device)\n\t            j = torch.randint(0, w - tw + 1, (batch_size,), device=device)\n\t        rows = torch.arange(th, dtype=torch.long, device=device) + i[:, None]\n\t        columns = torch.arange(tw, dtype=torch.long, device=device) + j[:, None]\n", "        padded = padded.permute(1, 0, 2, 3)\n\t        padded = padded[:, torch.arange(batch_size)[:, None, None], rows[:, torch.arange(th)[:, None]],\n\t                 columns[:, None]]\n\t        padded = padded.permute(1, 0, 2, 3)\n\t        x_pos = torch.arange(tw, dtype=torch.long, device=device).unsqueeze(0).repeat(th, 1).unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\t        y_pos = torch.arange(th, dtype=torch.long, device=device).unsqueeze(1).repeat(1, tw).unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\t        x_pos = x_pos + j.view(-1, 1, 1, 1)\n\t        y_pos = y_pos + i.view(-1, 1, 1, 1)\n\t        x_pos = (x_pos / (resolution - 1) - 0.5) * 2.\n\t        y_pos = (y_pos / (resolution - 1) - 0.5) * 2.\n", "        images_pos = torch.cat((x_pos, y_pos), dim=1)\n\t        return padded, images_pos\n\t    def __call__(self, net, images, patch_size, resolution, labels=None, augment_pipe=None):\n\t        images, images_pos = self.pachify(images, patch_size)\n\t        rnd_normal = torch.randn([images.shape[0], 1, 1, 1], device=images.device)\n\t        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n\t        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n\t        y, augment_labels = augment_pipe(images) if augment_pipe is not None else (images, None)\n\t        n = torch.randn_like(y) * sigma\n\t        yn = y + n\n", "        D_yn = net(yn, sigma, x_pos=images_pos, class_labels=labels, augment_labels=augment_labels)\n\t        loss = weight * ((D_yn - y) ** 2)\n\t        return loss\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "training/dataset.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\timport os\n\timport numpy as np\n\timport zipfile\n", "import PIL.Image\n\timport json\n\timport torch\n\timport dnnlib\n\ttry:\n\t    import pyspng\n\texcept ImportError:\n\t    pyspng = None\n\t#----------------------------------------------------------------------------\n\t# Abstract base class for datasets.\n", "class Dataset(torch.utils.data.Dataset):\n\t    def __init__(self,\n\t        name,                   # Name of the dataset.\n\t        raw_shape,              # Shape of the raw image data (NCHW).\n\t        max_size    = None,     # Artificially limit the size of the dataset. None = no limit. Applied before xflip.\n\t        use_labels  = False,    # Enable conditioning labels? False = label dimension is zero.\n\t        xflip       = False,    # Artificially double the size of the dataset via x-flips. Applied after max_size.\n\t        random_seed = 0,        # Random seed to use when applying max_size.\n\t        cache       = False,    # Cache images in CPU memory?\n\t    ):\n", "        self._name = name\n\t        self._raw_shape = list(raw_shape)\n\t        self._use_labels = use_labels\n\t        self._cache = cache\n\t        self._cached_images = dict() # {raw_idx: np.ndarray, ...}\n\t        self._raw_labels = None\n\t        self._label_shape = None\n\t        # Apply max_size.\n\t        self._raw_idx = np.arange(self._raw_shape[0], dtype=np.int64)\n\t        if (max_size is not None) and (self._raw_idx.size > max_size):\n", "            np.random.RandomState(random_seed % (1 << 31)).shuffle(self._raw_idx)\n\t            self._raw_idx = np.sort(self._raw_idx[:max_size])\n\t        # Apply xflip.\n\t        self._xflip = np.zeros(self._raw_idx.size, dtype=np.uint8)\n\t        if xflip:\n\t            self._raw_idx = np.tile(self._raw_idx, 2)\n\t            self._xflip = np.concatenate([self._xflip, np.ones_like(self._xflip)])\n\t    def _get_raw_labels(self):\n\t        if self._raw_labels is None:\n\t            self._raw_labels = self._load_raw_labels() if self._use_labels else None\n", "            if self._raw_labels is None:\n\t                self._raw_labels = np.zeros([self._raw_shape[0], 0], dtype=np.float32)\n\t            assert isinstance(self._raw_labels, np.ndarray)\n\t            assert self._raw_labels.shape[0] == self._raw_shape[0]\n\t            assert self._raw_labels.dtype in [np.float32, np.int64]\n\t            if self._raw_labels.dtype == np.int64:\n\t                assert self._raw_labels.ndim == 1\n\t                assert np.all(self._raw_labels >= 0)\n\t        return self._raw_labels\n\t    def close(self): # to be overridden by subclass\n", "        pass\n\t    def _load_raw_image(self, raw_idx): # to be overridden by subclass\n\t        raise NotImplementedError\n\t    def _load_raw_labels(self): # to be overridden by subclass\n\t        raise NotImplementedError\n\t    def __getstate__(self):\n\t        return dict(self.__dict__, _raw_labels=None)\n\t    def __del__(self):\n\t        try:\n\t            self.close()\n", "        except:\n\t            pass\n\t    def __len__(self):\n\t        return self._raw_idx.size\n\t    def __getitem__(self, idx):\n\t        raw_idx = self._raw_idx[idx]\n\t        image = self._cached_images.get(raw_idx, None)\n\t        if image is None:\n\t            image = self._load_raw_image(raw_idx)\n\t            if self._cache:\n", "                self._cached_images[raw_idx] = image\n\t        assert isinstance(image, np.ndarray)\n\t        assert list(image.shape) == self.image_shape\n\t        assert image.dtype == np.uint8\n\t        if self._xflip[idx]:\n\t            assert image.ndim == 3 # CHW\n\t            image = image[:, :, ::-1]\n\t        return image.copy(), self.get_label(idx)\n\t    def get_label(self, idx):\n\t        label = self._get_raw_labels()[self._raw_idx[idx]]\n", "        if label.dtype == np.int64:\n\t            onehot = np.zeros(self.label_shape, dtype=np.float32)\n\t            onehot[label] = 1\n\t            label = onehot\n\t        return label.copy()\n\t    def get_details(self, idx):\n\t        d = dnnlib.EasyDict()\n\t        d.raw_idx = int(self._raw_idx[idx])\n\t        d.xflip = (int(self._xflip[idx]) != 0)\n\t        d.raw_label = self._get_raw_labels()[d.raw_idx].copy()\n", "        return d\n\t    @property\n\t    def name(self):\n\t        return self._name\n\t    @property\n\t    def image_shape(self):\n\t        return list(self._raw_shape[1:])\n\t    @property\n\t    def num_channels(self):\n\t        assert len(self.image_shape) == 3 # CHW\n", "        return self.image_shape[0]\n\t    @property\n\t    def resolution(self):\n\t        assert len(self.image_shape) == 3 # CHW\n\t        assert self.image_shape[1] == self.image_shape[2]\n\t        return self.image_shape[1]\n\t    @property\n\t    def label_shape(self):\n\t        if self._label_shape is None:\n\t            raw_labels = self._get_raw_labels()\n", "            if raw_labels.dtype == np.int64:\n\t                self._label_shape = [int(np.max(raw_labels)) + 1]\n\t            else:\n\t                self._label_shape = raw_labels.shape[1:]\n\t        return list(self._label_shape)\n\t    @property\n\t    def label_dim(self):\n\t        assert len(self.label_shape) == 1\n\t        return self.label_shape[0]\n\t    @property\n", "    def has_labels(self):\n\t        return any(x != 0 for x in self.label_shape)\n\t    @property\n\t    def has_onehot_labels(self):\n\t        return self._get_raw_labels().dtype == np.int64\n\t#----------------------------------------------------------------------------\n\t# Dataset subclass that loads images recursively from the specified directory\n\t# or ZIP file.\n\tclass ImageFolderDataset(Dataset):\n\t    def __init__(self,\n", "        path,                   # Path to directory or zip.\n\t        resolution      = None, # Ensure specific resolution, None = highest available.\n\t        use_pyspng      = True, # Use pyspng if available?\n\t        **super_kwargs,         # Additional arguments for the Dataset base class.\n\t    ):\n\t        self._path = path\n\t        self._use_pyspng = use_pyspng\n\t        self._zipfile = None\n\t        if os.path.isdir(self._path):\n\t            self._type = 'dir'\n", "            self._all_fnames = {os.path.relpath(os.path.join(root, fname), start=self._path) for root, _dirs, files in os.walk(self._path) for fname in files}\n\t        elif self._file_ext(self._path) == '.zip':\n\t            self._type = 'zip'\n\t            self._all_fnames = set(self._get_zipfile().namelist())\n\t        else:\n\t            raise IOError('Path must point to a directory or zip')\n\t        PIL.Image.init()\n\t        self._image_fnames = sorted(fname for fname in self._all_fnames if self._file_ext(fname) in PIL.Image.EXTENSION)\n\t        if len(self._image_fnames) == 0:\n\t            raise IOError('No image files found in the specified path')\n", "        name = os.path.splitext(os.path.basename(self._path))[0]\n\t        raw_shape = [len(self._image_fnames)] + list(self._load_raw_image(0).shape)\n\t        if resolution is not None and (raw_shape[2] != resolution or raw_shape[3] != resolution):\n\t            raise IOError('Image files do not match the specified resolution')\n\t        super().__init__(name=name, raw_shape=raw_shape, **super_kwargs)\n\t    @staticmethod\n\t    def _file_ext(fname):\n\t        return os.path.splitext(fname)[1].lower()\n\t    def _get_zipfile(self):\n\t        assert self._type == 'zip'\n", "        if self._zipfile is None:\n\t            self._zipfile = zipfile.ZipFile(self._path)\n\t        return self._zipfile\n\t    def _open_file(self, fname):\n\t        if self._type == 'dir':\n\t            return open(os.path.join(self._path, fname), 'rb')\n\t        if self._type == 'zip':\n\t            return self._get_zipfile().open(fname, 'r')\n\t        return None\n\t    def close(self):\n", "        try:\n\t            if self._zipfile is not None:\n\t                self._zipfile.close()\n\t        finally:\n\t            self._zipfile = None\n\t    def __getstate__(self):\n\t        return dict(super().__getstate__(), _zipfile=None)\n\t    def _load_raw_image(self, raw_idx):\n\t        fname = self._image_fnames[raw_idx]\n\t        with self._open_file(fname) as f:\n", "            if self._use_pyspng and pyspng is not None and self._file_ext(fname) == '.png':\n\t                image = pyspng.load(f.read())\n\t            else:\n\t                image = np.array(PIL.Image.open(f))\n\t        if image.ndim == 2:\n\t            image = image[:, :, np.newaxis] # HW => HWC\n\t        image = image.transpose(2, 0, 1) # HWC => CHW\n\t        return image\n\t    def _load_raw_labels(self):\n\t        fname = 'dataset.json'\n", "        if fname not in self._all_fnames:\n\t            return None\n\t        with self._open_file(fname) as f:\n\t            labels = json.load(f)['labels']\n\t        if labels is None:\n\t            return None\n\t        labels = dict(labels)\n\t        labels = [labels[fname.replace('\\\\', '/')] for fname in self._image_fnames]\n\t        labels = np.array(labels)\n\t        labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])\n", "        return labels\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "training/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t# empty\n"]}
{"filename": "training/training_loop.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Main training loop.\"\"\"\n\timport os\n\timport time\n\timport copy\n", "import json\n\timport pickle\n\timport psutil\n\timport numpy as np\n\timport torch\n\timport dnnlib\n\tfrom torch_utils import distributed as dist\n\tfrom torch_utils import training_stats\n\tfrom torch_utils import misc\n\tfrom diffusers import AutoencoderKL\n", "def set_requires_grad(model, value):\n\t    for param in model.parameters():\n\t        param.requires_grad = value\n\t#----------------------------------------------------------------------------\n\tdef training_loop(\n\t    run_dir             = '.',      # Output directory.\n\t    dataset_kwargs      = {},       # Options for training set.\n\t    data_loader_kwargs  = {},       # Options for torch.utils.data.DataLoader.\n\t    network_kwargs      = {},       # Options for model and preconditioning.\n\t    loss_kwargs         = {},       # Options for loss function.\n", "    optimizer_kwargs    = {},       # Options for optimizer.\n\t    augment_kwargs      = None,     # Options for augmentation pipeline, None = disable.\n\t    seed                = 0,        # Global random seed.\n\t    batch_size          = 512,      # Total batch size for one training iteration.\n\t    batch_gpu           = None,     # Limit batch size per GPU, None = no limit.\n\t    total_kimg          = 200000,   # Training duration, measured in thousands of training images.\n\t    ema_halflife_kimg   = 500,      # Half-life of the exponential moving average (EMA) of model weights.\n\t    ema_rampup_ratio    = 0.05,     # EMA ramp-up coefficient, None = no rampup.\n\t    lr_rampup_kimg      = 10000,    # Learning rate ramp-up duration.\n\t    loss_scaling        = 1,        # Loss scaling factor for reducing FP16 under/overflows.\n", "    kimg_per_tick       = 50,       # Interval of progress prints.\n\t    snapshot_ticks      = 50,       # How often to save network snapshots, None = disable.\n\t    state_dump_ticks    = 500,      # How often to dump training state, None = disable.\n\t    resume_pkl          = None,     # Start from the given network snapshot, None = random initialization.\n\t    resume_state_dump   = None,     # Start from the given training state, None = reset training state.\n\t    resume_kimg         = 0,        # Start from the given training progress.\n\t    cudnn_benchmark     = True,     # Enable torch.backends.cudnn.benchmark?\n\t    real_p              = 0.5,\n\t    train_on_latents    = False,\n\t    progressive         = False,\n", "    device              = torch.device('cuda'),\n\t):\n\t    # Initialize.\n\t    start_time = time.time()\n\t    np.random.seed((seed * dist.get_world_size() + dist.get_rank()) % (1 << 31))\n\t    torch.manual_seed(np.random.randint(1 << 31))\n\t    torch.backends.cudnn.benchmark = cudnn_benchmark\n\t    torch.backends.cudnn.allow_tf32 = False\n\t    torch.backends.cuda.matmul.allow_tf32 = False\n\t    torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n", "    # Select batch size per GPU.\n\t    batch_gpu_total = batch_size // dist.get_world_size()\n\t    if batch_gpu is None or batch_gpu > batch_gpu_total:\n\t        batch_gpu = batch_gpu_total\n\t    num_accumulation_rounds = batch_gpu_total // batch_gpu\n\t    assert batch_size == batch_gpu * num_accumulation_rounds * dist.get_world_size()\n\t    # Load dataset.\n\t    dist.print0('Loading dataset...')\n\t    dataset_obj = dnnlib.util.construct_class_by_name(**dataset_kwargs) # subclass of training.dataset.Dataset\n\t    dataset_sampler = misc.InfiniteSampler(dataset=dataset_obj, rank=dist.get_rank(), num_replicas=dist.get_world_size(), seed=seed)\n", "    dataset_iterator = iter(torch.utils.data.DataLoader(dataset=dataset_obj, sampler=dataset_sampler, batch_size=batch_gpu, **data_loader_kwargs))\n\t    img_resolution, img_channels = dataset_obj.resolution, dataset_obj.num_channels\n\t    if train_on_latents:\n\t        # img_vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"vae\").to(device)\n\t        img_vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device)\n\t        img_vae.eval()\n\t        set_requires_grad(img_vae, False)\n\t        latent_scale_factor = 0.18215\n\t        img_resolution, img_channels = dataset_obj.resolution // 8, 4\n\t    else:\n", "        img_vae = None\n\t    # Construct network.\n\t    dist.print0('Constructing network...')\n\t    net_input_channels = img_channels + 2\n\t    interface_kwargs = dict(img_resolution=img_resolution,\n\t                            img_channels=net_input_channels,\n\t                            out_channels=4 if train_on_latents else dataset_obj.num_channels,\n\t                            label_dim=dataset_obj.label_dim)\n\t    net = dnnlib.util.construct_class_by_name(**network_kwargs, **interface_kwargs) # subclass of torch.nn.Module\n\t    net.train().requires_grad_(True).to(device)\n", "    if dist.get_rank() == 0:\n\t        with torch.no_grad():\n\t            images = torch.zeros([batch_gpu, img_channels, net.img_resolution, net.img_resolution], device=device)\n\t            sigma = torch.ones([batch_gpu], device=device)\n\t            x_pos = torch.zeros([batch_gpu, 2, net.img_resolution, net.img_resolution], device=device)\n\t            labels = torch.zeros([batch_gpu, net.label_dim], device=device)\n\t            misc.print_module_summary(net, [images, sigma, x_pos, labels], max_nesting=2)\n\t    # Setup optimizer.\n\t    dist.print0('Setting up optimizer...')\n\t    loss_fn = dnnlib.util.construct_class_by_name(**loss_kwargs) # training.loss.(VP|VE|EDM)Loss\n", "    optimizer = dnnlib.util.construct_class_by_name(params=net.parameters(), **optimizer_kwargs) # subclass of torch.optim.Optimizer\n\t    augment_pipe = dnnlib.util.construct_class_by_name(**augment_kwargs) if augment_kwargs is not None else None # training.augment.AugmentPipe\n\t    ddp = torch.nn.parallel.DistributedDataParallel(net, device_ids=[device], broadcast_buffers=False)\n\t    ema = copy.deepcopy(net).eval().requires_grad_(False)\n\t    # Resume training from previous snapshot.\n\t    if resume_pkl is not None:\n\t        dist.print0(f'Loading network weights from \"{resume_pkl}\"...')\n\t        if dist.get_rank() != 0:\n\t            torch.distributed.barrier() # rank 0 goes first\n\t        with dnnlib.util.open_url(resume_pkl, verbose=(dist.get_rank() == 0)) as f:\n", "            data = pickle.load(f)\n\t        if dist.get_rank() == 0:\n\t            torch.distributed.barrier() # other ranks follow\n\t        misc.copy_params_and_buffers(src_module=data['ema'], dst_module=net, require_all=False)\n\t        misc.copy_params_and_buffers(src_module=data['ema'], dst_module=ema, require_all=False)\n\t        del data # conserve memory\n\t    if resume_state_dump:\n\t        dist.print0(f'Loading training state from \"{resume_state_dump}\"...')\n\t        data = torch.load(resume_state_dump, map_location=torch.device('cpu'))\n\t        misc.copy_params_and_buffers(src_module=data['net'], dst_module=net, require_all=True)\n", "        optimizer.load_state_dict(data['optimizer_state'])\n\t        del data # conserve memory\n\t    # Train.\n\t    dist.print0(f'Training for {total_kimg} kimg...')\n\t    dist.print0()\n\t    cur_nimg = resume_kimg * 1000\n\t    cur_tick = 0\n\t    tick_start_nimg = cur_nimg\n\t    tick_start_time = time.time()\n\t    maintenance_time = tick_start_time - start_time\n", "    dist.update_progress(cur_nimg // 1000, total_kimg)\n\t    stats_jsonl = None\n\t    batch_mul_dict = {512: 1, 256: 2, 128: 4, 64: 16, 32: 32, 16: 64}\n\t    if train_on_latents:\n\t        p_list = np.array([(1 - real_p), real_p])\n\t        patch_list = np.array([img_resolution // 2, img_resolution])\n\t        batch_mul_avg = np.sum(p_list * np.array([2, 1]))\n\t    else:\n\t        p_list = np.array([(1-real_p)*2/5, (1-real_p)*3/5, real_p])\n\t        patch_list = np.array([img_resolution//4, img_resolution//2, img_resolution])\n", "        batch_mul_avg = np.sum(np.array(p_list) * np.array([4, 2, 1]))  # 2\n\t    while True:\n\t        # Accumulate gradients.\n\t        optimizer.zero_grad(set_to_none=True)\n\t        for round_idx in range(num_accumulation_rounds):\n\t            with misc.ddp_sync(ddp, (round_idx == num_accumulation_rounds - 1)):\n\t                if progressive:\n\t                    p_cumsum = p_list.cumsum()\n\t                    p_cumsum[-1] = 10.\n\t                    prog_mask = (cur_nimg // 1000 / total_kimg) <= p_cumsum\n", "                    patch_size = int(patch_list[prog_mask][0])\n\t                    batch_mul_avg = batch_mul_dict[patch_size] // batch_mul_dict[img_resolution]\n\t                else:\n\t                    patch_size = int(np.random.choice(patch_list, p=p_list))\n\t                batch_mul = batch_mul_dict[patch_size] // batch_mul_dict[img_resolution]\n\t                images, labels = [], []\n\t                for _ in range(batch_mul):\n\t                    images_, labels_ = next(dataset_iterator)\n\t                    images.append(images_), labels.append(labels_)\n\t                images, labels = torch.cat(images, dim=0), torch.cat(labels, dim=0)\n", "                del images_, labels_\n\t                images = images.to(device).to(torch.float32) / 127.5 - 1\n\t                if train_on_latents:\n\t                    with torch.no_grad():\n\t                        images = img_vae.encode(images)['latent_dist'].sample()\n\t                        images = latent_scale_factor * images\n\t                labels = labels.to(device)\n\t                loss = loss_fn(net=ddp, images=images, patch_size=patch_size, resolution=img_resolution,\n\t                               labels=labels, augment_pipe=augment_pipe)\n\t                training_stats.report('Loss/loss', loss)\n", "                loss.sum().mul(loss_scaling / batch_gpu_total / batch_mul).backward()\n\t                # loss.mean().mul(loss_scaling / batch_mul).backward()\n\t        # Update weights.\n\t        for g in optimizer.param_groups:\n\t            g['lr'] = optimizer_kwargs['lr'] * min(cur_nimg / max(lr_rampup_kimg * 1000, 1e-8), 1)\n\t        for param in net.parameters():\n\t            if param.grad is not None:\n\t                torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)\n\t        optimizer.step()\n\t        # Update EMA.\n", "        ema_halflife_nimg = ema_halflife_kimg * 1000\n\t        if ema_rampup_ratio is not None:\n\t            ema_halflife_nimg = min(ema_halflife_nimg, cur_nimg * ema_rampup_ratio)\n\t        ema_beta = 0.5 ** (batch_size * batch_mul_avg / max(ema_halflife_nimg, 1e-8))\n\t        for p_ema, p_net in zip(ema.parameters(), net.parameters()):\n\t            p_ema.copy_(p_net.detach().lerp(p_ema, ema_beta))\n\t        # Perform maintenance tasks once per tick.\n\t        cur_nimg += int(batch_size * batch_mul_avg)\n\t        done = (cur_nimg >= total_kimg * 1000)\n\t        if (not done) and (cur_tick != 0) and (cur_nimg < tick_start_nimg + kimg_per_tick * 1000):\n", "            continue\n\t        # Print status line, accumulating the same information in training_stats.\n\t        tick_end_time = time.time()\n\t        fields = []\n\t        fields += [f\"tick {training_stats.report0('Progress/tick', cur_tick):<5d}\"]\n\t        fields += [f\"kimg {training_stats.report0('Progress/kimg', cur_nimg / 1e3):<9.1f}\"]\n\t        fields += [f\"loss {loss.mean().item():<9.3f}\"]\n\t        fields += [f\"time {dnnlib.util.format_time(training_stats.report0('Timing/total_sec', tick_end_time - start_time)):<12s}\"]\n\t        fields += [f\"sec/tick {training_stats.report0('Timing/sec_per_tick', tick_end_time - tick_start_time):<7.1f}\"]\n\t        fields += [f\"sec/kimg {training_stats.report0('Timing/sec_per_kimg', (tick_end_time - tick_start_time) / (cur_nimg - tick_start_nimg) * 1e3):<7.2f}\"]\n", "        fields += [f\"maintenance {training_stats.report0('Timing/maintenance_sec', maintenance_time):<6.1f}\"]\n\t        fields += [f\"cpumem {training_stats.report0('Resources/cpu_mem_gb', psutil.Process(os.getpid()).memory_info().rss / 2**30):<6.2f}\"]\n\t        fields += [f\"gpumem {training_stats.report0('Resources/peak_gpu_mem_gb', torch.cuda.max_memory_allocated(device) / 2**30):<6.2f}\"]\n\t        fields += [f\"reserved {training_stats.report0('Resources/peak_gpu_mem_reserved_gb', torch.cuda.max_memory_reserved(device) / 2**30):<6.2f}\"]\n\t        torch.cuda.reset_peak_memory_stats()\n\t        dist.print0(' '.join(fields))\n\t        # Check for abort.\n\t        if (not done) and dist.should_stop():\n\t            done = True\n\t            dist.print0()\n", "            dist.print0('Aborting...')\n\t        # Save network snapshot.\n\t        if (snapshot_ticks is not None) and (done or cur_tick % snapshot_ticks == 0):\n\t            data = dict(ema=ema, loss_fn=loss_fn, augment_pipe=augment_pipe, dataset_kwargs=dict(dataset_kwargs))\n\t            for key, value in data.items():\n\t                if isinstance(value, torch.nn.Module):\n\t                    value = copy.deepcopy(value).eval().requires_grad_(False)\n\t                    misc.check_ddp_consistency(value)\n\t                    data[key] = value.cpu()\n\t                del value # conserve memory\n", "            if dist.get_rank() == 0:\n\t                with open(os.path.join(run_dir, f'network-snapshot-{cur_nimg//1000:06d}.pkl'), 'wb') as f:\n\t                    pickle.dump(data, f)\n\t            del data # conserve memory\n\t        # Save full dump of the training state.\n\t        if (state_dump_ticks is not None) and (done or cur_tick % state_dump_ticks == 0) and cur_tick != 0 and dist.get_rank() == 0:\n\t            torch.save(dict(net=net, optimizer_state=optimizer.state_dict()), os.path.join(run_dir, f'training-state-{cur_nimg//1000:06d}.pt'))\n\t        # Update logs.\n\t        training_stats.default_collector.update()\n\t        if dist.get_rank() == 0:\n", "            if stats_jsonl is None:\n\t                stats_jsonl = open(os.path.join(run_dir, 'stats.jsonl'), 'at')\n\t            stats_jsonl.write(json.dumps(dict(training_stats.default_collector.as_dict(), timestamp=time.time())) + '\\n')\n\t            stats_jsonl.flush()\n\t        dist.update_progress(cur_nimg // 1000, total_kimg)\n\t        # Update state.\n\t        cur_tick += 1\n\t        tick_start_nimg = cur_nimg\n\t        tick_start_time = time.time()\n\t        maintenance_time = tick_start_time - tick_end_time\n", "        if done:\n\t            break\n\t    # Done.\n\t    dist.print0()\n\t    dist.print0('Exiting...')\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "training/pos_embedding.py", "chunked_list": ["import torch\n\tfrom torch_utils import persistence\n\t@persistence.persistent_class\n\tclass Pos_Embedding(torch.nn.Module):\n\t    def __init__(self, num_freqs=16, input_dim=2, log_sampling=True):\n\t        super().__init__()\n\t        self.num_freqs = num_freqs\n\t        self.max_freq = num_freqs - 1\n\t        self.input_dim = input_dim\n\t        self.log_sampling = log_sampling\n", "        if self.log_sampling:\n\t            self.freq_bands = 2. ** torch.linspace(0., self.max_freq, steps=self.num_freqs)\n\t        else:\n\t            self.freq_bands = torch.linspace(2. ** 0., 2. ** self.max_freq, steps=self.num_freqs)\n\t        # embed_fns = []\n\t        # out_dim = 0\n\t        # d = input_dim\n\t        # for freq in freq_bands:\n\t        #     for p_fn in self.periodic_fns:\n\t        #         embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))\n", "        #         out_dim += d\n\t        #\n\t        # self.embed_fns = embed_fns\n\t        self.out_dim = int(input_dim * self.num_freqs * 2) # 2 is for [sin, cos] function list\n\t    def forward(self, x):\n\t        # concatenate in the channel dim\n\t        # assert x.shape[0] == self.input_dim\n\t        output = []\n\t        for freq in self.freq_bands:\n\t            for p_fn in [torch.sin, torch.cos]:\n", "                output.append(p_fn(x * freq))\n\t        return torch.cat(output, 1)"]}
{"filename": "training/augment.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Augmentation pipeline used in the paper\n\t\"Elucidating the Design Space of Diffusion-Based Generative Models\".\n\tBuilt around the same concepts that were originally proposed in the paper\n\t\"Training Generative Adversarial Networks with Limited Data\".\"\"\"\n", "import numpy as np\n\timport torch\n\tfrom torch_utils import persistence\n\tfrom torch_utils import misc\n\t#----------------------------------------------------------------------------\n\t# Coefficients of various wavelet decomposition low-pass filters.\n\twavelets = {\n\t    'haar': [0.7071067811865476, 0.7071067811865476],\n\t    'db1':  [0.7071067811865476, 0.7071067811865476],\n\t    'db2':  [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n", "    'db3':  [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n\t    'db4':  [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523],\n\t    'db5':  [0.003335725285001549, -0.012580751999015526, -0.006241490213011705, 0.07757149384006515, -0.03224486958502952, -0.24229488706619015, 0.13842814590110342, 0.7243085284385744, 0.6038292697974729, 0.160102397974125],\n\t    'db6':  [-0.00107730108499558, 0.004777257511010651, 0.0005538422009938016, -0.031582039318031156, 0.02752286553001629, 0.09750160558707936, -0.12976686756709563, -0.22626469396516913, 0.3152503517092432, 0.7511339080215775, 0.4946238903983854, 0.11154074335008017],\n\t    'db7':  [0.0003537138000010399, -0.0018016407039998328, 0.00042957797300470274, 0.012550998556013784, -0.01657454163101562, -0.03802993693503463, 0.0806126091510659, 0.07130921926705004, -0.22403618499416572, -0.14390600392910627, 0.4697822874053586, 0.7291320908465551, 0.39653931948230575, 0.07785205408506236],\n\t    'db8':  [-0.00011747678400228192, 0.0006754494059985568, -0.0003917403729959771, -0.00487035299301066, 0.008746094047015655, 0.013981027917015516, -0.04408825393106472, -0.01736930100202211, 0.128747426620186, 0.00047248457399797254, -0.2840155429624281, -0.015829105256023893, 0.5853546836548691, 0.6756307362980128, 0.3128715909144659, 0.05441584224308161],\n\t    'sym2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n\t    'sym3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n\t    'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427],\n\t    'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728],\n", "    'sym6': [0.015404109327027373, 0.0034907120842174702, -0.11799011114819057, -0.048311742585633, 0.4910559419267466, 0.787641141030194, 0.3379294217276218, -0.07263752278646252, -0.021060292512300564, 0.04472490177066578, 0.0017677118642428036, -0.007800708325034148],\n\t    'sym7': [0.002681814568257878, -0.0010473848886829163, -0.01263630340325193, 0.03051551316596357, 0.0678926935013727, -0.049552834937127255, 0.017441255086855827, 0.5361019170917628, 0.767764317003164, 0.2886296317515146, -0.14004724044296152, -0.10780823770381774, 0.004010244871533663, 0.010268176708511255],\n\t    'sym8': [-0.0033824159510061256, -0.0005421323317911481, 0.03169508781149298, 0.007607487324917605, -0.1432942383508097, -0.061273359067658524, 0.4813596512583722, 0.7771857517005235, 0.3644418948353314, -0.05194583810770904, -0.027219029917056003, 0.049137179673607506, 0.003808752013890615, -0.01495225833704823, -0.0003029205147213668, 0.0018899503327594609],\n\t}\n\t#----------------------------------------------------------------------------\n\t# Helpers for constructing transformation matrices.\n\tdef matrix(*rows, device=None):\n\t    assert all(len(row) == len(rows[0]) for row in rows)\n\t    elems = [x for row in rows for x in row]\n\t    ref = [x for x in elems if isinstance(x, torch.Tensor)]\n", "    if len(ref) == 0:\n\t        return misc.constant(np.asarray(rows), device=device)\n\t    assert device is None or device == ref[0].device\n\t    elems = [x if isinstance(x, torch.Tensor) else misc.constant(x, shape=ref[0].shape, device=ref[0].device) for x in elems]\n\t    return torch.stack(elems, dim=-1).reshape(ref[0].shape + (len(rows), -1))\n\tdef translate2d(tx, ty, **kwargs):\n\t    return matrix(\n\t        [1, 0, tx],\n\t        [0, 1, ty],\n\t        [0, 0, 1],\n", "        **kwargs)\n\tdef translate3d(tx, ty, tz, **kwargs):\n\t    return matrix(\n\t        [1, 0, 0, tx],\n\t        [0, 1, 0, ty],\n\t        [0, 0, 1, tz],\n\t        [0, 0, 0, 1],\n\t        **kwargs)\n\tdef scale2d(sx, sy, **kwargs):\n\t    return matrix(\n", "        [sx, 0,  0],\n\t        [0,  sy, 0],\n\t        [0,  0,  1],\n\t        **kwargs)\n\tdef scale3d(sx, sy, sz, **kwargs):\n\t    return matrix(\n\t        [sx, 0,  0,  0],\n\t        [0,  sy, 0,  0],\n\t        [0,  0,  sz, 0],\n\t        [0,  0,  0,  1],\n", "        **kwargs)\n\tdef rotate2d(theta, **kwargs):\n\t    return matrix(\n\t        [torch.cos(theta), torch.sin(-theta), 0],\n\t        [torch.sin(theta), torch.cos(theta),  0],\n\t        [0,                0,                 1],\n\t        **kwargs)\n\tdef rotate3d(v, theta, **kwargs):\n\t    vx = v[..., 0]; vy = v[..., 1]; vz = v[..., 2]\n\t    s = torch.sin(theta); c = torch.cos(theta); cc = 1 - c\n", "    return matrix(\n\t        [vx*vx*cc+c,    vx*vy*cc-vz*s, vx*vz*cc+vy*s, 0],\n\t        [vy*vx*cc+vz*s, vy*vy*cc+c,    vy*vz*cc-vx*s, 0],\n\t        [vz*vx*cc-vy*s, vz*vy*cc+vx*s, vz*vz*cc+c,    0],\n\t        [0,             0,             0,             1],\n\t        **kwargs)\n\tdef translate2d_inv(tx, ty, **kwargs):\n\t    return translate2d(-tx, -ty, **kwargs)\n\tdef scale2d_inv(sx, sy, **kwargs):\n\t    return scale2d(1 / sx, 1 / sy, **kwargs)\n", "def rotate2d_inv(theta, **kwargs):\n\t    return rotate2d(-theta, **kwargs)\n\t#----------------------------------------------------------------------------\n\t# Augmentation pipeline main class.\n\t# All augmentations are disabled by default; individual augmentations can\n\t# be enabled by setting their probability multipliers to 1.\n\t@persistence.persistent_class\n\tclass AugmentPipe:\n\t    def __init__(self, p=1,\n\t        xflip=0, yflip=0, rotate_int=0, translate_int=0, translate_int_max=0.125,\n", "        scale=0, rotate_frac=0, aniso=0, translate_frac=0, scale_std=0.2, rotate_frac_max=1, aniso_std=0.2, aniso_rotate_prob=0.5, translate_frac_std=0.125,\n\t        brightness=0, contrast=0, lumaflip=0, hue=0, saturation=0, brightness_std=0.2, contrast_std=0.5, hue_max=1, saturation_std=1,\n\t    ):\n\t        super().__init__()\n\t        self.p                  = float(p)                  # Overall multiplier for augmentation probability.\n\t        # Pixel blitting.\n\t        self.xflip              = float(xflip)              # Probability multiplier for x-flip.\n\t        self.yflip              = float(yflip)              # Probability multiplier for y-flip.\n\t        self.rotate_int         = float(rotate_int)         # Probability multiplier for integer rotation.\n\t        self.translate_int      = float(translate_int)      # Probability multiplier for integer translation.\n", "        self.translate_int_max  = float(translate_int_max)  # Range of integer translation, relative to image dimensions.\n\t        # Geometric transformations.\n\t        self.scale              = float(scale)              # Probability multiplier for isotropic scaling.\n\t        self.rotate_frac        = float(rotate_frac)        # Probability multiplier for fractional rotation.\n\t        self.aniso              = float(aniso)              # Probability multiplier for anisotropic scaling.\n\t        self.translate_frac     = float(translate_frac)     # Probability multiplier for fractional translation.\n\t        self.scale_std          = float(scale_std)          # Log2 standard deviation of isotropic scaling.\n\t        self.rotate_frac_max    = float(rotate_frac_max)    # Range of fractional rotation, 1 = full circle.\n\t        self.aniso_std          = float(aniso_std)          # Log2 standard deviation of anisotropic scaling.\n\t        self.aniso_rotate_prob  = float(aniso_rotate_prob)  # Probability of doing anisotropic scaling w.r.t. rotated coordinate frame.\n", "        self.translate_frac_std = float(translate_frac_std) # Standard deviation of frational translation, relative to image dimensions.\n\t        # Color transformations.\n\t        self.brightness         = float(brightness)         # Probability multiplier for brightness.\n\t        self.contrast           = float(contrast)           # Probability multiplier for contrast.\n\t        self.lumaflip           = float(lumaflip)           # Probability multiplier for luma flip.\n\t        self.hue                = float(hue)                # Probability multiplier for hue rotation.\n\t        self.saturation         = float(saturation)         # Probability multiplier for saturation.\n\t        self.brightness_std     = float(brightness_std)     # Standard deviation of brightness.\n\t        self.contrast_std       = float(contrast_std)       # Log2 standard deviation of contrast.\n\t        self.hue_max            = float(hue_max)            # Range of hue rotation, 1 = full circle.\n", "        self.saturation_std     = float(saturation_std)     # Log2 standard deviation of saturation.\n\t    def __call__(self, images):\n\t        N, C, H, W = images.shape\n\t        device = images.device\n\t        labels = [torch.zeros([images.shape[0], 0], device=device)]\n\t        # ---------------\n\t        # Pixel blitting.\n\t        # ---------------\n\t        if self.xflip > 0:\n\t            w = torch.randint(2, [N, 1, 1, 1], device=device)\n", "            w = torch.where(torch.rand([N, 1, 1, 1], device=device) < self.xflip * self.p, w, torch.zeros_like(w))\n\t            images = torch.where(w == 1, images.flip(3), images)\n\t            labels += [w]\n\t        if self.yflip > 0:\n\t            w = torch.randint(2, [N, 1, 1, 1], device=device)\n\t            w = torch.where(torch.rand([N, 1, 1, 1], device=device) < self.yflip * self.p, w, torch.zeros_like(w))\n\t            images = torch.where(w == 1, images.flip(2), images)\n\t            labels += [w]\n\t        if self.rotate_int > 0:\n\t            w = torch.randint(4, [N, 1, 1, 1], device=device)\n", "            w = torch.where(torch.rand([N, 1, 1, 1], device=device) < self.rotate_int * self.p, w, torch.zeros_like(w))\n\t            images = torch.where((w == 1) | (w == 2), images.flip(3), images)\n\t            images = torch.where((w == 2) | (w == 3), images.flip(2), images)\n\t            images = torch.where((w == 1) | (w == 3), images.transpose(2, 3), images)\n\t            labels += [(w == 1) | (w == 2), (w == 2) | (w == 3)]\n\t        if self.translate_int > 0:\n\t            w = torch.rand([2, N, 1, 1, 1], device=device) * 2 - 1\n\t            w = torch.where(torch.rand([1, N, 1, 1, 1], device=device) < self.translate_int * self.p, w, torch.zeros_like(w))\n\t            tx = w[0].mul(W * self.translate_int_max).round().to(torch.int64)\n\t            ty = w[1].mul(H * self.translate_int_max).round().to(torch.int64)\n", "            b, c, y, x = torch.meshgrid(*(torch.arange(x, device=device) for x in images.shape), indexing='ij')\n\t            x = W - 1 - (W - 1 - (x - tx) % (W * 2 - 2)).abs()\n\t            y = H - 1 - (H - 1 - (y + ty) % (H * 2 - 2)).abs()\n\t            images = images.flatten()[(((b * C) + c) * H + y) * W + x]\n\t            labels += [tx.div(W * self.translate_int_max), ty.div(H * self.translate_int_max)]\n\t        # ------------------------------------------------\n\t        # Select parameters for geometric transformations.\n\t        # ------------------------------------------------\n\t        I_3 = torch.eye(3, device=device)\n\t        G_inv = I_3\n", "        if self.scale > 0:\n\t            w = torch.randn([N], device=device)\n\t            w = torch.where(torch.rand([N], device=device) < self.scale * self.p, w, torch.zeros_like(w))\n\t            s = w.mul(self.scale_std).exp2()\n\t            G_inv = G_inv @ scale2d_inv(s, s)\n\t            labels += [w]\n\t        if self.rotate_frac > 0:\n\t            w = (torch.rand([N], device=device) * 2 - 1) * (np.pi * self.rotate_frac_max)\n\t            w = torch.where(torch.rand([N], device=device) < self.rotate_frac * self.p, w, torch.zeros_like(w))\n\t            G_inv = G_inv @ rotate2d_inv(-w)\n", "            labels += [w.cos() - 1, w.sin()]\n\t        if self.aniso > 0:\n\t            w = torch.randn([N], device=device)\n\t            r = (torch.rand([N], device=device) * 2 - 1) * np.pi\n\t            w = torch.where(torch.rand([N], device=device) < self.aniso * self.p, w, torch.zeros_like(w))\n\t            r = torch.where(torch.rand([N], device=device) < self.aniso_rotate_prob, r, torch.zeros_like(r))\n\t            s = w.mul(self.aniso_std).exp2()\n\t            G_inv = G_inv @ rotate2d_inv(r) @ scale2d_inv(s, 1 / s) @ rotate2d_inv(-r)\n\t            labels += [w * r.cos(), w * r.sin()]\n\t        if self.translate_frac > 0:\n", "            w = torch.randn([2, N], device=device)\n\t            w = torch.where(torch.rand([1, N], device=device) < self.translate_frac * self.p, w, torch.zeros_like(w))\n\t            G_inv = G_inv @ translate2d_inv(w[0].mul(W * self.translate_frac_std), w[1].mul(H * self.translate_frac_std))\n\t            labels += [w[0], w[1]]\n\t        # ----------------------------------\n\t        # Execute geometric transformations.\n\t        # ----------------------------------\n\t        if G_inv is not I_3:\n\t            cx = (W - 1) / 2\n\t            cy = (H - 1) / 2\n", "            cp = matrix([-cx, -cy, 1], [cx, -cy, 1], [cx, cy, 1], [-cx, cy, 1], device=device) # [idx, xyz]\n\t            cp = G_inv @ cp.t() # [batch, xyz, idx]\n\t            Hz = np.asarray(wavelets['sym6'], dtype=np.float32)\n\t            Hz_pad = len(Hz) // 4\n\t            margin = cp[:, :2, :].permute(1, 0, 2).flatten(1) # [xy, batch * idx]\n\t            margin = torch.cat([-margin, margin]).max(dim=1).values # [x0, y0, x1, y1]\n\t            margin = margin + misc.constant([Hz_pad * 2 - cx, Hz_pad * 2 - cy] * 2, device=device)\n\t            margin = margin.max(misc.constant([0, 0] * 2, device=device))\n\t            margin = margin.min(misc.constant([W - 1, H - 1] * 2, device=device))\n\t            mx0, my0, mx1, my1 = margin.ceil().to(torch.int32)\n", "            # Pad image and adjust origin.\n\t            images = torch.nn.functional.pad(input=images, pad=[mx0,mx1,my0,my1], mode='reflect')\n\t            G_inv = translate2d((mx0 - mx1) / 2, (my0 - my1) / 2) @ G_inv\n\t            # Upsample.\n\t            conv_weight = misc.constant(Hz[None, None, ::-1], dtype=images.dtype, device=images.device).tile([images.shape[1], 1, 1])\n\t            conv_pad = (len(Hz) + 1) // 2\n\t            images = torch.stack([images, torch.zeros_like(images)], dim=4).reshape(N, C, images.shape[2], -1)[:, :, :, :-1]\n\t            images = torch.nn.functional.conv2d(images, conv_weight.unsqueeze(2), groups=images.shape[1], padding=[0,conv_pad])\n\t            images = torch.stack([images, torch.zeros_like(images)], dim=3).reshape(N, C, -1, images.shape[3])[:, :, :-1, :]\n\t            images = torch.nn.functional.conv2d(images, conv_weight.unsqueeze(3), groups=images.shape[1], padding=[conv_pad,0])\n", "            G_inv = scale2d(2, 2, device=device) @ G_inv @ scale2d_inv(2, 2, device=device)\n\t            G_inv = translate2d(-0.5, -0.5, device=device) @ G_inv @ translate2d_inv(-0.5, -0.5, device=device)\n\t            # Execute transformation.\n\t            shape = [N, C, (H + Hz_pad * 2) * 2, (W + Hz_pad * 2) * 2]\n\t            G_inv = scale2d(2 / images.shape[3], 2 / images.shape[2], device=device) @ G_inv @ scale2d_inv(2 / shape[3], 2 / shape[2], device=device)\n\t            grid = torch.nn.functional.affine_grid(theta=G_inv[:,:2,:], size=shape, align_corners=False)\n\t            images = torch.nn.functional.grid_sample(images, grid, mode='bilinear', padding_mode='zeros', align_corners=False)\n\t            # Downsample and crop.\n\t            conv_weight = misc.constant(Hz[None, None, :], dtype=images.dtype, device=images.device).tile([images.shape[1], 1, 1])\n\t            conv_pad = (len(Hz) - 1) // 2\n", "            images = torch.nn.functional.conv2d(images, conv_weight.unsqueeze(2), groups=images.shape[1], stride=[1,2], padding=[0,conv_pad])[:, :, :, Hz_pad : -Hz_pad]\n\t            images = torch.nn.functional.conv2d(images, conv_weight.unsqueeze(3), groups=images.shape[1], stride=[2,1], padding=[conv_pad,0])[:, :, Hz_pad : -Hz_pad, :]\n\t        # --------------------------------------------\n\t        # Select parameters for color transformations.\n\t        # --------------------------------------------\n\t        I_4 = torch.eye(4, device=device)\n\t        M = I_4\n\t        luma_axis = misc.constant(np.asarray([1, 1, 1, 0]) / np.sqrt(3), device=device)\n\t        if self.brightness > 0:\n\t            w = torch.randn([N], device=device)\n", "            w = torch.where(torch.rand([N], device=device) < self.brightness * self.p, w, torch.zeros_like(w))\n\t            b = w * self.brightness_std\n\t            M = translate3d(b, b, b) @ M\n\t            labels += [w]\n\t        if self.contrast > 0:\n\t            w = torch.randn([N], device=device)\n\t            w = torch.where(torch.rand([N], device=device) < self.contrast * self.p, w, torch.zeros_like(w))\n\t            c = w.mul(self.contrast_std).exp2()\n\t            M = scale3d(c, c, c) @ M\n\t            labels += [w]\n", "        if self.lumaflip > 0:\n\t            w = torch.randint(2, [N, 1, 1], device=device)\n\t            w = torch.where(torch.rand([N, 1, 1], device=device) < self.lumaflip * self.p, w, torch.zeros_like(w))\n\t            M = (I_4 - 2 * luma_axis.ger(luma_axis) * w) @ M\n\t            labels += [w]\n\t        if self.hue > 0:\n\t            w = (torch.rand([N], device=device) * 2 - 1) * (np.pi * self.hue_max)\n\t            w = torch.where(torch.rand([N], device=device) < self.hue * self.p, w, torch.zeros_like(w))\n\t            M = rotate3d(luma_axis, w) @ M\n\t            labels += [w.cos() - 1, w.sin()]\n", "        if self.saturation > 0:\n\t            w = torch.randn([N, 1, 1], device=device)\n\t            w = torch.where(torch.rand([N, 1, 1], device=device) < self.saturation * self.p, w, torch.zeros_like(w))\n\t            M = (luma_axis.ger(luma_axis) + (I_4 - luma_axis.ger(luma_axis)) * w.mul(self.saturation_std).exp2()) @ M\n\t            labels += [w]\n\t        # ------------------------------\n\t        # Execute color transformations.\n\t        # ------------------------------\n\t        if M is not I_4:\n\t            images = images.reshape([N, C, H * W])\n", "            if C == 3:\n\t                images = M[:, :3, :3] @ images + M[:, :3, 3:]\n\t            elif C == 1:\n\t                M = M[:, :3, :].mean(dim=1, keepdims=True)\n\t                images = images * M[:, :, :3].sum(dim=2, keepdims=True) + M[:, :, 3:]\n\t            else:\n\t                raise ValueError('Image must be RGB (3 channels) or L (1 channel)')\n\t            images = images.reshape([N, C, H, W])\n\t        labels = torch.cat([x.to(torch.float32).reshape(N, -1) for x in labels], dim=1)\n\t        return images, labels\n", "#----------------------------------------------------------------------------\n"]}
{"filename": "torch_utils/distributed.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\timport os\n\timport torch\n\tfrom . import training_stats\n\t#----------------------------------------------------------------------------\n", "def init():\n\t    if 'MASTER_ADDR' not in os.environ:\n\t        os.environ['MASTER_ADDR'] = 'localhost'\n\t    if 'MASTER_PORT' not in os.environ:\n\t        os.environ['MASTER_PORT'] = '29500'\n\t    if 'RANK' not in os.environ:\n\t        os.environ['RANK'] = '0'\n\t    if 'LOCAL_RANK' not in os.environ:\n\t        os.environ['LOCAL_RANK'] = '0'\n\t    if 'WORLD_SIZE' not in os.environ:\n", "        os.environ['WORLD_SIZE'] = '1'\n\t    backend = 'gloo' if os.name == 'nt' else 'nccl'\n\t    torch.distributed.init_process_group(backend=backend, init_method='env://')\n\t    torch.cuda.set_device(int(os.environ.get('LOCAL_RANK', '0')))\n\t    sync_device = torch.device('cuda') if get_world_size() > 1 else None\n\t    training_stats.init_multiprocessing(rank=get_rank(), sync_device=sync_device)\n\t#----------------------------------------------------------------------------\n\tdef get_rank():\n\t    return torch.distributed.get_rank() if torch.distributed.is_initialized() else 0\n\t#----------------------------------------------------------------------------\n", "def get_world_size():\n\t    return torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1\n\t#----------------------------------------------------------------------------\n\tdef should_stop():\n\t    return False\n\t#----------------------------------------------------------------------------\n\tdef update_progress(cur, total):\n\t    _ = cur, total\n\t#----------------------------------------------------------------------------\n\tdef print0(*args, **kwargs):\n", "    if get_rank() == 0:\n\t        print(*args, **kwargs)\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "torch_utils/training_stats.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Facilities for reporting and collecting training statistics across\n\tmultiple processes and devices. The interface is designed to minimize\n\tsynchronization overhead as well as the amount of boilerplate in user\n\tcode.\"\"\"\n", "import re\n\timport numpy as np\n\timport torch\n\timport dnnlib\n\tfrom . import misc\n\t#----------------------------------------------------------------------------\n\t_num_moments    = 3             # [num_scalars, sum_of_scalars, sum_of_squares]\n\t_reduce_dtype   = torch.float32 # Data type to use for initial per-tensor reduction.\n\t_counter_dtype  = torch.float64 # Data type to use for the internal counters.\n\t_rank           = 0             # Rank of the current process.\n", "_sync_device    = None          # Device to use for multiprocess communication. None = single-process.\n\t_sync_called    = False         # Has _sync() been called yet?\n\t_counters       = dict()        # Running counters on each device, updated by report(): name => device => torch.Tensor\n\t_cumulative     = dict()        # Cumulative counters on the CPU, updated by _sync(): name => torch.Tensor\n\t#----------------------------------------------------------------------------\n\tdef init_multiprocessing(rank, sync_device):\n\t    r\"\"\"Initializes `torch_utils.training_stats` for collecting statistics\n\t    across multiple processes.\n\t    This function must be called after\n\t    `torch.distributed.init_process_group()` and before `Collector.update()`.\n", "    The call is not necessary if multi-process collection is not needed.\n\t    Args:\n\t        rank:           Rank of the current process.\n\t        sync_device:    PyTorch device to use for inter-process\n\t                        communication, or None to disable multi-process\n\t                        collection. Typically `torch.device('cuda', rank)`.\n\t    \"\"\"\n\t    global _rank, _sync_device\n\t    assert not _sync_called\n\t    _rank = rank\n", "    _sync_device = sync_device\n\t#----------------------------------------------------------------------------\n\t@misc.profiled_function\n\tdef report(name, value):\n\t    r\"\"\"Broadcasts the given set of scalars to all interested instances of\n\t    `Collector`, across device and process boundaries.\n\t    This function is expected to be extremely cheap and can be safely\n\t    called from anywhere in the training loop, loss function, or inside a\n\t    `torch.nn.Module`.\n\t    Warning: The current implementation expects the set of unique names to\n", "    be consistent across processes. Please make sure that `report()` is\n\t    called at least once for each unique name by each process, and in the\n\t    same order. If a given process has no scalars to broadcast, it can do\n\t    `report(name, [])` (empty list).\n\t    Args:\n\t        name:   Arbitrary string specifying the name of the statistic.\n\t                Averages are accumulated separately for each unique name.\n\t        value:  Arbitrary set of scalars. Can be a list, tuple,\n\t                NumPy array, PyTorch tensor, or Python scalar.\n\t    Returns:\n", "        The same `value` that was passed in.\n\t    \"\"\"\n\t    if name not in _counters:\n\t        _counters[name] = dict()\n\t    elems = torch.as_tensor(value)\n\t    if elems.numel() == 0:\n\t        return value\n\t    elems = elems.detach().flatten().to(_reduce_dtype)\n\t    moments = torch.stack([\n\t        torch.ones_like(elems).sum(),\n", "        elems.sum(),\n\t        elems.square().sum(),\n\t    ])\n\t    assert moments.ndim == 1 and moments.shape[0] == _num_moments\n\t    moments = moments.to(_counter_dtype)\n\t    device = moments.device\n\t    if device not in _counters[name]:\n\t        _counters[name][device] = torch.zeros_like(moments)\n\t    _counters[name][device].add_(moments)\n\t    return value\n", "#----------------------------------------------------------------------------\n\tdef report0(name, value):\n\t    r\"\"\"Broadcasts the given set of scalars by the first process (`rank = 0`),\n\t    but ignores any scalars provided by the other processes.\n\t    See `report()` for further details.\n\t    \"\"\"\n\t    report(name, value if _rank == 0 else [])\n\t    return value\n\t#----------------------------------------------------------------------------\n\tclass Collector:\n", "    r\"\"\"Collects the scalars broadcasted by `report()` and `report0()` and\n\t    computes their long-term averages (mean and standard deviation) over\n\t    user-defined periods of time.\n\t    The averages are first collected into internal counters that are not\n\t    directly visible to the user. They are then copied to the user-visible\n\t    state as a result of calling `update()` and can then be queried using\n\t    `mean()`, `std()`, `as_dict()`, etc. Calling `update()` also resets the\n\t    internal counters for the next round, so that the user-visible state\n\t    effectively reflects averages collected between the last two calls to\n\t    `update()`.\n", "    Args:\n\t        regex:          Regular expression defining which statistics to\n\t                        collect. The default is to collect everything.\n\t        keep_previous:  Whether to retain the previous averages if no\n\t                        scalars were collected on a given round\n\t                        (default: True).\n\t    \"\"\"\n\t    def __init__(self, regex='.*', keep_previous=True):\n\t        self._regex = re.compile(regex)\n\t        self._keep_previous = keep_previous\n", "        self._cumulative = dict()\n\t        self._moments = dict()\n\t        self.update()\n\t        self._moments.clear()\n\t    def names(self):\n\t        r\"\"\"Returns the names of all statistics broadcasted so far that\n\t        match the regular expression specified at construction time.\n\t        \"\"\"\n\t        return [name for name in _counters if self._regex.fullmatch(name)]\n\t    def update(self):\n", "        r\"\"\"Copies current values of the internal counters to the\n\t        user-visible state and resets them for the next round.\n\t        If `keep_previous=True` was specified at construction time, the\n\t        operation is skipped for statistics that have received no scalars\n\t        since the last update, retaining their previous averages.\n\t        This method performs a number of GPU-to-CPU transfers and one\n\t        `torch.distributed.all_reduce()`. It is intended to be called\n\t        periodically in the main training loop, typically once every\n\t        N training steps.\n\t        \"\"\"\n", "        if not self._keep_previous:\n\t            self._moments.clear()\n\t        for name, cumulative in _sync(self.names()):\n\t            if name not in self._cumulative:\n\t                self._cumulative[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n\t            delta = cumulative - self._cumulative[name]\n\t            self._cumulative[name].copy_(cumulative)\n\t            if float(delta[0]) != 0:\n\t                self._moments[name] = delta\n\t    def _get_delta(self, name):\n", "        r\"\"\"Returns the raw moments that were accumulated for the given\n\t        statistic between the last two calls to `update()`, or zero if\n\t        no scalars were collected.\n\t        \"\"\"\n\t        assert self._regex.fullmatch(name)\n\t        if name not in self._moments:\n\t            self._moments[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n\t        return self._moments[name]\n\t    def num(self, name):\n\t        r\"\"\"Returns the number of scalars that were accumulated for the given\n", "        statistic between the last two calls to `update()`, or zero if\n\t        no scalars were collected.\n\t        \"\"\"\n\t        delta = self._get_delta(name)\n\t        return int(delta[0])\n\t    def mean(self, name):\n\t        r\"\"\"Returns the mean of the scalars that were accumulated for the\n\t        given statistic between the last two calls to `update()`, or NaN if\n\t        no scalars were collected.\n\t        \"\"\"\n", "        delta = self._get_delta(name)\n\t        if int(delta[0]) == 0:\n\t            return float('nan')\n\t        return float(delta[1] / delta[0])\n\t    def std(self, name):\n\t        r\"\"\"Returns the standard deviation of the scalars that were\n\t        accumulated for the given statistic between the last two calls to\n\t        `update()`, or NaN if no scalars were collected.\n\t        \"\"\"\n\t        delta = self._get_delta(name)\n", "        if int(delta[0]) == 0 or not np.isfinite(float(delta[1])):\n\t            return float('nan')\n\t        if int(delta[0]) == 1:\n\t            return float(0)\n\t        mean = float(delta[1] / delta[0])\n\t        raw_var = float(delta[2] / delta[0])\n\t        return np.sqrt(max(raw_var - np.square(mean), 0))\n\t    def as_dict(self):\n\t        r\"\"\"Returns the averages accumulated between the last two calls to\n\t        `update()` as an `dnnlib.EasyDict`. The contents are as follows:\n", "            dnnlib.EasyDict(\n\t                NAME = dnnlib.EasyDict(num=FLOAT, mean=FLOAT, std=FLOAT),\n\t                ...\n\t            )\n\t        \"\"\"\n\t        stats = dnnlib.EasyDict()\n\t        for name in self.names():\n\t            stats[name] = dnnlib.EasyDict(num=self.num(name), mean=self.mean(name), std=self.std(name))\n\t        return stats\n\t    def __getitem__(self, name):\n", "        r\"\"\"Convenience getter.\n\t        `collector[name]` is a synonym for `collector.mean(name)`.\n\t        \"\"\"\n\t        return self.mean(name)\n\t#----------------------------------------------------------------------------\n\tdef _sync(names):\n\t    r\"\"\"Synchronize the global cumulative counters across devices and\n\t    processes. Called internally by `Collector.update()`.\n\t    \"\"\"\n\t    if len(names) == 0:\n", "        return []\n\t    global _sync_called\n\t    _sync_called = True\n\t    # Collect deltas within current rank.\n\t    deltas = []\n\t    device = _sync_device if _sync_device is not None else torch.device('cpu')\n\t    for name in names:\n\t        delta = torch.zeros([_num_moments], dtype=_counter_dtype, device=device)\n\t        for counter in _counters[name].values():\n\t            delta.add_(counter.to(device))\n", "            counter.copy_(torch.zeros_like(counter))\n\t        deltas.append(delta)\n\t    deltas = torch.stack(deltas)\n\t    # Sum deltas across ranks.\n\t    if _sync_device is not None:\n\t        torch.distributed.all_reduce(deltas)\n\t    # Update cumulative values.\n\t    deltas = deltas.cpu()\n\t    for idx, name in enumerate(names):\n\t        if name not in _cumulative:\n", "            _cumulative[name] = torch.zeros([_num_moments], dtype=_counter_dtype)\n\t        _cumulative[name].add_(deltas[idx])\n\t    # Return name-value pairs.\n\t    return [(name, _cumulative[name]) for name in names]\n\t#----------------------------------------------------------------------------\n\t# Convenience.\n\tdefault_collector = Collector()\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "torch_utils/persistence.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Facilities for pickling Python code alongside other data.\n\tThe pickled code is automatically imported into a separate Python module\n\tduring unpickling. This way, any previously exported pickles will remain\n\tusable even if the original code is no longer available, or if the current\n", "version of the code is not consistent with what was originally pickled.\"\"\"\n\timport sys\n\timport pickle\n\timport io\n\timport inspect\n\timport copy\n\timport uuid\n\timport types\n\timport dnnlib\n\t#----------------------------------------------------------------------------\n", "_version            = 6         # internal version number\n\t_decorators         = set()     # {decorator_class, ...}\n\t_import_hooks       = []        # [hook_function, ...]\n\t_module_to_src_dict = dict()    # {module: src, ...}\n\t_src_to_module_dict = dict()    # {src: module, ...}\n\t#----------------------------------------------------------------------------\n\tdef persistent_class(orig_class):\n\t    r\"\"\"Class decorator that extends a given class to save its source code\n\t    when pickled.\n\t    Example:\n", "        from torch_utils import persistence\n\t        @persistence.persistent_class\n\t        class MyNetwork(torch.nn.Module):\n\t            def __init__(self, num_inputs, num_outputs):\n\t                super().__init__()\n\t                self.fc = MyLayer(num_inputs, num_outputs)\n\t                ...\n\t        @persistence.persistent_class\n\t        class MyLayer(torch.nn.Module):\n\t            ...\n", "    When pickled, any instance of `MyNetwork` and `MyLayer` will save its\n\t    source code alongside other internal state (e.g., parameters, buffers,\n\t    and submodules). This way, any previously exported pickle will remain\n\t    usable even if the class definitions have been modified or are no\n\t    longer available.\n\t    The decorator saves the source code of the entire Python module\n\t    containing the decorated class. It does *not* save the source code of\n\t    any imported modules. Thus, the imported modules must be available\n\t    during unpickling, also including `torch_utils.persistence` itself.\n\t    It is ok to call functions defined in the same module from the\n", "    decorated class. However, if the decorated class depends on other\n\t    classes defined in the same module, they must be decorated as well.\n\t    This is illustrated in the above example in the case of `MyLayer`.\n\t    It is also possible to employ the decorator just-in-time before\n\t    calling the constructor. For example:\n\t        cls = MyLayer\n\t        if want_to_make_it_persistent:\n\t            cls = persistence.persistent_class(cls)\n\t        layer = cls(num_inputs, num_outputs)\n\t    As an additional feature, the decorator also keeps track of the\n", "    arguments that were used to construct each instance of the decorated\n\t    class. The arguments can be queried via `obj.init_args` and\n\t    `obj.init_kwargs`, and they are automatically pickled alongside other\n\t    object state. This feature can be disabled on a per-instance basis\n\t    by setting `self._record_init_args = False` in the constructor.\n\t    A typical use case is to first unpickle a previous instance of a\n\t    persistent class, and then upgrade it to use the latest version of\n\t    the source code:\n\t        with open('old_pickle.pkl', 'rb') as f:\n\t            old_net = pickle.load(f)\n", "        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)\n\t        misc.copy_params_and_buffers(old_net, new_net, require_all=True)\n\t    \"\"\"\n\t    assert isinstance(orig_class, type)\n\t    if is_persistent(orig_class):\n\t        return orig_class\n\t    assert orig_class.__module__ in sys.modules\n\t    orig_module = sys.modules[orig_class.__module__]\n\t    orig_module_src = _module_to_src(orig_module)\n\t    class Decorator(orig_class):\n", "        _orig_module_src = orig_module_src\n\t        _orig_class_name = orig_class.__name__\n\t        def __init__(self, *args, **kwargs):\n\t            super().__init__(*args, **kwargs)\n\t            record_init_args = getattr(self, '_record_init_args', True)\n\t            self._init_args = copy.deepcopy(args) if record_init_args else None\n\t            self._init_kwargs = copy.deepcopy(kwargs) if record_init_args else None\n\t            assert orig_class.__name__ in orig_module.__dict__\n\t            _check_pickleable(self.__reduce__())\n\t        @property\n", "        def init_args(self):\n\t            assert self._init_args is not None\n\t            return copy.deepcopy(self._init_args)\n\t        @property\n\t        def init_kwargs(self):\n\t            assert self._init_kwargs is not None\n\t            return dnnlib.EasyDict(copy.deepcopy(self._init_kwargs))\n\t        def __reduce__(self):\n\t            fields = list(super().__reduce__())\n\t            fields += [None] * max(3 - len(fields), 0)\n", "            if fields[0] is not _reconstruct_persistent_obj:\n\t                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])\n\t                fields[0] = _reconstruct_persistent_obj # reconstruct func\n\t                fields[1] = (meta,) # reconstruct args\n\t                fields[2] = None # state dict\n\t            return tuple(fields)\n\t    Decorator.__name__ = orig_class.__name__\n\t    Decorator.__module__ = orig_class.__module__\n\t    _decorators.add(Decorator)\n\t    return Decorator\n", "#----------------------------------------------------------------------------\n\tdef is_persistent(obj):\n\t    r\"\"\"Test whether the given object or class is persistent, i.e.,\n\t    whether it will save its source code when pickled.\n\t    \"\"\"\n\t    try:\n\t        if obj in _decorators:\n\t            return True\n\t    except TypeError:\n\t        pass\n", "    return type(obj) in _decorators # pylint: disable=unidiomatic-typecheck\n\t#----------------------------------------------------------------------------\n\tdef import_hook(hook):\n\t    r\"\"\"Register an import hook that is called whenever a persistent object\n\t    is being unpickled. A typical use case is to patch the pickled source\n\t    code to avoid errors and inconsistencies when the API of some imported\n\t    module has changed.\n\t    The hook should have the following signature:\n\t        hook(meta) -> modified meta\n\t    `meta` is an instance of `dnnlib.EasyDict` with the following fields:\n", "        type:       Type of the persistent object, e.g. `'class'`.\n\t        version:    Internal version number of `torch_utils.persistence`.\n\t        module_src  Original source code of the Python module.\n\t        class_name: Class name in the original Python module.\n\t        state:      Internal state of the object.\n\t    Example:\n\t        @persistence.import_hook\n\t        def wreck_my_network(meta):\n\t            if meta.class_name == 'MyNetwork':\n\t                print('MyNetwork is being imported. I will wreck it!')\n", "                meta.module_src = meta.module_src.replace(\"True\", \"False\")\n\t            return meta\n\t    \"\"\"\n\t    assert callable(hook)\n\t    _import_hooks.append(hook)\n\t#----------------------------------------------------------------------------\n\tdef _reconstruct_persistent_obj(meta):\n\t    r\"\"\"Hook that is called internally by the `pickle` module to unpickle\n\t    a persistent object.\n\t    \"\"\"\n", "    meta = dnnlib.EasyDict(meta)\n\t    meta.state = dnnlib.EasyDict(meta.state)\n\t    for hook in _import_hooks:\n\t        meta = hook(meta)\n\t        assert meta is not None\n\t    assert meta.version == _version\n\t    module = _src_to_module(meta.module_src)\n\t    assert meta.type == 'class'\n\t    orig_class = module.__dict__[meta.class_name]\n\t    decorator_class = persistent_class(orig_class)\n", "    obj = decorator_class.__new__(decorator_class)\n\t    setstate = getattr(obj, '__setstate__', None)\n\t    if callable(setstate):\n\t        setstate(meta.state) # pylint: disable=not-callable\n\t    else:\n\t        obj.__dict__.update(meta.state)\n\t    return obj\n\t#----------------------------------------------------------------------------\n\tdef _module_to_src(module):\n\t    r\"\"\"Query the source code of a given Python module.\n", "    \"\"\"\n\t    src = _module_to_src_dict.get(module, None)\n\t    if src is None:\n\t        src = inspect.getsource(module)\n\t        _module_to_src_dict[module] = src\n\t        _src_to_module_dict[src] = module\n\t    return src\n\tdef _src_to_module(src):\n\t    r\"\"\"Get or create a Python module for the given source code.\n\t    \"\"\"\n", "    module = _src_to_module_dict.get(src, None)\n\t    if module is None:\n\t        module_name = \"_imported_module_\" + uuid.uuid4().hex\n\t        module = types.ModuleType(module_name)\n\t        sys.modules[module_name] = module\n\t        _module_to_src_dict[module] = src\n\t        _src_to_module_dict[src] = module\n\t        exec(src, module.__dict__) # pylint: disable=exec-used\n\t    return module\n\t#----------------------------------------------------------------------------\n", "def _check_pickleable(obj):\n\t    r\"\"\"Check that the given object is pickleable, raising an exception if\n\t    it is not. This function is expected to be considerably more efficient\n\t    than actually pickling the object.\n\t    \"\"\"\n\t    def recurse(obj):\n\t        if isinstance(obj, (list, tuple, set)):\n\t            return [recurse(x) for x in obj]\n\t        if isinstance(obj, dict):\n\t            return [[recurse(x), recurse(y)] for x, y in obj.items()]\n", "        if isinstance(obj, (str, int, float, bool, bytes, bytearray)):\n\t            return None # Python primitive types are pickleable.\n\t        if f'{type(obj).__module__}.{type(obj).__name__}' in ['numpy.ndarray', 'torch.Tensor', 'torch.nn.parameter.Parameter']:\n\t            return None # NumPy arrays and PyTorch tensors are pickleable.\n\t        if is_persistent(obj):\n\t            return None # Persistent objects are pickleable, by virtue of the constructor check.\n\t        return obj\n\t    with io.BytesIO() as f:\n\t        pickle.dump(recurse(obj), f)\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "torch_utils/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t# empty\n"]}
{"filename": "torch_utils/misc.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\timport re\n\timport contextlib\n\timport numpy as np\n\timport torch\n", "import warnings\n\timport dnnlib\n\t#----------------------------------------------------------------------------\n\t# Cached construction of constant tensors. Avoids CPU=>GPU copy when the\n\t# same constant is used multiple times.\n\t_constant_cache = dict()\n\tdef constant(value, shape=None, dtype=None, device=None, memory_format=None):\n\t    value = np.asarray(value)\n\t    if shape is not None:\n\t        shape = tuple(shape)\n", "    if dtype is None:\n\t        dtype = torch.get_default_dtype()\n\t    if device is None:\n\t        device = torch.device('cpu')\n\t    if memory_format is None:\n\t        memory_format = torch.contiguous_format\n\t    key = (value.shape, value.dtype, value.tobytes(), shape, dtype, device, memory_format)\n\t    tensor = _constant_cache.get(key, None)\n\t    if tensor is None:\n\t        tensor = torch.as_tensor(value.copy(), dtype=dtype, device=device)\n", "        if shape is not None:\n\t            tensor, _ = torch.broadcast_tensors(tensor, torch.empty(shape))\n\t        tensor = tensor.contiguous(memory_format=memory_format)\n\t        _constant_cache[key] = tensor\n\t    return tensor\n\t#----------------------------------------------------------------------------\n\t# Replace NaN/Inf with specified numerical values.\n\ttry:\n\t    nan_to_num = torch.nan_to_num # 1.8.0a0\n\texcept AttributeError:\n", "    def nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None): # pylint: disable=redefined-builtin\n\t        assert isinstance(input, torch.Tensor)\n\t        if posinf is None:\n\t            posinf = torch.finfo(input.dtype).max\n\t        if neginf is None:\n\t            neginf = torch.finfo(input.dtype).min\n\t        assert nan == 0\n\t        return torch.clamp(input.unsqueeze(0).nansum(0), min=neginf, max=posinf, out=out)\n\t#----------------------------------------------------------------------------\n\t# Symbolic assert.\n", "try:\n\t    symbolic_assert = torch._assert # 1.8.0a0 # pylint: disable=protected-access\n\texcept AttributeError:\n\t    symbolic_assert = torch.Assert # 1.7.0\n\t#----------------------------------------------------------------------------\n\t# Context manager to temporarily suppress known warnings in torch.jit.trace().\n\t# Note: Cannot use catch_warnings because of https://bugs.python.org/issue29672\n\t@contextlib.contextmanager\n\tdef suppress_tracer_warnings():\n\t    flt = ('ignore', None, torch.jit.TracerWarning, None, 0)\n", "    warnings.filters.insert(0, flt)\n\t    yield\n\t    warnings.filters.remove(flt)\n\t#----------------------------------------------------------------------------\n\t# Assert that the shape of a tensor matches the given list of integers.\n\t# None indicates that the size of a dimension is allowed to vary.\n\t# Performs symbolic assertion when used in torch.jit.trace().\n\tdef assert_shape(tensor, ref_shape):\n\t    if tensor.ndim != len(ref_shape):\n\t        raise AssertionError(f'Wrong number of dimensions: got {tensor.ndim}, expected {len(ref_shape)}')\n", "    for idx, (size, ref_size) in enumerate(zip(tensor.shape, ref_shape)):\n\t        if ref_size is None:\n\t            pass\n\t        elif isinstance(ref_size, torch.Tensor):\n\t            with suppress_tracer_warnings(): # as_tensor results are registered as constants\n\t                symbolic_assert(torch.equal(torch.as_tensor(size), ref_size), f'Wrong size for dimension {idx}')\n\t        elif isinstance(size, torch.Tensor):\n\t            with suppress_tracer_warnings(): # as_tensor results are registered as constants\n\t                symbolic_assert(torch.equal(size, torch.as_tensor(ref_size)), f'Wrong size for dimension {idx}: expected {ref_size}')\n\t        elif size != ref_size:\n", "            raise AssertionError(f'Wrong size for dimension {idx}: got {size}, expected {ref_size}')\n\t#----------------------------------------------------------------------------\n\t# Function decorator that calls torch.autograd.profiler.record_function().\n\tdef profiled_function(fn):\n\t    def decorator(*args, **kwargs):\n\t        with torch.autograd.profiler.record_function(fn.__name__):\n\t            return fn(*args, **kwargs)\n\t    decorator.__name__ = fn.__name__\n\t    return decorator\n\t#----------------------------------------------------------------------------\n", "# Sampler for torch.utils.data.DataLoader that loops over the dataset\n\t# indefinitely, shuffling items as it goes.\n\tclass InfiniteSampler(torch.utils.data.Sampler):\n\t    def __init__(self, dataset, rank=0, num_replicas=1, shuffle=True, seed=0, window_size=0.5):\n\t        assert len(dataset) > 0\n\t        assert num_replicas > 0\n\t        assert 0 <= rank < num_replicas\n\t        assert 0 <= window_size <= 1\n\t        super().__init__(dataset)\n\t        self.dataset = dataset\n", "        self.rank = rank\n\t        self.num_replicas = num_replicas\n\t        self.shuffle = shuffle\n\t        self.seed = seed\n\t        self.window_size = window_size\n\t    def __iter__(self):\n\t        order = np.arange(len(self.dataset))\n\t        rnd = None\n\t        window = 0\n\t        if self.shuffle:\n", "            rnd = np.random.RandomState(self.seed)\n\t            rnd.shuffle(order)\n\t            window = int(np.rint(order.size * self.window_size))\n\t        idx = 0\n\t        while True:\n\t            i = idx % order.size\n\t            if idx % self.num_replicas == self.rank:\n\t                yield order[i]\n\t            if window >= 2:\n\t                j = (i - rnd.randint(window)) % order.size\n", "                order[i], order[j] = order[j], order[i]\n\t            idx += 1\n\t#----------------------------------------------------------------------------\n\t# Utilities for operating with torch.nn.Module parameters and buffers.\n\tdef params_and_buffers(module):\n\t    assert isinstance(module, torch.nn.Module)\n\t    return list(module.parameters()) + list(module.buffers())\n\tdef named_params_and_buffers(module):\n\t    assert isinstance(module, torch.nn.Module)\n\t    return list(module.named_parameters()) + list(module.named_buffers())\n", "@torch.no_grad()\n\tdef copy_params_and_buffers(src_module, dst_module, require_all=False):\n\t    assert isinstance(src_module, torch.nn.Module)\n\t    assert isinstance(dst_module, torch.nn.Module)\n\t    src_tensors = dict(named_params_and_buffers(src_module))\n\t    for name, tensor in named_params_and_buffers(dst_module):\n\t        assert (name in src_tensors) or (not require_all)\n\t        if name in src_tensors:\n\t            tensor.copy_(src_tensors[name])\n\t#----------------------------------------------------------------------------\n", "# Context manager for easily enabling/disabling DistributedDataParallel\n\t# synchronization.\n\t@contextlib.contextmanager\n\tdef ddp_sync(module, sync):\n\t    assert isinstance(module, torch.nn.Module)\n\t    if sync or not isinstance(module, torch.nn.parallel.DistributedDataParallel):\n\t        yield\n\t    else:\n\t        with module.no_sync():\n\t            yield\n", "#----------------------------------------------------------------------------\n\t# Check DistributedDataParallel consistency across processes.\n\tdef check_ddp_consistency(module, ignore_regex=None):\n\t    assert isinstance(module, torch.nn.Module)\n\t    for name, tensor in named_params_and_buffers(module):\n\t        fullname = type(module).__name__ + '.' + name\n\t        if ignore_regex is not None and re.fullmatch(ignore_regex, fullname):\n\t            continue\n\t        tensor = tensor.detach()\n\t        if tensor.is_floating_point():\n", "            tensor = nan_to_num(tensor)\n\t        other = tensor.clone()\n\t        torch.distributed.broadcast(tensor=other, src=0)\n\t        assert (tensor == other).all(), fullname\n\t#----------------------------------------------------------------------------\n\t# Print summary table of module hierarchy.\n\tdef print_module_summary(module, inputs, max_nesting=3, skip_redundant=True):\n\t    assert isinstance(module, torch.nn.Module)\n\t    assert not isinstance(module, torch.jit.ScriptModule)\n\t    assert isinstance(inputs, (tuple, list))\n", "    # Register hooks.\n\t    entries = []\n\t    nesting = [0]\n\t    def pre_hook(_mod, _inputs):\n\t        nesting[0] += 1\n\t    def post_hook(mod, _inputs, outputs):\n\t        nesting[0] -= 1\n\t        if nesting[0] <= max_nesting:\n\t            outputs = list(outputs) if isinstance(outputs, (tuple, list)) else [outputs]\n\t            outputs = [t for t in outputs if isinstance(t, torch.Tensor)]\n", "            entries.append(dnnlib.EasyDict(mod=mod, outputs=outputs))\n\t    hooks = [mod.register_forward_pre_hook(pre_hook) for mod in module.modules()]\n\t    hooks += [mod.register_forward_hook(post_hook) for mod in module.modules()]\n\t    # Run module.\n\t    outputs = module(*inputs)\n\t    for hook in hooks:\n\t        hook.remove()\n\t    # Identify unique outputs, parameters, and buffers.\n\t    tensors_seen = set()\n\t    for e in entries:\n", "        e.unique_params = [t for t in e.mod.parameters() if id(t) not in tensors_seen]\n\t        e.unique_buffers = [t for t in e.mod.buffers() if id(t) not in tensors_seen]\n\t        e.unique_outputs = [t for t in e.outputs if id(t) not in tensors_seen]\n\t        tensors_seen |= {id(t) for t in e.unique_params + e.unique_buffers + e.unique_outputs}\n\t    # Filter out redundant entries.\n\t    if skip_redundant:\n\t        entries = [e for e in entries if len(e.unique_params) or len(e.unique_buffers) or len(e.unique_outputs)]\n\t    # Construct table.\n\t    rows = [[type(module).__name__, 'Parameters', 'Buffers', 'Output shape', 'Datatype']]\n\t    rows += [['---'] * len(rows[0])]\n", "    param_total = 0\n\t    buffer_total = 0\n\t    submodule_names = {mod: name for name, mod in module.named_modules()}\n\t    for e in entries:\n\t        name = '<top-level>' if e.mod is module else submodule_names[e.mod]\n\t        param_size = sum(t.numel() for t in e.unique_params)\n\t        buffer_size = sum(t.numel() for t in e.unique_buffers)\n\t        output_shapes = [str(list(t.shape)) for t in e.outputs]\n\t        output_dtypes = [str(t.dtype).split('.')[-1] for t in e.outputs]\n\t        rows += [[\n", "            name + (':0' if len(e.outputs) >= 2 else ''),\n\t            str(param_size) if param_size else '-',\n\t            str(buffer_size) if buffer_size else '-',\n\t            (output_shapes + ['-'])[0],\n\t            (output_dtypes + ['-'])[0],\n\t        ]]\n\t        for idx in range(1, len(e.outputs)):\n\t            rows += [[name + f':{idx}', '-', '-', output_shapes[idx], output_dtypes[idx]]]\n\t        param_total += param_size\n\t        buffer_total += buffer_size\n", "    rows += [['---'] * len(rows[0])]\n\t    rows += [['Total', str(param_total), str(buffer_total), '-', '-']]\n\t    # Print table.\n\t    widths = [max(len(cell) for cell in column) for column in zip(*rows)]\n\t    print()\n\t    for row in rows:\n\t        print('  '.join(cell + ' ' * (width - len(cell)) for cell, width in zip(row, widths)))\n\t    print()\n\t    return outputs\n\t#----------------------------------------------------------------------------\n"]}
{"filename": "dnnlib/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\tfrom .util import EasyDict, make_cache_dir_path\n"]}
{"filename": "dnnlib/util.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\t#\n\t# This work is licensed under a Creative Commons\n\t# Attribution-NonCommercial-ShareAlike 4.0 International License.\n\t# You should have received a copy of the license along with this\n\t# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\t\"\"\"Miscellaneous utility classes and functions.\"\"\"\n\timport ctypes\n\timport fnmatch\n\timport importlib\n", "import inspect\n\timport numpy as np\n\timport os\n\timport shutil\n\timport sys\n\timport types\n\timport io\n\timport pickle\n\timport re\n\timport requests\n", "import html\n\timport hashlib\n\timport glob\n\timport tempfile\n\timport urllib\n\timport urllib.request\n\timport uuid\n\tfrom distutils.util import strtobool\n\tfrom typing import Any, List, Tuple, Union, Optional\n\t# Util classes\n", "# ------------------------------------------------------------------------------------------\n\tclass EasyDict(dict):\n\t    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n\t    def __getattr__(self, name: str) -> Any:\n\t        try:\n\t            return self[name]\n\t        except KeyError:\n\t            raise AttributeError(name)\n\t    def __setattr__(self, name: str, value: Any) -> None:\n\t        self[name] = value\n", "    def __delattr__(self, name: str) -> None:\n\t        del self[name]\n\tclass Logger(object):\n\t    \"\"\"Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file.\"\"\"\n\t    def __init__(self, file_name: Optional[str] = None, file_mode: str = \"w\", should_flush: bool = True):\n\t        self.file = None\n\t        if file_name is not None:\n\t            self.file = open(file_name, file_mode)\n\t        self.should_flush = should_flush\n\t        self.stdout = sys.stdout\n", "        self.stderr = sys.stderr\n\t        sys.stdout = self\n\t        sys.stderr = self\n\t    def __enter__(self) -> \"Logger\":\n\t        return self\n\t    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n\t        self.close()\n\t    def write(self, text: Union[str, bytes]) -> None:\n\t        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n\t        if isinstance(text, bytes):\n", "            text = text.decode()\n\t        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n\t            return\n\t        if self.file is not None:\n\t            self.file.write(text)\n\t        self.stdout.write(text)\n\t        if self.should_flush:\n\t            self.flush()\n\t    def flush(self) -> None:\n\t        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n", "        if self.file is not None:\n\t            self.file.flush()\n\t        self.stdout.flush()\n\t    def close(self) -> None:\n\t        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n\t        self.flush()\n\t        # if using multiple loggers, prevent closing in wrong order\n\t        if sys.stdout is self:\n\t            sys.stdout = self.stdout\n\t        if sys.stderr is self:\n", "            sys.stderr = self.stderr\n\t        if self.file is not None:\n\t            self.file.close()\n\t            self.file = None\n\t# Cache directories\n\t# ------------------------------------------------------------------------------------------\n\t_dnnlib_cache_dir = None\n\tdef set_cache_dir(path: str) -> None:\n\t    global _dnnlib_cache_dir\n\t    _dnnlib_cache_dir = path\n", "def make_cache_dir_path(*paths: str) -> str:\n\t    if _dnnlib_cache_dir is not None:\n\t        return os.path.join(_dnnlib_cache_dir, *paths)\n\t    if 'DNNLIB_CACHE_DIR' in os.environ:\n\t        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n\t    if 'HOME' in os.environ:\n\t        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n\t    if 'USERPROFILE' in os.environ:\n\t        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n\t    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)\n", "# Small util functions\n\t# ------------------------------------------------------------------------------------------\n\tdef format_time(seconds: Union[int, float]) -> str:\n\t    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n\t    s = int(np.rint(seconds))\n\t    if s < 60:\n\t        return \"{0}s\".format(s)\n\t    elif s < 60 * 60:\n\t        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n\t    elif s < 24 * 60 * 60:\n", "        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n\t    else:\n\t        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)\n\tdef format_time_brief(seconds: Union[int, float]) -> str:\n\t    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n\t    s = int(np.rint(seconds))\n\t    if s < 60:\n\t        return \"{0}s\".format(s)\n\t    elif s < 60 * 60:\n\t        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n", "    elif s < 24 * 60 * 60:\n\t        return \"{0}h {1:02}m\".format(s // (60 * 60), (s // 60) % 60)\n\t    else:\n\t        return \"{0}d {1:02}h\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24)\n\tdef ask_yes_no(question: str) -> bool:\n\t    \"\"\"Ask the user the question until the user inputs a valid answer.\"\"\"\n\t    while True:\n\t        try:\n\t            print(\"{0} [y/n]\".format(question))\n\t            return strtobool(input().lower())\n", "        except ValueError:\n\t            pass\n\tdef tuple_product(t: Tuple) -> Any:\n\t    \"\"\"Calculate the product of the tuple elements.\"\"\"\n\t    result = 1\n\t    for v in t:\n\t        result *= v\n\t    return result\n\t_str_to_ctype = {\n\t    \"uint8\": ctypes.c_ubyte,\n", "    \"uint16\": ctypes.c_uint16,\n\t    \"uint32\": ctypes.c_uint32,\n\t    \"uint64\": ctypes.c_uint64,\n\t    \"int8\": ctypes.c_byte,\n\t    \"int16\": ctypes.c_int16,\n\t    \"int32\": ctypes.c_int32,\n\t    \"int64\": ctypes.c_int64,\n\t    \"float32\": ctypes.c_float,\n\t    \"float64\": ctypes.c_double\n\t}\n", "def get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n\t    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n\t    type_str = None\n\t    if isinstance(type_obj, str):\n\t        type_str = type_obj\n\t    elif hasattr(type_obj, \"__name__\"):\n\t        type_str = type_obj.__name__\n\t    elif hasattr(type_obj, \"name\"):\n\t        type_str = type_obj.name\n\t    else:\n", "        raise RuntimeError(\"Cannot infer type name from input\")\n\t    assert type_str in _str_to_ctype.keys()\n\t    my_dtype = np.dtype(type_str)\n\t    my_ctype = _str_to_ctype[type_str]\n\t    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n\t    return my_dtype, my_ctype\n\tdef is_pickleable(obj: Any) -> bool:\n\t    try:\n\t        with io.BytesIO() as stream:\n\t            pickle.dump(obj, stream)\n", "        return True\n\t    except:\n\t        return False\n\t# Functionality to import modules/objects by name, and call functions by name\n\t# ------------------------------------------------------------------------------------------\n\tdef get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n\t    \"\"\"Searches for the underlying module behind the name to some python object.\n\t    Returns the module and the object name (original name with module part removed).\"\"\"\n\t    # allow convenience shorthands, substitute them by full names\n\t    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n", "    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n\t    # list alternatives for (module_name, local_obj_name)\n\t    parts = obj_name.split(\".\")\n\t    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n\t    # try each alternative in turn\n\t    for module_name, local_obj_name in name_pairs:\n\t        try:\n\t            module = importlib.import_module(module_name) # may raise ImportError\n\t            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n\t            return module, local_obj_name\n", "        except:\n\t            pass\n\t    # maybe some of the modules themselves contain errors?\n\t    for module_name, _local_obj_name in name_pairs:\n\t        try:\n\t            importlib.import_module(module_name) # may raise ImportError\n\t        except ImportError:\n\t            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n\t                raise\n\t    # maybe the requested attribute is missing?\n", "    for module_name, local_obj_name in name_pairs:\n\t        try:\n\t            module = importlib.import_module(module_name) # may raise ImportError\n\t            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n\t        except ImportError:\n\t            pass\n\t    # we are out of luck, but we have no idea why\n\t    raise ImportError(obj_name)\n\tdef get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n\t    \"\"\"Traverses the object name and returns the last (rightmost) python object.\"\"\"\n", "    if obj_name == '':\n\t        return module\n\t    obj = module\n\t    for part in obj_name.split(\".\"):\n\t        obj = getattr(obj, part)\n\t    return obj\n\tdef get_obj_by_name(name: str) -> Any:\n\t    \"\"\"Finds the python object with the given name.\"\"\"\n\t    module, obj_name = get_module_from_obj_name(name)\n\t    return get_obj_from_module(module, obj_name)\n", "def call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n\t    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n\t    assert func_name is not None\n\t    func_obj = get_obj_by_name(func_name)\n\t    assert callable(func_obj)\n\t    return func_obj(*args, **kwargs)\n\tdef construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n\t    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n\t    return call_func_by_name(*args, func_name=class_name, **kwargs)\n\tdef get_module_dir_by_obj_name(obj_name: str) -> str:\n", "    \"\"\"Get the directory path of the module containing the given object name.\"\"\"\n\t    module, _ = get_module_from_obj_name(obj_name)\n\t    return os.path.dirname(inspect.getfile(module))\n\tdef is_top_level_function(obj: Any) -> bool:\n\t    \"\"\"Determine whether the given object is a top-level function, i.e., defined at module scope using 'def'.\"\"\"\n\t    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n\tdef get_top_level_function_name(obj: Any) -> str:\n\t    \"\"\"Return the fully-qualified name of a top-level function.\"\"\"\n\t    assert is_top_level_function(obj)\n\t    module = obj.__module__\n", "    if module == '__main__':\n\t        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n\t    return module + \".\" + obj.__name__\n\t# File system helpers\n\t# ------------------------------------------------------------------------------------------\n\tdef list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n\t    \"\"\"List all files recursively in a given directory while ignoring given file and directory names.\n\t    Returns list of tuples containing both absolute and relative paths.\"\"\"\n\t    assert os.path.isdir(dir_path)\n\t    base_name = os.path.basename(os.path.normpath(dir_path))\n", "    if ignores is None:\n\t        ignores = []\n\t    result = []\n\t    for root, dirs, files in os.walk(dir_path, topdown=True):\n\t        for ignore_ in ignores:\n\t            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n\t            # dirs need to be edited in-place\n\t            for d in dirs_to_remove:\n\t                dirs.remove(d)\n\t            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n", "        absolute_paths = [os.path.join(root, f) for f in files]\n\t        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n\t        if add_base_to_relative:\n\t            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n\t        assert len(absolute_paths) == len(relative_paths)\n\t        result += zip(absolute_paths, relative_paths)\n\t    return result\n\tdef copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n\t    \"\"\"Takes in a list of tuples of (src, dst) paths and copies files.\n\t    Will create all necessary directories.\"\"\"\n", "    for file in files:\n\t        target_dir_name = os.path.dirname(file[1])\n\t        # will create all intermediate-level directories\n\t        if not os.path.exists(target_dir_name):\n\t            os.makedirs(target_dir_name)\n\t        shutil.copyfile(file[0], file[1])\n\t# URL helpers\n\t# ------------------------------------------------------------------------------------------\n\tdef is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n\t    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n", "    if not isinstance(obj, str) or not \"://\" in obj:\n\t        return False\n\t    if allow_file_urls and obj.startswith('file://'):\n\t        return True\n\t    try:\n\t        res = requests.compat.urlparse(obj)\n\t        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n\t            return False\n\t        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n\t        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n", "            return False\n\t    except:\n\t        return False\n\t    return True\n\tdef open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n\t    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n\t    assert num_attempts >= 1\n\t    assert not (return_filename and (not cache))\n\t    # Doesn't look like an URL scheme so interpret it as a local filename.\n\t    if not re.match('^[a-z]+://', url):\n", "        return url if return_filename else open(url, \"rb\")\n\t    # Handle file URLs.  This code handles unusual file:// patterns that\n\t    # arise on Windows:\n\t    #\n\t    # file:///c:/foo.txt\n\t    #\n\t    # which would translate to a local '/c:/foo.txt' filename that's\n\t    # invalid.  Drop the forward slash for such pathnames.\n\t    #\n\t    # If you touch this code path, you should test it on both Linux and\n", "    # Windows.\n\t    #\n\t    # Some internet resources suggest using urllib.request.url2pathname() but\n\t    # but that converts forward slashes to backslashes and this causes\n\t    # its own set of problems.\n\t    if url.startswith('file://'):\n\t        filename = urllib.parse.urlparse(url).path\n\t        if re.match(r'^/[a-zA-Z]:', filename):\n\t            filename = filename[1:]\n\t        return filename if return_filename else open(filename, \"rb\")\n", "    assert is_url(url)\n\t    # Lookup from cache.\n\t    if cache_dir is None:\n\t        cache_dir = make_cache_dir_path('downloads')\n\t    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n\t    if cache:\n\t        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n\t        if len(cache_files) == 1:\n\t            filename = cache_files[0]\n\t            return filename if return_filename else open(filename, \"rb\")\n", "    # Download.\n\t    url_name = None\n\t    url_data = None\n\t    with requests.Session() as session:\n\t        if verbose:\n\t            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n\t        for attempts_left in reversed(range(num_attempts)):\n\t            try:\n\t                with session.get(url) as res:\n\t                    res.raise_for_status()\n", "                    if len(res.content) == 0:\n\t                        raise IOError(\"No data received\")\n\t                    if len(res.content) < 8192:\n\t                        content_str = res.content.decode(\"utf-8\")\n\t                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n\t                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n\t                            if len(links) == 1:\n\t                                url = requests.compat.urljoin(url, links[0])\n\t                                raise IOError(\"Google Drive virus checker nag\")\n\t                        if \"Google Drive - Quota exceeded\" in content_str:\n", "                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n\t                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n\t                    url_name = match[1] if match else url\n\t                    url_data = res.content\n\t                    if verbose:\n\t                        print(\" done\")\n\t                    break\n\t            except KeyboardInterrupt:\n\t                raise\n\t            except:\n", "                if not attempts_left:\n\t                    if verbose:\n\t                        print(\" failed\")\n\t                    raise\n\t                if verbose:\n\t                    print(\".\", end=\"\", flush=True)\n\t    # Save to cache.\n\t    if cache:\n\t        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n\t        safe_name = safe_name[:min(len(safe_name), 128)]\n", "        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n\t        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n\t        os.makedirs(cache_dir, exist_ok=True)\n\t        with open(temp_file, \"wb\") as f:\n\t            f.write(url_data)\n\t        os.replace(temp_file, cache_file) # atomic\n\t        if return_filename:\n\t            return cache_file\n\t    # Return data as file object.\n\t    assert not return_filename\n", "    return io.BytesIO(url_data)\n"]}
