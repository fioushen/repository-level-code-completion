{"filename": "train_unet.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   train.py\n\t@Time    :   2023/05/12 19:24:02\n\t@Author  :   Wenbo Li\n\t@Desc    :   Main Class for training stable diffusion model\n\t\"\"\"\n\timport math\n\timport os\n", "import shutil\n\timport time\n\tfrom accelerate import Accelerator\n\tfrom accelerate.logging import get_logger\n\tfrom accelerate.utils import (\n\t    set_seed,\n\t    DummyOptim,\n\t    ProjectConfiguration,\n\t    DummyScheduler,\n\t    DeepSpeedPlugin,\n", ")\n\tfrom transformers import get_scheduler\n\timport logging\n\tfrom stable_diffusion.models.latent_diffusion import LatentDiffusion\n\tfrom utils.model_utils import build_models\n\tfrom utils.parse_args import load_config\n\tfrom utils.prepare_dataset import collate_fn, detransform, get_dataset, to_img\n\timport numpy as np\n\timport torch\n\timport torch.nn.functional as F\n", "from tqdm import tqdm\n\tfrom torch.utils.data import DataLoader, Dataset\n\tfrom diffusers import AutoencoderKL\n\tfrom torch.distributed.elastic.multiprocessing.errors import record\n\t# build environment\n\tos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\tos.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n\tos.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"\n\tlogger = get_logger(__name__)\n\tlogging.basicConfig(\n", "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n\t    datefmt=\"%m/%d/%Y %H:%M:%S\",\n\t    level=logging.INFO,\n\t)\n\tclass StableDiffusionTrainer:\n\t    def __init__(\n\t        self,\n\t        model: LatentDiffusion,\n\t        args,\n\t        cfg,\n", "        train_dataset,\n\t        eval_dataset,\n\t        collate_fn,\n\t    ):\n\t        # check params\n\t        assert train_dataset is not None, \"must specify an training dataset\"\n\t        assert (\n\t            eval_dataset is not None and cfg.train.log_interval > 0\n\t        ), \"if passed log_interval > 0, you must specify an evaluation dataset\"\n\t        self.model: LatentDiffusion = model\n", "        # test autoencoder\n\t        self.model.autoencoder = AutoencoderKL.from_pretrained(\n\t            \"runwayml/stable-diffusion-v1-5\",\n\t            subfolder=\"vae\",\n\t            cache_dir=\"data/pretrained\",\n\t        )\n\t        self.model.autoencoder.requires_grad_(False)\n\t        self.cfg = cfg\n\t        self.train_dataset: Dataset = train_dataset\n\t        self.eval_dataset: Dataset = eval_dataset\n", "        self.last_ckpt = None\n\t        # * 1. init accelerator\n\t        accelerator_log_kwcfg = {}\n\t        if cfg.log.with_tracking:\n\t            try:\n\t                accelerator_log_kwcfg[\"log_with\"] = cfg.log.report_to\n\t            except AttributeError:\n\t                print(\"need to specify report_to when passing in with_tracking=True\")\n\t            accelerator_log_kwcfg[\"logging_dir\"] = cfg.log.logging_dir\n\t        accelerator_project_config = ProjectConfiguration()\n", "        # check deepspeed\n\t        if cfg.train.use_deepspeed is True:\n\t            try:\n\t                import deepspeed\n\t            except ImportError as e:\n\t                raise ImportError(\n\t                    'You passed use_deepspeed=True, please install deepspeed by running `pip install deepspeed`, also deepspeed requies a matched cuda version, so you may need to run `conda install -c \"nvidia/label/cuda-11.5.0\" cuda-toolkit`, see https://anaconda.org/nvidia/cuda-toolkit for more options'\n\t                ) from e\n\t        self.accelerator = Accelerator(\n\t            gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n", "            **accelerator_log_kwcfg,\n\t            project_config=accelerator_project_config,\n\t            deepspeed_plugin=DeepSpeedPlugin(\n\t                zero_stage=2,\n\t                gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n\t                gradient_clipping=cfg.optim.max_grad_norm,\n\t                offload_optimizer_device=\"cpu\",\n\t                offload_param_device=\"cpu\",\n\t            )\n\t            if cfg.train.use_deepspeed is True\n", "            else None,\n\t        )\n\t        if cfg.log.with_tracking:\n\t            if cfg.log.report_to != \"wandb\":\n\t                raise NotImplementedError(\n\t                    \"Currently only support wandb, init trakcer for your platforms\"\n\t                )\n\t            if self.accelerator.is_main_process:\n\t                try:\n\t                    import wandb\n", "                except ImportError as e:\n\t                    raise ImportError(\n\t                        \"You passed with_tracking and report_to `wandb`, please install wandb by running `pip install wandb`\"\n\t                    ) from e\n\t                wandb_kwargs = {\n\t                    \"name\": f\"run_{time.strftime('%Y-%m-%d_%H:%M:%S', time.localtime())}\",\n\t                    \"notes\": \"train unet only\",\n\t                    \"group\": \"train unet\",\n\t                    \"tags\": [\"stable diffusion\", \"pytorch\"],\n\t                    \"entity\": \"liwenbo2099\",\n", "                    \"resume\": cfg.log.resume,\n\t                    \"save_code\": True,\n\t                    \"allow_val_change\": True,\n\t                }\n\t                self.accelerator.init_trackers(\n\t                    \"stable_diffusion_pytorch\",\n\t                    args,\n\t                    init_kwargs={\"wandb\": wandb_kwargs},\n\t                )\n\t                wandb.config.update(\n", "                    args, allow_val_change=True  # wandb_kwargs[\"allow_val_change\"]\n\t                )\n\t        logger.info(self.accelerator.state, main_process_only=False)\n\t        # * 2. set seed\n\t        if cfg.train.seed is not None:\n\t            set_seed(cfg.train.seed)\n\t        # * 4. build optimizer and lr_scheduler\n\t        self.optimizer = self.__build_optimizer(cfg.optim)\n\t        self.lr_scheduler = self.__build_lr_scheduler(cfg.optim)\n\t        # * 5. get dataset\n", "        self.train_dataloader = self.get_dataloader(\n\t            train=True,\n\t            dataset=self.train_dataset,\n\t            collate_fn=collate_fn,\n\t            batch_size=cfg.train.train_batch_size,\n\t            num_workers=cfg.dataset.dataloader_num_workers\n\t            or self.accelerator.num_processes,\n\t        )\n\t        self.eval_dataloader = self.get_dataloader(\n\t            train=False,\n", "            dataset=self.eval_dataset,\n\t            collate_fn=collate_fn,\n\t            batch_size=cfg.train.eval_batch_size,\n\t            num_workers=cfg.dataset.dataloader_num_workers\n\t            or self.accelerator.num_processes,\n\t        )\n\t        # * 5. Prepare everything with our `accelerator`.\n\t        (\n\t            self.model,\n\t            self.optimizer,\n", "            self.train_dataloader,\n\t            self.eval_dataloader,\n\t            self.lr_scheduler,\n\t        ) = self.accelerator.prepare(\n\t            self.model,\n\t            self.optimizer,\n\t            self.train_dataloader,\n\t            self.eval_dataloader,\n\t            self.lr_scheduler,\n\t        )\n", "        # * enable variable annotations so that we can easily debug\n\t        self.model: LatentDiffusion = self.model\n\t        self.train_dataloader: DataLoader = self.train_dataloader\n\t        self.eval_dataloader: DataLoader = self.eval_dataloader\n\t        # * 6. Move text_encoder and autoencoder to gpu and cast to weight_dtype\n\t        self.weight_dtype = torch.float32\n\t        if self.accelerator.mixed_precision == \"bf16\":\n\t            self.weight_dtype = torch.bfloat16\n\t        elif self.accelerator.mixed_precision == \"fp16\":\n\t            self.weight_dtype = torch.float16\n", "        self.model.text_encoder.to(self.accelerator.device, dtype=self.weight_dtype)\n\t        self.model.autoencoder.to(self.accelerator.device, dtype=self.weight_dtype)\n\t        self.model.unet.to(self.accelerator.device, dtype=self.weight_dtype)\n\t    def get_dataloader(\n\t        self, train, dataset, collate_fn, batch_size, num_workers\n\t    ) -> DataLoader:\n\t        return DataLoader(\n\t            dataset,\n\t            shuffle=train,\n\t            collate_fn=collate_fn,\n", "            batch_size=batch_size,\n\t            num_workers=num_workers,\n\t        )\n\t    def __build_optimizer(self, optim_cfg):\n\t        # Initialize the optimizer\n\t        if optim_cfg.use_8bit_adam:\n\t            try:\n\t                import bitsandbytes as bnb\n\t            except ImportError as e:\n\t                raise ImportError(\n", "                    \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n\t                ) from e\n\t            optimizer_cls = bnb.optim.AdamW8bit\n\t        else:\n\t            optimizer_cls = torch.optim.AdamW\n\t        # * code change to fit deepspeed\n\t        optimizer_cls = (\n\t            optimizer_cls\n\t            if self.accelerator.state.deepspeed_plugin is None\n\t            or \"optimizer\"\n", "            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n\t            else DummyOptim\n\t        )\n\t        return optimizer_cls(\n\t            # only train unet\n\t            self.model.unet.parameters(),\n\t            lr=optim_cfg.learning_rate,\n\t            weight_decay=optim_cfg.adam_weight_decay,\n\t        )\n\t    def __build_lr_scheduler(self, optim_cfg):\n", "        lr_scheduler = None\n\t        if (\n\t            self.accelerator.state.deepspeed_plugin is None\n\t            or \"scheduler\"\n\t            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n\t        ):\n\t            lr_scheduler = get_scheduler(\n\t                optim_cfg.scheduler_type,\n\t                optimizer=self.optimizer,\n\t                num_warmup_steps=optim_cfg.lr_warmup_steps\n", "                * self.cfg.train.gradient_accumulation_steps,\n\t                num_training_steps=self.cfg.train.max_train_steps\n\t                * self.cfg.train.gradient_accumulation_steps,\n\t            )\n\t        else:\n\t            lr_scheduler = DummyScheduler(\n\t                self.optimizer,\n\t                total_num_steps=self.cfg.train.max_train_steps,\n\t                warmup_num_steps=optim_cfg.lr_warmup_steps,\n\t            )\n", "        return lr_scheduler\n\t    def __resume_from_ckpt(self, ckpt_cfg):\n\t        \"resume from checkpoint or start a new train\"\n\t        ckpt_path = None\n\t        if ckpt_cfg.resume_from_checkpoint:\n\t            ckpt_path = os.path.basename(ckpt_cfg.resume_from_checkpoint)\n\t            if ckpt_cfg.resume_from_checkpoint == \"latest\":\n\t                # None, Get the most recent checkpoint or start from scratch\n\t                dirs = os.listdir(ckpt_cfg.ckpt_dir)\n\t                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n", "                dirs = sorted(\n\t                    dirs, key=lambda x: int(x.split(\"-\")[1])\n\t                )  # checkpoint-100 => 100\n\t                ckpt_path = dirs[-1] if len(dirs) > 0 else None\n\t        if ckpt_path is None:\n\t            self.accelerator.print(\n\t                f\"Checkpoint '{ckpt_cfg.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n\t            )\n\t            ckpt_cfg.resume_from_checkpoint = None\n\t        else:\n", "            self.accelerator.print(f\"Resuming from checkpoint {ckpt_path}\")\n\t            self.accelerator.load_state(os.path.join(ckpt_cfg.ckpt_dir, ckpt_path))\n\t        self.ckpt_path = ckpt_path\n\t    def __resume_train_state(self, train_cfg, ckpt_path):\n\t        \"\"\"\n\t        resume train steps and epochs\n\t        \"\"\"\n\t        # * Calculate train steps\n\t        # total batch num / accumulate steps => actual update steps per epoch\n\t        num_update_steps_per_epoch = math.ceil(\n", "            len(self.train_dataloader) / train_cfg.gradient_accumulation_steps\n\t        )\n\t        if train_cfg.max_train_steps is None:\n\t            train_cfg.max_train_steps = (\n\t                train_cfg.max_train_epochs * num_update_steps_per_epoch\n\t            )\n\t        else:\n\t            # override max_train_epochs\n\t            train_cfg.max_train_epochs = math.ceil(\n\t                train_cfg.max_train_steps / num_update_steps_per_epoch\n", "            )\n\t        # actual updated steps\n\t        self.global_step = int(ckpt_path.split(\"-\")[1]) if ckpt_path else 0\n\t        self.start_epoch = (\n\t            self.global_step // num_update_steps_per_epoch if ckpt_path else 0\n\t        )\n\t        # * change diffusers implementation: 20 % 6 = 2 = 2*(10 % 3)\n\t        self.resume_step = (\n\t            self.global_step\n\t            % (num_update_steps_per_epoch)\n", "            * train_cfg.gradient_accumulation_steps\n\t        )\n\t    def train(self):\n\t        cfg = self.cfg\n\t        # * 7. Resume training state and ckpt\n\t        self.__resume_from_ckpt(cfg.checkpoint)\n\t        self.__resume_train_state(cfg.train, self.ckpt_path)\n\t        total_batch_size = (\n\t            cfg.train.train_batch_size\n\t            * self.accelerator.num_processes\n", "            * cfg.train.gradient_accumulation_steps\n\t        )\n\t        self.checkpointing_steps = cfg.checkpoint.checkpointing_steps\n\t        if self.checkpointing_steps is not None and self.checkpointing_steps.isdigit():\n\t            self.checkpointing_steps = int(self.checkpointing_steps)\n\t        logger.info(\"****************Start Training******************\")\n\t        logger.info(f\"Total training data: {len(self.train_dataloader.dataset)}\")\n\t        if hasattr(self, \"eval_dataloader\") and self.eval_dataloader is not None:\n\t            logger.info(f\"Total eval data: {len(self.eval_dataloader.dataset)}\")\n\t        logger.info(f\"Total update steps: {cfg.train.max_train_steps}\")\n", "        logger.info(f\"Total Epochs: {cfg.train.max_train_epochs}\")\n\t        logger.info(f\"Total Batch size: {total_batch_size}\")\n\t        logger.info(f\"Resume from epoch={self.start_epoch}, step={self.resume_step}\")\n\t        logger.info(\"**********************************************\")\n\t        self.progress_bar = tqdm(\n\t            range(self.global_step, cfg.train.max_train_steps),\n\t            total=cfg.train.max_train_steps,\n\t            disable=not self.accelerator.is_main_process,\n\t            initial=self.global_step,  # @note: huggingface seemed to missed this, should first update to global step\n\t            desc=\"Step\",\n", "        )\n\t        self.model.train()\n\t        for epoch in range(self.start_epoch, cfg.train.max_train_epochs):\n\t            train_loss = 0\n\t            for step, batch in enumerate(self.train_dataloader):\n\t                # * Skip steps until we reach the resumed step\n\t                if (\n\t                    self.ckpt_path is not None\n\t                    and epoch == self.start_epoch\n\t                    and step < self.resume_step\n", "                ):\n\t                    if step % cfg.train.gradient_accumulation_steps == 0:\n\t                        # @note: huggingface seemed to missed this, global step should also be updated\n\t                        self.global_step += 1\n\t                        self.progress_bar.update(1)\n\t                    continue\n\t                with self.accelerator.accumulate(self.model.unet):\n\t                    loss = self.__one_step(batch)\n\t                    # gather loss across processes for logging\n\t                    avg_loss = self.accelerator.gather(\n", "                        loss.repeat(cfg.train.train_batch_size)\n\t                    ).mean()\n\t                    train_loss += avg_loss.item()\n\t                    # * 7. backward\n\t                    self.accelerator.backward(loss)\n\t                    if self.accelerator.sync_gradients:\n\t                        self.accelerator.clip_grad_norm_(\n\t                            self.model.unet.parameters(), cfg.optim.max_grad_norm\n\t                        )\n\t                    # * 8. update\n", "                    self.optimizer.step()\n\t                    self.lr_scheduler.step()\n\t                    self.optimizer.zero_grad()\n\t                # Checks if the accelerator has performed an optimization step behind the scenes\n\t                if self.accelerator.sync_gradients:\n\t                    self.progress_bar.update(1)\n\t                    self.global_step += 1\n\t                    if self.cfg.log.with_tracking:\n\t                        self.accelerator.log(\n\t                            {\n", "                                \"train_loss\": train_loss,\n\t                                \"lr\": self.lr_scheduler.get_last_lr()[0],\n\t                            },\n\t                            step=self.global_step,\n\t                        )\n\t                    train_loss = 0.0\n\t                    if (\n\t                        isinstance(self.checkpointing_steps, int)\n\t                        and self.global_step % self.checkpointing_steps == 0\n\t                    ):\n", "                        ckpt_path = os.path.join(\n\t                            cfg.checkpoint.ckpt_dir,\n\t                            f\"checkpoint-{self.global_step}\",\n\t                        )\n\t                        if (\n\t                            self.cfg.checkpoint.keep_last_only\n\t                            and self.accelerator.is_main_process\n\t                        ):\n\t                            # del last save path\n\t                            if self.last_ckpt is not None:\n", "                                shutil.rmtree(self.last_ckpt)\n\t                            self.last_ckpt = ckpt_path\n\t                        self.accelerator.save_state(ckpt_path)\n\t                        logger.info(f\"Saved state to {ckpt_path}\")\n\t                logs = {\n\t                    \"loss\": loss.detach().item(),\n\t                    \"lr\": self.lr_scheduler.get_last_lr()[0],\n\t                }\n\t                self.progress_bar.set_postfix(**logs)\n\t                if self.global_step >= cfg.train.max_train_steps:\n", "                    break\n\t                # =======================Evaluation==========================\n\t                if (\n\t                    self.global_step > 0\n\t                    and cfg.train.log_interval > 0\n\t                    and self.global_step % cfg.train.log_interval == 0\n\t                ):\n\t                    logger.info(\n\t                        f\"Evaluate on eval dataset [len: {len(self.eval_dataset)}]\"\n\t                    )\n", "                    self.model.eval()\n\t                    losses = []\n\t                    eval_bar = tqdm(\n\t                        self.eval_dataloader,\n\t                        disable=not self.accelerator.is_main_process,\n\t                    )\n\t                    for step, batch in enumerate(eval_bar):\n\t                        with torch.no_grad():\n\t                            loss = self.__one_step(batch)\n\t                        losses.append(\n", "                            self.accelerator.gather_for_metrics(\n\t                                loss.repeat(cfg.train.eval_batch_size)\n\t                            )\n\t                        )\n\t                    losses = torch.cat(losses)\n\t                    eval_loss = torch.mean(losses)\n\t                    logger.info(\n\t                        f\"global step {self.global_step}: eval_loss: {eval_loss}\"\n\t                    )\n\t                    if cfg.log.with_tracking:\n", "                        self.accelerator.log(\n\t                            {\n\t                                \"eval_loss\": eval_loss,\n\t                            },\n\t                            step=self.global_step,\n\t                        )\n\t                    # log image\n\t                    if self.cfg.log.log_image:\n\t                        prompt = \"a white cat wearing a hat\"\n\t                        sample = self.sample(prompt=prompt)\n", "                        if self.cfg.log.with_tracking:\n\t                            import wandb\n\t                            self.accelerator.log(\n\t                                {\n\t                                    \"sampled image\": wandb.Image(\n\t                                        sample, caption=prompt\n\t                                    ),\n\t                                },\n\t                                step=self.global_step,\n\t                            )\n", "                    self.model.train()  # back to train mode\n\t            # save ckpt for each epoch\n\t            if self.checkpointing_steps == \"epoch\":\n\t                ckpt_dir = f\"epoch_{epoch}\"\n\t                if cfg.checkpoint.ckpt_dir is not None:\n\t                    ckpt_dir = os.path.join(cfg.checkpoint.ckpt_dir, ckpt_dir)\n\t                if (\n\t                    self.cfg.checkpoint.keep_last_only\n\t                    and self.accelerator.is_main_process\n\t                ):\n", "                    # del last save path\n\t                    if self.last_ckpt is not None:\n\t                        shutil.rmtree(self.last_ckpt)\n\t                    self.last_ckpt = ckpt_path\n\t                self.accelerator.save_state(ckpt_path)\n\t                logger.info(f\"Saved state to {ckpt_path}\")\n\t        # end training\n\t        self.accelerator.wait_for_everyone()\n\t        if self.cfg.log.with_tracking:\n\t            self.accelerator.end_training()\n", "    def __one_step(self, batch: dict):\n\t        \"\"\"\n\t        __train_one_step: one diffusion backward step\n\t        Args:\n\t            - batch (dict):\n\t                  a batch of data, contains: pixel_values and input_ids\n\t        Returns:\n\t            - torch.Tensor:\n\t                  mse loss between sampled real noise and pred noise\n\t        \"\"\"\n", "        # * 1. encode image\n\t        latent_vector = self.model.autoencoder.encode(\n\t            batch[\"pixel_values\"].to(self.weight_dtype)\n\t        ).latent_dist.sample()\n\t        noise = torch.randn(latent_vector.shape).to(self.accelerator.device)\n\t        # * 2. Sample a random timestep for each image\n\t        timesteps = torch.randint(\n\t            self.model.noise_scheduler.noise_steps, (batch[\"pixel_values\"].shape[0],)\n\t        ).to(self.accelerator.device, dtype=torch.long)\n\t        # timesteps = timesteps.long()\n", "        # * 3. add noise to latent vector\n\t        x_t = self.model.noise_scheduler.add_noise(\n\t            original_samples=latent_vector, timesteps=timesteps, noise=noise\n\t        ).to(dtype=self.weight_dtype)\n\t        # * 4. get text encoding latent\n\t        tokenized_text = batch[\"input_ids\"]\n\t        # 90 % of the time we use the true text encoding, 10 % of the time we use an empty string\n\t        if np.random.random() < 0.1:\n\t            tokenized_text = self.model.text_encoder.tokenize(\n\t                [\"\"] * len(tokenized_text)\n", "            ).input_ids.to(self.accelerator.device)\n\t        text_condition = self.model.text_encoder.encode_text(\n\t            tokenized_text  # adding `.to(self.weight_dtype)` causes error...\n\t        )[0].to(self.weight_dtype)\n\t        # * 5. predict noise\n\t        pred_noise = self.model.pred_noise(\n\t            x_t, timesteps, text_condition, guidance_scale=self.cfg.train.guidance_scale\n\t        )\n\t        return F.mse_loss(pred_noise.float(), noise.float(), reduction=\"mean\")\n\t    def sample(\n", "        self,\n\t        image_size=64,\n\t        prompt: str = \"\",\n\t        guidance_scale: float = 7.5,\n\t        scale_factor=1.0,\n\t        save_dir: str = \"output\",\n\t    ):\n\t        \"Sample an image given prompt\"\n\t        # random noise\n\t        # to get the right latent size\n", "        img_util = torch.randn(size=(1, 3, image_size, image_size)).to(\n\t            self.accelerator.device, dtype=self.weight_dtype\n\t        )\n\t        noise = self.model.autoencoder.encode(img_util).latent_dist.sample()\n\t        noise = torch.rand_like(noise).to(\n\t            self.accelerator.device, dtype=self.weight_dtype\n\t        )\n\t        # tokenize prompt\n\t        tokenized_prompt = self.model.text_encoder.tokenize([prompt]).input_ids.to(\n\t            self.accelerator.device\n", "        )\n\t        context_emb = self.model.text_encoder.encode_text(tokenized_prompt)[0].to(\n\t            self.weight_dtype\n\t        )\n\t        x_0 = self.model.sample(\n\t            noised_sample=noise,\n\t            context_emb=context_emb,\n\t            guidance_scale=guidance_scale,\n\t            scale_factor=scale_factor,\n\t        )\n", "        sample = self.model.autoencoder.decode(x_0)\n\t        sample = detransform(sample.sample)\n\t        return to_img(sample, output_path=save_dir, name=\"unet_sample\")\n\t@record\n\tdef main():\n\t    args, cfg = load_config()\n\t    model = build_models(cfg.model, logger)\n\t    train_dataset = get_dataset(\n\t        cfg.dataset,\n\t        split=\"train\",\n", "        tokenizer=model.text_encoder.tokenizer,\n\t        logger=logger,\n\t    )\n\t    eval_dataset = get_dataset(\n\t        cfg.dataset,\n\t        split=\"validation\",\n\t        tokenizer=model.text_encoder.tokenizer,\n\t        logger=logger,\n\t    )\n\t    trainer = StableDiffusionTrainer(\n", "        model,\n\t        args,\n\t        cfg,\n\t        train_dataset,\n\t        eval_dataset,\n\t        collate_fn=collate_fn,\n\t    )\n\t    trainer.train()\n\tif __name__ == \"__main__\":\n\t    main()\n", "# to run without debug:\n\t# accelerate launch --config_file stable_diffusion/config/accelerate_config/deepspeed.yaml --main_process_port 29511 train_unet.py --use-deepspeed --with-tracking --log-image --max-train-steps 10000 --max-train-samples 700 --max-val-samples 50 --max-test-samples 50 --resume-from-checkpoint latest --ckpt-dir model/unet --learning-rate 5e-7 --keep-last-only\n"]}
{"filename": "train_autoencoder.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   train.py\n\t@Time    :   2023/05/12 19:24:02\n\t@Author  :   Wenbo Li\n\t@Desc    :   Main Class for training stable diffusion model\n\t\"\"\"\n\timport math\n\timport os\n", "import shutil\n\timport time\n\tfrom accelerate import Accelerator\n\tfrom accelerate.logging import get_logger\n\tfrom accelerate.utils import (\n\t    set_seed,\n\t    DummyOptim,\n\t    ProjectConfiguration,\n\t    DummyScheduler,\n\t    DeepSpeedPlugin,\n", ")\n\tfrom transformers import get_scheduler\n\timport logging\n\tfrom stable_diffusion.models.autoencoder import AutoEncoderKL\n\tfrom stable_diffusion.models.latent_diffusion import LatentDiffusion\n\tfrom stable_diffusion.modules.distributions import GaussianDistribution\n\tfrom utils.model_utils import build_models\n\tfrom utils.parse_args import load_config\n\tfrom utils.prepare_dataset import (\n\t    collate_fn,\n", "    detransform,\n\t    get_dataset,\n\t    get_transform,\n\t    sample_test_image,\n\t    to_img,\n\t)\n\timport numpy as np\n\timport torch\n\timport torch.nn.functional as F\n\tfrom tqdm import tqdm\n", "from torch.utils.data import DataLoader, Dataset\n\tfrom torch.distributed.elastic.multiprocessing.errors import record\n\tfrom PIL import Image\n\tfrom transformers import CLIPTokenizer\n\t# build environment\n\t# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n\tos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\tos.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n\tos.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"\n\tlogger = get_logger(__name__)\n", "logging.basicConfig(\n\t    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n\t    datefmt=\"%m/%d/%Y %H:%M:%S\",\n\t    level=logging.INFO,\n\t)\n\tclass AutoencoderKLTrainer:\n\t    def __init__(\n\t        self,\n\t        model: AutoEncoderKL,\n\t        args,\n", "        cfg,\n\t        train_dataset,\n\t        eval_dataset,\n\t        test_images,\n\t        collate_fn,\n\t    ):\n\t        # check params\n\t        assert train_dataset is not None, \"must specify an training dataset\"\n\t        assert (\n\t            eval_dataset is not None and cfg.train.log_interval > 0\n", "        ), \"if passed log_interval > 0, you must specify an evaluation dataset\"\n\t        self.model: AutoEncoderKL = model\n\t        # make sure model is in train mode\n\t        self.model.requires_grad_(True)\n\t        self.cfg = cfg\n\t        self.train_dataset: Dataset = train_dataset\n\t        self.eval_dataset: Dataset = eval_dataset\n\t        self.test_images = test_images\n\t        self.last_ckpt = None\n\t        # * 1. init accelerator\n", "        accelerator_log_kwcfg = {}\n\t        if cfg.log.with_tracking:\n\t            try:\n\t                accelerator_log_kwcfg[\"log_with\"] = cfg.log.report_to\n\t            except AttributeError:\n\t                print(\"need to specify report_to when passing in with_tracking=True\")\n\t            accelerator_log_kwcfg[\"logging_dir\"] = cfg.log.logging_dir\n\t        accelerator_project_config = ProjectConfiguration()\n\t        # check deepspeed\n\t        if cfg.train.use_deepspeed is True:\n", "            try:\n\t                import deepspeed\n\t            except ImportError as e:\n\t                raise ImportError(\n\t                    'You passed use_deepspeed=True, please install deepspeed by running `pip install deepspeed`, also deepspeed requies a matched cuda version, so you may need to run `conda install -c \"nvidia/label/cuda-11.5.0\" cuda-toolkit`, see https://anaconda.org/nvidia/cuda-toolkit for more options'\n\t                ) from e\n\t        self.accelerator = Accelerator(\n\t            gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n\t            **accelerator_log_kwcfg,\n\t            project_config=accelerator_project_config,\n", "            deepspeed_plugin=DeepSpeedPlugin(\n\t                zero_stage=2,\n\t                gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n\t                gradient_clipping=cfg.optim.max_grad_norm,\n\t                offload_optimizer_device=\"cpu\",\n\t                offload_param_device=\"cpu\",\n\t            )\n\t            if cfg.train.use_deepspeed is True\n\t            else None,\n\t        )\n", "        if cfg.log.with_tracking:\n\t            if cfg.log.report_to != \"wandb\":\n\t                raise NotImplementedError(\n\t                    \"Currently only support wandb, init trakcer for your platforms\"\n\t                )\n\t            if self.accelerator.is_main_process:\n\t                try:\n\t                    import wandb\n\t                except ImportError as e:\n\t                    raise ImportError(\n", "                        \"You passed with_tracking and report_to `wandb`, please install wandb by running `pip install wandb`\"\n\t                    ) from e\n\t                wandb_kwargs = {\n\t                    \"name\": f\"run_{time.strftime('%Y-%m-%d_%H:%M:%S', time.localtime())}\",\n\t                    \"notes\": \"train autoencoder\",\n\t                    \"group\": \"train autoencoder\",\n\t                    \"tags\": [\"stable diffusion\", \"pytorch\"],\n\t                    \"entity\": \"liwenbo2099\",\n\t                    \"resume\": cfg.log.resume,\n\t                    # \"id\": ,\n", "                    \"save_code\": True,\n\t                    \"allow_val_change\": True,\n\t                }\n\t                self.accelerator.init_trackers(\n\t                    \"stable_diffusion_pytorch\",\n\t                    args,\n\t                    init_kwargs={\"wandb\": wandb_kwargs},\n\t                )\n\t                wandb.config.update(\n\t                    args, allow_val_change=True  # wandb_kwargs[\"allow_val_change\"]\n", "                )\n\t        logger.info(self.accelerator.state, main_process_only=False)\n\t        # * 2. set seed\n\t        if cfg.train.seed is not None:\n\t            set_seed(cfg.train.seed)\n\t        # * 4. build optimizer and lr_scheduler\n\t        self.optimizer = self.__build_optimizer(cfg.optim)\n\t        self.lr_scheduler = self.__build_lr_scheduler(cfg.optim)\n\t        # * 5. get dataset\n\t        self.train_dataloader = self.get_dataloader(\n", "            train=True,\n\t            dataset=self.train_dataset,\n\t            collate_fn=collate_fn,\n\t            batch_size=cfg.train.train_batch_size,\n\t            num_workers=cfg.dataset.dataloader_num_workers\n\t            or self.accelerator.num_processes,\n\t        )\n\t        self.eval_dataloader = self.get_dataloader(\n\t            train=False,\n\t            dataset=self.eval_dataset,\n", "            collate_fn=collate_fn,\n\t            batch_size=cfg.train.eval_batch_size,\n\t            num_workers=cfg.dataset.dataloader_num_workers\n\t            or self.accelerator.num_processes,\n\t        )\n\t        # * 5. Prepare everything with our `accelerator`.\n\t        (\n\t            self.model,\n\t            self.optimizer,\n\t            self.train_dataloader,\n", "            self.eval_dataloader,\n\t            self.lr_scheduler,\n\t        ) = self.accelerator.prepare(\n\t            self.model,\n\t            self.optimizer,\n\t            self.train_dataloader,\n\t            self.eval_dataloader,\n\t            self.lr_scheduler,\n\t        )\n\t        # * enable variable annotations so that we can easily debug\n", "        self.model: AutoEncoderKL = self.model\n\t        self.train_dataloader: DataLoader = self.train_dataloader\n\t        self.eval_dataloader: DataLoader = self.eval_dataloader\n\t        # * 6. Move text_encoder and autoencoder to gpu and cast to weight_dtype\n\t        self.weight_dtype = torch.float32\n\t        if self.accelerator.mixed_precision == \"bf16\":\n\t            self.weight_dtype = torch.bfloat16\n\t        elif self.accelerator.mixed_precision == \"fp16\":\n\t            self.weight_dtype = torch.float16\n\t        self.model.to(self.accelerator.device, dtype=self.weight_dtype)\n", "    def get_dataloader(\n\t        self, train, dataset, collate_fn, batch_size, num_workers\n\t    ) -> DataLoader:\n\t        return DataLoader(\n\t            dataset,\n\t            shuffle=train,\n\t            collate_fn=collate_fn,\n\t            batch_size=batch_size,\n\t            num_workers=num_workers,\n\t        )\n", "    def __build_optimizer(self, optim_cfg):\n\t        # Initialize the optimizer\n\t        if optim_cfg.use_8bit_adam:\n\t            try:\n\t                import bitsandbytes as bnb\n\t            except ImportError as e:\n\t                raise ImportError(\n\t                    \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n\t                ) from e\n\t            optimizer_cls = bnb.optim.AdamW8bit\n", "        else:\n\t            optimizer_cls = torch.optim.AdamW\n\t        # * code change to fit deepspeed\n\t        optimizer_cls = (\n\t            optimizer_cls\n\t            if self.accelerator.state.deepspeed_plugin is None\n\t            or \"optimizer\"\n\t            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n\t            else DummyOptim\n\t        )\n", "        return optimizer_cls(\n\t            # only train unet\n\t            self.model.parameters(),\n\t            lr=optim_cfg.learning_rate,\n\t            weight_decay=optim_cfg.adam_weight_decay,\n\t        )\n\t    def __build_lr_scheduler(self, optim_cfg):\n\t        lr_scheduler = None\n\t        if (\n\t            self.accelerator.state.deepspeed_plugin is None\n", "            or \"scheduler\"\n\t            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n\t        ):\n\t            lr_scheduler = get_scheduler(\n\t                optim_cfg.scheduler_type,\n\t                optimizer=self.optimizer,\n\t                num_warmup_steps=optim_cfg.lr_warmup_steps\n\t                * self.cfg.train.gradient_accumulation_steps,\n\t                num_training_steps=self.cfg.train.max_train_steps\n\t                * self.cfg.train.gradient_accumulation_steps,\n", "            )\n\t        else:\n\t            lr_scheduler = DummyScheduler(\n\t                self.optimizer,\n\t                total_num_steps=self.cfg.train.max_train_steps,\n\t                warmup_num_steps=optim_cfg.lr_warmup_steps,\n\t            )\n\t        return lr_scheduler\n\t    def __resume_from_ckpt(self, ckpt_cfg):\n\t        \"resume from checkpoint or start a new train\"\n", "        ckpt_path = None\n\t        if ckpt_cfg.resume_from_checkpoint:\n\t            ckpt_path = os.path.basename(ckpt_cfg.resume_from_checkpoint)\n\t            if ckpt_cfg.resume_from_checkpoint == \"latest\":\n\t                # None, Get the most recent checkpoint or start from scratch\n\t                dirs = os.listdir(ckpt_cfg.ckpt_dir)\n\t                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n\t                dirs = sorted(\n\t                    dirs, key=lambda x: int(x.split(\"-\")[1])\n\t                )  # checkpoint-100 => 100\n", "                ckpt_path = dirs[-1] if len(dirs) > 0 else None\n\t        if ckpt_path is None:\n\t            self.accelerator.print(\n\t                f\"Checkpoint '{ckpt_cfg.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n\t            )\n\t            ckpt_cfg.resume_from_checkpoint = None\n\t        else:\n\t            self.accelerator.print(f\"Resuming from checkpoint {ckpt_path}\")\n\t            self.accelerator.load_state(os.path.join(ckpt_cfg.ckpt_dir, ckpt_path))\n\t        self.ckpt_path = ckpt_path\n", "    def __resume_train_state(self, train_cfg, ckpt_path):\n\t        \"\"\"\n\t        resume train steps and epochs\n\t        \"\"\"\n\t        # * Calculate train steps\n\t        # total batch num / accumulate steps => actual update steps per epoch\n\t        num_update_steps_per_epoch = math.ceil(\n\t            len(self.train_dataloader) / train_cfg.gradient_accumulation_steps\n\t        )\n\t        if train_cfg.max_train_steps is None:\n", "            train_cfg.max_train_steps = (\n\t                train_cfg.max_train_epochs * num_update_steps_per_epoch\n\t            )\n\t        else:\n\t            # override max_train_epochs\n\t            train_cfg.max_train_epochs = math.ceil(\n\t                train_cfg.max_train_steps / num_update_steps_per_epoch\n\t            )\n\t        # actual updated steps\n\t        self.global_step = int(ckpt_path.split(\"-\")[1]) if ckpt_path else 0\n", "        self.start_epoch = (\n\t            self.global_step // num_update_steps_per_epoch if ckpt_path else 0\n\t        )\n\t        # * change diffusers implementation: 20 % 6 = 2 = 2*(10 % 3)\n\t        self.resume_step = (\n\t            self.global_step\n\t            % (num_update_steps_per_epoch)\n\t            * train_cfg.gradient_accumulation_steps\n\t        )\n\t    def train(self):\n", "        cfg = self.cfg\n\t        # * 7. Resume training state and ckpt\n\t        self.__resume_from_ckpt(cfg.checkpoint)\n\t        self.__resume_train_state(cfg.train, self.ckpt_path)\n\t        total_batch_size = (\n\t            cfg.train.train_batch_size\n\t            * self.accelerator.num_processes\n\t            * cfg.train.gradient_accumulation_steps\n\t        )\n\t        self.checkpointing_steps = cfg.checkpoint.checkpointing_steps\n", "        if self.checkpointing_steps is not None and self.checkpointing_steps.isdigit():\n\t            self.checkpointing_steps = int(self.checkpointing_steps)\n\t        logger.info(\"****************Start Training******************\")\n\t        logger.info(f\"Total training data: {len(self.train_dataloader.dataset)}\")\n\t        if hasattr(self, \"eval_dataloader\") and self.eval_dataloader is not None:\n\t            logger.info(f\"Total eval data: {len(self.eval_dataloader.dataset)}\")\n\t        logger.info(f\"Total update steps: {cfg.train.max_train_steps}\")\n\t        logger.info(f\"Total Epochs: {cfg.train.max_train_epochs}\")\n\t        logger.info(f\"Total Batch size: {total_batch_size}\")\n\t        logger.info(f\"Resume from epoch={self.start_epoch}, step={self.resume_step}\")\n", "        logger.info(\"**********************************************\")\n\t        self.progress_bar = tqdm(\n\t            range(self.global_step, cfg.train.max_train_steps),\n\t            total=cfg.train.max_train_steps,\n\t            disable=not self.accelerator.is_main_process,\n\t            initial=self.global_step,  # @note: huggingface seemed to missed this, should first update to global step\n\t            desc=\"Step\",\n\t        )\n\t        self.model.train()\n\t        for epoch in range(self.start_epoch, cfg.train.max_train_epochs):\n", "            train_loss = 0\n\t            for step, batch in enumerate(self.train_dataloader):\n\t                # * Skip steps until we reach the resumed step\n\t                if (\n\t                    self.ckpt_path is not None\n\t                    and epoch == self.start_epoch\n\t                    and step < self.resume_step\n\t                ):\n\t                    if step % cfg.train.gradient_accumulation_steps == 0:\n\t                        # @note: huggingface seemed to missed this, global step should also be updated\n", "                        self.global_step += 1\n\t                        self.progress_bar.update(1)\n\t                    continue\n\t                with self.accelerator.accumulate(self.model):\n\t                    loss = self.__one_step(batch)\n\t                    # gather loss across processes for logging\n\t                    avg_loss = self.accelerator.gather(\n\t                        loss.repeat(cfg.train.train_batch_size)\n\t                    ).mean()\n\t                    train_loss += avg_loss.item()\n", "                    # * 7. backward\n\t                    self.accelerator.backward(loss)\n\t                    if self.accelerator.sync_gradients:\n\t                        self.accelerator.clip_grad_norm_(\n\t                            self.model.parameters(), cfg.optim.max_grad_norm\n\t                        )\n\t                    # * 8. update\n\t                    self.optimizer.step()\n\t                    self.lr_scheduler.step()\n\t                    self.optimizer.zero_grad()\n", "                # Checks if the accelerator has performed an optimization step behind the scenes\n\t                if self.accelerator.sync_gradients:\n\t                    self.progress_bar.update(1)\n\t                    self.global_step += 1\n\t                    if self.cfg.log.with_tracking:\n\t                        self.accelerator.log(\n\t                            {\n\t                                \"train_loss\": train_loss,\n\t                                \"lr\": self.lr_scheduler.get_last_lr()[0],\n\t                            },\n", "                            step=self.global_step,\n\t                        )\n\t                    train_loss = 0.0\n\t                    if (\n\t                        isinstance(self.checkpointing_steps, int)\n\t                        and self.global_step % self.checkpointing_steps == 0\n\t                    ):\n\t                        ckpt_path = os.path.join(\n\t                            cfg.checkpoint.ckpt_dir, f\"checkpoint-{self.global_step}\"\n\t                        )\n", "                        if (\n\t                            self.cfg.checkpoint.keep_last_only\n\t                            and self.accelerator.is_main_process\n\t                        ):\n\t                            # del last save path\n\t                            if self.last_ckpt is not None:\n\t                                shutil.rmtree(self.last_ckpt)\n\t                            self.last_ckpt = ckpt_path\n\t                            logger.info(f\"self.savepath={ckpt_path}\")\n\t                        # wait main process handle dir del and create\n", "                        self.accelerator.wait_for_everyone()\n\t                        # @note: when using deepspeed, we can't use is_main_process, or it will get stucked\n\t                        self.accelerator.save_state(ckpt_path)\n\t                        logger.info(f\"Saved state to {ckpt_path}\")\n\t                logs = {\n\t                    \"loss\": loss.detach().item(),\n\t                    \"lr\": self.lr_scheduler.get_last_lr()[0],\n\t                }\n\t                self.progress_bar.set_postfix(**logs)\n\t                if self.global_step >= cfg.train.max_train_steps:\n", "                    break\n\t                # =======================Evaluation==========================\n\t                if (\n\t                    self.global_step > 0\n\t                    and cfg.train.log_interval > 0\n\t                    and (self.global_step + 1) % cfg.train.log_interval == 0\n\t                ):\n\t                    logger.info(\n\t                        f\"Evaluate on eval dataset [len: {len(self.eval_dataset)}]\"\n\t                    )\n", "                    self.model.eval()\n\t                    losses = []\n\t                    eval_bar = tqdm(\n\t                        self.eval_dataloader,\n\t                        disable=not self.accelerator.is_main_process,\n\t                    )\n\t                    for step, batch in enumerate(eval_bar):\n\t                        with torch.no_grad():\n\t                            loss = self.__one_step(batch)\n\t                        losses.append(\n", "                            self.accelerator.gather_for_metrics(\n\t                                loss.repeat(cfg.train.eval_batch_size)\n\t                            )\n\t                        )\n\t                    losses = torch.cat(losses)\n\t                    eval_loss = torch.mean(losses)\n\t                    logger.info(\n\t                        f\"global step {self.global_step}: eval_loss: {eval_loss}\"\n\t                    )\n\t                    if cfg.log.with_tracking:\n", "                        self.accelerator.log(\n\t                            {\n\t                                \"eval_loss\": eval_loss,\n\t                            },\n\t                            step=self.global_step,\n\t                        )\n\t                    # log image\n\t                    if cfg.log.log_image:\n\t                        self.log_image()\n\t                    self.model.train()  # back to train mode\n", "            # save ckpt for each epoch\n\t            if self.checkpointing_steps == \"epoch\":\n\t                ckpt_path = f\"epoch_{epoch}\"\n\t                if cfg.checkpoint.ckpt_dir is not None:\n\t                    ckpt_path = os.path.join(cfg.ckpt_dir, ckpt_path, \"autoencoder\")\n\t                if (\n\t                    self.cfg.checkpoint.keep_last_only\n\t                    and self.accelerator.is_main_process\n\t                ):\n\t                    # del last save path\n", "                    if self.last_ckpt is not None:\n\t                        shutil.rmtree(self.last_ckpt)\n\t                    self.last_ckpt = ckpt_path\n\t                # @note: when using deepspeed, we can't use is_main_process, or it will get stucked\n\t                self.accelerator.save_state(ckpt_path)\n\t                logger.info(f\"Saved state to {ckpt_path}\")\n\t        # end training\n\t        self.accelerator.wait_for_everyone()\n\t        if self.cfg.log.with_tracking:\n\t            self.accelerator.end_training()\n", "    def __one_step(self, batch: dict):\n\t        \"\"\"\n\t        __train_one_step: one diffusion backward step\n\t        Args:\n\t            - batch (dict):\n\t                  a batch of data, contains: pixel_values and input_ids\n\t        Returns:\n\t            - torch.Tensor:\n\t                  recon loss + kl loss\n\t        \"\"\"\n", "        # * 1. encode image\n\t        img = batch[\"pixel_values\"].to(self.weight_dtype)\n\t        dist: GaussianDistribution = self.model.encode(img).latent_dist\n\t        latent_vector = dist.sample()\n\t        recon_image = self.model.decode(latent_vector)\n\t        recon_loss = F.mse_loss(img.float(), recon_image.float(), reduction=\"mean\")\n\t        kl_loss = dist.kl()[0].to(dtype=torch.float32)  # recon loss is float32\n\t        # @ recon loss is float32, pass float16 loss will raise bias correction error in deepspeed cpu adam\n\t        return recon_loss + self.cfg.model.autoencoder.kl_weight * kl_loss\n\t    def recon(self, image):\n", "        image = image.unsqueeze(0).to(\n\t            self.accelerator.device, dtype=self.weight_dtype\n\t        )  # [1, ...]\n\t        latent_vector = self.model.encode(image).latent_dist.sample()\n\t        recon_latent = self.model.decode(latent_vector)\n\t        recon_digit = detransform(recon_latent)\n\t        return to_img(recon_digit, output_path=\"output\", name=\"autoencoder\")\n\t    def log_image(self):\n\t        recons = [self.recon(img) for img in self.test_images]\n\t        if self.cfg.log.with_tracking:\n", "            import wandb\n\t            self.accelerator.log(\n\t                {\n\t                    \"original_imgs\": [wandb.Image(img) for img in self.test_images],\n\t                    \"recon_imgs\": [wandb.Image(recon) for recon in recons],\n\t                },\n\t                step=self.global_step,\n\t            )\n\t@record\n\tdef main():\n", "    args, cfg = load_config()\n\t    model = AutoEncoderKL(cfg.model.autoencoder)\n\t    tokenizer = CLIPTokenizer.from_pretrained(\n\t        \"runwayml/stable-diffusion-v1-5\",\n\t        subfolder=\"tokenizer\",\n\t        use_fast=False,\n\t        cache_dir=\"data/pretrained\",\n\t    )\n\t    train_dataset = get_dataset(\n\t        cfg.dataset,\n", "        split=\"train\",\n\t        tokenizer=tokenizer,\n\t        logger=logger,\n\t    )\n\t    eval_dataset = get_dataset(\n\t        cfg.dataset,\n\t        split=\"validation\",\n\t        tokenizer=tokenizer,\n\t        logger=logger,\n\t    )\n", "    test_images = sample_test_image(\n\t        cfg.dataset,\n\t        split=\"test\",\n\t        tokenizer=tokenizer,\n\t        logger=logger,\n\t        num=10,\n\t    )\n\t    trainer = AutoencoderKLTrainer(\n\t        model,\n\t        args,\n", "        cfg,\n\t        train_dataset,\n\t        eval_dataset,\n\t        test_images=test_images,\n\t        collate_fn=collate_fn,\n\t    )\n\t    trainer.train()\n\tif __name__ == \"__main__\":\n\t    main()\n\t# to run without debug:\n", "# accelerate launch --config_file stable_diffusion/config/accelerate_config/deepspeed.yaml --main_process_port 29511 train_autoencoder.py --use-deepspeed --with-tracking --log-image --max-train-steps 10000 --max-train-samples 700 --max-val-samples 50 --max-test-samples 50 --resume-from-checkpoint latest --ckpt-dir model/autoencoder --learning-rate 1e-4\n"]}
{"filename": "trainer_args.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   trainer_args.py\n\t@Time    :   2023/05/26 20:11:49\n\t@Author  :   Wenbo Li\n\t@Desc    :   Args for logging, training, optimizer, scheduler, etc.\n\t\"\"\"\n\tfrom dataclasses import dataclass, field\n\tfrom typing import Optional\n", "from stable_diffusion.dataclass import BaseDataclass\n\t@dataclass\n\tclass LogConfig(BaseDataclass):\n\t    logging_dir: str = field(default=\"logs\", metadata={\"help\": \"log directory\"})\n\t    with_tracking: bool = field(\n\t        default=False, metadata={\"help\": \"whether enable tracker\"}\n\t    )\n\t    report_to: str = field(\n\t        default=\"wandb\",\n\t        metadata={\"help\": \"tracker to use, only enabled when passed in --with_tracker\"},\n", "    )\n\t    resume: bool = field(\n\t        default=False, metadata={\"help\": \"whether resume from latest run\"}\n\t    )\n\t    log_image: bool = field(\n\t        default=False, metadata={\"help\": \"whether test image gen at each evaluation\"}\n\t    )\n\t    test_image: str = field(\n\t        default=\"data/test_images/test01.png\",\n\t        metadata={\"help\": \"test image path for log_image\"},\n", "    )\n\t@dataclass\n\tclass TrainConfig(BaseDataclass):\n\t    seed: int = field(default=42, metadata={\"help\": \"seed argument\"})\n\t    max_train_steps: int = field(\n\t        default=20000,\n\t        metadata={\"help\": \"total train steps, if provided, overrides max_train_epochs\"},\n\t    )\n\t    max_train_epochs: int = field(default=100, metadata={\"help\": \"max train epochs\"})\n\t    train_batch_size: int = field(\n", "        default=8, metadata={\"help\": \"train batch size per processor\"}\n\t    )\n\t    eval_batch_size: int = field(\n\t        default=8, metadata={\"help\": \"eval batch size per processor\"}\n\t    )\n\t    log_interval: int = field(\n\t        default=100,\n\t        metadata={\n\t            \"help\": \"do evaluation every n steps, default 0 means no evaluation during training\"\n\t        },\n", "    )\n\t    gradient_accumulation_steps: int = field(\n\t        default=4, metadata={\"help\": \"gradient accumulation steps\"}\n\t    )\n\t    use_deepspeed: bool = field(\n\t        default=False, metadata={\"help\": \"whether use deepspeed\"}\n\t    )\n\t    guidance_scale: float = field(\n\t        default=7.5, metadata={\"help\": \"guidance scale for classifier free guidance\"}\n\t    )\n", "@dataclass\n\tclass OptimConfig(BaseDataclass):\n\t    learning_rate: float = field(\n\t        default=4e-5, metadata={\"help\": \"learning rate argument\"}\n\t    )\n\t    adam_weight_decay: float = field(\n\t        default=0.1, metadata={\"help\": \"Adam weight decay argument\"}\n\t    )\n\t    use_8bit_adam: bool = field(\n\t        default=False, metadata={\"help\": \"Use 8-bit Adam argument\"}\n", "    )\n\t    max_grad_norm: float = field(\n\t        default=0.1, metadata={\"help\": \"max grad norm argument\"}\n\t    )\n\t    scheduler_type: str = field(\n\t        default=\"linear\", metadata={\"help\": \"scheduler type argument\"}\n\t    )\n\t    lr_warmup_steps: int = field(\n\t        default=500, metadata={\"help\": \"learning rate warm-up steps argument\"}\n\t    )\n", "# below are deprecated, now we use dataclass\n\tdef add_distributed_training_args(parser):\n\t    train_group = parser.add_argument_group(\"train\")\n\t    train_group.add_argument(\n\t        \"--logging_dir\", type=str, default=\"logs\", help=\"log directory\"\n\t    )\n\t    train_group.add_argument(\n\t        \"--with_tracker\",\n\t        type=str,\n\t        default=None,\n", "    )\n\t    train_group.add_argument(\"--report_to\", type=int, default=0, help=\"seed argument\")\n\t    train_group.add_argument(\"--seed\", type=int, default=0)\n\t    train_group.add_argument(\n\t        \"--train_batch_size\",\n\t        type=int,\n\t        default=8,\n\t    )\n\t    train_group.add_argument(\n\t        \"--max_train_steps\",\n", "        type=int,\n\t        default=20000,\n\t        help=\"total train steps, if provided, overrides max_train_epochs\",\n\t    )\n\t    train_group.add_argument(\n\t        \"--max_train_epochs\",\n\t        type=int,\n\t        default=100,\n\t        help=\"max train epochs, orverides by max_training_steps\",\n\t    )\n", "    train_group.add_argument(\n\t        \"--eval_batch_size\",\n\t        type=int,\n\t        default=1,\n\t    )\n\t    train_group.add_argument(\n\t        \"--gradient_accumulation_steps\",\n\t        type=int,\n\t        default=1,\n\t    )\n", "    return train_group\n\tdef add_optimization_args(parser):\n\t    optim_group = parser.add_argument_group(\"optim\")\n\t    optim_group.add_argument(\n\t        \"--learning_rate\",\n\t        type=float,\n\t        default=1e-4,\n\t    )\n\t    optim_group.add_argument(\"--adam_weight_decay\", type=float, default=0.1)\n\t    optim_group.add_argument(\n", "        \"--use_8bit_adam\",\n\t        action=\"store_true\",\n\t        default=False,\n\t    )\n\t    return optim_group\n\tdef add_lr_scheduler_args(parser):\n\t    lr_scheduler_group = parser.add_argument_group(\"lr_scheduler\")\n\t    lr_scheduler_group.add_argument(\n\t        \"--type\",\n\t        type=str,\n", "        default=\"linear\",\n\t    )\n\t    lr_scheduler_group.add_argument(\n\t        \"--lr_warmup_steps\",\n\t        type=int,\n\t        default=0,\n\t    )\n\t    return lr_scheduler_group\n"]}
{"filename": "test/test_args.py", "chunked_list": ["import argparse\n\tfrom dataclasses import dataclass, field, fields\n\tfrom omegaconf import DictConfig, OmegaConf\n\t# Define data classes\n\t@dataclass\n\tclass Train:\n\t    a1: int = field(default=1)\n\t@dataclass\n\tclass Val:\n\t    a2: int = field(default=2)\n", "# Create an argument parser\n\tparser = argparse.ArgumentParser()\n\t# Get a list of data classes\n\tdata_classes = [Train, Val]\n\t# Create argument groups and arguments for each data class\n\tfor data_class in data_classes:\n\t    group = parser.add_argument_group(data_class.__name__.lower())\n\t    for field_info in fields(data_class):\n\t        arg_name = \"--\" + field_info.name\n\t        arg_type = field_info.type\n", "        default_value = field_info.default\n\t        group.add_argument(arg_name, type=arg_type, default=default_value)\n\t# Parse the command-line arguments\n\targs = parser.parse_args()\n\t# Convert data classes to nested DictConfigs\n\tcfg = DictConfig({})\n\tfor data_class in data_classes:\n\t    data_instance = data_class()\n\t    for field_info in fields(data_class):\n\t        field_name = field_info.name\n", "        field_value = getattr(args, field_name)\n\t        setattr(data_instance, field_name, field_value)\n\t    group_name = data_class.__name__.lower()\n\t    group_config = OmegaConf.structured(data_instance)\n\t    cfg[group_name] = group_config\n\t# Access the arguments using the desired structure\n\tprint(cfg.train.a1)  # Output: 1\n\tprint(cfg.val.a2)  # Output: 2\n"]}
{"filename": "test/test_load_data.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   load_data.py\n\t@Time    :   2023/05/27 12:20:44\n\t@Author  :   Wenbo Li\n\t@Desc    :   test connection and path, If load dataset failed, you can also try this script\n\t\"\"\"\n\tfrom datasets import load_dataset\n\timport os\n", "# from huggingface_hub import snapshot_download\n\tload_dataset(\n\t    \"poloclub/diffusiondb\",\n\t    \"2m_first_100k\",\n\t    cache_dir=os.path.join(\"data/dataset\", \"poloclub/diffusiondb\"),\n\t)\n"]}
{"filename": "utils/checkpointing_args.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   checkpointing_args.py\n\t@Time    :   2023/05/26 20:09:15\n\t@Author  :   Wenbo Li\n\t@Desc    :   dataclass to store arguments for ckpt management\n\t\"\"\"\n\tfrom dataclasses import dataclass, field\n\tfrom typing import Optional\n", "from stable_diffusion.dataclass import BaseDataclass\n\t@dataclass\n\tclass CheckpointConfig(BaseDataclass):\n\t    keep_last_only: bool = field(\n\t        default=False,\n\t        metadata={\"help\": \"whether only keep the last ckpt\"},\n\t    )\n\t    ckpt_dir: str = field(\n\t        default=\"model\",\n\t        metadata={\"help\": \"dir to save and load checkpoints\"},\n", "    )\n\t    resume_from_checkpoint: Optional[str] = field(\n\t        default=None,\n\t        metadata={\n\t            \"help\": \"dir to load checkpoints from， None refers to a new run, pass ltest for a latest resume\"\n\t        },\n\t    )\n\t    checkpointing_steps: Optional[str] = field(\n\t        default=100,\n\t        metadata={\n", "            \"help\": \"Whether the various states should be saved at the end of every n steps\",\n\t        },\n\t    )\n\t# deprecated\n\tdef add_checkpoint_args(parser):\n\t    checkpoint_group = parser.add_argument_group(\"checkpoint\")\n\t    checkpoint_group.add_argument(\n\t        \"--resume_from_checkpoint\",\n\t        type=str,\n\t        default=None,\n", "        help=\"dir to load checkpoints from\",\n\t    )\n\t    checkpoint_group.add_argument(\n\t        \"--output_dir\",\n\t        type=str,\n\t        default=\"dir to save and load checkpoints\",\n\t    )\n"]}
{"filename": "utils/model_utils.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   model_utils.py\n\t@Time    :   2023/05/26 20:08:47\n\t@Author  :   Wenbo Li\n\t@Desc    :   util script to load and build models\n\t\"\"\"\n\tfrom stable_diffusion.models.autoencoder import AutoEncoderKL\n\tfrom stable_diffusion.models.clip_model import CLIPModel\n", "from stable_diffusion.models.latent_diffusion import LatentDiffusion\n\tfrom stable_diffusion.models.scheduler import DDPMScheduler\n\tfrom stable_diffusion.models.unet import UNetModel\n\t# deprecated\n\tdef add_model_args(parser):\n\t    model_group = parser.add_argument_group(\"model\")\n\t    UNetModel.add_unet_args(model_group)\n\t    DDPMScheduler.add_ddpm_args(model_group)\n\t    CLIPModel.add_clip_args(model_group)\n\t    AutoEncoderKL.add_autoencoder_args(model_group)\n", "    return model_group\n\tdef build_models(model_cfg, logger=None):\n\t    noise_scheduler = DDPMScheduler(model_cfg.ddpm)\n\t    unet = UNetModel(\n\t        model_cfg.autoencoder.latent_channels,\n\t        model_cfg.autoencoder.groups,\n\t        model_cfg.unet,\n\t    )\n\t    text_encoder = CLIPModel(model_cfg.clip)\n\t    text_encoder.requires_grad_(False)\n", "    autoencoder = AutoEncoderKL(model_cfg.autoencoder)\n\t    count_params(unet, logger=logger)\n\t    count_params(text_encoder, logger=logger)\n\t    count_params(autoencoder, logger=logger)\n\t    return LatentDiffusion(\n\t        unet,\n\t        autoencoder,\n\t        text_encoder,\n\t        noise_scheduler,\n\t    )\n", "def count_params(model, trainable_only=True, logger=None):\n\t    \"\"\"\n\t    Copied from original stable diffusion code [ldm/util.py](https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/util.py#L71)\n\t    Calculate model's total param count. If trainable_only is True then count only those requiring grads\n\t    \"\"\"\n\t    def numel(p):\n\t        return p.numel()\n\t    total_params = sum(\n\t        numel(p) for p in model.parameters() if not trainable_only or p.requires_grad\n\t    )\n", "    if logger is not None:\n\t        logger.info(\n\t            f\"{model.__class__.__name__} has {total_params * 1.e-6:.2f} M {'trainable' if trainable_only else ''} params.\"\n\t        )\n\t    return total_params\n"]}
{"filename": "utils/prepare_dataset.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   prepare_dataset.py\n\t@Time    :   2023/05/26 20:07:10\n\t@Author  :   Wenbo Li\n\t@Desc    :   code for loading and transform txt2img dataset\n\t\"\"\"\n\timport os\n\timport random\n", "from typing import Optional\n\timport numpy as np\n\timport torch\n\tfrom torchvision import transforms\n\tfrom huggingface_hub import snapshot_download\n\tfrom datasets import load_dataset\n\tfrom transformers import CLIPTokenizer\n\tfrom dataclasses import dataclass, field\n\tfrom stable_diffusion.dataclass import BaseDataclass\n\tfrom PIL import Image\n", "@dataclass\n\tclass DatasetConfig(BaseDataclass):\n\t    dataset: str = field(\n\t        default=\"poloclub/diffusiondb\",\n\t        metadata={\"help\": \"name of the dataset to use.\"},\n\t    )\n\t    subset: Optional[str] = field(\n\t        default=None,  # \"2m_first_10k\",\n\t        metadata={\"help\": \"subset of the dataset to use.\"},\n\t    )\n", "    # dataset: str = field(\n\t    #     default=\"lambdalabs/pokemon-blip-captions\",\n\t    # )\n\t    data_dir: str = field(\n\t        default=\"data/dataset\",\n\t        metadata={\"help\": \"Cache directory to store loaded dataset.\"},\n\t    )\n\t    dataloader_num_workers: int = field(\n\t        default=4, metadata={\"help\": \"number of workers for the dataloaders.\"}\n\t    )\n", "    resolution: int = field(default=64, metadata={\"help\": \"resolution of the images.\"})\n\t    center_crop: bool = field(\n\t        default=True, metadata={\"help\": \"whether to apply center cropping.\"}\n\t    )\n\t    random_flip: bool = field(\n\t        default=False, metadata={\"help\": \"whether to apply random flipping.\"}\n\t    )\n\t    max_train_samples: Optional[int] = field(\n\t        default=9000, metadata={\"help\": \"max number of training samples to load.\"}\n\t    )\n", "    max_val_samples: Optional[int] = field(\n\t        default=500, metadata={\"help\": \"max number of validation samples to load.\"}\n\t    )\n\t    max_test_samples: Optional[int] = field(\n\t        default=500, metadata={\"help\": \"max number of test samples to load.\"}\n\t    )\n\tdef add_dataset_args(parser):\n\t    dataset_group = parser.add_argument_group(\"dataset\")\n\t    dataset_group.add_argument(\n\t        \"--dataset\",\n", "        type=str,\n\t        default=\"poloclub/diffusiondb\",\n\t    )\n\t    dataset_group.add_argument(\n\t        \"--subset\",\n\t        type=str,\n\t        default=\"2m_first_100k\",\n\t    )\n\t    dataset_group.add_argument(\n\t        \"--data_dir\",\n", "        type=str,\n\t        default=\"data\",\n\t    )\n\t    dataset_group.add_argument(\n\t        \"--resolution\",\n\t        type=int,\n\t        default=64,\n\t    )\n\t    dataset_group.add_argument(\n\t        \"--center_crop\",\n", "        type=bool,\n\t        default=True,\n\t    )\n\t    dataset_group.add_argument(\n\t        \"--random_flip\",\n\t        type=bool,\n\t        default=True,\n\t    )\n\tdef collate_fn(examples):\n\t    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n", "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n\t    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n\t    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n\tdef tokenize_captions(examples, tokenizer, is_train=True):\n\t    captions = []\n\t    for caption in examples:\n\t        if isinstance(caption, str):\n\t            captions.append(caption)\n\t        elif isinstance(caption, (list, np.ndarray)):\n\t            # take a random caption if there are multiple\n", "            captions.append(random.choice(caption) if is_train else caption[0])\n\t        else:\n\t            raise ValueError(\n\t                \"Caption column should contain either strings or lists of strings.\"\n\t            )\n\t    inputs = tokenizer(\n\t        captions,\n\t        max_length=tokenizer.model_max_length,\n\t        padding=\"max_length\",\n\t        truncation=True,\n", "        return_tensors=\"pt\",\n\t    )\n\t    return inputs.input_ids\n\tdef get_transform(resolution, random_flip, center_crop):\n\t    return transforms.Compose(\n\t        [\n\t            transforms.Resize(\n\t                resolution, interpolation=transforms.InterpolationMode.BILINEAR\n\t            ),\n\t            transforms.CenterCrop(resolution)\n", "            if center_crop\n\t            else transforms.RandomCrop(resolution),\n\t            transforms.RandomHorizontalFlip()\n\t            if random_flip\n\t            else transforms.Lambda(lambda x: x),\n\t            transforms.ToTensor(),\n\t            transforms.Normalize([0.5], [0.5]),\n\t        ]\n\t    )\n\tdef detransform(latent: torch.Tensor):\n", "    latent = latent.squeeze().cpu().detach().numpy()\n\t    latent = np.transpose(latent, (1, 2, 0))  # [c,h,w] -> [h,w,c]\n\t    latent = (latent + 1) / 2\n\t    latent = np.clip(latent, 0, 1)\n\t    return (latent * 255).astype(np.uint8)\n\tdef to_img(digit_img, output_path: str = \"\", name=\"sample\"):\n\t    img = Image.fromarray(digit_img.astype(np.uint8))\n\t    img.save(os.path.join(output_path, f\"{name}.png\"))\n\t    return img\n\tdef get_dataset(\n", "    args,\n\t    split: str = \"train\",\n\t    tokenizer: CLIPTokenizer = None,\n\t    logger=None,\n\t):\n\t    # check params\n\t    assert tokenizer is not None, \"you need to specify a tokenizer\"\n\t    assert split in {\n\t        \"train\",\n\t        \"validation\",\n", "        \"test\",\n\t    }, \"split should be one of train, validation, test\"\n\t    # most of the txt2img datasets are not splited into train, validation and test, manually split it\n\t    dataset = load_dataset(\n\t        args.dataset,\n\t        args.subset,\n\t        cache_dir=os.path.join(args.data_dir, args.dataset),\n\t    )[\"train\"]\n\t    if args.max_train_samples is not None and split == \"train\":\n\t        if args.max_train_samples < len(dataset):\n", "            dataset = dataset.select(range(args.max_train_samples))\n\t        elif logger is not None:\n\t            logger.info(\n\t                f\"max_train_samples({args.max_train_samples}) is larger than the number of train samples({len(dataset)})\"\n\t            )\n\t    if args.max_val_samples is not None and split == \"validation\":\n\t        if args.max_train_samples + args.max_val_samples < len(dataset):\n\t            dataset = dataset.select(\n\t                range(\n\t                    args.max_train_samples,\n", "                    args.max_train_samples + args.max_val_samples,\n\t                )\n\t            )\n\t        elif logger is not None:\n\t            logger.info(\n\t                f\"max_val_samples({args.max_val_samples}) is larger than the number of val samples({len(dataset)})\"\n\t            )\n\t    if args.max_test_samples is not None and split == \"test\":\n\t        if args.max_train_samples + args.max_val_samples + args.max_test_samples < len(\n\t            dataset\n", "        ):\n\t            dataset = dataset.select(\n\t                range(\n\t                    args.max_train_samples + args.max_val_samples,\n\t                    args.max_train_samples\n\t                    + args.max_val_samples\n\t                    + args.max_test_samples,\n\t                )\n\t            )\n\t        elif logger is not None:\n", "            logger.info(\n\t                f\"max_test_samples({args.max_test_samples}) is larger than the number of test samples({len(dataset)})\"\n\t            )\n\t    image_column = [col for col in [\"image\", \"img\"] if col in dataset.column_names][0]\n\t    caption_colum = [\n\t        col for col in [\"text\", \"caption\", \"prompt\"] if col in dataset.column_names\n\t    ][0]\n\t    transform = get_transform(args.resolution, args.random_flip, args.center_crop)\n\t    def preprocess_train(examples):\n\t        \"\"\"tokenize captions and convert images to pixel values\"\"\"\n", "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n\t        examples[\"pixel_values\"] = [transform(image) for image in images]\n\t        examples[\"input_ids\"] = tokenize_captions(examples[caption_colum], tokenizer)\n\t        return examples\n\t    if logger is not None:\n\t        logger.info(\n\t            f\"Loaded {len(dataset)} {split} samples from dataset:{args.dataset}\"\n\t        )\n\t    return dataset.with_transform(preprocess_train)\n\tdef sample_test_image(args, split, tokenizer, logger, num: int = 10):\n", "    test_data = get_dataset(args, split=split, tokenizer=tokenizer, logger=logger)\n\t    images = []\n\t    for _ in range(num):\n\t        idx = np.random.randint(0, len(test_data))\n\t        images.append(test_data[idx][\"pixel_values\"])\n\t    return images\n"]}
{"filename": "utils/__init__.py", "chunked_list": []}
{"filename": "utils/parse_args.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   parse_args.py\n\t@Time    :   2023/05/25 10:46:53\n\t@Author  :   Wenbo Li\n\t@Desc    :   convert dataclass into argparse and group, copied a lot from fairseq\n\t\"\"\"\n\timport argparse\n\tfrom dataclasses import MISSING, fields\n", "from enum import Enum\n\tfrom typing import Any, Dict, List, Optional, Tuple\n\tfrom omegaconf import DictConfig, OmegaConf\n\tfrom stable_diffusion.dataclass import BaseDataclass\n\tfrom stable_diffusion.models.autoencoder import AutoencoderConfig\n\tfrom stable_diffusion.models.clip_model import ClipConfig\n\tfrom stable_diffusion.models.scheduler import DDPMConfig\n\tfrom stable_diffusion.models.unet import UnetConfig\n\tfrom trainer_args import (\n\t    LogConfig,\n", "    TrainConfig,\n\t    OptimConfig,\n\t)\n\tfrom utils.checkpointing_args import CheckpointConfig\n\tfrom utils.prepare_dataset import DatasetConfig\n\timport re, ast, inspect\n\t# TODO: fix bug: type\n\tdef convert_dataclass_to_argparse(\n\t    data_classes: List[BaseDataclass], parser: argparse.ArgumentParser\n\t):\n", "    \"\"\"Create argument groups and arguments for each data class\"\"\"\n\t    for data_class in data_classes:\n\t        group = parser.add_argument_group(\n\t            data_class.__name__.lower().replace(\"config\", \"\")\n\t        )\n\t        # iterate over each arg\n\t        for field_info in fields(data_class):\n\t            # get args name\n\t            arg_name = f\"--{field_info.name.replace('_', '-')}\"\n\t            # attributes of this argument\n", "            kwargs = {\"type\": type(field_info.type)}\n\t            default_value = field_info.default\n\t            if default_value is MISSING:\n\t                kwargs[\"required\"] = True\n\t            # if arg_name == \"--use-8bit-adam\":\n\t            #     kwargs[\"action\"] = \"store_true\"\n\t            if field_info.type == bool:\n\t                kwargs[\"action\"] = (\n\t                    \"store_false\" if field_info.default is True else \"store_true\"\n\t                )\n", "            print(\n\t                f\"parse dataclass={data_class.__name__.lower()}, arg_name={arg_name}, \"\n\t            )\n\t            group.add_argument(arg_name, **kwargs)\n\tdef get_parser_from_dataclass(\n\t    dataclasses: List[BaseDataclass],\n\t    parser: argparse.ArgumentParser,\n\t):\n\t    \"\"\"\n\t    implementation using code from [fairseq](https://github.com/facebookresearch/fairseq/blob/25c20e6a5e781e4ef05e23642f21c091ba64872e/fairseq/dataclass/utils.py#L53)\n", "    \"\"\"\n\t    # comments are from https://zhuanlan.zhihu.com/p/558760615?utm_id=0\n\t    def eval_str_list(x, x_type=float):\n\t        if x is None:\n\t            return None\n\t        if isinstance(x, str):\n\t            if len(x) == 0:\n\t                return []\n\t            x = ast.literal_eval(x)\n\t        try:\n", "            return list(map(x_type, x))\n\t        except TypeError:\n\t            return [x_type(x)]\n\t    def interpret_dc_type(field_type):\n\t        if isinstance(field_type, str):\n\t            raise RuntimeError(\"field should be a type\")\n\t        if field_type == Any:\n\t            return str\n\t        typestring = str(field_type)\n\t        if re.match(\n", "            r\"(typing.|^)Union\\[(.*), NoneType\\]$\", typestring\n\t        ) or typestring.startswith(\"typing.Optional\"):\n\t            return field_type.__args__[0]\n\t        return field_type\n\t    def gen_parser_from_dataclass(\n\t        parser: argparse.ArgumentParser,\n\t        dataclass_instance: BaseDataclass,  # FairseqDataclass,\n\t        delete_default: bool = False,\n\t        with_prefix: Optional[str] = None,\n\t    ) -> None:\n", "        \"\"\"\n\t        convert a dataclass instance to tailing parser arguments.\n\t        If `with_prefix` is provided, prefix all the keys in the resulting parser with it. It means that we are\n\t        building a flat namespace from a structured dataclass (see transformer_config.py for example).\n\t        \"\"\"\n\t        def argparse_name(name: str):\n\t            \"\"\"\n\t            检查两个Corner case，data一般是位置参数，不需要修饰。_name是dataclass隐含的私有参数，不需要修饰。\n\t            其他的都加上--，同时把变量名里的下划线改成短横线，这是因为linux在碰到 A_B 的时候，会先解析 A_ ，如果恰好\n\t            存在，就返回 {A_指向的内容}B，否则就返回 {空}B，会引起解析错误。\n", "            \"\"\"\n\t            if name == \"data\" and (with_prefix is None or with_prefix == \"\"):\n\t                # normally data is positional args, so we don't add the -- nor the prefix\n\t                return name\n\t            if name == \"_name\":\n\t                # private member, skip\n\t                return None\n\t            full_name = \"--\" + name.replace(\"_\", \"-\")\n\t            if with_prefix is not None and with_prefix != \"\":\n\t                # if a prefix is specified, construct the prefixed arg name\n", "                full_name = with_prefix + \"-\" + full_name[2:]  # strip -- when composing\n\t            return full_name\n\t        def get_kwargs_from_dc(\n\t            dataclass_instance: BaseDataclass, k: str  # : FairseqDataclass,\n\t        ) -> Dict[str, Any]:\n\t            \"\"\"k: dataclass attributes\"\"\"\n\t            kwargs = {}\n\t            field_type = dataclass_instance._get_type(k)\n\t            # 对filed_type进行合法性检查，对any和optional类型进行额外的处理，\n\t            # 如果是optional或者union，就返回不是NONE的类型\n", "            inter_type = interpret_dc_type(field_type)\n\t            field_default = dataclass_instance._get_default(k)\n\t            if isinstance(inter_type, type) and issubclass(inter_type, Enum):\n\t                field_choices = [t.value for t in list(inter_type)]\n\t            else:\n\t                field_choices = None\n\t            field_help = dataclass_instance._get_help(k)\n\t            # 有一些常量会被放在metadata里，这些常量更像是默认值，但还不清楚为什么不放在default里头\n\t            field_const = dataclass_instance._get_argparse_const(k)\n\t            if isinstance(field_default, str) and field_default.startswith(\"${\"):\n", "                kwargs[\"default\"] = field_default\n\t            else:\n\t                if field_default is MISSING:\n\t                    kwargs[\"required\"] = True\n\t                if field_choices is not None:\n\t                    kwargs[\"choices\"] = field_choices\n\t                # 这个地方很有意思，kwargs里的type居然不是一个固定值，而是一个lambda表达式，似乎是实际结合再转换？\n\t                if (\n\t                    isinstance(inter_type, type)\n\t                    and (issubclass(inter_type, List) or issubclass(inter_type, Tuple))\n", "                ) or (\"List\" in str(inter_type) or \"Tuple\" in str(inter_type)):\n\t                    if \"int\" in str(inter_type):\n\t                        kwargs[\"type\"] = lambda x: eval_str_list(x, int)\n\t                    elif \"float\" in str(inter_type):\n\t                        kwargs[\"type\"] = lambda x: eval_str_list(x, float)\n\t                    elif \"str\" in str(inter_type):\n\t                        kwargs[\"type\"] = lambda x: eval_str_list(x, str)\n\t                    else:\n\t                        raise NotImplementedError(\n\t                            \"parsing of type \" + str(inter_type) + \" is not implemented\"\n", "                        )\n\t                    if field_default is not MISSING:\n\t                        kwargs[\"default\"] = (\n\t                            \",\".join(map(str, field_default))\n\t                            if field_default is not None\n\t                            else None\n\t                        )\n\t                elif (\n\t                    isinstance(inter_type, type) and issubclass(inter_type, Enum)\n\t                ) or \"Enum\" in str(inter_type):\n", "                    kwargs[\"type\"] = str\n\t                    if field_default is not MISSING:\n\t                        if isinstance(field_default, Enum):\n\t                            kwargs[\"default\"] = field_default.value\n\t                        else:\n\t                            kwargs[\"default\"] = field_default\n\t                elif inter_type is bool:\n\t                    kwargs[\"action\"] = (\n\t                        \"store_false\" if field_default is True else \"store_true\"\n\t                    )\n", "                    kwargs[\"default\"] = field_default\n\t                else:\n\t                    kwargs[\"type\"] = inter_type\n\t                    if field_default is not MISSING:\n\t                        kwargs[\"default\"] = field_default\n\t            # build the help with the hierarchical prefix\n\t            if with_prefix is not None and with_prefix != \"\" and field_help is not None:\n\t                field_help = with_prefix[2:] + \": \" + field_help\n\t            kwargs[\"help\"] = field_help\n\t            if field_const is not None:\n", "                kwargs[\"const\"] = field_const\n\t                kwargs[\"nargs\"] = \"?\"\n\t            return kwargs\n\t        # _get_all_attributes可以获取一个数据类里所有的数据字段的变量名->[]\n\t        for k in dataclass_instance._get_all_attributes():\n\t            # _get_name是从__dataclass_fields__的字典里根据变量名去拿Fields里的name，但这俩应该是一样的？\n\t            # argparse_name会为大部分的变量名加上--，从而形成参数名。并把下划线转出短横线，防止解析错误\n\t            field_name = argparse_name(dataclass_instance._get_name(k))\n\t            # 返回默认类型\n\t            field_type = dataclass_instance._get_type(k)\n", "            if field_name is None:\n\t                continue\n\t            elif inspect.isclass(field_type) and issubclass(field_type, BaseDataclass):\n\t                # 如果该Fields是数据类，而且还是FairseqDtaclass的子类，就可以递归加入该子类的fields\n\t                # for fields that are of type FairseqDataclass, we can recursively\n\t                # add their fields to the namespace (so we add the args from model, task, etc. to the root namespace)\n\t                prefix = None\n\t                if with_prefix is not None:\n\t                    # if a prefix is specified, then we don't want to copy the subfields directly to the root namespace\n\t                    # but we prefix them with the name of the current field.\n", "                    prefix = field_name\n\t                gen_parser_from_dataclass(parser, field_type(), delete_default, prefix)\n\t                continue\n\t            kwargs = get_kwargs_from_dc(dataclass_instance, k)\n\t            field_args = [field_name]\n\t            # 获取这个参数的别名，比如batch_size和--max-sentence，输入参数的时候不管给哪个传递，都是一样的。\n\t            alias = dataclass_instance._get_argparse_alias(k)\n\t            if alias is not None:\n\t                field_args.append(alias)\n\t            if \"default\" in kwargs:\n", "                if isinstance(kwargs[\"default\"], str) and kwargs[\"default\"].startswith(\n\t                    \"${\"\n\t                ):\n\t                    if kwargs[\"help\"] is None:\n\t                        # this is a field with a name that will be added elsewhere\n\t                        continue\n\t                    else:\n\t                        del kwargs[\"default\"]\n\t                if delete_default and \"default\" in kwargs:\n\t                    del kwargs[\"default\"]\n", "            try:\n\t                # 这个地方蛮有意思，关于在实参和形参前加星号。形参加星号是告诉大家这个函数里会把他作为元组操作\n\t                # 如果对于形参是单星号，实参是元组但是没带单星号的，会把实参作为元组内的一个元素处理\n\t                # 从而在函数体里造成元组的元组的问题，如果实参前加了单星号，就会把实参从元组解成很多个变量传进去。\n\t                # https://www.cnblogs.com/sddai/p/14303453.html\n\t                parser.add_argument(*field_args, **kwargs)\n\t            except argparse.ArgumentError:\n\t                pass\n\t    for dataclass in dataclasses:\n\t        gen_parser_from_dataclass(parser, dataclass())\n", "def load_config(gen_args_fn=get_parser_from_dataclass):\n\t    # train data classes\n\t    train_data_classes = [\n\t        LogConfig,\n\t        TrainConfig,\n\t        OptimConfig,\n\t        DatasetConfig,\n\t        CheckpointConfig,\n\t    ]\n\t    # model data classes\n", "    model_data_classes = [\n\t        UnetConfig,\n\t        AutoencoderConfig,\n\t        ClipConfig,\n\t        DDPMConfig,\n\t    ]\n\t    # Create an argument parser\n\t    parser = argparse.ArgumentParser()\n\t    # add train data classes into argparse\n\t    gen_args_fn(train_data_classes, parser)\n", "    gen_args_fn(model_data_classes, parser)\n\t    # Parse the command-line arguments\n\t    args = parser.parse_args()\n\t    cfg = DictConfig({})\n\t    def convert_dataclass_to_dictconfig(dataclasses, cfg: DictConfig):\n\t        \"\"\"Convert data classes to nested DictConfigs\"\"\"\n\t        for data_class in dataclasses:\n\t            data_instance = data_class()\n\t            for field_info in fields(data_class):\n\t                field_name = field_info.name\n", "                field_value = getattr(args, field_name)\n\t                setattr(data_instance, field_name, field_value)\n\t            group_name: str = data_class.__name__.lower().replace(\"config\", \"\")\n\t            group_config: DictConfig = OmegaConf.structured(data_instance)\n\t            cfg[group_name] = group_config\n\t    # add train data classes into DictConfig\n\t    convert_dataclass_to_dictconfig(train_data_classes, cfg=cfg)\n\t    # nest model data classes into cfg.model\n\t    cfg[\"model\"] = DictConfig({})\n\t    convert_dataclass_to_dictconfig(model_data_classes, cfg=cfg[\"model\"])\n", "    return args, cfg\n"]}
{"filename": "scripts/__init__.py", "chunked_list": ["from .txt2img import sample\n"]}
{"filename": "scripts/txt2img.py", "chunked_list": ["import torch\n\timport os\n\timport sys\n\t# TODO: fix this\n\tsys.path.append(os.path.join(os.path.dirname(__file__), \"..\"))\n\tfrom stable_diffusion.models.latent_diffusion import LatentDiffusion\n\tfrom utils.model_utils import build_models\n\tfrom utils.parse_args import load_config\n\tfrom utils.prepare_dataset import detransform, to_img\n\tdef sample(\n", "    model: LatentDiffusion,\n\t    image_size=64,\n\t    prompt: str = \"\",\n\t    time_steps: int = 50,\n\t    guidance_scale: float = 7.5,\n\t    scale_factor=1.0,\n\t    save_dir: str = \"output\",\n\t    device: str = \"cuda\",\n\t    weight_dtype=torch.float16,\n\t):\n", "    \"Sample an image given prompt\"\n\t    model.to(device=device, dtype=weight_dtype)\n\t    # random noise\n\t    # to get the right latent size\n\t    img_util = torch.randn(size=(1, 3, image_size, image_size)).to(\n\t        device, dtype=weight_dtype\n\t    )\n\t    noise = model.autoencoder.encode(img_util).latent_dist.sample()\n\t    noise = torch.rand_like(noise).to(device, dtype=weight_dtype)\n\t    # tokenize prompt\n", "    tokenized_prompt = model.text_encoder.tokenize([prompt]).input_ids.to(device)\n\t    context_emb = model.text_encoder.encode_text(tokenized_prompt)[0].to(\n\t        device, weight_dtype\n\t    )\n\t    x_0 = model.sample(\n\t        noised_sample=noise,\n\t        context_emb=context_emb,\n\t        guidance_scale=guidance_scale,\n\t        scale_factor=scale_factor,\n\t        time_steps=time_steps,\n", "    )\n\t    sample = model.autoencoder.decode(x_0)\n\t    sample = detransform(sample)\n\t    to_img(sample, output_path=save_dir, name=\"txt2img\")\n\tif __name__ == \"__main__\":\n\t    args, cfg = load_config()\n\t    model: LatentDiffusion = build_models(cfg.model)\n\t    sample(model, prompt=\"a cat\", image_size=64, time_steps=50, save_dir=\"output\")\n"]}
{"filename": "stable_diffusion/dataclass.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   dataclass.py\n\t@Time    :   2023/05/26 20:09:51\n\t@Author  :   Wenbo Li\n\t@Desc    :   Base dataclass copied from fairseq, Configs inherit from this class so that fairseq script can better convert dataclass into argparse\n\t\"\"\"\n\tfrom dataclasses import _MISSING_TYPE, dataclass\n\tfrom typing import List, Any, Optional\n", "@dataclass\n\tclass BaseDataclass:\n\t    \"\"\"Copied wonderful code from [fairseq/fairseq/dataclass/configs.py](https://github.com/facebookresearch/fairseq/blob/25c20e6a5e781e4ef05e23642f21c091ba64872e/fairseq/dataclass/configs.py#L28)\"\"\"\n\t    # _name: Optional[str] = None\n\t    # @staticmethod\n\t    # def name():\n\t    #     return None\n\t    def _get_all_attributes(self) -> List[str]:\n\t        return list(self.__dataclass_fields__.keys())\n\t    def _get_meta(\n", "        self, attribute_name: str, meta: str, default: Optional[Any] = None\n\t    ) -> Any:\n\t        return self.__dataclass_fields__[attribute_name].metadata.get(meta, default)\n\t    def _get_name(self, attribute_name: str) -> str:\n\t        return self.__dataclass_fields__[attribute_name].name\n\t    def _get_default(self, attribute_name: str) -> Any:\n\t        if hasattr(self, attribute_name):\n\t            if str(getattr(self, attribute_name)).startswith(\"${\"):\n\t                return str(getattr(self, attribute_name))\n\t            elif str(self.__dataclass_fields__[attribute_name].default).startswith(\n", "                \"${\"\n\t            ):\n\t                return str(self.__dataclass_fields__[attribute_name].default)\n\t            elif (\n\t                getattr(self, attribute_name)\n\t                != self.__dataclass_fields__[attribute_name].default\n\t            ):\n\t                return getattr(self, attribute_name)\n\t        f = self.__dataclass_fields__[attribute_name]\n\t        if not isinstance(f.default_factory, _MISSING_TYPE):\n", "            return f.default_factory()\n\t        return f.default\n\t    def _get_type(self, attribute_name: str) -> Any:\n\t        return self.__dataclass_fields__[attribute_name].type\n\t    def _get_help(self, attribute_name: str) -> Any:\n\t        return self._get_meta(attribute_name, \"help\")\n\t    def _get_argparse_const(self, attribute_name: str) -> Any:\n\t        return self._get_meta(attribute_name, \"argparse_const\")\n\t    def _get_argparse_alias(self, attribute_name: str) -> Any:\n\t        return self._get_meta(attribute_name, \"argparse_alias\")\n", "    def _get_choices(self, attribute_name: str) -> Any:\n\t        return self._get_meta(attribute_name, \"choices\")\n"]}
{"filename": "stable_diffusion/__init__.py", "chunked_list": []}
{"filename": "stable_diffusion/models/clip_model.py", "chunked_list": ["from transformers import CLIPTextModel, CLIPTokenizer\n\tfrom torch import nn\n\tfrom dataclasses import dataclass, field\n\tfrom typing import Optional\n\tfrom stable_diffusion.dataclass import BaseDataclass\n\t@dataclass\n\tclass ClipConfig(BaseDataclass):\n\t    tokenizer: str = field(\n\t        default=\"runwayml/stable-diffusion-v1-5\",\n\t        metadata={\"help\": \"Tokenizer to use for text encoding.\"},\n", "    )\n\t    text_encoder: str = field(\n\t        default=\"runwayml/stable-diffusion-v1-5\",\n\t        metadata={\"help\": \"Text encoder model to use.\"},\n\t    )\n\t    max_seq_len: int = field(\n\t        default=77, metadata={\"help\": \"Maximum sequence length for tokenized text.\"}\n\t    )\n\t    model_dir: Optional[str] = field(\n\t        default=\"data/pretrained\",\n", "        metadata={\"help\": \"Path to a directory to store the pretrained CLIP model.\"},\n\t    )\n\tclass CLIPModel(nn.Module):\n\t    @staticmethod\n\t    def add_clip_args(model_parser):\n\t        clip_group = model_parser.add_argument_group(\"clip\")\n\t        clip_group.add_argument(\n\t            \"--tokenizer\",\n\t            type=str,\n\t            default=\"runwayml/stable-diffusion-v1-5\",\n", "        )\n\t        clip_group.add_argument(\n\t            \"--text_encoder\",\n\t            type=str,\n\t            default=\"runwayml/stable-diffusion-v1-5\",\n\t        )\n\t        clip_group.add_argument(\n\t            \"--max_seq_len\",\n\t            type=int,\n\t            default=77,\n", "        )\n\t        clip_group.add_argument(\n\t            \"--cache_dir\",\n\t            type=str,\n\t            default=None,\n\t            help=\"Path to a directory to store the pretrained clip model\",\n\t        )\n\t        return clip_group\n\t    def __init__(\n\t        self,\n", "        cfg,\n\t    ):\n\t        super().__init__()\n\t        self.max_seq_len = cfg.max_seq_len\n\t        self.text_encoder: CLIPTextModel = CLIPTextModel.from_pretrained(\n\t            cfg.text_encoder, cache_dir=cfg.model_dir, subfolder=\"text_encoder\"\n\t        )\n\t        self.tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(\n\t            cfg.tokenizer,\n\t            use_fast=False,\n", "            cache_dir=cfg.model_dir,\n\t            subfolder=\"tokenizer\",\n\t        )\n\t    def tokenize(\n\t        self,\n\t        prompt: str = \"\",\n\t        max_length: int = None,\n\t        padding: str = \"max_length\",\n\t        truncation: bool = True,\n\t    ):\n", "        return self.tokenizer(\n\t            prompt,\n\t            return_tensors=\"pt\",\n\t            padding=padding,\n\t            truncation=truncation,\n\t            max_length=(self.max_seq_len if max_length is None else max_length),\n\t        )\n\t    def encode_text(self, text):\n\t        \"\"\"Encode text to text embedding\n\t        Args:\n", "            - text (str):\n\t                  text to encode, shape = [batch, seq_len]\n\t        Returns:\n\t            - text_embedding (torch.Tensor):\n\t                  text embedding, shape = [batch, seq_len, d_model]\n\t        \"\"\"\n\t        return self.text_encoder(text)\n"]}
{"filename": "stable_diffusion/models/unet.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   unet_2d_conditional.py\n\t@Time    :   2023/05/14 15:37:56\n\t@Author  :   Wenbo Li\n\t@Desc    :   implementation of Unet2d model and sub modules\n\t\"\"\"\n\timport numpy as np\n\timport torch\n", "import torch.nn as nn\n\tfrom stable_diffusion.dataclass import BaseDataclass\n\tfrom .utils import (\n\t    build_conv_in,\n\t    build_input_blocks,\n\t    build_bottleneck,\n\t    build_output_blocks,\n\t    build_final_output,\n\t)\n\tfrom ..modules.timestep_embedding import sinusoidal_time_proj\n", "from dataclasses import dataclass, field\n\tfrom typing import List, Optional\n\t@dataclass\n\tclass UnetConfig(BaseDataclass):\n\t    num_res_blocks: int = field(\n\t        default=2, metadata={\"help\": \"Number of residual blocks at each level.\"}\n\t    )\n\t    n_heads: int = field(\n\t        default=8, metadata={\"help\": \"Number of attention heads in transformers.\"}\n\t    )\n", "    attention_resolutions: List[int] = field(\n\t        default_factory=lambda: [0, 1],\n\t        metadata={\n\t            \"help\": \"At which level attention should be performed. e.g., [1, 2] means attention is performed at level 1 and 2.\"\n\t        },\n\t    )\n\t    channels_list: List[int] = field(\n\t        default_factory=lambda: [160, 320],\n\t        metadata={\"help\": \"Channels at each level.\"},\n\t    )\n", "    time_emb_dim: Optional[int] = field(\n\t        default=512,\n\t        metadata={\n\t            \"help\": \"Time embedding dimension. If not specified, use 4 * channels_list[0] instead.\"\n\t        },\n\t    )\n\t    dropout: float = field(default=0.1, metadata={\"help\": \"Dropout rate.\"})\n\t    n_layers: int = field(default=2, metadata={\"help\": \"Number of transformer layers.\"})\n\t    context_dim: int = field(\n\t        default=768, metadata={\"help\": \"Embedding dim of context condition.\"}\n", "    )\n\tclass UNetModel(nn.Module):\n\t    r\"\"\"\n\t    The full `U-Net` model=`ε_θ(x_t, t, condition)` that takes noised latent x, time step and context condition, predicts the noise\n\t    `U-Net` as several levels(total `len(channel_mult)`), each level is a `TimestepEmbSequential`,\n\t    at each level there are sevreal(total `num_res_blocks`)\n\t    `ResBlocks`/`SpatialTransformer`/`AttentionBlock`/`UpSample`/`DownSample` blocks\n\t    Archetecture:\n\t      - input_blocks: [\n\t                    TimestepEmbSeq[`Conv2d]`,\n", "                    (num_levels-1) * (\n\t                        (num_res_blocks)* (TimestepEmbSeq[`Resblock`, Optioanl[`SpatialTransformer`]]),\n\t                        TimestepEmbSeq[`DownSample`]\n\t                    ),\n\t                    (num_res_blocks)* (TimestepEmbSeq[`Resblock`, Optioanl[`SpatialTransformer`]])\n\t            ]\n\t      - bottleneck: TimestepEmbSeq[`ResBlock`, `SpatialTransformer`, `ResBlock`]\n\t      - output_blocks: [\n\t                    (num_levels-1) * (\n\t                        (num_res_blocks)* (TimestepEmbSeq[`Resblock`, Optioanl[`SpatialTransformer`]]),\n", "                        TimestepEmbSeq[`UpSample`]\n\t                    ),\n\t                    (num_res_blocks)* (TimestepEmbSeq[`Resblock`, Optioanl[`SpatialTransformer`]])\n\t               ]\n\t      - out: [GroupNorm, SiLU, Conv2d]\n\t    Args:\n\t        - in_channels (int):\n\t                number of channels in the input feature map\n\t        - out_channels (int):\n\t                number of channels in the output feature map\n", "        - num_res_blocks (int):\n\t                number of residual blocks at each level\n\t        - n_heads (int):\n\t                num of attention heads in transformers\n\t        - attention_resolutions (List[int]):\n\t                at which level should attention be performed.\n\t                e.g. [1, 2] means attention is performed at level 1 and 2.\n\t        - channels_list (List[int]):\n\t                channels for each level of the UNet.\n\t        - dropout (float, optional):\n", "                dropout rate. Default: `0`.\n\t        - n_layers (int, optional):\n\t                num of transformer layers. Default: `1`.\n\t        - context_dim (int, optional):\n\t                embedding dim of context condition. Default: `768`.\n\t    \"\"\"\n\t    @staticmethod\n\t    def add_unet_args(parser):\n\t        unet_group = parser.add_argument_group(\"unet\")\n\t        unet_group.add_argument(\n", "            \"--num_res_blocks\",\n\t            type=int,\n\t            default=2,\n\t            help=\"number of residual blocks at each level\",\n\t        )\n\t        unet_group.add_argument(\n\t            \"--n_heads\",\n\t            type=int,\n\t            default=1,\n\t            help=\"num of attention heads in transformers\",\n", "        )\n\t        unet_group.add_argument(\n\t            \"--attention_resolutions\",\n\t            type=int,\n\t            nargs=\"+\",\n\t            default=[1],\n\t            help=\"at which level should attention be performed. e.g. [1, 2] means attention is performed at level 1 and 2.\",\n\t        )\n\t        unet_group.add_argument(\n\t            \"--channels_list\",\n", "            type=int,\n\t            nargs=\"+\",\n\t            default=[64, 128],\n\t            help=\"channels at each level\",\n\t        )\n\t        unet_group.add_argument(\n\t            \"--time_emb_dim\",\n\t            type=int,\n\t            default=None,\n\t            help=\"time embedding dimension, if not specified, use 4 * channels_list[0] instead\",\n", "        )\n\t        unet_group.add_argument(\n\t            \"--dropout\",\n\t            type=float,\n\t            default=0.0,\n\t            help=\"dropout rate\",\n\t        )\n\t        unet_group.add_argument(\n\t            \"--n_layers\",\n\t            type=int,\n", "            default=1,\n\t            help=\"num of transformer layers\",\n\t        )\n\t        unet_group.add_argument(\n\t            \"--context_dim\",\n\t            type=int,\n\t            default=768,\n\t            help=\"embedding dim of context condition\",\n\t        )\n\t    def __init__(\n", "        self,\n\t        latent_channels,\n\t        groups,\n\t        cfg,  # unet config\n\t    ):\n\t        super().__init__()\n\t        num_res_blocks = cfg.num_res_blocks\n\t        n_heads = cfg.n_heads\n\t        attention_resolutions = cfg.attention_resolutions\n\t        channels_list = cfg.channels_list\n", "        dropout = cfg.dropout\n\t        n_layers = cfg.n_layers\n\t        context_dim = cfg.context_dim\n\t        self.context_dim = cfg.context_dim\n\t        self.channels = channels_list[0]\n\t        # * 1. time emb\n\t        time_emb_dim = cfg.time_emb_dim or channels_list[0] * 4\n\t        timestep_input_dim = channels_list[0]\n\t        self.time_embedding = nn.Sequential(\n\t            nn.Linear(timestep_input_dim, time_emb_dim),\n", "            nn.SiLU(),\n\t            nn.Linear(time_emb_dim, time_emb_dim),\n\t        )\n\t        # * 2. conv in\n\t        self.conv_in = build_conv_in(latent_channels, self.channels)\n\t        # num of levels\n\t        levels = len(channels_list)\n\t        # * 3. input blocks\n\t        (\n\t            self.input_blocks,\n", "            input_block_channels,\n\t            mid_ch,\n\t            d_head,\n\t            attn_mult,\n\t        ) = build_input_blocks(\n\t            in_channels=self.channels,\n\t            num_res_blocks=num_res_blocks,\n\t            attention_resolutions=attention_resolutions,\n\t            n_heads=n_heads,\n\t            n_layers=n_layers,\n", "            dropout=dropout,\n\t            context_dim=context_dim,\n\t            levels=levels,\n\t            time_emb_dim=time_emb_dim,\n\t            channels_list=channels_list,\n\t            groups=groups,\n\t        )\n\t        # @ note: openai recalculated d_heads for attention in the bottoleneck, but that seems redundant(so as out_ch=ch and then ch=out_ch)\n\t        # see origin code: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/modules/diffusionmodules/openaimodel.py#L443\n\t        # * 4. bottleneck\n", "        # bottleneck has one resblock, then one attention block/spatial transformer, and then one resblock\n\t        self.middle_block = build_bottleneck(\n\t            in_ch=mid_ch,\n\t            time_emb_dim=time_emb_dim,\n\t            n_heads=n_heads,\n\t            d_head=d_head,\n\t            n_layers=n_layers,\n\t            dropout=dropout,\n\t            context_dim=context_dim,\n\t            groups=groups,\n", "        )\n\t        # * 5. output blocks\n\t        self.output_blocks, out_ch = build_output_blocks(\n\t            num_res_blocks=num_res_blocks,\n\t            attention_resolutions=attention_resolutions,\n\t            n_heads=n_heads,\n\t            n_layers=n_layers,\n\t            dropout=dropout,\n\t            context_dim=context_dim,\n\t            input_block_channels=input_block_channels,\n", "            levels=levels,\n\t            time_emb_dim=time_emb_dim,\n\t            channels_list=channels_list,\n\t            in_ch=mid_ch,\n\t            attn_mult=attn_mult,\n\t            groups=groups,\n\t        )\n\t        # * 6. final output\n\t        self.out = build_final_output(\n\t            out_ch=out_ch, out_channels=latent_channels, groups=groups\n", "        )\n\t    def time_proj(self, time_steps: torch.Tensor, max_len: int = 10000) -> torch.Tensor:\n\t        \"\"\"\n\t        time_step_embedding use sinusoidal time step embedding as default, feel free to try out other embeddings\n\t        Args:\n\t            - time_steps (torch.Tensor):\n\t                  time step of shape `[batch_size,]`\n\t            - max_len (int, optional):\n\t                  max len of embedding. Default: `10000`.\n\t        Returns:\n", "            - torch.Tensor:\n\t                  time embedding of shape `[batch, emb_dim]`\n\t        \"\"\"\n\t        return sinusoidal_time_proj(time_steps, self.channels, max_len)\n\t    def forward(\n\t        self,\n\t        x: torch.Tensor,\n\t        timesteps: torch.Tensor,\n\t        context_emb: Optional[torch.Tensor] = None,\n\t    ):\n", "        \"\"\"\n\t        forward pass, predict noise given noised image x, time step and context\n\t        Args:\n\t            - x (torch.Tensor):\n\t                  input latent of shape `[batch_size, channels, width, height]`\n\t            - timesteps (torch.Tensor):\n\t                  time steps of shape `[batch_size]`\n\t            - context_emb (torch.Tensor, optional):\n\t                  cross attention context embedding of shape `[batch_size, seq_len, context_dim]`. Default: None.\n\t        Returns:\n", "            - torch.Tensor:\n\t                  output latent of shape `[batch_size, channels, width, height]`\n\t        \"\"\"\n\t        # check parameters\n\t        if context_emb is not None:\n\t            assert (\n\t                context_emb.shape[-1] == self.context_dim\n\t            ), f\"context dim from passed in context({context_emb.shape}) should be equal to self.context_dim({self.context_dim})\"\n\t        # store input blocks for skip connection\n\t        x_input_blocks = []\n", "        # * Get time embedding\n\t        time_emb = self.time_proj(timesteps).to(x.dtype)\n\t        time_emb = self.time_embedding(time_emb)\n\t        # * conv in layer\n\t        x = self.conv_in(x)\n\t        x_input_blocks.append(x)\n\t        # * input blocks\n\t        for module in self.input_blocks:\n\t            x = module(x, time_emb, context_emb)\n\t            x_input_blocks.append(x)\n", "        # * bottleneck\n\t        x = self.middle_block(x, time_emb, context_emb)\n\t        # * output blocks\n\t        for module in self.output_blocks:\n\t            # skip connection from input blocks\n\t            x = torch.cat([x, x_input_blocks.pop()], dim=1)\n\t            x = module(x, time_emb, context_emb)\n\t        return self.out(x)\n"]}
{"filename": "stable_diffusion/models/scheduler.py", "chunked_list": ["from torch import nn\n\timport torch\n\tfrom typing import List\n\tfrom dataclasses import dataclass, field\n\tfrom stable_diffusion.dataclass import BaseDataclass\n\t@dataclass\n\tclass DDPMConfig(BaseDataclass):\n\t    noise_schedule: str = field(\n\t        default=\"linear\",\n\t        metadata={\n", "            \"help\": \"Noise schedule type.\",\n\t            \"choices\": [\"linear\", \"cosine\", \"cubic\"],\n\t        },\n\t    )\n\t    noise_steps: int = field(default=1000, metadata={\"help\": \"Number of noise steps.\"})\n\t    beta_start: float = field(\n\t        default=1e-4, metadata={\"help\": \"Starting value of beta.\"}\n\t    )\n\t    beta_end: float = field(default=0.02, metadata={\"help\": \"Ending value of beta.\"})\n\tclass DDPMScheduler:\n", "    @staticmethod\n\t    def add_ddpm_args(parser):\n\t        noise_group = parser.add_argument_group(\"ddpm\")\n\t        noise_group.add_argument(\n\t            \"--noise_schedule\",\n\t            type=str,\n\t            default=\"linear\",\n\t            choices=[\"linear\", \"cosine\", \"cubic\"],\n\t        )\n\t        noise_group.add_argument(\n", "            \"--noise_steps\",\n\t            type=int,\n\t            default=1000,\n\t        )\n\t        noise_group.add_argument(\n\t            \"--beta_start\",\n\t            type=float,\n\t            default=1e-4,\n\t        )\n\t        noise_group.add_argument(\n", "            \"--beta_end\",\n\t            type=float,\n\t            default=0.02,\n\t        )\n\t        return noise_group\n\t    def __init__(self, cfg):\n\t        \"\"\"modified from [labmlai - DDPMSampler](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/stable_diffusion/sampler/ddpm.py)\"\"\"\n\t        super().__init__()\n\t        self.noise_steps: int = cfg.noise_steps\n\t        self.noise_time_steps: torch.Tensor[List[int]] = torch.arange(\n", "            self.noise_steps - 1, -1, -1\n\t        )\n\t        self.betas = self.prepare_linear_noise_schedule(cfg.beta_start, cfg.beta_end)\n\t        self.alpha = 1.0 - self.betas\n\t        self.alphas_cumprod = torch.cumprod(self.alpha, dim=0)\n\t        #  $\\bar\\alpha_{t-1}$\n\t        alpha_bar_prev = torch.cat(\n\t            [self.alphas_cumprod.new_tensor([1.0]), self.alphas_cumprod[:-1]]\n\t        )\n\t        # $\\sqrt{\\bar\\alpha}$\n", "        self.sqrt_alpha_bar = self.alphas_cumprod**0.5\n\t        # $\\sqrt{1 - \\bar\\alpha}$\n\t        self.sqrt_1m_alpha_bar = (1.0 - self.alphas_cumprod) ** 0.5\n\t        # $\\frac{1}{\\sqrt{\\bar\\alpha_t}}$\n\t        self.sqrt_recip_alpha_bar = self.alphas_cumprod**-0.5\n\t        # $\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}$\n\t        self.sqrt_recip_m1_alpha_bar = (1 / self.alphas_cumprod - 1) ** 0.5\n\t        # $\\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t$\n\t        variance = self.betas * (1.0 - alpha_bar_prev) / (1.0 - self.alphas_cumprod)\n\t        # Clamped log of $\\tilde\\beta_t$\n", "        self.log_var = torch.log(torch.clamp(variance, min=1e-20))\n\t        # $\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$\n\t        self.mean_x0_coef = (\n\t            self.betas * (alpha_bar_prev**0.5) / (1.0 - self.alphas_cumprod)\n\t        )\n\t        # $\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$\n\t        self.mean_xt_coef = (\n\t            (1.0 - alpha_bar_prev)\n\t            * ((1 - self.betas) ** 0.5)\n\t            / (1.0 - self.alphas_cumprod)\n", "        )\n\t    def prepare_linear_noise_schedule(self, beta_start, beta_end):\n\t        \"\"\"\n\t        Returns the linear noise schedule\n\t        \"\"\"\n\t        return torch.linspace(beta_start, beta_end, self.noise_steps)\n\t    def add_noise(\n\t        self,\n\t        original_samples: torch.Tensor,\n\t        noise: torch.Tensor,\n", "        timesteps: torch.Tensor,\n\t    ):\n\t        r\"\"\"\n\t        sample x_t from q(x_t|x_0), where\n\t        `q(x_t|x_0) = N(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t)I)`\n\t        Modified from [Huggingface/Diffusers - scheduling_ddpm.py](https://github.com/huggingface/diffusers/blob/67cf0445ef48b1f913b90ce0025ac0c75673e32e/src/diffusers/schedulers/scheduling_ddpm.py#L419)\n\t        Args:\n\t            - original_samples (torch.Tensor):\n\t                  x_0, origin latent vector, shape=`[batch, channels, height, width]`\n\t            - noise (torch.Tensor):\n", "                  random noise, shape=`[batch, channels, height, width]`\n\t            - timesteps (torch.Tensor):\n\t                  time step to add noise, shape=`[batch,]`\n\t        Returns:\n\t            - noised latent vector (torch.Tensor):\n\t                    x_t, shape=`[batch, channels, height, width]`\n\t        \"\"\"\n\t        assert (\n\t            timesteps.max().item() < self.noise_steps\n\t        ), f\"timesteps({timesteps}) should be less than {self.noise_steps}\"\n", "        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n\t        alphas_cumprod = self.alphas_cumprod.to(\n\t            device=original_samples.device, dtype=original_samples.dtype\n\t        )\n\t        timesteps = timesteps.to(original_samples.device)\n\t        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n\t        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n\t        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n\t            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\t        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n", "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n\t        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n\t            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\t        noisy_samples = (\n\t            sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n\t        )\n\t        return noisy_samples\n\t    @torch.no_grad()\n\t    def step(\n\t        self,\n", "        pred_noise: torch.Tensor,\n\t        x_t: torch.Tensor,\n\t        time_step: int,\n\t        repeat_noise: bool = False,\n\t        scale_factor: float = 1.0,\n\t    ):\n\t        \"\"\"\n\t        predict x_t from\n\t        Sample 𝒙_{𝒕-1} from 𝒑_θ(𝒙_{𝒕-1} | 𝒙_𝒕) i.e. decode one step\n\t        Modified from [labmlai - DDPMSampler](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/stable_diffusion/sampler/ddpm.py)\n", "        Args:\n\t            - pred_noise (torch.Tensor):\n\t                  predicted noise from U-Net model = ε_θ(x_t, t, condition)\n\t            - x_t (torch.Tensor):\n\t                  noised latent at timestep=t, shape=`[batch_size, channels, height, width]`\n\t            - time_step (int):\n\t                  integer of timestep=t\n\t            - repeat_noise (bool, optional):\n\t                  whether use the same noise for all items in batch. Default: `False`.\n\t            - scale_factor (float, optional):\n", "                  scale_factor of noise. Default: `1.`.\n\t        \"\"\"\n\t        assert (\n\t            time_step < self.noise_steps\n\t        ), f\"timesteps({time_step}) should be less than {self.noise_steps}\"\n\t        bsz = x_t.shape[0]\n\t        # 1 / (\\sqrt{\\bar\\alpha_t})\n\t        sqrt_recip_alpha_bar = x_t.new_full(\n\t            (bsz, 1, 1, 1), self.sqrt_recip_alpha_bar[time_step]\n\t        )\n", "        # $\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}$\n\t        sqrt_recip_m1_alpha_bar = x_t.new_full(\n\t            (bsz, 1, 1, 1), self.sqrt_recip_m1_alpha_bar[time_step]\n\t        )\n\t        # Calculate $x_0$ with current $\\epsilon_\\theta$\n\t        # Eq (15) from DDPM paper: https://arxiv.org/pdf/2006.11239.pdf\n\t        # $$x_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}} x_t -  \\Big(\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}\\Big)\\epsilon_\\theta$$\n\t        x0 = sqrt_recip_alpha_bar * x_t - sqrt_recip_m1_alpha_bar * pred_noise\n\t        # $\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$\n\t        mean_x0_coef = x_t.new_full((bsz, 1, 1, 1), self.mean_x0_coef[time_step])\n", "        # $\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$\n\t        mean_xt_coef = x_t.new_full((bsz, 1, 1, 1), self.mean_xt_coef[time_step])\n\t        # Calculate $\\mu_t(x_t, t)$\n\t        # $$\\mu_t(x_t, t) = \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n\t        #    + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t$$\n\t        mean = mean_x0_coef * x0 + mean_xt_coef * x_t\n\t        # $\\log \\tilde\\beta_t$\n\t        log_var = x_t.new_full((bsz, 1, 1, 1), self.log_var[time_step])\n\t        # Do not add noise when $t = 1$ (final step sampling process).\n\t        # Note that `step` is `0` when $t = 1$)\n", "        if time_step == 0:\n\t            noise = torch.zeros(x_t.shape)\n\t        # If same noise is used for all samples in the batch\n\t        elif repeat_noise:\n\t            noise = torch.randn((1, *x_t.shape[1:]))\n\t        # Different noise for each sample\n\t        else:\n\t            noise = torch.randn(x_t.shape)\n\t        # Multiply noise by the temperature\n\t        noise = noise.to(mean.device, dtype=mean.dtype) * scale_factor\n", "        # Sample from,\n\t        # $$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\mu_\\theta(x_t, t), \\tilde\\beta_t \\mathbf{I} \\big)$$\n\t        x_prev = mean + (0.5 * log_var).exp() * noise\n\t        # return x_0 if we try to get x_0 directly from x_t, but it makes distortion more difficult to evaluate,\n\t        # see Eq (5) from DDPM paper: https://arxiv.org/pdf/2006.11239.pdf\n\t        return (\n\t            x_prev,\n\t            x0,\n\t        )\n"]}
{"filename": "stable_diffusion/models/__init__.py", "chunked_list": []}
{"filename": "stable_diffusion/models/autoencoder.py", "chunked_list": ["from typing import List\n\tfrom torch import nn\n\tfrom stable_diffusion.dataclass import BaseDataclass\n\tfrom ..modules.distributions import GaussianDistribution\n\timport torch\n\tfrom .utils import (\n\t    build_conv_in,\n\t    build_input_blocks,\n\t    build_bottleneck,\n\t    build_output_blocks,\n", "    build_final_output,\n\t)\n\tfrom dataclasses import dataclass, field\n\tfrom typing import List, Optional\n\t@dataclass\n\tclass AutoencoderConfig(BaseDataclass):\n\t    in_channels: int = field(\n\t        default=3, metadata={\"help\": \"Number of input channels of the input image.\"}\n\t    )\n\t    latent_channels: int = field(\n", "        default=4, metadata={\"help\": \"Embedding channels of the latent vector.\"}\n\t    )\n\t    out_channels: Optional[int] = field(\n\t        default=3,\n\t        metadata={\n\t            \"help\": \"Number of output channels of the decoded image. Should be the same as in_channels.\"\n\t        },\n\t    )\n\t    autoencoder_channels_list: List[int] = field(\n\t        default_factory=lambda: [64, 128],\n", "        metadata={\n\t            \"help\": \"Comma-separated list of channel multipliers for each level.\"\n\t        },\n\t    )\n\t    autoencoder_num_res_blocks: int = field(\n\t        default=2, metadata={\"help\": \"Number of residual blocks per level.\"}\n\t    )\n\t    groups: int = field(\n\t        default=32, metadata={\"help\": \"Number of groups for GroupNorm.\"}\n\t    )\n", "    kl_weight: float = field(default=1.0, metadata={\"help\": \"Weight of the KL loss.\"})\n\tclass AutoEncoderKL(nn.Module):\n\t    @staticmethod\n\t    def add_autoencoder_args(parser):\n\t        autoencoder = parser.add_argument_group(\"autoencoder\")\n\t        autoencoder.add_argument(\n\t            \"--in_channels\",\n\t            type=int,\n\t            default=3,\n\t            help=\"number of input channels of input image\",\n", "        )\n\t        autoencoder.add_argument(\n\t            \"--latent_channels\",\n\t            type=int,\n\t            default=4,\n\t            help=\"embedding channels of latent vector\",\n\t        )\n\t        autoencoder.add_argument(\n\t            \"--out_channels\",\n\t            type=int,\n", "            default=3,\n\t            help=\"number of output channels of decoded image, should be the same with in_channels\",\n\t        )\n\t        autoencoder.add_argument(\n\t            \"--autoencoder_channels_list\",\n\t            type=int,\n\t            nargs=\"+\",\n\t            default=[64, 128],\n\t            help=\"comma separated list of channels multipliers for each level\",\n\t        )\n", "        autoencoder.add_argument(\n\t            \"--autoencoder_num_res_blocks\",\n\t            type=int,\n\t            default=2,\n\t            help=\"number of residual blocks per level\",\n\t        )\n\t        autoencoder.add_argument(\n\t            \"--groups\",\n\t            type=int,\n\t            default=32,\n", "            help=\"number of groups for GroupNorm\",\n\t        )\n\t    def __init__(\n\t        self,\n\t        cfg,\n\t    ):\n\t        # check params\n\t        assert (\n\t            cfg.out_channels is None or cfg.out_channels == cfg.in_channels\n\t        ), f\"input channels({cfg.input_channels}) of image should be equal to output channels({cfg.out_channels})\"\n", "        super(AutoEncoderKL, self).__init__()\n\t        self.latent_channels = latent_channels = cfg.latent_channels\n\t        self.encoder = self.build_encoder(cfg)\n\t        self.decoder = self.build_decoder(cfg)\n\t        # Convolution to map from embedding space to\n\t        # quantized embedding space moments (mean and log variance)\n\t        self.quant_conv = nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1)\n\t        # Convolution to map from quantized embedding space back to\n\t        # embedding space\n\t        self.post_quant_conv = nn.Conv2d(latent_channels, latent_channels, 1)\n", "    @classmethod\n\t    def build_encoder(cls, cfg):\n\t        return Encoder(\n\t            in_channels=cfg.in_channels,\n\t            out_channels=cfg.latent_channels,\n\t            channels_list=cfg.autoencoder_channels_list,\n\t            num_res_blocks=cfg.autoencoder_num_res_blocks,\n\t            groups=cfg.groups,\n\t        )\n\t    @staticmethod\n", "    def build_decoder(cfg):\n\t        return Decoder(\n\t            in_channels=cfg.latent_channels,\n\t            out_channels=cfg.out_channels or cfg.in_channels,\n\t            channels_list=cfg.autoencoder_channels_list,\n\t            num_res_blocks=cfg.autoencoder_num_res_blocks,\n\t            groups=cfg.groups,\n\t        )\n\t    def encode(self, img: torch.Tensor) -> GaussianDistribution:\n\t        \"\"\"\n", "        Encode image into latent vector\n\t        Args:\n\t            - x (torch.Tensor):\n\t                  image, shape = `[batch, channel, height, width]`\n\t        Returns:\n\t            - gaussian distribution (torch.Tensor):\n\t        \"\"\"\n\t        z = self.encoder(img)\n\t        # Get the moments in the quantized embedding space\n\t        moments = self.quant_conv(z)\n", "        # Return the distribution(posterior)\n\t        return AutoEncoderKLOutput(GaussianDistribution(moments))\n\t    def decode(self, latent: torch.Tensor) -> torch.Tensor:\n\t        \"\"\"\n\t        Decode images from latent representation\n\t        Args:\n\t            - z(torch.Tensor):\n\t                  latent representation with shape `[batch_size, emb_channels, z_height, z_height]`\n\t        \"\"\"\n\t        # check params\n", "        assert (\n\t            latent.shape[1] == self.latent_channels\n\t        ), f\"Expected latent representation to have {self.latent_channels} channels, got {z.shape[1]}\"\n\t        z = self.post_quant_conv(latent)\n\t        return self.decoder(z)\n\tclass Encoder(nn.Module):\n\t    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        out_channels: int,\n", "        channels_list: List[int],\n\t        num_res_blocks: int,\n\t        groups: int = 4,\n\t    ):\n\t        super(Encoder, self).__init__()\n\t        self.conv_in = build_conv_in(\n\t            in_channels=in_channels, out_channels=channels_list[0]\n\t        )\n\t        levels = len(channels_list)\n\t        self.down, _, mid_ch, _, _ = build_input_blocks(\n", "            in_channels=channels_list[0],\n\t            num_res_blocks=num_res_blocks,\n\t            levels=levels,\n\t            channels_list=channels_list,\n\t            groups=groups,\n\t        )\n\t        self.bottleneck = build_bottleneck(\n\t            mid_ch, d_head=mid_ch, use_attn_only=True, groups=groups\n\t        )\n\t        self.out = build_final_output(\n", "            out_ch=mid_ch, out_channels=2 * out_channels, groups=groups\n\t        )\n\t    def forward(self, x):\n\t        x = self.conv_in(x)\n\t        for module in self.down:\n\t            x = module(x)\n\t        x = self.bottleneck(x)\n\t        for module in self.out:\n\t            x = module(x)\n\t        return x\n", "class Decoder(nn.Module):\n\t    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        out_channels: int,\n\t        channels_list: List[int],\n\t        num_res_blocks: int,\n\t        groups: int = 4,\n\t    ):\n\t        super(Decoder, self).__init__()\n", "        self.conv_in = build_conv_in(\n\t            in_channels=in_channels, out_channels=channels_list[0]\n\t        )\n\t        levels = len(channels_list)\n\t        self.bottleneck = build_bottleneck(\n\t            in_ch=channels_list[0],\n\t            d_head=channels_list[0],\n\t            use_attn_only=True,\n\t            groups=groups,\n\t        )\n", "        self.up, mid_ch = build_output_blocks(\n\t            num_res_blocks=num_res_blocks,\n\t            in_ch=channels_list[0],\n\t            levels=levels,\n\t            channels_list=channels_list,\n\t            groups=groups,\n\t        )\n\t        self.out = build_final_output(\n\t            out_ch=mid_ch, out_channels=out_channels, groups=groups\n\t        )\n", "    def forward(self, x):\n\t        x = self.conv_in(x)\n\t        x = self.bottleneck(x)\n\t        for module in self.up:\n\t            x = module(x)\n\t        for module in self.out:\n\t            x = module(x)\n\t        return x\n\t@dataclass\n\tclass AutoEncoderKLOutput:\n", "    latent_dist: \"GaussianDistribution\"\n"]}
{"filename": "stable_diffusion/models/latent_diffusion.py", "chunked_list": ["from typing import Optional\n\timport torch\n\tfrom torch import nn\n\tfrom tqdm import tqdm\n\tfrom stable_diffusion.models.autoencoder import AutoEncoderKL\n\tfrom stable_diffusion.models.clip_model import CLIPModel\n\tfrom stable_diffusion.models.scheduler import DDPMScheduler\n\tfrom stable_diffusion.models.unet import UNetModel\n\tclass LatentDiffusion(nn.Module):\n\t    def __init__(\n", "        self,\n\t        unet: UNetModel,\n\t        autoencoder: AutoEncoderKL,\n\t        text_encoder: CLIPModel,\n\t        noise_scheduler: DDPMScheduler,\n\t    ):\n\t        \"\"\"main class\"\"\"\n\t        super().__init__()\n\t        self.unet = unet\n\t        self.autoencoder = autoencoder\n", "        self.text_encoder = text_encoder\n\t        self.noise_scheduler = noise_scheduler\n\t    def pred_noise(\n\t        self,\n\t        noised_sample: torch.Tensor,\n\t        time_step: torch.Tensor,\n\t        context_emb: torch.Tensor,\n\t        guidance_scale: float = 1.0,\n\t    ):\n\t        \"\"\"\n", "        predicts the noise added on latent vector at time step=t\n\t        Args:\n\t            - x (torch.Tensor):\n\t                  noised latent vector, shape = `[batch, channels, height, width]`\n\t            - time_steps (torch.Tensor):\n\t                  time steps, shape = `[batch]`\n\t            - context_emb (torch.Tensor):\n\t                  conditional embedding, shape = `[batch, seq_len, d_model]`\n\t        Returns:\n\t            - pred noise (torch.Tensor):\n", "                  predicted noise, shape = `[batch, channels, height, width]`\n\t        \"\"\"\n\t        do_classifier_free_guidance = guidance_scale > 1\n\t        if not do_classifier_free_guidance:\n\t            return self.unet(noised_sample, time_step, context_emb)\n\t        t_in = torch.cat([time_step] * 2)\n\t        x_in = torch.cat([noised_sample] * 2)\n\t        bsz = noised_sample.shape[0]\n\t        tokenized_text = self.text_encoder.tokenize([\"\"] * bsz).input_ids.to(\n\t            noised_sample.device\n", "        )\n\t        uncond_emb = self.text_encoder.encode_text(\n\t            tokenized_text  # adding `.to(self.weight_dtype)` causes error...\n\t        )[0]\n\t        c_in = torch.cat([uncond_emb, context_emb])\n\t        pred_noise_cond, pred_noise_uncond = torch.chunk(\n\t            self.unet(x_in, t_in, c_in), 2, dim=0\n\t        )\n\t        return pred_noise_cond + guidance_scale * (pred_noise_cond - pred_noise_uncond)\n\t    def sample(\n", "        self,\n\t        noised_sample: torch.Tensor,\n\t        context_emb: torch.Tensor,\n\t        guidance_scale: float = 7.5,\n\t        repeat_noise: bool = False,\n\t        scale_factor: float = 1.0,\n\t        time_steps: Optional[int] = None,\n\t    ):\n\t        \"\"\"\n\t        Sample loop to get x_0 given x_t and conditional embedding\n", "        Args:\n\t            - noised_sample (torch.Tensor):\n\t                  original x_t of shape=`[batch, channels, height, width]`\n\t            - context_emb (torch.Tensor):\n\t                  conditional embedding of shape=`[batch, seq_len, context_dim]`\n\t            - guidence_scale (float, optional):\n\t                  scale used for classifer free guidance. Default: `7.5`.\n\t            - repeat_noise (bool, optional):\n\t                  whether use the same noise in a batch duuring each p_sample. Default: `False`.\n\t            - scale_factor (float, optional):\n", "                  scaling factor of noise. Default: `1.0`.\n\t        Returns:\n\t            - x_0 (torch.Tensor):\n\t                  denoised latent x_0 of shape=`[batch, channels, height, width]`\n\t        \"\"\"\n\t        bsz = noised_sample.shape[0]\n\t        # Get x_T\n\t        x = noised_sample\n\t        # Time steps to sample at $T - t', T - t' - 1, \\dots, 1$\n\t        # Sampling loop\n", "        if time_steps is not None:\n\t            noise_time_steps = range(time_steps - 1, -1, -1)\n\t        else:\n\t            noise_time_steps = self.noise_scheduler.noise_time_steps\n\t        progress_bar = tqdm(reversed(noise_time_steps), desc=\"Sampling\")\n\t        for step in progress_bar:\n\t            # fill time step t from int to tensor of shape=`[batch]`\n\t            time_step = x.new_full((bsz,), step, dtype=torch.long)\n\t            pred_noise = self.pred_noise(\n\t                noised_sample=x,\n", "                time_step=time_step,\n\t                context_emb=context_emb,\n\t                guidance_scale=guidance_scale,\n\t            )\n\t            # Sample x_{t-1}\n\t            x, pred_x0 = self.noise_scheduler.step(\n\t                pred_noise=pred_noise,\n\t                x_t=x,\n\t                time_step=step,\n\t                repeat_noise=repeat_noise,\n", "                scale_factor=scale_factor,\n\t            )\n\t        # Return $x_0$\n\t        return x\n"]}
{"filename": "stable_diffusion/models/utils.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   utils.py\n\t@Time    :   2023/05/15 12:55:11\n\t@Author  :   Wenbo Li\n\t@Desc    :   Util class for build model blocks\n\t\"\"\"\n\tfrom typing import Optional\n\timport numpy as np\n", "import torch\n\timport torch.nn as nn\n\tfrom torch.nn import ModuleList\n\tfrom ..modules.timestep_embedding import TimestepEmbedSequential\n\tfrom ..modules.resnet2d import DownSample, ResBlock, UpSample\n\tfrom ..modules.transformer import SpatialTransformer, CrossAttention\n\tdef zero_module(module):\n\t    \"\"\"\n\t    Zero out the parameters of a module and return it.\n\t    \"\"\"\n", "    for p in module.parameters():\n\t        p.detach().zero_()\n\t    return module\n\tdef build_conv_in(in_channels: int, out_channels: int):\n\t    return nn.Conv2d(in_channels, out_channels, 3, padding=1)\n\tdef build_input_blocks(\n\t    in_channels: int,\n\t    num_res_blocks: int,\n\t    attention_resolutions: Optional[list] = None,\n\t    n_heads: int = 1,\n", "    n_layers: int = 1,\n\t    dropout: float = 0.0,\n\t    context_dim: Optional[int] = None,\n\t    levels: int = 1,\n\t    time_emb_dim: Optional[int] = None,\n\t    channels_list: list = None,\n\t    groups: int = 4,\n\t):\n\t    input_blocks = nn.ModuleList([])\n\t    # num of channels at each block\n", "    # will use reversly in output blocks\n\t    input_block_channels = [in_channels]\n\t    # * add levels input blocks(down sample layers)\n\t    in_ch = in_channels\n\t    # @ note: in labmlai, they use level in attn_resolutions but in origin stable diffusion paper, they use ds = [1,2,4,...],total num levels\n\t    attn_mult = 1\n\t    d_head = None\n\t    for level in range(levels):\n\t        for _ in range(num_res_blocks):\n\t            # Residual block maps from previous number of channels to the number of\n", "            # channels in the current level\n\t            out_ch = channels_list[level]\n\t            layers = [\n\t                ResBlock(\n\t                    in_channels=in_ch,\n\t                    out_channels=out_ch,\n\t                    time_emb_dim=time_emb_dim,\n\t                    groups=groups,\n\t                )\n\t            ]\n", "            in_ch = (\n\t                out_ch  # this level's output channels is next level's input channels\n\t            )\n\t            # * add attention layers\n\t            if attention_resolutions is not None and attn_mult in attention_resolutions:\n\t                d_head = in_ch // n_heads\n\t                layers.append(\n\t                    SpatialTransformer(\n\t                        in_ch,\n\t                        n_heads=n_heads,\n", "                        d_head=d_head,\n\t                        n_layers=n_layers,\n\t                        dropout=dropout,\n\t                        context_dim=context_dim,\n\t                        groups=groups,\n\t                    )\n\t                )\n\t            input_blocks.append(TimestepEmbedSequential(*layers))\n\t            input_block_channels.append(in_ch)\n\t        # * add DownSample block except last level\n", "        if level != levels - 1:\n\t            input_blocks.append(TimestepEmbedSequential(DownSample(in_ch)))\n\t            # add additional in_channels for downsample\n\t            input_block_channels.append(in_ch)\n\t            attn_mult *= (\n\t                2  # double it for next itr, this won't be multiplied in the last layer\n\t            )\n\t    return input_blocks, input_block_channels, in_ch, d_head, attn_mult\n\tdef build_bottleneck(\n\t    in_ch: int,\n", "    time_emb_dim: Optional[int] = None,\n\t    n_heads: int = 1,\n\t    d_head: int = 1,\n\t    n_layers: int = 1,\n\t    dropout: float = 0.0,\n\t    context_dim: Optional[int] = None,\n\t    use_attn_only: bool = False,\n\t    groups: int = 4,\n\t):\n\t    return TimestepEmbedSequential(\n", "        ResBlock(in_channels=in_ch, time_emb_dim=time_emb_dim, dropout=dropout),\n\t        (\n\t            CrossAttention(\n\t                query_dim=in_ch, n_heads=n_heads, d_head=d_head, dropout=dropout\n\t            )\n\t            if use_attn_only\n\t            else SpatialTransformer(\n\t                in_ch,\n\t                n_heads=n_heads,\n\t                d_head=d_head,\n", "                n_layers=n_layers,\n\t                dropout=dropout,\n\t                context_dim=context_dim,\n\t                groups=groups,\n\t            )\n\t        ),\n\t        ResBlock(\n\t            in_channels=in_ch, time_emb_dim=time_emb_dim, dropout=dropout, groups=groups\n\t        ),\n\t    )\n", "def build_output_blocks(\n\t    num_res_blocks: int,\n\t    attention_resolutions: Optional[list] = None,\n\t    n_heads: int = 1,\n\t    n_layers: int = 1,\n\t    dropout: float = 0.0,\n\t    context_dim: Optional[int] = None,\n\t    input_block_channels: Optional[ModuleList] = None,\n\t    levels: int = 1,\n\t    time_emb_dim: Optional[int] = None,\n", "    channels_list: list = None,\n\t    in_ch: int = 1,\n\t    attn_mult: Optional[int] = None,\n\t    groups: int = 4,\n\t):\n\t    output_blocks = nn.ModuleList([])\n\t    for level in reversed(range(levels)):\n\t        # * add resblocks\n\t        for res_block in range(num_res_blocks + 1):\n\t            # Residual block maps from previous number of channels to the number of\n", "            # channels in the current level\n\t            # * note: here should add input_block_channels.pop() because input blocks are used as skip connection\n\t            out_ch = channels_list[level]\n\t            layers = [\n\t                ResBlock(\n\t                    in_channels=in_ch + input_block_channels.pop()\n\t                    if input_block_channels\n\t                    else in_ch,\n\t                    out_channels=out_ch,\n\t                    time_emb_dim=time_emb_dim,\n", "                    dropout=dropout,\n\t                    groups=groups,\n\t                )\n\t            ]\n\t            in_ch = out_ch\n\t            # * add attention layers\n\t            if attention_resolutions is not None and attn_mult in attention_resolutions:\n\t                d_head = in_ch // n_heads\n\t                layers.append(\n\t                    SpatialTransformer(\n", "                        in_ch,\n\t                        n_heads=n_heads,\n\t                        d_head=d_head,\n\t                        n_layers=n_layers,\n\t                        dropout=dropout,\n\t                        context_dim=context_dim,\n\t                        groups=groups,\n\t                    )\n\t                )\n\t            # *add UpSample except the last one, note that in reversed order, level==0 is the last\n", "            if level != 0 and res_block == num_res_blocks:\n\t                layers.append(TimestepEmbedSequential(UpSample(in_ch)))\n\t                if attn_mult:\n\t                    attn_mult //= 2\n\t            output_blocks.append(TimestepEmbedSequential(*layers))\n\t    return output_blocks, in_ch\n\tdef build_final_output(out_ch: int, out_channels: int, groups: int = 4):\n\t    return nn.Sequential(\n\t        nn.GroupNorm(groups, out_ch),\n\t        nn.SiLU(),\n", "        nn.Conv2d(\n\t            in_channels=out_ch, out_channels=out_channels, kernel_size=3, padding=1\n\t        ),\n\t    )\n\tif __name__ == \"__main__\":\n\t    batch = 10\n\t    image_size = 64\n\t    in_channels = 3\n\t    channels = 128\n\t    out_channels = 3\n", "    tim_emb_dim = 512\n\t    x = np.random.randn(batch, in_channels, image_size, image_size)\n\t    x = torch.from_numpy(x).float()\n\t    # test upsample\n\t    upsample = UpSample(\n\t        in_channels=channels,\n\t    )\n\t    up = upsample(x)\n\t    print(up.shape)\n\t    # test downsample\n", "    downsample = DownSample(\n\t        in_channels=channels,\n\t    )\n\t    down = downsample(x)\n\t    print(down.shape)\n\t    # test resblock\n\t    resblock = ResBlock(\n\t        in_channels=channels, out_channels=out_channels, time_emb_dim=tim_emb_dim\n\t    )\n\t    res = resblock(x, time_emb=torch.ones(size=(batch, tim_emb_dim)))\n", "    print(res.shape)\n"]}
{"filename": "stable_diffusion/modules/timestep_embedding.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   timestep_embedding.py\n\t@Time    :   2023/05/22 22:54:29\n\t@Author  :   Wenbo Li\n\t@Desc    :   Time step embedding implement\n\t\"\"\"\n\tfrom abc import abstractmethod\n\tfrom typing import Optional\n", "import torch\n\timport math\n\tfrom torch import nn\n\tfrom .resnet2d import ResBlock\n\tfrom .transformer import SpatialTransformer\n\tclass TimestepBlock(nn.Module):\n\t    \"\"\"\n\t    Any module where forward() takes timestep embeddings as a second argument.\n\t    \"\"\"\n\t    @abstractmethod\n", "    def forward(self, x, time_emb):\n\t        pass\n\tclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n\t    def forward(\n\t        self,\n\t        x: torch.Tensor,\n\t        time_emb: Optional[torch.Tensor] = None,\n\t        context: Optional[torch.Tensor] = None,\n\t    ):\n\t        \"\"\"\n", "        forward pass of TimestepEmbeddingSequential\n\t        passed in layers are basically:\n\t            `ResBlock`, `Conv2d`, `UpSample`,`DownSample`, `SpatialTransformer`\n\t        apply different forward pass depending on instance type\n\t        Args:\n\t            - x (Tensor):\n\t                  input shape=[batch, in_channels, height, width]\n\t            - time_emb (Tensor):\n\t                  input shape=[batch, time_emb_dim]\n\t            - context (Tensor, optional):\n", "                  input shape=[batch, seq_len, context_dim]. Default: None.\n\t        Returns:\n\t            - Tensor:\n\t                - output shape:\n\t                    - `ResBlock`:           [batch, out_channels, height, width]\n\t                    - `SpatialTransformer`: [batch, out_channels, height, width]\n\t                    - `Conv2d`:             [batch, out_channels, height, width]\n\t                    - `UpSample`:           [batch, out_channels, height*scale_factor, width*scale_factor]\n\t                    - `DownSample`:         [batch, out_channels, height/scale_factor, width/scale_factor]\n\t        \"\"\"\n", "        for module in self:\n\t            # pass ResBlock\n\t            if isinstance(module, ResBlock):\n\t                x = module(x, time_emb)\n\t            # pass spatial transformer\n\t            elif isinstance(module, SpatialTransformer):\n\t                x = module(x, context)\n\t            # pass Conv2d, UpSample, DownSample\n\t            else:\n\t                x = module(x)\n", "        return x\n\tdef sinusoidal_time_proj(\n\t    time_steps: torch.Tensor, emb_dim: int, max_len: int = 10000\n\t) -> torch.Tensor:\n\t    \"\"\"\n\t    sinusoidal time step embedding implementation\n\t    Args:\n\t        - time_steps (torch.Tensor):\n\t                time step of shape [batch_size,]\n\t        - emb_dim (int):\n", "                embed dimension\n\t        - max_len (int, optional):\n\t                max len of embedding. Default: 10000.\n\t    Returns:\n\t        - torch.Tensor:\n\t                time embedding of shape [batch, emb_dim]\n\t    \"\"\"\n\t    # half is sin, the other half is cos\n\t    half = emb_dim // 2\n\t    freq = torch.exp(\n", "        math.log(max_len)\n\t        / (half)\n\t        * torch.arange(\n\t            0, end=half, dtype=torch.float32\n\t        )  # get the position of each time step\n\t    ).to(time_steps.device)\n\t    # shape=[batch, 1]*[1, half] = [batch, half]\n\t    # freq[None]: shape=[half] -> [1, half]\n\t    # time_steps[:, None]: shape=[batch] -> [batch, 1]\n\t    args = time_steps[:, None].float() * freq[None]\n", "    # shape=[batch, emb_dim]\n\t    return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n"]}
{"filename": "stable_diffusion/modules/resnet2d.py", "chunked_list": ["import torch.nn as nn\n\timport numpy as np\n\timport torch\n\tfrom typing import Optional\n\timport torch.nn.functional as F\n\tfrom ..models import utils\n\tclass UpSample(nn.Module):\n\t    \"\"\"\n\t    constructor for UpSample layer\n\t    Architecture:\n", "        - Interpolate\n\t        - Conv2d\n\t    Args:\n\t        - in_channels (int):\n\t                input  num of channels\n\t        - out_channels (int, optional):\n\t                output  num of channels\n\t        - scale_factor (int, optional):\n\t                Up Sample by a factor of `scale factor`. Default: `2`.\n\t        - padding (int, optional):\n", "                padding for `Conv2d`. Default: `1`.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        out_channels: Optional[int] = None,\n\t        scale_factor: float = 2,\n\t        padding: int = 1,\n\t    ):\n\t        super().__init__()\n", "        self.in_channels = in_channels\n\t        # if has out_channels passed in, use it or use default = in_channels\n\t        self.out_channels = out_channels or in_channels\n\t        self.scale_factor = scale_factor\n\t        # * default we use Conv2d, use_conv and conv_nd are not implemented for simpilicity\n\t        self.conv = nn.Conv2d(\n\t            self.in_channels, self.out_channels, kernel_size=3, padding=padding\n\t        )\n\t    def forward(self, x) -> torch.Tensor:\n\t        \"\"\"\n", "        forward Up Sample layer which\n\t        Args:\n\t            - x (Tensor):\n\t                  input shape=`[batch, in_channels, height, width]`\n\t        Returns:\n\t            - Tensor:\n\t                  output shape=`[batch, out_channels, height*scale_factor, width*scale_factor]`\n\t        \"\"\"\n\t        assert (\n\t            x.shape[1] == self.in_channels\n", "        ), f\"input channel does not match: x.shape[1]({x.shape[1]}) != self.in_channels({self.in_channels}))\"\n\t        # * e.g. [1,3,256,256] -> [1,3,512,512]\n\t        x = F.interpolate(x, scale_factor=self.scale_factor, mode=\"nearest\")\n\t        x = self.conv(x)\n\t        return x\n\tclass DownSample(nn.Module):\n\t    \"\"\"\n\t    constructor for DownSample layer\n\t    Architecture:\n\t        - Conv2d\n", "        - Interpolate\n\t    Args:\n\t        - in_channels (int):\n\t                input  num of channels\n\t        - out_channels (int, optional):\n\t                output  num of channels\n\t        - scale_factor (int, optional):\n\t                Down Sample by a factor of `scale factor`. Default: 1/2.\n\t        - padding (int, optional):\n\t                padding for Conv2d. Default: 1.\n", "    \"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        out_channels: Optional[int] = None,\n\t        scale_factor: float = 1 / 2,\n\t        padding: int = 1,\n\t    ):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n", "        self.scale_factor = scale_factor\n\t        # if has out_channels passed in, use it or use default = in_channels\n\t        self.out_channels = out_channels or in_channels\n\t        # It turn out that origin openai unet model did not use UpSample and DownSample,\n\t        # instead, they use ResNetBlock with parameters up, down\n\t        # Here we use UpSample and DownSample to make it clean\n\t        # Change: => add stride=2, to downsample by a factor of 2\n\t        # or use scale factor = 1/2\n\t        # TODO: compare difference between stride=2 and scale_factor=1/2\n\t        self.conv = nn.Conv2d(\n", "            self.in_channels, self.out_channels, kernel_size=3, padding=padding\n\t        )\n\t    def forward(self, x) -> torch.Tensor:\n\t        \"\"\"\n\t        forward Down Sample layer which reduce height and width of x by a factor of 2\n\t        Args:\n\t            - x (Tensor):\n\t                  input shape=[batch, in_channels, height, width]\n\t        Returns:\n\t            - Tensor:\n", "                  output shape=[batch, out_channels, height/scale_factor, width/scale_factor]\n\t        \"\"\"\n\t        assert (\n\t            x.shape[1] == self.in_channels\n\t        ), f\"input channel does not match: x.shape[1]({x.shape[1]}) != self.in_channels({self.in_channels}))\"\n\t        x = self.conv(x)\n\t        x = F.interpolate(x, scale_factor=self.scale_factor, mode=\"nearest\")\n\t        return x\n\tclass ResBlock(nn.Module):\n\t    \"\"\"\n", "    ResBlock used in U-Net and AutoEncoder\n\t    Archetecture::\n\t        - in_layers = [`GroupNorm`, `SiLU`, `Conv2d`]\n\t        - time_emb = [`SiLU`, `Linear`]\n\t        - out_layers = [`GroupNorm`, `SiLU`, `Dropout`]\n\t    Args:\n\t        - in_channels (int):\n\t              input num of channels\n\t        - out_channels (Optional[int], optional):\n\t              output num of channels. Default: equal to `in_channels`.\n", "        - time_emb_dim (int, optional):\n\t              time embedding dim. Default: `512`.\n\t        - dropout (int, optional):\n\t              dropout rate. Default: `0.`\n\t        - padding (int, optional):\n\t              padding idx. Default: `1`.\n\t        - groups (int, optional):\n\t              num of groups for `GroupNorm`. Default: `2`.\n\t    \"\"\"\n\t    def __init__(\n", "        self,\n\t        in_channels: int,\n\t        out_channels: Optional[int] = None,\n\t        time_emb_dim: Optional[int] = None,\n\t        dropout: Optional[int] = 0,\n\t        padding: Optional[int] = 1,\n\t        groups: int = 2,\n\t    ) -> None:\n\t        super().__init__()\n\t        # check parameters\n", "        assert (\n\t            in_channels % groups == 0\n\t        ), f\"in_channels({in_channels}) must be divisible by num_groups({groups})\"\n\t        self.in_channels = in_channels\n\t        # if has out_channels passed in, use it or use default = in_channels\n\t        self.out_channels = out_channels or in_channels\n\t        self.time_emb_dim = time_emb_dim\n\t        self.dropout = dropout\n\t        self.in_layers = nn.Sequential(\n\t            nn.GroupNorm(num_groups=groups, num_channels=in_channels),\n", "            nn.SiLU(),\n\t            nn.Conv2d(\n\t                self.in_channels, self.out_channels, kernel_size=3, padding=padding\n\t            ),\n\t        )\n\t        # Time embedding\n\t        if self.time_emb_dim:\n\t            self.time_embedding = nn.Sequential(\n\t                nn.SiLU(),\n\t                nn.Linear(self.time_emb_dim, self.out_channels),\n", "            )\n\t        else:\n\t            self.time_embedding = nn.Identity()\n\t        self.out_layers = nn.Sequential(\n\t            nn.GroupNorm(num_groups=groups, num_channels=self.out_channels),\n\t            nn.SiLU(),\n\t            nn.Dropout(self.dropout),\n\t            # ? openai model used zero_module(conv_nd)\n\t            # TODO: figure out why zero_module is used\n\t            # [batch, in_channel, height, width] => [batch, out_channel, height, height]\n", "            utils.zero_module(\n\t                nn.Conv2d(\n\t                    self.out_channels, self.out_channels, kernel_size=3, padding=padding\n\t                )\n\t            ),\n\t        )\n\t        # Map input to output channel\n\t        if self.in_channels != self.out_channels:\n\t            self.skip_connection = nn.Conv2d(\n\t                self.in_channels, self.out_channels, kernel_size=1\n", "            )\n\t        else:\n\t            self.skip_connection = nn.Identity()\n\t    def forward(\n\t        self, x: torch.Tensor, time_emb: Optional[torch.Tensor] = None\n\t    ) -> torch.Tensor:\n\t        \"\"\"\n\t        forward pass of ResBlock\n\t        Args:\n\t            - x (torch.Tensor):\n", "                  input shape = `[batch, in_channels, height, width]`\n\t            - time_emb (torch.Tensor):\n\t                  input shape = `[batch, time_emb_dim]`\n\t        Returns:\n\t            - torch.Tensor:\n\t                  output shape = `[batch, out_channels, height, width]`\n\t        \"\"\"\n\t        if time_emb is not None:\n\t            assert (\n\t                x.shape[0] == time_emb.shape[0]\n", "            ), f\"batch size does not match: x.shape[0]({x.shape[0]}) != time_emb.shape[0]({time_emb.shape[0]})\"\n\t            assert (\n\t                time_emb.shape[1] == self.time_emb_dim\n\t            ), f\"time_emb_dim does not match: time_emb.shape[1]({time_emb.shape[1]}) != self.time_emb_dim({self.time_emb_dim})\"\n\t        # h: [batch, out_channels, height, width]\n\t        h = self.in_layers(x)\n\t        # [batch, time_emb_dim] => [batch, out_channels] => [batch, out_channels, 1, 1]\n\t        if time_emb is not None:\n\t            time_emb = self.time_embedding(time_emb)\n\t            h += time_emb[:, :, None, None]\n", "        h = self.out_layers(h)\n\t        return h + self.skip_connection(x)\n"]}
{"filename": "stable_diffusion/modules/transformer.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- encoding: utf-8 -*-\n\t\"\"\"\n\t@File    :   attention.py\n\t@Time    :   2023/05/14 16:03:43\n\t@Author  :   Wenbo Li\n\t@Desc    :   Transformer module for Stable Diffusion U-Net\n\t\"\"\"\n\tfrom typing import Optional\n\timport torch\n", "from torch import nn\n\tfrom einops import rearrange\n\tfrom ..models import utils\n\tclass CrossAttention(nn.Module):\n\t    \"\"\"\n\t    Cross attention module for transformer\n\t    Architecture:\n\t        - Q, K, V = Linear(x), Linear(context), Linear(context)\n\t        - sim = Q * K^T / sqrt(dk)\n\t        - attn = softmax(sim) * V\n", "        - out = Linear(attn)\n\t    Args:\n\t        - query_dim (int):\n\t                query dimension\n\t        - context_dim (Optional[int]):\n\t                context dimension, if not previded, equal to query_dim\n\t        - n_heads (int):\n\t                num of heads\n\t        - d_head (int):\n\t                dim of each head\n", "        - dropout (float, optional):\n\t                dropout rate. Default: `0.`.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        query_dim: int,\n\t        context_dim: Optional[int] = None,\n\t        n_heads: int = 1,\n\t        d_head: int = 1,\n\t        dropout: float = 0.0,\n", "    ):\n\t        super().__init__()\n\t        if not context_dim:\n\t            context_dim = query_dim\n\t        d_model = n_heads * d_head\n\t        self.n_heads = n_heads\n\t        self.scale = 1 / (d_head**0.5)  # 1 / sqrt(d_k) from paper\n\t        self.to_q = nn.Linear(query_dim, d_model, bias=False)\n\t        self.to_k = nn.Linear(context_dim, d_model, bias=False)\n\t        self.to_v = nn.Linear(context_dim, d_model, bias=False)\n", "        self.out = nn.Sequential(nn.Linear(d_model, query_dim), nn.Dropout(dropout))\n\t    def forward(\n\t        self,\n\t        query: torch.Tensor,\n\t        context_emb: torch.Tensor = None,\n\t        mask: torch.Tensor = None,\n\t    ):\n\t        \"\"\"\n\t        Cross attention forward pass\n\t        - first calculate similarity between query and context embedding: sim = Q * K^T / sqrt(dk)\n", "        - then calculate attention value: attn = softmax(sim) * V\n\t        - finally, linear projection and dropout\n\t        Args:\n\t            - query (torch.Tensor):\n\t                  feature map of shape `[batch, height*width, query_dim]`\n\t                  height*width is equivalent to tgt_len in origin transformer\n\t            - context_emb (torch.Tensor, optional):\n\t                  conditional embeddings of shape `[batch, seq_len, context_dim]`. Default: `None`.\n\t                  seq_len is equivalent to src_len in origin transformer\n\t            - mask (torch.Tensor, optional):\n", "                  mask of shape = `[batch, height*width, seq_len]`. Default: `None`.\n\t                  actually never used...\n\t        \"\"\"\n\t        # if no context_emb, equal to self-attention\n\t        # when use cross attn without wrapped spatial transformer, convert to [batch, height*width, d_model] first\n\t        convert = len(query.shape) == 4\n\t        if convert:\n\t            h = query.shape[2]\n\t            query = rearrange(query, \"b c h w -> b (h w) c\")\n\t        if context_emb is None:\n", "            context_emb = query\n\t        Q, K, V = self.to_q(query), self.to_k(context_emb), self.to_v(context_emb)\n\t        # q: [batch, h*w, d_model] -> [batch * n_head, h*w, d_head]\n\t        # k,v: [batch, seq_len, d_model] -> [batch * n_head, seq_len, d_head]\n\t        Q, K, V = map(\n\t            lambda t: rearrange(\n\t                t, \"b n (n_heads d_head) -> (b n_heads) n d_head\", n_heads=self.n_heads\n\t            ),\n\t            (Q, K, V),\n\t        )\n", "        # similarity = Q*K^T / sqrt(dk): [batch * n_head, h*w, d_head] * [batch * n_head, d_head, seq_len] -> [batch * n_head, h*w, seq_len]\n\t        sim = torch.einsum(\"b n d,b m d->b n m\", Q, K) * self.scale\n\t        if mask is not None:\n\t            # repeat for each head\n\t            # mask: [batch, height*width, seq_len] -> [batch * n_head, height*width, seq_len]\n\t            mask = mask.repeat(\"b n m, b h n m\", h=self.n_heads)\n\t            max_neg_value = -torch.finfo(sim.dtype).max\n\t            sim.masked_fill_(mask, max_neg_value)\n\t        # softmax\n\t        attn = sim.softmax(dim=-1)\n", "        # attn value = attn*V: [batch * n_head, h*w, seq_len] * [batch * n_head, seq_len, d_head] -> [batch * n_head, h*w, d_head]\n\t        attn_v = torch.einsum(\"b n m,b m d->b n d\", attn, V)\n\t        attn_v = rearrange(\n\t            attn_v, \"(b n_heads) n d_head -> b n (n_heads d_head)\", n_heads=self.n_heads\n\t        )\n\t        out = self.out(attn_v)\n\t        # convert it back to [batch, channels, height, width]\n\t        if convert:\n\t            out = rearrange(out, \"b (h w) c -> b c h w\", h=h)\n\t        return out\n", "class FeedForward(nn.Module):\n\t    \"\"\"\n\t    origin paper use linear-relu-dropout-linear: `FFN(x) = max(0, xW1 + b1)W2 + b2`, equation(2) from Attention is all you need(https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)\n\t    here we use FFN(x) = `Dropout(GEGLU(x))*W + b`, where `GEGLU(x) = (xW + b) * GELU(xV + c)`\n\t    Architecture:\n\t        - GEGLU(x) = (xW + b) * GELU(xV + c)\n\t        - Dropout(GEGLU(x))\n\t        - Linear(Dropout(GEGLU(x)))\n\t    Args:\n\t        - d_model (int):\n", "                d_model from transformer paper\n\t        - dim_mult (int, optional):\n\t                multiplicative factor for the hidden layer size. Default: `4`.\n\t        - dropout (float, optional):\n\t                dropout rate. Default: `0.`.\n\t    Returns:\n\t    TODO: x shape and output shape\n\t        - Tensor:\n\t                _description_\n\t    \"\"\"\n", "    def __init__(self, d_model: int, dim_mult: int = 4, dropout: float = 0.0):\n\t        super().__init__()\n\t        self.net = nn.Sequential(\n\t            GEGLU(d_model, dim_mult * d_model),\n\t            nn.Dropout(p=dropout),\n\t            nn.Linear(d_model * dim_mult, d_model),\n\t        )\n\t    def forward(self, x: torch.Tensor):\n\t        return self.net(x)\n\tclass GEGLU(nn.Module):\n", "    \"\"\"\n\t    GeGLU(x) = (xW + b) * GELU(xV + c) from paper: https://arxiv.org/abs/2002.05202\n\t    Architecture:\n\t        - xW + b, xV + c = Linear(x)\n\t        - out = (xW + b) * GELU(xV + c)\n\t    Args:\n\t        - in_features (int):\n\t                input feature dimension\n\t        - out_features (int):\n\t                output feature dimension\n", "    \"\"\"\n\t    def __init__(self, in_features: int, out_features: int):\n\t        super().__init__()\n\t        # xW + b and xV + c all together\n\t        self.proj = nn.Linear(in_features, out_features * 2)\n\t        self.gelu = nn.GELU()\n\t    def forward(self, x: torch.Tensor):\n\t        \"\"\"\n\t        forward pass of GEGLU\n\t        Args:\n", "            - x (torch.Tensor):\n\t                  shape=`[]`\n\t        Returns:\n\t            - torch.Tensor:\n\t                  shape=`[]`\n\t        \"\"\"\n\t        x, gate = self.proj(x).chunk(2, dim=-1)\n\t        return x * self.gelu(gate)\n\tclass BasicTransformerBlock(nn.Module):\n\t    \"\"\"\n", "    Basic Transformer Block\n\t    Architecture:\n\t        - self attention = `CrossAttention(context=None)`\n\t        - add norm\n\t        - cross attention = `CrossAttention(context)`\n\t        -add norm\n\t        - feed forward = `FeedForward`\n\t        - add norm\n\t    Args:\n\t        - d_model (int):\n", "                dim of embedding\n\t        - n_heads (int):\n\t                num of heads\n\t        - d_head (int):\n\t                dim of each head\n\t        - dropout (float, optional):\n\t                dropout rate. Default: `0.`.\n\t        - context_dim (int, optional):\n\t                dim of conditional context. Default: `768`.\n\t    \"\"\"\n", "    def __init__(\n\t        self,\n\t        d_model: int,\n\t        n_heads: int,\n\t        d_head: int,\n\t        dropout: float = 0.0,\n\t        context_dim: int = 768,\n\t    ):\n\t        super().__init__()\n\t        self.d_model = d_model\n", "        self.context_dim = context_dim\n\t        self.self_attn = CrossAttention(\n\t            d_model,\n\t            context_dim=d_model,\n\t            n_heads=n_heads,\n\t            d_head=d_head,\n\t            dropout=dropout,\n\t        )\n\t        self.norm1 = nn.LayerNorm(d_model)\n\t        self.cross_attn = CrossAttention(\n", "            d_model,\n\t            context_dim=context_dim,\n\t            n_heads=n_heads,\n\t            d_head=d_head,\n\t            dropout=dropout,\n\t        )\n\t        self.norm2 = nn.LayerNorm(d_model)\n\t        self.ffn = FeedForward(d_model, dropout=dropout)\n\t        self.norm3 = nn.LayerNorm(d_model)\n\t    def forward(self, x: torch.Tensor, context_emb: Optional[torch.Tensor] = None):\n", "        \"\"\"\n\t        forward pass of BasicTransformerBlock\n\t        Args:\n\t            - x (torch.Tensor):\n\t                   input embeddings of shape `[batch_size, height * width, d_model]`\n\t            - context (torch.Tensor, optional):\n\t                  conditional embeddings of shape `[batch_size,  seq_len, context_dim]`\n\t        Returns:\n\t            - torch.Tensor:\n\t                  x with attention and skip connection and normalization, shape=`[batch_size, height * width, d_model]`\n", "        \"\"\"\n\t        # check params\n\t        assert (\n\t            x.shape[-1] == self.d_model\n\t        ), f\"input dim {x.shape[-1]} should be equal to d_model {self.d_model}\"\n\t        if context_emb is not None:\n\t            assert (\n\t                context_emb.shape[-1] == self.context_dim\n\t            ), f\"context dim {context_emb.shape[-1]} should be equal to context_dim {self.context_dim}\"\n\t        # self attention\n", "        x = self.norm1(x + self.self_attn(x, context_emb=None))\n\t        # cross attention\n\t        x = self.norm2(x + self.cross_attn(x, context_emb=context_emb))\n\t        # feed forward\n\t        x = self.norm3(x + self.ffn(x))\n\t        return x\n\tclass SpatialTransformer(nn.Module):\n\t    \"\"\"\n\t    Transformer block for image-like data.\n\t    Architecture:\n", "        - norm = GroupNorm\n\t        - proj_in = Conv2d\n\t        - transformer_blocks = [BasicTransformerBlock] * n_layers\n\t        - proj_out = Conv2d\n\t    Args:\n\t        - in_channels (int):\n\t            input num of channels in the feature map\n\t        - n_heads (int):\n\t            num of attention heads\n\t        - d_head (int):\n", "            dim of each head\n\t        - n_layer (int, optional):\n\t            num of transformer block. Default: `1`.\n\t        - dropout (float, optional):\n\t            dropout rate. Default: `0.`.\n\t        - context_dim (int, optional):\n\t            dim of context condition. Default: `None`.\n\t        - groups (int, optional):\n\t            num of groups for GroupNorm. Default: `2`.\n\t    \"\"\"\n", "    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        n_heads: int,\n\t        d_head: int,\n\t        n_layers: int = 1,\n\t        dropout: float = 0.0,\n\t        context_dim: int = None,\n\t        groups: int = 2,\n\t    ):\n", "        super().__init__()\n\t        # check params\n\t        assert (\n\t            n_heads > 0\n\t        ), f\"n_heads({n_heads}) should be greater than 0 for SpatialTransformer\"\n\t        assert (\n\t            in_channels % groups == 0\n\t        ), f\"in_channels({in_channels}) should be divisible by num_groups({groups}) for GroupNorm\"\n\t        self.in_channels = in_channels\n\t        self.context_dim = context_dim\n", "        self.norm = nn.GroupNorm(groups, in_channels)\n\t        self.proj_in = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)\n\t        # Transformer layers\n\t        # @ note: origin openai code use inner_dim = n_heads * d_head, but if legacy, d_head = in_channels // n_heads\n\t        # => here we use in_channels for simiplicity\n\t        self.transformer_blocks = nn.ModuleList(\n\t            [\n\t                BasicTransformerBlock(\n\t                    in_channels,\n\t                    n_heads,\n", "                    d_head,\n\t                    dropout=dropout,\n\t                    context_dim=context_dim,\n\t                )\n\t                for _ in range(n_layers)\n\t            ]\n\t        )\n\t        self.proj_out = utils.zero_module(\n\t            nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)\n\t        )\n", "    def forward(\n\t        self, x: torch.Tensor, context_emb: torch.Tensor = None\n\t    ) -> torch.Tensor:\n\t        \"\"\"\n\t        forward pass\n\t        Args:\n\t            - x (torch.Tensor):\n\t                  feature map of shape `[batch_size, channels, height, width]`\n\t            - context_emb (torch.Tensor, optional):\n\t                  conditional embeddings of shape `[batch_size,  seq_len, context_dim]`. Default: `None`.\n", "        Returns:\n\t            - torch.Tensor:\n\t                  shape=`[batch_size, channels, height, width]`\n\t        \"\"\"\n\t        # check params\n\t        assert (\n\t            x.shape[1] == self.in_channels\n\t        ), f\"input channels {x.shape[1]} should be equal to in_channels {self.in_channels}\"\n\t        if context_emb is not None:\n\t            assert (\n", "                context_emb.shape[-1] == self.context_dim\n\t            ), f\"context dim {context_emb.shape[-1]} should be equal to context_dim {self.context_dim}\"\n\t        # use for skip connection\n\t        x_in = x\n\t        x = self.norm(x)\n\t        x = self.proj_in(x)\n\t        x = rearrange(x, \"b c h w -> b (h w) c\")\n\t        for module in self.transformer_blocks:\n\t            x = module(x, context_emb=context_emb)\n\t        x = rearrange(x, \"b (h w) c -> b c h w\", h=x_in.shape[2])\n", "        x = self.proj_out(x)\n\t        return x + x_in\n\tif __name__ == \"__main__\":\n\t    x = torch.randn(2, 128, 32, 32)\n\t    context = torch.randn(2, 10, 768)\n\t    model = SpatialTransformer(128, 4, 32, n_layers=2, context_dim=768)\n\t    y = model(x, context)\n\t    print(y.shape)\n"]}
{"filename": "stable_diffusion/modules/__init__.py", "chunked_list": []}
{"filename": "stable_diffusion/modules/distributions.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\tclass GaussianDistribution(nn.Module):\n\t    def __init__(self, moments: torch.Tensor):\n\t        super(GaussianDistribution, self).__init__\n\t        self.mean, self.log_var = torch.chunk(moments, 2, dim=1)\n\t    def sample(self):\n\t        std = torch.exp(0.5 * self.log_var)\n\t        eps = torch.randn_like(std)\n\t        return self.mean + eps * std\n", "    def kl(self):\n\t        self.var = torch.exp(self.log_var)\n\t        return 0.5 * torch.sum(\n\t            torch.pow(self.mean, 2) + self.var - 1.0 - self.log_var, dim=[1, 2, 3]\n\t        )\n"]}
