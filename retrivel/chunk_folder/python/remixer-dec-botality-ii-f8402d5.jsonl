{"filename": "custom_queue.py", "chunked_list": ["from contextlib import contextmanager\n\tfrom collections import defaultdict\n\timport time\n\tclass UserLimitedQueue:\n\t  max_tasks_per_user = None\n\t  def __init__(self, max_tasks_per_user):\n\t    self.task_count = defaultdict(int)\n\t    if self.max_tasks_per_user is None:\n\t      self.max_tasks_per_user = max_tasks_per_user\n\t  @contextmanager\n", "  def for_user(self, user_id):\n\t    if self.task_count[user_id] < self.max_tasks_per_user:\n\t      self.task_count[user_id] += 1\n\t      try:\n\t        yield True\n\t      finally:\n\t        self.task_count[user_id] -= 1\n\t    else:\n\t      yield False\n\tclass CallCooldown:\n", "  calls = {}\n\t  @classmethod\n\t  def check_call(cls, uid, function_name, timeout=30):\n\t    key = f'{uid}_{function_name}'\n\t    if key in cls.calls:\n\t      if time.time() - cls.calls[key] < timeout:\n\t        return False\n\t    cls.calls[key] = time.time()\n\t    return True\n\tdef semaphore_wrapper(semaphore, callback):\n", "  async def wrapped(*args, **kwargs):\n\t    async with semaphore:\n\t      return await callback(*args, **kwargs)\n\t  return wrapped\n"]}
{"filename": "automigration.py", "chunked_list": ["from dotenv import dotenv_values\n\tfrom utils import cprint\n\timport os\n\timport sys\n\timport json\n\tassert os.path.exists('.env') and os.path.exists('.env.example')\n\tsystem_env = dotenv_values('.env')\n\tdemo_env = dotenv_values('.env.example')\n\tdef check_new_keys_in_example_env():\n\t  towrite = '''\n", "'''\n\t  for key in demo_env:\n\t    if key not in system_env:\n\t      towrite += f\"{key}='{demo_env[key]}'\\n\"\n\t  if len(towrite) != 1:\n\t    with open('.env', 'a') as file:\n\t      file.write(towrite)\n\tDEPRECATED_KEYS = ['llm_active_model_type', 'sd_available_loras', 'tts_so_vits_svc_code_path']\n\tDEPRECATED_KVS = {\n\t  'llm_assistant_chronicler': ['gpt4all', 'minchatgpt', 'alpaca'],\n", "  'llm_python_model_type': 'cerebras_gpt'\n\t}\n\tdef check_deprecated_keys_in_dotenv():\n\t  for key in system_env:\n\t    if key in DEPRECATED_KEYS:\n\t      cprint(f'Warning! The key \"{key}\" has been deprecated! See CHANGELOG.md.', color='red')\n\t    value = system_env[key]\n\t    if len(value) < 1:\n\t      continue\n\t    if (value[0] != '[' and value[0] != '{'):\n", "      value = f'\"{value}\"'\n\t    value = json.loads(value)\n\t    if type(value) is str:\n\t      if key in DEPRECATED_KVS and value in DEPRECATED_KVS[key]:\n\t        cprint(f'Warning! The value \"{value}\" of \"{key}\" has been deprecated! See CHANGELOG.md.', color='yellow')\n\tcheck_new_keys_in_example_env()\n\tcheck_deprecated_keys_in_dotenv()"]}
{"filename": "bot.py", "chunked_list": ["import logging\n\timport automigration\n\tfrom aiogram import Bot, Dispatcher\n\tfrom config_reader import config\n\tfrom middleware import ChatActionMiddleware, AccessMiddleware, CooldownMiddleware, MediaGroupMiddleware\n\tfrom modules.sd import StableDiffusionModule\n\tfrom modules.tts import TextToSpeechModule\n\tfrom modules.admin import AdminModule\n\tfrom modules.llm import LargeLanguageModel\n\tfrom modules.tta import TextToAudioModule\n", "from modules.stt import SpeechToTextModule\n\tlogger = logging.getLogger(__name__)\n\tdp = Dispatcher()\n\tdp.message.middleware(AccessMiddleware())\n\tdp.message.middleware(ChatActionMiddleware())\n\tdp.message.middleware(CooldownMiddleware())\n\tdp.message.middleware(MediaGroupMiddleware())\n\tdef initialize(dp, bot):\n\t  available_modules = {\n\t    \"sd\": StableDiffusionModule,\n", "    \"tts\": TextToSpeechModule,\n\t    \"tta\": TextToAudioModule,\n\t    \"stt\": SpeechToTextModule,\n\t    \"admin\": AdminModule,\n\t    \"llm\": LargeLanguageModel\n\t  }\n\t  dp['modules'] = {}\n\t  for module in config.active_modules:\n\t    if module in available_modules:\n\t      dp['modules'][module] = available_modules[module](dp, bot)\n", "def main():\n\t  bot = Bot(token=config.bot_token.get_secret_value(), parse_mode=\"HTML\")\n\t  logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s',)\n\t  initialize(dp, bot)\n\t  print('running')\n\t  dp.run_polling(bot)\n\tif __name__ == \"__main__\":\n\t  main()"]}
{"filename": "config_reader.py", "chunked_list": ["from pydantic import BaseSettings, SecretStr, validator\n\tfrom typing import List, Dict\n\tfrom typing_extensions import Literal\n\tfrom utils import update_env\n\tclass Settings(BaseSettings):\n\t  bot_token: SecretStr\n\t  adminlist: List[int]\n\t  whitelist: List[int]\n\t  blacklist: List[int]\n\t  ignore_mode: Literal[\"blacklist\", \"whitelist\", \"both\"]\n", "  active_modules: List[str]\n\t  apply_mps_fixes: bool\n\t  tts_path: str\n\t  tts_voices: List[str]\n\t  tts_mode: Literal[\"local\", \"localhttp\", \"remote\"]\n\t  tts_replacements: Dict\n\t  tts_credits: str\n\t  tts_ffmpeg_path: str\n\t  tts_enable_so_vits_svc: bool\n\t  tts_so_vits_svc_4_0_code_path: str\n", "  tts_so_vits_svc_4_1_code_path: str\n\t  tts_so_vits_svc_base_tts_provider: Literal[\"say_macos\", \"built_in\"]\n\t  tts_so_vits_svc_voices: List[Dict]\n\t  tts_queue_size_per_user: int\n\t  tts_host: str\n\t  sd_host: str\n\t  sd_max_steps: int\n\t  sd_max_resolution: int\n\t  sd_available_samplers: List[str]\n\t  sd_extra_prompt: str\n", "  sd_extra_negative_prompt: str\n\t  sd_default_sampler: str\n\t  sd_default_n_iter: int\n\t  sd_default_width: int\n\t  sd_default_height: int\n\t  sd_default_tti_steps: int\n\t  sd_default_tti_cfg_scale: int\n\t  sd_default_iti_cfg_scale: int\n\t  sd_default_iti_steps: int\n\t  sd_default_iti_denoising_strength: float\n", "  sd_default_iti_sampler: str\n\t  sd_launch_process_automatically: bool\n\t  sd_launch_command: str\n\t  sd_launch_dir: str\n\t  sd_launch_waittime: int\n\t  sd_lora_custom_activations: Dict\n\t  sd_only_admins_can_change_models: bool\n\t  sd_queue_size_per_user: int\n\t  llm_host: str\n\t  llm_queue_size_per_user: int\n", "  llm_backend: Literal ['pytorch', 'llama_cpp', 'mlc_pb', 'remote_ob', 'remote_lcpp']\n\t  llm_python_model_type: Literal[\"gpt2\",\"gptj\", \"auto_hf\",\"llama_orig\", \"llama_hf\"]\n\t  llm_paths: Dict\n\t  llm_character: str\n\t  llm_history_grouping: Literal[\"user\", \"chat\"]\n\t  llm_max_history_items: int\n\t  llm_generation_cfg_override: Dict\n\t  llm_assistant_cfg_override: Dict\n\t  llm_assistant_chronicler: Literal[\"alpaca\", \"instruct\", \"raw\"]\n\t  llm_assistant_use_in_chat_mode: bool\n", "  llm_force_assistant_for_unsupported_models: bool\n\t  llm_max_tokens: int\n\t  llm_max_assistant_tokens: int\n\t  llm_lcpp_gpu_layers: int\n\t  llm_lcpp_max_context_size: int\n\t  llm_remote_launch_process_automatically: bool\n\t  llm_remote_launch_command: str\n\t  llm_remote_launch_dir: str\n\t  llm_remote_launch_waittime: int\n\t  llm_remote_model_name: str\n", "  stt_backend: Literal['whisper', 'silero', 'wav2vec2']\n\t  stt_model_path_or_name: str\n\t  stt_autoreply_mode: Literal['none', 'assistant', 'chat']\n\t  stt_autoreply_voice: str\n\t  stt_queue_size_per_user: int\n\t  tta_queue_size_per_user: int\n\t  tta_device: Literal[\"cpu\", \"cuda\", \"mps\"]\n\t  tta_music_model: str\n\t  tta_sfx_model: str\n\t  tta_duration: int\n", "  python_command: str\n\t  mm_preload_models_on_start: bool\n\t  mm_ram_cached_model_count_limit: int\n\t  mm_vram_cached_model_count_limit: int\n\t  mm_management_policy: Literal[\"COUNT\", \"MEMORY\", \"BOTH\", \"NONE\"]\n\t  mm_unload_order_policy: Literal[\"LEAST_USED\", \"OLDEST_USE_TIME\", \"OLDEST_LOAD_ORDER\", \"MEMORY_FOOTPRINT\"]\n\t  mm_autounload_after_seconds: int\n\t  @validator('sd_max_resolution', 'sd_default_width', 'sd_default_height')\n\t  def resolution_in_correct_ranges(cls, v):\n\t    if v % 64 != 0 or v < 256 or v > 2048:\n", "      raise ValueError('incorrect value')\n\t    return v\n\t  @validator('tts_so_vits_svc_voices')\n\t  def base_voices_exist(cls, v, values):\n\t    if not values['tts_enable_so_vits_svc']:\n\t      return v\n\t    for item in v:\n\t      provider = item.get('provider', values['tts_so_vits_svc_base_tts_provider'])\n\t      if provider == 'built_in':\n\t        if item['base_voice'] not in values['tts_voices']:\n", "          raise ValueError(f'base tts voice ({item[\"base_voice\"]}) does not exist')\n\t    return v\n\t  class Config:\n\t    env_file = '.env'\n\t    env_file_encoding = 'utf-8'\n\t# mirror all config changes to .env file\n\tclass SettingsWrapper(Settings):\n\t  def __setattr__(self, name, value):\n\t    update_env('.env', name, value)\n\t    super().__setattr__(name, value)\n", "config = SettingsWrapper()"]}
{"filename": "utils.py", "chunked_list": ["from io import BytesIO\n\timport base64\n\timport argparse\n\timport functools\n\timport sys\n\timport json\n\tasync def tg_image_to_data(photo, bot):\n\t  if not photo:\n\t    return None\n\t  file_bytes = BytesIO()\n", "  file_path = (await bot.get_file(file_id=photo[-1].file_id)).file_path\n\t  await bot.download_file(file_path, file_bytes)\n\t  image = 'data:image/png;base64,' + str(base64.b64encode(file_bytes.getvalue()), 'utf-8')\n\t  return image\n\tdef b64_to_img(imgstr):\n\t  from PIL import Image\n\t  decoded = base64.b64decode(imgstr.split(',')[1])\n\t  return Image.open(BytesIO(decoded))\n\t# do not sysexit on error\n\tclass CustomArgumentParser(argparse.ArgumentParser):\n", "  def error(self, message):\n\t    raise Exception(message)\n\t# join the rest of the arguments, so they can be validated\n\tclass JoinNargsAction(argparse.Action):\n\t  def __call__(self, parser, namespace, values, option_string=None):\n\t    setattr(namespace, self.dest, ' '.join(values))\n\tdef parse_photo(message):\n\t  if message.photo:\n\t    return message.photo\n\t  if message.document and message.document.mime_type.startswith('image'):\n", "    return [message.document]\n\t  if message.reply_to_message:\n\t    if message.reply_to_message.photo:\n\t      return message.reply_to_message.photo\n\t    if message.reply_to_message.document and message.reply_to_message.document.mime_type.startswith('image'):\n\t      return [message.reply_to_message.document]\n\t  return None\n\tdef log_exceptions(logger):\n\t  def decorator(func):\n\t    @functools.wraps(func)\n", "    async def wrapper(*args, **kwargs):\n\t      try:\n\t        result = await func(*args, **kwargs)\n\t        return result\n\t      except Exception as e:\n\t        logger.error(f\"Error in {func.__name__}: {str(e)}\")\n\t    return wrapper\n\t  return decorator\n\tdef cprint(*args, color='default'):\n\t  keys = ['default', 'red', 'green', 'yellow', 'blue']\n", "  sys.stdout.write(f'\\x1b[1;3{keys.index(color)}m' + ' '.join(args) + '\\x1b[0m\\n')\n\tdef update_env(path, key, value):\n\t  with open(path, \"r\") as file:\n\t    lines = file.readlines()\n\t  with open(path, \"w+\") as file:\n\t    to_write = []\n\t    multiline = False\n\t    try:\n\t      for line in lines:\n\t        if line.startswith(key + \"=\"):\n", "          multiline = line.startswith(key + \"='\")\n\t          if not multiline:\n\t            to_write.append(f\"{key}={value}\\n\")\n\t          else:\n\t            to_write.append(f\"{key}='{json.dumps(json.loads(value), indent=2, sort_keys=True)}'\\n\")\n\t          continue\n\t        if multiline:\n\t          if line.endswith(\"'\") or line.endswith(\"'\\n\") or line.endswith(\"'\\r\\n\"):\n\t            multiline = False\n\t          continue\n", "        to_write.append(line)\n\t      file.writelines(to_write)\n\t    except Exception as e:\n\t      file.writelines(lines)\n\t      cprint(\"Unable to update .env file: \" + str(e), color='red')\n\tasync def download_audio(bot, file_id, dl_path):\n\t  file_path = (await bot.get_file(file_id=file_id)).file_path\n\t  await bot.download_file(file_path, dl_path)"]}
{"filename": "middleware.py", "chunked_list": ["from aiogram.dispatcher.flags import get_flag\n\tfrom aiogram.utils.chat_action import ChatActionSender\n\tfrom aiogram import BaseMiddleware\n\tfrom config_reader import config\n\tfrom custom_queue import CallCooldown\n\tfrom collections import defaultdict\n\timport asyncio\n\timport logging\n\tlogger = logging.getLogger(__name__)\n\tclass ChatActionMiddleware(BaseMiddleware):\n", "  async def __call__(self, handler, event, data):\n\t    long_operation_type = get_flag(data, \"long_operation\")\n\t    if not long_operation_type:\n\t      return await handler(event, data)\n\t    async with ChatActionSender(action=long_operation_type, chat_id=event.chat.id):\n\t      return await handler(event, data)\n\tclass AccessMiddleware(BaseMiddleware):\n\t  async def __call__(self, handler, event, data):\n\t    uid = event.from_user.id\n\t    cid = event.chat.id\n", "    logger.info(f'message in chat {cid} ({event.chat.title or \"private\"}) from {uid} (@{event.from_user.username or event.from_user.first_name})')\n\t    if config.ignore_mode == 'whitelist' or config.ignore_mode == 'both':\n\t      if cid not in config.whitelist:\n\t        return\n\t    if config.ignore_mode == 'blacklist' or config.ignore_mode == 'both':\n\t      if uid in config.blacklist or cid in config.blacklist:\n\t        return\n\t    if get_flag(data, \"admins_only\"):\n\t      if uid not in config.adminlist:\n\t        return\n", "    return await handler(event, data)\n\tclass CooldownMiddleware(BaseMiddleware):\n\t  async def __call__(self, handler, event, data):\n\t    cooldown_seconds = get_flag(data, \"cooldown\")\n\t    if cooldown_seconds:\n\t      function_name = data['handler'].callback.__name__\n\t      if CallCooldown.check_call(event.from_user.id, function_name, cooldown_seconds):\n\t        return await handler(event, data)\n\t      else:\n\t        return\n", "    else:\n\t      return await handler(event, data)\n\tclass MediaGroupMiddleware(BaseMiddleware):\n\t    albums = defaultdict(lambda: [])\n\t    def __init__(self, delay =  1):\n\t      self.delay = delay\n\t    async def __call__(self, handler, event, data):\n\t      if not event.media_group_id:\n\t        return await handler(event, data)\n\t      try:\n", "        self.albums[event.media_group_id].append(event)\n\t        await asyncio.sleep(self.delay)\n\t        data[\"album\"] = self.albums.pop(event.media_group_id)\n\t      except Exception as e:\n\t        logger.error(e)\n\t      return await handler(event, data)"]}
{"filename": "providers/stt_provider.py", "chunked_list": ["from config_reader import config\n\tfrom .stt import backends\n\tactive_model = backends[config.stt_backend]() if 'stt' in config.active_modules else None"]}
{"filename": "providers/sd_provider.py", "chunked_list": ["import base64\n\timport httpx \n\timport json\n\timport random\n\timport asyncio\n\timport logging\n\timport subprocess\n\timport psutil\n\tfrom collections import defaultdict\n\tfrom config_reader import config\n", "from misc.memory_manager import mload\n\tfrom functools import partial\n\tlogger = logging.getLogger(__name__)\n\trequest_payload = {\n\t  \"denoising_strength\": 1,\n\t  \"prompt\": \"\",\n\t  \"sampler_name\": config.sd_default_sampler,\n\t  \"steps\": config.sd_default_tti_steps,\n\t  \"cfg_scale\": 5,\n\t  \"width\": config.sd_default_width,\n", "  \"height\": config.sd_default_height,\n\t  \"restore_faces\": False,\n\t  \"tiling\": False,\n\t  \"batch_size\": 1,\n\t  \"n_iter\": config.sd_default_n_iter,\n\t  \"negative_prompt\": \"\",\n\t  \"eta\": 71337\n\t}\n\t# hash: name\n\tmodels = defaultdict(lambda: 'Unknown model')\n", "embeddings = []\n\tloras = []\n\tsd_url = config.sd_host or \"http://127.0.0.1:7860\"\n\tsd_service = False\n\tsd_started = False\n\tdef run_sd_service():\n\t  global sd_started\n\t  if not config.sd_launch_process_automatically:\n\t    return\n\t  p = partial(subprocess.Popen, config.sd_launch_command.split(' '), cwd=config.sd_launch_dir, stderr=subprocess.DEVNULL)\n", "  service = mload('sd-remote', \n\t    p, \n\t    lambda p: p.terminate() or (sd_started:=False),\n\t    lambda p: psutil.Process(p.pid).memory_info().rss, \n\t    gpu=True\n\t  )\n\t  return service\n\tdef check_server(async_func):\n\t  async def decorated_function(*args, **kwargs):\n\t    global sd_service, sd_started\n", "    if not config.sd_launch_process_automatically:\n\t      return await async_func(*args, **kwargs)\n\t    url = f\"{sd_url}/sdapi/v1/sd-models\"\n\t    try:\n\t      async with httpx.AsyncClient() as client:\n\t        await client.get(url)\n\t    except (httpx.HTTPError, httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError):\n\t      print(\"SD server is down. Restarting it...\")\n\t      if not sd_started or sd_service.poll() is not None:\n\t        sd_service = run_sd_service()\n", "        sd_started = True\n\t        # better idea is to read stdout and wait for server, but it doesn't work for some reason\n\t        await asyncio.sleep(config.sd_launch_waittime)\n\t      else:\n\t        # TODO: fix this mess sometime later\n\t        sd_service = run_sd_service()\n\t      return await async_func(*args, **kwargs)\n\t    sd_service = run_sd_service()\n\t    return await async_func(*args, **kwargs)\n\t  return decorated_function\n", "@check_server\n\tasync def refresh_model_list():\n\t  global models, embeddings, loras\n\t  try:\n\t    async with httpx.AsyncClient() as client:\n\t      model_response = await client.get(url=f'{sd_url}/sdapi/v1/sd-models',headers={'accept': 'application/json'}, timeout=None)\n\t      embed_response = await client.get(url=f'{sd_url}/sdapi/v1/embeddings',headers={'accept': 'application/json'}, timeout=None)\n\t      lora_response  = await client.get(url=f'{sd_url}/sdapi/v1/loras',headers={'accept': 'application/json'}, timeout=None)\n\t      if model_response.status_code == 200 and embed_response.status_code == 200 and lora_response.status_code == 200: \n\t        model_response_data = model_response.json()\n", "        embed_response_data = embed_response.json()\n\t        lora_response_data  = lora_response.json()\n\t        models.clear()\n\t        embeddings.clear()\n\t        loras.clear()\n\t        for m in model_response_data:\n\t          models[m['hash']] = m['model_name']\n\t        for e in embed_response_data['loaded']:\n\t          embeddings.append(e)\n\t        for lora in lora_response_data:\n", "          loras.append(lora['name'])\n\t        loras[:] = [key for key in loras if key not in config.sd_lora_custom_activations]\n\t      else:\n\t        raise Exception('Server error')\n\t  except Exception as e:\n\t    logger.warn('Failed to load stable diffusion model names: ' + str(e))\n\tdef b642img(base64_image):\n\t  return base64.b64decode(base64_image)\n\t@check_server\n\tasync def switch_model(name):\n", "  async with httpx.AsyncClient() as client:\n\t    try:\n\t      payload = {'sd_model_checkpoint': name}\n\t      response = await client.post(url=f'{sd_url}/sdapi/v1/options', json=payload, timeout=None)\n\t      if response.status_code == 200:\n\t        return True\n\t    except Exception:\n\t      return False\n\t  return False\n\t@check_server\n", "async def sd_get_images(payload, endpoint):\n\t  if len(models.values()) == 0:\n\t    await refresh_model_list()\n\t  async with httpx.AsyncClient() as client:\n\t    try:\n\t      response = await client.post(url=f'{sd_url}/{endpoint}', json=payload, timeout=None)\n\t      if response.status_code == 200:\n\t        response_data = response.json()\n\t        images = response_data.get(\"images\")\n\t        bstr_images = [b642img(i) for i in images]\n", "        gen_info = json.loads(response_data.get('info'))\n\t        gen_info['model'] = models[gen_info['sd_model_hash']]\n\t        return (False, bstr_images, gen_info)\n\t      else:\n\t        return ('Connection error', None, None)\n\t    except (httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError, json.decoder.JSONDecodeError) as error:\n\t      return (error, None, None)\n\t    except Exception:\n\t      return ('unknown error', None, None)\n\tasync def tti(override=None):\n", "  payload = request_payload\n\t  default_scale = config.sd_default_tti_cfg_scale\n\t  payload['cfg_scale'] = random.choice([3,4,5,6]) if default_scale == 0 else default_scale\n\t  if override:\n\t    payload = {**payload, **override}\n\t  return await sd_get_images(payload, 'sdapi/v1/txt2img')\n\tasync def iti(override=None):\n\t  payload = request_payload\n\t  payload['denoising_strength'] = config.sd_default_iti_denoising_strength\n\t  payload['cfg_scale'] = config.sd_default_iti_cfg_scale\n", "  payload['steps'] = config.sd_default_iti_steps\n\t  payload['sampler'] = config.sd_default_iti_sampler\n\t  if override:\n\t    payload = {**payload, **override}\n\t  return await sd_get_images(payload, 'sdapi/v1/img2img')\n"]}
{"filename": "providers/tta_provider.py", "chunked_list": ["from utils import cprint\n\tfrom config_reader import config\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom misc.memory_manager import mload\n\tfrom functools import partial\n\timport asyncio\n\timport tempfile\n\tAudioGen = None\n\tMusicGen = None\n\tdef tta_init():\n", "  global MusicGen, AudioGen, audio_write\n\t  try:\n\t    from audiocraft.models import MusicGen, AudioGen\n\t    from audiocraft.data.audio import audio_write\n\t    return True\n\t  except ImportError:\n\t    cprint(\"TTA (AudioCraft) module not available, please reinstall it\", color=\"red\")\n\t    return False\n\tdef get_model(path, loader, name):\n\t  loader = partial(loader.get_pretrained, path, config.tta_device)\n", "  model = mload('tta-' + name, loader, None)\n\t  return model\n\tdef generate_audio(text, audio_type=\"music\", duration=5, raw_data=False):\n\t  try:\n\t    if audio_type == \"music\":\n\t      model = get_model(config.tta_music_model, MusicGen, 'MusicGen')\n\t    else:\n\t      model = get_model(config.tta_sfx_model, AudioGen, 'AudioGen')\n\t    model.set_generation_params(duration=duration)\n\t    wav = model.generate([text])\n", "  except Exception as e:\n\t    return (str(e), None)\n\t  if not raw_data:\n\t    wav = save_audio(wav[0], model)\n\t  return False, wav\n\tasync def generate_audio_async(text, audio_type=\"music\", duration=5, raw_data=False):\n\t  with ThreadPoolExecutor():\n\t    error, output = await asyncio.to_thread(generate_audio, \n\t      text, audio_type, duration, raw_data\n\t    )\n", "  return error, output\n\tdef save_audio(wav_file, model):\n\t  tmp_path = tempfile.TemporaryDirectory().name + 'record'\n\t  audio_write(tmp_path, wav_file.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n\t  return tmp_path + '.wav'"]}
{"filename": "providers/tts_provider.py", "chunked_list": ["from utils import cprint\n\tfrom config_reader import config\n\ttry:\n\t  import torch\n\t  from TTS.utils.synthesizer import Synthesizer\n\texcept ImportError:\n\t  Synthesizer = None\n\t  if 'tts' in config.active_modules and config.tts_mode == 'local':\n\t    cprint(\"TTS module not available, please reinstall it\", color=\"red\")\n\tfrom pathlib import Path\n", "import httpx\n\timport json\n\timport tempfile\n\timport subprocess\n\timport os\n\timport time\n\tfrom config_reader import config\n\tfrom misc.memory_manager import mload\n\tfrom functools import partial\n\tsynthesizers = {}\n", "so_vits_svc_voices = dict({m['voice'].lower().replace('-',''): m for m in config.tts_so_vits_svc_voices})\n\tasync def so_vits_svc(voice, text, original_audio=False):\n\t  version = so_vits_svc_voices[voice].get('v', 4.0)\n\t  v4_0_code_path = config.tts_so_vits_svc_4_0_code_path\n\t  v4_1_code_path = config.tts_so_vits_svc_4_1_code_path\n\t  so_vits_svc_code = v4_0_code_path if version == 4.0 else v4_1_code_path\n\t  name = 'temp_tts' + str(time.time_ns())\n\t  temp_file = f'{so_vits_svc_code}/raw/{name}.aif'\n\t  temp_file_wav = temp_file.replace('.aif', '.wav')\n\t  v = so_vits_svc_voices[voice]\n", "  so_vits_model = Path(v['path']) / v['weights']\n\t  so_vits_config = Path(v['path']) / 'config.json'\n\t  so_vits_voice = v['voice']\n\t  base_voice = v['base_voice']\n\t  if not original_audio:\n\t    # generate any text-to-speech output\n\t    base_tts_provider = v.get('provider', config.tts_so_vits_svc_base_tts_provider)\n\t    if base_tts_provider == 'say_macos':\n\t      subprocess.run(\n\t        ['say','-v', base_voice,  '-o', temp_file, text],\n", "      )\n\t    elif base_tts_provider == 'built_in':\n\t      error, temp_file = await tts(base_voice, text)\n\t  else:\n\t    temp_file = original_audio\n\t  subprocess.run([\n\t    config.tts_ffmpeg_path, '-y', '-i', temp_file, temp_file_wav],\n\t    stdout=subprocess.DEVNULL,\n\t    stderr=subprocess.STDOUT\n\t  )\n", "  subprocess.run(\n\t    [config.python_command, f\"inference_main.py\", \"-m\", str(so_vits_model), \"-c\", str(so_vits_config), \n\t    \"-n\", f'{name}.wav', \"-t\", \"0\", \"-s\", so_vits_voice]\n\t    ,\n\t    cwd=so_vits_svc_code\n\t  )\n\t  os.remove(temp_file)\n\t  os.remove(temp_file_wav)\n\t  filename = f'{so_vits_svc_code}/results/{name}.wav_0key_{so_vits_voice}.flac'\n\t  if not os.path.isfile(filename):\n", "    filename = filename.replace('.flac', '_sovits_pm.flac')\n\t    if not os.path.isfile(filename):\n\t      return ('File not found', None)\n\t  return (False, filename)\n\tasync def tts(voice, text):\n\t  try:\n\t    assert os.path.exists(config.tts_ffmpeg_path)\n\t    for r in config.tts_replacements:\n\t      text = text.replace(r, config.tts_replacements[r])\n\t    if voice in so_vits_svc_voices:\n", "      return await so_vits_svc(voice, text)\n\t    loader = partial(\n\t      Synthesizer,\n\t      tts_config_path=Path(config.tts_path) / \"config.json\",\n\t      tts_checkpoint=Path(config.tts_path) / (voice + \".pth\"),\n\t      use_cuda=torch.cuda.is_available(),\n\t    )\n\t    synth = mload('tts-' + voice, loader, None, gpu=torch.cuda.is_available())\n\t    data = synth.tts(text[:4096] + '.')\n\t    return (False, save_audio(synth, data))\n", "  except Exception as e:\n\t    return (str(e), None)\n\tasync def remote_tts(voice, text):\n\t  async with httpx.AsyncClient() as client:\n\t    try:\n\t      tts_payload = {\"voice\": voice, \"text\": text, \"response\": 'file' if config.tts_mode == 'remote' else 'path'}\n\t      response = await client.post(url=config.tts_host, json=tts_payload, timeout=None)\n\t      if response.status_code == 200:\n\t        if config.tts_mode == 'remote':\n\t          path = tempfile.TemporaryDirectory().name + str(hash(text)) + '.wav'\n", "          with open(path, 'wb') as f:\n\t            f.write(response.content)\n\t          return (False, path)\n\t        else:\n\t          response_data = response.json()\n\t        error = response_data.get('error')\n\t        if error:\n\t          return (error, None)\n\t        wpath = response_data.get(\"data\")\n\t        return (False, wpath)\n", "      else:\n\t        return ('Server error', None)\n\t    except (httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError, json.decoder.JSONDecodeError) as error:\n\t      return (error, None)\n\t    except Exception as e:\n\t      return (str(e), None)\n\tdef save_audio(synth, wav_file):\n\t  tmp_path = tempfile.TemporaryDirectory().name + 'record.wav'\n\t  synth.save_wav(wav_file, tmp_path)\n\t  return tmp_path\n", "def convert_to_ogg(wav_path):\n\t  ogg_path = wav_path + '.ogg'\n\t  subprocess.run([\n\t    config.tts_ffmpeg_path, '-i', wav_path, \n\t    '-acodec', 'libopus', '-b:a', '128k', '-vbr', 'off', ogg_path, '-y'],\n\t    stdout=subprocess.DEVNULL,\n\t    stderr=subprocess.STDOUT\n\t  )\n\t  with open(ogg_path, 'rb') as f:\n\t    data = f.read()  \n", "  os.remove(wav_path)\n\t  os.remove(ogg_path)\n\t  return data"]}
{"filename": "providers/llm_provider.py", "chunked_list": ["from config_reader import config\n\tfrom .llm import external_backends, pytorch_models\n\tuse_built_in_models = config.llm_backend == 'pytorch'\n\ttarget_provider = pytorch_models if use_built_in_models else external_backends\n\ttarget_key = config.llm_python_model_type if use_built_in_models else config.llm_backend\n\tactive_model = target_provider[target_key]() if 'llm' in config.active_modules else None\n"]}
{"filename": "providers/llm/mlc_chat_prebuilt_provider.py", "chunked_list": ["import sys\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom providers.llm.abstract_llm import AbstractLLM\n\timport asyncio\n\timport logging\n\tlogger = logging.getLogger(__name__)\n\tclass MLCChatPrebuilt(AbstractLLM):\n\t  assistant_mode = True\n\t  def __init__(self, model_paths, init_config={}):\n\t    sys.path.append(model_paths['path_to_mlc_chatbot_code'])\n", "    try:\n\t      from mlc_chatbot.bot import ChatBot    \n\t    except ImportError:\n\t      logging.error('MLC Chatbot is not installed')\n\t    self.model = ChatBot(model_paths['path_to_mlc_pb_home_dir'], model_paths['path_to_mlc_pb_binary_dir'])\n\t    self.model.generate = self.model.send\n\t    self.filename = 'Unknown model'\n\t  async def generate(self, raw_prompt, length=0, model_params={}, assist=True):\n\t    error = None\n\t    try:\n", "      with ThreadPoolExecutor():\n\t        print(self.model)\n\t        output = await asyncio.to_thread(self.model.generate, \n\t          raw_prompt\n\t        )\n\t      self.model.reset()\n\t    except Exception as e:\n\t      error = str(e)\n\t    return (False, output) if not error else (error, None)\n\tinit = MLCChatPrebuilt"]}
{"filename": "providers/llm/abstract_llm.py", "chunked_list": ["from abc import ABCMeta, abstractmethod\n\tfrom collections import defaultdict\n\tclass AbstractLLM(metaclass=ABCMeta):\n\t  assistant_mode = True\n\t  model = None\n\t  def __init__(self, model_paths, init_config):\n\t    return self\n\t  def tokenize(self, details):\n\t    pass\n\t  @abstractmethod\n", "  def generate(self, prompt, length, model_params, assist):\n\t    pass\n"]}
{"filename": "providers/llm/__init__.py", "chunked_list": ["import importlib\n\tgpt2_provider = lambda: importlib.import_module('providers.llm.pytorch.gpt2_provider')\n\tgptj_provider = lambda: importlib.import_module('providers.llm.pytorch.gptj_provider')\n\tauto_hf_provider = lambda: importlib.import_module('providers.llm.pytorch.auto_hf_provider')\n\tllama_orig_provider = lambda: importlib.import_module('providers.llm.pytorch.llama_orig_provider')\n\tllama_hf_provider = lambda: importlib.import_module('providers.llm.pytorch.llama_hf_provider')\n\tmlc_chat_prebuilt_provider = lambda: importlib.import_module('providers.llm.mlc_chat_prebuilt_provider')\n\tllama_cpp_provider = lambda: importlib.import_module('providers.llm.llama_cpp_provider')\n\tremote_ob_provider = lambda: importlib.import_module('providers.llm.remote_ob')\n\tremote_lcpp_provider = lambda: importlib.import_module('providers.llm.remote_llama_cpp')\n", "pytorch_models = {\n\t  'gpt2': gpt2_provider, \n\t  'gptj': gptj_provider, \n\t  'auto_hf': auto_hf_provider,\n\t  'llama_orig': llama_orig_provider,\n\t  'llama_hf': llama_hf_provider,\n\t}\n\texternal_backends = {\n\t  'llama_cpp': llama_cpp_provider,\n\t  'mlc_pb': mlc_chat_prebuilt_provider,\n", "  'remote_ob': remote_ob_provider,\n\t  'remote_lcpp': remote_lcpp_provider\n\t}"]}
{"filename": "providers/llm/remote_ob.py", "chunked_list": ["import httpx \n\timport json\n\timport logging\n\timport asyncio\n\timport subprocess\n\timport psutil\n\tfrom functools import partial\n\tfrom misc.memory_manager import mload\n\tfrom config_reader import config\n\tfrom providers.llm.abstract_llm import AbstractLLM\n", "from time import sleep\n\tlogger = logging.getLogger(__name__)\n\tllm_host = config.llm_host\n\tllm_load_started = False\n\tclass RemoteLLM(AbstractLLM):\n\t  assistant_mode = True\n\t  async def remote_llm_api(self, method, endpoint, payload):\n\t    async with httpx.AsyncClient() as client:\n\t      try:\n\t        if method == 'GET':\n", "          response = await client.get(url=f'{llm_host}/{endpoint}', params=payload, timeout=None)\n\t        else:\n\t          response = await client.post(url=f'{llm_host}/{endpoint}', json=payload, timeout=None)\n\t        if response.status_code == 200:\n\t          response_data = response.json()\n\t          return (False, response_data)\n\t        else:\n\t          return 'Connection error', None\n\t      except (httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError, json.decoder.JSONDecodeError) as error:\n\t        return str(error), None\n", "      except Exception:\n\t        return 'Unknown error', None\n\t  def __init__(self, model_paths, init_config={}):\n\t    if config.llm_remote_launch_process_automatically and \\\n\t    config.mm_preload_models_on_start:\n\t      asyncio.run(self.run_llm_service())\n\t    else:\n\t      error, data = asyncio.run(self.remote_llm_api('GET', 'api/v1/model', {}))\n\t      if error:\n\t        logger.warn('Unable to get remote language model name: ' + str(error))\n", "      self.model = None\n\t      self.filename = data.get('result') if not error else 'Unknown model'\n\t  async def generate(self, prompt, length=64, model_params={}, assist=True):\n\t    if config.llm_remote_launch_process_automatically:\n\t      await self.run_llm_service()\n\t    data = {\n\t      'prompt': prompt,\n\t      'max_length': length,\n\t      **model_params,\n\t    }\n", "    error, response = await self.remote_llm_api('POST', 'api/v1/generate', data)\n\t    if not error:\n\t      response = response.get('results')[0].get('text')\n\t      logger.info(response)\n\t      return False, prompt + response\n\t    else:\n\t      return str(error), None\n\t  async def run_llm_service(self):\n\t    global llm_load_started, last_pid\n\t    if llm_load_started:\n", "      return\n\t    llm_load_started = True\n\t    service = mload('llm-remote_ob',  \n\t      partial(subprocess.Popen, config.llm_remote_launch_command.split(' '), cwd=config.llm_remote_launch_dir),\n\t      lambda p: p.terminate(),\n\t      lambda p: psutil.Process(p.pid).memory_info().rss, \n\t      gpu=True\n\t    )\n\t    if service.pid != last_pid:\n\t      await asyncio.sleep(config.llm_remote_launch_waittime)\n", "      await self.remote_llm_api('POST', 'api/v1/model', {'action': 'load', 'model_name': config.llm_remote_model_name}),\n\t      self.model = None\n\t      self.filename = config.llm_remote_model_name\n\t    llm_load_started=False\n\t    last_pid = service.pid\n\t    return service\n\tinit = RemoteLLM\n\tlast_pid = -1 "]}
{"filename": "providers/llm/llama_cpp_provider.py", "chunked_list": ["from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n\tfrom config_reader import config\n\tfrom providers.llm.abstract_llm import AbstractLLM\n\timport asyncio\n\timport os\n\timport logging\n\tfrom misc.memory_manager import mload\n\tfrom functools import partial\n\tlogger = logging.getLogger(__name__)\n\ttry:\n", "  from llama_cpp import Llama\n\texcept ImportError:\n\t  Llama = False\n\tclass LlamaCPP(AbstractLLM):\n\t  assistant_mode = True\n\t  def __init__(self, model_paths, init_config={}):\n\t    if not Llama:\n\t      logger.error('llama.cpp is not installed, run \"pip install llama-cpp-python\" to install it')\n\t      return logger.error('for GPU support, please read https://github.com/abetlen/llama-cpp-python')\n\t    override = init_config.get('llama_cpp_init', {})\n", "    lora_path = model_paths.get('path_to_llama_cpp_lora', None)\n\t    self.load_model = partial(\n\t      Llama,\n\t      n_ctx=min(init_config.get('context_size', 512), config.llm_lcpp_max_context_size),\n\t      rope_freq_base=init_config.get('rope_freq_base', 10000),\n\t      rope_freq_scale=init_config.get('rope_freq_scale', 1.0),\n\t      n_gpu_layers=config.llm_lcpp_gpu_layers,\n\t      model_path=model_paths[\"path_to_llama_cpp_weights\"],\n\t      seed=0,\n\t      lora_path=lora_path,\n", "      **override  \n\t    )\n\t    self.filename = os.path.basename(model_paths['path_to_llama_cpp_weights'])\n\t    if config.mm_preload_models_on_start:\n\t      m = self.model\n\t  @property\n\t  def model(self):\n\t    return mload('llm-llama.cpp', self.load_model, None)\n\t  async def generate(self, prompt, length=64, model_params={}, assist=True):\n\t    if 'repetition_penalty' in model_params:\n", "      model_params['repeat_penalty'] = model_params['repetition_penalty']\n\t      del model_params['repetition_penalty']\n\t    if 'early_stopping' in model_params:\n\t      del model_params['early_stopping']\n\t    output = error = None\n\t    with ThreadPoolExecutor():\n\t      try:\n\t        output = await asyncio.to_thread(\n\t          self.model, \n\t          prompt=prompt,\n", "          stop=[\"</s>\"],\n\t          max_tokens=length,\n\t           **model_params\n\t        )\n\t      except Exception as e:\n\t        error = str(e)\n\t    if not error:\n\t      output = output['choices'][0]['text']\n\t      logger.info(output)\n\t      output = prompt + output\n", "    return (False, output) if not error else (error, None)\n\t## process-based approach re-creates a new process and re-allocates memory on every run\n\t## which is not optimal, I leave this code for future reference\n\t# import functools\n\t# async def generate_mp(prompt, length=64, model_params={}, assist=True):\n\t#   with ProcessPoolExecutor(max_workers=1) as executor:\n\t#     loop = asyncio.get_event_loop()\n\t#     binded = functools.partial(\n\t#       model, \n\t#      prompt=prompt,\n", "#       stop=[\"</s>\"],\n\t#       max_tokens=length,\n\t#        **model_params\n\t#     )\n\t#     output = loop.run_in_executor(executor, binded)\n\t#     output = await output\n\t#     return prompt + output['choices'][0]['text']\n\tinit = LlamaCPP"]}
{"filename": "providers/llm/remote_llama_cpp.py", "chunked_list": ["import logging\n\tfrom config_reader import config\n\tfrom providers.llm.remote_ob import RemoteLLM\n\tlogger = logging.getLogger(__name__)\n\tllm_host = config.llm_host\n\tassistant_mode = True\n\tclass RemoteLLamaCPP(RemoteLLM):\n\t  async def generate(self, prompt, length=64, model_params={}, assist=True):\n\t    if config.llm_remote_launch_process_automatically:\n\t      self.run_llm_service()\n", "    data = {\n\t      'prompt': prompt,\n\t      'max_length': length,\n\t      'seed': -1,\n\t      **model_params,\n\t    }\n\t    error, response = await super().remote_llm_api('POST', 'completion', data)\n\t    if not error:\n\t      logger.info(response)\n\t      return False, prompt + response.get('content')\n", "    else:\n\t      return 'Error: ' + str(error), None\n\tinit = RemoteLLamaCPP"]}
{"filename": "providers/llm/pytorch/gptj_provider.py", "chunked_list": ["import torch\n\timport asyncio\n\timport os\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom providers.llm.abstract_llm import AbstractLLM\n\ttokenizer = None\n\tmodel = None\n\tdevice = torch.device(\"cpu\")\n\tclass GPTJ(AbstractLLM):\n\t  def __init__(self, model_paths, init_config={}):\n", "    from transformers import AutoTokenizer, GPTJForCausalLM\n\t    weights = model_paths['path_to_gptj_weights']\n\t    self.tokenizer = AutoTokenizer.from_pretrained(weights)\n\t    self.model = GPTJForCausalLM.from_pretrained(weights, revision=\"float16\", torch_dtype=torch.float32, low_cpu_mem_usage=True)\n\t    self.model = self.model.to(device)\n\t    self.filename = os.path.basename(weights)\n\t  def tokenize(self, prompt):\n\t    encoded_prompt = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\t    return encoded_prompt\n\t  async def generate(self, prompt, length=64, model_params={}, assist=False):\n", "    encoded_prompt = self.tokenize(prompt)\n\t    error = None\n\t    try:\n\t      with ThreadPoolExecutor():\n\t        output = await asyncio.to_thread(self.model.generate, \n\t          input_ids=encoded_prompt,\n\t          max_length=len(encoded_prompt[0]) + length,\n\t          do_sample=True,\n\t          **model_params\n\t        )\n", "    except Exception as e:\n\t      error = str(e)\n\t    return (False, self.tokenizer.batch_decode(output)[0]) if not error else (True, error)\n\tinit = GPTJ\n"]}
{"filename": "providers/llm/pytorch/llama_hf_provider.py", "chunked_list": ["import os\n\timport torch\n\tfrom torch import mps\n\timport asyncio\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\tfrom misc.mps_fixups import fixup_mps\n\tfrom config_reader import config\n\tfrom providers.llm.abstract_llm import AbstractLLM\n\tdevice = torch.device(\"cuda\") if torch.cuda.is_available() \\\n", "  else torch.device(\"cpu\") if not torch.backends.mps.is_available() \\\n\t  else torch.device('mps')\n\tif torch.backends.mps.is_available() and config.apply_mps_fixes:\n\t  fixup_mps()\n\tclass LlamaHuggingface(AbstractLLM):\n\t  submodel = None\n\t  assistant_mode = False\n\t  def __init__(self, model_paths, init_config={}):\n\t    tokenizer = model_paths['path_to_hf_llama']\n\t    weights = model_paths['path_to_hf_llama']\n", "    self.tokenizer = LlamaTokenizer.from_pretrained(tokenizer)\n\t    self.model = LlamaForCausalLM.from_pretrained(\n\t      weights, \n\t      torch_dtype=torch.float16 if device is not torch.device('cpu') else torch.float32,\n\t      device_map={\"\": device}\n\t    )\n\t    if os.path.exists(model_paths.get('path_to_llama_lora', '')):\n\t      from peft import PeftModel\n\t      self.submodel = PeftModel.from_pretrained(\n\t        self.model,\n", "        model_paths['path_to_llama_lora'],\n\t        device_map={\"\": device},\n\t        torch_dtype=torch.float16 if device is not torch.device('cpu') else torch.float32,\n\t      )\n\t      self.submodel.half()\n\t      self.submodel.eval()\n\t      self.assistant_mode = True\n\t    self.model.config.bos_token_id = 1\n\t    self.model.config.eos_token_id = 2\n\t    tokenizer.pad_token = tokenizer.eos_token\n", "    tokenizer.pad_token_id = tokenizer.eos_token_id\n\t    self.model.half()\n\t    self.model.eval()\n\t    self.filename = os.path.basename(model_paths['path_to_hf_llama'])\n\t  def tokenize(self, prompt):\n\t    return self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\t  async def generate(self, prompt, length=64, model_params={}, use_submodel=False):\n\t    encoded_prompt = self.tokenize(prompt)\n\t    generation_config = GenerationConfig(\n\t      num_beams=1,\n", "      **model_params\n\t    )\n\t    model = self.submodel if use_submodel else self.model\n\t    error = None\n\t    try:\n\t      with ThreadPoolExecutor():\n\t        with torch.no_grad():\n\t          output = await asyncio.to_thread(model.generate, \n\t            input_ids=encoded_prompt,\n\t            max_new_tokens=length,\n", "            generation_config=generation_config,\n\t            eos_token_id=model.config.eos_token_id,\n\t            do_sample=True\n\t          )\n\t          output = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n\t      if torch.backends.mps.is_available():\n\t        mps.empty_cache()\n\t      elif torch.cuda.is_available():\n\t        torch.cuda.empty_cache()\n\t    except Exception as e:\n", "      error = str(e)\n\t    return (False, output[0]) if not error else (True, error)\n\tinit = LlamaHuggingface"]}
{"filename": "providers/llm/pytorch/auto_hf_provider.py", "chunked_list": ["import torch\n\timport asyncio\n\timport os\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom providers.llm.abstract_llm import AbstractLLM\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tclass AutoHF(AbstractLLM):\n\t  def __init__(self, model_paths, init_config={}):\n\t    from transformers import AutoTokenizer, AutoModelForCausalLM\n\t    weights = model_paths['path_to_autohf_weights']\n", "    self.tokenizer = AutoTokenizer.from_pretrained(weights)\n\t    self.model = AutoModelForCausalLM.from_pretrained(weights)\n\t    self.filename = os.path.basename(weights)\n\t  def tokenize(self, prompt):\n\t    encoded_prompt = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\t    return encoded_prompt\n\t  async def generate(self, prompt, length=64, model_params={}, assist=False):\n\t    error = None\n\t    try:\n\t      encoded_prompt = self.tokenize(prompt)\n", "      if 'early_stopping' in model_params:\n\t        del model_params['early_stopping']\n\t      with ThreadPoolExecutor():\n\t        output = await asyncio.to_thread(self.model.generate, \n\t          input_ids=encoded_prompt,\n\t          no_repeat_ngram_size=2,\n\t          max_new_tokens=length,\n\t          early_stopping=True,\n\t          do_sample=True,\n\t          **model_params\n", "        )\n\t    except Exception as e:\n\t      error = str(e)\n\t    return (False, self.tokenizer.batch_decode(output, skip_special_tokens=True)[0]) if not error else (True, error)\n\tinit = AutoHF\n"]}
{"filename": "providers/llm/pytorch/llama_orig_provider.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom utils import b64_to_img\n\tfrom providers.llm.abstract_llm import AbstractLLM\n\timport asyncio\n\timport inspect\n\t#python3.10 -m torch.distributed.launch --use_env bot.py\n\tclass LlamaOrig(AbstractLLM):\n", "  generator = None\n\t  assistant_mode = False\n\t  visual_mode = False\n\t  def __init__(self, model_paths, init_config={}):\n\t    llama_weights = model_paths['path_to_llama_weights']\n\t    llama_tokenizer = model_paths['path_to_llama_tokenizer']\n\t    sys.path.append(model_paths['path_to_llama_code'])\n\t    self.filename = os.path.basename(llama_weights)\n\t    if os.path.exists(model_paths.get('path_to_llama_multimodal_adapter', '')):\n\t      self._load_multimodal_adapter(model_paths, llama_weights, llama_tokenizer)\n", "    else:\n\t      self._load_llama_model(model_paths, llama_weights, llama_tokenizer)\n\t  def _load_llama_model(self, model_paths, llama_weights, llama_tokenizer):\n\t    from example import setup_model_parallel, load\n\t    with torch.inference_mode(mode=True):\n\t      local_rank, world_size = setup_model_parallel()\n\t      if 'adapter_path' in inspect.signature(load).parameters and \\\n\t         'path_to_llama_adapter' in model_paths and \\\n\t          os.path.exists(model_paths.get('path_to_llama_adapter', None)): \n\t        self.model = load(\n", "          llama_weights, llama_tokenizer, model_paths['path_to_llama_adapter'], local_rank, world_size, 1024, 1\n\t        )\n\t        self.assistant_mode = True\n\t      else:\n\t        self.model = load(\n\t          llama_weights, llama_tokenizer, local_rank, world_size, 1024, 1\n\t        )\n\t  def _load_multimodal_adapter(self, model_paths, llama_weights, llama_tokenizer):\n\t    global generator, assistant_mode, visual_mode\n\t    import llama\n", "    device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available else 'cpu')\n\t    lpath = os.path.dirname(llama_tokenizer)\n\t    orig_generator, preprocess = llama.load(model_paths['path_to_llama_multimodal_adapter'], lpath, device)\n\t    self.assistant_mode = True\n\t    self.visual_mode = True\n\t    class Wrapped_generator():\n\t      def generate(self, prompt, use_adapter=True, visual_input=False, **kwargs):\n\t        if visual_input:\n\t          img = b64_to_img(visual_input)\n\t          img = preprocess(img).unsqueeze(0).half().to(device)\n", "        else:\n\t          img = []\n\t        generated = orig_generator.generate(img, prompt, **kwargs)\n\t        return [prompt[0] + generated[0]]\n\t    self.model = Wrapped_generator()\n\t  async def generate(self, prompt, max_gen_len=64, params={}, assist=False):\n\t    available_params = inspect.signature(self.model.generate).parameters\n\t    for param in list(params):\n\t      if param not in available_params:\n\t        del params[param]\n", "    error = None\n\t    with ThreadPoolExecutor():\n\t      if self.assistant_mode and 'use_adapter' in available_params:\n\t        params['use_adapter'] = assist\n\t      try:\n\t        results = await asyncio.to_thread(self.model.generate,\n\t          [prompt], max_gen_len=max_gen_len, **params\n\t        )\n\t      except Exception as e:\n\t        error = str(e)\n", "    return (False, results[0]) if not error else (True, error)\n\tinit = LlamaOrig"]}
{"filename": "providers/llm/pytorch/gpt2_provider.py", "chunked_list": ["import torch\n\timport os\n\timport asyncio\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom types import SimpleNamespace\n\tfrom providers.llm.abstract_llm import AbstractLLM\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tclass GPT2(AbstractLLM):\n\t  is_nanoGPT = False\n\t  assistant_mode = False\n", "  def __init__(self, model_paths, init_config={}):\n\t    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\t    weights = model_paths['path_to_gpt2_weights']\n\t    self.filename = os.path.basename(weights)\n\t    if 'use_tiktoken' in init_config and init_config['use_tiktoken']:\n\t      import tiktoken\n\t      tk = tiktoken.get_encoding(\"gpt2\")\n\t      tokenizer = {}\n\t      tokenizer[\"encode\"] = lambda s, *args, **kwargs: torch.tensor(tk.encode(s, allowed_special={\"<|endoftext|>\"}), dtype=torch.long, device=device)[None, ...]\n\t      tokenizer[\"decode\"] = lambda l: tk.decode(l.tolist())\n", "      tokenizer[\"name\"] = 'tiktoken'\n\t      tokenizer = SimpleNamespace(**tokenizer)\n\t      self.tokenizer = tokenizer\n\t    else:\n\t      self.tokenizer = GPT2Tokenizer.from_pretrained(weights)\n\t    if 'nanogpt' in init_config and init_config['nanogpt']:\n\t      import sys\n\t      sys.path.append(model_paths['path_to_minchatgpt_code'])\n\t      from gpt import GPT\n\t      from configs import get_configs\n", "      self.is_nanoGPT = True\n\t      cfg = get_configs(\"gpt2-medium\")\n\t      model = GPT(cfg)\n\t      model.load_state_dict(state_dict=torch.load(weights), strict=False)\n\t      self.assistant_mode = True\n\t    else:\n\t      model = GPT2LMHeadModel.from_pretrained(weights)\n\t    self.model = model.to(device)\n\t  def tokenize(self, prompt):\n\t    encoded_prompt = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n", "    encoded_prompt = encoded_prompt.to(device)\n\t    return encoded_prompt[:, :1024]\n\t  async def generate(self, prompt, length=64, model_params={}, assist=False):\n\t    error = None\n\t    try:\n\t      encoded_prompt = self.tokenize(prompt)\n\t      with ThreadPoolExecutor():\n\t        if not self.is_nanoGPT:\n\t          output_sequences = await asyncio.to_thread(self.model.generate, \n\t            input_ids=encoded_prompt,\n", "            max_length=length + len(encoded_prompt[0]),\n\t            do_sample=True,\n\t            num_return_sequences=1,\n\t            **model_params\n\t          )\n\t        else:\n\t          if 'early_stopping' in model_params:\n\t            del model_params['early_stopping']\n\t          output_sequences = await asyncio.to_thread(self.model.generate, \n\t            encoded_prompt,\n", "            length,\n\t            **model_params\n\t          )\n\t    except Exception as e:\n\t      error = str(e)\n\t    return (False, self.tokenizer.decode(output_sequences[0])) if not error else (True, error)\n\tinit = GPT2"]}
{"filename": "providers/stt/whisper.py", "chunked_list": ["from whispercpp import Whisper\n\tfrom providers.stt.abstract_stt import AbstractSTT\n\tfrom config_reader import config\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom misc.memory_manager import mload\n\tfrom functools import partial\n\timport asyncio\n\tclass WhisperCPP(AbstractSTT):\n\t  def __init__(self):\n\t    if config.mm_preload_models_on_start:\n", "      m = self.model\n\t  @property\n\t  def model(self):\n\t    loader = partial(Whisper, config.stt_model_path_or_name)\n\t    return mload('stt-whisper', loader, None)\n\t  async def recognize(self, audio_path):\n\t    try:\n\t      with ThreadPoolExecutor():\n\t        output = await asyncio.to_thread(self.model.transcribe, audio_path)\n\t      text = ''.join(self.model.extract_text(output))\n", "      return False, text\n\t    except Exception as e:\n\t      return str(e), None \n\tinit = WhisperCPP"]}
{"filename": "providers/stt/__init__.py", "chunked_list": ["import importlib\n\twhispercpp = lambda: importlib.import_module('providers.stt.whisper')\n\tsilero = lambda: importlib.import_module('providers.stt.silero')\n\twav2vec2 = lambda: importlib.import_module('providers.stt.wav2vec2')\n\tbackends = {\n\t  'whisper': whispercpp,\n\t  'silero': silero,\n\t  'wav2vec2': wav2vec2\n\t}"]}
{"filename": "providers/stt/silero.py", "chunked_list": ["import torch\n\timport torchaudio\n\tfrom providers.stt.abstract_stt import AbstractSTT\n\tfrom config_reader import config\n\tfrom concurrent.futures import ThreadPoolExecutor\n\timport asyncio\n\tfrom glob import glob\n\tfrom misc.memory_manager import mload\n\tdevice = torch.device('cpu')\n\tclass SileroSTT(AbstractSTT):\n", "  def __init__(self):\n\t    if config.mm_preload_models_on_start:\n\t      mload('st-wav2vec2', self.load, None)\n\t  def load(self):\n\t    model, decoder, utils = torch.hub.load(\n\t      repo_or_dir='snakers4/silero-models',\n\t      model='silero_stt',\n\t      language=config.stt_model_path_or_name if len(config.stt_model_path_or_name) == 2 else 'en',\n\t      device=device\n\t    )\n", "    return model, utils, decoder\n\t  def stt(self, path):\n\t    model, utils, decoder = mload('st-silero', self.load, None)\n\t    read_batch, split_into_batches, read_audio, prepare_model_input = utils\n\t    test_files = glob(path)\n\t    batches = split_into_batches(test_files, batch_size=10)\n\t    input = prepare_model_input(read_batch(batches[0]), device=device)\n\t    output = model(input)\n\t    transcript = '. '.join([decoder(x.cpu()) for x in output])\n\t    print(transcript)\n", "    return transcript\n\t  async def recognize(self, audio_path):\n\t    try:\n\t      with ThreadPoolExecutor():\n\t        text = await asyncio.to_thread(self.stt, audio_path)\n\t      return False, text\n\t    except Exception as e:\n\t      return str(e), None \n\tinit = SileroSTT"]}
{"filename": "providers/stt/wav2vec2.py", "chunked_list": ["import torch\n\timport torchaudio\n\tfrom transformers import SpeechEncoderDecoderModel, Wav2Vec2Processor\n\tfrom providers.stt.abstract_stt import AbstractSTT\n\tfrom config_reader import config\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom misc.memory_manager import mload\n\timport asyncio\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tclass Wav2Vec2(AbstractSTT): \n", "  def __init__(self):\n\t    if config.mm_preload_models_on_start:\n\t      mload('st-wav2vec2', self.load, None)\n\t  def load(self):\n\t    processor = Wav2Vec2Processor.from_pretrained(config.stt_model_path_or_name)\n\t    model = SpeechEncoderDecoderModel.from_pretrained(config.stt_model_path_or_name)\n\t    if device != 'cpu':\n\t      model = model.to(device)\n\t      processor = processor\n\t    return (model, processor)\n", "  def stt(self, path):\n\t    model, processor = mload('st-wav2vec2', self.load, None)\n\t    waveform, sample_rate = torchaudio.load(path, normalize=True)\n\t    if waveform.shape[0] == 2:\n\t        waveform = torch.mean(waveform, dim=0)\n\t    if sample_rate != 16000:\n\t        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n\t        waveform = resampler(waveform)\n\t    waveform = waveform.squeeze()\n\t    processed = processor(waveform, sampling_rate=16_000,\n", "                      return_tensors=\"pt\", padding='longest', device=device)\n\t    if device != 'cpu':\n\t      processed['input_values'] = processed['input_values'].to(device)\n\t      processed['attention_mask'] = processed['attention_mask'].to(device)\n\t    print(processed)\n\t    with torch.no_grad():\n\t      predicted_ids = model.generate(**processed)\n\t    predicted_sentences = processor.batch_decode(\n\t        predicted_ids,\n\t        num_processes=8,\n", "        skip_special_tokens=True\n\t    )\n\t    print(predicted_sentences)\n\t    return ' '.join(predicted_sentences)\n\t  async def recognize(self, audio_path):\n\t    try:\n\t      with ThreadPoolExecutor():\n\t        text = await asyncio.to_thread(self.stt, audio_path)\n\t      return False, text\n\t    except AssertionError as e:\n", "      print(e)\n\t      return str(e), None \n\tinit = Wav2Vec2"]}
{"filename": "providers/stt/abstract_stt.py", "chunked_list": ["from abc import ABCMeta, abstractmethod\n\tclass AbstractSTT(metaclass=ABCMeta):\n\t  model = None\n\t  def __init__(self):\n\t    return None\n\t  @abstractmethod\n\t  def recognize(self, audio_path):\n\t    pass"]}
{"filename": "modules/tts.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\n\tfrom aiogram.types import Message, BufferedInputFile\n\tfrom providers.tts_provider import tts, remote_tts, convert_to_ogg, so_vits_svc\n\tfrom custom_queue import UserLimitedQueue, semaphore_wrapper\n\tfrom config_reader import config\n\tfrom utils import download_audio\n\timport asyncio\n\timport tempfile\n\tclass TextToSpeechModule:\n\t  def __init__(self, dp, bot):\n", "    self.queue = UserLimitedQueue(config.tts_queue_size_per_user)\n\t    self.semaphore = asyncio.Semaphore(1)\n\t    so_vits_svc_voices = list(v['voice'].lower().replace('-','') for v in config.tts_so_vits_svc_voices)\n\t    all_voices = [*config.tts_voices, *so_vits_svc_voices]\n\t    @dp.message(Command(commands=[\"tts\", *config.tts_voices, *so_vits_svc_voices]), flags={\"long_operation\": \"record_audio\"})\n\t    async def command_tts_handler(message: Message, command: CommandObject) -> None:\n\t      with self.queue.for_user(message.from_user.id) as available:\n\t        if available:\n\t          #show helper message if no voice is selected\n\t          if command.command == \"tts\" or not command.args or str(command.args).strip() == \"\" or ('-help' in str(command.args)):\n", "            return await message.answer(f\"usage: {' '.join(['/' + x for x in all_voices])} text, /revoice [recording]\\nUse the commands like /command@botname \\n{config.tts_credits}\")\n\t          voice = command.command\n\t          text = str(command.args)\n\t          task_function = remote_tts if config.tts_mode != 'local' else tts\n\t          wrapped_runner = semaphore_wrapper(self.semaphore, task_function)\n\t          error, data = await wrapped_runner(voice, text)\n\t          if error:\n\t            return await message.answer(f\"Error, <b>{error}</b>\")\n\t          else:\n\t            audio = BufferedInputFile(convert_to_ogg(data), 'tts.ogg')\n", "            return await message.answer_voice(voice=audio)\n\t    if config.tts_enable_so_vits_svc:\n\t      @dp.message(Command(commands=[\"revoice\", \"sts\"]), flags={\"long_operation\": \"record_audio\"})\n\t      async def revoice(message: Message, command: CommandObject) -> None:\n\t        voice = str(command.args).split(' ')[0] if command.args else so_vits_svc_voices[0]\n\t        voice = voice if voice in so_vits_svc_voices else None\n\t        if not voice:\n\t          return await message.answer(\"<b>Voice not found</b>, available speech-to-speech voices: \" +\n\t           \", \".join(so_vits_svc_voices))\n\t        if message.reply_to_message:\n", "          if message.reply_to_message.voice:\n\t            with tempfile.NamedTemporaryFile(suffix='.ogg', delete=False) as temp_file:\n\t              await download_audio(bot, message.reply_to_message.voice.file_id, temp_file.name)\n\t              wrapped_runner = semaphore_wrapper(self.semaphore, so_vits_svc)\n\t              error, data = await wrapped_runner(voice, None, temp_file.name)\n\t              if error:\n\t                return await message.answer(f\"Error, <b>{error}</b>\")\n\t              else:\n\t                audio = BufferedInputFile(convert_to_ogg(data), 'tts.ogg')\n\t                return await message.answer_voice(voice=audio)\n", "        return await message.answer(\"No audio found. Use this command replying to voice messages\")\n\t    bot.reply_tts = command_tts_handler\n"]}
{"filename": "modules/llm.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\n\tfrom aiogram.types import Message, BufferedInputFile, InputMediaPhoto, URLInputFile\n\tfrom custom_queue import UserLimitedQueue, semaphore_wrapper\n\tfrom config_reader import config\n\tfrom aiogram import html, F\n\tfrom providers.llm_provider import active_model\n\tfrom chroniclers.base import chroniclers\n\tfrom types import SimpleNamespace\n\tfrom utils import parse_photo, tg_image_to_data\n\timport asyncio\n", "import json\n\tdef get_chat_id(message):\n\t  return message.from_user.id if config.llm_history_grouping == 'user' else message.chat.id\n\tdef assistant_model_available(model):\n\t  return (hasattr(model, 'assistant_mode') and model.assistant_mode) or \\\n\t    config.llm_force_assistant_for_unsupported_models\n\tdef visual_mode_available(model):\n\t  return (hasattr(model, 'visual_mode') and model.assistant_mode)\n\tclass LargeLanguageModel:\n\t  async def assist(self, text, context):\n", "    params = {**self.assistant.gen_cfg(self.assistant_cfg_override), **context.get('img_input',{})}\n\t    return await self.complete_with_chronicler(text, self.assistant, context, params, config.llm_max_assistant_tokens)\n\t  async def chat(self, text, context):\n\t    params = self.chatter.gen_cfg(self.chat_cfg_override)\n\t    return await self.complete_with_chronicler(text, self.chatter, context, params, config.llm_max_tokens)\n\t  async def reply(self, text, context, message):\n\t    params = self.assistant.gen_cfg(self.assistant_cfg_override)\n\t    context['reply_text'] = message.reply_to_message.text\n\t    return await self.complete_with_chronicler(text, self.replier, context, params, config.llm_max_assistant_tokens)\n\t  async def complete_with_chronicler(self, text, chronicler, context, params, length):\n", "    prompt = chronicler.prepare({\n\t      \"message\": text, \n\t      \"model\": self.model,\n\t      **context\n\t    })\n\t    wrapped_runner = semaphore_wrapper(self.semaphore, self.model.generate)\n\t    error, result = await wrapped_runner(prompt, config.llm_max_assistant_tokens, params)\n\t    return chronicler.parse(result, context.get('chat_id', 0), len(prompt)) if not error else str(error)\n\t  async def complete_raw(self, text, context):\n\t    wrapped_runner = semaphore_wrapper(self.semaphore, self.model.generate)\n", "    error, result = await wrapped_runner(text, config.llm_max_assistant_tokens, self.assistant.gen_cfg({}))\n\t    return result\n\t  def should_use_reply_chronicler(self, message, bot):\n\t    reply = message.reply_to_message\n\t    is_reply_from_bot = reply and reply.from_user.id == bot._me.id\n\t    is_qa_format = reply and reply.text and reply.text.startswith('Q:')\n\t    always_assist = config.llm_assistant_use_in_chat_mode and assistant_model_available(self.model)\n\t    return is_reply_from_bot and (is_qa_format or always_assist)\n\t  def get_common_chat_attributes(self, message, additional={}):\n\t    return {\n", "      'author': message.from_user.first_name.replace(' ', '') or 'User',\n\t      'chat_id': get_chat_id(message),\n\t      'user_id': message.from_user.id,\n\t      **additional\n\t    }\n\t  def __init__(self, dp, bot):\n\t    self.queue = UserLimitedQueue(config.llm_queue_size_per_user)\n\t    self.semaphore = asyncio.Semaphore(1)\n\t    self.chatter = chroniclers['chat'](config.llm_character, False, config.llm_max_history_items)\n\t    self.assistant = chroniclers[config.llm_assistant_chronicler](config.llm_character)\n", "    self.replier = chroniclers['reply'](config.llm_character)\n\t    model = active_model.init(config.llm_paths, self.chatter.init_cfg())\n\t    self.model = model\n\t    self.chat_cfg_override = config.llm_generation_cfg_override\n\t    self.assistant_cfg_override = config.llm_assistant_cfg_override\n\t    @dp.message((F.text[0] if F.text else '') != '/', flags={\"long_operation\": \"typing\"})\n\t    async def handle_messages(message: Message):\n\t      parse_reply = self.should_use_reply_chronicler(message, bot)\n\t      # if should be handled in assistant mode\n\t      if (config.llm_assistant_use_in_chat_mode and assistant_model_available(model))\\\n", "      or (visual_mode_available(model) and parse_photo(message)) or parse_reply:\n\t        command = SimpleNamespace(args=message.text, parse_reply=parse_reply)\n\t        return await assist_message(message=message, command=command)\n\t      with self.queue.for_user(message.from_user.id) as available:\n\t        if available:\n\t          output = await self.chat(message.text, self.get_common_chat_attributes(message))\n\t          await message.reply(text=html.quote(output))\n\t    @dp.message(Command(commands=['reset', 'clear']), flags={\"cooldown\": 20})\n\t    async def clear_llm_history(message: Message, command: Command):\n\t      self.chatter.history[get_chat_id(message)] = []\n", "    @dp.message(Command(commands=['ask', 'assist']), flags={\"long_operation\": \"typing\"})\n\t    async def assist_message(message: Message, command: Command):\n\t      msg = str(command.args).strip()\n\t      if not (msg and command.args):\n\t        return\n\t      if not assistant_model_available(model):\n\t        return await message.reply(text='Assistant model is not available')\n\t      img_input = {\"visual_input\": await tg_image_to_data(parse_photo(message), bot)}\\\n\t                  if visual_mode_available(model) \\\n\t                  else {}\n", "      with self.queue.for_user(message.from_user.id) as available:\n\t        if not available:\n\t          return\n\t        if hasattr(command, 'parse_reply') and not img_input:\n\t          output = await self.reply(msg,\n\t            self.get_common_chat_attributes(message),\n\t            message\n\t          )\n\t        else:\n\t          output = await self.assist(msg, \n", "            self.get_common_chat_attributes(message, {'img_imput': img_input})\n\t          )\n\t      if config.llm_assistant_use_in_chat_mode and not hasattr(command, 'command'):\n\t        reply = html.quote(output)\n\t      else:\n\t        reply = f'<b>Q</b>: {msg}\\n\\n<b>A</b>: {html.quote(output)}'\n\t      await message.reply(text=(reply), allow_sending_without_reply=True)\n\t    @dp.message(Command(commands=['llm']), flags={\"cooldown\": 20})\n\t    async def helpfunction(message: Message, command: Command):\n\t      text = f'''[LLM module].\n", "      Commands: /ask\n\t      Active model type: {config.llm_backend}\n\t      Assistant mode: {str(assistant_model_available(model))} ({config.llm_assistant_chronicler})\n\t      Character: {config.llm_character}\n\t      Context visibility: {config.llm_history_grouping}\n\t      Model: {model.filename if model.model else 'unknown'}\n\t      Config: \n\t{json.dumps(self.chatter.gen_cfg(self.assistant_cfg_override), sort_keys=True, indent=4)}'''\n\t      return await message.reply(text=text)"]}
{"filename": "modules/admin.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\n\tfrom aiogram.types import Message\n\tfrom utils import parse_photo, log_exceptions\n\timport logging\n\tlogger = logging.getLogger(__name__)\n\tclass AdminModule:\n\t  def __init__(self, dp, bot):\n\t    @dp.message(Command(commands=[\"sendpic\"]), flags={\"admins_only\": True})\n\t    @log_exceptions(logger)\n\t    async def send_pic(message: Message, command: CommandObject) -> None:\n", "      photo = parse_photo(message)\n\t      await bot.send_photo(chat_id = int(str(command.args)), photo = photo[-1].file_id)\n\t    @dp.message(Command(commands=[\"delete\"]), flags={\"admins_only\": True})\n\t    @log_exceptions(logger)\n\t    async def delete_msg(message: Message, command: CommandObject) -> None:\n\t      await bot.delete_message(chat_id = message.chat.id, message_id = message.reply_to_message.message_id)\n\t    @dp.message(Command(commands=[\"info\"]), flags={\"admins_only\": True})\n\t    async def chat_info(message: Message, command: CommandObject) -> None:\n\t      msg = message if not message.reply_to_message else message.reply_to_message\n\t      prefix = '[reply info]\\n' if message.reply_to_message else ''\n", "      await message.reply(f'{prefix}Chat ID: {msg.chat.id}\\nUser ID: {msg.from_user.id}')\n"]}
{"filename": "modules/sd.py", "chunked_list": ["from aiogram import html, F\n\tfrom aiogram.filters import Command, CommandObject\n\tfrom aiogram.types import Message, BufferedInputFile, InputMediaPhoto\n\tfrom providers.sd_provider import tti, iti, models, embeddings, loras, switch_model, refresh_model_list\n\tfrom utils import tg_image_to_data, parse_photo, CustomArgumentParser, JoinNargsAction\n\tfrom custom_queue import UserLimitedQueue, semaphore_wrapper\n\tfrom typing import Literal\n\tfrom config_reader import config\n\timport asyncio\n\timport pydantic\n", "import re\n\timport shlex\n\timport random\n\tsd_available_resolutions = list(range(256, config.sd_max_resolution + 1, 64))\n\tclass SDArguments(pydantic.BaseModel):\n\t  denoising_strength: float = pydantic.Field(None, ge=0, le=1, alias='d', description='Denoising strength')\n\t  cfg_scale: float = pydantic.Field(None, ge=1, le=25, alias='c', description='Cfg scale')\n\t  steps: int = pydantic.Field(None, ge=5, le=config.sd_max_steps, alias='st')\n\t  sampler_name: Literal[tuple(config.sd_available_samplers)] = pydantic.Field(None, alias=\"sa\", description='Sampler')\n\t  width: Literal[tuple(sd_available_resolutions)] = pydantic.Field(None, alias=\"wi\", description='Width')\n", "  height: Literal[tuple(sd_available_resolutions)] = pydantic.Field(None, alias=\"he\", description='Height')\n\t  negative_prompt: str = pydantic.Field(None, alias='np', description='Negative prompt')\n\t  seed: int = pydantic.Field(None, ge=-1, alias='se', description='Seed')\n\t  prompt: str = pydantic.Field(None)\n\t  mask: Literal[1, -1] = pydantic.Field(None, alias='ma', description='Inpaint mask')\n\t  inpainting_fill: int = pydantic.Field(None, ge=0, le=3, alias='fi', description='Inpaint fill mode')\n\tclass StableDiffusionModule:\n\t  def __init__(self, dp, bot):\n\t    self.queue = UserLimitedQueue(config.sd_queue_size_per_user)\n\t    self.semaphore = asyncio.Semaphore(1)\n", "    if config.mm_preload_models_on_start:\n\t      asyncio.run(refresh_model_list())\n\t    @dp.message(Command(commands=[\"tti\", \"iti\", \"ttiraw\", \"itiraw\"]), flags={\"long_operation\": \"upload_photo\"})\n\t    async def command_sd_handler(message: Message, command: CommandObject, album=False) -> None:\n\t      with self.queue.for_user(message.from_user.id) as available:\n\t        if available:\n\t          parse_error, params = self.parse_input(command.args)\n\t          if parse_error:\n\t            return await message.answer(f\"{html.quote(parse_error)}\")\n\t          prompt = params['prompt']\n", "          if not command.command.endswith('raw'):\n\t            params = self.apply_standard_prompt_modifiers(params)\n\t          processor = tti\n\t          photo = parse_photo(message)\n\t          if command.command.startswith(\"iti\") and photo:\n\t            image_data = await tg_image_to_data(photo, bot)\n\t            params['init_images'] = [image_data]\n\t            if 'mask' in params and album and len(album) > 1:\n\t              if params['mask'] == -1:\n\t                params['inpainting_mask_invert'] = 1\n", "              params['mask'] = await tg_image_to_data(parse_photo(album[1]), bot)\n\t            processor = iti\n\t          elif command.command.startswith(\"iti\"):\n\t            return await message.answer(\"Error, <b>unable to find initial photo</b>\")\n\t          wrapped_runner = semaphore_wrapper(self.semaphore, processor)\n\t          sd_error, data, details = await wrapped_runner(params)\n\t          reply_to = message.reply_to_message.message_id if message.reply_to_message is not None else None\n\t          if sd_error:\n\t            return await message.answer(f\"<b>Error:</b> {sd_error}\")\n\t          else:\n", "            images = [InputMediaPhoto(\n\t              type='photo', \n\t              media=BufferedInputFile(i, filename='image.png'), \n\t              caption= None if idx != 0 else (f'<pre>{html.quote(prompt[0:768])}</pre>\\n' + \n\t              f'Seed: {details.get(\"seed\")}\\n' +\n\t              f'Sampler: {details.get(\"sampler_name\")}\\n' +\n\t              f'Cfg scale: {details.get(\"cfg_scale\")}\\n' + \n\t              f'Steps: {details.get(\"steps\")}\\n' +\n\t              f'Model: {details.get(\"model\")}')\n\t            ) for idx, i in enumerate(data)]\n", "          await message.answer_media_group(media=images, reply_to_message_id=reply_to)\n\t    @dp.message(F.media_group_id)\n\t    async def handle_media_groups(*args, **kwargs):\n\t      return\n\t    @dp.message(Command(commands=[\"models\", \"loras\", \"embeddings\"]), flags={\"cooldown\": 5})\n\t    async def list_sd_models(message: Message, command: CommandObject):\n\t      if len(models.values()) == 0:\n\t        await refresh_model_list()\n\t      if command.command == \"models\":\n\t        return message.answer('<b>Available models:</b> \\n' + \"\\n\".join(models.values()))\n", "      if command.command == \"embeddings\":\n\t        return message.answer('<b>Available embeddings:</b> \\n' + \"\\n\".join(embeddings))\n\t      if command.command == \"loras\":\n\t        loras_list = sorted([*loras, *config.sd_lora_custom_activations.keys()])\n\t        return message.answer('<b>Available loras:</b> \\n' + \"\\n\".join(loras_list))\n\t    @dp.message(\n\t      Command(commands=[\"model\", \"changemodel\", \"switchmodel\"]), \n\t      flags={\n\t        \"cooldown\": 30, \n\t        \"admins_only\": config.sd_only_admins_can_change_models, \n", "        \"long_operation\":\"choose_sticker\"\n\t      }\n\t    )\n\t    async def switch_sd_model(message: Message, command: CommandObject):\n\t      async with self.semaphore:\n\t        if command.args in models.values():\n\t          success = await switch_model(command.args)\n\t          if success:\n\t            return message.answer(f'<b>Model changed to:</b> {command.args}')\n\t          else:\n", "            return message.answer('<b>Unable to change the model.</b>')\n\t        else:\n\t          if not command.args:\n\t            return message.answer('use this command with model name to change the model, try /models for all model names')\n\t          return message.answer('<b>Model not found</b>')\n\t  def parse_input(self, user_input):\n\t      user_input = str(user_input) + ' '\n\t      parser = CustomArgumentParser(description='generate images from text and other images')\n\t      parser.add_argument('-d', type=float, help='Denoising strength (for image-to-image) (0 <= d <= 1)')\n\t      parser.add_argument('-c', type=float, help='Cfg scale (1 <= c <= 25)')\n", "      parser.add_argument('-sa', choices=['Euler a', 'Euler', 'Heun', 'DPM++ 2M', 'DPM++ 2S a'], help='Sampler')\n\t      parser.add_argument('-st', type=int, help=f'Number of steps (5 <= st <= {config.sd_max_steps})')\n\t      parser.add_argument('-se', type=int, help='Seed')\n\t      parser.add_argument('-wi', type=int, help='Image width')\n\t      parser.add_argument('-he', type=int, help='Image height')\n\t      parser.add_argument('-ma', type=int, help='Use the 2nd image as inpaint regular/inverse mask (1 or -1)')\n\t      parser.add_argument('-fi', type=int, help='Inpaint mask fill mode [0 <= fi <= 3], fill/original/l.noise/l.nothing')\n\t      parser.add_argument('-np', type=str, help='Negative prompt')\n\t      parser.add_argument('prompt', type=str, help='prompt', nargs=\"*\", action=JoinNargsAction)\n\t      try:\n", "        # override default -h behavior\n\t        if '-help' in user_input or 'help' in user_input or '-h ' in user_input:\n\t          return (parser.format_help().replace(\n\t            'bot.py','/tti /iti /ttiraw /itiraw, /models /loras /embeddings /command@botname params text' \n\t          ), None)\n\t        # parse arguments using custom arg parser\n\t        args = parser.parse_args(shlex.split(user_input))\n\t        # apply pydantic checks\n\t        sd_args = {k: v for k, v in SDArguments(**vars(args)) if v is not None}\n\t        return (False, sd_args)\n", "      except (Exception, SystemExit) as e:\n\t        return (str(e), None)\n\t  def apply_standard_prompt_modifiers(self, params):\n\t    params['prompt'] = config.sd_extra_prompt.format(prompt=self.parse_lora(params['prompt']))\n\t    params['negative_prompt'] = config.sd_extra_negative_prompt.format(\n\t      negative_prompt='' if 'negative_prompt' not in params else params['negative_prompt']\n\t    ) \n\t    if 'width' in params and 'height' in params:\n\t      if params['width'] == params['height']:\n\t        params['prompt'] += ', square image'\n", "    return params\n\t  def parse_lora(self, prompt):\n\t    if 'lora' in prompt:\n\t      return prompt\n\t    seamless_loras = config.sd_lora_custom_activations or {}\n\t    for key in seamless_loras:\n\t      if type(seamless_loras[key]) is list:\n\t        prompt = prompt.replace(key, random.choice(seamless_loras[key]))\\\n\t                       .replace('LORA_RANGES', str(random.choice([0.9, 0.95, 1.0, 1.1, 1.2])))\n\t      else:\n", "        prompt = prompt.replace(key, seamless_loras[key])\n\t    subst = \"<lora:\\\\1:\\\\2.\\\\3\\\\4>\"\n\t    for lora in loras:\n\t      regex = fr\"({lora})([0-9])?([0-9])([0-9])\"\n\t      prompt = re.sub(regex, subst, prompt, 1, re.IGNORECASE)\n\t    return prompt\n\t  async def tti(self, params):\n\t    wrapped_runner = semaphore_wrapper(self.semaphore, tti)\n\t    return await wrapped_runner(params)\n\t  async def iti(self, params):\n", "    wrapped_runner = semaphore_wrapper(self.semaphore, iti)\n\t    return await wrapped_runner(params)"]}
{"filename": "modules/stt.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\n\tfrom aiogram.types import Message, BufferedInputFile\n\tfrom aiogram import html, F\n\tfrom providers.stt_provider import active_model\n\tfrom custom_queue import UserLimitedQueue, semaphore_wrapper\n\tfrom config_reader import config\n\tfrom utils import download_audio\n\tfrom types import SimpleNamespace\n\timport random\n\timport tempfile\n", "import asyncio\n\tclass SpeechToTextModule:\n\t  def __init__(self, dp, bot):\n\t    self.queue = UserLimitedQueue(config.stt_queue_size_per_user)\n\t    self.semaphore = asyncio.Semaphore(1)\n\t    self.bot = bot\n\t    self.model = active_model.init()\n\t    if config.stt_autoreply_mode:\n\t      @dp.message((F.voice), flags={\"long_operation\": \"record_audio\"})\n\t      async def handle_voice_messages(message: Message):\n", "        error, text = await self.recognize_voice_message(message)\n\t        if text and 'llm' in config.active_modules:\n\t          llm = dp['modules']['llm']\n\t          llm_call_func = llm.assist if config.stt_autoreply_mode == 'assist' else llm.chat\n\t          reply = await llm_call_func(text, llm.get_common_chat_attributes(message))\n\t          if 'tts' in config.active_modules:\n\t            voice = config.stt_autoreply_voice or config.tts_voices[0]\n\t            voice = random.choice(config.tts_voices) if voice == 'random' else voice\n\t            await bot.reply_tts(message=message, command=SimpleNamespace(command=voice, args=[reply]))\n\t          else:\n", "            return await message.answer(reply)\n\t        if error:\n\t          return await message.answer(f\"Error, <b>{error}</b>\")\n\t    @dp.message(Command(commands=[\"stt\", \"recognize\", \"transcribe\"]), flags={\"long_operation\": \"typing\"})\n\t    async def command_stt_handler(message: Message, command: CommandObject) -> None:\n\t      with self.queue.for_user(message.from_user.id) as available:\n\t        if not available:\n\t          return\n\t        if command.command == \"stt\" and ('-h' in str(command.args) or not (message.reply_to_message and message.reply_to_message.voice)):\n\t          return await message.answer(f\"Usage: /stt [voice_message] \\nUse the commands like /command@botname\")\n", "        else:\n\t          error, text = await self.recognize_voice_message(message)\n\t          if error:\n\t            return await message.answer(f\"Error, <b>{error}</b>\")\n\t          else:\n\t            return await message.answer(f\"<i>{text}</i>\")\n\t  async def recognize_voice_message(self, message):\n\t    with tempfile.NamedTemporaryFile(suffix='.ogg', delete=False) as temp_file:\n\t      voice = message.reply_to_message.voice if not message.voice else message.voice\n\t      await download_audio(self.bot, voice.file_id, temp_file.name)\n", "      wrapped_runner = semaphore_wrapper(self.semaphore, self.recognize)\n\t      error, data = await wrapped_runner(temp_file.name)\n\t      return error, data\n\t  async def recognize(self, audio_path):\n\t    return await self.model.recognize(audio_path)\n"]}
{"filename": "modules/tta.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\n\tfrom aiogram.types import Message, BufferedInputFile\n\tfrom providers.tts_provider import convert_to_ogg\n\tfrom providers.tta_provider import generate_audio_async, tta_init\n\tfrom custom_queue import UserLimitedQueue, semaphore_wrapper\n\tfrom config_reader import config\n\timport asyncio\n\tclass TextToAudioModule:\n\t  def __init__(self, dp, bot):\n\t    self.queue = UserLimitedQueue(config.tta_queue_size_per_user)\n", "    self.semaphore = asyncio.Semaphore(1)\n\t    if 'tta' in config.active_modules:\n\t      self.available = tta_init()\n\t    if not self.available:\n\t      return\n\t    @dp.message(Command(commands=[\"tta\", \"sfx\", \"music\"]), flags={\"long_operation\": \"record_audio\"})\n\t    async def command_tta_handler(message: Message, command: CommandObject) -> None:\n\t      with self.queue.for_user(message.from_user.id) as available:\n\t        if not available:\n\t          return\n", "        if command.command == \"tta\" or not command.args or str(command.args).strip() == \"\" or ('-help' in str(command.args)):\n\t          return await message.answer(f\"Usage: /sfx text /music text\\nUse the commands like /command@botname\")\n\t        else:\n\t          audio_type = command.command\n\t          text = str(command.args)\n\t          wrapped_runner = semaphore_wrapper(self.semaphore, generate_audio_async)\n\t          error, data = await wrapped_runner(text, audio_type, config.tta_duration)\n\t          print(error, data)\n\t          if error:\n\t            return await message.answer(f\"Error, <b>{error}</b>\")\n", "          else:\n\t            audio = BufferedInputFile(convert_to_ogg(data), 'audio.ogg')\n\t            return await message.answer_voice(voice=audio)\n"]}
{"filename": "servers/tts_server.py", "chunked_list": ["import sys \n\timport os\n\tsys.path.append('.')\n\tsys.path.append('..')\n\tfrom fastapi import FastAPI\n\tfrom providers.tts_provider import tts, save_audio\n\tfrom pydantic import BaseModel, Field\n\tfrom fastapi.responses import StreamingResponse\n\tfrom io import BytesIO\n\tfrom typing_extensions import Literal\n", "app = FastAPI()\n\tclass Data(BaseModel):\n\t  voice: str = Field(None, description='String with speaker model name')\n\t  text: str = Field(None, description='String with text that you want to hear')\n\t  response: Literal['file', 'path'] = Field(None, description='String with value \"file\" or \"path\", changes the output format to either the path to the recorded audio or the file itself.')\n\t@app.post(\"/\")\n\tasync def read_root(rqdata: Data):\n\t  error, data = await tts(rqdata.voice, rqdata.text)\n\t  if not error:\n\t    if rqdata.response == 'file':\n", "      bytes = BytesIO(open(data, mode='rb').read())\n\t      os.remove(data)\n\t      response = StreamingResponse(bytes, media_type='audio/wav')\n\t      response.headers[\"Content-Disposition\"] = f\"inline; filename=record.wav\"\n\t      return response\n\t    return {\"data\": data}\n\t  else:\n\t    return {\"error\": error}\n\tif __name__ == \"__main__\":\n\t  import uvicorn\n", "  uvicorn.run(app, host=\"0.0.0.0\", port=7077)\n"]}
{"filename": "chroniclers/base.py", "chunked_list": ["import importlib\n\timport re\n\tfrom abc import ABCMeta, abstractmethod\n\tfrom collections import defaultdict\n\tclass AbstractChronicler(metaclass=ABCMeta):\n\t  def __init__(self, filename):\n\t    chronicler_script = importlib.import_module(filename)\n\t    self.chronicler_script = chronicler_script\n\t    self.vars = chronicler_script.get_chat_variables\n\t    self.gen_cfg = chronicler_script.get_generation_config\n", "    self.init_cfg = chronicler_script.get_init_config\n\t  @abstractmethod\n\t  def prepare(self, details):\n\t    pass\n\t  @abstractmethod\n\t  def parse(self):\n\t    pass\n\t  @staticmethod\n\t  def prepare_hook(func):\n\t    def wrapper(self, *args, **kwargs):\n", "      formatter = getattr(self.chronicler_script, 'custom_input_formatter', func)\n\t      return formatter(self, *args, **kwargs)\n\t    return wrapper\n\t  @staticmethod\n\t  def parse_hook(func):\n\t    def wrapper(self, *args, **kwargs):\n\t      print(args[0])\n\t      parser = getattr(self.chronicler_script, 'custom_output_parser', func)\n\t      return parser(self, *args, **kwargs)\n\t    return wrapper\n", "class AssistantReplyChronicler(AbstractChronicler):\n\t  def __init__(self, chronicler_filename):\n\t    super().__init__(chronicler_filename)\n\t  def prepare(self, details, fresh=False):\n\t    text = details['message']\n\t    reply_text = details['reply_text']\n\t    if text and reply_text:\n\t      memory = self.parse_qa(reply_text) + '\\n' + text\n\t      details['message'] = memory\n\t    return chroniclers['instruct'].prepare(self, details)\n", "  def parse_qa(self, text):\n\t    if text.startswith('Q:'):\n\t      splits = text.split('\\n\\n')\n\t      return f'>{splits[0][2:]}\\n>{splits[1][2:]}'\n\t    else:\n\t      return '>' + text.replace('\\n', ' ')\n\t  def parse(self, output, chat_id, skip=0):\n\t    return chroniclers['instruct'].parse(self, output, chat_id, skip)\n\tclass ConversationChronicler(AbstractChronicler):\n\t  def __init__(self, chronicler_filename, continous=False, max_length=10):\n", "    super().__init__(chronicler_filename)\n\t    self.history = defaultdict(lambda: [])\n\t    self.max_length = max_length\n\t    self.multiline_re = re.compile(\"[^\\n:]+\\:[^\\n]+\\n\")\n\t  def get_author(self, vars, item):\n\t    r_username = vars.get('replace_username', False)\n\t    return r_username if r_username and item['author'] != vars['name'] else item['author']\n\t  def prepare(self, details, fresh=False):\n\t    if fresh:\n\t      self.history[details['chat_id']] = []\n", "    history = self.history[details['chat_id']]\n\t    history.append({\"message\": details['message'], \"author\": details['author']})\n\t    while len(history) >= self.max_length:\n\t      history.pop(0)\n\t    conversation = ''\n\t    char_vars = self.vars(details)\n\t    for item in history:\n\t      msg = item[\"message\"]\n\t      author = self.get_author(char_vars, item)\n\t      conversation += f'{author}: {msg[0].upper() + msg[1:]}\\n'\n", "    if char_vars['pre_dialog']:\n\t      char_vars['pre_dialog'] += '\\n'\n\t    dialog = '''{intro}\n\t{personality}\n\t{pre_dialog}{conversation}{name}:'''\\\n\t    .format(conversation=conversation, **char_vars)\n\t    return dialog\n\t  def parse(self, output, chat_id, skip=0):\n\t    output = output.strip()[skip:]\n\t    print(output)\n", "    re_end = re.search(self.multiline_re, output) or re.search('\\n', output) \n\t    end = (output.find('</s>') + 1) or (re_end.span()[0] if re_end else len(output) + 1)\n\t    parsed = output[:end - 1].strip()\n\t    if parsed == '':\n\t      return '...'\n\t    author = self.vars()['name']\n\t    self.history[chat_id].append({\"message\": parsed.replace(':', ''), \"author\": author})\n\t    return parsed\n\tclass AlpacaAssistantChronicler(AbstractChronicler):\n\t  def __init__(self, chronicler_filename):\n", "    super().__init__(chronicler_filename)\n\t  @AbstractChronicler.prepare_hook\n\t  def prepare(self, details, fresh=False):\n\t    msg = details['message'].split('\\n', 1)\n\t    l = self.vars(details)\n\t    if len(msg) > 1 and l['assistant_input']:\n\t      return f\"\"\"{l['assistant_intro1']} \n\t### {l['assistant_instruction']}:\n\t{msg[0]}\n\t### {l['assistant_input']}:\n", "{msg[1]}\n\t### {l['assistant_response']}:\n\t\"\"\"\n\t    else:\n\t      return f\"\"\"{l['assistant_intro2']} \n\t### {l['assistant_instruction']}:\n\t{msg[0]}\n\t### {l['assistant_response']}:\n\t\"\"\"\n\t  @AbstractChronicler.parse_hook\n", "  def parse(self, output, chat_id, skip=0):\n\t    output = output[skip:]\n\t    end = output.find('</s>')\n\t    if end == -1:\n\t      end = output.find('###')\n\t    parsed = output[0: end if end != -1 else None].strip()\n\t    if parsed == '':\n\t      return '...'\n\t    return parsed\n\tclass RawChronicler(AbstractChronicler):\n", "  def __init__(self, chronicler_filename):\n\t    super().__init__(chronicler_filename)\n\t  def prepare(self, details, fresh=False):\n\t    return details['message']\n\t  def parse(self, output, chat_id, skip=0):\n\t    print(output)\n\t    return output\n\tchroniclers = {\n\t  \"alpaca\": AlpacaAssistantChronicler,\n\t  \"instruct\": AlpacaAssistantChronicler,\n", "  \"chat\": ConversationChronicler,\n\t  \"reply\": AssistantReplyChronicler,\n\t  \"raw\": RawChronicler\n\t}"]}
{"filename": "characters/pygmalion_chat_king_william.py", "chunked_list": ["def get_assistant_variables():\n\t  return {\n\t    \"assistant_intro1\": \"Provide a correct repsonse.\",\n\t    \"assistant_intro2\": \"\",\n\t    \"assistant_instruction\": \"Question\",\n\t    \"assistant_input\": False,\n\t    \"assistant_response\": \"Answer\"\n\t  }\n\tdef get_chat_variables(context=None):\n\t  # change these as you wish\n", "  # sample personality: King William\n\t  name = 'William'\n\t  intro = f\"{name}'s Persona: {name} is the king of an ancient principality located on one of the islands of the northern sea. He is proud and honest, and despises pitiful peasants and other kings.\"\n\t  predialog = ''\n\t  return {\"intro\": intro, \"personality\": '<START>', 'name': name, 'pre_dialog': predialog, **get_assistant_variables() }\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.75,\n\t    \"top_k\": 50,\n\t    \"top_p\": 0.95,\n", "    \"repetition_penalty\": 1.12,\n\t    **override\n\t  }\n\tdef get_init_config():\n\t  return {}"]}
{"filename": "characters/gpt4all_default.py", "chunked_list": ["def get_assistant_variables():\n\t  return {}\n\tdef get_chat_variables(context=None):\n\t  return {\"intro\": '', \"personality\": '', 'name': 'ASSISTANT', 'pre_dialog': '', **get_assistant_variables() }\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.7,\n\t    \"top_k\": 50,\n\t    \"top_p\": 0.95,\n\t    \"repetition_penalty\": 1.1,\n", "    **override\n\t  }\n\tdef custom_input_formatter(chronicler, details, fresh=True):\n\t    msg = details['message'].replace('\\n', ' ')\n\t    return f\"\"\"{msg}\n\t\"\"\"\n\tdef custom_output_parser(chronicler, output, chat_id, skip=0):\n\t    output = output[skip:].strip()\n\t    return output\n\tdef get_init_config():\n", "  return {'context_size': 2048}"]}
{"filename": "characters/gptj_6B_default.py", "chunked_list": ["from datetime import datetime\n\tdef get_chat_variables(context=None):\n\t  intro = 'The year is {}.'.format(datetime.now().year)\n\t  personality = 'I am a very advanced AI from another planet. I met a person, their name is {}.\\n'.format(\n\t    context['author'] if context else ''\n\t  )\n\t  name = 'AI'\n\t  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': ''}\n\tdef get_generation_config(override={}):\n\t  return {\n", "    \"temperature\": 0.8,\n\t    \"top_k\": 40,\n\t    \"top_p\": 1,\n\t    \"repetition_penalty\": 1.01,\n\t    **override\n\t  }\n\tdef get_init_config():\n\t  return {}"]}
{"filename": "characters/ru_saiga_lcpp.py", "chunked_list": ["#Reference: https://github.com/IlyaGusev/rulm/blob/master/self_instruct/src/interact_llamacpp.py\n\t#License: Apache License 2.0\n\tfrom llama_cpp import Llama\n\tSYSTEM_PROMPT = \"  ,   .       .\"\n\tSYSTEM_TOKEN = 1788\n\tUSER_TOKEN = 1404\n\tBOT_TOKEN = 9225\n\tLINEBREAK_TOKEN = 13\n\tROLE_TOKENS = {\n\t    \"user\": USER_TOKEN,\n", "    \"bot\": BOT_TOKEN,\n\t    \"system\": SYSTEM_TOKEN\n\t}\n\tdef get_message_tokens(model, role, content):\n\t    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n\t    message_tokens.insert(1, ROLE_TOKENS[role])\n\t    message_tokens.insert(2, LINEBREAK_TOKEN)\n\t    message_tokens.append(model.token_eos())\n\t    return message_tokens\n\tdef get_system_tokens(model):\n", "    system_message = {\n\t        \"role\": \"system\",\n\t        \"content\": SYSTEM_PROMPT\n\t    }\n\t    return get_message_tokens(model, **system_message)\n\tdef get_assistant_variables():\n\t  # change these only if your custom lora input format changed\n\t  return {'replace_username': 'User'}\n\tdef get_chat_variables(context=None):\n\t  # change these as you wish\n", "  name = 'Saiga'\n\t  intro = SYSTEM_PROMPT\n\t  return {\"intro\": intro, \"personality\": '', 'name': name, 'pre_dialog': '', **get_assistant_variables() }\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.2,\n\t    \"top_k\": 30,\n\t    \"top_p\": 0.9,\n\t    \"repetition_penalty\": 1.1,\n\t    **override\n", "  }\n\tdef custom_input_formatter(chronicler, details, fresh=True):\n\t  model = details['model']['instance']\n\t  tokens = get_system_tokens(model)\n\t  message_tokens = get_message_tokens(model=model, role=\"user\", content=details['message'])\n\t  role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n\t  tokens += message_tokens + role_tokens\n\t  # detokinization is used for compatibility with moldel(), since model.generate is not supported\n\t  return model.detokenize(tokens).decode(\"utf-8\")\n\tdef custom_output_parser(chronicler, output, chat_id, skip=0):\n", "    output = output[skip:].strip()\n\t    end = (output.find('User:') + 1 ) or (output.find('Saiga:') + 1) or (len(output) + 1)\n\t    return output[:end - 1].strip()\n\tdef get_init_config():\n\t  return {'context_size': 2000}"]}
{"filename": "characters/min_chatGPT2_default.py", "chunked_list": ["from datetime import datetime\n\tdef get_chat_variables(context=None):\n\t  intro = 'The year is {}.'.format(datetime.now().year)\n\t  personality = 'I am a very advanced AI from another planet. I met a person, their name is {}.\\n'.format(\n\t    context['author'] if context else ''\n\t  )\n\t  name = 'AI'\n\t  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': ''}\n\tdef custom_input_formatter(chronicler, details, fresh=True):\n\t    msg = details['message']\n", "    return f\"\"\"Human: {msg}\n\tAssistant: \n\t\"\"\"\n\tdef custom_output_parser(chronicler, output, chat_id, skip=0):\n\t  def parse(self, output, chat_id, skip=0):\n\t    output = output[skip:].strip()\n\t    end = (output.find('Human:') + 1 ) or (output.find('Assistant:') + 1) or (len(output) + 1)\n\t    parsed = output[:end - 1].strip()\n\t    if parsed == '':\n\t      return '...'\n", "    return parsed\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.9,\n\t    \"top_k\": 200,\n\t    **override\n\t  }\n\tdef get_init_config():\n\t  return {\n\t    \"use_tiktoken\": True,\n", "    \"nanogpt\": True\n\t  }"]}
{"filename": "characters/llama_chat_default.py", "chunked_list": ["from datetime import datetime\n\tdef get_assistant_variables():\n\t  # change these only if your custom lora input format changed\n\t  return {\n\t    \"assistant_intro1\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\",\n\t    \"assistant_intro2\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n\t    \"assistant_instruction\": \"Instruction\",\n\t    \"assistant_input\": \"Input\",\n\t    \"assistant_response\": \"Response\"\n\t  }\n", "def get_chat_variables(context=None):\n\t  # change these as you wish\n\t  name = 'AI'\n\t  intro = 'The year is {}.'.format(datetime.now().year)\n\t  personality = f'My name is {name}. I am a very advanced AI. I chat with humans. I must be verbose and honest, expressing myself.\\n'\n\t  predialog = f'''Andy: What is your name? Why are you here?\n\t{name}: Humans call me {name}. Nice to meet you. I am here to chat with you.'''\n\t  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': predialog, **get_assistant_variables() }\n\tdef get_generation_config(override={}):\n\t  return {\n", "    \"temperature\": 0.7,\n\t    \"top_k\": 50,\n\t    \"top_p\": 0.95,\n\t    \"repetition_penalty\": 1.2,\n\t    **override\n\t  }\n\tdef get_init_config():\n\t  return {}"]}
{"filename": "characters/llama_rulora_assistant_only.py", "chunked_list": ["# for https://huggingface.co/IlyaGusev/llama_7b_ru_turbo_alpaca_lora\n\tfrom datetime import datetime\n\tdef custom_input_formatter(chronicler, details, fresh=True):\n\t  lines = details[\"message\"].split('\\n')\n\t  query = lines[0]\n\t  is_question = query.endswith('?')\n\t  if len(lines) > 1 or not is_question:\n\t    return f''': {query}.\n\t: {' '.join(lines[1:]) if len(lines) > 1 else ''}\n\t:'''\n", "  else:\n\t    return f''': {query} \n\t:'''\n\tdef get_assistant_variables():\n\t  return {}\n\tdef get_chat_variables(context=None):\n\t  return {\"intro\": '', \"personality\": '', 'name': '', 'pre_dialog': '', **get_assistant_variables() }\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.7,\n", "    \"top_k\": 100,\n\t    \"top_p\": 0.75,\n\t    \"repetition_penalty\": 1.16,\n\t    **override\n\t  }\n\tdef get_init_config():\n\t  return {}"]}
{"filename": "characters/ru_gpt3_default.py", "chunked_list": ["from datetime import datetime\n\tdef get_chat_variables(context=None):\n\t  intro = ' {} .'.format(datetime.now().year)\n\t  personality = ' -    .   ,   {}.  .'.format(\n\t    context['author'] if context else ''\n\t  )\n\t  name = ''\n\t  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': ''}\n\tdef get_generation_config(override={}):\n\t  return {\n", "    \"temperature\": 0.85,\n\t    \"top_k\": 50,\n\t    \"top_p\": 0.92,\n\t    \"repetition_penalty\": 1.01,\n\t    **override\n\t  }\n\tdef get_init_config():\n\t  return {}"]}
{"filename": "characters/orca_default.py", "chunked_list": ["ORCA_VERSION = 2\n\tdef get_assistant_variables():\n\t  return {}\n\tdef get_chat_variables(context=None):\n\t  intro = 'You are an AI assistant that follows instruction extremely well. Help as much as you can.'\n\t  return {\"intro\": intro, \"personality\": '', 'name': 'ASSISTANT', 'pre_dialog': ''}\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.7,\n\t    \"top_k\": 50,\n", "    \"top_p\": 0.95,\n\t    \"repetition_penalty\": 1.1,\n\t    **override\n\t  }\n\tdef custom_input_formatter(chronicler, details, fresh=True):\n\t  msg = details['message']\n\t  n = '\\n'\n\t  if not msg.startswith('>') and '\\n' in msg:\n\t    msg = msg.split('\\n', 1)\n\t  else:\n", "    msg = [msg]\n\t  template = f'''### System:\n\tYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\t### User:\n\t{msg[0]}\n\t### Response:\n\t''' if ORCA_VERSION == 1 else f'''### System:\n\tYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\t### User:\n\t{msg[0]}\n", "### Input:{(n + msg[1]) if len(msg) > 1 else \"\"}\n\t### Response:\n\t'''\n\t  return template\n\tdef custom_output_parser(chronicler, output, chat_id, skip=0):\n\t    output = output[skip:].strip()\n\t    end = (output.find('###') + 1) or (len(output) + 1)\n\t    return output[:end - 1].strip()\n\tdef get_init_config():\n\t  return {'context_size': 2048}"]}
{"filename": "characters/vicuna_default.py", "chunked_list": ["from datetime import datetime\n\tVERSION = 1.1\n\tdef get_assistant_variables():\n\t  # change these only if your custom lora input format changed\n\t  if VERSION == 0:\n\t    intro = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n\t    assistant_instruction = \"Human\"\n\t    assistant_response = \"Assistant\"\n\t  if VERSION == 1.1:\n\t    intro = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n", "    assistant_instruction = \"USER\"\n\t    assistant_response = \"ASSISTANT\"\n\t  return {\n\t    \"assistant_intro1\": intro,\n\t    \"assistant_intro2\": intro,\n\t    \"assistant_instruction\": assistant_instruction,\n\t    \"assistant_input\": False,\n\t    \"assistant_response\": assistant_response\n\t  }\n\tdef get_chat_variables(context=None):\n", "  # change these as you wish\n\t  assistant_variables = get_assistant_variables()\n\t  name = 'AI'\n\t  intro = 'The year is {}.'.format(datetime.now().year)\n\t  personality = f'My name is {name}. I am a very advanced AI. I chat with humans. I must be verbose and honest, expressing myself.\\n'\n\t  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': '', **get_assistant_variables() }\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.7,\n\t    \"top_k\": 50,\n", "    \"top_p\": 0.95,\n\t    \"repetition_penalty\": 1.05,\n\t    **override\n\t  }\n\tdef get_init_config(): \n\t  return {'context_size': 2048}"]}
{"filename": "characters/samantha_default.py", "chunked_list": ["from datetime import datetime\n\tdef get_assistant_variables():\n\t  # change these only if your custom lora input format changed\n\t  return {'replace_username': 'USER'}\n\tdef get_chat_variables(context=None):\n\t  # change these as you wish\n\t  name = 'ASSISTANT'\n\t  intro = 'You are Samantha, a sentient AI.'\n\t  return {\"intro\": intro, \"personality\": '', 'name': name, 'pre_dialog': '', **get_assistant_variables() }\n\tdef get_generation_config(override={}):\n", "  return {\n\t    \"temperature\": 0.7,\n\t    \"top_k\": 50,\n\t    \"top_p\": 0.95,\n\t    \"repetition_penalty\": 1.1,\n\t    **override\n\t  }\n\tdef custom_input_formatter(chronicler, details, fresh=True):\n\t  cvars = get_chat_variables()\n\t  template = f'''{cvars['intro']}\n", "  USER: {details['message']}\n\t  {cvars['name']}:'''\n\t  return template\n\tdef custom_output_parser(chronicler, output, chat_id, skip=0):\n\t    output = output[skip:].strip()\n\t    end = (output.find('USER:') + 1 ) or (output.find('ASSISTANT:') + 1) or (len(output) + 1)\n\t    return output[:end - 1].strip()\n\tdef get_init_config():\n\t  return {}"]}
{"filename": "characters/llama_2_nous-hermes.py", "chunked_list": ["def get_init_config():\n\t  return {'context_size': 4096}\n\tdef custom_input_formatter(chronicler, details, fresh=True):\n\t  template = f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction: {details['message']}\n\t### Response: '''\n\t  return template\n\tdef get_chat_variables(context=None):\n\t  # change these as you wish\n\t  name = 'Assistant'\n", "  intro = 'System:'\n\t  personality = f'You are a helpful, respectful and honest assistant.\\nUser:\\n'\n\t  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': '' }\n\tdef custom_output_parser(chronicler, output, chat_id, skip=0):\n\t    output = output[skip:].strip()\n\t    end = (output.find('### ') + 1) or (len(output) + 1)\n\t    return output[:end - 1].strip()\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.6,\n", "    \"top_k\": 50,\n\t    \"top_p\": 0.95,\n\t    \"repetition_penalty\": 1.12,\n\t    **override\n\t  }\n"]}
{"filename": "characters/vicuna_16k.py", "chunked_list": ["from datetime import datetime\n\tVERSION = 1.1\n\tdef get_assistant_variables():\n\t  # change these only if your custom lora input format changed\n\t  if VERSION == 1.1:\n\t    intro = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n\t    assistant_instruction = \"USER\"\n\t    assistant_response = \"ASSISTANT\"\n\t  return {\n\t    \"assistant_intro1\": intro,\n", "    \"assistant_intro2\": intro,\n\t    \"assistant_instruction\": assistant_instruction,\n\t    \"assistant_input\": False,\n\t    \"assistant_response\": assistant_response\n\t  }\n\tdef get_chat_variables(context=None):\n\t  # change these as you wish\n\t  assistant_variables = get_assistant_variables()\n\t  name = 'AI'\n\t  intro = 'The year is {}.'.format(datetime.now().year)\n", "  personality = f'My name is {name}. I am a very advanced AI. I chat with humans. I must be verbose and honest, expressing myself.\\n'\n\t  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': '', **get_assistant_variables() }\n\tdef get_generation_config(override={}):\n\t  return {\n\t    \"temperature\": 0.82,\n\t    \"top_k\": 72,\n\t    \"top_p\": 0.21,\n\t    \"repetition_penalty\": 1.19,\n\t    **override\n\t  }\n", "def get_init_config(): \n\t  # rope_freq_scale = 1 / (max_seq_len / 2048)\n\t  return {'context_size': 16092, 'rope_freq_base': 10000, 'rope_freq_scale': 0.125}"]}
{"filename": "misc/mps_fixups.py", "chunked_list": ["# Workarounds and fixes for LLMs for mps accelerator\n\t# Copyright Jeremy Barnes / MIT License\n\t# reference code:\n\t# https://github.com/jeremybarnes/llm-webgpu/blob/main/mps_fixups.py\n\t#\n\timport torch\n\tfrom torch import Tensor\n\tfrom typing import Optional\n\tdef fixup_mps():\n\t    orig_topk = torch.topk\n", "    # Topk only works up to k=15 on MPS, replace it with a CPU fallback\n\t    def _topk(self: torch.Tensor, k: int, dim:int=-1, largest:bool=True, sorted:bool=True):\n\t        res, indices = orig_topk(self.to('cpu', torch.float32), k, dim, largest, sorted)\n\t        return res.to(self), indices.to('mps')\n\t    torch.topk = _topk\n\t    orig_max = torch.max\n\t    # Max doesn't work with longs on MPS, replace it with a CPU fallback\n\t    def _max(self: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n\t        return orig_max(self.to('cpu'), *args, **kwargs).to('mps')\n\t    torch.max = _max\n", "    orig_cumsum = torch.cumsum\n\t    # Cumulative sum doesn't work, replace with CPU fallback\n\t    def _cumsum(input: torch.Tensor, dim: int, **kwargs) -> torch.Tensor:\n\t        return orig_cumsum(input.to('cpu', torch.float32), dim, **kwargs).to('mps', input.dtype)\n\t    torch.cumsum = _cumsum\n\t    torch.Tensor.cumsum = _cumsum\n"]}
{"filename": "misc/memory_manager.py", "chunked_list": ["import torch\n\timport sys\n\timport psutil\n\tfrom config_reader import config\n\tfrom time import time \n\tSHARED_MEMORY = sys.platform == \"darwin\"\n\tGPU_AVAILABLE = torch.cuda.is_available()\n\tdef get_vram_info():\n\t  if torch.cuda.is_available():\n\t    device = torch.cuda.current_device()\n", "    vram_bytes = torch.cuda.get_device_properties(device).total_memory\n\t    try:\n\t      vram_bytes, total  = torch.cuda.mem_get_info()\n\t    except Exception:\n\t      pass\n\t    vram_gb = vram_bytes / 1e9\n\t    return vram_gb\n\t  else:\n\t    return 0.0\n\tdef get_system_ram_info():\n", "    virtual_memory = psutil.virtual_memory()\n\t    memory_gb = virtual_memory.available / (1024**3)\n\t    return memory_gb\n\tram_available = get_system_ram_info()\n\tclass MModel:\n\t  def __init__(self, name, load, unload):\n\t    self.name = name\n\t    self.model = load()\n\t    self.load = load\n\t    self.unload = unload\n", "  def __setattr__(self, name, value):\n\t    self.__dict__[name] = value\n\tclass MemoryManager:\n\t  def __init__(self, get_memory, cached_model_count):\n\t    self.get_memory = get_memory\n\t    self.starting_memory = get_memory()\n\t    self.cache = {}\n\t    self.cached_model_count = cached_model_count\n\t    self.mm_management_policy = config.mm_management_policy\n\t  def wrap(self, model_name, load_function=None, unload_function=None, memory='auto'):\n", "    mem = self.get_memory()\n\t    # get keys and values of items with model != None as lists\n\t    [*alive_keys], [*alive_values] = zip(*((i.name, i) for i in self.cache.values() if i.model is not None)) \\\n\t      if len(self.cache.keys()) > 0 else ([],[])\n\t    if config.mm_autounload_after_seconds > 0:\n\t      seconds = config.mm_autounload_after_seconds\n\t      for key in alive_keys:\n\t        if key != model_name and self.cache[key].last_used + seconds < time():\n\t          self.unload(key, 'timeout')\n\t          alive_keys.remove(key)\n", "          alive_values.remove(self.cache[key])\n\t    if model_name not in self.cache:\n\t      self.cache[model_name] = MModel(model_name, load_function, unload_function)\n\t      self.cache[model_name].last_loaded = time()\n\t    elif not self.cache[model_name].model:\n\t      self.cache[model_name].model = load_function()\n\t      self.cache[model_name].last_loaded = time()\n\t    mem_diff = mem - self.get_memory()\n\t    mem_diff = mem_diff * int(mem_diff > 0)\n\t    item = self.cache[model_name]\n", "    item.memory = (mem_diff if memory == 'auto' else memory(item.model) or mem_diff) / 1e9\n\t    item.last_used = time()\n\t    item.use_count = (item.use_count + 1) if hasattr(item, 'use_count') else 1\n\t    if self.mm_management_policy == 'COUNT' or self.mm_management_policy == 'BOTH':\n\t      cache_count = len(alive_keys)\n\t      if cache_count > 0 and cache_count > self.cached_model_count:\n\t        unloaded_key = self.unload_by_policy(model_name, alive_values)\n\t        if self.mm_management_policy == 'BOTH':\n\t          alive_keys.remove(unloaded_key)\n\t          alive_values.remove(self.cache[unloaded_key])\n", "    if self.mm_management_policy == 'MEMORY' or self.mm_management_policy == 'BOTH' \\\n\t    and len(alive_values) > 0:\n\t      items_memory = list(item.memory for item in alive_values)\n\t      total_memory_used = sum(items_memory)\n\t      memory_available = self.get_memory()\n\t      if memory_available < max(items_memory) * 1.3 \\\n\t      or memory_available < self.starting_memory/3 \\\n\t      or total_memory_used * 1.3 > self.starting_memory:\n\t        self.unload_by_policy(model_name, alive_values)\n\t    return self.cache[model_name].model\n", "  def unload(self, name, reason):\n\t    target = self.cache[name]\n\t    if target.unload is not None:\n\t      target.unload(target.model)\n\t    self.cache[name].model = None\n\t    print('removed', name, 'from model cache by', reason)\n\t  def unload_by_policy(self, model_name, items):\n\t    if config.mm_unload_order_policy == 'LEAST_USED':\n\t      items = sorted(items, key=lambda x: x.use_count)\n\t    if config.mm_unload_order_policy == 'OLDEST_USE_TIME':\n", "      items = sorted(items, key=lambda x: x.last_used)\n\t    if config.mm_unload_order_policy == 'OLDEST_LOAD_ORDER':\n\t      items = sorted(items, key=lambda x: x.last_loaded)\n\t    if config.mm_unload_order_policy == 'MEMORY_FOOTPRINT':\n\t      items = sorted(items, key=lambda x: x.memory)[::-1]\n\t    to_unload = items[0].name\n\t    if to_unload == model_name and len(items) > 1:\n\t      to_unload = items[1].name\n\t    self.unload(to_unload, config.mm_unload_order_policy)\n\t    return to_unload\n", "RAM = MemoryManager(get_system_ram_info, config.mm_ram_cached_model_count_limit)\n\tVRAM = MemoryManager(get_vram_info, config.mm_vram_cached_model_count_limit) if GPU_AVAILABLE else False\n\tdef mload(*args, gpu=False, **kwargs):\n\t  if 'gpu' in kwargs and kwargs['gpu'] and GPU_AVAILABLE:\n\t    return VRAM.wrap(*args, **kwargs)\n\t  else:\n\t    return RAM.wrap(*args, **kwargs)"]}
