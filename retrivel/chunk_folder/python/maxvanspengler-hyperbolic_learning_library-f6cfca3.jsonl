{"filename": "tests/test_manifold_tensor.py", "chunked_list": ["import pytest\n\timport torch\n\tfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\tfrom hypll.tensors import ManifoldTensor\n\t@pytest.fixture\n\tdef manifold_tensor() -> ManifoldTensor:\n\t    return ManifoldTensor(\n\t        data=[\n\t            [1.0, 2.0],\n\t            [3.0, 4.0],\n", "        ],\n\t        manifold=PoincareBall(c=Curvature()),\n\t        man_dim=-1,\n\t        requires_grad=True,\n\t    )\n\tdef test_attributes(manifold_tensor: ManifoldTensor):\n\t    # Check if the standard attributes are set correctly\n\t    # TODO: fix this once __eq__ has been implemented on manifolds\n\t    assert isinstance(manifold_tensor.manifold, PoincareBall)\n\t    assert manifold_tensor.man_dim == 1\n", "    # Check if non-callable attributes are taken from tensor attribute\n\t    assert manifold_tensor.is_cpu\n\t@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"requires cuda\")\n\tdef test_device_methods(manifold_tensor: ManifoldTensor):\n\t    # Check if we can move the manifold tensor to the gpu while keeping it intact\n\t    manifold_tensor = manifold_tensor.cuda()\n\t    assert isinstance(manifold_tensor, ManifoldTensor)\n\t    assert manifold_tensor.is_cuda\n\t    assert isinstance(manifold_tensor.manifold, PoincareBall)\n\t    assert manifold_tensor.man_dim == 1\n", "    # And move it back to the cpu\n\t    manifold_tensor = manifold_tensor.cpu()\n\t    assert isinstance(manifold_tensor, ManifoldTensor)\n\t    assert manifold_tensor.is_cpu\n\t    assert isinstance(manifold_tensor.manifold, PoincareBall)\n\t    assert manifold_tensor.man_dim == 1\n\tdef test_slicing(manifold_tensor: ManifoldTensor):\n\t    # First test slicing with the usual numpy slicing\n\t    manifold_tensor = ManifoldTensor(\n\t        data=torch.ones(2, 3, 4, 5, 6, 7, 8, 9),\n", "        manifold=PoincareBall(Curvature()),\n\t        man_dim=-2,\n\t    )\n\t    sliced_tensor = manifold_tensor[1, None, [0, 2], ..., None, 2:5, :, 1]\n\t    # Explanation of the output size: the dimension of size 2 dissappears because of the integer\n\t    # index. Then, a dim of size 1 is added by None. The size 3 dimension reduces to 2 because\n\t    # of the list of indices. The Ellipsis skips the dimensions of sizes 4, 5 and 6. Then another\n\t    # None leads to an insertion of a dimension of size 1. The dimension with size 7 is reduced\n\t    # to 3 because of the 2:5 slice. The manifold dimension of size 8 is left alone and the last\n\t    # dimension is removed because of an integer index.\n", "    assert list(sliced_tensor.size()) == [1, 2, 4, 5, 6, 1, 3, 8]\n\t    assert sliced_tensor.man_dim == 7\n\t    # Now we try to slice into the manifold dimension, which should raise an error\n\t    with pytest.raises(ValueError):\n\t        manifold_tensor[1, None, [0, 2], ..., None, 2:5, 5, 1]\n\t    # Next, we try long tensor indexing, which is used in embeddings\n\t    embedding_manifold_tensor = ManifoldTensor(\n\t        data=torch.ones(10, 3),\n\t        manifold=PoincareBall(Curvature()),\n\t        man_dim=-1,\n", "    )\n\t    indices = torch.Tensor(\n\t        [\n\t            [1, 2],\n\t            [3, 4],\n\t        ]\n\t    ).long()\n\t    embedding_selection = embedding_manifold_tensor[indices]\n\t    assert list(embedding_selection.size()) == [2, 2, 3]\n\t    assert embedding_selection.man_dim == 2\n", "    # This should fail if the man_dim is 0 on the embedding tensor though\n\t    embedding_manifold_tensor = ManifoldTensor(\n\t        data=torch.ones(10, 3),\n\t        manifold=PoincareBall(Curvature()),\n\t        man_dim=0,\n\t    )\n\t    with pytest.raises(ValueError):\n\t        embedding_manifold_tensor[indices]\n\t    # Lastly, embeddings with more dimensions should work too\n\t    embedding_manifold_tensor = ManifoldTensor(\n", "        data=torch.ones(10, 3, 3, 3),\n\t        manifold=PoincareBall(Curvature()),\n\t        man_dim=2,\n\t    )\n\t    embedding_selection = embedding_manifold_tensor[indices]\n\t    assert list(embedding_selection.size()) == [2, 2, 3, 3, 3]\n\t    assert embedding_selection.man_dim == 3\n\tdef test_torch_ops(manifold_tensor: ManifoldTensor):\n\t    # We want torch functons to raise an error\n\t    with pytest.raises(TypeError):\n", "        torch.norm(manifold_tensor)\n\t    # Same for torch.Tensor methods (callable attributes)\n\t    with pytest.raises(AttributeError):\n\t        manifold_tensor.mean()\n"]}
{"filename": "tests/manifolds/poincare_ball/test_curvature.py", "chunked_list": ["from pytest_mock import MockerFixture\n\tfrom hypll.manifolds.poincare_ball import Curvature\n\tdef test_curvature(mocker: MockerFixture) -> None:\n\t    mocked_constraining_strategy = mocker.MagicMock()\n\t    mocked_constraining_strategy.return_value = 1.33  # dummy value\n\t    curvature = Curvature(value=1.0, constraining_strategy=mocked_constraining_strategy)\n\t    assert curvature() == 1.33\n\t    mocked_constraining_strategy.assert_called_once_with(1.0)\n"]}
{"filename": "tests/manifolds/poincare_ball/test_poincare_ball.py", "chunked_list": ["import torch\n\tfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\tfrom hypll.tensors import ManifoldTensor\n\tdef test_flatten__man_dim_equals_start_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = PoincareBall(c=Curvature())\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=1,\n", "        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n\t    assert flattened.shape == (2, 8)\n\t    assert flattened.man_dim == 1\n\tdef test_flatten__man_dim_equals_start_dim__set_end_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = PoincareBall(c=Curvature())\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n", "        manifold=manifold,\n\t        man_dim=1,\n\t        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=2)\n\t    assert flattened.shape == (2, 4, 2)\n\t    assert flattened.man_dim == 1\n\tdef test_flatten__man_dim_larger_start_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = PoincareBall(c=Curvature())\n", "    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=2,\n\t        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n\t    assert flattened.shape == (2, 8)\n\t    assert flattened.man_dim == 1\n\tdef test_flatten__man_dim_larger_start_dim__set_end_dim() -> None:\n", "    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = PoincareBall(c=Curvature())\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=2,\n\t        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=2)\n\t    assert flattened.shape == (2, 4, 2)\n", "    assert flattened.man_dim == 1\n\tdef test_flatten__man_dim_smaller_start_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = PoincareBall(c=Curvature())\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=0,\n\t        requires_grad=True,\n\t    )\n", "    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n\t    assert flattened.shape == (2, 8)\n\t    assert flattened.man_dim == 0\n\tdef test_flatten__man_dim_larger_end_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = PoincareBall(c=Curvature())\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=2,\n", "        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=0, end_dim=1)\n\t    assert flattened.shape == (4, 2, 2)\n\t    assert flattened.man_dim == 1\n"]}
{"filename": "tests/manifolds/euclidean/test_euclidean.py", "chunked_list": ["import torch\n\tfrom hypll.manifolds.euclidean import Euclidean\n\tfrom hypll.tensors import ManifoldTensor\n\tdef test_flatten__man_dim_equals_start_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = Euclidean()\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=1,\n", "        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n\t    assert flattened.shape == (2, 8)\n\t    assert flattened.man_dim == 1\n\tdef test_flatten__man_dim_equals_start_dim__set_end_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = Euclidean()\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n", "        manifold=manifold,\n\t        man_dim=1,\n\t        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=2)\n\t    assert flattened.shape == (2, 4, 2)\n\t    assert flattened.man_dim == 1\n\tdef test_flatten__man_dim_larger_start_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = Euclidean()\n", "    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=2,\n\t        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n\t    assert flattened.shape == (2, 8)\n\t    assert flattened.man_dim == 1\n\tdef test_flatten__man_dim_larger_start_dim__set_end_dim() -> None:\n", "    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = Euclidean()\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=2,\n\t        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=2)\n\t    assert flattened.shape == (2, 4, 2)\n", "    assert flattened.man_dim == 1\n\tdef test_flatten__man_dim_smaller_start_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = Euclidean()\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=0,\n\t        requires_grad=True,\n\t    )\n", "    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n\t    assert flattened.shape == (2, 8)\n\t    assert flattened.man_dim == 0\n\tdef test_flatten__man_dim_larger_end_dim() -> None:\n\t    tensor = torch.randn(2, 2, 2, 2)\n\t    manifold = Euclidean()\n\t    manifold_tensor = ManifoldTensor(\n\t        data=tensor,\n\t        manifold=manifold,\n\t        man_dim=2,\n", "        requires_grad=True,\n\t    )\n\t    flattened = manifold.flatten(manifold_tensor, start_dim=0, end_dim=1)\n\t    assert flattened.shape == (4, 2, 2)\n\t    assert flattened.man_dim == 1\n"]}
{"filename": "tests/nn/test_change_manifold.py", "chunked_list": ["import torch\n\tfrom hypll.manifolds.euclidean import Euclidean\n\tfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\tfrom hypll.nn import ChangeManifold\n\tfrom hypll.tensors import ManifoldTensor, TangentTensor\n\tdef test_change_manifold__euclidean_to_euclidean() -> None:\n\t    # Define inputs.\n\t    manifold = Euclidean()\n\t    inputs = ManifoldTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n\t    # Apply change manifold.\n", "    change_manifold = ChangeManifold(target_manifold=Euclidean())\n\t    outputs = change_manifold(inputs)\n\t    # Assert outputs are correct.\n\t    assert isinstance(outputs, ManifoldTensor)\n\t    assert outputs.shape == inputs.shape\n\t    assert change_manifold.target_manifold == outputs.manifold\n\t    assert outputs.man_dim == 1\n\t    assert torch.allclose(inputs.tensor, outputs.tensor)\n\tdef test_change_manifold__euclidean_to_poincare_ball() -> None:\n\t    # Define inputs.\n", "    manifold = Euclidean()\n\t    inputs = ManifoldTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n\t    # Apply change manifold.\n\t    change_manifold = ChangeManifold(\n\t        target_manifold=PoincareBall(c=Curvature()),\n\t    )\n\t    outputs = change_manifold(inputs)\n\t    # Assert outputs are correct.\n\t    assert isinstance(outputs, ManifoldTensor)\n\t    assert outputs.shape == inputs.shape\n", "    assert change_manifold.target_manifold == outputs.manifold\n\t    assert outputs.man_dim == 1\n\tdef test_change_manifold__poincare_ball_to_euclidean() -> None:\n\t    # Define inputs.\n\t    manifold = PoincareBall(c=Curvature(0.1))\n\t    tangents = TangentTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n\t    inputs = manifold.expmap(tangents)\n\t    # Apply change manifold.\n\t    change_manifold = ChangeManifold(target_manifold=Euclidean())\n\t    outputs = change_manifold(inputs)\n", "    # Assert outputs are correct.\n\t    assert isinstance(outputs, ManifoldTensor)\n\t    assert outputs.shape == inputs.shape\n\t    assert change_manifold.target_manifold == outputs.manifold\n\t    assert outputs.man_dim == 1\n\tdef test_change_manifold__poincare_ball_to_poincare_ball() -> None:\n\t    # Define inputs.\n\t    manifold = PoincareBall(c=Curvature(0.1))\n\t    tangents = TangentTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n\t    inputs = manifold.expmap(tangents)\n", "    # Apply change manifold.\n\t    change_manifold = ChangeManifold(\n\t        target_manifold=PoincareBall(c=Curvature(1.0)),\n\t    )\n\t    outputs = change_manifold(inputs)\n\t    # Assert outputs are correct.\n\t    assert isinstance(outputs, ManifoldTensor)\n\t    assert outputs.shape == inputs.shape\n\t    assert change_manifold.target_manifold == outputs.manifold\n\t    assert outputs.man_dim == 1\n"]}
{"filename": "tests/nn/test_convolution.py", "chunked_list": ["import pytest\n\tfrom hypll.nn.modules import convolution\n\tdef test__output_side_length() -> None:\n\t    assert (\n\t        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=1, padding=0)\n\t        == 30\n\t    )\n\t    assert (\n\t        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=2, padding=0)\n\t        == 15\n", "    )\n\tdef test__output_side_length__padding() -> None:\n\t    assert (\n\t        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=1, padding=1)\n\t        == 32\n\t    )\n\t    assert (\n\t        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=2, padding=1)\n\t        == 16\n\t    )\n", "    # Reproduce https://github.com/maxvanspengler/hyperbolic_learning_library/issues/33\n\t    assert (\n\t        convolution._output_side_length(input_side_length=224, kernel_size=11, stride=4, padding=2)\n\t        == 55\n\t    )\n\tdef test__output_side_length__raises() -> None:\n\t    with pytest.raises(RuntimeError) as e:\n\t        convolution._output_side_length(input_side_length=32, kernel_size=33, stride=1, padding=0)\n\t    with pytest.raises(RuntimeError) as e:\n\t        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=33, padding=0)\n"]}
{"filename": "tests/nn/test_flatten.py", "chunked_list": ["from pytest_mock import MockerFixture\n\tfrom hypll.nn import HFlatten\n\tdef test_hflatten(mocker: MockerFixture) -> None:\n\t    hflatten = HFlatten(start_dim=1, end_dim=-1)\n\t    mocked_manifold_tensor = mocker.MagicMock()\n\t    flattened = hflatten(mocked_manifold_tensor)\n\t    mocked_manifold_tensor.flatten.assert_called_once_with(start_dim=1, end_dim=-1)\n"]}
{"filename": "hypll/__init__.py", "chunked_list": []}
{"filename": "hypll/utils/layer_utils.py", "chunked_list": ["from typing import Callable\n\tfrom torch.nn import Module\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor\n\tdef check_if_manifolds_match(layer: Module, input: ManifoldTensor) -> None:\n\t    if layer.manifold != input.manifold:\n\t        raise ValueError(\n\t            f\"Manifold of {layer.__class__.__name__} layer is {layer.manifold} \"\n\t            f\"but input has manifold {input.manifold}\"\n\t        )\n", "def check_if_man_dims_match(layer: Module, man_dim: int, input: ManifoldTensor) -> None:\n\t    if man_dim < 0:\n\t        new_man_dim = input.dim() + man_dim\n\t    else:\n\t        new_man_dim = man_dim\n\t    if input.man_dim != new_man_dim:\n\t        raise ValueError(\n\t            f\"Layer of type {layer.__class__.__name__} expects the manifold dimension to be {man_dim}, \"\n\t            f\"but input has manifold dimension {input.man_dim}\"\n\t        )\n", "def op_in_tangent_space(op: Callable, manifold: Manifold, input: ManifoldTensor) -> ManifoldTensor:\n\t    input = manifold.logmap(x=None, y=input)\n\t    input.tensor = op(input.tensor)\n\t    return manifold.expmap(input)\n"]}
{"filename": "hypll/utils/tensor_utils.py", "chunked_list": ["from torch import broadcast_shapes, equal\n\tfrom hypll.tensors import ManifoldTensor, TangentTensor\n\tdef check_dims_with_broadcasting(*args: ManifoldTensor) -> int:\n\t    # Check if shapes can be broadcasted together\n\t    shapes = [a.size() for a in args]\n\t    try:\n\t        broadcasted_shape = broadcast_shapes(*shapes)\n\t    except RuntimeError:\n\t        raise ValueError(f\"Shapes of inputs were {shapes} and cannot be broadcasted together\")\n\t    # Find the manifold dimensions after broadcasting\n", "    max_dim = len(broadcasted_shape)\n\t    man_dims = []\n\t    for a in args:\n\t        if isinstance(a, ManifoldTensor):\n\t            man_dims.append(a.man_dim + (max_dim - a.dim()))\n\t        elif isinstance(a, TangentTensor):\n\t            man_dims.append(a.broadcasted_man_dim + (max_dim - a.dim()))\n\t    for md in man_dims[1:]:\n\t        if man_dims[0] != md:\n\t            raise ValueError(\"Manifold dimensions of inputs after broadcasting do not match.\")\n", "    return man_dims[0]\n\tdef check_tangent_tensor_positions(*args: TangentTensor) -> None:\n\t    manifold_points = [a.manifold_points for a in args]\n\t    if any([mp is None for mp in manifold_points]):\n\t        if not all(mp is None for mp in manifold_points):\n\t            raise ValueError(f\"Some but not all tangent tensors are located at the origin\")\n\t    else:\n\t        broadcasted_shape = broadcast_shapes(*[a.size() for a in args])\n\t        broadcasted_manifold_points = [\n\t            mp.tensor.broadcast_to(broadcasted_shape) for mp in manifold_points\n", "        ]\n\t        for bmp in broadcasted_manifold_points[1:]:\n\t            if not equal(broadcasted_manifold_points[0], bmp):\n\t                raise ValueError(\n\t                    f\"Tangent tensors are positioned at the different points on the manifold\"\n\t                )\n"]}
{"filename": "hypll/utils/__init__.py", "chunked_list": []}
{"filename": "hypll/utils/math.py", "chunked_list": ["from math import exp, lgamma\n\tdef beta_func(a: int, b: int) -> float:\n\t    log_beta = lgamma(a) + lgamma(b) - lgamma(a + b)\n\t    return exp(log_beta)\n"]}
{"filename": "hypll/optim/sgd.py", "chunked_list": ["from collections.abc import Iterable\n\tfrom typing import Union\n\tfrom torch import no_grad\n\tfrom torch.optim import Optimizer\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.manifolds.euclidean import Euclidean\n\tfrom hypll.tensors import ManifoldParameter, ManifoldTensor\n\tclass RiemannianSGD(Optimizer):\n\t    def __init__(\n\t        self,\n", "        params: Iterable[Union[ManifoldParameter, ManifoldTensor]],\n\t        lr: float,\n\t        momentum: float = 0,\n\t        dampening: float = 0,\n\t        weight_decay: float = 0,\n\t        nesterov: bool = False,\n\t    ) -> None:\n\t        if lr < 0.0:\n\t            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n\t        if momentum < 0.0:\n", "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n\t        if weight_decay < 0.0:\n\t            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n\t        if nesterov and (momentum <= 0 or dampening != 0):\n\t            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n\t        defaults = dict(\n\t            lr=lr,\n\t            momentum=momentum,\n\t            dampening=dampening,\n\t            weight_decay=weight_decay,\n", "            nesterov=nesterov,\n\t        )\n\t        super(RiemannianSGD, self).__init__(params=params, defaults=defaults)\n\t    def step(self) -> None:\n\t        with no_grad():\n\t            for group in self.param_groups:\n\t                lr = group[\"lr\"]\n\t                momentum = group[\"momentum\"]\n\t                dampening = group[\"dampening\"]\n\t                weight_decay = group[\"weight_decay\"]\n", "                nestorov = group[\"nesterov\"]\n\t                for param in group[\"params\"]:\n\t                    if isinstance(param, ManifoldParameter):\n\t                        grad = param.grad\n\t                        if grad is None:\n\t                            continue\n\t                        grad = ManifoldTensor(\n\t                            data=grad, manifold=Euclidean(), man_dim=param.man_dim\n\t                        )\n\t                        state = self.state[param]\n", "                        if len(state) == 0:\n\t                            if momentum > 0:\n\t                                state[\"momentum_buffer\"] = ManifoldTensor(\n\t                                    data=grad.tensor.clone(),\n\t                                    manifold=Euclidean(),\n\t                                    man_dim=param.man_dim,\n\t                                )\n\t                        manifold: Manifold = param.manifold\n\t                        grad.tensor.add_(\n\t                            param.tensor, alpha=weight_decay\n", "                        )  # TODO: check if this makes sense\n\t                        grad = manifold.euc_to_tangent(x=param, u=grad)\n\t                        if momentum > 0:\n\t                            momentum_buffer = manifold.euc_to_tangent(\n\t                                x=param, u=state[\"momentum_buffer\"]\n\t                            )\n\t                            momentum_buffer.tensor.mul_(momentum).add_(\n\t                                grad.tensor, alpha=1 - dampening\n\t                            )\n\t                            if nestorov:\n", "                                grad.tensor.add_(momentum_buffer.tensor, alpha=momentum)\n\t                            else:\n\t                                grad = momentum_buffer\n\t                            grad.tensor = -lr * grad.tensor\n\t                            new_param = manifold.expmap(grad)\n\t                            momentum_buffer = manifold.transp(v=momentum_buffer, y=new_param)\n\t                            # momentum_buffer.tensor.copy_(new_momentum_buffer.tensor)\n\t                            param.tensor.copy_(new_param.tensor)\n\t                        else:\n\t                            grad.tensor = -lr * grad.tensor\n", "                            new_param = manifold.expmap(v=grad)\n\t                            param.tensor.copy_(new_param.tensor)\n\t                    else:\n\t                        grad = param.grad\n\t                        if grad is None:\n\t                            continue\n\t                        state = self.state[param]\n\t                        if len(state) == 0:\n\t                            if momentum > 0:\n\t                                state[\"momentum_buffer\"] = grad.clone()\n", "                        grad.add_(param, alpha=weight_decay)\n\t                        if momentum > 0:\n\t                            momentum_buffer = state[\"momentum_buffer\"]\n\t                            momentum_buffer.mul_(momentum).add_(grad, alpha=1 - dampening)\n\t                            if nestorov:\n\t                                grad = grad.add_(momentum_buffer, alpha=momentum)\n\t                            else:\n\t                                grad = momentum_buffer\n\t                            new_param = param - lr * grad\n\t                            new_momentum_buffer = momentum_buffer\n", "                            momentum_buffer.copy_(new_momentum_buffer)\n\t                            param.copy_(new_param)\n\t                        else:\n\t                            new_param = param - lr * grad\n\t                            param.copy_(new_param)\n"]}
{"filename": "hypll/optim/__init__.py", "chunked_list": ["from .adam import RiemannianAdam\n\tfrom .sgd import RiemannianSGD\n"]}
{"filename": "hypll/optim/adam.py", "chunked_list": ["from collections.abc import Iterable\n\tfrom typing import Union\n\tfrom torch import max, no_grad, zeros_like\n\tfrom torch.optim import Optimizer\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.manifolds.euclidean import Euclidean\n\tfrom hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n\tclass RiemannianAdam(Optimizer):\n\t    def __init__(\n\t        self,\n", "        params: Iterable[Union[ManifoldParameter, ManifoldTensor]],\n\t        lr: float,\n\t        betas: tuple[float, float] = (0.9, 0.999),\n\t        eps: float = 1e-8,\n\t        weight_decay: float = 0,\n\t        amsgrad: bool = False,\n\t    ) -> None:\n\t        if lr < 0.0:\n\t            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n\t        if eps < 0.0:\n", "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n\t        if not 0.0 <= betas[0] < 1.0:\n\t            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n\t        if not 0.0 <= betas[1] < 1.0:\n\t            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n\t        if weight_decay < 0.0:\n\t            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n\t        defaults = dict(\n\t            lr=lr,\n\t            betas=betas,\n", "            eps=eps,\n\t            weight_decay=weight_decay,\n\t            amsgrad=amsgrad,\n\t        )\n\t        super(RiemannianAdam, self).__init__(params=params, defaults=defaults)\n\t    def step(self) -> None:\n\t        # TODO: refactor this and add some comments, because it's currently unreadable\n\t        with no_grad():\n\t            for group in self.param_groups:\n\t                betas = group[\"betas\"]\n", "                weight_decay = group[\"weight_decay\"]\n\t                eps = group[\"eps\"]\n\t                lr = group[\"lr\"]\n\t                amsgrad = group[\"amsgrad\"]\n\t                for param in group[\"params\"]:\n\t                    if isinstance(param, ManifoldParameter):\n\t                        manifold: Manifold = param.manifold\n\t                        grad = param.grad\n\t                        if grad is None:\n\t                            continue\n", "                        grad = ManifoldTensor(\n\t                            data=grad, manifold=Euclidean(), man_dim=param.man_dim\n\t                        )\n\t                        state = self.state[param]\n\t                        if len(state) == 0:\n\t                            state[\"step\"] = 0\n\t                            state[\"exp_avg\"] = zeros_like(param.tensor)\n\t                            state[\"exp_avg_sq\"] = zeros_like(param.tensor)\n\t                            if amsgrad:\n\t                                state[\"max_exp_avg_sq\"] = zeros_like(param.tensor)\n", "                        state[\"step\"] += 1\n\t                        exp_avg = state[\"exp_avg\"]\n\t                        exp_avg_sq = state[\"exp_avg_sq\"]\n\t                        # TODO: check if this next line makes sense, because I don't think so\n\t                        grad.tensor.add_(param.tensor, alpha=weight_decay)\n\t                        grad = manifold.euc_to_tangent(x=param, u=grad)\n\t                        exp_avg.mul_(betas[0]).add_(grad.tensor, alpha=1 - betas[0])\n\t                        exp_avg_sq.mul_(betas[1]).add_(\n\t                            manifold.inner(u=grad, v=grad, keepdim=True, safe_mode=False),\n\t                            alpha=1 - betas[1],\n", "                        )\n\t                        bias_correction1 = 1 - betas[0] ** state[\"step\"]\n\t                        bias_correction2 = 1 - betas[1] ** state[\"step\"]\n\t                        if amsgrad:\n\t                            max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n\t                            max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n\t                            denom = max_exp_avg_sq.div(bias_correction2).sqrt_()\n\t                        else:\n\t                            denom = exp_avg_sq.div(bias_correction2).sqrt_()\n\t                        direction = -lr * exp_avg.div(bias_correction1) / denom.add_(eps)\n", "                        direction = TangentTensor(\n\t                            data=direction,\n\t                            manifold_points=param,\n\t                            manifold=manifold,\n\t                            man_dim=param.man_dim,\n\t                        )\n\t                        exp_avg_man = TangentTensor(\n\t                            data=exp_avg,\n\t                            manifold_points=param,\n\t                            manifold=manifold,\n", "                            man_dim=param.man_dim,\n\t                        )\n\t                        new_param = manifold.expmap(direction)\n\t                        exp_avg_new = manifold.transp(v=exp_avg_man, y=new_param)\n\t                        param.tensor.copy_(new_param.tensor)\n\t                        exp_avg.copy_(exp_avg_new.tensor)\n\t                    else:\n\t                        grad = param.grad\n\t                        if grad is None:\n\t                            continue\n", "                        state = self.state[param]\n\t                        if len(state) == 0:\n\t                            state[\"step\"] = 0\n\t                            state[\"exp_avg\"] = zeros_like(param)\n\t                            state[\"exp_avg_sq\"] = zeros_like(param)\n\t                            if amsgrad:\n\t                                state[\"max_exp_avg_sq\"] = zeros_like(param)\n\t                        state[\"step\"] += 1\n\t                        exp_avg = state[\"exp_avg\"]\n\t                        exp_avg_sq = state[\"exp_avg_sq\"]\n", "                        grad.add_(param, alpha=weight_decay)\n\t                        exp_avg.mul_(betas[0]).add_(grad, alpha=1 - betas[0])\n\t                        exp_avg_sq.mul_(betas[1]).add_(\n\t                            grad.square().sum(dim=-1, keepdim=True), alpha=1 - betas[1]\n\t                        )\n\t                        bias_correction1 = 1 - betas[0] ** state[\"step\"]\n\t                        bias_correction2 = 1 - betas[1] ** state[\"step\"]\n\t                        if amsgrad:\n\t                            max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n\t                            max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n", "                            denom = max_exp_avg_sq.div(bias_correction2).sqrt_()\n\t                        else:\n\t                            denom = exp_avg_sq.div(bias_correction2).sqrt_()\n\t                        direction = exp_avg.div(bias_correction1) / denom.add_(eps)\n\t                        new_param = param - lr * direction\n\t                        exp_avg_new = exp_avg\n\t                        param.copy_(new_param)\n\t                        exp_avg.copy_(exp_avg_new)\n"]}
{"filename": "hypll/tensors/tangent_tensor.py", "chunked_list": ["from typing import Any, Optional\n\tfrom torch import Tensor, broadcast_shapes, tensor\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors.manifold_tensor import ManifoldTensor\n\tclass TangentTensor:\n\t    def __init__(\n\t        self,\n\t        data,\n\t        manifold_points: Optional[ManifoldTensor] = None,\n\t        manifold: Optional[Manifold] = None,\n", "        man_dim: int = -1,\n\t        requires_grad: bool = False,\n\t    ) -> None:\n\t        # Create tangent vector tensor with correct device and dtype\n\t        if isinstance(data, Tensor):\n\t            self.tensor = data\n\t        else:\n\t            self.tensor = tensor(data, requires_grad=requires_grad)\n\t        # Store manifold dimension as a nonnegative integer\n\t        if man_dim >= 0:\n", "            self.man_dim = man_dim\n\t        else:\n\t            self.man_dim = self.tensor.dim() + man_dim\n\t            if self.man_dim < 0:\n\t                raise ValueError(\n\t                    f\"Dimension out of range (expected to be in range of \"\n\t                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n\t                )\n\t        if manifold_points is not None:\n\t            # Check if the manifold points and tangent vectors are broadcastable together\n", "            try:\n\t                broadcasted_size = broadcast_shapes(self.tensor.size(), manifold_points.size())\n\t            except RuntimeError:\n\t                raise ValueError(\n\t                    f\"The shapes of the manifold points tensor {manifold_points.size()} and \"\n\t                    f\"the tangent vector tensor {self.tensor.size()} are not broadcastable \"\n\t                    f\"togther.\"\n\t                )\n\t            # Check if the manifold dimensions match after broadcasting\n\t            dim = len(broadcasted_size)\n", "            broadcast_man_dims = [\n\t                manifold_points.man_dim + dim - manifold_points.tensor.dim(),\n\t                self.man_dim + dim - self.tensor.dim(),\n\t            ]\n\t            if broadcast_man_dims[0] != broadcast_man_dims[1]:\n\t                raise ValueError(\n\t                    f\"After broadcasting the manifold points with the tangent vectors, the \"\n\t                    f\"manifold dimension computed from the manifold points should match the \"\n\t                    f\"manifold dimension computed from the supplied man_dim, but these are\"\n\t                    f\"{broadcast_man_dims}, respectively.\"\n", "                )\n\t        # Check if the supplied manifolds match\n\t        if manifold_points is not None and manifold is not None:\n\t            if manifold_points.manifold != manifold:\n\t                raise ValueError(\n\t                    f\"The manifold of the manifold_points and the provided manifold should match, \"\n\t                    f\"but are {manifold_points.manifold} and {manifold}, respectively.\"\n\t                )\n\t        self.manifold_points = manifold_points\n\t        self.manifold = manifold or manifold_points.manifold\n", "        self.man_dim = man_dim\n\t    def __getattr__(self, name: str) -> Any:\n\t        # TODO: go through https://pytorch.org/docs/stable/tensors.html and check which methods\n\t        # are relevant.\n\t        if hasattr(self.tensor, name):\n\t            torch_attribute = getattr(self.tensor, name)\n\t            if callable(torch_attribute):\n\t                raise AttributeError(\n\t                    f\"Attempting to apply the torch.Tensor method {name} on a TangentTensor.\"\n\t                    f\"Use TangentTensor.tensor.{name} or TangentTensor.manifold_points.tensor \"\n", "                    f\"instead.\"\n\t                )\n\t            else:\n\t                return torch_attribute\n\t        else:\n\t            raise AttributeError(\n\t                f\"Neither {self.__class__.__name__}, nor torch.Tensor has attribute {name}\"\n\t            )\n\t    def cuda(self, device=None):\n\t        new_tensor = self.tensor.cuda(device)\n", "        new_manifold_points = self.manifold_points.cuda(device)\n\t        return TangentTensor(\n\t            data=new_tensor,\n\t            manifold_points=new_manifold_points,\n\t            manifold=self.manifold,\n\t            man_dim=self.man_dim,\n\t        )\n\t    def cpu(self):\n\t        new_tensor = self.tensor.cpu()\n\t        new_manifold_points = self.manifold_points.cpu()\n", "        return TangentTensor(\n\t            data=new_tensor,\n\t            manifold_points=new_manifold_points,\n\t            manifold=self.manifold,\n\t            man_dim=self.man_dim,\n\t        )\n\t    def to(self, *args, **kwargs):\n\t        new_tensor = self.tensor.to(*args, **kwargs)\n\t        new_manifold_points = self.manifold_points(*args, **kwargs)\n\t        return TangentTensor(\n", "            data=new_tensor,\n\t            manifold_points=new_manifold_points,\n\t            manifold=self.manifold,\n\t            man_dim=self.man_dim,\n\t        )\n\t    def size(self, dim: Optional[int] = None):\n\t        if self.manifold_points is None:\n\t            manifold_points_size = None\n\t        manifold_points_size = (\n\t            self.manifold_points.size() if self.manifold_points is not None else ()\n", "        )\n\t        broadcasted_size = broadcast_shapes(self.tensor.size(), manifold_points_size)\n\t        if dim is None:\n\t            return broadcasted_size\n\t        else:\n\t            return broadcasted_size[dim]\n\t    @property\n\t    def broadcasted_man_dim(self):\n\t        return self.man_dim + self.dim() - self.tensor.dim()\n\t    def dim(self):\n", "        return len(self.size())\n\t    @classmethod\n\t    def __torch_function__(cls, func, types, args=(), kwargs=None):\n\t        # TODO: check if there are torch functions that should be allowed\n\t        raise TypeError(\n\t            f\"Attempting to apply the torch function {func} on a TangentTensor.\"\n\t            f\"Use TangentTensor.tensor as argument to {func} instead.\"\n\t        )\n"]}
{"filename": "hypll/tensors/__init__.py", "chunked_list": ["from .manifold_parameter import ManifoldParameter\n\tfrom .manifold_tensor import ManifoldTensor\n\tfrom .tangent_tensor import TangentTensor\n"]}
{"filename": "hypll/tensors/manifold_parameter.py", "chunked_list": ["from typing import Any\n\timport torch\n\tfrom torch import Tensor, tensor\n\tfrom torch.nn import Parameter\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors.manifold_tensor import ManifoldTensor\n\tclass ManifoldParameter(ManifoldTensor, Parameter):\n\t    _allowed_methods = [\n\t        torch._has_compatible_shallow_copy_type,  # Required for torch.nn.Parameter\n\t        torch.Tensor.copy_,  # Required to load ManifoldParameters state dicts\n", "    ]\n\t    def __new__(cls, data, manifold, man_dim, requires_grad=True):\n\t        return super(ManifoldTensor, cls).__new__(cls)\n\t    # TODO: Create a mixin class containing the methods for this class and for ManifoldTensor\n\t    # to avoid all the boilerplate stuff.\n\t    def __init__(\n\t        self, data, manifold: Manifold, man_dim: int = -1, requires_grad: bool = True\n\t    ) -> None:\n\t        super(ManifoldParameter, self).__init__(data=data, manifold=manifold)\n\t        if isinstance(data, Parameter):\n", "            self.tensor = data\n\t        elif isinstance(data, Tensor):\n\t            self.tensor = Parameter(data=data, requires_grad=requires_grad)\n\t        else:\n\t            self.tensor = Parameter(data=tensor(data), requires_grad=requires_grad)\n\t        self.manifold = manifold\n\t        if man_dim >= 0:\n\t            self.man_dim = man_dim\n\t        else:\n\t            self.man_dim = self.tensor.dim() + man_dim\n", "            if self.man_dim < 0:\n\t                raise ValueError(\n\t                    f\"Dimension out of range (expected to be in range of \"\n\t                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n\t                )\n\t    def __getattr__(self, name: str) -> Any:\n\t        # TODO: go through https://pytorch.org/docs/stable/tensors.html and check which methods\n\t        # are relevant.\n\t        if hasattr(self.tensor, name):\n\t            torch_attribute = getattr(self.tensor, name)\n", "            if callable(torch_attribute):\n\t                raise AttributeError(\n\t                    f\"Attempting to apply the torch.nn.Parameter method {name} on a ManifoldParameter.\"\n\t                    f\"Use ManifoldTensor.tensor.{name} instead.\"\n\t                )\n\t            else:\n\t                return torch_attribute\n\t        else:\n\t            raise AttributeError(\n\t                f\"Neither {self.__class__.__name__}, nor torch.Tensor has attribute {name}\"\n", "            )\n\t    @classmethod\n\t    def __torch_function__(cls, func, types, args=(), kwargs=None):\n\t        if func.__class__.__name__ == \"method-wrapper\" or func in cls._allowed_methods:\n\t            args = [a.tensor if isinstance(a, ManifoldTensor) else a for a in args]\n\t            if kwargs is None:\n\t                kwargs = {}\n\t            kwargs = {k: (v.tensor if isinstance(v, ManifoldTensor) else v) for k, v in kwargs}\n\t            return func(*args, **kwargs)\n\t        # if func.__name__ == \"__get__\":\n", "        #     return func(args[0].tensor)\n\t        # TODO: check if there are torch functions that should be allowed\n\t        raise TypeError(\n\t            f\"Attempting to apply the torch function {func} on a ManifoldParameter. \"\n\t            f\"Use ManifoldParameter.tensor as argument to {func} instead.\"\n\t        )\n"]}
{"filename": "hypll/tensors/manifold_tensor.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import Optional\n\tfrom torch import Tensor, long, tensor\n\tfrom hypll.manifolds import Manifold\n\tclass ManifoldTensor:\n\t    \"\"\"Represents a tensor on a manifold.\n\t    Attributes:\n\t        tensor:\n\t            Torch tensor of points on the manifold.\n\t        manifold:\n", "            Manifold instance.\n\t        man_dim:\n\t            Dimension along which points are on the manifold.\n\t    \"\"\"\n\t    def __init__(\n\t        self, data: Tensor, manifold: Manifold, man_dim: int = -1, requires_grad: bool = False\n\t    ) -> None:\n\t        \"\"\"Creates an instance of ManifoldTensor.\n\t        Args:\n\t            data:\n", "                Torch tensor of points on the manifold.\n\t            manifold:\n\t                Manifold instance.\n\t            man_dim:\n\t                Dimension along which points are on the manifold. -1 by default.\n\t        TODO(Philipp, 05/23): Let's get rid of requires_grad if possible.\n\t        \"\"\"\n\t        self.tensor = data if isinstance(data, Tensor) else tensor(data, requires_grad=True)\n\t        self.manifold = manifold\n\t        if man_dim >= 0:\n", "            self.man_dim = man_dim\n\t        else:\n\t            self.man_dim = self.tensor.dim() + man_dim\n\t            if self.man_dim < 0:\n\t                raise ValueError(\n\t                    f\"Dimension out of range (expected to be in range of \"\n\t                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n\t                )\n\t    def __getitem__(self, *args):\n\t        # Catch some undefined behaviour by checking if args is a single element\n", "        if len(args) != 1:\n\t            raise ValueError(\n\t                f\"No support for slicing with these arguments. If you think there should be \"\n\t                f\"support, please consider opening a issue on GitHub describing your case.\"\n\t            )\n\t        # Deal with the case where the argument is a long tensor\n\t        if isinstance(args[0], Tensor) and args[0].dtype == long:\n\t            if self.man_dim == 0:\n\t                raise ValueError(\n\t                    f\"Long tensor indexing is only possible when the manifold dimension \"\n", "                    f\"is not 0, but the manifold dimension is {self.man_dim}\"\n\t                )\n\t            new_tensor = self.tensor.__getitem__(*args)\n\t            new_man_dim = self.man_dim + args[0].dim() - 1\n\t            return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n\t        # Convert the args to a list and replace Ellipsis by the correct number of full slices\n\t        arg_list = list(args[0])\n\t        if Ellipsis in arg_list:\n\t            ell_id = arg_list.index(Ellipsis)\n\t            colon_repeats = self.dim() - sum(1 for a in arg_list if a is not None) + 1\n", "            arg_list[ell_id : ell_id + 1] = colon_repeats * [slice(None, None, None)]\n\t        new_tensor = self.tensor.__getitem__(*args)\n\t        output_man_dim = self.man_dim\n\t        counter = self.man_dim + 1\n\t        # Compute output manifold dimension\n\t        for arg in arg_list:\n\t            # None values add a dimension\n\t            if arg is None:\n\t                output_man_dim += 1\n\t                continue\n", "            # Integers remove a dimension\n\t            elif isinstance(arg, int):\n\t                output_man_dim -= 1\n\t                counter -= 1\n\t            # Other values leave the dimension intact\n\t            else:\n\t                counter -= 1\n\t            # When the counter hits 0 and the next term isn't None, we hit the man_dim term\n\t            if counter == 0:\n\t                if isinstance(arg, int) or isinstance(arg, list):\n", "                    raise ValueError(\n\t                        f\"Attempting to slice into the manifold dimension, but this is not a \"\n\t                        \"valid operation\"\n\t                    )\n\t                # If we get past the man_dim term, the output man_dim doesn't change anymore\n\t                break\n\t        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=output_man_dim)\n\t    def cpu(self) -> ManifoldTensor:\n\t        \"\"\"Returns a copy of this object with self.tensor in CPU memory.\"\"\"\n\t        new_tensor = self.tensor.cpu()\n", "        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\t    def cuda(self, device=None) -> ManifoldTensor:\n\t        \"\"\"Returns a copy of this object with self.tensor in CUDA memory.\"\"\"\n\t        new_tensor = self.tensor.cuda(device)\n\t        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\t    def dim(self) -> int:\n\t        \"\"\"Returns the number of dimensions of self.tensor.\"\"\"\n\t        return self.tensor.dim()\n\t    def detach(self) -> ManifoldTensor:\n\t        \"\"\"Returns a new Tensor, detached from the current graph.\"\"\"\n", "        detached = self.tensor.detach()\n\t        return ManifoldTensor(data=detached, manifold=self.manifold, man_dim=self.man_dim)\n\t    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> ManifoldTensor:\n\t        \"\"\"Flattens tensor by reshaping it. If start_dim or end_dim are passed,\n\t        only dimensions starting with start_dim and ending with end_dim are flattend.\n\t        \"\"\"\n\t        return self.manifold.flatten(self, start_dim=start_dim, end_dim=end_dim)\n\t    @property\n\t    def is_cpu(self):\n\t        return self.tensor.is_cpu\n", "    @property\n\t    def is_cuda(self):\n\t        return self.tensor.is_cuda\n\t    def is_floating_point(self) -> bool:\n\t        \"\"\"Returns true if the tensor is of dtype float.\"\"\"\n\t        return self.tensor.is_floating_point()\n\t    def project(self) -> ManifoldTensor:\n\t        \"\"\"Projects the tensor to the manifold.\"\"\"\n\t        return self.manifold.project(x=self)\n\t    @property\n", "    def shape(self):\n\t        \"\"\"Alias for size().\"\"\"\n\t        return self.size()\n\t    def size(self, dim: Optional[int] = None):\n\t        \"\"\"Returns the size of self.tensor.\"\"\"\n\t        if dim is None:\n\t            return self.tensor.size()\n\t        else:\n\t            return self.tensor.size(dim)\n\t    def squeeze(self, dim=None):\n", "        \"\"\"Returns a squeezed version of the manifold tensor.\"\"\"\n\t        if dim == self.man_dim or (dim is None and self.size(self.man_dim) == 1):\n\t            raise ValueError(\"Attempting to squeeze the manifold dimension\")\n\t        if dim is None:\n\t            new_tensor = self.tensor.squeeze()\n\t            new_man_dim = self.man_dim - sum(self.size(d) == 1 for d in range(self.man_dim))\n\t        else:\n\t            new_tensor = self.tensor.squeeze(dim=dim)\n\t            new_man_dim = self.man_dim - (1 if dim < self.man_dim else 0)\n\t        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n", "    def to(self, *args, **kwargs) -> ManifoldTensor:\n\t        \"\"\"Returns a new tensor with the specified device and (optional) dtype.\"\"\"\n\t        new_tensor = self.tensor.to(*args, **kwargs)\n\t        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\t    def transpose(self, dim0: int, dim1: int) -> ManifoldTensor:\n\t        \"\"\"Returns a transposed version of the manifold tensor. The given dimensions\n\t        dim0 and dim1 are swapped.\n\t        \"\"\"\n\t        if self.man_dim == dim0:\n\t            new_man_dim = dim1\n", "        elif self.man_dim == dim1:\n\t            new_man_dim = dim0\n\t        new_tensor = self.tensor.transpose(dim0, dim1)\n\t        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n\t    @classmethod\n\t    def __torch_function__(cls, func, types, args=(), kwargs=None):\n\t        # TODO: check if there are torch functions that should be allowed\n\t        raise TypeError(\n\t            f\"Attempting to apply the torch function {func} on a ManifoldTensor. \"\n\t            f\"Use ManifoldTensor.tensor as argument to {func} instead.\"\n", "        )\n"]}
{"filename": "hypll/manifolds/__init__.py", "chunked_list": ["from .base import Manifold\n"]}
{"filename": "hypll/manifolds/poincare_ball/curvature.py", "chunked_list": ["from typing import Callable\n\timport torch\n\tfrom torch import Tensor, as_tensor\n\tfrom torch.nn import Module\n\tfrom torch.nn.functional import softplus\n\tfrom torch.nn.parameter import Parameter\n\tclass Curvature(Module):\n\t    \"\"\"Class representing curvature of a manifold.\n\t    Attributes:\n\t        value:\n", "            Learnable parameter indicating curvature of the manifold. The actual\n\t            curvature is calculated as constraining_strategy(value).\n\t        constraining_strategy:\n\t            Function applied to the curvature value in order to constrain the\n\t            curvature of the manifold. By default uses softplus to guarantee\n\t            positive curvature.\n\t        requires_grad:\n\t            If the curvature requires gradient. False by default.\n\t    \"\"\"\n\t    def __init__(\n", "        self,\n\t        value: float = 1.0,\n\t        constraining_strategy: Callable[[Tensor], Tensor] = softplus,\n\t        requires_grad: bool = False,\n\t    ):\n\t        super(Curvature, self).__init__()\n\t        self.value = Parameter(\n\t            data=as_tensor(value, dtype=torch.float32),\n\t            requires_grad=requires_grad,\n\t        )\n", "        self.constraining_strategy = constraining_strategy\n\t    def forward(self) -> Tensor:\n\t        \"\"\"Returns curvature calculated as constraining_strategy(value).\"\"\"\n\t        return self.constraining_strategy(self.value)\n"]}
{"filename": "hypll/manifolds/poincare_ball/manifold.py", "chunked_list": ["import functools\n\tfrom typing import Optional, Union\n\timport torch\n\tfrom torch import Tensor, empty, eye, no_grad\n\tfrom torch.nn import Parameter\n\tfrom torch.nn.common_types import _size_2_t\n\tfrom torch.nn.functional import softplus, unfold\n\tfrom torch.nn.init import normal_, zeros_\n\tfrom hypll.manifolds.base import Manifold\n\tfrom hypll.manifolds.euclidean import Euclidean\n", "from hypll.manifolds.poincare_ball.curvature import Curvature\n\tfrom hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n\tfrom hypll.utils.math import beta_func\n\tfrom hypll.utils.tensor_utils import (\n\t    check_dims_with_broadcasting,\n\t    check_tangent_tensor_positions,\n\t)\n\tfrom .math.diffgeom import (\n\t    dist,\n\t    euc_to_tangent,\n", "    expmap,\n\t    expmap0,\n\t    gyration,\n\t    inner,\n\t    logmap,\n\t    logmap0,\n\t    mobius_add,\n\t    project,\n\t    transp,\n\t)\n", "from .math.linalg import poincare_fully_connected, poincare_hyperplane_dists\n\tfrom .math.stats import frechet_mean, frechet_variance, midpoint\n\tclass PoincareBall(Manifold):\n\t    \"\"\"Class representing the Poincare ball model of hyperbolic space.\n\t    Implementation based on the geoopt implementation, but changed to use\n\t    hyperbolic torch functions.\n\t    Attributes:\n\t        c:\n\t            Curvature of the manifold.\n\t    \"\"\"\n", "    def __init__(self, c: Curvature):\n\t        \"\"\"Initializes an instance of PoincareBall manifold.\n\t        Examples:\n\t            >>> from hypll.manifolds.poincare_ball import PoincareBall, Curvature\n\t            >>> curvature = Curvature(value=1.0)\n\t            >>> manifold = Manifold(c=curvature)\n\t        \"\"\"\n\t        super(PoincareBall, self).__init__()\n\t        self.c = c\n\t    def mobius_add(self, x: ManifoldTensor, y: ManifoldTensor) -> ManifoldTensor:\n", "        dim = check_dims_with_broadcasting(x, y)\n\t        new_tensor = mobius_add(x=x.tensor, y=y.tensor, c=self.c(), dim=dim)\n\t        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=dim)\n\t    def project(self, x: ManifoldTensor, eps: float = -1.0) -> ManifoldTensor:\n\t        new_tensor = project(x=x.tensor, c=self.c(), dim=x.man_dim, eps=eps)\n\t        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=x.man_dim)\n\t    def expmap(self, v: TangentTensor) -> ManifoldTensor:\n\t        dim = v.broadcasted_man_dim\n\t        if v.manifold_points is None:\n\t            new_tensor = expmap0(v=v.tensor, c=self.c(), dim=dim)\n", "        else:\n\t            new_tensor = expmap(x=v.manifold_points.tensor, v=v.tensor, c=self.c(), dim=dim)\n\t        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=dim)\n\t    def logmap(self, x: Optional[ManifoldTensor], y: ManifoldTensor):\n\t        if x is None:\n\t            dim = y.man_dim\n\t            new_tensor = logmap0(y=y.tensor, c=self.c(), dim=y.man_dim)\n\t        else:\n\t            dim = check_dims_with_broadcasting(x, y)\n\t            new_tensor = logmap(x=x.tensor, y=y.tensor, c=self.c(), dim=dim)\n", "        return TangentTensor(data=new_tensor, manifold_points=x, manifold=self, man_dim=dim)\n\t    def gyration(self, u: ManifoldTensor, v: ManifoldTensor, w: ManifoldTensor) -> ManifoldTensor:\n\t        dim = check_dims_with_broadcasting(u, v, w)\n\t        new_tensor = gyration(u=u.tensor, v=v.tensor, w=w.tensor, c=self.c(), dim=dim)\n\t        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=dim)\n\t    def transp(self, v: TangentTensor, y: ManifoldTensor) -> TangentTensor:\n\t        dim = check_dims_with_broadcasting(v, y)\n\t        tangent_vectors = transp(\n\t            x=v.manifold_points.tensor, y=y.tensor, v=v.tensor, c=self.c(), dim=dim\n\t        )\n", "        return TangentTensor(\n\t            data=tangent_vectors,\n\t            manifold_points=y,\n\t            manifold=self,\n\t            man_dim=dim,\n\t        )\n\t    def dist(self, x: ManifoldTensor, y: ManifoldTensor) -> Tensor:\n\t        dim = check_dims_with_broadcasting(x, y)\n\t        return dist(x=x.tensor, y=y.tensor, c=self.c(), dim=dim)\n\t    def inner(\n", "        self, u: TangentTensor, v: TangentTensor, keepdim: bool = False, safe_mode: bool = True\n\t    ) -> Tensor:\n\t        dim = check_dims_with_broadcasting(u, v)\n\t        if safe_mode:\n\t            check_tangent_tensor_positions(u, v)\n\t        return inner(\n\t            x=u.manifold_points.tensor, u=u.tensor, v=v.tensor, c=self.c(), dim=dim, keepdim=keepdim\n\t        )\n\t    def euc_to_tangent(self, x: ManifoldTensor, u: ManifoldTensor) -> TangentTensor:\n\t        dim = check_dims_with_broadcasting(x, u)\n", "        tangent_vectors = euc_to_tangent(x=x.tensor, u=u.tensor, c=self.c(), dim=x.man_dim)\n\t        return TangentTensor(\n\t            data=tangent_vectors,\n\t            manifold_points=x,\n\t            manifold=self,\n\t            man_dim=dim,\n\t        )\n\t    def hyperplane_dists(self, x: ManifoldTensor, z: ManifoldTensor, r: Optional[Tensor]) -> Tensor:\n\t        if x.man_dim != 1 or z.man_dim != 0:\n\t            raise ValueError(\n", "                f\"Expected the manifold dimension of the inputs to be 1 and the manifold \"\n\t                f\"dimension of the hyperplane orientations to be 0, but got {x.man_dim} and \"\n\t                f\"{z.man_dim}, respectively\"\n\t            )\n\t        return poincare_hyperplane_dists(x=x.tensor, z=z.tensor, r=r, c=self.c())\n\t    def fully_connected(\n\t        self, x: ManifoldTensor, z: ManifoldTensor, bias: Optional[Tensor]\n\t    ) -> ManifoldTensor:\n\t        if z.man_dim != 0:\n\t            raise ValueError(\n", "                f\"Expected the manifold dimension of the hyperplane orientations to be 0, but got \"\n\t                f\"{z.man_dim} instead\"\n\t            )\n\t        new_tensor = poincare_fully_connected(\n\t            x=x.tensor, z=z.tensor, bias=bias, c=self.c(), dim=x.man_dim\n\t        )\n\t        new_tensor = ManifoldTensor(data=new_tensor, manifold=self, man_dim=x.man_dim)\n\t        return self.project(new_tensor)\n\t    def frechet_mean(\n\t        self,\n", "        x: ManifoldTensor,\n\t        batch_dim: Union[int, list[int]] = 0,\n\t        keepdim: bool = False,\n\t    ) -> ManifoldTensor:\n\t        if isinstance(batch_dim, int):\n\t            batch_dim = [batch_dim]\n\t        output_man_dim = x.man_dim - sum(bd < x.man_dim for bd in batch_dim)\n\t        new_tensor = frechet_mean(\n\t            x=x.tensor, c=self.c(), vec_dim=x.man_dim, batch_dim=batch_dim, keepdim=keepdim\n\t        )\n", "        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=output_man_dim)\n\t    def midpoint(\n\t        self,\n\t        x: ManifoldTensor,\n\t        batch_dim: int = 0,\n\t        w: Optional[Tensor] = None,\n\t        keepdim: bool = False,\n\t    ) -> ManifoldTensor:\n\t        if isinstance(batch_dim, int):\n\t            batch_dim = [batch_dim]\n", "        if x.man_dim in batch_dim:\n\t            raise ValueError(\n\t                f\"Tried to aggregate over dimensions {batch_dim}, but input has manifold \"\n\t                f\"dimension {x.man_dim} and cannot aggregate over this dimension\"\n\t            )\n\t        # Output manifold dimension is shifted left for each batch dim that disappears\n\t        man_dim_shift = sum(bd < x.man_dim for bd in batch_dim)\n\t        new_man_dim = x.man_dim - man_dim_shift if not keepdim else x.man_dim\n\t        new_tensor = midpoint(\n\t            x=x.tensor, c=self.c(), man_dim=x.man_dim, batch_dim=batch_dim, w=w, keepdim=keepdim\n", "        )\n\t        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=new_man_dim)\n\t    def frechet_variance(\n\t        self,\n\t        x: ManifoldTensor,\n\t        mu: Optional[ManifoldTensor] = None,\n\t        batch_dim: Union[int, list[int]] = -1,\n\t        keepdim: bool = False,\n\t    ) -> Tensor:\n\t        if mu is not None:\n", "            mu = mu.tensor\n\t        # TODO: Check if x and mu have compatible man_dims\n\t        return frechet_variance(\n\t            x=x.tensor,\n\t            c=self.c(),\n\t            mu=mu,\n\t            vec_dim=x.man_dim,\n\t            batch_dim=batch_dim,\n\t            keepdim=keepdim,\n\t        )\n", "    def construct_dl_parameters(\n\t        self, in_features: int, out_features: int, bias: bool = True\n\t    ) -> tuple[ManifoldParameter, Optional[Parameter]]:\n\t        weight = ManifoldParameter(\n\t            data=empty(in_features, out_features),\n\t            manifold=Euclidean(),\n\t            man_dim=0,\n\t        )\n\t        if bias:\n\t            b = Parameter(data=empty(out_features))\n", "        else:\n\t            b = None\n\t        return weight, b\n\t    def reset_parameters(self, weight: ManifoldParameter, bias: Optional[Parameter]) -> None:\n\t        in_features, out_features = weight.size()\n\t        if in_features <= out_features:\n\t            with no_grad():\n\t                weight.tensor.copy_(1 / 2 * eye(in_features, out_features))\n\t        else:\n\t            normal_(\n", "                weight.tensor,\n\t                mean=0,\n\t                std=(2 * in_features * out_features) ** -0.5,\n\t            )\n\t        if bias is not None:\n\t            zeros_(bias)\n\t    def unfold(\n\t        self,\n\t        input: ManifoldTensor,\n\t        kernel_size: _size_2_t,\n", "        dilation: _size_2_t = 1,\n\t        padding: _size_2_t = 0,\n\t        stride: _size_2_t = 1,\n\t    ) -> ManifoldTensor:\n\t        # TODO: may have to cache some of this stuff for efficiency.\n\t        in_channels = input.size(1)\n\t        if len(kernel_size) == 2:\n\t            kernel_vol = kernel_size[0] * kernel_size[1]\n\t        else:\n\t            kernel_vol = kernel_size**2\n", "            kernel_size = (kernel_size, kernel_size)\n\t        beta_ni = beta_func(in_channels / 2, 1 / 2)\n\t        beta_n = beta_func(in_channels * kernel_vol / 2, 1 / 2)\n\t        input = self.logmap(x=None, y=input)\n\t        input.tensor = input.tensor * beta_n / beta_ni\n\t        new_tensor = unfold(\n\t            input=input.tensor,\n\t            kernel_size=kernel_size,\n\t            dilation=dilation,\n\t            padding=padding,\n", "            stride=stride,\n\t        )\n\t        new_tensor = TangentTensor(data=new_tensor, manifold_points=None, manifold=self, man_dim=1)\n\t        return self.expmap(new_tensor)\n\t    def flatten(self, x: ManifoldTensor, start_dim: int = 1, end_dim: int = -1) -> ManifoldTensor:\n\t        \"\"\"Flattens a manifold tensor by reshaping it. If start_dim or end_dim are passed,\n\t        only dimensions starting with start_dim and ending with end_dim are flattend.\n\t        If the manifold dimension of the input tensor is among the dimensions which\n\t        are flattened, applies beta-concatenation to the points on the manifold.\n\t        Otherwise simply flattens the tensor using torch.flatten.\n", "        Updates the manifold dimension if necessary.\n\t        \"\"\"\n\t        start_dim = x.dim() + start_dim if start_dim < 0 else start_dim\n\t        end_dim = x.dim() + end_dim if end_dim < 0 else end_dim\n\t        # Get the range of dimensions to flatten.\n\t        dimensions_to_flatten = x.shape[start_dim + 1 : end_dim + 1]\n\t        if start_dim <= x.man_dim and end_dim >= x.man_dim:\n\t            # Use beta concatenation to flatten the manifold dimension of the tensor.\n\t            #\n\t            # Start by applying logmap at the origin and computing the betas.\n", "            tangents = self.logmap(None, x)\n\t            n_i = x.shape[x.man_dim]\n\t            n = n_i * functools.reduce(lambda a, b: a * b, dimensions_to_flatten)\n\t            beta_n = beta_func(n / 2, 0.5)\n\t            beta_n_i = beta_func(n_i / 2, 0.5)\n\t            # Flatten the tensor and rescale.\n\t            tangents.tensor = torch.flatten(\n\t                input=tangents.tensor,\n\t                start_dim=start_dim,\n\t                end_dim=end_dim,\n", "            )\n\t            tangents.tensor = tangents.tensor * beta_n / beta_n_i\n\t            # Set the new manifold dimension\n\t            tangents.man_dim = start_dim\n\t            # Apply exponential map at the origin.\n\t            return self.expmap(tangents)\n\t        else:\n\t            flattened = torch.flatten(\n\t                input=x.tensor,\n\t                start_dim=start_dim,\n", "                end_dim=end_dim,\n\t            )\n\t            man_dim = x.man_dim if end_dim > x.man_dim else x.man_dim - len(dimensions_to_flatten)\n\t            return ManifoldTensor(data=flattened, manifold=x.manifold, man_dim=man_dim)\n"]}
{"filename": "hypll/manifolds/poincare_ball/__init__.py", "chunked_list": ["from .curvature import Curvature\n\tfrom .manifold import PoincareBall\n"]}
{"filename": "hypll/manifolds/poincare_ball/math/diffgeom.py", "chunked_list": ["import torch\n\tdef mobius_add(x: torch.Tensor, y: torch.Tensor, c: torch.Tensor, dim: int = -1):\n\t    broadcasted_dim = max(x.dim(), y.dim())\n\t    dim = dim if dim >= 0 else broadcasted_dim + dim\n\t    x2 = x.pow(2).sum(\n\t        dim=dim - broadcasted_dim + x.dim(),\n\t        keepdim=True,\n\t    )\n\t    y2 = y.pow(2).sum(\n\t        dim=dim - broadcasted_dim + y.dim(),\n", "        keepdim=True,\n\t    )\n\t    xy = (x * y).sum(dim=dim, keepdim=True)\n\t    num = (1 + 2 * c * xy + c * y2) * x + (1 - c * x2) * y\n\t    denom = 1 + 2 * c * xy + c**2 * x2 * y2\n\t    return num / denom.clamp_min(1e-15)\n\tdef project(x: torch.Tensor, c: torch.Tensor, dim: int = -1, eps: float = -1.0):\n\t    if eps < 0:\n\t        if x.dtype == torch.float32:\n\t            eps = 4e-3\n", "        else:\n\t            eps = 1e-5\n\t    maxnorm = (1 - eps) / ((c + 1e-15) ** 0.5)\n\t    maxnorm = torch.where(c.gt(0), maxnorm, c.new_full((), 1e15))\n\t    norm = x.norm(dim=dim, keepdim=True, p=2).clamp_min(1e-15)\n\t    cond = norm > maxnorm\n\t    projected = x / norm * maxnorm\n\t    return torch.where(cond, projected, x)\n\tdef expmap0(v: torch.Tensor, c: torch.Tensor, dim: int = -1):\n\t    v_norm_c_sqrt = v.norm(dim=dim, keepdim=True).clamp_min(1e-15) * c.sqrt()\n", "    return project(torch.tanh(v_norm_c_sqrt) * v / v_norm_c_sqrt, c, dim=dim)\n\tdef logmap0(y: torch.Tensor, c: torch.Tensor, dim: int = -1):\n\t    y_norm_c_sqrt = y.norm(dim=dim, keepdim=True).clamp_min(1e-15) * c.sqrt()\n\t    return torch.atanh(y_norm_c_sqrt) * y / y_norm_c_sqrt\n\tdef expmap(x: torch.Tensor, v: torch.Tensor, c: torch.Tensor, dim: int = -1):\n\t    broadcasted_dim = max(x.dim(), v.dim())\n\t    dim = dim if dim >= 0 else broadcasted_dim + dim\n\t    v_norm = v.norm(dim=dim - broadcasted_dim + v.dim(), keepdim=True).clamp_min(1e-15)\n\t    lambda_x = 2 / (\n\t        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n", "    ).clamp_min(1e-15)\n\t    c_sqrt = c.sqrt()\n\t    second_term = torch.tanh(c_sqrt * lambda_x * v_norm / 2) * v / (c_sqrt * v_norm)\n\t    return project(mobius_add(x, second_term, c, dim=dim), c, dim=dim)\n\tdef logmap(x: torch.Tensor, y: torch.Tensor, c: torch.Tensor, dim: int = -1):\n\t    broadcasted_dim = max(x.dim(), y.dim())\n\t    dim = dim if dim >= 0 else broadcasted_dim + dim\n\t    min_x_y = mobius_add(-x, y, c, dim=dim)\n\t    min_x_y_norm = min_x_y.norm(dim=dim, keepdim=True).clamp_min(1e-15)\n\t    lambda_x = 2 / (\n", "        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n\t    ).clamp_min(1e-15)\n\t    c_sqrt = c.sqrt()\n\t    return 2 / (c_sqrt * lambda_x) * torch.atanh(c_sqrt * min_x_y_norm) * min_x_y / min_x_y_norm\n\tdef gyration(\n\t    u: torch.Tensor,\n\t    v: torch.Tensor,\n\t    w: torch.Tensor,\n\t    c: torch.Tensor,\n\t    dim: int = -1,\n", "):\n\t    broadcasted_dim = max(u.dim(), v.dim(), w.dim())\n\t    dim = dim if dim >= 0 else broadcasted_dim + dim\n\t    u2 = u.pow(2).sum(dim=dim - broadcasted_dim + u.dim(), keepdim=True)\n\t    v2 = v.pow(2).sum(dim=dim - broadcasted_dim + v.dim(), keepdim=True)\n\t    uv = (u * v).sum(dim=dim - broadcasted_dim + max(u.dim(), v.dim()), keepdim=True)\n\t    uw = (u * w).sum(dim=dim - broadcasted_dim + max(u.dim(), w.dim()), keepdim=True)\n\t    vw = (v * w).sum(dim=dim - broadcasted_dim + max(v.dim(), w.dim()), keepdim=True)\n\t    K2 = c**2\n\t    a = -K2 * uw * v2 + c * vw + 2 * K2 * uv * vw\n", "    b = -K2 * vw * u2 - c * uw\n\t    d = 1 + 2 * c * uv + K2 * u2 * v2\n\t    return w + 2 * (a * u + b * v) / d.clamp_min(1e-15)\n\tdef transp(\n\t    x: torch.Tensor,\n\t    y: torch.Tensor,\n\t    v: torch.Tensor,\n\t    c: torch.Tensor,\n\t    dim: int = -1,\n\t):\n", "    broadcasted_dim = max(x.dim(), y.dim(), v.dim())\n\t    dim = dim if dim >= 0 else broadcasted_dim + dim\n\t    lambda_x = 2 / (\n\t        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n\t    ).clamp_min(1e-15)\n\t    lambda_y = 2 / (\n\t        1 - c * y.pow(2).sum(dim=dim - broadcasted_dim + y.dim(), keepdim=True)\n\t    ).clamp_min(1e-15)\n\t    return gyration(y, -x, v, c, dim=dim) * lambda_x / lambda_y\n\tdef dist(\n", "    x: torch.Tensor,\n\t    y: torch.Tensor,\n\t    c: torch.Tensor,\n\t    dim: int = -1,\n\t    keepdim: bool = False,\n\t) -> torch.Tensor:\n\t    return (\n\t        2\n\t        / c.sqrt()\n\t        * (c.sqrt() * mobius_add(-x, y, c, dim=dim).norm(dim=dim, keepdim=keepdim)).atanh()\n", "    )\n\tdef inner(\n\t    x: torch.Tensor,\n\t    u: torch.Tensor,\n\t    v: torch.Tensor,\n\t    c: torch.Tensor,\n\t    dim: int = -1,\n\t    keepdim: bool = False,\n\t) -> torch.Tensor:\n\t    broadcasted_dim = max(x.dim(), u.dim(), v.dim())\n", "    dim = dim if dim >= 0 else broadcasted_dim + dim\n\t    lambda_x = 2 / (\n\t        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n\t    ).clamp_min(1e-15)\n\t    dot_prod = (u * v).sum(dim=dim, keepdim=keepdim)\n\t    return lambda_x.square() * dot_prod\n\tdef euc_to_tangent(\n\t    x: torch.Tensor,\n\t    u: torch.Tensor,\n\t    c: torch.Tensor,\n", "    dim: int = -1,\n\t) -> torch.Tensor:\n\t    broadcasted_dim = max(x.dim(), u.dim())\n\t    dim = dim if dim >= 0 else broadcasted_dim + dim\n\t    lambda_x = 2 / (\n\t        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n\t    ).clamp_min(1e-15)\n\t    return u / lambda_x**2\n"]}
{"filename": "hypll/manifolds/poincare_ball/math/__init__.py", "chunked_list": []}
{"filename": "hypll/manifolds/poincare_ball/math/linalg.py", "chunked_list": ["from typing import Optional\n\timport torch\n\tdef poincare_hyperplane_dists(\n\t    x: torch.Tensor,\n\t    z: torch.Tensor,\n\t    r: Optional[torch.Tensor],\n\t    c: torch.Tensor,\n\t    dim: int = -1,\n\t) -> torch.Tensor:\n\t    \"\"\"\n", "    The Poincare signed distance to hyperplanes operation.\n\t    Parameters\n\t    ----------\n\t    x : tensor\n\t        contains input values\n\t    z : tensor\n\t        contains the hyperbolic vectors describing the hyperplane orientations\n\t    r : tensor\n\t        contains the hyperplane offsets\n\t    c : tensor\n", "        curvature of the Poincare disk\n\t    Returns\n\t    -------\n\t    tensor\n\t        signed distances of input w.r.t. the hyperplanes, denoted by v_k(x) in\n\t        the HNN++ paper\n\t    \"\"\"\n\t    dim_shifted_x = x.movedim(source=dim, destination=-1)\n\t    c_sqrt = c.sqrt()\n\t    lam = 2 * (1 - c * dim_shifted_x.pow(2).sum(dim=-1, keepdim=True))\n", "    z_norm = z.norm(dim=0).clamp_min(1e-15)\n\t    # Computation can be simplified if there is no offset\n\t    if r is None:\n\t        dim_shifted_output = (\n\t            2\n\t            * z_norm\n\t            / c_sqrt\n\t            * torch.asinh(c_sqrt * lam / z_norm * torch.matmul(dim_shifted_x, z))\n\t        )\n\t    else:\n", "        two_csqrt_r = 2.0 * c_sqrt * r\n\t        dim_shifted_output = (\n\t            2\n\t            * z_norm\n\t            / c_sqrt\n\t            * torch.asinh(\n\t                c_sqrt * lam / z_norm * torch.matmul(dim_shifted_x, z) * two_csqrt_r.cosh()\n\t                - (lam - 1) * two_csqrt_r.sinh()\n\t            )\n\t        )\n", "    return dim_shifted_output.movedim(source=-1, destination=dim)\n\tdef poincare_fully_connected(\n\t    x: torch.Tensor,\n\t    z: torch.Tensor,\n\t    bias: Optional[torch.Tensor],\n\t    c: torch.Tensor,\n\t    dim: int = -1,\n\t) -> torch.Tensor:\n\t    \"\"\"\n\t    The Poincare fully connected layer operation.\n", "    Parameters\n\t    ----------\n\t    x : tensor\n\t        contains the layer inputs\n\t    z : tensor\n\t        contains the hyperbolic vectors describing the hyperplane orientations\n\t    bias : tensor\n\t        contains the biases (hyperplane offsets)\n\t    c : tensor\n\t        curvature of the Poincare disk\n", "    Returns\n\t    -------\n\t    tensor\n\t        Poincare FC transformed hyperbolic tensor, commonly denoted by y\n\t    \"\"\"\n\t    c_sqrt = c.sqrt()\n\t    x = poincare_hyperplane_dists(x=x, z=z, r=bias, c=c, dim=dim)\n\t    x = (c_sqrt * x).sinh() / c_sqrt\n\t    return x / (1 + (1 + c * x.pow(2).sum(dim=dim, keepdim=True)).sqrt())\n"]}
{"filename": "hypll/manifolds/poincare_ball/math/stats.py", "chunked_list": ["from typing import Optional, Union\n\timport torch\n\timport torch.nn as nn\n\tfrom .diffgeom import dist\n\t_TOLEPS = {torch.float32: 1e-6, torch.float64: 1e-12}\n\tclass FrechetMean(torch.autograd.Function):\n\t    \"\"\"\n\t    This implementation is copied mostly from:\n\t        https://github.com/CUVL/Differentiable-Frechet-Mean.git\n\t    which is itself based on the paper:\n", "        https://arxiv.org/abs/2003.00335\n\t    Both by Aaron Lou (et al.)\n\t    \"\"\"\n\t    @staticmethod\n\t    def forward(ctx, x, c, vec_dim, batch_dim, keepdim):\n\t        # Convert input dimensions to positive values\n\t        vec_dim = vec_dim if vec_dim > 0 else x.dim() + vec_dim\n\t        batch_dim = [bd if bd >= 0 else x.dim() + bd for bd in batch_dim]\n\t        # Compute some dim ids for later\n\t        output_vec_dim = vec_dim - sum(bd < vec_dim for bd in batch_dim)\n", "        batch_start_id = -len(batch_dim) - 1\n\t        batch_stop_id = -2\n\t        original_dims = [x.size(bd) for bd in batch_dim]\n\t        # Move dims around and flatten the batch dimensions\n\t        x = x.movedim(\n\t            source=batch_dim + [vec_dim],\n\t            destination=list(range(batch_start_id, 0)),\n\t        )\n\t        x = x.flatten(\n\t            start_dim=batch_start_id, end_dim=batch_stop_id\n", "        )  # ..., prod(batch_dim), vec_dim\n\t        # Compute frechet mean and store variables\n\t        mean = frechet_ball_forward(X=x, K=c, rtol=_TOLEPS[x.dtype], atol=_TOLEPS[x.dtype])\n\t        ctx.save_for_backward(x, mean, c)\n\t        ctx.vec_dim = vec_dim\n\t        ctx.batch_dim = batch_dim\n\t        ctx.output_vec_dim = output_vec_dim\n\t        ctx.batch_start_id = batch_start_id\n\t        ctx.original_dims = original_dims\n\t        # Reorder dimensions to match dimensions of input\n", "        mean = mean.movedim(\n\t            source=list(range(output_vec_dim - mean.dim(), 0)),\n\t            destination=list(range(output_vec_dim + 1, mean.dim())) + [output_vec_dim],\n\t        )\n\t        # Add dimensions back to mean if keepdim\n\t        if keepdim:\n\t            for bd in sorted(batch_dim):\n\t                mean = mean.unsqueeze(bd)\n\t        return mean\n\t    @staticmethod\n", "    def backward(ctx, grad_output):\n\t        x, mean, c = ctx.saved_tensors\n\t        # Reshift dims in grad to match the dims of the mean that was stored in ctx\n\t        grad_output = grad_output.movedim(\n\t            source=list(range(ctx.output_vec_dim - mean.dim(), 0)),\n\t            destination=[-1] + list(range(ctx.output_vec_dim - mean.dim(), -1)),\n\t        )\n\t        dx, dc = frechet_ball_backward(X=x, y=mean, grad=grad_output, K=c)\n\t        vec_dim = ctx.vec_dim\n\t        batch_dim = ctx.batch_dim\n", "        batch_start_id = ctx.batch_start_id\n\t        original_dims = ctx.original_dims\n\t        # Unflatten the batch dimensions\n\t        dx = dx.unflatten(dim=-2, sizes=original_dims)\n\t        # Move the vector dimension back\n\t        dx = dx.movedim(\n\t            source=list(range(batch_start_id, 0)),\n\t            destination=batch_dim + [vec_dim],\n\t        )\n\t        return dx, dc, None, None, None\n", "def frechet_mean(\n\t    x: torch.Tensor,\n\t    c: torch.Tensor,\n\t    vec_dim: int = -1,\n\t    batch_dim: Union[int, list[int]] = 0,\n\t    keepdim: bool = False,\n\t) -> torch.Tensor:\n\t    if isinstance(batch_dim, int):\n\t        batch_dim = [batch_dim]\n\t    return FrechetMean.apply(x, c, vec_dim, batch_dim, keepdim)\n", "def midpoint(\n\t    x: torch.Tensor,\n\t    c: torch.Tensor,\n\t    man_dim: int = -1,\n\t    batch_dim: Union[int, list[int]] = 0,\n\t    w: Optional[torch.Tensor] = None,\n\t    keepdim: bool = False,\n\t) -> torch.Tensor:\n\t    lambda_x = 2 / (1 - c * x.pow(2).sum(dim=man_dim, keepdim=True)).clamp_min(1e-15)\n\t    if w is None:\n", "        numerator = (lambda_x * x).sum(dim=batch_dim, keepdim=True)\n\t        denominator = (lambda_x - 1).sum(dim=batch_dim, keepdim=True)\n\t    else:\n\t        # TODO: test weights\n\t        numerator = (lambda_x * w * x).sum(dim=batch_dim, keepdim=True)\n\t        denominator = ((lambda_x - 1) * w).sum(dim=batch_dim, keepdim=True)\n\t    frac = numerator / denominator.clamp_min(1e-15)\n\t    midpoint = 1 / (1 + (1 - c * frac.pow(2).sum(dim=man_dim, keepdim=True)).sqrt()) * frac\n\t    if not keepdim:\n\t        midpoint = midpoint.squeeze(dim=batch_dim)\n", "    return midpoint\n\tdef frechet_variance(\n\t    x: torch.Tensor,\n\t    c: torch.Tensor,\n\t    mu: Optional[torch.Tensor] = None,\n\t    vec_dim: int = -1,\n\t    batch_dim: Union[int, list[int]] = 0,\n\t    keepdim: bool = False,\n\t) -> torch.Tensor:  # TODO\n\t    \"\"\"\n", "    Args\n\t    ----\n\t        x (tensor): points of shape [..., points, dim]\n\t        mu (tensor): mean of shape [..., dim]\n\t        where the ... of the three variables match\n\t    Returns\n\t    -------\n\t        tensor of shape [...]\n\t    \"\"\"\n\t    if isinstance(batch_dim, int):\n", "        batch_dim = [batch_dim]\n\t    if mu is None:\n\t        mu = frechet_mean(x=x, c=c, vec_dim=vec_dim, batch_dim=batch_dim, keepdim=True)\n\t    if x.dim() != mu.dim():\n\t        for bd in sorted(batch_dim):\n\t            mu = mu.unsqueeze(bd)\n\t    distance = dist(x=x, y=mu, c=c, dim=vec_dim, keepdim=keepdim)\n\t    distance = distance.pow(2)\n\t    return distance.mean(dim=batch_dim, keepdim=keepdim)\n\tdef l_prime(y: torch.Tensor) -> torch.Tensor:\n", "    cond = y < 1e-12\n\t    val = 4 * torch.ones_like(y)\n\t    ret = torch.where(cond, val, 2 * (1 + 2 * y).acosh() / (y.pow(2) + y).sqrt())\n\t    return ret\n\tdef frechet_ball_forward(\n\t    X: torch.Tensor,\n\t    K: torch.Tensor = torch.Tensor([1]),\n\t    max_iter: int = 1000,\n\t    rtol: float = 1e-6,\n\t    atol: float = 1e-6,\n", ") -> torch.Tensor:\n\t    \"\"\"\n\t    Args\n\t    ----\n\t        X (tensor): point of shape [..., points, dim]\n\t        K (float): curvature (must be negative)\n\t    Returns\n\t    -------\n\t        frechet mean (tensor): shape [..., dim]\n\t    \"\"\"\n", "    mu = X[..., 0, :].clone()\n\t    x_ss = X.pow(2).sum(dim=-1)\n\t    mu_prev = mu\n\t    iters = 0\n\t    for _ in range(max_iter):\n\t        mu_ss = mu.pow(2).sum(dim=-1)\n\t        xmu_ss = (X - mu.unsqueeze(-2)).pow(2).sum(dim=-1)\n\t        alphas = l_prime(K * xmu_ss / ((1 - K * x_ss) * (1 - K * mu_ss.unsqueeze(-1)))) / (\n\t            1 - K * x_ss\n\t        )\n", "        alphas = alphas\n\t        c = (alphas * x_ss).sum(dim=-1)\n\t        b = (alphas.unsqueeze(-1) * X).sum(dim=-2)\n\t        a = alphas.sum(dim=-1)\n\t        b_ss = b.pow(2).sum(dim=-1)\n\t        eta = (a + K * c - ((a + K * c).pow(2) - 4 * K * b_ss).sqrt()) / (2 * K * b_ss).clamp_min(\n\t            1e-15\n\t        )\n\t        mu = eta.unsqueeze(-1) * b\n\t        dist = (mu - mu_prev).norm(dim=-1)\n", "        prev_dist = mu_prev.norm(dim=-1)\n\t        if (dist < atol).all() or (dist / prev_dist < rtol).all():\n\t            break\n\t        mu_prev = mu\n\t        iters += 1\n\t    return mu\n\tdef darcosh(x):\n\t    cond = x < 1 + 1e-7\n\t    x = torch.where(cond, 2 * torch.ones_like(x), x)\n\t    x = torch.where(~cond, 2 * (x).acosh() / torch.sqrt(x**2 - 1), x)\n", "    return x\n\tdef d2arcosh(x):\n\t    cond = x < 1 + 1e-7\n\t    x = torch.where(cond, -2 / 3 * torch.ones_like(x), x)\n\t    x = torch.where(\n\t        ~cond,\n\t        2 / (x**2 - 1) - 2 * x * x.acosh() / ((x**2 - 1) ** (3 / 2)),\n\t        x,\n\t    )\n\t    return x\n", "def grad_var(\n\t    X: torch.Tensor,\n\t    y: torch.Tensor,\n\t    K: torch.Tensor,\n\t) -> torch.Tensor:\n\t    \"\"\"\n\t    Args\n\t    ----\n\t        X (tensor): point of shape [..., points, dim]\n\t        y (tensor): mean point of shape [..., dim]\n", "        K (float): curvature (must be negative)\n\t    Returns\n\t    -------\n\t        grad (tensor): gradient of variance [..., dim]\n\t    \"\"\"\n\t    yl = y.unsqueeze(-2)\n\t    xnorm = 1 - K * X.pow(2).sum(dim=-1)\n\t    ynorm = 1 - K * yl.pow(2).sum(dim=-1)\n\t    xynorm = (X - yl).pow(2).sum(dim=-1)\n\t    D = xnorm * ynorm\n", "    v = 1 + 2 * K * xynorm / D\n\t    Dl = D.unsqueeze(-1)\n\t    vl = v.unsqueeze(-1)\n\t    first_term = (X - yl) / Dl\n\t    sec_term = -K / Dl.pow(2) * yl * xynorm.unsqueeze(-1) * xnorm.unsqueeze(-1)\n\t    return -(4 * darcosh(vl) * (first_term + sec_term)).sum(dim=-2)\n\tdef inverse_hessian(\n\t    X: torch.Tensor,\n\t    y: torch.Tensor,\n\t    K: torch.Tensor,\n", ") -> torch.Tensor:\n\t    \"\"\"\n\t    Args\n\t    ----\n\t        X (tensor): point of shape [..., points, dim]\n\t        y (tensor): mean point of shape [..., dim]\n\t        K (float): curvature (must be negative)\n\t    Returns\n\t    -------\n\t        inv_hess (tensor): inverse hessian of [..., points, dim, dim]\n", "    \"\"\"\n\t    yl = y.unsqueeze(-2)\n\t    xnorm = 1 - K * X.pow(2).sum(dim=-1)\n\t    ynorm = 1 - K * yl.pow(2).sum(dim=-1)\n\t    xynorm = (X - yl).pow(2).sum(dim=-1)\n\t    D = xnorm * ynorm\n\t    v = 1 + 2 * K * xynorm / D\n\t    Dl = D.unsqueeze(-1)\n\t    vl = v.unsqueeze(-1)\n\t    vll = vl.unsqueeze(-1)\n", "    \"\"\"\n\t    \\partial T/ \\partial y\n\t    \"\"\"\n\t    first_const = -8 * (K**2) * xnorm / D.pow(2)\n\t    matrix_val = (first_const.unsqueeze(-1) * yl).unsqueeze(-1) * (X - yl).unsqueeze(-2)\n\t    first_term = matrix_val + matrix_val.transpose(-1, -2)\n\t    sec_const = 16 * (K**3) * xnorm.pow(2) / D.pow(3) * xynorm\n\t    sec_term = (sec_const.unsqueeze(-1) * yl).unsqueeze(-1) * yl.unsqueeze(-2)\n\t    third_const = 4 * K / D + 4 * (K**2) * xnorm / D.pow(2) * xynorm\n\t    third_term = third_const.reshape(*third_const.shape, 1, 1) * torch.eye(y.shape[-1]).to(\n", "        X\n\t    ).reshape((1,) * len(third_const.shape) + (y.shape[-1], y.shape[-1]))\n\t    Ty = first_term + sec_term + third_term\n\t    \"\"\"\n\t    T\n\t    \"\"\"\n\t    first_term = -K / Dl * (X - yl)\n\t    sec_term = K.pow(2) / Dl.pow(2) * yl * xynorm.unsqueeze(-1) * xnorm.unsqueeze(-1)\n\t    T = 4 * (first_term + sec_term)\n\t    \"\"\"\n", "    inverse of shape [..., points, dim, dim]\n\t    \"\"\"\n\t    first_term = d2arcosh(vll) * T.unsqueeze(-1) * T.unsqueeze(-2)\n\t    sec_term = darcosh(vll) * Ty\n\t    hessian = (first_term + sec_term).sum(dim=-3) / K\n\t    inv_hess = torch.inverse(hessian)\n\t    return inv_hess\n\tdef frechet_ball_backward(\n\t    X: torch.Tensor,\n\t    y: torch.Tensor,\n", "    grad: torch.Tensor,\n\t    K: torch.Tensor,\n\t) -> tuple[torch.Tensor]:\n\t    \"\"\"\n\t    Args\n\t    ----\n\t        X (tensor): point of shape [..., points, dim]\n\t        y (tensor): mean point of shape [..., dim]\n\t        grad (tensor): gradient\n\t        K (float): curvature (must be negative)\n", "    Returns\n\t    -------\n\t        gradients (tensor, tensor, tensor):\n\t            gradient of X [..., points, dim], curvature []\n\t    \"\"\"\n\t    if not torch.is_tensor(K):\n\t        K = torch.tensor(K).to(X)\n\t    with torch.no_grad():\n\t        inv_hess = inverse_hessian(X, y, K=K)\n\t    with torch.enable_grad():\n", "        # clone variables\n\t        X = nn.Parameter(X.detach())\n\t        y = y.detach()\n\t        K = nn.Parameter(K)\n\t        grad = (inv_hess @ grad.unsqueeze(-1)).squeeze()\n\t        gradf = grad_var(X, y, K)\n\t        dx, dK = torch.autograd.grad(-gradf.squeeze(), (X, K), grad)\n\t    return dx, dK\n"]}
{"filename": "hypll/manifolds/base/manifold.py", "chunked_list": ["from __future__ import annotations\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import TYPE_CHECKING, Optional, Union\n\tfrom torch import Tensor\n\tfrom torch.nn import Module, Parameter\n\tfrom torch.nn.common_types import _size_2_t\n\t# TODO: find a less hacky solution for this\n\tif TYPE_CHECKING:\n\t    from hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n\tclass Manifold(Module, ABC):\n", "    def __init__(self) -> None:\n\t        super(Manifold, self).__init__()\n\t    @abstractmethod\n\t    def project(self, x: ManifoldTensor, eps: float = -1.0) -> ManifoldTensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def expmap(self, v: TangentTensor) -> ManifoldTensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def logmap(self, x: Optional[ManifoldTensor], y: ManifoldTensor) -> TangentTensor:\n", "        raise NotImplementedError\n\t    @abstractmethod\n\t    def transp(self, v: TangentTensor, y: ManifoldTensor) -> TangentTensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def dist(self, x: ManifoldTensor, y: ManifoldTensor) -> Tensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def euc_to_tangent(self, x: ManifoldTensor, u: ManifoldTensor) -> TangentTensor:\n\t        raise NotImplementedError\n", "    @abstractmethod\n\t    def hyperplane_dists(self, x: ManifoldTensor, z: ManifoldTensor, r: Optional[Tensor]) -> Tensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def fully_connected(\n\t        self, x: ManifoldTensor, z: ManifoldTensor, bias: Optional[Tensor]\n\t    ) -> ManifoldTensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def frechet_mean(\n", "        self,\n\t        x: ManifoldTensor,\n\t        batch_dim: Union[int, list[int]] = 0,\n\t        keepdim: bool = False,\n\t    ) -> ManifoldTensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def midpoint(\n\t        self,\n\t        x: ManifoldTensor,\n", "        batch_dim: Union[int, list[int]] = 0,\n\t        w: Optional[Tensor] = None,\n\t        keepdim: bool = False,\n\t    ) -> ManifoldTensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def frechet_variance(\n\t        self,\n\t        x: ManifoldTensor,\n\t        mu: Optional[ManifoldTensor] = None,\n", "        batch_dim: Union[int, list[int]] = -1,\n\t        keepdim: bool = False,\n\t    ) -> Tensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def construct_dl_parameters(\n\t        self, in_features: int, out_features: int, bias: bool = True\n\t    ) -> Union[ManifoldParameter, tuple[ManifoldParameter, Parameter]]:\n\t        # TODO: make an annotation object for the return type of this method\n\t        raise NotImplementedError\n", "    @abstractmethod\n\t    def reset_parameters(self, weight: ManifoldParameter, bias: Parameter) -> None:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def flatten(self, x: ManifoldTensor, start_dim: int = 1, end_dim: int = -1) -> ManifoldTensor:\n\t        raise NotImplementedError\n\t    @abstractmethod\n\t    def unfold(\n\t        self,\n\t        input: ManifoldTensor,\n", "        kernel_size: _size_2_t,\n\t        dilation: _size_2_t = 1,\n\t        padding: _size_2_t = 0,\n\t        stride: _size_2_t = 1,\n\t    ) -> ManifoldTensor:\n\t        raise NotImplementedError\n"]}
{"filename": "hypll/manifolds/base/__init__.py", "chunked_list": ["from .manifold import Manifold\n"]}
{"filename": "hypll/manifolds/euclidean/manifold.py", "chunked_list": ["from math import sqrt\n\tfrom typing import Optional, Union\n\timport torch\n\tfrom torch import Tensor, broadcast_shapes, empty, matmul, var\n\tfrom torch.nn import Parameter\n\tfrom torch.nn.common_types import _size_2_t\n\tfrom torch.nn.functional import unfold\n\tfrom torch.nn.init import _calculate_fan_in_and_fan_out, kaiming_uniform_, uniform_\n\tfrom hypll.manifolds.base import Manifold\n\tfrom hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n", "from hypll.utils.tensor_utils import (\n\t    check_dims_with_broadcasting,\n\t    check_tangent_tensor_positions,\n\t)\n\tclass Euclidean(Manifold):\n\t    def __init__(self):\n\t        super(Euclidean, self).__init__()\n\t    def project(self, x: ManifoldTensor, eps: float = -1.0) -> ManifoldTensor:\n\t        return x\n\t    def expmap(self, v: TangentTensor) -> ManifoldTensor:\n", "        if v.manifold_points is None:\n\t            new_tensor = v.tensor\n\t        else:\n\t            new_tensor = v.manifold_points.tensor + v.tensor\n\t        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=v.broadcasted_man_dim)\n\t    def logmap(self, x: Optional[ManifoldTensor], y: ManifoldTensor) -> TangentTensor:\n\t        if x is None:\n\t            dim = y.man_dim\n\t            new_tensor = y.tensor\n\t        else:\n", "            dim = check_dims_with_broadcasting(x, y)\n\t            new_tensor = y.tensor - x.tensor\n\t        return TangentTensor(data=new_tensor, manifold_points=x, manifold=self, man_dim=dim)\n\t    def transp(self, v: TangentTensor, y: ManifoldTensor) -> TangentTensor:\n\t        dim = check_dims_with_broadcasting(v, y)\n\t        output_shape = broadcast_shapes(v.size(), y.size())\n\t        new_tensor = v.tensor.broadcast_to(output_shape)\n\t        return TangentTensor(data=new_tensor, manifold_points=y, manifold=self, man_dim=dim)\n\t    def dist(self, x: ManifoldTensor, y: ManifoldTensor) -> Tensor:\n\t        dim = check_dims_with_broadcasting(x, y)\n", "        return (y.tensor - x.tensor).norm(dim=dim)\n\t    def inner(\n\t        self, u: TangentTensor, v: TangentTensor, keepdim: bool = False, safe_mode: bool = True\n\t    ) -> Tensor:\n\t        dim = check_dims_with_broadcasting(u, v)\n\t        if safe_mode:\n\t            check_tangent_tensor_positions(u, v)\n\t        return (u.tensor * v.tensor).sum(dim=dim, keepdim=keepdim)\n\t    def euc_to_tangent(self, x: ManifoldTensor, u: ManifoldTensor) -> TangentTensor:\n\t        dim = check_dims_with_broadcasting(x, u)\n", "        return TangentTensor(\n\t            data=u.tensor,\n\t            manifold_points=x,\n\t            manifold=self,\n\t            man_dim=dim,\n\t        )\n\t    def hyperplane_dists(self, x: ManifoldTensor, z: ManifoldTensor, r: Optional[Tensor]) -> Tensor:\n\t        if x.man_dim != 1 or z.man_dim != 0:\n\t            raise ValueError(\n\t                f\"Expected the manifold dimension of the inputs to be 1 and the manifold \"\n", "                f\"dimension of the hyperplane orientations to be 0, but got {x.man_dim} and \"\n\t                f\"{z.man_dim}, respectively\"\n\t            )\n\t        if r is None:\n\t            return matmul(x.tensor, z.tensor)\n\t        else:\n\t            return matmul(x.tensor, z.tensor) + r\n\t    def fully_connected(\n\t        self, x: ManifoldTensor, z: ManifoldTensor, bias: Optional[Tensor]\n\t    ) -> ManifoldTensor:\n", "        if z.man_dim != 0:\n\t            raise ValueError(\n\t                f\"Expected the manifold dimension of the hyperplane orientations to be 0, but got \"\n\t                f\"{z.man_dim} instead\"\n\t            )\n\t        dim_shifted_x_tensor = x.tensor.movedim(source=x.man_dim, destination=-1)\n\t        dim_shifted_new_tensor = matmul(dim_shifted_x_tensor, z.tensor)\n\t        if bias is not None:\n\t            dim_shifted_new_tensor = dim_shifted_new_tensor + bias\n\t        new_tensor = dim_shifted_new_tensor.movedim(source=-1, destination=x.man_dim)\n", "        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=x.man_dim)\n\t    def frechet_mean(\n\t        self,\n\t        x: ManifoldTensor,\n\t        batch_dim: Union[int, list[int]] = 0,\n\t        keepdim: bool = False,\n\t    ) -> ManifoldTensor:\n\t        if isinstance(batch_dim, int):\n\t            batch_dim = [batch_dim]\n\t        if x.man_dim in batch_dim:\n", "            raise ValueError(\n\t                f\"Tried to aggregate over dimensions {batch_dim}, but input has manifold \"\n\t                f\"dimension {x.man_dim} and cannot aggregate over this dimension\"\n\t            )\n\t        # Output manifold dimension is shifted left for each batch dim that disappears\n\t        man_dim_shift = sum(bd < x.man_dim for bd in batch_dim)\n\t        new_man_dim = x.man_dim - man_dim_shift if not keepdim else x.man_dim\n\t        mean_tensor = x.tensor.mean(dim=batch_dim, keepdim=keepdim)\n\t        return ManifoldTensor(data=mean_tensor, manifold=self, man_dim=new_man_dim)\n\t    def midpoint(\n", "        self,\n\t        x: ManifoldTensor,\n\t        batch_dim: Union[int, list[int]] = 0,\n\t        w: Optional[Tensor] = None,\n\t        keepdim: bool = False,\n\t    ) -> ManifoldTensor:\n\t        return self.frechet_mean(x=x, batch_dim=batch_dim, keepdim=keepdim)\n\t    def frechet_variance(\n\t        self,\n\t        x: ManifoldTensor,\n", "        mu: Optional[ManifoldTensor] = None,\n\t        batch_dim: Union[int, list[int]] = -1,\n\t        keepdim: bool = False,\n\t    ) -> Tensor:\n\t        if isinstance(batch_dim, int):\n\t            batch_dim = [batch_dim]\n\t        if mu is None:\n\t            return var(x.tensor, dim=batch_dim, keepdim=keepdim)\n\t        else:\n\t            if x.dim() != mu.dim():\n", "                for bd in sorted(batch_dim):\n\t                    mu.man_dim += 1 if bd <= mu.man_dim else 0\n\t                    mu.tensor = mu.tensor.unsqueeze(bd)\n\t            if mu.man_dim != x.man_dim:\n\t                raise ValueError(\"Input tensor and mean do not have matching manifold dimensions\")\n\t            n = 1\n\t            for bd in batch_dim:\n\t                n *= x.size(dim=bd)\n\t            return (x.tensor - mu.tensor).pow(2).sum(dim=batch_dim, keepdim=keepdim) / (n - 1)\n\t    def construct_dl_parameters(\n", "        self, in_features: int, out_features: int, bias: bool = True\n\t    ) -> tuple[ManifoldParameter, Optional[Parameter]]:\n\t        weight = ManifoldParameter(\n\t            data=empty(in_features, out_features),\n\t            manifold=self,\n\t            man_dim=0,\n\t        )\n\t        if bias:\n\t            b = Parameter(data=empty(out_features))\n\t        else:\n", "            b = None\n\t        return weight, b\n\t    def reset_parameters(self, weight: ManifoldParameter, bias: Parameter) -> None:\n\t        kaiming_uniform_(weight.tensor, a=sqrt(5))\n\t        if bias is not None:\n\t            fan_in, _ = _calculate_fan_in_and_fan_out(weight.tensor)\n\t            bound = 1 / sqrt(fan_in) if fan_in > 0 else 0\n\t            uniform_(bias, -bound, bound)\n\t    def unfold(\n\t        self,\n", "        input: ManifoldTensor,\n\t        kernel_size: _size_2_t,\n\t        dilation: _size_2_t = 1,\n\t        padding: _size_2_t = 0,\n\t        stride: _size_2_t = 1,\n\t    ) -> ManifoldTensor:\n\t        new_tensor = unfold(\n\t            input=input.tensor,\n\t            kernel_size=kernel_size,\n\t            dilation=dilation,\n", "            padding=padding,\n\t            stride=stride,\n\t        )\n\t        return ManifoldTensor(data=new_tensor, manifold=input.manifold, man_dim=1)\n\t    def flatten(self, x: ManifoldTensor, start_dim: int = 1, end_dim: int = -1) -> ManifoldTensor:\n\t        \"\"\"Flattens a manifold tensor by reshaping it. If start_dim or end_dim are passed,\n\t        only dimensions starting with start_dim and ending with end_dim are flattend.\n\t        Updates the manifold dimension if necessary.\n\t        \"\"\"\n\t        start_dim = x.dim() + start_dim if start_dim < 0 else start_dim\n", "        end_dim = x.dim() + end_dim if end_dim < 0 else end_dim\n\t        # Get the range of dimensions to flatten.\n\t        dimensions_to_flatten = x.shape[start_dim + 1 : end_dim + 1]\n\t        # Get the new manifold dimension.\n\t        if start_dim <= x.man_dim and end_dim >= x.man_dim:\n\t            man_dim = start_dim\n\t        elif end_dim <= x.man_dim:\n\t            man_dim = x.man_dim - len(dimensions_to_flatten)\n\t        else:\n\t            man_dim = x.man_dim\n", "        # Flatten the tensor and return the new instance.\n\t        flattened = torch.flatten(\n\t            input=x.tensor,\n\t            start_dim=start_dim,\n\t            end_dim=end_dim,\n\t        )\n\t        return ManifoldTensor(data=flattened, manifold=x.manifold, man_dim=man_dim)\n"]}
{"filename": "hypll/manifolds/euclidean/__init__.py", "chunked_list": ["from .manifold import Euclidean\n"]}
{"filename": "hypll/nn/__init__.py", "chunked_list": ["from .modules.activation import HReLU\n\tfrom .modules.batchnorm import HBatchNorm, HBatchNorm2d\n\tfrom .modules.change_manifold import ChangeManifold\n\tfrom .modules.container import TangentSequential\n\tfrom .modules.convolution import HConvolution2d\n\tfrom .modules.embedding import HEmbedding\n\tfrom .modules.flatten import HFlatten\n\tfrom .modules.linear import HLinear\n\tfrom .modules.pooling import HAvgPool2d, HMaxPool2d\n"]}
{"filename": "hypll/nn/modules/pooling.py", "chunked_list": ["from functools import partial\n\tfrom typing import Optional\n\tfrom torch.nn import Module\n\tfrom torch.nn.common_types import _size_2_t, _size_any_t\n\tfrom torch.nn.functional import max_pool2d\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor\n\tfrom hypll.utils.layer_utils import (\n\t    check_if_man_dims_match,\n\t    check_if_manifolds_match,\n", "    op_in_tangent_space,\n\t)\n\tclass HAvgPool2d(Module):\n\t    def __init__(\n\t        self,\n\t        kernel_size: _size_2_t,\n\t        manifold: Manifold,\n\t        stride: Optional[_size_2_t] = None,\n\t        padding: _size_2_t = 0,\n\t        use_midpoint: bool = False,\n", "    ):\n\t        super().__init__()\n\t        self.kernel_size = (\n\t            kernel_size\n\t            if isinstance(kernel_size, tuple) and len(kernel_size) == 2\n\t            else (kernel_size, kernel_size)\n\t        )\n\t        self.manifold = manifold\n\t        self.stride = stride if (stride is not None) else self.kernel_size\n\t        self.padding = (\n", "            padding if isinstance(padding, tuple) and len(padding) == 2 else (padding, padding)\n\t        )\n\t        self.use_midpoint = use_midpoint\n\t    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n\t        check_if_manifolds_match(layer=self, input=input)\n\t        check_if_man_dims_match(layer=self, man_dim=1, input=input)\n\t        batch_size, channels, height, width = input.size()\n\t        out_height = int((height + 2 * self.padding[0] - self.kernel_size[0]) / self.stride[0] + 1)\n\t        out_width = int((width + 2 * self.padding[1] - self.kernel_size[1]) / self.stride[1] + 1)\n\t        unfolded_input = self.manifold.unfold(\n", "            input=input,\n\t            kernel_size=self.kernel_size,\n\t            padding=self.padding,\n\t            stride=self.stride,\n\t        )\n\t        per_kernel_view = unfolded_input.tensor.view(\n\t            batch_size,\n\t            channels,\n\t            self.kernel_size[0] * self.kernel_size[1],\n\t            unfolded_input.size(-1),\n", "        )\n\t        x = ManifoldTensor(data=per_kernel_view, manifold=self.manifold, man_dim=1)\n\t        if self.use_midpoint:\n\t            aggregates = self.manifold.midpoint(x=x, batch_dim=2)\n\t        else:\n\t            aggregates = self.manifold.frechet_mean(x=x, batch_dim=2)\n\t        return ManifoldTensor(\n\t            data=aggregates.tensor.reshape(batch_size, channels, out_height, out_width),\n\t            manifold=self.manifold,\n\t            man_dim=1,\n", "        )\n\tclass _HMaxPoolNd(Module):\n\t    def __init__(\n\t        self,\n\t        kernel_size: _size_any_t,\n\t        manifold: Manifold,\n\t        stride: Optional[_size_any_t] = None,\n\t        padding: _size_any_t = 0,\n\t        dilation: _size_any_t = 1,\n\t        return_indices: bool = False,\n", "        ceil_mode: bool = False,\n\t    ) -> None:\n\t        super().__init__()\n\t        self.kernel_size = kernel_size\n\t        self.manifold = manifold\n\t        self.stride = stride if (stride is not None) else kernel_size\n\t        self.padding = padding\n\t        self.dilation = dilation\n\t        self.return_indices = return_indices\n\t        self.ceil_mode = ceil_mode\n", "class HMaxPool2d(_HMaxPoolNd):\n\t    kernel_size: _size_2_t\n\t    stride: _size_2_t\n\t    padding: _size_2_t\n\t    dilation: _size_2_t\n\t    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n\t        check_if_manifolds_match(layer=self, input=input)\n\t        # TODO: check if defining this partial func each forward pass is slow.\n\t        # If it is, put this stuff inside the init or add kwargs to op_in_tangent_space.\n\t        max_pool2d_partial = partial(\n", "            max_pool2d,\n\t            kernel_size=self.kernel_size,\n\t            stride=self.stride,\n\t            padding=self.padding,\n\t            dilation=self.dilation,\n\t            ceil_mode=self.ceil_mode,\n\t            return_indices=self.return_indices,\n\t        )\n\t        return op_in_tangent_space(op=max_pool2d_partial, manifold=self.manifold, input=input)\n"]}
{"filename": "hypll/nn/modules/change_manifold.py", "chunked_list": ["from torch import Tensor\n\tfrom torch.nn import Module\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor, TangentTensor\n\tclass ChangeManifold(Module):\n\t    \"\"\"Changes the manifold of the input manifold tensor to the target manifold.\n\t    Attributes:\n\t        target_manifold:\n\t            Manifold the output tensor should be on.\n\t    \"\"\"\n", "    def __init__(self, target_manifold: Manifold):\n\t        super(ChangeManifold, self).__init__()\n\t        self.target_manifold = target_manifold\n\t    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n\t        \"\"\"Changes the manifold of the input tensor to self.target_manifold.\n\t        By default, applies logmap and expmap at the origin to switch between\n\t        manifold. Applies shortcuts if possible.\n\t        Args:\n\t            x:\n\t                Input manifold tensor.\n", "        Returns:\n\t            Tensor on the target manifold.\n\t        \"\"\"\n\t        match (x.manifold, self.target_manifold):\n\t            # TODO(Philipp, 05/23): Apply shortcuts where possible: For example,\n\t            # case (PoincareBall(), PoincareBall()): ...\n\t            #\n\t            # By default resort to logmap + expmap at the origin.\n\t            case _:\n\t                tangent_tensor = x.manifold.logmap(None, x)\n", "                return self.target_manifold.expmap(tangent_tensor)\n"]}
{"filename": "hypll/nn/modules/embedding.py", "chunked_list": ["from torch import Tensor, empty, no_grad, normal, ones_like, zeros_like\n\tfrom torch.nn import Module\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n\tclass HEmbedding(Module):\n\t    def __init__(\n\t        self,\n\t        num_embeddings: int,\n\t        embedding_dim: int,\n\t        manifold: Manifold,\n", "    ) -> None:\n\t        super(HEmbedding, self).__init__()\n\t        self.num_embeddings = num_embeddings\n\t        self.embedding_dim = embedding_dim\n\t        self.manifold = manifold\n\t        self.weight = ManifoldParameter(\n\t            data=empty((num_embeddings, embedding_dim)), manifold=manifold, man_dim=-1\n\t        )\n\t        self.reset_parameters()\n\t    def reset_parameters(self):\n", "        with no_grad():\n\t            new_weight = normal(\n\t                mean=zeros_like(self.weight.tensor),\n\t                std=ones_like(self.weight.tensor),\n\t            )\n\t            new_weight = TangentTensor(\n\t                data=new_weight,\n\t                manifold_points=None,\n\t                manifold=self.manifold,\n\t                man_dim=-1,\n", "            )\n\t            self.weight.copy_(self.manifold.expmap(new_weight).tensor)\n\t    def forward(self, input: Tensor) -> ManifoldTensor:\n\t        return self.weight[input]\n"]}
{"filename": "hypll/nn/modules/convolution.py", "chunked_list": ["from torch.nn import Module\n\tfrom torch.nn.common_types import _size_1_t, _size_2_t\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor\n\tfrom hypll.utils.layer_utils import check_if_man_dims_match, check_if_manifolds_match\n\tclass HConvolution2d(Module):\n\t    \"\"\"Applies a 2D convolution over a hyperbolic input signal.\n\t    Attributes:\n\t        in_channels:\n\t            Number of channels in the input image.\n", "        out_channels:\n\t            Number of channels produced by the convolution.\n\t        kernel_size:\n\t            Size of the convolving kernel.\n\t        manifold:\n\t            Hyperbolic manifold of the tensors.\n\t        bias:\n\t            If True, adds a learnable bias to the output. Default: True\n\t        stride:\n\t            Stride of the convolution. Default: 1\n", "        padding:\n\t            Padding added to all four sides of the input. Default: 0\n\t        id_init:\n\t            Use identity initialization (True) if appropriate or use HNN++ initialization (False).\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        out_channels: int,\n\t        kernel_size: _size_2_t,\n", "        manifold: Manifold,\n\t        bias: bool = True,\n\t        stride: int = 1,\n\t        padding: int = 0,\n\t        id_init: bool = True,\n\t    ) -> None:\n\t        super(HConvolution2d, self).__init__()\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.kernel_size = (\n", "            kernel_size\n\t            if isinstance(kernel_size, tuple) and len(kernel_size) == 2\n\t            else (kernel_size, kernel_size)\n\t        )\n\t        self.kernel_vol = self.kernel_size[0] * self.kernel_size[1]\n\t        self.manifold = manifold\n\t        self.stride = stride\n\t        self.padding = padding\n\t        self.id_init = id_init\n\t        self.has_bias = bias\n", "        self.weights, self.bias = self.manifold.construct_dl_parameters(\n\t            in_features=self.kernel_vol * in_channels,\n\t            out_features=out_channels,\n\t            bias=self.has_bias,\n\t        )\n\t        self.reset_parameters()\n\t    def reset_parameters(self) -> None:\n\t        \"\"\"Resets parameter weights based on the manifold.\"\"\"\n\t        self.manifold.reset_parameters(weight=self.weights, bias=self.bias)\n\t    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n", "        \"\"\"Does a forward pass of the 2D convolutional layer.\n\t        Args:\n\t            x:\n\t                Manifold tensor of shape (B, C_in, H, W) with manifold dimension 1.\n\t        Returns:\n\t            Manifold tensor of shape (B, C_in, H_out, W_out) with manifold dimension 1.\n\t        Raises:\n\t            ValueError: If the manifolds or manifold dimensions don't match.\n\t        \"\"\"\n\t        check_if_manifolds_match(layer=self, input=x)\n", "        check_if_man_dims_match(layer=self, man_dim=1, input=x)\n\t        batch_size, height, width = x.size(0), x.size(2), x.size(3)\n\t        out_height = _output_side_length(\n\t            input_side_length=height,\n\t            kernel_size=self.kernel_size[0],\n\t            padding=self.padding,\n\t            stride=self.stride,\n\t        )\n\t        out_width = _output_side_length(\n\t            input_side_length=width,\n", "            kernel_size=self.kernel_size[1],\n\t            padding=self.padding,\n\t            stride=self.stride,\n\t        )\n\t        x = self.manifold.unfold(\n\t            input=x,\n\t            kernel_size=self.kernel_size,\n\t            padding=self.padding,\n\t            stride=self.stride,\n\t        )\n", "        x = self.manifold.fully_connected(x=x, z=self.weights, bias=self.bias)\n\t        x = ManifoldTensor(\n\t            data=x.tensor.reshape(batch_size, self.out_channels, out_height, out_width),\n\t            manifold=x.manifold,\n\t            man_dim=1,\n\t        )\n\t        return x\n\tdef _output_side_length(\n\t    input_side_length: int, kernel_size: _size_1_t, padding: int, stride: int\n\t) -> int:\n", "    \"\"\"Calculates the output side length of the kernel.\n\t    Based on https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html.\n\t    \"\"\"\n\t    if kernel_size > input_side_length:\n\t        raise RuntimeError(\n\t            f\"Encountered invalid kernel size {kernel_size} \"\n\t            f\"larger than input side length {input_side_length}\"\n\t        )\n\t    if stride > input_side_length:\n\t        raise RuntimeError(\n", "            f\"Encountered invalid stride {stride} \"\n\t            f\"larger than input side length {input_side_length}\"\n\t        )\n\t    return 1 + (input_side_length + 2 * padding - (kernel_size - 1) - 1) // stride\n"]}
{"filename": "hypll/nn/modules/fold.py", "chunked_list": ["from torch.nn import Module\n\tfrom torch.nn.common_types import _size_2_t\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor\n\tfrom hypll.utils.layer_utils import check_if_man_dims_match, check_if_manifolds_match\n\tclass HUnfold(Module):\n\t    def __init__(\n\t        self,\n\t        kernel_size: _size_2_t,\n\t        manifold: Manifold,\n", "        dilation: _size_2_t = 1,\n\t        padding: _size_2_t = 0,\n\t        stride: _size_2_t = 1,\n\t    ) -> None:\n\t        self.kernel_size = kernel_size\n\t        self.manifold = manifold\n\t        self.dilation = dilation\n\t        self.padding = padding\n\t        self.stride = stride\n\t    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n", "        check_if_manifolds_match(layer=self, input=input)\n\t        check_if_man_dims_match(layer=self, man_dim=1, input=input)\n\t        return self.manifold.unfold(\n\t            input=input,\n\t            kernel_size=self.kernel_size,\n\t            dilation=self.dilation,\n\t            padding=self.padding,\n\t            stride=self.stride,\n\t        )\n"]}
{"filename": "hypll/nn/modules/container.py", "chunked_list": ["from torch.nn import Module, Sequential\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor\n\tfrom hypll.utils.layer_utils import check_if_manifolds_match\n\tclass TangentSequential(Module):\n\t    def __init__(self, seq: Sequential, manifold: Manifold) -> None:\n\t        super(TangentSequential, self).__init__()\n\t        self.seq = seq\n\t        self.manifold = manifold\n\t    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n", "        check_if_manifolds_match(layer=self, input=input)\n\t        man_dim = input.man_dim\n\t        input = self.manifold.logmap(x=None, y=input)\n\t        for module in self.seq:\n\t            input.tensor = module(input.tensor)\n\t        return self.manifold.expmap(input)\n"]}
{"filename": "hypll/nn/modules/__init__.py", "chunked_list": []}
{"filename": "hypll/nn/modules/linear.py", "chunked_list": ["from torch.nn import Module\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor\n\tfrom hypll.utils.layer_utils import check_if_man_dims_match, check_if_manifolds_match\n\tclass HLinear(Module):\n\t    \"\"\"Poincare fully connected linear layer\"\"\"\n\t    def __init__(\n\t        self,\n\t        in_features: int,\n\t        out_features: int,\n", "        manifold: Manifold,\n\t        bias: bool = True,\n\t    ) -> None:\n\t        super(HLinear, self).__init__()\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.manifold = manifold\n\t        self.has_bias = bias\n\t        # TODO: torch stores weights transposed supposedly due to efficiency\n\t        # https://discuss.pytorch.org/t/why-does-the-linear-module-seems-to-do-unnecessary-transposing/6277/7\n", "        # We may want to do the same\n\t        self.z, self.bias = self.manifold.construct_dl_parameters(\n\t            in_features=in_features, out_features=out_features, bias=self.has_bias\n\t        )\n\t        self.reset_parameters()\n\t    def reset_parameters(self) -> None:\n\t        self.manifold.reset_parameters(self.z, self.bias)\n\t    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n\t        check_if_manifolds_match(layer=self, input=x)\n\t        check_if_man_dims_match(layer=self, man_dim=-1, input=x)\n", "        return self.manifold.fully_connected(x=x, z=self.z, bias=self.bias)\n"]}
{"filename": "hypll/nn/modules/batchnorm.py", "chunked_list": ["from torch import tensor, zeros\n\tfrom torch.nn import Module, Parameter\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor, TangentTensor\n\tfrom hypll.utils.layer_utils import check_if_manifolds_match\n\tclass HBatchNorm(Module):\n\t    \"\"\"\n\t    Basic implementation of hyperbolic batch normalization.\n\t    Based on:\n\t        https://arxiv.org/abs/2003.00335\n", "    \"\"\"\n\t    def __init__(\n\t        self,\n\t        features: int,\n\t        manifold: Manifold,\n\t        use_midpoint: bool = False,\n\t    ) -> None:\n\t        super(HBatchNorm, self).__init__()\n\t        self.features = features\n\t        self.manifold = manifold\n", "        self.use_midpoint = use_midpoint\n\t        # TODO: Store bias on manifold\n\t        self.bias = Parameter(zeros(features))\n\t        self.weight = Parameter(tensor(1.0))\n\t    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n\t        check_if_manifolds_match(layer=self, input=x)\n\t        bias_on_manifold = self.manifold.expmap(\n\t            v=TangentTensor(data=self.bias, manifold_points=None, manifold=self.manifold)\n\t        )\n\t        if self.use_midpoint:\n", "            input_mean = self.manifold.midpoint(x=x, batch_dim=0)\n\t        else:\n\t            input_mean = self.manifold.frechet_mean(x=x, batch_dim=0)\n\t        input_var = self.manifold.frechet_variance(x=x, mu=input_mean, batch_dim=0)\n\t        input_logm = self.manifold.transp(\n\t            v=self.manifold.logmap(input_mean, x),\n\t            y=bias_on_manifold,\n\t        )\n\t        input_logm.tensor = (self.weight / (input_var + 1e-6)).sqrt() * input_logm.tensor\n\t        output = self.manifold.expmap(input_logm)\n", "        return output\n\tclass HBatchNorm2d(Module):\n\t    \"\"\"\n\t    2D implementation of hyperbolic batch normalization.\n\t    Based on:\n\t        https://arxiv.org/abs/2003.00335\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        features: int,\n", "        manifold: Manifold,\n\t        use_midpoint: bool = False,\n\t    ) -> None:\n\t        super(HBatchNorm2d, self).__init__()\n\t        self.features = features\n\t        self.manifold = manifold\n\t        self.use_midpoint = use_midpoint\n\t        self.norm = HBatchNorm(\n\t            features=features,\n\t            manifold=manifold,\n", "            use_midpoint=use_midpoint,\n\t        )\n\t    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n\t        check_if_manifolds_match(layer=self, input=x)\n\t        batch_size, height, width = x.size(0), x.size(2), x.size(3)\n\t        flat_x = ManifoldTensor(\n\t            data=x.tensor.permute(0, 2, 3, 1).flatten(start_dim=0, end_dim=2),\n\t            manifold=x.manifold,\n\t            man_dim=-1,\n\t        )\n", "        flat_x = self.norm(flat_x)\n\t        new_tensor = flat_x.tensor.reshape(batch_size, height, width, self.features).permute(\n\t            0, 3, 1, 2\n\t        )\n\t        return ManifoldTensor(data=new_tensor, manifold=x.manifold, man_dim=1)\n"]}
{"filename": "hypll/nn/modules/activation.py", "chunked_list": ["from torch.nn import Module\n\tfrom torch.nn.functional import relu\n\tfrom hypll.manifolds import Manifold\n\tfrom hypll.tensors import ManifoldTensor\n\tfrom hypll.utils.layer_utils import check_if_manifolds_match, op_in_tangent_space\n\tclass HReLU(Module):\n\t    def __init__(self, manifold: Manifold) -> None:\n\t        super(HReLU, self).__init__()\n\t        self.manifold = manifold\n\t    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n", "        check_if_manifolds_match(layer=self, input=input)\n\t        return op_in_tangent_space(\n\t            op=relu,\n\t            manifold=self.manifold,\n\t            input=input,\n\t        )\n"]}
{"filename": "hypll/nn/modules/flatten.py", "chunked_list": ["from torch.nn import Module, functional\n\tfrom hypll.tensors import ManifoldTensor\n\tclass HFlatten(Module):\n\t    \"\"\"Flattens a contiguous range of dims into a tensor.\n\t    Attributes:\n\t        start_dim:\n\t            First dimension to flatten (default = 1).\n\t        end_dim:\n\t            Last dimension to flatten (default = -1).\n\t    \"\"\"\n", "    def __init__(self, start_dim: int = 1, end_dim: int = -1):\n\t        super(HFlatten, self).__init__()\n\t        self.start_dim = start_dim\n\t        self.end_dim = end_dim\n\t    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n\t        \"\"\"Flattens the manifold input tensor.\"\"\"\n\t        return x.flatten(start_dim=self.start_dim, end_dim=self.end_dim)\n"]}
{"filename": "tutorials/cifar10_resnet_tutorial.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\tTraining a Poincare ResNet\n\t==========================\n\tThis is an implementation based on the Poincare Resnet paper, which can be found at:\n\t- https://arxiv.org/abs/2303.14027\n\tDue to the complexity of hyperbolic operations we strongly advise to only run this tutorial with a \n\tGPU.\n\tWe will perform the following steps in order:\n\t1. Define a hyperbolic manifold\n", "2. Load and normalize the CIFAR10 training and test datasets using ``torchvision``\n\t3. Define a Poincare ResNet\n\t4. Define a loss function and optimizer\n\t5. Train the network on the training data\n\t6. Test the network on the test data\n\t\"\"\"\n\t##############################\n\t# 0. Grab the available device\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\timport torch\n", "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\t#############################\n\t# 1. Define the Poincare ball\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\tfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\t# Making the curvature a learnable parameter is usually suboptimal but can\n\t# make training smoother. An initial curvature of 0.1 has also been shown\n\t# to help during training.\n\tmanifold = PoincareBall(c=Curvature(value=0.1, requires_grad=True))\n\t###############################\n", "# 2. Load and normalize CIFAR10\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\timport torchvision\n\timport torchvision.transforms as transforms\n\t########################################################################\n\t# .. note::\n\t#     If running on Windows and you get a BrokenPipeError, try setting\n\t#     the num_worker of torch.utils.data.DataLoader() to 0.\n\ttransform = transforms.Compose(\n\t    [\n", "        transforms.ToTensor(),\n\t        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n\t    ]\n\t)\n\tbatch_size = 128\n\ttrainset = torchvision.datasets.CIFAR10(\n\t    root=\"./data\", train=True, download=True, transform=transform\n\t)\n\ttrainloader = torch.utils.data.DataLoader(\n\t    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n", ")\n\ttestset = torchvision.datasets.CIFAR10(\n\t    root=\"./data\", train=False, download=True, transform=transform\n\t)\n\ttestloader = torch.utils.data.DataLoader(\n\t    testset, batch_size=batch_size, shuffle=False, num_workers=2\n\t)\n\t###############################\n\t# 3. Define a Poincare ResNet\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n", "# This implementation is based on the Poincare ResNet paper, which can\n\t# be found at https://arxiv.org/abs/2303.14027 and which, in turn, is\n\t# based on the original Euclidean implementation described in the paper\n\t# Deep Residual Learning for Image Recognition by He et al. from 2015:\n\t# https://arxiv.org/abs/1512.03385.\n\tfrom typing import Optional\n\tfrom torch import nn\n\tfrom hypll import nn as hnn\n\tfrom hypll.tensors import ManifoldTensor\n\tclass PoincareResidualBlock(nn.Module):\n", "    def __init__(\n\t        self,\n\t        in_channels: int,\n\t        out_channels: int,\n\t        manifold: PoincareBall,\n\t        stride: int = 1,\n\t        downsample: Optional[nn.Sequential] = None,\n\t    ):\n\t        # We can replace each operation in the usual ResidualBlock by a manifold-agnostic\n\t        # operation and supply the PoincareBall object to these operations.\n", "        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.manifold = manifold\n\t        self.stride = stride\n\t        self.downsample = downsample\n\t        self.conv1 = hnn.HConvolution2d(\n\t            in_channels=in_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=3,\n", "            manifold=manifold,\n\t            stride=stride,\n\t            padding=1,\n\t        )\n\t        self.bn1 = hnn.HBatchNorm2d(features=out_channels, manifold=manifold)\n\t        self.relu = hnn.HReLU(manifold=self.manifold)\n\t        self.conv2 = hnn.HConvolution2d(\n\t            in_channels=out_channels,\n\t            out_channels=out_channels,\n\t            kernel_size=3,\n", "            manifold=manifold,\n\t            padding=1,\n\t        )\n\t        self.bn2 = hnn.HBatchNorm2d(features=out_channels, manifold=manifold)\n\t    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n\t        residual = x\n\t        x = self.conv1(x)\n\t        x = self.bn1(x)\n\t        x = self.relu(x)\n\t        x = self.conv2(x)\n", "        x = self.bn2(x)\n\t        if self.downsample is not None:\n\t            residual = self.downsample(residual)\n\t        # We replace the addition operation inside the skip connection by a Mobius addition.\n\t        x = self.manifold.mobius_add(x, residual)\n\t        x = self.relu(x)\n\t        return x\n\tclass PoincareResNet(nn.Module):\n\t    def __init__(\n\t        self,\n", "        channel_sizes: list[int],\n\t        group_depths: list[int],\n\t        manifold: PoincareBall,\n\t    ):\n\t        # For the Poincare ResNet itself we again replace each layer by a manifold-agnostic one\n\t        # and supply the PoincareBall to each of these. We also replace the ResidualBlocks by\n\t        # the manifold-agnostic one defined above.\n\t        super().__init__()\n\t        self.channel_sizes = channel_sizes\n\t        self.group_depths = group_depths\n", "        self.manifold = manifold\n\t        self.conv = hnn.HConvolution2d(\n\t            in_channels=3,\n\t            out_channels=channel_sizes[0],\n\t            kernel_size=3,\n\t            manifold=manifold,\n\t            padding=1,\n\t        )\n\t        self.bn = hnn.HBatchNorm2d(features=channel_sizes[0], manifold=manifold)\n\t        self.relu = hnn.HReLU(manifold=manifold)\n", "        self.group1 = self._make_group(\n\t            in_channels=channel_sizes[0],\n\t            out_channels=channel_sizes[0],\n\t            depth=group_depths[0],\n\t        )\n\t        self.group2 = self._make_group(\n\t            in_channels=channel_sizes[0],\n\t            out_channels=channel_sizes[1],\n\t            depth=group_depths[1],\n\t            stride=2,\n", "        )\n\t        self.group3 = self._make_group(\n\t            in_channels=channel_sizes[1],\n\t            out_channels=channel_sizes[2],\n\t            depth=group_depths[2],\n\t            stride=2,\n\t        )\n\t        self.avg_pool = hnn.HAvgPool2d(kernel_size=8, manifold=manifold)\n\t        self.fc = hnn.HLinear(in_features=channel_sizes[2], out_features=10, manifold=manifold)\n\t    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n", "        x = self.conv(x)\n\t        x = self.bn(x)\n\t        x = self.relu(x)\n\t        x = self.group1(x)\n\t        x = self.group2(x)\n\t        x = self.group3(x)\n\t        x = self.avg_pool(x)\n\t        x = self.fc(x.squeeze())\n\t        return x\n\t    def _make_group(\n", "        self,\n\t        in_channels: int,\n\t        out_channels: int,\n\t        depth: int,\n\t        stride: int = 1,\n\t    ) -> nn.Sequential:\n\t        if stride == 1:\n\t            downsample = None\n\t        else:\n\t            downsample = hnn.HConvolution2d(\n", "                in_channels=in_channels,\n\t                out_channels=out_channels,\n\t                kernel_size=1,\n\t                manifold=self.manifold,\n\t                stride=stride,\n\t            )\n\t        layers = [\n\t            PoincareResidualBlock(\n\t                in_channels=in_channels,\n\t                out_channels=out_channels,\n", "                manifold=self.manifold,\n\t                stride=stride,\n\t                downsample=downsample,\n\t            )\n\t        ]\n\t        for _ in range(1, depth):\n\t            layers.append(\n\t                PoincareResidualBlock(\n\t                    in_channels=out_channels,\n\t                    out_channels=out_channels,\n", "                    manifold=self.manifold,\n\t                )\n\t            )\n\t        return nn.Sequential(*layers)\n\t# Now, let's create a thin Poincare ResNet with channel sizes [4, 8, 16] and with a depth of 20\n\t# layers.\n\tnet = PoincareResNet(\n\t    channel_sizes=[4, 8, 16],\n\t    group_depths=[3, 3, 3],\n\t    manifold=manifold,\n", ").to(device)\n\t#########################################\n\t# 4. Define a Loss function and optimizer\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t# Let's use a Classification Cross-Entropy loss and RiemannianAdam optimizer.\n\tcriterion = nn.CrossEntropyLoss()\n\t# net.parameters() includes the learnable curvature \"c\" of the manifold.\n\tfrom hypll.optim import RiemannianAdam\n\toptimizer = RiemannianAdam(net.parameters(), lr=0.001)\n\t######################\n", "# 5. Train the network\n\t# ^^^^^^^^^^^^^^^^^^^^\n\t# We simply have to loop over our data iterator, project the inputs onto the\n\t# manifold, and feed them to the network and optimize. We will train for a limited\n\t# number of epochs here due to the long training time of this model.\n\tfrom hypll.tensors import TangentTensor\n\tfor epoch in range(2):  # Increase this number to at least 100 for good results\n\t    running_loss = 0.0\n\t    for i, data in enumerate(trainloader, 0):\n\t        # get the inputs; data is a list of [inputs, labels]\n", "        inputs, labels = data[0].to(device), data[1].to(device)\n\t        # move the inputs to the manifold\n\t        tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n\t        manifold_inputs = manifold.expmap(tangents)\n\t        # zero the parameter gradients\n\t        optimizer.zero_grad()\n\t        # forward + backward + optimize\n\t        outputs = net(manifold_inputs)\n\t        loss = criterion(outputs.tensor, labels)\n\t        loss.backward()\n", "        optimizer.step()\n\t        # print statistics\n\t        running_loss += loss.item()\n\t        print(f\"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}\")\n\t        running_loss = 0.0\n\tprint(\"Finished Training\")\n\t######################################\n\t# 6. Test the network on the test data\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t#\n", "# Let us look at how the network performs on the whole dataset.\n\tcorrect = 0\n\ttotal = 0\n\t# since we're not training, we don't need to calculate the gradients for our outputs\n\twith torch.no_grad():\n\t    for data in testloader:\n\t        images, labels = data[0].to(device), data[1].to(device)\n\t        # move the images to the manifold\n\t        tangents = TangentTensor(data=images, man_dim=1, manifold=manifold)\n\t        manifold_images = manifold.expmap(tangents)\n", "        # calculate outputs by running images through the network\n\t        outputs = net(manifold_images)\n\t        # the class with the highest energy is what we choose as prediction\n\t        _, predicted = torch.max(outputs.tensor, 1)\n\t        total += labels.size(0)\n\t        correct += (predicted == labels).sum().item()\n\tprint(f\"Accuracy of the network on the 10000 test images: {100 * correct // total} %\")\n"]}
{"filename": "tutorials/poincare_embeddings_tutorial.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\tTraining Poincare embeddings for the mammals subset from WordNet\n\t================================================================\n\tThis implementation is based on the \"Poincare Embeddings for Learning Hierarchical Representations\"\n\tpaper by Maximilian Nickel and Douwe Kiela, which can be found at: \n\t- https://arxiv.org/pdf/1705.08039.pdf\n\tTheir implementation can be found at:\n\t- https://github.com/facebookresearch/poincare-embeddings\n\tWe will perform the following steps in order:\n", "1. Load the mammals subset from the WordNet hierarchy using NetworkX\n\t2. Create a dataset containing the graph from which we can sample\n\t3. Initialize the Poincare ball on which the embeddings will be trained\n\t4. Define the Poincare embedding model\n\t5. Define the Poincare embedding loss function\n\t6. Perform a few \"burn-in\" training epochs with reduced learning rate\n\t\"\"\"\n\t######################################################################\n\t# 1. Load the mammals subset from the WordNet hierarchy using NetworkX\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n", "import json\n\timport os\n\timport networkx as nx\n\t# If you have stored the wordnet_mammals.json file differently that the definition here,\n\t# adjust the mammals_json_file string to correctly point to this json file. The file itself can\n\t# be found in the repository under tutorials/data/wordnet_mammals.json.\n\troot = os.path.dirname(os.path.abspath(__file__))\n\tmammals_json_file = os.path.join(root, \"data\", \"wordnet_mammals.json\")\n\twith open(mammals_json_file, \"r\") as json_file:\n\t    graph_dict = json.load(json_file)\n", "mammals_graph = nx.node_link_graph(graph_dict)\n\t###################################################################\n\t# 2. Create a dataset containing the graph from which we can sample\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\timport random\n\timport torch\n\tfrom torch.utils.data import DataLoader, Dataset\n\tclass MammalsEmbeddingDataset(Dataset):\n\t    def __init__(\n\t        self,\n", "        mammals: nx.DiGraph,\n\t    ):\n\t        super().__init__()\n\t        self.mammals = mammals\n\t        self.edges_list = list(mammals.edges())\n\t    # This Dataset object has a sample for each edge in the graph.\n\t    def __len__(self) -> int:\n\t        return len(self.edges_list)\n\t    def __getitem__(self, idx: int):\n\t        # For each existing edge in the graph we choose 10 fake or negative edges, which we build\n", "        # from the idx-th existing edge. So, first we grab this edge from the graph.\n\t        rel = self.edges_list[idx]\n\t        # Next, we take our source node rel[0] and see which nodes in the graph are not a child of\n\t        # this node.\n\t        negative_target_nodes = list(\n\t            self.mammals.nodes() - nx.descendants(self.mammals, rel[0]) - {rel[0]}\n\t        )\n\t        # Then, we sample at most 5 of these negative target nodes...\n\t        negative_target_sample_size = min(5, len(negative_target_nodes))\n\t        negative_target_nodes_sample = random.sample(\n", "            negative_target_nodes, negative_target_sample_size\n\t        )\n\t        # and add these to a tensor which will be used as input for our embedding model.\n\t        edges = torch.tensor([rel] + [[rel[0], neg] for neg in negative_target_nodes_sample])\n\t        # Next, we do the same with our target node rel[1], but now where we sample from nodes\n\t        # which aren't a parent of it.\n\t        negative_source_nodes = list(\n\t            self.mammals.nodes() - nx.ancestors(self.mammals, rel[1]) - {rel[1]}\n\t        )\n\t        # We sample from these negative source nodes until we have a total of 10 negative edges...\n", "        negative_source_sample_size = 10 - negative_target_sample_size\n\t        negative_source_nodes_sample = random.sample(\n\t            negative_source_nodes, negative_source_sample_size\n\t        )\n\t        # and add these to the tensor that we created above.\n\t        edges = torch.cat(\n\t            tensors=(edges, torch.tensor([[neg, rel[1]] for neg in negative_source_nodes_sample])),\n\t            dim=0,\n\t        )\n\t        # Lastly, we create a tensor containing the labels of the edges, indicating whether it's a\n", "        # True or a False edge.\n\t        edge_label_targets = torch.cat(tensors=[torch.ones(1).bool(), torch.zeros(10).bool()])\n\t        return edges, edge_label_targets\n\t# Now, we construct the dataset.\n\tdataset = MammalsEmbeddingDataset(\n\t    mammals=mammals_graph,\n\t)\n\tdataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n\t#########################################################################\n\t# 3. Initialize the Poincare ball on which the embeddings will be trained\n", "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\tfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\tpoincare_ball = PoincareBall(Curvature(1.0))\n\t########################################\n\t# 4. Define the Poincare embedding model\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\timport hypll.nn as hnn\n\tclass PoincareEmbedding(hnn.HEmbedding):\n\t    def __init__(\n\t        self,\n", "        num_embeddings: int,\n\t        embedding_dim: int,\n\t        manifold: PoincareBall,\n\t    ):\n\t        super().__init__(num_embeddings, embedding_dim, manifold)\n\t    # The model outputs the distances between the nodes involved in the input edges as these are\n\t    # used to compute the loss.\n\t    def forward(self, edges: torch.Tensor) -> torch.Tensor:\n\t        embeddings = super().forward(edges)\n\t        edge_distances = self.manifold.dist(x=embeddings[:, :, 0, :], y=embeddings[:, :, 1, :])\n", "        return edge_distances\n\t# We want to embed every node into a 2-dimensional Poincare ball.\n\tmodel = PoincareEmbedding(\n\t    num_embeddings=len(mammals_graph.nodes()),\n\t    embedding_dim=2,\n\t    manifold=poincare_ball,\n\t)\n\t################################################\n\t# 5. Define the Poincare embedding loss function\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n", "# This function is given in equation (5) of the Poincare Embeddings paper.\n\tdef poincare_embeddings_loss(dists: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n\t    logits = dists.neg().exp()\n\t    numerator = torch.where(condition=targets, input=logits, other=0).sum(dim=-1)\n\t    denominator = logits.sum(dim=-1)\n\t    loss = (numerator / denominator).log().mean().neg()\n\t    return loss\n\t#######################################################################\n\t# 6. Perform a few \"burn-in\" training epochs with reduced learning rate\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n", "from hypll.optim import RiemannianSGD\n\t# The learning rate of 0.3 is dived by 10 during burn-in.\n\toptimizer = RiemannianSGD(\n\t    params=model.parameters(),\n\t    lr=0.3 / 10,\n\t)\n\t# Perform training as we would usually.\n\tfor epoch in range(10):\n\t    average_loss = 0\n\t    for idx, (edges, edge_label_targets) in enumerate(dataloader):\n", "        optimizer.zero_grad()\n\t        dists = model(edges)\n\t        loss = poincare_embeddings_loss(dists=dists, targets=edge_label_targets)\n\t        loss.backward()\n\t        optimizer.step()\n\t        average_loss += loss\n\t    average_loss /= len(dataloader)\n\t    print(f\"Burn-in epoch {epoch} loss: {average_loss}\")\n\t#########################\n\t# 6. Train the embeddings\n", "# ^^^^^^^^^^^^^^^^^^^^^^^\n\t# Now we use the actual learning rate 0.3.\n\toptimizer = RiemannianSGD(\n\t    params=model.parameters(),\n\t    lr=0.3,\n\t)\n\tfor epoch in range(300):\n\t    average_loss = 0\n\t    for idx, (edges, edge_label_targets) in enumerate(dataloader):\n\t        optimizer.zero_grad()\n", "        dists = model(edges)\n\t        loss = poincare_embeddings_loss(dists=dists, targets=edge_label_targets)\n\t        loss.backward()\n\t        optimizer.step()\n\t        average_loss += loss\n\t    average_loss /= len(dataloader)\n\t    print(f\"Epoch {epoch} loss: {average_loss}\")\n\t# You have now trained your own Poincare Embeddings!\n"]}
{"filename": "tutorials/cifar10_tutorial.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t\"\"\"\n\tTraining a Hyperbolic Classifier\n\t================================\n\tThis is an adaptation of torchvision's tutorial \"Training a Classifier\" to \n\thyperbolic space. The original tutorial can be found here:\n\t- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n\tTraining a Hyperbolic Image Classifier\n\t--------------------------------------\n\tWe will do the following steps in order:\n", "1. Define a hyperbolic manifold\n\t2. Load and normalize the CIFAR10 training and test datasets using ``torchvision``\n\t3. Define a hyperbolic Convolutional Neural Network\n\t4. Define a loss function and optimizer\n\t5. Train the network on the training data\n\t6. Test the network on the test data\n\t\"\"\"\n\t########################################################################\n\t# 1. Define a hyperbolic manifold\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n", "# We use the Poincaré ball model for the purposes of this tutorial.\n\tfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\t# Making the curvature a learnable parameter is usually suboptimal but can\n\t# make training smoother.\n\tmanifold = PoincareBall(c=Curvature(requires_grad=True))\n\t########################################################################\n\t# 2. Load and normalize CIFAR10\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\timport torch\n\timport torchvision\n", "import torchvision.transforms as transforms\n\t########################################################################\n\t# .. note::\n\t#     If running on Windows and you get a BrokenPipeError, try setting\n\t#     the num_worker of torch.utils.data.DataLoader() to 0.\n\ttransform = transforms.Compose(\n\t    [\n\t        transforms.ToTensor(),\n\t        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n\t    ]\n", ")\n\tbatch_size = 4\n\ttrainset = torchvision.datasets.CIFAR10(\n\t    root=\"./data\", train=True, download=True, transform=transform\n\t)\n\ttrainloader = torch.utils.data.DataLoader(\n\t    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n\t)\n\ttestset = torchvision.datasets.CIFAR10(\n\t    root=\"./data\", train=False, download=True, transform=transform\n", ")\n\ttestloader = torch.utils.data.DataLoader(\n\t    testset, batch_size=batch_size, shuffle=False, num_workers=2\n\t)\n\tclasses = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n\t########################################################################\n\t# 3. Define a hyperbolic Convolutional Neural Network\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t# Let's rebuild the convolutional neural network from torchvision's tutorial\n\t# using hyperbolic modules.\n", "from torch import nn\n\tfrom hypll import nn as hnn\n\tclass Net(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.conv1 = hnn.HConvolution2d(\n\t            in_channels=3, out_channels=6, kernel_size=5, manifold=manifold\n\t        )\n\t        self.pool = hnn.HMaxPool2d(kernel_size=2, manifold=manifold, stride=2)\n\t        self.conv2 = hnn.HConvolution2d(\n", "            in_channels=6, out_channels=16, kernel_size=5, manifold=manifold\n\t        )\n\t        self.fc1 = hnn.HLinear(in_features=16 * 5 * 5, out_features=120, manifold=manifold)\n\t        self.fc2 = hnn.HLinear(in_features=120, out_features=84, manifold=manifold)\n\t        self.fc3 = hnn.HLinear(in_features=84, out_features=10, manifold=manifold)\n\t        self.relu = hnn.HReLU(manifold=manifold)\n\t    def forward(self, x):\n\t        x = self.pool(self.relu(self.conv1(x)))\n\t        x = self.pool(self.relu(self.conv2(x)))\n\t        x = x.flatten(start_dim=1)\n", "        x = self.relu(self.fc1(x))\n\t        x = self.relu(self.fc2(x))\n\t        x = self.fc3(x)\n\t        return x\n\tnet = Net()\n\t########################################################################\n\t# 4. Define a Loss function and optimizer\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t# Let's use a Classification Cross-Entropy loss and RiemannianAdam optimizer.\n\t# Adam is preferred because hyperbolic linear layers can sometimes have training\n", "# difficulties early on due to poor initialization.\n\tcriterion = nn.CrossEntropyLoss()\n\t# net.parameters() includes the learnable curvature \"c\" of the manifold.\n\tfrom hypll.optim import RiemannianAdam\n\toptimizer = RiemannianAdam(net.parameters(), lr=0.001)\n\t########################################################################\n\t# 5. Train the network\n\t# ^^^^^^^^^^^^^^^^^^^^\n\t# This is when things start to get interesting.\n\t# We simply have to loop over our data iterator, project the inputs onto the\n", "# manifold, and feed them to the network and optimize.\n\tfrom hypll.tensors import TangentTensor\n\tfor epoch in range(2):  # loop over the dataset multiple times\n\t    running_loss = 0.0\n\t    for i, data in enumerate(trainloader, 0):\n\t        # get the inputs; data is a list of [inputs, labels]\n\t        inputs, labels = data\n\t        # move the inputs to the manifold\n\t        tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n\t        manifold_inputs = manifold.expmap(tangents)\n", "        # zero the parameter gradients\n\t        optimizer.zero_grad()\n\t        # forward + backward + optimize\n\t        outputs = net(manifold_inputs)\n\t        loss = criterion(outputs.tensor, labels)\n\t        loss.backward()\n\t        optimizer.step()\n\t        # print statistics\n\t        running_loss += loss.item()\n\t        if i % 2000 == 1999:  # print every 2000 mini-batches\n", "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n\t            running_loss = 0.0\n\tprint(\"Finished Training\")\n\t########################################################################\n\t# Let's quickly save our trained model:\n\tPATH = \"./cifar_net.pth\"\n\ttorch.save(net.state_dict(), PATH)\n\t########################################################################\n\t# Next, let's load back in our saved model (note: saving and re-loading the model\n\t# wasn't necessary here, we only did it to illustrate how to do so):\n", "net = Net()\n\tnet.load_state_dict(torch.load(PATH))\n\t########################################################################\n\t# 6. Test the network on the test data\n\t# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t#\n\t# Let us look at how the network performs on the whole dataset.\n\tcorrect = 0\n\ttotal = 0\n\t# since we're not training, we don't need to calculate the gradients for our outputs\n", "with torch.no_grad():\n\t    for data in testloader:\n\t        images, labels = data\n\t        # move the images to the manifold\n\t        tangents = TangentTensor(data=images, man_dim=1, manifold=manifold)\n\t        manifold_images = manifold.expmap(tangents)\n\t        # calculate outputs by running images through the network\n\t        outputs = net(manifold_images)\n\t        # the class with the highest energy is what we choose as prediction\n\t        _, predicted = torch.max(outputs.tensor, 1)\n", "        total += labels.size(0)\n\t        correct += (predicted == labels).sum().item()\n\tprint(f\"Accuracy of the network on the 10000 test images: {100 * correct // total} %\")\n\t########################################################################\n\t# That looks way better than chance, which is 10% accuracy (randomly picking\n\t# a class out of 10 classes).\n\t# Seems like the network learnt something.\n\t#\n\t# Hmmm, what are the classes that performed well, and the classes that did\n\t# not perform well:\n", "# prepare to count predictions for each class\n\tcorrect_pred = {classname: 0 for classname in classes}\n\ttotal_pred = {classname: 0 for classname in classes}\n\t# again no gradients needed\n\twith torch.no_grad():\n\t    for data in testloader:\n\t        images, labels = data\n\t        # move the images to the manifold\n\t        tangents = TangentTensor(data=images, man_dim=1, manifold=manifold)\n\t        manifold_images = manifold.expmap(tangents)\n", "        outputs = net(manifold_images)\n\t        _, predictions = torch.max(outputs.tensor, 1)\n\t        # collect the correct predictions for each class\n\t        for label, prediction in zip(labels, predictions):\n\t            if label == prediction:\n\t                correct_pred[classes[label]] += 1\n\t            total_pred[classes[label]] += 1\n\t# print accuracy for each class\n\tfor classname, correct_count in correct_pred.items():\n\t    accuracy = 100 * float(correct_count) / total_pred[classname]\n", "    print(f\"Accuracy for class: {classname:5s} is {accuracy:.1f} %\")\n\t########################################################################\n\t#\n\t# Training on GPU\n\t# ----------------\n\t# Just like how you transfer a Tensor onto the GPU, you transfer the neural\n\t# net onto the GPU.\n\t#\n\t# Let's first define our device as the first visible cuda device if we have\n\t# CUDA available:\n", "#\n\t# .. code:: python\n\t#\n\t#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\t#\n\t#\n\t# Assuming that we are on a CUDA machine, this should print a CUDA device:\n\t#\n\t# .. code:: python\n\t#\n", "#     print(device)\n\t#\n\t#\n\t# The rest of this section assumes that ``device`` is a CUDA device.\n\t#\n\t# Then these methods will recursively go over all modules and convert their\n\t# parameters and buffers to CUDA tensors:\n\t#\n\t# .. code:: python\n\t#\n", "#     net.to(device)\n\t#\n\t#\n\t# Remember that you will have to send the inputs and targets at every step\n\t# to the GPU too:\n\t#\n\t# .. code:: python\n\t#\n\t#         inputs, labels = data[0].to(device), data[1].to(device)\n\t#\n", "#\n\t# **Goals achieved**:\n\t#\n\t# - Train a small hyperbolic neural network to classify images.\n\t#\n"]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# For the full list of built-in configuration values, see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\timport os\n\timport sys\n\tsys.path.insert(0, os.path.abspath(\"../..\"))  # Source code dir relative to this file\n\tproject = \"hypll\"\n", "copyright = '2023, \"\"'\n\tauthor = '\"\"'\n\t# -- General configuration ---------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\textensions = [\n\t    \"sphinx_copybutton\",\n\t    \"sphinx.ext.autodoc\",  # Core library for html generation from docstrings\n\t    \"sphinx.ext.autosummary\",\n\t    \"sphinx.ext.napoleon\",\n\t    \"sphinx_gallery.gen_gallery\",\n", "    \"sphinx_tabs.tabs\",\n\t]\n\ttemplates_path = [\"_templates\"]\n\texclude_patterns = []\n\t# Autodoc options:\n\tautodoc_class_signature = \"separated\"\n\tautodoc_default_options = {\n\t    \"members\": True,\n\t    \"member-order\": \"bysource\",\n\t    \"special-members\": \"__init__\",\n", "    \"undoc-members\": True,\n\t    \"exclude-members\": \"__weakref__\",\n\t}\n\tautodoc_default_flags = [\n\t    \"members\",\n\t    \"undoc-members\",\n\t    \"special-members\",\n\t    \"show-inheritance\",\n\t]\n\t# Napoleon options:\n", "napoleon_google_docstring = True\n\tnapoleon_numpy_docstring = False\n\tnapoleon_include_init_with_doc = False\n\tnapoleon_include_private_with_doc = False\n\tnapoleon_include_special_with_doc = False\n\tnapoleon_use_admonition_for_examples = False\n\tnapoleon_use_admonition_for_notes = False\n\tnapoleon_use_admonition_for_references = False\n\tnapoleon_use_ivar = False\n\tnapoleon_use_param = False\n", "napoleon_use_rtype = False\n\tnapoleon_type_aliases = None\n\t# Sphinx gallery config:\n\tsphinx_gallery_conf = {\n\t    \"examples_dirs\": \"../../tutorials\",\n\t    \"gallery_dirs\": \"tutorials/\",\n\t    \"filename_pattern\": \"\",\n\t    # TODO(Philipp, 06/23): Figure out how we can build and host tutorials on RTD.\n\t    \"plot_gallery\": \"False\",\n\t}\n", "# -- Options for HTML output -------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\thtml_theme = \"alabaster\"\n\thtml_static_path = [\"_static\"]\n"]}
