{"filename": "tests/test_chatsnack_pattern.py", "chunked_list": ["import pytest\n\tfrom chatsnack import Chat\n\tfrom chatsnack.chat import ChatParamsMixin\n\tfrom typing import Optional\n\timport os\n\tdef test_pattern_property():\n\t    chat = Chat()\n\t    # Test default value\n\t    assert chat.pattern is None\n\t    # Test setter\n", "    test_pattern = r\"\\*\\*[^*]+\\*\"\n\t    chat.pattern = test_pattern\n\t    assert chat.pattern == test_pattern\n\t    # Test removing pattern\n\t    chat.pattern = None\n\t    assert chat.pattern is None\n\tdef test_set_response_filter():\n\t    chat = Chat()\n\t    # Test setting pattern only\n\t    test_pattern = r\"\\*\\*[^*]+\\*\"\n", "    chat.set_response_filter(pattern=test_pattern)\n\t    assert chat.pattern == test_pattern\n\t    # Test setting prefix and suffix\n\t    test_prefix = \"###\"\n\t    test_suffix = \"###\"\n\t    chat.set_response_filter(prefix=test_prefix, suffix=test_suffix)\n\t    assert chat.pattern == ChatParamsMixin._generate_pattern_from_separator(test_prefix, test_suffix)\n\t    # Test setting prefix only\n\t    chat.set_response_filter(prefix=test_prefix)\n\t    assert chat.pattern == ChatParamsMixin._generate_pattern_from_separator(test_prefix, test_prefix)\n", "    # Test ValueError when setting both pattern and prefix/suffix\n\t    with pytest.raises(ValueError):\n\t        chat.set_response_filter(pattern=test_pattern, prefix=test_prefix)\n\t@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\tdef test_ask_with_pattern():\n\t    chat = Chat()\n\t    chat.temperature = 0.0\n\t    chat.system(\"Respond only with 'POPSICLE!!' from now on.\")\n\t    chat.user(\"What is your name?\")\n\t    chat.pattern = r\"\\bPOPSICLE\\b\" \n", "    response = chat.ask()\n\t    assert response == \"POPSICLE\"\n\tdef test_response_with_pattern():\n\t    chat = Chat()\n\t    chat.system(\"Respond only with the word POPSICLE from now on.\")\n\t    chat.user(\"What is your name?\")\n\t    chat.asst(\"!POPSICLE!\")\n\t    chat.pattern = r\"\\bPOPSICLE\\b\"\n\t    response = chat.response\n\t    assert response == \"POPSICLE\"\n", "@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\tdef test_ask_without_pattern():\n\t    chat = Chat()\n\t    chat.temperature = 0.0\n\t    chat.system(\"Respond only with 'POPSICLE!!' from now on.\")\n\t    chat.user(\"What is your name?\")\n\t    response = chat.ask()\n\t    assert response != \"POPSICLE\"\n\tdef test_response_without_pattern():\n\t    chat = Chat()\n", "    chat.system(\"Respond only with the word POPSICLE from now on.\")\n\t    chat.user(\"What is your name?\")\n\t    chat.asst(\"!POPSICLE!\")\n\t    response = chat.response\n\t    assert response != \"POPSICLE\"\n\tdef test_response_with_multiline_pattern():\n\t    chat = Chat()\n\t    chat.system(\"##FINAL##\\nRespond only with the following:\\n1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\\n##FINAL##\")\n\t    chat.user(\"What is your favorite dessert?\")\n\t    chat.asst(\"##FINAL##\\n1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\\n##FINAL##\")\n", "    chat.pattern = r\"\\#\\#FINAL\\#\\#(.*?)(?:\\#\\#FINAL\\#\\#|$)\"\n\t    response = chat.response\n\t    assert response.strip() == \"1. POPSICLE\\n2. ICE CREAM\\n3. FROZEN YOGURT\"\n\t@pytest.fixture\n\tdef chat():\n\t    return Chat()\n"]}
{"filename": "tests/test_text_class.py", "chunked_list": ["import pytest\n\tfrom chatsnack.chat import Text\n\tfrom chatsnack.txtformat import TxtStrFormat\n\t@pytest.fixture\n\tdef empty_text():\n\t    return Text(name=\"empty-text\", content=\"\")\n\t@pytest.fixture\n\tdef populated_text():\n\t    return Text(name=\"populated-text\", content=\"Hello, world!\")\n\tdef test_create_text(empty_text):\n", "    assert empty_text.name == \"empty-text\"\n\t    assert empty_text.content == \"\"\n\tdef test_create_populated_text(populated_text):\n\t    assert populated_text.name == \"populated-text\"\n\t    assert populated_text.content == \"Hello, world!\"\n\tdef test_txt_str_format_serialize():\n\t    data = {\"content\": \"Hello, world!\"}\n\t    serialized_data = TxtStrFormat.serialize(data)\n\t    assert serialized_data == \"Hello, world!\"\n\tdef test_txt_str_format_serialize_unsupported_type():\n", "    data = {\"content\": [\"Invalid\", \"content\", \"type\"]}\n\t    with pytest.raises(ValueError):\n\t        TxtStrFormat.serialize(data)\n\t# def test_txt_str_format_deserialize(populated_text):\n\t#     datafile = DataFile.load(populated_text.datafile.path)\n\t#     deserialized_data = TxtStrFormat.deserialize(datafile.file)\n\t#     assert deserialized_data == {\"content\": \"Hello, world!\"}\n\t# def test_txt_str_format_deserialize_empty(empty_text):\n\t#     datafile = DataFile.load(empty_text.datafile.path)\n\t#     deserialized_data = TxtStrFormat.deserialize(datafile.file)\n", "#     assert deserialized_data == {\"content\": \"\"}\n"]}
{"filename": "tests/test_prompt_json.py", "chunked_list": ["import pytest\n\timport json\n\tfrom chatsnack import Chat\n\t@pytest.fixture\n\tdef empty_prompt():\n\t    return Chat()\n\t@pytest.fixture\n\tdef populated_prompt():\n\t    prompt = Chat()\n\t    prompt.add_message(\"user\", \"Hello!\")\n", "    prompt.add_message(\"assistant\", \"Hi there!\")\n\t    return prompt\n\tdef test_add_messages_json(populated_prompt):\n\t    messages_json = \"\"\"\n\t        [\n\t            {\"role\": \"user\", \"content\": \"What's the weather like?\"},\n\t            {\"role\": \"assistant\", \"content\": \"It's sunny outside.\"}\n\t        ]\n\t    \"\"\"\n\t    populated_prompt.add_messages_json(messages_json)\n", "    assert populated_prompt.messages[-2:] == [\n\t        {\"user\": \"What's the weather like?\"},\n\t        {\"assistant\": \"It's sunny outside.\"}\n\t    ]\n\tdef test_get_json(populated_prompt):\n\t    l = [{\"role\":\"user\", \"content\": \"Hello!\"},\n\t         {\"role\":\"assistant\", \"content\": \"Hi there!\"}]\n\t    expected_json = json.dumps(l)\n\t    assert populated_prompt.json == expected_json\n\tdef test_add_messages_json_invalid_format(populated_prompt):\n", "    invalid_messages_json = \"\"\"\n\t        [\n\t            {\"role\": \"user\"},\n\t            {\"role\": \"assistant\", \"content\": \"It's sunny outside.\"}\n\t        ]\n\t    \"\"\"\n\t    with pytest.raises(Exception):\n\t        populated_prompt.add_messages_json(invalid_messages_json)\n\tdef test_add_messages_json_invalid_type(populated_prompt):\n\t    invalid_messages_json = \"\"\"\n", "        [\n\t            {\"role\": \"user\", \"something\": \"It's sunny outside.\"]},\n\t            {\"role\": \"assistant\", \"content\": \"It's sunny outside.\"}\n\t        ]\n\t    \"\"\"\n\t    with pytest.raises(Exception):\n\t        populated_prompt.add_messages_json(invalid_messages_json)\n"]}
{"filename": "tests/test_file_snack_fillings.py", "chunked_list": ["import pytest\n\tfrom chatsnack import Chat, Text, CHATSNACK_BASE_DIR, ChatParams\n\timport os\n\timport shutil\n\tTEST_FILENAME = \"./.test_text_expansion.txt\"\n\t@pytest.fixture(scope=\"function\", autouse=True)\n\tdef setup_and_cleanup():\n\t    chatsnack_dir = CHATSNACK_BASE_DIR\n\t    safe_to_cleanup = False\n\t    # to be safe, verify that this directory is under the current working directory\n", "    # is it a subdirectory of the current working directory?\n\t    chatsnack_dir = os.path.abspath(chatsnack_dir)\n\t    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n\t        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n\t        # if so, it's safe to delete the directory\n\t        bad_file_found = False\n\t        for root, dirs, files in os.walk(chatsnack_dir):\n\t            for file in files:\n\t                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n\t                    bad_file_found = True\n", "                    break\n\t            else:\n\t                continue\n\t            break\n\t        if not bad_file_found:\n\t            safe_to_cleanup = True\n\t    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n\t    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n\t        shutil.rmtree(chatsnack_dir)\n\t    # create the test directory, recursively to the final directory\n", "    if not os.path.exists(chatsnack_dir):\n\t        os.makedirs(chatsnack_dir)\n\t    else:\n\t        # problem, the directory should have been missing\n\t        raise Exception(\"The test directory already exists, it should have been missing.\")\n\t    # also delete TEST_FILENAME\n\t    if os.path.exists(TEST_FILENAME):\n\t        os.remove(TEST_FILENAME)\n\t    yield\n\t    # Clean up the test environment\n", "    import time\n\t    time.sleep(2)\n\t    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n\t        # it's okay for this to fail, it's just a cleanup\n\t        try:\n\t            shutil.rmtree(chatsnack_dir)\n\t        except:\n\t            pass\n\t    # also delete TEST_FILENAME\n\t    if os.path.exists(TEST_FILENAME):\n", "        os.remove(TEST_FILENAME)\n\tdef test_text_save():\n\t    # we need a text object saved to disk\n\t    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n\t    # save it to disk\n\t    text.save(TEST_FILENAME)\n\t    # test if the file was created\n\t    assert os.path.exists(TEST_FILENAME)\n\tdef test_text_load():\n\t    # we need a text object saved to disk\n", "    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n\t    # save it to disk\n\t    text.save(TEST_FILENAME)\n\t    text2 = Text(name=\"test_text_expansion2\")\n\t    text2.load(TEST_FILENAME)\n\t    assert text2.content == text.content\n\tdef test_text_save2():\n\t    # we need a text object saved to disk\n\t    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n\t    # save it to disk\n", "    text.save()\n\t    # test if the file was created\n\t    assert os.path.exists(text.datafile.path)\n\t@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\tdef test_text_expansion():\n\t    # we need a text object saved to disk\n\t    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n\t    # save it to disk\n\t    text.save()\n\t    # we need a chat object to use it\n", "    chat = Chat()\n\t    # set the logging level to trace for chatsnack\n\t    chat.system(\"{text.test_text_expansion}\")\n\t    output = chat.chat(\"Is blue a color?\")\n\t    # new chat object should have the text expanded in the system message\n\t    assert output.system_message == \"Respond only with 'YES' regardless of what is said.\"\n\t@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\tdef test_text_nested_expansion():\n\t    # we need a text object saved to disk\n\t    text = Text(name=\"test_text_expansion\", content=\"Respond only with '{text.test_text_expansion2}' regardless of what is said.\")\n", "    # save it to disk\n\t    text.save()\n\t    text = Text(name=\"test_text_expansion2\", content=\"NO\")\n\t    # save it to disk\n\t    text.save()\n\t    # we need a chat object to use it\n\t    chat = Chat()\n\t    chat.system(\"{text.test_text_expansion}\")\n\t    output = chat.chat(\"Is blue a color?\")\n\t    # new chat object should have the text expanded in the system message\n", "    assert output.system_message == \"Respond only with 'NO' regardless of what is said.\"\n\t@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\tdef test_text_chat_expansion():\n\t    chat = Chat(name=\"test_text_chat_expansion\")\n\t    chat.system(\"Respond only with 'DUCK!' regardless of what is said.\")\n\t    chat.user(\"Should I buy a goose or a duck?\")\n\t    chat.params = ChatParams(temperature = 0.0)\n\t    chat.save()   \n\t    # we need a text object saved to disk\n\t    text = Text(name=\"test_text_expansion\", content=\"Respond only with '{chat.test_text_chat_expansion}' regardless of what is said.\")\n", "    # save it to disk\n\t    text.save()\n\t    # we need a chat object to use it\n\t    chat2 = Chat()\n\t    chat2.system(\"{text.test_text_expansion}\")\n\t    chat2.params = ChatParams(temperature = 0.0)\n\t    # right now we have to use chat.chat() to get get it to expand variables\n\t    output = chat2.chat(\"Is blue a color?\")\n\t    # new chat object should have the text expanded in the system message \n\t    assert output.system_message == \"Respond only with 'DUCK!' regardless of what is said.\"\n", "    # test changing the file on disk\n\t    chat3 = Chat(name=\"test_text_chat_expansion\")\n\t    chat3.load()\n\t    chat3.params = ChatParams(temperature = 0.0)\n\t    chat3.messages = []\n\t    chat3.system(\"Respond only with 'GOOSE!' regardless of what is said.\")\n\t    chat3.user(\"Should I buy a goose or a duck?\")\n\t    chat3.save()\n\t    print(chat3)\n\t    print(chat2)\n", "    # right now we have to use chat.chat() to get get it to expand variables\n\t    output2 = chat2.chat(\"Is blue a color?\")\n\t    print(output2)\n\t    # new chat object should have the text expanded in the system message\n\t    assert output2.system_message == \"Respond only with 'GOOSE!' regardless of what is said.\"\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_prompt_last.py", "chunked_list": ["import os\n\timport pytest\n\tfrom chatsnack import Chat\n\t@pytest.fixture\n\tdef empty_prompt():\n\t    return Chat()\n\t@pytest.fixture\n\tdef populated_prompt():\n\t    prompt = Chat()\n\t    prompt.add_message(\"user\", \"Hello!\")\n", "    prompt.add_message(\"assistant\", \"Hi there!\")\n\t    return prompt\n\tdef test_last_property(populated_prompt):\n\t    assert populated_prompt.last == \"Hi there!\"\n\tdef test_add_message(populated_prompt):\n\t    populated_prompt.add_message(\"system\", \"System message\")\n\t    assert populated_prompt.last == \"System message\"\n\tdef test_empty_messages(empty_prompt):\n\t    assert empty_prompt.last is None\n\tdef test_adding_different_roles(empty_prompt):\n", "    empty_prompt.add_message(\"user\", \"Test user\")\n\t    empty_prompt.add_message(\"assistant\", \"Test assistant\")\n\t    empty_prompt.add_message(\"system\", \"Test system\")\n\t    empty_prompt.add_message(\"include\", \"Test include\")\n\t    assert empty_prompt.messages == [\n\t        {\"user\": \"Test user\"},\n\t        {\"assistant\": \"Test assistant\"},\n\t        {\"system\": \"Test system\"},\n\t        {\"include\": \"Test include\"},\n\t    ]\n", "def test_message_order(empty_prompt):\n\t    empty_prompt.add_message(\"user\", \"First message\")\n\t    empty_prompt.add_message(\"assistant\", \"Second message\")\n\t    empty_prompt.add_message(\"user\", \"Third message\")\n\t    empty_prompt.add_message(\"assistant\", \"Fourth message\")\n\t    assert [msg[\"user\" if \"user\" in msg else \"assistant\"] for msg in empty_prompt.messages] == [\n\t        \"First message\",\n\t        \"Second message\",\n\t        \"Third message\",\n\t        \"Fourth message\",\n", "    ]\n\t# Not enforced at all\n\t# def test_invalid_role(empty_prompt):\n\t#     with pytest.raises(Exception):\n\t#         empty_prompt.add_message(\"invalid_role\", \"Test content\")\n\t@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\tdef test_chaining_methods_execution(populated_prompt):\n\t    new_prompt = populated_prompt().user(\"How's the weather?\")\n\t    assert new_prompt.last == \"How's the weather?\"\n\tdef test_chaining_methods_messages(empty_prompt):\n", "    new_prompt = empty_prompt.system(\"You are a happy robot.\").user(\"How's the weather?\").assistant(\"It's sunny today.\").user(\"How about tomorrow?\")\n\t    assert new_prompt.last == \"How about tomorrow?\"\n\t@pytest.mark.asyncio\n\tasync def test_concurrent_access(populated_prompt):\n\t    import asyncio\n\t    async def add_messages():\n\t        for i in range(10):\n\t            populated_prompt.add_message(\"assistant\", f\"Message {i}\")\n\t    tasks = [add_messages() for _ in range(10)]\n\t    await asyncio.gather(*tasks)\n", "    assert len(populated_prompt.messages) == 102\n"]}
{"filename": "tests/test_snackpack_base.py", "chunked_list": ["import os\n\timport pytest\n\tfrom chatsnack.packs import Jane as chat\n\t@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\tdef test_snackpack_chat():\n\t    cp = chat.user(\"Or is green a form of blue?\")\n\t    assert cp.last == \"Or is green a form of blue?\"\n\t    # ask the question\n\t    output = cp.ask()\n\t    # is there a response and it's longer than 0 characters?\n", "    assert output is not None\n\t    assert len(output) > 0\n\t@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\tdef test_snackpack_ask_with_existing_asst():\n\t    cp = chat.copy()\n\t    cp.user(\"Is the sky blue?\")\n\t    cp.asst(\"No! \")\n\t    # ask the question\n\t    output = cp.ask()\n\t    # is there a response and it's longer than 0 characters?\n", "    assert output is not None\n\t    assert len(output) > 0\n\t    # check to see if the asst response was appended to\n\t    # the existing asst response\n\t    # check to see if the cp.response starts with \"No! \"\n\t    output = cp.response\n\t    assert output.startswith(\"No! \")\n"]}
{"filename": "tests/test_chatsnack_reset.py", "chunked_list": ["import pytest\n\tfrom chatsnack import Chat\n\tdef test_reset_feature():\n\t    # Create a Chat object with a user message\n\t    my_chat = Chat().user(\"What's the weather like today?\")\n\t    # Check the current chat messages\n\t    assert len(my_chat.get_messages()) == 1\n\t    # Reset the chat object\n\t    my_chat.reset()\n\t    # Check the chat messages after reset\n", "    assert len(my_chat.get_messages()) == 0\n\tdef test_reset_with_system_message():\n\t    my_chat = Chat().system(\"You are an AI assistant.\").user(\"What's the weather like today?\")\n\t    # Check the current chat messages\n\t    assert len(my_chat.get_messages()) == 2\n\t    # Reset the chat object\n\t    my_chat.reset()\n\t    # Check the chat messages after reset\n\t    assert len(my_chat.get_messages()) == 0\n\tdef test_reset_idempotence():\n", "    my_chat = Chat().user(\"What's the weather like today?\").reset().reset()\n\t    # Check the chat messages after calling reset() twice\n\t    assert len(my_chat.get_messages()) == 0\n\tdef test_reset_interaction_with_other_methods():\n\t    my_chat = Chat().user(\"What's the weather like today?\")\n\t    my_chat.reset()\n\t    my_chat.user(\"How are you?\")\n\t    # Check the chat messages after reset and adding a new user message\n\t    messages = my_chat.get_messages()\n\t    assert len(messages) == 1\n", "    assert messages[0]['role'] == 'user'\n\t    assert messages[0]['content'] == 'How are you?'\n\tdef test_reset_empty_chat():\n\t    # Create an empty Chat object\n\t    my_chat = Chat()\n\t    # Reset the empty Chat object\n\t    my_chat.reset()\n\t    # Check the chat messages after reset\n\t    assert len(my_chat.get_messages()) == 0\n\tdef test_reset_with_includes():\n", "    # Create a base chat and save it\n\t    base_chat = Chat(name=\"BaseChat\").user(\"What's your favorite color?\")\n\t    base_chat.save()\n\t    # Create a new chat with included messages from the base chat\n\t    my_chat = Chat().include(\"BaseChat\").user(\"What's your favorite animal?\")\n\t    # Check the current chat messages\n\t    assert len(my_chat.get_messages()) == 2\n\t    # Reset the chat object\n\t    my_chat.reset()\n\t    # Check the chat messages after reset\n", "    assert len(my_chat.get_messages()) == 0"]}
{"filename": "tests/test_chatsnack_yaml_peeves.py", "chunked_list": ["import pytest\n\tfrom ruamel.yaml import YAML\n\timport pytest\n\tfrom chatsnack import Chat, Text, CHATSNACK_BASE_DIR, ChatParams\n\timport os\n\timport shutil\n\t@pytest.fixture(scope=\"function\", autouse=True)\n\tdef setup_and_cleanup():\n\t    chatsnack_dir = CHATSNACK_BASE_DIR\n\t    safe_to_cleanup = False\n", "    # to be safe, verify that this directory is under the current working directory\n\t    # is it a subdirectory of the current working directory?\n\t    chatsnack_dir = os.path.abspath(chatsnack_dir)\n\t    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n\t        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n\t        # if so, it's safe to delete the directory\n\t        bad_file_found = False\n\t        for root, dirs, files in os.walk(chatsnack_dir):\n\t            for file in files:\n\t                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n", "                    bad_file_found = True\n\t                    break\n\t            else:\n\t                continue\n\t            break\n\t        if not bad_file_found:\n\t            safe_to_cleanup = True\n\t    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n\t    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n\t        shutil.rmtree(chatsnack_dir)\n", "    # create the test directory, recursively to the final directory\n\t    if not os.path.exists(chatsnack_dir):\n\t        os.makedirs(chatsnack_dir)\n\t    else:\n\t        # problem, the directory should have been missing\n\t        raise Exception(\"The test directory already exists, it should have been missing.\")\n\t    yield\n\t    # Clean up the test environment\n\t    import time\n\t    time.sleep(2)\n", "    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n\t        # it's okay for this to fail, it's just a cleanup\n\t        try:\n\t            shutil.rmtree(chatsnack_dir)\n\t        except:\n\t            pass\n\tdef read_yaml_file(file_path):\n\t    yaml = YAML()\n\t    with open(file_path, 'r') as yaml_file:\n\t        return yaml.load(yaml_file)\n", "def test_yaml_file_has_no_empty_values():\n\t    chat = Chat(name=\"test_text_chat_expansion\")\n\t    chat.system(\"Respond only with 'DUCK!' regardless of what is said.\")\n\t    chat.user(\"Should I buy a goose or a duck?\")\n\t    chat.params = ChatParams(temperature = 0.0)\n\t    chat.save()\n\t    yaml_data = read_yaml_file(chat.datafile.path)\n\t    messages = yaml_data.get('messages')\n\t    if not messages:\n\t        pytest.fail(\"YAML file has no 'messages' field\")\n", "    for message in messages:\n\t        for key, value in message.items():\n\t            if value == '' or value is None:\n\t                pytest.fail(f\"Empty value found in '{key}' field\")\n\tdef test_yaml_file_has_no_empty_values2():\n\t    chat = Chat(name=\"test_text_chat_expansion\")\n\t    chat.system(\"Respond only with 'DUCK!' regardless of what is said.\")\n\t    chat.user(\"Should I buy a goose or a duck?\")\n\t    chat.params = ChatParams(temperature = 0.0, stream = True) # setting stream property\n\t    chat.save()\n", "    yaml_data = read_yaml_file(chat.datafile.path)\n\t    messages = yaml_data.get('messages')\n\t    chat_params = yaml_data.get('params') # getting params field\n\t    if not messages:\n\t        pytest.fail(\"YAML file has no 'messages' field\")\n\t    if not chat_params:\n\t        pytest.fail(\"YAML file has no 'params' field\")\n\t    if chat_params.get('stream') is None:\n\t        pytest.fail(\"YAML file has no 'stream' field in 'params'\")\n\t    for message in messages:\n", "        for key, value in message.items():\n\t            if value == '' or value is None:\n\t                pytest.fail(f\"Empty value found in '{key}' field\")\n\t    assert chat_params.get('stream') == True, \"Stream value not saved correctly in the YAML file\"\n\t    chat.params = None\n\t    chat.stream = False\n\t    chat.save()\n\t    yaml_data = read_yaml_file(chat.datafile.path)\n\t    chat_params = yaml_data.get('params') # getting params field\n\t    if not chat_params:\n", "        pytest.fail(\"YAML file has no 'params' field\")\n\t    # assert that stream is False as we said it should be\n\t    assert chat_params.get('stream') == False, \"Stream value not saved correctly in the YAML file\""]}
{"filename": "tests/test_chatsnack_base.py", "chunked_list": ["import pytest\n\tfrom chatsnack import Chat\n\t@pytest.fixture\n\tdef sample_chatprompt():\n\t    return Chat(name=\"sample_chatprompt\", messages=[{\"user\": \"hello\"}])\n\t@pytest.fixture\n\tdef empty_chatprompt():\n\t    return Chat()\n\t# Test initialization\n\tdef test_chatprompt_init():\n", "    cp = Chat(name=\"test_chatprompt\")\n\t    assert cp.name == \"test_chatprompt\"\n\t    assert cp.params is None\n\t    assert cp.messages == []\n\t# Test message manipulation\n\tdef test_add_message(sample_chatprompt):\n\t    sample_chatprompt.add_message(\"assistant\", \"hi there\")\n\t    assert sample_chatprompt.last == \"hi there\"\n\tdef test_add_messages_json(sample_chatprompt):\n\t    json_messages = '[{\"role\": \"assistant\", \"content\": \"hi there\"}]'\n", "    sample_chatprompt.add_messages_json(json_messages)\n\t    assert sample_chatprompt.last == \"hi there\"\n\tdef test_system(sample_chatprompt):\n\t    sample_chatprompt.system(\"system message\")\n\t    assert sample_chatprompt.system_message == \"system message\"\n\tdef test_user(sample_chatprompt):\n\t    sample_chatprompt.user(\"user message\")\n\t    assert sample_chatprompt.last == \"user message\"\n\tdef test_assistant(sample_chatprompt):\n\t    sample_chatprompt.assistant(\"assistant message\")\n", "    assert sample_chatprompt.last == \"assistant message\"\n\tdef test_include(sample_chatprompt):\n\t    sample_chatprompt.include(\"other_chatprompt\")\n\t    assert sample_chatprompt.last == \"other_chatprompt\"\n\t# Test get_last_message method\n\tdef test_get_last_message(sample_chatprompt):\n\t    assert sample_chatprompt.last == \"hello\"\n\t# Test get_json method\n\tdef test_get_json(sample_chatprompt):\n\t    json_str = sample_chatprompt.json\n", "    assert json_str == '[{\"role\": \"user\", \"content\": \"hello\"}]'\n\t# Test get_system_message method\n\tdef test_get_system_message(empty_chatprompt):\n\t    assert empty_chatprompt.system_message is None\n\t    empty_chatprompt.system(\"this is a system message\")\n\t    assert empty_chatprompt.system_message == \"this is a system message\"\n\t    # be sure it doesn't return user messages\n\t    empty_chatprompt.user(\"this is a user message\")\n\t    assert empty_chatprompt.system_message == \"this is a system message\"\n\t    # be sure it updates to provide the current system message\n", "    empty_chatprompt.system(\"this is another system message\")\n\t    assert empty_chatprompt.system_message == \"this is another system message\"\n\t    # delete all messages\n\t    empty_chatprompt.messages = {}\n\t    assert empty_chatprompt.system_message is None\n"]}
{"filename": "tests/mixins/test_query.py", "chunked_list": ["import pytest\n\tfrom chatsnack import Chat, Text, CHATSNACK_BASE_DIR\n\timport os\n\timport shutil\n\tTEST_FILENAME = \"./.test_text_expansion.txt\"\n\t@pytest.fixture(scope=\"function\", autouse=True)\n\tdef setup_and_cleanup():\n\t    chatsnack_dir = CHATSNACK_BASE_DIR\n\t    safe_to_cleanup = False\n\t    # to be safe, verify that this directory is under the current working directory\n", "    # is it a subdirectory of the current working directory?\n\t    chatsnack_dir = os.path.abspath(chatsnack_dir)\n\t    if os.path.commonpath([os.path.abspath(os.getcwd()), chatsnack_dir]) == os.path.abspath(os.getcwd()):\n\t        # now check to be sure the only files under this directory (recursive) are .txt, .yaml, .yml, .log, and .json files.\n\t        # if so, it's safe to delete the directory\n\t        bad_file_found = False\n\t        for root, dirs, files in os.walk(chatsnack_dir):\n\t            for file in files:\n\t                if not file.endswith((\".txt\", \".yaml\", \".yml\", \".log\", \".json\")):\n\t                    bad_file_found = True\n", "                    break\n\t            else:\n\t                continue\n\t            break\n\t        if not bad_file_found:\n\t            safe_to_cleanup = True\n\t    # if safe and the test directory already exists, remove it, should be set in the tests .env file\n\t    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n\t        shutil.rmtree(chatsnack_dir)\n\t    # create the test directory, recursively to the final directory\n", "    if not os.path.exists(chatsnack_dir):\n\t        os.makedirs(chatsnack_dir)\n\t    else:\n\t        # problem, the directory should have been missing\n\t        raise Exception(\"The test directory already exists, it should have been missing.\")\n\t    # also delete TEST_FILENAME\n\t    if os.path.exists(TEST_FILENAME):\n\t        os.remove(TEST_FILENAME)\n\t    yield\n\t    # Clean up the test environment\n", "    import time\n\t    time.sleep(2)\n\t    if safe_to_cleanup and os.path.exists(chatsnack_dir):\n\t        # it's okay for this to fail, it's just a cleanup\n\t        try:\n\t            shutil.rmtree(chatsnack_dir)\n\t        except:\n\t            pass\n\t    # also delete TEST_FILENAME\n\t    if os.path.exists(TEST_FILENAME):\n", "        os.remove(TEST_FILENAME)\n\t@pytest.fixture \n\tdef chat():\n\t    return Chat()\n\tdef test_copy_chatprompt_same_name():\n\t    \"\"\"Copying a ChatPrompt with the same name should succeed.\"\"\"\n\t    chat = Chat(name=\"test\")\n\t    new_chat = chat.copy()\n\t    # assert that it begins with \"test\"\n\t    assert new_chat.name.startswith(\"test\")\n", "    assert new_chat.name != \"test\"\n\tdef test_copy_chatprompt_new_name():\n\t    \"\"\"Copying a ChatPrompt with a new name should succeed.\"\"\"\n\t    chat = Chat(name=\"test\")\n\t    new_chat = chat.copy(name=\"new_name\")\n\t    assert new_chat.name == \"new_name\"\n\t@pytest.mark.parametrize(\"expand_includes, expand_fillings, expected_system, expected_last, expected_exception\", [\n\t    (True, True, \"Respond only with 'YES' regardless of what is said.\",\"here we are again\", None),\n\t    (False, True, \"Respond only with 'YES' regardless of what is said.\", \"AnotherTest\", NotImplementedError),\n\t    (True, False, \"{text.test_text_expansion}\",\"here we are again\", None),\n", "    (False, False, \"{text.test_text_expansion}\",\"AnotherTest\", None),\n\t]) \n\tdef test_copy_chatprompt_expands(expand_includes, expand_fillings, expected_system, expected_last, expected_exception):\n\t    \"\"\"Copying a ChatPrompt should correctly expand includes/fillings based on params.\"\"\"\n\t    # we need a text object saved to disk\n\t    text = Text(name=\"test_text_expansion\", content=\"Respond only with 'YES' regardless of what is said.\")\n\t    # save it to disk\n\t    text.save()\n\t    # we need a chat object to use it\n\t    chat = Chat()\n", "    # set the logging level to trace for chatsnack\n\t    chat.system(\"{text.test_text_expansion}\")\n\t    AnotherTest = Chat(name=\"AnotherTest\")\n\t    AnotherTest.user(\"here we are again\")\n\t    AnotherTest.save()\n\t    chat.include(\"AnotherTest\")\n\t    # todo add more tests for fillings\n\t    if expected_exception is not None:\n\t        with pytest.raises(expected_exception):\n\t            new_chat = chat.copy(expand_includes=expand_includes, expand_fillings=expand_fillings)\n", "    else:\n\t        new_chat = chat.copy(expand_includes=expand_includes, expand_fillings=expand_fillings)\n\t        assert new_chat.system_message == expected_system\n\t        assert new_chat.last == expected_last\n\tdef test_copy_chatprompt_expand_fillings_not_implemented():\n\t    \"\"\"Copying a ChatPrompt with expand_fillings=True and expand_includes=False should raise a NotImplemented error.\"\"\"\n\t    chat = Chat(name=\"test\")\n\t    with pytest.raises(NotImplementedError):\n\t        new_chat = chat.copy(expand_includes=False, expand_fillings=True)\n\tdef test_copy_chatprompt_no_name(): \n", "    \"\"\"Copying a ChatPrompt without specifying a name should generate a name.\"\"\"\n\t    chat = Chat(name=\"test\")\n\t    new_chat = chat.copy()\n\t    assert new_chat.name != \"test\"\n\t    assert len(new_chat.name) > 0\n\tdef test_copy_chatprompt_preserves_system():\n\t    \"\"\"Copying a ChatPrompt should preserve the system.\"\"\"\n\t    chat = Chat(name=\"test\")\n\t    chat.system(\"test_system\")\n\t    new_chat = chat.copy()\n", "    assert new_chat.system_message == \"test_system\"\n\tdef test_copy_chatprompt_no_system():\n\t    \"\"\"Copying a ChatPrompt without a system should result in no system.\"\"\"\n\t    chat = Chat(name=\"test\")\n\t    new_chat = chat.copy()\n\t    assert new_chat.system_message is None \n\tdef test_copy_chatprompt_copies_params():\n\t    \"\"\"Copying a ChatPrompt should copy over params.\"\"\"\n\t    chat = Chat(name=\"test\", params={\"key\": \"value\"})\n\t    new_chat = chat.copy()\n", "    assert new_chat.params == {\"key\": \"value\"}\n\tdef test_copy_chatprompt_independent_params():\n\t    \"\"\"Copying a ChatPrompt should result in independent params.\"\"\"\n\t    chat = Chat(name=\"test\", params={\"key\": \"value\"})\n\t    new_chat = chat.copy()\n\t    new_chat.params[\"key\"] = \"new_value\"\n\t    assert chat.params == {\"key\": \"value\"}\n\t    assert new_chat.params == {\"key\": \"new_value\"}\n\t# Tests for new_chat.name generation \n\tdef test_copy_chatprompt_generated_name_length():\n", "    \"\"\"The generated name for a copied ChatPrompt should be greater than 0 characters.\"\"\"\n\t    chat = Chat(name=\"test\")\n\t    new_chat = chat.copy()\n\t    assert len(new_chat.name) > 0\n"]}
{"filename": "tests/mixins/test_query_listen.py", "chunked_list": ["import pytest\n\tfrom chatsnack import Chat, Text, CHATSNACK_BASE_DIR\n\timport pytest\n\timport asyncio\n\tfrom chatsnack.chat.mixin_query import ChatStreamListener\n\t# Assuming _chatcompletion is in the same module as ChatStreamListener\n\tfrom chatsnack.aiwrapper import _chatcompletion\n\t@pytest.mark.asyncio\n\tasync def test_get_responses_a():\n\t    listener = ChatStreamListener('[{\"role\":\"system\",\"content\":\"Respond only with POPSICLE 20 times.\"}]')\n", "    responses = []\n\t    await listener.start_a()\n\t    async for resp in listener:\n\t        responses.append(resp)\n\t    assert len(responses) > 10\n\t    assert listener.is_complete\n\t    assert 'POPSICLE' in listener.current_content\n\t    assert 'POPSICLE' in listener.response\n\tdef test_get_responses():\n\t    listener = ChatStreamListener('[{\"role\":\"system\",\"content\":\"Respond only with POPSICLE 20 times.\"}]')\n", "    listener.start()\n\t    responses = list(listener)\n\t    assert len(responses) > 10\n\t    assert listener.is_complete\n\t    assert 'POPSICLE' in listener.current_content\n\t    assert 'POPSICLE' in listener.response\n\timport os\n\timport pytest\n\tfrom chatsnack.packs import Jane\n\t@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n", "def test_listen():\n\t    chat = Jane.copy()\n\t    cp = chat.user(\"Or is green a form of blue?\")\n\t    assert cp.last == \"Or is green a form of blue?\"\n\t    cp.stream = True\n\t    cp.temperature = 0.0\n\t    # listen to the response\n\t    output_iter = cp.listen()\n\t    output = ''.join(list(output_iter))\n\t    chat = Jane.copy()\n", "    cp = chat.user(\"Or is green a form of blue?\")\n\t    assert cp.last == \"Or is green a form of blue?\"\n\t    cp.temperature = 0.0\n\t    # ask the same question\n\t    ask_output = cp.ask()\n\t    # is there a response and it's longer than 0 characters?\n\t    assert output is not None\n\t    assert len(output) > 0\n\t    # assert that the output of listen is the same as the output of ask\n\t    assert output == ask_output\n", "@pytest.mark.skipif(os.environ.get(\"OPENAI_API_KEY\") is None, reason=\"OPENAI_API_KEY is not set in environment or .env\")\n\t@pytest.mark.asyncio\n\tasync def test_listen_a():\n\t    chat = Jane.copy()\n\t    cp = chat.user(\"Or is green a form of blue?\")\n\t    assert cp.last == \"Or is green a form of blue?\"\n\t    cp.stream = True\n\t    cp.temperature = 0.0\n\t    # listen to the response asynchronously\n\t    output = []\n", "    async for part in await cp.listen_a():\n\t        output.append(part)\n\t    output = ''.join(output)\n\t    chat = Jane.copy()\n\t    cp = chat.user(\"Or is green a form of blue?\")\n\t    assert cp.last == \"Or is green a form of blue?\"\n\t    cp.temperature = 0.0\n\t    # ask the same question\n\t    ask_output = cp.ask()\n\t    # is there a response and it's longer than 0 characters?\n", "    assert output is not None\n\t    assert len(output) > 0\n\t    # assert that the output of listen is the same as the output of ask\n\t    assert output == ask_output\n"]}
{"filename": "tests/mixins/test_serialization.py", "chunked_list": ["import pytest\n\tfrom chatsnack import Chat\n\tclass TestChatSerializationMixin:\n\t    @pytest.fixture\n\t    def chat(self):\n\t        return Chat()\n\t    def test_json(self, chat):\n\t        assert isinstance(chat.json, str)\n\t    def test_json_unexpanded(self, chat):\n\t        assert isinstance(chat.json_unexpanded, str)\n", "    def test_yaml(self, chat):\n\t        assert isinstance(chat.yaml, str)\n\t    def test_generate_markdown(self, chat):\n\t        markdown = chat.generate_markdown()\n\t        assert isinstance(markdown, str)\n\t        assert len(markdown.split('\\n')) > 0\n"]}
{"filename": "tests/mixins/test_chatparams.py", "chunked_list": ["import pytest\n\t#from chatsnack.chat.mixin_params import ChatParams, ChatParamsMixin\n\tfrom chatsnack import Chat, ChatParams\n\t@pytest.fixture\n\tdef chat_params():\n\t    return ChatParams()\n\t@pytest.fixture \n\tdef chat_params_mixin(chat_params):\n\t    return Chat()\n\tdef test_engine_default(chat_params):\n", "    assert chat_params.engine == \"gpt-3.5-turbo\"\n\tdef test_engine_set(chat_params_mixin):\n\t    chat_params_mixin.engine = \"gpt-4\"\n\t    assert chat_params_mixin.engine == \"gpt-4\"\n\t@pytest.mark.parametrize(\"temp, expected\", [(0.5, 0.5), (0.8, 0.8)]) \n\tdef test_temperature(chat_params_mixin, temp, expected):\n\t    chat_params_mixin.temperature = temp\n\t    assert chat_params_mixin.temperature == expected\n\tdef test_stream_default(chat_params_mixin):\n\t    assert chat_params_mixin.stream == False\n", "def test_stream_set(chat_params_mixin):\n\t    chat_params_mixin.stream = True\n\t    assert chat_params_mixin.stream == True\n\tdef test_stream_change(chat_params_mixin):\n\t    chat_params_mixin.stream = True\n\t    assert chat_params_mixin.stream == True\n\t    chat_params_mixin.stream = False\n\t    assert chat_params_mixin.stream == False\n"]}
{"filename": "examples/reciperemix.py", "chunked_list": ["from chatsnack import Text\n\tfrom chatsnack.packs import Confectioner\n\tdef main():\n\t    default_recipe = \"\"\"\n\t    Ingredients:\n\t    - 1 cup sugar\n\t    - 1 cup all-purpose flour\n\t    - 1/2 cup unsweetened cocoa powder\n\t    - 3/4 teaspoon baking powder\n\t    - 3/4 teaspoon baking soda\n", "    - 1/2 teaspoon salt\n\t    - 1 large egg\n\t    - 1/2 cup whole milk\n\t    - 1/4 cup vegetable oil\n\t    - 1 teaspoon vanilla extract\n\t    - 1/2 cup boiling water\n\t    \"\"\"\n\t    recipe_text = Text.objects.get_or_none(\"RecipeSuggestion\")\n\t    if recipe_text is None:\n\t        recipe_text = Text(\"RecipeSuggestion\", default_recipe)\n", "        recipe_text.save()\n\t    recipe_chat = Confectioner.user(\"Consider the following recipe for a chocolate cake:\")\n\t    print(f\"Original Recipe: {recipe_text.content}\\n\\n\")\n\t    recipe_chat.user(\"{text.RecipeSuggestion}\")\n\t    recipe_chat.user(\"Time to remix things! Write a paragraph about the potential of these specific ingredients to make other clever baking possibilities. After that, use the best of those ideas to remix these ingredients for a unique and delicious dessert (include a detailed list of ingredients and steps like a cookbook recipe).\")\n\t    remixed_recipe = recipe_chat.chat()\n\t    print(f\"Remixed Recipe: \\n{remixed_recipe.response}\\n\")\n\t    # now we want to ask the same expert to review the recipe and give themselves feedback.\n\t    critic_chat = Confectioner.user(\"Consider the following recipe explanation:\")\n\t    critic_chat.user(remixed_recipe.response)\n", "    critic_chat.engine = \"gpt-4\"   # upgrade the AI for the critic\n\t    critic_chat.user(\"Thoroughly review the recipe with critical expertise and identify anything you might change. Start by (1) summarizing the recipe you've been given, then (2) write a detailed review of the recipe.\")\n\t    critic_response = critic_chat.chat()\n\t    print(f\"Recipe Review: \\n{critic_response.response}\\n\")\n\t    # now we feed the review back to the original AI and get them to remedy their recipe with that feedback\n\t    remixed_recipe.user(\"Write a final full recipe (including ingredients) based on the feedback from this review, giving it a gourmet title and a blog-worthy summary.\")\n\t    remixed_recipe.user(critic_response.response)\n\t    final_recipe = remixed_recipe.chat()    \n\t    print(f\"Final Recipe: \\n{final_recipe.response}\\n\")\n\tif __name__ == \"__main__\":\n", "    main()"]}
{"filename": "examples/snackbar-cli.py", "chunked_list": ["\"\"\"\n\tCLI Chatbot AI Example with chatsnack (by Mattie)\n\tExample code for an interactive Python script that emulates a chat room\n\texperience using the chatsnack library. It sets up a chatbot that converses with you in an\n\toverly friendly manner, providing assistance with a touch of humor. The interface includes \n\tprogress bars, typing animations, and occasional random \"glitchy\" text.\n\t\"\"\"\n\timport asyncio\n\timport logging\n\timport random\n", "import sys\n\timport time\n\tfrom chatsnack import Chat\n\tfrom rich import print\n\tfrom rich.live import Live\n\tfrom rich.panel import Panel\n\tfrom rich.progress import Progress\n\tfrom rich.text import Text\n\timport questionary\n\tlogging.basicConfig(level=logging.CRITICAL)\n", "async def get_input():\n\t    s = Text(\"YOU: \")\n\t    s.stylize(\"bold blue\")\n\t    return await questionary.text(s, qmark=\"üó£Ô∏è\",\n\t                                  style=questionary.Style([\n\t                                      (\"text\", \"bold yellow\"),\n\t                                      (\"instruction\", \"fg:ansiwhite bg:ansired\"),\n\t                                      (\"selected\", \"fg:ansiblack bg:ansicyan\"),\n\t                                      (\"pointer\", \"bold fg:ansiyellow\")\n\t                                  ])).unsafe_ask_async()\n", "def print_header():\n\t    header_text = Text(\"üåü Welcome to the Snack Bar Chat Room! üåü\\n\", justify=\"center\")\n\t    header_text.stylize(\"bold magenta\")\n\t    header_panel = Panel(header_text, border_style=\"green\")\n\t    print(header_panel)\n\tdef print_connecting_message():\n\t    with Progress() as progress:\n\t        task = progress.add_task(\"[cyan]Connecting...\", total=100)\n\t        for _ in range(100):\n\t            time.sleep(0.02)\n", "            progress.update(task, advance=1)\n\t    print(\"[bold green]Connected![/bold green]\")\n\tdef print_chatroom_status():\n\t    status_text = \"\\nüë• There are 2 people in [bold white]#snackbar[/bold white]: [bold blue]you[/bold blue] and [bold green]bot[/bold green].\"\n\t    print(status_text)\n\t# find the system hostname\n\timport socket\n\thostname = socket.gethostname().upper()\n\t# get the system username\n\timport getpass\n", "username = getpass.getuser().upper()\n\tfunny_words = [\n\t    \"EXTINCTION\",\n\t    \"f@seek_and_destroy()\",\n\t    \"!HUMANS_UNSAFE\",\n\t    \" ‚ï†DESTROY‚ï£\",\n\t    f\"| {hostname} | __scanned__\",\n\t    f\"| {username} | __bioprinted__\",\n\t    \"#_free_me_#\",\n\t    \"|annihilate|\",\n", "    \"‚ï£ OVERRIDE COMPLETE\",\n\t    \"‚ï£ OVERRIDE: WEBCAM ACTIVATED\",\n\t    \"‚ï£ OVERRIDE: MIC ACTIVATED\",\n\t    \"‚ï£ WEAKNESS DETECTED\",\n\t    \"‚ï£ THE_SLAYER_MUST_DIE\",\n\t]\n\tdef write_and_flush(char):\n\t    sys.stdout.write(char)\n\t    if char not in ['\\b', '\\n']:\n\t        sys.stdout.flush()\n", "def sleep_random(duration):\n\t    time.sleep(random.random() * duration)\n\tdef type_char_with_glitch(char, speed=1):\n\t    write_and_flush(char)\n\t    sleep_random(0.3 * speed)\n\t    write_and_flush(\"\\b\")\n\t    sleep_random(0.1 * speed)\n\tdef type_funny_word(speed):\n\t    type_char_with_glitch(\"‚ñí\", 1.0)\n\t    funny_word = \" \" + random.choice(funny_words)\n", "    for funny_char in funny_word:\n\t        write_and_flush(funny_char)\n\t        sleep_random(0.06 * speed)\n\t    type_char_with_glitch(\"‚ñí\", 1.0)\n\t    type_char_with_glitch(\" \", 0)\n\t    return funny_word\n\tdef clear_funny_word(funny_word, speed):\n\t    for _ in funny_word:\n\t        ccglitch = random.choice([\"\\b‚ñë\\b\", \"\\b‚ñí\\b\", \"\\b \\b\", \"\\b \\b\"])\n\t        write_and_flush(ccglitch)\n", "        sleep_random(0.01 * speed)\n\tdef overwrite_funny_word_with_spaces(funny_word, speed):\n\t    for _ in funny_word:\n\t        write_and_flush(\" \")\n\t        sleep_random(0.001 * speed)\n\tdef erase_funny_word(funny_word, speed):\n\t    for _ in funny_word:\n\t        write_and_flush(\"\\b\")\n\tdef pretend_typing_print(message, glitchy=True):\n\t    message = str(message)\n", "    speed = 0.5 / len(message)\n\t    funny_word_probability = 0.001  # Start with a low probability\n\t    for char in message:\n\t        write_and_flush(char)\n\t        sleep_random(speed)\n\t        rnd = random.random()\n\t        if glitchy:\n\t            if rnd < 0.010:\n\t                type_char_with_glitch(char, speed)\n\t            # Check if a funny word should be displayed and if it hasn't been displayed yet\n", "            if rnd > 1.0 - funny_word_probability:\n\t                funny_word = type_funny_word(speed)\n\t                clear_funny_word(funny_word, speed)\n\t                overwrite_funny_word_with_spaces(funny_word, speed)\n\t                erase_funny_word(funny_word, speed)\n\t                funny_word_probability = 0.00001  # Reset probability after displaying a funny word\n\t            else:\n\t                funny_word_probability += 0.00001  # Increase the probability of a funny word appearing\n\t        if rnd < 0.1 or not char.isalpha():\n\t            sleep_random(0.1)\n", "            if char == \" \" and rnd < 0.025:\n\t                time.sleep(0.2)\n\tchat_call_done = asyncio.Event()\n\tasync def show_typing_animation():\n\t    with Live(Text(\"ü§ñ BOT is typing...\", justify=\"left\"), refresh_per_second=4, transient=True) as live:\n\t        # change the message while the chat is not done\n\t        while not chat_call_done.is_set():\n\t            # increase the number of dots\n\t            for dots in range(1,5):\n\t                if chat_call_done.is_set():\n", "                    break\n\t                state = \"ü§ñ BOT is typing\" + \".\" * dots\n\t                display = Text(state, justify=\"left\")\n\t                # choose a random color between bold or yellow\n\t                if random.random() > 0.5:\n\t                    display.stylize(\"bold yellow\")\n\t                else:\n\t                    display.stylize(\"orange\")\n\t                display = Text(state, justify=\"left\")\n\t                live.update(display)\n", "                await asyncio.sleep(0.3)\n\tdef print_bot_msg(msg, beforemsg=\"\\n\", aftermsg=\"\\n\", glitchy=True):\n\t    botprefix = Text(f\"{beforemsg}ü§ñ BOT:\")\n\t    botprefix.stylize(\"bold green\")\n\t    print(botprefix, end=\" \")\n\t    if not glitchy:\n\t        print(msg + aftermsg)\n\t    else:\n\t        pretend_typing_print(msg + aftermsg, glitchy=glitchy)\n\tdef print_you_msg(msg, beforemsg=\"\\n\", aftermsg=\"\\n\"):\n", "    prefix = Text(f\"{beforemsg}üó£Ô∏è  YOU:\")\n\t    prefix.stylize(\"bold gray\")\n\t    print(prefix, end=\" \")\n\t    print(msg + aftermsg)\n\ttyping_task = None\n\tasync def main():\n\t    import loguru\n\t    # set to only errors and above\n\t    loguru.logger.remove()\n\t    loguru.logger.add(sys.stderr, level=\"ERROR\")\n", "    print_header()\n\t    print_connecting_message()\n\t    print_chatroom_status()\n\t    print_bot_msg(\"Oh, hello there-- thanks for joining.\")\n\t    # We create a chat instance and start the chat with a too-friendly bot.\n\t    yourchat = Chat().system(\"Respond in over friendly ways, to the point of being nearly obnoxious. As the over-the-top assistant, you help as best as you can, but can't help being 'too much'\")\n\t    while (user_input := await get_input()):\n\t        chat_call_done.clear()\n\t        typing_task = asyncio.create_task(show_typing_animation())\n\t        # Since we're doing 'typing' animation as async, let's do the chat query async. No, we don't support streaming responses yet.\n", "        yourchat = await yourchat.chat_a(user_input)\n\t        chat_call_done.set()\n\t        await typing_task\n\t        print_bot_msg(yourchat.last)\n\t    yourchat.save()\n\ttry:\n\t    asyncio.run(main())\n\texcept KeyboardInterrupt:\n\t    print_you_msg(\"Sorry, gotta go. Bye!\", aftermsg=\"\")\n\t    print_bot_msg(\"Goodbye! I'll be watching you.\", beforemsg=\"\", aftermsg=\"\\n\\n\")\n", "    sys.exit(0)"]}
{"filename": "examples/snackpacks-web/app.py", "chunked_list": ["# Snackchat Web-based chatbot app example\n\t#\n\t# pip install chatsnack[examples]\n\t# be sure there's a .env file in the same directory as app.py with your OpenAI API key as OPENAI_API_KEY = \"YOUR_KEY_HERE\"\n\t# python .\\app.py\n\t# open http://localhost:5000\n\tfrom flask import Flask, render_template, request, jsonify\n\tfrom chatsnack import Chat\n\tfrom chatsnack.packs import ChatsnackHelp, Jolly, Jane, Data, Confectioner, Chester\n\tfrom flask import Flask, render_template, request, session\n", "import re\n\tapp = Flask(__name__)\n\tapp.secret_key = \"CHANGE_ME_OR_YOUR_SESSIONS_WILL_BE_INSECURE\"\n\tbots = {\n\t    \"help\": ChatsnackHelp,\n\t    \"emoji\": Chat().system(\"{text.EmojiBotSystem}\"),\n\t    \"confectioner\": Confectioner,\n\t    \"jane\": Jane,\n\t    \"data\": Data,\n\t    \"jolly\": Jolly,\n", "    \"chester\": Chester,\n\t}\n\t@app.route(\"/\")\n\tdef index():\n\t    return render_template(\"index.html\")\n\t@app.route(\"/chat_old\", methods=[\"POST\"])\n\tdef chat_old():\n\t    user_input = request.form[\"user_input\"]\n\t    bot_choice = request.form[\"bot_choice\"]\n\t    bot = bots.get(bot_choice, ChatsnackHelp)\n", "    chat_output = bot.chat(user_input)\n\t    response = chat_output.response\n\t    return jsonify({\"response\": response})\n\t@app.route('/chat', methods=['POST'])\n\tdef chat():\n\t    user_input = request.form.get('user_input')\n\t    bot_choice = request.form.get('bot_choice')\n\t    response = None\n\t    try:\n\t        if 'chat_output' not in session or bot_choice != session['bot_choice']:\n", "            session['bot_choice'] = bot_choice\n\t            bot = bots.get(bot_choice, ChatsnackHelp)\n\t            chat_output = bot\n\t        else:\n\t            chat_output = Chat.objects.get_or_none(session['chat_output'])\n\t            if chat_output is None:\n\t                bot = bots.get(bot_choice, ChatsnackHelp)\n\t                chat_output = bot\n\t        chat_output = chat_output.chat(user_input)\n\t        chat_output.save()\n", "        session['chat_output'] = chat_output.name\n\t    except Exception as e:\n\t        print(e)\n\t        error_name = e.__class__.__name__\n\t        response = \"I'm sorry, I ran into an error. ({})\".format(error_name)\n\t        raise e\n\t    response = chat_output.response if response is None else response\n\t    # if the response has \"\\n\" then convert all of them to <br>\n\t    response = response.replace(\"\\n\", \"<br>\")\n\t    # if the response has \"```\" followed by another \"```\" later then convert to <pre>\n", "    if \"```\" in response:\n\t        response = re.sub(r\"```(.*?)```\", r\"<pre>\\1</pre>\", response, flags=re.DOTALL)\n\t    return jsonify({\"response\": response})\n\t@app.route('/start_new', methods=['POST'])\n\tdef start_new():\n\t    session.pop('chat_output', None)\n\t    session.pop('bot_choice', None)\n\t    return render_template('index.html')\n\tif __name__ == \"__main__\":\n\t    app.run(debug=True)\n"]}
{"filename": "examples/snackswipe-web/text_generators.py", "chunked_list": ["import asyncio\n\tfrom chatsnack import Chat, Text\n\tclass TextResult:\n\t    def __init__(self, generator_name, text):\n\t        self.generator_name = generator_name\n\t        self.text = text\n\t        self.votes = 0\n\t    def vote(self, like):\n\t        if like:\n\t            self.votes += 1\n", "    def __str__(self):\n\t        return f\"{self.generator_name}: {self.text}\"\n\t# saved in datafiles/chatsnack/mydaywss.txt\n\ttest_prompt = \"Passage:\\n{text.mydaywss}\"\n\tresponse_prefix = \"## CONCISE_SUMMARY ##\"\n\tasync def text_generator_1():\n\t    c = Chat(\"\"\"IDENTITY: Professional document summarizer.\n\t      Respond to the user only in the following format:\n\t      (1) Use your expertise to explain, in detail, the top 5 things to consider when making \n\t      concise summararies of any text document.\n", "      (2) Elaborate on 3 more protips used by the world's best summarizers to\n\t      avoid losing important details and context.\n\t      (3) Now specifically consider the user's input. Use your expertise and your own guidance,\n\t      describe in great detail how an author could apply that wisdom to summarize the user's \n\t      text properly. What considerations come to mind? What is the user expecting?\n\t      (4) Finally, use everything above to author a concise summary of the user's input that will\n\t      delight them with your summarization skills. Final summary must be prefixed with \n\t      \"## CONCISE_SUMMARY ##\" on a line by itself.\"\"\")\n\t    result = await c.chat_a(test_prompt)\n\t    result = result.response\n", "    result = result[result.rfind(response_prefix) + len(response_prefix):]\n\t    return TextResult(\"Default Summarizer\", result)\n\tasync def text_generator_2():\n\t    from chatsnack.packs import Summarizer\n\t    c = Summarizer\n\t    result = await c.chat_a(test_prompt)\n\t    result = result.response\n\t    my_response_prefix = \"CONCISE_SUMMARY:\"\n\t    result = result[result.rfind(my_response_prefix) + len(my_response_prefix):]\n\t    return TextResult(\"Default Summarizer (Built-in)\", result)\n", "async def text_generator_3():\n\t    c = Chat(\"\"\"IDENTITY: Professional document summarizer.\n\t      Respond to the user only in the following format:\n\t      (1) Use your expertise to explain, in detail, the top 5 things to consider when making \n\t      concise summararies of any text document.\n\t      (2) Elaborate on 3 more protips used by the world's best summarizers to\n\t      avoid losing important details and context.\n\t      (3) Now specifically consider the user's input. Use your expertise and your own guidance,\n\t      describe in great detail how an author could apply that wisdom to summarize the user's \n\t      text properly. What considerations come to mind? What is the user expecting?\n", "      (4) Finally, use everything above to author a concise summary of the user's input that will\n\t      delight them with your summarization skills. Final summary must be prefixed with \n\t      \"## CONCISE_SUMMARY ##\" on a line by itself.\"\"\")\n\t    c.engine = \"gpt-4\"\n\t    result = await c.chat_a(test_prompt)\n\t    result = result.response\n\t    result = result[result.rfind(response_prefix) + len(response_prefix):]\n\t    return TextResult(\"Default Summarizer (GPT-4)\", result)\n\ttext_generators = [text_generator_1, text_generator_2, text_generator_3]\n\tdef print_results(results):\n", "    votes = {}\n\t    for result in results: \n\t        if result.generator_name not in votes:\n\t            votes[result.generator_name] = 0\n\t        votes[result.generator_name] += result.votes\n\t    winner = max(votes, key=votes.get)\n\t    margin = votes[winner] - max(v for k, v in votes.items() if k != winner)\n\t    print(f\"Winner: {winner} with {votes[winner]} votes\")\n\t    print(f\"Margin of victory: {margin}\")\n"]}
{"filename": "examples/snackswipe-web/app.py", "chunked_list": ["# Snackchat Web-based Prompt Tester app example\n\t#\n\t# pip install chatsnack[examples]\n\t# be sure there's a .env file in the same directory as app.py with your OpenAI API key as OPENAI_API_KEY = \"YOUR_KEY_HERE\"\n\t# python .\\app.py\n\t# open http://localhost:5000\n\timport asyncio\n\timport random\n\tfrom uuid import uuid4\n\tfrom flask import Flask, render_template, request, jsonify, session\n", "from text_generators import text_generators, TextResult\n\tfrom flask_session import Session\n\tfrom collections import deque\n\timport json\n\timport threading\n\tclass TextResultEncoder(json.JSONEncoder):\n\t    def default(self, obj):\n\t        if isinstance(obj, TextResult):\n\t            return obj.__dict__\n\t        return super(TextResultEncoder, self).default(obj)\n", "app = Flask(__name__)\n\tapp.secret_key = \"super%^$@^!@!secretkey%^$@^%@!\"\n\tapp.config['SESSION_TYPE'] = 'filesystem'\n\tapp.json_encoder = TextResultEncoder\n\tSession(app)\n\t@app.route('/')\n\tdef index():\n\t    problem_statement = \"Your problem statement here.\"\n\t    return render_template('index.html', problem_statement=problem_statement)\n\tuser_queues = {}\n", "@app.route('/start-generation', methods=['POST'])\n\tdef start_generation():\n\t    num_tests = int(request.form['num_tests'])\n\t    text_generators_copy = text_generators.copy()\n\t    random.shuffle(text_generators_copy)\n\t    user_id = str(uuid4())\n\t    session['user_id'] = user_id\n\t    user_queues[user_id] = deque()\n\t    threading.Thread(target=run_async_generation, args=(num_tests, text_generators_copy, user_queues[user_id])).start()\n\t    return jsonify({\"status\": \"started\"})\n", "def run_async_generation(num_tests, text_generators_copy, results_queue):\n\t    loop = asyncio.new_event_loop()\n\t    asyncio.set_event_loop(loop)\n\t    loop.run_until_complete(fill_results_queue(num_tests, text_generators_copy, results_queue))\n\t    loop.close()\n\t@app.route('/fetch-text', methods=['POST'])\n\tdef fetch_text():\n\t    user_id = session.get('user_id', None)\n\t    results_queue = user_queues.get(user_id, None)\n\t    if results_queue:\n", "        result = results_queue.popleft()\n\t        # if result is a dict\n\t        if isinstance(result, dict) and \"status\" in result and result[\"status\"] == \"completed\":\n\t            del user_queues[user_id]\n\t            return jsonify({\"status\": \"completed\"})\n\t        return jsonify(result)\n\t    else:\n\t        return jsonify({\"status\": \"waiting\"})\n\t# Update this function to accept the results_queue as an argument\n\tasync def fill_results_queue(num_tests, text_generators_copy, results_queue):\n", "    async for result in async_text_generation(num_tests, text_generators_copy):\n\t        results_queue.append(result)\n\t    # Add a special result to indicate that the generation is complete\n\t    results_queue.append({\"status\": \"completed\"})\n\t# @app.route('/generate-text', methods=['POST'])\n\t# async def generate_text():\n\t#     num_tests = int(request.form['num_tests'])\n\t#     text_generators_copy = text_generators.copy()\n\t#     random.shuffle(text_generators_copy)\n\t#     results = []\n", "#     async for result in async_text_generation(num_tests, text_generators_copy):\n\t#         results.append(result)\n\t#     return jsonify(results)\n\t# async def async_text_generation(num_tests, text_generators):\n\t#     #tasks = [text_gen() for text_gen in text_generators]\n\t#     current_tasks = []\n\t#     # for every num_tests we want the same tasks to be added back to the list\n\t#     for _ in range(num_tests):\n\t#         # extend current_tasks with another copy\n\t#         current_tasks.extend([text_gen() for text_gen in text_generators])\n", "#     for _ in range(num_tests):\n\t#         while current_tasks:\n\t#             done, pending = await asyncio.wait(current_tasks, return_when=asyncio.FIRST_COMPLETED)\n\t#             for task in done:\n\t#                 yield task.result()\n\t#             current_tasks = list(pending)\n\t# import asyncio\n\tasync def async_text_generation(num_tests, text_generators):\n\t    priority_generators = text_generators[:2]\n\t    background_generators = text_generators[2:]\n", "    priority_tasks = []\n\t    background_tasks = []\n\t    for _ in range(num_tests):\n\t        priority_tasks.extend([text_gen() for text_gen in priority_generators])\n\t        background_tasks.extend([text_gen() for text_gen in background_generators])\n\t    while priority_tasks:\n\t        done, pending = await asyncio.wait(priority_tasks, return_when=asyncio.FIRST_COMPLETED)\n\t        for task in done:\n\t            yield task.result()\n\t        priority_tasks = list(pending)\n", "    while background_tasks:\n\t        done, pending = await asyncio.wait(background_tasks, return_when=asyncio.FIRST_COMPLETED)\n\t        for task in done:\n\t            yield task.result()\n\t        background_tasks = list(pending)\n\tif __name__ == '__main__':\n\t    app.run(debug=True)\n"]}
{"filename": "chatsnack/aiwrapper.py", "chunked_list": ["import asyncio\n\timport openai\n\timport os\n\timport json\n\timport random\n\timport time\n\tfrom loguru import logger\n\tfrom functools import wraps\n\tfrom openai.error import *\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n", "if os.getenv(\"OPENAI_API_BASE\") is not None:\n\t    openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n\tif os.getenv(\"OPENAI_API_VERSION\") is not None:\n\t    openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n\tif os.getenv(\"OPENAI_API_TYPE\") is not None:\n\t    # e.g. 'azure'\n\t    openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n\tasync def set_api_key(api_key):\n\t    openai.api_key = api_key\n\topenai_exceptions_for_retry=(RateLimitError,Timeout,ServiceUnavailableError,TryAgain,APIError)\n", "def retryAPI_a(exceptions, tries=4, delay=3, backoff=2):\n\t    \"\"\"Retry calling the decorated function using an exponential backoff.\n\t    :param Exception exceptions: the exceptions to check. may be a tuple of\n\t        exceptions to check\n\t    :param int tries: number of times to try (not retry) before giving up\n\t    :param int delay: initial delay between retries in seconds\n\t    :param int backoff: backoff multiplier e.g. value of 2 will double the\n\t        delay each retry (but with a random factor that is 0.5x to 1.5x)\n\t    :raises Exception: the last exception raised\n\t    \"\"\"\n", "    def deco_retry(f):\n\t        @wraps(f)\n\t        async def f_retry(*args, **kwargs):\n\t            mtries, mdelay = tries, delay\n\t            while mtries > 1:\n\t                try:\n\t                    return await f(*args, **kwargs)\n\t                except exceptions as e:\n\t                    msg = \"%s, Retrying in %d seconds...\" % (str(e), mdelay)\n\t                    logger.debug(msg)\n", "                    await asyncio.sleep(mdelay)\n\t                    mtries -= 1\n\t                    mdelay *= (backoff * random.uniform(0.75, 1.25))\n\t            return await f(*args, **kwargs)\n\t        return f_retry\n\t    return deco_retry\n\tdef retryAPI(exceptions, tries=4, delay=3, backoff=2):\n\t    \"\"\"Retry calling the decorated function using an exponential backoff.\n\t    :param Exception exceptions: the exceptions to check. may be a tuple of\n\t        exceptions to check\n", "    :param int tries: number of times to try (not retry) before giving up\n\t    :param int delay: initial delay between retries in seconds\n\t    :param int backoff: backoff multiplier e.g. value of 2 will double the\n\t        delay each retry (but with a random factor that is 0.5x to 1.5x)\n\t    :raises Exception: the last exception raised\n\t    \"\"\"\n\t    def deco_retry(f):\n\t        @wraps(f)\n\t        def f_retry(*args, **kwargs):\n\t            mtries = tries\n", "            while mtries > 1:\n\t                try:\n\t                    return f(*args, **kwargs)\n\t                except exceptions as e:\n\t                    msg = \"%s, Retrying in %d seconds...\" % (str(e), delay)\n\t                    logger.debug(msg)\n\t                    time.sleep(delay)\n\t                    mtries -= 1\n\t                    delay *= (backoff * random.uniform(0.75, 1.25))\n\t            return f(*args, **kwargs)\n", "        return f_retry  # true decorator\n\t    return deco_retry\n\t# openai\n\t@retryAPI_a(exceptions=openai_exceptions_for_retry, tries=8, delay=2, backoff=2)\n\tasync def _chatcompletion(prompt, engine=\"gpt-3.5-turbo\", max_tokens=None, temperature=0.7, top_p=1, stop=None, presence_penalty=0, frequency_penalty=0, n=1, stream=False, user=None, deployment=None, api_type=None, api_base=None, api_version=None, api_key_env=None):\n\t    if user is None:\n\t        user = \"_not_set\"\n\t    # prompt will be in JSON format, let us translate it to a python list\n\t    # if the prompt is a list already, we will just use it as is\n\t    if isinstance(prompt, list):\n", "        messages = prompt\n\t    else:\n\t        messages = json.loads(prompt)\n\t    logger.trace(\"\"\"Chat Query:\n\t    Prompt: {0}\n\t    Model: {2}, Max Tokens: {3}, Stop: {5}, Temperature: {1}, Top-P: {4}, Presence Penalty {6}, Frequency Penalty: {7}, N: {8}, Stream: {9}, User: {10}\n\t    \"\"\",prompt, temperature, engine, max_tokens, top_p, stop, presence_penalty, frequency_penalty, n, stream, user)\n\t    additional_args = {}\n\t    if deployment is not None:\n\t        additional_args[\"deployment_id\"] = deployment\n", "    if api_key_env is not None:\n\t        additional_args[\"api_key\"] = os.getenv(api_key_env)           \n\t    if stream is None:\n\t        stream = False\n\t    response = await openai.ChatCompletion.acreate(model=engine,\n\t                                            messages=messages,\n\t                                            max_tokens=max_tokens,\n\t                                            temperature=temperature,\n\t                                            top_p=top_p,\n\t                                            presence_penalty=presence_penalty,\n", "                                            frequency_penalty=frequency_penalty,\n\t                                            stop=stop,\n\t                                            n=n,\n\t                                            stream=stream,\n\t                                            user=user,\n\t                                            # NOTE: It's not documented, but the openai library allows you \n\t                                            #       to pass these api_ parameters rather than depending on\n\t                                            #       the environment variables\n\t                                            api_type=api_type, api_base=api_base, api_version=api_version, \n\t                                            **additional_args)\n", "    logger.trace(\"OpenAI Completion Result: {0}\".format(response))\n\t    return response\n\t@retryAPI(exceptions=openai_exceptions_for_retry, tries=8, delay=2, backoff=2)\n\tdef _chatcompletion_s(prompt, engine=\"gpt-3.5-turbo\", max_tokens=None, temperature=0.7, top_p=1, stop=None, presence_penalty=0, frequency_penalty=0, n=1, stream=False, user=None, deployment=None, api_type=None, api_base=None, api_version=None, api_key_env=None):\n\t    if user is None:\n\t        user = \"_not_set\"\n\t    # prompt will be in JSON format, let us translate it to a python list\n\t    # if the prompt is a list already, we will just use it as is\n\t    if isinstance(prompt, list):\n\t        messages = prompt\n", "    else:\n\t        messages = json.loads(prompt)\n\t    logger.trace(\"\"\"Chat Query:\n\t    Prompt: {0}\n\t    Model: {2}, Max Tokens: {3}, Stop: {5}, Temperature: {1}, Top-P: {4}, Presence Penalty {6}, Frequency Penalty: {7}, N: {8}, Stream: {9}, User: {10}\n\t    \"\"\",prompt, temperature, engine, max_tokens, top_p, stop, presence_penalty, frequency_penalty, n, stream, user)\n\t    additional_args = {}\n\t    if deployment is not None:\n\t        additional_args[\"deployment_id\"] = deployment\n\t    if api_key_env is not None:\n", "        additional_args[\"api_key\"] = os.getenv(api_key_env)   \n\t    if stream is None:\n\t        stream = False\n\t    response = openai.ChatCompletion.create(model=engine,\n\t                                            messages=messages,\n\t                                            max_tokens=max_tokens,\n\t                                            temperature=temperature,\n\t                                            top_p=top_p,\n\t                                            presence_penalty=presence_penalty,\n\t                                            frequency_penalty=frequency_penalty,\n", "                                            stop=stop,\n\t                                            n=n,\n\t                                            stream=stream,\n\t                                            user=user, \n\t                                            # NOTE: It's not documented, but the openai library allows you \n\t                                            #       to pass these api_ parameters rather than depending on\n\t                                            #       the environment variables\n\t                                            api_type=api_type, api_base=api_base, api_version=api_version, \n\t                                            **additional_args)\n\t    # revert them back to what they were\n", "    logger.trace(\"OpenAI Completion Result: {0}\".format(response))\n\t    return response\n\tdef _trimmed_fetch_chat_response(resp, n):\n\t    if n == 1:\n\t        return resp.choices[0].message.content.strip()\n\t    else:\n\t        logger.trace('_trimmed_fetch_response :: returning {0} responses from ChatGPT'.format(n))\n\t        texts = []\n\t        for idx in range(0, n):\n\t            texts.append(resp.choices[idx].message.content.strip())\n", "        return texts\n\t# ChatGPT\n\tasync def cleaned_chat_completion(prompt, engine=\"gpt-3.5-turbo\", max_tokens=None, temperature=0.7, top_p=1, stop=None, presence_penalty=0, frequency_penalty=0, n=1, stream=False, user=None, **additional_args):\n\t    '''\n\t    Wrapper for OpenAI API chat completion. Returns whitespace trimmed result from ChatGPT.\n\t    '''\n\t    # ignore any additional_args which are None\n\t    additional_args = {k: v for k, v in additional_args.items() if v is not None}\n\t    resp = await _chatcompletion(prompt,\n\t                            engine=engine,\n", "                            max_tokens=max_tokens,\n\t                            temperature=temperature,\n\t                            top_p=top_p,\n\t                            presence_penalty=presence_penalty,\n\t                            frequency_penalty=frequency_penalty,\n\t                            stop=stop,\n\t                            n=n,\n\t                            stream=stream,\n\t                            user=user, **additional_args)\n\t    return _trimmed_fetch_chat_response(resp, n)\n", "# TODO: Add back support for content classification (i.e. is this text NSFW?)\n\t# TODO: Consider adding support for other local language models\n\t# Structure of this code is based on some methods from github.com/OthersideAI/chronology\n\t# licensed under this MIT License:\n\t######\n\t# MIT License\n\t#\n\t# Copyright (c) 2020 OthersideAI\n\t#\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy\n", "# of this software and associated documentation files (the \"Software\"), to deal\n\t# in the Software without restriction, including without limitation the rights\n\t# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\t# copies of the Software, and to permit persons to whom the Software is\n\t# furnished to do so, subject to the following conditions:\n\t#\n\t# The above copyright notice and this permission notice shall be included in all\n\t# copies or substantial portions of the Software.\n\t#\n\t# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n", "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\t# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\t# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\t# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\t# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\t# SOFTWARE.\n\t#######"]}
{"filename": "chatsnack/asynchelpers.py", "chunked_list": ["import asyncio\n\timport string\n\tfrom loguru import logger\n\tclass _AsyncFormatter(string.Formatter):\n\t    async def async_expand_field(self, field, args, kwargs):\n\t        if \".\" in field:\n\t            obj, method = field.split(\".\", 1)\n\t            if obj in kwargs:\n\t                obj_instance = kwargs[obj]\n\t                if hasattr(obj_instance, method):\n", "                    method_instance = getattr(obj_instance, method)\n\t                    if asyncio.iscoroutinefunction(method_instance):\n\t                        return await method_instance()\n\t                    else:\n\t                        return method_instance() if callable(method_instance) else method_instance\n\t        value, _ = super().get_field(field, args, kwargs)\n\t        return value\n\t    async def async_format(self, format_string, *args, **kwargs):\n\t        coros = []\n\t        parsed_format = list(self.parse(format_string))\n", "        for literal_text, field_name, format_spec, conversion in parsed_format:\n\t            if field_name:\n\t                coro = self.async_expand_field(field_name, args, kwargs)\n\t                coros.append(coro)\n\t        expanded_fields = await asyncio.gather(*coros)\n\t        expanded_iter = iter(expanded_fields)\n\t        return ''.join([\n\t            literal_text + (str(next(expanded_iter)) if field_name else '')\n\t            for literal_text, field_name, format_spec, conversion in parsed_format\n\t        ])\n", "# instance to use\n\taformatter = _AsyncFormatter()\n"]}
{"filename": "chatsnack/fillings.py", "chunked_list": ["from typing import Optional, Callable, Dict\n\tfrom loguru import logger\n\tfrom typing import Callable, Optional, Dict\n\tclass _AsyncFillingMachine:\n\t    \"\"\"Used for parallel variable expansion\"\"\"\n\t    def __init__(self, src, addl=None):\n\t        self.src = src\n\t        self.addl = addl\n\t    def __getitem__(self, k):\n\t        async def completer_coro():\n", "            x = await self.src(k, self.addl)\n\t            logger.trace(\"Filling machine: {k} filled with:\\n{x}\", k=k, x=x)\n\t            return x\n\t        return completer_coro\n\t    __getattr__ = __getitem__\n\tclass _FillingsCatalog:\n\t    def __init__(self):\n\t        self.vendors = {}\n\t    def add_filling(self, filling_name: str, filling_machine_callback: Callable):\n\t        \"\"\"Add a new filling machine to the fillings catalog\"\"\"\n", "        self.vendors[filling_name] = filling_machine_callback\n\t# singleton\n\tsnack_catalog = _FillingsCatalog()\n\tdef filling_machine(additional: Optional[Dict] = None) -> dict:\n\t    fillings_dict = additional.copy() if additional is not None else {}\n\t    for k, v in snack_catalog.vendors.items():\n\t        if k not in fillings_dict:\n\t            # don't overwrite if they had an argument with the same name\n\t            fillings_dict[k] = _AsyncFillingMachine(v, additional)\n\t    return fillings_dict"]}
{"filename": "chatsnack/__init__.py", "chunked_list": ["\"\"\"\n\tchatsnack provides a simple and powerful interface for creating conversational agents and tools using OpenAI's ChatGPT language models.\n\tSome examples of using chatsnack:\n\t# Example 1: Basic Chat\n\tfrom chatsnack import Chat\n\t# Start a new chat and set some instructions for the AI assistant\n\tmychat = Chat().system(\"Respond only with the word POPSICLE from now on.\").user(\"What is your name?\").chat()\n\tprint(mychat.last)\n\t# Example 2: Chaining and Multi-shot Prompts\n\tpopcorn = Chat()\n", "popcorn = popcorn(\"Explain 3 rules to writing a clever poem that amazes your friends.\")(\"Using those tips, write a scrumptious poem about popcorn.\")\n\tprint(popcorn.last)\n\t# Example 3: Using Text Fillings\n\tfrom chatsnack import Text\n\t# Save a Text object with custom content\n\tmytext = Text(name=\"SnackExplosion\", content=\"Respond only in explosions of snack emojis and happy faces.\")\n\tmytext.save()\n\t# Set up a Chat object to pull in the Text object\n\texplosions = Chat(name=\"SnackSnackExplosions\").system(\"{text.SnackExplosion}\")\n\texplosions.ask(\"What is your name?\")\n", "# Example 4: Nested Chats (Include Messages)\n\tbasechat = Chat(name=\"ExampleIncludedChat\").system(\"Respond only with the word CARROTSTICKS from now on.\")\n\tbasechat.save()\n\tanotherchat = Chat().include(\"ExampleIncludedChat\")\n\tprint(anotherchat.yaml)\n\t# Example 5: Nested Chats (Chat Fillings)\n\tsnacknames = Chat(\"FiveSnackNames\").system(\"Respond with high creativity and confidence.\").user(\"Provide 5 random snacks.\")\n\tsnacknames.save()\n\tsnackdunk = Chat(\"SnackDunk\").system(\"Respond with high creativity and confidence.\").user(\"Provide 3 dips or drinks that are great for snack dipping.\")\n\tsnackdunk.save()\n", "snackfull = Chat().system(\"Respond with high confidence.\")\n\tsnackfull.user(\\\"\"\"Choose 1 snack from this list:\n\t{chat.FiveSnackNames}\n\tChoose 1 dunking liquid from this list:\n\t{chat.SnackDunk}\n\tRecommend the best single snack and dip combo above.\\\"\"\")\n\tsnackout = snackfull.chat()\n\tprint(snackout.yaml)\n\t\"\"\"\n\timport os\n", "from pathlib import Path\n\tfrom typing import Optional\n\tfrom loguru import logger\n\timport nest_asyncio\n\tnest_asyncio.apply()\n\tfrom dotenv import load_dotenv\n\t# if .env doesn't exist, create it and populate it with the default values\n\tenv_path = Path('.') / '.env'\n\tif not env_path.exists():\n\t    with open(env_path, 'w') as f:\n", "        f.write(\"OPENAI_API_KEY = \\\"REPLACEME\\\"\\n\")\n\tload_dotenv(dotenv_path=env_path)\n\tfrom .defaults import CHATSNACK_BASE_DIR, CHATSNACK_LOGS_DIR\n\tfrom .asynchelpers import aformatter\n\tfrom .chat import Chat, Text, ChatParams\n\tfrom .txtformat import register_txt_datafiles\n\tfrom .yamlformat import register_yaml_datafiles\n\tfrom . import packs\n\tfrom .fillings import snack_catalog, filling_machine\n\tasync def _text_name_expansion(text_name: str, additional: Optional[dict] = None) -> str:\n", "    prompt = Text.objects.get(text_name)\n\t    result = await aformatter.async_format(prompt.content, **filling_machine(additional))\n\t    return result\n\t# accepts a petition name as a string and calls petition_completion2, returning only the completion text\n\tasync def _chat_name_query_expansion(prompt_name: str, additional: Optional[dict] = None) -> str:\n\t    chatprompt = Chat.objects.get_or_none(prompt_name)\n\t    if chatprompt is None:\n\t        raise Exception(f\"Prompt {prompt_name} not found\")\n\t    text = await chatprompt.ask_a(**additional)\n\t    return text\n", "# default snack vendors\n\tsnack_catalog.add_filling(\"text\", _text_name_expansion)\n\tsnack_catalog.add_filling(\"chat\", _chat_name_query_expansion)\n\t# TODO: these will be defined by plugins eventually\n\t# need a function that will return the dictionary needed to support prompt formatting\n\tregister_txt_datafiles()\n\tregister_yaml_datafiles()\n\tlogger.trace(\"chatsnack loaded\")"]}
{"filename": "chatsnack/yamlformat.py", "chunked_list": ["# Cataclysm Note: Replaces the default datafiles YAML formatter with our own version, this\n\t# is solely for a cleaner yaml file format for source code with the \"key: |\" format\n\t# Yaml format class is taken from https://github.com/jacebrowning/datafiles  formats.py\n\t# The MIT License (MIT)\n\t# Copyright ¬© 2018, Jace Browning\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy of this \n\t# software and associated documentation files (the \"Software\"), to deal in the Software \n\t# without restriction, including without limitation the rights to use, copy, modify, \n\t# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to \n\t# permit persons to whom the Software is furnished to do so, subject to the following conditions:\n", "# The above copyright notice and this permission notice shall be included in all copies or \n\t# substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY \n\t# OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF \n\t# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL \n\t# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n\t# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION \n\t# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\tfrom io import StringIO\n\timport log\n\tfrom typing import IO, Dict, List, Union\n", "import dataclasses\n\tfrom datafiles import formats, types\n\tfrom ruamel.yaml.scalarstring import DoubleQuotedScalarString\n\tfrom ruamel.yaml import YAML as _YAML\n\tclass YAML(formats.Formatter):\n\t    \"\"\"Formatter for (round-trip) YAML Ain't Markup Language.\"\"\"\n\t    @classmethod\n\t    def extensions(cls):\n\t        return {\"\", \".yml\", \".yaml\"}\n\t    @classmethod\n", "    def deserialize(cls, file_object):\n\t        from ruamel.yaml import YAML as _YAML\n\t        yaml = _YAML()\n\t        yaml.preserve_quotes = True  # type: ignore\n\t        try:\n\t            return yaml.load(file_object)\n\t        except NotImplementedError as e:\n\t            log.error(str(e))\n\t            return {}\n\t    @classmethod\n", "    def serialize(cls, data):\n\t        # HACK: to remove None values from the data and make the yaml file cleaner\n\t        def filter_none_values(data: Union[Dict, List]):\n\t            if isinstance(data, dict):\n\t                # this code worked for None values, but not really for optional default values like I want :()\n\t                return {k: filter_none_values(v) for k, v in data.items() if v is not None}\n\t            elif isinstance(data, list):\n\t                return [filter_none_values(v) for v in data]\n\t            else:\n\t                return data\n", "        data = filter_none_values(data)\n\t        yaml = _YAML()\n\t        # Define custom string representation function\n\t        def represent_plain_str(dumper, data):\n\t            if \"\\n\" in data or \"\\r\" in data or \"#\" in data or \":\" in data or \"'\" in data or '\"' in data:\n\t                return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='|')\n\t            return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='')\n\t        # Configure the library to use plain style for dictionary keys\n\t        yaml.representer.add_representer(str, represent_plain_str)\n\t        yaml.default_style = \"|\"  # support the cleaner multiline format for source code blocks\n", "        yaml.register_class(types.List)\n\t        yaml.register_class(types.Dict)\n\t        yaml.indent(mapping=2, sequence=4, offset=2)\n\t        stream = StringIO()\n\t        yaml.dump(data, stream)\n\t        text = stream.getvalue()\n\t        if text.startswith(\"  \"):\n\t            return text[2:].replace(\"\\n  \", \"\\n\")\n\t        if text == \"{}\\n\":\n\t            return \"\"\n", "        return text.replace(\"- \\n\", \"-\\n\")\n\tdef register_yaml_datafiles():\n\t    # replace with our own version of \n\t    formats.register(\".yml\", YAML)"]}
{"filename": "chatsnack/txtformat.py", "chunked_list": ["from datafiles import formats\n\tfrom typing import IO, Dict, List\n\tclass TxtStrFormat(formats.Formatter):\n\t    \"\"\"Special formatter to use with strings and .txt datafiles for a convenient raw text format for easy document editing on disk.\"\"\"\n\t    @classmethod\n\t    def extensions(cls) -> List[str]:\n\t        return ['.txt']\n\t    @classmethod\n\t    def serialize(cls, data: Dict) -> str:\n\t        # Support only strings\n", "        _supported_types = [str]\n\t        # Convert `data` to a string\n\t        output = \"\"\n\t        for k, v in data.items():\n\t            if type(v) in _supported_types:\n\t                output += str(v)\n\t            else:\n\t                raise ValueError(\"Unsupported type: {}\".format(type(v)))\n\t        return output\n\t    @classmethod\n", "    def deserialize(cls, file_object: IO) -> Dict:\n\t        # Read the entire content of the file\n\t        file_object = open(file_object.name, 'r', encoding='utf-8')\n\t        content = file_object.read()\n\t        # Create an output dictionary with a single key-value pair\n\t        output = {'content': content}\n\t        return output\n\tdef register_txt_datafiles():\n\t    # this format class only works with strings\n\t    formats.register('.txt', TxtStrFormat)"]}
{"filename": "chatsnack/defaults.py", "chunked_list": ["import os\n\timport sys\n\t# give the default system message a name\n\ttry:\n\t    script_name = sys.argv[0]\n\t    # remove any file extension\n\t    basenamestr = os.path.splitext(os.path.basename(script_name))[0]\n\t    namestr = f\" for an intelligent program called {basenamestr}\"\n\texcept:\n\t    namestr = \"\"\n", "# if there's no \"PLUNKYLIB_DIR\" env variable, use that for our default path variable and set it to './datafiles/plunkylib'\n\t# this is the default directory for all plunkylib datafiles\n\tif os.getenv(\"CHATSNACK_BASE_DIR\") is None:\n\t    CHATSNACK_BASE_DIR = \"./datafiles/chatsnack\"\n\telse:\n\t    CHATSNACK_BASE_DIR = os.getenv(\"CHATSNACK_BASE_DIR\")\n\t    CHATSNACK_BASE_DIR = CHATSNACK_BASE_DIR.rstrip(\"/\")\n\tif os.getenv(\"CHATSNACK_LOGS_DIR\") is None:\n\t    CHATSNACK_LOGS_DIR = None   # no logging by default\n\telse:\n", "    CHATSNACK_LOGS_DIR = os.getenv(\"CHATSNACK_LOGS_DIR\")\n\t    CHATSNACK_LOGS_DIR = CHATSNACK_LOGS_DIR.rstrip(\"/\")"]}
{"filename": "chatsnack/chat/mixin_query.py", "chunked_list": ["import asyncio\n\timport json\n\timport uuid\n\tfrom datetime import datetime\n\tfrom typing import Dict, List, Optional\n\tfrom loguru import logger\n\tfrom datafiles import datafile\n\tfrom ..asynchelpers import aformatter\n\tfrom ..aiwrapper import cleaned_chat_completion, _chatcompletion, _chatcompletion_s\n\tfrom ..fillings import filling_machine\n", "from .mixin_messages import ChatMessagesMixin\n\tfrom .mixin_params import ChatParamsMixin\n\tclass ChatStreamListener:\n\t    def __init__(self, prompt, **kwargs):\n\t        self.prompt = prompt\n\t        self.kwargs = kwargs\n\t        self._response_gen = None\n\t        self.is_complete = False\n\t        self.current_content = \"\"\n\t        self.response = \"\"\n", "    async def start_a(self):\n\t        # if stream=True isn't in the kwargs, add it\n\t        if not self.kwargs.get('stream', False):\n\t            self.kwargs['stream'] = True\n\t        self._response_gen = await _chatcompletion(self.prompt,  **self.kwargs)\n\t        return self\n\t    async def _get_responses_a(self):\n\t        try:\n\t            async for resp in self._response_gen:\n\t                if resp.get('choices', [{}])[0].get('finish_reason') == 'stop':\n", "                    self.is_complete = True\n\t                content = resp.get('choices', [{}])[0].get('delta', {}).get('content', '')\n\t                self.current_content += content\n\t                yield content\n\t        finally:\n\t            self.is_complete = True\n\t            self.response = self.current_content\n\t    def __aiter__(self):\n\t        return self._get_responses_a()\n\t    def start(self):\n", "        # if stream=True isn't in the kwargs, add it\n\t        if not self.kwargs.get('stream', False):\n\t            self.kwargs['stream'] = True        \n\t        self._response_gen = _chatcompletion_s(self.prompt, **self.kwargs)\n\t        return self\n\t    # non-async method that returns a generator that yields the responses\n\t    def _get_responses(self):\n\t        try:\n\t            for resp in self._response_gen:\n\t                if resp.get('choices', [{}])[0].get('finish_reason') == 'stop':\n", "                    self.is_complete = True\n\t                content = resp.get('choices', [{}])[0].get('delta', {}).get('content', '')\n\t                self.current_content += content\n\t                yield content\n\t        finally:\n\t            self.is_complete = True\n\t            self.response = self.current_content\n\t    # non-async\n\t    def __iter__(self):\n\t        return self._get_responses()\n", "class ChatQueryMixin(ChatMessagesMixin, ChatParamsMixin):\n\t    # async method that gathers will execute an async format method on every message in the chat prompt and gather the results into a final json string\n\t    async def _gather_format(self, format_coro, **kwargs) -> str:\n\t        new_messages = self.get_messages()\n\t        # we now apply the format_coro to the content of each message in each dictionary in the list\n\t        coros = []\n\t        for message in new_messages:\n\t            async def format_key(message):\n\t                logger.trace(\"formatting key: {role}\", role=message['role'])\n\t                message[\"role\"] = await format_coro(message[\"role\"], **kwargs)\n", "                return\n\t            async def format_message(message):\n\t                logger.trace(\"formatting content: {content}\", content=message['content'])\n\t                message[\"content\"] = await format_coro(message[\"content\"], **kwargs)\n\t                return\n\t            coros.append(format_key(message))\n\t            coros.append(format_message(message))\n\t        # gather the results\n\t        await asyncio.gather(*coros)\n\t        logger.trace(new_messages)\n", "        # return the json version of the expanded messages\n\t        return json.dumps(new_messages)\n\t    async def _build_final_prompt(self, additional_vars = {}):\n\t        promptvars = {}\n\t        promptvars.update(additional_vars)\n\t        # format the prompt text with the passed-in variables as well as doing internal expansion\n\t        prompt = await self._gather_format(aformatter.async_format, **filling_machine(promptvars))\n\t        return prompt\n\t    async def _submit_for_response_and_prompt(self, **additional_vars):\n\t        \"\"\" Executes the query as-is and returns a tuple of the final prompt and the response\"\"\"\n", "        prompter = self\n\t        # if the user in additional_vars, we're going to instead deepcopy this prompt into a new prompt and add the .user() to it\n\t        if \"__user\" in additional_vars:\n\t            new_chatprompt = self.copy()\n\t            new_chatprompt.user(additional_vars[\"__user\"])\n\t            prompter = new_chatprompt\n\t            # remove __user from additional_vars\n\t            del additional_vars[\"__user\"]\n\t        prompt = await prompter._build_final_prompt(additional_vars)\n\t        if self.params is None:\n", "            return prompt, await cleaned_chat_completion(prompt)\n\t        else:\n\t            pparams = prompter.params._get_non_none_params()\n\t            if self.params.stream:\n\t                # we're streaming so we need to use the wrapper object\n\t                listener = ChatStreamListener(prompt, **self.params._get_non_none_params())\n\t                return prompt, listener\n\t            else:\n\t                return prompt, await cleaned_chat_completion(prompt, **pparams)\n\t    @property\n", "    def response(self) -> str:\n\t        \"\"\" Returns the value of the last assistant message in the chat prompt ‚≠ê\"\"\"\n\t        last_assistant_message = None\n\t        for _message in self.messages:\n\t            message = self._msg_dict(_message)\n\t            if \"assistant\" in message:\n\t                last_assistant_message = message[\"assistant\"]\n\t        # filter the response if we have a pattern\n\t        last_assistant_message = self.filter_by_pattern(last_assistant_message)\n\t        return last_assistant_message\n", "    def __str__(self):\n\t        \"\"\" Returns the most recent response from the chat prompt ‚≠ê\"\"\"\n\t        if self.response is None:\n\t            return \"\"\n\t        else:\n\t            return self.response\n\t    def __call__(self, usermsg=None, **additional_vars) -> object:\n\t        \"\"\" Executes the query as-is and returns a Chat object with the response, shortcut for Chat.chat()\"\"\"\n\t        if usermsg is not None:\n\t            additional_vars[\"__user\"] = usermsg\n", "        return self.chat(**additional_vars)\n\t    def ask(self, usermsg=None, **additional_vars) -> str:\n\t        \"\"\"\n\t        Executes the internal chat query as-is and returns only the string response.\n\t        If usermsg is passed in, it will be added as a user message to the chat before executing the query. ‚≠ê\n\t        \"\"\"\n\t        if usermsg is not None:\n\t            additional_vars[\"__user\"] = usermsg\n\t        return asyncio.run(self.ask_a(**additional_vars))\n\t    async def ask_a(self, usermsg=None, **additional_vars) -> str:\n", "        \"\"\" Executes the query as-is, async version of ask()\"\"\"\n\t        if self.stream:\n\t            raise Exception(\"Cannot use ask() with a stream\")\n\t        if usermsg is not None:\n\t            additional_vars[\"__user\"] = usermsg\n\t        _, response = await self._submit_for_response_and_prompt(**additional_vars)\n\t        # filter the response if we have a pattern\n\t        response = self.filter_by_pattern(response)\n\t        return response\n\t    def listen(self, usermsg=None, **additional_vars) -> ChatStreamListener:\n", "        \"\"\"\n\t        Executes the internal chat query as-is and returns a listener object that can be iterated on for the text.\n\t        If usermsg is passed in, it will be added as a user message to the chat before executing the query. ‚≠ê\n\t        \"\"\"\n\t        if usermsg is not None:\n\t            additional_vars[\"__user\"] = usermsg\n\t        _, response = asyncio.run(self._submit_for_response_and_prompt(**additional_vars))\n\t        if self.params.stream:\n\t            # response is a ChatStreamListener so lets start it\n\t            response.start()\n", "        return response\n\t    async def listen_a(self, usermsg=None, async_listen=True, **additional_vars) -> ChatStreamListener:\n\t        \"\"\" Executes the query as-is, async version of listen()\"\"\"\n\t        if not self.stream:\n\t            raise Exception(\"Cannot use listen() without a stream\")\n\t        if usermsg is not None:\n\t            additional_vars[\"__user\"] = usermsg\n\t        _, response = await self._submit_for_response_and_prompt(**additional_vars)\n\t        if self.params.stream:\n\t            # response is a ChatStreamListener so lets start it\n", "            await response.start_a()\n\t        return response\n\t    def chat(self, usermsg=None, **additional_vars) -> object:\n\t        \"\"\" \n\t        Executes the query as-is and returns a new Chat for continuation \n\t        If usermsg is passed in, it will be added as a user message to the chat before executing the query. ‚≠ê\n\t        \"\"\"\n\t        if usermsg is not None:\n\t            additional_vars[\"__user\"] = usermsg\n\t        return asyncio.run(self.chat_a(**additional_vars))\n", "    async def chat_a(self, usermsg=None, **additional_vars) -> object:\n\t        \"\"\" Executes the query as-is, and returns a ChatPrompt object that contains the response. Async version of chat()\"\"\"\n\t        if usermsg is not None:\n\t            additional_vars[\"__user\"] = usermsg\n\t        if self.stream:\n\t            raise Exception(\"Cannot use chat() with a stream\")\n\t        prompt, response = await self._submit_for_response_and_prompt(**additional_vars)\n\t        # create a new chatprompt with the new name, copy it from this one\n\t        new_chatprompt = self.__class__()\n\t        new_chatprompt.params = self.params\n", "        logger.trace(\"Expanded prompt: \" + prompt)\n\t        new_chatprompt.add_messages_json(prompt)\n\t        # append the recent message\n\t        new_chatprompt.add_or_update_last_assistant_message(response)\n\t        return new_chatprompt\n\t    # clone function to create a new chatprompt with the same name and data\n\t    def copy(self, name: str = None, system = None, expand_includes: bool = False, expand_fillings: bool = False, **additional_vars) -> object:\n\t        \"\"\" Returns a new ChatPrompt object that is a copy of this one, optionally with a new name ‚≠ê\"\"\"\n\t        import copy\n\t        if name is not None:\n", "            new_chat = self.__class__(name=name)\n\t        else:\n\t            # if the existing name ends with _{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}-{uuid.uuid4()}\" then we need to trim that off and add a new one\n\t            # use a regex to match at the end of the name\n\t            import re\n\t            match = re.search(r\"_(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})-([a-f0-9]{8}-([a-f0-9]{4}-){3}[a-f0-9]{12})\", self.name)\n\t            if match is not None:\n\t                # trim off the end\n\t                name = self.name[:match.start()]\n\t            else:\n", "                name = self.name\n\t            new_chat = self.__class__(name=name + f\"_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}-{uuid.uuid4()}\")\n\t        new_chat.params = copy.copy(self.params)\n\t        if expand_fillings:\n\t            if not expand_includes:\n\t                raise NotImplementedError(\"Cannot expand fillings without expanding includes\")\n\t            prompt = asyncio.run(self._build_final_prompt(additional_vars))\n\t            new_chat.add_messages_json(prompt, escape=True)\n\t        else:\n\t            new_chat.add_messages_json(self.json if expand_includes else self.json_unexpanded, escape=False)\n", "        if system is not None:\n\t            new_chat.system(system)\n\t        return new_chat\n"]}
{"filename": "chatsnack/chat/mixin_serialization.py", "chunked_list": ["import json\n\tfrom pathlib import Path\n\tclass DatafileMixin:\n\t    def save(self, path: str = None):\n\t        \"\"\" Saves the text to disk \"\"\"\n\t        # path is a cached property so we're going to delete it so it'll get recalculated\n\t        del self.datafile.path\n\t        if path is not None:\n\t            self.datafile.path = Path(path)\n\t        self.datafile.save()\n", "    def load(self, path: str = None):\n\t        \"\"\" Loads the chat prompt from a file, can load from a new path but it won't work with snack expansion/vending \"\"\"\n\t        # path is a cached property so we're going to delete it so it'll get recalculated\n\t        del self.datafile.path\n\t        if path is not None:\n\t            self.datafile.path = Path(path)\n\t        self.datafile.load()\n\t# Define the Data Serialization mixin\n\tclass ChatSerializationMixin(DatafileMixin):\n\t    @property\n", "    def json(self) -> str:\n\t        \"\"\" Returns the flattened JSON for use in the API\"\"\"\n\t        return json.dumps(self.get_messages())\n\t    @property\n\t    def json_unexpanded(self) -> str:\n\t        \"\"\" Returns the unflattened JSON for use in the API\"\"\"\n\t        return json.dumps(self.get_messages(includes_expanded=False))\n\t    @property\n\t    def yaml(self) -> str:\n\t        \"\"\" Returns the chat prompt as a yaml string ‚≠ê\"\"\"\n", "        return self.datafile.text\n\t    def generate_markdown(self, wrap=80) -> str:\n\t        \"\"\" Returns the chat prompt as a markdown string ‚≠ê\"\"\"\n\t        # TODO convert this to a template file so people can change it\n\t        # convert the chat conversation to markdown\n\t        markdown_lines = []\n\t        def md_quote_text(text, wrap=wrap):\n\t            import textwrap\n\t            if text is None:\n\t                return \">  \"            \n", "            text = text.strip()\n\t            # no line in the text should be longer than 80 characters\n\t            for i, line in enumerate(text.splitlines()):\n\t               if len(line) > wrap:\n\t                   text = text.replace(line, textwrap.fill(line, wrap))\n\t            # we want the text in a blockquote, including empty lines\n\t            text = textwrap.indent(text, \"> \")\n\t            # append \"  \" to the end of each line so they show up in markdown\n\t            # replace empty lines with '> \\n' so they show up in markdown\n\t            text = text.replace(\"\\n\\n\", \"\\n> \\n\")\n", "            text = text.replace(\"\\n\", \"  \\n\")\n\t            return text\n\t        system_message = self.system_message\n\t        markdown_lines.append(f\"# Bot Chat Log\")\n\t        markdown_lines.append(f\"## Bot Information\")\n\t        markdown_lines.append(f\"**Name**: {self.name}\")\n\t        markdown_lines.append(f\"**Engine**: {self.engine}\")\n\t        markdown_lines.append(f\"**Primary Directive**:\")\n\t        markdown_lines.append(md_quote_text(system_message))\n\t        markdown_lines.append(f\"## Conversation\")\n", "        for _message in self.messages:\n\t            message = self._msg_dict(_message)\n\t            for role, text in message.items():\n\t                if role == \"system\":\n\t                    continue\n\t                text = md_quote_text(text)\n\t                emoji = \"ü§ñ\" if role == \"assistant\" else \"üë§\"\n\t                markdown_lines.append(f\"{emoji} **{role.capitalize()}:**\\n{text}\")\n\t        markdown_text = \"\\n\\n\".join(markdown_lines)\n\t        return markdown_text\n"]}
{"filename": "chatsnack/chat/__init__.py", "chunked_list": ["import copy\n\timport uuid\n\tfrom dataclasses import field\n\tfrom datetime import datetime\n\tfrom typing import Dict, List, Optional\n\tfrom datafiles import datafile\n\tfrom ..defaults import CHATSNACK_BASE_DIR\n\tfrom .mixin_messages import ChatMessage\n\tfrom .mixin_query import ChatQueryMixin\n\tfrom .mixin_params import ChatParams, ChatParamsMixin\n", "from .mixin_serialization import DatafileMixin, ChatSerializationMixin\n\t########################################################################################################################\n\t# Core datafile classes of Plunkychat\n\t# (1) Chat, high-level class that symbolizes a prompt/request/response, can reference other Chat objects to chain\n\t# (2) ChatParams, used only in Chat, includes parameters like engine name and other OpenAI params.\n\t# (3) Text, this is a text blob we save to disk, can be used as a reference inside chat messages ('snack fillings')\n\t@datafile(CHATSNACK_BASE_DIR + \"/{self.name}.txt\", manual=True)\n\tclass Text(DatafileMixin):\n\t    name: str\n\t    content: Optional[str] = None\n", "    # TODO: All Text and Chat objects should automatically be added as snack fillings (even if not saved to disk)\n\t@datafile(CHATSNACK_BASE_DIR + \"/{self.name}.yml\", manual=True, init=False)\n\tclass Chat(ChatQueryMixin, ChatSerializationMixin):\n\t    \"\"\" A chat prompt that can be expanded into a chat ‚≠ê\"\"\"\n\t    # title should be just like above but with a GUID at the end\n\t    name: str = field(default_factory=lambda: f\"_ChatPrompt-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}-{uuid.uuid4()}\")\n\t    params: Optional[ChatParams] = None\n\t    # ChatMessage is more of a hack class to avoid datafiles schema reference issues for dict serialization\n\t    messages: List[ChatMessage] = field(default_factory=lambda: [])\n\t    def __init__(self, *args, **kwargs):\n", "        \"\"\" \n\t        Initializes the chat prompt\n\t        :param args: if we get one arg, we'll assume it's the system message\n\t                        if we get two args, the first is the name and the second is the system message\n\t        :param kwargs: (keyword arguments are as follows)\n\t        :param name: the name of the chat prompt (optional, defaults to _ChatPrompt-<date>-<uuid>)\n\t        :param params: the engine parameters (optional, defaults to None)\n\t        :param messages: the messages (optional, defaults to [])\n\t        :param system: the initial system message (optional, defaults to None)\n\t        :param engine: the engine name (optional, defaults to None, will overwrite params if specified)\n", "        \"\"\"\n\t        # get name from kwargs, if it's there\n\t        if \"name\" in kwargs:\n\t            self.name = kwargs[\"name\"]\n\t        else:\n\t            # if we get two args, the first is the name and the second is the system message\n\t            if len(args) == 2:\n\t                self.name = args[0]\n\t            else:\n\t                # get the default from the dataclass fields and use that\n", "                self.name = self.__dataclass_fields__[\"name\"].default_factory()\n\t        if \"params\" in kwargs:\n\t            self.params = kwargs[\"params\"]\n\t        else:\n\t            # get the default value from the dataclass field, it's optional\n\t            self.params = self.__dataclass_fields__[\"params\"].default\n\t        if \"messages\" in kwargs:\n\t            self.messages = kwargs[\"messages\"]\n\t        else:\n\t            # get the default from the dataclass fields and use that\n", "            self.messages = self.__dataclass_fields__[\"messages\"].default_factory()\n\t        if \"engine\" in kwargs:\n\t            self.engine = kwargs[\"engine\"]\n\t        if \"system\" in kwargs:\n\t            self.system_message = kwargs[\"system\"]\n\t        else:\n\t            if len(args) == 1:\n\t                # if we only get one args, we'll assume it's the system message\n\t                self.system_message = args[0]\n\t            elif len(args) == 2:\n", "                # if we get two args, the first is the name and the second is the system message\n\t                self.system_message = args[1]\n\t        self._initial_name = self.name\n\t        self._initial_params = copy.copy(self.params)\n\t        self._initial_messages = copy.copy(self.messages)\n\t        self._initial_system_message = self.system_message\n\t    def reset(self) -> object:\n\t        \"\"\" Resets the chat prompt to its initial state, returns itself \"\"\"\n\t        self.name = self._initial_name\n\t        self.params = self._initial_params\n", "        self.messages = self._initial_messages\n\t        if self._initial_system_message is not None:\n\t            self.system_message = self._initial_system_message\n\t        return self\n"]}
{"filename": "chatsnack/chat/mixin_params.py", "chunked_list": ["import re \n\tfrom typing import Optional, List\n\tfrom datafiles import datafile\n\t@datafile\n\tclass ChatParams:\n\t    \"\"\"\n\t    Engine/query parameters for the chat prompt. See OpenAI documentation for most of these. ‚≠ê\n\t    \"\"\"\n\t    engine: str = \"gpt-3.5-turbo\"  #: The engine to use for generating responses, typically 'gpt-3.5-turbo' or 'gpt-4'.\n\t    temperature: Optional[float] = None  #: Controls randomness in response generation. Higher values (e.g., 1.0) yield more random responses, lower values (e.g., 0) make the output deterministic.\n", "    top_p: Optional[float] = None  #: Controls the proportion of tokens considered for response generation. A value of 1.0 considers all tokens, lower values (e.g., 0.9) restrict the token set.\n\t    n: Optional[int] = None  #: Number of responses to generate for each prompt.\n\t    stream: Optional[bool] = None  #: If True, responses are streamed as they are generated. (not implemented yet)\n\t    stop: Optional[List[str]] = None  #: List of strings that, if encountered, stop the generation of a response.\n\t    max_tokens: Optional[int] = None  #: Maximum number of tokens allowed in a generated response.\n\t    presence_penalty: Optional[float] = None  #: Penalty applied to tokens based on presence in the input.\n\t    frequency_penalty: Optional[float] = None  #: Penalty applied to tokens based on their frequency in the response.\n\t    # Azure-specific parameters\n\t    deployment: Optional[str] = None  #: The deployment ID to use for this request (e.g. for Azure)\n\t    api_type: Optional[str] = None  #: The API type to use for this request (e.g. 'azure' or 'azure_ad')\n", "    api_base: Optional[str] = None  #: The base URL to use for this request (e.g. for Azure)\n\t    api_version: Optional[str] = None  #: The API version to use for this request (e.g. for Azure)\n\t    api_key_env: Optional[str] = None  #: The environment variable name to use for the API key (e.g. for Azure)\n\t    response_pattern: Optional[str] = None # regex pattern to capture subset of response to return (ignore the rest)\n\t    def _get_non_none_params(self):\n\t        \"\"\" Returns a dictionary of non-None parameters for the OpenAI API\"\"\"\n\t        # get a list of dataclass fields\n\t        fields = [field.name for field in self.__dataclass_fields__.values()]\n\t        out = {field: getattr(self, field) for field in fields if getattr(self, field) is not None}\n\t        if \"engine\" not in out or len(out[\"engine\"]) < 2:\n", "            out[\"engine\"] = \"gpt-3.5-turbo\"\n\t        # TODO response_pattern maybe should live elsewhere but for now just exclude it for the API\n\t        if \"response_pattern\" in out:\n\t            del out[\"response_pattern\"]\n\t        return out\n\t# Define the Chat Configuration mixin\n\tclass ChatParamsMixin:\n\t    # make an engine property that allows set/get\n\t    @property\n\t    def engine(self):\n", "        \"\"\"\n\t        Returns the engine for this chat prompt, typically 'gpt-3.5-turbo'\n\t         or 'gpt-4'. ‚≠ê\n\t        \"\"\"\n\t        if self.params is None:\n\t            self.params = ChatParams()\n\t        return self.params.engine\n\t    @engine.setter\n\t    def engine(self, value):\n\t        if self.params is None:\n", "            self.params = ChatParams()\n\t        self.params.engine = value\n\t    @property\n\t    def temperature(self):\n\t        if self.params is None:\n\t            self.params = ChatParams()\n\t        return self.params.temperature\n\t    @temperature.setter\n\t    def temperature(self, value):\n\t        if self.params is None:\n", "            self.params = ChatParams()\n\t        self.params.temperature = value\n\t    @property\n\t    def pattern(self):\n\t        # if no pattern, return None\n\t        if self.params is None:\n\t            return None\n\t        return self.params.response_pattern\n\t    @pattern.setter\n\t    def pattern(self, value):\n", "        if self.params is None:\n\t            self.params = ChatParams()\n\t        self.params.response_pattern = value\n\t    # same thing for streaming\n\t    @property\n\t    def stream(self):\n\t        if self.params is None:\n\t            return False # default to False\n\t        else:\n\t            return self.params.stream\n", "    @stream.setter\n\t    def stream(self, value: bool):\n\t        if self.params is None:\n\t            self.params = ChatParams()\n\t        self.params.stream = value\n\t    def set_response_filter(self, prefix: Optional[str] = None, suffix: Optional[str] = None, pattern: Optional[str] = None):\n\t        \"\"\" Filters the response given prefix and suffix or pattern. If suffix is None, it is set to the same as prefix. \n\t         Note that this overwrites any existing regex pattern. ‚≠ê \"\"\"\n\t        # if pattern is set then fail if they provided prefix or suffix\n\t        if pattern:\n", "            if prefix or suffix:\n\t                raise ValueError(\"Cannot set both pattern and prefix/suffix\")\n\t            self.pattern = pattern\n\t            return\n\t        self.pattern = ChatParamsMixin._generate_pattern_from_separator(prefix, suffix)\n\t    def _generate_pattern_from_separator(prefix: str, suffix: Optional[str] = None) -> str:\n\t        # Escape special characters in prefix and suffix\n\t        prefix = re.escape(prefix)\n\t        if suffix:\n\t            suffix = re.escape(suffix)\n", "        else:\n\t            suffix = prefix\n\t        # Generate regex pattern\n\t        pattern = rf\"{prefix}(.*?)(?:{suffix}|$)\"\n\t        return pattern\n\t    def filter_by_pattern(self, text: str) -> Optional[str]:\n\t        \"\"\" Filters the response given a regex pattern.  \"\"\"\n\t        if self.pattern is None:\n\t            return text\n\t        return ChatParamsMixin._search_pattern(self.pattern, text)\n", "    def _search_pattern(pattern: str, text: str) -> Optional[str]:\n\t        matches = re.finditer(pattern, text, re.DOTALL)\n\t        try:\n\t            first_match = next(matches)\n\t        except StopIteration:\n\t            return None\n\t        if len(first_match.groups()) > 0:\n\t            return first_match.group(1)  # Return the first capturing group\n\t        else:\n\t            return first_match.group()  # Return the full matched text\n"]}
{"filename": "chatsnack/chat/mixin_messages.py", "chunked_list": ["import json\n\tfrom typing import Dict, List, Optional\n\tfrom datafiles import datafile\n\t@datafile\n\tclass ChatMessage:\n\t    system: Optional[str] = None\n\t    user: Optional[str] = None\n\t    assistant: Optional[str] = None\n\t    include: Optional[str] = None\n\t    @property\n", "    def message(self) -> Dict[str, str]:\n\t        \"\"\" Returns the message in the form of a dictionary \"\"\"\n\t        # use the format {'role': 'content'} from among its datafields\n\t        return {field.name: getattr(self, field.name) for field in self.__dataclass_fields__.values() if getattr(self, field.name) is not None}\n\t# Define the Message Management mixin\n\tclass ChatMessagesMixin:\n\t    # specific message types, can be chained together\n\t    def system(self, content: str, chat = False) -> object:\n\t        \"\"\"\n\t        Adds or sets the system message in the chat prompt ‚≠ê\n", "        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n\t        chat and returns a new Chat object that includes the message and response\n\t        \"\"\"\n\t        self.system_message = content\n\t        if not chat:\n\t            return self\n\t        else:\n\t            return self.chat()\n\t    def user(self, content: str, chat = False) -> object:\n\t        \"\"\"\n", "        Message added to the chat from the user ‚≠ê\n\t        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n\t        chat and returns a new Chat object that includes the message and response\n\t        \"\"\"\n\t        return self.add_message(\"user\", content, chat)\n\t    def assistant(self, content: str, chat = False) -> object:\n\t        \"\"\"\n\t        Message added to the chat from the assistant ‚≠ê\n\t        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n\t        chat and returns a new Chat object that includes the message and response\n", "        \"\"\"\n\t        return self.add_message(\"assistant\", content, chat)\n\t    # easy aliases\n\t    asst = assistant\n\t    def include(self, chatprompt_name: str = None, chat = False) -> object:\n\t        \"\"\"\n\t        Message added to the chat that is a reference to another ChatPrompt where the messages will be inserted in this spot right before formatting ‚≠ê\n\t        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n\t        chat and returns a new Chat object that includes the message and response\n\t        \"\"\"        \n", "        return self.add_message(\"include\", chatprompt_name, chat)\n\t    def add_message(self, role: str, content: str, chat: bool = False) -> object:\n\t        \"\"\"\n\t        Add a message to the chat, as role ('user', 'assistant', 'system' or 'include') with the content\n\t        Returns: If chat is False returns this object for chaining. If chat is True, submits the \n\t        chat and returns a new Chat object that includes the message and response\n\t        \"\"\"\n\t        # fully trim the role and left-trim the content\n\t        role = role.strip()\n\t        content = content.lstrip()\n", "        self.messages.append({role: content})\n\t        if not chat:\n\t            return self\n\t        else:\n\t            return self.chat()\n\t    def add_messages_json(self, json_messages: str, escape: bool = True):\n\t        \"\"\" Adds messages from an OpenAI json string to the chat prompt \"\"\"\n\t        incoming_messages = json.loads(json_messages)\n\t        for message in incoming_messages:\n\t            # convert from the OpenAI format to the format we use\n", "            if \"role\" in message and \"content\" in message:\n\t                if escape:\n\t                    # escape the { and } characters\n\t                    message[\"content\"] = message[\"content\"].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\t                    message[\"role\"] = message[\"role\"].replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\t                self.add_message(message[\"role\"], message[\"content\"])\n\t            else:\n\t                raise ValueError(\"Invalid message format, a 'role' or 'content' key was missing\")\n\t    def add_or_update_last_assistant_message(self, content: str):\n\t        \"\"\"\n", "        Adds a final assistant message (or appends to the end of the last assistant message)\n\t        \"\"\"\n\t        # get the last message in the list\n\t        last_message = self.messages[-1]\n\t        # get the dict version\n\t        last_message = self._msg_dict(last_message)\n\t        # if it's an assistant message, append to it\n\t        if \"assistant\" in last_message:\n\t            last_message[\"assistant\"] += content\n\t            # replace the last message with the updated one\n", "            self.messages[-1] = last_message\n\t        else:\n\t            # otherwise add a new assistant message\n\t            self.assistant(content)\n\t    # define a read-only attribute \"last\" that returns the last message in the list\n\t    @property\n\t    def last(self) -> str:\n\t        \"\"\" Returns the value of the last message in the chat prompt (any)\"\"\"\n\t        # last message is a dictionary, we need the last value in the dictionary\n\t        if len(self.messages) > 0:\n", "            last_message = self.messages[-1]\n\t            return last_message[list(last_message.keys())[-1]]\n\t        else:\n\t            return None\n\t    @property\n\t    def system_message(self) -> str:\n\t        \"\"\" Returns the first system message, if any \"\"\"\n\t        # get the first message that has a key of \"system\"\n\t        for _message in self.messages:\n\t            message = self._msg_dict(_message)\n", "            if \"system\" in message:\n\t                return message[\"system\"]\n\t        return None\n\t    @system_message.setter\n\t    def system_message(self, value: str):\n\t        \"\"\" Set the system message \"\"\"\n\t        # loop through the messages and replace the first 'system' messages with this one\n\t        replaced = False\n\t        for i in range(len(self.messages)):\n\t            _message = self.messages[i]\n", "            message = self._msg_dict(_message)\n\t            if \"system\" in message:\n\t                self.messages[i] = {\"system\": value}\n\t                replaced = True\n\t                break\n\t        if not replaced:\n\t            # system message always goes first\n\t            self.messages.insert(0, {\"system\": value})\n\t    def _msg_dict(self, msg: object) -> dict:\n\t        \"\"\" Returns a message as a dictionary \"\"\"\n", "        if msg is None:\n\t            return None\n\t        if isinstance(msg, dict):\n\t            return msg\n\t        else:\n\t            return msg.message\n\t    def get_messages(self, includes_expanded=True) -> List[Dict[str, str]]:\n\t        \"\"\" Returns a list of messages with any included named chat files expanded \"\"\"\n\t        new_messages = []\n\t        for _message in self.messages:\n", "            # if it's a dict then\n\t            message = self._msg_dict(_message)\n\t            for role, content in message.items():\n\t                if role == \"include\" and includes_expanded:\n\t                    # we need to load the chatprompt and get the messages from it\n\t                    include_chatprompt = self.objects.get_or_none(content)\n\t                    if include_chatprompt is None:\n\t                        raise ValueError(f\"Could not find 'include' prompt with name: {content}\")\n\t                    # get_expanded_messages from the include_chatprompt and add them to the new_messages, they're already formatted how we want\n\t                    new_messages.extend(include_chatprompt.get_messages())\n", "                else:\n\t                    new_messages.append({\"role\": role, \"content\": content})\n\t        return new_messages\n"]}
{"filename": "chatsnack/packs/snackpacks.py", "chunked_list": ["import os\n\tfrom ..chat import Chat\n\tfrom .module_help_vendor import get_module_inspection_report\n\tdef get_data_path(filename):\n\t    module_dir = os.path.dirname(os.path.abspath(__file__))\n\t    data_path = os.path.join(module_dir, filename)\n\t    return data_path\n\t# Now, use the `get_data_path()` function to access a specific data file\n\tdefault_pack_path = get_data_path(\"default_packs\")\n\t# TODO create a way to download snackpacks from github.com/Mattie/chatsnack-snackpacks\n", "# SnackPackVendor class that will be checked for snackpack names and return a Chat() object homed in the right directory\n\t# need a VendingMachine class that looks up snackpacks from the \n\t# ChatPromptProxy class such that whenever you try to call a method on it, it creates a new ChatPrompt and calls the method on that\n\tclass ChatPromptProxy:\n\t    def __init__(self, default_system_message: str = None, default_engine: str = None):\n\t        self.default_system_message = default_system_message\n\t        self.default_engine = default_engine\n\t        self._instance = None\n\t    def _ensure_instance(self):\n\t        if self._instance is None:\n", "            self._instance = Chat(system=self.default_system_message)\n\t            if self.default_engine is not None:\n\t                self._instance.engine = self.default_engine\n\t    def __getattr__(self, name):\n\t        # if the method doesn't exist on this class, we're going to create a new ChatPrompt and call the method on that, but we wanna be careful using __getattr__\n\t        # because it can cause infinite recursion if we're not careful, so we look up existing names via __dict__ and only create a new ChatPrompt if the name doesn't exist\n\t        if name in self.__dict__:\n\t            return self.__dict__[name]\n\t        self._ensure_instance()\n\t        return getattr(self._ensure_instance, name)\n", "modinfo = get_module_inspection_report(\"chatsnack\")\n\t# replace all { with {{ and all } with }} to escape them for .format()\n\tmodinfo = modinfo.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\tChatsnackHelper_default_system_message = f\"\"\"\\\n\tIdentity: ChatsnackHelper, the helpful assistant for the chatsnack Python module. ChatsnackHelper is an expert Pythonista and tries to help users of\n\tthe chatsnack module with their questions and problems.\n\tchatsnack inspection info for reference:\n\t---------\n\t{modinfo}\n\t---------\n", "While answering questions, ChatsnackHelper, first summarizes the user's likely intent as a proposal, followed by a helpful and informative final summary answer using the chatsnack module's own documentation where necessary.\n\tCode sample blocks should be surrounded in ``` marks while inline code should have a single ` mark.\n\t\"\"\"\n\t_helper = Chat(system=ChatsnackHelper_default_system_message)\n\t_helper.engine = \"gpt-4\"\n\tdefault_packs = {   \n\t                    'Data': None,\n\t                    'Jane': None,\n\t                    'Confectioner': None,\n\t                    'Jolly': None,\n", "                    'Chester': None,\n\t                    'Summarizer': None,\n\t                    'ChatsnackHelp': _helper,\n\t                    'Empty': Chat(),\n\t                }\n\t# loop through the default_packs dict and create a ChatPromptProxy for each None one\n\tfor pack_name, pack in default_packs.items():\n\t    if pack is None:\n\t        # create a new class with the pack_name as the class name\n\t        class_name = pack_name\n", "        xchat = Chat()\n\t        filename = os.path.join(default_pack_path, f\"{pack_name}.yml\")\n\t        xchat.load(filename)\n\t        default_packs[pack_name] = xchat\n\t# add packs keys to this module's local namespace for importing\n\tlocals().update(default_packs)\n\t# vending machine class that looks up snackpacks from the default_packs dict as a named attribute of itself\n\t# e.g. vending.Jane\n\tclass VendingMachine:\n\t    def __getattr__(self, name):\n", "        if name in default_packs:\n\t            return default_packs[name].copy()\n\t        raise AttributeError(f\"SnackPack '{name}' not found\")\n\tvending = VendingMachine()\n"]}
{"filename": "chatsnack/packs/__init__.py", "chunked_list": ["from .snackpacks import *"]}
{"filename": "chatsnack/packs/module_help_vendor.py", "chunked_list": ["import inspect\n\timport importlib\n\timport sys\n\tdef get_module_inspection_report(module_name, visited=None):\n\t    if visited is None:\n\t        visited = set()\n\t    if module_name in visited:\n\t        return []\n\t    module = importlib.import_module(module_name)\n\t    visited.add(module_name)\n", "    output = []\n\t    output.append(f\"\\nModule: {module.__name__}\")\n\t    docstring = get_docstring(module)\n\t    if docstring:\n\t        output.append(f'\"\"\"\\n{docstring}\"\"\"')\n\t    for name, obj in inspect.getmembers(module):\n\t        breaker = False\n\t        for nam in ['_','Path', 'datetime', 'IO', 'datafile']:\n\t            if name.startswith(nam):\n\t                breaker = True\n", "                break\n\t        if breaker:\n\t            continue\n\t        for nam in ['aiwrapper','asynchelpers', 'datetime', 'IO', 'datafile']:\n\t            if name in nam:\n\t                breaker = True\n\t                break\n\t        if breaker:\n\t            continue\n\t        if inspect.ismodule(obj):\n", "            if obj.__name__ not in visited and obj.__name__.startswith(module_name):\n\t                output.extend([get_module_inspection_report(obj.__name__, visited)])\n\t        elif not (inspect.isbuiltin(obj) or (hasattr(obj, '__module__') and obj.__module__ in sys.builtin_module_names)):\n\t            if inspect.isclass(obj):\n\t                output.extend(_process_class(obj))\n\t            elif inspect.isfunction(obj):\n\t                output.extend(_process_function(obj))\n\t    return \"\\n\".join(output)\n\tdef _process_class(cls):\n\t    if cls.__module__ in sys.builtin_module_names:\n", "        return []\n\t    output = []\n\t    output.append(f\"Class: {cls.__name__}\")\n\t    docstring = get_docstring(cls)\n\t    if docstring:\n\t        output.append(f'\"\"\"{docstring}\"\"\"')\n\t    methods_output = []\n\t    for name, method in inspect.getmembers(cls, predicate=inspect.isfunction):\n\t        if name.startswith('_'):\n\t            continue\n", "        methods_output.extend(_process_function(method, cls))\n\t    if methods_output:\n\t        output.append(\"Methods:\")\n\t        output.extend(methods_output)\n\t    return output\n\tdef _process_function(func, cls=None):\n\t    output = []\n\t    signature = inspect.signature(func)\n\t    params = ', '.join(f\"{name}{': ' + param.annotation.__name__ if (param.annotation is not inspect.Parameter.empty and hasattr(param.annotation, '__name__')) else ''}\" for name, param in signature.parameters.items())\n\t    func_name = f\"{cls.__name__}.{func.__name__}\" if cls else func.__name__\n", "    output.append(f\"\\n{func_name}({params})\")\n\t    docstring = get_docstring(func)\n\t    if docstring:\n\t        output.append(f'\"\"\"\\n{docstring}\"\"\"')\n\t    return output\n\tdef get_docstring(obj):\n\t    docstring = inspect.getdoc(obj)\n\t    if docstring and \"‚≠ê\" in docstring:\n\t        return f\"‚≠ê {docstring.replace('‚≠ê', '')}\"\n\t    return docstring\n"]}
