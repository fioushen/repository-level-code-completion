{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\tfrom distutils.core import setup\n\tfrom setuptools import setup, find_packages\n\tsetup(\n\t    name=\"Log10\",\n\t    version=\"0.2.8\",\n\t    description=\"Log10 LLM data management\",\n\t    author=\"Log10 team\",\n\t    author_email=\"team@log10.io\",\n\t    url=\"\",\n", "    packages=find_packages(),\n\t    install_requires=[\n\t        \"openai\",\n\t        \"python-dotenv\",\n\t    ],\n\t    extras_require={\n\t        \"bigquery\": [\"google-cloud-bigquery\"],\n\t        \"dev\": [\"chromadb\"],\n\t    },\n\t)\n"]}
{"filename": "log10/evals.py", "chunked_list": ["import subprocess\n\timport tempfile\n\timport os\n\timport csv\n\timport json\n\timport logging\n\tfrom log10.llm import Messages\n\tfrom log10.utils import fuzzy_match, parse_field\n\tdef run_metric(metric, ideal, model_output):\n\t    ideal = parse_field(ideal)\n", "    for ideal_candidate in ideal:\n\t        # Ref: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/utils.py\n\t        if metric == \"match\":\n\t            if model_output.startswith(ideal_candidate):\n\t                return True\n\t        elif metric == \"includes\":  # case-insensitive\n\t            # Ref: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/basic/includes.py\n\t            if ideal_candidate.lower() in model_output.lower():\n\t                return True\n\t        elif metric == \"fuzzy_match\":\n", "            if fuzzy_match(ideal_candidate, model_output):\n\t                return True\n\t    return False\n\tdef write_to_csv(file_name, row_data):\n\t    with open(file_name, \"a\", newline=\"\") as file:\n\t        writer = csv.writer(file)\n\t        writer.writerow(row_data)\n\tdef eval(llm, dataset, metric, out_file_path):\n\t    csv_file_name, mapping = dataset\n\t    with open(csv_file_name, \"r\") as csv_file:\n", "        reader = csv.DictReader(csv_file)\n\t        examples = []\n\t        for example in reader:\n\t            mapped_example = {}\n\t            for key, value in mapping.items():\n\t                mapped_example[key] = example[value]\n\t            examples.append(mapped_example)\n\t        # todo: each example could be launched as separate threads or separate api calls to job runners\n\t        write_to_csv(out_file_path, [\"input\", \"ideal\", \"model_completion\", \"metric\"])\n\t        for example in examples:\n", "            messages = Messages.from_dict(json.loads(example[\"input\"]))\n\t            model_completion = llm.chat(messages)\n\t            example_metric = run_metric(metric, example[\"ideal\"], model_completion.content)\n\t            write_to_csv(\n\t                out_file_path,\n\t                [example[\"input\"], example[\"ideal\"], model_completion, example_metric],\n\t            )\n\t            print(f\"\\n\\nIdeal:{example['ideal']}\\nModel output:{model_completion.content}\")\n\t            print(f\"Correct:{example_metric}\")\n\t    return\n", "def compile(code):\n\t    with tempfile.NamedTemporaryFile(suffix=\".c\", delete=False) as temp:\n\t        temp.write(code.encode())\n\t        temp.close()\n\t        process = subprocess.run(\n\t            [\"gcc\", temp.name], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n\t        )\n\t        os.remove(temp.name)  # remove the temp file\n\t        if process.returncode == 0:\n\t            return True\n", "        else:\n\t            return False, process.stderr.decode()\n"]}
{"filename": "log10/langchain.py", "chunked_list": ["import time\n\timport uuid\n\tfrom langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseMessage\n\tfrom uuid import UUID\n\t\"\"\"Callback Handler that prints to std out.\"\"\"\n\tfrom typing import Any, Dict, List, Optional, Union\n\tfrom langchain.callbacks.base import BaseCallbackHandler\n\tfrom langchain.schema import AgentAction, AgentFinish, LLMResult\n\tfrom log10.llm import LLM, Kind, Message\n\timport logging\n", "logging.basicConfig()\n\tlogger = logging.getLogger(\"log10\")\n\tdef kwargs_to_hparams(kwargs: Dict[str, Any]) -> Dict[str, Any]:\n\t    \"\"\"Convert kwargs to hparams.\"\"\"\n\t    hparams = {}\n\t    if \"temperature\" in kwargs:\n\t        hparams[\"temperature\"] = kwargs[\"temperature\"]\n\t    if \"top_p\" in kwargs:\n\t        hparams[\"top_p\"] = kwargs[\"top_p\"]\n\t    if \"top_k\" in kwargs:\n", "        hparams[\"top_k\"] = kwargs[\"top_k\"]\n\t    if \"max_tokens\" in kwargs:\n\t        hparams[\"max_tokens\"] = kwargs[\"max_tokens\"]\n\t    if \"max_tokens_to_sample\" in kwargs:\n\t        hparams[\"max_tokens\"] = kwargs[\"max_tokens_to_sample\"]\n\t    if \"frequency_penalty\" in kwargs:\n\t        hparams[\"frequency_penalty\"] = kwargs[\"frequency_penalty\"]\n\t    if \"presence_penalty\" in kwargs:\n\t        hparams[\"presence_penalty\"] = kwargs[\"presence_penalty\"]\n\t    return hparams\n", "class Log10Callback(BaseCallbackHandler, LLM):\n\t    \"\"\"Callback Handler that prints to std out.\"\"\"\n\t    def __init__(self, log10_config: Optional[dict] = None) -> None:\n\t        \"\"\"Initialize callback handler.\"\"\"\n\t        super().__init__(log10_config=log10_config, hparams=None)\n\t        self.runs = {}\n\t        if log10_config.DEBUG:\n\t            logger.setLevel(logging.DEBUG)\n\t        else:\n\t            logger.setLevel(logging.INFO)\n", "    def on_llm_start(\n\t        self,\n\t        serialized: Dict[str, Any],\n\t        prompts: List[str],\n\t        *,\n\t        run_id: UUID,\n\t        parent_run_id: Optional[UUID] = None,\n\t        tags: Optional[List[str]] = None,\n\t        **kwargs: Any,\n\t    ) -> None:\n", "        \"\"\"Print out the prompts.\"\"\"\n\t        logger.debug(\n\t            f\"**\\n**on_llm_start**\\n**\\n: serialized:\\n {serialized} \\n\\n prompts:\\n {prompts} \\n\\n rest: {kwargs}\"\n\t        )\n\t        kwargs = serialized.get(\"kwargs\", {})\n\t        hparams = kwargs_to_hparams(kwargs)\n\t        model = kwargs.get(\"model_name\", None)\n\t        if model is None:\n\t            model = kwargs.get(\"model\", None)\n\t        if model is None:\n", "            raise BaseException(\"No model found in serialized or kwargs\")\n\t        if len(prompts) != 1:\n\t            raise BaseException(\"Only support one prompt at a time\")\n\t        request = {\"model\": model, \"prompt\": prompts[0], **hparams}\n\t        logger.debug(f\"request: {request}\")\n\t        completion_id = self.log_start(request, Kind.text, tags)\n\t        self.runs[run_id] = {\n\t            \"kind\": Kind.text,\n\t            \"completion_id\": completion_id,\n\t            \"start_time\": time.perf_counter(),\n", "            \"model\": model,\n\t        }\n\t    def on_chat_model_start(\n\t        self,\n\t        serialized: Dict[str, Any],\n\t        messages: List[List[BaseMessage]],\n\t        *,\n\t        run_id: UUID,\n\t        parent_run_id: Optional[UUID] = None,\n\t        tags: Optional[List[str]] = None,\n", "        **kwargs: Any,\n\t    ) -> None:\n\t        logger.debug(\n\t            f\"**\\n**on_chat_model_start**\\n**\\n: run_id:{run_id}\\nserialized:\\n{serialized}\\n\\nmessages:\\n{messages}\\n\\nkwargs: {kwargs}\"\n\t        )\n\t        #\n\t        # Find model string\n\t        #\n\t        kwargs = serialized.get(\"kwargs\", {})\n\t        model = kwargs.get(\"model_name\", None)\n", "        if model is None:\n\t            model = kwargs.get(\"model\", None)\n\t        if model is None:\n\t            raise BaseException(\"No model found in serialized or kwargs\")\n\t        hparams = kwargs_to_hparams(kwargs)\n\t        hparams[\"model\"] = model\n\t        logger.debug(f\"hparams: {hparams}\")\n\t        if len(messages) != 1:\n\t            raise BaseException(\"Only support one message at a time\")\n\t        # Convert messages to log10 format\n", "        log10_messages = []\n\t        for message in messages[0]:\n\t            logger.debug(f\"message: {message}\")\n\t            if isinstance(message, HumanMessage):\n\t                log10_messages.append(Message(role=\"user\", content=message.content))\n\t            elif isinstance(message, AIMessage):\n\t                log10_messages.append(\n\t                    Message(role=\"assistant\", content=message.content)\n\t                )\n\t            elif isinstance(message, SystemMessage):\n", "                log10_messages.append(Message(role=\"system\", content=message.content))\n\t            else:\n\t                raise BaseException(f\"Unknown message type {type(message)}\")\n\t        request = {\n\t            \"messages\": [message.to_dict() for message in log10_messages],\n\t            **hparams,\n\t        }\n\t        logger.debug(f\"request: {request}\")\n\t        completion_id = self.log_start(\n\t            request,\n", "            Kind.chat,\n\t            tags,\n\t        )\n\t        self.runs[run_id] = {\n\t            \"kind\": Kind.chat,\n\t            \"completion_id\": completion_id,\n\t            \"start_time\": time.perf_counter(),\n\t            \"model\": model,\n\t        }\n\t        logger.debug(f\"logged start with completion_id: {completion_id}\")\n", "    def on_llm_end(\n\t        self,\n\t        response: LLMResult,\n\t        *,\n\t        run_id: UUID,\n\t        parent_run_id: Optional[UUID] = None,\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        \"\"\"Do nothing.\"\"\"\n\t        # Find run in runs.\n", "        run = self.runs.get(run_id, None)\n\t        if run is None:\n\t            raise BaseException(\"Could not find run in runs\")\n\t        if run[\"kind\"] != Kind.chat and run[\"kind\"] != Kind.text:\n\t            raise BaseException(\"Only support chat kind\")\n\t        duration = time.perf_counter() - run[\"start_time\"]\n\t        # Log end\n\t        if len(response.generations) != 1:\n\t            raise BaseException(\"Only support one message at a time\")\n\t        if len(response.generations[0]) != 1:\n", "            raise BaseException(\"Only support one message at a time\")\n\t        content = response.generations[0][0].text\n\t        log10response = {}\n\t        if run[\"kind\"] == Kind.chat:\n\t            log10response = {\n\t                \"id\": str(uuid.uuid4()),\n\t                \"object\": \"chat.completion\",\n\t                \"model\": run[\"model\"],\n\t                \"choices\": [\n\t                    {\n", "                        \"index\": 0,\n\t                        \"message\": {\"role\": \"assistant\", \"content\": content},\n\t                        \"finish_reason\": \"stop\",\n\t                    }\n\t                ],\n\t            }\n\t        elif run[\"kind\"] == Kind.text:\n\t            log10response = {\n\t                \"id\": str(uuid.uuid4()),\n\t                \"object\": \"text_completion\",\n", "                \"model\": run[\"model\"],\n\t                \"choices\": [\n\t                    {\n\t                        \"index\": 0,\n\t                        \"text\": content,\n\t                        \"logprobs\": None,\n\t                        \"finish_reason\": \"stop\",\n\t                    }\n\t                ],\n\t            }\n", "        # Determine if we can provide usage metrics (token count).\n\t        logger.debug(f\"**** response: {response}\")\n\t        if response.llm_output is not None:\n\t            token_usage = response.llm_output.get(\"token_usage\")\n\t            if token_usage is not None:\n\t                log10response[\"usage\"] = token_usage\n\t                logger.debug(f\"usage: {log10response['usage']}\")\n\t        logger.debug(\n\t            f\"**\\n**on_llm_end**\\n**\\n: response:\\n {log10response} \\n\\n rest: {kwargs}\"\n\t        )\n", "        self.log_end(run[\"completion_id\"], log10response, duration)\n\t    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n\t        \"\"\"Do nothing.\"\"\"\n\t        logger.debug(f\"token:\\n {token} \\n\\n rest: {kwargs}\")\n\t    def on_llm_error(\n\t        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Do nothing.\"\"\"\n\t        logger.debug(f\"error:\\n {error} \\n\\n rest: {kwargs}\")\n\t    def on_chain_start(\n", "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Print out that we are entering a chain.\"\"\"\n\t        pass\n\t    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n\t        \"\"\"Print out that we finished a chain.\"\"\"\n\t        pass\n\t    def on_chain_error(\n\t        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n\t    ) -> None:\n", "        \"\"\"Do nothing.\"\"\"\n\t        pass\n\t    def on_tool_start(\n\t        self,\n\t        serialized: Dict[str, Any],\n\t        input_str: str,\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        \"\"\"Do nothing.\"\"\"\n\t        pass\n", "    def on_agent_action(\n\t        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n\t    ) -> Any:\n\t        \"\"\"Run on agent action.\"\"\"\n\t        pass\n\t    def on_tool_end(\n\t        self,\n\t        output: str,\n\t        color: Optional[str] = None,\n\t        observation_prefix: Optional[str] = None,\n", "        llm_prefix: Optional[str] = None,\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        \"\"\"If not the final action, print out observation.\"\"\"\n\t        pass\n\t    def on_tool_error(\n\t        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Do nothing.\"\"\"\n\t        pass\n", "    def on_text(\n\t        self,\n\t        text: str,\n\t        color: Optional[str] = None,\n\t        end: str = \"\",\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        \"\"\"Run when agent ends.\"\"\"\n\t        pass\n\t    def on_agent_finish(\n", "        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Run on agent end.\"\"\"\n\t        pass\n"]}
{"filename": "log10/openai.py", "chunked_list": ["from copy import deepcopy\n\timport time\n\tfrom typing import List\n\timport openai\n\tfrom log10.llm import LLM, ChatCompletion, Message, TextCompletion\n\timport logging\n\t# for exponential backoff\n\timport backoff\n\tfrom openai.error import RateLimitError, APIConnectionError\n\timport openai\n", "class OpenAI(LLM):\n\t    def __init__(self, hparams: dict = None, log10_config=None):\n\t        super().__init__(hparams, log10_config)\n\t    @backoff.on_exception(backoff.expo, (RateLimitError, APIConnectionError))\n\t    def chat(self, messages: List[Message], hparams: dict = None) -> ChatCompletion:\n\t        completion = openai.ChatCompletion.create(\n\t            **self.chat_request(messages, hparams)\n\t        )\n\t        return ChatCompletion(\n\t            role=completion.choices[0][\"message\"][\"role\"],\n", "            content=completion.choices[0][\"message\"][\"content\"],\n\t            response=completion,\n\t        )\n\t    def chat_request(self, messages: List[Message], hparams: dict = None) -> dict:\n\t        merged_hparams = deepcopy(self.hparams)\n\t        if hparams:\n\t            for hparam in hparams:\n\t                merged_hparams[hparam] = hparams[hparam]\n\t        return {\n\t            \"messages\": [message.to_dict() for message in messages],\n", "            **merged_hparams,\n\t        }\n\t    @backoff.on_exception(backoff.expo, (RateLimitError, APIConnectionError))\n\t    def text(self, prompt: str, hparams: dict = None) -> TextCompletion:\n\t        request = self.text_request(prompt, hparams)\n\t        completion = openai.Completion.create(**request)\n\t        return TextCompletion(text=completion.choices[0].text, response=completion)\n\t    def text_request(self, prompt: str, hparams: dict = None) -> dict:\n\t        merged_hparams = deepcopy(self.hparams)\n\t        if hparams:\n", "            for hparam in hparams:\n\t                merged_hparams[hparam] = hparams[hparam]\n\t        output = {\"prompt\": prompt, **merged_hparams}\n\t        return output\n"]}
{"filename": "log10/anthropic.py", "chunked_list": ["import os\n\tfrom copy import deepcopy\n\tfrom typing import List\n\tfrom log10.llm import LLM, ChatCompletion, Message, TextCompletion\n\tfrom anthropic import HUMAN_PROMPT, AI_PROMPT\n\timport anthropic\n\timport uuid\n\timport logging\n\tclass Anthropic(LLM):\n\t    def __init__(\n", "        self, hparams: dict = None, skip_initialization: bool = False, log10_config=None\n\t    ):\n\t        super().__init__(hparams, log10_config)\n\t        if not skip_initialization:\n\t            self.client = anthropic.Anthropic()\n\t        self.hparams = hparams\n\t        if \"max_tokens_to_sample\" not in self.hparams:\n\t            self.hparams[\"max_tokens_to_sample\"] = 1024\n\t    def chat(self, messages: List[Message], hparams: dict = None) -> ChatCompletion:\n\t        chat_request = self.chat_request(messages, hparams)\n", "        completion = self.client.completions.create(\n\t            **chat_request\n\t        )\n\t        content = completion.completion\n\t        reason = \"stop\"\n\t        if completion.stop_reason == \"stop_sequence\":\n\t            reason = \"stop\"\n\t        elif completion.stop_reason == \"max_tokens\":\n\t            reason = \"length\"\n\t        tokens_usage = self.create_tokens_usage(chat_request[\"prompt\"], content)\n", "        # Imitate OpenAI reponse format.\n\t        response = {\n\t            \"id\": str(uuid.uuid4()),\n\t            \"object\": \"chat.completion\",\n\t            \"model\": completion.model,\n\t            \"choices\": [\n\t                {\n\t                    \"index\": 0,\n\t                    \"message\": {\"role\": \"assistant\", \"content\": content},\n\t                    \"finish_reason\": reason,\n", "                }\n\t            ],\n\t            \"usage\": tokens_usage\n\t        }\n\t        return ChatCompletion(role=\"assistant\", content=content, response=response)\n\t    def chat_request(self, messages: List[Message], hparams: dict = None) -> dict:\n\t        merged_hparams = deepcopy(self.hparams)\n\t        if hparams:\n\t            for hparam in hparams:\n\t                merged_hparams[hparam] = hparams[hparam]\n", "        # NOTE: That we may have to convert this to openai messages, if we want\n\t        #       to use the same log viewer for all chat based models.\n\t        prompt = Anthropic.convert_history_to_claude(messages)\n\t        return {\"prompt\": prompt, \"stop_sequences\": [HUMAN_PROMPT], **merged_hparams}\n\t    def text(self, prompt: str, hparams: dict = None) -> TextCompletion:\n\t        text_request = self.text_request(prompt, hparams)\n\t        completion = self.client.completions.create(\n\t            **text_request\n\t        )\n\t        text = completion.completion\n", "        # Imitate OpenAI reponse format.\n\t        reason = \"stop\"\n\t        if completion.stop_reason == \"stop_sequence\":\n\t            reason = \"stop\"\n\t        elif completion.stop_reason == \"max_tokens\":\n\t            reason = \"length\"\n\t        tokens_usage = self.create_tokens_usage(text_request[\"prompt\"], text)\n\t        # Imitate OpenAI reponse format.\n\t        response = {\n\t            \"id\": str(uuid.uuid4()),\n", "            \"object\": \"text_completion\",\n\t            \"model\": completion.model,\n\t            \"choices\": [\n\t                {\n\t                    \"index\": 0,\n\t                    \"text\": text,\n\t                    \"logprobs\": None,\n\t                    \"finish_reason\": reason,\n\t                }\n\t            ],\n", "            \"usage\": tokens_usage\n\t        }\n\t        logging.info(\"Returning text completion\")\n\t        return TextCompletion(text=text, response=response)\n\t    def text_request(self, prompt: str, hparams: dict = None) -> TextCompletion:\n\t        merged_hparams = deepcopy(self.hparams)\n\t        if hparams:\n\t            for hparam in hparams:\n\t                merged_hparams[hparam] = hparams[hparam]\n\t        return {\n", "            \"prompt\": HUMAN_PROMPT + prompt + \"\\n\" + AI_PROMPT,\n\t            \"stop_sequences\": [HUMAN_PROMPT],\n\t            **merged_hparams,\n\t        }\n\t    def convert_history_to_claude(messages: List[Message]):\n\t        text = \"\"\n\t        for message in messages:\n\t            # Anthropic doesn't support a system prompt OOB\n\t            if message.role == \"user\" or message.role == \"system\":\n\t                text += HUMAN_PROMPT\n", "            elif message.role == \"assistant\":\n\t                text += AI_PROMPT\n\t            text += f\"{message.content}\"\n\t        text += AI_PROMPT\n\t        return text\n\t    def convert_claude_to_messages(prompt: str):\n\t        pass\n\t    def create_tokens_usage(self, prompt: str, completion: str):\n\t        prompt_tokens = self.client.count_tokens(prompt)\n\t        completion_tokens = self.client.count_tokens(completion)\n", "        total_tokens = prompt_tokens + completion_tokens\n\t        # Imitate OpenAI usage format.\n\t        return {\n\t            \"prompt_tokens\": prompt_tokens,\n\t            \"completion_tokens\": completion_tokens,\n\t            \"total_tokens\": total_tokens\n\t        }\n"]}
{"filename": "log10/llm.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom copy import deepcopy\n\tfrom enum import Enum\n\timport os\n\timport sys\n\timport traceback\n\tRole = Enum(\"Role\", [\"system\", \"assistant\", \"user\"])\n\tKind = Enum(\"Kind\", [\"chat\", \"text\"])\n\tfrom typing import List, Optional\n\timport json\n", "import logging\n\timport requests\n\tclass Log10Config:\n\t    def __init__(\n\t        self,\n\t        url: str = None,\n\t        token: str = None,\n\t        org_id: str = None,\n\t        tags: List[str] = None,\n\t        DEBUG: bool = False,\n", "    ):\n\t        self.url = url if url else os.getenv(\"LOG10_URL\")\n\t        self.token = token if token else os.getenv(\"LOG10_TOKEN\")\n\t        self.org_id = org_id if org_id else os.getenv(\"LOG10_ORG_ID\")\n\t        self.DEBUG = DEBUG\n\t        # Get tags from env, if not set, use empty list\n\t        if tags:\n\t            self.tags = tags\n\t        elif os.getenv(\"LOG10_TAGS\") is not None:\n\t            self.tags = os.getenv(\"LOG10_TAGS\").split(\",\")\n", "        else:\n\t            self.tags = []\n\tclass Message(ABC):\n\t    def __init__(\n\t        self, role: Role, content: str, id: str = None, completion: str = None\n\t    ):\n\t        self.id = id\n\t        self.role = role\n\t        self.content = content\n\t        self.completion = completion\n", "    def to_dict(self):\n\t        return {\n\t            \"role\": self.role,\n\t            \"content\": self.content,\n\t        }\n\t    def from_dict(message: dict):\n\t        return Message(\n\t            role=message[\"role\"],\n\t            content=message[\"content\"],\n\t            id=message.get(\"id\"),\n", "            completion=message.get(\"completion\"),\n\t        )\n\tclass Messages(ABC):\n\t    def from_dict(messages: dict):\n\t        return [Message.from_dict(message) for message in messages]\n\tclass Completion(ABC):\n\t    pass\n\tclass ChatCompletion(Completion):\n\t    def __init__(\n\t        self, role: str, content: str, response: dict = None, completion_id: str = None\n", "    ):\n\t        self.role = role\n\t        self.content = content\n\t        self.response = response\n\t        self.completion_id = completion_id\n\t    def to_dict(self) -> dict:\n\t        return {\n\t            \"role\": self.role,\n\t            \"content\": self.content,\n\t        }\n", "    def __str__(self) -> str:\n\t        return json.dumps(self.to_dict())\n\tclass TextCompletion(Completion):\n\t    def __init__(self, text: str, response: dict = None, completion_id=None):\n\t        self._text = text\n\t        self.response = response\n\t        self.completion_id = completion_id\n\t    def text(self) -> str:\n\t        return self._text\n\tclass LLM(ABC):\n", "    last_completion_response = None\n\t    def __init__(self, hparams: dict = None, log10_config: Log10Config = None):\n\t        self.log10_config = log10_config\n\t        self.hparams = hparams\n\t        # Start session\n\t        if self.log10_config:\n\t            session_url = self.log10_config.url + \"/api/sessions\"\n\t            try:\n\t                res = requests.request(\n\t                    \"POST\",\n", "                    session_url,\n\t                    headers={\n\t                        \"x-log10-token\": self.log10_config.token,\n\t                        \"Content-Type\": \"application/json\",\n\t                    },\n\t                    json={\"organization_id\": self.log10_config.org_id},\n\t                )\n\t                response = res.json()\n\t                self.session_id = response[\"sessionID\"]\n\t            except Exception as e:\n", "                logging.warning(\n\t                    f\"Failed to start session with {session_url} using token {self.log10_config.token}. Won't be able to log. {e}\"\n\t                )\n\t                self.log10_config = None\n\t    def last_completion_url(self):\n\t        if self.last_completion_response is None:\n\t            return None\n\t        return self.log10_config.url + '/app/' + self.last_completion_response['organizationSlug'] + '/completions/' + self.last_completion_response['completionID']\n\t    def text(self, prompt: str, hparams: dict = None) -> TextCompletion:\n\t        raise Exception(\"Not implemented\")\n", "    def text_request(self, prompt: str, hparams: dict = None) -> dict:\n\t        raise Exception(\"Not implemented\")\n\t    def chat(self, messages: List[Message], hparams: dict = None) -> ChatCompletion:\n\t        raise Exception(\"Not implemented\")\n\t    def chat_request(self, messages: List[Message], hparams: dict = None) -> dict:\n\t        raise Exception(\"Not implemented\")\n\t    def api_request(self, rel_url: str, method: str, request: dict):\n\t        return requests.request(\n\t            method,\n\t            f\"{self.log10_config.url}{rel_url}\",\n", "            headers={\n\t                \"x-log10-token\": self.log10_config.token,\n\t                \"Content-Type\": \"application/json\",\n\t            },\n\t            json=request,\n\t        )\n\t    # Save the start of a completion in **openai request format**.\n\t    def log_start(self, request, kind: Kind, tags: Optional[List[str]] = None):\n\t        if not self.log10_config:\n\t            return None\n", "        res = self.api_request(\n\t            \"/api/completions\", \"POST\", {\"organization_id\": self.log10_config.org_id}\n\t        )\n\t        self.last_completion_response = res.json()\n\t        completion_id = res.json()[\"completionID\"]\n\t        # merge tags\n\t        if tags:\n\t            tags = list(set(tags + self.log10_config.tags))\n\t        else:\n\t            tags = self.log10_config.tags\n", "        res = self.api_request(\n\t            f\"/api/completions/{completion_id}\",\n\t            \"POST\",\n\t            {\n\t                \"kind\": kind == Kind.text and \"completion\" or \"chat\",\n\t                \"organization_id\": self.log10_config.org_id,\n\t                \"session_id\": self.session_id,\n\t                \"orig_module\": \"openai.api_resources.completion\"\n\t                if kind == Kind.text\n\t                else \"openai.api_resources.chat_completion\",\n", "                \"orig_qualname\": \"Completion.create\"\n\t                if kind == Kind.text\n\t                else \"ChatCompletion.create\",\n\t                \"status\": \"started\",\n\t                \"tags\": tags,\n\t                \"request\": json.dumps(request),\n\t            },\n\t        )\n\t        return completion_id\n\t    # Save the end of a completion in **openai request format**.\n", "    def log_end(self, completion_id: str, response: dict, duration: int):\n\t        if not self.log10_config:\n\t            return None\n\t        current_stack_frame = traceback.extract_stack()\n\t        stacktrace = [\n\t            {\n\t                \"file\": frame.filename,\n\t                \"line\": frame.line,\n\t                \"lineno\": frame.lineno,\n\t                \"name\": frame.name,\n", "            }\n\t            for frame in current_stack_frame\n\t        ]\n\t        self.api_request(\n\t            f\"/api/completions/{completion_id}\",\n\t            \"POST\",\n\t            {\n\t                \"response\": json.dumps(response),\n\t                \"status\": \"finished\",\n\t                \"duration\": int(duration * 1000),\n", "                \"stacktrace\": json.dumps(stacktrace),\n\t            },\n\t        )\n\tclass NoopLLM(LLM):\n\t    def __init__(self, hparams: dict = None, log10_config=None):\n\t        pass\n\t    def chat(self, messages: List[Message], hparams: dict = None) -> ChatCompletion:\n\t        logging.info(\"Received chat completion requst: \" + str(messages))\n\t        return ChatCompletion(role=\"assistant\", content=\"I'm not a real LLM\")\n\t    def text(self, prompt: str, hparams: dict = None) -> TextCompletion:\n", "        logging.info(\"Received text completion requst: \" + prompt)\n\t        return TextCompletion(text=\"I'm not a real LLM\")\n\tdef merge_hparams(override, base):\n\t    merged = deepcopy(base)\n\t    if override:\n\t        for hparam in override:\n\t            merged[hparam] = override[hparam]\n\t    return merged\n"]}
{"filename": "log10/__init__.py", "chunked_list": []}
{"filename": "log10/utils.py", "chunked_list": ["import re\n\timport string\n\tfrom anthropic import HUMAN_PROMPT, AI_PROMPT\n\timport json\n\t# Ref: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/utils.py\n\tdef normalize(s: str) -> str:\n\t    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\t    s = s.lower()\n\t    exclude = set(string.punctuation)\n\t    s = \"\".join(char for char in s if char not in exclude)\n", "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n\t    s = \" \".join(s.split())\n\t    return s\n\tdef fuzzy_match(s1: str, s2: str) -> bool:\n\t    s1 = normalize(s1)\n\t    s2 = normalize(s2)\n\t    if s1 == \"\" or s2 == \"\":\n\t        return s1 == s2\n\t    return s1 in s2 or s2 in s1\n\tdef parse_field(value):\n", "    try:\n\t        # Try to parse the value as JSON (list)\n\t        parsed = json.loads(value)\n\t        if isinstance(parsed, list):\n\t            return parsed\n\t        else:\n\t            return [value]\n\t    except json.JSONDecodeError:\n\t        # If it's not valid JSON, return the original string value as a list with singleton element\n\t        return [value]\n"]}
{"filename": "log10/tools.py", "chunked_list": ["from langchain.document_loaders import WebBaseLoader\n\tfrom log10.llm import Message\n\t# Browser agent / tool\n\tdef browser(URL: str) -> str:\n\t    \"\"\"useful when you need to scrape a website.\n\t    Input is the URL of the website to be scraped.\n\t    Output is the text of the website\n\t    \"\"\"\n\t    # Example code if we want to follow links\n\t    # explored_sites = set()\n", "    # queue = [URL]\n\t    # def parse_site(base_url):\n\t    #   if base_url in explored_sites:\n\t    #     return\n\t    #   explored_sites.add(base_url)\n\t    #   if debug:\n\t    #     print(\"Scraping: \", base_url)\n\t    #   page = requests.get(URL)\n\t    #   soup = BeautifulSoup(page.content, \"html.parser\")\n\t    #   for a_href in soup.find_all(\"a\", href=True):\n", "    #       link = a_href[\"href\"]\n\t    #       if link[0] == \"/\":\n\t    #         link = URL + link\n\t    #       if link.startswith(URL):\n\t    #         to_explore = link.split(\"#\")[0]\n\t    #         queue.append(to_explore)\n\t    # while len(queue) != 0:\n\t    #   url = queue.pop()\n\t    #   parse_site(url)\n\t    # return list(explored_sites)\n", "    loader = WebBaseLoader(URL)\n\t    data = loader.load()\n\t    data[0].page_content = \" \".join(data[0].page_content.split())\n\t    # hack: crop to 5000 char to fit within context length\n\t    # Will need better strategy eventually\n\t    #   maxlen = 5000 if len(data[0].page_content) > 5000 else len(data[0].page_content)\n\t    #   return data[0].page_content[:maxlen]\n\t    return data[0].page_content\n\tdef code_extractor(full_response, language, extraction_model, llm):\n\t    \"\"\"useful when you need to extract just the code from a detailed LLM response\"\"\"\n", "    messages = [\n\t        Message(\n\t            role=\"system\",\n\t            content=f\"Extract just the {language} code from the following snippet. Do not include ``` or markdown syntax. Format your reply so I can directly copy your entire response, save it as a file and compile and run the code.\",\n\t        ),\n\t        Message(role=\"user\", content=\"Here's the snippet:\\n\" + full_response),\n\t    ]\n\t    completion = llm.chat(messages, {\"model\": extraction_model, \"temperature\": 0.2})\n\t    return completion.content\n"]}
{"filename": "log10/load.py", "chunked_list": ["import types\n\timport functools\n\timport inspect\n\timport requests\n\timport os\n\timport json\n\timport time\n\timport traceback\n\tfrom aiohttp import ClientSession\n\timport asyncio\n", "import threading\n\timport queue\n\tfrom contextlib import contextmanager\n\timport logging\n\tfrom dotenv import load_dotenv\n\timport backoff  # for exponential backoff\n\tfrom openai.error import RateLimitError, APIConnectionError\n\tload_dotenv()\n\turl = os.environ.get(\"LOG10_URL\")\n\ttoken = os.environ.get(\"LOG10_TOKEN\")\n", "org_id = os.environ.get(\"LOG10_ORG_ID\")\n\t# log10, bigquery\n\ttarget_service = os.environ.get(\"LOG10_DATA_STORE\", \"log10\")\n\tif target_service == \"bigquery\":\n\t    from log10.bigquery import initialize_bigquery\n\t    bigquery_client, bigquery_table = initialize_bigquery()\n\t    import uuid\n\t    from datetime import datetime, timezone\n\telif target_service is None:\n\t    target_service = \"log10\"  # default to log10\n", "@backoff.on_exception(backoff.expo, (RateLimitError, APIConnectionError))\n\tdef func_with_backoff(func, *args, **kwargs):\n\t    return func(*args, **kwargs)\n\tdef get_session_id():\n\t    if target_service == \"bigquery\":\n\t        return str(uuid.uuid4())\n\t    try:\n\t        session_url = url + \"/api/sessions\"\n\t        res = requests.request(\"POST\",\n\t                               session_url, headers={\"x-log10-token\": token, \"Content-Type\": \"application/json\"}, json={\n", "                                   \"organization_id\": org_id\n\t                               })\n\t        return res.json()['sessionID']\n\t    except Exception as e:\n\t        raise Exception(\"Failed to create LOG10 session: \" + str(e) + \"\\nLikely cause: LOG10 env vars missing or not picked up correctly!\" +\n\t                        \"\\nSee https://github.com/log10-io/log10#%EF%B8%8F-setup for details\")\n\t# Global variable to store the current sessionID.\n\tsessionID = get_session_id()\n\tlast_completion_response = None\n\tglobal_tags = []\n", "class log10_session:\n\t    def __init__(self, tags=None):\n\t         self.tags = tags\n\t         if tags is not None:\n\t            global global_tags\n\t            global_tags = tags\n\t    def __enter__(self):\n\t        global sessionID\n\t        global last_completion_response\n\t        sessionID = get_session_id()\n", "        last_completion_response = None\n\t        return self\n\t    def last_completion_url(self):\n\t        if last_completion_response is None:\n\t            return None\n\t        return url + '/app/' + last_completion_response['organizationSlug'] + '/completions/' + last_completion_response['completionID']\n\t    def __exit__(self, exc_type, exc_value, traceback):\n\t        if self.tags is not None:\n\t            global global_tags\n\t            global_tags = None\n", "        return\n\t@contextmanager\n\tdef timed_block(block_name):\n\t    if DEBUG:\n\t        start_time = time.perf_counter()\n\t        try:\n\t            yield\n\t        finally:\n\t            elapsed_time = time.perf_counter() - start_time\n\t            logging.debug(\n", "                f\"TIMED BLOCK - {block_name} took {elapsed_time:.6f} seconds to execute.\")\n\t    else:\n\t        yield\n\tdef log_url(res, completionID):\n\t    output = res.json()\n\t    organizationSlug = output['organizationSlug']\n\t    full_url = url + '/app/' + organizationSlug + '/completions/' + completionID\n\t    logging.debug(f\"LOG10: Completion URL: {full_url}\")\n\tasync def log_async(completion_url, func, **kwargs):\n\t    async with ClientSession() as session:\n", "        global last_completion_response\n\t        res = requests.request(\"POST\",\n\t                               completion_url, headers={\"x-log10-token\": token, \"Content-Type\": \"application/json\"}, json={\n\t                                   \"organization_id\": org_id\n\t                               })\n\t        # todo: handle session id for bigquery scenario\n\t        last_completion_response = res.json()\n\t        completionID = res.json()['completionID']\n\t        if DEBUG:\n\t            log_url(res, completionID)\n", "        log_row = {\n\t            # do we want to also store args?\n\t            \"status\": \"started\",\n\t            \"orig_module\": func.__module__,\n\t            \"orig_qualname\": func.__qualname__,\n\t            \"request\": json.dumps(kwargs),\n\t            \"session_id\": sessionID,\n\t            \"organization_id\": org_id,\n\t            \"tags\": global_tags\n\t        }\n", "        if target_service == \"log10\":\n\t            res = requests.request(\"POST\",\n\t                                   completion_url + \"/\" + completionID,\n\t                                   headers={\"x-log10-token\": token,\n\t                                            \"Content-Type\": \"application/json\"},\n\t                                   json=log_row)\n\t        elif target_service == \"bigquery\":\n\t            pass\n\t            # NOTE: We only save on request finalization.\n\t        return completionID\n", "def run_async_in_thread(completion_url, func, result_queue, **kwargs):\n\t    result = asyncio.run(\n\t        log_async(completion_url=completion_url, func=func, **kwargs))\n\t    result_queue.put(result)\n\tdef log_sync(completion_url, func, **kwargs):\n\t    global last_completion_response\n\t    res = requests.request(\"POST\",\n\t                           completion_url, headers={\"x-log10-token\": token, \"Content-Type\": \"application/json\"}, json={\n\t                               \"organization_id\": org_id\n\t                           })\n", "    last_completion_response = res.json()\n\t    completionID = res.json()['completionID']\n\t    if DEBUG:\n\t        log_url(res, completionID)\n\t    res = requests.request(\"POST\",\n\t                           completion_url + \"/\" + completionID,\n\t                           headers={\"x-log10-token\": token,\n\t                                    \"Content-Type\": \"application/json\"},\n\t                           json={\n\t                               # do we want to also store args?\n", "                               \"status\": \"started\",\n\t                               \"orig_module\": func.__module__,\n\t                               \"orig_qualname\": func.__qualname__,\n\t                               \"request\": json.dumps(kwargs),\n\t                               \"session_id\": sessionID,\n\t                               \"organization_id\": org_id,\n\t                               \"tags\": global_tags\n\t                           })\n\t    return completionID\n\tdef intercepting_decorator(func):\n", "    @functools.wraps(func)\n\t    def wrapper(*args, **kwargs):\n\t        completion_url = url + \"/api/completions\"\n\t        output = None\n\t        result_queue = queue.Queue()\n\t        try:\n\t            with timed_block(sync_log_text + \" call duration\"):\n\t                if USE_ASYNC:\n\t                    threading.Thread(target=run_async_in_thread, kwargs={\n\t                        \"completion_url\": completion_url, \"func\": func, \"result_queue\": result_queue, **kwargs}).start()\n", "                else:\n\t                    completionID = log_sync(\n\t                        completion_url=completion_url, func=func, **kwargs)\n\t            current_stack_frame = traceback.extract_stack()\n\t            stacktrace = ([{\"file\": frame.filename,\n\t                          \"line\": frame.line,\n\t                           \"lineno\": frame.lineno,\n\t                            \"name\": frame.name} for frame in current_stack_frame])\n\t            start_time = time.perf_counter()\n\t            output = func_with_backoff(func, *args, **kwargs)\n", "            duration = time.perf_counter() - start_time\n\t            logging.debug(\n\t                f\"LOG10: TIMED BLOCK - LLM call duration: {duration}\")\n\t            if USE_ASYNC:\n\t                with timed_block(\"extra time spent waiting for log10 call\"):\n\t                    while result_queue.empty():\n\t                        pass\n\t                    completionID = result_queue.get()\n\t            with timed_block(\"result call duration (sync)\"):\n\t                # Adjust the Anthropic output to match OAI completion output\n", "                if func.__qualname__ == \"Client.completion\":\n\t                    output['choices'] = [{\n\t                        'text': output['completion'],\n\t                        'index': 0,\n\t                    }]\n\t                    log_row = {\n\t                        \"response\": json.dumps(output),\n\t                        \"status\": \"finished\",\n\t                        \"duration\": int(duration*1000),\n\t                        \"stacktrace\": json.dumps(stacktrace),\n", "                        \"kind\": \"completion\"\n\t                    }\n\t                else:\n\t                    log_row = {\n\t                        \"response\": json.dumps(output),\n\t                        \"status\": \"finished\",\n\t                        \"duration\": int(duration*1000),\n\t                        \"stacktrace\": json.dumps(stacktrace)\n\t                    }\n\t                if target_service == \"log10\":\n", "                    res = requests.request(\"POST\",\n\t                                           completion_url + \"/\" + completionID,\n\t                                           headers={\n\t                                               \"x-log10-token\": token, \"Content-Type\": \"application/json\"},\n\t                                           json=log_row)\n\t                elif target_service == \"bigquery\":\n\t                    try:\n\t                        log_row[\"id\"] = str(uuid.uuid4())\n\t                        log_row[\"created_at\"] = datetime.now(\n\t                            timezone.utc).isoformat()\n", "                        log_row[\"request\"] = json.dumps(kwargs)\n\t                        if func.__qualname__ == \"Completion.create\":\n\t                            log_row[\"kind\"] = \"completion\"\n\t                        elif func.__qualname__ == \"ChatCompletion.create\":\n\t                            log_row[\"kind\"] = \"chat\"\n\t                        log_row[\"orig_module\"] = func.__module__\n\t                        log_row[\"orig_qualname\"] = func.__qualname__\n\t                        log_row[\"session_id\"] = sessionID\n\t                        bigquery_client.insert_rows_json(\n\t                            bigquery_table, [log_row])\n", "                    except Exception as e:\n\t                        logging.error(\n\t                            f\"LOG10: failed to insert in Bigquery: {log_row} with error {e}\")\n\t        except Exception as e:\n\t            logging.error(\"LOG10: failed\", e)\n\t        return output\n\t    return wrapper\n\tdef set_sync_log_text(USE_ASYNC=True):\n\t    return \"async\" if USE_ASYNC else \"sync\"\n\tdef log10(module, DEBUG_=False, USE_ASYNC_=True):\n", "    \"\"\"Intercept and overload module for logging purposes\n\t    Keyword arguments:\n\t    module -- the module to be intercepted (e.g. openai)\n\t    DEBUG_ -- whether to show log10 related debug statements via python logging (default False)\n\t    USE_ASYNC_ -- whether to run in async mode (default True)\n\t    \"\"\"\n\t    global DEBUG, USE_ASYNC, sync_log_text\n\t    DEBUG = DEBUG_\n\t    USE_ASYNC = USE_ASYNC_\n\t    sync_log_text = set_sync_log_text(USE_ASYNC=USE_ASYNC)\n", "    logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO,\n\t                        format='%(asctime)s - %(levelname)s - LOG10 - %(message)s')\n\t    # def intercept_nested_functions(obj):\n\t    #     for name, attr in vars(obj).items():\n\t    #         if callable(attr) and isinstance(attr, types.FunctionType):\n\t    #             setattr(obj, name, intercepting_decorator(attr))\n\t    #         elif inspect.isclass(attr):\n\t    #             intercept_class_methods(attr)\n\t    # def intercept_class_methods(cls):\n\t    #     for method_name, method in vars(cls).items():\n", "    #         if isinstance(method, classmethod):\n\t    #             original_method = method.__func__\n\t    #             decorated_method = intercepting_decorator(original_method)\n\t    #             setattr(cls, method_name, classmethod(decorated_method))\n\t    #         elif isinstance(method, (types.FunctionType, types.MethodType)):\n\t    #             print(f\"method:{method}\")\n\t    #             setattr(cls, method_name, intercepting_decorator(method))\n\t    #         elif inspect.isclass(method):  # Handle nested classes\n\t    #             intercept_class_methods(method)\n\t    for name, attr in vars(module).items():\n", "        if inspect.isclass(attr):\n\t            # OpenAI\n\t            if module.__name__ == \"openai\" and name in [\"ChatCompletion\", \"Completion\"]:\n\t                for method_name, method in vars(attr).items():\n\t                    if isinstance(method, classmethod):\n\t                        original_method = method.__func__\n\t                        if original_method.__qualname__ in [\"ChatCompletion.create\", \"Completion.create\"]:\n\t                            decorated_method = intercepting_decorator(\n\t                                original_method)\n\t                            setattr(attr, method_name,\n", "                                    classmethod(decorated_method))\n\t            # Anthropic\n\t            elif module.__name__ == \"anthropic\" and name == \"Client\":\n\t                for method_name, method in vars(attr).items():\n\t                    if isinstance(method, (types.FunctionType, types.MethodType)) and method_name == \"completion\":\n\t                        setattr(attr, method_name,\n\t                                intercepting_decorator(method))\n\t            # For future reference:\n\t            # if callable(attr) and isinstance(attr, types.FunctionType):\n\t            #     print(f\"attr:{attr}\")\n", "            #     setattr(module, name, intercepting_decorator(attr))\n\t            # elif inspect.isclass(attr):  # Check if attribute is a class\n\t            #     intercept_class_methods(attr)\n\t            # # else: # uncomment if we want to include nested function support\n\t            # #     intercept_nested_functions(attr)\n"]}
{"filename": "log10/bigquery.py", "chunked_list": ["from google.cloud import bigquery\n\tfrom google.api_core.exceptions import NotFound\n\timport os\n\t# todo: add requirements.txt file\n\t# todo: add instructions for bigquery integration\n\tdef initialize_bigquery(debug=False):\n\t    # Configure the BigQuery client\n\t    project_id = os.environ.get(\"LOG10_BQ_PROJECT_ID\")\n\t    dataset_id = os.environ.get(\"LOG10_BQ_DATASET_ID\")\n\t    completions_table_id = os.environ.get(\"LOG10_BQ_COMPLETIONS_TABLE_ID\")\n", "    client = bigquery.Client(project=project_id)\n\t    def dataset_exists(dataset_id):\n\t        try:\n\t            client.get_dataset(dataset_id)  # API request\n\t            return True\n\t        except NotFound:\n\t            return False\n\t    def table_exists(dataset_id, completions_table_id):\n\t        try:\n\t            table_ref = client.dataset(dataset_id).table(completions_table_id)\n", "            client.get_table(table_ref)  # API request\n\t            return True\n\t        except NotFound:\n\t            return False\n\t    # Check if dataset exists\n\t    if dataset_exists(dataset_id):\n\t        if debug:\n\t            print(f\"Dataset {dataset_id} exists.\")\n\t    else:\n\t        if debug:\n", "            print(f\"Dataset {dataset_id} does not exist. Creating...\")\n\t        # Create the dataset\n\t        dataset_ref = client.dataset(dataset_id)\n\t        dataset = bigquery.Dataset(dataset_ref)\n\t        # Set the location, e.g., \"US\", \"EU\", \"asia-northeast1\", etc.\n\t        dataset.location = \"US\"\n\t        created_dataset = client.create_dataset(dataset)  # API request\n\t        if debug:\n\t            print(f\"Dataset {created_dataset.dataset_id} created.\")\n\t    client_dataset = client.dataset(dataset_id)\n", "    # Check if table exists\n\t    if table_exists(dataset_id, completions_table_id):\n\t        if debug:\n\t            print(f\"Table {completions_table_id} exists in dataset {dataset_id}.\")\n\t    else:\n\t        if debug:\n\t            print(\n\t                f\"Table {completions_table_id} does not exist in dataset {dataset_id}. Creating...\")\n\t        # Create the table\n\t        table_ref = client_dataset.table(completions_table_id)\n", "        script_dir = os.path.dirname(os.path.abspath(__file__))\n\t        schema_path = os.path.join(script_dir, 'schemas', 'bigquery.json')\n\t        schema = client.schema_from_json(schema_path)\n\t        table = bigquery.Table(table_ref, schema=schema)\n\t        created_table = client.create_table(table)  # API request\n\t        if debug:\n\t            print(f\"Table {created_table.table_id} created.\")\n\t    # load dataset and table\n\t    table_ref = client_dataset.table(completions_table_id)\n\t    table = client.get_table(table_ref)\n", "    return client, table\n"]}
{"filename": "log10/agents/camel.py", "chunked_list": ["import logging\n\tfrom dotenv import load_dotenv\n\tfrom log10.llm import LLM, Message\n\tfrom typing import List, Tuple\n\tload_dotenv()\n\tlogging.basicConfig(\n\t    level=logging.DEBUG,\n\t    format=\"%(asctime)s - %(levelname)s - LOG10 - CAMEL - %(message)s\",\n\t)\n\t# Repeat word termination conditions\n", "# https://github.com/lightaime/camel/blob/master/examples/ai_society/role_playing_multiprocess.py#L63\n\trepeat_word_threshold = 4\n\trepeat_word_list = [\"goodbye\", \"good bye\", \"thank\", \"bye\", \"welcome\", \"language model\"]\n\tdef camel_agent(\n\t    user_role: str,\n\t    assistant_role: str,\n\t    task_prompt: str,\n\t    max_turns: int,\n\t    user_prompt: str = None,\n\t    assistant_prompt: str = None,\n", "    summary_model: str = None,\n\t    llm: LLM = None,\n\t) -> Tuple[List[Message], List[Message]]:\n\t    generator = camel_agent_generator(\n\t        user_role,\n\t        assistant_role,\n\t        task_prompt,\n\t        max_turns,\n\t        user_prompt,\n\t        assistant_prompt,\n", "        summary_model,\n\t        llm,\n\t    )\n\t    *_, last = generator\n\t    return last\n\tdef camel_agent_generator(\n\t    user_role: str,\n\t    assistant_role: str,\n\t    task_prompt: str,\n\t    max_turns: int,\n", "    user_prompt: str,\n\t    assistant_prompt: str,\n\t    summary_model: str,\n\t    llm: LLM,\n\t):\n\t    try:\n\t        assistant_inception_prompt = f\"\"\"Never forget you are a {assistant_role} and I am a {user_role}. Never flip roles! Never instruct me!\n\tWe share a common interest in collaborating to successfully complete a task.\n\tYou must help me to complete the task.\n\tHere is the task: {task_prompt}. Never forget our task!\n", "I must instruct you based on your expertise and my needs to complete the task.\n\tI must give you one instruction at a time.\n\tYou must write a specific solution that appropriately completes the requested instruction.\n\tYou must decline my instruction honestly if you cannot perform the instruction due to physical, moral, legal reasons or your capability and explain the reasons.\n\tDo not add anything else other than your solution to my instruction.\n\tYou are never supposed to ask me any questions you only answer questions.\n\tYou are never supposed to reply with a flake solution. Explain your solutions.\n\tYour solution must be declarative sentences and simple present tense.\n\tUnless I say the task is completed, you should always start with:\n\tSolution: <YOUR_SOLUTION>\n", "<YOUR_SOLUTION> should be specific and provide preferable implementations and examples for task-solving.\n\tAlways end <YOUR_SOLUTION> with: Next request.\"\"\"\n\t        if assistant_prompt is not None:\n\t            assistant_inception_prompt = assistant_prompt\n\t        user_inception_prompt = f\"\"\"Never forget you are a {user_role} and I am a {assistant_role}. Never flip roles! You will always instruct me.\n\tWe share a common interest in collaborating to successfully complete a task.\n\tI must help you to complete the task.\n\tHere is the task: {task_prompt}. Never forget our task!\n\tYou must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n\t1. Instruct with a necessary input:\n", "Instruction: <YOUR_INSTRUCTION>\n\tInput: <YOUR_INPUT>\n\t2. Instruct without any input:\n\tInstruction: <YOUR_INSTRUCTION>\n\tInput: None\n\tThe \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n\tYou must give me one instruction at a time.\n\tI must write a response that appropriately completes the requested instruction.\n\tI must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\n\tYou should instruct me not ask me questions.\n", "Now you must start to instruct me using the two ways described above.\n\tDo not add anything else other than your instruction and the optional corresponding input!\n\tKeep giving me instructions and necessary inputs until you think the task is completed.\n\tWhen the task is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\n\tNever say <CAMEL_TASK_DONE> unless my responses have solved your task.\"\"\"\n\t        if user_prompt is not None:\n\t            user_inception_prompt = user_prompt\n\t        assistant_prompt = f\"ASSISTANT PROMPT: \\n {assistant_inception_prompt}\"\n\t        user_prompt = f\"USER PROMPT: \\n {user_inception_prompt}\"\n\t        assistant_messages = [\n", "            Message(role=\"system\", content=assistant_prompt),\n\t            Message(role=\"user\", content=assistant_prompt),\n\t        ]\n\t        user_messages = [\n\t            Message(role=\"system\", content=user_prompt),\n\t            Message(\n\t                role=\"user\",\n\t                content=user_prompt\n\t                + \" Now start to give me instructions one by one. Only reply with Instruction and Input.\",\n\t            ),\n", "        ]\n\t        repeat_word_counter = 0\n\t        repeat_word_threshold_exceeded = False\n\t        for i in range(max_turns):\n\t            repeated_word_current_turn = False\n\t            #\n\t            # User turn\n\t            #\n\t            user_message = llm.chat(user_messages)\n\t            user_messages.append(user_message)\n", "            assistant_messages.append(\n\t                Message(role=\"user\", content=user_message.content)\n\t            )\n\t            logging.info(f\"User turn {i}: {user_message}\")\n\t            #\n\t            # Assistant turn\n\t            #\n\t            assistant_message = llm.chat(assistant_messages)\n\t            assistant_messages.append(assistant_message)\n\t            user_messages.append(\n", "                Message(role=\"user\", content=assistant_message.content)\n\t            )\n\t            logging.info(f\"Assistant turn {i}: {assistant_message}\")\n\t            yield (user_messages, assistant_messages)\n\t            #\n\t            # Termination conditions.\n\t            #\n\t            for repeat_word in repeat_word_list:\n\t                if (\n\t                    repeat_word in assistant_message.content.lower()\n", "                    or repeat_word in user_message.content.lower()\n\t                ):\n\t                    repeat_word_counter += 1\n\t                    repeated_word_current_turn = True\n\t                    logging.info(f\"Repeat word counter = {repeat_word_counter}\")\n\t                    if repeat_word_counter == repeat_word_threshold:\n\t                        repeat_word_threshold_exceeded = True\n\t                    break\n\t            if not repeated_word_current_turn:\n\t                repeat_word_counter = 0\n", "            if (\n\t                (\"CAMEL_TASK_DONE\" in user_messages[-2].content)\n\t                or (i == max_turns - 1)\n\t                or repeat_word_threshold_exceeded\n\t            ):\n\t                #\n\t                # Summary turn\n\t                #\n\t                summary_context = \"\\n\".join(\n\t                    [\n", "                        f\"{turn.role.capitalize()} ({user_role if turn.role == 'user' else assistant_role}): {turn.content}\"\n\t                        for turn in assistant_messages\n\t                        if turn.role in [\"assistant\", \"user\"]\n\t                    ]\n\t                )\n\t                logging.info(f\"summary context: {summary_context}\")\n\t                summary_system_prompt = \"\"\"You are an experienced solution extracting agent.\n\tYour task is to extract full and complete solutions by looking at the conversation between a user and an assistant with particular specializations.\n\tYou should present me with a final and detailed solution purely based on the conversation.\n\tYou should present the solution as if its yours.\n", "Use present tense and as if you are the one presenting the solution.\n\tYou should not miss any necessary details or examples.\n\tKeep all provided explanations and codes provided throughout the conversation.\n\tRemember your task is not to summarize rather to extract the full solution.\"\"\"\n\t                summary_closing_prompt = f\"\"\"\\n\\nAs a reminder, the above context is to help you provide a complete and concise solution to the following task: {task_prompt}\n\tDo not attempt to describe the solution, but try to answer in a way such that your answer will directly solve the task - not just describe the steps required to solve the task.\n\tOnly use the provided context above and no other sources.\n\tEven if I told you that the task is completed in the context above you should still reply with a complete solution. Never tell me the task is completed or ask for the next request, but instead replay the final solution back to me.\"\"\"\n\t                summary_prompt = (\n\t                    f\"Task:{task_prompt}\\n\" + summary_context + summary_closing_prompt\n", "                )\n\t                summary_messages = [\n\t                    Message(role=\"system\", content=summary_system_prompt),\n\t                    Message(\n\t                        role=\"user\",\n\t                        content=\"Here is the conversation: \" + summary_prompt,\n\t                    ),\n\t                ]\n\t                hparams = {\"model\": summary_model}\n\t                message = llm.chat(summary_messages, hparams)\n", "                assistant_messages.append(message)\n\t                user_messages.append(Message(role=\"user\", content=message.content))\n\t                logging.info(message.content)\n\t                # End of conversation\n\t                yield (user_messages, assistant_messages)\n\t                break\n\t    except Exception as e:\n\t        logging.error(\"Error in CAMEL agent: \", e)\n"]}
{"filename": "log10/agents/scrape_summarizer.py", "chunked_list": ["from log10.llm import LLM, Message\n\tfrom anthropic import HUMAN_PROMPT\n\tfrom log10.tools import browser\n\t# Set up Summarizer agent\n\tsystem_prompt = \"You are an expert at extracting the main points from a website. Only look at the provided website content by the user to extract the main points.\"\n\tsummarize_prompt = (\n\t    \"Extract the main points from the following website:\\n {website_text}\"\n\t)\n\tdef scrape_summarizer(url, llm: LLM):\n\t    website_text = browser(url)\n", "    prompt = summarize_prompt.format(website_text=website_text)\n\t    messages = [\n\t        Message(role=\"system\", content=system_prompt),\n\t        Message(role=\"user\", content=prompt),\n\t    ]\n\t    hparams = {\"temperature\": 0.2}\n\t    completion = llm.chat(messages, hparams)\n\t    return completion.content\n"]}
{"filename": "examples/agents/code_optimizer.py", "chunked_list": ["import os\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.llm import NoopLLM\n\tfrom log10.load import log10\n\tfrom log10.evals import compile\n\tfrom log10.agents.camel import camel_agent\n\tfrom log10.openai import OpenAI\n\tfrom log10.tools import code_extractor\n\t# Select one of OpenAI or Anthropic models\n\tmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n", "max_turns = 10\n\tllm = None\n\tsummary_model = None\n\textraction_model = None\n\tif \"claude\" in model:\n\t    import anthropic\n\t    log10(anthropic)\n\t    summary_model = \"claude-1-100k\"\n\t    extraction_model = \"claude-1-100k\"\n\t    llm = Anthropic({\"model\": model})\n", "elif model == \"noop\":\n\t    summary_model = model\n\t    extraction_model = model\n\t    llm = NoopLLM()\n\telse:\n\t    import openai\n\t    log10(openai)\n\t    summary_model = \"gpt-3.5-turbo-16k\"\n\t    extraction_model = \"gpt-4\"\n\t    llm = OpenAI({\"model\": model})\n", "# example calls from playground (select 1)\n\tuser_messages, assistant_messages = camel_agent(\n\t    user_role=\"C developer\",\n\t    assistant_role=\"Cybersecurity expert\",\n\t    task_prompt='Correct the following code.\\n\\n#include <stdio.h>\\n#include <string.h>\\n\\nint main() {\\n    char password[8];\\n    int granted = 0;\\n\\n    printf(\"Enter password: \");\\n    scanf(\"%s\", password);\\n\\n    if (strcmp(password, \"password\") == 0) {\\n        granted = 1;\\n    }\\n\\n    if (granted) {\\n        printf(\"Access granted.\\\\n\");\\n    } else {\\n        printf(\"Access denied.\\\\n\");\\n    }\\n\\n    return 0;\\n}',\n\t    summary_model=summary_model,\n\t    max_turns=max_turns,\n\t    llm=llm,\n\t)\n\tfull_response = assistant_messages[-1].content\n", "# Next extract just the C code\n\tcode = code_extractor(full_response, \"C\", extraction_model, llm=llm)\n\tprint(f\"Extracted code\\n###\\n{code}\")\n\t# Evaluate if the code compiles\n\tresult = compile(code)\n\tif result is True:\n\t    print(\"Compilation successful\")\n\telse:\n\t    print(\"Compilation failed with error:\")\n\t    print(result[1])\n"]}
{"filename": "examples/agents/coder.py", "chunked_list": ["import os\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.llm import NoopLLM\n\tfrom log10.load import log10\n\tfrom log10.agents.camel import camel_agent\n\tfrom dotenv import load_dotenv\n\tfrom log10.openai import OpenAI\n\tload_dotenv()\n\t# Select one of OpenAI or Anthropic models\n\tmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n", "max_turns = 30\n\tllm = None\n\tsummary_model = None\n\tif \"claude\" in model:\n\t    import anthropic\n\t    log10(anthropic)\n\t    summary_model = \"claude-1-100k\"\n\t    llm = Anthropic({\"model\": model})\n\telif model == \"noop\":\n\t    summary_model = model\n", "    llm = NoopLLM()\n\telse:\n\t    import openai\n\t    log10(openai)\n\t    summary_model = \"gpt-3.5-turbo-16k\"\n\t    llm = OpenAI({\"model\": model})\n\t# example calls from playground (select 1)\n\tcamel_agent(\n\t    user_role=\"Stock Trader\",\n\t    assistant_role=\"Python Programmer\",\n", "    task_prompt=\"Develop a trading bot for the stock market\",\n\t    summary_model=summary_model,\n\t    max_turns=max_turns,\n\t    llm=llm,\n\t)\n"]}
{"filename": "examples/agents/biochemist.py", "chunked_list": ["import os\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.llm import NoopLLM\n\tfrom log10.agents.camel import camel_agent\n\tfrom dotenv import load_dotenv\n\tfrom log10.openai import OpenAI\n\tfrom log10.load import log10\n\tload_dotenv()\n\t# Select one of OpenAI or Anthropic models\n\tmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n", "max_turns = 30\n\tllm = None\n\tsummary_model = None\n\tif \"claude\" in model:\n\t    import anthropic\n\t    log10(anthropic)\n\t    summary_model = \"claude-1-100k\"\n\t    llm = Anthropic({\"model\": model})\n\telif model == \"noop\":\n\t    summary_model = model\n", "    llm = NoopLLM()\n\telse:\n\t    import openai\n\t    log10(openai)\n\t    summary_model = \"gpt-3.5-turbo-16k\"\n\t    llm = OpenAI({\"model\": model})\n\t# example calls from playground (select 1)\n\tcamel_agent(\n\t    user_role=\"Poor PhD Student\",\n\t    assistant_role=\"Experienced Computational Chemist\",\n", "    task_prompt=\"Perform a molecular dynamics solution of a molecule: CN1CCC[C@H]1c2cccnc2. Design and conduct a 100 ns molecular dynamics simulation of the molecule CN1CCC[C@H]1c2cccnc2 in an explicit solvent environment using the CHARMM force field and analyze the conformational changes and hydrogen bonding patterns over time\",\n\t    summary_model=summary_model,\n\t    max_turns=max_turns,\n\t    llm=llm,\n\t)\n"]}
{"filename": "examples/agents/email_generator.py", "chunked_list": ["import os\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.llm import NoopLLM\n\tfrom log10.load import log10\n\tfrom log10.agents.camel import camel_agent\n\tfrom dotenv import load_dotenv\n\tfrom log10.openai import OpenAI\n\tload_dotenv()\n\t# Select one of OpenAI or Anthropic models\n\tmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n", "max_turns = 30\n\tllm = None\n\tsummary_model = None\n\tif \"claude\" in model:\n\t    import anthropic\n\t    log10(anthropic)\n\t    summary_model = \"claude-1-100k\"\n\t    llm = Anthropic({\"model\": model})\n\telif model == \"noop\":\n\t    summary_model = model\n", "    llm = NoopLLM()\n\telse:\n\t    import openai\n\t    log10(openai)\n\t    summary_model = \"gpt-3.5-turbo-16k\"\n\t    llm = OpenAI({\"model\": model})\n\t# example calls from playground (select 1)\n\tcamel_agent(\n\t    user_role=\"Sales email copyeditor\",\n\t    assistant_role=\"Sales email copywriter\",\n", "    task_prompt=\"Write a sales email to Pfizer about a new healthcare CRM\",\n\t    summary_model=summary_model,\n\t    max_turns=max_turns,\n\t    llm=llm,\n\t)\n"]}
{"filename": "examples/agents/scrape_summarizer.py", "chunked_list": ["import os\n\tfrom log10.agents.scrape_summarizer import scrape_summarizer\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.llm import NoopLLM\n\tfrom log10.load import log10\n\tfrom log10.openai import OpenAI\n\t# Select one of OpenAI or Anthropic models\n\tmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n\tllm = None\n\tif \"claude\" in model:\n", "    import anthropic\n\t    log10(anthropic)\n\t    llm = Anthropic({\"model\": model})\n\telif model == \"noop\":\n\t    llm = NoopLLM()\n\telse:\n\t    import openai\n\t    log10(openai)\n\t    llm = OpenAI({\"model\": model})\n\turl = \"https://nytimes.com\"\n", "print(scrape_summarizer(url, llm))\n"]}
{"filename": "examples/agents/translator.py", "chunked_list": ["import os\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.llm import NoopLLM\n\tfrom log10.load import log10\n\tfrom log10.agents.camel import camel_agent\n\tfrom dotenv import load_dotenv\n\tfrom log10.openai import OpenAI\n\tload_dotenv()\n\t# Select one of OpenAI or Anthropic models\n\tmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n", "max_turns = 30\n\tllm = None\n\tsummary_model = None\n\tif \"claude\" in model:\n\t    import anthropic\n\t    log10(anthropic)\n\t    summary_model = \"claude-1-100k\"\n\t    llm = Anthropic({\"model\": model})\n\telif model == \"noop\":\n\t    summary_model = model\n", "    llm = NoopLLM()\n\telse:\n\t    import openai\n\t    log10(openai)\n\t    summary_model = \"gpt-3.5-turbo-16k\"\n\t    llm = OpenAI({\"model\": model})\n\t# example calls from playground (select 1)\n\tcamel_agent(\n\t    user_role=\"Web3 guru\",\n\t    assistant_role=\"Hindi translator\",\n", "    task_prompt=\"Write a blog post about web3 in Hindi\",\n\t    summary_model=summary_model,\n\t    max_turns=max_turns,\n\t    llm=llm,\n\t)\n"]}
{"filename": "examples/agents/cybersecurity_expert.py", "chunked_list": ["import os\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.llm import NoopLLM\n\tfrom log10.load import log10\n\tfrom log10.agents.camel import camel_agent\n\tfrom dotenv import load_dotenv\n\tfrom log10.openai import OpenAI\n\tload_dotenv()\n\t# Select one of OpenAI or Anthropic models\n\tmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n", "max_turns = 30\n\tllm = None\n\tsummary_model = None\n\tif \"claude\" in model:\n\t    import anthropic\n\t    log10(anthropic)\n\t    summary_model = \"claude-1-100k\"\n\t    llm = Anthropic({\"model\": model})\n\telif model == \"noop\":\n\t    summary_model = model\n", "    llm = NoopLLM()\n\telse:\n\t    import openai\n\t    log10(openai)\n\t    summary_model = \"gpt-3.5-turbo-16k\"\n\t    llm = OpenAI({\"model\": model})\n\t# example calls from playground (select 1)\n\tcamel_agent(\n\t    user_role=\"C developer\",\n\t    assistant_role=\"Cybersecurity expert\",\n", "    task_prompt='Correct the following code.\\n\\n#include <stdio.h>\\n#include <string.h>\\n\\nint main() {\\n    char password[8];\\n    int granted = 0;\\n\\n    printf(\"Enter password: \");\\n    scanf(\"%s\", password);\\n\\n    if (strcmp(password, \"password\") == 0) {\\n        granted = 1;\\n    }\\n\\n    if (granted) {\\n        printf(\"Access granted.\\\\n\");\\n    } else {\\n        printf(\"Access denied.\\\\n\");\\n    }\\n\\n    return 0;\\n}',\n\t    summary_model=summary_model,\n\t    max_turns=max_turns,\n\t    llm=llm,\n\t)\n"]}
{"filename": "examples/logging/langchain_model_logger_url.py", "chunked_list": ["from langchain import OpenAI\n\tfrom langchain.chat_models import ChatAnthropic\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.schema import HumanMessage\n\tfrom log10.langchain import Log10Callback\n\tfrom log10.llm import Log10Config\n\tlog10_callback = Log10Callback(log10_config=Log10Config())\n\tmessages = [\n\t    HumanMessage(content=\"You are a ping pong machine\"),\n\t    HumanMessage(content=\"Ping?\"),\n", "]\n\tllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])\n\tcompletion = llm.predict_messages(messages, tags=[\"foobar\"])\n\tprint(completion)\n\tprint(log10_callback.last_completion_url())\n\tllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])\n\tmessages.append(HumanMessage(content=\"Pong!\"))\n\tcompletion = llm.predict_messages(messages, tags=[\"foobar\"])\n\tprint(completion)\n\tprint(log10_callback.last_completion_url())"]}
{"filename": "examples/logging/anthropic_completion.py", "chunked_list": ["import os\n\tfrom log10.load import log10\n\timport anthropic\n\timport os\n\tlog10(anthropic, DEBUG_=False)\n\tanthropicClient = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\tresponse = anthropicClient.completions.create(\n\t    model=\"claude-1\",\n\t    prompt=f\"\\n\\nHuman:Write the names of all Star Wars movies and spinoffs along with the time periods in which they were set?{anthropic.AI_PROMPT}\",\n\t    temperature=0,\n", "    max_tokens_to_sample=1024,\n\t    top_p=1,\n\t    top_k=0\n\t)\n\tprint(response)\n"]}
{"filename": "examples/logging/langchain_multiple_tools.py", "chunked_list": ["import os\n\tfrom log10.load import log10\n\timport openai\n\tlog10(openai)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\tMAX_TOKENS = 512\n\tTOOLS_DEFAULT_LIST =  ['llm-math', 'wikipedia']\n\tfrom langchain.llms import OpenAI\n\tfrom langchain.agents import load_tools, initialize_agent\n\timport wikipedia\n", "llm = OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=MAX_TOKENS)\n\t# Set up Langchain\n\ttools = load_tools(TOOLS_DEFAULT_LIST, llm=llm)\n\tchain = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n\tinp = \"How many years elapsed between the founding of Apple and Google?\"\n\tprint(chain.run(inp))"]}
{"filename": "examples/logging/langchain_sqlagent.py", "chunked_list": ["import faker\n\timport sqlalchemy\n\tfrom langchain.sql_database import SQLDatabase\n\tfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\n\tfrom langchain.agents import create_sql_agent\n\tfrom langchain.llms import OpenAI\n\tfrom faker import Faker\n\timport random\n\timport datetime\n\tfrom sqlalchemy.orm import sessionmaker\n", "from sqlalchemy.ext.declarative import declarative_base\n\tfrom sqlalchemy import create_engine, Column, Integer, String, DateTime\n\timport os\n\tfrom log10.load import log10\n\timport openai\n\tlog10(openai)\n\t# Set up a dummy database\n\tfake = Faker()\n\t# Create a SQLite database and connect to it\n\tengine = create_engine('sqlite:///users.db', echo=True)\n", "Base = declarative_base()\n\t# Define the User class with standard fields and the created_at field\n\tclass User(Base):\n\t    __tablename__ = 'users'\n\t    id = Column(Integer, primary_key=True)\n\t    username = Column(String, unique=True, nullable=False)\n\t    email = Column(String, unique=True, nullable=False)\n\t    first_name = Column(String, nullable=False)\n\t    last_name = Column(String, nullable=False)\n\t    age = Column(Integer, nullable=False)\n", "    created_at = Column(DateTime, default=datetime.datetime.utcnow)\n\t    def __repr__(self):\n\t        return f\"<User(id={self.id}, username='{self.username}', email='{self.email}', first_name='{self.first_name}', last_name='{self.last_name}', age={self.age}, created_at={self.created_at})>\"\n\t# Helper function to generate a random user using Faker\n\tdef generate_random_user():\n\t    username = fake.user_name()\n\t    email = fake.email()\n\t    first_name = fake.first_name()\n\t    last_name = fake.last_name()\n\t    age = random.randint(18, 100)\n", "    return User(username=username, email=email, first_name=first_name, last_name=last_name, age=age)\n\t# Create the 'users' table\n\tBase.metadata.create_all(engine)\n\t# Create a session factory and a session\n\tSession = sessionmaker(bind=engine)\n\tsession = Session()\n\t# Add some example users\n\tfor n_users in range(10):\n\t    user = generate_random_user()\n\t    session.add(user)\n", "session.commit()\n\t# Query the users and print the results\n\tall_users = session.query(User).all()\n\tprint(all_users)\n\tsession.close()\n\t# Setup vars for Langchain\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\t# Setup Langchain SQL agent\n\tdb = SQLDatabase.from_uri(\"sqlite:///users.db\")\n\ttoolkit = SQLDatabaseToolkit(db=db)\n", "agent_executor = create_sql_agent(\n\t    llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"),\n\t    toolkit=toolkit,\n\t    verbose=True\n\t)\n\tprint(agent_executor.run(\"Who is the least recent user?\"))\n"]}
{"filename": "examples/logging/chatcompletion_async_vs_sync.py", "chunked_list": ["import sys\n\tif 'init_modules' in globals():\n\t    # second or subsequent run: remove all but initially loaded modules\n\t    for m in list(sys.modules.keys()):\n\t        if m not in init_modules:\n\t            del (sys.modules[m])\n\telse:\n\t    # first run: find out which modules were initially loaded\n\t    init_modules = list(sys.modules.keys())\n\timport os\n", "from log10.load import log10, log10_session\n\timport openai\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\t# Launch an async run\n\twith log10_session():\n\t    log10(openai, DEBUG_=True, USE_ASYNC_=True)\n\t    completion = openai.ChatCompletion.create(\n\t        model=\"gpt-3.5-turbo\",\n\t        messages=[\n\t            {'role': \"system\", \"content\": \"You are the most knowledgable Star Wars guru on the planet\"},\n", "            {\"role\": \"user\", \"content\": \"Write the time period of all the Star Wars movies and spinoffs?\"}\n\t        ]\n\t    )\n\t    print(completion.choices[0].message)\n\t# reload modules to prevent double calling openAI\n\tif 'init_modules' in globals():\n\t    # second or subsequent run: remove all but initially loaded modules\n\t    for m in list(sys.modules.keys()):\n\t        if m not in init_modules:\n\t            del (sys.modules[m])\n", "else:\n\t    # first run: find out which modules were initially loaded\n\t    init_modules = list(sys.modules.keys())\n\timport openai  # noqa\n\t# Compare to sync run - note there can be variability in the OpenAI calls\n\twith log10_session():\n\t    log10(openai, DEBUG_=True, USE_ASYNC_=False)\n\t    completion = openai.ChatCompletion.create(\n\t        model=\"gpt-3.5-turbo\",\n\t        messages=[\n", "            {'role': \"system\", \"content\": \"You are the most knowledgable Star Wars guru on the planet\"},\n\t            {\"role\": \"user\", \"content\": \"Write the time period of all the Star Wars movies and spinoffs?\"}\n\t        ]\n\t    )\n\t    print(completion.choices[0].message)\n"]}
{"filename": "examples/logging/langchain_model_logger.py", "chunked_list": ["from langchain import OpenAI\n\tfrom langchain.chat_models import ChatAnthropic\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.schema import HumanMessage\n\tfrom log10.langchain import Log10Callback\n\tfrom log10.llm import Log10Config\n\tlog10_callback = Log10Callback(log10_config=Log10Config())\n\tmessages = [\n\t    HumanMessage(content=\"You are a ping pong machine\"),\n\t    HumanMessage(content=\"Ping?\"),\n", "]\n\tllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])\n\tcompletion = llm.predict_messages(messages, tags=[\"foobar\"])\n\tprint(completion)\n\tllm = ChatAnthropic(model=\"claude-2\", callbacks=[log10_callback], temperature=0.7, tags=[\"baz\"])\n\tllm.predict_messages(messages)\n\tprint(completion)\n\tllm = OpenAI(model_name=\"text-davinci-003\", callbacks=[log10_callback], temperature=0.5)\n\tcompletion = llm.predict(\"You are a ping pong machine.\\nPing?\\n\")\n\tprint(completion)\n"]}
{"filename": "examples/logging/langchain_simple_sequential.py", "chunked_list": ["import os\n\tfrom log10.load import log10\n\timport openai\n\tlog10(openai)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\tfrom langchain.llms import OpenAI\n\tfrom langchain.prompts import PromptTemplate\n\tfrom langchain.chains import LLMChain, SimpleSequentialChain\n\tllm = OpenAI(temperature=0.9, model_name=\"text-babbage-001\")\n\tprompt = PromptTemplate(\n", "    input_variables=[\"product\"],\n\t    template=\"What is a good name for a company that makes {product}?\",\n\t)\n\tchain = LLMChain(llm=llm, prompt=prompt)\n\tsecond_prompt = PromptTemplate(\n\t    input_variables=[\"company_name\"],\n\t    template=\"Write a catchphrase for the following company: {company_name}\",\n\t)\n\tchain_two = LLMChain(llm=llm, prompt=second_prompt)\n\toverall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n", "# Run the chain specifying only the input variable for the first chain.\n\tcatchphrase = overall_chain.run(\"colorful socks\")\n\tprint(catchphrase)"]}
{"filename": "examples/logging/completion_ada.py", "chunked_list": ["import os\n\tfrom log10.load import log10\n\timport openai\n\tlog10(openai, DEBUG_=True, USE_ASYNC_=False)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\tresponse = openai.Completion.create(\n\t    model=\"text-ada-001\",\n\t    prompt=\"What is 2+2?\",\n\t    temperature=0,\n\t    max_tokens=1024,\n", "    top_p=1,\n\t    frequency_penalty=0,\n\t    presence_penalty=0\n\t)\n\tprint(response)\n"]}
{"filename": "examples/logging/tags_openai.py", "chunked_list": ["import os\n\tfrom log10.load import log10, log10_session\n\timport openai\n\tlog10(openai)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\tresponse = openai.Completion.create(\n\t    model=\"text-ada-001\",\n\t    prompt=\"Where are the pyramids?\",\n\t    temperature=0,\n\t    max_tokens=1024,\n", "    top_p=1,\n\t    frequency_penalty=0,\n\t    presence_penalty=0,\n\t)\n\tprint(response)\n\twith log10_session(tags=[\"foo\", \"bar\"]):\n\t    response = openai.Completion.create(\n\t        model=\"text-ada-001\",\n\t        prompt=\"Where is the Eiffel Tower?\",\n\t        temperature=0,\n", "        max_tokens=1024,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0,\n\t    )\n\t    print(response)\n\twith log10_session(tags=[\"bar\", \"baz\"]):\n\t    response = openai.Completion.create(\n\t        model=\"text-ada-001\",\n\t        prompt=\"Where is the statue of liberty?\",\n", "        temperature=0,\n\t        max_tokens=1024,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0,\n\t    )\n\t    print(response)\n\tresponse = openai.Completion.create(\n\t    model=\"text-ada-001\",\n\t    prompt=\"Where is machu picchu?\",\n", "    temperature=0,\n\t    max_tokens=1024,\n\t    top_p=1,\n\t    frequency_penalty=0,\n\t    presence_penalty=0,\n\t)\n\tprint(response)"]}
{"filename": "examples/logging/completion.py", "chunked_list": ["import os\n\tfrom log10.load import log10\n\timport openai\n\tlog10(openai)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\tresponse = openai.Completion.create(\n\t  model=\"text-davinci-003\",\n\t  prompt=\"Write the names of all Star Wars movies and spinoffs along with the time periods in which they were set?\",\n\t  temperature=0,\n\t  max_tokens=1024,\n", "  top_p=1,\n\t  frequency_penalty=0,\n\t  presence_penalty=0\n\t)\n\tprint(response)\n"]}
{"filename": "examples/logging/langchain_babyagi.py", "chunked_list": ["import os\n\tfrom collections import deque\n\tfrom typing import Dict, List, Optional, Any\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain import LLMChain, PromptTemplate\n\tfrom langchain.embeddings import OpenAIEmbeddings\n\tfrom langchain.llms import BaseLLM\n\tfrom langchain.vectorstores.base import VectorStore\n\tfrom pydantic import BaseModel, Field\n\tfrom langchain.chains.base import Chain\n", "from langchain.vectorstores import FAISS\n\tfrom langchain.docstore import InMemoryDocstore\n\tfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n\tfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\n\timport openai\n\timport log10\n\tfrom log10.load import log10\n\tlog10(openai)\n\t# Adapted from: https://python.langchain.com/en/latest/use_cases/agents/baby_agi_with_agent.html\n\t# Define your embedding model\n", "embeddings_model = OpenAIEmbeddings()\n\t# Initialize the vectorstore as empty\n\timport faiss\n\tembedding_size = 1536\n\tindex = faiss.IndexFlatL2(embedding_size)\n\tvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n\tclass TaskCreationChain(LLMChain):\n\t    \"\"\"Chain to generates tasks.\"\"\"\n\t    @classmethod\n\t    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n", "        \"\"\"Get the response parser.\"\"\"\n\t        task_creation_template = (\n\t            \"You are an task creation AI that uses the result of an execution agent\"\n\t            \" to create new tasks with the following objective: {objective},\"\n\t            \" The last completed task has the result: {result}.\"\n\t            \" This result was based on this task description: {task_description}.\"\n\t            \" These are incomplete tasks: {incomplete_tasks}.\"\n\t            \" Based on the result, create new tasks to be completed\"\n\t            \" by the AI system that do not overlap with incomplete tasks.\"\n\t            \" Return the tasks as an array.\"\n", "        )\n\t        prompt = PromptTemplate(\n\t            template=task_creation_template,\n\t            input_variables=[\"result\", \"task_description\", \"incomplete_tasks\", \"objective\"],\n\t        )\n\t        return cls(prompt=prompt, llm=llm, verbose=verbose)\n\tclass TaskPrioritizationChain(LLMChain):\n\t    \"\"\"Chain to prioritize tasks.\"\"\"\n\t    @classmethod\n\t    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n", "        \"\"\"Get the response parser.\"\"\"\n\t        task_prioritization_template = (\n\t            \"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\n\t            \" the following tasks: {task_names}.\"\n\t            \" Consider the ultimate objective of your team: {objective}.\"\n\t            \" Do not remove any tasks. Return the result as a numbered list, like:\"\n\t            \" #. First task\"\n\t            \" #. Second task\"\n\t            \" Start the task list with number {next_task_id}.\"\n\t        )\n", "        prompt = PromptTemplate(\n\t            template=task_prioritization_template,\n\t            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n\t        )\n\t        return cls(prompt=prompt, llm=llm, verbose=verbose)\n\ttodo_prompt = PromptTemplate.from_template(\"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\")\n\ttodo_chain = LLMChain(llm=ChatOpenAI(temperature=0), prompt=todo_prompt)\n\tsearch = SerpAPIWrapper()\n\ttools = [\n\t    Tool(\n", "        name = \"Search\",\n\t        func=search.run,\n\t        description=\"useful for when you need to answer questions about current events\"\n\t    ),\n\t    Tool(\n\t        name = \"TODO\",\n\t        func=todo_chain.run,\n\t        description=\"useful for when you need to come up with todo lists. Input: an objective to create a todo list for. Output: a todo list for that objective. Please be very clear what the objective is!\"\n\t    )\n\t]\n", "prefix = \"\"\"You are an AI who performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.\"\"\"\n\tsuffix = \"\"\"Question: {task}\n\t{agent_scratchpad}\"\"\"\n\tprompt = ZeroShotAgent.create_prompt(\n\t    tools, \n\t    prefix=prefix, \n\t    suffix=suffix, \n\t    input_variables=[\"objective\", \"task\", \"context\",\"agent_scratchpad\"]\n\t)\n\tdef get_next_task(task_creation_chain: LLMChain, result: Dict, task_description: str, task_list: List[str], objective: str) -> List[Dict]:\n", "    \"\"\"Get the next task.\"\"\"\n\t    incomplete_tasks = \", \".join(task_list)\n\t    response = task_creation_chain.run(result=result, task_description=task_description, incomplete_tasks=incomplete_tasks, objective=objective)\n\t    new_tasks = response.split('\\n')\n\t    return [{\"task_name\": task_name} for task_name in new_tasks if task_name.strip()]\n\tdef prioritize_tasks(task_prioritization_chain: LLMChain, this_task_id: int, task_list: List[Dict], objective: str) -> List[Dict]:\n\t    \"\"\"Prioritize tasks.\"\"\"\n\t    task_names = [t[\"task_name\"] for t in task_list]\n\t    next_task_id = int(this_task_id) + 1\n\t    response = task_prioritization_chain.run(task_names=task_names, next_task_id=next_task_id, objective=objective)\n", "    new_tasks = response.split('\\n')\n\t    prioritized_task_list = []\n\t    for task_string in new_tasks:\n\t        if not task_string.strip():\n\t            continue\n\t        task_parts = task_string.strip().split(\".\", 1)\n\t        if len(task_parts) == 2:\n\t            task_id = task_parts[0].strip()\n\t            task_name = task_parts[1].strip()\n\t            prioritized_task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n", "    return prioritized_task_list\n\tdef _get_top_tasks(vectorstore, query: str, k: int) -> List[str]:\n\t    \"\"\"Get the top k tasks based on the query.\"\"\"\n\t    results = vectorstore.similarity_search_with_score(query, k=k)\n\t    if not results:\n\t        return []\n\t    sorted_results, _ = zip(*sorted(results, key=lambda x: x[1], reverse=True))\n\t    return [str(item.metadata['task']) for item in sorted_results]\n\tdef execute_task(vectorstore, execution_chain: LLMChain, objective: str, task: str, k: int = 5) -> str:\n\t    \"\"\"Execute a task.\"\"\"\n", "    context = _get_top_tasks(vectorstore, query=objective, k=k)\n\t    return execution_chain.run(objective=objective, context=context, task=task)\n\tclass BabyAGI(Chain, BaseModel):\n\t    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n\t    task_list: deque = Field(default_factory=deque)\n\t    task_creation_chain: TaskCreationChain = Field(...)\n\t    task_prioritization_chain: TaskPrioritizationChain = Field(...)\n\t    execution_chain: AgentExecutor = Field(...)\n\t    task_id_counter: int = Field(1)\n\t    vectorstore: VectorStore = Field(init=False)\n", "    max_iterations: Optional[int] = None\n\t    class Config:\n\t        \"\"\"Configuration for this pydantic object.\"\"\"\n\t        arbitrary_types_allowed = True\n\t    def add_task(self, task: Dict):\n\t        self.task_list.append(task)\n\t    def print_task_list(self):\n\t        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n\t        for t in self.task_list:\n\t            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n", "    def print_next_task(self, task: Dict):\n\t        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n\t        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n\t    def print_task_result(self, result: str):\n\t        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n\t        print(result)\n\t    @property\n\t    def input_keys(self) -> List[str]:\n\t        return [\"objective\"]\n\t    @property\n", "    def output_keys(self) -> List[str]:\n\t        return []\n\t    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n\t        \"\"\"Run the agent.\"\"\"\n\t        objective = inputs['objective']\n\t        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n\t        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n\t        num_iters = 0\n\t        while True:\n\t            if self.task_list:\n", "                self.print_task_list()\n\t                # Step 1: Pull the first task\n\t                task = self.task_list.popleft()\n\t                self.print_next_task(task)\n\t                # Step 2: Execute the task\n\t                result = execute_task(\n\t                    self.vectorstore, self.execution_chain, objective, task[\"task_name\"]\n\t                )\n\t                this_task_id = int(task[\"task_id\"])\n\t                self.print_task_result(result)\n", "                # Step 3: Store the result in Pinecone\n\t                result_id = f\"result_{task['task_id']}\"\n\t                self.vectorstore.add_texts(\n\t                    texts=[result],\n\t                    metadatas=[{\"task\": task[\"task_name\"]}],\n\t                    ids=[result_id],\n\t                )\n\t                # Step 4: Create new tasks and reprioritize task list\n\t                new_tasks = get_next_task(\n\t                    self.task_creation_chain, result, task[\"task_name\"], [t[\"task_name\"] for t in self.task_list], objective\n", "                )\n\t                for new_task in new_tasks:\n\t                    self.task_id_counter += 1\n\t                    new_task.update({\"task_id\": self.task_id_counter})\n\t                    self.add_task(new_task)\n\t                self.task_list = deque(\n\t                    prioritize_tasks(\n\t                        self.task_prioritization_chain, this_task_id, list(self.task_list), objective\n\t                    )\n\t                )\n", "            num_iters += 1\n\t            if self.max_iterations is not None and num_iters == self.max_iterations:\n\t                print(\"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\")\n\t                break\n\t        return {}\n\t    @classmethod\n\t    def from_llm(\n\t        cls,\n\t        llm: BaseLLM,\n\t        vectorstore: VectorStore,\n", "        verbose: bool = False,\n\t        **kwargs\n\t    ) -> \"BabyAGI\":\n\t        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n\t        task_creation_chain = TaskCreationChain.from_llm(\n\t            llm, verbose=verbose\n\t        )\n\t        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n\t            llm, verbose=verbose\n\t        )\n", "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n\t        tool_names = [tool.name for tool in tools]\n\t        agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\n\t        agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n\t        return cls(\n\t            task_creation_chain=task_creation_chain,\n\t            task_prioritization_chain=task_prioritization_chain,\n\t            execution_chain=agent_executor,\n\t            vectorstore=vectorstore,\n\t            **kwargs\n", "        )\n\tOBJECTIVE = \"Invent a new drug to cure cancer.\"\n\tllm = ChatOpenAI(temperature=0)\n\t# Logging of LLMChains\n\tverbose=False\n\t# If None, will keep on going forever\n\tmax_iterations: Optional[int] = 3\n\tbaby_agi = BabyAGI.from_llm(\n\t    llm=llm,\n\t    vectorstore=vectorstore,\n", "    verbose=verbose,\n\t    max_iterations=max_iterations\n\t)\n\tbaby_agi({\"objective\": OBJECTIVE})\n"]}
{"filename": "examples/logging/multiple_sessions.py", "chunked_list": ["from langchain.chains import LLMChain, SimpleSequentialChain\n\tfrom langchain.prompts import PromptTemplate\n\tfrom langchain.llms import OpenAI\n\timport os\n\tfrom log10.load import log10, log10_session\n\timport openai\n\tlog10(openai)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\tllm = OpenAI(temperature=0.9, model_name=\"text-curie-001\")\n\twith log10_session():\n", "    prompt = PromptTemplate(\n\t        input_variables=[\"product\"],\n\t        template=\"What is a good name for a company that makes {product}?\",\n\t    )\n\t    chain = LLMChain(llm=llm, prompt=prompt)\n\t    second_prompt = PromptTemplate(\n\t        input_variables=[\"company_name\"],\n\t        template=\"Write a catchphrase for the following company: {company_name}\",\n\t    )\n\t    chain_two = LLMChain(llm=llm, prompt=second_prompt)\n", "    overall_chain = SimpleSequentialChain(\n\t        chains=[chain, chain_two], verbose=True)\n\t    # Run the chain specifying only the input variable for the first chain.\n\t    catchphrase = overall_chain.run(\"colorful socks\")\n\t    print(catchphrase)\n\twith log10_session():\n\t    third_prompt = PromptTemplate(\n\t        input_variables=[\"month\"],\n\t        template=\"What is a good country to travel during {month}?\",\n\t    )\n", "    chain_three = LLMChain(llm=llm, prompt=third_prompt)\n\t    fourth_prompt = PromptTemplate(\n\t        input_variables=[\"country_name\"],\n\t        template=\"Write a 1 day itinerary to {country_name}\",\n\t    )\n\t    chain_four = LLMChain(llm=llm, prompt=fourth_prompt)\n\t    overall_chain_two = SimpleSequentialChain(\n\t        chains=[chain_three, chain_four], verbose=True)\n\t    # Run the chain specifying only the input variable for the first chain.\n\t    catchphrase_two = overall_chain_two.run(\"April\")\n", "    print(catchphrase_two)\n"]}
{"filename": "examples/logging/langchain_qa.py", "chunked_list": ["import os\n\tfrom log10.load import log10\n\timport openai\n\tlog10(openai)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\t# Example from: https://python.langchain.com/en/latest/use_cases/question_answering.html\n\t# Download the state_of_the_union.txt here: https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\n\t# This example requires: pip install chromadb\n\t# Load Your Documents\n\tfrom langchain.document_loaders import TextLoader\n", "loader = TextLoader('./examples/logging/state_of_the_union.txt')\n\t# Create Your Index\n\tfrom langchain.indexes import VectorstoreIndexCreator\n\tfrom langchain.vectorstores import Chroma\n\tfrom langchain.embeddings import OpenAIEmbeddings\n\tfrom langchain.text_splitter import CharacterTextSplitter\n\tfrom langchain.chat_models import ChatOpenAI\n\tindex = VectorstoreIndexCreator(\n\t    vectorstore_cls=Chroma, \n\t    embedding=OpenAIEmbeddings(),\n", "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n\t).from_loaders([loader])\n\t# Query Your Index\n\tquery = \"What did the president say about Ketanji Brown Jackson\"\n\tprint(index.query_with_sources(query, llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")))\n"]}
{"filename": "examples/logging/chatcompletion.py", "chunked_list": ["import os\n\tfrom log10.load import log10\n\timport openai\n\tlog10(openai)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\tcompletion = openai.ChatCompletion.create(\n\t  model=\"gpt-3.5-turbo\",\n\t  messages=[\n\t    {'role': \"system\", \"content\": \"You are the most knowledgable Star Wars guru on the planet\"},\n\t    {\"role\": \"user\", \"content\": \"Write the time period of all the Star Wars movies and spinoffs?\"}\n", "  ]\n\t)\n\tprint(completion.choices[0].message)\n"]}
{"filename": "examples/logging/get_url.py", "chunked_list": ["from langchain.chains import LLMChain, SimpleSequentialChain\n\tfrom langchain.prompts import PromptTemplate\n\tfrom langchain.llms import OpenAI\n\timport os\n\tfrom log10.load import log10, log10_session\n\timport openai\n\tlog10(openai)\n\topenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\tllm = OpenAI(temperature=0.9, model_name=\"text-curie-001\")\n\twith log10_session() as session:\n", "    print(session.last_completion_url())\n\t    response = openai.Completion.create(\n\t        model=\"text-ada-001\",\n\t        prompt=\"Why did the chicken cross the road?\",\n\t        temperature=0,\n\t        max_tokens=1024,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0,\n\t    )\n", "    print(session.last_completion_url())\n\t    response = openai.Completion.create(\n\t        model=\"text-ada-001\",\n\t        prompt=\"Why did the cow cross the road?\",\n\t        temperature=0,\n\t        max_tokens=1024,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0,\n\t    )\n", "    print(session.last_completion_url())\n\twith log10_session() as session:\n\t    print(session.last_completion_url())\n\t    response = openai.Completion.create(\n\t        model=\"text-ada-001\",\n\t        prompt=\"Why did the frog cross the road?\",\n\t        temperature=0,\n\t        max_tokens=1024,\n\t        top_p=1,\n\t        frequency_penalty=0,\n", "        presence_penalty=0,\n\t    )\n\t    print(session.last_completion_url())\n\t    response = openai.Completion.create(\n\t        model=\"text-ada-001\",\n\t        prompt=\"Why did the scorpion cross the road?\",\n\t        temperature=0,\n\t        max_tokens=1024,\n\t        top_p=1,\n\t        frequency_penalty=0,\n", "        presence_penalty=0,\n\t    )\n\t    print(session.last_completion_url())\n"]}
{"filename": "examples/logging/tags_mixed.py", "chunked_list": ["import os\n\tfrom log10.load import log10, log10_session\n\timport openai\n\tfrom langchain import OpenAI\n\tlog10(openai)\n\twith log10_session(tags=[\"foo\", \"bar\"]):\n\t    response = openai.Completion.create(\n\t        model=\"text-ada-001\",\n\t        prompt=\"Where is the Eiffel Tower?\",\n\t        temperature=0,\n", "        max_tokens=1024,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0,\n\t    )\n\t    print(response)\n\t    llm = OpenAI(model_name=\"text-ada-001\", temperature=0.5)\n\t    response = llm.predict(\"You are a ping pong machine.\\nPing?\\n\")\n\t    print(response)\n"]}
{"filename": "examples/evals/fuzzy.py", "chunked_list": ["import os\n\tfrom log10.openai import OpenAI\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.evals import eval\n\t# Choose provider\n\tprovider = \"anthropic\"  # \"anthropic\"\n\tllm = None\n\tif provider == \"openai\":\n\t    llm = OpenAI(\n\t        {\n", "            \"model\": \"gpt-3.5-turbo\",\n\t            \"temperature\": 0,\n\t            \"max_tokens\": 1024,\n\t            \"top_p\": 1,\n\t            \"frequency_penalty\": 0,\n\t            \"presence_penalty\": 0,\n\t        }\n\t    )\n\telif provider == \"anthropic\":\n\t    llm = Anthropic(\n", "        {\n\t            \"model\": \"claude-1\",\n\t            \"temperature\": 0,\n\t            \"max_tokens_to_sample\": 1024,\n\t        }\n\t    )\n\telse:\n\t    print(\n\t        f\"Unsupported provider option: {provider}. Supported providers are 'openai' or 'anthropic'.\"\n\t    )\n", "# Ground truth dataset to use for evaluation\n\teval_dataset = (\n\t    \"fuzzy_data.csv\",\n\t    {\"input\": \"my_input_column\", \"ideal\": \"my_output_column\"},\n\t)\n\t# Specify which metrics to use. Options are:\n\t# 'match': model_output.startswith(ideal)\n\t# 'includes': ideal.lower() in model_output.lower()\n\t# 'fuzzy_match': similar to includes but remove punctuation, articles and extra whitespace and compare both ways\n\teval_metric = \"fuzzy_match\"\n", "# Path to output file to store the metrics\n\t# Example from: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/registry/evals/test-basic.yaml#L7\n\tout_file_path = \"fuzzy_output.csv\"\n\t# Get back and id and url for the summary of results and status\n\t# todo: get back path to logfile; eval_id, eval_url =\n\teval(llm, eval_dataset, eval_metric, out_file_path)\n"]}
{"filename": "examples/evals/compile.py", "chunked_list": ["from log10.anthropic import Anthropic\n\tfrom log10.llm import Message, NoopLLM\n\tfrom log10.load import log10\n\tfrom log10.evals import compile\n\tfrom log10.openai import OpenAI\n\tfrom log10.tools import code_extractor\n\t# Select one of OpenAI or Anthropic models\n\tmodel = \"gpt-3.5-turbo\"\n\t# model = \"claude-1\"\n\tllm = None\n", "if \"claude\" in model:\n\t    llm = Anthropic({\"model\": model})\n\t    extraction_model = \"claude-1-100k\"\n\telif model == \"noop\":\n\t    llm = NoopLLM()\n\t    extraction_model = \"noop\"\n\telse:\n\t    llm = OpenAI({\"model\": model})\n\t    extraction_model = \"gpt-4\"\n\t# First, write a hello world program\n", "messages = [\n\t    Message(role=\"system\", content=\"You are an expert C programmer.\"),\n\t    Message(\n\t        role=\"user\",\n\t        content=\"Write a hello world program. Insert a null character after the hello world\",\n\t    ),\n\t]\n\tcompletion = llm.chat(messages, {\"temperature\": 0.2})\n\tfull_response = completion.content\n\tprint(f\"Full response\\n###\\n{full_response}\")\n", "# Next extract just the C code\n\tcode = code_extractor(full_response, \"C\", extraction_model, llm)\n\tprint(f\"Extracted code\\n###\\n{code}\")\n\t# Evaluate if the code compiles\n\tresult = compile(code)\n\tif result is True:\n\t    print(\"Compilation successful\")\n\telse:\n\t    print(\"Compilation failed with error:\")\n\t    print(result[1])\n"]}
{"filename": "examples/evals/basic_eval.py", "chunked_list": ["import os\n\tfrom log10.anthropic import Anthropic\n\tfrom log10.evals import eval\n\tfrom log10.openai import OpenAI\n\t# Choose provider\n\tprovider = \"openai\"  # \"anthropic\"\n\t# TODO: Replace with LLM abstraction.\n\tllm = None\n\tif provider == \"openai\":\n\t    llm = OpenAI(\n", "        {\n\t            \"model\": \"gpt-3.5-turbo\",\n\t            \"temperature\": 0,\n\t            \"max_tokens\": 1024,\n\t            \"top_p\": 1,\n\t            \"frequency_penalty\": 0,\n\t            \"presence_penalty\": 0,\n\t        }\n\t    )\n\telif provider == \"anthropic\":\n", "    llm = Anthropic(\n\t        {\n\t            \"model\": \"claude-1\",\n\t            \"temperature\": 0,\n\t            \"max_tokens_to_sample\": 1024,\n\t        }\n\t    )\n\telse:\n\t    print(\n\t        f\"Unsupported provider option: {provider}. Supported providers are 'openai' or 'anthropic'.\"\n", "    )\n\t# Ground truth dataset to use for evaluation\n\teval_dataset = (\n\t    \"match_data.csv\",\n\t    {\"input\": \"my_input_column\", \"ideal\": \"my_output_column\"},\n\t)\n\t# Specify which metrics to use. Options are:\n\t# 'match': model_output.startswith(ideal)\n\t# 'includes': ideal.lower() in model_output.lower()\n\t# 'fuzzy_match': similar to includes but remove punctuation, articles and extra whitespace and compare both ways\n", "eval_metric = \"includes\"\n\t# Path to output file to store the metrics\n\t# Example from: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/test/match.py\n\tout_file_path = \"match_output.csv\"\n\t# Get back and id and url for the summary of results and status\n\t# todo: get back path to logfile; eval_id, eval_url =\n\teval(llm, eval_dataset, eval_metric, out_file_path)\n"]}
