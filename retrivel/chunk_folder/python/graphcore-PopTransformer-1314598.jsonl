{"filename": "poptransformer/sessions.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport math\n\timport popart\n\tfrom poptransformer.utils import REGISTRY\n\tclass Session:\n\t    logger = REGISTRY.get('logger')\n\t    def __init__(self, **kwargs):\n\t        self.logger.info(f'initializing {self.__class__.__name__}')\n\t        self.execution_cache_name = kwargs.get('execution_cache_name', None)\n", "        self.disable_outlining = kwargs.get('disable_outlining', False)\n\t        self.unstable_softmax = kwargs.get('unstable_softmax', False)\n\t        self.disable_matmul_multi_stage_reduce = kwargs.get('disable_matmul_multi_stage_reduce', False)\n\t        self.constant_folding_of_multiple_consumers = kwargs.get('constant_folding_of_multiple_consumers', False)\n\t        self.use_loop_candidate_creator = kwargs.get('use_loop_candidate_creator', True)\n\t        self.profile_name = kwargs.get('profile_name', None)\n\t        self.enable_pipeline = REGISTRY.get('enable_pipeline')\n\t        self.batch_per_step = REGISTRY.get('batch_per_step')\n\t    def compile(self, model):\n\t        self.model = model\n", "        self._build_data_flow()\n\t        self._build_device_info()\n\t        self._build_options()\n\t        self._build_inference_session()\n\t        self.logger.info('compiling')\n\t        self.session.prepareDevice()\n\t        self.session.weightsFromHost()\n\t        self._anchor_arrays = self.session.initAnchorArrays()\n\t        self.logger.info('compiled')\n\t    def run(self, input_dict):\n", "        stepio = popart.PyStepIO(input_dict, self.anchor_arrays)\n\t        self.session.run(stepio)\n\t    def _build_data_flow(self):\n\t        self.logger.info(f'building data flow, with batch_per_step: {self.batch_per_step}.')\n\t        self.data_flow = popart.DataFlow(self.batch_per_step, self.model.model_output)\n\t    def _build_device_info(self):\n\t        self.logger.info('building device info')\n\t        ipu_num = pow(2, math.ceil(math.log2(self.model.stage_num * self.model.num_replicas)))\n\t        self.device_info = popart.DeviceManager().acquireAvailableDevice(ipu_num)\n\t        self.logger.info(\n", "            f'acquired {ipu_num} ipu with {self.model.stage_num} stage, {self.model.num_replicas} replica')\n\t    def _build_options(self):\n\t        self.logger.info('building session options')\n\t        self.options = popart.SessionOptions()\n\t        self.options.enableOutlining = not self.disable_outlining\n\t        self.options.enableNonStableSoftmax = self.unstable_softmax\n\t        self.options.enableReplicatedGraphs = self.model.num_replicas != 1\n\t        self.options.replicatedGraphCount = self.model.num_replicas\n\t        self.options.enableConstantFoldingOfMultipleConsumers = self.constant_folding_of_multiple_consumers\n\t        self.options.useLoopCandidateCreator = self.use_loop_candidate_creator\n", "        self.options.enablePipelining = self.enable_pipeline\n\t        if self.disable_matmul_multi_stage_reduce:\n\t            self.options.matmulOptions = {\"enableMultiStageReduce\": \"false\"}\n\t        if self.model.stage_num != 1:\n\t            self.options.virtualGraphMode = popart.VirtualGraphMode.Manual\n\t            self.logger.info('setting virtual graph model to manual')\n\t        if self.execution_cache_name:\n\t            self.options.enableEngineCaching = True\n\t            self.options.cachePath = self.execution_cache_name\n\t            self.logger.info(f'saving execution cache in {self.execution_cache_name}')\n", "        if self.profile_name:\n\t            self.options.engineOptions = {\n\t                \"autoReport.all\": \"true\",\n\t                \"autoReport.directory\": f'profiles/{self.profile_name}',\n\t                \"autoReport.executionProfileProgramRunCount\":\"2\",\n\t                \"debug.allowOutOfMemory\": \"true\"\n\t            }\n\t            self.logger.info(f'saving profiles in profiles/{self.profile_name}')\n\t    def _build_inference_session(self):\n\t        self.logger.info('building inference session')\n", "        self.session = popart.InferenceSession(\n\t            fnModel=self.model.model_proto,\n\t            deviceInfo=self.device_info,\n\t            dataFlow=self.data_flow,\n\t            userOptions=self.options\n\t        )\n\t    @property\n\t    def anchor_arrays(self):\n\t        return self._anchor_arrays\n"]}
{"filename": "poptransformer/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n"]}
{"filename": "poptransformer/utils/options_scope.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom .registry import REGISTRY\n\tclass OptionsScope:\n\t    def __init__(self, amp=None, partialtype=None, serial_factor=None, serial_mode=None):\n\t        self.amp = amp\n\t        self.partialtype = partialtype\n\t        self.serial_factor = serial_factor\n\t        self.serial_mode = serial_mode\n\t        self.default_amp = None\n", "        self.default_partialtype = None\n\t        self.default_serial_factor = None\n\t        self.default_serial_mode = 'none'\n\t    def __enter__(self):\n\t        if self.amp is not None:\n\t            self.default_amp = REGISTRY.get('amp')\n\t            REGISTRY.update('amp', self.amp)\n\t            REGISTRY.get('logger').debug(f'using amp: {self.amp}')\n\t        if self.partialtype is not None:\n\t            self.default_partialtype = REGISTRY.get('partialtype')\n", "            REGISTRY.update('partialtype', self.partialtype)\n\t            REGISTRY.get('logger').debug(f'using partialtype: {self.partialtype}')\n\t        if self.serial_factor is not None:\n\t            self.default_serial_factor = REGISTRY.get('serial_factor')\n\t            self.default_serial_mode = REGISTRY.get('serial_mode')\n\t            REGISTRY.update('serial_factor', self.serial_factor)\n\t            REGISTRY.update('serial_mode', self.serial_mode)\n\t            REGISTRY.get('logger').debug(f'using serialize: {self.serial_factor}, mode: {self.serial_mode}')\n\t        return self\n\t    def __exit__(self, exc_type, exc_value, traceback):\n", "        if self.default_amp != self.amp:\n\t            REGISTRY.update('amp', self.default_amp)\n\t            REGISTRY.get('logger').debug(f'exit and using default_amp: {self.default_amp}')\n\t        if self.default_partialtype != self.partialtype:\n\t            REGISTRY.update('partialtype', self.default_partialtype)\n\t            REGISTRY.get('logger').debug(f'exit and using default_partialtype: {self.default_partialtype}')\n\t        if self.default_serial_factor != self.serial_factor:\n\t            REGISTRY.update('serial_factor', self.default_serial_factor)\n\t            REGISTRY.update('serial_mode', self.default_serial_mode)\n\t            REGISTRY.get('logger').debug(f'exit and using default_serial_factor: {self.default_serial_factor}')\n", "            REGISTRY.get('logger').debug(f'exit and using serial_mode: {self.serial_mode}')\n\t        return False\n"]}
{"filename": "poptransformer/utils/registry.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport popart\n\tclass Registry:\n\t    def __init__(self):\n\t        self._registry = {}\n\t    def register(self, key, value):\n\t        if key in self._registry:\n\t            raise KeyError(f\"Duplicate key: {key}\")\n\t        self._registry[key] = value\n", "    def update(self, key, value):\n\t        try:\n\t            self._registry[key] = value\n\t        except KeyError:\n\t            raise KeyError(f\"key: {key} not found, please register first\")\n\t    def get(self, key):\n\t        try:\n\t            return self._registry[key]\n\t        except KeyError:\n\t            raise KeyError(f'key: {key} not found')\n", "REGISTRY = Registry()\n\tREGISTRY.register('main_graph', popart.Builder(opsets={'ai.onnx': 10, 'ai.graphcore': 1}))\n\tREGISTRY.register('serial_factor', None)\n\tREGISTRY.register('serial_mode', None)\n\tREGISTRY.register('partialtype', None)\n\tREGISTRY.register('amp', None)\n\tREGISTRY.register('state_dict', {})\n\t# main graph is to be built for certain, we build it here, move to other place if there be more comments\n"]}
{"filename": "poptransformer/utils/tensor_type.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport numpy as np\n\tclass TensorType:\n\t    all_precision = ['fp32', 'fp16', 'int4', 'fp8', 'fp8_weight']\n\t    np_map = {'fp32': np.float32, 'fp16': np.float16, 'int4': np.float16, 'fp8': np.float16, 'fp8_weight': np.float16}\n\t    popart_map = {'fp32': 'FLOAT', 'fp16': 'FLOAT16', 'int4': 'FLOAT16', 'fp8': 'FLOAT16', 'fp8_weight': 'FLOAT16'}\n\t    def __init__(self, precision):\n\t        assert precision in self.all_precision\n\t        self.precision = precision\n", "    @property\n\t    def np_float_type(self):\n\t        return self.np_map[self.precision]\n\t    @property\n\t    def popart_float_type(self):\n\t        return self.popart_map[self.precision]\n"]}
{"filename": "poptransformer/utils/prepare.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport logging\n\timport popart\n\tfrom hydra.utils import instantiate\n\tfrom .registry import REGISTRY\n\tfrom .tensor_type import TensorType\n\tdef register_global_args(config):\n\t    global_args = instantiate(config.global_args)\n\t    for key, value in global_args.items():\n", "        REGISTRY.register(key, value)\n\t    tensor_type = TensorType(global_args.precision)\n\t    REGISTRY.register('tensor_type', tensor_type)\n\tdef register_config_logger(config):\n\t    popart.getLogger().setLevel(config.popart_log_level.upper())\n\t    logger = logging.getLogger('poptransformer')\n\t    logger.setLevel(config.log_level.upper())\n\t    REGISTRY.register('logger', logger)\n\tdef prepare_model_session(config):\n\t    register_global_args(config)\n", "    register_config_logger(config)\n\t    model = instantiate(config.model)\n\t    session = instantiate(config.session)\n\t    model.build_graph()\n\t    model.graph.saveInitializersExternally(\n\t        model.initializers,\n\t        'saved_initializer.onnx'\n\t    )\n\t    session.compile(model)\n\t    return session, model\n"]}
{"filename": "poptransformer/utils/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom .registry import REGISTRY\n\tfrom .device_scope import DeviceScope\n\tfrom .options_scope import OptionsScope\n\tfrom .tensor_type import TensorType\n\tfrom .prepare import prepare_model_session\n\tfrom .param_handler.param_handler import ParamHandler\n\tfrom .param_handler.tensor_parallel_strategy import shard, shard_fused_qkv, build_sharded_weight, repeat\n"]}
{"filename": "poptransformer/utils/device_scope.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom contextlib import ExitStack\n\tclass DeviceScope:\n\t    def __init__(self, graph, virtual_graph_id=None, pipeline_stage_id=None, enable_pipeline=False, outline_attr=None):\n\t        self.graph = graph\n\t        self.virtual_graph_id = virtual_graph_id\n\t        self.pipeline_stage_id = pipeline_stage_id\n\t        self.enable_pipeline = enable_pipeline\n\t        self.outline_attr = outline_attr\n", "    def __enter__(self):\n\t        self.stack = ExitStack()\n\t        if self.virtual_graph_id is not None:\n\t            self.stack.enter_context(self.graph.virtualGraph(self.virtual_graph_id))\n\t        if self.pipeline_stage_id is not None and self.enable_pipeline:\n\t            self.stack.enter_context(self.graph.pipelineStage(self.pipeline_stage_id))\n\t        if self.outline_attr is not None:\n\t            self.stack.enter_context(self.graph.outlineAttributes(self.outline_attr))\n\t        return self\n\t    def __exit__(self, exc_type, exc_value, traceback):\n", "        self.stack.close()\n\t        return False\n"]}
{"filename": "poptransformer/utils/param_handler/param_handler.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom ..registry import REGISTRY\n\tfrom .precision_strategy import PrecisionStrategyMap\n\tfrom .tensor_parallel_strategy import TPStragetgyMap\n\tclass ParamHandler:\n\t    def __init__(self, host_layer, tp_strategy_name='identity', **vs_setting):\n\t        self.host_layer = host_layer\n\t        self.tp_strategy = TPStragetgyMap[tp_strategy_name]\n\t        self.precision_strategy = PrecisionStrategyMap[self.precision]\n", "        self.vs_setting = vs_setting\n\t    @property\n\t    def num_replicas(self):\n\t        return REGISTRY.get('num_replicas')\n\t    @property\n\t    def precision(self):\n\t        return REGISTRY.get('precision')\n\t    def process_linear_weight(self, weight_np, weight_key):\n\t        weight_fn_tp = self.tp_strategy['weight_fn']\n\t        weight_fn_precision = self.precision_strategy['weight_fn']\n", "        weight_np = weight_fn_tp(weight_np, self.num_replicas, self.tp_strategy['weight_axis'])\n\t        weight_np = weight_fn_precision(\n\t            host_layer=self.host_layer,\n\t            weight_np=weight_np,\n\t            weight_key=weight_key,\n\t            weight_fn_tp=weight_fn_tp,\n\t            num_replicas=self.num_replicas,\n\t            weight_axis=self.tp_strategy['weight_axis'],\n\t            **self.vs_setting\n\t        )\n", "        return weight_np\n\t    def process_linear_bias(self, bias):\n\t        bias_fn_tp = self.tp_strategy['bias_fn']\n\t        bias = bias_fn_tp(bias, self.num_replicas, 0)\n\t        return bias\n\t    def matmul(self, graph, x, weight):\n\t        x, weight = self._prepare_matmul(graph, x, weight)\n\t        y = self._matmul(graph, x, weight)\n\t        y = self._post_process_matmul(graph, y)\n\t        return y\n", "    def _prepare_matmul(self, graph, x, weight):\n\t        prepare_fn_tp = self.tp_strategy['prepare_matmul']\n\t        prepare_fn_precision = self.precision_strategy['prepare_matmul']\n\t        x, weight = prepare_fn_tp(graph, x, weight)\n\t        x, weight = prepare_fn_precision(graph, x, weight)\n\t        return x, weight\n\t    def _post_process_matmul(self, graph, y):\n\t        prepare_fn_tp = self.tp_strategy['post_process_matmul']\n\t        prepare_fn_precision = self.precision_strategy['post_process_matmul']\n\t        y = prepare_fn_tp(graph, y)\n", "        y = prepare_fn_precision(graph, y)\n\t        return y\n\t    def _matmul(self, graph, x, weight):\n\t        matmul_fn = self.precision_strategy['matmul_fn']\n\t        y = matmul_fn(graph, x, weight)\n\t        return y\n"]}
{"filename": "poptransformer/utils/param_handler/precision_strategy.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport numpy as np\n\tfrom poprt.utils import convert_float_to_uint8\n\tfrom poptransformer import ops\n\tfrom ..registry import REGISTRY\n\tfrom .tensor_parallel_strategy import shard, repeat\n\tdef weight_fn_identity(host_layer, weight_np, weight_key, weight_fn_tp, num_replicas, weight_axis, **vs_setting):\n\t    return weight_np\n\tdef weight_fn_int4(host_layer, weight_np, weight_key, weight_fn_tp, num_replicas, weight_axis, **vs_setting):\n", "    if weight_np.dtype == np.int8:  # Embedding/LM are FP16 precision\n\t        if len(weight_np.shape) == 3:   # TP:[num_replicas, shape1, shape2]\n\t            weight_np = weight_np.transpose(0,2,1)\n\t        elif len(weight_np.shape) == 2:\n\t            weight_np = weight_np.transpose(1, 0)\n\t        else:\n\t            raise ValueError(f\"weight_np can only have rank 2 or 3, but got {len(weight_np.shape)}.\")\n\t        scale_key = weight_key + '_scale'\n\t        scale_np = host_layer.get_param_from_state_dict(scale_key, shape_list=(weight_np.shape[1],))\n\t        if num_replicas > 1 and len(weight_np.shape)==3:\n", "            if weight_axis == 0:\n\t                scale_np = repeat(scale_np, num_replicas, 0)\n\t            elif weight_axis in [-1, 1]:\n\t                scale_np = shard(scale_np, num_replicas, 0)\n\t            else:\n\t                raise ValueError(f\"weight_axis can only be 0,1,-1, but got {weight_axis}.\")\n\t        host_layer.add_initialized_input_tensor(scale_np, scale_key, **vs_setting)\n\t    return weight_np\n\tdef weight_fn_fp8(host_layer, weight_np, weight_key, weight_fn_tp, num_replicas, weight_axis, **vs_setting):\n\t    scale_key = weight_key + '_scale'\n", "    scale_np = np.array([-1]).astype(np.int32)\n\t    if num_replicas > 1:\n\t        scale_np = np.repeat(np.expand_dims(scale_np, 0), num_replicas, axis=0)\n\t    host_layer.add_initialized_input_tensor(scale_np, scale_key, **vs_setting)\n\t    weight_np = convert_float_to_uint8(weight_np.astype(np.float32), 'F143', -1)\n\t    return weight_np\n\tdef prepare_float32_16_matmul(graph, x, weight):\n\t    return x, weight\n\tdef prepare_int4_matmul(graph, x, weight):\n\t    scale = weight + '_scale'\n", "    if scale in REGISTRY.get('main_graph').getInputTensorIds():\n\t        weight = ops.int4_to_half(graph, weight, scale, x, axis=1)\n\t    return x, weight\n\tdef prepare_fp8_matmul(graph, x, weight):\n\t    scale = weight + '_scale'\n\t    if scale in REGISTRY.get('main_graph').getInputTensorIds():\n\t        x = ops.half_to_uint8(graph, x, scale)\n\t    return x, weight\n\tdef prepare_fp8_weight_matmul(graph, x, weight):\n\t    return x, weight\n", "def matmul_identity(graph, x, weight):\n\t    return ops.matmul(graph, x, weight)\n\tdef matmul_int4(graph, x, weight):\n\t    return matmul_identity(graph, x, weight)\n\tdef matmul_fp8(graph, x, weight):\n\t    scale = weight + '_scale'\n\t    if scale in REGISTRY.get('main_graph').getInputTensorIds():\n\t        return ops.fp8_matmul(graph, x, weight, scale, scale, 'F143', 'F143')\n\t    return ops.matmul(graph, x, weight)\n\tdef post_process_float32_16_matmul(graph, y):\n", "    return y\n\tdef post_process_int4_matmul(graph, y):\n\t    return y\n\tdef post_process_fp8_matmul(graph, y):\n\t    return y\n\tPrecisionStrategyMap = {\n\t    'fp16': {\n\t        'weight_fn': weight_fn_identity,\n\t        'prepare_matmul': prepare_float32_16_matmul,\n\t        'matmul_fn': matmul_identity,\n", "        'post_process_matmul': post_process_float32_16_matmul},\n\t    'fp32': {\n\t        'weight_fn': weight_fn_identity,\n\t        'prepare_matmul': prepare_float32_16_matmul,\n\t        'matmul_fn': matmul_identity,\n\t        'post_process_matmul': post_process_float32_16_matmul},\n\t    'int4': {\n\t        'weight_fn': weight_fn_int4,\n\t        'prepare_matmul': prepare_int4_matmul,\n\t        'matmul_fn': matmul_int4,\n", "        'post_process_matmul': post_process_int4_matmul},\n\t    'fp8': {\n\t        'weight_fn': weight_fn_fp8,\n\t        'prepare_matmul': prepare_fp8_matmul,\n\t        'matmul_fn': matmul_fp8,\n\t        'post_process_matmul': post_process_fp8_matmul},\n\t    'fp8_weight': {\n\t        'weight_fn': weight_fn_fp8,\n\t        'prepare_matmul': prepare_fp8_weight_matmul,\n\t        'matmul_fn': matmul_fp8,\n", "        'post_process_matmul': post_process_fp8_matmul}\n\t}\n"]}
{"filename": "poptransformer/utils/param_handler/tensor_parallel_strategy.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom math import ceil\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.utils.registry import REGISTRY\n\tdef shard(param: np.ndarray, num_replicas: int, axis: int) -> np.array:\n\t    \"\"\"Shard array along a given axis\"\"\"\n\t    if axis < 0:\n\t        axis = len(param.shape) + axis\n", "    if param.shape[axis] % num_replicas != 0:\n\t        pads = [(0, 0) for i in range(len(param.shape))]\n\t        pads[axis] = (0, num_replicas - param.shape[axis] % num_replicas)\n\t        param = np.pad(param, pads)\n\t    return np.ascontiguousarray(np.concatenate(np.split(param[np.newaxis, ...], num_replicas, axis=axis + 1)))\n\tdef repeat(param: np.ndarray, num_replicas: int, axis: int = 0) -> np.array:\n\t    \"\"\"Repeat array along new axis inserted at position `axis`\"\"\"\n\t    return np.repeat(np.expand_dims(param, axis), num_replicas, axis=axis)\n\tdef build_sharded_weight(param, num_replicas, vocab_size, embedding_size):\n\t    shard_size = ceil(vocab_size / num_replicas)\n", "    n_pad = shard_size * num_replicas - param.shape[0]\n\t    param = np.pad(param, ((0, n_pad), (0, 0)))\n\t    param = param.reshape(num_replicas, shard_size, embedding_size)\n\t    param = np.pad(param, ((0, 0), (0, 1), (0, 0)))\n\t    return param\n\tdef shard_fused_qkv(param, num_replicas, axis):\n\t    q_split, k_split, v_split = [\n\t        np.split(part, num_replicas, axis=axis) for part in np.split(param, 3, axis=axis)\n\t    ]\n\t    sharded_param = np.concatenate(\n", "        [np.concatenate([q_split[i], k_split[i], v_split[i]], axis=axis)[np.newaxis, ...]\n\t            for i in range(num_replicas)]\n\t    )\n\t    sharded_param = np.ascontiguousarray(sharded_param)\n\t    return sharded_param\n\tdef shard_multi_query_qkv(param, n_shards, axis):\n\t    q_size = REGISTRY.get(\"query_size\")\n\t    kv_size = REGISTRY.get(\"key_value_size\")\n\t    q_split, k_split, v_split = [\n\t        np.split(part, n_shards, axis=axis) for part in np.split(param, [q_size, q_size+kv_size], axis=axis)\n", "    ]\n\t    sharded_param = np.concatenate(\n\t        [np.concatenate([q_split[i], k_split[i], v_split[i]], axis=axis)[np.newaxis, ...]\n\t            for i in range(n_shards)]\n\t    )\n\t    sharded_param = np.ascontiguousarray(sharded_param)\n\t    return sharded_param\n\tdef identity(param, num_replicas, axis):\n\t    return param\n\tdef identity_prepare_matmul(graph, x, weight):\n", "    return x, weight\n\tdef identity_post_process_matmul(graph, y):\n\t    return y\n\tTPStragetgyMap = {\n\t    'start': {\n\t        'weight_fn': shard,\n\t        'weight_axis': -1,\n\t        'bias_fn': shard,\n\t        'prepare_matmul': identity_prepare_matmul,\n\t        'post_process_matmul': identity_post_process_matmul},\n", "    'end': {\n\t        'weight_fn': shard,\n\t        'weight_axis': 0,\n\t        'bias_fn': repeat,\n\t        'prepare_matmul': identity_prepare_matmul,\n\t        'post_process_matmul': ops.replicated_all_reduce},\n\t    'fused_qkv': {\n\t        'weight_fn': shard_fused_qkv,\n\t        'weight_axis': -1,\n\t        'bias_fn': shard_fused_qkv,\n", "        'prepare_matmul': identity_prepare_matmul,\n\t        'post_process_matmul': identity_post_process_matmul},\n\t    'multi_query_qkv': {\n\t        'weight_fn': shard_multi_query_qkv,\n\t        'weight_axis': -1,\n\t        'bias_fn': shard_multi_query_qkv,\n\t        'prepare_matmul': identity_prepare_matmul,\n\t        'post_process_matmul': identity_post_process_matmul},\n\t    'identity': {\n\t        'weight_fn': identity,\n", "        'weight_axis': 0,\n\t        'bias_fn': identity,\n\t        'prepare_matmul': identity_prepare_matmul,\n\t        'post_process_matmul': identity_post_process_matmul},\n\t}\n"]}
{"filename": "poptransformer/utils/param_handler/__init__.py", "chunked_list": []}
{"filename": "poptransformer/ops/customized.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport ctypes\n\timport numpy as np\n\timport poprt\n\tso_path = '../../custom_ops.so'\n\tctypes.cdll.LoadLibrary(so_path)\n\tdef kv_cache(graph, step, updater, max_len, sequence_axis=1, step_len=1):\n\t    output = graph.customOp(\n\t        inputs=[step, updater],\n", "        opName=\"KVCache\",\n\t        domain=\"ai.graphcore\",\n\t        opVersion=1,\n\t        numOutputs=1,\n\t        attributes={\n\t            'max_len': max_len,\n\t            'step_len':  step_len,\n\t            'sequence_axis': sequence_axis,\n\t            'fp8': False,\n\t        },\n", "    )[0]\n\t    return output\n\tdef remap_tensor(graph, x, fwd_after_matmul=0, name='remap'):\n\t    output = graph.customOp(\n\t        opName='Remap',\n\t        domain='ai.graphcore',\n\t        opVersion=1,\n\t        inputs=[x],\n\t        attributes={\n\t            'fwd_grain_size': 8,\n", "            'bwd_grain_size': 8,\n\t            'fwd_clone_layout': 0,\n\t            'bwd_clone_layout': 0,\n\t            'fwd_after_matmul': fwd_after_matmul,\n\t            'bwd_after_matmul': 0,\n\t            'debug_str': name\n\t        }\n\t    )[0]\n\t    return output\n\tdef softmax_ce(graph, x, dim, stable_mode=True):\n", "    output = graph.customOp(\n\t        opName=\"SoftmaxV3\",\n\t        domain=\"ai.graphcore\",\n\t        opVersion=1,\n\t        inputs=[x],\n\t        attributes={'axis': dim, 'stable_mode': stable_mode}\n\t    )[0]\n\t    return output\n\t# TODO: NormCE -> FastNorm\n\tdef layer_norm_ce(graph, x, gamma, beta, eps, batch_size, sequence_length, input_size):\n", "    return _layer_norm_ce(graph, x, gamma, beta, batch_size, sequence_length, input_size, eps)\n\tdef _layer_norm_ce(\n\t        graph, x, weight, bias, batch_size, sequence_length, input_size, epsilon,\n\t        fwd_grain_size=-1, bwd_grain_size=-1, fwd_after_matmul=0, bwd_after_matmul=0,\n\t        stable_algo=False, name='layer_norm_ce'):\n\t    with graph.nameScope(name):\n\t        data_size = np.prod([batch_size, sequence_length, input_size])\n\t        num_groups = int(data_size // input_size)\n\t        assert input_size & 1 == 0\n\t        assert input_size < 8192\n", "        fwd_grain_size = input_size if fwd_grain_size != -1 else fwd_grain_size\n\t        bwd_grain_size = input_size if bwd_grain_size != -1 else bwd_grain_size\n\t        output = graph.customOp(\n\t            opName=\"NormCE\",\n\t            domain=\"ai.graphcore\",\n\t            opVersion=1,\n\t            inputs=[x, weight, bias],\n\t            attributes={\n\t                'fwd_after_matmul': 1 if fwd_after_matmul else 0,\n\t                'bwd_after_matmul': 1 if bwd_after_matmul else 0,\n", "                'fwd_grain_size': fwd_grain_size,\n\t                'bwd_grain_size': bwd_grain_size,\n\t                'epsilon': epsilon,\n\t                'num_groups': num_groups,\n\t                'stable_algo': stable_algo,\n\t                'debug_str': graph.getNameScope() + 'op'\n\t            }\n\t        )[0]\n\t    return output\n\tdef replicated_allgather(graph, matmul_output):\n", "    output = graph.customOp(\n\t        opName=\"ReplicatedAllGather\",\n\t        opVersion=1,\n\t        domain=\"ai.graphcore\",\n\t        inputs=[matmul_output],\n\t        attributes={})[0]  # shape is 1 dim\n\t    return output\n\tdef int4_to_half(graph, x, scale, ref, axis=1, remap=1):\n\t    x = graph.customOp(\n\t            inputs=[x, scale, ref],\n", "            opName=\"Int4ToHalf\",\n\t            domain=\"ai.graphcore\",\n\t            opVersion=1,\n\t            attributes={\n\t                \"axis\": axis,\n\t                \"remap\": remap},\n\t        )[0]\n\t    return x\n\tdef half_to_uint8(graph, x, fp8_scale, fp8_format='F143'):\n\t    output = graph.customOp(\n", "        opName=\"CastToUint8WithoutShapeInfer\",\n\t        opVersion=1,\n\t        domain=\"ai.graphcore\",\n\t        inputs=[x, fp8_scale],\n\t        attributes={\"fp8Format\": fp8_format},\n\t    )[0]\n\t    return output\n\tdef fp8_matmul(graph, input_l, input_r, fp8_scale_lhs, fp8_scale_rhs, fp8_format_lhs='F143', fp8_format_rhs='F143'):\n\t    output = graph.customOp(\n\t        opName=\"FP8MatMulWithoutShapeInfer\",\n", "        opVersion=1,\n\t        domain=\"ai.graphcore\",\n\t        inputs=[input_l, input_r, fp8_scale_lhs, fp8_scale_rhs],\n\t        attributes={\n\t            \"fp8FormatLHS\": fp8_format_lhs,\n\t            \"fp8FormatRHS\": fp8_format_rhs,\n\t        },\n\t    )[0]\n\t    return output\n"]}
{"filename": "poptransformer/ops/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom .popart import *\n\tfrom .customized import *\n"]}
{"filename": "poptransformer/ops/popart.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport numpy as np\n\tfrom poptransformer.ops.customized import int4_to_half\n\tfrom poptransformer.utils.registry import REGISTRY\n\tdef pad(graph, x, pad_list, mode='constant', constant_value=0.0):\n\t    return graph.aiOnnx.pad([x], pads=pad_list, mode=mode, value=constant_value)\n\tdef randomnormal(graph, shape, dtype, scale):\n\t    return graph.aiOnnx.randomnormal(shape=shape, dtype=dtype, scale=scale)\n\tdef random_sample(graph, logits, k, dim=1):\n", "    _, next_token = topk(graph, logits, axis=dim, k=k)\n\t    rand = randomnormal(graph, [1, k], 1, 1)\n\t    rand = argmax(graph, rand, 1)\n\t    return dynamic_slice(graph, next_token, rand, axes=[dim], sizes=[1])\n\tdef add(graph, x, y):\n\t    return graph.aiOnnx.add([x, y])\n\tdef less(graph, x, y):\n\t    return graph.aiOnnx.less([x, y])\n\tdef greater(graph, x, y):\n\t    return graph.aiOnnx.greater([x, y])\n", "def sum_op(graph, inputs):\n\t    return graph.aiOnnx.sum(inputs)\n\tdef reduce_sum(graph, x, axes, keepdims):\n\t    return graph.aiOnnx.reducesum([x], axes=axes, keepdims=keepdims)\n\tdef bitwiseor(graph, x, y):\n\t    return graph.aiGraphcore.bitwiseor([x, y])\n\tdef sub(graph, x, y):\n\t    return graph.aiOnnx.sub([x, y])\n\tdef div(graph, x, y):\n\t    return graph.aiOnnx.div([x, y])\n", "def matmul(graph, x, y):\n\t    o = graph.aiOnnx.matmul([x, y])\n\t    amp = REGISTRY.get('amp')\n\t    partialtype = REGISTRY.get('partialtype')\n\t    serial_factor = REGISTRY.get('serial_factor')\n\t    serial_mode = REGISTRY.get('serial_mode')\n\t    if amp is not None:\n\t        graph.setAvailableMemoryProportion(o, amp)\n\t    if partialtype is not None:\n\t        graph.setPartialsType(o, partialtype)\n", "    if serial_factor is not None:\n\t        graph.setSerializeMatMul({o}, mode=serial_mode, factor=serial_factor)\n\t    return o\n\tdef constant(graph, tensor, tensor_name='constant'):\n\t    return graph.aiOnnx.constant(tensor, debugContext=tensor_name)\n\tdef _group_norm(graph, x, gamma, beta, eps):\n\t    return graph.aiGraphcore.groupnormalization([x, gamma, beta], 1, eps)[0]\n\tdef group_norm(graph, x, gamma, beta, eps, batch_size, sequence_length, input_size):\n\t    x = reshape(graph, x, [batch_size * sequence_length, input_size])\n\t    x = _group_norm(graph, x, gamma, beta, eps)\n", "    x = reshape(graph, x, [batch_size, sequence_length, input_size])\n\t    return x\n\tdef gather(graph, weight, input_ids):\n\t    return graph.aiOnnx.gather([weight, input_ids])\n\tdef grouped_gather(graph, weight, indexs, axis=0, group_size=1):\n\t    return graph.aiGraphcore.groupedgather([weight, indexs], axis=axis, group_size=group_size)\n\tdef mul(graph, x, y):\n\t    return graph.aiOnnx.mul([x, y])\n\tdef tanh(graph, x):\n\t    return graph.aiOnnx.tanh([x])\n", "def split(graph, x, num_outputs, axis, splits, name='split'):\n\t    return graph.aiOnnx.split([x], num_outputs=num_outputs, axis=axis, split=splits, debugContext=name)\n\tdef transpose(graph, x, perm):\n\t    return graph.aiOnnx.transpose([x], perm=perm)\n\tdef reshape(graph, x, shape):\n\t    shape = constant(graph, np.asarray(shape, dtype=np.int32))\n\t    return graph.aiOnnx.reshape([x, shape])\n\tdef static_slice(graph, x, starts, ends, axes):\n\t    return graph.aiGraphcore.slice([x], starts=starts, ends=ends, axes=axes)\n\tdef dynamic_slice(graph, x, index, axes, sizes):\n", "    return graph.aiGraphcore.dynamicslice([x, index], axes=axes, sizes=sizes)\n\tdef dynamic_update(graph, x, index, slice_tensor, axes, sizes):\n\t    return graph.aiGraphcore.dynamicupdate([x, index, slice_tensor], axes=axes, sizes=sizes)\n\tdef cast(graph, x, popart_float_type):\n\t    return graph.aiOnnx.cast([x], popart_float_type)\n\tdef unsqueeze(graph, x, dim_list):\n\t    return graph.aiOnnx.unsqueeze([x], dim_list)\n\tdef squeeze(graph, x, dim_list):\n\t    return graph.aiOnnx.squeeze([x], dim_list)\n\tdef equal(graph, x, y):\n", "    return graph.aiOnnx.equal([x, y])\n\tdef where(graph, cond, x1, x2):\n\t    return graph.aiOnnx.where([cond, x1, x2])\n\tdef softmax(graph, x, dim, stable_mode=None):\n\t    if stable_mode:\n\t        raise ValueError('set use unstable softmax in session, or use type ce ')\n\t    return graph.aiOnnx.softmax([x], dim)\n\tdef argmax(graph, x, axis, keepdims=True):\n\t    return graph.aiOnnx.argmax([x], axis=axis, keepdims=keepdims)\n\tdef topk(graph, x, axis, k):\n", "    k = constant(graph, np.array(k).astype(np.int64), 'k')\n\t    return graph.aiOnnx.topk([x, k], axis=axis)\n\tdef printtensor(graph, x, title):\n\t    return graph.aiGraphcore.printtensor([x], title=title)\n\tdef gelu(graph, x):\n\t    return graph.aiGraphcore.gelu([x])\n\tdef concat(graph, x, y, axis):\n\t    return graph.aiOnnx.concat([x, y], axis=axis)\n\tdef concat_sequence(graph, inputs, axis):\n\t    return graph.aiOnnx.concat(inputs, axis=axis)\n", "def log_softmax(graph, x, axis=-1):\n\t    return graph.aiOnnx.logsoftmax([x], axis)\n\tdef clip(graph, x, clip_min=10, clip_max=100):\n\t    output = graph.aiOnnx.clip([x], max=clip_max, min=clip_min)\n\t    return output\n\tdef expand(graph, x, shape_list):\n\t    expand_shape = constant(graph, np.array(\n\t        shape_list, dtype=np.int32), 'shape')\n\t    return graph.aiOnnx.expand([x, expand_shape])\n\tdef bitwiseand(graph, x, y):\n", "    return graph.aiGraphcore.bitwiseand([x, y])\n\tdef tile(graph, x, repeats):\n\t    repeats = constant(graph, np.array(repeats, dtype=np.int64), 'repeats')\n\t    return graph.aiOnnx.tile([x, repeats], 'tile')\n\tdef loop(graph, max_loop_num, loop_graph_input_list, loop_graph):\n\t    max_loop = constant(graph, np.array(max_loop_num).astype(np.int32), 'max_loop')\n\t    init_cond = constant(graph, np.array(True).astype(np.bool_), 'cond')\n\t    loop_outputs = graph.aiOnnx.loop(\n\t        [max_loop, init_cond] + loop_graph_input_list,\n\t        len(loop_graph_input_list),\n", "        loop_graph,\n\t    )\n\t    return loop_outputs\n\tdef call_sub_graph(graph, input_list, sub_graph):\n\t    return graph.aiGraphcore.call(input_list, 1, sub_graph)\n\tdef replicated_all_reduce(graph, x):\n\t    return graph.aiGraphcore.replicatedallreduce([x])\n\tdef conv(graph, x, weight, bias, kernel_shape, strides, pads, dilations, group):\n\t    args = [x, weight] if bias is None else [x, weight, bias]\n\t    o = graph.aiOnnx.conv(\n", "        args=args,\n\t        kernel_shape=kernel_shape,\n\t        strides=strides,\n\t        pads=pads,\n\t        dilations=dilations,\n\t        group=group\n\t    )\n\t    amp = REGISTRY.get('amp')\n\t    partialtype = REGISTRY.get('partialtype')\n\t    if amp is not None:\n", "        graph.setAvailableMemoryProportion(o, amp)\n\t    if partialtype is not None:\n\t        graph.setPartialsType(o, partialtype)\n\t    return o\n\tdef relu(graph, x):\n\t    return graph.aiOnnx.relu([x])\n\tdef sigmoid(graph, x):\n\t    return graph.aiOnnx.sigmoid([x])\n\tdef exp(graph, x):\n\t    return graph.aiOnnx.exp([x])\n", "def maximum(graph, x, y):\n\t    cond = greater(graph, x, y)\n\t    output = where(graph, cond, x, y)\n\t    return output\n\tdef swish(graph, x):\n\t    return graph.aiGraphcore.swish([x])\n\tdef mean(graph, x):\n\t    return graph.aiOnnx.mean([x])\n\tdef sqrt(graph, x):\n\t    return graph.aiOnnx.sqrt([x])\n", "def reciprocal(graph, x):\n\t    return graph.aiOnnx.reciprocal([x])\n\tdef reducemean(graph, x):\n\t    return graph.aiOnnx.reducemean([x],axes=[-1])\n"]}
{"filename": "poptransformer/layers/conv.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom .base_layer import BaseLayer\n\tclass BaseConv1d(BaseLayer):\n\t    def __init__(\n\t        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.output_size = output_size\n", "        self.kernels = [kernels]\n\t        self.strides = [strides]\n\t        self.pads = [pads] * 2 if isinstance(pads, int) else pads\n\t        self.dilations = [dilations]\n\t        self.groups = groups\n\t        self.bias = bias\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        weight_key = '.'.join([self.context, 'weight'])\n\t        weight_shape = [self.output_size, self.input_size // self.groups] + self.kernels\n", "        weight_np = self.get_param_from_state_dict(weight_key, weight_shape)\n\t        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\t        if self.bias:\n\t            bias_key = '.'.join([self.context, 'bias'])\n\t            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n\t            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\t    def __call__(self, graph, x):\n\t        x = ops.conv(\n\t            graph=graph,\n\t            x=x,\n", "            weight=self.weight_id,\n\t            bias=self.bias_id if self.bias else None,\n\t            kernel_shape=self.kernels,\n\t            strides=self.strides,\n\t            pads=self.pads,\n\t            dilations=self.dilations,\n\t            group=self.groups\n\t        )\n\t        return x\n\tclass TPConv1d(BaseConv1d):\n", "    pass\n\tclass BaseConv2d(BaseLayer):\n\t    def __init__(\n\t        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.output_size = output_size\n\t        self.kernels = [kernels] * 2 if isinstance(kernels, int) else kernels\n\t        self.strides = [strides] * 2 if isinstance(strides, int) else strides\n\t        self.pads = [pads] * 4 if isinstance(pads, int) else pads\n", "        self.dilations = [dilations] * 2 if isinstance(dilations, int) else dilations\n\t        self.groups = groups\n\t        self.bias = bias\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        weight_key = '.'.join([self.context, 'weight'])\n\t        weight_shape = [self.output_size, self.input_size // self.groups] + self.kernels\n\t        weight_np = self.get_param_from_state_dict(weight_key, weight_shape)\n\t        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\t        if self.bias:\n", "            bias_key = '.'.join([self.context, 'bias'])\n\t            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n\t            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\t    def __call__(self, graph, x):\n\t        x = ops.conv(\n\t            graph=graph,\n\t            x=x,\n\t            weight=self.weight_id,\n\t            bias=self.bias_id if self.bias else None,\n\t            kernel_shape=self.kernels,\n", "            strides=self.strides,\n\t            pads=self.pads,\n\t            dilations=self.dilations,\n\t            group=self.groups\n\t        )\n\t        return x\n\tclass TPConv2d(BaseConv2d):\n\t    pass\n\tclass Conv1d(TPConv1d, BaseConv1d):\n\t    layer_class_map = {\n", "        'tp': TPConv1d,\n\t        'shard': BaseConv1d\n\t    }\n\t    def __init__(\n\t        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n", "        super().__init__(context, name, input_size, output_size, kernels, strides, pads, dilations, groups, bias)\n\t    def __call__(self, graph, x):\n\t        return self.layer_class.__call__(self, graph, x)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n\tclass Conv2d(TPConv2d, BaseConv2d):\n\t    layer_class_map = {\n\t        'tp': TPConv2d,\n\t        'shard': BaseConv2d\n\t    }\n", "    def __init__(\n\t        self, context, name, input_size, output_size, kernels=1, strides=1, pads=0, dilations=1, groups=1, bias=True):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, input_size, output_size, kernels, strides, pads, dilations, groups, bias)\n\t    def __call__(self, graph, x):\n\t        return self.layer_class.__call__(self, graph, x)\n", "    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/layers/layer_norm.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom .base_layer import BaseLayer\n\tclass BaseLayerNorm(BaseLayer):\n\t    norm_fn_map = {'group': ops.group_norm, 'ce': ops.layer_norm_ce}\n\t    def __init__(self, context, name, input_size, eps):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.eps = eps\n", "        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        weight_key = '.'.join([self.context, 'weight'])\n\t        weight_np = self.get_param_from_state_dict(weight_key, [self.input_size])\n\t        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\t        bias_key = '.'.join([self.context, 'bias'])\n\t        bias_np = self.get_param_from_state_dict(bias_key, [self.input_size])\n\t        self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\t    def __call__(self, graph, x, sequence_length, norm_type):\n\t        with graph.nameScope(self.context):\n", "            norm_fn = self.norm_fn_map.get(norm_type, None)\n\t            if not norm_fn:\n\t                raise ValueError(f\"Invalid norm_fn {norm_type}, options: {self.norm_fn_map.keys()}\")\n\t            x = norm_fn(\n\t                graph, x, self.weight_id, self.bias_id, self.eps, self.batch_size, sequence_length, self.input_size)\n\t            return x\n\tclass TPLayerNorm(BaseLayerNorm):\n\t    pass\n\tclass LayerNorm(TPLayerNorm, BaseLayerNorm):\n\t    layer_class_map = {\n", "        'tp': TPLayerNorm,\n\t        'shard': BaseLayerNorm}\n\t    def __init__(self, context, name, input_size, eps):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, input_size, eps)\n\t    def __call__(self, graph, x, sequence_length, norm_type):\n", "        return self.layer_class.__call__(self, graph, x, sequence_length, norm_type)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/layers/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport math\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.utils import build_sharded_weight\n\tfrom .base_layer import BaseLayer\n\tclass BaseEmbedding(BaseLayer):\n\t    def __init__(self, context, name, vocab_size, embed_size):\n\t        super().__init__(context, name)\n", "        self.vocab_size = vocab_size\n\t        self.embed_size = embed_size\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        weight_key = '.'.join([self.context, 'weight'])\n\t        weight_np = self.get_param_from_state_dict(weight_key, [self.vocab_size, self.embed_size])\n\t        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            return ops.gather(graph, self.weight_id, input_ids)\n", "class TPEmbedding(BaseEmbedding):\n\t    def collect_bind_layer_weights(self):\n\t        self.vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n\t        self.vocab_per_ipu = math.ceil(self.vocab_size / self.num_replicas)\n\t        weight_key = '.'.join([self.context, 'weight'])\n\t        weight_np = self.get_param_from_state_dict(weight_key, [self.vocab_size, self.embed_size])\n\t        weight_np = build_sharded_weight(weight_np, self.num_replicas, self.vocab_size, self.embed_size)\n\t        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key, **self.vs_setting)\n\t        index_offset_np = np.expand_dims(np.arange(self.num_replicas, dtype=np.int32), [1, 2]) * self.vocab_per_ipu\n\t        self.index_offset = self.add_initialized_input_tensor(index_offset_np, 'index_offset', **self.vs_setting)\n", "    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            replicated_input_ids = ops.sub(graph, input_ids, self.index_offset)\n\t            cond1 = ops.less(graph, replicated_input_ids, ops.constant(graph, np.array([0], dtype=np.int32)))\n\t            cond2 = ops.greater(\n\t                graph, replicated_input_ids, ops.constant(graph, np.array([self.vocab_per_ipu], dtype=np.int32)))\n\t            cond = ops.bitwiseor(graph, ops.cast(graph, cond1, 'INT32'), ops.cast(graph, cond2, 'INT32'))\n\t            cond = ops.cast(graph, cond, 'BOOL')\n\t            fill_value_np = self.vocab_per_ipu * np.ones(\n\t                (self.num_replicas, self.batch_size, sequence_length), dtype=np.int32)\n", "            fill_value = self.add_initialized_input_tensor(fill_value_np, 'fill_value', **self.vs_setting)\n\t            updated_replicated_input_ids = ops.where(graph, cond, fill_value, replicated_input_ids)\n\t            output = ops.gather(graph, self.weight_id, updated_replicated_input_ids)\n\t            return output\n\tclass Embedding(TPEmbedding, BaseEmbedding):\n\t    layer_class_map = {\n\t        'tp': TPEmbedding,\n\t        'shard': BaseEmbedding}\n\t    def __init__(self, context, name, vocab_size, embed_size):\n\t        model_type = self.model_type\n", "        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, vocab_size, embed_size)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/layers/mlp.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom .base_layer import BaseLayer\n\tfrom .linear import Linear\n\tclass BaseMLP(BaseLayer):\n\t    act_fn_map = {'gelu': ops.gelu}\n\t    def __init__(self, context, name, input_size, hidden_size, act_fn):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n", "        self.hidden_size = hidden_size\n\t        self.act_fn = self.act_fn_map.get(act_fn, None)\n\t        if not self.act_fn:\n\t            raise ValueError(f\"Invalid act_fn {act_fn}, options: {self.act_fn_map.keys()}\")\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.c_fc = Linear(self.context, 'c_fc', self.input_size, self.hidden_size)\n\t        self.c_proj = Linear(self.context, 'c_proj', self.hidden_size, self.input_size)\n\t    def __call__(self, graph, x):\n\t        with graph.nameScope(self.context):\n", "            x = ops.reshape(graph, x, [-1, self.input_size])\n\t            x = self.c_fc(graph, x)\n\t            x = self.act_fn(graph, x)\n\t            x = self.c_proj(graph, x)\n\t            x = ops.reshape(graph, x, [self.batch_size, -1, self.input_size])\n\t            return x\n\tclass TPMLP(BaseMLP):\n\t    def collect_bind_layer_weights(self):\n\t        fc_tp_setting = {\n\t            'strategy_name': 'start',\n", "        }\n\t        self.c_fc = Linear(self.context, 'c_fc', self.input_size, self.hidden_size, **fc_tp_setting)\n\t        proj_tp_setting = {\n\t            'strategy_name': 'end',\n\t        }\n\t        self.c_proj = Linear(self.context, 'c_proj', self.hidden_size, self.input_size, **proj_tp_setting)\n\tclass MLP(TPMLP, BaseMLP):\n\t    layer_class_map = {\n\t        'tp': TPMLP,\n\t        'shard': BaseMLP}\n", "    def __init__(self, context, name, input_size, hidden_size, act_fn):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, input_size, hidden_size, act_fn)\n\t    def __call__(self, graph, x):\n\t        return self.layer_class.__call__(self, graph, x)\n\t    def collect_bind_layer_weights(self):\n", "        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/layers/lm_head.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport math\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom .linear import Linear\n\tfrom .base_layer import BaseLayer\n\tclass BaseLMHead(BaseLayer):\n\t    def __init__(self, context, name, topk, vocab_size, embedding_size, tie_weight=None):\n\t        super().__init__(context, name)\n", "        self.topk = topk\n\t        self.vocab_size = vocab_size\n\t        self.embedding_size = embedding_size\n\t        self.virtual_id = None\n\t        self.use_tied_embedding = False\n\t        self.tie_weight = tie_weight\n\t        self.collect_bind_layer_weights()\n\t    def tie_embedding(self, weight_id):\n\t        self.head.weight_id = weight_id\n\t        self.use_tied_embedding = True\n", "        self.logger.info(f'setting weight id to {weight_id}')\n\t    def set_virtual_id(self, virtual_id):\n\t        self.virtual_id = virtual_id\n\t        self.logger.info(f'setting virtual id to {virtual_id}')\n\t    def collect_bind_layer_weights(self):\n\t        self.head = Linear(self.context, None, self.embedding_size, self.vocab_size, False)\n\t        if self.tie_weight is not None:\n\t            self.tie_embedding(self.tie_weight)\n\t    def __call__(self, graph, logits, sequence_length):\n\t        with graph.virtualGraph(self.virtual_id):\n", "            if self.use_tied_embedding:\n\t                self.head.weight_id = ops.transpose(graph, self.head.weight_id, [1, 0])\n\t            logits = self.head(graph, logits)\n\t            if self.topk != 1:\n\t                next_token = ops.random_sample(graph, logits, k=self.topk, dim=2)\n\t            else:\n\t                _, next_token = ops.topk(graph, logits, axis=2, k=1)\n\t            next_token = ops.squeeze(graph, next_token, [2])\n\t            return next_token\n\tclass TPLMHead(BaseLMHead):\n", "    def collect_bind_layer_weights(self):\n\t        if not self.tie_weight:\n\t            lm_tp_settings = {\n\t                'strategy_name': 'start',\n\t            }\n\t        else:\n\t            lm_tp_settings = {\n\t                'strategy_name': 'identity',\n\t            }\n\t        self.head = Linear(self.context, None, self.embedding_size, self.vocab_size, False, **lm_tp_settings)\n", "        if self.tie_weight:\n\t            self.tie_embedding(self.tie_weight)\n\t    def __call__(self, graph, logits, sequence_length):\n\t        with graph.virtualGraph(self.virtual_id):\n\t            vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n\t            vocab_per_ipu = math.ceil(self.vocab_size / self.num_replicas)\n\t            index_offset_np = np.expand_dims(np.arange(self.num_replicas, dtype=np.int32), [1, 2]) * vocab_per_ipu\n\t            index_offset = self.add_initialized_input_tensor(index_offset_np, 'index_offset', **vs_setting)\n\t            if self.use_tied_embedding:\n\t                self.head.weight_id = ops.transpose(graph, self.head.weight_id, [1, 0])\n", "                self.use_tied_embedding = False # Avoid to transpose twice in 2 stage mode.\n\t            logits = self.head(graph, logits)\n\t            pad_idx = ops.equal(graph, ops.constant(graph, np.array(0.0).astype(self.np_float_type), 'zero'), logits)\n\t            logits = ops.where(\n\t                graph, pad_idx, ops.constant(graph, np.array(-10000.0).astype(self.np_float_type), '-10000'), logits)\n\t            next_token_prob, next_token_ = ops.topk(graph, logits, axis=2, k=self.topk)\n\t            next_token = ops.add(graph, next_token_, index_offset)\n\t            next_token_prob = ops.replicated_allgather(graph, next_token_prob)\n\t            next_token_topk = ops.replicated_allgather(graph, next_token)\n\t            next_token_prob_shape = [sequence_length * self.topk * self.num_replicas, self.batch_size]\n", "            next_token_prob = ops.transpose(\n\t                graph,\n\t                ops.reshape(graph, next_token_prob, next_token_prob_shape),\n\t                [1, 0]\n\t            )\n\t            next_token_topk_shape = [sequence_length * self.topk * self.num_replicas, self.batch_size]\n\t            next_token_topk = ops.transpose(\n\t                graph,\n\t                ops.reshape(graph, next_token_topk, next_token_topk_shape),\n\t                [1, 0]\n", "            )\n\t            next_token_prob = ops.reshape(\n\t                graph, next_token_prob, [self.batch_size, sequence_length, self.topk * self.num_replicas])\n\t            next_token_topk = ops.reshape(\n\t                graph, next_token_topk, [self.batch_size, sequence_length, self.topk * self.num_replicas])\n\t            next_token_idx = ops.argmax(graph, next_token_prob, axis=2)  # [B,1,1]\n\t            next_token_idx = ops.squeeze(graph, next_token_idx, [1, 2])  # (B,)\n\t            next_token_topk = ops.squeeze(graph, next_token_topk, [1])  # [B,topk * num_replica]\n\t            next_token = ops.grouped_gather(\n\t                graph, next_token_topk, next_token_idx, axis=1, group_size=self.batch_size)  # (B,)\n", "            next_token = ops.reshape(graph, next_token, [self.batch_size, -1])\n\t            return next_token\n\tclass LMHead(TPLMHead, BaseLMHead):\n\t    layer_class_map = {\n\t        'tp': TPLMHead,\n\t        'shard': BaseLMHead}\n\t    def __init__(self, context, name, topk, vocab_size, embedding_size, tie_weight=None):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n", "            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, topk, vocab_size, embedding_size, tie_weight)\n\t    def __call__(self, graph, logits, sequence_length):\n\t        return self.layer_class.__call__(self, graph, logits, sequence_length)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/layers/base_layer.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom abc import abstractmethod\n\timport numpy as np\n\tfrom popart import VariableRetrievalMode, VariableSettings, CommGroupType, CommGroup\n\tfrom poptransformer.utils import REGISTRY, DeviceScope, OptionsScope\n\tclass BaseLayer:\n\t    # class attributes which can be used by child classes\n\t    logger = REGISTRY.get('logger')\n\t    vs_type_map = {\n", "        'consecutive': CommGroupType.Consecutive,\n\t        'all': CommGroupType.All,\n\t        'orthogonal': CommGroupType.Orthogonal}\n\t    retrieval_mode_map = {\n\t        'one_per_group': VariableRetrievalMode.OnePerGroup,\n\t        'all_replicas': VariableRetrievalMode.AllReplicas}\n\t    def __init__(self, context, name):\n\t        self.context = '.'.join([context, name]) if (context and name) else context or name\n\t        self.name = name\n\t    @property\n", "    def model_type(self):\n\t        return REGISTRY.get('model_type')\n\t    @property\n\t    def batch_per_step(self):\n\t        return REGISTRY.get('batch_per_step')\n\t    @property\n\t    def batch_size(self):\n\t        return REGISTRY.get('batch_size')\n\t    @property\n\t    def num_replicas(self):\n", "        return REGISTRY.get('num_replicas')\n\t    @property\n\t    def main_graph(self):\n\t        return REGISTRY.get('main_graph')\n\t    @property\n\t    def state_dict(self):\n\t        return REGISTRY.get('state_dict')\n\t    @property\n\t    def popart_float_type(self):\n\t        return REGISTRY.get('tensor_type').popart_float_type\n", "    @property\n\t    def np_float_type(self):\n\t        return REGISTRY.get('tensor_type').np_float_type\n\t    @property\n\t    def precision(self):\n\t        return REGISTRY.get('tensor_type').precision\n\t    @property\n\t    def enable_pipeline(self):\n\t        return REGISTRY.get('enable_pipeline')\n\t    def option_scope(self, amp=None, partialtype=None, serial_factor=None, serial_mode=None):\n", "        return OptionsScope(amp, partialtype, serial_factor, serial_mode)\n\t    def device_scope(self, graph, virtual_graph_id=None, pipeline_stage_id=None, outline_attr=None):\n\t        return DeviceScope(graph, virtual_graph_id, pipeline_stage_id, self.enable_pipeline, outline_attr)\n\t    @abstractmethod\n\t    def __call__(self, graph, *args):\n\t        # build the graph with weights / layers built from the collect_bind_layer_weights\n\t        pass\n\t    @abstractmethod\n\t    def collect_bind_layer_weights(self):\n\t        # build/process the weight / layers\n", "        # should be exectute in the __init__ fn, we may have to find a way to exectute it automatically\n\t        pass\n\t    def build_variable_setting(self, vs_type='consecutive', group_size=1, retrieval_mode='one_per_group'):\n\t        vs_type = self.vs_type_map.get(vs_type, None)\n\t        retrieval_mode = self.retrieval_mode_map.get(retrieval_mode, None)\n\t        assert vs_type, f\"Invalid vs_type: {vs_type}\"\n\t        assert retrieval_mode, f\"Invalid retrieval_mode: {retrieval_mode}\"\n\t        variableSettings = VariableSettings(\n\t            CommGroup(vs_type, replicaGroupSize=group_size),\n\t            retrieval_mode\n", "        )\n\t        return variableSettings\n\t    def add_initialized_input_tensor(self, weight_np, weight_key, **variable_setting):\n\t        if variable_setting:\n\t            variable_setting = self.build_variable_setting(**variable_setting)\n\t            weight_id = self.main_graph.addInitializedInputTensor(weight_np, variable_setting, weight_key)\n\t        else:\n\t            weight_id = self.main_graph.addInitializedInputTensor(weight_np, debugContext=weight_key)\n\t        return weight_id\n\t    def get_param_from_state_dict(self, weight_key, shape_list):\n", "        weight_np = self.state_dict.get(weight_key, None)\n\t        if weight_np is None:\n\t            self.logger.info(f'{weight_key} not found, using random tensor')\n\t            assert shape_list, f'no shape provided for random initializing the tensor {weight_key}'\n\t            weight_np = np.random.randn(*shape_list).astype(dtype=self.np_float_type)\n\t        else:\n\t            weight_np = weight_np.numpy()\n\t            if shape_list:\n\t                self.logger.info(f\"loading {weight_key} with shape {weight_np.shape}, dtype {weight_np.dtype}.\")\n\t                if self.precision not in ['int4', 'fp8', 'fp8_weight']:\n", "                    assert sum(\n\t                        (abs(a - b) for a, b in zip(weight_np.shape, shape_list))\n\t                        ) == 0, f'{weight_key} shape {weight_np.shape} not matched with provided {shape_list}'\n\t            else:\n\t                self.logger.warning(f'shape not provided or using int4/fp8 weight, skip shape check for {weight_key}')\n\t        return weight_np\n"]}
{"filename": "poptransformer/layers/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom .base_layer import BaseLayer\n\tfrom .lm_head import LMHead\n\tfrom .linear import Linear\n\tfrom .mlp import MLP\n\tfrom .embedding import Embedding\n\tfrom .layer_norm import LayerNorm\n\tfrom .conv import Conv2d\n\tfrom .conv import Conv1d\n"]}
{"filename": "poptransformer/layers/linear.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom poptransformer.utils import ParamHandler\n\tfrom .base_layer import BaseLayer\n\tclass BaseLinear(BaseLayer):\n\t    def __init__(self, context, name, input_size, output_size, use_bias=True, **kwargs):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.output_size = output_size\n", "        self.use_bias = use_bias\n\t        self.kwargs = kwargs\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.param_handler = ParamHandler(host_layer=self)\n\t        weight_key = '.'.join([self.context, 'weight'])\n\t        weight_np = self.get_param_from_state_dict(weight_key, [self.output_size, self.input_size])\n\t        weight_np = weight_np.transpose(1, 0)\n\t        weight_np = self.param_handler.process_linear_weight(weight_np, weight_key)\n\t        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n", "        if self.use_bias:\n\t            bias_key = '.'.join([self.context, 'bias'])\n\t            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n\t            bias_np = self.param_handler.process_linear_bias(bias_np)\n\t            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key)\n\t    def __call__(self, graph, x):\n\t        with graph.nameScope(self.context):\n\t            x = self.param_handler.matmul(graph, x, self.weight_id)\n\t            x = ops.add(graph, x, self.bias_id) if self.use_bias else x\n\t        return x\n", "class TPLinear(BaseLinear):\n\t    def collect_bind_layer_weights(self):\n\t        vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n\t        self.param_handler = ParamHandler(\n\t            host_layer=self,\n\t            tp_strategy_name=self.kwargs.get('strategy_name'),\n\t            **vs_setting\n\t        )\n\t        weight_key = '.'.join([self.context, 'weight'])\n\t        weight_np = self.get_param_from_state_dict(weight_key, [self.output_size, self.input_size])\n", "        weight_np = weight_np.transpose(1, 0)\n\t        weight_np = self.param_handler.process_linear_weight(weight_np, weight_key)\n\t        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key, **vs_setting)\n\t        if self.use_bias:\n\t            bias_key = '.'.join([self.context, 'bias'])\n\t            bias_np = self.get_param_from_state_dict(bias_key, [self.output_size])\n\t            bias_np = self.param_handler.process_linear_bias(bias_np)\n\t            self.bias_id = self.add_initialized_input_tensor(bias_np, bias_key, **vs_setting)\n\tclass Linear(TPLinear, BaseLinear):\n\t    layer_class_map = {\n", "        'tp': TPLinear,\n\t        'shard': BaseLinear\n\t    }\n\t    def __init__(self, context, name, input_size, output_size, use_bias=True, **kwargs):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, input_size, output_size, use_bias, **kwargs)\n", "    def __call__(self, graph, x):\n\t        return self.layer_class.__call__(self, graph, x)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/layers/rms_layer_norm.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers.layer_norm import BaseLayerNorm\n\tclass BaseRMSLayerNorm(BaseLayerNorm):\n\t    def collect_bind_layer_weights(self):\n\t        weight_key = '.'.join([self.context, 'weight'])\n\t        weight_np = self.get_param_from_state_dict(weight_key, [self.input_size])\n\t        self.weight_id = self.add_initialized_input_tensor(weight_np, weight_key)\n", "    def __call__(self, graph, x):\n\t        variance_epsilon = ops.constant(graph, np.array(self.eps).astype(np.float32), 'variance_epsilon')\n\t        variance = ops.cast(graph, x, 'FLOAT')\n\t        variance = ops.mul(graph, variance, variance)\n\t        variance = ops.reducemean(graph, variance)\n\t        variance = ops.add(graph, variance, variance_epsilon)\n\t        variance = ops.sqrt(graph, variance)\n\t        variance = ops.reciprocal(graph, variance)\n\t        variance = ops.cast(graph, variance, self.popart_float_type)\n\t        x = ops.mul(graph, x, variance)\n", "        return ops.mul(graph, x, self.weight_id)\n\tclass TPRMSLayerNorm(BaseRMSLayerNorm):\n\t    pass\n\tclass RMSLayerNorm(TPRMSLayerNorm, BaseRMSLayerNorm):\n\t    layer_class_map = {\n\t        'tp': TPRMSLayerNorm,\n\t        'shard': BaseRMSLayerNorm}\n\t    def __init__(self, context, name, input_size, eps):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n", "        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, input_size, eps)\n\t    def __call__(self, graph, x):\n\t        return self.layer_class.__call__(self, graph, x)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom .base_model import HFDecBaseModel, HFDec2stageBaseModel\n\tfrom .gpt2.model import GPT2DecModel\n\tfrom .chatglm.model import ChatGLMDecModel\n\tfrom .rwkv.model import RWKVDecodeModel\n\tfrom .chatglm2.model import ChatGLM2DecModel\n\tfrom .llama2.model import LLAMA2DecModel\n"]}
{"filename": "poptransformer/models/base_model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom abc import abstractmethod\n\tfrom collections import OrderedDict\n\tfrom transformers import AutoTokenizer\n\tfrom transformers import AutoConfig\n\tfrom transformers import AutoModel\n\tfrom transformers import AutoModelForCausalLM\n\timport numpy as np\n\timport popart\n", "from poptransformer.utils import REGISTRY, DeviceScope, OptionsScope\n\tfrom poptransformer import ops\n\tclass BaseModel:\n\t    graph = REGISTRY.get('main_graph')\n\t    logger = REGISTRY.get('logger')\n\t    tensor_type = REGISTRY.get('tensor_type')\n\t    def __init__(self, **kwargs):\n\t        self.logger.info(f'Initializing model class: {self.__class__.__name__}')\n\t        self.anchor_return_type = kwargs.get('anchor_return_type', 'ALL')\n\t        self.layer_per_ipu = kwargs.get('layer_per_ipu', [])\n", "    @property\n\t    def enable_pipeline(self):\n\t        return REGISTRY.get('enable_pipeline')\n\t    @property\n\t    def batch_size(self):\n\t        return REGISTRY.get('batch_size')\n\t    @property\n\t    def batch_per_step(self):\n\t        return REGISTRY.get('batch_per_step')\n\t    @property\n", "    def model_type(self):\n\t        return REGISTRY.get('model_type')\n\t    @property\n\t    def num_replicas(self):\n\t        return REGISTRY.get('num_replicas')\n\t    @property\n\t    def stage_num(self):\n\t        return max(len(self.layer_per_ipu), 1)\n\t    @property\n\t    def popart_float_type(self):\n", "        return REGISTRY.get('tensor_type').popart_float_type\n\t    @property\n\t    def np_float_type(self):\n\t        return REGISTRY.get('tensor_type').np_float_type\n\t    @property\n\t    def precision(self):\n\t        return REGISTRY.get('tensor_type').precision\n\t    @abstractmethod\n\t    def prepare_state_dict(self):\n\t        # build self.state_dict\n", "        # then it will be registed to REGISTER for layers usage\n\t        pass\n\t    @abstractmethod\n\t    def build_graph(self):\n\t        # build model's graph\n\t        pass\n\t    @abstractmethod\n\t    def build_input_dict(self, **kwargs):\n\t        # process input, build dict,\n\t        # will be wrapped with stepio and feed to graph later\n", "        pass\n\t    @abstractmethod\n\t    def build_output_dict(self, anchor_arrays):\n\t        # process outputs in session.anchor_arrays,\n\t        # return a dict for layer usage\n\t        pass\n\t    def device_scope(self, graph, virtual_graph_id=None, pipeline_stage_id=None, outline_attr=None):\n\t        return DeviceScope(graph, virtual_graph_id, pipeline_stage_id, self.enable_pipeline, outline_attr)\n\t    def option_scope(self, amp=None, partialtype=None, serial_factor=None, serial_mode=None):\n\t        return OptionsScope(amp, partialtype, serial_factor, serial_mode)\n", "    def register_state_dict(self):\n\t        # register the state dict to REGISTER, will be used in layer\n\t        assert self.state_dict\n\t        REGISTRY.update('state_dict', self.state_dict)\n\t    @property\n\t    def initializers(self):\n\t        # all weights' id added into the graph\n\t        return [tensor_id for tensor_id in self.graph.getInputTensorIds()\n\t                if self.graph.isInitializer(tensor_id)]\n\t    @property\n", "    def model_output(self):\n\t        output_tensor_ids = self.get_output_tensor_ids(self.graph)\n\t        anchor_return_type = popart.AnchorReturnType(self.anchor_return_type)\n\t        return {key: anchor_return_type for key in output_tensor_ids}\n\t    @property\n\t    def model_proto(self):\n\t        return self.graph.getModelProto()\n\t    def add_input_tensor(self, graph, popart_dtype, shape, name):\n\t        if self.model_type in ['shard']:\n\t            return graph.addInputTensor(\n", "                popart.TensorInfo(popart_dtype, shape), debugContext=name)\n\t        if self.model_type == 'tp':\n\t            input_setting = popart.InputSettings(\n\t                popart.ReplicatedStreamMode.Broadcast)\n\t            return graph.addInputTensor(\n\t                popart.TensorInfo(popart_dtype, shape), input_setting, debugContext=name)\n\t        raise ValueError(f\"Invalid model_type: {self.model_type}\")\n\t    def add_input_tensor_from_parent_graph(self, graph, tensor_id):\n\t        if isinstance(tensor_id, str):\n\t            graph.addInputTensorFromParentGraph(tensor_id)\n", "        elif isinstance(tensor_id, list):\n\t            for i in tensor_id:\n\t                self.add_input_tensor_from_parent_graph(graph, i)\n\t        else:\n\t            raise ValueError(f\"Invalid tensor_id: {tensor_id}\")\n\t    def add_untyped_input_tensor(self, graph, tensor_id):\n\t        if isinstance(tensor_id, list):\n\t            all_tensor_ids = []\n\t            for i in tensor_id:\n\t                all_tensor_ids.append(graph.addUntypedInputTensor(i))\n", "            return all_tensor_ids\n\t        if isinstance(tensor_id, str):\n\t            return graph.addUntypedInputTensor(tensor_id)\n\t        raise ValueError(f\"Invalid tensor_id: {tensor_id}\")\n\t    def add_output_tensor(self, graph, output):\n\t        if isinstance(output, list):\n\t            for i in output:\n\t                graph.addOutputTensor(i)\n\t        elif isinstance(output, str):\n\t            graph.addOutputTensor(output)\n", "        else:\n\t            raise ValueError(f\"Invalid output: {output}\")\n\t    def get_input_tensor_ids(self, graph):\n\t        return [tensor_id for tensor_id in graph.getInputTensorIds()\n\t                if not graph.isInitializer(tensor_id)]\n\t    def get_output_tensor_ids(self, graph):\n\t        return graph.getOutputTensorIds()\n\t    def create_sub_graph(self, graph, name, sub_graph_inputs, is_loop_graph=False):\n\t        sub_graph = graph.createSubgraphBuilder()\n\t        sub_graph.setGraphName(name)\n", "        if is_loop_graph:\n\t            self.add_input_tensor(sub_graph, 'INT32', [], 'max_loop')\n\t            self.add_input_tensor(sub_graph, 'BOOL', [], 'cond')\n\t        self.add_input_tensor_from_parent_graph(sub_graph, sub_graph_inputs)\n\t        return sub_graph\n\tclass HFBaseModel(BaseModel):\n\t    \"\"\"\n\t    hugggingface base model,\n\t    with the process fn for loading hf model, config and tokenizer\n\t    \"\"\"\n", "    hf_model_class_name_map = {\n\t        'auto_model': AutoModel,\n\t        'auto_model_for_causallm': AutoModelForCausalLM}\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n\t        self.hf_model_name = kwargs.get('hf_model_name', None)\n\t        self.hf_model_class_name = kwargs.get('hf_model_class_name', None)\n\t        self.override_hfconfig_from_json = kwargs.get('override_hfconfig_from_json', None)\n\t        self.hf_cache_dir = kwargs.get('hf_cache_dir', './temp/')\n\t        self.prepare_state_dict()\n", "    def process_hf_model_state_dict(self):\n\t        self.logger.info('no prescale process on hf state dict')\n\t    def prepare_state_dict(self):\n\t        assert self.hf_model_name, f\"Invalid hf_model_name: {self.hf_model_name}\"\n\t        self.hf_tokenizer = AutoTokenizer.from_pretrained(\n\t            self.hf_model_name,\n\t            cache_dir=self.hf_cache_dir,\n\t            pad_token='[PAD]'\n\t        )\n\t        self.logger.info(f'initialized tokenizer by model_name: {self.hf_model_name}')\n", "        if not self.override_hfconfig_from_json:\n\t            model_class = self.hf_model_class_name_map.get(self.hf_model_class_name, None)\n\t            assert model_class, f\"Invalid hf_model_class_name: {self.hf_model_class_name}\"\n\t            self.logger.info(f'initializing hf model class: {model_class.__name__}')\n\t            self.hf_model = model_class.from_pretrained(self.hf_model_name, cache_dir=self.hf_cache_dir)\n\t            self.logger.info(f'loading pretrained hf model: {self.hf_model_name}')\n\t            self.hf_config = self.hf_model.config\n\t            self.process_hf_model_state_dict()\n\t            if self.precision != 'fp32':\n\t                self.hf_model.half()\n", "                self.logger.info(f'casting model to {self.precision}')\n\t            self.state_dict = self.hf_model.state_dict()\n\t        else:\n\t            self.logger.info('using overrided config, no state dict loaded')\n\t            self.hf_config = AutoConfig.from_pretrained(self.override_hfconfig_from_json)\n\t            self.state_dict = {}    # No states if using your own model configurations.\n\t        self.register_state_dict()\n\tclass HFDecBaseModel(HFBaseModel):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n", "        self.early_stop = kwargs.get('early_stop', False)\n\t        self.max_length = kwargs.get('max_length', 128)\n\t        max_loop = kwargs.get('max_loop', None)\n\t        self.max_loop = max_loop if max_loop else self.max_length\n\t    @abstractmethod\n\t    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n\t        # TODO: Add params docstring.\n\t        # return model graph\n\t        pass\n\t    def build_default_inputs(self):\n", "        # build input tensor here, no need to feed to step io if not used.\n\t        default_inputs = OrderedDict()\n\t        default_inputs['input_ids_container'] = self.add_input_tensor(\n\t            self.graph, 'INT32', [self.batch_size, self.max_length], 'input_ids')\n\t        default_inputs['step'] = self.add_input_tensor(\n\t            self.graph, 'INT32', [], 'step')\n\t        default_inputs['attention_mask'] = self.add_input_tensor(\n\t            self.graph, 'INT32', [self.batch_size, self.max_length], 'attention_mask')\n\t        default_inputs['position_ids'] = self.add_input_tensor(\n\t            self.graph, 'INT32', [self.batch_size, self.max_length], 'position_ids')\n", "        default_inputs['stop_mask'] = self.add_input_tensor(\n\t            self.graph, 'INT32', [], 'stop_mask')\n\t        return default_inputs\n\t    def build_model_graph_inputs(self, graph, inputs):\n\t        with graph.nameScope('mask'):\n\t            attention_mask_value = [[0] + [-10000 for i in range(self.max_length - 1)]] * self.batch_size\n\t            attention_mask_value = np.array(attention_mask_value).astype(self.np_float_type)\n\t            attention_mask = ops.constant(\n\t                graph, attention_mask_value, 'attention_mask')\n\t            inputs['attention_mask'] = attention_mask\n", "        with graph.nameScope('step'):\n\t            inputs['step'] = ops.constant(self.graph, np.array(0).astype(np.int32), 'init_step')\n\t        with graph.nameScope('stop_mask'):\n\t            inputs['stop_mask'] = ops.constant(\n\t                self.graph, np.zeros((self.batch_size,)).astype(np.int32), 'stop_mask')\n\t        del inputs['position_ids']\n\t        return inputs\n\t    def build_graph(self):\n\t        default_inputs = self.build_default_inputs()\n\t        with self.graph.virtualGraph(0):\n", "            model_graph_inputs = self.build_model_graph_inputs(self.graph, default_inputs)\n\t        model_graph = self.create_sub_graph(\n\t            self.graph,\n\t            'model_graph',\n\t            list(model_graph_inputs.values()),\n\t            True\n\t        )\n\t        model_outputs = self.build_model_graph(model_graph, model_graph_inputs, sequence_length=1)\n\t        self.build_post_model_graph(model_graph, model_graph_inputs, model_outputs)\n\t        with self.device_scope(self.graph, 0):\n", "            outputs = ops.loop(\n\t                self.graph,\n\t                self.max_loop,\n\t                list(model_graph_inputs.values()),\n\t                model_graph\n\t            )[:2]\n\t        self.add_output_tensor(self.graph, outputs)\n\t    def build_post_model_graph(self, model_graph, model_graph_inputs, model_outputs):\n\t        # continue build model graph for post inference step\n\t        stage_offset = model_outputs['stage_offset']\n", "        next_ids = model_outputs['next_ids']\n\t        attention_mask = model_graph_inputs['attention_mask']\n\t        step = model_graph_inputs['step']\n\t        input_ids_container = model_graph_inputs['input_ids_container']\n\t        stop_mask = model_graph_inputs['stop_mask']\n\t        with self.device_scope(model_graph, 0, pipeline_stage_id=stage_offset):\n\t            next_iput_ids_container, next_step, next_attention_mask, id_to_update= self.step_containers(\n\t                model_graph, input_ids_container, step, attention_mask, next_ids\n\t            )\n\t            next_stop_mask, keep_going_cond = self.step_loop_cond(model_graph, id_to_update, stop_mask)\n", "        self.add_output_tensor(\n\t            model_graph,\n\t            [keep_going_cond, next_iput_ids_container, next_step, next_attention_mask, next_stop_mask])\n\t        # build model graph for post inference step\n\t    def step_containers(self, graph, input_ids_container, step, attention_mask, next_ids):\n\t        with graph.nameScope('step_containers'):\n\t            if self.hf_tokenizer.pad_token_id:\n\t                pad_token_id = self.hf_tokenizer.pad_token_id\n\t            else:\n\t                pad_token_id = self.hf_config.pad_token_id\n", "            step_add_value = ops.constant(graph, np.array(1).astype(np.int32), '1')\n\t            next_step = ops.add(graph, step, step_add_value)\n\t            if attention_mask is not None:\n\t                attention_mask_add = ops.constant(\n\t                    graph, np.zeros((self.batch_size, 1), dtype=self.np_float_type), 'attention_mask_add')\n\t                next_attention_mask = ops.dynamic_update(\n\t                    graph, attention_mask, next_step, attention_mask_add, axes=[1], sizes=[1])\n\t            else:\n\t                next_attention_mask = None\n\t            input_ids_slice = ops.dynamic_slice(\n", "                graph, input_ids_container, next_step, axes=[1], sizes=[1])\n\t            pad_id = ops.constant(graph, np.array(pad_token_id), 'pad_id')\n\t            id_update_cond = ops.equal(graph, input_ids_slice, pad_id)\n\t            id_to_update = ops.where(graph, id_update_cond, next_ids, input_ids_slice)\n\t            next_iput_ids_container = ops.dynamic_update(\n\t                graph, input_ids_container, next_step, id_to_update, axes=[1], sizes=[1])\n\t        return next_iput_ids_container, next_step, next_attention_mask, id_to_update\n\t    def step_loop_cond(self, graph, id_to_update, stop_mask):\n\t        with graph.nameScope('step_loop_cond'):\n\t            if self.hf_tokenizer.eos_token_id:\n", "                eos_token_id = self.hf_tokenizer.eos_token_id\n\t            else:\n\t                eos_token_id = self.hf_config.eos_token_id\n\t            temp_id_to_update = ops.squeeze(graph, id_to_update, [1])\n\t            eos_id = ops.constant(graph, np.array(eos_token_id), 'eos_id')\n\t            current_stop_mask = ops.equal(graph, temp_id_to_update, eos_id)\n\t            current_stop_mask = ops.cast(graph, current_stop_mask, 'INT32')\n\t            next_stop_mask = ops.bitwiseor(graph, stop_mask, current_stop_mask)\n\t            if self.early_stop:\n\t                mask = ops.reduce_sum(graph, stop_mask, axes=[0], keepdims=False)\n", "                batch_size_constant = ops.constant(\n\t                    graph, np.array(self.batch_size).astype(np.int32), 'batch_size')\n\t                keep_going_cond = ops.less(graph, mask, batch_size_constant)\n\t            else:\n\t                keep_going_cond = ops.constant(graph, np.array(True).astype(np.bool_), 'cond')\n\t        return next_stop_mask, keep_going_cond\n\tclass HFDec2stageBaseModel(HFDecBaseModel):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n\t        self.input_length = kwargs.get('input_length', 32)\n", "        max_loop = kwargs.get('max_loop', None)\n\t        self.max_loop = max_loop if max_loop else self.max_length - self.input_length\n\t    @abstractmethod\n\t    def build_model_graph_1st(\n\t        self,\n\t        input_ids_container,\n\t        step,\n\t        attention_mask,\n\t        position_ids,\n\t        init_past_list,\n", "        sequence_length,\n\t    ):\n\t        # return model graph\n\t        pass\n\t    @abstractmethod\n\t    def build_model_graph_2nd(\n\t        self,\n\t        input_ids_container,\n\t        step,\n\t        attention_mask,\n", "        stop_mask,\n\t        position_ids,\n\t        layer_past_list,\n\t        sequence_length,\n\t    ):\n\t        # return model graph\n\t        pass\n\t    @abstractmethod\n\t    def build_positions_ids(self, position_ids_container):\n\t        pass\n", "    def build_graph(self):\n\t        input_ids_container = self.add_input_tensor(\n\t            self.graph, 'INT32', [self.batch_size, self.max_length], 'input_ids')\n\t        position_ids_container = self.add_input_tensor(\n\t            self.graph, 'INT32', [self.batch_size, self.max_length], 'position_ids')\n\t        attention_mask = self.add_input_tensor(\n\t            self.graph, 'INT32', [self.batch_size, self.max_length], 'attention_mask')\n\t        step = ops.constant(self.graph, np.array(0).astype(np.int32), 'init_step')\n\t        stop_mask = ops.constant(\n\t            self.graph, np.zeros((self.batch_size,)).astype(np.int32), 'stop_mask')\n", "        cache_shape = (\n\t            2,\n\t            self.batch_size,\n\t            self.hf_config.num_attention_heads // self.num_replicas,\n\t            self.max_length,\n\t            self.hf_config.hidden_size // self.hf_config.num_attention_heads,\n\t        )\n\t        init_past_list = [\n\t            ops.constant(\n\t                self.graph,\n", "                np.zeros(cache_shape).astype(self.np_float_type),\n\t                f\"init_past_{str(i)}\",\n\t            )\n\t            for i in range(self.hf_config.num_layers)\n\t        ]\n\t        with self.graph.virtualGraph(0):\n\t            position_ids_stage_1 = ops.static_slice(\n\t                self.graph, position_ids_container, [0], [self.input_length], [1]\n\t            )\n\t            attention_mask = ops.sub(\n", "                self.graph,\n\t                attention_mask,\n\t                ops.constant(self.graph, np.ones(1, dtype=np.int32), '1'),\n\t            )\n\t            attention_mask = ops.cast(self.graph, attention_mask, self.popart_float_type)\n\t            attention_mask = ops.mul(\n\t                self.graph,\n\t                attention_mask,\n\t                ops.constant(self.graph, np.ones(1, dtype=self.np_float_type)*10000, '10000')\n\t            )\n", "            attention_mask = ops.unsqueeze(self.graph, attention_mask, [1, 2])\n\t            attention_mask_stage_1 = ops.static_slice(\n\t                self.graph,\n\t                attention_mask,\n\t                starts=[0],\n\t                ends=[self.input_length],\n\t                axes=[3],\n\t            )\n\t        input_ids_container, step, _, layer_present_list = self.build_model_graph_1st(\n\t            input_ids_container,\n", "            step,\n\t            attention_mask_stage_1,\n\t            position_ids_stage_1,\n\t            init_past_list,\n\t            self.input_length\n\t        )\n\t        with self.graph.virtualGraph(0):\n\t            position_ids_stage_2 = ops.static_slice(\n\t                self.graph,\n\t                position_ids_container,\n", "                [self.input_length - 1],\n\t                [self.input_length],\n\t                [1],\n\t            )\n\t            attention_mask_updater = ops.constant(\n\t                self.graph,\n\t                np.zeros((self.batch_size, 1, 1, 1), dtype=self.np_float_type),\n\t                \"attention_mask_updater\",\n\t            )\n\t            # next_attention_mask = [0] + attention_mask_container[:-1]\n", "            attention_mask = ops.static_slice(\n\t                self.graph, attention_mask, starts=[0], ends=[-1], axes=[3]\n\t            )\n\t            attention_mask_stage_2 = ops.concat(\n\t                self.graph, attention_mask_updater, attention_mask, axis=3\n\t            )\n\t        stage2_graph = self.build_model_graph_2nd(\n\t            input_ids_container,\n\t            step,\n\t            attention_mask_stage_2,\n", "            stop_mask,\n\t            position_ids_stage_2,\n\t            layer_present_list,\n\t            1\n\t        )\n\t        with self.device_scope(self.graph, 0, 0):\n\t            output_id, step = ops.loop(\n\t                self.graph,\n\t                self.max_loop,\n\t                [input_ids_container,\n", "                 step, attention_mask_stage_2,\n\t                 stop_mask,\n\t                 position_ids_stage_2] + layer_present_list,\n\t                stage2_graph\n\t            )[:2]\n\t        self.add_output_tensor(self.graph, [output_id, step])\n"]}
{"filename": "poptransformer/models/chatglm/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport re\n\timport math\n\timport torch\n\timport numpy as np\n\tfrom transformers import AutoTokenizer, AutoConfig, AutoModel\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import LayerNorm, BaseLayer, MLP\n\tfrom poptransformer.models import HFDec2stageBaseModel\n", "from poptransformer.models.chatglm.embedding import ChatGLMEmbedding\n\tfrom poptransformer.models.chatglm.attention import RotaryAttention\n\tfrom poptransformer.models.chatglm.lm_head import LMHead\n\tdef vocab_size_calibration(vocab_size,\n\t                           num_embedding_splits=1,\n\t                           num_lmhead_splits=2):\n\t    r1 = math.ceil(vocab_size / num_embedding_splits)\n\t    r2 = math.ceil((r1 + 2) / num_lmhead_splits)\n\t    corrected_vocab_size = (r2 * num_lmhead_splits - 2) * num_embedding_splits\n\t    print(f\"Vocab size adjust from {vocab_size} to {corrected_vocab_size}.\")\n", "    return corrected_vocab_size\n\tclass ChatGLMBlock(BaseLayer):\n\t    def __init__(\n\t        self,\n\t        context,\n\t        name,\n\t        input_size,\n\t        eps,\n\t        num_attention_heads,\n\t        num_layers,\n", "        max_length,\n\t        layer_index,\n\t    ):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.eps = eps\n\t        self.num_attention_heads = num_attention_heads\n\t        self.max_length = max_length\n\t        self.num_layers = num_layers\n\t        self.layer_index = layer_index\n", "        self.rotary_dim = input_size // (num_attention_heads * 2)\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.layer_norm1 = LayerNorm(\n\t            self.context, \"input_layernorm\", self.input_size, self.eps\n\t        )\n\t        self.attention = RotaryAttention(\n\t            self.context,\n\t            \"attention\",\n\t            self.input_size,\n", "            self.num_attention_heads,\n\t            self.max_length,\n\t            self.layer_index,\n\t            self.rotary_dim,\n\t        )\n\t        self.layer_norm2 = LayerNorm(\n\t            self.context, \"post_attention_layernorm\", self.input_size, self.eps\n\t        )\n\t        self.mlp = MLP(\n\t            self.context, \"mlp\", self.input_size, self.input_size * 4, \"gelu\"\n", "        )\n\t    def __call__(\n\t        self,\n\t        graph,\n\t        x,\n\t        layer_past,\n\t        position_ids,\n\t        block_position_ids,\n\t        sequence_length,\n\t        step,\n", "        attention_mask,\n\t        norm_type=\"ce\",\n\t        softmax_type=\"ce\",\n\t        **kwargs,\n\t    ):\n\t        matmul_kwargs = {\n\t            \"amp\": 0.2,\n\t            \"partialtype\": \"half\" if sequence_length > 1 else \"float\",\n\t        }\n\t        with graph.nameScope(self.context):\n", "            attention_input = self.layer_norm1(\n\t                graph, x, sequence_length, norm_type)\n\t            with self.option_scope(**matmul_kwargs):\n\t                attention_output, layer_present = self.attention(\n\t                    graph,\n\t                    attention_input,\n\t                    layer_past,\n\t                    position_ids,\n\t                    block_position_ids,\n\t                    step,\n", "                    attention_mask,\n\t                    sequence_length,\n\t                    softmax_type,\n\t                )\n\t            alpha = ops.constant(graph, np.array([(2.0 * self.num_layers) ** 0.5], dtype=self.np_float_type))\n\t            temp_x = ops.add(graph, ops.mul(graph, attention_input, alpha), attention_output)\n\t            mlp_input = self.layer_norm2(graph, temp_x, sequence_length, norm_type)\n\t            with self.option_scope(**matmul_kwargs):\n\t                mlp_output = self.mlp(graph, mlp_input)\n\t            output = ops.add(graph, ops.mul(graph, mlp_input, alpha), mlp_output)\n", "        return output, layer_present\n\tclass Transformer(BaseLayer):\n\t    def __init__(\n\t        self,\n\t        context,\n\t        name,\n\t        vocab_size,\n\t        hidden_size,\n\t        eps,\n\t        num_attention_heads,\n", "        max_length,\n\t        num_layers,\n\t        layer_per_ipu,\n\t        max_position,\n\t        num_embedding_partitions=4,\n\t    ):\n\t        super().__init__(context, name)\n\t        self.vocab_size = vocab_size\n\t        self.hidden_size = hidden_size\n\t        self.num_embedding_partitions = num_embedding_partitions\n", "        self.eps = eps\n\t        self.num_attention_heads = num_attention_heads\n\t        self.max_length = max_length\n\t        self.num_layers = num_layers\n\t        self.layer_per_ipu = layer_per_ipu\n\t        self.max_position = max_position\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.embedding = ChatGLMEmbedding(\n\t            self.context,\n", "            None,\n\t            self.vocab_size,\n\t            self.hidden_size,\n\t            self.num_embedding_partitions,\n\t        )\n\t        self.blocks = [\n\t            ChatGLMBlock(\n\t                self.context,\n\t                \"layers.\" + str(layer_index),\n\t                self.hidden_size,\n", "                self.eps,\n\t                self.num_attention_heads,\n\t                self.num_layers,\n\t                self.max_length,\n\t                layer_index,\n\t            )\n\t            for layer_index in range(self.num_layers)\n\t        ]\n\t        self.layer_norm = LayerNorm(self.context, \"final_layernorm\", self.hidden_size, self.eps)\n\t    def __call__(\n", "        self,\n\t        graph,\n\t        input_ids,\n\t        layer_past_list,\n\t        position_ids,\n\t        block_position_ids,\n\t        step,\n\t        attention_mask,\n\t        sequence_length,\n\t        **kwargs,\n", "    ):\n\t        norm_type = kwargs.get(\"norm_type\", \"ce\")\n\t        softmax_type = kwargs.get(\"softmax_type\", \"ce\")\n\t        return_last = kwargs.get(\"return_last\", True)\n\t        outline_blocks = kwargs.get(\"outline_blocks\", \"single_block\")\n\t        if outline_blocks:\n\t            self.logger.info(\"outlining transformer blocks\")\n\t            self.logger.info(\"please make sure disable outlining in session options is set to False\")\n\t        with graph.virtualGraph(0):\n\t            hidden_states = self.embedding(graph, input_ids, sequence_length)\n", "            hidden_states = ops.replicated_all_reduce(graph, hidden_states)\n\t        end_points = np.cumsum(self.layer_per_ipu)\n\t        layer_present_list = []\n\t        for i in range(self.num_layers):\n\t            stage_offset = sum(i >= end_points)\n\t            if outline_blocks is None:\n\t                outline_attr = None\n\t            elif outline_blocks == 'single_block':\n\t                outline_attr = {'block': f'sub_{i}'}\n\t            elif outline_blocks == 'multi_block':\n", "                outline_attr = {'block': f'sub_{stage_offset}'}\n\t            else:\n\t                raise ValueError(\n\t                    f'invalid value {outline_blocks} for outline_blocks')\n\t            with self.device_scope(graph, stage_offset, stage_offset, outline_attr):\n\t                hidden_states, present = self.blocks[i](\n\t                    graph,\n\t                    hidden_states,\n\t                    layer_past_list[i],\n\t                    position_ids,\n", "                    block_position_ids,\n\t                    sequence_length,\n\t                    step,\n\t                    attention_mask,\n\t                    norm_type,\n\t                    softmax_type,\n\t                )\n\t            self.logger.info(f\"block {i} placed on IPU {stage_offset}\")\n\t            layer_present_list.append(present)\n\t        with graph.virtualGraph(stage_offset):\n", "            hidden_states = self.layer_norm(graph, hidden_states, sequence_length, norm_type)\n\t            if return_last:\n\t                hidden_states = ops.static_slice(\n\t                    graph, hidden_states, [sequence_length - 1], [sequence_length], [1]\n\t                )\n\t                hidden_states = ops.squeeze(graph, hidden_states, [1])\n\t        return hidden_states, layer_present_list\n\tclass ChatGLMDecModel(HFDec2stageBaseModel):\n\t    def __init__(self, **kwargs):\n\t        self.num_embedding_partitions = kwargs.get(\"num_embedding_partitions\", 4)\n", "        super().__init__(**kwargs)\n\t        self.outline_blocks = kwargs.get(\"outline_blocks\", True)\n\t        self.transformer = Transformer(\n\t            None,\n\t            \"transformer\",\n\t            self.hf_config.vocab_size,\n\t            self.hf_config.hidden_size,\n\t            self.hf_config.layernorm_epsilon,\n\t            self.hf_config.num_attention_heads,\n\t            self.max_length,\n", "            self.hf_config.num_layers,\n\t            self.layer_per_ipu,\n\t            self.hf_config.max_sequence_length,\n\t            self.num_embedding_partitions,\n\t        )\n\t        self.topk = 1\n\t        self.lm_head = LMHead(\n\t            context=\"\",\n\t            name='lm_head',\n\t            vocab_size=self.hf_config.vocab_size,\n", "            topk=self.topk,\n\t            embedding_size=self.hf_config.hidden_size,\n\t            embedding_weights=self.transformer.embedding.weight_ids,\n\t            num_embedding_partitions=self.num_embedding_partitions,\n\t            token_offsets=self.transformer.embedding.token_offsets,\n\t            embedding_pad_mask=self.transformer.embedding.embedding_pad_mask,\n\t        )\n\t        self.lm_head.set_virtual_id(0)\n\t    def prepare_state_dict(self):\n\t        hf_args = {\n", "            \"pretrained_model_name_or_path\": self.hf_model_name,\n\t            \"trust_remote_code\": True,\n\t            \"revision\": \"v1.1.0\" if self.precision != \"int4\" else \"v0.1.0\",\n\t            \"cache_dir\": self.hf_cache_dir,\n\t        }\n\t        self.hf_tokenizer = AutoTokenizer.from_pretrained(**hf_args)\n\t        self.logger.info(\n\t            f\"initialized tokenizer by model_name: {self.hf_model_name}\")\n\t        self.hf_config = AutoConfig.from_pretrained(**hf_args)\n\t        if self.precision == \"int4\":\n", "            if self.model_type == \"shard\":\n\t                self.hf_config.vocab_size = vocab_size_calibration(\n\t                                                self.hf_config.vocab_size,\n\t                                                num_embedding_splits=self.num_embedding_partitions,\n\t                                                num_lmhead_splits=6)\n\t        self.hf_config.num_layers = sum(self.layer_per_ipu)\n\t        self.logger.info(f\"loading pretrained hf model: {self.hf_model_name}\")\n\t        self.hf_model = AutoModel.from_pretrained(**hf_args).half().eval()\n\t        self.state_dict = self.hf_model.state_dict()\n\t        tensor_names = list(self.state_dict.keys())\n", "        for k in tensor_names:\n\t            if \"dense_h_to_4h\" in k:\n\t                self.state_dict[k.replace(\"dense_h_to_4h\", \"c_fc\")] = self.state_dict.pop(k)\n\t            if \"dense_4h_to_h\" in k:\n\t                self.state_dict[k.replace(\"dense_4h_to_h\", \"c_proj\")] = self.state_dict.pop(k)\n\t        self.register_state_dict()\n\t    def build_model_graph_1st(\n\t        self,\n\t        input_ids_container,\n\t        step,\n", "        attention_mask,\n\t        position_ids,\n\t        init_past_list,\n\t        sequence_length=1,\n\t    ):\n\t        model_graph = self.graph\n\t        with self.device_scope(model_graph, virtual_graph_id=0):\n\t            input_ids = ops.static_slice(\n\t                model_graph,\n\t                input_ids_container,\n", "                starts=[0],\n\t                ends=[sequence_length],\n\t                axes=[1],\n\t            )\n\t            # stage 1: block_position_ids = [0, 0, ..., 0]\n\t            block_position_ids = ops.constant(\n\t                model_graph,\n\t                np.zeros((self.batch_size, sequence_length)).astype(np.int32),\n\t                \"block_position_ids\",\n\t            )\n", "        hidden_states, layer_present_list = self.transformer(\n\t            model_graph,\n\t            input_ids,\n\t            init_past_list,\n\t            position_ids,\n\t            block_position_ids,\n\t            step,\n\t            attention_mask,\n\t            sequence_length=sequence_length,\n\t            outline_blocks=self.outline_blocks,\n", "        )\n\t        with self.device_scope(model_graph, virtual_graph_id=0):\n\t            matmul_kwargs = {\n\t                \"amp\": 0.2,\n\t                \"partialtype\": \"half\",\n\t            }\n\t            if self.precision==\"int4\" and self.model_type==\"shard\":\n\t                matmul_kwargs.update({\n\t                    \"serial_factor\": 6,\n\t                    \"serial_mode\": \"output_channels\"\n", "                })\n\t            with self.option_scope(**matmul_kwargs):\n\t                next_ids = self.lm_head(model_graph, hidden_states, 1)\n\t        with self.device_scope(model_graph, virtual_graph_id=0):\n\t            next_input_ids = ops.constant(\n\t                model_graph,\n\t                np.ones((self.batch_size, 1), dtype=np.int32)\n\t                * self.hf_tokenizer.bos_token_id,\n\t                \"decode_start_id\",\n\t            )\n", "            # next_ids is useless for stage 2, use sub and add to avoid being pruned\n\t            next_input_ids = ops.add(model_graph, next_input_ids, next_ids)\n\t            next_input_ids = ops.sub(model_graph, next_input_ids, next_ids)\n\t            step_add_value = ops.constant(\n\t                model_graph, np.array(self.input_length).astype(np.int32), \"1\"\n\t            )\n\t            next_step = ops.add(model_graph, step, step_add_value)\n\t            next_iput_ids_container = ops.dynamic_update(\n\t                model_graph,\n\t                input_ids_container,\n", "                next_step,\n\t                next_input_ids,\n\t                axes=[1],\n\t                sizes=[1],\n\t            )\n\t        return next_iput_ids_container, next_step, position_ids, layer_present_list\n\t    def build_model_graph_2nd(\n\t        self,\n\t        input_ids_container,\n\t        step,\n", "        attention_mask,\n\t        stop_mask,\n\t        position_ids,\n\t        layer_past_list,\n\t        sequence_length=1,\n\t    ):\n\t        model_graph = self.graph.createSubgraphBuilder()\n\t        model_graph.setGraphName(\"model_graph\")\n\t        # add inputs for loop op\n\t        self.add_input_tensor(model_graph, \"BOOL\", [], \"cond_place_holder\")\n", "        self.add_input_tensor(model_graph, \"INT32\", [], \"max_loop_place_holder\")\n\t        # add inputs for graph\n\t        input_ids_container = self.add_untyped_input_tensor(model_graph, input_ids_container)\n\t        step = self.add_untyped_input_tensor(model_graph, step)\n\t        attention_mask = self.add_untyped_input_tensor(model_graph, attention_mask)\n\t        stop_mask = self.add_untyped_input_tensor(model_graph, stop_mask)\n\t        position_ids = self.add_untyped_input_tensor(model_graph, position_ids)\n\t        layer_past_list = self.add_untyped_input_tensor(model_graph, layer_past_list)\n\t        with self.device_scope(model_graph, virtual_graph_id=0):\n\t            input_ids = ops.dynamic_slice(\n", "                model_graph,\n\t                input_ids_container,\n\t                step,\n\t                axes=[1],\n\t                sizes=[sequence_length],\n\t            )\n\t            # stage 2: block_position_ids = [step-input_length+1]\n\t            constant_pos = ops.constant(\n\t                model_graph,\n\t                np.array(self.input_length - 1).astype(np.int32),\n", "                \"block_pos_constant\",\n\t            )\n\t            block_position_ids = ops.sub(model_graph, step, constant_pos)\n\t        hidden_states, layer_present_list = self.transformer(\n\t            model_graph,\n\t            input_ids,\n\t            layer_past_list,\n\t            position_ids,\n\t            block_position_ids,\n\t            step,\n", "            attention_mask,\n\t            sequence_length=sequence_length,\n\t            outline_blocks=self.outline_blocks,\n\t        )\n\t        with self.device_scope(model_graph, virtual_graph_id=0):\n\t            matmul_kwargs = {\n\t                \"amp\": 0.2,\n\t                \"partialtype\": \"float\",\n\t            }\n\t            if self.precision==\"int4\" and self.model_type==\"shard\":\n", "                matmul_kwargs.update({\n\t                    \"serial_factor\": 6,\n\t                    \"serial_mode\": \"output_channels\"\n\t                })\n\t            with self.option_scope(**matmul_kwargs):\n\t                next_ids = self.lm_head(model_graph, hidden_states, sequence_length)\n\t        with self.device_scope(model_graph, virtual_graph_id=0):\n\t            next_input_ids = next_ids\n\t            step_add_value = ops.constant(model_graph, np.array(1).astype(np.int32), \"1\")\n\t            next_step = ops.add(model_graph, step, step_add_value)\n", "            next_iput_ids_container = ops.dynamic_update(\n\t                model_graph,\n\t                input_ids_container,\n\t                next_step,\n\t                next_input_ids,\n\t                axes=[1],\n\t                sizes=[1],\n\t            )\n\t            attention_mask_updater = ops.constant(\n\t                model_graph,\n", "                np.zeros((self.batch_size, 1, 1, 1), dtype=self.np_float_type),\n\t                \"attention_mask_updater\",\n\t            )\n\t            attention_mask = ops.static_slice(\n\t                model_graph, attention_mask, starts=[0], ends=[-1], axes=[3]\n\t            )\n\t            next_attention_mask = ops.concat(\n\t                model_graph, attention_mask_updater, attention_mask, axis=3\n\t            )\n\t            next_stop_mask, keep_going_cond = self.step_loop_cond(\n", "                model_graph, next_input_ids, stop_mask\n\t            )\n\t        output_list = [\n\t            keep_going_cond,\n\t            next_iput_ids_container,\n\t            next_step,\n\t            next_attention_mask,\n\t            next_stop_mask,\n\t            position_ids,\n\t        ]\n", "        self.add_output_tensor(\n\t            model_graph,\n\t            output_list + layer_present_list,\n\t        )\n\t        return model_graph\n\t    def build_input_dict(self, **kwargs):\n\t        query = kwargs.get(\"input_string\", \"\")\n\t        use_history = kwargs.get(\"use_history\", False)\n\t        global_round_count = 0\n\t        if use_history:\n", "            prompt = f\"[Round {global_round_count}]\\n{query}\\n\"\n\t            round_count = 0\n\t            for i in range(len(history) - 1, -1, -1):\n\t                old_query, response = history[i]\n\t                history_text = f\"[Round {i}]\\n{old_query}\\n{response}\\n\"\n\t                prompt_length = self.hf_tokenizer(\n\t                    history_text + prompt, return_tensors=\"pt\"\n\t                )[\"input_ids\"].shape[1]\n\t                if prompt_length <= self.input_length + 1:  # don't count '130001'\n\t                    prompt = history_text + prompt\n", "                    round_count += 1\n\t                else:\n\t                    break\n\t            history = history[-round_count:]\n\t        else:\n\t            prompt = query\n\t        inputs = self.hf_tokenizer(prompt, return_tensors=\"pt\", max_length=self.input_length)\n\t        input_ids = inputs[\"input_ids\"][..., :-1]\n\t        input_ids_container = torch.zeros(1, self.max_length).int()\n\t        input_ids_container[:, : input_ids.size(1)] = input_ids[0]\n", "        input_ids_container = input_ids_container.repeat(self.batch_size, 1)\n\t        position_ids = inputs[\"position_ids\"][:, 0, :-1]\n\t        last_position_id = inputs[\"position_ids\"][:, 0, -1]\n\t        position_ids_container = torch.zeros(1, self.max_length).int()\n\t        position_ids_container[:, : position_ids.size(1)] = position_ids[0]\n\t        position_ids_container[:, position_ids.size(1):] = last_position_id\n\t        position_ids_container = position_ids_container.repeat(self.batch_size, 1)\n\t        attention_mask_container = torch.zeros(1, self.max_length).int()\n\t        # where 1 is masked while 0 is not\n\t        attention_mask_container[:, : input_ids.size(1)] = 1\n", "        attention_mask_container = attention_mask_container.repeat(self.batch_size, 1)\n\t        return {\n\t            \"input_ids\": input_ids_container.numpy(),\n\t            \"position_ids\": position_ids_container.numpy(),\n\t            \"attention_mask\": attention_mask_container.numpy(),\n\t        }\n\t    def build_output_dict(self, anchor_arrays):\n\t        def process_response(response):\n\t            response = response.strip()\n\t            response = response.replace(\"[[]]\", \"2023\")\n", "            punkts = [\n\t                [\",\", \"\"],\n\t                [\"!\", \"\"],\n\t                [\":\", \"\"],\n\t                [\";\", \"\"],\n\t                [\"\\?\", \"\"],\n\t            ]\n\t            for item in punkts:\n\t                response = re.sub(\n\t                    r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response\n", "                )\n\t                response = re.sub(\n\t                    r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response\n\t                )\n\t            return response\n\t        output, decode_step = (\n\t            anchor_arrays[\"Loop:0\"],\n\t            anchor_arrays[\"Loop:1\"],\n\t        )\n\t        if len(output.shape) == 3:  # [num_replicas, batch_size, max_length]\n", "            output, decode_step = output[0], decode_step[0]\n\t        # Fixme: only support 1 batch.\n\t        output = output.tolist()[0] # [max_length]\n\t        if self.hf_config.bos_token_id in output:\n\t            output = output[output.index(self.hf_config.bos_token_id):]\n\t        if self.hf_config.eos_token_id in output:\n\t            output = output[: output.index(self.hf_config.eos_token_id)]\n\t        output = self.hf_tokenizer.decode(output)\n\t        output = process_response(output)\n\t        return {\"output\": output, \"decode_step\": decode_step}\n"]}
{"filename": "poptransformer/models/chatglm/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer, Embedding\n\tdef split_embedding_weight(weight, vocab_size, count, pad_num=0):\n\t    \"\"\"\n\t    1. For example vocab_size=150528, count=4, the weight will be splited into 4 parts,\n\t    Each part has 37634 tokens = 37632 valid + 2 pad:\n\t        Emb1: [0(PAD), ..., 37631, 37632, 37633(PAD)]\n", "        Emb2: [0(PAD), ..., 37631, 37632, 37633(PAD)]\n\t        Emb3: [0(PAD), ..., 37631, 37632, 37633(PAD)]\n\t        Emb4: [0(PAD), ..., 37631, 37632, 37633(PAD)]\n\t    2. Besides, input ids should be cliped to avoid out of bounds:\n\t                    IPU-0             IPU-1              IPU-2                IPU-3\n\t                |--37632--|      |---37632---|      |---37632---|       |----37632----|\n\t        input:  [0, ..., 37631, 37632, ..., 75263, 75264, ..., 112895, 112896, ..., 150527]\n\t        input1: clip(input, -1, 37632) -> sub(-1)\n\t        input2: clip(input, 37631, 75264) -> sub(37631)\n\t        input3: clip(input, 75263, 112896) -> sub(75263)\n", "        input4: clip(input, 112895, 150528) -> sub(112895)\n\t        final_input = input1 + input2 + input3 + input4\n\t    \"\"\"\n\t    split = []  # [tensor, clip_min, clip_max, offset]\n\t    assert vocab_size % count == 0\n\t    num_valid_token = vocab_size // count\n\t    weight_list = np.split(weight, count, 0)\n\t    pad = np.zeros((1, weight.shape[1]), dtype=weight.dtype)\n\t    for i in range(count):\n\t        weight_ = weight_list[i]\n", "        # Each partial embedding has 2 pad tokens.\n\t        weight_ = np.concatenate([pad, weight_], axis=0)\n\t        weight_ = np.concatenate([weight_, pad], axis=0)\n\t        clip_min = i * num_valid_token - 1\n\t        clip_max = (i + 1) * num_valid_token\n\t        offset = clip_min\n\t        split.append([weight_, clip_min, clip_max, offset])\n\t    pad_mask = np.zeros((1, weight_.shape[0]), dtype=weight_.dtype)\n\t    if pad_num > 0:\n\t        pad_mask[:, -(pad_num + 1) :] = -1000.0\n", "    return split, pad_mask\n\tclass BaseChatGLMEmbedding(BaseLayer):\n\t    def __init__(self, context, name, vocab_size, embd_size, num_embedding_partitions=1):\n\t        super().__init__(context, name)\n\t        self.vocab_size = vocab_size\n\t        self.embd_size = embd_size\n\t        self.num_embedding_partitions = num_embedding_partitions\n\t        self.embedding_split_size = vocab_size // num_embedding_partitions + 2\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n", "        raise NotImplementedError\n\t    def __call__(self, input_ids, sequence_length):\n\t        raise NotImplementedError\n\tclass ShardChatGLMEmbedding(BaseChatGLMEmbedding):\n\t    def collect_bind_layer_weights(self):\n\t        weight_key = \".\".join([self.context, \"word_embeddings\", \"weight\"])\n\t        weight_np = self.get_param_from_state_dict(weight_key, None)\n\t        pad_num = self.vocab_size - weight_np.shape[0]\n\t        if pad_num > 0:\n\t            pad_weight = np.zeros((pad_num, weight_np.shape[1]), dtype=weight_np.dtype)\n", "            weight_np = np.concatenate([weight_np, pad_weight], axis=0)\n\t        self.weights, self.embedding_pad_mask = split_embedding_weight(\n\t            weight_np, self.vocab_size, self.num_embedding_partitions, pad_num\n\t        )\n\t        self.weight_ids = []\n\t        self.token_offsets = []\n\t        self.clips = []\n\t        for i in range(self.num_embedding_partitions):\n\t            weight_, clip_min, clip_max, offset = self.weights[i]\n\t            weight_id = self.add_initialized_input_tensor(weight_, f\"token_weight_{i}\")\n", "            self.weight_ids.append(weight_id)\n\t            self.token_offsets.append(offset)\n\t            self.clips.append([clip_min, clip_max])\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        inputs_embeds_list = []\n\t        with graph.nameScope(self.context):\n\t            for i in range(self.num_embedding_partitions):\n\t                with graph.virtualGraph(i):\n\t                    # input_ids = ops.add(graph, input_ids, ops.constant(graph, np.array([0], dtype=np.int32)))\n\t                    input_ids_ = ops.clip(\n", "                        graph, input_ids, self.clips[i][0], self.clips[i][1]\n\t                    )\n\t                    input_ids_ = ops.sub(\n\t                        graph,\n\t                        input_ids_,\n\t                        ops.constant(\n\t                            graph,\n\t                            np.array([self.token_offsets[i]], dtype=np.int32),\n\t                            f\"token_offset_{i}\",\n\t                        ),\n", "                    )\n\t                    inputs_embeds_ = ops.gather(graph, self.weight_ids[i], input_ids_)\n\t                    inputs_embeds_list.append(inputs_embeds_)\n\t            with graph.virtualGraph(self.num_embedding_partitions - 1):\n\t                inputs_embeds = ops.sum_op(graph, inputs_embeds_list)\n\t                inputs_embeds = ops.remap_tensor(graph, inputs_embeds)\n\t        return inputs_embeds\n\tclass TPChatGLMEmbedding(BaseChatGLMEmbedding):\n\t    def collect_bind_layer_weights(self):\n\t        self.wte = Embedding(self.context, 'word_embeddings', self.vocab_size, self.embd_size)\n", "        # For unify the interface\n\t        self.weight_ids = self.wte.weight_id\n\t        self.token_offsets = [1]\n\t        self.embedding_pad_mask = None\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            input_embeds = self.wte(graph, input_ids, sequence_length)\n\t            # embeds = ops.remap_tensor(graph, input_embeds)\n\t            embeds = graph.aiGraphcore.replicatedallreduce([input_embeds])\n\t        return embeds\n", "class ChatGLMEmbedding(TPChatGLMEmbedding, ShardChatGLMEmbedding):\n\t    layer_class_map = {\n\t        \"tp\": TPChatGLMEmbedding,\n\t        \"shard\": ShardChatGLMEmbedding,\n\t    }\n\t    def __init__(self, context, name, vocab_size, embd_size, num_embedding_partitions):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(\n", "                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n\t            )\n\t        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n\t        super().__init__(context, name, vocab_size, embd_size, num_embedding_partitions)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/chatglm/lm_head.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport math\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer, Linear\n\tclass BaseLMHead(BaseLayer):\n\t    def __init__(self,\n\t                 context,\n\t                 name,\n", "                 vocab_size,\n\t                 topk,\n\t                 embedding_size,\n\t                 embedding_weights,\n\t                 num_embedding_partitions=1,\n\t                 token_offsets=None,\n\t                 embedding_pad_mask=None):\n\t        super().__init__(context, name)\n\t        self.embedding_size = embedding_size\n\t        self.vocab_size = vocab_size\n", "        self.topk = topk\n\t        self.embedding_weights = embedding_weights\n\t        self.num_embedding_partitions = num_embedding_partitions\n\t        self.embedding_split_size = vocab_size // num_embedding_partitions + 2\n\t        self.lm_split_count = 6 if self.precision == \"int4\" else 2\n\t        self.lm_split_size = self.embedding_split_size // self.lm_split_count\n\t        self.logits_pad_mask = np.zeros(\n\t            (1, self.embedding_split_size), dtype=self.np_float_type\n\t        )\n\t        self.logits_pad_mask[:, 0] = -1000.0\n", "        self.logits_pad_mask[:, -1] = -1000.0\n\t        self.token_offsets = token_offsets\n\t        self.embedding_pad_mask = embedding_pad_mask\n\t        # for TP\n\t        self.virtual_id = None\n\t        self.use_tied_embedding = False\n\t        self.tie_weight = embedding_weights\n\t        self.collect_bind_layer_weights()\n\t    def set_virtual_id(self, virtual_id):\n\t        pass\n", "    def collect_bind_layer_weights(self):\n\t        pass\n\t    def __call__(self, graph, hidden_states, sequence_length):\n\t        next_token_prob_list = []\n\t        next_token_list = []\n\t        for i in range(self.num_embedding_partitions):\n\t            with graph.virtualGraph(i):\n\t                transposed_token_weight_ = ops.transpose(\n\t                    graph, self.embedding_weights[i], [1, 0]\n\t                )\n", "                # (B, H) * (H, V/n + 2) -> (B, V/n + 2)\n\t                res = []\n\t                for j in range(self.lm_split_count):\n\t                    y_ = ops.static_slice(\n\t                        graph,\n\t                        transposed_token_weight_,\n\t                        [self.lm_split_size * j],\n\t                        [self.lm_split_size * (j+1)],\n\t                        [1]\n\t                    )\n", "                    with self.option_scope(amp=0.2):\n\t                        o_ = ops.matmul(graph, hidden_states, y_)\n\t                    res.append(o_)\n\t                logits_ = ops.concat_sequence(graph, res, axis=1)\n\t                logits_ = ops.add(\n\t                    graph,\n\t                    logits_,\n\t                    ops.constant(graph, self.logits_pad_mask,\n\t                                 f\"logits_pad_mask_{i}\"),\n\t                )\n", "                if i == self.num_embedding_partitions - 1:\n\t                    logits_ = ops.add(\n\t                        graph,\n\t                        logits_,\n\t                        ops.constant(\n\t                            graph,\n\t                            self.embedding_pad_mask,\n\t                            f\"embedding_pad_mask_{i}\",\n\t                        ),\n\t                    )\n", "                # (B, V/n + 2) -> (B, k)\n\t                next_token_prob_, next_token_ = ops.topk(\n\t                    graph, logits_, axis=1, k=self.topk)\n\t                next_token_ = ops.add(\n\t                    graph,\n\t                    next_token_,\n\t                    ops.constant(\n\t                        graph,\n\t                        np.array([self.token_offsets[i]], dtype=np.int32),\n\t                        f\"token_offset_{i}\",\n", "                    ),\n\t                )\n\t                next_token_prob_list.append(next_token_prob_)\n\t                next_token_list.append(next_token_)\n\t        with graph.virtualGraph(self.num_embedding_partitions - 1):\n\t            next_token_probs = ops.concat_sequence(\n\t                graph, next_token_prob_list, 1)\n\t            next_tokens = ops.concat_sequence(graph, next_token_list, 1)\n\t            idx = ops.argmax(graph, next_token_probs, 1)\n\t            idx = ops.squeeze(graph, idx, [0])\n", "            next_token = ops.dynamic_slice(graph, next_tokens, idx, [1], [1])\n\t        return next_token\n\tclass TPLMHead(BaseLMHead):\n\t    def tie_embedding(self, weight_id):\n\t        self.head.weight_id = weight_id\n\t        self.use_tied_embedding = True\n\t        self.logger.info(f'setting weight id to {weight_id}')\n\t    def set_virtual_id(self, virtual_id):\n\t        self.virtual_id = virtual_id\n\t        self.logger.info(f'setting virtual id to {virtual_id}')\n", "    def collect_bind_layer_weights(self):\n\t        if not self.tie_weight:\n\t            lm_tp_settings = {\n\t                'strategy_name': 'start',\n\t            }\n\t        else:\n\t            lm_tp_settings = {\n\t                'strategy_name': 'identity',\n\t            }\n\t        self.head = Linear(self.context, None, self.embedding_size,\n", "                           self.vocab_size, False, **lm_tp_settings)\n\t        if self.tie_weight:\n\t            self.tie_embedding(self.tie_weight)\n\t    def __call__(self, graph, logits, sequence_length):\n\t        with graph.virtualGraph(self.virtual_id):\n\t            vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n\t            vocab_per_ipu = math.ceil(self.vocab_size / self.num_replicas)\n\t            index_offset_np = np.expand_dims(\n\t                np.arange(self.num_replicas, dtype=np.int32), [1, 2]) * vocab_per_ipu\n\t            index_offset = self.add_initialized_input_tensor(\n", "                index_offset_np, 'index_offset', **vs_setting)\n\t            if self.use_tied_embedding:\n\t                self.head.weight_id = ops.transpose(\n\t                    graph, self.head.weight_id, [1, 0])\n\t                # Avoid to transpose twice in 2 stage mode.\n\t                self.use_tied_embedding = False\n\t            logits = self.head(graph, logits)\n\t            logits = ops.unsqueeze(graph, logits, [1])  # Align with sharding\n\t            pad_idx = ops.equal(graph, ops.constant(graph, np.array(\n\t                0.0).astype(self.np_float_type), 'zero'), logits)\n", "            logits = ops.where(\n\t                graph, pad_idx, ops.constant(graph, np.array(-10000.0).astype(self.np_float_type), '-10000'), logits)\n\t            next_token_prob, next_token_ = ops.topk(\n\t                graph, logits, axis=2, k=self.topk)\n\t            next_token = ops.add(graph, next_token_, index_offset)\n\t            next_token_prob = ops.replicated_allgather(graph, next_token_prob)\n\t            next_token_topk = ops.replicated_allgather(graph, next_token)\n\t            next_token_prob_shape = [sequence_length * self.topk * self.num_replicas, self.batch_size]\n\t            next_token_prob = ops.transpose(\n\t                graph,\n", "                ops.reshape(graph, next_token_prob, next_token_prob_shape),\n\t                [1, 0]\n\t            )\n\t            next_token_topk_shape = [\n\t                sequence_length * self.topk * self.num_replicas, self.batch_size]\n\t            next_token_topk = ops.transpose(\n\t                graph,\n\t                ops.reshape(graph, next_token_topk, next_token_topk_shape),\n\t                [1, 0]\n\t            )\n", "            next_token_prob = ops.reshape(\n\t                graph, next_token_prob, [self.batch_size, sequence_length, self.topk * self.num_replicas])\n\t            next_token_topk = ops.reshape(\n\t                graph, next_token_topk, [self.batch_size, sequence_length, self.topk * self.num_replicas])\n\t            next_token_idx = ops.argmax(graph, next_token_prob, axis=2)  # [B,1,1]\n\t            next_token_idx = ops.squeeze(graph, next_token_idx, [1, 2])  # (B,)\n\t            next_token_topk = ops.squeeze(graph, next_token_topk, [1])  # [B,topk * num_replica]\n\t            next_token = ops.grouped_gather(\n\t                graph, next_token_topk, next_token_idx, axis=1, group_size=self.batch_size)  # (B,)\n\t            next_token = ops.reshape(graph, next_token, [self.batch_size, -1])\n", "            return next_token\n\tclass LMHead(TPLMHead, BaseLMHead):\n\t    layer_class_map = {\"tp\": TPLMHead, \"shard\": BaseLMHead}\n\t    def __init__(\n\t        self,\n\t        context,\n\t        name,\n\t        vocab_size,\n\t        topk,\n\t        embedding_size,\n", "        embedding_weights,\n\t        num_embedding_partitions=1,\n\t        token_offsets=None,\n\t        embedding_pad_mask=None,\n\t    ):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(\n\t                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n", "            )\n\t        self.logger.debug(\n\t            f\"initializing model type: {self.layer_class.__name__}\")\n\t        super().__init__(\n\t            context, name, vocab_size, topk, embedding_size, embedding_weights,\n\t            num_embedding_partitions, token_offsets, embedding_pad_mask)\n\t    def __call__(self, graph, hidden_states, sequence_length):\n\t        return self.layer_class.__call__(self, graph, hidden_states, sequence_length)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/chatglm/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport math\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer\n\tfrom poptransformer.layers import Linear\n\tclass BaseRotaryAttention(BaseLayer):\n\t    softmax_fn_map = {\n\t        \"aionnx\": ops.softmax,\n", "        \"ce\": ops.softmax_ce,\n\t    }\n\t    def __init__(self, context, name, input_size, num_head, cache_max_length, layer_index, rotary_dim):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.num_head = num_head\n\t        self.head_size = self.input_size // self.num_head\n\t        self.cache_max_length = cache_max_length\n\t        self.layer_index = layer_index\n\t        self.rotary_dim = rotary_dim\n", "        self.scale = input_size // num_head\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.c_attn = Linear(self.context, \"query_key_value\",\n\t                             self.input_size, self.input_size * 3)\n\t        self.c_proj = Linear(self.context, \"dense\",\n\t                             self.input_size, self.input_size)\n\t    def kv_cache(self, graph, kv, layer_past, sequence_length):\n\t        \"\"\"Implement cache for key-value pairs without using custom op.\n\t        Equivalent to `ops.kv_cache`.\n", "        \"\"\"\n\t        with graph.nameScope(\"attn_past_update\"):\n\t            # layer_past: [2, B, N, L, h]\n\t            layer_past = ops.static_slice(\n\t                graph, layer_past, [0], [-sequence_length], [3]\n\t            )\n\t            layer_past = ops.concat(graph, kv, layer_past, 3)\n\t        return layer_past\n\t    def forward_qkv(self, graph, x, layer_past, step, position_ids, block_position_ids):\n\t        qkv = self.c_attn(graph, x)\n", "        qkv = ops.reshape(\n\t            graph,\n\t            qkv,\n\t            [self.batch_size, self.sequence_length,\n\t                self.num_head, 3 * self.head_size],\n\t        )\n\t        temp_splits = [self.head_size] * 3\n\t        q, k, v = ops.split(graph, qkv, num_outputs=3,\n\t                            axis=3, splits=temp_splits)\n\t        cos1, sin1 = self.fixed_pos_embedding(graph, position_ids)\n", "        cos2, sin2 = self.fixed_pos_embedding(graph, block_position_ids)\n\t        q1, q2 = ops.split(\n\t            graph, q, num_outputs=2, axis=-1, splits=[self.rotary_dim, self.rotary_dim], name=\"rope_split_q\",)\n\t        k1, k2 = ops.split(\n\t            graph, k, num_outputs=2, axis=-1, splits=[self.rotary_dim, self.rotary_dim], name=\"rope_split_k\")\n\t        q1, k1 = self.apply_rotary_pos_emb_index(graph, q1, k1, cos1, sin1)\n\t        q2, k2 = self.apply_rotary_pos_emb_index(graph, q2, k2, cos2, sin2)\n\t        q = ops.concat(graph, q1, q2, axis=-1)\n\t        k = ops.concat(graph, k1, k2, axis=-1)\n\t        q = ops.transpose(graph, q, [0, 2, 1, 3])  # q: [B, N, L, h]\n", "        kv = ops.concat(graph, k, v, axis=0)\n\t        kv = ops.reshape(\n\t            graph,\n\t            kv,\n\t            shape=[2, self.batch_size, self.sequence_length,\n\t                   self.num_head, self.head_size]\n\t        )\n\t        kv = ops.transpose(graph, kv, perm=[0, 1, 3, 2, 4])\n\t        layer_present = self.kv_cache(\n\t            graph, kv, layer_past, self.sequence_length)\n", "        layer_present = ops.remap_tensor(graph, layer_present)\n\t        if self.sequence_length != 1 and self.sequence_length < self.cache_max_length:\n\t            layer_present_temp = kv\n\t        else:\n\t            layer_present_temp = layer_present\n\t        k, v = ops.split(\n\t            graph, layer_present_temp, 2, axis=0, splits=[1, 1], name=\"split_past\"\n\t        )\n\t        k = ops.squeeze(graph, k, [0])\n\t        v = ops.squeeze(graph, v, [0])\n", "        return q, k, v, layer_present\n\t    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n\t        temp_k = ops.transpose(graph, k, [0, 1, 3, 2])\n\t        layer_scale_coeff_value = np.array(\n\t            [self.layer_index + 1], dtype=self.np_float_type)\n\t        layer_scale_coeff = ops.constant(\n\t            graph, layer_scale_coeff_value, f\"softmax_scale_{self.layer_index}\"\n\t        )\n\t        attention_scale_value = np.array(\n\t            [1.0 / (math.sqrt(self.head_size) * (self.layer_index + 1))],\n", "            dtype=self.np_float_type,\n\t        )\n\t        attention_scale = ops.constant(\n\t            graph, attention_scale_value, f\"attention_scale_{self.layer_index}\"\n\t        )\n\t        q = ops.mul(graph, q, attention_scale)\n\t        score = ops.matmul(graph, q, temp_k)\n\t        if self.sequence_length != 1:\n\t            score = ops.remap_tensor(graph, score, fwd_after_matmul=True)\n\t        score = ops.mul(graph, score, layer_scale_coeff)\n", "        score = ops.add(graph, score, attention_mask)\n\t        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n\t        if not softmax_fn:\n\t            raise ValueError(\n\t                f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\"\n\t            )\n\t        score = softmax_fn(\n\t            graph, score, -1, stable_mode=self.sequence_length != 1)\n\t        return score\n\t    def forward_output(self, graph, score, v):\n", "        score = ops.matmul(graph, score, v)\n\t        score = ops.transpose(graph, score, [0, 2, 1, 3])\n\t        score = ops.reshape(\n\t            graph, score, [self.batch_size, self.sequence_length, -1]\n\t        )\n\t        output = self.c_proj(graph, score)\n\t        return output\n\t    def fixed_pos_embedding(self, graph, position_id):\n\t        # rotary_dim = hidden_size_per_head // 2\n\t        # cos, sin = [L, rotary_dim]\n", "        inv_freq_value = np.array(\n\t            [\n\t                1.0 / (10000 ** (i / self.rotary_dim))\n\t                for i in range(0, self.rotary_dim, 2)\n\t            ]\n\t        ).astype(self.np_float_type)\n\t        inv_freq = ops.constant(graph, inv_freq_value, \"inv_freq\")\n\t        inv_freq = ops.reshape(graph, inv_freq, [1, -1])\n\t        # position_id -> [L, B]\n\t        position_id = ops.cast(\n", "            graph, ops.reshape(graph, position_id,\n\t                               [-1, 1]), self.popart_float_type\n\t        )\n\t        freqs = ops.matmul(graph, position_id, inv_freq)\n\t        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n\t        emb = ops.concat(graph, freqs, freqs, axis=-1)\n\t        # emb -> [L, rotary_dim] -> [1, L, 1, rotary_dim]\n\t        emb = ops.reshape(graph, emb, shape=[1, -1, 1, self.rotary_dim])\n\t        cos, sin = graph.aiOnnx.cos([emb]), graph.aiOnnx.sin([emb])\n\t        return cos, sin\n", "    def rotate_half(self, graph, x):\n\t        x1, x2 = ops.split(\n\t            graph,\n\t            x,\n\t            num_outputs=2,\n\t            axis=-1,\n\t            splits=[self.rotary_dim // 2, self.rotary_dim // 2],\n\t            name=\"rope_split\",\n\t        )\n\t        x2 = ops.mul(graph, x2, ops.constant(\n", "            graph, np.array([-1]).astype(self.np_float_type)))\n\t        return ops.concat(graph, x2, x1, axis=-1)\n\t    def apply_rotary_pos_emb_index(self, graph, q, k, cos, sin):\n\t        # position_id: [B, L], q, k: [B, L, N, rotary_dim], sin, cos: [L, rotary_dim] -> [1, L, 1, rotary_dim]\n\t        q = ops.add(\n\t            graph,\n\t            ops.mul(graph, q, cos),\n\t            ops.mul(graph, self.rotate_half(graph, q), sin)\n\t        )\n\t        k = ops.add(\n", "            graph,\n\t            ops.mul(graph, k, cos),\n\t            ops.mul(graph, self.rotate_half(graph, k), sin),\n\t        )\n\t        return q, k\n\t    def __call__(\n\t        self,\n\t        graph,\n\t        x,\n\t        layer_past,\n", "        position_ids,\n\t        block_position_ids,\n\t        step,\n\t        attention_mask,\n\t        sequence_length,\n\t        softmax_type=\"ce\"\n\t    ):\n\t        with graph.nameScope(self.context):\n\t            self.sequence_length = sequence_length\n\t            q, k, v, layer_present = self.forward_qkv(\n", "                graph, x, layer_past, step, position_ids, block_position_ids\n\t            )\n\t            score = self.forward_attention(\n\t                graph, q, k, attention_mask, softmax_type)\n\t            output = self.forward_output(graph, score, v)\n\t            return output, layer_present\n\tclass TPRotaryAttention(BaseRotaryAttention):\n\t    def collect_bind_layer_weights(self):\n\t        self.num_head_before_tp = self.num_head\n\t        self.num_head = self.num_head // self.num_replicas\n", "        assert self.num_head_before_tp == self.num_head * self.num_replicas, \\\n\t            f\"Heads {self.num_head_before_tp} can not be exact divided by replicas {self.num_replicas}.\"\n\t        qkv_tp_setting = {\n\t            'strategy_name': 'start',\n\t        }\n\t        proj_tp_setting = {\n\t            'strategy_name': 'end',\n\t        }\n\t        self.c_attn = Linear(\n\t            self.context, \"query_key_value\", self.input_size, self.input_size * 3, **qkv_tp_setting)\n", "        self.c_proj = Linear(\n\t            self.context, \"dense\", self.input_size, self.input_size, **proj_tp_setting)\n\tclass RotaryAttention(TPRotaryAttention, BaseRotaryAttention):\n\t    layer_class_map = {\n\t        \"tp\": TPRotaryAttention,\n\t        \"shard\": BaseRotaryAttention,\n\t    }\n\t    def __init__(self, context, name, input_size, num_head, cache_max_length, layer_index, rotary_dim):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n", "        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(\n\t            f\"initializing model type: {self.layer_class.__name__}\")\n\t        super().__init__(context, name, input_size, num_head, cache_max_length, layer_index, rotary_dim)\n\t    def __call__(\n\t        self,\n\t        graph,\n\t        x,\n\t        layer_past,\n", "        position_ids,\n\t        block_position_ids,\n\t        step,\n\t        attention_mask,\n\t        sequence_length,\n\t        softmax_type=\"ce\"\n\t    ):\n\t        return self.layer_class.__call__(\n\t            self, graph, x, layer_past, position_ids, block_position_ids,\n\t            step, attention_mask, sequence_length, softmax_type,)\n", "    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n\t    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n\t        return self.layer_class.forward_attention(self, graph, q, k, attention_mask, softmax_type)\n\t    def forward_qkv(self, graph, x, layer_past, step, position_ids, block_position_ids):\n\t        return self.layer_class.forward_qkv(\n\t            self, graph, x, layer_past, step, position_ids, block_position_ids\n\t        )\n\t    def forward_output(self, graph, score, v):\n\t        return self.layer_class.forward_output(self, graph, score, v)\n", "    def fixed_pos_embedding(self, graph, position_id):\n\t        return self.layer_class.fixed_pos_embedding(self, graph, position_id)\n\t    def rotate_half(self, graph, x):\n\t        return self.layer_class.rotate_half(self, graph, x)\n\t    def apply_rotary_pos_emb_index(self, graph, q, k, cos, sin):\n\t        return self.layer_class.apply_rotary_pos_emb_index(self, graph, q, k, cos, sin)\n"]}
{"filename": "poptransformer/models/rwkv/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport torch\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import LayerNorm, BaseLayer, LMHead\n\tfrom poptransformer.models.base_model import HFDecBaseModel\n\tfrom poptransformer.models.rwkv.ffn import RWKVFeedforward\n\tfrom poptransformer.models.rwkv.attention import RWKVAttention\n\tfrom poptransformer.models.rwkv.embedding import RWKVEmbedding\n", "class RWKVBlock(BaseLayer):\n\t    def __init__(self, context, name, hidden_size, intermediate_size, attention_hidden_size, eps, layer_id):\n\t        super().__init__(context, name)\n\t        self.hidden_size = hidden_size\n\t        self.intermediate_size = intermediate_size\n\t        self.attention_hidden_size = attention_hidden_size\n\t        self.eps = eps\n\t        self.layer_id = layer_id\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n", "        if self.layer_id == 0:\n\t            self.pre_ln = LayerNorm(self.context, 'pre_ln', self.hidden_size, self.eps)\n\t        self.ln1 = LayerNorm(self.context, 'ln1', self.hidden_size, self.eps)\n\t        self.ln2 = LayerNorm(self.context, 'ln2', self.hidden_size, self.eps)\n\t        self.attention = RWKVAttention(\n\t            self.context, 'attention', self.hidden_size, self.attention_hidden_size, self.layer_id)\n\t        self.feed_forward = RWKVFeedforward(\n\t            self.context, 'feed_forward', self.hidden_size, self.intermediate_size, self.layer_id)\n\t    def __call__(self, graph, hidden, layer_state, sequence_length, norm_type='group'):\n\t        if self.layer_id == 0:\n", "            hidden = self.pre_ln(graph, hidden, sequence_length, norm_type)\n\t        temp = self.ln1(graph, hidden, sequence_length, norm_type)\n\t        attention, layer_state = self.attention(graph, temp, layer_state)\n\t        hidden = ops.add(graph, hidden, attention)\n\t        temp = self.ln2(graph, hidden, sequence_length, norm_type)\n\t        feed_forward, layer_state = self.feed_forward(graph, temp, layer_state)\n\t        hidden = ops.add(graph, hidden, feed_forward)\n\t        return hidden, layer_state\n\tclass RWKVModel(BaseLayer):\n\t    def __init__(\n", "        self,\n\t        context,\n\t        name,\n\t        hidden_size,\n\t        intermediate_size,\n\t        attention_hidden_size,\n\t        eps,\n\t        vocab_size,\n\t        num_hidden_layers,\n\t        rescale_every,\n", "        layer_per_ipu\n\t    ):\n\t        super().__init__(context, name)\n\t        self.hidden_size = hidden_size\n\t        self.intermediate_size = intermediate_size\n\t        self.attention_hidden_size = attention_hidden_size\n\t        self.eps = eps\n\t        self.vocab_size = vocab_size\n\t        self.num_hidden_layers = num_hidden_layers\n\t        self.rescale_every = rescale_every\n", "        self.layer_per_ipu = layer_per_ipu\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.embeddings = RWKVEmbedding(self.context, 'embeddings', self.vocab_size, self.hidden_size)\n\t        self.blocks = [\n\t            RWKVBlock(\n\t                context=self.context,\n\t                name=f'blocks.{i}',\n\t                hidden_size=self.hidden_size,\n\t                intermediate_size=self.intermediate_size,\n", "                attention_hidden_size=self.attention_hidden_size,\n\t                eps=self.eps,\n\t                layer_id=i\n\t            ) for i in range(self.num_hidden_layers)\n\t        ]\n\t        self.ln_out = LayerNorm(self.context, 'ln_out', self.hidden_size, self.eps)\n\t    def __call__(self, graph, input_ids, states, sequence_length, norm_type='ce', **kwargs):\n\t        hidden = self.embeddings(graph, input_ids, sequence_length)\n\t        new_states = []\n\t        outline_blocks = kwargs.get('outline_blocks', None)\n", "        end_points = np.cumsum(self.layer_per_ipu)\n\t        for index, block in enumerate(self.blocks):\n\t            stage_offset = sum(index >= end_points)\n\t            if outline_blocks is None:\n\t                outline_attr = None\n\t            elif outline_blocks == 'single_block':\n\t                outline_attr = {'block': f'sub_{index}'}\n\t            elif outline_blocks == 'multi_block':\n\t                outline_attr = {'block': f'sub_{stage_offset}'}\n\t            else:\n", "                raise ValueError(f'invalid value {outline_blocks} for outline_blocks')\n\t            layer_state = states[5 * index: 5 * index + 5]\n\t            with self.device_scope(graph, stage_offset, None, outline_attr):\n\t                hidden, layer_state = block(graph, hidden, layer_state, sequence_length, norm_type)\n\t                if (index + 1) % self.rescale_every == 0:\n\t                    hidden = ops.div(graph, hidden, ops.constant(graph, np.array(2, dtype=self.np_float_type), '2'))\n\t                new_states.append(layer_state)\n\t        hidden = self.ln_out(graph, hidden, sequence_length, norm_type=norm_type)\n\t        return hidden, new_states\n\tclass RWKVDecodeModel(HFDecBaseModel):\n", "    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n\t        self.max_length = kwargs.get('max_length')\n\t        self.layer_per_ipu = kwargs.get('layer_per_ipu')\n\t        self.outline_blocks = kwargs.get('outline_blocks', True)\n\t        self.topk = kwargs.get('topk')\n\t        self.cell = RWKVModel(\n\t            context=None,\n\t            name='rwkv',\n\t            hidden_size=self.hf_config.hidden_size,\n", "            intermediate_size=self.hf_config.intermediate_size,\n\t            attention_hidden_size=self.hf_config.attention_hidden_size,\n\t            eps=self.hf_config.layer_norm_epsilon,\n\t            vocab_size=self.hf_config.vocab_size,\n\t            num_hidden_layers=self.hf_config.num_hidden_layers,\n\t            rescale_every=self.hf_config.rescale_every,\n\t            layer_per_ipu=self.layer_per_ipu\n\t        )\n\t        self.head = LMHead(\n\t            context=None,\n", "            name='head',\n\t            topk=self.topk,\n\t            vocab_size=self.hf_config.vocab_size,\n\t            embedding_size=self.hf_config.hidden_size\n\t        )\n\t        self.head.set_virtual_id(len(self.layer_per_ipu) - 1)\n\t    def process_hf_model_state_dict(self):\n\t        with torch.no_grad():\n\t            for block_id, block in enumerate(self.hf_model.rwkv.blocks):\n\t                if self.hf_config.rescale_every > 0:\n", "                    block.attention.output.weight.div_(2 ** int(block_id // self.hf_config.rescale_every))\n\t                    block.feed_forward.value.weight.div_(2 ** int(block_id // self.hf_config.rescale_every))\n\t    def build_model_graph_inputs(self, graph, inputs):\n\t        with graph.nameScope('step'):\n\t            inputs['step'] = ops.constant(self.graph, np.array(0).astype(np.int32), 'init_step')\n\t        with graph.nameScope('stop_mask'):\n\t            inputs['stop_mask'] = ops.constant(\n\t                self.graph, np.zeros((self.batch_size,)).astype(np.int32), 'stop_mask')\n\t        with graph.nameScope('states'):\n\t            for i in range(self.hf_config.num_hidden_layers):\n", "                for j in range(2):\n\t                    inputs[f'state_{i}_{j}'] = ops.constant(\n\t                        self.graph,\n\t                        np.zeros(\n\t                        (self.batch_size, 1, self.hf_config.hidden_size), dtype=self.np_float_type),\n\t                        f'layer{i}_{j}'\n\t                    )\n\t                for j in range(2, 4):\n\t                    inputs[f'state_{i}_{j}'] = ops.constant(\n\t                        self.graph,\n", "                        np.zeros(\n\t                            (self.batch_size, 1, self.hf_config.hidden_size // self.num_replicas), dtype=np.float32),\n\t                        f'layer{i}_{j}'\n\t                    )\n\t                inputs[f'state_{i}_{j+1}'] = ops.constant(\n\t                    self.graph,\n\t                    np.zeros(\n\t                        (self.batch_size, 1, self.hf_config.hidden_size // self.num_replicas), dtype=np.float32) - 1e30,\n\t                    f'layer{i}_{j+1}'\n\t                )\n", "        del inputs['position_ids']\n\t        del inputs['attention_mask']\n\t        return inputs\n\t    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n\t        input_ids_container = model_graph_inputs['input_ids_container']\n\t        step = model_graph_inputs['step']\n\t        states = [i for i in model_graph_inputs.values() if 'states' in i]\n\t        with self.device_scope(model_graph, 0, 0):\n\t            input_ids = ops.dynamic_slice(model_graph, input_ids_container, step, axes=[1], sizes=[sequence_length])\n\t            logits, states = self.cell(\n", "                graph=model_graph,\n\t                input_ids=input_ids,\n\t                states=states,\n\t                sequence_length=sequence_length,\n\t                norm_type='ce',\n\t                outline_blocks=self.outline_blocks\n\t            )\n\t        next_ids = self.head(\n\t            model_graph,\n\t            logits,\n", "            sequence_length=sequence_length\n\t        )\n\t        model_outputs =  {'next_ids': next_ids, 'stage_offset': 1}\n\t        for index_i, i in enumerate(states):\n\t            for index_j, j in enumerate(i):\n\t                model_outputs[f'states/layer_{index_i}_{index_j}'] = j\n\t        return model_outputs\n\t    def build_post_model_graph(self, model_graph, model_graph_inputs, model_outputs):\n\t        # continue build model graph for post inference step\n\t        stage_offset = model_outputs['stage_offset']\n", "        next_ids = model_outputs['next_ids']\n\t        step = model_graph_inputs['step']\n\t        input_ids_container = model_graph_inputs['input_ids_container']\n\t        stop_mask = model_graph_inputs['stop_mask']\n\t        with self.device_scope(model_graph, 0, pipeline_stage_id=stage_offset):\n\t            next_iput_ids_container, next_step, _, id_to_update= self.step_containers(\n\t                model_graph, input_ids_container, step, None, next_ids\n\t            )\n\t            next_stop_mask, keep_going_cond = self.step_loop_cond(model_graph, id_to_update, stop_mask)\n\t        self.add_output_tensor(\n", "            model_graph,\n\t            [keep_going_cond, next_iput_ids_container, next_step, next_stop_mask] +\n\t            [v for i, v in model_outputs.items() if 'states' in i]\n\t        )\n\t    def build_input_dict(self, **kwargs):\n\t        input_string_list = list(kwargs.get('input_string_list', []))\n\t        batch_size = self.batch_size * self.batch_per_step\n\t        if len(input_string_list) >= batch_size:\n\t            input_string_list = input_string_list[:batch_size]\n\t            self.logger.info('num input strings is larger than batch size, truncating')\n", "        else:\n\t            input_string_list.extend([''] * (batch_size - len(input_string_list)))\n\t            self.logger.info('num input string is smaller than batch size, adding fake inputs')\n\t        inputs = self.hf_tokenizer(\n\t            input_string_list,\n\t            max_length=self.max_length,\n\t            padding='max_length',\n\t            return_tensors='np',\n\t            return_attention_mask=False,\n\t            add_special_tokens=False\n", "        )\n\t        return {'input_ids': inputs['input_ids']}\n\t    def build_output_dict(self, anchor_arrays):\n\t        output, decode_step = anchor_arrays['Loop:0'], anchor_arrays['Loop:1']\n\t        if len(output.shape) == 3:\n\t            output, decode_step = output[0], decode_step[0]\n\t        output = self.hf_tokenizer.batch_decode(output, skip_special_tokens=True)\n\t        output = {str(i): v for i, v in enumerate(output)}\n\t        return {'output': output, 'decode_step': decode_step}\n"]}
{"filename": "poptransformer/models/rwkv/ffn.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer\n\tfrom poptransformer.layers import Linear\n\tclass BaseRWKVFeedforward(BaseLayer):\n\t    def __init__(self, context, name, hidden_size, intermediate_size, layer_id):\n\t        super().__init__(context, name)\n\t        self.hidden_size = hidden_size\n", "        self.intermediate_size = intermediate_size\n\t        self.layer_id = layer_id\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.key_linear = Linear(self.context, 'key', self.hidden_size, self.intermediate_size, use_bias=False)\n\t        self.receptance_linear = Linear(self.context, 'receptance', self.hidden_size, self.hidden_size, use_bias=False)\n\t        self.value_linear = Linear(self.context, 'value', self.intermediate_size, self.hidden_size, use_bias=False)\n\t        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n\t        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n\t        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n", "        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n\t        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n\t        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)\n\t    def gate(self, graph, a, w, b):\n\t        y1 = ops.mul(graph, a, w)\n\t        y2 = ops.sub(graph, ops.constant(graph, np.array(1).astype(self.np_float_type)), w)\n\t        y2 = ops.mul(graph, y2, b)\n\t        y = ops.add(graph, y1, y2)\n\t        return y\n\t    def __call__(self, graph, hidden, layer_state):\n", "        temp_layer_state = layer_state[0]\n\t        key = self.gate(graph, hidden, self.time_mix_key, temp_layer_state)\n\t        receptance = self.gate(graph, hidden, self.time_mix_receptance, temp_layer_state)\n\t        layer_state[0] = hidden\n\t        key = self.key_linear(graph, key)\n\t        key = ops.relu(graph, key)\n\t        key = ops.mul(graph, key, key)\n\t        value = self.value_linear(graph, key)\n\t        receptance = self.receptance_linear(graph, receptance)\n\t        receptance = ops.sigmoid(graph, receptance)\n", "        output = ops.mul(graph, receptance, value)\n\t        return output, layer_state\n\tclass TPRWKVFeedforward(BaseRWKVFeedforward):\n\t    def collect_bind_layer_weights(self):\n\t        key_tp_setting = {\n\t            'strategy_name': 'start',\n\t        }\n\t        value_tp_setting = {\n\t            'strategy_name': 'end',\n\t        }\n", "        self.key_linear = Linear(\n\t            self.context, 'key', self.hidden_size, self.intermediate_size, use_bias=False, **key_tp_setting)\n\t        self.receptance_linear = Linear(\n\t            self.context, 'receptance', self.hidden_size, self.hidden_size, use_bias=False, **key_tp_setting)\n\t        self.value_linear = Linear(\n\t            self.context, 'value', self.intermediate_size, self.hidden_size, use_bias=False, **value_tp_setting)\n\t        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n\t        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n\t        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n\t        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n", "        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n\t        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)\n\t    def __call__(self, graph, hidden, layer_state):\n\t        temp_layer_state = layer_state[0]\n\t        key = self.gate(graph, hidden, self.time_mix_key, temp_layer_state)\n\t        receptance = self.gate(graph, hidden, self.time_mix_receptance, temp_layer_state)\n\t        layer_state[0] = hidden\n\t        key = self.key_linear(graph, key)\n\t        key = ops.relu(graph, key)\n\t        key = ops.mul(graph, key, key)\n", "        value = self.value_linear(graph, key)\n\t        receptance = self.receptance_linear(graph, receptance)\n\t        receptance = ops.replicated_allgather(graph, receptance)\n\t        receptance = ops.sigmoid(graph, receptance)\n\t        output = ops.mul(graph, receptance, value)\n\t        return output, layer_state\n\tclass RWKVFeedforward(TPRWKVFeedforward, BaseRWKVFeedforward):\n\t    layer_class_map = {\n\t        'tp': TPRWKVFeedforward,\n\t        'shard': BaseRWKVFeedforward}\n", "    def __init__(self, context, name, hidden_size, intermediate_size, layer_id):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, hidden_size, intermediate_size, layer_id)\n\t    def __call__(self, graph, hidden, layer_state):\n\t        return self.layer_class.__call__(self, graph, hidden, layer_state)\n\t    def collect_bind_layer_weights(self):\n", "        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/rwkv/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer, Embedding\n\tclass BaseRWKVEmbedding(BaseLayer):\n\t    def __init__(self, context, name, vocab_size, embd_size):\n\t        super().__init__(context, name)\n\t        self.vocab_size = vocab_size\n\t        self.embd_size = embd_size\n\t        self.collect_bind_layer_weights()\n", "    def collect_bind_layer_weights(self):\n\t        self.embedding = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            input_embeds = self.embedding(graph, input_ids, sequence_length)\n\t        return input_embeds\n\tclass TPRWKVEmbedding(BaseRWKVEmbedding):\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            input_embeds = self.embedding(graph, input_ids, sequence_length)\n", "            input_embeds = graph.aiGraphcore.replicatedallreduce([input_embeds])\n\t        return input_embeds\n\tclass RWKVEmbedding(TPRWKVEmbedding, BaseRWKVEmbedding):\n\t    layer_class_map = {\n\t        'tp': TPRWKVEmbedding,\n\t        'shard': BaseRWKVEmbedding}\n\t    def __init__(self, context, name, vocab_size, embd_size):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n", "            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, vocab_size, embd_size)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/rwkv/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer\n\tfrom poptransformer.layers import Linear\n\tfrom poptransformer.utils.param_handler.tensor_parallel_strategy import shard\n\tclass BaseRWKVAttention(BaseLayer):\n\t    def __init__(self, context, name, hidden_size, attention_hidden_size, layer_id):\n\t        super().__init__(context, name)\n", "        self.hidden_size = hidden_size\n\t        self.attention_hidden_size = attention_hidden_size\n\t        self.layer_id = layer_id\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.key_linear = Linear(\n\t            self.context, 'key', self.hidden_size, self.attention_hidden_size, use_bias=False)\n\t        self.receptance_linear = Linear(\n\t            self.context, 'receptance', self.hidden_size, self.attention_hidden_size, use_bias=False)\n\t        self.value_linear = Linear(\n", "            self.context, 'value', self.hidden_size, self.attention_hidden_size, use_bias=False)\n\t        self.output_linear = Linear(\n\t            self.context, 'output', self.attention_hidden_size, self.hidden_size, use_bias=False)\n\t        time_decay_key = '.'.join([self.context, 'time_decay'])\n\t        time_decay_np = self.get_param_from_state_dict(time_decay_key, [self.hidden_size])\n\t        self.time_decay = self.add_initialized_input_tensor(time_decay_np, time_decay_key)\n\t        time_first_key = '.'.join([self.context, 'time_first'])\n\t        time_first_np = self.get_param_from_state_dict(time_first_key, [self.hidden_size])\n\t        self.time_first = self.add_initialized_input_tensor(time_first_np, time_first_key)\n\t        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n", "        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n\t        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n\t        time_mix_value_name = '.'.join([self.context, 'time_mix_value'])\n\t        time_mix_value_np = self.get_param_from_state_dict(time_mix_value_name, [1, 1, self.hidden_size])\n\t        self.time_mix_value = self.add_initialized_input_tensor(time_mix_value_np, time_mix_value_name)\n\t        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n\t        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n\t        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)\n\t    def gate(self, graph, a, w, b):\n\t        y1 = ops.mul(graph, a, w)\n", "        y2 = ops.sub(graph, ops.constant(graph, np.array(1).astype(self.np_float_type)), w)\n\t        y2 = ops.mul(graph, y2, b)\n\t        y = ops.add(graph, y1, y2)\n\t        return y\n\t    def __call__(self, graph, hidden, layer_state):\n\t        with graph.nameScope('extract_key_value'):\n\t            num_state, den_state, max_state = layer_state[2:]\n\t            key = self.gate(graph, hidden, self.time_mix_key, layer_state[1])\n\t            value = self.gate(graph, hidden, self.time_mix_value, layer_state[1])\n\t            receptance = self.gate(graph, hidden, self.time_mix_receptance, layer_state[1])\n", "            layer_state[1] = hidden\n\t            key = self.key_linear(graph, key)\n\t            value = self.value_linear(graph, value)\n\t            receptance = self.receptance_linear(graph, receptance)\n\t            receptance = ops.sigmoid(graph, receptance)\n\t        if self.precision == 'fp16':\n\t            time_decay = ops.cast(graph, self.time_decay, 'FLOAT')\n\t            key = ops.cast(graph, key, 'FLOAT')\n\t            value = ops.cast(graph, value, 'FLOAT')\n\t            time_first = ops.cast(graph, self.time_first, 'FLOAT')\n", "        with graph.nameScope('rwkv_linear_attention'):\n\t            temp1 = ops.add(graph, key, time_first)\n\t            max_for_output = ops.maximum(graph, max_state, temp1)\n\t            e1 = ops.exp(graph, ops.sub(graph, max_state, max_for_output))\n\t            e2 = ops.exp(graph, ops.sub(graph, temp1, max_for_output))\n\t            numerator = ops.add(graph, ops.mul(graph, e1, num_state), ops.mul(graph, e2, value))\n\t            denominator = ops.add(graph, ops.mul(graph, e1, den_state), e2)\n\t            output = ops.div(graph, numerator, denominator)\n\t            time_decay = ops.exp(graph, time_decay)\n\t            temp2 = ops.sub(graph, max_state, time_decay)\n", "            max_for_state = ops.maximum(graph, temp2, key)\n\t            e1 = ops.exp(graph, ops.sub(graph, temp2, max_for_state))\n\t            e2 = ops.exp(graph, ops.sub(graph, key, max_for_state))\n\t            layer_state[2] = ops.add(graph, ops.mul(graph, e1, num_state), ops.mul(graph, e2, value))\n\t            layer_state[3] = ops.add(graph, ops.mul(graph, e1, den_state), e2)\n\t            layer_state[4] = max_for_state\n\t        if self.precision == 'fp16':\n\t            output = ops.cast(graph, output, self.popart_float_type)\n\t        output = ops.mul(graph, receptance, output)\n\t        output = self.output_linear(graph, output)\n", "        return output, layer_state\n\tclass TPRWKVAttention(BaseRWKVAttention):\n\t    def collect_bind_layer_weights(self):\n\t        key_tp_setting = {\n\t            'strategy_name': 'start',\n\t        }\n\t        output_tp_setting = {\n\t            'strategy_name': 'end',\n\t        }\n\t        self.key_linear = Linear(\n", "            self.context, 'key', self.hidden_size, self.attention_hidden_size, use_bias=False, **key_tp_setting)\n\t        self.receptance_linear = Linear(\n\t            self.context, 'receptance', self.hidden_size, self.attention_hidden_size, use_bias=False, **key_tp_setting)\n\t        self.value_linear = Linear(\n\t            self.context, 'value', self.hidden_size, self.attention_hidden_size, use_bias=False, **key_tp_setting)\n\t        self.output_linear = Linear(\n\t            self.context, 'output', self.attention_hidden_size, self.hidden_size, use_bias=False, **output_tp_setting)\n\t        vs_setting = {'vs_type': 'consecutive', 'group_size': 1}\n\t        time_decay_key = '.'.join([self.context, 'time_decay'])\n\t        time_decay_np = self.get_param_from_state_dict(time_decay_key, [self.hidden_size])\n", "        time_decay_np = shard(time_decay_np, self.num_replicas, -1)\n\t        self.time_decay = self.add_initialized_input_tensor(time_decay_np, time_decay_key, **vs_setting)\n\t        time_first_key = '.'.join([self.context, 'time_first'])\n\t        time_first_np = self.get_param_from_state_dict(time_first_key, [self.hidden_size])\n\t        time_first_np = shard(time_first_np, self.num_replicas, -1)\n\t        self.time_first = self.add_initialized_input_tensor(time_first_np, time_first_key, **vs_setting)\n\t        time_mix_key_name = '.'.join([self.context, 'time_mix_key'])\n\t        time_mix_key_np = self.get_param_from_state_dict(time_mix_key_name, [1, 1, self.hidden_size])\n\t        self.time_mix_key = self.add_initialized_input_tensor(time_mix_key_np, time_mix_key_name)\n\t        time_mix_value_name = '.'.join([self.context, 'time_mix_value'])\n", "        time_mix_value_np = self.get_param_from_state_dict(time_mix_value_name, [1, 1, self.hidden_size])\n\t        self.time_mix_value = self.add_initialized_input_tensor(time_mix_value_np, time_mix_value_name)\n\t        time_mix_receptance_name = '.'.join([self.context, 'time_mix_receptance'])\n\t        time_mix_receptance_np = self.get_param_from_state_dict(time_mix_receptance_name, [1, 1, self.hidden_size])\n\t        self.time_mix_receptance = self.add_initialized_input_tensor(time_mix_receptance_np, time_mix_receptance_name)\n\tclass RWKVAttention(TPRWKVAttention, BaseRWKVAttention):\n\t    layer_class_map = {\n\t        'tp': TPRWKVAttention,\n\t        'shard': BaseRWKVAttention}\n\t    def __init__(self, context, name, hidden_size, attention_hidden_size, layer_id):\n", "        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, hidden_size, attention_hidden_size, layer_id)\n\t    def __call__(self, graph, hidden, layer_state):\n\t        return self.layer_class.__call__(self, graph, hidden, layer_state)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/gpt2/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport torch\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import MLP, LayerNorm, BaseLayer, LMHead\n\tfrom poptransformer.models import HFDecBaseModel\n\tfrom poptransformer.models.gpt2.embedding import TransformerEmbedding\n\tfrom poptransformer.models.gpt2.attention import Attention\n\tclass TransformerBlock(BaseLayer):\n", "    def __init__(self, context, name, input_size, eps, n_head, max_length):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.eps = eps\n\t        self.n_head = n_head\n\t        self.max_length = max_length\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.layer_norm1 = LayerNorm(self.context, 'ln_1', self.input_size, self.eps)\n\t        self.attention = Attention(\n", "            self.context, 'attn', self.input_size, self.n_head, self.max_length)\n\t        self.layer_norm2 = LayerNorm(self.context, 'ln_2', self.input_size, self.eps)\n\t        self.mlp = MLP(self.context, 'mlp', self.input_size, self.input_size * 4, 'gelu')\n\t    def __call__(self, graph, x, sequence_length, step, attention_mask, norm_type='ce', softmax_type='ce', **kwargs):\n\t        with graph.nameScope(self.context):\n\t            temp_x = self.layer_norm1(graph, x, sequence_length, norm_type)\n\t            temp_x = self.attention(graph, temp_x, step, attention_mask, sequence_length, softmax_type)\n\t            x = ops.add(graph, x, temp_x)\n\t            temp_x = self.layer_norm2(graph, x, sequence_length, norm_type)\n\t            temp_x = self.mlp(graph, temp_x)\n", "            x = ops.add(graph, x, temp_x)\n\t        return x\n\tclass Transformer(BaseLayer):\n\t    def __init__(self, context, name, vocab_size, embd_size, eps,\n\t                 n_head, max_length, n_layer, layer_per_ipu, max_position):\n\t        super().__init__(context, name)\n\t        self.vocab_size = vocab_size\n\t        self.embd_size = embd_size\n\t        self.eps = eps\n\t        self.n_head = n_head\n", "        self.max_length = max_length\n\t        self.n_layer = n_layer\n\t        self.layer_per_ipu = layer_per_ipu\n\t        self.max_position = max_position\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.embedding = TransformerEmbedding(\n\t            self.context,\n\t            None,\n\t            self.vocab_size,\n", "            self.embd_size,\n\t            self.max_position\n\t        )\n\t        self.blocks = [\n\t            TransformerBlock(\n\t                self.context,\n\t                'h.'+str(i),\n\t                self.embd_size,\n\t                self.eps,\n\t                self.n_head,\n", "                self.max_length,\n\t            )\n\t            for i in range(self.n_layer)\n\t        ]\n\t        self.layer_norm = LayerNorm(self.context, 'ln_f', self.embd_size, self.eps)\n\t    def __call__(self, graph, input_ids, position_ids, step, attention_mask, sequence_length, **kwargs):\n\t        # TODO type hints  sequence length is int here\n\t        norm_type = kwargs.get('norm_type', 'ce')\n\t        softmax_type = kwargs.get('softmax_type', 'ce')\n\t        return_last = kwargs.get('return_last', False)\n", "        outline_blocks = kwargs.get('outline_blocks', 'single_block')\n\t        if outline_blocks:\n\t            self.logger.info('outlining transformer blocks')\n\t            self.logger.info('please make sure disable outlining in session options is set to False')\n\t        with self.device_scope(graph, 0, 0):\n\t            hidden_states = self.embedding(graph, input_ids, position_ids, sequence_length)\n\t        end_points = np.cumsum(self.layer_per_ipu)\n\t        for i in range(self.n_layer):\n\t            stage_offset = sum(i >= end_points)\n\t            if outline_blocks is None:\n", "                outline_attr = None\n\t            elif outline_blocks == 'single_block':\n\t                outline_attr = {'block': f'sub_{i}'}\n\t            elif outline_blocks == 'multi_block':\n\t                outline_attr = {'block': f'sub_{stage_offset}'}\n\t            else:\n\t                raise ValueError(f'invalid value {outline_blocks} for outline_blocks')\n\t            with self.device_scope(graph, stage_offset, stage_offset, outline_attr):\n\t                hidden_states = self.blocks[i](\n\t                    graph, hidden_states, sequence_length, step, attention_mask, norm_type, softmax_type)\n", "                self.logger.info(f'block {i} placed on IPU {stage_offset}')\n\t        with self.device_scope(graph, stage_offset, stage_offset):\n\t            if return_last:\n\t                hidden_states = ops.static_slice(graph, hidden_states, [0], [1], [1])\n\t            last_sequence_length = 1 if return_last else sequence_length\n\t            hidden_states = self.layer_norm(graph, hidden_states, last_sequence_length, norm_type)\n\t        return hidden_states, stage_offset\n\tclass GPT2DecModel(HFDecBaseModel):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n", "        # TODO support topk > 1 for TP\n\t        self.topk = kwargs.get('topk', 1)\n\t        if self.model_type == 'tp':\n\t            assert self.topk == 1\n\t        self.outline_blocks = kwargs.get('outline_blocks', True)\n\t        self.transformer = Transformer(\n\t            None,\n\t            'transformer',\n\t            self.hf_config.vocab_size,\n\t            self.hf_config.n_embd,\n", "            self.hf_config.layer_norm_epsilon,\n\t            self.hf_config.n_head,\n\t            self.max_length,\n\t            self.hf_config.n_layer,\n\t            self.layer_per_ipu,\n\t            self.hf_config.n_positions,\n\t        )\n\t        self.lm_head = LMHead(\n\t            context=None,\n\t            name='lm_head',\n", "            topk=self.topk,\n\t            vocab_size=self.hf_config.vocab_size,\n\t            embedding_size=self.hf_config.n_embd,\n\t            tie_weight=self.transformer.embedding.wte.weight_id\n\t        )\n\t        self.lm_head.set_virtual_id(0)\n\t    def process_hf_model_state_dict(self):\n\t        with torch.no_grad():\n\t            n_embd = self.hf_config.n_embd\n\t            head_size = n_embd // self.hf_config.n_head\n", "            scale_value = float(1 / head_size ** 0.5)\n\t            for block in self.hf_model.transformer.h:\n\t                block.attn.c_attn.weight[:, :n_embd] = block.attn.c_attn.weight[:, :n_embd] * scale_value\n\t                block.attn.c_attn.bias[: n_embd] = block.attn.c_attn.bias[:n_embd] * scale_value\n\t                block.attn.c_attn.weight.transpose_(0, 1)\n\t                block.attn.c_proj.weight.transpose_(0, 1)\n\t                block.mlp.c_fc.weight.transpose_(0, 1)\n\t                block.mlp.c_proj.weight.transpose_(0, 1)\n\t        self.logger.info('prescale applied')\n\t    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n", "        input_ids_container = model_graph_inputs['input_ids_container']\n\t        attention_mask = model_graph_inputs['attention_mask']\n\t        step = model_graph_inputs['step']\n\t        with self.device_scope(model_graph, 0, 0):\n\t            input_ids = ops.dynamic_slice(model_graph, input_ids_container, step, axes=[1], sizes=[sequence_length])\n\t            temp_attention_mask = ops.unsqueeze(model_graph, attention_mask, [1, 2])\n\t            if sequence_length != 1:\n\t                position_ids_value = np.array([np.arange(self.max_length)] * self.batch_size)\n\t                position_ids_container = ops.constant(\n\t                    model_graph, position_ids_value.astype(np.int32), 'position_ids')\n", "                position_ids = ops.dynamic_slice(model_graph, position_ids_container, step, [1], [sequence_length])\n\t            else:\n\t                position_ids = ops.unsqueeze(model_graph, step, [0])\n\t        logits, stage_offset = self.transformer(\n\t            model_graph,\n\t            input_ids,\n\t            position_ids,\n\t            step,\n\t            temp_attention_mask,\n\t            sequence_length=sequence_length,\n", "            outline_blocks=self.outline_blocks\n\t        )\n\t        with self.device_scope(model_graph, pipeline_stage_id=stage_offset):\n\t            next_ids = self.lm_head(\n\t                model_graph,\n\t                logits,\n\t                sequence_length=sequence_length\n\t            )\n\t        model_outputs =  {'next_ids': next_ids, 'stage_offset': stage_offset}\n\t        return model_outputs\n", "    def build_input_dict(self, **kwargs):\n\t        input_string_list = list(kwargs.get('input_string_list', []))\n\t        batch_size = self.batch_size * self.batch_per_step\n\t        if len(input_string_list) >= batch_size:\n\t            input_string_list = input_string_list[:batch_size]\n\t            self.logger.info('num input strings is larger than batch size, truncating')\n\t        else:\n\t            input_string_list.extend([''] * (batch_size - len(input_string_list)))\n\t            self.logger.info('num input string is smaller than batch size, adding fake inputs')\n\t        inputs = self.hf_tokenizer(\n", "            input_string_list,\n\t            max_length=self.max_length,\n\t            padding='max_length',\n\t            return_tensors='np',\n\t            return_attention_mask=False,\n\t            add_special_tokens=False\n\t        )\n\t        return {'input_ids': inputs['input_ids']}\n\t    def build_output_dict(self, anchor_arrays):\n\t        output, decode_step = anchor_arrays['Loop:0'], anchor_arrays['Loop:1']\n", "        if len(output.shape) == 3:\n\t            output, decode_step = output[0], decode_step[0]\n\t        output = self.hf_tokenizer.batch_decode(output, skip_special_tokens=True)\n\t        output = {str(i): v for i, v in enumerate(output)}\n\t        return {'output': output, 'decode_step': decode_step}\n"]}
{"filename": "poptransformer/models/gpt2/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer, Embedding\n\tclass BaseTransformerEmbedding(BaseLayer):\n\t    def __init__(self, context, name, vocab_size, embd_size, max_position):\n\t        super().__init__(context, name)\n\t        self.vocab_size = vocab_size\n\t        self.embd_size = embd_size\n\t        self.max_position = max_position\n", "        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.wte = Embedding(self.context, 'wte', self.vocab_size, self.embd_size)\n\t        self.wpe = Embedding(self.context, 'wpe', self.max_position, self.embd_size)\n\t    def __call__(self, graph, input_ids, position_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            input_embeds = self.wte(graph, input_ids, sequence_length)\n\t            pos_embeds = self.wpe(graph, position_ids, sequence_length)\n\t            embeds = ops.add(graph, input_embeds, pos_embeds)\n\t        return ops.remap_tensor(graph, embeds)\n", "class TPTransformerEmbedding(BaseTransformerEmbedding):\n\t    def __call__(self, graph, input_ids, position_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            input_embeds = self.wte(graph, input_ids, sequence_length)\n\t            pos_embeds = self.wpe(graph, position_ids, sequence_length)\n\t            embeds = ops.add(graph, input_embeds, pos_embeds)\n\t            embeds = ops.remap_tensor(graph, embeds)\n\t            embeds = graph.aiGraphcore.replicatedallreduce([embeds])\n\t        return embeds\n\tclass TransformerEmbedding(TPTransformerEmbedding, BaseTransformerEmbedding):\n", "    layer_class_map = {\n\t        'tp': TPTransformerEmbedding,\n\t        'shard': BaseTransformerEmbedding}\n\t    def __init__(self, context, name, vocab_size, embd_size, max_position):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, vocab_size, embd_size, max_position)\n", "    def __call__(self, graph, input_ids, position_ids, sequence_length):\n\t        return self.layer_class.__call__(self, graph, input_ids, position_ids, sequence_length)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/gpt2/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer\n\tfrom poptransformer.layers import Linear\n\tclass BaseAttention(BaseLayer):\n\t    softmax_fn_map = {\n\t        'aionnx': ops.softmax,\n\t        'ce': ops.softmax_ce,\n\t    }\n", "    def __init__(self, context, name, input_size, num_head, cache_max_length):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.num_head = num_head\n\t        self.head_size = self.input_size // self.num_head\n\t        self.cache_max_length = cache_max_length\n\t        self.scale = input_size // num_head\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.c_attn = Linear(self.context, 'c_attn', self.input_size, self.input_size * 3)\n", "        self.c_proj = Linear(self.context, 'c_proj', self.input_size, self.input_size)\n\t    def forward_qkv(self, graph, x, step):\n\t        qkv = self.c_attn(graph, x)\n\t        temp_splits = [self.input_size, self.input_size * 2]\n\t        q, kv = ops.split(graph, qkv, num_outputs=2, axis=2, splits=temp_splits)\n\t        temp_q_shape = [self.batch_size, self.sequence_length, self.num_head, self.head_size]\n\t        q = ops.reshape(graph, q, temp_q_shape)\n\t        temp_kv_shape = [self.batch_size, self.sequence_length, 2, self.num_head, self.head_size]\n\t        kv = ops.reshape(graph, kv, temp_kv_shape)\n\t        q = ops.transpose(graph, q, [0, 2, 1, 3])\n", "        kv = ops.transpose(graph, kv, [2, 0, 3, 1, 4])\n\t        layer_past = ops.kv_cache(graph, step, kv, self.cache_max_length, 3, self.sequence_length)\n\t        layer_past = ops.remap_tensor(graph, layer_past)\n\t        if self.sequence_length != 1 and self.sequence_length < self.cache_max_length:\n\t            layer_past = ops.static_slice(graph, layer_past, [0], [self.sequence_length], [3])\n\t        k, v = ops.split(graph, layer_past, 2, axis=0, splits=[1, 1], name='split_past')\n\t        k = ops.squeeze(graph, k, [0])\n\t        v = ops.squeeze(graph, v, [0])\n\t        return q, k, v\n\t    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n", "        temp_k = ops.transpose(graph, k, [0, 1, 3, 2])\n\t        score = ops.matmul(graph, q, temp_k)\n\t        if self.sequence_length != 1:\n\t            score = ops.remap_tensor(graph, score, fwd_after_matmul=True)\n\t        score = ops.add(graph, score, attention_mask)\n\t        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n\t        if not softmax_fn:\n\t            raise ValueError(f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\")\n\t        score = softmax_fn(graph, score, -1, stable_mode=self.sequence_length != 1)\n\t        return score\n", "    def forward_output(self, graph, score, v):\n\t        score = ops.matmul(graph, score, v)\n\t        score = ops.transpose(graph, score, [0, 2, 1, 3])\n\t        score = ops.reshape(graph, score, [self.batch_size, self.sequence_length, self.input_size])\n\t        output = self.c_proj(graph, score)\n\t        return output\n\t    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n\t        with graph.nameScope(self.context):\n\t            self.sequence_length = sequence_length\n\t            q, k, v = self.forward_qkv(graph, x, step)\n", "            score = self.forward_attention(graph, q, k, attention_mask, softmax_type)\n\t            output = self.forward_output(graph, score, v)\n\t            return output\n\tclass TPAttention(BaseAttention):\n\t    def collect_bind_layer_weights(self):\n\t        qkv_tp_setting = {'strategy_name': 'fused_qkv'}\n\t        self.c_attn = Linear(self.context, 'c_attn', self.input_size, self.input_size * 3, **qkv_tp_setting)\n\t        proj_tp_setting = {'strategy_name': 'end'}\n\t        self.c_proj = Linear(self.context, 'c_proj', self.input_size, self.input_size, **proj_tp_setting)\n\t        self.input_size = self.input_size // self.num_replicas\n", "        self.num_head = self.num_head // self.num_replicas\n\tclass Attention(TPAttention, BaseAttention):\n\t    layer_class_map = {\n\t        'tp': TPAttention,\n\t        'shard': BaseAttention}\n\t    def __init__(self, context, name, input_size, num_head, cache_max_length):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n", "        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, input_size, num_head, cache_max_length)\n\t    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n\t        return self.layer_class.__call__(self, graph, x, step, attention_mask, sequence_length, softmax_type)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n\t    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n\t        return self.layer_class.forward_attention(self, graph, q, k, attention_mask, softmax_type)\n\t    def forward_qkv(self, graph, x, step):\n\t        return self.layer_class.forward_qkv(self, graph, x, step)\n", "    def forward_output(self, graph, score, v):\n\t        return self.layer_class.forward_output(self, graph, score, v)\n"]}
{"filename": "poptransformer/models/llama2/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# This is a re-implementation of Llama 2 by Graphcore Ltd\n\t# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\timport numpy as np\n\tfrom transformers import AutoTokenizer\n\tfrom transformers import AutoConfig\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer\n\tfrom poptransformer.models.llama2.embedding import TransformerEmbedding\n\tfrom poptransformer.models.llama2.mlp import MLP\n", "from poptransformer.layers.rms_layer_norm import RMSLayerNorm\n\tfrom poptransformer.models.llama2.attention import Attention\n\tfrom poptransformer.layers.lm_head import LMHead\n\tfrom poptransformer.models import HFDecBaseModel\n\tclass TransformerBlock(BaseLayer):\n\t    def __init__(self, context, name, input_size, eps, n_head,intermediate_size, max_length, fp8_cache=False):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.eps = eps\n\t        self.n_head = n_head\n", "        self.intermediate_size = intermediate_size\n\t        self.max_length = max_length\n\t        self.fp8_cache = fp8_cache\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.layer_norm1 = RMSLayerNorm(self.context, 'input_layernorm', self.input_size, self.eps)\n\t        self.attention = Attention(\n\t            self.context, 'self_attn', self.input_size, self.n_head, self.max_length, self.fp8_cache)\n\t        self.layer_norm2 = RMSLayerNorm(self.context, 'post_attention_layernorm', self.input_size, self.eps)\n\t        self.mlp = MLP(self.context, 'mlp', self.input_size, self.intermediate_size, 'swish')\n", "    def __call__(self, graph, x, sequence_length, step, attention_mask, norm_type='ce', softmax_type='ce', **kwargs):\n\t        with graph.nameScope(self.context):\n\t            temp_x = self.layer_norm1(graph, x)\n\t            temp_x = self.attention(graph, temp_x, step, attention_mask, sequence_length, softmax_type)\n\t            x = ops.add(graph, x, temp_x)\n\t            temp_x = self.layer_norm2(graph, x)\n\t            temp_x = self.mlp(graph, temp_x)\n\t            x = ops.add(graph, x, temp_x)\n\t        return x\n\tclass Transformer(BaseLayer):\n", "    def __init__(self, context, name, vocab_size, embd_size, eps,\n\t                 n_head, intermediate_size, max_length, n_layer, layer_per_ipu, fp8_cache=False):\n\t        super().__init__(context, name)\n\t        self.vocab_size = vocab_size\n\t        self.embd_size = embd_size\n\t        self.eps = eps\n\t        self.n_head = n_head\n\t        self.intermediate_size = intermediate_size\n\t        self.max_length = max_length\n\t        self.n_layer = n_layer\n", "        self.layer_per_ipu = layer_per_ipu\n\t        self.fp8_cache = fp8_cache\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.embedding = TransformerEmbedding(\n\t            self.context,\n\t            \"embed_tokens\",\n\t            self.vocab_size,\n\t            self.embd_size,\n\t        )\n", "        self.blocks = [\n\t            TransformerBlock(\n\t                self.context,\n\t                'layers.'+str(i),\n\t                self.embd_size,\n\t                self.eps,\n\t                self.n_head,\n\t                self.intermediate_size,\n\t                self.max_length,\n\t                self.fp8_cache\n", "            )\n\t            for i in range(self.n_layer)\n\t        ]\n\t        self.layer_norm = RMSLayerNorm(self.context, 'norm', self.embd_size, self.eps)\n\t    def __call__(self, graph, input_ids, position_ids, step, attention_mask, sequence_length, **kwargs):\n\t        norm_type = kwargs.get('norm_type', 'ce')\n\t        softmax_type = kwargs.get('softmax_type', 'ce')\n\t        outline_blocks = kwargs.get('outline_blocks', 'single_block')\n\t        if outline_blocks:\n\t            self.logger.info('outlining transformer blocks')\n", "            self.logger.info('please make sure disable outlining in session options is set to False')\n\t        with self.device_scope(graph,0,0):\n\t            hidden_states = self.embedding(graph, input_ids, sequence_length)\n\t        end_points = np.cumsum(self.layer_per_ipu)\n\t        for i in range(self.n_layer):\n\t            stage_offset = sum(i >= end_points)\n\t            with self.device_scope(graph, stage_offset, stage_offset):\n\t                hidden_states = self.blocks[i](\n\t                    graph, hidden_states, sequence_length, step, attention_mask, norm_type, softmax_type)\n\t                self.logger.info(f'block {i} placed on IPU {stage_offset}')\n", "        with self.device_scope(graph,stage_offset,stage_offset):\n\t            hidden_states = self.layer_norm(graph, hidden_states)\n\t        return hidden_states\n\tclass LLAMA2DecModel(HFDecBaseModel):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n\t        self.fp8_cache = kwargs.get('fp8_cache', False)\n\t        self.topk = kwargs.get('topk', 1)\n\t        self.outline_blocks = kwargs.get('outline_blocks', False)\n\t        self.transformer = Transformer(\n", "            None,\n\t            'model',\n\t            self.hf_config.vocab_size,\n\t            self.hf_config.hidden_size,\n\t            self.hf_config.rms_norm_eps,\n\t            self.hf_config.num_attention_heads,\n\t            self.hf_config.intermediate_size,\n\t            self.max_length,\n\t            self.hf_config.num_hidden_layers,\n\t            self.layer_per_ipu,\n", "            self.fp8_cache,\n\t        )\n\t        self.lm_head = LMHead(None, 'lm_head', self.topk, self.hf_config.vocab_size, self.hf_config.hidden_size)\n\t    def prepare_state_dict(self):\n\t        self.hf_tokenizer = AutoTokenizer.from_pretrained(\n\t            self.hf_model_name,\n\t            cache_dir=self.hf_cache_dir,\n\t            pad_token='[PAD]'\n\t        )\n\t        #TODO find out why need this setting of padding_side\n", "        self.hf_tokenizer.padding_side = \"right\"\n\t        self.logger.info(f'initialized tokenizer by model_name: {self.hf_model_name}')\n\t        if not self.override_hfconfig_from_json:\n\t            model_class = self.hf_model_class_name_map.get(self.hf_model_class_name, None)\n\t            assert model_class, f\"Invalid hf_model_class_name: {self.hf_model_class_name}\"\n\t            assert self.hf_model_name, f\"Invalid hf_model_name: {self.hf_model_name}\"\n\t            self.logger.info(f'initializing hf model class: {model_class.__name__}')\n\t            self.hf_model = model_class.from_pretrained(self.hf_model_name, cache_dir=self.hf_cache_dir)\n\t            self.logger.info(f'loading pretrained hf model: {self.hf_model_name}')\n\t            self.hf_config = self.hf_model.config\n", "            if self.precision != 'fp32':\n\t                self.hf_model.half()\n\t                self.logger.info(f'casting model to {self.precision}')\n\t            self.state_dict = self.hf_model.state_dict()\n\t            self.register_state_dict()\n\t        else:\n\t            self.logger.info('using overrided config, no state dict loaded')\n\t            self.hf_config = AutoConfig.from_pretrained(self.override_hfconfig_from_json)\n\t            self.state_dict = {}\n\t    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n", "        input_ids_container = model_graph_inputs['input_ids_container']\n\t        attention_mask = model_graph_inputs['attention_mask']\n\t        step = model_graph_inputs['step']\n\t        with self.device_scope(model_graph,0,0):\n\t            input_ids = ops.dynamic_slice(model_graph, input_ids_container, step, axes=[1], sizes=[sequence_length])\n\t            temp_attention_mask = ops.unsqueeze(model_graph, attention_mask, [1, 2])\n\t            if sequence_length != 1:\n\t                position_ids_value = np.array([np.arange(self.max_length)] * self.batch_size)\n\t                position_ids_container = ops.constant(\n\t                    model_graph, position_ids_value.astype(np.int32), 'position_ids')\n", "                position_ids = ops.dynamic_slice(model_graph, position_ids_container, step, [1], [sequence_length])\n\t            else:\n\t                position_ids = ops.unsqueeze(model_graph, step, [0])\n\t        logits = self.transformer(\n\t            model_graph,\n\t            input_ids,\n\t            position_ids,\n\t            step,\n\t            temp_attention_mask,\n\t            sequence_length=sequence_length,\n", "            outline_blocks=self.outline_blocks\n\t        )\n\t        with self.device_scope(model_graph,len(self.layer_per_ipu)-1,len(self.layer_per_ipu)-1):\n\t            self.lm_head.set_virtual_id(len(self.layer_per_ipu)-1)\n\t            next_ids = self.lm_head(\n\t                model_graph,\n\t                logits,\n\t                sequence_length=sequence_length\n\t            )\n\t        model_outputs =  {'next_ids': next_ids, 'stage_offset': len(self.layer_per_ipu)}\n", "        return model_outputs\n\t    def build_input_dict(self, **kwargs):\n\t        input_string_list = list(kwargs.get('input_string_list', []))\n\t        batch_size = self.batch_size * self.batch_per_step\n\t        if len(input_string_list) >= batch_size:\n\t            input_string_list = input_string_list[:batch_size]\n\t            self.logger.info('num input strings is larger than batch size, truncating')\n\t        else:\n\t            input_string_list.extend([input_string_list[0]] * (batch_size - len(input_string_list)))\n\t            self.logger.info('num input string is smaller than batch size, adding fake inputs')\n", "        inputs = self.hf_tokenizer(\n\t            input_string_list,\n\t            max_length=self.max_length,\n\t            padding='max_length',\n\t            return_tensors='np',\n\t            return_attention_mask=False,\n\t            add_special_tokens=False\n\t        )\n\t        return {'input_ids': inputs['input_ids']}\n\t    def build_output_dict(self, anchor_arrays):\n", "        output, decode_step = anchor_arrays['Loop:0'], anchor_arrays['Loop:1']\n\t        if len(output.shape) == 3:\n\t            output, decode_step = output[0], decode_step[0]\n\t            # TODO try deal with this automatically\n\t        if len(output.shape) == 4:\n\t            output, decode_step = output[1][0], decode_step[0][0]\n\t        output = self.hf_tokenizer.batch_decode(output, skip_special_tokens=True)\n\t        output = {str(i): v for i, v in enumerate(output)}\n\t        return {'output': output, 'decode_step': decode_step}\n"]}
{"filename": "poptransformer/models/llama2/embedding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# This is a re-implementation of Llama 2 by Graphcore Ltd\n\t# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer, Embedding\n\tclass BaseTransformerEmbedding(BaseLayer):\n\t    def __init__(self, context, name, vocab_size, embd_size):\n\t        super().__init__(context, name)\n\t        self.vocab_size = vocab_size\n\t        self.embd_size = embd_size\n", "        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.wte = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            embeds = self.wte(graph, input_ids, sequence_length)\n\t        return  embeds\n\tclass TPTransformerEmbedding(BaseTransformerEmbedding):\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n", "            embeds = self.wte(graph, input_ids, sequence_length)\n\t            embeds = graph.aiGraphcore.replicatedallreduce([embeds])\n\t        return embeds\n\tclass TransformerEmbedding(TPTransformerEmbedding, BaseTransformerEmbedding):\n\t    layer_class_map = {\n\t        'tp': TPTransformerEmbedding,\n\t        'shard': BaseTransformerEmbedding}\n\t    def __init__(self, context, name, vocab_size, embd_size):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n", "        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, vocab_size, embd_size)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/llama2/mlp.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# This is a re-implementation of Llama 2 by Graphcore Ltd\n\t# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import Linear\n\tfrom poptransformer.layers.mlp import BaseMLP\n\tclass ShardMLP(BaseMLP):\n\t    act_fn_map = {\n\t        'gelu': ops.gelu,\n\t        'swish': ops.swish\n", "    }\n\t    def collect_bind_layer_weights(self):\n\t        self.gate_proj = Linear(self.context, 'gate_proj', self.input_size, self.hidden_size, use_bias=False)\n\t        self.up_proj = Linear(self.context, 'up_proj', self.input_size, self.hidden_size, use_bias=False)\n\t        self.down_proj = Linear(self.context, 'down_proj', self.hidden_size, self.input_size, use_bias=False)\n\t    def __call__(self, graph, x):\n\t        x = ops.reshape(graph, x, [-1, self.input_size])\n\t        gate_output = self.gate_proj(graph, x)\n\t        gate_output = self.act_fn(graph, gate_output)\n\t        up_output = self.up_proj(graph, x)\n", "        up_output = ops.mul(graph, up_output, gate_output)\n\t        output = self.down_proj(graph, up_output)\n\t        output = ops.reshape(graph, output, [self.batch_size, -1, self.input_size])\n\t        return output\n\tclass TPMLP(ShardMLP):\n\t    def collect_bind_layer_weights(self):\n\t        gate_tp_settings = {\n\t            'strategy_name': 'start',\n\t        }\n\t        up_proj_tp_settings = {\n", "            'strategy_name': 'start',\n\t        }\n\t        down_proj_tp_settings = {\n\t            'strategy_name': 'end',\n\t        }\n\t        self.gate_proj = Linear(\n\t            self.context, 'gate_proj', self.input_size, self.hidden_size, use_bias=False, **gate_tp_settings)\n\t        self.up_proj = Linear(\n\t            self.context, 'up_proj', self.input_size, self.hidden_size, use_bias=False, **up_proj_tp_settings)\n\t        self.down_proj = Linear(\n", "            self.context, 'down_proj', self.hidden_size, self.input_size, use_bias=False, **down_proj_tp_settings)\n\tclass MLP(TPMLP, ShardMLP):\n\t    layer_class_map = {\n\t        'tp': TPMLP,\n\t        'shard': ShardMLP}\n\t    def __init__(self, context, name, input_size, hidden_size, act_fn):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}\")\n", "        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, input_size, hidden_size, act_fn)\n\t    def __call__(self, graph, x):\n\t        return self.layer_class.__call__(self, graph, x)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/llama2/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# This is a re-implementation of Llama 2 by Graphcore Ltd\n\t# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n\timport math\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.utils import shard, repeat, shard_fused_qkv\n\tfrom poptransformer.layers import BaseLayer\n\tfrom poptransformer.layers import Linear\n\tclass BaseAttention(BaseLayer):\n", "    softmax_fn_map = {\n\t        'aionnx': ops.softmax,\n\t        'ce': ops.softmax_ce,\n\t    }\n\t    def __init__(self, context, name, input_size, num_head, cache_max_length, fp8_cache=False):\n\t        super().__init__(context, name)\n\t        self.input_size = input_size\n\t        self.num_head = num_head\n\t        self.head_size = self.input_size // self.num_head\n\t        self.cache_max_length = cache_max_length\n", "        self.fp8_cache = fp8_cache\n\t        self.scale = input_size // num_head\n\t        self.collect_bind_layer_weights()\n\t    def fixed_pos_embedding(self, graph, step, head_dim):\n\t        inv_freq_value = np.array(\n\t            [1.0 / (10000 ** (i / head_dim)) for i in range(0, head_dim, 2)]).astype(self.np_float_type)\n\t        inv_freq = ops.constant(graph, inv_freq_value, 'inv_freq')\n\t        inv_freq = ops.reshape(graph, inv_freq, [1, -1])\n\t        ind = ops.reshape(graph, step, [-1, 1])\n\t        ind = ops.cast(graph, ind, self.popart_float_type)\n", "        sinusoid_inp = ops.matmul(graph, ind, inv_freq)\n\t        return (graph.aiOnnx.sin([sinusoid_inp]), graph.aiOnnx.cos([sinusoid_inp]))\n\t    def rotate_half(self, graph, x, n_head, head_dim, batch_size=1):\n\t        x1, x2 = ops.split(graph, x, 2, 3, [head_dim//2, head_dim//2], \"split_rotate_every_two\")\n\t        x2 = ops.mul(graph, x2, ops.constant(graph, np.array([-1]).astype(self.np_float_type)))\n\t        x = ops.concat(graph, x2, x1, 3)\n\t        return ops.reshape(graph, x, [batch_size, n_head, 1, head_dim])\n\t    def apply_rotary_pos_emb(self, graph, q, k, sincos,  n_head, head_dim,batch_size=1):\n\t        sin = ops.concat(graph, sincos[0], sincos[0],1)\n\t        sin = ops.reshape(graph, sin, [1, 1, 1, -1])\n", "        cos = ops.concat(graph, sincos[1], sincos[1],1)\n\t        cos = ops.reshape(graph, cos, [1, 1, 1, -1])\n\t        q_rotate_every_two = self.rotate_half(graph, q, n_head, head_dim, batch_size)\n\t        q = ops.add(graph, ops.mul(graph, q, cos), ops.mul(graph, q_rotate_every_two, sin))\n\t        k_rotate_every_two = self.rotate_half(graph, k, n_head, head_dim, batch_size)\n\t        k = ops.add(graph, ops.mul(graph, k, cos), ops.mul(graph, k_rotate_every_two, sin))\n\t        return q, k\n\t    def collect_bind_layer_weights(self):\n\t        self.q_proj = Linear(self.context, 'q_proj', self.input_size, self.input_size, use_bias=False)\n\t        self.k_proj = Linear(self.context, 'k_proj', self.input_size, self.input_size, use_bias=False)\n", "        self.v_proj = Linear(self.context, 'v_proj', self.input_size, self.input_size, use_bias=False)\n\t        self.o_proj = Linear(self.context, 'o_proj', self.input_size, self.input_size, use_bias=False)\n\t    def forward_qkv(self, graph, x, step):\n\t        q = self.q_proj(graph, x)\n\t        k = self.k_proj(graph, x)\n\t        v = self.v_proj(graph, x)\n\t        q = ops.reshape(graph, q, [self.batch_size, self.sequence_length, self.num_head, self.head_size])\n\t        k = ops.reshape(graph, k, [self.batch_size, self.sequence_length, self.num_head, self.head_size])\n\t        v = ops.reshape(graph, v, [self.batch_size, self.sequence_length, self.num_head, self.head_size])\n\t        q = ops.transpose(graph, q, [0, 2, 1, 3])  # q: [B, N, L, H]\n", "        k = ops.transpose(graph, k, [0, 2, 1, 3])  # k: [B, N, L, H]\n\t        v = ops.transpose(graph, v, [0, 2, 1, 3])  # v: [B, N, L, H]\n\t        sincos = self.fixed_pos_embedding(graph, step, self.head_size)\n\t        q,k = self.apply_rotary_pos_emb(graph, q, k, sincos, self.num_head, self.head_size, self.batch_size)\n\t        kv = ops.concat(graph, k, v, 0)  #kv: [2, B, N, L, H]\n\t        kv = ops.reshape( graph, kv, [2, self.batch_size, self.num_head, self.sequence_length, self.head_size])\n\t        # layer_past: [2, B, N, L, h]\n\t        with graph.nameScope('attn_past_update'):\n\t            layer_past = ops.kv_cache(graph, step, kv, self.cache_max_length, 3, self.sequence_length)\n\t            layer_past_key, layer_past_value = ops.split(\n", "                graph, layer_past, 2, axis=0, splits=[1, 1], name='split_past'\n\t            )\n\t            layer_past_key = ops.squeeze(graph, layer_past_key, [0])\n\t            layer_past_value = ops.squeeze(graph, layer_past_value, [0])\n\t            layer_past_key_temp = ops.transpose(\n\t                graph, layer_past_key, [0, 1, 3, 2])\n\t        return  q, layer_past_key_temp, layer_past_value\n\t    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n\t        w = ops.matmul(graph, q, k)\n\t        w = ops.mul(graph, w, ops.constant(graph, np.array([1/math.sqrt(self.head_size)]).astype(self.np_float_type)))\n", "        w = ops.add(graph, w, attention_mask)\n\t        w = ops.cast(graph, w, 'FLOAT')\n\t        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n\t        if not softmax_fn:\n\t            raise ValueError(f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\")\n\t        w = softmax_fn(graph, w, -1, stable_mode=self.sequence_length != 1)\n\t        w = ops.cast(graph, w, self.popart_float_type)\n\t        return w\n\t    def forward_output(self, graph, score, v):\n\t        a = ops.matmul(graph, score, v)\n", "        a = ops.transpose(graph, a, [0, 2, 1, 3])\n\t        a = ops.reshape(graph, a, [self.batch_size, self.sequence_length, -1])\n\t        return self.o_proj(graph, a)\n\t    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n\t        with graph.nameScope(self.context):\n\t            self.sequence_length = sequence_length\n\t            q, k, v = self.forward_qkv(graph, x, step)\n\t            score = self.forward_attention(graph, q, k, attention_mask, softmax_type)\n\t            output = self.forward_output(graph, score, v)\n\t            return output\n", "class TPAttention(BaseAttention):\n\t    def collect_bind_layer_weights(self):\n\t        self.num_head_beforeTP = self.num_head\n\t        self.num_head = math.ceil(self.num_head / self.num_replicas)\n\t        assert self.num_head_beforeTP == self.num_head * self.num_replicas\n\t        qkv_tp_settings = {\n\t            'strategy_name': 'start',\n\t        }\n\t        proj_tp_setting = {\n\t            'strategy_name': 'end',\n", "        }\n\t        self.q_proj = Linear(\n\t            self.context, 'q_proj', self.input_size, self.input_size, use_bias=False, **qkv_tp_settings)\n\t        self.k_proj = Linear(\n\t            self.context, 'k_proj', self.input_size, self.input_size, use_bias=False, **qkv_tp_settings)\n\t        self.v_proj = Linear(\n\t            self.context, 'v_proj', self.input_size, self.input_size, use_bias=False, **qkv_tp_settings)\n\t        self.o_proj = Linear(\n\t            self.context, 'o_proj', self.input_size, self.input_size, use_bias=False, **proj_tp_setting)\n\tclass Attention(TPAttention, BaseAttention):\n", "    layer_class_map = {\n\t        'tp': TPAttention,\n\t        'shard': BaseAttention}\n\t    def __init__(self, context, name, input_size, num_head, cache_max_length, fp8_cache=False):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\")\n\t        self.logger.debug(f'initializing model type: {self.layer_class.__name__}')\n\t        super().__init__(context, name, input_size, num_head, cache_max_length, fp8_cache)\n", "    def __call__(self, graph, x, step, attention_mask, sequence_length, softmax_type='ce'):\n\t        return self.layer_class.__call__(self, graph, x, step, attention_mask, sequence_length, softmax_type)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n\t    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n\t        return self.layer_class.forward_attention(self, graph, q, k, attention_mask, softmax_type)\n\t    def forward_qkv(self, graph, x, step):\n\t        return self.layer_class.forward_qkv(self, graph, x, step)\n\t    def forward_output(self, graph, score, v):\n\t        return self.layer_class.forward_output(self, graph, score, v)\n"]}
{"filename": "poptransformer/models/chatglm2/model.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport torch\n\timport numpy as np\n\tfrom transformers import AutoModel, AutoConfig, AutoTokenizer\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer, LMHead\n\tfrom poptransformer.layers.rms_layer_norm import RMSLayerNorm\n\tfrom poptransformer.models.chatglm2.emebdding import TransformerEmbedding\n\tfrom poptransformer.models.chatglm2.attention import MultiQueryAttention\n", "from poptransformer.models.chatglm2.mlp import MLP\n\tfrom poptransformer.models import HFDecBaseModel\n\tfrom poptransformer.utils import REGISTRY\n\tdef rearrange_tp_weights(weight, num_heads, num_replicas, axis=0):\n\t    hidden_size = weight.shape[axis]\n\t    offset = hidden_size // 2\n\t    head_size = hidden_size // num_heads\n\t    group_size = num_heads // num_replicas // 2\n\t    if axis == 1:\n\t        weight = weight.T\n", "    weights = []\n\t    # Divide weight into groups for each replica and rearange them.\n\t    for i in range(0, num_heads // 2, group_size):\n\t        weight_1 = weight[\n\t            i * head_size : (i + group_size) * head_size\n\t        ]\n\t        weight_2 = weight[\n\t            offset + i * head_size : offset + (i + group_size) * head_size\n\t        ]\n\t        weights.extend([weight_1, weight_2])\n", "    weight_new = torch.cat(weights, axis=0)\n\t    if axis == 1:\n\t        weight_new = weight_new.T\n\t    return weight_new\n\tclass ChatGLM2Block(BaseLayer):\n\t    def __init__(\n\t        self,\n\t        context,\n\t        name,\n\t        hidden_size,\n", "        layernorm_epsilon,\n\t        multi_query_group_num,\n\t        num_attention_heads,\n\t        ffn_hidden_size,\n\t        max_length,\n\t        layer_number,\n\t    ):\n\t        super().__init__(context, name)\n\t        self.hidden_size = hidden_size\n\t        self.multi_query_group_num = multi_query_group_num\n", "        self.num_attention_heads = num_attention_heads\n\t        self.ffn_hidden_size = ffn_hidden_size\n\t        self.layernorm_epsilon = layernorm_epsilon\n\t        self.max_length = max_length\n\t        self.layer_number = layer_number\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.layer_norm1 = RMSLayerNorm(\n\t            self.context, \"input_layernorm\", self.hidden_size, self.layernorm_epsilon\n\t        )\n", "        self.attention = MultiQueryAttention(\n\t            self.context,\n\t            \"self_attention\",\n\t            self.hidden_size,\n\t            self.multi_query_group_num,\n\t            self.num_attention_heads,\n\t            self.max_length,\n\t            self.layer_number,\n\t        )\n\t        self.layer_norm2 = RMSLayerNorm(\n", "            self.context,\n\t            \"post_attention_layernorm\",\n\t            self.hidden_size,\n\t            self.layernorm_epsilon,\n\t        )\n\t        self.mlp = MLP(self.context, \"mlp\", self.hidden_size, self.ffn_hidden_size)\n\t    def __call__(\n\t        self,\n\t        graph,\n\t        x,\n", "        sequence_length,\n\t        step,\n\t        attention_mask,\n\t        softmax_type=\"ce\",\n\t        **kwargs,\n\t    ):\n\t        matmul_kwargs = {\"amp\": 0.4, \"partialtype\": \"half\"}\n\t        with graph.nameScope(self.context):\n\t            temp_x = self.layer_norm1(graph, x)\n\t            with self.option_scope(**matmul_kwargs):\n", "                temp_x = self.attention(\n\t                    graph, temp_x, step, attention_mask, sequence_length, softmax_type\n\t                )\n\t            x = ops.add(graph, x, temp_x)\n\t            temp_x = self.layer_norm2(graph, x)\n\t            with self.option_scope(**matmul_kwargs):\n\t                temp_x = self.mlp(graph, temp_x)\n\t            x = ops.add(graph, x, temp_x)\n\t        return x\n\tclass Transformer(BaseLayer):\n", "    def __init__(\n\t        self,\n\t        context,\n\t        name,\n\t        padded_vocab_size,\n\t        num_embedding_partitions,\n\t        hidden_size,\n\t        layernorm_epsilon,\n\t        multi_query_group_num,\n\t        num_attention_heads,\n", "        ffn_hidden_size,\n\t        max_length,\n\t        num_layers,\n\t        layer_per_ipu,\n\t    ):\n\t        super().__init__(context, name)\n\t        self.padded_vocab_size = padded_vocab_size\n\t        self.num_embedding_partitions = num_embedding_partitions\n\t        self.hidden_size = hidden_size\n\t        self.layernorm_epsilon = layernorm_epsilon\n", "        self.multi_query_group_num = multi_query_group_num\n\t        self.num_attention_heads = num_attention_heads\n\t        self.ffn_hidden_size = ffn_hidden_size\n\t        self.max_length = max_length\n\t        self.num_layers = num_layers\n\t        self.layer_per_ipu = layer_per_ipu\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.embedding = TransformerEmbedding(\n\t            self.context,\n", "            \"embedding.word_embeddings\",\n\t            self.padded_vocab_size,\n\t            self.hidden_size,\n\t        )\n\t        self.blocks = [\n\t            ChatGLM2Block(\n\t                self.context,\n\t                \"encoder.layers.\" + str(i),\n\t                self.hidden_size,\n\t                self.layernorm_epsilon,\n", "                self.multi_query_group_num,\n\t                self.num_attention_heads,\n\t                self.ffn_hidden_size,\n\t                self.max_length,\n\t                layer_number=i + 1,\n\t            )\n\t            for i in range(self.num_layers)\n\t        ]\n\t        self.layer_norm = RMSLayerNorm(\n\t            self.context,\n", "            \"encoder.final_layernorm\",\n\t            self.hidden_size,\n\t            self.layernorm_epsilon,\n\t        )\n\t    def __call__(\n\t        self,\n\t        graph,\n\t        input_ids,\n\t        position_ids,\n\t        step,\n", "        attention_mask,\n\t        sequence_length,\n\t        **kwargs,\n\t    ):\n\t        softmax_type = kwargs.get(\"softmax_type\", \"ce\")\n\t        with self.device_scope(graph, 0, 0):\n\t            hidden_states = self.embedding(graph, input_ids, sequence_length)\n\t        end_points = np.cumsum(self.layer_per_ipu)\n\t        for i in range(self.num_layers):\n\t            stage_offset = sum(i >= end_points)\n", "            with self.device_scope(graph, stage_offset, stage_offset):\n\t                hidden_states = self.blocks[i](\n\t                    graph,\n\t                    hidden_states,\n\t                    sequence_length,\n\t                    step,\n\t                    attention_mask,\n\t                    softmax_type,\n\t                )\n\t                self.logger.info(f\"block {i} placed on IPU {stage_offset}\")\n", "        with self.device_scope(graph, stage_offset, stage_offset):\n\t            hidden_states = self.layer_norm(\n\t                graph, hidden_states\n\t            )\n\t        return hidden_states\n\tclass ChatGLM2DecModel(HFDecBaseModel):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n\t        self.outline_blocks = kwargs.get(\"outline_blocks\", False)\n\t        self.num_embedding_partitions = kwargs.get(\"num_embedding_partitions\", 1)\n", "        self.transformer = Transformer(\n\t            None,\n\t            \"transformer\",\n\t            self.hf_config.padded_vocab_size,\n\t            self.num_embedding_partitions,\n\t            self.hf_config.hidden_size,\n\t            self.hf_config.layernorm_epsilon,\n\t            self.hf_config.multi_query_group_num,\n\t            self.hf_config.num_attention_heads,\n\t            self.hf_config.ffn_hidden_size,\n", "            self.max_length,\n\t            self.hf_config.num_layers,\n\t            self.layer_per_ipu,\n\t        )\n\t        self.lm_head = LMHead(\n\t            None,\n\t            \"transformer.output_layer\",\n\t            1,\n\t            self.hf_config.padded_vocab_size,\n\t            self.hf_config.hidden_size,\n", "        )\n\t    def prepare_state_dict(self):\n\t        hf_args = {\n\t            \"pretrained_model_name_or_path\": self.hf_model_name,\n\t            \"trust_remote_code\": True,\n\t            \"cache_dir\": self.hf_cache_dir,\n\t        }\n\t        self.hf_tokenizer = AutoTokenizer.from_pretrained(**hf_args)\n\t        self.logger.info(f\"initialized tokenizer by model_name: {self.hf_model_name}\")\n\t        self.hf_config = AutoConfig.from_pretrained(**hf_args)\n", "        # self.hf_config.num_layers = 4\n\t        self.logger.info(f\"loading pretrained hf model: {self.hf_model_name}\")\n\t        self.hf_model = AutoModel.from_pretrained(**hf_args).eval()\n\t        if self.precision != \"fp32\":\n\t            self.hf_model.half()\n\t            self.logger.info(f\"casting model to {self.precision}\")\n\t        self.state_dict = self.hf_model.state_dict()\n\t        tensor_names = list(self.state_dict.keys())\n\t        for k in tensor_names:\n\t            # Split qkv weight into q & kv for the need of TP mode\n", "            if \"query_key_value\" in k:\n\t                weight_np = self.state_dict.pop(k)\n\t                weight_1, weight_2 = np.split(\n\t                    weight_np, [self.hf_config.hidden_size], axis=0\n\t                )\n\t                self.state_dict[k.replace(\"query_key_value\", \"query\")] = weight_1\n\t                self.state_dict[k.replace(\"query_key_value\", \"key_value\")] = weight_2\n\t            if \"dense_h_to_4h\" in k:\n\t                weight_np = self.state_dict.pop(k)\n\t                weight_1, weight_2 = weight_np.chunk(2, dim=0)\n", "                self.state_dict[\n\t                    k.replace(\"dense_h_to_4h\", \"dense_h_to_4h_1\")\n\t                ] = weight_1\n\t                self.state_dict[\n\t                    k.replace(\"dense_h_to_4h\", \"dense_h_to_4h_2\")\n\t                ] = weight_2\n\t        REGISTRY.register(\"query_size\", self.hf_config.hidden_size)\n\t        REGISTRY.register(\n\t            \"key_value_size\",\n\t            self.hf_config.multi_query_group_num * self.hf_config.kv_channels,\n", "        )\n\t        # Rearange weights for tp mode\n\t        if self.model_type == \"tp\":\n\t            for k in self.state_dict.keys():\n\t                if \"query\" in k or (\"dense.weight\" in k and \"scale\" not in k):\n\t                    self.state_dict[k] = rearrange_tp_weights(\n\t                        self.state_dict[k],\n\t                        self.hf_config.num_attention_heads,\n\t                        num_replicas=self.num_replicas,\n\t                        axis=0 if \"query\" in k else 1,\n", "                    )\n\t        self.register_state_dict()\n\t    def build_model_graph(self, model_graph, model_graph_inputs, sequence_length=1):\n\t        input_ids_container = model_graph_inputs[\"input_ids_container\"]\n\t        attention_mask = model_graph_inputs[\"attention_mask\"]\n\t        step = model_graph_inputs[\"step\"]\n\t        with self.device_scope(model_graph, 0, 0):\n\t            input_ids = ops.dynamic_slice(\n\t                model_graph,\n\t                input_ids_container,\n", "                step,\n\t                axes=[1],\n\t                sizes=[sequence_length],\n\t            )\n\t            temp_attention_mask = ops.unsqueeze(model_graph, attention_mask, [1, 2])\n\t            if sequence_length != 1:\n\t                position_ids_value = np.array(\n\t                    [np.arange(self.max_length)] * self.batch_size\n\t                )\n\t                position_ids_container = ops.constant(\n", "                    model_graph, position_ids_value.astype(np.int32), \"position_ids\"\n\t                )\n\t                position_ids = ops.dynamic_slice(\n\t                    model_graph, position_ids_container, step, [1], [sequence_length]\n\t                )\n\t            else:\n\t                position_ids = ops.unsqueeze(model_graph, step, [0])\n\t        logits = self.transformer(\n\t            model_graph,\n\t            input_ids,\n", "            position_ids,\n\t            step,\n\t            temp_attention_mask,\n\t            sequence_length=sequence_length,\n\t            outline_blocks=self.outline_blocks,\n\t        )\n\t        with self.device_scope(\n\t            model_graph, len(self.layer_per_ipu) - 1, len(self.layer_per_ipu) - 1\n\t        ):\n\t            self.lm_head.set_virtual_id(len(self.layer_per_ipu) - 1)\n", "            next_ids = self.lm_head(\n\t                model_graph, logits, sequence_length=sequence_length\n\t            )\n\t        model_outputs = {\n\t            \"next_ids\": next_ids,\n\t            \"stage_offset\": self.transformer.num_layers + 1,\n\t        }\n\t        return model_outputs\n\t    def build_input_dict(self, **kwargs):\n\t        query = kwargs.get(\"input_string\", \"\")\n", "        prompt = self.hf_tokenizer.build_prompt(query, history=[])\n\t        inputs = self.hf_tokenizer([prompt], return_tensors=\"np\")\n\t        input_ids = inputs[\"input_ids\"]\n\t        input_ids_padding = np.zeros((1, self.max_length), dtype=np.int32)\n\t        input_ids_padding[:, : input_ids.shape[1]] = input_ids\n\t        return {\"input_ids\": input_ids_padding}\n\t    def build_output_dict(self, anchor_arrays):\n\t        output, decode_step = anchor_arrays[\"Loop:0\"], anchor_arrays[\"Loop:1\"]\n\t        if len(output.shape) == 3:\n\t            output, decode_step = output[0], decode_step[0]\n", "        output = self.hf_tokenizer.batch_decode(output, skip_special_tokens=True)\n\t        output = {str(i): v for i, v in enumerate(output)}\n\t        return {\"output\": output, \"decode_step\": decode_step}\n"]}
{"filename": "poptransformer/models/chatglm2/mlp.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import Linear\n\tfrom poptransformer.layers import BaseLayer\n\tclass BaseMLP(BaseLayer):\n\t    def __init__(self, context, name, hidden_size, ffn_hidden_size):\n\t        super().__init__(context, name)\n\t        self.hidden_size = hidden_size\n\t        self.ffn_hidden_size = ffn_hidden_size\n", "        self.act_fn = ops.swish\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n\t        self.gate_proj = Linear(\n\t            self.context,\n\t            \"dense_h_to_4h_1\",\n\t            self.hidden_size,\n\t            self.ffn_hidden_size,\n\t            use_bias=False,\n\t        )\n", "        self.up_proj = Linear(\n\t            self.context,\n\t            \"dense_h_to_4h_2\",\n\t            self.hidden_size,\n\t            self.ffn_hidden_size,\n\t            use_bias=False,\n\t        )\n\t        self.down_proj = Linear(\n\t            self.context,\n\t            \"dense_4h_to_h\",\n", "            self.ffn_hidden_size,\n\t            self.hidden_size,\n\t            use_bias=False,\n\t        )\n\t    def __call__(self, graph, x):\n\t        x = ops.reshape(graph, x, [-1, self.hidden_size])\n\t        gate_output = self.gate_proj(graph, x)\n\t        gate_output = self.act_fn(graph, gate_output)\n\t        up_output = self.up_proj(graph, x)\n\t        up_output = ops.mul(graph, up_output, gate_output)\n", "        output = self.down_proj(graph, up_output)\n\t        output = ops.reshape(graph, output, [self.batch_size, -1, self.hidden_size])\n\t        return output\n\tclass TPMLP(BaseMLP):\n\t    def collect_bind_layer_weights(self):\n\t        gate_tp_settings = {\n\t            \"strategy_name\": \"start\",\n\t        }\n\t        up_proj_tp_settings = {\n\t            \"strategy_name\": \"start\",\n", "        }\n\t        down_proj_tp_settings = {\n\t            \"strategy_name\": \"end\",\n\t        }\n\t        self.gate_proj = Linear(\n\t            self.context,\n\t            \"dense_h_to_4h_1\",\n\t            self.hidden_size,\n\t            self.ffn_hidden_size,\n\t            use_bias=False,\n", "            **gate_tp_settings,\n\t        )\n\t        self.up_proj = Linear(\n\t            self.context,\n\t            \"dense_h_to_4h_2\",\n\t            self.hidden_size,\n\t            self.ffn_hidden_size,\n\t            use_bias=False,\n\t            **up_proj_tp_settings,\n\t        )\n", "        self.down_proj = Linear(\n\t            self.context,\n\t            \"dense_4h_to_h\",\n\t            self.ffn_hidden_size,\n\t            self.hidden_size,\n\t            use_bias=False,\n\t            **down_proj_tp_settings,\n\t        )\n\tclass MLP(TPMLP, BaseMLP):\n\t    layer_class_map = {\"tp\": TPMLP, \"shard\": BaseMLP}\n", "    def __init__(self, context, name, hidden_size, ffn_hidden_size):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(f\"Invalid model_type {model_type}\")\n\t        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n\t        super().__init__(context, name, hidden_size, ffn_hidden_size)\n\t    def __call__(self, graph, x):\n\t        return self.layer_class.__call__(self, graph, x)\n\t    def collect_bind_layer_weights(self):\n", "        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/chatglm2/emebdding.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer, Embedding\n\tclass BaseTransformerEmbedding(BaseLayer):\n\t    def __init__(self, context, name, vocab_size, embd_size):\n\t        super().__init__(context, name)\n\t        self.vocab_size = vocab_size\n\t        self.embd_size = embd_size\n\t        self.collect_bind_layer_weights()\n", "    def collect_bind_layer_weights(self):\n\t        self.wte = Embedding(self.context, None, self.vocab_size, self.embd_size)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            embeds = self.wte(graph, input_ids, sequence_length)\n\t        return embeds\n\tclass TPTransformerEmbedding(BaseTransformerEmbedding):\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        with graph.nameScope(self.context):\n\t            embeds = self.wte(graph, input_ids, sequence_length)\n", "            embeds = graph.aiGraphcore.replicatedallreduce([embeds])\n\t        return embeds\n\tclass TransformerEmbedding(TPTransformerEmbedding, BaseTransformerEmbedding):\n\t    layer_class_map = {\"tp\": TPTransformerEmbedding, \"shard\": BaseTransformerEmbedding}\n\t    def __init__(self, context, name, vocab_size, embd_size):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n\t        if not self.layer_class:\n\t            raise ValueError(\n\t                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n", "            )\n\t        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n\t        super().__init__(context, name, vocab_size, embd_size)\n\t    def __call__(self, graph, input_ids, sequence_length):\n\t        return self.layer_class.__call__(self, graph, input_ids, sequence_length)\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n"]}
{"filename": "poptransformer/models/chatglm2/attention.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport math\n\timport numpy as np\n\tfrom poptransformer import ops\n\tfrom poptransformer.layers import BaseLayer\n\tfrom poptransformer.layers import Linear\n\tfrom poptransformer.layers.linear import BaseLinear\n\tclass BaseMultiQueryAttention(BaseLayer):\n\t    softmax_fn_map = {\n", "        \"aionnx\": ops.softmax,\n\t        \"ce\": ops.softmax_ce,\n\t    }\n\t    def __init__(\n\t        self,\n\t        context,\n\t        name,\n\t        hidden_size,\n\t        multi_query_group_num,\n\t        num_attention_heads,\n", "        cache_max_length,\n\t        layer_number,\n\t        add_qkv_bias,\n\t    ):\n\t        super().__init__(context, name)\n\t        self.hidden_size = hidden_size\n\t        self.multi_query_group_num = multi_query_group_num\n\t        self.num_attention_heads = num_attention_heads\n\t        self.hidden_size_per_attention_head = (\n\t            self.hidden_size // self.num_attention_heads\n", "        )\n\t        self.cache_max_length = cache_max_length\n\t        self.layer_number = layer_number\n\t        self.add_qkv_bias = add_qkv_bias\n\t        # 1 stage mode\n\t        self.input_length = 1\n\t        self.rotary_dim = self.hidden_size_per_attention_head\n\t        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)\n\t        self.collect_bind_layer_weights()\n\t    def collect_bind_layer_weights(self):\n", "        self.query = Linear(\n\t            self.context,\n\t            \"query\",\n\t            self.hidden_size,\n\t            self.hidden_size,\n\t            use_bias=self.add_qkv_bias,\n\t        )\n\t        self.key_value = Linear(\n\t            self.context,\n\t            \"key_value\",\n", "            self.hidden_size,\n\t            2 * self.hidden_size_per_attention_head * self.multi_query_group_num,\n\t            use_bias=self.add_qkv_bias,\n\t        )\n\t        self.dense = Linear(\n\t            self.context, \"dense\", self.hidden_size, self.hidden_size, use_bias=False\n\t        )\n\t    def fixed_pos_embedding(self, graph, position_id, dim):\n\t        # H = 4096, n = 32, h = 128, dim = 128 // 2 = 64\n\t        # cos, sin = [L, dim]\n", "        inv_freq_value = np.array(\n\t            [1.0 / (10000 ** (i / dim)) for i in range(0, dim, 2)]\n\t        ).astype(np.float32)\n\t        inv_freq = ops.constant(graph, inv_freq_value, \"inv_freq\")\n\t        inv_freq = ops.reshape(graph, inv_freq, [1, -1])\n\t        # position_id -> [L, B]\n\t        position_id = ops.reshape(graph, position_id, [-1, 1])\n\t        # Notice: fp16 precision is not suitable for large integers.\n\t        position_id = ops.cast(graph, position_id, \"FLOAT\")\n\t        freqs = ops.matmul(graph, position_id, inv_freq)\n", "        emb = ops.concat(graph, freqs, freqs, axis=-1)\n\t        # emb -> [L, dim] -> [1, L, 1, dim]\n\t        emb = ops.reshape(graph, emb, shape=[1, -1, 1, dim])\n\t        emb = ops.cast(graph, emb, self.popart_float_type)\n\t        cos, sin = graph.aiOnnx.cos([emb]), graph.aiOnnx.sin([emb])\n\t        return cos, sin\n\t    def rotate_half(self, graph, x):\n\t        x1, x2 = ops.split(\n\t            graph,\n\t            x,\n", "            num_outputs=2,\n\t            axis=-1,\n\t            splits=[self.rotary_dim // 4, self.rotary_dim // 4],\n\t            name=\"rope_split\",\n\t        )\n\t        x2 = ops.mul(graph, x2, ops.constant(graph, np.array([-1]).astype(np.float32)))\n\t        return ops.concat(graph, x2, x1, axis=-1)\n\t    def apply_rotary_pos_emb(self, graph, x, apply_rotary_pos_emb, num_heads):\n\t        # position_id: [B, L], x: [B, L, N, rotary_dim], sin, cos: [L, rotary_dim] -> [1, L, 1, rotary_dim]\n\t        cos, sin = apply_rotary_pos_emb\n", "        x = ops.reshape(\n\t            graph, x, shape=[self.batch_size, self.input_length, num_heads, -1, 2]\n\t        )\n\t        x = ops.transpose(graph, x, perm=[0, 1, 2, 4, 3])\n\t        x = ops.reshape(\n\t            graph, x, shape=[self.batch_size, self.input_length, num_heads, -1]\n\t        )\n\t        x = ops.add(\n\t            graph,\n\t            ops.mul(graph, x, cos),\n", "            ops.mul(graph, self.rotate_half(graph, x), sin),\n\t        )\n\t        x = ops.reshape(\n\t            graph, x, shape=[self.batch_size, self.input_length, num_heads, 2, -1]\n\t        )\n\t        x = ops.transpose(graph, x, perm=[0, 1, 2, 4, 3])\n\t        x = ops.reshape(\n\t            graph, x, shape=[self.batch_size, self.input_length, num_heads, -1]\n\t        )\n\t        return x\n", "    def rotary_embedding(self, graph, q, k, position_ids):\n\t        with graph.nameScope(\"build_rotary\"):\n\t            rotary_pos_emb = self.fixed_pos_embedding(\n\t                graph, position_ids, dim=self.rotary_dim // 2\n\t            )\n\t            q1, q2 = ops.split(\n\t                graph,\n\t                q,\n\t                num_outputs=2,\n\t                axis=-1,\n", "                splits=[self.rotary_dim // 2, self.rotary_dim // 2],\n\t                name=\"rope_split_q\",\n\t            )\n\t            k1, k2 = ops.split(\n\t                graph,\n\t                k,\n\t                num_outputs=2,\n\t                axis=-1,\n\t                splits=[self.rotary_dim // 2, self.rotary_dim // 2],\n\t                name=\"rope_split_k\",\n", "            )\n\t        with graph.nameScope(\"apply_rotary\"):\n\t            q1 = self.apply_rotary_pos_emb(\n\t                graph, q1, rotary_pos_emb, self.num_attention_heads\n\t            )\n\t            k1 = self.apply_rotary_pos_emb(\n\t                graph, k1, rotary_pos_emb, self.multi_query_group_num\n\t            )\n\t            q = ops.concat(graph, q1, q2, axis=-1)\n\t            k = ops.concat(graph, k1, k2, axis=-1)\n", "        return q, k\n\t    def forward_qkv(self, graph, x, step):\n\t        q = self.query(graph, x)\n\t        mixed_kv = self.key_value(graph, x)\n\t        k, v = ops.split(\n\t            graph,\n\t            mixed_kv,\n\t            num_outputs=2,\n\t            axis=-1,\n\t            splits=[\n", "                self.multi_query_group_num * self.hidden_size_per_attention_head,\n\t                self.multi_query_group_num * self.hidden_size_per_attention_head,\n\t            ],\n\t        )\n\t        q = ops.reshape(\n\t            graph,\n\t            q,\n\t            shape=[\n\t                self.batch_size,\n\t                self.sequence_length,\n", "                self.num_attention_heads,\n\t                self.hidden_size_per_attention_head,\n\t            ],\n\t        )\n\t        k = ops.reshape(\n\t            graph,\n\t            k,\n\t            shape=[\n\t                self.batch_size,\n\t                self.sequence_length,\n", "                self.multi_query_group_num,\n\t                self.hidden_size_per_attention_head,\n\t            ],\n\t        )\n\t        v = ops.reshape(\n\t            graph,\n\t            v,\n\t            shape=[\n\t                self.batch_size,\n\t                self.sequence_length,\n", "                self.multi_query_group_num,\n\t                self.hidden_size_per_attention_head,\n\t            ],\n\t        )\n\t        q, k = self.rotary_embedding(graph, q, k, position_ids=step)\n\t        # q = [B, N, L, h]\n\t        q = ops.transpose(graph, q, perm=[0, 2, 1, 3])\n\t        kv = ops.concat(graph, k, v, 0)\n\t        kv = ops.reshape(\n\t            graph,\n", "            kv,\n\t            [\n\t                2,\n\t                self.batch_size,\n\t                self.sequence_length,\n\t                self.multi_query_group_num,\n\t                self.hidden_size_per_attention_head,\n\t            ],\n\t        )\n\t        # kv = [2, B, n, L, h]\n", "        kv = ops.transpose(graph, kv, perm=[0, 1, 3, 2, 4])\n\t        with graph.nameScope(\"attn_past_update\"):\n\t            layer_past = ops.kv_cache(\n\t                graph, step, kv, self.cache_max_length, 3, self.sequence_length\n\t            )\n\t            k, v = ops.split(\n\t                graph, layer_past, 2, axis=0, splits=[1, 1], name=\"split_past\"\n\t            )\n\t            # k, v = [B, n, L, h]\n\t            k = ops.squeeze(graph, k, [0])\n", "            v = ops.squeeze(graph, v, [0])\n\t            # [B, n, L, h] -> [B, N, L, h]\n\t            k = ops.unsqueeze(graph, k, [2])\n\t            k = ops.expand(\n\t                graph,\n\t                k,\n\t                [1, 1, self.num_attention_heads // self.multi_query_group_num, 1, 1],\n\t            )\n\t            k = ops.reshape(\n\t                graph,\n", "                k,\n\t                shape=[\n\t                    self.batch_size,\n\t                    self.num_attention_heads,\n\t                    self.cache_max_length,\n\t                    self.hidden_size_per_attention_head,\n\t                ],\n\t            )\n\t            v = ops.unsqueeze(graph, v, [2])\n\t            v = ops.expand(\n", "                graph,\n\t                v,\n\t                [1, 1, self.num_attention_heads // self.multi_query_group_num, 1, 1],\n\t            )\n\t            v = ops.reshape(\n\t                graph,\n\t                v,\n\t                shape=[\n\t                    self.batch_size,\n\t                    self.num_attention_heads,\n", "                    self.cache_max_length,\n\t                    self.hidden_size_per_attention_head,\n\t                ],\n\t            )\n\t            # k = [B, N, h, L]\n\t            k = ops.transpose(graph, k, [0, 1, 3, 2])\n\t        return q, k, v\n\t    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n\t        attention_scores = ops.matmul(graph, q, k)\n\t        norm_factor = ops.constant(\n", "            graph, np.array([1.0 / self.norm_factor]).astype(self.np_float_type)\n\t        )\n\t        attention_scores = ops.mul(graph, attention_scores, norm_factor)\n\t        attention_scores = ops.add(graph, attention_scores, attention_mask)\n\t        softmax_fn = self.softmax_fn_map.get(softmax_type, None)\n\t        if not softmax_fn:\n\t            raise ValueError(\n\t                f\"Invalid softmax_fn {softmax_type}, options: {self.softmax_fn_map.keys()}\"\n\t            )\n\t        attention_probs = softmax_fn(\n", "            graph, attention_scores, -1, stable_mode=self.sequence_length != 1\n\t        )\n\t        attention_probs = ops.cast(graph, attention_probs, self.popart_float_type)\n\t        return attention_probs\n\t    def forward_output(self, graph, score, v):\n\t        context_layer = ops.matmul(graph, score, v)\n\t        context_layer = ops.transpose(graph, context_layer, [0, 2, 1, 3])\n\t        context_layer = ops.reshape(\n\t            graph, context_layer, [self.batch_size, self.sequence_length, -1]\n\t        )\n", "        output = self.dense(graph, context_layer)\n\t        return output\n\t    def __call__(\n\t        self, graph, x, step, attention_mask, sequence_length, softmax_type=\"ce\"\n\t    ):\n\t        with graph.nameScope(self.context):\n\t            self.sequence_length = sequence_length\n\t            q, k, v = self.forward_qkv(graph, x, step)\n\t            score = self.forward_attention(graph, q, k, attention_mask, softmax_type)\n\t            output = self.forward_output(graph, score, v)\n", "        return output\n\tclass TPMultiQueryAttention(BaseMultiQueryAttention):\n\t    def collect_bind_layer_weights(self):\n\t        qkv_tp_settings = {\n\t            \"strategy_name\": \"multi_query_qkv\",}\n\t        proj_tp_setting = {\n\t            \"strategy_name\": \"end\",\n\t        }\n\t        self.query = Linear(\n\t            self.context,\n", "            \"query\",\n\t            self.hidden_size,\n\t            self.hidden_size,\n\t            use_bias=self.add_qkv_bias,\n\t            **qkv_tp_settings,\n\t        )\n\t        self.key_value = BaseLinear(\n\t            self.context,\n\t            \"key_value\",\n\t            self.hidden_size,\n", "            2 * self.hidden_size_per_attention_head * self.multi_query_group_num,\n\t            use_bias=self.add_qkv_bias,\n\t        )\n\t        self.dense = Linear(\n\t            self.context,\n\t            \"dense\",\n\t            self.hidden_size,\n\t            self.hidden_size,\n\t            use_bias=False,\n\t            **proj_tp_setting,\n", "        )\n\t        self.num_attention_heads = self.num_attention_heads // self.num_replicas\n\tclass MultiQueryAttention(TPMultiQueryAttention, BaseMultiQueryAttention):\n\t    layer_class_map = {\n\t        \"tp\": TPMultiQueryAttention,\n\t        \"shard\": BaseMultiQueryAttention,\n\t    }\n\t    def __init__(\n\t        self,\n\t        context,\n", "        name,\n\t        hidden_size,\n\t        multi_query_group_num,\n\t        num_head,\n\t        cache_max_length,\n\t        layer_number,\n\t        add_qkv_bias=True,\n\t    ):\n\t        model_type = self.model_type\n\t        self.layer_class = self.layer_class_map.get(model_type, None)\n", "        if not self.layer_class:\n\t            raise ValueError(\n\t                f\"Invalid model_type {model_type}, options: {self.layer_class_map.keys()}\"\n\t            )\n\t        self.logger.debug(f\"initializing model type: {self.layer_class.__name__}\")\n\t        super().__init__(\n\t            context,\n\t            name,\n\t            hidden_size,\n\t            multi_query_group_num,\n", "            num_head,\n\t            cache_max_length,\n\t            layer_number,\n\t            add_qkv_bias,\n\t        )\n\t    def __call__(\n\t        self, graph, x, step, attention_mask, sequence_length, softmax_type=\"ce\"\n\t    ):\n\t        return self.layer_class.__call__(\n\t            self, graph, x, step, attention_mask, sequence_length, softmax_type\n", "        )\n\t    def collect_bind_layer_weights(self):\n\t        return self.layer_class.collect_bind_layer_weights(self)\n\t    def forward_attention(self, graph, q, k, attention_mask, softmax_type):\n\t        return self.layer_class.forward_attention(\n\t            self, graph, q, k, attention_mask, softmax_type\n\t        )\n\t    def forward_qkv(self, graph, x, step):\n\t        return self.layer_class.forward_qkv(self, graph, x, step)\n\t    def forward_output(self, graph, score, v):\n", "        return self.layer_class.forward_output(self, graph, score, v)\n"]}
{"filename": "examples/chatglm/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport time\n\timport sys\n\timport os\n\timport hydra\n\tsys.path.append(\"../../\")\n\tos.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n\tfrom poptransformer.utils import prepare_model_session\n\tdef run_inference(session, model, *args, **kwargs):\n", "    input_dict = model.build_input_dict(*args, **kwargs)\n\t    all_time = []\n\t    for _ in range(10):\n\t        session.run(input_dict)\n\t    for _ in range(10):\n\t        start = time.perf_counter()\n\t        session.run(input_dict)\n\t        end = time.perf_counter()\n\t        all_time.append(end - start)\n\t    output = model.build_output_dict(session.anchor_arrays)\n", "    model.logger.info(output)\n\t    decode_step = output[\"decode_step\"].item()\n\t    num_output_tokens = decode_step - model.input_length\n\t    latency = sum(all_time) / len(all_time)\n\t    latency_per_token = latency / decode_step\n\t    latency_per_output_token = latency / num_output_tokens\n\t    throughput = int(model.batch_size * num_output_tokens / latency)\n\t    performance = (\n\t        \"\\nPerformance: \\n\"\n\t        + f\"batch size: {model.batch_size}, precision: {model.precision}\\n\"\n", "        + f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\"\n\t        + f\"Latency per token: {latency_per_token*1000.0:.3f} ms/token \\n\"\n\t        + f\"Latency per output token: {latency_per_output_token*1000.0:.3f} ms/token\\n\"\n\t        + f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n\t    )\n\t    model.logger.info(performance)\n\t@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\n\tdef main(config):\n\t    session, model = prepare_model_session(config)\n\t    run_inference(session, model, **config.inputs)\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "examples/rwkv/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport time\n\timport sys\n\timport os\n\timport hydra\n\tsys.path.append('../../')\n\tos.environ['HYDRA_FULL_ERROR'] = '1'\n\tfrom poptransformer.utils import prepare_model_session\n\tdef run_inference(session, model, *args, **kwargs):\n", "    input_dict = model.build_input_dict(*args, **kwargs)\n\t    all_time = []\n\t    for _ in range(10):\n\t        session.run(input_dict)\n\t    for _ in range(10):\n\t        start = time.perf_counter()\n\t        session.run(input_dict)\n\t        end = time.perf_counter()\n\t        all_time.append(end-start)\n\t    output = model.build_output_dict(session.anchor_arrays)\n", "    decode_step = output['decode_step'].item()\n\t    latency = sum(all_time) / len(all_time)\n\t    latency_per_token = latency / (decode_step * model.batch_per_step)\n\t    throughput = int(model.batch_size * model.batch_per_step * model.max_length / latency)\n\t    model.logger.info(output)\n\t    performance = \"\\nPerformance: \\n\" + \\\n\t                f\"batch size: {model.batch_size}, precision: {model.precision}\\n\" + \\\n\t                f\"batch per step: {model.batch_per_step}\\n\" + \\\n\t                f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\" + \\\n\t                f\"Latency per token: {latency_per_token*1000.0:.3f} ms/token \\n\" + \\\n", "                f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n\t    model.logger.info(performance)\n\t@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\n\tdef main(config):\n\t    session, model = prepare_model_session(config)\n\t    run_inference(session, model, **config.inputs)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "examples/gpt2/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport time\n\timport sys\n\timport os\n\timport hydra\n\tsys.path.append('../../')\n\tos.environ['HYDRA_FULL_ERROR'] = '1'\n\tfrom poptransformer.utils import prepare_model_session\n\tdef run_inference(session, model, *args, **kwargs):\n", "    input_dict = model.build_input_dict(*args, **kwargs)\n\t    all_time = []\n\t    for _ in range(10):\n\t        session.run(input_dict)\n\t    for _ in range(10):\n\t        start = time.perf_counter()\n\t        session.run(input_dict)\n\t        end = time.perf_counter()\n\t        all_time.append(end-start)\n\t    output = model.build_output_dict(session.anchor_arrays)\n", "    decode_step = output['decode_step'].item()\n\t    latency = sum(all_time) / len(all_time)\n\t    latency_per_token = latency / (decode_step * model.batch_per_step)\n\t    throughput = int(model.batch_size * model.batch_per_step * model.max_length / latency)\n\t    model.logger.info(output)\n\t    performance = \"\\nPerformance: \\n\" + \\\n\t                f\"batch size: {model.batch_size}, precision: {model.precision}\\n\" + \\\n\t                f\"batch per step: {model.batch_per_step}\\n\" + \\\n\t                f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\" + \\\n\t                f\"Latency per token: {latency_per_token*1000.0:.3f} ms/token \\n\" + \\\n", "                f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n\t    model.logger.info(performance)\n\t@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\n\tdef main(config):\n\t    session, model = prepare_model_session(config)\n\t    run_inference(session, model, **config.inputs)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "examples/llama2/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport time\n\timport sys\n\timport os\n\timport hydra\n\tsys.path.append('../../')\n\tos.environ['HYDRA_FULL_ERROR'] = '1'\n\tfrom poptransformer.utils import prepare_model_session\n\tdef run_inference(session, model, *args, **kwargs):\n", "    input_dict = model.build_input_dict(*args, **kwargs)\n\t    all_time = []\n\t    for _ in range(1):\n\t        session.run(input_dict)\n\t    for _ in range(3):\n\t        start = time.perf_counter()\n\t        session.run(input_dict)\n\t        end = time.perf_counter()\n\t        all_time.append(end-start)\n\t    output = model.build_output_dict(session.anchor_arrays)\n", "    decode_step = output['decode_step'].item()\n\t    latency = sum(all_time) / len(all_time)\n\t    latency_per_token = latency / (decode_step * model.batch_per_step)\n\t    throughput = int(model.batch_size * model.batch_per_step * model.max_length / latency)\n\t    model.logger.info(output)\n\t    performance = \"\\nPerformance: \\n\" + \\\n\t                f\"batch size: {model.batch_size}, precision: {model.precision}\\n\" + \\\n\t                f\"batch per step: {model.batch_per_step}\\n\" + \\\n\t                f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\" + \\\n\t                f\"Latency per token: {latency_per_token*1000.0:.3f} ms/token \\n\" + \\\n", "                f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n\t    model.logger.info(performance)\n\t@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\n\tdef main(config):\n\t    session, model = prepare_model_session(config)\n\t    run_inference(session, model, **config.inputs)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "examples/chatglm2/inference.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport time\n\timport sys\n\timport os\n\timport hydra\n\tsys.path.append(\"../../\")\n\tos.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n\tfrom poptransformer.utils import prepare_model_session\n\tdef run_inference(session, model, *args, **kwargs):\n", "    input_dict = model.build_input_dict(*args, **kwargs)\n\t    all_time = []\n\t    for _ in range(10):\n\t        session.run(input_dict)\n\t    for _ in range(10):\n\t        start = time.perf_counter()\n\t        session.run(input_dict)\n\t        end = time.perf_counter()\n\t        all_time.append(end - start)\n\t    output = model.build_output_dict(session.anchor_arrays)\n", "    decode_step = output[\"decode_step\"].item()\n\t    num_output_tokens = decode_step\n\t    latency = sum(all_time) / len(all_time)\n\t    latency_per_output_token = latency / num_output_tokens\n\t    throughput = int(model.batch_size * num_output_tokens / latency)\n\t    model.logger.info(output)\n\t    performance = (\n\t        \"\\nPerformance: \\n\"\n\t        + f\"batch size: {model.batch_size}, precision: {model.precision}\\n\"\n\t        + f\"max_length: {model.max_length}, decode_step: {decode_step}\\n\"\n", "        + f\"Latency per output token: {latency_per_output_token*1000.0:.3f} ms/token\\n\"\n\t        + f\"Throughput: {throughput} token/s, Total Latency: {latency*1000.0:.3f} ms\\n\"\n\t    )\n\t    model.logger.info(performance)\n\t@hydra.main(version_base=None, config_path=\"conf\", config_name=\"sharding\")\n\tdef main(config):\n\t    session, model = prepare_model_session(config)\n\t    run_inference(session, model, **config.inputs)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
