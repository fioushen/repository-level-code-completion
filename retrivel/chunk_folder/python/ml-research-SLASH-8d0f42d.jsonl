{"filename": "src/einsum_wrapper.py", "chunked_list": ["import torch\n\timport torch.nn as nn   \n\timport numpy as np\n\tfrom EinsumNetwork import EinsumNetwork, Graph\n\tdevice = torch.device('cuda:0')\n\t#wrapper class to create an Einsum Network given a specific structure and parameters\n\tclass EiNet(EinsumNetwork.EinsumNetwork):\n\t    def __init__(self , \n\t                 use_em,\n\t                 structure = 'poon-domingos',\n", "                 pd_num_pieces = [4],\n\t                 depth = 8,\n\t                 num_repetitions = 20,\n\t                 num_var = 784,\n\t                 class_count = 3,\n\t                 K = 10,\n\t                 num_sums = 10,\n\t                 pd_height = 28,\n\t                 pd_width = 28,\n\t                 learn_prior = True\n", "                ):\n\t        # Structure\n\t        self.structure = structure        \n\t        self.class_count = class_count        \n\t        classes = np.arange(class_count)  # [0,1,2,..,n-1]\n\t        # Define the prior, i.e. P(C) and make it learnable.\n\t        self.learnable_prior = learn_prior\n\t        # P(C) is needed to apply the Bayes' theorem and to retrive\n\t        # P(C|X) = P(X|C)*(P(C) / P(X)\n\t        if self.class_count == 4:\n", "            self.prior = torch.tensor([(1/3)*(2/3), (1/3)*(2/3), (1/3)*(2/3), (1/3)], dtype=torch.float, requires_grad=True, device=device).log()\n\t        else:\n\t            self.prior = torch.ones(class_count, device=device, dtype=torch.float)\n\t        self.prior.fill_(1 / class_count)\n\t        self.prior.log_()\n\t        if self.learnable_prior:\n\t            print(\"P(C) is learnable.\")\n\t            self.prior.requires_grad_()\n\t        self.K = K\n\t        self.num_sums = num_sums\n", "        # 'poon-domingos'\n\t        self.pd_num_pieces = pd_num_pieces  # [10, 28],[4],[7]\n\t        self.pd_height = pd_height\n\t        self.pd_width = pd_width\n\t        # 'binary-trees'\n\t        self.depth = depth\n\t        self.num_repetitions = num_repetitions\n\t        self.num_var = num_var\n\t        # drop-out rate\n\t        # self.drop_out = drop_out\n", "        # print(\"The drop-out rate:\", self.drop_out)\n\t        # EM-settings\n\t        self.use_em = use_em\n\t        online_em_frequency = 1\n\t        online_em_stepsize = 0.05  # 0.05\n\t        print(\"train SPN with EM:\",self.use_em)\n\t        # exponential_family = EinsumNetwork.BinomialArray\n\t        # exponential_family = EinsumNetwork.CategoricalArray\n\t        exponential_family = EinsumNetwork.NormalArray\n\t        exponential_family_args = None\n", "        if exponential_family == EinsumNetwork.BinomialArray:\n\t            exponential_family_args = {'N': 255}\n\t        if exponential_family == EinsumNetwork.CategoricalArray:\n\t            exponential_family_args = {'K': 1366120}\n\t        if exponential_family == EinsumNetwork.NormalArray:\n\t            exponential_family_args = {'min_var': 1e-6, 'max_var': 0.1}\n\t        # Make EinsumNetwork\n\t        if self.structure == 'poon-domingos':\n\t            pd_delta = [[self.pd_height / d, self.pd_width / d] for d in self.pd_num_pieces]\n\t            graph = Graph.poon_domingos_structure(shape=(self.pd_height, self.pd_width), delta=pd_delta)\n", "        elif self.structure == 'binary-trees':\n\t            graph = Graph.random_binary_trees(num_var=self.num_var, depth=self.depth, num_repetitions=self.num_repetitions)\n\t        else:\n\t            raise AssertionError(\"Unknown Structure\")\n\t        args = EinsumNetwork.Args(\n\t                num_var=self.num_var,\n\t                num_dims=1,\n\t                num_classes=self.class_count,\n\t                num_sums=self.num_sums,\n\t                num_input_distributions=self.K,\n", "                exponential_family=exponential_family,\n\t                exponential_family_args=exponential_family_args,\n\t                use_em=self.use_em,\n\t                online_em_frequency=online_em_frequency,\n\t                online_em_stepsize=online_em_stepsize)\n\t        super().__init__(graph, args)\n\t        super().initialize()\n\t    def get_log_likelihoods(self, x):\n\t        log_likelihood = super().forward(x)\n\t        return log_likelihood\n", "    def forward(self, x, marg_idx=None, type=1):\n\t        # PRIOR\n\t        if type == 4:\n\t            expanded_prior = self.prior.expand(x.shape[0], self.prior.shape[0])\n\t            return expanded_prior\n\t        else:\n\t            # Obtain P(X|C) in log domain\n\t            if marg_idx:  # If marginalisation mask is passed\n\t                self.set_marginalization_idx(marg_idx)\n\t                log_likelihood = super().forward(x)\n", "                self.set_marginalization_idx(None)\n\t                likelihood = torch.nn.functional.softmax(log_likelihood, dim=1)\n\t            else:\n\t                log_likelihood = super().forward(x)\n\t            #LIKELIHOOD\n\t            if type == 2:\n\t                likelihood = torch.nn.functional.softmax(log_likelihood, dim=1)\n\t                # Sanity check for NaN-values\n\t                if torch.isnan(log_likelihood).sum() > 0:\n\t                    print(\"likelihood nan\")\n", "                return likelihood\n\t            else:\n\t                # Apply Bayes' Theorem to obtain P(C|X) instead of P(X|C)\n\t                # as it is provided by the EiNet\n\t                # 1. Computation of the prior, i.e. P(C), is already being\n\t                # dealt with at the initialisation of the EiNet.\n\t                # 2. Compute the normalization constant P(X)\n\t                z = torch.logsumexp(log_likelihood + self.prior, dim=1)\n\t                # 3. Compute the posterior, i.e. P(C|X) = (P(X|C) * P(C)) / P(X)\n\t                posterior_log = (log_likelihood + self.prior - z[:, None])  # log domain\n", "                #posterior = posterior_log.exp()  # decimal domain\n\t            #POSTERIOR\n\t            if type == 1:\n\t                posterior = torch.nn.functional.softmax(posterior_log, dim=1)\n\t                # Sanity check for NaN-values\n\t                if torch.isnan(z).sum() > 0:\n\t                    print(\"z nan\")\n\t                if torch.isnan(posterior).sum() > 0:\n\t                    print(\"posterior nan\")\n\t                return posterior\n", "            #JOINT\n\t            elif type == 3:\n\t                #compute the joint P(X|C) * P(C) \n\t                joint = torch.nn.functional.softmax(log_likelihood + self.prior, dim=1)\n\t                return joint\n"]}
{"filename": "src/__init__.py", "chunked_list": []}
{"filename": "src/slot_attention_module.py", "chunked_list": ["\"\"\"\n\tSlot attention model based on code of tkipf and the corresponding paper Locatello et al. 2020\n\t\"\"\"\n\tfrom torch import nn\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision.models as models\n\timport numpy as np\n\tfrom torchsummary import summary\n\tdef build_grid(resolution):\n", "    ranges = [np.linspace(0., 1., num=res) for res in resolution]\n\t    grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n\t    grid = np.stack(grid, axis=-1)\n\t    grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n\t    grid = np.expand_dims(grid, axis=0)\n\t    grid = grid.astype(np.float32)\n\t    return np.concatenate([grid, 1.0 - grid], axis=-1)\n\tdef spatial_broadcast(slots, resolution):\n\t    \"\"\"Broadcast slot features to a 2D grid and collapse slot dimension.\"\"\"\n\t    # `slots` has shape: [batch_size, num_slots, slot_size].\n", "    slots = torch.reshape(slots, [slots.shape[0] * slots.shape[1], 1, 1, slots.shape[2]])\n\t    grid = slots.repeat(1, resolution[0], resolution[1], 1) #repeat expands the data along differnt dimensions\n\t    # `grid` has shape: [batch_size*num_slots, width, height, slot_size].\n\t    return grid\n\tdef unstack_and_split(x, batch_size, n_slots, num_channels=3):\n\t  \"\"\"Unstack batch dimension and split into channels and alpha mask.\"\"\"\n\t  # unstacked = torch.reshape(x, [batch_size, -1] + list(x.shape[1:]))\n\t  # channels, masks = torch.split(unstacked, [num_channels, 1], dim=-1)\n\t  unstacked = torch.reshape(x, [batch_size, n_slots] + list(x.shape[1:]))\n\t  channels, masks = torch.split(unstacked, [num_channels, 1], dim=2)\n", "  return channels, masks\n\tclass SlotAttention(nn.Module):\n\t    def __init__(self, num_slots, dim, iters=3, eps=1e-8, hidden_dim=128):\n\t        super().__init__()\n\t        self.num_slots = num_slots\n\t        self.iters = iters\n\t        self.eps = eps\n\t        self.scale = dim ** -0.5 #named D in the paper\n\t        self.slots_mu = nn.Parameter(torch.randn(1, 1, dim)) #randomly initialize sigma and mu \n\t        self.slots_log_sigma = nn.Parameter(torch.randn(1, 1, dim)).abs().to(device='cuda')\n", "        #self.slots_mu = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(1,1,dim), gain=1.0)) #randomly initialize sigma and mu \n\t        #self.slots_log_sigma = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(1,1,dim), gain=1.0))\n\t        self.project_q = nn.Linear(dim, dim, bias=True) #query  projection\n\t        self.project_k = nn.Linear(dim, dim, bias=True) #\n\t        self.project_v = nn.Linear(dim, dim, bias=True) #feature key projection\n\t        self.gru = nn.GRUCell(dim, dim)\n\t        hidden_dim = max(dim, hidden_dim)\n\t        self.mlp = nn.Sequential(\n\t            nn.Linear(dim, hidden_dim),\n\t            nn.ReLU(inplace=True),\n", "            nn.Linear(hidden_dim, dim)\n\t        )\n\t        self.norm_inputs = nn.LayerNorm(dim, eps=1e-05)\n\t        self.norm_slots = nn.LayerNorm(dim, eps=1e-05)\n\t        self.norm_mlp = nn.LayerNorm(dim, eps=1e-05)\n\t        self.attn = 0\n\t    def forward(self, inputs, num_slots=None):\n\t        b, n, d = inputs.shape #b is the batchsize, n is the dimensionsize of the features, d is the amount of features([15, 1024, 32])\n\t        n_s = num_slots if num_slots is not None else self.num_slots\n\t        mu = self.slots_mu.expand(b, n_s, -1) #mu and sigma are shared by all slots\n", "        sigma = self.slots_log_sigma.expand(b, n_s, -1)\n\t        slots = torch.normal(mu, sigma) #sample slots from mu and sigma\n\t        #slots = torch.normal(mu, sigma.exp()) #sample slots from mu and sigma\n\t        inputs = self.norm_inputs(inputs) #layer normalization of inputs \n\t        k, v = self.project_k(inputs), self.project_v(inputs) #*self.scale\n\t        for _ in range(self.iters):\n\t            slots_prev = slots #store old slots\n\t            slots = self.norm_slots(slots) #layer norm of slots\n\t            q = self.project_q(slots) #emit a query for all slots\n\t            dots = torch.einsum('bid,bjd->bij', q, k) * self.scale #is M in the paper, has shape 1024(feature map)| 7(slot amount)\n", "            attn = dots.softmax(dim=1) + self.eps #calcualte the softmax for each slot which is also 1024 * 7\n\t            attn = attn / attn.sum(dim=-1, keepdim=True) #weighted mean\n\t            updates = torch.einsum('bjd,bij->bid', v, attn)\n\t            #recurrently update the slots with the slot updates and the previous slots\n\t            slots = self.gru(\n\t                updates.reshape(-1, d),\n\t                slots_prev.reshape(-1, d)\n\t            )\n\t            #apply 2 layer relu mlp to GRU output\n\t            slots = slots.reshape(b, -1, d)\n", "            slots = slots + self.mlp(self.norm_mlp(slots))\n\t        self.attn = attn\n\t        return slots\n\tclass SlotAttention_encoder(nn.Module):\n\t    def __init__(self, in_channels, hidden_channels, clevr_encoding):\n\t        super(SlotAttention_encoder, self).__init__()\n\t        if clevr_encoding:\n\t            self.network = nn.Sequential(\n\t                nn.Conv2d(in_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n\t                nn.ReLU(inplace=True),\n", "                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(2, 2), padding=2),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(2, 2), padding=2),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n\t                nn.ReLU(inplace=True))\n\t        else:\n\t            self.network = nn.Sequential(\n\t                nn.Conv2d(in_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n\t                nn.ReLU(inplace=True),\n", "                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n\t                nn.ReLU(inplace=True))\n\t    def forward(self, x):\n\t        return self.network(x)\n\tclass MLP(nn.Module):\n\t    def __init__(self, hidden_channels):\n", "        super(MLP, self).__init__()\n\t        self.network = nn.Sequential(\n\t            nn.Linear(hidden_channels, hidden_channels),\n\t            nn.ReLU(inplace=True),\n\t            nn.Linear(hidden_channels, hidden_channels),\n\t        )\n\t    def forward(self, x):\n\t        return self.network(x)\n\tclass SoftPositionEmbed(nn.Module):\n\t    \"\"\"Adds soft positional embedding with learnable projection.\"\"\"\n", "    def __init__(self, hidden_size, resolution, device=\"cuda:0\"):\n\t        \"\"\"Builds the soft position embedding layer.\n\t        Args:\n\t          hidden_size: Size of input feature dimension.\n\t          resolution: Tuple of integers specifying width and height of grid.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.dense = nn.Linear(4, hidden_size)\n\t        # self.grid = torch.FloatTensor(build_grid(resolution))\n\t        # self.grid = self.grid.to(device)\n", "        # for nn.DataParallel\n\t        self.register_buffer(\"grid\", torch.FloatTensor(build_grid(resolution)))\n\t        self.resolution = resolution[0]\n\t        self.hidden_size = hidden_size\n\t    def forward(self, inputs):\n\t        return inputs + self.dense(self.grid).view((-1, self.hidden_size, self.resolution, self.resolution))\n\tclass SlotAttention_classifier(nn.Module):\n\t    def __init__(self, in_channels, out_channels):\n\t        super(SlotAttention_classifier, self).__init__()\n\t        self.network = nn.Sequential(\n", "            nn.Linear(in_channels, in_channels),  # nn.Conv1d(in_channels, in_channels, 1, stride=1, groups=in_channels)\n\t            nn.ReLU(inplace=True),\n\t            nn.Linear(in_channels, out_channels),\n\t            nn.Sigmoid()\n\t        )\n\t    def forward(self, x):\n\t        return self.network(x)\n\tclass SlotAttention_model(nn.Module):\n\t    def __init__(self, n_slots, n_iters, n_attr,\n\t                 in_channels=3,\n", "                 encoder_hidden_channels=64,\n\t                 attention_hidden_channels=128,\n\t                 mlp_prediction = False,\n\t                 device=\"cuda\",\n\t                 clevr_encoding=False):\n\t        super(SlotAttention_model, self).__init__()\n\t        self.n_slots = n_slots\n\t        self.n_iters = n_iters\n\t        self.n_attr = n_attr\n\t        self.n_attr = n_attr + 1  # additional slot to indicate if it is a object or empty slot\n", "        self.device = device\n\t        self.encoder_cnn = SlotAttention_encoder(in_channels=in_channels, hidden_channels=encoder_hidden_channels , clevr_encoding=clevr_encoding)\n\t        self.encoder_pos = SoftPositionEmbed(encoder_hidden_channels, (32, 32), device=device)# changed from 128* 128\n\t        self.layer_norm = nn.LayerNorm(encoder_hidden_channels, eps=1e-05)\n\t        self.mlp = MLP(hidden_channels=encoder_hidden_channels)\n\t        self.slot_attention = SlotAttention(num_slots=n_slots, dim=encoder_hidden_channels, iters=n_iters, eps=1e-8,\n\t                                            hidden_dim=attention_hidden_channels)\n\t        #for set prediction baseline\n\t        self.mlp_prediction = mlp_prediction\n\t        self.mlp_classifier = SlotAttention_classifier(in_channels=encoder_hidden_channels, out_channels=self.n_attr)\n", "        self.softmax = nn.Softmax(dim=1)\n\t    def forward(self, img):\n\t        # `x` has shape: [batch_size, width, height, num_channels].\n\t        # SLOT ATTENTION ENCODER\n\t        x = self.encoder_cnn(img)\n\t        x = self.encoder_pos(x)\n\t        x = torch.flatten(x, start_dim=2)\n\t        # permute channel dimensions\n\t        x = x.permute(0, 2, 1)\n\t        x = self.layer_norm(x)\n", "        x = self.mlp(x)\n\t        slots = self.slot_attention(x)\n\t        # slots has shape: [batch_size, num_slots, slot_size].\n\t        if self.mlp_prediction:        \n\t            x = self.mlp_classifier(slots)\n\t            return x\n\t        else:\n\t            return slots\n\tif __name__ == \"__main__\":\n\t    x = torch.rand(15, 3, 32, 32).cuda()\n", "    net = SlotAttention_model(n_slots=11, n_iters=3, n_attr=18,\n\t                              encoder_hidden_channels=32, attention_hidden_channels=64,\n\t                              decoder_hidden_channels=32, decoder_initial_size=(8, 8))\n\t    net = net.cuda()\n\t    output = net(x)\n\t    summary(net, (3, 32, 32))\n"]}
{"filename": "src/utils.py", "chunked_list": ["import numpy as np\n\timport os\n\timport torch\n\timport errno\n\tfrom PIL import Image\n\timport torch \n\tfrom torch.utils.tensorboard import SummaryWriter\n\timport time\n\timport matplotlib\n\timport matplotlib.pyplot as plt\n", "from matplotlib.ticker import MaxNLocator\n\timport seaborn as sns\n\timport random\n\tdef mkdir_p(path):\n\t    \"\"\"Linux mkdir -p\"\"\"\n\t    try:\n\t        os.makedirs(path)\n\t    except OSError as exc:  # Python >2.5\n\t        if exc.errno == errno.EEXIST and os.path.isdir(path):\n\t            pass\n", "        else:\n\t            raise\n\tdef one_hot(x, K, dtype=torch.float):\n\t    \"\"\"One hot encoding\"\"\"\n\t    with torch.no_grad():\n\t        ind = torch.zeros(x.shape + (K,), dtype=dtype, device=x.device)\n\t        ind.scatter_(-1, x.unsqueeze(-1), 1)\n\t        return ind\n\tdef save_image_stack(samples, num_rows, num_columns, filename, margin=5, margin_gray_val=1., frame=0, frame_gray_val=0.0):\n\t    \"\"\"Save image stack in a tiled image\"\"\"\n", "    # for gray scale, convert to rgb\n\t    if len(samples.shape) == 3:\n\t        samples = np.stack((samples,) * 3, -1)\n\t    height = samples.shape[1]\n\t    width = samples.shape[2]\n\t    samples -= samples.min()\n\t    samples /= samples.max()\n\t    img = margin_gray_val * np.ones((height*num_rows + (num_rows-1)*margin, width*num_columns + (num_columns-1)*margin, 3))\n\t    for h in range(num_rows):\n\t        for w in range(num_columns):\n", "            img[h*(height+margin):h*(height+margin)+height, w*(width+margin):w*(width+margin)+width, :] = samples[h*num_columns + w, :]\n\t    framed_img = frame_gray_val * np.ones((img.shape[0] + 2*frame, img.shape[1] + 2*frame, 3))\n\t    framed_img[frame:(frame+img.shape[0]), frame:(frame+img.shape[1]), :] = img\n\t    img = Image.fromarray(np.round(framed_img * 255.).astype(np.uint8))\n\t    img.save(filename)\n\tdef sample_matrix_categorical(p):\n\t    \"\"\"Sample many Categorical distributions represented as rows in a matrix.\"\"\"\n\t    with torch.no_grad():\n\t        cp = torch.cumsum(p[:, 0:-1], -1)\n\t        rand = torch.rand((cp.shape[0], 1), device=cp.device)\n", "        rand_idx = torch.sum(rand > cp, -1).long()\n\t        return rand_idx\n\tdef set_manual_seed(seed:int=1):\n\t    \"\"\"Set the seed for the PRNGs.\"\"\"\n\t    os.environ['PYTHONASHSEED'] = str(seed)\n\t    random.seed(seed)\n\t    np.random.seed(seed)\n\t    torch.manual_seed(seed)\n\t    if torch.cuda.is_available():\n\t        torch.cuda.manual_seed(seed)\n", "        torch.cuda.manual_seed_all(seed)\n\t        torch.cuda.benchmark = True\n\tdef time_delta_now(t_start: float, simple_format=False, ret_sec=False) -> str:\n\t    \"\"\"\n\t    Convert a timestamp into a human readable timestring.\n\t    Parameters\n\t    ----------\n\t    t_start : float\n\t        The timestamp describing the begin of any event.\n\t    Returns\n", "    -------\n\t    Human readable timestring.\n\t    \"\"\"\n\t    a = t_start\n\t    b = time.time()  # current epoch time\n\t    c = b - a  # seconds\n\t    days = round(c // 86400)\n\t    hours = round(c // 3600 % 24)\n\t    minutes = round(c // 60 % 60)\n\t    seconds = round(c % 60)\n", "    millisecs = round(c % 1 * 1000)\n\t    if simple_format:\n\t        return f\"{hours}h:{minutes}m:{seconds}s\"\n\t    return f\"{days} days, {hours} hours, {minutes} minutes, {seconds} seconds, {millisecs} milliseconds\", c\n\tdef time_delta(c: float, simple_format=False,) -> str:\n\t    c# seconds\n\t    days = round(c // 86400)\n\t    hours = round(c // 3600 % 24)\n\t    minutes = round(c // 60 % 60)\n\t    seconds = round(c % 60)\n", "    millisecs = round(c % 1 * 1000)\n\t    if simple_format:\n\t        return f\"{hours}h:{minutes}m:{seconds}s\"\n\t    return f\"{days} days, {hours} hours, {minutes} minutes, {seconds} seconds, {millisecs} milliseconds\", c\n\tdef export_results(test_accuracy_list, train_accuracy_list,\n\t                   export_path, export_suffix, \n\t                   confusion_matrix , exp_dict):\n\t    #set matplotlib styles\n\t    plt.style.use(['science','grid'])\n\t    matplotlib.rcParams.update(\n", "        {\n\t            \"font.family\": \"serif\",\n\t            \"text.usetex\": False,\n\t            \"legend.fontsize\": 22\n\t        }\n\t    )\n\t    fig, axs = plt.subplots(1, 1 , figsize=(10,21))\n\t    # fig.suptitle(exp_dict['exp_name'], fontsize=16)\n\t    #axs[0]\n\t    axs.plot(test_accuracy_list[:,1],test_accuracy_list[:,0], label='test accuracy')\n", "    #axs[0]\n\t    axs.plot(train_accuracy_list[:,1],train_accuracy_list[:,0], label='train accuracy')\n\t    #axs[0].\n\t    axs.legend(loc=\"lower right\")\n\t    #axs[0].\n\t    axs.set(xlabel='epochs', ylabel='accuracy')\n\t    #axs[0].\n\t    axs.xaxis.set_major_locator(MaxNLocator(integer=True))\n\t    #ax.[0, 1].set_xticklabels([0,1,2,3,4,5,6,7,8,9])\n\t    #ax.[0, 1].set_yticklabels([0,1,2,3,4,5,6,7,8,9])\n", "    #axs[0, 1].set(xlabel='target', ylabel='prediction')\n\t    #axs[1] = sns.heatmap(confusion_matrix, linewidths=2, cmap=\"viridis\")\n\t    if exp_dict['structure'] == 'poon-domingos':\n\t        text = \"trainable parameters = {}, lr = {}, batchsize= {}, time_per_epoch= {},\\n structure= {}, pd_num_pieces = {}\".format(\n\t            exp_dict['num_trainable_params'], exp_dict['lr'],\n\t            exp_dict['bs'], exp_dict['train_time'],            \n\t            exp_dict['structure'], exp_dict['pd_num_pieces'] )    \n\t    else:\n\t        text = \"trainable parameters = {}, lr = {}, batchsize= {}, time_per_epoch= {},\\n structure= {}, num_repetitions = {} , depth = {}\".format(\n\t            exp_dict['num_trainable_params'], exp_dict['lr'],\n", "            exp_dict['bs'],exp_dict['train_time'],\n\t            exp_dict['structure'], exp_dict['num_repetitions'], exp_dict['depth'])\n\t    #plt.gcf().text(\n\t    #0.5,\n\t    #0.02,\n\t    #text,\n\t    #ha=\"center\",\n\t    #fontsize=12,\n\t    #linespacing=1.5,\n\t    #bbox=dict(\n", "    #    facecolor=\"grey\", alpha=0.2, edgecolor=\"black\", boxstyle=\"round,pad=1\"\n\t    #))\n\t    fig.savefig(export_path, format=\"svg\")\n\t    plt.show()\n\t    #Tensorboard outputs\n\t    writer = SummaryWriter(\"../../results\", filename_suffix=export_suffix)\n\t    for train_acc_elem, test_acc_elem in zip(train_accuracy_list, test_accuracy_list):\n\t        writer.add_scalar('Accuracy/train', train_acc_elem[0], train_acc_elem[1])\n\t        writer.add_scalar('Accuracy/test', test_acc_elem[0], test_acc_elem[1])\n"]}
{"filename": "src/datasets.py", "chunked_list": ["import os\n\timport tempfile\n\timport  urllib.request\n\timport shutil\n\tfrom zipfile import ZipFile\n\timport gzip\n\timport utils\n\tdef maybe_download(directory, url_base, filename, suffix='.zip'):\n\t    '''\n\t    Downloads the specified dataset and extracts it\n", "    @param directory:\n\t    @param url_base: URL where to find the file\n\t    @param filename: name of the file to be downloaded\n\t    @param suffix: suffix of the file\n\t    :returns: true if nothing went wrong downloading \n\t    '''\n\t    filepath = os.path.join(directory, filename)\n\t    if os.path.isfile(filepath):\n\t        return False\n\t    if not os.path.isdir(directory):\n", "        utils.mkdir_p(directory)\n\t    url = url_base  +filename\n\t    _, zipped_filepath = tempfile.mkstemp(suffix=suffix)\n\t    print('Downloading {} to {}'.format(url, zipped_filepath))\n\t    urllib.request.urlretrieve(url, zipped_filepath)\n\t    print('{} Bytes'.format(os.path.getsize(zipped_filepath)))\n\t    print('Move to {}'.format(filepath))\n\t    shutil.move(zipped_filepath, filepath)\n\t    return True\n\tdef extract_dataset(directory, filepath, filepath_extracted):\n", "    if not os.path.isdir(filepath_extracted):\n\t        print('unzip ',filepath, \" to\", filepath_extracted)\n\t        with ZipFile(filepath, 'r') as zipObj:\n\t           # Extract all the contents of zip file in current directory\n\t           zipObj.extractall(directory)\n\tdef maybe_download_shapeworld4():\n\t    '''\n\t    Downloads the shapeworld4 dataset if it is not downloaded yet\n\t    '''\n\t    directory = \"../../data/\"\n", "    file_name= \"shapeworld4.zip\"\n\t    maybe_download(directory, \"https://hessenbox.tu-darmstadt.de/dl/fiEE3hftM4n1gBGn4HJLKUkU/\", file_name)\n\t    filepath = os.path.join(directory, file_name)\n\t    filepath_extracted = os.path.join(directory,\"shapeworld4\")\n\t    extract_dataset(directory, filepath, filepath_extracted)\n\tdef maybe_download_shapeworld_cogent():\n\t    '''\n\t    Downloads the shapeworld4 cogent dataset if it is not downloaded yet\n\t    '''\n\t    directory = \"../../data/\"\n", "    file_name= \"shapeworld_cogent.zip\"\n\t    maybe_download(directory, \"https://hessenbox.tu-darmstadt.de/dl/fi3CDjPRsYgAvotHcC8GPaWj/\", file_name)\n\t    filepath = os.path.join(directory, file_name)\n\t    filepath_extracted = os.path.join(directory,\"shapeworld_cogent\")\n\t    extract_dataset(directory, filepath, filepath_extracted)\n"]}
{"filename": "src/SLASH/__init__.py", "chunked_list": []}
{"filename": "src/SLASH/slash.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tNeurASP: Embracing Neural Networks into Answer Set Programming\n\tZhun Yang, Adam Ishay, Joohyung Lee. Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence\n\tMain track. Pages 1755-1762.\n\thttps://github.com/azreasoners/NeurASP\n\t\"\"\"\n\timport re\n\timport sys\n\timport time\n", "import clingo\n\timport torch\n\timport torch.nn as nn\n\timport numpy as np\n\timport time\n\tsys.path.append('../')\n\tsys.path.append('../../')\n\tsys.path.append('../../SLASH/')\n\tfrom operator import itemgetter\n\tfrom mvpp import MVPP\n", "from sklearn.metrics import confusion_matrix\n\tfrom tqdm import tqdm\n\tfrom joblib import Parallel, delayed\n\timport scipy \n\tfrom itertools import count\n\tdef pad_3d_tensor(target, framework, bs, ruleNum, max_classes):\n\t    '''\n\t    Turns a 3d list with unequal length into a padded tensor or 3D numpy array \n\t    @param target: nested list\n\t    @param framework: type of the framework (either numpy or pytorch) of the output\n", "    @param bs: len of the first dimension of the list\n\t    @param ruleNum: max len of the second dimension of the list\n\t    @param max_classes:  max len of the third dimension of the list\n\t    :return:returns a tensor or 3D numpy array\n\t    '''\n\t    if framework == 'numpy':\n\t        padded = torch.tensor([np.hstack((np.asarray(row, dtype=np.float32),  [0] * (max_classes - len(row))) ) for batch in target for row in batch]).type(torch.FloatTensor).view(bs, ruleNum, max_classes)    \n\t    if framework == 'torch':\n\t        padded = torch.stack([torch.hstack((row,  torch.tensor([0] * (max_classes - len(row)), device=\"cuda\" ) ) ) for batch in target for row in batch ]).view(ruleNum, bs, max_classes)\n\t    return padded\n", "def replace_plus_minus_occurences(pi_prime):\n\t    \"\"\"\n\t    For a given query/program replace all occurences of the +- Notation with the ASP readable counterpart eg. digit(0, +i1, -0) -> digit(0, 1, i1, 0)\n\t    @param pi_prime: input query/program as string\n\t    :return:returns the ASP readable counterpart of the given query/program and a dict of all used operators per npp\n\t    \"\"\"\n\t    pat_pm = r'(\\s*[a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+([A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*)\\)' #1 +- p(c|x) posterior\n\t    pat_mp = r'(\\s*[a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+([A-Z]*[a-zA-Z0-9_]*\\s*)\\)' #2 -+ p(x|c) likelihood\n\t    pat_mm = r'(\\s*[a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*)\\)' #3 -- p(x,c) joint\n\t    pat_pp = r'(\\s*[a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+([A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+([A-Z]*[a-zA-Z0-9_]*\\s*)\\)' #3 -- p(c) prior\n", "    #pattern for vqa relations with two vars\n\t    pat_pm2 = r'([a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+\\(([a-zA-Z0-9_ ]*,[a-zA-Z0-9_ ]*)\\),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*)\\)'\n\t    #track which operator(+-,-+,--) was used for which npp\n\t    npp_operators= {}\n\t    for match in re.findall(pat_pm, pi_prime):\n\t        if match[0] not in npp_operators:\n\t            npp_operators[match[0]] = {}\n\t        npp_operators[match[0]][1] = True\n\t    for match in re.findall(pat_mp, pi_prime):\n\t        if match[0] not in npp_operators:\n", "            npp_operators[match[0]] = {}\n\t        npp_operators[match[0]][2] = True\n\t    for match in re.findall(pat_mm, pi_prime):\n\t        if match[0] not in npp_operators:\n\t            npp_operators[match[0]] = {}\n\t        npp_operators[match[0]][3] = True\n\t    for match in re.findall(pat_pp, pi_prime):\n\t        if match[0] not in npp_operators:\n\t            npp_operators[match[0]] = {}\n\t        npp_operators[match[0]][4] = True\n", "    for match in re.findall(pat_pm2, pi_prime):\n\t        if match[0] not in npp_operators:\n\t            npp_operators[match[0]] = {}\n\t        npp_operators[match[0]][1] = True\n\t    #replace found matches with asp compatible form for npp occuring in rules\n\t    #example: digit(0,+A,-N1) -> digit(0,1,A,N1)\n\t    pi_prime = re.sub( pat_pm, r'\\1(\\2,1,\\3,\\4)', pi_prime)\n\t    pi_prime = re.sub( pat_mp, r'\\1(\\2,2,\\3,\\4)', pi_prime)\n\t    pi_prime = re.sub( pat_mm, r'\\1(\\2,3,\\3,\\4)', pi_prime)\n\t    pi_prime = re.sub( pat_pp, r'\\1(\\2,4,\\3,\\4)', pi_prime)\n", "    pi_prime = re.sub( pat_pm2, r'\\1(\\2,1,\\3,\\4)', pi_prime)\n\t    return pi_prime, npp_operators\n\tdef compute_models_splitwise(big_M_split, query_batch_split, dmvpp, method, k, same_threshold, obj_filter, vqa_params):\n\t        \"\"\"\n\t        Computes the potential solutions and P(Q) for a split of a batch.\n\t        @param big_M_split:\n\t        @param query_batch_split:\n\t        @param dmvpp:\n\t        @param method:\n\t        @param k: \n", "        @same_threshold:\n\t        @obj_filter:\n\t        @vqa_params: \n\t        :return: returns the potential solutions, the atom indices for the gradient computation and the probability P(Q)\n\t        \"\"\"\n\t        query_batch_split = query_batch_split.tolist()\n\t        #create a list to store the stable models into\n\t        model_batch_list_split = []\n\t        models_idx_list = []\n\t        #iterate over all queries\n", "        for bidx, query in enumerate(query_batch_split):\n\t            #produce gradients for every data entry\n\t            dmvpp.M = big_M_split[:,bidx,:]\n\t            dmvpp.normalize_M()\n\t            #if len(dmvpp.parameters[ruleIdx]) == 1:\n\t                #    dmvpp.parameters[ruleIdx] =  [dmvpp.parameters[ruleIdx][0][0],1-dmvpp.parameters[ruleIdx][0][0]]\n\t            query, _ = replace_plus_minus_occurences(query)\n\t            #exact (default): uses all/k-first stable models for WMC \n\t            if method == 'exact': \n\t                models = dmvpp.find_all_SM_under_query(query)\n", "            #uses only one most probable stable model for WMC  \n\t            elif method == 'most_prob':\n\t                models = dmvpp.find_k_most_probable_SM_under_query_noWC(query, k=1)\n\t            #uses the k most probable stables models for WMC\n\t            elif method == 'top_k':\n\t                models = dmvpp.find_k_most_probable_SM_under_query_noWC(query,k=k)\n\t                #gradients = dmvpp.mvppLearn(models)\n\t            #reduces the npp grounding to the most probable instantiations\n\t            elif method == \"same\":\n\t                if obj_filter is not None: #in the vqa case we want to filter out objects which are not in the image\n", "                    models = dmvpp.find_SM_vqa_with_same(query, k=k, threshold=same_threshold, obj_filter=obj_filter[bidx], vqa_params= vqa_params)\n\t                else:\n\t                    models = dmvpp.find_SM_with_same(query, k=k, threshold=same_threshold)\n\t            #reduces the npp grounding to the most probable instantiations and then uses the top k stable models for WMC\n\t            elif method == \"same_top_k\":\n\t                models = dmvpp.find_k_most_probable_SM_with_same(query,k=k)\n\t            else:\n\t                print('Error: the method \\'%s\\' should be either \\'exact\\', \\'most_prob\\',\\'top_k\\',\\'same_top_k\\'  or \\'same\\'', method)\n\t            #old NeurASP code - Can also be reenabled in SLASH\n\t            # #samples atoms from the npp until k stable models are found\n", "            # elif method == 'sampling':\n\t            #     models = dmvpp.sample_query(query, num=k)\n\t            # elif method == 'network_prediction':\n\t            #     models = dmvpp.find_k_most_probable_SM_under_query_noWC(query, k=1)\n\t            #     check = SLASH.satisfy(models[0], mvpp['program_asp'] + query)\n\t            #     if check:\n\t            #         continue\n\t            # elif method == 'penalty':\n\t            #     models = dmvpp.find_all_SM_under_query()\n\t            #     models_noSM = [model for model in models if not SLASH.satisfy(model, mvpp['program_asp'] + query)]\n", "            models_idx = dmvpp.collect_atom_idx(models)\n\t            models_idx_list.append(models_idx)\n\t            model_batch_list_split.append(models)\n\t        return model_batch_list_split, models_idx_list\n\tdef compute_vqa_splitwise(big_M_split, query_batch_split, dmvpp, obj_filter_split, k, pred_vector_size, vqa_params):\n\t    \"\"\"\n\t    Computes the gradients, stable models and P(Q) for a split of a batch.\n\t    @param networkOutput_split:\n\t    @param query_batch_split:\n\t    @param mvpp:\n", "    @param n:\n\t    @param normalProbs:\n\t    @param dmvpp:\n\t    @param method:\n\t    @param opt:\n\t    @param k: \n\t    :return:returns the gradients, the stable models and the probability P(Q)\n\t    \"\"\"\n\t    query_batch_split = query_batch_split.tolist()\n\t    #create a list to store the target predictions into\n", "    pred_batch_list_split = []\n\t    #iterate over all queries\n\t    for bidx, query in enumerate(query_batch_split):\n\t        dmvpp.M = big_M_split[:,bidx,:]\n\t        dmvpp.normalize_M()\n\t        #if len(dmvpp.parameters[ruleIdx]) == 1:\n\t            #    dmvpp.parameters[ruleIdx] =  [dmvpp.parameters[ruleIdx][0][0],1-dmvpp.parameters[ruleIdx][0][0]]\n\t        #query, _ = replace_plus_minus_occurences(query)\n\t        #find all targets with same\n\t        models = dmvpp.find_SM_vqa_with_same(query, k=k, obj_filter= obj_filter_split[bidx], threshold={\"relation\":1, \"name\":1, \"attr\":1}, train=False, vqa_params=vqa_params)\n", "        #compute probabilites of targets in the model\n\t        # first retrieve the object ids of targets in the model  \n\t        pred_vec = np.zeros(pred_vector_size)\n\t        #if we have a model\n\t        if len(models) > 0:\n\t            targets = {}\n\t            models = [model for model in models if model != []] #remove empty models\n\t            #go through all models and check for targets\n\t            for model in models:    \n\t                if len(model)> 1:   \n", "                    model_prob = 0\n\t                    model_has_npp_atom = False\n\t                    for atom in model:\n\t                        #we found the target atom and save the id of the object\n\t                        if re.match(r'target', atom):\n\t                            #add target id\n\t                            target_id = int(re.match(r'target\\(([0-9]*)\\)',atom)[1])\n\t                        else:\n\t                            #if the non target atom is a ground npp atom get its probability and add it \n\t                            if atom in dmvpp.ga_map:\n", "                                rule_idx, atom_idx = dmvpp.ga_map[atom]\n\t                                model_prob += dmvpp.M[rule_idx, atom_idx].log()\n\t                                model_has_npp_atom = True\n\t                    # only add model prob for real probability values < 1\n\t                    if model_has_npp_atom:\n\t                        #if this is the first model with a target of that id\n\t                        if target_id not in targets:\n\t                            targets[target_id] = []\n\t                        targets[target_id].append(model_prob)\n\t            #get the maximum value for each target object\n", "            for target_id in targets:\n\t                pred_vec[target_id] = torch.tensor(targets[target_id]).max().exp()\n\t        #add prediction vector entry for this query to the batch list containing all queries\n\t        pred_batch_list_split.append(pred_vec)\n\t    return pred_batch_list_split\n\tclass SLASH(object):\n\t    def __init__(self, dprogram, networkMapping, optimizers, gpu=True):\n\t        \"\"\"\n\t        @param dprogram: a string for a NeurASP program\n\t        @param networkMapping: a dictionary maps network names to neural networks modules\n", "        @param optimizers: a dictionary maps network names to their optimizers\n\t        @param gpu: a Boolean denoting whether the user wants to use GPU for training and testing\n\t        \"\"\"\n\t        # the neural models should be run on a GPU for better performance. The gradient computation is vectorized but in some cases it makes sense\n\t        # to run them on the cpu. For example in the case of the MNIST addition we create a lot of small tensors and putting them all on the gpu is significant overhead\n\t        # while the computation itself is comparable for small tensors. \n\t        # As a rule of thumb you can say that with more date or more npps it makes more sense to use the gpu for fast gradient computation.\n\t        self.device = torch.device('cuda' if torch.cuda.is_available() and gpu else 'cpu')\n\t        self.grad_comp_device = torch.device('cuda' if torch.cuda.is_available() and gpu else 'cpu')\n\t        self.dprogram = dprogram\n", "        self.const = {} # the mapping from c to v for rule #const c=v.\n\t        self.n = {} # the mapping from network name to an integer n denoting the domain size; n would be 1 or N (>=3); note that n=2 in theory is implemented as n=1\n\t        self.max_n = 2 # integer denoting biggest domain of all npps\n\t        self.e = {} # the mapping from network name to an integer e\n\t        self.domain = {} # the mapping from network name to the domain of the predicate in that network atom\n\t        self.normalProbs = None # record the probabilities from normal prob rules\n\t        self.networkOutputs = {}\n\t        self.networkGradients = {}\n\t        self.networkTypes = {}\n\t        if gpu==True:\n", "            self.networkMapping = {key : nn.DataParallel(networkMapping[key].to(self.device)) for key in networkMapping}\n\t        else:\n\t            self.networkMapping = networkMapping\n\t        self.optimizers = optimizers\n\t        # self.mvpp is a dictionary consisting of 4 keys: \n\t        # 1. 'program': a string denoting an MVPP program where the probabilistic rules generated from network are followed by other rules;\n\t        # 2. 'networkProb': a list of lists of tuples, each tuple is of the form (model, i ,term, j)\n\t        # 3. 'atom': a list of list of atoms, where each list of atoms is corresponding to a prob. rule\n\t        # 4. 'networkPrRuleNum': an integer denoting the number of probabilistic rules generated from network\n\t        self.mvpp = {'networkProb': [], 'atom': [], 'networkPrRuleNum': 0,'networkPrRuleNumWithoutBinary': 0, 'binary_rule_belongings':{}, 'program': '','networkProbSinglePred':{}}\n", "        self.mvpp['program'], self.mvpp['program_pr'], self.mvpp['program_asp'], self.npp_operators = self.parse(query='')\n\t        self.stableModels = [] # a list of stable models, where each stable model is a list\n\t        self.prob_q = [] # a list of probabilites for each query in the batch\n\t    def constReplacement(self, t):\n\t        \"\"\" Return a string obtained from t by replacing all c with v if '#const c=v.' is present\n\t        @param t: a string, which is a term representing an input to a neural network\n\t        \"\"\"\n\t        t = t.split(',')\n\t        t = [self.const[i.strip()] if i.strip() in self.const else i.strip() for i in t]\n\t        return ','.join(t)\n", "    def networkAtom2MVPPrules(self, networkAtom, npp_operators):\n\t        \"\"\"\n\t        @param networkAtom: a string of a neural atom\n\t        @param countIdx: a Boolean value denoting whether we count the index for the value of m(t, i)[j]\n\t        \"\"\"\n\t        # STEP 1: obtain all information\n\t        regex = '^(npp)\\((.+)\\((.+)\\),\\((.+)\\)\\)$'\n\t        out = re.search(regex, networkAtom)        \n\t        network_type = out.group(1)\n\t        m = out.group(2)\n", "        e, inf_type, t = out.group(3).split(',', 2) # in case t is of the form t1,...,tk, we only split on the second comma\n\t        domain = out.group(4).split(',')\n\t        inf_type =int(inf_type)\n\t        #TODO how do we get the network type?\n\t        self.networkTypes[m] = network_type\n\t        t = self.constReplacement(t)\n\t        # check the value of e\n\t        e = e.strip()\n\t        e = int(self.constReplacement(e))\n\t        n = len(domain)\n", "        if n == 2:\n\t            n = 1\n\t        self.n[m] = n\n\t        if self.max_n <= n:\n\t            self.max_n = n\n\t        self.e[m] = e\n\t        self.domain[m] = domain\n\t        if m not in self.networkOutputs:\n\t            self.networkOutputs[m] = {}\n\t        for o in npp_operators[m]:\n", "            if o not in self.networkOutputs[m]:\n\t                self.networkOutputs[m][o] = {}\n\t            self.networkOutputs[m][o][t]= None    \n\t        # STEP 2: generate MVPP rules\n\t        mvppRules = []\n\t        # we have different translations when n = 2 (i.e., n = 1 in implementation) or when n > 2\n\t        if n == 1:\n\t            for i in range(e):\n\t                rule = '@0.0 {}({}, {}, {}, {}); @0.0 {}({},{}, {}, {}).'.format(m, i, inf_type, t, domain[0], m, i, inf_type, t, domain[1])\n\t                prob = [tuple((m, i, inf_type, t, 0))]\n", "                atoms = ['{}({}, {}, {}, {})'.format(m, i,inf_type, t, domain[0]), '{}({},{},{}, {})'.format(m, i, inf_type,  t, domain[1])]\n\t                mvppRules.append(rule)\n\t                self.mvpp['networkProb'].append(prob)\n\t                self.mvpp['atom'].append(atoms)\n\t                self.mvpp['networkPrRuleNum'] += 1\n\t                self.mvpp['networkPrRuleNumWithoutBinary'] += 1\n\t            self.mvpp['networkProbSinglePred'][m] = True\n\t        elif n > 2:\n\t            if m == \"attr\": #TODO special case hack to map a model to multiple npp's as in the attributes in vqa\n\t                self.mvpp['binary_rule_belongings'][self.mvpp['networkPrRuleNum']] = (m,t)\n", "            for i in range(e):\n\t                rule = ''\n\t                prob = []\n\t                atoms = []\n\t                for j in range(n):\n\t                    atom = '{}({},{}, {}, {})'.format(m,  i, inf_type, t, domain[j])\n\t                    rule += '@0.0 {}({},{}, {}, {}); '.format(m, i,inf_type, t, domain[j])\n\t                    prob.append(tuple((m, i, inf_type, t, j)))\n\t                    atoms.append(atom)\n\t            mvppRules.append(rule[:-2]+'.')\n", "            self.mvpp['networkProb'].append(prob)\n\t            self.mvpp['atom'].append(atoms)\n\t            self.mvpp['networkPrRuleNum'] += 1\n\t            self.mvpp['networkPrRuleNumWithoutBinary'] += 1\n\t        else:\n\t            print('Error: the number of element in the domain %s is less than 2' % domain)\n\t        return mvppRules\n\t    def parse(self, query=''):\n\t        dprogram = self.dprogram + query\n\t        # 1. Obtain all const definitions c for each rule #const c=v.\n", "        regex = '#const\\s+(.+)=(.+).'\n\t        out = re.search(regex, dprogram)\n\t        if out:\n\t            self.const[out.group(1).strip()] = out.group(2).strip()\n\t        # 2. Generate prob. rules for grounded network atoms\n\t        clingo_control = clingo.Control([\"--warn=none\"])\n\t        # 2.1 remove weak constraints and comments\n\t        program = re.sub(r'\\n:~ .+\\.[ \\t]*\\[.+\\]', '\\n', dprogram)\n\t        program = re.sub(r'\\n%[^\\n]*', '\\n', program)\n\t        # 2.2 replace [] with ()\n", "        program = program.replace('[', '(').replace(']', ')')\n\t        # 2.3 use MVPP package to parse prob. rules and obtain ASP counter-part\n\t        mvpp = MVPP(program)\n\t        #if mvpp.parameters and not self.normalProbs:\n\t        #    self.normalProbs = mvpp.parameters\n\t        pi_prime = mvpp.pi_prime\n\t        #2.4 parse +-Notation and add a const to flag the operation in the npp call\n\t        pi_prime = pi_prime.replace(' ','').replace('#const','#const ')\n\t        #replace all occurences of the +- calls\n\t        pi_prime, npp_operators = replace_plus_minus_occurences(pi_prime)\n", "        #extend npps definitions with the operators found\n\t        #example: npp(digit(1,X),(0,1,2,3,4,5,6,7,8,9)):-img(X). with a +- and -- call in the program \n\t        # -> npp(digit(1,3,X),(0,1,2,3,4,5,6,7,8,9)):-img(X). npp(digit(1,1,X),(0,1,2,3,4,5,6,7,8,9)):-img(X).\n\t        #pat_npp = r'(npp\\()([a-z]*[a-zA-Z0-9_]*)(\\([0-9]*,)([A-Z]*[a-zA-Z0-9_]*\\),\\((?:[a-z0-9_]*,)*[a-z0-9_]*\\)\\))(:-[a-z][a-zA-Z0-9_]*\\([A-Z][a-zA-Z0-9_]*\\))?.'\n\t        #can match fact with arity > 1\n\t        pat_npp = r'(npp\\()([a-z]*[a-zA-Z0-9_]*)(\\([0-9]*,)([A-Z]*)([A-Z]*[a-zA-Z0-9_]*\\),\\((?:[a-z0-9_]*,)*[a-z0-9_]*\\)\\))(:-[a-z][a-zA-Z0-9_]*\\([A-Z][a-zA-Z0-9_,]*\\))?.'\n\t        def npp_sub(match):\n\t            \"\"\"\n\t            filters program for npp occurances\n\t            \"\"\"\n", "            npp_extended =\"\"\n\t            for o in npp_operators[match.group(2)]:\n\t                if match.group(6) is None:\n\t                    body = \"\"\n\t                    var_replacement = match.group(4)\n\t                elif re.match(r':-[a-z]*\\([0-9a-zA-Z_]*,[0-9a-zA-Z_]*\\)', match.group(6)):\n\t                    body = match.group(6)\n\t                    var_replacement = re.sub(r'(:-)([a-zA-Z0-9]*)\\(([a-z0-9A-Z]*,[a-z0-9A-Z]*)\\)', r\"\\3\" ,body)\n\t                else: \n\t                    body = match.group(6)\n", "                    var_replacement = match.group(4)\n\t                npp_extended = '{}{}{}{}{}{}{}{}.\\n'.format(match.group(1), match.group(2),match.group(3),o,\",\", var_replacement,match.group(5), body)+ npp_extended \n\t            return npp_extended\n\t        pi_prime = re.sub(pat_npp, npp_sub, pi_prime)\n\t        # 2.5 use clingo to generate all grounded network atoms and turn them into prob. rules\n\t        clingo_control.add(\"base\", [], pi_prime)\n\t        clingo_control.ground([(\"base\", [])])\n\t        #symbolic mappings map the constants to functions in the ASP program\n\t        symbols = [atom.symbol for atom in clingo_control.symbolic_atoms]\n\t        #iterate over all NPP atoms and extract information for the MVPP program\n", "        mvppRules = [self.networkAtom2MVPPrules(str(atom),npp_operators) for atom in symbols if (atom.name == 'npp')]\n\t        mvppRules = [rule for rules in mvppRules for rule in rules]\n\t        # 3. obtain the ASP part in the original NeurASP program after +- replacements\n\t        lines = [line.strip() for line in pi_prime.split('\\n') if line and not re.match(\"^\\s*npp\\(\", line)]\n\t        return '\\n'.join(mvppRules + lines), '\\n'.join(mvppRules), '\\n'.join(lines), npp_operators\n\t    @staticmethod\n\t    def satisfy(model, asp):\n\t        \"\"\"\n\t        Return True if model satisfies the asp program; False otherwise\n\t        @param model: a stable model in the form of a list of atoms, where each atom is a string\n", "        @param asp: an ASP program (constraints) in the form of a string\n\t        \"\"\"\n\t        asp_with_facts = asp + '\\n'\n\t        for atom in model:\n\t            asp_with_facts += atom + '.\\n'\n\t        clingo_control = clingo.Control(['--warn=none'])\n\t        clingo_control.add('base', [], asp_with_facts)\n\t        clingo_control.ground([('base', [])])\n\t        result = clingo_control.solve()\n\t        if str(result) == 'SAT':\n", "            return True\n\t        return False\n\t    def infer(self, dataDic, query='', mvpp='',  dataIdx=None):\n\t        \"\"\"\n\t        @param dataDic: a dictionary that maps terms to tensors/np-arrays\n\t        @param query: a string which is a set of constraints denoting a query\n\t        @param mvpp: an MVPP program used in inference\n\t        \"\"\"\n\t        mvppRules = ''\n\t        facts = ''\n", "        # Step 1: get the output of each neural network\n\t        for m in self.networkOutputs:\n\t            self.networkMapping[m].eval()\n\t            for o in self.networkOutputs[m]:\n\t                for t in self.networkOutputs[m][o]:\n\t                    if dataIdx is not None:\n\t                        dataTensor = dataDic[t][dataIdx]\n\t                    else:\n\t                        dataTensor = dataDic[t]\n\t                    self.networkOutputs[m][o][t] = self.networkMapping[m](dataTensor).view(-1).tolist()\n", "        for ruleIdx in range(self.mvpp['networkPrRuleNum']):\n\t            probs = [self.networkOutputs[m][inf_type][t][i*self.n[m]+j] for (m, i, inf_type, t, j) in self.mvpp['networkProb'][ruleIdx]]\n\t            #probs = [self.networkOutputs[m][inf_type][t][0][i*self.n[m]+j] for (m, i, inf_type, t, j) in self.mvpp['networkProb'][ruleIdx]]\n\t            if len(probs) == 1:\n\t                mvppRules += '@{:.15f} {}; @{:.15f} {}.\\n'.format(probs[0], self.mvpp['atom'][ruleIdx][0], 1 - probs[0], self.mvpp['atom'][ruleIdx][1])\n\t            else:\n\t                tmp = ''\n\t                for atomIdx, prob in enumerate(probs):\n\t                    tmp += '@{:.15f} {}; '.format(prob, self.mvpp['atom'][ruleIdx][atomIdx])\n\t                mvppRules += tmp[:-2] + '.\\n'\n", "        # Step 3: find an optimal SM under query\n\t        dmvpp = MVPP(facts + mvppRules + mvpp)\n\t        return dmvpp.find_k_most_probable_SM_under_query_noWC(query, k=1)\n\t    def split_network_outputs(self, big_M, obj_filter, query_batch, p_num):\n\t        if len(query_batch)< p_num:\n\t            if type(query_batch) == tuple:\n\t                p_num = len(query_batch)\n\t            else:\n\t                p_num=query_batch.shape[0]\n\t        #partition dictionary for different processors                \n", "        splits = np.arange(0, int(p_num))\n\t        partition = int(len(query_batch) / p_num)\n\t        partition_mod = len(query_batch) % p_num \n\t        partition = [partition]*p_num\n\t        partition[-1]+= partition_mod\n\t        query_batch_split = np.split(query_batch,np.cumsum(partition))[:-1]\n\t        if obj_filter is not  None:\n\t            obj_filter = np.array(obj_filter)\n\t            obj_filter_split = np.split(obj_filter, np.cumsum(partition))[:-1]\n\t        else:\n", "            obj_filter_split = None \n\t        big_M_splits = torch.split(big_M, dim=1, split_size_or_sections=partition)\n\t        return big_M_splits, query_batch_split, obj_filter_split, p_num\n\t    def learn(self, dataset_loader, epoch, method='exact', opt=False, k_num=0, p_num=1, slot_net=None, hungarian_matching=False, vqa=False, marginalisation_masks=None,  writer=None, same_threshold=0.99, batched_pass=False, vqa_params=None):\n\t        \"\"\"\n\t        @param dataset_loader: a pytorch dataloader object returning a dictionary e.g {im1: [bs,28,28], im2:[bs,28,28]} and the queries\n\t        @param epoch: an integer denoting the current epoch\n\t        @param method: a string in {'exact', 'same',''} denoting whether the gradients are computed exactly or by sampling\n\t        @param opt: stands for optimal -> if true we select optimal stable models\n\t        @param k_num: select k stable models, default k_num=0 means all stable models\n", "        @param p_num: a positive integer denoting the number of processor cores to be used during the training\n\t        @param slot_net: a slot attention network for the set prediction experiment mapping from an image to slots\n\t        @param vqa: VQA option for the vqa experiment. Enables VQA specific datahandling\n\t        @param hungarian_matching: boolean denoting wherever the matching is done in the LP or if the results of the hungarian matching should be integrated in the LP\n\t        @param marginalisation_masks: a list entailing one marginalisation mask for each batch of dataList\n\t        @param writer: Tensorboard writer for plotting metrics\n\t        @param same_threshold: Threshold when method = same or same_top_k. Can be either a scalar value or a dict mapping treshold to networks m {\"digit\":0.99}\n\t        @param batched_pass: boolean to forward all t belonging to the same m in one pass instead of t passes\n\t        \"\"\"\n\t        assert p_num >= 1 and isinstance(p_num, int), 'Error: the number of processors used should greater equals one and a natural number'\n", "        # get the mvpp program by self.mvpp\n\t        #old NeurASP code. Can be reanbled if needed\n\t        #if method == 'network_prediction':\n\t        #    dmvpp = MVPP(self.mvpp['program_pr'], max_n= self.max_n)\n\t        #elif method == 'penalty':\n\t        #    dmvpp = MVPP(self.mvpp['program_pr'], max_n= self.max_n)\n\t        #add the npps to the program now or later\n\t        if method == 'same' or method == 'same_top_k':\n\t            dmvpp = MVPP(self.mvpp['program'], prob_ground=True , binary_rule_belongings= self.mvpp['binary_rule_belongings'], max_n= self.max_n)\n\t        else:\n", "            dmvpp = MVPP(self.mvpp['program'], max_n= self.max_n)\n\t        # we train all neural network models\n\t        for m in self.networkMapping:\n\t            self.networkMapping[m].train() #torch training mode\n\t            self.networkMapping[m].module.train() #torch training mode\n\t        total_loss = []\n\t        sm_per_batch_list = []\n\t        #store time per epoch\n\t        forward_time = []\n\t        asp_time = []\n", "        gradient_time  = []\n\t        backward_time = []\n\t        #iterate over all batches\n\t        pbar = tqdm(total=len(dataset_loader))\n\t        for batch_idx, (batch) in enumerate(dataset_loader):      \n\t            start_time = time.time()\n\t            if hungarian_matching:\n\t                data_batch, query_batch, obj_enc_batch = batch\n\t            elif vqa:\n\t                data_batch, query_batch, obj_filter, _ = batch\n", "            else:\n\t                data_batch, query_batch = batch\n\t            # If we have marginalisation masks, than we have to pick one for the batch\n\t            if marginalisation_masks is not None:\n\t                marg_mask = marginalisation_masks[i]\n\t            else:\n\t                marg_mask = None\n\t            #STEP 0: APPLY SLOT ATTENTION TO TRANSFORM IMAGE TO SLOTS\n\t            #we have a map which is : im: im_data\n\t            #we want a map which is : s1: slot1_data, s2: slot2_data, s3: slot3_data\n", "            if slot_net is not None:\n\t                slot_net.train()\n\t                dataTensor_after_slot = slot_net(data_batch['im'].to(self.device)) #forward the image\n\t                #add the slot outputs to the data batch\n\t                for slot_num in range(slot_net.n_slots):\n\t                    key = 's'+str(slot_num+1)\n\t                    data_batch[key] = dataTensor_after_slot[:,slot_num,:]\n\t            #data is a dictionary. we need to edit its key if the key contains a defined const c\n\t            #where c is defined in rule #const c=v.\n\t            data_batch_keys = list(data_batch.keys())              \n", "            for key in data_batch_keys:\n\t                data_batch[self.constReplacement(key)] = data_batch.pop(key)\n\t            # Step 1: get the output of each network and initialize the gradients\n\t            networkOutput = {}\n\t            networkLLOutput = {}\n\t            #iterate over all networks            \n\t            for m in self.networkOutputs:\n\t                if m not in networkOutput:\n\t                    networkOutput[m] = {}\n\t                #iterate over all output types and forwarded the input t trough the network\n", "                networkLLOutput[m] = {}\n\t                for o in self.networkOutputs[m]: \n\t                    if o not in networkOutput[m]:\n\t                        networkOutput[m][o] = {}\n\t                    #one forward pass to get the outputs\n\t                    if self.networkTypes[m] == 'npp':\n\t                        #forward all t belonging to the same m in one pass instead of t passes\n\t                        if batched_pass:\n\t                            #collect all inputs t for every network m \n\t                            dataTensor = [data_batch.get(key).to(device=self.device) for key in self.networkOutputs[m][o].keys()]\n", "                            dataTensor = torch.cat(dataTensor)\n\t                            len_keys = len(query_batch)\n\t                            output = self.networkMapping[m].forward(\n\t                                                            dataTensor.to(self.device),\n\t                                                            marg_idx=marg_mask,\n\t                                                            type=o)\n\t                            outputs = output.split(len_keys)\n\t                            networkOutput[m][o] = {**networkOutput[m][o], **dict(zip(self.networkOutputs[m][o].keys(), outputs))}\n\t                        else:\n\t                            for t in self.networkOutputs[m][o]:\n", "                                dataTensor = data_batch[t]                                    \n\t                                #we have a list of data elements but want a Tensor of the form [batchsize,...]\n\t                                if isinstance(dataTensor, list):\n\t                                    dataTensor = torch.stack(dataTensor).squeeze(dim=1) #TODO re-enable\n\t                                #foward the data\n\t                                networkOutput[m][o][t] = self.networkMapping[m].forward(\n\t                                                                                dataTensor.to(self.device),\n\t                                                                                marg_idx=marg_mask,\n\t                                                                                type=o)\n\t                                #if the network predicts only one probability we add a placeholder for the false class\n", "                                if m in self.mvpp['networkProbSinglePred']:\n\t                                    networkOutput[m][o][t] = torch.stack((networkOutput[m][o][t], torch.zeros_like(networkOutput[m][o][t])), dim=-1)\n\t                    #store the outputs of the neural networks as a class variable\n\t                    self.networkOutputs[m][o] = networkOutput[m][o] #this is of shape [first batch entry, second batch entry,...]\n\t            #match the outputs with the hungarian matching and create a predicate to add to the logic program\n\t            #NOTE: this is a hacky solution which leaves open the question wherever we can have neural mappings in our logic program\n\t            if hungarian_matching is True:\n\t                obj_batch = {}\n\t                if \"shade\" in networkOutput:\n\t                    obj_batch['color'] = obj_enc_batch[:,:,0:9] # [500, 4,20] #[c,c,c,c,c,c,c,c,c , s,s,s,s , h,h,h, z,z,z, confidence]\n", "                    obj_batch['shape'] = obj_enc_batch[:,:,9:13]\n\t                    obj_batch['shade'] = obj_enc_batch[:,:,13:16]\n\t                    obj_batch['size'] = obj_enc_batch[:,:,16:19]\n\t                    concepts = ['color', 'shape', 'shade','size']\n\t                    slots = ['s1', 's2', 's3','s4']\n\t                    num_obs = 4\n\t                else:        \n\t                    obj_batch['size'] = obj_enc_batch[:,:,0:3] # [500, 4,20] #[c,c,c,c,c,c,c,c,c , s,s,s,s , h,h,h, z,z,z, confidence]\n\t                    obj_batch['material'] = obj_enc_batch[:,:,3:6]\n\t                    obj_batch['shape'] = obj_enc_batch[:,:,6:10]\n", "                    obj_batch['color'] = obj_enc_batch[:,:,10:19]\n\t                    concepts = ['color', 'shape', 'material','size']\n\t                    slots = ['s1', 's2', 's3','s4','s5','s6','s7','s8','s9','s10']\n\t                    num_obs = 10\n\t                kl_cost_matrix = torch.zeros((num_obs,num_obs,len(query_batch)))\n\t                #build KL cost matrix\n\t                for obj_id in range(0,num_obs):\n\t                    for slot_idx, slot in enumerate(slots):\n\t                        summed_kl = 0\n\t                        for concept in concepts:\n", "                            b = obj_batch[concept][:,obj_id].type(torch.FloatTensor)\n\t                            a = networkOutput[concept][1][slot].detach().cpu()\n\t                            summed_kl += torch.cdist(a[:,None,:],b[:,None,:]).squeeze()\n\t                        kl_cost_matrix[obj_id, slot_idx] = summed_kl \n\t                kl_cost_matrix = np.einsum(\"abc->cab\", kl_cost_matrix.cpu().numpy())\n\t                indices = np.array(\n\t                list(map(scipy.optimize.linear_sum_assignment, kl_cost_matrix)))\n\t                def slot_name_comb(x):\n\t                    return ''.join([\"slot_name_comb(o{}, s{}). \".format(i[0]+1, i[1]+1)  for i in x])\n\t                assignments = np.array(list(map(slot_name_comb, np.einsum(\"abc->acb\",indices))))\n", "                query_batch = list(map(str.__add__, query_batch, assignments))\n\t            #stack all nn outputs in matrix M\n\t            big_M = torch.zeros([ len(self.mvpp['networkProb']),len(query_batch), self.max_n], device=self.grad_comp_device)\n\t            c = 0\n\t            for m in networkOutput:\n\t                for o in networkOutput[m]:\n\t                    for t in networkOutput[m][o]:\n\t                        big_M[c,:, :networkOutput[m][o][t].shape[1]]= networkOutput[m][o][t].detach().to('cpu')\n\t                        c+=1\n\t            #set all matrices in the dmvpp class to the cpu for model computation\n", "            dmvpp.put_selection_mask_on_device('cpu')\n\t            big_M = big_M.to(device='cpu')\n\t            dmvpp.M =  dmvpp.M.to(device='cpu')\n\t            #normalize the SLASH copy of M \n\t            #big_M = normalize_M(big_M, dmvpp.non_binary_idx, dmvpp.selection_mask)\n\t            step1 = time.time()\n\t            forward_time.append(step1 - start_time)\n\t            #### Step 2: compute stable models and the gradients\n\t            #we split the network outputs such that we can put them on different processes\n\t            if vqa: \n", "                big_M_splits, query_batch_split, obj_filter_split, p_num = self.split_network_outputs(big_M, obj_filter, query_batch, p_num)\n\t            else:\n\t                big_M_splits, query_batch_split, _, p_num = self.split_network_outputs(big_M, None, query_batch, p_num)\n\t                obj_filter_split = [None] * len(query_batch_split)\n\t            split_outputs = Parallel(n_jobs=p_num,backend='loky')( #backend='loky')(\n\t                delayed(compute_models_splitwise)\n\t                (\n\t                    big_M_splits[i].detach(), query_batch_split[i],\n\t                        dmvpp, method, k_num, same_threshold, obj_filter_split[i], vqa_params\n\t                )\n", "                        for i in range(p_num))\n\t            del big_M_splits\n\t            #concatenate potential solutions, the atom indices and query p(Q) from all splits back into a single batch\n\t            model_batch_list_splits = []\n\t            models_idx_list = []\n\t            #collect the models and model computation times\n\t            for i in range(p_num):\n\t                model_batch_list_splits.extend(split_outputs[i][0])\n\t                models_idx_list.extend(split_outputs[i][1])   #batch, model, atoms\n\t            #amount of Stable models used for gradient computations per batch\n", "            sm_per_batch = np.sum([ len(sm_batch)  for splits in model_batch_list_splits for sm_batch in splits ])\n\t            sm_per_batch_list.append(sm_per_batch)\n\t            #save stable models\n\t            try:\n\t                model_batch_list = np.concatenate(model_batch_list_splits)\n\t                self.stableModels = model_batch_list\n\t            except ValueError as e:\n\t                pass\n\t                #print(\"fix later\")\n\t                #print(e)\n", "                #for i in range(0, len(model_batch_list_splits)):\n\t                #    print(\"NUM:\",i)\n\t                #    print(model_batch_list_splits[i])\n\t            step2 = time.time()\n\t            asp_time.append(step2 - step1)\n\t            #compute gradients\n\t            dmvpp.put_selection_mask_on_device(self.grad_comp_device)\n\t            big_M = big_M.to(device=self.grad_comp_device)\n\t            gradients_batch_list = []\n\t            for bidx in count(start=0, step=1):\n", "                if bidx < model_batch_list_splits.__len__():\n\t                    dmvpp.M = big_M[:,bidx,:]\n\t                    dmvpp.normalize_M()\n\t                    gradients_batch_list.append(dmvpp.mvppLearn(model_batch_list_splits[bidx], models_idx_list[bidx], self.grad_comp_device))\n\t                else:\n\t                    break\n\t            del big_M\n\t            #stack all gradients\n\t            gradient_tensor = torch.stack(gradients_batch_list)\n\t            #store the gradients, the stable models and p(Q) of the last batch processed\n", "            self.networkGradients = gradient_tensor\n\t            # Step 3: update parameters in neural networks\n\t            step3 = time.time()\n\t            gradient_time.append(step3 - step2)\n\t            networkOutput_stacked = torch.zeros([gradient_tensor.shape[1], gradient_tensor.shape[0], gradient_tensor.shape[2]], device=torch.device('cuda:0'))\n\t            gradient_tensor = gradient_tensor.swapaxes(0,1)\n\t            org_idx = (gradient_tensor.sum(dim=2) != 0)#.flatten(0,1)\n\t            #add all NN outputs which have a gradient into tensor for backward pass\n\t            c = 0\n\t            for m in networkOutput:\n", "                for o in networkOutput[m]:\n\t                    for t in networkOutput[m][o]:\n\t                        for bidx in range(0,org_idx.shape[1]): #iterate over batch\n\t                            if org_idx[c, bidx]:\n\t                                networkOutput_stacked[c,bidx, :networkOutput[m][o][t].shape[1]]= networkOutput[m][o][t][bidx]\n\t                        c+=1\n\t            #multiply every probability with its gradient\n\t            gradient_tensor.requires_grad=True\n\t            result = torch.einsum(\"abc, abc -> abc\", gradient_tensor.to(device='cuda'),networkOutput_stacked)\n\t            not_used_npps = org_idx.sum() / (result.shape[0]* result.shape[1])\n", "            result = result[result.abs() != 0 ].sum()\n\t            #in the case vqa case we need to only use the npps over existing objects\n\t            if vqa:\n\t                total_obj = 0 \n\t                for of in obj_filter:\n\t                    total_obj += 2* of + of * (of-1)\n\t                not_used_npps = org_idx.sum() / total_obj\n\t            #get the number of discrete properties, e.g. sum all npps times the atoms entailing the npps \n\t            sum_discrete_properties = sum([len(listElem) for listElem in self.mvpp['atom']]) * len(query_batch)\n\t            sum_discrete_properties = torch.Tensor([sum_discrete_properties]).to(device=self.device)\n", "            #scale to actualy used npps\n\t            sum_discrete_properties = sum_discrete_properties * not_used_npps\n\t            #get the mean over the sum of discrete properties\n\t            result_ll = result / sum_discrete_properties\n\t            #backward pass\n\t            #for gradient descent we minimize the negative log likelihood    \n\t            result_nll = -result_ll\n\t            #reset optimizers\n\t            for  m in self.optimizers:\n\t                self.optimizers[m].zero_grad()\n", "            #append the loss value\n\t            total_loss.append(result_nll.cpu().detach().numpy())\n\t            #backward pass\n\t            result_nll.backward(retain_graph=True)\n\t            #apply gradients with each optimizer\n\t            for m in self.optimizers:\n\t                self.optimizers[m].step()\n\t            last_step = time.time()\n\t            backward_time.append(last_step - step3)\n\t            if writer is not None:\n", "                writer.add_scalar('train/loss_per_batch', result_nll.cpu().detach().numpy(), batch_idx+epoch*len(dataset_loader))\n\t                writer.add_scalar('train/sm_per_batch', sm_per_batch, batch_idx+epoch*len(dataset_loader))\n\t            pbar.update()\n\t        pbar.close()\n\t        if writer is not None:\n\t            writer.add_scalar('train/forward_time', np.sum(forward_time), epoch)\n\t            writer.add_scalar('train/asp_time', np.sum(asp_time), epoch)\n\t            writer.add_scalar('train/gradient_time', np.sum(gradient_time), epoch)\n\t            writer.add_scalar('train/backward_time', np.sum(backward_time), epoch)\n\t            writer.add_scalar('train/sm_per_epoch', np.sum(sm_per_batch_list), epoch)\n", "        print(\"avg loss over batches:\", np.mean(total_loss))\n\t        print(\"1. forward time: \", np.sum(forward_time))\n\t        print(\"2. asp time:\", np.sum(asp_time))\n\t        print(\"3. gradient time:\", np.sum(gradient_time))\n\t        print(\"4. backward time: \", np.sum(backward_time))\n\t        print(\"SM processed\", np.sum(sm_per_batch_list))\n\t        return np.mean(total_loss), forward_time, asp_time, gradient_time, backward_time, sm_per_batch_list\n\t    def testNetwork(self, network, testLoader, ret_confusion=False):\n\t        \"\"\"\n\t        Return a real number in [0,100] denoting accuracy\n", "        @network is the name of the neural network or probabilisitc circuit to check the accuracy. \n\t        @testLoader is the input and output pairs.\n\t        \"\"\"\n\t        self.networkMapping[network].eval()\n\t        # check if total prediction is correct\n\t        correct = 0\n\t        total = 0\n\t        # check if each single prediction is correct\n\t        singleCorrect = 0\n\t        singleTotal = 0\n", "        #list to collect targets and predictions for confusion matrix\n\t        y_target = []\n\t        y_pred = []\n\t        with torch.no_grad():\n\t            for data, target in testLoader:\n\t                output = self.networkMapping[network](data.to(self.device))\n\t                if self.n[network] > 2 :\n\t                    pred = output.argmax(dim=-1, keepdim=True) # get the index of the max log-probability\n\t                    target = target.to(self.device).view_as(pred)\n\t                    correctionMatrix = (target.int() == pred.int()).view(target.shape[0], -1)\n", "                    y_target = np.concatenate( (y_target, target.int().flatten().cpu() ))\n\t                    y_pred = np.concatenate( (y_pred , pred.int().flatten().cpu()) )\n\t                    correct += correctionMatrix.all(1).sum().item()\n\t                    total += target.shape[0]\n\t                    singleCorrect += correctionMatrix.sum().item()\n\t                    singleTotal += target.numel()\n\t                else: \n\t                    pred = np.array([int(i[0]<0.5) for i in output.tolist()])\n\t                    target = target.numpy()\n\t                    correct += (pred.reshape(target.shape) == target).sum()\n", "                    total += len(pred)\n\t        accuracy = correct / total\n\t        if self.n[network] > 2:\n\t            singleAccuracy = singleCorrect / singleTotal\n\t        else:\n\t            singleAccuracy = 0\n\t        if ret_confusion:\n\t            confusionMatrix = confusion_matrix(np.array(y_target), np.array(y_pred))\n\t            return accuracy, singleAccuracy, confusionMatrix\n\t        return accuracy, singleAccuracy\n", "    # We interprete the most probable stable model(s) as the prediction of the inference mode\n\t    # and check the accuracy of the inference mode by checking whether the query is satisfied by the prediction\n\t    def testInferenceResults(self, dataset_loader):\n\t        \"\"\" Return a real number in [0,1] denoting the accuracy\n\t        @param dataset_loader: a dataloader object loading a dataset to test on\n\t        \"\"\"\n\t        correct = 0\n\t        len_dataset = 0\n\t        #iterate over batch\n\t        for data_batch, query_batch in dataset_loader:\n", "            len_dataset += len(query_batch)\n\t            #iterate over each entry in batch\n\t            for dataIdx in range(0, len(query_batch)):\n\t                models = self.infer(data_batch, query=':- mistake.', mvpp=self.mvpp['program_asp'],  dataIdx= dataIdx)\n\t                query,_ =  replace_plus_minus_occurences(query_batch[dataIdx])\n\t                for model in models:\n\t                    if self.satisfy(model, query):\n\t                        correct += 1\n\t                        break\n\t        accuracy = 100. * correct / len_dataset\n", "        return accuracy\n\t    def testConstraint(self, dataset_loader, mvppList):\n\t        \"\"\"\n\t        @param dataList: a list of dictionaries, where each dictionary maps terms to tensors/np-arrays\n\t        @param queryList: a list of strings, where each string is a set of constraints denoting a query\n\t        @param mvppList: a list of MVPP programs (each is a string)\n\t        \"\"\"\n\t        # we evaluate all nerual networks\n\t        for func in self.networkMapping:\n\t            self.networkMapping[func].eval()\n", "        # we count the correct prediction for each mvpp program\n\t        count = [0]*len(mvppList)\n\t        len_data = 0\n\t        for data_batch, query_batch in dataset_loader:\n\t            len_data += len(query_batch)\n\t            # data is a dictionary. we need to edit its key if the key contains a defined const c\n\t            # where c is defined in rule #const c=v.\n\t            data_batch_keys = list(data_batch.keys())\n\t            for key in data_batch_keys:\n\t                data_batch[self.constReplacement(key)] = data_batch.pop(key)\n", "            # Step 1: get the output of each neural network\n\t            for m in self.networkOutputs:\n\t                for o in self.networkOutputs[m]: #iterate over all output types and forwarded the input t trough the network\n\t                    for t in self.networkOutputs[m][o]:\n\t                        self.networkOutputs[m][o][t] = self.networkMapping[m].forward(\n\t                                                                                    data_batch[t].to(self.device),\n\t                                                                                    marg_idx=None,\n\t                                                                                    type=o)\n\t            # Step 2: turn the network outputs into a set of ASP facts            \n\t            aspFactsList = []\n", "            for bidx in range(len(query_batch)):\n\t                aspFacts = ''\n\t                for ruleIdx in range(self.mvpp['networkPrRuleNum']):\n\t                    #get the network outputs for the current element in the batch and put it into the correct rule\n\t                    probs = [self.networkOutputs[m][inf_type][t][bidx][i*self.n[m]+j] for (m, i, inf_type, t, j) in self.mvpp['networkProb'][ruleIdx]]\n\t                    if len(probs) == 1:\n\t                        atomIdx = int(probs[0] < 0.5) # t is of index 0 and f is of index 1\n\t                    else:\n\t                        atomIdx = probs.index(max(probs))\n\t                    aspFacts += self.mvpp['atom'][ruleIdx][atomIdx] + '.\\n'\n", "                aspFactsList.append(aspFacts)\n\t            # Step 3: check whether each MVPP program is satisfied\n\t            for bidx in range(len(query_batch)):\n\t                for programIdx, program in enumerate(mvppList):\n\t                    query,_ =  replace_plus_minus_occurences(query_batch[bidx])\n\t                    program,_ = replace_plus_minus_occurences(program)\n\t                    # if the program has weak constraints\n\t                    if re.search(r':~.+\\.[ \\t]*\\[.+\\]', program) or re.search(r':~.+\\.[ \\t]*\\[.+\\]', query):\n\t                        choiceRules = ''\n\t                        for ruleIdx in range(self.mvpp['networkPrRuleNum']):\n", "                            choiceRules += '1{' + '; '.join(self.mvpp['atom'][ruleIdx]) + '}1.\\n'\n\t                        mvpp = MVPP(program+choiceRules)\n\t                        models = mvpp.find_all_opt_SM_under_query_WC(query=query)\n\t                        models = [set(model) for model in models] # each model is a set of atoms\n\t                        targetAtoms = aspFacts[bidx].split('.\\n')\n\t                        targetAtoms = set([atom.strip().replace(' ','') for atom in targetAtoms if atom.strip()])\n\t                        if any(targetAtoms.issubset(model) for model in models):\n\t                            count[programIdx] += 1\n\t                    else:\n\t                        mvpp = MVPP(aspFacts[bidx] + program)\n", "                        if mvpp.find_one_SM_under_query(query=query):\n\t                            count[programIdx] += 1\n\t        for programIdx, program in enumerate(mvppList):\n\t            print('The accuracy for constraint {} is {}({}/{})'.format(programIdx+1, float(count[programIdx])/len_data,float(count[programIdx]),len_data ))\n\t    def forward_slot_attention_pipeline(self, slot_net, dataset_loader):\n\t        \"\"\"\n\t        Makes one forward pass trough the slot attention pipeline to obtain the probabilities/log likelihoods for all classes for each object. \n\t        The pipeline includes  the SlotAttention module followed by probabilisitc circuits for probabilites for the discrete properties.\n\t        @param slot_net: The SlotAttention module\n\t        @param dataset_loader: Dataloader containing a shapeworld/clevr dataset to be forwarded\n", "        \"\"\"\n\t        with torch.no_grad():\n\t            probabilities = {}  # map to store all output probabilities(posterior)\n\t            slot_map = {} #map to store all slot module outputs\n\t            for data_batch, _,_ in dataset_loader:\n\t                #forward img to get slots\n\t                dataTensor_after_slot = slot_net(data_batch['im'].to(self.device))#SHAPE [BS,SLOTS, SLOTSIZE]\n\t                #dataTensor_after_slot has shape [bs, num_slots, slot_vector_length]\n\t                _, num_slots ,_ = dataTensor_after_slot.shape \n\t                for sdx in range(0, num_slots):\n", "                    slot_map[\"s\"+str(sdx)] = dataTensor_after_slot[:,sdx,:]\n\t                #iterate over all slots and forward them through all nets (shape + color + ... )\n\t                for key in slot_map:\n\t                    if key not in probabilities:\n\t                        probabilities[key] = {}\n\t                    for network in self.networkMapping:\n\t                        posterior= self.networkMapping[network].forward(slot_map[key])#[BS, num_discrete_props]\n\t                        if network not in probabilities[key]:\n\t                            probabilities[key][network] = posterior\n\t                        else: \n", "                            probabilities[key][network] = torch.cat((probabilities[key][network], posterior))\n\t            return probabilities\n\t    def get_recall(self, logits, labels,  topk=3):\n\t        # Calculate the recall\n\t        _, pred_idx = logits.topk(topk, 1, True, True) #get idx of the biggest k values in the prediction tensor\n\t        #gather gets the elements from a given indices tensor. Here we get the elements at the same positions from our (top5) predictions\n\t        #we then sum all entries up along the axis and therefore count the top-5 entries in the labels tensors at the prediction position indices\n\t        correct = torch.sum(labels.gather(1, pred_idx), dim=1)\n\t        #now we sum up all true labels. We clamp them to be maximum top5\n\t        correct_label = torch.clamp(torch.sum(labels, dim = 1), 0, topk)\n", "        #we now can compare if the number of correctly found top-5 labels on the predictions vector is the same as on the same positions as the GT vector\n\t        #if the num of gt labels is zero (question has no answer) then we get a nan value for the div by 0 -> we replace this with recall 1 \n\t        recall = torch.mean(torch.nan_to_num(correct / correct_label,1)).item()\n\t        return recall\n\t    def testVQA(self, test_loader, p_num = 1, k=5, vqa_params= None):\n\t        \"\"\"\n\t        @param test_loader: a dataloader object \n\t        @param p_num: integer denoting the number of processes to split the batches on\n\t        @param k: denotes how many targets pred are used for the recall metric\n\t        \"\"\"\n", "        start_test = time.time()\n\t        dmvpp = MVPP(self.mvpp['program'],prob_ground=True,binary_rule_belongings= self.mvpp['binary_rule_belongings'])\n\t        networkOutput = {}\n\t        # we train all neural network models\n\t        for m in self.networkMapping:\n\t            self.networkMapping[m].eval() #torch training mode\n\t            self.networkMapping[m].module.eval() #torch training mode\n\t        pred_vector_list = []\n\t        gt_vector_list = []\n\t        #iterate over all batches\n", "        #pbar = tqdm(total=len(test_loader))\n\t        for batch_idx, (batch) in enumerate(test_loader):            \n\t            data_batch, query_batch, obj_filter, target = batch\n\t            # Step 1: get the output of each neural network\n\t            for m in self.networkOutputs:\n\t                if m not in networkOutput:\n\t                    networkOutput[m] = {}\n\t                #iterate over all inference types o and forwarded the input t trough the network\n\t                for o in self.networkOutputs[m]: \n\t                    if o not in networkOutput[m]:\n", "                        networkOutput[m][o] = {}\n\t                    if self.networkTypes[m] == 'npp':\n\t                        #collect all inputs t for every network m \n\t                        dataTensor = [data_batch.get(key).to(device=self.device) for key in self.networkOutputs[m][o].keys()]\n\t                        dataTensor = torch.cat(dataTensor)\n\t                        len_keys = len(query_batch)\n\t                        output = self.networkMapping[m].forward(\n\t                                                        dataTensor.to(self.device),\n\t                                                        marg_idx=None,\n\t                                                        type=o)\n", "                        outputs = output.split(len_keys)\n\t                        networkOutput[m][o] = {**networkOutput[m][o], **dict(zip(self.networkOutputs[m][o].keys(), outputs))}\n\t            #stack all nn outputs in matrix M\n\t            big_M = torch.zeros([ len(self.mvpp['networkProb']),len(query_batch), self.max_n], device=self.grad_comp_device)\n\t            c = 0\n\t            for m in networkOutput:\n\t                for o in networkOutput[m]:\n\t                    for t in networkOutput[m][o]:\n\t                        big_M[c,:, :networkOutput[m][o][t].shape[1]]= networkOutput[m][o][t].detach().to(self.grad_comp_device)\n\t                        c+=1\n", "            big_M_splits, query_batch_split, obj_filter_split, p_num = self.split_network_outputs(big_M, obj_filter, query_batch, p_num)\n\t            # Step 2: compute stable models using the probabilities\n\t            # we get a list of pred vectors where each index represents if the object is a target and its corresponding target probability \n\t            dmvpp.put_selection_mask_on_device('cpu')\n\t            pred_vector_list_split = Parallel(n_jobs=p_num,backend='loky')( #backend='loky')(\n\t                delayed(compute_vqa_splitwise)\n\t                (\n\t                    big_M_splits[i].detach().cpu(), query_batch_split[i],\n\t                        dmvpp,  obj_filter_split[i], k, target.shape[1], vqa_params\n\t                )\n", "                        for i in range(p_num))\n\t            #stack together the target vectors from the different procceses\n\t            pred_vector_list.append(torch.stack([torch.tensor(i) for p in pred_vector_list_split for i in p ]))\n\t            gt_vector_list.append(target.clone().detach())\n\t            #pbar.update()\n\t        #pbar.close()\n\t        #stack together the target vectors from the different batches\n\t        pred_vector = torch.cat(pred_vector_list, dim=0)\n\t        gt_vector = torch.cat(gt_vector_list, dim=0)\n\t        #compute the recall\n", "        recall = self.get_recall(pred_vector, gt_vector, 5)\n\t        #print(\"RECALL\",recall)\n\t        return recall, time.time() - start_test\n"]}
{"filename": "src/SLASH/mvpp.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tNeurASP: Embracing Neural Networks into Answer Set Programming\n\tZhun Yang, Adam Ishay, Joohyung Lee. Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence\n\tMain track. Pages 1755-1762.\n\thttps://github.com/azreasoners/NeurASP\n\t\"\"\"\n\tfrom doctest import FAIL_FAST\n\timport itertools\n\tfrom itertools import count\n", "import math\n\timport os.path\n\timport os\n\timport re\n\timport sys\n\timport time\n\timport clingo\n\timport numpy as np\n\timport torch\n\tclass MVPP(object):                 \n", "    def __init__(self, program, k=1, eps=0.000001, prob_ground = False, binary_rule_belongings={}, max_n=0):\n\t        self.k = k\n\t        self.eps = eps\n\t        # each element in self.pc is a list of atoms (one list for one prob choice rule)\n\t        self.pc = []\n\t        # each element in self.parameters is a list of probabilities\n\t        self.parameters = []\n\t        # each element in self.learnable is a list of Boolean values\n\t        self.learnable = []\n\t        # self.asp is the ASP part of the LPMLN program\n", "        self.asp = \"\"\n\t        # self.pi_prime is the ASP program \\Pi' defined for the semantics\n\t        self.pi_prime = \"\"\n\t        # self.remain_probs is a list of probs, each denotes a remaining prob given those non-learnable probs\n\t        self.remain_probs = []\n\t        # self.ga_map is a dictionary containing the map ground atom <-> (ruleId, atomId).\n\t        # Thereby, ruleId is a NPP call and atomId is graound atom (outcome) index in a NPP call.\n\t        self.ga_map = {}\n\t        # self.M is the 2d tensor containig probabilities for all NPP calls\n\t        self.M = torch.empty((2,3), dtype=torch.float32)\n", "        self.max_n = max_n\n\t        self.device = 'cpu'\n\t        self.binary_rule_belongings = binary_rule_belongings\n\t        self.pc, self.parameters, self.learnable, self.asp, self.pi_prime, self.remain_probs, self.ga_map = self.parse(program, prob_ground)\n\t        self.get_non_binary_idx()\n\t        if self.parameters != []:\n\t            self.create_selection_mask()\n\t    def debug_mvpp(self):\n\t        print(\"pc\",self.pc)\n\t        print(\"params\", self.parameters)\n", "        print(\"learnable\", self.learnable)\n\t        print(\"asp\", self.asp)\n\t        print(\"pi_prime\", self.pi_prime)\n\t    def get_non_binary_idx(self):\n\t        non_binary_idx = []\n\t        #from the idx to delete generate the ones to keep\n\t        for i in count(start=0, step=1):\n\t            if i < len(self.parameters):\n\t                if i not in self.binary_rule_belongings:\n\t                    non_binary_idx.append(i)\n", "            else:\n\t                break\n\t        self.non_binary_idx = non_binary_idx\n\t    def create_selection_mask(self):\n\t        selection_mask = torch.zeros([self.parameters.__len__(), max(par.__len__() for par in self.parameters)], dtype=torch.bool, device='cpu')\n\t        for rule_idx, rule in enumerate(self.parameters):\n\t            selection_mask[rule_idx,0:len(self.parameters[rule_idx])] = True\n\t        self.selection_mask = selection_mask\n\t    def put_selection_mask_on_device(self, device):\n\t        self.selection_mask = self.selection_mask.to(device=device)\n", "    def parse(self, program, prob_ground):\n\t        pc = []\n\t        parameters = []\n\t        learnable = []\n\t        asp = \"\"\n\t        pi_prime = \"\"\n\t        remain_probs = []\n\t        ga_map = {}\n\t        npp_choices = {}\n\t        npp_choices_inftype_sorted = []\n", "        lines = []\n\t        # if program is a file\n\t        if os.path.isfile(program):\n\t            with open(program, 'r') as program:\n\t                lines = program.readlines()\n\t        # if program is a string containing all rules of an LPMLN program\n\t        elif type(program) is str and re.sub(r'\\n%[^\\n]*', '\\n', program).strip().endswith(('.', ']')):\n\t            lines = program.split('\\n')\n\t        else:\n\t            print(program)\n", "            print(\"Error! The MVPP program {} is not valid.\".format(program))\n\t            sys.exit()\n\t        #iterate over all lines\n\t        # 1. build the choice rules 1{...}1\n\t        # 2. store which npp values are learnable\n\t        for line in lines:\n\t            #if re.match(r\".*[0-1]\\.?[0-9]*\\s.*;.*\", line): \n\t            if re.match(r'.*[0-1]\\.[0-9]*\\s.*(;|\\.).*', line):\n\t                #out = re.search(r'@[0-9]*\\.[0-9]*([a-z][a-zA-Z0-9_]*)\\(([0-9]*),([0-9]),([a-z]*[a-zA-Z0-9]*)', line.replace(\" \",\"\"), flags=0)\n\t                out = re.search(r'@[0-9]*\\.[0-9]*([a-z][a-zA-Z0-9_]*)\\(([0-9]*),([0-9]),([a-z]*[a-zA-Z0-9]*|[a-z]*[a-zA-Z0-9]*,[a-z]*[a-zA-Z0-9]*),([a-z]*[a-zA-Z0-9_]*\\))', line.replace(\" \",\"\"), flags=0)\n", "                npp_name = out.group(1)\n\t                npp_e = out.group(2)\n\t                npp_inftype = out.group(3)# is the identifier for the inference type e.g. 1,2,3,4 \n\t                npp_input = out.group(4)\n\t                list_of_atoms = []\n\t                list_of_probs = []\n\t                list_of_bools = []\n\t                choices = line.strip()[:-1].split(\";\")\n\t                for choice in choices:\n\t                    prob, atom = choice.strip().split(\" \", maxsplit=1)\n", "                    # Note that we remove all spaces in atom since clingo output does not contain space in atom\n\t                    list_of_atoms.append(atom.replace(\" \", \"\"))\n\t                    if prob.startswith(\"@\"):\n\t                        list_of_probs.append(float(prob[1:]))\n\t                        list_of_bools.append(True)\n\t                    else:\n\t                        list_of_probs.append(float(prob))\n\t                        list_of_bools.append(False)\n\t                pc.append(list_of_atoms)\n\t                parameters.append(list_of_probs)\n", "                learnable.append(list_of_bools)\n\t                #save a dictionary containing the npp and add all inference type instances to it\n\t                if (npp_name, npp_input, npp_e) not in npp_choices:\n\t                    npp_choices[(npp_name, npp_input, npp_e )]= []\n\t                npp_choices[(npp_name, npp_input,npp_e)] += list_of_atoms\n\t                #save npp to an ordered list to access them for probabilistic grounding\n\t                npp_choices_inftype_sorted.append([npp_name, npp_input, npp_inftype, npp_e, np.array(list_of_atoms) ])\n\t            else:\n\t                asp += (line.strip()+\"\\n\")\n\t        self.npp_choices_inftype_sorted = npp_choices_inftype_sorted\n", "        if prob_ground is False:\n\t            #create choice rules for npp atoms\n\t            for atom in npp_choices.keys():\n\t                if len(npp_choices[atom]) == 1:\n\t                    pi_prime += \"1{\"+\"; \".join(npp_choices[atom])+\"}1.\\n\"\n\t                else:\n\t                    pi_prime += \"0{\"+\";\".join(npp_choices[atom])+\"}1.\\n\"\n\t        pi_prime += asp\n\t        for ruleIdx, list_of_bools in enumerate(learnable):\n\t            remain_prob = 1\n", "            for atomIdx, b in enumerate(list_of_bools):\n\t                if b == False:\n\t                    remain_prob -= parameters[ruleIdx][atomIdx]\n\t            remain_probs.append(remain_prob)\n\t        for ruleIdx, list_of_atoms in enumerate(pc):\n\t            for atomIdx, atom in enumerate(list_of_atoms):\n\t                ga_map[atom] = (ruleIdx, atomIdx)\n\t        return pc, parameters, learnable, asp, pi_prime, remain_probs, ga_map\n\t    def normalize_M(self):\n\t        tmp = self.M[self.non_binary_idx]\n", "        tmp[self.M[self.non_binary_idx] >=1] = 1-self.eps\n\t        tmp[self.M[self.non_binary_idx] <=self.eps] = self.eps\n\t        self.M[self.non_binary_idx] = tmp\n\t        self.M *= self.selection_mask\n\t        # determine denominator\n\t        denom = self.M[self.non_binary_idx].sum(dim=1)\n\t        self.M[self.non_binary_idx] /= denom[:,None]\n\t    def prob_of_interpretation(self, I, model_idx_list = None):\n\t        prob = 1.0\n\t        #if we have indices\n", "        if model_idx_list is not None: \n\t            prob = 1.0           \n\t            for rule_idx, atom_idx in model_idx_list:\n\t                prob *= self.M[rule_idx][atom_idx]\n\t            return prob\n\t        else:\n\t            # I must be a list of atoms, where each atom is a string\n\t            while not isinstance(I[0], str):\n\t                I = I[0]\n\t            for _, atom in enumerate(I):\n", "                if atom in self.ga_map:\n\t                    ruleIdx, atomIdx = self.ga_map[atom]\n\t                    prob *= self.M[ruleIdx][atomIdx]\n\t            return prob\n\t   # k = 0 means to find all stable models\n\t    def find_k_SM_under_query(self, query, k=3):\n\t        program = self.pi_prime + query\n\t        clingo_control = clingo.Control([\"--warn=none\", str(int(k))])\n\t        models = []\n\t        try:\n", "            clingo_control.add(\"base\", [], program)\n\t        except:\n\t            print(\"\\nPi': \\n{}\".format(program))\n\t        clingo_control.ground([(\"base\", [])])\n\t        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n\t        if len(models) ==0:\n\t            exit()\n\t        models = [[str(atom) for atom in model] for model in models]\n\t        return models\n\t    # we assume query is a string containing a valid Clingo program, \n", "    # and each query is written in constraint form\n\t    def find_one_SM_under_query(self, query):\n\t        return self.find_k_SM_under_query(query, 1)\n\t    # we assume query is a string containing a valid Clingo program, \n\t    # and each query is written in constraint form\n\t    def find_all_SM_under_query(self, query):\n\t        return self.find_k_SM_under_query(query, 0)\n\t    def get_pareto_idx(self,arr, threshold, k=np.inf):\n\t        \"\"\"\n\t        Returns a list of indices ordered by ascending probability value that sum up to be bigger than a given threshold\n", "        @param arr: array of probabilites\n\t        @param threshold: treshold of summed probabilities to keep\n\t        \"\"\"\n\t        sum = 0\n\t        idx_list = []\n\t        #sort probability indices in descending order\n\t        #sorted_idx = np.argsort(arr)[::-1]\n\t        sorted_idx = torch.argsort(arr,descending=True)\n\t        #iterate over indeces and add them to the list until the sum of the corresponding reaches the treshold\n\t        for i in sorted_idx:\n", "            sum += arr[i] \n\t            idx_list.append(i.cpu())\n\t            if idx_list.__len__() == k:\n\t                return idx_list\n\t            if sum >= threshold:\n\t                return idx_list\n\t        return sorted_idx\n\t    def find_SM_with_same(self, query, k=1, threshold=0.99):\n\t        pi_prime = self.pi_prime\n\t        #create choice rules for npp atoms\n", "        # add choice rules for NPPs\n\t        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\t            #set threshold from dict or scalar value\n\t            if type(threshold) == dict:\n\t                t = threshold[rule[0]] #select npp specific threshold\n\t            else:\n\t                t = threshold\n\t            #if we have a binary npp\n\t            if rule_idx in self.binary_rule_belongings:\n\t                for atom_idx, atom in enumerate(self.pc[rule_idx]):\n", "                    if self.M[rule_idx][atom_idx] > 1 - t:\n\t                        pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n\t            else:\n\t                #get the ids after applying same with threshold t\n\t                k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t)\n\t                pi_prime += \"1{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\t        program = pi_prime + query\n\t        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n\t        models = []\n\t        try:\n", "            clingo_control.add(\"base\", [], program)\n\t        except:\n\t            print(\"\\nPi': \\n{}\".format(program))\n\t        clingo_control.ground([(\"base\", [])])\n\t        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n\t        models = [[str(atom) for atom in model] for model in models]  \n\t        if len(models) > 0:\n\t            return models[ -np.minimum(len(models),k) :]\n\t        else:\n\t            return []\n", "    def find_k_most_probable_SM_with_same(self, query, k=1, threshold=0.99):\n\t        pi_prime = self.pi_prime\n\t        #create choice rules for npp atoms\n\t        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\t            #set threshold from dict or scalar value\n\t            if type(threshold) == dict:\n\t                t = threshold[rule[0]] #select npp specific threshold\n\t            else:\n\t                t = threshold\n\t            #if we have a binary npp\n", "            if rule_idx in self.binary_rule_belongings:\n\t                for atom_idx, atom in enumerate(self.pc[rule_idx]):\n\t                    if self.M[rule_idx][atom_idx] > 1 - t:\n\t                        pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n\t            else:\n\t                #get the ids after applying same with threshold t\n\t                k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t)\n\t                pi_prime += \"1{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\t                # for each probabilistic rule with n atoms, add n weak constraints\n\t                for atomIdx in k_idx:\n", "                    if self.M[rule_idx][atomIdx] < 0.00674:\n\t                        penalty = -1000 * -5\n\t                    else:\n\t                        #penalty = int(-1000 * math.log(self.parameters[rule_idx][atomIdx]))\n\t                        penalty = int(-1000 * math.log(self.M[rule_idx][atomIdx]))\n\t                    pi_prime += ':~ {}. [{}, {}, {}]\\n'.format(self.pc[rule_idx][atomIdx], penalty, rule_idx, atomIdx)\n\t        program = pi_prime + query\n\t        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n\t        models = []\n\t        try:\n", "            clingo_control.add(\"base\", [], program)\n\t        except:\n\t            print(\"\\nPi': \\n{}\".format(program))\n\t        clingo_control.ground([(\"base\", [])])\n\t        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n\t        models = [[str(atom) for atom in model] for model in models]  \n\t        if len(models) > 0:\n\t            return models[ -np.minimum(len(models),k) :]\n\t        else:\n\t            return []\n", "    # there might be some duplications in SMs when optimization option is used\n\t    # and the duplications are removed by this method\n\t    def remove_duplicate_SM(self, models):\n\t        models.sort()\n\t        return list(models for models,_ in itertools.groupby(models))\n\t    # Note that the MVPP program cannot contain weak constraints\n\t    def find_all_most_probable_SM_under_query_noWC(self, query):\n\t        \"\"\"Return a list of stable models, each is a list of strings\n\t        @param query: a string of a set of constraints/facts\n\t        \"\"\"\n", "        program = self.pi_prime + query + '\\n'\n\t        # for each probabilistic rule with n atoms, add n weak constraints\n\t        for ruleIdx, atoms in enumerate(self.pc):\n\t            for atomIdx, atom in enumerate(atoms):\n\t                if self.M[ruleIdx][atomIdx] < 0.00674:\n\t                    penalty = -1000 * -5\n\t                else:\n\t                    penalty = int(-1000 * math.log(self.M[ruleIdx][atomIdx]))\n\t                program += ':~ {}. [{}, {}, {}]\\n'.format(atom, penalty, ruleIdx, atomIdx)\n\t        clingo_control = clingo.Control(['--warn=none', '--opt-mode=optN', '0', '-t', '8'])\n", "        models = []\n\t        clingo_control.add(\"base\", [], program)\n\t        clingo_control.ground([(\"base\", [])])\n\t        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)) if model.optimality_proven else None)\n\t        models = [[str(atom) for atom in model] for model in models]\n\t        return self.remove_duplicate_SM(models)\n\t    def find_k_most_probable_SM_under_query_noWC(self, query='', k = 1):\n\t        \"\"\"Return a list of a single stable model, which is a list of strings\n\t        @param query: a string of a set of constraints/facts\n\t        \"\"\"\n", "        program = self.pi_prime + query + '\\n'\n\t        # for each probabilistic rule with n atoms, add n weak constraints\n\t        for ruleIdx, atoms in enumerate(self.pc):\n\t            for atomIdx, atom in enumerate(atoms):\n\t                if self.M[ruleIdx][atomIdx] < 0.00674: #< 0.00674:\n\t                    penalty = -1000 * -5\n\t                else:\n\t                    penalty = int(-1000 * math.log(self.M[ruleIdx][atomIdx]))\n\t                program += ':~ {}. [{}, {}, {}]\\n'.format(atom, penalty, ruleIdx, atomIdx)\n\t        clingo_control = clingo.Control(['--warn=none', '-t', '8'])#8 parallel mode param\n", "        models = []\n\t        clingo_control.add(\"base\", [], program)\n\t        clingo_control.ground([(\"base\", [])])\n\t        clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n\t        models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n\t        return models[ -np.minimum(len(models),k) :] #return the last k models. The models are ordered by optimization values e.g how likely they are\n\t    def find_SM_vqa_with_same(self, query='', k=1, obj_filter=np.inf, threshold=0.99, vqa_params= {}, train=True):\n\t        \"\"\"Return a list of a single stable model, which is a list of strings\n\t        @param query: a string of a set of constraints/facts\n\t        \"\"\"\n", "        pi_prime = self.pi_prime\n\t        temp = query.split(\"///\")\n\t        query = temp[0]\n\t        temp.pop(0)\n\t        attr_relation_filter = temp\n\t        l = vqa_params['l']\n\t        l_split = vqa_params['l_split'] #change to 20\n\t        num_names = vqa_params['num_names']\n\t        max_models = vqa_params[\"max_models\"]\n\t        asp_timeout = vqa_params[\"asp_timeout\"]\n", "        rel_atoms = {} #dictionary which contains all atom idx as keys and ruleIdx as values\n\t        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\t            #set threshold from dict or scalar value\n\t            if type(threshold) == dict:\n\t                t = threshold[rule[0]] #select npp specific threshold\n\t            else:\n\t                t = threshold\n\t            #check what the object number for filtering #relation (5, 7) min 5 max 7\n\t            obj_idx_min = np.array(rule[1].split(\",\")).astype(np.int).min()\n\t            obj_idx_max = np.array(rule[1].split(\",\")).astype(np.int).max()\n", "            #during training we only consider target objects \n\t            if obj_idx_min < obj_filter and obj_idx_max < obj_filter:\n\t                #if we have a binary npp\n\t                if rule_idx  in self.binary_rule_belongings:\n\t                    for atom_idx, atom in enumerate(self.pc[rule_idx]):\n\t                        if self.M[rule_idx][atom_idx] > 1 - t:\n\t                            if any(\",\"+ar_filter+\")\" in self.pc[rule_idx][atom_idx] for ar_filter in attr_relation_filter):\n\t                                pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n\t                else:\n\t                    if 'relation' in self.pc[rule_idx][0]:\n", "                        for atom_idx, atom in enumerate(self.pc[rule_idx]):\n\t                            if any(\",\"+ar_filter+\")\" in atom for ar_filter in attr_relation_filter):\n\t                                if atom_idx not in rel_atoms.keys():\n\t                                    rel_atoms[atom_idx] = [rule_idx]\n\t                                else:\n\t                                    rel_atoms[atom_idx].append(rule_idx)\n\t                    else: \n\t                        #get the ids after applying same with threshold t\n\t                        if train: \n\t                            k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t, num_names)\n", "                        else:\n\t                            k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t, num_names)\n\t                            #on data generalization test we used -1\n\t                        pi_prime += \"0{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\t        program = pi_prime + query + '\\n'\n\t        models = []\n\t        #set number of relations to be considered\n\t        #check if relations exist\n\t        if rel_atoms:\n\t            #if all ruleIdx for relations are smaller than l use this number instead\n", "            if len(rel_atoms[list(rel_atoms.keys())[0]]) < l:\n\t                l = len(rel_atoms[list(rel_atoms.keys())[0]])\n\t                l_split = l # set l split also to l because we cant make splits m of a list with n elements if m > n \n\t        #if we dont have any relations continue with solving as usual\n\t        else:\n\t            clingo_control = clingo.Control([str(0),'--warn=none', \"--project\"])#8 parallel mode param\n\t            #if we dont have have relations\n\t            clingo_control.add(\"base\", [], program)\n\t            clingo_control.ground([(\"base\", [])])\n\t            try:\n", "                clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n\t            except:\n\t                print(\"smth went wrong during solving\")\n\t            models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n\t            #print(\"one query done without relations\")\n\t            return models\n\t        #contains relation entries\n\t        rel_atoms_most_prob= {}\n\t        #for C2 keep all relataions in place\n\t        # l = len(rel_atoms[list(rel_atoms.keys())[0]])\n", "        #for every relation get the l likeliest relations\n\t        for key in rel_atoms:\n\t            top_l_idx = self.M[rel_atoms[key], key].topk(l)[1]\n\t            for i in np.array(rel_atoms[key])[top_l_idx]:\n\t                if key not in rel_atoms_most_prob:\n\t                    rel_atoms_most_prob[key] = [i]\n\t                else:\n\t                    rel_atoms_most_prob[key].append(i)\n\t        rel_splits = np.array_split(np.arange(l), int(l / l_split))\n\t        # rel_splits = np.array_split(np.arange(l), 1)\n", "        #contains all relations that have to be considerd iteratively\n\t        atoms_rel = {}\n\t        ground_times = []\n\t        solve_times = []\n\t        models_prev_it = []\n\t        #iteratively add all different relations \n\t        solving_start = time.time()\n\t        for split in rel_splits:\n\t            models = []\n\t            # transform dict of atom_idx:rule_idx to dict of rule_idx:atom_idx\n", "            for key in rel_atoms_most_prob.keys():\n\t                for atom in np.array(rel_atoms_most_prob[key])[split]:\n\t                    if atom not in atoms_rel:\n\t                        atoms_rel[atom] = [key]\n\t                    else:\n\t                        atoms_rel[atom].append(key)\n\t            #build the relation string\n\t            rel_str = \"\"\n\t            for rule_idx in atoms_rel.keys():\n\t                atom_indices = atoms_rel[rule_idx]\n", "                rel_str += \"0{\"+\"; \".join([self.pc[rule_idx][atom_idx] for atom_idx in atom_indices])+\"}1.\\n\"\n\t            clingo_control = clingo.Control([str(max_models),'--warn=none', \"--project\"])#8 parallel mode param\n\t            program_with_rel = program + rel_str\n\t            t1 = time.time()\n\t            try:\n\t                clingo_control.add(\"base\", [], program_with_rel)\n\t                clingo_control.ground([(\"base\", [])])\n\t            except:\n\t                print(\"smth went wrong during grounding\")\n\t                f = open(\"dump/grounding_error\"+str(time.time())+\".txt\", \"a\")\n", "                f.write(program_with_rel)\n\t                f.close()\n\t                return models_prev_it\n\t            t2 = time.time()\n\t            ground_time = t2-t1\n\t            ground_times.append(ground_time)\n\t            try:\n\t                with clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)), async_=True) as hnd:\n\t                    hnd.wait(asp_timeout)\n\t                    hnd.cancel()\n", "            except:\n\t                print(\"smth went wrong during solving\")\n\t                f = open(\"dump/solving_error\"+str(time.time())+\".txt\", \"a\")\n\t                f.write(program_with_rel)\n\t                f.close()\n\t                return models_prev_it\n\t            t3 = time.time()\n\t            solve_time = t3-t2\n\t            solve_times.append(solve_time)\n\t            models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n", "            if len(models_prev_it) <= len(models):\n\t                models_prev_it = models #store models so we can return them when the solving fails\n\t            else:\n\t                return models_prev_it #if we have more models in the previous iteration return them\n\t            #if we have enough models or the time is over \n\t            if len(models) > max_models or (time.time()-solving_start) >= asp_timeout or ground_time >= asp_timeout:\n\t                #print(\"1.one query done with relations\",len(models), (time.time()-solving_start), \"ground times\", ground_time, \"solve times\", solve_time)\n\t                break\n\t        return models\n\t    def find_all_opt_SM_under_query_WC(self, query):\n", "        \"\"\" Return a list of stable models, each is a list of strings\n\t        @param query: a string of a set of constraints/facts\n\t        \"\"\"\n\t        program = self.pi_prime + query\n\t        clingo_control = clingo.Control(['--warn=none', '--opt-mode=optN', '0'])\n\t        models = []\n\t        try:\n\t            clingo_control.add(\"base\", [], program)\n\t        except:\n\t            print('\\nSyntax Error in Program: Pi\\': \\n{}'.format(program))\n", "            sys.exit()\n\t        clingo_control.ground([(\"base\", [])])\n\t        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)) if model.optimality_proven else None)\n\t        models = [[str(atom) for atom in model] for model in models]\n\t        return self.remove_duplicate_SM(models)\n\t    # compute P(Q)\n\t    def inference_query_exact(self, query):\n\t        prob = 0\n\t        models = self.find_all_SM_under_query(query)\n\t        for I in models:\n", "            prob += self.prob_of_interpretation(I)\n\t        return prob\n\t    # computes P(Q) given a list of stable models satisfying Q\n\t    def sum_probability_for_stable_models(self, models):\n\t        prob = 0\n\t        for I in models:\n\t            prob += self.prob_of_interpretation(I)\n\t        return prob\n\t    def gradient(self, ruleIdx, atomIdx, query):\n\t        # we will compute P(I)/p_i where I satisfies query and c=v_i\n", "        p_query_i = 0\n\t        # we will compute P(I)/p_j where I satisfies query and c=v_j for i!=j\n\t        p_query_j = 0\n\t        # we will compute P(I) where I satisfies query\n\t        p_query = 0\n\t        # 1st, we generate all I that satisfies query\n\t        models = self.find_k_SM_under_query(query, k=3)\n\t        # 2nd, we iterate over each model I, and check if I satisfies c=v_i\n\t        c_equal_vi = self.pc[ruleIdx][atomIdx]\n\t        p_i = self.parameters[ruleIdx][atomIdx]\n", "        for I in models:\n\t            p_I = self.prob_of_interpretation(I)\n\t            p_query += p_I\n\t            if c_equal_vi in I:\n\t                p_query_i += p_I/p_i\n\t            else:\n\t                for atomIdx2, p_j in enumerate(self.parameters[ruleIdx]):\n\t                    c_equal_vj = self.pc[ruleIdx][atomIdx2]\n\t                    if c_equal_vj in I:\n\t                        p_query_j += p_I/p_j\n", "        # 3rd, we compute gradient\n\t        gradient = (p_query_i-p_query_j)/p_query\n\t        return gradient\n\t    def collect_atom_idx(self, models):\n\t        models_idx = []\n\t        for model in models:\n\t            model_idx = []\n\t            #add a one for every atom that is in the model\n\t            for i in count(start=0, step=1):\n\t                if i < model.__len__():\n", "                    if model[i] in self.ga_map:\n\t                        ruleIdx, atomIdx = self.ga_map[model[i]]\n\t                        model_idx.append((ruleIdx,atomIdx))                        \n\t                else:\n\t                    break\n\t            models_idx.append(model_idx)\n\t        return models_idx\n\t    # Function generates a 2d gradient mask for one model\n\t    def gen_grad_mask(self, model_idx_list, grad_device):\n\t        '''\n", "        generates a positive gradient mask and a negative gradient mask\n\t        '''\n\t        gradient_mask = torch.zeros(self.M.shape, dtype=torch.float, device=grad_device)\n\t        gradient_mask_neg = torch.zeros(self.M.shape, dtype=torch.float, device=grad_device)\n\t        #add a one for every atom that is in the model\n\t        for i in count(start=0, step=1):\n\t            if i < model_idx_list.__len__():\n\t                #ruleIdx, atomIdx = self.ga_map[model[i]]\n\t                ruleIdx, atomIdx = model_idx_list[i]\n\t                gradient_mask[ruleIdx][atomIdx] = 1\n", "                if ruleIdx not in self.binary_rule_belongings:\n\t                    gradient_mask_neg[ruleIdx] = -1\n\t                    gradient_mask_neg[ruleIdx][atomIdx] = 0\n\t            else:\n\t                break\n\t        return gradient_mask, gradient_mask_neg\n\t    def mvppLearn(self, models, model_idx_list, grad_device):\n\t        probs = []\n\t        #compute P(I) for every model I\n\t        for i in count(start=0, step=1):\n", "            if i < models.__len__():\n\t                probs.append(self.prob_of_interpretation(models[i], model_idx_list[i]))\n\t            else:\n\t                break\n\t        probs = torch.tensor(probs, dtype=torch.float, device=grad_device)\n\t        denominator = probs.sum()\n\t        #if the model is empty return an empty gradient matrix\n\t        if len(models) == 0 or denominator == 0:\n\t            return torch.zeros([len(self.parameters), self.max_n], dtype=torch.float, device=grad_device)\n\t        summed_numerator = torch.zeros([len(self.parameters), self.max_n], dtype=torch.float, device=grad_device)\n", "        #create an tensor for every model\n\t        splits = torch.split(torch.tensor(np.arange(0, len(models))),10)\n\t        #iterate over all splits\n\t        for s in count(start=0, step=1):\n\t            if s < splits.__len__():\n\t                # Calculate gradients in tensor fashion\n\t                gradient_mask = []\n\t                gradient_mask_neg=[]\n\t                #iterate over all splits in models\n\t                for i in count(start=0, step=1):\n", "                    if i < splits[s].__len__():\n\t                        pos, neg = self.gen_grad_mask(model_idx_list[splits[s][i]],grad_device)\n\t                        gradient_mask.append(pos)\n\t                        gradient_mask_neg.append(neg)\n\t                    else:\n\t                        break\n\t                gradient_mask = torch.stack(gradient_mask) *  self.selection_mask\n\t                gradient_mask_neg = torch.stack(gradient_mask_neg) * self.selection_mask\n\t                if gradient_mask.dim() == 2: #if we only have one model\n\t                    gradient_mask.unsqueeze(0)\n", "                    gradient_mask_neg.unsqueeze(0)\n\t                #create the gradient tensor:\n\t                #generate c=vi\n\t                c_eq_vi = torch.einsum('kij,ij -> kij', gradient_mask, self.M)\n\t                #compute sum of atoms in c=vi \n\t                c_eq_vi_sum = torch.einsum('kij -> ki', c_eq_vi)\n\t                #generate c!=vi from the sum of atoms in c=vi\n\t                c_not_eq_vi = torch.einsum('kij,ki -> kij', gradient_mask_neg, c_eq_vi_sum)\n\t                #numerator is the sum of both P(I)/c=vi and P(I)/c!=vi (no sign flip necessary due to the correct mask)\n\t                numerator = c_eq_vi + c_not_eq_vi\n", "                numerator[numerator != 0] = 1/ numerator[numerator != 0]  \n\t                numerator = torch.einsum('kij,k -> kij',numerator , probs[splits[s]])\n\t                #sum over all potential solutions\n\t                summed_numerator += torch.einsum('kij -> ij', numerator)\n\t                #gradient is the fraction of both\n\t            else:\n\t                break\n\t        grad_tensor = summed_numerator/denominator\n\t        return grad_tensor\n\t    # gradients are stored in numpy array instead of list\n", "    # query is a string\n\t    def gradients_one_query(self, query, opt=False, k=0):\n\t        \"\"\"Return an np-array denoting the gradients\n\t        @param query: a string for query\n\t        @param opt: a Boolean denoting whether we use optimal stable models instead of stable models\n\t        \"\"\"\n\t        if opt:\n\t            models = self.find_all_opt_SM_under_query_WC(query)\n\t        else:\n\t            models = self.find_k_SM_under_query(query, k)\n", "        return self.mvppLearn(models), models\n\t    # gradients are stored in numpy array instead of list\n\t    def gradients_multi_query(self, list_of_query):\n\t        gradients = [[0.0 for item in l] for l in self.parameters]\n\t        for query in list_of_query:\n\t            gradients = [[c+d for c,d in zip(i,j)] for i,j in zip(gradients,self.gradients_one_query(query))]\n\t        return gradients\n\t    # list_of_query is either a list of strings or a file containing queries separated by \"#evidence\"\n\t    def learn_exact(self, list_of_query, lr=0.01, thres=0.0001, max_iter=None):\n\t        # if list_of_query is an evidence file, we need to first turn it into a list of strings\n", "        if type(list_of_query) is str and os.path.isfile(list_of_query):\n\t            with open(list_of_query, 'r') as f:\n\t                list_of_query = f.read().strip().strip(\"#evidence\").split(\"#evidence\")\n\t        print(\"Start learning by exact computation with {} queries...\\n\\nInitial parameters: {}\".format(len(list_of_query), self.parameters))\n\t        time_init = time.time()\n\t        check_continue = True\n\t        iteration = 1\n\t        while check_continue:\n\t            old_parameters = self.parameters\n\t            print(\"\\n#### Iteration {} ####\\n\".format(iteration))\n", "            check_continue = False\n\t            dif = [[lr*grad for grad in l] for l in self.gradients_multi_query(list_of_query)]\n\t            for ruleIdx, list_of_bools in enumerate(self.learnable):\n\t            # 1st, we turn each gradient into [-0.2, 0.2]\n\t                for atomIdx, b in enumerate(list_of_bools):\n\t                    if b == True:\n\t                        if dif[ruleIdx][atomIdx] > 0.2 :\n\t                            dif[ruleIdx][atomIdx] = 0.2\n\t                        elif dif[ruleIdx][atomIdx] < -0.2:\n\t                            dif[ruleIdx][atomIdx] = -0.2\n", "            self.parameters = [[c+d for c,d in zip(i,j)] for i,j in zip(dif,self.parameters)]\n\t            self.normalize_probs()\n\t            # we termintate if the change of the parameters is lower than thres\n\t            dif = [[abs(c-d) for c,d in zip(i,j)] for i,j in zip(old_parameters,self.parameters)]\n\t            print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n\t            print(\"Current parameters: {}\".format(self.parameters))\n\t            maxdif = max([max(l) for l in dif])\n\t            print(\"Max change on probabilities: {}\".format(maxdif))\n\t            iteration += 1\n\t            if maxdif > thres:\n", "                check_continue = True\n\t            if max_iter is not None:\n\t                if iteration > max_iter:\n\t                    check_continue = False\n\t        print(\"\\nFinal parameters: {}\".format(self.parameters))\n\t    ##############################\n\t    ####### Sampling Method ######\n\t    ##############################\n\t    # it will generate k sample stable models for a k-coherent program under a specific total choice\n\t    def k_sample(self):\n", "        asp_with_facts = self.asp\n\t        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n\t        models = []\n\t        for ruleIdx,list_of_atoms in enumerate(self.pc):\n\t            tmp = np.random.choice(list_of_atoms, 1, p=self.parameters[ruleIdx])\n\t            asp_with_facts += tmp[0]+\".\\n\"\n\t        clingo_control.add(\"base\", [], asp_with_facts)\n\t        clingo_control.ground([(\"base\", [])])\n\t        result = clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n\t        models = [[str(atom) for atom in model] for model in models]\n", "        return models\n\t    # it will generate k*num sample stable models\n\t    def sample(self, num=1):\n\t        models = []\n\t        for i in range(num):\n\t            models = models + self.k_sample()\n\t        return models\n\t    # it will generate at least num of samples that satisfy query\n\t    def sample_query(self, query, num=50):\n\t        count = 0\n", "        models = []\n\t        while count < num:\n\t            asp_with_facts = self.asp\n\t            asp_with_facts += query\n\t            clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n\t            models_tmp = []\n\t            for ruleIdx,list_of_atoms in enumerate(self.pc):\n\t                p = np.array(self.parameters[ruleIdx]) \n\t                p /= p.sum()\n\t                tmp = np.random.choice(list_of_atoms, 1, p=p)\n", "                asp_with_facts += tmp[0]+\".\\n\"\n\t            clingo_control.add(\"base\", [], asp_with_facts)\n\t            clingo_control.ground([(\"base\", [])])\n\t            result = clingo_control.solve([], lambda model: models_tmp.append(model.symbols(shown=True)))\n\t            if str(result) == \"SAT\":\n\t                models_tmp = [[str(atom) for atom in model] for model in models_tmp]\n\t                count += len(models_tmp)\n\t                models = models + models_tmp\n\t            elif str(result) == \"UNSAT\":\n\t                pass\n", "            else:\n\t                print(\"Error! The result of a clingo call is not SAT nor UNSAT!\")\n\t        return models\n\t    # it will generate at least num of samples that satisfy query\n\t    def sample_query2(self, query, num=50):\n\t        count = 0\n\t        models = []\n\t        candidate_sm = []\n\t        # we first find out all stable models that satisfy query\n\t        program = self.pi_prime + query\n", "        clingo_control = clingo.Control(['0', '--warn=none'])\n\t        clingo_control.add('base', [], program)\n\t        clingo_control.ground([('base', [])])\n\t        clingo_control.solve([], lambda model: candidate_sm.append(model.symbols(shown=True)))\n\t        candidate_sm = [[str(atom) for atom in model] for model in candidate_sm]\n\t        probs = [self.prob_of_interpretation(model) for model in candidate_sm]\n\t        while count < num:\n\t            asp_with_facts = self.pi_prime\n\t            asp_with_facts += query\n\t            clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n", "            models_tmp = []\n\t            for ruleIdx,list_of_atoms in enumerate(self.pc):\n\t                tmp = np.random.choice(list_of_atoms, 1, p=self.parameters[ruleIdx])\n\t                asp_with_facts += tmp[0]+\".\\n\"\n\t            clingo_control.add(\"base\", [], asp_with_facts)\n\t            clingo_control.ground([(\"base\", [])])\n\t            result = clingo_control.solve([], lambda model: models_tmp.append(model.symbols(shown=True)))\n\t            if str(result) == \"SAT\":\n\t                models_tmp = [[str(atom) for atom in model] for model in models_tmp]\n\t                count += len(models_tmp)\n", "                models = models + models_tmp\n\t            elif str(result) == \"UNSAT\":\n\t                pass\n\t            else:\n\t                print(\"Error! The result of a clingo call is not SAT nor UNSAT!\")\n\t        return models\n\t    # we compute the gradients (numpy array) w.r.t. all probs in the ruleIdx-th rule\n\t    # given models that satisfy query\n\t    def gradient_given_models(self, ruleIdx, models):\n\t        arity = len(self.parameters[ruleIdx])\n", "        # we will compute N(O) and N(O,c=v_i)/p_i for each i\n\t        n_O = 0\n\t        n_i = [0]*arity\n\t        # 1st, we compute N(O)\n\t        n_O = len(models)\n\t        # 2nd, we compute N(O,c=v_i)/p_i for each i\n\t        for model in models:\n\t            for atomIdx, atom in enumerate(self.pc[ruleIdx]):\n\t                if atom in model:\n\t                    n_i[atomIdx] += 1\n", "        for atomIdx, p_i in enumerate(self.parameters[ruleIdx]):\n\t            n_i[atomIdx] = n_i[atomIdx]/p_i\n\t        # 3rd, we compute the derivative of L'(O) w.r.t. p_i for each i\n\t        tmp = np.array(n_i) * (-1)\n\t        summation = np.sum(tmp)\n\t        gradients = np.array([summation]*arity)\n\t        for atomIdx, p_i in enumerate(self.parameters[ruleIdx]):\n\t            gradients[atomIdx] = gradients[atomIdx] + 2* n_i[atomIdx]\n\t        gradients = gradients / n_O\n\t        return gradients\n", "    # gradients are stored in numpy array instead of list\n\t    # query is a string\n\t    def gradients_one_query_by_sampling(self, query, num=50):\n\t        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n\t        # 1st, we generate at least num of stable models that satisfy query\n\t        models = self.sample_query(query=query, num=num)\n\t        # 2nd, we compute the gradients w.r.t. the probs in each rule\n\t        for ruleIdx,list_of_bools in enumerate(self.learnable):\n\t            gradients[ruleIdx] = self.gradient_given_models(ruleIdx, models)\n\t            for atomIdx, b in enumerate(list_of_bools):\n", "                if b == False:\n\t                    gradients[ruleIdx][atomIdx] = 0\n\t        return gradients\n\t    # we compute the gradients (numpy array) w.r.t. all probs given list_of_query\n\t    def gradients_multi_query_by_sampling(self, list_of_query, num=50):\n\t        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n\t        # we itereate over all query\n\t        for query in list_of_query:\n\t            # 1st, we generate at least num of stable models that satisfy query\n\t            models = self.sample_query(query=query, num=num) \n", "            # 2nd, we accumulate the gradients w.r.t. the probs in each rule\n\t            for ruleIdx,list_of_bools in enumerate(self.learnable):\n\t                gradients[ruleIdx] += self.gradient_given_models(ruleIdx, models)\n\t                for atomIdx, b in enumerate(list_of_bools):\n\t                    if b == False:\n\t                        gradients[ruleIdx][atomIdx] = 0\n\t        return gradients\n\t    # we compute the gradients (numpy array) w.r.t. all probs given list_of_query\n\t    # while we generate at least one sample without considering probability distribution\n\t    def gradients_multi_query_by_one_sample(self, list_of_query):\n", "        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n\t        # we itereate over all query\n\t        for query in list_of_query:\n\t            # 1st, we generate one stable model that satisfy query\n\t            models = self.find_one_SM_under_query(query=query)\n\t            # 2nd, we accumulate the gradients w.r.t. the probs in each rule\n\t            for ruleIdx,list_of_bools in enumerate(self.learnable):\n\t                gradients[ruleIdx] += self.gradient_given_models(ruleIdx, models)\n\t                for atomIdx, b in enumerate(list_of_bools):\n\t                    if b == False:\n", "                        gradients[ruleIdx][atomIdx] = 0\n\t        return gradients\n\t    # list_of_query is either a list of strings or a file containing queries separated by \"#evidence\"\n\t    def learn_by_sampling(self, list_of_query, num_of_samples=50, lr=0.01, thres=0.0001, max_iter=None, num_pretrain=1):\n\t        # Step 0: Evidence Preprocessing: if list_of_query is an evidence file, \n\t        # we need to first turn it into a list of strings\n\t        if type(list_of_query) is str and os.path.isfile(list_of_query):\n\t            with open(list_of_query, 'r') as f:\n\t                list_of_query = f.read().strip().strip(\"#evidence\").split(\"#evidence\")\n\t        print(\"Start learning by sampling with {} queries...\\n\\nInitial parameters: {}\".format(len(list_of_queries), self.parameters))\n", "        time_init = time.time()\n\t        # Step 1: Parameter Pre-training: we pretrain the parameters \n\t        # so that it's easier to generate sample stable models\n\t        assert type(num_pretrain) is int\n\t        if num_pretrain >= 1:\n\t            print(\"\\n#######################################################\\nParameter Pre-training for {} iterations...\\n#######################################################\".format(num_pretrain))\n\t            for iteration in range(num_pretrain):\n\t                print(\"\\n#### Iteration {} for Pre-Training ####\\nGenerating 1 stable model for each query...\\n\".format(iteration+1))\n\t                dif = lr * self.gradients_multi_query_by_one_sample(list_of_query)\n\t                self.parameters = (np.array(self.parameters) + dif).tolist()\n", "                self.normalize_probs()\n\t                print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n\t                print(\"Current parameters: {}\".format(self.parameters))\n\t        # Step 2: Parameter Training: we train the parameters using \"list_of_query until\"\n\t        # (i) the max change on probabilities is lower than \"thres\", or\n\t        # (ii) the number of iterations is more than \"max_iter\"\n\t        print(\"\\n#######################################################\\nParameter Training for {} iterations or until converge...\\n#######################################################\".format(max_iter))\n\t        check_continue = True\n\t        iteration = 1\n\t        while check_continue:\n", "            print(\"\\n#### Iteration {} ####\".format(iteration))\n\t            old_parameters = np.array(self.parameters)            \n\t            check_continue = False\n\t            print(\"Generating {} stable model(s) for each query...\\n\".format(num_of_samples))\n\t            dif = lr * self.gradients_multi_query_by_sampling(list_of_query, num=num_of_samples)\n\t            self.parameters = (np.array(self.parameters) + dif).tolist()\n\t            self.normalize_probs()\n\t            print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n\t            print(\"Current parameters: {}\".format(self.parameters))\n\t            # we termintate if the change of the parameters is lower than thres\n", "            dif = np.array(self.parameters) - old_parameters\n\t            dif = abs(max(dif.min(), dif.max(), key=abs))\n\t            print(\"Max change on probabilities: {}\".format(dif))\n\t            iteration += 1\n\t            if dif > thres:\n\t                check_continue = True\n\t            if max_iter is not None:\n\t                if iteration > max_iter:\n\t                    check_continue = False\n\t        print(\"\\nFinal parameters: {}\".format(self.parameters))\n"]}
{"filename": "src/experiments/__init__.py", "chunked_list": []}
{"filename": "src/experiments/mnist_top_k/train.py", "chunked_list": ["print(\"start importing...\")\n\timport time\n\timport sys\n\timport argparse\n\timport datetime\n\tsys.path.append('../../')\n\tsys.path.append('../../SLASH/')\n\tsys.path.append('../../EinsumNetworks/src/')\n\t#torch, numpy, ...\n\timport torch\n", "from torch.utils.tensorboard import SummaryWriter\n\tfrom torchvision.transforms import transforms\n\timport torchvision\n\timport numpy as np\n\t#own modules\n\tfrom dataGen import MNIST_Addition\n\tfrom einsum_wrapper import EiNet\n\tfrom network_nn import Net_nn\n\t#import slash\n\tfrom slash import SLASH\n", "import utils\n\tfrom utils import set_manual_seed\n\tfrom pathlib import Path\n\tfrom rtpt import RTPT\n\tprint(\"...done\")\n\tdef get_args():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--seed\", type=int, default=10, help=\"Random generator seed for all frameworks\"\n\t    )\n", "    parser.add_argument(\n\t        \"--epochs\", type=int, default=10, help=\"Number of epochs to train with\"\n\t    )\n\t    parser.add_argument(\n\t        \"--lr\", type=float, default=0.01, help=\"Learning rate of model\"\n\t    )\n\t    parser.add_argument(\n\t        \"--network-type\",\n\t        choices=[\"nn\",\"pc\"],\n\t        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n", "    )\n\t    parser.add_argument(\n\t        \"--pc-structure\",\n\t        choices=[\"poon-domingos\",\"binary-trees\"],\n\t        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n\t    )\n\t    parser.add_argument(\n\t        \"--method\",\n\t        choices=[\"exact\",\"top_k\",\"same\"],\n\t        help=\"How many images should be used in the addition\",\n", "    )\n\t    parser.add_argument(\n\t        \"--k\", type=int, default=0, help=\"Maximum number of stable model to be used\"\n\t    )\n\t    parser.add_argument(\n\t        \"--images-per-addition\",\n\t        choices=[\"2\",\"3\",\"4\",\"6\"],\n\t        help=\"How many images should be used in the addition\",\n\t    )\n\t    parser.add_argument(\n", "        \"--batch-size\", type=int, default=100, help=\"Batch size to train with\"\n\t    )\n\t    parser.add_argument(\n\t        \"--num-workers\", type=int, default=6, help=\"Number of threads for data loader\"\n\t    )\n\t    parser.add_argument(\n\t        \"--p-num\", type=int, default=8, help=\"Number of processes to devide the batch for parallel processing\"\n\t    )\n\t    parser.add_argument(\"--credentials\", type=str, help=\"Credentials for rtpt\")\n\t    args = parser.parse_args()\n", "    if args.network_type == 'pc':\n\t        args.use_pc = True\n\t    else:\n\t        args.use_pc = False\n\t    return args\n\tdef slash_mnist_addition():\n\t    args = get_args()\n\t    print(args)\n\t    # Set the seeds for PRNG\n\t    set_manual_seed(args.seed)\n", "    # Create RTPT object\n\t    rtpt = RTPT(name_initials=args.credentials, experiment_name='SLASH MNIST pick-k', max_iterations=args.epochs)\n\t    # Start the RTPT tracking\n\t    rtpt.start()\n\t    i_num = int(args.images_per_addition)\n\t    if i_num == 2:\n\t        program = '''\n\t        img(i1). img(i2).\n\t        addition(A,B,N):- digit(0,+A,-N1), digit(0,+B,-N2), N=N1+N2, A!=B.\n\t        npp(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n", "        '''\n\t    elif i_num == 3:\n\t        program = '''\n\t        img(i1). img(i2). img(i3).\n\t        addition(A,B,C,N):- digit(0,+A,-N1), digit(0,+B,-N2), digit(0,+C,-N3), N=N1+N2+N3, A!=B, A!=C, B!=C.\n\t        npp(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n\t        '''\n\t    elif i_num == 4:\n\t        program = '''\n\t        img(i1). img(i2). img(i3). img(i4).\n", "        addition(A,B,C,D,N):- digit(0,+A,-N1), digit(0,+B,-N2), digit(0,+C,-N3), digit(0,+D,-N4), N=N1+N2+N3+N4, A != B, A!=C, A!=D, B!= C, B!= D, C!=D.\n\t        npp(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n\t        '''\n\t    elif i_num == 6:\n\t        program = '''\n\t        img(i1). img(i2). img(i3). img(i4). img(i5). img(i6).\n\t        addition(i1,i2,i3,i4,i5,i6,N):- digit(0,+A,-N1), digit(0,+B,-N2), digit(0,+C,-N3), digit(0,+D,-N4), digit(0,+E,-N5), digit(0,+F,-N6), N=N1+N2+N3+N4+N5+N6, A != B, A!=C, A!=D, A!=E, A!=F, B!= C, B!= D, B!=E, B!=F, C!=D, C!=E, C!=F, D!=E, D!=F, E!=F.\n\t        npp(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n\t        '''\n\t    exp_name= str(args.method)+\"/\" +args.network_type+\"_i\"+str(i_num)+\"_k\"+ str(args.k)\n", "    saveModelPath = 'data/'+exp_name+'/slash_digit_addition_models_seed'+str(args.seed)+'.pt'\n\t    Path(\"data/\"+exp_name+\"/\").mkdir(parents=True, exist_ok=True)\n\t    #use neural net or probabilisitc circuit\n\t    if args.network_type == 'pc':\n\t        #setup new SLASH program given the network parameters\n\t        if args.pc_structure == 'binary-trees':\n\t            m = EiNet(structure = 'binary-trees',\n\t                      depth = 3,\n\t                      num_repetitions = 20,\n\t                      use_em = False,\n", "                      num_var = 784,\n\t                      class_count = 10,\n\t                      learn_prior = True)\n\t        elif args.pc_structure == 'poon-domingos': \n\t            m = EiNet(structure = 'poon-domingos',\n\t                      pd_num_pieces = [4,7,28],\n\t                      use_em = False,\n\t                      num_var = 784,\n\t                      class_count = 10,\n\t                      pd_width = 28,\n", "                      pd_height = 28,\n\t                      learn_prior = True)\n\t        else:\n\t            print(\"pc structure learner unknown\")\n\t    else:\n\t        m = Net_nn()    \n\t    #trainable paramas\n\t    num_trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n\t    num_params = sum(p.numel() for p in m.parameters())\n\t    print(\"training with {} trainable params and {} params in total\".format(num_trainable_params,num_params))\n", "    #create the SLASH Program\n\t    nnMapping = {'digit': m}\n\t    optimizers = {'digit': torch.optim.Adam(m.parameters(), lr=args.lr, eps=1e-7)}\n\t    SLASHobj = SLASH(program, nnMapping, optimizers)\n\t    SLASHobj.grad_comp_device ='cpu' #set gradient computation to cpu\n\t    #metric lists\n\t    train_accuracy_list = []\n\t    test_accuracy_list = []\n\t    confusion_matrix_list = []\n\t    loss_list = []\n", "    startTime = time.time()\n\t    forward_time_list = []\n\t    asp_time_list = []\n\t    backward_time_list = []\n\t    sm_per_batch_list = [] \n\t    train_test_times = []\n\t    #load data\n\t    #if we are using spns we need to flatten the data(Tensor has form [bs, 784])\n\t    if args.use_pc: \n\t        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081, )), transforms.Lambda(lambda x: torch.flatten(x))])\n", "    #if not we can keep the dimensions(Tensor has form [bs,28,28])\n\t    else: \n\t        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081, ))]) \n\t    data_path = 'data/labels/train_data_s'+str(i_num)+'.txt'\n\t    mnist_addition_dataset = MNIST_Addition(torchvision.datasets.MNIST(root='./data/', train=True, download=True, transform=transform), data_path, i_num, args.use_pc)\n\t    train_dataset_loader = torch.utils.data.DataLoader(mnist_addition_dataset, shuffle=True,batch_size=args.batch_size,pin_memory=True, num_workers=8)\n\t    test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data/', train=False, transform=transform), batch_size=100, shuffle=True)\n\t    train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data/', train=True, transform=transform), batch_size=100, shuffle=True)\n\t    # Evaluate the performanve directly after initialisation\n\t    time_test = time.time()\n", "    test_acc, _, confusion_matrix = SLASHobj.testNetwork('digit', test_loader, ret_confusion=True)\n\t    train_acc, _ = SLASHobj.testNetwork('digit', train_loader)\n\t    confusion_matrix_list.append(confusion_matrix)\n\t    train_accuracy_list.append([train_acc,0])\n\t    test_accuracy_list.append([test_acc, 0])\n\t    timestamp_test = utils.time_delta_now(time_test, simple_format=True)\n\t    timestamp_total = utils.time_delta_now(startTime, simple_format=True)\n\t    train_test_times.append([0.0, timestamp_test, timestamp_total])\n\t    # Save and print statistics\n\t    print('Train Acc: {:0.2f}%, Test Acc: {:0.2f}%'.format(train_acc, test_acc))\n", "    print('--- train time:  ---', 0)\n\t    print('--- test time:  ---' , timestamp_test)\n\t    print('--- total time from beginning:  ---', timestamp_total)\n\t    # Export results and networks\n\t    print('Storing the trained model into {}'.format(saveModelPath))\n\t    torch.save({\"addition_net\": m.state_dict(),\n\t                \"test_accuracy_list\": test_accuracy_list,\n\t                \"train_accuracy_list\":train_accuracy_list,\n\t                \"confusion_matrix_list\":confusion_matrix_list,\n\t                \"num_params\": num_trainable_params,\n", "                \"args\":args,\n\t                \"exp_name\":exp_name,\n\t                \"train_test_times\": train_test_times,\n\t                \"program\":program}, saveModelPath)\n\t    start_e= 0\n\t    # Train and evaluate the performance\n\t    for e in range(start_e, args.epochs):\n\t        print('Epoch {}...'.format(e+1))\n\t        #one epoch of training\n\t        time_train= time.time()        \n", "        loss, forward_time, asp_time, backward_time, sm_per_batch, model_computation_time, gradient_computation_time = SLASHobj.learn(dataset_loader = train_dataset_loader,\n\t                       epoch=e, method=args.method, p_num=args.p_num, k_num = args.k , same_threshold=0.99)\n\t        timestamp_train = utils.time_delta_now(time_train, simple_format=True)\n\t        #store detailed timesteps per batch\n\t        forward_time_list.append(forward_time)\n\t        asp_time_list.append(asp_time)\n\t        backward_time_list.append(backward_time)\n\t        sm_per_batch_list.append(sm_per_batch)\n\t        time_test = time.time()\n\t        test_acc, _, confusion_matrix = SLASHobj.testNetwork('digit', test_loader, ret_confusion=True)\n", "        confusion_matrix_list.append(confusion_matrix)\n\t        train_acc, _ = SLASHobj.testNetwork('digit', train_loader)        \n\t        train_accuracy_list.append([train_acc,e])\n\t        test_accuracy_list.append([test_acc, e])\n\t        timestamp_test = utils.time_delta_now(time_test, simple_format=True)\n\t        timestamp_total = utils.time_delta_now(startTime, simple_format=True)\n\t        loss_list.append(loss)\n\t        train_test_times.append([timestamp_train, timestamp_test, timestamp_total])\n\t        # Save and print statistics\n\t        print('Train Acc: {:0.2f}%, Test Acc: {:0.2f}%'.format(train_acc, test_acc))\n", "        print('--- train time:  ---', timestamp_train)\n\t        print('--- test time:  ---' , timestamp_test)\n\t        print('--- total time from beginning:  ---', timestamp_total)\n\t        # Export results and networks\n\t        print('Storing the trained model into {}'.format(saveModelPath))\n\t        torch.save({\"addition_net\": m.state_dict(),\n\t                    \"resume\": {\n\t                        \"optimizer_digit\":optimizers['digit'].state_dict(),\n\t                        \"epoch\":e\n\t                            },\n", "                    \"test_accuracy_list\": test_accuracy_list,\n\t                    \"train_accuracy_list\":train_accuracy_list,\n\t                    \"confusion_matrix_list\":confusion_matrix_list,\n\t                    \"num_params\": num_trainable_params,\n\t                    \"args\":args,\n\t                    \"exp_name\":exp_name,\n\t                    \"train_test_times\": train_test_times,\n\t                    \"forward_time_list\":forward_time_list,\n\t                    \"asp_time_list\":asp_time_list,\n\t                    \"backward_time_list\":backward_time_list,\n", "                    \"sm_per_batch_list\":sm_per_batch_list,\n\t                    \"loss\": loss_list,\n\t                    \"program\":program}, saveModelPath)\n\t        # Update the RTPT\n\t        rtpt.step(subtitle=f\"accuracy={test_acc:2.2f}\")\n\tif __name__ == \"__main__\":\n\t    slash_mnist_addition()"]}
{"filename": "src/experiments/mnist_top_k/network_nn.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tclass Net_nn(nn.Module):\n\t    def __init__(self):\n\t        super(Net_nn, self).__init__()\n\t        self.encoder = nn.Sequential(\n\t            nn.Conv2d(1, 6, 5),  # 6 is the output chanel size; 5 is the kernal size; 1 (chanel) 28 28 -> 6 24 24\n\t            nn.MaxPool2d(2, 2),  # kernal size 2; stride size 2; 6 24 24 -> 6 12 12\n\t            nn.ReLU(True),       # inplace=True means that it will modify the input directly thus save memory\n\t            nn.Conv2d(6, 16, 5), # 6 12 12 -> 16 8 8\n", "            nn.MaxPool2d(2, 2),  # 16 8 8 -> 16 4 4\n\t            nn.ReLU(True) \n\t        )\n\t        self.classifier =  nn.Sequential(\n\t            nn.Linear(16 * 4 * 4, 120),\n\t            nn.ReLU(),\n\t            nn.Linear(120, 84),\n\t            nn.ReLU(),\n\t            nn.Linear(84, 10),\n\t            nn.Softmax(1)\n", "        )\n\t    def forward(self, x, marg_idx=None, type=1):\n\t        assert type == 1, \"only posterior computations are available for this network\"\n\t        # If the list of the pixel numbers to be marginalised is given,\n\t        # then genarate a marginalisation mask from it and apply to the\n\t        # tensor 'x'\n\t        if marg_idx:\n\t            batch_size = x.shape[0]\n\t            with torch.no_grad():\n\t                marg_mask = torch.ones_like(x, device=x.device).reshape(batch_size, 1, -1)\n", "                marg_mask[:, :, marg_idx] = 0\n\t                marg_mask = marg_mask.reshape_as(x)\n\t                marg_mask.requires_grad_(False)\n\t            x = torch.einsum('ijkl,ijkl->ijkl', x, marg_mask)\n\t        x = self.encoder(x)\n\t        x = x.view(-1, 16 * 4 * 4)\n\t        x = self.classifier(x)\n\t        return x\n"]}
{"filename": "src/experiments/mnist_top_k/__init__.py", "chunked_list": []}
{"filename": "src/experiments/mnist_top_k/dataGen.py", "chunked_list": ["import torch\n\tfrom torch.utils.data import Dataset\n\timport numpy as np\n\tclass MNIST_Addition(Dataset):\n\t    def __init__(self, dataset, examples, num_i, flat_for_pc):\n\t        self.data = list()\n\t        self.dataset = dataset\n\t        self.num_i = num_i\n\t        self.flat_for_pc = flat_for_pc\n\t        with open(examples) as f:\n", "            for line in f:\n\t                line = line.strip().split(' ')\n\t                self.data.append(tuple([int(i) for i in line]))\n\t    def __getitem__(self, index):\n\t        if self.num_i == 2:\n\t            i1, i2, l = self.data[index]\n\t            l = ':- not addition(i1, i2, {}).'.format(l)\n\t            if self.flat_for_pc:\n\t                return {'i1': self.dataset[i1][0].flatten(), 'i2': self.dataset[i2][0].flatten()}, l\n\t            else:\n", "                return {'i1': self.dataset[i1][0], 'i2': self.dataset[i2][0]}, l\n\t        elif self.num_i == 3:\n\t            i1, i2, i3, l = self.data[index]\n\t            l = ':- not addition(i1, i2, i3, {}).'.format(l)\n\t            if self.flat_for_pc:\n\t                return {'i1': self.dataset[i1][0].flatten(), 'i2': self.dataset[i2][0].flatten(), 'i3': self.dataset[i3][0].flatten()}, l\n\t            else:\n\t                return {'i1': self.dataset[i1][0], 'i2': self.dataset[i2][0], 'i3': self.dataset[i3][0]}, l\n\t        elif self.num_i == 4:\n\t            i1, i2, i3, i4, l = self.data[index]\n", "            l = ':- not addition(i1, i2, i3, i4, {}).'.format(l)\n\t            if self.flat_for_pc:\n\t                return {'i1': self.dataset[i1][0].flatten(), 'i2': self.dataset[i2][0].flatten(), 'i3': self.dataset[i3][0].flatten(), 'i4': self.dataset[i4][0].flatten()}, l\n\t            else:\n\t                return {'i1': self.dataset[i1][0], 'i2': self.dataset[i2][0], 'i3': self.dataset[i3][0], 'i4': self.dataset[i4][0]}, l\n\t        elif self.num_i == 6:\n\t            i1, i2, i3, i4, i5,i6, l = self.data[index]\n\t            l = ':- not addition(i1, i2, i3, i4, i5, i6, {}).'.format(l)\n\t            if self.flat_for_pc:\n\t                return {'i1': self.dataset[i1][0].flatten(), 'i2': self.dataset[i2][0].flatten(), 'i3': self.dataset[i3][0].flatten(), 'i4': self.dataset[i4][0].flatten(), 'i5': self.dataset[i5][0].flatten(), 'i6': self.dataset[i6][0].flatten()}, l\n", "            else:\n\t                return {'i1': self.dataset[i1][0], 'i2': self.dataset[i2][0], 'i3': self.dataset[i3][0], 'i4': self.dataset[i4][0], 'i5': self.dataset[i5][0], 'i6': self.dataset[i6][0]}, l\n\t    def __len__(self):\n\t        return len(self.data)"]}
{"filename": "src/experiments/baseline_slot_attention/set_utils.py", "chunked_list": ["import scipy.optimize\n\timport torch\n\timport torch.nn.functional as F\n\timport numpy as np\n\timport os\n\tdef save_args(args, writer):\n\t\t# store args as txt file\n\t\twith open(os.path.join(writer.log_dir, 'args.txt'), 'w') as f:\n\t\t\tfor arg in vars(args):\n\t\t\t\tf.write(f\"\\n{arg}: {getattr(args, arg)}\")\n", "def hungarian_matching(attrs, preds_attrs, verbose=0):\n\t\t\"\"\"\n\t\tReceives unordered predicted set and orders this to match the nearest GT set.\n\t\t:param attrs:\n\t\t:param preds_attrs:\n\t\t:param verbose:\n\t\t:return:\n\t\t\"\"\"\n\t\tassert attrs.shape[1] == preds_attrs.shape[1]\n\t\tassert attrs.shape == preds_attrs.shape\n", "\tfrom scipy.optimize import linear_sum_assignment\n\t\tmatched_preds_attrs = preds_attrs.clone()\n\t\tfor sample_id in range(attrs.shape[0]):\n\t\t\t# using euclidean distance\n\t\t\tcost_matrix = torch.cdist(attrs[sample_id], preds_attrs[sample_id]).detach().cpu()\n\t\t\tidx_mapping = linear_sum_assignment(cost_matrix)\n\t\t\t# convert to tuples of [(row_id, col_id)] of the cost matrix\n\t\t\tidx_mapping = [(idx_mapping[0][i], idx_mapping[1][i]) for i in range(len(idx_mapping[0]))]\n\t\t\tfor i, (row_id, col_id) in enumerate(idx_mapping):\n\t\t\t\tmatched_preds_attrs[sample_id, row_id, :] = preds_attrs[sample_id, col_id, :]\n", "\t\tif verbose:\n\t\t\t\tprint('GT: {}'.format(attrs[sample_id]))\n\t\t\t\tprint('Pred: {}'.format(preds_attrs[sample_id]))\n\t\t\t\tprint('Cost Matrix: {}'.format(cost_matrix))\n\t\t\t\tprint('idx mapping: {}'.format(idx_mapping))\n\t\t\t\tprint('Matched Pred: {}'.format(matched_preds_attrs[sample_id]))\n\t\t\t\tprint('\\n')\n\t\t\t\t# exit()\n\t\treturn matched_preds_attrs\n\tdef average_precision_shapeworld(pred, attributes, distance_threshold, dataset):\n", "\t\"\"\"Computes the average precision for CLEVR.\n\t\tThis function computes the average precision of the predictions specifically\n\t\tfor the CLEVR dataset. First, we sort the predictions of the model by\n\t\tconfidence (highest confidence first). Then, for each prediction we check\n\t\twhether there was a corresponding object in the input image. A prediction is\n\t\tconsidered a true positive if the discrete features are predicted correctly\n\t\tand the predicted position is within a certain distance from the ground truth\n\t\tobject.\n\t\tArgs:\n\t\t\t  pred: Tensor of shape [batch_size, num_elements, dimension] containing\n", "\t\t\tpredictions. The last dimension is expected to be the confidence of the\n\t\t\t\tprediction.\n\t\t\t  attributes: Tensor of shape [batch_size, num_elements, dimension] containing\n\t\t\t\tpredictions.\n\t\t\t  distance_threshold: Threshold to accept match. -1 indicates no threshold.\n\t\tReturns:\n\t\t\t  Average precision of the predictions.\n\t\t\"\"\"\n\t\t[batch_size, _, element_size] = attributes.shape\n\t\t[_, predicted_elements, _] = pred.shape\n", "\tdef unsorted_id_to_image(detection_id, predicted_elements):\n\t\t\t\"\"\"Find the index of the image from the unsorted detection index.\"\"\"\n\t\t\treturn int(detection_id // predicted_elements)\n\t\tflat_size = batch_size * predicted_elements\n\t\tflat_pred = np.reshape(pred, [flat_size, element_size])\n\t\t# sort_idx = np.argsort(flat_pred[:, -1], axis=0)[::-1]  # Reverse order.\n\t\tsort_idx = np.argsort(flat_pred[:, 0], axis=0)[::-1]  # Reverse order.\n\t\tsorted_predictions = np.take_along_axis(\n\t\t\tflat_pred, np.expand_dims(sort_idx, axis=1), axis=0)\n\t\tidx_sorted_to_unsorted = np.take_along_axis(\n", "\t\tnp.arange(flat_size), sort_idx, axis=0)\n\t\tdef process_targets_shapeworld4(target):\n\t\t\t\"\"\"Unpacks the target into the CLEVR properties.\"\"\"\n\t\t\t#col_enc + shape_enc + shade_enc + size_enc\n\t\t\treal_obj = target[0]\n\t\t\tcolor = np.argmax(target[1:9])\n\t\t\tshape = np.argmax(target[9:12])\n\t\t\tshade = np.argmax(target[12:14])\n\t\t\tsize = np.argmax(target[14:16])\n\t\t\treturn np.array([0,0,0]), size, shade, shape, color, real_obj\n", "\tdef process_targets_clevr(target):\n\t\t\t\"\"\"Unpacks the target into the CLEVR properties.\"\"\"\n\t\t\t#col_enc + shape_enc + shade_enc + size_enc\n\t\t\treal_obj = target[0]\n\t\t\tsize = np.argmax(target[1:3])\n\t\t\tmaterial = np.argmax(target[3:5])\n\t\t\tshape = np.argmax(target[5:8])\n\t\t\tcolor = np.argmax(target[8:16]) \n\t\t\treturn np.array([0,0,0]), size, material, shape, color, real_obj\n\t\tdef process_targets(target):\n", "\t\tif dataset == \"shapeworld4\":\n\t\t\t\treturn process_targets_shapeworld4(target)\n\t\t\telif dataset == \"clevr\":\n\t\t\t\treturn process_targets_clevr(target)\n\t\ttrue_positives = np.zeros(sorted_predictions.shape[0])\n\t\tfalse_positives = np.zeros(sorted_predictions.shape[0])\n\t\tdetection_set = set()\n\t\tfor detection_id in range(sorted_predictions.shape[0]):\n\t\t\t# Extract the current prediction.\n\t\t\tcurrent_pred = sorted_predictions[detection_id, :]\n", "\t\t# Find which image the prediction belongs to. Get the unsorted index from\n\t\t\t# the sorted one and then apply to unsorted_id_to_image function that undoes\n\t\t\t# the reshape.\n\t\t\toriginal_image_idx = unsorted_id_to_image(\n\t\t\t\tidx_sorted_to_unsorted[detection_id], predicted_elements)\n\t\t\t# Get the ground truth image.\n\t\t\tgt_image = attributes[original_image_idx, :, :]\n\t\t\t# Initialize the maximum distance and the id of the groud-truth object that\n\t\t\t# was found.\n\t\t\tbest_distance = 10000\n", "\t\tbest_id = None\n\t\t\t# Unpack the prediction by taking the argmax on the discrete\n\t\t\t# attributes.\n\t\t\t(pred_coords, pred_object_size, pred_material, pred_shape, pred_color,\n\t\t\t _) = process_targets(current_pred)\n\t\t\t# Loop through all objects in the ground-truth image to check for hits.\n\t\t\tfor target_object_id in range(gt_image.shape[0]):\n\t\t\t\ttarget_object = gt_image[target_object_id, :]\n\t\t\t\t# Unpack the targets taking the argmax on the discrete attributes.\n\t\t\t\t(target_coords, target_object_size, target_material, target_shape,\n", "\t\t\t target_color, target_real_obj) = process_targets(target_object)\n\t\t\t\t# Only consider real objects as matches.\n\t\t\t\tif target_real_obj:\n\t\t\t\t\t# For the match to be valid all attributes need to be correctly\n\t\t\t\t\t# predicted.\n\t\t\t\t\tpred_attr = [\n\t\t\t\t\t\tpred_object_size,\n\t\t\t\t\t\tpred_material,\n\t\t\t\t\t\tpred_shape,\n\t\t\t\t\t\tpred_color]\n", "\t\t\t\ttarget_attr = [\n\t\t\t\t\t\ttarget_object_size,\n\t\t\t\t\t\ttarget_material,\n\t\t\t\t\t\ttarget_shape,\n\t\t\t\t\t\ttarget_color]\n\t\t\t\t\tmatch = pred_attr == target_attr\n\t\t\t\t\tif match:\n\t\t\t\t\t\t# If a match was found, we check if the distance is below the\n\t\t\t\t\t\t# specified threshold. Recall that we have rescaled the coordinates\n\t\t\t\t\t\t# in the dataset from [-3, 3] to [0, 1], both for `target_coords` and\n", "\t\t\t\t\t# `pred_coords`. To compare in the original scale, we thus need to\n\t\t\t\t\t\t# multiply the distance values by 6 before applying the\n\t\t\t\t\t\t# norm.\n\t\t\t\t\t\tdistance = np.linalg.norm(\n\t\t\t\t\t\t\t(target_coords - pred_coords) * 6.)\n\t\t\t\t\t\t# If this is the best match we've found so far we remember\n\t\t\t\t\t\t# it.\n\t\t\t\t\t\tif distance < best_distance:\n\t\t\t\t\t\t\tbest_distance = distance\n\t\t\t\t\t\t\tbest_id = target_object_id\n", "\t\tif best_distance < distance_threshold or distance_threshold == -1:\n\t\t\t\t# We have detected an object correctly within the distance confidence.\n\t\t\t\t# If this object was not detected before it's a true positive.\n\t\t\t\tif best_id is not None:\n\t\t\t\t\tif (original_image_idx, best_id) not in detection_set:\n\t\t\t\t\t\ttrue_positives[detection_id] = 1\n\t\t\t\t\t\tdetection_set.add((original_image_idx, best_id))\n\t\t\t\t\telse:\n\t\t\t\t\t\tfalse_positives[detection_id] = 1\n\t\t\t\telse:\n", "\t\t\t\tfalse_positives[detection_id] = 1\n\t\t\telse:\n\t\t\t\tfalse_positives[detection_id] = 1\n\t\taccumulated_fp = np.cumsum(false_positives)\n\t\taccumulated_tp = np.cumsum(true_positives)\n\t\trecall_array = accumulated_tp / np.sum(attributes[:, :, 0])\n\t\tprecision_array = np.divide(\n\t\t\taccumulated_tp,\n\t\t\t(accumulated_fp + accumulated_tp))\n\t\treturn compute_average_precision(\n", "\t\tnp.array(precision_array, dtype=np.float32),\n\t\t\tnp.array(recall_array, dtype=np.float32))\n\tdef compute_average_precision(precision, recall):\n\t\t\"\"\"Computation of the average precision from precision and recall arrays.\"\"\"\n\t\trecall = recall.tolist()\n\t\tprecision = precision.tolist()\n\t\trecall = [0] + recall + [1]\n\t\tprecision = [0] + precision + [0]\n\t\tfor i in range(len(precision) - 1, -0, -1):\n\t\t\tprecision[i - 1] = max(precision[i - 1], precision[i])\n", "\tindices_recall = [\n\t\t\ti for i in range(len(recall) - 1) if recall[1:][i] != recall[:-1][i]\n\t\t]\n\t\taverage_precision = 0.\n\t\tfor i in indices_recall:\n\t\t\taverage_precision += precision[i + 1] * (recall[i + 1] - recall[i])\n\t\treturn average_precision\n\tdef hungarian_loss(predictions, targets):\n\t\t# permute dimensions for pairwise distance computation between all slots\n\t\tpredictions = predictions.permute(0, 2, 1)\n", "\ttargets = targets.permute(0, 2, 1)\n\t\t# predictions and targets shape :: (n, c, s)\n\t\tpredictions, targets = outer(predictions, targets)\n\t\t# squared_error shape :: (n, s, s)\n\t\tsquared_error = F.smooth_l1_loss(predictions, targets.expand_as(predictions), reduction=\"none\").mean(1)\n\t\tsquared_error_np = squared_error.detach().cpu().numpy()\n\t\tindices = map(hungarian_loss_per_sample, squared_error_np)\n\t\tlosses = [\n\t\t\tsample[row_idx, col_idx].mean()\n\t\t\tfor sample, (row_idx, col_idx) in zip(squared_error, indices)\n", "\t]\n\t\ttotal_loss = torch.mean(torch.stack(list(losses)))\n\t\treturn total_loss\n\tdef hungarian_loss_per_sample(sample_np):\n\t\treturn scipy.optimize.linear_sum_assignment(sample_np)\n\tdef scatter_masked(tensor, mask, binned=False, threshold=None):\n\t\ts = tensor[0].detach().cpu()\n\t\tmask = mask[0].detach().clamp(min=0, max=1).cpu()\n\t\tif binned:\n\t\t\ts = s * 128\n", "\t\ts = s.view(-1, s.size(-1))\n\t\t\tmask = mask.view(-1)\n\t\tif threshold is not None:\n\t\t\tkeep = mask.view(-1) > threshold\n\t\t\ts = s[:, keep]\n\t\t\tmask = mask[keep]\n\t\treturn s, mask\n\tdef outer(a, b=None):\n\t\t\"\"\" Compute outer product between a and b (or a and a if b is not specified). \"\"\"\n\t\tif b is None:\n", "\t\tb = a\n\t\tsize_a = tuple(a.size()) + (b.size()[-1],)\n\t\tsize_b = tuple(b.size()) + (a.size()[-1],)\n\t\ta = a.unsqueeze(dim=-1).expand(*size_a)\n\t\tb = b.unsqueeze(dim=-2).expand(*size_b)\n\t\treturn a, b"]}
{"filename": "src/experiments/baseline_slot_attention/train.py", "chunked_list": ["import os\n\timport argparse\n\tfrom datetime import datetime\n\timport time\n\timport sys\n\tsys.path.append('../../')\n\timport torch\n\timport torch.nn.functional as F\n\timport matplotlib\n\tmatplotlib.use(\"Agg\")\n", "import torchvision.transforms as transforms\n\timport torchvision.datasets as datasets\n\timport torch.multiprocessing as mp\n\timport scipy.optimize\n\timport numpy as np\n\t#from tqdm import tqdm\n\tfrom rtpt import RTPT\n\timport matplotlib.pyplot as plt\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom dataGen import SHAPEWORLD4,CLEVR, get_loader\n", "import slot_attention_module as model\n\timport set_utils as set_utils\n\timport utils as misc_utils\n\tdef get_args():\n\t    parser = argparse.ArgumentParser()\n\t    # generic params\n\t    parser.add_argument(\n\t        \"--name\",\n\t        default=datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"),\n\t        help=\"Name to store the log file as\",\n", "    )\n\t    parser.add_argument(\"--resume\", help=\"Path to log file to resume from\")\n\t    parser.add_argument(\n\t        \"--seed\", type=int, default=10, help=\"Random generator seed for all frameworks\"\n\t    )\n\t    parser.add_argument(\n\t        \"--epochs\", type=int, default=10, help=\"Number of epochs to train with\"\n\t    )\n\t    parser.add_argument(\n\t        \"--ap-log\", type=int, default=10, help=\"Number of epochs before logging AP\"\n", "    )\n\t    parser.add_argument(\n\t        \"--lr\", type=float, default=1e-2, help=\"Outer learning rate of model\"\n\t    )\n\t    parser.add_argument(\n\t        \"--warmup-epochs\", type=int, default=10, help=\"Number of steps fpr learning rate warm up\"\n\t    )\n\t    parser.add_argument(\n\t        \"--decay-epochs\", type=int, default=10, help=\"Number of steps fpr learning rate decay\"\n\t    )\n", "    parser.add_argument(\n\t        \"--batch-size\", type=int, default=32, help=\"Batch size to train with\"\n\t    )\n\t    parser.add_argument(\n\t        \"--num-workers\", type=int, default=6, help=\"Number of threads for data loader\"\n\t    )\n\t    parser.add_argument(\n\t        \"--dataset\",\n\t        choices=[\"shapeworld4\", \"clevr\"],\n\t        help=\"Use shapeworld4 dataset\",\n", "    )\n\t    parser.add_argument(\n\t        \"--cogent\", action='store_true',\n\t        help=\"Evaluate on the CoGenT test of the dataset\",\n\t    )\n\t    parser.add_argument(\n\t        \"--no-cuda\",\n\t        action=\"store_true\",\n\t        help=\"Run on CPU instead of GPU (not recommended)\",\n\t    )\n", "    parser.add_argument(\n\t        \"--train-only\", action=\"store_true\", help=\"Only run training, no evaluation\"\n\t    )\n\t    parser.add_argument(\n\t        \"--eval-only\", action=\"store_true\", help=\"Only run evaluation, no training\"\n\t    )\n\t    parser.add_argument(\"--multi-gpu\", action=\"store_true\", help=\"Use multiple GPUs\")\n\t    parser.add_argument(\"--credentials\", type=str, help=\"Credentials for rtpt\")\n\t    parser.add_argument(\"--export-dir\", type=str, help=\"Directory to output samples to\")\n\t    parser.add_argument(\"--data-dir\", type=str, help=\"Directory to data\")\n", "    # Slot attention params\n\t    parser.add_argument('--n-slots', default=10, type=int,\n\t                        help='number of slots for slot attention module')\n\t    parser.add_argument('--n-iters-slot-att', default=3, type=int,\n\t                        help='number of iterations in slot attention module')\n\t    parser.add_argument('--n-attr', default=18, type=int,\n\t                        help='number of attributes per object')\n\t    args = parser.parse_args()\n\t    if args.no_cuda:\n\t        args.device = 'cpu'\n", "    else:\n\t        args.device = 'cuda:0'\n\t    misc_utils.set_manual_seed(args.seed)\n\t    args.name += f'-{args.seed}'\n\t    return args\n\tdef run(net, loader, optimizer, criterion, writer, args, test_cond = None, train=False, epoch=0):\n\t    if train:\n\t        net.train()\n\t        prefix = \"train\"\n\t        torch.set_grad_enabled(True)\n", "    else:\n\t        net.eval()\n\t        prefix = \"test\"\n\t        torch.set_grad_enabled(False)\n\t        preds_all = torch.zeros(0, args.n_slots, args.n_attr)\n\t        target_all = torch.zeros(0, args.n_slots, args.n_attr)\n\t    iters_per_epoch = len(loader)\n\t    for i, sample in enumerate(loader, start=epoch * iters_per_epoch):\n\t        start = time.time()\n\t        # input is either a set or an image\n", "        if 'cuda' in args.device:\n\t            imgs, target_set = map(lambda x: x.cuda(), sample)\n\t        else:\n\t            imgs, target_set = sample\n\t        load = time.time()\n\t        #print(\"\\nload\", load-start)\n\t        output = net.forward(imgs)\n\t        loss = set_utils.hungarian_loss(output, target_set)\n\t        forward = time.time()\n\t        #print(\"forward\", forward-load)\n", "        if train:\n\t            # apply lr schedulers\n\t            if epoch < args.warmup_epochs:\n\t                lr = args.lr * ((epoch + 1) / args.warmup_epochs)\n\t            else:\n\t                lr = args.lr\n\t            lr = lr * 0.5 ** ((epoch + 1) / args.decay_epochs)\n\t            optimizer.param_groups[0]['lr'] = lr\n\t            optimizer.zero_grad()\n\t            loss.backward(retain_graph=True)\n", "            optimizer.step()\n\t            backward = time.time()\n\t            #print(\"backward\", backward-forward)\n\t            writer.add_scalar(\"train/loss_baseline\", loss.item(), global_step=i)\n\t            log = time.time()\n\t            #print(\"log\", log-backward)\n\t            #writer.add_scalar(\"lr/\", optimizer.param_groups[0][\"lr\"], global_step=i)\n\t        else:\n\t            if i % iters_per_epoch == 0:\n\t                preds_all = torch.cat((preds_all, output.detach().cpu()), 0)\n", "                target_all = torch.cat((target_all, target_set.detach().cpu()), 0)\n\t                ap = [\n\t                    set_utils.average_precision_shapeworld(preds_all.detach().cpu().numpy(),\n\t                                                      target_all.detach().cpu().numpy(), d, args.dataset)\n\t                    for d in [-1]  # since we are not using 3D coords #[-1., 1., 0.5, 0.25, 0.125]\n\t                ]\n\t                print(f\"\\nCurrent AP: \", ap[0], \" %\\n\")\n\t                if test_cond == \"a\":\n\t                    writer.add_scalar(\"test/ap_cond_a\", ap[0], global_step=i)\n\t                elif test_cond == \"b\":\n", "                    writer.add_scalar(\"test/ap_cond_b\", ap[0], global_step=i)\n\t                else: \n\t                    writer.add_scalar(\"test/ap\", ap[0], global_step=i)\n\t                return ap\n\t    if train:\n\t        print(f\"Epoch {epoch} Train Loss: {loss.item()}\")\n\tdef main():\n\t    args = get_args()\n\t    writer = SummaryWriter(os.path.join(\"runs\", args.name), purge_step=0)\n\t    if args.cogent:\n", "        if args.dataset == \"clevr\":\n\t            dataset_train = CLEVR(args.data_dir, \"trainA\")\n\t            dataset_test_a = CLEVR(args.data_dir, \"valA\")\n\t            dataset_test_b = CLEVR(args.data_dir, \"valB\")\n\t        elif args.dataset == \"shapeworld4\":\n\t            dataset_train = SHAPEWORLD4(args.data_dir, \"train_a\")\n\t            dataset_test_a = SHAPEWORLD4(args.data_dir, \"val_a\")\n\t            dataset_test_b = SHAPEWORLD4(args.data_dir, \"val_b\")\n\t    else:\n\t        if args.dataset == \"clevr\":\n", "            dataset_train = CLEVR(args.data_dir, \"train\")\n\t            dataset_test = CLEVR(args.data_dir, \"val\")\n\t        elif args.dataset == \"shapeworld4\":\n\t            dataset_train = SHAPEWORLD4(args.data_dir, \"train\")\n\t            dataset_test = SHAPEWORLD4(args.data_dir, \"val\")\n\t    print('data loaded')\n\t    if not args.eval_only:\n\t        train_loader = get_loader(\n\t            dataset_train, batch_size=args.batch_size, num_workers=args.num_workers\n\t        )\n", "    if not args.train_only:\n\t        if args.cogent:\n\t            test_loader_a = get_loader(\n\t                dataset_test_a,\n\t                batch_size=5000,\n\t                num_workers=args.num_workers,\n\t                shuffle=False)\n\t            test_loader_b = get_loader(\n\t                dataset_test_b,\n\t                batch_size=5000,\n", "                num_workers=args.num_workers,\n\t                shuffle=False)\n\t        else:\n\t            test_loader = get_loader(\n\t                dataset_test,\n\t                batch_size=5000,\n\t                num_workers=args.num_workers,\n\t                shuffle=False)\n\t    # print(torch.cuda.is_available())\n\t    if args.dataset == \"shapeworld4\":\n", "        net = model.SlotAttention_model(n_slots=4, n_iters=3, n_attr=15,\n\t                                        encoder_hidden_channels=32,\n\t                                        attention_hidden_channels=64,\n\t                                        mlp_prediction=True,\n\t                                        device=args.device)\n\t    elif args.dataset == \"clevr\":\n\t        net = model.SlotAttention_model(n_slots=10, n_iters=3, n_attr=15,\n\t                                        encoder_hidden_channels=64,\n\t                                        attention_hidden_channels=128,\n\t                                        mlp_prediction=True,\n", "                                        device=args.device,\n\t                                        clevr_encoding=True)\n\t    args.n_attr = net.n_attr\n\t    if not args.no_cuda:\n\t        net = net.cuda()\n\t    optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n\t    criterion = torch.nn.SmoothL1Loss()\n\t    # Create RTPT object\n\t    rtpt = RTPT(name_initials=args.credentials, experiment_name=f\"Set prediction baseline \",\n\t                max_iterations=args.epochs)\n", "    # store args as txt file\n\t    set_utils.save_args(args, writer)\n\t    ap_list = []\n\t    ap_list_a = [] # for cogent\n\t    ap_list_b = [] # for cogent\n\t    total_train_time = 0\n\t    total_test_time = 0\n\t    time_list =[]\n\t    time_start = time.time()\n\t    start_epoch = 0\n", "    if args.resume:\n\t        print(\"Loading ckpt ...\")\n\t        log = torch.load(args.resume)\n\t        weights = log[\"weights\"]\n\t        net.load_state_dict(weights, strict=True)\n\t        start_epoch = log[\"args\"][\"epochs_trained\"] + 1\n\t        ap_list = log[\"ap_list\"]\n\t        ap_list_a = log[\"ap_list_a\"]\n\t        ap_list_b = log[\"ap_list_b\"]\n\t        time_list =log[\"time_list\"]\n", "        print(\"continue with epoch\", start_epoch)\n\t        # Start the RTPT tracking\n\t    rtpt.start()\n\t    for epoch in np.arange(start_epoch, args.epochs):\n\t        print(\"Epoch:\", epoch)\n\t        if not args.eval_only:\n\t            train_start = time.time()\n\t            run(net, train_loader, optimizer, criterion, writer, args, train=True, epoch=epoch)\n\t            time_train = time.time() - train_start\n\t            rtpt.step()\n", "        if not args.train_only:\n\t            test_start = time.time()\n\t            if args.cogent:\n\t                ap_a = run(net, test_loader_a, None, criterion, writer, args, test_cond=\"a\", train=False, epoch=epoch)\n\t                ap_list_a.append(ap_a)\n\t                ap_b = run(net, test_loader_b, None, criterion, writer, args, test_cond=\"b\", train=False, epoch=epoch)\n\t                ap_list_b.append(ap_b)\n\t            else:\n\t                ap = run(net, test_loader, None, criterion, writer, args, train=False, epoch=epoch)\n\t                ap_list.append(ap)\n", "            time_test = time.time() - test_start \n\t            if args.eval_only:\n\t                exit()\n\t        torch.cuda.empty_cache()\n\t        args.epochs_trained = epoch\n\t        total_test_time += time_test\n\t        total_train_time += time_train\n\t        time_list.append([total_train_time, total_test_time, time_train, time_test])\n\t        results = {\n\t                \"name\": args.name,\n", "                \"weights\": net.state_dict(),\n\t                \"args\": vars(args),\n\t                \"ap_list\": ap_list, #empty for cogent\n\t                \"ap_list_a\": ap_list_a,\n\t                \"ap_list_b\": ap_list_b,\n\t                \"time\": time_list}\n\t        torch.save(results, os.path.join(\"runs\", args.name, args.name))\n\t        if args.eval_only:\n\t            break\n\t        print(\"total time\", misc_utils.time_delta_now(time_start))\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "src/experiments/baseline_slot_attention/dataGen.py", "chunked_list": ["import torch\n\timport torchvision\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.transforms import transforms\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision import transforms\n\tfrom skimage import io\n\timport os\n\timport numpy as np\n\timport random\n", "import torch\n\timport matplotlib.pyplot as plt\n\tfrom PIL import Image, ImageFile\n\tImageFile.LOAD_TRUNCATED_IMAGES = True\n\timport json\n\timport datasets as datasets\n\tdef get_loader(dataset, batch_size, num_workers=8, shuffle=True):\n\t    '''\n\t    Returns and iterable dataset with specified batchsize and shuffling.\n\t    '''\n", "    return torch.utils.data.DataLoader(\n\t        dataset,\n\t        shuffle=shuffle,\n\t        batch_size=batch_size,\n\t        num_workers=num_workers\n\t)\n\tdef get_encoding_shapeworld(color, shape, shade, size):\n\t    if color == 'red':\n\t        col_enc = [1,0,0,0,0,0,0,0]\n\t    elif color == 'blue':\n", "        col_enc = [0,1,0,0,0,0,0,0]\n\t    elif color == 'green':\n\t        col_enc = [0,0,1,0,0,0,0,0]\n\t    elif color == 'gray':\n\t        col_enc = [0,0,0,1,0,0,0,0]\n\t    elif color == 'brown':\n\t        col_enc = [0,0,0,0,1,0,0,0]\n\t    elif color == 'magenta':\n\t        col_enc = [0,0,0,0,0,1,0,0]\n\t    elif color == 'cyan':\n", "        col_enc = [0,0,0,0,0,0,1,0]\n\t    elif color == 'yellow':\n\t        col_enc = [0,0,0,0,0,0,0,1]\n\t    if shape == 'circle':\n\t        shape_enc = [1,0,0]\n\t    elif shape == 'triangle':\n\t        shape_enc = [0,1,0]\n\t    elif shape == 'square':\n\t        shape_enc = [0,0,1]    \n\t    if shade == 'bright':\n", "        shade_enc = [1,0]\n\t    elif shade =='dark':\n\t        shade_enc = [0,1]\n\t    if size == 'small':\n\t        size_enc = [1,0]\n\t    elif size == 'big':\n\t        size_enc = [0,1]\n\t    return np.array([1]+ col_enc + shape_enc + shade_enc + size_enc)\n\tclass SHAPEWORLD4(Dataset):\n\t    def __init__(self, root, mode, learn_concept='default', bg_encoded=True):\n", "        datasets.maybe_download_shapeworld4()\n\t        self.root = root\n\t        self.mode = mode\n\t        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\t        #dictionary of the form {'image_idx':'img_path'}\n\t        self.img_paths = {}\n\t        for file in os.scandir(os.path.join(root, 'images', mode)):\n\t            img_path = file.path\n\t            img_path_idx =   img_path.split(\"/\")\n\t            img_path_idx = img_path_idx[-1]\n", "            img_path_idx = img_path_idx[:-4][6:]\n\t            try:\n\t                img_path_idx =  int(img_path_idx)\n\t                self.img_paths[img_path_idx] = img_path\n\t            except:\n\t                print(\"path:\",img_path_idx)\n\t        count = 0\n\t        #target maps of the form {'target:idx': observation string} or {'target:idx': obj encoding}\n\t        self.obj_map = {}\n\t        with open(os.path.join(root, 'labels', mode,\"world_model.json\")) as f:\n", "            worlds = json.load(f)\n\t            #iterate over all objects\n\t            for world in worlds:\n\t                num_objects = 0\n\t                target_obs = \"\"\n\t                obj_enc = []\n\t                for entity in world['entities']:\n\t                    color = entity['color']['name']\n\t                    shape = entity['shape']['name']\n\t                    shade_val = entity['color']['shade']\n", "                    if shade_val == 0.0:\n\t                        shade = 'bright'\n\t                    else:\n\t                        shade = 'dark'\n\t                    size_val = entity['shape']['size']['x']\n\t                    if size_val == 0.075:\n\t                        size = 'small'\n\t                    elif size_val == 0.15:\n\t                        size = 'big'\n\t                    name = 'o' + str(num_objects+1)\n", "                    obj_enc.append(get_encoding_shapeworld(color, shape, shade, size))\n\t                    num_objects += 1\n\t                #bg encodings\n\t                for i in range(num_objects, 4):\n\t                    name = 'o' + str(num_objects+1)\n\t                    obj_enc.append(np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]))\n\t                    num_objects += 1\n\t                self.obj_map[count] = torch.Tensor(obj_enc)\n\t                count+=1\n\t    def __getitem__(self, index):\n", "        #get the image\n\t        img_path = self.img_paths[index]\n\t        img = io.imread(img_path)[:, :, :3]\n\t        transform = transforms.Compose([\n\t            transforms.ToPILImage(),\n\t            #transforms.CenterCrop(250),\n\t            #transforms.Resize((32, 32)),\n\t            transforms.ToTensor(),\n\t        ])\n\t        img = transform(img)\n", "        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\t        return img, self.obj_map[index]#, mask\n\t    def __len__(self):\n\t        return len(self.img_paths)\n\tdef get_encoding_clevr(size, material, shape, color ):\n\t    #size (small, large, bg)\n\t    if size == \"small\":\n\t        size_enc = [1,0]\n\t    elif size == \"large\":\n\t        size_enc = [0,1]\n", "    #material (rubber, metal, bg)\n\t    if material == \"rubber\":\n\t        material_enc = [1,0]\n\t    elif material == \"metal\":\n\t        material_enc = [0,1]\n\t    #shape (cube, sphere, cylinder, bg)\n\t    if shape == \"cube\":\n\t        shape_enc = [1,0,0]\n\t    elif shape == \"sphere\":\n\t        shape_enc = [0,1,0]\n", "    elif shape == \"cylinder\":\n\t        shape_enc = [0,0,1]\n\t    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n\t    if color == \"gray\":\n\t        color_enc = [1,0,0,0,0,0,0,0]\n\t    elif color == \"red\":\n\t        color_enc = [0,1,0,0,0,0,0,0]\n\t    elif color == \"blue\":\n\t        color_enc = [0,0,1,0,0,0,0,0]\n\t    elif color == \"green\":\n", "        color_enc = [0,0,0,1,0,0,0,0]\n\t    elif color == \"brown\":\n\t        color_enc = [0,0,0,0,1,0,0,0]\n\t    elif color == \"purple\":\n\t        color_enc = [0,0,0,0,0,1,0,0]\n\t    elif color == \"cyan\":\n\t        color_enc = [0,0,0,0,0,0,1,0]\n\t    elif color == \"yellow\":\n\t        color_enc = [0,0,0,0,0,0,0,1]\n\t    return np.array([1] + size_enc + material_enc + shape_enc + color_enc )\n", "class CLEVR(Dataset):\n\t    def __init__(self, root, mode, img_paths=None, files_names=None, obj_num=None):\n\t        self.root = root  # The root folder of the dataset\n\t        self.mode = mode  # The mode of 'train' or 'val'\n\t        self.files_names = files_names # The list of the files names with correct nuber of objects\n\t        if obj_num is not None:\n\t            self.obj_num = obj_num  # The upper limit of number of objects \n\t        else:\n\t            self.obj_num = 10\n\t        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n", "        #list of sorted image paths\n\t        self.img_paths = []\n\t        if img_paths:\n\t            self.img_paths = img_paths\n\t        else:                    \n\t            #open directory and save all image paths\n\t            for file in os.scandir(os.path.join(root, 'images', mode)):\n\t                img_path = file.path\n\t                if '.png' in img_path:\n\t                    self.img_paths.append(img_path)\n", "        self.img_paths.sort()\n\t        self.img_paths = np.array(self.img_paths, dtype=str)\n\t        count = 0\n\t        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n\t        #self.obj_map = {}\n\t        count = 0        \n\t        #We have up to 10 objects in the image, load the json file\n\t        with open(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\")) as f:\n\t            data = json.load(f)\n\t            self.obj_map = np.empty((len(data['scenes']),10,16), dtype=np.float32)\n", "            #iterate over each scene and create the query string and obj encoding\n\t            print(\"parsing scences\")\n\t            for scene in data['scenes']:\n\t                obj_encoding_list = []\n\t                if self.files_names:\n\t                    if any(scene['image_filename'] in file_name for file_name in files_names):                    \n\t                        num_objects = 0\n\t                        for idx, obj in enumerate(scene['objects']):\n\t                            obj_encoding_list.append(get_encoding_clevr(obj['size'], obj['material'], obj['shape'], obj['color']))\n\t                            num_objects = idx+1 #store the num of objects \n", "                        #fill in background objects\n\t                        for idx in range(num_objects, self.obj_num):\n\t                            obj_encoding_list.append([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n\t                        self.obj_map[count] = np.array(obj_encoding_list)\n\t                        count += 1\n\t                else:\n\t                    num_objects=0\n\t                    for idx, obj in enumerate(scene['objects']):\n\t                        obj_encoding_list.append(get_encoding_clevr(obj['size'], obj['material'], obj['shape'], obj['color']))\n\t                        num_objects = idx+1 #store the num of objects \n", "                    #fill in background objects\n\t                    for idx in range(num_objects, 10):\n\t                        obj_encoding_list.append([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n\t                    self.obj_map[scene['image_index']] = np.array(obj_encoding_list, dtype=np.float32)\n\t            print(\"done\")\n\t        if self.files_names:\n\t            print(f'Correctly found images {count} out of {len(files_names)}')\n\t    def __getitem__(self, index):\n\t        #get the image\n\t        img_path = self.img_paths[index]\n", "        img = io.imread(img_path)[:, :, :3]\n\t        img = Image.fromarray(img).resize((128,128)) #using transforms to resize gets us a shrared-memory leak :(\n\t        transform = transforms.Compose([\n\t            #transforms.ToPILImage(),\n\t            #transforms.CenterCrop((29, 221,64, 256)), #why do we need to crop?\n\t            #transforms.Resize((128, 128)),\n\t            transforms.ToTensor(),\n\t        ])\n\t        img = transform(img)\n\t        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n", "        return img, self.obj_map[index]#, mask\n\t    def __len__(self):\n\t        return self.img_paths.shape[0]"]}
{"filename": "src/experiments/vqa/cmd_args2.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\timport argparse\n\timport sys\n\timport random\n", "import numpy as np\n\timport torch\n\timport os\n\timport logging\n\t# Utility function for take in yes/no and convert it to boolean\n\tdef convert_str_to_bool(cmd_args):\n\t    for key, val in vars(cmd_args).items():\n\t        if val == \"yes\":\n\t            setattr(cmd_args, key, True)\n\t        elif val == \"no\":\n", "            setattr(cmd_args, key, False)\n\tdef str2bool(v):\n\t    if isinstance(v, bool):\n\t       return v\n\t    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n\t        return True\n\t    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n\t        return False\n\t    else:\n\t        raise argparse.ArgumentTypeError('Boolean value expected.')\n", "class LearningSetting(object):\n\t    def __init__(self):\n\t        data_dir = os.path.abspath(os.path.join(os.path.abspath(__file__), \"../../data\"))\n\t        dataset_dir = os.path.join(data_dir, \"dataset\")\n\t        knowledge_base_dir = os.path.join(data_dir, \"knowledge_base\")\n\t        self.parser = argparse.ArgumentParser(description='Argparser', allow_abbrev=True)\n\t        # Logistics\n\t        self.parser.add_argument(\"--seed\", default=1234, type=int, help=\"set random seed\")\n\t        self.parser.add_argument(\"--gpu\", default=0, type=int, help=\"GPU id\")\n\t        self.parser.add_argument('--timeout', default=600, type=int, help=\"Execution timeout\")\n", "        # Learning settings\n\t        self.parser.add_argument(\"--name_threshold\", default=0, type=float)\n\t        self.parser.add_argument(\"--attr_threshold\", default=0, type=float)\n\t        self.parser.add_argument(\"--rela_threshold\", default=0, type=float)\n\t        # self.parser.add_argument(\"--name_topk_n\", default=-1, type=int)\n\t        # self.parser.add_argument(\"--attr_topk_n\", default=-1, type=int)\n\t        # self.parser.add_argument(\"--rela_topk_n\", default=80, type=int)\n\t        self.parser.add_argument(\"--topk\", default=3, type=int)\n\t        # training settings\n\t        self.parser.add_argument('--feat_dim', type=int, default=2048)\n", "        self.parser.add_argument('--n_epochs', type=int, default=20)\n\t        self.parser.add_argument('--batch_size', type=int, default=1)\n\t        self.parser.add_argument('--max_workers', type=int, default=1)\n\t        self.parser.add_argument('--axiom_update_size', type=int, default=4)\n\t        self.parser.add_argument('--name_lr', type=float, default=0.0001)\n\t        self.parser.add_argument('--attr_lr', type=float, default=0.0001)\n\t        self.parser.add_argument('--rela_lr', type=float, default=0.0001)\n\t        self.parser.add_argument('--reinforce', type=str2bool, nargs='?', const=True, default=False) # reinforce only support single thread\n\t        self.parser.add_argument('--replays', type=int, default=5)\n\t        self.parser.add_argument('--model_dir', default=data_dir+'/model_ckpts_sg')\n", "        # self.parser.add_argument('--model_dir', default=None)\n\t        self.parser.add_argument('--log_name', default='model.log')\n\t        self.parser.add_argument('--feat_f', default=data_dir+'/features.npy')\n\t        self.parser.add_argument('--train_f', default=dataset_dir+'/task_list/train_tasks_c2_10.pkl')\n\t        self.parser.add_argument('--val_f', default=dataset_dir+'/task_list/val_tasks.pkl')\n\t        self.parser.add_argument('--test_f', default=dataset_dir+'/task_list/test_tasks_c2_1000.pkl')\n\t        self.parser.add_argument('--cul_prov', type=bool, default=False)\n\t        self.parser.add_argument('--meta_f', default=data_dir+'/gqa_info.json')\n\t        self.parser.add_argument('--scene_f', default=data_dir+'/gqa_formatted_scene_graph.pkl')\n\t        self.parser.add_argument('--image_data_f', default=data_dir+'/image_data.json')\n", "        self.parser.add_argument('--dataset_type', default='name') # name, relation, attr:<groupindex>\n\t        self.parser.add_argument('--function', default=None) #KG_Find / Hypernym_Find / Find_Name / Find_Attr / Relate / Relate_Reverse / And / Or\n\t        self.parser.add_argument('--knowledge_base_dir', default=knowledge_base_dir)\n\t        # self.parser.add_argument('--interp_size', type=int, default=2)\n\t        self.parser.add_argument('--save_dir', default=data_dir+\"/problog_data\")\n\t        self.args = self.parser.parse_args(sys.argv[1:])\n\tls = LearningSetting()\n\tcmd_args = ls.args\n\t# print(cmd_args)\n\t# Fix random seed for debugging purpose\n", "if (ls.args.seed != None):\n\t    random.seed(ls.args.seed)\n\t    np.random.seed(ls.args.seed)\n\t    torch.manual_seed(ls.args.seed)\n\tif not type(cmd_args.gpu) == None:\n\t    os.environ['CUDA_VISIBLE_DEVICES'] = str(cmd_args.gpu)\n\t    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\tif cmd_args.model_dir is not None:\n\t    if not os.path.exists(cmd_args.model_dir):\n\t        os.makedirs(cmd_args.model_dir)\n", "    log_path = os.path.join(cmd_args.model_dir, cmd_args.log_name)\n\t    logging.basicConfig(filename=log_path, filemode='w', level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\n\t    logging.info(cmd_args)\n\t    logging.info(\"start!\")\n"]}
{"filename": "src/experiments/vqa/vqa_utils.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\timport os\n\timport json\n\timport argparse\n", "import numpy as np\n\timport torch\n\timport random\n\timport pickle\n\tfrom sklearn import metrics\n\tTIME_OUT = 120\n\t###########################################################################\n\t# basic utilities\n\tdef to_image_id_dict(dict_list):\n\t    ii_dict = {}\n", "    for dc in dict_list:\n\t        image_id = dc['image_id']\n\t        ii_dict[image_id] = dc\n\t    return ii_dict\n\tarticles = ['a', 'an', 'the', 'some', 'it']\n\tdef remove_article(st):\n\t    st = st.split()\n\t    st = [word for word in st if word not in articles]\n\t    return \" \".join(st)\n\t################################################################################################################\n", "# AUC calculation\n\tdef get_recall(labels, logits, topk=2):\n\t    #a = torch.tensor([[0, 1, 1], [1, 1, 0], [0, 0, 1],[1, 1, 1]])\n\t    #b = torch.tensor([[0, 0, 0], [0, 1, 0], [0, 0, 1],[0, 1, 1]])\n\t    # Calculate the recall\n\t    _, pred = logits.topk(topk, 1, True, True) #get idx of the biggest k values in the tensor\n\t    print(pred)\n\t    pred = pred.t() #transpose\n\t    #gather gets the elements from a given indices tensor. Here we get the elements at the same positions from our (top5) predictions\n\t    #we then sum all entries up along the axis and therefore count the top-5 entries in the labels tensors at the prediction position indices\n", "    correct = torch.sum(labels.gather(1, pred.t()), dim=1)\n\t    print(\"correct\", correct)\n\t    #now we some up all true labels. We clamp them to be maximum top5\n\t    correct_label = torch.clamp(torch.sum(labels, dim = 1), 0, topk)\n\t    #we now can compare if the number of correctly  found top-5 labels on the predictions vector is the same as on the same positions as the GT vector\n\t    print(\"correct label\", correct_label)\n\t    accuracy = torch.mean(correct / correct_label).item()\n\t    return accuracy\n\tdef single_auc_score(label, pred):\n\t    label = label\n", "    pred = [p.item() if not (type(p) == float or type(p) == int)\n\t            else p for p in pred]\n\t    if len(set(label)) == 1:\n\t        auc = -1\n\t    else:\n\t        fpr, tpr, thresholds = metrics.roc_curve(label, pred, pos_label=1)\n\t        auc = metrics.auc(fpr, tpr)\n\t    return auc\n\tdef auc_score(labels, preds):\n\t    if type(labels) == torch.Tensor:\n", "        labels = labels.long().cpu().detach().numpy()\n\t    if type(preds) == torch.Tensor:\n\t        preds = preds.cpu().detach().numpy()\n\t    if (type(labels) == torch.Tensor or type(labels) == np.ndarray) and len(labels.shape) == 2:\n\t        aucs = []\n\t        for label, pred in zip(labels, preds):\n\t            auc_single = single_auc_score(label, pred)\n\t            if not auc_single < 0:\n\t                aucs.append(auc_single)\n\t        auc = np.array(aucs).mean()\n", "    else:\n\t        auc = single_auc_score(labels, preds)\n\t    return auc\n\tdef to_binary_labels(labels, attr_num):\n\t    if labels.shape[-1] == attr_num:\n\t        return labels\n\t    binary_labels = torch.zeros((labels.shape[0], attr_num))\n\t    for ct, label in enumerate(labels):\n\t        binary_labels[ct][label] = 1\n\t    return binary_labels\n", "##############################################################################\n\t# model loading\n\tdef get_default_args():\n\t    DATA_ROOT = os.path.abspath(os.path.join(\n\t        os.path.abspath(__file__), \"../../data\"))\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--gpu', type=int, default=2)\n\t    parser.add_argument('--seed', type=int, default=1234)\n\t    parser.add_argument('--feat_dim', type=int, default=2048)\n\t    # parser.add_argument('--type', default='name')\n", "    parser.add_argument('--model_dir', default=DATA_ROOT + '/model_ckpts')\n\t    parser.add_argument('--meta_f', default=DATA_ROOT + '/preprocessing/gqa_info.json')\n\t    args = parser.parse_args()\n\t    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\t    np.random.seed(args.seed)\n\t    torch.manual_seed(args.seed)\n\t    random.seed(args.seed)\n\t    return args\n\tdef gen_task(formatted_scene_graph_path, vg_image_data_path, questions_path, features_path, n=100, q_length=None):\n\t    with open(questions_path, 'rb') as questions_file:\n", "        questions = pickle.load(questions_file)\n\t    with open(formatted_scene_graph_path, 'rb') as vg_scene_graphs_file:\n\t        scene_graphs = pickle.load(vg_scene_graphs_file)\n\t    with open(vg_image_data_path, 'r') as vg_image_data_file:\n\t        image_data = json.load(vg_image_data_file)\n\t    features = np.load(features_path, allow_pickle=True)\n\t    features = features.item()\n\t    image_data = to_image_id_dict(image_data)\n\t    for question in questions[:n]:\n\t        if q_length is not None:\n", "            current_len = len(question['clauses'])\n\t            if not current_len == q_length:\n\t                continue\n\t        # functions = question[\"clauses\"]\n\t        image_id = question[\"image_id\"]\n\t        scene_graph = scene_graphs[image_id]\n\t        cur_image_data = image_data[image_id]\n\t        if not scene_graph['image_id'] == image_id or not cur_image_data['image_id'] == image_id:\n\t            raise Exception(\"Mismatched image id\")\n\t        info = {}\n", "        info['image_id'] = image_id\n\t        info['scene_graph'] = scene_graph\n\t        info['url'] = cur_image_data['url']\n\t        info['question'] = question\n\t        info['object_feature'] = []\n\t        info['object_ids'] = []\n\t        for obj_id in scene_graph['names'].keys():\n\t            info['object_ids'].append(obj_id)\n\t            info['object_feature'].append(features.get(obj_id))\n\t        yield(info)\n"]}
{"filename": "src/experiments/vqa/train.py", "chunked_list": ["print(\"start importing...\")\n\timport time\n\timport sys\n\timport argparse\n\timport datetime\n\tsys.path.append('../../')\n\tsys.path.append('../../SLASH/')\n\tsys.path.append('../../EinsumNetworks/src/')\n\t#torch, numpy, ...\n\timport torch\n", "from torch.utils.tensorboard import SummaryWriter\n\tfrom torchvision.transforms import transforms\n\timport torchvision\n\timport numpy as np\n\timport json\n\t#own modules\n\tfrom dataGen import VQA\n\tfrom einsum_wrapper import EiNet\n\tfrom network_nn import Net_nn\n\tfrom tqdm import tqdm\n", "#import slash\n\tfrom slash import SLASH\n\timport os\n\tfrom datetime import date\n\timport utils\n\tfrom utils import set_manual_seed\n\tfrom pathlib import Path\n\tfrom rtpt import RTPT\n\timport pickle\n\tfrom knowledge_graph import RULES, KG\n", "from dataGen import name_npp, relation_npp, attribute_npp\n\tfrom models import name_clf, rela_clf, attr_clf, rela_einet, name_einet, attr_einet\n\tprint(\"...done\")\n\tdef get_args():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--seed\", type=int, default=10, help=\"Random generator seed for all frameworks\"\n\t    )\n\t    parser.add_argument(\n\t        \"--epochs\", type=int, default=10, help=\"Number of epochs to train with\"\n", "    )\n\t    parser.add_argument(\n\t        \"--lr\", type=float, default=0.01, help=\"Learning rate of model\"\n\t    )\n\t    parser.add_argument(\n\t        \"--network-type\",\n\t        choices=[\"nn\",\"pc\"],\n\t        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n\t    )\n\t    parser.add_argument(\n", "        \"--pc-structure\",\n\t        choices=[\"poon-domingos\",\"binary-trees\"],\n\t        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n\t    )\n\t    parser.add_argument(\n\t        \"--batch-size\", type=int, default=100, help=\"Batch size to train with\"\n\t    )\n\t    parser.add_argument(\n\t        \"--num-workers\", type=int, default=6, help=\"Number of threads for data loader\"\n\t    )\n", "    parser.add_argument(\n\t        \"--p-num\", type=int, default=8, help=\"Number of processes to devide the batch for parallel processing\"\n\t    )\n\t    parser.add_argument(\n\t        \"--exp-name\", type=str, default=\"vqa\", help=\"Name of the experiment\"\n\t    )\n\t    parser.add_argument(\"--credentials\", type=str, help=\"Credentials for rtpt\")\n\t    args = parser.parse_args()\n\t    if args.network_type == 'pc':\n\t        args.use_pc = True\n", "    else:\n\t        args.use_pc = False\n\t    return args\n\tdef determine_max_objects(task_file):\n\t    with open (task_file, 'rb') as tf:\n\t        tasks = pickle.load(tf)\n\t        print(\"taskfile len\",len(tasks))\n\t    #get the biggest number of objects in the image\n\t    max_objects = 0\n\t    for tidx, task in enumerate(tasks):\n", "        all_oid = task['question']['input']\n\t        len_all_oid = len(all_oid)\n\t        #store the biggest number of objects in an image\n\t        if len_all_oid > max_objects:\n\t            max_objects = len_all_oid\n\t    return max_objects\n\tdef slash_vqa():\n\t    args = get_args()\n\t    print(args)\n\t    # Set the seeds for PRNG\n", "    set_manual_seed(args.seed)\n\t    # Create RTPT object\n\t    rtpt = RTPT(name_initials=args.credentials, experiment_name='SLASH VQA', max_iterations=args.epochs)\n\t    # Start the RTPT tracking\n\t    rtpt.start()\n\t    exp_name = args.exp_name+ \"_\"+date.today().strftime(\"%d_%m_%Y\")\n\t    writer = SummaryWriter(os.path.join(\"runs\", exp_name, str(args.seed)), purge_step=0)\n\t    print(exp_name)\n\t    Path(\"data/\"+exp_name+\"/\").mkdir(parents=True, exist_ok=True)\n\t    #TODO workaround that adds +- notation \n", "    program_example = \"\"\"\n\t%scallop conversion rules\n\tname(O,N) :-  name(0,+O,-N).\n\tattr(O,A) :-  attr(0, +O, -A).\n\trelation(O1,O2,N) :-  relation(0, +(O1,O2), -N).\n\t    \"\"\"\n\t    # datasets\n\t    #train_f = \"dataset/task_list/train_tasks.pkl\"  # Training dataset\n\t    train_f = \"dataset/task_list/train_tasks_c2_10000.pkl\"\n\t    val_f = \"dataset/task_list/val_tasks.pkl\"  # Test datset\n", "    test_f = \"dataset/task_list/test_tasks.pkl\"  # Test datset\n\t    # test_f = {\"c2\":\"dataset/task_list/test_tasks_c2_1000.pkl\",\n\t    #           \"c3\":\"dataset/task_list/test_tasks_c3_1000.pkl\",\n\t    #           \"c4\":\"dataset/task_list/test_tasks_c4_1000.pkl\",\n\t    #           \"c5\":\"dataset/task_list/test_tasks_c5_1000.pkl\",\n\t    #           \"c6\":\"dataset/task_list/test_tasks_c6_1000.pkl\"}\n\t    vqa_params = {\"l\":100,\n\t                \"l_split\":5,\n\t                \"num_names\":500,\n\t                \"max_models\":10000,\n", "                \"asp_timeout\": 30 }\n\t    num_obj = []\n\t    num_obj.append(determine_max_objects(train_f))\n\t    num_obj.append(determine_max_objects(val_f))\n\t    if type(test_f) == str: \n\t        num_obj.append(determine_max_objects(test_f))\n\t    #if we have multiple test files\n\t    elif type(test_f) == dict:\n\t        for key in test_f:   \n\t            num_obj.append(determine_max_objects(test_f[key]))\n", "    NUM_OBJECTS = np.max(num_obj)\n\t    NUM_OBJECTS = 70\n\t    train_data = VQA(\"train\", train_f, NUM_OBJECTS)\n\t    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=4)\n\t    val_data = VQA(\"val\", val_f, NUM_OBJECTS)\n\t    val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n\t    if type(test_f) == str: \n\t        test_data = VQA(\"test\", test_f, NUM_OBJECTS)\n\t        test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n\t    #if we have multiple test files\n", "    elif type(test_f) == dict: \n\t        for key in test_f:\n\t            test_data = VQA(\"test\", test_f[key], NUM_OBJECTS)\n\t            test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n\t            test_f[key] = test_loader\n\t    #create the SLASH Program\n\t    # nnMapping = {'relation': rela_einet, 'name':name_einet , \"attr\":attr_einet}\n\t    # optimizers = {'relation': torch.optim.Adam(rela_einet.parameters(), lr=args.lr, eps=1e-7),\n\t    #                   'name': torch.optim.Adam(name_einet.parameters(), lr=args.lr, eps=1e-7),\n\t    #                   'attr': torch.optim.Adam(attr_einet.parameters(), lr=args.lr, eps=1e-7)}\n", "    nnMapping = {'relation':rela_clf, 'name':name_clf , \"attr\":attr_clf}\n\t    optimizers = {'relation': torch.optim.Adam(rela_clf.parameters(), lr=args.lr, eps=1e-7),\n\t                      'name': torch.optim.Adam(name_clf.parameters(), lr=args.lr, eps=1e-7),\n\t                      'attr': torch.optim.Adam(attr_clf.parameters(), lr=args.lr, eps=1e-7)}\n\t    num_trainable_params = [sum(p.numel() for p in rela_clf.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in name_clf.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in attr_clf.parameters() if p.requires_grad)]\n\t    print(\"num traiable params:\", num_trainable_params)\n\t    loss_list = []\n\t    forward_time_list = []\n", "    asp_time_list = []\n\t    gradient_time_list = []\n\t    backward_time_list = []\n\t    train_recall_list = []\n\t    val_recall_list = []\n\t    test_recall_list = []\n\t    sm_per_batch_list = []\n\t    best_test_recall = torch.zeros((1), dtype=torch.float)\n\t    best_val_recall = torch.zeros((1), dtype=torch.float)\n\t    #setup dataloader for the datasets\n", "    all_oid = np.arange(0,NUM_OBJECTS)\n\t    object_string = \"\".join([ f\"object({oid1},{oid2}). \" for oid1 in all_oid for oid2 in all_oid if oid1 != oid2])\n\t    object_string = \"\".join([\"\".join([f\"object({oid}). \" for oid in all_oid]), object_string])\n\t    #parse the SLASH program\n\t    print(\"parse all train programs\")\n\t    program = \"\".join([KG, RULES, object_string, name_npp, relation_npp, attribute_npp, program_example])\n\t    SLASHobj = SLASH(program, nnMapping, optimizers)\n\t    for e in range(0, args.epochs):\n\t        print(\"---TRAIN---\")\n\t        loss, forward_time, gradient_time, asp_time, backward_time, sm_per_batch = SLASHobj.learn(dataset_loader=train_loader,\n", "            epoch=e, method=\"same\", k_num=0, p_num=args.p_num, same_threshold={\"relation\":0.999, \"name\":0.999, \"attr\":0.99}, writer=writer, \n\t            vqa=True, batched_pass=True, vqa_params= vqa_params)\n\t        loss_list.append(loss)\n\t        forward_time_list.append(forward_time)\n\t        asp_time_list.append(asp_time)\n\t        gradient_time_list.append(gradient_time)\n\t        backward_time_list.append(backward_time)\n\t        sm_per_batch_list.append(sm_per_batch)\n\t        time_metrics = {\"forward_time\":forward_time_list,\n\t                        \"asp_time\":asp_time_list,\n", "                        \"backward_time\":backward_time_list,\n\t                        \"gradient_time\":gradient_time_list,\n\t                        \"sm_per_batch\":sm_per_batch_list\n\t                        }\n\t        writer.add_scalar('train/loss', loss, e)\n\t        saveModelPath = 'data/'+exp_name+'/slash_vqa_models_seed'+str(args.seed)+'_epoch_'+str(e)+'.pt'\n\t        # Export results and networks\n\t        print('Storing the trained model into {}'.format(saveModelPath))\n\t        # print(\"---TRAIN---\")\n\t        # recall_5_train = SLASHobj.testVQA(train_loader_test_mode, args.p_num, vqa_params=vqa_params)\n", "        # writer.add_scalar('train/recall', recall_5_train, e)\n\t        # print(\"recall@5 train\", recall_5_train)\n\t        print(\"---VAL---\")\n\t        recall_5_val, val_time = SLASHobj.testVQA(val_loader, args.p_num, vqa_params=vqa_params)\n\t        writer.add_scalar('val/recall', recall_5_val, e)\n\t        val_recall_list.append(recall_5_val)\n\t        print(\"val--recall@5:\", recall_5_val, \", val_time:\", val_time )\n\t        val_recall_list.append(recall_5_val)\n\t        print(\"---TEST---\")\n\t        if type(test_f) == str:\n", "            recall_5_test, test_time = SLASHobj.testVQA(test_loader, args.p_num, vqa_params=vqa_params)\n\t            writer.add_scalar('test/recall', recall_5_test, e)\n\t            print(\"test-recall@5\", recall_5_test)\n\t            test_recall_list.append(recall_5_test)\n\t        elif type(test_f) == dict:\n\t            test_time = 0\n\t            recalls = []\n\t            for key in test_f:\n\t                recall_5_test, tt = SLASHobj.testVQA(test_f[key], args.p_num, vqa_params=vqa_params)\n\t                test_time += tt\n", "                recalls.append(recall_5_test)\n\t                print(\"test-recall@5_{}\".format(key), recall_5_test, \", test_time:\", tt )\n\t                writer.add_scalar('test/recall_{}'.format(key), recall_5_test, e)\n\t            writer.add_scalar('test/recall_c_all', np.array(recalls).mean(), e)\n\t            print(\"test-recall@5_c_all\", np.mean(recalls), \", test_time:\", test_time)\n\t            test_recall_list.append(recalls)\n\t        # Check if the new best value for recall@5 on val dataset is reached.\n\t        # If so, then save the new best performing model and test performance\n\t        if best_val_recall < recall_5_val:\n\t            best_val_recall = recall_5_val\n", "            print(\"Found the new best performing model and store it!\")\n\t            saveBestModelPath = 'data/'+exp_name+'/slash_vqa_models_seed'+str(args.seed)+'best.pt'\n\t            torch.save({\"relation_clf\": rela_clf.state_dict(),\n\t                    \"name_clf\":  name_clf.state_dict(),\n\t                    \"attr_clf\":attr_clf.state_dict(),\n\t                    \"resume\": {\n\t                        \"optimizer_rela\":optimizers['relation'].state_dict(),\n\t                        \"optimizer_name\":optimizers['name'].state_dict(),\n\t                        \"optimizer_attr\":optimizers['attr'].state_dict(),\n\t                        \"epoch\":e\n", "                            },\n\t                    \"args\":args,\n\t                    \"loss\": loss_list,\n\t                    \"train_recall_list\":train_recall_list,\n\t                    \"val_recall_list\":val_recall_list,\n\t                    \"test_recall_list\":test_recall_list,\n\t                    \"time_metrics\":time_metrics,\n\t                    \"program\":program,\n\t                    \"vqa_params\":vqa_params}, saveBestModelPath)\n\t        print(\"storing the model\")\n", "        torch.save({\"relation_clf\": rela_clf.state_dict(),\n\t                    \"name_clf\":  name_clf.state_dict(),\n\t                    \"attr_clf\":attr_clf.state_dict(),\n\t                    \"resume\": {\n\t                        \"optimizer_rela\":optimizers['relation'].state_dict(),\n\t                        \"optimizer_name\":optimizers['name'].state_dict(),\n\t                        \"optimizer_attr\":optimizers['attr'].state_dict(),\n\t                        \"epoch\":e\n\t                            },\n\t                    \"args\":args,\n", "                    \"loss\": loss_list,\n\t                    \"train_recall_list\":train_recall_list,\n\t                    \"val_recall_list\":val_recall_list,\n\t                    \"test_recall_list\":test_recall_list,\n\t                    \"time_metrics\":time_metrics,\n\t                    \"program\":program,\n\t                    \"vqa_params\":vqa_params}, saveModelPath)\n\t        # Update the RTPT\n\t        rtpt.step(subtitle=f\"loss={loss:2.2f};recall@5={recall_5_test:2.2f}\")\n\tif __name__ == \"__main__\":\n", "    slash_vqa()\n"]}
{"filename": "src/experiments/vqa/knowledge_graph.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\timport re \n\tKG_FACTS = '''\n\t%FACTS\n", "is_a(\"boat\", \"watercraft\").\n\tis_a(\"photograph\", \"artwork\").\n\tis_a(\"chalk\", \"writing implement\").\n\tis_a(\"tv\", \"electrical appliance\").\n\tis_a(\"screen\", \"electronic device\").\n\tis_a(\"chocolate\", \"dessert\").\n\tis_a(\"pig\", \"even-toed ungulate\").\n\tis_a(\"pig\", \"mammal\").\n\tis_a(\"alcohol\", \"alcoholic beverage\").\n\tis_a(\"wine\", \"alcoholic beverage\").\n", "is_a(\"picture\", \"artwork\").\n\tis_a(\"zebra\", \"herbivore\").\n\tis_a(\"skillet\", \"kitchen utensil\").\n\tis_a(\"horse\", \"mammal\").\n\tis_a(\"rice\", \"staple food\").\n\tis_a(\"rice\", \"grains\").\n\tis_a(\"zebra\", \"mammal\").\n\tis_a(\"refrigerator\", \"electrical appliance\").\n\tis_a(\"soda\", \"soft drinks\").\n\tis_a(\"horse\", \"herbivore\").\n", "is_a(\"air conditioner\", \"electrical appliance\").\n\tis_a(\"container\", \"kitchen utensil\").\n\tis_a(\"cat\", \"feline\").\n\tis_a(\"cat\", \"mammal\").\n\tis_a(\"zebra\", \"odd-toed ungulate\").\n\tis_a(\"spoon\", \"eating utensil\").\n\tis_a(\"lemon\", \"citrus fruit\").\n\tis_a(\"lime\", \"citrus fruit\").\n\tis_a(\"soap\", \"toiletry\").\n\tis_a(\"dog\", \"mammal\").\n", "is_a(\"dog\", \"carnivora\").\n\tis_a(\"giraffe\", \"even-toed ungulate\").\n\tis_a(\"giraffe\", \"mammal\").\n\tis_a(\"giraffe\", \"herbivore\").\n\tis_a(\"elephant\", \"mammal\").\n\tis_a(\"bear\", \"carnivora\").\n\tis_a(\"duck\", \"bird\").\n\tis_a(\"sheep\", \"mammal\").\n\tis_a(\"sheep\", \"even-toed ungulate\").\n\tis_a(\"computer\", \"electronic device\").\n", "is_a(\"fork\", \"eating utensil\").\n\tis_a(\"pen\", \"writing implement\").\n\tis_a(\"speaker\", \"electronic device\").\n\tis_a(\"camera\", \"electronic device\").\n\tis_a(\"bus\", \"public transports\").\n\tis_a(\"tiger\", \"feline\").\n\tis_a(\"painting\", \"artwork\").\n\tis_a(\"headphones\", \"electronic device\").\n\tis_a(\"ostrich\", \"bird\").\n\tis_a(\"projector\", \"electronic device\").\n", "is_a(\"mousepad\", \"electronic device\").\n\tis_a(\"monitor\", \"electronic device\").\n\tis_a(\"orange\", \"citrus fruit\").\n\tis_a(\"keyboard\", \"electronic device\").\n\tis_a(\"airplane\", \"public transports\").\n\tis_a(\"airplane\", \"aircraft\").\n\tis_a(\"pot\", \"kitchen utensil\").\n\tis_a(\"microwave\", \"electrical appliance\").\n\tis_a(\"bird\", \"class\").\n\tis_a(\"fox\", \"canidae\").\n", "is_a(\"oven\", \"electrical appliance\").\n\tis_a(\"kettle\", \"kitchen utensil\").\n\tis_a(\"tea pot\", \"kitchen utensil\").\n\tis_a(\"washing machine\", \"electrical appliance\").\n\tis_a(\"donkey\", \"mammal\").\n\tis_a(\"macaroni\", \"main course\").\n\tis_a(\"propeller\", \"aircraft\").\n\tis_a(\"sink\", \"kitchen utensil\").\n\tis_a(\"landscape\", \"artwork\").\n\tis_a(\"blade\", \"kitchen utensil\").\n", "is_a(\"marker\", \"writing implement\").\n\tis_a(\"tv\", \"electronic device\").\n\tis_a(\"radio\", \"electronic device\").\n\tis_a(\"printer\", \"electronic device\").\n\tis_a(\"horse\", \"odd-toed ungulate\").\n\tis_a(\"train\", \"public transports\").\n\tis_a(\"sheep\", \"herbivore\").\n\tis_a(\"knife\", \"eating utensil\").\n\tis_a(\"wii\", \"electronic device\").\n\tis_a(\"guitar\", \"instruments\").\n", "is_a(\"bull\", \"mammal\").\n\tis_a(\"pan\", \"kitchen utensil\").\n\tis_a(\"grapefruit\", \"citrus fruit\").\n\tis_a(\"lizard\", \"reptile\").\n\tis_a(\"jet\", \"aircraft\").\n\tis_a(\"mouse\", \"electronic device\").\n\tis_a(\"toothbrush\", \"toiletry\").\n\tis_a(\"lion\", \"feline\").\n\tis_a(\"lion\", \"carnivora\").\n\tis_a(\"tea kettle\", \"kitchen utensil\").\n", "is_a(\"cake\", \"dessert\").\n\tis_a(\"laptop\", \"electronic device\").\n\tis_a(\"cow\", \"mammal\").\n\tis_a(\"calf\", \"mammal\").\n\tis_a(\"phone\", \"electronic device\").\n\tis_a(\"pencil\", \"writing implement\").\n\tis_a(\"game controller\", \"electronic device\").\n\tis_a(\"pizza\", \"main course\").\n\tis_a(\"pizza\", \"fast food\").\n\tis_a(\"calculator\", \"electronic device\").\n", "is_a(\"ice cream\", \"dessert\").\n\tis_a(\"cookie\", \"dessert\").\n\tis_a(\"grill\", \"kitchen utensil\").\n\tis_a(\"goat\", \"mammal\").\n\tis_a(\"microphone\", \"electronic device\").\n\tis_a(\"polar bear\", \"mammal\").\n\tis_a(\"statue\", \"artwork\").\n\tis_a(\"cow\", \"even-toed ungulate\").\n\tis_a(\"chopsticks\", \"eating utensil\").\n\tis_a(\"cow\", \"herbivore\").\n", "is_a(\"cattle\", \"even-toed ungulate\").\n\tis_a(\"dishwasher\", \"electrical appliance\").\n\tis_a(\"blender\", \"kitchen utensil\").\n\tis_a(\"coffee pot\", \"kitchen utensil\").\n\tis_a(\"cooking pot\", \"kitchen utensil\").\n\tis_a(\"coffee maker\", \"electrical appliance\").\n\tis_a(\"water\", \"liquid\").\n\tis_a(\"dvd\", \"electronic device\").\n\tis_a(\"ship\", \"watercraft\").\n\tis_a(\"fax machine\", \"electronic device\").\n", "is_a(\"lotion\", \"toiletry\").\n\tis_a(\"pepper\", \"condiment\").\n\tis_a(\"noodles\", \"main course\").\n\tis_a(\"bread\", \"staple food\").\n\tis_a(\"cereal\", \"grains\").\n\tis_a(\"drum\", \"instruments\").\n\tis_a(\"graffiti\", \"artwork\").\n\tis_a(\"deer\", \"herbivore\").\n\tis_a(\"deer\", \"even-toed ungulate\").\n\tis_a(\"cutting board\", \"kitchen utensil\").\n", "is_a(\"knife block\", \"kitchen utensil\").\n\tis_a(\"whisk\", \"kitchen utensil\").\n\tis_a(\"burger\", \"fast food\").\n\tis_a(\"burger\", \"main course\").\n\tis_a(\"heater\", \"electrical appliance\").\n\tis_a(\"rolling pin\", \"kitchen utensil\").\n\tis_a(\"dog\", \"canidae\").\n\tis_a(\"cattle\", \"mammal\").\n\tis_a(\"harp\", \"instruments\").\n\tis_a(\"cat\", \"carnivora\").\n", "is_a(\"pigeon\", \"bird\").\n\tis_a(\"juicer\", \"kitchen utensil\").\n\tis_a(\"subway\", \"public transports\").\n\tis_a(\"tiger\", \"mammal\").\n\tis_a(\"penguin\", \"bird\").\n\tis_a(\"pizza oven\", \"electrical appliance\").\n\tis_a(\"pizza cutter\", \"kitchen utensil\").\n\tis_a(\"bear\", \"mammal\").\n\tis_a(\"seagull\", \"bird\").\n\tis_a(\"ladle\", \"kitchen utensil\").\n", "is_a(\"foil\", \"kitchen utensil\").\n\tis_a(\"wii controller\", \"electronic device\").\n\tis_a(\"router\", \"electronic device\").\n\tis_a(\"panda\", \"mammal\").\n\tis_a(\"monkey\", \"mammal\").\n\tis_a(\"spatula\", \"kitchen utensil\").\n\tis_a(\"rabbit\", \"herbivore\").\n\tis_a(\"bunny\", \"mammal\").\n\tis_a(\"mixing bowl\", \"kitchen utensil\").\n\tis_a(\"cola\", \"soft drinks\").\n", "is_a(\"turtle\", \"reptile\").\n\tis_a(\"frying pan\", \"kitchen utensil\").\n\tis_a(\"cheesecake\", \"dessert\").\n\tis_a(\"baking sheet\", \"kitchen utensil\").\n\tis_a(\"dryer\", \"electrical appliance\").\n\tis_a(\"beer\", \"alcoholic beverage\").\n\tis_a(\"calf\", \"herbivore\").\n\tis_a(\"sailboat\", \"watercraft\").\n\tis_a(\"toaster oven\", \"electrical appliance\").\n\tis_a(\"bull\", \"herbivore\").\n", "is_a(\"lion\", \"mammal\").\n\tis_a(\"dvd player\", \"electronic device\").\n\tis_a(\"toothpick\", \"eating utensil\").\n\tis_a(\"squirrel\", \"mammal\").\n\tis_a(\"squirrel\", \"rodent\").\n\tis_a(\"tongs\", \"kitchen utensil\").\n\tis_a(\"moose\", \"even-toed ungulate\").\n\tis_a(\"moose\", \"mammal\").\n\tis_a(\"pasta\", \"main course\").\n\tis_a(\"pony\", \"mammal\").\n", "is_a(\"sword\", \"weapon\").\n\tis_a(\"fox\", \"mammal\").\n\tis_a(\"toaster\", \"electrical appliance\").\n\tis_a(\"eagle\", \"bird\").\n\tis_a(\"lipstick\", \"cosmetic\").\n\tis_a(\"calf\", \"even-toed ungulate\").\n\tis_a(\"rifle\", \"weapon\").\n\tis_a(\"piano\", \"instruments\").\n\tis_a(\"cupcake\", \"dessert\").\n\tis_a(\"feline\", \"family\").\n", "is_a(\"chicken\", \"bird\").\n\tis_a(\"pie\", \"dessert\").\n\tis_a(\"dish drainer\", \"kitchen utensil\").\n\tis_a(\"lamb\", \"even-toed ungulate\").\n\tis_a(\"lamb\", \"mammal\").\n\tis_a(\"goat\", \"even-toed ungulate\").\n\tis_a(\"goat\", \"herbivore\").\n\tis_a(\"lamb\", \"herbivore\").\n\tis_a(\"puppy\", \"canidae\").\n\tis_a(\"basil\", \"condiment\").\n", "is_a(\"fries\", \"fast food\").\n\tis_a(\"grater\", \"kitchen utensil\").\n\tis_a(\"sauce\", \"condiment\").\n\tis_a(\"kitten\", \"feline\").\n\tis_a(\"hippo\", \"even-toed ungulate\").\n\tis_a(\"hippo\", \"mammal\").\n\tis_a(\"bass\", \"instruments\").\n\tis_a(\"antelope\", \"herbivore\").\n\tis_a(\"polar bear\", \"carnivora\").\n\tis_a(\"corn\", \"grains\").\n", "is_a(\"corn\", \"staple food\").\n\tis_a(\"brownie\", \"dessert\").\n\tis_a(\"cash\", \"currency\").\n\tis_a(\"cereal\", \"staple food\").\n\tis_a(\"owl\", \"bird\").\n\tis_a(\"flamingo\", \"bird\").\n\tis_a(\"monkey\", \"primate\").\n\tis_a(\"bomb\", \"weapon\").\n\tis_a(\"pizza pan\", \"kitchen utensil\").\n\tis_a(\"bull\", \"even-toed ungulate\").\n", "is_a(\"rice cooker\", \"electrical appliance\").\n\tis_a(\"sushi\", \"main course\").\n\tis_a(\"vacuum\", \"electrical appliance\").\n\tis_a(\"toothpaste\", \"toiletry\").\n\tis_a(\"frog\", \"amphibian\").\n\tis_a(\"antelope\", \"mammal\").\n\tis_a(\"hen\", \"bird\").\n\tis_a(\"puppy\", \"mammal\").\n\tis_a(\"puppy\", \"carnivora\").\n\tis_a(\"double decker\", \"public transports\").\n", "is_a(\"alligator\", \"reptile\").\n\tis_a(\"deer\", \"mammal\").\n\tis_a(\"cinnamon\", \"condiment\").\n\tis_a(\"tongs\", \"eating utensil\").\n\tis_a(\"kitten\", \"carnivora\").\n\tis_a(\"kitten\", \"mammal\").\n\tis_a(\"cattle\", \"herbivore\").\n\tis_a(\"antelope\", \"even-toed ungulate\").\n\tis_a(\"parrot\", \"bird\").\n\tis_a(\"ipod\", \"electronic device\").\n", "is_a(\"dolphin\", \"even-toed ungulate\").\n\tis_a(\"leopard\", \"mammal\").\n\tis_a(\"fox\", \"carnivora\").\n\tis_a(\"gun\", \"weapon\").\n\tis_a(\"canoe\", \"watercraft\").\n\tis_a(\"food processor\", \"electrical appliance\").\n\tis_a(\"hamburger\", \"main course\").\n\tis_a(\"hamburger\", \"fast food\").\n\tis_a(\"console\", \"electronic device\").\n\tis_a(\"xbox controller\", \"electronic device\").\n", "is_a(\"video camera\", \"electronic device\").\n\tis_a(\"mammal\", \"class\").\n\tis_a(\"helicopter\", \"aircraft\").\n\tis_a(\"spear\", \"weapon\").\n\tis_a(\"hummingbird\", \"bird\").\n\tis_a(\"goose\", \"bird\").\n\tis_a(\"rodent\", \"order\").\n\tis_a(\"shampoo\", \"toiletry\").\n\tis_a(\"violin\", \"instruments\").\n\tis_a(\"coin\", \"currency\").\n", "is_a(\"dove\", \"bird\").\n\tis_a(\"champagne\", \"alcoholic beverage\").\n\tis_a(\"camel\", \"even-toed ungulate\").\n\tis_a(\"bison\", \"even-toed ungulate\").\n\tis_a(\"bison\", \"herbivore\").\n\tis_a(\"trumpet\", \"instruments\").\n\tis_a(\"rabbit\", \"mammal\").\n\tis_a(\"kangaroo\", \"mammal\").\n\tis_a(\"wolf\", \"carnivora\").\n\tis_a(\"guacamole\", \"condiment\").\n", "is_a(\"wok\", \"kitchen utensil\").\n\tis_a(\"tiger\", \"carnivora\").\n\tis_a(\"panda bear\", \"mammal\").\n\tis_a(\"syrup\", \"condiment\").\n\tis_a(\"moose\", \"herbivore\").\n\tis_a(\"hawk\", \"bird\").\n\tis_a(\"beaver\", \"mammal\").\n\tis_a(\"liquor\", \"alcoholic beverage\").\n\tis_a(\"shaving cream\", \"toiletry\").\n\tis_a(\"utensil holder\", \"kitchen utensil\").\n", "is_a(\"cake pan\", \"kitchen utensil\").\n\tis_a(\"seal\", \"mammal\").\n\tis_a(\"seal\", \"carnivora\").\n\tis_a(\"poodle\", \"mammal\").\n\tis_a(\"raccoon\", \"carnivora\").\n\tis_a(\"ice maker\", \"electrical appliance\").\n\tis_a(\"hamster\", \"rodent\").\n\tis_a(\"rat\", \"mammal\").\n\tis_a(\"crow\", \"bird\").\n\tis_a(\"swan\", \"bird\").\n", "is_a(\"whale\", \"even-toed ungulate\").\n\tis_a(\"garlic\", \"condiment\").\n\tis_a(\"wolf\", \"mammal\").\n\tis_a(\"mustard\", \"condiment\").\n\tis_a(\"deodorant\", \"toiletry\").\n\tis_a(\"hand dryer\", \"electrical appliance\").\n\tis_a(\"wheat\", \"staple food\").\n\tis_a(\"wheat\", \"grains\").\n\tis_a(\"pudding\", \"dessert\").\n\tis_a(\"accordion\", \"instruments\").\n", "is_a(\"peacock\", \"bird\").\n\tis_a(\"crayon\", \"writing implement\").\n\tis_a(\"gorilla\", \"primate\").\n\tis_a(\"cheetah\", \"carnivora\").\n\tis_a(\"cheetah\", \"mammal\").\n\tis_a(\"leopard\", \"feline\").\n\tis_a(\"baking pan\", \"kitchen utensil\").\n\tis_a(\"mascara\", \"cosmetic\").\n\tis_a(\"gorilla\", \"mammal\").\n\tis_a(\"fingernail polish\", \"cosmetic\").\n", "is_a(\"rhino\", \"odd-toed ungulate\").\n\tis_a(\"hamster\", \"mammal\").\n\tis_a(\"rat\", \"rodent\").\n\tis_a(\"sugar\", \"condiment\").\n\tis_a(\"finch\", \"bird\").\n\tis_a(\"skewer\", \"eating utensil\").\n\tis_a(\"cheeseburger\", \"main course\").\n\tis_a(\"snake\", \"reptile\").\n\tis_a(\"bald eagle\", \"bird\").\n\tis_a(\"cheetah\", \"feline\").\n", "is_a(\"bison\", \"mammal\").\n\tis_a(\"rhino\", \"mammal\").\n\tis_a(\"camel\", \"herbivore\").\n\tis_a(\"camel\", \"mammal\").\n\tis_a(\"ketchup\", \"condiment\").\n\tis_a(\"ape\", \"primate\").\n\tis_a(\"ape\", \"mammal\").\n\tis_a(\"gravy\", \"condiment\").\n\tis_a(\"barley\", \"grains\").\n\tis_a(\"butter\", \"condiment\").\n", "is_a(\"copier\", \"electronic device\").\n\tis_a(\"robin\", \"bird\").\n\tis_a(\"reptile\", \"class\").\n\tis_a(\"beaver\", \"rodent\").\n\tis_a(\"perfume\", \"cosmetic\").\n\tis_a(\"eyeshadow\", \"cosmetic\").\n\tis_a(\"wolf\", \"canidae\").\n\tis_a(\"badger\", \"mammal\").\n\tis_a(\"leopard\", \"carnivora\").\n\tis_a(\"eyeliner\", \"cosmetic\").\n", "is_a(\"people\", \"mammal\").\n\tis_a(\"salt\", \"condiment\").\n\tis_a(\"grey cat\", \"mammal\").\n\tis_a(\"alpaca\", \"even-toed ungulate\").\n\tis_a(\"raccoon\", \"mammal\").\n\tis_a(\"perfume\", \"toiletry\").\n\tis_a(\"caramel\", \"condiment\").\n\tis_a(\"clarinet\", \"instruments\").\n\tis_a(\"otter\", \"carnivora\").\n\tis_a(\"athlete\", \"person\").\n", "is_a(\"occupation\", \"person\").\n\tis_a(\"baseball position\", \"person\").\n\tis_a(\"tennis player\", \"athlete\").\n\tis_a(\"baseball player\", \"athlete\").\n\tis_a(\"soccer player\", \"athlete\").\n\tis_a(\"basketball player\", \"athlete\").\n\tis_a(\"frisbee player\", \"athlete\").\n\tis_a(\"football player\", \"athlete\").\n\tis_a(\"volleyball player\", \"athlete\").\n\tis_a(\"billiards player\", \"athlete\").\n", "is_a(\"hockey player\", \"athlete\").\n\tis_a(\"golfer\", \"athlete\").\n\tis_a(\"surfer\", \"athlete\").\n\tis_a(\"biker\", \"athlete\").\n\tis_a(\"swimmer\", \"athlete\").\n\tis_a(\"runner\", \"athlete\").\n\tis_a(\"jogger\", \"athlete\").\n\tis_a(\"skier\", \"athlete\").\n\tis_a(\"skateboarder\", \"athlete\").\n\tis_a(\"skater\", \"athlete\").\n", "is_a(\"snowboarder\", \"athlete\").\n\tis_a(\"police\", \"occupation\").\n\tis_a(\"teacher\", \"occupation\").\n\tis_a(\"student\", \"occupation\").\n\tis_a(\"pilot\", \"occupation\").\n\tis_a(\"cowboy\", \"occupation\").\n\tis_a(\"soldier\", \"occupation\").\n\tis_a(\"fisherman\", \"occupation\").\n\tis_a(\"worker\", \"occupation\").\n\tis_a(\"photographer\", \"occupation\").\n", "is_a(\"performer\", \"occupation\").\n\tis_a(\"farmer\", \"occupation\").\n\tis_a(\"policeman\", \"occupation\").\n\tis_a(\"officer\", \"occupation\").\n\tis_a(\"vendor\", \"occupation\").\n\tis_a(\"shopper\", \"occupation\").\n\tis_a(\"bus driver\", \"occupation\").\n\tis_a(\"driver\", \"occupation\").\n\tis_a(\"jockey\", \"occupation\").\n\tis_a(\"engineer\", \"occupation\").\n", "is_a(\"doctor\", \"occupation\").\n\tis_a(\"chef\", \"occupation\").\n\tis_a(\"baker\", \"occupation\").\n\tis_a(\"bartender\", \"occupation\").\n\tis_a(\"waiter\", \"occupation\").\n\tis_a(\"waitress\", \"occupation\").\n\tis_a(\"customer\", \"occupation\").\n\tis_a(\"player\", \"occupation\").\n\tis_a(\"athlete\", \"occupation\").\n\tis_a(\"coach\", \"occupation\").\n", "is_a(\"actor\", \"occupation\").\n\tis_a(\"batter\", \"baseball position\").\n\tis_a(\"catcher\", \"baseball position\").\n\tis_a(\"pitcher\", \"baseball position\").\n\tis_a(\"umpire\", \"baseball position\").\n\tis_a(\"santa\", \"character\").\n\tis_a(\"mickey mouse\", \"character\").\n\tis_a(\"snoopy\", \"character\").\n\tis_a(\"pikachu\", \"character\").\n\tis_a(\"leg\", \"part of body\").\n", "is_a(\"tail\", \"part of body\").\n\tis_a(\"lap\", \"part of body\").\n\tis_a(\"neck\", \"part of body\").\n\tis_a(\"foot\", \"part of body\").\n\tis_a(\"face\", \"part of body\").\n\tis_a(\"arm\", \"part of body\").\n\tis_a(\"hand\", \"part of body\").\n\tis_a(\"wrist\", \"part of body\").\n\tis_a(\"shoulder\", \"part of body\").\n\tis_a(\"head\", \"part of body\").\n", "is_a(\"horn\", \"part of body\").\n\tis_a(\"tusk\", \"part of body\").\n\tis_a(\"racing\", \"sport\").\n\tis_a(\"baseball\", \"sport\").\n\tis_a(\"soccer\", \"sport\").\n\tis_a(\"skiing\", \"sport\").\n\tis_a(\"basketball\", \"sport\").\n\tis_a(\"polo\", \"sport\").\n\tis_a(\"tennis\", \"sport\").\n\tis_a(\"surfing\", \"sport\").\n", "is_a(\"riding\", \"sport\").\n\tis_a(\"skateboarding\", \"sport\").\n\tis_a(\"skate\", \"sport\").\n\tis_a(\"swimming\", \"sport\").\n\tis_a(\"snowboarding\", \"sport\").\n\tis_a(\"christmas\", \"event\").\n\tis_a(\"thanksgiving\", \"event\").\n\tis_a(\"wedding\", \"event\").\n\tis_a(\"birthday\", \"event\").\n\tis_a(\"halloween\", \"event\").\n", "is_a(\"party\", \"event\").\n\tis_a(\"cat\", \"animal\").\n\tis_a(\"kitten\", \"animal\").\n\tis_a(\"dog\", \"animal\").\n\tis_a(\"puppy\", \"animal\").\n\tis_a(\"poodle\", \"animal\").\n\tis_a(\"bull\", \"animal\").\n\tis_a(\"cow\", \"animal\").\n\tis_a(\"cattle\", \"animal\").\n\tis_a(\"bison\", \"animal\").\n", "is_a(\"calf\", \"animal\").\n\tis_a(\"pig\", \"animal\").\n\tis_a(\"ape\", \"animal\").\n\tis_a(\"monkey\", \"animal\").\n\tis_a(\"gorilla\", \"animal\").\n\tis_a(\"rat\", \"animal\").\n\tis_a(\"squirrel\", \"animal\").\n\tis_a(\"hamster\", \"animal\").\n\tis_a(\"deer\", \"animal\").\n\tis_a(\"moose\", \"animal\").\n", "is_a(\"alpaca\", \"animal\").\n\tis_a(\"elephant\", \"animal\").\n\tis_a(\"goat\", \"animal\").\n\tis_a(\"sheep\", \"animal\").\n\tis_a(\"lamb\", \"animal\").\n\tis_a(\"antelope\", \"animal\").\n\tis_a(\"rhino\", \"animal\").\n\tis_a(\"hippo\", \"animal\").\n\tis_a(\"giraffe\", \"animal\").\n\tis_a(\"zebra\", \"animal\").\n", "is_a(\"horse\", \"animal\").\n\tis_a(\"pony\", \"animal\").\n\tis_a(\"donkey\", \"animal\").\n\tis_a(\"camel\", \"animal\").\n\tis_a(\"panda\", \"animal\").\n\tis_a(\"panda bear\", \"animal\").\n\tis_a(\"bear\", \"animal\").\n\tis_a(\"polar bear\", \"animal\").\n\tis_a(\"seal\", \"animal\").\n\tis_a(\"fox\", \"animal\").\n", "is_a(\"raccoon\", \"animal\").\n\tis_a(\"tiger\", \"animal\").\n\tis_a(\"wolf\", \"animal\").\n\tis_a(\"lion\", \"animal\").\n\tis_a(\"leopard\", \"animal\").\n\tis_a(\"cheetah\", \"animal\").\n\tis_a(\"badger\", \"animal\").\n\tis_a(\"rabbit\", \"animal\").\n\tis_a(\"bunny\", \"animal\").\n\tis_a(\"beaver\", \"animal\").\n", "is_a(\"kangaroo\", \"animal\").\n\tis_a(\"dinosaur\", \"animal\").\n\tis_a(\"dragon\", \"animal\").\n\tis_a(\"fish\", \"animal\").\n\tis_a(\"whale\", \"animal\").\n\tis_a(\"dolphin\", \"animal\").\n\tis_a(\"crab\", \"animal\").\n\tis_a(\"shark\", \"animal\").\n\tis_a(\"octopus\", \"animal\").\n\tis_a(\"lobster\", \"animal\").\n", "is_a(\"oyster\", \"animal\").\n\tis_a(\"butterfly\", \"animal\").\n\tis_a(\"bee\", \"animal\").\n\tis_a(\"fly\", \"animal\").\n\tis_a(\"ant\", \"animal\").\n\tis_a(\"firefly\", \"animal\").\n\tis_a(\"snail\", \"animal\").\n\tis_a(\"spider\", \"animal\").\n\tis_a(\"bird\", \"animal\").\n\tis_a(\"penguin\", \"animal\").\n", "is_a(\"pigeon\", \"animal\").\n\tis_a(\"seagull\", \"animal\").\n\tis_a(\"finch\", \"animal\").\n\tis_a(\"robin\", \"animal\").\n\tis_a(\"ostrich\", \"animal\").\n\tis_a(\"goose\", \"animal\").\n\tis_a(\"owl\", \"animal\").\n\tis_a(\"duck\", \"animal\").\n\tis_a(\"hawk\", \"animal\").\n\tis_a(\"eagle\", \"animal\").\n", "is_a(\"swan\", \"animal\").\n\tis_a(\"chicken\", \"animal\").\n\tis_a(\"hen\", \"animal\").\n\tis_a(\"hummingbird\", \"animal\").\n\tis_a(\"parrot\", \"animal\").\n\tis_a(\"crow\", \"animal\").\n\tis_a(\"flamingo\", \"animal\").\n\tis_a(\"peacock\", \"animal\").\n\tis_a(\"bald eagle\", \"animal\").\n\tis_a(\"dove\", \"animal\").\n", "is_a(\"snake\", \"animal\").\n\tis_a(\"lizard\", \"animal\").\n\tis_a(\"alligator\", \"animal\").\n\tis_a(\"turtle\", \"animal\").\n\tis_a(\"frog\", \"animal\").\n\tis_a(\"butterfly\", \"insect\").\n\tis_a(\"bee\", \"insect\").\n\tis_a(\"fly\", \"insect\").\n\tis_a(\"ant\", \"insect\").\n\tis_a(\"firefly\", \"insect\").\n", "is_a(\"swan\", \"aquatic bird\").\n\tis_a(\"penguin\", \"aquatic bird\").\n\tis_a(\"duck\", \"aquatic bird\").\n\tis_a(\"goose\", \"aquatic bird\").\n\tis_a(\"seagull\", \"aquatic bird\").\n\tis_a(\"flamingo\", \"aquatic bird\").\n\tis_a(\"tree\", \"plant\").\n\tis_a(\"flower\", \"plant\").\n\tis_a(\"bamboo\", \"tree\").\n\tis_a(\"palm tree\", \"tree\").\n", "is_a(\"pine\", \"tree\").\n\tis_a(\"pine tree\", \"tree\").\n\tis_a(\"oak tree\", \"tree\").\n\tis_a(\"christmas tree\", \"tree\").\n\tis_a(\"flower\", \"flower\").\n\tis_a(\"sunflower\", \"flower\").\n\tis_a(\"daisy\", \"flower\").\n\tis_a(\"orchid\", \"flower\").\n\tis_a(\"seaweed\", \"flower\").\n\tis_a(\"blossom\", \"flower\").\n", "is_a(\"lily\", \"flower\").\n\tis_a(\"rose\", \"flower\").\n\tis_a(\"sweater\", \"tops\").\n\tis_a(\"pullover\", \"tops\").\n\tis_a(\"blouse\", \"tops\").\n\tis_a(\"blazer\", \"tops\").\n\tis_a(\"cardigan\", \"tops\").\n\tis_a(\"halter\", \"tops\").\n\tis_a(\"parka\", \"tops\").\n\tis_a(\"turtleneck\", \"tops\").\n", "is_a(\"hoodie\", \"tops\").\n\tis_a(\"bikini\", \"tops\").\n\tis_a(\"tank top\", \"tops\").\n\tis_a(\"vest\", \"tops\").\n\tis_a(\"jersey\", \"tops\").\n\tis_a(\"t shirt\", \"tops\").\n\tis_a(\"polo shirt\", \"tops\").\n\tis_a(\"dress shirt\", \"tops\").\n\tis_a(\"undershirt\", \"tops\").\n\tis_a(\"shirt\", \"tops\").\n", "is_a(\"sweatshirt\", \"tops\").\n\tis_a(\"jacket\", \"tops\").\n\tis_a(\"jeans\", \"pants\").\n\tis_a(\"khaki\", \"pants\").\n\tis_a(\"denim\", \"pants\").\n\tis_a(\"jogger\", \"pants\").\n\tis_a(\"capris\", \"pants\").\n\tis_a(\"trunks\", \"pants\").\n\tis_a(\"leggings\", \"pants\").\n\tis_a(\"trousers\", \"pants\").\n", "is_a(\"shorts\", \"pants\").\n\tis_a(\"snow pants\", \"pants\").\n\tis_a(\"shoe\", \"shoes\").\n\tis_a(\"heel\", \"shoes\").\n\tis_a(\"tennis shoe\", \"shoes\").\n\tis_a(\"boot\", \"shoes\").\n\tis_a(\"sneaker\", \"shoes\").\n\tis_a(\"sandal\", \"shoes\").\n\tis_a(\"cleat\", \"shoes\").\n\tis_a(\"slipper\", \"shoes\").\n", "is_a(\"flip flop\", \"shoes\").\n\tis_a(\"flipper\", \"shoes\").\n\tis_a(\"nightdress\", \"dress\").\n\tis_a(\"kimono\", \"dress\").\n\tis_a(\"onesie\", \"dress\").\n\tis_a(\"sundress\", \"dress\").\n\tis_a(\"wedding dress\", \"dress\").\n\tis_a(\"strapless\", \"dress\").\n\tis_a(\"pants\", \"clothing\").\n\tis_a(\"tops\", \"clothing\").\n", "is_a(\"dress\", \"clothing\").\n\tis_a(\"skirt\", \"clothing\").\n\tis_a(\"coat\", \"clothing\").\n\tis_a(\"suit\", \"clothing\").\n\tis_a(\"jumpsuit\", \"clothing\").\n\tis_a(\"gown\", \"clothing\").\n\tis_a(\"robe\", \"clothing\").\n\tis_a(\"bathrobe\", \"clothing\").\n\tis_a(\"socks\", \"clothing\").\n\tis_a(\"helmet\", \"hat\").\n", "is_a(\"cap\", \"hat\").\n\tis_a(\"beanie\", \"hat\").\n\tis_a(\"visor\", \"hat\").\n\tis_a(\"hood\", \"hat\").\n\tis_a(\"bandana\", \"hat\").\n\tis_a(\"baseball cap\", \"hat\").\n\tis_a(\"headband\", \"hat\").\n\tis_a(\"cowboy hat\", \"hat\").\n\tis_a(\"chef hat\", \"hat\").\n\tis_a(\"mitt\", \"glove\").\n", "is_a(\"baseball glove\", \"glove\").\n\tis_a(\"baseball mitt\", \"glove\").\n\tis_a(\"mitten\", \"glove\").\n\tis_a(\"necklace\", \"jewelry\").\n\tis_a(\"ring\", \"jewelry\").\n\tis_a(\"earring\", \"jewelry\").\n\tis_a(\"bracelet\", \"jewelry\").\n\tis_a(\"shoes\", \"accessory\").\n\tis_a(\"scarf\", \"accessory\").\n\tis_a(\"tie\", \"accessory\").\n", "is_a(\"veil\", \"accessory\").\n\tis_a(\"mask\", \"accessory\").\n\tis_a(\"apron\", \"accessory\").\n\tis_a(\"poncho\", \"accessory\").\n\tis_a(\"cape\", \"accessory\").\n\tis_a(\"belt\", \"accessory\").\n\tis_a(\"handkerchief\", \"accessory\").\n\tis_a(\"hairband\", \"accessory\").\n\tis_a(\"life jacket\", \"accessory\").\n\tis_a(\"lanyard\", \"accessory\").\n", "is_a(\"crown\", \"accessory\").\n\tis_a(\"garland\", \"accessory\").\n\tis_a(\"wristband\", \"accessory\").\n\tis_a(\"watch\", \"accessory\").\n\tis_a(\"wristwatch\", \"accessory\").\n\tis_a(\"pocket watch\", \"accessory\").\n\tis_a(\"hat\", \"accessory\").\n\tis_a(\"glove\", \"accessory\").\n\tis_a(\"jewelry\", \"accessory\").\n\tis_a(\"glasses\", \"accessory\").\n", "is_a(\"eye glasses\", \"glasses\").\n\tis_a(\"sunglasses\", \"glasses\").\n\tis_a(\"goggles\", \"glasses\").\n\tis_a(\"shrimp\", \"meat\").\n\tis_a(\"ribs\", \"meat\").\n\tis_a(\"steak\", \"meat\").\n\tis_a(\"beef\", \"meat\").\n\tis_a(\"egg\", \"meat\").\n\tis_a(\"egg shell\", \"meat\").\n\tis_a(\"chicken\", \"meat\").\n", "is_a(\"chicken breast\", \"meat\").\n\tis_a(\"pepperoni\", \"meat\").\n\tis_a(\"bacon\", \"meat\").\n\tis_a(\"ham\", \"meat\").\n\tis_a(\"sausage\", \"meat\").\n\tis_a(\"pork\", \"meat\").\n\tis_a(\"bacon\", \"pork\").\n\tis_a(\"ham\", \"pork\").\n\tis_a(\"pepperoni\", \"sausage\").\n\tis_a(\"salami\", \"sausage\").\n", "is_a(\"apple\", \"fruit\").\n\tis_a(\"pineapple\", \"fruit\").\n\tis_a(\"banana\", \"fruit\").\n\tis_a(\"olives\", \"fruit\").\n\tis_a(\"orange\", \"fruit\").\n\tis_a(\"grapes\", \"fruit\").\n\tis_a(\"strawberry\", \"fruit\").\n\tis_a(\"cherry\", \"fruit\").\n\tis_a(\"lemon\", \"fruit\").\n\tis_a(\"lime\", \"fruit\").\n", "is_a(\"mango\", \"fruit\").\n\tis_a(\"peach\", \"fruit\").\n\tis_a(\"tangerine\", \"fruit\").\n\tis_a(\"grape\", \"fruit\").\n\tis_a(\"kiwi\", \"fruit\").\n\tis_a(\"pear\", \"fruit\").\n\tis_a(\"watermelon\", \"fruit\").\n\tis_a(\"berry\", \"fruit\").\n\tis_a(\"blueberry\", \"fruit\").\n\tis_a(\"raspberry\", \"fruit\").\n", "is_a(\"cranberry\", \"fruit\").\n\tis_a(\"raisin\", \"fruit\").\n\tis_a(\"gourd\", \"fruit\").\n\tis_a(\"grapefruit\", \"fruit\").\n\tis_a(\"melon\", \"fruit\").\n\tis_a(\"pomegranate\", \"fruit\").\n\tis_a(\"papaya\", \"fruit\").\n\tis_a(\"coconut\", \"fruit\").\n\tis_a(\"citrus fruit\", \"fruit\").\n\tis_a(\"tangerine\", \"citrus fruit\").\n", "is_a(\"onion\", \"vegetable\").\n\tis_a(\"pumpkin\", \"vegetable\").\n\tis_a(\"spinach\", \"vegetable\").\n\tis_a(\"broccoli\", \"vegetable\").\n\tis_a(\"mushroom\", \"vegetable\").\n\tis_a(\"carrot\", \"vegetable\").\n\tis_a(\"cabbage\", \"vegetable\").\n\tis_a(\"potato\", \"vegetable\").\n\tis_a(\"lettuce\", \"vegetable\").\n\tis_a(\"tomato\", \"vegetable\").\n", "is_a(\"beans\", \"vegetable\").\n\tis_a(\"squash\", \"vegetable\").\n\tis_a(\"cucumber\", \"vegetable\").\n\tis_a(\"eggplant\", \"vegetable\").\n\tis_a(\"celery\", \"vegetable\").\n\tis_a(\"pepper\", \"vegetable\").\n\tis_a(\"chili\", \"vegetable\").\n\tis_a(\"parsley\", \"vegetable\").\n\tis_a(\"sweet potato\", \"vegetable\").\n\tis_a(\"olive\", \"vegetable\").\n", "is_a(\"zucchini\", \"vegetable\").\n\tis_a(\"artichoke\", \"vegetable\").\n\tis_a(\"cauliflower\", \"vegetable\").\n\tis_a(\"avocado\", \"vegetable\").\n\tis_a(\"herb\", \"vegetable\").\n\tis_a(\"beet\", \"vegetable\").\n\tis_a(\"pea\", \"vegetable\").\n\tis_a(\"nut\", \"nut\").\n\tis_a(\"walnut\", \"nut\").\n\tis_a(\"pecan\", \"nut\").\n", "is_a(\"peanut\", \"nut\").\n\tis_a(\"pistachio\", \"nut\").\n\tis_a(\"almond\", \"nut\").\n\tis_a(\"dip\", \"condiment\").\n\tis_a(\"pesto\", \"condiment\").\n\tis_a(\"hummus\", \"condiment\").\n\tis_a(\"peanut butter\", \"condiment\").\n\tis_a(\"ginger\", \"condiment\").\n\tis_a(\"toast\", \"breakfast food\").\n\tis_a(\"cereal\", \"breakfast food\").\n", "is_a(\"doughnut\", \"breakfast food\").\n\tis_a(\"waffle\", \"breakfast food\").\n\tis_a(\"egg\", \"breakfast food\").\n\tis_a(\"pancake\", \"breakfast food\").\n\tis_a(\"beans\", \"side dishes\").\n\tis_a(\"broccoli\", \"side dishes\").\n\tis_a(\"potato\", \"side dishes\").\n\tis_a(\"salad\", \"side dishes\").\n\tis_a(\"cabbage\", \"side dishes\").\n\tis_a(\"squash\", \"side dishes\").\n", "is_a(\"mushroom\", \"side dishes\").\n\tis_a(\"fries\", \"side dishes\").\n\tis_a(\"maize\", \"staple food\").\n\tis_a(\"millet\", \"staple food\").\n\tis_a(\"sorghum\", \"staple food\").\n\tis_a(\"rice\", \"soft food\").\n\tis_a(\"ice cream\", \"soft food\").\n\tis_a(\"chocolate\", \"soft food\").\n\tis_a(\"cake\", \"soft food\").\n\tis_a(\"cupcake\", \"soft food\").\n", "is_a(\"cheesecake\", \"soft food\").\n\tis_a(\"pie\", \"soft food\").\n\tis_a(\"pudding\", \"soft food\").\n\tis_a(\"sauce\", \"soft food\").\n\tis_a(\"dip\", \"soft food\").\n\tis_a(\"sugar\", \"soft food\").\n\tis_a(\"caramel\", \"soft food\").\n\tis_a(\"ketchup\", \"soft food\").\n\tis_a(\"pesto\", \"soft food\").\n\tis_a(\"gravy\", \"soft food\").\n", "is_a(\"guacamole\", \"soft food\").\n\tis_a(\"hummus\", \"soft food\").\n\tis_a(\"peanut butter\", \"soft food\").\n\tis_a(\"butter\", \"soft food\").\n\tis_a(\"syrup\", \"soft food\").\n\tis_a(\"mustard\", \"soft food\").\n\tis_a(\"meat\", \"solid food\").\n\tis_a(\"side dishes\", \"solid food\").\n\tis_a(\"fruit\", \"solid food\").\n\tis_a(\"main course\", \"solid food\").\n", "is_a(\"vegetable\", \"solid food\").\n\tis_a(\"pizza\", \"food\").\n\tis_a(\"sandwich\", \"food\").\n\tis_a(\"hot dog\", \"food\").\n\tis_a(\"noodles\", \"food\").\n\tis_a(\"pasta\", \"food\").\n\tis_a(\"donut\", \"food\").\n\tis_a(\"cupcake\", \"food\").\n\tis_a(\"bread\", \"food\").\n\tis_a(\"rice\", \"food\").\n", "is_a(\"cereal\", \"food\").\n\tis_a(\"chips\", \"food\").\n\tis_a(\"bun\", \"food\").\n\tis_a(\"cake\", \"food\").\n\tis_a(\"doughnut\", \"food\").\n\tis_a(\"fries\", \"food\").\n\tis_a(\"burger\", \"food\").\n\tis_a(\"hamburger\", \"food\").\n\tis_a(\"porridge\", \"food\").\n\tis_a(\"pie\", \"food\").\n", "is_a(\"vegetable\", \"food\").\n\tis_a(\"nut\", \"food\").\n\tis_a(\"meat\", \"food\").\n\tis_a(\"fruit\", \"food\").\n\tis_a(\"grains\", \"food\").\n\tis_a(\"side dishes\", \"food\").\n\tis_a(\"dessert\", \"food\").\n\tis_a(\"main course\", \"food\").\n\tis_a(\"breakfast food\", \"food\").\n\tis_a(\"milk\", \"drinks\").\n", "is_a(\"juice\", \"drinks\").\n\tis_a(\"soda\", \"drinks\").\n\tis_a(\"cola\", \"drinks\").\n\tis_a(\"cappuccino\", \"drinks\").\n\tis_a(\"milkshake\", \"drinks\").\n\tis_a(\"lemonade\", \"drinks\").\n\tis_a(\"liquor\", \"drinks\").\n\tis_a(\"alcohol\", \"drinks\").\n\tis_a(\"beer\", \"drinks\").\n\tis_a(\"wine\", \"drinks\").\n", "is_a(\"champagne\", \"drinks\").\n\tis_a(\"coffee\", \"drinks\").\n\tis_a(\"tea\", \"drinks\").\n\tis_a(\"water\", \"drinks\").\n\tis_a(\"soft drinks\", \"drinks\").\n\tis_a(\"alcoholic beverage\", \"drinks\").\n\tis_a(\"milk\", \"beverage\").\n\tis_a(\"juice\", \"beverage\").\n\tis_a(\"soda\", \"beverage\").\n\tis_a(\"cola\", \"beverage\").\n", "is_a(\"cappuccino\", \"beverage\").\n\tis_a(\"milkshake\", \"beverage\").\n\tis_a(\"lemonade\", \"beverage\").\n\tis_a(\"liquor\", \"beverage\").\n\tis_a(\"alcohol\", \"beverage\").\n\tis_a(\"beer\", \"beverage\").\n\tis_a(\"wine\", \"beverage\").\n\tis_a(\"champagne\", \"beverage\").\n\tis_a(\"coffee\", \"beverage\").\n\tis_a(\"tea\", \"beverage\").\n", "is_a(\"water\", \"beverage\").\n\tis_a(\"shelf\", \"furniture\").\n\tis_a(\"bookshelf\", \"furniture\").\n\tis_a(\"bookcase\", \"furniture\").\n\tis_a(\"drawer\", \"furniture\").\n\tis_a(\"entertainment center\", \"furniture\").\n\tis_a(\"medicine cabinet\", \"furniture\").\n\tis_a(\"table\", \"furniture\").\n\tis_a(\"end table\", \"furniture\").\n\tis_a(\"dining table\", \"furniture\").\n", "is_a(\"picnic table\", \"furniture\").\n\tis_a(\"side table\", \"furniture\").\n\tis_a(\"coffee table\", \"furniture\").\n\tis_a(\"banquet table\", \"furniture\").\n\tis_a(\"desk\", \"furniture\").\n\tis_a(\"computer desk\", \"furniture\").\n\tis_a(\"tv stand\", \"furniture\").\n\tis_a(\"bed\", \"furniture\").\n\tis_a(\"mattress\", \"furniture\").\n\tis_a(\"nightstand\", \"furniture\").\n", "is_a(\"counter\", \"furniture\").\n\tis_a(\"blind\", \"furniture\").\n\tis_a(\"cabinet\", \"furniture\").\n\tis_a(\"wardrobe\", \"furniture\").\n\tis_a(\"chair\", \"furniture\").\n\tis_a(\"armchair\", \"furniture\").\n\tis_a(\"folding chair\", \"furniture\").\n\tis_a(\"beach chair\", \"furniture\").\n\tis_a(\"office chair\", \"furniture\").\n\tis_a(\"recliner\", \"furniture\").\n", "is_a(\"bench\", \"furniture\").\n\tis_a(\"stool\", \"furniture\").\n\tis_a(\"bar stool\", \"furniture\").\n\tis_a(\"seat\", \"furniture\").\n\tis_a(\"couch\", \"furniture\").\n\tis_a(\"sofa\", \"furniture\").\n\tis_a(\"ottoman\", \"furniture\").\n\tis_a(\"closet\", \"furniture\").\n\tis_a(\"dresser\", \"furniture\").\n\tis_a(\"cupboard\", \"furniture\").\n", "is_a(\"lamp\", \"furniture\").\n\tis_a(\"spatula\", \"kitchenware\").\n\tis_a(\"colander\", \"kitchenware\").\n\tis_a(\"tongs\", \"kitchenware\").\n\tis_a(\"blade\", \"kitchenware\").\n\tis_a(\"cutting board\", \"kitchenware\").\n\tis_a(\"foil\", \"kitchenware\").\n\tis_a(\"dishwasher\", \"kitchenware\").\n\tis_a(\"sink\", \"kitchenware\").\n\tis_a(\"microwave\", \"kitchenware\").\n", "is_a(\"blender\", \"kitchenware\").\n\tis_a(\"toaster\", \"kitchenware\").\n\tis_a(\"oven\", \"kitchenware\").\n\tis_a(\"stove\", \"kitchenware\").\n\tis_a(\"grill\", \"kitchenware\").\n\tis_a(\"fridge\", \"kitchenware\").\n\tis_a(\"container\", \"kitchenware\").\n\tis_a(\"pot\", \"kitchenware\").\n\tis_a(\"kettle\", \"kitchenware\").\n\tis_a(\"mixer\", \"kitchenware\").\n", "is_a(\"electrical appliance\", \"home appliance\").\n\tis_a(\"kitchenware\", \"home appliance\").\n\tis_a(\"radiator\", \"home appliance\").\n\tis_a(\"stove\", \"home appliance\").\n\tis_a(\"gas stove\", \"home appliance\").\n\tis_a(\"spoon\", \"tableware\").\n\tis_a(\"knife\", \"tableware\").\n\tis_a(\"fork\", \"tableware\").\n\tis_a(\"chopsticks\", \"tableware\").\n\tis_a(\"tray\", \"tableware\").\n", "is_a(\"pizza tray\", \"tableware\").\n\tis_a(\"placemat\", \"tableware\").\n\tis_a(\"dishes\", \"tableware\").\n\tis_a(\"napkin\", \"tableware\").\n\tis_a(\"plate\", \"tableware\").\n\tis_a(\"saucer\", \"tableware\").\n\tis_a(\"cup\", \"tableware\").\n\tis_a(\"coffee cup\", \"tableware\").\n\tis_a(\"glass\", \"tableware\").\n\tis_a(\"wine glass\", \"tableware\").\n", "is_a(\"water glass\", \"tableware\").\n\tis_a(\"mug\", \"tableware\").\n\tis_a(\"beer mug\", \"tableware\").\n\tis_a(\"coffee mug\", \"tableware\").\n\tis_a(\"bowl\", \"tableware\").\n\tis_a(\"straw\", \"tableware\").\n\tis_a(\"tablecloth\", \"tableware\").\n\tis_a(\"cloth\", \"tableware\").\n\tis_a(\"basket\", \"tableware\").\n\tis_a(\"candle\", \"tableware\").\n", "is_a(\"can\", \"tableware\").\n\tis_a(\"salt shaker\", \"tableware\").\n\tis_a(\"pepper shaker\", \"tableware\").\n\tis_a(\"vessel\", \"tableware\").\n\tis_a(\"vase\", \"tableware\").\n\tis_a(\"eating utensil\", \"utensil\").\n\tis_a(\"kitchen utensil\", \"utensil\").\n\tis_a(\"shampoo bottle\", \"bottle\").\n\tis_a(\"perfume bottle\", \"bottle\").\n\tis_a(\"soap bottle\", \"bottle\").\n", "is_a(\"ketchup bottle\", \"bottle\").\n\tis_a(\"spray bottle\", \"bottle\").\n\tis_a(\"mustard bottle\", \"bottle\").\n\tis_a(\"water bottle\", \"bottle\").\n\tis_a(\"wine bottle\", \"bottle\").\n\tis_a(\"soda bottle\", \"bottle\").\n\tis_a(\"beer bottle\", \"bottle\").\n\tis_a(\"adidas\", \"logo\").\n\tis_a(\"nike\", \"logo\").\n\tis_a(\"apple logo\", \"logo\").\n", "is_a(\"adidas\", \"brand\").\n\tis_a(\"nike\", \"brand\").\n\tis_a(\"laptop\", \"computer\").\n\tis_a(\"bedspread\", \"bedding\").\n\tis_a(\"pillow\", \"bedding\").\n\tis_a(\"duvet\", \"bedding\").\n\tis_a(\"duvet cover\", \"bedding\").\n\tis_a(\"quilt\", \"bedding\").\n\tis_a(\"sheet\", \"bedding\").\n\tis_a(\"blanket\", \"bedding\").\n", "is_a(\"mattress\", \"bedding\").\n\tis_a(\"teddy bear\", \"toy\").\n\tis_a(\"rubber duck\", \"toy\").\n\tis_a(\"lego\", \"toy\").\n\tis_a(\"stuffed bear\", \"toy\").\n\tis_a(\"stuffed dog\", \"toy\").\n\tis_a(\"stuffed animal\", \"toy\").\n\tis_a(\"toy car\", \"toy\").\n\tis_a(\"balloon\", \"toy\").\n\tis_a(\"doll\", \"toy\").\n", "is_a(\"kite\", \"toy\").\n\tis_a(\"frisbee\", \"toy\").\n\tis_a(\"teddy bear\", \"stuffed animal\").\n\tis_a(\"stuffed bear\", \"stuffed animal\").\n\tis_a(\"stuffed dog\", \"stuffed animal\").\n\tis_a(\"cable\", \"electronic device\").\n\tis_a(\"hard drive\", \"electronic device\").\n\tis_a(\"charger\", \"electronic device\").\n\tis_a(\"cd\", \"electronic device\").\n\tis_a(\"remote\", \"electronic device\").\n", "is_a(\"controller\", \"electronic device\").\n\tis_a(\"telephone\", \"electronic device\").\n\tis_a(\"bicycle\", \"vehicle\").\n\tis_a(\"cart\", \"vehicle\").\n\tis_a(\"wagon\", \"vehicle\").\n\tis_a(\"carriage\", \"vehicle\").\n\tis_a(\"stroller\", \"vehicle\").\n\tis_a(\"motorcycle\", \"vehicle\").\n\tis_a(\"scooter\", \"vehicle\").\n\tis_a(\"subway\", \"vehicle\").\n", "is_a(\"train\", \"vehicle\").\n\tis_a(\"car\", \"vehicle\").\n\tis_a(\"planter\", \"vehicle\").\n\tis_a(\"tractor\", \"vehicle\").\n\tis_a(\"crane\", \"vehicle\").\n\tis_a(\"aircraft\", \"vehicle\").\n\tis_a(\"watercraft\", \"vehicle\").\n\tis_a(\"trailer\", \"vehicle\").\n\tis_a(\"truck\", \"vehicle\").\n\tis_a(\"fire truck\", \"vehicle\").\n", "is_a(\"bus\", \"vehicle\").\n\tis_a(\"school bus\", \"vehicle\").\n\tis_a(\"ambulance\", \"vehicle\").\n\tis_a(\"double decker\", \"vehicle\").\n\tis_a(\"taxi\", \"vehicle\").\n\tis_a(\"sedan\", \"car\").\n\tis_a(\"minivan\", \"car\").\n\tis_a(\"van\", \"car\").\n\tis_a(\"pickup\", \"car\").\n\tis_a(\"jeep\", \"car\").\n", "is_a(\"suv\", \"car\").\n\tis_a(\"micro\", \"car\").\n\tis_a(\"hatchback\", \"car\").\n\tis_a(\"coupe\", \"car\").\n\tis_a(\"station wagon\", \"car\").\n\tis_a(\"roadster\", \"car\").\n\tis_a(\"cabriolet\", \"car\").\n\tis_a(\"muscle car\", \"car\").\n\tis_a(\"sport car\", \"car\").\n\tis_a(\"super car\", \"car\").\n", "is_a(\"limousine\", \"car\").\n\tis_a(\"cuv\", \"car\").\n\tis_a(\"campervan\", \"car\").\n\tis_a(\"engine\", \"part of vehicle\").\n\tis_a(\"cargo\", \"part of vehicle\").\n\tis_a(\"steering wheel\", \"part of vehicle\").\n\tis_a(\"kickstand\", \"part of vehicle\").\n\tis_a(\"wheel\", \"part of vehicle\").\n\tis_a(\"tire\", \"part of vehicle\").\n\tis_a(\"windshield\", \"part of vehicle\").\n", "is_a(\"propeller\", \"part of vehicle\").\n\tis_a(\"paddle\", \"part of vehicle\").\n\tis_a(\"locomotive\", \"part of vehicle\").\n\tis_a(\"letters\", \"symbol\").\n\tis_a(\"words\", \"symbol\").\n\tis_a(\"numbers\", \"symbol\").\n\tis_a(\"roman numerals\", \"symbol\").\n\tis_a(\"snowboard\", \"sports equipment\").\n\tis_a(\"skateboard\", \"sports equipment\").\n\tis_a(\"surfboard\", \"sports equipment\").\n", "is_a(\"skis\", \"sports equipment\").\n\tis_a(\"frisbee\", \"sports equipment\").\n\tis_a(\"ball\", \"sports equipment\").\n\tis_a(\"tennis ball\", \"sports equipment\").\n\tis_a(\"soccer ball\", \"sports equipment\").\n\tis_a(\"kite\", \"sports equipment\").\n\tis_a(\"hurdle\", \"sports equipment\").\n\tis_a(\"racket\", \"sports equipment\").\n\tis_a(\"baseball bat\", \"sports equipment\").\n\tis_a(\"baseball helmet\", \"sports equipment\").\n", "is_a(\"baseball glove\", \"sports equipment\").\n\tis_a(\"baseball mitt\", \"sports equipment\").\n\tis_a(\"goggles\", \"sports equipment\").\n\tis_a(\"bedroom\", \"place\").\n\tis_a(\"living room\", \"place\").\n\tis_a(\"dining room\", \"place\").\n\tis_a(\"kitchen\", \"place\").\n\tis_a(\"bathroom\", \"place\").\n\tis_a(\"alcove\", \"place\").\n\tis_a(\"attic\", \"place\").\n", "is_a(\"basement\", \"place\").\n\tis_a(\"closet\", \"place\").\n\tis_a(\"home office\", \"place\").\n\tis_a(\"pantry\", \"place\").\n\tis_a(\"shower\", \"place\").\n\tis_a(\"staircase\", \"place\").\n\tis_a(\"tent\", \"place\").\n\tis_a(\"hall\", \"place\").\n\tis_a(\"balcony\", \"place\").\n\tis_a(\"patio\", \"place\").\n", "is_a(\"factory\", \"place\").\n\tis_a(\"lab\", \"place\").\n\tis_a(\"office\", \"place\").\n\tis_a(\"classroom\", \"place\").\n\tis_a(\"building\", \"place\").\n\tis_a(\"apartment\", \"place\").\n\tis_a(\"restaurant\", \"place\").\n\tis_a(\"alley\", \"place\").\n\tis_a(\"bar\", \"place\").\n\tis_a(\"supermarket\", \"place\").\n", "is_a(\"shop\", \"place\").\n\tis_a(\"market\", \"place\").\n\tis_a(\"store\", \"place\").\n\tis_a(\"mall\", \"place\").\n\tis_a(\"plaza\", \"place\").\n\tis_a(\"theater\", \"place\").\n\tis_a(\"courtyard\", \"place\").\n\tis_a(\"gas station\", \"place\").\n\tis_a(\"restroom\", \"place\").\n\tis_a(\"library\", \"place\").\n", "is_a(\"dormitory\", \"place\").\n\tis_a(\"aquarium\", \"place\").\n\tis_a(\"school\", \"place\").\n\tis_a(\"bank\", \"place\").\n\tis_a(\"hospital\", \"place\").\n\tis_a(\"casino\", \"place\").\n\tis_a(\"baseball field\", \"place\").\n\tis_a(\"bleachers\", \"place\").\n\tis_a(\"dugout\", \"place\").\n\tis_a(\"football field\", \"place\").\n", "is_a(\"soccer field\", \"place\").\n\tis_a(\"golf course\", \"place\").\n\tis_a(\"stadium\", \"place\").\n\tis_a(\"court\", \"place\").\n\tis_a(\"tennis court\", \"place\").\n\tis_a(\"ski lift\", \"place\").\n\tis_a(\"ski slope\", \"place\").\n\tis_a(\"swimming pool\", \"place\").\n\tis_a(\"pool\", \"place\").\n\tis_a(\"playground\", \"place\").\n", "is_a(\"park\", \"place\").\n\tis_a(\"resort\", \"place\").\n\tis_a(\"skate park\", \"place\").\n\tis_a(\"station\", \"place\").\n\tis_a(\"bus station\", \"place\").\n\tis_a(\"train station\", \"place\").\n\tis_a(\"bus stop\", \"place\").\n\tis_a(\"airport\", \"place\").\n\tis_a(\"tarmac\", \"place\").\n\tis_a(\"freeway\", \"place\").\n", "is_a(\"street\", \"place\").\n\tis_a(\"tunnel\", \"place\").\n\tis_a(\"highway\", \"place\").\n\tis_a(\"driveway\", \"place\").\n\tis_a(\"road\", \"place\").\n\tis_a(\"crosswalk\", \"place\").\n\tis_a(\"overpass\", \"place\").\n\tis_a(\"runway\", \"place\").\n\tis_a(\"railway\", \"place\").\n\tis_a(\"parking lot\", \"place\").\n", "is_a(\"track\", \"place\").\n\tis_a(\"bridge\", \"place\").\n\tis_a(\"airfield\", \"place\").\n\tis_a(\"shore\", \"place\").\n\tis_a(\"beach\", \"place\").\n\tis_a(\"harbor\", \"place\").\n\tis_a(\"jetty\", \"place\").\n\tis_a(\"dock\", \"place\").\n\tis_a(\"pier\", \"place\").\n\tis_a(\"sidewalk\", \"place\").\n", "is_a(\"lane\", \"place\").\n\tis_a(\"curb\", \"place\").\n\tis_a(\"crosswalkhouse\", \"place\").\n\tis_a(\"home\", \"place\").\n\tis_a(\"hotel\", \"place\").\n\tis_a(\"farm\", \"place\").\n\tis_a(\"barn\", \"place\").\n\tis_a(\"garage\", \"place\").\n\tis_a(\"corn field\", \"place\").\n\tis_a(\"corral\", \"place\").\n", "is_a(\"garden\", \"place\").\n\tis_a(\"orchard\", \"place\").\n\tis_a(\"tower\", \"place\").\n\tis_a(\"windmill\", \"place\").\n\tis_a(\"church\", \"place\").\n\tis_a(\"temple\", \"place\").\n\tis_a(\"chapel\", \"place\").\n\tis_a(\"shrine\", \"place\").\n\tis_a(\"lighthouse\", \"place\").\n\tis_a(\"clock tower\", \"place\").\n", "is_a(\"arch\", \"place\").\n\tis_a(\"dam\", \"place\").\n\tis_a(\"zoo\", \"place\").\n\tis_a(\"ocean\", \"place\").\n\tis_a(\"lake\", \"place\").\n\tis_a(\"pond\", \"place\").\n\tis_a(\"river\", \"place\").\n\tis_a(\"raft\", \"place\").\n\tis_a(\"creekswamp\", \"place\").\n\tis_a(\"waterfall\", \"place\").\n", "is_a(\"wave\", \"place\").\n\tis_a(\"canyon\", \"place\").\n\tis_a(\"cliff\", \"place\").\n\tis_a(\"desert\", \"place\").\n\tis_a(\"mountain\", \"place\").\n\tis_a(\"hill\", \"place\").\n\tis_a(\"valley\", \"place\").\n\tis_a(\"plain\", \"place\").\n\tis_a(\"air\", \"place\").\n\tis_a(\"land\", \"place\").\n", "is_a(\"sky\", \"place\").\n\tis_a(\"bamboo forest\", \"place\").\n\tis_a(\"forest\", \"place\").\n\tis_a(\"jungle\", \"place\").\n\tis_a(\"yard\", \"place\").\n\tis_a(\"field\", \"place\").\n\tis_a(\"savanna\", \"place\").\n\tis_a(\"bayou\", \"place\").\n\tis_a(\"city\", \"place\").\n\tis_a(\"downtown\", \"place\").\n", "is_a(\"wild\", \"place\").\n\tis_a(\"bedroom\", \"room\").\n\tis_a(\"living room\", \"room\").\n\tis_a(\"dining room\", \"room\").\n\tis_a(\"kitchen\", \"room\").\n\tis_a(\"bathroom\", \"room\").\n\tis_a(\"alcove\", \"room\").\n\tis_a(\"attic\", \"room\").\n\tis_a(\"basement\", \"room\").\n\tis_a(\"closet\", \"room\").\n", "is_a(\"home office\", \"room\").\n\tis_a(\"office\", \"room\").\n\tis_a(\"pantry\", \"room\").\n\tis_a(\"shower\", \"room\").\n\tis_a(\"staircase\", \"room\").\n\tis_a(\"hall\", \"room\").\n\tis_a(\"balcony\", \"room\").\n\tis_a(\"hydrant\", \"object\").\n\tis_a(\"manhole cover\", \"object\").\n\tis_a(\"fountain\", \"object\").\n", "is_a(\"line\", \"object\").\n\tis_a(\"parking meter\", \"object\").\n\tis_a(\"mailbox\", \"object\").\n\tis_a(\"pole\", \"object\").\n\tis_a(\"street light\", \"object\").\n\tis_a(\"sign\", \"object\").\n\tis_a(\"street sign\", \"object\").\n\tis_a(\"stop sign\", \"object\").\n\tis_a(\"traffic sign\", \"object\").\n\tis_a(\"parking signtraffic light\", \"object\").\n", "is_a(\"bench\", \"object\").\n\tis_a(\"trash can\", \"object\").\n\tis_a(\"cone\", \"object\").\n\tis_a(\"dispenser\", \"object\").\n\tis_a(\"vending machine\", \"object\").\n\tis_a(\"toolbox\", \"object\").\n\tis_a(\"buoy\", \"object\").\n\tis_a(\"dumpster\", \"object\").\n\tis_a(\"garbage\", \"object\").\n\tis_a(\"umbrella\", \"object\").\n", "is_a(\"canopy\", \"object\").\n\tis_a(\"backpack\", \"object\").\n\tis_a(\"luggage\", \"object\").\n\tis_a(\"purse\", \"object\").\n\tis_a(\"wallet\", \"object\").\n\tis_a(\"bag\", \"object\").\n\tis_a(\"handbag\", \"object\").\n\tis_a(\"shopping bag\", \"object\").\n\tis_a(\"trash bag\", \"object\").\n\tis_a(\"pouch\", \"object\").\n", "is_a(\"suitcase\", \"object\").\n\tis_a(\"box\", \"object\").\n\tis_a(\"crate\", \"object\").\n\tis_a(\"sack\", \"object\").\n\tis_a(\"cardboard\", \"object\").\n\tis_a(\"light bulb\", \"object\").\n\tis_a(\"christmas light\", \"object\").\n\tis_a(\"ceiling light\", \"object\").\n\tis_a(\"clock\", \"object\").\n\tis_a(\"alarm clock\", \"object\").\n", "is_a(\"gift\", \"object\").\n\tis_a(\"wheelchair\", \"object\").\n\tis_a(\"beach umbrella\", \"object\").\n\tis_a(\"parachute\", \"object\").\n\tis_a(\"feeder\", \"object\").\n\tis_a(\"fire extinguisher\", \"object\").\n\tis_a(\"tissue box\", \"object\").\n\tis_a(\"paper dispenser\", \"object\").\n\tis_a(\"soap dispenser\", \"object\").\n\tis_a(\"napkin dispenser\", \"object\").\n", "is_a(\"towel dispenser\", \"object\").\n\tis_a(\"spray can\", \"object\").\n\tis_a(\"paint brush\", \"object\").\n\tis_a(\"cash register\", \"object\").\n\tis_a(\"candle holder\", \"object\").\n\tis_a(\"bell\", \"object\").\n\tis_a(\"lock\", \"object\").\n\tis_a(\"cigarette\", \"object\").\n\tis_a(\"curtain\", \"object\").\n\tis_a(\"carpet\", \"object\").\n", "is_a(\"rug\", \"object\").\n\tis_a(\"thermometer\", \"object\").\n\tis_a(\"fence\", \"object\").\n\tis_a(\"barrier\", \"object\").\n\tis_a(\"stick\", \"object\").\n\tis_a(\"rope\", \"object\").\n\tis_a(\"chain\", \"object\").\n\tis_a(\"hook\", \"object\").\n\tis_a(\"cage\", \"object\").\n\tis_a(\"chalk\", \"object\").\n", "is_a(\"chalkboard\", \"object\").\n\tis_a(\"money\", \"object\").\n\tis_a(\"coin\", \"object\").\n\tis_a(\"shield\", \"object\").\n\tis_a(\"armor\", \"object\").\n\tis_a(\"seat belt\", \"object\").\n\tis_a(\"chimney\", \"object\").\n\tis_a(\"fishing pole\", \"object\").\n\tis_a(\"bottle\", \"object\").\n\tis_a(\"bandage\", \"object\").\n", "is_a(\"lipstick\", \"object\").\n\tis_a(\"wig\", \"object\").\n\tis_a(\"shaving cream\", \"object\").\n\tis_a(\"deodorant\", \"object\").\n\tis_a(\"lotion\", \"object\").\n\tis_a(\"sink\", \"object\").\n\tis_a(\"faucet\", \"object\").\n\tis_a(\"fireplace\", \"object\").\n\tis_a(\"shower\", \"object\").\n\tis_a(\"fan\", \"object\").\n", "is_a(\"light switch\", \"object\").\n\tis_a(\"figure\", \"object\").\n\tis_a(\"frame\", \"object\").\n\tis_a(\"picture frame\", \"object\").\n\tis_a(\"door frame\", \"object\").\n\tis_a(\"window frame\", \"object\").\n\tis_a(\"lamp\", \"object\").\n\tis_a(\"table lamp\", \"object\").\n\tis_a(\"desk lamp\", \"object\").\n\tis_a(\"lamps\", \"object\").\n", "is_a(\"floor lamp\", \"object\").\n\tis_a(\"sconce\", \"object\").\n\tis_a(\"chandelier\", \"object\").\n\tis_a(\"bathtub\", \"object\").\n\tis_a(\"urinal\", \"object\").\n\tis_a(\"soap dish\", \"object\").\n\tis_a(\"fans\", \"object\").\n\tis_a(\"string\", \"object\").\n\tis_a(\"shade\", \"object\").\n\tis_a(\"tarp\", \"object\").\n", "is_a(\"handle\", \"object\").\n\tis_a(\"knob\", \"object\").\n\tis_a(\"hammer\", \"object\").\n\tis_a(\"screw\", \"object\").\n\tis_a(\"broom\", \"object\").\n\tis_a(\"sponge\", \"object\").\n\tis_a(\"cane\", \"object\").\n\tis_a(\"knife block\", \"object\").\n\tis_a(\"waste basket\", \"object\").\n\tis_a(\"satellite dish\", \"object\").\n", "is_a(\"shopping cart\", \"object\").\n\tis_a(\"tape\", \"object\").\n\tis_a(\"cord\", \"object\").\n\tis_a(\"power line\", \"object\").\n\tis_a(\"book\", \"object\").\n\tis_a(\"newspaper\", \"object\").\n\tis_a(\"magazine\", \"object\").\n\tis_a(\"paper\", \"object\").\n\tis_a(\"notebook\", \"object\").\n\tis_a(\"notepad\", \"object\").\n", "is_a(\"cookbook\", \"object\").\n\tis_a(\"map\", \"object\").\n\tis_a(\"envelope\", \"object\").\n\tis_a(\"pen\", \"object\").\n\tis_a(\"pencil\", \"object\").\n\tis_a(\"marker\", \"object\").\n\tis_a(\"crayon\", \"object\").\n\tis_a(\"pencil sharpener\", \"object\").\n\tis_a(\"ruler\", \"object\").\n\tis_a(\"binder\", \"object\").\n", "is_a(\"scissors\", \"object\").\n\tis_a(\"stapler\", \"object\").\n\tis_a(\"staples\", \"object\").\n\tis_a(\"glue stick\", \"object\").\n\tis_a(\"clip\", \"object\").\n\tis_a(\"folder\", \"object\").\n\tis_a(\"briefcase\", \"object\").\n\tis_a(\"vehicle\", \"object\").\n\tis_a(\"sports equipment\", \"object\").\n\tis_a(\"artwork\", \"object\").\n", "is_a(\"writing implement\", \"object\").\n\tis_a(\"electronic device\", \"object\").\n\tis_a(\"instruments\", \"object\").\n\tis_a(\"toy\", \"object\").\n\tis_a(\"bedding\", \"object\").\n\tis_a(\"weapon\", \"object\").\n\tis_a(\"utensil\", \"object\").\n\tis_a(\"tableware\", \"object\").\n\tis_a(\"home appliance\", \"object\").\n\tis_a(\"kitchenware\", \"object\").\n", "is_a(\"furniture\", \"object\").\n\tis_a(\"food\", \"object\").\n\tis_a(\"drinks\", \"object\").\n\tis_a(\"beverage\", \"object\").\n\tis_a(\"accessory\", \"object\").\n\tis_a(\"clothing\", \"object\").\n\tis_a(\"animal\", \"object\").\n\tis_a(\"plant\", \"object\").\n\tis_a(\"alpaca\", \"herbivore\").\n\tis_a(\"amphibian\", \"class\").\n", "is_a(\"odd-toed ungulate\", \"order\").\n\tis_a(\"even-toed ungulate\", \"order\").\n\tis_a(\"primate\", \"order\").\n\tis_a(\"carnivoran\", \"order\").\n\tis_a(\"canidae\", \"family\").\n\tis_a(\"perfume\", \"liquid\").\n\tis_a(\"cosmetic\", \"toiletry\").\n\tis_a(\"antiperspirant\", \"toiletry\").\n\tis_a(\"eyebrow pencil\", \"cosmetic\").\n\tis_a(\"face powder\", \"cosmetic\").\n", "is_a(\"facial moisturizer\", \"cosmetic\").\n\tis_a(\"rouge\", \"cosmetic\").\n\t'''\n\tKG_REL= '''\n\toa_rel(\"is used for\", \"floor\", \"standing on\").\n\toa_rel(\"is used for\", \"shelf\", \"storing foods\").\n\toa_rel(\"is used for\", \"wall\", \"holding up roof\").\n\toa_rel(\"is used for\", \"chair\", \"sitting on\").\n\toa_rel(\"is used for\", \"table\", \"holding things\").\n\toa_rel(\"is used for\", \"bookshelf\", \"storing magazines\").\n", "oa_rel(\"is used for\", \"bookshelf\", \"holding books\").\n\toa_rel(\"can\", \"vase\", \"holds flowers\").\n\toa_rel(\"usually appears in\", \"book\", \"office\").\n\toa_rel(\"is used for\", \"fireplace\", \"burning things\").\n\toa_rel(\"is used for\", \"window\", \"letting light in\").\n\toa_rel(\"can\", \"grass\", \"turn brown\").\n\toa_rel(\"can\", \"fish\", \"navigate via polarised light\").\n\toa_rel(\"can\", \"cat\", \"eat fish\").\n\toa_rel(\"is used for\", \"bridge\", \"crossing valley\").\n\toa_rel(\"is used for\", \"stool\", \"reaching high places\").\n", "oa_rel(\"is\", \"screen\", \"electric\").\n\toa_rel(\"is used for\", \"carpet\", \"saving floor\").\n\toa_rel(\"is used for\", \"store\", \"buying and selling\").\n\toa_rel(\"is used for\", \"box\", \"putting things in\").\n\toa_rel(\"is used for\", \"dresser\", \"supporting mirror\").\n\toa_rel(\"can\", \"tree\", \"shade car\").\n\toa_rel(\"can be\", \"paper\", \"cut\").\n\toa_rel(\"is used for\", \"table\", \"writing at\").\n\toa_rel(\"is used for\", \"pot\", \"cooking stew\").\n\toa_rel(\"can\", \"gas stove\", \"heat pot\").\n", "oa_rel(\"is used for\", \"patio\", \"sitting outside\").\n\toa_rel(\"can\", \"umbrella\", \"protect you from sun\").\n\toa_rel(\"can be\", \"door\", \"opened or closed\").\n\toa_rel(\"is used for\", \"seat\", \"sitting on\").\n\toa_rel(\"can\", \"remote\", \"control tv\").\n\toa_rel(\"is used for\", \"remote\", \"remotely controlling TV\").\n\toa_rel(\"is\", \"console\", \"electric\").\n\toa_rel(\"can\", \"car\", \"travel on road\").\n\toa_rel(\"is used for\", \"window\", \"letting fresh air in\").\n\toa_rel(\"is used for\", \"car\", \"transporting handful of people\").\n", "oa_rel(\"is used for\", \"road\", \"driving car on\").\n\toa_rel(\"can\", \"cart\", \"follow horse\").\n\toa_rel(\"can\", \"horse\", \"pull cart\").\n\toa_rel(\"is used for\", \"luggage\", \"carrying things\").\n\toa_rel(\"usually appears in\", \"shelf\", \"bedroom\").\n\toa_rel(\"has\", \"cupcake\", \"starch\").\n\toa_rel(\"is used for\", \"container\", \"holding foods\").\n\toa_rel(\"is\", \"chocolate\", \"sticky\").\n\toa_rel(\"is used for\", \"tray\", \"holding food\").\n\toa_rel(\"is used for\", \"paper\", \"drawing on\").\n", "oa_rel(\"is made from\", \"hot dog\", \"flour\").\n\toa_rel(\"usually appears in\", \"blanket\", \"bedroom\").\n\toa_rel(\"is made from\", \"cinnamon roll\", \"flour\").\n\toa_rel(\"is used for\", \"sugar\", \"sweetening food\").\n\toa_rel(\"is used for\", \"basket\", \"carrying something\").\n\toa_rel(\"usually appears in\", \"tray\", \"restaurant\").\n\toa_rel(\"is made from\", \"dough\", \"flour\").\n\toa_rel(\"requires\", \"sauteing\", \"pan\").\n\toa_rel(\"is used for\", \"table\", \"eating at\").\n\toa_rel(\"is used for\", \"window\", \"looking outside\").\n", "oa_rel(\"is used for\", \"bowl\", \"holding fruit\").\n\toa_rel(\"is used for\", \"wall\", \"hanging picture\").\n\toa_rel(\"is used for\", \"glass\", \"holding drinks\").\n\toa_rel(\"can\", \"hammer\", \"break glass\").\n\toa_rel(\"can\", \"knife\", \"cut you\").\n\toa_rel(\"requires\", \"slicing\", \"knife\").\n\toa_rel(\"has\", \"cake\", \"starch\").\n\toa_rel(\"usually appears in\", \"tub\", \"bathroom\").\n\toa_rel(\"is used for\", \"plate\", \"holding pizza\").\n\toa_rel(\"is used for\", \"garage\", \"parking car\").\n", "oa_rel(\"is used for\", \"bar\", \"meeting friends\").\n\toa_rel(\"can\", \"suv\", \"travel on road\").\n\toa_rel(\"is used for\", \"bar\", \"getting drunk\").\n\toa_rel(\"is\", \"alcohol\", \"harmful\").\n\toa_rel(\"can\", \"bottle\", \"hold liquid\").\n\toa_rel(\"is used for\", \"drinks\", \"satisfying thirst\").\n\toa_rel(\"is used for\", \"stool\", \"tying shoes\").\n\toa_rel(\"can\", \"vacuum\", \"clean floor\").\n\toa_rel(\"can\", \"wine\", \"age in bottle\").\n\toa_rel(\"is\", \"wine\", \"liquid\").\n", "oa_rel(\"is used for\", \"lamp\", \"lighting room\").\n\toa_rel(\"usually appears in\", \"wine glass\", \"restaurant\").\n\toa_rel(\"is used for\", \"tablecloth\", \"keeping table clean\").\n\toa_rel(\"is used for\", \"tablecloth\", \"decoration\").\n\toa_rel(\"usually appears in\", \"plate\", \"restaurant\").\n\toa_rel(\"is used for\", \"barn\", \"keeping animals\").\n\toa_rel(\"is used for\", \"bedroom\", \"sleeping\").\n\toa_rel(\"usually appears in\", \"bed\", \"bedroom\").\n\toa_rel(\"is used for\", \"pillow\", \"make seat softer\").\n\toa_rel(\"usually appears in\", \"mirror\", \"bathroom\").\n", "oa_rel(\"can\", \"curtain\", \"keep light out of room\").\n\toa_rel(\"can\", \"tv\", \"display images\").\n\toa_rel(\"is used for\", \"dresser\", \"storing cloth\").\n\toa_rel(\"is used for\", \"door\", \"making room private\").\n\toa_rel(\"is used for\", \"wall\", \"divide open space into smaller areas\").\n\toa_rel(\"is\", \"lamp\", \"electric\").\n\toa_rel(\"is used for\", \"tv\", \"entertainment\").\n\toa_rel(\"can\", \"truck\", \"pull cars\").\n\toa_rel(\"can\", \"cart\", \"transport things\").\n\toa_rel(\"is used for\", \"boat\", \"transporting people\").\n", "oa_rel(\"can\", \"tree\", \"shade lawn\").\n\toa_rel(\"is\", \"water\", \"fluid\").\n\toa_rel(\"is used for\", \"bicycle\", \"transporting people\").\n\toa_rel(\"is used for\", \"street\", \"transportation\").\n\toa_rel(\"is used for\", \"bottle\", \"holding juice\").\n\toa_rel(\"is used for\", \"table\", \"playing game at\").\n\toa_rel(\"is used for\", \"fireplace\", \"heating home\").\n\toa_rel(\"is used for\", \"car\", \"transporting people\").\n\toa_rel(\"is used for\", \"driveway\", \"transportation\").\n\toa_rel(\"is\", \"computer\", \"electric\").\n", "oa_rel(\"is used for\", \"table\", \"putting things on\").\n\toa_rel(\"is used for\", \"clock\", \"measuring passage of time\").\n\toa_rel(\"is used for\", \"necklace\", \"decoration\").\n\toa_rel(\"is used for\", \"book\", \"learning\").\n\toa_rel(\"is used for\", \"fence\", \"keeping pets in\").\n\toa_rel(\"is used for\", \"field\", \"grazing animals\").\n\toa_rel(\"is used for\", \"floor\", \"walking on\").\n\toa_rel(\"can\", \"grass\", \"grow on hill\").\n\toa_rel(\"can\", \"car\", \"transport people\").\n\toa_rel(\"usually appears in\", \"coffee cup\", \"dining room\").\n", "oa_rel(\"is used for\", \"couch\", \"lying on\").\n\toa_rel(\"usually appears in\", \"switch\", \"bedroom\").\n\toa_rel(\"is used for\", \"rug\", \"covering area near front door\").\n\toa_rel(\"is used for\", \"rug\", \"covering floor\").\n\toa_rel(\"requires\", \"sauteing\", \"skillet\").\n\toa_rel(\"is used for\", \"stove\", \"boiling water\").\n\toa_rel(\"is used for\", \"fence\", \"enclosing space\").\n\toa_rel(\"can\", \"bear\", \"climb tree\").\n\toa_rel(\"is used for\", \"book\", \"reading for pleasure\").\n\toa_rel(\"is used for\", \"bar\", \"meeting people\").\n", "oa_rel(\"is used for\", \"road\", \"transportation\").\n\toa_rel(\"can\", \"bus\", \"carry passengers\").\n\toa_rel(\"can\", \"bus\", \"travel on road\").\n\toa_rel(\"usually appears in\", \"mirror\", \"bedroom\").\n\toa_rel(\"is used for\", \"van\", \"transporting goods\").\n\toa_rel(\"is used for\", \"phone\", \"communicating\").\n\toa_rel(\"usually appears in\", \"paper\", \"office\").\n\toa_rel(\"can be\", \"food\", \"eaten\").\n\toa_rel(\"is used for\", \"spoon\", \"eating food that isn't very solid\").\n\toa_rel(\"is used for\", \"spoon\", \"scooping food\").\n", "oa_rel(\"is\", \"dessert\", \"sweet\").\n\toa_rel(\"usually appears in\", \"steak\", \"dinner\").\n\toa_rel(\"is\", \"water\", \"liquid\").\n\toa_rel(\"can\", \"bowl\", \"keep water in\").\n\toa_rel(\"requires\", \"cooking\", \"cooking utensils\").\n\toa_rel(\"has\", \"watermelon\", \"vitamin C\").\n\toa_rel(\"can be\", \"fruit\", \"eaten\").\n\toa_rel(\"is used for\", \"wall\", \"hanging art work\").\n\toa_rel(\"is\", \"ocean\", \"fluid\").\n\toa_rel(\"can\", \"chicken\", \"be pet\").\n", "oa_rel(\"is used for\", \"bowl\", \"holding cereal\").\n\toa_rel(\"usually appears in\", \"carrot\", \"salad\").\n\toa_rel(\"usually appears in\", \"lettuce\", \"salad\").\n\toa_rel(\"is\", \"sauce\", \"sticky\").\n\toa_rel(\"is used for\", \"plate\", \"holding food\").\n\toa_rel(\"is used for\", \"food\", \"eating\").\n\toa_rel(\"has\", \"cabbage\", \"vitamin C\").\n\toa_rel(\"is used for\", \"bar\", \"drinking alcohol\").\n\toa_rel(\"has\", \"cola\", \"water\").\n\toa_rel(\"usually appears in\", \"laptop\", \"office\").\n", "oa_rel(\"is used for\", \"desk\", \"putting computer on\").\n\toa_rel(\"is used for\", \"bed\", \"lying down\").\n\toa_rel(\"is used for\", \"mouse\", \"interfacing with computer\").\n\toa_rel(\"is used for\", \"refrigerator\", \"chilling drinks\").\n\toa_rel(\"is used for\", \"drinks\", \"drinking\").\n\toa_rel(\"is used for\", \"street sign\", \"giving instructions to road users\").\n\toa_rel(\"can\", \"tree\", \"grow new branches\").\n\toa_rel(\"requires\", \"cooling off\", \"air conditioner\").\n\toa_rel(\"can\", \"water\", \"feel wet\").\n\toa_rel(\"is used for\", \"umbrella\", \"protection from rain\").\n", "oa_rel(\"is\", \"apple\", \"healthy\").\n\toa_rel(\"can\", \"car\", \"move quickly\").\n\toa_rel(\"can\", \"glass\", \"hold liquid\").\n\toa_rel(\"can\", \"horse\", \"pull wagon\").\n\toa_rel(\"usually appears in\", \"candle\", \"dining room\").\n\toa_rel(\"is used for\", \"bench\", \"lying down\").\n\toa_rel(\"can\", \"cup\", \"store liquid\").\n\toa_rel(\"is used for\", \"curtain\", \"covering window\").\n\toa_rel(\"can\", \"umbrella\", \"shield one from rain or sun\").\n\toa_rel(\"is used for\", \"curtain\", \"blocking light\").\n", "oa_rel(\"is used for\", \"bowl\", \"holding beans\").\n\toa_rel(\"can\", \"cat\", \"kill birds\").\n\toa_rel(\"is used for\", \"sidewalk\", \"skating on\").\n\toa_rel(\"is used for\", \"sidewalk\", \"walking dog\").\n\toa_rel(\"usually appears in\", \"clip\", \"office\").\n\toa_rel(\"usually appears in\", \"lid\", \"bathroom\").\n\toa_rel(\"is\", \"zebra\", \"herbivorous\").\n\toa_rel(\"is used for\", \"paper\", \"writing on\").\n\toa_rel(\"is used for\", \"pen\", \"signing checks\").\n\toa_rel(\"usually appears in\", \"monitor\", \"office\").\n", "oa_rel(\"is used for\", \"glove\", \"protecting hand\").\n\toa_rel(\"can\", \"trailer\", \"travel on road\").\n\toa_rel(\"can\", \"truck\", \"pull trailer\").\n\toa_rel(\"has\", \"shrimp\", \"iron\").\n\toa_rel(\"has\", \"shrimp\", \"vitamin D\").\n\toa_rel(\"is\", \"lime\", \"sour\").\n\toa_rel(\"has\", \"meat\", \"vitamin B\").\n\toa_rel(\"is made from\", \"bacon\", \"pork\").\n\toa_rel(\"requires\", \"making pizza\", \"sauce\").\n\toa_rel(\"is used for\", \"bus station\", \"waiting for bus\").\n", "oa_rel(\"is used for\", \"van\", \"transporting handful of people\").\n\toa_rel(\"can\", \"car\", \"spend gas\").\n\toa_rel(\"is used for\", \"track\", \"subway to run on\").\n\toa_rel(\"is used for\", \"train\", \"transporting goods\").\n\toa_rel(\"is used for\", \"blinds\", \"keep out light from houses\").\n\toa_rel(\"can\", \"computer\", \"help people\").\n\toa_rel(\"usually appears in\", \"holder\", \"bathroom\").\n\toa_rel(\"usually appears in\", \"mug\", \"restaurant\").\n\toa_rel(\"is used for\", \"glasses\", \"improving eyesight\").\n\toa_rel(\"is used for\", \"horse\", \"riding\").\n", "oa_rel(\"can\", \"tree\", \"grow branch\").\n\toa_rel(\"has\", \"donut\", \"starch\").\n\toa_rel(\"is\", \"frosting\", \"sweet\").\n\toa_rel(\"is used for\", \"shoes\", \"protecting feet\").\n\toa_rel(\"is used for\", \"handbag\", \"carrying things\").\n\toa_rel(\"can\", \"cat\", \"see well in dark\").\n\toa_rel(\"can\", \"bat\", \"hit baseball\").\n\toa_rel(\"is used for\", \"tennis ball\", \"hitting with racket\").\n\toa_rel(\"is used for\", \"sink\", \"washing up face\").\n\toa_rel(\"can\", \"towel\", \"dry hair\").\n", "oa_rel(\"usually appears in\", \"soap\", \"bathroom\").\n\toa_rel(\"is\", \"cake\", \"sweet\").\n\toa_rel(\"is made from\", \"cheese\", \"milk\").\n\toa_rel(\"is used for\", \"bowl\", \"holding food\").\n\toa_rel(\"is\", \"dog\", \"soft\").\n\toa_rel(\"is used for\", \"seat\", \"resting\").\n\toa_rel(\"is used for\", \"hotel\", \"sleeping away from home\").\n\toa_rel(\"is used for\", \"track\", \"trains to run on\").\n\toa_rel(\"is used for\", \"motorcycle\", \"transporting people\").\n\toa_rel(\"can\", \"cat\", \"clean itself often\").\n", "oa_rel(\"is used for\", \"ring\", \"decoration\").\n\toa_rel(\"is used for\", \"umbrella\", \"keeping sun off you\").\n\toa_rel(\"is used for\", \"boat\", \"floating and moving on water\").\n\toa_rel(\"has\", \"bun\", \"starch\").\n\toa_rel(\"is used for\", \"rug\", \"making floor warmer\").\n\toa_rel(\"is used for\", \"rug\", \"covering just outside shower\").\n\toa_rel(\"is used for\", \"cabinet\", \"storing glasses\").\n\toa_rel(\"usually appears in\", \"fridge\", \"kitchen\").\n\toa_rel(\"is used for\", \"kitchen\", \"cooking food\").\n\toa_rel(\"can\", \"butterfly\", \"fly\").\n", "oa_rel(\"is used for\", \"blanket\", \"sleeping under\").\n\toa_rel(\"is used for\", \"pillow\", \"sleeping\").\n\toa_rel(\"is used for\", \"bench\", \"sitting on\").\n\toa_rel(\"can\", \"elephant\", \"lift logs from ground\").\n\toa_rel(\"has\", \"elephant\", \"trunk\").\n\toa_rel(\"can\", \"grass\", \"stain pants\").\n\toa_rel(\"can\", \"grass\", \"continue to grow\").\n\toa_rel(\"can\", \"elephant\", \"carry trunk\").\n\toa_rel(\"can\", \"bear\", \"fish with it's paw\").\n\toa_rel(\"is used for\", \"fence\", \"containing animals\").\n", "oa_rel(\"can\", \"snow\", \"be packed into ball\").\n\toa_rel(\"can\", \"horse\", \"rest standing up\").\n\toa_rel(\"can\", \"jeep\", \"climb hills\").\n\toa_rel(\"is used for\", \"watch\", \"measuring passage of time\").\n\toa_rel(\"is used for\", \"computer\", \"studying\").\n\toa_rel(\"is\", \"laptop\", \"electric\").\n\toa_rel(\"is used for\", \"candle\", \"decoration\").\n\toa_rel(\"is used for\", \"mouse\", \"controlling computer\").\n\toa_rel(\"has\", \"giraffe\", \"marsupium\").\n\toa_rel(\"is used for\", \"hot dog\", \"eating\").\n", "oa_rel(\"has\", \"beer\", \"alcohol\").\n\toa_rel(\"is used for\", \"napkin\", \"wiping mouth\").\n\toa_rel(\"is used for\", \"carpet\", \"decorating apartment\").\n\toa_rel(\"is used for\", \"keyboard\", \"interfacing with computer\").\n\toa_rel(\"is used for\", \"monitor\", \"displaying images\").\n\toa_rel(\"is used for\", \"stool\", \"sitting on\").\n\toa_rel(\"is used for\", \"handbag\", \"storing things\").\n\toa_rel(\"is used for\", \"lane\", \"driving car on\").\n\toa_rel(\"is used for\", \"desk\", \"reading at\").\n\toa_rel(\"is used for\", \"mat\", \"protection something\").\n", "oa_rel(\"is used for\", \"bus\", \"transporting people\").\n\toa_rel(\"is used for\", \"bus\", \"mass transit for city\").\n\toa_rel(\"is used for\", \"trash can\", \"storing trash\").\n\toa_rel(\"can\", \"turtle\", \"live much longer than people\").\n\toa_rel(\"is\", \"candy\", \"sweet\").\n\toa_rel(\"usually appears in\", \"chair\", \"bedroom\").\n\toa_rel(\"is used for\", \"carpet\", \"walking on\").\n\toa_rel(\"usually appears in\", \"chalkboard\", \"classroom\").\n\toa_rel(\"is used for\", \"scooter\", \"transporting handful of people\").\n\toa_rel(\"is used for\", \"toy\", \"having fun\").\n", "oa_rel(\"is used for\", \"fan\", \"circulating air\").\n\toa_rel(\"is\", \"charger\", \"electric\").\n\toa_rel(\"is used for\", \"chair\", \"resting\").\n\toa_rel(\"is used for\", \"bookshelf\", \"storing books\").\n\toa_rel(\"can\", \"bottle\", \"store wine\").\n\toa_rel(\"can\", \"monitor\", \"display images\").\n\toa_rel(\"is used for\", \"keyboard\", \"controlling computer\").\n\toa_rel(\"is used for\", \"keyboard\", \"entering text\").\n\toa_rel(\"can\", \"radiator\", \"heat room\").\n\toa_rel(\"can\", \"fan\", \"cool air\").\n", "oa_rel(\"usually appears in\", \"desk\", \"bedroom\").\n\toa_rel(\"is\", \"projector\", \"electric\").\n\toa_rel(\"is used for\", \"computer\", \"data storage\").\n\toa_rel(\"is used for\", \"desk\", \"placing something on\").\n\toa_rel(\"is used for\", \"keyboard\", \"entering data\").\n\toa_rel(\"is\", \"headphones\", \"electric\").\n\toa_rel(\"is used for\", \"window\", \"keeping cold air out\").\n\toa_rel(\"can\", \"computer\", \"save files on disk\").\n\toa_rel(\"is\", \"monitor\", \"electric\").\n\toa_rel(\"is used for\", \"office\", \"business\").\n", "oa_rel(\"is used for\", \"door\", \"entering or exiting area\").\n\toa_rel(\"is used for\", \"classroom\", \"learning\").\n\toa_rel(\"can\", \"monitors\", \"show text\").\n\toa_rel(\"is used for\", \"theater\", \"watching movie\").\n\toa_rel(\"is used for\", \"balcony\", \"viewing, resting, or eating at\").\n\toa_rel(\"is used for\", \"spatula\", \"turning food\").\n\toa_rel(\"is used for\", \"candle\", \"creating ambience\").\n\toa_rel(\"can\", \"car\", \"carry few persons\").\n\toa_rel(\"can\", \"rat\", \"eat wires\").\n\toa_rel(\"usually appears in\", \"office chair\", \"office\").\n", "oa_rel(\"can\", \"cup\", \"hold liquids\").\n\toa_rel(\"is used for\", \"restaurant\", \"drinking\").\n\toa_rel(\"is used for\", \"restaurant\", \"meeting people\").\n\toa_rel(\"is made from\", \"paper\", \"wood\").\n\toa_rel(\"is used for\", \"shirt\", \"covering upperbody\").\n\toa_rel(\"can\", \"printer\", \"print pictures\").\n\toa_rel(\"can\", \"minivan\", \"travel on road\").\n\toa_rel(\"is used for\", \"fountain\", \"decoration\").\n\toa_rel(\"can\", \"poodle\", \"live in house\").\n\toa_rel(\"usually appears in\", \"pot\", \"kitchen\").\n", "oa_rel(\"is used for\", \"kitchen\", \"storing food\").\n\toa_rel(\"is used for\", \"sheet\", \"covering bed\").\n\toa_rel(\"is used for\", \"hospital\", \"delivering babies\").\n\toa_rel(\"is used for\", \"bed\", \"sitting on\").\n\toa_rel(\"is used for\", \"cabinet\", \"storing dishes\").\n\toa_rel(\"can\", \"bus\", \"transport people\").\n\toa_rel(\"can\", \"bottle\", \"hold water\").\n\toa_rel(\"can\", \"horse\", \"be tamed\").\n\toa_rel(\"can\", \"airplane\", \"arrive at airport\").\n\toa_rel(\"is used for\", \"truck\", \"carrying cargo\").\n", "oa_rel(\"is used for\", \"airport\", \"waiting for airplane\").\n\toa_rel(\"is used for\", \"stop sign\", \"controlling traffic\").\n\toa_rel(\"is used for\", \"boat\", \"transportation at sea\").\n\toa_rel(\"can\", \"dog\", \"guard house\").\n\toa_rel(\"requires\", \"washing dishes\", \"faucet\").\n\toa_rel(\"is used for\", \"straw\", \"drinking beverage\").\n\toa_rel(\"is made from\", \"donut\", \"flour\").\n\toa_rel(\"is made from\", \"pastry\", \"flour\").\n\toa_rel(\"usually appears in\", \"glass\", \"restaurant\").\n\toa_rel(\"is used for\", \"phone\", \"sending email\").\n", "oa_rel(\"can\", \"truck\", \"travel on road\").\n\toa_rel(\"can\", \"water\", \"act as reflector\").\n\toa_rel(\"is used for\", \"raft\", \"keeping people out of water\").\n\toa_rel(\"is used for\", \"raft\", \"traveling on water\").\n\toa_rel(\"can\", \"dog\", \"detect odors better than humans can\").\n\toa_rel(\"can\", \"bird\", \"live in house\").\n\toa_rel(\"usually appears in\", \"customer\", \"bar\").\n\toa_rel(\"is\", \"banana\", \"sweet\").\n\toa_rel(\"is used for\", \"sofa\", \"lying down\").\n\toa_rel(\"is\", \"oven\", \"electric\").\n", "oa_rel(\"requires\", \"making pizza\", \"oven\").\n\toa_rel(\"is used for\", \"lamp\", \"illuminating area\").\n\toa_rel(\"is used for\", \"vase\", \"holding flowers\").\n\toa_rel(\"can be\", \"cake\", \"cut\").\n\toa_rel(\"can\", \"bus\", \"run\").\n\toa_rel(\"is used for\", \"bus stop\", \"waiting for bus\").\n\toa_rel(\"is used for\", \"street light\", \"illuminating area\").\n\toa_rel(\"can\", \"traffic light\", \"stop cars\").\n\toa_rel(\"can\", \"horse\", \"carry people\").\n\toa_rel(\"is used for\", \"motorcycle\", \"riding\").\n", "oa_rel(\"can\", \"ship\", \"carry cargo\").\n\toa_rel(\"is used for\", \"ship\", \"keeping people out of water\").\n\toa_rel(\"is used for\", \"boat\", \"traveling on water\").\n\toa_rel(\"is made from\", \"chips\", \"potato\").\n\toa_rel(\"requires\", \"measuring up\", \"measuring cup\").\n\toa_rel(\"usually appears in\", \"dish soap\", \"bathroom\").\n\toa_rel(\"is a sub-event of\", \"cleaning clothing\", \"operating washing machine\").\n\toa_rel(\"can\", \"horse\", \"pull buggy to picnic\").\n\toa_rel(\"can\", \"cart\", \"travel on road\").\n\toa_rel(\"is used for\", \"street\", \"driving car on\").\n", "oa_rel(\"can\", \"bus\", \"drive down street\").\n\toa_rel(\"is used for\", \"shelf\", \"holding books\").\n\toa_rel(\"is used for\", \"restaurant\", \"selling food\").\n\toa_rel(\"can\", \"refrigerator\", \"stock food\").\n\toa_rel(\"is used for\", \"refrigerator\", \"freezing food\").\n\toa_rel(\"is used for\", \"couch\", \"sitting on\").\n\toa_rel(\"usually appears in\", \"sofa\", \"living room\").\n\toa_rel(\"can\", \"dog\", \"live in house\").\n\toa_rel(\"usually appears in\", \"wine glass\", \"bar\").\n\toa_rel(\"has\", \"corn\", \"vitamin B\").\n", "oa_rel(\"can\", \"airplane\", \"go fast\").\n\toa_rel(\"is used for\", \"wine\", \"getting drunk\").\n\toa_rel(\"is used for\", \"sink\", \"washing hands\").\n\toa_rel(\"is used for\", \"horse\", \"transporting people\").\n\toa_rel(\"can\", \"horse\", \"jump higher than people\").\n\toa_rel(\"usually appears in\", \"plate\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"lamp\", \"bedroom\").\n\toa_rel(\"is used for\", \"bed\", \"napping on\").\n\toa_rel(\"is used for\", \"sheet\", \"covering mattress\").\n\toa_rel(\"is used for\", \"fan\", \"cooling air\").\n", "oa_rel(\"can\", \"refrigerator\", \"keep food cold\").\n\toa_rel(\"is used for\", \"blanket\", \"keeping warm at night\").\n\toa_rel(\"is used for\", \"door\", \"controlling access\").\n\toa_rel(\"usually appears in\", \"notebook\", \"office\").\n\toa_rel(\"is used for\", \"kitchen\", \"preparing food\").\n\toa_rel(\"requires\", \"cleaning house\", \"pail\").\n\toa_rel(\"can\", \"pitcher\", \"throw fast ball\").\n\toa_rel(\"is\", \"radiator\", \"electric\").\n\toa_rel(\"usually appears in\", \"christmas tree\", \"christmas\").\n\toa_rel(\"has\", \"wine\", \"alcohol\").\n", "oa_rel(\"is used for\", \"sidewalk\", \"riding skateboards\").\n\toa_rel(\"is used for\", \"bottle\", \"holding drinks\").\n\toa_rel(\"is used for\", \"luggage\", \"storing clothes for trip\").\n\toa_rel(\"is used for\", \"sunglasses\", \"protecting eyes from sunlight\").\n\toa_rel(\"is used for\", \"sidewalk\", \"walking on\").\n\toa_rel(\"is used for\", \"clock\", \"knowing time\").\n\toa_rel(\"is used for\", \"bridge\", \"crossing river\").\n\toa_rel(\"is used for\", \"river\", \"canoeing\").\n\toa_rel(\"is used for\", \"curtain\", \"decorating room\").\n\toa_rel(\"is used for\", \"bathroom\", \"brushing teeth\").\n", "oa_rel(\"can\", \"fire truck\", \"travel on road\").\n\toa_rel(\"is used for\", \"market\", \"buying and selling\").\n\toa_rel(\"is used for\", \"shelf\", \"storing items\").\n\toa_rel(\"can\", \"boat\", \"travel over water\").\n\toa_rel(\"is\", \"printer\", \"electric\").\n\toa_rel(\"is used for\", \"teddy bear\", \"cuddling\").\n\toa_rel(\"can\", \"umbrella\", \"fold up\").\n\toa_rel(\"is used for\", \"train\", \"transporting people\").\n\toa_rel(\"can\", \"jeep\", \"travel on road\").\n\toa_rel(\"is used for\", \"toy\", \"entertainment\").\n", "oa_rel(\"can\", \"batter\", \"strike out\").\n\toa_rel(\"is used for\", \"helmet\", \"protecting head\").\n\toa_rel(\"can\", \"horse\", \"be pet\").\n\toa_rel(\"is used for\", \"motorcycle\", \"transporting handful of people\").\n\toa_rel(\"is used for\", \"baseball bat\", \"hitting something\").\n\toa_rel(\"is used for\", \"house\", \"dwelling\").\n\toa_rel(\"can\", \"train\", \"arrive at station\").\n\toa_rel(\"can\", \"bus\", \"transport many people at once\").\n\toa_rel(\"is used for\", \"crane\", \"transporting people\").\n\toa_rel(\"can\", \"taxi\", \"travel on road\").\n", "oa_rel(\"is used for\", \"cabinet\", \"storing pills\").\n\toa_rel(\"usually appears in\", \"bucket\", \"bathroom\").\n\toa_rel(\"is\", \"xbox controller\", \"electric\").\n\toa_rel(\"is used for\", \"goggles\", \"preventing particulates from striking eyes\").\n\toa_rel(\"can\", \"helmet\", \"protect head from impact\").\n\toa_rel(\"is used for\", \"watch\", \"knowing time\").\n\toa_rel(\"usually appears in\", \"calculator\", \"office\").\n\toa_rel(\"can be\", \"window\", \"opened or closed\").\n\toa_rel(\"is\", \"light bulb\", \"electric\").\n\toa_rel(\"is used for\", \"snow\", \"skiing on\").\n", "oa_rel(\"usually appears in\", \"napkin\", \"dining room\").\n\toa_rel(\"usually appears in\", \"dishes\", \"restaurant\").\n\toa_rel(\"can\", \"dog\", \"guard building\").\n\toa_rel(\"is used for\", \"steering wheel\", \"controlling direction car turns\").\n\toa_rel(\"can\", \"van\", \"travel on road\").\n\toa_rel(\"can\", \"airplane\", \"transport many people at once\").\n\toa_rel(\"usually appears in\", \"fireplace\", \"living room\").\n\toa_rel(\"is\", \"wii controller\", \"electric\").\n\toa_rel(\"is used for\", \"guitar\", \"playing music\").\n\toa_rel(\"is\", \"speaker\", \"electric\").\n", "oa_rel(\"is used for\", \"boat\", \"fishing\").\n\toa_rel(\"can\", \"steering wheel\", \"control direction of car\").\n\toa_rel(\"is used for\", \"car\", \"transportation\").\n\toa_rel(\"is used for\", \"fork\", \"moving food to mouth\").\n\toa_rel(\"usually appears in\", \"water glass\", \"bar\").\n\toa_rel(\"is used for\", \"bench\", \"resting\").\n\toa_rel(\"can\", \"boat\", \"swim on water\").\n\toa_rel(\"usually appears in\", \"menu\", \"restaurant\").\n\toa_rel(\"can\", \"bull\", \"charge matador\").\n\toa_rel(\"is used for\", \"speaker\", \"listening to music\").\n", "oa_rel(\"can\", \"hammers\", \"nail wood\").\n\toa_rel(\"can\", \"monkeys\", \"use tool\").\n\toa_rel(\"is used for\", \"pot\", \"holding water\").\n\toa_rel(\"is\", \"elephant\", \"herbivorous\").\n\toa_rel(\"is used for\", \"vehicle\", \"transportation\").\n\toa_rel(\"is used for\", \"bicycle\", \"riding\").\n\toa_rel(\"can\", \"truck\", \"carry cargo\").\n\toa_rel(\"can\", \"stethoscopes\", \"listen to heart\").\n\toa_rel(\"is used for\", \"suitcase\", \"packing clothes for trip\").\n\toa_rel(\"is\", \"fruit\", \"healthy\").\n", "oa_rel(\"has\", \"grapefruit\", \"vitamin C\").\n\toa_rel(\"can\", \"airplane\", \"cross ocean\").\n\toa_rel(\"is used for\", \"runway\", \"aircraft takeoff\").\n\toa_rel(\"is used for\", \"shower\", \"cleaning body\").\n\toa_rel(\"is used for\", \"bathroom\", \"taking bath\").\n\toa_rel(\"is used for\", \"sink\", \"cleaning dishes\").\n\toa_rel(\"is used for\", \"menu\", \"ordering food\").\n\toa_rel(\"can\", \"kite\", \"fly\").\n\toa_rel(\"can\", \"truck\", \"move heavy loads\").\n\toa_rel(\"is used for\", \"blanket\", \"covering things\").\n", "oa_rel(\"is used for\", \"church\", \"weddings\").\n\toa_rel(\"is used for\", \"court\", \"playing tennis\").\n\toa_rel(\"is used for\", \"bus\", \"transporting lots of people\").\n\toa_rel(\"can\", \"lamp\", \"illuminate\").\n\toa_rel(\"is used for\", \"computer\", \"finding information\").\n\toa_rel(\"usually appears in\", \"salt shaker\", \"dining room\").\n\toa_rel(\"is used for\", \"rug\", \"standing on\").\n\toa_rel(\"is used for\", \"camera\", \"taking pictures\").\n\toa_rel(\"usually appears in\", \"candle\", \"restaurant\").\n\toa_rel(\"is used for\", \"door\", \"separating rooms\").\n", "oa_rel(\"can\", \"knife\", \"cut that cake\").\n\toa_rel(\"usually appears in\", \"spoon\", \"dining room\").\n\toa_rel(\"is used for\", \"bowl\", \"holding apples\").\n\toa_rel(\"can\", \"knives\", \"cut food\").\n\toa_rel(\"can\", \"oven\", \"roast\").\n\toa_rel(\"is a sub-event of\", \"baking cake\", \"preheating oven\").\n\toa_rel(\"is used for\", \"pot\", \"holding liquid\").\n\toa_rel(\"is used for\", \"soap\", \"cleaning somethings\").\n\toa_rel(\"requires\", \"cleaning clothing\", \"soap\").\n\toa_rel(\"has\", \"avocado\", \"vitamin B\").\n", "oa_rel(\"can be\", \"vegetable\", \"eaten\").\n\toa_rel(\"is\", \"mouse\", \"electric\").\n\toa_rel(\"is used for\", \"desk\", \"writing upon\").\n\toa_rel(\"is used for\", \"keyboard\", \"typing\").\n\toa_rel(\"usually appears in\", \"toothbrush\", \"bathroom\").\n\toa_rel(\"usually appears in\", \"bathtub\", \"bathroom\").\n\toa_rel(\"is used for\", \"luggage\", \"carrying clothes on trip\").\n\toa_rel(\"is used for\", \"microwave\", \"cooking food fast\").\n\toa_rel(\"can\", \"oven\", \"warm meal\").\n\toa_rel(\"requires\", \"roasting\", \"oven\").\n", "oa_rel(\"can be\", \"shirt\", \"hung\").\n\toa_rel(\"is used for\", \"home\", \"dwelling\").\n\toa_rel(\"is used for\", \"fence\", \"marking property lines\").\n\toa_rel(\"can\", \"baseball\", \"travel very fast\").\n\toa_rel(\"is used for\", \"platform\", \"standing on\").\n\toa_rel(\"is\", \"wine\", \"fluid\").\n\toa_rel(\"is used for\", \"glasses\", \"correcting vision\").\n\toa_rel(\"can\", \"bench\", \"seat people\").\n\toa_rel(\"usually appears in\", \"toilet brush\", \"bathroom\").\n\toa_rel(\"is used for\", \"bathroom\", \"peeing\").\n", "oa_rel(\"requires\", \"washing dishes\", \"sink\").\n\toa_rel(\"is used for\", \"refrigerator\", \"keeping food cold\").\n\toa_rel(\"is\", \"stove\", \"electric\").\n\toa_rel(\"usually appears in\", \"veil\", \"wedding\").\n\toa_rel(\"is\", \"cigarette\", \"harmful\").\n\toa_rel(\"is used for\", \"beverage\", \"drinking\").\n\toa_rel(\"usually appears in\", \"soup\", \"dinner\").\n\toa_rel(\"is used for\", \"bowl\", \"holding soup\").\n\toa_rel(\"is used for\", \"airplane\", \"traversing skies\").\n\toa_rel(\"is used for\", \"cup\", \"holding drinks\").\n", "oa_rel(\"has\", \"bread\", \"starch\").\n\toa_rel(\"is used for\", \"phone\", \"surfing internet\").\n\toa_rel(\"can\", \"truck\", \"ship goods\").\n\toa_rel(\"is used for\", \"gas station\", \"buying gas\").\n\toa_rel(\"can\", \"sedan\", \"travel on road\").\n\toa_rel(\"is used for\", \"baseball field\", \"playing baseball\").\n\toa_rel(\"is used for\", \"hospital\", \"healing sick people\").\n\toa_rel(\"can\", \"heater\", \"heat room\").\n\toa_rel(\"usually appears in\", \"drink\", \"bar\").\n\toa_rel(\"usually appears in\", \"folder\", \"office\").\n", "oa_rel(\"is\", \"ocean\", \"liquid\").\n\toa_rel(\"is used for\", \"meat\", \"eating\").\n\toa_rel(\"is used for\", \"toilet\", \"depositing human waste\").\n\toa_rel(\"is used for\", \"blanket\", \"covering bed\").\n\toa_rel(\"can\", \"water\", \"dribble\").\n\toa_rel(\"is used for\", \"bicycle\", \"transporting handful of people\").\n\toa_rel(\"is used for\", \"belt\", \"holding pants\").\n\toa_rel(\"can\", \"donkey\", \"carry load of supplies\").\n\toa_rel(\"usually appears in\", \"closet\", \"bedroom\").\n\toa_rel(\"is used for\", \"couch\", \"sleeping\").\n", "oa_rel(\"is\", \"radio\", \"electric\").\n\toa_rel(\"can\", \"computer\", \"cost lot of money\").\n\toa_rel(\"is used for\", \"bed\", \"resting\").\n\toa_rel(\"can be\", \"umbrella\", \"opened or closed\").\n\toa_rel(\"is used for\", \"dumpster\", \"storing trash\").\n\toa_rel(\"is used for\", \"frisbee\", \"exercise\").\n\toa_rel(\"can\", \"boat\", \"sail on pond\").\n\toa_rel(\"requires\", \"baking bread\", \"oven\").\n\toa_rel(\"is\", \"ice cream\", \"sticky\").\n\toa_rel(\"can\", \"cup\", \"hold coffee\").\n", "oa_rel(\"has\", \"coffee\", \"water\").\n\toa_rel(\"can\", \"dog\", \"shake hands\").\n\toa_rel(\"is used for\", \"mall\", \"gathering shops\").\n\toa_rel(\"is used for\", \"canoe\", \"traveling on water\").\n\toa_rel(\"is used for\", \"canoe\", \"transporting people\").\n\toa_rel(\"is used for\", \"ship\", \"transporting people\").\n\toa_rel(\"usually appears in\", \"cup\", \"restaurant\").\n\toa_rel(\"usually appears in\", \"mug\", \"dining room\").\n\toa_rel(\"is\", \"bear\", \"carnivorous\").\n\toa_rel(\"is used for\", \"couch\", \"taking nap\").\n", "oa_rel(\"is used for\", \"beverage\", \"satisfying thirst\").\n\toa_rel(\"is used for\", \"airplane\", \"transporting people\").\n\toa_rel(\"can\", \"airplane\", \"seat passengers\").\n\toa_rel(\"usually appears in\", \"chair\", \"restaurant\").\n\toa_rel(\"has\", \"broccoli\", \"vitamin B\").\n\toa_rel(\"is\", \"ketchup\", \"fluid\").\n\toa_rel(\"usually appears in\", \"elephant\", \"grassland\").\n\toa_rel(\"is used for\", \"living room\", \"entertaining guests\").\n\toa_rel(\"is used for\", \"sofa\", \"resting\").\n\toa_rel(\"is used for\", \"ship\", \"transporting goods\").\n", "oa_rel(\"can\", \"ruler\", \"measure distance\").\n\toa_rel(\"can\", \"docks\", \"shore boats\").\n\toa_rel(\"is used for\", \"calculator\", \"doing mathematical calculations\").\n\toa_rel(\"is made from\", \"ice cream\", \"milk\").\n\toa_rel(\"is used for\", \"shelf\", \"storing dishes\").\n\toa_rel(\"has\", \"juice\", \"water\").\n\toa_rel(\"has\", \"milk\", \"vitamin B\").\n\toa_rel(\"is\", \"ketchup\", \"sticky\").\n\toa_rel(\"can\", \"hammer\", \"hit nail\").\n\toa_rel(\"can\", \"axe\", \"chop wood\").\n", "oa_rel(\"is used for\", \"scissors\", \"cutting ribbons\").\n\toa_rel(\"usually appears in\", \"grapes\", \"salad\").\n\toa_rel(\"is used for\", \"traffic light\", \"controlling flows of traffic\").\n\toa_rel(\"can\", \"washing machine\", \"clean clothes\").\n\toa_rel(\"is used for\", \"desk\", \"working\").\n\toa_rel(\"requires\", \"barbecue\", \"grill\").\n\toa_rel(\"is used for\", \"blender\", \"mixing food\").\n\toa_rel(\"is used for\", \"highway\", \"driving car on\").\n\toa_rel(\"is used for\", \"frisbee\", \"catching\").\n\toa_rel(\"can\", \"rain\", \"cause floods\").\n", "oa_rel(\"usually appears in\", \"candle\", \"birthday party\").\n\toa_rel(\"is used for\", \"sailboat\", \"floating and moving on water\").\n\toa_rel(\"is used for\", \"pot\", \"boiling water\").\n\toa_rel(\"is used for\", \"pot\", \"planting plant\").\n\toa_rel(\"is used for\", \"pen\", \"writing\").\n\toa_rel(\"is used for\", \"fruit\", \"eating\").\n\toa_rel(\"can\", \"steering wheel\", \"control car\").\n\toa_rel(\"is used for\", \"airplane\", \"transporting goods\").\n\toa_rel(\"is used for\", \"bathroom\", \"washing hands\").\n\toa_rel(\"usually appears in\", \"tray\", \"dining room\").\n", "oa_rel(\"can\", \"pitcher\", \"strike out baseball hitter\").\n\toa_rel(\"is used for\", \"earring\", \"decoration\").\n\toa_rel(\"is used for\", \"bathtub\", \"cleaning body\").\n\toa_rel(\"is used for\", \"truck\", \"transporting goods\").\n\toa_rel(\"is used for\", \"doll\", \"playing with child\").\n\toa_rel(\"is used for\", \"binder\", \"binding papers together\").\n\toa_rel(\"usually appears in\", \"cup\", \"dining room\").\n\toa_rel(\"is used for\", \"engine\", \"powering\").\n\toa_rel(\"is used for\", \"home\", \"housing family\").\n\toa_rel(\"is used for\", \"book\", \"getting knowledge\").\n", "oa_rel(\"can\", \"cat\", \"live in house\").\n\toa_rel(\"usually appears in\", \"tissue box\", \"bathroom\").\n\toa_rel(\"is used for\", \"bowl\", \"holding cream\").\n\toa_rel(\"is\", \"tea\", \"liquid\").\n\toa_rel(\"is used for\", \"bottle\", \"storing liquids\").\n\toa_rel(\"can\", \"bird\", \"sing\").\n\toa_rel(\"is used for\", \"fence\", \"creating privacy\").\n\toa_rel(\"is\", \"apple\", \"sweet\").\n\toa_rel(\"can\", \"knife\", \"cut apple\").\n\toa_rel(\"is used for\", \"seat\", \"waiting\").\n", "oa_rel(\"is used for\", \"fruit\", \"making juice\").\n\toa_rel(\"can\", \"fish\", \"swim\").\n\toa_rel(\"can\", \"microwave\", \"heat food\").\n\toa_rel(\"is used for\", \"cup\", \"handling liquid\").\n\toa_rel(\"is used for\", \"blanket\", \"keeping warm when sleeping\").\n\toa_rel(\"is used for\", \"carpet\", \"covering floor\").\n\toa_rel(\"is used for\", \"market\", \"selling foodstuffs\").\n\toa_rel(\"is used for\", \"wii\", \"playing video games\").\n\toa_rel(\"can\", \"knife\", \"hurt you\").\n\toa_rel(\"requires\", \"dicing\", \"knife\").\n", "oa_rel(\"can\", \"corn\", \"provide complex carbohydrates\").\n\toa_rel(\"can\", \"truck\", \"use diesel fuel\").\n\toa_rel(\"is used for\", \"stove\", \"heating food\").\n\toa_rel(\"can\", \"boat\", \"take you to island\").\n\toa_rel(\"can\", \"train\", \"carry freight\").\n\toa_rel(\"is used for\", \"cake\", \"celebrating someones special event\").\n\toa_rel(\"is used for\", \"bell\", \"getting people's attention\").\n\toa_rel(\"can\", \"noodle\", \"provide complex carbohydrates\").\n\toa_rel(\"is used for\", \"train\", \"transporting lots of people\").\n\toa_rel(\"is used for\", \"refrigerator\", \"chilling food\").\n", "oa_rel(\"requires\", \"washing dishes\", \"dishwasher\").\n\toa_rel(\"is used for\", \"menu\", \"listing choices\").\n\toa_rel(\"is used for\", \"phone\", \"talking to someone\").\n\toa_rel(\"is used for\", \"doll\", \"having fun\").\n\toa_rel(\"is used for\", \"shelf\", \"storing books\").\n\toa_rel(\"can\", \"toilet\", \"flush\").\n\toa_rel(\"can\", \"stove\", \"heat pot\").\n\toa_rel(\"usually appears in\", \"bowl\", \"kitchen\").\n\toa_rel(\"is used for\", \"kitchen\", \"eating food\").\n\toa_rel(\"usually appears in\", \"sink\", \"kitchen\").\n", "oa_rel(\"is used for\", \"kettle\", \"boiling water\").\n\toa_rel(\"is used for\", \"sink\", \"soaking dishes\").\n\toa_rel(\"is used for\", \"soap\", \"washing dishes\").\n\toa_rel(\"usually appears in\", \"rag\", \"living room\").\n\toa_rel(\"is used for\", \"sink\", \"washing dishes\").\n\toa_rel(\"usually appears in\", \"blender\", \"kitchen\").\n\toa_rel(\"requires\", \"blending\", \"blender\").\n\toa_rel(\"usually appears in\", \"plate\", \"dining room\").\n\toa_rel(\"requires\", \"toasting\", \"toaster\").\n\toa_rel(\"can\", \"refrigerator\", \"store food for long times\").\n", "oa_rel(\"usually appears in\", \"oven\", \"kitchen\").\n\toa_rel(\"requires\", \"baking\", \"oven\").\n\toa_rel(\"usually appears in\", \"dishwasher\", \"kitchen\").\n\toa_rel(\"is used for\", \"stove\", \"cooking stew\").\n\toa_rel(\"requires\", \"making coffee\", \"coffee maker\").\n\toa_rel(\"usually appears in\", \"towel rack\", \"bathroom\").\n\toa_rel(\"is used for\", \"stove\", \"grilling steak\").\n\toa_rel(\"can\", \"oven\", \"warm pie\").\n\toa_rel(\"requires\", \"baking cake\", \"oven\").\n\toa_rel(\"can\", \"stove\", \"heat pot of water\").\n", "oa_rel(\"requires\", \"boiling\", \"cooking pot\").\n\toa_rel(\"usually appears in\", \"stove\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"placemat\", \"dining room\").\n\toa_rel(\"usually appears in\", \"microwave\", \"kitchen\").\n\toa_rel(\"is used for\", \"mug\", \"holding drinks\").\n\toa_rel(\"usually appears in\", \"pan\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"towel\", \"bathroom\").\n\toa_rel(\"is\", \"dryer\", \"electric\").\n\toa_rel(\"is used for\", \"living room\", \"having party\").\n\toa_rel(\"is a sub-event of\", \"cleaning clothing\", \"putting clothing in washer\").\n", "oa_rel(\"can\", \"coffee maker\", \"making coffee\").\n\toa_rel(\"can\", \"microwave\", \"warm up coffee\").\n\toa_rel(\"is used for\", \"camera\", \"photography\").\n\toa_rel(\"requires\", \"mincing\", \"knife\").\n\toa_rel(\"can\", \"bridge\", \"cross river\").\n\toa_rel(\"usually appears in\", \"brush\", \"bathroom\").\n\toa_rel(\"is used for\", \"river\", \"transportation\").\n\toa_rel(\"can\", \"bird\", \"land on branch\").\n\toa_rel(\"usually appears in\", \"coffee cup\", \"restaurant\").\n\toa_rel(\"is\", \"coffee maker\", \"electric\").\n", "oa_rel(\"is\", \"dishwasher\", \"electric\").\n\toa_rel(\"is used for\", \"closet\", \"storing clothes\").\n\toa_rel(\"can\", \"water\", \"be liquid, ice, or steam\").\n\toa_rel(\"is used for\", \"temple\", \"praying\").\n\toa_rel(\"is used for\", \"carpet\", \"covering ugly floor\").\n\toa_rel(\"is a sub-event of\", \"cleaning house\", \"vacuuming carpet\").\n\toa_rel(\"can\", \"airplane\", \"land in field\").\n\toa_rel(\"is used for\", \"closet\", \"hanging clothes\").\n\toa_rel(\"can\", \"dog\", \"guide blind\").\n\toa_rel(\"can\", \"umbrella\", \"shade you from sun\").\n", "oa_rel(\"is used for\", \"pot\", \"making soup\").\n\toa_rel(\"is used for\", \"sidewalk\", \"riding bike on\").\n\toa_rel(\"is\", \"fax machine\", \"electric\").\n\toa_rel(\"is used for\", \"keyboard\", \"coding\").\n\toa_rel(\"is\", \"sheep\", \"herbivorous\").\n\toa_rel(\"is used for\", \"pen\", \"drawing\").\n\toa_rel(\"usually appears in\", \"mouse\", \"office\").\n\toa_rel(\"is used for\", \"dining room\", \"eating\").\n\toa_rel(\"usually appears in\", \"bowl\", \"restaurant\").\n\toa_rel(\"has\", \"orange\", \"vitamin C\").\n", "oa_rel(\"is used for\", \"sheet\", \"sleeping on\").\n\toa_rel(\"is used for\", \"fork\", \"eating pie\").\n\toa_rel(\"requires\", \"making pizza\", \"salt\").\n\toa_rel(\"is\", \"knife\", \"dangerous\").\n\toa_rel(\"is used for\", \"knife\", \"cutting food\").\n\toa_rel(\"is used for\", \"fork\", \"lifting food from plate to mouth\").\n\toa_rel(\"usually appears in\", \"saucer\", \"dining room\").\n\toa_rel(\"is\", \"cream\", \"fluid\").\n\toa_rel(\"requires\", \"beating egg\", \"spoon\").\n\toa_rel(\"usually appears in\", \"straw\", \"restaurant\").\n", "oa_rel(\"is used for\", \"salt\", \"salting food\").\n\toa_rel(\"usually appears in\", \"placemat\", \"restaurant\").\n\toa_rel(\"is made from\", \"pasta\", \"flour\").\n\toa_rel(\"requires\", \"baking cake\", \"egg\").\n\toa_rel(\"is\", \"salt\", \"salty\").\n\toa_rel(\"has\", \"strawberry\", \"vitamin C\").\n\toa_rel(\"is made from\", \"butter\", \"milk\").\n\toa_rel(\"is a sub-event of\", \"baking cake\", \"mixing butter with sugar\").\n\toa_rel(\"is\", \"oatmeal\", \"fluid\").\n\toa_rel(\"is made from\", \"juice\", \"fruit\").\n", "oa_rel(\"is made from\", \"casserole\", \"flour\").\n\toa_rel(\"requires\", \"cutting\", \"knife\").\n\toa_rel(\"has\", \"rice\", \"starch\").\n\toa_rel(\"requires\", \"chopping\", \"knife\").\n\toa_rel(\"usually appears in\", \"stool\", \"bar\").\n\toa_rel(\"is used for\", \"refrigerator\", \"making ice\").\n\toa_rel(\"can\", \"oven\", \"heat meal\").\n\toa_rel(\"is a sub-event of\", \"baking bread\", \"preheating oven\").\n\toa_rel(\"can\", \"boat\", \"sail through sea\").\n\toa_rel(\"is used for\", \"scissors\", \"cutting string\").\n", "oa_rel(\"requires\", \"mincing\", \"scissors\").\n\toa_rel(\"is used for\", \"living room\", \"watching tv\").\n\toa_rel(\"usually appears in\", \"couch\", \"living room\").\n\toa_rel(\"is used for\", \"stove\", \"frying burgers\").\n\toa_rel(\"is used for\", \"ballon\", \"decoration\").\n\toa_rel(\"is used for\", \"drum\", \"banging\").\n\toa_rel(\"is\", \"beer\", \"harmful\").\n\toa_rel(\"can\", \"bicycle\", \"travel on road\").\n\toa_rel(\"is used for\", \"computer\", \"doing mathematical calculations\").\n\toa_rel(\"usually appears in\", \"chair\", \"living room\").\n", "oa_rel(\"is used for\", \"projector\", \"showing presentations\").\n\toa_rel(\"can\", \"computer\", \"save information\").\n\toa_rel(\"is used for\", \"lobby\", \"meeting guests\").\n\toa_rel(\"can\", \"fence\", \"divide property\").\n\toa_rel(\"is used for\", \"container\", \"holding something\").\n\toa_rel(\"is used for\", \"sailboat\", \"traveling on water\").\n\toa_rel(\"can\", \"sailboat\", \"travel over water\").\n\toa_rel(\"is used for\", \"mailbox\", \"sending letters\").\n\toa_rel(\"is used for\", \"factory\", \"manufacture goods\").\n\toa_rel(\"is used for\", \"broom\", \"sweeping floors\").\n", "oa_rel(\"requires\", \"cleaning house\", \"broom\").\n\toa_rel(\"is used for\", \"scooter\", \"riding\").\n\toa_rel(\"can\", \"airplanes\", \"carry people\").\n\toa_rel(\"can\", \"hammer\", \"strike nail\").\n\toa_rel(\"is used for\", \"van\", \"transporting people\").\n\toa_rel(\"can\", \"refrigerator\", \"cool milk\").\n\toa_rel(\"is used for\", \"bracelet\", \"decoration\").\n\toa_rel(\"can\", \"ruler\", \"guide lines\").\n\toa_rel(\"usually appears in\", \"dresser\", \"bedroom\").\n\toa_rel(\"can\", \"snake\", \"eat egg\").\n", "oa_rel(\"requires\", \"beating egg\", \"mixer\").\n\toa_rel(\"is used for\", \"spoon\", \"moving liquid food to mouth\").\n\toa_rel(\"is used for\", \"spoon\", \"moving food to mouth\").\n\toa_rel(\"is made from\", \"sandwich\", \"flour\").\n\toa_rel(\"is used for\", \"elephant\", \"transporting people\").\n\toa_rel(\"is used for\", \"house\", \"housing family\").\n\toa_rel(\"can\", \"rat\", \"live in house\").\n\toa_rel(\"is used for\", \"carpet\", \"protecting feet from floor\").\n\toa_rel(\"is used for\", \"church\", \"worship\").\n\toa_rel(\"can\", \"elephant\", \"weight 1000 kilos\").\n", "oa_rel(\"is used for\", \"fan\", \"cooling people\").\n\toa_rel(\"is used for\", \"curtain\", \"get privacy\").\n\toa_rel(\"is made from\", \"pizza slice\", \"flour\").\n\toa_rel(\"is used for\", \"stapler\", \"stapling papers together\").\n\toa_rel(\"usually appears in\", \"mug\", \"bar\").\n\toa_rel(\"is used for\", \"boat\", \"keeping people out of water\").\n\toa_rel(\"is made from\", \"chocolate\", \"cacao seeds\").\n\toa_rel(\"can\", \"squirrel\", \"store nuts for winter\").\n\toa_rel(\"is used for\", \"airplane\", \"carrying cargo\").\n\toa_rel(\"usually appears in\", \"tractor\", \"farm\").\n", "oa_rel(\"can\", \"stove\", \"heat food\").\n\toa_rel(\"has\", \"banana\", \"vitamin B\").\n\toa_rel(\"is a sub-event of\", \"making juice\", \"cutting fruit\").\n\toa_rel(\"is used for\", \"oven\", \"preparing food\").\n\toa_rel(\"is used for\", \"museum\", \"displaying old objects\").\n\toa_rel(\"is used for\", \"train station\", \"waiting train\").\n\toa_rel(\"can\", \"frisbee\", \"descend slowly by hovering\").\n\toa_rel(\"can\", \"cat\", \"eat meat\").\n\toa_rel(\"can\", \"batter\", \"hit baseball\").\n\toa_rel(\"is used for\", \"computer\", \"storing information\").\n", "oa_rel(\"can\", \"camera\", \"record scene\").\n\toa_rel(\"usually appears in\", \"cattle\", \"farm\").\n\toa_rel(\"can\", \"hammer\", \"nail board\").\n\toa_rel(\"can\", \"bird\", \"spread wings\").\n\toa_rel(\"is\", \"keyboard\", \"electric\").\n\toa_rel(\"is used for\", \"stapler\", \"holding papers together\").\n\toa_rel(\"can\", \"computer\", \"power down\").\n\toa_rel(\"is used for\", \"thermometer\", \"measuring temperature\").\n\toa_rel(\"is used for\", \"runway\", \"landing airplanes\").\n\toa_rel(\"has\", \"zebra\", \"stripes\").\n", "oa_rel(\"can\", \"baseball bat\", \"hit baseball\").\n\toa_rel(\"usually appears in\", \"hair dryer\", \"bathroom\").\n\toa_rel(\"can\", \"ambulance\", \"travel on road\").\n\toa_rel(\"is used for\", \"sofa\", \"sitting on\").\n\toa_rel(\"can\", \"dog\", \"please human\").\n\toa_rel(\"can\", \"raft\", \"travel over water\").\n\toa_rel(\"is used for\", \"freeway\", \"driving car on\").\n\toa_rel(\"is a sub-event of\", \"cleaning clothing\", \"hanging clothing up\").\n\toa_rel(\"is used for\", \"lighthouse\", \"signaling danger\").\n\toa_rel(\"is used for\", \"airplane\", \"travelling long distances\").\n", "oa_rel(\"can\", \"monkeys\", \"climb tree\").\n\toa_rel(\"is used for\", \"train\", \"carrying cargo\").\n\toa_rel(\"is used for\", \"printer\", \"printing pictures\").\n\toa_rel(\"is used for\", \"printer\", \"printing books\").\n\toa_rel(\"is used for\", \"train\", \"travelling long distances\").\n\toa_rel(\"can\", \"elephant\", \"life tree\").\n\toa_rel(\"can\", \"cat\", \"wash itself\").\n\toa_rel(\"can\", \"train\", \"arrive at city\").\n\toa_rel(\"is used for\", \"library\", \"finding information\").\n\toa_rel(\"can\", \"turtle\", \"live in house\").\n", "oa_rel(\"can\", \"pigeon\", \"fly\").\n\toa_rel(\"can\", \"axe\", \"hurt people\").\n\toa_rel(\"can\", \"cat\", \"please humans\").\n\toa_rel(\"has\", \"hot chocolate\", \"caffeine\").\n\toa_rel(\"is\", \"sponge\", \"soft\").\n\toa_rel(\"usually appears in\", \"stapler\", \"office\").\n\toa_rel(\"is used for\", \"scooter\", \"transporting people\").\n\toa_rel(\"usually appears in\", \"telephone\", \"office\").\n\toa_rel(\"can\", \"toaster\", \"brown toast\").\n\toa_rel(\"can\", \"bird\", \"lay eggs\").\n", "oa_rel(\"is used for\", \"mailbox\", \"receiving packages\").\n\toa_rel(\"is used for\", \"restaurant\", \"meeting friends\").\n\toa_rel(\"can\", \"rabbit\", \"live in house\").\n\toa_rel(\"usually appears in\", \"tv\", \"living room\").\n\toa_rel(\"usually appears in\", \"straw\", \"dining room\").\n\toa_rel(\"has\", \"pineapple\", \"vitamin C\").\n\toa_rel(\"usually appears in\", \"potato\", \"salad\").\n\toa_rel(\"is\", \"orange\", \"sour\").\n\toa_rel(\"is used for\", \"stool\", \"resting\").\n\toa_rel(\"is used for\", \"piano\", \"playing music\").\n", "oa_rel(\"can\", \"suv\", \"transport people\").\n\toa_rel(\"is used for\", \"grill\", \"grilling hamburgers\").\n\toa_rel(\"can\", \"ship\", \"near island\").\n\toa_rel(\"is used for\", \"phones\", \"listening to music\").\n\toa_rel(\"is\", \"refrigerator\", \"electric\").\n\toa_rel(\"usually appears in\", \"guitar\", \"rock band\").\n\toa_rel(\"requires\", \"making juice\", \"juicer\").\n\toa_rel(\"is used for\", \"subway\", \"transporting people\").\n\toa_rel(\"is used for\", \"train car\", \"transporting people\").\n\toa_rel(\"is made from\", \"fries\", \"potato\").\n", "oa_rel(\"is\", \"chicken\", \"omnivorous\").\n\toa_rel(\"is used for\", \"oven\", \"heating food\").\n\toa_rel(\"is used for\", \"oven\", \"baking food\").\n\toa_rel(\"usually appears in\", \"container\", \"kitchen\").\n\toa_rel(\"is used for\", \"fork\", \"eating solid food\").\n\toa_rel(\"requires\", \"making pizza\", \"vegetables\").\n\toa_rel(\"is\", \"puppy\", \"soft\").\n\toa_rel(\"can\", \"owl\", \"see at night\").\n\toa_rel(\"can\", \"pilot\", \"fly airplane\").\n\toa_rel(\"is used for\", \"keyboard\", \"typing letters onto windows\").\n", "oa_rel(\"can\", \"charger\", \"charge battery\").\n\toa_rel(\"can\", \"computer\", \"stream video\").\n\toa_rel(\"is used for\", \"dresser\", \"holding cloth\").\n\toa_rel(\"usually appears in\", \"hay\", \"barn\").\n\toa_rel(\"is used for\", \"mailbox\", \"receiving bills\").\n\toa_rel(\"is used for\", \"frisbee\", \"entertainment\").\n\toa_rel(\"usually appears in\", \"water glass\", \"dining room\").\n\toa_rel(\"can\", \"train\", \"transport many people at once\").\n\toa_rel(\"can\", \"horse\", \"jump over objects\").\n\toa_rel(\"can\", \"dog\", \"follow its master\").\n", "oa_rel(\"usually appears in\", \"penguin\", \"ocean\").\n\toa_rel(\"can\", \"bear\", \"eat most types of food\").\n\toa_rel(\"is used for\", \"couch\", \"relaxing\").\n\toa_rel(\"is used for\", \"fireplace\", \"getting warm\").\n\toa_rel(\"usually appears in\", \"refrigerator\", \"kitchen\").\n\toa_rel(\"is used for\", \"phone\", \"finding information\").\n\toa_rel(\"is\", \"horse\", \"herbivorous\").\n\toa_rel(\"requires\", \"making juice\", \"fruit\").\n\toa_rel(\"has\", \"egg\", \"vitamin B\").\n\toa_rel(\"is a sub-event of\", \"baking cake\", \"mixing egg with sugar\").\n", "oa_rel(\"usually appears in\", \"turkey\", \"thanksgiving\").\n\toa_rel(\"is used for\", \"waste basket\", \"storing trash\").\n\toa_rel(\"is used for\", \"cake\", \"celebrating birthday\").\n\toa_rel(\"is used for\", \"cake\", \"eating\").\n\toa_rel(\"usually appears in\", \"tissue\", \"bathroom\").\n\toa_rel(\"is used for\", \"speaker\", \"producing sound\").\n\toa_rel(\"usually appears in\", \"bear\", \"jungle\").\n\toa_rel(\"has\", \"monkey\", \"two legs\").\n\toa_rel(\"usually appears in\", \"mat\", \"living room\").\n\toa_rel(\"is used for\", \"fork\", \"piercing food\").\n", "oa_rel(\"is\", \"juice\", \"liquid\").\n\toa_rel(\"can\", \"kitten\", \"live in house\").\n\toa_rel(\"is used for\", \"barn\", \"storing farming equipment\").\n\toa_rel(\"is used for\", \"chopsticks\", \"moving food to mouth\").\n\toa_rel(\"is\", \"soup\", \"fluid\").\n\toa_rel(\"is used for\", \"fruit stand\", \"buying and selling fruit\").\n\toa_rel(\"can\", \"horse\", \"carry riders\").\n\toa_rel(\"usually appears in\", \"curtain\", \"bedroom\").\n\toa_rel(\"is used for\", \"dog\", \"herding sheep\").\n\toa_rel(\"can\", \"cat\", \"be companion\").\n", "oa_rel(\"usually appears in\", \"dining table\", \"dining room\").\n\toa_rel(\"is made from\", \"bun\", \"flour\").\n\toa_rel(\"has\", \"egg\", \"iron\").\n\toa_rel(\"is\", \"microwave\", \"electric\").\n\toa_rel(\"can\", \"bear\", \"hunt rabbit\").\n\toa_rel(\"is\", \"juice\", \"fluid\").\n\toa_rel(\"requires\", \"mixing\", \"mixing bowl\").\n\toa_rel(\"is used for\", \"cutting board\", \"cutting food\").\n\toa_rel(\"requires\", \"baking cake\", \"butter\").\n\toa_rel(\"is used for\", \"bread\", \"making toast\").\n", "oa_rel(\"has\", \"beans\", \"starch\").\n\toa_rel(\"is made from\", \"toast\", \"flour\").\n\toa_rel(\"usually appears in\", \"mousepad\", \"office\").\n\toa_rel(\"can\", \"dog\", \"dig holes in yard\").\n\toa_rel(\"has\", \"nut\", \"vitamin B\").\n\toa_rel(\"requires\", \"frying\", \"frying pan\").\n\toa_rel(\"is made from\", \"macaroni\", \"flour\").\n\toa_rel(\"usually appears in\", \"glass\", \"dining room\").\n\toa_rel(\"is used for\", \"bathroom\", \"clean humans\").\n\toa_rel(\"is used for\", \"frisbee\", \"throwing\").\n", "oa_rel(\"usually appears in\", \"bedspread\", \"bedroom\").\n\toa_rel(\"is used for\", \"tongs\", \"grasping food\").\n\toa_rel(\"is used for\", \"vegetable\", \"eating\").\n\toa_rel(\"requires\", \"mixing\", \"spoon\").\n\toa_rel(\"has\", \"tomato\", \"vitamin C\").\n\toa_rel(\"is\", \"peach\", \"sweet\").\n\toa_rel(\"is a sub-event of\", \"making pizza\", \"baking pizza in oven\").\n\toa_rel(\"is\", \"heater\", \"electric\").\n\toa_rel(\"is used for\", \"guitar\", \"playing chords\").\n\toa_rel(\"can\", \"jellyfish\", \"hurt person\").\n", "oa_rel(\"is used for\", \"ipod\", \"listening to music\").\n\toa_rel(\"can\", \"vacuum\", \"clean carpet\").\n\toa_rel(\"is used for\", \"harbor\", \"store boats\").\n\toa_rel(\"is\", \"butter\", \"sticky\").\n\toa_rel(\"can\", \"refrigerator\", \"cool warm food\").\n\toa_rel(\"is used for\", \"binder\", \"holding papers together\").\n\toa_rel(\"is\", \"vegetables\", \"healthy\").\n\toa_rel(\"usually appears in\", \"napkin\", \"restaurant\").\n\toa_rel(\"is\", \"beer\", \"liquid\").\n\toa_rel(\"has\", \"pancake\", \"starch\").\n", "oa_rel(\"can\", \"frisbee\", \"fly\").\n\toa_rel(\"is used for\", \"bathroom\", \"washing up\").\n\toa_rel(\"is used for\", \"restaurant\", \"eating\").\n\toa_rel(\"can\", \"van\", \"spend gas\").\n\toa_rel(\"can\", \"rain\", \"wet clothes\").\n\toa_rel(\"is used for\", \"ship\", \"transportation at sea\").\n\toa_rel(\"is\", \"ice cream\", \"sweet\").\n\toa_rel(\"is made from\", \"cake\", \"flour\").\n\toa_rel(\"can\", \"school bus\", \"travel on road\").\n\toa_rel(\"can\", \"airplane\", \"circle airfield\").\n", "oa_rel(\"usually appears in\", \"tablecloth\", \"restaurant\").\n\toa_rel(\"is used for\", \"airplane\", \"transporting lots of people\").\n\toa_rel(\"can\", \"turtle\", \"hide in its shell\").\n\toa_rel(\"can\", \"oven\", \"brown chicken\").\n\toa_rel(\"usually appears in\", \"nightstand\", \"bedroom\").\n\toa_rel(\"usually appears in\", \"bowl\", \"dining room\").\n\toa_rel(\"is used for\", \"pencil\", \"drawing\").\n\toa_rel(\"has\", \"cheese\", \"calcium\").\n\toa_rel(\"is a sub-event of\", \"cleaning house\", \"vacuuming floors\").\n\toa_rel(\"can\", \"cat\", \"sleep most of day\").\n", "oa_rel(\"usually appears in\", \"faucet\", \"bathroom\").\n\toa_rel(\"has\", \"soda\", \"water\").\n\toa_rel(\"is used for\", \"bell\", \"making noise\").\n\toa_rel(\"usually appears in\", \"hair clip\", \"bathroom\").\n\toa_rel(\"can\", \"dishwasher\", \"wash dirty dishes\").\n\toa_rel(\"is used for\", \"shop\", \"buying and selling\").\n\toa_rel(\"usually appears in\", \"rolling pin\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"toaster oven\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"waiter\", \"restaurant\").\n\toa_rel(\"usually appears in\", \"lobster\", \"water\").\n", "oa_rel(\"has\", \"lemon\", \"vitamin C\").\n\toa_rel(\"is used for\", \"condiment\", \"flavoring food\").\n\toa_rel(\"can\", \"ice maker\", \"making ice\").\n\toa_rel(\"has\", \"beer\", \"water\").\n\toa_rel(\"usually appears in\", \"wine glass\", \"dining room\").\n\toa_rel(\"can\", \"computer\", \"boot from hard drive\").\n\toa_rel(\"can\", \"cell phone\", \"ring\").\n\toa_rel(\"is used for\", \"rug\", \"prevent scratches on floor\").\n\toa_rel(\"usually appears in\", \"keyboard\", \"office\").\n\toa_rel(\"is used for\", \"hair dryer\", \"dry hair\").\n", "oa_rel(\"is used for\", \"apartment\", \"housing family\").\n\toa_rel(\"usually appears in\", \"cutting board\", \"kitchen\").\n\toa_rel(\"is made from\", \"pizza pie\", \"flour\").\n\toa_rel(\"usually appears in\", \"shower curtain\", \"bathroom\").\n\toa_rel(\"is used for\", \"pantry\", \"keeping food organized\").\n\toa_rel(\"has\", \"elephant\", \"long nose\").\n\toa_rel(\"is used for\", \"spoon\", \"people to eat soup with\").\n\toa_rel(\"is made from\", \"bagel\", \"flour\").\n\toa_rel(\"usually appears in\", \"bill\", \"restaurant\").\n\toa_rel(\"can\", \"knife\", \"cut potato\").\n", "oa_rel(\"has\", \"mushroom\", \"vitamin D\").\n\toa_rel(\"can\", \"computer\", \"speed up research\").\n\toa_rel(\"usually appears in\", \"pumpkin\", \"halloween\").\n\toa_rel(\"usually appears in\", \"salt shaker\", \"restaurant\").\n\toa_rel(\"can\", \"van\", \"carry few persons\").\n\toa_rel(\"usually appears in\", \"pen\", \"office\").\n\toa_rel(\"usually appears in\", \"coffee mug\", \"restaurant\").\n\toa_rel(\"has\", \"kiwi\", \"vitamin C\").\n\toa_rel(\"is\", \"pear\", \"sweet\").\n\toa_rel(\"can\", \"grain\", \"provide complex carbohydrates\").\n", "oa_rel(\"is used for\", \"screw\", \"attaching item to something else\").\n\toa_rel(\"is used for\", \"computer\", \"playing games\").\n\toa_rel(\"usually appears in\", \"coffee mug\", \"dining room\").\n\toa_rel(\"is used for\", \"soap\", \"washing clothes\").\n\toa_rel(\"is used for\", \"bookshelf\", \"storing novels\").\n\toa_rel(\"is used for\", \"hotel\", \"temporary residence\").\n\toa_rel(\"is used for\", \"computer\", \"surfing internet\").\n\toa_rel(\"usually appears in\", \"planter\", \"farm\").\n\toa_rel(\"is\", \"desert\", \"dangerous\").\n\toa_rel(\"can\", \"bird\", \"attempt to fly\").\n", "oa_rel(\"is used for\", \"air conditioner\", \"cooling air\").\n\toa_rel(\"usually appears in\", \"piano\", \"orchestra\").\n\toa_rel(\"requires\", \"making pizza\", \"cheese\").\n\toa_rel(\"is\", \"blender\", \"electric\").\n\toa_rel(\"is\", \"cliff\", \"dangerous\").\n\toa_rel(\"is used for\", \"restaurant\", \"purchasing meals\").\n\toa_rel(\"is\", \"soda\", \"fluid\").\n\toa_rel(\"is used for\", \"mailbox\", \"storing mail\").\n\toa_rel(\"can\", \"knife\", \"cut cheese\").\n\toa_rel(\"requires\", \"crushing\", \"knife\").\n", "oa_rel(\"usually appears in\", \"binder\", \"office\").\n\toa_rel(\"usually appears in\", \"toilet paper\", \"bathroom\").\n\toa_rel(\"usually appears in\", \"sheet\", \"bedroom\").\n\toa_rel(\"can\", \"air conditioner\", \"cool air\").\n\toa_rel(\"is\", \"sauce\", \"fluid\").\n\toa_rel(\"can\", \"gas stove\", \"heat food\").\n\toa_rel(\"usually appears in\", \"toilet\", \"bathroom\").\n\toa_rel(\"can\", \"refrigerator\", \"keep ice cold\").\n\toa_rel(\"usually appears in\", \"pillow\", \"bedroom\").\n\toa_rel(\"is used for\", \"dining room\", \"drinking\").\n", "oa_rel(\"is used for\", \"bread\", \"eating\").\n\toa_rel(\"is a sub-event of\", \"juicing\", \"cutting fruit in half\").\n\toa_rel(\"usually appears in\", \"fish\", \"water\").\n\toa_rel(\"can\", \"oven\", \"bake\").\n\toa_rel(\"is used for\", \"oven\", \"cooking\").\n\toa_rel(\"usually appears in\", \"saucer\", \"restaurant\").\n\toa_rel(\"is used for\", \"office\", \"holding meeting\").\n\toa_rel(\"is made from\", \"beer\", \"hops\").\n\toa_rel(\"can\", \"computer\", \"process information\").\n\toa_rel(\"is used for\", \"comb\", \"removing tangles from hair\").\n", "oa_rel(\"requires\", \"washing dishes\", \"water\").\n\toa_rel(\"is used for\", \"office\", \"working\").\n\toa_rel(\"usually appears in\", \"tap\", \"bathroom\").\n\toa_rel(\"usually appears in\", \"foil\", \"kitchen\").\n\toa_rel(\"can\", \"dog\", \"be companion\").\n\toa_rel(\"is used for\", \"raft\", \"transporting people\").\n\toa_rel(\"can\", \"helmets\", \"prevent head injuries\").\n\toa_rel(\"is\", \"sword\", \"dangerous\").\n\toa_rel(\"has\", \"beans\", \"iron\").\n\toa_rel(\"is\", \"pepper\", \"spicy\").\n", "oa_rel(\"is used for\", \"elephant\", \"riding\").\n\toa_rel(\"is used for\", \"toothbrush\", \"cleaning teeth\").\n\toa_rel(\"has\", \"broccoli\", \"vitamin C\").\n\toa_rel(\"can\", \"rice\", \"provide complex carbohydrates\").\n\toa_rel(\"can\", \"dog\", \"learn to fetch things\").\n\toa_rel(\"can be\", \"steak\", \"cut\").\n\toa_rel(\"is made from\", \"onion ring\", \"onion\").\n\toa_rel(\"has\", \"cheese\", \"vitamin D\").\n\toa_rel(\"is a sub-event of\", \"making juice\", \"running ingredients through juicer\").\n\toa_rel(\"is used for\", \"spoon\", \"eating liquids\").\n", "oa_rel(\"usually appears in\", \"knife\", \"kitchen\").\n\toa_rel(\"is\", \"pony\", \"herbivorous\").\n\toa_rel(\"can\", \"grain\", \"provide energy\").\n\toa_rel(\"is a sub-event of\", \"making coffee\", \"brewing coffee\").\n\toa_rel(\"has\", \"pastry\", \"starch\").\n\toa_rel(\"usually appears in\", \"mat\", \"bathroom\").\n\toa_rel(\"is used for\", \"teddy bear\", \"having fun\").\n\toa_rel(\"is made from\", \"cream\", \"milk\").\n\toa_rel(\"has\", \"blueberry\", \"vitamin C\").\n\toa_rel(\"is\", \"juice\", \"healthy\").\n", "oa_rel(\"can be\", \"chicken\", \"roasted\").\n\toa_rel(\"can\", \"waiter\", \"serve food\").\n\toa_rel(\"can\", \"bird\", \"be pet\").\n\toa_rel(\"is used for\", \"spoon\", \"drinking\").\n\toa_rel(\"can\", \"airplane\", \"fly\").\n\toa_rel(\"is used for\", \"sailboat\", \"transportation at sea\").\n\toa_rel(\"is\", \"lamb\", \"herbivorous\").\n\toa_rel(\"is used for\", \"barn\", \"feeding animals\").\n\toa_rel(\"usually appears in\", \"fork\", \"restaurant\").\n\toa_rel(\"is made from\", \"bread\", \"flour\").\n", "oa_rel(\"has\", \"spinach\", \"vitamin B\").\n\toa_rel(\"requires\", \"making pizza\", \"meat\").\n\toa_rel(\"can\", \"cat\", \"be pet\").\n\toa_rel(\"is used for\", \"baseball\", \"hitting\").\n\toa_rel(\"requires\", \"coring\", \"knife\").\n\toa_rel(\"is used for\", \"comb\", \"styling hair\").\n\toa_rel(\"can\", \"computer\", \"stream media\").\n\toa_rel(\"can\", \"suv\", \"carry few persons\").\n\toa_rel(\"requires\", \"cleaning clothing\", \"water\").\n\toa_rel(\"can\", \"cat\", \"jump onto table or chair\").\n", "oa_rel(\"is\", \"strawberry\", \"sweet\").\n\toa_rel(\"can\", \"catcher\", \"catch\").\n\toa_rel(\"is\", \"dog\", \"omnivorous\").\n\toa_rel(\"is used for\", \"toaster\", \"toasting bread\").\n\toa_rel(\"can be\", \"drinks\", \"drunk\").\n\toa_rel(\"can\", \"police\", \"carry gun while at work\").\n\toa_rel(\"is made from\", \"wine\", \"grapes\").\n\toa_rel(\"is\", \"cat\", \"carnivorous\").\n\toa_rel(\"is a sub-event of\", \"cleaning house\", \"polishing furniture\").\n\toa_rel(\"can\", \"bird\", \"fly\").\n", "oa_rel(\"is\", \"eagle\", \"carnivorous\").\n\toa_rel(\"is used for\", \"rug\", \"walking on\").\n\toa_rel(\"is\", \"sweet potato\", \"sweet\").\n\toa_rel(\"is used for\", \"oven\", \"roasting\").\n\toa_rel(\"is used for\", \"dog\", \"providing friendship\").\n\toa_rel(\"is\", \"gravy\", \"sticky\").\n\toa_rel(\"is used for\", \"grill\", \"grilling steak\").\n\toa_rel(\"can\", \"bear\", \"stand on their hind legs\").\n\toa_rel(\"is used for\", \"farm\", \"raising crops\").\n\toa_rel(\"can\", \"cow\", \"supply humans with milk\").\n", "oa_rel(\"is made from\", \"biscuit\", \"flour\").\n\toa_rel(\"usually appears in\", \"seagull\", \"ocean\").\n\toa_rel(\"is\", \"chili\", \"spicy\").\n\toa_rel(\"usually appears in\", \"toaster\", \"kitchen\").\n\toa_rel(\"can\", \"bread\", \"provide complex carbohydrates\").\n\toa_rel(\"is used for\", \"lipstick\", \"coloring lips\").\n\toa_rel(\"can\", \"computer\", \"run programs\").\n\toa_rel(\"is\", \"broth\", \"fluid\").\n\toa_rel(\"has\", \"broccoli\", \"calcium\").\n\toa_rel(\"is used for\", \"baseball\", \"pitching\").\n", "oa_rel(\"is used for\", \"apple\", \"making juice\").\n\toa_rel(\"has\", \"toast\", \"starch\").\n\toa_rel(\"usually appears in\", \"tv stand\", \"living room\").\n\toa_rel(\"can be\", \"chicken\", \"fried\").\n\toa_rel(\"can\", \"dog\", \"come to its master\").\n\toa_rel(\"can\", \"frog\", \"spring out of pond\").\n\toa_rel(\"is used for\", \"computer\", \"sending email\").\n\toa_rel(\"is a sub-event of\", \"juicing\", \"pressing halves on juicer\").\n\toa_rel(\"is used for\", \"air conditioner\", \"cooling people\").\n\toa_rel(\"is used for\", \"scissors\", \"cutting paper or cloth\").\n", "oa_rel(\"can\", \"dog\", \"sleep long time\").\n\toa_rel(\"usually appears in\", \"spatula\", \"kitchen\").\n\toa_rel(\"has\", \"alcohol\", \"water\").\n\toa_rel(\"is used for\", \"bank\", \"saving money\").\n\toa_rel(\"can\", \"bird\", \"fly high\").\n\toa_rel(\"can\", \"screwdriver\", \"turn screw\").\n\toa_rel(\"is\", \"cat\", \"soft\").\n\toa_rel(\"is used for\", \"meat\", \"getting protein\").\n\toa_rel(\"is used for\", \"canoe\", \"floating and moving on water\").\n\toa_rel(\"is used for\", \"tunnel\", \"transportation\").\n", "oa_rel(\"has\", \"sweet potato\", \"vitamin C\").\n\toa_rel(\"is\", \"giraffe\", \"herbivorous\").\n\toa_rel(\"is used for\", \"screw\", \"fastening two objects together\").\n\toa_rel(\"has\", \"tea\", \"water\").\n\toa_rel(\"has\", \"tea\", \"caffeine\").\n\toa_rel(\"can\", \"cat\", \"mind getting wet\").\n\toa_rel(\"can\", \"jeep\", \"spend gas\").\n\toa_rel(\"is made from\", \"noodles\", \"flour\").\n\toa_rel(\"is used for\", \"sugar\", \"adding taste to food\").\n\toa_rel(\"is used for\", \"cow\", \"milking\").\n", "oa_rel(\"is\", \"butter\", \"fluid\").\n\toa_rel(\"usually appears in\", \"knife\", \"dining room\").\n\toa_rel(\"is\", \"coffee\", \"liquid\").\n\toa_rel(\"is\", \"coffee\", \"fluid\").\n\toa_rel(\"is\", \"milk\", \"fluid\").\n\toa_rel(\"can\", \"airplane\", \"travel through many time zones\").\n\toa_rel(\"is used for\", \"canoe\", \"keeping people out of water\").\n\toa_rel(\"has\", \"giraffe\", \"long neck\").\n\toa_rel(\"has\", \"beef\", \"iron\").\n\toa_rel(\"can\", \"ship\", \"go across sea\").\n", "oa_rel(\"can\", \"seagull\", \"fly\").\n\toa_rel(\"is used for\", \"baseball field\", \"playing baseball with team\").\n\toa_rel(\"can\", \"hats\", \"go on hat rack\").\n\toa_rel(\"requires\", \"cooking\", \"food\").\n\toa_rel(\"is\", \"oil\", \"liquid\").\n\toa_rel(\"requires\", \"sauteing\", \"oil\").\n\toa_rel(\"is\", \"toaster\", \"electric\").\n\toa_rel(\"can\", \"computer\", \"mine data\").\n\toa_rel(\"can\", \"bird\", \"chirp\").\n\toa_rel(\"is\", \"cow\", \"herbivorous\").\n", "oa_rel(\"can\", \"knife\", \"be both tools and weapons\").\n\toa_rel(\"usually appears in\", \"chef\", \"restaurant\").\n\toa_rel(\"can be\", \"clothing\", \"hung\").\n\toa_rel(\"is\", \"milk\", \"nutritious\").\n\toa_rel(\"usually appears in\", \"bookshelf\", \"bedroom\").\n\toa_rel(\"is\", \"cream\", \"sticky\").\n\toa_rel(\"is made from\", \"burrito\", \"flour\").\n\toa_rel(\"is a sub-event of\", \"baking bread\", \"gathering ingredients\").\n\toa_rel(\"has\", \"coffee\", \"caffeine\").\n\toa_rel(\"can be\", \"cardigan\", \"hung\").\n", "oa_rel(\"can\", \"bear\", \"swim\").\n\toa_rel(\"is used for\", \"tattoo\", \"decoration\").\n\toa_rel(\"can be\", \"pizza\", \"eaten\").\n\toa_rel(\"can\", \"bird\", \"feed worms to its young\").\n\toa_rel(\"is used for\", \"museum\", \"displaying historical artifacts\").\n\toa_rel(\"can be\", \"pizza\", \"cut\").\n\toa_rel(\"usually appears in\", \"kettle\", \"kitchen\").\n\toa_rel(\"is used for\", \"milk\", \"feeding baby\").\n\toa_rel(\"is\", \"air conditioner\", \"electric\").\n\toa_rel(\"usually appears in\", \"duck\", \"water\").\n", "oa_rel(\"is used for\", \"mailbox\", \"sending packages\").\n\toa_rel(\"requires\", \"grating\", \"grater\").\n\toa_rel(\"has\", \"wine\", \"water\").\n\toa_rel(\"is\", \"milk\", \"liquid\").\n\toa_rel(\"can\", \"horse\", \"finish race\").\n\toa_rel(\"usually appears in\", \"vegetable\", \"dinner\").\n\toa_rel(\"is made from\", \"cupcake\", \"flour\").\n\toa_rel(\"is\", \"polar bear\", \"carnivorous\").\n\toa_rel(\"is used for\", \"pickup\", \"transporting goods\").\n\toa_rel(\"is used for\", \"hotel\", \"staying overnight\").\n", "oa_rel(\"is used for\", \"headphones\", \"listening to music\").\n\toa_rel(\"can\", \"pony\", \"be pet\").\n\toa_rel(\"can\", \"horse\", \"jump over hurdles\").\n\toa_rel(\"has\", \"peacock\", \"large\").\n\toa_rel(\"is used for\", \"ship\", \"traveling on water\").\n\toa_rel(\"is used for\", \"ship\", \"carrying cargo\").\n\toa_rel(\"usually appears in\", \"ladle\", \"kitchen\").\n\toa_rel(\"has\", \"beans\", \"calcium\").\n\toa_rel(\"is\", \"marshmallow\", \"sweet\").\n\toa_rel(\"usually appears in\", \"pepper shaker\", \"restaurant\").\n", "oa_rel(\"usually appears in\", \"pencil\", \"office\").\n\toa_rel(\"is\", \"soda\", \"liquid\").\n\toa_rel(\"is made from\", \"lemonade\", \"lemon\").\n\toa_rel(\"is\", \"lemonade\", \"liquid\").\n\toa_rel(\"can\", \"dog\", \"smell drugs\").\n\toa_rel(\"usually appears in\", \"blade\", \"kitchen\").\n\toa_rel(\"is\", \"lemon\", \"bitter\").\n\toa_rel(\"is used for\", \"soap\", \"washing hands\").\n\toa_rel(\"is a sub-event of\", \"baking cake\", \"pouring batter in cake pan\").\n\toa_rel(\"is made from\", \"pizza\", \"flour\").\n", "oa_rel(\"is made from\", \"coffee\", \"coffee beans\").\n\toa_rel(\"is used for\", \"temple\", \"worship\").\n\toa_rel(\"is used for\", \"ship\", \"floating and moving on water\").\n\toa_rel(\"has\", \"cheese\", \"vitamin B\").\n\toa_rel(\"is made from\", \"pancake\", \"flour\").\n\toa_rel(\"can\", \"cat\", \"hunt mice\").\n\toa_rel(\"can\", \"owl\", \"hear slightest rustle\").\n\toa_rel(\"usually appears in\", \"giraffe\", \"grassland\").\n\toa_rel(\"usually appears in\", \"urinal\", \"bathroom\").\n\toa_rel(\"has\", \"egg\", \"vitamin D\").\n", "oa_rel(\"is used for\", \"pencil\", \"writing\").\n\toa_rel(\"usually appears in\", \"carpet\", \"bedroom\").\n\toa_rel(\"requires\", \"juicing\", \"juicer\").\n\toa_rel(\"is\", \"coffee\", \"bitter\").\n\toa_rel(\"can\", \"owl\", \"fly\").\n\toa_rel(\"is used for\", \"office\", \"conducting business\").\n\toa_rel(\"can\", \"screw\", \"hold things together\").\n\toa_rel(\"can\", \"police\", \"arrest\").\n\toa_rel(\"has\", \"spinach\", \"iron\").\n\toa_rel(\"has\", \"beef\", \"vitamin B\").\n", "oa_rel(\"can\", \"squirrel\", \"store nuts\").\n\toa_rel(\"requires\", \"making pizza\", \"pizza pan\").\n\toa_rel(\"requires\", \"coring\", \"slicer\").\n\toa_rel(\"can\", \"thermometer\", \"measure temperature\").\n\toa_rel(\"is made from\", \"cookie\", \"flour\").\n\toa_rel(\"is\", \"beer\", \"fluid\").\n\toa_rel(\"has\", \"pepper\", \"vitamin C\").\n\toa_rel(\"is\", \"gun\", \"dangerous\").\n\toa_rel(\"can\", \"soldier\", \"fight battle\").\n\toa_rel(\"is used for\", \"apartment\", \"dwelling\").\n", "oa_rel(\"has\", \"spinach\", \"vitamin C\").\n\toa_rel(\"usually appears in\", \"pizza tray\", \"dining room\").\n\toa_rel(\"is used for\", \"vacuum\", \"cleaning carpet\").\n\toa_rel(\"is a sub-event of\", \"cleaning house\", \"getting vacuum out\").\n\toa_rel(\"is used for\", \"vending machine\", \"buying drinks\").\n\toa_rel(\"is used for\", \"ground\", \"standing on\").\n\toa_rel(\"is\", \"dip\", \"fluid\").\n\toa_rel(\"can\", \"bird\", \"learn to fly\").\n\toa_rel(\"usually appears in\", \"sheep\", \"meadow\").\n\toa_rel(\"usually appears in\", \"turkey\", \"christmas\").\n", "oa_rel(\"usually appears in\", \"bread\", \"dinner\").\n\toa_rel(\"has\", \"monkey\", \"two arms\").\n\toa_rel(\"has\", \"milk\", \"water\").\n\toa_rel(\"is\", \"honey\", \"sweet\").\n\toa_rel(\"is\", \"lemon\", \"sour\").\n\toa_rel(\"usually appears in\", \"fork\", \"dining room\").\n\toa_rel(\"is used for\", \"highway\", \"transportation\").\n\toa_rel(\"has\", \"almond\", \"vitamin B\").\n\toa_rel(\"is made from\", \"yogurt\", \"milk\").\n\toa_rel(\"can\", \"waitress\", \"serve food\").\n", "oa_rel(\"can\", \"pickup\", \"travel on road\").\n\toa_rel(\"can\", \"bird\", \"build nest\").\n\toa_rel(\"can\", \"jeep\", \"transport people\").\n\toa_rel(\"is used for\", \"sheep\", \"shearing\").\n\toa_rel(\"is used for\", \"library\", \"reading books\").\n\toa_rel(\"can\", \"banjo\", \"play bluegrass music\").\n\toa_rel(\"is used for\", \"sugar\", \"making drinks sweet\").\n\toa_rel(\"usually appears in\", \"crab\", \"ocean\").\n\toa_rel(\"is used for\", \"toothbrush\", \"keeping you teeth clean\").\n\toa_rel(\"requires\", \"cleaning clothing\", \"washing machine\").\n", "oa_rel(\"usually appears in\", \"lobster\", \"ocean\").\n\toa_rel(\"is\", \"game controller\", \"electric\").\n\toa_rel(\"is used for\", \"doll\", \"entertainment\").\n\toa_rel(\"is used for\", \"computer\", \"doing calculation\").\n\toa_rel(\"can\", \"horse\", \"run faster than most humans\").\n\toa_rel(\"is used for\", \"drum\", \"banging out rhythms\").\n\toa_rel(\"has\", \"spinach\", \"vitamin D\").\n\toa_rel(\"is used for\", \"kettle\", \"heating water\").\n\toa_rel(\"can\", \"snail\", \"wave their antennae\").\n\toa_rel(\"is used for\", \"oil\", \"frying food in\").\n", "oa_rel(\"usually appears in\", \"baking sheet\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"whale\", \"water\").\n\toa_rel(\"can\", \"whale\", \"swim\").\n\toa_rel(\"is\", \"dip\", \"sticky\").\n\toa_rel(\"has\", \"juice\", \"vitamin C\").\n\toa_rel(\"can\", \"canoe\", \"travel over water\").\n\toa_rel(\"can\", \"eagle\", \"fly\").\n\toa_rel(\"has\", \"sweet potato\", \"calcium\").\n\toa_rel(\"is used for\", \"museum\", \"preserving historical artifacts\").\n\toa_rel(\"is made from\", \"omelette\", \"flour\").\n", "oa_rel(\"is used for\", \"toaster\", \"making toast\").\n\toa_rel(\"usually appears in\", \"cake\", \"birthday party\").\n\toa_rel(\"has\", \"rice\", \"vitamin B\").\n\toa_rel(\"is\", \"gravy\", \"fluid\").\n\toa_rel(\"usually appears in\", \"wedding cake\", \"wedding\").\n\toa_rel(\"is used for\", \"kettle\", \"making tea\").\n\toa_rel(\"usually appears in\", \"dolphin\", \"ocean\").\n\toa_rel(\"is\", \"shark\", \"dangerous\").\n\toa_rel(\"is\", \"ipod\", \"electric\").\n\toa_rel(\"usually appears in\", \"notepad\", \"office\").\n", "oa_rel(\"usually appears in\", \"tablecloth\", \"dining room\").\n\toa_rel(\"can be\", \"lemon\", \"squeezed\").\n\toa_rel(\"can\", \"horse\", \"be raced and ridden by humans\").\n\toa_rel(\"has\", \"fish\", \"vitamin D\").\n\toa_rel(\"usually appears in\", \"speaker\", \"office\").\n\toa_rel(\"is used for\", \"ship\", \"travelling long distances\").\n\toa_rel(\"can\", \"wine\", \"be ingredient in recipe\").\n\toa_rel(\"requires\", \"baking bread\", \"dough\").\n\toa_rel(\"is used for\", \"classroom\", \"teaching\").\n\toa_rel(\"requires\", \"processing\", \"food processor\").\n", "oa_rel(\"has\", \"chocolate\", \"caffeine\").\n\toa_rel(\"can\", \"cat\", \"sense with their whiskers\").\n\toa_rel(\"is made from\", \"tortilla\", \"flour\").\n\toa_rel(\"can\", \"fly\", \"fly\").\n\toa_rel(\"can\", \"bat\", \"fly\").\n\toa_rel(\"is\", \"tea\", \"fluid\").\n\toa_rel(\"is used for\", \"axe\", \"chopping wood\").\n\toa_rel(\"is used for\", \"double decker\", \"transporting lots of people\").\n\toa_rel(\"is\", \"sugar\", \"sweet\").\n\toa_rel(\"has\", \"raspberry\", \"vitamin C\").\n", "oa_rel(\"is\", \"monkey\", \"omnivorous\").\n\toa_rel(\"usually appears in\", \"student\", \"classroom\").\n\toa_rel(\"requires\", \"making pizza\", \"olive oil\").\n\toa_rel(\"is used for\", \"knife\", \"slicing\").\n\toa_rel(\"has\", \"tiger\", \"stripes\").\n\toa_rel(\"usually appears in\", \"pencil sharpener\", \"office\").\n\toa_rel(\"can\", \"dog\", \"sense danger\").\n\toa_rel(\"can\", \"shuttle\", \"fly\").\n\toa_rel(\"is used for\", \"condiment\", \"adding taste to food\").\n\toa_rel(\"can be\", \"lemon\", \"eaten\").\n", "oa_rel(\"has\", \"corn\", \"starch\").\n\toa_rel(\"can\", \"cat\", \"purr\").\n\toa_rel(\"is used for\", \"mailbox\", \"receiving letters\").\n\toa_rel(\"is used for\", \"frisbee\", \"having fun\").\n\toa_rel(\"can\", \"police\", \"direct traffic\").\n\toa_rel(\"usually appears in\", \"goat\", \"barn\").\n\toa_rel(\"usually appears in\", \"grill\", \"kitchen\").\n\toa_rel(\"is used for\", \"sausage\", \"getting protein\").\n\toa_rel(\"is used for\", \"cafe\", \"having snack\").\n\toa_rel(\"usually appears in\", \"zebra\", \"grassland\").\n", "oa_rel(\"is\", \"pig\", \"omnivorous\").\n\toa_rel(\"has\", \"cranberry\", \"vitamin C\").\n\toa_rel(\"can\", \"suv\", \"move quickly\").\n\toa_rel(\"can\", \"frog\", \"catch fly\").\n\toa_rel(\"usually appears in\", \"wine\", \"bar\").\n\toa_rel(\"is used for\", \"factory\", \"manufacture things\").\n\toa_rel(\"is\", \"olive oil\", \"liquid\").\n\toa_rel(\"requires\", \"making pizza\", \"mixing bowl\").\n\toa_rel(\"is used for\", \"sugar\", \"imparting specific flavor\").\n\toa_rel(\"is made from\", \"pita\", \"flour\").\n", "oa_rel(\"is\", \"banana\", \"healthy\").\n\toa_rel(\"can\", \"laptop\", \"save files on disk\").\n\toa_rel(\"usually appears in\", \"hand soap\", \"bathroom\").\n\toa_rel(\"is used for\", \"air conditioner\", \"lowering air temperature\").\n\toa_rel(\"is used for\", \"bus\", \"transportation\").\n\toa_rel(\"can be\", \"jeans\", \"hung\").\n\toa_rel(\"is used for\", \"teddy bear\", \"entertainment\").\n\toa_rel(\"is used for\", \"supermarket\", \"buying and selling\").\n\toa_rel(\"can\", \"dog\", \"be pet\").\n\toa_rel(\"is made from\", \"burger\", \"flour\").\n", "oa_rel(\"is made from\", \"hamburger\", \"flour\").\n\toa_rel(\"is made from\", \"tea\", \"leaves of camellia sinensis\").\n\toa_rel(\"is\", \"peanut butter\", \"fluid\").\n\toa_rel(\"can\", \"subway\", \"transport many people at once\").\n\toa_rel(\"usually appears in\", \"toothpaste\", \"bathroom\").\n\toa_rel(\"is used for\", \"sailboat\", \"keeping people out of water\").\n\toa_rel(\"is a sub-event of\", \"making pizza\", \"rising dough\").\n\toa_rel(\"can\", \"monkeys\", \"throw things\").\n\toa_rel(\"is used for\", \"backyard\", \"planting flowers\").\n\toa_rel(\"is a sub-event of\", \"baking bread\", \"rising dough\").\n", "oa_rel(\"requires\", \"cooking\", \"heat\").\n\toa_rel(\"can\", \"shark\", \"swim\").\n\toa_rel(\"can\", \"frog\", \"catch flies with its tongue\").\n\toa_rel(\"is used for\", \"computer\", \"enjoyment\").\n\toa_rel(\"is used for\", \"double decker\", \"transporting people\").\n\toa_rel(\"usually appears in\", \"chalk\", \"classroom\").\n\toa_rel(\"can\", \"flamingo\", \"fly\").\n\toa_rel(\"requires\", \"peeling\", \"peeler\").\n\toa_rel(\"is used for\", \"condiment\", \"imparting specific flavor\").\n\toa_rel(\"usually appears in\", \"pork\", \"dinner\").\n", "oa_rel(\"usually appears in\", \"knife block\", \"kitchen\").\n\toa_rel(\"is made from\", \"cheesecake\", \"flour\").\n\toa_rel(\"has\", \"lime\", \"vitamin C\").\n\toa_rel(\"can\", \"ship\", \"travel over water\").\n\toa_rel(\"can\", \"bird\", \"perch\").\n\toa_rel(\"has\", \"wheat\", \"starch\").\n\toa_rel(\"is a sub-event of\", \"baking bread\", \"put batter in oven\").\n\toa_rel(\"usually appears in\", \"groom\", \"wedding\").\n\toa_rel(\"is\", \"video camera\", \"electric\").\n\toa_rel(\"is\", \"cream\", \"sweet\").\n", "oa_rel(\"is used for\", \"cafe\", \"meeting people\").\n\toa_rel(\"is used for\", \"freeway\", \"transportation\").\n\toa_rel(\"is used for\", \"toothpaste\", \"cleaning teeth\").\n\toa_rel(\"usually appears in\", \"teacher\", \"classroom\").\n\toa_rel(\"can\", \"dove\", \"fly\").\n\toa_rel(\"is used for\", \"laptop\", \"studying\").\n\toa_rel(\"has\", \"cereal\", \"starch\").\n\toa_rel(\"can be\", \"jacket\", \"hung\").\n\toa_rel(\"usually appears in\", \"monkey\", \"jungle\").\n\toa_rel(\"is used for\", \"scooter\", \"transportation\").\n", "oa_rel(\"is\", \"toothpaste\", \"sticky\").\n\toa_rel(\"is\", \"lotion\", \"fluid\").\n\toa_rel(\"usually appears in\", \"chopsticks\", \"restaurant\").\n\toa_rel(\"can\", \"hammer\", \"nail nail\").\n\toa_rel(\"is used for\", \"soap\", \"bathing\").\n\toa_rel(\"can be\", \"food\", \"cut\").\n\toa_rel(\"has\", \"mango\", \"vitamin C\").\n\toa_rel(\"is\", \"calf\", \"herbivorous\").\n\toa_rel(\"is\", \"cola\", \"fluid\").\n\toa_rel(\"usually appears in\", \"spoon\", \"restaurant\").\n", "oa_rel(\"is used for\", \"drum\", \"making rhythm\").\n\toa_rel(\"is used for\", \"sugar\", \"flavoring food\").\n\toa_rel(\"is used for\", \"salt\", \"imparting specific flavor\").\n\toa_rel(\"can\", \"sedan\", \"transport people\").\n\toa_rel(\"usually appears in\", \"drinks\", \"dinner\").\n\toa_rel(\"is used for\", \"comb\", \"combing hair\").\n\toa_rel(\"is\", \"donkey\", \"herbivorous\").\n\toa_rel(\"usually appears in\", \"tongs\", \"kitchen\").\n\toa_rel(\"is used for\", \"hairbrush\", \"styling hair\").\n\toa_rel(\"usually appears in\", \"comb\", \"bathroom\").\n", "oa_rel(\"usually appears in\", \"tiger\", \"jungle\").\n\toa_rel(\"usually appears in\", \"mill\", \"barn\").\n\toa_rel(\"is a sub-event of\", \"baking cake\", \"gathering ingredients\").\n\toa_rel(\"can\", \"duck\", \"fly\").\n\toa_rel(\"can be\", \"duck\", \"roasted\").\n\toa_rel(\"usually appears in\", \"swan\", \"water\").\n\toa_rel(\"can\", \"minivan\", \"carry few persons\").\n\toa_rel(\"usually appears in\", \"santa\", \"christmas\").\n\toa_rel(\"is used for\", \"couch\", \"resting\").\n\toa_rel(\"is\", \"peanut butter\", \"sticky\").\n", "oa_rel(\"requires\", \"mixing\", \"electric mixer\").\n\toa_rel(\"can\", \"duck\", \"swim\").\n\toa_rel(\"usually appears in\", \"pizza tray\", \"restaurant\").\n\toa_rel(\"is made from\", \"whipped cream\", \"milk\").\n\toa_rel(\"is\", \"guacamole\", \"fluid\").\n\toa_rel(\"can\", \"cat\", \"jump amazingly high\").\n\toa_rel(\"can\", \"swan\", \"fly\").\n\toa_rel(\"is used for\", \"bread\", \"eating\").\n\toa_rel(\"can be\", \"toast\", \"toasted\").\n\toa_rel(\"usually appears in\", \"menu\", \"bar\").\n", "oa_rel(\"can\", \"parrot\", \"imitate human voices\").\n\toa_rel(\"has\", \"milk\", \"calcium\").\n\toa_rel(\"has\", \"cappuccino\", \"caffeine\").\n\toa_rel(\"is\", \"caramel\", \"fluid\").\n\toa_rel(\"usually appears in\", \"computer\", \"office\").\n\toa_rel(\"is used for\", \"attic\", \"storing books\").\n\toa_rel(\"is used for\", \"phone\", \"listening to music\").\n\toa_rel(\"is\", \"hard drive\", \"electric\").\n\toa_rel(\"can be\", \"wine\", \"drunk\").\n\toa_rel(\"can\", \"kitten\", \"be pet\").\n", "oa_rel(\"is used for\", \"cafe\", \"meeting friends\").\n\toa_rel(\"usually appears in\", \"wii\", \"living room\").\n\toa_rel(\"can\", \"flamingo\", \"be pet\").\n\toa_rel(\"is made from\", \"loaf\", \"flour\").\n\toa_rel(\"is used for\", \"laptop\", \"doing calculation\").\n\toa_rel(\"has\", \"milkshake\", \"water\").\n\toa_rel(\"can\", \"alligator\", \"swim\").\n\toa_rel(\"can\", \"laptop\", \"process information\").\n\toa_rel(\"can\", \"swan\", \"swim\").\n\toa_rel(\"is used for\", \"library\", \"borrowing books\").\n", "oa_rel(\"usually appears in\", \"knife\", \"restaurant\").\n\toa_rel(\"is used for\", \"truck\", \"transporting people\").\n\toa_rel(\"is used for\", \"truck\", \"transportation\").\n\toa_rel(\"can\", \"goose\", \"swim\").\n\toa_rel(\"is\", \"ice cream\", \"fluid\").\n\toa_rel(\"requires\", \"baking bread\", \"flour\").\n\toa_rel(\"has\", \"champagne\", \"alcohol\").\n\toa_rel(\"is used for\", \"salt\", \"flavoring food\").\n\toa_rel(\"usually appears in\", \"goose\", \"water\").\n\toa_rel(\"usually appears in\", \"ostrich\", \"grassland\").\n", "oa_rel(\"is used for\", \"camel\", \"transporting people\").\n\toa_rel(\"is used for\", \"grill\", \"cooking foods\").\n\toa_rel(\"is used for\", \"laptop\", \"finding information\").\n\toa_rel(\"is\", \"deer\", \"herbivorous\").\n\toa_rel(\"is used for\", \"backyard\", \"growing garden\").\n\toa_rel(\"can\", \"frog\", \"jump very high\").\n\toa_rel(\"can\", \"parrot\", \"fly\").\n\toa_rel(\"is used for\", \"drum\", \"hitting\").\n\toa_rel(\"has\", \"pepperoni\", \"vitamin B\").\n\toa_rel(\"is\", \"milk\", \"healthy\").\n", "oa_rel(\"is used for\", \"mall\", \"meeting friends\").\n\toa_rel(\"can\", \"double decker\", \"travel on road\").\n\toa_rel(\"can\", \"van\", \"move quickly\").\n\toa_rel(\"is\", \"spear\", \"dangerous\").\n\toa_rel(\"usually appears in\", \"alligator\", \"water\").\n\toa_rel(\"has\", \"tofu\", \"iron\").\n\toa_rel(\"is used for\", \"chicken\", \"getting protein\").\n\toa_rel(\"is used for\", \"motorcycle\", \"transportation\").\n\toa_rel(\"is used for\", \"grill\", \"barbecuing foods\").\n\toa_rel(\"usually appears in\", \"whisk\", \"kitchen\").\n", "oa_rel(\"is used for\", \"hotel\", \"sleeping\").\n\toa_rel(\"is used for\", \"laptop\", \"playing games\").\n\toa_rel(\"is used for\", \"hammer\", \"pounding in nails\").\n\toa_rel(\"is used for\", \"shoe\", \"protecting feet\").\n\toa_rel(\"usually appears in\", \"toiletries\", \"bathroom\").\n\toa_rel(\"is used for\", \"restaurant\", \"eating meal without cooking\").\n\toa_rel(\"requires\", \"cleaning house\", \"vacuum\").\n\toa_rel(\"can\", \"bee\", \"fly\").\n\toa_rel(\"is used for\", \"refrigerator\", \"storing foods\").\n\toa_rel(\"is\", \"goat\", \"herbivorous\").\n", "oa_rel(\"requires\", \"baking cake\", \"cake pan\").\n\toa_rel(\"is made from\", \"pretzel\", \"flour\").\n\toa_rel(\"usually appears in\", \"alcohol\", \"bar\").\n\toa_rel(\"is\", \"alcohol\", \"liquid\").\n\toa_rel(\"is\", \"champagne\", \"fluid\").\n\toa_rel(\"usually appears in\", \"dishes\", \"dining room\").\n\toa_rel(\"is used for\", \"laptop\", \"doing mathematical calculations\").\n\toa_rel(\"is\", \"owl\", \"carnivorous\").\n\toa_rel(\"can\", \"suv\", \"spend gas\").\n\toa_rel(\"usually appears in\", \"beer\", \"bar\").\n", "oa_rel(\"can\", \"toaster\", \"brown bread\").\n\toa_rel(\"is used for\", \"water\", \"drinking\").\n\toa_rel(\"is used for\", \"screwdriver\", \"inserting screw\").\n\toa_rel(\"is used for\", \"salt\", \"adding taste to food\").\n\toa_rel(\"usually appears in\", \"octopus\", \"ocean\").\n\toa_rel(\"is\", \"blood\", \"liquid\").\n\toa_rel(\"has\", \"yogurt\", \"calcium\").\n\toa_rel(\"can\", \"helicopter\", \"fly\").\n\toa_rel(\"usually appears in\", \"computer desk\", \"office\").\n\toa_rel(\"is made from\", \"waffle\", \"flour\").\n", "oa_rel(\"is made from\", \"brownie\", \"flour\").\n\toa_rel(\"is used for\", \"lotion\", \"moisturizing skin\").\n\toa_rel(\"has\", \"bagel\", \"starch\").\n\toa_rel(\"has\", \"peanut butter\", \"iron\").\n\toa_rel(\"is\", \"food processor\", \"electric\").\n\toa_rel(\"is\", \"olive oil\", \"fluid\").\n\toa_rel(\"can\", \"hummingbird\", \"fly\").\n\toa_rel(\"usually appears in\", \"pepper shaker\", \"dining room\").\n\toa_rel(\"can\", \"frog\", \"be pet\").\n\toa_rel(\"is used for\", \"hotel\", \"staying on vacations\").\n", "oa_rel(\"is used for\", \"train\", \"transportation\").\n\toa_rel(\"usually appears in\", \"waitress\", \"restaurant\").\n\toa_rel(\"is used for\", \"metal stand\", \"sitting on\").\n\toa_rel(\"requires\", \"coring\", \"corer\").\n\toa_rel(\"can\", \"van\", \"transport people\").\n\toa_rel(\"is\", \"champagne\", \"liquid\").\n\toa_rel(\"is used for\", \"backyard\", \"growing vegetables\").\n\toa_rel(\"can\", \"beaker\", \"measure liquid\").\n\toa_rel(\"is\", \"shampoo\", \"sticky\").\n\toa_rel(\"is\", \"moose\", \"herbivorous\").\n", "oa_rel(\"requires\", \"baking cake\", \"dough\").\n\toa_rel(\"is\", \"whipped cream\", \"sweet\").\n\toa_rel(\"can\", \"gas stove\", \"heat eater\").\n\toa_rel(\"usually appears in\", \"octopus\", \"water\").\n\toa_rel(\"is used for\", \"bicycle\", \"transportation\").\n\toa_rel(\"is\", \"kitten\", \"soft\").\n\toa_rel(\"can\", \"bartender\", \"mix classic cocktails\").\n\toa_rel(\"is\", \"cattle\", \"herbivorous\").\n\toa_rel(\"can\", \"cell phone\", \"make call\").\n\toa_rel(\"can\", \"can opener\", \"open cans\").\n", "oa_rel(\"is\", \"rice cooker\", \"electric\").\n\toa_rel(\"is used for\", \"pantry\", \"storing food\").\n\toa_rel(\"is used for\", \"dishwasher\", \"washing dishes\").\n\toa_rel(\"is\", \"peach\", \"healthy\").\n\toa_rel(\"has\", \"peacock\", \"elaborate plumage\").\n\toa_rel(\"can be\", \"juice\", \"drunk\").\n\toa_rel(\"can\", \"rabbit\", \"be pet\").\n\toa_rel(\"is used for\", \"wine\", \"drinking\").\n\toa_rel(\"requires\", \"making coffee\", \"coffee beans\").\n\toa_rel(\"is used for\", \"drum\", \"playing music\").\n", "oa_rel(\"is a sub-event of\", \"baking bread\", \"kneading dough\").\n\toa_rel(\"can\", \"police\", \"tail suspect\").\n\toa_rel(\"is used for\", \"suv\", \"transporting people\").\n\toa_rel(\"is used for\", \"pickup\", \"transporting handful of people\").\n\toa_rel(\"is\", \"milkshake\", \"liquid\").\n\toa_rel(\"is\", \"router\", \"electric\").\n\toa_rel(\"is a sub-event of\", \"making pizza\", \"shaping dough into thin circle\").\n\toa_rel(\"is\", \"bull\", \"herbivorous\").\n\toa_rel(\"usually appears in\", \"shark\", \"water\").\n\toa_rel(\"is used for\", \"stuffed animal\", \"entertainment\").\n", "oa_rel(\"is\", \"hawk\", \"carnivorous\").\n\toa_rel(\"is used for\", \"sailboat\", \"transporting people\").\n\toa_rel(\"is\", \"shaving cream\", \"sticky\").\n\toa_rel(\"is used for\", \"banana\", \"eating\").\n\toa_rel(\"can\", \"chicken\", \"sing\").\n\toa_rel(\"is used for\", \"mitt\", \"protecting hand\").\n\toa_rel(\"usually appears in\", \"crab\", \"water\").\n\toa_rel(\"is\", \"lion\", \"carnivorous\").\n\toa_rel(\"usually appears in\", \"water glass\", \"restaurant\").\n\toa_rel(\"usually appears in\", \"dvd player\", \"living room\").\n", "oa_rel(\"is used for\", \"listening to music\", \"entertainment\").\n\toa_rel(\"can\", \"robin\", \"winter down south\").\n\toa_rel(\"is\", \"shampoo\", \"fluid\").\n\toa_rel(\"is\", \"oil\", \"fluid\").\n\toa_rel(\"is\", \"tiger\", \"carnivorous\").\n\toa_rel(\"is\", \"bomb\", \"dangerous\").\n\toa_rel(\"can\", \"cereal\", \"provide complex carbohydrates\").\n\toa_rel(\"usually appears in\", \"printer\", \"office\").\n\toa_rel(\"has\", \"alcohol\", \"alcohol\").\n\toa_rel(\"is used for\", \"hairbrush\", \"combing hair\").\n", "oa_rel(\"has\", \"almond\", \"calcium\").\n\toa_rel(\"is\", \"lion\", \"dangerous\").\n\toa_rel(\"is used for\", \"projector\", \"showing films\").\n\toa_rel(\"usually appears in\", \"champagne\", \"wedding\").\n\toa_rel(\"is\", \"milk\", \"good for baby\").\n\toa_rel(\"usually appears in\", \"wok\", \"kitchen\").\n\toa_rel(\"is used for\", \"sugar\", \"sweetening coffee\").\n\toa_rel(\"usually appears in\", \"dish drainer\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"baking pan\", \"kitchen\").\n\toa_rel(\"is\", \"alcohol\", \"fluid\").\n", "oa_rel(\"is used for\", \"ruler\", \"measuring lengths\").\n\toa_rel(\"is used for\", \"beer\", \"drinking\").\n\toa_rel(\"can\", \"poodle\", \"be pet\").\n\toa_rel(\"is used for\", \"cafe\", \"drinking coffee\").\n\toa_rel(\"can\", \"theater\", \"show movie\").\n\toa_rel(\"requires\", \"washing dishes\", \"dish cloth\").\n\toa_rel(\"can\", \"puppy\", \"be pet\").\n\toa_rel(\"usually appears in\", \"shampoo\", \"bathroom\").\n\toa_rel(\"is\", \"blood\", \"fluid\").\n\toa_rel(\"is\", \"toothpaste\", \"fluid\").\n", "oa_rel(\"usually appears in\", \"penguin\", \"water\").\n\toa_rel(\"can\", \"heater\", \"warm feet\").\n\toa_rel(\"is\", \"pear\", \"healthy\").\n\toa_rel(\"is\", \"toaster oven\", \"electric\").\n\toa_rel(\"is used for\", \"helicopter\", \"traversing skies\").\n\toa_rel(\"is\", \"cooking oil\", \"fluid\").\n\toa_rel(\"requires\", \"frying\", \"cooking oil\").\n\toa_rel(\"is a sub-event of\", \"baking cake\", \"put cake pan in oven\").\n\toa_rel(\"usually appears in\", \"bartender\", \"bar\").\n\toa_rel(\"is\", \"guacamole\", \"sticky\").\n", "oa_rel(\"can\", \"seal\", \"position itself on rock\").\n\toa_rel(\"is used for\", \"screwdriver\", \"installing or removing screws\").\n\toa_rel(\"usually appears in\", \"glue stick\", \"office\").\n\toa_rel(\"is made from\", \"egg roll\", \"flour\").\n\toa_rel(\"usually appears in\", \"pizza pan\", \"kitchen\").\n\toa_rel(\"is\", \"hummus\", \"fluid\").\n\toa_rel(\"can be\", \"hot dog\", \"eaten\").\n\toa_rel(\"usually appears in\", \"ruler\", \"office\").\n\toa_rel(\"is used for\", \"donut\", \"eating\").\n\toa_rel(\"has\", \"grains\", \"vitamin B\").\n", "oa_rel(\"is\", \"liquor\", \"liquid\").\n\toa_rel(\"is\", \"liquor\", \"harmful\").\n\toa_rel(\"is used for\", \"helicopter\", \"transporting people\").\n\toa_rel(\"is used for\", \"backyard\", \"family activities\").\n\toa_rel(\"is\", \"lotion\", \"sticky\").\n\toa_rel(\"usually appears in\", \"staples\", \"office\").\n\toa_rel(\"can\", \"goose\", \"fly\").\n\toa_rel(\"is\", \"milkshake\", \"fluid\").\n\toa_rel(\"can\", \"eagle\", \"feed worms to its young\").\n\toa_rel(\"can\", \"peacock\", \"fly\").\n", "oa_rel(\"usually appears in\", \"bride\", \"wedding\").\n\toa_rel(\"is a sub-event of\", \"mincing\", \"cutting ingredients into tinier pieces\").\n\toa_rel(\"is\", \"cola\", \"liquid\").\n\toa_rel(\"is\", \"rabbit\", \"herbivorous\").\n\toa_rel(\"is used for\", \"backyard\", \"children to play in\").\n\toa_rel(\"can\", \"penguin\", \"swim\").\n\toa_rel(\"is\", \"hand dryer\", \"electric\").\n\toa_rel(\"is\", \"kitten\", \"carnivorous\").\n\toa_rel(\"has\", \"oyster\", \"iron\").\n\toa_rel(\"is used for\", \"camel\", \"riding\").\n", "oa_rel(\"is\", \"gorilla\", \"omnivorous\").\n\toa_rel(\"can\", \"rat\", \"be pet\").\n\toa_rel(\"is used for\", \"truck\", \"transporting handful of people\").\n\toa_rel(\"is used for\", \"instruments\", \"making music\").\n\toa_rel(\"is\", \"lemonade\", \"fluid\").\n\toa_rel(\"is used for\", \"backyard\", \"having barbecues\").\n\toa_rel(\"usually appears in\", \"trumpet\", \"orchestra\").\n\toa_rel(\"is used for\", \"laptop\", \"surfing internet\").\n\toa_rel(\"has\", \"cappuccino\", \"water\").\n\toa_rel(\"is\", \"caramel\", \"sticky\").\n", "oa_rel(\"has\", \"cheesecake\", \"starch\").\n\toa_rel(\"is\", \"chocolate\", \"sweet\").\n\toa_rel(\"can\", \"bicycle\", \"travel on road\").\n\toa_rel(\"is used for\", \"backyard\", \"dogs to run around in\").\n\toa_rel(\"is used for\", \"wine\", \"satisfying thirst\").\n\toa_rel(\"is\", \"perfume\", \"liquid\").\n\toa_rel(\"usually appears in\", \"beer mug\", \"bar\").\n\toa_rel(\"can\", \"snake\", \"be pet\").\n\toa_rel(\"is\", \"rifle\", \"dangerous\").\n\toa_rel(\"can be\", \"coffee\", \"drunk\").\n", "oa_rel(\"has\", \"tofu\", \"calcium\").\n\toa_rel(\"can\", \"air conditioner\", \"warm air\").\n\toa_rel(\"is used for\", \"pantry\", \"storing kitchen items\").\n\toa_rel(\"requires\", \"making pizza\", \"flour\").\n\toa_rel(\"is\", \"strawberry\", \"healthy\").\n\toa_rel(\"is used for\", \"laptop\", \"enjoyment\").\n\toa_rel(\"has\", \"waffle\", \"starch\").\n\toa_rel(\"usually appears in\", \"lion\", \"grassland\").\n\toa_rel(\"can\", \"eagle\", \"spot prey from afar\").\n\toa_rel(\"requires\", \"cleaning clothing\", \"detergent\").\n", "oa_rel(\"usually appears in\", \"dolphin\", \"water\").\n\toa_rel(\"is\", \"pesto\", \"sticky\").\n\toa_rel(\"can\", \"duck\", \"attempt to fly\").\n\toa_rel(\"is\", \"tiger\", \"dangerous\").\n\toa_rel(\"is used for\", \"backyard\", \"having party\").\n\toa_rel(\"is\", \"lime\", \"healthy\").\n\toa_rel(\"has\", \"lemonade\", \"water\").\n\toa_rel(\"can\", \"wheat\", \"provide complex carbohydrates\").\n\toa_rel(\"is\", \"yogurt\", \"fluid\").\n\toa_rel(\"usually appears in\", \"frog\", \"water\").\n", "oa_rel(\"usually appears in\", \"bass\", \"rock band\").\n\toa_rel(\"is used for\", \"couch\", \"lying down\").\n\toa_rel(\"usually appears in\", \"lizard\", \"dessert\").\n\toa_rel(\"has\", \"liquor\", \"water\").\n\toa_rel(\"can\", \"jeep\", \"move quickly\").\n\toa_rel(\"usually appears in\", \"shark\", \"ocean\").\n\toa_rel(\"usually appears in\", \"mixing bowl\", \"kitchen\").\n\toa_rel(\"is\", \"ice maker\", \"electric\").\n\toa_rel(\"usually appears in\", \"hamburger\", \"lunch\").\n\toa_rel(\"is made from\", \"cheeseburger\", \"flour\").\n", "oa_rel(\"is used for\", \"accordion\", \"polka music\").\n\toa_rel(\"is used for\", \"hammer\", \"pulling out nails\").\n\toa_rel(\"is a sub-event of\", \"baking cake\", \"stirring flour\").\n\toa_rel(\"can be\", \"cake\", \"eaten\").\n\toa_rel(\"has\", \"chicken\", \"vitamin B\").\n\toa_rel(\"is used for\", \"water bottle\", \"holding drinks\").\n\toa_rel(\"is\", \"gummy bear\", \"sweet\").\n\toa_rel(\"is\", \"fast food\", \"unhealthy\").\n\toa_rel(\"usually appears in\", \"ape\", \"jungle\").\n\toa_rel(\"is used for\", \"lemon\", \"making juice\").\n", "oa_rel(\"usually appears in\", \"hairbrush\", \"bathroom\").\n\toa_rel(\"can be\", \"water\", \"drunk\").\n\toa_rel(\"is used for\", \"cleat\", \"protecting feet\").\n\toa_rel(\"can\", \"penguin\", \"be pet\").\n\toa_rel(\"usually appears in\", \"pizza oven\", \"kitchen\").\n\toa_rel(\"has\", \"kangaroo\", \"two legs\").\n\toa_rel(\"is used for\", \"perfume\", \"aroma\").\n\toa_rel(\"requires\", \"baking cake\", \"flour\").\n\toa_rel(\"can be\", \"tea\", \"drunk\").\n\toa_rel(\"usually appears in\", \"whale\", \"ocean\").\n", "oa_rel(\"is\", \"cappuccino\", \"fluid\").\n\toa_rel(\"is\", \"cappuccino\", \"liquid\").\n\toa_rel(\"is\", \"washing machine\", \"electric\").\n\toa_rel(\"can\", \"turtle\", \"be pet\").\n\toa_rel(\"can\", \"frog\", \"swim\").\n\toa_rel(\"is used for\", \"subway\", \"transporting lots of people\").\n\toa_rel(\"usually appears in\", \"champagne\", \"bar\").\n\toa_rel(\"usually appears in\", \"skillet\", \"kitchen\").\n\toa_rel(\"is\", \"cotton candy\", \"sweet\").\n\toa_rel(\"usually appears in\", \"otter\", \"water\").\n", "oa_rel(\"can\", \"robin\", \"fly\").\n\toa_rel(\"is used for\", \"boat\", \"transportation\").\n\toa_rel(\"is used for\", \"crane\", \"lifting heavy weight\").\n\toa_rel(\"usually appears in\", \"grater\", \"kitchen\").\n\toa_rel(\"is used for\", \"trumpet\", \"playing music\").\n\toa_rel(\"is used for\", \"cafeteria\", \"eating\").\n\toa_rel(\"is\", \"yogurt\", \"sweet\").\n\toa_rel(\"is used for\", \"mall\", \"shopping\").\n\toa_rel(\"is used for\", \"boot\", \"protecting feet\").\n\toa_rel(\"can\", \"laptop\", \"run programs\").\n", "oa_rel(\"can be\", \"vest\", \"hung\").\n\toa_rel(\"is used for\", \"helmet\", \"protecting head\").\n\toa_rel(\"is\", \"vinegar\", \"liquid\").\n\toa_rel(\"is used for\", \"beans\", \"eating\").\n\toa_rel(\"can\", \"lizard\", \"sun to warm up\").\n\toa_rel(\"is used for\", \"milk\", \"satisfying thirst\").\n\toa_rel(\"can\", \"pickup\", \"spend gas\").\n\toa_rel(\"can\", \"hawk\", \"fly\").\n\toa_rel(\"can\", \"police\", \"tail criminal\").\n\toa_rel(\"is\", \"fast food\", \"cheap\").\n", "oa_rel(\"can\", \"violin\", \"play beautiful music\").\n\toa_rel(\"usually appears in\", \"eagle\", \"jungle\").\n\toa_rel(\"usually appears in\", \"beer mug\", \"restaurant\").\n\toa_rel(\"is used for\", \"hairbrush\", \"removing tangles from hair\").\n\toa_rel(\"is\", \"seal\", \"carnivorous\").\n\toa_rel(\"is used for\", \"factory\", \"making products\").\n\toa_rel(\"usually appears in\", \"chopsticks\", \"dining room\").\n\toa_rel(\"is\", \"pudding\", \"sweet\").\n\toa_rel(\"is used for\", \"shampoo\", \"washing hair\").\n\toa_rel(\"has\", \"oatmeal\", \"starch\").\n", "oa_rel(\"has\", \"kangaroo\", \"two arms\").\n\toa_rel(\"can\", \"jeep\", \"carry few persons\").\n\toa_rel(\"can\", \"lizard\", \"be pet\").\n\toa_rel(\"is used for\", \"attic\", \"storing dishes\").\n\toa_rel(\"is\", \"liquor\", \"fluid\").\n\toa_rel(\"can\", \"minivan\", \"spend gas\").\n\toa_rel(\"usually appears in\", \"fax machine\", \"office\").\n\toa_rel(\"can be\", \"polo shirt\", \"hung\").\n\toa_rel(\"is\", \"pizza oven\", \"electric\").\n\toa_rel(\"has\", \"gorilla\", \"two legs\").\n", "oa_rel(\"can\", \"minivan\", \"transport people\").\n\toa_rel(\"can\", \"snake\", \"hurt\").\n\toa_rel(\"requires\", \"making pizza\", \"yeast\").\n\toa_rel(\"can\", \"snake\", \"be dangerous\").\n\toa_rel(\"usually appears in\", \"cake pan\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"shaving cream\", \"bathroom\").\n\toa_rel(\"is\", \"crow\", \"omnivorous\").\n\toa_rel(\"usually appears in\", \"turtle\", \"water\").\n\toa_rel(\"is used for\", \"hairbrush\", \"brushing hair\").\n\toa_rel(\"is used for\", \"sneaker\", \"protecting feet\").\n", "oa_rel(\"can\", \"rice cooker\", \"cooking rice\").\n\toa_rel(\"is used for\", \"attic\", \"storing things that are not used\").\n\toa_rel(\"can\", \"hen\", \"fly\").\n\toa_rel(\"is used for\", \"water\", \"satisfying thirst\").\n\toa_rel(\"can\", \"camel\", \"work for days without water\").\n\toa_rel(\"has\", \"champagne\", \"water\").\n\toa_rel(\"is made from\", \"milkshake\", \"milk\").\n\toa_rel(\"can\", \"crow\", \"fly\").\n\toa_rel(\"is used for\", \"beer\", \"satisfying thirst\").\n\toa_rel(\"can\", \"parrot\", \"be pet\").\n", "oa_rel(\"is used for\", \"shampoo\", \"curing dandruff\").\n\toa_rel(\"is used for\", \"screwdriver\", \"fixing loose screws\").\n\toa_rel(\"is used for\", \"jet\", \"transporting people\").\n\toa_rel(\"is used for\", \"guitar\", \"making music\").\n\toa_rel(\"can\", \"sedan\", \"spend gas\").\n\toa_rel(\"is used for\", \"sedan\", \"transporting people\").\n\toa_rel(\"is used for\", \"attic\", \"keeping stuff in\").\n\toa_rel(\"is\", \"pesto\", \"fluid\").\n\toa_rel(\"is used for\", \"violin\", \"playing music\").\n\toa_rel(\"can be\", \"soda\", \"drunk\").\n", "oa_rel(\"can\", \"dolphin\", \"swim\").\n\toa_rel(\"is\", \"dark chocolate\", \"bitter\").\n\toa_rel(\"is used for\", \"cereal\", \"eating\").\n\toa_rel(\"has\", \"liquor\", \"alcohol\").\n\toa_rel(\"is\", \"wolf\", \"dangerous\").\n\toa_rel(\"has\", \"steak\", \"vitamin B\").\n\toa_rel(\"can\", \"bartender\", \"serve alcoholic or soft drink beverages\").\n\toa_rel(\"is used for\", \"drum\", \"making music\").\n\toa_rel(\"is made from\", \"bread loaf\", \"flour\").\n\toa_rel(\"is\", \"alpaca\", \"herbivorous\").\n", "oa_rel(\"is\", \"vacuum\", \"electric\").\n\toa_rel(\"is used for\", \"fork\", \"moving food\").\n\toa_rel(\"is used for\", \"mall\", \"buying and selling\").\n\toa_rel(\"can\", \"chicken\", \"fly\").\n\toa_rel(\"usually appears in\", \"electric toothbrush\", \"bathroom\").\n\toa_rel(\"usually appears in\", \"leopard\", \"jungle\").\n\toa_rel(\"usually appears in\", \"utensil holder\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"drum\", \"orchestra\").\n\toa_rel(\"requires\", \"cooking\", \"ingredients\").\n\toa_rel(\"can\", \"fork\", \"lift food\").\n", "oa_rel(\"can\", \"laptop\", \"save information\").\n\toa_rel(\"is used for\", \"baseball glove\", \"protecting hand\").\n\toa_rel(\"is used for\", \"attic\", \"storing clothes\").\n\toa_rel(\"can\", \"bald eagle\", \"fly\").\n\toa_rel(\"usually appears in\", \"banquet table\", \"restaurant\").\n\toa_rel(\"has\", \"bacon\", \"vitamin B\").\n\toa_rel(\"is used for\", \"laptop\", \"sending email\").\n\toa_rel(\"has\", \"ape\", \"two legs\").\n\toa_rel(\"can\", \"chicken\", \"spread wings\").\n\toa_rel(\"is\", \"shaving cream\", \"fluid\").\n", "oa_rel(\"can\", \"duck\", \"lay eggs\").\n\toa_rel(\"has\", \"gorilla\", \"two arms\").\n\toa_rel(\"can be\", \"shorts\", \"hung\").\n\toa_rel(\"is\", \"vinegar\", \"fluid\").\n\toa_rel(\"can be\", \"beer\", \"drunk\").\n\toa_rel(\"is used for\", \"accordion\", \"playing music\").\n\toa_rel(\"can be\", \"pants\", \"hung\").\n\toa_rel(\"can\", \"pickup\", \"move quickly\").\n\toa_rel(\"is made from\", \"oreo\", \"flour\").\n\toa_rel(\"is\", \"rabbit\", \"soft\").\n", "oa_rel(\"is\", \"silk\", \"soft\").\n\toa_rel(\"is used for\", \"laptop\", \"data storage\").\n\toa_rel(\"can be\", \"milk\", \"drunk\").\n\toa_rel(\"usually appears in\", \"drum\", \"rock band\").\n\toa_rel(\"usually appears in\", \"wedding gown\", \"wedding\").\n\toa_rel(\"is used for\", \"soda\", \"satisfying thirst\").\n\toa_rel(\"is used for\", \"sandal\", \"protecting feet\").\n\toa_rel(\"is\", \"hippo\", \"herbivorous\").\n\toa_rel(\"is used for\", \"cafe\", \"eating cookies\").\n\toa_rel(\"can be\", \"skirt\", \"hung\").\n", "oa_rel(\"can\", \"turtle\", \"live to be 200 years old\").\n\toa_rel(\"can\", \"laptop\", \"speed up research\").\n\toa_rel(\"has\", \"chicken liver\", \"iron\").\n\toa_rel(\"has\", \"beef liver\", \"iron\").\n\toa_rel(\"has\", \"rabbit\", \"two long ears\").\n\toa_rel(\"has\", \"ape\", \"two arms\").\n\toa_rel(\"requires\", \"cleaning clothing\", \"washing powders\").\n\toa_rel(\"requires\", \"cleaning house\", \"mop\").\n\toa_rel(\"requires\", \"baking bread\", \"yeast\").\n\toa_rel(\"requires\", \"baking bread\", \"bread pan\").\n", "oa_rel(\"requires\", \"beating egg\", \"electric mixer\").\n\toa_rel(\"is a sub-event of\", \"making pizza\", \"mixing flour, yeast, olive oil\").\n\toa_rel(\"is a sub-event of\", \"making pizza\", \"add sauce, cheese, meats, vegetables\").\n\toa_rel(\"is a sub-event of\", \"making coffee\", \"grinding coffee beans\").\n\toa_rel(\"can be\", \"\", \"diced\").\n\toa_rel(\"can be\", \"\", \"sliced\").\n\toa_rel(\"can be\", \"\", \"minced\").\n\toa_rel(\"can be\", \"\", \"shred\").\n\toa_rel(\"can be\", \"\", \"baked\").\n\toa_rel(\"is made of\", \"sushi\", \"rice\").\n", "oa_rel(\"is made from\", \"whey\", \"milk\").\n\toa_rel(\"is created by\", \"\", \"baking\").\n\toa_rel(\"is\", \"fast food\", \"bad for health\").\n\toa_rel(\"is\", \"doughnut\", \"sweet\").\n\toa_rel(\"is\", \"fudge\", \"sweet\").\n\toa_rel(\"is\", \"milk chocolate\", \"sweet\").\n\toa_rel(\"is\", \"cooking oil\", \"liquid\").\n\toa_rel(\"is\", \"hummus\", \"sticky\").\n\toa_rel(\"is\", \"leopard\", \"dangerous\").\n\toa_rel(\"is\", \"cheetah\", \"dangerous\").\n", "oa_rel(\"is\", \"bison\", \"herbivorous\").\n\toa_rel(\"is\", \"ape\", \"omnivorous\").\n\toa_rel(\"is\", \"cheetah\", \"carnivorous\").\n\toa_rel(\"is\", \"raccoon\", \"carnivorous\").\n\toa_rel(\"is\", \"otter\", \"carnivorous\").\n\toa_rel(\"is\", \"copier\", \"electric\").\n\toa_rel(\"is used for\", \"fork\", \"piercing solid food\").\n\toa_rel(\"is used for\", \"fork\", \"moving solid food\").\n\toa_rel(\"is used for\", \"spoon\", \"eating soft food\").\n\toa_rel(\"is used for\", \"spoon\", \"moving soft food to mouth\").\n", "oa_rel(\"is used for\", \"spoon\", \"moving soft food\").\n\toa_rel(\"is used for\", \"spoon\", \"scooping soft food\").\n\toa_rel(\"is capable of\", \"microwave\", \"heating food\").\n\toa_rel(\"is used for\", \"harp\", \"playing music\").\n\toa_rel(\"can be used for\", \"drum\", \"playing in orchestra\").\n\toa_rel(\"can be used for\", \"wood\", \"campfires\").\n\toa_rel(\"is used for\", \"perfume\", \"perfuming\").\n\toa_rel(\"can\", \"steering wheel\", \"control vehicle\").\n\toa_rel(\"can\", \"steering wheel\", \"control direction of vehicle\").\n\toa_rel(\"can\", \"finch\", \"fly\").\n", "oa_rel(\"can\", \"firefly\", \"fly\").\n\toa_rel(\"can\", \"snake\", \"bite\").\n\toa_rel(\"can\", \"lizard\", \"sun itself on rock\").\n\toa_rel(\"can\", \"turtle\", \"swim\").\n\toa_rel(\"can\", \"otter\", \"swim\").\n\toa_rel(\"can\", \"starch\", \"provide complex carbohydrates\").\n\toa_rel(\"can\", \"starch\", \"provide energy\").\n\toa_rel(\"is good at\", \"chef\", \"cook food\").\n\toa_rel(\"is good at\", \"chef\", \"prepare food\").\n\toa_rel(\"is good at\", \"chef\", \"season food\").\n", "oa_rel(\"is good at\", \"chef\", \"cook fish\").\n\toa_rel(\"is good at\", \"chef\", \"season meat\").\n\toa_rel(\"can\", \"catcher\", \"catch baseball\").\n\toa_rel(\"can\", \"snowplows\", \"clear snow from roads\").\n\toa_rel(\"can\", \"cell phone\", \"vibrate\").\n\toa_rel(\"can\", \"helmet\", \"prevent head injuries\").\n\toa_rel(\"can\", \"spoon\", \"scoop food\").\n\toa_rel(\"usually appears in\", \"main course\", \"dinner\").\n\toa_rel(\"usually appears in\", \"side dishes\", \"dinner\").\n\toa_rel(\"usually appears in\", \"fast food\", \"lunch\").\n", "oa_rel(\"usually appears in\", \"soft drinks\", \"lunch\").\n\toa_rel(\"usually appears in\", \"violin\", \"orchestra\").\n\toa_rel(\"usually appears in\", \"clarinet\", \"orchestra\").\n\toa_rel(\"usually appears in\", \"trumpet\", \"brass band\").\n\toa_rel(\"usually appears in\", \"gas stove\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"cooking pot\", \"kitchen\").\n\toa_rel(\"usually appears in\", \"beer mug\", \"dining room\").\n\toa_rel(\"usually appears in\", \"barkeeper\", \"bar\").\n\toa_rel(\"usually appears in\", \"cocktail cabinet\", \"bar\").\n\toa_rel(\"usually appears in\", \"liquor\", \"bar\").\n", "oa_rel(\"usually appears in\", \"restaurant table\", \"restaurant\").\n\toa_rel(\"usually appears in\", \"copier\", \"office\").\n\toa_rel(\"usually appears in\", \"camel\", \"dessert\").\n\toa_rel(\"usually appears in\", \"jaguar\", \"jungle\").\n\toa_rel(\"usually appears in\", \"gorilla\", \"jungle\").\n\toa_rel(\"usually appears in\", \"bison\", \"grassland\").\n\toa_rel(\"usually appears in\", \"rhino\", \"grassland\").\n\toa_rel(\"usually appears in\", \"moose\", \"meadow\").\n\tis_a(person_type_01, person).\n\tis_a(person_type_02, person).\n", "is_a(person_type_03, person).\n\tis_a(person_type_04, person).\n\tis_a(person_type_05, person).\n\tis_a(man, person_type_01).\n\tis_a(woman, person_type_01).\n\tis_a(lady, person_type_02).\n\tis_a(gentleman, person_type_02).\n\tis_a(boy, person_type_03).\n\tis_a(girl, person_type_03).\n\tis_a(baby, person_type_04).\n", "is_a(toddler, person_type_04).\n\tis_a(child, person_type_04).\n\tis_a(teenager, person_type_04).\n\tis_a(adult, person_type_04).\n\tis_a(elder, person_type_04).\n\tis_a(pedestrian, person_type_05).\n\tis_a(passenger, person_type_05).\n\tis_a(spectator, person_type_05).\n\tis_a(tourist, person_type_05).\n\tis_a(spectators, person_type_05).\n", "is_a(customers, person_type_05).\n\tis_a(visitor, person_type_05).\n\t'''\n\tdef npp_sub(match):\n\t    #print(match.group(0))\n\t    #print(match.group(0).replace(\" \", \"_\").replace('\"','').replace(\"-\",\"\"))\n\t    return match.group(0).replace(\" \", \"_\").replace('\"','').replace(\"-\",\"_\").replace(\",\",\"\")\n\tKG_REL = KG_REL.replace('\", \"','\"; \"').lower()\n\tKG_REL = re.sub(r\"\\\"[a-z \\-,]*\\\"\", npp_sub, KG_REL).replace(\";\",\",\").replace(\", ,\",\", \")\n\tKG_FACTS= KG_FACTS.replace('\", \"','\"; \"').lower()\n", "KG_FACTS = re.sub(r\"\\\"[a-z \\-,]*\\\"\", npp_sub, KG_FACTS).replace(\";\",\",\").replace(\", ,\",\", \")\n\tKG = KG_REL + KG_FACTS\n\tRULES_old = '''is_a(A, C) :- is_a(A, B), is_a(B, C).\n\tis_a(N, \"thing\") :- name(N, T).\n\tname(N1, Oid) :- name(N2, Oid), is_a(N2, N1).\n\toa_rel(\"is a type of\", Obj, Attr) :- is_a(Obj, Attr).\n\toa_rel(R, Obj, Attr) :- in_oa_rel(R, Obj, Attr).\n\toa_rel(R, Obj, Attr) :- is_a(Obj, T), oa_rel(R, T, Attr).'''\n\t# RULES_OLD = '''\n\t# %sixRULES\n", "# is_a(A, C) :- is_a(A, B), is_a(B, C). \n\t# is_a(N, thing) :- name(N, T).\n\t# name(N1, Oid) :- name(N2, Oid), is_a(N2, N1).\n\t# oa_rel(is_a_type_of, Obj, Attr) :- is_a(Obj, Attr).\n\t# oa_rel(R, Obj, Attr) :- in_oa_rel(R, Obj, Attr).\n\t# oa_rel(R, Obj, Attr) :- is_a(Obj, T), oa_rel(R, T, Attr).\n\t# '''\n\tRULES = '''\n\t%sixRULES\n\tis_a(A, C) :- is_a(A, B), is_a(B, C). \n", "is_a(N, thing) :- name(T,N).\n\tname(Oid,N1) :- name(Oid,N2), is_a(N2,N1).\n\toa_rel(is_a_type_of, Obj, Attr) :- is_a(Obj, Attr).\n\t%oa_rel(R, Obj, Attr) :- in_oa_rel(R, Obj, Attr).\n\toa_rel(R, Obj, Attr) :- is_a(Obj, T), oa_rel(R, T, Attr).\n\t'''\n\t\"\"\"\n\t['window', 'man', 'shirt', 'tree', 'wall', 'person', 'building', 'ground', 'sky', 'sign', 'head', 'pole', 'hand', 'grass', 'hair', 'leg', 'car', 'woman', 'leaves', 'table', 'trees', 'ear', 'pants', 'people', 'eye', 'door', 'water', 'fence', 'wheel', 'nose', 'chair', 'floor', 'arm', 'jacket', 'hat', 'shoe', 'tail', 'face', 'leaf', 'clouds', 'number', 'letter', 'plate', 'windows', 'shorts', 'road', 'sidewalk', 'flower', 'bag', 'helmet', 'snow', 'rock', 'boy', 'tire', 'logo', 'cloud', 'roof', 'glass', 'street', 'foot', 'legs', 'umbrella', 'post', 'jeans', 'mouth', 'boat', 'cap', 'bottle', 'girl', 'bush', 'shoes', 'flowers', 'glasses', 'field', 'picture', 'mirror', 'bench', 'box', 'bird', 'dirt', 'clock', 'neck', 'food', 'letters', 'bowl', 'shelf', 'bus', 'train', 'pillow', 'trunk', 'horse', 'plant', 'coat', 'airplane', 'lamp', 'wing', 'kite', 'elephant', 'paper', 'seat', 'dog', 'cup', 'house', 'counter', 'sheep', 'street light', 'glove', 'banana', 'branch', 'giraffe', 'rocks', 'cow', 'book', 'truck', 'racket', 'flag', 'ceiling', 'skateboard', 'cabinet', 'eyes', 'ball', 'zebra', 'bike', 'wheels', 'sand', 'hands', 'surfboard', 'frame', 'feet', 'windshield', 'finger', 'motorcycle', 'player', 'bushes', 'hill', 'child', 'bed', 'cat', 'sink', 'container', 'sock', 'tie', 'towel', 'traffic light', 'pizza', 'paw', 'backpack', 'collar', 'mountain', 'lid', 'basket', 'vase', 'phone', 'animal', 'sticker', 'branches', 'donut', 'lady', 'mane', 'license plate', 'cheese', 'fur', 'laptop', 'uniform', 'wire', 'fork', 'beach', 'wrist', 'buildings', 'word', 'desk', 'toilet', 'cars', 'curtain', 'pot', 'bear', 'ears', 'tag', 'dress', 'tower', 'faucet', 'screen', 'cell phone', 'watch', 'keyboard', 'arrow', 'sneakers', 'stone', 'blanket', 'broccoli', 'orange', 'numbers', 'drawer', 'knife', 'fruit', 'ocean', 't-shirt', 'cord', 'guy', 'spots', 'apple', 'napkin', 'cone', 'bread', 'bananas', 'sweater', 'cake', 'bicycle', 'skis', 'vehicle', 'room', 'couch', 'frisbee', 'horn', 'air', 'plants', 'trash can', 'camera', 'paint', 'ski', 'tomato', 'tiles', 'belt', 'words', 'television', 'wires', 'tray', 'socks', 'pipe', 'bat', 'rope', 'bathroom', 'carrot', 'suit', 'books', 'boot', 'sauce', 'ring', 'spoon', 'bricks', 'meat', 'van', 'bridge', 'goggles', 'platform', 'gravel', 'vest', 'label', 'stick', 'pavement', 'beak', 'refrigerator', 'computer', 'wetsuit', 'mountains', 'gloves', 'balcony', 'tree trunk', 'carpet', 'skirt', 'palm tree', 'fire hydrant', 'chain', 'kitchen', 'jersey', 'candle', 'remote control', 'shore', 'boots', 'rug', 'suitcase', 'computer mouse', 'clothes', 'street sign', 'pocket', 'outlet', 'can', 'snowboard', 'net', 'horns', 'pepper', 'doors', 'stairs', 'scarf', 'gate', 'graffiti', 'purse', 'luggage', 'beard', 'vegetables', 'bracelet', 'necklace', 'wristband', 'parking lot', 'park', 'train tracks', 'onion', 'dish', 'statue', 'sun', 'vegetable', 'sandwich', 'arms', 'star', 'doorway', 'wine', 'path', 'skier', 'teeth', 'men', 'stove', 'crust', 'weeds', 'chimney', 'chairs', 'feathers', 'monitor', 'home plate', 'speaker', 'fingers', 'steps', 'catcher', 'trash', 'forest', 'blinds', 'log', 'bathtub', 'eye glasses', 'outfit', 'cabinets', 'countertop', 'lettuce', 'pine tree', 'oven', 'city', 'walkway', 'jar', 'cart', 'tent', 'curtains', 'painting', 'dock', 'cockpit', 'step', 'pen', 'poster', 'light switch', 'hot dog', 'frosting', 'bucket', 'bun', 'pepperoni', 'crowd', 'thumb', 'propeller', 'symbol', 'liquid', 'tablecloth', 'pan', 'runway', 'train car', 'teddy bear', 'baby', 'microwave', 'headband', 'earring', 'store', 'spectator', 'fan', 'headboard', 'straw', 'stop sign', 'skin', 'pillows', 'display', 'couple', 'ladder', 'bottles', 'sprinkles', 'power lines', 'tennis ball', 'papers', 'decoration', 'burner', 'birds', 'umpire', 'grill', 'brush', 'cable', 'tongue', 'smoke', 'canopy', 'wings', 'controller', 'carrots', 'river', 'taxi', 'drain', 'spectators', 'sheet', 'game', 'chicken', 'american flag', 'mask', 'stones', 'batter', 'topping', 'wine glass', 'suv', 'tank top', 'scissors', 'barrier', 'hills', 'olive', 'planter', 'animals', 'plates', 'cross', 'mug', 'baseball', 'boats', 'telephone pole', 'mat', 'crosswalk', 'lips', 'mushroom', 'hay', 'toilet paper', 'tape', 'hillside', 'surfer', 'apples', 'donuts', 'station', 'mustache', 'children', 'pond', 'tires', 'toy', 'walls', 'flags', 'boxes', 'sofa', 'tomatoes', 'bags', 'sandals', 'onions', 'sandal', 'oranges', 'paws', 'pitcher', 'houses', 'baseball bat', 'moss', 'duck', 'apron', 'airport', 'light bulb', 'drawers', 'toothbrush', 'shelves', 'potato', 'light fixture', 'umbrellas', 'drink', 'heart', 'fence post', 'egg', 'hose', 'power line', 'fireplace', 'icing', 'nightstand', 'vehicles', 'magnet', 'beer', 'hook', 'comforter', 'lake', 'bookshelf', 'fries', 'peppers', 'coffee table', 'sweatshirt', 'shower', 'jet', 'water bottle', 'cows', 'entrance', 'driver', 'towels', 'soap', 'sail', 'crate', 'utensil', 'salad', 'kites', 'paddle', 'mound', 'tree branch']\n\t['white', 'black', 'green', 'blue', 'brown', 'red', 'gray', 'large', 'small', 'wooden', 'yellow', 'tall', 'metal', 'orange', 'long', 'dark', 'silver', 'pink', 'standing', 'clear', 'round', 'glass', 'open', 'sitting', 'short', 'parked', 'plastic', 'walking', 'brick', 'tan', 'purple', 'striped', 'colorful', 'cloudy', 'hanging', 'concrete', 'blond', 'bare', 'empty', 'young', 'old', 'closed', 'baseball', 'happy', 'bright', 'wet', 'gold', 'stone', 'smiling', 'light', 'dirty', 'flying', 'shiny', 'plaid', 'on', 'thin', 'square', 'tennis', 'little', 'sliced', 'leafy', 'playing', 'thick', 'beige', 'steel', 'calm', 'rectangular', 'dry', 'tiled', 'leather', 'eating', 'painted', 'ceramic', 'pointy', 'lying', 'surfing', 'snowy', 'paved', 'clean', 'fluffy', 'electric', 'cooked', 'grassy', 'stacked', 'full', 'covered', 'paper', 'framed', 'lit', 'blurry', 'grazing', 'flat', 'leafless', 'skiing', 'curved', 'light brown', 'beautiful', 'decorative', 'up', 'folded', 'sandy', 'chain-link', 'arched', 'overcast', 'cut', 'wide', 'running', 'waiting', 'ripe', 'long sleeved', 'furry', 'rusty', 'short sleeved', 'down', 'light blue', 'cloudless', 'dark brown', 'high', 'hazy', 'fresh', 'chocolate', 'cream colored', 'baby', 'worn', 'bent', 'light colored', 'rocky', 'skinny', 'curly', 'patterned', 'driving', 'jumping', 'maroon', 'riding', 'raised', 'lush', 'dark blue', 'off', 'cardboard', 'reflective', 'bald', 'iron', 'floral', 'black and white', 'melted', 'piled', 'skateboarding', 'rubber', 'talking', 'chrome', 'wire', 'puffy', 'broken', 'smooth', 'low', 'evergreen', 'narrow', 'denim', 'grouped', 'wicker', 'straight', 'triangular', 'sunny', 'dried', 'bushy', 'resting', 'sleeping', 'wrinkled', 'adult', 'dark colored', 'hairy', 'khaki', 'stuffed', 'wavy', 'nike', 'chopped', 'curled', 'shirtless', 'splashing', 'posing', 'upside down', 'ski', 'water', 'pointing', 'double decker', 'glazed', 'male', 'marble', 'fried', 'rock', 'ornate', 'wild', 'shining', 'tinted', 'asphalt', 'filled', 'floating', 'burnt', 'crossed', 'fuzzy', 'outdoors', 'overhead', 'potted', 'muddy', 'pale', 'decorated', 'swinging', 'asian', 'sharp', 'floppy', 'outstretched', 'rough', 'drinking', 'displayed', 'lined', 'having meeting', 'reflected', 'delicious', 'barefoot', 'plain', 'healthy', 'printed', 'frosted', 'crouching', 'written', 'illuminated', 'aluminum', 'digital', 'skating', 'trimmed', 'patchy', 'bending', 'soft', 'toasted', 'neon', 'choppy', 'fake', 'wispy', 'toy', 'knit', 'uncooked', 'vertical', 'tied', 'female', 'straw', 'grilled', 'rolled', 'rounded', 'wrapped', 'attached', 'messy', 'bathroom', 'swimming', 'deep', 'pretty', 'sleeveless', 'granite', 'rainbow colored', 'fallen', 'modern', 'kneeling', 'kitchen', 'turned', 'used', 'murky', 'busy', 'snowboarding', 'still', 'soccer', 'mounted', 'antique', 'street', 'watching', 'slanted', 'faded', 'glowing', 'teal', 'gravel', 'balding', 'outdoor', 'heavy', 'cracked', 'snow', 'baked', 'fancy', 'transparent', 'roman', 'vintage', 'old fashioned', 'cooking', 'horizontal', 'crouched', 'shredded', 'computer', 'docked', 'navy', 'octagonal', 'shallow', 'sparse', 'hard', 'cotton', 'fat', 'waving', 'carpeted', 'electronic', 'protective', 'foggy', 'rippled', 'cloth', 'squatting', 'shaggy', 'scattered', 'new', 'barren', 'stained', 'wireless', 'designed', 'reading', 'overgrown', 'apple', 'looking down', 'sliding', 'ocean', 'plush', 'tilted', 'shaded', 'dusty', 'flowered', 'crumpled', 'dotted', 'mesh', 'collared', 'traffic', 'woven', 'athletic', 'lighted', 'power', 'clay', 'made', 'crispy', 'elderly', 'bunched', 'warm', 'cluttered', 'brunette', 'textured', 'spread', 'brass', 'chinese', 'styrofoam', 'elevated', 'palm', 'blowing', 'copper', 'foamy', 'unripe', 'cheese', 'sheer', 'padded', 'half full', 'shaped', 'laughing', 'winter', 'manicured', 'indoors', 'speckled', 'batting', 'loose', 'caucasian', 'peeling', 'diced', 'dense', 'adidas', 'military', 'tin', 'halved', 'jeans', 'wired', 'mixed', 'bronze', 'alert', 'neat', 'public', 'drawn', 'cobblestone', 'massive', 'real', 'peeled', 'assorted', 'sunlit', 'chipped', 'blooming', 'american', 'perched', 'shaved', 'spiky', 'wool', 'bamboo', 'gloved', 'spraying', 'commercial', 'steep', 'braided', 'steamed', 'tight', 'partly cloudy', 'unpeeled', 'carved', 'damaged', 'edged', 'directional', 'strong', 'pulled back', 'wrinkly', 'muscular', 'tabby', 'mowed', 'disposable', 'portable', 'shut', 'looking up', 'sad', 'packed', 'rippling', 'vanilla', 'eaten', 'barbed', 'safety', 'shingled', 'roasted', 'juicy', 'pine', 'shadowed', 'diamond', 'crystal', 'angled', 'weathered', 'homemade', 'seasoned', 'paneled', 'sloped', 'sweet', 'miniature', 'domed', 'pizza', 'torn', 'crowded', 'tail', 'beaded', 'suspended', 'cushioned', 'tangled', 'stormy', 'blank', 'tasty', 'hitting', 'deciduous', 'frozen', 'license', 'fenced', 'sprinkled', 'feathered', 'dull', 'tropical', 'twisted', 'crooked', 'wine', 'browned', 'cylindrical', 'greasy', 'abandoned', 'capital', 'toilet', 'unlit', 'iced', 'upholstered', 'gloomy', 'professional', 'creamy', 'analog', 'jagged', 'oblong', 'ruffled', 'connected', 'ivory', 'christmas', 'upper', 'coffee', 'lace', 'ridged', 'comfortable', 'foreign', 'knotted', 'french', 'support', 'performing trick', 'artificial', 'spiral', 'crumbled', 'dangling', 'cordless', 'wedding', 'fire', 'rustic', 'trash', 'uneven', 'grated', 'curvy', 'telephone', 'unoccupied', 'curious', 'rotten', 'oriental', 'folding', 'crusty', 'rimmed', 'birthday', 'polished', 'funny', 'crashing', 'powdered', 'lower', 'winding', 'calico', 'banana', 'angry', 'inflated', 'scrambled', 'wii', 'vast', 'sleepy', 'rainy', 'groomed', 'glossy', 'wooded', 'sturdy', 'vibrant', 'misty', 'beer', 'office', 'melting', 'silk', 'discolored', 'corded', 'formal', 'vacant', 'oversized', 'vinyl', 'spinning', 'simple', 'translucent', 'mature', 'tomato', 'crisp', 'boiled', 'recessed', 'gas', 'handmade', 'clumped', 'railroad', 'rugged', 'sculpted', 'burning', 'patched', 'industrial', 'chubby', 'ugly', 'garbage', 'powerful', 'elongated', 'pepper', 'abundant', 'opaque', 'ornamental', 'fluorescent', 'soda', 'packaged', 'inflatable', 'unpaved', 'soap', 'strawberry', 'park', 'exterior', 'intricate', 'sealed', 'quilted', 'hollow', 'breakable', 'pastel', 'urban', 'wrist', 'breaking', 'regular', 'forested', 'abstract', 'unhappy', 'incomplete', 'fine', 'chalk', 'coarse', 'polo', 'unhealthy', 'irregular', 'polar', 'uncomfortable', 'typical', 'complete', 'scarce', 'immature']\n\t['right', 'left', 'on', 'wearing', 'near', 'in', 'of', 'behind', 'in front of', 'under', 'holding', 'with', 'by', 'on the side of', 'watching', 'inside', 'carrying', 'eating', 'at', 'riding', 'playing', 'covered by', 'covering', 'touching', 'on the front of', 'full of', 'riding on', 'using', 'covered in', 'sitting at', 'playing with', 'crossing', 'on the back of', 'surrounded by', 'reflected in', 'covered with', 'flying', 'pulled by', 'contain', 'standing by', 'driving on', 'surrounding', 'walking down', 'printed on', 'attached to', 'talking on', 'facing', 'driving', 'worn on', 'floating in', 'grazing on', 'on the bottom of', 'standing in front of', 'topped with', 'playing in', 'walking with', 'swimming in', 'driving down', 'pushed by', 'playing on', 'close to', 'waiting for', 'between', 'running on', 'tied to', 'on the edge of', 'holding onto', 'talking to', 'eating from', 'perched on', 'parked by', 'reaching for', 'connected to', 'grazing in', 'floating on', 'wrapped around', 'skiing on', 'stacked on', 'parked at', 'standing at', 'walking across', 'plugged into', 'stuck on', 'stuck in', 'drinking from', 'seen through', 'sitting by', 'sitting in front of', 'looking out', 'parked in front of', 'petting', 'wrapped in', 'selling', 'parked along', 'coming from', 'lying inside', 'sitting inside', 'walking by', 'sitting with', 'making', 'walking through', 'hung on', 'walking along', 'going down', 'leaving', 'running in', 'flying through', 'mounted to', 'on the other side of', 'licking', 'sniffing', 'followed by', 'following', 'riding in', 'biting', 'parked alongside', 'chasing', 'leading', 'hanging off', 'helping', 'coming out of', 'hanging out of', 'staring at', 'walking toward', 'served on', 'hugging', 'entering', 'skiing in', 'looking in', 'draped over', 'tied around', 'exiting', 'looking down at', 'looking into', 'drawn on', 'balancing on', 'jumping over', 'posing with', 'reflecting in', 'eating at', 'walking up', 'sewn on', 'reflected on', 'about to hit', 'getting on', 'observing', 'approaching', 'traveling on', 'walking towards', 'wading in', 'growing by', 'displayed on', 'growing along', 'mixed with', 'grabbing', 'jumping on', 'scattered on', 'climbing', 'pointing at', 'opening', 'taller than', 'going into', 'decorated by', 'decorating', 'preparing', 'coming down', 'tossing', 'eating in', 'growing from', 'chewing', 'washing', 'herding', 'skiing down', 'looking through', 'picking up', 'trying to catch', 'standing against', 'looking over', 'typing on', 'piled on', 'tying', 'shining through', 'smoking', 'cleaning', 'walking to', 'smiling at', 'guiding', 'dragging', 'chained to', 'going through', 'enclosing', 'adjusting', 'running through', 'skating on', 'photographing', 'smelling', 'kissing', 'falling off', 'decorated with', 'walking into', 'worn around', 'walking past', 'blowing out', 'towing', 'jumping off', 'sprinkled on', 'moving', 'running across', 'hidden by', 'traveling down', 'splashing', 'hang from', 'looking toward', 'kept in', 'cooked in', 'displayed in', 'sitting atop', 'brushing', 'buying', 'in between', 'larger than', 'smaller than', 'pouring', 'playing at', 'longer than', 'higher than', 'jumping in', 'shorter than', 'bigger than']\n", "\"\"\"\n\t\"\"\" NAMES\n\t0 window\n\t1 man\n\t2 shirt\n\t3 tree\n\t4 wall\n\t5 person\n\t6 building\n\t7 ground\n", "8 sky\n\t9 sign\n\t10 head\n\t11 pole\n\t12 hand\n\t13 grass\n\t14 hair\n\t15 leg\n\t16 car\n\t17 woman\n", "18 leaves\n\t19 table\n\t20 trees\n\t21 ear\n\t22 pants\n\t23 people\n\t24 eye\n\t25 door\n\t26 water\n\t27 fence\n", "28 wheel\n\t29 nose\n\t30 chair\n\t31 floor\n\t32 arm\n\t33 jacket\n\t34 hat\n\t35 shoe\n\t36 tail\n\t37 face\n", "38 leaf\n\t39 clouds\n\t40 number\n\t41 letter\n\t42 plate\n\t43 windows\n\t44 shorts\n\t45 road\n\t46 sidewalk\n\t47 flower\n", "48 bag\n\t49 helmet\n\t50 snow\n\t51 rock\n\t52 boy\n\t53 tire\n\t54 logo\n\t55 cloud\n\t56 roof\n\t57 glass\n", "58 street\n\t59 foot\n\t60 legs\n\t61 umbrella\n\t62 post\n\t63 jeans\n\t64 mouth\n\t65 boat\n\t66 cap\n\t67 bottle\n", "68 girl\n\t69 bush\n\t70 shoes\n\t71 flowers\n\t72 glasses\n\t73 field\n\t74 picture\n\t75 mirror\n\t76 bench\n\t77 box\n", "78 bird\n\t79 dirt\n\t80 clock\n\t81 neck\n\t82 food\n\t83 letters\n\t84 bowl\n\t85 shelf\n\t86 bus\n\t87 train\n", "88 pillow\n\t89 trunk\n\t90 horse\n\t91 plant\n\t92 coat\n\t93 airplane\n\t94 lamp\n\t95 wing\n\t96 kite\n\t97 elephant\n", "98 paper\n\t99 seat\n\t100 dog\n\t101 cup\n\t102 house\n\t103 counter\n\t104 sheep\n\t105 street light\n\t106 glove\n\t107 banana\n", "108 branch\n\t109 giraffe\n\t110 rocks\n\t111 cow\n\t112 book\n\t113 truck\n\t114 racket\n\t115 flag\n\t116 ceiling\n\t117 skateboard\n", "118 cabinet\n\t119 eyes\n\t120 ball\n\t121 zebra\n\t122 bike\n\t123 wheels\n\t124 sand\n\t125 hands\n\t126 surfboard\n\t127 frame\n", "128 feet\n\t129 windshield\n\t130 finger\n\t131 motorcycle\n\t132 player\n\t133 bushes\n\t134 hill\n\t135 child\n\t136 bed\n\t137 cat\n", "138 sink\n\t139 container\n\t140 sock\n\t141 tie\n\t142 towel\n\t143 traffic light\n\t144 pizza\n\t145 paw\n\t146 backpack\n\t147 collar\n", "148 mountain\n\t149 lid\n\t150 basket\n\t151 vase\n\t152 phone\n\t153 animal\n\t154 sticker\n\t155 branches\n\t156 donut\n\t157 lady\n", "158 mane\n\t159 license plate\n\t160 cheese\n\t161 fur\n\t162 laptop\n\t163 uniform\n\t164 wire\n\t165 fork\n\t166 beach\n\t167 wrist\n", "168 buildings\n\t169 word\n\t170 desk\n\t171 toilet\n\t172 cars\n\t173 curtain\n\t174 pot\n\t175 bear\n\t176 ears\n\t177 tag\n", "178 dress\n\t179 tower\n\t180 faucet\n\t181 screen\n\t182 cell phone\n\t183 watch\n\t184 keyboard\n\t185 arrow\n\t186 sneakers\n\t187 stone\n", "188 blanket\n\t189 broccoli\n\t190 orange\n\t191 numbers\n\t192 drawer\n\t193 knife\n\t194 fruit\n\t195 ocean\n\t196 t-shirt\n\t197 cord\n", "198 guy\n\t199 spots\n\t200 apple\n\t201 napkin\n\t202 cone\n\t203 bread\n\t204 bananas\n\t205 sweater\n\t206 cake\n\t207 bicycle\n", "208 skis\n\t209 vehicle\n\t210 room\n\t211 couch\n\t212 frisbee\n\t213 horn\n\t214 air\n\t215 plants\n\t216 trash can\n\t217 camera\n", "218 paint\n\t219 ski\n\t220 tomato\n\t221 tiles\n\t222 belt\n\t223 words\n\t224 television\n\t225 wires\n\t226 tray\n\t227 socks\n", "228 pipe\n\t229 bat\n\t230 rope\n\t231 bathroom\n\t232 carrot\n\t233 suit\n\t234 books\n\t235 boot\n\t236 sauce\n\t237 ring\n", "238 spoon\n\t239 bricks\n\t240 meat\n\t241 van\n\t242 bridge\n\t243 goggles\n\t244 platform\n\t245 gravel\n\t246 vest\n\t247 label\n", "248 stick\n\t249 pavement\n\t250 beak\n\t251 refrigerator\n\t252 computer\n\t253 wetsuit\n\t254 mountains\n\t255 gloves\n\t256 balcony\n\t257 tree trunk\n", "258 carpet\n\t259 skirt\n\t260 palm tree\n\t261 fire hydrant\n\t262 chain\n\t263 kitchen\n\t264 jersey\n\t265 candle\n\t266 remote control\n\t267 shore\n", "268 boots\n\t269 rug\n\t270 suitcase\n\t271 computer mouse\n\t272 clothes\n\t273 street sign\n\t274 pocket\n\t275 outlet\n\t276 can\n\t277 snowboard\n", "278 net\n\t279 horns\n\t280 pepper\n\t281 doors\n\t282 stairs\n\t283 scarf\n\t284 gate\n\t285 graffiti\n\t286 purse\n\t287 luggage\n", "288 beard\n\t289 vegetables\n\t290 bracelet\n\t291 necklace\n\t292 wristband\n\t293 parking lot\n\t294 park\n\t295 train tracks\n\t296 onion\n\t297 dish\n", "298 statue\n\t299 sun\n\t300 vegetable\n\t301 sandwich\n\t302 arms\n\t303 star\n\t304 doorway\n\t305 wine\n\t306 path\n\t307 skier\n", "308 teeth\n\t309 men\n\t310 stove\n\t311 crust\n\t312 weeds\n\t313 chimney\n\t314 chairs\n\t315 feathers\n\t316 monitor\n\t317 home plate\n", "318 speaker\n\t319 fingers\n\t320 steps\n\t321 catcher\n\t322 trash\n\t323 forest\n\t324 blinds\n\t325 log\n\t326 bathtub\n\t327 eye glasses\n", "328 outfit\n\t329 cabinets\n\t330 countertop\n\t331 lettuce\n\t332 pine tree\n\t333 oven\n\t334 city\n\t335 walkway\n\t336 jar\n\t337 cart\n", "338 tent\n\t339 curtains\n\t340 painting\n\t341 dock\n\t342 cockpit\n\t343 step\n\t344 pen\n\t345 poster\n\t346 light switch\n\t347 hot dog\n", "348 frosting\n\t349 bucket\n\t350 bun\n\t351 pepperoni\n\t352 crowd\n\t353 thumb\n\t354 propeller\n\t355 symbol\n\t356 liquid\n\t357 tablecloth\n", "358 pan\n\t359 runway\n\t360 train car\n\t361 teddy bear\n\t362 baby\n\t363 microwave\n\t364 headband\n\t365 earring\n\t366 store\n\t367 spectator\n", "368 fan\n\t369 headboard\n\t370 straw\n\t371 stop sign\n\t372 skin\n\t373 pillows\n\t374 display\n\t375 couple\n\t376 ladder\n\t377 bottles\n", "378 sprinkles\n\t379 power lines\n\t380 tennis ball\n\t381 papers\n\t382 decoration\n\t383 burner\n\t384 birds\n\t385 umpire\n\t386 grill\n\t387 brush\n", "388 cable\n\t389 tongue\n\t390 smoke\n\t391 canopy\n\t392 wings\n\t393 controller\n\t394 carrots\n\t395 river\n\t396 taxi\n\t397 drain\n", "398 spectators\n\t399 sheet\n\t400 game\n\t401 chicken\n\t402 american flag\n\t403 mask\n\t404 stones\n\t405 batter\n\t406 topping\n\t407 wine glass\n", "408 suv\n\t409 tank top\n\t410 scissors\n\t411 barrier\n\t412 hills\n\t413 olive\n\t414 planter\n\t415 animals\n\t416 plates\n\t417 cross\n", "418 mug\n\t419 baseball\n\t420 boats\n\t421 telephone pole\n\t422 mat\n\t423 crosswalk\n\t424 lips\n\t425 mushroom\n\t426 hay\n\t427 toilet paper\n", "428 tape\n\t429 hillside\n\t430 surfer\n\t431 apples\n\t432 donuts\n\t433 station\n\t434 mustache\n\t435 children\n\t436 pond\n\t437 tires\n", "438 toy\n\t439 walls\n\t440 flags\n\t441 boxes\n\t442 sofa\n\t443 tomatoes\n\t444 bags\n\t445 sandals\n\t446 onions\n\t447 sandal\n", "448 oranges\n\t449 paws\n\t450 pitcher\n\t451 houses\n\t452 baseball bat\n\t453 moss\n\t454 duck\n\t455 apron\n\t456 airport\n\t457 light bulb\n", "458 drawers\n\t459 toothbrush\n\t460 shelves\n\t461 potato\n\t462 light fixture\n\t463 umbrellas\n\t464 drink\n\t465 heart\n\t466 fence post\n\t467 egg\n", "468 hose\n\t469 power line\n\t470 fireplace\n\t471 icing\n\t472 nightstand\n\t473 vehicles\n\t474 magnet\n\t475 beer\n\t476 hook\n\t477 comforter\n", "478 lake\n\t479 bookshelf\n\t480 fries\n\t481 peppers\n\t482 coffee table\n\t483 sweatshirt \n\t484 shower\n\t485 jet\n\t486 water bottle\n\t487 cows\n", "488 entrance\n\t489 driver\n\t490 towels\n\t491 soap\n\t492 sail\n\t493 crate\n\t494 utensil\n\t495 salad\n\t496 kites\n\t497 paddle\n", "498 mound\n\t499 tree branch\n\t\"\"\"\n\t\"\"\" attributes\n\t0 white\n\t1 black\n\t2 green\n\t3 blue\n\t4 brown\n\t5 red\n", "6 gray\n\t7 large\n\t8 small\n\t9 wooden\n\t10 yellow\n\t11 tall\n\t12 metal\n\t13 orange\n\t14 long\n\t15 dark\n", "16 silver\n\t17 pink\n\t18 standing\n\t19 clear\n\t20 round\n\t21 glass\n\t22 open\n\t23 sitting\n\t24 short\n\t25 parked\n", "26 plastic\n\t27 walking\n\t28 brick\n\t29 tan\n\t30 purple\n\t31 striped\n\t32 colorful\n\t33 cloudy\n\t34 hanging\n\t35 concrete\n", "36 blond\n\t37 bare\n\t38 empty\n\t39 young\n\t40 old\n\t41 closed\n\t42 baseball\n\t43 happy\n\t44 bright\n\t45 wet\n", "46 gold\n\t47 stone\n\t48 smiling\n\t49 light\n\t50 dirty\n\t51 flying\n\t52 shiny\n\t53 plaid\n\t54 on\n\t55 thin\n", "56 square\n\t57 tennis\n\t58 little\n\t59 sliced\n\t60 leafy\n\t61 playing\n\t62 thick\n\t63 beige\n\t64 steel\n\t65 calm\n", "66 rectangular\n\t67 dry\n\t68 tiled\n\t69 leather\n\t70 eating\n\t71 painted\n\t72 ceramic\n\t73 pointy\n\t74 lying\n\t75 surfing\n", "76 snowy\n\t77 paved\n\t78 clean\n\t79 fluffy\n\t80 electric\n\t81 cooked\n\t82 grassy\n\t83 stacked\n\t84 full\n\t85 covered\n", "86 paper\n\t87 framed\n\t88 lit\n\t89 blurry\n\t90 grazing\n\t91 flat\n\t92 leafless\n\t93 skiing\n\t94 curved\n\t95 light brown\n", "96 beautiful\n\t97 decorative\n\t98 up\n\t99 folded\n\t100 sandy\n\t101 chain-link\n\t102 arched\n\t103 overcast\n\t104 cut\n\t105 wide\n", "106 running\n\t107 waiting\n\t108 ripe\n\t109 long sleeved\n\t110 furry\n\t111 rusty\n\t112 short sleeved\n\t113 down\n\t114 light blue\n\t115 cloudless\n", "116 dark brown\n\t117 high\n\t118 hazy\n\t119 fresh\n\t120 chocolate\n\t121 cream colored\n\t122 baby\n\t123 worn\n\t124 bent\n\t125 light colored\n", "126 rocky\n\t127 skinny\n\t128 curly\n\t129 patterned\n\t130 driving\n\t131 jumping\n\t132 maroon\n\t133 riding\n\t134 raised\n\t135 lush\n", "136 dark blue\n\t137 off\n\t138 cardboard\n\t139 reflective\n\t140 bald\n\t141 iron\n\t142 floral\n\t143 black and white\n\t144 melted\n\t145 piled\n", "146 skateboarding\n\t147 rubber\n\t148 talking\n\t149 chrome\n\t150 wire\n\t151 puffy\n\t152 broken\n\t153 smooth\n\t154 low\n\t155 evergreen\n", "156 narrow\n\t157 denim\n\t158 grouped\n\t159 wicker\n\t160 straight\n\t161 triangular\n\t162 sunny\n\t163 dried\n\t164 bushy\n\t165 resting\n", "166 sleeping\n\t167 wrinkled\n\t168 adult\n\t169 dark colored\n\t170 hairy\n\t171 khaki\n\t172 stuffed\n\t173 wavy\n\t174 nike\n\t175 chopped\n", "176 curled\n\t177 shirtless\n\t178 splashing\n\t179 posing\n\t180 upside down\n\t181 ski\n\t182 water\n\t183 pointing\n\t184 double decker\n\t185 glazed\n", "186 male\n\t187 marble\n\t188 fried\n\t189 rock\n\t190 ornate\n\t191 wild\n\t192 shining\n\t193 tinted\n\t194 asphalt\n\t195 filled\n", "196 floating\n\t197 burnt\n\t198 crossed\n\t199 fuzzy\n\t200 outdoors\n\t201 overhead\n\t202 potted\n\t203 muddy\n\t204 pale\n\t205 decorated\n", "206 swinging\n\t207 asian\n\t208 sharp\n\t209 floppy\n\t210 outstretched\n\t211 rough\n\t212 drinking\n\t213 displayed\n\t214 lined\n\t215 having meeting\n", "216 reflected\n\t217 delicious\n\t218 barefoot\n\t219 plain\n\t220 healthy\n\t221 printed\n\t222 frosted\n\t223 crouching\n\t224 written\n\t225 illuminated\n", "226 aluminum\n\t227 digital\n\t228 skating\n\t229 trimmed\n\t230 patchy\n\t231 bending\n\t232 soft\n\t233 toasted\n\t234 neon\n\t235 choppy\n", "236 fake\n\t237 wispy\n\t238 toy\n\t239 knit\n\t240 uncooked\n\t241 vertical\n\t242 tied\n\t243 female\n\t244 straw\n\t245 grilled\n", "246 rolled\n\t247 rounded\n\t248 wrapped\n\t249 attached\n\t250 messy\n\t251 bathroom\n\t252 swimming\n\t253 deep\n\t254 pretty\n\t255 sleeveless\n", "256 granite\n\t257 rainbow colored\n\t258 fallen\n\t259 modern\n\t260 kneeling\n\t261 kitchen\n\t262 turned\n\t263 used\n\t264 murky\n\t265 busy\n", "266 snowboarding\n\t267 still\n\t268 soccer\n\t269 mounted\n\t270 antique\n\t271 street\n\t272 watching\n\t273 slanted\n\t274 faded\n\t275 glowing\n", "276 teal\n\t277 gravel\n\t278 balding\n\t279 outdoor\n\t280 heavy\n\t281 cracked\n\t282 snow\n\t283 baked\n\t284 fancy\n\t285 transparent\n", "286 roman\n\t287 vintage\n\t288 old fashioned\n\t289 cooking\n\t290 horizontal\n\t291 crouched\n\t292 shredded\n\t293 computer\n\t294 docked\n\t295 navy\n", "296 octagonal\n\t297 shallow\n\t298 sparse\n\t299 hard\n\t300 cotton\n\t301 fat\n\t302 waving\n\t303 carpeted\n\t304 electronic\n\t305 protective\n", "306 foggy\n\t307 rippled\n\t308 cloth\n\t309 squatting\n\t310 shaggy\n\t311 scattered\n\t312 new\n\t313 barren\n\t314 stained\n\t315 wireless\n", "316 designed\n\t317 reading\n\t318 overgrown\n\t319 apple\n\t320 looking down\n\t321 sliding\n\t322 ocean\n\t323 plush\n\t324 tilted\n\t325 shaded\n", "326 dusty\n\t327 flowered\n\t328 crumpled\n\t329 dotted\n\t330 mesh\n\t331 collared\n\t332 traffic\n\t333 woven\n\t334 athletic\n\t335 lighted\n", "336 power\n\t337 clay\n\t338 made\n\t339 crispy\n\t340 elderly\n\t341 bunched\n\t342 warm\n\t343 cluttered\n\t344 brunette\n\t345 textured\n", "346 spread\n\t347 brass\n\t348 chinese\n\t349 styrofoam\n\t350 elevated\n\t351 palm\n\t352 blowing\n\t353 copper\n\t354 foamy\n\t355 unripe\n", "356 cheese\n\t357 sheer\n\t358 padded\n\t359 half full\n\t360 shaped\n\t361 laughing\n\t362 winter\n\t363 manicured\n\t364 indoors\n\t365 speckled\n", "366 batting\n\t367 loose\n\t368 caucasian\n\t369 peeling\n\t370 diced\n\t371 dense\n\t372 adidas\n\t373 military\n\t374 tin\n\t375 halved\n", "376 jeans\n\t377 wired\n\t378 mixed\n\t379 bronze\n\t380 alert\n\t381 neat\n\t382 public\n\t383 drawn\n\t384 cobblestone\n\t385 massive\n", "386 real\n\t387 peeled\n\t388 assorted\n\t389 sunlit\n\t390 chipped\n\t391 blooming\n\t392 american\n\t393 perched\n\t394 shaved\n\t395 spiky\n", "396 wool\n\t397 bamboo\n\t398 gloved\n\t399 spraying\n\t400 commercial\n\t401 steep\n\t402 braided\n\t403 steamed\n\t404 tight\n\t405 partly cloudy\n", "406 unpeeled\n\t407 carved\n\t408 damaged\n\t409 edged\n\t410 directional\n\t411 strong\n\t412 pulled back\n\t413 wrinkly\n\t414 muscular\n\t415 tabby\n", "416 mowed\n\t417 disposable\n\t418 portable\n\t419 shut\n\t420 looking up\n\t421 sad\n\t422 packed\n\t423 rippling\n\t424 vanilla\n\t425 eaten\n", "426 barbed\n\t427 safety\n\t428 shingled\n\t429 roasted\n\t430 juicy\n\t431 pine\n\t432 shadowed\n\t433 diamond\n\t434 crystal\n\t435 angled\n", "436 weathered\n\t437 homemade\n\t438 seasoned\n\t439 paneled\n\t440 sloped\n\t441 sweet\n\t442 miniature\n\t443 domed\n\t444 pizza\n\t445 torn\n", "446 crowded\n\t447 tail\n\t448 beaded\n\t449 suspended\n\t450 cushioned\n\t451 tangled\n\t452 stormy\n\t453 blank\n\t454 tasty\n\t455 hitting\n", "456 deciduous\n\t457 frozen\n\t458 license\n\t459 fenced\n\t460 sprinkled\n\t461 feathered\n\t462 dull\n\t463 tropical\n\t464 twisted\n\t465 crooked\n", "466 wine\n\t467 browned\n\t468 cylindrical\n\t469 greasy\n\t470 abandoned\n\t471 capital\n\t472 toilet\n\t473 unlit\n\t474 iced\n\t475 upholstered\n", "476 gloomy\n\t477 professional\n\t478 creamy\n\t479 analog\n\t480 jagged\n\t481 oblong\n\t482 ruffled\n\t483 connected\n\t484 ivory\n\t485 christmas\n", "486 upper\n\t487 coffee\n\t488 lace\n\t489 ridged\n\t490 comfortable\n\t491 foreign\n\t492 knotted\n\t493 french\n\t494 support\n\t495 performing trick\n", "496 artificial\n\t497 spiral\n\t498 crumbled\n\t499 dangling\n\t500 cordless\n\t501 wedding\n\t502 fire\n\t503 rustic\n\t504 trash\n\t505 uneven\n", "506 grated\n\t507 curvy\n\t508 telephone\n\t509 unoccupied\n\t510 curious\n\t511 rotten\n\t512 oriental\n\t513 folding\n\t514 crusty\n\t515 rimmed\n", "516 birthday\n\t517 polished\n\t518 funny\n\t519 crashing\n\t520 powdered\n\t521 lower\n\t522 winding\n\t523 calico\n\t524 banana\n\t525 angry\n", "526 inflated\n\t527 scrambled\n\t528 wii\n\t529 vast\n\t530 sleepy\n\t531 rainy\n\t532 groomed\n\t533 glossy\n\t534 wooded\n\t535 sturdy\n", "536 vibrant\n\t537 misty\n\t538 beer\n\t539 office\n\t540 melting\n\t541 silk\n\t542 discolored\n\t543 corded\n\t544 formal\n\t545 vacant\n", "546 oversized\n\t547 vinyl\n\t548 spinning\n\t549 simple\n\t550 translucent\n\t551 mature\n\t552 tomato\n\t553 crisp\n\t554 boiled\n\t555 recessed\n", "556 gas\n\t557 handmade\n\t558 clumped\n\t559 railroad\n\t560 rugged\n\t561 sculpted\n\t562 burning\n\t563 patched\n\t564 industrial\n\t565 chubby\n", "566 ugly\n\t567 garbage\n\t568 powerful\n\t569 elongated\n\t570 pepper\n\t571 abundant\n\t572 opaque\n\t573 ornamental\n\t574 fluorescent\n\t575 soda\n", "576 packaged\n\t577 inflatable\n\t578 unpaved\n\t579 soap\n\t580 strawberry\n\t581 park\n\t582 exterior\n\t583 intricate\n\t584 sealed\n\t585 quilted\n", "586 hollow\n\t587 breakable\n\t588 pastel\n\t589 urban\n\t590 wrist\n\t591 breaking\n\t592 regular\n\t593 forested\n\t594 abstract\n\t595 unhappy\n", "596 incomplete\n\t597 fine\n\t598 chalk\n\t599 coarse\n\t600 polo\n\t601 unhealthy\n\t602 irregular\n\t603 polar\n\t604 uncomfortable\n\t605 typical\n", "606 complete\n\t607 scarce\n\t608 immature\n\t\"\"\"\n\t\"\"\" RELATIONS\n\t0 right\n\t1 left\n\t2 on\n\t3 wearing\n\t4 near\n", "5 in\n\t6 of\n\t7 behind\n\t8 in front of\n\t9 under\n\t10 holding\n\t11 with\n\t12 by\n\t13 on the side of\n\t14 watching\n", "15 inside\n\t16 carrying\n\t17 eating\n\t18 at\n\t19 riding\n\t20 playing\n\t21 covered by\n\t22 covering\n\t23 touching\n\t24 on the front of\n", "25 full of\n\t26 riding on\n\t27 using\n\t28 covered in\n\t29 sitting at\n\t30 playing with\n\t31 crossing\n\t32 on the back of\n\t33 surrounded by\n\t34 reflected in\n", "35 covered with\n\t36 flying\n\t37 pulled by\n\t38 contain\n\t39 standing by\n\t40 driving on\n\t41 surrounding\n\t42 walking down\n\t43 printed on\n\t44 attached to\n", "45 talking on\n\t46 facing\n\t47 driving\n\t48 worn on\n\t49 floating in\n\t50 grazing on\n\t51 on the bottom of\n\t52 standing in front of\n\t53 topped with\n\t54 playing in\n", "55 walking with\n\t56 swimming in\n\t57 driving down\n\t58 pushed by\n\t59 playing on\n\t60 close to\n\t61 waiting for\n\t62 between\n\t63 running on\n\t64 tied to\n", "65 on the edge of\n\t66 holding onto\n\t67 talking to\n\t68 eating from\n\t69 perched on\n\t70 parked by\n\t71 reaching for\n\t72 connected to\n\t73 grazing in\n\t74 floating on\n", "75 wrapped around\n\t76 skiing on\n\t77 stacked on\n\t78 parked at\n\t79 standing at\n\t80 walking across\n\t81 plugged into\n\t82 stuck on\n\t83 stuck in\n\t84 drinking from\n", "85 seen through\n\t86 sitting by\n\t87 sitting in front of\n\t88 looking out\n\t89 parked in front of\n\t90 petting\n\t91 wrapped in\n\t92 selling\n\t93 parked along\n\t94 coming from\n", "95 lying inside\n\t96 sitting inside\n\t97 walking by\n\t98 sitting with\n\t99 making\n\t100 walking through\n\t101 hung on\n\t102 walking along\n\t103 going down\n\t104 leaving\n", "105 running in\n\t106 flying through\n\t107 mounted to\n\t108 on the other side of\n\t109 licking\n\t110 sniffing\n\t111 followed by\n\t112 following\n\t113 riding in\n\t114 biting\n", "115 parked alongside\n\t116 chasing\n\t117 leading\n\t118 hanging off\n\t119 helping\n\t120 coming out of\n\t121 hanging out of\n\t122 staring at\n\t123 walking toward\n\t124 served on\n", "125 hugging\n\t126 entering\n\t127 skiing in\n\t128 looking in\n\t129 draped over\n\t130 tied around\n\t131 exiting\n\t132 looking down at\n\t133 looking into\n\t134 drawn on\n", "135 balancing on\n\t136 jumping over\n\t137 posing with\n\t138 reflecting in\n\t139 eating at\n\t140 walking up\n\t141 sewn on\n\t142 reflected on\n\t143 about to hit\n\t144 getting on\n", "145 observing\n\t146 approaching\n\t147 traveling on\n\t148 walking towards\n\t149 wading in\n\t150 growing by\n\t151 displayed on\n\t152 growing along\n\t153 mixed with\n\t154 grabbing\n", "155 jumping on\n\t156 scattered on\n\t157 climbing\n\t158 pointing at\n\t159 opening\n\t160 taller than\n\t161 going into\n\t162 decorated by\n\t163 decorating\n\t164 preparing\n", "165 coming down\n\t166 tossing\n\t167 eating in\n\t168 growing from\n\t169 chewing\n\t170 washing\n\t171 herding\n\t172 skiing down\n\t173 looking through\n\t174 picking up\n", "175 trying to catch\n\t176 standing against\n\t177 looking over\n\t178 typing on\n\t179 piled on\n\t180 tying\n\t181 shining through\n\t182 smoking\n\t183 cleaning\n\t184 walking to\n", "185 smiling at\n\t186 guiding\n\t187 dragging\n\t188 chained to\n\t189 going through\n\t190 enclosing\n\t191 adjusting\n\t192 running through\n\t193 skating on\n\t194 photographing\n", "195 smelling\n\t196 kissing\n\t197 falling off\n\t198 decorated with\n\t199 walking into\n\t200 worn around\n\t201 walking past\n\t202 blowing out\n\t203 towing\n\t204 jumping off\n", "205 sprinkled on\n\t206 moving\n\t207 running across\n\t208 hidden by\n\t209 traveling down\n\t210 splashing\n\t211 hang from\n\t212 looking toward\n\t213 kept in\n\t214 cooked in\n", "215 displayed in\n\t216 sitting atop\n\t217 brushing\n\t218 buying\n\t219 in between\n\t220 larger than\n\t221 smaller than\n\t222 pouring\n\t223 playing at\n\t224 longer than\n", "225 higher than\n\t226 jumping in\n\t227 shorter than\n\t228 bigger than\n\t\"\"\""]}
{"filename": "src/experiments/vqa/network_nn.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tclass Net_nn(nn.Module):\n\t    def __init__(self):\n\t        super(Net_nn, self).__init__()\n\t        self.encoder = nn.Sequential(\n\t            nn.Conv2d(1, 6, 5),  # 6 is the output chanel size; 5 is the kernal size; 1 (chanel) 28 28 -> 6 24 24\n\t            nn.MaxPool2d(2, 2),  # kernal size 2; stride size 2; 6 24 24 -> 6 12 12\n\t            nn.ReLU(True),       # inplace=True means that it will modify the input directly thus save memory\n\t            nn.Conv2d(6, 16, 5), # 6 12 12 -> 16 8 8\n", "            nn.MaxPool2d(2, 2),  # 16 8 8 -> 16 4 4\n\t            nn.ReLU(True) \n\t        )\n\t        self.classifier =  nn.Sequential(\n\t            nn.Linear(16 * 4 * 4, 120),\n\t            nn.ReLU(),\n\t            nn.Linear(120, 84),\n\t            nn.ReLU(),\n\t            nn.Linear(84, 10),\n\t            nn.Softmax(1)\n", "        )\n\t    def forward(self, x, marg_idx=None, type=1):\n\t        assert type == 1, \"only posterior computations are available for this network\"\n\t        # If the list of the pixel numbers to be marginalised is given,\n\t        # then genarate a marginalisation mask from it and apply to the\n\t        # tensor 'x'\n\t        if marg_idx:\n\t            batch_size = x.shape[0]\n\t            with torch.no_grad():\n\t                marg_mask = torch.ones_like(x, device=x.device).reshape(batch_size, 1, -1)\n", "                marg_mask[:, :, marg_idx] = 0\n\t                marg_mask = marg_mask.reshape_as(x)\n\t                marg_mask.requires_grad_(False)\n\t            x = torch.einsum('ijkl,ijkl->ijkl', x, marg_mask)\n\t        x = self.encoder(x)\n\t        x = x.view(-1, 16 * 4 * 4)\n\t        x = self.classifier(x)\n\t        return x\n"]}
{"filename": "src/experiments/vqa/models.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n", "import torch.nn.functional as F\n\timport os\n\timport sys\n\ttorch.autograd.set_detect_anomaly(True)   \n\tsys.path.append('../../EinsumNetworks/src/')\n\tfrom einsum_wrapper import EiNet\n\tclass MLPClassifier(nn.Module):\n\t    def __init__(self, input_dim, latent_dim, output_dim, n_layers, dropout_rate, softmax):\n\t        super(MLPClassifier, self).__init__()\n\t        self.softmax = softmax\n", "        self.output_dim = output_dim\n\t        layers = []\n\t        layers.append(nn.Linear(input_dim, latent_dim))\n\t        layers.append(nn.ReLU())\n\t        layers.append(nn.BatchNorm1d(latent_dim))\n\t        #layers.append(nn.InstanceNorm1d(latent_dim))\n\t        layers.append(nn.Dropout(dropout_rate))\n\t        for _ in range(n_layers - 1):\n\t            layers.append(nn.Linear(latent_dim, latent_dim))\n\t            layers.append(nn.ReLU())\n", "            layers.append(nn.BatchNorm1d(latent_dim))\n\t            #layers.append(nn.InstanceNorm1d(latent_dim))\n\t            layers.append(nn.Dropout(dropout_rate))\n\t        layers.append(nn.Linear(latent_dim, output_dim))\n\t        self.net = nn.Sequential(*layers)\n\t    def forward(self, x, marg_idx=None, type=None):\n\t        if x.sum() == 0:\n\t            return torch.ones([x.shape[0], self.output_dim], device='cuda')\n\t        idx = x.sum(dim=1)!=0 # get idx of true objects\n\t        logits = torch.zeros(x.shape[0], self.output_dim, device='cuda')\n", "        logits[idx] = self.net(x[idx])\n\t        if self.softmax:\n\t            probs = F.softmax(logits, dim=1)\n\t        else:\n\t            probs = torch.sigmoid(logits)\n\t        return probs\n\t# FasterCNN object feature size\n\tfeature_dim = 2048 \n\tname_clf = MLPClassifier(\n\t    input_dim=feature_dim,\n", "    output_dim=500,\n\t    latent_dim=1024,\n\t    n_layers=2,\n\t    dropout_rate=0.3,\n\t    softmax=True\n\t)\n\trela_clf = MLPClassifier(\n\t    input_dim=(feature_dim+4)*2,  \n\t    output_dim=229,       \n\t    latent_dim=1024,\n", "    n_layers=1,\n\t    dropout_rate=0.5,\n\t    softmax=True\n\t)\n\tattr_clf = MLPClassifier(\n\t    input_dim=feature_dim,\n\t    output_dim=609,\n\t    latent_dim=1024,\n\t    n_layers=1,\n\t    dropout_rate=0.3,\n", "    softmax=False\n\t)\n\tname_einet = EiNet(structure = 'poon-domingos',\n\t                      pd_num_pieces = [4],\n\t                      use_em = False,\n\t                      num_var = 2048,\n\t                      class_count = 500,\n\t                      pd_width = 32,\n\t                      pd_height = 64,\n\t                      learn_prior = True)\n", "rela_einet = EiNet(structure = 'poon-domingos',\n\t                      pd_num_pieces = [4],\n\t                      use_em = False,\n\t                      num_var = 4104,\n\t                      class_count = 229,\n\t                      pd_width = 72,\n\t                      pd_height = 57,\n\t                      learn_prior = True)\n\tattr_einet = EiNet(structure = 'poon-domingos',\n\t                      pd_num_pieces = [4],\n", "                      use_em = False,\n\t                      num_var = 2048,\n\t                      class_count = 609,\n\t                      pd_width = 32,\n\t                      pd_height = 64,\n\t                      learn_prior = True)"]}
{"filename": "src/experiments/vqa/__init__.py", "chunked_list": []}
{"filename": "src/experiments/vqa/dataGen.py", "chunked_list": ["import torch\n\tfrom torch.utils.data import Dataset\n\timport numpy as np\n\timport json\n\timport pickle\n\timport re\n\timport itertools\n\tfrom trainer import ClauseNTrainer  # the class for training, testing, etc.\n\tfrom preprocess import Query  # the abstract class to transform the strings of VQA to an abstraction of a query\n\tfrom tqdm import tqdm\n", "def get_SLASH_npp(outcomes:list, tag:str) -> str:\n\t    \"\"\" Function to generate a valid NPP given a list of outcomes\n\t        Inputs:\n\t            outcomes<list>: list of strings entailing the outcomes\n\t            tag<str>: the name tag of the npp\n\t        Outputs:\n\t            npp<str>: string descibing a valid NPP\n\t    \"\"\"\n\t    if tag == \"relation\":\n\t        str_outcomes = \", \"\n", "        str_outcomes = str_outcomes.join([f'{outcome.replace(\" \", \"_\").replace(\"-\", \"_\")}' for outcome in outcomes])\n\t        npp = \"\".join([f\"npp({tag}(1,X), \", \"[\", str_outcomes , \"]) :- object(X1,X2).\"])\n\t    else: \n\t        str_outcomes = \", \"\n\t        str_outcomes = str_outcomes.join([f'{outcome.replace(\" \", \"_\").replace(\"-\", \"_\")}' for outcome in outcomes])\n\t        npp = \"\".join([f\"npp({tag}(1,X), \", \"[\", str_outcomes , \"]) :- object(X).\"])\n\t    return npp\n\tdef get_SLASH_query(query_content:str , target_objects= None,non_target_objects=None, show_filter=None, num_objects=0) -> str:\n\t    \"\"\" Given the string of SCALLOP query rewrite it to a SLASH variant.\n\t        Inputs:\n", "            query_content<str>: string entailing SCALLOP's version of a query.\n\t        Outputs:\n\t            main_rule_and_query<str>: SLASH' version of the given query\n\t    \"\"\"\n\t    #adjust VQAR scallop format to SLASH by deleting the query(target(O)). entry.\n\t    tmp = query_content.split(\"\\n\")\n\t    main_rule = \"\\n\".join([t for t in tmp[:-1] if t!=''])\n\t    #store attributes and relations from the query here\n\t    attr_rel_filter = \"\"\n\t    #add target constraints for training.\n", "    query = \":- target(X), target(Y), X != Y.\\n\" #display only one target per solution\n\t    if target_objects is not None:\n\t        #if an answer exists add them as a constraint. One of them has to be in a solution\n\t        if len(target_objects) > 0:\n\t            query += \":-\"\n\t            for target_object in target_objects:\n\t                query += \" not target({});\".format(int(target_object))\n\t            query= query[:-1] +\".\\n\"\n\t        #if non answer objects exist add them as a constraint.\n\t        #Solution is UNSAT if it contains one of these answers.\n", "        for non_target_object in non_target_objects: \n\t            query += \":- target({}).\".format(int(non_target_object))\n\t        query += \"\\n\"\n\t        #show statement to disable all printing\n\t        show_statements = \"#show. \\n\"\n\t    #add show statements for testing\n\t    else: \n\t        show_statements = \"#show target/1.\\n\"\n\t    #add show statements for training and testing\n\t    #to build showstatements we use a list of tuples with target head and body as a tuple\n", "    # example: ('O2', ['attr(dirty, O2)', 'relation(left, O1, O2)',...]) as an intermediate \n\t    # we call this intermediate representation show filter and transform it afterwards to the show statements\n\t    #if we have two different rules (and/or)\n\t    if len(show_filter)== 2:\n\t        if show_filter[0][2] != show_filter[1][2]:\n\t            show_filter[0][1].extend(show_filter[1][1])\n\t            show_filter = [show_filter[0]]\n\t    #iterate over all show filters\n\t    for filter in show_filter: #filter[0] is the target variable\n\t        #iterate over the body\n", "        for var_descr in filter[1]:\n\t            #filter attributes and relation predicates\n\t            matches_attr_rel = re.findall(r'(relation|attr)\\(([ a-zA-Z0-9]*|[ a-zA-Z0-9]*,[ a-zA-Z0-9]*),([-_ a-z]*)\\)',var_descr )\n\t            #if we found a relation or attribute iterate over them\n\t            for match in matches_attr_rel:\n\t                #every show statement/filter corresponds to one subgoal of the target query.\n\t                #We want to have them seperate so we use a choice rule to show subgoals seperatly\n\t                attr_rel_filter = attr_rel_filter + \"///\" +str(match[2]).replace(\" \",\"\")#extend the attribute/relation filter\n\t                #for relations we only want one relation combination to fulfills the target requirement\n\t                #we add a count constraint for this\n", "                if match[0] == 'relation' :#and target_objects is not None:\n\t                    query += \":- #count {{ {}(0,1,{},{}): target({}), {}}} > 1.\\n\".format(match[0],match[1],match[2],filter[0], \", \".join(filter[1]))\n\t                #now build the ASP readable show statement\n\t                show_statements += \"#show {}(0,1,{},{}): target({}), {}.\\n\".format(match[0],match[1],match[2],filter[0], \", \".join(filter[1]))\n\t            #filter names and add the showstatements\n\t            matches_name = re.findall(r'name\\(([ a-zA-Z0-9]*),([ -_a-zA-Z0-9]*)\\)',var_descr)\n\t            for match in matches_name:\n\t                show_statements += \"#show name(0,1,{}, X) : target({}), name(0,1,{},X), {}, {}.\\n\".format(match[0],  filter[0], match[0], \", \".join(filter[1]), \"s(0)\")\n\t    #correct wrong KG translations\n\t    def correct_kg_intput(str):\n", "        return str.replace(\"vitamin_B\", \"vitamin_b\").replace(\"even-toed\",\"even_toed\").replace(\"sub-event\",\"sub_event\").replace(\"odd-toed\",\"odd_toed\").replace('t-shirt','t_shirt').replace(\"chain-link\",\"chain_link\").replace(\"-ungulate\",\"_ungulate\")\n\t    query = correct_kg_intput(query)\n\t    #join all together in one query\n\t    query_rule_and_show_statements = \"\".join([\"%\\t Target rule: \\n\", main_rule,\"\\n\",query,  \"\\n%\\tShow statement:\\n\", show_statements,\"\\n\"] )\n\t    query_rule_and_show_statements = correct_kg_intput(query_rule_and_show_statements)\n\t    return query_rule_and_show_statements, attr_rel_filter\n\tmeta_f = \"dataset/gqa_info.json\"  # Meta file - Graph Question Answering information\n\twith open (meta_f, 'r') as meta_f:\n\t    meta_info = json.load(meta_f)\n\t#load the set of names, attributes and relations\n", "names = list(meta_info['name']['canon'].keys())\n\tattributes = list(meta_info['attr']['canon'].keys())\n\trelations = list(meta_info['rel']['canon'].keys())\n\t#names= [\"giraffe\",\"tigger\",\"lion\"] #debug\n\t#relations = [\"left\", \"right\",\"above\"] #debug\n\t#attributes = [\"gren\", \"blue\",\"striped\",\"shiny\"] #debug\n\t#NPPS\n\tname_npp = get_SLASH_npp(names, \"name\")\n\trelation_npp = get_SLASH_npp(relations, \"relation\")\n\tattribute_npp = get_SLASH_npp(attributes, \"attr\")\n", "class VQA(Dataset):\n\t    '''\n\t    VQA dataset consisting of:\n\t    objects\n\t    flag_addinf: The flag for getting dictionary with additional informations for visualisations.\n\t    '''\n\t    def __init__(self, dataset,  task_file, num_objects =None, flag_addinf=False):\n\t        self.data = list()\n\t        self.dataset = dataset\n\t        self.flag_addinf = flag_addinf\n", "        self.max_objects = 0\n\t        self.dataset_size = 0\n\t        with open (task_file, 'rb') as tf:\n\t            tasks = pickle.load(tf)\n\t        print(\"dataloading done\")\n\t        self.object_list = []\n\t        self.query_list = []\n\t        self.feature_and_bb_list = []\n\t        self.num_true_objects = []\n\t        self.targets = []\n", "        self.additional_information_list = []\n\t        self.attr_rel_filter_list = []\n\t        #if num_objects is not None:\n\t        self.num_objects = num_objects\n\t        print(\"taskfile len:{}, working with {} objects\".format(len(tasks), self.num_objects))\n\t        #some statistics\n\t        self.num_rela=0\n\t        self.num_attr=0\n\t        #hist = np.zeros(self.num_objects+1)\n\t        obj_list = np.arange(0, self.num_objects)\n", "        #CREATE and empty scene dict \n\t        empty_scene_dict = {}\n\t        #add empty objects for relations\n\t        for id1 in obj_list:\n\t            for id2 in obj_list:\n\t                key = str(id1)+\",\"+str(id2)\n\t                if key not in empty_scene_dict:\n\t                    empty_scene_dict[key] = torch.zeros([4104])\n\t        #add empty objects for names and attributes\n\t            for id in obj_list:\n", "                empty_scene_dict[str(id)] = torch.zeros([2048])\n\t        no_ans = 0\n\t        for tidx, task in enumerate(tqdm(tasks)):\n\t            # if task['image_id'] != 2337789:\n\t            #    continue\n\t            target = np.zeros(self.num_objects)\n\t            clause_n_trainer = ClauseNTrainer(train_data_loader=tasks, val_data_loader=tasks, test_data_loader=tasks)\n\t            #OBJECTS\n\t            all_oid = task['question']['input']\n\t            all_oid = [int(oid) for oid in all_oid]\n", "            len_all_oid = len(all_oid)\n\t            #only consider images with less then num_objects\n\t            if self.num_objects is not None:\n\t                if len_all_oid > self.num_objects:\n\t                    continue\n\t                else:\n\t                    self.dataset_size +=1\n\t            #map object ids to ids lower than num_objects\n\t            correct_oids = task['question']['output']\n\t            if len(correct_oids) == 0 :\n", "                no_ans +=1\n\t            len_correct_oid = len(correct_oids)\n\t            not_correct_oids = [x for x in all_oid if (x not in correct_oids)]\n\t            all_oid = correct_oids + not_correct_oids #this list contains all objects but puts the true objects first\n\t            #map object ids to the inverval [0,25]\n\t            oid_map = {}\n\t            for idx, oid in enumerate(all_oid):\n\t                oid_map[oid] = idx\n\t            #map the correct answers\n\t            correct_oids_mapped = [oid_map[correct_oid] for correct_oid in correct_oids]\n", "            not_correct_oids_mapped = [oid_map[not_correct_oid] for not_correct_oid in not_correct_oids]\n\t            #print(\"the correct answers are:\", correct_oids)\n\t            #set gt vector true for correct objects\n\t            for oid in correct_oids_mapped:\n\t                target[oid] = 1\n\t            #create the QUERY\n\t            query = task[\"question\"][\"clauses\"]\n\t            query = Query(query)\n\t            query_content = clause_n_trainer.query_manager.transformer.transform(query)\n\t            if 'rela'  in clause_n_trainer.query_manager.transformer.show_filter[0][1][0]:\n", "               self.num_rela += 1\n\t               #continue\n\t            if 'attr'  in clause_n_trainer.query_manager.transformer.show_filter[0][1][0]:\n\t               self.num_attr += 1\n\t               #continue\n\t            #if \"hand\" not in query_content and self.dataset==\"test\":\n\t            #    self.dataset_size -=1\n\t            #    continue\n\t            #if (\"controlling_flows_of_traffic\" not in query_content) and self.dataset==\"test\":\n\t            #    self.dataset_size -=1\n", "            #    continue\n\t            #collect bounding boxes and object features \n\t            #scene_dict stores all combinations while data_dict contains only the query-relevant ones\n\t            scene_dict = empty_scene_dict.copy()\n\t            # Put objects into scene dict\n\t            #feature maps for attributes and names are of size 2048, bounding boxes are of size 4 \n\t            for  oid in all_oid:\n\t                scene_dict[str(oid_map[oid])] = torch.tensor(task['object_feature'][task['object_ids'].index(oid)])\n\t            #feature maps for relations\n\t            for  oid1 in all_oid:\n", "                for  oid2 in all_oid:\n\t                    key = str(oid_map[oid1])+\",\"+str(oid_map[oid2])\n\t                    scene_dict[key] = torch.cat([torch.tensor(task['object_feature'][task['object_ids'].index(oid1)]), torch.tensor(task['scene_graph']['bboxes'][oid1]),\n\t                                                torch.tensor(task['object_feature'][task['object_ids'].index(oid2)]), torch.tensor(task['scene_graph']['bboxes'][oid2])])    \n\t            #use only objects in the query for training\n\t            if dataset == \"train\":\n\t                query_rule_and_show_statement, attr_rel_filter = get_SLASH_query(query_content, np.array(correct_oids_mapped), np.array(not_correct_oids_mapped), clause_n_trainer.query_manager.transformer.show_filter,num_objects = len_all_oid) # Translate the query to SLASH\n\t            #use all objects for evaluation\n\t            else:\n\t                query_rule_and_show_statement, attr_rel_filter= get_SLASH_query(query_content, show_filter =clause_n_trainer.query_manager.transformer.show_filter, num_objects = len_all_oid) # Translate the query to SLASH\n", "            # if 'relation' in query_rule_and_show_statement:\n\t            #    self.dataset_size -=1\n\t            #    continue\n\t            # if 'rule_1' in query_rule_and_show_statement:\n\t            #    self.dataset_size -=1\n\t            #    continue\n\t            #queries without any relations or rule work up to 25 objects\n\t            # if 'attr' in query_rule_and_show_statement:\n\t            #    self.dataset_size -=1\n\t            #    continue\n", "            self.attr_rel_filter_list.append(attr_rel_filter)\n\t            #we add the number of true objects for each image\n\t            #later we can then remove NPPs which are not true objects\n\t            self.num_true_objects.append(np.array(len_all_oid))\n\t            #if 'flag_addinf' is True, generate \"additional_information_dict\"\n\t            #and fill it with entries from \"task\" dictionary.\n\t            if flag_addinf:\n\t                additional_information_dict = {}\n\t                #add image_id\n\t                additional_information_dict['image_id'] = task['image_id']\n", "                #add image's URL\n\t                additional_information_dict['image_URL'] = task['url']\n\t                #add original object ids\n\t                additional_information_dict['original_object_ids'] = task['question']['input']\n\t                #add original answer object's id\n\t                additional_information_dict['original_answer_object_id'] = task['question']['output']\n\t                #add the mapping from obj_ids to obj*\n\t                additional_information_dict['mapping'] = oid_map\n\t                #add the answers in form of obj*\n\t                additional_information_dict['correct_oids_mapped'] = correct_oids_mapped\n", "                #add the rest of obj*, which are not answers to the question\n\t                additional_information_dict['not_correct_oids_mapped'] = not_correct_oids_mapped\n\t                #add bounding boxes of every object in the image\n\t                additional_information_dict['bboxes'] = task['scene_graph']['bboxes']\n\t                #add query in the FOL format\n\t                additional_information_dict['query'] = query_rule_and_show_statement\n\t                additional_information_dict['scene_graph'] = task['scene_graph']\n\t                self.additional_information_list.append(additional_information_dict)\n\t                # print(self.additional_information_list[0]['image_id'])\n\t            # add data, objects, queries and query rules to the dataset\n", "            self.feature_and_bb_list.append(scene_dict)\n\t            #self.object_list.append(object_string)\n\t            self.query_list.append(query_rule_and_show_statement)\n\t            self.targets.append(target)\n\t            # if self.dataset_size >= 100 and self.dataset == \"train\":\n\t            #   break\n\t            # elif self.dataset_size >= 100 and self.dataset == \"test\":\n\t            #   break\n\t            # elif self.dataset_size >= 100 and self.dataset == \"val\":\n\t            #   break\n", "        # temp = np.cumsum(hist)\n\t        # for hidx, h in enumerate(hist):\n\t        #     if h != 0:\n\t        #         print(hidx, h, temp[hidx]/tasks.__len__())\n\t        print(len(self.query_list),len(self.targets))\n\t        print(\"dataset size \", self.dataset,\":\",self.dataset_size)\n\t        print(\"queries with no answer\", no_ans)\n\t        print(\"Num relations\", self.num_rela)\n\t        print(\"Num attr\", self.num_attr)\n\t    def __getitem__(self, index):\n", "        if self.flag_addinf:\n\t            return self.feature_and_bb_list[index], self.query_list[index], self.num_true_objects[index], self.targets[index] , self.additional_information_list[index]\n\t        else:\n\t            return self.feature_and_bb_list[index], self.query_list[index]+self.attr_rel_filter_list[index] , self.num_true_objects[index], self.targets[index]\n\t    def __len__(self):\n\t        return self.dataset_size\n\t        #return len(self.feature_and_bb_list)"]}
{"filename": "src/experiments/vqa/preprocess.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\t# Prog = Conj | Logic\n\t# Logic = biOp Prog Prog\n\t# Conj = and Des Conj | Des\n", "# Des = rela Relaname O1 O2 | attr Attr O\n\tclass Variable():\n\t    def __init__(self, id):\n\t        self.var_id = f\"O{id}\"\n\t        self.name_id = f\"N{id}\"\n\t        self.name = []\n\t        self.hypernyms = []\n\t        self.attrs = []\n\t        self.kgs = []\n\t        # The relations where this object functions as a subject\n", "        self.sub_relas = []\n\t        self.obj_relas = []\n\t    def has_rela(self):\n\t        if len(self.sub_relas) == 0 and len(self.obj_relas) == 0:\n\t            return False\n\t        return True\n\t    def get_name_id(self):\n\t        # if (not len(self.hypernyms) == 0) and len(self.name) == 0:\n\t        #     return True, self.name_id\n\t        # if (not len(self.kgs) == 0) and len(self.name) == 0:\n", "        #     return True, self.name_id\n\t        return False, self.name_id\n\t    def set_name(self, name):\n\t        if name in self.name:\n\t            return\n\t        self.name.append(name)\n\t    def set_kg(self, kg):\n\t        if kg not in self.kgs:\n\t            self.kgs.append(kg)\n\t    def set_hypernym(self, hypernym):\n", "        if hypernym not in self.hypernyms:\n\t            self.hypernyms.append(hypernym)\n\t    def set_attr(self, attr):\n\t        if attr not in self.attrs:\n\t            self.attrs.append(attr)\n\t    def set_obj_relas(self, obj_rela):\n\t        if obj_rela not in self.obj_relas:\n\t            self.obj_relas.append(obj_rela)\n\t    def set_sub_relas(self, sub_rela):\n\t        if sub_rela not in self.sub_relas:\n", "            self.sub_relas.append(sub_rela)\n\t    def get_neighbor(self):\n\t        neighbors = []\n\t        for rela in self.sub_relas:\n\t            neighbors.append(rela.obj)\n\t        for rela in self.obj_relas:\n\t            neighbors.append(rela.sub)\n\t        return neighbors\n\t    def update(self, other):\n\t        self.hypernyms = list(set(self.name + other.name))\n", "        self.hypernyms = list(set(self.hypernyms + other.hypernyms))\n\t        self.attrs = list(set(self.attrs + other.attrs))\n\t        self.kgs = list(set(self.kgs + other.kgs))\n\t    def to_datalog(self, with_name=False, with_rela=True):\n\t        name_query = []\n\t        if (len(self.name) == 0) and with_name:\n\t            name_query.append(\"name({}, {})\".format(self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), self.var_id.replace(\" \",\"_\").replace(\".\",\"_\")))\n\t        if (not len(self.name) == 0):\n\t            for n in self.name:\n\t                #name_query.append(f\"name(\\\"{n}\\\", {self.var_id})\")\n", "                n = n.replace(\" \",\"_\").replace(\".\",\"_\")\n\t                #name_query.append(f\"name(0,1,{self.var_id},{n})\")\n\t                name_query.append(f\"name({self.var_id},{n})\")\n\t        #attr_query = [ f\"attr(\\\"{attr}\\\", {self.var_id})\" for attr in self.attrs]\n\t        #attr_query = [ \"attr(0,1,{}, {})\".format(self.var_id.replace(\" \",\"_\"),attr.replace(\" \",\"_\")) for attr in self.attrs]\n\t        attr_query = [ \"attr({}, {})\".format(self.var_id.replace(\" \",\"_\"),attr.replace(\" \",\"_\")) for attr in self.attrs]\n\t        #hypernym_query = [f\"name(\\\"{hypernym}\\\", {self.var_id})\" for hypernym in self.hypernyms]\n\t        #hypernym_query = [\"name(0,1,{}, {})\".format(self.var_id.replace(\" \",\"_\").replace(\".\",\"_\"),hypernym.replace(\" \",\"_\").replace(\".\",\"_\")) for hypernym in self.hypernyms]\n\t        hypernym_query = [\"name({}, {})\".format(self.var_id.replace(\" \",\"_\").replace(\".\",\"_\"),hypernym.replace(\" \",\"_\").replace(\".\",\"_\")) for hypernym in self.hypernyms]\n\t        kg_query = []\n", "        for kg in self.kgs:\n\t            restriction = list(filter(lambda x: not x == 'BLANK' and not x == '', kg))\n\t            assert (len(restriction) == 2)\n\t            rel = restriction[0].replace(\" \",\"_\")\n\t            usage = restriction[1].replace(\" \",\"_\")\n\t            #kg_query += [f\"name({self.name_id}, {self.var_id}), oa_rel({rel}, {self.name_id}, {usage})\"]\n\t            #kg_query +=     [\"name({}, {}), oa_rel({}, {}, {})\".format(self.name_id.replace(\" \",\"_\").replace(\".\",\"_\") ,self.var_id.replace(\" \",\"_\").replace(\".\",\"_\") ,rel.replace(\" \",\"_\").replace(\".\",\"_\"), self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), usage.replace(\" \",\"_\").replace(\".\",\"_\"))]\n\t            #kg_query +=     [\"name(0,1,{}, {}), oa_rel({}, {}, {})\".format(self.var_id.replace(\" \",\"_\").replace(\".\",\"_\") ,self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), rel.replace(\" \",\"_\").replace(\".\",\"_\"), self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), usage.replace(\" \",\"_\").replace(\".\",\"_\"))]\n\t            kg_query +=     [\"name({}, {}), oa_rel({}, {}, {})\".format(self.var_id.replace(\" \",\"_\").replace(\".\",\"_\") ,self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), rel.replace(\" \",\"_\").replace(\".\",\"_\"), self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), usage.replace(\" \",\"_\").replace(\".\",\"_\"))]\n\t        if with_rela:\n", "            rela_query = [rela.to_datalog() for rela in self.sub_relas]\n\t        else:\n\t            rela_query = []\n\t        program = name_query + attr_query + hypernym_query + kg_query + rela_query\n\t        #print(program)\n\t        return program\n\tclass Relation():\n\t    def __init__(self, rela_name, sub, obj):\n\t        self.rela_name = rela_name\n\t        self.sub = sub\n", "        self.obj = obj\n\t        self.sub.set_sub_relas(self)\n\t        self.obj.set_obj_relas(self)\n\t    def substitute(self, v1, v2):\n\t        if self.sub == v1:\n\t            self.sub = v2\n\t        if self.obj == v1:\n\t            self.obj = v2\n\t    def to_datalog(self):\n\t        #rela_query = f\"relation(\\\"{self.rela_name}\\\", {self.sub.var_id},  {self.obj.var_id})\"\n", "        #rela_query = \"relation(0,1,{}, {},  {})\".format( self.sub.var_id.replace(\" \",\"_\"), self.obj.var_id.replace(\" \",\"_\"),self.rela_name.replace(\" \",\"_\"))\n\t        rela_query = \"relation({}, {},  {})\".format( self.sub.var_id.replace(\" \",\"_\"), self.obj.var_id.replace(\" \",\"_\"),self.rela_name.replace(\" \",\"_\"))\n\t        return rela_query\n\t# This is for binary operations on variables\n\tclass BiOp():\n\t    def __init__(self, op_name, v1, v2):\n\t        self.op_name = op_name\n\t        self.v1 = v1\n\t        self.v2 = v2\n\t    def to_datalog(self):\n", "        raise NotImplementedError\n\tclass Or(BiOp):\n\t    def __init__(self, v1, v2):\n\t        super().__init__('or', v1, v2)\n\t    def to_datalog(self):\n\t        pass\n\tclass And(BiOp):\n\t    def __init__(self, v1, v2):\n\t        super().__init__('and', v1, v2)\n\t    def to_datalog(self):\n", "        pass\n\tclass Query():\n\t    def __init__(self, query):\n\t        self.vars = []\n\t        self.relations = []\n\t        self.operations = []\n\t        self.stack = []\n\t        self.preprocess(query)\n\t    def get_target(self):\n\t        pass\n", "    def get_new_var(self):\n\t        self.vars.append(Variable(len(self.vars)))\n\t    def preprocess(self, query):\n\t        # for clause in question[\"program\"]:\n\t        for clause in query:\n\t            if clause['function'] == \"Initial\":\n\t                if not len(self.vars) == 0:\n\t                    self.stack.append(self.vars[-1])\n\t                self.get_new_var()\n\t                self.root = self.vars[-1]\n", "            # logic operations\n\t            elif clause['function'] == \"And\":\n\t                v = self.stack.pop()\n\t                self.operations.append(And(v, self.vars[-1]))\n\t                self.root = self.operations[-1]\n\t            elif clause['function'] == \"Or\":\n\t                v = self.stack.pop()\n\t                self.operations.append(Or(v, self.vars[-1]))\n\t                self.root = self.operations[-1]\n\t            # find operations\n", "            elif clause['function'] == \"KG_Find\":\n\t                self.vars[-1].set_kg(clause['text_input'])\n\t            elif clause['function'] == \"Hypernym_Find\":\n\t                self.vars[-1].set_hypernym(clause['text_input'])\n\t            elif clause['function'] == \"Find_Name\":\n\t                self.vars[-1].set_name(clause['text_input'])\n\t            elif clause['function'] == \"Find_Attr\":\n\t                self.vars[-1].set_attr(clause['text_input'])\n\t            elif clause['function'] == \"Relate_Reverse\":\n\t                self.get_new_var()\n", "                self.root = self.vars[-1]\n\t                obj = self.vars[-2]\n\t                sub = self.vars[-1]\n\t                rela_name = clause['text_input']\n\t                relation = Relation(rela_name, sub, obj)\n\t                self.relations.append(relation)\n\t            elif clause['function'] == \"Relate\":\n\t                self.get_new_var()\n\t                self.root = self.vars[-1]\n\t                sub = self.vars[-2]\n", "                obj = self.vars[-1]\n\t                rela_name = clause['text_input']\n\t                relation = Relation(rela_name, sub, obj)\n\t                self.relations.append(relation)\n\t            else:\n\t                raise Exception(f\"Not handled function: {clause['function']}\")\n\t# Optimizers for optimization\n\tclass QueryOptimizer():\n\t    def __init__(self, name):\n\t        self.name = name\n", "    def optimize(self, query):\n\t        raise NotImplementedError\n\t# This only works for one and operation at the end\n\t# This is waited for update\n\t# class AndQueryOptimizer(QueryOptimizer):\n\t#     def __init__(self):\n\t#         super().__init__(\"AndQueryOptimizer\")\n\t#     # For any and operation, this can be rewritten as a single object\n\t#     def optimize(self, query):\n\t#         if len(query.operations) == 0:\n", "#             return query\n\t#         assert(len(query.operations) == 1)\n\t#         operation = query.operations[0]\n\t#         # merge every subtree into one\n\t#         if operation.name == \"and\":\n\t#             v1 = operation.v1\n\t#             v2 = operation.v2\n\t#             v1.merge(v2)\n\t#             for relation in query.relations:\n\t#                 relation.substitute(v2, v1)\n", "#             query.vars.remove(v2)\n\t#             if query.root == operation:\n\t#                 query.root = v1\n\t#         return query\n\tclass HypernymOptimizer(QueryOptimizer):\n\t    def __init__(self):\n\t        super().__init__(\"HypernymOptimizer\")\n\t    def optimize(self, query):\n\t        if (query.name is not None and not query.hypernyms == []):\n\t            query.hypernyms = []\n", "        return query\n\tclass KGOptimizer(QueryOptimizer):\n\t    def __init__(self):\n\t        super().__init__(\"HypernymOptimizer\")\n\t    def optimize(self, query):\n\t        if (query.name is not None and not query.kgs == []):\n\t            query.kgs = []\n\t        return query\n"]}
{"filename": "src/experiments/vqa/query_lib.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\timport pickle\n\timport os\n\timport subprocess\n", "from transformer import DetailTransformer, SimpleTransformer\n\tfrom preprocess import Query\n\tfrom knowledge_graph import KG, RULES\n\t# animals = [\"giraffe\", \"cat\", \"kitten\", \"dog\", \"puppy\", \"poodle\", \"bull\", \"cow\", \"cattle\", \"bison\", \"calf\", \"pig\", \"ape\", \"monkey\", \"gorilla\", \"rat\", \"squirrel\", \"hamster\", \"deer\", \"moose\", \"alpaca\", \"elephant\", \"goat\", \"sheep\", \"lamb\", \"antelope\", \"rhino\", \"hippo\",  \"zebra\", \"horse\", \"pony\", \"donkey\", \"camel\", \"panda\", \"panda bear\", \"bear\", \"polar bear\", \"seal\", \"fox\", \"raccoon\", \"tiger\", \"wolf\", \"lion\", \"leopard\", \"cheetah\", \"badger\", \"rabbit\", \"bunny\", \"beaver\", \"kangaroo\", \"dinosaur\", \"dragon\", \"fish\", \"whale\", \"dolphin\", \"crab\", \"shark\", \"octopus\", \"lobster\", \"oyster\", \"butterfly\", \"bee\", \"fly\", \"ant\", \"firefly\", \"snail\", \"spider\", \"bird\", \"penguin\", \"pigeon\", \"seagull\", \"finch\", \"robin\", \"ostrich\", \"goose\", \"owl\", \"duck\", \"hawk\", \"eagle\", \"swan\", \"chicken\", \"hen\", \"hummingbird\", \"parrot\", \"crow\", \"flamingo\", \"peacock\", \"bald eagle\", \"dove\", \"snake\", \"lizard\", \"alligator\", \"turtle\", \"frog\", \"animal\"]\n\tclass QueryManager():\n\t    def __init__(self, save_dir):\n\t        self.save_dir = save_dir\n\t        self.transformer = SimpleTransformer()\n\t    def save_file(self, file_name, content):\n\t        save_path = os.path.join (self.save_dir, file_name)\n", "        with open (save_path, \"w\") as save_file:\n\t            save_file.write(content)\n\t    def delete_file(self, file_name):\n\t        save_path = os.path.join (self.save_dir, file_name)\n\t        if os.path.exists(save_path):\n\t            os.remove(save_path)\n\t    def fact_prob_to_file(self, fact_tps, fact_probs):\n\t        scene_tps = []\n\t        (name_tps, attr_tps, rela_tps) = fact_tps\n\t        (name_probs, attr_probs, rela_probs) = fact_probs\n", "        cluster_ntp = {}\n\t        for (oid, name), prob in zip(name_tps, name_probs):\n\t            if not oid in cluster_ntp:\n\t                cluster_ntp[oid] = [(name, prob)]\n\t            else:\n\t                cluster_ntp[oid].append((name, prob))\n\t        for oid, name_list in cluster_ntp.items():\n\t            name_tps = []\n\t            for (name, prob) in name_list:\n\t                # if not name in animals[:5]:\n", "                #     continue\n\t                name_tps.append(f'{prob}::name(\"{name}\", {int(oid)})')\n\t            name_content = \";\\n\".join(name_tps) + \".\"\n\t            scene_tps.append(name_content)\n\t        for attr_tp, prob in zip(attr_tps, attr_probs):\n\t            # if not attr_tp[1] == \"tall\":\n\t            #     continue\n\t            scene_tps.append(f'{prob}::attr(\"{attr_tp[1]}\", {int(attr_tp[0])}).')\n\t        for rela_tp, prob in zip(rela_tps, rela_probs):\n\t            # if not rela_tp[0] == \"left\":\n", "            #     continue\n\t            scene_tps.append(f'{prob}::relation(\"{rela_tp[0]}\", {int(rela_tp[1])}, {int(rela_tp[2])}).')\n\t        return \"\\n\".join(scene_tps)\n\t    def process_result(self, result):\n\t        output = result.stdout.decode()\n\t        lines = output.split(\"\\n\")\n\t        targets = {}\n\t        for line in lines:\n\t            if line == '':\n\t                continue\n", "            if not '\\t' in line:\n\t                continue\n\t            info = line.split('\\t')\n\t            # No target found\n\t            if 'X' in info[0]:\n\t                break\n\t            target_name = int(info[0][7:-2])\n\t            target_prob = float(info[1])\n\t            targets[target_name] = target_prob\n\t        return targets\n", "    def get_result(self, task, fact_tps, fact_probs):\n\t        timeout = False\n\t        question = task[\"question\"][\"clauses\"]\n\t        file_name = f\"{task['question']['question_id']}.pl\"\n\t        save_path = os.path.join (self.save_dir, file_name)\n\t        query = Query(question)\n\t        query_content = self.transformer.transform(query)\n\t        scene_content = self.fact_prob_to_file(fact_tps, fact_probs)\n\t        content = KG+ \"\\n\" + RULES + \"\\n\" + scene_content + \"\\n\" + query_content\n\t        self.save_file(file_name, content)\n", "        try:\n\t            result = subprocess.run([\"problog\", save_path], capture_output=True, timeout=10)\n\t            targets = self.process_result(result)\n\t        except:\n\t            # time out here\n\t            timeout = True\n\t            targets = {}\n\t        # self.delete_file(file_name)\n\t        return targets, timeout\n\t    def get_relas(self, query):\n", "        relations = []\n\t        for clause in query:\n\t            if 'Relate' in clause['function']:\n\t                relations.append(clause['text_input'])\n\t        return relations\n"]}
{"filename": "src/experiments/vqa/test.py", "chunked_list": ["print(\"start importing...\")\n\timport time\n\timport sys\n\timport argparse\n\timport datetime\n\tsys.path.append('../../')\n\tsys.path.append('../../SLASH/')\n\tsys.path.append('../../EinsumNetworks/src/')\n\t#torch, numpy, ...\n\timport torch\n", "from torch.utils.tensorboard import SummaryWriter\n\tfrom torchvision.transforms import transforms\n\timport torchvision\n\timport numpy as np\n\timport json\n\t#own modules\n\tfrom dataGen import VQA\n\tfrom einsum_wrapper import EiNet\n\tfrom network_nn import Net_nn\n\tfrom tqdm import tqdm\n", "#import slash\n\tfrom slash import SLASH\n\timport os\n\timport utils\n\tfrom utils import set_manual_seed\n\tfrom pathlib import Path\n\tfrom rtpt import RTPT\n\timport pickle\n\tfrom knowledge_graph import RULES, KG\n\tfrom dataGen import name_npp, relation_npp, attribute_npp\n", "from models import name_clf, rela_clf, attr_clf \n\tprint(\"...done\")\n\tdef get_args():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--seed\", type=int, default=10, help=\"Random generator seed for all frameworks\"\n\t    )\n\t    parser.add_argument(\n\t        \"--network-type\",\n\t        choices=[\"nn\",\"pc\"],\n", "        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n\t    )\n\t    parser.add_argument(\n\t        \"--pc-structure\",\n\t        choices=[\"poon-domingos\",\"binary-trees\"],\n\t        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n\t    )\n\t    parser.add_argument(\n\t        \"--batch-size\", type=int, default=100, help=\"Batch size to train with\"\n\t    )\n", "    parser.add_argument(\n\t        \"--num-workers\", type=int, default=6, help=\"Number of threads for data loader\"\n\t    )\n\t    parser.add_argument(\n\t        \"--p-num\", type=int, default=8, help=\"Number of processes to devide the batch for parallel processing\"\n\t    )\n\t    parser.add_argument(\"--credentials\", type=str, help=\"Credentials for rtpt\")\n\t    args = parser.parse_args()\n\t    if args.network_type == 'pc':\n\t        args.use_pc = True\n", "    else:\n\t        args.use_pc = False\n\t    return args\n\tdef determine_max_objects(task_file):\n\t    with open (task_file, 'rb') as tf:\n\t        tasks = pickle.load(tf)\n\t        print(\"taskfile len\",len(tasks))\n\t    #get the biggest number of objects in the image\n\t    max_objects = 0\n\t    for tidx, task in enumerate(tasks):\n", "        all_oid = task['question']['input']\n\t        len_all_oid = len(all_oid)\n\t        #store the biggest number of objects in an image\n\t        if len_all_oid > max_objects:\n\t            max_objects = len_all_oid\n\t    return max_objects\n\tdef slash_vqa():\n\t    args = get_args()\n\t    print(args)\n\t    # Set the seeds for PRNG\n", "    set_manual_seed(args.seed)\n\t    # Create RTPT object\n\t    rtpt = RTPT(name_initials=args.credentials, experiment_name='SLASH VQA', max_iterations=1)\n\t    # Start the RTPT tracking\n\t    rtpt.start()\n\t    #writer = SummaryWriter(os.path.join(\"runs\",\"vqa\", str(args.seed)), purge_step=0)\n\t    #exp_name = 'vqa3'\n\t    #Path(\"data/\"+exp_name+\"/\").mkdir(parents=True, exist_ok=True)\n\t    #saveModelPath = 'data/'+exp_name+'/slash_vqa_models_seed'+str(args.seed)+'.pt'\n\t    #TODO workaround that adds +- notation \n", "    program_example = \"\"\"\n\t%scallop conversion rules\n\tname(O,N) :-  name(0,+O,-N).\n\tattr(O,A) :-  attr(0, +O, -A).\n\trelation(O1,O2,N) :-  relation(0, +(O1,O2), -N).\n\t\"\"\"\n\t    #test_f = \"dataset/task_list/test_tasks_c3_1000.pkl\"  # Test datset\n\t    test_f = {\"c2\":\"dataset/task_list/test_tasks_c2_1000.pkl\",\n\t              \"c3\":\"dataset/task_list/test_tasks_c3_1000.pkl\",\n\t              \"c4\":\"dataset/task_list/test_tasks_c4_1000.pkl\",\n", "              \"c5\":\"dataset/task_list/test_tasks_c5_1000.pkl\",\n\t              \"c6\":\"dataset/task_list/test_tasks_c6_1000.pkl\"\n\t              }\n\t    num_obj = []\n\t    if type(test_f) == str: \n\t        num_obj.append(determine_max_objects(test_f))\n\t    #if we have multiple test files\n\t    elif type(test_f) == dict:\n\t        for key in test_f:   \n\t            num_obj.append(determine_max_objects(test_f[key]))\n", "    NUM_OBJECTS = np.max(num_obj)\n\t    NUM_OBJECTS = 70\n\t    vqa_params = {\"l\":200,\n\t            \"l_split\":100,\n\t            \"num_names\":500,\n\t            \"max_models\":10000,\n\t            \"asp_timeout\": 60}\n\t    #load models #data/vqa18_10/slash_vqa_models_seed0_epoch_0.pt\n\t    #src/experiments/vqa/\n\t    #saved_models = torch.load(\"data/test/slash_vqa_models_seed42_epoch_9.pt\")\n", "    saved_models = torch.load(\"data/vqa_debug_relations_17_04_2023/slash_vqa_models_seed0_epoch_2.pt\")\n\t    print(saved_models.keys())\n\t    rela_clf.load_state_dict(saved_models['relation_clf'])\n\t    name_clf.load_state_dict(saved_models['name_clf'])\n\t    attr_clf.load_state_dict(saved_models['attr_clf'])\n\t    #create the SLASH Program , ,\n\t    nnMapping = {'relation': rela_clf, 'name':name_clf , \"attr\":attr_clf}\n\t    optimizers = {'relation': torch.optim.Adam(rela_clf.parameters(), lr=0.001, eps=1e-7),\n\t                      'name': torch.optim.Adam(name_clf.parameters(), lr=0.001, eps=1e-7),\n\t                      'attr': torch.optim.Adam(attr_clf.parameters(), lr=0.001, eps=1e-7)}\n", "    all_oid = np.arange(0,NUM_OBJECTS)\n\t    object_string = \"\".join([ f\"object({oid1},{oid2}). \" for oid1 in all_oid for oid2 in all_oid if oid1 != oid2])\n\t    object_string = \"\".join([\"\".join([f\"object({oid}). \" for oid in all_oid]), object_string])\n\t    #parse the SLASH program\n\t    print(\"create SLASH program\")\n\t    program = \"\".join([KG, RULES, object_string, name_npp, relation_npp, attribute_npp, program_example])\n\t    SLASHobj = SLASH(program, nnMapping, optimizers)\n\t    #load the data\n\t    if type(test_f) == str: \n\t        test_data = VQA(\"test\", test_f, NUM_OBJECTS)\n", "        test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n\t    #if we have multiple test files\n\t    elif type(test_f) == dict: \n\t        for key in test_f:\n\t            test_data = VQA(\"test\", test_f[key], NUM_OBJECTS)\n\t            test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n\t            test_f[key] = test_loader\n\t    print(\"---TEST---\")\n\t    if type(test_f) == str:\n\t        recall_5_test, test_time = SLASHobj.testVQA(test_loader, args.p_num, vqa_params=vqa_params)\n", "        print(\"test-recall@5\", recall_5_test)\n\t    elif type(test_f) == dict:\n\t        test_time = 0\n\t        recalls = []\n\t        for key in test_f:\n\t            recall_5_test, tt = SLASHobj.testVQA(test_f[key], args.p_num, vqa_params=vqa_params)\n\t            test_time += tt\n\t            recalls.append(recall_5_test)\n\t            print(\"test-recall@5_{}\".format(key), recall_5_test, \", test_time:\", tt )\n\t        print(\"test-recall@5_c_all\", np.mean(recalls), \", test_time:\", test_time)\n", "if __name__ == \"__main__\":\n\t    slash_vqa()\n"]}
{"filename": "src/experiments/vqa/sg_model.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n", "import torch.nn.functional as F\n\timport os\n\ttorch.autograd.set_detect_anomaly(True)\n\tdef load_model(model, model_f, device):\n\t    print('loading model from %s' % model_f)\n\t    model.load_state_dict(torch.load(model_f, map_location=device))\n\t    model.eval()\n\tclass MLPClassifier(nn.Module):\n\t    def __init__(self, input_dim, latent_dim, output_dim, n_layers, dropout_rate):\n\t        super(MLPClassifier, self).__init__()\n", "        layers = []\n\t        layers.append(nn.Linear(input_dim, latent_dim))\n\t        layers.append(nn.ReLU())\n\t        layers.append(nn.BatchNorm1d(latent_dim))\n\t        layers.append(nn.Dropout(dropout_rate))\n\t        for _ in range(n_layers - 1):\n\t            layers.append(nn.Linear(latent_dim, latent_dim))\n\t            layers.append(nn.ReLU())\n\t            layers.append(nn.BatchNorm1d(latent_dim))\n\t            layers.append(nn.Dropout(dropout_rate))\n", "        layers.append(nn.Linear(latent_dim, output_dim))\n\t        self.net = nn.Sequential(*layers)\n\t    def forward(self, x):\n\t        logits = self.net(x)\n\t        return logits\n\tclass SceneGraphModel:\n\t    def __init__(self, feat_dim, n_names, n_attrs, n_rels, device, model_dir=None):\n\t        self.feat_dim = feat_dim\n\t        self.n_names = n_names\n\t        self.n_attrs = n_attrs\n", "        self.n_rels = n_rels\n\t        self.device = device\n\t        self._init_models()\n\t        if model_dir is not None:\n\t            self._load_models(model_dir)\n\t    def _load_models(self, model_dir):\n\t        for type in ['name', 'relation', 'attribute']:\n\t            load_model(\n\t                model=self.models[type],\n\t                model_f=model_dir+'/%s_best_epoch.pt' % type,\n", "                device=self.device\n\t            )\n\t    def _init_models(self):\n\t        name_clf = MLPClassifier(\n\t            input_dim=self.feat_dim,\n\t            output_dim=self.n_names,\n\t            latent_dim=1024,\n\t            n_layers=2,\n\t            dropout_rate=0.3\n\t        )\n", "        rela_clf = MLPClassifier(\n\t            input_dim=(self.feat_dim+4)*2,  # 4: bbox\n\t            output_dim=self.n_rels+1,       # 1: None\n\t            latent_dim=1024,\n\t            n_layers=1,\n\t            dropout_rate=0.5\n\t        )\n\t        attr_clf = MLPClassifier(\n\t            input_dim=self.feat_dim,\n\t            output_dim=self.n_attrs,\n", "            latent_dim=1024,\n\t            n_layers=1,\n\t            dropout_rate=0.3\n\t        )\n\t        self.models = {\n\t            'name': name_clf,\n\t            'attribute': attr_clf,\n\t            'relation': rela_clf\n\t        }\n\t    def predict(self, type, inputs):\n", "        # type == 'name', inputs == (obj_feat_np_array)\n\t        # type == 'relation', inputs == (sub_feat_np_array, obj_feat_np_array, sub_bbox_np_array, obj_bbox_np_array)\n\t        # type == 'attribute', inputs == (obj_feat_np_array)\n\t        model = self.models[type].to(self.device)\n\t        inputs = torch.cat([torch.from_numpy(x).float() for x in inputs]).reshape(len(inputs), -1).to(self.device)\n\t        logits = model(inputs)\n\t        if type == 'attribute':\n\t            probs = torch.sigmoid(logits)\n\t        else:\n\t            probs = F.softmax(logits, dim=1)\n", "        return logits, probs\n\t    def batch_predict(self, type, inputs, batch_split):\n\t        model = self.models[type].to(self.device)\n\t        inputs = torch.cat([torch.from_numpy(x).float() for x in inputs]).reshape(len(inputs), -1).to(self.device)\n\t        logits = model(inputs)\n\t        if type == 'attribute':\n\t            probs = torch.sigmoid(logits)\n\t        else:\n\t            current_split = 0\n\t            probs = []\n", "            for split in batch_split:\n\t                current_logits = logits[current_split:split]\n\t                # batched_logits = logits.reshape(batch_shape[0], batch_shape[1], -1)\n\t                current_probs = F.softmax(current_logits, dim=1)\n\t                # probs = probs.reshape(inputs.shape[0], -1)\n\t                probs.append(current_probs)\n\t                current_split = split\n\t            probs = torch.cat(probs).reshape(inputs.shape[0], -1)\n\t        return logits, probs\n"]}
{"filename": "src/experiments/vqa/word_idx_translator.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\timport json\n\tclass Idx2Word():\n\t    def __init__(self, meta_info, use_canon=False):\n", "        self.setup(meta_info)\n\t        self.attr_canon = meta_info['attr']['canon']\n\t        self.name_canon = meta_info['name']['canon']\n\t        self.rela_canon = meta_info['rel']['canon']\n\t        self.attr_alias = meta_info['attr']['alias']\n\t        self.rela_alias = meta_info['rel']['alias']\n\t        self.attr_to_idx_dict = meta_info['attr']['idx']\n\t        self.rela_to_idx_dict = meta_info['rel']['idx']\n\t        self.name_to_idx_dict = meta_info['name']['idx']\n\t        self.use_canon = use_canon\n", "        # print(\"here\")\n\t    def setup(self, meta_info):\n\t        attr_to_idx = meta_info['attr']['idx']\n\t        rela_to_idx = meta_info['rel']['idx']\n\t        name_to_idx = meta_info['name']['idx']\n\t        attr_freq = meta_info['attr']['freq']\n\t        rela_freq = meta_info['rel']['freq']\n\t        name_freq = meta_info['name']['freq']\n\t        # attr_group = meta_info['attr']['group']\n\t        def setup_single(to_idx, freq, group=None):\n", "            idx_to_name = {}\n\t            for name in freq:\n\t                if name not in to_idx:\n\t                    continue\n\t                idx = to_idx[name]\n\t                if type(idx) == list:\n\t                    if not idx[0] in idx_to_name.keys():\n\t                        idx_to_name[idx[0]] = {}\n\t                    idx_to_name[idx[0]][idx[1]] = name\n\t                else:\n", "                    idx_to_name[idx] = name\n\t            return idx_to_name\n\t        self.idx_to_name_dict = setup_single(name_to_idx, name_freq)\n\t        self.idx_to_rela_dict = setup_single(rela_to_idx, rela_freq)\n\t        self.idx_to_attr_dict = setup_single(attr_to_idx, attr_freq)\n\t        # self.idx_to_attr_dict = setup_single(attr_to_idx, attr_freq, attr_group)\n\t    def get_name_ct(self):\n\t        return len(self.idx_to_name_dict)\n\t    def get_rela_ct(self):\n\t        return len(self.idx_to_rela_dict)\n", "    def get_attr_ct(self):\n\t        return len(self.idx_to_attr_dict)\n\t    def get_names(self):\n\t        return list(self.idx_to_name_dict.values())\n\t    def idx_to_name(self, idx):\n\t        if idx is None:\n\t            return None\n\t        if type(idx) == str:\n\t            return idx\n\t        if len(self.idx_to_name_dict) == idx:\n", "            return None\n\t        if idx == -1:\n\t            return None\n\t        return self.idx_to_name_dict[idx]\n\t    def idx_to_rela(self, idx):\n\t        if idx is None:\n\t            return None\n\t        if idx == -1:\n\t            return None\n\t        if type(idx) == str:\n", "            return idx\n\t        if len(self.idx_to_rela_dict) == idx:\n\t            return None\n\t        return self.idx_to_rela_dict[idx]\n\t    def idx_to_attr(self, idx):\n\t        if idx is None:\n\t            return None\n\t        if type(idx) == str:\n\t            return idx\n\t        if len(self.idx_to_attr_dict) == idx:\n", "            return None\n\t        if idx == -1:\n\t            return None\n\t        # return self.idx_to_attr_dict[idx[0]][idx[1]]\n\t        return self.idx_to_attr_dict[idx]\n\t    def attr_to_idx(self, attr):\n\t        if attr is None:\n\t            return attr\n\t        if self.use_canon:\n\t            if attr in self.attr_canon.keys():\n", "                attr = self.attr_canon[attr]\n\t        if attr in self.attr_alias.keys():\n\t            attr = self.attr_alias[attr]\n\t        if attr not in self.attr_to_idx_dict.keys():\n\t            return None\n\t        return self.attr_to_idx_dict[attr]\n\t    def name_to_idx(self, name):\n\t        if name is None:\n\t            return name\n\t        if self.use_canon:\n", "            if name in self.name_canon.keys():\n\t                name = self.name_canon[name]\n\t        if name not in self.name_to_idx_dict.keys():\n\t            return None\n\t        return self.name_to_idx_dict[name]\n\t    def rela_to_idx(self, rela):\n\t        if rela is None:\n\t            return rela\n\t        if self.use_canon:\n\t            if rela in self.rela_canon.keys():\n", "                rela = self.rela_canon[rela]\n\t        if rela in self.rela_alias.keys():\n\t            rela = self.rela_alias[rela]\n\t        if rela not in self.rela_to_idx_dict.keys():\n\t            return None\n\t        return self.rela_to_idx_dict[rela]\n\tdef process_program(program, meta_info):\n\t    new_program = []\n\t    for clause in program:\n\t        new_clause = {}\n", "        new_clause['function'] = clause['function']\n\t        if 'output' in clause.keys():\n\t            new_clause['output'] = clause['output']\n\t        if clause['function'] == \"Hypernym_Find\":\n\t            name = clause['text_input'][0]\n\t            attr = clause['text_input'][1]\n\t            new_clause['text_input'] = [name, attr] + clause['text_input'][2:]\n\t        elif clause['function'] == \"Find\":\n\t            name = clause['text_input'][0]\n\t            new_clause['text_input'] = [name] + clause['text_input'][1:]\n", "        elif clause['function'] == \"Relate_Reverse\":\n\t            relation = clause['text_input']\n\t            new_clause['text_input'] = relation\n\t        elif clause['function'] == \"Relate\":\n\t            relation = clause['text_input']\n\t            new_clause['text_input'] = relation\n\t        else:\n\t            if 'text_input' in clause.keys():\n\t                new_clause['text_input'] = clause['text_input']\n\t        new_program.append(new_clause)\n", "    return new_program\n\tdef process_questions(questions_path, new_question_path, meta_info):\n\t    new_questions = {}\n\t    with open(questions_path, 'r') as questions_file:\n\t        questions = json.load(questions_file)\n\t    # process questions\n\t    for question in questions:\n\t        image_id = question[\"image_id\"]\n\t        # process questions\n\t        if image_id not in new_questions.keys():\n", "            new_questions[image_id] = {}\n\t        new_question = new_questions[image_id]\n\t        new_question['question_id'] = question['question_id']\n\t        new_question['program'] = process_program(\n\t            question[\"program\"], meta_info)\n\t        program = question['program']\n\t        new_question['target'] = program[-2][\"output\"]\n\t        new_question['question'] = question['question']\n\t        new_question['answer'] = question['answer']\n\t    with open(new_question_path, 'w') as new_question_file:\n", "        json.dump(new_questions, new_question_file)\n"]}
{"filename": "src/experiments/vqa/trainer.py", "chunked_list": ["\"\"\"\n\tThe source code is based on:\n\tScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\n\tJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\n\tAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\n\thttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\t\"\"\"\n\timport os\n\timport json\n\timport numpy as np\n", "import sys\n\tfrom tqdm import tqdm\n\timport torch\n\timport torch.optim as optim\n\tfrom torch.optim.lr_scheduler import StepLR\n\tfrom torch.nn import BCELoss\n\timport time\n\timport math\n\timport statistics\n\t# import concurrent.futures\n", "supervised_learning_path = os.path.abspath(os.path.join(\n\t    os.path.abspath(__file__), \"../../supervised_learning\"))\n\tsys.path.insert(0, supervised_learning_path)\n\tcommon_path = os.path.abspath(os.path.join(os.path.abspath(__file__), \"../..\"))\n\tsys.path.insert(0, common_path)\n\tfrom query_lib import QueryManager\n\t#from src.experiments.vqa.cmd_args2 import cmd_args\n\tfrom word_idx_translator import Idx2Word\n\tfrom sg_model import SceneGraphModel\n\tfrom learning import get_fact_probs\n", "from vqa_utils import auc_score, get_recall\n\tdef prog_analysis(datapoint):\n\t    knowledge_op = ['Hypernym_Find', 'KG_Find']\n\t    rela_op = ['Relate', 'Relate_Reverse']\n\t    clauses = datapoint['question']['clauses']\n\t    ops = [ clause['function'] for clause in clauses ]\n\t    kg_num = sum([1 if op in knowledge_op else 0 for op in ops])\n\t    rela_num = sum([1 if op in rela_op else 0 for op in ops])\n\t    return (kg_num, rela_num)\n\tclass ClauseNTrainer():\n", "    def __init__(self,\n\t                 train_data_loader,\n\t                 val_data_loader,\n\t                 test_data_loader=None,\n\t                 #model_dir=cmd_args.model_dir, n_epochs=cmd_args.n_epochs,\n\t                 #save_dir=cmd_args.save_dir,\n\t                 #meta_f=cmd_args.meta_f, knowledge_base_dir=cmd_args.knowledge_base_dir,\n\t                 #axiom_update_size=cmd_args.axiom_update_size):\n\t                 model_dir=\"data_model/\", n_epochs=2,\n\t                 save_dir=\"data_save/\",\n", "                 meta_f=\"dataset/gqa_info.json\", knowledge_base_dir=\"\",\n\t                 axiom_update_size=\"\"):\n\t        self.model_dir = model_dir\n\t        model_exists = self._model_exists(model_dir)\n\t        if not model_exists:\n\t            load_model_dir = None\n\t        else:\n\t            load_model_dir = model_dir\n\t        if train_data_loader is not None:\n\t            self.train_data = train_data_loader\n", "        if val_data_loader is not None:\n\t            self.val_data = val_data_loader\n\t        if test_data_loader is not None:\n\t            self.val_data = test_data_loader\n\t        self.is_train = test_data_loader is None\n\t        meta_info = json.load(open(meta_f, 'r'))\n\t        self.idx2word = Idx2Word(meta_info)\n\t        self.n_epochs = n_epochs\n\t        self.query_manager = QueryManager(save_dir)\n\t        self.axiom_update_size = axiom_update_size\n", "        self.wmc_funcs = {}\n\t        # load dictionary from previous training results\n\t        self.sg_model = SceneGraphModel(\n\t            feat_dim=64,\n\t            n_names=meta_info['name']['num'],\n\t            n_attrs=meta_info['attr']['num'],\n\t            n_rels=meta_info['rel']['num'],\n\t            device=torch.device('cuda'),\n\t            model_dir=load_model_dir\n\t        )\n", "        self.sg_model_dict = self.sg_model.models\n\t        self.loss_func = BCELoss()\n\t        if self.is_train:\n\t            self.optimizers = {}\n\t            self.schedulers = {}\n\t            for model_type, model_info in self.sg_model_dict.items():\n\t                if model_type == 'name':\n\t                    self.optimizers['name'] = optim.Adam(\n\t                        model_info.parameters(), lr=0.01)\n\t                    self.schedulers['name'] = StepLR(\n", "                        self.optimizers['name'], step_size=10, gamma=0.1)\n\t                    # self.loss_func['name'] = F.cross_entropy\n\t                if model_type == 'relation':\n\t                    self.optimizers['rel'] = optim.Adam(\n\t                        model_info.parameters(), lr=0.01)\n\t                    self.schedulers['rel'] = StepLR(\n\t                        self.optimizers['rel'], step_size=10, gamma=0.1)\n\t                if model_type == 'attribute':\n\t                    self.optimizers['attr'] = optim.Adam(\n\t                        model_info.parameters(), lr=0.01)\n", "                    self.schedulers['attr'] = StepLR(\n\t                        self.optimizers['attr'], step_size=10, gamma=0.1)\n\t        # self.pool = mp.Pool(cmd_args.max_workers)\n\t        # self.batch = cmd_args.trainer_batch\n\t    def _model_exists(self, model_dir):\n\t        name_f = os.path.join(self.model_dir, 'name_best_epoch.pt')\n\t        rela_f = os.path.join(self.model_dir, 'relation_best_epoch.pt')\n\t        attr_f = os.path.join(self.model_dir, 'attribute_best_epoch.pt')\n\t        if not os.path.exists(name_f):\n\t            return False\n", "        if not os.path.exists(rela_f):\n\t            return False\n\t        if not os.path.exists(attr_f):\n\t            return False\n\t        return True\n\t    def _get_optimizer(self, data_type):\n\t        optimizer = self.optimizers[data_type]\n\t        return optimizer\n\t    def _step_all(self):\n\t        for optim_type, optim_info in self.optimizers.items():\n", "            optim_info.step()\n\t    def _step_scheduler(self):\n\t        for scheduler_type, scheduler_info in self.schedulers.items():\n\t            scheduler_info.step()\n\t    def _zero_all(self):\n\t        for optim_type, optim_info in self.optimizers.items():\n\t            optim_info.zero_grad()\n\t    def _train_all(self):\n\t        for model_type, model_info in self.sg_model_dict.items():\n\t            if model_type == 'name':\n", "                model_info.train()\n\t                model_info.share_memory()\n\t            if model_type == 'relation':\n\t                model_info.train()\n\t                model_info.share_memory()\n\t            if model_type == 'attribute':\n\t                model_info.train()\n\t                model_info.share_memory()\n\t    def _eval_all(self):\n\t        for model_type, model_info in self.sg_model_dict.items():\n", "            if model_type == 'name':\n\t                model_info.eval()\n\t                model_info.share_memory()\n\t            if model_type == 'relation':\n\t                model_info.eval()\n\t                model_info.share_memory()\n\t            if model_type == 'attribute':\n\t                model_info.eval()\n\t                model_info.share_memory()\n\t    def _save_all(self):\n", "        for model_type, model_info in self.sg_model_dict.items():\n\t            if model_type == 'name':\n\t                save_f = os.path.join(self.model_dir, 'name_best_epoch.pt')\n\t                torch.save(model_info.state_dict(), save_f)\n\t            if model_type == 'relation':\n\t                save_f = os.path.join(self.model_dir, 'relation_best_epoch.pt')\n\t                torch.save(model_info.state_dict(), save_f)\n\t            if model_type == 'attribute':\n\t                save_f = os.path.join(self.model_dir, 'attribute_best_epoch.pt')\n\t                torch.save(model_info.state_dict(), save_f)\n", "    def loss_acc(self, targets, correct, all_oids, is_train=True):\n\t        pred = []\n\t        for oid in all_oids:\n\t            if oid in targets:\n\t                pred.append(targets[oid])\n\t            else:\n\t                pred.append(0)\n\t        labels = [1 if obj in correct else 0 for obj in all_oids]\n\t        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n\t        pred_tensor = torch.tensor(pred, dtype=torch.float32)\n", "        pred_tensor = pred_tensor.reshape(1, -1)\n\t        labels_tensor = labels_tensor.reshape(1, -1)\n\t        loss = self.loss_func(pred_tensor, labels_tensor)\n\t        recall = get_recall(labels_tensor, pred_tensor)\n\t        if math.isnan(recall):\n\t            recall = -1\n\t        return loss.item(), recall\n\t    def _pass(self, datapoint, is_train=True):\n\t        correct = datapoint['question']['output']\n\t        all_oid = datapoint['question']['input']\n", "        fact_tps, fact_probs = get_fact_probs(self.sg_model, datapoint, self.idx2word, self.query_manager)\n\t        result, timeout = self.query_manager.get_result(datapoint, fact_tps, fact_probs)\n\t        if not timeout:\n\t            loss, acc = self.loss_acc(result, correct, all_oid)\n\t        else:\n\t            loss = -1\n\t            acc = -1\n\t        return acc, loss, timeout\n\t    def _train_epoch(self, ct):\n\t        self._train_all()\n", "        aucs = []\n\t        losses = []\n\t        timeouts = 0\n\t        pbar = tqdm(self.train_data)\n\t        for datapoint in pbar:\n\t            auc, loss, timeout = self._pass(datapoint, is_train=True)\n\t            if not timeout:\n\t                if auc >= 0:\n\t                    aucs.append(auc)\n\t                losses.append(loss)\n", "            else:\n\t                timeouts += 1\n\t            pbar.set_description(\n\t                f'[loss: {np.array(losses).mean()}, auc: {np.array(aucs).mean()}, timeouts: {timeouts}]')\n\t            torch.cuda.empty_cache()\n\t            self._step_all()\n\t            self._zero_all()\n\t        return np.mean(losses), np.mean(aucs)\n\t    def _val_epoch(self):\n\t        self._eval_all()\n", "        timeouts = 0\n\t        aucs = []\n\t        losses = []\n\t        time_out_prog_kg = {}\n\t        time_out_prog_rela = {}\n\t        success_prog_kg = {}\n\t        success_prog_rela = {}\n\t        pbar = tqdm(self.val_data)\n\t        with torch.no_grad():\n\t            for datapoint in pbar:\n", "                kg_num, rela_num = prog_analysis(datapoint)\n\t                auc, loss, timeout = self._pass(datapoint, is_train=False)\n\t                if not timeout:\n\t                    aucs.append(auc)\n\t                    losses.append(loss)\n\t                    if not kg_num in success_prog_kg:\n\t                        success_prog_kg[kg_num] = 0\n\t                    if not rela_num in success_prog_rela:\n\t                        success_prog_rela[rela_num] = 0\n\t                    success_prog_kg[kg_num] += 1\n", "                    success_prog_rela[rela_num] += 1\n\t                else:\n\t                    timeouts += 1\n\t                    if not kg_num in time_out_prog_kg:\n\t                        time_out_prog_kg[kg_num] = 0\n\t                    if not rela_num in time_out_prog_rela:\n\t                        time_out_prog_rela[rela_num] = 0\n\t                    time_out_prog_kg[kg_num] += 1\n\t                    time_out_prog_rela[rela_num] += 1\n\t                if not len(aucs) == 0:\n", "                    pbar.set_description(\n\t                        f'[loss: {np.array(losses).mean()}, auc: {np.array(aucs).mean()}, timeouts: {timeouts}]')\n\t        print(f\"succ kg: {success_prog_kg}, succ rela: {success_prog_rela}\")\n\t        print(f\"timeout kg: {time_out_prog_kg}, timeout rela: {time_out_prog_rela}\")\n\t        return np.mean(losses), np.mean(aucs)\n\t    def train(self):\n\t        assert self.is_train\n\t        best_val_loss = np.inf\n\t        for epoch in range(self.n_epochs):\n\t            train_loss, train_acc = self._train_epoch(epoch)\n", "            val_loss, val_acc = self._val_epoch()\n\t            self._step_scheduler()\n\t            print(\n\t                '[Epoch %d/%d] [training loss: %.2f, auc: %.2f] [validation loss: %.2f, auc: %.2f]' %\n\t                (epoch, self.n_epochs, train_loss, train_acc, val_loss, val_acc)\n\t            )\n\t            if val_loss < best_val_loss:\n\t                best_val_loss = val_loss\n\t                print('saving best models')\n\t                self._save_all()\n", "    def test(self):\n\t        assert not self.is_train\n\t        test_loss, test_acc = self._val_epoch()\n\t        print('[test loss: %.2f, acc: %.2f]' % (test_loss, test_acc))\n"]}
{"filename": "src/experiments/slash_attention/ap_utils.py", "chunked_list": ["import numpy as np\n\timport torch \n\timport time\n\timport random\n\t'''\n\tAll helper functions to compute the average precision for different datasets\n\t'''\n\tdef get_obj_encodings(dataloader):\n\t    '''\n\t    Collects all object encodings in a single tensor for the test dataset\n", "    @param dataloader: Dataloader object for a shapeworld/clevr like dataset that returns an object encoding \n\t    '''\n\t    obj_list = []\n\t    for _,_,obj_encoding in dataloader:\n\t        for i in range(0, len(obj_encoding)):\n\t            obj_list.append(obj_encoding[i])\n\t    obj_list = torch.stack(obj_list)\n\t    return obj_list\n\tdef inference_map_to_array(inference, only_color=False, only_shape=False, only_shade=False, only_size=False, only_material=False):\n\t    '''\n", "    Returns an encoding of the output objects having shape [bs, num_slots, properties] for the slot attention experiment. The encoding can be used then for the AP metric.\n\t    properties depends on the number of classes + 1 for the prediction confidence  predicted e.g. red,green,blue + prediction confidence -> 3+1 = 4\n\t    @param inference: Hashmap of the form {'slot':{'color' : [bs,num_colors], ...}, ...}\n\t    '''\n\t    all_object_encodings = None #contains the encoding all slots over the batchsize\n\t    #iterate over all slots and collect the properties \n\t    for slot in inference:\n\t        slot_object_encoding = torch.Tensor([]) #contains the encoding for one slot over the batchsize\n\t        prediction_confidence = None\n\t        #iterate over all properties and put them together into one vector\n", "        for prop in inference[slot]:\n\t            slot_object_encoding = torch.cat((slot_object_encoding, inference[slot][prop].cpu()), axis=1)\n\t            #select the prediction confidence. #We want to select the highest prediction value for the selected properties e.g. col, shape, col+shape\n\t            if (prop == 'color' and only_color) or (prop == 'shape' and only_shape) or (prop == 'shade' and only_shade) or (prop == 'size' and only_size) or (prop == 'material' and only_material) or (only_color is False and only_shape is False and only_shade is False and only_size is False): \n\t                #collect the argmax prediction values for each property\n\t                if prediction_confidence is None:\n\t                    prediction_confidence = torch.max(inference[slot][prop].cpu(), axis=1)[0][None,:]\n\t                else:\n\t                    prediction_confidence = torch.cat((prediction_confidence, torch.max(inference[slot][prop].cpu(), axis=1)[0][None,:]))\n\t        #take the mean over the prediction confidence if we have more than one property and then append it to the properties tensor\n", "        if only_color is False and only_shape is False:\n\t            prediction_confidence = prediction_confidence.mean(axis=0)\n\t        else:\n\t            prediction_confidence = prediction_confidence.squeeze()\n\t        slot_object_encoding = torch.cat((slot_object_encoding, prediction_confidence[:,None]), axis=1)\n\t        #concatenate all slot encodings\n\t        if all_object_encodings is None:\n\t            all_object_encodings = slot_object_encoding[None,:,:]\n\t        else:\n\t            all_object_encodings = torch.cat((all_object_encodings, slot_object_encoding[None,:,:]), axis=0)\n", "    return torch.einsum(\"abc->bac\", all_object_encodings)\n\tdef average_precision(pred, attributes, distance_threshold, dataset='', subset_size=None, only_color=False, only_shape=False, only_shade=False, only_size=False, only_material=False):\n\t    \"\"\"Computes the average precision for CLEVR or Shapeworld.\n\t    This function computes the average precision of the predictions specifically\n\t    for the CLEVR dataset. First, we sort the predictions of the model by\n\t    confidence (highest confidence first). Then, for each prediction we check\n\t    whether there was a corresponding object in the input image. A prediction is\n\t    considered a true positive if the discrete features are predicted correctly\n\t    and the predicted position is within a certain distance from the ground truth\n\t    object.\n", "    Args:\n\t    pred: Tensor of shape [batch_size, num_elements, dimension] containing\n\t      predictions. The last dimension is expected to be the confidence of the\n\t      prediction.\n\t    attributes: Tensor of shape [batch_size, num_elements, dimension] containing\n\t      ground-truth object properties.\n\t    distance_threshold: Threshold to accept match. -1 indicates no threshold.\n\t    only_color: Only consider color as a relevant property for a match.\n\t    only_shape: Only consider shape as a relevant property for a match.\n\t    only_shade: Consider only shade as a relevant property for a match. SHAPEWORLD\n", "    only_material: Consider only material as a relevant property for a match. CLEVR\n\t    Returns:\n\t    Average precision of the predictions.\n\t    \"\"\"\n\t    if subset_size is not None:\n\t        idx = random.sample(list(np.arange(0,attributes.shape[0])), subset_size)\n\t        attributes = attributes[idx,:,:]\n\t        pred = pred[idx,:,:]\n\t    [batch_size, _, element_size] = attributes.shape\n\t    [_, predicted_elements, _] = pred.shape\n", "    def unsorted_id_to_image(detection_id, predicted_elements):\n\t        \"\"\"Find the index of the image from the unsorted detection index.\"\"\"\n\t        return int(detection_id // predicted_elements)\n\t    flat_size = batch_size * predicted_elements\n\t    flat_pred = np.reshape(pred, [flat_size, element_size])\n\t    sort_idx = np.argsort(flat_pred[:, -1], axis=0)[::-1]  # Reverse order.\n\t    sorted_predictions = np.take_along_axis(\n\t        flat_pred, np.expand_dims(sort_idx, axis=1), axis=0)\n\t    idx_sorted_to_unsorted = np.take_along_axis(\n\t      np.arange(flat_size), sort_idx, axis=0)\n", "    def process_targets_CLEVR(target):\n\t        \"\"\"Unpacks the target into the CLEVR properties.\"\"\"\n\t        size = np.argmax(target[:3])\n\t        material = np.argmax(target[3:6])\n\t        shape = np.argmax(target[6:10])\n\t        color = np.argmax(target[10:19])\n\t        real_obj = target[19]\n\t        attr = [size, material, shape, color]\n\t        is_bg = False\n\t        if color == 8 and shape == 3 and material == 2 and size == 2:\n", "            is_bg = True\n\t        if only_color: #consider only the color properties for comparison\n\t            attr = [color]                \n\t        if only_shape: #consider only the shape properties for comparison\n\t            attr = [shape]                \n\t        if only_material: #consider only the shade properties for comparison\n\t            attr = [material]                \n\t        if only_size: #consider only the size properties for comparison\n\t            attr = [size]             \n\t        coords = np.array([0,0,0]) # We don't learn the coordinates of the objects\n", "        # return coords, object_size, material, shape, color, real_obj\n\t        return coords, attr, is_bg, real_obj\n\t    def process_targets_SHAPEWORLD4(target):\n\t        \"\"\"Unpacks the target into the Shapeworld properties.\"\"\"\n\t        #[c,c,c,c,c,c,c,c,c , s,s,s,s , h,h,h, z,z,z, confidence]\n\t        color = np.argmax(target[:9])\n\t        shape = np.argmax(target[9:13])\n\t        shade = np.argmax(target[13:16])\n\t        size = np.argmax(target[16:19])\n\t        real_obj = target[19]\n", "        is_bg= False\n\t        attr = [shape, color, shade, size]\n\t        if color == 8 and shape == 3 and shade == 2 and size == 2:\n\t            is_bg = True\n\t        if only_color: #consider only the color properties for comparison\n\t            attr = [color]                \n\t        if only_shape: #consider only the shape properties for comparison\n\t            attr = [shape]                \n\t        if only_shade: #consider only the shade properties for comparison\n\t            attr = [shade]                \n", "        if only_size: #consider only the size properties for comparison\n\t            attr = [size]                \n\t        coords = np.array([0,0,0])\n\t        return coords, attr, is_bg, real_obj\n\t    def process_targets_SHAPEWORLD_OOD(target):\n\t        \"\"\"Unpacks the target into the Shapeworld ood properties.\"\"\"\n\t        #[c,c,c,c,c,c,c,c,c , s,s,s,s,s,s , h,h,h, z,z,z, confidence]\n\t        color = np.argmax(target[:9])\n\t        shape = np.argmax(target[9:15])\n\t        shade = np.argmax(target[15:18])\n", "        size = np.argmax(target[18:21])\n\t        real_obj = target[21]\n\t        is_bg= False\n\t        attr = [shape, color, shade, size]\n\t        if color == 8 and shape == 5 and shade == 2 and size == 2:\n\t            is_bg = True\n\t        if only_color: #consider only the color properties for comparison\n\t            attr = [color]                \n\t        if only_shape: #consider only the shape properties for comparison\n\t            attr = [shape]                \n", "        if only_shade: #consider only the shade properties for comparison\n\t            attr = [shade]                \n\t        if only_size: #consider only the size properties for comparison\n\t            attr = [size]                \n\t        coords = np.array([0,0,0])\n\t        return coords, attr, is_bg, real_obj\n\t    #switch for different dataset encodings\n\t    def process_targets(target):\n\t        if dataset == 'SHAPEWORLD4':\n\t            return process_targets_SHAPEWORLD4(target)\n", "        elif dataset == 'SHAPEWORLD_OOD':\n\t            return process_targets_SHAPEWORLD_OOD(target)\n\t        elif dataset == 'CLEVR':\n\t            return process_targets_CLEVR(target)\n\t        else:\n\t            raise RuntimeError('AP metric not implemented for dataset '+dataset)\n\t    true_positives = np.zeros(sorted_predictions.shape[0])\n\t    false_positives = np.zeros(sorted_predictions.shape[0])\n\t    true_negatives = np.zeros(sorted_predictions.shape[0]) \n\t    detection_set = set()\n", "    match_count = 0\n\t    for detection_id in range(sorted_predictions.shape[0]):\n\t        # Extract the current prediction.\n\t        current_pred = sorted_predictions[detection_id, :]\n\t        # Find which image the prediction belongs to. Get the unsorted index from\n\t        # the sorted one and then apply to unsorted_id_to_image function that undoes\n\t        # the reshape.\n\t        original_image_idx = unsorted_id_to_image(\n\t            idx_sorted_to_unsorted[detection_id], predicted_elements)\n\t        # Get the ground truth image.\n", "        gt_image = attributes[original_image_idx, :, :]\n\t        # Initialize the maximum distance and the id of the groud-truth object that\n\t        # was found.\n\t        best_distance = 10000\n\t        best_id = None\n\t        # Unpack the prediction by taking the argmax on the discrete attributes.\n\t        #(pred_coords, pred_object_size, pred_material, pred_shape, pred_color,_) = process_targets(current_pred)\n\t        (pred_coords, pred_attr, is_bg, _) = process_targets(current_pred)\n\t        # Loop through all objects in the ground-truth image to check for hits.\n\t        for target_object_id in range(gt_image.shape[0]):\n", "            target_object = gt_image[target_object_id, :]\n\t            # Unpack the targets taking the argmax on the discrete attributes.\n\t            #(target_coords, target_object_size, target_material, target_shape,\n\t            #   target_color, target_real_obj) = process_targets(target_object)\n\t            (target_coords, target_attr ,_, target_real_obj) = process_targets(target_object)\n\t            # Only consider real objects as matches.\n\t            if target_real_obj:\n\t                # For the match to be valid all attributes need to be correctly\n\t                # predicted.\n\t                match = pred_attr == target_attr\n", "                if match:\n\t                    # If a match was found, we check if the distance is below the\n\t                    # specified threshold. Recall that we have rescaled the coordinates\n\t                    # in the dataset from [-3, 3] to [0, 1], both for `target_coords` and\n\t                    # `pred_coords`. To compare in the original scale, we thus need to\n\t                    # multiply the distance values by 6 before applying the norm.\n\t                    distance = np.linalg.norm((target_coords - pred_coords) * 6.)\n\t                    match_count +=1\n\t                    # If this is the best match we've found so far in terms of distance we remember it.\n\t                    if distance < best_distance:\n", "                        best_distance = distance\n\t                        best_id = target_object_id\n\t                    #If we found a match we need to check if another object with the same attributes was already assigned to it.\n\t                    elif distance_threshold == -1 and (original_image_idx,target_object_id) not in detection_set:\n\t                        best_id = target_object_id\n\t        if best_distance < distance_threshold or distance_threshold == -1:\n\t            # We have detected an object correctly within the distance confidence.\n\t            # If this object was not detected before it's a true positive.\n\t            if best_id is not None:\n\t                if (original_image_idx, best_id) not in detection_set and is_bg:\n", "                    true_negatives[detection_id] = 1\n\t                    detection_set.add((original_image_idx, best_id))\n\t                elif (original_image_idx, best_id) not in detection_set:\n\t                    true_positives[detection_id] = 1\n\t                    detection_set.add((original_image_idx, best_id))\n\t                else:\n\t                    false_positives[detection_id] = 1\n\t            else:\n\t                false_positives[detection_id] = 1\n\t        else:\n", "            false_positives[detection_id] = 1\n\t    accumulated_fp = np.cumsum(false_positives)\n\t    accumulated_tp = np.cumsum(true_positives)\n\t    accumulated_tn = np.cumsum(true_negatives)\n\t    #save tp, fp, tn\n\t    true_positives = accumulated_tp[-1]\n\t    false_positives = accumulated_fp[-1]\n\t    true_negatives = accumulated_tn[-1]\n\t    #the relevant examples is the amount of object substracted by the background detection per image(true negative)\n\t    relevant_examples = batch_size * predicted_elements - true_negatives\n", "    recall_array = accumulated_tp / relevant_examples\n\t    precision_array = np.divide(accumulated_tp, (accumulated_fp + accumulated_tp))\n\t    #print(detection_set)\n\t    objects_detected = np.zeros((batch_size,predicted_elements)) \n\t    for detection in detection_set:\n\t        objects_detected[detection[0], detection[1]] = 1\n\t    #check if all detections are true and then count all correctly classified images\n\t    correctly_classified = np.sum(np.all(objects_detected, axis=1))\n\t    return compute_average_precision(\n\t      np.array(precision_array, dtype=np.float32),\n", "      np.array(recall_array, dtype=np.float32)), true_positives, false_positives, true_negatives, correctly_classified\n\tdef compute_average_precision(precision, recall):\n\t    \"\"\"Computation of the average precision from precision and recall arrays.\"\"\"\n\t    recall = recall.tolist()\n\t    precision = precision.tolist()\n\t    recall = [0] + recall + [1]\n\t    precision = [0] + precision + [0]\n\t    for i in range(len(precision) - 1, -0, -1):\n\t        precision[i - 1] = max(precision[i - 1], precision[i])\n\t    indices_recall = [\n", "      i for i in range(len(recall) - 1) if recall[1:][i] != recall[:-1][i]\n\t    ]\n\t    average_precision = 0.\n\t    for i in indices_recall:\n\t        average_precision += precision[i + 1] * (recall[i + 1] - recall[i])\n\t    return average_precision"]}
{"filename": "src/experiments/slash_attention/clevr/train.py", "chunked_list": ["print(\"start importing...\")\n\timport time\n\timport sys\n\timport os\n\timport json\n\tsys.path.append('../../../')\n\tsys.path.append('../../../SLASH/')\n\tsys.path.append('../../../EinsumNetworks/src/')\n\twb = True\n\timport wandb\n", "#torch, numpy, ...\n\timport torch\n\tfrom torch.utils.tensorboard import SummaryWriter\n\ttorch.cuda.empty_cache()\n\timport numpy as np\n\timport importlib\n\t#own modules\n\tfrom dataGen import CLEVR\n\tfrom auxiliary import get_files_names_and_paths, get_slash_program\n\tfrom einsum_wrapper import EiNet\n", "from slash import SLASH\n\timport utils\n\timport ap_utils\n\tfrom utils import set_manual_seed\n\tfrom slot_attention_module import SlotAttention_model\n\tfrom slot_attention_module import SlotAttention_model\n\tfrom pathlib import Path\n\tfrom rtpt import RTPT\n\tprint(\"...done\")\n\tdef slash_slot_attention(exp_name , exp_dict):\n", "    # Set the seeds for PRNG\n\t    set_manual_seed(exp_dict['seed'])\n\t    # Create RTPT object\n\t    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name=f'SLASH Attention CLEVR %s' % exp_dict['obj_num'], max_iterations=int(exp_dict['epochs']))\n\t    # Start the RTPT tracking\n\t    rtpt.start()\n\t    # create save paths and tensorboard writer\n\t    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n\t    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n\t    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n", "    # save args\n\t    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n\t        json.dump(exp_dict, json_file, indent=4)\n\t    print(\"Experiment parameters:\", exp_dict)\n\t    if wb:\n\t        #start a new wandb run to track this script\n\t        wandb.init(\n\t        # set the wandb project where this run will be logged\n\t        project=\"slash_attention_clevr\",\n\t        # track hyperparameters and run metadata\n", "        config=exp_dict)\n\t    #setup new SLASH program given the network parameters\n\t    program = get_slash_program(exp_dict['obj_num'])\n\t    #setup new SLASH program given the network parameters\n\t    if exp_dict['structure'] == 'poon-domingos':\n\t        exp_dict['depth'] = None\n\t        exp_dict['num_repetitions'] = None\n\t        print(\"using poon-domingos\")\n\t    elif exp_dict['structure'] == 'binary-trees':\n\t        exp_dict['pd_num_pieces'] = None\n", "        print(\"using binary-trees\")\n\t    #size network\n\t    size_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n\t        pd_width = 8,\n\t        pd_height = 8,\n\t        class_count = 3,\n", "        use_em = exp_dict['use_em'],\n\t        learn_prior = exp_dict['learn_prior'])\n\t    #material network\n\t    material_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n\t        pd_width = 8,\n\t        pd_height = 8,\n", "        class_count = 3,\n\t        use_em = exp_dict['use_em'],\n\t        learn_prior = exp_dict['learn_prior'])\n\t    #shape network\n\t    shape_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n\t        pd_width = 8,\n", "        pd_height = 8,\n\t        class_count = 4,\n\t        use_em = exp_dict['use_em'],\n\t        learn_prior = exp_dict['learn_prior'])\n\t    #color network\n\t    color_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n", "        pd_width = 8,\n\t        pd_height = 8,\n\t        class_count = 9,\n\t        use_em = exp_dict['use_em'],\n\t        learn_prior = exp_dict['learn_prior'])\n\t    #create the Slot Attention network\n\t    slot_net = SlotAttention_model(n_slots=exp_dict['obj_num'], n_iters=3, n_attr=18,\n\t                                   encoder_hidden_channels=64, attention_hidden_channels=128, clevr_encoding=True)\n\t    slot_net = slot_net.to(device='cuda')\n\t    #trainable params\n", "    num_trainable_params = [sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in material_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n\t    num_params = [sum(p.numel() for p in size_net.parameters()), \n\t                  sum(p.numel() for p in material_net.parameters()), \n\t                  sum(p.numel() for p in shape_net.parameters()), \n\t                  sum(p.numel() for p in color_net.parameters()),\n\t                  sum(p.numel() for p in slot_net.parameters())]\n", "    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n\t    slot_net_params = list(slot_net.parameters())\n\t    smsc_params = list(size_net.parameters()) + list(material_net.parameters()) + list(shape_net.parameters())  + list(color_net.parameters()) \n\t    #create the SLASH Program\n\t    nnMapping = {'size': size_net,\n\t                 'material': material_net,\n\t                 'shape': shape_net,\n\t                 'color': color_net\n\t                }\n\t    #OPTIMIZERS\n", "    optimizers = {'smsc': torch.optim.Adam([\n\t                                        {'params':smsc_params}],\n\t                                        lr=exp_dict['lr'], eps=1e-7),\n\t                 'slot': torch.optim.Adam([\n\t                                        {'params': slot_net_params}],\n\t                                        lr=0.0004, eps=1e-7)}\n\t    SLASHobj = SLASH(program, nnMapping, optimizers)\n\t    SLASHobj.grad_comp_device ='cpu' #set gradient computation to cpu\n\t    print(\"using learning rate warmup and decay\")\n\t    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n", "    decay_epochs = exp_dict['lr_decay_steps']\n\t    slot_base_lr = 0.0004\n\t    #metric lists\n\t    test_ap_list = [] #stores average precsion values\n\t    test_metric_list = [] #stores tp, fp, tn values\n\t    lr_list = [] # store learning rate\n\t    loss_list = []  # store training loss\n\t    startTime = time.time()\n\t    train_test_times = []\n\t    sm_per_batch_list = []\n", "    forward_time_list = []\n\t    asp_time_list = []\n\t    gradient_time_list = []\n\t    backward_time_list = []\n\t    # Load data\n\t    obj_num = exp_dict['obj_num']\n\t    root = '/SLASH/data/CLEVR_v1.0/'\n\t    mode = 'train'\n\t    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n\t    train_dataset_loader = torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=True,batch_size=exp_dict['bs'],num_workers=8)\n", "    mode = 'val'\n\t    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n\t    test_dataset_loader= torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=False,batch_size=exp_dict['bs'], num_workers=8)\n\t    obj_encodings_gt = ap_utils.get_obj_encodings(test_dataset_loader)\n\t    print(\"loaded data\")\n\t    # Resume the training if requested\n\t    start_e= 0\n\t    if exp_dict['resume']:\n\t        print(\"resuming experiment\")\n\t        saved_model = torch.load(saveModelPath)\n", "        #load pytorch models\n\t        color_net.load_state_dict(saved_model['color_net'])\n\t        shape_net.load_state_dict(saved_model['shape_net'])\n\t        material_net.load_state_dict(saved_model['material_net'])\n\t        size_net.load_state_dict(saved_model['size_net'])\n\t        slot_net.load_state_dict(saved_model['slot_net'])\n\t        #optimizers and shedulers\n\t        optimizers['smsc'].load_state_dict(saved_model['resume']['optimizer_smsc'])\n\t        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n\t        start_e = saved_model['resume']['epoch']\n", "        #metrics\n\t        test_ap_list = saved_model['test_ap_list']\n\t        test_metric_list = saved_model['test_metric_list']\n\t        lr_list = saved_model['lr_list']\n\t        loss_list = saved_model['loss_list']\n\t        train_test_times = saved_model['train_test_times']\n\t        sm_per_batch_list = saved_model['sm_per_batch_list']  \n\t    # train the network and evaluate the performance\n\t    for e in range(start_e, exp_dict['epochs']):\n\t        #we have three datasets right now train, val and test with 20k, 5k and 100 samples\n", "        #TRAIN\n\t        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n\t        time_train = time.time()\n\t        #apply lr schedulers\n\t        if e < warmup_epochs:\n\t            lr = slot_base_lr * ((e+1)/warmup_epochs)\n\t        else:\n\t            lr = slot_base_lr\n\t        lr = lr * 0.5**((e+1)/decay_epochs)\n\t        optimizers['slot'].param_groups[0]['lr'] = lr\n", "        lr_list.append([lr,e])\n\t        print(\"LR SAm:\", \"{:.6f}\".format(lr), optimizers['slot'].param_groups[0]['lr'])\n\t        loss, forward_time, asp_time, gradient_time, backward_time, sm_per_batch  = SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net,hungarian_matching = True, method=exp_dict['method'], p_num=exp_dict['p_num'], k_num=1,\n\t                              epoch=e, writer = writer, batched_pass = True)\n\t        forward_time_list.append(forward_time)\n\t        asp_time_list.append(asp_time)\n\t        gradient_time_list.append(gradient_time)\n\t        backward_time_list.append(backward_time)\n\t        sm_per_batch_list.append(sm_per_batch)\n\t        loss_list.append(loss)\n", "        writer.add_scalar(\"train/loss\", loss, global_step=e)\n\t        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n\t        #TEST\n\t        time_test = time.time()\n\t        #forward test batch\n\t        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader)\n\t        #compute the average precision, tp, fp, tn for color+shape+material+size\n\t        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n\t        #use only a fraction of the test data, except every 100th epoch\n\t        if e % 100 == 0:\n", "            subset_size = None\n\t        else:\n\t            subset_size = 1500\n\t        ap, true_positives,false_positives, true_negatives, correctly_classified  = ap_utils.average_precision(pred, obj_encodings_gt, -1, \"CLEVR\",subset_size = subset_size)\n\t        print(\"avg precision \", ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\", correctly_classified)\n\t        '''\n\t        #color\n\t        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n\t        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c = ap_utils.average_precision(pred, obj_encodings_gt, -1, \"CLEVR\", only_color=True)\n\t        print(\"avg precision color\", ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\", correctly_classified_c)\n", "        #shape              \n\t        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n\t        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s = ap_utils.average_precision(pred_s, obj_encodings_gt, -1, \"CLEVR\", only_shape=True)\n\t        print(\"avg precision shape\", ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\", correctly_classified_s)\n\t        #material              \n\t        pred_m = ap_utils.inference_map_to_array(inference, only_material=True).cpu().numpy()\n\t        ap_m, true_positives_m, false_positives_m, true_negatives_m, correctly_classified_m = ap_utils.average_precision(pred_m, obj_encodings_gt, -1, \"CLEVR\", only_material=True)\n\t        print(\"avg precision material\", ap_m, \"tp\", true_positives_m, \"fp\", false_positives_m, \"tn\", true_negatives_m, \"correctly classified\", correctly_classified_m)\n\t        #size              \n\t        pred_x = ap_utils.inference_map_to_array(inference, only_size=True).cpu().numpy()\n", "        ap_x, true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x = ap_utils.average_precision(pred_x, obj_encodings_gt, -1, \"CLEVR\", only_size=True)\n\t        print(\"avg precision size\", ap_x, \"tp\", true_positives_x, \"fp\", false_positives_x, \"tn\", true_negatives_x, \"correctly classified\", correctly_classified_x)\n\t        #store ap, tp, fp, tn\n\t        test_ap_list.append([ap, ap_c, ap_s, ap_m, ap_x, e])                    \n\t        test_metric_list.append([true_positives, false_positives, true_negatives, correctly_classified,\n\t                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n\t                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s,\n\t                                 true_positives_m, false_positives_m, true_negatives_m, correctly_classified_m,\n\t                                 true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x])\n\t        '''\n", "        if wb:\n\t            wandb.log({\"ap\": ap,\n\t                        \"loss\":loss,\n\t                        \"forward_time\":np.sum(forward_time),\n\t                        \"asp_time\":np.sum(asp_time),\n\t                        \"gradient_time\": np.sum(gradient_time),\n\t                        \"backward_time\": np.sum(backward_time),\n\t                        \"sm_per_batch\": np.sum(sm_per_batch)})\n\t        test_ap_list.append([ap, e])                    \n\t        test_metric_list.append([true_positives, false_positives, true_negatives, correctly_classified])\n", "        #Tensorboard outputs\n\t        writer.add_scalar(\"test/ap\", ap, global_step=e)\n\t        #writer.add_scalar(\"test/ap_c\", ap_c, global_step=e)\n\t        #writer.add_scalar(\"test/ap_s\", ap_s, global_step=e)\n\t        #writer.add_scalar(\"test/ap_m\", ap_m, global_step=e)\n\t        #writer.add_scalar(\"test/ap_x\", ap_x, global_step=e)\n\t        #Time measurements\n\t        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n\t        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n\t        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n", "        train_test_times_np = np.array(train_test_times)\n\t        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n\t        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n\t        print('--- total time from beginning:  ---', timestamp_total )\n\t        #save the neural network  such that we can use it later\n\t        print('Storing the trained model into {}'.format(saveModelPath))\n\t        torch.save({\"color_net\":color_net.state_dict(),\n\t                    \"shape_net\":shape_net.state_dict(),                    \n\t                    \"material_net\":material_net.state_dict(),\n\t                    \"size_net\":size_net.state_dict(),\n", "                    \"slot_net\":slot_net.state_dict(),\n\t                    \"resume\": {\n\t                        \"optimizer_smsc\":optimizers['smsc'].state_dict(),\n\t                        \"optimizer_slot\":optimizers['slot'].state_dict(),\n\t                        \"epoch\":e\n\t                    },\n\t                    \"test_ap_list\":test_ap_list,\n\t                    \"loss_list\":loss_list,                    \n\t                    \"sm_per_batch_list\":sm_per_batch_list,\n\t                    \"test_metric_list\":test_metric_list,\n", "                    \"lr_list\":lr_list,\n\t                    \"num_params\":num_params,\n\t                    \"train_test_times\": train_test_times,\n\t                    \"time\": {\n\t                        \"forward_time\":forward_time_list,\n\t                        \"asp_time\": asp_time_list,\n\t                        \"gradient_time\":gradient_time_list,\n\t                        \"backward_time\":backward_time_list,\n\t                    },\n\t                    \"exp_dict\":exp_dict,\n", "                    \"program\":program}, saveModelPath)\n\t        # Update the RTPT\n\t        rtpt.step()\n\t    if wb:\n\t        wandb.finish()\n"]}
{"filename": "src/experiments/slash_attention/clevr/__init__.py", "chunked_list": []}
{"filename": "src/experiments/slash_attention/clevr/dataGen.py", "chunked_list": ["import torch\n\timport torchvision\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.transforms import transforms\n\t# from torch.utils.data import Dataset\n\t# from torchvision import transforms\n\tfrom skimage import io\n\timport os\n\timport numpy as np\n\timport torch\n", "import matplotlib.pyplot as plt\n\tfrom PIL import ImageFile\n\tImageFile.LOAD_TRUNCATED_IMAGES = True\n\timport json\n\tfrom tqdm import tqdm\n\tdef object_encoding(size, material, shape, color ):\n\t    #size (small, large, bg)\n\t    if size == \"small\":\n\t        size_enc = [1,0,0]\n\t    elif size == \"large\":\n", "        size_enc = [0,1,0]\n\t    elif size == \"bg\":\n\t        size_enc = [0,0,1]\n\t    #material (rubber, metal, bg)\n\t    if material == \"rubber\":\n\t        material_enc = [1,0,0]\n\t    elif material == \"metal\":\n\t        material_enc = [0,1,0]\n\t    elif material == \"bg\":\n\t        material_enc = [0,0,1]\n", "    #shape (cube, sphere, cylinder, bg)\n\t    if shape == \"cube\":\n\t        shape_enc = [1,0,0,0]\n\t    elif shape == \"sphere\":\n\t        shape_enc = [0,1,0,0]\n\t    elif shape == \"cylinder\":\n\t        shape_enc = [0,0,1,0]\n\t    elif shape == \"bg\":\n\t        shape_enc = [0,0,0,1]\n\t    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n", "    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n\t    #color  1      2     3     4     5     6        7     8      9\n\t    if color == \"gray\":\n\t        color_enc = [1,0,0,0,0,0,0,0,0]\n\t    elif color == \"red\":\n\t        color_enc = [0,1,0,0,0,0,0,0,0]\n\t    elif color == \"blue\":\n\t        color_enc = [0,0,1,0,0,0,0,0,0]\n\t    elif color == \"green\":\n\t        color_enc = [0,0,0,1,0,0,0,0,0]\n", "    elif color == \"brown\":\n\t        color_enc = [0,0,0,0,1,0,0,0,0]\n\t    elif color == \"purple\":\n\t        color_enc = [0,0,0,0,0,1,0,0,0]\n\t    elif color == \"cyan\":\n\t        color_enc = [0,0,0,0,0,0,1,0,0]\n\t    elif color == \"yellow\":\n\t        color_enc = [0,0,0,0,0,0,0,1,0]\n\t    elif color == \"bg\":\n\t        color_enc = [0,0,0,0,0,0,0,0,1]\n", "    return size_enc + material_enc + shape_enc + color_enc +[1]\n\tclass CLEVR(Dataset):\n\t    def __init__(self, root, mode, img_paths=None, files_names=None, obj_num=None):\n\t        self.root = root  # The root folder of the dataset\n\t        self.mode = mode  # The mode of 'train' or 'val'\n\t        self.files_names = files_names # The list of the files names with correct nuber of objects\n\t        if obj_num is not None:\n\t            self.obj_num = obj_num  # The upper limit of number of objects \n\t        else:\n\t            self.obj_num = 10\n", "        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\t        #list of sorted image paths\n\t        self.img_paths = []\n\t        if img_paths:\n\t            self.img_paths = img_paths\n\t        else:                    \n\t            #open directory and save all image paths\n\t            for file in os.scandir(os.path.join(root, 'images', mode)):\n\t                img_path = file.path\n\t                if '.png' in img_path:\n", "                    self.img_paths.append(img_path)\n\t        self.img_paths.sort()\n\t        count = 0\n\t        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n\t        self.query_map = {}\n\t        self.obj_map = {}\n\t        count = 0        \n\t        #We have up to 10 objects in the image, load the json file\n\t        with open(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\")) as f:\n\t            data = json.load(f)\n", "            #iterate over each scene and create the query string and obj encoding\n\t            print(\"parsing scences\")\n\t            for scene in tqdm(data['scenes']):\n\t                target_query = \"\"\n\t                obj_encoding_list = []\n\t                if self.files_names:\n\t                    if any(scene['image_filename'] in file_name for file_name in files_names):                    \n\t                        num_objects = 0\n\t                        for idx, obj in enumerate(scene['objects']):\n\t                            target_query += \" :- not object(o{}, {}, {}, {}, {}).\".format(idx+1, obj['size'], obj['material'], obj['shape'], obj['color'])\n", "                            obj_encoding_list.append(object_encoding(obj['size'], obj['material'], obj['shape'], obj['color']))\n\t                            num_objects = idx+1 #store the num of objects \n\t                        #fill in background objects\n\t                        for idx in range(num_objects, self.obj_num):\n\t                            target_query += \" :- not object(o{}, bg, bg, bg, bg).\".format(idx+1)\n\t                            obj_encoding_list.append([0,0,1, 0,0,1, 0,0,0,1, 0,0,0,0,0,0,0,0,1, 1])\n\t                        self.query_map[count] = target_query\n\t                        self.obj_map[count] = np.array(obj_encoding_list)\n\t                        count += 1\n\t                else:\n", "                    num_objects=0\n\t                    for idx, obj in enumerate(scene['objects']):\n\t                        target_query += \" :- not object(o{}, {}, {}, {}, {}).\".format(idx+1, obj['size'], obj['material'], obj['shape'], obj['color'])\n\t                        obj_encoding_list.append(object_encoding(obj['size'], obj['material'], obj['shape'], obj['color']))\n\t                        num_objects = idx+1 #store the num of objects \n\t                    #fill in background objects\n\t                    for idx in range(num_objects, 10):\n\t                        target_query += \" :- not object(o{}, bg, bg, bg, bg).\".format(idx+1)\n\t                        obj_encoding_list.append([0,0,1, 0,0,1, 0,0,0,1, 0,0,0,0,0,0,0,0,1, 1])\n\t                    self.query_map[scene['image_index']] = target_query\n", "                    self.obj_map[scene['image_index']] = np.array(obj_encoding_list)\n\t            print(\"done\")\n\t        if self.files_names:\n\t            print(f'Correctly found images {count} out of {len(files_names)}')\n\t        #print(np.array(list(self.obj_map.values()))[0:20])\n\t    def __getitem__(self, index):\n\t        #get the image\n\t        img_path = self.img_paths[index]\n\t        img = io.imread(img_path)[:, :, :3]\n\t        transform = transforms.Compose([\n", "            transforms.ToPILImage(),\n\t            #transforms.CenterCrop((29, 221,64, 256)), #why do we need to crop?\n\t            transforms.Resize((128, 128)),\n\t            transforms.ToTensor(),\n\t        ])\n\t        img = transform(img)\n\t        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\t        return {'im':img}, self.query_map[index] ,self.obj_map[index]\n\t    def __len__(self):\n\t        return len(self.img_paths)\n"]}
{"filename": "src/experiments/slash_attention/clevr/slash_attention_clevr.py", "chunked_list": ["#!/usr/bin/env python\n\t# coding: utf-8\n\timport train\n\timport datetime\n\tseed = 0\n\tobj_num = 10\n\tdate_string = datetime.datetime.today().strftime('%d-%m-%Y')\n\texperiments = {f'CLEVR{obj_num}': {'structure':'poon-domingos', 'pd_num_pieces':[4], 'learn_prior':True,\n\t                         'lr': 0.01, 'bs':512, 'epochs':1000,\n\t                        'lr_warmup_steps':8, 'lr_decay_steps':360, 'use_em':False, 'resume':False,\n", "                        'method':'most_prob',\n\t                         'start_date':date_string, 'credentials':'DO', 'p_num':16, 'seed':seed, 'obj_num':obj_num\n\t                        }}\n\tfor exp_name in experiments:\n\t    print(exp_name)\n\t    train.slash_slot_attention(exp_name, experiments[exp_name])\n"]}
{"filename": "src/experiments/slash_attention/clevr/auxiliary.py", "chunked_list": ["import json\n\timport os\n\tfrom pathlib import Path\n\tdef get_files_names_and_paths(root:str='./data/CLEVR_v1.0/', mode:str='val', obj_num:int=4):\n\t    data_file = Path(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\"))\n\t    data_file.parent.mkdir(parents=True, exist_ok=True)\n\t    if data_file.exists():\n\t        print('File exists. Parsing file...')\n\t    else:\n\t        print(f'The JSON file {data_file} does not exist!')\n", "        quit()\n\t    img_paths = []\n\t    files_names = []\n\t    with open(data_file, 'r') as json_file:\n\t        json_data = json.load(json_file)\n\t        for scene in json_data['scenes']:\n\t            if len(scene['objects']) <= obj_num:\n\t                img_paths.append(Path(os.path.join(root,'images/'+mode+'/'+scene['image_filename'])))\n\t                files_names.append(scene['image_filename'])\n\t    print(\"...done \")\n", "    return img_paths, files_names\n\tdef get_slash_program(obj_num:int=4):\n\t    program = ''\n\t    if obj_num == 10:\n\t        program ='''\n\t    slot(s1).\n\t    slot(s2).\n\t    slot(s3).\n\t    slot(s4).\n\t    slot(s5).\n", "    slot(s6).\n\t    slot(s7).\n\t    slot(s8).\n\t    slot(s9).\n\t    slot(s10).\n\t    name(o1).\n\t    name(o2).\n\t    name(o3).\n\t    name(o4).\n\t    name(o5).\n", "    name(o6).\n\t    name(o7).\n\t    name(o8).\n\t    name(o9).\n\t    name(o10).\n\t        '''\n\t    elif obj_num ==4:\n\t        program ='''\n\t    slot(s1).\n\t    slot(s2).\n", "    slot(s3).\n\t    slot(s4).\n\t    name(o1).\n\t    name(o2).\n\t    name(o3).\n\t    name(o4).\n\t        '''\n\t    elif obj_num ==6:\n\t        program ='''\n\t    slot(s1).\n", "    slot(s2).\n\t    slot(s3).\n\t    slot(s4).\n\t    slot(s5).\n\t    slot(s6).\n\t    name(o1).\n\t    name(o2).\n\t    name(o3).\n\t    name(o4).\n\t    name(o5).\n", "    name(o6).\n\t        '''\n\t    else:\n\t        print(f'The number of objects {obj_num} is wrong!')\n\t        quit()\n\t    program +='''        \n\t    %assign each name a slot\n\t    %{slot_name_comb(N,X): slot(X)}=1 :- name(N). %problem we have dublicated slots\n\t    %remove each model which has multiple slots asigned to the same name\n\t    %:- slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2. \n", "    %build the object ontop of the slot assignment\n\t    object(N, S, M, P, C) :- size(0, +X, -S), material(0, +X, -M), shape(0, +X, -P), color(0, +X, -C), slot(X), name(N), slot_name_comb(N,X).\n\t    %define the SPNs\n\t    npp(size(1,X),[small, large, bg]) :- slot(X).\n\t    npp(material(1,X),[rubber, metal, bg]) :- slot(X).\n\t    npp(shape(1,X),[cube, sphere, cylinder, bg]) :- slot(X).\n\t    npp(color(1,X),[gray, red, blue, green, brown, purple, cyan, yellow, bg]) :- slot(X).\n\t    '''\n\t    return program"]}
{"filename": "src/experiments/slash_attention/shapeworld4/slash_attention_shapeworld4.py", "chunked_list": ["#!/usr/bin/env python\n\t# coding: utf-8\n\timport train\n\timport datetime\n\t#Python script to start the shapeworld4 slot attention experiment\n\t#Define your experiment(s) parameters as a hashmap having the following parameters\n\texample_structure = {'experiment_name': \n\t                   {'structure': 'poon-domingos',\n\t                    'pd_num_pieces': [4],\n\t                    'lr': 0.01, #the learning rate to train the SPNs with, the slot attention module has a fixed lr=0.0004  \n", "                    'bs':50, #the batchsize\n\t                    'epochs':1000, #number of epochs to train\n\t                    'lr_warmup':True, #boolean indicating the use of learning rate warm up\n\t                    'lr_warmup_steps':25, #number of epochs to warm up the slot attention module, warmup does not apply to the SPNs\n\t                    'start_date':\"01-01-0001\", #current date\n\t                    'resume':False, #you can stop the experiment and set this parameter to true to load the last state and continue learning\n\t                    'credentials':'DO', #your credentials for the rtpt class\n\t                    'hungarian_matching': True,\n\t                    'explanation': \"\"\"Running the whole SlotAttention+Slash pipeline using poon-domingos as SPN structure learner.\"\"\"}}\n\t#EXPERIMENTS\n", "date_string = datetime.datetime.today().strftime('%d-%m-%Y')\n\tfor seed in [0,1,2,3,4]:\n\t    experiments = {'shapeworld4': \n\t                   {'structure': 'poon-domingos', 'pd_num_pieces': [4],\n\t                    'lr': 0.01, 'bs':512, 'epochs':1000, \n\t                    'lr_warmup_steps':8, 'lr_decay_steps':360,\n\t                    'start_date':date_string, 'resume':False, 'credentials':'DO','seed':seed,\n\t                    'p_num':16, 'method':'same_top_k', 'hungarian_matching': False,\n\t                    'explanation': \"\"\"Running the whole SlotAttention+Slash pipeline using poon-domingos as SPN structure learner.\"\"\"}\n\t                    }\n", "    print(\"shapeworld4\")\n\t    train.slash_slot_attention(\"shapeworld4\", experiments[\"shapeworld4\"])\n"]}
{"filename": "src/experiments/slash_attention/shapeworld4/train.py", "chunked_list": ["print(\"start importing...\")\n\timport time\n\timport sys\n\timport os\n\timport json\n\twb = True\n\timport wandb\n\tsys.path.append('../../../')\n\tsys.path.append('../../../SLASH/')\n\tsys.path.append('../../../EinsumNetworks/src/')\n", "#torch, numpy, ...\n\timport torch\n\tfrom torch.utils.tensorboard import SummaryWriter\n\timport numpy as np\n\t#own modules\n\tfrom dataGen import SHAPEWORLD4\n\tfrom einsum_wrapper import EiNet\n\tfrom slash import SLASH\n\timport utils\n\timport ap_utils\n", "from utils import set_manual_seed\n\tfrom slot_attention_module import SlotAttention_model\n\tfrom pathlib import Path\n\tfrom rtpt import RTPT\n\tprint(\"...done\")\n\tdef slash_slot_attention(exp_name , exp_dict):\n\t    program =\"\"\"\n\t    slot(s1).\n\t    slot(s2).\n\t    slot(s3).\n", "    slot(s4).\n\t    name(o1).\n\t    name(o2).\n\t    name(o3).\n\t    name(o4).\n\t    %assign each name a slot\n\t    %build the object ontop of the slot assignment\n\t    object(N,C,S,H,Z) :- color(0, +X, -C), shape(0, +X, -S), shade(0, +X, -H), size(0, +X, -Z), slot(X), name(N), slot_name_comb(N,X).\n\t    npp(shade(1,X),[bright, dark, bg]) :- slot(X).\n\t    npp(color(1,X),[red, blue, green, gray, brown, magenta, cyan, yellow, black]) :- slot(X).\n", "    npp(shape(1,X),[circle, triangle, square, bg]) :- slot(X).\n\t    npp(size(1,X),[small, big, bg]) :- slot(X).\n\t    \"\"\"\n\t    #solve the matching problem in the logic program\n\t    if exp_dict['hungarian_matching'] == False:\n\t        program += \"\"\"\n\t        %assign each name a slot\n\t        {slot_name_comb(N,X): slot(X) }=1 :- name(N). %problem we have dublicated slots\n\t        %remove each model which has multiple slots asigned to the same name\n\t        %:-  slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2.\n", "        %remove the duplicates\n\t        {slot_name_comb(N,X): name(N) }=1 :- slot(X). \n\t        \"\"\"\n\t    # Set the seeds for PRNG\n\t    set_manual_seed(exp_dict['seed'])\n\t    # Create RTPT object\n\t    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name='SLASH Attention Shapeworld4', max_iterations=int(exp_dict['epochs']))\n\t    # Start the RTPT tracking\n\t    rtpt.start()\n\t    # create save paths and tensorboard writer\n", "    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n\t    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n\t    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n\t    if wb:\n\t        #start a new wandb run to track this script\n\t        wandb.init(\n\t        # set the wandb project where this run will be logged\n\t        project=\"slash_attention\",\n\t        # track hyperparameters and run metadata\n\t        config=exp_dict)\n", "    # save args\n\t    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n\t        json.dump(exp_dict, json_file, indent=4)\n\t    print(\"Experiment parameters:\", exp_dict)\n\t    #NETWORKS\n\t    if exp_dict['structure'] == 'poon-domingos':\n\t        exp_dict['depth'] = None\n\t        exp_dict['num_repetitions'] = None\n\t        print(\"using poon-domingos\")\n\t    elif exp_dict['structure'] == 'binary-trees':\n", "        exp_dict['pd_num_pieces'] = None\n\t        print(\"using binary-trees\")\n\t    #color network\n\t    color_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 32,\n\t        class_count=9,\n\t        pd_width=8,pd_height=4,\n", "        use_em= False)\n\t    #shape network\n\t    shape_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 32,\n\t        class_count=4,\n\t        pd_width=8,pd_height=4,\n\t        use_em= False)\n", "    #shade network\n\t    shade_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 32,\n\t        class_count=3,\n\t        pd_width=8,pd_height=4,\n\t        use_em= False)\n\t    #size network\n", "    size_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 32,\n\t        class_count=3,\n\t        pd_width=8,pd_height=4,\n\t        use_em= False)\n\t    #create the Slot Attention network\n\t    slot_net = SlotAttention_model(n_slots=4, n_iters=3, n_attr=18,\n", "                                encoder_hidden_channels=32, attention_hidden_channels=64)\n\t    slot_net = slot_net.to(device='cuda')\n\t    #count trainable params\n\t    num_trainable_params = [sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in shade_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n\t    num_params = [sum(p.numel() for p in color_net.parameters()), \n\t                  sum(p.numel() for p in shape_net.parameters()),\n", "                  sum(p.numel() for p in shade_net.parameters()),\n\t                  sum(p.numel() for p in size_net.parameters()),\n\t                  sum(p.numel() for p in slot_net.parameters())]\n\t    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n\t    slot_net_params = list(slot_net.parameters())\n\t    csss_params = list(color_net.parameters()) + list(shape_net.parameters()) + list(shade_net.parameters()) + list(size_net.parameters())\n\t    #create the SLASH Program\n\t    nnMapping = {'color': color_net,\n\t                 'shape':shape_net,\n\t                 'shade':shade_net,\n", "                 'size':size_net}\n\t    #OPTIMIZERS\n\t    optimizers = {'csss': torch.optim.Adam([\n\t                                            {'params':csss_params}],\n\t                                            lr=exp_dict['lr'], eps=1e-7),\n\t                 'slot': torch.optim.Adam([\n\t                                            {'params': slot_net_params}],\n\t                                            lr=0.0004, eps=1e-7)}\n\t    SLASHobj = SLASH(program, nnMapping, optimizers)\n\t    SLASHobj.grad_comp_device ='cpu' #set gradient computation to cpu\n", "    print(\"using learning rate warmup and decay\")\n\t    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n\t    decay_epochs = exp_dict['lr_decay_steps']\n\t    slot_base_lr = 0.0004\n\t    #metric lists\n\t    test_ap_list = [] #stores average precsion values\n\t    test_metric_list = [] #stores tp, fp, tn values\n\t    lr_list = [] # store learning rate\n\t    loss_list = []  # store training loss\n\t    startTime = time.time()\n", "    train_test_times = []\n\t    sm_per_batch_list = []\n\t    forward_time_list = []\n\t    asp_time_list = []\n\t    gradient_time_list = []\n\t    backward_time_list = []\n\t    print(\"loading data...\")\n\t    #if the hungarian matching algorithm is used we need to pass the object encodings to SLASH\n\t    if exp_dict['hungarian_matching']:\n\t        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"train\", ret_obj_encoding=True), shuffle=True,batch_size=exp_dict['bs'], num_workers=8)\n", "    else:\n\t        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"train\"), shuffle=True,batch_size=exp_dict['bs'], num_workers=8)\n\t    test_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"val\", ret_obj_encoding=True), shuffle=False,batch_size=exp_dict['bs'], num_workers=8)\n\t    obj_encodings_gt = ap_utils.get_obj_encodings(test_dataset_loader)\n\t    print(\"...done\")\n\t    start_e= 0\n\t    if exp_dict['resume']:\n\t        print(\"resuming experiment\")\n\t        saved_model = torch.load(saveModelPath)\n\t        #load pytorch models\n", "        color_net.load_state_dict(saved_model['color_net'])\n\t        shape_net.load_state_dict(saved_model['shape_net'])\n\t        shade_net.load_state_dict(saved_model['shade_net'])\n\t        size_net.load_state_dict(saved_model['size_net'])\n\t        slot_net.load_state_dict(saved_model['slot_net'])\n\t        #optimizers and shedulers\n\t        optimizers['csss'].load_state_dict(saved_model['resume']['optimizer_csss'])\n\t        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n\t        start_e = saved_model['resume']['epoch']\n\t        #metrics\n", "        test_ap_list = saved_model['test_ap_list']\n\t        test_metric_list = saved_model['test_metric_list']\n\t        lr_list = saved_model['lr_list']\n\t        loss_list = saved_model['loss_list']\n\t        train_test_times = saved_model['train_test_times']\n\t        sm_per_batch_list = saved_model['sm_per_batch_list']\n\t    for e in range(start_e, exp_dict['epochs']):\n\t        #TRAIN\n\t        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n\t        time_train= time.time()\n", "        #apply lr schedulers to the SAm\n\t        if e < warmup_epochs:\n\t            lr = slot_base_lr * ((e+1)/warmup_epochs)\n\t        else:\n\t            lr = slot_base_lr\n\t        lr = lr * 0.5**((e+1)/decay_epochs)\n\t        optimizers['slot'].param_groups[0]['lr'] = lr\n\t        lr_list.append([lr,e])\n\t        print(\"LR SAm:\", \"{:.6f}\".format(lr), optimizers['slot'].param_groups[0]['lr'])\n\t        #SLASH training\n", "        loss, forward_time, asp_time, gradient_time, backward_time, sm_per_batch= SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net, hungarian_matching=exp_dict['hungarian_matching'], method=exp_dict['method'], p_num = exp_dict['p_num'], k_num=1, batched_pass = True,\n\t                        epoch=e, writer = writer)\n\t        forward_time_list.append(forward_time)\n\t        asp_time_list.append(asp_time)\n\t        gradient_time_list.append(gradient_time)\n\t        backward_time_list.append(backward_time)\n\t        sm_per_batch_list.append(sm_per_batch)\n\t        loss_list.append(loss)\n\t        writer.add_scalar(\"train/loss\", loss, global_step=e)\n\t        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n", "        #TEST\n\t        time_test = time.time()\n\t        #forward test batch\n\t        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader)                          \n\t        #compute the average precision, tp, fp, tn for color+shape+shade+size, color, shape, shade, size\n\t        #color+shape+shade+size\n\t        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n\t        ap, true_positives,false_positives, true_negatives, correctly_classified = ap_utils.average_precision(pred, obj_encodings_gt,-1, \"SHAPEWORLD4\")\n\t        print(\"avg precision \",ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\",correctly_classified)\n\t        #color\n", "        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n\t        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c= ap_utils.average_precision(pred, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_color = True)\n\t        print(\"avg precision color\",ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\",correctly_classified_c)\n\t        #shape              \n\t        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n\t        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s= ap_utils.average_precision(pred_s, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_shape = True)\n\t        print(\"avg precision shape\",ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\",correctly_classified_s)\n\t        #shade              \n\t        pred_h = ap_utils.inference_map_to_array(inference, only_shade=True).cpu().numpy()\n\t        ap_h, true_positives_h, false_positives_h, true_negatives_h, correctly_classified_h= ap_utils.average_precision(pred_h, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_shade = True)\n", "        print(\"avg precision shade\",ap_h, \"tp\", true_positives_h, \"fp\", false_positives_h, \"tn\", true_negatives_h, \"correctly classified\",correctly_classified_h)\n\t        #shape              \n\t        pred_x = ap_utils.inference_map_to_array(inference, only_size=True).cpu().numpy()\n\t        ap_x, true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x= ap_utils.average_precision(pred_x, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_size = True)\n\t        print(\"avg precision size\",ap_x, \"tp\", true_positives_x, \"fp\", false_positives_x, \"tn\", true_negatives_x, \"correctly classified\",correctly_classified_x)\n\t        #store ap, tp, fp, tn\n\t        test_ap_list.append([ap, ap_c, ap_s, ap_h, ap_x, e])                    \n\t        test_metric_list.append([true_positives, false_positives, true_negatives,correctly_classified,\n\t                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n\t                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s,\n", "                                 true_positives_h, false_positives_h, true_negatives_h, correctly_classified_h,\n\t                                 true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x])\n\t        #Tensorboard outputs\n\t        writer.add_scalar(\"test/ap\", ap, global_step=e)\n\t        writer.add_scalar(\"test/ap_c\", ap_c, global_step=e)\n\t        writer.add_scalar(\"test/ap_s\", ap_s, global_step=e)\n\t        writer.add_scalar(\"test/ap_h\", ap_h, global_step=e)\n\t        writer.add_scalar(\"test/ap_x\", ap_x, global_step=e)\n\t        if wb:\n\t            wandb.log({\"ap\": ap,\n", "                        \"ap_c\":ap_c,\n\t                        \"ap_s\":ap_s,\n\t                        \"ap_h\":ap_h,\n\t                        \"ap_x\":ap_x,\n\t                        \"loss\":loss,\n\t                        \"forward_time\":forward_time,\n\t                        \"asp_time\":asp_time,\n\t                        \"gradient_time\": gradient_time,\n\t                        \"backward_time\": backward_time,\n\t                        \"sm_per_batch\": sm_per_batch})\n", "        #Time measurements\n\t        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n\t        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n\t        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n\t        train_test_times_np = np.array(train_test_times)\n\t        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n\t        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n\t        print('--- total time from beginning:  ---', timestamp_total )\n\t        #save the neural network  such that we can use it later\n\t        print('Storing the trained model into {}'.format(saveModelPath))\n", "        torch.save({\"shape_net\":  shape_net.state_dict(), \n\t                    \"color_net\": color_net.state_dict(),\n\t                    \"shade_net\": shade_net.state_dict(),\n\t                    \"size_net\": size_net.state_dict(),\n\t                    \"slot_net\": slot_net.state_dict(),\n\t                    \"resume\": {\n\t                        \"optimizer_csss\":optimizers['csss'].state_dict(),\n\t                        \"optimizer_slot\": optimizers['slot'].state_dict(),\n\t                        \"epoch\":e\n\t                    },\n", "                    \"test_ap_list\":test_ap_list,\n\t                    \"loss_list\":loss_list,\n\t                    \"sm_per_batch_list\":sm_per_batch_list,\n\t                    \"test_metric_list\":test_metric_list,\n\t                    \"lr_list\": lr_list,\n\t                    \"num_params\": num_params,\n\t                    \"train_test_times\": train_test_times,\n\t                    \"exp_dict\":exp_dict,\n\t                    \"time\": {\n\t                        \"forward_time\":forward_time_list,\n", "                        \"asp_time\": asp_time_list,\n\t                        \"gradient_time\":gradient_time_list,\n\t                        \"backward_time\":backward_time_list,\n\t                    },\n\t                    \"program\":program}, saveModelPath)\n\t        # Update the RTPT\n\t        rtpt.step(subtitle=f\"ap={ap:2.2f}\")\n\t    if wb:\n\t        wandb.finish()\n"]}
{"filename": "src/experiments/slash_attention/shapeworld4/__init__.py", "chunked_list": []}
{"filename": "src/experiments/slash_attention/shapeworld4/dataGen.py", "chunked_list": ["import torch\n\timport torchvision\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.transforms import transforms\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision import transforms\n\tfrom skimage import io\n\timport os\n\timport numpy as np\n\timport torch\n", "from PIL import  ImageFile\n\tImageFile.LOAD_TRUNCATED_IMAGES = True\n\timport json\n\timport datasets\n\tdef get_encoding(color, shape, shade, size):\n\t    if color == 'red':\n\t        col_enc = [1,0,0,0,0,0,0,0,0]\n\t    elif color == 'blue':\n\t        col_enc = [0,1,0,0,0,0,0,0,0]\n\t    elif color == 'green':\n", "        col_enc = [0,0,1,0,0,0,0,0,0]\n\t    elif color == 'gray':\n\t        col_enc = [0,0,0,1,0,0,0,0,0]\n\t    elif color == 'brown':\n\t        col_enc = [0,0,0,0,1,0,0,0,0]\n\t    elif color == 'magenta':\n\t        col_enc = [0,0,0,0,0,1,0,0,0]\n\t    elif color == 'cyan':\n\t        col_enc = [0,0,0,0,0,0,1,0,0]\n\t    elif color == 'yellow':\n", "        col_enc = [0,0,0,0,0,0,0,1,0]\n\t    elif color == 'black':\n\t        col_enc = [0,0,0,0,0,0,0,0,1]\n\t    if shape == 'circle':\n\t        shape_enc = [1,0,0,0]\n\t    elif shape == 'triangle':\n\t        shape_enc = [0,1,0,0]\n\t    elif shape == 'square':\n\t        shape_enc = [0,0,1,0]    \n\t    elif shape == 'bg':\n", "        shape_enc = [0,0,0,1]\n\t    if shade == 'bright':\n\t        shade_enc = [1,0,0]\n\t    elif shade =='dark':\n\t        shade_enc = [0,1,0]\n\t    elif shade == 'bg':\n\t        shade_enc = [0,0,1]\n\t    if size == 'small':\n\t        size_enc = [1,0,0]\n\t    elif size == 'big':\n", "        size_enc = [0,1,0]\n\t    elif size == 'bg':\n\t        size_enc = [0,0,1]\n\t    return col_enc + shape_enc + shade_enc + size_enc + [1]\n\tclass SHAPEWORLD4(Dataset):\n\t    def __init__(self, root, mode, ret_obj_encoding=False):\n\t        datasets.maybe_download_shapeworld4()\n\t        self.ret_obj_encoding = ret_obj_encoding\n\t        self.root = root\n\t        self.mode = mode\n", "        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\t        #dictionary of the form {'image_idx':'img_path'}\n\t        self.img_paths = {}\n\t        for file in os.scandir(os.path.join(root, 'images', mode)):\n\t            img_path = file.path\n\t            img_path_idx =   img_path.split(\"/\")\n\t            img_path_idx = img_path_idx[-1]\n\t            img_path_idx = img_path_idx[:-4][6:]\n\t            try:\n\t                img_path_idx =  int(img_path_idx)\n", "                self.img_paths[img_path_idx] = img_path\n\t            except:\n\t                print(\"path:\",img_path_idx)\n\t        count = 0\n\t        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n\t        self.query_map = {}\n\t        self.obj_map = {}\n\t        with open(os.path.join(root, 'labels', mode,\"world_model.json\")) as f:\n\t            worlds = json.load(f)\n\t            #iterate over all objects\n", "            for world in worlds:\n\t                num_objects = 0\n\t                target_query = \"\"\n\t                obj_enc = []\n\t                for entity in world['entities']:\n\t                    color = entity['color']['name']\n\t                    shape = entity['shape']['name']\n\t                    shade_val = entity['color']['shade']\n\t                    if shade_val == 0.0:\n\t                        shade = 'bright'\n", "                    else:\n\t                        shade = 'dark'\n\t                    size_val = entity['shape']['size']['x']\n\t                    if size_val == 0.075:\n\t                        size = 'small'\n\t                    elif size_val == 0.15:\n\t                        size = 'big'\n\t                    name = 'o' + str(num_objects+1)\n\t                    target_query = target_query+ \":- not object({},{},{},{},{}). \".format(name, color, shape, shade, size)\n\t                    obj_enc.append(get_encoding(color, shape, shade, size))\n", "                    num_objects += 1\n\t                #bg encodings\n\t                for i in range(num_objects, 4):\n\t                    name = 'o' + str(num_objects+1)\n\t                    target_query = target_query+ \":- not object({},black,bg, bg, bg). \".format(name)\n\t                    obj_enc.append(get_encoding(\"black\",\"bg\",\"bg\",\"bg\"))\n\t                    num_objects += 1\n\t                self.query_map[count] = target_query\n\t                self.obj_map[count] = np.array(obj_enc)\n\t                count+=1\n", "    def __getitem__(self, index):\n\t        #get the image\n\t        img_path = self.img_paths[index]\n\t        img = io.imread(img_path)[:, :, :3]\n\t        transform = transforms.Compose([\n\t            transforms.ToPILImage(),\n\t            transforms.ToTensor(),\n\t        ])\n\t        img = transform(img)\n\t        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n", "        if self.ret_obj_encoding:\n\t            return {'im':img}, self.query_map[index] ,self.obj_map[index]\n\t        else:\n\t            return {'im':img}, self.query_map[index]\n\t    def __len__(self):\n\t        return len(self.img_paths)\n"]}
{"filename": "src/experiments/slash_attention/clevr_cogent/train.py", "chunked_list": ["print(\"start importing...\")\n\timport time\n\timport sys\n\timport os\n\timport json\n\tsys.path.append('../../../')\n\tsys.path.append('../../../SLASH/')\n\tsys.path.append('../../../EinsumNetworks/src/')\n\t#torch, numpy, ...\n\timport torch\n", "from torch.utils.tensorboard import SummaryWriter\n\ttorch.cuda.empty_cache()\n\timport numpy as np\n\timport importlib\n\t#own modules\n\tfrom dataGen import CLEVR\n\tfrom auxiliary import get_files_names_and_paths, get_slash_program\n\tfrom einsum_wrapper import EiNet\n\tfrom slash import SLASH\n\timport utils\n", "import ap_utils\n\tfrom utils import set_manual_seed\n\tfrom slot_attention_module import SlotAttention_model\n\tfrom slot_attention_module import SlotAttention_model\n\tfrom pathlib import Path\n\tfrom rtpt import RTPT\n\tprint(\"...done\")\n\tdef slash_slot_attention(exp_name , exp_dict):\n\t    # Set the seeds for PRNG\n\t    set_manual_seed(exp_dict['seed'])\n", "    # Create RTPT object\n\t    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name=f'SLASH Attention CLEVR-Cogent %s' % exp_dict['obj_num'], max_iterations=int(exp_dict['epochs']))\n\t    # Start the RTPT tracking\n\t    rtpt.start()\n\t    # create save paths and tensorboard writer\n\t    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n\t    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n\t    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n\t    # save args\n\t    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n", "        json.dump(exp_dict, json_file, indent=4)\n\t    print(\"Experiment parameters:\", exp_dict)\n\t    #setup new SLASH program given the network parameters\n\t    program = get_slash_program(exp_dict['obj_num'])\n\t    #setup new SLASH program given the network parameters\n\t    if exp_dict['structure'] == 'poon-domingos':\n\t        exp_dict['depth'] = None\n\t        exp_dict['num_repetitions'] = None\n\t        print(\"using poon-domingos\")\n\t    elif exp_dict['structure'] == 'binary-trees':\n", "        exp_dict['pd_num_pieces'] = None\n\t        print(\"using binary-trees\")\n\t    #size network\n\t    size_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n\t        pd_width = 8,\n\t        pd_height = 8,\n", "        class_count = 3,\n\t        use_em = exp_dict['use_em'],\n\t        learn_prior = exp_dict['learn_prior'])\n\t    #material network\n\t    material_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n\t        pd_width = 8,\n", "        pd_height = 8,\n\t        class_count = 3,\n\t        use_em = exp_dict['use_em'],\n\t        learn_prior = exp_dict['learn_prior'])\n\t    #shape network\n\t    shape_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n", "        pd_width = 8,\n\t        pd_height = 8,\n\t        class_count = 4,\n\t        use_em = exp_dict['use_em'],\n\t        learn_prior = exp_dict['learn_prior'])\n\t    #color network\n\t    color_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n", "        num_var = 64,\n\t        pd_width = 8,\n\t        pd_height = 8,\n\t        class_count = 9,\n\t        use_em = exp_dict['use_em'],\n\t        learn_prior = exp_dict['learn_prior'])\n\t    #create the Slot Attention network\n\t    slot_net = SlotAttention_model(n_slots=exp_dict['obj_num'], n_iters=3, n_attr=18,\n\t                                   encoder_hidden_channels=64, attention_hidden_channels=128, clevr_encoding=True)\n\t    slot_net = slot_net.to(device='cuda')\n", "    #trainable params\n\t    num_trainable_params = [sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in material_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n\t    num_params = [sum(p.numel() for p in size_net.parameters()), \n\t                  sum(p.numel() for p in material_net.parameters()), \n\t                  sum(p.numel() for p in shape_net.parameters()), \n\t                  sum(p.numel() for p in color_net.parameters()),\n", "                  sum(p.numel() for p in slot_net.parameters())]\n\t    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n\t    slot_net_params = list(slot_net.parameters())\n\t    smsc_params = list(size_net.parameters()) + list(material_net.parameters()) + list(shape_net.parameters())  + list(color_net.parameters()) \n\t    #create the SLASH Program\n\t    nnMapping = {'size': size_net,\n\t                 'material': material_net,\n\t                 'shape': shape_net,\n\t                 'color': color_net\n\t                }\n", "    #OPTIMIZERS\n\t    optimizers = {'smsc': torch.optim.Adam([\n\t                                        {'params':smsc_params}],\n\t                                        lr=exp_dict['lr'], eps=1e-7),\n\t                 'slot': torch.optim.Adam([\n\t                                        {'params': slot_net_params}],\n\t                                        lr=0.0004, eps=1e-7)}\n\t    SLASHobj = SLASH(program, nnMapping, optimizers)\n\t    print(\"using learning rate warmup and decay\")\n\t    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n", "    decay_epochs = exp_dict['lr_decay_steps']\n\t    slot_base_lr = 0.0004\n\t    #metric lists\n\t    test_ap_list_a = [] #stores average precsion values\n\t    test_ap_list_b = [] #stores average precsion values\n\t    test_metric_list_a = [] #stores tp, fp, tn values\n\t    test_metric_list_b = [] #stores tp, fp, tn values\n\t    lr_list = [] # store learning rate\n\t    loss_list = []  # store training loss\n\t    startTime = time.time()\n", "    train_test_times = []\n\t    sm_per_batch_list = []\n\t    # Load data\n\t    obj_num = exp_dict['obj_num']\n\t    root = '/SLASH/data/CLEVR_CoGenT_v1.0/'\n\t    mode = 'trainA'\n\t    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n\t    train_dataset_loader = torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=True,batch_size=exp_dict['bs'],pin_memory=True, num_workers=4)\n\t    mode = 'valA'\n\t    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n", "    test_dataset_loader_a= torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=False,batch_size=exp_dict['bs'],pin_memory=True, num_workers=4)\n\t    obj_encodings_gt_a = ap_utils.get_obj_encodings(test_dataset_loader_a)\n\t    mode = 'valB'\n\t    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n\t    test_dataset_loader_b= torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=False,batch_size=exp_dict['bs'],pin_memory=True, num_workers=4)\n\t    obj_encodings_gt_b = ap_utils.get_obj_encodings(test_dataset_loader_b)\n\t    print(\"loaded data\")\n\t    # Resume the training if requested\n\t    start_e= 0\n\t    if exp_dict['resume']:\n", "        print(\"resuming experiment\")\n\t        saved_model = torch.load(saveModelPath)\n\t        #load pytorch models\n\t        color_net.load_state_dict(saved_model['color_net'])\n\t        shape_net.load_state_dict(saved_model['shape_net'])\n\t        material_net.load_state_dict(saved_model['material_net'])\n\t        size_net.load_state_dict(saved_model['size_net'])\n\t        slot_net.load_state_dict(saved_model['slot_net'])\n\t        #optimizers and shedulers\n\t        optimizers['smsc'].load_state_dict(saved_model['resume']['optimizer_smsc'])\n", "        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n\t        start_e = saved_model['resume']['epoch']\n\t        #metrics\n\t        test_ap_list_a = saved_model['test_ap_list_a']\n\t        test_ap_list_b = saved_model['test_ap_list_b']\n\t        test_metric_list_a = saved_model['test_metric_list_a']\n\t        test_metric_list_b = saved_model['test_metric_list_b']\n\t        lr_list = saved_model['lr_list']\n\t        loss_list = saved_model['loss_list']\n\t        train_test_times = saved_model['train_test_times']\n", "        sm_per_batch_list = saved_model['sm_per_batch_list']  \n\t    # train the network and evaluate the performance\n\t    for e in range(start_e, exp_dict['epochs']):\n\t        #we have three datasets right now train, val and test with 20k, 5k and 100 samples\n\t        #TRAIN\n\t        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n\t        time_train = time.time()\n\t        #apply lr schedulers\n\t        if e < warmup_epochs:\n\t            lr = slot_base_lr * ((e+1)/warmup_epochs)\n", "        else:\n\t            lr = slot_base_lr\n\t        lr = lr * 0.5**((e+1)/decay_epochs)\n\t        optimizers['slot'].param_groups[0]['lr'] = lr\n\t        lr_list.append([lr,e])\n\t        print(\"LR SAm:\", \"{:.6f}\".format(lr), optimizers['slot'].param_groups[0]['lr'])\n\t        loss,_,_,_,sm_per_batch  = SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net,hungarian_matching = True, method=exp_dict['method'], p_num=exp_dict['p_num'], k_num=1,\n\t                              epoch=e, writer = writer)\n\t        sm_per_batch_list.append(sm_per_batch)\n\t        loss_list.append(loss)\n", "        writer.add_scalar(\"train/loss\", loss, global_step=e)\n\t        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n\t        #TEST\n\t        time_test = time.time()\n\t        #use only a fraction of the test data, except every 100th epoch\n\t        if e % 100 == 0:\n\t            subset_size = None\n\t        else:\n\t            subset_size = 1500\n\t        ### CONDITION A\n", "        print(\"condition a\")\n\t        #forward test batch\n\t        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader_a)\n\t        #compute the average precision, tp, fp, tn for color+shape+material+size\n\t        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n\t        ap_a, true_positives_a,false_positives_a, true_negatives_a, correctly_classified_a  = ap_utils.average_precision(pred, obj_encodings_gt_a, -1, \"CLEVR\",subset_size = subset_size)\n\t        print(\"avg precision A \", ap_a, \"tp\", true_positives_a, \"fp\", false_positives_a, \"tn\", true_negatives_a, \"correctly classified\", correctly_classified_a)\n\t        test_ap_list_a.append([ap_a, e])                    \n\t        test_metric_list_a.append([true_positives_a, false_positives_a, true_negatives_a, correctly_classified_a])\n\t        ### CONDITION B\n", "        print(\"condition b\")\n\t        #forward test batch\n\t        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader_b)\n\t        #compute the average precision, tp, fp, tn for color+shape+material+size\n\t        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n\t        ap_b, true_positives_b,false_positives_b, true_negatives_b, correctly_classified_b  = ap_utils.average_precision(pred, obj_encodings_gt_b, -1, \"CLEVR\",subset_size = subset_size)\n\t        print(\"avg precision A \", ap_b, \"tp\", true_positives_b, \"fp\", false_positives_b, \"tn\", true_negatives_b, \"correctly classified\", correctly_classified_b)\n\t        test_ap_list_b.append([ap_b, e])                    \n\t        test_metric_list_b.append([true_positives_b, false_positives_b, true_negatives_b, correctly_classified_b])\n\t        #Tensorboard outputs\n", "        writer.add_scalar(\"test/ap_a\", ap_a, global_step=e)\n\t        writer.add_scalar(\"test/ap_b\", ap_b, global_step=e)\n\t        #Time measurements\n\t        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n\t        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n\t        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n\t        train_test_times_np = np.array(train_test_times)\n\t        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n\t        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n\t        print('--- total time from beginning:  ---', timestamp_total )\n", "        #save the neural network  such that we can use it later\n\t        print('Storing the trained model into {}'.format(saveModelPath))\n\t        torch.save({\"color_net\":color_net.state_dict(),\n\t                    \"shape_net\":shape_net.state_dict(),                    \n\t                    \"material_net\":material_net.state_dict(),\n\t                    \"size_net\":size_net.state_dict(),\n\t                    \"slot_net\":slot_net.state_dict(),\n\t                    \"resume\": {\n\t                        \"optimizer_smsc\":optimizers['smsc'].state_dict(),\n\t                        \"optimizer_slot\":optimizers['slot'].state_dict(),\n", "                        \"epoch\":e\n\t                    },\n\t                    \"test_ap_list_a\":test_ap_list_a,\n\t                    \"test_ap_list_b\":test_ap_list_b,\n\t                    \"loss_list\":loss_list,                    \n\t                    \"sm_per_batch_list\":sm_per_batch_list,\n\t                    \"test_metric_list_a\":test_metric_list_a,\n\t                    \"test_metric_list_b\":test_metric_list_b,\n\t                    \"lr_list\":lr_list,\n\t                    \"num_params\":num_params,\n", "                    \"train_test_times\": train_test_times,\n\t                    \"exp_dict\":exp_dict,\n\t                    \"program\":program}, saveModelPath)\n\t        # Update the RTPT\n\t        rtpt.step(subtitle=f\"ap_b={ap_b:2.2f}\")\n"]}
{"filename": "src/experiments/slash_attention/clevr_cogent/__init__.py", "chunked_list": []}
{"filename": "src/experiments/slash_attention/clevr_cogent/dataGen.py", "chunked_list": ["import torch\n\timport torchvision\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.transforms import transforms\n\t# from torch.utils.data import Dataset\n\t# from torchvision import transforms\n\tfrom skimage import io\n\timport os\n\timport numpy as np\n\timport torch\n", "import matplotlib.pyplot as plt\n\tfrom PIL import ImageFile\n\tImageFile.LOAD_TRUNCATED_IMAGES = True\n\timport json\n\tfrom tqdm import tqdm\n\tdef class_lookup(idx):\n\t    '''\n\t    Shapeworld dataset generation generates a class id for each class. Returns color, shape and object encoding given the class id.\n\t    The class order was generated by ShapeWorld.\n\t    '''\n", "    #obj encoding ['red','blue','green','black',  'circle' , 'triangle','square', 'bg', 'object_confidence(1)']\n\t    if  idx == 0:\n\t        return \"red\", \"circle\", [1,0,0,0  ,1,0,0,0 , 1]\n\t    elif idx ==1:\n\t        return \"green\", \"circle\" ,[0,0,1,0  ,1,0,0,0 , 1]\n\t    elif idx ==2:\n\t        return \"blue\", \"circle\", [0,1,0,0  ,1,0,0,0 , 1]\n\t    elif idx ==3:\n\t        return \"red\", \"square\", [1,0,0,0  ,0,0,1,0 , 1]\n\t    elif idx ==4:\n", "        return \"green\", \"square\", [0,0,1,0  ,0,0,1,0 , 1]\n\t    elif idx ==5:\n\t        return \"blue\", \"square\" , [0,1,0,0  ,0,0,1,0 , 1]\n\t    elif idx ==6:\n\t        return \"red\", \"triangle\", [1,0,0,0  ,0,1,0,0 , 1]\n\t    elif idx ==7:\n\t        return \"green\", \"triangle\", [0,0,1,0  ,0,1,0,0 , 1]\n\t    elif idx ==8:\n\t        return \"blue\", \"triangle\", [0,1,0,0  ,0,1,0,0 , 1]\n\tdef object_encoding(size, material, shape, color ):\n", "    #size (small, large, bg)\n\t    if size == \"small\":\n\t        size_enc = [1,0,0]\n\t    elif size == \"large\":\n\t        size_enc = [0,1,0]\n\t    elif size == \"bg\":\n\t        size_enc = [0,0,1]\n\t    #material (rubber, metal, bg)\n\t    if material == \"rubber\":\n\t        material_enc = [1,0,0]\n", "    elif material == \"metal\":\n\t        material_enc = [0,1,0]\n\t    elif material == \"bg\":\n\t        material_enc = [0,0,1]\n\t    #shape (cube, sphere, cylinder, bg)\n\t    if shape == \"cube\":\n\t        shape_enc = [1,0,0,0]\n\t    elif shape == \"sphere\":\n\t        shape_enc = [0,1,0,0]\n\t    elif shape == \"cylinder\":\n", "        shape_enc = [0,0,1,0]\n\t    elif shape == \"bg\":\n\t        shape_enc = [0,0,0,1]\n\t    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n\t    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n\t    #color  1      2     3     4     5     6        7     8      9\n\t    if color == \"gray\":\n\t        color_enc = [1,0,0,0,0,0,0,0,0]\n\t    elif color == \"red\":\n\t        color_enc = [0,1,0,0,0,0,0,0,0]\n", "    elif color == \"blue\":\n\t        color_enc = [0,0,1,0,0,0,0,0,0]\n\t    elif color == \"green\":\n\t        color_enc = [0,0,0,1,0,0,0,0,0]\n\t    elif color == \"brown\":\n\t        color_enc = [0,0,0,0,1,0,0,0,0]\n\t    elif color == \"purple\":\n\t        color_enc = [0,0,0,0,0,1,0,0,0]\n\t    elif color == \"cyan\":\n\t        color_enc = [0,0,0,0,0,0,1,0,0]\n", "    elif color == \"yellow\":\n\t        color_enc = [0,0,0,0,0,0,0,1,0]\n\t    elif color == \"bg\":\n\t        color_enc = [0,0,0,0,0,0,0,0,1]\n\t    return size_enc + material_enc + shape_enc + color_enc +[1]\n\tclass CLEVR(Dataset):\n\t    def __init__(self, root, mode, img_paths=None, files_names=None, obj_num=None):\n\t        self.root = root  # The root folder of the dataset\n\t        self.mode = mode  # The mode of 'train' or 'val'\n\t        self.files_names = files_names # The list of the files names with correct nuber of objects\n", "        if obj_num is not None:\n\t            self.obj_num = obj_num  # The upper limit of number of objects \n\t        else:\n\t            self.obj_num = 10\n\t        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\t        #list of sorted image paths\n\t        self.img_paths = []\n\t        if img_paths:\n\t            self.img_paths = img_paths\n\t        else:                    \n", "            #open directory and save all image paths\n\t            for file in os.scandir(os.path.join(root, 'images', mode)):\n\t                img_path = file.path\n\t                if '.png' in img_path:\n\t                    self.img_paths.append(img_path)\n\t        self.img_paths.sort()\n\t        count = 0\n\t        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n\t        self.query_map = {}\n\t        self.obj_map = {}\n", "        count = 0        \n\t        #We have up to 10 objects in the image, load the json file\n\t        with open(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\")) as f:\n\t            data = json.load(f)\n\t            #iterate over each scene and create the query string and obj encoding\n\t            print(\"parsing scences\")\n\t            for scene in tqdm(data['scenes']):\n\t                target_query = \"\"\n\t                obj_encoding_list = []\n\t                if self.files_names:\n", "                    if any(scene['image_filename'] in file_name for file_name in files_names):                    \n\t                        num_objects = 0\n\t                        for idx, obj in enumerate(scene['objects']):\n\t                            target_query += \" :- not object(o{}, {}, {}, {}, {}).\".format(idx+1, obj['size'], obj['material'], obj['shape'], obj['color'])\n\t                            obj_encoding_list.append(object_encoding(obj['size'], obj['material'], obj['shape'], obj['color']))\n\t                            num_objects = idx+1 #store the num of objects \n\t                        #fill in background objects\n\t                        for idx in range(num_objects, self.obj_num):\n\t                            target_query += \" :- not object(o{}, bg, bg, bg, bg).\".format(idx+1)\n\t                            obj_encoding_list.append([0,0,1, 0,0,1, 0,0,0,1, 0,0,0,0,0,0,0,0,1, 1])\n", "                        self.query_map[count] = target_query\n\t                        self.obj_map[count] = np.array(obj_encoding_list)\n\t                        count += 1\n\t                else:\n\t                    num_objects=0\n\t                    for idx, obj in enumerate(scene['objects']):\n\t                        target_query += \" :- not object(o{}, {}, {}, {}, {}).\".format(idx+1, obj['size'], obj['material'], obj['shape'], obj['color'])\n\t                        obj_encoding_list.append(object_encoding(obj['size'], obj['material'], obj['shape'], obj['color']))\n\t                        num_objects = idx+1 #store the num of objects \n\t                    #fill in background objects\n", "                    for idx in range(num_objects, 10):\n\t                        target_query += \" :- not object(o{}, bg, bg, bg, bg).\".format(idx+1)\n\t                        obj_encoding_list.append([0,0,1, 0,0,1, 0,0,0,1, 0,0,0,0,0,0,0,0,1, 1])\n\t                    self.query_map[scene['image_index']] = target_query\n\t                    self.obj_map[scene['image_index']] = np.array(obj_encoding_list)\n\t            print(\"done\")\n\t        if self.files_names:\n\t            print(f'Correctly found images {count} out of {len(files_names)}')\n\t        #print(np.array(list(self.obj_map.values()))[0:20])\n\t    def __getitem__(self, index):\n", "        #get the image\n\t        img_path = self.img_paths[index]\n\t        img = io.imread(img_path)[:, :, :3]\n\t        transform = transforms.Compose([\n\t            transforms.ToPILImage(),\n\t            #transforms.CenterCrop((29, 221,64, 256)), #why do we need to crop?\n\t            transforms.Resize((128, 128)),\n\t            transforms.ToTensor(),\n\t        ])\n\t        img = transform(img)\n", "        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\t        return {'im':img}, self.query_map[index] ,self.obj_map[index]\n\t    def __len__(self):\n\t        return len(self.img_paths)\n"]}
{"filename": "src/experiments/slash_attention/clevr_cogent/slash_attention_clevr.py", "chunked_list": ["#!/usr/bin/env python\n\t# coding: utf-8\n\timport train\n\timport datetime\n\tseed = 4\n\tobj_num = 10\n\tdate_string = datetime.datetime.today().strftime('%d-%m-%Y')\n\texperiments = {f'CLEVR{obj_num}_seed_{seed}': {'structure':'poon-domingos', 'pd_num_pieces':[4], 'learn_prior':True,\n\t                         'lr': 0.01, 'bs':512, 'epochs':1000,\n\t                        'lr_warmup_steps':8, 'lr_decay_steps':360, 'use_em':False, 'resume':False,\n", "                        'method':'most_prob',\n\t                         'start_date':date_string, 'credentials':'DO', 'p_num':16, 'seed':seed, 'obj_num':obj_num\n\t                        }}\n\tfor exp_name in experiments:\n\t    print(exp_name)\n\t    train.slash_slot_attention(exp_name, experiments[exp_name])\n"]}
{"filename": "src/experiments/slash_attention/clevr_cogent/auxiliary.py", "chunked_list": ["import json\n\timport os\n\tfrom pathlib import Path\n\tdef get_files_names_and_paths(root:str='./data/CLEVR_v1.0/', mode:str='val', obj_num:int=4):\n\t    data_file = Path(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\"))\n\t    data_file.parent.mkdir(parents=True, exist_ok=True)\n\t    if data_file.exists():\n\t        print('File exists. Parsing file...')\n\t    else:\n\t        print(f'The JSON file {data_file} does not exist!')\n", "        quit()\n\t    img_paths = []\n\t    files_names = []\n\t    with open(data_file, 'r') as json_file:\n\t        json_data = json.load(json_file)\n\t        for scene in json_data['scenes']:\n\t            if len(scene['objects']) <= obj_num:\n\t                img_paths.append(Path(os.path.join(root,'images/'+mode+'/'+scene['image_filename'])))\n\t                files_names.append(scene['image_filename'])\n\t    print(\"...done \")\n", "    return img_paths, files_names\n\tdef get_slash_program(obj_num:int=4):\n\t    program = ''\n\t    if obj_num == 10:\n\t        program ='''\n\t    slot(s1).\n\t    slot(s2).\n\t    slot(s3).\n\t    slot(s4).\n\t    slot(s5).\n", "    slot(s6).\n\t    slot(s7).\n\t    slot(s8).\n\t    slot(s9).\n\t    slot(s10).\n\t    name(o1).\n\t    name(o2).\n\t    name(o3).\n\t    name(o4).\n\t    name(o5).\n", "    name(o6).\n\t    name(o7).\n\t    name(o8).\n\t    name(o9).\n\t    name(o10).\n\t        '''\n\t    elif obj_num ==4:\n\t        program ='''\n\t    slot(s1).\n\t    slot(s2).\n", "    slot(s3).\n\t    slot(s4).\n\t    name(o1).\n\t    name(o2).\n\t    name(o3).\n\t    name(o4).\n\t        '''\n\t    elif obj_num ==6:\n\t        program ='''\n\t    slot(s1).\n", "    slot(s2).\n\t    slot(s3).\n\t    slot(s4).\n\t    slot(s5).\n\t    slot(s6).\n\t    name(o1).\n\t    name(o2).\n\t    name(o3).\n\t    name(o4).\n\t    name(o5).\n", "    name(o6).\n\t        '''\n\t    else:\n\t        print(f'The number of objects {obj_num} is wrong!')\n\t        quit()\n\t    program +='''        \n\t    %assign each name a slot\n\t    %{slot_name_comb(N,X): slot(X)}=1 :- name(N). %problem we have dublicated slots\n\t    %remove each model which has multiple slots asigned to the same name\n\t    %:- slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2. \n", "    %build the object ontop of the slot assignment\n\t    object(N, S, M, P, C) :- size(0, +X, -S), material(0, +X, -M), shape(0, +X, -P), color(0, +X, -C), slot(X), name(N), slot_name_comb(N,X).\n\t    %define the SPNs\n\t    npp(size(1,X),[small, large, bg]) :- slot(X).\n\t    npp(material(1,X),[rubber, metal, bg]) :- slot(X).\n\t    npp(shape(1,X),[cube, sphere, cylinder, bg]) :- slot(X).\n\t    npp(color(1,X),[gray, red, blue, green, brown, purple, cyan, yellow, bg]) :- slot(X).\n\t    '''\n\t    return program"]}
{"filename": "src/experiments/slash_attention/cogent/train.py", "chunked_list": ["print(\"start importing...\")\n\timport os\n\timport time\n\timport sys\n\timport json\n\tsys.path.append('../../../')\n\tsys.path.append('../../../SLASH/')\n\tsys.path.append('../../../EinsumNetworks/src/')\n\t#torch, numpy, ...\n\timport torch\n", "from torch.utils.tensorboard import SummaryWriter\n\tfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\tfrom warmup_scheduler import GradualWarmupScheduler\n\timport numpy as np\n\t#own modules\n\tfrom dataGen import SHAPEWORLD_COGENT\n\tfrom einsum_wrapper import EiNet\n\tfrom slash import SLASH\n\timport ap_utils\n\timport utils\n", "from utils import set_manual_seed\n\tfrom slot_attention_module import SlotAttention_model\n\tfrom pathlib import Path\n\tfrom rtpt import RTPT\n\t#seeds\n\ttorch.manual_seed(42)\n\tnp.random.seed(42)\n\tprint(\"...done\")\n\tdef slash_slot_attention(exp_name , exp_dict):\n\t    program ='''\n", "    slot(s1).\n\t    slot(s2).\n\t    slot(s3).\n\t    slot(s4).\n\t    name(o1).\n\t    name(o2).\n\t    name(o3).\n\t    name(o4).\n\t    %build the object ontop of the slot assignment\n\t    object(N,C,S,H,Z) :- color(0, +X, -C), shape(0, +X, -S), shade(0, +X, -H), size(0, +X, -Z), slot(X), name(N), slot_name_comb(N,X).\n", "    npp(color(1,X),[red, blue, green, gray, brown, magenta, cyan, yellow, black]) :- slot(X).\n\t    npp(shape(1,X),[circle, triangle, square, bg]) :- slot(X).\n\t    npp(shade(1,X),[bright, dark, bg]) :- slot(X).\n\t    npp(size(1,X),[small, big, bg]) :- slot(X).\n\t    '''\n\t    if exp_dict['hungarian_matching'] == False:\n\t        program += \"\"\"\n\t        %assign each name a slot\n\t        {slot_name_comb(N,X): slot(X) }=1 :- name(N). %problem we have dublicated slots\n\t        %remove each model which has multiple slots asigned to the same name\n", "        %:-  slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2.\n\t        {slot_name_comb(N,X): name(N) }=1 :- slot(X). %problem we have dublicated slots\n\t        \"\"\"\n\t    # Set the seeds for PRNG\n\t    set_manual_seed(exp_dict['seed'])\n\t    # Create RTPT object\n\t    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name='SLASH Shapeworld4 CoGenT', max_iterations=int(exp_dict['epochs']))\n\t    # Start the RTPT tracking\n\t    rtpt.start()\n\t    # create save paths and tensorboard writer\n", "    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n\t    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n\t    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n\t    # save args\n\t    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n\t        json.dump(exp_dict, json_file, indent=4)\n\t    print(\"Experiment parameters:\", exp_dict)\n\t    #setup new SLASH program given the network parameters\n\t    if exp_dict['structure'] == 'poon-domingos':\n\t        exp_dict['depth'] = None\n", "        exp_dict['num_repetitions'] = None\n\t        print(\"using poon-domingos\")\n\t    elif exp_dict['structure'] == 'binary-trees':\n\t        exp_dict['pd_num_pieces'] = None\n\t        print(\"using binary-trees\")\n\t    #NETWORKS\n\t    #color network\n\t    color_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n", "        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n\t        class_count=9,\n\t        pd_width=8,pd_height=8,\n\t        use_em= False)\n\t    #shape network\n\t    shape_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n", "        num_var = 64,\n\t        class_count=4,\n\t        pd_width=8,pd_height=8,\n\t        use_em= False)\n\t    #shade network\n\t    shade_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n", "        class_count=3,\n\t        pd_width=8,pd_height=8,\n\t        use_em= False)\n\t    #size network\n\t    size_net = EiNet(structure = exp_dict['structure'],\n\t        pd_num_pieces = exp_dict['pd_num_pieces'],\n\t        depth = exp_dict['depth'],\n\t        num_repetitions = exp_dict['num_repetitions'],\n\t        num_var = 64,\n\t        class_count=3,\n", "        pd_width=8,pd_height=8,\n\t        use_em= False)\n\t    #create the Slot Attention network\n\t    slot_net = SlotAttention_model(n_slots=4, n_iters=3, n_attr=18,\n\t                                encoder_hidden_channels=64, attention_hidden_channels=64)\n\t    slot_net = slot_net.to(device='cuda')\n\t    #count trainable params\n\t    num_trainable_params = [sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in shade_net.parameters() if p.requires_grad),\n", "                            sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n\t                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n\t    num_params = [sum(p.numel() for p in color_net.parameters()), \n\t                  sum(p.numel() for p in shape_net.parameters()),\n\t                  sum(p.numel() for p in shade_net.parameters()),\n\t                  sum(p.numel() for p in size_net.parameters()),\n\t                  sum(p.numel() for p in slot_net.parameters())]\n\t    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n\t    slot_net_params = list(slot_net.parameters())\n\t    csss_params = list(color_net.parameters()) + list(shape_net.parameters()) + list(shade_net.parameters()) + list(size_net.parameters())\n", "    #create the SLASH Program\n\t    nnMapping = {'color': color_net,\n\t                 'shape':shape_net,\n\t                 'shade':shade_net,\n\t                 'size':size_net}\n\t    #OPTIMIZERS and LEARNING RATE SHEDULING\n\t    print(\"training probabilisitc circuits and SlotAttention module\")\n\t    optimizers = {'csss': torch.optim.Adam([\n\t                                            {'params':csss_params}],\n\t                                            lr=exp_dict['lr'], eps=1e-7),\n", "                 'slot': torch.optim.Adam([\n\t                                            {'params': slot_net_params}],\n\t                                            lr=0.0004, eps=1e-7)}\n\t    SLASHobj = SLASH(program, nnMapping, optimizers)\n\t    print(\"using learning rate warmup and decay\")\n\t    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n\t    decay_epochs = exp_dict['lr_decay_steps']\n\t    slot_base_lr = 0.0004\n\t    #metric lists\n\t    test_ap_list_a = [] #stores average precsion values for test set with condition a\n", "    test_ap_list_b = [] #stores average precsion values for test set with condition b\n\t    test_metric_list_a = [] #stores tp, fp, tn values for test set with condition a\n\t    test_metric_list_b = [] #stores tp, fp, tn values for test set with condition b\n\t    lr_list = [] # store learning rate\n\t    loss_list = []  # store training loss\n\t    startTime = time.time()\n\t    train_test_times = []\n\t    sm_per_batch_list = []\n\t    # load datasets\n\t    print(\"loading data...\")\n", "    #if the hungarian matching algorithm is used we need to pass the object encodings to SLASH\n\t    if exp_dict['hungarian_matching']:\n\t        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD_COGENT('../../data/shapeworld_cogent/',\"train_a\", ret_obj_encoding=True), shuffle=True,batch_size=exp_dict['bs'],pin_memory=False, num_workers=4)\n\t    else:\n\t        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD_COGENT('../../data/shapeworld_cogent/',\"train_a\"), shuffle=True,batch_size=exp_dict['bs'],pin_memory=False, num_workers=4)\n\t    test_dataset_loader_a = torch.utils.data.DataLoader(SHAPEWORLD_COGENT('../../data/shapeworld_cogent/',\"val_a\", ret_obj_encoding=True), shuffle=False,batch_size=exp_dict['bs'],pin_memory=False, num_workers=4)\n\t    test_dataset_loader_b = torch.utils.data.DataLoader(SHAPEWORLD_COGENT('../../data/shapeworld_cogent/',\"val_b\", ret_obj_encoding=True), shuffle=False,batch_size=exp_dict['bs'],pin_memory=False, num_workers=4)\n\t    obj_encodings_gt_a = ap_utils.get_obj_encodings(test_dataset_loader_a)\n\t    obj_encodings_gt_b = ap_utils.get_obj_encodings(test_dataset_loader_b)\n\t    print(\"...done\")\n", "    start_e= 0\n\t    if exp_dict['resume']:\n\t        print(\"resuming experiment\")\n\t        saved_model = torch.load(saveModelPath)\n\t        #load pytorch models\n\t        color_net.load_state_dict(saved_model['color_net'])\n\t        shape_net.load_state_dict(saved_model['shape_net'])\n\t        shade_net.load_state_dict(saved_model['shade_net'])\n\t        size_net.load_state_dict(saved_model['size_net'])\n\t        slot_net.load_state_dict(saved_model['slot_net'])\n", "        #optimizers and shedulers\n\t        optimizers['csss'].load_state_dict(saved_model['resume']['optimizer_csss'])\n\t        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n\t        start_e = saved_model['resume']['epoch']\n\t        #metrics\n\t        test_ap_list_a = saved_model['test_ap_list_a']\n\t        test_ap_list_b = saved_model['test_ap_list_b']\n\t        test_metric_list_a = saved_model['test_metric_list_a']\n\t        test_metric_list_b = saved_model['test_metric_list_b']\n\t        lr_list = saved_model['lr_list']\n", "        loss_list = saved_model['loss_list']\n\t        train_test_times = saved_model['train_test_times']\n\t        sm_per_batch_list = saved_model['sm_per_batch_list']\n\t    for e in range(start_e, exp_dict['epochs']):\n\t        #TRAIN\n\t        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n\t        time_train= time.time()\n\t        #apply lr schedulers to the SAm\n\t        if e < warmup_epochs:\n\t            lr = slot_base_lr * ((e+1)/warmup_epochs)\n", "        else:\n\t            lr = slot_base_lr\n\t        lr = lr * 0.5**((e+1)/decay_epochs)\n\t        optimizers['slot'].param_groups[0]['lr'] = lr\n\t        lr_list.append([lr,e])\n\t        loss,_,_,_,sm_per_batch = SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net, hungarian_matching=exp_dict['hungarian_matching'], method=exp_dict['method'], p_num = exp_dict['p_num'], k_num=1,\n\t                        epoch=e, writer = writer)\n\t        sm_per_batch_list.append(sm_per_batch)\n\t        loss_list.append(loss)\n\t        writer.add_scalar(\"train/loss\", loss, global_step=e)\n", "        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n\t        #TEST\n\t        time_test = time.time()\n\t        ### CONDITION A\n\t        print(\"condition a\")\n\t        #forward test batch a\n\t        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader_a)\n\t        #compute the average precision, tp, fp, tn for color+shape+shade+size, color, shape\n\t        #color+shape+shade+size\n\t        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n", "        ap, true_positives,false_positives, true_negatives, correctly_classified = ap_utils.average_precision(pred, obj_encodings_gt_a,-1, \"SHAPEWORLD4\")\n\t        print(\"avg precision(a) \",ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\",correctly_classified)\n\t        #color\n\t        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n\t        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c= ap_utils.average_precision(pred_c, obj_encodings_gt_a,-1, \"SHAPEWORLD4\", only_color = True)\n\t        print(\"avg precision(a) color\",ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\",correctly_classified_c)\n\t        #shape              \n\t        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n\t        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s= ap_utils.average_precision(pred_s, obj_encodings_gt_a,-1, \"SHAPEWORLD4\", only_shape = True)\n\t        print(\"avg precision(a) shape\",ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\",correctly_classified_s)\n", "        #store ap, tp, fp, tn\n\t        test_ap_list_a.append([ap, ap_c, ap_s, e])                    \n\t        test_metric_list_a.append([true_positives, false_positives, true_negatives,correctly_classified,\n\t                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n\t                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s])\n\t        writer.add_scalar(\"test/ap_cond_a\", ap, global_step=e)\n\t        writer.add_scalar(\"test/ap_c_cond_a\", ap_c, global_step=e)\n\t        writer.add_scalar(\"test/ap_s_cond_a\", ap_s, global_step=e)\n\t        ### CONDITION B\n\t        print(\"condition b\")\n", "         #forward test batch a\n\t        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader_b)\n\t        #compute the average precision, tp, fp, tn for color+shape+shade+size, color, shap\n\t        #color+shape+shade+size\n\t        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n\t        ap, true_positives,false_positives, true_negatives, correctly_classified = ap_utils.average_precision(pred, obj_encodings_gt_b,-1, \"SHAPEWORLD4\")\n\t        print(\"avg precision(b) \",ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\",correctly_classified)\n\t        #color\n\t        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n\t        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c= ap_utils.average_precision(pred, obj_encodings_gt_b,-1, \"SHAPEWORLD4\", only_color = True)\n", "        print(\"avg precision(b) color\",ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\",correctly_classified_c)\n\t        #shape              \n\t        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n\t        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s= ap_utils.average_precision(pred_s, obj_encodings_gt_b,-1, \"SHAPEWORLD4\", only_shape = True)\n\t        print(\"avg precision(b) shape\",ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\",correctly_classified_s)\n\t        #store ap, tp, fp, tn\n\t        test_ap_list_b.append([ap, ap_c, ap_s, e])                    \n\t        test_metric_list_b.append([true_positives, false_positives, true_negatives,correctly_classified,\n\t                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n\t                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s])\n", "        writer.add_scalar(\"test/ap_cond_b\", ap, global_step=e)\n\t        writer.add_scalar(\"test/ap_c_cond_b\", ap_c, global_step=e)\n\t        writer.add_scalar(\"test/ap_s_cond_b\", ap_s, global_step=e)\n\t        #Time measurements\n\t        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n\t        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n\t        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n\t        train_test_times_np = np.array(train_test_times)        \n\t        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n\t        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n", "        print('--- total time from beginning:  ---', timestamp_total )\n\t        #save the neural network  such that we can use it later\n\t        print('Storing the trained model into {}'.format(saveModelPath))\n\t        torch.save({\"shape_net\":  shape_net.state_dict(), \n\t                    \"color_net\": color_net.state_dict(),\n\t                    \"shade_net\": shade_net.state_dict(),\n\t                    \"size_net\": size_net.state_dict(),\n\t                    \"slot_net\": slot_net.state_dict(),\n\t                    \"resume\": {\n\t                        \"optimizer_csss\":optimizers['csss'].state_dict(),\n", "                        \"optimizer_slot\": optimizers['slot'].state_dict(),\n\t                        \"epoch\":e\n\t                    },\n\t                    \"test_ap_list_a\":test_ap_list_a,\n\t                    \"test_ap_list_b\":test_ap_list_b,\n\t                    \"loss_list\":loss_list,\n\t                    \"sm_per_batch_list\":sm_per_batch_list,\n\t                    \"test_metric_list_a\":test_metric_list_a,\n\t                    \"test_metric_list_b\":test_metric_list_b,\n\t                    \"lr_list\": lr_list,\n", "                    \"num_params\": num_params,\n\t                    \"train_test_times\": train_test_times,\n\t                    \"exp_dict\":exp_dict,\n\t                    \"program\":program}, saveModelPath)\n\t        # Update the RTPT\n\t        rtpt.step()\n"]}
{"filename": "src/experiments/slash_attention/cogent/__init__.py", "chunked_list": []}
{"filename": "src/experiments/slash_attention/cogent/dataGen.py", "chunked_list": ["import torch\n\timport torchvision\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision.transforms import transforms\n\tfrom torch.utils.data import Dataset\n\tfrom torchvision import transforms\n\tfrom skimage import io\n\timport os\n\timport numpy as np\n\timport torch\n", "from PIL import ImageFile\n\tImageFile.LOAD_TRUNCATED_IMAGES = True\n\timport json\n\timport datasets\n\tdef get_encoding(color, shape, shade, size):\n\t    if color == 'red':\n\t        col_enc = [1,0,0,0,0,0,0,0,0]\n\t    elif color == 'blue':\n\t        col_enc = [0,1,0,0,0,0,0,0,0]\n\t    elif color == 'green':\n", "        col_enc = [0,0,1,0,0,0,0,0,0]\n\t    elif color == 'gray':\n\t        col_enc = [0,0,0,1,0,0,0,0,0]\n\t    elif color == 'brown':\n\t        col_enc = [0,0,0,0,1,0,0,0,0]\n\t    elif color == 'magenta':\n\t        col_enc = [0,0,0,0,0,1,0,0,0]\n\t    elif color == 'cyan':\n\t        col_enc = [0,0,0,0,0,0,1,0,0]\n\t    elif color == 'yellow':\n", "        col_enc = [0,0,0,0,0,0,0,1,0]\n\t    elif color == 'black':\n\t        col_enc = [0,0,0,0,0,0,0,0,1]\n\t    if shape == 'circle':\n\t        shape_enc = [1,0,0,0]\n\t    elif shape == 'triangle':\n\t        shape_enc = [0,1,0,0]\n\t    elif shape == 'square':\n\t        shape_enc = [0,0,1,0]    \n\t    elif shape == 'bg':\n", "        shape_enc = [0,0,0,1]\n\t    if shade == 'bright':\n\t        shade_enc = [1,0,0]\n\t    elif shade =='dark':\n\t        shade_enc = [0,1,0]\n\t    elif shade == 'bg':\n\t        shade_enc = [0,0,1]\n\t    if size == 'small':\n\t        size_enc = [1,0,0]\n\t    elif size == 'big':\n", "        size_enc = [0,1,0]\n\t    elif size == 'bg':\n\t        size_enc = [0,0,1]\n\t    return col_enc + shape_enc + shade_enc + size_enc + [1]\n\tclass SHAPEWORLD_COGENT(Dataset):\n\t    def __init__(self, root, mode, ret_obj_encoding=False):\n\t        datasets.maybe_download_shapeworld_cogent()\n\t        self.ret_obj_encoding= ret_obj_encoding\n\t        self.root = root\n\t        self.mode = mode\n", "        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\t        #dictionary of the form {'image_idx':'img_path'}\n\t        self.img_paths = {}\n\t        for file in os.scandir(os.path.join(root, 'images', mode)):\n\t            img_path = file.path\n\t            img_path_idx =   img_path.split(\"/\")\n\t            img_path_idx = img_path_idx[-1]\n\t            img_path_idx = img_path_idx[:-4][6:]\n\t            try:\n\t                img_path_idx =  int(img_path_idx)\n", "                self.img_paths[img_path_idx] = img_path\n\t            except:\n\t                print(\"path:\",img_path_idx)\n\t        count = 0\n\t        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n\t        self.query_map = {}\n\t        self.obj_map = {}\n\t        with open(os.path.join(root, 'labels', mode,\"world_model.json\")) as f:\n\t            worlds = json.load(f)\n\t            objects = 0\n", "            bgs = 0\n\t            #iterate over all objects\n\t            for world in worlds:\n\t                num_objects = 0\n\t                target_query = \"\"\n\t                obj_enc = []\n\t                for entity in world['entities']:\n\t                    color = entity['color']['name']\n\t                    shape = entity['shape']['name']\n\t                    shade_val = entity['color']['shade']\n", "                    if shade_val == 0.0:\n\t                        shade = 'bright'\n\t                    else:\n\t                        shade = 'dark'\n\t                    size_val = entity['shape']['size']['x']\n\t                    if size_val == 0.075:\n\t                        size = 'small'\n\t                    elif size_val == 0.15:\n\t                        size = 'big'\n\t                    name = 'o' + str(num_objects+1)\n", "                    target_query = target_query+ \":- not object({},{},{},{},{}). \".format(name, color, shape, shade, size)\n\t                    obj_enc.append(get_encoding(color, shape, shade, size))\n\t                    num_objects += 1\n\t                    objects +=1 \n\t                #bg encodings\n\t                for i in range(num_objects, 4):\n\t                    name = 'o' + str(num_objects+1)\n\t                    target_query = target_query+ \":- not object({},black,bg, bg, bg). \".format(name)\n\t                    obj_enc.append(get_encoding(\"black\",\"bg\",\"bg\",\"bg\"))\n\t                    num_objects += 1\n", "                    bgs +=1\n\t                self.query_map[count] = target_query\n\t                self.obj_map[count] = np.array(obj_enc)\n\t                count+=1\n\t            print(\"num objects\",objects)\n\t            print(\"num bgs\",bgs)\n\t    def __getitem__(self, index):\n\t        #get the image\n\t        img_path = self.img_paths[index]\n\t        img = io.imread(img_path)[:, :, :3]\n", "        transform = transforms.Compose([\n\t            transforms.ToPILImage(),\n\t            transforms.ToTensor(),\n\t        ])\n\t        img = transform(img)\n\t        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\t        if self.ret_obj_encoding:\n\t            return {'im':img}, self.query_map[index] ,self.obj_map[index]\n\t        else:\n\t            return {'im':img}, self.query_map[index]        \n", "    def __len__(self):\n\t        return len(self.img_paths)\n"]}
{"filename": "src/experiments/slash_attention/cogent/slash_attention_cogent.py", "chunked_list": ["#!/usr/bin/env python\n\t# coding: utf-8\n\timport train\n\timport numpy as np\n\timport torch\n\timport torchvision\n\timport datetime\n\tdate_string = datetime.datetime.today().strftime('%d-%m-%Y')\n\t#Python script to start the shapeworld4 slot attention experiment\n\t#Define your experiment(s) parameters as a hashmap having the following parameters\n", "example_structure = {'experiment_name': \n\t                   {'structure': 'poon-domingos',\n\t                    'pd_num_pieces': [4],\n\t                    'lr': 0.01, #the learning rate to train the SPNs with, the slot attention module has a fixed lr=0.0004  \n\t                    'bs':512, #the batchsize\n\t                    'epochs':1000, #number of epochs to train\n\t                    'lr_warmup_steps':25, #number of epochs to warm up the slot attention module, warmup does not apply to the SPNs\n\t                    'lr_decay_steps':100, #number of epochs it takes to decay to 50% of the specified lr\n\t                    'start_date':\"01-01-0001\", #current date\n\t                    'resume':False, #you can stop the experiment and set this parameter to true to load the last state and continue learning\n", "                    'credentials':'AS', #your credentials for the rtpt class\n\t                    'explanation': \"\"\"Training on Condtion A, Testing on Condtion A and B to evaluate generalization of the model.\"\"\"}}\n\texperiments ={'shapeworld4_cogent_hung': \n\t                   {'structure': 'poon-domingos', 'pd_num_pieces': [4],\n\t                    'lr': 0.01, 'bs':512, 'epochs':1000,\n\t                    'lr_warmup_steps':8, 'lr_decay_steps':360,\n\t                    'start_date':date_string, 'resume':False,\n\t                    'credentials':'DO', 'seed':3, 'learn_prior':True,\n\t                    'p_num':16, 'hungarian_matching':True, 'method':'probabilistic_grounding_top_k',\n\t                    'explanation': \"\"\"Training on Condtion A, Testing on Condtion A and B to evaluate generalization of the model.\"\"\"}}\n", "#train the network\n\tfor exp_name in experiments:\n\t    print(exp_name)\n\t    train.slash_slot_attention(exp_name, experiments[exp_name])\n"]}
