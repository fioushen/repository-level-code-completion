{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\t# coding=utf-8\n\t\"\"\"The setup script.\"\"\"\n\timport os\n\tfrom setuptools import find_packages, setup  # type: ignore\n\twith open(\"README.md\") as readme_file:\n\t    long_description = readme_file.read()\n\tinstall_requires = [\n\t    \"numpy==1.23.2\",\n\t    \"torch@https://download.pytorch.org/whl/cu116/torch-1.12.1%2Bcu116-cp38-cp38-linux_x86_64.whl\",  # Specific version for CUDA 11.6\n", "    \"pytorch-lightning==1.7.3\",\n\t    \"torchvision@https://download.pytorch.org/whl/cu116/torchvision-0.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl\",  # Specific version for CUDA 11.6'\n\t    \"pydantic==1.9.1\",\n\t    \"tensorboard>=2.9\",\n\t    \"mlflow>=1.26\",\n\t    \"hydra-core==1.2.0\",\n\t    \"python-dotenv>=0.20\",\n\t    \"tqdm==4.64\",\n\t    \"rich>=12.4\",\n\t    \"hydra-submitit-launcher==1.2.0\",\n", "    \"hydra-optuna-sweeper==1.2.0\",\n\t    \"hydra-colorlog==1.2.0\",\n\t    \"matplotlib>=3.5.3\",\n\t    \"seaborn>=0.11.2\",\n\t    \"pandas>=1.4.1\",\n\t    \"scikit-learn>=1.1.2\",\n\t]\n\t# X-transformers has an 'entmax' package which is broken -- requires torch for install, which will not be installed yet\n\t# fails on tox\n\tif not os.environ.get(\"IS_TOX\", True):\n", "    install_requires.append(\"x-transformers==0.22.3\")\n\tsetup(\n\t    author=\"\",\n\t    long_description=long_description,\n\t    long_description_content_type=\"text/markdown\",\n\t    python_requires=\">=3.9\",\n\t    classifiers=[\n\t        \"Development Status :: 1 - Planning\",\n\t        \"Natural Language :: English\",\n\t        \"Programming Language :: Python :: 3\",\n", "    ],\n\t    description=\"STAPLER\",\n\t    install_requires=install_requires,\n\t    extras_require={\n\t        \"dev\": [\"pytest\", \"numpydoc\", \"pylint\", \"black==22.3.0\"],\n\t    },\n\t    license=\"\",\n\t    include_package_data=True,\n\t    name=\"stapler\",\n\t    test_suite=\"tests\",\n", "    url=\"https://github.com/NKI-AI/STAPLER\",\n\t    py_modules=[\"stapler\"]\n\t    # version=version,\n\t    # zip_safe=False,\n\t)\n"]}
{"filename": "tools/train_5_fold.py", "chunked_list": ["import dotenv\n\timport hydra\n\tfrom omegaconf import DictConfig\n\t# load environment variables from `.env` file if it exists\n\t# recursively searches for `.env` in all folders starting from work dir\n\tdotenv.load_dotenv(override=True)\n\t@hydra.main(\n\t    config_path=\"../config\",\n\t    config_name=\"train_5_fold.yaml\",\n\t    version_base=\"1.2\",\n", ")\n\tdef main(config: DictConfig):\n\t    # needed to import stapler\n\t    import sys\n\t    sys.path.append('../')\n\t    # Imports can be nested inside @hydra.main to optimize tab completion\n\t    # https://github.com/facebookresearch/hydra/issues/934\n\t    from stapler.entrypoints import stapler_entrypoint\n\t    from stapler.utils.io_utils import extras, print_config, validate_config, ensemble_5_fold_output\n\t    # Validate config -- Fails if there are mandatory missing values\n", "    validate_config(config)\n\t    # Applies optional utilities\n\t    extras(config)\n\t    if config.get(\"print_config\"):\n\t        print_config(config, resolve=True)\n\t    result_dict = {}\n\t    for i in range(5):\n\t        result_dict[i] = stapler_entrypoint(config, i)\n\t    if config.test_after_training:\n\t        ensemble_5_fold_output(output_path=config.paths.output_dir, test_dataset_path=config.datamodule.test_data_path)\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tools/pretrain.py", "chunked_list": ["import dotenv\n\timport hydra\n\tfrom omegaconf import DictConfig\n\t# load environment variables from `.env` file if it exists\n\t# recursively searches for `.env` in all folders starting from work dir\n\tdotenv.load_dotenv(override=True)\n\t@hydra.main(\n\t    config_path=\"../config\",\n\t    config_name=\"pretrain.yaml\",\n\t    version_base=\"1.2\",\n", ")\n\tdef main(config: DictConfig):\n\t    # needed to import stapler\n\t    import sys\n\t    sys.path.append('../')\n\t    # Imports can be nested inside @hydra.main to optimize tab completion\n\t    # https://github.com/facebookresearch/hydra/issues/934\n\t    from stapler.entrypoints import stapler_entrypoint\n\t    from stapler.utils.io_utils import extras, print_config, validate_config\n\t    # Validate config -- Fails if there are mandatory missing values\n", "    validate_config(config)\n\t    # Applies optional utilities\n\t    extras(config)\n\t    if config.get(\"print_config\"):\n\t        print_config(config, resolve=True)\n\t    # Train model\n\t    return stapler_entrypoint(config)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tools/test.py", "chunked_list": ["import dotenv\n\timport hydra\n\tfrom omegaconf import DictConfig\n\t# load environment variables from `.env` file if it exists\n\t# recursively searches for `.env` in all folders starting from work dir\n\tdotenv.load_dotenv(override=True)\n\t@hydra.main(\n\t    config_path=\"../config\",\n\t    config_name=\"test.yaml\",\n\t    version_base=\"1.2\",\n", ")\n\tdef main(config: DictConfig):\n\t    # needed to import stapler\n\t    import sys\n\t    sys.path.append('../')\n\t    # Imports can be nested inside @hydra.main to optimize tab completion\n\t    # https://github.com/facebookresearch/hydra/issues/934\n\t    from stapler.entrypoints import stapler_entrypoint\n\t    from stapler.utils.io_utils import extras, print_config, validate_config, ensemble_5_fold_output\n\t    # Validate config -- Fails if there are mandatory missing values\n", "    validate_config(config)\n\t    # Applies optional utilities\n\t    extras(config)\n\t    if config.get(\"print_config\"):\n\t        print_config(config, resolve=True)\n\t    stapler_entrypoint(config)\n\t    ensemble_5_fold_output(output_path=config.paths.output_dir,  test_dataset_path=config.datamodule.test_data_path)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "stapler/entrypoints.py", "chunked_list": ["from __future__ import annotations\n\timport os\n\tfrom typing import Any, List, Optional\n\timport hydra\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import Callback, LightningDataModule, LightningModule, Trainer, seed_everything\n\tfrom pytorch_lightning.loggers import LightningLoggerBase\n\tfrom torch.utils.data import DataLoader, Dataset\n\tfrom stapler.utils.io_utils import get_pylogger, log_hyperparameters, save_output\n\tlogger = get_pylogger(__name__)\n", "def stapler_entrypoint(config: DictConfig, fold: Optional[int] = None) -> Optional[float]:\n\t    \"\"\"Contains the training pipeline. Can additionally evaluate model on a testset, using best\n\t    weights achieved during training.\n\t    Args:\n\t        fold: The fold to train on. If None, the entire dataset is used.\n\t        config (DictConfig): Configuration composed by Hydra.\n\t    Returns:\n\t        Optional[float]: Metric score for hyperparameter optimization.\n\t    \"\"\"\n\t    # Set seed for random number generators in pytorch, numpy and python.random\n", "    if config.get(\"seed\"):\n\t        seed_everything(config.seed, workers=True)\n\t    # Init lightning datamodule\n\t    if config.datamodule.get(\"_target_\"):\n\t        logger.info(f\"Instantiating datamodule <{config.datamodule._target_}>\")\n\t        if fold is not None:\n\t            datamodule: LightningDataModule = hydra.utils.instantiate(config.datamodule, fold=fold)\n\t        else:\n\t            datamodule: LightningDataModule = hydra.utils.instantiate(config.datamodule)\n\t        datamodule.setup()\n", "        max_seq_len = datamodule.train_dataset.max_seq_len\n\t    else:\n\t        raise NotImplementedError(f\"No datamodule target found in <{config.datamodule}>\")\n\t    # Init transforms\n\t    transforms: dict[str, Any] | None = None\n\t    if \"transforms\" in config:\n\t        transforms = {}\n\t        for stage in config.transforms:\n\t            if not config.transforms[stage].get(\"_target_\"):\n\t                raise NotImplementedError(f\"No augmentations target found in <{config.transforms[stage]}>\")\n", "            logger.info(f\"Instantiating {stage} augmentations <{config.transforms[stage]._target_}>\")  # noqa\n\t            transforms[stage] = hydra.utils.instantiate(\n\t                config.transforms[stage],\n\t                mask_token_id=datamodule.tokenizer.mask_token_id,\n\t                pad_token_id=datamodule.tokenizer.pad_token_id,\n\t                mask_ignore_token_ids=[datamodule.tokenizer.cls_token_id,\n\t                                       datamodule.tokenizer.sep_token_id,\n\t                                       datamodule.tokenizer.unk_token_id],\n\t                _convert_=\"partial\",\n\t            )\n", "    # Init transformer\n\t    if config.model.get(\"_target_\"):\n\t        logger.info(f\"Instantiating model <{config.model._target_}>\")\n\t        model: LightningModule = hydra.utils.instantiate(config.model, max_seq_len=max_seq_len)\n\t    else:\n\t        raise NotImplementedError(f\"No model target found in <{config.model}>\")\n\t    # Init Loss\n\t    if config.loss.get(\"_target_\"):\n\t        logger.info(f\"Instantiating loss <{config.loss._target_}>\")\n\t        loss: LightningModule = hydra.utils.instantiate(config.loss)\n", "    else:\n\t        raise NotImplementedError(f\"No loss target found in <{config.loss}>\")\n\t    # Init lightning model\n\t    if config.lit_module.get(\"_target_\"):\n\t        logger.info(f\"Instantiating model <{config.lit_module._target_}>\")\n\t        model: LightningModule = hydra.utils.instantiate(\n\t            config.lit_module, model=model, loss=loss, transforms=transforms\n\t        )\n\t    else:\n\t        raise NotImplementedError(f\"No model target found in <{config.lit_module}>\")\n", "    # Init lightning callbacks\n\t    callbacks: List[Callback] = []\n\t    if \"callbacks\" in config:\n\t        for _, cb_conf in config.callbacks.items():\n\t            if \"_target_\" in cb_conf:\n\t                logger.info(f\"Instantiating callback <{cb_conf._target_}>\")\n\t                callbacks.append(hydra.utils.instantiate(cb_conf))\n\t    # Init lightning loggers\n\t    lightning_loggers: List[LightningLoggerBase] = []\n\t    if \"logger\" in config:\n", "        for _, lg_conf in config.logger.items():\n\t            if \"_target_\" in lg_conf:\n\t                logger.info(f\"Instantiating logger <{lg_conf._target_}>\")\n\t                lightning_loggers.append(hydra.utils.instantiate(lg_conf))\n\t    # Init lightning trainer\n\t    if config.trainer.get(\"_target_\"):\n\t        logger.info(f\"Instantiating trainer <{config.trainer._target_}>\")\n\t        trainer: Trainer = hydra.utils.instantiate(\n\t            config.trainer, callbacks=callbacks, logger=lightning_loggers, _convert_=\"partial\"\n\t        )\n", "    else:\n\t        raise NotImplementedError(f\"No trainer target found in <{config.trainer}>\")\n\t    # Send some parameters from config to all lightning loggers\n\t    logger.info(\"Logging hyperparameters...\")\n\t    log_hyperparameters(config=config, model=model, trainer=trainer)\n\t    # Train the model\n\t    if config.get(\"train\"):\n\t        logger.info(\"Starting training...\")\n\t        trainer.fit(model=model, datamodule=datamodule)\n\t    # Get metric score for hyperparameter optimization\n", "    optimized_metric = config.get(\"optimized_metric\")\n\t    if optimized_metric and optimized_metric not in trainer.callback_metrics:\n\t        raise Exception(\n\t            \"Metric for hyperparameter optimization not found. \"\n\t            \"Make sure the `optimized_metric` in `hparams_search` config is correct.\"\n\t        )\n\t    score = trainer.callback_metrics.get(optimized_metric)\n\t    # Test the model\n\t    if config.get(\"test_after_training\"):\n\t        ckpt_path = \"best\" if config.get(\"train\") else None\n", "        logger.info(\"Starting testing!\")\n\t        output = trainer.predict(model=model, datamodule=datamodule, ckpt_path=ckpt_path, return_predictions=True)\n\t        save_output(output=output, path=config.paths.output_dir, append=str(fold))\n\t    elif config.get(\"test_from_ckpt\"):\n\t        logger.info(\"Starting testing!\")\n\t        ckpt_names = [file_name for file_name in os.listdir(config.test_from_ckpt_path) if file_name.endswith(\".ckpt\")]\n\t        if len(ckpt_names) != 5:\n\t            raise Exception(\"There should be 5 checkpoints in the `config.test_from_ckpt_path` directory.\")\n\t        for i, ckpt_name in enumerate(ckpt_names):\n\t            checkpoint = os.path.join(config.test_from_ckpt_path, ckpt_name)\n", "            output = trainer.predict(model=model, datamodule=datamodule, ckpt_path=checkpoint, return_predictions=True)\n\t            save_output(output=output, path=config.paths.output_dir, append=str(i))\n\t    # Make sure everything closed properly\n\t    logger.info(\"Finalizing!\")\n\t    # Print path to best checkpoint\n\t    if not config.trainer.get(\"fast_dev_run\") and config.get(\"train\"):\n\t        logger.info(f\"Best model ckpt at {trainer.checkpoint_callback.best_model_path}\")\n\t    # Return metric score for hyperparameter optimization\n\t    return score\n"]}
{"filename": "stapler/loss.py", "chunked_list": ["from __future__ import annotations\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import Any, Union\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.nn.functional import cross_entropy\n\tclass LossFactory(nn.Module):\n\t    \"\"\"Loss factory to construct the total loss.\"\"\"\n\t    def __init__(\n\t        self,\n", "        losses: list,\n\t    ):\n\t        \"\"\"\n\t        Parameters\n\t        ----------\n\t        losses : list\n\t            List of losses which are functions which accept `(input, batch, weight)`. batch will be a dict(str,Any) containing\n\t            for instance the labels and any other needed data. The weight will be applied per loss.\n\t        \"\"\"\n\t        super().__init__()\n", "        self._losses = []\n\t        for loss in losses:\n\t            self._losses += list(loss.values())\n\t        self._weights = [torch.tensor(loss.weight) for loss in self._losses]\n\t    def forward(self, input: torch.Tensor, batch: dict[str, Any]):\n\t        total_loss = sum(\n\t            [\n\t                weight.to(batch[\"input\"].device) * curr_loss(input, batch)\n\t                for weight, curr_loss in zip(self._weights, self._losses)\n\t            ]\n", "        )\n\t        return total_loss\n\t# abstract class for losses\n\tclass Loss(ABC):\n\t    def __init__(self, weight: float = 1.0):\n\t        super().__init__()\n\t        self.weight = weight\n\t    @abstractmethod\n\t    def __call__(self, input: torch.Tensor, batch: dict[str, Any]):\n\t        pass\n", "class MLMLoss(Loss):\n\t    def __init__(self, weight: float = 1.0, pad_token_id: int = 0):\n\t        super().__init__(weight)\n\t        self.mlm_loss = cross_entropy\n\t        self.pad_token_id = pad_token_id\n\t    def __call__(self, input: dict[str, torch.Tensor], batch: dict[str, Any]):\n\t        pred_mlm = input[\"mlm_logits\"].transpose(1, 2)\n\t        loss = self.mlm_loss(pred_mlm, batch[\"mlm_labels\"], ignore_index=self.pad_token_id)\n\t        return loss\n\tclass CLSLoss(Loss):\n", "    def __init__(self, weight: float = 1.0):\n\t        super().__init__(weight)\n\t        self.cls_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n\t    def __call__(self, input: dict[str, torch.Tensor], batch: dict[str, Any]):\n\t        pred_cls = input[\"cls_logits\"]\n\t        loss = self.cls_loss(pred_cls, batch[\"cls_labels\"])\n\t        return loss\n"]}
{"filename": "stapler/__init__.py", "chunked_list": []}
{"filename": "stapler/transforms/masking.py", "chunked_list": ["\"\"\"Create a class that masks the input data. In particular the input will be a tokenized sequence of amino acids. The masking will be done by replacing the amino acids with a mask token.\"\"\"\n\tfrom __future__ import annotations\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom stapler.transforms.transforms import Transform\n\tclass Masking(Transform):\n\t    def __init__(\n\t        self,\n\t        mask_token_id: int,\n", "        pad_token_id: int,\n\t        mask_prob: float,\n\t        replace_prob: float,\n\t        mask_ignore_token_ids: list[int] | None = None,\n\t        **kwargs,\n\t    ):\n\t        super().__init__(**kwargs)\n\t        self.mask_token_id = mask_token_id\n\t        self.pad_token_id = pad_token_id\n\t        self.mask_ignore_token_ids = mask_ignore_token_ids or []\n", "        self.mask_prob = mask_prob\n\t        self.replace_prob = replace_prob\n\t    def __call__(self, input_dict: dict[str, torch.Tensor]):\n\t        \"\"\"Mask the input data.\n\t        Args:\n\t            x (torch.Tensor): The input data, consisting of batches of tokenized sequences.\n\t        Returns:\n\t            tuple: A tuple containing masked_input, labels, and mask_indices.\n\t        \"\"\"\n\t        input_dict[\"original_input\"] = input_dict[\"input\"].clone()\n", "        x = input_dict[\"input\"]\n\t        x = x.clone()\n\t        mask = torch.rand(x.shape) < self.mask_prob\n\t        mask = mask.to(x.device)\n\t        # Exclude [PAD] token and tokens in mask_ignore_token_ids from being masked\n\t        for token_id in [self.pad_token_id] + self.mask_ignore_token_ids:\n\t            token_id = torch.tensor(token_id, device=x.device)\n\t            mask &= x != token_id\n\t        # Replace tokens with [MASK] based on replace_prob\n\t        replace = torch.rand(x.shape) < self.replace_prob\n", "        replace = replace.to(x.device)\n\t        masked_input = x.clone()\n\t        masked_input[mask & replace] = self.mask_token_id\n\t        # Create labels tensor with [PAD] token for unmasked positions\n\t        labels = x.clone()\n\t        labels[~mask] = self.pad_token_id\n\t        # Get mask_indices\n\t        mask_indices = torch.nonzero(mask, as_tuple=True)\n\t        input_dict[\"input\"] = masked_input\n\t        input_dict[\"mlm_labels\"] = labels\n", "        input_dict[\"mlm_mask_indices\"] = mask_indices\n\t        return input_dict\n"]}
{"filename": "stapler/transforms/transforms.py", "chunked_list": ["\"\"\"Contains the TransformFactory class, which is used to instantiate the correct transform\"\"\"\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import Any, Dict, List, Optional, Union\n\timport torch\n\t# Abstract class for the transforms\n\tclass Transform(ABC):\n\t    def __init__(self, **kwargs) -> None:\n\t        pass\n\t    @abstractmethod\n\t    def __call__(self, data: torch.Tensor) -> Any:\n", "        pass\n\t# Class for padding the sequences to a fixed length\n\tclass PadSequence(Transform):\n\t    def __init__(self, pad_token_id: int, max_seq_len: int) -> None:\n\t        self.pad_token_id = pad_token_id\n\t        self.max_seq_len = max_seq_len\n\t    def __call__(self, data: torch.Tensor) -> torch.Tensor:\n\t        # Pad the sequences to the max length\n\t        data = torch.nn.functional.pad(data, (0, self.max_seq_len - data.shape[0]), \"constant\", self.pad_token_id)\n\t        return data\n", "class TransformFactory:\n\t    def __init__(self, transforms: list, **kwargs) -> None:\n\t        self.transforms = []\n\t        for transform in transforms:\n\t            self.transforms.append(transform(**kwargs))\n\t    def __call__(self, data: torch.Tensor) -> Any:\n\t        for transform in self.transforms:\n\t            data = transform(data)\n\t        return data\n"]}
{"filename": "stapler/utils/io_utils.py", "chunked_list": ["\"\"\"Here we want to place any Input/Output utilities\"\"\"\n\timport os\n\timport json\n\timport logging\n\timport warnings\n\timport pandas as pd\n\tfrom typing import Any, Sequence\n\timport pytorch_lightning as pl\n\timport rich\n\timport rich.syntax\n", "import rich.tree\n\tfrom omegaconf import DictConfig, ListConfig, OmegaConf\n\tfrom omegaconf.errors import InterpolationKeyError\n\tfrom pytorch_lightning.utilities import rank_zero_only\n\tdef get_pylogger(name=__name__) -> logging.Logger:\n\t    \"\"\"Initializes multi-GPU-friendly python command line logger.\"\"\"\n\t    logger = logging.getLogger(name)\n\t    # this ensures all logging levels get marked with the rank zero decorator\n\t    # otherwise logs would get multiplied for each GPU process in multi-GPU setup\n\t    logging_levels = (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\")\n", "    for level in logging_levels:\n\t        setattr(logger, level, rank_zero_only(getattr(logger, level)))\n\t    return logger\n\t@rank_zero_only\n\tdef log_hyperparameters(\n\t        config: DictConfig,\n\t        model: pl.LightningModule,\n\t        trainer: pl.Trainer,\n\t) -> None:\n\t    \"\"\"Controls which config parts are saved by Lightning loggers.\n", "    Additionaly saves:\n\t    - number of model parameters\n\t    \"\"\"\n\t    if not trainer.logger:\n\t        return\n\t    hparams = {}\n\t    # choose which parts of hydra config will be saved to loggers\n\t    hparams[\"model\"] = config[\"model\"]\n\t    # save number of model parameters\n\t    hparams[\"model/params/total\"] = sum(p.numel() for p in model.parameters())\n", "    hparams[\"model/params/trainable\"] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\t    hparams[\"model/params/non_trainable\"] = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n\t    hparams[\"datamodule\"] = config[\"datamodule\"]\n\t    hparams[\"trainer\"] = config[\"trainer\"]\n\t    if \"seed\" in config:\n\t        hparams[\"seed\"] = config[\"seed\"]\n\t    if \"callbacks\" in config:\n\t        hparams[\"callbacks\"] = config[\"callbacks\"]\n\t    # send hparams to all loggers\n\t    trainer.logger.log_hyperparams(hparams)\n", "def debug_function(x: float):\n\t    \"\"\"\n\t    Function to use for debugging (e.g. github workflow testing)\n\t    :param x:\n\t    :return: x^2\n\t    \"\"\"\n\t    return x ** 2\n\tdef validate_config(cfg: Any):\n\t    if isinstance(cfg, ListConfig):\n\t        for x in cfg:\n", "            validate_config(x)\n\t    elif isinstance(cfg, DictConfig):\n\t        for name, v in cfg.items():\n\t            if name == \"hydra\":\n\t                print(\"Skipped validating hydra native configs\")\n\t                continue\n\t            try:\n\t                validate_config(v)\n\t            except InterpolationKeyError:\n\t                print(f\"Skipped validating {name}: {v}\")\n", "                continue\n\t@rank_zero_only\n\tdef print_config(\n\t        config: DictConfig,\n\t        fields: Sequence[str] = (\n\t                \"trainer\",\n\t                \"model\",\n\t                \"experiment\",\n\t                \"datamodule\",\n\t                \"callbacks\",\n", "                \"logger\",\n\t                \"test_after_training\",\n\t                \"seed\",\n\t                \"name\",\n\t        ),\n\t        resolve: bool = True,\n\t) -> None:\n\t    \"\"\"Prints content of DictConfig using Rich library and its tree structure.\n\t    Args:\n\t        config (DictConfig): Configuration composed by Hydra.\n", "        fields (Sequence[str], optional): Determines which main fields from config will\n\t        be printed and in what order.\n\t        resolve (bool, optional): Whether to resolve reference fields of DictConfig.\n\t    \"\"\"\n\t    style = \"dim\"\n\t    tree = rich.tree.Tree(\"CONFIG\", style=style, guide_style=style)\n\t    for field in fields:\n\t        branch = tree.add(field, style=style, guide_style=style)\n\t        config_section = config.get(field)\n\t        branch_content = str(config_section)\n", "        if isinstance(config_section, DictConfig):\n\t            branch_content = OmegaConf.to_yaml(config_section, resolve=resolve)\n\t        branch.add(rich.syntax.Syntax(branch_content, \"yaml\"))\n\t    rich.print(tree)\n\t    with open(\"config_tree.log\", \"w\") as fp:\n\t        rich.print(tree, file=fp)\n\tdef extras(config: DictConfig) -> None:\n\t    \"\"\"A couple of optional utilities, controlled by main config file:\n\t    - disabling warnings\n\t    - forcing debug friendly configuration\n", "    - verifying experiment name is set when running in experiment mode\n\t    Modifies DictConfig in place.\n\t    Args:\n\t        config (DictConfig): Configuration composed by Hydra.\n\t    \"\"\"\n\t    logger = get_pylogger(__name__)\n\t    # disable python warnings if <config.ignore_warnings=True>\n\t    if config.get(\"ignore_warnings\"):\n\t        logger.info(\"Disabling python warnings! <config.ignore_warnings=True>\")\n\t        warnings.filterwarnings(\"ignore\")\n", "    # verify experiment name is set when running in experiment mode\n\t    if config.get(\"enforce_tags\") and (not config.get(\"tags\") or config.get(\"tags\") == [\"dev\"]):\n\t        logger.info(\n\t            \"Running in experiment mode without tags specified\"\n\t            \"Use `python run.py experiment=some_experiment tags=['some_tag',...]`, or change it in the experiment yaml\"\n\t        )\n\t        logger.info(\"Exiting...\")\n\t        exit()\n\t    # force debugger friendly configuration if <config.trainer.fast_dev_run=True>\n\t    # debuggers don't like GPUs and multiprocessing\n", "    if config.trainer.get(\"fast_dev_run\"):\n\t        logger.info(\"Forcing debugger friendly configuration! <config.trainer.fast_dev_run=True>\")\n\t        if config.trainer.get(\"gpus\"):\n\t            config.trainer.gpus = 0\n\t        if config.datamodule.get(\"pin_memory\"):\n\t            config.datamodule.pin_memory = False\n\t        if config.datamodule.get(\"num_workers\"):\n\t            config.datamodule.num_workers = 0\n\tdef save_output(output, path, append):\n\t    logger = get_pylogger(__name__)\n", "    logger.info(\"Saving test results to json at {}\".format(path))\n\t    json_path = os.path.join(path, f\"test_results{append}.json\")\n\t    with open(json_path, \"w\") as f:\n\t        json.dump(output, f)\n\t    logger.info(f\"Test results saved to {json_path}\")\n\tdef ensemble_5_fold_output(output_path, test_dataset_path):\n\t    logger = get_pylogger(__name__)\n\t    logger.info(\"Checking 5-fold test results from {}\".format(output_path))\n\t    df_test = pd.read_csv(test_dataset_path)\n\t    pred_cls_mean = [0] * len(df_test)\n", "    for i in range(5):\n\t        with open(os.path.join(output_path, f'test_results{i}.json')) as f:\n\t            data = json.load(f)\n\t            pred_cls = []\n\t            label_cls = []\n\t            for batch in data:\n\t                pred_cls.append(batch['preds_cls'])\n\t                label_cls.append(batch['labels_cls'])\n\t            pred_cls = [item for sublist in pred_cls for item in sublist]\n\t            label_cls = [item for sublist in label_cls for item in sublist]\n", "            if df_test['label_true_pair'].tolist() != label_cls:\n\t                raise ValueError('Labels from the test dataset are not same as the labels in the model output')\n\t            pred_cls_mean = [x + y / 5 for x, y in zip(pred_cls_mean, pred_cls)]\n\t    df_test['pred_cls'] = pred_cls_mean\n\t    df_test.to_csv(os.path.join(output_path, f'predictions_5_fold_ensamble.csv'))\n\t    logger.info(f\"Ensembled test results saved to {output_path}\")\n"]}
{"filename": "stapler/utils/model_utils.py", "chunked_list": ["import numpy as np\n\timport torch\n\tclass CosineWarmupScheduler(torch.optim.lr_scheduler._LRScheduler):\n\t    def __init__(self, optimizer, warmup, max_iters):\n\t        self.warmup = warmup\n\t        self.max_num_iters = max_iters\n\t        super().__init__(optimizer)\n\t    def get_lr(self):\n\t        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n\t        return [base_lr * lr_factor for base_lr in self.base_lrs]\n", "    def get_lr_factor(self, epoch):\n\t        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n\t        if epoch <= self.warmup:\n\t            lr_factor *= epoch * 1.0 / self.warmup\n\t        return lr_factor\n"]}
{"filename": "stapler/utils/__init__.py", "chunked_list": []}
{"filename": "stapler/utils/masking_utils.py", "chunked_list": ["import math\n\tfrom functools import reduce\n\timport torch\n\tclass InputMasker:\n\t    def __init__(self, mask_prob, replace_prob, mask_token_id, pad_token_id, mask_ignore_token_ids):\n\t        self.mask_prob = mask_prob\n\t        self.replace_prob = replace_prob\n\t        self.mask_token_id = mask_token_id\n\t        self.pad_token_id = pad_token_id\n\t        self.mask_ignore_token_ids = mask_ignore_token_ids\n", "    def mask_input(self, input):\n\t        # do not mask [PAD] tokens, or any other tokens in the tokens designated to be excluded ([CLS], [SEP])\n\t        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n\t        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n\t        # get mask indices\n\t        mask_indices = torch.nonzero(mask, as_tuple=True)\n\t        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n\t        masked_input = input.clone().detach()\n\t        # [mask] input\n\t        replace_prob = prob_mask_like(input, self.replace_prob)\n", "        masked_input = masked_input.masked_fill(mask * replace_prob, self.mask_token_id)\n\t        # mask out any tokens to padding tokens that were not going to be masked\n\t        labels = input.masked_fill(~mask, self.pad_token_id)\n\t        return masked_input, labels, mask_indices\n\t    def __call__(self, batch):\n\t        batch[\"original_input\"] = batch[\"input\"]\n\t        batch[\"input\"], batch[\"mlm_labels\"], batch[\"mlm_mask_indices\"] = self.mask_input(batch[\"input\"])\n\t        return batch\n\tdef prob_mask_like(t, prob):\n\t    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n", "def mask_with_tokens(t, token_ids):\n\t    \"\"\"\n\t    :param t: input tensor with dimensions (BATCH, AA_SEQUENCE, AA_dimension)\n\t    :param token_ids: token ids that are excluded from masking\n\t    :return:\n\t    \"\"\"\n\t    init_no_mask = torch.full_like(t, False, dtype=torch.bool)  # copy shape and init with False\n\t    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)  # get the matching indices and make True\n\t    return mask\n\tdef get_mask_subset_with_prob(mask, prob):\n", "    batch, seq_len, device = *mask.shape, mask.device  # shape of the mask and the device\n\t    max_masked = math.ceil(prob * seq_len)  # max n of aa masked per seq\n\t    num_tokens = mask.sum(\n\t        dim=-1, keepdim=True\n\t    )  # number of aas that are allowed to be masked (TRUE if allowed to be masked)\n\t    mask_excess = mask.cumsum(dim=-1) > (num_tokens * prob).ceil()\n\t    mask_excess = mask_excess[:, :max_masked]  # prevent masking more than allowed\n\t    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n\t    _, sampled_indices = rand.topk(max_masked, dim=-1)\n\t    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)  # indices of the aa's that will be masked\n", "    new_mask = torch.zeros((batch, seq_len + 1), device=device)  # init empty mask\n\t    new_mask.scatter_(-1, sampled_indices, 1)  # 1 for every index that is masked, 0 if not\n\t    return new_mask[:, 1:].bool()\n"]}
{"filename": "stapler/datamodule/pretrain_datamodule.py", "chunked_list": ["from __future__ import annotations\n\tfrom pathlib import Path\n\tfrom typing import Any, Dict, Optional, Union\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader\n\tfrom stapler.datamodule.components.pretrain_dataset import PretrainDatasetTcrEpitope\n\tfrom stapler.datamodule.components.tokenizers import Tokenizer\n\t# A minimialistic lightning datamodule for the pretraining\n\tclass PretrainDataModuleTcrEpitope(LightningDataModule):\n", "    def __init__(\n\t        self,\n\t        tcrs_path: Union[str, Path],\n\t        epitopes_path: Union[str, Path],\n\t        tokenizer: Tokenizer,\n\t        transform: Optional[Any] = None,\n\t        padder: Optional[Any] = None,\n\t        batch_size: int = 32,\n\t        num_workers: int = 4,\n\t        pin_memory: bool = True,\n", "        persistent_workers: bool = True,\n\t    ) -> None:\n\t        super().__init__()\n\t        # Save the parameters\n\t        self.save_hyperparameters()\n\t        self.tcrs_path = tcrs_path\n\t        self.epitopes_path = epitopes_path\n\t        self.transform = transform\n\t        self.tokenizer = tokenizer\n\t        self.padder = padder\n", "    def setup(self, stage: Optional[str] = None) -> None:\n\t        self.train_dataset = PretrainDatasetTcrEpitope(\n\t            self.tcrs_path,\n\t            self.epitopes_path,\n\t            self.tokenizer,\n\t            self.transform,\n\t            self.padder,\n\t        )\n\t    def train_dataloader(self) -> DataLoader:\n\t        return DataLoader(\n", "            self.train_dataset,\n\t            batch_size=self.hparams.batch_size,\n\t            shuffle=True,\n\t            num_workers=self.hparams.num_workers,\n\t            persistent_workers=self.hparams.num_workers > 0,\n\t            pin_memory=self.hparams.pin_memory,\n\t        )\n"]}
{"filename": "stapler/datamodule/train_datamodule.py", "chunked_list": ["from __future__ import annotations\n\tfrom pathlib import Path\n\tfrom typing import Any, Dict, Optional, Union\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\tfrom stapler.datamodule.components.tokenizers import Tokenizer\n\tfrom stapler.datamodule.components.train_dataset import TrainDataset\n\tfrom stapler.datamodule.dataloader.general_dataloader import create_dataloader\n\t# A minimialistic lightning datamodule for the training\n", "class TrainDataModule(LightningDataModule):\n\t    def __init__(\n\t        self,\n\t        train_data_path: Union[str, Path],\n\t        test_data_path: Union[str, Path],\n\t        tokenizer: Tokenizer,\n\t        transform: Optional[Any] = None,\n\t        padder: Optional[Any] = None,\n\t        fold: Optional[int] = None,\n\t        batch_size: int = 32,\n", "        num_workers: int = 4,\n\t        pin_memory: bool = True,\n\t        persistent_workers: bool = True,\n\t        weighted_class_sampling: bool = False,\n\t        weighted_epitope_sampling: bool = False,\n\t    ) -> None:\n\t        super().__init__()\n\t        # Save the parameters\n\t        self.save_hyperparameters()\n\t        self.train_data_path = Path(train_data_path)\n", "        self.test_data_path = Path(test_data_path)\n\t        self.transform = transform\n\t        self.tokenizer = tokenizer\n\t        self.padder = padder\n\t        self.fold = fold\n\t    def setup(self, stage: Optional[str] = None) -> None:\n\t        if self.fold is not None:\n\t            self.val_data_path_fold = self.train_data_path.parent / f\"{self.train_data_path.stem.strip('.csv')}_val-fold{self.fold}.csv\"\n\t            self.train_data_path_fold = self.train_data_path.parent / f\"{self.train_data_path.stem.strip('.csv')}_train-fold{self.fold}.csv\"\n\t            self.val_dataset = TrainDataset(\n", "                self.val_data_path_fold,\n\t                self.tokenizer,\n\t                None,\n\t                self.padder,\n\t            )\n\t            self.train_dataset = TrainDataset(\n\t                self.train_data_path_fold,\n\t                self.tokenizer,\n\t                self.transform,\n\t                self.padder,\n", "            )\n\t        else:\n\t            self.train_dataset = TrainDataset(\n\t                self.train_data_path,\n\t                self.tokenizer,\n\t                self.transform,\n\t                self.padder,\n\t            )\n\t        self.test_dataset = TrainDataset(\n\t            self.test_data_path,\n", "            self.tokenizer,\n\t            None,\n\t            self.padder,\n\t        )\n\t    def train_dataloader(self) -> DataLoader:\n\t        train_dataloader = create_dataloader(\n\t            dataset=self.train_dataset,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n\t            pin_memory=self.hparams.pin_memory,\n", "            persistent_workers=self.hparams.num_workers > 0,\n\t            weighted_class_sampling=self.hparams.weighted_class_sampling,\n\t            weighted_epitope_sampling=self.hparams.weighted_epitope_sampling\n\t        )\n\t        return train_dataloader\n\t    def predict_dataloader(self) -> DataLoader:\n\t        predict_dataloader = create_dataloader(\n\t            dataset=self.test_dataset,\n\t            batch_size=self.hparams.batch_size,\n\t            num_workers=self.hparams.num_workers,\n", "            pin_memory=self.hparams.pin_memory,\n\t            persistent_workers=self.hparams.num_workers > 0,\n\t            weighted_class_sampling=False,\n\t            weighted_epitope_sampling=False\n\t        )\n\t        return predict_dataloader\n\t    def val_dataloader(self) -> DataLoader | None:\n\t        if self.fold is not None:\n\t            # create a validation data_loader using the same parameters as the test data_loader\n\t            val_dataloader = create_dataloader(\n", "                dataset=self.val_dataset,\n\t                batch_size=self.hparams.batch_size,\n\t                num_workers=self.hparams.num_workers,\n\t                pin_memory=self.hparams.pin_memory,\n\t                persistent_workers=self.hparams.num_workers > 0,\n\t                weighted_class_sampling=False,\n\t                weighted_epitope_sampling=False\n\t            )\n\t            return val_dataloader\n\t        else:\n", "            return None\n"]}
{"filename": "stapler/datamodule/__init__.py", "chunked_list": []}
{"filename": "stapler/datamodule/components/pretrain_dataset.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Any, Optional, Tuple, Union\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom torch.utils.data import Dataset\n\tfrom stapler.datamodule.components.tokenizers import Tokenizer\n\tclass PretrainDatasetTcrEpitope(Dataset):\n\t    def __init__(\n\t        self,\n", "        tcrs_path: Union[str, Path],\n\t        epitopes_path: Union[str, Path],\n\t        tokenizer: Tokenizer,\n\t        transform: Optional[Any] = None,\n\t        padder: Optional[Any] = None,\n\t    ) -> None:\n\t        tcr_data = pd.read_csv(tcrs_path)\n\t        epitope_data = pd.read_csv(epitopes_path)\n\t        self.tcr_df = tcr_data[[\"cdr3_alpha_aa\", \"cdr3_beta_aa\"]]\n\t        self.epitope_df = epitope_data[\"epitope_aa\"]\n", "        self.tcrs = self.tcr_df.to_numpy()\n\t        self.epitopes = self.epitope_df.to_numpy()\n\t        self.transform = transform\n\t        self.tokenizer = tokenizer\n\t        self.padder = padder(self.tokenizer.pad_token_id, self.max_seq_len)\n\t    # property for maximum sequence length\n\t    @property\n\t    def max_seq_len(self) -> dict[str, torch.Tensor]:\n\t        # max of tcrs[0] (strings) + max tcrs[1] (strings) + epitope (strings) + 3\n\t        return (\n", "            self.tcr_df[\"cdr3_alpha_aa\"].str.len().max()\n\t            + self.tcr_df[\"cdr3_beta_aa\"].str.len().max()\n\t            + self.epitope_df.str.len().max()\n\t            + 3\n\t        )\n\t    def __getitem__(self, index: int) -> dict[str, torch.Tensor]:\n\t        tcr_a, tcr_b = self.tcrs[index]\n\t        # Create random index for epitope\n\t        epitope = self.epitopes[np.random.randint(0, len(self.epitopes))]  # TODO check randomness per epoch\n\t        sample = \"[CLS] \" + \" \".join(tcr_a) + \" [SEP] \" + \" \".join(epitope) + \" [SEP] \" + \" \".join(tcr_b)\n", "        sample = self.tokenizer.encode(sample)\n\t        sample = torch.from_numpy(np.asarray(sample))\n\t        if self.transform:\n\t            sample = self.transform(sample)\n\t        if self.padder:\n\t            sample = self.padder(sample)\n\t        output_dict = {\"input\": sample}\n\t        return output_dict\n\t    def __len__(self) -> int:\n\t        return len(self.tcrs)\n"]}
{"filename": "stapler/datamodule/components/__init__.py", "chunked_list": []}
{"filename": "stapler/datamodule/components/train_dataset.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Any, Optional, Tuple, Union\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom torch.utils.data import Dataset\n\tfrom stapler.datamodule.components.tokenizers import Tokenizer\n\tclass TrainDataset(Dataset):\n\t    def __init__(\n\t        self,\n", "        train_data_path: Union[str, Path],\n\t        tokenizer: Tokenizer,\n\t        transform: Optional[Any] = None,\n\t        padder: Optional[Any] = None,\n\t    ) -> None:\n\t        train_data = pd.read_csv(train_data_path)\n\t        self.tcr_df = train_data[[\"full_seq_reconstruct_alpha_aa\", \"full_seq_reconstruct_beta_aa\"]]\n\t        self.epitope_df = train_data[\"epitope_aa\"]\n\t        self.labels = train_data[\"label_true_pair\"]\n\t        self.tcrs = self.tcr_df.to_numpy()\n", "        self.epitopes = self.epitope_df.to_numpy()\n\t        self.transform = transform\n\t        self.tokenizer = tokenizer\n\t        self.padder = padder(self.tokenizer.pad_token_id, self.max_seq_len)\n\t    # property for maximum sequence length\n\t    @property\n\t    def max_seq_len(self) -> dict[str, torch.Tensor]:\n\t        # max of tcrs[0] (strings) + max tcrs[1] (strings) + epitope (strings) + 3\n\t        return (\n\t            self.tcr_df[\"full_seq_reconstruct_alpha_aa\"].str.len().max()\n", "            + self.tcr_df[\"full_seq_reconstruct_beta_aa\"].str.len().max()\n\t            + self.epitope_df.str.len().max()\n\t            + 3\n\t        )\n\t    def __getitem__(self, index: int) -> dict[str, torch.Tensor]:\n\t        tcr_a, tcr_b = self.tcrs[index]\n\t        epitope = self.epitopes[index]\n\t        label = torch.tensor(int(self.labels[index]))\n\t        sample = \"[CLS] \" + \" \".join(tcr_a) + \" [SEP] \" + \" \".join(epitope) + \" [SEP] \" + \" \".join(tcr_b)\n\t        sample = self.tokenizer.encode(sample)\n", "        sample = torch.from_numpy(np.asarray(sample))\n\t        if self.transform:\n\t            sample = self.transform(sample)\n\t        if self.padder:\n\t            sample = self.padder(sample)\n\t        output_dict = {\"input\": sample, \"cls_labels\": label}\n\t        return output_dict\n\t    def __len__(self) -> int:\n\t        return len(self.tcrs)\n"]}
{"filename": "stapler/datamodule/components/tokenizers.py", "chunked_list": ["import re\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import List, Tuple, Union\n\tclass Tokenizer(ABC):\n\t    def __init__(self, add_special_tokens=True) -> None:\n\t        self.vocab_dict = {}\n\t        if add_special_tokens:\n\t            self.special_tokens = [\"[UNK]\", \"[SEP]\", \"[CLS]\", \"[MASK]\"]\n\t            self.pad_token = \"[PAD]\"\n\t        else:\n", "            self.special_tokens = []\n\t            raise RuntimeError(\"Not supported yet\")\n\t    @abstractmethod\n\t    def tokenize(self, text: str) -> List[str]:\n\t        pass\n\t    @abstractmethod\n\t    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n\t        pass\n\t    @abstractmethod\n\t    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n", "        pass\n\t    @abstractmethod\n\t    def encode(self, text: str) -> Tuple[List[int], List[int]]:\n\t        pass\n\tclass BasicTokenizer(Tokenizer):\n\t    def __init__(self, vocabulary: Union[str, List[str]], add_special_tokens=True) -> None:\n\t        super().__init__(add_special_tokens)\n\t        # Add the vocabulary to the vocab_dict\n\t        if isinstance(vocabulary, list):\n\t            # join the strings for each entry in the list\n", "            vocabulary = \"\".join(vocabulary)\n\t        self.vocabulary = vocabulary\n\t        # first token is pad token\n\t        self.vocab_dict = {self.pad_token: 0}\n\t        self.pad_token_id = self.vocab_dict[\"[PAD]\"]\n\t        self.vocab_dict.update({tok: i for i, tok in enumerate(self.vocabulary, len(self.vocab_dict))})\n\t        if add_special_tokens:\n\t            self.vocab_dict.update({tok: i for i, tok in enumerate(self.special_tokens, len(self.vocab_dict))})\n\t            self.unk_token_id = self.vocab_dict[\"[UNK]\"]\n\t            self.sep_token_id = self.vocab_dict[\"[SEP]\"]\n", "            self.cls_token_id = self.vocab_dict[\"[CLS]\"]\n\t            self.mask_token_id = self.vocab_dict[\"[MASK]\"]\n\t    def tokenize(self, text: str) -> List[str]:\n\t        substrings = text.split()\n\t        split_tokens = []\n\t        for substring in substrings:\n\t            if substring in self.vocab_dict:\n\t                split_tokens.append(substring)\n\t                continue\n\t            for charac in substring:\n", "                if charac.lower() in self.vocab_dict:\n\t                    split_tokens.append(charac.lower())\n\t                else:\n\t                    split_tokens.extend([\"[UNK]\"])\n\t        return split_tokens\n\t    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n\t        return [self.vocab_dict[token] for token in tokens]\n\t    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n\t        return [self.vocabulary[i] for i in ids]\n\t    def encode(self, text: str) -> List[int]:\n", "        \"TODO: add transformers encode arguments and functionality\"\n\t        tokens = self.tokenize(text)\n\t        token_ids = self.convert_tokens_to_ids(tokens)\n\t        return token_ids\n"]}
{"filename": "stapler/datamodule/dataloader/general_dataloader.py", "chunked_list": ["\"\"\"create_dataloader function, that returns weighted dataloader or normal dataloader dependent on argument\"\"\"\n\tfrom typing import Any, Optional, Union\n\timport numpy as np\n\tfrom collections import Counter\n\tfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\tfrom stapler.datamodule.components.train_dataset import TrainDataset\n\tdef create_dataloader(\n\t    dataset: TrainDataset,\n\t    batch_size: int = 32,\n\t    num_workers: int = 4,\n", "    pin_memory: bool = True,\n\t    persistent_workers: bool = True,\n\t    weighted_class_sampling: bool = False,\n\t    weighted_epitope_sampling: bool = False,\n\t) -> DataLoader:\n\t    \"\"\"Create dataloader for training\n\t    Args:\n\t        batch_size (int, optional): Batch size. Defaults to 32.\n\t        num_workers (int, optional): Number of workers. Defaults to 4.\n\t        pin_memory (bool, optional): Pin memory. Defaults to True.\n", "        persistent_workers (bool, optional): Persistent workers. Defaults to True.\n\t        weighted_class_sampling (bool, optional): Whether to use weighted class sampling. Defaults to False.\n\t        weighted_epitope_sampling (bool, optional): Whether to use weighted epitope sampling. Defaults to False.\n\t    Returns:\n\t        DataLoader: Dataloader\n\t    \"\"\"\n\t    weights = [1 for _ in range(len(dataset))]\n\t    if weighted_class_sampling:\n\t        # Calculate the weights for each sample\n\t        labels = dataset.labels * 1\n", "        class_counts = Counter(labels)\n\t        class_weights = {label: 1.0 / count for label, count in class_counts.items()}\n\t        # add the class weights to the weights list\n\t        class_weights_list = [class_weights[label] for label in labels]\n\t        weights = [w1 * w2 for w1, w2 in zip(weights, class_weights_list)]\n\t    if weighted_epitope_sampling:\n\t        # Calculate the weights for each sample\n\t        epitopes = list(dataset.epitope_df)\n\t        epitopes_counts = Counter(epitopes)\n\t        # calulate IDF for each epitope (inverse document frequency; IDF = log2(1 / (frequency / total)))\n", "        epitopes_weights = {epitope: np.log2(1 / (count/len(epitopes))) for epitope, count in epitopes_counts.items()}\n\t        # create a list of weights for each sample\n\t        epitope_weights_list = [epitopes_weights[epitope] for epitope in epitopes]\n\t        weights = [w1 * w2 for w1, w2 in zip(weights, epitope_weights_list)]\n\t    if weighted_class_sampling or weighted_epitope_sampling:\n\t        # Create a WeightedRandomSampler with the calculated weights\n\t        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\t    else:\n\t        sampler = None\n\t    dataloader = DataLoader(\n", "        dataset,\n\t        batch_size=batch_size,\n\t        num_workers=num_workers,\n\t        pin_memory=pin_memory,\n\t        persistent_workers=persistent_workers,\n\t        sampler=sampler,\n\t    )\n\t    return dataloader\n"]}
{"filename": "stapler/models/stapler_transformer.py", "chunked_list": ["from __future__ import annotations\n\timport torch\n\timport torch.nn as nn\n\tfrom x_transformers.x_transformers import TransformerWrapper\n\tclass STAPLERTransformer(TransformerWrapper):\n\t    def __init__(\n\t        self,\n\t        num_tokens: int = 25,\n\t        cls_dropout: float = 0.0,\n\t        checkpoint_path: str | None = None,\n", "        output_classification: bool = True,\n\t        classification_head: nn.Module | None = None,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n\t        STAPLERTransformer extends TransformerWrapper to perform token-level and sequence-level classification tasks.\n\t        Args:\n\t            output_classification (bool): Whether to output logits for classification tasks. Defaults to True.\n\t            classification_head (nn.Module): Custom sequence classification head. Defaults to None.\n\t            **kwargs: Keyword arguments for the TransformerWrapper.\n", "        \"\"\"\n\t        super().__init__(num_tokens=num_tokens, **kwargs)\n\t        self.output_classification = output_classification\n\t        self.hidden_dim = self.attn_layers.dim\n\t        self.num_tokens = num_tokens\n\t        self.to_logits = nn.Linear(self.hidden_dim, self.num_tokens)\n\t        if checkpoint_path:\n\t            self.load_model(checkpoint_path)\n\t        if self.output_classification:\n\t            if classification_head is not None:  # For custom classification head\n", "                self.to_cls = classification_head\n\t            else:\n\t                self.to_cls = nn.Sequential(\n\t                    nn.Linear(self.hidden_dim, self.hidden_dim),\n\t                    nn.Tanh(),\n\t                    nn.Dropout(cls_dropout),\n\t                    nn.Linear(self.hidden_dim, 2),\n\t                )\n\t    def forward(self, x, **kwargs):\n\t        \"\"\"\n", "        Forward pass of the STAPLERTransformer.\n\t        Args:\n\t            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n\t            **kwargs: Additional keyword arguments for the TransformerWrapper.\n\t        Returns:\n\t            output_dict (dict): Dictionary containing logits for token-level and sequence-level classification tasks.\n\t        \"\"\"\n\t        x = super().forward(x, **kwargs)\n\t        output_dict = {}\n\t        logits = self.to_logits(x)\n", "        output_dict[\"mlm_logits\"] = logits\n\t        if self.output_classification:\n\t            cls_logit = self.to_cls(x[:, 0, :])\n\t            output_dict[\"cls_logits\"] = cls_logit\n\t        return output_dict\n\t    def load_model(self, checkpoint_path: str):\n\t        \"\"\"Locate state dict in lightning checkpoint and load into model.\"\"\"\n\t        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t        checkpoint = torch.load(checkpoint_path, map_location=device)\n\t        state_dict = checkpoint[\"state_dict\"]\n", "        # Remove \"model.\" or \"transformer.\" prefix from state dict keys and remove any keys containing 'to_cls'\n\t        try:\n\t            state_dict = {\n\t                key.replace(\"model.\", \"\").replace(\"transformer.\", \"\"): value\n\t                for key, value in state_dict.items()\n\t                if \"to_cls\" not in key\n\t            }\n\t        except Exception as e:\n\t            print(\"Error loading state dict. Please check the checkpoint file.\")\n\t            raise e\n", "        self.load_state_dict(state_dict)\n"]}
{"filename": "stapler/models/__init__.py", "chunked_list": []}
{"filename": "stapler/models/benchmark_models/fully_connected.py", "chunked_list": ["# TODO: make this an actual model, not pl.module\n\timport pytorch_lightning as pl\n\timport torch\n\timport torchmetrics\n\tfrom torch import nn as nn\n\tfrom torch.nn import functional as F\n\tfrom stapler.utils.model_utils import CosineWarmupScheduler\n\tclass FFWD(pl.LightningModule):\n\t    def __init__(\n\t        self,\n", "        model,\n\t        input_size,\n\t        hidden_size,\n\t        num_classes,\n\t        learning_rate,\n\t        # batch_size,\n\t        token_dim,\n\t        max_epochs,\n\t        num_hidden_layers,\n\t        dropout,\n", "    ):\n\t        super().__init__()\n\t        self.model = model\n\t        self.input_size = input_size\n\t        self.token_dim = token_dim\n\t        self.num_hidden_layers = num_hidden_layers\n\t        self.num_classes = num_classes\n\t        self.hidden_size = hidden_size\n\t        self.learning_rate = learning_rate\n\t        self.dropout = dropout\n", "        self.max_epochs = max_epochs\n\t        # create an embedding layer\n\t        self.token_emb = nn.Embedding(self.token_dim, self.token_dim)\n\t        # create a fully connected layer\n\t        # first hidden layer (after flattening the input)\n\t        self.l1 = nn.Linear(self.token_dim * self.input_size, self.hidden_size)\n\t        # loop over cfg.num_layers to create the layers 2 until cfg.num_layers\n\t        # and store them in self.linears\n\t        self.linears = nn.ModuleList(\n\t            [nn.Linear(self.hidden_size, self.hidden_size) for _ in range(self.num_hidden_layers)]\n", "        )\n\t        # output layer\n\t        self.last_layer = nn.Linear(self.hidden_size, self.num_classes)\n\t        # self.reduce = Reduce(' b l h -> b h', 'mean')\n\t        self.cls_criterion = nn.CrossEntropyLoss()\n\t        # lightning metrics\n\t        metrics_train = torchmetrics.MetricCollection(\n\t            [\n\t                torchmetrics.Accuracy(threshold=0.5),\n\t                torchmetrics.AveragePrecision(pos_label=1),\n", "                torchmetrics.AUROC(pos_label=1),\n\t                torchmetrics.F1Score(threshold=0.5),\n\t            ]\n\t        )\n\t        metrics_val = torchmetrics.MetricCollection(\n\t            [\n\t                torchmetrics.Accuracy(threshold=0.5),\n\t                torchmetrics.AveragePrecision(pos_label=1),\n\t                torchmetrics.AUROC(pos_label=1),\n\t                torchmetrics.F1Score(threshold=0.5),\n", "            ]\n\t        )\n\t        self.train_metrics = metrics_train.clone(prefix=\"train_\")\n\t        self.valid_metrics = metrics_val.clone(prefix=\"val_\")\n\t    def init_(self):\n\t        nn.init.kaiming_normal_(self.token_emb.weight)\n\t    def forward(self, batch):\n\t        if len(batch) == 2:\n\t            inputs, labels = batch\n\t            epitopes_batch = []\n", "            reference_id = []\n\t        elif len(batch) == 4:\n\t            inputs, labels, epitopes_batch, reference_id = batch\n\t        else:\n\t            raise ValueError(\n\t                \"Wrong input size (batch should contain 2 (inputs, labels) or 3 elements (inputs, labels, epitopes)\"\n\t            )\n\t        # print(sum(labels)/len(labels))\n\t        # embed the inputs\n\t        out = self.token_emb(inputs)\n", "        # flatten all dimensions except the batch dimension (0)\n\t        out = torch.flatten(out, 1)\n\t        # linear layers from input*token_dim to hidden_size\n\t        out = F.relu(self.l1(out))\n\t        # loop over cfg.num_layers until cfg.num_layers\n\t        for i, l in enumerate(self.linears):\n\t            # x = self.linears[i](x) + l(x)\n\t            out = F.relu(self.linears[i](out))\n\t        # dropout before the output layer\n\t        out = F.dropout(out, p=self.dropout, training=self.training)\n", "        # last linear layer from hidden_size to num_classes\n\t        out = F.relu(self.last_layer(out))\n\t        loss = self.cls_criterion(out, torch.squeeze(labels).long())\n\t        # predictions and accuracy\n\t        true_labels = torch.squeeze(labels).int().detach()\n\t        # softmax over the cls_logit\n\t        cls_logit_softmax = torch.nn.functional.softmax(out, dim=-1)\n\t        # extract the postive cls_logit\n\t        cls_logit_pos = cls_logit_softmax[:, 1]\n\t        return {\n", "            \"loss\": loss,\n\t            \"preds\": cls_logit_pos,\n\t            \"target\": true_labels,\n\t            \"epitopes\": epitopes_batch,\n\t            \"reference_id\": reference_id,\n\t        }\n\t    def training_step(self, batch, batch_idx):\n\t        outputs = self.forward(batch)\n\t        self.log(\n\t            \"train_loss\",\n", "            outputs[\"loss\"],\n\t            batch_size=len(batch[0]),\n\t            on_step=True,\n\t            on_epoch=False,\n\t            prog_bar=False,\n\t            logger=True,\n\t        )\n\t        output = self.train_metrics(outputs[\"preds\"], outputs[\"target\"])\n\t        self.log_dict(output, batch_size=len(batch[0]))\n\t        return outputs[\"loss\"]\n", "    def validation_step(self, batch, batch_idx):\n\t        outputs = self.forward(batch)\n\t        self.log(\n\t            \"val_loss\",\n\t            outputs[\"loss\"],\n\t            batch_size=len(batch[0]),\n\t            on_step=False,\n\t            on_epoch=True,\n\t            prog_bar=True,\n\t            logger=True,\n", "        )\n\t        output = self.valid_metrics(outputs[\"preds\"], outputs[\"target\"])\n\t        self.log_dict(output, batch_size=len(batch[0]), logger=True, on_epoch=True)\n\t    def configure_optimizers(self):\n\t        # optim = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n\t        # return optim\n\t        optim = torch.optim.AdamW(\n\t            self.parameters(), lr=self.learning_rate, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01\n\t        )  # 0.0001 LEARNING_RATE)\n\t        # https://pytorch-lightning-bolts.readthedocs.io/en/latest/learning_rate_schedulers.html\n", "        # linear warmup and cosine decay\n\t        scheduler = CosineWarmupScheduler(optimizer=optim, warmup=10, max_iters=self.max_epochs)\n\t        return [optim], [scheduler]\n"]}
{"filename": "stapler/models/benchmark_models/__init__.py", "chunked_list": []}
{"filename": "stapler/lit_modules/__init__.py", "chunked_list": []}
{"filename": "stapler/lit_modules/lit_module_train.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import Any\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom torch import nn\n\tfrom torch.optim import Optimizer\n\t# import torchmetrics accurcy\n\tfrom torchmetrics import Accuracy, AveragePrecision\n\tfrom stapler.transforms.transforms import Transform\n\tfrom stapler.utils.io_utils import get_pylogger\n", "logger = get_pylogger(__name__)\n\tclass STAPLERLitModule(pl.LightningModule):\n\t    def __init__(\n\t        self,\n\t        model: nn.Module,\n\t        optimizer: Optimizer,\n\t        transforms: dict[str, Transform],\n\t        loss: nn.Module,\n\t        scheduler: torch.optim.lr_scheduler._LRScheduler | None = None,\n\t    ):\n", "        super().__init__()\n\t        self.save_hyperparameters(logger=False, ignore=[\"model\"])\n\t        self.model = model\n\t        self.transforms = transforms\n\t        self.loss = loss\n\t        self.optimizer = optimizer\n\t        self.scheduler = scheduler\n\t        # accuracy metric (pretraining and fine-tuning)\n\t        self.train_mlm_acc = Accuracy(task=\"multiclass\", num_classes=25)\n\t        self.val_mlm_acc = Accuracy(task=\"multiclass\", num_classes=25)\n", "        # average_precision metric (fine-tuning only)\n\t        self.train_cls_ap = AveragePrecision(pos_label=1, task=\"binary\")\n\t        self.val_cls_ap = AveragePrecision(pos_label=1, task=\"binary\")\n\t        self.test_cls_ap = AveragePrecision(pos_label=1, task=\"binary\")\n\t    def forward(self, batch: dict[str, torch.Tensor], stage: str, **kwargs):\n\t        if self.transforms and stage in self.transforms:\n\t            batch = self.transforms[stage](batch)\n\t        preds = self.model(batch[\"input\"], return_attn=False, return_embeddings=True, **kwargs)\n\t        # dict with loss and preds\n\t        output = {\"preds\": preds}\n", "        return output\n\t    def training_step(self, batch, batch_idx):\n\t        output_dict = self.forward(batch, stage=\"fit\")\n\t        preds = output_dict[\"preds\"]\n\t        loss = self.loss(preds, batch)\n\t        batch_size = batch[\"input\"].shape[0]\n\t        # log\n\t        labels_mlm, preds_mlm = self.extract_mlm_labels_and_preds(batch, preds)\n\t        labels_cls, preds_cls = self.extract_cls_labels_and_preds(batch, preds)\n\t        self.train_mlm_acc(labels_mlm, preds_mlm)\n", "        self.train_cls_ap(preds_cls, labels_cls)\n\t        self.log(\"train_cls_ap\", self.train_cls_ap, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n\t        self.log(\"train_mlm_acc\", self.train_mlm_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n\t        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n\t        return loss\n\t    def validation_step(self, batch, batch_idx):\n\t        \"\"\"Validation step. Not used during pretraining.\"\"\"\n\t        output_dict = self.forward(batch, stage=\"validate\")\n\t        # loss = output_dict[\"loss\"] # TODO: implement loss for validation (CLS loss)\n\t        preds = output_dict[\"preds\"]\n", "        batch_size = batch[\"input\"].shape[0]\n\t        # log\n\t        labels_cls, preds_cls = self.extract_cls_labels_and_preds(batch, preds)\n\t        self.val_cls_ap(preds_cls, labels_cls)\n\t        self.log(\"val_cls_ap\", self.val_cls_ap, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n\t        output_dict['preds_cls'] = preds_cls\n\t        output_dict['labels_cls'] = labels_cls\n\t        # output_dict['loss'] = loss\n\t        return output_dict\n\t    def predict_step(self, batch: dict[str, torch.Tensor], batch_idx: int, dataloader_idx: int | None = None) -> Any:\n", "        \"\"\"Predict step. Not used during pretraining.\"\"\"\n\t        output_dict = self.forward(batch, stage=\"test\")\n\t        preds = output_dict[\"preds\"]\n\t        labels_cls, preds_cls = self.extract_cls_labels_and_preds(batch, preds)\n\t        return {'preds_cls': preds_cls.tolist(), 'labels_cls': labels_cls.tolist()}\n\t    def test_step(self, batch, batch_idx):\n\t        \"\"\"Test step. Not used during pretraining.\"\"\"\n\t        output_dict = self.forward(batch, stage=\"test\")\n\t        preds = output_dict[\"preds\"]\n\t        batch_size = batch[\"input\"].shape[0]\n", "        # log\n\t        labels_cls, preds_cls = self.extract_cls_labels_and_preds(batch, preds)\n\t        self.test_cls_ap(preds_cls, labels_cls)\n\t        self.log(\"test_cls_ap\", self.test_cls_ap, on_step=True, on_epoch=True, prog_bar=False, logger=True, batch_size=batch_size)\n\t        return None\n\t    def extract_mlm_labels_and_preds(self, batch, preds):\n\t        mlm_preds = preds[\"mlm_logits\"]\n\t        mlm_labels = batch[\"mlm_labels\"]\n\t        mlm_mask_indices = batch[\"mlm_mask_indices\"]\n\t        # Get the predicted token indices\n", "        preds_indices = torch.argmax(mlm_preds, dim=-1)\n\t        # Extract the labels and predicted tokens for masked positions\n\t        masked_labels = mlm_labels[mlm_mask_indices]\n\t        masked_preds = preds_indices[mlm_mask_indices]\n\t        return masked_labels, masked_preds\n\t    def extract_cls_labels_and_preds(self, batch, preds):\n\t        cls_labels = batch[\"cls_labels\"]\n\t        cls_preds = preds[\"cls_logits\"]\n\t        # softmax, thaking the positive prediction\n\t        cls_preds_softmax = torch.softmax(cls_preds, dim=-1)[:, 1]\n", "        return cls_labels, cls_preds_softmax\n\t    def configure_optimizers(self):\n\t        optim = self.optimizer(params=self.parameters())\n\t        if self.scheduler is not None:\n\t            scheduler = self.scheduler(optimizer=optim)\n\t            return [optim], [scheduler]\n\t        else:\n\t            return optim\n"]}
{"filename": "stapler/lit_modules/lit_module_pretrain.py", "chunked_list": ["from typing import Any\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom torch import nn\n\tfrom torch.optim import Optimizer\n\t# import torchmetrics accurcy\n\tfrom torchmetrics import Accuracy, AveragePrecision\n\tfrom stapler.transforms.transforms import Transform\n\tfrom stapler.utils.io_utils import get_pylogger\n\tlogger = get_pylogger(__name__)\n", "class STAPLERLitModule(pl.LightningModule):\n\t    def __init__(\n\t        self,\n\t        model: nn.Module,\n\t        optimizer: Optimizer,\n\t        transforms: dict[str, Transform],\n\t        loss: nn.Module,\n\t        scheduler: torch.optim.lr_scheduler._LRScheduler,\n\t    ):\n\t        super().__init__()\n", "        self.save_hyperparameters(logger=False, ignore=[\"model\"])\n\t        self.model = model\n\t        self.transforms = transforms\n\t        self.loss = loss\n\t        self.optimizer = optimizer\n\t        self.scheduler = scheduler\n\t        # accuracy metric (pretraining and fine-tuning)\n\t        self.train_mlm_acc = Accuracy(task=\"multiclass\", num_classes=25)\n\t        self.val_mlm_acc = Accuracy(task=\"multiclass\", num_classes=25)\n\t    def forward(self, batch: dict[str, torch.Tensor], stage: str, **kwargs):\n", "        if self.transforms and stage in self.transforms:\n\t            batch = self.transforms[stage](batch)\n\t        preds = self.model(batch[\"input\"], return_attn=False, return_embeddings=True, **kwargs)\n\t        loss = self.loss(preds, batch)\n\t        # dict with loss and preds\n\t        output = {\"loss\": loss, \"preds\": preds}\n\t        return output\n\t    def training_step(self, batch, batch_idx):\n\t        output_dict = self.forward(batch, stage=\"fit\")\n\t        loss = output_dict[\"loss\"]\n", "        preds = output_dict[\"preds\"]\n\t        batch_size = batch[\"input\"].shape[0]\n\t        # log\n\t        labels_mlm, preds_mlm = self.extract_mlm_labels_and_preds(batch, preds)\n\t        self.train_mlm_acc(labels_mlm, preds_mlm)\n\t        self.log(\n\t            \"train_mlm_acc\",\n\t            self.train_mlm_acc,\n\t            on_step=False,\n\t            on_epoch=True,\n", "            prog_bar=True,\n\t            logger=True,\n\t            batch_size=batch_size,\n\t        )\n\t        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=batch_size)\n\t        return loss\n\t    def extract_mlm_labels_and_preds(self, batch, preds):\n\t        mlm_preds = preds[\"mlm_logits\"]\n\t        mlm_labels = batch[\"mlm_labels\"]\n\t        mlm_mask_indices = batch[\"mlm_mask_indices\"]\n", "        # Get the predicted token indices\n\t        preds_indices = torch.argmax(mlm_preds, dim=-1)\n\t        # Extract the labels and predicted tokens for masked positions\n\t        masked_labels = mlm_labels[mlm_mask_indices]\n\t        masked_preds = preds_indices[mlm_mask_indices]\n\t        return masked_labels, masked_preds\n\t    def configure_optimizers(self):\n\t        optim = self.optimizer(params=self.parameters())\n\t        if self.scheduler is not None:\n\t            scheduler = self.scheduler(optimizer=optim)\n", "            return [optim], [scheduler]\n\t        else:\n\t            return optim\n"]}
