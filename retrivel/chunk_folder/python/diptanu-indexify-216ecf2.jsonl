{"filename": "langchain/indexify_langchain/__init__.py", "chunked_list": ["from indexify_langchain.memory.indexify import IndexifyMemory\n\tfrom indexify import DEFAULT_INDEXIFY_URL\n\t__all__ = [\"IndexifyMemory\", \"DEFAULT_INDEXIFY_URL\"]"]}
{"filename": "langchain/indexify_langchain/memory/indexify.py", "chunked_list": ["from typing import Any, Dict, List\n\tfrom langchain.memory.chat_memory import BaseChatMemory\n\tfrom indexify.data_containers import Message\n\tfrom indexify.memory import Memory\n\t'''\n\tThis class will initialize the Indexify class with the indexify_url your installation\n\tExample:\n\tmemory = IndexifyMemory(indexify_url=\"http://10.0.0.1:8900/\")\n\t'''\n\tclass IndexifyMemory(BaseChatMemory):\n", "    human_prefix: str = \"Human\"\n\t    ai_prefix: str = \"AI\"\n\t    memory_key: str = \"history\"\n\t    indexify_url: str = \"http://localhost:8900\"\n\t    indexify_index_name: str = \"default\"\n\t    memory: Memory = None\n\t    init: bool = False\n\t    def __init__(self,\n\t                 human_prefix: str = \"Human\",\n\t                 ai_prefix: str = \"AI\",\n", "                 memory_key: str = \"history\",\n\t                 indexify_url: str = \"http://localhost:8900\",\n\t                 indexify_index_name: str = \"default\",\n\t                 **kwargs: Any):\n\t        super().__init__(**kwargs)\n\t        self.human_prefix = human_prefix\n\t        self.ai_prefix = ai_prefix\n\t        self.memory_key = memory_key\n\t        self.indexify_url = indexify_url\n\t        self.indexify_index_name = indexify_index_name\n", "        self.memory: Memory = Memory(self.indexify_url, self.indexify_index_name)\n\t    @property\n\t    def memory_variables(self) -> List[str]:\n\t        return [self.memory_key]\n\t    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n\t        self.may_init()\n\t        input_str, output_str = self._get_input_output(inputs, outputs)\n\t        self.memory.add(Message(self.human_prefix, input_str))\n\t        self.memory.add(Message(self.ai_prefix, output_str))\n\t    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n", "        self.may_init()\n\t        all_messages = \"\"\n\t        for message in self.memory.all():\n\t            all_messages += f\"{message.role}: {message.text}\\n\"\n\t        return {self.memory_key: all_messages}\n\t    def clear(self) -> None:\n\t        # Recreate the memory\n\t        self.memory.create()\n\t    def may_init(self) -> None:\n\t        if not self.init:\n", "            self.memory.create()\n\t            self.init = True\n"]}
{"filename": "langchain/indexify_langchain/memory/__init__.py", "chunked_list": []}
{"filename": "langchain/tests/__init__.py", "chunked_list": []}
{"filename": "langchain/examples/agent_memory.py", "chunked_list": ["from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n\tfrom langchain import OpenAI, LLMChain\n\tfrom langchain.utilities import GoogleSearchAPIWrapper\n\tfrom indexify_langchain import IndexifyMemory, DEFAULT_INDEXIFY_URL\n\t'''\n\tFor this script to work we need to set the following environment settings.\n\texport GOOGLE_API_KEY=\"\"\n\texport GOOGLE_CSE_ID=\"\"\n\texport OPENAI_API_KEY=\"\"\n\tYou can get the Google's CSE and API key from following URL's:\n", "https://programmablesearchengine.google.com/controlpanel/create\n\thttps://console.cloud.google.com/projectselector2/google/maps-apis/credentials\n\tOpenAI API Key from:\n\thttps://platform.openai.com/account/api-keys\n\tNote: This example is a modification from langchain documentation here:\n\thttps://python.langchain.com/docs/modules/memory/how_to/agent_with_memory\n\t'''\n\t# Single line Magic to add vectorized memory!\n\tmemory = IndexifyMemory(memory_key=\"chat_history\", indexify_url=DEFAULT_INDEXIFY_URL)\n\t# Initialize Google search for the langchain.\n", "search = GoogleSearchAPIWrapper()\n\ttools = [\n\t    Tool(\n\t        name=\"Search\",\n\t        func=search.run,\n\t        description=\"useful for when you need to answer questions about current events\",\n\t    )\n\t]\n\tprefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\n\tsuffix = \"\"\"Begin!\"\n", "{chat_history}\n\tQuestion: {input}\n\t{agent_scratchpad}\"\"\"\n\tprompt = ZeroShotAgent.create_prompt(\n\t    tools,\n\t    prefix=prefix,\n\t    suffix=suffix,\n\t    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n\t)\n\tllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\n", "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n\tagent_chain = AgentExecutor.from_agent_and_tools(\n\t    agent=agent, tools=tools, verbose=True, memory=memory\n\t)\n\tdef ask(question):\n\t    print(f\"User question: {question}\")\n\t    agent_chain.run(input=question)\n\t# Ask your questions off. Helper function just prints the question before starting the chain!\n\task(\"How many people live in USA?\")\n\task(\"what is their national anthem called?\")\n", "ask(\"How many per state?\")\n\task(\"How many lines is the song?\")\n"]}
{"filename": "sdk-py/indexify/data_containers.py", "chunked_list": ["from enum import Enum\n\tfrom typing import List\n\tfrom dataclasses import dataclass, field\n\tclass TextSplitter(str, Enum):\n\t    NEWLINE = \"new_line\"\n\t    REGEX = \"regex\"\n\t    NOOP = \"noop\"\n\t    def __str__(self) -> str:\n\t        return self.value.lower()\n\t@dataclass\n", "class TextChunk:\n\t    text: str\n\t    metadata: dict[str, any] = field(default_factory=dict)\n\t    def to_dict(self):\n\t        return {\"text\": self.text, \"metadata\": self.metadata}\n\t@dataclass\n\tclass Message:\n\t    role: str\n\t    text: str\n\t    metadata: dict[str, any] = field(default_factory=dict)\n", "    def to_dict(self):\n\t        return {\"role\": self.role, \"text\": self.text, \"metadata\": self.metadata}\n\t@dataclass\n\tclass SearchChunk:\n\t    index: str\n\t    query: str\n\t    k: int\n\t    def to_dict(self):\n\t        return {\"index\": self.index, \"query\": self.query, \"k\": self.k}\n\t@dataclass\n", "class SearchResult:\n\t    results: List[TextChunk]\n"]}
{"filename": "sdk-py/indexify/__init__.py", "chunked_list": ["from .index import Index, AIndex\n\tfrom .memory import Memory, AMemory\n\tfrom .repository import Repository, ARepository\n\tfrom .data_containers import TextChunk, Message\n\tfrom .utils import wait_until\n\tDEFAULT_INDEXIFY_URL = \"http://localhost:8900\"\n\t__all__ = [\"Index\", \"Memory\", \"Repository\", \"AIndex\", \"AMemory\", \"ARepository\",\n\t           \"Message\", \"TextChunk\", \"DEFAULT_INDEXIFY_URL\", \"wait_until\", \"IndexifyMemory\"]\n"]}
{"filename": "sdk-py/indexify/utils.py", "chunked_list": ["import asyncio\n\tfrom enum import Enum\n\timport json\n\tclass ApiException(Exception):\n\t    def __init__(self, message: str) -> None:\n\t        super().__init__(message)\n\tclass Metric(str, Enum):\n\t    COSINE = \"cosine\"\n\t    DOT = \"dot\"\n\t    EUCLIDEAN = \"euclidean\"\n", "    def __str__(self) -> str:\n\t        return self.name.lower()\n\tasync def _get_payload(response):\n\t    try:\n\t        resp = await response.text()\n\t        return json.loads(resp)\n\t    except:\n\t        raise ApiException(resp)\n\tdef wait_until(functions):\n\t    single_result = False\n", "    if not isinstance(functions, list):\n\t        single_result = True\n\t        functions = [functions]\n\t    holder = []\n\t    async def run_and_capture_result():\n\t        holder.append(await asyncio.gather(*functions))\n\t    asyncio.run(run_and_capture_result())\n\t    if single_result:\n\t        return holder[0][0]  # single result\n\t    else:\n", "        return holder[0]  # list of results\n"]}
{"filename": "sdk-py/indexify/memory.py", "chunked_list": ["import aiohttp\n\tfrom .data_containers import *\n\tfrom .utils import _get_payload, wait_until\n\tclass AMemory:\n\t    def __init__(self, url, repository=\"default\"):\n\t        self._session_id = None\n\t        self._url = url\n\t        self._repo = repository\n\t    async def create(self) -> str:\n\t        async with aiohttp.ClientSession() as session:\n", "            async with session.post(f\"{self._url}/memory/create\", json={\"repository\": self._repo}) as resp:\n\t                resp = await _get_payload(resp)\n\t                self._session_id = resp[\"session_id\"]\n\t        return self._session_id\n\t    async def add(self, *messages: Message) -> None:\n\t        parsed_messages = []\n\t        for message in messages:\n\t            parsed_messages.append(message.to_dict())\n\t        req = {\"session_id\": self._session_id, \"repository\": self._repo, \"messages\": parsed_messages}\n\t        async with aiohttp.ClientSession() as session:\n", "            async with session.post(f\"{self._url}/memory/add\", json=req) as resp:\n\t                return await _get_payload(resp)\n\t    async def all(self) -> list[Message]:\n\t        req = {\"session_id\": self._session_id, \"repository\": self._repo}\n\t        async with aiohttp.ClientSession() as session:\n\t            async with session.get(f\"{self._url}/memory/get\", json=req) as resp:\n\t                payload = await _get_payload(resp)\n\t                messages = []\n\t                for raw_message in payload[\"messages\"]:\n\t                    messages.append(Message(raw_message[\"role\"], raw_message[\"text\"], raw_message[\"metadata\"]))\n", "                return messages\n\tclass Memory(AMemory):\n\t    def __init__(self, url, repository=\"default\"):\n\t        AMemory.__init__(self, url, repository)\n\t    def create(self) -> str:\n\t        return wait_until(AMemory.create(self))\n\t    def add(self, *messages: Message) -> None:\n\t        wait_until(AMemory.add(self, *messages))\n\t    def all(self) -> list[Message]:\n\t        return wait_until(AMemory.all(self))\n"]}
{"filename": "sdk-py/indexify/repository.py", "chunked_list": ["import aiohttp\n\tfrom .data_containers import *\n\tfrom .utils import _get_payload, wait_until\n\tclass ARepository:\n\t    def __init__(self, url: str, name: str = \"default\"):\n\t        self._url = url\n\t        self._name = name\n\t    async def run_extractors(self, repository: str = \"default\") -> dict:\n\t        req = {\"repository\": repository}\n\t        async with aiohttp.ClientSession() as session:\n", "            async with session.post(f\"{self._url}/repository/run_extractors\", json=req) as resp:\n\t                return await _get_payload(resp)\n\t    async def add(self, *chunks: TextChunk) -> None:\n\t        parsed_chunks = []\n\t        for chunk in chunks:\n\t            parsed_chunks.append(chunk.to_dict())\n\t        req = {\"documents\": parsed_chunks, \"repository\": self._name}\n\t        async with aiohttp.ClientSession() as session:\n\t            async with session.post(f\"{self._url}/repository/add_texts\", json=req) as resp:\n\t                return await _get_payload(resp)\n", "class Repository(ARepository):\n\t    def __init__(self, url, name):\n\t        ARepository.__init__(self, url, name)\n\t    def add(self, *chunks: TextChunk) -> None:\n\t        return wait_until(ARepository.add(self, *chunks))\n\t    def run_extractors(self, repository: str = \"default\") -> dict:\n\t        return wait_until(ARepository.run_extractors(self, repository))\n"]}
{"filename": "sdk-py/indexify/index.py", "chunked_list": ["import aiohttp\n\tfrom .data_containers import *\n\tfrom .utils import _get_payload, wait_until\n\tclass AIndex:\n\t    def __init__(self, url: str, index: str = \"default/default\"):\n\t        self._url = url\n\t        self._index = index\n\t    async def search(self, query: str, top_k: int) -> list[TextChunk]:\n\t        req = SearchChunk(index=self._index, query=query, k=top_k)\n\t        async with aiohttp.ClientSession() as session:\n", "            async with session.get(f\"{self._url}/index/search\", json=req.to_dict()) as resp:\n\t                payload = await _get_payload(resp)\n\t                result = []\n\t                for res in payload[\"results\"]:\n\t                    result.append(TextChunk(text=res[\"text\"], metadata=res[\"metadata\"]))\n\t                return result\n\tclass Index(AIndex):\n\t    def __init__(self, url, index):\n\t        AIndex.__init__(self, url, index)\n\t    def search(self, query: str, top_k: int) -> list[TextChunk]:\n", "        wait_until(AIndex.search(self, query, top_k))\n"]}
{"filename": "sdk-py/examples/memory_example.py", "chunked_list": ["from indexify import Memory, Message, DEFAULT_INDEXIFY_URL\n\tclass DemoMemoryExample:\n\t    def __init__(self):\n\t        self._memory = Memory(DEFAULT_INDEXIFY_URL, \"default\")\n\t    def execute(self):\n\t        print(\"Running memory example...\")\n\t        # Create a memory session\n\t        session = self._memory.create()\n\t        # Add to the vector and persistence memory\n\t        self._memory.add(Message(\"human\", \"Indexify is amazing!\"),\n", "                         Message(\"assistant\", \"How are you planning on using Indexify?!\"))\n\t        # Get all the memory events for a given session.\n\t        response = self._memory.all()\n\t        print([message.to_dict() for message in response])\n\tif __name__ == '__main__':\n\t    demo = DemoMemoryExample()\n\t    demo.execute()\n"]}
{"filename": "sdk-py/examples/question_answer_demo.py", "chunked_list": ["from indexify import AIndex, ARepository, TextChunk, DEFAULT_INDEXIFY_URL, wait_until\n\tfrom datasets import load_dataset\n\tclass DemoQA:\n\t    def __init__(self):\n\t        self.repository = ARepository(DEFAULT_INDEXIFY_URL, \"default\")\n\t        self.idx = AIndex(DEFAULT_INDEXIFY_URL, \"default/default\")\n\t    def execute(self):\n\t        # Add All Wikipedia articles\n\t        datasets = load_dataset('squad', split='train')\n\t        q_a_all = []\n", "        futures = []\n\t        print(\"Running QA example...\")\n\t        print(\"Adding all Wikipedia articles to the index...\")\n\t        for i in range(0, 10):\n\t            context: str = datasets[i][\"context\"]\n\t            question = datasets[i][\"question\"]\n\t            answers = datasets[i][\"answers\"]\n\t            futures.append(self.repository.add(TextChunk(context)))\n\t            q_a_all.append((question, answers))\n\t        wait_until(futures)\n", "        print(\"Running extractors now... workaround for concurrency issues\")\n\t        resp = wait_until(self.repository.run_extractors(\"default\"))\n\t        print(f\"number of extracted entities: {resp}\")\n\t        print(\"Starting search now...\")\n\t        for q_a in q_a_all:\n\t            question = q_a[0]\n\t            values = wait_until(self.idx.search(question, 1))\n\t            print(f\"Question: {question}, \\nContext / Answer can be found in: {values[0].text}\")\n\tif __name__ == '__main__':\n\t    demo = DemoQA()\n", "    demo.execute()\n"]}
{"filename": "sdk-py/examples/benchmark.py", "chunked_list": ["import argparse\n\timport time\n\tfrom indexify import AIndex, ARepository, AMemory, TextChunk, DEFAULT_INDEXIFY_URL, wait_until, Message\n\tfrom datasets import load_dataset\n\tclass BulkUploadRepository:\n\t    def __init__(self, url):\n\t        self.url = url\n\t        self.repository = ARepository(url)\n\t        self.idx = AIndex(url)\n\t    def benchmark_repository(self, concurrency: int, loop: int):\n", "        datasets = load_dataset('squad', split='train')\n\t        for j in range(0, loop):\n\t            start_time = time.time()\n\t            futures = []\n\t            for i in range(0, concurrency):\n\t                context: str = datasets[i][\"context\"]\n\t                futures.append(self.repository.add(TextChunk(context)))\n\t            wait_until(futures)\n\t            print(f\"repository.add seconds: {(time.time() - start_time)}\")\n\t        print(\"Running extractors now... workaround for concurrency issues\")\n", "        resp = wait_until(self.repository.run_extractors(\"default\"))\n\t        print(f\"number of extracted entities: {resp}\")\n\t        for j in range(0, loop):\n\t            start_time = time.time()\n\t            futures = []\n\t            for i in range(0, concurrency):\n\t                question = datasets[i][\"question\"]\n\t                futures.append(self.idx.search(question, 1))\n\t            wait_until(futures)\n\t            print(f\"repository.search seconds: {(time.time() - start_time)}\")\n", "    def benchmark_memory(self, concurrency: int, loop: int):\n\t        for j in range(0, loop):\n\t            start_time = time.time()\n\t            memory_list = []\n\t            for i in range(0, concurrency):\n\t                memory_list.append(AMemory(self.url))\n\t            futures_create = []\n\t            for i in range(0, concurrency):\n\t                futures_create.append(memory_list[i].create())\n\t            wait_until(futures_create)\n", "            print(f\"memory.create seconds: {(time.time() - start_time)}\")\n\t            futures_add = []\n\t            for i in range(0, concurrency):\n\t                futures_add.append(memory_list[i].add(Message(\"human\", \"Indexify is amazing!\"),\n\t                                                      Message(\"assistant\", \"How are you planning on using Indexify?!\")))\n\t            wait_until(futures_add)\n\t            print(f\"memory.add seconds: {(time.time() - start_time)}\")\n\t            futures_all = []\n\t            for i in range(0, concurrency):\n\t                futures_all.append(self._memory.all())\n", "            wait_until(futures_all)\n\t            print(f\"memory.all seconds: {(time.time() - start_time)}\")\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"-u\", \"--url\", default=DEFAULT_INDEXIFY_URL, help=\"indexify url to connect to\")\n\t    parser.add_argument(\"-c\", \"--concurrency\", default=10,\n\t                        help=\"We deal with Async functions its how often we wait\")\n\t    parser.add_argument(\"-l\", \"--loop\", default=100,\n\t                        help=\"Number of loops with concurrency set as per -c\")\n\t    args = parser.parse_args()\n", "    print(f\"looping for {args.loop}, with concurrency is set to be: {args.concurrency}, \"\n\t          f\"will run {args.loop * args.concurrency} requests\")\n\t    demo = BulkUploadRepository(args.url)\n\t    demo.benchmark_repository(args.concurrency, args.loop)\n\t    demo.benchmark_memory(args.concurrency, args.loop)\n"]}
{"filename": "src_py/tests/test_st_embeddings.py", "chunked_list": ["import unittest\n\tfrom indexify_py.sentence_transformer import SentenceTransformersEmbedding\n\tfrom indexify_py.dpr import DPREmbeddings\n\tclass TestSTEmbeddings(unittest.TestCase):\n\t    def __init__(self, *args, **kwargs):\n\t        super(TestSTEmbeddings, self).__init__(*args, **kwargs)\n\t    def test_query_embedding(self):\n\t        st = SentenceTransformersEmbedding(\"all-MiniLM-L6-v2\")\n\t        embeddings = st.embed_query(\"hello world\")\n\t        self.assertEqual(len(embeddings), 384)\n", "    def test_ctx_embeddings(self):\n\t        st = SentenceTransformersEmbedding(\"all-MiniLM-L6-v2\")\n\t        embeddings = st.embed_ctx([\"hello\", \"world\"])\n\t        self.assertEqual(len(embeddings), 2)\n\t        self.assertEqual(len(embeddings[0]), 384)\n\t    def test_tokenize(self):\n\t        st = SentenceTransformersEmbedding(\"all-MiniLM-L6-v2\")\n\t        chunks = st.tokenize([\"hello\", \"world hi\"])\n\t        self.assertEqual(len(chunks), 2)\n\t        #self.assertEqual(len(chunks[0]), 1)\n", "        self.assertEqual(chunks[1], \"world hi\")\n\t    def test_token_decode_encode(self):\n\t        st = SentenceTransformersEmbedding(\"all-MiniLM-L6-v2\")\n\t        tokens = st.tokenizer_encode([\"hello\", \"world hi\"])\n\t        texts = st.tokenizer_decode(tokens)\n\t        self.assertEqual(len(texts), 2)\n\t        self.assertAlmostEqual(texts[0], \"hello\")\n\t        self.assertAlmostEqual(texts[1], \"world hi\")\n\tclass TestDPR(unittest.TestCase):\n\t    def __init__(self, *args, **kwargs):\n", "        super(TestDPR, self).__init__(*args, **kwargs)\n\t    def test_query_embedding(self):\n\t        st = DPREmbeddings(\"facebook-dpr-ctx_encoder-multiset-base\", \"facebook-dpr-question_encoder-multiset-base\")\n\t        embeddings = st.embed_query(\"hello world\")\n\t        self.assertEqual(len(embeddings), 768)\n\t    def test_ctx_embeddings(self):\n\t        st = DPREmbeddings(\"facebook-dpr-ctx_encoder-multiset-base\", \"facebook-dpr-question_encoder-multiset-base\")\n\t        embeddings = st.embed_ctx([\"hello\", \"world\"])\n\t        self.assertEqual(len(embeddings), 2)\n\t        self.assertEqual(len(embeddings[0]), 768)\n", "    def test_token_decode_encode(self):\n\t        st = DPREmbeddings(\"facebook-dpr-ctx_encoder-multiset-base\", \"facebook-dpr-question_encoder-multiset-base\")\n\t        tokens = st.tokenizer_encode([\"hello\", \"world hi\"])\n\t        texts = st.tokenizer_decode(tokens)\n\t        self.assertEqual(len(texts), 2)\n\t        self.assertAlmostEqual(texts[0], \"hello\")\n\t        self.assertAlmostEqual(texts[1], \"world hi\")\n\t    def test_tokenize(self):\n\t        st = DPREmbeddings(\"facebook-dpr-ctx_encoder-multiset-base\", \"facebook-dpr-question_encoder-multiset-base\")\n\t        chunks = st.tokenize([\"hello\", \"world hi\"])\n", "        self.assertEqual(len(chunks), 2)\n\t        #self.assertEqual(len(chunks[0]), 1)\n\t        self.assertEqual(chunks[1], \"world hi\")\n\tif __name__ == \"__main__\":\n\t    unittest.main()"]}
{"filename": "src_py/tests/test_dpr.py", "chunked_list": ["import unittest\n\tfrom indexify_py.dpr_onnx import OnnxDPR\n\tclass TestDPREmbeddings(unittest.TestCase):\n\t    def __init__(self, *args, **kwargs):\n\t        super(TestDPREmbeddings, self).__init__(*args, **kwargs)\n\t    @classmethod\n\t    def setUpClass(cls):\n\t        cls._dpr = OnnxDPR()\n\t    def test_embedding_query(self):\n\t        query = \"What is the capital of France?\"\n", "        embeddings = self._dpr.generate_embeddings_query(query)\n\t        self.assertEqual(len(embeddings), 768)\n\t    def test_embedding_context(self):\n\t        context = [\"Paris is the capital of France.\", \"London is the capital of England.\"]\n\t        embeddings = self._dpr.generate_embeddings_ctx(context)\n\t        self.assertEqual(len(embeddings), 2)\n\t        self.assertEqual(len(embeddings[0]), 768)\n\tif __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "src_py/tests/test_entity_extractor.py", "chunked_list": ["import unittest\n\tfrom src_py.indexify_py.entity_extractor import EntityExtractor\n\tclass TestEntityExtractor(unittest.TestCase):\n\t    def __init__(self, *args, **kwargs):\n\t        super(TestEntityExtractor, self).__init__(*args, **kwargs)\n\t    @classmethod\n\t    def setUpClass(cls):\n\t        cls._entityextractor = EntityExtractor()\n\t        cls._otherextractor = EntityExtractor(model_name=\"dslim/bert-large-NER\")\n\t    def test_extractor(self):\n", "        input = \"My name is Wolfgang and I live in Berlin\"\n\t        entities = self._entityextractor.extract(input)\n\t        print(entities)\n\t        self.assertEqual(len(entities), 2)\n\t    def test_other_extractor(self):\n\t        input = \"My name is Wolfgang and I live in Berlin\"\n\t        entities = self._otherextractor.extract(input)\n\t        print(entities)\n\t        self.assertEqual(len(entities), 2)\n\tif __name__ == \"__main__\":\n", "    unittest.main()"]}
{"filename": "src_py/tests/__init__.py", "chunked_list": []}
{"filename": "src_py/indexify_py/sentence_transformer.py", "chunked_list": ["from transformers import AutoTokenizer, AutoModel\n\timport torch\n\timport torch.nn.functional as F\n\tfrom typing import List\n\tdef mean_pooling(model_output, attention_mask):\n\t    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n\t    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n\t    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\tclass SentenceTransformersEmbedding:\n\t    def __init__(self, model_name) -> None:\n", "        self._model_name = model_name\n\t        self._tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{model_name}')\n\t        self._model = AutoModel.from_pretrained(f'sentence-transformers/{model_name}')\n\t    def embed_ctx(self, inputs: List[str])  -> List[List[float]]:\n\t        result = self._embed(inputs)\n\t        return result.tolist()\n\t    def embed_query(self, query: str) -> List[float]:\n\t       result = self._embed([query])\n\t       return result[0].tolist()\n\t    def _embed(self, inputs: List[str]) -> torch.Tensor:\n", "        encoded_input = self._tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n\t        with torch.no_grad():\n\t            model_output = self._model(**encoded_input)\n\t        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\t        return F.normalize(sentence_embeddings, p=2, dim=1)\n\t    def tokenizer_encode(self, inputs: List[str]) -> List[List[int]]:\n\t        return self._tokenizer.batch_encode_plus(inputs)['input_ids']\n\t    def tokenizer_decode(self, tokens: List[List[int]]) -> List[str]:\n\t        return self._tokenizer.batch_decode(tokens, skip_special_tokens=True)\n\t    def tokenize(self, inputs: List[str]) -> List[List[str]]:\n", "        result = []\n\t        for input in inputs:\n\t            result.extend(self._tokenize(input))\n\t        return result\n\t    def _tokenize(self, input: str) -> List[str]:\n\t        max_length = self._tokenizer.model_max_length\n\t        chunks = []\n\t        chunk = []\n\t        chunk_length = 0\n\t        words = input.split(' ')\n", "        for word in words:\n\t            tokens = self._tokenizer.tokenize(word)\n\t            token_length = len(tokens)\n\t            if chunk_length + token_length <= max_length:\n\t                chunk.append(word)\n\t                chunk_length += token_length\n\t            else:\n\t                chunks.append(' '.join(chunk))\n\t                chunk = [word]\n\t                chunk_length = token_length\n", "        chunks.append(' '.join(chunk))\n\t        return chunks"]}
{"filename": "src_py/indexify_py/dpr_onnx.py", "chunked_list": ["import torch\n\tfrom optimum.onnxruntime import ORTModelForFeatureExtraction\n\timport torch.nn.functional as F\n\tfrom transformers import AutoTokenizer, Pipeline\n\tfrom pathlib import Path\n\tfrom typing import List\n\tdef cls_pooling(model_output, attention_mask):\n\t    return model_output[0][:, 0]\n\tdef mean_pooling(model_output, attention_mask):\n\t    token_embeddings = model_output[\n", "        0\n\t    ]  # First element of model_output contains all token embeddings\n\t    input_mask_expanded = (\n\t        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n\t    )\n\t    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n\t    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\t    return sum_embeddings / sum_mask\n\tclass SentenceEmbeddingPipeline(Pipeline):\n\t    def _sanitize_parameters(self, **kwargs):\n", "        # we don't have any hyperameters to sanitize\n\t        preprocess_kwargs = {}\n\t        return preprocess_kwargs, {}, {}\n\t    def preprocess(self, inputs):\n\t        encoded_inputs = self.tokenizer(\n\t            inputs, padding=True, truncation=True, return_tensors=\"pt\"\n\t        )\n\t        return encoded_inputs\n\t    def _forward(self, model_inputs):\n\t        outputs = self.model(**model_inputs)\n", "        return {\"outputs\": outputs, \"attention_mask\": model_inputs[\"attention_mask\"]}\n\t    def postprocess(self, model_outputs):\n\t        # Perform pooling\n\t        sentence_embeddings = mean_pooling(\n\t            model_outputs[\"outputs\"], model_outputs[\"attention_mask\"]\n\t        )\n\t        # Normalize embeddings\n\t        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\t        return sentence_embeddings.squeeze().tolist()\n\tclass OnnxDPR:\n", "    def __init__(\n\t        self,\n\t        ctx_model: str = \"sentence-transformers/facebook-dpr-ctx_encoder-multiset-base\",\n\t        ques_model: str = \"sentence-transformers/facebook-dpr-question_encoder-multiset-base\",\n\t    ):\n\t        self._ctx_model = ORTModelForFeatureExtraction.from_pretrained(ctx_model, export=True)\n\t        self._ques_model = ORTModelForFeatureExtraction.from_pretrained(ques_model, export=True)\n\t        self._tokenizer_ctx_model = AutoTokenizer.from_pretrained(ctx_model)\n\t        self._tokenizer_ques_model = AutoTokenizer.from_pretrained(ques_model)\n\t        self._ctx_pipeline = SentenceEmbeddingPipeline(self._ctx_model, self._tokenizer_ctx_model)\n", "        self._ques_pipeline = SentenceEmbeddingPipeline(self._ques_model, self._tokenizer_ques_model)\n\t    def generate_embeddings_ctx(self, inputs: List[str]):\n\t        embeddings = self._ctx_pipeline(inputs)\n\t        return embeddings\n\t    def generate_embeddings_query(self, query: str):\n\t        embeddings = self._ques_pipeline(query)\n\t        return embeddings\n\t## TODO Move this to proper unit tests later\n\tif __name__ == \"__main__\":\n\t    dpr = OnnxDPR()\n", "    print(dpr.generate_embeddings_ctx([\"What is the capital of England?\"]))\n\t    print(dpr.generate_embeddings_query(\"What is the capital of England?\"))\n"]}
{"filename": "src_py/indexify_py/entity_extractor.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom typing import List\n\tfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\tfrom transformers import pipeline\n\tfrom decimal import Decimal \n\tfrom enum import Enum\n\tfrom typing import Optional\n\t@dataclass\n\tclass EntityAttributes:\n\t    score: Decimal \n", "@dataclass\n\tclass Entity:\n\t   name: str\n\t   value: str\n\t   attributes: EntityAttributes\n\tclass EntityType(Enum): \n\t    def get_entity_type(type: str) -> str:\n\t        entity_type_map = {\n\t            \"B-PER\": \"Person\",\n\t            \"B-ORG\": \"Organization\",\n", "            \"I-ORG\": \"Organization\",\n\t            \"B-LOC\": \"Location\",\n\t            \"I-LOC\": \"Location\"\n\t         }   \n\t        return entity_type_map.get(type, \"INVALID\")\n\tclass EntityExtractor:\n\t    def __init__(self, model_name: str = \"dslim/bert-base-NER\"):\n\t        self.model_name = model_name\n\t        # Load model - https://huggingface.co/dslim/bert-base-NER\n\t        self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n", "        self._model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n\t        self._ctx_pipeline = pipeline(\"ner\", model=self._model, tokenizer=self._tokenizer)\n\t    def extract(self, text: str) -> List[Entity]:\n\t         ner_list = self._ctx_pipeline(text)\n\t         entities = []\n\t         for ner in ner_list:\n\t             name = EntityType.get_entity_type(ner[\"entity\"])\n\t             value = ner[\"word\"]\n\t             score = ner[\"score\"]\n\t             entity = Entity(name=name, value=value, attributes=EntityAttributes(score=score))\n", "             entities.append(entity)\n\t         return entities\n"]}
{"filename": "src_py/indexify_py/__init__.py", "chunked_list": []}
{"filename": "src_py/indexify_py/dpr.py", "chunked_list": ["from transformers import AutoTokenizer, AutoModel\n\timport torch\n\timport torch.nn.functional as F\n\tfrom typing import List\n\tdef cls_pooling(model_output, attention_mask):\n\t    return model_output[0][:,0]\n\tclass DPREmbeddings:\n\t    def __init__(self, ctx_model_name=\"facebook-dpr-ctx_encoder-multiset-base\", query_model_name=\"facebook-dpr-question_encoder-multiset-base\") -> None:\n\t        self._ctx_tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{ctx_model_name}')\n\t        self._ctx_model = AutoModel.from_pretrained(f'sentence-transformers/{ctx_model_name}')\n", "        self._query_tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{query_model_name}')\n\t        self._query_model = AutoModel.from_pretrained(f'sentence-transformers/{query_model_name}')\n\t    def embed_ctx(self, inputs: List[str])  -> List[List[float]]:\n\t        encoded_input = self._ctx_tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n\t        with torch.no_grad():\n\t            model_output = self._ctx_model(**encoded_input)\n\t        sentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n\t        return F.normalize(sentence_embeddings, p=2, dim=1).tolist()\n\t    def embed_query(self, input: str) -> torch.Tensor:\n\t        encoded_input = self._query_tokenizer(input, padding=True, truncation=True, return_tensors='pt')\n", "        with torch.no_grad():\n\t            model_output = self._query_model(**encoded_input)\n\t        sentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n\t        return F.normalize(sentence_embeddings, p=2, dim=1)[0].tolist()\n\t    def tokenizer_encode(self, inputs: List[str]) -> List[List[int]]:\n\t        return self._ctx_tokenizer.batch_encode_plus(inputs)['input_ids']\n\t    def tokenizer_decode(self, tokens: List[List[int]]) -> List[str]:\n\t        return self._ctx_tokenizer.batch_decode(tokens, skip_special_tokens=True)\n\t    def tokenize(self, inputs: List[str]) -> List[List[str]]:\n\t        result = []\n", "        for input in inputs:\n\t            result.extend(self._tokenize(input))\n\t        return result\n\t    def _tokenize(self, input: str) -> List[str]:\n\t        max_length = self._ctx_tokenizer.model_max_length\n\t        chunks = []\n\t        chunk = []\n\t        chunk_length = 0\n\t        words = input.split(' ')\n\t        for word in words:\n", "            tokens = self._ctx_tokenizer.tokenize(word)\n\t            token_length = len(tokens)\n\t            if chunk_length + token_length <= max_length:\n\t                chunk.append(word)\n\t                chunk_length += token_length\n\t            else:\n\t                chunks.append(' '.join(chunk))\n\t                chunk = [word]\n\t                chunk_length = token_length\n\t        chunks.append(' '.join(chunk))\n", "        return chunks"]}
{"filename": "src_py/indexify_py/extractor.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import Any\n\tclass Extractor(ABC):\n\t    @abstractmethod\n\t    def extract(self, content: Any, attrs: dict[str, Any]) -> Any:\n\t        \"\"\"\n\t        Extracts information from the content.\n\t        \"\"\"\n\t        pass\n\t    @abstractmethod\n", "    def get_name(self) -> str:\n\t        \"\"\"\n\t        Returns the name of the extractor.\n\t        \"\"\"\n\t        pass\n\t    @abstractmethod\n\t    def get_schema(self) -> str:\n\t        \"\"\"\n\t        Returns the schema of the extractor in JSON schema format.\n\t        It should have the following fields:\n", "        {\n\t            \"type\": \"extraction object\",\n\t            \"description\": \"string\",\n\t            \"properties\": {\n\t                \"property_name_1\": \"property_type\",\n\t                \"property_name_2\": \"property_type\",\n\t            }\n\t        }\n\t        \"\"\"\n\t        pass"]}
