{"filename": "build_scripts/pyproject_patcher.py", "chunked_list": ["import toml\n\timport os\n\timport sys\n\tif __name__ == \"__main__\":\n\t    file = \"pyproject.toml\"\n\t    if len(sys.argv) > 1:\n\t        file = sys.argv[1]\n\t    name = os.environ.get(\"PYPROJECT_NAME\",\"llm-rs\")\n\t    try:\n\t        data = toml.load(open(file,\"r\",encoding=\"utf-8\"))\n", "        data[\"project\"][\"name\"] = name\n\t        toml.dump(data,open(\"pyproject.toml.new\",\"w\",encoding=\"utf-8\"))\n\t        #Only replace if no error occured\n\t        os.remove(\"pyproject.toml\")\n\t        os.rename(\"pyproject.toml.new\",\"pyproject.toml\")\n\t        print(f\"Changed projectname of file {file} to {name}!\")\n\t    except Exception as e:\n\t        print(f\"Could not change projectname of file {file} to {name}!\")\n\t        print(e)\n"]}
{"filename": "build_scripts/repair_windows_wheels.py", "chunked_list": ["from delvewheel._wheel_repair import WheelRepair\n\timport sys\n\timport pathlib\n\tif __name__ == \"__main__\":\n\t    args = sys.argv\n\t    wheel_dir = args[1] if len(args) > 1 else \".\"\n\t    wheel_dir = pathlib.Path(wheel_dir).absolute()\n\t    print(f\"Repairing wheels in {wheel_dir}\")\n\t    wheels = list(wheel_dir.glob(\"*.whl\"))\n\t    print(f\"Found {len(wheels)} wheels\")\n", "    target_dir = pathlib.Path(\"./wheelhouse\").absolute()\n\t    for wheel in wheels:\n\t        print(f\"Repairing {wheel}\")\n\t        wr = WheelRepair(whl_path=wheel,\n\t                         extract_dir=None,\n\t                         add_dlls=None,\n\t                         no_dlls=None,\n\t                         ignore_in_wheel=True, \n\t                         verbose=0,\n\t                         test=[])\n", "        wr.show()\n\t        wr.repair(target=target_dir,\n\t                  no_mangles=set(),\n\t                  no_mangle_all=False,\n\t                  strip=False,\n\t                  lib_sdir=\".libs\",\n\t                  no_diagnostic=False)\n"]}
{"filename": "llm_rs/auto.py", "chunked_list": ["from .config import QuantizationType,ContainerType,SessionConfig\n\timport os\n\timport pathlib\n\tfrom .models import Mpt,GptNeoX,GptJ,Gpt2,Bloom,Llama\n\tfrom .base_model import Model\n\timport logging\n\tfrom typing import Optional, List, Union,Type,Dict, Callable\n\timport os\n\tfrom enum import Enum, auto\n\tfrom dataclasses import dataclass\n", "import json\n\tfrom blake3 import blake3\n\tfrom huggingface_hub import snapshot_download\n\tfrom huggingface_hub.utils import validate_repo_id, HFValidationError\n\tfrom huggingface_hub import cached_assets_path\n\tclass QuantizationVersions(Enum):\n\t    Not_Quantized=auto()\n\t    V1=auto()\n\t    V2=auto()\n\tclass KnownModels(Enum):\n", "    GptNeoX = auto()\n\t    Mpt = auto()\n\t    GptJ = auto()\n\t    Gpt2 = auto()\n\t    Bloom = auto()\n\t    Llama = auto()\n\t_QUANTIZATION_TYPE_MAP = {\n\t    \"Q4_0\": QuantizationType.Q4_0,\n\t    \"Q4_1\": QuantizationType.Q4_1,\n\t    \"Q5_0\": QuantizationType.Q5_0,\n", "    \"Q5_1\": QuantizationType.Q5_1,\n\t    \"Q8_0\": QuantizationType.Q8_0,\n\t    \"F16\": QuantizationType.F16\n\t}\n\t_CONTAINER_TYPE_MAP = {\n\t    \"GGML\": ContainerType.GGML,\n\t    \"GGJT\": ContainerType.GGJT\n\t}\n\t_KNOWN_MODELS_MAP = {\n\t    KnownModels.GptNeoX: GptNeoX,\n", "    KnownModels.Mpt: Mpt,\n\t    KnownModels.GptJ: GptJ,\n\t    KnownModels.Gpt2: Gpt2,\n\t    KnownModels.Bloom: Bloom,\n\t    KnownModels.Llama: Llama\n\t}\n\t_STRING_TO_KNOWN_MODEL_MAP = {\n\t    \"gpt2\": KnownModels.Gpt2,\n\t    \"starcoder\": KnownModels.Gpt2,\n\t    \"gpt_neox\": KnownModels.GptNeoX,\n", "    \"dolly-v2\": KnownModels.GptNeoX,\n\t    \"llama\": KnownModels.Llama,\n\t    \"mpt\": KnownModels.Mpt,\n\t    \"gptj\": KnownModels.GptJ,\n\t    \"bloom\": KnownModels.Bloom,\n\t}\n\tCURRENT_QUANTIZATION_VERSION = QuantizationVersions.V2\n\tclass PathType(Enum):\n\t    DIR = auto()\n\t    FILE = auto()\n", "    REPO = auto()\n\t    UNKNOWN = auto() \n\tdef _get_path_type(path: Union[str,os.PathLike]) -> PathType:\n\t    p = pathlib.Path(path)\n\t    if p.is_file():\n\t        return PathType.FILE\n\t    elif p.is_dir():\n\t        return PathType.DIR\n\t    try:\n\t        validate_repo_id(str(path))\n", "        return PathType.REPO\n\t    except HFValidationError:\n\t        pass\n\t    return PathType.UNKNOWN\n\t@dataclass()\n\tclass ModelMetadata():\n\t    \"\"\"\n\t    A dataclass to store metadata about a model.\n\t    \"\"\"\n\t    model: KnownModels\n", "    quantization: QuantizationType\n\t    container: ContainerType\n\t    quantization_version: QuantizationVersions=QuantizationVersions.Not_Quantized\n\t    converter:str=\"llm-rs\"\n\t    hash:Optional[str]=None\n\t    base_model:Optional[str]=None\n\t    def add_hash(self,model_path:Union[str,os.PathLike]):\n\t        h  = blake3(max_threads=blake3.AUTO)\n\t        b  = bytearray(128_000_000)\n\t        mv = memoryview(b)\n", "        with open(model_path, 'rb', buffering=0) as f:\n\t            for n in iter(lambda : f.readinto(mv), 0):\n\t                h.update(mv[:n])\n\t        self.hash=h.hexdigest()\n\t    def serialize(self):\n\t        return {\n\t            \"model\": self.model.name,\n\t            \"quantization\": repr(self.quantization).split(\".\")[-1],\n\t            \"quantization_version\": self.quantization_version.name,\n\t            \"container\": repr(self.container).split(\".\")[-1],\n", "            \"converter\": self.converter,\n\t            \"hash\": self.hash,\n\t            \"base_model\": self.base_model\n\t        }\n\t    @staticmethod\n\t    def deserialize(metadata_dict:Dict[str,str])->\"ModelMetadata\":\n\t        return ModelMetadata(\n\t            model = KnownModels[metadata_dict[\"model\"]],\n\t            quantization = _QUANTIZATION_TYPE_MAP[metadata_dict[\"quantization\"]],\n\t            quantization_version= QuantizationVersions[metadata_dict[\"quantization_version\"]],\n", "            container = _CONTAINER_TYPE_MAP[metadata_dict[\"container\"]],\n\t            converter = metadata_dict[\"converter\"],\n\t            hash = metadata_dict.get(\"hash\"),\n\t            base_model = metadata_dict.get(\"base_model\")\n\t        )\n\t@dataclass\n\tclass AutoConfig():\n\t    repo_type:Optional[str] = None\n\t    model_type: Optional[str] = None\n\t    @classmethod\n", "    def from_pretrained(\n\t        cls,\n\t        model_path_or_repo_id: Union[str, os.PathLike],\n\t        **kwargs,\n\t    ) -> 'AutoConfig':\n\t        path_type = _get_path_type(model_path_or_repo_id)\n\t        path = pathlib.Path(model_path_or_repo_id)\n\t        if path_type == PathType.UNKNOWN:\n\t            raise ValueError(\n\t                f\"Model path '{model_path_or_repo_id}' doesn't exist.\")\n", "        elif path_type == PathType.FILE:\n\t            path = path.resolve().parent\n\t        auto_config = AutoConfig()\n\t        if path_type == PathType.DIR:\n\t            cls._update_from_dir(str(path), auto_config)\n\t        elif path_type == PathType.REPO:\n\t            cls._update_from_repo(str(model_path_or_repo_id), auto_config)\n\t        return auto_config\n\t    @classmethod\n\t    def _update_from_repo(\n", "        cls,\n\t        repo_id: str,\n\t        auto_config: 'AutoConfig',\n\t    ) -> None:\n\t        path = snapshot_download(repo_id=repo_id, allow_patterns='config.json')\n\t        cls._update_from_dir(path, auto_config)\n\t    @classmethod\n\t    def _update_from_dir(cls, path: str, auto_config: 'AutoConfig') -> None:\n\t        resolved_path = (pathlib.Path(path) / 'config.json').resolve()\n\t        if resolved_path.is_file():\n", "            cls._update_from_file(str(resolved_path), auto_config)\n\t        else:\n\t            raise ValueError(f\"Config path '{resolved_path}' doesn't exist.\")\n\t    @classmethod\n\t    def _update_from_file(cls, path: str, auto_config: 'AutoConfig') -> None:\n\t        with open(path) as f:\n\t            config = json.load(f)\n\t        auto_config.model_type = config.get('model_type')\n\t        if 'repo_type' in config:\n\t            auto_config.repo_type = config.get('repo_type')\n", "        elif len(config) == 1:\n\t            auto_config.repo_type = \"GGML\"\n\t        else:\n\t            auto_config.repo_type = \"HuggingFace\"\n\tclass AutoModel():\n\t    \"\"\"\n\t    Utility to load models, without having to specify the model type.\n\t    \"\"\"\n\t    @classmethod\n\t    def has_metadata_file(cls,model_file:Union[str,os.PathLike])->bool:\n", "        path = pathlib.Path(model_file)\n\t        metadata_file = path.with_suffix(\".meta\")\n\t        return metadata_file.exists()\n\t    @classmethod\n\t    def load_metadata(cls,model_file:Union[str,os.PathLike])->ModelMetadata:\n\t        path = pathlib.Path(model_file)\n\t        if not path.is_file():\n\t            raise ValueError(f\"Model file '{model_file}' is not a file!\")\n\t        if not path.exists():\n\t            raise ValueError(f\"Model file '{model_file}' does not exist!\")\n", "        metadata_file = path.with_suffix(\".meta\")\n\t        if not metadata_file.exists():\n\t            raise ValueError(f\"Model file '{model_file}' does not have a metadata file '{metadata_file}'! If you want to autoload this model, please specify the model type.\")\n\t        metadata = ModelMetadata.deserialize(json.loads(metadata_file.read_text()))\n\t        return metadata\n\t    @classmethod\n\t    def _infer_model_type(cls,model_file:Union[str,os.PathLike],known_model:Optional[KnownModels]=None,config:Optional[AutoConfig]=None)->Type[Model]:\n\t        model_to_lookup = None\n\t        if known_model:\n\t            model_to_lookup = known_model\n", "        elif cls.has_metadata_file(model_file):\n\t            metadata = cls.load_metadata(model_file)\n\t            model_to_lookup = metadata.model\n\t        elif config and config.model_type:\n\t            if config.model_type.lower() in _STRING_TO_KNOWN_MODEL_MAP:\n\t                model_to_lookup = _STRING_TO_KNOWN_MODEL_MAP[config.model_type.lower()]\n\t            else:\n\t                raise ValueError(f\"Unknown model type '{config.model_type}' in config file '{model_file}'! Please specify the model type via `known_model`.\")\n\t        else:\n\t            raise ValueError(f\"Model file '{model_file}' does not have a metadata or config file and no model type was specified! Please specify the model type via `known_model`.\")\n", "        if model_to_lookup in _KNOWN_MODELS_MAP:\n\t            return _KNOWN_MODELS_MAP[model_to_lookup]\n\t        else:\n\t            raise ValueError(f\"Unknown model type '{model_to_lookup}'\")\n\t    @classmethod\n\t    def from_file(cls, \n\t                  path:Union[str,os.PathLike],\n\t                  config:Optional[AutoConfig],\n\t                  model_type: Optional[KnownModels] = None,\n\t                  session_config:SessionConfig=SessionConfig(),\n", "                  tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n\t                  lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n\t                  verbose:bool=False,\n\t                  use_hf_tokenizer:bool=True)->Model:\n\t        tokenizer = tokenizer_path_or_repo_id\n\t        if use_hf_tokenizer and tokenizer is None:\n\t            try:\n\t                metadata = cls.load_metadata(path)\n\t                tokenizer = metadata.base_model\n\t            except Exception as e:\n", "                logging.warning(f\"Could not load metadata for model '{path}'!\")\n\t            if tokenizer is None or tokenizer == \"\":\n\t                logging.warning(f\"Model file '{path}' does not have a base_model specified in its metadata file but wants to use a huggingface-tokenizer! Please expilicitly specify a tokenizer via `tokenizer_path_or_repo_id` if you intend to use a huggingface-tokenizer.\")\n\t        model = cls._infer_model_type(path,model_type,config)\n\t        return model(path,session_config,tokenizer_path_or_repo_id,lora_paths,verbose)\n\t    @classmethod\n\t    def from_pretrained(cls,\n\t        model_path_or_repo_id: Union[str,os.PathLike],\n\t        model_file: Optional[str] = None,\n\t        model_type: Optional[KnownModels] = None,\n", "        session_config:SessionConfig=SessionConfig(),\n\t        tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n\t        lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n\t        verbose:bool=False,\n\t        use_hf_tokenizer:bool=True,\n\t        default_quantization:QuantizationType=QuantizationType.Q4_0,\n\t        default_container:ContainerType=ContainerType.GGJT)->Model:\n\t        try: \n\t            config = AutoConfig.from_pretrained(\n\t                model_path_or_repo_id,\n", "            )\n\t        except ValueError:\n\t            logging.warning(\"Could not find config.json in repo, assuming GGML model...\")\n\t            config = AutoConfig(repo_type=\"GGML\")\n\t        if model_file:\n\t            config.repo_type = \"GGML\"\n\t        path_type = _get_path_type(model_path_or_repo_id)\n\t        if path_type == PathType.UNKNOWN:\n\t            raise ValueError(f\"Unknown path type for '{model_path_or_repo_id}'\")\n\t        elif path_type == PathType.FILE:\n", "            return cls.from_file(model_path_or_repo_id,config,model_type,session_config,tokenizer_path_or_repo_id,lora_paths,verbose,use_hf_tokenizer)\n\t        else:\n\t            if path_type == PathType.REPO:\n\t                if config.repo_type != \"GGML\":\n\t                    logging.warning(\"Found normal HuggingFace model, starting conversion...\")\n\t                    return cls.from_transformer(model_path_or_repo_id, session_config, tokenizer_path_or_repo_id, lora_paths, verbose, use_hf_tokenizer,default_quantization, default_container)\n\t                resolved_path = cls._find_model_path_from_repo(str(model_path_or_repo_id),model_file)\n\t                return cls.from_file(resolved_path,config,model_type,session_config,tokenizer_path_or_repo_id,lora_paths,verbose,use_hf_tokenizer)\n\t            elif path_type == PathType.DIR:\n\t                resolved_path = cls._find_model_path_from_dir(str(model_path_or_repo_id),model_file)\n", "                return cls.from_file(resolved_path,config,model_type,session_config,tokenizer_path_or_repo_id,lora_paths,verbose,use_hf_tokenizer)\n\t            else:\n\t                raise ValueError(f\"Unknown path type '{path_type}'\")\n\t    @classmethod\n\t    def _find_model_path_from_dir(\n\t        cls,\n\t        directory: str,\n\t        filename: Optional[str] = None,\n\t    ) -> str:\n\t        path = pathlib.Path(directory).resolve()\n", "        if filename:\n\t            file = (path / filename)\n\t            if not file.is_file():\n\t                raise ValueError(\n\t                    f\"Model file '{filename}' not found in '{path}'\")\n\t            return str(file)\n\t        files = [\n\t            f for f in path.iterdir()\n\t            if f.is_file() and f.name.endswith('.bin')\n\t        ]\n", "        if len(files) == 0:\n\t            raise ValueError(f\"No model files found in '{path}'\")\n\t        elif len(files) > 1:\n\t            raise ValueError(\n\t                f\"Multiple model files found in '{path}'! Please specify one of the following model files to load: '{','.join([f.name for f in files])}'\"\n\t            )\n\t        return str(files[0].resolve())\n\t    @classmethod\n\t    def _find_model_path_from_repo(\n\t        cls,\n", "        repo_id: str,\n\t        filename: Optional[str] = None,\n\t    ) -> str:\n\t        cache_directory = cached_assets_path(\"llm-rs\",namespace=repo_id)\n\t        #we dont want to download the whole repo, just the metadata\n\t        directory = snapshot_download(repo_id=repo_id,\n\t                                allow_patterns='*.meta',\n\t                                local_dir=cache_directory)\n\t        path = pathlib.Path(directory).resolve()\n\t        files = [\n", "            f for f in path.iterdir()\n\t            if f.is_file() and f.name.endswith('.meta')\n\t        ]\n\t        if len(files) == 0 and filename is None:\n\t            raise ValueError(f\"No model files found in '{repo_id}'! Either specify the model file via the `model_file` parameter or make sure the repository contains a *.meta files for each model.\")\n\t        if len(files) > 1 and filename is None:\n\t            raise ValueError(\n\t                f\"Multiple model files found in '{repo_id}'! Please specify one of the following model files to load: '{','.join([f.name.replace('.meta','.bin') for f in files])}' \"\n\t            )\n\t        #if we have a filename, we can just download that file\n", "        filename = filename if filename else files[0].name.replace('.meta','.bin')\n\t        model_directory = snapshot_download(repo_id=repo_id,\n\t                                allow_patterns=filename,\n\t                                local_dir=cache_directory)\n\t        return cls._find_model_path_from_dir(model_directory, filename=filename)\n\t    @classmethod\n\t    def from_transformer(cls,\n\t        model_path_or_repo_id: Union[str,os.PathLike],\n\t        session_config:SessionConfig=SessionConfig(),\n\t        tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n", "        lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n\t        verbose:bool=False,\n\t        use_hf_tokenizer:bool=True,\n\t        default_quantization:QuantizationType=QuantizationType.Q4_0,\n\t        default_container:ContainerType=ContainerType.GGJT):\n\t        try:\n\t            from .convert import AutoConverter\n\t            from .convert.auto_converter import get_name_from_config\n\t            from transformers import AutoConfig\n\t        except ImportError:\n", "            raise ImportError(\"Model conversion needs additional dependencies. Please install 'llm-rs[convert]' to use this functionality!\")\n\t        config = AutoConfig.from_pretrained(model_path_or_repo_id,trust_remote_code=True)\n\t        model_name = get_name_from_config(config)\n\t        export_path = cached_assets_path(\"llm-rs\",namespace=model_name)\n\t        converted_model = AutoConverter.convert(model_path_or_repo_id,export_path)\n\t        if default_quantization != QuantizationType.F16:\n\t            converted_model = AutoQuantizer.quantize(converted_model,quantization=default_quantization,container=default_container)\n\t        return cls.from_file(converted_model,None,session_config,tokenizer_path_or_repo_id,lora_paths,verbose,use_hf_tokenizer)\n\t# Hack to make the quantization type enum hashable\n\t_APPENDIX_MAP = {\n", "    QuantizationType.Q4_0.__repr__(): \"q4_0\",\n\t    QuantizationType.Q4_1.__repr__(): \"q4_1\",\n\t    QuantizationType.Q5_0.__repr__(): \"q5_0\",\n\t    QuantizationType.Q5_1.__repr__(): \"q5_1\",\n\t    QuantizationType.Q8_0.__repr__(): \"q8_0\",\n\t}\n\tclass AutoQuantizer():\n\t    \"\"\"\n\t    Utility to quantize models, without having to specify the model type.\n\t    \"\"\"\n", "    @staticmethod\n\t    def quantize(\n\t        model_file:Union[str,os.PathLike],\n\t        target_path:Optional[Union[str,os.PathLike]]=None,\n\t        quantization:QuantizationType=QuantizationType.Q4_0,\n\t        container:ContainerType=ContainerType.GGJT,\n\t        callback:Optional[Callable[[str],None]]=None\n\t        )->Union[str,os.PathLike]:\n\t        metadata=AutoModel.load_metadata(model_file)\n\t        if metadata.quantization != QuantizationType.F16:\n", "            raise ValueError(f\"Model '{model_file}' is already quantized to '{metadata.quantization}'\")\n\t        model_type = AutoModel._infer_model_type(model_file)\n\t        if target_path is None:\n\t            target_path = os.path.dirname(model_file)\n\t        def build_target_name()->str:\n\t            output_path = pathlib.Path(target_path)\n\t            if output_path.is_file():\n\t                return str(output_path)\n\t            else:\n\t                output_path.mkdir(parents=True,exist_ok=True)\n", "                model_path = pathlib.Path(model_file)\n\t                appendix = \"\"\n\t                if quantization.__repr__() in _APPENDIX_MAP:\n\t                    appendix += f\"-{_APPENDIX_MAP[quantization.__repr__()]}\"\n\t                if container == ContainerType.GGJT:\n\t                    appendix += \"-ggjt\"\n\t                filename = model_path.stem.replace(\"-f16\",\"\") + appendix + model_path.suffix\n\t                return str(output_path / filename)\n\t        target_file = build_target_name()\n\t        if pathlib.Path(target_file).exists():\n", "            logging.warning(f\"Target file '{target_file}' already exists, skipping quantization\")\n\t            return target_file\n\t        logging.info(f\"Quantizing model '{model_file}' to '{target_file}'\")\n\t        model_type.quantize(str(model_file),target_file,quantization,container,callback=callback)\n\t        metadata_file = pathlib.Path(target_file).with_suffix(\".meta\")\n\t        quantized_metadata = ModelMetadata(model=metadata.model,quantization=quantization,container=container,quantization_version=CURRENT_QUANTIZATION_VERSION,base_model=metadata.base_model)\n\t        quantized_metadata.add_hash(target_file)\n\t        logging.info(f\"Writing metadata file '{metadata_file}'\")\n\t        metadata_file.write_text(json.dumps(quantized_metadata.serialize(),indent=4))\n\t        logging.info(f\"Finished quantizing model '{model_file}' to '{target_file}'\")\n", "        return target_file\n"]}
{"filename": "llm_rs/__init__.py", "chunked_list": ["try:\n\t    from .llm_rs import *\n\texcept ImportError as e:\n\t    print(\"DLLs were not boundled with this package. Trying to locate them...\")\n\t    import os\n\t    import platform\n\t    #Try to locate CUDA_PATH environment variable\n\t    cuda_path = os.environ.get(\"CUDA_PATH\",None)\n\t    if cuda_path:\n\t        print(f\"Found CUDA_PATH environment variable: {cuda_path}\")\n", "        if platform.system() == \"Windows\":\n\t            cuda_path = os.path.join(cuda_path,\"bin\")\n\t        else:\n\t            cuda_path = os.path.join(cuda_path,\"lib64\")\n\t        print(f\"Adding {cuda_path} to DLL search path...\")\n\t        os.add_dll_directory(cuda_path)\n\t    try:\n\t        from .llm_rs import *\n\t    except ImportError as inner_e:\n\t        raise  ImportError(\"Could not locate DLLs. Please check the documentation for more information.\")\n", "from .config import GenerationConfig, SessionConfig, Precision, ContainerType, QuantizationType\n\tfrom .models import Llama, GptJ, Gpt2, Bloom, GptNeoX, Mpt\n\tfrom .auto import AutoModel, ModelMetadata, KnownModels, AutoQuantizer\n\t__doc__ = llm_rs.__doc__\n\tif hasattr(llm_rs, \"__all__\"):\n\t    __all__ = llm_rs.__all__"]}
{"filename": "llm_rs/base_model.py", "chunked_list": ["from typing import Optional, Callable, List, Union, Generator\n\tfrom abc import ABC\n\timport os\n\tfrom .config import GenerationConfig, SessionConfig, ContainerType, QuantizationType\n\tfrom .results import GenerationResult\n\t#Theoretically this is incorrect as the 'model' doesnt actually exist, but it is a good enough approximation for now. \n\tclass Model(ABC):\n\t    \"\"\"\n\t    Wrapper around a llm model.\n\t    \"\"\"\n", "    config:SessionConfig\n\t    @property\n\t    def path(self)->str: ...\n\t    @property\n\t    def verbose(self)->bool: ...\n\t    @property\n\t    def lora_paths(self)->Optional[List[str]]: ...\n\t    def  __init__(self,\n\t                  path:Union[str,os.PathLike],\n\t                  session_config:SessionConfig=SessionConfig(),\n", "                  tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None,\n\t                  lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n\t                  verbose:bool=False) -> None: ...\n\t    def generate(self,prompt:str,\n\t                 generation_config:Optional[GenerationConfig]=None,\n\t                 callback:Optional[Callable[[str],Optional[bool]]]=None) -> GenerationResult: \n\t        \"\"\"\n\t        Generates text from a prompt.\n\t        \"\"\" \n\t        ...\n", "    def generate(self,prompt:str) -> List[float]: \n\t        \"\"\"\n\t        Embed a given prompt into vector representation.\n\t        \"\"\" \n\t        ...\n\t    def stream(self,prompt:str,\n\t                 generation_config:Optional[GenerationConfig]=None,\n\t                 ) -> Generator[str,None,None]: \n\t        \"\"\"\n\t        Streams text from a prompt.\n", "        \"\"\" \n\t        ...\n\t    def tokenize(self,text:str) -> List[int]:\n\t        \"\"\"\n\t        Tokenizes a string into a list of tokens.\n\t        \"\"\"\n\t        ...\n\t    def decode(self,tokens:List[int]) -> str:\n\t        \"\"\"\n\t        Decodes a list of tokens into a string.\n", "        \"\"\"\n\t        ...\n\t    @staticmethod\n\t    def quantize(source:str,destination:str,quantization:QuantizationType=QuantizationType.Q4_0,container:ContainerType=ContainerType.GGJT,callback:Optional[Callable[[str],None]]=None)->None:\n\t        \"\"\"\n\t        Quantizes the model.\n\t        \"\"\"\n\t        ..."]}
{"filename": "llm_rs/repository.py", "chunked_list": ["from huggingface_hub import HfApi, CommitOperationAdd, CommitOperationDelete\n\tfrom huggingface_hub import create_repo,metadata_update,cached_assets_path\n\tfrom typing import List, Optional, Union\n\timport json\n\timport os\n\timport pathlib\n\timport logging \n\tclass Repository():\n\t    def __init__(self,name:str,user_or_organization:Optional[str]=None,token:Optional[str]=None,private:bool=False) -> None:\n\t        self.name = f\"{user_or_organization}/{name}\" if user_or_organization else name\n", "        self.api = HfApi(token=token)\n\t        try:\n\t            self.url = str(create_repo(self.name,token=token,private=private))\n\t            self.name = self.url.replace(f\"{self.api.endpoint}/\",\"\")\n\t            # Uplaod the config file\n\t            config_path = cached_assets_path(\"llm-rs\",namespace=self.name) / \"config.json\"\n\t            with open(config_path,\"w\") as f:\n\t                f.write(json.dumps({\"repo_type\":\"GGML\"}))\n\t            self.api.create_commit(\n\t                self.name,\n", "                operations=[CommitOperationAdd(path_in_repo=\"config.json\", path_or_fileobj=config_path)],\n\t                commit_message=\"Auto initialized repo via llm-rs\",)\n\t            metadata={}\n\t            metadata[\"pipeline_tag\"]= \"text-generation\"\n\t            metadata[\"tags\"] = [\"llm-rs\",\"ggml\"]\n\t            metadata_update(self.name, metadata, token=token)\n\t        except Exception as e:\n\t            self.url = create_repo(self.name,token=token,private=private,exist_ok=True)\n\t            self.name = self.url.replace(f\"{self.api.endpoint}/\",\"\")\n\t    def upload(self,model_file:Union[str,os.PathLike],delete_old:bool=True):\n", "        # search for the metadata file\n\t        path = pathlib.Path(model_file)\n\t        metadata_path = path.with_suffix(\".meta\")\n\t        if not path.exists():\n\t            raise FileNotFoundError(f\"Could not find model file {model_file}\")\n\t        if delete_old:\n\t            to_delete = [path.name]\n\t            if metadata_path.exists():\n\t                to_delete.append(metadata_path.name)\n\t            for n in to_delete:\n", "                try:\n\t                    self.api.create_commit(\n\t                        self.name,\n\t                        operations=[CommitOperationDelete(path_in_repo=n,is_folder=False)],\n\t                        commit_message=f\"Delete old file: '{n}'\",\n\t                        )\n\t                except Exception as e:\n\t                    logging.error(f\"Could not delete old file {n} from repo {self.name}\")\n\t        upload_operations = [CommitOperationAdd(path_in_repo=path.name, path_or_fileobj=path)] \n\t        if metadata_path.exists():\n", "            upload_operations.append(CommitOperationAdd(path_in_repo=metadata_path.name, path_or_fileobj=metadata_path))\n\t        self.api.create_commit(\n\t            self.name,\n\t            upload_operations,\n\t            commit_message=f\"Upload new model file: '{path.name}'\",)\n"]}
{"filename": "llm_rs/langchain/langchain.py", "chunked_list": ["try:\n\t    from langchain.llms.base import LLM\n\t    from langchain.embeddings.base import Embeddings\n\t    from langchain.callbacks.manager import CallbackManagerForLLMRun\n\texcept ImportError:\n\t    raise ImportError(\n\t        'To use the llm_rs.langchain module, please install llm-rs with the additional \"langchain\" dependencies via: pip install llm-rs[langchain]')\n\tfrom typing import Any, Dict, Optional, Sequence, Union, List\n\timport os\n\tfrom pydantic import root_validator\n", "from ..auto import AutoModel, KnownModels\n\tfrom ..config import GenerationConfig, SessionConfig\n\tfrom ..base_model import Model\n\tclass RustformersLLM(LLM,Embeddings):\n\t    \"\"\"\n\t    Langchain-Wrapper around a Rustformers model.\n\t    \"\"\"\n\t    model: Optional[Model] = None #: :meta private:\n\t    model_path_or_repo_id: Union[str,os.PathLike]\n\t    \"\"\"The path to the model file or directory or the name of a Hugging Face Hub\n", "    model repo.\"\"\"\n\t    model_type: Optional[KnownModels] = None\n\t    \"\"\"The model type.\"\"\"\n\t    model_file: Optional[str] = None\n\t    \"\"\"The name of the model file in repo or directory.\"\"\"\n\t    session_config:SessionConfig=SessionConfig()\n\t    \"\"\"Session config for the model.\"\"\"\n\t    generation_config:GenerationConfig=GenerationConfig()\n\t    \"\"\"Generation config for the model.\"\"\"\n\t    tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None\n", "    \"\"\"The path to the tokenizer file or directory or the name of a Hugging Face Hub repo containing the tokenizer.\"\"\"\n\t    use_hf_tokenizer:bool=True\n\t    \"\"\"Whether to use the Hugging Face tokenizer or the integrated GGML tokenizer.\"\"\"\n\t    lora_paths:Optional[List[Union[str,os.PathLike]]]=None\n\t    \"\"\"Paths to the lora files.\"\"\"\n\t    verbose:bool=False\n\t    \"\"\"Whether to print the loading process.\"\"\"\n\t    @property\n\t    def _identifying_params(self) -> Dict[str, Any]:\n\t        \"\"\"Get the identifying parameters.\"\"\"\n", "        return {\n\t            'model_path_or_repo_id': self.model_path_or_repo_id,\n\t            'model_type': self.model_type,\n\t            'model_file': self.model_file,\n\t            'session_config': self.session_config,\n\t            'generation_config': self.generation_config,\n\t            'tokenizer_path_or_repo_id': self.tokenizer_path_or_repo_id,\n\t            'lora_paths': self.lora_paths,\n\t            'use_hf_tokenizer': self.use_hf_tokenizer,\n\t            'verbose': self.verbose\n", "        }\n\t    @property\n\t    def _llm_type(self) -> str:\n\t        \"\"\"Return type of llm.\"\"\"\n\t        return 'rustformers'\n\t    @root_validator()\n\t    def validate_environment(cls, values: Dict) -> Dict:\n\t        \"\"\"Validate and load model from a local file or remote repo.\"\"\"\n\t        values['model'] = AutoModel.from_pretrained(\n\t            model_path_or_repo_id= values['model_path_or_repo_id'],\n", "            model_type=values['model_type'],\n\t            model_file=values['model_file'],\n\t            session_config=values['session_config'],\n\t            tokenizer_path_or_repo_id= values['tokenizer_path_or_repo_id'],\n\t            lora_paths=values['lora_paths'],\n\t            use_hf_tokenizer=values['use_hf_tokenizer'],\n\t            verbose=values['verbose']\n\t        )\n\t        return values\n\t    def _call(\n", "        self,\n\t        prompt: str,\n\t        stop: Optional[Sequence[str]] = None,\n\t        run_manager: Optional[CallbackManagerForLLMRun] = None,\n\t    ) -> str:\n\t        \"\"\"Generate text from a prompt.\n\t        Args:\n\t            prompt: The prompt to generate text from.\n\t            stop: A list of sequences to stop generation when encountered.\n\t        Returns:\n", "            The generated text.\n\t        \"\"\"\n\t        text = []\n\t        generation_config = self.generation_config\n\t        if stop:\n\t            generation_config.stop_words = list(stop)\n\t        for chunk in self.model.stream(prompt, generation_config=generation_config):\n\t            text.append(chunk)\n\t            if run_manager:\n\t                run_manager.on_llm_new_token(chunk, verbose=self.verbose)\n", "        return ''.join(text)\n\t    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n\t        \"\"\"Embed search docs.\"\"\"\n\t        embeddings = []\n\t        for text in texts:\n\t            embeddings.append(self.embed_query(text))\n\t    def embed_query(self, text: str) -> List[float]:\n\t        \"\"\"Embed query text.\"\"\"\n\t        return self.model.embed(text)"]}
{"filename": "llm_rs/langchain/__init__.py", "chunked_list": ["from .langchain import RustformersLLM"]}
{"filename": "llm_rs/convert/auto_converter.py", "chunked_list": ["from transformers import AutoConfig\n\timport os\n\tfrom typing import Union\n\tfrom .models import MPTConverter,GptNeoXConverter,GptJConverter,Gpt2Converter,BloomConverter,LlamaConverter\n\timport logging \n\timport pathlib\n\t_ARCHITECTURE_CONVERTER_MAP = {\n\t    \"MPTForCausalLM\": MPTConverter,\n\t    \"GPTNeoXForCausalLM\": GptNeoXConverter,\n\t    \"GPTJForCausalLM\": GptJConverter,\n", "    \"GPT2LMHeadModel\": Gpt2Converter,\n\t    \"BloomForCausalLM\":BloomConverter,\n\t    \"LLaMAForCausalLM\":LlamaConverter,\n\t    \"LlamaForCausalLM\":LlamaConverter #Open-LLaMA uses this as architecture name\n\t}\n\tclass AutoConverter():\n\t    @staticmethod\n\t    def convert(pretrained_model_name_or_path:Union[str,os.PathLike],output_path:Union[str,os.PathLike])->str:\n\t        config = AutoConfig.from_pretrained(pretrained_model_name_or_path,trust_remote_code=True)\n\t        architecture = config.architectures[0]\n", "        model_name = get_name_from_config(config)\n\t        adapter=None\n\t        if architecture not in _ARCHITECTURE_CONVERTER_MAP:\n\t            raise ValueError(f\"Unsupported architecture '{architecture}' for '{model_name}'\")\n\t        adapter = _ARCHITECTURE_CONVERTER_MAP[architecture]\n\t        output_file = build_path(output_path,model_name)\n\t        if os.path.exists(output_file):\n\t            logging.warning(f\"Skipping {model_name} via {adapter.__name__} because '{output_file}' already exists!\")\n\t            return output_file\n\t        logging.info(f\"Converting {model_name} via {adapter.__name__} to '{output_file}'\")\n", "        adapter(pretrained_model_name_or_path).convert(output_file)\n\t        return output_file\n\tdef get_name_from_config(config:AutoConfig)->str:\n\t    raw_name:str = \"\"\n\t    if hasattr(config,\"name_or_path\"):\n\t        raw_name = config.name_or_path\n\t    elif hasattr(config,\"model_type\"):\n\t        raw_name = config.model_type\n\t    else:\n\t        raw_name = config.architectures[0]\n", "    return raw_name.split(\"/\")[-1]\n\tdef build_path(output_path:Union[str,os.PathLike],model_name:str)->str:\n\t    output_path = pathlib.Path(output_path)\n\t    #User specified a file, so just use that\n\t    if output_path.is_file():\n\t        return output_path\n\t    #User specified a directory => auto generate a file name\n\t    output_path.mkdir(parents=True,exist_ok=True)\n\t    return os.path.join(output_path,f\"{model_name}-f16.bin\")\n"]}
{"filename": "llm_rs/convert/__init__.py", "chunked_list": ["from .auto_converter import AutoConverter\n\tfrom .models.mpt import MPTConverter"]}
{"filename": "llm_rs/convert/models/gpt2.py", "chunked_list": ["import numpy as np\n\tfrom ._base import BaseAdapter\n\tfrom typing import Union,Tuple,Optional,BinaryIO \n\tfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,GPT2LMHeadModel\n\timport os\n\timport torch\n\timport struct\n\timport numpy as np\n\tfrom ...auto import KnownModels\n\timport re\n", "#based on https://github.com/ggerganov/ggml/blob/master/examples/gpt-2/convert-h5-to-ggml.py\n\tclass Gpt2Converter(BaseAdapter):\n\t    model_type:KnownModels=KnownModels.Gpt2\n\t    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n\t        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n\t        model = GPT2LMHeadModel.from_pretrained(\n\t            pretrained_model_name_or_path,\n\t            torch_dtype=torch.float16,\n\t            low_cpu_mem_usage=True,\n\t            )\n", "        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n\t        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\t        return config,tokenizer,model\n\t    def _write_hyperparameters(self,out_file:BinaryIO):\n\t        hyperparameters = self.config.to_dict()\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_positions\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_embd\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_head\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layer\"]))\n", "    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n\t        return name.endswith(\"attn.masked_bias\") or name.endswith(\".attn.bias\") \n\t    def _rename_weights(self, name: str) -> str:\n\t        new_name=name\n\t        # rename headers to keep compatibility\n\t        # cerebras renames, see https://github.com/ggerganov/ggml/blob/master/examples/gpt-2/convert-cerebras-to-ggml.py\n\t        if name == \"transformer.ln_f.weight\":\n\t            new_name = \"model/ln_f/g\"\n\t        elif name == \"transformer.ln_f.bias\":\n\t            new_name = \"model/ln_f/b\"\n", "        elif name == \"transformer.wte.weight\":\n\t            new_name = \"model/wte\"\n\t        elif name == \"transformer.wpe.weight\":\n\t            new_name = \"model/wpe\"\n\t        elif name == \"lm_head.weight\":\n\t            new_name = \"model/lm_head\"\n\t        elif re.match(r\"transformer.h\\.\\d+\\.ln_1\\.weight\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/ln_1/g\"\n\t        elif re.match(r\"transformer.h\\.\\d+\\.ln_1\\.bias\", name):\n", "            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/ln_1/b\"\n\t        elif re.match(r\"transformer.h\\.\\d+\\.attn\\.c_attn\\.weight\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/attn/c_attn/w\"\n\t        elif re.match(r\"transformer.h\\.\\d+\\.attn\\.c_attn\\.bias\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/attn/c_attn/b\"\n\t        elif re.match(r\"transformer.h\\.\\d+\\.attn\\.c_proj\\.weight\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n", "            new_name = f\"model/h{i}/attn/c_proj/w\"\n\t        elif re.match(r\"transformer.h.\\d+.attn.c_proj.bias\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/attn/c_proj/b\"\n\t        elif re.match(r\"transformer.h.\\d+.ln_2.weight\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/ln_2/g\"\n\t        elif re.match(r\"transformer.h.\\d+.ln_2.bias\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/ln_2/b\"\n", "        elif re.match(r\"transformer.h.\\d+.mlp.c_fc.weight\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/mlp/c_fc/w\"\n\t        elif re.match(r\"transformer.h.\\d+.mlp.c_fc.bias\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/mlp/c_fc/b\"\n\t        elif re.match(r\"transformer.h.\\d+.mlp.c_proj.weight\", name):\n\t            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/mlp/c_proj/w\"\n\t        elif re.match(r\"transformer.h.\\d+.mlp.c_proj.bias\", name):\n", "            i = re.findall(\"\\d+\", name)[0]\n\t            new_name = f\"model/h{i}/mlp/c_proj/b\"\n\t        return new_name\n\t    def _write_vocabulary(self, out_file: BinaryIO):\n\t        # write the vocabulary size\n\t        out_file.write(struct.pack(\"i\", self.config.vocab_size))\n\t        return super()._write_vocabulary(out_file)\n\t    def _transform_weights(self, name: str, weight: torch.Tensor) -> torch.Tensor:\n\t        # for efficiency - transpose these matrices:\n\t        if name.endswith(\"/mlp/c_proj/w\") or name.endswith(\"/mlp/c_fc/w\") or name.endswith(\"/attn/c_proj/w\") or name.endswith(\"/attn/c_attn/w\"):\n", "            #see this issue: https://github.com/pytorch/pytorch/issues/50275\n\t            return weight.transpose(0,1)\n\t        return weight\n\t    def _filter_f16_weights(self, name: str, data: np.ndarray) -> bool:\n\t        n_dims = len(data.shape)\n\t        return (name == \"model/wte\" or name == \"model/lm_head\" or name[-2:] == \"/g\" or name[-2:] == \"/w\") and n_dims == 2\n"]}
{"filename": "llm_rs/convert/models/gptj.py", "chunked_list": ["import numpy as np\n\tfrom ._base import BaseAdapter\n\tfrom typing import Union,Tuple,Optional,BinaryIO \n\tfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,GPTJForCausalLM\n\timport os\n\timport torch\n\timport struct\n\timport numpy as np\n\tfrom ...auto import KnownModels\n\t#based on https://github.com/ggerganov/ggml/blob/master/examples/gpt-j/convert-h5-to-ggml.py\n", "class GptJConverter(BaseAdapter):\n\t    model_type:KnownModels=KnownModels.GptJ\n\t    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n\t        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n\t        model = GPTJForCausalLM.from_pretrained(\n\t            pretrained_model_name_or_path,\n\t            torch_dtype=torch.float16,\n\t            low_cpu_mem_usage=True,\n\t            )\n\t        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n", "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\t        return config,tokenizer,model\n\t    def _write_hyperparameters(self,out_file:BinaryIO):\n\t        hyperparameters = self.config.to_dict()\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_positions\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_embd\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_head\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layer\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"rotary_dim\"]))\n", "    def _write_vocabulary(self, out_file: BinaryIO):\n\t        # write the vocabulary size\n\t        out_file.write(struct.pack(\"i\", self.config.vocab_size))\n\t        return super()._write_vocabulary(out_file)\n\t    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n\t        return name.endswith(\"attn.masked_bias\") or name.endswith(\".attn.bias\") "]}
{"filename": "llm_rs/convert/models/bloom.py", "chunked_list": ["import numpy as np\n\tfrom ._base import BaseAdapter\n\tfrom typing import Union,Tuple,Optional,BinaryIO \n\tfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,BloomForCausalLM\n\timport os\n\timport torch\n\timport struct\n\timport numpy as np\n\tfrom ...auto import KnownModels\n\t#based on https://github.com/NouamaneTazi/bloomz.cpp/blob/main/convert-hf-to-ggml.py\n", "conv_map = {\n\t    'word_embeddings'       : 'tok_embeddings',\n\t    \"word_embeddings_layernorm\": 'norm',\n\t        'input_layernorm'        : 'attention_norm',\n\t        'self_attention.query_key_value': 'attention.query_key_value',\n\t        'self_attention.dense':          'attention.wo',\n\t        'post_attention_layernorm': 'ffn_norm',\n\t        'mlp.dense_h_to_4h'           : 'feed_forward.w1',\n\t        'mlp.dense_4h_to_h'           : 'feed_forward.w2',\n\t        'ln_f'                        : 'output_norm',\n", "        'lm_head' : 'output',\n\t        }\n\tclass BloomConverter(BaseAdapter):\n\t    model_type:KnownModels=KnownModels.Bloom\n\t    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n\t        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n\t        model = BloomForCausalLM.from_pretrained(\n\t            pretrained_model_name_or_path,\n\t            torch_dtype=torch.float16,\n\t            low_cpu_mem_usage=True,\n", "            )\n\t        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n\t        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\t        return config,tokenizer,model\n\t    def _write_hyperparameters(self,out_file:BinaryIO):\n\t        hyperparameters = self.config.to_dict()\n\t        hyperparameters[\"multiple_of\"] = 1\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"multiple_of\"]))\n", "        out_file.write(struct.pack(\"i\", hyperparameters[\"n_head\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layer\"]))\n\t    def _rename_weights(self, name: str) -> str:\n\t        #Arrrrrg i hate this, but upstream is inconsistent with naming\n\t        nn = name\n\t        if name != \"lm_head.weight\":\n\t            nn = nn.split(\".\")[1:]\n\t        else:\n\t            nn = nn.split(\".\")\n\t        if nn[0] == \"h\":\n", "            nn[0] = \"layers\"\n\t            mapped = conv_map[\".\".join(nn[2:-1])]\n\t            name = \".\".join(nn[:2] + [mapped] + nn[-1:])\n\t        else:\n\t            mapped = conv_map[\".\".join(nn[:-1])]\n\t            name = \".\".join([mapped] + nn[-1:])\n\t        return name\n\t    def _transform_weights(self, name: str, weight: torch.Tensor) ->  torch.Tensor:\n\t        if \"query_key_value\" in name:\n\t            q, k, v = weight.reshape(self.config.n_head, 3, -1).unbind(1)\n", "            return torch.cat([q, k, v], dim=0).reshape_as(weight)\n\t        return weight\n\t    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n\t        return name.endswith(\"attn.masked_bias\") or name.endswith(\".attn.bias\") "]}
{"filename": "llm_rs/convert/models/llama.py", "chunked_list": ["from ._base import BaseAdapter,GGJT_MAGIC\n\tfrom typing import Union,Tuple,Optional,BinaryIO,List,Iterable\n\tfrom transformers import LlamaConfig,LlamaForCausalLM,LlamaTokenizer,AutoConfig,AutoTokenizer,AutoModelForCausalLM\n\timport os\n\timport torch\n\timport struct\n\tfrom ...auto import KnownModels\n\timport re\n\timport numpy as np\n\timport logging\n", "#based on https://github.com/ggerganov/llama.cpp/blob/master/convert.py\n\tclass LlamaConverter(BaseAdapter):\n\t    model_type:KnownModels=KnownModels.Llama\n\t    file_magic=GGJT_MAGIC\n\t    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoConfig,AutoTokenizer,AutoModelForCausalLM]:\n\t        config = LlamaConfig.from_pretrained(pretrained_model_name_or_path)\n\t        model = LlamaForCausalLM.from_pretrained(\n\t            pretrained_model_name_or_path,\n\t            torch_dtype=torch.float16,\n\t            low_cpu_mem_usage=True,\n", "            )\n\t        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n\t        tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name)\n\t        def make_tensors_list() -> List[str]:\n\t            ret = [\n\t                'tok_embeddings.weight',\n\t                'norm.weight',\n\t                'output.weight',\n\t            ]\n\t            for i in range(80):  # maximum number of layer\n", "                ret += [\n\t                    f'layers.{i}.attention.wq.weight',\n\t                    f'layers.{i}.attention.wk.weight',\n\t                    f'layers.{i}.attention.wv.weight',\n\t                    f'layers.{i}.attention.wo.weight',\n\t                    f'layers.{i}.attention_norm.weight',\n\t                    f'layers.{i}.feed_forward.w1.weight',\n\t                    f'layers.{i}.feed_forward.w2.weight',\n\t                    f'layers.{i}.feed_forward.w3.weight',\n\t                    f'layers.{i}.ffn_norm.weight',\n", "                ]\n\t            return ret\n\t        self.TENSORS_LIST = make_tensors_list()\n\t        self.TENSORS_SET = set(self.TENSORS_LIST)\n\t        self.mapping = {\n\t            \"model.layers.{i}.self_attn.q_proj.weight\": \"layers.{i}.attention.wq.weight\",\n\t            \"model.layers.{i}.self_attn.k_proj.weight\": \"layers.{i}.attention.wk.weight\",\n\t            \"model.layers.{i}.self_attn.v_proj.weight\": \"layers.{i}.attention.wv.weight\",\n\t            \"model.layers.{i}.self_attn.o_proj.weight\": \"layers.{i}.attention.wo.weight\",\n\t            \"model.layers.{i}.mlp.gate_proj.weight\": \"layers.{i}.feed_forward.w1.weight\",\n", "            \"model.layers.{i}.mlp.down_proj.weight\": \"layers.{i}.feed_forward.w2.weight\",\n\t            \"model.layers.{i}.mlp.up_proj.weight\": \"layers.{i}.feed_forward.w3.weight\",\n\t            \"model.layers.{i}.input_layernorm.weight\": \"layers.{i}.attention_norm.weight\",\n\t            \"model.layers.{i}.post_attention_layernorm.weight\": \"layers.{i}.ffn_norm.weight\",\n\t        }\n\t        return config,tokenizer,model\n\t    def _write_hyperparameters(self,out_file:BinaryIO):\n\t        hyperparameters = self.config.to_dict()\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]))\n", "        out_file.write(struct.pack(\"i\", 256))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"num_attention_heads\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"num_hidden_layers\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]//hyperparameters[\"num_attention_heads\"])) # rot (obsolete)\n\t    def _rename_weights(self, name: str) -> str:\n\t        if name == \"model.embed_tokens.weight\":\n\t            return \"tok_embeddings.weight\"\n\t        elif name == \"model.norm.weight\":\n\t            return \"norm.weight\"\n\t        elif name == \"lm_head.weight\":\n", "            return \"output.weight\"\n\t        for old, new in self.mapping.items():\n\t            pattern = old.replace(\"{i}\", \"(\\d+)\")\n\t            match = re.fullmatch(pattern, name)\n\t            if match:\n\t                return new.replace(\"{i}\", match.group(1))\n\t        return name\n\t    def _transform_weights(self, name: str, weight: torch.Tensor) -> torch.Tensor:\n\t        to_permut = [r\"layers.(\\d+).attention.wq.weight\",r\"layers.(\\d+).attention.wk.weight\"]\n\t        for pattern in to_permut:\n", "            match = re.fullmatch(pattern, name)\n\t            if match:\n\t                n_heads = self.config.num_attention_heads\n\t                numpy_weights:np.ndarray = weight.numpy()\n\t                reshaped = (numpy_weights.reshape(n_heads, 2, numpy_weights.shape[0] // n_heads // 2, *numpy_weights.shape[1:])\n\t                   .swapaxes(1, 2)\n\t                   .reshape(numpy_weights.shape))\n\t                logging.info(f\"Permuting {name} from {weight.shape} to {reshaped.shape}\")\n\t                return torch.from_numpy(reshaped)\n\t        return weight\n", "    def _filter_weights_after_rename(self, name: str, weight: torch.Tensor) -> bool:\n\t            return name not in self.TENSORS_SET\n\t    def _write_vocabulary(self,out_file:BinaryIO):\n\t        sentence_piece_tokenizer = self.tokenizer.sp_model\n\t        logging.info(f\"Processing vocabulary with size {self.config.vocab_size}\")\n\t        def sentencepiece_tokens() -> Iterable[Tuple[bytes, float]]:\n\t            tokenizer = sentence_piece_tokenizer\n\t            for i in range(tokenizer.vocab_size()):\n\t                text: bytes\n\t                if tokenizer.is_unknown(i):\n", "                    text = \" \\u2047 \".encode(\"utf-8\")\n\t                elif tokenizer.is_control(i):\n\t                    text = b\"\"\n\t                elif tokenizer.is_byte(i):\n\t                    piece = tokenizer.id_to_piece(i)\n\t                    if len(piece) != 6:\n\t                        raise Exception(f\"Invalid token: {piece}\")\n\t                    byte_value = int(piece[3:-1], 16)\n\t                    text = struct.pack(\"B\", byte_value)\n\t                else:\n", "                    text = tokenizer.id_to_piece(i).replace(\"\\u2581\", \" \").encode(\"utf-8\")\n\t                score: float = tokenizer.get_score(i)\n\t                yield text, score\n\t        for text, score in sentencepiece_tokens():\n\t            out_file.write(struct.pack(\"i\", len(text)))\n\t            out_file.write(text)\n\t            out_file.write(struct.pack(\"f\", score))\n"]}
{"filename": "llm_rs/convert/models/__init__.py", "chunked_list": ["from .bloom import BloomConverter\n\tfrom .gpt2 import Gpt2Converter\n\tfrom .gptj import GptJConverter\n\tfrom .gptneox import GptNeoXConverter\n\tfrom .llama import LlamaConverter\n\tfrom .mpt import MPTConverter"]}
{"filename": "llm_rs/convert/models/gptneox.py", "chunked_list": ["import numpy as np\n\tfrom ._base import BaseAdapter\n\tfrom typing import Union,Tuple,Optional,BinaryIO \n\tfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\n\timport os\n\timport torch\n\timport struct\n\timport numpy as np\n\tfrom ...auto import KnownModels\n\t#based on https://github.com/ggerganov/ggml/tree/master/examples/gpt-neox\n", "class GptNeoXConverter(BaseAdapter):\n\t    model_type:KnownModels=KnownModels.GptNeoX\n\t    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n\t        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n\t        model = AutoModelForCausalLM.from_pretrained(\n\t            pretrained_model_name_or_path,\n\t            torch_dtype=torch.float16,\n\t            low_cpu_mem_usage=True,\n\t            )\n\t        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else pretrained_model_name_or_path \n", "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\t        return config,tokenizer,model\n\t    def _write_hyperparameters(self,out_file:BinaryIO):\n\t        hyperparameters = self.config.to_dict()\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"max_position_embeddings\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"hidden_size\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"num_attention_heads\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"num_hidden_layers\"]))\n\t        out_file.write(struct.pack(\"i\", int(hyperparameters[\"rotary_pct\"]*(hyperparameters[\"hidden_size\"]//hyperparameters[\"num_attention_heads\"]))))\n", "        out_file.write(struct.pack(\"i\", hyperparameters[\"use_parallel_residual\"] if \"use_parallel_residual\" in hyperparameters else True))\n\t    def _filter_weights(self, name: str, weight: torch.Tensor) -> bool:\n\t        return name.endswith(\".attention.masked_bias\") or     \\\n\t       name.endswith(\".attention.bias\") or \\\n\t       name.endswith(\".attention.rotary_emb.inv_freq\")"]}
{"filename": "llm_rs/convert/models/mpt.py", "chunked_list": ["from ._base import BaseAdapter\n\tfrom typing import Union,Tuple,Optional,BinaryIO \n\tfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\n\timport os\n\timport torch\n\timport struct\n\tfrom ...auto import KnownModels\n\t#based on https://github.com/ggerganov/ggml/blob/master/examples/mpt/convert-h5-to-ggml.py\n\tclass MPTConverter(BaseAdapter):\n\t    model_type:KnownModels=KnownModels.Mpt\n", "    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoTokenizer,AutoModelForCausalLM]:\n\t        config = AutoConfig.from_pretrained(pretrained_model_name_or_path,trust_remote_code=True)\n\t        model = AutoModelForCausalLM.from_pretrained(\n\t            pretrained_model_name_or_path,\n\t            torch_dtype=torch.float16,\n\t            trust_remote_code=True,\n\t            low_cpu_mem_usage=True,\n\t            )\n\t        #MPT uses the `EleutherAI/gpt-neox-20b` tokenizer => use it if no tokenizer is specified\n\t        tokenizer_name = pretrained_tokenizer_name_or_path if pretrained_tokenizer_name_or_path else \"EleutherAI/gpt-neox-20b\" \n", "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\t        return config,tokenizer,model\n\t    def _write_hyperparameters(self,out_file:BinaryIO):\n\t        hyperparameters = self.config.to_dict()\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"d_model\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"max_seq_len\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_heads\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"n_layers\"]))\n\t        out_file.write(struct.pack(\"i\", hyperparameters[\"vocab_size\"]))\n\t        out_file.write(struct.pack(\"f\", hyperparameters[\"attn_config\"][\"alibi_bias_max\"]))\n", "        out_file.write(struct.pack(\"f\", hyperparameters[\"attn_config\"][\"clip_qkv\"] or 0.0))"]}
{"filename": "llm_rs/convert/models/_base.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import Union,Tuple,Any,Optional,BinaryIO\n\tfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\n\timport os\n\timport struct\n\tfrom enum import Enum\n\timport logging\n\timport numpy as np\n\tfrom ...config import QuantizationType,ContainerType\n\tfrom ...auto import ModelMetadata, KnownModels, QuantizationVersions\n", "import pathlib\n\timport json\n\timport torch\n\tGGML_MAGIC = 0x67676D6C\n\tGGMF_MAGIC = 0x67676D66\n\tGGJT_MAGIC = 0x67676a74\n\tclass FileTypes(Enum):\n\t    FP32 = 0\n\t    FP16 = 1\n\tclass BaseAdapter(ABC):\n", "    model_type:KnownModels=None\n\t    file_magic:int=GGML_MAGIC\n\t    version:int=1\n\t    def  __init__(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None) -> None:\n\t        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n\t        self.config,self.tokenizer,self.model= self.load(pretrained_model_name_or_path,pretrained_tokenizer_name_or_path)\n\t    @abstractmethod\n\t    def load(self,pretrained_model_name_or_path:Union[str,os.PathLike],pretrained_tokenizer_name_or_path:Optional[Union[str,os.PathLike]]=None)->Tuple[AutoConfig,AutoTokenizer,AutoModelForCausalLM]:\n\t        ...\n\t    @abstractmethod\n", "    def _write_hyperparameters(self,out_file:BinaryIO):\n\t        ...\n\t    def _write_vocabulary(self,out_file:BinaryIO):\n\t        # TODO: temporary hack to not deal with implementing the tokenizer\n\t        logging.info(f\"Processing vocabulary with size {self.config.vocab_size}\")\n\t        dot_token = self.tokenizer.encode(\".\")[0]\n\t        for i in range(self.config.vocab_size):\n\t            text = self.tokenizer.decode([dot_token, i]).encode(\"utf-8\")\n\t            # remove the first byte (it's always '.')\n\t            text = text[1:]\n", "            out_file.write(struct.pack(\"i\", len(text)))\n\t            out_file.write(text)\n\t    def _filter_weights(self,name:str,weight:torch.Tensor)->bool:\n\t        \"\"\"Filter weights that should be skipped\"\"\"\n\t        return False\n\t    def _filter_weights_after_rename(self,name:str,weight:torch.Tensor)->bool:\n\t        \"\"\"Filter weights that should be skipped\"\"\"\n\t        return False\n\t    def _filter_f16_weights(self,name:str,data:np.ndarray)->bool:\n\t        \"\"\"Filter weights that should be stored as fp16\"\"\"\n", "        n_dims = len(data.shape)\n\t        return name.endswith(\".weight\") and n_dims == 2\n\t    def _rename_weights(self,name:str)->str:\n\t        \"\"\"Rename weights that should be renamed\"\"\"\n\t        return name\n\t    def _transform_weights(self,name:str,weight:torch.Tensor)->torch.Tensor:\n\t        \"\"\"Transform weights that should be transformed\"\"\"\n\t        return weight\n\t    def _write_weights(self,out_file:BinaryIO):\n\t        weights = self.model.state_dict()\n", "        for name, weight in weights.items():\n\t            if self._filter_weights(name,weight):\n\t                logging.info(f\"Skipping layer '{name}'\")\n\t                continue\n\t            name = self._rename_weights(name)\n\t            weight = self._transform_weights(name,weight)\n\t            if self._filter_weights_after_rename(name,weight):\n\t                logging.info(f\"Skipping layer '{name}'\")\n\t                continue\n\t            data = weight.squeeze().numpy()\n", "            n_dims = len(data.shape);    \n\t            type = FileTypes.FP32\n\t            if self._filter_f16_weights(name,data):\n\t                data = data.astype(np.float16)\n\t                type = FileTypes.FP16\n\t            else:\n\t                data = data.astype(np.float32)\n\t            encoded_name = name.encode(\"utf-8\")\n\t            out_file.write(struct.pack(\"iii\", n_dims, len(encoded_name), type.value))\n\t            if self.file_magic == GGJT_MAGIC or self.file_magic == GGMF_MAGIC:\n", "                out_file.write(struct.pack(\"i\" * len(data.shape), *data.shape[::-1]))\n\t            else:\n\t                for i in range(n_dims):\n\t                    out_file.write(struct.pack(\"i\", data.shape[n_dims - 1 - i]))\n\t            out_file.write(encoded_name)\n\t            if self.file_magic == GGJT_MAGIC:\n\t                # pad to 32 bytes\n\t                out_file.seek((out_file.tell() + 31) & -32)\n\t            # data\n\t            data.tofile(out_file)\n", "            logging.info(f\"Converted layer '{name}' with shape {data.shape}\")\n\t    def write_magic(self,out_file:BinaryIO):\n\t        out_file.write(struct.pack(\"i\", self.file_magic))\n\t        if self.file_magic == GGJT_MAGIC or self.file_magic == GGMF_MAGIC:\n\t            out_file.write(struct.pack(\"i\", self.version))\n\t    def write_file_type(self,out_file:BinaryIO,file_type:FileTypes=FileTypes.FP16):\n\t        out_file.write(struct.pack(\"i\", file_type.value))\n\t    def convert(self,output_file:Union[str,os.PathLike])->None:\n\t        with open(output_file, \"wb\") as out_file:\n\t            # write magic\n", "            self.write_magic(out_file)\n\t            logging.info(f\"Processing hyperparameters ...\")\n\t            self._write_hyperparameters(out_file)\n\t            self.write_file_type(out_file)\n\t            self._write_vocabulary(out_file)\n\t            self._write_weights(out_file)\n\t            logging.info(f\"Done converting model to GGML format. Saved to '{output_file}'\")\n\t        #Create the *.meta file needed for automatic loading\n\t        metadata_file = pathlib.Path(output_file).with_suffix(\".meta\")\n\t        metadata = ModelMetadata(\n", "            model = self.model_type,\n\t            quantization = QuantizationType.F16,\n\t            container = ContainerType.GGML,\n\t            quantization_version=QuantizationVersions.Not_Quantized,\n\t            base_model=str(self.pretrained_model_name_or_path),\n\t            )\n\t        metadata.add_hash(output_file)\n\t        metadata_file.write_text(json.dumps(metadata.serialize(),indent=4))\n\t        logging.info(f\"Created metadata file at '{output_file}'\")\n"]}
{"filename": "llm_rs/haystack/__init__.py", "chunked_list": ["from .haystack import RustformersInvocationLayer"]}
{"filename": "llm_rs/haystack/haystack.py", "chunked_list": ["try:\n\t    from haystack.nodes import PromptModelInvocationLayer\n\t    from haystack.nodes.prompt.invocation_layer import DefaultTokenStreamingHandler\n\texcept ImportError:\n\t     raise ImportError(\n\t        'To use the llm_rs.haystack module, please install llm-rs with the additional \"haystack\" dependencies e.g. via: pip install llm-rs[haystack]')\n\timport os\n\tfrom typing import Dict, List, Union, Type, Optional\n\tfrom ..auto import AutoModel,KnownModels\n\tfrom ..config import QuantizationType,ContainerType,SessionConfig,GenerationConfig\n", "from ..base_model import Model\n\timport logging \n\tlogger = logging.getLogger(__name__)\n\tclass RustformersInvocationLayer(PromptModelInvocationLayer):\n\t    def __init__(self, model_name_or_path: Union[str,os.PathLike], \n\t        max_length: int = 512,\n\t        model_file: Optional[str] = None,\n\t        model_type: Optional[KnownModels] = None,\n\t        session_config:SessionConfig=SessionConfig(),\n\t        tokenizer_path_or_repo_id: Optional[Union[str,os.PathLike]]=None,\n", "        lora_paths:Optional[List[Union[str,os.PathLike]]]=None,\n\t        verbose:bool=False,\n\t        use_hf_tokenizer:bool=True,\n\t        default_quantization:QuantizationType=QuantizationType.Q4_0,\n\t        default_container:ContainerType=ContainerType.GGJT,\n\t        **kwargs):\n\t        \"\"\"\n\t        Creates a new RustformersInvocationLayer instance.\n\t        :param model_name_or_path: The name or path of the underlying model.\n\t        :param kwargs: See `AutoModel.from_pretrained`.\n", "        \"\"\"\n\t        if model_name_or_path is None or len(model_name_or_path) == 0:\n\t            raise ValueError(\"model_name_or_path cannot be None or empty string\")\n\t        self.model_name_or_path = model_name_or_path\n\t        self.max_length = max_length \n\t        self.model:Model = AutoModel.from_pretrained(model_path_or_repo_id = model_name_or_path,\n\t            model_file=model_file,\n\t            model_type=model_type,\n\t            session_config=session_config,\n\t            tokenizer_path_or_repo_id=tokenizer_path_or_repo_id,\n", "            lora_paths=lora_paths,\n\t            verbose=verbose,\n\t            use_hf_tokenizer=use_hf_tokenizer,\n\t            default_quantization=default_quantization,\n\t            default_container=default_container)\n\t    def _ensure_token_limit(self, prompt: Union[str, List[Dict[str, str]]]) -> Union[str, List[Dict[str, str]]]:\n\t        \"\"\"Ensure that length of the prompt and answer is within the maximum token length of the PromptModel.\n\t        :param prompt: Prompt text to be sent to the generative model.\n\t        \"\"\"\n\t        if not isinstance(prompt, str):\n", "            raise ValueError(f\"Prompt must be of type str but got {type(prompt)}\")\n\t        context_length = self.model.config.context_length\n\t        tokenized_prompt = self.model.tokenize(prompt)\n\t        if len(tokenized_prompt) + self.max_length > context_length:\n\t            logger.warning(\n\t            \"The prompt has been truncated from %s tokens to %s tokens so that the prompt length and \"\n\t            \"answer length (%s tokens) fit within the max token limit (%s tokens). \"\n\t            \"Shorten the prompt to prevent it from being cut off\",\n\t            len(tokenized_prompt),\n\t            max(0, context_length -  self.max_length),\n", "            self.max_length,\n\t            context_length,\n\t            )\n\t            return self.model.decode(tokenized_prompt[:max(0, context_length -  self.max_length)])\n\t        return prompt\n\t    def invoke(self, *args, **kwargs):\n\t        \"\"\"\n\t        It takes a prompt and returns a list of generated text using the underlying model.\n\t        :return: A list of generated text.\n\t        \"\"\"\n", "        prompt = kwargs.pop(\"prompt\",None)\n\t        if not prompt:\n\t            raise ValueError(\"`prompt` cannot be None\")\n\t        generation_config = kwargs.pop(\"generation_config\", GenerationConfig())\n\t        generation_config.max_new_tokens = self.max_length\n\t        stream = kwargs.pop(\"stream\", False)\n\t        stream_handler = kwargs.get(\"stream_handler\", DefaultTokenStreamingHandler())\n\t        generated_text = []\n\t        for token in self.model.stream(prompt=prompt,generation_config=generation_config):\n\t            generated_text.append(token)\n", "            if stream:\n\t                stream_handler(token)\n\t        return generated_text\n\t    def supports(cls, model_name_or_path: str, **kwargs) -> bool:\n\t        \"\"\"\n\t        Checks if the given model is supported by this invocation layer.\n\t        :param model_name_or_path: The name or path of the model.\n\t        :param kwargs: Additional keyword arguments passed to the underlying model which might be used to determine\n\t        if the model is supported.\n\t        :return: True if this invocation layer supports the model, False otherwise.\n", "        \"\"\"\n\t        #I guess there is not much to validate here ¯\\_(ツ)_/¯\n\t        return model_name_or_path is not None and len(model_name_or_path) > 0"]}
{"filename": "examples/haystack_example.py", "chunked_list": ["from haystack.nodes import PromptModel\n\tfrom llm_rs.haystack import RustformersInvocationLayer\n\tfrom llm_rs import KnownModels,SessionConfig\n\t#Enable `use_gpu` to use GPU acceleration\n\tsession_config = SessionConfig(use_gpu=False)\n\tmodel = PromptModel(\"TheBloke/Llama-2-7B-Chat-GGML\",\n\t                    max_length=4096,\n\t                    invocation_layer_class=RustformersInvocationLayer,\n\t                    model_kwargs={\n\t                        \"model_file\":\"llama-2-7b-chat.ggmlv3.q4_K_S.bin\",\n", "                        \"session_config\":session_config,\n\t                        \"verbose\":True,\n\t                        })\n\tprompt= \"\"\"\n\tSystem: You are a helpful, respectful and honest assistant.\n\tUser: Tell me a Story about a Lama riding the crab named Ferris in about 1000 words.\n\tAssistant:\n\t\"\"\"\n\tmodel.invoke(prompt=prompt,stream=True)"]}
{"filename": "examples/langchain_example.py", "chunked_list": ["from llm_rs.langchain import RustformersLLM\n\tfrom llm_rs import KnownModels, SessionConfig\n\tfrom langchain import PromptTemplate\n\tfrom langchain.chains import LLMChain\n\tfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n\ttemplate=\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{instruction}\n\t### Response:\n\tAnswer:\"\"\"\n", "prompt = PromptTemplate(input_variables=[\"instruction\"],template=template,)\n\tllm = RustformersLLM(model_path_or_repo_id=\"TheBloke/Nous-Hermes-13B-GGML\",\n\t                     model_file=\"nous-hermes-13b.ggmlv3.q4_K_S.bin\",\n\t                     verbose=True,\n\t                     model_type=KnownModels.Llama,\n\t                     session_config=SessionConfig(use_gpu=True),\n\t                     callbacks=[StreamingStdOutCallbackHandler()]\n\t)\n\tchain = LLMChain(llm=llm, prompt=prompt)\n\tchain.run(\"Write me some Cypher Querry language examples for Neo4j. Try to use the example movie dataset. Give me 5 examples of how to create nodes and relationships and how to query them.\")"]}
