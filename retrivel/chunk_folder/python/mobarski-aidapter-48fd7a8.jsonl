{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tsetup(\n\t\tname='aidapter',\n\t\tversion='0.6.3',\n\t\tdescription='AI adapter / facade',\n\t\tauthor='Maciej Obarski',\n\t\tinstall_requires=[\n\t\t\t'retry',\n\t        'tqdm',\n\t        'requests',\n", "        'diskcache',\n\t\t],\n\t\tpackages=find_packages()\n\t)\n"]}
{"filename": "aidapter/api_hf.py", "chunked_list": ["from . import base\n\timport requests\n\timport json\n\timport os\n\t# TODO: handle error messages\n\tdef hf_api_query(payload, model_id, endpoint):\n\t    api_url = f\"https://api-inference.huggingface.co/{endpoint}/{model_id}\"\n\t    headers = {'Authorization': f'Bearer {os.environ[\"HF_API_TOKEN\"]}'} # TODO\n\t    #\n\t    data = json.dumps(payload)\n", "    raw_resp = requests.request(\"POST\", api_url, headers=headers, data=data)\n\t    resp = json.loads(raw_resp.content.decode(\"utf-8\"))\n\t    return resp\n\t# === TEXT ========================================================================================\n\tclass TextModel(base.CompletionModel):\n\t    # TODO\n\t    def transform_one(self, prompt, **kw) -> dict:\n\t        #\n\t        resp = hf_api_query(prompt, self.name, 'models')\n\t        output_text = resp[0]['generated_text'] \n", "        #\n\t        out = {}\n\t        out['output'] = output_text\n\t        return out\n\t# === EMBEDDING ===================================================================================\n\t# REF: https://huggingface.co/blog/getting-started-with-embeddings\n\t# REF: https://huggingface.co/spaces/mteb/leaderboard\n\t# model_id = 'thenlper/gte-small' # model size: 0.07GB, width: 384\n\t# model_id = 'BAAI/bge-small-en'  # model size: 0.13GB, width: 384\n\tclass EmbeddingModel(base.EmbeddingModel):\n", "    def transform_one(self, text, **kw):\n\t        return self.embed_batch([text], **kw)[0]\n\t    def embed_batch(self, texts, **kw):\n\t        limit = kw.get('limit')\n\t        #\n\t        resp = hf_api_query(texts, self.name, 'pipeline/feature-extraction')\n\t        #\n\t        out = []\n\t        for x in resp:\n\t            out.append({\n", "                 'output': x[:limit],\n\t            })\n\t        return out\n"]}
{"filename": "aidapter/base.py", "chunked_list": ["from tqdm import tqdm\n\tfrom retry.api import retry_call\n\tfrom multiprocessing.dummy import Pool\n\tfrom itertools import islice\n\tfrom datetime import date\n\tfrom time import time\n\timport hashlib\n\t# TODO: callbacks: before / after, one vs many\n\t# COMPLETION\n\tclass BaseModel:\n", "    RENAME_KWARGS = {}\n\t    DEFAULT_KWARGS = {}\n\t    def __init__(self, name, api_kwargs, options=''):\n\t        self.kwargs = self.DEFAULT_KWARGS.copy()\n\t        self.kwargs.update(api_kwargs)\n\t        self.options = options\n\t        self.name = name\n\t        self.cache = DummyKV()\n\t        self.usage = DummyKV()\n\t        self.retry_tries = 5\n", "        self.retry_delay = 0.1\n\t        self.retry_backoff = 3\n\t        self.workers = 4\n\t        self.batch = 1\n\t        self.show_progress = False\n\t        self.id = '' # this will be set by the factory function\n\t    # INTERNAL\n\t    def get_api_kwargs(self, kw):\n\t        kwargs = self.kwargs.copy()\n\t        for k in self.DEFAULT_KWARGS:\n", "            if k in kw:\n\t                kwargs[k] = kw[k]\n\t        return kwargs\n\t    def rename_kwargs(self, kwargs):\n\t        return {self.RENAME_KWARGS.get(k,k):v for k,v in kwargs.items()}\n\t    def register_usage(self, usage):\n\t        if usage:\n\t            keys = self.get_usage_keys()\n\t            for key in keys:\n\t                data = self.usage.get(key, {})\n", "                use_incr = hasattr(data, 'incr')\n\t                for k,v in usage.items():\n\t                    if use_incr:\n\t                        data.incr(k, v)\n\t                    else:\n\t                        data[k] = data.get(k,0) + v\n\t                self.usage[key] = data\n\t    def get_usage_keys(self):\n\t        day = str(date.today()) # iso format\n\t        keys = [\n", "            f'total:{self.name}',\n\t            f'day:{day}:{self.name}',\n\t        ]\n\t        return keys\n\t    # TRANSFORM\n\t    def transform(self, input, debug=False, cache='use', **kw):\n\t        if type(input)==str:\n\t            resp = self.transform_one_cached(input, cache=cache, **kw)\n\t            out = resp['output']\n\t        else:\n", "            resp = self.transform_many(input, cache=cache, **kw)\n\t            out = [x['output'] for x in resp]\n\t        self.usage.sync() # TODO: only if changed\n\t        self.cache.sync() # TODO: only if changed\n\t        if debug:\n\t            return resp\n\t        else:\n\t            return out\n\t    def transform_one_retry(self, input, **kw) -> dict:\n\t        return retry_call(\n", "            self.transform_one,\n\t            fargs=[input],\n\t            fkwargs=kw,\n\t            tries=self.retry_tries,\n\t            delay=self.retry_delay,\n\t            backoff=self.retry_backoff,\n\t        )\n\t    def transform_many(self, inputs, cache='use', **kw) -> list[dict]:\n\t        def worker(prompt):\n\t            return self.transform_one_cached(prompt, cache=cache, register=False, **kw)\n", "        data = inputs if self.batch==1 else batched(inputs, self.batch)\n\t        # TODO batched\n\t        with Pool(self.workers) as pool:\n\t            out = []\n\t            with tqdm(total=len(data), disable=not self.show_progress) as progress:\n\t                for x in pool.imap(worker, data):\n\t                    out.append(x)\n\t                    self.register_usage(x.get('usage',{}))\n\t                    progress.update()\n\t        return out\n", "    # TODO: cache=save\n\t    # TODO: dont save cache if empty\n\t    def transform_one_cached(self, input, cache='use', register=True, **kw) -> dict:\n\t        t0 = time()\n\t        kwargs = self.get_api_kwargs(kw)\n\t        cache_key = self.get_cache_key(input, kwargs, kw)\n\t        if self.skip_cache_condition(kwargs, kw, cache):\n\t            resp = self.transform_one_retry(input, **kw)\n\t            resp['usage'] = resp.get('usage', {})\n\t            resp['usage']['cache_skip'] = 1\n", "        else:\n\t            if cache_key in self.cache:\n\t                resp = self.cache[cache_key]\n\t                resp['usage'] = {f'cached_{k}':v for k,v in resp.get('usage',{}).items() if 'cache' not in k}\n\t                resp['usage']['cache_hit'] = 1\n\t            else:\n\t                resp = self.transform_one_retry(input, **kw)\n\t                resp['usage'] = resp.get('usage', {})\n\t                resp['usage']['cache_miss'] = 1\n\t                self.cache[cache_key] = resp\n", "        resp['usage']['time'] = time() - t0\n\t        if register:\n\t            self.register_usage(resp['usage'])\n\t        return resp\n\t    def skip_cache_condition(self, kwargs, kw, cache):\n\t        return cache in ('skip','save')\n\t    def get_cache_key(self, input, kwargs, kw):\n\t        all_kwargs = kwargs.copy()\n\t        all_kwargs.update(kw)\n\t        kwargs_str = str([(k,all_kwargs[k]) for k in sorted(all_kwargs)])\n", "        cache_key = self.name +':'+ md5(f'{input}:{kwargs_str}')\n\t        return cache_key\n\tclass CompletionModel(BaseModel):\n\t    DEFAULT_KWARGS = {'temperature':0, 'stop':[], 'limit':100}\n\t    def complete(self, prompt, debug=False, cache='use', **kw):\n\t        return self.transform(prompt, debug=debug, cache=cache, **kw)\n\t    def skip_cache_condition(self, kwargs, kw, cache):\n\t        return (kwargs.get('temperature',0)!=0 or cache=='skip') and cache!='force'\n\t    # MOCK\n\t    def transform_one(self, prompt, **kw) -> dict:\n", "        \"mock\"\n\t        kwargs = self.get_api_kwargs(kw)\n\t        # mock\n\t        system = kw.get('system','')\n\t        full_prompt = f'{system}\\n\\n{prompt}' if system else prompt\n\t        #\n\t        kwargs = self.rename_kwargs(kwargs)\n\t        from time import sleep\n\t        sleep(1)\n\t        resp = {}\n", "        #\n\t        resp['output'] = f'{full_prompt} DUMMY RESPONSE'\n\t        resp['usage'] = {'dummy_tokens': 3}\n\t        resp['kwargs'] = kwargs\n\t        return resp\n\t# EMBEDDING\n\tclass EmbeddingModel(BaseModel):\n\t    def embed(self, text, debug=False, cache='use', **kw):\n\t        return self.transform(text, debug=debug, cache=cache, **kw)\n\t    # MOCK\n", "    def transform_one(self, text, debug=False, cache='use', **kw):\n\t        limit = kw.get('limit')\n\t        return {\n\t            'output': [1,2,3,4,5][:limit],\n\t        }\n\t# HELPERS\n\tdef md5(text):\n\t    return hashlib.md5(text.encode()).hexdigest()\n\tdef batched(data, n, as_type=list):\n\t    it = iter(data)\n", "    while batch := as_type(islice(it, n)):\n\t        yield batch\n\tclass DummyKV(dict):\n\t    \"in-memory key-value store with shelve-like interface\"\n\t    def sync(self):\n\t        pass\n\t    def close(self):\n\t        pass\n"]}
{"filename": "aidapter/api_cohere.py", "chunked_list": ["# REF: https://cohere.ai/pricing\n\t# REF: https://dashboard.cohere.ai/api-keys\n\t# REF: https://docs.cohere.ai/reference/generate\n\t# REF: https://docs.cohere.ai/reference/embed\n\t# REF: https://docs.cohere.ai/reference/tokenize\n\tfrom . import base\n\timport cohere\n\timport sys\n\timport os\n\tdef use_key(key):\n", "\tcohere.api_key = key\n\tif not getattr(cohere, 'api_key', None):\n\t\tuse_key(os.getenv('CO_API_KEY',''))\n\tclass TextModel(base.CompletionModel):\n\t    RENAME_KWARGS  = {'stop':'stop_sequences', 'limit':'max_tokens'}\n\t    def __init__(self, name, kwargs):\n\t        super().__init__(name, kwargs)\n\t        self.client = cohere.Client(cohere.api_key)\n\t    def transform_one(self, prompt, **kw) -> dict:\n\t        kwargs = self.get_api_kwargs(kw)\n", "        kwargs['stop'] = kwargs.get('stop') or [] # FIX empty value\n\t        kwargs['model'] = self.name\n\t        #\n\t        system = kw.get('system','')\n\t        start = kw.get('start','')\n\t        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n\t        full_prompt += start\n\t        kwargs['prompt'] = full_prompt\n\t        #\n\t        kwargs = self.rename_kwargs(kwargs)\n", "        resp = self.client.generate(**kwargs)\n\t        output_text = resp[0]\n\t        #\n\t        out = {}\n\t        out['output'] = start + output_text # TODO: detect and handle start duplication\n\t        out['usage'] = {} # TODO\n\t        out['kwargs'] = kwargs\n\t        out['resp'] = resp\n\t        # TODO usage\n\t        # TODO error\n", "        return out\n\tclass EmbeddingModel(base.EmbeddingModel):\n\t    def __init__(self, name, kwargs):\n\t        super().__init__(name, kwargs)\n\t        self.client = cohere.Client(cohere.api_key)\n\t    def transform_one(self, text, **kw):\n\t        return self.embed_batch([text], **kw)[0]\n\t    def embed_batch(self, texts, **kw):\n\t        limit = kw.get('limit')\n\t        resp = self.client.embed(texts, model=self.name)\n", "        #\n\t        out = []\n\t        for x in resp.embeddings:\n\t            out.append({'output': x[:limit]})\n\t        return out\n"]}
{"filename": "aidapter/api_hf2.py", "chunked_list": ["from . import base2 as base\n\timport requests\n\timport json\n\timport os\n\tdef hf_api_query(payload, model_id, endpoint):\n\t    api_url = f\"https://api-inference.huggingface.co/{endpoint}/{model_id}\"\n\t    headers = {'Authorization': f'Bearer {os.environ[\"HF_API_TOKEN\"]}'} # TODO\n\t    #\n\t    data = json.dumps(payload)\n\t    raw_resp = requests.request(\"POST\", api_url, headers=headers, data=data)\n", "    resp = json.loads(raw_resp.content.decode(\"utf-8\"))\n\t    # handle error messages\n\t    if isinstance(resp, dict) and 'error' in resp:\n\t        error = resp['error']\n\t        est_time = resp.get('estimated_time')\n\t        msg = error\n\t        if est_time:\n\t            msg += f' (estimated time: {est_time:0.1f}s)'\n\t        raise ValueError(msg)\n\t    return resp\n", "# === EMBEDDING ===================================================================================\n\t# REF: https://huggingface.co/blog/getting-started-with-embeddings\n\t# REF: https://huggingface.co/spaces/mteb/leaderboard\n\t# model_id = 'thenlper/gte-small' # model size: 0.07GB, width: 384\n\t# model_id = 'BAAI/bge-small-en'  # model size: 0.13GB, width: 384\n\t# TODO: different models -> different output shapes\n\tclass EmbeddingModel(base.BaseModelV2):\n\t    brand = 'huggingface'\n\t    def embed(self, inputs, **kwargs):\n\t        return self.transform(inputs, **kwargs)\n", "    def transform_batch(self, inputs, **kwargs):\n\t        limit = kwargs.get('limit')\n\t        resp = hf_api_query(inputs, self.name, 'pipeline/feature-extraction')\n\t        output = [x[:limit] for x in resp]\n\t        self.register_usage({'api-calls':1})\n\t        return output\n\t# === TEXT ========================================================================================\n\tclass TextModel(base.BaseModelV2):\n\t    brand = 'huggingface'\n\t    def complete(self, prompt, **kwargs):\n", "        return self.transform_many(prompt, **kwargs)\n\t    def transform_batch(self, prompts, **kwargs):\n\t        resp = hf_api_query(prompts, self.name, 'models')\n\t        output = [x['generated_text'] for x in resp]\n\t        return output\n"]}
{"filename": "aidapter/api_transformers.py", "chunked_list": ["from . import base\n\timport transformers\n\timport torch\n\timport sys\n\timport os\n\tdef use_key(key):\n\t\tpass\n\tclass TextModel(base.CompletionModel):\n\t    RENAME_KWARGS  = {'limit':'max_new_tokens'}\n\t    def __init__(self, name, kwargs, options):\n", "        super().__init__(name, kwargs)\n\t        self.tokenizer = transformers.AutoTokenizer.from_pretrained(name)\n\t        # OPTIONS\n\t        kw = {}\n\t        if '16bit' in options:\n\t            kw['torch_dtype'] = torch.float16\n\t        elif 'bloat16' in options:\n\t            kw['torch_dtype'] = torch.bfloat16\n\t        elif '8bit' in options:\n\t            kw['load_in_8bit'] = True\n", "        elif '4bit' in options:\n\t            kw['load_in_4bit'] = True\n\t        else:\n\t             kw['torch_dtype'] = 'auto'\n\t        if 'trust' in options:\n\t            kw['trust_remote_code'] = True\n\t        self.model = transformers.AutoModelForCausalLM.from_pretrained(name, device_map=\"auto\", **kw)\n\t    def transform_one(self, prompt, **kw) -> dict:\n\t        kwargs = self.get_api_kwargs(kw)\n\t        kwargs['stop'] = kwargs.get('stop') or [] # FIX empty value\n", "        kwargs['model'] = self.name\n\t        # full_prompt\n\t        system = kw.get('system','')\n\t        start = kw.get('start','')\n\t        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n\t        full_prompt += start\n\t        kwargs['prompt'] = full_prompt\n\t        #\n\t        #kwargs = self.rename_kwargs(kwargs) # NOT USED - direct mapping below\n\t        final_kwargs = dict(\n", "            max_new_tokens = kwargs['limit'],\n\t            temperature = kwargs['temperature'],\n\t            pad_token_id = self.tokenizer.eos_token_id,\n\t        )\n\t        # stop early if stop criteria is met\n\t        if kwargs['stop']:\n\t            def stop_fun(ids, scores, **_):\n\t                output_text = self.tokenizer.decode(ids[0])[len(full_prompt):]\n\t                for s in kwargs['stop']:\n\t                    if s in output_text:\n", "                        return True\n\t            final_kwargs['stopping_criteria'] = [stop_fun]\n\t        #\n\t        prompt_tokens = self.tokenizer.encode(full_prompt, return_tensors='pt')\n\t        resp = self.model.generate(\n\t                prompt_tokens.to(\"cuda\"),\n\t                **final_kwargs\n\t            )\n\t        resp_text = self.tokenizer.decode(resp[0], skip_special_tokens=True)\n\t        output_text = resp_text[len(full_prompt):]\n", "        # remove stop criteria from output\n\t        if kwargs['stop']:\n\t            for s in sorted(kwargs['stop'], key=lambda x: len(x), reverse=True):\n\t                if s in output_text:\n\t                    output_text = output_text.split(s)[0]\n\t        #\n\t        out = {}\n\t        out['output'] = start + output_text\n\t        out['usage'] = {\n\t            'prompt_tokens': prompt_tokens.shape[1],\n", "            'resp_tokens': resp.shape[1],\n\t            'total_tokens': prompt_tokens.shape[1] + resp.shape[1],\n\t            'prompt_chars': len(full_prompt),\n\t            'resp_chars': len(out['output']),\n\t            'total_chars': len(full_prompt) + len(out['output']),\n\t        }\n\t        out['kwargs'] = final_kwargs\n\t        out['resp'] = {'resp':resp, 'prompt_tokens':prompt_tokens}\n\t        # TODO usage\n\t        # TODO error\n", "        return out\n\t    def raw_embed_one(self, text, **kw):\n\t        input = self.tokenizer.encode(text, return_tensors='pt')\n\t        outputs = self.model(input, output_hidden_states=True)\n\t        last_hidden_state = outputs.hidden_states[-1][0]\n\t        fun = lambda x: x[0].tolist()\n\t        return fun(last_hidden_state)\n"]}
{"filename": "aidapter/api_anthropic.py", "chunked_list": ["# REF: https://console.anthropic.com/docs/api/reference\n\tfrom . import base\n\timport anthropic\n\timport sys\n\timport os\n\tdef use_key(key):\n\t\tanthropic.api_key = key\n\tif not getattr(anthropic, 'api_key', None):\n\t\tuse_key(os.getenv('ANTHROPIC_API_KEY',''))\n\tclass ChatModel(base.CompletionModel):\n", "    RENAME_KWARGS  = {'stop':'stop_sequences', 'limit':'max_tokens_to_sample'}\n\t    def __init__(self, name, kwargs):\n\t        super().__init__(name, kwargs)\n\t        self.client = anthropic.Client(anthropic.api_key)\n\t    def transform_one(self, prompt, **kw) -> dict:\n\t        kwargs = self.get_api_kwargs(kw)\n\t        kwargs['stop'] = kwargs.get('stop') or [] # FIX empty value\n\t        kwargs['model'] = self.name\n\t        #\n\t        system = kw.get('system','')\n", "        start = kw.get('start','')\n\t        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n\t        full_prompt += start\n\t        kwargs['prompt'] = f\"{anthropic.HUMAN_PROMPT} {full_prompt}{anthropic.AI_PROMPT}\"\n\t        #\n\t        kwargs = self.rename_kwargs(kwargs)\n\t        resp = self.client.completion(**kwargs)\n\t        output_text = resp.get('completion','')\n\t        #\n\t        out = {}\n", "        out['output'] = start + output_text # TODO detect and handle start duplication\n\t        out['usage'] = {\n\t            'prompt_tokens': anthropic.count_tokens(kwargs['prompt']),\n\t            'resp_tokens': anthropic.count_tokens(output_text),\n\t            'total_tokens': anthropic.count_tokens(kwargs['prompt']) + anthropic.count_tokens(output_text),\n\t            'prompt_chars': len(kwargs['prompt']),\n\t            'resp_chars': len(output_text),\n\t            'total_chars': len(kwargs['prompt']) + len(output_text),\n\t        }\n\t        out['kwargs'] = kwargs\n", "        out['resp'] = resp\n\t        # TODO usage\n\t        # TODO resp['error']\n\t        return out\n"]}
{"filename": "aidapter/__init__.py", "chunked_list": ["# TODO: kind : brand : name ???\n\tdef model(model_id, **kwargs):\n\t\t\"model factory function\"\n\t\tbrand,_,name = model_id.partition(':')\n\t\tname,_,options = name.partition(':')\n\t\t#\n\t\tif brand=='anthropic':\n\t\t\tfrom . import api_anthropic\n\t\t\tmodel =  api_anthropic.ChatModel(name, kwargs)\n\t\telif brand=='openai':\n", "\t\tfrom . import api_openai\n\t\t\tfrom . import api_openai2\n\t\t\tif 'embed' in options or 'embedding' in name:\n\t\t\t\tmodel = api_openai2.EmbeddingModel(name, kwargs, options)\n\t\t\telif name.startswith('gpt-'):\n\t\t\t\tmodel = api_openai.ChatModel(name, kwargs)\n\t\t\telse:\n\t\t\t\tmodel = api_openai.TextModel(name, kwargs)\n\t\telif brand=='cohere':\n\t\t\tfrom . import api_cohere\n", "\t\tif 'embed' in name:\n\t\t\t\tmodel = api_cohere.EmbeddingModel(name, kwargs)\n\t\t\telse:\n\t\t\t\tmodel = api_cohere.TextModel(name, kwargs)\n\t\telif brand=='transformers':\n\t\t\tfrom . import api_transformers\n\t\t\tmodel = api_transformers.TextModel(name, kwargs, options)\n\t\telif brand=='vllm':\n\t\t\tfrom . import api_vllm\n\t\t\tmodel = api_vllm.TextModel(name, kwargs, options)\n", "\telif brand=='sentence-transformers':\n\t\t\tfrom . import api_sentence_transformers\n\t\t\tmodel = api_sentence_transformers.EmbeddingModel(name, kwargs, options)\n\t\telif brand=='hf':\n\t\t\tfrom . import api_hf\n\t\t\tif 'embed' in options:\n\t\t\t\tmodel = api_hf.EmbeddingModel(name, kwargs, options)\n\t\t\telse:\n\t\t\t\tmodel = api_hf.TextModel(name, kwargs, options)\n\t\telif brand=='hf2' or brand=='huggingface':\n", "\t\tfrom . import api_hf2\n\t\t\tif 'embed' in options:\n\t\t\t\tmodel = api_hf2.EmbeddingModel(name, kwargs, options)\n\t\t\telse:\n\t\t\t\tmodel = api_hf2.TextModel(name, kwargs, options)\n\t\telse:\n\t\t\traise ValueError(f'unknown brand: {brand}')\n\t\t#\n\t\tmodel.id = model_id\n\t\tmodel.id_safe = model_id.replace('/','--')\n", "\treturn model\n"]}
{"filename": "aidapter/api_sentence_transformers.py", "chunked_list": ["from . import base\n\tfrom sentence_transformers import SentenceTransformer\n\tdef use_key(key):\n\t\tpass\n\tclass EmbeddingModel(base.EmbeddingModel):\n\t    def __init__(self, name, kwargs, options):\n\t        super().__init__(name, kwargs)\n\t        self.model = SentenceTransformer(name)\n\t    def transform_one(self, text, **kw):\n\t        return self.embed_batch([text], **kw)[0]\n", "    def embed_batch(self, texts, **kw):\n\t        limit = kw.get('limit')\n\t        resp = self.model.encode(texts)\n\t        #\n\t        out = []\n\t        for x in resp:\n\t            out.append({'output': list(x)[:limit]})\n\t        return out\n"]}
{"filename": "aidapter/api_openai.py", "chunked_list": ["# REF: https://platform.openai.com/docs/api-reference/completions\n\tfrom . import base\n\timport openai\n\timport json\n\timport sys\n\timport os\n\tdef use_key(key):\n\t    openai.api_key = key\n\tif not openai.api_key:\n\t    use_key(os.getenv('OPENAI_API_KEY',''))\n", "class ChatModel(base.CompletionModel):\n\t    RENAME_KWARGS = {'limit':'max_tokens'}\n\t    def transform_one(self, prompt, **kw) -> dict:\n\t        kwargs = self.get_api_kwargs(kw)\n\t        kwargs['stop'] = kwargs.get('stop') or None # FIX empty value\n\t        kwargs['model'] = self.name\n\t        #\n\t        system = kw.get('system','')\n\t        start = kw.get('start','')\n\t        messages = []\n", "        if system:\n\t              messages += [{'role':'system', 'content':system}]\n\t        messages += [{'role':'user', 'content':prompt+start}]\n\t        kwargs['messages'] = messages\n\t        #kwargs['max_tokens'] = limit # TODO\n\t        functions = kw.get('functions',[]) or kwargs.get('functions',[]) # NEW\n\t        kwargs['functions'] = [get_signature(f) for f in functions]\n\t        #\n\t        kwargs = self.rename_kwargs(kwargs)\n\t        resp = openai.ChatCompletion.create(**kwargs)\n", "        output_text = resp['choices'][0]['message'].get('content','')\n\t        fc = function_call = resp['choices'][0]['message'].get('function_call',{})\n\t        #\n\t        out = {}\n\t        if function_call:\n\t            out['output'] = {'function_name':fc['name'], 'arguments':json.loads(fc['arguments'])}\n\t        else:\n\t            out['output'] = start + output_text # TODO: detect and handle start duplication\n\t        out['usage'] = resp['usage']\n\t        out['kwargs'] = kwargs\n", "        out['resp'] = resp\n\t        return out\n\tclass TextModel(base.CompletionModel):\n\t    RENAME_KWARGS = {'limit':'max_tokens'}\n\t    def transform_one(self, prompt, **kw) -> dict:\n\t        kwargs = self.get_api_kwargs(kw)\n\t        kwargs['stop'] = kwargs.get('stop') or None # FIX empty value\n\t        kwargs['model'] = self.name\n\t        #\n\t        system = kw.get('system','')\n", "        start = kw.get('start','')\n\t        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n\t        full_prompt += start\n\t        kwargs['prompt'] = full_prompt\n\t        #\n\t        kwargs = self.rename_kwargs(kwargs)\n\t        resp = openai.Completion.create(**kwargs)\n\t        output_text = resp['choices'][0]['text']\n\t        #\n\t        out = {}\n", "        out['output'] = start + output_text\n\t        out['usage'] = resp['usage']\n\t        out['kwargs'] = kwargs\n\t        out['resp'] = resp\n\t        return out\n\tclass EmbeddingModel(base.EmbeddingModel):\n\t    def transform_one(self, text, **kw):\n\t        return self.embed_batch([text], **kw)[0]\n\t    def embed_batch(self, texts, **kw):\n\t        limit = kw.get('limit')\n", "        kwargs = {'input':texts, 'model':self.name}\n\t        resp = openai.Embedding.create(**kwargs)\n\t        #\n\t        out = []\n\t        for x in resp['data']:\n\t            out.append({\n\t                 'output': x['embedding'][:limit],\n\t            })\n\t        return out\n\timport inspect\n", "def get_signature(fun):\n\t    params = inspect.signature(fun).parameters\n\t    param_dict = {p:{} for p in params}\n\t    return {\n\t        'name':fun.__name__,\n\t        'description':fun.__doc__ or '',\n\t        'parameters':{'type':'object','properties':param_dict}\n\t    }\n"]}
{"filename": "aidapter/base2.py", "chunked_list": ["from multiprocessing.dummy import Pool\n\tfrom retry import retry\n\t# TODO: keys ???\n\t# TODO: kwargs vs options ???\n\t# DONE: iter vs list vs single\n\t# DONE: as_iter\n\t# DONE: kwargs\n\t# DONE: usage\n\t# DONE: retry\n\t# DONE: cache\n", "# DONE: progress\n\t# DONE: batch\n\t# DONE: workers\n\tclass BaseModelV2:\n\t    def __init__(self, name, kwargs, options):\n\t        \"Initialize a model.\"\n\t        self.name = name\n\t        self.brand = 'base-v2'\n\t        self.usage = {}\n\t        self.cache = {}\n", "        self.workers = 2\n\t        self.batch = 4\n\t        self.options = options\n\t        self.model_kwargs = kwargs\n\t        self.retry_kwargs = None\n\t        self.memoize_kwargs = {'name':f'{name}:default'}\n\t    def transform(self, inputs:str|list[str], as_iter=False, **kwargs):\n\t        \"Transform a single input or a list of inputs.\"\n\t        if isinstance(inputs, str):\n\t            return list(self.transform_many([inputs], **kwargs))[0]\n", "        elif as_iter:\n\t            return self.transform_many(inputs, **kwargs) # returns generator\n\t        else:\n\t            return list(self.transform_many(inputs, **kwargs))\n\t    def transform_many(self, inputs: list[str], **kwargs):\n\t        \"Transform a list of inputs in batches / parallel workers.\"\n\t        data = inputs if self.batch == 1 else batched(inputs, self.batch)\n\t        # build worker\n\t        def worker(_inputs):\n\t            return self.transform_batch(_inputs, **kwargs)\n", "        if self.retry_kwargs:\n\t            worker = retry(**self.retry_kwargs)(worker)\n\t        if kwargs.get('cache',True):\n\t            worker = self.get_memoize()(worker)\n\t        # run workers\n\t        with Pool(self.workers) as pool:\n\t            for resp in pool.imap(worker, data):\n\t                yield from resp\n\t    def transform_batch(self, inputs, **kwargs):\n\t        \"Transform a batch of inputs (MOCK implementation)\"\n", "        from time import sleep\n\t        sleep(0.5)\n\t        resp = inputs\n\t        self.register_usage({'api-calls':1})\n\t        return resp\n\t    # # #\n\t    def register_usage(self, kv: dict):\n\t        \"Register usage metrics.\"\n\t        for k,v in kv.items():\n\t            key = self.get_usage_key(k)\n", "            incr(self.usage, key, v)\n\t    # # #\n\t    def get_memoize(self):\n\t        \"Get a memoization function.\"\n\t        try:\n\t            return self.cache.memoize(**self.memoize_kwargs)\n\t        except:\n\t            return lambda x:x\n\t    def get_usage_key(self, k):\n\t        \"Get a key for a given metric that will be tracked in the usage key-value store.\"\n", "        return f'{k}:{self.brand}:{self.name}'\n\t# HELPERS\n\tfrom itertools import islice\n\tdef batched(data, n, as_type=list):\n\t    \"Batch an iterable into fixed-length chunks.\"\n\t    it = iter(data)\n\t    while batch := as_type(islice(it, n)):\n\t        yield batch\n\tdef incr(obj, key, val):\n\t    \"Increment a value in a dict or diskcache (for tracking usage)\"\n", "    if hasattr(obj, 'incr'):\n\t        obj.incr(key, val)\n\t    else:\n\t        obj[key] = obj.get(key,0) + val\n\t# === SANDBOX =====================================================================================\n\tif __name__=='__main__':\n\t    import diskcache as dc\n\t    from functools import partial\n\t    from tqdm import tqdm\n\t    m = BaseModelV2('test',{},{})\n", "    m.batch = 2\n\t    m.workers = 1\n\t    print(m.transform('XXX'))\n\t    print(m.transform(['YYY']))\n\t    print(m.transform(['AAA','BBB']))\n\t    for data in [\"11 22 33 44 55 66 77 88 99\".split(' '), ['XX'], 'YY']:\n\t        pg = tqdm(total=len(data))\n\t        out = m.transform(data, as_iter=True)\n\t        for x in out:\n\t            pg.update(1)\n", "            print(x)\n\t    print(m.usage)\n"]}
{"filename": "tests/test_cohere.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\timport aidapter\n\tfor model_id in ['cohere:command-light']:\n\t    print(f'=== {model_id} ===')\n\t    model = aidapter.model(model_id)\n\t    model.retry_tries = 1\n\t    print(model.complete('2+2='))\n\t    print(model.complete(['2+2=','7*6=']))\n\t    print(model.complete('2+2=', system=\"answer with words only (don't use numbers)\"))\n\t    print(model.complete('2+2=', debug=True))\n", "    print(model.usage)\n\t    print()\n"]}
{"filename": "tests/test_stop.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\timport aidapter\n\tmodel_id = 'transformers:RWKV/rwkv-raven-3b'\n\t#model_id = 'openai:text-davinci-003'\n\t#model_id = 'openai:gpt-3.5-turbo'\n\t#model_id = 'anthropic:claude-v1'\n\t#model_id = 'cohere:command-light'\n\tmodel = aidapter.model(model_id)\n\tprint('===')\n\tprint(model.complete('Alice: Hello, my name is Alice.\\nBob:'))\n", "print('===')\n\tprint(model.complete('Alice: Hello, my name is Alice.\\n', start='Bob:'))\n\tprint('===')\n\tprint(model.complete('Alice: Hello, my name is Alice.\\n', start='Bob:', stop=['Alice:','.']))\n"]}
{"filename": "tests/test_hf_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\tfrom pprint import pprint\n\timport aidapter\n\tmodel = aidapter.model('hf:thenlper/gte-small:embed')\n\tvector = model.embed('mighty indeed', limit=5)\n\tpprint(vector)\n\tvectors = model.embed(['this is the way', 'so say we all'], limit=5)\n\tpprint(vectors)\n\tpprint(model.usage)\n\tmodel = aidapter.model('hf:gpt2')\n", "print(model.complete(['2+2=']))\n\tprint(model.complete(['2+2=','7*6=']))\n"]}
{"filename": "tests/test_openai_functions.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\t### \n\timport aidapter\n\tdef get_weather(city):\n\t    \"get weather info in a city; city must be all caps after ISO country code and a : separator (e.g. FR:PARIS)\"\n\t    pass\n\tmodel = aidapter.model('openai:gpt-3.5-turbo-0613')\n\tx=model.complete('Whats the weather in the capital of Poland?', functions=[get_weather])\n\tprint(x)\n\t# {'function_name': 'get_weather', 'arguments': {'city': 'PL:WARSAW'}}\n"]}
{"filename": "tests/test_hf2_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\tfrom pprint import pprint\n\timport diskcache as dc\n\timport aidapter\n\tfor model_id in ['huggingface:thenlper/gte-small:embed']:\n\t    model = aidapter.model(model_id)\n\t    model.cache = dc.Cache('/tmp/aidapter/hf')\n\t    pprint( model.embed(['mighty indeed'], limit=5) )\n\t    pprint( model.embed( 'mighty indeed',  limit=5) )\n\t    vectors = model.embed(['this is the way', 'so say we all'], limit=5, cache=False)\n", "    pprint(vectors)\n\t    print('USAGE', model.usage)\n"]}
{"filename": "tests/test_openai.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\timport aidapter\n\tfor model_id in ['openai:gpt-3.5-turbo','openai:text-davinci-003']:\n\t    print(f'=== {model_id} ===')\n\t    model = aidapter.model(model_id)\n\t    print(model.complete('2+2='))\n\t    print(model.complete(['2+2=','7*6=']))\n\t    print(model.complete('2+2=', system=\"answer with words only (don't use numbers)\"))\n\t    print(model.complete('2+2=', debug=True))\n\t    print(model.usage)\n", "    print()\n"]}
{"filename": "tests/test_anthropic.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\timport aidapter\n\tfor model_id in ['anthropic:claude-instant-v1.1']:\n\t    print(f'=== {model_id} ===')\n\t    model = aidapter.model(model_id, temperature=0.1)\n\t    print(model.complete('2+2='))\n\t    print(model.complete(['2+2=','7*6=']))\n\t    print(model.complete('2+2=', system=\"answer with words only (don't use numbers)\"))\n\t    print(model.complete('2+2=', debug=True))\n\t    print(model.usage)\n", "    print()\n"]}
{"filename": "tests/test_base.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\tfrom aidapter.base import BaseModel\n\timport shelve\n\tm = BaseModel('base',{'x':123})\n\tm.cache = shelve.open('/tmp/aidapter-cache.shelve')\n\tm.usage = shelve.open('/tmp/aidapter-usage.shelve')\n\tprint(m.complete('hello', stop=['aa','bb'], debug=True))\n\tprint(m.complete(range(13)))\n\tprint('-'*80)\n\tprint(m.complete(range(13)))\n", "print('-'*80)\n\tprint(m.complete(range(6), temperature=1.0))\n\tprint('-'*80)\n\tprint(m.cache)\n\tprint('-'*80)\n\tprint(dict(m.usage))\n"]}
{"filename": "tests/test_transformers.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\t#import os\n\t#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128' # PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 \n\timport aidapter\n\tfrom time import time\n\t#model_id = 'openai:text-ada-001'\n\t# model_id = 'transformers:roneneldan/TinyStories-33M'\n\t#model_id = 'transformers:RWKV/rwkv-raven-1b5'\n\t#model_id = 'transformers:tiiuae/falcon-rw-1b:trust'\n\t#model_id = 'transformers:RWKV/rwkv-raven-3b'\n", "# model_id = 'transformers:TheBloke/guanaco-7B-HF:4bit'\n\t# model_id = 'transformers:project-baize/baize-v2-7b:4bit'\n\t# model_id = 'transformers:tiiuae/falcon-7b:trust'\n\t# model_id = 'transformers:ehartford/Wizard-Vicuna-13B-Uncensored:4bit'\n\t# model_id = 'transformers:ehartford/WizardLM-30B-Uncensored:4bit'\n\t# model_id = 'transformers:timdettmers/guanaco-33b-merged:4bit'\n\t#model_id = 'transformers:WizardLM/WizardCoder-15B-V1.0:4bit'\n\t#model_id = 'transformers:cerebras/btlm-3b-8k-base:trust'\n\t#model_id = 'transformers:conceptofmind/LLongMA-2-7b:trust'\n\t#model_id = 'transformers:mosaicml/mpt-7b-8k-instruct:trust,16bit'\n", "#model_id = 'transformers:WizardLM/WizardLM-13B-V1.2:4bit'\n\tmodel_id = 'transformers:NousResearch/Nous-Hermes-Llama2-13b:4bit' # BEST\n\tfrom pprint import pprint\n\tprint(f'=== {model_id} ===')\n\tmodel = aidapter.model(model_id)\n\tmodel.retry_tries = 1\n\t#print(model.complete('there was a little girl who', debug=False))\n\t#print()\n\tmath_prompts = ['2+2=','7*6=','3-7=','2^8=']\n\tpprint(model.complete(math_prompts, debug=False, limit=5, stop=[]))\n", "pprint(model.complete(math_prompts, debug=False, limit=5, stop=[\"\\n\",\"2\"]))\n\tprint(model.id)\n\t# How would you compare fitd and pbta?\n\twhile True:\n\t    prompt = input('> ')\n\t    if prompt == '': break\n\t    t0 = time()\n\t    resp = model.complete(prompt)\n\t    print(resp)\n\t    dt = time()-t0\n", "    print(f\"DONE in {dt:.2f} seconds ({len(resp)/dt:.2f} chars/sec)\")\n\t# NOPE:\n\t# model_id = 'transformers:RWKV/rwkv-raven-14b:8bit' # NOPE\n\t# model_id = 'transformers:RWKV/rwkv-raven-14b:4bit' # NOPE\n\t# model_id = 'transformers:openaccess-ai-collective/mpt-7b-replit-update:4bit,trust' # NOPE\n\t# model_id = 'transformers:tiiuae/falcon-40b:4bit,trust' # NOPE\n"]}
{"filename": "tests/test_transformers_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\tfrom pprint import pprint\n\timport aidapter\n\tmodel = aidapter.model('sentence-transformers:multi-qa-mpnet-base-dot-v1')\n\tvector = model.embed('mighty indeed', limit=5)\n\tpprint(vector)\n\tvectors = model.embed(['this is the way', 'so say we all'], limit=5)\n\tpprint(vectors)\n\tpprint(model.usage)\n"]}
{"filename": "tests/test_openai_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\tfrom pprint import pprint\n\timport aidapter\n\tmodel = aidapter.model('openai:text-embedding-ada-002')\n\tvector = model.embed('mighty indeed', limit=5)\n\tpprint(vector)\n\tvectors = model.embed(['this is the way', 'so say we all'], limit=5)\n\tpprint(vectors)\n\tpprint(model.usage)\n"]}
{"filename": "tests/test_cohere_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\tfrom pprint import pprint\n\timport aidapter\n\tmodel = aidapter.model('cohere:embed-english-light-v2.0')\n\tvector = model.embed('mighty indeed', limit=5)\n\tpprint(vector)\n\tvectors = model.embed(['this is the way', 'so say we all'], limit=5)\n\tpprint(vectors)\n\tpprint(model.usage)\n"]}
