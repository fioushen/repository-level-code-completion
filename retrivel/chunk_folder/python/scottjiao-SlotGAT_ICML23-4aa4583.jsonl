{"filename": "NC/methods/SlotGAT/conv.py", "chunked_list": ["\"\"\"Torch modules for graph attention networks(GAT).\"\"\"\n\t# pylint: disable= no-member, arguments-differ, invalid-name\n\tfrom shutil import ExecError\n\timport torch as th\n\tfrom torch import nn\n\timport torch\n\tfrom dgl import function as fn\n\tfrom dgl.nn.pytorch import edge_softmax\n\tfrom dgl._ffi.base import DGLError\n\tfrom dgl.nn.pytorch.utils import Identity\n", "from dgl.utils import expand_as_pair\n\timport torch\n\timport torch.nn.functional as F\n\timport numpy as np\n\tfrom torch.profiler import profile, record_function, ProfilerActivity\n\t# pylint: disable=W0235\n\tclass slotGATConv(nn.Module):\n\t    \"\"\"\n\t    Adapted from\n\t    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n", "    \"\"\"\n\t    def __init__(self,\n\t                 edge_feats,\n\t                 num_etypes,\n\t                 in_feats,\n\t                 out_feats,\n\t                 num_heads,\n\t                 feat_drop=0.,\n\t                 attn_drop=0.,\n\t                 negative_slope=0.2,\n", "                 residual=False,\n\t                 activation=None,\n\t                 allow_zero_in_degree=False,\n\t                 bias=False,\n\t                 alpha=0.,\n\t                 num_ntype=None, eindexer=None,inputhead=False, dataRecorder=None):\n\t        super(slotGATConv, self).__init__()\n\t        self._edge_feats = edge_feats\n\t        self._num_heads = num_heads\n\t        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n", "        self._out_feats = out_feats\n\t        self._allow_zero_in_degree = allow_zero_in_degree\n\t        self.edge_emb = nn.Embedding(num_etypes, edge_feats) if edge_feats else None\n\t        self.eindexer=eindexer\n\t        self.num_ntype=num_ntype \n\t        self.attentions=None\n\t        self.dataRecorder=dataRecorder\n\t        if isinstance(in_feats, tuple):\n\t            raise NotImplementedError()\n\t        else:\n", "            self.fc = nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats, out_feats * num_heads)))\n\t        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False) if edge_feats else None\n\t        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats   *self.num_ntype)))\n\t        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats*self.num_ntype)))\n\t        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats))) if edge_feats else None\n\t        self.feat_drop = nn.Dropout(feat_drop)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\t        if residual:\n\t            if self._in_dst_feats != out_feats:\n", "                self.res_fc =nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats, out_feats * num_heads)))\n\t            else:\n\t                self.res_fc = Identity()\n\t        else:\n\t            self.register_buffer('res_fc', None)\n\t        self.reset_parameters()\n\t        self.activation = activation\n\t        self.bias = bias\n\t        self.alpha = alpha\n\t        self.inputhead=inputhead\n", "    def reset_parameters(self):\n\t        gain = nn.init.calculate_gain('relu')\n\t        if hasattr(self, 'fc'):\n\t            nn.init.xavier_normal_(self.fc, gain=gain)\n\t        else:\n\t            raise NotImplementedError()\n\t        nn.init.xavier_normal_(self.attn_l, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_r, gain=gain)\n\t        if self._edge_feats:\n\t            nn.init.xavier_normal_(self.attn_e, gain=gain) \n", "        if isinstance(self.res_fc, nn.Linear):\n\t            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n\t        elif isinstance(self.res_fc, Identity):\n\t            pass\n\t        elif isinstance(self.res_fc, nn.Parameter):\n\t            nn.init.xavier_normal_(self.res_fc, gain=gain)\n\t        if self._edge_feats:\n\t            nn.init.xavier_normal_(self.fc_e.weight, gain=gain) \n\t    def set_allow_zero_in_degree(self, set_value):\n\t        self._allow_zero_in_degree = set_value\n", "    def forward(self, graph, feat, e_feat,get_out=[\"\"], res_attn=None):\n\t        with graph.local_scope():\n\t            if not self._allow_zero_in_degree:\n\t                if (graph.in_degrees() == 0).any():\n\t                    raise DGLError('There are 0-in-degree nodes in the graph, '\n\t                                   'output for those nodes will be invalid. '\n\t                                   'This is harmful for some applications, '\n\t                                   'causing silent performance regression. '\n\t                                   'Adding self-loop on the input graph by '\n\t                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n", "                                   'the issue. Setting ``allow_zero_in_degree`` '\n\t                                   'to be `True` when constructing this module will '\n\t                                   'suppress the check and let the code run.')\n\t            if isinstance(feat, tuple):\n\t                raise NotImplementedError()\n\t            else:\n\t                #feature transformation first\n\t                h_src = h_dst = self.feat_drop(feat)   #num_nodes*(num_ntype*input_dim)\n\t                if self.inputhead:\n\t                    h_src=h_src.view(-1,1,self.num_ntype,self._in_src_feats)\n", "                else:\n\t                    h_src=h_src.view(-1,self._num_heads,self.num_ntype,int(self._in_src_feats/self._num_heads))\n\t                h_dst=h_src=h_src.permute(2,0,1,3).flatten(2)  #num_ntype*num_nodes*(in_feat_dim)\n\t                if \"getEmb\" in get_out:\n\t                    self.emb=h_dst.cpu().detach()\n\t                #self.fc with num_ntype*(in_feat_dim)*(out_feats * num_heads)\n\t                feat_dst = torch.bmm(h_src,self.fc)  #num_ntype*num_nodes*(out_feats * num_heads)\n\t                feat_src = feat_dst =feat_dst.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n\t                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n\t                if graph.is_block:\n", "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n\t                e_feat = self.edge_emb(e_feat) if self._edge_feats else None\n\t                e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)  if self._edge_feats else None\n\t                ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1) if self._edge_feats else 0  #(-1, self._num_heads, 1) \n\t                el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n\t                er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n\t                graph.srcdata.update({'ft': feat_src, 'el': el})\n\t                graph.dstdata.update({'er': er})\n\t                graph.edata.update({'ee': ee}) if self._edge_feats else None\n\t                graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n", "                e_=graph.edata.pop('e')\n\t                ee=graph.edata.pop('ee') if self._edge_feats else 0\n\t                e=e_+ee\n\t                e = self.leaky_relu(e)\n\t            # compute softmax\n\t            a=self.attn_drop(edge_softmax(graph, e))\n\t            if res_attn is not None:\n\t                a=a * (1-self.alpha) + res_attn * self.alpha \n\t            if self.dataRecorder[\"status\"]==\"FinalTesting\":\n\t                if \"attention\" not in self.dataRecorder[\"data\"]:\n", "                    self.dataRecorder[\"data\"][\"attention\"]=[]\n\t                self.dataRecorder[\"data\"][\"attention\"].append(a)\n\t            graph.edata['a'] = a\n\t            # then message passing\n\t            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n\t                             fn.sum('m', 'ft'))\n\t            rst = graph.dstdata['ft'] \n\t            # residual\n\t            if self.res_fc is not None:\n\t                if self._in_dst_feats != self._out_feats:\n", "                    resval =torch.bmm(h_src,self.res_fc)\n\t                    resval =resval.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n\t                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n\t                else:\n\t                    resval = self.res_fc(h_src).view(h_dst.shape[0], -1, self._out_feats*self.num_ntype)  #Identity\n\t                rst = rst + resval\n\t            # bias\n\t            if self.bias:\n\t                rst = rst + self.bias_param\n\t            # activation\n", "            if self.activation:\n\t                rst = self.activation(rst)\n\t            self.attentions=graph.edata.pop('a').detach()\n\t            torch.cuda.empty_cache()\n\t            return rst, self.attentions\n\tclass changedGATConv(nn.Module):\n\t    \"\"\"\n\t    Adapted from\n\t    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n\t    \"\"\"\n", "    def __init__(self,\n\t                 edge_feats,\n\t                 num_etypes,\n\t                 in_feats,\n\t                 out_feats,\n\t                 num_heads,\n\t                 feat_drop=0.,\n\t                 attn_drop=0.,\n\t                 negative_slope=0.2,\n\t                 residual=False,\n", "                 activation=None,\n\t                 allow_zero_in_degree=False,\n\t                 bias=False,\n\t                 alpha=0.,\n\t                 num_ntype=None,  eindexer=None):\n\t        super(changedGATConv, self).__init__()\n\t        self._edge_feats = edge_feats\n\t        self._num_heads = num_heads\n\t        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n\t        self._out_feats = out_feats\n", "        self._allow_zero_in_degree = allow_zero_in_degree\n\t        self.edge_emb = nn.Embedding(num_etypes, edge_feats)  \n\t        self.eindexer=eindexer\n\t        if isinstance(in_feats, tuple):\n\t            self.fc_src = nn.Linear(\n\t                self._in_src_feats, out_feats * num_heads, bias=False)\n\t            self.fc_dst = nn.Linear(\n\t                self._in_dst_feats, out_feats * num_heads, bias=False)\n\t            raise Exception(\"!!!\")\n\t        else:\n", "            self.fc = nn.Linear(\n\t                    self._in_src_feats, out_feats * num_heads, bias=False) \n\t        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False)\n\t        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats,num_etypes)))\n\t        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n\t        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n\t        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats)))\n\t        self.feat_drop = nn.Dropout(feat_drop)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.leaky_relu = nn.LeakyReLU(negative_slope)\n", "        if residual:\n\t            if self._in_dst_feats != out_feats:\n\t                self.res_fc = nn.Linear(\n\t                    self._in_dst_feats, num_heads * out_feats, bias=False)\n\t            else:\n\t                self.res_fc = Identity()\n\t        else:\n\t            self.register_buffer('res_fc', None)\n\t        self.reset_parameters()\n\t        self.activation = activation\n", "        self.bias = bias\n\t        if bias:\n\t            self.bias_param = nn.Parameter(th.zeros((1, num_heads, out_feats))) \n\t        self.alpha = alpha\n\t    def reset_parameters(self):\n\t        gain = nn.init.calculate_gain('relu')\n\t        if hasattr(self, 'fc'):\n\t            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n\t        else:\n\t            raise NotImplementedError()\n", "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n\t            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_l, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_r, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_e, gain=gain)\n\t        if isinstance(self.res_fc, nn.Linear):\n\t            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n\t        nn.init.xavier_normal_(self.fc_e.weight, gain=gain)\n\t    def set_allow_zero_in_degree(self, set_value):\n\t        self._allow_zero_in_degree = set_value\n", "    def forward(self, graph, feat, e_feat, res_attn=None):\n\t        with graph.local_scope():\n\t            if not self._allow_zero_in_degree:\n\t                if (graph.in_degrees() == 0).any():\n\t                    raise DGLError('There are 0-in-degree nodes in the graph, '\n\t                                   'output for those nodes will be invalid. '\n\t                                   'This is harmful for some applications, '\n\t                                   'causing silent performance regression. '\n\t                                   'Adding self-loop on the input graph by '\n\t                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n", "                                   'the issue. Setting ``allow_zero_in_degree`` '\n\t                                   'to be `True` when constructing this module will '\n\t                                   'suppress the check and let the code run.')\n\t            if isinstance(feat, tuple):\n\t                h_src = self.feat_drop(feat[0])\n\t                h_dst = self.feat_drop(feat[1])\n\t                if not hasattr(self, 'fc_src'):\n\t                    self.fc_src, self.fc_dst = self.fc, self.fc\n\t                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n\t                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n", "                raise Exception(\"!!!\")\n\t            else:\n\t                #feature transformation first\n\t                h_src = h_dst = self.feat_drop(feat)\n\t                feat_src = feat_dst = self.fc(h_src).view(\n\t                        -1, self._num_heads, self._out_feats)\n\t                if graph.is_block:\n\t                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n\t            e_feat = self.edge_emb(e_feat)\n\t            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)\n", "            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1)\n\t            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n\t            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n\t            graph.srcdata.update({'ft': feat_src, 'el': el})\n\t            graph.dstdata.update({'er': er})\n\t            graph.edata.update({'ee': ee})\n\t            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n\t            e = self.leaky_relu(graph.edata.pop('e')+graph.edata.pop('ee'))\n\t            # compute softmax\n\t            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n", "            if res_attn is not None:\n\t                graph.edata['a'] = graph.edata['a'] * (1-self.alpha) + res_attn * self.alpha\n\t            # then message passing\n\t            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n\t                             fn.sum('m', 'ft'))\n\t            rst = graph.dstdata['ft']\n\t            # residual\n\t            if self.res_fc is not None:\n\t                resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n\t                rst = rst + resval\n", "            # bias\n\t            if self.bias:\n\t                rst = rst + self.bias_param\n\t            # activation\n\t            if self.activation:\n\t                rst = self.activation(rst)\n\t            return rst, graph.edata.pop('a').detach()\n\t# pylint: enable=W0235\n\tclass myGATConv(nn.Module):\n\t    \"\"\"\n", "    Adapted from\n\t    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n\t    \"\"\"\n\t    def __init__(self,\n\t                 edge_feats,\n\t                 num_etypes,\n\t                 in_feats,\n\t                 out_feats,\n\t                 num_heads,\n\t                 feat_drop=0.,\n", "                 attn_drop=0.,\n\t                 negative_slope=0.2,\n\t                 residual=False,\n\t                 activation=None,\n\t                 allow_zero_in_degree=False,\n\t                 bias=False,\n\t                 alpha=0.):\n\t        super(myGATConv, self).__init__()\n\t        self._edge_feats = edge_feats\n\t        self._num_heads = num_heads\n", "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n\t        self._out_feats = out_feats\n\t        self._allow_zero_in_degree = allow_zero_in_degree\n\t        self.edge_emb = nn.Embedding(num_etypes, edge_feats)\n\t        if isinstance(in_feats, tuple):\n\t            self.fc_src = nn.Linear(\n\t                self._in_src_feats, out_feats * num_heads, bias=False)\n\t            self.fc_dst = nn.Linear(\n\t                self._in_dst_feats, out_feats * num_heads, bias=False)\n\t            raise Exception(\"!!!\")\n", "        else:\n\t            self.fc = nn.Linear(\n\t                self._in_src_feats, out_feats * num_heads, bias=False)\n\t        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False)\n\t        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n\t        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n\t        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats)))\n\t        self.feat_drop = nn.Dropout(feat_drop)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.leaky_relu = nn.LeakyReLU(negative_slope)\n", "        if residual:\n\t            if self._in_dst_feats != out_feats:\n\t                self.res_fc = nn.Linear(\n\t                    self._in_dst_feats, num_heads * out_feats, bias=False)\n\t            else:\n\t                self.res_fc = Identity()\n\t        else:\n\t            self.register_buffer('res_fc', None)\n\t        self.reset_parameters()\n\t        self.activation = activation\n", "        self.bias = bias\n\t        if bias:\n\t            self.bias_param = nn.Parameter(th.zeros((1, num_heads, out_feats)))\n\t        self.alpha = alpha\n\t    def reset_parameters(self):\n\t        gain = nn.init.calculate_gain('relu')\n\t        if hasattr(self, 'fc'):\n\t            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n\t        else:\n\t            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n", "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_l, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_r, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_e, gain=gain)\n\t        if isinstance(self.res_fc, nn.Linear):\n\t            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n\t        nn.init.xavier_normal_(self.fc_e.weight, gain=gain)\n\t    def set_allow_zero_in_degree(self, set_value):\n\t        self._allow_zero_in_degree = set_value\n\t    def forward(self, graph, feat, e_feat, res_attn=None):\n", "        with graph.local_scope():\n\t            if not self._allow_zero_in_degree:\n\t                if (graph.in_degrees() == 0).any():\n\t                    raise DGLError('There are 0-in-degree nodes in the graph, '\n\t                                   'output for those nodes will be invalid. '\n\t                                   'This is harmful for some applications, '\n\t                                   'causing silent performance regression. '\n\t                                   'Adding self-loop on the input graph by '\n\t                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n\t                                   'the issue. Setting ``allow_zero_in_degree`` '\n", "                                   'to be `True` when constructing this module will '\n\t                                   'suppress the check and let the code run.')\n\t            if isinstance(feat, tuple):\n\t                h_src = self.feat_drop(feat[0])\n\t                h_dst = self.feat_drop(feat[1])\n\t                if not hasattr(self, 'fc_src'):\n\t                    self.fc_src, self.fc_dst = self.fc, self.fc\n\t                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n\t                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n\t                raise Exception(\"!!!\")\n", "            else:\n\t                h_src = h_dst = self.feat_drop(feat)\n\t                feat_src = feat_dst = self.fc(h_src).view(\n\t                    -1, self._num_heads, self._out_feats)\n\t                if graph.is_block:\n\t                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n\t            e_feat = self.edge_emb(e_feat)\n\t            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)\n\t            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1)\n\t            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n", "            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n\t            graph.srcdata.update({'ft': feat_src, 'el': el})\n\t            graph.dstdata.update({'er': er})\n\t            graph.edata.update({'ee': ee})\n\t            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n\t            e = self.leaky_relu(graph.edata.pop('e')+graph.edata.pop('ee'))\n\t            # compute softmax\n\t            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n\t            if res_attn is not None:\n\t                graph.edata['a'] = graph.edata['a'] * (1-self.alpha) + res_attn * self.alpha\n", "            # message passing\n\t            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n\t                             fn.sum('m', 'ft'))\n\t            rst = graph.dstdata['ft']\n\t            # residual\n\t            if self.res_fc is not None:\n\t                resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n\t                rst = rst + resval\n\t            # bias\n\t            if self.bias:\n", "                rst = rst + self.bias_param\n\t            # activation\n\t            if self.activation:\n\t                rst = self.activation(rst)\n\t            return rst, graph.edata.pop('a').detach()\n"]}
{"filename": "NC/methods/SlotGAT/GNN.py", "chunked_list": ["import torch\n\timport torch as th\n\timport torch.nn.functional as F\n\timport torch.nn as nn\n\timport dgl\n\tfrom dgl.nn.pytorch import GraphConv\n\timport math\n\timport dgl.function as fn\n\tfrom dgl.nn.pytorch import edge_softmax, GATConv\n\tfrom conv import myGATConv,changedGATConv,slotGATConv\n", "from torch.profiler import profile, record_function, ProfilerActivity\n\tfrom torch_geometric.typing import OptPairTensor, OptTensor, Size, Tensor\n\tfrom typing import Callable, Tuple, Union\n\tfrom torch_scatter import scatter_add\n\tfrom torch_geometric.utils import add_remaining_self_loops\n\tfrom dgl._ffi.base import DGLError\n\tfrom typing import List, NamedTuple, Optional, Tuple, Union\n\tfrom torch.nn import Linear\n\tfrom torch_geometric.nn.conv import MessagePassing, GCNConv\n\tclass Adj(NamedTuple):\n", "    edge_index: torch.Tensor\n\t    edge_features: torch.Tensor\n\t    size: Tuple[int, int]\n\t    target_size: int\n\t    def to(self, *args, **kwargs):\n\t        return Adj(\n\t            self.edge_index.to(*args, **kwargs),\n\t            self.edge_features.to(*args, **kwargs),\n\t            self.size,\n\t            self.target_size\n", "        )\n\tclass MLP(nn.Module):\n\t    def __init__(self,\n\t                 g,\n\t                 in_dims,\n\t                 num_hidden,\n\t                 num_classes,\n\t                 num_layers,\n\t                 activation,\n\t                 dropout):\n", "        super(MLP, self).__init__()\n\t        self.num_classes=num_classes\n\t        self.layers = nn.ModuleList()\n\t        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n\t        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        # input layer\n\t        self.layers.append(nn.Linear(num_hidden, num_hidden, bias=True))\n\t        # hidden layers\n\t        for i in range(num_layers - 1):\n", "            self.layers.append(nn.Linear(num_hidden, num_hidden))\n\t        # output layer\n\t        self.layers.append(nn.Linear(num_hidden, num_classes))\n\t        for ly in self.layers:\n\t            nn.init.xavier_normal_(ly.weight, gain=1.414)\n\t        self.dropout = nn.Dropout(p=dropout)\n\t    def forward(self, features_list, e_feat):\n\t        h = []\n\t        for fc, feature in zip(self.fc_list, features_list):\n\t            h.append(fc(feature))\n", "        h = torch.cat(h, 0)\n\t        for i, layer in enumerate(self.layers):\n\t            encoded_embeddings=h\n\t            h = self.dropout(h)\n\t            h = layer(h)\n\t            h=F.relu(h) if i<len(self.layers) else h\n\t        return h,encoded_embeddings\n\tclass LabelPropagation(nn.Module):\n\t    r\"\"\"\n\t    Description\n", "    -----------\n\t    Introduced in `Learning from Labeled and Unlabeled Data with Label Propagation <https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.3864&rep=rep1&type=pdf>`_\n\t    .. math::\n\t        \\mathbf{Y}^{\\prime} = \\alpha \\cdot \\mathbf{D}^{-1/2} \\mathbf{A}\n\t        \\mathbf{D}^{-1/2} \\mathbf{Y} + (1 - \\alpha) \\mathbf{Y},\n\t    where unlabeled data is inferred by labeled data via propagation.\n\t    Parameters\n\t    ----------\n\t        num_layers: int\n\t            The number of propagations.\n", "        alpha: float\n\t            The :math:`\\alpha` coefficient.\n\t    \"\"\"\n\t    def __init__(self, num_layers, alpha):\n\t        super(LabelPropagation, self).__init__()\n\t        self.num_layers = num_layers\n\t        self.alpha = alpha\n\t    @torch.no_grad()\n\t    def forward(self, g, labels, mask,get_out=\"False\"):    # labels.shape=(number of nodes of type 0)  may contain false labels, therefore here the mask argument which provides the training nodes' idx is important\n\t        with g.local_scope():\n", "            if labels.dtype == torch.long:\n\t                labels = F.one_hot(labels.view(-1)).to(torch.float32)\n\t            y=torch.zeros((g.num_nodes(),labels.shape[1])).to(labels.device)\n\t            y[mask] = labels[mask]\n\t            last = (1 - self.alpha) * y\n\t            degs = g.in_degrees().float().clamp(min=1)\n\t            norm = torch.pow(degs, -0.5).to(labels.device).unsqueeze(1)\n\t            for _ in range(self.num_layers):\n\t                # Assume the graphs to be undirected\n\t                g.ndata['h'] = y * norm\n", "                g.update_all(fn.copy_u('h', 'm'), fn.sum('m', 'h'))\n\t                y = last + self.alpha * g.ndata.pop('h') * norm\n\t                y=F.normalize(y,p=1,dim=1)   #normalize y by row with p-1-norm\n\t                y[mask] = labels[mask]\n\t                last = (1 - self.alpha) * y\n\t            return y,None\n\tclass slotGAT(nn.Module):\n\t    def __init__(self,\n\t                 g,\n\t                 edge_dim,\n", "                 num_etypes,\n\t                 in_dims,\n\t                 num_hidden,\n\t                 num_classes,\n\t                 num_layers,\n\t                 heads,\n\t                 activation,\n\t                 feat_drop,\n\t                 attn_drop,\n\t                 negative_slope,\n", "                 residual,\n\t                 alpha,\n\t                 num_ntype,\n\t                 eindexer, aggregator=\"SA\",predicted_by_slot=\"None\", addLogitsTrain=\"None\",  SAattDim=32,dataRecorder=None,targetTypeAttention=\"False\",vis_data_saver=None):\n\t        super(slotGAT, self).__init__()\n\t        self.g = g\n\t        self.num_layers = num_layers\n\t        self.gat_layers = nn.ModuleList()\n\t        self.activation = activation\n\t        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims]) \n", "        self.num_ntype=num_ntype\n\t        self.num_classes=num_classes\n\t        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\t        self.predicted_by_slot=predicted_by_slot \n\t        self.addLogitsTrain=addLogitsTrain \n\t        self.SAattDim=SAattDim \n\t        self.vis_data_saver=vis_data_saver\n\t        self.dataRecorder=dataRecorder\n\t        if aggregator==\"SA\":\n\t            last_dim=num_classes\n", "            self.macroLinear=nn.Linear(last_dim, self.SAattDim, bias=True);nn.init.xavier_normal_(self.macroLinear.weight, gain=1.414);nn.init.normal_(self.macroLinear.bias, std=1.414*math.sqrt(1/(self.macroLinear.bias.flatten().shape[0])))\n\t            self.macroSemanticVec=nn.Parameter(torch.FloatTensor(self.SAattDim,1));nn.init.normal_(self.macroSemanticVec,std=1)\n\t        self.last_fc = nn.Parameter(th.FloatTensor(size=(num_classes*self.num_ntype, num_classes))) ;nn.init.xavier_normal_(self.last_fc, gain=1.414)\n\t        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        # input projection (no residual)\n\t        self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n\t            num_hidden, num_hidden, heads[0],\n\t            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer,inputhead=True, dataRecorder=dataRecorder))\n\t        # hidden layers\n", "        for l in range(1, num_layers):\n\t            # due to multi-head, the in_dim = num_hidden * num_heads\n\t            self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n\t                num_hidden* heads[l-1] , num_hidden, heads[l],\n\t                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer, dataRecorder=dataRecorder))\n\t        # output projection\n\t        self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n\t            num_hidden* heads[-2] , num_classes, heads[-1],\n\t            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer, dataRecorder=dataRecorder))\n\t        self.aggregator=aggregator\n", "        self.by_slot=[f\"by_slot_{nt}\" for nt in range(g.num_ntypes)]\n\t        assert aggregator in ([\"onedimconv\",\"average\",\"last_fc\",\"max\",\"SA\"]+self.by_slot)\n\t        if self.aggregator==\"onedimconv\":\n\t            self.nt_aggr=nn.Parameter(torch.FloatTensor(1,1,self.num_ntype,1));nn.init.normal_(self.nt_aggr,std=1) \n\t        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n\t    def l2byslot(self,x):\n\t        x=x.view(-1, self.num_ntype,int(x.shape[1]/self.num_ntype))\n\t        x=x / (torch.max(torch.norm(x, dim=2, keepdim=True), self.epsilon))\n\t        x=x.flatten(1)\n\t        return x\n", "    def forward(self, features_list,e_feat, get_out=\"False\"):\n\t        with record_function(\"model_forward\"):\n\t            encoded_embeddings=None\n\t            h = []\n\t            for nt_id,(fc, feature) in enumerate(zip(self.fc_list, features_list)):\n\t                nt_ft=fc(feature)\n\t                emsen_ft=torch.zeros([nt_ft.shape[0],nt_ft.shape[1]*self.num_ntype]).to(feature.device)\n\t                emsen_ft[:,nt_ft.shape[1]*nt_id:nt_ft.shape[1]*(nt_id+1)]=nt_ft\n\t                h.append(emsen_ft)   # the id is decided by the node types\n\t            h = torch.cat(h, 0)        #  num_nodes*(num_type*hidden_dim)\n", "            res_attn = None\n\t            for l in range(self.num_layers):\n\t                h, res_attn = self.gat_layers[l](self.g, h, e_feat,get_out=get_out, res_attn=res_attn)   #num_nodes*num_heads*(num_ntype*hidden_dim)\n\t                h = h.flatten(1)    #num_nodes*(num_heads*num_ntype*hidden_dim)\n\t                encoded_embeddings=h\n\t            # output projection\n\t            logits, _ = self.gat_layers[-1](self.g, h, e_feat,get_out=get_out, res_attn=None)   #num_nodes*num_heads*num_ntype*hidden_dim\n\t        if self.aggregator==\"SA\" :\n\t            logits=logits.squeeze(1)\n\t            logits=self.l2byslot(logits)\n", "            logits=logits.view(-1, self.num_ntype,int(logits.shape[1]/self.num_ntype))\n\t            if \"getSlots\" in get_out:\n\t                self.logits=logits.detach()\n\t            slot_scores=(F.tanh(self.macroLinear(logits))@self.macroSemanticVec).mean(0,keepdim=True)  #num_slots\n\t            self.slot_scores=F.softmax(slot_scores,dim=1)\n\t            logits=(logits*self.slot_scores).sum(1)\n\t            if  self.dataRecorder[\"meta\"][\"getSAAttentionScore\"]==\"True\":\n\t                self.dataRecorder[\"data\"][f\"{self.dataRecorder['status']}_SAAttentionScore\"]=self.slot_scores.flatten().tolist() #count dist\n\t        #average across the ntype info\n\t        if self.predicted_by_slot!=\"None\" and self.training==False:\n", "            with record_function(\"predict_by_slot\"):\n\t                logits=logits.view(-1,1,self.num_ntype,self.num_classes)\n\t                if self.predicted_by_slot==\"max\":\n\t                    if \"getMaxSlot\" in  get_out:\n\t                        maxSlotIndexesWithLabels=logits.max(2)[1].squeeze(1)\n\t                        logits_indexer=logits.max(2)[0].max(2)[1]\n\t                        self.maxSlotIndexes=torch.gather(maxSlotIndexesWithLabels,1,logits_indexer)\n\t                    logits=logits.max(2)[0]\n\t                elif self.predicted_by_slot==\"all\":\n\t                    if \"getSlots\" in get_out:\n", "                        self.logits=logits.detach()\n\t                    logits=logits.view(-1,1,self.num_ntype,self.num_classes).mean(2)  #average??\n\t                else:\n\t                    target_slot=int(self.predicted_by_slot)\n\t                    logits=logits[:,:,target_slot,:].squeeze(2)\n\t        else:\n\t            #with record_function(\"slot_aggregation\"):\n\t            if self.aggregator==\"average\":\n\t                logits=logits.view(-1,1,self.num_ntype,self.num_classes).mean(2)\n\t            elif self.aggregator==\"onedimconv\":\n", "                logits=(logits.view(-1,1,self.num_ntype,self.num_classes)*F.softmax(self.leaky_relu(self.nt_aggr),dim=2)).sum(2)\n\t            elif self.aggregator==\"last_fc\":\n\t                logits=logits.view(-1,1,self.num_ntype,self.num_classes)\n\t                logits=logits.flatten(1)\n\t                logits=logits.matmul(self.last_fc).unsqueeze(1)\n\t            elif self.aggregator==\"max\":\n\t                logits=logits.view(-1,1,self.num_ntype,self.num_classes).max(2)[0]\n\t            elif self.aggregator==\"None\":\n\t                logits=logits.view(-1,1, self.num_ntype,self.num_classes).flatten(2)\n\t            elif  self.aggregator== \"SA\":\n", "                logits=logits.view(-1,1, 1,self.num_classes).flatten(2)\n\t            else:\n\t                raise NotImplementedError()\n\t        #average across the heads\n\t        ### logits = [num_nodes *  num_of_heads *num_classes]\n\t        self.logits_mean=logits.flatten().mean()\n\t        logits = logits.mean(1)\n\t        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n\t        logits = logits / (torch.max(torch.norm(logits, dim=1, keepdim=True), self.epsilon))\n\t        return logits, encoded_embeddings    #hidden_logits\n", "class changedGAT(nn.Module):\n\t    def __init__(self,\n\t                 g,\n\t                 edge_dim,\n\t                 num_etypes,\n\t                 in_dims,\n\t                 num_hidden,\n\t                 num_classes,\n\t                 num_layers,\n\t                 heads,\n", "                 activation,\n\t                 feat_drop,\n\t                 attn_drop,\n\t                 negative_slope,\n\t                 residual,\n\t                 alpha,\n\t                 num_ntype,\n\t                 eindexer, ):\n\t        super(changedGAT, self).__init__()\n\t        self.g = g\n", "        self.num_layers = num_layers\n\t        self.gat_layers = nn.ModuleList()\n\t        self.activation = activation\n\t        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims]) \n\t        #self.ae_drop=nn.Dropout(feat_drop)\n\t        #if ae_layer==\"last_hidden\":\n\t            #self.lc_ae=nn.ModuleList([nn.Linear(num_hidden * heads[-2],num_hidden, bias=True),nn.Linear(num_hidden,num_ntype, bias=True)])\n\t        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        # input projection (no residual)\n", "        self.gat_layers.append(changedGATConv(edge_dim, num_etypes,\n\t            num_hidden, num_hidden, heads[0],\n\t            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer))\n\t        # hidden layers\n\t        for l in range(1, num_layers):\n\t            # due to multi-head, the in_dim = num_hidden * num_heads\n\t            self.gat_layers.append(changedGATConv(edge_dim, num_etypes,\n\t                num_hidden * heads[l-1], num_hidden, heads[l],\n\t                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha,num_ntype=num_ntype, eindexer=eindexer))\n\t        # output projection\n", "        self.gat_layers.append(changedGATConv(edge_dim, num_etypes,\n\t            num_hidden * heads[-2], num_classes, heads[-1],\n\t            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha,num_ntype=num_ntype,  eindexer=eindexer))\n\t        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n\t    def forward(self, features_list, e_feat,get_out=\"False\"):\n\t        hidden_logits=None\n\t        h = []\n\t        for fc, feature in zip(self.fc_list, features_list):\n\t            h.append(fc(feature))   # the id is decided by the node types\n\t        h = torch.cat(h, 0)\n", "        res_attn = None\n\t        for l in range(self.num_layers):\n\t            h, res_attn = self.gat_layers[l](self.g, h, e_feat, res_attn=res_attn)\n\t            h = h.flatten(1)\n\t            #if self.ae_layer==\"last_hidden\":\n\t            encoded_embeddings=h\n\t            \"\"\"for i in range(len(self.lc_ae)):\n\t                _h=self.lc_ae[i](_h)\n\t                if i==0:\n\t                    _h=self.ae_drop(_h)\n", "                    _h=F.relu(_h)\n\t            hidden_logits=_h\"\"\"\n\t        # output projection\n\t        logits, _ = self.gat_layers[-1](self.g, h, e_feat, res_attn=None)\n\t        logits = logits.mean(1)\n\t        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n\t        logits = logits / (torch.max(torch.norm(logits, dim=1, keepdim=True), self.epsilon))\n\t        return logits, encoded_embeddings    #hidden_logits\n\tclass myGAT(nn.Module):\n\t    def __init__(self,\n", "                 g,\n\t                 edge_dim,\n\t                 num_etypes,\n\t                 in_dims,\n\t                 num_hidden,\n\t                 num_classes,\n\t                 num_layers,\n\t                 heads,\n\t                 activation,\n\t                 feat_drop,\n", "                 attn_drop,\n\t                 negative_slope,\n\t                 residual,\n\t                 alpha, dataRecorder=None):\n\t        super(myGAT, self).__init__()\n\t        self.g = g\n\t        self.num_layers = num_layers\n\t        self.gat_layers = nn.ModuleList()\n\t        self.activation = activation\n\t        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n", "        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        # input projection (no residual)\n\t        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n\t            num_hidden, num_hidden, heads[0],\n\t            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha))\n\t        # hidden layers\n\t        for l in range(1, num_layers):\n\t            # due to multi-head, the in_dim = num_hidden * num_heads\n\t            self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n", "                num_hidden * heads[l-1], num_hidden, heads[l],\n\t                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha))\n\t        # output projection\n\t        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n\t            num_hidden * heads[-2], num_classes, heads[-1],\n\t            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha))\n\t        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n\t        self.dataRecorder=dataRecorder\n\t    def forward(self, features_list, e_feat, get_out=\"False\"):\n\t        h = []\n", "        for fc, feature in zip(self.fc_list, features_list):\n\t            h.append(fc(feature))   # the id is decided by the node types\n\t        h = torch.cat(h, 0)\n\t        res_attn = None\n\t        for l in range(self.num_layers):\n\t            h, res_attn = self.gat_layers[l](self.g, h, e_feat, res_attn=res_attn)\n\t            h = h.flatten(1)\n\t            encoded_embeddings=h\n\t        # output projection\n\t        logits, _ = self.gat_layers[-1](self.g, h, e_feat, res_attn=None)\n", "        logits = logits.mean(1)\n\t        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n\t        logits = logits / (torch.max(torch.norm(logits, dim=1, keepdim=True), self.epsilon))\n\t        return logits,encoded_embeddings\n\tclass RGAT(nn.Module):\n\t    def __init__(self,\n\t                 gs,\n\t                 in_dims,\n\t                 num_hidden,\n\t                 num_classes,\n", "                 num_layers,\n\t                 heads,\n\t                 activation,\n\t                 feat_drop,\n\t                 attn_drop,\n\t                 negative_slope,\n\t                 residual ):\n\t        super(GAT, self).__init__()\n\t        self.gs = gs\n\t        self.num_layers = num_layers\n", "        self.gat_layers = nn.ModuleList([nn.ModuleList() for i in range(len(gs))])\n\t        self.activation = activation\n\t        self.weights = nn.Parameter(torch.zeros((len(in_dims), num_layers+1, len(gs))))\n\t        self.sm = nn.Softmax(2)\n\t        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n\t        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        for i in range(len(gs)):\n\t            # input projection (no residual)\n\t            self.gat_layers[i].append(GATConv(\n", "                num_hidden, num_hidden, heads[0],\n\t                feat_drop, attn_drop, negative_slope, False, self.activation))\n\t            # hidden layers\n\t            for l in range(1, num_layers):\n\t                # due to multi-head, the in_dim = num_hidden * num_heads\n\t                self.gat_layers[i].append(GATConv(\n\t                    num_hidden * heads[l-1], num_hidden, heads[l],\n\t                    feat_drop, attn_drop, negative_slope, residual, self.activation))\n\t            # output projection\n\t            self.gat_layers[i].append(GATConv(\n", "                num_hidden * heads[-2], num_classes, heads[-1],\n\t                feat_drop, attn_drop, negative_slope, residual, None))\n\t    def forward(self, features_list):\n\t        nums = [feat.size(0) for feat in features_list]\n\t        weights = self.sm(self.weights)\n\t        h = []\n\t        for fc, feature in zip(self.fc_list, features_list):\n\t            h.append(fc(feature))\n\t        h = torch.cat(h, 0)\n\t        for l in range(self.num_layers):\n", "            out = []\n\t            for i in range(len(self.gs)):\n\t                out.append(torch.split(self.gat_layers[i][l](self.gs[i], h).flatten(1), nums))\n\t            h = []\n\t            for k in range(len(nums)):\n\t                tmp = []\n\t                for i in range(len(self.gs)):\n\t                    tmp.append(out[i][k]*weights[k,l,i])\n\t                h.append(sum(tmp))\n\t            h = torch.cat(h, 0)\n", "        out = []\n\t        for i in range(len(self.gs)):\n\t            out.append(torch.split(self.gat_layers[i][-1](self.gs[i], h).mean(1), nums))\n\t        logits = []\n\t        for k in range(len(nums)):\n\t            tmp = []\n\t            for i in range(len(self.gs)):\n\t                tmp.append(out[i][k]*weights[k,-1,i])\n\t            logits.append(sum(tmp))\n\t        logits = torch.cat(logits, 0)\n", "        return logits\n\tclass GAT(nn.Module):\n\t    def __init__(self,\n\t                 g,\n\t                 in_dims,\n\t                 num_hidden,\n\t                 num_classes,\n\t                 num_layers,\n\t                 heads,\n\t                 activation,\n", "                 feat_drop,\n\t                 attn_drop,\n\t                 negative_slope,\n\t                 residual,dataRecorder=None ):\n\t        super(GAT, self).__init__()\n\t        self.g = g\n\t        self.num_layers = num_layers\n\t        self.gat_layers = nn.ModuleList()\n\t        self.activation = activation\n\t        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n", "        self.dataRecorder=dataRecorder\n\t        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        # input projection (no residual)\n\t        self.gat_layers.append(GATConv(\n\t            num_hidden, num_hidden, heads[0],\n\t            feat_drop, attn_drop, negative_slope, False, self.activation))\n\t        # hidden layers\n\t        for l in range(1, num_layers):\n\t            # due to multi-head, the in_dim = num_hidden * num_heads\n", "            self.gat_layers.append(GATConv(\n\t                num_hidden * heads[l-1], num_hidden, heads[l],\n\t                feat_drop, attn_drop, negative_slope, residual, self.activation))\n\t        # output projection\n\t        self.gat_layers.append(GATConv(\n\t            num_hidden * heads[-2], num_classes, heads[-1],\n\t            feat_drop, attn_drop, negative_slope, residual, None))\n\t    def forward(self, features_list, e_feat,get_out=\"False\"):\n\t        h = []\n\t        for fc, feature in zip(self.fc_list, features_list):\n", "            h.append(fc(feature))\n\t        h = torch.cat(h, 0)\n\t        for l in range(self.num_layers):\n\t            h = self.gat_layers[l](self.g, h).flatten(1)\n\t            encoded_embeddings=h\n\t        # output projection\n\t        logits = self.gat_layers[-1](self.g, h).mean(1)\n\t        return logits,encoded_embeddings\n\tclass GCN(nn.Module):\n\t    def __init__(self,\n", "                 g,\n\t                 in_dims,\n\t                 num_hidden,\n\t                 num_classes,\n\t                 num_layers,\n\t                 activation,\n\t                 dropout, dataRecorder=None):\n\t        super(GCN, self).__init__()\n\t        self.g = g\n\t        self.layers = nn.ModuleList()\n", "        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n\t        self.dataRecorder=dataRecorder\n\t        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        # input layer\n\t        self.layers.append(GraphConv(num_hidden, num_hidden, activation=activation, weight=False))\n\t        # hidden layers\n\t        for i in range(num_layers - 1):\n\t            self.layers.append(GraphConv(num_hidden, num_hidden, activation=activation))\n\t        # output layer\n", "        self.layers.append(GraphConv(num_hidden, num_classes))\n\t        self.dropout = nn.Dropout(p=dropout)\n\t    def forward(self, features_list, e_feat,get_out=\"False\"):\n\t        h = []\n\t        for fc, feature in zip(self.fc_list, features_list):\n\t            h.append(fc(feature))\n\t        h = torch.cat(h, 0)\n\t        for i, layer in enumerate(self.layers):\n\t            encoded_embeddings=h\n\t            h = self.dropout(h)\n", "            h = layer(self.g, h)\n\t        return h,encoded_embeddings\n\tdef gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False,\n\t             add_self_loops=True, dtype=None,norm=\"D^{-1/2}(A+I)D^{-1/2}\",attn_drop=None):\n\t    fill_value = 2. if improved else 1.\n\t    num_nodes = int(edge_index.max()) + 1 if num_nodes is None else num_nodes\n\t    if edge_weight is None:\n\t        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n\t                                 device=edge_index.device)\n\t    if add_self_loops:\n", "        edge_index, tmp_edge_weight = add_remaining_self_loops(\n\t            edge_index, edge_weight, fill_value, num_nodes)\n\t        assert tmp_edge_weight is not None\n\t        edge_weight = tmp_edge_weight\n\t    row, col = edge_index[0], edge_index[1]\n\t    deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n\t    if norm==\"D^{-1/2}(A+I)D^{-1/2}\":\n\t        deg_inv_sqrt = deg.pow_(-0.5)\n\t        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n\t        return edge_index, attn_drop(deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col])\n", "    elif norm==\"D^{-1}(A+I)\":\n\t        deg_inv_sqrt = deg.pow_(-1)\n\t        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n\t        return edge_index, attn_drop(deg_inv_sqrt[row] * edge_weight )\n\t    elif norm==\"(A+I)D^{-1}\":\n\t        deg_inv_sqrt = deg.pow_(-1)\n\t        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n\t        return edge_index, attn_drop(deg_inv_sqrt[col] * edge_weight )\n\t    elif norm==\"(A+I)\":\n\t        return edge_index, attn_drop(edge_weight )\n", "    else:\n\t        raise Exception(f\"No specified norm: {norm}\")\n"]}
{"filename": "NC/methods/SlotGAT/pipeline_utils.py", "chunked_list": ["import random\n\timport queue\n\timport time\n\timport subprocess\n\timport multiprocessing\n\tfrom threading import main_thread\n\timport os\n\timport pandas as pd\n\timport copy\n\tdef get_tasks_for_online(dataset_and_hypers):\n", "    pass\n\tdef get_tasks_linear_around(task_space,best_hyper):\n\t    tasks=[]\n\t    for param_in_space,param_values in task_space.items():\n\t        assert param_in_space in best_hyper.keys()\n\t        for param_value in param_values:\n\t            temp_t={}\n\t            #copy best hyper except specified param\n\t            for param_in_best,value_in_best in best_hyper.items():\n\t                if \"search_\" in param_in_best:\n", "                    temp_t[param_in_best]= f\"[{value_in_best}]\"  if param_in_best!=param_in_space else f\"[{param_value}]\" \n\t                else:\n\t                    temp_t[param_in_best]=value_in_best if param_in_best!=param_in_space else param_value\n\t            tasks.append(temp_t)\n\t    return tasks\n\tdef get_tasks(task_space):\n\t    tasks=[{}]\n\t    for k,v in task_space.items():\n\t        tasks=expand_task(tasks,k,v)\n\t    return tasks\n", "def expand_task(tasks,k,v):\n\t    temp_tasks=[]\n\t    if type(v) is str and type(eval(v)) is list:\n\t        for value in eval(v):\n\t            if k.startswith(\"search_\"):\n\t                value=str([value])\n\t            for t in tasks:\n\t                temp_t=copy.deepcopy(t)\n\t                temp_t[k]=value\n\t                temp_tasks.append(temp_t)\n", "    elif type(v) is list:\n\t        for value in v:\n\t            for t in tasks:\n\t                temp_t=copy.deepcopy(t)\n\t                temp_t[k]=value\n\t                temp_tasks.append(temp_t)\n\t    else:\n\t        for t in tasks:\n\t            temp_t=copy.deepcopy(t)\n\t            temp_t[k]=v\n", "            temp_tasks.append(temp_t)\n\t    return temp_tasks\n\tdef proc_yes(yes,args_dict):\n\t    temp_yes=[]\n\t    for name in yes:\n\t        temp_yes.append(f\"{name}_{args_dict[name]}\")\n\t    return temp_yes\n\tdef get_best_hypers_from_csv(dataset,net,yes,no,metric=\"2_valAcc\"):\n\t    print(f\"yes: {yes}, no: {no}\")\n\t    #get search best hypers\n", "    fns=[]\n\t    for root, dirs, files in os.walk(\"./log\", topdown=False):\n\t        for name in files:\n\t            FLAG=1\n\t            if \"old\" in root:\n\t                continue\n\t            if \".py\" in name:\n\t                continue\n\t            if \".txt\" in name:\n\t                continue\n", "            if \".csv\" not in name:\n\t                continue\n\t            for n in no:\n\t                if n in name:\n\t                    FLAG=0\n\t            for y in yes:\n\t                if y not in name:\n\t                    FLAG=0\n\t            if FLAG==0:\n\t                continue\n", "            if dataset in name:\n\t                name0=name.replace(\"_GTN\",\"\",1) if \"kdd\" not in name else name\n\t                if net in name0 :\n\t                    fn=os.path.join(root, name)\n\t                    fns.append(fn)\n\t    score_max=0\n\t    print(fns)\n\t    if fns==[]:\n\t        raise Exception\n\t    for fn in fns:\n", "        param_data=pd.read_csv(fn)\n\t        param_data_sorted=param_data.sort_values(by=metric,ascending=False).head(1)\n\t        #print(param_data_sorted.columns)\n\t        param_mapping={\"1_Lr\":\"search_lr\",\n\t        \"1_Wd\":\"search_weight_decay\",\n\t        \"1_featType\":\"feats-type\",\n\t        \"1_hiddenDim\":\"search_hidden_dim\",\n\t        \"1_numLayers\":\"search_num_layers\",\n\t        \"1_numOfHeads\":\"search_num_heads\",}\n\t        score=param_data_sorted[metric].iloc[0]\n", "        if score>score_max:\n\t            print(   f\"score:{score}\\t {param_data_sorted} bigger than current score {score_max} \"  )\n\t            best_hypers={}\n\t            score_max=score\n\t            best_param_data_sorted=param_data_sorted\n\t            for col_name in param_data_sorted.columns:\n\t                if col_name.startswith(\"1_\"):\n\t                    if param_mapping[col_name].startswith(\"search_\"):\n\t                        best_hypers[param_mapping[col_name]]=f\"[{param_data_sorted[col_name].iloc[0]}]\"\n\t                    else:\n", "                        best_hypers[param_mapping[col_name]]=f\"{param_data_sorted[col_name].iloc[0]}\"\n\t        print(f\"Best Score:{score_max}\\t {best_param_data_sorted}\")\n\t    return best_hypers\n\tdef get_best_hypers(dataset,net,yes,no):\n\t    print(f\"yes: {yes}, no: {no}\")\n\t    #get search best hypers\n\t    best={}\n\t    fns=[]\n\t    for root, dirs, files in os.walk(\"./log\", topdown=False):\n\t        for name in files:\n", "            FLAG=1\n\t            if \"old\" in root:\n\t                continue\n\t            if \".py\" in name:\n\t                continue\n\t            if \".txt\" in name:\n\t                continue\n\t            for n in no:\n\t                if n in name:\n\t                    FLAG=0\n", "            for y in yes:\n\t                if y not in name:\n\t                    FLAG=0\n\t            if FLAG==0:\n\t                continue\n\t            if dataset in name:\n\t                name0=name.replace(\"_GTN\",\"\",1) if \"kdd\" not in name else name\n\t                if net in name0 :\n\t                    fn=os.path.join(root, name)\n\t                    fns.append(fn)\n", "    score_max=0\n\t    print(fns)\n\t    if fns==[]:\n\t        raise Exception\n\t    for fn in fns:\n\t        path=fn\n\t        FLAG0=False\n\t        FLAG1=False\n\t        with open(fn,\"r\") as f:\n\t            for line in f:\n", "                if \"Best trial\" in line and FLAG0==False:\n\t                    FLAG0=True\n\t                    FLAG1=False\n\t                    continue\n\t                if FLAG0==True:\n\t                    if \"Value\" in line:\n\t                        _,score=line.strip(\"\\n\").replace(\" \",\"\").split(\":\")\n\t                        score=float(score)\n\t                        continue\n\t                    if \"Params:\" in line:\n", "                        FLAG1=True\n\t                        count=0\n\t                        continue\n\t                if FLAG1==True and score>=score_max and \"    \" in line and count<=5:\n\t                    param,value=line.strip(\"\\n\").replace(\" \",\"\").split(\":\")\n\t                    best[param]=value\n\t                    score_max=score\n\t                    FLAG0=False\n\t                    count+=1\n\t        print(best)\n", "        best_hypers={}\n\t        for key in best.keys():\n\t            best_hypers[\"search_\"+key]=f\"\"\"[{best[key]}]\"\"\"\n\t    return best_hypers\n\tclass Run( multiprocessing.Process):\n\t    def __init__(self,task,pool=0,idx=0,tc=0,start_time=0):\n\t        super().__init__()\n\t        self.task=task\n\t        self.log=os.path.join(task['study_name'])\n\t        self.idx=idx\n", "        self.pool=pool\n\t        self.device=None\n\t        self.tc=tc\n\t        self.start_time=start_time\n\t        #self.pbar=pbar\n\t    def run(self):\n\t        print(f\"{'*'*10} study  {self.log} no.{self.idx} waiting for device\")\n\t        count=0\n\t        device_units=[]\n\t        while True:\n", "            if len(device_units)>0:\n\t                try:\n\t                    unit=self.pool.get(timeout=10*random.random())\n\t                except queue.Empty:\n\t                    for unit in device_units:\n\t                            self.pool.put(unit)\n\t                    print(f\"Hold {str(device_units)} and waiting for too long! Throw back and go to sleep\")\n\t                    time.sleep(10*random.random())\n\t                    device_units=[]\n\t                    count=0\n", "                    continue\n\t            else:\n\t                unit=self.pool.get()\n\t            if len(device_units)>0:  # consistency check\n\t                if unit[0]!=device_units[-1][0]:\n\t                    print(f\"Get {str(device_units)} and {unit} not consistent devices and throw back it\")\n\t                    self.pool.put(unit)\n\t                    time.sleep(10*random.random())\n\t                    continue\n\t            count+=1\n", "            device_units.append(unit)\n\t            if count==self.task['cost']:\n\t                break\n\t        print(f\"{'-'*10}  study  {self.log} no.{self.idx} get the devices {str(device_units)} and start working\")\n\t        self.device=device_units[0][0]\n\t        try:\n\t            exit_command=get_command_from_argsDict(self.task,self.device,self.idx)\n\t            print(f\"running: {exit_command}\")\n\t            subprocess.run(exit_command,shell=True)\n\t        finally:\n", "            for unit in device_units:\n\t                self.pool.put(unit)\n\t            #localtime = time.asctime( time.localtime(time.time()) )\n\t        end_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\t        print(f\"Start time: {self.start_time}\\nEnd time: {end_time}\\nwith {self.idx}/{self.tc} tasks\")\n\t        print(f\"  {'<'*10} end  study  {self.log} no.{self.idx} of command \")\n\tdef get_command_from_argsDict(args_dict,gpu,idx):\n\t    command='python -W ignore run_analysis.py  '\n\t    for key in args_dict.keys():\n\t        command+=f\" --{key} {args_dict[key]} \"\n", "    command+=f\" --gpu {gpu} \"\n\t    if os.name!=\"nt\":\n\t        command+=f\"   > ./log/{args_dict['study_name']}.txt  \"\n\t    return command\n\tdef run_command_in_parallel(args_dict,gpus,worker_num):\n\t    command='python -W ignore run_dist.py  '\n\t    for key in args_dict.keys():\n\t        command+=f\" --{key} {args_dict[key]} \"\n\t    process_queue=[]\n\t    for gpu in gpus:\n", "        command+=f\" --gpu {gpu} \"\n\t        command+=f\"   > ./log/{args_dict['study_name']}.txt  \"\n\t        for _ in range(worker_num):\n\t            print(f\"running: {command}\")\n\t            p=Run(command)\n\t            p.daemon=True\n\t            p.start()\n\t            process_queue.append(p)\n\t            time.sleep(5)\n\t    for p in process_queue:\n", "        p.join()\n\tdef config_study_name(prefix,specified_args,extract_dict):\n\t    study_name=prefix\n\t    for k in specified_args:\n\t        v=extract_dict[k]\n\t        study_name+=f\"_{k}_{v}\"\n\t    if study_name[0]==\"_\":\n\t        study_name=study_name.replace(\"_\",\"\",1)\n\t    study_storage=f\"sqlite:///db/{study_name}.db\"\n\t    return study_name,study_storage\n"]}
{"filename": "NC/methods/SlotGAT/run_analysis.py", "chunked_list": ["from re import I\n\timport sys\n\timport pickle\n\tfrom numpy.core.numeric import identity\n\tsys.path.append('../../')\n\timport time\n\timport argparse\n\timport os\n\timport torch\n\timport torch.nn as nn\n", "import torch.nn.functional as F\n\timport numpy as np\n\timport random\n\tfrom utils.pytorchtools import EarlyStopping\n\tfrom utils.data import load_data\n\tfrom utils.tools import func_args_parse,single_feat_net,vis_data_collector,blank_profile,writeIntoCsvLogger,count_torch_tensor\n\t#from utils.tools import index_generator, evaluate_results_nc, parse_minibatch\n\tfrom GNN import myGAT,changedGAT,GAT,GCN,slotGAT,LabelPropagation,MLP\n\timport dgl\n\tfrom torch.profiler import profile, record_function, ProfilerActivity\n", "from torch.profiler import tensorboard_trace_handler\n\tfrom sklearn.manifold import TSNE\n\t#import wandb\n\timport threading\n\tfrom tqdm import tqdm\n\timport json\n\tdef set_seed(seed):\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n", "    np.random.seed(seed)  # Numpy module.\n\t    random.seed(seed)  # Python random module.\n\t    torch.use_deterministic_algorithms(True,warn_only=True)\n\t    torch.backends.cudnn.enabled = False \n\t    torch.backends.cudnn.benchmark = False\n\t    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n\t    os.environ['PYTHONHASHSEED'] = str(seed)\n\tset_seed(2022)\n\tfeature_usage_dict={0:\"loaded features\",\n\t1:\"only target node features (zero vec for others)\",\n", "2:\"only target node features (id vec for others)\",\n\t3:\"all id vec. Default is 2\",\n\t4:\"only term features (id vec for others)\",\n\t5:\"only term features (zero vec for others)\",\n\t}\n\tap = argparse.ArgumentParser(description='MRGNN testing for the DBLP dataset')\n\tap.add_argument('--feats-type', type=int, default=0,\n\t                help='Type of the node features used. ' +\n\t                        '0 - loaded features; ' +\n\t                        '1 - only target node features (zero vec for others); ' +\n", "                        '2 - only target node features (id vec for others); ' +\n\t                        '3 - all id vec. Default is 2;' +\n\t                    '4 - only term features (id vec for others);' + \n\t                    '5 - only term features (zero vec for others).')\n\tap.add_argument('--use_trained', type=str, default=\"False\")\n\tap.add_argument('--trained_dir', type=str, default=\"outputs\")\n\tap.add_argument('--save_trained', type=str, default=\"False\")\n\tap.add_argument('--save_dir', type=str, default=\"outputs\")\n\t#ap.add_argument('--hidden-dim', type=int, default=64, help='Dimension of the node hidden state. Default is 64.')\n\tap.add_argument('--num-heads', type=int, default=8, help='Number of the attention heads. Default is 8.')\n", "ap.add_argument('--epoch', type=int, default=300, help='Number of epochs.')\n\tap.add_argument('--patience', type=int, default=30, help='Patience.')\n\tap.add_argument('--repeat', type=int, default=30, help='Repeat the training and testing for N times. Default is 1.')\n\t#ap.add_argument('--num-layers', type=int, default=2)\n\t#ap.add_argument('--lr', type=float, default=5e-4)\n\tap.add_argument('--dropout_feat', type=float, default=0.5)\n\tap.add_argument('--dropout_attn', type=float, default=0.5)\n\t#ap.add_argument('--weight-decay', type=float, default=1e-4)\n\tap.add_argument('--slope', type=float, default=0.05)\n\tap.add_argument('--residual', type=str, default=\"True\")\n", "ap.add_argument('--dataset', type=str)\n\tap.add_argument('--edge-feats', type=int, default=64)\n\tap.add_argument('--run', type=int, default=1)\n\tap.add_argument('--cost', type=int, default=1)\n\tap.add_argument('--gpu', type=str, default=\"0\")\n\t#ap.add_argument('--hiddens', type=str, default=\"64_32\")\n\tap.add_argument('--activation', type=str, default=\"elu\")\n\tap.add_argument('--bias', type=str, default=\"true\")\n\tap.add_argument('--net', type=str, default=\"myGAT\")\n\tap.add_argument('--L2_norm', type=str, default=\"False\")\n", "ap.add_argument('--task_property', type=str, default=\"notSpecified\")\n\tap.add_argument('--study_name', type=str, default=\"temp\")\n\tap.add_argument('--verbose', type=str, default=\"False\") \n\tap.add_argument('--slot_aggregator', type=str, default=\"None\") \n\tap.add_argument('--SAattDim', type=int, default=3) \n\tap.add_argument('--LP_alpha', type=float, default=0.5)  #1,0.99,0.5\n\tap.add_argument('--get_out', default=\"\")  \n\t#ap.add_argument('--get_out_tasks', default=\"\")  \n\tap.add_argument('--profile', default=\"False\")  \n\tap.add_argument('--get_out_tsne', default=\"False\")  \n", "ap.add_argument('--normalize', default=\"True\")  \n\t# to search\n\tap.add_argument('--search_num_heads', type=str, default=\"[8]\")\n\tap.add_argument('--search_lr', type=str, default=\"[1e-3,5e-4,1e-4]\")\n\tap.add_argument('--search_weight_decay', type=str, default=\"[5e-4,1e-4,1e-5]\")\n\tap.add_argument('--search_hidden_dim', type=str, default=\"[64,128]\")\n\tap.add_argument('--search_num_layers', type=str, default=\"[2]\")\n\ttorch.set_num_threads(4)\n\targs = ap.parse_args()\n\tos.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n", "try:\n\t    torch.cuda.set_device(int(args.gpu))\n\texcept :\n\t    pass\n\tdef sp_to_spt(mat):\n\t    coo = mat.tocoo()\n\t    values = coo.data\n\t    indices = np.vstack((coo.row, coo.col))\n\t    i = torch.LongTensor(indices)\n\t    v = torch.FloatTensor(values)\n", "    shape = coo.shape\n\t    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n\tdef mat2tensor(mat):\n\t    if type(mat) is np.ndarray:\n\t        return torch.from_numpy(mat).type(torch.FloatTensor)\n\t    return sp_to_spt(mat)\n\tdef run_model_DBLP(trial=None):\n\t    #data preparation\n\t    num_heads=int(eval(args.search_num_heads)[0]);assert len(eval(args.search_num_heads))==1\n\t    lr=float(eval(args.search_lr)[0]);assert len(eval(args.search_lr))==1\n", "    weight_decay=float(eval(args.search_weight_decay)[0]);assert len(eval(args.search_weight_decay))==1\n\t    hidden_dim=int(eval(args.search_hidden_dim)[0]);assert len(eval(args.search_hidden_dim))==1\n\t    num_layers=int(eval(args.search_num_layers)[0]);assert len(eval(args.search_num_layers))==1\n\t    if True:\n\t        get_out=args.get_out.split(\"_\")\n\t        getSAAttentionScore=\"True\" if \"getSAAttentionScore\" in args.get_out else \"False\"\n\t        dataRecorder={\"meta\":{\n\t            \"getSAAttentionScore\":getSAAttentionScore,\n\t        },\"data\":{},\"status\":\"None\"}\n\t        feats_type = args.feats_type\n", "        slot_aggregator=args.slot_aggregator   \n\t        multi_labels=True if args.dataset in [\"IMDB\",\"IMDB_hgb\"] else False  #imdb online\n\t        dl_mode='multi' if multi_labels else 'bi'\n\t        features_list, adjM, labels, train_val_test_idx, dl = load_data(args.dataset,multi_labels=multi_labels)\n\t        class_num=max(labels)+1 if not multi_labels else len(labels[0])   \n\t        vis_data_saver=vis_data_collector() \n\t        running_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\t        print(running_time)\n\t        device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n\t        features_list = [mat2tensor(features).to(device) for features in features_list] \n", "        if feats_type == 0:\n\t            in_dims = [features.shape[1] for features in features_list]\n\t        elif feats_type == 1 or feats_type == 5:\n\t            save = 0 if feats_type == 1 else 2\n\t            in_dims = []#[features_list[0].shape[1]] + [10] * (len(features_list) - 1)\n\t            for i in range(0, len(features_list)):\n\t                if i == save:\n\t                    in_dims.append(features_list[i].shape[1])\n\t                else:\n\t                    in_dims.append(10)\n", "                    features_list[i] = torch.zeros((features_list[i].shape[0], 10)).to(device)\n\t        elif feats_type == 2 or feats_type == 4:\n\t            save = feats_type - 2\n\t            in_dims = [features.shape[0] for features in features_list]\n\t            for i in range(0, len(features_list)):\n\t                if i == save:\n\t                    in_dims[i] = features_list[i].shape[1]\n\t                    continue\n\t                dim = features_list[i].shape[0]\n\t                indices = np.vstack((np.arange(dim), np.arange(dim)))\n", "                indices = torch.LongTensor(indices)\n\t                values = torch.FloatTensor(np.ones(dim))\n\t                features_list[i] = torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device)\n\t        elif feats_type == 3:\n\t            in_dims = [features.shape[0] for features in features_list]\n\t            for i in range(len(features_list)):\n\t                dim = features_list[i].shape[0]\n\t                indices = np.vstack((np.arange(dim), np.arange(dim)))\n\t                indices = torch.LongTensor(indices)\n\t                values = torch.FloatTensor(np.ones(dim))\n", "                features_list[i] = torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device)\n\t        labels = torch.LongTensor(labels).to(device)  if not multi_labels else  torch.FloatTensor(labels).to(device)\n\t        train_idx = train_val_test_idx['train_idx']\n\t        train_idx = np.sort(train_idx)\n\t        val_idx = train_val_test_idx['val_idx']\n\t        val_idx = np.sort(val_idx)\n\t        test_idx = train_val_test_idx['test_idx']\n\t        test_idx = np.sort(test_idx)\n\t        edge2type = {}\n\t        for k in dl.links['data']:\n", "            for u,v in zip(*dl.links['data'][k].nonzero()):\n\t                edge2type[(u,v)] = k\n\t        count_self=0\n\t        for i in range(dl.nodes['total']):\n\t            FLAG=0\n\t            if (i,i) not in edge2type:\n\t                edge2type[(i,i)] = len(dl.links['count'])\n\t                FLAG=1\n\t        count_self+=FLAG\n\t        count_reverse=0\n", "        for k in dl.links['data']:\n\t            FLAG=0\n\t            for u,v in zip(*dl.links['data'][k].nonzero()):\n\t                if (v,u) not in edge2type:\n\t                    edge2type[(v,u)] = count_reverse+1+len(dl.links['count'])\n\t                    FLAG=1\n\t            count_reverse+=FLAG\n\t        num_etype=len(dl.links['count'])+count_self+count_reverse\n\t        g = dgl.DGLGraph(adjM+(adjM.T))\n\t        g = dgl.remove_self_loop(g)\n", "        g = dgl.add_self_loop(g)\n\t        g = g.to(device)\n\t        e_feat = []\n\t        count=0\n\t        count_mappings={}\n\t        counted_dict={}\n\t        eid=0\n\t        etype_ids={}\n\t        g_=g.cpu()\n\t        for u, v in tqdm(zip(*g_.edges())):\n", "            u =u.item() #u.cpu().item()\n\t            v =v.item() #v.cpu().item()\n\t            if not counted_dict.setdefault(edge2type[(u,v)],False) :\n\t                count_mappings[edge2type[(u,v)]]=count\n\t                counted_dict[edge2type[(u,v)]]=True\n\t                count+=1\n\t            e_feat.append(count_mappings[edge2type[(u,v)]])\n\t            if edge2type[(u,v)] in etype_ids.keys():\n\t                etype_ids[edge2type[(u,v)]].append(eid)\n\t            else:\n", "                etype_ids[edge2type[(u,v)]]=[eid]\n\t            eid+=1\n\t        e_feat = torch.tensor(e_feat, dtype=torch.long).to(device)\n\t        g.etype_ids=etype_ids\n\t        reduc=\"mean\"\n\t        loss = nn.BCELoss(reduction=reduc) if multi_labels else F.nll_loss\n\t        loss_val = nn.BCELoss() if multi_labels else F.nll_loss\n\t        g.edge_type_indexer=F.one_hot(e_feat).to(device)\n\t        num_ntypes=len(features_list)\n\t        num_nodes=dl.nodes['total']\n", "        g.node_idx_by_ntype=[]\n\t        g.num_ntypes=num_ntypes\n\t        g.node_ntype_indexer=torch.zeros(num_nodes,num_ntypes).to(device)\n\t        ntype_dims=[]\n\t        idx_count=0\n\t        ntype_count=0\n\t        for feature in features_list:\n\t            temp=[]\n\t            for _ in feature:\n\t                temp.append(idx_count)\n", "                g.node_ntype_indexer[idx_count][ntype_count]=1\n\t                idx_count+=1\n\t            g.node_idx_by_ntype.append(temp)\n\t            ntype_dims.append(feature.shape[1])\n\t            ntype_count+=1 \n\t        eindexer=None\n\t    LP_alpha=args.LP_alpha\n\t    ma_F1s=[]\n\t    mi_F1s=[]\n\t    val_accs=[]\n", "    val_losses_neg=[]\n\t    toCsvRepetition=[]\n\t    for re in range(args.repeat):\n\t        training_times=[]\n\t        inference_times=[]\n\t        #re-id the train-validation in each repeat\n\t        tr_len,val_len=len(train_idx),len(val_idx)\n\t        total_idx=np.concatenate([train_idx,val_idx])\n\t        total_idx=np.random.permutation(total_idx)\n\t        train_idx,val_idx=total_idx[0:tr_len],total_idx[tr_len:tr_len+val_len] \n", "        net_wrapper=single_feat_net\n\t        t_re0=time.time()\n\t        num_classes = dl.labels_train['num_classes']\n\t        heads = [num_heads] * num_layers + [1]\n\t        if args.net=='myGAT':\n\t            GNN=myGAT\n\t            fargs,fkargs=func_args_parse(g, args.edge_feats, num_etype, in_dims, hidden_dim, num_classes, num_layers, heads, F.elu, args.dropout_feat,args.dropout_attn, args.slope, True, 0.05,dataRecorder = dataRecorder)\n\t        elif args.net=='changedGAT':\n\t            GNN=changedGAT\n\t            fargs,fkargs=func_args_parse(g, args.edge_feats, num_etype, in_dims, hidden_dim, num_classes, num_layers, heads, F.elu, args.dropout_feat,args.dropout_attn, args.slope, True, 0.05,num_ntype=num_ntypes, eindexer=eindexer, dataRecorder = dataRecorder)\n", "        elif args.net=='slotGAT':\n\t            GNN=slotGAT\n\t            fargs,fkargs=func_args_parse(g, args.edge_feats, num_etype, in_dims, hidden_dim, num_classes, num_layers, heads, F.elu, args.dropout_feat,args.dropout_attn, args.slope, True, 0.05,num_ntype=num_ntypes, eindexer=eindexer, aggregator=slot_aggregator ,SAattDim=args.SAattDim,dataRecorder=dataRecorder,vis_data_saver=vis_data_saver)\n\t        elif args.net=='GAT':\n\t            GNN=GAT\n\t            fargs,fkargs=func_args_parse(g, in_dims, hidden_dim, num_classes, num_layers, heads, F.elu, args.dropout_feat,args.dropout_attn, args.slope, True, dataRecorder = dataRecorder)\n\t        elif args.net=='GCN':\n\t            GNN=GCN\n\t            fargs,fkargs=func_args_parse(g, in_dims, hidden_dim, num_classes, num_layers, F.relu, args.dropout_feat, dataRecorder = dataRecorder)\n\t        elif args.net=='LabelPropagation':\n", "            #net=LabelPropagation(num_layers, LP_alpha)\n\t            GNN=LabelPropagation\n\t            fargs,fkargs=func_args_parse(num_layers, LP_alpha)\n\t        elif args.net=='MLP':\n\t            GNN=MLP\n\t            fargs,fkargs=func_args_parse(g,in_dims,hidden_dim,num_classes,num_layers,F.relu,args.dropout_feat)\n\t        else:\n\t            raise NotImplementedError()\n\t        net=net_wrapper(GNN,*fargs,**fkargs)\n\t        print(f\"model using: {net.__class__.__name__}\")  if args.verbose==\"True\" else None\n", "        net.to(device)\n\t        if args.use_trained==\"True\":\n\t            ckp_fname=os.path.join(args.trained_dir,args.net,args.dataset,str(re),\"model.pt\")\n\t        else:\n\t            if args.net==\"LabelPropagation\":\n\t                pass\n\t            else:\n\t                optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n\t            net.train()\n\t            if args.save_trained==\"True\":\n", "                # files in save_dir should be considered ***important***\n\t                ckp_fname=os.path.join(args.save_dir,args.net,args.dataset,str(re),\"model.pt\")\n\t                os.makedirs(os.path.dirname(ckp_fname),exist_ok=True)\n\t            else:\n\t                # files in checkpoint could be considered to be deleted\n\t                t=time.localtime()\n\t                str_t=f\"{t.tm_year:0>4d}{t.tm_mon:0>2d}{t.tm_hour:0>2d}{t.tm_min:0>2d}{t.tm_sec:0>2d}{int(time.time()*1000)%1000}\"\n\t                ckp_dname=os.path.join('checkpoint',str_t)\n\t                os.mkdir(ckp_dname)\n\t                ckp_fname=os.path.join(ckp_dname,'checkpoint_{}_{}_re_{}_feat_{}_heads_{}_{}.pt'.format(args.dataset, num_layers,re,args.feats_type,num_heads,net.__class__.__name__))\n", "            early_stopping = EarlyStopping(patience=args.patience, verbose=False, save_path=ckp_fname)\n\t            if args.profile==\"True\":\n\t                profile_func=profile\n\t            elif args.profile==\"False\":\n\t                profile_func=blank_profile\n\t            with profile_func(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True,schedule=torch.profiler.schedule(\n\t                    wait=2,\n\t                    warmup=2,\n\t                    active=6,\n\t                    repeat=1),on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./profiler/trace_\"+args.study_name)) as prof:\n", "                for epoch in range(args.epoch):\n\t                    training_time_start=time.time()\n\t                    if args.net==\"LabelPropagation\"  :\n\t                        continue\n\t                    t_0_start = time.time()\n\t                    # training\n\t                    net.train()\n\t                    with record_function(\"model_inference\"):\n\t                        net.dataRecorder[\"status\"]=\"Training\"\n\t                        logits,_ = net(features_list, e_feat) \n", "                        net.dataRecorder[\"status\"]=\"None\"\n\t                    logp = F.log_softmax(logits, 1) if not multi_labels else F.sigmoid(logits)\n\t                    train_loss = loss(logp[train_idx], labels[train_idx])# if not multi_labels else loss(logp[train_idx], labels[train_idx])\n\t                    # autograd\n\t                    optimizer.zero_grad()\n\t                    with record_function(\"model_backward\"):\n\t                        train_loss.backward()\n\t                        optimizer.step()\n\t                    t_0_end = time.time()\n\t                    training_time_end=time.time()\n", "                    training_times.append(training_time_end-training_time_start)\n\t                    t_1_start = time.time()\n\t                    #validation\n\t                    net.eval()\n\t                    with torch.no_grad():\n\t                        net.dataRecorder[\"status\"]=\"Validation\"\n\t                        logits,_ = net(features_list, e_feat)\n\t                        net.dataRecorder[\"status\"]=\"None\"\n\t                        logp = F.log_softmax(logits, 1) if not multi_labels else F.sigmoid(logits)\n\t                        val_loss = loss_val(logp[val_idx], labels[val_idx])\n", "                    t_1_end = time.time()\n\t                    # print validation info\n\t                    print('Epoch {:05d} | Train_Loss: {:.4f} | train Time: {:.4f} | Val_Loss {:.4f} | val Time(s) {:.4f}'.format(\n\t                        epoch, train_loss.item(), t_0_end-t_0_start,val_loss.item(), t_1_end - t_1_start)) if args.verbose==\"True\" else None\n\t                    # early stopping\n\t                    early_stopping(val_loss, net)\n\t                    if epoch>args.epoch/2 and early_stopping.early_stop:\n\t                        #print('Early stopping!')\n\t                        break\n\t                    prof.step()\n", "        # validation with evaluate_results_nc\n\t        if args.net!=\"LabelPropagation\":\n\t            net.load_state_dict(torch.load(ckp_fname),strict=False)\n\t        net.eval()\n\t        with torch.no_grad():            \n\t            net.dataRecorder[\"status\"]=\"FinalValidation\"\n\t            infer_time_start=time.time()\n\t            logits,_ = net(features_list, e_feat,get_out=get_out) if args.net!=\"LabelPropagation\"  else net(g,labels,mask=train_idx)\n\t            logp = F.log_softmax(logits, 1) if not multi_labels else F.sigmoid(logits)\n\t            val_loss = loss_val(logp[val_idx], labels[val_idx])\n", "            net.dataRecorder[\"status\"]=\"None\"\n\t            val_logits = logits[val_idx]\n\t            pred=val_logits.argmax(axis=1) if not multi_labels else (val_logits>0).int()\n\t            all_pred=logits.argmax(axis=1) if not multi_labels else (logits>0).int()           \n\t        val_results=dl.evaluate_by_group(all_pred,val_idx,train=True,mode=dl_mode)\n\t        test_results=dl.evaluate_by_group(all_pred,test_idx,train=False,mode=dl_mode)\n\t        infer_time_end=time.time()\n\t        inference_times.append(infer_time_end-infer_time_start)\n\t        vis_data_saver.collect_in_run(test_results[\"micro-f1\"],\"micro-f1\",re=re)\n\t        vis_data_saver.collect_in_run(test_results[\"macro-f1\"],\"macro-f1\",re=re)\n", "        training_time_mean=sum(training_times)/(len(training_times)+1e-10)\n\t        training_time_total=sum(training_times)\n\t        inference_time_mean=sum(inference_times)/(len(inference_times)+1e-10)\n\t        inference_time_total=sum(inference_times)\n\t        global peak_gpu_memory\n\t        peak_gpu_memory_by_torch=torch.cuda.max_memory_allocated()/1024/1024/1024\n\t        toCsv={ \"0_dataset\":args.dataset,\n\t                \"0_net\":args.net,\n\t                \"0_aggregator\":args.slot_aggregator,\n\t                \"0_getout\":args.get_out,\n", "                \"1_featType\":feats_type,\n\t                \"1_numOfEpoch\":args.epoch,\n\t                \"1_numLayers\":num_layers,\n\t                \"1_hiddenDim\":hidden_dim,\n\t                \"1_SAattDim\":args.SAattDim,\n\t                \"1_numOfHeads\":num_heads,\n\t                \"1_Lr\":lr,\n\t                \"1_Wd\":weight_decay,\n\t                \"1_dropoutFeat\":args.dropout_feat,\n\t                \"1_dropoutAttn\":args.dropout_attn,\n", "                \"1_L2_norm\":args.L2_norm,\n\t                \"2_valAcc_mean\":val_results[\"acc\"],\n\t                \"2_valAcc_std\":val_results[\"acc\"],\n\t                \"2_valMiPre_mean\":val_results[\"micro-pre\"],\n\t                \"2_valMiPre_std\":val_results[\"micro-pre\"],\n\t                \"2_valMaPre_mean\":val_results[\"macro-pre\"],\n\t                \"2_valMaPre_std\":val_results[\"macro-pre\"],\n\t                \"2_valMiRec_mean\":val_results[\"micro-rec\"],\n\t                \"2_valMiRec_std\":val_results[\"micro-rec\"],\n\t                \"2_valMaRec_mean\":val_results[\"macro-rec\"],\n", "                \"2_valMaRec_std\":val_results[\"macro-rec\"],\n\t                \"2_valMiF1_mean\":val_results[\"micro-f1\"],\n\t                \"2_valMiF1_std\":val_results[\"micro-f1\"],\n\t                \"2_valMaF1_mean\":val_results[\"macro-f1\"],\n\t                \"2_valMaF1_std\":val_results[\"macro-f1\"],\n\t                \"3_testAcc_mean\":test_results[\"acc\"],\n\t                \"3_testAcc_std\":test_results[\"acc\"],\n\t                \"3_testMiPre_mean\":test_results[\"micro-pre\"],\n\t                \"3_testMiPre_std\":test_results[\"micro-pre\"],\n\t                \"3_testMaPre_mean\":test_results[\"macro-pre\"],\n", "                \"3_testMaPre_std\":test_results[\"macro-pre\"],\n\t                \"3_testMiRec_mean\":test_results[\"micro-rec\"],\n\t                \"3_testMiRec_std\":test_results[\"micro-rec\"],\n\t                \"3_testMaRec_mean\":test_results[\"macro-rec\"],\n\t                \"3_testMaRec_std\":test_results[\"macro-rec\"],\n\t                \"3_testMiF1_mean\":test_results[\"micro-f1\"],\n\t                \"3_testMiF1_std\":test_results[\"micro-f1\"],\n\t                \"3_testMaF1_mean\":test_results[\"macro-f1\"],\n\t                \"3_testMaF1_std\":test_results[\"macro-f1\"], \n\t                \"3_training_time_per_epoch_mean\":training_time_mean,\n", "                \"3_training_time_per_epoch_total\":training_time_total,\n\t                \"3_inference_time_per_epoch_mean\":inference_time_mean,\n\t                \"3_inference_time_per_epoch_total\":inference_time_total,\n\t                \"3_peak_memory\":peak_gpu_memory_by_torch,\n\t                }\n\t        toCsvRepetition.append(toCsv)\n\t        if not multi_labels:\n\t            val_acc=val_results[\"acc\"]\n\t            val_accs.append(val_acc)\n\t            score=sum(val_accs)/len(val_accs)\n", "        else:\n\t            val_losses_neg.append(1/(1+val_loss))\n\t            score=sum(val_losses_neg)/len(val_losses_neg)\n\t        # testing with evaluate_results_nc\n\t        if args.net!=\"LabelPropagation\":\n\t            net.load_state_dict(torch.load(ckp_fname),strict=False) \n\t        net.eval()\n\t        test_logits = []\n\t        with torch.no_grad():\n\t            net.dataRecorder[\"status\"]=\"FinalTesting\"\n", "            logits,_ = net(features_list, e_feat,get_out=get_out) if args.net!=\"LabelPropagation\"  else net(g,labels,mask=train_idx)\n\t            net.dataRecorder[\"status\"]=\"None\" \n\t            test_logits = logits[test_idx]\n\t            pred = test_logits.cpu().numpy().argmax(axis=1) if not multi_labels else (test_logits.cpu().numpy()>0).astype(int)\n\t            onehot = np.eye(num_classes, dtype=np.int32)\n\t            pred = onehot[pred] if not multi_labels else  pred\n\t            d=dl.evaluate(pred,mode=dl_mode)\n\t            print(d) if args.verbose==\"True\" else None\n\t        ma_F1s.append(d[\"macro-f1\"])\n\t        mi_F1s.append(d[\"micro-f1\"])\n", "        t_re1=time.time();t_re=t_re1-t_re0\n\t    vis_data_saver.collect_whole_process(round(float(100*np.mean(np.array(ma_F1s)) ),2),name=\"macro-f1-mean\");vis_data_saver.collect_whole_process(round(float(100*np.std(np.array(ma_F1s)) ),2),name=\"macro-f1-std\");vis_data_saver.collect_whole_process(round(float(100*np.mean(np.array(mi_F1s)) ),2),name=\"micro-f1-mean\");vis_data_saver.collect_whole_process(round(float(100*np.std(np.array(mi_F1s)) ),2),name=\"micro-f1-std\")\n\t    print(f\"mean and std of macro-f1: {  100*np.mean(np.array(ma_F1s)) :.1f}\\u00B1{  100*np.std(np.array(ma_F1s)) :.1f}\");print(f\"mean and std of micro-f1: {  100*np.mean(np.array(mi_F1s)) :.1f}\\u00B1{  100*np.std(np.array(mi_F1s)) :.1f}\") \n\t    #print(optimizer) if args.verbose==\"True\" else None\n\t    toCsvAveraged={}\n\t    for tocsv in toCsvRepetition:\n\t        for name in tocsv.keys():\n\t            if name.startswith(\"1_\"):\n\t                toCsvAveraged[name]=tocsv[name]\n\t            else:\n", "                if name not in toCsvAveraged.keys():\n\t                    toCsvAveraged[name]=[]\n\t                toCsvAveraged[name].append(tocsv[name])\n\t    for name in toCsvAveraged.keys():\n\t        if not name.startswith(\"1_\") :\n\t            if type(toCsvAveraged[name][0]) is str:\n\t                toCsvAveraged[name]=toCsvAveraged[name][0]\n\t            elif \"_mean\" in name:\n\t                toCsvAveraged[name]=sum(toCsvAveraged[name])/len(toCsvAveraged[name])\n\t            elif \"_total\" in name:\n", "                toCsvAveraged[name]=sum(toCsvAveraged[name])/len(toCsvAveraged[name])\n\t            elif \"_std\" in name:\n\t                toCsvAveraged[name]= np.std(np.array(toCsvAveraged[name])) \n\t            elif \"peak\" in name:\n\t                toCsvAveraged[name]=max(toCsvAveraged[name])\n\t            else:\n\t                raise Exception()\n\t    writeIntoCsvLogger(toCsvAveraged,f\"./log/{args.study_name}.csv\")\n\t    fn=os.path.join(\"log\",args.study_name)\n\t    if os.path.exists(fn):\n", "        m=\"a\"\n\t    else:\n\t        m=\"w\"\n\t    with open(fn,m) as f:\n\t        f.write(f\"score {  score :.4f}  mean and std of macro-f1: {  100*np.mean(np.array(ma_F1s)) :.1f}\\u00B1{  100*np.std(np.array(ma_F1s)) :.1f} micro-f1: {  100*np.mean(np.array(mi_F1s)) :.1f}\\u00B1{  100*np.std(np.array(mi_F1s)) :.1f}\\n\") \n\t        if trial:\n\t            f.write(f\"trial.params: {str(trial.params)}\"+\"\\n\")\n\t    return score\n\tdef remove_ckp_files(ckp_dname):\n\t    import shutil\n", "    shutil.rmtree(ckp_dname) \n\tif __name__ == '__main__':\n\t    peak_gpu_memory=0\n\t    run_model_DBLP(trial=None)\n"]}
{"filename": "NC/methods/SlotGAT/run_train_slotGAT_on_all_dataset.py", "chunked_list": ["import time\n\timport subprocess\n\timport multiprocessing\n\tfrom threading import main_thread\n\tfrom pipeline_utils import get_best_hypers,run_command_in_parallel,config_study_name,Run,get_tasks,get_tasks_linear_around,get_tasks_for_online\n\timport os\n\timport copy\n\t#time.sleep(60*60*4)\n\tos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\tif not os.path.exists(\"./log\"):\n", "    os.mkdir(\"./log\")\n\tif not os.path.exists(\"./checkpoint\"):\n\t    os.mkdir(\"./checkpoint\")\n\tif not os.path.exists(\"./analysis\"):\n\t    os.mkdir(\"./analysis\")\n\tif not os.path.exists(\"./model_files\"):\n\t    os.mkdir(\"./model_files\")\n\tresources_dict={\"0\":1,\"1\":1}   #id:load\n\tstart_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\tresources=resources_dict.keys()\n", "pool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\n\tfor i in resources:\n\t    for j in range(resources_dict[i]):\n\t        pool.put(i+str(j))\n\tprefix=\"get_results\";specified_args=[\"dataset\",  \"net\", ]\n\tnets=[\"slotGAT\"]\n\tdataset_restrict=[]\n\tfixed_info_by_net={\"slotGAT\":\n\t                        {\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\", \"verbose\":\"True\",\n\t                         \"use_trained\":\"False\",\n", "                        #\"trained_dir\":\"outputs\",\n\t                        \"save_trained\":\"True\",\n\t                        \"save_dir\":\"outputs\",},\n\t}\n\tdataset_and_hypers_by_net={\n\t        \"slotGAT\":\n\t            {\n\t            (\"IMDB\",1,5):\n\t                {\"search_hidden_dim\":128,\"search_num_layers\":3,\"search_lr\":0.0001,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":3,\"dropout_feat\":0.8,\"dropout_attn\":0.2},\n\t            (\"ACM\",1,5):\n", "                {\"search_hidden_dim\":64,\"search_num_layers\":2,\"search_lr\":0.001,\"search_weight_decay\":0.0001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.8,\"dropout_attn\":0.8},\n\t            (\"DBLP\",1,5):\n\t                {\"search_hidden_dim\":64,\"search_num_layers\":4,\"search_lr\":0.0001,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.5,\"dropout_attn\":0.5},\n\t            (\"Freebase\",1,5):\n\t                {\"search_hidden_dim\":16,\"search_num_layers\":2,\"search_lr\":0.0005,\"search_weight_decay\":0.001,\"feats-type\":2,\"num-heads\":8,\"epoch\":300,\"SAattDim\":8,\"dropout_feat\":0.5,\"dropout_attn\":0.5,\"edge-feats\":\"0\"},\n\t            (\"PubMed_NC\",1,5):\n\t                {\"search_hidden_dim\":128,\"search_num_layers\":2,\"search_lr\":0.005,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":3,\"dropout_feat\":0.2,\"dropout_attn\":0.8,}\n\t             },\n\t    }\n\tdef getTasks(fixed_info,dataset_and_hypers):\n", "    for k,v in dataset_and_hypers.items():\n\t        for k1,v1 in v.items():\n\t            if \"search_\" in k1:\n\t                if type(v1)!=str:\n\t                    v[k1]=f\"[{v1}]\"\n\t    tasks_list=[]\n\t    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n\t        if len(dataset_restrict)>0 and dataset not in dataset_restrict:\n\t            continue\n\t        args_dict={}\n", "        for dict_to_add in [task,fixed_info]:\n\t            for k,v in dict_to_add.items():\n\t                args_dict[k]=v \n\t        args_dict['dataset']=dataset\n\t        #args_dict['trial_num']=trial_num\n\t        args_dict['repeat']=repeat\n\t        study_name,study_storage=config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n\t        args_dict['study_name']=study_name\n\t        #args_dict['study_storage']=study_storage\n\t        args_dict['cost']=cost\n", "        tasks_list.append(args_dict)\n\t    print(\"tasks_list:\", tasks_list)\n\t    return tasks_list \n\ttasks_list=[]\n\tfor net in nets:\n\t    tasks_list.extend(getTasks(fixed_info_by_net[net],dataset_and_hypers_by_net[net]))\n\tsub_queues=[]\n\titems=len(tasks_list)%60\n\tfor i in range(items):\n\t    sub_queues.append(tasks_list[60*i:(60*i+60)])\n", "sub_queues.append(tasks_list[(60*items+60):])\n\tif items==0:\n\t    sub_queues.append(tasks_list)\n\t## split the tasks, or it may exceeds of maximal size of sub-processes of OS.\n\tidx=0\n\ttc=len(tasks_list)\n\tfor sub_tasks_list in sub_queues:\n\t    process_queue=[]\n\t    for i in range(len(sub_tasks_list)):\n\t        idx+=1\n", "        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n\t        p.daemon=True\n\t        p.start()\n\t        process_queue.append(p)\n\t    for p in process_queue:\n\t        p.join()\n\tprint('end all')\n\tend_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())"]}
{"filename": "NC/methods/SlotGAT/run_use_slotGAT_on_all_dataset.py", "chunked_list": ["import time\n\timport subprocess\n\timport multiprocessing\n\tfrom threading import main_thread\n\tfrom pipeline_utils import get_best_hypers,run_command_in_parallel,config_study_name,Run,get_tasks,get_tasks_linear_around,get_tasks_for_online\n\timport os\n\timport copy\n\t#time.sleep(60*60*4)\n\tos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\tif not os.path.exists(\"./log\"):\n", "    os.mkdir(\"./log\")\n\tif not os.path.exists(\"./checkpoint\"):\n\t    os.mkdir(\"./checkpoint\")\n\tif not os.path.exists(\"./analysis\"):\n\t    os.mkdir(\"./analysis\")\n\tif not os.path.exists(\"./outputs\"):\n\t    os.mkdir(\"./outputs\")\n\tresources_dict={\"0\":1,\"1\":1}   #id:load \n\tstart_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\tresources=resources_dict.keys()\n", "pool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\n\tfor i in resources:\n\t    for j in range(resources_dict[i]):\n\t        pool.put(i+str(j)) \n\tprefix=\"get_results_use_trained\";specified_args=[\"dataset\",  \"net\", ]\n\tnets=[\"slotGAT\"] \n\tdataset_restrict=[]\n\tfixed_info_by_net={\"slotGAT\":\n\t                        {\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\", \"verbose\":\"True\",\n\t                        \"use_trained\":\"True\",\n", "                        \"trained_dir\":\"outputs\",\n\t                        \"save_trained\":\"False\",\n\t                        #\"save_dir\":\"outputs\",\n\t                        },\n\t}\n\tdataset_and_hypers_by_net={\n\t        \"slotGAT\":\n\t            {\n\t             (\"IMDB\",1,5):\n\t                {\"search_hidden_dim\":128,\"search_num_layers\":3,\"search_lr\":0.0001,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":3,\"dropout_feat\":0.8,\"dropout_attn\":0.2},\n", "            (\"ACM\",1,5):\n\t                {\"search_hidden_dim\":64,\"search_num_layers\":2,\"search_lr\":0.001,\"search_weight_decay\":0.0001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.8,\"dropout_attn\":0.8},\n\t            (\"DBLP\",1,5): \n\t                {\"search_hidden_dim\":64,\"search_num_layers\":4,\"search_lr\":0.0001,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":32,\"dropout_feat\":0.5,\"dropout_attn\":0.5},\n\t            (\"Freebase\",1,5):\n\t                {\"search_hidden_dim\":16,\"search_num_layers\":2,\"search_lr\":0.0005,\"search_weight_decay\":0.001,\"feats-type\":2,\"num-heads\":8,\"epoch\":300,\"SAattDim\":8,\"dropout_feat\":0.5,\"dropout_attn\":0.5,\"edge-feats\":\"0\"},\n\t            (\"PubMed_NC\",1,5):\n\t                {\"search_hidden_dim\":128,\"search_num_layers\":2,\"search_lr\":0.005,\"search_weight_decay\":0.001,\"feats-type\":1,\"num-heads\":8,\"epoch\":300,\"SAattDim\":3,\"dropout_feat\":0.2,\"dropout_attn\":0.8,}\n\t            },\n\t    }\n", "def getTasks(fixed_info,dataset_and_hypers):\n\t    for k,v in dataset_and_hypers.items():\n\t        for k1,v1 in v.items():\n\t            if \"search_\" in k1:\n\t                if type(v1)!=str:\n\t                    v[k1]=f\"[{v1}]\"\n\t    tasks_list=[]\n\t    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n\t        if len(dataset_restrict)>0 and dataset not in dataset_restrict:\n\t            continue\n", "        args_dict={}\n\t        for dict_to_add in [task,fixed_info]:\n\t            for k,v in dict_to_add.items():\n\t                args_dict[k]=v \n\t        args_dict['dataset']=dataset\n\t        #args_dict['trial_num']=trial_num\n\t        args_dict['repeat']=repeat\n\t        study_name,study_storage=config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n\t        args_dict['study_name']=study_name\n\t        #args_dict['study_storage']=study_storage\n", "        args_dict['cost']=cost\n\t        tasks_list.append(args_dict)\n\t    print(\"tasks_list:\", tasks_list)\n\t    return tasks_list \n\ttasks_list=[]\n\tfor net in nets:\n\t    tasks_list.extend(getTasks(fixed_info_by_net[net],dataset_and_hypers_by_net[net]))\n\tsub_queues=[]\n\titems=len(tasks_list)%60\n\tfor i in range(items):\n", "    sub_queues.append(tasks_list[60*i:(60*i+60)])\n\tsub_queues.append(tasks_list[(60*items+60):])\n\tif items==0:\n\t    sub_queues.append(tasks_list)\n\t## split the tasks, or it may exceeds of maximal size of sub-processes of OS.\n\tidx=0\n\ttc=len(tasks_list)\n\tfor sub_tasks_list in sub_queues:\n\t    process_queue=[]\n\t    for i in range(len(sub_tasks_list)):\n", "        idx+=1\n\t        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n\t        p.daemon=True\n\t        p.start()\n\t        process_queue.append(p)\n\t    for p in process_queue:\n\t        p.join()\n\tprint('end all')\n\tend_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())"]}
{"filename": "NC/methods/SlotGAT/utils/pytorchtools.py", "chunked_list": ["import numpy as np\n\timport torch\n\tclass EarlyStopping:\n\t    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\t    def __init__(self, patience, verbose=False, delta=0, save_path='checkpoint.pt'):\n\t        \"\"\"\n\t        Args:\n\t            patience (int): How long to wait after last time validation loss improved.\n\t                            Default: 7\n\t            verbose (bool): If True, prints a message for each validation loss improvement.\n", "                            Default: False\n\t            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n\t                            Default: 0\n\t        \"\"\"\n\t        self.patience = patience\n\t        self.verbose = verbose\n\t        self.counter = 0\n\t        self.best_score = None\n\t        self.early_stop = False\n\t        self.val_loss_min = np.Inf\n", "        self.delta = delta\n\t        self.save_path = save_path\n\t    def __call__(self, val_loss, model):\n\t        score = -val_loss\n\t        if self.best_score is None:\n\t            self.best_score = score\n\t            self.save_checkpoint(val_loss, model)\n\t        elif score < self.best_score - self.delta:\n\t            self.counter += 1\n\t            if self.verbose:\n", "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n\t            if self.counter >= self.patience:\n\t                self.early_stop = True\n\t        else:\n\t            self.best_score = score\n\t            self.save_checkpoint(val_loss, model)\n\t            self.counter = 0\n\t    def save_checkpoint(self, val_loss, model):\n\t        \"\"\"Saves model when validation loss decrease.\"\"\"\n\t        if self.verbose:\n", "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model to {self.save_path}...')\n\t        torch.save(model.state_dict(), self.save_path)\n\t        self.val_loss_min = val_loss\n"]}
{"filename": "NC/methods/SlotGAT/utils/__init__.py", "chunked_list": []}
{"filename": "NC/methods/SlotGAT/utils/tools.py", "chunked_list": ["import torch\n\timport dgl\n\timport numpy as np\n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n\tfrom sklearn.cluster import KMeans\n\tfrom sklearn.svm import LinearSVC\n\timport os\n\timport torch.nn.functional as F\n\timport torch.nn as nn\n", "import copy\n\timport json\n\timport pickle\n\tfrom matplotlib import pyplot as plt\n\tdef count_torch_tensor(t):\n\t    t=t.flatten(0).cpu()\n\t    c={}\n\t    for n in t:\n\t        n=n.item()\n\t        if n not in c:\n", "            c[n]=0\n\t        c[n]+=1\n\t    c=sorted(list(c.items()),key=lambda x:x[0])\n\t    return c\n\tdef strList(l):\n\t    return [str(x) for x in l]\n\tdef writeIntoCsvLogger(dictToWrite,file_name):\n\t    #read file\n\t    to_write_names=sorted(list(dictToWrite.keys()))\n\t    if not os.path.exists(file_name):\n", "        to_write_line=[]\n\t        for n in to_write_names:\n\t            to_write_line.append(dictToWrite[n])\n\t        with open(file_name,\"w\") as f:\n\t            f.write(  \",\".join(strList(to_write_names)) +\"\\n\")\n\t            f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n\t    else:\n\t        with open(file_name,\"r\") as f:\n\t            rec=[]\n\t            for line in f:\n", "                line=line.strip(\"\\n\").split(\",\")\n\t                rec.append(line)\n\t        #ensure they have same names\n\t        row_names=sorted(rec[0])\n\t        if to_write_names!=row_names:\n\t            collected_names_not_in=[]\n\t            for n in to_write_names:\n\t                if n not in row_names:\n\t                    for i,n_r in enumerate(rec):\n\t                        if i==0:\n", "                            rec[0].append(n)\n\t                        else:\n\t                            rec[i].append(\"\")\n\t                row_names.append(n)\n\t            for n_r in row_names:\n\t                if n_r not in to_write_names:\n\t                    dictToWrite[n_r]=\"\"\n\t                    to_write_names.append(n_r)\n\t            to_write_line=[]\n\t            for n in rec[0]:\n", "                to_write_line.append(dictToWrite[n])\n\t            rec.append(to_write_line)\n\t            with open(file_name,\"w\") as f:\n\t                for line_list in rec:\n\t                    f.write(  \",\".join(strList(line_list)) +\"\\n\")\n\t        else:\n\t            to_write_line=[]\n\t            for n in rec[0]:\n\t                to_write_line.append(dictToWrite[n])\n\t            with open(file_name,\"a\") as f:\n", "                f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n\t        re_order_csv(file_name)\n\tdef re_order_csv(file_name):\n\t    with open(file_name,\"r\") as f:\n\t        rec=[]\n\t        for line in f:\n\t            line=line.strip(\"\\n\").split(\",\")\n\t            rec.append(line)\n\t    row_names=sorted(enumerate(rec[0]),key=lambda x:x[1])\n\t    row_names_idx=[i[0] for i in row_names]    \n", "    row_names_=[i[1] for i in row_names]    \n\t    if row_names_idx==sorted(row_names_idx):\n\t        print(\"No need to reorder\")\n\t        return None\n\t    else:\n\t        print(\"reordering\")\n\t        with open(file_name,\"w\") as f:\n\t            for line_list in rec:\n\t                to_write_line=[ line_list[row_names_idx[i]]  for i in range(len(line_list))  ]\n\t                f.write(  \",\".join(to_write_line) +\"\\n\")\n", "def func_args_parse(*args,**kargs):\n\t    return args,kargs\n\tclass blank_profile():\n\t    def __init__(self,*args,**kwargs):\n\t        pass\n\t    def __enter__(self,*args,**kwargs):\n\t        return self\n\t    def __exit__(self,*args,**kwargs):\n\t        pass\n\t    def step(self):\n", "        pass\n\tclass vis_data_collector():\n\t    #all data must be simple python objects like int or 'str'\n\t    def __init__(self):\n\t        self.data_dict={}\n\t        self.tensor_dict={}\n\t        #formatting:\n\t        #\n\t        # {\"meta\":{****parameters and study name},\"re-1\":{\"epoch-0\":{\"loss\":0.1,\"w1\":1},\"epoch-1\":{\"loss\":0.2,\"w1\":2},...}}\n\t    def save_meta(self,meta_data,meta_name):\n", "        self.data_dict[\"meta\"]={meta_name:meta_data}\n\t        self.data_dict[\"meta\"][\"tensor_names\"]=[]\n\t    def collect_in_training(self,data,name,re,epoch,r=4):\n\t        if f\"re-{re}\" not in self.data_dict.keys():\n\t            self.data_dict[f\"re-{re}\" ]={}\n\t        if f\"epoch-{epoch}\" not in self.data_dict[f\"re-{re}\" ].keys():\n\t            self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"]={}\n\t        if type(data)==float:\n\t            data=round(data,r)\n\t        self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"][name]=data\n", "    def collect_in_run(self,data,name,re,r=4):\n\t        if f\"re-{re}\" not in self.data_dict.keys():\n\t            self.data_dict[f\"re-{re}\" ]={}\n\t        if type(data)==float:\n\t            data=round(data,r)\n\t        self.data_dict[f\"re-{re}\" ][name]=data\n\t    def collect_whole_process(self,data,name):\n\t        self.data_dict[name]=data\n\t    def collect_whole_process_tensor(self,data,name):\n\t        self.tensor_dict[name]=data\n", "        self.data_dict[\"meta\"][\"tensor_names\"].append(name)\n\t    def save(self,fn):\n\t        f = open(fn+\".json\", 'w')\n\t        json.dump(self.data_dict, f, indent=4)\n\t        f.close()\n\t        for k,v in self.tensor_dict.items():\n\t            torch.save(v,fn+\"_\"+k+\".pt\")\n\t    def load(self,fn):\n\t        f = open(fn, 'r')\n\t        self.data_dict= json.load(f)\n", "        f.close()\n\t        for name in self.data_dict[\"meta\"][\"tensor_names\"]:\n\t            self.tensor_dict[name]=torch.load(name+\".pt\")\n\t    def trans_to_numpy(self,name,epoch_range=None):\n\t        data=[]\n\t        re=0\n\t        while f\"re-{re}\" in self.data_dict.keys():\n\t            data.append([])\n\t            for i in range(epoch_range):\n\t                data[re].append(self.data_dict[f\"re-{re}\"][f\"epoch-{i}\"][name])\n", "            re+=1\n\t        data=np.array(data)\n\t        return np.mean(data,axis=0),np.std(data,axis=0)\n\t    def visualize_tsne(self,dn,node_idx_by_ntype):\n\t        from matplotlib.pyplot import figure\n\t        figure(figsize=(16, 9), dpi=80)\n\t        ncs=[\"r\",\"b\",\"y\",\"g\",\"chocolate\",\"deeppink\"]\n\t        print(dn)\n\t        layers=[]\n\t        heads=[]\n", "        ets=[]\n\t        #\"tsne_emb_layer_0_slot_0\"\n\t        for k,v in self.data_dict.items():\n\t            if \"tsne_emb_layer\" in k:\n\t                temp=k.split(\"_\")\n\t                if int(temp[3]) not in layers:\n\t                    layers.append(int(temp[3]))\n\t                if temp[4]==\"slot\":\n\t                    if int(temp[5]) not in ets:\n\t                        ets.append(int(temp[5]))\n", "        layers,heads,ets=sorted(layers),sorted(heads),sorted(ets)\n\t        print(layers,heads,ets)\n\t        #heads plot\n\t        for layer in layers:\n\t            fig,ax=plt.subplots(2,int((len(node_idx_by_ntype)+1)/2))\n\t            fig.set_size_inches(16,9)\n\t            fig.set_dpi(100)\n\t            nts=list(range(len(node_idx_by_ntype)))\n\t            for nt in nts:\n\t                subax=ax[int((nt)/(len(nts)/2))][int((nt)%(len(nts)/2))]\n", "                subax.cla()\n\t                datas=np.array(self.data_dict[f\"tsne_emb_layer_{layer}_slot_{nt}\"]).T\n\t                print(datas.shape)\n\t                for nt_j in nts:\n\t                    x=datas[0][ node_idx_by_ntype[nt_j]]\n\t                    y=datas[1][ node_idx_by_ntype[nt_j]]\n\t                    subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1,label=f\"type {nt_j} with center ({np.mean(x):.2f},{np.mean(y):.2f}) and radius {(np.std(x)+np.std(y))/2:.2f}\")\n\t                subax.set_xlim(-100,100)\n\t                subax.set_ylim(-100,100)\n\t                subax.set_title(f\"layer_{layer}_slot_{nt}\")\n", "                plt.title(f\"layer_{layer}_slot_{nt}\")\n\t                lgnd=subax.legend()\n\t                for lh in lgnd.legendHandles:\n\t                    lh._sizes=[10]\n\t            fig.suptitle(f\"embedding_tsne_layer_{layer}\")\n\t            plt.savefig(os.path.join(dn,f\"slot_embeddings_layer_{layer}.png\"))\n\tdef single_feat_net(net,*args,**kargs):\n\t    return net(*args,**kargs)\n\tdef idx_to_one_hot(idx_arr):\n\t    one_hot = np.zeros((idx_arr.shape[0], idx_arr.max() + 1))\n", "    one_hot[np.arange(idx_arr.shape[0]), idx_arr] = 1\n\t    return one_hot\n\tdef kmeans_test(X, y, n_clusters, repeat=10):\n\t    nmi_list = []\n\t    ari_list = []\n\t    for _ in range(repeat):\n\t        kmeans = KMeans(n_clusters=n_clusters)\n\t        y_pred = kmeans.fit_predict(X)\n\t        nmi_score = normalized_mutual_info_score(y, y_pred, average_method='arithmetic')\n\t        ari_score = adjusted_rand_score(y, y_pred)\n", "        nmi_list.append(nmi_score)\n\t        ari_list.append(ari_score)\n\t    return np.mean(nmi_list), np.std(nmi_list), np.mean(ari_list), np.std(ari_list)\n\tdef svm_test(X, y, test_sizes=(0.2, 0.4, 0.6, 0.8), repeat=10):\n\t    random_states = [182318 + i for i in range(repeat)]\n\t    result_macro_f1_list = []\n\t    result_micro_f1_list = []\n\t    for test_size in test_sizes:\n\t        macro_f1_list = []\n\t        micro_f1_list = []\n", "        for i in range(repeat):\n\t            X_train, X_test, y_train, y_test = train_test_split(\n\t                X, y, test_size=test_size, shuffle=True, random_state=random_states[i])\n\t            svm = LinearSVC(dual=False)\n\t            svm.fit(X_train, y_train)\n\t            y_pred = svm.predict(X_test)\n\t            macro_f1 = f1_score(y_test, y_pred, average='macro')\n\t            micro_f1 = f1_score(y_test, y_pred, average='micro')\n\t            macro_f1_list.append(macro_f1)\n\t            micro_f1_list.append(micro_f1)\n", "        result_macro_f1_list.append((np.mean(macro_f1_list), np.std(macro_f1_list)))\n\t        result_micro_f1_list.append((np.mean(micro_f1_list), np.std(micro_f1_list)))\n\t    return result_macro_f1_list, result_micro_f1_list\n\tdef evaluate_results_nc(embeddings, labels, num_classes):\n\t    print('SVM test')\n\t    svm_macro_f1_list, svm_micro_f1_list = svm_test(embeddings, labels)\n\t    print('Macro-F1: ' + ', '.join(['{:.6f}~{:.6f} ({:.1f})'.format(macro_f1_mean, macro_f1_std, train_size) for\n\t                                    (macro_f1_mean, macro_f1_std), train_size in\n\t                                    zip(svm_macro_f1_list, [0.8, 0.6, 0.4, 0.2])]))\n\t    print('Micro-F1: ' + ', '.join(['{:.6f}~{:.6f} ({:.1f})'.format(micro_f1_mean, micro_f1_std, train_size) for\n", "                                    (micro_f1_mean, micro_f1_std), train_size in\n\t                                    zip(svm_micro_f1_list, [0.8, 0.6, 0.4, 0.2])]))\n\t    print('K-means test')\n\t    nmi_mean, nmi_std, ari_mean, ari_std = kmeans_test(embeddings, labels, num_classes)\n\t    print('NMI: {:.6f}~{:.6f}'.format(nmi_mean, nmi_std))\n\t    print('ARI: {:.6f}~{:.6f}'.format(ari_mean, ari_std))\n\t    return svm_macro_f1_list, svm_micro_f1_list, nmi_mean, nmi_std, ari_mean, ari_std\n\tdef parse_adjlist(adjlist, edge_metapath_indices, samples=None):\n\t    edges = []\n\t    nodes = set()\n", "    result_indices = []\n\t    for row, indices in zip(adjlist, edge_metapath_indices):\n\t        row_parsed = list(map(int, row.split(' ')))\n\t        nodes.add(row_parsed[0])\n\t        if len(row_parsed) > 1:\n\t            # sampling neighbors\n\t            if samples is None:\n\t                neighbors = row_parsed[1:]\n\t                result_indices.append(indices)\n\t            else:\n", "                # undersampling frequent neighbors\n\t                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n\t                p = []\n\t                for count in counts:\n\t                    p += [(count ** (3 / 4)) / count] * count\n\t                p = np.array(p)\n\t                p = p / p.sum()\n\t                samples = min(samples, len(row_parsed) - 1)\n\t                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n\t                neighbors = [row_parsed[i + 1] for i in sampled_idx]\n", "                result_indices.append(indices[sampled_idx])\n\t        else:\n\t            neighbors = []\n\t            result_indices.append(indices)\n\t        for dst in neighbors:\n\t            nodes.add(dst)\n\t            edges.append((row_parsed[0], dst))\n\t    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n\t    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n\t    result_indices = np.vstack(result_indices)\n", "    return edges, result_indices, len(nodes), mapping\n\tdef parse_minibatch(adjlists, edge_metapath_indices_list, idx_batch, device, samples=None):\n\t    g_list = []\n\t    result_indices_list = []\n\t    idx_batch_mapped_list = []\n\t    for adjlist, indices in zip(adjlists, edge_metapath_indices_list):\n\t        edges, result_indices, num_nodes, mapping = parse_adjlist(\n\t            [adjlist[i] for i in idx_batch], [indices[i] for i in idx_batch], samples)\n\t        g = dgl.DGLGraph(multigraph=True)\n\t        g.add_nodes(num_nodes)\n", "        if len(edges) > 0:\n\t            sorted_index = sorted(range(len(edges)), key=lambda i : edges[i])\n\t            g.add_edges(*list(zip(*[(edges[i][1], edges[i][0]) for i in sorted_index])))\n\t            result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n\t        else:\n\t            result_indices = torch.LongTensor(result_indices).to(device)\n\t        #g.add_edges(*list(zip(*[(dst, src) for src, dst in sorted(edges)])))\n\t        #result_indices = torch.LongTensor(result_indices).to(device)\n\t        g_list.append(g)\n\t        result_indices_list.append(result_indices)\n", "        idx_batch_mapped_list.append(np.array([mapping[idx] for idx in idx_batch]))\n\t    return g_list, result_indices_list, idx_batch_mapped_list\n\tdef parse_adjlist_LastFM(adjlist, edge_metapath_indices, samples=None, exclude=None, offset=None, mode=None):\n\t    edges = []\n\t    nodes = set()\n\t    result_indices = []\n\t    for row, indices in zip(adjlist, edge_metapath_indices):\n\t        row_parsed = list(map(int, row.split(' ')))\n\t        nodes.add(row_parsed[0])\n\t        if len(row_parsed) > 1:\n", "            # sampling neighbors\n\t            if samples is None:\n\t                if exclude is not None:\n\t                    if mode == 0:\n\t                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[:, [0, 1, -1, -2]]]\n\t                    else:\n\t                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[:, [0, 1, -1, -2]]]\n\t                    neighbors = np.array(row_parsed[1:])[mask]\n\t                    result_indices.append(indices[mask])\n\t                else:\n", "                    neighbors = row_parsed[1:]\n\t                    result_indices.append(indices)\n\t            else:\n\t                # undersampling frequent neighbors\n\t                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n\t                p = []\n\t                for count in counts:\n\t                    p += [(count ** (3 / 4)) / count] * count\n\t                p = np.array(p)\n\t                p = p / p.sum()\n", "                samples = min(samples, len(row_parsed) - 1)\n\t                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n\t                if exclude is not None:\n\t                    if mode == 0:\n\t                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n\t                    else:\n\t                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n\t                    neighbors = np.array([row_parsed[i + 1] for i in sampled_idx])[mask]\n\t                    result_indices.append(indices[sampled_idx][mask])\n\t                else:\n", "                    neighbors = [row_parsed[i + 1] for i in sampled_idx]\n\t                    result_indices.append(indices[sampled_idx])\n\t        else:\n\t            neighbors = [row_parsed[0]]\n\t            indices = np.array([[row_parsed[0]] * indices.shape[1]])\n\t            if mode == 1:\n\t                indices += offset\n\t            result_indices.append(indices)\n\t        for dst in neighbors:\n\t            nodes.add(dst)\n", "            edges.append((row_parsed[0], dst))\n\t    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n\t    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n\t    result_indices = np.vstack(result_indices)\n\t    return edges, result_indices, len(nodes), mapping\n\tdef parse_minibatch_LastFM(adjlists_ua, edge_metapath_indices_list_ua, user_artist_batch, device, samples=None, use_masks=None, offset=None):\n\t    g_lists = [[], []]\n\t    result_indices_lists = [[], []]\n\t    idx_batch_mapped_lists = [[], []]\n\t    for mode, (adjlists, edge_metapath_indices_list) in enumerate(zip(adjlists_ua, edge_metapath_indices_list_ua)):\n", "        for adjlist, indices, use_mask in zip(adjlists, edge_metapath_indices_list, use_masks[mode]):\n\t            if use_mask:\n\t                edges, result_indices, num_nodes, mapping = parse_adjlist_LastFM(\n\t                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, user_artist_batch, offset, mode)\n\t            else:\n\t                edges, result_indices, num_nodes, mapping = parse_adjlist_LastFM(\n\t                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, offset=offset, mode=mode)\n\t            g = dgl.DGLGraph(multigraph=True)\n\t            g.add_nodes(num_nodes)\n\t            if len(edges) > 0:\n", "                sorted_index = sorted(range(len(edges)), key=lambda i : edges[i])\n\t                g.add_edges(*list(zip(*[(edges[i][1], edges[i][0]) for i in sorted_index])))\n\t                result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n\t            else:\n\t                result_indices = torch.LongTensor(result_indices).to(device)\n\t            g_lists[mode].append(g)\n\t            result_indices_lists[mode].append(result_indices)\n\t            idx_batch_mapped_lists[mode].append(np.array([mapping[row[mode]] for row in user_artist_batch]))\n\t    return g_lists, result_indices_lists, idx_batch_mapped_lists\n\tclass index_generator:\n", "    def __init__(self, batch_size, num_data=None, indices=None, shuffle=True):\n\t        if num_data is not None:\n\t            self.num_data = num_data\n\t            self.indices = np.arange(num_data)\n\t        if indices is not None:\n\t            self.num_data = len(indices)\n\t            self.indices = np.copy(indices)\n\t        self.batch_size = batch_size\n\t        self.iter_counter = 0\n\t        self.shuffle = shuffle\n", "        if shuffle:\n\t            np.random.shuffle(self.indices)\n\t    def next(self):\n\t        if self.num_iterations_left() <= 0:\n\t            self.reset()\n\t        self.iter_counter += 1\n\t        return np.copy(self.indices[(self.iter_counter - 1) * self.batch_size:self.iter_counter * self.batch_size])\n\t    def num_iterations(self):\n\t        return int(np.ceil(self.num_data / self.batch_size))\n\t    def num_iterations_left(self):\n", "        return self.num_iterations() - self.iter_counter\n\t    def reset(self):\n\t        if self.shuffle:\n\t            np.random.shuffle(self.indices)\n\t        self.iter_counter = 0\n"]}
{"filename": "NC/methods/SlotGAT/utils/data.py", "chunked_list": ["import networkx as nx\n\timport numpy as np\n\timport scipy\n\timport pickle\n\timport scipy.sparse as sp\n\tdef load_data(prefix='DBLP',multi_labels=False ):\n\t    from scripts.data_loader import data_loader\n\t    dl = data_loader('../../data/'+prefix)\n\t    features = []\n\t    for i in range(len(dl.nodes['count'])):\n", "        th = dl.nodes['attr'][i]\n\t        if th is None:\n\t            features.append(sp.eye(dl.nodes['count'][i]))\n\t        else:\n\t            features.append(th)\n\t    adjM = sum(dl.links['data'].values())\n\t    labels = np.zeros((dl.nodes['count'][0], dl.labels_train['num_classes']), dtype=int)\n\t    val_ratio = 0.2\n\t    train_idx = np.nonzero(dl.labels_train['mask'])[0]\n\t    np.random.shuffle(train_idx)\n", "    split = int(train_idx.shape[0]*val_ratio)\n\t    val_idx = train_idx[:split]\n\t    train_idx = train_idx[split:]\n\t    train_idx = np.sort(train_idx)\n\t    val_idx = np.sort(val_idx)\n\t    test_idx = np.nonzero(dl.labels_test['mask'])[0]\n\t    labels[train_idx] = dl.labels_train['data'][train_idx]\n\t    labels[val_idx] = dl.labels_train['data'][val_idx]\n\t    #if prefix != 'IMDB':\n\t    if not multi_labels:\n", "        labels = labels.argmax(axis=1)\n\t    train_val_test_idx = {}\n\t    train_val_test_idx['train_idx'] = train_idx\n\t    train_val_test_idx['val_idx'] = val_idx\n\t    train_val_test_idx['test_idx'] = test_idx\n\t    return features,\\\n\t           adjM, \\\n\t           labels,\\\n\t           train_val_test_idx,\\\n\t            dl\n"]}
{"filename": "NC/methods/SlotGAT/utils/preprocess.py", "chunked_list": ["import numpy as np\n\timport scipy.sparse\n\timport networkx as nx\n\tdef get_metapath_adjacency_matrix(adjM, type_mask, metapath):\n\t    \"\"\"\n\t    :param M: the raw adjacency matrix\n\t    :param type_mask: an array of types of all node\n\t    :param metapath\n\t    :return: a list of metapath-based adjacency matrices\n\t    \"\"\"\n", "    out_adjM = scipy.sparse.csr_matrix(adjM[np.ix_(type_mask == metapath[0], type_mask == metapath[1])])\n\t    for i in range(1, len(metapath) - 1):\n\t        out_adjM = out_adjM.dot(scipy.sparse.csr_matrix(adjM[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])]))\n\t    return out_adjM.toarray()\n\t# networkx.has_path may search too\n\tdef get_metapath_neighbor_pairs(M, type_mask, expected_metapaths):\n\t    \"\"\"\n\t    :param M: the raw adjacency matrix\n\t    :param type_mask: an array of types of all node\n\t    :param expected_metapaths: a list of expected metapaths\n", "    :return: a list of python dictionaries, consisting of metapath-based neighbor pairs and intermediate paths\n\t    \"\"\"\n\t    outs = []\n\t    for metapath in expected_metapaths:\n\t        # consider only the edges relevant to the expected metapath\n\t        mask = np.zeros(M.shape, dtype=bool)\n\t        for i in range((len(metapath) - 1) // 2):\n\t            temp = np.zeros(M.shape, dtype=bool)\n\t            temp[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])] = True\n\t            temp[np.ix_(type_mask == metapath[i + 1], type_mask == metapath[i])] = True\n", "            mask = np.logical_or(mask, temp)\n\t        partial_g_nx = nx.from_numpy_matrix((M * mask).astype(int))\n\t        # only need to consider the former half of the metapath\n\t        # e.g., we only need to consider 0-1-2 for the metapath 0-1-2-1-0\n\t        metapath_to_target = {}\n\t        for source in (type_mask == metapath[0]).nonzero()[0]:\n\t            for target in (type_mask == metapath[(len(metapath) - 1) // 2]).nonzero()[0]:\n\t                # check if there is a possible valid path from source to target node\n\t                has_path = False\n\t                single_source_paths = nx.single_source_shortest_path(\n", "                    partial_g_nx, source, cutoff=(len(metapath) + 1) // 2 - 1)\n\t                if target in single_source_paths:\n\t                    has_path = True\n\t                #if nx.has_path(partial_g_nx, source, target):\n\t                if has_path:\n\t                    shortests = [p for p in nx.all_shortest_paths(partial_g_nx, source, target) if\n\t                                 len(p) == (len(metapath) + 1) // 2]\n\t                    if len(shortests) > 0:\n\t                        metapath_to_target[target] = metapath_to_target.get(target, []) + shortests\n\t        metapath_neighbor_paris = {}\n", "        for key, value in metapath_to_target.items():\n\t            for p1 in value:\n\t                for p2 in value:\n\t                    metapath_neighbor_paris[(p1[0], p2[0])] = metapath_neighbor_paris.get((p1[0], p2[0]), []) + [\n\t                        p1 + p2[-2::-1]]\n\t        outs.append(metapath_neighbor_paris)\n\t    return outs\n\tdef get_networkx_graph(neighbor_pairs, type_mask, ctr_ntype):\n\t    indices = np.where(type_mask == ctr_ntype)[0]\n\t    idx_mapping = {}\n", "    for i, idx in enumerate(indices):\n\t        idx_mapping[idx] = i\n\t    G_list = []\n\t    for metapaths in neighbor_pairs:\n\t        edge_count = 0\n\t        sorted_metapaths = sorted(metapaths.items())\n\t        G = nx.MultiDiGraph()\n\t        G.add_nodes_from(range(len(indices)))\n\t        for (src, dst), paths in sorted_metapaths:\n\t            for _ in range(len(paths)):\n", "                G.add_edge(idx_mapping[src], idx_mapping[dst])\n\t                edge_count += 1\n\t        G_list.append(G)\n\t    return G_list\n\tdef get_edge_metapath_idx_array(neighbor_pairs):\n\t    all_edge_metapath_idx_array = []\n\t    for metapath_neighbor_pairs in neighbor_pairs:\n\t        sorted_metapath_neighbor_pairs = sorted(metapath_neighbor_pairs.items())\n\t        edge_metapath_idx_array = []\n\t        for _, paths in sorted_metapath_neighbor_pairs:\n", "            edge_metapath_idx_array.extend(paths)\n\t        edge_metapath_idx_array = np.array(edge_metapath_idx_array, dtype=int)\n\t        all_edge_metapath_idx_array.append(edge_metapath_idx_array)\n\t        print(edge_metapath_idx_array.shape)\n\t    return all_edge_metapath_idx_array\n"]}
{"filename": "NC/methods/SlotGAT/analysis/get_good_tsnes.py", "chunked_list": ["from matplotlib import pyplot as plt\n\timport numpy as np\n\timport os\n\timport json\n\timport sys\n\timport argparse\n\timport time\n\timport math\n\timport random\n\timport torch\n", "import torch.nn as nn\n\timport matplotlib as mpl\n\tplt.rcParams[\"font.family\"] = \"Times New Roman\"\n\tmpl.rcParams['xtick.labelsize']=0\n\tmpl.rcParams['ytick.labelsize']=0\n\tabcd=\"abcdefghijklmnopqrstuvwxyz\"\n\tclass vis_data_collector():\n\t    #all data must be simple python objects like int or 'str'\n\t    def __init__(self):\n\t        self.data_dict={}\n", "        self.tensor_dict={}\n\t        #formatting:\n\t        #\n\t        # {\"meta\":{****parameters and study name},\"re-1\":{\"epoch-0\":{\"loss\":0.1,\"w1\":1},\"epoch-1\":{\"loss\":0.2,\"w1\":2},...}}\n\t    def save_meta(self,meta_data,meta_name):\n\t        self.data_dict[\"meta\"]={meta_name:meta_data}\n\t        self.data_dict[\"meta\"][\"tensor_names\"]=[]\n\t    def collect_in_training(self,data,name,re,epoch,r=4):\n\t        if f\"re-{re}\" not in self.data_dict.keys():\n\t            self.data_dict[f\"re-{re}\" ]={}\n", "        if f\"epoch-{epoch}\" not in self.data_dict[f\"re-{re}\" ].keys():\n\t            self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"]={}\n\t        if type(data)==float:\n\t            data=round(data,r)\n\t        self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"][name]=data\n\t    def collect_in_run(self,data,name,re,r=4):\n\t        if f\"re-{re}\" not in self.data_dict.keys():\n\t            self.data_dict[f\"re-{re}\" ]={}\n\t        if type(data)==float:\n\t            data=round(data,r)\n", "        self.data_dict[f\"re-{re}\" ][name]=data\n\t    def collect_whole_process(self,data,name):\n\t        self.data_dict[name]=data\n\t    def collect_whole_process_tensor(self,data,name):\n\t        self.tensor_dict[name]=data\n\t        self.data_dict[\"meta\"][\"tensor_names\"].append(name)\n\t    def save(self,fn):\n\t        f = open(fn+\".json\", 'w')\n\t        json.dump(self.data_dict, f, indent=4)\n\t        f.close()\n", "        for k,v in self.tensor_dict.items():\n\t            torch.save(v,fn+\"_\"+k+\".pt\")\n\t    def load(self,fn):\n\t        f = open(fn, 'r')\n\t        self.data_dict= json.load(f)\n\t        f.close()\n\t        if \"meta\" in self.data_dict.keys():\n\t            for name in self.data_dict[\"meta\"][\"tensor_names\"]:\n\t                self.tensor_dict[name]=torch.load(name+\".pt\")\n\t    def trans_to_numpy(self,name,epoch_range=None):\n", "        data=[]\n\t        re=0\n\t        while f\"re-{re}\" in self.data_dict.keys():\n\t            data.append([])\n\t            for i in range(epoch_range):\n\t                data[re].append(self.data_dict[f\"re-{re}\"][f\"epoch-{i}\"][name])\n\t            re+=1\n\t        data=np.array(data)\n\t        return np.mean(data,axis=0),np.std(data,axis=0)\n\t    def visualize_tsne(self,dn,node_idx_by_ntype,dataset):\n", "        from matplotlib.pyplot import figure\n\t        figure(figsize=(16, 12), dpi=300)\n\t        ncs=[\"r\",\"b\",\"y\",\"g\",\"chocolate\",\"deeppink\"]\n\t        print(dn)\n\t        layers=[]\n\t        heads=[]\n\t        ets=[]\n\t        #\"tsne_emb_layer_0_slot_0\"\n\t        for k,v in self.data_dict.items():\n\t            if \"tsne_emb_layer\" in k:\n", "                temp=k.split(\"_\")\n\t                if int(temp[3]) not in layers:\n\t                    layers.append(int(temp[3]))\n\t                if temp[4]==\"slot\":\n\t                    if int(temp[5]) not in ets:\n\t                        ets.append(int(temp[5]))\n\t        layers,heads,ets=sorted(layers),sorted(heads),sorted(ets)\n\t        print(layers,heads,ets)\n\t        #heads plot\n\t        for layer in layers:\n", "            fig,ax=plt.subplots(2,int((len(node_idx_by_ntype)+1)/2))\n\t            fig.set_size_inches(16,12)\n\t            fig.set_dpi(300)\n\t            nts=list(range(len(node_idx_by_ntype)))\n\t            for nt in nts:\n\t                subax=ax[int((nt)/(len(nts)/2))][int((nt)%(len(nts)/2))]\n\t                subax.cla()\n\t                datas=np.array(self.data_dict[f\"tsne_emb_layer_{layer}_slot_{nt}\"]).T\n\t                print(datas.shape)\n\t                x_lower,x_upper=0,0\n", "                y_lower,y_upper=0,0\n\t                for nt_j in nts:\n\t                    x=datas[0][ node_idx_by_ntype[nt_j]]\n\t                    y=datas[1][ node_idx_by_ntype[nt_j]]\n\t                   # subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1,label=f\"type {nt_j} with center ({np.mean(x):.2f},{np.mean(y):.2f}) and radius {(np.std(x)+np.std(y))/2:.2f}\")\n\t                    #if nt==1:\n\t                    subax.scatter(x=x,y=y,c=ncs[nt_j],s=3,label=f\"Type {nt_j}\")\n\t                    #else:\n\t                    #    subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1)\n\t                    #set 90% pencentile of lim\n", "                    xs=sorted(x)\n\t                    ys=sorted(y)\n\t                    x_lower,x_upper=min(xs[int(len(xs)*0.05)],x_lower),max(xs[int(len(xs)*0.95)],x_upper)\n\t                    y_lower,y_upper=min(ys[int(len(ys)*0.05)],y_lower),max(ys[int(len(ys)*0.95)],y_upper)\n\t                #subax.set_xticks(fontsize=12)    \n\t                #subax.set_yticks(fontsize=12)    \n\t                subax.set_xlim(x_lower,x_upper)\n\t                subax.set_ylim(y_lower,y_upper)\n\t                subax.tick_params(direction='in', length=0, width=0)\n\t                #subax.set_xlim(-200,200)\n", "                #subax.set_ylim(-200,200)\n\t                #subax.set_title(f\" Layer {layer} Slot {nt}\")\n\t                subax.set_xlabel(f\"({abcd[nt]}) Slot {nt}\",fontsize=55)\n\t                #plt.title(f\"Layer {layer} Slot {nt}\")\n\t                #if  nt==1:\n\t                    #lgnd=subax.legend(loc=\"lower right\", bbox_to_anchor=(1.3, 0.6))\n\t                    #for lh in lgnd.legendHandles:\n\t                        #lh._sizes=[16]\n\t            #fig.legend()\n\t            handles, labels = subax.get_legend_handles_labels()\n", "            lgnd=fig.legend(handles, labels, loc='right',fontsize=50,bbox_to_anchor=(1.09, 1.04), ncol=4,handletextpad=0.01)\n\t            #lgnd=fig.legend(bbox_to_anchor=(1.3, 0.6))\n\t            #lgnd=fig.legend()\n\t            for lh in lgnd.legendHandles:\n\t                lh._sizes=[100]\n\t            fig.tight_layout() \n\t            #fig.suptitle(f\"Tsne Embedding of Layer {layer}\")\n\t            plt.savefig(os.path.join(dn,f\"{dataset}_slot_embeddings_layer_{layer}.png\"),bbox_inches=\"tight\")\n\tfile_names=[] # to specify!!!\n\tfor fn,dataset_info_fn in file_names:\n", "    vis=vis_data_collector()\n\t    vis_dataset_info=vis_data_collector()\n\t    vis_dataset_info.load(dataset_info_fn)\n\t    node_idx_by_ntype=vis_dataset_info.data_dict[\"node_idx_by_ntype\"]\n\t    vis.load(fn)\n\t    vis.visualize_tsne(os.path.dirname(fn),node_idx_by_ntype,dataset=dataset_info_fn.split(\"_\")[2])\n"]}
{"filename": "NC/scripts/NC_F1.py", "chunked_list": ["import json\n\tfrom collections import Counter\n\tfrom sklearn.metrics import f1_score\n\timport numpy as np\n\tfrom sklearn.preprocessing import MultiLabelBinarizer\n\timport os\n\timport sys\n\tclass F1:\n\t    def __init__(self, data_name, pred_files, label_classes, args):\n\t        self.label_classes = label_classes\n", "        self.F1_list = {'macro': [], 'micro': []}\n\t        if len(pred_files)==0:\n\t            self.F1_mean = {'macro_mean': 0, 'micro_mean': 0}\n\t            self.F1_std = {'macro_std': 0, 'micro_std': 0}\n\t        else:\n\t            true_file = os.path.join(args.ground_dir, data_name, 'label.dat.test')\n\t            self.ture_label = self.load_labels(true_file)\n\t            for pred_file in pred_files:\n\t                pred_label = self.load_labels(pred_file)\n\t                ans = self.evaluate_F1(pred_label)\n", "                self.F1_list['macro'].append(ans['macro'])\n\t                self.F1_list['micro'].append(ans['micro'])\n\t            self.F1_mean = {'macro_mean':np.mean(self.F1_list['macro']), 'micro_mean':np.mean(self.F1_list['micro'])}\n\t            self.F1_std = {'macro_std': np.std(self.F1_list['macro']), 'micro_std': np.std(self.F1_list['micro'])}\n\t    def load_labels(self, name):\n\t        \"\"\"\n\t        return labels dict\n\t            num_classes: total number of labels\n\t            total: total number of labeled data\n\t            count: number of labeled data for each node type\n", "            data: a numpy matrix with shape (self.nodes['total'], self.labels['num_classes'])\n\t        \"\"\"\n\t        labels = {'num_classes': 0, 'total': 0, 'count': Counter(), 'data': {}}\n\t        nc = 0\n\t        with open(name, 'r', encoding='utf-8') as f:\n\t            for line in f:\n\t                th = line.split('\\t')\n\t                node_id, node_name, node_type, node_label = int(th[0]), th[1], int(th[2]), th[3]\n\t                if node_label.strip()==\"\":\n\t                    node_label = []\n", "                else:\n\t                    node_label = list(map(float, node_label.rstrip().split(',')))\n\t                for i in range(len(node_label)):\n\t                    node_label[i]=int(node_label[i])\n\t                    nc = max(nc, node_label[i]+1)\n\t                labels['data'][node_id] = node_label\n\t                labels['count'][node_type] += 1\n\t                labels['total'] += 1\n\t        labels['num_classes'] = nc\n\t        return labels\n", "    def evaluate_F1(self,pred_label):\n\t        y_true, y_pred =[], []\n\t        ans={'macro':0, 'micro':0}\n\t        for k in self.ture_label['data'].keys():\n\t            y_true.append(self.ture_label['data'][k])\n\t            if k not in pred_label['data']:\n\t                return ans\n\t            y_pred.append(pred_label['data'][k])\n\t        if self.label_classes>2:\n\t            mlp = MultiLabelBinarizer([i for i in range(self.label_classes)])\n", "            y_pred = mlp.fit_transform(y_pred)\n\t            y_true = mlp.fit_transform(y_true)\n\t        ans['macro'] = f1_score(y_true, y_pred, average='macro')\n\t        ans['micro'] = f1_score(y_true, y_pred, average='micro')\n\t        return ans\n\timport argparse\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser(description=\"Evaluate F1 for NC result.\")\n\t    parser.add_argument('--pred_zip', type=str, default=\"nc.zip\",\n\t                        help='Compressed pred files.')\n", "    parser.add_argument('--ground_dir', type=str, default=\"../data\",\n\t                        help='Dir of ground files.')\n\t    parser.add_argument('--log', type=str, default=\"nc.log\",\n\t                        help='output file')\n\t    return parser.parse_args()\n\timport zipfile\n\tdef extract_zip(zip_path, extract_path):\n\t    zip = zipfile.ZipFile(zip_path, 'r')\n\t    zip.extractall(extract_path)\n\t    return zip.namelist()\n", "def write_log(log_file, log_msg):\n\t    with open(log_file, 'w') as log_handle:\n\t        log_handle.write(log_msg)\n\tdef delete_files(files_):\n\t    for f in files_:\n\t        if os.path.exists(f):\n\t            os.remove(f)\n\tif __name__ == '__main__':\n\t    # get argument settings.\n\t    args = parse_args()\n", "    zip_path = args.pred_zip\n\t    log_msg = ''\n\t    if not os.path.exists(zip_path):\n\t        log_msg = 'ERROR: No such zip file!'\n\t        write_log(args.log, log_msg)\n\t        sys.exit()\n\t    extract_path = 'nc'\n\t    extract_file_list = extract_zip(zip_path, extract_path)\n\t    extract_file_list = [os.path.join(extract_path, f_) for f_ in extract_file_list]\n\t    data_list = ['DBLP', 'IMDB', 'ACM', 'Freebase']\n", "    class_count = {'DBLP':2, 'IMDB':5,'ACM':2,'Freebase':2}\n\t    res={}\n\t    detect_data_files = []\n\t    for data_name in data_list:\n\t        pred_files = []\n\t        for i in range(1,6):\n\t            file_name = os.path.join(extract_path, f'{data_name}_{i}.txt')\n\t            if not os.path.exists(file_name):\n\t                continue\n\t            pred_files.append(file_name)\n", "            detect_data_files.append(file_name)\n\t        if len(pred_files)>0 and len(pred_files)!=5:\n\t            log_msg = f'ERROR: Please check the size of {data_name} dataset!'\n\t            write_log(args.log, log_msg)\n\t            delete_files(extract_file_list)\n\t            sys.exit()\n\t        res[data_name] = F1(data_name,pred_files,class_count[data_name],args)\n\t    if len(detect_data_files) == 0:\n\t        log_msg = f'ERROR: No file detected, please confirm that ' \\\n\t                  f'the data file is in the top directory of the compressed package!'\n", "        write_log(args.log, log_msg)\n\t        sys.exit()\n\t    delete_files(extract_file_list)\n\t    hgb_score_list = []\n\t    for data_name in data_list:\n\t        hgb_score_list.append(res[data_name].F1_mean['macro_mean'])\n\t        hgb_score_list.append(res[data_name].F1_mean['micro_mean'])\n\t    hgb_score = np.mean(hgb_score_list)\n\t    detail_json = {}\n\t    log_msg = f'{hgb_score}###'\n", "    for data_name in data_list:\n\t        detail_json[data_name] = {}\n\t        detail_json[data_name][\"F1 mean\"] = res[data_name].F1_mean\n\t        detail_json[data_name][\"F1 std\"] = res[data_name].F1_std\n\t    log_msg += json.dumps(detail_json)\n\t    write_log(args.log, log_msg)\n\t    sys.exit()\n"]}
{"filename": "NC/scripts/__init__.py", "chunked_list": []}
{"filename": "NC/scripts/data_loader.py", "chunked_list": ["import os\n\timport numpy as np\n\timport scipy.sparse as sp\n\tfrom collections import Counter, defaultdict\n\tfrom sklearn.metrics import f1_score,multilabel_confusion_matrix,accuracy_score,auc,precision_score,recall_score\n\timport time\n\timport copy\n\tclass bcolors:\n\t    HEADER = '\\033[95m'\n\t    OKBLUE = '\\033[94m'\n", "    OKCYAN = '\\033[96m'\n\t    OKGREEN = '\\033[92m'\n\t    WARNING = '\\033[93m'\n\t    FAIL = '\\033[91m'\n\t    ENDC = '\\033[0m'\n\t    BOLD = '\\033[1m'\n\t    UNDERLINE = '\\033[4m'\n\tclass data_loader:\n\t    def __init__(self, path ): \n\t        self.path = path\n", "        self.nodes = self.load_nodes()\n\t        self.links = self.load_links()\n\t        self.labels_train = self.load_labels('label.dat')\n\t        self.labels_test = self.load_labels('label.dat.test')\n\t    def get_sub_graph(self, node_types_tokeep):\n\t        \"\"\"\n\t        node_types_tokeep is a list or set of node types that you want to keep in the sub-graph\n\t        We only support whole type sub-graph for now.\n\t        This is an in-place update function!\n\t        return: old node type id to new node type id dict, old edge type id to new edge type id dict\n", "        \"\"\"\n\t        keep = set(node_types_tokeep)\n\t        new_node_type = 0\n\t        new_node_id = 0\n\t        new_nodes = {'total':0, 'count':Counter(), 'attr':{}, 'shift':{}}\n\t        new_links = {'total':0, 'count':Counter(), 'meta':{}, 'data':defaultdict(list)}\n\t        new_labels_train = {'num_classes':0, 'total':0, 'count':Counter(), 'data':None, 'mask':None}\n\t        new_labels_test = {'num_classes':0, 'total':0, 'count':Counter(), 'data':None, 'mask':None}\n\t        old_nt2new_nt = {}\n\t        old_idx = []\n", "        for node_type in self.nodes['count']:\n\t            if node_type in keep:\n\t                nt = node_type\n\t                nnt = new_node_type\n\t                old_nt2new_nt[nt] = nnt\n\t                cnt = self.nodes['count'][nt]\n\t                new_nodes['total'] += cnt\n\t                new_nodes['count'][nnt] = cnt\n\t                new_nodes['attr'][nnt] = self.nodes['attr'][nt]\n\t                new_nodes['shift'][nnt] = new_node_id\n", "                beg = self.nodes['shift'][nt]\n\t                old_idx.extend(range(beg, beg+cnt))\n\t                cnt_label_train = self.labels_train['count'][nt]\n\t                new_labels_train['count'][nnt] = cnt_label_train\n\t                new_labels_train['total'] += cnt_label_train\n\t                cnt_label_test = self.labels_test['count'][nt]\n\t                new_labels_test['count'][nnt] = cnt_label_test\n\t                new_labels_test['total'] += cnt_label_test\n\t                new_node_type += 1\n\t                new_node_id += cnt\n", "        new_labels_train['num_classes'] = self.labels_train['num_classes']\n\t        new_labels_test['num_classes'] = self.labels_test['num_classes']\n\t        for k in ['data', 'mask']:\n\t            new_labels_train[k] = self.labels_train[k][old_idx]\n\t            new_labels_test[k] = self.labels_test[k][old_idx]\n\t        old_et2new_et = {}\n\t        new_edge_type = 0\n\t        for edge_type in self.links['count']:\n\t            h, t = self.links['meta'][edge_type]\n\t            if h in keep and t in keep:\n", "                et = edge_type\n\t                net = new_edge_type\n\t                old_et2new_et[et] = net\n\t                new_links['total'] += self.links['count'][et]\n\t                new_links['count'][net] = self.links['count'][et]\n\t                new_links['meta'][net] = tuple(map(lambda x:old_nt2new_nt[x], self.links['meta'][et]))\n\t                new_links['data'][net] = self.links['data'][et][old_idx][:, old_idx]\n\t                new_edge_type += 1\n\t        self.nodes = new_nodes\n\t        self.links = new_links\n", "        self.labels_train = new_labels_train\n\t        self.labels_test = new_labels_test\n\t        return old_nt2new_nt, old_et2new_et\n\t    def get_meta_path(self, meta=[]):\n\t        \"\"\"\n\t        Get meta path matrix\n\t            meta is a list of edge types (also can be denoted by a pair of node types)\n\t            return a sparse matrix with shape [node_num, node_num]\n\t        \"\"\"\n\t        ini = sp.eye(self.nodes['total'])\n", "        meta = [self.get_edge_type(x) for x in meta]\n\t        for x in meta:\n\t            ini = ini.dot(self.links['data'][x]) if x >= 0 else ini.dot(self.links['data'][-x - 1].T)\n\t        return ini\n\t    def dfs(self, now, meta, meta_dict):\n\t        if len(meta) == 0:\n\t            meta_dict[now[0]].append(now)\n\t            return\n\t        th_mat = self.links['data'][meta[0]] if meta[0] >= 0 else self.links['data'][-meta[0] - 1].T\n\t        th_node = now[-1]\n", "        for col in th_mat[th_node].nonzero()[1]:\n\t            self.dfs(now+[col], meta[1:], meta_dict)\n\t    def get_full_meta_path(self, meta=[], symmetric=False):\n\t        \"\"\"\n\t        Get full meta path for each node\n\t            meta is a list of edge types (also can be denoted by a pair of node types)\n\t            return a dict of list[list] (key is node_id)\n\t        \"\"\"\n\t        meta = [self.get_edge_type(x) for x in meta]\n\t        if len(meta) == 1:\n", "            meta_dict = {}\n\t            start_node_type = self.links['meta'][meta[0]][0] if meta[0]>=0 else self.links['meta'][-meta[0]-1][1]\n\t            for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n\t                meta_dict[i] = []\n\t                self.dfs([i], meta, meta_dict)\n\t        else:\n\t            meta_dict1 = {}\n\t            meta_dict2 = {}\n\t            mid = len(meta) // 2\n\t            meta1 = meta[:mid]\n", "            meta2 = meta[mid:]\n\t            start_node_type = self.links['meta'][meta1[0]][0] if meta1[0]>=0 else self.links['meta'][-meta1[0]-1][1]\n\t            for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n\t                meta_dict1[i] = []\n\t                self.dfs([i], meta1, meta_dict1)\n\t            start_node_type = self.links['meta'][meta2[0]][0] if meta2[0]>=0 else self.links['meta'][-meta2[0]-1][1]\n\t            for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n\t                meta_dict2[i] = []\n\t            if symmetric:\n\t                for k in meta_dict1:\n", "                    paths = meta_dict1[k]\n\t                    for x in paths:\n\t                        meta_dict2[x[-1]].append(list(reversed(x)))\n\t            else:\n\t                for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n\t                    self.dfs([i], meta2, meta_dict2)\n\t            meta_dict = {}\n\t            start_node_type = self.links['meta'][meta1[0]][0] if meta1[0]>=0 else self.links['meta'][-meta1[0]-1][1]\n\t            for i in range(self.nodes['shift'][start_node_type], self.nodes['shift'][start_node_type]+self.nodes['count'][start_node_type]):\n\t                meta_dict[i] = []\n", "                for beg in meta_dict1[i]:\n\t                    for end in meta_dict2[beg[-1]]:\n\t                        meta_dict[i].append(beg + end[1:])\n\t        return meta_dict\n\t    def gen_file_for_evaluate(self, test_idx, label, file_path, mode='bi'):\n\t        if test_idx.shape[0] != label.shape[0]:\n\t            return\n\t        if mode == 'multi':\n\t            multi_label=[]\n\t            for i in range(label.shape[0]):\n", "                label_list = [str(j) for j in range(label[i].shape[0]) if label[i][j]==1]\n\t                multi_label.append(','.join(label_list))\n\t            label=multi_label\n\t        elif mode=='bi':\n\t            label = np.array(label)\n\t        else:\n\t            return\n\t        with open(file_path, \"w\") as f:\n\t            for nid, l in zip(test_idx, label):\n\t                f.write(f\"{nid}\\t\\t{self.get_node_type(nid)}\\t{l}\\n\")\n", "    def evaluate(self, pred, mode='bi'):\n\t        #print(f\"{bcolors.WARNING}Warning: If you want to obtain test score, please submit online on biendata.{bcolors.ENDC}\")\n\t        y_true = self.labels_test['data'][self.labels_test['mask']]\n\t        micro = f1_score(y_true, pred, average='micro')\n\t        macro = f1_score(y_true, pred, average='macro')\n\t        result = {\n\t            'micro-f1': micro,\n\t            'macro-f1': macro,\n\t        }\n\t        if  mode=='multi':\n", "            mcm=multilabel_confusion_matrix(y_true, pred)\n\t            result.update({\"mcm\":mcm})\n\t        return result\n\t    def evaluate_by_group(self,pred,group_ids,train=False,mode=\"bi\"):\n\t        #pred should be all-prediction\n\t        if len(group_ids)<1:\n\t            return {'micro-f1': \"NoNodes\",'macro-f1': \"NoNodes\",}\n\t        if train:\n\t            labels=self.labels_train['data']\n\t        else:\n", "            labels=self.labels_test['data']\n\t        labels=labels[group_ids]\n\t        if mode==\"bi\":\n\t            labels=labels.argmax(-1)\n\t        y_true=labels\n\t        pred=pred[group_ids].cpu().detach()\n\t        micro = f1_score(labels, pred, average='micro')\n\t        macro = f1_score(labels, pred, average='macro')\n\t        result = {\n\t            'micro-f1': micro,\n", "            'macro-f1': macro,\n\t            'num':len(group_ids),\n\t            \"acc\":\"\"\n\t        }\n\t        #if  mode=='multi':\n\t            #mcm=multilabel_confusion_matrix(labels, pred)\n\t            #result.update({\"mcm\":str(mcm)})\n\t            #pass\n\t        if mode==\"bi\":\n\t            result[\"acc\"]=accuracy_score(labels,pred)\n", "        result[\"micro-pre\"]=precision_score(y_true,pred, average='micro')\n\t        result[\"macro-pre\"]=precision_score(y_true,pred, average='macro')\n\t        result[\"micro-rec\"]=recall_score(y_true, pred, average='micro')\n\t        result[\"macro-rec\"]=recall_score(y_true, pred, average='macro')\n\t        return result\n\t    def load_labels(self, name):\n\t        \"\"\"\n\t        return labels dict\n\t            num_classes: total number of labels\n\t            total: total number of labeled data\n", "            count: number of labeled data for each node type\n\t            data: a numpy matrix with shape (self.nodes['total'], self.labels['num_classes'])\n\t            mask: to indicate if that node is labeled, if False, that line of data is masked\n\t        \"\"\"\n\t        labels = {'num_classes':0, 'total':0, 'count':Counter(), 'data':None, 'mask':None}\n\t        nc = 0\n\t        mask = np.zeros(self.nodes['total'], dtype=bool)\n\t        data = [None for i in range(self.nodes['total'])]\n\t        with open(os.path.join(self.path, name), 'r', encoding='utf-8') as f:\n\t            for line in f:\n", "                th = line.split('\\t')\n\t                node_id, node_name, node_type, node_label = int(th[0]), th[1], int(th[2]), list(map(int, th[3].split(',')))\n\t                for label in node_label:\n\t                    nc = max(nc, label+1)\n\t                mask[node_id] = True\n\t                data[node_id] = node_label\n\t                labels['count'][node_type] += 1\n\t                labels['total'] += 1\n\t        labels['num_classes'] = nc\n\t        new_data = np.zeros((self.nodes['total'], labels['num_classes']), dtype=int)\n", "        for i,x in enumerate(data):\n\t            if x is not None:\n\t                for j in x:\n\t                    new_data[i, j] = 1\n\t        labels['data'] = new_data\n\t        labels['mask'] = mask\n\t        return labels\n\t    def get_node_type(self, node_id):\n\t        for i in range(len(self.nodes['shift'])):\n\t            if node_id < self.nodes['shift'][i]+self.nodes['count'][i]:\n", "                return i\n\t    def get_edge_type(self, info):\n\t        if type(info) is int or len(info) == 1:\n\t            return info\n\t        for i in range(len(self.links['meta'])):\n\t            if self.links['meta'][i] == info:\n\t                return i\n\t        info = (info[1], info[0])\n\t        for i in range(len(self.links['meta'])):\n\t            if self.links['meta'][i] == info:\n", "                return -i - 1\n\t        raise Exception('No available edge type')\n\t    def get_edge_info(self, edge_id):\n\t        return self.links['meta'][edge_id]\n\t    def list_to_sp_mat(self, li):\n\t        data = [x[2] for x in li]\n\t        i = [x[0] for x in li]\n\t        j = [x[1] for x in li]\n\t        return sp.coo_matrix((data, (i,j)), shape=(self.nodes['total'], self.nodes['total'])).tocsr()\n\t    def load_links(self):\n", "        \"\"\"\n\t        return links dict\n\t            total: total number of links\n\t            count: a dict of int, number of links for each type\n\t            meta: a dict of tuple, explaining the link type is from what type of node to what type of node\n\t            data: a dict of sparse matrices, each link type with one matrix. Shapes are all (nodes['total'], nodes['total'])\n\t        \"\"\"\n\t        links = {'total':0, 'count':Counter(), 'meta':{}, 'data':defaultdict(list)}\n\t        r_ids=[]\n\t        with open(os.path.join(self.path, 'link.dat'), 'r', encoding='utf-8') as f:\n", "            for line in f:\n\t                th = line.split('\\t')\n\t                h_id, t_id, r_id, link_weight = int(th[0]), int(th[1]), int(th[2]), float(th[3])\n\t                if h_id in self.old_to_new_id_mapping.keys() and t_id in self.old_to_new_id_mapping.keys() :\n\t                    h_id=self.old_to_new_id_mapping[h_id]\n\t                    t_id=self.old_to_new_id_mapping[t_id]\n\t                    if r_id not in links['meta']:\n\t                        h_type = self.get_node_type(h_id)\n\t                        t_type = self.get_node_type(t_id)\n\t                        links['meta'][r_id] = (h_type, t_type)\n", "                    links['data'][r_id].append((h_id, t_id, link_weight))\n\t                    links['count'][r_id] += 1\n\t                    links['total'] += 1\n\t                    if r_id not in r_ids:\n\t                        r_ids.append(r_id)\n\t        r_ids=sorted(r_ids)\n\t        temp_meta={}\n\t        for i in range(len(links['meta'].keys())):\n\t            temp_meta[i]=links['meta'][r_ids[i]]\n\t        links['meta']=temp_meta\n", "        temp_count={}\n\t        for i in range(len(links['count'].keys())):\n\t            temp_count[i]=links['count'][r_ids[i]]\n\t        links['count']=temp_count\n\t        temp_data={}\n\t        for i in range(len(links['data'].keys())):\n\t            temp_data[i]=links['data'][r_ids[i]]\n\t        links['data']=temp_data\n\t        new_data = {}\n\t        for r_id in links['data']:\n", "            new_data[r_id] = self.list_to_sp_mat(links['data'][r_id])\n\t        links['data'] = new_data\n\t        return links\n\t    def load_nodes(self):\n\t        \"\"\"\n\t        return nodes dict\n\t            total: total number of nodes\n\t            count: a dict of int, number of nodes for each type\n\t            attr: a dict of np.array (or None), attribute matrices for each type of nodes\n\t            shift: node_id shift for each type. You can get the id range of a type by \n", "                        [ shift[node_type], shift[node_type]+count[node_type] )\n\t        \"\"\"\n\t        nodes = {'total':0, 'count':Counter(), 'attr':{}, 'shift':{}}\n\t        node_ids=[]\n\t        print(self.path)\n\t        #with open(os.path.join(self.path, 'new.txt'), 'w') as f:\n\t            #f.write(\"1\")\n\t        with open(os.path.join(self.path, 'node.dat'), 'r', encoding='utf-8') as f:\n\t            for line in f:\n\t                th = line.split('\\t')\n", "                if len(th) == 4:\n\t                    # Then this line of node has attribute\n\t                    node_id, node_name, node_type, node_attr = th\n\t                    node_id = int(node_id)\n\t                    node_type = int(node_type)\n\t                    node_attr = list(map(float, node_attr.split(',')))\n\t                    node_ids.append(node_id)\n\t                    nodes['count'][node_type] += 1\n\t                    nodes['attr'][node_id] = node_attr\n\t                    nodes['total'] += 1\n", "                elif len(th) == 3:\n\t                    # Then this line of node doesn't have attribute\n\t                    node_id, node_name, node_type = th\n\t                    node_id = int(node_id)\n\t                    node_type = int(node_type)\n\t                    node_ids.append(node_id)\n\t                    nodes['count'][node_type] += 1\n\t                    nodes['total'] += 1\n\t                else:\n\t                    raise Exception(\"Too few information to parse!\")\n", "        #type_id mapping\n\t        temp_count=Counter()\n\t        for k in nodes['count']:\n\t            temp_count[k]=nodes['count'][k]\n\t        nodes['count']=temp_count\n\t        #node_id mapping\n\t        self.old_to_new_id_mapping={}\n\t        self.new_to_old_id_mapping={}\n\t        node_ids=sorted(node_ids)\n\t        for new_id in range(len(node_ids)):\n", "            self.old_to_new_id_mapping[node_ids[new_id]]=new_id\n\t            self.new_to_old_id_mapping[new_id]=node_ids[new_id]\n\t        temp_attr={}\n\t        for old_id in node_ids:\n\t            if old_id in nodes['attr'].keys():\n\t                temp_attr[self.old_to_new_id_mapping[old_id]]=nodes['attr'][old_id]\n\t        nodes['attr']=temp_attr\n\t        shift = 0\n\t        attr = {}\n\t        for i in range(len(nodes['count'])):\n", "            nodes['shift'][i] = shift\n\t            if shift in nodes['attr']:\n\t                mat = []\n\t                for j in range(shift, shift+nodes['count'][i]):\n\t                    mat.append(nodes['attr'][j])\n\t                attr[i] = np.array(mat)\n\t            else:\n\t                attr[i] = None\n\t            shift += nodes['count'][i]\n\t        nodes['attr'] = attr\n", "        return nodes\n"]}
{"filename": "LP/methods/slotGAT/conv.py", "chunked_list": ["\"\"\"Torch modules for graph attention networks(GAT).\"\"\"\n\t# pylint: disable= no-member, arguments-differ, invalid-name\n\timport torch as th\n\timport torch\n\tfrom torch import nn\n\tfrom dgl import function as fn\n\tfrom dgl.nn.pytorch import edge_softmax\n\tfrom dgl._ffi.base import DGLError\n\tfrom dgl.nn.pytorch.utils import Identity\n\tfrom dgl.utils import expand_as_pair\n", "class slotGATConv(nn.Module):\n\t    \"\"\"\n\t    Adapted from\n\t    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n\t    \"\"\"\n\t    def __init__(self,\n\t                 edge_feats,\n\t                 num_etypes,\n\t                 in_feats,\n\t                 out_feats,\n", "                 num_heads,\n\t                 feat_drop=0.,\n\t                 attn_drop=0.,\n\t                 negative_slope=0.2,\n\t                 residual=False,\n\t                 activation=None,\n\t                 allow_zero_in_degree=False,\n\t                 bias=False,\n\t                 alpha=0.,\n\t                 num_ntype=None,eindexer=None,inputhead=False):\n", "        super(slotGATConv, self).__init__()\n\t        self._edge_feats = edge_feats\n\t        self._num_heads = num_heads\n\t        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n\t        self._out_feats = out_feats\n\t        self._allow_zero_in_degree = allow_zero_in_degree\n\t        self.edge_emb = nn.Embedding(num_etypes, edge_feats) if edge_feats else None \n\t        self.eindexer=eindexer\n\t        self.num_ntype=num_ntype \n\t        self.attentions=None   \n", "        if isinstance(in_feats, tuple):\n\t            raise NotImplementedError()\n\t        else:\n\t            self.fc = nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats , out_feats * num_heads)))\n\t            \"\"\"else:\n\t                self.fc =nn.ModuleList([nn.Linear(\n\t                    self._in_src_feats, out_feats * num_heads, bias=False)  for _ in range(num_ntype)] )\n\t                raise Exception(\"!!!\")\"\"\"\n\t        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False) if edge_feats else None \n\t        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats   *self.num_ntype)))\n", "        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats*self.num_ntype)))\n\t        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats))) if edge_feats else None\n\t        self.feat_drop = nn.Dropout(feat_drop)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\t        if residual:\n\t            if self._in_dst_feats != out_feats:\n\t                self.res_fc =nn.Parameter(th.FloatTensor(size=(self.num_ntype, self._in_src_feats , out_feats * num_heads)))\n\t            else:\n\t                self.res_fc = Identity()\n", "        else:\n\t            self.register_buffer('res_fc', None)\n\t        self.reset_parameters()\n\t        self.activation = activation\n\t        self.bias = bias\n\t        if bias:\n\t            raise NotImplementedError()\n\t        self.alpha = alpha\n\t        self.inputhead=inputhead\n\t    def reset_parameters(self):\n", "        gain = nn.init.calculate_gain('relu')\n\t        if hasattr(self, 'fc'):\n\t            nn.init.xavier_normal_(self.fc, gain=gain)\n\t        else:\n\t            raise Exception(\"!!!\")\n\t            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n\t            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_l, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_r, gain=gain) \n\t        if  self._edge_feats:\n", "            nn.init.xavier_normal_(self.attn_e, gain=gain) \n\t        if isinstance(self.res_fc, nn.Linear):\n\t            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n\t        elif isinstance(self.res_fc, Identity):\n\t            pass\n\t        elif isinstance(self.res_fc, nn.Parameter):\n\t            nn.init.xavier_normal_(self.res_fc, gain=gain)\n\t        if self._edge_feats:\n\t            nn.init.xavier_normal_(self.fc_e.weight, gain=gain) \n\t    def set_allow_zero_in_degree(self, set_value):\n", "        self._allow_zero_in_degree = set_value\n\t    def forward(self, graph, feat, e_feat,get_out=[\"\"], res_attn=None):\n\t        with graph.local_scope():\n\t            node_idx_by_ntype=graph.node_idx_by_ntype\n\t            if not self._allow_zero_in_degree:\n\t                if (graph.in_degrees() == 0).any():\n\t                    raise DGLError('There are 0-in-degree nodes in the graph, '\n\t                                   'output for those nodes will be invalid. '\n\t                                   'This is harmful for some applications, '\n\t                                   'causing silent performance regression. '\n", "                                   'Adding self-loop on the input graph by '\n\t                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n\t                                   'the issue. Setting ``allow_zero_in_degree`` '\n\t                                   'to be `True` when constructing this module will '\n\t                                   'suppress the check and let the code run.')\n\t            if isinstance(feat, tuple):\n\t                raise Exception(\"!!!\")\n\t                h_src = self.feat_drop(feat[0])\n\t                h_dst = self.feat_drop(feat[1])\n\t                if not hasattr(self, 'fc_src'):\n", "                    self.fc_src, self.fc_dst = self.fc, self.fc\n\t                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n\t                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n\t            else:\n\t                #feature transformation first\n\t                h_src = h_dst = self.feat_drop(feat)   #num_nodes*(num_ntype*input_dim)\n\t                if self.inputhead:\n\t                    h_src=h_src.view(-1,1,self.num_ntype,self._in_src_feats)\n\t                else:\n\t                    h_src=h_src.view(-1,self._num_heads,self.num_ntype,int(self._in_src_feats/self._num_heads))\n", "                h_dst=h_src=h_src.permute(2,0,1,3).flatten(2)  #num_ntype*num_nodes*(in_feat_dim)\n\t                if \"getEmb\" in get_out:\n\t                    self.emb=h_dst.cpu().detach()\n\t                #self.fc with num_ntype*(in_feat_dim)*(out_feats * num_heads) \n\t                feat_dst = torch.bmm(h_src,self.fc)  #num_ntype*num_nodes*(out_feats * num_heads)\n\t                feat_src = feat_dst =feat_dst.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n\t                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n\t                if graph.is_block:\n\t                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n\t            e_feat = self.edge_emb(e_feat) if self._edge_feats else None\n", "            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)  if self._edge_feats else None\n\t            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1) if self._edge_feats else 0  #(-1, self._num_heads, 1) \n\t            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n\t            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n\t            graph.srcdata.update({'ft': feat_src, 'el': el})\n\t            graph.dstdata.update({'er': er})\n\t            graph.edata.update({'ee': ee}) if self._edge_feats else None\n\t            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n\t            e_=graph.edata.pop('e')\n\t            ee=graph.edata.pop('ee') if self._edge_feats else 0\n", "            e=e_+ee\n\t            e = self.leaky_relu(e)\n\t            # compute softmax\n\t            a=self.attn_drop(edge_softmax(graph, e))\n\t            if res_attn is not None:\n\t                a=a * (1-self.alpha) + res_attn * self.alpha \n\t            graph.edata['a'] = a\n\t            # then message passing\n\t            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n\t                             fn.sum('m', 'ft'))\n", "            rst = graph.dstdata['ft']\n\t            # residual\n\t            if self.res_fc is not None:\n\t                if self._in_dst_feats != self._out_feats:\n\t                    resval =torch.bmm(h_src,self.res_fc)\n\t                    resval =resval.permute(1,0,2).view(                 #num_nodes*num_heads*(num_ntype*hidden_dim)\n\t                        -1,self.num_ntype ,self._num_heads, self._out_feats).permute(0,2,1,3).flatten(2)\n\t                    #resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n\t                else:\n\t                    resval = self.res_fc(h_src).view(h_dst.shape[0], -1, self._out_feats*self.num_ntype)  #Identity\n", "                rst = rst + resval\n\t            # bias\n\t            if self.bias:\n\t                rst = rst + self.bias_param\n\t            # activation\n\t            if self.activation:\n\t                rst = self.activation(rst)\n\t            self.attentions=graph.edata.pop('a').detach()\n\t            torch.cuda.empty_cache()\n\t            return rst, self.attentions\n", "# pylint: enable=W0235\n\tclass myGATConv(nn.Module):\n\t    \"\"\"\n\t    Adapted from\n\t    https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n\t    \"\"\"\n\t    def __init__(self,\n\t                 edge_feats,\n\t                 num_etypes,\n\t                 in_feats,\n", "                 out_feats,\n\t                 num_heads,\n\t                 feat_drop=0.,\n\t                 attn_drop=0.,\n\t                 negative_slope=0.2,\n\t                 residual=False,\n\t                 activation=None,\n\t                 allow_zero_in_degree=False,\n\t                 bias=False,\n\t                 alpha=0.):\n", "        super(myGATConv, self).__init__()\n\t        self._edge_feats = edge_feats\n\t        self._num_heads = num_heads\n\t        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n\t        self._out_feats = out_feats\n\t        self._allow_zero_in_degree = allow_zero_in_degree\n\t        self.edge_emb = nn.Embedding(num_etypes, edge_feats)\n\t        if isinstance(in_feats, tuple):\n\t            self.fc_src = nn.Linear(\n\t                self._in_src_feats, out_feats * num_heads, bias=False)\n", "            self.fc_dst = nn.Linear(\n\t                self._in_dst_feats, out_feats * num_heads, bias=False)\n\t        else:\n\t            self.fc = nn.Linear(\n\t                self._in_src_feats, out_feats * num_heads, bias=False)\n\t        self.fc_e = nn.Linear(edge_feats, edge_feats*num_heads, bias=False)\n\t        self.attn_l = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n\t        self.attn_r = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats)))\n\t        self.attn_e = nn.Parameter(th.FloatTensor(size=(1, num_heads, edge_feats)))\n\t        self.feat_drop = nn.Dropout(feat_drop)\n", "        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\t        if residual:\n\t            if self._in_dst_feats != out_feats:\n\t                self.res_fc = nn.Linear(\n\t                    self._in_dst_feats, num_heads * out_feats, bias=False)\n\t            else:\n\t                self.res_fc = Identity()\n\t        else:\n\t            self.register_buffer('res_fc', None)\n", "        self.reset_parameters()\n\t        self.activation = activation\n\t        self.bias = bias\n\t        if bias:\n\t            self.bias_param = nn.Parameter(th.zeros((1, num_heads, out_feats)))\n\t        self.alpha = alpha\n\t    def reset_parameters(self):\n\t        gain = nn.init.calculate_gain('relu')\n\t        if hasattr(self, 'fc'):\n\t            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n", "        else:\n\t            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n\t            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_l, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_r, gain=gain)\n\t        nn.init.xavier_normal_(self.attn_e, gain=gain)\n\t        if isinstance(self.res_fc, nn.Linear):\n\t            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n\t        nn.init.xavier_normal_(self.fc_e.weight, gain=gain)\n\t    def set_allow_zero_in_degree(self, set_value):\n", "        self._allow_zero_in_degree = set_value\n\t    def forward(self, graph, feat, e_feat, res_attn=None):\n\t        with graph.local_scope():\n\t            if not self._allow_zero_in_degree:\n\t                if (graph.in_degrees() == 0).any():\n\t                    raise DGLError('There are 0-in-degree nodes in the graph, '\n\t                                   'output for those nodes will be invalid. '\n\t                                   'This is harmful for some applications, '\n\t                                   'causing silent performance regression. '\n\t                                   'Adding self-loop on the input graph by '\n", "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n\t                                   'the issue. Setting ``allow_zero_in_degree`` '\n\t                                   'to be `True` when constructing this module will '\n\t                                   'suppress the check and let the code run.')\n\t            if isinstance(feat, tuple):\n\t                h_src = self.feat_drop(feat[0])\n\t                h_dst = self.feat_drop(feat[1])\n\t                if not hasattr(self, 'fc_src'):\n\t                    self.fc_src, self.fc_dst = self.fc, self.fc\n\t                feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n", "                feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n\t            else:\n\t                h_src = h_dst = self.feat_drop(feat)\n\t                feat_src = feat_dst = self.fc(h_src).view(\n\t                    -1, self._num_heads, self._out_feats)\n\t                if graph.is_block:\n\t                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n\t            e_feat = self.edge_emb(e_feat)\n\t            e_feat = self.fc_e(e_feat).view(-1, self._num_heads, self._edge_feats)\n\t            ee = (e_feat * self.attn_e).sum(dim=-1).unsqueeze(-1)\n", "            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n\t            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n\t            graph.srcdata.update({'ft': feat_src, 'el': el})\n\t            graph.dstdata.update({'er': er})\n\t            graph.edata.update({'ee': ee})\n\t            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n\t            e = self.leaky_relu(graph.edata.pop('e')+graph.edata.pop('ee'))\n\t            # compute softmax\n\t            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n\t            if res_attn is not None:\n", "                graph.edata['a'] = graph.edata['a'] * (1-self.alpha) + res_attn * self.alpha\n\t            # message passing\n\t            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n\t                             fn.sum('m', 'ft'))\n\t            rst = graph.dstdata['ft']\n\t            # residual\n\t            if self.res_fc is not None:\n\t                resval = self.res_fc(h_dst).view(h_dst.shape[0], -1, self._out_feats)\n\t                rst = rst + resval\n\t            # bias\n", "            if self.bias:\n\t                rst = rst + self.bias_param\n\t            # activation\n\t            if self.activation:\n\t                rst = self.activation(rst)\n\t            return rst, graph.edata.pop('a').detach()\n"]}
{"filename": "LP/methods/slotGAT/GNN.py", "chunked_list": ["import torch\n\timport torch as th\n\timport torch.nn as nn\n\timport dgl\n\tfrom dgl.nn.pytorch import GraphConv\n\timport math\n\timport torch.nn.functional as F\n\timport dgl.function as fn\n\tfrom dgl.nn.pytorch import edge_softmax, GATConv\n\tfrom conv import myGATConv,slotGATConv\n", "from dgl._ffi.base import DGLError\n\tfrom torch.profiler import profile, record_function, ProfilerActivity\n\t\"\"\"\n\tclass DistMult(nn.Module):\n\t    def __init__(self, num_rel, dim):\n\t        super(DistMult, self).__init__()\n\t        self.W = nn.Parameter(torch.FloatTensor(size=(num_rel, dim, dim)))\n\t        nn.init.xavier_normal_(self.W, gain=1.414)\n\t    def forward(self, left_emb, right_emb, r_id):\n\t        thW = self.W[r_id]\n", "        left_emb = torch.unsqueeze(left_emb, 1)\n\t        right_emb = torch.unsqueeze(right_emb, 2)\n\t        return torch.bmm(torch.bmm(left_emb, thW), right_emb).squeeze()\"\"\"\n\tclass DistMult(nn.Module):\n\t    def __init__(self, num_rel, dim):\n\t        super(DistMult, self).__init__()\n\t        self.W = nn.Parameter(torch.FloatTensor(size=(num_rel, dim, dim)))\n\t        nn.init.xavier_normal_(self.W, gain=1.414)\n\t    def forward(self, left_emb, right_emb, r_id,slot_num=None,prod_aggr=None,sigmoid=\"after\"):\n\t        if not prod_aggr:\n", "            thW = self.W[r_id]\n\t            left_emb = torch.unsqueeze(left_emb, 1)\n\t            right_emb = torch.unsqueeze(right_emb, 2)\n\t            #return torch.bmm(torch.bmm(left_emb, thW), right_emb).squeeze()\n\t            scores=torch.zeros(right_emb.shape[0]).to(right_emb.device)\n\t            for i in range(int(max(r_id))+1):\n\t                scores[r_id==i]=torch.bmm(torch.matmul(left_emb[r_id==i], self.W[i]), right_emb[r_id==i]).squeeze()\n\t            return scores\n\t        else:\n\t            raise Exception\n", "class Dot(nn.Module):\n\t    def __init__(self):\n\t        super(Dot, self).__init__()\n\t    def forward(self, left_emb, right_emb, r_id,slot_num=None,prod_aggr=None,sigmoid=\"after\"):\n\t        if not prod_aggr:\n\t            left_emb = torch.unsqueeze(left_emb, 1)\n\t            right_emb = torch.unsqueeze(right_emb, 2)\n\t            return torch.bmm(left_emb, right_emb).squeeze()\n\t        else:\n\t            left_emb = left_emb.view(-1,slot_num,int(left_emb.shape[1]/slot_num))\n", "            right_emb = right_emb.view(-1,int(right_emb.shape[1]/slot_num),slot_num)\n\t            x=torch.bmm(left_emb, right_emb)# num_sampled_edges* num_slot*num_slot\n\t            if prod_aggr==\"all\":\n\t                x=x.flatten(1)\n\t                x=x.sum(1)\n\t                return x\n\t            x=torch.diagonal(x,0,1,2) # num_sampled_edges* num_slot\n\t            if sigmoid==\"before\":\n\t                x=F.sigmoid(x)\n\t            if prod_aggr==\"mean\":\n", "                x=x.mean(1)\n\t            elif prod_aggr==\"max\":\n\t                x=x.max(1)[0]\n\t            elif prod_aggr==\"sum\":\n\t                x=x.sum(1)\n\t            else:\n\t                raise Exception()\n\t            return x\n\tclass myGAT(nn.Module):\n\t    def __init__(self,\n", "                 g,\n\t                 edge_dim,\n\t                 num_etypes,\n\t                 in_dims,\n\t                 num_hidden,\n\t                 num_classes,\n\t                 num_layers,\n\t                 heads,\n\t                 activation,\n\t                 feat_drop,\n", "                 attn_drop,\n\t                 negative_slope,\n\t                 residual,\n\t                 alpha,\n\t                 decode='distmult',inProcessEmb=\"True\",l2use=\"True\",dataRecorder=None,get_out=None):\n\t        super(myGAT, self).__init__()\n\t        self.g = g\n\t        self.num_layers = num_layers\n\t        self.gat_layers = nn.ModuleList()\n\t        self.activation = activation\n", "        self.inProcessEmb=inProcessEmb\n\t        self.l2use=l2use\n\t        self.dataRecorder=dataRecorder\n\t        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n\t        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        # input projection (no residual)\n\t        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n\t            num_hidden, num_hidden, heads[0],\n\t            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha))\n", "        # hidden layers\n\t        for l in range(1, num_layers):\n\t            # due to multi-head, the in_dim = num_hidden * num_heads\n\t            self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n\t                num_hidden * heads[l-1], num_hidden, heads[l],\n\t                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha))\n\t        # output projection\n\t        self.gat_layers.append(myGATConv(edge_dim, num_etypes,\n\t            num_hidden * heads[-2], num_classes, heads[-1],\n\t            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha))\n", "        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n\t        if decode == 'distmult':\n\t            self.decoder = DistMult(num_etypes, num_classes*(num_layers+2))\n\t        elif decode == 'dot':\n\t            self.decoder = Dot()\n\t        self.get_out=get_out\n\t    def l2_norm(self, x):\n\t        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n\t        return x / (torch.max(torch.norm(x, dim=1, keepdim=True), self.epsilon))\n\t    def forward(self, features_list, e_feat, left, right, mid):\n", "        h = []\n\t        for fc, feature in zip(self.fc_list, features_list):\n\t            h.append(fc(feature))\n\t        h = torch.cat(h, 0)\n\t        emb = [self.l2_norm(h)]\n\t        res_attn = None\n\t        for l in range(self.num_layers):\n\t            h, res_attn = self.gat_layers[l](self.g, h, e_feat, res_attn=res_attn)\n\t            emb.append(self.l2_norm(h.mean(1)))\n\t            h = h.flatten(1)\n", "        # output projection\n\t        logits, _ = self.gat_layers[-1](self.g, h, e_feat, res_attn=res_attn)#None)\n\t        logits = logits.mean(1)\n\t        logits = self.l2_norm(logits)\n\t        #emb.append(logits)\n\t        if self.inProcessEmb==\"True\":\n\t            emb.append(logits)\n\t        else:\n\t            emb=[logits]\n\t        logits = torch.cat(emb, 1)\n", "        left_emb = logits[left]\n\t        right_emb = logits[right]\n\t        return F.sigmoid(self.decoder(left_emb, right_emb, mid))\n\tclass slotGAT(nn.Module):\n\t    def __init__(self,\n\t                 g,\n\t                 edge_dim,\n\t                 num_etypes,\n\t                 in_dims,\n\t                 num_hidden,\n", "                 num_classes,\n\t                 num_layers,\n\t                 heads,\n\t                 activation,\n\t                 feat_drop,\n\t                 attn_drop,\n\t                 negative_slope,\n\t                 residual,\n\t                 alpha,\n\t                 num_ntype,\n", "                 eindexer,aggregator=\"average\",predicted_by_slot=\"None\", get_out=[\"\"],\n\t                 decode='distmult',inProcessEmb=\"True\",l2BySlot=\"False\",prod_aggr=None,sigmoid=\"after\",l2use=\"True\",SAattDim=128,dataRecorder=None):\n\t        super(slotGAT, self).__init__()\n\t        self.g = g\n\t        self.num_layers = num_layers\n\t        self.gat_layers = nn.ModuleList()\n\t        self.heads=heads\n\t        self.activation = activation\n\t        self.fc_list = nn.ModuleList([nn.Linear(in_dim, num_hidden, bias=True) for in_dim in in_dims])\n\t        self.num_ntype=num_ntype\n", "        self.num_classes=num_classes\n\t        self.leaky_relu = nn.LeakyReLU(negative_slope)\n\t        self.predicted_by_slot=predicted_by_slot  \n\t        self.inProcessEmb=inProcessEmb \n\t        self.l2BySlot=l2BySlot\n\t        self.prod_aggr=prod_aggr\n\t        self.sigmoid=sigmoid\n\t        self.l2use=l2use\n\t        self.SAattDim=SAattDim\n\t        self.dataRecorder=dataRecorder\n", "        self.get_out=get_out \n\t        self.num_etypes=num_etypes\n\t        self.num_hidden=num_hidden\n\t        self.last_fc = nn.Parameter(th.FloatTensor(size=(num_classes*self.num_ntype, num_classes))) ;nn.init.xavier_normal_(self.last_fc, gain=1.414)\n\t        for fc in self.fc_list:\n\t            nn.init.xavier_normal_(fc.weight, gain=1.414)\n\t        # input projection (no residual)\n\t        self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n\t            num_hidden, num_hidden, heads[0],\n\t            feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha,num_ntype=num_ntype,eindexer=eindexer,inputhead=True))\n", "        # hidden layers\n\t        for l in range(1, num_layers):\n\t            # due to multi-head, the in_dim = num_hidden * num_heads\n\t            self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n\t                num_hidden* heads[l-1] , num_hidden, heads[l],\n\t                feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha,num_ntype=num_ntype,eindexer=eindexer))\n\t        # output projection\n\t        self.gat_layers.append(slotGATConv(edge_dim, num_etypes,\n\t            num_hidden* heads[-2] , num_classes, heads[-1],\n\t            feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha,num_ntype=num_ntype,eindexer=eindexer))\n", "        self.aggregator=aggregator\n\t        if aggregator==\"SA\":\n\t            if self.inProcessEmb==\"True\":\n\t                last_dim=num_hidden*(2+num_layers)\n\t            else:\n\t                last_dim=num_hidden\n\t            self.macroLinear=nn.Linear(last_dim, self.SAattDim, bias=True);nn.init.xavier_normal_(self.macroLinear.weight, gain=1.414);nn.init.normal_(self.macroLinear.bias, std=1.414*math.sqrt(1/(self.macroLinear.bias.flatten().shape[0])))\n\t            self.macroSemanticVec=nn.Parameter(torch.FloatTensor(self.SAattDim,1));nn.init.normal_(self.macroSemanticVec,std=1)\n\t        self.by_slot=[f\"by_slot_{nt}\" for nt in range(num_ntype)]\n\t        assert aggregator in ([\"average\",\"last_fc\",\"max\",\"None\",\"SA\"]+self.by_slot)\n", "        #self.get_out=get_out\n\t        self.epsilon = torch.FloatTensor([1e-12]).cuda()\n\t        if decode == 'distmult':\n\t            if self.aggregator==\"None\":\n\t                num_classes=num_classes*num_ntype\n\t            self.decoder = DistMult(num_etypes, num_classes*(num_layers+2))\n\t        elif decode == 'dot':\n\t            self.decoder = Dot()\n\t    def forward(self, features_list,e_feat, left, right, mid, get_out=\"False\"): \n\t        encoded_embeddings=None\n", "        h = []\n\t        for nt_id,(fc, feature) in enumerate(zip(self.fc_list, features_list)):\n\t            nt_ft=fc(feature)\n\t            emsen_ft=torch.zeros([nt_ft.shape[0],nt_ft.shape[1]*self.num_ntype]).to(feature.device)\n\t            emsen_ft[:,nt_ft.shape[1]*nt_id:nt_ft.shape[1]*(nt_id+1)]=nt_ft\n\t            h.append(emsen_ft)   # the id is decided by the node types\n\t        h = torch.cat(h, 0)        #  num_nodes*(num_type*hidden_dim)\n\t        emb = [self.aggr_func(self.l2_norm(h,l2BySlot=self.l2BySlot))]\n\t        res_attn = None\n\t        for l in range(self.num_layers):\n", "            h, res_attn = self.gat_layers[l](self.g, h, e_feat,get_out=get_out, res_attn=res_attn)   #num_nodes*num_heads*(num_ntype*hidden_dim)\n\t            emb.append(self.aggr_func(self.l2_norm(h.mean(1),l2BySlot=self.l2BySlot)))\n\t            h = h.flatten(1)#num_nodes*(num_heads*num_ntype*hidden_dim)\n\t        # output projection\n\t        logits, _ = self.gat_layers[-1](self.g, h, e_feat,get_out=get_out, res_attn=res_attn)#None)   #num_nodes*num_heads*num_ntype*hidden_dim\n\t        logits = logits.mean(1)\n\t        if self.predicted_by_slot!=\"None\" and self.training==False:\n\t            logits=logits.view(-1,1,self.num_ntype,self.num_classes)\n\t            if self.predicted_by_slot==\"max\":\n\t                if \"getMaxSlot\" in  get_out:\n", "                    maxSlotIndexesWithLabels=logits.max(2)[1].squeeze(1)\n\t                    logits_indexer=logits.max(2)[0].max(2)[1]\n\t                    self.maxSlotIndexes=torch.gather(maxSlotIndexesWithLabels,1,logits_indexer)\n\t                logits=logits.max(2)[0]\n\t            elif self.predicted_by_slot==\"all\":\n\t                if \"getSlots\" in get_out:\n\t                    self.logits=logits.detach()\n\t                logits=logits.view(-1,1,self.num_ntype,self.num_classes).mean(2)\n\t            else:\n\t                target_slot=int(self.predicted_by_slot)\n", "                logits=logits[:,:,target_slot,:].squeeze(2)\n\t        else:\n\t            logits=self.aggr_func(self.l2_norm(logits,l2BySlot=self.l2BySlot))\n\t        if self.inProcessEmb==\"True\":\n\t            emb.append(logits)\n\t        else:\n\t            emb=[logits]\n\t        if self.aggregator==\"None\" and self.inProcessEmb==\"True\":\n\t            emb=[ x.view(-1, self.num_ntype,int(x.shape[1]/self.num_ntype))   for x in emb]\n\t            o = torch.cat(emb, 2).flatten(1)\n", "        else:\n\t            o = torch.cat(emb, 1)\n\t        if self.aggregator==\"SA\" :\n\t            o=o.view(-1, self.num_ntype,int(o.shape[1]/self.num_ntype))\n\t            slot_scores=(F.tanh( self.macroLinear(o))  @  self.macroSemanticVec).mean(0,keepdim=True)  #num_slots\n\t            self.slot_scores=F.softmax(slot_scores,dim=1)\n\t            o=(o*self.slot_scores).sum(1)  \n\t        left_emb = o[left]\n\t        right_emb = o[right]\n\t        if self.sigmoid==\"after\":\n", "            logits=self.decoder(left_emb, right_emb, mid,slot_num=self.num_ntype,prod_aggr=self.prod_aggr)\n\t            logits=F.sigmoid(logits)\n\t        elif self.sigmoid==\"before\":\n\t            logits=self.decoder(left_emb, right_emb, mid,slot_num=self.num_ntype,prod_aggr=self.prod_aggr,sigmoid=self.sigmoid)\n\t        elif self.sigmoid==\"None\":\n\t            left_emb=self.l2_norm(left_emb,l2BySlot=self.l2BySlot)\n\t            right_emb=self.l2_norm(right_emb,l2BySlot=self.l2BySlot)\n\t            logits=self.decoder(left_emb, right_emb, mid,slot_num=self.num_ntype,prod_aggr=self.prod_aggr)\n\t        else:\n\t            raise Exception()\n", "        return logits\n\t    def l2_norm(self, x,l2BySlot=\"False\"):\n\t        # This is an equivalent replacement for tf.l2_normalize, see https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/math/l2_normalize for more information.\n\t        if self.l2use==\"True\":\n\t            if l2BySlot==\"False\":\n\t                return x / (torch.max(torch.norm(x, dim=1, keepdim=True), self.epsilon))\n\t            elif l2BySlot==\"True\":\n\t                x=x.view(-1, self.num_ntype,int(x.shape[1]/self.num_ntype))\n\t                x=x / (torch.max(torch.norm(x, dim=2, keepdim=True), self.epsilon))\n\t                x=x.flatten(1)\n", "                return x\n\t        elif self.l2use==\"False\":\n\t            return x\n\t        else:\n\t            raise Exception()\n\t    def aggr_func(self,logits):\n\t        if self.aggregator==\"average\":\n\t            logits=logits.view(-1, self.num_ntype,self.num_classes).mean(1)\n\t        elif self.aggregator==\"last_fc\":\n\t            logits=logits.view(-1,self.num_ntype,self.num_classes)\n", "            logits=logits.flatten(1)\n\t            logits=logits.matmul(self.last_fc).unsqueeze(1)\n\t        elif self.aggregator==\"max\":\n\t            logits=logits.view(-1,self.num_ntype,self.num_classes).max(1)[0]\n\t        elif self.aggregator==\"None\" or \"SA\":\n\t            logits=logits.view(-1, self.num_ntype,self.num_classes).flatten(1)\n\t        else:\n\t            raise NotImplementedError()\n\t        return logits\n"]}
{"filename": "LP/methods/slotGAT/pipeline_utils.py", "chunked_list": ["import random\n\timport queue\n\timport time\n\timport subprocess\n\timport multiprocessing\n\tfrom threading import main_thread\n\timport os\n\timport pandas as pd\n\timport copy\n\tclass Run( multiprocessing.Process):\n", "    def __init__(self,task,pool=0,idx=0,tc=0,start_time=0):\n\t        super().__init__()\n\t        self.task=task\n\t        self.log=os.path.join(task['study_name'])\n\t        self.idx=idx\n\t        self.pool=pool\n\t        self.device=None\n\t        self.tc=tc\n\t        self.start_time=start_time\n\t        #self.pbar=pbar\n", "    def run(self):\n\t        print(f\"{'*'*10} study  {self.log} no.{self.idx} waiting for device\")\n\t        count=0\n\t        device_units=[]\n\t        while True:\n\t            if len(device_units)>0:\n\t                try:\n\t                    unit=self.pool.get(timeout=10*random.random())\n\t                except queue.Empty:\n\t                    for unit in device_units:\n", "                            self.pool.put(unit)\n\t                    print(f\"Hold {str(device_units)} and waiting for too long! Throw back and go to sleep\")\n\t                    time.sleep(10*random.random())\n\t                    device_units=[]\n\t                    count=0\n\t                    continue\n\t            else:\n\t                unit=self.pool.get()\n\t            if len(device_units)>0:  # consistency check\n\t                if unit[0]!=device_units[-1][0]:\n", "                    print(f\"Get {str(device_units)} and {unit} not consistent devices and throw back it\")\n\t                    self.pool.put(unit)\n\t                    time.sleep(10*random.random())\n\t                    continue\n\t            count+=1\n\t            device_units.append(unit)\n\t            if count==self.task['cost']:\n\t                break\n\t        print(f\"{'-'*10}  study  {self.log} no.{self.idx} get the devices {str(device_units)} and start working\")\n\t        self.device=device_units[0][0]\n", "        try:\n\t            exit_command=get_command_from_argsDict(self.task,self.device,self.idx)\n\t            print(f\"running: {exit_command}\")\n\t            subprocess.run(exit_command,shell=True)\n\t        finally:\n\t            for unit in device_units:\n\t                self.pool.put(unit)\n\t            #localtime = time.asctime( time.localtime(time.time()) )\n\t        end_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\t        print(f\"Start time: {self.start_time}\\nEnd time: {end_time}\\nwith {self.idx}/{self.tc} tasks\")\n", "        print(f\"  {'<'*10} end  study  {self.log} no.{self.idx} of command \")\n\tdef get_command_from_argsDict(args_dict,gpu,idx):\n\t    command='python -W ignore run_dist.py  '\n\t    for key in args_dict.keys():\n\t        command+=f\" --{key} {args_dict[key]} \"\n\t    command+=f\" --gpu {gpu} \"\n\t    if os.name!=\"nt\":\n\t        command+=f\"   > ./log/{args_dict['study_name']}.txt  \"\n\t    return command\n\tdef config_study_name(prefix,specified_args,extract_dict):\n", "    study_name=prefix\n\t    for k in specified_args:\n\t        v=extract_dict[k]\n\t        study_name+=f\"_{k}_{v}\"\n\t    if study_name[0]==\"_\":\n\t        study_name=study_name.replace(\"_\",\"\",1) \n\t    return study_name \n"]}
{"filename": "LP/methods/slotGAT/run_dist.py", "chunked_list": ["import sys\n\tsys.path.append('../../')\n\timport time\n\timport argparse\n\timport json\n\tfrom collections import defaultdict\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport numpy as np\n", "from utils.tools import writeIntoCsvLogger,vis_data_collector,blank_profile\n\tfrom utils.pytorchtools import EarlyStopping\n\tfrom utils.data import load_data\n\tfrom GNN import myGAT,slotGAT\n\tfrom matplotlib import pyplot as plt\n\timport dgl\n\tfrom torch.profiler import profile, record_function, ProfilerActivity\n\tfrom torch.profiler import tensorboard_trace_handler\n\timport os\n\timport random\n", "torch.set_num_threads(4)\n\tdef sp_to_spt(mat):\n\t    coo = mat.tocoo()\n\t    values = coo.data\n\t    indices = np.vstack((coo.row, coo.col))\n\t    i = torch.LongTensor(indices)\n\t    v = torch.FloatTensor(values)\n\t    shape = coo.shape\n\t    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n\tdef mat2tensor(mat):\n", "    if type(mat) is np.ndarray:\n\t        return torch.from_numpy(mat).type(torch.FloatTensor)\n\t    return sp_to_spt(mat)\n\tdef set_seed(seed):\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n\t    np.random.seed(seed)  # Numpy module.\n\t    random.seed(seed)  # Python random module.\n\t    torch.use_deterministic_algorithms(True)\n", "    torch.backends.cudnn.enabled = False \n\t    torch.backends.cudnn.benchmark = False\n\t    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n\t    os.environ['PYTHONHASHSEED'] = str(seed)\n\tdef run_model_DBLP(args):\n\t    set_seed(args.seed)\n\t    get_out=args.get_out.split(\"_\") \n\t    dataRecorder={\"meta\":{},\"data\":{},\"status\":\"None\"}\n\t    os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n\t    exp_info=f\"exp setting: {vars(args)}\"\n", "    vis_data_saver=vis_data_collector()\n\t    vis_data_saver.save_meta(exp_info,\"exp_info\")\n\t    try:\n\t        torch.cuda.set_device(int(args.gpu))\n\t    except :\n\t        pass\n\t    feats_type = args.feats_type\n\t    features_list, adjM, dl = load_data(args.dataset)\n\t    device = torch.device('cuda' if (torch.cuda.is_available() and args.gpu!=\"cpu\") else 'cpu')\n\t    features_list = [mat2tensor(features).to(device) for features in features_list]\n", "    if feats_type == 0:\n\t        in_dims = [features.shape[1] for features in features_list]\n\t    elif feats_type == 1 or feats_type == 5:\n\t        save = 0 if feats_type == 1 else 2\n\t        in_dims = []#[features_list[0].shape[1]] + [10] * (len(features_list) - 1)\n\t        for i in range(0, len(features_list)):\n\t            if i == save:\n\t                in_dims.append(features_list[i].shape[1])\n\t            else:\n\t                in_dims.append(10)\n", "                features_list[i] = torch.zeros((features_list[i].shape[0], 10)).to(device)\n\t    elif feats_type == 2 or feats_type == 4:\n\t        save = feats_type - 2\n\t        in_dims = [features.shape[0] for features in features_list]\n\t        for i in range(0, len(features_list)):\n\t            if i == save:\n\t                in_dims[i] = features_list[i].shape[1]\n\t                continue\n\t            dim = features_list[i].shape[0]\n\t            indices = np.vstack((np.arange(dim), np.arange(dim)))\n", "            indices = torch.LongTensor(indices)\n\t            values = torch.FloatTensor(np.ones(dim))\n\t            features_list[i] = torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device)\n\t    elif feats_type == 3:\n\t        in_dims = [features.shape[0] for features in features_list]\n\t        for i in range(len(features_list)):\n\t            dim = features_list[i].shape[0]\n\t            indices = np.vstack((np.arange(dim), np.arange(dim)))\n\t            indices = torch.LongTensor(indices)\n\t            values = torch.FloatTensor(np.ones(dim))\n", "            features_list[i] = torch.sparse.FloatTensor(indices, values, torch.Size([dim, dim])).to(device)\n\t    edge2type = {}\n\t    for k in dl.links['data']:\n\t        for u,v in zip(*dl.links['data'][k].nonzero()):\n\t            edge2type[(u,v)] = k\n\t    for i in range(dl.nodes['total']):\n\t        if (i,i) not in edge2type:\n\t            edge2type[(i,i)] = len(dl.links['count'])\n\t    for k in dl.links['data']:\n\t        for u,v in zip(*dl.links['data'][k].nonzero()):\n", "            if (v,u) not in edge2type:\n\t                edge2type[(v,u)] = k+1+len(dl.links['count'])\n\t    g = dgl.DGLGraph(adjM+(adjM.T))\n\t    g = dgl.remove_self_loop(g)\n\t    g = dgl.add_self_loop(g)\n\t    g = g.to(device)\n\t    e_feat = []\n\t    for u, v in zip(*g.edges()):\n\t        u = u.cpu().item()\n\t        v = v.cpu().item()\n", "        e_feat.append(edge2type[(u,v)])\n\t    e_feat = torch.tensor(e_feat, dtype=torch.long).to(device)\n\t    g.edge_type_indexer=F.one_hot(e_feat).to(device)\n\t    total = len(list(dl.links_test['data'].keys()))\n\t    num_ntypes=len(features_list)\n\t    #num_layers=len(hiddens)-1\n\t    num_nodes=dl.nodes['total']\n\t    num_etype=len(dl.links['count'])\n\t    g.node_idx_by_ntype=[]\n\t    g.node_ntype_indexer=torch.zeros(num_nodes,num_ntypes).to(device)\n", "    ntype_dims=[]\n\t    idx_count=0\n\t    ntype_count=0\n\t    for feature in features_list:\n\t        temp=[]\n\t        for _ in feature:\n\t            temp.append(idx_count)\n\t            g.node_ntype_indexer[idx_count][ntype_count]=1\n\t            idx_count+=1\n\t        g.node_idx_by_ntype.append(temp)\n", "        ntype_dims.append(feature.shape[1])\n\t        ntype_count+=1\n\t    ntypes=g.node_ntype_indexer.argmax(1)\n\t    ntype_indexer=g.node_ntype_indexer\n\t    res_2hops=[]\n\t    res_randoms=[]\n\t    toCsvRepetition=[]\n\t    for re in range(args.repeat):\n\t        print(f\"re : {re} starts!\\n\\n\\n\")\n\t        res_2hop = defaultdict(float)\n", "        res_random = defaultdict(float)\n\t        train_pos, valid_pos = dl.get_train_valid_pos()#edge_types=[test_edge_type])\n\t        # train_pos {etype0: [[...], [...]], etype1: [[...], [...]]}\n\t        # valid_pos {etype0: [[...], [...]], etype1: [[...], [...]]}\n\t        num_classes = args.hidden_dim\n\t        heads = [args.num_heads] * args.num_layers + [args.num_heads]\n\t        num_ntype=len(features_list)\n\t        g.num_ntypes=num_ntype \n\t        eindexer=None\n\t        prod_aggr=args.prod_aggr if args.prod_aggr!=\"None\" else None\n", "        if args.net==\"myGAT\":\n\t            net = myGAT(g, args.edge_feats, len(dl.links['count'])*2+1, in_dims, args.hidden_dim, num_classes, args.num_layers, heads, F.elu , args.dropout_feat,args.dropout_attn, args.slope, eval(args.residual), args.residual_att,decode=args.decoder,dataRecorder=dataRecorder,get_out=get_out) \n\t        elif args.net==\"slotGAT\":\n\t            net = slotGAT(g, args.edge_feats, len(dl.links['count'])*2+1, in_dims, args.hidden_dim, num_classes, args.num_layers, heads, F.elu,  args.dropout_feat,args.dropout_attn, args.slope, eval(args.residual), args.residual_att,  num_ntype,   \n\t                 eindexer,decode=args.decoder,aggregator=args.slot_aggregator,inProcessEmb=args.inProcessEmb,l2BySlot=args.l2BySlot,prod_aggr=prod_aggr,sigmoid=args.sigmoid,l2use=args.l2use,SAattDim=args.SAattDim,dataRecorder=dataRecorder,get_out=get_out,predicted_by_slot=args.predicted_by_slot)\n\t        print(net) if args.verbose==\"True\" else None\n\t        net.to(device)\n\t        epoch_val_loss=0\n\t        val_res_RocAucRandom=0\n\t        val_res_MRRRandom=0\n", "        if args.use_trained==\"True\":\n\t            ckp_fname=os.path.join(args.trained_dir,args.net,args.dataset,str(re),\"model.pt\")\n\t        else:\n\t            optimizer = torch.optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n\t            # training loop\n\t            net.train()\n\t            try:\n\t                if not os.path.exists('checkpoint'):\n\t                    os.mkdir('checkpoint')\n\t                t=time.localtime()\n", "                str_t=f\"{t.tm_year:0>4d}{t.tm_mon:0>2d}{t.tm_mday:0>2d}{t.tm_hour:0>2d}{t.tm_min:0>2d}{t.tm_sec:0>2d}{int(time.time()*1000)%1000}\"\n\t                ckp_dname=os.path.join('checkpoint',str_t)\n\t                os.mkdir(ckp_dname)\n\t            except:\n\t                time.sleep(1)\n\t                t=time.localtime()\n\t                str_t=f\"{t.tm_year:0>4d}{t.tm_mon:0>2d}{t.tm_mday:0>2d}{t.tm_hour:0>2d}{t.tm_min:0>2d}{t.tm_sec:0>2d}{int(time.time()*1000)%1000}\"\n\t                ckp_dname=os.path.join('checkpoint',str_t)\n\t                os.mkdir(ckp_dname)\n\t            if args.save_trained==\"True\":\n", "                # files in save_dir should be considered ***important***\n\t                ckp_fname=os.path.join(args.save_dir,args.net,args.dataset,str(re),\"model.pt\")\n\t                #ckp_fname=os.path.join(args.save_dir,args.study_name,args.net,args.dataset,str(re),\"model.pt\")\n\t                os.makedirs(os.path.dirname(ckp_fname),exist_ok=True)\n\t            else:\n\t                ckp_fname=os.path.join(ckp_dname,'checkpoint_{}_{}_re_{}_feat_{}_heads_{}_{}.pt'.format(args.dataset, args.num_layers,re,args.feats_type,args.num_heads,net.__class__.__name__))\n\t            early_stopping = EarlyStopping(patience=args.patience, verbose=True, save_path=ckp_fname)\n\t            loss_func = nn.BCELoss()\n\t            train_losses=[]\n\t            val_losses=[]\n", "            if args.profile==\"True\":\n\t                profile_func=profile\n\t            elif args.profile==\"False\":\n\t                profile_func=blank_profile\n\t            with profile_func(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True,schedule=torch.profiler.schedule(\n\t                    wait=2,\n\t                    warmup=2,\n\t                    active=6,\n\t                    repeat=1),on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./profiler/trace_\"+args.study_name)) as prof:    \n\t                for epoch in range(args.epoch):\n", "                    train_pos_head_full = np.array([])\n\t                    train_pos_tail_full = np.array([])\n\t                    train_neg_head_full = np.array([])\n\t                    train_neg_tail_full = np.array([])\n\t                    r_id_full = np.array([])\n\t                    for test_edge_type in dl.links_test['data'].keys():\n\t                        train_neg = dl.get_train_neg(edge_types=[test_edge_type])[test_edge_type]\n\t                        # train_neg [[0, 0, 0, 0, 0, 0, 0, 0, 0, ...], [9294, 8277, 4484, 7413, 1883, 5117, 9256, 3106, 636, ...]]\n\t                        train_pos_head_full = np.concatenate([train_pos_head_full, np.array(train_pos[test_edge_type][0])])\n\t                        train_pos_tail_full = np.concatenate([train_pos_tail_full, np.array(train_pos[test_edge_type][1])])\n", "                        train_neg_head_full = np.concatenate([train_neg_head_full, np.array(train_neg[0])])\n\t                        train_neg_tail_full = np.concatenate([train_neg_tail_full, np.array(train_neg[1])])\n\t                        r_id_full = np.concatenate([r_id_full, np.array([test_edge_type]*len(train_pos[test_edge_type][0]))])\n\t                    train_idx = np.arange(len(train_pos_head_full))\n\t                    np.random.shuffle(train_idx)\n\t                    batch_size = args.batch_size\n\t                    epoch_val_loss=0\n\t                    c=0\n\t                    val_res_RocAucRandom=0\n\t                    val_res_MRRRandom=0\n", "                    for step, start in enumerate(range(0, len(train_pos_head_full), args.batch_size)):\n\t                        t_start = time.time()\n\t                        # training\n\t                        net.train()\n\t                        train_pos_head = train_pos_head_full[train_idx[start:start+batch_size]]\n\t                        train_neg_head = train_neg_head_full[train_idx[start:start+batch_size]]\n\t                        train_pos_tail = train_pos_tail_full[train_idx[start:start+batch_size]]\n\t                        train_neg_tail = train_neg_tail_full[train_idx[start:start+batch_size]]\n\t                        r_id = r_id_full[train_idx[start:start+batch_size]]\n\t                        left = np.concatenate([train_pos_head, train_neg_head])  #to get heads embeddings\n", "                        right = np.concatenate([train_pos_tail, train_neg_tail])   #to get tail embeddings\n\t                        mid = np.concatenate([r_id, r_id])  #specify edge types\n\t                        labels = torch.FloatTensor(np.concatenate([np.ones(train_pos_head.shape[0]), np.zeros(train_neg_head.shape[0])])).to(device)\n\t                        with record_function(\"model_inference\"):\n\t                            net.dataRecorder[\"status\"]=\"Training\"\n\t                            logits = net(features_list, e_feat, left, right, mid)\n\t                            net.dataRecorder[\"status\"]=\"None\"\n\t                        logp = logits\n\t                        train_loss = loss_func(logp, labels)\n\t                        # autograd\n", "                        optimizer.zero_grad()\n\t                        with record_function(\"model_backward\"):\n\t                            train_loss.backward()\n\t                            optimizer.step()\n\t                        t_end = time.time()\n\t                        # print training info\n\t                        print('Epoch {:05d}, Step{:05d} | Train_Loss: {:.4f} | Time: {:.4f}'.format(epoch, step, train_loss.item(), t_end-t_start))  if args.verbose==\"True\" else None\n\t                        train_losses.append(train_loss.item())\n\t                        t_start = time.time()\n\t                        # validation\n", "                        net.eval()\n\t                        #print(\"validation!\")\n\t                        with torch.no_grad():\n\t                            valid_pos_head = np.array([])\n\t                            valid_pos_tail = np.array([])\n\t                            valid_neg_head = np.array([])\n\t                            valid_neg_tail = np.array([])\n\t                            valid_r_id = np.array([])\n\t                            for test_edge_type in dl.links_test['data'].keys():\n\t                                valid_neg = dl.get_valid_neg(edge_types=[test_edge_type])[test_edge_type]\n", "                                valid_pos_head = np.concatenate([valid_pos_head, np.array(valid_pos[test_edge_type][0])])\n\t                                valid_pos_tail = np.concatenate([valid_pos_tail, np.array(valid_pos[test_edge_type][1])])\n\t                                valid_neg_head = np.concatenate([valid_neg_head, np.array(valid_neg[0])])\n\t                                valid_neg_tail = np.concatenate([valid_neg_tail, np.array(valid_neg[1])])\n\t                                valid_r_id = np.concatenate([valid_r_id, np.array([test_edge_type]*len(valid_pos[test_edge_type][0]))])\n\t                            left = np.concatenate([valid_pos_head, valid_neg_head])\n\t                            right = np.concatenate([valid_pos_tail, valid_neg_tail])\n\t                            mid = np.concatenate([valid_r_id, valid_r_id])\n\t                            labels = torch.FloatTensor(np.concatenate([np.ones(valid_pos_head.shape[0]), np.zeros(valid_neg_head.shape[0])])).to(device)\n\t                            logits = net(features_list, e_feat, left, right, mid)\n", "                            logp = logits\n\t                            val_loss = loss_func(logp, labels)\n\t                            pred = logits.cpu().numpy()\n\t                            edge_list = np.concatenate([left.reshape((1,-1)), right.reshape((1,-1))], axis=0)\n\t                            labels = labels.cpu().numpy()\n\t                            val_res = dl.evaluate(edge_list, pred, labels)\n\t                            val_res_RocAucRandom+=val_res[\"roc_auc\"]*left.shape[0]\n\t                            val_res_MRRRandom+=val_res[\"MRR\"]*left.shape[0]\n\t                            epoch_val_loss+=val_loss.item()*left.shape[0]\n\t                            c+=left.shape[0]\n", "                        t_end = time.time()\n\t                        # print validation info\n\t                        print('Epoch {:05d} | Val_Loss {:.4f} | Time(s) {:.4f}'.format(\n\t                            epoch, val_loss.item(), t_end - t_start))  if args.verbose==\"True\" else None\n\t                        val_losses.append(val_loss.item())\n\t                        # early stopping\n\t                        early_stopping(val_loss, net)\n\t                        if early_stopping.early_stop:\n\t                            print('Early stopping!')  if args.verbose==\"True\" else None\n\t                            break\n", "                        prof.step()\n\t                    epoch_val_loss=epoch_val_loss/c\n\t                    val_res_RocAucRandom=val_res_RocAucRandom/c\n\t                    val_res_MRRRandom=val_res_MRRRandom/c\n\t        first_flag = True\n\t        for test_edge_type in dl.links_test['data'].keys():\n\t            # testing with evaluate_results_nc\n\t            net.load_state_dict(torch.load(ckp_fname))\n\t            net.eval()\n\t            test_logits = []\n", "            with torch.no_grad():\n\t                test_neigh, test_label = dl.get_test_neigh()\n\t                test_neigh = test_neigh[test_edge_type]\n\t                test_label = test_label[test_edge_type]\n\t                left = np.array(test_neigh[0])\n\t                right = np.array(test_neigh[1])\n\t                mid = np.zeros(left.shape[0], dtype=np.int32)\n\t                mid[:] = test_edge_type\n\t                labels = torch.FloatTensor(test_label).to(device)\n\t                net.dataRecorder[\"status\"]=\"Test2Hop\"\n", "                logits = net(features_list, e_feat, left, right, mid)\n\t                net.dataRecorder[\"status\"]=\"None\"\n\t                pred = logits.cpu().numpy()\n\t                edge_list = np.concatenate([left.reshape((1,-1)), right.reshape((1,-1))], axis=0)\n\t                labels = labels.cpu().numpy()\n\t                first_flag = False\n\t                res = dl.evaluate(edge_list, pred, labels)\n\t                #print(f\"res {res}\")\n\t                for k in res:\n\t                    res_2hop[k] += res[k]\n", "            with torch.no_grad():\n\t                test_neigh, test_label = dl.get_test_neigh_w_random()\n\t                test_neigh = test_neigh[test_edge_type]\n\t                test_label = test_label[test_edge_type]\n\t                left = np.array(test_neigh[0])\n\t                right = np.array(test_neigh[1])\n\t                mid = np.zeros(left.shape[0], dtype=np.int32)\n\t                mid[:] = test_edge_type\n\t                labels = torch.FloatTensor(test_label).to(device)\n\t                net.dataRecorder[\"status\"]=\"TestRandom\"\n", "                logits = net(features_list, e_feat, left, right, mid)\n\t                net.dataRecorder[\"status\"]=\"None\"\n\t                pred = logits.cpu().numpy()\n\t                edge_list = np.concatenate([left.reshape((1,-1)), right.reshape((1,-1))], axis=0)\n\t                labels = labels.cpu().numpy()\n\t                res = dl.evaluate(edge_list, pred, labels)\n\t                #print(f\"res {res}\")\n\t                for k in res:\n\t                    res_random[k] += res[k]\n\t        for k in res_2hop:\n", "            res_2hop[k] /= total\n\t        for k in res_random:\n\t            res_random[k] /= total\n\t        res_2hops.append(res_2hop)\n\t        res_randoms.append(res_random)\n\t        toCsv={ \"1_featType\":feats_type,\n\t            \"1_numLayers\":args.num_layers,\n\t            \"1_hiddenDim\":args.hidden_dim,\n\t            \"1_SAAttDim\":args.SAattDim,\n\t            \"1_numOfHeads\":args.num_heads,\n", "            \"1_numOfEpoch\":args.epoch,\n\t            \"1_Lr\":args.lr,\n\t            \"1_Wd\":args.weight_decay,\n\t            \"1_decoder\":args.decoder,\n\t            \"1_batchSize\":args.batch_size,\n\t            \"1_residual-att\":args.residual_att,\n\t            \"1_residual\":args.residual,\n\t            \"1_dropoutFeat\":args.dropout_feat,\n\t            \"1_dropoutAttn\":args.dropout_attn,\n\t            #\"2_valAcc\":val_results[\"acc\"],\n", "            #\"2_valMiPre\":val_results[\"micro-pre\"],\n\t            \"2_valLossNeg_mean\":-epoch_val_loss,\n\t            \"2_valRocAucRandom_mean\":val_res_RocAucRandom,\n\t            \"2_valMRRRandom_mean\":val_res_MRRRandom,\n\t            \"3_testRocAuc2hop_mean\":res_2hop[\"roc_auc\"],\n\t            \"3_testMRR2hop_mean\":res_2hop[\"MRR\"],\n\t            \"3_testRocAucRandom_mean\":res_random[\"roc_auc\"],\n\t            \"3_testMRRRandom_mean\":res_random[\"MRR\"],\n\t            \"2_valLossNeg_std\":-epoch_val_loss,\n\t            \"2_valRocAucRandom_std\":val_res_RocAucRandom,\n", "            \"2_valMRRRandom_std\":val_res_MRRRandom,\n\t            \"3_testRocAuc2hop_std\":res_2hop[\"roc_auc\"],\n\t            \"3_testMRR2hop_std\":res_2hop[\"MRR\"],\n\t            \"3_testRocAucRandom_std\":res_random[\"roc_auc\"],\n\t            \"3_testMRRRandom_std\":res_random[\"MRR\"],}\n\t        toCsvRepetition.append(toCsv)\n\t    print(f\"res_2hops {res_2hops}\")  if args.verbose==\"True\" else None\n\t    print(f\"res_randoms {res_randoms}\")  if args.verbose==\"True\" else None\n\t    toCsvAveraged={}\n\t    for tocsv in toCsvRepetition:\n", "        for name in tocsv.keys():\n\t            if name.startswith(\"1_\"):\n\t                toCsvAveraged[name]=tocsv[name]\n\t            else:\n\t                if name not in toCsvAveraged.keys():\n\t                    toCsvAveraged[name]=[]\n\t                toCsvAveraged[name].append(tocsv[name])\n\t    print(toCsvAveraged)\n\t    for name in toCsvAveraged.keys():\n\t        if not name.startswith(\"1_\") :\n", "            if type(toCsvAveraged[name][0]) is str:\n\t                toCsvAveraged[name]=toCsvAveraged[name][0]\n\t            else:\n\t                if \"_mean\" in name:\n\t                    toCsvAveraged[name]=np.mean(np.array(toCsvAveraged[name])) \n\t                elif \"_std\" in name:\n\t                    toCsvAveraged[name]= np.std(np.array(toCsvAveraged[name])) \n\t    #toCsvAveraged[\"5_expInfo\"]=exp_info\n\t    writeIntoCsvLogger(toCsvAveraged,f\"./log/{args.study_name}.csv\")\n\t    #########################################\n", "    #####        data preprocess\n\t    #########################################\n\t    if not os.path.exists(f\"./analysis\"):\n\t        os.mkdir(\"./analysis\")\n\t    if not os.path.exists(f\"./analysis/{args.study_name}\"):\n\t        os.mkdir(f\"./analysis/{args.study_name}\")\n\t    if get_out !=['']:\n\t        vis_data_saver.save(os.path.join(f\"./analysis/{args.study_name}\",args.study_name+\".visdata\"))\n\t        #vis_data_saver.save(os.path.join(f\"./analysis/{args.study_name}\",args.study_name+\".visdata\"))\n\tif __name__ == '__main__':\n", "    ap = argparse.ArgumentParser(description='Run Model on LP tasks.')\n\t    ap.add_argument('--feats-type', type=int, default=3,\n\t                    help='Type of the node features used. ' +\n\t                         '0 - loaded features; ' +\n\t                         '1 - only target node features (zero vec for others); ' +\n\t                         '2 - only target node features (id vec for others); ' +\n\t                         '3 - all id vec. Default is 2;' +\n\t                        '4 - only term features (id vec for others);' + \n\t                        '5 - only term features (zero vec for others).')\n\t    ap.add_argument('--seed', type=int, default=111, help='Random seed.')\n", "    ap.add_argument('--use_trained', type=str, default=\"False\")\n\t    ap.add_argument('--trained_dir', type=str, default=\"outputs\")\n\t    ap.add_argument('--save_trained', type=str, default=\"False\")\n\t    ap.add_argument('--save_dir', type=str, default=\"outputs\")\n\t    ap.add_argument('--hidden-dim', type=int, default=64, help='Dimension of the node hidden state. Default is 64.')\n\t    ap.add_argument('--num-heads', type=int, default=2, help='Number of the attention heads. Default is 8.')\n\t    ap.add_argument('--epoch', type=int, default=300, help='Number of epochs.')\n\t    ap.add_argument('--patience', type=int, default=40, help='Patience.')\n\t    ap.add_argument('--num-layers', type=int, default=2)\n\t    ap.add_argument('--lr', type=float, default=5e-4)\n", "    ap.add_argument('--dropout_feat', type=float, default=0.5)\n\t    ap.add_argument('--dropout_attn', type=float, default=0.5)\n\t    ap.add_argument('--weight-decay', type=float, default=1e-4)\n\t    ap.add_argument('--slope', type=float, default=0.01)\n\t    ap.add_argument('--dataset', type=str)\n\t    ap.add_argument('--net', type=str, default='myGAT')\n\t    ap.add_argument('--gpu', type=str, default=\"0\")\n\t    ap.add_argument('--verbose', type=str, default='False')\n\t    ap.add_argument('--edge-feats', type=int, default=32)\n\t    ap.add_argument('--batch-size', type=int, default=1024)\n", "    ap.add_argument('--decoder', type=str, default='distmult')  \n\t    ap.add_argument('--inProcessEmb', type=str, default='True')\n\t    ap.add_argument('--l2BySlot', type=str, default='True')\n\t    ap.add_argument('--l2use', type=str, default='True')\n\t    ap.add_argument('--prod_aggr', type=str, default='None')\n\t    ap.add_argument('--sigmoid', type=str, default='after') \n\t    ap.add_argument('--get_out', default=\"\")  \n\t    ap.add_argument('--profile', default=\"False\")  \n\t    ap.add_argument('--run', type=int, default=1)\n\t    ap.add_argument('--cost', type=int, default=1)\n", "    ap.add_argument('--repeat', type=int, default=10, help='Repeat the training and testing for N times. Default is 1.')\n\t    ap.add_argument('--task_property', type=str, default=\"notSpecified\")\n\t    ap.add_argument('--study_name', type=str, default=\"temp\")\n\t    ap.add_argument('--residual_att', type=float, default=0.05)\n\t    ap.add_argument('--residual', type=str, default=\"True\")\n\t    ap.add_argument('--SAattDim', type=int, default=128)\n\t    ap.add_argument('--slot_aggregator', type=str, default=\"SA\") \n\t    ap.add_argument('--predicted_by_slot', type=str, default=\"None\") \n\t    args = ap.parse_args()\n\t    run_model_DBLP(args)\n"]}
{"filename": "LP/methods/slotGAT/run_train_slotGAT_on_all_dataset.py", "chunked_list": ["import time\n\timport subprocess\n\timport multiprocessing\n\tfrom threading import main_thread\n\tfrom pipeline_utils import config_study_name,Run\n\timport os \n\tresources_dict={\"0\":1,\"1\":1}   #id:load\n\t#dataset_to_evaluate=[(\"pubmed_HNE_LP\",1,5)]   # dataset,cost,repeat\n\tprefix=\"get_results\";specified_args=[\"dataset\",   \"net\"]\n\tstart_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n", "fixed_info_by_selected_keys={\n\t    \"slotGAT\":{\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\",\"inProcessEmb\":\"True\",\n\t                        \"use_trained\":\"False\",\n\t                        #\"trained_dir\":\"outputs\",\n\t                        \"save_trained\":\"True\",\n\t                        \"save_dir\":\"outputs\",\n\t                        } }\n\tdataset_and_hypers={\n\t    (\"PubMed_LP\",1,5):\n\t        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[4]\",\"lr\":\"[1e-3]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"distmult\"],\"batch-size\":[8192,],\"dropout_feat\":[0.5],\"dropout_attn\":[0.5],\"residual_att\":[0.2],\"residual\":[\"True\"],\"SAattDim\":[32]},\n", "    (\"LastFM\",1,5):\n\t        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[8]\",\"lr\":\"[5e-4]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"dot\"],\"batch-size\":[8192,],\"SAattDim\":[64],\"dropout_feat\":[0.2],\"dropout_attn\":[0.9],\"residual_att\":[0.5],\"residual\":[\"True\"]}\n\t    }\n\tdef getTasks(fixed_info,dataset_and_hypers):\n\t    for k,v in dataset_and_hypers.items():\n\t        for k1,v1 in v.items():\n\t            if \"search_\" in k1:\n\t                if type(v1)!=str:\n\t                    v[k1]=f\"[{v1}]\"\n\t            # if list str, get the first one\n", "            if  type(v1)==list:\n\t                v[k1]=v1[0]\n\t            if type(v1)==str:\n\t                if v1[0]==\"[\" and v1[-1]==\"]\":\n\t                    v[k1]=eval(v1)[0]\n\t    tasks_list=[]\n\t    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n\t        args_dict={}\n\t        for dict_to_add in [task,fixed_info]:\n\t            for k,v in dict_to_add.items():\n", "                args_dict[k]=v\n\t        net=args_dict['net']\n\t        args_dict['dataset']=dataset\n\t        #args_dict['trial_num']=trial_num\n\t        args_dict['repeat']=repeat\n\t        study_name=config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n\t        args_dict['study_name']=study_name\n\t        args_dict['cost']=cost\n\t        tasks_list.append(args_dict)\n\t    print(\"tasks_list:\", tasks_list)\n", "    return tasks_list \n\ttasks_list=[]\n\tfor k in fixed_info_by_selected_keys.keys():\n\t    tasks_list.extend(getTasks(fixed_info_by_selected_keys[k],dataset_and_hypers))\n\tresources=resources_dict.keys()\n\tpool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\n\tfor i in resources:\n\t    for j in range(resources_dict[i]):\n\t        pool.put(i+str(j))\n\tsub_queues=[]\n", "items=len(tasks_list)%60\n\tfor i in range(items):\n\t    sub_queues.append(tasks_list[60*i:(60*i+60)])\n\tsub_queues.append(tasks_list[(60*items+60):])\n\tif items==0:\n\t    sub_queues.append(tasks_list)\n\t## split the tasks, or it may exceeds of maximal size of sub-processes of OS.\n\tidx=0\n\ttc=len(tasks_list)\n\tfor sub_tasks_list in sub_queues:\n", "    process_queue=[]\n\t    for i in range(len(sub_tasks_list)):\n\t        idx+=1\n\t        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n\t        p.daemon=True\n\t        p.start()\n\t        process_queue.append(p)\n\t    for p in process_queue:\n\t        p.join()\n\tprint('end all')\n", "end_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\tprint(f\"Start time: {start_time}\\nEnd time: {end_time}\\n\")\n"]}
{"filename": "LP/methods/slotGAT/run_use_slotGAT_on_all_dataset.py", "chunked_list": ["import time \n\timport multiprocessing\n\tfrom threading import main_thread\n\tfrom pipeline_utils import config_study_name,Run\n\timport os \n\t#time.sleep(60*60*4)\n\tif not os.path.exists(\"outputs\"):\n\t    os.mkdir(\"outputs\")\n\tif not os.path.exists(\"log\"):\n\t    os.mkdir(\"log\")\n", "if not os.path.exists(\"checkpoint\"):\n\t    os.mkdir(\"checkpoint\")\n\tresources_dict={\"0\":1,\"1\":1}   #id:load\n\t#dataset_to_evaluate=[(\"pubmed_HNE_LP\",1,5)]   # dataset,cost,repeat\n\tprefix=\"get_results_trained\";specified_args=[\"dataset\",   \"net\"]\n\tstart_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\tfixed_info_by_selected_keys={\n\t    \"slotGAT\":{\"task_property\":prefix,\"net\":\"slotGAT\",\"slot_aggregator\":\"SA\",\"inProcessEmb\":\"True\",\n\t                        \"use_trained\":\"True\",\n\t                        \"trained_dir\":\"outputs\",\n", "                        \"save_trained\":\"False\",\n\t                        #\"save_dir\":\"outputs\",\n\t                        } }\n\tdataset_and_hypers={\n\t    (\"PubMed_LP\",1,5):\n\t        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[4]\",\"lr\":\"[1e-3]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"distmult\"],\"batch-size\":[8192,],\"dropout_feat\":[0.5],\"dropout_attn\":[0.5],\"residual_att\":[0.2],\"residual\":[\"True\"],\"SAattDim\":[32]},\n\t    (\"LastFM\",1,5):\n\t        {\"hidden-dim\":\"[64]\",\"num-layers\":\"[8]\",\"lr\":\"[5e-4]\",\"weight-decay\":\"[1e-4]\",\"feats-type\":[2],\"num-heads\":[2],\"epoch\":[1000],\"decoder\":[\"dot\"],\"batch-size\":[8192,],\"SAattDim\":[64],\"dropout_feat\":[0.2],\"dropout_attn\":[0.9],\"residual_att\":[0.5],\"residual\":[\"True\"]}\n\t    }\n\tdef getTasks(fixed_info,dataset_and_hypers):\n", "    for k,v in dataset_and_hypers.items():\n\t        for k1,v1 in v.items():\n\t            if \"search_\" in k1:\n\t                if type(v1)!=str:\n\t                    v[k1]=f\"[{v1}]\"\n\t            # if list str, get the first one\n\t            if  type(v1)==list:\n\t                v[k1]=v1[0]\n\t            if type(v1)==str:\n\t                if v1[0]==\"[\" and v1[-1]==\"]\":\n", "                    v[k1]=eval(v1)[0]\n\t    tasks_list=[]\n\t    for (dataset,cost,repeat),(task) in dataset_and_hypers.items():\n\t        args_dict={}\n\t        for dict_to_add in [task,fixed_info]:\n\t            for k,v in dict_to_add.items():\n\t                args_dict[k]=v\n\t        net=args_dict['net']\n\t        args_dict['dataset']=dataset\n\t        #args_dict['trial_num']=trial_num\n", "        args_dict['repeat']=repeat\n\t        study_name =config_study_name(prefix=prefix,specified_args=specified_args,extract_dict=args_dict)\n\t        args_dict['study_name']=study_name \n\t        args_dict['cost']=cost\n\t        tasks_list.append(args_dict)\n\t    print(\"tasks_list:\", tasks_list)\n\t    return tasks_list \n\ttasks_list=[]\n\tfor k in fixed_info_by_selected_keys.keys():\n\t    tasks_list.extend(getTasks(fixed_info_by_selected_keys[k],dataset_and_hypers))\n", "resources=resources_dict.keys()\n\tpool=multiprocessing.Queue( sum([  v  for k,v in resources_dict.items()   ])  )\n\tfor i in resources:\n\t    for j in range(resources_dict[i]):\n\t        pool.put(i+str(j))\n\tsub_queues=[]\n\titems=len(tasks_list)%60\n\tfor i in range(items):\n\t    sub_queues.append(tasks_list[60*i:(60*i+60)])\n\tsub_queues.append(tasks_list[(60*items+60):])\n", "if items==0:\n\t    sub_queues.append(tasks_list)\n\t## split the tasks, or it may exceeds of maximal size of sub-processes of OS.\n\tidx=0\n\ttc=len(tasks_list)\n\tfor sub_tasks_list in sub_queues:\n\t    process_queue=[]\n\t    for i in range(len(sub_tasks_list)):\n\t        idx+=1\n\t        p=Run(sub_tasks_list[i],idx=idx,tc=tc,pool=pool,start_time=start_time)\n", "        p.daemon=True\n\t        p.start()\n\t        process_queue.append(p)\n\t    for p in process_queue:\n\t        p.join()\n\tprint('end all')\n\tend_time=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\tprint(f\"Start time: {start_time}\\nEnd time: {end_time}\\n\")\n"]}
{"filename": "LP/methods/slotGAT/utils/pytorchtools.py", "chunked_list": ["import numpy as np\n\timport torch\n\tclass EarlyStopping:\n\t    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\t    def __init__(self, patience, verbose=False, delta=0, save_path='checkpoint.pt'):\n\t        \"\"\"\n\t        Args:\n\t            patience (int): How long to wait after last time validation loss improved.\n\t                            Default: 7\n\t            verbose (bool): If True, prints a message for each validation loss improvement.\n", "                            Default: False\n\t            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n\t                            Default: 0\n\t        \"\"\"\n\t        self.patience = patience\n\t        self.verbose = verbose\n\t        self.counter = 0\n\t        self.best_score = None\n\t        self.early_stop = False\n\t        self.val_loss_min = np.Inf\n", "        self.delta = delta\n\t        self.save_path = save_path\n\t    def __call__(self, val_loss, model):\n\t        score = -val_loss\n\t        if self.best_score is None:\n\t            self.best_score = score\n\t            self.save_checkpoint(val_loss, model)\n\t        elif score < self.best_score - self.delta:\n\t            self.counter += 1\n\t            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n", "            if self.counter >= self.patience:\n\t                self.early_stop = True\n\t        else:\n\t            self.best_score = score\n\t            self.save_checkpoint(val_loss, model)\n\t            self.counter = 0\n\t    def save_checkpoint(self, val_loss, model):\n\t        \"\"\"Saves model when validation loss decrease.\"\"\"\n\t        if self.verbose:\n\t            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n", "        torch.save(model.state_dict(), self.save_path)\n\t        self.val_loss_min = val_loss\n"]}
{"filename": "LP/methods/slotGAT/utils/__init__.py", "chunked_list": []}
{"filename": "LP/methods/slotGAT/utils/tools.py", "chunked_list": ["import torch\n\timport dgl\n\timport numpy as np\n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.metrics import f1_score, normalized_mutual_info_score, adjusted_rand_score\n\tfrom sklearn.cluster import KMeans\n\tfrom sklearn.svm import LinearSVC\n\timport os\n\timport json\n\tclass blank_profile():\n", "    def __init__(self,*args,**kwargs):\n\t        pass\n\t    def __enter__(self,*args,**kwargs):\n\t        return self\n\t    def __exit__(self,*args,**kwargs):\n\t        pass\n\t    def step(self):\n\t        pass\n\tclass vis_data_collector():\n\t    #all data must be simple python objects like int or 'str'\n", "    def __init__(self):\n\t        self.data_dict={}\n\t        self.tensor_dict={}\n\t        #formatting:\n\t        #\n\t        # {\"meta\":{****parameters and study name},\"re-1\":{\"epoch-0\":{\"loss\":0.1,\"w1\":1},\"epoch-1\":{\"loss\":0.2,\"w1\":2},...}}\n\t    def save_meta(self,meta_data,meta_name):\n\t        self.data_dict[\"meta\"]={meta_name:meta_data}\n\t        self.data_dict[\"meta\"][\"tensor_names\"]=[]\n\t    def collect_in_training(self,data,name,re,epoch,r=4):\n", "        if f\"re-{re}\" not in self.data_dict.keys():\n\t            self.data_dict[f\"re-{re}\" ]={}\n\t        if f\"epoch-{epoch}\" not in self.data_dict[f\"re-{re}\" ].keys():\n\t            self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"]={}\n\t        if type(data)==float:\n\t            data=round(data,r)\n\t        self.data_dict[f\"re-{re}\" ][f\"epoch-{epoch}\"][name]=data\n\t    def collect_in_run(self,data,name,re,r=4):\n\t        if f\"re-{re}\" not in self.data_dict.keys():\n\t            self.data_dict[f\"re-{re}\" ]={}\n", "        if type(data)==float:\n\t            data=round(data,r)\n\t        self.data_dict[f\"re-{re}\" ][name]=data\n\t    def collect_whole_process(self,data,name):\n\t        self.data_dict[name]=data\n\t    def collect_whole_process_tensor(self,data,name):\n\t        self.tensor_dict[name]=data\n\t        self.data_dict[\"meta\"][\"tensor_names\"].append(name)\n\t    def save(self,fn):\n\t        f = open(fn+\".json\", 'w')\n", "        json.dump(self.data_dict, f, indent=4)\n\t        f.close()\n\t        for k,v in self.tensor_dict.items():\n\t            torch.save(v,fn+\"_\"+k+\".pt\")\n\t    def load(self,fn):\n\t        f = open(fn, 'r')\n\t        self.data_dict= json.load(f)\n\t        f.close()\n\t        for name in self.data_dict[\"meta\"][\"tensor_names\"]:\n\t            self.tensor_dict[name]=torch.load(name+\".pt\")\n", "    def trans_to_numpy(self,name,epoch_range=None):\n\t        data=[]\n\t        re=0\n\t        while f\"re-{re}\" in self.data_dict.keys():\n\t            data.append([])\n\t            for i in range(epoch_range):\n\t                data[re].append(self.data_dict[f\"re-{re}\"][f\"epoch-{i}\"][name])\n\t            re+=1\n\t        data=np.array(data)\n\t        return np.mean(data,axis=0),np.std(data,axis=0)\n", "    def visualize_tsne(self,dn,node_idx_by_ntype):\n\t        from matplotlib.pyplot import figure\n\t        figure(figsize=(16, 9), dpi=80)\n\t        ncs=[\"r\",\"b\",\"y\",\"g\",\"chocolate\",\"deeppink\"]\n\t        print(dn)\n\t        layers=[]\n\t        heads=[]\n\t        ets=[]\n\t        for k,v in self.data_dict.items():\n\t            if \"attention_hist_layer\" in k:\n", "                temp=k.split(\"_\")\n\t                if int(temp[3]) not in layers:\n\t                    layers.append(int(temp[3]))\n\t                if temp[4]==\"head\":\n\t                    if int(temp[5]) not in heads:\n\t                        heads.append(int(temp[5]))\n\t                if temp[4]==\"et\":\n\t                    if int(temp[5]) not in ets:\n\t                        ets.append(int(temp[5]))\n\t        layers,heads,ets=sorted(layers),sorted(heads),sorted(ets)\n", "        #print(layers,heads,ets)\n\t        #heads plot\n\t        for layer in layers:\n\t            fig,ax=plt.subplots(2,int((len(node_idx_by_ntype)+1)/2))\n\t            fig.set_size_inches(16,9)\n\t            fig.set_dpi(100)\n\t            nts=list(range(len(node_idx_by_ntype)))\n\t            for nt in nts:\n\t                subax=ax[int((nt)/(len(nts)/2))][int((nt)%(len(nts)/2))]\n\t                subax.cla()\n", "                datas=np.array(self.data_dict[f\"tsne_emb_layer_{layer}_slot_{nt}\"]).T\n\t                print(datas.shape)\n\t                for nt_j in nts:\n\t                    x=datas[0][ node_idx_by_ntype[nt_j]]\n\t                    y=datas[1][ node_idx_by_ntype[nt_j]]\n\t                    subax.scatter(x=x,y=y,c=ncs[nt_j],s=0.1,label=f\"type {nt_j} with center ({np.mean(x):.2f},{np.mean(y):.2f}) and radius {(np.std(x)+np.std(y))/2:.2f}\")\n\t                subax.set_xlim(-100,100)\n\t                subax.set_ylim(-100,100)\n\t                subax.set_title(f\"layer_{layer}_slot_{nt}\")\n\t                plt.title(f\"layer_{layer}_slot_{nt}\")\n", "                lgnd=subax.legend()\n\t                for lh in lgnd.legendHandles:\n\t                    lh._sizes=[10]\n\t            fig.suptitle(f\"embedding_tsne_layer_{layer}\")\n\t            plt.savefig(os.path.join(dn,f\"slot_embeddings_layer_{layer}.png\"))\n\tdef count_torch_tensor(t):\n\t    t=t.flatten(0).cpu()\n\t    c={}\n\t    for n in t:\n\t        n=n.item()\n", "        if n not in c:\n\t            c[n]=0\n\t        c[n]+=1\n\t    c=sorted(list(c.items()),key=lambda x:x[0])\n\t    return c\n\tdef strList(l):\n\t    return [str(x) for x in l]\n\tdef writeIntoCsvLogger(dictToWrite,file_name):\n\t    #read file\n\t    to_write_names=sorted(list(dictToWrite.keys()))\n", "    if not os.path.exists(file_name):\n\t        to_write_line=[]\n\t        for n in to_write_names:\n\t            to_write_line.append(dictToWrite[n])\n\t        with open(file_name,\"w\") as f:\n\t            f.write(  \",\".join(strList(to_write_names)) +\"\\n\")\n\t            f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n\t    else:\n\t        with open(file_name,\"r\") as f:\n\t            rec=[]\n", "            for line in f:\n\t                line=line.strip(\"\\n\").split(\",\")\n\t                rec.append(line)\n\t        #ensure they have same names\n\t        row_names=sorted(rec[0])\n\t        if to_write_names!=row_names:\n\t            collected_names_not_in=[]\n\t            for n in to_write_names:\n\t                if n not in row_names:\n\t                    for i,n_r in enumerate(rec):\n", "                        if i==0:\n\t                            rec[0].append(n)\n\t                        else:\n\t                            rec[i].append(\"\")\n\t                row_names.append(n)\n\t            for n_r in row_names:\n\t                if n_r not in to_write_names:\n\t                    dictToWrite[n_r]=\"\"\n\t                    to_write_names.append(n_r)\n\t            to_write_line=[]\n", "            for n in rec[0]:\n\t                to_write_line.append(dictToWrite[n])\n\t            rec.append(to_write_line)\n\t            with open(file_name,\"w\") as f:\n\t                for line_list in rec:\n\t                    f.write(  \",\".join(strList(line_list)) +\"\\n\")\n\t        else:\n\t            to_write_line=[]\n\t            for n in rec[0]:\n\t                to_write_line.append(dictToWrite[n])\n", "            with open(file_name,\"a\") as f:\n\t                f.write(  \",\".join(strList(to_write_line)) +\"\\n\")\n\t        re_order_csv(file_name)\n\tdef re_order_csv(file_name):\n\t    with open(file_name,\"r\") as f:\n\t        rec=[]\n\t        for line in f:\n\t            line=line.strip(\"\\n\").split(\",\")\n\t            rec.append(line)\n\t    row_names=sorted(enumerate(rec[0]),key=lambda x:x[1])\n", "    row_names_idx=[i[0] for i in row_names]    \n\t    row_names_=[i[1] for i in row_names]    \n\t    if row_names_idx==sorted(row_names_idx):\n\t        print(\"No need to reorder\")\n\t        return None\n\t    else:\n\t        print(\"reordering\")\n\t        with open(file_name,\"w\") as f:\n\t            for line_list in rec:\n\t                to_write_line=[ line_list[row_names_idx[i]]  for i in range(len(line_list))  ]\n", "                f.write(  \",\".join(to_write_line) +\"\\n\")\n\tdef idx_to_one_hot(idx_arr):\n\t    one_hot = np.zeros((idx_arr.shape[0], idx_arr.max() + 1))\n\t    one_hot[np.arange(idx_arr.shape[0]), idx_arr] = 1\n\t    return one_hot\n\tdef kmeans_test(X, y, n_clusters, repeat=10):\n\t    nmi_list = []\n\t    ari_list = []\n\t    for _ in range(repeat):\n\t        kmeans = KMeans(n_clusters=n_clusters)\n", "        y_pred = kmeans.fit_predict(X)\n\t        nmi_score = normalized_mutual_info_score(y, y_pred, average_method='arithmetic')\n\t        ari_score = adjusted_rand_score(y, y_pred)\n\t        nmi_list.append(nmi_score)\n\t        ari_list.append(ari_score)\n\t    return np.mean(nmi_list), np.std(nmi_list), np.mean(ari_list), np.std(ari_list)\n\tdef svm_test(X, y, test_sizes=(0.2, 0.4, 0.6, 0.8), repeat=10):\n\t    random_states = [182318 + i for i in range(repeat)]\n\t    result_macro_f1_list = []\n\t    result_micro_f1_list = []\n", "    for test_size in test_sizes:\n\t        macro_f1_list = []\n\t        micro_f1_list = []\n\t        for i in range(repeat):\n\t            X_train, X_test, y_train, y_test = train_test_split(\n\t                X, y, test_size=test_size, shuffle=True, random_state=random_states[i])\n\t            svm = LinearSVC(dual=False)\n\t            svm.fit(X_train, y_train)\n\t            y_pred = svm.predict(X_test)\n\t            macro_f1 = f1_score(y_test, y_pred, average='macro')\n", "            micro_f1 = f1_score(y_test, y_pred, average='micro')\n\t            macro_f1_list.append(macro_f1)\n\t            micro_f1_list.append(micro_f1)\n\t        result_macro_f1_list.append((np.mean(macro_f1_list), np.std(macro_f1_list)))\n\t        result_micro_f1_list.append((np.mean(micro_f1_list), np.std(micro_f1_list)))\n\t    return result_macro_f1_list, result_micro_f1_list\n\tdef evaluate_results_nc(embeddings, labels, num_classes):\n\t    print('SVM test')\n\t    svm_macro_f1_list, svm_micro_f1_list = svm_test(embeddings, labels)\n\t    print('Macro-F1: ' + ', '.join(['{:.6f}~{:.6f} ({:.1f})'.format(macro_f1_mean, macro_f1_std, train_size) for\n", "                                    (macro_f1_mean, macro_f1_std), train_size in\n\t                                    zip(svm_macro_f1_list, [0.8, 0.6, 0.4, 0.2])]))\n\t    print('Micro-F1: ' + ', '.join(['{:.6f}~{:.6f} ({:.1f})'.format(micro_f1_mean, micro_f1_std, train_size) for\n\t                                    (micro_f1_mean, micro_f1_std), train_size in\n\t                                    zip(svm_micro_f1_list, [0.8, 0.6, 0.4, 0.2])]))\n\t    print('K-means test')\n\t    nmi_mean, nmi_std, ari_mean, ari_std = kmeans_test(embeddings, labels, num_classes)\n\t    print('NMI: {:.6f}~{:.6f}'.format(nmi_mean, nmi_std))\n\t    print('ARI: {:.6f}~{:.6f}'.format(ari_mean, ari_std))\n\t    return svm_macro_f1_list, svm_micro_f1_list, nmi_mean, nmi_std, ari_mean, ari_std\n", "def parse_adjlist(adjlist, edge_metapath_indices, samples=None):\n\t    edges = []\n\t    nodes = set()\n\t    result_indices = []\n\t    for row, indices in zip(adjlist, edge_metapath_indices):\n\t        row_parsed = list(map(int, row.split(' ')))\n\t        nodes.add(row_parsed[0])\n\t        if len(row_parsed) > 1:\n\t            # sampling neighbors\n\t            if samples is None:\n", "                neighbors = row_parsed[1:]\n\t                result_indices.append(indices)\n\t            else:\n\t                # undersampling frequent neighbors\n\t                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n\t                p = []\n\t                for count in counts:\n\t                    p += [(count ** (3 / 4)) / count] * count\n\t                p = np.array(p)\n\t                p = p / p.sum()\n", "                samples = min(samples, len(row_parsed) - 1)\n\t                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n\t                neighbors = [row_parsed[i + 1] for i in sampled_idx]\n\t                result_indices.append(indices[sampled_idx])\n\t        else:\n\t            neighbors = []\n\t            result_indices.append(indices)\n\t        for dst in neighbors:\n\t            nodes.add(dst)\n\t            edges.append((row_parsed[0], dst))\n", "    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n\t    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n\t    result_indices = np.vstack(result_indices)\n\t    return edges, result_indices, len(nodes), mapping\n\tdef parse_minibatch(adjlists, edge_metapath_indices_list, idx_batch, device, samples=None):\n\t    g_list = []\n\t    result_indices_list = []\n\t    idx_batch_mapped_list = []\n\t    for adjlist, indices in zip(adjlists, edge_metapath_indices_list):\n\t        edges, result_indices, num_nodes, mapping = parse_adjlist(\n", "            [adjlist[i] for i in idx_batch], [indices[i] for i in idx_batch], samples)\n\t        g = dgl.DGLGraph(multigraph=True)\n\t        g.add_nodes(num_nodes)\n\t        if len(edges) > 0:\n\t            sorted_index = sorted(range(len(edges)), key=lambda i : edges[i])\n\t            g.add_edges(*list(zip(*[(edges[i][1], edges[i][0]) for i in sorted_index])))\n\t            result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n\t        else:\n\t            result_indices = torch.LongTensor(result_indices).to(device)\n\t        #g.add_edges(*list(zip(*[(dst, src) for src, dst in sorted(edges)])))\n", "        #result_indices = torch.LongTensor(result_indices).to(device)\n\t        g_list.append(g)\n\t        result_indices_list.append(result_indices)\n\t        idx_batch_mapped_list.append(np.array([mapping[idx] for idx in idx_batch]))\n\t    return g_list, result_indices_list, idx_batch_mapped_list\n\tdef parse_adjlist_LastFM(adjlist, edge_metapath_indices, samples=None, exclude=None, offset=None, mode=None):\n\t    edges = []\n\t    nodes = set()\n\t    result_indices = []\n\t    for row, indices in zip(adjlist, edge_metapath_indices):\n", "        row_parsed = list(map(int, row.split(' ')))\n\t        nodes.add(row_parsed[0])\n\t        if len(row_parsed) > 1:\n\t            # sampling neighbors\n\t            if samples is None:\n\t                if exclude is not None:\n\t                    if mode == 0:\n\t                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[:, [0, 1, -1, -2]]]\n\t                    else:\n\t                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[:, [0, 1, -1, -2]]]\n", "                    neighbors = np.array(row_parsed[1:])[mask]\n\t                    result_indices.append(indices[mask])\n\t                else:\n\t                    neighbors = row_parsed[1:]\n\t                    result_indices.append(indices)\n\t            else:\n\t                # undersampling frequent neighbors\n\t                unique, counts = np.unique(row_parsed[1:], return_counts=True)\n\t                p = []\n\t                for count in counts:\n", "                    p += [(count ** (3 / 4)) / count] * count\n\t                p = np.array(p)\n\t                p = p / p.sum()\n\t                samples = min(samples, len(row_parsed) - 1)\n\t                sampled_idx = np.sort(np.random.choice(len(row_parsed) - 1, samples, replace=False, p=p))\n\t                if exclude is not None:\n\t                    if mode == 0:\n\t                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for u1, a1, u2, a2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n\t                    else:\n\t                        mask = [False if [u1, a1 - offset] in exclude or [u2, a2 - offset] in exclude else True for a1, u1, a2, u2 in indices[sampled_idx][:, [0, 1, -1, -2]]]\n", "                    neighbors = np.array([row_parsed[i + 1] for i in sampled_idx])[mask]\n\t                    result_indices.append(indices[sampled_idx][mask])\n\t                else:\n\t                    neighbors = [row_parsed[i + 1] for i in sampled_idx]\n\t                    result_indices.append(indices[sampled_idx])\n\t        else:\n\t            neighbors = [row_parsed[0]]\n\t            indices = np.array([[row_parsed[0]] * indices.shape[1]])\n\t            if mode == 1:\n\t                indices += offset\n", "            result_indices.append(indices)\n\t        for dst in neighbors:\n\t            nodes.add(dst)\n\t            edges.append((row_parsed[0], dst))\n\t    mapping = {map_from: map_to for map_to, map_from in enumerate(sorted(nodes))}\n\t    edges = list(map(lambda tup: (mapping[tup[0]], mapping[tup[1]]), edges))\n\t    result_indices = np.vstack(result_indices)\n\t    return edges, result_indices, len(nodes), mapping\n\tdef parse_minibatch_LastFM(adjlists_ua, edge_metapath_indices_list_ua, user_artist_batch, device, samples=None, use_masks=None, offset=None):\n\t    g_lists = [[], []]\n", "    result_indices_lists = [[], []]\n\t    idx_batch_mapped_lists = [[], []]\n\t    for mode, (adjlists, edge_metapath_indices_list) in enumerate(zip(adjlists_ua, edge_metapath_indices_list_ua)):\n\t        for adjlist, indices, use_mask in zip(adjlists, edge_metapath_indices_list, use_masks[mode]):\n\t            if use_mask:\n\t                edges, result_indices, num_nodes, mapping = parse_adjlist_LastFM(\n\t                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, user_artist_batch, offset, mode)\n\t            else:\n\t                edges, result_indices, num_nodes, mapping = parse_adjlist_LastFM(\n\t                    [adjlist[row[mode]] for row in user_artist_batch], [indices[row[mode]] for row in user_artist_batch], samples, offset=offset, mode=mode)\n", "            g = dgl.DGLGraph(multigraph=True)\n\t            g.add_nodes(num_nodes)\n\t            if len(edges) > 0:\n\t                sorted_index = sorted(range(len(edges)), key=lambda i : edges[i])\n\t                g.add_edges(*list(zip(*[(edges[i][1], edges[i][0]) for i in sorted_index])))\n\t                result_indices = torch.LongTensor(result_indices[sorted_index]).to(device)\n\t            else:\n\t                result_indices = torch.LongTensor(result_indices).to(device)\n\t            g_lists[mode].append(g)\n\t            result_indices_lists[mode].append(result_indices)\n", "            idx_batch_mapped_lists[mode].append(np.array([mapping[row[mode]] for row in user_artist_batch]))\n\t    return g_lists, result_indices_lists, idx_batch_mapped_lists\n\tclass index_generator:\n\t    def __init__(self, batch_size, num_data=None, indices=None, shuffle=True):\n\t        if num_data is not None:\n\t            self.num_data = num_data\n\t            self.indices = np.arange(num_data)\n\t        if indices is not None:\n\t            self.num_data = len(indices)\n\t            self.indices = np.copy(indices)\n", "        self.batch_size = batch_size\n\t        self.iter_counter = 0\n\t        self.shuffle = shuffle\n\t        if shuffle:\n\t            np.random.shuffle(self.indices)\n\t    def next(self):\n\t        if self.num_iterations_left() <= 0:\n\t            self.reset()\n\t        self.iter_counter += 1\n\t        return np.copy(self.indices[(self.iter_counter - 1) * self.batch_size:self.iter_counter * self.batch_size])\n", "    def num_iterations(self):\n\t        return int(np.ceil(self.num_data / self.batch_size))\n\t    def num_iterations_left(self):\n\t        return self.num_iterations() - self.iter_counter\n\t    def reset(self):\n\t        if self.shuffle:\n\t            np.random.shuffle(self.indices)\n\t        self.iter_counter = 0\n"]}
{"filename": "LP/methods/slotGAT/utils/data.py", "chunked_list": ["import networkx as nx\n\timport numpy as np\n\timport scipy\n\timport pickle\n\timport scipy.sparse as sp\n\tdef load_data(prefix='DBLP'):\n\t    from scripts.data_loader import data_loader\n\t    dl = data_loader('../../data/'+prefix)\n\t    features = []\n\t    for i in range(len(dl.nodes['count'])):\n", "        th = dl.nodes['attr'][i]\n\t        if th is None:\n\t            features.append(sp.eye(dl.nodes['count'][i]))\n\t        else:\n\t            features.append(th)\n\t    adjM = sum(dl.links['data'].values())\n\t    return features,\\\n\t           adjM, \\\n\t            dl\n"]}
{"filename": "LP/methods/slotGAT/utils/preprocess.py", "chunked_list": ["import numpy as np\n\timport scipy.sparse\n\timport networkx as nx\n\tdef get_metapath_adjacency_matrix(adjM, type_mask, metapath):\n\t    \"\"\"\n\t    :param M: the raw adjacency matrix\n\t    :param type_mask: an array of types of all node\n\t    :param metapath\n\t    :return: a list of metapath-based adjacency matrices\n\t    \"\"\"\n", "    out_adjM = scipy.sparse.csr_matrix(adjM[np.ix_(type_mask == metapath[0], type_mask == metapath[1])])\n\t    for i in range(1, len(metapath) - 1):\n\t        out_adjM = out_adjM.dot(scipy.sparse.csr_matrix(adjM[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])]))\n\t    return out_adjM.toarray()\n\t# networkx.has_path may search too\n\tdef get_metapath_neighbor_pairs(M, type_mask, expected_metapaths):\n\t    \"\"\"\n\t    :param M: the raw adjacency matrix\n\t    :param type_mask: an array of types of all node\n\t    :param expected_metapaths: a list of expected metapaths\n", "    :return: a list of python dictionaries, consisting of metapath-based neighbor pairs and intermediate paths\n\t    \"\"\"\n\t    outs = []\n\t    for metapath in expected_metapaths:\n\t        # consider only the edges relevant to the expected metapath\n\t        mask = np.zeros(M.shape, dtype=bool)\n\t        for i in range((len(metapath) - 1) // 2):\n\t            temp = np.zeros(M.shape, dtype=bool)\n\t            temp[np.ix_(type_mask == metapath[i], type_mask == metapath[i + 1])] = True\n\t            temp[np.ix_(type_mask == metapath[i + 1], type_mask == metapath[i])] = True\n", "            mask = np.logical_or(mask, temp)\n\t        partial_g_nx = nx.from_numpy_matrix((M * mask).astype(int))\n\t        # only need to consider the former half of the metapath\n\t        # e.g., we only need to consider 0-1-2 for the metapath 0-1-2-1-0\n\t        metapath_to_target = {}\n\t        for source in (type_mask == metapath[0]).nonzero()[0]:\n\t            for target in (type_mask == metapath[(len(metapath) - 1) // 2]).nonzero()[0]:\n\t                # check if there is a possible valid path from source to target node\n\t                has_path = False\n\t                single_source_paths = nx.single_source_shortest_path(\n", "                    partial_g_nx, source, cutoff=(len(metapath) + 1) // 2 - 1)\n\t                if target in single_source_paths:\n\t                    has_path = True\n\t                #if nx.has_path(partial_g_nx, source, target):\n\t                if has_path:\n\t                    shortests = [p for p in nx.all_shortest_paths(partial_g_nx, source, target) if\n\t                                 len(p) == (len(metapath) + 1) // 2]\n\t                    if len(shortests) > 0:\n\t                        metapath_to_target[target] = metapath_to_target.get(target, []) + shortests\n\t        metapath_neighbor_paris = {}\n", "        for key, value in metapath_to_target.items():\n\t            for p1 in value:\n\t                for p2 in value:\n\t                    metapath_neighbor_paris[(p1[0], p2[0])] = metapath_neighbor_paris.get((p1[0], p2[0]), []) + [\n\t                        p1 + p2[-2::-1]]\n\t        outs.append(metapath_neighbor_paris)\n\t    return outs\n\tdef get_networkx_graph(neighbor_pairs, type_mask, ctr_ntype):\n\t    indices = np.where(type_mask == ctr_ntype)[0]\n\t    idx_mapping = {}\n", "    for i, idx in enumerate(indices):\n\t        idx_mapping[idx] = i\n\t    G_list = []\n\t    for metapaths in neighbor_pairs:\n\t        edge_count = 0\n\t        sorted_metapaths = sorted(metapaths.items())\n\t        G = nx.MultiDiGraph()\n\t        G.add_nodes_from(range(len(indices)))\n\t        for (src, dst), paths in sorted_metapaths:\n\t            for _ in range(len(paths)):\n", "                G.add_edge(idx_mapping[src], idx_mapping[dst])\n\t                edge_count += 1\n\t        G_list.append(G)\n\t    return G_list\n\tdef get_edge_metapath_idx_array(neighbor_pairs):\n\t    all_edge_metapath_idx_array = []\n\t    for metapath_neighbor_pairs in neighbor_pairs:\n\t        sorted_metapath_neighbor_pairs = sorted(metapath_neighbor_pairs.items())\n\t        edge_metapath_idx_array = []\n\t        for _, paths in sorted_metapath_neighbor_pairs:\n", "            edge_metapath_idx_array.extend(paths)\n\t        edge_metapath_idx_array = np.array(edge_metapath_idx_array, dtype=int)\n\t        all_edge_metapath_idx_array.append(edge_metapath_idx_array)\n\t        print(edge_metapath_idx_array.shape)\n\t    return all_edge_metapath_idx_array\n"]}
{"filename": "LP/scripts/__init__.py", "chunked_list": []}
{"filename": "LP/scripts/rm.py", "chunked_list": ["import random\n\tprint(random.randrange(0,4))"]}
{"filename": "LP/scripts/data_loader.py", "chunked_list": ["import os\n\timport numpy as np\n\timport scipy.sparse as sp\n\tfrom collections import Counter, defaultdict, OrderedDict\n\tfrom sklearn.metrics import f1_score, auc, roc_auc_score, precision_recall_curve\n\timport random\n\timport copy\n\tclass bcolors:\n\t    HEADER = '\\033[95m'\n\t    OKBLUE = '\\033[94m'\n", "    OKCYAN = '\\033[96m'\n\t    OKGREEN = '\\033[92m'\n\t    WARNING = '\\033[93m'\n\t    FAIL = '\\033[91m'\n\t    ENDC = '\\033[0m'\n\t    BOLD = '\\033[1m'\n\t    UNDERLINE = '\\033[4m'\n\tclass data_loader:\n\t    def __init__(self, path, edge_types=[]):\n\t        self.path = path\n", "        self.splited = False\n\t        self.nodes = self.load_nodes()\n\t        self.links = self.load_links('link.dat')\n\t        self.links_test = self.load_links('link.dat.test')\n\t        self.test_types = list(self.links_test['data'].keys()) if edge_types == [] else edge_types\n\t        self.types = self.load_types('node.dat')\n\t        self.train_pos, self.valid_pos = self.get_train_valid_pos()\n\t        self.train_neg, self.valid_neg = self.get_train_neg(), self.get_valid_neg()\n\t        self.gen_transpose_links()\n\t        self.nonzero = False\n", "    def get_train_valid_pos(self, train_ratio=0.9):\n\t        if self.splited:\n\t            return self.train_pos, self.valid_pos\n\t        else:\n\t            edge_types = self.links['data'].keys()\n\t            train_pos, valid_pos = dict(), dict()\n\t            for r_id in edge_types:\n\t                train_pos[r_id] = [[], []]\n\t                valid_pos[r_id] = [[], []]\n\t                row, col = self.links['data'][r_id].nonzero()\n", "                last_h_id = -1\n\t                for (h_id, t_id) in zip(row, col):\n\t                    if h_id != last_h_id:\n\t                        train_pos[r_id][0].append(h_id)\n\t                        train_pos[r_id][1].append(t_id)\n\t                        last_h_id = h_id\n\t                    else:\n\t                        if random.random() < train_ratio:\n\t                            train_pos[r_id][0].append(h_id)\n\t                            train_pos[r_id][1].append(t_id)\n", "                        else:\n\t                            valid_pos[r_id][0].append(h_id)\n\t                            valid_pos[r_id][1].append(t_id)\n\t                            self.links['data'][r_id][h_id, t_id] = 0\n\t                            self.links['count'][r_id] -= 1\n\t                            self.links['total'] -= 1\n\t                self.links['data'][r_id].eliminate_zeros()\n\t            self.splited = True\n\t            return train_pos, valid_pos\n\t    def get_sub_graph(self, node_types_tokeep):\n", "        \"\"\"\n\t        node_types_tokeep is a list or set of node types that you want to keep in the sub-graph\n\t        We only support whole type sub-graph for now.\n\t        This is an in-place update function!\n\t        return: old node type id to new node type id dict, old edge type id to new edge type id dict\n\t        \"\"\"\n\t        keep = set(node_types_tokeep)\n\t        new_node_type = 0\n\t        new_node_id = 0\n\t        new_nodes = {'total': 0, 'count': Counter(), 'attr': {}, 'shift': {}}\n", "        new_links = {'total': 0, 'count': Counter(), 'meta': {}, 'data': defaultdict(list)}\n\t        new_labels_train = {'num_classes': 0, 'total': 0, 'count': Counter(), 'data': None, 'mask': None}\n\t        new_labels_test = {'num_classes': 0, 'total': 0, 'count': Counter(), 'data': None, 'mask': None}\n\t        old_nt2new_nt = {}\n\t        old_idx = []\n\t        for node_type in self.nodes['count']:\n\t            if node_type in keep:\n\t                nt = node_type\n\t                nnt = new_node_type\n\t                old_nt2new_nt[nt] = nnt\n", "                cnt = self.nodes['count'][nt]\n\t                new_nodes['total'] += cnt\n\t                new_nodes['count'][nnt] = cnt\n\t                new_nodes['attr'][nnt] = self.nodes['attr'][nt]\n\t                new_nodes['shift'][nnt] = new_node_id\n\t                beg = self.nodes['shift'][nt]\n\t                old_idx.extend(range(beg, beg + cnt))\n\t                cnt_label_train = self.labels_train['count'][nt]\n\t                new_labels_train['count'][nnt] = cnt_label_train\n\t                new_labels_train['total'] += cnt_label_train\n", "                cnt_label_test = self.labels_test['count'][nt]\n\t                new_labels_test['count'][nnt] = cnt_label_test\n\t                new_labels_test['total'] += cnt_label_test\n\t                new_node_type += 1\n\t                new_node_id += cnt\n\t        new_labels_train['num_classes'] = self.labels_train['num_classes']\n\t        new_labels_test['num_classes'] = self.labels_test['num_classes']\n\t        for k in ['data', 'mask']:\n\t            new_labels_train[k] = self.labels_train[k][old_idx]\n\t            new_labels_test[k] = self.labels_test[k][old_idx]\n", "        old_et2new_et = {}\n\t        new_edge_type = 0\n\t        for edge_type in self.links['count']:\n\t            h, t = self.links['meta'][edge_type]\n\t            if h in keep and t in keep:\n\t                et = edge_type\n\t                net = new_edge_type\n\t                old_et2new_et[et] = net\n\t                new_links['total'] += self.links['count'][et]\n\t                new_links['count'][net] = self.links['count'][et]\n", "                new_links['meta'][net] = tuple(map(lambda x: old_nt2new_nt[x], self.links['meta'][et]))\n\t                new_links['data'][net] = self.links['data'][et][old_idx][:, old_idx]\n\t                new_edge_type += 1\n\t        self.nodes = new_nodes\n\t        self.links = new_links\n\t        self.labels_train = new_labels_train\n\t        self.labels_test = new_labels_test\n\t        return old_nt2new_nt, old_et2new_et\n\t    def get_meta_path(self, meta=[]):\n\t        \"\"\"\n", "        Get meta path matrix\n\t            meta is a list of edge types (also can be denoted by a pair of node types)\n\t            return a sparse matrix with shape [node_num, node_num]\n\t        \"\"\"\n\t        ini = sp.eye(self.nodes['total'])\n\t        meta = [self.get_edge_type(x) for x in meta]\n\t        for x in meta:\n\t            ini = ini.dot(self.links['data'][x]) if x >= 0 else ini.dot(self.links['data_trans'][-x - 1])\n\t        return ini\n\t    def get_nonzero(self):\n", "        self.nonzero = True\n\t        self.re_cache = defaultdict(dict)\n\t        for k in self.links['data']:\n\t            th_mat = self.links['data'][k]\n\t            for i in range(th_mat.shape[0]):\n\t                th = th_mat[i].nonzero()[1]\n\t                self.re_cache[k][i] = th\n\t        for k in self.links['data_trans']:\n\t            th_mat = self.links['data_trans'][k]\n\t            for i in range(th_mat.shape[0]):\n", "                th = th_mat[i].nonzero()[1]\n\t                self.re_cache[-k - 1][i] = th\n\t    def dfs(self, now, meta, meta_dict):\n\t        if len(meta) == 0:\n\t            meta_dict[now[0]].append(now)\n\t            return\n\t        # th_mat = self.links['data'][meta[0]] if meta[0] >= 0 else self.links['data_trans'][-meta[0] - 1]\n\t        th_node = now[-1]\n\t        for col in self.re_cache[meta[0]][th_node]:  # th_mat[th_node].nonzero()[1]:\n\t            self.dfs(now + [col], meta[1:], meta_dict)\n", "    def get_full_meta_path(self, meta=[], symmetric=False):\n\t        \"\"\"\n\t        Get full meta path for each node\n\t            meta is a list of edge types (also can be denoted by a pair of node types)\n\t            return a dict of list[list] (key is node_id)\n\t        \"\"\"\n\t        if not self.nonzero:\n\t            self.get_nonzero()\n\t        meta = [self.get_edge_type(x) for x in meta]\n\t        if len(meta) == 1:\n", "            meta_dict = {}\n\t            start_node_type = self.links['meta'][meta[0]][0] if meta[0] >= 0 else self.links['meta'][-meta[0] - 1][1]\n\t            trav = range(self.nodes['shift'][start_node_type],\n\t                         self.nodes['shift'][start_node_type] + self.nodes['count'][start_node_type])\n\t            for i in trav:\n\t                meta_dict[i] = []\n\t                self.dfs([i], meta, meta_dict)\n\t        else:\n\t            meta_dict1 = {}\n\t            meta_dict2 = {}\n", "            mid = len(meta) // 2\n\t            meta1 = meta[:mid]\n\t            meta2 = meta[mid:]\n\t            start_node_type = self.links['meta'][meta1[0]][0] if meta1[0] >= 0 else self.links['meta'][-meta1[0] - 1][1]\n\t            trav = range(self.nodes['shift'][start_node_type],\n\t                         self.nodes['shift'][start_node_type] + self.nodes['count'][start_node_type])\n\t            for i in trav:\n\t                meta_dict1[i] = []\n\t                self.dfs([i], meta1, meta_dict1)\n\t            start_node_type = self.links['meta'][meta2[0]][0] if meta2[0] >= 0 else self.links['meta'][-meta2[0] - 1][1]\n", "            trav = range(self.nodes['shift'][start_node_type],\n\t                         self.nodes['shift'][start_node_type] + self.nodes['count'][start_node_type])\n\t            for i in trav:\n\t                meta_dict2[i] = []\n\t            if symmetric:\n\t                for k in meta_dict1:\n\t                    paths = meta_dict1[k]\n\t                    for x in paths:\n\t                        meta_dict2[x[-1]].append(list(reversed(x)))\n\t            else:\n", "                for i in trav:\n\t                    self.dfs([i], meta2, meta_dict2)\n\t            meta_dict = {}\n\t            start_node_type = self.links['meta'][meta1[0]][0] if meta1[0] >= 0 else self.links['meta'][-meta1[0] - 1][1]\n\t            for i in range(self.nodes['shift'][start_node_type],\n\t                           self.nodes['shift'][start_node_type] + self.nodes['count'][start_node_type]):\n\t                meta_dict[i] = []\n\t                for beg in meta_dict1[i]:\n\t                    for end in meta_dict2[beg[-1]]:\n\t                        meta_dict[i].append(beg + end[1:])\n", "        return meta_dict\n\t    def gen_file_for_evaluate(self, edge_list, confidence, edge_type, file_path, flag):\n\t        \"\"\"\n\t        :param edge_list: shape(2, edge_num)\n\t        :param confidence: shape(edge_num,)\n\t        :param edge_type: shape(1)\n\t        :param file_path: string\n\t        \"\"\"\n\t        op = \"w\" if flag else \"a\"\n\t        with open(file_path, op) as f:\n", "            for l,r,c in zip(edge_list[0], edge_list[1], confidence):\n\t                f.write(f\"{l}\\t{r}\\t{edge_type}\\t{c}\\n\")\n\t    @staticmethod\n\t    def evaluate(edge_list, confidence, labels):\n\t        \"\"\"\n\t        :param edge_list: shape(2, edge_num)\n\t        :param confidence: shape(edge_num,)\n\t        :param labels: shape(edge_num,)\n\t        :return: dict with all scores we need\n\t        \"\"\"\n", "        confidence = np.array(confidence)\n\t        labels = np.array(labels)\n\t        roc_auc = roc_auc_score(labels, confidence)\n\t        mrr_list, cur_mrr = [], 0\n\t        t_dict, labels_dict, conf_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n\t        for i, h_id in enumerate(edge_list[0]):\n\t            t_dict[h_id].append(edge_list[1][i])\n\t            labels_dict[h_id].append(labels[i])\n\t            conf_dict[h_id].append(confidence[i])\n\t        for h_id in t_dict.keys():\n", "            conf_array = np.array(conf_dict[h_id])\n\t            rank = np.argsort(-conf_array)\n\t            sorted_label_array = np.array(labels_dict[h_id])[rank]\n\t            pos_index = np.where(sorted_label_array == 1)[0]\n\t            if len(pos_index) == 0:\n\t                continue\n\t            pos_min_rank = np.min(pos_index)\n\t            cur_mrr = 1 / (1 + pos_min_rank)\n\t            mrr_list.append(cur_mrr)\n\t        mrr = np.mean(mrr_list)\n", "        return {'roc_auc': roc_auc, 'MRR': mrr}\n\t    def get_node_type(self, node_id):\n\t        for i in range(len(self.nodes['shift'])):\n\t            if node_id < self.nodes['shift'][i] + self.nodes['count'][i]:\n\t                return i\n\t    def get_edge_type(self, info):\n\t        if type(info) is int or len(info) == 1:\n\t            return info\n\t        for i in range(len(self.links['meta'])):\n\t            if self.links['meta'][i] == info:\n", "                return i\n\t        info = (info[1], info[0])\n\t        for i in range(len(self.links['meta'])):\n\t            if self.links['meta'][i] == info:\n\t                return -i - 1\n\t        raise Exception('No available edge type')\n\t    def get_edge_info(self, edge_id):\n\t        return self.links['meta'][edge_id]\n\t    def list_to_sp_mat(self, li):\n\t        data = [x[2] for x in li]\n", "        i = [x[0] for x in li]\n\t        j = [x[1] for x in li]\n\t        return sp.coo_matrix((data, (i, j)), shape=(self.nodes['total'], self.nodes['total'])).tocsr()\n\t    def load_types(self, name):\n\t        \"\"\"\n\t        return types dict\n\t            types: list of types\n\t            total: total number of nodes\n\t            data: a dictionary of type of all nodes)\n\t        \"\"\"\n", "        types = {'types': list(), 'total': 0, 'data': dict()}\n\t        with open(os.path.join(self.path, name), 'r', encoding='utf-8') as f:\n\t            for line in f:\n\t                th = line.strip().split('\\t')\n\t                node_id, node_name, node_type = int(th[0]), th[1], int(th[2])\n\t                types['data'][node_id] = node_type\n\t                types['types'].append(node_type)\n\t                types['total'] += 1\n\t        types['types'] = list(set(types['types']))\n\t        return types\n", "    def get_train_neg(self, edge_types=[]):\n\t        edge_types = self.test_types if edge_types == [] else edge_types\n\t        train_neg = dict()\n\t        for r_id in edge_types:\n\t            h_type, t_type = self.links['meta'][r_id]\n\t            t_range = (self.nodes['shift'][t_type], self.nodes['shift'][t_type] + self.nodes['count'][t_type])\n\t            '''get neg_neigh'''\n\t            train_neg[r_id] = [[], []]\n\t            for h_id in self.train_pos[r_id][0]:\n\t                train_neg[r_id][0].append(h_id)\n", "                neg_t = random.randrange(t_range[0], t_range[1])\n\t                train_neg[r_id][1].append(neg_t)\n\t        return train_neg\n\t    def get_valid_neg(self, edge_types=[]):\n\t        edge_types = self.test_types if edge_types == [] else edge_types\n\t        valid_neg = dict()\n\t        for r_id in edge_types:\n\t            h_type, t_type = self.links['meta'][r_id]\n\t            t_range = (self.nodes['shift'][t_type], self.nodes['shift'][t_type] + self.nodes['count'][t_type])\n\t            '''get neg_neigh'''\n", "            valid_neg[r_id] = [[], []]\n\t            for h_id in self.valid_pos[r_id][0]:\n\t                valid_neg[r_id][0].append(h_id)\n\t                neg_t = random.randrange(t_range[0], t_range[1])\n\t                valid_neg[r_id][1].append(neg_t)\n\t        return valid_neg\n\t    def get_test_neigh_2hop(self):\n\t        return self.get_test_neigh()\n\t    def get_test_neigh(self):\n\t        random.seed(1)\n", "        neg_neigh, pos_neigh, test_neigh, test_label = dict(), dict(), dict(), dict()\n\t        edge_types = self.test_types\n\t        '''get sec_neigh'''\n\t        pos_links = 0\n\t        for r_id in self.links['data'].keys():\n\t            pos_links += self.links['data'][r_id] + self.links['data'][r_id].T\n\t        for r_id in self.links_test['data'].keys():\n\t            pos_links += self.links_test['data'][r_id] + self.links_test['data'][r_id].T\n\t        for r_id in self.valid_pos.keys():\n\t            values = [1] * len(self.valid_pos[r_id][0])\n", "            valid_of_rel = sp.coo_matrix((values, self.valid_pos[r_id]), shape=pos_links.shape)\n\t            pos_links += valid_of_rel\n\t        r_double_neighs = np.dot(pos_links, pos_links)\n\t        data = r_double_neighs.data\n\t        data[:] = 1\n\t        r_double_neighs = \\\n\t            sp.coo_matrix((data, r_double_neighs.nonzero()), shape=np.shape(pos_links), dtype=int) \\\n\t            - sp.coo_matrix(pos_links, dtype=int) \\\n\t            - sp.lil_matrix(np.eye(np.shape(pos_links)[0], dtype=int))\n\t        data = r_double_neighs.data\n", "        pos_count_index = np.where(data > 0)\n\t        row, col = r_double_neighs.nonzero()\n\t        r_double_neighs = sp.coo_matrix((data[pos_count_index], (row[pos_count_index], col[pos_count_index])),\n\t                                        shape=np.shape(pos_links))\n\t        row, col = r_double_neighs.nonzero()\n\t        data = r_double_neighs.data\n\t        sec_index = np.where(data > 0)\n\t        row, col = row[sec_index], col[sec_index]\n\t        relation_range = [self.nodes['shift'][k] for k in range(len(self.nodes['shift']))] + [self.nodes['total']]\n\t        for r_id in self.links_test['data'].keys():\n", "            neg_neigh[r_id] = defaultdict(list)\n\t            h_type, t_type = self.links_test['meta'][r_id]\n\t            r_id_index = np.where((row >= relation_range[h_type]) & (row < relation_range[h_type + 1])\n\t                                  & (col >= relation_range[t_type]) & (col < relation_range[t_type + 1]))[0]\n\t            # r_num = np.zeros((3, 3))\n\t            # for h_id, t_id in zip(row, col):\n\t            #     r_num[self.get_node_type(h_id)][self.get_node_type(t_id)] += 1\n\t            r_row, r_col = row[r_id_index], col[r_id_index]\n\t            for h_id, t_id in zip(r_row, r_col):\n\t                neg_neigh[r_id][h_id].append(t_id)\n", "        for r_id in edge_types:\n\t            '''get pos_neigh'''\n\t            pos_neigh[r_id] = defaultdict(list)\n\t            (row, col), data = self.links_test['data'][r_id].nonzero(), self.links_test['data'][r_id].data\n\t            for h_id, t_id in zip(row, col):\n\t                pos_neigh[r_id][h_id].append(t_id)\n\t            '''sample neg as same number as pos for each head node'''\n\t            test_neigh[r_id] = [[], []]\n\t            pos_list = [[], []]\n\t            test_label[r_id] = []\n", "            for h_id in sorted(list(pos_neigh[r_id].keys())):\n\t                pos_list[0] = [h_id] * len(pos_neigh[r_id][h_id])\n\t                pos_list[1] = pos_neigh[r_id][h_id]\n\t                test_neigh[r_id][0].extend(pos_list[0])\n\t                test_neigh[r_id][1].extend(pos_list[1])\n\t                test_label[r_id].extend([1] * len(pos_list[0]))\n\t                neg_list = random.choices(neg_neigh[r_id][h_id], k=len(pos_list[0])) if len(\n\t                    neg_neigh[r_id][h_id]) != 0 else []\n\t                test_neigh[r_id][0].extend([h_id] * len(neg_list))\n\t                test_neigh[r_id][1].extend(neg_list)\n", "                test_label[r_id].extend([0] * len(neg_list))\n\t        return test_neigh, test_label\n\t    def get_test_neigh_w_random(self):\n\t        random.seed(1)\n\t        all_had_neigh = defaultdict(list)\n\t        neg_neigh, pos_neigh, neigh, label = dict(), dict(), dict(), dict()\n\t        edge_types = self.test_types\n\t        '''get pos_links of train and test data'''\n\t        pos_links = 0\n\t        for r_id in self.links['data'].keys():\n", "            pos_links += self.links['data'][r_id] + self.links['data'][r_id].T\n\t        for r_id in self.links_test['data'].keys():\n\t            pos_links += self.links_test['data'][r_id] + self.links_test['data'][r_id].T\n\t        for r_id in self.valid_pos.keys():\n\t            values = [1] * len(self.valid_pos[r_id][0])\n\t            valid_of_rel = sp.coo_matrix((values, self.valid_pos[r_id]), shape=pos_links.shape)\n\t            pos_links += valid_of_rel\n\t        row, col = pos_links.nonzero()\n\t        for h_id, t_id in zip(row, col):\n\t            all_had_neigh[h_id].append(t_id)\n", "        for h_id in all_had_neigh.keys():\n\t            all_had_neigh[h_id] = set(all_had_neigh[h_id])\n\t        for r_id in edge_types:\n\t            h_type, t_type = self.links_test['meta'][r_id]\n\t            t_range = (self.nodes['shift'][t_type], self.nodes['shift'][t_type] + self.nodes['count'][t_type])\n\t            '''get pos_neigh and neg_neigh'''\n\t            pos_neigh[r_id], neg_neigh[r_id] = defaultdict(list), defaultdict(list)\n\t            (row, col), data = self.links_test['data'][r_id].nonzero(), self.links_test['data'][r_id].data\n\t            for h_id, t_id in zip(row, col):\n\t                pos_neigh[r_id][h_id].append(t_id)\n", "                neg_t = random.randrange(t_range[0], t_range[1])\n\t                while neg_t in all_had_neigh[h_id]:\n\t                    neg_t = random.randrange(t_range[0], t_range[1])\n\t                neg_neigh[r_id][h_id].append(neg_t)\n\t            '''get the neigh'''\n\t            neigh[r_id] = [[], []]\n\t            pos_list = [[], []]\n\t            neg_list = [[], []]\n\t            label[r_id] = []\n\t            for h_id in sorted(list(pos_neigh[r_id].keys())):\n", "                pos_list[0] = [h_id] * len(pos_neigh[r_id][h_id])\n\t                pos_list[1] = pos_neigh[r_id][h_id]\n\t                neigh[r_id][0].extend(pos_list[0])\n\t                neigh[r_id][1].extend(pos_list[1])\n\t                label[r_id].extend([1] * len(pos_neigh[r_id][h_id]))\n\t                neg_list[0] = [h_id] * len(neg_neigh[r_id][h_id])\n\t                neg_list[1] = neg_neigh[r_id][h_id]\n\t                neigh[r_id][0].extend(neg_list[0])\n\t                neigh[r_id][1].extend(neg_list[1])\n\t                label[r_id].extend([0] * len(neg_neigh[r_id][h_id]))\n", "        return neigh, label\n\t    def get_test_neigh_full_random(self):\n\t        edge_types = self.test_types\n\t        random.seed(1)\n\t        '''get pos_links of train and test data'''\n\t        all_had_neigh = defaultdict(list)\n\t        pos_links = 0\n\t        for r_id in self.links['data'].keys():\n\t            pos_links += self.links['data'][r_id] + self.links['data'][r_id].T\n\t        for r_id in self.links_test['data'].keys():\n", "            pos_links += self.links_test['data'][r_id] + self.links_test['data'][r_id].T\n\t        for r_id in self.valid_pos.keys():\n\t            values = [1] * len(self.valid_pos[r_id][0])\n\t            valid_of_rel = sp.coo_matrix((values, self.valid_pos[r_id]), shape=pos_links.shape)\n\t            pos_links += valid_of_rel\n\t        row, col = pos_links.nonzero()\n\t        for h_id, t_id in zip(row, col):\n\t            all_had_neigh[h_id].append(t_id)\n\t        for h_id in all_had_neigh.keys():\n\t            all_had_neigh[h_id] = set(all_had_neigh[h_id])\n", "        test_neigh, test_label = dict(), dict()\n\t        for r_id in edge_types:\n\t            test_neigh[r_id] = [[], []]\n\t            test_label[r_id] = []\n\t            h_type, t_type = self.links_test['meta'][r_id]\n\t            h_range = (self.nodes['shift'][h_type], self.nodes['shift'][h_type] + self.nodes['count'][h_type])\n\t            t_range = (self.nodes['shift'][t_type], self.nodes['shift'][t_type] + self.nodes['count'][t_type])\n\t            (row, col), data = self.links_test['data'][r_id].nonzero(), self.links_test['data'][r_id].data\n\t            for h_id, t_id in zip(row, col):\n\t                test_neigh[r_id][0].append(h_id)\n", "                test_neigh[r_id][1].append(t_id)\n\t                test_label[r_id].append(1)\n\t                neg_h = random.randrange(h_range[0], h_range[1])\n\t                neg_t = random.randrange(t_range[0], t_range[1])\n\t                while neg_t in all_had_neigh[neg_h]:\n\t                    neg_h = random.randrange(h_range[0], h_range[1])\n\t                    neg_t = random.randrange(t_range[0], t_range[1])\n\t                test_neigh[r_id][0].append(neg_h)\n\t                test_neigh[r_id][1].append(neg_t)\n\t                test_label[r_id].append(0)\n", "        return test_neigh, test_label\n\t    def gen_transpose_links(self):\n\t        self.links['data_trans'] = defaultdict()\n\t        for r_id in self.links['data'].keys():\n\t            self.links['data_trans'][r_id] = self.links['data'][r_id].T\n\t    def load_links(self, name):\n\t        \"\"\"\n\t        return links dict\n\t            total: total number of links\n\t            count: a dict of int, number of links for each type\n", "            meta: a dict of tuple, explaining the link type is from what type of node to what type of node\n\t            data: a dict of sparse matrices, each link type with one matrix. Shapes are all (nodes['total', nodes['total'])\n\t        \"\"\"\n\t        links = {'total': 0, 'count': Counter(), 'meta': {}, 'data': defaultdict(list)}\n\t        with open(os.path.join(self.path, name), 'r', encoding='utf-8') as f:\n\t            for line in f:\n\t                th = line.split('\\t')\n\t                h_id, t_id, r_id, link_weight = int(th[0]), int(th[1]), int(th[2]), float(th[3])\n\t                if r_id not in links['meta']:\n\t                    h_type = self.get_node_type(h_id)\n", "                    t_type = self.get_node_type(t_id)\n\t                    links['meta'][r_id] = (h_type, t_type)\n\t                links['data'][r_id].append((h_id, t_id, link_weight))\n\t                links['count'][r_id] += 1\n\t                links['total'] += 1\n\t        new_data = {}\n\t        for r_id in links['data']:\n\t            new_data[r_id] = self.list_to_sp_mat(links['data'][r_id])\n\t        links['data'] = new_data\n\t        return links\n", "    def load_nodes(self):\n\t        \"\"\"\n\t        return nodes dict\n\t        total: total number of nodes\n\t        count: a dict of int, number of nodes for each type\n\t        attr: a dict of np.array (or None), attribute matrices for each type of nodes\n\t        shift: node_id shift for each type. You can get the id range of a type by\n\t                    [ shift[node_type], shift[node_type]+count[node_type] )\n\t        \"\"\"\n\t        nodes = {'total': 0, 'count': Counter(), 'attr': {}, 'shift': {}}\n", "        with open(os.path.join(self.path, 'node.dat'), 'r', encoding='utf-8') as f:\n\t            for line in f:\n\t                th = line.split('\\t')\n\t                if len(th) == 4:\n\t                    # Then this line of node has attribute\n\t                    node_id, node_name, node_type, node_attr = th\n\t                    node_id = int(node_id)\n\t                    node_type = int(node_type)\n\t                    node_attr = list(map(float, node_attr.split(',')))\n\t                    nodes['count'][node_type] += 1\n", "                    nodes['attr'][node_id] = node_attr\n\t                    nodes['total'] += 1\n\t                elif len(th) == 3:\n\t                    # Then this line of node doesn't have attribute\n\t                    node_id, node_name, node_type = th\n\t                    node_id = int(node_id)\n\t                    node_type = int(node_type)\n\t                    nodes['count'][node_type] += 1\n\t                    nodes['total'] += 1\n\t                else:\n", "                    raise Exception(\"Too few information to parse!\")\n\t        shift = 0\n\t        attr = {}\n\t        for i in range(len(nodes['count'])):\n\t            nodes['shift'][i] = shift\n\t            if shift in nodes['attr']:\n\t                mat = []\n\t                for j in range(shift, shift + nodes['count'][i]):\n\t                    mat.append(nodes['attr'][j])\n\t                attr[i] = np.array(mat)\n", "            else:\n\t                attr[i] = None\n\t            shift += nodes['count'][i]\n\t        nodes['attr'] = attr\n\t        return nodes\n"]}
{"filename": "LP/scripts/LP_AUC_MRR.py", "chunked_list": ["from collections import Counter\n\tfrom sklearn.metrics import f1_score,roc_auc_score\n\timport numpy as np\n\timport os\n\tfrom collections import defaultdict\n\timport sys\n\timport json\n\tclass AUC_MRR:\n\t    def __init__(self, data_name, pred_files):\n\t        self.AUC_list = []\n", "        self.MRR_list = []\n\t        if len(pred_files)==0:\n\t            self.AUC_mean = np.mean(0)\n\t            self.MRR_mean = np.mean(0)\n\t            self.AUC_std = np.std(0)\n\t            self.MRR_std = np.std(0)\n\t        else:\n\t            true_file = os.path.join(args.ground_dir, data_name, 'link.dat.test')\n\t            self.links_true = self.load_links(true_file)\n\t            for pred_file in pred_files:\n", "                self.links_test = self.load_links(pred_file)\n\t                ans = self.evaluate_AUC_MRR()\n\t                self.AUC_list.append(ans['AUC'])\n\t                self.MRR_list.append(ans['MRR'])\n\t            self.AUC_mean = np.mean(self.AUC_list)\n\t            self.MRR_mean = np.mean(self.MRR_list)\n\t            self.AUC_std = np.std(self.AUC_list)\n\t            self.MRR_std = np.std(self.MRR_list)\n\t    def load_links(self, file_name):\n\t        \"\"\"\n", "        return links dict\n\t            total: total number of links\n\t            count: a dict of int, number of links for each type\n\t            data: a list, where each link type have a dict with {(head_id, tail_id): confidence}\n\t        \"\"\"\n\t        links = {'total': 0, 'count': Counter(), 'data': defaultdict(dict)}\n\t        with open(file_name, 'r', encoding='utf-8') as f:\n\t            for line in f:\n\t                th = line.split('\\t')\n\t                h_id, t_id, r_id, link_weight = int(th[0]), int(th[1]), int(th[2]), float(th[3])\n", "                links['data'][r_id][(h_id, t_id)] = link_weight\n\t                links['count'][r_id] += 1\n\t                links['total'] += 1\n\t        return links\n\t    def evaluate_AUC_MRR(self):\n\t        ans_all_link_type = {'AUC':[],'MRR':[]}\n\t        for link_type in  self.links_test['count'].keys():\n\t            edge_list = {0: [], 1: []}\n\t            confidence = []\n\t            labels = []\n", "            for h_t in self.links_test['data'][link_type].keys():\n\t                edge_list[0].append(h_t[0])\n\t                edge_list[1].append(h_t[1])\n\t                confidence.append(self.links_test['data'][link_type][h_t])\n\t                if h_t in self.links_true['data'][link_type]:\n\t                    labels.append(1.0)\n\t                else:\n\t                    labels.append(0.)\n\t            ans = self.evaluate(edge_list, confidence, labels)\n\t            ans_all_link_type['AUC'].append(ans['AUC'])\n", "            ans_all_link_type['MRR'].append(ans['MRR'])\n\t        ans_all_link_type['AUC'] = np.mean(ans_all_link_type['AUC'])\n\t        ans_all_link_type['MRR'] = np.mean(ans_all_link_type['MRR'])\n\t        return ans_all_link_type\n\t    @staticmethod\n\t    def evaluate(edge_list, confidence, labels):\n\t        \"\"\"\n\t        :param edge_list: shape(2, edge_num)\n\t        :param confidence: shape(edge_num,)\n\t        :param labels: shape(edge_num,)\n", "        :return: dict with all scores we need\n\t        \"\"\"\n\t        confidence = np.array(confidence)\n\t        labels = np.array(labels)\n\t        roc_auc = roc_auc_score(labels, confidence)\n\t        mrr_list, cur_mrr = [], 0\n\t        t_dict, labels_dict, conf_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n\t        for i, h_id in enumerate(edge_list[0]):\n\t            t_dict[h_id].append(edge_list[1][i])\n\t            labels_dict[h_id].append(labels[i])\n", "            conf_dict[h_id].append(confidence[i])\n\t        for h_id in t_dict.keys():\n\t            conf_array = np.array(conf_dict[h_id])\n\t            rank = np.argsort(-conf_array)\n\t            sorted_label_array = np.array(labels_dict[h_id])[rank]\n\t            pos_index = np.where(sorted_label_array == 1)[0]\n\t            if len(pos_index) == 0:\n\t                continue\n\t            pos_min_rank = np.min(pos_index)\n\t            cur_mrr = 1 / (1 + pos_min_rank)\n", "            mrr_list.append(cur_mrr)\n\t        mrr = np.mean(mrr_list)\n\t        return {'AUC': roc_auc, 'MRR': mrr}\n\timport argparse\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser(description=\"Evaluate AUC and MRR for LP result.\")\n\t    parser.add_argument('--pred_zip', type=str, default=\"lp.zip\",\n\t                        help='Compressed pred files.')\n\t    parser.add_argument('--ground_dir', type=str, default=\"../data\",\n\t                        help='Dir of ground files.')\n", "    parser.add_argument('--log', type=str, default=\"lp.log\",\n\t                        help='output file')\n\t    return parser.parse_args()\n\timport zipfile\n\tdef extract_zip(zip_path, extract_path):\n\t    zip = zipfile.ZipFile(zip_path, 'r')\n\t    zip.extractall(extract_path)\n\t    return zip.namelist()\n\tdef write_log(log_file, log_msg):\n\t    with open(log_file, 'w') as log_handle:\n", "        log_handle.write(log_msg)\n\tdef delete_files(files_):\n\t    for f in files_:\n\t        if os.path.exists(f):\n\t            os.remove(f)\n\tif __name__ == '__main__':\n\t    # get argument settings.\n\t    args = parse_args()\n\t    zip_path = args.pred_zip\n\t    if not os.path.exists(zip_path):\n", "        log_msg = 'ERROR: No such zip file!'\n\t        write_log(args.log, log_msg)\n\t        sys.exit()\n\t    extract_path='lp'\n\t    extract_file_list = extract_zip(zip_path, extract_path)\n\t    extract_file_list = [os.path.join(extract_path,f_) for f_ in extract_file_list]\n\t    data_list = ['amazon', 'LastFM', 'PubMed']\n\t    res={}\n\t    detect_data_files = []\n\t    for data_name in data_list:\n", "        pred_files = []\n\t        for i in range(1, 6):\n\t            file_name = os.path.join(extract_path, f'{data_name}_{i}.txt')\n\t            if not os.path.exists(file_name):\n\t                continue\n\t            pred_files.append(file_name)\n\t            detect_data_files.append(file_name)\n\t        if len(pred_files) > 0 and len(pred_files) != 5:\n\t            log_msg = f'ERROR: Please check the size of {data_name} dataset!'\n\t            write_log(args.log, log_msg)\n", "            delete_files(extract_file_list)\n\t            sys.exit()\n\t        res[data_name] = AUC_MRR(data_name, pred_files)\n\t    if len(detect_data_files) == 0:\n\t        log_msg = f'ERROR: No file detected, please confirm that ' \\\n\t                  f'the data file is in the top directory of the compressed package!'\n\t        write_log(args.log, log_msg)\n\t        sys.exit()\n\t    delete_files(extract_file_list)\n\t    hgb_score_list = []\n", "    for data_name in data_list:\n\t        hgb_score_list.append(res[data_name].AUC_mean)\n\t        hgb_score_list.append(res[data_name].MRR_mean)\n\t    hgb_score = np.mean(hgb_score_list)\n\t    detail_json = {}\n\t    log_msg = f'{hgb_score}###'\n\t    for data_name in data_list:\n\t        detail_json[data_name] = {}\n\t        detail_json[data_name]['AUC_mean'] = res[data_name].AUC_mean\n\t        detail_json[data_name]['AUC_std'] = res[data_name].AUC_std\n", "        detail_json[data_name]['MRR_mean'] = res[data_name].MRR_mean\n\t        detail_json[data_name]['MRR_std'] = res[data_name].MRR_std\n\t    log_msg += json.dumps(detail_json)\n\t    write_log(args.log, log_msg)\n\t    sys.exit()\n"]}
