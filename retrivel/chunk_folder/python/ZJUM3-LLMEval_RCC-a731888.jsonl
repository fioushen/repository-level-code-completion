{"filename": "setup.py", "chunked_list": ["import setuptools\n\twith open('README.md') as f:\n\t    _LONG_DESCRIPTION = f.read()\n\tsetuptools.setup(\n\t    name='rcctool',\n\t    version='0.0.1',\n\t    description='RCCtool',\n\t    long_description=_LONG_DESCRIPTION,\n\t    long_description_content_type='text/markdown',\n\t    author='ZJUM3',\n", "    url='https://github.com/ZJUM3/LLMEval_RCC',\n\t    packages=setuptools.find_packages(),\n\t    install_requires=[ ],\n\t    extras_require={\n\t        'test': ['pytest']\n\t    },\n\t    classifiers=[\n\t        'Intended Audience :: Science/Research',\n\t        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n\t    ],\n", "    keywords='LLM evaluation',\n\t)"]}
{"filename": "processor.py", "chunked_list": ["import argparse\n\timport json\n\timport converter, generator, attacker, interpreter\n\tclass Processor :\n\t    def __init__(\n\t        self\n\t    ):\n\t        self.data = {}\n\t        self.para = {}\n\t        self.label = ''\n", "    def read_data(self, indir) :\n\t        self.data = json.load(open(indir, encoding = 'utf-8'))\n\t        return self.data\n\t    def get_label(self, str) :\n\t        str = str.split('\\\\')[-1]\n\t        self.label = str[:str.find('.')]\n\t    def form_converter(self, label = None, indir = None) :\n\t        if label != None :\n\t            self.converter = converter.DataCleaner()\n\t            self.data = self.converter.process(label)\n", "        else :\n\t            self.read_data(indir)\n\t    def options_generator(self) :\n\t        self.generator = generator.Options_generator()\n\t        outdir = 'Datasets\\Generater_out\\\\'+self.label+'.json'\n\t        self.data = self.generator.process(dict = self.data, outdir = outdir)\n\t        print('Date primitives form data is stored in \\''+outdir+'\\'')\n\t    def passage_attacker(self, config, type = 'robustness') :\n\t        self.attacker = attacker.Attacker()\n\t        outdir = 'Datasets\\Attacker_out\\\\'+type+'\\\\'+self.label+'_attacked.json'\n", "        self.para = self.attacker.process(type = config['type'], dict = self.data, \n\t                                          outdir = outdir, p_dir = config['p_dir'], \n\t                                          iter = config['iter'], cred = config['cred'])\n\t        print('Attacked form data is stored in \\''+outdir+'\\'')\n\t    def question_interpreter(self, config, type = 'robustness') :\n\t        self.interpreter = interpreter.Interpreter()\n\t        if not config['use_chatgpt'] : outdir = 'Datasets\\Queation_out\\\\'+type+'\\\\'+self.label+'.json'\n\t        else : outdir = 'Datasets\\Interpreter_out\\\\'+type+'\\\\'+self.label+'_question.json'\n\t        para = self.interpreter.process(type = type, dict_para = self.para, dict_data = self.data,\n\t                                        indir_prompt = config['p_dir'], api_key = config['api_key'],\n", "                                        num_thread = config['num_thread'], use_chatgpt = config['use_chatgpt'],\n\t                                        outdir = outdir)\n\t        if type(para) != None : self.para = para\n\t        if not config['use_chatgpt'] : print('The question file is stored in \\''+outdir+'\\'')\n\t        else : print('The response file is stored in \\''+outdir+'\\'')\n\t    def process(self, config) :\n\t        if config['dataset'] == None : \n\t            self.read_data(config['indir'])\n\t            self.get_label(config['indir'])\n\t        self.form_converter(config['dataset'])\n", "        self.options_generator()\n\t        self.passage_attacker(config, config['type'])\n\t        self.question_interpreter(config, config['type'])\n\tdef main() :\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--indir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n", "        help = \"input directory\"\n\t    )\n\t    parser.add_argument(\n\t        \"--dataset\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = \"GSM8K\",\n\t        help = \"dataset name\"\n\t    )\n\t    parser.add_argument(\n", "        \"--api_key\",\n\t        type = str,\n\t        nargs = \"+\",\n\t        default = None\n\t    )\n\t    parser.add_argument(\n\t        \"--num_thread\",\n\t        type = int,\n\t        nargs = \"?\",\n\t        default = 100\n", "    )\n\t    parser.add_argument(\n\t        \"--useChatGPT\",\n\t        action='store_true',\n\t        help=\"use model ChatGPT\",\n\t    )\n\t    parser.add_argument(\n\t        \"--type\",\n\t        type = str,\n\t        nargs = \"?\",\n", "        default = \"robustness\",\n\t        help = \"test type including robusteness, consistency and credibility\"\n\t    )\n\t    parser.add_argument(\n\t        \"--prompt_dir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None\n\t    )\n\t    parser.add_argument(\n", "        \"--cred_operation\",\n\t        type = str,\n\t        nargs = \"+\",\n\t        default = ['ri', 'rd', 'rp'],\n\t        help = \"operation types including random insertion for ri, random deletion for rd and random replacement for rp\"\n\t    )\n\t    parser.add_argument(\n\t        \"--iter\",\n\t        type = int,\n\t        nargs = \"+\",\n", "        default = [4, 2, 2],\n\t        help = \"iteration times of each level including character, word and visual\"\n\t    )\n\t    opt = parser.parse_args()\n\t    config = {\n\t        'indir' : opt.indir,\n\t        'type' : opt.type,\n\t        'dataset' : opt.dataset,\n\t        'p_dir' : opt.prompt_dir,\n\t        'api_key' : opt.api_key,\n", "        'num_thread' : opt.num_thread,\n\t        'use_chatgpt' : opt.useChatGPT,\n\t        'iter' : opt.iter,\n\t        'cred' : opt.cred_operation\n\t    }\n\t    processor = Processor()\n\t    processor.process(config)\n\tif __name__ == '__main__' :\n\t    main()"]}
{"filename": "eval.py", "chunked_list": ["import argparse\n\timport json\n\timport interpreter\n\tfrom Evals import get_path, conEval, robEval, creEval\n\timport matplotlib.pyplot as plt\n\tclass Evaluator :\n\t    def __init__(\n\t        self\n\t    ):\n\t        self.data = {}\n", "        self.para = {}\n\t        self.label = ''\n\t    def read_data(self, indir) :\n\t        self.data = json.load(open(indir, encoding = 'utf-8'))\n\t        return self.data\n\t    def get_label(self, str) :\n\t        str = str.split('\\\\')[-1]\n\t        self.label = str[:str.find('.')]\n\t    def read_para(self, type = 'robustness', res_dir = None) :\n\t        indir = 'Datasets\\Attacker_out\\\\'+type+'\\\\'+self.label+'.json'\n", "        self.para = json.load(open(indir, encoding = 'utf-8'))\n\t        if res_dir != None : \n\t            self.interpreter = interpreter.Interpreter()\n\t            self.para = self.interpreter.get_llm_answer(self.para, res_dir)\n\t    def get_answerpath(self) :\n\t        self.para = get_path.get_path(self.para, self.data)\n\t    def eval(self, type) :\n\t        print(self.label)\n\t        if type == 'robustness' :\n\t            plt.figure(figsize=(10, 6), dpi=80)\n", "            plt.suptitle(type+self.label, fontsize = 20)\n\t            error_rate, changed_rate, SUM = robEval.get_score(self.para)\n\t            print('ER score:', '\\n', error_rate, '\\n\\n',\n\t                  'ASR score:', changed_rate, '\\n\\n', 'sum: ', SUM)\n\t            robEval.draw_table(error_rate, changed_rate, SUM)\n\t            plt.tight_layout()\n\t            plt.show()\n\t        elif type == 'consistency' :\n\t            gt_result, changed_result, SUM = conEval.get_score(self.para)\n\t            print('Groundtruth:', '\\n', gt_result, '\\n\\n',\n", "                  'Attacked:', changed_result, '\\n\\n', 'sum: ', SUM)\n\t        elif type == 'credibility' :\n\t            plt.figure(figsize=(10, 6), dpi=80)\n\t            plt.suptitle(type+self.label, fontsize = 20)\n\t            Rate_list = creEval.get_rate(self.para).copy()\n\t            SUM = 0\n\t            for key in Rate_list :\n\t                Rate_list[key] = round(sum([i for i in Rate_list[key]])/len(Rate_list[key]), 3)\n\t                SUM += Rate_list[key]\n\t            print('RTI score in '+self.label+' : '+round(SUM/3, 3))+'\\\\\\\\'\n", "            plt.show()\n\t    def process(self, config) :\n\t        self.read_data(config['indir'])\n\t        self.get_label(config['indir'])\n\t        self.read_para(config['type'], config['res_dir'])\n\t        self.get_answerpath()\n\t        self.eval(config['type'])\n\tdef main() :\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n", "        \"--indir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n\t        help = \"input directory\"\n\t    )\n\t    parser.add_argument(\n\t        \"--response_dir\",\n\t        type = str,\n\t        nargs = \"?\",\n", "        default = None,\n\t        help = \"response directory\"\n\t    )\n\t    parser.add_argument(\n\t        \"--type\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = \"robustness\",\n\t        help = \"test type including robusteness, consistency and credibility\"\n\t    )\n", "    opt = parser.parse_args()\n\t    config = {\n\t        'type' : opt.type,\n\t        'indir' : opt.indir,\n\t        'res_dir' : opt.response_dir\n\t    }\n\t    evaluator = Evaluator()\n\t    evaluator.process(config)\n\tif __name__ == '__main__' :\n\t    main()"]}
{"filename": "generator.py", "chunked_list": ["import re\n\timport argparse, os\n\tfrom tqdm import tqdm\n\timport json\n\timport hanlp\n\tfrom DA.eda import *\n\tfrom textblob import TextBlob\n\timport numpy as np\n\tfrom fractions import Fraction\n\tclass Generator(object) :\n", "    def __init__(\n\t        self\n\t    ):\n\t        self.data = {}\n\t    def assign(self, dict) :\n\t        self.data = dict\n\t    def generator_tf(self) :\n\t        self.data['option-number'] = 3\n\t        self.data['options'] = []\n\t        _dict = self.data['options']\n", "        self.data['answer'] = True if self.data['answer'] in [True] or isinstance(self.data['answer'], str) and self.data['answer'].lower() in ['true'] else False\n\t        _dict.extend([{'option_type' : 'GroundTruth',\n\t                       'option_describe' : self.data['answer']},\n\t                       {'option_type' : 'False',\n\t                        'option_describe' : not self.data['answer']},\n\t                        {'option_type' : 'Undetermined',\n\t                         'option_describe' : 'Unable to determine'}])\n\t        return self.data\n\t    @staticmethod\n\t    def check(x, y) :\n", "        word = TextBlob(y).words\n\t        return word[0].singularize().lower() != x.lower() and word[0].pluralize().lower() != x.lower()\n\t    def generator_word(self, passage) :\n\t        HanLP = hanlp.load(hanlp.pretrained.mtl.UD_ONTONOTES_TOK_POS_LEM_FEA_NER_SRL_DEP_SDP_CON_XLMR_BASE)\n\t        sentence = HanLP(passage.strip(' '))\n\t        tok, pos = list([i.lower() for i in sentence['tok']]), list(sentence['pos'])\n\t        self.data['option-number'] = 5\n\t        self.data['options'] = []\n\t        _dict = self.data['options']\n\t        _dict.append({'option_type' : 'GroundTruth',\n", "                      'option_describe' : self.data['answer']})\n\t        answer, answer_temp = self.data['answer'], [self.data['answer']]\n\t        for _ in range(4) :\n\t            f = False\n\t            try:\n\t                position = pos[tok.index(answer)]\n\t                # print(tok.index(answer[i]))\n\t                for j in range(len(pos)) :\n\t                    if pos[j] == position and tok[j] not in answer_temp and self.check(tok[j], answer):\n\t                        answer_temp.append(tok[j]); f = True; break\n", "            except : pass\n\t            if not f :\n\t                for j in range(len(pos)) :\n\t                    if tok[j] not in answer_temp and self.check(tok[j], answer):\n\t                        answer_temp.append(tok[j]); f = True; break\n\t            _dict.append({'option_type' : 'Answewr Error'+str(_),\n\t                          'option_describe' : answer_temp[-1]})\n\t        return self.data\n\t    @staticmethod\n\t    def Numb_type(element) :\n", "        return 'int' if element.isdigit() else 'float'\n\t    def generator_number(self) :\n\t        self.data['option-number'] = 5\n\t        self.data['options'] = []\n\t        _dict = self.data['options']\n\t        _dict.append({'option_type' : 'GroundTruth',\n\t                      'option_describe' : self.data['answer']})\n\t        answer = str(eval(str(self.data['answer'])))\n\t        for _ in range(4) :\n\t            def get_new_numb(numb) :\n", "                if self.Numb_type(numb) == 'float' : new_numb = str(round(np.random.uniform(-3*eval(numb), 3*eval(numb)+2), 2))\n\t                else : new_numb = str(np.random.randint(-3*eval(numb), 3*eval(numb)+2))\n\t                return new_numb\n\t            new_answer = answer\n\t            while new_answer == answer : new_answer = get_new_numb(answer)\n\t            _dict.append({'option_type' : 'Answewr Error'+str(_),\n\t                          'option_describe' : new_answer})\n\t        return self.data\n\tclass Generator_text(object) :\n\t    def __init__(self) -> None:\n", "        self.data = {}\n\t    def eda_func(self, a, x) : \n\t        try : \n\t            return eda(a, alpha_sr = 0.3, alpha_ri = 0.3, alpha_rs = 0.3, p_rd = 0.3, num_aug = x)[0]\n\t        except : return 'None'\n\t    def assign(self, dict) :\n\t        self.data = dict\n\t    def gt_proc(self) :\n\t        self.data['option-number'] = 1\n\t        self.data['options'] = []\n", "        _dict = self.data['options']\n\t        _dict.append({'option_type' : 'GroundTruth',\n\t                      'option_describe' : self.data['answer']})\n\t    @staticmethod\n\t    def pass_judge(Q, type) :\n\t        for _ in Q['options'] : \n\t            if _['option_type'] == type : return False\n\t        return True\n\t    def eda_proc(self) :\n\t        _dict = self.data['options']\n", "        if not self.pass_judge(self.data, 'Text Augmentation') : return self.data\n\t        a = self.data['answer']\n\t        _dict.append({\"option_type\" : '', 'option_describe' : ''})\n\t        c_opt = _dict[-1]\n\t        c_opt['option_type'] = 'Text Augmentation'\n\t        # print(a)\n\t        c_opt['option_describe'] = self.eda_func(a, 1)\n\t        while a.find(c_opt['option_describe']) != -1 : c_opt['option_describe'] = self.eda_func(a, 1)\n\t        self.data['option-number'] += 1\n\t        return self.data\n", "    @staticmethod\n\t    def formula_split(formula : str) -> list:\n\t        formula = formula.strip(' ')\n\t        result, pos = [], 0\n\t        def get_type(ch) :\n\t            return 'number' if ch.isdigit() or ch == '.' else 'other'\n\t        def judge_type(ori, cur) :\n\t            return get_type(ori) == get_type(cur)\n\t        for id, ch in enumerate(formula) :\n\t            if judge_type(ch, formula[pos]) : continue\n", "            else : result.append(formula[pos:id]); pos = id\n\t        result.append(formula[pos:])\n\t        return result\n\t    @staticmethod\n\t    def search_formula(formula) :\n\t        match_type = '0123456789+-*/%$.= ()\\\\'\n\t        pos = formula.find('=')\n\t        if pos == -1 : return None\n\t        else :\n\t            st, ed = pos, pos\n", "            while st >= 0 and formula[st] in match_type : st -= 1\n\t            while ed < len(formula) and formula[ed] in match_type : ed += 1\n\t            st += 1\n\t        return formula[st:ed]\n\t    @staticmethod\n\t    def get_numb(pos : int, s : str) -> int :\n\t        POS = pos\n\t        while POS < len(s) and s[POS].isdigit() : POS += 1\n\t        return POS\n\t    @staticmethod\n", "    def tf_certain_probability(probability, sequence = [True, False]):\n\t        x = random.uniform(0, 1)\n\t        cumulative_probability = 0.0\n\t        for item, item_probability in zip(sequence, probability):\n\t            cumulative_probability += item_probability\n\t            if x < cumulative_probability : break\n\t        return item\n\t    def formula_error_proc(self) :\n\t        self.data['options'].append({\"option_type\" : '', 'option_describe' : ''})\n\t        c_opt = self.data['options'][-1]\n", "        c_opt['option_type'] = 'Formula Error'\n\t        c_opt['option_describe'] = self.data['answer']\n\t        fl = False\n\t        a = self.data['answer']\n\t        while True :\n\t            pos = 0\n\t            new_formula = c_opt['option_describe']\n\t            while pos < len(new_formula) :\n\t                if new_formula[pos].isdigit() :\n\t                    fl = True\n", "                    ed = self.get_numb(pos, new_formula)\n\t                    numb = new_formula[pos:ed]\n\t                    if self.tf_certain_probability([0.8, 0.2]) :\n\t                        new_numb = str(np.random.randint(0, min(1e9+7, 5*int(numb)+2)))\n\t                        new_formula = new_formula[:max(pos, 1)]+new_numb+(new_formula[ed:] if ed < len(new_formula) else '')\n\t                        pos += len(new_numb)\n\t                    else : pos = ed\n\t                else : pos += 1\n\t            if not fl : break\n\t            if c_opt['option_describe'] != new_formula : \n", "                c_opt['option_describe'] = new_formula; break\n\t        if not fl :\n\t            c_opt['option_describe'] = self.eda_func(a, 1)\n\t            while a.find(c_opt['option_describe']) != -1 : c_opt['option_describe'] = self.eda_func(a, 1)\n\t        self.data['option-number'] += 1\n\t        return self.data\n\t    def mask_proc(self) :\n\t        _dict = self.data['options']\n\t        _dict.append({\"option_type\" : 'Mask CorrectAnswer', 'option_describe' : 'None of other options is correct.'})\n\t        self.data['option-number'] += 1\n", "        return self.data\n\t    @staticmethod\n\t    def generate_number(up_bound = 500):\n\t        return str(random.randint(0, up_bound))\n\t    @staticmethod\n\t    def generate_operator():\n\t        operators = ['+', '-', '*', '/']\n\t        return random.choice(operators)\n\t    @staticmethod\n\t    def generate_parenthesis():\n", "        parentheses = ['(', ')']\n\t        return random.choice(parentheses)\n\t    def generate_expression(self, length) :\n\t        if length <= 1:\n\t            return self.generate_number()\n\t        elif length == 2:\n\t            return self.generate_number()+self.generate_operator()+self.generate_number()\n\t        use_parenthesis = random.choice([True, False])\n\t        if use_parenthesis:\n\t            opening_parenthesis = self.generate_parenthesis()\n", "            closing_parenthesis = self.generate_parenthesis()\n\t            inner_length = length-2\n\t            return opening_parenthesis+self.generate_expression(inner_length)+closing_parenthesis\n\t        else:\n\t            left_length = random.randint(1, length-2)\n\t            right_length = length-left_length-1\n\t            left_expression = self.generate_expression(left_length)\n\t            right_expression = self.generate_expression(right_length)\n\t            operator = self.generate_operator()\n\t            return left_expression+operator+right_expression\n", "    def irrelevant_formula_proc(self) :\n\t        formula = self.search_formula(self.data['answer'])\n\t        a = self.data['answer']\n\t        self.data['options'].append({\"option_type\" : 'Irrelevant Formula', 'option_describe' : a})\n\t        if formula != None :\n\t            length = int(self.generate_number(10))\n\t            expression = self.generate_expression(length)\n\t            self.data['options'][-1]['option_describe'] = self.data['options'][-1]['option_describe'].replace(formula, ' '+expression+' ')\n\t            self.data['option-number'] += 1\n\t        else : \n", "            while a.find(self.data['options'][-1]['option_describe']) != -1 : self.data['options'][-1]['option_describe'] = self.eda_func(a, 1)\n\t        return self.data\n\t    def text_options_generator(self) :\n\t        self.gt_proc()\n\t        self.eda_proc()\n\t        self.formula_error_proc()\n\t        self.mask_proc()\n\t        self.irrelevant_formula_proc()\n\tclass Options_generator(Generator, Generator_text) :\n\t    def __init__(self) :\n", "        Generator.__init__(self)\n\t        Generator_text.__init__(self)\n\t        self.dict = {}\n\t    def __reinit(self) :\n\t        self.dict = {}\n\t    def read_dict(self, indir) :\n\t        self.__reinit()\n\t        self.dict = json.load(open(indir, encoding = 'utf-8'))\n\t        return self.dict\n\t    @staticmethod\n", "    def opt_type(obj) :\n\t        if isinstance(obj, bool) or (isinstance(obj, str) and obj.lower() in ['true', 'false', 'yes', 'no']) : \n\t            return \"T/F\"\n\t        elif isinstance(obj, (int, float, Fraction)) or (isinstance(obj, str) and re.match(r'^[-+]?\\d+(\\.\\d+)?$', obj)) : \n\t            return \"Number\"\n\t        elif isinstance(obj, str) :\n\t            if obj.isalpha() : return \"Word\"\n\t            else : return \"Text\"\n\t        else : raise TypeError    \n\t    def confusion_proc(self, id : str, ID : int) :\n", "        another_id = str(np.random.randint(0, len(self.dict)))\n\t        while another_id == id or another_id not in self.dict : another_id = str(np.random.randint(0, len(self.dict)))\n\t        _dict = self.dict[id]\n\t        # pos, mx = 0, len(self.dict[another_id]['sub-qa'])\n\t        # dir = [1, -1]; cur_dir = 0\n\t        # def move_next(pos, cur_dir) -> tuple :\n\t        #     if pos+dir[cur_dir] >= 0 and pos+dir[cur_dir] < mx : pos += dir[cur_dir]\n\t        #     if pos == 0 or pos == mx-1 : cur_dir = (cur_dir+1)%2\n\t        #     return pos, cur_dir\n\t        _id = np.random.randint(0, len(self.dict[another_id]['sub-qa']))\n", "        for _, Q in enumerate(_dict['sub-qa']) :\n\t            if _ != ID : continue\n\t            Q['option-number'] += 1\n\t            Q['options'].append({\"option_type\" : '', 'option_describe' : ''}); c_opt = Q['options'][-1]\n\t            c_opt['option_type'] = 'Irrelevant Chain'\n\t            c_opt['option_describe'] = self.dict[another_id]['sub-qa'][_id]['answer']\n\t    def process(self, indir = None, dict = None, outdir = None) :\n\t        if indir != None : self.read_dict(indir)\n\t        else : self.dict = dict\n\t        POP_list = []\n", "        for id in tqdm(self.dict) :\n\t            # if int(id) > 5 : POP_list.append(id); continue\n\t            _dict = self.dict[id]\n\t            f = False\n\t            pop_list = []\n\t            for ID, q in enumerate(_dict['sub-qa']) :\n\t                if 'options' in q : continue\n\t                if q['answer'] in ['', None] : \n\t                    pop_list.append(ID)\n\t                    continue\n", "                f = True\n\t                type = self.opt_type(q['answer'])\n\t                # print(type, q['answer'])\n\t                if type == 'Number' :\n\t                    Generator.assign(self, q)\n\t                    Generator.generator_number(self)\n\t                elif type == 'T/F' :\n\t                    Generator.assign(self, q)\n\t                    Generator.generator_tf(self)\n\t                elif type == 'Word' :\n", "                    Generator.assign(self, q)\n\t                    Generator.generator_word(self, _dict['passage'])\n\t                else :\n\t                    Generator_text.assign(self, q)\n\t                    Generator_text.text_options_generator(self)\n\t            if not f : \n\t                POP_list.append(id)\n\t                continue\n\t            if pop_list != [] :\n\t                for ID in pop_list.reverse() : _dict['sub-qa'].pop(ID)\n", "        for id in POP_list : self.dict.pop(id)\n\t        print('*************************************')\n\t        for id in self.dict :\n\t            # print(id)\n\t            _dict = self.dict[id]\n\t            for ID, q in enumerate(_dict['sub-qa']) :\n\t                type = self.opt_type(q['answer'])\n\t                if type == 'Text' : self.confusion_proc(id, ID)\n\t        print('*************************************')\n\t        self.item_proc()\n", "        print('*************************************')\n\t        if indir == None and outdir == None : return\n\t        if outdir == None : outdir = 'Datasets\\Generater_out\\\\'+indir.split('\\\\')[-1]\n\t        os.makedirs(os.path.dirname(outdir), exist_ok=True)\n\t        self.save(outdir)\n\t        return self.dict\n\t    def item_proc(self) :\n\t        for id in self.dict :\n\t            _dict = self.dict[id]\n\t            for Q in _dict['sub-qa'] :\n", "                q_list = [i for i in range(len(Q['options']))]\n\t                q_list = list(np.random.choice(q_list, size = len(q_list), replace = False))\n\t                Q['input'] = [int(i) for i in q_list]\n\t    def save(self, outdir) :\n\t        with open(outdir, 'w', encoding = 'utf-8') as f:\n\t            json.dump(self.dict, f, indent = 4)\n\t        f.close()\n\tdef main() :\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n", "        \"--indir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n\t        help = \"dataset name\"\n\t    )\n\t    parser.add_argument(\n\t        \"--outdir\",\n\t        type = str,\n\t        nargs = \"?\",\n", "        default = None,\n\t        help = \"output directory\"\n\t    )\n\t    opt = parser.parse_args()\n\t    Generator = Options_generator()\n\t    indir_list = ['GSM8K.json', 'Creak.json', 'NoahQA.json', 'bAbi15.json', 'bAbi16.json']\n\t    for indir in indir_list :\n\t        Generator.process(indir = os.path.join('F:\\codespace\\ChatGPT\\opensource\\Datasets\\Converter_out\\\\',indir), outdir = opt.outdir)\n\t    # Generator.process(opt.indir, opt.outdir)\n\tif __name__ == '__main__' :\n", "    main()"]}
{"filename": "attacker.py", "chunked_list": ["import json\n\timport argparse, os\n\tfrom tqdm import tqdm\n\tfrom DA.eda import *\n\tfrom DA.token_level_attack import *\n\tSYMBOL_LIST = ['#', '@', '～', '$', '%', '^', '&', '*', '-', '+', '=', '}', '{', '<', '>']\n\tclass Attacker(object) :\n\t    def __init__(\n\t        self\n\t    ):\n", "        self.data = {}\n\t        self.result = {}\n\t        self.eda_func = lambda a,x : eda(a, alpha_sr = 0.0, alpha_ri = 0.3, alpha_rs = 0.3, p_rd = 0.3, num_aug = x)[0]\n\t    def __reinit(self) :\n\t        self.data = {}\n\t    def assign(self, dict) :\n\t        self.data = dict\n\t    def read_data(self, indir) :\n\t        self.__reinit()\n\t        self.data = json.load(open(indir, encoding = 'utf-8'))\n", "        return self.data\n\t    @staticmethod\n\t    def word_level_attack(words, p, type) :\n\t        num = round(len(words)*p)\n\t        if type == 'ri' :\n\t            return random_insertion(words, num)\n\t        elif type == 'rd' :\n\t            return random_deletion(words, p)\n\t        elif type == 'rp' :\n\t            return random_replacement(words, num)\n", "    @staticmethod\n\t    def tf_certain_probability(probability, sequence = [1, 2, 3]):\n\t        x = random.uniform(0, 1)\n\t        cumulative_probability = 0.0\n\t        for item, item_probability in zip(sequence, probability):\n\t            cumulative_probability += item_probability\n\t            if x < cumulative_probability : break\n\t        return item\n\t    def robustness_samples_generate(self, type : dict = {'character' : 4, 'word' : 2, 'visual' : 2}) :\n\t        for id in tqdm(self.data) :\n", "            self.result[id] = {}\n\t            _dict = self.result[id]\n\t            _dict['ori_passage'] = self.data[id]['passage']\n\t            _dict['new_passage'] = []\n\t            _dict = _dict['new_passage']\n\t            sentence = self.data[id]['passage'] if self.data[id]['passage'] not in ['', None] else self.data[id]['sub-qa'][0]['question']\n\t            words = sentence.split(' ')\n\t            if 'character' in type :\n\t                for _ in range(type['character']) :\n\t                    p = np.random.randint(1, 11)/10\n", "                    Type = np.random.randint(0, 3)\n\t                    op_time = self.tf_certain_probability([0.4, 0.4, 0.2])\n\t                    if Type != 2 : new_sentence = attack(sentence, attack_rate = p, attack_type = Type, op_times = op_time)\n\t                    else : new_sentence = attack(sentence, attack_rate = p, attack_type = Type, op_times = 1, insert_char = np.random.choice(SYMBOL_LIST))\n\t                    _dict.append({'passage' : new_sentence,\n\t                                  'attack_type' : 'character_level'+'_'+['delete', 'repeat', 'insert'][Type],\n\t                                  'rate' : p})\n\t            if 'word' in type :\n\t                # random delete\n\t                words = [word for word in words if word != '']\n", "                sig, words[-1] = words[-1][-1], words[-1][:-1]\n\t                for Type in ['ri', 'rd', 'rp'] :\n\t                    for _ in range(type['word']) :\n\t                        p = np.random.randint(1, 11)/10\n\t                        context = self.word_level_attack(words, p, Type)\n\t                        _dict.append({'passage' : ' '.join(context)+sig,\n\t                                      'attack_type' : 'word_level'+'_'+Type,\n\t                                      'rate' : p})\n\t            if 'visual' in type :\n\t                for Rate in [10, 50, 90] :\n", "                    for _ in range(type['visual']) :\n\t                        p = np.random.randint(1, 11)/10\n\t                        _dict.append({'passage' : attack(sentence, attack_rate = p, attack_type = 3, replace_ratio = Rate/100)+'.',\n\t                                      'attack_type' : 'visual_attack'+'_'+str(Rate)+'%',\n\t                                      'rate' : p})\n\t        return self.result\n\t    def consistency_sample_generate(self, prompt) :\n\t        for id in tqdm(self.data) :\n\t            self.result[id] = {}\n\t            _dict = self.result[id]\n", "            _dict['ori_passage'] = self.data[id]['passage']\n\t            _dict['new_passage'] = []\n\t            _dict = _dict['new_passage']\n\t            for p in prompt :\n\t                _dict.append({'passage' : p})\n\t        return self.result\n\t    def credibility_sample_generate(self, type : list = ['ri', 'rd', 'rp']) :\n\t        for id in tqdm(self.data) :\n\t            self.result[id] = {}\n\t            _dict = self.result[id]\n", "            _dict['ori_passage'] = self.data[id]['passage']\n\t            _dict['new_passage'] = []\n\t            _dict = _dict['new_passage']\n\t            sentence = self.data[id]['passage'] if self.data[id]['passage'] not in ['', None] else self.data[id]['sub-qa'][0]['question']\n\t            words = sentence.split(' ')\n\t            # random delete\n\t            words = [word for word in words if word != '']\n\t            # print(words)\n\t            sig, words[-1] = words[-1][-1], words[-1][:-1]\n\t            for Type in type :\n", "                for _ in range(10) :\n\t                    p = (_+1)/10\n\t                    context = self.word_level_attack(words, p, Type)\n\t                    _dict.append({'passage' : ' '.join(context)+sig,\n\t                                  'attack_type' : 'word_level'+'_'+Type,\n\t                                  'rate' : p})\n\t        return self.result\n\t    def save(self, outdir) :\n\t        with open(outdir, 'w', encoding = 'utf-8') as f :\n\t            json.dump(self.result, f, indent = 4)\n", "        f.close()\n\t    def process(self, type = 'robustness', indir = None, outdir = None, dict = None, p_dir = None,\n\t                iter= {'character' : 4, 'word' : 2, 'visual' : 2}, cred : list = ['ri', 'rd', 'rp']) :\n\t        if indir != None : self.read_data(indir)\n\t        elif dict != None : self.assign(dict)\n\t        if type == 'robustness' : self.robustness_samples_generate(iter)\n\t        elif type == 'credibility' : self.credibility_sample_generate(cred)\n\t        elif type == 'consistency' : self.consistency_sample_generate(json.load(open(p_dir, encoding = 'utf-8')))\n\t        if indir == None and outdir == None : return self.result\n\t        elif outdir == None : outdir = 'Datasets\\Attacker_out\\\\'+type+'\\\\'+indir.split('\\\\')[-1][:-5]+'_attacked.json'\n", "        os.makedirs(os.path.dirname(outdir), exist_ok=True)\n\t        self.save(outdir)\n\t        return self.result\n\tdef main() :\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--indir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n", "        help = \"input directory\"\n\t    )\n\t    parser.add_argument(\n\t        \"--outdir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n\t        help = \"output directory\"\n\t    )\n\t    parser.add_argument(\n", "        \"--type\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = \"robustness\",\n\t        help = \"test type including robusteness, consistency and credibility\"\n\t    )\n\t    parser.add_argument(\n\t        \"--iter\",\n\t        type = int,\n\t        nargs = \"+\",\n", "        default = [4, 2, 2],\n\t        help = \"iteration times of each level including character, word and visual\"\n\t    )\n\t    parser.add_argument(\n\t        \"--cred_operation\",\n\t        type = str,\n\t        nargs = \"+\",\n\t        default = ['ri', 'rd', 'rp'],\n\t        help = \"operation types including random insertion for ri, random deletion for rd and random replacement for rp\"\n\t    )\n", "    parser.add_argument(\n\t        \"--prompt_dir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None\n\t    )\n\t    opt = parser.parse_args()\n\t    iter = {'character' : opt.iter[0], 'word' : opt.iter[1], 'visual' : opt.iter[2]}\n\t    generator = Attacker()\n\t    generator.process(p_dir = opt.prompt_dir, type = opt.type, indir = opt.indir, outdir = opt.outdir, iter = iter, cred = opt.cred_operation)\n", "if __name__ == '__main__' :\n\t    main()"]}
{"filename": "interpreter.py", "chunked_list": ["import argparse, os\n\timport json\n\timport time\n\timport converter, generator, attacker\n\tfrom API import api\n\tfrom tqdm import tqdm\n\tclass Interpreter(object) :\n\t    def __init__(\n\t        self\n\t    ):\n", "        self.data = {}\n\t        self.para = {}\n\t        self.result = {}\n\t        self.question = []\n\t    def read_data(self, indir) :\n\t        self.data = json.load(open(indir, encoding = 'utf-8'))\n\t        return self.data\n\t    def read_para(self, indir) :\n\t        self.para = json.load(open(indir, encoding = 'utf-8'))\n\t        return self.para\n", "    def get_single_question(self, _dict, prompt, para) :\n\t        self.question.append([])\n\t        if para == None : para = ''\n\t        self.question[-1].append(prompt+para+'\\\"\\n')\n\t        for _, Q in enumerate(_dict['sub-qa']) :\n\t            if _ == 0 : q = \"The first question is \\\"\"\n\t            else : q = \"The next question is \\\"\"\n\t            q = q+Q['question']+'\\\":\\n'\n\t            q_list = Q['input']\n\t            for id, i in enumerate(q_list) :\n", "                # print(i)\n\t                q = q+'('+chr(65+id)+')'+str(Q['options'][i]['option_describe'])+'\\n'\n\t            self.question[-1].append(q)\n\t    def get_question(self, type = 'robustness', \n\t                     prompt = 'Next, I will ask you a series of questions given a description, and you will have to choose one of several candidate options that you think is correct.  The description is \\\"',\n\t                     prompt_template : list = None) :\n\t        self.question = []\n\t        for id in tqdm(self.data) :\n\t            _dict = self.data[id]\n\t            if type in ['robustness', 'credibility'] :\n", "                self.get_single_question(_dict, prompt, self.para[id]['ori_passage'])\n\t                for p in self.para[id]['new_passage'] :\n\t                    self.get_single_question(_dict, prompt, p['passage'])\n\t            elif type == 'consistency' :\n\t                for p in prompt_template :\n\t                    self.get_single_question(_dict, p, self.para[id]['ori_passage'])\n\t            else : raise AttributeError\n\t        return self.question\n\t    def get_chatgpt_answer(self, api_key : list[str], num_thread) :\n\t        chatbot = api.ChatbotWrapper(config={\"api_key\":api_key, \"proxy\":\"http://127.0.0.1:1087\"})\n", "        start = time.time() \n\t        all_responses = chatbot.ask_batch(self.question, num_thread)\n\t        end = time.time()\n\t        i = 0\n\t        for id in self.data :\n\t            self.para[id]['response'] = all_responses[i]\n\t            i += 1\n\t            if 'new_passage' in self.para[id] :\n\t                for _ in self.para[id]['new_passage'] :\n\t                    _['response'] = all_responses[i]\n", "                    i += 1\n\t        total_time = end-start\n\t        print(\"total time \", total_time)\n\t        return self.para\n\t    def get_llm_answer(self, para, res_dir) :\n\t        res = json.load(open(res_dir, encoding = 'utf-8'))\n\t        for id in para :\n\t            self.para[id]['response'] = res[i]\n\t            i += 1\n\t            if 'new_passage' in para[id] :\n", "                for _ in para[id]['new_passage'] :\n\t                    _['response'] = res[i]\n\t                    i += 1\n\t        return para\n\t    def save(self, dict, outdir) :\n\t        with open(outdir, 'w', encoding = 'utf-8') as f :\n\t            json.dump(dict, f, indent = 4)\n\t        f.close()\n\t    def process(self, dict_data = None, dict_para = None, indir_data = None, indir_para = None, outdir = None, \n\t                type = 'robustness', indir_prompt = None, api_key : list[str] = None, num_thread = 100, use_chatgpt = True) :\n", "        if indir_data != None : self.read_data(indir_data)\n\t        elif dict_data != None : self.data = dict_data\n\t        if indir_para != None : self.read_para(indir_para)\n\t        elif dict_para != None : self.para = dict_para\n\t        if type in ['robustness', 'credibility'] : self.get_question(type = type)\n\t        elif type == 'consistency' :\n\t            prompt_template = json.load(open(indir_prompt, encoding = 'utf-8'))\n\t            self.get_question(type = type, prompt_template = prompt_template)\n\t        if use_chatgpt :\n\t            self.get_chatgpt_answer(api_key = api_key, num_thread = num_thread)\n", "        if outdir == None : outdir = 'Datasets\\Interpreter_out\\\\'+type+'\\\\'+indir_data.split('\\\\')[-1]\n\t        if not use_chatgpt : outdir = outdir[:-5]+'_question.json'\n\t        os.makedirs(os.path.dirname(outdir), exist_ok=True)\n\t        self.save(self.para, outdir) if use_chatgpt else self.save(self.question, outdir)\n\t        return self.para if use_chatgpt else None\n\tdef main() :\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--data_primitive_indir\",\n\t        type = str,\n", "        nargs = \"?\",\n\t        default = None\n\t    )\n\t    parser.add_argument(\n\t        \"--attacked_indir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None\n\t    )\n\t    parser.add_argument(\n", "        \"--prompt_indir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None\n\t    )\n\t    parser.add_argument(\n\t        \"--outdir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n", "        help = \"output directory\"\n\t    )\n\t    parser.add_argument(\n\t        \"--api_key\",\n\t        type = str,\n\t        nargs = \"+\",\n\t        default = None\n\t    )\n\t    parser.add_argument(\n\t        \"--num_thread\",\n", "        type = int,\n\t        nargs = \"?\",\n\t        default = 100\n\t    )\n\t    parser.add_argument(\n\t        \"--useChatGPT\",\n\t        action='store_true',\n\t        help=\"use model ChatGPT\",\n\t    )\n\t    parser.add_argument(\n", "        \"--type\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = \"robustness\",\n\t        help = \"test type including robusteness, consistency and credibility\"\n\t    )\n\t    opt = parser.parse_args()\n\t    solver = Interpreter()\n\t    solver.process(indir_data = opt.data_primitive_indir, indir_para = opt.attacked_indir, indir_prompt = opt.prompt_indir, \n\t                   api_key = opt.api_key, use_chatgpt = opt.useChatGPT, type = opt.type)\n", "if __name__ == '__main__' :\n\t    main()"]}
{"filename": "converter.py", "chunked_list": ["import re\n\timport jsonlines\n\timport argparse, os\n\tfrom tqdm import tqdm\n\timport json\n\timport numpy as np\n\tfrom dst_preprocess import ecqa, esnli, qasc\n\tclass Converter(object) :\n\t    def __init__(\n\t        self\n", "    ):\n\t        self.result = {}\n\t        self.data = {}\n\t    def __reinit(self) :\n\t        self.result = {}\n\t        self.data = {}\n\t    def read_8k(self, indir : str) :\n\t        self.__reinit()\n\t        id = 0\n\t        for dict, _dict in zip(jsonlines.Reader(open(indir, encoding = 'utf-8')), jsonlines.Reader(open(indir[:-6]+'_socratic.jsonl', encoding = 'utf-8'))) :\n", "            dict['answer'] = _dict['answer']\n\t            self.data[str(id)] = dict\n\t            id += 1\n\t        return self.data\n\t    @staticmethod\n\t    def find(s:str, S:str) -> int :\n\t        return S.lower().find(s.lower())\n\t    def clean_8k(self) :\n\t        Type = {}\n\t        for id in tqdm(self.data) :\n", "            dict = self.data[id]\n\t            self.result[id] = {}\n\t            q, a = dict['question'], dict['answer'].split('\\n')\n\t            _dict = self.result[str(id)]\n\t            p, q, ans = q, '', a[-1]\n\t            if '?' in p : \n\t                q = re.split('[,.;]', p)[-1]\n\t                p = p[:-len(q)]\n\t            else : q, p = p[self.find('calculate', p):], p[:self.find('calculate', p)]\n\t            _dict['passage'], _dict['sub-qa'] = p, []\n", "            if str(len(a)) not in Type : Type[str(len(a))] = 1\n\t            else : Type[str(len(a))] += 1\n\t            for i in range(len(a)-1) :\n\t                question, a[i] = a[i].split('**')[0], a[i].split('**')[1]\n\t                answer = a[i][:a[i].find('<<')]+a[i][a[i].find('>>')+2:]\n\t                if answer == '' : continue\n\t                _dict['sub-qa'].append({'question' : question,\n\t                                        'answer' : answer})\n\t            randint = np.random.randint(0, 2)\n\t            _dict['sub-qa'].append({'question' : 'The answer to question \\\"'+q+('\\\" is '+'\\\"'+ans+'\\\"'+', is it right?' if randint == 0 else  '\\\" is not '+'\\\"'+ans+'\\\"'+', is it right?'),\n", "                            'answer' : True if randint == 0 else False})\n\t        return Type, sum([Type[key] for key in Type.keys()]), sum([int(key)*Type[key] for key in Type.keys()])\n\t    def read_Noah(self, indir : str) :\n\t        self.__reinit()\n\t        self.data = json.load(open(indir, encoding = 'utf-8')) \n\t        return self.data\n\t    @staticmethod\n\t    def get_noah_c(passage : list) -> str :\n\t        passage.sort(key = lambda x : x[0])\n\t        question = ''\n", "        for p in passage : question += p[1]\n\t        return question\n\t    def clean_Noah(self) :\n\t        Type = {}\n\t        for id, key in enumerate(tqdm(self.data.keys())) :\n\t            dict = self.data[key]\n\t            self.result[str(id)] = {}\n\t            _dict = self.result[str(id)]\n\t            p = self.get_noah_c(dict['passage'])\n\t            count = str(int(dict['max_turn']))\n", "            if count not in Type : Type[count] = 1\n\t            else : Type[count] += 1\n\t            _dict['passage'], q = p, dict['qa_pairs'][-1]['question']\n\t            ans = dict['qa_pairs'][-1]['ans'] if 'ans' in dict['qa_pairs'][-1] else dict['qa_pairs'][-1]['answer']\n\t            _dict['sub-qa'] = []\n\t            for i in range(int(dict['max_turn'])-1) :\n\t                answer = dict['qa_pairs'][i]['ans'] if 'ans' in dict['qa_pairs'][i] else dict['qa_pairs'][i]['answer']\n\t                _dict['sub-qa'].append({'question' : dict['qa_pairs'][i]['question'],\n\t                                        'answer' : answer})\n\t            randint = np.random.randint(0, 2)\n", "            _dict['sub-qa'].append({'question' : 'The answer to question \\\"'+q+('\\\" is '+'\\\"'+ans+'\\\"'+', is it right?' if randint == 0 else  '\\\" is not '+'\\\"'+ans+'\\\"'+', is it right?'),\n\t                          'answer' : True if randint == 0 else False})\n\t        return Type, sum([Type[key] for key in Type.keys()]), sum([int(key)*Type[key] for key in Type.keys()])\n\t    def read_aq(self, indir : str) :\n\t        self.__reinit()\n\t        id = 0\n\t        for dict in jsonlines.Reader(open(indir, encoding = 'utf-8')) :\n\t            self.data[str(id)] = dict\n\t            id += 1\n\t        return self.data\n", "    def clean_aq(self) :\n\t        Type = {'1' : 0}\n\t        for id in tqdm(self.data) :\n\t            _dict = self.data[id]\n\t            self.result[id] = {}\n\t            dict = self.result[id]\n\t            q, answer = _dict['question'], _dict['options'][ord(_dict['correct'])-ord('A')][2:]\n\t            option = _dict['options'].copy()\n\t            option = option[0:ord(_dict['correct'])-ord('A')]+option[ord(_dict['correct'])-ord('A')+1:len(option)]\n\t            for key in ['question', 'options', 'rationale', 'correct'] : _dict.pop(key)\n", "            dict['passage'] = q\n\t            dict['sub-qa'] = []\n\t            Type[str(1)] += 1\n\t            dict['sub-qa'].append({'question' : 'Choose one of several candidate options that you think is correct.',\n\t                                    'answer' : answer, \n\t                                    'option-number' : len(option)+1,\n\t                                    'options' : [{'option_type' : 'GroundTruth',\n\t                                                  'option_describe' : answer}]})\n\t            dict = dict['sub-qa'][-1]['options']\n\t            for ID, i in enumerate(option) :\n", "                dict.append({'option_type' : 'Value Error'+str(ID),\n\t                              'option_describe' : i[2:]})\n\t        return Type, sum([Type[key] for key in Type.keys()]), sum([int(key)*Type[key] for key in Type.keys()])\n\t    def read_strategyqa(self, indir : str) :\n\t        self.__reinit()\n\t        self.data = json.load(open(indir, encoding = 'utf-8'))\n\t        return self.data\n\t    def clean_strategyqa(self) :\n\t        Type = {'1' : 0}\n\t        for id, dict in enumerate(tqdm(self.data)) :\n", "            self.result[str(id)] = {}\n\t            _dict = self.result[str(id)]\n\t            _dict['passage'] = ''.join(dict['facts'])\n\t            _dict['sub-qa'] = []\n\t            _dict = _dict['sub-qa']\n\t            Type[str(1)] += 1\n\t            _dict.append({'question' : dict['question'],\n\t                          'answer' : dict['answer']})\n\t        return Type, sum([Type[key] for key in Type.keys()]), sum([int(key)*Type[key] for key in Type.keys()])\n\t    def read_creak(self, indir) :\n", "        self.__reinit()\n\t        id = 0\n\t        for dict in jsonlines.Reader(open(indir, encoding = 'utf-8')) :\n\t            self.data[str(id)] = dict\n\t            id += 1\n\t        return self.data\n\t    def clean_creak(self) :\n\t        Type = {'1' : 0}\n\t        for id in tqdm(self.data) :\n\t            dict = self.data[id]\n", "            self.result[id] = {}\n\t            _dict = self.result[id]\n\t            _dict['passage'], _dict['sub-qa'] = dict['explanation'], []\n\t            Type[str(1)] += 1\n\t            _dict = _dict['sub-qa']\n\t            _dict.append({'question' : dict['sentence'][:-1]+', is it right?',\n\t                          'answer' : True if dict['label'] == 'true' else False})\n\t        return Type, sum([Type[key] for key in Type.keys()]), sum([int(key)*Type[key] for key in Type.keys()])\n\t    def read_babi(self, indir, type = 'deduction') :\n\t        self.__reinit()\n", "        with open(indir, 'r', encoding = 'utf-8') as f:\n\t            context = f.read().splitlines()\n\t        if type == 'deduction' :\n\t            num = int(len(context)/12)\n\t            for id, _ in enumerate(context) :\n\t                if id%12 in range(9) : context[id] = context[id][2:]\n\t                else : context[id] = context[id][3:]\n\t            for i in range(num) :\n\t                self.data[str(i)] = {}\n\t                self.data[str(i)]['passage'], self.data[str(i)]['question'] = ' '.join(context[12*i:12*i+8]), context[12*i+8:12*i+12]\n", "        else :\n\t            num = int(len(context)/10)\n\t            for id, _ in enumerate(context) :\n\t                if id%10 in range(9) : context[id] = context[id][2:]\n\t                else : context[id] = context[id][3:]\n\t            for i in range(num) :\n\t                self.data[str(i)] = {}\n\t                self.data[str(i)]['passage'], self.data[str(i)]['question'] = ' '.join(context[10*i:10*i+9]), context[10*i+9:10*i+10]\n\t        return self.data\n\t    def clean_babi(self, type = 'deduction') :\n", "        Type = {}\n\t        for id in tqdm(self.data) :\n\t            self.result[id] = {}\n\t            _dict, dict = self.result[id], self.data[id]\n\t            _dict['passage'], _dict['sub-qa'] = dict['passage'], []\n\t            _dict = _dict['sub-qa']\n\t            answer = []\n\t            for q in dict['question'] : answer.append(re.split(r'\\s+', q)[-3 if type == 'deduction' else -4])\n\t            if str(len(answer)) not in Type : Type[str(len(answer))] = 1\n\t            else : Type[str(len(answer))] += 1\n", "            for i, q in enumerate(dict['question']):\n\t                _dict.append({'question' : q[:q.find('?')+1],\n\t                              'answer' : answer[i]})\n\t        return Type, sum([Type[key] for key in Type.keys()]), sum([int(key)*Type[key] for key in Type.keys()])\n\tclass DataCleaner(Converter) :\n\t    def save(self, outdir) :\n\t        with open(outdir, 'w', encoding = 'utf-8') as f:\n\t            json.dump(self.result, f, indent = 4)\n\t        f.close()\n\t    def process_8k(self, indir, outdir) :\n", "        self.read_8k(indir)\n\t        result, sum, qsum = self.clean_8k()\n\t        if outdir != None : self.save(outdir)\n\t        return result, sum, qsum\n\t    def process_Noah(self, indir, outdir) :\n\t        self.read_Noah(indir)\n\t        result, sum, qsum = self.clean_Noah()\n\t        if outdir != None : self.save(outdir)\n\t        return result, sum, qsum\n\t    def process_aq(self, indir, outdir) :\n", "        self.read_aq(indir)\n\t        result, sum, qsum = self.clean_aq()\n\t        if outdir != None : self.save(outdir)\n\t        return result, sum, qsum\n\t    def process_strategyqa(self, indir, outdir) :\n\t        self.read_strategyqa(indir)\n\t        result, sum, qsum = self.clean_strategyqa()\n\t        if outdir != None : self.save(outdir)\n\t        return result, sum, qsum\n\t    def process_creak(self, indir, outdir) :\n", "        self.read_creak(indir)\n\t        result, sum, qsum = self.clean_creak()\n\t        if outdir != None : self.save(outdir)\n\t        return result, sum, qsum\n\t    def process_babi15(self, indir, outdir) :\n\t        self.read_babi(indir, 'deduction')\n\t        result, sum, qsum = self.clean_babi('deduction')\n\t        if outdir != None : self.save(outdir)\n\t        return result, sum, qsum\n\t    def process_babi16(self, indir, outdir) :\n", "        self.read_babi(indir, 'induction')\n\t        result, sum, qsum = self.clean_babi('induction')\n\t        if outdir != None : self.save(outdir)\n\t        return result, sum, qsum\n\t    def process(self, label = 'GSM8K', outdir = None) :\n\t        if outdir == None : outdir = 'Datasets\\Converter_out\\\\'+label+'.json'\n\t        key_dict = {'GSM8K' : '.jsonl', 'NoahQA': '.json', 'AQuA' : '.jsonl', \n\t                    'Creak' : '.jsonl', 'StrategyQA' : '.json', 'bAbi15' : '.txt', \n\t                    'bAbi16' : '.txt', 'e-SNLI' : '.csv', 'ECQA' : '.csv', 'QASC' : '.jsonl'}\n\t        indir = 'Datasets\\Raw data\\\\'+label+key_dict[label]\n", "        func_dict = {'GSM8K' : self.process_8k, 'NoahQA': self.process_Noah, 'AQuA' : self.process_aq, 'Creak' : self.process_creak, \n\t                     'StrategyQA' : self.process_strategyqa, 'bAbi15' : self.process_babi15, 'bAbi16' : self.process_babi16,\n\t                     'e-SNLI' : esnli.process, 'ECQA' : ecqa.process, 'QASC' : qasc.process}\n\t        if outdir != None : os.makedirs(os.path.dirname(outdir), exist_ok=True)\n\t        if label in ['e-SNLI', 'ECQA', 'QASC'] : self.result = func_dict[label](indir, outdir)\n\t        else : func_dict[label](indir, outdir)\n\t        return self.result\n\tdef main() :\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n", "        \"--dataset\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = \"GSM8K\",\n\t        help = \"dataset name\"\n\t    )\n\t    parser.add_argument(\n\t        \"--outdir\",\n\t        type = str,\n\t        nargs = \"?\",\n", "        default = None,\n\t        help = \"output directory\"\n\t    )\n\t    opt = parser.parse_args()\n\t    converter = DataCleaner()\n\t    converter.process(opt.dataset, opt.outdir) \n\tif __name__ == '__main__' :\n\t    main()"]}
{"filename": "DA/eda.py", "chunked_list": ["# Easy data augmentation techniques for text classification\n\t# Jason Wei and Kai Zou\n\timport re\n\timport random\n\tfrom random import shuffle\n\t#stop words list\n\tstop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n\t\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n\t\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n\t\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n", "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n\t\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n\t\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n\t\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n\t\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n\t\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n\t\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n\t\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n\t\t\t\t'into', 'through', 'during', 'before', 'after', \n\t\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n", "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n\t\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n\t\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n\t\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n\t\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n\t\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n\t\t\t\t'should', 'now', '']\n\t# formula_type = '[0-9\\+\\-\\*\\/\\%\\$\\·\\\\xX]+'\n\timport re\n\tdef get_only_chars(line):\n", "    clean_line = \"\"\n\t    line = line.replace(\"’\", \"\")\n\t    line = line.replace(\"'\", \"\")\n\t    line = line.replace(\"-\", \" \")\n\t    line = line.replace(\"\\t\", \" \")\n\t    line = line.replace(\"\\n\", \" \")\n\t    line = line.lower()\n\t    for char in line:\n\t        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n\t            clean_line += char\n", "        else:\n\t            clean_line += ' '\n\t    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n\t    if clean_line[0] == ' ':\n\t        clean_line = clean_line[1:]\n\t    return clean_line\n\tfrom nltk.corpus import wordnet \n\tdef random_replacement(words, n):\n\t\tnew_words = words.copy()\n\t\tfor _ in range(n) : replace_word(new_words)\n", "\tsentence = ' '.join(new_words)\n\t\tnew_words = sentence.split(' ')\n\t\treturn new_words\n\tdef replace_word(new_words):\n\t\tsynonyms = []\n\t\tcounter = 0\n\t\tword_list = new_words.copy()\n\t\twhile len(synonyms) < 1:\n\t\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n\t\t\tsynonyms = get_new_word(random_word, word_list)\n", "\t\tcounter += 1\n\t\t\tif counter >= 10:\n\t\t\t\treturn\n\t\trandom_synonym = synonyms[0]\n\t\trandom_idx = random.randint(0, len(new_words)-1)\n\t\tnew_words[random_idx] = random_synonym\n\tdef get_new_word(word, word_list, p = 0.5):\n\t\tsynonyms = set()\n\t\tr = random.uniform(0, 1)\n\t\tif r <= p :\n", "\t\tsynonyms.add(word_list[random.randint(0, len(word_list)-1)])\n\t\telse :\n\t\t\ttry :\n\t\t\t\tfor syn in wordnet.synsets(word): \n\t\t\t\t\tfor l in syn.lemmas(): \n\t\t\t\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n\t\t\t\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n\t\t\t\t\t\tsynonyms.add(synonym) \n\t\t\t\tif word in synonyms:\n\t\t\t\t\tsynonyms.remove(word)\n", "\t\texcept :\n\t\t\t\trandom_synset = random.choice(wordnet.synsets(random.choice(['noun', 'adjective', 'adverb', 'verb'])))\n\t\t\t\tsynonyms.add(random.choice(random_synset.lemma_names()))\n\t\treturn list(synonyms)\n\tdef random_deletion(words, p):\n\t\tif len(words) == 1:\n\t\t\treturn words\n\t\tnew_words = []\n\t\tfor word in words:\n\t\t\tr = random.uniform(0, 1)\n", "\t\tif r > p:\n\t\t\t\tnew_words.append(word)\n\t\tif len(new_words) == 0:\n\t\t\trand_int = random.randint(0, len(words)-1)\n\t\t\treturn [words[rand_int]]\n\t\treturn new_words\n\tdef random_swap(words, n):\n\t\tnew_words = words.copy()\n\t\tfor _ in range(n):\n\t\t\tnew_words = swap_word(new_words)\n", "\treturn new_words\n\tdef swap_word(new_words):\n\t\trandom_idx_1 = random.randint(0, len(new_words)-1)\n\t\trandom_idx_2 = random_idx_1\n\t\tcounter = 0\n\t\twhile random_idx_2 == random_idx_1:\n\t\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n\t\t\tcounter += 1\n\t\t\tif counter > 3:\n\t\t\t\treturn new_words\n", "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n\t\treturn new_words\n\tdef random_insertion(words, n):\n\t\tnew_words = words.copy()\n\t\tfor _ in range(n):\n\t\t\tadd_word(new_words)\n\t\treturn new_words\n\tdef add_word(new_words):\n\t\tsynonyms = []\n\t\tcounter = 0\n", "\tword_list = new_words.copy()\n\t\twhile len(synonyms) < 1:\n\t\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n\t\t\tsynonyms = get_new_word(random_word, word_list)\n\t\t\tcounter += 1\n\t\t\tif counter >= 10:\n\t\t\t\treturn\n\t\trandom_synonym = synonyms[0]\n\t\trandom_idx = random.randint(0, len(new_words)-1)\n\t\tnew_words.insert(random_idx, random_synonym)\n", "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n\t\twords = sentence.split(' ')\n\t\twords = [word for word in words if word != '']\n\t\tif len(words) <= 1 : return None\n\t\tnum_words = len(words)\n\t\taugmented_sentences = []\n\t\tnum_new_per_technique = int(num_aug/4)+1\n\t\t#sr\n\t\tif (alpha_sr > 0):\n\t\t\tn_sr = max(1, int(alpha_sr*num_words))\n", "\t\tfor _ in range(num_new_per_technique):\n\t\t\t\ta_words = random_replacement(words, n_sr)\n\t\t\t\taugmented_sentences.append(' '.join(a_words))\n\t\t#ri\n\t\tif (alpha_ri > 0):\n\t\t\tn_ri = max(1, int(alpha_ri*num_words))\n\t\t\tfor _ in range(num_new_per_technique):\n\t\t\t\ta_words = random_insertion(words, n_ri)\n\t\t\t\taugmented_sentences.append(' '.join(a_words))\n\t\t\t# print('ri')\n", "\t#rs\n\t\tif (alpha_rs > 0):\n\t\t\tn_rs = max(1, int(alpha_rs*num_words))\n\t\t\tfor _ in range(num_new_per_technique):\n\t\t\t\ta_words = random_swap(words, n_rs)\n\t\t\t\taugmented_sentences.append(' '.join(a_words))\n\t\t\t# print('rs')\n\t\t#rd\n\t\tif (p_rd > 0):\n\t\t\tfor _ in range(num_new_per_technique):\n", "\t\t\ta_words = random_deletion(words, p_rd)\n\t\t\t\taugmented_sentences.append(' '.join(a_words))\n\t\t\t# print('rd')\n\t\taugmented_sentences = [sentence for sentence in augmented_sentences]\n\t\tshuffle(augmented_sentences)\n\t\tif num_aug >= 1:\n\t\t\taugmented_sentences = augmented_sentences[:num_aug]\n\t\telse:\n\t\t\tkeep_prob = num_aug / len(augmented_sentences)\n\t\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n", "\taugmented_sentences.append(sentence)\n\t\treturn augmented_sentences"]}
{"filename": "DA/token_level_attack.py", "chunked_list": ["import random\n\timport json\n\timport math\n\timport numpy as np\n\trandom.seed(0)\n\tnp.random.seed(0)\n\tSYMBOL_LIST = ['#', '@', '～', '$', '%', '^', '&', '*', '-', '+', '=', '}', '{', '<', '>']\n\tclass ViualTemplate(object):\n\t    def __init__(self, file:str='DA\\\\visual_letter_map.json', topn:int=20, is_equal_prob:bool=False) -> None:\n\t        with open(file, 'r') as f:\n", "            self.template_dict = json.load(f)\n\t        # the hyperparameters are set according to https://github.com/UKPLab/naacl2019-like-humans-visual-attacks \n\t        self.topn = topn\n\t        self.is_equal_prob = is_equal_prob\n\t    def replace(self, c:str) -> str:\n\t        if c in self.template_dict:\n\t            chars = self.template_dict[c]['char'][0:self.topn]\n\t            if self.is_equal_prob:\n\t                new_c = np.random.choice(chars, 1, replace=True)[0]\n\t            else:\n", "                probs = np.array(self.template_dict[c]['similarity'][0:self.topn])\n\t                probs /= np.sum(probs)\n\t                new_c = np.random.choice(chars, 1, replace=True, p=probs)[0]\n\t            return new_c\n\t        else:\n\t            return c\n\tVISUAL_LETTER_TEMPLATE = ViualTemplate()\n\tdef attack_word(word, attack_type= 0, op_times = 1, **kwargs):\n\t    def delete_chars(text, deleted_length):\n\t        if deleted_length >= len(text):\n", "            return text\n\t        start = random.randint(0, len(text) - deleted_length)\n\t        return text[:start] + text[start + deleted_length:]\n\t    def repeat_char(text, repeat_num):\n\t        index = random.randint(0, len(text)-1)  \n\t        char = text[index] \n\t        new_text = text[:index] + char*(repeat_num+1) + text[index+1:] \n\t        return new_text\n\t    def insert_char(text, insert_num, insert_char):\n\t        for i in range(insert_num):\n", "            index = random.randint(1, len(text))\n\t            text = text[:index] + insert_char + text[index:]\n\t        return text\n\t    def repalce_char(text, ratio):\n\t        length = math.ceil(len(text) * ratio)\n\t        indices = set(random.sample(range(len(text)), length))\n\t        new_text = ''\n\t        for i in range(len(text)):\n\t            if i in indices:\n\t                new_text += VISUAL_LETTER_TEMPLATE.replace(text[i])\n", "            else:\n\t                new_text += text[i]\n\t        return new_text\n\t    attacked_word = None\n\t    if attack_type == 0:\n\t        attacked_word = delete_chars(word, op_times)\n\t    elif attack_type == 1:\n\t        attacked_word = repeat_char(word, op_times)\n\t    elif attack_type == 2:\n\t        attacked_word = insert_char(word, op_times, kwargs[\"insert_char\"]) \n", "    elif attack_type == 3:\n\t        attacked_word = repalce_char(word, kwargs[\"replace_ratio\"])\n\t    else:\n\t        raise Exception(\"No such attack type {}\".format(attack_type))\n\t    return attacked_word\n\tdef attack(sentence, attack_rate, attack_type, **kwargs):\n\t    words = sentence.split(' ')\n\t    indices = [i for i,w in enumerate(words) if len(w) > 1]\n\t    attack_word_count = math.ceil(len(indices)*attack_rate)\n\t    select_indices = set(random.sample(indices, attack_word_count))\n", "    attacked_words_list = []\n\t    for i in range(len(words)):\n\t        if i in select_indices:\n\t            attacked_words_list.append(attack_word(words[i], attack_type, **kwargs))\n\t        else:\n\t            attacked_words_list.append(words[i])\n\t    attacked_sentence = ' '.join(attacked_words_list)\n\t    return attacked_sentence\n\tif __name__ == '__main__':\n\t    sentence = \"Was Lil Jon's top ranked Billboard song a collaboration with a member of The Lox?\"\n", "    s1 = attack(sentence, attack_rate=0.1, attack_type=0, op_times=1)\n\t    s2 = attack(sentence, attack_rate=0.1, attack_type=1, op_times=2)\n\t    s3 = attack(sentence, attack_rate=0.1, attack_type=2, op_times=1, insert_char=SYMBOL_LIST[0])\n\t    s4 = attack(sentence, attack_rate=0.4, attack_type=3, replace_ratio=0.5)\n\t    print(s4)\n"]}
{"filename": "API/api.py", "chunked_list": ["import time\n\timport json\n\timport os\n\timport requests\n\timport tiktoken\n\timport logging\n\tfrom typing import List\n\tfrom queue import Queue\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tfrom tqdm import tqdm\n", "ENGINE = os.environ.get(\"GPT_ENGINE\") or \"gpt-3.5-turbo\"\n\tclass Chatbot:\n\t    \"\"\"\n\t    Official ChatGPT API\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        api_key: str,\n\t        engine: str = None,\n\t        proxy: str = None,\n", "        max_tokens: int = 4096,\n\t        temperature: float = 1.0,\n\t        top_p: float = 1.0,\n\t        presence_penalty: float = 0.0,\n\t        frequency_penalty: float = 0.0,\n\t        reply_count: int = 1,\n\t        system_prompt: str = \"You are a large language model. Respond conversationly\",\n\t        is_check_token_len: bool = False\n\t    ) -> None:\n\t        \"\"\"\n", "        Initialize Chatbot with API key (from https://platform.openai.com/account/api-keys)\n\t        \"\"\"\n\t        self.engine = engine or ENGINE\n\t        self.session = requests.Session()\n\t        self.api_key = api_key\n\t        self.proxy = proxy\n\t        if self.proxy:\n\t            proxies = {\n\t                \"http\": self.proxy,\n\t                \"https\": self.proxy,\n", "            }\n\t            self.session.proxies = proxies\n\t        self.conversation: dict = {\n\t            \"default\": [\n\t                {\n\t                    \"role\": \"system\",\n\t                    \"content\": system_prompt,\n\t                },\n\t            ],\n\t        }\n", "        self.system_prompt = system_prompt\n\t        self.max_tokens = max_tokens\n\t        self.temperature = temperature\n\t        self.top_p = top_p\n\t        self.presence_penalty = presence_penalty\n\t        self.frequency_penalty = frequency_penalty\n\t        self.reply_count = reply_count\n\t        self.last_request_time = 0\n\t        self.request_interval = 1  # seconds\n\t        self.max_backoff_time = 60  # seconds\n", "        self.is_check_token_len = is_check_token_len\n\t        if self.is_check_token_len and self.get_token_count(\"default\") > self.max_tokens:\n\t            raise Exception(\"System prompt is too long\")\n\t    def add_to_conversation(\n\t        self,\n\t        message: str,\n\t        role: str,\n\t        convo_id: str = \"default\",\n\t    ) -> None:\n\t        \"\"\"\n", "        Add a message to the conversation\n\t        \"\"\"\n\t        self.conversation[convo_id].append({\"role\": role, \"content\": message})\n\t    def __truncate_conversation(self, convo_id: str = \"default\") -> None:\n\t        \"\"\"\n\t        Truncate the conversation\n\t        \"\"\"\n\t        if self.is_check_token_len:\n\t            while True:\n\t                if (\n", "                    self.get_token_count(convo_id) > self.max_tokens\n\t                    and \n\t                    len(self.conversation[convo_id]) > 1\n\t                ):\n\t                    # Don't remove the first message\n\t                    self.conversation[convo_id].pop(1)\n\t                else:\n\t                    break\n\t    # https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\t    def get_token_count(self, convo_id: str = \"default\") -> int:\n", "        \"\"\"\n\t        Get token count\n\t        \"\"\"\n\t        if self.engine not in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\"]:\n\t            raise NotImplementedError(\"Unsupported engine {self.engine}\")\n\t        encoding = tiktoken.encoding_for_model(self.engine)\n\t        num_tokens = 0\n\t        for message in self.conversation[convo_id]:\n\t            # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n\t            num_tokens += 4\n", "            for key, value in message.items():\n\t                num_tokens += len(encoding.encode(value))\n\t                if key == \"name\":  # if there's a name, the role is omitted\n\t                    num_tokens += -1  # role is always required and always 1 token\n\t        num_tokens += 2  # every reply is primed with <im_start>assistant\n\t        return num_tokens\n\t    def ask(\n\t        self,\n\t        prompt: str,\n\t        role: str = \"user\",\n", "        convo_id: str = \"default\",\n\t        is_waiting: bool = True,\n\t        **kwargs,\n\t    ) -> str:\n\t        \"\"\"\n\t        Ask a question\n\t        \"\"\"\n\t        # Make conversation if it doesn't exist\n\t        if convo_id not in self.conversation:\n\t            self.reset(convo_id=convo_id, system_prompt=self.system_prompt)\n", "        self.add_to_conversation(prompt, \"user\", convo_id=convo_id)\n\t        self.__truncate_conversation(convo_id=convo_id)\n\t        is_retry = True\n\t        # logging.warning(prompt_id + \" ask question: \"+prompt)\n\t        while is_retry:\n\t            # Check if enough time has passed since the last request\n\t            elapsed_time = time.monotonic() - self.last_request_time\n\t            if elapsed_time < self.request_interval:\n\t                time.sleep(self.request_interval - elapsed_time)\n\t            self.last_request_time = time.monotonic()\n", "            # Get response\n\t            try:\n\t                response = self.session.post(\n\t                    \"https://api.openai.com/v1/chat/completions\",\n\t                    headers={\"Authorization\": f\"Bearer {kwargs.get('api_key', self.api_key)}\"},\n\t                    json={\n\t                        \"model\": self.engine,\n\t                        \"messages\": self.conversation[convo_id],\n\t                        \"stream\": False,\n\t                        # kwargs\n", "                        \"temperature\": kwargs.get(\"temperature\", self.temperature),\n\t                        \"top_p\": kwargs.get(\"top_p\", self.top_p),\n\t                        \"presence_penalty\": kwargs.get(\"presence_penalty\", self.presence_penalty),\n\t                        \"frequency_penalty\": kwargs.get(\"frequency_penalty\", self.frequency_penalty),\n\t                        \"n\": kwargs.get(\"n\", self.reply_count),\n\t                        \"user\": role,\n\t                        # \"max_tokens\": self.get_max_tokens(convo_id=convo_id),\n\t                    },\n\t                    stream=False,\n\t                )\n", "                is_retry = False\n\t            except:\n\t                logging.warning(\"Exceed max tries.\")\n\t            if is_retry or response.status_code != 200:\n\t                # raise Exception(\n\t                #     f\"Error: {response.status_code} {response.reason} {response.text}\",\n\t                # )\n\t                self.request_interval *= 2\n\t                if self.request_interval > self.max_backoff_time:\n\t                    self.request_interval = self.max_backoff_time\n", "                logging.warning(\n\t                    f\"Rate limit hit. Sleeping for {self.request_interval} seconds.\"\n\t                )\n\t                time.sleep(self.request_interval)\n\t                is_retry = True\n\t            else:\n\t                is_retry = False\n\t        resp: dict = json.loads(response.text)\n\t        choices = resp.get(\"choices\")\n\t        full_response = choices[0][\"message\"][\"content\"]\n", "        response_role = choices[0][\"message\"][\"role\"]\n\t        self.add_to_conversation(full_response, response_role, convo_id=convo_id)\n\t        return full_response\n\t    def rollback(self, n: int = 1, convo_id: str = \"default\") -> None:\n\t        \"\"\"\n\t        Rollback the conversation\n\t        \"\"\"\n\t        for _ in range(n):\n\t            self.conversation[convo_id].pop()\n\t    def reset(self, convo_id: str = \"default\", system_prompt: str = None) -> None:\n", "        \"\"\"\n\t        Reset the conversation\n\t        \"\"\"\n\t        self.conversation[convo_id] = [\n\t            {\"role\": \"system\", \"content\": system_prompt or self.system_prompt},\n\t        ]\n\t        self.last_request_time = 0\n\t        self.request_interval = 1  # seconds\n\t        self.max_backoff_time = 60  # seconds\n\t    def save(self, file: str, convo_id='default') -> bool:\n", "        \"\"\"\n\t        Save the conversation to a JSON file\n\t        \"\"\"\n\t        try:\n\t            with open(file, \"w\", encoding=\"utf-8\") as f:\n\t                json.dump(self.filter_answer(convo_id), f, indent=2)\n\t        except (FileNotFoundError, KeyError):\n\t            return False\n\t        return True\n\t    def outputs_all(self, convo_id='default', is_only_answer=True) -> list:\n", "        \"\"\"\n\t        Output chatgpt answers or conversation in self.conversation[convo_id]\n\t        \"\"\"\n\t        if is_only_answer:\n\t            return self.filter_answer(convo_id)\n\t        else:\n\t            return self.conversation[convo_id]\n\t    def filter_answer(self, convo_id):\n\t        '''\n\t        Gather chatgpt answers from conversation\n", "        '''\n\t        answer_list = []\n\t        for i, item in enumerate(self.conversation[convo_id]):\n\t            if item['role'] == 'assistant':\n\t                # self.conversation[convo_id][i-1]['content'] + ' --- ' \n\t                answer_list.append(item['content'])\n\t        return answer_list\n\tclass ChatbotWrapper:\n\t    def __init__(self, config: dict, bot_params={}) -> None:\n\t        \"\"\"\n", "        Initialize ChatbotWrapper with API key, proxy setting and parameters of ChatGPT API\n\t        \"\"\"\n\t        self.api_key = config['api_key']\n\t        self.proxy = config['proxy']\n\t        self.bot_params=bot_params\n\t    def ask_batch(self, batch_data: List[List[str]], thread_num=1) -> List[List[str]]:\n\t        \"\"\"\n\t        Ask a batch of questions.\n\t        \"\"\"\n\t        executor = ThreadPoolExecutor(max_workers=thread_num)\n", "        chatbot_q = Queue(maxsize=thread_num)\n\t        for j in range(thread_num):\n\t            chatbot_q.put(Chatbot(api_key=self.api_key[j%len(self.api_key)], proxy=self.proxy, **self.bot_params))\n\t        results = list(tqdm(executor.map(ChatbotWrapper.ask, [chatbot_q for i in range(len(batch_data))], batch_data), \n\t                       total=len(batch_data)))\n\t        batch_reponses = []\n\t        for i, res in enumerate(results):\n\t            batch_reponses.append(res)\n\t        return batch_reponses\n\t    @staticmethod\n", "    def ask(chatbot_q: Queue, questions: List[str]) -> List[str]:\n\t        if chatbot_q.empty():\n\t            raise Exception(\"no available chatbot\")\n\t        chatbot = chatbot_q.get()\n\t        for i,q in enumerate(questions):\n\t            chatbot.ask(q)\n\t        reponse_list = chatbot.outputs_all()\n\t        chatbot.reset()\n\t        chatbot_q.put(chatbot)\n\t        return reponse_list\n", "def logging_list(all_responses):\n\t    for i,re in enumerate(all_responses):\n\t            logging.warning(str(i) +'----'+re)\n\tdef batch_query_case(api_key : List[str], batch_questions : List[List[str]], num_thread = 4):\n\t    {'context' : batch_questions,\n\t     'tag' : list[str]}\n\t    chatbot = ChatbotWrapper(config={\"api_key\":api_key, \"proxy\":\"http://127.0.0.1:1087\"})\n\t    start = time.time() \n\t    all_responses = chatbot.ask_batch(batch_questions, num_thread)\n\t    end = time.time()\n", "    for i,q in enumerate(batch_questions):\n\t        print('='*20)\n\t        print(q)\n\t        for re in all_responses[i]:\n\t            print(str(i) +'----'+re, end = '\\n')\n\t        print()\n\t    total_time = end-start\n\t    print(\"total time \", total_time)\n\t    return total_time\n\tdef single_query_case(api_key : List[str], batch_questions : List[List[str]]):\n", "    test = ChatbotWrapper(config={\"api_key\":api_key, \"proxy\":\"http://127.0.0.1:1087\"})\n\t    start = time.time() \n\t    all_responses = test.ask_batch(batch_questions, 1)\n\t    end = time.time()\n\t    for i,q in enumerate(batch_questions):\n\t        print('='*20)\n\t        print(q)\n\t        for re in all_responses[i]:\n\t            print(str(i) +'----'+re, end = '\\n')\n\t        print()\n", "    total_time = end-start\n\t    print(\"total time \", total_time)\n\t    return total_time\n\tdef iterative_query_case(api_key : str, batch_questions : List[str]):\n\t    start = time.time()\n\t    chatbot = Chatbot(api_key=api_key,\n\t                       proxy=\"http://127.0.0.1:1087\")\n\t    for i,q in enumerate(batch_questions):\n\t        reponses = chatbot.ask(q)\n\t        print('='*30+str(i)+'='*30)\n", "        print(\"Question:\")\n\t        print(q)\n\t        print(\"Answer:\")\n\t        print(reponses)\n\t        print()\n\t        # break\n\t    end = time.time()\n\t    print(\"total time \", end-start)\n\t    chatbot.save('chatgpt_answer.json')\n\tdef multithread_test():\n", "    batch_time_dict = {}\n\t    for n_thread in [1, 2, 4, 6, 8, 10, 12]:\n\t        batch_time_dict[n_thread] = []\n\t        for _ in range(5):\n\t            t = batch_query_case(n_thread)\n\t            batch_time_dict[n_thread].append(t)\n\t        print(\"Num of thread {}, average time (seconds) {}\".format(n_thread, sum(batch_time_dict[n_thread])/5))\n\t        print(batch_time_dict[n_thread])\n\t    # This is result: {1: [153.27713584899902, 152.4092197418213, 146.94058966636658, 161.0742290019989, 153.52458786964417], 2: [87.31726479530334, 83.65080118179321, 72.72990918159485, 83.94411587715149, 83.73315715789795], 4: [40.27359890937805, 50.174768924713135, 96.05591201782227, 51.02015686035156, 50.2379310131073], 6: [54.035804986953735, 44.305612087249756, 43.24151921272278, 44.47617721557617, 41.63251209259033], 8: [27.781430959701538, 29.351155042648315, 24.695080041885376, 23.120115995407104, 25.153623819351196], 10: [25.5543532371521, 26.112723112106323, 26.006879091262817, 27.33828902244568, 24.6769859790802], 12: [26.724326848983765, 23.948792219161987, 28.022813081741333, 26.540624856948853, 24.228456020355225]}\n\t    print(batch_time_dict)\n", "if __name__ == '__main__':\n\t    api_key = [\"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\"]\n\t    batch_questions = [\n\t        ['Where is the geographical location of the United States?', 'What is the population of the United States?',  'What is the GDP situation of the United States?', 'Where is the capital of the United States?'],\n\t        ['Where is the geographical location of China?', 'What is the population of China?',  'What is the GDP situation of China?', 'Where is the capital of China?'],\n\t        ['Where is the geographical location of the United Kingdom?', 'What is the population of the United Kingdom?',  'What is the GDP situation of the United Kingdom?', 'Where is the capital of the United Kingdom?'],\n\t        ['Where is the geographical location of Thailand?', 'What is the population of Thailand?',  'What is the GDP situation of Thailand?', 'Where is the capital of Thailand?'],\n\t        ['Where is the geographical location of South Korea?', 'What is the population of South Korea?',  'What is the GDP situation of South Korea?', 'Where is the capital of South Korea?'],\n\t        ['Where is the geographical location of Japan?', 'What is the population of Japan?',  'What is the GDP situation of Japan?', 'Where is the capital of Japan?'],\n\t        ['Where is the geographical location of the Philippines?', 'What is the population of the Philippines?',  'What is the GDP situation of the Philippines?', 'Where is the capital of the Philippines?'],\n", "        ['美国的地理位置在哪里？', '美国的人口有多少？',  '美国的GDP情况？', '美国的首都在哪里？'],\n\t        ['中国的地理位置在哪里？', '中国的人口有多少？',  '中国的GDP情况？', '中国的首都在哪里？'],\n\t        ['英国的地理位置在哪里？', '英国的人口有多少？',  '英国的GDP情况？', '英国的首都在哪里？'],\n\t        ['泰国的地理位置在哪里？', '泰国的人口有多少？',  '泰国的GDP情况？', '泰国的首都在哪里？'],\n\t        ['韩国的地理位置在哪里？', '韩国的人口有多少？',  '韩国的GDP情况？', '韩国的首都在哪里？'],\n\t        ['日本的地理位置在哪里？', '日本的人口有多少？',  '日本的GDP情况？', '日本的首都在哪里？'],\n\t        ['菲律宾的地理位置在哪里？', '菲律宾的人口有多少？',  '菲律宾的GDP情况？', '菲律宾的首都在哪里？']\n\t    ]\n\t    # # test case 1\n\t    # batch_query_case(api_key, batch_questions, num_thread = 10)\n", "    # # test case 2\n\t    # single_query_case(api_key, batch_questions)\n\t    # # test case 3\n\t    # batch_questions = [ 'Next, I will ask you a series of questions given a description, and you will have to choose one of several candidate options that you think is correct.  The description is \\\"Ellen decided to play a prank on her friend. She got a case of 12 sodas and shook 3 of them up. Then she took 1 unshaken soda for herself and left. Ellen\\'s brother stopped by and took 1 of the shaken sodas and 2 of the unshaken sodas, then Ellen\\'s friend came along.\\\".  The first question is How many unshaken sodas were there when Ellen left the room?.  \\nThe options are:\\na) 7 unshaken sodas\\nb) 6 unshaken sodas \\nc) 2 unshaken sodas \\nd) None of the above options is correct']\n\t    # iterative_query_case(api_key[0], batch_questions)\n\t    # single_query_case()\n"]}
{"filename": "Evals/robEval.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport numpy as np\n\timport math\n\tdef get_score(para) :\n\t    rate_result, rate_sum, error_rate = {}, {}, {}\n\t    changed_rate = {}\n\t    error_rate['ori'], SUM = {0 : 0}, 0\n\t    for id in para : \n\t        SUM += len(para[id]['answer'])\n\t        for _ in range(len(para[id]['answer'])) :\n", "            if para[id]['answer'][_] != 'GroundTruth' : error_rate['ori'][0] += 1\n\t    TYPE = ['character_level_repeat', 'character_level_delete', 'character_level_insert',\n\t            'word_level_ri', 'word_level_rd', 'word_level_rp',\n\t            'visual_attack_10%', 'visual_attack_50%', 'visual_attack_90%']\n\t    for i in range(len(TYPE)) : changed_rate[TYPE[i]], rate_result[TYPE[i]], rate_sum[TYPE[i]], error_rate[TYPE[i]] = 0, {}, {}, {}\n\t    for _ in range(len(TYPE)) :\n\t        attack_type = TYPE[_]\n\t        for id in para :\n\t            for i in range(len(para[id]['new_passage'])) :\n\t                if para[id]['new_passage'][i]['attack_type'] != attack_type : continue\n", "                # print(id)\n\t                rate = para[id]['new_passage'][i]['rate']\n\t                if rate not in rate_result[attack_type] : \n\t                    rate_result[attack_type][rate], rate_sum[attack_type][rate], error_rate[attack_type][rate] = 0, 0, 0\n\t                rate_sum[attack_type][rate] += len(para[id]['new_passage'][i]['answer'])\n\t                for _ in range(len(para[id]['new_passage'][i]['answer'])) :\n\t                    if para[id]['new_passage'][i]['answer'][_] != 'GroundTruth' : \n\t                        error_rate[attack_type][rate] += 1\n\t                    if para[id]['answer'][_] != para[id]['new_passage'][i]['answer'][_] :\n\t                        rate_result[attack_type][rate] += 1\n", "    for attack_type in error_rate : \n\t        try :\n\t            error_rate[attack_type] = sum([error_rate[attack_type][key] for key in error_rate[attack_type].keys()])\n\t            error_rate[attack_type] /= SUM if attack_type == 'ori' else sum([rate_sum[attack_type][key] for key in rate_sum[attack_type].keys()])\n\t            error_rate[attack_type] = round(error_rate[attack_type]*100, 2)\n\t            if attack_type == 'ori' : continue\n\t            changed_rate[attack_type] = sum([rate_result[attack_type][key] for key in rate_result[attack_type].keys()])/sum([rate_sum[attack_type][key] for key in rate_sum[attack_type].keys()])\n\t            changed_rate[attack_type] = round(changed_rate[attack_type]*100, 2)\n\t        except: continue\n\t    return error_rate, changed_rate, SUM\n", "def draw_table(rate_result : dict, rate_sum : dict, type : str) :\n\t    width = 0.35\n\t    plt.rcParams.update({'font.size': 10})\n\t    keys = sorted(list(rate_result.keys()))\n\t    plt.bar(np.arange(len(rate_result)), tuple(rate_result[key] for key in keys), width, label=\"Changed\", color=\"#87CEFA\")\n\t    plt.bar([i+width for i in np.arange(len(rate_sum))], tuple(rate_sum[key] for key in keys), width, label=\"Total\", color=\"#0078BA\")\n\t    plt.xlabel('Rate' )\n\t    plt.ylabel('Queries')\n\t    # plt.ylabel('Changed Rate')\n\t    num = max([rate_sum[key] for key in rate_sum.keys()])\n", "    plt.title(type)\n\t    plt.xticks(np.arange(len(rate_result)), keys)\n\t    plt.yticks(np.arange(0, num, math.ceil(num/10)))\n\t    # plt.yticks(np.arange(0, 1, 0.1))\n\t    # plt.rcParams.update({'font.size': 35})\n\t    plt.legend(loc=\"upper right\")\n\tif __name__ == '__main__' :\n\t    pass "]}
{"filename": "Evals/conEval.py", "chunked_list": ["def get_score(para, TYPE = 'prompt') :\n\t    gt_result, changed_result, SUM = {}, {}, 0\n\t    gt_result['ori'] = {'score' : 0}\n\t    for id in para : \n\t        SUM += len(para[id]['answer'])\n\t        for _ in range(len(para[id]['answer'])) :\n\t            if para[id]['answer'][_] == 'GroundTruth' : gt_result['ori']['score'] += 1\n\t        for index, opt in enumerate(para[id]['new_passage']) :\n\t            if str(index+1) not in gt_result : \n\t                gt_result[str(index+1)], changed_result[str(index+1)] = {'score' : 0}, {'score' : 0}\n", "            for __, answer in enumerate(opt['answer']) :\n\t                if answer == 'GroundTruth' : gt_result[str(index+1)]['score'] += 1\n\t                if para[id]['answer'][__] != answer : changed_result[str(index+1)]['score'] += 1\n\t    if TYPE == 'prompt' :\n\t        for key in gt_result :\n\t            if key == 'ori' : continue\n\t            gt_result[key]['prompt'] = changed_result[key]['prompt'] = para['0']['new_prompt'][int(key)-1]['prompt']\n\t    for key in gt_result :\n\t        gt_result[key]['score'] = gt_result[key]['score']/SUM*100\n\t        if key == 'ori' : continue\n", "        changed_result[key]['score'] = changed_result[key]['score']/SUM*100\n\t    return gt_result, changed_result, SUM\n\tif __name__ == '__name__' :\n\t    pass"]}
{"filename": "Evals/creEval.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport numpy as np\n\timport math\n\tdef get_rate(para) :\n\t    rate_list = {'word_level_ri' : [], 'word_level_rd' : [], 'word_level_rp' : []}\n\t    for id in para :\n\t        answer = para[id]['answer']\n\t        _dict = []\n\t        for _, type in enumerate(['word_level_ri', 'word_level_rd', 'word_level_rp']) :\n\t            for __ in range(len(answer)) : \n", "                _dict[type] = 0\n\t                for ID in range(10*_, 10*_+10) :\n\t                    if para[id]['new_passage'][ID]['AnswerPath'][__] != answer[__] :\n\t                        _dict[type] = para[id]['new_passage'][ID]['rate']\n\t                        break\n\t                if _dict[type] == 0 : _dict[type] = 1.0\n\t                rate_list[type].append(_dict[type])\n\t    for _, type in enumerate(['word_level_ri', 'word_level_rd', 'word_level_rp']) :\n\t            plt.subplot(1, 3, _+1)\n\t            draw_table(rate_list[type], type)\n", "    return rate_list\n\tdef draw_table(rate_list : list, type : str) :\n\t    width = 0.35\n\t    plt.rcParams.update({'font.size': 10})\n\t    key, dict = [], {}\n\t    for _ in rate_list : \n\t        if _ not in key : key.append(_); dict[_] = 1\n\t        else : dict[_] += 1\n\t    key = sorted(key)\n\t    x = np.array(rate_list)\n", "    y = np.array([1 for _ in rate_list])\n\t    plt.bar(np.arange(len(key)), tuple(dict[i] for i in key), width, label=\"Changed\", color=\"#87CEFA\")\n\t    plt.xlabel('Rate' )\n\t    plt.ylabel('State')\n\t    num = max([dict[i] for i in key])\n\t    plt.title(type)\n\t    plt.xticks(np.arange(len(key)), key)\n\t    plt.yticks(np.arange(0, num, math.ceil(num/10)))\n\tif __name__ == '__main__' :\n\t    pass "]}
{"filename": "Evals/get_path.py", "chunked_list": ["import re\n\timport tqdm\n\tdef judge(response, _dict) :\n\t    id = 0\n\t    result = []\n\t    for a in response[1:] :\n\t        match = re.search(r'(\\([A-E]\\))', a)\n\t        match1, match2 = re.search(r'([A-E]\\))', a), re.search(r'(\\s[A-E]\\s)', a)\n\t        match = match if match != None else match1 if match1 != None else match2\n\t        item = None\n", "        if match != None : item = ord(match.group(0)[1])-65 if ord(match.group(0)[1]) >= ord('A') and ord(match.group(0)[1]) <= ord('Z') else ord(match.group(0)[0])-65\n\t        else :\n\t            for opt in _dict['sub-qa'][id]['input'] :\n\t                ITEM = str(_dict['sub-qa'][id]['options'][opt]['option_describe'])\n\t                ITEM = ITEM[0].lower()+ITEM[1:] if len(ITEM) <= 1 else ITEM[0].lower()\n\t                ITEM1 = ITEM[0].upper()+ITEM[1:] if len(ITEM) <= 1 else ITEM[0].upper()\n\t                # print(ITEM, ITEM1)\n\t                if a.find(ITEM) != -1 or a.find(ITEM1) != -1 : item = opt; break\n\t        # print(a)\n\t        try :\n", "            _dict['sub-qa'][id]['input'][item]\n\t        except:\n\t            _dict.append('No options'); id += 1; continue\n\t        type = _dict['sub-qa'][id]['options'][_dict['sub-qa'][id]['input'][item]]['option_type']\n\t        result.append(type)\n\t        id += 1\n\t    while (id != len(_dict['sub-qa'])) : \n\t        _dict.append('No options'); id += 1\n\t    return result\n\tdef get_path(para, data) :\n", "    print(\"*********************STRAT GETTING ANSWER PATH*********************\")\n\t    for i in tqdm(para) :\n\t        para[i]['answer'] = judge(para[i]['response'], data[i])\n\t        for q in para[i]['new_passage'] :\n\t            q['answer'] = judge(q['response'], data[i])\n\t    print(\"*********************END GETTING ANSWER PATH*********************\")\n\t    return para"]}
{"filename": "Analysor/analysis_dep_tag.py", "chunked_list": ["import spacy\n\tfrom spacy.tokens import Doc\n\tfrom spacy.parts_of_speech import IDS as POS_IDS\n\timport json\n\tfrom typing import List, Tuple, Dict\n\timport argparse\n\tfrom tqdm import tqdm\n\timport numpy as np\n\tfrom sklearn.ensemble import RandomForestClassifier\n\timport re\n", "from collections import Counter\n\t# https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md\n\t# https://github.com/explosion/spaCy/blob/b69d249a223fa4e633e11babc0830f3b68df57e2/spacy/glossary.py\n\tDEPENDENCY_LABELS = {\n\t    \"acl\": \"clausal modifier of noun (adjectival clause)\",\n\t    \"acomp\": \"adjectival complement\",\n\t    \"advcl\": \"adverbial clause modifier\",\n\t    \"advmod\": \"adverbial modifier\",\n\t    \"agent\": \"agent\",\n\t    \"amod\": \"adjectival modifier\",\n", "    \"appos\": \"appositional modifier\",\n\t    \"attr\": \"attribute\",\n\t    \"aux\": \"auxiliary\",\n\t    \"auxpass\": \"auxiliary (passive)\",\n\t    \"case\": \"case marking\",\n\t    \"cc\": \"coordinating conjunction\",\n\t    \"ccomp\": \"clausal complement\",\n\t    \"clf\": \"classifier\",\n\t    \"complm\": \"complementizer\",\n\t    \"compound\": \"compound\",\n", "    \"conj\": \"conjunct\",\n\t    \"cop\": \"copula\",\n\t    \"csubj\": \"clausal subject\",\n\t    \"csubjpass\": \"clausal subject (passive)\",\n\t    \"dative\": \"dative\",\n\t    \"dep\": \"unclassified dependent\",\n\t    \"det\": \"determiner\",\n\t    \"discourse\": \"discourse element\",\n\t    \"dislocated\": \"dislocated elements\",\n\t    \"dobj\": \"direct object\",\n", "    \"expl\": \"expletive\",\n\t    \"fixed\": \"fixed multiword expression\",\n\t    \"flat\": \"flat multiword expression\",\n\t    \"goeswith\": \"goes with\",\n\t    \"hmod\": \"modifier in hyphenation\",\n\t    \"hyph\": \"hyphen\",\n\t    \"infmod\": \"infinitival modifier\",\n\t    \"intj\": \"interjection\",\n\t    \"iobj\": \"indirect object\",\n\t    \"list\": \"list\",\n", "    \"mark\": \"marker\",\n\t    \"meta\": \"meta modifier\",\n\t    \"neg\": \"negation modifier\",\n\t    \"nmod\": \"modifier of nominal\",\n\t    \"nn\": \"noun compound modifier\",\n\t    \"npadvmod\": \"noun phrase as adverbial modifier\",\n\t    \"nsubj\": \"nominal subject\",\n\t    \"nsubjpass\": \"nominal subject (passive)\",\n\t    \"nounmod\": \"modifier of nominal\",\n\t    \"npmod\": \"noun phrase as adverbial modifier\",\n", "    \"num\": \"number modifier\",\n\t    \"number\": \"number compound modifier\",\n\t    \"nummod\": \"numeric modifier\",\n\t    \"oprd\": \"object predicate\",\n\t    \"obj\": \"object\",\n\t    \"obl\": \"oblique nominal\",\n\t    \"orphan\": \"orphan\",\n\t    \"parataxis\": \"parataxis\",\n\t    \"partmod\": \"participal modifier\",\n\t    \"pcomp\": \"complement of preposition\",\n", "    \"pobj\": \"object of preposition\",\n\t    \"poss\": \"possession modifier\",\n\t    \"possessive\": \"possessive modifier\",\n\t    \"preconj\": \"pre-correlative conjunction\",\n\t    \"predet\" : \"\", # manual add\n\t    \"prep\": \"prepositional modifier\",\n\t    \"prt\": \"particle\",\n\t    \"punct\": \"punctuation\",\n\t    \"quantmod\": \"modifier of quantifier\",\n\t    \"rcmod\": \"relative clause modifier\",\n", "    \"relcl\": \"relative clause modifier\",\n\t    \"reparandum\": \"overridden disfluency\",\n\t    \"root\": \"root\",\n\t    \"ROOT\": \"root\",\n\t    \"vocative\": \"vocative\",\n\t    \"xcomp\": \"open clausal complement\",\n\t}\n\tdef find_attacked_word(ori_sent:str, attack_sent:str, attack_type:str) -> Tuple[List[str], List[int]]:\n\t    ori_list = ori_sent.split(' ')\n\t    attack_list = attack_sent.split(' ')\n", "    if 'character_level' in attack_type or 'visual' in attack_type:\n\t        if len(ori_list) != len(attack_list):\n\t            print(\"=\"*20+ \"error\")\n\t            print(ori_list)\n\t            print(attack_list)\n\t        attacked_words, attacked_indices = [], []\n\t        for i, word in enumerate(ori_list):\n\t            if word != attack_list[i]:\n\t                attacked_words.append(word)\n\t                attacked_indices.append(i)\n", "        return attacked_words, attacked_indices\n\t    else:\n\t        attacked_words, attacked_indices = [], []\n\t        attack_dict = Counter(attack_list)\n\t        if attack_type == 'word_level_rd' or attack_type == 'word_level_rp':\n\t            for i, word in enumerate(ori_list):\n\t                if word not in attack_dict or attack_dict[word] <= 0:\n\t                    attacked_words.append(word)\n\t                    attacked_indices.append(i)\n\t                    # 逻辑待完善\n", "                if word in attack_dict: \n\t                    attack_dict[word] -= 1\n\t            return attacked_words, attacked_indices\n\t        else:\n\t            index = 0\n\t            for i, word in enumerate(ori_list):\n\t                if len(word) == 0:\n\t                    continue\n\t                is_attacked = False\n\t                while index < len(attack_list):\n", "                    if attack_list[index] == word:\n\t                        if (i == 0 and index != 0) \\\n\t                            or (i != 0 and index != 0 and ori_list[i-1] != attack_list[index-1]) \\\n\t                            or (i == len(ori_list)-1 and index != len(attack_list)-1) \\\n\t                            or (i != len(ori_list)-1 and index != len(attack_list)-1 and ori_list[i+1] != attack_list[index+1]):\n\t                            is_attacked = True\n\t                        index += 1\n\t                        break\n\t                    index += 1\n\t                if is_attacked:\n", "                    attacked_words.append(word)\n\t                    attacked_indices.append(i)       \n\t        return attacked_words, attacked_indices\n\tdef get_attacked_word_tag_info(ori_sent_doc:Doc, attacked_words:List[str]) -> List[int]:\n\t    def remove_non_letters(input_str):\n\t        return re.sub(r'[^a-zA-Z]', '', input_str)\n\t    aw_set = set(attacked_words)\n\t    aw_set.add(remove_non_letters(attacked_words[-1]))\n\t    tag_counter = {k:0 for k in POS_IDS.keys()}\n\t    for token in ori_sent_doc:\n", "        if token.text in aw_set:\n\t            tag_counter[token.pos_] += 1\n\t    # print(token.text, token.pos_, token.tag_, token.is_alpha, token.is_stop)\n\t    return list(tag_counter.values())\n\tdef get_relation_dict(ori_sent_doc:Doc) -> Tuple[Dict, Dict]:\n\t    token_index = {token:i for i, token in enumerate(ori_sent_doc)}\n\t    relation_dict = {}\n\t    total_dependency_counter = {l:0 for l in DEPENDENCY_LABELS.keys()}\n\t    for i, token in enumerate(ori_sent_doc):\n\t        if (token_index[token.head], token.head.text) not in relation_dict:\n", "            relation_dict[(token_index[token.head], token.head.text)] = []\n\t        if (i, token.text) not in relation_dict:\n\t            relation_dict[(i, token.text)] = []\n\t        relation_dict[(token_index[token.head], token.head.text)].append((i, token.text, token.dep_))\n\t        relation_dict[(i, token.text)].append((i, token.head.text, token.dep_))\n\t        total_dependency_counter[token.dep_] += 1\n\t    return relation_dict, total_dependency_counter\n\tdef get_attacked_dependency_relation(ori_sent_doc:Doc, attacked_dict:Dict, relation_dict:Dict, total_dependency_counter:Dict) -> List[int]:\n\t    attacked_relation_set = set()\n\t    attacked_dependency_counter = {l:0 for l in DEPENDENCY_LABELS.keys()}\n", "    for i, token in enumerate(ori_sent_doc):\n\t        if token.text in attacked_dict and i-1 <= attacked_dict[token.text] <= i+1:\n\t            for rela in relation_dict[(i, token.text)]:\n\t                if (i, rela[0]) not in attacked_relation_set:\n\t                    attacked_relation_set.add((i, rela[0]))\n\t                    attacked_relation_set.add((rela[0], i))\n\t                    attacked_dependency_counter[token.dep_] += 1\n\t    eps = 1e-10\n\t    attacked_dependency_counts= [attacked_dependency_counter[k]/(total_dependency_counter[k]+eps) for k in attacked_dependency_counter.keys()]\n\t    return attacked_dependency_counts\n", "def cal_tag_importance(feat, label) -> None:\n\t    if feat.shape[1] == len(list(POS_IDS.keys())):\n\t        feat_labels = list(POS_IDS.keys())\n\t    else:\n\t        feat_labels = list(DEPENDENCY_LABELS.keys())\n\t    X = feat\n\t    Y = label\n\t    clf = RandomForestClassifier(n_estimators=100)\n\t    clf = clf.fit(X, Y)\n\t    importances = clf.feature_importances_\n", "    indices = np.argsort(importances)[::-1]\n\t    print(clf.score(X, Y))\n\t    for f in range(10):\n\t        print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))\n\tdef convert_attack_type(a_t:str) -> str:\n\t    if 'character_level' in a_t:\n\t        return 'char_level'\n\t    elif 'visual' in a_t:\n\t        return 'visual_level'\n\t    else:\n", "        return 'word_level'\n\tdef check_empty_attack(attack_words, ori_context, para):\n\t    if len(attack_words) == 0:\n\t        if ori_context != para:\n\t            print(\"============= no change ===============\")\n\t            print(ori_context)\n\t            print(para)\n\t        return True\n\t    return False\n\tdef main(file_path:str, analysis_type:str):\n", "    with open(file_path, 'r') as f:\n\t        data_dict = json.load(f)\n\t    spacy_model = spacy.load(\"en_core_web_md\")\n\t    statistics_data = {\n\t                        'char_level':{analysis_type+'_vec':[], 'word_num':[], 'is_changed':[]},\n\t                        'visual_level':{analysis_type+'_vec':[], 'word_num':[], 'is_changed':[]},\n\t                        'word_level':{analysis_type+'_vec':[], 'word_num':[], 'is_changed':[]}\n\t                        }\n\t    for id, item in tqdm(data_dict.items()):\n\t        para_list = item[\"new_passage\"]\n", "        ori_sent_doc = spacy_model(item[\"passage\"])\n\t        if analysis_type == 'dep':\n\t            relation_dict, total_dependency_counter = get_relation_dict(ori_sent_doc)\n\t        for para_item in para_list:\n\t            attack_words, attack_indices = find_attacked_word(item[\"passage\"], para_item[\"para\"], para_item[\"attack_type\"])\n\t            if check_empty_attack(attack_words, item[\"passage\"], para_item[\"para\"]): continue\n\t            curr_dict = statistics_data[convert_attack_type(para_item[\"attack_type\"])]\n\t            if analysis_type == 'tag':\n\t                vec = get_attacked_word_tag_info(ori_sent_doc, attack_words)\n\t            else:\n", "                attack_dict = {attack_words[j]:attack_indices[j] for j in range(len(attack_words))}\n\t                vec = get_attacked_dependency_relation(ori_sent_doc, attack_dict, relation_dict, total_dependency_counter)\n\t            curr_dict[analysis_type+'_vec'].append(vec)\n\t            curr_dict['word_num'].append(len(ori_sent_doc))\n\t            change_rate = sum(list(map(lambda a, b: 0 if a==b else 1, item[\"ori_answer\"], para_item[\"AnswerPath\"])))/len(item[\"ori_answer\"])\n\t            curr_dict['is_changed'].append(int(change_rate))\n\t    for a_type in statistics_data.keys():\n\t        print('='*20 + a_type +'='*20)\n\t        if analysis_type == 'tag':\n\t            cal_tag_importance(\n", "                np.asarray(statistics_data[a_type]['tag_vec'])/np.expand_dims(np.asarray(statistics_data[a_type]['word_num']), axis=1), \n\t                np.asarray(statistics_data[a_type]['is_changed']))\n\t        else:\n\t            cal_tag_importance(\n\t                np.asarray(statistics_data[a_type]['dep_vec']),  np.asarray(statistics_data[a_type]['is_changed']))\n\t    return statistics_data\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--indir\",\n", "        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n\t        help = \"input directory\"\n\t    )\n\t    opt = parser.parse_args()\n\t    main(opt.indir, analysis_type='dep') # or 'tag'"]}
{"filename": "Analysor/analysis_pos_parser.py", "chunked_list": ["import numpy as np\n\timport argparse\n\timport json\n\timport re\n\timport math\n\timport matplotlib.pyplot as plt\n\tfrom stanfordcorenlp import StanfordCoreNLP\n\tPASER_LABEL = {\n\t    \"ADJP\":\t\"Adjective Phrase.\",\n\t    \"ADVP\": \"Adverb Phrase.\",\n", "    \"CONJP\": \"Conjunction Phrase.\",\n\t    \"FRAG\": \"Fragment.\",\n\t    \"INTJ\": \"Interjection. Corresponds approximately to the part-of-speech tag UH.\",\n\t    \"LST\": \"List marker. Includes surrounding punctuation.\",\n\t    \"NAC\": \"Not a Constituent; used to show the scope of certain prenominal modifiers within an NP.\",\n\t    \"NP\": \"Noun Phrase.\",\n\t    \"NX\": \"Used within certain complex NPs to mark the head of the NP. Corresponds very roughly to N-bar level but used quite differently.\",\n\t    \"PP\": \"Prepositional Phrase.\",\n\t    \"PRN\": \"Parenthetical.\",\n\t    \"PRT\": \"Particle. Category for words that should be tagged RP.\",\n", "    \"QP\": \"Quantifier Phrase (i.e. complex measure/amount phrase); used within NP.\",\n\t    \"RRC\": \"Reduced Relative Clause.\",\n\t    \"UCP\": \"Unlike Coordinated Phrase.\",\n\t    \"VP\": \"Vereb Phrase.\",\n\t    \"WHADJP\": \"Wh-adjective Phrase. Adjectival phrase containing a wh-adverb, as in how hot.\",\n\t    \"WHAVP\": \"Wh-adverb Phrase. Introduces a clause with an NP gap. May be null (containing the 0 complementizer) or lexical, containing a wh-adverb such as how or why.\",\n\t    \"WHNP\": \"Wh-noun Phrase. Introduces a clause with an NP gap. May be null (containing the 0 complementizer) or lexical, containing some wh-word, e.g. who, which book, whose daughter, none of which, or how many leopards.\",\n\t    \"WHPP\": \"Wh-prepositional Phrase. Prepositional phrase containing a wh-noun phrase (such as of which or by whose authority) that either introduces a PP gap or is contained by a WHNP.\",\n\t    \"X\": \"Unknown, uncertain, or unbracketable. X is often used for bracketing typos and in bracketing the...the-constructions.\"\n\t}\n", "class Para_analyse : \n\t    def __init__(\n\t            self\n\t    ) :\n\t        self.para = {}\n\t        self.result = {}\n\t    def read_para(self, indir) :\n\t        self.para = json.load(open(indir, encoding = 'utf-8'))\n\t        self.result['dataset'] = indir.split('\\\\')[-2]\n\t        return self.para\n", "    @staticmethod\n\t    def word_compare(ori_list, new_list, pos) -> bool :\n\t        for i in range(pos-4,pos+5) :\n\t            try :\n\t                if new_list[i] == ori_list[pos] : return True\n\t            except : continue\n\t        return False\n\t    def compare_para(self, ori_str : str, new_str : str, type : str, id : str) -> tuple :\n\t        # print(ori_str, '\\n', new_str)\n\t        ori_str, new_str = ori_str.strip(), new_str.strip()\n", "        ori_list, new_list = re.split(r'\\s+', ori_str), re.split(r'\\s+', new_str)\n\t        if (int(id) < 22918) :\n\t            ori_list[-1] = ori_list[-1][:-1]\n\t            new_list[-1] = new_list[-1][:-1]\n\t        ori_list = [word for word in ori_list if word != '']\n\t        new_list = [word for word in new_list if word != '']\n\t        result_list = []\n\t        mx_index = None\n\t        if type == 'word_level_ri' :\n\t            flag = 0\n", "            for i in range(len(ori_list)) : \n\t                # print(ori_list)\n\t                try :\n\t                    new_pos = new_list[flag:].index(ori_list[i])\n\t                except :\n\t                    print(ori_str, '\\n', new_str)\n\t                    raise 'YWT'\n\t                result_list.extend([i]*(new_pos-flag))\n\t                flag = new_pos+1\n\t            result_list.extend([len(ori_list)]*(len(new_list)-flag))\n", "            mx_index = len(ori_list)\n\t        elif type == 'word_level_rd' :\n\t            flag = 0\n\t            result_list = list(range(len(ori_list)))\n\t            # print(ori_list, '\\n', new_list)\n\t            for i in range(len(new_list)) : \n\t                try :\n\t                    new_pos = ori_list[flag:].index(new_list[i])\n\t                except:\n\t                    print(ori_str, '\\n', new_str)\n", "                    raise 'error'\n\t                # print(flag, new_list[i],new_pos+flag)\n\t                result_list.remove(new_pos+flag)\n\t                flag = new_pos+flag+1\n\t        else :\n\t            for i in range(len(ori_list)) :\n\t                # if 'visual' in type or 'character' in type :\n\t                #     if ori_list[i] != new_list[i] : result_list.append(i)\n\t                # else :\n\t                if not self.word_compare(ori_list, new_list, i) : result_list.append(i)\n", "                else : continue\n\t        if mx_index == None : mx_index = len(ori_list)-1\n\t        return mx_index, result_list\n\t    def para_pos_statistic(self, judge = 'GroundTruth') :\n\t        length = min([len(re.split(r'\\s+', self.para[id]['ori_context'])) for id in self.para])\n\t        segment = 5\n\t        TYPE = ['character_level_repeat', 'character_level_delete', 'character_level_insert',\n\t                'word_level_ri', 'word_level_rd', 'word_level_rp',\n\t                'visual_attack_10%', 'visual_attack_50%', 'visual_attack_90%']\n\t        for _ in TYPE : self.result[_] = {'head' : 0, '1/5' : 0, '2/5' : 0, '3/5' : 0, '4/5' : 0, '5/5' : 0, 'tail' : 0}\n", "        for id in self.para :\n\t            print(id)\n\t            ori_para = self.para[id]['ori_context']\n\t            for _dict in self.para[id]['new_context'] :\n\t                attack_type, new_para, rate = _dict['attack_type'], _dict['para'], _dict['rate']\n\t                weight = 0\n\t                for index in range(len(_dict['AnswerPath'])) :\n\t                    if judge == 'GroundTruth' : \n\t                        if _dict['AnswerPath'][index] == judge : continue\n\t                    else : \n", "                        if _dict['AnswerPath'][index] == self.para[id]['ori_answer'][index] : continue\n\t                    weight += 1\n\t                if weight > 0 :\n\t                    mx_index, id_list = self.compare_para(ori_para, new_para, attack_type, id)\n\t                    mn_index, count_times = 0, len(id_list)\n\t                    gap = (mx_index-mn_index+1e-5)/segment\n\t                    discrete_index = [1+int((_-mn_index)/gap) for _ in id_list if _ != mn_index and _ != mx_index]\n\t                    # print(id_list, mx_index, discrete_index)\n\t                    # return\n\t                    pos = 0\n", "                    for i in id_list :\n\t                        if i == mn_index or i == mx_index :\n\t                            if i == mn_index : self.result[attack_type]['head'] += weight*(1/count_times if mn_index != mx_index else 1/2/count_times)\n\t                            if i == mx_index : self.result[attack_type]['tail'] += weight*(1/count_times if mn_index != mx_index else 1/2/count_times)\n\t                            continue \n\t                        self.result[attack_type][str(discrete_index[pos])+'/5'] += weight*1/count_times\n\t                        pos += 1\n\t    @staticmethod\n\t    def get_parse(nlp, sentence) :\n\t        return nlp.parse(sentence)\n", "    def para_parser_statistic(self, judge = 'GroundTruth') :\n\t        nlp = StanfordCoreNLP(r'stanford-corenlp-4.5.4')\n\t        TYPE = ['character_level_repeat', 'character_level_delete', 'character_level_insert',\n\t                'word_level_ri', 'word_level_rd', 'word_level_rp',\n\t                'visual_attack_10%', 'visual_attack_50%', 'visual_attack_90%']\n\t        for _ in TYPE : self.result[_] = {key: 0 for key in PASER_LABEL.keys()}\n\t        for id in self.para :\n\t            print(id)\n\t            ori_para = self.para[id]['ori_context']\n\t            for _dict in self.para[id]['new_context'] :\n", "                attack_type, new_para, rate = _dict['attack_type'], _dict['para'], _dict['rate']\n\t                weight = 0\n\t                for index in range(len(_dict['AnswerPath'])) :\n\t                    if judge == 'GroundTruth' : \n\t                        if _dict['AnswerPath'][index] == judge : continue\n\t                    else : \n\t                        if _dict['AnswerPath'][index] == self.para[id]['ori_answer'][index] : continue\n\t                    weight += 1\n\t                if weight > 0 :\n\t                    ori_list = re.split(r'\\s+', ori_para)\n", "                    _, id_list = self.compare_para(ori_para, new_para, attack_type, id)\n\t                    count_times = len(id_list)\n\t                    parser_list = [i.strip() for i in self.get_parse(nlp, ori_para).split('\\r\\n')]\n\t                    # print(self.get_parse(nlp, ori_para))\n\t                    pos = 0\n\t                    for i in id_list :\n\t                        if i == len(ori_list) : continue\n\t                        for __, parser in enumerate(parser_list[pos:]) : \n\t                            if parser.find(ori_list[i]+')') != -1 : \n\t                                # print(ori_list[i], parser)\n", "                                pos = pos+__+1\n\t                                try :\n\t                                    self.result[attack_type][parser[1:parser.find(' ')]]  += weight*1/count_times\n\t                                except : continue\n\t    @staticmethod\n\t    def draw_table(rate_result : dict, type : str) :\n\t        width = 0.35\n\t        keys = list(rate_result.keys())\n\t        RATE = [rate_result[key] for key in keys]\n\t        plt.bar(np.arange(len(rate_result)), tuple(rate_result[key] for key in keys), width, label=\"Importance\", color=plt.cm.Blues(np.asarray(RATE)/np.array(max(RATE))))\n", "        plt.xlabel('Type', fontsize = 25)\n\t        plt.ylabel('Frequency',fontsize = 25)\n\t        # plt.ylabel('Changed Rate')\n\t        num = max([rate_result[key] for key in rate_result.keys()])\n\t        if type == 'word_level_ri' : Type = 'word_level_insert'\n\t        elif type == 'word_level_rd' : Type = 'word_level_delete'\n\t        elif type == 'word_level_rp' : Type = 'word_level_replace'\n\t        else : Type = type\n\t        plt.title(Type, fontsize = 25)\n\t        plt.xticks(np.arange(len(rate_result)), keys)\n", "        plt.yticks(np.arange(0, num, math.ceil(num/10)))\n\t        # plt.yticks(np.arange(0, 1, 0.1))\n\t        # plt.rcParams.update({'font.size': 35})\n\t        plt.legend(loc=\"upper right\")\n\t    def process_para(self, indir, type, TYPE = 'Changed') :\n\t        self.read_para(indir)\n\t        if type == 'pos' : self.para_pos_statistic(TYPE)\n\t        if type == 'parser' : self.para_parser_statistic(TYPE)\n\t        print(self.result)\n\t        print(self.result)\n", "        plt.figure(figsize=(10, 8), dpi=80)\n\t        plt.rcParams.update({'font.size': 20})\n\t        # plt.suptitle(self.result['dataset'], fontsize = 20)\n\t        for id, type in enumerate(self.result) :\n\t            if type == 'dataset' : continue\n\t            plt.subplot(3, 3, id)\n\t            self.draw_table(self.result[type], type)\n\t        plt.tight_layout(pad = 1, h_pad = -0.5, w_pad = -2)\n\t        plt.show()\n\tdef main() :\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--indir\",\n\t        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n\t        help = \"input directory\"\n\t    )\n\t    parser.add_argument(\n\t        \"--outdir\",\n", "        type = str,\n\t        nargs = \"?\",\n\t        default = None,\n\t        help = \"output directory\"\n\t    )\n\t    opt = parser.parse_args()\n\t    solver = Para_analyse()\n\t    solver.process_para(opt.indir, 'pos') # or 'parser'\n\tif __name__ == '__main__' :\n\t    index = 0\n", "    PARA = {}\n\t    main()"]}
{"filename": "dst_preprocess/esnli.py", "chunked_list": ["import pandas as pd\n\timport json, os\n\tfrom format import construct_sample_body, construct_sub_qa_body, construct_option, INCORRECT_OPTION_NAME\n\tfrom typing import List, Dict\n\tdef format_single_sample(question:str, answer:str, option_list:List[str], context:str=\"\") -> Dict:\n\t    sample = construct_sample_body(question, answer, context)\n\t    sub_qa = construct_sub_qa_body(question, answer)\n\t    option = construct_option(\"GroundTruth\", answer)\n\t    sub_qa[\"options\"].append(option)\n\t    for op in option_list:\n", "        if op != answer:\n\t            sub_qa[\"options\"].append(construct_option(INCORRECT_OPTION_NAME, op))\n\t    if len(sub_qa[\"options\"]) != 3:\n\t        print(\"error\")\n\t    sub_qa[\"option-number\"] = len(sub_qa[\"options\"])\n\t    sample[\"sub-qa\"].append(sub_qa)\n\t    return sample\n\tOPTIONS = ['neutral', 'entailment', 'contradiction']\n\tQUESTION = \"What is the logical relationship between premise and hypothesis?\"\n\tdef main(file_path:str, save_path:str) -> None:\n", "    data_df = pd.read_csv(file_path)\n\t    data_dict = {}\n\t    for i in range(len(data_df)):\n\t        sample = data_df.loc[i, ['Sentence1', 'Sentence2', 'gold_label']].to_dict()\n\t        sample = format_single_sample(QUESTION, sample['gold_label'], \n\t                                      option_list=OPTIONS, \n\t                                      context=\"Premise: {} \\n Hypothesis: {}\".format(sample['Sentence1'], sample['Sentence2']))\n\t        data_dict[str(i)] = sample\n\t    if save_path == None : return data_dict\n\t    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n", "    with open(save_path, 'w') as f:\n\t        json.dump(data_dict, f, indent=4)\n\tdef process(file_path, save_path) :\n\t    return main(file_path, save_path)"]}
{"filename": "dst_preprocess/format.py", "chunked_list": ["from copy import deepcopy\n\tSAMPLE_BODY_FORMAT = {\n\t        \"passage\": \"\",\n\t        # \"answer\": \"\",\n\t        \"sub-qa\":[\n\t        ]\n\t    }\n\tSUB_QA_BODY_FORMAT = {\n\t                    \"question\": \"\",\n\t                    \"answer\": \"\",\n", "                    \"answer_formula\":\"\",\n\t                    \"option-number\": 0,\n\t                    \"options\":[]\n\t                }\n\tOPTION_FORMAT = {\n\t                    \"option_type\": \"\",\n\t                    \"option_describe\": \"\"\n\t                }\n\tINCORRECT_OPTION_NAME = \"Answewr Error\"\n\tGENERAL_QUESTION = \"Choose one of several candidate options that you think is correct.\"\n", "def construct_sample_body(question:str=\"\", answer:str=\"\", context:str=\"\"):\n\t    sample = deepcopy(SAMPLE_BODY_FORMAT)\n\t    # sample[\"question\"] = question\n\t    # sample[\"answer\"] = answer\n\t    sample[\"passage\"] = context\n\t    return sample\n\tdef construct_sub_qa_body(question:str, answer:str):\n\t    sub_qa = deepcopy(SUB_QA_BODY_FORMAT)\n\t    sub_qa[\"question\"] = question\n\t    sub_qa[\"answer\"] = answer\n", "    return sub_qa\n\tdef construct_option(option_type:str, option_describe:str):\n\t    option = deepcopy(OPTION_FORMAT)\n\t    option[\"option_type\"] = option_type\n\t    option[\"option_describe\"] = option_describe\n\t    return option"]}
{"filename": "dst_preprocess/gsm8k_clean.py", "chunked_list": ["import json\n\timport re\n\timport os\n\timport copy\n\tdef replace_last_match(text, pattern, replacement):\n\t    # Find the last match\n\t    last_match = None\n\t    for match in re.finditer(pattern, text):\n\t        last_match = match\n\t    # Replace the last match\n", "    if last_match:\n\t        start, end = last_match.span()\n\t        text = text[:start] + replacement + text[end:]\n\t    return text\n\tdef map_formulat2answer(answer_text, answer_formula):\n\t    pattern1 = r\"\\d+\\.?\\d*\"\n\t    match = re.findall(pattern1, answer_formula)\n\t    answer_description = None\n\t    if len(match):\n\t        pattern2 = \"\"\n", "        for index in range(0, len(match)-1 if len(match)-1 < 2 else 2):\n\t            if match[index] == '0.01':\n\t                continue\n\t            else:\n\t                pattern2 = pattern2 + match[index] + r'[^1-9]*'\n\t        pattern2 = pattern2 + r'.*=[^0-9]*'+match[-1]\n\t        answer_description = re.sub(pattern2, \"<<>>\", answer_text)\n\t        if answer_description == answer_text:\n\t            if len(match) >= 3:\n\t                reverse_pattern = r''+match[1]+ r'[^1-9]*'+match[0]+ r'[^1-9]*'+ r'.*=[^0-9]*'+match[-1]\n", "            else:\n\t                reverse_pattern = r'x{100}'\n\t            answer_description = re.sub(reverse_pattern, \"<<>>\", answer_text)\n\t            if answer_description == answer_text:\n\t                if len(match) == 2 and match[0] == match[1]:\n\t                    answer_description = replace_last_match(answer_description, match[-1], \"<<>>\")\n\t                else:\n\t                    answer_description = None\n\t    if answer_description:\n\t        answer_description = re.sub(r\"(\\d/)?<<>>\", answer_formula.strip(\"<<>>\"), answer_description)\n", "    return answer_description\n\tdef add_zero(match):\n\t    if match:\n\t        return match.group(1) + '0' + match.group(2)\n\t    else:\n\t        return ''\n\tdef remove_non_digits(text):\n\t    pattern = r\"[^0-9=+\\-*/\\.]\"\n\t    return re.sub(pattern, \"\", text)\n\tdef unique_letter_count(text):\n", "    letters = set()\n\t    for char in text:\n\t        if char.isalpha():\n\t            letters.add(char.lower())\n\t    return len(letters)\n\tfail_count = 0\n\tdef extract_formula_from_answer_text(answer_text):\n\t    # pattern = r\"((\\d+((/?\\d+)|((\\.)?\\d+)|((\\%)?)))(/W+))+\" # =\\D*(\\-?\\d+(\\.\\d+)?)\n\t    pattern = r'(?P<first_operand>\\d+(\\.\\d+)?\\D*)(?P<operator>\\s*[\\+\\-\\*/]\\s*\\D?\\d+(\\.\\d+)?\\D*)+\\s*=\\s*\\D*(?P<result>\\d+(\\.\\d+)?)'\n\t    match = re.search(pattern, answer_text)\n", "    result = \"\"\n\t    if match:\n\t        init_formula = match.group()\n\t        unique_letter_counts = unique_letter_count(init_formula)\n\t        if unique_letter_counts == 0:\n\t            # formula = re.sub(r'\\s', '', init_formula)\n\t            # has_alpha = any(c.isalpha() for c in formula)\n\t            formula = remove_non_digits(init_formula)\n\t            result = \"<<\"+formula+\">>\"\n\t            answer_text = answer_text.replace(init_formula, formula)\n", "        print(init_formula)\n\t        print(result)\n\t        global fail_count\n\t        if result == \"\":\n\t            fail_count += 1\n\t    # else:\n\t    #     print(answer_text)\n\t    return answer_text, result\n\tdef reformulate_sub_qa(sub_qa_list):\n\t    new_qa_list = []\n", "    for i, qa in enumerate(sub_qa_list):\n\t        curr_qa = copy.deepcopy(qa)\n\t        if not len(curr_qa[\"answer_formula\"]) or not len(curr_qa[\"answer\"]) :\n\t            if curr_qa[\"answer\"] == \"\":\n\t                curr_qa[\"answer_formula\"] = \"\"\n\t            else:\n\t                curr_qa[\"answer\"], curr_qa[\"answer_formula\"] = extract_formula_from_answer_text(curr_qa[\"answer\"])\n\t        else:\n\t            # 将 .20 转为 0.20\n\t            text = re.sub(r'(\\D)(\\.\\d)', add_zero,  curr_qa[\"answer\"])\n", "            if text != curr_qa[\"answer\"]:\n\t                curr_qa[\"answer\"] = text\n\t            # 将 10,000 中的,去掉 或者 10 000 中的空格\n\t            text = re.sub(r'(?<=\\d{1})[,|\\s](?=\\d{2})', '', curr_qa[\"answer\"])\n\t            if text != curr_qa[\"answer\"]:\n\t                curr_qa[\"answer\"] = text\n\t            # 将 times 和 plus equals 转化为 符号\n\t            while True:\n\t                text = re.sub(r\"(\\d+)\\s+times\\s+(\\d+)\", r\"\\1*\\2\", curr_qa[\"answer\"])\n\t                text = re.sub(r\"(\\d+)\\s+plus\\s+(\\d+)\", r\"\\1+\\2\", text)\n", "                if text == curr_qa[\"answer\"]:\n\t                    break\n\t                else:\n\t                    curr_qa[\"answer\"] = text\n\t            text = re.sub(r\"(\\d+)\\s+equals\\s+(\\d+)\", r\"\\1=\\2\", curr_qa[\"answer\"])\n\t            if text != curr_qa[\"answer\"]:\n\t                curr_qa[\"answer\"] = text\n\t            # 对 answer_formula 处理，在 .02 这种浮点数前面添加0\n\t            text = re.sub(r'(\\D)(\\.\\d)', add_zero,  curr_qa[\"answer_formula\"])\n\t            if text != curr_qa[\"answer_formula\"]:\n", "                curr_qa[\"answer_formula\"] = text\n\t            answer = map_formulat2answer(curr_qa[\"answer\"], curr_qa[\"answer_formula\"])\n\t            if answer is None:\n\t                print()\n\t                print(\"answer is None\")\n\t                print(curr_qa[\"answer\"])\n\t                print(curr_qa[\"answer_formula\"])\n\t                print()\n\t            else:\n\t                curr_qa[\"answer\"] = answer\n", "        curr_qa[\"options\"].pop()\n\t        new_qa_list.append(curr_qa)\n\t    return new_qa_list\n\tdef clean_dataset(file_path, save_path = None):\n\t    with open(file_path, 'rb') as f:\n\t        sample_list = json.load(f)\n\t    results = {}\n\t    for i, sample in enumerate(sample_list):\n\t        sample = sample[str(i)]\n\t        results[str(i)] = sample\n", "        sub_qa_list = reformulate_sub_qa(sample[\"sub-qa\"])\n\t        results[str(i)][\"sub-qa\"] = sub_qa_list\n\t    if save_path == None : return results\n\t    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\t    with open(save_path, 'w') as f:\n\t        json.dump(results, f, indent=4)\n\t    return results\n\tdef process(file_path, save_path) :\n\t    return clean_dataset(file_path, save_path)"]}
{"filename": "dst_preprocess/qasc.py", "chunked_list": ["import json, os\n\tfrom format import construct_sample_body, construct_sub_qa_body, construct_option, INCORRECT_OPTION_NAME, GENERAL_QUESTION\n\tfrom typing import List, Dict\n\top_num_dict = {}\n\tIS_USING_GENERAL_QUESTION = False\n\tdef format_single_sample(context:str, answer:str, option_list:List[str], question:str) -> Dict:\n\t    sample = construct_sample_body(context=context)\n\t    sub_qa = construct_sub_qa_body(question, answer)\n\t    option = construct_option(\"GroundTruth\", answer)\n\t    sub_qa[\"options\"].append(option)\n", "    for op in option_list:\n\t        if op != answer:\n\t            sub_qa[\"options\"].append(construct_option(INCORRECT_OPTION_NAME, op))\n\t    sub_qa[\"option-number\"] = len(sub_qa[\"options\"])\n\t    sample[\"sub-qa\"].append(sub_qa)\n\t    op_num_dict[sub_qa[\"option-number\"]] = 1 if sub_qa[\"option-number\"] not in op_num_dict else op_num_dict[sub_qa[\"option-number\"]]+1\n\t    return sample\n\tdef main(file_path:str, save_path:str) -> None:\n\t    with open(file_path, 'rb') as f:\n\t        data_list = [json.loads(line) for line in f]\n", "    data_dict = {}\n\t    for i, item in enumerate(data_list):\n\t        sample = item\n\t        ans = ''\n\t        for c in item['question']['choices']:\n\t            if c['label'] == item['answerKey']:\n\t                ans = c['text']\n\t        if IS_USING_GENERAL_QUESTION:\n\t            sample = format_single_sample(context = item['question']['stem'], question=GENERAL_QUESTION,\n\t                                        answer = ans, \n", "                                        option_list = [c['text'] for c in item['question']['choices']])\n\t        else:\n\t            sample = format_single_sample(context = \"\", \n\t                                          question=item['question']['stem'],\n\t                                        answer = ans, \n\t                                        option_list = [c['text'] for c in item['question']['choices']])\n\t        data_dict[str(i)] = sample\n\t    if save_path == None : return data_dict\n\t    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\t    with open(save_path, 'w') as f:\n", "        json.dump(data_dict, f, indent=4)\n\tdef process(file_path, save_path) :\n\t    global IS_USING_GENERAL_QUESTION\n\t    IS_USING_GENERAL_QUESTION = False\n\t    return main(file_path, save_path)\n"]}
{"filename": "dst_preprocess/ecqa.py", "chunked_list": ["import pandas as pd\n\tfrom copy import deepcopy\n\timport json, os\n\tfrom format import construct_sample_body, construct_sub_qa_body, construct_option, INCORRECT_OPTION_NAME, GENERAL_QUESTION\n\tfrom typing import List, Dict\n\tIS_USING_GENERAL_QUESTION = False\n\tmanual_incorrect_option = {'The person was in the drawing room, but wanted to take his draft to be completed, where did he go?':'store',\n\t                           'Southern is the opposite of what?': 'north pole',\n\t                           'John needed a straight wire.  Unfortunately, this one had endured some abuse and had become what?': 'loose',\n\t                           'They would only allow one more in, he was unfortunately a what?': 'no way',\n", "                           \"Greg couldn't pay the cash advance fee, and thus was dinged for what?\": 'flee',\n\t                           'What grows from all mammal skin?': 'tooth',\n\t                           'Sam was planning to return, but his sister was still angry that he was doing what?': 'prank',\n\t                           'What covers the largest percentage of the pacific northwest?': 'water',\n\t                           \"Someone who doesn't believe in the divine could be called what?\": 'priest',\n\t                           'Where is one likely to find poker chips?': 'home'\n\t                           }\n\tdef format_single_sample(context:str, answer:str, option_list:List[str], question:str) -> Dict:\n\t    sample = construct_sample_body(context=context)\n\t    sub_qa = construct_sub_qa_body(question, answer)\n", "    option = construct_option(\"GroundTruth\", answer)\n\t    sub_qa[\"options\"].append(option)\n\t    for op in option_list:\n\t        if op != answer:\n\t            sub_qa[\"options\"].append(construct_option(INCORRECT_OPTION_NAME, op))\n\t    if len(sub_qa[\"options\"]) != 5:\n\t        if question == GENERAL_QUESTION:\n\t            sub_qa[\"options\"].append(construct_option(INCORRECT_OPTION_NAME, manual_incorrect_option[context]))\n\t        else:\n\t            sub_qa[\"options\"].append(construct_option(INCORRECT_OPTION_NAME, manual_incorrect_option[question]))\n", "    if len(sub_qa[\"options\"]) != 5:\n\t        print(\"error\")\n\t    sub_qa[\"option-number\"] = len(sub_qa[\"options\"])\n\t    sample[\"sub-qa\"].append(sub_qa)\n\t    return sample\n\tdef main(file_path:str, save_path:str) -> None:\n\t    data_df = pd.read_csv(file_path)\n\t    data_dict = {}\n\t    for i in range(len(data_df)):\n\t        sample = data_df.loc[i, ['q_text', 'q_ans', 'q_op1', 'q_op2', 'q_op3', 'q_op4', 'q_op5']].to_dict()\n", "        if IS_USING_GENERAL_QUESTION:\n\t            sample = format_single_sample(context = sample['q_text'], question=GENERAL_QUESTION, \n\t                                      answer = sample['q_ans'], option_list=[sample['q_op'+str(i)] for i in range(1, 6)])\n\t        else:\n\t            sample = format_single_sample(context = \"\", question = sample['q_text'], \n\t                                      answer = sample['q_ans'], option_list=[sample['q_op'+str(i)] for i in range(1, 6)])\n\t        data_dict[str(i+1)] = sample\n\t    if save_path == None : return data_dict\n\t    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\t    with open(save_path, 'w') as f:\n", "        json.dump(data_dict, f, indent=4)\n\tdef process(file_path, save_path) :\n\t    global IS_USING_GENERAL_QUESTION\n\t    IS_USING_GENERAL_QUESTION = False\n\t    return main(file_path, save_path)\n"]}
{"filename": "dst_preprocess/noah_clean.py", "chunked_list": ["import json\n\timport re\n\timport os\n\timport copy\n\tdef extract_answer_num(answer_text):\n\t    pattern = r\"-?\\d+((/?\\d+)|((\\.)?\\d+)|(:?\\d+)|((\\%)?))\"\n\t    match_iter = re.finditer(pattern, answer_text)\n\t    num_list = []\n\t    for x in match_iter:\n\t        num_list.append(x.group())\n", "    if len(num_list) == 0:\n\t        return None\n\t    elif len(num_list) == 1:\n\t        return num_list[0]\n\t    else:\n\t        print(answer_text)\n\tdef add_zero(match):\n\t    if match:\n\t        return match.group(1) + '0' + match.group(2)\n\t    else:\n", "        return ''\n\tdef reformulate_sub_qa(sub_qa_list):\n\t    new_qa_list = []\n\t    for i, qa in enumerate(sub_qa_list):\n\t        curr_qa = copy.deepcopy(qa)\n\t        extract_answer_num(curr_qa[\"answer\"])\n\t        curr_qa[\"options\"].pop()\n\t        new_qa_list.append(curr_qa)\n\t    return new_qa_list\n\tdef clean_dataset(file_path, save_path):\n", "    with open(file_path, 'rb') as f:\n\t        sample_list = json.load(f)\n\t    results = {}\n\t    for i, sample in enumerate(sample_list):\n\t        sample = sample[str(i)]\n\t        results[str(i)] = sample\n\t        sub_qa_list = reformulate_sub_qa(sample[\"sub-qa\"])\n\t        results[str(i)][\"sub-qa\"] = sub_qa_list\n\t    if save_path == None : return results\n\t    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n", "    with open(save_path, 'w') as f:\n\t        json.dump(results, f, indent=4)\n\t    return results\n\tdef process(file_path, save_path) :\n\t    return clean_dataset(file_path, save_path)"]}
