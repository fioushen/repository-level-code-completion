{"filename": "main.py", "chunked_list": ["import logging\n\timport os\n\timport time\n\timport hydra\n\tfrom runner import SelfSLRunner, SupervisedRunner\n\tfrom trainer.utils import fix_seed\n\tlogger = logging.getLogger(__name__)\n\tdef check_existence_result() -> bool:\n\t    if os.path.exists(\"results.json\"):\n\t        print(\"Skip this trial, because it has already been run.\")\n", "        return True\n\t    else:\n\t        return False\n\t@hydra.main(config_path=\"conf\", config_name=\"config.yaml\", version_base=None)\n\tdef main(config):\n\t    if check_existence_result():\n\t        return\n\t    fix_seed(config.seed)\n\t    if config.train_mode == \"supervised\":\n\t        runner = SupervisedRunner(config)\n", "    elif config.train_mode == \"self_sl\":\n\t        runner = SelfSLRunner(config)\n\t    else:\n\t        raise ValueError(f\"train mode {config.train_mode} is not defined.\")\n\t    runner.run()\n\t    time.sleep(1)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "data/tabular_datamodule.py", "chunked_list": ["import logging\n\tfrom typing import Callable, Dict, Optional, Sequence, Union\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom sklearn.model_selection import train_test_split\n\tfrom torch import LongTensor, Tensor\n\tfrom torch.utils.data import DataLoader, Dataset\n\timport data.datasets as datasets\n\tfrom data.datasets.tabular_dataframe import TabularDataFrame\n", "logger = logging.getLogger(__name__)\n\t# Copied from https://github.com/pfnet-research/deep-table.\n\t# Modified by somaonishi and shoyameguro.\n\tclass TabularDataset(Dataset):\n\t    def __init__(\n\t        self,\n\t        data: pd.DataFrame,\n\t        task: str = \"binary\",\n\t        continuous_columns: Optional[Sequence[str]] = None,\n\t        categorical_columns: Optional[Sequence[str]] = None,\n", "        target: Optional[Union[str, Sequence[str]]] = None,\n\t        transform: Optional[Callable] = None,\n\t        unlabel_data_rate: Optional[float] = None,\n\t        seed=42,\n\t    ) -> None:\n\t        \"\"\"\n\t        Args:\n\t            data (pandas.DataFrame): DataFrame.\n\t            task (str): One of \"binary\", \"multiclass\", \"regression\".\n\t                Defaults to \"binary\".\n", "            continuous_cols (sequence of str, optional): Sequence of names of\n\t                continuous features (columns). Defaults to None.\n\t            categorical_cols (sequence of str, optional): Sequence of names of\n\t                categorical features (columns). Defaults to None.\n\t            target (str, optional): If None, `np.zeros` is set as target.\n\t                Defaults to None.\n\t            transform (callable): Method of converting Tensor data.\n\t                Defaults to None.\n\t        \"\"\"\n\t        super().__init__()\n", "        self.task = task\n\t        self.num = data.shape[0]\n\t        self.categorical_columns = categorical_columns if categorical_columns else []\n\t        self.continuous_columns = continuous_columns if continuous_columns else []\n\t        if unlabel_data_rate is not None:\n\t            if task == \"regression\":\n\t                stratify = None\n\t            else:\n\t                stratify = data[target]\n\t            data, unlabeled_data = train_test_split(\n", "                data, test_size=unlabel_data_rate, stratify=stratify, random_state=seed\n\t            )\n\t            self.num = data.shape[0]\n\t            self.unlabeled_num = unlabeled_data.shape[0]\n\t            logger.info(f\"labeled data size: {self.num}\")\n\t            logger.info(f\"unlabeled data size: {self.unlabeled_num}\")\n\t        if self.continuous_columns:\n\t            self.continuous = data[self.continuous_columns].values\n\t            if unlabel_data_rate is not None:\n\t                self.unlabeled_continuous = unlabeled_data[self.continuous_columns].values\n", "        if self.categorical_columns:\n\t            self.categorical = data[categorical_columns].values\n\t            if unlabel_data_rate is not None:\n\t                self.unlabeled_categorical = unlabeled_data[categorical_columns].values\n\t        if target:\n\t            self.target = data[target].values\n\t            if isinstance(target, str):\n\t                self.target = self.target.reshape(-1, 1)\n\t        else:\n\t            self.target = np.zeros((self.num, 1))\n", "        self.transform = transform\n\t        self.unlabel_data_rate = unlabel_data_rate\n\t    def __len__(self) -> int:\n\t        return self.num\n\t    def __getitem__(self, idx: int) -> Dict[str, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            idx (int): The index of the sample in the dataset.\n\t        Returns:\n\t            dict[str, Tensor]:\n", "                The returned dict has the keys {\"target\", \"continuous\", \"categorical\"}\n\t                and its values. If no continuous/categorical features, the returned value is `[]`.\n\t        \"\"\"\n\t        if self.task == \"multiclass\":\n\t            x = {\n\t                \"target\": torch.LongTensor(self.target[idx]),\n\t                \"continuous\": Tensor(self.continuous[idx]) if self.continuous_columns else [],\n\t                \"categorical\": LongTensor(self.categorical[idx]) if self.categorical_columns else [],\n\t            }\n\t        elif self.task in {\"binary\", \"regression\"}:\n", "            x = {\n\t                \"target\": torch.Tensor(self.target[idx]),\n\t                \"continuous\": Tensor(self.continuous[idx]) if self.continuous_columns else [],\n\t                \"categorical\": LongTensor(self.categorical[idx]) if self.categorical_columns else [],\n\t            }\n\t        else:\n\t            raise ValueError(f\"task: {self.task} must be 'multiclass' or 'binary' or 'regression'\")\n\t        if self.transform is not None:\n\t            x = self.transform(x)\n\t        if hasattr(self, \"unlabeled_num\"):\n", "            unlabel_idx = np.random.randint(0, self.unlabeled_num)\n\t            unlabel = {\n\t                \"continuous\": Tensor(self.unlabeled_continuous[unlabel_idx]) if self.continuous_columns else [],\n\t                \"categorical\": LongTensor(self.unlabeled_categorical[unlabel_idx]) if self.categorical_columns else [],\n\t            }\n\t            return x, unlabel\n\t        else:\n\t            return x\n\tclass TabularDatamodule:\n\t    def __init__(\n", "        self,\n\t        dataset: TabularDataFrame,\n\t        transform: Optional[Callable] = None,\n\t        train_sampler: Optional[torch.utils.data.Sampler] = None,\n\t        batch_size: int = 128,\n\t        num_workers: int = 3,\n\t        seed: int = 42,\n\t        val_size: float = 0.1,\n\t    ) -> None:\n\t        # self.dataset = dataset\n", "        self.__num_categories = dataset.num_categories()\n\t        self.categorical_columns = dataset.categorical_columns\n\t        self.continuous_columns = dataset.continuous_columns\n\t        self.cat_cardinalities = dataset.cat_cardinalities(True)\n\t        self.target = dataset.target_columns\n\t        self.d_out = dataset.dim_out\n\t        dataframes = dataset.processed_dataframes(val_size=val_size, seed=seed)\n\t        self.train = dataframes[\"train\"]\n\t        self.val = dataframes[\"val\"]\n\t        self.test = dataframes[\"test\"]\n", "        for k, v in dataframes.items():\n\t            logger.info(f\"{k} dataset shape: {v.shape}\")\n\t        self.task = dataset.task\n\t        self.transform = transform\n\t        self.train_sampler = train_sampler\n\t        self.batch_size = batch_size\n\t        self.num_workers = num_workers\n\t        if self.task == \"regression\":\n\t            self.y_std = dataset.y_std\n\t        self.seed = seed\n", "    @property\n\t    def num_categories(self) -> int:\n\t        return self.__num_categories\n\t    @property\n\t    def num_continuous_features(self) -> int:\n\t        return len(self.continuous_columns)\n\t    @property\n\t    def num_categorical_features(self) -> int:\n\t        return len(self.categorical_columns)\n\t    def dataloader(self, mode: str, batch_size: Optional[int] = None, transform=None, unlabel_data_rate=None):\n", "        assert mode in {\"train\", \"val\", \"test\"}\n\t        if not hasattr(self, mode):\n\t            return None\n\t        data = getattr(self, mode)\n\t        if mode == \"test\":\n\t            transform = None\n\t        if transform is None:\n\t            transform = self.transform\n\t        dataset = TabularDataset(\n\t            data=data,\n", "            task=self.task,\n\t            categorical_columns=self.categorical_columns,\n\t            continuous_columns=self.continuous_columns,\n\t            target=self.target,\n\t            transform=transform,\n\t            unlabel_data_rate=unlabel_data_rate,\n\t            seed=self.seed,\n\t        )\n\t        return DataLoader(\n\t            dataset,\n", "            batch_size if batch_size is not None else self.batch_size,\n\t            shuffle=True if mode == \"train\" else False,\n\t            num_workers=self.num_workers,\n\t            sampler=self.train_sampler if mode == \"train\" else None,\n\t        )\n\tdef get_datamodule(config) -> TabularDatamodule:\n\t    if type(config.data) == int:\n\t        dataset = datasets.OpenmlDataset(data_id=config.data, config=config)\n\t    else:\n\t        dataset = getattr(datasets, config.data)(config=config)\n", "    return TabularDatamodule(dataset, batch_size=config.batch_size, seed=config.seed, val_size=config.val_size)\n"]}
{"filename": "data/__init__.py", "chunked_list": ["from data.tabular_datamodule import TabularDatamodule, get_datamodule\n"]}
{"filename": "data/datasets/openml.py", "chunked_list": ["import openml\n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.preprocessing import LabelEncoder\n\tfrom .tabular_dataframe import TabularDataFrame\n\texceptions_binary = [45062]\n\texceptions_multiclass = [44135]\n\tdef get_task_and_dim_out(data_id, df, columns, cate_indicator, target_col):\n\t    target_idx = columns.index(target_col)\n\t    if data_id in exceptions_binary:\n\t        task = \"binary\"\n", "        dim_out = 1\n\t    elif data_id in exceptions_multiclass:\n\t        task = \"multiclass\"\n\t        dim_out = int(df[target_col].nunique())\n\t    elif cont_checker(df, target_col, cate_indicator[target_idx]):\n\t        task = \"regression\"\n\t        dim_out = 1\n\t    elif int(df[target_col].nunique()) == 2:\n\t        task = \"binary\"\n\t        dim_out = 1\n", "    else:\n\t        task = \"multiclass\"\n\t        dim_out = int(df[target_col].nunique())\n\t    return task, dim_out\n\tdef cont_checker(df, col, is_cate):\n\t    return not is_cate and df[col].dtype != bool and df[col].dtype != object\n\tdef cate_checker(df, col, is_cate):\n\t    return is_cate or df[col].dtype == bool or df[col].dtype == object\n\tdef get_columns_list(df, columns, cate_indicator, target_col, checker):\n\t    return [col for col, is_cate in zip(columns, cate_indicator) if col != target_col and checker(df, col, is_cate)]\n", "def print_dataset_details(dataset: openml.datasets.OpenMLDataset, data_id):\n\t    df, _, cate_indicator, columns = dataset.get_data(dataset_format=\"dataframe\")\n\t    print(dataset.name)\n\t    print(dataset.openml_url)\n\t    print(df)\n\t    target_col = dataset.default_target_attribute\n\t    print(\"Nan count\", df.isna().sum().sum())\n\t    print(\"cont\", get_columns_list(df, columns, cate_indicator, target_col, cont_checker))\n\t    print(\"cate\", get_columns_list(df, columns, cate_indicator, target_col, cate_checker))\n\t    print(\"target\", target_col)\n", "    task, dim_out = get_task_and_dim_out(data_id, df, columns, cate_indicator, target_col)\n\t    print(f\"task: {task}\")\n\t    print(f\"dim_out: {dim_out}\")\n\t    exit()\n\tclass OpenmlDataset(TabularDataFrame):\n\t    def __init__(self, data_id, config, download: bool = False) -> None:\n\t        super().__init__(config=config, download=download)\n\t        dataset = openml.datasets.get_dataset(data_id)\n\t        if config.show_data_detail:\n\t            print_dataset_details(dataset, data_id)\n", "        df, _, cate_indicator, columns = dataset.get_data(dataset_format=\"dataframe\")\n\t        self.all_columns = columns\n\t        target_col = dataset.default_target_attribute\n\t        self.continuous_columns = get_columns_list(df, columns, cate_indicator, target_col, cont_checker)\n\t        self.categorical_columns = get_columns_list(df, columns, cate_indicator, target_col, cate_checker)\n\t        self.task, self.dim_out = get_task_and_dim_out(data_id, df, columns, cate_indicator, target_col)\n\t        self.target_columns = [target_col]\n\t        if self.task != \"regression\":\n\t            df[target_col] = LabelEncoder().fit_transform(df[target_col])\n\t            self.train, self.test = train_test_split(\n", "                df, test_size=0.2, stratify=df[target_col], random_state=self.config.seed\n\t            )\n\t        else:\n\t            self.train, self.test = train_test_split(df, test_size=0.2, random_state=self.config.seed)\n"]}
{"filename": "data/datasets/adult.py", "chunked_list": ["import os\n\timport pandas as pd\n\tfrom sklearn.model_selection import train_test_split\n\tfrom .tabular_dataframe import TabularDataFrame\n\tclass Adult(TabularDataFrame):\n\t    dim_out = 1\n\t    all_columns = [\n\t        \"age\",\n\t        \"workclass\",\n\t        \"fnlwgt\",\n", "        \"education\",\n\t        \"education-num\",\n\t        \"marital-status\",\n\t        \"occupation\",\n\t        \"relationship\",\n\t        \"race\",\n\t        \"sex\",\n\t        \"capital-gain\",\n\t        \"capital-loss\",\n\t        \"hours-per-week\",\n", "        \"native-country\",\n\t        \"income\",\n\t    ]\n\t    continuous_columns = [\n\t        \"age\",\n\t        \"fnlwgt\",\n\t        \"education-num\",\n\t        \"capital-gain\",\n\t        \"capital-loss\",\n\t        \"hours-per-week\",\n", "    ]\n\t    categorical_columns = [\n\t        \"workclass\",\n\t        \"education\",\n\t        \"marital-status\",\n\t        \"occupation\",\n\t        \"relationship\",\n\t        \"race\",\n\t        \"sex\",\n\t        \"native-country\",\n", "    ]\n\t    target_columns = [\"income\"]\n\t    task = \"binary\"\n\t    def __init__(self, config, download: bool = False) -> None:\n\t        super().__init__(config=config, download=download)\n\t        df = pd.read_csv(os.path.join(self.raw_folder, \"adult.csv\"))\n\t        df.columns = self.all_columns\n\t        df[\"income\"] = df[\"income\"].replace({\"<=50K\": 1, \">50K\": 0})\n\t        self.train, self.test = train_test_split(\n\t            df, test_size=0.2, stratify=df[\"income\"], random_state=self.config.seed\n", "        )\n"]}
{"filename": "data/datasets/__init__.py", "chunked_list": ["from .adult import Adult\n\tfrom .ca_housing import CAHousing\n\tfrom .openml import OpenmlDataset\n"]}
{"filename": "data/datasets/utils.py", "chunked_list": ["import numpy as np\n\tfrom sklearn.preprocessing import QuantileTransformer\n\tdef quantiletransform(df_cont, noise=1e-3, seed=42):\n\t    df_cont = df_cont.copy()\n\t    sc = QuantileTransformer(\n\t        output_distribution=\"normal\",\n\t        n_quantiles=max(min(df_cont.shape[0] // 30, 1000), 10),\n\t        subsample=int(1e9),\n\t        random_state=seed,\n\t    )\n", "    stds = np.std(df_cont.values, axis=0, keepdims=True)\n\t    noise_std = noise / np.maximum(stds, noise)\n\t    normal = np.random.default_rng(seed).standard_normal(df_cont.shape)\n\t    df_cont += noise_std * normal\n\t    sc.fit(df_cont)\n\t    return sc\n"]}
{"filename": "data/datasets/tabular_dataframe.py", "chunked_list": ["import logging\n\timport os\n\tfrom typing import Dict, List, Optional, Sequence, Tuple, Union\n\timport pandas as pd\n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.preprocessing import OrdinalEncoder\n\tfrom torchvision.datasets.utils import check_integrity\n\tfrom .utils import quantiletransform\n\tlogger = logging.getLogger(__name__)\n\t# Copied from https://github.com/pfnet-research/deep-table.\n", "# Modified by somaonishi and shoyameguro.\n\tclass TabularDataFrame(object):\n\t    \"\"\"Base class for datasets\"\"\"\n\t    def __init__(self, config, download: bool = False) -> None:\n\t        \"\"\"\n\t        Args:\n\t            root (str): Path to the root of datasets for saving/loading.\n\t            download (bool): If True, you must implement `self.download` method\n\t                in the child class. Defaults to False.\n\t        \"\"\"\n", "        self.config = config\n\t        self.root = config.data_dir\n\t        if download:\n\t            self.download()\n\t    @property\n\t    def mirrors(self) -> None:\n\t        pass\n\t    @property\n\t    def resources(self) -> None:\n\t        pass\n", "    @property\n\t    def raw_folder(self) -> str:\n\t        \"\"\"The folder where raw data will be stored\"\"\"\n\t        return os.path.join(self.root, self.__class__.__name__, \"raw\")\n\t    def _check_exists(self, fpath: Sequence[Tuple[str, Union[str, None]]]) -> bool:\n\t        \"\"\"\n\t        Args:\n\t            fpath (sequence of tuple[str, (str or None)]): Each value has the format\n\t                [file_path, (md5sum or None)]. Checking if files are correctly\n\t                stored in `self.raw_folder`. If `md5sum` is provided, checking\n", "                the file itself is valid.\n\t        \"\"\"\n\t        return all(check_integrity(os.path.join(self.raw_folder, path[0]), md5=path[1]) for path in fpath)\n\t    def download(self) -> None:\n\t        \"\"\"\n\t        Implement this function if the dataset is downloadable.\n\t        See :func:`~deep_table.data.datasets.adult.Adult` for an example implementation.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    def cat_cardinalities(self, use_unk: bool = True) -> Optional[List[int]]:\n", "        \"\"\"List of the numbers of the categories of each column.\n\t        Args:\n\t            use_unk (bool): If True, each feature (column) has \"unknown\" categories.\n\t        Returns:\n\t            List[int], optional: List of cardinalities. i-th value denotes\n\t                the number of categories which i-th column has.\n\t        \"\"\"\n\t        cardinalities = []\n\t        df_train = self.get_dataframe(train=True)\n\t        df_train_cat = df_train[self.categorical_columns]\n", "        cardinalities = df_train_cat.nunique().values.astype(int)\n\t        if use_unk:\n\t            cardinalities += 1\n\t        cardinalities_list = cardinalities.tolist()\n\t        return cardinalities_list\n\t    def get_dataframe(self, train: bool = True) -> pd.DataFrame:\n\t        \"\"\"\n\t        Args:\n\t            train (bool): If True, the returned value is `pd.DataFrame` for train.\n\t                If False, the returned value is `pd.DataFrame` for test.\n", "        Returns:\n\t            `pd.DataFrame`\n\t        \"\"\"\n\t        if train:\n\t            return self.train\n\t        else:\n\t            return self.test\n\t    def get_classify_dataframe(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n\t        df_train = self.get_dataframe(train=True)\n\t        df_test = self.get_dataframe(train=False)\n", "        df_train, df_val = train_test_split(\n\t            df_train,\n\t            test_size=val_size,\n\t            stratify=df_train[self.target_columns],\n\t            random_state=seed,\n\t        )\n\t        if self.config.train_size < 1:\n\t            logger.info(f\"Change train set size {len(df_train)} -> {int(len(df_train) * self.config.train_size)}.\")\n\t            df_train, _ = train_test_split(\n\t                df_train,\n", "                train_size=self.config.train_size,\n\t                stratify=df_train[self.target_columns],\n\t                random_state=seed,\n\t            )\n\t        classify_dfs = {\n\t            \"train\": df_train,\n\t            \"val\": df_val,\n\t            \"test\": df_test,\n\t        }\n\t        if len(classify_dfs[\"val\"]) > 20000:\n", "            logger.info(\"validation size reduction: {} -> 20000\".format(len(classify_dfs[\"val\"])))\n\t            classify_dfs[\"val\"], _ = train_test_split(\n\t                classify_dfs[\"val\"],\n\t                train_size=20000,\n\t                stratify=classify_dfs[\"val\"][self.target_columns],\n\t                random_state=seed,\n\t            )\n\t        return classify_dfs\n\t    def get_regression_dataframe(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n\t        df_train = self.get_dataframe(train=True)\n", "        df_test = self.get_dataframe(train=False)\n\t        df_train, df_val = train_test_split(df_train, test_size=val_size, random_state=seed)\n\t        if self.config.train_size < 1:\n\t            logger.info(f\"Change train set size {len(df_train)} -> {int(len(df_train) * self.config.train_size)}.\")\n\t            df_train, _ = train_test_split(\n\t                df_train,\n\t                train_size=self.config.train_size,\n\t                random_state=seed,\n\t            )\n\t        regression_dfs = {\n", "            \"train\": df_train,\n\t            \"val\": df_val,\n\t            \"test\": df_test,\n\t        }\n\t        self.y_mean = regression_dfs[\"train\"][self.target_columns].to_numpy().mean()\n\t        self.y_std = regression_dfs[\"train\"][self.target_columns].to_numpy().std()\n\t        for key in regression_dfs.keys():\n\t            regression_dfs[key] = self._regression_encoder(regression_dfs[key])\n\t        if len(regression_dfs[\"val\"]) > 20000:\n\t            logger.info(\"validation size reduction: {} -> 20000\".format(len(regression_dfs[\"val\"])))\n", "            regression_dfs[\"val\"], _ = train_test_split(regression_dfs[\"val\"], train_size=20000, random_state=seed)\n\t        return regression_dfs\n\t    def processed_dataframes(self, val_size, seed) -> Dict[str, pd.DataFrame]:\n\t        \"\"\"\n\t        Returns:\n\t            dict[str, DataFrame]: The value has the keys \"train\", \"val\" and \"test\".\n\t        \"\"\"\n\t        if self.task == \"regression\":\n\t            dfs = self.get_regression_dataframe(val_size=val_size, seed=seed)\n\t        else:\n", "            dfs = self.get_classify_dataframe(val_size=val_size, seed=seed)\n\t        # preprocessing\n\t        if self.categorical_columns != []:\n\t            categorical_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1).fit(\n\t                dfs[\"train\"][self.categorical_columns]\n\t            )\n\t        # only apply DL model.\n\t        if self.continuous_columns != []:\n\t            continuous_encoder = quantiletransform(dfs[\"train\"][self.continuous_columns], seed=seed)\n\t        for key in dfs.keys():\n", "            if self.categorical_columns != []:\n\t                dfs[key][self.categorical_columns] = (\n\t                    categorical_encoder.transform(dfs[key][self.categorical_columns]) + 1\n\t                )\n\t            if self.continuous_columns != []:\n\t                dfs[key][self.continuous_columns] = continuous_encoder.transform(dfs[key][self.continuous_columns])\n\t        return dfs\n\t    def _regression_encoder(self, df):\n\t        df[self.target_columns] = (df[self.target_columns] - self.y_mean) / self.y_std\n\t        return df\n", "    def num_categories(self, use_unk: bool = True) -> int:\n\t        \"\"\"Total numbers of categories\n\t        Args:\n\t            use_unk (bool): If True, the returned value is calculated\n\t                as there are unknown categories.\n\t        \"\"\"\n\t        return sum(self.cat_cardinalities(use_unk=use_unk))\n"]}
{"filename": "data/datasets/ca_housing.py", "chunked_list": ["from sklearn.datasets import fetch_california_housing\n\tfrom sklearn.model_selection import train_test_split\n\tfrom .tabular_dataframe import TabularDataFrame\n\tclass CAHousing(TabularDataFrame):\n\t    dim_out = 1\n\t    all_columns = [\n\t        \"MedInc\",\n\t        \"HouseAge\",\n\t        \"AveRooms\",\n\t        \"AveBedrms\",\n", "        \"Population\",\n\t        \"AveOccup\",\n\t        \"Latitude\",\n\t        \"Longitude\",\n\t        \"MedHouseVal\",\n\t    ]\n\t    continuous_columns = [\n\t        \"MedInc\",\n\t        \"HouseAge\",\n\t        \"AveRooms\",\n", "        \"AveBedrms\",\n\t        \"Population\",\n\t        \"AveOccup\",\n\t        \"Latitude\",\n\t        \"Longitude\",\n\t    ]\n\t    categorical_columns = []\n\t    target_columns = [\"MedHouseVal\"]\n\t    task = \"regression\"\n\t    def __init__(self, config, download: bool = False) -> None:\n", "        super().__init__(config=config, download=download)\n\t        df = fetch_california_housing(as_frame=True).frame\n\t        self.train, self.test = train_test_split(df, test_size=0.2, random_state=self.config.seed)\n\t    def download(self) -> None:\n\t        pass\n\t    # def raw_dataframe(self, train: bool = True) -> pd.DataFrame:\n\t    #     if train:\n\t    #         return self.train\n\t    #     else:\n\t    #         return self.test\n", "    # def processed_dataframes(self, *args, **kwargs) -> Dict[str, pd.DataFrame]:\n\t    #     df_train = self.raw_dataframe(train=True)\n\t    #     df_test = self.raw_dataframe(train=False)\n\t    #     df_train, df_val = train_test_split(df_train, **kwargs)\n\t    #     dfs = {\n\t    #         \"train\": df_train,\n\t    #         \"val\": df_val,\n\t    #         \"test\": df_test,\n\t    #     }\n\t    #     # preprocessing\n", "    #     # only apply DL model.\n\t    #     sc = quantiletransform(df_train[self.continuous_columns], seed=self.config.seed)\n\t    #     self.y_mean = df_train[self.target_columns].to_numpy().mean()\n\t    #     self.y_std = df_train[self.target_columns].to_numpy().std()\n\t    #     for key in dfs.keys():\n\t    #         dfs[key][self.continuous_columns] = sc.transform(dfs[key][self.continuous_columns])\n\t    #         dfs[key] = self._label_encoder(dfs[key])\n\t    #     return dfs\n\t    # def _label_encoder(self, df):\n\t    #     df[self.target_columns] = (df[self.target_columns] - self.y_mean) / self.y_std\n", "    #     return df\n"]}
{"filename": "runner/self_sl.py", "chunked_list": ["import logging\n\timport torch.nn as nn\n\timport model\n\timport trainer\n\tfrom model import FTTransformer\n\tfrom trainer import FTTransTraniner\n\tfrom trainer.self_supervised.base import BaseSSLTrainer\n\tfrom .base import BaseRunner\n\tlogger = logging.getLogger(__name__)\n\tclass SelfSLRunner(BaseRunner):\n", "    def __init__(self, config) -> None:\n\t        super().__init__(config)\n\t    def self_sl_init(self):\n\t        Model = getattr(model, self.config.model.name)\n\t        logger.info(f\"Model is {self.config.model.name}.\")\n\t        num_features = self.datamodule.num_continuous_features + self.datamodule.num_categorical_features\n\t        self.model: FTTransformer = Model.make_default(\n\t            num_features=num_features,\n\t            n_num_features=self.datamodule.num_continuous_features,\n\t            cat_cardinalities=self.datamodule.cat_cardinalities,\n", "            last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n\t            d_out=self.datamodule.d_out,\n\t        )\n\t        self.model.to(self.device)\n\t        optimizer = self.model.make_default_optimizer()\n\t        criterion = None\n\t        Trainer = getattr(trainer, self.config.model.trainer)\n\t        logger.info(f\"Trainer is {self.config.model.trainer}.\")\n\t        assert issubclass(Trainer, BaseSSLTrainer), \"Trainer must be a subclass of BaseSLTrainer.\"\n\t        del self.config.model.params.p\n", "        self.self_sl_trainer: BaseSSLTrainer = Trainer(\n\t            datamodule=self.datamodule,\n\t            batch_size=self.config.batch_size,\n\t            eval_batch_size=self.config.eval_batch_size,\n\t            model=self.model,\n\t            optimizer=optimizer,\n\t            criterion=criterion,\n\t            device=self.device,\n\t            epochs=200,\n\t            patience=10,\n", "            eval_metric=\"val/self-sl-loss\",\n\t            eval_less_is_better=True,\n\t            mixed_fp16=self.config.mixed_fp16,\n\t            **self.config.model.params,\n\t        )\n\t    def sl_init(self):\n\t        state_dict = self.model.state_dict()\n\t        num_features = self.datamodule.num_continuous_features + self.datamodule.num_categorical_features\n\t        self.model = FTTransformer.make_default(\n\t            num_features=num_features,\n", "            n_num_features=self.datamodule.num_continuous_features,\n\t            cat_cardinalities=self.datamodule.cat_cardinalities,\n\t            last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n\t            d_out=self.datamodule.d_out,\n\t        )\n\t        missing, unexpected = self.model.load_state_dict(state_dict, strict=False)\n\t        # for p in self.model.parameters():\n\t        #     p.requires_grad = False\n\t        # for p in self.model.head.parameters():\n\t        #     p.requires_grad = True\n", "        self.model.to(self.device)\n\t        optimizer = self.model.make_default_optimizer()\n\t        criterion = (\n\t            nn.BCEWithLogitsLoss()\n\t            if self.datamodule.task == \"binary\"\n\t            else nn.CrossEntropyLoss()\n\t            if self.datamodule.task == \"multiclass\"\n\t            else nn.MSELoss()\n\t        )\n\t        self.sl_trainer = FTTransTraniner(\n", "            datamodule=self.datamodule,\n\t            batch_size=self.config.batch_size,\n\t            eval_batch_size=self.config.eval_batch_size,\n\t            model=self.model,\n\t            optimizer=optimizer,\n\t            criterion=criterion,\n\t            device=self.device,\n\t            epochs=self.config.epochs,\n\t            patience=self.config.patience,\n\t            eval_metric=self.config.eval.metric,\n", "            eval_less_is_better=self.config.eval.less_is_better,\n\t            mixed_fp16=self.config.mixed_fp16,\n\t            save_model=self.config.save_model,\n\t            tensorbord_dir=\"./self_supervised\",\n\t        )\n\t    def run(self):\n\t        self.self_sl_init()\n\t        self.self_sl_trainer.train()\n\t        self.sl_init()\n\t        self.sl_trainer.train()\n", "        self.sl_trainer.print_evaluate()\n"]}
{"filename": "runner/base.py", "chunked_list": ["import logging\n\timport torch\n\tfrom hydra.utils import to_absolute_path\n\tfrom data import get_datamodule\n\tlogger = logging.getLogger(__name__)\n\tclass BaseRunner:\n\t    def __init__(self, config) -> None:\n\t        self.config = config\n\t        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\t        logger.info(f\"Select {self.device}\")\n", "        config.data_dir = to_absolute_path(config.data_dir)\n\t        self.datamodule = get_datamodule(config)\n"]}
{"filename": "runner/__init__.py", "chunked_list": ["from .self_sl import SelfSLRunner\n\tfrom .supervised import SupervisedRunner\n"]}
{"filename": "runner/supervised.py", "chunked_list": ["import logging\n\timport torch.nn as nn\n\timport model\n\timport trainer\n\tfrom model import FTTransformer\n\tfrom trainer.supervised.base import BaseSupervisedTrainer\n\tfrom .base import BaseRunner\n\tlogger = logging.getLogger(__name__)\n\tclass SupervisedRunner(BaseRunner):\n\t    def __init__(self, config) -> None:\n", "        super().__init__(config)\n\t    def supervised_init(self):\n\t        Model = getattr(model, self.config.model.name)\n\t        logger.info(f\"Model is {self.config.model.name}.\")\n\t        num_features = self.datamodule.num_continuous_features + self.datamodule.num_categorical_features\n\t        self.model: FTTransformer = Model.make_default(\n\t            num_features=num_features,\n\t            n_num_features=self.datamodule.num_continuous_features,\n\t            cat_cardinalities=self.datamodule.cat_cardinalities,\n\t            last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n", "            d_out=self.datamodule.d_out,\n\t        )\n\t        self.model.to(self.device)\n\t        optimizer = self.model.make_default_optimizer()\n\t        criterion = (\n\t            nn.BCEWithLogitsLoss()\n\t            if self.datamodule.task == \"binary\"\n\t            else nn.CrossEntropyLoss()\n\t            if self.datamodule.task == \"multiclass\"\n\t            else nn.MSELoss()\n", "        )\n\t        Trainer = getattr(trainer, self.config.model.trainer)\n\t        logger.info(f\"Trainer is {self.config.model.trainer}.\")\n\t        self.trainer: BaseSupervisedTrainer = Trainer(\n\t            datamodule=self.datamodule,\n\t            batch_size=self.config.batch_size,\n\t            eval_batch_size=self.config.eval_batch_size,\n\t            model=self.model,\n\t            optimizer=optimizer,\n\t            criterion=criterion,\n", "            device=self.device,\n\t            epochs=self.config.epochs,\n\t            patience=self.config.patience,\n\t            eval_metric=self.config.eval.metric,\n\t            eval_less_is_better=self.config.eval.less_is_better,\n\t            mixed_fp16=self.config.mixed_fp16,\n\t            save_model=self.config.save_model,\n\t            **self.config.model.params,\n\t        )\n\t    def run(self):\n", "        self.supervised_init()\n\t        self.trainer.train()\n\t        self.trainer.print_evaluate()\n"]}
{"filename": "model/__init__.py", "chunked_list": ["from .core.fttrans import FTTransformer\n\tfrom .hidden_mix_alpha import FTTransformerWithHiddenMix\n\tfrom .mask_token import FTTransformerWithMaskToken\n\tfrom .mixup import FTTransformerWithMixup\n"]}
{"filename": "model/mixup.py", "chunked_list": ["from typing import Optional, Tuple\n\timport torch\n\tfrom torch import Tensor\n\tfrom .core import FTTransformer\n\tdef mixup_process(x: Tensor, lam: float, target_reweighted: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n\t    \"\"\"\n\t    Args:\n\t        x (tensor): the input of shape (b, n, d)\n\t        lam (float): the mixing coefficient\n\t        target_reweighted (tensor): the target labels of shape (b, num_classes)\n", "    Returns:\n\t        new_x (tensor): the output of shape (b, n, d) representing the mixed input data\n\t        target_reweighted (tensor): the mixed target labels of shape (b, num_classes)\n\t    \"\"\"\n\t    indices = torch.randperm(x.shape[0])\n\t    new_x = x * (1 - lam) + x[indices] * lam\n\t    if target_reweighted is not None:\n\t        target_shuffled_onehot = target_reweighted[indices]\n\t        target_reweighted = target_reweighted * (1 - lam) + target_shuffled_onehot * lam\n\t        return new_x, target_reweighted\n", "    else:\n\t        return new_x\n\tclass FTTransformerWithMixup(FTTransformer):\n\t    def forward(\n\t        self,\n\t        x_num: Optional[Tensor],\n\t        x_cat: Optional[Tensor],\n\t        target: Tensor = None,\n\t        lam: Tensor = None,\n\t    ) -> Tuple[Tensor, Optional[Tensor]]:\n", "        \"\"\"\n\t        Args:\n\t            x_num (tensor): the input of shape (b, n) representing the numerical values\n\t            x_cat (tensor): the input of shape (b, n) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t            lam (tensor): the mixing coefficient\n\t        Returns:\n\t            x (tensor): the output of shape (b, num_classes) representing the model's prediction\n\t            new_target (tensor): the mixed target labels of shape (b, num_classes)\n\t        \"\"\"\n", "        \"\"\"\n\t        Supervised\n\t        \"\"\"\n\t        x = self.feature_tokenizer(x_num, x_cat)\n\t        x = x + self.pos_embedding\n\t        x = self.cls_token(x)\n\t        x = self.transformer(x)\n\t        x = x[:, -1]\n\t        x = self.normalization(x)\n\t        x = self.activation(x)\n", "        if target is not None:\n\t            assert lam is not None\n\t            x, new_target = mixup_process(x, lam, target)\n\t            x = self.head(x)\n\t            return x, new_target\n\t        else:\n\t            return self.head(x)\n\t    def forward_no_labelmix(\n\t        self,\n\t        x_num: Optional[Tensor],\n", "        x_cat: Optional[Tensor],\n\t        alpha: float = 0.0,\n\t    ) -> Tensor:\n\t        \"\"\"\n\t        Args:\n\t            x_num (tensor): the input of shape (b, n) representing the numerical values\n\t            x_cat (tensor): the input of shape (b, n) representing the categorical values\n\t            alpha (float): the mixing coefficient\n\t        Returns:\n\t            tensor: the output of shape (b, num_classes) representing the model's prediction\n", "        \"\"\"\n\t        \"\"\"\n\t        Self-SL\n\t        0.0      <= alpha <= 1.0\n\t        original <=   x   <= other\n\t        \"\"\"\n\t        x = self.feature_tokenizer(x_num, x_cat)\n\t        x = x + self.pos_embedding\n\t        x = self.cls_token(x)\n\t        x = self.transformer(x)\n", "        x = x[:, -1]\n\t        x = self.normalization(x)\n\t        x = self.activation(x)\n\t        x = mixup_process(x, alpha)\n\t        return self.head(x)\n"]}
{"filename": "model/mask_token.py", "chunked_list": ["from typing import Any, Dict, List, Optional, Type, Union\n\timport torch\n\timport torch.nn as nn\n\tfrom rtdl.modules import _INTERNAL_ERROR_MESSAGE\n\tfrom torch import Tensor\n\tfrom .core import FeatureTokenizer, FTTransformer, Head, Transformer\n\tclass FTTransformerWithMaskToken(FTTransformer):\n\t    def __init__(\n\t        self,\n\t        num_features,\n", "        feature_tokenizer: FeatureTokenizer,\n\t        transformer: Transformer,\n\t        head: Head,\n\t    ) -> None:\n\t        super().__init__(num_features, feature_tokenizer, transformer, head)\n\t        self.mask_token = nn.Parameter(Tensor(1, 1, feature_tokenizer.d_token))\n\t        self.initialization_.apply(self.mask_token, feature_tokenizer.d_token)\n\t    def optimization_param_groups(self) -> List[Dict[str, Any]]:\n\t        \"\"\"The replacement for :code:`.parameters()` when creating optimizers.\n\t        Example::\n", "            optimizer = AdamW(\n\t                model.optimization_param_groups(), lr=1e-4, weight_decay=1e-5\n\t            )\n\t        \"\"\"\n\t        no_wd_names = [\"feature_tokenizer\", \"normalization\", \".bias\", \"pos_embedding\", \"mask_token\"]\n\t        assert isinstance(getattr(self, no_wd_names[0], None), FeatureTokenizer), _INTERNAL_ERROR_MESSAGE\n\t        assert (\n\t            sum(1 for name, _ in self.named_modules() if no_wd_names[1] in name)\n\t            == len(self.transformer.blocks) * 2\n\t            - int(\"attention_normalization\" not in self.transformer.blocks[0])  # type: ignore\n", "            + 1\n\t        ), _INTERNAL_ERROR_MESSAGE\n\t        def needs_wd(name):\n\t            return all(x not in name for x in no_wd_names)\n\t        return [\n\t            {\"params\": [v for k, v in self.named_parameters() if needs_wd(k)]},\n\t            {\n\t                \"params\": [v for k, v in self.named_parameters() if not needs_wd(k)],\n\t                \"weight_decay\": 0.0,\n\t            },\n", "        ]\n\t    def random_masking(self, x: Tensor, mask_ratio: float) -> Tensor:\n\t        \"\"\"\n\t        Args:\n\t            x (tensor): the input of shape (b, n, d).\n\t            mask_ratio (float): the ratio of data points to be masked\n\t        Returns:\n\t            x_masked (tensor): the output of shape (b, n, d) representing the masked data\n\t        \"\"\"\n\t        b, n, d = x.shape\n", "        mask = torch.bernoulli(torch.ones(b, n) * mask_ratio)\n\t        mask = mask.unsqueeze(-1).to(torch.float).to(x.device)\n\t        mask = mask.repeat(1, 1, d)\n\t        mask_tokens = self.mask_token.repeat(b, n, 1)\n\t        x_masked = x * (1 - mask) + mask_tokens * mask\n\t        return x_masked\n\t    def forward(\n\t        self,\n\t        x_num: Optional[Tensor],\n\t        x_cat: Optional[Tensor],\n", "        mask_ratio: float = 0.0,\n\t        bias_after_mask: bool = True,\n\t    ) -> Tensor:\n\t        \"\"\"\n\t        Args:\n\t            x_num (tensor): the input of shape (b, n) representing the numerical values\n\t            x_cat (tensor): the input of shape (b, n) representing the categorical values\n\t            mask_ratio (float): the ratio of data points to be masked\n\t            bias_after_mask (bool): whether to add the positional embedding before or after masking\n\t        Returns:\n", "            tensor: the output of shape (b, num_classes) representing the model's prediction\n\t        \"\"\"\n\t        x = self.feature_tokenizer(x_num, x_cat)\n\t        if not bias_after_mask:\n\t            x = x + self.pos_embedding\n\t        if mask_ratio > 0.0:\n\t            x = self.random_masking(x, mask_ratio)\n\t        if bias_after_mask:\n\t            x = x + self.pos_embedding\n\t        x = self.cls_token(x)\n", "        x = self.transformer(x)\n\t        x = x[:, -1]\n\t        x = self.normalization(x)\n\t        x = self.activation(x)\n\t        return self.head(x)\n"]}
{"filename": "model/hidden_mix_alpha.py", "chunked_list": ["from typing import Optional, Tuple\n\timport torch\n\tfrom torch import Tensor\n\tfrom .core import FTTransformer\n\tdef mask_generator(x: Tensor, lam: float) -> Tensor:\n\t    \"\"\"\n\t    Args:\n\t        x (tensor): the input of shape (b, n, d)\n\t        lam (float): the scalar coefficient to keep unmasked.\n\t    Returns:\n", "        mask (tensor): the binary mask of shape (b, n, d)\n\t    \"\"\"\n\t    b, n, d = x.shape\n\t    ids_noise = torch.rand(b, d, device=x.device)\n\t    ids_shuffle = torch.argsort(ids_noise, dim=1)\n\t    len_unmask = int(lam * d)\n\t    ids_unmask = ids_shuffle[:, :len_unmask]\n\t    mask = torch.zeros(b, d, device=x.device)\n\t    mask[torch.arange(b)[:, None], ids_unmask] = 1\n\t    mask = mask.unsqueeze(1)\n", "    mask = mask.repeat(1, n, 1)\n\t    return mask\n\tdef hidden_mix(x: Tensor, target: Optional[Tensor], lam: float) -> Tuple[Tensor, Optional[Tensor]]:\n\t    \"\"\"\n\t    Args:\n\t        x (tensor): the input of shape (b, n, d)\n\t        target (tensor): the target labels of shape (b, num_classes)\n\t        lam (float): the scalar coefficient to keep unmasked.\n\t    Returns:\n\t        new_x (tensor): the output of shape (b, n, d) representing the mixed input data\n", "        label (tensor): the mixed target labels of shape (b, num_classes)\n\t    \"\"\"\n\t    mask = mask_generator(x, lam)\n\t    indices = torch.randperm(x.shape[0])\n\t    new_x = x * mask + x[indices] * (1 - mask)\n\t    if target is not None:\n\t        label = lam * target + (1 - lam) * target[indices]\n\t        return new_x, label\n\t    else:\n\t        return new_x, None\n", "class FTTransformerWithHiddenMix(FTTransformer):\n\t    def forward(\n\t        self,\n\t        x_num: Optional[Tensor],\n\t        x_cat: Optional[Tensor],\n\t        target: Tensor = None,\n\t        lam: Tensor = None,\n\t    ) -> Tensor:\n\t        \"\"\"\n\t        Args:\n", "            x_num (tensor): the input of shape (b, n) representing the numerical values\n\t            x_cat (tensor): the input of shape (b, n) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t            lam (tensor): the scalar coefficient to keep unmasked.\n\t        Returns:\n\t            tensor: the output of shape (b, num_classes) representing the model's prediction\n\t        \"\"\"\n\t        x = self.feature_tokenizer(x_num, x_cat)\n\t        x = x + self.pos_embedding\n\t        if target is not None or lam is not None:\n", "            assert lam is not None\n\t            x, new_target = hidden_mix(x, target, lam)\n\t            x = self.cls_token(x)\n\t            x = self.transformer(x)\n\t            x = x[:, -1]\n\t            x = self.normalization(x)\n\t            x = self.activation(x)\n\t            return self.head(x), new_target\n\t        else:\n\t            x = self.cls_token(x)\n", "            x = self.transformer(x)\n\t            x = x[:, -1]\n\t            x = self.normalization(x)\n\t            x = self.activation(x)\n\t            return self.head(x)\n"]}
{"filename": "model/core/transformer.py", "chunked_list": ["import math\n\timport time\n\timport warnings\n\tfrom typing import Dict, List, Optional, Tuple, Union, cast\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom rtdl.modules import (\n\t    _INTERNAL_ERROR_MESSAGE,\n\t    ModuleType,\n\t    MultiheadAttention,\n", "    _all_or_none,\n\t    _is_glu_activation,\n\t    _make_nn_module,\n\t)\n\tfrom torch import Tensor\n\tclass MultiheadAttentionWithMask(MultiheadAttention):\n\t    def forward(\n\t        self,\n\t        x_q: Tensor,\n\t        x_kv: Tensor,\n", "        key_compression: Optional[nn.Linear],\n\t        value_compression: Optional[nn.Linear],\n\t        attn_mask: Optional[Tensor] = None,\n\t    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n\t        \"\"\"Perform the forward pass.\n\t        Args:\n\t            x_q: query tokens\n\t            x_kv: key-value tokens\n\t            key_compression: Linformer-style compression for keys\n\t            value_compression: Linformer-style compression for values\n", "        Returns:\n\t            (tokens, attention_stats)\n\t        \"\"\"\n\t        assert _all_or_none(\n\t            [key_compression, value_compression]\n\t        ), \"If key_compression is (not) None, then value_compression must (not) be None\"\n\t        q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n\t        for tensor in [q, k, v]:\n\t            assert tensor.shape[-1] % self.n_heads == 0, _INTERNAL_ERROR_MESSAGE\n\t        if key_compression is not None:\n", "            k = key_compression(k.transpose(1, 2)).transpose(1, 2)\n\t            v = value_compression(v.transpose(1, 2)).transpose(1, 2)  # type: ignore\n\t        batch_size = len(q)\n\t        d_head_key = k.shape[-1] // self.n_heads\n\t        d_head_value = v.shape[-1] // self.n_heads\n\t        n_q_tokens = q.shape[1]\n\t        q = self._reshape(q)\n\t        k = self._reshape(k)\n\t        attention_logits = q @ k.transpose(1, 2) / math.sqrt(d_head_key)\n\t        if attn_mask is not None:\n", "            attention_logits = attention_logits.masked_fill(attn_mask, float(\"-inf\"))\n\t        attention_probs = F.softmax(attention_logits, dim=-1)\n\t        if self.dropout is not None:\n\t            attention_probs = self.dropout(attention_probs)\n\t        x = attention_probs @ self._reshape(v)\n\t        x = (\n\t            x.reshape(batch_size, self.n_heads, n_q_tokens, d_head_value)\n\t            .transpose(1, 2)\n\t            .reshape(batch_size, n_q_tokens, self.n_heads * d_head_value)\n\t        )\n", "        if self.W_out is not None:\n\t            x = self.W_out(x)\n\t        return x, {\n\t            \"attention_logits\": attention_logits,\n\t            \"attention_probs\": attention_probs,\n\t        }\n\tclass Transformer(nn.Module):\n\t    \"\"\"Transformer with extra features.\n\t    This module is the backbone of `FTTransformer`.\"\"\"\n\t    WARNINGS = {\"first_prenormalization\": True, \"prenormalization\": True}\n", "    class FFN(nn.Module):\n\t        \"\"\"The Feed-Forward Network module used in every `Transformer` block.\"\"\"\n\t        def __init__(\n\t            self,\n\t            *,\n\t            d_token: int,\n\t            d_hidden: int,\n\t            bias_first: bool,\n\t            bias_second: bool,\n\t            dropout: float,\n", "            activation: ModuleType,\n\t        ):\n\t            super().__init__()\n\t            self.linear_first = nn.Linear(\n\t                d_token,\n\t                d_hidden * (2 if _is_glu_activation(activation) else 1),\n\t                bias_first,\n\t            )\n\t            self.activation = _make_nn_module(activation)\n\t            self.dropout = nn.Dropout(dropout)\n", "            self.linear_second = nn.Linear(d_hidden, d_token, bias_second)\n\t        def forward(self, x: Tensor) -> Tensor:\n\t            x = self.linear_first(x)\n\t            x = self.activation(x)\n\t            x = self.dropout(x)\n\t            x = self.linear_second(x)\n\t            return x\n\t    def __init__(\n\t        self,\n\t        *,\n", "        d_token: int,\n\t        n_blocks: int,\n\t        attention_n_heads: int,\n\t        attention_dropout: float,\n\t        attention_initialization: str,\n\t        attention_normalization: str,\n\t        ffn_d_hidden: int,\n\t        ffn_dropout: float,\n\t        ffn_activation: str,\n\t        ffn_normalization: str,\n", "        residual_dropout: float,\n\t        prenormalization: bool,\n\t        first_prenormalization: bool,\n\t        last_layer_query_idx: Union[None, List[int], slice],\n\t        n_tokens: Optional[int],\n\t        kv_compression_ratio: Optional[float],\n\t        kv_compression_sharing: Optional[str],\n\t    ) -> None:\n\t        super().__init__()\n\t        if isinstance(last_layer_query_idx, int):\n", "            raise ValueError(\n\t                \"last_layer_query_idx must be None, list[int] or slice. \"\n\t                f\"Do you mean last_layer_query_idx=[{last_layer_query_idx}] ?\"\n\t            )\n\t        if not prenormalization:\n\t            assert (\n\t                not first_prenormalization\n\t            ), \"If `prenormalization` is False, then `first_prenormalization` must be False\"\n\t        assert _all_or_none([n_tokens, kv_compression_ratio, kv_compression_sharing]), (\n\t            \"If any of the following arguments is (not) None, then all of them must (not) be None: \"\n", "            \"n_tokens, kv_compression_ratio, kv_compression_sharing\"\n\t        )\n\t        assert kv_compression_sharing in [None, \"headwise\", \"key-value\", \"layerwise\"]\n\t        if not prenormalization:\n\t            if self.WARNINGS[\"prenormalization\"]:\n\t                warnings.warn(\n\t                    \"prenormalization is set to False. Are you sure about this? \"\n\t                    \"The training can become less stable. \"\n\t                    \"You can turn off this warning by tweaking the \"\n\t                    \"rtdl.Transformer.WARNINGS dictionary.\",\n", "                    UserWarning,\n\t                )\n\t            assert (\n\t                not first_prenormalization\n\t            ), \"If prenormalization is False, then first_prenormalization is ignored and must be set to False\"\n\t        if prenormalization and first_prenormalization and self.WARNINGS[\"first_prenormalization\"]:\n\t            warnings.warn(\n\t                \"first_prenormalization is set to True. Are you sure about this? \"\n\t                \"For example, the vanilla FTTransformer with \"\n\t                \"first_prenormalization=True performs SIGNIFICANTLY worse. \"\n", "                \"You can turn off this warning by tweaking the \"\n\t                \"rtdl.Transformer.WARNINGS dictionary.\",\n\t                UserWarning,\n\t            )\n\t            time.sleep(3)\n\t        def make_kv_compression():\n\t            assert n_tokens and kv_compression_ratio, _INTERNAL_ERROR_MESSAGE  # for mypy\n\t            # https://github.com/pytorch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/linformer/linformer_src/modules/multihead_linear_attention.py#L83\n\t            return nn.Linear(n_tokens, int(n_tokens * kv_compression_ratio), bias=False)\n\t        self.shared_kv_compression = (\n", "            make_kv_compression() if kv_compression_ratio and kv_compression_sharing == \"layerwise\" else None\n\t        )\n\t        self.prenormalization = prenormalization\n\t        self.last_layer_query_idx = last_layer_query_idx\n\t        self.blocks = nn.ModuleList([])\n\t        for layer_idx in range(n_blocks):\n\t            layer = nn.ModuleDict(\n\t                {\n\t                    \"attention\": MultiheadAttentionWithMask(\n\t                        d_token=d_token,\n", "                        n_heads=attention_n_heads,\n\t                        dropout=attention_dropout,\n\t                        bias=True,\n\t                        initialization=attention_initialization,\n\t                    ),\n\t                    \"ffn\": Transformer.FFN(\n\t                        d_token=d_token,\n\t                        d_hidden=ffn_d_hidden,\n\t                        bias_first=True,\n\t                        bias_second=True,\n", "                        dropout=ffn_dropout,\n\t                        activation=ffn_activation,\n\t                    ),\n\t                    \"attention_residual_dropout\": nn.Dropout(residual_dropout),\n\t                    \"ffn_residual_dropout\": nn.Dropout(residual_dropout),\n\t                    \"output\": nn.Identity(),  # for hooks-based introspection\n\t                }\n\t            )\n\t            if layer_idx or not prenormalization or first_prenormalization:\n\t                layer[\"attention_normalization\"] = _make_nn_module(attention_normalization, d_token)\n", "            layer[\"ffn_normalization\"] = _make_nn_module(ffn_normalization, d_token)\n\t            if kv_compression_ratio and self.shared_kv_compression is None:\n\t                layer[\"key_compression\"] = make_kv_compression()\n\t                if kv_compression_sharing == \"headwise\":\n\t                    layer[\"value_compression\"] = make_kv_compression()\n\t                else:\n\t                    assert kv_compression_sharing == \"key-value\", _INTERNAL_ERROR_MESSAGE\n\t            self.blocks.append(layer)\n\t    def _get_kv_compressions(self, layer):\n\t        return (\n", "            (self.shared_kv_compression, self.shared_kv_compression)\n\t            if self.shared_kv_compression is not None\n\t            else (layer[\"key_compression\"], layer[\"value_compression\"])\n\t            if \"key_compression\" in layer and \"value_compression\" in layer\n\t            else (layer[\"key_compression\"], layer[\"key_compression\"])\n\t            if \"key_compression\" in layer\n\t            else (None, None)\n\t        )\n\t    def _start_residual(self, layer, stage, x):\n\t        assert stage in [\"attention\", \"ffn\"], _INTERNAL_ERROR_MESSAGE\n", "        x_residual = x\n\t        if self.prenormalization:\n\t            norm_key = f\"{stage}_normalization\"\n\t            if norm_key in layer:\n\t                x_residual = layer[norm_key](x_residual)\n\t        return x_residual\n\t    def _end_residual(self, layer, stage, x, x_residual):\n\t        assert stage in [\"attention\", \"ffn\"], _INTERNAL_ERROR_MESSAGE\n\t        x_residual = layer[f\"{stage}_residual_dropout\"](x_residual)\n\t        x = x + x_residual\n", "        if not self.prenormalization:\n\t            x = layer[f\"{stage}_normalization\"](x)\n\t        return x\n\t    def forward(self, x: Tensor, attn_mask: Optional[Tensor] = None) -> Tensor:\n\t        assert x.ndim == 3, \"The input must have 3 dimensions: (n_objects, n_tokens, d_token)\"\n\t        for layer_idx, layer in enumerate(self.blocks):\n\t            layer = cast(nn.ModuleDict, layer)\n\t            query_idx = self.last_layer_query_idx if layer_idx + 1 == len(self.blocks) else None\n\t            x_residual = self._start_residual(layer, \"attention\", x)\n\t            x_residual, _ = layer[\"attention\"](\n", "                x_residual if query_idx is None else x_residual[:, query_idx],\n\t                x_residual,\n\t                attn_mask=attn_mask if query_idx is None or attn_mask is None else attn_mask[:, query_idx],\n\t                *self._get_kv_compressions(layer),\n\t            )\n\t            if query_idx is not None:\n\t                x = x[:, query_idx]\n\t            x = self._end_residual(layer, \"attention\", x, x_residual)\n\t            x_residual = self._start_residual(layer, \"ffn\", x)\n\t            x_residual = layer[\"ffn\"](x_residual)\n", "            x = self._end_residual(layer, \"ffn\", x, x_residual)\n\t            x = layer[\"output\"](x)\n\t        return x\n"]}
{"filename": "model/core/__init__.py", "chunked_list": ["from .fttrans import FeatureTokenizer, FTTransformer, Head\n\tfrom .transformer import Transformer\n"]}
{"filename": "model/core/fttrans.py", "chunked_list": ["from typing import Any, Dict, List, Optional, Type, Union\n\timport rtdl\n\timport torch\n\timport torch.nn as nn\n\tfrom rtdl.modules import (\n\t    _INTERNAL_ERROR_MESSAGE,\n\t    CategoricalFeatureTokenizer,\n\t    NumericalFeatureTokenizer,\n\t    _TokenInitialization,\n\t)\n", "from torch import Tensor\n\tfrom .transformer import Transformer\n\tclass FeatureTokenizer(rtdl.modules.FeatureTokenizer):\n\t    def __init__(\n\t        self,\n\t        n_num_features: int,\n\t        cat_cardinalities: List[int],\n\t        d_token: int,\n\t        bias: bool = False,\n\t    ) -> None:\n", "        super().__init__(n_num_features, cat_cardinalities, d_token)\n\t        self.num_tokenizer = (\n\t            NumericalFeatureTokenizer(\n\t                n_features=n_num_features,\n\t                d_token=d_token,\n\t                bias=bias,\n\t                initialization=self.initialization,\n\t            )\n\t            if n_num_features\n\t            else None\n", "        )\n\t        self.cat_tokenizer = (\n\t            CategoricalFeatureTokenizer(cat_cardinalities, d_token, bias, self.initialization)\n\t            if cat_cardinalities\n\t            else None\n\t        )\n\tclass Head(nn.Module):\n\t    \"\"\"The final module of the `Transformer` that performs BERT-like inference.\"\"\"\n\t    def __init__(\n\t        self,\n", "        *,\n\t        d_in: int,\n\t        bias: bool,\n\t        d_out: int,\n\t    ):\n\t        super().__init__()\n\t        self.linear = nn.Linear(d_in, d_out, bias)\n\t    def forward(self, x: Tensor) -> Tensor:\n\t        x = self.linear(x)\n\t        return x\n", "class ProjectionHead(nn.Module):\n\t    def __init__(self, d_in: int, projection_dim=64) -> None:\n\t        super().__init__()\n\t        self.f1 = nn.Linear(d_in, d_in, bias=False)\n\t        self.act = nn.ReLU()\n\t        self.f2 = nn.Linear(d_in, projection_dim, bias=False)\n\t    def forward(self, x: Tensor) -> Tensor:\n\t        x = self.f1(x)\n\t        x = self.act(x)\n\t        x = self.f2(x)\n", "        return x\n\tclass FTTransformer(rtdl.FTTransformer):\n\t    def __init__(\n\t        self,\n\t        num_features,\n\t        feature_tokenizer: FeatureTokenizer,\n\t        transformer: Transformer,\n\t        head: Head,\n\t    ) -> None:\n\t        super().__init__(feature_tokenizer, transformer)\n", "        self.pos_embedding = nn.Parameter(Tensor(num_features, feature_tokenizer.d_token))\n\t        self.initialization_ = _TokenInitialization.from_str(\"uniform\")\n\t        self.initialization_.apply(self.pos_embedding, feature_tokenizer.d_token)\n\t        if transformer.prenormalization:\n\t            self.normalization = nn.LayerNorm(feature_tokenizer.d_token)\n\t        else:\n\t            self.normalization = nn.Identity()\n\t        self.activation = nn.ReLU()\n\t        self.head = head\n\t    def optimization_param_groups(self) -> List[Dict[str, Any]]:\n", "        \"\"\"The replacement for :code:`.parameters()` when creating optimizers.\n\t        Example::\n\t            optimizer = AdamW(\n\t                model.optimization_param_groups(), lr=1e-4, weight_decay=1e-5\n\t            )\n\t        \"\"\"\n\t        no_wd_names = [\"feature_tokenizer\", \"normalization\", \".bias\", \"pos_embedding\"]\n\t        assert isinstance(getattr(self, no_wd_names[0], None), FeatureTokenizer), _INTERNAL_ERROR_MESSAGE\n\t        assert (\n\t            sum(1 for name, _ in self.named_modules() if no_wd_names[1] in name)\n", "            == len(self.transformer.blocks) * 2\n\t            - int(\"attention_normalization\" not in self.transformer.blocks[0])  # type: ignore\n\t            + 1\n\t        ), _INTERNAL_ERROR_MESSAGE\n\t        def needs_wd(name):\n\t            return all(x not in name for x in no_wd_names)\n\t        return [\n\t            {\"params\": [v for k, v in self.named_parameters() if needs_wd(k)]},\n\t            {\n\t                \"params\": [v for k, v in self.named_parameters() if not needs_wd(k)],\n", "                \"weight_decay\": 0.0,\n\t            },\n\t        ]\n\t    def make_default_optimizer(self, lr=1e-4) -> torch.optim.AdamW:\n\t        \"\"\"Make the optimizer for the default FT-Transformer.\"\"\"\n\t        return torch.optim.AdamW(\n\t            self.optimization_param_groups(),\n\t            lr=lr,\n\t            weight_decay=1e-5,\n\t        )\n", "    @classmethod\n\t    def make_default(\n\t        cls: Type[\"FTTransformer\"],\n\t        *,\n\t        num_features: int,\n\t        n_num_features: int,\n\t        cat_cardinalities: Optional[List[int]],\n\t        n_blocks: int = 3,\n\t        last_layer_query_idx: Union[None, List[int], slice] = None,\n\t        kv_compression_ratio: Optional[float] = None,\n", "        kv_compression_sharing: Optional[str] = None,\n\t        d_out: int,\n\t    ) -> \"FTTransformer\":\n\t        transformer_config = cls.get_default_transformer_config(n_blocks=n_blocks)\n\t        for arg_name in [\n\t            \"last_layer_query_idx\",\n\t            \"kv_compression_ratio\",\n\t            \"kv_compression_sharing\",\n\t            \"d_out\",\n\t        ]:\n", "            transformer_config[arg_name] = locals()[arg_name]\n\t        return cls._make(num_features, n_num_features, cat_cardinalities, transformer_config)\n\t    @classmethod\n\t    def _make(\n\t        cls,\n\t        num_features,\n\t        n_num_features,\n\t        cat_cardinalities,\n\t        transformer_config,\n\t    ):\n", "        feature_tokenizer = FeatureTokenizer(\n\t            n_num_features=n_num_features,\n\t            cat_cardinalities=cat_cardinalities,\n\t            d_token=transformer_config[\"d_token\"],\n\t            bias=False,\n\t        )\n\t        if transformer_config[\"d_out\"] is None:\n\t            transformer_config[\"head_activation\"] = None\n\t        if transformer_config[\"kv_compression_ratio\"] is not None:\n\t            transformer_config[\"n_tokens\"] = feature_tokenizer.n_tokens + 2\n", "        head = Head(\n\t            d_in=transformer_config[\"d_token\"],\n\t            d_out=transformer_config[\"d_out\"],\n\t            bias=True,\n\t        )\n\t        del (\n\t            transformer_config[\"d_out\"],\n\t            transformer_config[\"head_activation\"],\n\t            transformer_config[\"head_normalization\"],\n\t        )\n", "        return cls(\n\t            num_features,\n\t            feature_tokenizer,\n\t            transformer=Transformer(**transformer_config),\n\t            head=head,\n\t        )\n\t    def forward(self, x_num: Optional[Tensor], x_cat: Optional[Tensor]) -> Tensor:\n\t        x = self.feature_tokenizer(x_num, x_cat)\n\t        x = x + self.pos_embedding\n\t        x = self.cls_token(x)\n", "        x = self.transformer(x)\n\t        x = x[:, -1]\n\t        x = self.normalization(x)\n\t        x = self.activation(x)\n\t        return self.head(x)\n"]}
{"filename": "script/self_sl/run_all.py", "chunked_list": ["import argparse\n\timport subprocess\n\tfrom pathlib import Path\n\tdef main(args):\n\t    script_path = Path(__file__).parent / \"all.sh\"\n\t    if \",\" in args.data:\n\t        for data in args.data.split(\",\"):\n\t            cmd = [\n\t                str(script_path),\n\t                data,\n", "                args.train_size,\n\t                args.max_seed,\n\t            ]\n\t            subprocess.run(cmd)\n\t    else:\n\t        cmd = [\n\t            str(script_path),\n\t            args.data,\n\t            args.train_size,\n\t            args.max_seed,\n", "        ]\n\t        subprocess.run(cmd)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"data\", type=str)\n\t    parser.add_argument(\"train_size\", type=str)\n\t    parser.add_argument(\"-m\", \"--max-seed\", type=str, default=\"10\")\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "script/sl/run_all.py", "chunked_list": ["import argparse\n\timport subprocess\n\tfrom pathlib import Path\n\tdef main(args):\n\t    script_path = Path(__file__).parent / \"all.sh\"\n\t    if \",\" in args.data:\n\t        for data in args.data.split(\",\"):\n\t            cmd = [\n\t                str(script_path),\n\t                data,\n", "                args.train_size,\n\t                args.max_seed,\n\t            ]\n\t            subprocess.run(cmd)\n\t    else:\n\t        cmd = [\n\t            str(script_path),\n\t            args.data,\n\t            args.train_size,\n\t            args.max_seed,\n", "        ]\n\t        subprocess.run(cmd)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"data\", type=str)\n\t    parser.add_argument(\"train_size\", type=str)\n\t    parser.add_argument(\"-m\", \"--max-seed\", type=str, default=\"10\")\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "trainer/base.py", "chunked_list": ["import logging\n\timport os\n\tfrom typing import Optional, Tuple, Union\n\timport numpy as np\n\timport torch\n\tfrom timm.scheduler.scheduler import Scheduler\n\tfrom torch import Tensor\n\tfrom torch.cuda.amp import GradScaler\n\tfrom torch.nn.modules.loss import _Loss\n\tfrom torch.optim import Optimizer\n", "from torch.utils.data import DataLoader\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom tqdm import tqdm\n\tfrom data import TabularDatamodule\n\tfrom model.core.fttrans import FTTransformer\n\tfrom .utils import EarlyStopping, auto_batch_size, save_json\n\tlogger = logging.getLogger(__name__)\n\tclass BaseTrainer:\n\t    def __init__(\n\t        self,\n", "        datamodule: TabularDatamodule,\n\t        batch_size: Union[int, str],\n\t        eval_batch_size: int,\n\t        model: FTTransformer,\n\t        optimizer: Optimizer,\n\t        criterion: _Loss,\n\t        epochs: int,\n\t        device: torch._C.device,\n\t        patience: int = 16,\n\t        eval_metric: str = \"val/loss\",\n", "        eval_less_is_better: bool = True,\n\t        scheduler: Scheduler = None,\n\t        mixed_fp16: bool = False,\n\t        tensorbord_dir: str = \"./supervised\",\n\t        save_model: bool = False,\n\t    ) -> None:\n\t        \"\"\"\n\t        Args:\n\t            datamodule (TabularDatamodule): providing the dataset and dataloaders\n\t            batch_size (Union[int, str]): the batch size used for training\n", "            eval_batch_size (int): the batch size used for evaluation\n\t            model (FTTransformer): the FTTransformer model used for training\n\t            optimizer (Optimizer): used for updating the model parameters during training\n\t            criterion (_Loss): the loss function\n\t            epochs (int): the total number of training epochs\n\t            device (torch._C.device): the device to be used for computations (e.g., \"cpu\", \"cuda\")\n\t            patience (int): the number of epochs to wait for improvement\n\t            eval_metric (str): the evaluation metric\n\t            eval_less_is_better (bool): the flag representing whether the lower value indicates better performance\n\t            scheduler (Scheduler): used for adjusting the learning rate during training\n", "            mixed_fp16 (bool): whether to use mixed precision training with FP16\n\t            tensorbord_dir (str): the directory path to save TensorBoard logs\n\t            save_model (bool): whether to save the best model during training\n\t        \"\"\"\n\t        self.datamodule = datamodule\n\t        if batch_size == \"auto\":\n\t            batch_size = auto_batch_size(len(datamodule.train))\n\t            logger.info(f\"Use auto batch size; choose {batch_size}\")\n\t        self.batch_size = batch_size\n\t        self.eval_batch_size = eval_batch_size\n", "        self.model = model\n\t        self.optimizer = optimizer\n\t        self.criterion = criterion\n\t        self.epochs = epochs\n\t        self.eval_metric = eval_metric\n\t        self.eval_less_is_better = eval_less_is_better\n\t        self.early_stopping = EarlyStopping(patience)\n\t        self.scheduler = scheduler\n\t        if mixed_fp16:\n\t            self.scaler = GradScaler()\n", "        else:\n\t            self.scaler = None\n\t        self.save_model = save_model\n\t        self.device = device\n\t        os.makedirs(tensorbord_dir, exist_ok=True)\n\t        self.writer = SummaryWriter(tensorbord_dir)\n\t    def apply_device(self, data: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            data (tensor): the input of shape (b, n)\n", "        Returns:\n\t            cont (tensor): the output of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the output of shape (b, num_cate) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        \"\"\"\n\t        cont = data[\"continuous\"]\n\t        cate = data[\"categorical\"]\n\t        if \"target\" in data:\n\t            target = data[\"target\"].to(self.device)\n\t        else:\n", "            target = None\n\t        if self.datamodule.task == \"multiclass\" and target is not None:\n\t            target = target.squeeze(1)\n\t        if cont != []:\n\t            cont = cont.to(self.device)\n\t        else:\n\t            cont = None\n\t        if cate != []:\n\t            cate = cate.to(self.device)\n\t        else:\n", "            cate = None\n\t        return cont, cate, target\n\t    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None):\n\t        raise NotImplementedError()\n\t    def train_dataloader(self):\n\t        return self.datamodule.dataloader(\"train\", batch_size=self.batch_size)\n\t    def train_per_epoch(self, dataloader: DataLoader, pbar_epoch: tqdm, epoch: int):\n\t        raise NotImplementedError()\n\t    @torch.no_grad()\n\t    def eval(self, mode: str = \"test\"):\n", "        raise NotImplementedError()\n\t    def train(self) -> None:\n\t        if self.eval_less_is_better:\n\t            best_score = np.inf\n\t        else:\n\t            best_score = -np.inf\n\t        dataloader = self.train_dataloader()\n\t        for epoch in range(1, self.epochs + 1):\n\t            with tqdm(total=len(dataloader), bar_format=\"{l_bar}{bar}{r_bar}{bar:-10b}\") as pbar_epoch:\n\t                scores = self.train_per_epoch(dataloader, pbar_epoch, epoch)\n", "                scores.update(self.eval(\"val\"))\n\t                pbar_epoch.set_postfix(scores)\n\t                for tag, score in scores.items():\n\t                    self.writer.add_scalar(tag, score, epoch)\n\t            if self.eval_less_is_better:\n\t                self.early_stopping(scores[self.eval_metric], self.model)\n\t                if best_score > scores[self.eval_metric]:\n\t                    best_score = scores[self.eval_metric]\n\t            else:\n\t                self.early_stopping(-scores[self.eval_metric], self.model)\n", "                if best_score < scores[self.eval_metric]:\n\t                    best_score = scores[self.eval_metric]\n\t            if self.early_stopping.early_stop:\n\t                logger.info(f\"early stopping {epoch} / {self.epochs}\")\n\t                break\n\t        self.model.load_state_dict(self.early_stopping.best_model)\n\t    def print_evaluate(self) -> None:\n\t        scores = self.eval()\n\t        if self.save_model:\n\t            torch.save(self.early_stopping.best_model, \"./best_model\")\n", "        save_json(scores, \"./\")\n\t        for key, score in scores.items():\n\t            logger.info(f\"{key}: {score}\")\n"]}
{"filename": "trainer/__init__.py", "chunked_list": ["from .self_supervised import (\n\t    FTTransHiddenMixSSLTrainer,\n\t    FTTransMaskTokenSSLTrainer,\n\t    FTTransMixupSSLTrainer,\n\t    FTTransSCARFSSLTrainer,\n\t)\n\tfrom .supervised import (\n\t    FTTransTraniner,\n\t    FTTransWithCutmixTraniner,\n\t    FTTransWithHiddenMixTraniner,\n", "    FTTransWithMaskTokenTraniner,\n\t    FTTransWithMixupTraniner,\n\t    FTTransWithSCARFTraniner,\n\t)\n"]}
{"filename": "trainer/utils.py", "chunked_list": ["import json\n\timport os\n\timport random\n\tfrom copy import deepcopy\n\tfrom typing import Dict, Union\n\timport numpy as np\n\timport torch\n\tdef fix_seed(seed=42):\n\t    # Python random\n\t    random.seed(seed)\n", "    # Numpy\n\t    np.random.seed(seed)\n\t    # Pytorch\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    torch.backends.cudnn.deterministic = True\n\t    torch.use_deterministic_algorithms = True\n\tdef auto_batch_size(train_size: int):\n\t    if train_size > 50000:\n\t        return 1024\n", "    elif 50000 >= train_size > 10000:\n\t        return 512\n\t    elif 10000 >= train_size > 5000:\n\t        return 256\n\t    elif 5000 >= train_size > 1000:\n\t        return 128\n\t    else:\n\t        return 64\n\tdef save_json(data: Dict[str, Union[int, float, str]], save_dir: str):\n\t    with open(os.path.join(save_dir, \"results.json\"), mode=\"wt\", encoding=\"utf-8\") as f:\n", "        json.dump(data, f, ensure_ascii=False, indent=2)\n\tdef load_json(path) -> Dict[str, Union[int, float, str]]:\n\t    with open(path, mode=\"rt\", encoding=\"utf-8\") as f:\n\t        data = json.load(f)\n\t    return data\n\t# Copied from https://github.com/Bjarten/early-stopping-pytorch\n\tclass EarlyStopping:\n\t    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n\t    def __init__(self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\", trace_func=print):\n\t        \"\"\"\n", "        Args:\n\t            patience (int): How long to wait after last time validation loss improved.\n\t                            Default: 7\n\t            verbose (bool): If True, prints a message for each validation loss improvement.\n\t                            Default: False\n\t            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n\t                            Default: 0\n\t            path (str): Path for the checkpoint to be saved to.\n\t                            Default: 'checkpoint.pt'\n\t            trace_func (function): trace print function.\n", "                            Default: print\n\t        \"\"\"\n\t        self.patience = patience\n\t        self.verbose = verbose\n\t        self.counter = 0\n\t        self.best_score = None\n\t        self.best_model = None\n\t        self.early_stop = False\n\t        self.val_loss_min = np.Inf\n\t        self.delta = delta\n", "        self.path = path\n\t        self.trace_func = trace_func\n\t    def __call__(self, val_loss, model):\n\t        score = -val_loss\n\t        if self.best_score is None:\n\t            self.best_score = score\n\t            self.save_checkpoint(val_loss, model)\n\t        elif score < self.best_score + self.delta and self.patience is None:\n\t            return\n\t        elif score < self.best_score + self.delta:\n", "            self.counter += 1\n\t            self.trace_func(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n\t            if self.counter >= self.patience:\n\t                self.early_stop = True\n\t        else:\n\t            self.best_score = score\n\t            self.save_checkpoint(val_loss, model)\n\t            self.counter = 0\n\t    def save_checkpoint(self, val_loss, model):\n\t        \"\"\"Saves model when validation loss decrease.\"\"\"\n", "        if self.verbose:\n\t            self.trace_func(\n\t                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n\t            )\n\t        # torch.save(model.state_dict(), self.path)\n\t        self.best_model = deepcopy(model.state_dict())\n\t        self.val_loss_min = val_loss\n\t    def reset(self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\", trace_func=print):\n\t        self.patience = patience\n\t        self.verbose = verbose\n", "        self.counter = 0\n\t        self.best_score = None\n\t        self.early_stop = False\n\t        self.val_loss_min = np.Inf\n\t        self.delta = delta\n\t        self.path = path\n\t        self.trace_func = trace_func\n"]}
{"filename": "trainer/supervised/fttrans_w_hidden_mix.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\timport numpy as np\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import Tensor\n\tfrom .base_da import BaseDATrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransWithHiddenMixTraniner(BaseDATrainer):\n\t    def __init__(self, alpha: float = 0.5, label_mix: bool = True, **kwargs) -> None:\n", "        \"\"\"\n\t        Args:\n\t            alpha (float): the parameter used in the hidden-mix augmentation\n\t            label_mix (bool): whether to apply label mixing\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        logger.info(f\"Set hidden mix alpha to {alpha}.\")\n\t        self.alpha = alpha\n\t        self.label_mix = label_mix\n\t    def get_lambda(self) -> float:\n", "        \"\"\"Return lambda\"\"\"\n\t        if self.alpha > 0.0:\n\t            lam = np.random.beta(self.alpha, self.alpha)\n\t        else:\n\t            lam = 1.0\n\t        torch.tensor([lam]).float().to(self.device)\n\t        return lam\n\t    def forward_w_da(\n\t        self,\n\t        cont: Optional[Tensor] = None,\n", "        cate: Optional[Tensor] = None,\n\t        target: Tensor = None,\n\t    ) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n", "            target (tensor): the target labels of shape (b, num_classes)\n\t        \"\"\"\n\t        lam = self.get_lambda()\n\t        if self.datamodule.d_out > 1:\n\t            target = F.one_hot(target, self.datamodule.d_out)\n\t        if self.label_mix:\n\t            out, target = self.model(cont, cate, target, lam)\n\t        else:\n\t            lam = max(lam, 1 - lam)\n\t            out, _ = self.model(cont, cate, target, lam)\n", "            if self.datamodule.d_out > 1:\n\t                target = torch.argmax(target, dim=1)\n\t        return out, target\n"]}
{"filename": "trainer/supervised/fttrans_w_scarf.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\timport torch\n\tfrom torch import Tensor\n\tfrom .base_da import BaseDATrainer\n\tlogger = logging.getLogger(__name__)\n\tclass SCARFDA:\n\t    def __init__(self, df, cont_cols: list, cate_cols: list, mask_ratio: float, device: str) -> None:\n\t        \"\"\"\n\t        Args:\n", "            df (DataFrame): the original dataset\n\t            cont_cols (List): the column names of continuous values\n\t            cate_cals (List): the column names of categorica values\n\t            mask_ratio (float): the ratio of data points to be masked\n\t            device (str): the device to be used for computations (e.g., \"cpu\", \"cuda\")\n\t        \"\"\"\n\t        self.cont = torch.tensor(df[cont_cols].values, device=device)\n\t        self.cate = torch.tensor(df[cate_cols].values, device=device)\n\t        self.device = device\n\t        self.mask_ratio = mask_ratio\n", "    def scarf_augument(self, x: Tensor, all_data: Tensor) -> Tensor:\n\t        \"\"\"\n\t        Args:\n\t            x (tensor): the input of shape (b, n)\n\t            all_data (tensor): the entire dataset\n\t        Returns:\n\t            x_tilde (tensor): the output of shape (b, n) representing the masked data\n\t        \"\"\"\n\t        mask = torch.bernoulli(torch.ones(x.shape) * self.mask_ratio)\n\t        mask = mask.to(torch.float).to(self.device)\n", "        batch_size = x.shape[0]\n\t        no, dim = all_data.shape\n\t        x_bar = torch.zeros([batch_size, dim]).to(self.device)\n\t        for i in range(dim):\n\t            idx = torch.randint(0, no, (batch_size,))\n\t            x_bar[:, i] = all_data[idx, i]\n\t        x_tilde = x * (1 - mask) + x_bar * mask\n\t        return x_tilde\n\t    def __call__(self, x: Tensor, mode: str) -> Tensor:\n\t        \"\"\"\n", "        Args:\n\t            x (tensor): the input of shape (b, n)\n\t            mode (str): the mode of data should be either \"cont\" or \"cate\"\n\t        Returns:\n\t            tensor: the output of shape (b, n) representing the new data after SCARF augmentation\n\t        \"\"\"\n\t        if mode == \"cont\":\n\t            data = self.cont\n\t        elif mode == \"cate\":\n\t            data = self.cate\n", "        else:\n\t            raise ValueError(f\"unexpected values: {mode}\")\n\t        return self.scarf_augument(x, data)\n\tclass FTTransWithSCARFTraniner(BaseDATrainer):\n\t    def __init__(\n\t        self,\n\t        da_mode: str = \"scarf\",\n\t        mask_ratio: float = 0.2,\n\t        **kwargs,\n\t    ) -> None:\n", "        \"\"\"\n\t        Args:\n\t            da_mode (str): the data augmentation mode\n\t            mask_ratio (float): the ratio of data points to be masked\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        logger.info(f\"DA mode is {da_mode}.\")\n\t        self.mask_ratio = mask_ratio\n\t        self.da_mode = da_mode\n\t        self.scarf_da_train = SCARFDA(\n", "            self.datamodule.train,\n\t            self.datamodule.continuous_columns,\n\t            self.datamodule.categorical_columns,\n\t            mask_ratio,\n\t            self.device,\n\t        )\n\t        self.scarf_da_val = SCARFDA(\n\t            self.datamodule.val,\n\t            self.datamodule.continuous_columns,\n\t            self.datamodule.categorical_columns,\n", "            mask_ratio,\n\t            self.device,\n\t        )\n\t    def vime_augument(self, x: Tensor) -> Tensor:\n\t        \"\"\"\n\t        Args:\n\t            x (tensor): the input of shape (b, n)\n\t        Returns:\n\t            x_tilde (tensor): the output of shape (b, n) representing the new data after VIME augmentation\n\t        \"\"\"\n", "        mask = torch.bernoulli(torch.ones(x.shape) * self.mask_ratio)\n\t        mask = mask.to(torch.float).to(self.device)\n\t        no, dim = x.shape\n\t        x_bar = torch.zeros([no, dim]).to(self.device)\n\t        for i in range(dim):\n\t            idx = torch.randperm(no)\n\t            x_bar[:, i] = x[idx, i]\n\t        x_tilde = x * (1 - mask) + x_bar * mask\n\t        return x_tilde\n\t    def apply_data_augmentation(\n", "        self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None\n\t    ) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t        Returns:\n\t            cont (tensor): the output of shape (b, num_cont) representing the new data after SCARF augmentation\n\t            cate (tensor): the output of shape (b, num_cate) representing the new data after SCARF augmentation\n\t        \"\"\"\n", "        if self.da_mode == \"vime\":\n\t            if cont is not None:\n\t                cont = self.vime_augument(cont).to(torch.float)\n\t            if cate is not None:\n\t                cate = self.vime_augument(cate).to(torch.long)\n\t        elif self.da_mode == \"scarf\":\n\t            if self.model.train:\n\t                scarf_da = self.scarf_da_train\n\t            else:\n\t                scarf_da = self.scarf_da_val\n", "            if cont is not None:\n\t                cont = scarf_da(cont, \"cont\").to(torch.float)\n\t            if cate is not None:\n\t                cate = scarf_da(cate, \"cate\").to(torch.long)\n\t        return cont, cate\n\t    def forward_w_da(\n\t        self,\n\t        cont: Optional[Tensor] = None,\n\t        cate: Optional[Tensor] = None,\n\t        target: Optional[Tensor] = None,\n", "    ) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        \"\"\"\n", "        cont, cate = self.apply_data_augmentation(cont, cate)\n\t        out = self.model(cont, cate)\n\t        return out, target\n"]}
{"filename": "trainer/supervised/base.py", "chunked_list": ["import logging\n\tfrom statistics import mean\n\tfrom typing import Optional, Tuple\n\timport numpy as np\n\timport torch\n\tfrom sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\n\tfrom torch import Tensor\n\tfrom torch.cuda.amp import autocast\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm import tqdm\n", "from ..base import BaseTrainer\n\tlogger = logging.getLogger(__name__)\n\tclass BaseSupervisedTrainer(BaseTrainer):\n\t    def __init__(self, **kwargs) -> None:\n\t        super().__init__(**kwargs)\n\t    def forward(\n\t        self,\n\t        cont: Optional[Tensor] = None,\n\t        cate: Optional[Tensor] = None,\n\t        target: Tensor = None,\n", "    ) -> Tuple[Tensor, Optional[Tensor]]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            out (tensor): the output of shape (b, num_classes) representing the model's prediction\n\t            loss (float): the model's supervised loss\n\t        \"\"\"\n", "        raise NotImplementedError()\n\t    def train_per_epoch(self, dataloader: DataLoader, pbar_epoch: tqdm, epoch: int) -> dict:\n\t        self.model.train()\n\t        all_loss = []\n\t        if self.scheduler is not None:\n\t            self.scheduler.step()\n\t        for batch in dataloader:\n\t            pbar_epoch.update(1)\n\t            self.optimizer.zero_grad()\n\t            with autocast(enabled=self.scaler is not None):\n", "                cont, cate, target = self.apply_device(batch)\n\t                _, loss = self.forward(cont, cate, target)\n\t            if self.scaler is not None:\n\t                self.scaler.scale(loss).backward()\n\t                self.scaler.step(self.optimizer)\n\t                self.scaler.update()\n\t            else:\n\t                loss.backward()\n\t                self.optimizer.step()\n\t            all_loss.append(loss.item())\n", "            scores = {\"train/loss\": mean(all_loss)}\n\t            pbar_epoch.set_description(f\"epoch[{epoch} / {self.epochs}]\")\n\t            pbar_epoch.set_postfix(scores)\n\t        return scores\n\t    @torch.no_grad()\n\t    def eval(self, mode: str = \"test\"):\n\t        self.model.eval()\n\t        all_target = []\n\t        all_pred = []\n\t        all_loss = []\n", "        for batch in self.datamodule.dataloader(mode, self.eval_batch_size):\n\t            with autocast(enabled=self.scaler is not None):\n\t                cont, cate, target = self.apply_device(batch)\n\t                out = self.model(cont, cate)\n\t                loss = self.criterion(out, target)\n\t            all_target.append(target.cpu())\n\t            all_pred.append(out.cpu())\n\t            all_loss.append(loss.item())\n\t        all_target = torch.cat(all_target, dim=0)\n\t        all_pred = torch.cat(all_pred, dim=0)\n", "        mean_loss = mean(all_loss)\n\t        score = {f\"{mode}/loss\": mean_loss}\n\t        if self.datamodule.task == \"binary\":\n\t            label = (all_pred.numpy() > 0.5).astype(np.int)\n\t            all_pred = torch.sigmoid(all_pred.float()).numpy()\n\t            score.update(\n\t                {\n\t                    f\"{mode}/acc\": accuracy_score(all_target, label),\n\t                    f\"{mode}/auc\": roc_auc_score(all_target, all_pred),\n\t                }\n", "            )\n\t        elif self.datamodule.task == \"multiclass\":\n\t            label = all_pred.argmax(1).numpy()\n\t            score.update({f\"{mode}/acc\": accuracy_score(all_target, label)})\n\t        else:\n\t            assert self.datamodule.task == \"regression\"\n\t            score.update(\n\t                {f\"{mode}/rmse\": mean_squared_error(all_target, all_pred.numpy()) ** 0.5 * self.datamodule.y_std}\n\t            )\n\t        return score\n"]}
{"filename": "trainer/supervised/fttrans_w_mixup.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\timport numpy as np\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import Tensor\n\tfrom .base_da import BaseDATrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransWithMixupTraniner(BaseDATrainer):\n\t    def __init__(self, alpha: float = 0.1, **kwargs) -> None:\n", "        \"\"\"\n\t        Args:\n\t            alpha (float): the parameter used in the mixup augmentation\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        logger.info(f\"Set mixup alpha to {alpha}.\")\n\t        self.alpha = alpha\n\t    def get_lambda(self) -> float:\n\t        \"\"\"Return lambda\"\"\"\n\t        if self.alpha > 0.0:\n", "            lam = np.random.beta(self.alpha, self.alpha)\n\t        else:\n\t            lam = 0.0\n\t        torch.tensor([lam]).float().to(self.device)\n\t        return lam\n\t    def forward_w_da(\n\t        self,\n\t        cont: Optional[Tensor] = None,\n\t        cate: Optional[Tensor] = None,\n\t        target: Tensor = None,\n", "    ) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        \"\"\"\n", "        lam = self.get_lambda()\n\t        if self.datamodule.d_out > 1:\n\t            target = F.one_hot(target, self.datamodule.d_out)\n\t        out, target = self.model(cont, cate, target, lam)\n\t        return out, target\n"]}
{"filename": "trainer/supervised/fttrans_w_mask_token.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\tfrom torch import Tensor\n\tfrom .base_da import BaseDATrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransWithMaskTokenTraniner(BaseDATrainer):\n\t    def __init__(\n\t        self,\n\t        mask_ratio: float = 0.1,\n\t        bias_after_mask: bool = True,\n", "        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Args:\n\t            mask_ratio (float): the ratio of data points to be masked\n\t            bias_after_mask (bool): whether to add the positional embedding before or after masking\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        self.mask_ratio = mask_ratio\n\t        self.bias_after_mask = bias_after_mask\n", "    def forward_w_da(\n\t        self,\n\t        cont: Optional[Tensor] = None,\n\t        cate: Optional[Tensor] = None,\n\t        target: Optional[Tensor] = None,\n\t    ) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n", "            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        \"\"\"\n\t        out = self.model(\n\t            cont,\n\t            cate,\n\t            mask_ratio=self.mask_ratio,\n\t            bias_after_mask=self.bias_after_mask,\n", "        )\n\t        return out, target\n"]}
{"filename": "trainer/supervised/base_da.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\timport numpy as np\n\tfrom torch import Tensor\n\tfrom .base import BaseSupervisedTrainer\n\tlogger = logging.getLogger(__name__)\n\tclass BaseDATrainer(BaseSupervisedTrainer):\n\t    def __init__(self, p: float = 0.5, **kwargs) -> None:\n\t        super().__init__(**kwargs)\n\t        self.p = p\n", "    def forward_w_da(\n\t        self,\n\t        cont: Optional[Tensor] = None,\n\t        cate: Optional[Tensor] = None,\n\t        target: Optional[Tensor] = None,\n\t    ) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n", "            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    def forward(\n\t        self,\n\t        cont: Optional[Tensor] = None,\n\t        cate: Optional[Tensor] = None,\n", "        target: Tensor = None,\n\t    ) -> Tuple[Tensor, Optional[Tensor]]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            out (tensor): the output of shape (b, num_classes) representing the model's prediction\n\t            loss (float): the model's supervised loss\n", "        \"\"\"\n\t        if self.p > np.random.rand():\n\t            out, target = self.forward_w_da(cont, cate, target)\n\t        else:\n\t            out = self.model(cont, cate)\n\t        if target is not None:\n\t            loss = self.criterion(out, target)\n\t            return out, loss\n\t        else:\n\t            return out\n"]}
{"filename": "trainer/supervised/__init__.py", "chunked_list": ["from .fttrans import FTTransTraniner\n\tfrom .fttrans_w_cutmix import FTTransWithCutmixTraniner\n\tfrom .fttrans_w_hidden_mix import FTTransWithHiddenMixTraniner\n\tfrom .fttrans_w_mask_token import FTTransWithMaskTokenTraniner\n\tfrom .fttrans_w_mixup import FTTransWithMixupTraniner\n\tfrom .fttrans_w_scarf import FTTransWithSCARFTraniner\n"]}
{"filename": "trainer/supervised/fttrans.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\tfrom torch import Tensor\n\tfrom .base import BaseSupervisedTrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransTraniner(BaseSupervisedTrainer):\n\t    def __init__(self, **kwargs) -> None:\n\t        super().__init__(**kwargs)\n\t    def forward(\n\t        self,\n", "        cont: Optional[Tensor] = None,\n\t        cate: Optional[Tensor] = None,\n\t        target: Tensor = None,\n\t    ) -> Tuple[Tensor, Optional[Tensor]]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n", "            out (tensor): the output of shape (b, num_classes) representing the model's prediction\n\t            loss (tensor): the model's loss\n\t        \"\"\"\n\t        out = self.model(cont, cate)\n\t        if target is not None:\n\t            loss = self.criterion(out, target)\n\t            return out, loss\n\t        else:\n\t            return out\n"]}
{"filename": "trainer/supervised/fttrans_w_cutmix.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\timport numpy as np\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch import Tensor\n\tfrom .base_da import BaseDATrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransWithCutmixTraniner(BaseDATrainer):\n\t    def __init__(\n", "        self,\n\t        alpha: float = 0.1,\n\t        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Args:\n\t            alpha (float): the parameter used in the cutmix augmentation\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        logger.info(f\"Set mixup alpha to {alpha}.\")\n", "        self.alpha = alpha\n\t    def get_lambda(self) -> float:\n\t        \"\"\"Return lambda\"\"\"\n\t        if self.alpha > 0.0:\n\t            lam = np.random.beta(self.alpha, self.alpha)\n\t        else:\n\t            lam = 1.0\n\t        torch.tensor([lam]).float().to(self.device)\n\t        return lam\n\t    def mask_generator(self, x: Tensor, lam: float) -> Tensor:\n", "        \"\"\"\n\t        Args:\n\t            x (tensor): the input of shape (b, n)\n\t            lam (float): The ratio to mask\n\t        Returns:\n\t            mask (tensor): the binary mask of shape (b, n)\n\t        \"\"\"\n\t        b, n = x.shape\n\t        ids_noise = torch.rand(b, n, device=x.device)\n\t        ids_shuffle = torch.argsort(ids_noise, dim=1)\n", "        len_keep = int(n * lam)\n\t        ids_keep = ids_shuffle[:, :len_keep]\n\t        mask = torch.ones(b, n, device=x.device)\n\t        mask[torch.arange(b)[:, None], ids_keep] = mask[torch.arange(b)[:, None], ids_keep] * 0.0\n\t        return mask\n\t    def cutmix(self, x: Tensor, target: Tensor, lam: float) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            x (tensor): the input of shape (b, n)\n\t            lam (float): The ratio to mask\n", "        Returns:\n\t            new_x (tensor): the output of shape (b, n) representing the new data after cutmix augmentation\n\t            label (tensor): the mixed target labels of shape (b, num_classes)\n\t        \"\"\"\n\t        indices = torch.randperm(x.shape[0])\n\t        mask = self.mask_generator(x, lam)\n\t        new_x = x * (1 - mask) + x[indices] * mask\n\t        new_mask = (x == new_x).to(torch.float)\n\t        new_lam = new_mask.mean(1).unsqueeze(1)\n\t        label = target * new_lam + target[indices] * (1 - new_lam)\n", "        return new_x, label\n\t    def concat_data(self, cont: Optional[Tensor], cate: Optional[Tensor]) -> Tensor:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, n) representing the continuous values\n\t            cate (tensor): the input of shape (b, n) representing the categorical values\n\t        Returns:\n\t            x (tensor): A output of shape (b, n) representing the concatenated input values\n\t        \"\"\"\n\t        if cont is not None and cate is not None:\n", "            x = torch.cat([cont, cate], dim=1)\n\t        elif cate is None:\n\t            x = cont\n\t        else:\n\t            x = cate\n\t        return x\n\t    def cutmix_process(\n\t        self, cont: Optional[Tensor], cate: Optional[Tensor], target: Tensor\n\t    ) -> Tuple[Tensor, Tensor, Tensor]:\n\t        \"\"\"\n", "        Args:\n\t            cont (Tensor): The input tensor of shape (b, num_cont) representing the continuous values.\n\t            cate (Tensor): The input tensor of shape (b, num_cate) representing the categorical values.\n\t            target (Tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            cont (Tensor): the augmented continuous values\n\t            cate (Tensor): the augmented categorical values\n\t            target (Tensor): the mixed target labels of shape (b, num_classes)\n\t        \"\"\"\n\t        lam = self.get_lambda()\n", "        if self.datamodule.d_out > 1:\n\t            target = F.one_hot(target, self.datamodule.d_out)\n\t        x = self.concat_data(cont, cate)\n\t        if cont is not None:\n\t            n_cont = cont.shape[1]\n\t        else:\n\t            n_cont = 0\n\t        x_new, target = self.cutmix(x, target, lam)\n\t        if cont is not None:\n\t            cont = x_new[:, :n_cont].to(torch.float)\n", "        if cate is not None:\n\t            cate = x_new[:, n_cont:].to(torch.long)\n\t        return cont, cate, target\n\t    def forward_w_da(\n\t        self,\n\t        cont: Optional[Tensor] = None,\n\t        cate: Optional[Tensor] = None,\n\t        target: Tensor = None,\n\t    ) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n", "        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        Returns:\n\t            out (tensor): the output of shape (b, num_classes) representing the model's prediction with data augmentation\n\t            target (tensor): the target labels of shape (b, num_classes)\n\t        \"\"\"\n\t        cont, cate, target = self.cutmix_process(cont, cate, target)\n\t        out = self.model(cont, cate)\n", "        return out, target\n"]}
{"filename": "trainer/self_supervised/fttrans_w_hidden_mix.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\timport numpy as np\n\timport torch\n\tfrom torch import Tensor\n\tfrom .base import BaseSSLTrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransHiddenMixSSLTrainer(BaseSSLTrainer):\n\t    def __init__(\n\t        self,\n", "        alpha: float = 0.5,\n\t        label_mix=None,\n\t        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Args:\n\t            alpha (float): the parameter used in the hidden-mix augmentation\n\t            label_mix (bool): whether to apply label mixing\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n", "        self.alpha = alpha\n\t    def get_lambda(self) -> float:\n\t        \"\"\"Return lambda\"\"\"\n\t        if self.alpha > 0.0:\n\t            lam = np.random.beta(self.alpha, self.alpha)\n\t        else:\n\t            lam = 1.0\n\t        torch.tensor([lam]).float().to(self.device)\n\t        return lam\n\t    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n", "        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t        Returns:\n\t            loss (float): the model's contrastive loss\n\t        \"\"\"\n\t        z_0 = self.model(cont, cate)\n\t        lam = self.get_lambda()\n\t        lam = max(lam, 1 - lam)\n", "        z_1, _ = self.model(cont, cate, lam=lam)\n\t        loss = self.forward_loss(z_0, z_1)\n\t        return None, loss\n"]}
{"filename": "trainer/self_supervised/fttrans_w_scarf.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\timport torch\n\tfrom torch import Tensor\n\tfrom ..supervised.fttrans_w_scarf import SCARFDA\n\tfrom .base import BaseSSLTrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransSCARFSSLTrainer(BaseSSLTrainer):\n\t    def __init__(\n\t        self,\n", "        da_mode: str = \"scarf\",\n\t        mask_ratio: float = 0.2,\n\t        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Args:\n\t            da_mode (str): the data augmentation mode\n\t            mask_ratio (float): the ratio of data points to be masked\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n", "        logger.info(f\"DA mode is {da_mode}.\")\n\t        self.mask_ratio = mask_ratio\n\t        self.da_mode = da_mode\n\t        self.scarf_da_train = SCARFDA(\n\t            self.datamodule.train,\n\t            self.datamodule.continuous_columns,\n\t            self.datamodule.categorical_columns,\n\t            mask_ratio,\n\t            self.device,\n\t        )\n", "        self.scarf_da_val = SCARFDA(\n\t            self.datamodule.val,\n\t            self.datamodule.continuous_columns,\n\t            self.datamodule.categorical_columns,\n\t            mask_ratio,\n\t            self.device,\n\t        )\n\t    def vime_augument(self, x) -> Tensor:\n\t        \"\"\"\n\t        Args:\n", "            x (tensor): the input of shape (b, n)\n\t        Returns:\n\t            x_tilde (tensor): the output of shape (b, n) representing the new data after VIME augmentation\n\t        \"\"\"\n\t        mask = torch.bernoulli(torch.ones(x.shape) * self.mask_ratio)\n\t        mask = mask.to(torch.float).to(self.device)\n\t        no, dim = x.shape\n\t        x_bar = torch.zeros([no, dim]).to(self.device)\n\t        for i in range(dim):\n\t            idx = torch.randperm(no)\n", "            x_bar[:, i] = x[idx, i]\n\t        x_tilde = x * (1 - mask) + x_bar * mask\n\t        return x_tilde\n\t    def apply_data_augmentation(\n\t        self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None\n\t    ) -> Tuple[Tensor, Tensor]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n", "        Returns:\n\t            cont (tensor): the output of shape (b, num_cont) representing the new data after SCARF augmentation\n\t            cate (tensor): the output of shape (b, num_cate) representing the new data after SCARF augmentation\n\t        \"\"\"\n\t        if self.da_mode == \"vime\":\n\t            if cont is not None:\n\t                cont = self.vime_augument(cont).to(torch.float)\n\t            if cate is not None:\n\t                cate = self.vime_augument(cate).to(torch.long)\n\t        elif self.da_mode == \"scarf\":\n", "            if self.model.train:\n\t                scarf_da = self.scarf_da_train\n\t            else:\n\t                scarf_da = self.scarf_da_val\n\t            if cont is not None:\n\t                cont = scarf_da(cont, \"cont\").to(torch.float)\n\t            if cate is not None:\n\t                cate = scarf_da(cate, \"cate\").to(torch.long)\n\t        return cont, cate\n\t    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n", "        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t        Returns:\n\t            loss (float): the model's contrastive loss\n\t        \"\"\"\n\t        cont_da, cate_da = self.apply_data_augmentation(cont, cate)\n\t        z_0 = self.model(cont, cate)\n\t        z_1 = self.model(cont_da, cate_da)\n", "        loss = self.forward_loss(z_0, z_1)\n\t        return None, loss\n"]}
{"filename": "trainer/self_supervised/base.py", "chunked_list": ["from statistics import mean\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch import Tensor\n\tfrom torch.cuda.amp import autocast\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm import tqdm\n\tfrom model.core.fttrans import ProjectionHead\n\tfrom ..base import BaseTrainer\n", "# Copied from https://github.com/clabrugere/pytorch-scarf/\n\tclass NTXent(nn.Module):\n\t    def __init__(self, temperature=1.0):\n\t        \"\"\"NT-Xent loss for contrastive learning using cosine distance as similarity metric as used in [SimCLR](https://arxiv.org/abs/2002.05709).\n\t        Implementation adapted from https://theaisummer.com/simclr/#simclr-loss-implementation\n\t        Args:\n\t            temperature (float, optional): scaling factor of the similarity metric. Defaults to 1.0.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.temperature = temperature\n", "    def forward(self, z_i: Tensor, z_j: Tensor):\n\t        \"\"\"Compute NT-Xent loss using only anchor and positive batches of samples. Negative samples are the 2*(N-1) samples in the batch\n\t        Args:\n\t            z_i (torch.tensor): anchor batch of samples\n\t            z_j (torch.tensor): positive batch of samples\n\t        Returns:\n\t            float: loss\n\t        \"\"\"\n\t        batch_size = z_i.size(0)\n\t        # compute similarity between the sample's embedding and its corrupted view\n", "        z = torch.cat([z_i, z_j], dim=0)\n\t        similarity = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2)\n\t        sim_ij = torch.diag(similarity, batch_size)\n\t        sim_ji = torch.diag(similarity, -batch_size)\n\t        positives = torch.cat([sim_ij, sim_ji], dim=0)\n\t        mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=torch.bool, device=z_i.device)).float()\n\t        numerator = torch.exp(positives / self.temperature)\n\t        denominator = mask * torch.exp(similarity / self.temperature)\n\t        all_losses = -torch.log(numerator / torch.sum(denominator, dim=1))\n\t        loss = torch.sum(all_losses) / (2 * batch_size)\n", "        return loss\n\tclass BaseSSLTrainer(BaseTrainer):\n\t    def __init__(self, **kwargs) -> None:\n\t        super().__init__(**kwargs, tensorbord_dir=\"./self_supervised\")\n\t        new_head = ProjectionHead(self.model.head.linear.in_features)\n\t        self.model.head = new_head.to(self.device)\n\t        self.ssl_loss = NTXent()\n\t    def train_per_epoch(self, dataloader: DataLoader, pbar_epoch: tqdm, epoch: int) -> dict:\n\t        self.model.train()\n\t        all_loss = []\n", "        if self.scheduler is not None:\n\t            self.scheduler.step()\n\t        for batch in dataloader:\n\t            pbar_epoch.update(1)\n\t            self.optimizer.zero_grad()\n\t            with autocast(enabled=self.scaler is not None):\n\t                cont, cate, _ = self.apply_device(batch)\n\t                _, loss = self.forward(cont, cate)\n\t            if self.scaler is not None:\n\t                self.scaler.scale(loss).backward()\n", "                self.scaler.step(self.optimizer)\n\t                self.scaler.update()\n\t            else:\n\t                loss.backward()\n\t                self.optimizer.step()\n\t            all_loss.append(loss.item())\n\t            scores = {\"train/self-sl-loss\": mean(all_loss)}\n\t            pbar_epoch.set_description(f\"epoch[{epoch} / {self.epochs}]\")\n\t            pbar_epoch.set_postfix(scores)\n\t        return scores\n", "    def forward_loss(self, z_i, z_j) -> float:\n\t        return self.ssl_loss(z_i, z_j)\n\t    @torch.no_grad()\n\t    def eval(self, mode: str = \"val\") -> dict:\n\t        self.model.eval()\n\t        all_loss = []\n\t        for batch in self.datamodule.dataloader(mode, self.eval_batch_size):\n\t            with autocast(enabled=self.scaler is not None):\n\t                cont, cate, _ = self.apply_device(batch)\n\t                _, loss = self.forward(cont, cate)\n", "            all_loss.append(loss.item())\n\t        mean_loss = mean(all_loss)\n\t        score = {f\"{mode}/self-sl-loss\": mean_loss}\n\t        return score\n"]}
{"filename": "trainer/self_supervised/fttrans_w_mixup.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\timport numpy as np\n\timport torch\n\tfrom torch import Tensor\n\tfrom .base import BaseSSLTrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransMixupSSLTrainer(BaseSSLTrainer):\n\t    def __init__(\n\t        self,\n", "        alpha: float = 0.1,\n\t        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Args:\n\t            alpha (float): the parameter used in the mixup augmentation\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        logger.info(f\"Set mixup alpha to {alpha}.\")\n\t        self.alpha = alpha\n", "    def get_lambda(self) -> float:\n\t        \"\"\"Return lambda\"\"\"\n\t        if self.alpha > 0.0:\n\t            lam = np.random.beta(self.alpha, self.alpha)\n\t        else:\n\t            lam = 0.0\n\t        torch.tensor([lam]).float().to(self.device)\n\t        return lam\n\t    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n\t        \"\"\"\n", "        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t        Returns:\n\t            loss (float): the model's contrastive loss\n\t        \"\"\"\n\t        z_0 = self.model(cont, cate)\n\t        lam = self.get_lambda()\n\t        lam = min(lam, 1 - lam)\n\t        z_1 = self.model.forward_no_labelmix(cont, cate, alpha=lam)\n", "        loss = self.forward_loss(z_0, z_1)\n\t        return None, loss\n"]}
{"filename": "trainer/self_supervised/fttrans_w_mask_token.py", "chunked_list": ["import logging\n\tfrom typing import Optional, Tuple\n\tfrom torch import Tensor\n\tfrom .base import BaseSSLTrainer\n\tlogger = logging.getLogger(__name__)\n\tclass FTTransMaskTokenSSLTrainer(BaseSSLTrainer):\n\t    def __init__(\n\t        self,\n\t        mask_ratio: float = 0.1,\n\t        bias_after_mask: bool = True,\n", "        **kwargs,\n\t    ) -> None:\n\t        \"\"\"\n\t        Args:\n\t            mask_ratio (float): the ratio of data points to be masked\n\t            bias_after_mask (bool): whether to add the positional embedding before or after masking\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        self.mask_ratio = mask_ratio\n\t        self.bias_after_mask = bias_after_mask\n", "    def forward(self, cont: Optional[Tensor] = None, cate: Optional[Tensor] = None) -> Tuple[None, float]:\n\t        \"\"\"\n\t        Args:\n\t            cont (tensor): the input of shape (b, num_cont) representing the continuous values\n\t            cate (tensor): the input of shape (b, num_cate) representing the categorical values\n\t        Returns:\n\t            loss (float): the model's contrastive loss\n\t        \"\"\"\n\t        z_0 = self.model(cont, cate)\n\t        z_1 = self.model(\n", "            cont,\n\t            cate,\n\t            mask_ratio=self.mask_ratio,\n\t            bias_after_mask=self.bias_after_mask,\n\t        )\n\t        loss = self.forward_loss(z_0, z_1)\n\t        return None, loss\n"]}
{"filename": "trainer/self_supervised/__init__.py", "chunked_list": ["from .fttrans_w_hidden_mix import FTTransHiddenMixSSLTrainer\n\tfrom .fttrans_w_mask_token import FTTransMaskTokenSSLTrainer\n\tfrom .fttrans_w_mixup import FTTransMixupSSLTrainer\n\tfrom .fttrans_w_scarf import FTTransSCARFSSLTrainer\n"]}
