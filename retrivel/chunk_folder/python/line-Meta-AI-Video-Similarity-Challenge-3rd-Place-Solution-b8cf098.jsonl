{"filename": "editing_prediction/video_generation_AugLy.py", "chunked_list": ["import argparse\n\timport csv\n\timport glob\n\timport json\n\timport os\n\timport random\n\timport tempfile\n\tfrom pathlib import Path\n\timport decord\n\timport numpy as np\n", "import pandas as pd\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision\n\timport torchvision.transforms as T\n\tfrom augly.video import (\n\t    BlendVideos,\n\t    Brightness,\n\t    ColorJitter,\n\t    Contrast,\n", "    Crop,\n\t    Grayscale,\n\t    HFlip,\n\t    HStack,\n\t    InsertInBackground,\n\t    Loop,\n\t    Overlay,\n\t    OverlayDots,\n\t    OverlayEmoji,\n\t    OverlayShapes,\n", "    OverlayText,\n\t    Pad,\n\t    PerspectiveTransformAndShake,\n\t    RandomAspectRatio,\n\t    RandomBlur,\n\t    RandomEncodingQuality,\n\t    RandomNoise,\n\t    RandomPixelization,\n\t    RandomVideoSpeed,\n\t    Rotate,\n", "    VFlip,\n\t    VStack,\n\t)\n\tdef select_emoji_path():\n\t    emoji_dir_all = glob.glob(\"AugLy/augly/assets/twemojis/*\")\n\t    emoji_dir = random.choice(emoji_dir_all)\n\t    emoji_pic_all = glob.glob(f\"{emoji_dir}/*.png\")\n\t    emoji_pic = random.choice(emoji_pic_all)\n\t    return emoji_pic\n\tdef decord_clip_reader(path: Path, start: float, duration: float, count: int):\n", "    num_threads = 0\n\t    ctx = decord.cpu(0)\n\t    reader = decord.VideoReader(\n\t        path.as_posix(),\n\t        ctx=ctx,\n\t        num_threads=num_threads,\n\t    )\n\t    n_frames = len(reader)\n\t    timestamps = reader.get_frame_timestamp(np.arange(n_frames))\n\t    frame_pos = np.linspace(start, start + duration, count)\n", "    frame_ids = np.searchsorted(timestamps[:, 0], frame_pos)\n\t    frame_ids = np.minimum(frame_ids, n_frames - 1)\n\t    frames = reader.get_batch(frame_ids).asnumpy()\n\t    tensor = torch.as_tensor(frames)\n\t    tensor = torch.permute(tensor, (0, 3, 1, 2))\n\t    return tensor\n\tdef main(args):\n\t    gt_match_csv = f\"{args.output}/cvl_train_matching_ground_truth.csv\"\n\t    edit_query_meta_csv = f\"{args.output}/cvl_train_query_metadata.csv\"\n\t    if not Path(gt_match_csv).is_file():\n", "        os.makedirs(f\"{args.output}/query\", exist_ok=True)\n\t        os.makedirs(f\"{args.output}/cvl_train_augly_metadata\", exist_ok=True)\n\t        with open(gt_match_csv, \"a\") as f_gt:\n\t            writer_gt = csv.writer(f_gt, delimiter=\",\")\n\t            writer_gt.writerow(\n\t                [\n\t                    \"query_id\",\n\t                    \"ref_id\",\n\t                    \"query_start\",\n\t                    \"query_end\",\n", "                    \"ref_start\",\n\t                    \"ref_end\",\n\t                ]\n\t            )\n\t        with open(edit_query_meta_csv, \"a\") as f_meta:\n\t            writer_meta = csv.writer(f_meta, delimiter=\",\")\n\t            writer_meta.writerow(\n\t                [\n\t                    \"video_id\",\n\t                    \"duration_sec\",\n", "                    \"frames_per_sec\",\n\t                    \"width\",\n\t                    \"height\",\n\t                    \"rn\",\n\t                    \"base_query_id\",\n\t                ]\n\t            )\n\t    ref_meta = pd.read_csv(args.meta_path)\n\t    # for i in range(0, 1):\n\t    for i in range(args.sample_range[0], args.sample_range[1]):\n", "        ref_id = random.choice(list(Path(args.data_path).glob(\"*.mp4\")))\n\t        r_meta = ref_meta[ref_meta[\"video_id\"] == ref_id.stem]\n\t        # select r_start, r_end from r_meta (>5s, mean 12 s, Max: 54, Min: 2.4)\n\t        r_dura_org = float(r_meta[\"duration_sec\"])\n\t        if r_dura_org > 5:\n\t            r_dura_copy = random.uniform(4, min(r_dura_org - 1, 60))\n\t            r_start = random.uniform(0, r_dura_org - r_dura_copy)\n\t            r_end = r_start + r_dura_copy\n\t            # extract r_start, r_end from ref video -> decord_clip_reader\n\t            count = int(r_meta[\"frames_per_sec\"] * r_dura_copy)\n", "            copy_ref = decord_clip_reader(\n\t                path=ref_id, start=r_start, duration=r_dura_copy, count=count\n\t            )\n\t        else:\n\t            continue\n\t        query_id = random.choice(list(Path(args.data_path).glob(\"*.mp4\")))\n\t        q_meta = ref_meta[ref_meta[\"video_id\"] == query_id.stem]\n\t        # extract 0~end from query video\n\t        q_dura_org = float(q_meta[\"duration_sec\"])\n\t        count = int(q_meta[\"frames_per_sec\"] * q_dura_org)\n", "        query_all = decord_clip_reader(\n\t            path=query_id, start=0.0, duration=q_dura_org, count=count\n\t        )\n\t        # resize to fit query video\n\t        if copy_ref.shape[2:] != query_all.shape[2:]:\n\t            transform = T.Resize((query_all.shape[2], query_all.shape[3]))\n\t            copy_ref = transform(copy_ref)\n\t        copy_ref = torch.permute(copy_ref, (0, 2, 3, 1))\n\t        query_all = torch.permute(query_all, (0, 2, 3, 1))\n\t        # select q_start, q_end from q_meta\n", "        q_insert_frame = random.choice(range(0, query_all.shape[0]))\n\t        # insert\n\t        frameNo = query_all.shape[0] + copy_ref.shape[0]\n\t        copy_video = torch.zeros(frameNo, query_all.shape[1], query_all.shape[2], 3)\n\t        copy_video[:q_insert_frame, :, :, :] = query_all[:q_insert_frame, :, :, :]\n\t        q_insert_end = q_insert_frame + copy_ref.shape[0]\n\t        copy_video[q_insert_end:, :, :, :] = query_all[q_insert_frame:, :, :, :]\n\t        copy_video[q_insert_frame:q_insert_end, :, :, :] = copy_ref\n\t        video_in2 = random.choice(list(Path(args.data_path).glob(\"*.mp4\")))\n\t        emoji_path = select_emoji_path()\n", "        brightness_level = random.uniform(-0.6, 0.6)\n\t        contrast_level = random.uniform(-2.0, 2.0)\n\t        saturation_factor = random.uniform(0.0, 3.0)\n\t        rotate_degrees = random.choice([i for i in range(-30, 30, 5)])\n\t        # rotate_degrees = random.choice([-90, 90])\n\t        crop_left = random.uniform(0, 0.3)\n\t        crop_top = random.uniform(0, 0.3)\n\t        crop_right = random.uniform(0.7, 1.0)\n\t        crop_bottom = random.uniform(0.7, 1.0)\n\t        pad_w_factor = random.uniform(0, 0.25)\n", "        pad_h_factor = random.uniform(0, 0.25)\n\t        pad_color_r = random.randint(0, 255)\n\t        pad_color_g = random.randint(0, 255)\n\t        pad_color_b = random.randint(0, 255)\n\t        overlay_num_dots = random.randint(50, 250)\n\t        overlay_dot_type = random.choice([\"colored\", \"blur\"])\n\t        overlay_random_movement = random.choice([True, False])\n\t        overlay_num_shapes = random.randint(1, 3)\n\t        overlay_text_len = random.randint(5, 15)\n\t        shake_sigma = random.uniform(10, 50)\n", "        shake_radius = random.uniform(0, 1.0)\n\t        emoji_x_factor = random.uniform(0.1, 0.6)\n\t        emoji_y_factor = random.uniform(0.1, 0.6)\n\t        emoji_opacity = random.uniform(0.5, 1.0)\n\t        emoji_size = random.uniform(0.2, 0.6)\n\t        opacity = random.uniform(0.1, 0.5)\n\t        overlay_size = random.choice([1.0, 1.0, 1.0, 0.8, 0.9])\n\t        overlay_xy = [[0.2, 0], [0.3, 0], [0.4, 0], [0.5, 0]]\n\t        overlay_xy_i = random.randint(0, len(overlay_xy) - 1)\n\t        overlay_x_factor = overlay_xy[overlay_xy_i][0]\n", "        overlay_y_factor = overlay_xy[overlay_xy_i][1]\n\t        augly_method = [\n\t            Brightness(level=brightness_level),\n\t            Contrast(level=contrast_level),\n\t            ColorJitter(saturation_factor=2.5),\n\t            Grayscale(),\n\t            RandomBlur(min_sigma=4.0, max_sigma=12.0),\n\t            RandomPixelization(min_ratio=0.1, max_ratio=0.7),\n\t            RandomAspectRatio(min_ratio=0.5, max_ratio=2.0),\n\t            RandomVideoSpeed(min_factor=0.2, max_factor=5.0),\n", "            Rotate(degrees=rotate_degrees),\n\t            Crop(left=crop_left, top=crop_top, right=crop_right, bottom=crop_bottom),\n\t            Pad(\n\t                w_factor=pad_w_factor,\n\t                h_factor=pad_w_factor,\n\t                color=(pad_color_r, pad_color_g, pad_color_b),\n\t            ),\n\t            RandomNoise(min_level=15, max_level=60),\n\t            RandomEncodingQuality(min_quality=10, max_quality=45),\n\t            HFlip(),\n", "            VFlip(),\n\t            Loop(),\n\t            OverlayDots(\n\t                num_dots=overlay_num_dots,\n\t                dot_type=overlay_dot_type,\n\t                random_movement=overlay_random_movement,\n\t            ),\n\t            OverlayShapes(num_shapes=overlay_num_shapes),\n\t            OverlayText(text_len=overlay_text_len),\n\t            PerspectiveTransformAndShake(sigma=shake_sigma, shake_radius=shake_radius),\n", "            OverlayEmoji(\n\t                emoji_path=emoji_path,\n\t                x_factor=emoji_x_factor,\n\t                y_factor=emoji_y_factor,\n\t                opacity=emoji_opacity,\n\t                emoji_size=emoji_size,\n\t            ),\n\t            BlendVideos(str(video_in2), opacity=opacity, overlay_size=overlay_size),\n\t            HStack(str(video_in2)),\n\t            VStack(str(video_in2)),\n", "            Overlay(\n\t                str(video_in2), x_factor=overlay_x_factor, y_factor=overlay_y_factor\n\t            ),\n\t            InsertInBackground(str(video_in2), offset_factor=0.1),\n\t        ]\n\t        edit_method = random.choice(augly_method)\n\t        # save video\n\t        meta_list = []\n\t        out_vid_path = f\"{args.output}/query/Q4{str(i).zfill(5)}.mp4\"\n\t        out_meta_path = (\n", "            f\"{args.output}/cvl_train_augly_metadata/Q4{str(i).zfill(5)}.json\"\n\t        )\n\t        with tempfile.TemporaryDirectory() as dir:\n\t            copy_vid_path = f\"{dir}/copy_video.mp4\"\n\t            torchvision.io.write_video(\n\t                filename=copy_vid_path,\n\t                video_array=copy_video,\n\t                fps=float(q_meta[\"frames_per_sec\"]),\n\t            )\n\t            edit_method(copy_vid_path, out_vid_path, metadata=meta_list)\n", "        factor = 1\n\t        if meta_list[0][\"name\"] == \"change_video_speed\":\n\t            factor = meta_list[0][\"factor\"]\n\t        with open(gt_match_csv, \"a\") as f_gt:\n\t            writer_gt = csv.writer(f_gt, delimiter=\",\")\n\t            q_start_sec = q_insert_frame / float(q_meta[\"frames_per_sec\"]) / factor\n\t            q_end_sec = (\n\t                q_start_sec\n\t                + copy_ref.shape[0] / float(q_meta[\"frames_per_sec\"]) / factor\n\t            )\n", "            row = [\n\t                f\"Q4{str(i).zfill(5)}\",\n\t                ref_id.stem,\n\t                q_start_sec,\n\t                q_end_sec,\n\t                r_start,\n\t                r_end,\n\t            ]\n\t            writer_gt.writerow(row)\n\t        with open(edit_query_meta_csv, \"a\") as f_meta:\n", "            writer_meta = csv.writer(f_meta, delimiter=\",\")\n\t            duration_sec = frameNo / float(q_meta[\"frames_per_sec\"]) / factor\n\t            row = [\n\t                f\"Q4{str(i).zfill(5)}\",\n\t                duration_sec,\n\t                float(q_meta[\"frames_per_sec\"]),\n\t                int(q_meta[\"width\"]),\n\t                int(q_meta[\"height\"]),\n\t                i,\n\t                query_id.stem,\n", "            ]\n\t            writer_meta.writerow(row)\n\t        with open(out_meta_path, \"w\") as f:\n\t            json.dump(meta_list[0], f)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--data_path\", type=str, default=\"competition_data/train/reference/\"\n\t    )\n\t    parser.add_argument(\n", "        \"--meta_path\",\n\t        type=str,\n\t        default=\"competition_data/train/train_reference_metadata.csv\",\n\t    )\n\t    parser.add_argument(\"--output\", type=str, default=\"gen_data/train_reference\")\n\t    parser.add_argument(\"--sample_range\", nargs=\"*\", type=int, required=True)\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "editing_prediction/training/train.py", "chunked_list": ["import argparse\n\timport shutil\n\tfrom pathlib import Path\n\timport pandas as pd\n\timport pytorch_lightning as pl\n\timport torch\n\timport torch.backends.cudnn as cudnn\n\timport torch.nn as nn\n\tfrom pytorch_lightning.callbacks import ModelCheckpoint\n\tfrom torchvision import transforms\n", "from utils.configs import timm_model_config\n\tfrom utils.dataset import VscDataModule\n\tfrom utils.model import LitModel\n\tdef train(args: argparse.Namespace):\n\t    cudnn.benchmark = True\n\t    pl.seed_everything(args.seed, workers=True)\n\t    labels = (\n\t        pd.read_csv(args.dataset_dir_path / \"label.csv\")\n\t        .sort_values(\"label_idx\")\n\t        .drop_duplicates()\n", "        .sort_values(\"label_idx\")[\"label\"]\n\t        .tolist()\n\t    )\n\t    mean, std, input_size = timm_model_config(args.model_name)\n\t    train_transform = nn.Sequential(\n\t        transforms.ConvertImageDtype(torch.float32),\n\t        transforms.Normalize(mean=mean, std=std),\n\t        transforms.Resize(size=input_size),\n\t    )\n\t    valid_transform = nn.Sequential(\n", "        transforms.ConvertImageDtype(torch.float32),\n\t        transforms.Normalize(mean=mean, std=std),\n\t        transforms.Resize(size=input_size),\n\t    )\n\t    data_module = VscDataModule(\n\t        dataset_dir_path=args.dataset_dir_path,\n\t        labels=labels,\n\t        frames_per_video=args.frames_per_video,\n\t        train_transform=train_transform,\n\t        valid_transform=valid_transform,\n", "        batch_size=args.batch_size,\n\t        num_workers=args.num_workers,\n\t        jit=False,\n\t    )\n\t    model = LitModel(\n\t        model_name=args.model_name,\n\t        labels=labels,\n\t    )\n\t    checkpoint_callback = ModelCheckpoint(\n\t        dirpath=Path(__file__).parent / \"training_log\",\n", "        monitor=\"valid_loss\",\n\t        save_last=True,\n\t        save_top_k=1,\n\t    )\n\t    trainer = pl.Trainer(\n\t        strategy=args.ddp_strategy,\n\t        accelerator=args.accelerator,\n\t        devices=args.devices,\n\t        max_epochs=args.epochs,\n\t        num_sanity_val_steps=-1,\n", "        callbacks=[checkpoint_callback],\n\t    )\n\t    trainer.fit(model, datamodule=data_module)\n\t    shutil.copy(\n\t        checkpoint_callback.best_model_path,\n\t        \"model.ckpt\",\n\t    )\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--dataset-dir-path\", required=True, type=Path)\n", "    parser.add_argument(\"--frames-per-video\", required=True, type=int)\n\t    parser.add_argument(\"--model-name\", type=str, required=True)\n\t    parser.add_argument(\"--epochs\", type=int, required=True)\n\t    parser.add_argument(\"--batch-size\", type=int, required=True)\n\t    parser.add_argument(\"--seed\", default=sum(ord(x) for x in \"vsc\"))\n\t    parser.add_argument(\"--devices\", nargs=\"+\", type=int)\n\t    parser.add_argument(\"--num-workers\", type=int, default=0)\n\t    parser.add_argument(\"--accelerator\", default=\"gpu\")\n\t    parser.add_argument(\"--ddp-strategy\", default=\"ddp\")\n\t    args = parser.parse_args()\n", "    train(args)\n"]}
{"filename": "editing_prediction/training/prepare.py", "chunked_list": ["import argparse\n\timport json\n\tfrom pathlib import Path\n\timport numpy as np\n\timport pandas as pd\n\tfrom sklearn.model_selection import train_test_split\n\tAUG_LABEL_MAPPING = {\n\t    \"rotate90\": \"rotate90_270\",\n\t    \"rotate270\": \"rotate90_270\",\n\t    \"rotate180\": \"rotate180_vflip\",\n", "    \"vflip\": \"rotate180_vflip\",\n\t    \"hflip\": \"other\",\n\t    \"change_video_speed\": \"other\",\n\t}\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--video-dir-path\", required=True, type=Path)\n\t    parser.add_argument(\"--augly-metadata-path\", required=True, type=Path)\n\t    args = parser.parse_args()\n\t    seed = 1234\n", "    dataset_dir = Path(\"./dataset\")\n\t    video_dir_path = args.video_dir_path\n\t    augly_metadata_path = args.augly_metadata_path\n\t    # Gather metadata\n\t    metadata = []\n\t    for p in sorted(augly_metadata_path.glob(\"*\")):\n\t        with open(p) as f:\n\t            m = json.load(f)\n\t        video_id = p.stem.split(\"_\", 1)[0]\n\t        row = {\n", "            \"video_id\": video_id,\n\t            \"video_path\": (video_dir_path / f\"{video_id}.mp4\").as_posix(),\n\t            \"aug_type\": m.get(\"sub_name\", m[\"name\"]),\n\t        }\n\t        metadata.append(row)\n\t    metadata = pd.DataFrame(metadata)\n\t    # Assign label id\n\t    aug_types = sorted(metadata[\"aug_type\"].unique())\n\t    aug_types = pd.DataFrame({\"aug_type\": aug_types})\n\t    aug_types[\"label\"] = aug_types[\"aug_type\"].replace(AUG_LABEL_MAPPING)\n", "    labels = sorted(aug_types[\"label\"].unique())\n\t    label_idx = pd.Series(np.arange(len(labels)), index=labels, name=\"label_idx\")\n\t    aug_label_mapping = aug_types.join(label_idx, on=\"label\", how=\"left\").set_index(\n\t        \"aug_type\"\n\t    )\n\t    print(\"Labels\")\n\t    print(aug_label_mapping.to_markdown())\n\t    metadata = metadata.join(aug_label_mapping, on=\"aug_type\", how=\"left\")\n\t    # Grouping\n\t    sorted_tuple = lambda x: tuple(sorted(x))\n", "    metadata = (\n\t        metadata.drop_duplicates()\n\t        .groupby([\"video_id\", \"video_path\"])\n\t        .agg(\n\t            {\n\t                \"aug_type\": sorted_tuple,\n\t                \"label\": sorted_tuple,\n\t                \"label_idx\": sorted_tuple,\n\t            }\n\t        )\n", "        .reset_index()\n\t    )\n\t    # Random split\n\t    label_counts = (\n\t        metadata[\"label\"]\n\t        .value_counts()\n\t        .reset_index(name=\"c\")\n\t        .rename(columns={\"index\": \"label\"})\n\t    )\n\t    label_counts[\"stratify_label\"] = label_counts.apply(\n", "        lambda x: x[\"label\"] if x[\"c\"] >= 10 else (\"minor\",), axis=1\n\t    )\n\t    stratifier = metadata.merge(label_counts, on=\"label\")[\"stratify_label\"]\n\t    train, valid = train_test_split(\n\t        metadata,\n\t        test_size=0.2,\n\t        stratify=stratifier,\n\t        random_state=seed,\n\t    )\n\t    print(f\"Train datset: {train.shape}\")\n", "    print(f\"Valid datset: {valid.shape}\")\n\t    dataset_dir.mkdir(parents=True, exist_ok=True)\n\t    aug_label_mapping.to_csv(dataset_dir / \"label.csv\", index=False)\n\t    train.to_parquet(dataset_dir / \"train.parquet\", index=False)\n\t    valid.to_parquet(dataset_dir / \"valid.parquet\", index=False)\n"]}
{"filename": "editing_prediction/training/utils/model.py", "chunked_list": ["import pytorch_lightning as pl\n\timport timm\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchmetrics\n\tclass LitModel(pl.LightningModule):\n\t    def __init__(self, model_name, labels):\n\t        super().__init__()\n\t        self.labels = labels\n", "        num_classes = len(labels)\n\t        if model_name.startswith(\"hf-hub:\"):\n\t            model = timm.create_model(model_name, pretrained=True)\n\t            model.head[-1] = nn.Linear(1024, num_classes)\n\t            self.model = model\n\t        else:\n\t            self.model = timm.create_model(\n\t                model_name, num_classes=num_classes, pretrained=True\n\t            )\n\t        self.criterion = nn.BCEWithLogitsLoss()\n", "        self.valid_acc = torchmetrics.MultioutputWrapper(\n\t            torchmetrics.Accuracy(\"binary\", average=None),\n\t            num_classes,\n\t        )\n\t        self.valid_auc = torchmetrics.MultioutputWrapper(\n\t            torchmetrics.AUROC(\"binary\", average=None),\n\t            num_classes,\n\t        )\n\t        self.valid_ap = torchmetrics.MultioutputWrapper(\n\t            torchmetrics.AveragePrecision(\"binary\", average=None),\n", "            num_classes,\n\t        )\n\t    def forward(self, x):\n\t        return self.model(x)\n\t    def training_step(self, batch, batch_idx):\n\t        x, y = batch\n\t        logit = self(x)\n\t        loss = self.criterion(logit, y)\n\t        self.log(\"train_loss\", loss)\n\t        return loss\n", "    def validation_step(self, batch, batch_idx):\n\t        x, y = batch\n\t        logit = self(x)\n\t        y_hat = torch.sigmoid(logit)\n\t        loss = self.criterion(logit, y)\n\t        self.valid_acc.update(y_hat, y)\n\t        self.valid_auc.update(y_hat, y)\n\t        self.valid_ap.update(y_hat, y.long())\n\t        self.log(\"valid_loss\", loss)\n\t    def validation_epoch_end(self, outputs):\n", "        acc = self.valid_acc.compute()\n\t        auc = self.valid_auc.compute()\n\t        ap = self.valid_ap.compute()\n\t        self.log_dict(\n\t            {f\"valid_{l}_acc\": x for l, x in zip(self.labels, acc)}, sync_dist=True\n\t        )\n\t        self.log_dict(\n\t            {f\"valid_{l}_auc\": x for l, x in zip(self.labels, auc)}, sync_dist=True\n\t        )\n\t        self.log_dict(\n", "            {f\"valid_{l}_ap\": x for l, x in zip(self.labels, ap)}, sync_dist=True\n\t        )\n\t        self.valid_acc.reset()\n\t        self.valid_auc.reset()\n\t        self.valid_ap.reset()\n\t    def configure_optimizers(self):\n\t        return torch.optim.Adam(self.parameters(), lr=0.0001)\n"]}
{"filename": "editing_prediction/training/utils/configs.py", "chunked_list": ["import timm\n\tdef timm_model_config(model_name):\n\t    if model_name == \"hf-hub:timm/convnext_base.clip_laion2b_augreg_ft_in1k\":\n\t        mean = (0.48145466, 0.4578275, 0.40821073)\n\t        std = (0.26862954, 0.26130258, 0.27577711)\n\t        input_size = (256, 256)\n\t    elif model_name == \"hf-hub:timm/convnext_base.clip_laiona_augreg_ft_in1k_384\":\n\t        mean = (0.48145466, 0.4578275, 0.40821073)\n\t        std = (0.26862954, 0.26130258, 0.27577711)\n\t        input_size = (384, 384)\n", "    else:\n\t        mean = timm.get_pretrained_cfg_value(model_name, \"mean\")\n\t        std = timm.get_pretrained_cfg_value(model_name, \"std\")\n\t        input_size = timm.get_pretrained_cfg_value(model_name, \"input_size\")[1:]\n\t    return mean, std, input_size\n"]}
{"filename": "editing_prediction/training/utils/dataset.py", "chunked_list": ["import json\n\tfrom pathlib import Path\n\tfrom typing import Callable, Dict, List, Optional\n\timport decord\n\timport numpy as np\n\timport pandas as pd\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom sklearn.model_selection import train_test_split\n\tfrom torch.utils.data import DataLoader\n", "class Dataset(torch.utils.data.Dataset):\n\t    def __init__(\n\t        self,\n\t        dataset: pd.DataFrame,\n\t        n_labels: int,\n\t        transform: Optional[Callable] = None,\n\t        tensor_format: str = \"CHW\",\n\t        random_relpos_offset: float = 0.0,\n\t    ):\n\t        self.dataset = dataset\n", "        self.n_labels = n_labels\n\t        self.onehot = torch.eye(n_labels, dtype=torch.float32)\n\t        self.transform = transform\n\t        self.tensor_format = tensor_format\n\t        self.random_relpos_offset = random_relpos_offset\n\t    def __len__(self):\n\t        return len(self.dataset)\n\t    def __getitem__(self, i):\n\t        row = self.dataset.iloc[i]\n\t        num_threads = 0\n", "        ctx = decord.cpu(0)\n\t        reader = decord.VideoReader(\n\t            row[\"video_path\"],\n\t            ctx=ctx,\n\t            num_threads=num_threads,\n\t        )\n\t        n_frames = len(reader)\n\t        relpos = row[\"relative_pos\"]\n\t        if self.random_relpos_offset > 0.0:\n\t            relpos += (np.random.random() * 2 - 1) * self.random_relpos_offset\n", "        pos = int((n_frames * relpos).round())\n\t        try:\n\t            frame = reader[pos].asnumpy()\n\t        except:\n\t            print(row.to_dict())\n\t            raise\n\t        tensor = torch.as_tensor(frame)\n\t        if self.transform:\n\t            tensor = tensor.permute(2, 0, 1)\n\t            tensor = self.transform(tensor)\n", "            tensor = tensor.permute(1, 2, 0)\n\t        if self.tensor_format == \"CHW\":\n\t            tensor = tensor.permute(2, 0, 1)\n\t        label = self.onehot[row[\"label_idx\"].tolist()].max(dim=0).values\n\t        return tensor, label\n\tclass VscDataModule(pl.LightningDataModule):\n\t    def __init__(\n\t        self,\n\t        dataset_dir_path: Path,\n\t        labels: List[str],\n", "        frames_per_video: int,\n\t        batch_size: int,\n\t        train_transform: Optional[Callable] = None,\n\t        valid_transform: Optional[Callable] = None,\n\t        tensor_format: str = \"CHW\",\n\t        num_workers: int = 1,\n\t        jit: bool = True,\n\t    ):\n\t        super().__init__()\n\t        self.dataset_dir_path = dataset_dir_path\n", "        self.labels = labels\n\t        self.frames_per_video = frames_per_video\n\t        self.batch_size = batch_size\n\t        self.train_transform = train_transform\n\t        self.valid_transform = valid_transform\n\t        self.tensor_format = tensor_format\n\t        self.num_workers = num_workers\n\t        self.jit = jit\n\t    def setup(self, stage: str):\n\t        if stage == \"fit\":\n", "            train = pd.read_parquet(self.dataset_dir_path / \"train.parquet\")\n\t            valid = pd.read_parquet(self.dataset_dir_path / \"valid.parquet\")\n\t            t_train = self.train_transform\n\t            t_valid = self.valid_transform\n\t            if self.jit:\n\t                t_train = t_train and torch.jit.script(t_train)\n\t                t_valid = t_valid and torch.jit.script(t_valid)\n\t            # make dataset\n\t            self.train = self._make_dataset(train, len(self.labels), t_train, 0.05)\n\t            self.valid = self._make_dataset(valid, len(self.labels), t_valid, 0.0)\n", "    def _gather_augly_metadata(self) -> pd.DataFrame:\n\t        metadata = []\n\t        for p in sorted(self.augly_metadata_path.glob(\"*\")):\n\t            with open(p) as f:\n\t                m = json.load(f)\n\t            video_id = p.stem.split(\"_\", 1)[0]\n\t            row = {\n\t                \"video_id\": video_id,\n\t                \"video_path\": (self.video_dir_path / f\"{video_id}.mp4\").as_posix(),\n\t                \"aug_type\": m[\"name\"],\n", "            }\n\t            metadata.append(row)\n\t        return pd.DataFrame(metadata)\n\t    def _make_dataset(\n\t        self,\n\t        metadata: List[dict],\n\t        n_labels: int,\n\t        transform: Optional[Callable],\n\t        random_relpos_offset: float,\n\t    ) -> pd.DataFrame:\n", "        dataset = pd.DataFrame(metadata).join(\n\t            pd.DataFrame(\n\t                {\n\t                    \"relative_pos\": np.linspace(\n\t                        0.1, 0.9, self.frames_per_video, dtype=float\n\t                    )\n\t                }\n\t            ),\n\t            how=\"cross\",\n\t        )\n", "        return Dataset(\n\t            dataset, n_labels, transform, self.tensor_format, random_relpos_offset\n\t        )\n\t    def train_dataloader(self):\n\t        return DataLoader(\n\t            self.train,\n\t            batch_size=self.batch_size,\n\t            shuffle=True,\n\t            num_workers=self.num_workers,\n\t            pin_memory=True,\n", "            drop_last=True,\n\t        )\n\t    def val_dataloader(self):\n\t        return DataLoader(\n\t            self.valid,\n\t            batch_size=self.batch_size,\n\t            shuffle=False,\n\t            num_workers=self.num_workers,\n\t            pin_memory=True,\n\t            drop_last=False,\n", "        )\n"]}
{"filename": "editing_prediction/training/utils/__init__.py", "chunked_list": []}
{"filename": "meta-vsc-descriptor-runtime/submission_src/main.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\timport subprocess\n\timport time\n\tfrom pathlib import Path\n\tfrom shutil import copyfile\n\timport numpy as np\n\timport pandas as pd\n\tROOT_DIRECTORY = Path(\"/code_execution\")\n\tDATA_DIRECTORY = Path(\"/data\")\n", "QRY_VIDEOS_DIRECTORY = DATA_DIRECTORY / \"query\"\n\tOUTPUT_FILE = ROOT_DIRECTORY / \"subset_query_descriptors.npz\"\n\tQUERY_SUBSET_FILE = DATA_DIRECTORY / \"query_subset.csv\"\n\tdef main():\n\t    num_videos = len(pd.read_csv(QUERY_SUBSET_FILE))\n\t    time_deadline_sec = num_videos * (10 + 1)  # 10 sec per video + 1 sec for overhead\n\t    start_time = time.time()\n\t    cmd = f\"\"\"\n\t    conda run --no-capture-output -n condaenv python -m src.inference \\\n\t        --accelerator=cuda --processes=1 --fps 2 --stride 3 \\\n", "        --dataset_path {str(QRY_VIDEOS_DIRECTORY)} \\\n\t        --output_file {str(OUTPUT_FILE)} \\\n\t        --read_type tensor \\\n\t        --video_reader DECORD\n\t    \"\"\"\n\t    subprocess.run(cmd.split())\n\t    end_time = time.time()\n\t    elapsed_time = end_time - start_time\n\t    print(f\"Elapsed time: {elapsed_time} sec. (deadline: {time_deadline_sec} sec)\")\n\t    if elapsed_time > time_deadline_sec:\n", "        print(\"Time limit exceeded.\")\n\t    else:\n\t        print(\"Time limit not exceeded.\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/inference_full.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Inference script.\n\tThis is split into inference and inference_impl to avoid initializing cuda\n\tbefore processes are created that may use cuda, which can lead to errors\n\tin some runtime environments.\n\tWe import inference_impl, which imports libraries that may initialize cuda,\n", "in two circumstances: from worker processes after the main process has\n\tforked workers, or from the main process after worker processes have been\n\tjoined.\n\t\"\"\"\n\timport argparse\n\timport logging\n\timport os\n\timport tempfile\n\tfrom pathlib import Path\n\tfrom typing import List\n", "import pandas as pd\n\timport tqdm\n\tfrom src.inference import Accelerator, VideoReaderType\n\tfrom src.inference_impl import search\n\tfrom src.vsc.index import VideoFeature\n\tfrom src.vsc.storage import load_features, store_features\n\tfrom torch import multiprocessing\n\tparser = argparse.ArgumentParser()\n\tinference_parser = parser.add_argument_group(\"Inference\")\n\tinference_parser.add_argument(\"--batch_size\", type=int, default=32)\n", "inference_parser.add_argument(\"--distributed_rank\", type=int, default=0)\n\tinference_parser.add_argument(\"--distributed_size\", type=int, default=1)\n\tinference_parser.add_argument(\"--processes\", type=int, default=1)\n\tinference_parser.add_argument(\n\t    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n\t)\n\tinference_parser.add_argument(\"--output_path\", required=True)\n\tinference_parser.add_argument(\"--scratch_path\", required=False)\n\tdataset_parser = parser.add_argument_group(\"Dataset\")\n\t# multiple dataset path\n", "dataset_parser.add_argument(\"--dataset_paths\", nargs=\"+\")\n\tdataset_parser.add_argument(\"--gt_path\")\n\tdataset_parser.add_argument(\"--fps\", default=1, type=float)\n\tdataset_parser.add_argument(\"--stride\", type=int)\n\tdataset_parser.add_argument(\"--len_cap\", type=int)\n\tdataset_parser.add_argument(\"--read_type\", default=\"tensor\", type=str)\n\tdataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")\n\tdataset_parser.add_argument(\n\t    \"--video_reader\", choices=[x.name for x in VideoReaderType], default=\"FFMPEGPY\"\n\t)\n", "dataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")\n\tdataset_parser.add_argument(\"--tta\", action=\"store_true\")\n\tdataset_parser.add_argument(\n\t    \"--mode\", default=\"whole\", choices=[\"whole\", \"eval\", \"test\", \"tune\"]\n\t)\n\tlogging.basicConfig(\n\t    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n\t    level=logging.INFO,\n\t    datefmt=\"%Y-%m-%d %H:%M:%S\",\n\t)\n", "logger = logging.getLogger(\"inference.py\")\n\tlogger.setLevel(logging.INFO)\n\tdef main(args):\n\t    success = False\n\t    if args.processes > 1 and args.distributed_size > 1:\n\t        raise Exception(\n\t            \"Set either --processes (single-machine distributed) or \"\n\t            \"both --distributed_size and --distributed_rank (arbitrary \"\n\t            \"distributed)\"\n\t        )\n", "    if args.video_reader == \"DECORD\":\n\t        import subprocess\n\t        subprocess.run(\n\t            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n\t            check=True,\n\t        )\n\t    with tempfile.TemporaryDirectory() as tmp_path:\n\t        splits = [\n\t            \"_\".join(p.split(\"/\")[-2:]) for p in args.dataset_paths\n\t        ]  # ./vsc/eval_subset/reference -> eval_subset_reference\n", "        os.makedirs(args.output_path, exist_ok=True)\n\t        if args.scratch_path:\n\t            os.makedirs(args.scratch_path, exist_ok=True)\n\t        else:\n\t            args.scratch_path = tmp_path\n\t        if args.processes > 1:\n\t            processes = []\n\t            logger.info(f\"Spawning {args.processes} processes\")\n\t            accelerator = Accelerator[args.accelerator.upper()]\n\t            backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n", "            multiprocessing.set_start_method(\"spawn\")\n\t            worker_files = []\n\t            try:\n\t                for rank in range(args.processes):\n\t                    output_files = [\n\t                        os.path.join(args.scratch_path, f\"{split}_{rank}.npz\")\n\t                        for split in splits\n\t                    ]\n\t                    worker_files.append(output_files)\n\t                    p = multiprocessing.Process(\n", "                        target=distributed_worker_process,\n\t                        args=(\n\t                            args,\n\t                            rank,\n\t                            args.processes,\n\t                            backend,\n\t                            output_files,\n\t                            args.dataset_paths,\n\t                            args.tta,\n\t                        ),\n", "                    )\n\t                    processes.append(p)\n\t                    p.start()\n\t                worker_success = []\n\t                for p in processes:\n\t                    p.join()\n\t                    worker_success.append(p.exitcode == os.EX_OK)\n\t                success = all(worker_success)\n\t            finally:\n\t                for p in processes:\n", "                    p.kill()\n\t            if success:\n\t                def merge_feature_files(filenames, output_filename: str) -> int:\n\t                    features = []\n\t                    for fn in filenames:\n\t                        features.extend(load_features(fn))\n\t                    features = sorted(features, key=lambda x: x.video_id)\n\t                    store_features(output_filename, features)\n\t                    return len(features)\n\t                output_files_each_split = [list(x) for x in zip(*worker_files)]\n", "                for files, split in zip(output_files_each_split, splits):\n\t                    output_file = os.path.join(args.output_path, f\"{split}.npz\")\n\t                    num_files = merge_feature_files(files, output_file)\n\t                    logger.info(\n\t                        f\"Features for {num_files} videos saved to {output_file}\"\n\t                    )\n\t        else:\n\t            output_files = [\n\t                os.path.join(args.output_path, f\"{split}.npz\") for split in splits\n\t            ]\n", "            worker_process(\n\t                args,\n\t                args.distributed_rank,\n\t                args.distributed_size,\n\t                output_files,\n\t                args.dataset_paths,\n\t                args.tta,\n\t            )\n\t            success = True\n\t    if success:\n", "        logger.info(\"Inference succeeded.\")\n\t    else:\n\t        logger.error(\"Inference FAILED!\")\n\t    if not success or args.gt_path is None:\n\t        return\n\t    logger.info(\"Evaluating results\")\n\t    video_features_dict = {}\n\t    for split in splits:\n\t        video_features_dict[split] = load_features(\n\t            os.path.join(args.output_path, f\"{split}.npz\")\n", "        )\n\t    evaluate(\n\t        queries=video_features_dict[splits[0]],\n\t        refs=video_features_dict[splits[1]],\n\t        noises=video_features_dict[splits[-1]],\n\t        gt_path=args.gt_path,\n\t        output_path=args.output_path,\n\t    )\n\tdef distributed_worker_process(pargs, rank, world_size, backend, *args, **kwargs):\n\t    from torch import distributed\n", "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n\t    os.environ[\"MASTER_PORT\"] = \"19529\"\n\t    distributed.init_process_group(backend, rank=rank, world_size=world_size)\n\t    worker_process(pargs, rank, world_size, *args, **kwargs)\n\tdef worker_process(\n\t    args,\n\t    rank,\n\t    world_size,\n\t    output_files: List[str],\n\t    dataset_paths: List[str],\n", "    tta: bool = False,\n\t):\n\t    from src.inference_impl import VideoDataset, get_device, run_inference\n\t    from src.model import create_copy_type_pred_model, create_model_in_runtime\n\t    from torch.utils.data import DataLoader\n\t    logger.info(f\"Starting worker {rank} of {world_size}.\")\n\t    device = get_device(args, rank, world_size)\n\t    logger.info(\"Loading model\")\n\t    model, transforms = create_model_in_runtime(transforms_device=device)\n\t    model = model.to(device).eval().half()\n", "    copytype_model = create_copy_type_pred_model(transforms_device=device)\n\t    copytype_model = copytype_model.to(device).eval()\n\t    logger.info(\"Setting up dataset\")\n\t    extensions = args.video_extensions.split(\",\")\n\t    video_reader = VideoReaderType[args.video_reader.upper()]\n\t    if tta:\n\t        if len(dataset_paths) == 3:\n\t            do_tta_list = [\n\t                True,\n\t                False,\n", "                False,\n\t            ]\n\t        elif len(dataset_paths) == 4:\n\t            do_tta_list = [\n\t                True,\n\t                False,\n\t                True,\n\t                False,\n\t            ]\n\t        elif len(dataset_paths) == 1:\n", "            do_tta_list = [\n\t                True,\n\t            ]\n\t        else:\n\t            raise ValueError(\"TTA requires 3 or 4 datasets\")\n\t    else:\n\t        do_tta_list = [False] * len(dataset_paths)\n\t    for output_filename, dataset_path, do_tta in zip(\n\t        output_files, dataset_paths, do_tta_list\n\t    ):\n", "        dataset = VideoDataset(\n\t            dataset_path,\n\t            fps=args.fps,\n\t            read_type=args.read_type,\n\t            batch_size=1,\n\t            extensions=extensions,\n\t            distributed_world_size=world_size,\n\t            distributed_rank=rank,\n\t            video_reader=video_reader,\n\t            ffmpeg_path=args.ffmpeg_path,\n", "            filter_by_asr=do_tta,\n\t        )\n\t        loader = DataLoader(dataset, batch_size=None, pin_memory=device.type == \"cuda\")\n\t        progress = tqdm.tqdm(total=dataset.num_videos())\n\t        video_features = []\n\t        for vf in run_inference(\n\t            dataloader=loader,\n\t            model=model,\n\t            transforms=transforms,\n\t            tta=do_tta,\n", "            copytype_model=copytype_model,\n\t            batch_size=args.batch_size,\n\t        ):\n\t            video_features.append(vf)\n\t            progress.update()\n\t        store_features(output_filename, video_features)\n\tdef evaluate(queries, refs, noises, gt_path, output_path, sn_method=\"SN\"):\n\t    import faiss\n\t    from src.postproc import sliding_pca_with_ref\n\t    from src.score_normalization import (\n", "        negative_embedding_subtraction,\n\t        score_normalize_with_ref,\n\t    )\n\t    from src.vsc.metrics import (\n\t        CandidatePair,\n\t        Match,\n\t        average_precision,\n\t    )\n\t    stride = args.stride if args.stride is not None else args.fps\n\t    video_features, pca_matrix = sliding_pca_with_ref(\n", "        queries=queries,\n\t        refs=refs,\n\t        noises=noises,\n\t        stride=stride,\n\t    )\n\t    queries = video_features[\"query\"]\n\t    refs = video_features[\"ref\"]\n\t    noises = video_features[\"noise\"]\n\t    store_features(Path(output_path) / \"noise.npz\", noises)\n\t    faiss.write_VectorTransform(pca_matrix, str(Path(output_path) / \"pca_matrix.bin\"))\n", "    if sn_method == \"SN\":\n\t        queries, refs = score_normalize_with_ref(\n\t            queries=queries,\n\t            refs=refs,\n\t            score_norm_refs=noises,\n\t            beta=1.0,\n\t        )\n\t    else:\n\t        queries, refs = negative_embedding_subtraction(\n\t            queries=queries,\n", "            refs=refs,\n\t            score_norm_refs=noises,\n\t            pre_l2_normalize=False,\n\t            post_l2_normalize=False,\n\t            beta=0.8,\n\t            k=10,\n\t            alpha=2.0,\n\t        )\n\t    store_features(Path(output_path) / \"processed_query.npz\", queries)\n\t    store_features(Path(output_path) / \"processed_ref.npz\", refs)\n", "    candidates = search(queries, refs)\n\t    candidates = sorted(candidates, key=lambda x: x.score, reverse=True)\n\t    gt_matches = Match.read_csv(gt_path, is_gt=True)\n\t    ap = average_precision(CandidatePair.from_matches(gt_matches), candidates)\n\t    CandidatePair.write_csv(candidates, Path(output_path) / \"candidates.csv\")\n\t    logger.info(f\"uAP: {ap.ap:.4f}\")\n\tdef tune(\n\t    queries,\n\t    refs,\n\t    noises,\n", "    gt_path,\n\t    output_path,\n\t):\n\t    from sklearn.model_selection import ParameterGrid\n\t    from src.postproc import sliding_pca, sliding_pca_with_ref\n\t    from src.score_normalization import (\n\t        negative_embedding_subtraction,\n\t        score_normalize_with_ref,\n\t    )\n\t    from src.vsc.metrics import (\n", "        CandidatePair,\n\t        Match,\n\t        average_precision,\n\t        evaluate_matching_track,\n\t    )\n\t    param_grid = [\n\t        {\n\t            \"sn_method\": [\"NES\"],\n\t            \"beta\": [1.2, 1.5],\n\t            \"stride\": [3],\n", "            \"fps\": [2],\n\t            \"k\": [10, 20],\n\t            \"alpha\": [1.0, 2.0],\n\t        }\n\t    ]\n\t    rows = []\n\t    for params in ParameterGrid(param_grid):\n\t        if params[\"fps\"] == 1:\n\t            _queries = [\n\t                VideoFeature(\n", "                    video_id=_.video_id,\n\t                    feature=_.feature[::2],\n\t                    timestamps=_.timestamps[::2],\n\t                )\n\t                for _ in queries\n\t            ]\n\t            _refs = [\n\t                VideoFeature(\n\t                    video_id=_.video_id,\n\t                    feature=_.feature[::2],\n", "                    timestamps=_.timestamps[::2],\n\t                )\n\t                for _ in refs\n\t            ]\n\t            _noises = [\n\t                VideoFeature(\n\t                    video_id=_.video_id,\n\t                    feature=_.feature[::2],\n\t                    timestamps=_.timestamps[::2],\n\t                )\n", "                for _ in noises\n\t            ]\n\t        else:\n\t            _queries = queries\n\t            _refs = refs\n\t            _noises = noises\n\t        video_features, pca_matrix = sliding_pca_with_ref(\n\t            queries=_queries,\n\t            refs=_refs,\n\t            noises=_noises,\n", "            stride=params[\"stride\"],\n\t        )\n\t        _queries = video_features[\"query\"]\n\t        _refs = video_features[\"ref\"]\n\t        _noises = video_features[\"noise\"]\n\t        if params[\"sn_method\"] == \"SN\":\n\t            norm_queries, norm_refs = score_normalize_with_ref(\n\t                queries=_queries,\n\t                refs=_refs,\n\t                score_norm_refs=_noises,\n", "                beta=params[\"beta\"],\n\t            )\n\t        else:\n\t            norm_queries, norm_refs = negative_embedding_subtraction(\n\t                queries=_queries,\n\t                refs=_refs,\n\t                score_norm_refs=_noises,\n\t                pre_l2_normalize=False,\n\t                post_l2_normalize=False,\n\t                beta=params[\"beta\"],\n", "                k=params[\"k\"],\n\t                alpha=params[\"alpha\"],\n\t            )\n\t        candidates = search(norm_queries, norm_refs)\n\t        candidates = sorted(candidates, key=lambda x: x.score, reverse=True)\n\t        gt_matches = Match.read_csv(gt_path, is_gt=True)\n\t        ap = average_precision(CandidatePair.from_matches(gt_matches), candidates)\n\t        rows.append(\n\t            {\n\t                \"uAP\": ap.ap,\n", "                **params,\n\t            }\n\t        )\n\t        print(rows[-1])\n\t    df = pd.DataFrame(rows)\n\t    print(df.to_csv())\n\t    df.to_csv(\"tuning_result.csv\", index=False)\n\tif __name__ == \"__main__\":\n\t    args = parser.parse_args()\n\t    if args.mode == \"eval\":\n", "        evaluate(\n\t            queries=load_features(os.path.join(args.output_path, f\"train_query.npz\")),\n\t            refs=load_features(os.path.join(args.output_path, f\"train_reference.npz\")),\n\t            noises=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n\t            gt_path=args.gt_path,\n\t            output_path=args.output_path,\n\t            sn_method=\"SN\",\n\t        )\n\t    elif args.mode == \"test\":\n\t        evaluate(\n", "            queries=load_features(\n\t                os.path.join(args.output_path, f\"phase_2_uB82_query.npz\")\n\t            ),\n\t            refs=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n\t            noises=load_features(\n\t                os.path.join(args.output_path, f\"train_reference.npz\")\n\t            ),\n\t            gt_path=args.gt_path,\n\t            output_path=args.output_path,\n\t            sn_method=\"SN\",\n", "        )\n\t    elif args.mode == \"tune\":\n\t        tune(\n\t            queries=load_features(os.path.join(args.output_path, f\"train_query.npz\")),\n\t            refs=load_features(os.path.join(args.output_path, f\"train_reference.npz\")),\n\t            noises=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n\t            gt_path=args.gt_path,\n\t            output_path=args.output_path,\n\t        )\n\t    else:\n", "        main(args)\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/postproc.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\tfrom typing import Dict, List\n\timport numpy as np\n\tfrom numpy.lib.stride_tricks import sliding_window_view\n\tfrom src.vsc.index import VideoFeature\n\tdef _sliding_window_and_concat(\n\t    vfs: List[VideoFeature], stride: int = 1\n\t) -> Dict[str, List[VideoFeature]]:\n\t    new_vfs = []\n", "    kernel = [0.1, 0.2, 0.4, 0.2, 0.1]\n\t    num_stacks = len(kernel)\n\t    for vf in vfs:\n\t        n, d = vf.feature.shape\n\t        num_views = 1\n\t        for i in range(1, len(vf.timestamps)):\n\t            if vf.timestamps[i] <= vf.timestamps[i - 1]:\n\t                num_views += 1\n\t        if n / num_views <= stride:\n\t            continue\n", "        reshaped_feats = vf.feature.reshape(num_views, -1, d)\n\t        reshaped_ts = vf.timestamps.reshape(num_views, -1)\n\t        new_feats = []\n\t        new_timestamps = []\n\t        for i in range(num_views):\n\t            new_feat = reshaped_feats[i]\n\t            new_ts = reshaped_ts[i]\n\t            new_feat = np.concatenate(\n\t                [new_feat[: num_stacks // 2], new_feat, new_feat[-(num_stacks // 2) :]],\n\t                axis=0,\n", "            )\n\t            new_feat = sliding_window_view(new_feat, num_stacks, axis=0)\n\t            assert len(new_feat) == len(reshaped_feats[i])\n\t            if stride > 1:\n\t                new_feat = new_feat[stride // 2 :: stride]\n\t                new_ts = new_ts[stride // 2 :: stride]\n\t            weight = np.array(kernel).reshape(1, 1, -1)\n\t            new_feat = new_feat * weight\n\t            new_feat = new_feat.transpose(0, 2, 1).reshape(\n\t                -1, new_feat.shape[1] * num_stacks\n", "            )\n\t            new_feats.append(new_feat)\n\t            new_timestamps.append(new_ts)\n\t        new_feats = np.concatenate(new_feats, axis=0)\n\t        new_timestamps = np.concatenate(new_timestamps, axis=0)\n\t        new_vfs.append(\n\t            VideoFeature(\n\t                video_id=vf.video_id,\n\t                timestamps=new_timestamps,\n\t                feature=new_feats,\n", "            )\n\t        )\n\t    return new_vfs\n\tdef _fit_pca(noises, n_components=512) -> Dict[str, List[VideoFeature]]:\n\t    import faiss\n\t    noise_feats = np.concatenate([vf.feature for vf in noises])\n\t    noise_feats = noise_feats.astype(np.float32)\n\t    mat = faiss.PCAMatrix(noise_feats.shape[-1], n_components)\n\t    mat.train(noise_feats)\n\t    assert mat.is_trained\n", "    return mat\n\tdef _apply_pca(vfs, mat) -> Dict[str, List[VideoFeature]]:\n\t    new_vfs = []\n\t    for vf in vfs:\n\t        new_feat = mat.apply(vf.feature.astype(np.float32))\n\t        # new_feat = new_feat / np.linalg.norm(new_feat, axis=-1, keepdims=True)\n\t        new_vfs.append(\n\t            VideoFeature(\n\t                video_id=vf.video_id,\n\t                timestamps=vf.timestamps,\n", "                feature=new_feat,\n\t            )\n\t        )\n\t    return new_vfs\n\tdef sliding_pca(\n\t    queries: List[VideoFeature],\n\t    mat: \"faiss.PCAMatrix\",\n\t    stride: int = 1,\n\t) -> List[VideoFeature]:\n\t    queries = _sliding_window_and_concat(queries, stride=stride)\n", "    queries = _apply_pca(queries, mat)\n\t    return queries\n\tdef sliding_pca_with_ref(\n\t    queries: List[VideoFeature],\n\t    refs: List[VideoFeature],\n\t    noises: List[VideoFeature],\n\t    stride: int = 1,\n\t    n_components: int = 512,\n\t) -> Dict[str, List[VideoFeature]]:\n\t    video_features = {\n", "        \"query\": _sliding_window_and_concat(queries, stride=stride),\n\t        \"ref\": _sliding_window_and_concat(refs, stride=stride),\n\t        \"noise\": _sliding_window_and_concat(noises, stride=stride),\n\t    }\n\t    mat = _fit_pca(video_features[\"noise\"], n_components=n_components)\n\t    for k, vfs in video_features.items():\n\t        video_features[k] = _apply_pca(vfs, mat)\n\t    return video_features, mat\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/model.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\tfrom __future__ import annotations\n\timport timm\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torchvision import transforms\n\tclass ISCNet(nn.Module):\n\t    \"\"\"\n", "    Feature extractor for image copy-detection task.\n\t    Args:\n\t        backbone (`nn.Module`):\n\t            Backbone module.\n\t        fc_dim (`int=256`):\n\t            Feature dimension of the fc layer.\n\t        p (`float=1.0`):\n\t            Power used in gem pooling for training.\n\t        eval_p (`float=1.0`):\n\t            Power used in gem pooling for evaluation. In practice, using a larger power\n", "            for evaluation than training can yield a better performance.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        backbone: nn.Module,\n\t        fc_dim: int = 256,\n\t        p: float = 1.0,\n\t        eval_p: float = 1.0,\n\t        l2_normalize=True,\n\t    ):\n", "        super().__init__()\n\t        self.backbone = backbone\n\t        if hasattr(backbone, \"num_features\"):\n\t            self.is_cnn = False\n\t            in_channels = backbone.num_features\n\t        else:\n\t            self.is_cnn = True\n\t            in_channels = backbone.feature_info.info[-1][\"num_chs\"]\n\t        self.fc = nn.Linear(in_channels, fc_dim, bias=False)\n\t        self.bn = nn.BatchNorm1d(fc_dim)\n", "        self._init_params()\n\t        self.p = p\n\t        self.eval_p = eval_p\n\t        self.l2_normalize = l2_normalize\n\t    def _init_params(self):\n\t        nn.init.xavier_normal_(self.fc.weight)\n\t        nn.init.constant_(self.bn.weight, 1)\n\t        nn.init.constant_(self.bn.bias, 0)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        if self.is_cnn:\n", "            batch_size = x.shape[0]\n\t            x = self.backbone(x)[-1]\n\t            p = self.p if self.training else self.eval_p\n\t            x = gem(x, p).view(batch_size, -1)\n\t        else:\n\t            x = self.backbone(x)\n\t        x = self.fc(x)\n\t        x = self.bn(x)\n\t        if self.l2_normalize:\n\t            x = F.normalize(x)\n", "        return x\n\tdef gem(x, p=3, eps=1e-6):\n\t    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)\n\tclass ConstDivider(nn.Module):\n\t    def __init__(self, c=255.0):\n\t        super().__init__()\n\t        self.c = c\n\t    def forward(self, x):\n\t        return x / self.c\n\tdef create_model_in_runtime(transforms_device=\"cpu\"):\n", "    weight_path = \"./model_assets/isc_ft_v107.pth.tar\"\n\t    ckpt = torch.load(weight_path)\n\t    arch = ckpt[\"arch\"]  # tf_efficientnetv2_m_in21ft1k\n\t    input_size = ckpt[\"args\"].input_size\n\t    backbone = timm.create_model(arch, features_only=True)\n\t    model = ISCNet(\n\t        backbone=backbone,\n\t        fc_dim=256,\n\t        p=1.0,\n\t        eval_p=1.0,\n", "        l2_normalize=True,\n\t    )\n\t    model.to(\"cuda\").train(False)\n\t    state_dict = {}\n\t    for s in ckpt[\"state_dict\"]:\n\t        state_dict[s.replace(\"module.\", \"\")] = ckpt[\"state_dict\"][s]\n\t    model.load_state_dict(state_dict)\n\t    if transforms_device == \"cpu\":\n\t        preprocessor = transforms.Compose(\n\t            [\n", "                transforms.Resize((input_size, input_size)),\n\t                # transforms.ToTensor(),\n\t                ConstDivider(c=255.0),\n\t                transforms.Normalize(\n\t                    mean=backbone.default_cfg[\"mean\"],\n\t                    std=backbone.default_cfg[\"std\"],\n\t                ),\n\t            ]\n\t        )\n\t    else:\n", "        preprocessor = nn.Sequential(\n\t            transforms.Resize((input_size, input_size)),\n\t            ConstDivider(c=255.0),\n\t            transforms.Normalize(\n\t                mean=backbone.default_cfg[\"mean\"],\n\t                std=backbone.default_cfg[\"std\"],\n\t            ),\n\t        )\n\t        preprocessor = torch.jit.script(preprocessor)\n\t        preprocessor.to(transforms_device)\n", "    return model, preprocessor\n\tclass CopyTypePredModel(nn.Module):\n\t    def __init__(self, model_name, labels, transforms):\n\t        super().__init__()\n\t        self.model = timm.create_model(model_name, num_classes=len(labels))\n\t        self.transforms = transforms\n\t        self.labels = labels\n\t    def forward(self, x):\n\t        x = self.transforms(x)\n\t        logit = self.model(x)\n", "        proba = torch.sigmoid(logit)\n\t        return {l: proba[:, i] for i, l in enumerate(self.labels)}\n\tdef create_copy_type_pred_model(transforms_device=\"cpu\"):\n\t    weight_path = \"./model_assets/copy_type_pred__convnext_clip.ckpt\"\n\t    state_dict = torch.load(weight_path, map_location=\"cpu\")[\"state_dict\"]\n\t    arch = \"convnext_base_in22ft1k\"\n\t    mean = (0.48145466, 0.4578275, 0.40821073)\n\t    std = (0.26862954, 0.26130258, 0.27577711)\n\t    input_size = (256, 256)\n\t    labels = [\n", "        \"add_noise\",\n\t        \"blend_videos\",\n\t        \"blur\",\n\t        \"brightness\",\n\t        \"change_aspect_ratio\",\n\t        \"color_jitter\",\n\t        \"contrast\",\n\t        \"crop\",\n\t        \"encoding_quality\",\n\t        \"grayscale\",\n", "        \"hstack\",\n\t        \"other\",\n\t        \"overlay\",\n\t        \"overlay_dots\",\n\t        \"overlay_emoji\",\n\t        \"overlay_shapes\",\n\t        \"overlay_text\",\n\t        \"pad\",\n\t        \"perspective_transform_and_shake\",\n\t        \"pixelization\",\n", "        \"rotate\",\n\t        \"rotate180_vflip\",\n\t        \"rotate90_270\",\n\t        \"vstack\",\n\t        \"vstack_hstack\",\n\t    ]\n\t    if transforms_device == \"cpu\":\n\t        preprocessor = transforms.Compose(\n\t            [\n\t                transforms.ConvertImageDtype(torch.float32),\n", "                transforms.Normalize(mean=mean, std=std),\n\t                transforms.Resize(size=input_size),\n\t            ]\n\t        )\n\t    else:\n\t        preprocessor = nn.Sequential(\n\t            transforms.ConvertImageDtype(torch.float32),\n\t            transforms.Normalize(mean=mean, std=std),\n\t            transforms.Resize(size=input_size),\n\t        )\n", "        preprocessor = torch.jit.script(preprocessor)\n\t        preprocessor.to(transforms_device)\n\t    model = CopyTypePredModel(arch, labels=labels, transforms=preprocessor)\n\t    model.load_state_dict(state_dict)\n\t    model.to(\"cuda\").train(False)\n\t    return model\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/inference.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Inference script.\n\tThis is split into inference and inference_impl to avoid initializing cuda\n\tbefore processes are created that may use cuda, which can lead to errors\n\tin some runtime environments.\n\tWe import inference_impl, which imports libraries that may initialize cuda,\n", "in two circumstances: from worker processes after the main process has\n\tforked workers, or from the main process after worker processes have been\n\tjoined.\n\t\"\"\"\n\timport argparse\n\timport enum\n\timport logging\n\timport os\n\timport tempfile\n\tfrom torch import multiprocessing\n", "class InferenceTransforms(enum.Enum):\n\t    # Aspect-ratio preserving resize to 288\n\t    RESIZE_288 = enum.auto()\n\t    # Resize the short edge to 320, then take the center crop\n\t    RESIZE_320_CENTER = enum.auto()\n\tclass Accelerator(enum.Enum):\n\t    CPU = enum.auto()\n\t    CUDA = enum.auto()\n\tclass VideoReaderType(enum.Enum):\n\t    FFMPEG = enum.auto()\n", "    FFMPEGPY = enum.auto()\n\t    DECORD = enum.auto()\n\tparser = argparse.ArgumentParser()\n\tinference_parser = parser.add_argument_group(\"Inference\")\n\t# inference_parser.add_argument(\"--torchscript_path\", required=True)\n\tinference_parser.add_argument(\"--batch_size\", type=int, default=32)\n\tinference_parser.add_argument(\"--distributed_rank\", type=int, default=0)\n\tinference_parser.add_argument(\"--distributed_size\", type=int, default=1)\n\tinference_parser.add_argument(\"--processes\", type=int, default=1)\n\tinference_parser.add_argument(\n", "    \"--transforms\",\n\t    choices=[x.name for x in InferenceTransforms],\n\t    default=\"RESIZE_320_CENTER\",\n\t)\n\tinference_parser.add_argument(\n\t    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n\t)\n\tinference_parser.add_argument(\"--output_file\", required=True)\n\tinference_parser.add_argument(\"--scratch_path\", required=False)\n\tdataset_parser = parser.add_argument_group(\"Dataset\")\n", "dataset_parser.add_argument(\"--dataset_path\", required=True)\n\tdataset_parser.add_argument(\"--fps\", default=1, type=float)\n\tdataset_parser.add_argument(\"--stride\", type=int)\n\tdataset_parser.add_argument(\"--len_cap\", type=int)\n\tdataset_parser.add_argument(\"--read_type\", default=\"pil\", type=str)\n\tdataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")\n\tdataset_parser.add_argument(\n\t    \"--video_reader\", choices=[x.name for x in VideoReaderType], default=\"FFMPEG\"\n\t)\n\tdataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")\n", "dataset_parser.add_argument(\"--score_norm_features\", default=\"./noise.npz\")\n\tdataset_parser.add_argument(\n\t    \"--reference_features\", default=\"./reference_descriptor.npz\"\n\t)\n\tdataset_parser.add_argument(\"--pca_matrix\", default=\"./pca_matrix.bin\")\n\tdataset_parser.add_argument(\"--gt_path\")\n\tlogging.basicConfig(\n\t    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n\t    level=logging.INFO,\n\t    datefmt=\"%Y-%m-%d %H:%M:%S\",\n", ")\n\tlogger = logging.getLogger(\"inference.py\")\n\tlogger.setLevel(logging.INFO)\n\tdef main(args):\n\t    success = False\n\t    if args.processes > 1 and args.distributed_size > 1:\n\t        raise Exception(\n\t            \"Set either --processes (single-machine distributed) or \"\n\t            \"both --distributed_size and --distributed_rank (arbitrary \"\n\t            \"distributed)\"\n", "        )\n\t    if args.video_reader == \"DECORD\":\n\t        import subprocess\n\t        subprocess.run(\n\t            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n\t            check=True,\n\t        )\n\t    with tempfile.TemporaryDirectory() as tmp_path:\n\t        os.makedirs(os.path.dirname(args.output_file), exist_ok=True)\n\t        if args.scratch_path:\n", "            os.makedirs(args.scratch_path, exist_ok=True)\n\t        else:\n\t            args.scratch_path = tmp_path\n\t        if args.processes > 1:\n\t            processes = []\n\t            logger.info(f\"Spawning {args.processes} processes\")\n\t            accelerator = Accelerator[args.accelerator.upper()]\n\t            backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n\t            multiprocessing.set_start_method(\"spawn\")\n\t            worker_files = []\n", "            try:\n\t                for rank in range(args.processes):\n\t                    worker_file = os.path.join(args.scratch_path, f\"{rank}.npz\")\n\t                    worker_files.append(worker_file)\n\t                    p = multiprocessing.Process(\n\t                        target=distributed_worker_process,\n\t                        args=(args, rank, args.processes, backend, worker_file),\n\t                    )\n\t                    processes.append(p)\n\t                    p.start()\n", "                worker_success = []\n\t                for p in processes:\n\t                    p.join()\n\t                    worker_success.append(p.exitcode == os.EX_OK)\n\t                success = all(worker_success)\n\t            finally:\n\t                for p in processes:\n\t                    p.kill()\n\t            if success:\n\t                from .inference_impl import merge_feature_files  # @manual\n", "                num_files = merge_feature_files(worker_files, args.output_file)\n\t                logger.info(\n\t                    f\"Features for {num_files} videos saved to {args.output_file}\"\n\t                )\n\t        else:\n\t            worker_process(\n\t                args, args.distributed_rank, args.distributed_size, args.output_file\n\t            )\n\t            success = True\n\t    if success:\n", "        logger.info(\"Inference succeeded.\")\n\t    else:\n\t        logger.error(\"Inference FAILED!\")\n\tdef distributed_worker_process(args, rank, world_size, backend, output_filename):\n\t    from torch import distributed\n\t    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n\t    os.environ[\"MASTER_PORT\"] = \"19529\"\n\t    distributed.init_process_group(backend, rank=rank, world_size=world_size)\n\t    worker_process(args, rank, world_size, output_filename)\n\tdef worker_process(*args):\n", "    # Late import: initialize cuda after worker spawn.\n\t    from src.inference_impl import worker_process as worker_impl  # @manual\n\t    return worker_impl(*args)\n\tif __name__ == \"__main__\":\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/inference_impl.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport glob\n\timport itertools\n\timport logging\n\timport os\n\tfrom pathlib import Path\n\tfrom typing import Iterable, List, Optional, Tuple\n", "import faiss\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\timport torch.nn as nn\n\timport tqdm\n\tfrom src.inference import Accelerator, VideoReaderType\n\tfrom src.model import create_copy_type_pred_model, create_model_in_runtime\n\tfrom src.postproc import sliding_pca\n\tfrom src.score_normalization import score_normalize\n", "from src.tta import (\n\t    TTA4ViewsTransform,\n\t    TTA5ViewsTransform,\n\t    TTAHorizontalStackTransform,\n\t    TTAVerticalStackTransform,\n\t)\n\tfrom src.video_reader.ffmpeg_py_video_reader import FFMpegPyVideoReader\n\tfrom src.video_reader.ffmpeg_video_reader import FFMpegVideoReader\n\tfrom src.vsc.candidates import CandidateGeneration, MaxScoreAggregation\n\tfrom src.vsc.index import VideoFeature\n", "from src.vsc.metrics import (\n\t    CandidatePair,\n\t    Match,\n\t    average_precision,\n\t    evaluate_matching_track,\n\t)\n\tfrom src.vsc.storage import load_features, store_features\n\tfrom torch.utils.data import DataLoader, IterableDataset\n\tfrom torch.utils.data._utils.collate import default_collate\n\tfrom torchvision import transforms\n", "logging.basicConfig(\n\t    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n\t    level=logging.INFO,\n\t    datefmt=\"%Y-%m-%d %H:%M:%S\",\n\t)\n\tlogger = logging.getLogger(\"inference_impl.py\")\n\tlogger.setLevel(logging.INFO)\n\tclass VideoDataset(IterableDataset):\n\t    \"\"\"Decodes video frames at a fixed FPS via ffmpeg.\"\"\"\n\t    def __init__(\n", "        self,\n\t        path: str,\n\t        fps: float,\n\t        read_type: str = \"pil\",\n\t        batch_size=None,\n\t        img_transform=None,\n\t        extensions=(\"mp4\",),\n\t        distributed_rank=0,\n\t        distributed_world_size=1,\n\t        video_reader=VideoReaderType.FFMPEG,\n", "        ffmpeg_path=\"ffmpeg\",\n\t        filter_by_asr=False,\n\t    ):\n\t        assert distributed_rank < distributed_world_size\n\t        self.path = path\n\t        self.fps = fps\n\t        self.read_type = read_type\n\t        self.batch_size = batch_size\n\t        self.img_transform = img_transform\n\t        self.video_reader = video_reader\n", "        self.ffmpeg_path = ffmpeg_path\n\t        if len(extensions) == 1:\n\t            filenames = glob.glob(os.path.join(path, f\"*.{extensions[0]}\"))\n\t        else:\n\t            filenames = glob.glob(os.path.join(path, \"*.*\"))\n\t            filenames = (fn for fn in filenames if fn.rsplit(\".\", 1)[-1] in extensions)\n\t        filenames = [\n\t            name for name in filenames if Path(name).stem not in [\"R102796\", \"R133364\"]\n\t        ]\n\t        self.videos = sorted(filenames)\n", "        if not self.videos:\n\t            raise Exception(\"No videos found!\")\n\t        assert distributed_rank < distributed_world_size\n\t        self.rank = distributed_rank\n\t        self.world_size = distributed_world_size\n\t        self.selected_videos = [\n\t            (i, video)\n\t            for (i, video) in enumerate(self.videos)\n\t            if (i % self.world_size) == self.rank\n\t        ]\n", "    def num_videos(self) -> int:\n\t        return len(self.selected_videos)\n\t    def __iter__(self):\n\t        for i, video in self.selected_videos:\n\t            if self.batch_size:\n\t                frames = self.read_frames(i, video)\n\t                while True:\n\t                    batch = list(itertools.islice(frames, self.batch_size))\n\t                    if not batch:\n\t                        break\n", "                    yield default_collate(batch)\n\t            else:\n\t                yield from self.read_frames(i, video)\n\t    def read_frames(self, video_id, video):\n\t        video_name = os.path.basename(video)\n\t        name = os.path.basename(video_name).split(\".\")[0]\n\t        if self.video_reader == VideoReaderType.FFMPEG:\n\t            reader = FFMpegVideoReader(\n\t                video_path=video,\n\t                required_fps=self.fps,\n", "                output_type=self.read_type,\n\t                ffmpeg_path=self.ffmpeg_path,\n\t            )\n\t        elif self.video_reader == VideoReaderType.FFMPEGPY:\n\t            reader = FFMpegPyVideoReader(\n\t                video_path=video, required_fps=self.fps, output_type=self.read_type\n\t            )\n\t        elif self.video_reader == VideoReaderType.DECORD:\n\t            from src.video_reader.decord_video_reader import DecordVideoReader\n\t            reader = DecordVideoReader(\n", "                video_path=video, required_fps=self.fps, output_type=self.read_type\n\t            )\n\t        else:\n\t            raise ValueError(f\"VideoReaderType: {self.video_reader} not supported\")\n\t        for start_timestamp, end_timestamp, frame in reader.frames():\n\t            if self.img_transform:\n\t                frame = self.img_transform(frame)\n\t            record = {\n\t                \"name\": name,\n\t                \"timestamp\": (np.array(start_timestamp) + np.array(end_timestamp)) / 2,\n", "                \"input\": frame,\n\t            }\n\t            yield record\n\tdef should_use_cuda(args) -> bool:\n\t    accelerator = Accelerator[args.accelerator.upper()]\n\t    return accelerator == Accelerator.CUDA\n\tdef get_device(args, rank, world_size):\n\t    if should_use_cuda(args):\n\t        assert torch.cuda.is_available()\n\t        num_devices = torch.cuda.device_count()\n", "        if args.processes > num_devices:\n\t            raise Exception(\n\t                f\"Asked for {args.processes} processes and cuda, but only \"\n\t                f\"{num_devices} devices found\"\n\t            )\n\t        if args.processes > 1 or world_size <= num_devices:\n\t            device_num = rank\n\t        else:\n\t            device_num = 0\n\t        torch.cuda.set_device(device_num)\n", "        return torch.device(\"cuda\", device_num)\n\t    return torch.device(\"cpu\")\n\tdef search(\n\t    queries: List[VideoFeature],\n\t    refs: List[VideoFeature],\n\t    retrieve_per_query: float = 1200.0,\n\t    candidates_per_query: float = 25.0,\n\t) -> List[CandidatePair]:\n\t    aggregation = MaxScoreAggregation()\n\t    logger.info(\"Searching\")\n", "    cg = CandidateGeneration(refs, aggregation)\n\t    num_to_retrieve = int(retrieve_per_query * len(queries))\n\t    candidates = cg.query(queries, global_k=num_to_retrieve)\n\t    num_candidates = int(candidates_per_query * len(queries))\n\t    candidates = candidates[:num_candidates]\n\t    logger.info(\"Got %d candidates\", len(candidates))\n\t    return candidates\n\tdef worker_process(args, rank, world_size, output_filename):\n\t    logger.info(f\"Starting worker {rank} of {world_size}.\")\n\t    device = get_device(args, rank, world_size)\n", "    logger.info(\"Loading model\")\n\t    model, transforms = create_model_in_runtime(transforms_device=device)\n\t    model = model.to(device).eval().half()\n\t    copytype_model = create_copy_type_pred_model(transforms_device=device)\n\t    copytype_model = copytype_model.to(device).eval()\n\t    logger.info(\"Setting up dataset\")\n\t    extensions = args.video_extensions.split(\",\")\n\t    video_reader = VideoReaderType[args.video_reader.upper()]\n\t    dataset = VideoDataset(\n\t        args.dataset_path,\n", "        fps=args.fps,\n\t        read_type=args.read_type,\n\t        # img_transform=transforms,\n\t        # batch_size=args.batch_size,\n\t        batch_size=1,\n\t        extensions=extensions,\n\t        distributed_world_size=world_size,\n\t        distributed_rank=rank,\n\t        video_reader=video_reader,\n\t        ffmpeg_path=args.ffmpeg_path,\n", "        filter_by_asr=False,\n\t    )\n\t    loader = DataLoader(dataset, batch_size=None, pin_memory=device.type == \"cuda\")\n\t    progress = tqdm.tqdm(total=dataset.num_videos())\n\t    queries = []\n\t    for vf in run_inference(\n\t        dataloader=loader,\n\t        model=model,\n\t        transforms=transforms,\n\t        tta=True,\n", "        copytype_model=copytype_model,\n\t        batch_size=args.batch_size,\n\t    ):\n\t        queries.append(vf)\n\t        progress.update()\n\t    del loader\n\t    del model\n\t    del copytype_model\n\t    del dataset\n\t    alpha = 1.0\n", "    factors = [(1 / (vf.feature @ vf.feature.T).mean()) ** alpha for vf in queries]\n\t    if args.score_norm_features:\n\t        stride = args.stride if args.stride is not None else args.fps\n\t        print(\"stride\", stride, args.stride, args.fps)\n\t        pca_matrix = faiss.read_VectorTransform(args.pca_matrix)\n\t        queries = sliding_pca(queries=queries, mat=pca_matrix, stride=stride)\n\t        queries = score_normalize(\n\t            queries,\n\t            load_features(args.score_norm_features),\n\t            beta=1.0,\n", "        )\n\t    queries = [\n\t        VideoFeature(\n\t            video_id=vf.video_id,\n\t            timestamps=vf.timestamps,\n\t            feature=vf.feature * factor,\n\t        )\n\t        for vf, factor in zip(queries, factors)\n\t    ]\n\t    store_features(output_filename, queries)\n", "    logger.info(\n\t        f\"Wrote worker {rank} features for {len(queries)} videos to {output_filename}\"\n\t    )\n\tdef video_level_loader(\n\t    frame_level_dataloader: DataLoader,\n\t) -> Iterable[Tuple[str, torch.Tensor, torch.Tensor]]:\n\t    name, imgs, timestamps = None, [], []\n\t    for frames in frame_level_dataloader:\n\t        _names, _imgs, _ts = frames[\"name\"], frames[\"input\"], frames[\"timestamp\"]\n\t        assert _names[0] == _names[-1]  # single-video batches\n", "        _name = _names[0]\n\t        if name is not None and name != _name:\n\t            yield name, torch.concat(imgs), torch.concat(timestamps)\n\t            name, imgs, timestamps = _name, [_imgs], [_ts]\n\t        else:\n\t            name = _name\n\t            imgs.append(_imgs)\n\t            timestamps.append(_ts)\n\t    yield name, torch.concat(imgs), torch.concat(timestamps)\n\tdef batch_forward(\n", "    model: nn.Module, inputs: torch.Tensor, batch_size: int, transforms=None\n\t) -> torch.Tensor:\n\t    device = next(model.parameters()).device\n\t    dtype = next(model.parameters()).dtype\n\t    outputs = []\n\t    for i in range(0, len(inputs), batch_size):\n\t        x = inputs[i : i + batch_size]\n\t        if transforms is not None:\n\t            x = transforms(x)\n\t        if dtype == torch.float16:\n", "            x = x.half()\n\t        x = model(x.to(device))\n\t        if isinstance(x, dict):\n\t            x = {k: v.cpu() for k, v in x.items()}\n\t        else:\n\t            x = x.cpu()\n\t        outputs.append(x)\n\t    if isinstance(outputs[0], dict):\n\t        output_dict = {}\n\t        for k in outputs[0]:\n", "            output_dict[k] = torch.cat([d[k] for d in outputs], dim=0)\n\t        return output_dict\n\t    else:\n\t        return torch.cat(outputs, dim=0)\n\t@torch.no_grad()\n\tdef run_inference(\n\t    dataloader, model, transforms=None, tta=True, copytype_model=None, batch_size=32\n\t) -> Iterable[VideoFeature]:\n\t    for name, imgs, timestamps in video_level_loader(dataloader):\n\t        n_imgs = len(imgs)\n", "        _transforms = transforms\n\t        thresh = 0.15\n\t        # Extend transforms\n\t        if _transforms is not None and tta:\n\t            if copytype_model is not None:\n\t                pred = batch_forward(copytype_model, imgs, batch_size=batch_size)\n\t                assert len(pred[\"hstack\"]) == len(\n\t                    imgs\n\t                ), f\"{len(pred['hstack'])=} != {len(imgs)=}\"\n\t                assert len(pred[\"vstack\"]) == len(\n", "                    imgs\n\t                ), f\"{len(pred['vstack'])=} != {len(imgs)=}\"\n\t                vstack_hstack_prob = torch.median(pred[\"vstack_hstack\"])\n\t                hstack_prob = torch.median(pred[\"hstack\"])\n\t                vstack_prob = torch.median(pred[\"vstack\"])\n\t                if vstack_hstack_prob > thresh:\n\t                    _transforms = TTA4ViewsTransform(_transforms)\n\t                    print('vstack_hstack', vstack_hstack_prob)\n\t                elif hstack_prob > thresh:\n\t                    _transforms = TTAHorizontalStackTransform(_transforms)\n", "                elif vstack_prob > thresh:\n\t                    _transforms = TTAVerticalStackTransform(_transforms)\n\t            # if copytype model is not given\n\t            else:\n\t                _transforms = TTA5ViewsTransform(_transforms)\n\t        if _transforms is None:\n\t            _transforms = torch.nn.Identity()\n\t        feature = batch_forward(\n\t            model,\n\t            imgs,\n", "            batch_size=batch_size,\n\t            transforms=_transforms,\n\t        ).numpy()\n\t        num_views = len(feature) // n_imgs\n\t        timestamps = np.tile(timestamps.numpy(), num_views)\n\t        yield VideoFeature(\n\t            video_id=name,\n\t            timestamps=timestamps,\n\t            feature=feature,\n\t        )\n", "def merge_feature_files(filenames: List[str], output_filename: str) -> int:\n\t    features = []\n\t    for fn in filenames:\n\t        features.extend(load_features(fn))\n\t    store_features(output_filename, features)\n\t    return len(features)\n\tdef validate_total_descriptors(\n\t    video_features: List[VideoFeature], meta: Optional[pd.DataFrame] = None\n\t):\n\t    _ids = [_.video_id for _ in video_features]\n", "    n_features = sum([_.feature.shape[0] for _ in video_features])\n\t    if meta is None:\n\t        meta_root = Path(\"./metadata_root\")\n\t        meta_paths = [\n\t            meta_root / \"train\" / \"train_query_metadata.csv\",\n\t            meta_root / \"train\" / \"train_reference_metadata.csv\",\n\t            meta_root / \"test\" / \"test_query_metadata.csv\",\n\t            meta_root / \"test\" / \"test_reference_metadata.csv\",\n\t        ]\n\t        meta = pd.concat([pd.read_csv(path) for path in meta_paths])\n", "    meta = meta.set_index(\"video_id\").loc[_ids]\n\t    total_seconds = meta.duration_sec.apply(np.ceil).sum()\n\t    logger.info(f\"Saw {n_features} vectors, max allowed is {total_seconds}\")\n\t    if n_features > total_seconds:\n\t        logger.info(\"*Warning*: Too many vectors\")\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/score_normalization.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport dataclasses\n\timport logging\n\tfrom typing import Callable, List, Tuple\n\timport faiss  # @manual\n\timport numpy as np\n\tfrom sklearn.preprocessing import normalize\n", "from src.vsc.candidates import CandidateGeneration, MaxScoreAggregation\n\tfrom src.vsc.index import VideoFeature\n\tlogger = logging.getLogger(\"score_normalization.py\")\n\tlogger.setLevel(logging.INFO)\n\tdef transform_features(\n\t    features: List[VideoFeature], transform: Callable\n\t) -> List[VideoFeature]:\n\t    return [\n\t        dataclasses.replace(feature, feature=transform(feature.feature))\n\t        for feature in features\n", "    ]\n\tdef score_normalize(\n\t    queries: List[VideoFeature],\n\t    score_norm_refs: List[VideoFeature],\n\t    l2_normalize: bool = True,\n\t    replace_dim: bool = True,\n\t    beta: float = 1.0,\n\t) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n\t    \"\"\"\n\t    CSLS style score normalization (as used in the Image Similarity Challenge)\n", "    has the following form. We compute a bias term for each query:\n\t      bias(query) = - beta * sim(query, noise)\n\t    then compute score normalized similarity by incorporating this as an\n\t    additive term for each query:\n\t      sim_sn(query, ref) = sim(query, ref) + bias(query)\n\t    sim(query, ref) is inner product similarity (query * ref), and\n\t    sim(query, noise) is some function of query similarity to a noise dataset\n\t    (score_norm_refs here), such as the similarity to the nearest neighbor.\n\t    We encode the bias term as an extra dimension in the query descriptor,\n\t    and add a constant 1 dimension to reference descriptors, so that inner-\n", "    product similarity is the score-normalized similarity:\n\t      query' = [query bias(query)]\n\t      ref' = [ref 1]\n\t      query' * ref' = (query * ref) + (bias(query) * 1)\n\t          = sim(query, ref) + bias(query) = sim_sn(query, ref)\n\t    \"\"\"\n\t    if score_norm_refs is not None and replace_dim:\n\t        # Make space for the additional score normalization dimension.\n\t        # We could also use PCA dim reduction, but re-centering can be\n\t        # destructive.\n", "        logger.info(\"Replacing dimension\")\n\t        sn_features = np.concatenate([ref.feature for ref in score_norm_refs], axis=0)\n\t        low_var_dim = sn_features.var(axis=0).argmin()\n\t        queries, score_norm_refs = [\n\t            transform_features(\n\t                x, lambda feature: np.delete(feature, low_var_dim, axis=1)\n\t            )\n\t            for x in [queries, score_norm_refs]\n\t        ]\n\t    if l2_normalize:\n", "        logger.info(\"L2 normalizing\")\n\t        queries, score_norm_refs = [\n\t            transform_features(x, normalize) for x in [queries, score_norm_refs]\n\t        ]\n\t    logger.info(\"Applying score normalization\")\n\t    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n\t    if faiss.get_num_gpus() > 0:\n\t        index = faiss.index_cpu_to_all_gpus(index)\n\t    adapted_queries = []\n\t    # Add the additive normalization term to the queries as an extra dimension.\n", "    for query in queries:\n\t        # KNN search is ok here (versus a threshold/radius/range search) since\n\t        # we're not searching the dataset we're evaluating on.\n\t        similarity, ids = index.search(query.feature, 1)\n\t        norm_term = -beta * similarity[:, :1]\n\t        feature = np.concatenate([query.feature, norm_term], axis=1)\n\t        adapted_queries.append(dataclasses.replace(query, feature=feature))\n\t    return adapted_queries\n\tdef score_normalize_with_ref(\n\t    queries: List[VideoFeature],\n", "    refs: List[VideoFeature],\n\t    score_norm_refs: List[VideoFeature],\n\t    l2_normalize: bool = True,\n\t    replace_dim: bool = True,\n\t    beta: float = 1.0,\n\t    return_adapted_score_norm_refs: bool = False,\n\t) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n\t    if {f.video_id for f in refs}.intersection({f.video_id for f in score_norm_refs}):\n\t        raise Exception(\n\t            \"Normalizing on the dataset we're evaluating on is against VSC rules. \"\n", "            \"An independent dataset is needed.\"\n\t        )\n\t    if score_norm_refs is not None and replace_dim:\n\t        # Make space for the additional score normalization dimension.\n\t        # We could also use PCA dim reduction, but re-centering can be\n\t        # destructive.\n\t        logger.info(\"Replacing dimension\")\n\t        sn_features = np.concatenate([ref.feature for ref in score_norm_refs], axis=0)\n\t        low_var_dim = sn_features.var(axis=0).argmin()\n\t        queries, refs, score_norm_refs = [\n", "            transform_features(\n\t                x, lambda feature: np.delete(feature, low_var_dim, axis=1)\n\t            )\n\t            for x in [queries, refs, score_norm_refs]\n\t        ]\n\t    if l2_normalize:\n\t        logger.info(\"L2 normalizing\")\n\t        queries, refs, score_norm_refs = [\n\t            transform_features(x, normalize) for x in [queries, refs, score_norm_refs]\n\t        ]\n", "    logger.info(\"Applying score normalization\")\n\t    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n\t    if faiss.get_num_gpus() > 0:\n\t        index = faiss.index_cpu_to_all_gpus(index)\n\t    adapted_queries = []\n\t    # Add the additive normalization term to the queries as an extra dimension.\n\t    for query in queries:\n\t        # KNN search is ok here (versus a threshold/radius/range search) since\n\t        # we're not searching the dataset we're evaluating on.\n\t        similarity, ids = index.search(query.feature, 1)\n", "        norm_term = -beta * similarity[:, :1]\n\t        feature = np.concatenate([query.feature, norm_term], axis=1)\n\t        adapted_queries.append(dataclasses.replace(query, feature=feature))\n\t    adapted_refs = []\n\t    for ref in refs:\n\t        ones = np.ones_like(ref.feature[:, :1])\n\t        feature = np.concatenate([ref.feature, ones], axis=1)\n\t        adapted_refs.append(dataclasses.replace(ref, feature=feature))\n\t    output = (adapted_queries, adapted_refs)\n\t    if return_adapted_score_norm_refs:\n", "        adapted_score_norm_refs = []\n\t        for score_norm_ref in score_norm_refs:\n\t            ones = np.ones_like(score_norm_ref.feature[:, :1])\n\t            feature = np.concatenate([score_norm_ref.feature, ones], axis=1)\n\t            adapted_score_norm_refs.append(\n\t                dataclasses.replace(score_norm_ref, feature=feature)\n\t            )\n\t        output += (adapted_score_norm_refs,)\n\t    return output\n\tdef negative_embedding_subtraction(\n", "    queries: List[VideoFeature],\n\t    refs: List[VideoFeature],\n\t    score_norm_refs: List[VideoFeature],\n\t    pre_l2_normalize: bool = False,\n\t    post_l2_normalize: bool = False,\n\t    beta: float = 1.0,\n\t    k: int = 10,\n\t    alpha: float = 1.0,\n\t) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n\t    # impl of https://arxiv.org/abs/2112.04323\n", "    if pre_l2_normalize:\n\t        logger.info(\"L2 normalizing\")\n\t        queries, refs, score_norm_refs = [\n\t            transform_features(x, normalize) for x in [queries, refs, score_norm_refs]\n\t        ]\n\t    logger.info(\"Applying negative embedding subtraction\")\n\t    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n\t    if faiss.get_num_gpus() > 0:\n\t        index = faiss.index_cpu_to_all_gpus(index)\n\t    negative_embeddings = np.concatenate([vf.feature for vf in score_norm_refs], axis=0)\n", "    adapted_queries = []\n\t    for query in queries:\n\t        similarity, ids = index.search(query.feature, k=k)\n\t        weights = similarity[..., None] ** alpha\n\t        topk_negative_embeddings = negative_embeddings[ids] * weights\n\t        subtracted_embedding = topk_negative_embeddings.mean(axis=1) * beta\n\t        adapted_embedding = query.feature - subtracted_embedding\n\t        adapted_queries.append(dataclasses.replace(query, feature=adapted_embedding))\n\t    adapted_refs = []\n\t    for ref in refs:\n", "        similarity, ids = index.search(ref.feature, k=k)\n\t        weights = similarity[..., None] ** alpha\n\t        topk_negative_embeddings = negative_embeddings[ids] * weights\n\t        subtracted_embedding = topk_negative_embeddings.mean(axis=1) * beta\n\t        adapted_embedding = ref.feature - subtracted_embedding\n\t        adapted_refs.append(dataclasses.replace(ref, feature=adapted_embedding))\n\t    if post_l2_normalize:\n\t        logger.info(\"L2 normalizing\")\n\t        adapted_queries, adapted_refs = [\n\t            transform_features(x, normalize) for x in [adapted_queries, adapted_refs]\n", "        ]\n\t    return adapted_queries, adapted_refs\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/tta.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torchvision\n\tclass TTA30ViewsTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n\t        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n", "        *_, h, w = x.shape\n\t        x_top = x[..., : h // 2, :]  # top\n\t        x_bottom = x[..., h // 2 :, :]  # bottom\n\t        x_left = x[..., :, : w // 2]  # left\n\t        x_right = x[..., :, w // 2 :]  # right\n\t        if self.base_transforms is not None:\n\t            x = self.base_transforms(x)\n\t            x_top = self.base_transforms(x_top)\n\t            x_bottom = self.base_transforms(x_bottom)\n\t            x_left = self.base_transforms(x_left)\n", "            x_right = self.base_transforms(x_right)\n\t        crops = [\n\t            x,\n\t            torchvision.transforms.functional.rotate(x, angle=90),\n\t            torchvision.transforms.functional.rotate(x, angle=180),\n\t            torchvision.transforms.functional.rotate(x, angle=270),\n\t            torchvision.transforms.functional.hflip(x),\n\t            torchvision.transforms.functional.vflip(x),\n\t            x_top,\n\t            torchvision.transforms.functional.rotate(x_top, angle=90),\n", "            torchvision.transforms.functional.rotate(x_top, angle=180),\n\t            torchvision.transforms.functional.rotate(x_top, angle=270),\n\t            torchvision.transforms.functional.hflip(x_top),\n\t            torchvision.transforms.functional.vflip(x_top),\n\t            x_bottom,\n\t            torchvision.transforms.functional.rotate(x_bottom, angle=90),\n\t            torchvision.transforms.functional.rotate(x_bottom, angle=180),\n\t            torchvision.transforms.functional.rotate(x_bottom, angle=270),\n\t            torchvision.transforms.functional.hflip(x_bottom),\n\t            torchvision.transforms.functional.vflip(x_bottom),\n", "            x_left,\n\t            torchvision.transforms.functional.rotate(x_left, angle=90),\n\t            torchvision.transforms.functional.rotate(x_left, angle=180),\n\t            torchvision.transforms.functional.rotate(x_left, angle=270),\n\t            torchvision.transforms.functional.hflip(x_left),\n\t            torchvision.transforms.functional.vflip(x_left),\n\t            x_right,\n\t            torchvision.transforms.functional.rotate(x_right, angle=90),\n\t            torchvision.transforms.functional.rotate(x_right, angle=180),\n\t            torchvision.transforms.functional.rotate(x_right, angle=270),\n", "            torchvision.transforms.functional.hflip(x_right),\n\t            torchvision.transforms.functional.vflip(x_right),\n\t        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n\tclass TTA24ViewsTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n\t        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n", "        *_, h, w = x.shape\n\t        x_top_left = x[..., : h // 2, : w // 2]  # top_left\n\t        x_top_right = x[..., : h // 2, w // 2 :]  # top_right\n\t        x_bottom_left = x[..., h // 2 :, : w // 2]  # bottom_left\n\t        x_bottom_right = x[..., h // 2 :, w // 2 :]  # bottom_right\n\t        if self.base_transforms is not None:\n\t            x_top_left = self.base_transforms(x_top_left)\n\t            x_top_right = self.base_transforms(x_top_right)\n\t            x_bottom_left = self.base_transforms(x_bottom_left)\n\t            x_bottom_right = self.base_transforms(x_bottom_right)\n", "        crops = [\n\t            x_top_left,\n\t            torchvision.transforms.functional.rotate(x_top_left, angle=90),\n\t            torchvision.transforms.functional.rotate(x_top_left, angle=180),\n\t            torchvision.transforms.functional.rotate(x_top_left, angle=270),\n\t            torchvision.transforms.functional.hflip(x_top_left),\n\t            torchvision.transforms.functional.vflip(x_top_left),\n\t            x_top_right,\n\t            torchvision.transforms.functional.rotate(x_top_right, angle=90),\n\t            torchvision.transforms.functional.rotate(x_top_right, angle=180),\n", "            torchvision.transforms.functional.rotate(x_top_right, angle=270),\n\t            torchvision.transforms.functional.hflip(x_top_right),\n\t            torchvision.transforms.functional.vflip(x_top_right),\n\t            x_bottom_left,\n\t            torchvision.transforms.functional.rotate(x_bottom_left, angle=90),\n\t            torchvision.transforms.functional.rotate(x_bottom_left, angle=180),\n\t            torchvision.transforms.functional.rotate(x_bottom_left, angle=270),\n\t            torchvision.transforms.functional.hflip(x_bottom_left),\n\t            torchvision.transforms.functional.vflip(x_bottom_left),\n\t            x_bottom_right,\n", "            torchvision.transforms.functional.rotate(x_bottom_right, angle=90),\n\t            torchvision.transforms.functional.rotate(x_bottom_right, angle=180),\n\t            torchvision.transforms.functional.rotate(x_bottom_right, angle=270),\n\t            torchvision.transforms.functional.hflip(x_bottom_right),\n\t            torchvision.transforms.functional.vflip(x_bottom_right),\n\t        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n\tclass TTA5ViewsTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n", "        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n\t        *_, h, w = x.shape\n\t        x_top = x[..., : h // 2, :]  # top\n\t        x_bottom = x[..., h // 2 :, :]  # bottom\n\t        x_left = x[..., :, : w // 2]  # left\n\t        x_right = x[..., :, w // 2 :]  # right\n\t        if self.base_transforms is not None:\n\t            x = self.base_transforms(x)\n", "            x_top = self.base_transforms(x_top)\n\t            x_bottom = self.base_transforms(x_bottom)\n\t            x_left = self.base_transforms(x_left)\n\t            x_right = self.base_transforms(x_right)\n\t        crops = [\n\t            x,\n\t            x_top,\n\t            x_bottom,\n\t            x_left,\n\t            x_right,\n", "        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n\tclass TTA4ViewsTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n\t        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n\t        *_, h, w = x.shape\n\t        x_top_left = x[..., : h // 2, : w // 2]  # top_left\n", "        x_top_right = x[..., : h // 2, w // 2 :]  # top_right\n\t        x_bottom_left = x[..., h // 2 :, : w // 2]  # bottom_left\n\t        x_bottom_right = x[..., h // 2 :, w // 2 :]  # bottom_right\n\t        if self.base_transforms is not None:\n\t            x_top_left = self.base_transforms(x_top_left)\n\t            x_top_right = self.base_transforms(x_top_right)\n\t            x_bottom_left = self.base_transforms(x_bottom_left)\n\t            x_bottom_right = self.base_transforms(x_bottom_right)\n\t        crops = [\n\t            x_top_left,\n", "            x_top_right,\n\t            x_bottom_left,\n\t            x_bottom_right,\n\t        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n\tclass TTAHorizontalStackTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n\t        super().__init__()\n\t        self.base_transforms = base_transforms\n", "    def forward(self, x) -> torch.Tensor:\n\t        *_, h, w = x.shape\n\t        x_left = x[..., :, : w // 2]  # left\n\t        x_right = x[..., :, w // 2 :]  # right\n\t        if self.base_transforms is not None:\n\t            x_left = self.base_transforms(x_left)\n\t            x_right = self.base_transforms(x_right)\n\t        crops = [\n\t            x_left,\n\t            x_right,\n", "        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n\tclass TTAVerticalStackTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n\t        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n\t        *_, h, w = x.shape\n\t        x_top = x[..., : h // 2, :]  # top\n", "        x_bottom = x[..., h // 2 :, :]  # bottom\n\t        if self.base_transforms is not None:\n\t            x_top = self.base_transforms(x_top)\n\t            x_bottom = self.base_transforms(x_bottom)\n\t        crops = [\n\t            x_top,\n\t            x_bottom,\n\t        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/vsc/metrics.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport collections\n\timport dataclasses\n\timport enum\n\timport itertools\n\tfrom collections import defaultdict\n\tfrom math import sqrt\n", "from typing import Collection, Dict, List, NamedTuple, Optional, TextIO, Tuple, Union\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport pandas as pd\n\tfrom sklearn.metrics import average_precision_score\n\tclass Dataset(enum.Enum):\n\t    QUERIES = \"Q\"\n\t    REFS = \"R\"\n\tdef format_video_id(video_id: Union[str, int], dataset: Optional[Dataset]) -> str:\n\t    if isinstance(video_id, (int, np.integer)):\n", "        if dataset is None:\n\t            raise ValueError(\n\t                \"Unable to convert integer video_id without a Dataset enum\"\n\t            )\n\t        return f\"{dataset.value}{video_id:06d}\"\n\t    assert isinstance(\n\t        video_id, str\n\t    ), f\"unexpected video_id: {video_id} of type {type(video_id)}\"\n\t    if dataset is not None:\n\t        assert (\n", "            video_id[0] == dataset.value\n\t        ), f\"dataset mismatch? got {video_id} for dataset {dataset}\"\n\t    return video_id\n\t@dataclasses.dataclass\n\tclass CandidatePair:\n\t    query_id: str\n\t    ref_id: str\n\t    score: float\n\t    @classmethod\n\t    def to_dataframe(\n", "        cls,\n\t        candidates: Collection[\"CandidatePair\"],\n\t    ) -> pd.DataFrame:\n\t        return pd.DataFrame(\n\t            [\n\t                {\n\t                    \"query_id\": format_video_id(c.query_id, Dataset.QUERIES),\n\t                    \"ref_id\": format_video_id(c.ref_id, Dataset.REFS),\n\t                    \"score\": c.score,\n\t                }\n", "                for c in candidates\n\t            ],\n\t        )\n\t    @classmethod\n\t    def write_csv(\n\t        cls, candidates: Collection[\"CandidatePair\"], file: Union[str, TextIO]\n\t    ):\n\t        df = cls.to_dataframe(candidates)\n\t        df.to_csv(file, index=False)\n\t    @classmethod\n", "    def read_csv(cls, file: Union[str, TextIO]) -> List[\"CandidatePair\"]:\n\t        df = pd.read_csv(file)\n\t        pairs = []\n\t        for _, row in df.iterrows():\n\t            query_id = format_video_id(row.query_id, Dataset.QUERIES)\n\t            ref_id = format_video_id(row.ref_id, Dataset.REFS)\n\t            pairs.append(\n\t                CandidatePair(query_id=query_id, ref_id=ref_id, score=row.score)\n\t            )\n\t        return pairs\n", "    @classmethod\n\t    def from_matches(cls, matches: Collection[\"Match\"]) -> List[\"CandidatePair\"]:\n\t        scores = collections.defaultdict(float)\n\t        for match in matches:\n\t            key = (match.query_id, match.ref_id)\n\t            scores[key] = max(match.score, scores[key])\n\t        return [\n\t            CandidatePair(query_id=query_id, ref_id=ref_id, score=score)\n\t            for ((query_id, ref_id), score) in scores.items()\n\t        ]\n", "@dataclasses.dataclass\n\tclass PrecisionRecallCurve:\n\t    precisions: np.ndarray\n\t    recalls: np.ndarray\n\t    scores: np.ndarray\n\t    def plot(self, ax=None, **kwargs):\n\t        if ax is None:\n\t            _, ax = plt.subplots()\n\t            ax.set_xlabel(\"recall\")\n\t            ax.set_ylabel(\"precision\")\n", "            ax.set_xlim(0, 1.05)\n\t            ax.set_ylim(0, 1.05)\n\t        ax.plot(self.recalls, self.precisions, **kwargs)\n\t        return ax\n\t@dataclasses.dataclass\n\tclass AveragePrecision:\n\t    ap: float\n\t    pr_curve: PrecisionRecallCurve\n\t    simple_ap: Optional[float] = None\n\tclass Intervals:\n", "    # Non-overlapping, ordered by interval start.\n\t    intervals: List[Tuple[float, float]]\n\t    def __init__(self, intervals: Optional[List[Tuple[float, float]]] = None):\n\t        self.intervals = intervals or []\n\t        self._dedup()\n\t    def add(self, interval: Tuple[float, float]):\n\t        \"\"\"Add an interval.\"\"\"\n\t        self.intervals.append(interval)\n\t        self._dedup()\n\t    def union(self, intervals: \"Intervals\") -> \"Intervals\":\n", "        return Intervals(self.intervals + intervals.intervals)\n\t    def total_length(self) -> float:\n\t        length = 0.0\n\t        for start, end in self.intervals:\n\t            length += end - start\n\t        return length\n\t    def intersect_length(self, intervals: \"Intervals\") -> float:\n\t        \"\"\"Compute the total_length of the intersection of two Intervals.\n\t        This works by taking the sum of their lengths, and subtracting\n\t        the length of their union.\n", "        |A n B| = |A| + |B| - |A U B|\n\t        \"\"\"\n\t        union = self.union(intervals)\n\t        return self.total_length() + intervals.total_length() - union.total_length()\n\t    def _dedup(self):\n\t        if len(self.intervals) <= 1:\n\t            return\n\t        deduped = []\n\t        intervals = sorted(self.intervals)\n\t        current_start, current_end = intervals[0]\n", "        for start, end in intervals[1:]:\n\t            if start <= current_end:\n\t                # Overlap case\n\t                current_end = max(end, current_end)\n\t            else:\n\t                # Non-overlap case\n\t                deduped.append((current_start, current_end))\n\t                current_start, current_end = start, end\n\t        deduped.append((current_start, current_end))\n\t        self.intervals = deduped\n", "    def __str__(self):\n\t        return str(self.intervals)\n\t    __repr__ = __str__\n\tclass Axis(enum.Enum):\n\t    QUERY = enum.auto()\n\t    REF = enum.auto()\n\tclass Match(NamedTuple):\n\t    \"\"\"A ground-truth match or predicted match.\"\"\"\n\t    query_id: str\n\t    ref_id: str\n", "    score: float\n\t    query_start: float\n\t    query_end: float\n\t    ref_start: float\n\t    ref_end: float\n\t    def pair_id(self):\n\t        return (self.query_id, self.ref_id)\n\t    def interval(self, axis: Axis) -> Tuple[float, float]:\n\t        if axis == Axis.QUERY:\n\t            return (self.query_start, self.query_end)\n", "        else:\n\t            return (self.ref_start, self.ref_end)\n\t    def intersection_area(self, bbox: \"Match\") -> float:\n\t        # Compute the intersection boarders\n\t        inter_q_start = max(self.query_start, bbox.query_start)\n\t        inter_r_start = max(self.ref_start, bbox.ref_start)\n\t        inter_q_end = min(self.query_end, bbox.query_end)\n\t        inter_r_end = min(self.ref_end, bbox.ref_end)\n\t        # Compute the area of intersection rectangle\n\t        return abs(\n", "            max((inter_q_end - inter_q_start, 0))\n\t            * max((inter_r_end - inter_r_start), 0)\n\t        )\n\t    def overlaps(self, bbox: \"Match\") -> bool:\n\t        return self.intersection_area(bbox) > 0.0\n\t    @classmethod\n\t    def write_csv(\n\t        cls, matches: Collection[\"Match\"], file: Union[str, TextIO], drop_dup=False\n\t    ):\n\t        df = pd.DataFrame([match._asdict() for match in matches], columns=cls._fields)\n", "        if drop_dup:\n\t            df[\"score\"] = df.groupby(\n\t                [\n\t                    \"query_id\",\n\t                    \"ref_id\",\n\t                    \"query_start\",\n\t                    \"query_end\",\n\t                    \"ref_start\",\n\t                    \"ref_end\",\n\t                ]\n", "            )[\"score\"].transform(\"max\")\n\t            df.drop_duplicates(\n\t                [\n\t                    \"query_id\",\n\t                    \"ref_id\",\n\t                    \"query_start\",\n\t                    \"query_end\",\n\t                    \"ref_start\",\n\t                    \"ref_end\",\n\t                ],\n", "                keep=\"first\",\n\t                inplace=True,\n\t            )\n\t        df = df.sort_values(by=\"score\", ascending=False)\n\t        df.to_csv(file, index=False)\n\t    @classmethod\n\t    def read_csv(\n\t        cls, file: Union[str, TextIO], is_gt=False, check=True\n\t    ) -> List[\"Match\"]:\n\t        df = pd.read_csv(file)\n", "        df[\"query_id\"] = df.query_id.map(lambda x: format_video_id(x, Dataset.QUERIES))\n\t        df[\"ref_id\"] = df.ref_id.map(lambda x: format_video_id(x, Dataset.REFS))\n\t        if is_gt:\n\t            df[\"score\"] = 1.0\n\t        if check:\n\t            for field in cls._fields:\n\t                assert not df[field].isna().any()\n\t        return [Match(**record) for record in df.to_dict(\"records\")]\n\tclass VideoPair:\n\t    \"\"\"A video pair item that contains information regarding the gt and pred bboxes.\n", "    Provide functionalities for the combination of new predictions with the\n\t    existing ones and the computation of their intersection with the gt bboxes,\n\t    ignoring the gt bboxes that do not overlap with any prediction.\n\t    \"\"\"\n\t    gts: List[Match]\n\t    preds: List[Match]\n\t    def __init__(\n\t        self,\n\t    ):\n\t        self.intersections = {axis: 0.0 for axis in Axis}\n", "        self.totals = {axis: 0.0 for axis in Axis}\n\t        self.gts = []\n\t        self.preds = []\n\t    def total_gt_length(self, axis: Axis) -> float:\n\t        return Intervals([gt.interval(axis) for gt in self.gts]).total_length()\n\t    def total_pred_length(self, axis: Axis) -> float:\n\t        return Intervals([pred.interval(axis) for pred in self.preds]).total_length()\n\t    def gt_overlaps(self, gt: Match) -> bool:\n\t        \"\"\"Checks if the provided gt bbox overlaps with at least one pred bbox.\"\"\"\n\t        for pred in self.preds:\n", "            if gt.overlaps(pred):\n\t                return True\n\t        return False\n\t    def add_gt(self, bbox: Match):\n\t        self.gts.append(bbox)\n\t    def add_prediction(\n\t        self, bbox: Match\n\t    ) -> Tuple[Dict[Axis, float], Dict[Axis, float]]:\n\t        \"\"\"Add a prediction to the corresponding list and calculates the\n\t        differences in the intersections with the gt and the total video\n", "        length covered for both query and reference axes.\n\t        \"\"\"\n\t        self.preds.append(bbox)\n\t        # A subset of GTs to consider for intersection (but not total GT length).\n\t        gts_to_consider = [gt for gt in self.gts if self.gt_overlaps(gt)]\n\t        intersect_deltas = {}\n\t        total_deltas = {}\n\t        for axis in Axis:\n\t            pred_ints = Intervals([pred.interval(axis) for pred in self.preds])\n\t            gt_ints = Intervals([gt.interval(axis) for gt in gts_to_consider])\n", "            # New intersection and total length on this axis\n\t            intersect_length = pred_ints.intersect_length(gt_ints)\n\t            prediction_length = pred_ints.total_length()\n\t            # Compute differences\n\t            intersect_deltas[axis] = intersect_length - self.intersections[axis]\n\t            total_deltas[axis] = prediction_length - self.totals[axis]\n\t            # Update with new values\n\t            self.intersections[axis] = intersect_length\n\t            self.totals[axis] = prediction_length\n\t        return intersect_deltas, total_deltas\n", "def match_metric(\n\t    gts: Collection[Match],\n\t    predictions: Collection[Match],\n\t) -> AveragePrecision:\n\t    \"\"\"Matching track metric:\n\t    Computes the AP based on the VCSL approach for the\n\t    calculation of Precision and Recall.\n\t    AP = \\sum_{i=1}^N P(i) R(i)\n\t    where, P(i) = sqrt(P_q * P_r) and R(i) = sqrt(R_q * R_r)\n\t    calculated as in the VCSL.\n", "    \"\"\"  # noqa: W605\n\t    predictions = sorted(predictions, key=lambda x: x.score, reverse=True)\n\t    # Initialize video pairs and load their gt bboxs\n\t    video_pairs = defaultdict(VideoPair)\n\t    for gt in gts:\n\t        video_pairs[gt.pair_id()].add_gt(gt)\n\t    # Get the total gt length for each axis\n\t    gt_total_lengths = {axis: 0.0 for axis in Axis}\n\t    for _, v in video_pairs.items():\n\t        for axis in Axis:\n", "            gt_total_lengths[axis] += v.total_gt_length(axis)\n\t    # Loop through the predictions\n\t    recall = 0.0\n\t    metric = 0.0\n\t    intersections = {axis: 0.0 for axis in Axis}\n\t    totals = {axis: 0.0 for axis in Axis}\n\t    pr_recalls = []\n\t    pr_precisions = []\n\t    pr_scores = []\n\t    # Update metrics for all predictions with the same score as a group.\n", "    for score, prediction_group in itertools.groupby(\n\t        predictions, key=lambda x: x.score\n\t    ):\n\t        for prediction in prediction_group:\n\t            # Given new predictions, we only need the differences in the intersection with\n\t            # gt and total video length covered for both query and reference axes.\n\t            # This derives from the sum of differences for every pair id\n\t            intersection_deltas, total_deltas = video_pairs[\n\t                prediction.pair_id()\n\t            ].add_prediction(prediction)\n", "            for axis in Axis:\n\t                # Accumulate the differences to the corresponding values\n\t                intersections[axis] += intersection_deltas[axis]\n\t                totals[axis] += total_deltas[axis]\n\t        recalls = {}\n\t        precisions = {}\n\t        for axis in Axis:\n\t            recalls[axis] = intersections[axis] / gt_total_lengths[axis]\n\t            precisions[axis] = intersections[axis] / totals[axis]\n\t        new_recall = sqrt(recalls[Axis.QUERY] * recalls[Axis.REF])\n", "        precision = sqrt(precisions[Axis.QUERY] * precisions[Axis.REF])\n\t        # Compute metric\n\t        delta_recall = new_recall - recall\n\t        metric += precision * delta_recall\n\t        recall = new_recall\n\t        if delta_recall > 0:\n\t            pr_recalls.append(recall)\n\t            pr_precisions.append(precision)\n\t            pr_scores.append(score)\n\t    curve = PrecisionRecallCurve(\n", "        np.array(pr_precisions), np.array(pr_recalls), np.array(pr_scores)\n\t    )\n\t    return AveragePrecision(metric, curve)\n\t@dataclasses.dataclass\n\tclass MatchingTrackMetrics:\n\t    # Our main evaluation metric.\n\t    segment_ap: AveragePrecision\n\t    # This metric reflects only pairwise matching, and not localization.\n\t    pairwise_micro_ap: AveragePrecision\n\tdef evaluate_matching_track(\n", "    ground_truth_filename: str, predictions_filename: str\n\t) -> MatchingTrackMetrics:\n\t    \"\"\"Matching track evaluation.\n\t    Predictions are expected to be a CSV file, with a column names in the header.\n\t    The following columns must be present, in any order:\n\t        query_id: str, the ID of the query for this match\n\t        ref_id: str, the ID of the reference for this match\n\t        query_start: float, the start of the query segment in seconds\n\t        query_end: float, the end of the query segment in seconds\n\t        ref_start: float, the start of the reference segment in seconds\n", "        ref_end: float, the end of the reference segment in seconds\n\t        score: float, the score of this prediction (a higher score indicates a\n\t            more confident prediction)\n\t    Note that ground-truth matches are specified using the same format, but score\n\t    is not used.\n\t    \"\"\"\n\t    gt = Match.read_csv(ground_truth_filename, is_gt=True)\n\t    predictions = Match.read_csv(predictions_filename)\n\t    metric = match_metric(gt, predictions)\n\t    # Auxiliary metric: pairwise uAP\n", "    gt_pairs = CandidatePair.from_matches(gt)\n\t    pairs = CandidatePair.from_matches(predictions)\n\t    pair_ap = average_precision(gt_pairs, pairs)\n\t    return MatchingTrackMetrics(segment_ap=metric, pairwise_micro_ap=pair_ap)\n\tdef average_precision(\n\t    ground_truth: Collection[CandidatePair], predictions: Collection[CandidatePair]\n\t) -> AveragePrecision:\n\t    gt_pairs = {(pair.query_id, pair.ref_id) for pair in ground_truth}\n\t    if len(gt_pairs) != len(ground_truth):\n\t        raise AssertionError(\"Duplicates detected in ground truth\")\n", "    predicted_pairs = {(pair.query_id, pair.ref_id) for pair in predictions}\n\t    if len(predicted_pairs) != len(predictions):\n\t        raise AssertionError(\"Duplicates detected in predictions\")\n\t    # AP calculation that aligns with DrivenData's backend implementation.\n\t    canonical_ap = drivendata_average_precision(\n\t        predicted=CandidatePair.to_dataframe(predictions),\n\t        ground_truth=CandidatePair.to_dataframe(ground_truth),\n\t    )\n\t    predictions = sorted(predictions, key=lambda x: x.score, reverse=True)\n\t    scores = np.array([pair.score for pair in predictions])\n", "    correct = np.array(\n\t        [(pair.query_id, pair.ref_id) in gt_pairs for pair in predictions]\n\t    )\n\t    total_pairs = len(gt_pairs)\n\t    # precision = correct_so_far / total_pairs_so_far\n\t    cumulative_correct = np.cumsum(correct)\n\t    cumulative_predicted = np.arange(len(correct)) + 1\n\t    recall = cumulative_correct / total_pairs\n\t    precision = cumulative_correct / cumulative_predicted\n\t    # Simple AP computation.\n", "    simple_ap = np.sum(precision * correct) / total_pairs\n\t    # Get precision and recall where correct is true\n\t    indices = np.nonzero(correct)[0]\n\t    curve = PrecisionRecallCurve(precision[indices], recall[indices], scores[indices])\n\t    return AveragePrecision(ap=canonical_ap, pr_curve=curve, simple_ap=simple_ap)\n\tdef drivendata_average_precision(\n\t    predicted: pd.DataFrame,\n\t    ground_truth: pd.DataFrame,\n\t):\n\t    \"\"\"Canonical AP implementation used for the challenge.\"\"\"\n", "    SCORE_COL = \"score\"\n\t    QUERY_ID_COL = \"query_id\"\n\t    DATABASE_ID_COL = \"ref_id\"\n\t    actual = ground_truth[[\"query_id\", \"ref_id\"]]\n\t    if (\n\t        not np.isfinite(predicted[SCORE_COL]).all()\n\t        or np.isnan(predicted[SCORE_COL]).any()\n\t    ):\n\t        raise ValueError(\"Scores must be finite.\")\n\t    predicted = predicted.sort_values(SCORE_COL, ascending=False)\n", "    merged = predicted.merge(\n\t        right=actual.assign(actual=1.0),\n\t        how=\"left\",\n\t        on=[QUERY_ID_COL, DATABASE_ID_COL],\n\t    ).fillna({\"actual\": 0.0})\n\t    # We may not predict for every ground truth, so calculate unadjusted AP then adjust it\n\t    unadjusted_ap = (\n\t        average_precision_score(merged[\"actual\"].values, merged[SCORE_COL].values)\n\t        if merged[\"actual\"].sum()\n\t        else 0.0\n", "    )\n\t    # Rescale average precisions based on total ground truth positive counts\n\t    predicted_n_pos = int(merged[\"actual\"].sum())\n\t    # avoid rows added to validate query ids, will have blank ref_id\n\t    actual_n_pos = int(actual[DATABASE_ID_COL].notna().sum())\n\t    adjusted_ap = unadjusted_ap * (predicted_n_pos / actual_n_pos)\n\t    return adjusted_ap\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/vsc/candidates.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import List\n\timport numpy as np\n\tfrom src.vsc.index import PairMatches, VideoFeature, VideoIndex\n\tfrom src.vsc.metrics import CandidatePair\n\tclass ScoreAggregation(ABC):\n", "    @abstractmethod\n\t    def aggregate(self, match: PairMatches) -> float:\n\t        pass\n\t    def score(self, match: PairMatches) -> CandidatePair:\n\t        score = self.aggregate(match)\n\t        return CandidatePair(query_id=match.query_id, ref_id=match.ref_id, score=score)\n\tclass MaxScoreAggregation(ScoreAggregation):\n\t    def aggregate(self, match: PairMatches) -> float:\n\t        return np.max([m.score for m in match.matches])\n\tclass CandidateGeneration:\n", "    def __init__(self, references: List[VideoFeature], aggregation: ScoreAggregation):\n\t        self.aggregation = aggregation\n\t        dim = references[0].dimensions()\n\t        self.index = VideoIndex(dim)\n\t        self.index.add(references)\n\t    def query(self, queries: List[VideoFeature], global_k: int) -> List[CandidatePair]:\n\t        matches = self.index.search(queries, global_k=global_k)\n\t        candidates = [self.aggregation.score(match) for match in matches]\n\t        candidates = sorted(candidates, key=lambda match: match.score, reverse=True)\n\t        return candidates\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/vsc/storage.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\tfrom typing import List, Optional\n\timport numpy as np\n\tfrom src.vsc.index import VideoFeature\n\tfrom src.vsc.metrics import Dataset, format_video_id\n\tdef store_features(f, features: List[VideoFeature], dataset: Optional[Dataset] = None):\n\t    video_ids = []\n", "    feats = []\n\t    timestamps = []\n\t    for feature in features:\n\t        video_id = format_video_id(feature.video_id, dataset)\n\t        video_ids.append(np.full(len(feature), video_id))\n\t        feats.append(feature.feature)\n\t        timestamps.append(feature.timestamps)\n\t    video_ids = np.concatenate(video_ids)\n\t    feats = np.concatenate(feats)\n\t    timestamps = np.concatenate(timestamps)\n", "    np.savez(f, video_ids=video_ids, features=feats, timestamps=timestamps)\n\tdef same_value_ranges(values):\n\t    start = 0\n\t    value = values[start]\n\t    for i, v in enumerate(values):\n\t        if v == value:\n\t            continue\n\t        yield value, start, i\n\t        start = i\n\t        value = values[start]\n", "    yield value, start, len(values)\n\tdef load_features(f, dataset: Optional[Dataset] = None):\n\t    data = np.load(f, allow_pickle=False)\n\t    video_ids = data[\"video_ids\"]\n\t    feats = data[\"features\"]\n\t    timestamps = data[\"timestamps\"]\n\t    ts_dims = len(timestamps.shape)\n\t    if timestamps.shape[0] != feats.shape[0]:\n\t        raise ValueError(\n\t            f\"Expected the same number of timestamps as features: got \"\n", "            f\"{timestamps.shape[0]} timestamps for {feats.shape[0]} features\"\n\t        )\n\t    if not (ts_dims == 1 or timestamps.shape[1:] == (2,)):\n\t        print(f\"timestamps.shape[1:]: {timestamps.shape[1:]}\")\n\t        print(f\"timestamps.shape[1:] == [2]: {timestamps.shape[1:] == [2]}\")\n\t        raise ValueError(f\"Unexpected timestamp shape. Got {timestamps.shape}\")\n\t    results = []\n\t    for video_id, start, end in same_value_ranges(video_ids):\n\t        video_id = format_video_id(video_id, dataset)\n\t        results.append(\n", "            VideoFeature(\n\t                video_id=video_id,\n\t                timestamps=timestamps[start:end],\n\t                feature=feats[start:end, :],\n\t            )\n\t        )\n\t    return results\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/vsc/index.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport collections\n\timport logging\n\tfrom dataclasses import dataclass\n\tfrom typing import Iterable, List, NamedTuple, Tuple\n\timport faiss  # @manual\n\timport numpy as np\n", "from faiss.contrib import exhaustive_search  # @manual\n\tSearchIndices = Tuple[int, int, float]\n\t@dataclass\n\tclass VideoMetadata:\n\t    video_id: str\n\t    timestamps: np.ndarray  # either Nx2 (start and end timestamps) or N\n\t    def __len__(self):\n\t        return self.timestamps.shape[0]\n\t    def get_timestamps(self, idx: int) -> Tuple[float, float]:\n\t        t = self.timestamps[idx]\n", "        if len(self.timestamps.shape) == 1:\n\t            return (t, t)\n\t        return (t[0], t[1])\n\t@dataclass\n\tclass VideoFeature(VideoMetadata):\n\t    feature: np.ndarray\n\t    def __post_init__(self):\n\t        assert self.feature.shape[0] == len(\n\t            self.timestamps\n\t        ), \"Mismatched timestamps / feature size\"\n", "    def metadata(self):\n\t        return VideoMetadata(video_id=self.video_id, timestamps=self.timestamps)\n\t    def dimensions(self):\n\t        return self.feature.shape[1]\n\tclass PairMatch(NamedTuple):\n\t    query_timestamps: Tuple[float, float]\n\t    ref_timestamps: Tuple[float, float]\n\t    score: float\n\t@dataclass\n\tclass PairMatches:\n", "    query_id: str\n\t    ref_id: str\n\t    matches: List[PairMatch]\n\t    def records(self):\n\t        for match in self.matches:\n\t            yield {\n\t                \"query_id\": self.query_id,\n\t                \"ref_id\": self.ref_id,\n\t                \"query_start\": match.query_timestamps[0],\n\t                \"query_end\": match.query_timestamps[1],\n", "                \"ref_start\": match.ref_timestamps[0],\n\t                \"ref_end\": match.ref_timestamps[1],\n\t                \"score\": match.score,\n\t            }\n\tclass VideoIndex:\n\t    def __init__(\n\t        self,\n\t        dim: int,\n\t        codec_str: str = \"Flat\",\n\t        metric: int = faiss.METRIC_INNER_PRODUCT,\n", "    ):\n\t        self.dim = dim\n\t        self.index = faiss.index_factory(self.dim, codec_str, metric)\n\t        self.video_clip_idx = []\n\t        self.video_clip_to_video_ids = []\n\t        self.video_metadata = {}\n\t    def add(self, db: List[VideoFeature]):\n\t        for vf in db:\n\t            self.video_clip_idx.extend(list(range(vf.feature.shape[0])))\n\t            self.video_clip_to_video_ids.extend(\n", "                [vf.video_id for _ in range(vf.feature.shape[0])]\n\t            )\n\t            self.video_metadata[vf.video_id] = vf.metadata()\n\t            self.index.add(vf.feature)\n\t    def search(\n\t        self,\n\t        queries: List[VideoFeature],\n\t        global_k: int,\n\t    ) -> List[PairMatches]:\n\t        query_ids = []\n", "        query_indices = []\n\t        for q in queries:\n\t            query_ids.extend([q.video_id] * len(q))\n\t            query_indices.extend(range(len(q)))\n\t        query_metadatas = {q.video_id: q.metadata() for q in queries}\n\t        query_features = np.concatenate([q.feature for q in queries])\n\t        if global_k < 0:\n\t            # Negative values cause us to use vanilla KNN search\n\t            k = -global_k\n\t            logging.warn(\n", "                \"Using local k for KNN search. Warning: this is against the \"\n\t                \"VSC rules, since predictions for a query-ref pair are not \"\n\t                \"independent of other references. KNN search is provided for \"\n\t                \"comparison.\"\n\t            )\n\t            search_indices = self._knn_search(query_features, k)\n\t        else:\n\t            search_indices = self._global_threshold_knn_search(query_features, global_k)\n\t        pair_nns = collections.defaultdict(list)\n\t        for i, j, score in search_indices:\n", "            query_id = query_ids[i]\n\t            query_idx = query_indices[i]\n\t            query_metadata = query_metadatas[query_id]\n\t            ref_id = self.video_clip_to_video_ids[j]\n\t            ref_idx = self.video_clip_idx[j]\n\t            ref_metadata = self.video_metadata[ref_id]\n\t            match = PairMatch(\n\t                query_timestamps=query_metadata.get_timestamps(query_idx),\n\t                ref_timestamps=ref_metadata.get_timestamps(ref_idx),\n\t                score=score,\n", "            )\n\t            pair_nns[query_id, ref_id].append(match)\n\t        return [\n\t            PairMatches(query_id, ref_id, matches)\n\t            for ((query_id, ref_id), matches) in pair_nns.items()\n\t        ]\n\t    def _global_threshold_knn_search(\n\t        self, query_features: np.ndarray, global_k: int\n\t    ) -> Iterable[SearchIndices]:\n\t        use_similarity = self.index.metric_type == faiss.METRIC_INNER_PRODUCT\n", "        initial_radius = -1e10 if use_similarity else 1e10\n\t        _, limits, similarity, indices = exhaustive_search.range_search_max_results(\n\t            self.index,\n\t            exhaustive_search.exponential_query_iterator(query_features),\n\t            initial_radius,\n\t            max_results=2 * global_k,\n\t            min_results=global_k,\n\t            ngpu=-1,  # use GPU if available\n\t        )\n\t        nq = query_features.shape[0]\n", "        search_indices = []\n\t        for i in range(nq):\n\t            for j in range(limits[i], limits[i + 1]):\n\t                search_indices.append((i, indices[j], similarity[j]))\n\t        search_indices.sort(key=lambda x: x[2], reverse=use_similarity)\n\t        if len(search_indices) > global_k:\n\t            search_indices = search_indices[:global_k]\n\t        return search_indices\n\t    def _knn_search(self, query_features: np.ndarray, k) -> Iterable[SearchIndices]:\n\t        index = self.index\n", "        if faiss.get_num_gpus() > 0:\n\t            logging.info(\"Moving index to GPU\")\n\t            index = faiss.index_cpu_to_all_gpus(self.index)\n\t        logging.info(\"Performing KNN search\")\n\t        similarity, ids = index.search(query_features, k)\n\t        for i in range(ids.shape[0]):\n\t            for j in range(ids.shape[1]):\n\t                yield (i, ids[i, j], similarity[i, j])\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/ffmpeg_video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport os\n\timport subprocess\n\timport tempfile\n\tfrom typing import Iterable, Optional, Tuple\n\tfrom PIL import Image\n\tfrom src.video_reader.video_reader import VideoReader\n", "from torchvision.datasets.folder import default_loader\n\tfrom torchvision.io.image import ImageReadMode, read_image\n\tImageT = Image.Image\n\tclass FFMpegVideoReader(VideoReader):\n\t    def __init__(\n\t        self, video_path: str, required_fps: float, output_type: str, ffmpeg_path: str\n\t    ):\n\t        self.ffmpeg_path = ffmpeg_path\n\t        super().__init__(video_path, required_fps, output_type)\n\t    @property\n", "    def fps(self) -> Optional[float]:\n\t        return None\n\t    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n\t        with tempfile.TemporaryDirectory() as dir, open(os.devnull, \"w\") as null:\n\t            subprocess.check_call(\n\t                [\n\t                    self.ffmpeg_path,\n\t                    \"-nostdin\",\n\t                    \"-y\",\n\t                    \"-i\",\n", "                    self.video_path,\n\t                    \"-start_number\",\n\t                    \"0\",\n\t                    \"-q\",\n\t                    \"0\",\n\t                    \"-vf\",\n\t                    \"fps=%f\" % self.required_fps,\n\t                    os.path.join(dir, \"%07d.png\"),\n\t                ],\n\t                stderr=null,\n", "            )\n\t            i = 0\n\t            while True:\n\t                frame_fn = os.path.join(dir, f\"{i:07d}.png\")\n\t                if not os.path.exists(frame_fn):\n\t                    break\n\t                if self.output_type == \"pil\":\n\t                    img = default_loader(frame_fn)\n\t                elif self.output_type == \"tensor\":\n\t                    img = read_image(frame_fn, mode=ImageReadMode.RGB)\n", "                i += 1\n\t                yield ((i - 1) / self.original_fps, i / self.original_fps, img)\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/ffmpeg_py_video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport warnings\n\tfrom typing import Iterable, Optional, Tuple\n\timport ffmpeg\n\timport numpy as np\n\timport torch\n\tfrom PIL import Image\n", "from src.video_reader.video_reader import VideoReader\n\tImageT = Image.Image\n\tclass FFMpegPyVideoReader(VideoReader):\n\t    def __init__(self, video_path: str, required_fps: float, output_type: str):\n\t        super().__init__(video_path, required_fps, output_type)\n\t    @property\n\t    def fps(self) -> Optional[float]:\n\t        return None\n\t    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n\t        buffer, _ = (\n", "            ffmpeg.input(self.video_path)\n\t            .video.filter(\"fps\", self.required_fps)\n\t            .output(\"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\")\n\t            .run(capture_stdout=True, capture_stderr=True)\n\t        )\n\t        meta = ffmpeg.probe(self.video_path)[\"streams\"][0]\n\t        width, height = meta[\"width\"], meta[\"height\"]\n\t        if self.output_type == \"pil\":\n\t            frames = np.frombuffer(buffer, dtype=np.uint8).reshape(-1, height, width, 3)\n\t            for i, frame in enumerate(frames):\n", "                img = Image.fromarray(frame).convert(\"RGB\")\n\t                yield i / self.original_fps, (i + 1) / self.original_fps, img\n\t        elif self.output_type == \"tensor\":\n\t            with warnings.catch_warnings():\n\t                warnings.filterwarnings(\n\t                    \"ignore\", message=\"The given buffer is not writable\"\n\t                )\n\t                frames = (\n\t                    torch.frombuffer(buffer, dtype=torch.uint8)\n\t                    .reshape((-1, height, width, 3))\n", "                    .permute(0, 3, 1, 2)\n\t                )\n\t            for i, frame in enumerate(frames):\n\t                yield i / self.original_fps, (i + 1) / self.original_fps, frame\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/__init__.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import Iterable, Optional, Tuple, Union\n\timport torch\n\tfrom PIL import Image\n\tImageT = Union[Image.Image, torch.Tensor]\n\tclass VideoReader(ABC):\n", "    def __init__(self, video_path: str, required_fps: float, output_type: str) -> None:\n\t        self.video_path = video_path\n\t        self.required_fps = required_fps\n\t        self.output_type = output_type\n\t        self.original_fps = max(1, self.fps) if self.fps else 1\n\t        self.video_frames = None\n\t    @property\n\t    @abstractmethod\n\t    def fps(self) -> Optional[float]:\n\t        pass\n", "    @abstractmethod\n\t    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n\t        \"\"\"\n\t        returns a tuple of [start_time, end_time, Image]\n\t        \"\"\"\n\t        pass\n"]}
{"filename": "meta-vsc-descriptor-runtime/submission_src/src/video_reader/decord_video_reader.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\timport os\n\tfrom typing import Iterable, Optional, Tuple\n\timport decord\n\timport numpy as np\n\timport torch\n\tfrom PIL import Image\n\tfrom src.video_reader.video_reader import VideoReader\n\tfrom torchvision.datasets.folder import default_loader\n", "ImageT = Image.Image\n\tclass DecordVideoReader(VideoReader):\n\t    def __init__(self, video_path: str, required_fps: float, output_type: str):\n\t        super().__init__(video_path, required_fps, output_type)\n\t    @property\n\t    def fps(self) -> Optional[float]:\n\t        return self.required_fps\n\t    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n\t        reader = decord.VideoReader(\n\t            self.video_path,\n", "            ctx=decord.cpu(0),\n\t            num_threads=0,\n\t        )\n\t        n_frames = len(reader)\n\t        timestamps = reader.get_frame_timestamp(np.arange(n_frames))\n\t        end_timestamps = timestamps[:, 1]\n\t        duration = end_timestamps[-1].tolist()\n\t        fps = min(self.required_fps, n_frames / duration)\n\t        count = max(1, np.round(duration * fps))\n\t        step_size = 1 / fps\n", "        frame_pos = (np.arange(count) + 0.5) * step_size\n\t        frame_ids = np.searchsorted(end_timestamps, frame_pos)\n\t        frame_ids = np.minimum(frame_ids, n_frames - 1)\n\t        frames = reader.get_batch(frame_ids).asnumpy()\n\t        for i, frame in enumerate(frames):\n\t            if self.output_type == \"pil\":\n\t                img = Image.fromarray(frame).convert(\"RGB\")\n\t            elif self.output_type == \"tensor\":\n\t                img = torch.as_tensor(frame).permute(2, 0, 1)\n\t            yield (\n", "                i / self.original_fps,\n\t                (i + 1) / self.original_fps,\n\t                img,\n\t            )\n"]}
{"filename": "meta-vsc-descriptor-runtime/runtime/validation.py", "chunked_list": ["#!/usr/bin/env python3\n\t\"\"\"\n\tDescriptor Track validation script\n\t\"\"\"\n\timport logging\n\tfrom argparse import ArgumentParser, Namespace\n\timport numpy as np\n\timport pandas as pd\n\tparser = ArgumentParser()\n\tparser.add_argument(\n", "    \"--query_features\",\n\t    help=\"Path containing query features\",\n\t    type=str,\n\t    required=True,\n\t)\n\tparser.add_argument(\n\t    \"--ref_features\",\n\t    help=\"Path containing reference features\",\n\t    type=str,\n\t    required=True,\n", ")\n\tparser.add_argument(\n\t    \"--query_metadata\",\n\t    help=\"Path containing query metadata\",\n\t    type=str,\n\t    required=True,\n\t)\n\tparser.add_argument(\n\t    \"--ref_metadata\",\n\t    help=\"Path containing reference metadata\",\n", "    type=str,\n\t    required=True,\n\t)\n\tparser.add_argument(\"--subset\", help=\"Path containing query subset ids\", type=str)\n\tlogging.basicConfig(\n\t    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n\t    level=logging.INFO,\n\t    datefmt=\"%Y-%m-%d %H:%M:%S\",\n\t)\n\tlogger = logging.getLogger(\"descriptor_eval_lib.py\")\n", "logger.setLevel(logging.INFO)\n\tclass DataValidationError(AssertionError):\n\t    pass\n\tdef validate_total_descriptors(dataset: str, n_features: int, total_seconds: float):\n\t    if n_features > total_seconds:\n\t        raise DataValidationError(\n\t            f\"Number of {dataset} video features must not exceed one feature per second. \"\n\t            f\"Saw {n_features} vectors, max allowed is {total_seconds}\"\n\t        )\n\tdef validate_lengths(dataset: str, features_npz):\n", "    n_video_ids = len(features_npz[\"video_ids\"])\n\t    n_timestamps = len(features_npz[\"timestamps\"])\n\t    n_features = len(features_npz[\"features\"])\n\t    if not (n_video_ids == n_timestamps == n_features):\n\t        raise DataValidationError(\n\t            f\"Arrays lengths for {dataset} do not match. \"\n\t            f\"video_ids: {n_video_ids}; \"\n\t            f\"timestamps: {n_timestamps}; \"\n\t            f\"features: {n_features}. \"\n\t        )\n", "def validate_descriptor_dtype(dataset: str, features_array: np.ndarray):\n\t    if features_array.dtype != np.float32:\n\t        raise DataValidationError(\n\t            f\"Features array for {dataset} is not float32. \"\n\t            f\"dtype is: {features_array.dtype}\"\n\t        )\n\tdef validate_descriptor_dim(\n\t    dataset: str, features_array: np.ndarray, max_dim: int = 512\n\t):\n\t    if (submitted_dim := features_array.shape[1]) > max_dim:\n", "        raise DataValidationError(\n\t            f\"Features array for {dataset} exceeds max dim of {max_dim}: \"\n\t            f\"submitted dim is {submitted_dim}.\"\n\t        )\n\tdef validate_sorted_ids(dataset: str, video_ids: np.array):\n\t    is_sorted = video_ids[:-1] <= video_ids[1:]\n\t    if not np.all(is_sorted):\n\t        indices = np.argwhere(~is_sorted)\n\t        raise DataValidationError(f\"Video ids not sorted at index {indices[0]}.\")\n\tdef main(args: Namespace):\n", "    query_features = np.load(args.query_features, allow_pickle=False)\n\t    ref_features = np.load(args.ref_features, allow_pickle=False)\n\t    query_meta = pd.read_csv(args.query_metadata)\n\t    ref_meta = pd.read_csv(args.ref_metadata)\n\t    if args.subset:\n\t        subset = pd.read_csv(args.subset)\n\t        query_meta = query_meta.set_index(\"video_id\").loc[subset.video_id]\n\t    query_total_seconds = query_meta.duration_sec.apply(np.ceil).sum()\n\t    ref_total_seconds = ref_meta.duration_sec.apply(np.ceil).sum()\n\t    validate_total_descriptors(\n", "        \"query\", query_features[\"features\"].shape[0], query_total_seconds\n\t    )\n\t    validate_total_descriptors(\n\t        \"reference\", ref_features[\"features\"].shape[0], ref_total_seconds\n\t    )\n\t    validate_lengths(\"query\", query_features)\n\t    validate_lengths(\"reference\", ref_features)\n\t    validate_sorted_ids(\"query\", query_features[\"video_ids\"])\n\t    validate_sorted_ids(\"reference\", ref_features[\"video_ids\"])\n\t    validate_descriptor_dtype(\"query\", query_features[\"features\"])\n", "    validate_descriptor_dtype(\"reference\", ref_features[\"features\"])\n\t    validate_descriptor_dim(\"query\", query_features[\"features\"], max_dim=512)\n\t    validate_descriptor_dim(\"reference\", ref_features[\"features\"], max_dim=512)\n\tif __name__ == \"__main__\":\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "meta-vsc-descriptor-runtime/runtime/conftest.py", "chunked_list": ["def pytest_addoption(parser):\n\t    parser.addoption(\"--submission-path\", action=\"store\", default=\"submission.csv\")\n"]}
{"filename": "meta-vsc-descriptor-runtime/runtime/tests/test_validation.py", "chunked_list": ["import numpy as np\n\timport pytest\n\timport validation\n\t# Run with python -m pytest tests/test_validation.py from runtime/\n\tdef test_dim_too_large():\n\t    features = np.random.randn(10, 513).astype(\"float32\")\n\t    with pytest.raises(validation.DataValidationError):\n\t        validation.validate_descriptor_dim(\"test\", features, max_dim=512)\n\tdef test_bad_dtype():\n\t    features = np.random.randn(10, 64).astype(\"float64\")\n", "    with pytest.raises(validation.DataValidationError):\n\t        validation.validate_descriptor_dtype(\"test\", features)\n\tdef test_unsorted_ids():\n\t    video_ids = np.array([\"Q200001\", \"Q200001\", \"Q200002\", \"Q200001\"])\n\t    with pytest.raises(validation.DataValidationError):\n\t        validation.validate_sorted_ids(\"test\", video_ids)\n\tdef test_total_descriptors():\n\t    features = np.random.randn(100, 64).astype(\"float32\")\n\t    total_seconds = 50\n\t    with pytest.raises(validation.DataValidationError):\n", "        validation.validate_total_descriptors(\"test\", features.shape[0], total_seconds)\n\tdef test_length_validation():\n\t    video_ids = np.array([\"Q200001\", \"Q200001\", \"Q200002\", \"Q200003\"])\n\t    timestamps = np.array([[0, 10], [10, 20], [0, 10]])\n\t    features = np.random.randn(4, 16).astype(\"float32\")\n\t    submission = {\n\t        \"video_ids\": video_ids,\n\t        \"timestamps\": timestamps,\n\t        \"features\": features,\n\t    }\n", "    with pytest.raises(validation.DataValidationError):\n\t        validation.validate_lengths(\"test\", submission)\n\t    timestamps = np.array([[0, 10], [10, 20], [0, 10], [10, 20]])\n\t    features = np.random.randn(3, 16).astype(\"float32\")\n\t    submission = {\n\t        \"video_ids\": video_ids,\n\t        \"timestamps\": timestamps,\n\t        \"features\": features,\n\t    }\n\t    with pytest.raises(validation.DataValidationError):\n", "        validation.validate_lengths(\"test\", submission)\n\t    video_ids = np.array([\"Q200001\", \"Q200001\"])\n\t    timestamps = np.array([[0, 10], [10, 20], [0, 10], [10, 20]])\n\t    submission = {\n\t        \"video_ids\": video_ids,\n\t        \"timestamps\": timestamps,\n\t        \"features\": features,\n\t    }\n\t    with pytest.raises(validation.DataValidationError):\n\t        validation.validate_lengths(\"test\", submission)\n"]}
{"filename": "meta-vsc-descriptor-runtime/runtime/tests/test_packages.py", "chunked_list": ["# adapted from pangeo https://github.com/pangeo-data/pangeo-docker-images/blob/master/tests/test_pangeo-notebook.py\n\timport importlib\n\timport subprocess\n\timport warnings\n\timport pytest\n\tpackages = [\n\t    # these are problem libraries that don't always seem to import, mostly due\n\t    # to dependencies outside the python world\n\t    \"fastai\",\n\t    \"keras\",\n", "    \"numpy\",\n\t    \"pandas\",\n\t    \"scipy\",\n\t    \"sklearn\",  # scikit-learn\n\t    \"tensorflow\",\n\t    \"torch\",  # pytorch\n\t    \"faiss\",\n\t    \"augly\",\n\t    \"PIL\",\n\t    \"PIL.Image\",\n", "]\n\t@pytest.mark.parametrize(\"package_name\", packages, ids=packages)\n\tdef test_import(package_name):\n\t    importlib.import_module(package_name)\n\tdef test_gpu_packages():\n\t    try:\n\t        subprocess.check_call([\"nvidia-smi\"])\n\t        import torch\n\t        assert torch.cuda.is_available()\n\t        import tensorflow as tf\n", "        assert tf.test.is_built_with_cuda()\n\t        assert tf.config.list_physical_devices(\"GPU\")\n\t        import faiss\n\t        assert faiss.get_num_gpus() > 0\n\t    except FileNotFoundError:\n\t        warnings.warn(\n\t            \"Skipping GPU import tests since nvidia-smi is not present on test machine.\"\n\t        )\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/main.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\timport subprocess\n\timport time\n\tfrom pathlib import Path\n\timport numpy as np\n\timport pandas as pd\n\tROOT_DIRECTORY = Path(\"/code_execution\")\n\tDATA_DIRECTORY = Path(\"/data\")\n\tQRY_VIDEOS_DIRECTORY = DATA_DIRECTORY / \"query\"\n", "OUTPUT_FILE = ROOT_DIRECTORY / \"subset_matches.csv\"\n\tQUERY_SUBSET_FILE = DATA_DIRECTORY / \"query_subset.csv\"\n\tQUERY_META_FILE = DATA_DIRECTORY / \"query_metadata.csv\"\n\tREF_META_FILE = DATA_DIRECTORY / \"reference_metadata.csv\"\n\tdef main():\n\t    num_videos = len(pd.read_csv(QUERY_SUBSET_FILE))\n\t    time_deadline_sec = num_videos * (10 + 1)  # 10 sec per video + 1 sec for overhead\n\t    start_time = time.time()\n\t    cmd = f\"\"\"\n\t    conda run --no-capture-output -n condaenv python -m src.inference \\\n", "    --accelerator=cuda --processes=1 --fps 2 \\\n\t    --dataset_path {str(QRY_VIDEOS_DIRECTORY)} \\\n\t    --output_file {str(OUTPUT_FILE)} \\\n\t    --read_type tensor \\\n\t    --video_reader DECORD\n\t    \"\"\"\n\t    subprocess.run(cmd.split())\n\t    end_time = time.time()\n\t    elapsed_time = end_time - start_time\n\t    print(f\"Elapsed time: {elapsed_time} sec. (deadline: {time_deadline_sec} sec)\")\n", "    if elapsed_time > time_deadline_sec:\n\t        print(\"Time limit exceeded.\")\n\t    else:\n\t        print(\"Time limit not exceeded.\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/inference_full.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Inference script.\n\tThis is split into inference and inference_impl to avoid initializing cuda\n\tbefore processes are created that may use cuda, which can lead to errors\n\tin some runtime environments.\n\tWe import inference_impl, which imports libraries that may initialize cuda,\n", "in two circumstances: from worker processes after the main process has\n\tforked workers, or from the main process after worker processes have been\n\tjoined.\n\t\"\"\"\n\timport argparse\n\timport logging\n\timport os\n\timport tempfile\n\tfrom pathlib import Path\n\tfrom typing import List\n", "import pandas as pd\n\timport tqdm\n\tfrom src.inference import Accelerator, VideoReaderType\n\tfrom src.vsc.storage import load_features, store_features\n\tfrom torch import multiprocessing\n\tparser = argparse.ArgumentParser()\n\tinference_parser = parser.add_argument_group(\"Inference\")\n\tinference_parser.add_argument(\"--batch_size\", type=int, default=32)\n\tinference_parser.add_argument(\"--distributed_rank\", type=int, default=0)\n\tinference_parser.add_argument(\"--distributed_size\", type=int, default=1)\n", "inference_parser.add_argument(\"--processes\", type=int, default=1)\n\tinference_parser.add_argument(\n\t    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n\t)\n\tinference_parser.add_argument(\"--output_path\", nargs=\"+\")\n\tinference_parser.add_argument(\"--scratch_path\", required=False)\n\tdataset_parser = parser.add_argument_group(\"Dataset\")\n\t# multiple dataset path\n\tdataset_parser.add_argument(\"--dataset_paths\", nargs=\"+\")\n\tdataset_parser.add_argument(\"--gt_path\")\n", "dataset_parser.add_argument(\"--fps\", default=1, type=float)\n\tdataset_parser.add_argument(\"--len_cap\", type=int)\n\tdataset_parser.add_argument(\"--read_type\", default=\"tensor\", type=str)\n\tdataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")\n\tdataset_parser.add_argument(\n\t    \"--video_reader\", choices=[x.name for x in VideoReaderType], default=\"FFMPEGPY\"\n\t)\n\tdataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")\n\tdataset_parser.add_argument(\"--tta\", action=\"store_true\")\n\tdataset_parser.add_argument(\n", "    \"--mode\", default=\"whole\", choices=[\"whole\", \"eval\", \"test\", \"tune\"]\n\t)\n\tlogging.basicConfig(\n\t    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n\t    level=logging.INFO,\n\t    datefmt=\"%Y-%m-%d %H:%M:%S\",\n\t)\n\tlogger = logging.getLogger(\"inference.py\")\n\tlogger.setLevel(logging.INFO)\n\tdef main(args):\n", "    success = False\n\t    if args.processes > 1 and args.distributed_size > 1:\n\t        raise Exception(\n\t            \"Set either --processes (single-machine distributed) or \"\n\t            \"both --distributed_size and --distributed_rank (arbitrary \"\n\t            \"distributed)\"\n\t        )\n\t    if args.video_reader == \"DECORD\":\n\t        import subprocess\n\t        subprocess.run(\n", "            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n\t            check=True,\n\t        )\n\t    with tempfile.TemporaryDirectory() as tmp_path:\n\t        splits = [\n\t            \"_\".join(p.split(\"/\")[-2:]) for p in args.dataset_paths\n\t        ]  # ./vsc/eval_subset/reference -> eval_subset_reference\n\t        os.makedirs(args.output_path, exist_ok=True)\n\t        if args.scratch_path:\n\t            os.makedirs(args.scratch_path, exist_ok=True)\n", "        else:\n\t            args.scratch_path = tmp_path\n\t        if args.processes > 1:\n\t            processes = []\n\t            logger.info(f\"Spawning {args.processes} processes\")\n\t            accelerator = Accelerator[args.accelerator.upper()]\n\t            backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n\t            multiprocessing.set_start_method(\"spawn\")\n\t            worker_files = []\n\t            try:\n", "                for rank in range(args.processes):\n\t                    output_files = [\n\t                        os.path.join(args.scratch_path, f\"{split}_{rank}.npz\")\n\t                        for split in splits\n\t                    ]\n\t                    worker_files.append(output_files)\n\t                    p = multiprocessing.Process(\n\t                        target=distributed_worker_process,\n\t                        args=(\n\t                            args,\n", "                            rank,\n\t                            args.processes,\n\t                            backend,\n\t                            output_files,\n\t                            args.dataset_paths,\n\t                            args.tta,\n\t                        ),\n\t                    )\n\t                    processes.append(p)\n\t                    p.start()\n", "                worker_success = []\n\t                for p in processes:\n\t                    p.join()\n\t                    worker_success.append(p.exitcode == os.EX_OK)\n\t                success = all(worker_success)\n\t            finally:\n\t                for p in processes:\n\t                    p.kill()\n\t            if success:\n\t                def merge_feature_files(filenames, output_filename: str) -> int:\n", "                    features = []\n\t                    for fn in filenames:\n\t                        features.extend(load_features(fn))\n\t                    features = sorted(features, key=lambda x: x.video_id)\n\t                    store_features(output_filename, features)\n\t                    return len(features)\n\t                output_files_each_split = [list(x) for x in zip(*worker_files)]\n\t                for files, split in zip(output_files_each_split, splits):\n\t                    output_file = os.path.join(args.output_path, f\"{split}.npz\")\n\t                    num_files = merge_feature_files(files, output_file)\n", "                    logger.info(\n\t                        f\"Features for {num_files} videos saved to {output_file}\"\n\t                    )\n\t        else:\n\t            output_files = [\n\t                os.path.join(args.output_path, f\"{split}.npz\") for split in splits\n\t            ]\n\t            worker_process(\n\t                args,\n\t                args.distributed_rank,\n", "                args.distributed_size,\n\t                output_files,\n\t                args.dataset_paths,\n\t                args.tta,\n\t            )\n\t            success = True\n\t    if success:\n\t        logger.info(\"Inference succeeded.\")\n\t    else:\n\t        logger.error(\"Inference FAILED!\")\n", "    if not success or args.gt_path is None:\n\t        return\n\t    logger.info(\"Evaluating results\")\n\t    evaluate(\n\t        queries=load_features(os.path.join(args.output_path, f\"{splits[0]}.npz\")),\n\t        refs=load_features(os.path.join(args.output_path, f\"{splits[1]}.npz\")),\n\t        noises=load_features(os.path.join(args.output_path, f\"{splits[-1]}.npz\")),\n\t        gt_path=args.gt_path,\n\t        output_path=args.output_path,\n\t    )\n", "def distributed_worker_process(pargs, rank, world_size, backend, *args, **kwargs):\n\t    from torch import distributed\n\t    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n\t    os.environ[\"MASTER_PORT\"] = \"19529\"\n\t    distributed.init_process_group(backend, rank=rank, world_size=world_size)\n\t    worker_process(pargs, rank, world_size, *args, **kwargs)\n\tdef worker_process(\n\t    args,\n\t    rank,\n\t    world_size,\n", "    output_files: List[str],\n\t    dataset_paths: List[str],\n\t    tta: bool = False,\n\t):\n\t    from src.inference_impl import VideoDataset, get_device, run_inference\n\t    from src.model import create_model_in_runtime\n\t    from torch.utils.data import DataLoader\n\t    logger.info(f\"Starting worker {rank} of {world_size}.\")\n\t    device = get_device(args, rank, world_size)\n\t    logger.info(\"Loading model\")\n", "    model, transforms = create_model_in_runtime(transforms_device=device)\n\t    model = model.to(device).eval()\n\t    logger.info(\"Setting up dataset\")\n\t    extensions = args.video_extensions.split(\",\")\n\t    video_reader = VideoReaderType[args.video_reader.upper()]\n\t    if tta:\n\t        if len(dataset_paths) == 3:\n\t            do_tta_list = [\n\t                True,\n\t                False,\n", "                False,\n\t            ]\n\t        elif len(dataset_paths) == 4:\n\t            do_tta_list = [\n\t                True,\n\t                False,\n\t                True,\n\t                False,\n\t            ]\n\t        elif len(dataset_paths) == 1:\n", "            do_tta_list = [\n\t                True,\n\t            ]\n\t        else:\n\t            raise ValueError(\"TTA requires 3 or 4 datasets\")\n\t    else:\n\t        do_tta_list = [False] * len(dataset_paths)\n\t    for output_filename, dataset_path, do_tta in zip(\n\t        output_files, dataset_paths, do_tta_list\n\t    ):\n", "        batch_size = 1 if do_tta else args.batch_size\n\t        dataset = VideoDataset(\n\t            dataset_path,\n\t            fps=args.fps,\n\t            read_type=args.read_type,\n\t            batch_size=batch_size,\n\t            extensions=extensions,\n\t            distributed_world_size=world_size,\n\t            distributed_rank=rank,\n\t            video_reader=video_reader,\n", "            ffmpeg_path=args.ffmpeg_path,\n\t            filter_by_asr=do_tta,\n\t        )\n\t        loader = DataLoader(dataset, batch_size=None, pin_memory=device.type == \"cuda\")\n\t        progress = tqdm.tqdm(total=dataset.num_videos())\n\t        video_features = []\n\t        for vf in run_inference(loader, model, device, transforms, do_tta):\n\t            video_features.append(vf)\n\t            progress.update()\n\t        store_features(output_filename, video_features)\n", "def evaluate(queries, refs, noises, gt_path, output_path, sn_method=\"SN\"):\n\t    import faiss\n\t    from src.inference_impl import match\n\t    from src.postproc import sliding_pca, sliding_pca_with_ref\n\t    from src.score_normalization import (\n\t        negative_embedding_subtraction,\n\t        score_normalize_with_ref,\n\t    )\n\t    from src.vsc.metrics import (\n\t        CandidatePair,\n", "        Match,\n\t        average_precision,\n\t        evaluate_matching_track,\n\t    )\n\t    stride = 2\n\t    video_features, pca_matrix = sliding_pca_with_ref(\n\t        queries=queries,\n\t        refs=refs,\n\t        noises=noises,\n\t        stride=stride,\n", "    )\n\t    queries = video_features[\"query\"]\n\t    refs = video_features[\"ref\"]\n\t    noises = video_features[\"noise\"]\n\t    store_features(Path(output_path) / \"noise.npz\", noises)\n\t    faiss.write_VectorTransform(pca_matrix, str(Path(output_path) / \"pca_matrix.bin\"))\n\t    if sn_method == \"SN\":\n\t        queries, refs = score_normalize_with_ref(\n\t            queries=queries,\n\t            refs=refs,\n", "            score_norm_refs=noises,\n\t            beta=1.2,\n\t        )\n\t    else:\n\t        queries, refs = negative_embedding_subtraction(\n\t            queries=queries,\n\t            refs=refs,\n\t            score_norm_refs=noises,\n\t            pre_l2_normalize=False,\n\t            post_l2_normalize=False,\n", "            beta=0.8,\n\t            k=10,\n\t            alpha=2.0,\n\t        )\n\t    store_features(Path(output_path) / \"processed_ref.npz\", refs)\n\t    candidates, matches = match(\n\t        queries=queries,\n\t        refs=refs,\n\t        output_file=None,\n\t        return_results=True,\n", "        similarity_bias=0.5 if sn_method == \"SN\" else 0.0,\n\t        model_type=\"TN\",\n\t        tn_max_step=5,\n\t        min_length=3,\n\t        tn_top_k=2,\n\t        max_path=200,\n\t        min_sim=0.1,\n\t        max_iou=1.0,\n\t    )\n\t    gt_matches = Match.read_csv(gt_path, is_gt=True)\n", "    ap = average_precision(CandidatePair.from_matches(gt_matches), candidates)\n\t    candidates = sorted(candidates, key=lambda x: x.score, reverse=True)\n\t    CandidatePair.write_csv(candidates, Path(output_path) / \"candidates.csv\")\n\t    match_file = Path(output_path) / \"matches.csv\"\n\t    Match.write_csv(matches, match_file, drop_dup=True)\n\t    match_metrics = evaluate_matching_track(gt_path, match_file)\n\t    logger.info(f\"uAP: {ap.ap:.4f}, segmentAP: {match_metrics.segment_ap.ap:.4f}\")\n\tdef tune(\n\t    queries,\n\t    refs,\n", "    noises,\n\t    gt_path,\n\t    output_path,\n\t):\n\t    from sklearn.model_selection import ParameterGrid\n\t    from src.inference_impl import match\n\t    from src.score_normalization import (\n\t        negative_embedding_subtraction,\n\t        score_normalize_with_ref,\n\t    )\n", "    from src.vsc.metrics import (\n\t        CandidatePair,\n\t        Match,\n\t        average_precision,\n\t        evaluate_matching_track,\n\t    )\n\t    param_grid = [\n\t        {\n\t            \"sn_method\": [\"SN\"],\n\t            \"beta\": [1.2],\n", "            # \"k\": [10],\n\t            # \"alpha\": [2.0],\n\t            \"similarity_bias\": [0.5],\n\t            \"retrieve_per_query\": [1200],\n\t            \"candidates_per_query\": [25],\n\t            \"tn_max_step\": [5],\n\t            \"min_length\": [3],\n\t            \"tn_top_k\": [2],\n\t            \"max_path\": [70, 100, 150, 200],\n\t            \"min_sim\": [0.1],\n", "            \"max_iou\": [1.0],\n\t        }\n\t    ]\n\t    rows = []\n\t    for params in ParameterGrid(param_grid):\n\t        if params[\"sn_method\"] == \"SN\":\n\t            norm_queries, norm_refs = score_normalize_with_ref(\n\t                queries=queries,\n\t                refs=refs,\n\t                score_norm_refs=noises,\n", "                beta=params[\"beta\"],\n\t            )\n\t        else:\n\t            norm_queries, norm_refs = negative_embedding_subtraction(\n\t                queries=queries,\n\t                refs=refs,\n\t                score_norm_refs=noises,\n\t                pre_l2_normalize=False,\n\t                post_l2_normalize=False,\n\t                beta=params[\"beta\"],\n", "                k=params[\"k\"],\n\t                alpha=params[\"alpha\"],\n\t            )\n\t        candidates, matches = match(\n\t            queries=norm_queries,\n\t            refs=norm_refs,\n\t            output_file=None,\n\t            return_results=True,\n\t            similarity_bias=params[\"similarity_bias\"],\n\t            model_type=\"TN\",\n", "            tn_max_step=params[\"tn_max_step\"],\n\t            min_length=params[\"min_length\"],\n\t            concurrency=16,\n\t            tn_top_k=params[\"tn_top_k\"],\n\t            max_path=params[\"max_path\"],\n\t            min_sim=params[\"min_sim\"],\n\t            max_iou=params[\"max_iou\"],\n\t            discontinue=3,\n\t            sum_sim=8,\n\t            ave_sim=0.3,\n", "            diagonal_thres=10,\n\t            iou_thresh=0.9,\n\t            min_bins=1,\n\t            max_peaks=100,\n\t            min_peaks=10,\n\t            retrieve_per_query=params[\"retrieve_per_query\"],\n\t            candidates_per_query=params[\"candidates_per_query\"],\n\t        )\n\t        gt_matches = Match.read_csv(gt_path, is_gt=True)\n\t        ap = average_precision(CandidatePair.from_matches(gt_matches), candidates)\n", "        # candidates = sorted(candidates, key=lambda x: x.score, reverse=True)\n\t        # CandidatePair.write_csv(candidates, Path(output_path) / 'candidates.csv')\n\t        print(f\"{len(matches)=}\")\n\t        matches = pd.DataFrame(matches)\n\t        matches = matches[\n\t            [\n\t                \"query_id\",\n\t                \"ref_id\",\n\t                \"query_start\",\n\t                \"query_end\",\n", "                \"ref_start\",\n\t                \"ref_end\",\n\t                \"score\",\n\t            ]\n\t        ]\n\t        matches = matches.sort_values(\"score\", ascending=False)\n\t        match_file = Path(output_path) / \"tune_matches.csv\"\n\t        matches.to_csv(match_file, index=False)\n\t        match_metrics = evaluate_matching_track(gt_path, match_file)\n\t        rows.append(\n", "            {\n\t                \"uAP\": ap.ap,\n\t                \"segmentAP\": match_metrics.segment_ap.ap,\n\t                \"len_matches\": len(matches),\n\t                **params,\n\t            }\n\t        )\n\t        print(rows[-1])\n\t    df = pd.DataFrame(rows)\n\t    print(df.to_csv())\n", "    df.to_csv(\"tuning_result.csv\", index=False)\n\tif __name__ == \"__main__\":\n\t    args = parser.parse_args()\n\t    if args.mode == \"eval\":\n\t        evaluate(\n\t            queries=load_features(os.path.join(args.output_path, f\"train_query.npz\")),\n\t            refs=load_features(os.path.join(args.output_path, f\"train_reference.npz\")),\n\t            noises=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n\t            gt_path=args.gt_path,\n\t            output_path=args.output_path,\n", "            sn_method=\"SN\",\n\t        )\n\t    elif args.mode == \"test\":\n\t        evaluate(\n\t            queries=load_features(\n\t                os.path.join(args.output_path, f\"phase_2_uB82_query.npz\")\n\t            ),\n\t            refs=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n\t            noises=load_features(\n\t                os.path.join(args.output_path, f\"train_reference.npz\")\n", "            ),\n\t            gt_path=args.gt_path,\n\t            output_path=args.output_path,\n\t            sn_method=\"SN\",\n\t        )\n\t    elif args.mode == \"tune\":\n\t        tune(\n\t            queries=load_features(os.path.join(args.output_path, f\"train_query.npz\")),\n\t            refs=load_features(os.path.join(args.output_path, f\"train_reference.npz\")),\n\t            noises=load_features(os.path.join(args.output_path, f\"test_reference.npz\")),\n", "            gt_path=args.gt_path,\n\t            output_path=args.output_path,\n\t        )\n\t    else:\n\t        main(args)\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/postproc.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\tfrom typing import Dict, List\n\timport numpy as np\n\tfrom numpy.lib.stride_tricks import sliding_window_view\n\tfrom src.vsc.index import VideoFeature\n\tdef _sliding_window_and_concat(\n\t    vfs: List[VideoFeature], stride: int = 1\n\t) -> Dict[str, List[VideoFeature]]:\n\t    new_vfs = []\n", "    kernel = [0.1, 0.2, 0.4, 0.2, 0.1]\n\t    num_stacks = len(kernel)\n\t    for vf in vfs:\n\t        n, d = vf.feature.shape\n\t        num_views = 1\n\t        for i in range(1, len(vf.timestamps)):\n\t            if vf.timestamps[i] <= vf.timestamps[i - 1]:\n\t                num_views += 1\n\t        if n / num_views <= stride:\n\t            continue\n", "        reshaped_feats = vf.feature.reshape(num_views, -1, d)\n\t        reshaped_ts = vf.timestamps.reshape(num_views, -1)\n\t        new_feats = []\n\t        new_timestamps = []\n\t        for i in range(num_views):\n\t            new_feat = reshaped_feats[i]\n\t            new_ts = reshaped_ts[i]\n\t            new_feat = np.concatenate(\n\t                [new_feat[: num_stacks // 2], new_feat, new_feat[-(num_stacks // 2) :]],\n\t                axis=0,\n", "            )\n\t            new_feat = sliding_window_view(new_feat, num_stacks, axis=0)\n\t            assert len(new_feat) == len(reshaped_feats[i])\n\t            if stride > 1:\n\t                new_feat = new_feat[stride // 2 :: stride]\n\t                new_ts = new_ts[stride // 2 :: stride]\n\t            weight = np.array(kernel).reshape(1, 1, -1)\n\t            new_feat = new_feat * weight\n\t            new_feat = new_feat.transpose(0, 2, 1).reshape(\n\t                -1, new_feat.shape[1] * num_stacks\n", "            )\n\t            new_feats.append(new_feat)\n\t            new_timestamps.append(new_ts)\n\t        new_feats = np.concatenate(new_feats, axis=0)\n\t        new_timestamps = np.concatenate(new_timestamps, axis=0)\n\t        new_vfs.append(\n\t            VideoFeature(\n\t                video_id=vf.video_id,\n\t                timestamps=new_timestamps,\n\t                feature=new_feats,\n", "            )\n\t        )\n\t    return new_vfs\n\tdef _fit_pca(noises, n_components=512) -> Dict[str, List[VideoFeature]]:\n\t    import faiss\n\t    noise_feats = np.concatenate([vf.feature for vf in noises])\n\t    noise_feats = noise_feats.astype(np.float32)\n\t    mat = faiss.PCAMatrix(noise_feats.shape[-1], n_components)\n\t    mat.train(noise_feats)\n\t    assert mat.is_trained\n", "    return mat\n\tdef _apply_pca(vfs, mat) -> Dict[str, List[VideoFeature]]:\n\t    new_vfs = []\n\t    for vf in vfs:\n\t        new_feat = mat.apply(vf.feature.astype(np.float32))\n\t        # new_feat = new_feat / np.linalg.norm(new_feat, axis=-1, keepdims=True)\n\t        new_vfs.append(\n\t            VideoFeature(\n\t                video_id=vf.video_id,\n\t                timestamps=vf.timestamps,\n", "                feature=new_feat,\n\t            )\n\t        )\n\t    return new_vfs\n\tdef sliding_pca(\n\t    queries: List[VideoFeature],\n\t    mat: \"faiss.PCAMatrix\",\n\t    stride: int = 1,\n\t) -> List[VideoFeature]:\n\t    queries = _sliding_window_and_concat(queries, stride=stride)\n", "    queries = _apply_pca(queries, mat)\n\t    return queries\n\tdef sliding_pca_with_ref(\n\t    queries: List[VideoFeature],\n\t    refs: List[VideoFeature],\n\t    noises: List[VideoFeature],\n\t    stride: int = 1,\n\t    n_components: int = 512,\n\t) -> Dict[str, List[VideoFeature]]:\n\t    video_features = {\n", "        \"query\": _sliding_window_and_concat(queries, stride=stride),\n\t        \"ref\": _sliding_window_and_concat(refs, stride=stride),\n\t        \"noise\": _sliding_window_and_concat(noises, stride=stride),\n\t    }\n\t    mat = _fit_pca(video_features[\"noise\"], n_components=n_components)\n\t    for k, vfs in video_features.items():\n\t        video_features[k] = _apply_pca(vfs, mat)\n\t    return video_features, mat\n\tdef ensemble_match_results(\n\t    path: List[str], output_file: str = \"match_ensemble.csv\"\n", ") -> str:\n\t    import os\n\t    import pandas as pd\n\t    if path[0][-4:] == \".csv\":\n\t        match_result_paths = path\n\t    else:\n\t        match_result_paths = [f\"{p}/matches.csv\" for p in path]\n\t    results = []\n\t    for result_path in match_result_paths:\n\t        results.append(pd.read_csv(result_path))\n", "    df_all = pd.concat(results, axis=0, ignore_index=True)\n\t    df_ensemble = pd.DataFrame(\n\t        columns=[\n\t            \"query_id\",\n\t            \"ref_id\",\n\t            \"query_start\",\n\t            \"query_end\",\n\t            \"ref_start\",\n\t            \"ref_end\",\n\t            \"score\",\n", "        ]\n\t    )\n\t    query_id_all = set(df_all[\"query_id\"])\n\t    for query_id in query_id_all:\n\t        df_topk = df_all.loc[df_all[\"query_id\"] == query_id].sort_values(\n\t            by=\"score\", ascending=False\n\t        )[:400]\n\t        df_ensemble = pd.concat([df_ensemble, df_topk], ignore_index=True)\n\t    df_ensemble[\"score\"] = df_ensemble.groupby(\n\t        [\"query_id\", \"ref_id\", \"query_start\", \"query_end\", \"ref_start\", \"ref_end\"]\n", "    )[\"score\"].transform(\"max\")\n\t    df_ensemble.drop_duplicates(\n\t        [\"query_id\", \"ref_id\", \"query_start\", \"query_end\", \"ref_start\", \"ref_end\"],\n\t        keep=\"first\",\n\t        inplace=True,\n\t    )\n\t    match_file_final = f\"{output_file}\"\n\t    df_ensemble.to_csv(match_file_final, index=False)\n\t    return match_file_final\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/model.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\tfrom __future__ import annotations\n\timport timm\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torchvision import transforms\n\tclass ISCNet(nn.Module):\n\t    \"\"\"\n", "    Feature extractor for image copy-detection task.\n\t    Args:\n\t        backbone (`nn.Module`):\n\t            Backbone module.\n\t        fc_dim (`int=256`):\n\t            Feature dimension of the fc layer.\n\t        p (`float=1.0`):\n\t            Power used in gem pooling for training.\n\t        eval_p (`float=1.0`):\n\t            Power used in gem pooling for evaluation. In practice, using a larger power\n", "            for evaluation than training can yield a better performance.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        backbone: nn.Module,\n\t        fc_dim: int = 256,\n\t        p: float = 1.0,\n\t        eval_p: float = 1.0,\n\t        l2_normalize=True,\n\t    ):\n", "        super().__init__()\n\t        self.backbone = backbone\n\t        if hasattr(backbone, \"num_features\"):\n\t            self.is_cnn = False\n\t            in_channels = backbone.num_features\n\t        else:\n\t            self.is_cnn = True\n\t            in_channels = backbone.feature_info.info[-1][\"num_chs\"]\n\t        self.fc = nn.Linear(in_channels, fc_dim, bias=False)\n\t        self.bn = nn.BatchNorm1d(fc_dim)\n", "        self._init_params()\n\t        self.p = p\n\t        self.eval_p = eval_p\n\t        self.l2_normalize = l2_normalize\n\t    def _init_params(self):\n\t        nn.init.xavier_normal_(self.fc.weight)\n\t        nn.init.constant_(self.bn.weight, 1)\n\t        nn.init.constant_(self.bn.bias, 0)\n\t    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\t        if self.is_cnn:\n", "            batch_size = x.shape[0]\n\t            x = self.backbone(x)[-1]\n\t            p = self.p if self.training else self.eval_p\n\t            x = gem(x, p).view(batch_size, -1)\n\t        else:\n\t            x = self.backbone(x)\n\t        x = self.fc(x)\n\t        x = self.bn(x)\n\t        if self.l2_normalize:\n\t            x = F.normalize(x)\n", "        return x\n\tdef gem(x, p=3, eps=1e-6):\n\t    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)\n\tclass ConstDivider(nn.Module):\n\t    def __init__(self, c=255.0):\n\t        super().__init__()\n\t        self.c = c\n\t    def forward(self, x):\n\t        return x / self.c\n\tdef create_model_in_runtime(transforms_device=\"cpu\"):\n", "    weight_path = \"./model_assets/isc_ft_v107.pth.tar\"\n\t    ckpt = torch.load(weight_path)\n\t    arch = ckpt[\"arch\"]  # tf_efficientnetv2_m_in21ft1k\n\t    input_size = ckpt[\"args\"].input_size\n\t    backbone = timm.create_model(arch, features_only=True)\n\t    model = ISCNet(\n\t        backbone=backbone,\n\t        fc_dim=256,\n\t        p=1.0,\n\t        eval_p=1.0,\n", "        l2_normalize=True,\n\t    )\n\t    model.to(\"cuda\").train(False)\n\t    state_dict = {}\n\t    for s in ckpt[\"state_dict\"]:\n\t        state_dict[s.replace(\"module.\", \"\")] = ckpt[\"state_dict\"][s]\n\t    model.load_state_dict(state_dict)\n\t    if transforms_device == \"cpu\":\n\t        preprocessor = transforms.Compose(\n\t            [\n", "                transforms.Resize((input_size, input_size)),\n\t                # transforms.ToTensor(),\n\t                ConstDivider(c=255.0),\n\t                transforms.Normalize(\n\t                    mean=backbone.default_cfg[\"mean\"],\n\t                    std=backbone.default_cfg[\"std\"],\n\t                ),\n\t            ]\n\t        )\n\t    else:\n", "        preprocessor = nn.Sequential(\n\t            transforms.Resize((input_size, input_size)),\n\t            ConstDivider(c=255.0),\n\t            transforms.Normalize(\n\t                mean=backbone.default_cfg[\"mean\"],\n\t                std=backbone.default_cfg[\"std\"],\n\t            ),\n\t        )\n\t        preprocessor = torch.jit.script(preprocessor)\n\t        preprocessor.to(transforms_device)\n", "    return model, preprocessor\n\tdef create_model_in_runtime_2(transforms_device=\"cpu\"):\n\t    weight_path = \"./model_assets/disc21_ft_vit_base_r50_s16_224_in21k.pth\"\n\t    state_dict = torch.load(weight_path, map_location=\"cpu\")\n\t    arch = \"vit_base_r50_s16_224_in21k\"\n\t    input_size = 448\n\t    feature_dim = 512\n\t    try:\n\t        backbone = timm.create_model(arch, features_only=True, pretrained=False)\n\t    except:\n", "        backbone = timm.create_model(\n\t            arch, pretrained=False, num_classes=0, img_size=(input_size, input_size)\n\t        )\n\t    model = ISCNet(\n\t        backbone=backbone,\n\t        fc_dim=feature_dim,\n\t        p=1.0,\n\t        eval_p=1.0,\n\t        l2_normalize=True,\n\t    )\n", "    model.load_state_dict(state_dict)\n\t    model.to(\"cuda\").train(False)\n\t    if transforms_device == \"cpu\":\n\t        preprocessor = transforms.Compose(\n\t            [\n\t                transforms.Resize((input_size, input_size)),\n\t                # transforms.ToTensor(),\n\t                ConstDivider(c=255.0),\n\t                transforms.Normalize(\n\t                    mean=backbone.default_cfg[\"mean\"],\n", "                    std=backbone.default_cfg[\"std\"],\n\t                ),\n\t            ]\n\t        )\n\t    else:\n\t        preprocessor = nn.Sequential(\n\t            transforms.Resize((input_size, input_size)),\n\t            ConstDivider(c=255.0),\n\t            transforms.Normalize(\n\t                mean=backbone.default_cfg[\"mean\"],\n", "                std=backbone.default_cfg[\"std\"],\n\t            ),\n\t        )\n\t        preprocessor = torch.jit.script(preprocessor)\n\t        preprocessor.to(transforms_device)\n\t    return model, preprocessor\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/inference.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t\"\"\"Inference script.\n\tThis is split into inference and inference_impl to avoid initializing cuda\n\tbefore processes are created that may use cuda, which can lead to errors\n\tin some runtime environments.\n\tWe import inference_impl, which imports libraries that may initialize cuda,\n", "in two circumstances: from worker processes after the main process has\n\tforked workers, or from the main process after worker processes have been\n\tjoined.\n\t\"\"\"\n\timport argparse\n\timport enum\n\timport glob\n\timport logging\n\timport os\n\timport tempfile\n", "from torch import multiprocessing\n\tclass InferenceTransforms(enum.Enum):\n\t    # Aspect-ratio preserving resize to 288\n\t    RESIZE_288 = enum.auto()\n\t    # Resize the short edge to 320, then take the center crop\n\t    RESIZE_320_CENTER = enum.auto()\n\tclass Accelerator(enum.Enum):\n\t    CPU = enum.auto()\n\t    CUDA = enum.auto()\n\tclass VideoReaderType(enum.Enum):\n", "    FFMPEG = enum.auto()\n\t    FFMPEGPY = enum.auto()\n\t    DECORD = enum.auto()\n\tparser = argparse.ArgumentParser()\n\tinference_parser = parser.add_argument_group(\"Inference\")\n\t# inference_parser.add_argument(\"--torchscript_path\", required=True)\n\tinference_parser.add_argument(\"--batch_size\", type=int, default=32)\n\tinference_parser.add_argument(\"--distributed_rank\", type=int, default=0)\n\tinference_parser.add_argument(\"--distributed_size\", type=int, default=1)\n\tinference_parser.add_argument(\"--processes\", type=int, default=1)\n", "inference_parser.add_argument(\n\t    \"--transforms\",\n\t    choices=[x.name for x in InferenceTransforms],\n\t    default=\"RESIZE_320_CENTER\",\n\t)\n\tinference_parser.add_argument(\n\t    \"--accelerator\", choices=[x.name.lower() for x in Accelerator], default=\"cpu\"\n\t)\n\tinference_parser.add_argument(\"--output_file\", required=True)\n\tinference_parser.add_argument(\"--scratch_path\", required=False)\n", "dataset_parser = parser.add_argument_group(\"Dataset\")\n\tdataset_parser.add_argument(\"--dataset_path\", required=True)\n\tdataset_parser.add_argument(\"--fps\", default=1, type=float)\n\tdataset_parser.add_argument(\"--stride\", type=int)\n\tdataset_parser.add_argument(\"--read_type\", default=\"pil\", type=str)\n\tdataset_parser.add_argument(\"--video_extensions\", default=\"mp4\")\n\tdataset_parser.add_argument(\n\t    \"--video_reader\", choices=[x.name for x in VideoReaderType], default=\"FFMPEG\"\n\t)\n\tdataset_parser.add_argument(\"--ffmpeg_path\", default=\"ffmpeg\")\n", "dataset_parser.add_argument(\n\t    \"--score_norm_features\",\n\t    default=[\"./src/isc/test_noise.npz\", \"./src/vit/test_noise.npz\"],\n\t)\n\tdataset_parser.add_argument(\n\t    \"--reference_features\",\n\t    default=[\"./src/isc/test_processed_ref.npz\", \"./src/vit/test_processed_ref.npz\"],\n\t)\n\tdataset_parser.add_argument(\"--model\", default=[\"isc\", \"vit\"])\n\tdataset_parser.add_argument(\n", "    \"--pca_matrix\",\n\t    default=[\"./src/isc/test_pca_matrix.bin\", \"./src/vit/test_pca_matrix.bin\"],\n\t)\n\tdataset_parser.add_argument(\"--gt_path\")\n\tlogging.basicConfig(\n\t    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n\t    level=logging.INFO,\n\t    datefmt=\"%Y-%m-%d %H:%M:%S\",\n\t)\n\tlogger = logging.getLogger(\"inference.py\")\n", "logger.setLevel(logging.INFO)\n\tdef main(args):\n\t    success = False\n\t    if args.processes > 1 and args.distributed_size > 1:\n\t        raise Exception(\n\t            \"Set either --processes (single-machine distributed) or \"\n\t            \"both --distributed_size and --distributed_rank (arbitrary \"\n\t            \"distributed)\"\n\t        )\n\t    if args.video_reader == \"DECORD\":\n", "        import subprocess\n\t        subprocess.run(\n\t            [\"pip\", \"install\", \"wheels/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\"],\n\t            check=True,\n\t        )\n\t    for model, ref_path, noise_path, pca_matrix in zip(\n\t        args.model, args.reference_features, args.score_norm_features, args.pca_matrix\n\t    ):\n\t        with tempfile.TemporaryDirectory() as tmp_path:\n\t            os.makedirs(os.path.dirname(args.output_file), exist_ok=True)\n", "            if args.scratch_path:\n\t                os.makedirs(args.scratch_path, exist_ok=True)\n\t            else:\n\t                args.scratch_path = tmp_path\n\t            if args.processes > 1:\n\t                processes = []\n\t                logger.info(f\"Spawning {args.processes} processes\")\n\t                accelerator = Accelerator[args.accelerator.upper()]\n\t                backend = \"nccl\" if accelerator == Accelerator.CUDA else \"gloo\"\n\t                # multiprocessing.set_start_method(\"spawn\")\n", "                try:\n\t                    multiprocessing.set_start_method(\"spawn\")\n\t                except RuntimeError:\n\t                    pass\n\t                worker_files = []\n\t                try:\n\t                    for rank in range(args.processes):\n\t                        worker_file = os.path.join(args.scratch_path, f\"{rank}.npz\")\n\t                        worker_files.append(worker_file)\n\t                        p = multiprocessing.Process(\n", "                            target=distributed_worker_process,\n\t                            args=(\n\t                                args,\n\t                                rank,\n\t                                args.processes,\n\t                                backend,\n\t                                worker_file,\n\t                                model,\n\t                                ref_path,\n\t                                noise_path,\n", "                                pca_matrix,\n\t                            ),\n\t                        )\n\t                        processes.append(p)\n\t                        p.start()\n\t                    worker_success = []\n\t                    for p in processes:\n\t                        p.join()\n\t                        worker_success.append(p.exitcode == os.EX_OK)\n\t                    success = all(worker_success)\n", "                finally:\n\t                    for p in processes:\n\t                        p.kill()\n\t                if success:\n\t                    from .inference_impl import merge_feature_files  # @manual\n\t                    num_files = merge_feature_files(worker_files, args.output_file)\n\t                    logger.info(\n\t                        f\"Features for {num_files} videos saved to {args.output_file}\"\n\t                    )\n\t            else:\n", "                worker_process(\n\t                    args,\n\t                    args.distributed_rank,\n\t                    args.distributed_size,\n\t                    args.output_file,\n\t                    model,\n\t                    ref_path,\n\t                    noise_path,\n\t                    pca_matrix,\n\t                )\n", "                success = True\n\t    matches_list = glob.glob(f\"{os.path.dirname(args.output_file)}/output/*.csv\")\n\t    from src.postproc import ensemble_match_results\n\t    math_file = ensemble_match_results(matches_list, args.output_file)\n\t    if success:\n\t        logger.info(\"Inference succeeded.\")\n\t    else:\n\t        logger.error(\"Inference FAILED!\")\n\tdef distributed_worker_process(\n\t    args, rank, world_size, backend, output_filename, model, ref_path, noise_path, pca_matrix\n", "):\n\t    from torch import distributed\n\t    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n\t    os.environ[\"MASTER_PORT\"] = \"19529\"\n\t    distributed.init_process_group(backend, rank=rank, world_size=world_size)\n\t    worker_process(\n\t        args, rank, world_size, output_filename, model, ref_path, noise_path, pca_matrix\n\t    )\n\tdef worker_process(*args):\n\t    # Late import: initialize cuda after worker spawn.\n", "    from src.inference_impl import worker_process as worker_impl  # @manual\n\t    return worker_impl(*args)\n\tif __name__ == \"__main__\":\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/inference_impl.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport glob\n\timport itertools\n\timport logging\n\timport os\n\tfrom pathlib import Path\n\tfrom typing import Iterable, List, Tuple\n", "import faiss\n\timport numpy as np\n\timport torch\n\timport tqdm\n\tfrom src.inference import Accelerator, VideoReaderType\n\tfrom src.model import create_model_in_runtime, create_model_in_runtime_2\n\tfrom src.postproc import sliding_pca\n\tfrom src.score_normalization import score_normalize\n\tfrom src.tta import (\n\t    TTA4ViewsTransform,\n", "    TTA5ViewsTransform,\n\t)\n\tfrom src.video_reader.ffmpeg_py_video_reader import FFMpegPyVideoReader\n\tfrom src.video_reader.ffmpeg_video_reader import FFMpegVideoReader\n\tfrom src.vsc.candidates import CandidateGeneration, MaxScoreAggregation\n\tfrom src.vsc.index import VideoFeature\n\tfrom src.vsc.localization import (\n\t    VCSLLocalizationMatchScore,\n\t)\n\tfrom src.vsc.metrics import (\n", "    CandidatePair,\n\t    Match,\n\t)\n\tfrom src.vsc.storage import load_features, store_features\n\tfrom torch.utils.data import DataLoader, IterableDataset\n\tfrom torch.utils.data._utils.collate import default_collate\n\tlogging.basicConfig(\n\t    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n\t    level=logging.INFO,\n\t    datefmt=\"%Y-%m-%d %H:%M:%S\",\n", ")\n\tlogger = logging.getLogger(\"inference_impl.py\")\n\tlogger.setLevel(logging.INFO)\n\tclass VideoDataset(IterableDataset):\n\t    \"\"\"Decodes video frames at a fixed FPS via ffmpeg.\"\"\"\n\t    def __init__(\n\t        self,\n\t        path: str,\n\t        fps: float,\n\t        read_type: str = \"pil\",\n", "        batch_size=None,\n\t        img_transform=None,\n\t        extensions=(\"mp4\",),\n\t        distributed_rank=0,\n\t        distributed_world_size=1,\n\t        video_reader=VideoReaderType.FFMPEG,\n\t        ffmpeg_path=\"ffmpeg\",\n\t        filter_by_asr=False,\n\t    ):\n\t        assert distributed_rank < distributed_world_size\n", "        self.path = path\n\t        self.fps = fps\n\t        self.read_type = read_type\n\t        self.batch_size = batch_size\n\t        self.img_transform = img_transform\n\t        self.video_reader = video_reader\n\t        self.ffmpeg_path = ffmpeg_path\n\t        if len(extensions) == 1:\n\t            filenames = glob.glob(os.path.join(path, f\"*.{extensions[0]}\"))\n\t        else:\n", "            filenames = glob.glob(os.path.join(path, \"*.*\"))\n\t            filenames = (fn for fn in filenames if fn.rsplit(\".\", 1)[-1] in extensions)\n\t        filenames = [\n\t            name for name in filenames if Path(name).stem not in [\"R102796\", \"R133364\"]\n\t        ]\n\t        self.videos = sorted(filenames)\n\t        if not self.videos:\n\t            raise Exception(\"No videos found!\")\n\t        assert distributed_rank < distributed_world_size\n\t        self.rank = distributed_rank\n", "        self.world_size = distributed_world_size\n\t        self.selected_videos = [\n\t            (i, video)\n\t            for (i, video) in enumerate(self.videos)\n\t            if (i % self.world_size) == self.rank\n\t        ]\n\t    def num_videos(self) -> int:\n\t        return len(self.selected_videos)\n\t    def __iter__(self):\n\t        for i, video in self.selected_videos:\n", "            if self.batch_size:\n\t                frames = self.read_frames(i, video)\n\t                while True:\n\t                    batch = list(itertools.islice(frames, self.batch_size))\n\t                    if not batch:\n\t                        break\n\t                    yield default_collate(batch)\n\t            else:\n\t                yield from self.read_frames(i, video)\n\t    def read_frames(self, video_id, video):\n", "        video_name = os.path.basename(video)\n\t        name = os.path.basename(video_name).split(\".\")[0]\n\t        if self.video_reader == VideoReaderType.FFMPEG:\n\t            reader = FFMpegVideoReader(\n\t                video_path=video,\n\t                required_fps=self.fps,\n\t                output_type=self.read_type,\n\t                ffmpeg_path=self.ffmpeg_path,\n\t            )\n\t        elif self.video_reader == VideoReaderType.FFMPEGPY:\n", "            reader = FFMpegPyVideoReader(\n\t                video_path=video, required_fps=self.fps, output_type=self.read_type\n\t            )\n\t        elif self.video_reader == VideoReaderType.DECORD:\n\t            from src.video_reader.decord_video_reader import DecordVideoReader\n\t            reader = DecordVideoReader(\n\t                video_path=video, required_fps=self.fps, output_type=self.read_type\n\t            )\n\t        else:\n\t            raise ValueError(f\"VideoReaderType: {self.video_reader} not supported\")\n", "        for start_timestamp, end_timestamp, frame in reader.frames():\n\t            if self.img_transform:\n\t                frame = self.img_transform(frame)\n\t            record = {\n\t                \"name\": name,\n\t                \"timestamp\": (np.array(start_timestamp) + np.array(end_timestamp)) / 2,\n\t                \"input\": frame,\n\t            }\n\t            yield record\n\tdef should_use_cuda(args) -> bool:\n", "    accelerator = Accelerator[args.accelerator.upper()]\n\t    return accelerator == Accelerator.CUDA\n\tdef get_device(args, rank, world_size):\n\t    if should_use_cuda(args):\n\t        assert torch.cuda.is_available()\n\t        num_devices = torch.cuda.device_count()\n\t        if args.processes > num_devices:\n\t            raise Exception(\n\t                f\"Asked for {args.processes} processes and cuda, but only \"\n\t                f\"{num_devices} devices found\"\n", "            )\n\t        if args.processes > 1 or world_size <= num_devices:\n\t            device_num = rank\n\t        else:\n\t            device_num = 0\n\t        torch.cuda.set_device(device_num)\n\t        return torch.device(\"cuda\", device_num)\n\t    return torch.device(\"cpu\")\n\tdef search(\n\t    queries: List[VideoFeature],\n", "    refs: List[VideoFeature],\n\t    retrieve_per_query: float = 1200.0,\n\t    candidates_per_query: float = 25.0,\n\t) -> List[CandidatePair]:\n\t    aggregation = MaxScoreAggregation()\n\t    logger.info(\"Searching\")\n\t    cg = CandidateGeneration(refs, aggregation)\n\t    num_to_retrieve = int(retrieve_per_query * len(queries))\n\t    candidates = cg.query(queries, global_k=num_to_retrieve)\n\t    num_candidates = int(candidates_per_query * len(queries))\n", "    candidates = candidates[:num_candidates]\n\t    logger.info(\"Got %d candidates\", len(candidates))\n\t    return candidates\n\tdef localize_and_verify(\n\t    queries: List[VideoFeature],\n\t    refs: List[VideoFeature],\n\t    candidates: List[CandidatePair],\n\t    localize_per_query: float = 5.0,\n\t    model_type: str = \"TN\",\n\t    tn_max_step=5,\n", "    min_length=4,\n\t    concurrency=16,\n\t    similarity_bias=0.5,\n\t    tn_top_k=5,\n\t    max_path=10,\n\t    min_sim=0.2,\n\t    max_iou=0.3,\n\t    discontinue=3,\n\t    sum_sim=8,\n\t    ave_sim=0.3,\n", "    diagonal_thres=10,\n\t    iou_thresh=0.9,\n\t    min_bins=1,\n\t    max_peaks=100,\n\t    min_peaks=10,\n\t) -> List[Match]:\n\t    num_to_localize = int(len(queries) * localize_per_query)\n\t    candidates = candidates[:num_to_localize]\n\t    alignment = VCSLLocalizationMatchScore(\n\t        queries,\n", "        refs,\n\t        model_type=model_type,\n\t        tn_max_step=tn_max_step,\n\t        min_length=min_length,\n\t        concurrency=concurrency,\n\t        similarity_bias=similarity_bias,\n\t        tn_top_k=tn_top_k,\n\t        max_path=max_path,\n\t        min_sim=min_sim,\n\t        max_iou=max_iou,\n", "        discontinue=discontinue,\n\t        sum_sim=sum_sim,\n\t        ave_sim=ave_sim,\n\t        diagonal_thres=diagonal_thres,\n\t        iou_thresh=iou_thresh,\n\t        min_bins=min_bins,\n\t        max_peaks=max_peaks,\n\t        min_peaks=min_peaks,\n\t    )\n\t    matches = []\n", "    logger.info(\"Aligning %s candidate pairs\", len(candidates))\n\t    BATCH_SIZE = 512\n\t    i = 0\n\t    while i < len(candidates):\n\t        batch = candidates[i : i + BATCH_SIZE]\n\t        matches.extend(alignment.localize_all(batch))\n\t        i += len(batch)\n\t        logger.info(\n\t            \"Aligned %d pairs of %d; %d predictions so far\",\n\t            i,\n", "            len(candidates),\n\t            len(matches),\n\t        )\n\t    return matches\n\tdef match(\n\t    queries: List[VideoFeature],\n\t    refs: List[VideoFeature],\n\t    output_file: str = None,\n\t    return_results: bool = False,\n\t    model_type: str = \"TN\",\n", "    tn_max_step=5,\n\t    min_length=4,\n\t    concurrency=16,\n\t    similarity_bias=0.5,\n\t    tn_top_k=5,\n\t    max_path=10,\n\t    min_sim=0.2,\n\t    max_iou=0.3,\n\t    discontinue=3,\n\t    sum_sim=8,\n", "    ave_sim=0.3,\n\t    diagonal_thres=10,\n\t    iou_thresh=0.9,\n\t    min_bins=1,\n\t    max_peaks=100,\n\t    min_peaks=10,\n\t    retrieve_per_query: float = 1200.0,\n\t    candidates_per_query: float = 25.0,\n\t) -> Tuple[str, str]:\n\t    # Search\n", "    candidates = search(\n\t        queries,\n\t        refs,\n\t        retrieve_per_query=retrieve_per_query,\n\t        candidates_per_query=candidates_per_query,\n\t    )\n\t    # Localize and verify\n\t    matches = localize_and_verify(\n\t        queries,\n\t        refs,\n", "        candidates,\n\t        model_type=model_type,\n\t        tn_max_step=tn_max_step,\n\t        min_length=min_length,\n\t        concurrency=concurrency,\n\t        similarity_bias=similarity_bias,\n\t        tn_top_k=tn_top_k,\n\t        max_path=max_path,\n\t        min_sim=min_sim,\n\t        max_iou=max_iou,\n", "        discontinue=discontinue,\n\t        sum_sim=sum_sim,\n\t        ave_sim=ave_sim,\n\t        diagonal_thres=diagonal_thres,\n\t        iou_thresh=iou_thresh,\n\t        min_bins=min_bins,\n\t        max_peaks=max_peaks,\n\t        min_peaks=min_peaks,\n\t    )\n\t    # matches_file = os.path.join(output_path, \"matches.csv\")\n", "    Match.write_csv(matches, output_file, drop_dup=True)\n\t    if return_results:\n\t        return candidates, matches\n\tdef worker_process(\n\t    args,\n\t    rank,\n\t    world_size,\n\t    output_filename,\n\t    model_name,\n\t    ref_path,\n", "    noise_path,\n\t    pca_matrix_path,\n\t):\n\t    logger.info(f\"Starting worker {rank} of {world_size}.\")\n\t    device = get_device(args, rank, world_size)\n\t    logger.info(\"Loading model\")\n\t    if model_name == \"isc\":\n\t        model, transforms = create_model_in_runtime(transforms_device=device)\n\t    elif model_name == \"vit\":\n\t        model, transforms = create_model_in_runtime_2(transforms_device=device)\n", "    else:\n\t        raise ValueError(f\"Model {args.model} is not supported\")\n\t    model = model.to(device).eval()\n\t    logger.info(\"Setting up dataset\")\n\t    extensions = args.video_extensions.split(\",\")\n\t    video_reader = VideoReaderType[args.video_reader.upper()]\n\t    dataset = VideoDataset(\n\t        args.dataset_path,\n\t        fps=args.fps,\n\t        read_type=args.read_type,\n", "        batch_size=1,\n\t        extensions=extensions,\n\t        distributed_world_size=world_size,\n\t        distributed_rank=rank,\n\t        video_reader=video_reader,\n\t        ffmpeg_path=args.ffmpeg_path,\n\t        filter_by_asr=True,\n\t    )\n\t    loader = DataLoader(dataset, batch_size=None, pin_memory=device.type == \"cuda\")\n\t    progress = tqdm.tqdm(total=dataset.num_videos())\n", "    queries = []\n\t    for vf in run_inference(loader, model, device, transforms):\n\t        queries.append(vf)\n\t        progress.update()\n\t    del loader\n\t    del model\n\t    del dataset\n\t    if noise_path:\n\t        stride = args.stride if args.stride is not None else int(args.fps)\n\t        pca_matrix = faiss.read_VectorTransform(pca_matrix_path)\n", "        queries = sliding_pca(queries=queries, mat=pca_matrix, stride=stride)\n\t        queries = score_normalize(\n\t            queries,\n\t            load_features(noise_path),\n\t            beta=1.2,\n\t        )\n\t    os.makedirs(output_filename.replace(\"subset_matches.csv\", \"output\"), exist_ok=True)\n\t    # store_features(output_filename.replace('subset_matches.csv', f'output/subset_queries_{model_name}.npz'), queries)\n\t    refs = load_features(ref_path)\n\t    match(\n", "        queries=queries,\n\t        refs=refs,\n\t        output_file=output_filename.replace(\n\t            \"subset_matches.csv\", f\"output/matches_{model_name}.csv\"\n\t        ),\n\t        tn_max_step=5,\n\t        min_length=3,\n\t        tn_top_k=2,\n\t        max_path=200,\n\t        min_sim=0.1,\n", "        max_iou=1.0,\n\t    )\n\t@torch.no_grad()\n\tdef run_inference(\n\t    dataloader, model, device, transforms=None, tta=True\n\t) -> Iterable[VideoFeature]:\n\t    name = None\n\t    embeddings = []\n\t    timestamps = []\n\t    for batch in dataloader:\n", "        names = batch[\"name\"]\n\t        assert names[0] == names[-1]  # single-video batches\n\t        if name is not None and name != names[0]:\n\t            timestamps = np.concatenate(timestamps, axis=0)\n\t            feature = np.concatenate(embeddings, axis=0)\n\t            if tta:\n\t                timestamps = timestamps.reshape(-1, num_views).transpose(1, 0).ravel()\n\t                feature = (\n\t                    feature.reshape(-1, num_views, feature.shape[1])\n\t                    .transpose(1, 0, 2)\n", "                    .reshape(-1, feature.shape[1])\n\t                )\n\t            yield VideoFeature(\n\t                video_id=name,\n\t                timestamps=timestamps,\n\t                feature=feature,\n\t            )\n\t            embeddings = []\n\t            timestamps = []\n\t        name = names[0]\n", "        if transforms is not None:\n\t            if tta:\n\t                tta_transforms = TTA5ViewsTransform(transforms)\n\t                img = tta_transforms(batch[\"input\"].to(device))\n\t            else:\n\t                img = transforms(batch[\"input\"].to(device))\n\t            num_views = img.shape[0] // batch[\"input\"].shape[0]\n\t            ts = batch[\"timestamp\"].numpy()\n\t            ts = np.repeat(ts, num_views)\n\t        else:\n", "            img = batch[\"input\"].to(device)\n\t            ts = batch[\"timestamp\"].numpy()\n\t            num_views = 1\n\t        emb = model(img).cpu().numpy()\n\t        embeddings.append(emb)\n\t        timestamps.append(ts)\n\t    timestamps = np.concatenate(timestamps, axis=0)\n\t    feature = np.concatenate(embeddings, axis=0)\n\t    if tta:\n\t        timestamps = timestamps.reshape(-1, num_views).transpose(1, 0).ravel()\n", "        feature = (\n\t            feature.reshape(-1, num_views, feature.shape[1])\n\t            .transpose(1, 0, 2)\n\t            .reshape(-1, feature.shape[1])\n\t        )\n\t    yield VideoFeature(\n\t        video_id=name,\n\t        timestamps=timestamps,\n\t        feature=feature,\n\t    )\n", "def merge_feature_files(filenames: List[str], output_filename: str) -> int:\n\t    features = []\n\t    for fn in filenames:\n\t        features.extend(load_features(fn))\n\t    store_features(output_filename, features)\n\t    return len(features)\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/score_normalization.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport dataclasses\n\timport logging\n\tfrom typing import Callable, List, Tuple\n\timport faiss  # @manual\n\timport numpy as np\n\tfrom sklearn.preprocessing import normalize\n", "from src.vsc.candidates import CandidateGeneration, MaxScoreAggregation\n\tfrom src.vsc.index import VideoFeature\n\tlogger = logging.getLogger(\"score_normalization.py\")\n\tlogger.setLevel(logging.INFO)\n\tdef transform_features(\n\t    features: List[VideoFeature], transform: Callable\n\t) -> List[VideoFeature]:\n\t    return [\n\t        dataclasses.replace(feature, feature=transform(feature.feature))\n\t        for feature in features\n", "    ]\n\tdef score_normalize(\n\t    queries: List[VideoFeature],\n\t    score_norm_refs: List[VideoFeature],\n\t    l2_normalize: bool = True,\n\t    replace_dim: bool = True,\n\t    beta: float = 1.0,\n\t) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n\t    \"\"\"\n\t    CSLS style score normalization (as used in the Image Similarity Challenge)\n", "    has the following form. We compute a bias term for each query:\n\t      bias(query) = - beta * sim(query, noise)\n\t    then compute score normalized similarity by incorporating this as an\n\t    additive term for each query:\n\t      sim_sn(query, ref) = sim(query, ref) + bias(query)\n\t    sim(query, ref) is inner product similarity (query * ref), and\n\t    sim(query, noise) is some function of query similarity to a noise dataset\n\t    (score_norm_refs here), such as the similarity to the nearest neighbor.\n\t    We encode the bias term as an extra dimension in the query descriptor,\n\t    and add a constant 1 dimension to reference descriptors, so that inner-\n", "    product similarity is the score-normalized similarity:\n\t      query' = [query bias(query)]\n\t      ref' = [ref 1]\n\t      query' * ref' = (query * ref) + (bias(query) * 1)\n\t          = sim(query, ref) + bias(query) = sim_sn(query, ref)\n\t    \"\"\"\n\t    if score_norm_refs is not None and replace_dim:\n\t        # Make space for the additional score normalization dimension.\n\t        # We could also use PCA dim reduction, but re-centering can be\n\t        # destructive.\n", "        logger.info(\"Replacing dimension\")\n\t        sn_features = np.concatenate([ref.feature for ref in score_norm_refs], axis=0)\n\t        low_var_dim = sn_features.var(axis=0).argmin()\n\t        queries, score_norm_refs = [\n\t            transform_features(\n\t                x, lambda feature: np.delete(feature, low_var_dim, axis=1)\n\t            )\n\t            for x in [queries, score_norm_refs]\n\t        ]\n\t    if l2_normalize:\n", "        logger.info(\"L2 normalizing\")\n\t        queries, score_norm_refs = [\n\t            transform_features(x, normalize) for x in [queries, score_norm_refs]\n\t        ]\n\t    logger.info(\"Applying score normalization\")\n\t    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n\t    if faiss.get_num_gpus() > 0:\n\t        index = faiss.index_cpu_to_all_gpus(index)\n\t    adapted_queries = []\n\t    # Add the additive normalization term to the queries as an extra dimension.\n", "    for query in queries:\n\t        # KNN search is ok here (versus a threshold/radius/range search) since\n\t        # we're not searching the dataset we're evaluating on.\n\t        similarity, ids = index.search(query.feature, 1)\n\t        norm_term = -beta * similarity[:, :1]\n\t        feature = np.concatenate([query.feature, norm_term], axis=1)\n\t        adapted_queries.append(dataclasses.replace(query, feature=feature))\n\t    return adapted_queries\n\tdef score_normalize_with_ref(\n\t    queries: List[VideoFeature],\n", "    refs: List[VideoFeature],\n\t    score_norm_refs: List[VideoFeature],\n\t    l2_normalize: bool = True,\n\t    replace_dim: bool = True,\n\t    beta: float = 1.0,\n\t    return_adapted_score_norm_refs: bool = False,\n\t) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n\t    if {f.video_id for f in refs}.intersection({f.video_id for f in score_norm_refs}):\n\t        raise Exception(\n\t            \"Normalizing on the dataset we're evaluating on is against VSC rules. \"\n", "            \"An independent dataset is needed.\"\n\t        )\n\t    if score_norm_refs is not None and replace_dim:\n\t        # Make space for the additional score normalization dimension.\n\t        # We could also use PCA dim reduction, but re-centering can be\n\t        # destructive.\n\t        logger.info(\"Replacing dimension\")\n\t        sn_features = np.concatenate([ref.feature for ref in score_norm_refs], axis=0)\n\t        low_var_dim = sn_features.var(axis=0).argmin()\n\t        queries, refs, score_norm_refs = [\n", "            transform_features(\n\t                x, lambda feature: np.delete(feature, low_var_dim, axis=1)\n\t            )\n\t            for x in [queries, refs, score_norm_refs]\n\t        ]\n\t    if l2_normalize:\n\t        logger.info(\"L2 normalizing\")\n\t        queries, refs, score_norm_refs = [\n\t            transform_features(x, normalize) for x in [queries, refs, score_norm_refs]\n\t        ]\n", "    logger.info(\"Applying score normalization\")\n\t    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n\t    if faiss.get_num_gpus() > 0:\n\t        index = faiss.index_cpu_to_all_gpus(index)\n\t    adapted_queries = []\n\t    # Add the additive normalization term to the queries as an extra dimension.\n\t    for query in queries:\n\t        # KNN search is ok here (versus a threshold/radius/range search) since\n\t        # we're not searching the dataset we're evaluating on.\n\t        similarity, ids = index.search(query.feature, 1)\n", "        norm_term = -beta * similarity[:, :1]\n\t        feature = np.concatenate([query.feature, norm_term], axis=1)\n\t        adapted_queries.append(dataclasses.replace(query, feature=feature))\n\t    adapted_refs = []\n\t    for ref in refs:\n\t        ones = np.ones_like(ref.feature[:, :1])\n\t        feature = np.concatenate([ref.feature, ones], axis=1)\n\t        adapted_refs.append(dataclasses.replace(ref, feature=feature))\n\t    output = (adapted_queries, adapted_refs)\n\t    if return_adapted_score_norm_refs:\n", "        adapted_score_norm_refs = []\n\t        for score_norm_ref in score_norm_refs:\n\t            ones = np.ones_like(score_norm_ref.feature[:, :1])\n\t            feature = np.concatenate([score_norm_ref.feature, ones], axis=1)\n\t            adapted_score_norm_refs.append(\n\t                dataclasses.replace(score_norm_ref, feature=feature)\n\t            )\n\t        output += (adapted_score_norm_refs,)\n\t    return output\n\tdef negative_embedding_subtraction(\n", "    queries: List[VideoFeature],\n\t    refs: List[VideoFeature],\n\t    score_norm_refs: List[VideoFeature],\n\t    pre_l2_normalize: bool = False,\n\t    post_l2_normalize: bool = False,\n\t    beta: float = 1.0,\n\t    k: int = 10,\n\t    alpha: float = 1.0,\n\t) -> Tuple[List[VideoFeature], List[VideoFeature]]:\n\t    # impl of https://arxiv.org/abs/2112.04323\n", "    if pre_l2_normalize:\n\t        logger.info(\"L2 normalizing\")\n\t        queries, refs, score_norm_refs = [\n\t            transform_features(x, normalize) for x in [queries, refs, score_norm_refs]\n\t        ]\n\t    logger.info(\"Applying negative embedding subtraction\")\n\t    index = CandidateGeneration(score_norm_refs, MaxScoreAggregation()).index.index\n\t    if faiss.get_num_gpus() > 0:\n\t        index = faiss.index_cpu_to_all_gpus(index)\n\t    negative_embeddings = np.concatenate([vf.feature for vf in score_norm_refs], axis=0)\n", "    adapted_queries = []\n\t    for query in queries:\n\t        similarity, ids = index.search(query.feature, k=k)\n\t        weights = similarity[..., None] ** alpha\n\t        topk_negative_embeddings = negative_embeddings[ids] * weights\n\t        subtracted_embedding = topk_negative_embeddings.mean(axis=1) * beta\n\t        adapted_embedding = query.feature - subtracted_embedding\n\t        adapted_queries.append(dataclasses.replace(query, feature=adapted_embedding))\n\t    adapted_refs = []\n\t    for ref in refs:\n", "        similarity, ids = index.search(ref.feature, k=k)\n\t        weights = similarity[..., None] ** alpha\n\t        topk_negative_embeddings = negative_embeddings[ids] * weights\n\t        subtracted_embedding = topk_negative_embeddings.mean(axis=1) * beta\n\t        adapted_embedding = ref.feature - subtracted_embedding\n\t        adapted_refs.append(dataclasses.replace(ref, feature=adapted_embedding))\n\t    if post_l2_normalize:\n\t        logger.info(\"L2 normalizing\")\n\t        adapted_queries, adapted_refs = [\n\t            transform_features(x, normalize) for x in [adapted_queries, adapted_refs]\n", "        ]\n\t    return adapted_queries, adapted_refs\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/tta.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torchvision\n\tclass TTA30ViewsTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n\t        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n", "        *_, h, w = x.shape\n\t        x_top = x[..., : h // 2, :]  # top\n\t        x_bottom = x[..., h // 2 :, :]  # bottom\n\t        x_left = x[..., :, : w // 2]  # left\n\t        x_right = x[..., :, w // 2 :]  # right\n\t        if self.base_transforms is not None:\n\t            x = self.base_transforms(x)\n\t            x_top = self.base_transforms(x_top)\n\t            x_bottom = self.base_transforms(x_bottom)\n\t            x_left = self.base_transforms(x_left)\n", "            x_right = self.base_transforms(x_right)\n\t        crops = [\n\t            x,\n\t            torchvision.transforms.functional.rotate(x, angle=90),\n\t            torchvision.transforms.functional.rotate(x, angle=180),\n\t            torchvision.transforms.functional.rotate(x, angle=270),\n\t            torchvision.transforms.functional.hflip(x),\n\t            torchvision.transforms.functional.vflip(x),\n\t            x_top,\n\t            torchvision.transforms.functional.rotate(x_top, angle=90),\n", "            torchvision.transforms.functional.rotate(x_top, angle=180),\n\t            torchvision.transforms.functional.rotate(x_top, angle=270),\n\t            torchvision.transforms.functional.hflip(x_top),\n\t            torchvision.transforms.functional.vflip(x_top),\n\t            x_bottom,\n\t            torchvision.transforms.functional.rotate(x_bottom, angle=90),\n\t            torchvision.transforms.functional.rotate(x_bottom, angle=180),\n\t            torchvision.transforms.functional.rotate(x_bottom, angle=270),\n\t            torchvision.transforms.functional.hflip(x_bottom),\n\t            torchvision.transforms.functional.vflip(x_bottom),\n", "            x_left,\n\t            torchvision.transforms.functional.rotate(x_left, angle=90),\n\t            torchvision.transforms.functional.rotate(x_left, angle=180),\n\t            torchvision.transforms.functional.rotate(x_left, angle=270),\n\t            torchvision.transforms.functional.hflip(x_left),\n\t            torchvision.transforms.functional.vflip(x_left),\n\t            x_right,\n\t            torchvision.transforms.functional.rotate(x_right, angle=90),\n\t            torchvision.transforms.functional.rotate(x_right, angle=180),\n\t            torchvision.transforms.functional.rotate(x_right, angle=270),\n", "            torchvision.transforms.functional.hflip(x_right),\n\t            torchvision.transforms.functional.vflip(x_right),\n\t        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n\tclass TTA24ViewsTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n\t        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n", "        *_, h, w = x.shape\n\t        x_top_left = x[..., : h // 2, : w // 2]  # top_left\n\t        x_top_right = x[..., : h // 2, w // 2 :]  # top_right\n\t        x_bottom_left = x[..., h // 2 :, : w // 2]  # bottom_left\n\t        x_bottom_right = x[..., h // 2 :, w // 2 :]  # bottom_right\n\t        if self.base_transforms is not None:\n\t            x_top_left = self.base_transforms(x_top_left)\n\t            x_top_right = self.base_transforms(x_top_right)\n\t            x_bottom_left = self.base_transforms(x_bottom_left)\n\t            x_bottom_right = self.base_transforms(x_bottom_right)\n", "        crops = [\n\t            x_top_left,\n\t            torchvision.transforms.functional.rotate(x_top_left, angle=90),\n\t            torchvision.transforms.functional.rotate(x_top_left, angle=180),\n\t            torchvision.transforms.functional.rotate(x_top_left, angle=270),\n\t            torchvision.transforms.functional.hflip(x_top_left),\n\t            torchvision.transforms.functional.vflip(x_top_left),\n\t            x_top_right,\n\t            torchvision.transforms.functional.rotate(x_top_right, angle=90),\n\t            torchvision.transforms.functional.rotate(x_top_right, angle=180),\n", "            torchvision.transforms.functional.rotate(x_top_right, angle=270),\n\t            torchvision.transforms.functional.hflip(x_top_right),\n\t            torchvision.transforms.functional.vflip(x_top_right),\n\t            x_bottom_left,\n\t            torchvision.transforms.functional.rotate(x_bottom_left, angle=90),\n\t            torchvision.transforms.functional.rotate(x_bottom_left, angle=180),\n\t            torchvision.transforms.functional.rotate(x_bottom_left, angle=270),\n\t            torchvision.transforms.functional.hflip(x_bottom_left),\n\t            torchvision.transforms.functional.vflip(x_bottom_left),\n\t            x_bottom_right,\n", "            torchvision.transforms.functional.rotate(x_bottom_right, angle=90),\n\t            torchvision.transforms.functional.rotate(x_bottom_right, angle=180),\n\t            torchvision.transforms.functional.rotate(x_bottom_right, angle=270),\n\t            torchvision.transforms.functional.hflip(x_bottom_right),\n\t            torchvision.transforms.functional.vflip(x_bottom_right),\n\t        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n\tclass TTA5ViewsTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n", "        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n\t        *_, h, w = x.shape\n\t        x_top = x[..., : h // 2, :]  # top\n\t        x_bottom = x[..., h // 2 :, :]  # bottom\n\t        x_left = x[..., :, : w // 2]  # left\n\t        x_right = x[..., :, w // 2 :]  # right\n\t        if self.base_transforms is not None:\n\t            x = self.base_transforms(x)\n", "            x_top = self.base_transforms(x_top)\n\t            x_bottom = self.base_transforms(x_bottom)\n\t            x_left = self.base_transforms(x_left)\n\t            x_right = self.base_transforms(x_right)\n\t        crops = [\n\t            x,\n\t            x_top,\n\t            x_bottom,\n\t            x_left,\n\t            x_right,\n", "        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n\tclass TTA4ViewsTransform(nn.Module):\n\t    def __init__(self, base_transforms=None):\n\t        super().__init__()\n\t        self.base_transforms = base_transforms\n\t    def forward(self, x) -> torch.Tensor:\n\t        *_, h, w = x.shape\n\t        x_top_left = x[..., : h // 2, : w // 2]  # top_left\n", "        x_top_right = x[..., : h // 2, w // 2 :]  # top_right\n\t        x_bottom_left = x[..., h // 2 :, : w // 2]  # bottom_left\n\t        x_bottom_right = x[..., h // 2 :, w // 2 :]  # bottom_right\n\t        if self.base_transforms is not None:\n\t            x_top_left = self.base_transforms(x_top_left)\n\t            x_top_right = self.base_transforms(x_top_right)\n\t            x_bottom_left = self.base_transforms(x_bottom_left)\n\t            x_bottom_right = self.base_transforms(x_bottom_right)\n\t        crops = [\n\t            x_top_left,\n", "            x_top_right,\n\t            x_bottom_left,\n\t            x_bottom_right,\n\t        ]\n\t        crops = torch.cat(crops, dim=0)\n\t        return crops\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/vta.py", "chunked_list": ["import io\n\timport itertools\n\timport json\n\timport time\n\tfrom functools import partial\n\tfrom multiprocessing import Pool\n\tfrom typing import Any, Dict, List, Tuple\n\timport networkx as nx\n\timport numpy as np\n\timport torch\n", "from loguru import logger\n\tfrom networkx.algorithms.dag import dag_longest_path\n\tfrom numba import njit, prange\n\tfrom tslearn.metrics import dtw_path_from_metric\n\tdef chamfer_sim_cpu(q: np.ndarray, r: np.ndarray):\n\t    sim = np.tensordot(q, r.T, axes=1)\n\t    chamfer_sim_1 = np.squeeze(\n\t        np.mean(np.max(sim, axis=1, keepdims=True), axis=2, keepdims=True)\n\t    )\n\t    chamfer_sim_2 = np.squeeze(\n", "        np.mean(np.max(sim, axis=2, keepdims=True), axis=1, keepdims=True)\n\t    )\n\t    return (chamfer_sim_1 + chamfer_sim_2) / 2\n\tdef chamfer_sim_gpu(q: np.ndarray, r: np.ndarray):\n\t    sim = torch.tensordot(q, r.T, 1)\n\t    chamfer_sim_1 = torch.squeeze(torch.mean(torch.amax(sim, 1, True), 2, True))\n\t    chamfer_sim_2 = torch.squeeze(torch.mean(torch.amax(sim, 2, True), 1, True))\n\t    return ((chamfer_sim_1 + chamfer_sim_2) / 2).cpu().numpy()\n\tdef sim_norm(sim: np.ndarray, lower_bound=0, upper_bound=0.3):\n\t    return np.clip(sim, lower_bound, upper_bound) / (upper_bound - lower_bound)\n", "def sim_map_cpu(\n\t    qid, rid, q: np.ndarray, r: np.ndarray, normalize_input=False, similarity_type=\"cos\"\n\t):\n\t    if normalize_input:\n\t        q = q / np.linalg.norm(q, axis=1, keepdims=True)\n\t        r = r / np.linalg.norm(r, axis=1, keepdims=True)\n\t    if similarity_type == \"cos\":\n\t        return qid, rid, np.dot(q, r.T)\n\t    elif similarity_type == \"chamfer\":\n\t        return qid, rid, sim_norm(chamfer_sim_cpu(q, r))\n", "    else:\n\t        raise ValueError(f\"Unknown method {similarity_type}\")\n\tdef sim_map_gpu(\n\t    qid,\n\t    rid,\n\t    q: np.ndarray,\n\t    r: np.ndarray,\n\t    normalize_input=False,\n\t    similarity_type=\"cos\",\n\t    device=0,\n", "):\n\t    with torch.cuda.device(device):\n\t        q = torch.from_numpy(q).cuda()\n\t        r = torch.from_numpy(r).cuda()\n\t    if normalize_input:\n\t        q = torch.nn.functional.normalize(q, dim=1, p=2)\n\t        r = torch.nn.functional.normalize(r, dim=1, p=2)\n\t    if similarity_type == \"cos\":\n\t        return qid, rid, torch.matmul(q, r.T).cpu().numpy()\n\t    elif similarity_type == \"chamfer\":\n", "        return qid, rid, sim_norm(chamfer_sim_gpu(q, r))\n\t    else:\n\t        raise ValueError(f\"Unknown method {similarity_type}\")\n\tclass VideoSimMapModel(object):\n\t    def __init__(self, concurrency=4):\n\t        self.concurrency = concurrency\n\t        self.pool = Pool(self.concurrency)\n\t    def forward(\n\t        self,\n\t        data: List[Tuple[str, str, np.array, np.array]],\n", "        use_cuda=True,\n\t        normalize_input=False,\n\t        similarity_type=\"cos\",\n\t        device=0,\n\t    ) -> List[Any]:\n\t        if use_cuda:\n\t            func = partial(\n\t                sim_map_gpu,\n\t                normalize_input=normalize_input,\n\t                similarity_type=similarity_type,\n", "                device=device,\n\t            )\n\t        else:\n\t            func = partial(\n\t                sim_map_cpu,\n\t                normalize_input=normalize_input,\n\t                similarity_type=similarity_type,\n\t            )\n\t        return self.pool.starmap(func, data)\n\tdef iou(bbox, gt):\n", "    \"\"\"\n\t    :param bbox: (n, 4)\n\t    :param gt: (m, 4)\n\t    :return: (n, m)\n\t    \"\"\"\n\t    if len(bbox) == 0 or len(gt) == 0:\n\t        return np.array(0)\n\t    lt = np.maximum(bbox[:, None, :2], gt[:, :2])  # left_top (x, y)\n\t    rb = np.minimum(bbox[:, None, 2:], gt[:, 2:])  # right_bottom (x, y)\n\t    wh = np.maximum(rb - lt + 1, 0)  # inter_area (w, h)\n", "    inter_areas = wh[:, :, 0] * wh[:, :, 1]  # shape: (n, m)\n\t    box_areas = (bbox[:, 2] - bbox[:, 0] + 1) * (bbox[:, 3] - bbox[:, 1] + 1)\n\t    gt_areas = (gt[:, 2] - gt[:, 0] + 1) * (gt[:, 3] - gt[:, 1] + 1)\n\t    IoU = inter_areas / (box_areas[:, None] + gt_areas - inter_areas)\n\t    return np.array(IoU)\n\tdef zero_runs(a):\n\t    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n\t    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n\t    absdiff = np.abs(np.diff(iszero))\n\t    # Runs start and end where absdiff is 1.\n", "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n\t    return ranges\n\tdef cut_path(path: np.ndarray, diagonal_thres):\n\t    # range [start, end)\n\t    vertical_ranges = zero_runs(np.diff(path[:, 0]))\n\t    vertical_ranges[:, 1] += 1\n\t    horizontal_ranges = zero_runs(np.diff(path[:, 1]))\n\t    horizontal_ranges[:, 1] += 1\n\t    vertical_ranges = vertical_ranges[\n\t        np.diff(vertical_ranges, axis=-1).squeeze(axis=-1) > diagonal_thres\n", "    ]\n\t    horizontal_ranges = horizontal_ranges[\n\t        np.diff(horizontal_ranges, axis=-1).squeeze(axis=-1) > diagonal_thres\n\t    ]\n\t    discard_ranges = np.concatenate([vertical_ranges, horizontal_ranges], axis=0)\n\t    discard_ranges = discard_ranges[discard_ranges[:, 0].argsort()]\n\t    endpoints = discard_ranges.ravel()\n\t    if len(endpoints) == 0:\n\t        keep_ranges = np.array([[0, len(path)]], dtype=np.int32)\n\t    else:\n", "        endpoints = (\n\t            endpoints[1:] if endpoints[0] == 0 else np.concatenate([[0], endpoints])\n\t        )\n\t        endpoints = (\n\t            endpoints[:-1]\n\t            if endpoints[-1] == len(path)\n\t            else np.concatenate([endpoints, [len(path)]])\n\t        )\n\t        keep_ranges = endpoints.reshape(-1, 2)\n\t    return keep_ranges\n", "def dtw(sim_matrix: np.ndarray, discontinue=3, min_sim=0.2, min_length=5, max_iou=0.3):\n\t    # ATTENTION: sim_matrix represents similarity, we need a distance matrix\n\t    path, sim_score = dtw_path_from_metric(1 - sim_matrix, metric=\"precomputed\")\n\t    path = np.array(path)\n\t    # remove horizontal and vertical paths\n\t    keep_ranges = cut_path(path, diagonal_thres=discontinue)\n\t    keep_ranges = keep_ranges[\n\t        np.diff(keep_ranges, axis=-1).squeeze(axis=-1) > min_length\n\t    ]\n\t    result_list = []\n", "    for s, e in keep_ranges:\n\t        assert s < e, f\"{s} < {e}, {sim_matrix.shape}\"\n\t        sub_path = path[s:e]\n\t        mean_sim = np.mean(sim_matrix[sub_path[:, 0], sub_path[:, 1]])\n\t        if (\n\t            mean_sim > min_sim\n\t            and (sub_path[-1][0] - sub_path[0][0]) > min_length\n\t            and (sub_path[-1][1] - sub_path[0][1]) > min_length\n\t        ):\n\t            result_list.append(\n", "                [\n\t                    int(sub_path[0][0]),\n\t                    int(sub_path[0][1]),\n\t                    int(sub_path[-1][0]),\n\t                    int(sub_path[-1][1]),\n\t                ]\n\t            )\n\t    return result_list\n\tdef find_path(dp_mat, back_trace_mat):\n\t    max_i, max_j = np.unravel_index(np.argmax(dp_mat), dp_mat.shape)\n", "    path = [(max_i, max_j)]\n\t    while back_trace_mat[max_i, max_j] != -1:\n\t        if back_trace_mat[max_i, max_j] == 0:\n\t            max_i, max_j = max_i - 1, max_j - 1\n\t        elif back_trace_mat[max_i, max_j] == 1:\n\t            max_i, max_j = max_i - 1, max_j\n\t        else:\n\t            max_i, max_j = max_i, max_j - 1\n\t        if dp_mat[max_i, max_j] == np.NINF:\n\t            break\n", "        path.append((max_i, max_j))\n\t    path = np.array(path, dtype=np.int32)[::-1, :]\n\t    return path\n\t@njit()\n\tdef njit_dp_matrix(sim_mat: np.ndarray, discontinue=3, min_sim=0):\n\t    dp_mat = sim_mat.copy()\n\t    M, N = sim_mat.shape[:2]\n\t    accu_unmatch_mat = np.zeros(dp_mat.shape, dtype=np.int32)\n\t    back_trace_mat = -np.ones(dp_mat.shape, dtype=np.int8)\n\t    for i in prange(1, M):\n", "        for j in prange(1, N):\n\t            cand_locs = [(i - 1, j - 1), (i - 1, j), (i, j - 1)]\n\t            # (i-1, j-1) top-left\n\t            top_left = dp_mat[i - 1, j - 1]\n\t            # (i-1, j) top\n\t            top = dp_mat[i - 1, j]\n\t            # (i, j-1) left\n\t            left = dp_mat[i, j - 1]\n\t            values = np.array(\n\t                [\n", "                    top_left + sim_mat[i, j],\n\t                    top + 0.5 * sim_mat[i, j],\n\t                    left + 0.5 * sim_mat[i, j],\n\t                ]\n\t            )\n\t            max_ind = np.argmax(values)\n\t            max_value = values[max_ind]\n\t            prev_loc = cand_locs[max_ind]\n\t            # sim value is too small\n\t            unmatch = sim_mat[i, j] < min_sim\n", "            if unmatch:\n\t                accu_unmatch_mat[i, j] = accu_unmatch_mat[prev_loc] + 1\n\t            if accu_unmatch_mat[i, j] <= discontinue:\n\t                back_trace_mat[i, j] = max_ind\n\t                dp_mat[i, j] = max_value\n\t    return dp_mat, accu_unmatch_mat, back_trace_mat\n\tdef dp(\n\t    sim_matrix: np.ndarray,\n\t    discontinue=3,\n\t    min_sim=1,\n", "    ave_sim=1.3,\n\t    min_length=5,\n\t    diagonal_thres=30,\n\t):\n\t    # implemented mPDP from Pattern-Based Near-Duplicate Video Retrieval and Localization on Web-Scale Videos\n\t    # rescale to make cosine-similarity scores non-negative\n\t    sim_matrix += 1\n\t    dp_mat, accu_unmatch_mat, back_trace_mat = njit_dp_matrix(\n\t        sim_matrix, discontinue=discontinue, min_sim=min_sim\n\t    )\n", "    result_list = []\n\t    cnt = 100\n\t    while cnt > 0:\n\t        path = find_path(dp_mat, back_trace_mat)\n\t        if dp_mat[path[-1][0], path[-1][1]] == np.NINF:\n\t            break\n\t        r1, c1 = int(path[0][0]), int(path[0][1])\n\t        r2, c2 = int(path[-1][0]), int(path[-1][1])\n\t        dp_mat[r1 : r2 + 1, c1 : c2 + 1] = np.NINF\n\t        keep_ranges = cut_path(path, diagonal_thres=diagonal_thres)\n", "        keep_ranges = keep_ranges[\n\t            np.diff(keep_ranges, axis=-1).squeeze(axis=-1) > min_length\n\t        ]\n\t        for s, e in keep_ranges:\n\t            sub_path = path[s:e]\n\t            mean_sim = np.mean(sim_matrix[sub_path[:, 0], sub_path[:, 1]])\n\t            if (\n\t                mean_sim > ave_sim\n\t                and (sub_path[-1][0] - sub_path[0][0]) > min_length\n\t                and (sub_path[-1][1] - sub_path[0][1]) > min_length\n", "            ):\n\t                result_list.append(\n\t                    [\n\t                        int(sub_path[0][0]),\n\t                        int(sub_path[0][1]),\n\t                        int(sub_path[-1][0]),\n\t                        int(sub_path[-1][1]),\n\t                    ]\n\t                )\n\t        cnt -= 1\n", "    return result_list\n\tdef tn(\n\t    views: List,\n\t    sims: np.ndarray,\n\t    tn_max_step: int = 10,\n\t    tn_top_k: int = 5,\n\t    max_path: int = 10,\n\t    min_sim: float = 0.2,\n\t    min_length: int = 4,\n\t    max_iou: float = 0.3,\n", ") -> List[List[int]]:\n\t    \"\"\"\n\t    TN method for video temporal alignment.\n\t    Reimplemented paper:\n\t    {Tan H K, Ngo C W, Hong R, et al. Scalable detection of partial near-duplicate videos by visual-temporal consistency\n\t     [C]//Proceedings of the 17th ACM international conference on Multimedia. 2009: 145-154.}\n\t    Parameters\n\t    ----------\n\t    sims: input similarity map computed from a copied video pair.\n\t    tn_max_step: max step range in TN.\n", "    tn_top_k: Top k frame similarity selection in TN.\n\t    max_path: max loop for multiply segments detection.\n\t    min_sim: min average similarity score for each aligned segment.\n\t    min_length: min segment length.\n\t    max_iou: max iou for filtering overlap segments (bbox).\n\t    Returns\n\t    -------\n\t    list of temporal aligned copied segments, [query_min, ref_min, query_max, ref_max] for each segment\n\t    \"\"\"\n\t    q_view, r_view = views\n", "    q_frame = sims.shape[0] // q_view\n\t    r_frame = sims.shape[1] // r_view\n\t    infringe_box_list = []\n\t    infringe_box_score_list = []\n\t    path = 0\n\t    node_pair2id = {}\n\t    node_pair2id[(-1, -1)] = 0\n\t    node_id2pair = {}\n\t    node_id2pair[0] = (-1, -1)  # source\n\t    node_num = 1\n", "    DG = nx.DiGraph()\n\t    DG.add_node(0)\n\t    # get top-k values and indices, shape (Q_LEN, top_k)\n\t    top = min(tn_top_k, sims.shape[1])\n\t    topk_indices = np.argsort(-sims)[:, :top]\n\t    topk_sims = np.take_along_axis(sims, topk_indices, axis=-1)\n\t    # add nodes\n\t    for qf_idx in range(sims.shape[0]):\n\t        for k in range(top):\n\t            rf_idx = topk_indices[qf_idx][k]\n", "            node_id2pair[node_num] = (qf_idx, rf_idx)\n\t            node_pair2id[(qf_idx, rf_idx)] = node_num\n\t            DG.add_node(node_num)\n\t            node_num += 1\n\t    # create graph by adding edges\n\t    for q_i in range(sims.shape[0]):\n\t        r_i = topk_indices[q_i]\n\t        intermediate_rs = np.empty((0,), dtype=np.int32)\n\t        # implements Constraints C1 by limiting range end\n\t        qj_range = [\n", "            i\n\t            for i in range(\n\t                q_i + 1, min((q_i // q_frame + 1) * q_frame, q_i + tn_max_step)\n\t            )\n\t        ]\n\t        for q_j in qj_range:\n\t            r_j = topk_indices[q_j]  # shape (top_k, )\n\t            r_diff = r_j[:, None] - r_i  # dst - src, shape (top_k, top_k)\n\t            # Constraints C2\n\t            C2 = (r_diff > 0) & (r_diff < tn_max_step) & (r_j % r_frame > r_i % r_frame)\n", "            # Constraints C4\n\t            s_j = topk_sims[q_j]  # shape (top_k, )\n\t            s_j = np.repeat(\n\t                s_j.reshape(-1, 1), r_diff.shape[1], axis=1\n\t            )  # shape (top_k, top_k)\n\t            C4 = s_j >= min_sim\n\t            # val_rows, val_cols = np.where(C2 & C3 & C4)\n\t            val_rows, val_cols = np.where(C2 & C4)\n\t            val_sims = s_j[val_rows, val_cols]\n\t            # update intermediate_rs\n", "            valid_r_j = r_j[val_rows]\n\t            intermediate_rs = np.unique(np.concatenate([intermediate_rs, valid_r_j]))\n\t            edges = [\n\t                (\n\t                    node_pair2id[(q_i, r_i[c])],\n\t                    node_pair2id[(q_j, r_j[r])],\n\t                    dict(weight=s),\n\t                )\n\t                for c, r, s in zip(val_cols, val_rows, val_sims)\n\t            ]\n", "            DG.add_edges_from(edges)\n\t    # logger.info(\"Graph N {} E {} for sim {}x{}\", DG.number_of_nodes(), DG.number_of_edges(), sims.shape[0],\n\t    #             sims.shape[1])\n\t    # link sink node\n\t    for i in range(0, node_num - 1):\n\t        j = node_num - 1\n\t        pair_i = node_id2pair[i]\n\t        pair_j = node_id2pair[j]\n\t        if i == 0:\n\t            DG.add_edge(i, j, weight=0)\n", "        else:\n\t            if (\n\t                pair_j[0] % q_frame > pair_i[0] % q_frame\n\t                and pair_j[1] % r_frame > pair_i[1] % r_frame\n\t                and pair_j[0] - pair_i[0] <= tn_max_step\n\t                and pair_j[1] - pair_i[1] <= tn_max_step\n\t            ):\n\t                DG.add_edge(i, j, weight=0)\n\t    while True:\n\t        if path > max_path:\n", "            break\n\t        longest_path = dag_longest_path(DG)\n\t        for i in range(1, len(longest_path)):\n\t            DG.add_edge(longest_path[i - 1], longest_path[i], weight=0.0)\n\t        if 0 in longest_path:\n\t            longest_path.remove(0)  # remove source node\n\t        if node_num - 1 in longest_path:\n\t            longest_path.remove(node_num - 1)  # remove sink node\n\t        path_query = [node_id2pair[node_id][0] for node_id in longest_path]\n\t        path_refer = [node_id2pair[node_id][1] for node_id in longest_path]\n", "        if len(path_query) == 0:\n\t            break\n\t        score = 0.0\n\t        for (qf_idx, rf_idx) in zip(path_query, path_refer):\n\t            score += sims[qf_idx][rf_idx]\n\t        if score > 0:\n\t            query_min, query_max = min(path_query), max(path_query)\n\t            refer_min, refer_max = min(path_refer), max(path_refer)\n\t        else:\n\t            query_min, query_max = 0, 0\n", "            refer_min, refer_max = 0, 0\n\t        ave_length = (refer_max - refer_min + query_max - query_min) / 2\n\t        ious = iou(\n\t            np.expand_dims(\n\t                np.array([query_min, refer_min, query_max, refer_max]), axis=0\n\t            ),\n\t            np.array(infringe_box_list),\n\t        )\n\t        if (\n\t            ave_length != 0\n", "            and score / ave_length > min_sim\n\t            and min(refer_max - refer_min, query_max - query_min) > min_length\n\t            and ious.max() < max_iou\n\t        ):\n\t            infringe_box_list.append(\n\t                [int(query_min), int(refer_min), int(query_max), int(refer_max)]\n\t            )\n\t            infringe_box_score_list.append(\n\t                [\n\t                    int(query_min),\n", "                    int(refer_min),\n\t                    int(query_max),\n\t                    int(refer_max),\n\t                    score / ave_length,\n\t                ]\n\t            )\n\t        path += 1\n\t    return infringe_box_score_list\n\tdef hv(sims: np.ndarray, iou_thresh=0.9, min_sim=0.2, max_peaks=100):\n\t    infringe_box_list = (\n", "        []\n\t    )  ## box_type = [int(query_min), int(refer_min), int(query_max), int(refer_max)]\n\t    ## step1: remove all pairs lower than min_sim\n\t    sims[sims < min_sim] = 0.0\n\t    ## step2: calculate the time_bins histogram\n\t    query_inds, refer_inds = np.where(sims >= min_sim)\n\t    sigma_inds = np.unique(refer_inds - query_inds)\n\t    sigma_hists = dict()\n\t    for s_i in range(sigma_inds.shape[0]):\n\t        sigma = sigma_inds[s_i]\n", "        if sigma not in sigma_hists:\n\t            sigma_hists[sigma] = dict()\n\t            sigma_hists[sigma][\"score\"] = 0.0\n\t            sigma_hists[sigma][\"matches\"] = list()\n\t        start_idx = -sigma if sigma < 0 else 0\n\t        end_idx = sims.shape[1] - sigma  # if sigma>0 else sims.shape[1] - sigma\n\t        end_idx = min(max(end_idx, 0), sims.shape[0])\n\t        query_idx = range(start_idx, end_idx)\n\t        refer_idx = range(start_idx + sigma, end_idx + sigma)\n\t        sub_sims = sims[query_idx, refer_idx]\n", "        sigma_hists[sigma][\"score\"] = float(np.sum(sub_sims))\n\t        sigma_hists[sigma][\"matches\"] = [\n\t            [query_idx[x], refer_idx[x], sub_sims[x]] for x in range(len(query_idx))\n\t        ]\n\t    ## step3: refine the final matches\n\t    sorted_sigma_hists = sorted(\n\t        sigma_hists.items(), key=lambda x: x[1][\"score\"], reverse=True\n\t    )\n\t    del sigma_hists\n\t    sorted_sigma_hists = sorted_sigma_hists[:max_peaks]\n", "    \"\"\"\n\t    final_hists = dict()\n\t    for sigma, sum in sorted_sigma_hists:\n\t        matches = sum['matches']\n\t        final_hists[sigma] = dict()\n\t        final_hists[sigma]['score'] = 0.\n\t        final_hists[sigma]['matches'] = list()\n\t        for match in matches:\n\t            query_id, refer_id, score = match\n\t            if abs(query_id - refer_id - sigma) < min_peaks and match not in final_hists[sigma]['matches']:\n", "                 final_hists[sigma]['score'] += score\n\t                 final_hists[sigma]['matches'].append(match)\n\t    sorted_final_hists = sorted(final_hists.items(), key=lambda x: x[1]['score'], reverse=True)\n\t    \"\"\"\n\t    ## step4: output the final infringe_box_list\n\t    for sigma, sum in sorted_sigma_hists:\n\t        if sum[\"score\"] <= 0.0:\n\t            continue\n\t        matches = sum[\"matches\"]\n\t        query_ids = [x[0] for x in matches]\n", "        refer_ids = [x[1] for x in matches]\n\t        query_min = min(query_ids)\n\t        query_max = max(query_ids)\n\t        refer_min = min(refer_ids)\n\t        refer_max = max(refer_ids)\n\t        cur_box = [int(query_min), int(refer_min), int(query_max), int(refer_max)]\n\t        ## === add nms\n\t        ious = iou(\n\t            np.expand_dims(cur_box, axis=0),\n\t            np.array(infringe_box_list, dtype=np.float32),\n", "        )\n\t        if np.any(ious > iou_thresh):\n\t            continue\n\t        infringe_box_list.append(cur_box)\n\t    return infringe_box_list\n\tdef func_wrapper_with_exception(rid, views, item, func):\n\t    try:\n\t        return rid, func(views, item)\n\t    except Exception as e:\n\t        logger.exception(\"Fail to run with rid {}\", rid)\n", "        raise RuntimeError(f\"Fail to run with data with {rid}\") from e\n\tclass BaseVtaModel(object):\n\t    def __init__(self, concurrency, func_to_run):\n\t        self.pool = Pool(concurrency)\n\t        self.func_to_run = func_to_run\n\t    def forward(self, data: List[Tuple[str, str, np.array, np.array]]) -> List[Any]:\n\t        sim_func = partial(sim_map_cpu, normalize_input=False)\n\t        sim_list = self.pool.map(sim_func, data)\n\t        sim_list = [(f\"{q}-{r}\", v) for q, r, v in sim_list]\n\t        return self.forward_sim(sim_list)\n", "    def forward_sim(self, data: List[Tuple[str, List, np.array]]) -> List[Any]:\n\t        algo = partial(func_wrapper_with_exception, func=self.func_to_run)\n\t        results = self.pool.starmap(algo, data)\n\t        return results\n\tclass DtwModel(BaseVtaModel):\n\t    def __init__(\n\t        self,\n\t        concurrency=4,\n\t        version=\"v1\",\n\t        discontinue=3,\n", "        min_sim=0.2,\n\t        min_length=5,\n\t        max_iou=0.3,\n\t        **kwargs,\n\t    ):\n\t        self.min_length = min_length\n\t        self.min_sim = min_sim\n\t        self.max_iou = max_iou\n\t        self.discontinue = discontinue\n\t        self.version = version\n", "        func = partial(\n\t            dtw,\n\t            discontinue=self.discontinue,\n\t            min_sim=self.min_sim,\n\t            min_length=self.min_length,\n\t            max_iou=self.max_iou,\n\t        )\n\t        super(DtwModel, self).__init__(concurrency=concurrency, func_to_run=func)\n\tclass DpVtaModel(BaseVtaModel):\n\t    def __init__(\n", "        self,\n\t        concurrency=4,\n\t        version=\"v1\",\n\t        discontinue=3,\n\t        min_sim=0.0,\n\t        min_length=5,\n\t        max_iou=0.3,\n\t        sum_sim=8,\n\t        ave_sim=0.3,\n\t        diagonal_thres=10,\n", "        **kwargs,\n\t    ):\n\t        self.min_sim = min_sim\n\t        self.min_length = min_length\n\t        self.max_iou = max_iou\n\t        self.sum_sim = sum_sim\n\t        self.ave_sim = ave_sim\n\t        self.discontinue = discontinue\n\t        self.diagonal_thres = diagonal_thres\n\t        self.version = version\n", "        func = partial(\n\t            dp,\n\t            discontinue=self.discontinue,\n\t            min_sim=self.min_sim,\n\t            min_length=self.min_length,\n\t            ave_sim=self.ave_sim,\n\t            diagonal_thres=self.diagonal_thres,\n\t        )\n\t        super(DpVtaModel, self).__init__(concurrency=concurrency, func_to_run=func)\n\tclass TnVtaModel(BaseVtaModel):\n", "    def __init__(\n\t        self,\n\t        concurrency=4,\n\t        version=\"v1\",\n\t        tn_max_step=10,\n\t        tn_top_k=5,\n\t        max_path=10,\n\t        min_sim=0.2,\n\t        min_length=5,\n\t        max_iou=0.3,\n", "        **kwargs,\n\t    ):\n\t        self.tn_max_step = tn_max_step\n\t        self.tn_top_k = tn_top_k\n\t        self.max_path = max_path\n\t        self.min_sim = min_sim\n\t        self.min_length = min_length\n\t        self.max_iou = max_iou\n\t        self.version = version\n\t        func = partial(\n", "            tn,\n\t            tn_max_step=self.tn_max_step,\n\t            tn_top_k=self.tn_top_k,\n\t            max_path=self.max_path,\n\t            min_sim=self.min_sim,\n\t            min_length=self.min_length,\n\t            max_iou=self.max_iou,\n\t        )\n\t        super(TnVtaModel, self).__init__(concurrency=concurrency, func_to_run=func)\n\tclass HvVtaModel(BaseVtaModel):\n", "    def __init__(\n\t        self,\n\t        concurrency=4,\n\t        version=\"v1\",\n\t        iou_thresh=0.9,\n\t        min_sim=0.0,\n\t        min_bins=1,\n\t        max_peaks=100,\n\t        min_peaks=10,\n\t        **kwargs,\n", "    ):\n\t        self.min_sim = min_sim\n\t        self.min_bins = min_bins\n\t        self.max_peaks = max_peaks\n\t        self.min_peaks = min_peaks\n\t        self.iou_thresh = iou_thresh\n\t        self.version = version\n\t        func = partial(\n\t            hv,\n\t            iou_thresh=self.iou_thresh,\n", "            min_sim=self.min_sim,\n\t            max_peaks=self.max_peaks,\n\t        )\n\t        super(HvVtaModel, self).__init__(concurrency=concurrency, func_to_run=func)\n\tdef build_vta_model(method=\"DTW\", concurrency=4, **config) -> BaseVtaModel:\n\t    if method == \"DTW\":\n\t        return DtwModel(concurrency=concurrency, version=\"v1\", **config)\n\t    elif method == \"TN\":\n\t        return TnVtaModel(concurrency=concurrency, version=\"v1\", **config)\n\t    elif method == \"DP\":\n", "        return DpVtaModel(concurrency=concurrency, version=\"v1\", **config)\n\t    elif method == \"HV\":\n\t        return HvVtaModel(concurrency=concurrency, version=\"v1\", **config)\n\t    else:\n\t        raise ValueError(f\"Unknown method {method}\")\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/metrics.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport collections\n\timport dataclasses\n\timport enum\n\timport itertools\n\tfrom collections import defaultdict\n\tfrom math import sqrt\n", "from typing import Collection, Dict, List, NamedTuple, Optional, TextIO, Tuple, Union\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport pandas as pd\n\tfrom sklearn.metrics import average_precision_score\n\tclass Dataset(enum.Enum):\n\t    QUERIES = \"Q\"\n\t    REFS = \"R\"\n\tdef format_video_id(video_id: Union[str, int], dataset: Optional[Dataset]) -> str:\n\t    if isinstance(video_id, (int, np.integer)):\n", "        if dataset is None:\n\t            raise ValueError(\n\t                \"Unable to convert integer video_id without a Dataset enum\"\n\t            )\n\t        return f\"{dataset.value}{video_id:06d}\"\n\t    assert isinstance(\n\t        video_id, str\n\t    ), f\"unexpected video_id: {video_id} of type {type(video_id)}\"\n\t    if dataset is not None:\n\t        assert (\n", "            video_id[0] == dataset.value\n\t        ), f\"dataset mismatch? got {video_id} for dataset {dataset}\"\n\t    return video_id\n\t@dataclasses.dataclass\n\tclass CandidatePair:\n\t    query_id: str\n\t    ref_id: str\n\t    score: float\n\t    @classmethod\n\t    def to_dataframe(\n", "        cls,\n\t        candidates: Collection[\"CandidatePair\"],\n\t    ) -> pd.DataFrame:\n\t        return pd.DataFrame(\n\t            [\n\t                {\n\t                    \"query_id\": format_video_id(c.query_id, Dataset.QUERIES),\n\t                    \"ref_id\": format_video_id(c.ref_id, Dataset.REFS),\n\t                    \"score\": c.score,\n\t                }\n", "                for c in candidates\n\t            ],\n\t        )\n\t    @classmethod\n\t    def write_csv(\n\t        cls, candidates: Collection[\"CandidatePair\"], file: Union[str, TextIO]\n\t    ):\n\t        df = cls.to_dataframe(candidates)\n\t        df.to_csv(file, index=False)\n\t    @classmethod\n", "    def read_csv(cls, file: Union[str, TextIO]) -> List[\"CandidatePair\"]:\n\t        df = pd.read_csv(file)\n\t        pairs = []\n\t        for _, row in df.iterrows():\n\t            query_id = format_video_id(row.query_id, Dataset.QUERIES)\n\t            ref_id = format_video_id(row.ref_id, Dataset.REFS)\n\t            pairs.append(\n\t                CandidatePair(query_id=query_id, ref_id=ref_id, score=row.score)\n\t            )\n\t        return pairs\n", "    @classmethod\n\t    def from_matches(cls, matches: Collection[\"Match\"]) -> List[\"CandidatePair\"]:\n\t        scores = collections.defaultdict(float)\n\t        for match in matches:\n\t            key = (match.query_id, match.ref_id)\n\t            scores[key] = max(match.score, scores[key])\n\t        return [\n\t            CandidatePair(query_id=query_id, ref_id=ref_id, score=score)\n\t            for ((query_id, ref_id), score) in scores.items()\n\t        ]\n", "@dataclasses.dataclass\n\tclass PrecisionRecallCurve:\n\t    precisions: np.ndarray\n\t    recalls: np.ndarray\n\t    scores: np.ndarray\n\t    def plot(self, ax=None, **kwargs):\n\t        if ax is None:\n\t            _, ax = plt.subplots()\n\t            ax.set_xlabel(\"recall\")\n\t            ax.set_ylabel(\"precision\")\n", "            ax.set_xlim(0, 1.05)\n\t            ax.set_ylim(0, 1.05)\n\t        ax.plot(self.recalls, self.precisions, **kwargs)\n\t        return ax\n\t@dataclasses.dataclass\n\tclass AveragePrecision:\n\t    ap: float\n\t    pr_curve: PrecisionRecallCurve\n\t    simple_ap: Optional[float] = None\n\tclass Intervals:\n", "    # Non-overlapping, ordered by interval start.\n\t    intervals: List[Tuple[float, float]]\n\t    def __init__(self, intervals: Optional[List[Tuple[float, float]]] = None):\n\t        self.intervals = intervals or []\n\t        self._dedup()\n\t    def add(self, interval: Tuple[float, float]):\n\t        \"\"\"Add an interval.\"\"\"\n\t        self.intervals.append(interval)\n\t        self._dedup()\n\t    def union(self, intervals: \"Intervals\") -> \"Intervals\":\n", "        return Intervals(self.intervals + intervals.intervals)\n\t    def total_length(self) -> float:\n\t        length = 0.0\n\t        for start, end in self.intervals:\n\t            length += end - start\n\t        return length\n\t    def intersect_length(self, intervals: \"Intervals\") -> float:\n\t        \"\"\"Compute the total_length of the intersection of two Intervals.\n\t        This works by taking the sum of their lengths, and subtracting\n\t        the length of their union.\n", "        |A n B| = |A| + |B| - |A U B|\n\t        \"\"\"\n\t        union = self.union(intervals)\n\t        return self.total_length() + intervals.total_length() - union.total_length()\n\t    def _dedup(self):\n\t        if len(self.intervals) <= 1:\n\t            return\n\t        deduped = []\n\t        intervals = sorted(self.intervals)\n\t        current_start, current_end = intervals[0]\n", "        for start, end in intervals[1:]:\n\t            if start <= current_end:\n\t                # Overlap case\n\t                current_end = max(end, current_end)\n\t            else:\n\t                # Non-overlap case\n\t                deduped.append((current_start, current_end))\n\t                current_start, current_end = start, end\n\t        deduped.append((current_start, current_end))\n\t        self.intervals = deduped\n", "    def __str__(self):\n\t        return str(self.intervals)\n\t    __repr__ = __str__\n\tclass Axis(enum.Enum):\n\t    QUERY = enum.auto()\n\t    REF = enum.auto()\n\tclass Match(NamedTuple):\n\t    \"\"\"A ground-truth match or predicted match.\"\"\"\n\t    query_id: str\n\t    ref_id: str\n", "    query_start: float\n\t    query_end: float\n\t    ref_start: float\n\t    ref_end: float\n\t    score: float\n\t    def pair_id(self):\n\t        return (self.query_id, self.ref_id)\n\t    def interval(self, axis: Axis) -> Tuple[float, float]:\n\t        if axis == Axis.QUERY:\n\t            return (self.query_start, self.query_end)\n", "        else:\n\t            return (self.ref_start, self.ref_end)\n\t    def intersection_area(self, bbox: \"Match\") -> float:\n\t        # Compute the intersection boarders\n\t        inter_q_start = max(self.query_start, bbox.query_start)\n\t        inter_r_start = max(self.ref_start, bbox.ref_start)\n\t        inter_q_end = min(self.query_end, bbox.query_end)\n\t        inter_r_end = min(self.ref_end, bbox.ref_end)\n\t        # Compute the area of intersection rectangle\n\t        return abs(\n", "            max((inter_q_end - inter_q_start, 0))\n\t            * max((inter_r_end - inter_r_start), 0)\n\t        )\n\t    def overlaps(self, bbox: \"Match\") -> bool:\n\t        return self.intersection_area(bbox) > 0.0\n\t    @classmethod\n\t    def write_csv(\n\t        cls, matches: Collection[\"Match\"], file: Union[str, TextIO], drop_dup=False\n\t    ):\n\t        df = pd.DataFrame([match._asdict() for match in matches], columns=cls._fields)\n", "        if drop_dup:\n\t            df[\"score\"] = df.groupby(\n\t                [\n\t                    \"query_id\",\n\t                    \"ref_id\",\n\t                    \"query_start\",\n\t                    \"query_end\",\n\t                    \"ref_start\",\n\t                    \"ref_end\",\n\t                ]\n", "            )[\"score\"].transform(\"max\")\n\t            df.drop_duplicates(\n\t                [\n\t                    \"query_id\",\n\t                    \"ref_id\",\n\t                    \"query_start\",\n\t                    \"query_end\",\n\t                    \"ref_start\",\n\t                    \"ref_end\",\n\t                ],\n", "                keep=\"first\",\n\t                inplace=True,\n\t            )\n\t        df = df.sort_values(by=\"score\", ascending=False)\n\t        df.to_csv(file, index=False)\n\t    @classmethod\n\t    def read_csv(\n\t        cls, file: Union[str, TextIO], is_gt=False, check=True\n\t    ) -> List[\"Match\"]:\n\t        df = pd.read_csv(file)\n", "        df[\"query_id\"] = df.query_id.map(lambda x: format_video_id(x, Dataset.QUERIES))\n\t        df[\"ref_id\"] = df.ref_id.map(lambda x: format_video_id(x, Dataset.REFS))\n\t        if is_gt:\n\t            df[\"score\"] = 1.0\n\t        if check:\n\t            for field in cls._fields:\n\t                assert not df[field].isna().any()\n\t        return [Match(**record) for record in df.to_dict(\"records\")]\n\tclass VideoPair:\n\t    \"\"\"A video pair item that contains information regarding the gt and pred bboxes.\n", "    Provide functionalities for the combination of new predictions with the\n\t    existing ones and the computation of their intersection with the gt bboxes,\n\t    ignoring the gt bboxes that do not overlap with any prediction.\n\t    \"\"\"\n\t    gts: List[Match]\n\t    preds: List[Match]\n\t    def __init__(\n\t        self,\n\t    ):\n\t        self.intersections = {axis: 0.0 for axis in Axis}\n", "        self.totals = {axis: 0.0 for axis in Axis}\n\t        self.gts = []\n\t        self.preds = []\n\t    def total_gt_length(self, axis: Axis) -> float:\n\t        return Intervals([gt.interval(axis) for gt in self.gts]).total_length()\n\t    def total_pred_length(self, axis: Axis) -> float:\n\t        return Intervals([pred.interval(axis) for pred in self.preds]).total_length()\n\t    def gt_overlaps(self, gt: Match) -> bool:\n\t        \"\"\"Checks if the provided gt bbox overlaps with at least one pred bbox.\"\"\"\n\t        for pred in self.preds:\n", "            if gt.overlaps(pred):\n\t                return True\n\t        return False\n\t    def add_gt(self, bbox: Match):\n\t        self.gts.append(bbox)\n\t    def add_prediction(\n\t        self, bbox: Match\n\t    ) -> Tuple[Dict[Axis, float], Dict[Axis, float]]:\n\t        \"\"\"Add a prediction to the corresponding list and calculates the\n\t        differences in the intersections with the gt and the total video\n", "        length covered for both query and reference axes.\n\t        \"\"\"\n\t        self.preds.append(bbox)\n\t        # A subset of GTs to consider for intersection (but not total GT length).\n\t        gts_to_consider = [gt for gt in self.gts if self.gt_overlaps(gt)]\n\t        intersect_deltas = {}\n\t        total_deltas = {}\n\t        for axis in Axis:\n\t            pred_ints = Intervals([pred.interval(axis) for pred in self.preds])\n\t            gt_ints = Intervals([gt.interval(axis) for gt in gts_to_consider])\n", "            # New intersection and total length on this axis\n\t            intersect_length = pred_ints.intersect_length(gt_ints)\n\t            prediction_length = pred_ints.total_length()\n\t            # Compute differences\n\t            intersect_deltas[axis] = intersect_length - self.intersections[axis]\n\t            total_deltas[axis] = prediction_length - self.totals[axis]\n\t            # Update with new values\n\t            self.intersections[axis] = intersect_length\n\t            self.totals[axis] = prediction_length\n\t        return intersect_deltas, total_deltas\n", "def match_metric(\n\t    gts: Collection[Match],\n\t    predictions: Collection[Match],\n\t) -> AveragePrecision:\n\t    \"\"\"Matching track metric:\n\t    Computes the AP based on the VCSL approach for the\n\t    calculation of Precision and Recall.\n\t    AP = \\sum_{i=1}^N P(i) R(i)\n\t    where, P(i) = sqrt(P_q * P_r) and R(i) = sqrt(R_q * R_r)\n\t    calculated as in the VCSL.\n", "    \"\"\"  # noqa: W605\n\t    predictions = sorted(predictions, key=lambda x: x.score, reverse=True)\n\t    # Initialize video pairs and load their gt bboxs\n\t    video_pairs = defaultdict(VideoPair)\n\t    for gt in gts:\n\t        video_pairs[gt.pair_id()].add_gt(gt)\n\t    # Get the total gt length for each axis\n\t    gt_total_lengths = {axis: 0.0 for axis in Axis}\n\t    for _, v in video_pairs.items():\n\t        for axis in Axis:\n", "            gt_total_lengths[axis] += v.total_gt_length(axis)\n\t    # Loop through the predictions\n\t    recall = 0.0\n\t    metric = 0.0\n\t    intersections = {axis: 0.0 for axis in Axis}\n\t    totals = {axis: 0.0 for axis in Axis}\n\t    pr_recalls = []\n\t    pr_precisions = []\n\t    pr_scores = []\n\t    # Update metrics for all predictions with the same score as a group.\n", "    for score, prediction_group in itertools.groupby(\n\t        predictions, key=lambda x: x.score\n\t    ):\n\t        for prediction in prediction_group:\n\t            # Given new predictions, we only need the differences in the intersection with\n\t            # gt and total video length covered for both query and reference axes.\n\t            # This derives from the sum of differences for every pair id\n\t            intersection_deltas, total_deltas = video_pairs[\n\t                prediction.pair_id()\n\t            ].add_prediction(prediction)\n", "            for axis in Axis:\n\t                # Accumulate the differences to the corresponding values\n\t                intersections[axis] += intersection_deltas[axis]\n\t                totals[axis] += total_deltas[axis]\n\t        recalls = {}\n\t        precisions = {}\n\t        for axis in Axis:\n\t            recalls[axis] = intersections[axis] / gt_total_lengths[axis]\n\t            precisions[axis] = intersections[axis] / totals[axis]\n\t        new_recall = sqrt(recalls[Axis.QUERY] * recalls[Axis.REF])\n", "        precision = sqrt(precisions[Axis.QUERY] * precisions[Axis.REF])\n\t        # Compute metric\n\t        delta_recall = new_recall - recall\n\t        metric += precision * delta_recall\n\t        recall = new_recall\n\t        if delta_recall > 0:\n\t            pr_recalls.append(recall)\n\t            pr_precisions.append(precision)\n\t            pr_scores.append(score)\n\t    curve = PrecisionRecallCurve(\n", "        np.array(pr_precisions), np.array(pr_recalls), np.array(pr_scores)\n\t    )\n\t    return AveragePrecision(metric, curve)\n\t@dataclasses.dataclass\n\tclass MatchingTrackMetrics:\n\t    # Our main evaluation metric.\n\t    segment_ap: AveragePrecision\n\t    # This metric reflects only pairwise matching, and not localization.\n\t    pairwise_micro_ap: AveragePrecision\n\tdef evaluate_matching_track(\n", "    ground_truth_filename: str, predictions_filename: str\n\t) -> MatchingTrackMetrics:\n\t    \"\"\"Matching track evaluation.\n\t    Predictions are expected to be a CSV file, with a column names in the header.\n\t    The following columns must be present, in any order:\n\t        query_id: str, the ID of the query for this match\n\t        ref_id: str, the ID of the reference for this match\n\t        query_start: float, the start of the query segment in seconds\n\t        query_end: float, the end of the query segment in seconds\n\t        ref_start: float, the start of the reference segment in seconds\n", "        ref_end: float, the end of the reference segment in seconds\n\t        score: float, the score of this prediction (a higher score indicates a\n\t            more confident prediction)\n\t    Note that ground-truth matches are specified using the same format, but score\n\t    is not used.\n\t    \"\"\"\n\t    gt = Match.read_csv(ground_truth_filename, is_gt=True)\n\t    predictions = Match.read_csv(predictions_filename)\n\t    metric = match_metric(gt, predictions)\n\t    # Auxiliary metric: pairwise uAP\n", "    gt_pairs = CandidatePair.from_matches(gt)\n\t    pairs = CandidatePair.from_matches(predictions)\n\t    pair_ap = average_precision(gt_pairs, pairs)\n\t    return MatchingTrackMetrics(segment_ap=metric, pairwise_micro_ap=pair_ap)\n\tdef average_precision(\n\t    ground_truth: Collection[CandidatePair], predictions: Collection[CandidatePair]\n\t) -> AveragePrecision:\n\t    gt_pairs = {(pair.query_id, pair.ref_id) for pair in ground_truth}\n\t    if len(gt_pairs) != len(ground_truth):\n\t        raise AssertionError(\"Duplicates detected in ground truth\")\n", "    predicted_pairs = {(pair.query_id, pair.ref_id) for pair in predictions}\n\t    if len(predicted_pairs) != len(predictions):\n\t        raise AssertionError(\"Duplicates detected in predictions\")\n\t    # AP calculation that aligns with DrivenData's backend implementation.\n\t    canonical_ap = drivendata_average_precision(\n\t        predicted=CandidatePair.to_dataframe(predictions),\n\t        ground_truth=CandidatePair.to_dataframe(ground_truth),\n\t    )\n\t    predictions = sorted(predictions, key=lambda x: x.score, reverse=True)\n\t    scores = np.array([pair.score for pair in predictions])\n", "    correct = np.array(\n\t        [(pair.query_id, pair.ref_id) in gt_pairs for pair in predictions]\n\t    )\n\t    total_pairs = len(gt_pairs)\n\t    # precision = correct_so_far / total_pairs_so_far\n\t    cumulative_correct = np.cumsum(correct)\n\t    cumulative_predicted = np.arange(len(correct)) + 1\n\t    recall = cumulative_correct / total_pairs\n\t    precision = cumulative_correct / cumulative_predicted\n\t    # Simple AP computation.\n", "    simple_ap = np.sum(precision * correct) / total_pairs\n\t    # Get precision and recall where correct is true\n\t    indices = np.nonzero(correct)[0]\n\t    curve = PrecisionRecallCurve(precision[indices], recall[indices], scores[indices])\n\t    return AveragePrecision(ap=canonical_ap, pr_curve=curve, simple_ap=simple_ap)\n\tdef drivendata_average_precision(\n\t    predicted: pd.DataFrame,\n\t    ground_truth: pd.DataFrame,\n\t):\n\t    \"\"\"Canonical AP implementation used for the challenge.\"\"\"\n", "    SCORE_COL = \"score\"\n\t    QUERY_ID_COL = \"query_id\"\n\t    DATABASE_ID_COL = \"ref_id\"\n\t    actual = ground_truth[[\"query_id\", \"ref_id\"]]\n\t    if (\n\t        not np.isfinite(predicted[SCORE_COL]).all()\n\t        or np.isnan(predicted[SCORE_COL]).any()\n\t    ):\n\t        raise ValueError(\"Scores must be finite.\")\n\t    predicted = predicted.sort_values(SCORE_COL, ascending=False)\n", "    merged = predicted.merge(\n\t        right=actual.assign(actual=1.0),\n\t        how=\"left\",\n\t        on=[QUERY_ID_COL, DATABASE_ID_COL],\n\t    ).fillna({\"actual\": 0.0})\n\t    # We may not predict for every ground truth, so calculate unadjusted AP then adjust it\n\t    unadjusted_ap = (\n\t        average_precision_score(merged[\"actual\"].values, merged[SCORE_COL].values)\n\t        if merged[\"actual\"].sum()\n\t        else 0.0\n", "    )\n\t    # Rescale average precisions based on total ground truth positive counts\n\t    predicted_n_pos = int(merged[\"actual\"].sum())\n\t    # avoid rows added to validate query ids, will have blank ref_id\n\t    actual_n_pos = int(actual[DATABASE_ID_COL].notna().sum())\n\t    adjusted_ap = unadjusted_ap * (predicted_n_pos / actual_n_pos)\n\t    return adjusted_ap\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/candidates.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.neration\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import List\n\timport numpy as np\n\tfrom src.vsc.index import PairMatches, VideoFeature, VideoIndex\n\tfrom src.vsc.metrics import CandidatePair\n\tclass ScoreAggregation(ABC):\n", "    @abstractmethod\n\t    def aggregate(self, match: PairMatches) -> float:\n\t        pass\n\t    def score(self, match: PairMatches) -> CandidatePair:\n\t        score = self.aggregate(match)\n\t        return CandidatePair(query_id=match.query_id, ref_id=match.ref_id, score=score)\n\tclass MaxScoreAggregation(ScoreAggregation):\n\t    def aggregate(self, match: PairMatches) -> float:\n\t        return np.max([m.score for m in match.matches])\n\tclass CandidateGeneration:\n", "    def __init__(self, references: List[VideoFeature], aggregation: ScoreAggregation):\n\t        self.aggregation = aggregation\n\t        dim = references[0].dimensions()\n\t        self.index = VideoIndex(dim)\n\t        self.index.add(references)\n\t    def query(self, queries: List[VideoFeature], global_k: int) -> List[CandidatePair]:\n\t        matches = self.index.search(queries, global_k=global_k)\n\t        candidates = [self.aggregation.score(match) for match in matches]\n\t        candidates = sorted(candidates, key=lambda match: match.score, reverse=True)\n\t        return candidates\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/storage.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\tfrom typing import List, Optional\n\timport numpy as np\n\tfrom src.vsc.index import VideoFeature\n\tfrom src.vsc.metrics import Dataset, format_video_id\n\tdef store_features(f, features: List[VideoFeature], dataset: Optional[Dataset] = None):\n\t    video_ids = []\n", "    feats = []\n\t    timestamps = []\n\t    for feature in features:\n\t        video_id = format_video_id(feature.video_id, dataset)\n\t        video_ids.append(np.full(len(feature), video_id))\n\t        feats.append(feature.feature)\n\t        timestamps.append(feature.timestamps)\n\t    video_ids = np.concatenate(video_ids)\n\t    feats = np.concatenate(feats)\n\t    timestamps = np.concatenate(timestamps)\n", "    np.savez(f, video_ids=video_ids, features=feats, timestamps=timestamps)\n\tdef same_value_ranges(values):\n\t    start = 0\n\t    value = values[start]\n\t    for i, v in enumerate(values):\n\t        if v == value:\n\t            continue\n\t        yield value, start, i\n\t        start = i\n\t        value = values[start]\n", "    yield value, start, len(values)\n\tdef load_features(f, dataset: Optional[Dataset] = None):\n\t    data = np.load(f, allow_pickle=False)\n\t    video_ids = data[\"video_ids\"]\n\t    feats = data[\"features\"]\n\t    timestamps = data[\"timestamps\"]\n\t    ts_dims = len(timestamps.shape)\n\t    if timestamps.shape[0] != feats.shape[0]:\n\t        raise ValueError(\n\t            f\"Expected the same number of timestamps as features: got \"\n", "            f\"{timestamps.shape[0]} timestamps for {feats.shape[0]} features\"\n\t        )\n\t    if not (ts_dims == 1 or timestamps.shape[1:] == (2,)):\n\t        print(f\"timestamps.shape[1:]: {timestamps.shape[1:]}\")\n\t        print(f\"timestamps.shape[1:] == [2]: {timestamps.shape[1:] == [2]}\")\n\t        raise ValueError(f\"Unexpected timestamp shape. Got {timestamps.shape}\")\n\t    results = []\n\t    for video_id, start, end in same_value_ranges(video_ids):\n\t        video_id = format_video_id(video_id, dataset)\n\t        results.append(\n", "            VideoFeature(\n\t                video_id=video_id,\n\t                timestamps=timestamps[start:end],\n\t                feature=feats[start:end, :],\n\t            )\n\t        )\n\t    return results\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/index.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport collections\n\timport logging\n\tfrom dataclasses import dataclass\n\tfrom typing import Iterable, List, NamedTuple, Tuple\n\timport faiss  # @manual\n\timport numpy as np\n", "from faiss.contrib import exhaustive_search  # @manual\n\tSearchIndices = Tuple[int, int, float]\n\t@dataclass\n\tclass VideoMetadata:\n\t    video_id: str\n\t    timestamps: np.ndarray  # either Nx2 (start and end timestamps) or N\n\t    def __len__(self):\n\t        return self.timestamps.shape[0]\n\t    def get_timestamps(self, idx: int) -> Tuple[float, float]:\n\t        t = self.timestamps[idx]\n", "        if len(self.timestamps.shape) == 1:\n\t            return (t, t)\n\t        return (t[0], t[1])\n\t@dataclass\n\tclass VideoFeature(VideoMetadata):\n\t    feature: np.ndarray\n\t    def __post_init__(self):\n\t        assert self.feature.shape[0] == len(\n\t            self.timestamps\n\t        ), \"Mismatched timestamps / feature size\"\n", "    def metadata(self):\n\t        return VideoMetadata(video_id=self.video_id, timestamps=self.timestamps)\n\t    def dimensions(self):\n\t        return self.feature.shape[1]\n\tclass PairMatch(NamedTuple):\n\t    query_timestamps: Tuple[float, float]\n\t    ref_timestamps: Tuple[float, float]\n\t    score: float\n\t@dataclass\n\tclass PairMatches:\n", "    query_id: str\n\t    ref_id: str\n\t    matches: List[PairMatch]\n\t    def records(self):\n\t        for match in self.matches:\n\t            yield {\n\t                \"query_id\": self.query_id,\n\t                \"ref_id\": self.ref_id,\n\t                \"query_start\": match.query_timestamps[0],\n\t                \"query_end\": match.query_timestamps[1],\n", "                \"ref_start\": match.ref_timestamps[0],\n\t                \"ref_end\": match.ref_timestamps[1],\n\t                \"score\": match.score,\n\t            }\n\tclass VideoIndex:\n\t    def __init__(\n\t        self,\n\t        dim: int,\n\t        codec_str: str = \"Flat\",\n\t        metric: int = faiss.METRIC_INNER_PRODUCT,\n", "    ):\n\t        self.dim = dim\n\t        self.index = faiss.index_factory(self.dim, codec_str, metric)\n\t        self.video_clip_idx = []\n\t        self.video_clip_to_video_ids = []\n\t        self.video_metadata = {}\n\t    def add(self, db: List[VideoFeature]):\n\t        for vf in db:\n\t            self.video_clip_idx.extend(list(range(vf.feature.shape[0])))\n\t            self.video_clip_to_video_ids.extend(\n", "                [vf.video_id for _ in range(vf.feature.shape[0])]\n\t            )\n\t            self.video_metadata[vf.video_id] = vf.metadata()\n\t            self.index.add(vf.feature)\n\t    def search(\n\t        self,\n\t        queries: List[VideoFeature],\n\t        global_k: int,\n\t    ) -> List[PairMatches]:\n\t        query_ids = []\n", "        query_indices = []\n\t        for q in queries:\n\t            query_ids.extend([q.video_id] * len(q))\n\t            query_indices.extend(range(len(q)))\n\t        query_metadatas = {q.video_id: q.metadata() for q in queries}\n\t        query_features = np.concatenate([q.feature for q in queries])\n\t        if global_k < 0:\n\t            # Negative values cause us to use vanilla KNN search\n\t            k = -global_k\n\t            logging.warn(\n", "                \"Using local k for KNN search. Warning: this is against the \"\n\t                \"VSC rules, since predictions for a query-ref pair are not \"\n\t                \"independent of other references. KNN search is provided for \"\n\t                \"comparison.\"\n\t            )\n\t            search_indices = self._knn_search(query_features, k)\n\t        else:\n\t            search_indices = self._global_threshold_knn_search(query_features, global_k)\n\t        pair_nns = collections.defaultdict(list)\n\t        for i, j, score in search_indices:\n", "            query_id = query_ids[i]\n\t            query_idx = query_indices[i]\n\t            query_metadata = query_metadatas[query_id]\n\t            ref_id = self.video_clip_to_video_ids[j]\n\t            ref_idx = self.video_clip_idx[j]\n\t            ref_metadata = self.video_metadata[ref_id]\n\t            match = PairMatch(\n\t                query_timestamps=query_metadata.get_timestamps(query_idx),\n\t                ref_timestamps=ref_metadata.get_timestamps(ref_idx),\n\t                score=score,\n", "            )\n\t            pair_nns[query_id, ref_id].append(match)\n\t        return [\n\t            PairMatches(query_id, ref_id, matches)\n\t            for ((query_id, ref_id), matches) in pair_nns.items()\n\t        ]\n\t    def _global_threshold_knn_search(\n\t        self, query_features: np.ndarray, global_k: int\n\t    ) -> Iterable[SearchIndices]:\n\t        use_similarity = self.index.metric_type == faiss.METRIC_INNER_PRODUCT\n", "        initial_radius = -1e10 if use_similarity else 1e10\n\t        _, limits, similarity, indices = exhaustive_search.range_search_max_results(\n\t            self.index,\n\t            exhaustive_search.exponential_query_iterator(query_features),\n\t            initial_radius,\n\t            max_results=2 * global_k,\n\t            min_results=global_k,\n\t            ngpu=-1,  # use GPU if available\n\t        )\n\t        nq = query_features.shape[0]\n", "        search_indices = []\n\t        for i in range(nq):\n\t            for j in range(limits[i], limits[i + 1]):\n\t                search_indices.append((i, indices[j], similarity[j]))\n\t        search_indices.sort(key=lambda x: x[2], reverse=use_similarity)\n\t        if len(search_indices) > global_k:\n\t            search_indices = search_indices[:global_k]\n\t        return search_indices\n\t    def _knn_search(self, query_features: np.ndarray, k) -> Iterable[SearchIndices]:\n\t        index = self.index\n", "        if faiss.get_num_gpus() > 0:\n\t            logging.info(\"Moving index to GPU\")\n\t            index = faiss.index_cpu_to_all_gpus(self.index)\n\t        logging.info(\"Performing KNN search\")\n\t        similarity, ids = index.search(query_features, k)\n\t        for i in range(ids.shape[0]):\n\t            for j in range(ids.shape[1]):\n\t                yield (i, ids[i, j], similarity[i, j])\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/vsc/localization.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport abc\n\tfrom typing import List\n\timport numpy as np\n\timport torch\n\tfrom src.vsc.index import VideoFeature\n\tfrom src.vsc.metrics import CandidatePair, Match\n", "class Localization(abc.ABC):\n\t    @abc.abstractmethod\n\t    def localize(self, candidate: CandidatePair) -> List[Match]:\n\t        pass\n\t    def localize_all(self, candidates: List[CandidatePair]) -> List[Match]:\n\t        matches = []\n\t        for candidate in candidates:\n\t            matches.extend(self.localize(candidate))\n\t        return matches\n\tclass LocalizationWithMetadata(Localization):\n", "    def __init__(self, queries: List[VideoFeature], refs: List[VideoFeature]):\n\t        self.queries = {m.video_id: m for m in queries}\n\t        self.refs = {m.video_id: m for m in refs}\n\t    def similarity(self, candidate: CandidatePair):\n\t        a = self.queries[candidate.query_id].feature\n\t        b = self.refs[candidate.ref_id].feature\n\t        return np.matmul(a, b.T)\n\t    def count_views(self, candidate: CandidatePair):\n\t        a = self.queries[candidate.query_id].timestamps\n\t        q_views = np.count_nonzero(a == a[0])\n", "        b = self.refs[candidate.ref_id].timestamps\n\t        r_views = np.count_nonzero(b == b[0])\n\t        return [q_views, r_views]\n\tclass VCSLLocalization(LocalizationWithMetadata):\n\t    def __init__(self, queries, refs, model_type, similarity_bias=0.0, **kwargs):\n\t        super().__init__(queries, refs)\n\t        # Late import: allow OSS use without VCSL installed\n\t        from src.vsc.vta import build_vta_model  # @manual\n\t        self.model = build_vta_model(model_type, **kwargs)\n\t        self.similarity_bias = similarity_bias\n", "    def similarity(self, candidate: CandidatePair):\n\t        \"\"\"Add an optional similarity bias.\n\t        Some localization methods do not tolerate negative values well.\n\t        \"\"\"\n\t        return super().similarity(candidate) + self.similarity_bias\n\t    def localize_all(self, candidates: List[CandidatePair]) -> List[Match]:\n\t        # sims = [(f\"{c.query_id}-{c.ref_id}\", self.similarity(c)) for c in candidates]\n\t        sims = [\n\t            (f\"{c.query_id}-{c.ref_id}\", self.count_views(c), self.similarity(c))\n\t            for c in candidates\n", "        ]\n\t        results = self.model.forward_sim(sims)\n\t        assert len(results) == len(candidates)\n\t        matches = []\n\t        for (candidate, (key, _, sim), result) in zip(candidates, sims, results):\n\t            query: VideoFeature = self.queries[candidate.query_id]\n\t            ref: VideoFeature = self.refs[candidate.ref_id]\n\t            assert key == result[0]\n\t            for box in result[1]:\n\t                (x1, y1, x2, y2, score) = box\n", "                match = Match(\n\t                    query_id=candidate.query_id,\n\t                    ref_id=candidate.ref_id,\n\t                    query_start=query.get_timestamps(x1)[0],\n\t                    query_end=query.get_timestamps(x2)[1],\n\t                    ref_start=ref.get_timestamps(y1)[0],\n\t                    ref_end=ref.get_timestamps(y2)[1],\n\t                    score=0.0,\n\t                )\n\t                # score = self.score(candidate, match, box, sim)\n", "                match = match._replace(score=score)\n\t                matches.append(match)\n\t        return matches\n\t    def localize(self, candidate: CandidatePair) -> List[Match]:\n\t        return self.localize_all([candidate])\n\t    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n\t        return 1.0\n\tclass VCSLLocalizationMaxSim(VCSLLocalization):\n\t    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n\t        x1, y1, x2, y2, _score = box\n", "        return similarity[x1:x2, y1:y2].max() - self.similarity_bias\n\tclass VCSLLocalizationMatchScore(VCSLLocalization):\n\t    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n\t        x1, y1, x2, y2, _score = box\n\t        return _score\n\tclass VCSLLocalizationCandidateScore(VCSLLocalization):\n\t    def score(self, candidate: CandidatePair, match: Match, box, similarity) -> float:\n\t        return candidate.score\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/ffmpeg_video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport os\n\timport subprocess\n\timport tempfile\n\tfrom typing import Iterable, Optional, Tuple\n\tfrom PIL import Image\n\tfrom src.video_reader.video_reader import VideoReader\n", "from torchvision.datasets.folder import default_loader\n\tfrom torchvision.io.image import ImageReadMode, read_image\n\tImageT = Image.Image\n\tclass FFMpegVideoReader(VideoReader):\n\t    def __init__(\n\t        self, video_path: str, required_fps: float, output_type: str, ffmpeg_path: str\n\t    ):\n\t        self.ffmpeg_path = ffmpeg_path\n\t        super().__init__(video_path, required_fps, output_type)\n\t    @property\n", "    def fps(self) -> Optional[float]:\n\t        return None\n\t    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n\t        with tempfile.TemporaryDirectory() as dir, open(os.devnull, \"w\") as null:\n\t            subprocess.check_call(\n\t                [\n\t                    self.ffmpeg_path,\n\t                    \"-nostdin\",\n\t                    \"-y\",\n\t                    \"-i\",\n", "                    self.video_path,\n\t                    \"-start_number\",\n\t                    \"0\",\n\t                    \"-q\",\n\t                    \"0\",\n\t                    \"-vf\",\n\t                    \"fps=%f\" % self.required_fps,\n\t                    os.path.join(dir, \"%07d.png\"),\n\t                ],\n\t                stderr=null,\n", "            )\n\t            i = 0\n\t            while True:\n\t                frame_fn = os.path.join(dir, f\"{i:07d}.png\")\n\t                if not os.path.exists(frame_fn):\n\t                    break\n\t                if self.output_type == \"pil\":\n\t                    img = default_loader(frame_fn)\n\t                elif self.output_type == \"tensor\":\n\t                    img = read_image(frame_fn, mode=ImageReadMode.RGB)\n", "                i += 1\n\t                yield ((i - 1) / self.original_fps, i / self.original_fps, img)\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/ffmpeg_py_video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport warnings\n\tfrom typing import Iterable, Optional, Tuple\n\timport ffmpeg\n\timport numpy as np\n\timport torch\n\tfrom PIL import Image\n", "from src.video_reader.video_reader import VideoReader\n\tImageT = Image.Image\n\tclass FFMpegPyVideoReader(VideoReader):\n\t    def __init__(self, video_path: str, required_fps: float, output_type: str):\n\t        super().__init__(video_path, required_fps, output_type)\n\t    @property\n\t    def fps(self) -> Optional[float]:\n\t        return None\n\t    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n\t        buffer, _ = (\n", "            ffmpeg.input(self.video_path)\n\t            .video.filter(\"fps\", self.required_fps)\n\t            .output(\"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\")\n\t            .run(capture_stdout=True, capture_stderr=True)\n\t        )\n\t        meta = ffmpeg.probe(self.video_path)[\"streams\"][0]\n\t        width, height = meta[\"width\"], meta[\"height\"]\n\t        if self.output_type == \"pil\":\n\t            frames = np.frombuffer(buffer, dtype=np.uint8).reshape(-1, height, width, 3)\n\t            for i, frame in enumerate(frames):\n", "                img = Image.fromarray(frame).convert(\"RGB\")\n\t                yield i / self.original_fps, (i + 1) / self.original_fps, img\n\t        elif self.output_type == \"tensor\":\n\t            with warnings.catch_warnings():\n\t                warnings.filterwarnings(\n\t                    \"ignore\", message=\"The given buffer is not writable\"\n\t                )\n\t                frames = (\n\t                    torch.frombuffer(buffer, dtype=torch.uint8)\n\t                    .reshape((-1, height, width, 3))\n", "                    .permute(0, 3, 1, 2)\n\t                )\n\t            for i, frame in enumerate(frames):\n\t                yield i / self.original_fps, (i + 1) / self.original_fps, frame\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/__init__.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/video_reader.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t#\n\t# This source code is licensed under the MIT license found in the\n\t# LICENSE file in the root directory of this source tree.\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import Iterable, Optional, Tuple, Union\n\timport torch\n\tfrom PIL import Image\n\tImageT = Union[Image.Image, torch.Tensor]\n\tclass VideoReader(ABC):\n", "    def __init__(self, video_path: str, required_fps: float, output_type: str) -> None:\n\t        self.video_path = video_path\n\t        self.required_fps = required_fps\n\t        self.output_type = output_type\n\t        self.original_fps = max(1, self.fps) if self.fps else 1\n\t        self.video_frames = None\n\t    @property\n\t    @abstractmethod\n\t    def fps(self) -> Optional[float]:\n\t        pass\n", "    @abstractmethod\n\t    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n\t        \"\"\"\n\t        returns a tuple of [start_time, end_time, Image]\n\t        \"\"\"\n\t        pass\n"]}
{"filename": "meta-vsc-matching-runtime/submission_src/src/video_reader/decord_video_reader.py", "chunked_list": ["\"\"\"\n\tCopyright 2023 LINE Corporation\n\tLINE Corporation licenses this file to you under the Apache License,\n\tversion 2.0 (the \"License\"); you may not use this file except in compliance\n\twith the License. You may obtain a copy of the License at:\n\t    https://www.apache.org/licenses/LICENSE-2.0\n\tUnless required by applicable law or agreed to in writing, software\n\tdistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n\tWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n\tLicense for the specific language governing permissions and limitations\n", "under the License.\n\t\"\"\"\n\timport os\n\tfrom typing import Iterable, Optional, Tuple\n\timport decord\n\timport numpy as np\n\timport torch\n\tfrom PIL import Image\n\tfrom src.video_reader.video_reader import VideoReader\n\tfrom torchvision.datasets.folder import default_loader\n", "ImageT = Image.Image\n\tclass DecordVideoReader(VideoReader):\n\t    def __init__(self, video_path: str, required_fps: float, output_type: str):\n\t        super().__init__(video_path, required_fps, output_type)\n\t    @property\n\t    def fps(self) -> Optional[float]:\n\t        return self.required_fps\n\t    def frames(self) -> Iterable[Tuple[float, float, ImageT]]:\n\t        reader = decord.VideoReader(\n\t            self.video_path,\n", "            ctx=decord.cpu(0),\n\t            num_threads=0,\n\t        )\n\t        n_frames = len(reader)\n\t        timestamps = reader.get_frame_timestamp(np.arange(n_frames))\n\t        end_timestamps = timestamps[:, 1]\n\t        duration = end_timestamps[-1].tolist()\n\t        fps = min(self.required_fps, n_frames / duration)\n\t        count = max(1, np.round(duration * fps))\n\t        step_size = 1 / fps\n", "        frame_pos = (np.arange(count) + 0.5) * step_size\n\t        frame_ids = np.searchsorted(end_timestamps, frame_pos)\n\t        frame_ids = np.minimum(frame_ids, n_frames - 1)\n\t        frames = reader.get_batch(frame_ids).asnumpy()\n\t        for i, frame in enumerate(frames):\n\t            if self.output_type == \"pil\":\n\t                img = Image.fromarray(frame).convert(\"RGB\")\n\t            elif self.output_type == \"tensor\":\n\t                img = torch.as_tensor(frame).permute(2, 0, 1)\n\t            yield (\n", "                i / self.original_fps,\n\t                (i + 1) / self.original_fps,\n\t                img,\n\t            )\n"]}
{"filename": "meta-vsc-matching-runtime/runtime/validation.py", "chunked_list": ["#!/usr/bin/env python3\n\t\"\"\"\n\tMatching Track validation script\n\t\"\"\"\n\tfrom argparse import ArgumentParser, Namespace\n\tfrom typing import List\n\timport pandas as pd\n\tparser = ArgumentParser()\n\tparser.add_argument(\n\t    \"--path\",\n", "    help=\"Path to match csv\",\n\t    type=str,\n\t    required=True,\n\t)\n\tclass DataValidationError(AssertionError):\n\t    pass\n\tdef validate_column_order(columns: List[str]):\n\t    expected_cols = [\n\t        \"query_id\",\n\t        \"ref_id\",\n", "        \"query_start\",\n\t        \"query_end\",\n\t        \"ref_start\",\n\t        \"ref_end\",\n\t        \"score\",\n\t    ]\n\t    for i, (expected, found) in enumerate(zip(expected_cols, columns)):\n\t        if expected != found:\n\t            raise DataValidationError(\n\t                f\"Columns in incorrect order. Expected {expected} in position {i}, but got {found}.\"\n", "            )\n\tdef validate_timestamps(df: pd.DataFrame):\n\t    for col in [\"query_start\", \"query_end\", \"ref_start\", \"ref_end\"]:\n\t        if not (df[col] >= 0).all():\n\t            raise DataValidationError(\n\t                f\"Found negative timestamps in {col}: \"\n\t                f\"all timestamps should be greater than or equal to zero.\"\n\t            )\n\t    if not (df[\"query_start\"] <= df[\"query_end\"]).all():\n\t        raise DataValidationError(\n", "            f\"Found query start timestamps greater than query end timestamps: \"\n\t            f\"all end timestamps should be greater than start timestamps.\"\n\t        )\n\t    if not (df[\"ref_start\"] <= df[\"ref_end\"]).all():\n\t        raise DataValidationError(\n\t            f\"Found query start timestamps greater than query end timestamps: \"\n\t            f\"all end timestamps should be greater than start timestamps.\"\n\t        )\n\tdef main(args: Namespace):\n\t    matches_df = pd.read_csv(args.path)\n", "    validate_column_order(matches_df.columns)\n\t    validate_timestamps(matches_df)\n\tif __name__ == \"__main__\":\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "meta-vsc-matching-runtime/runtime/conftest.py", "chunked_list": ["def pytest_addoption(parser):\n\t    parser.addoption(\"--submission-path\", action=\"store\", default=\"submission.csv\")\n"]}
{"filename": "meta-vsc-matching-runtime/runtime/tests/test_packages.py", "chunked_list": ["# adapted from pangeo https://github.com/pangeo-data/pangeo-docker-images/blob/master/tests/test_pangeo-notebook.py\n\timport importlib\n\timport subprocess\n\timport warnings\n\timport pytest\n\tpackages = [\n\t    # these are problem libraries that don't always seem to import, mostly due\n\t    # to dependencies outside the python world\n\t    \"fastai\",\n\t    \"keras\",\n", "    \"numpy\",\n\t    \"pandas\",\n\t    \"scipy\",\n\t    \"sklearn\",  # scikit-learn\n\t    \"tensorflow\",\n\t    \"torch\",  # pytorch\n\t    \"faiss\",\n\t]\n\t@pytest.mark.parametrize(\"package_name\", packages, ids=packages)\n\tdef test_import(package_name):\n", "    importlib.import_module(package_name)\n\tdef test_gpu_packages():\n\t    try:\n\t        subprocess.check_call([\"nvidia-smi\"])\n\t        import torch\n\t        assert torch.cuda.is_available()\n\t        import tensorflow as tf\n\t        assert tf.test.is_built_with_cuda()\n\t        assert tf.config.list_physical_devices(\"GPU\")\n\t    except FileNotFoundError:\n", "        warnings.warn(\n\t            \"Skipping GPU import tests since nvidia-smi is not present on test machine.\"\n\t        )\n"]}
