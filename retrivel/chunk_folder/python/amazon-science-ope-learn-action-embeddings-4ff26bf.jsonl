{"filename": "entry_point.py", "chunked_list": ["import argparse\n\timport importlib\n\timport json\n\timport os\n\timport pkgutil\n\tfrom logging import getLogger\n\tfrom types import SimpleNamespace\n\tfrom jobs.abstracts.abstract_job import AbstractJob\n\tlogger = getLogger(__name__)\n\tdef get_job_object(job_class_name: str, relative_import=\"jobs\") -> AbstractJob:\n", "    for module in pkgutil.iter_modules([f\"{os.path.dirname(os.path.abspath(__file__))}/jobs\"]):\n\t        job_module = importlib.import_module(f\"{relative_import}.{module.name}\")\n\t        try:\n\t            job_class = getattr(job_module, job_class_name)\n\t            return job_class()\n\t        except AttributeError:\n\t            continue\n\t    raise RuntimeError(\"could not find job class\")\n\tdef init():\n\t    parser = argparse.ArgumentParser()\n", "    parser.add_argument(\"--config\", type=str)\n\t    args, unknown = parser.parse_known_args()\n\t    if len(unknown) > 0:\n\t        logger.info(f\"Unknown arguments passed: {unknown}\")\n\t    cfg = json.loads(args.config, object_hook=lambda d: SimpleNamespace(**d))\n\t    get_job_object(cfg.job_class_name).main(cfg)\n\tif __name__ == \"__main__\":\n\t    init()\n"]}
{"filename": "cli.py", "chunked_list": ["import importlib\n\timport logging\n\timport pkgutil\n\timport os\n\timport click\n\tfrom experiments.abstracts.abstract_experiments import AbstractExperiment\n\tlog = logging.getLogger()\n\t@click.group()\n\tdef cli():\n\t    pass\n", "@cli.command()\n\t@click.option('--experiment_name', '-n', help='Please provide a unique name')\n\t@click.option('--experiment_class', '-c', help='Please provide experiment class name eg. NActionExperiment')\n\tdef run(experiment_name: str, experiment_class: str):\n\t    get_experiment_object(experiment_class).run(experiment_name)\n\t    click.echo(\"Running Experiment on Sage maker Training Jobs. Please wait until jobs are finished\")\n\t    click.echo(\"Check status here: https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs\")\n\t@cli.command()\n\t@click.option('--experiment_name', '-n', help='Please provide the experiment name you set when running the experiment', default=None)\n\t@click.option('--experiment_class', '-c', help='Please provide experiment class name eg. NActionExperiment')\n", "@click.option('--local_path', '-d', help='If the results are stored locally, provide the directory location', default=None)\n\tdef output(experiment_name: str, experiment_class: str, local_path: str):\n\t    click.echo(\"Getting Experiment output on Sage maker Training Jobs.\")\n\t    get_experiment_object(experiment_class).get_output(experiment_name, local_path=local_path)\n\t    click.echo(\"Output has finished successfully\")\n\tdef get_experiment_object(experiment_class_name: str, relative_import=\"experiments\") -> AbstractExperiment:\n\t    for module in pkgutil.iter_modules([f\"{os.path.dirname(os.path.abspath(__file__))}/experiments\"]):\n\t        experiment_module = importlib.import_module(f'{relative_import}.{module.name}')\n\t        try:\n\t            experiment_class = getattr(experiment_module, experiment_class_name)\n", "            return experiment_class()\n\t        except AttributeError:\n\t            continue\n\t    raise RuntimeError(\"could not find experiment class\")\n\tif __name__ == '__main__':\n\t    cli()\n"]}
{"filename": "jobs/__init__.py", "chunked_list": []}
{"filename": "jobs/n_val_data_job.py", "chunked_list": ["from jobs.abstracts.abstract_synthetic_job import AbstractSyntheticJob\n\tclass NValDataJob(AbstractSyntheticJob):\n\t    def main(self, cfg):\n\t        return self.run(cfg, \"n_val_data\", cfg.n_val_data_list)\n"]}
{"filename": "jobs/n_actions_job.py", "chunked_list": ["from jobs.abstracts.abstract_synthetic_job import AbstractSyntheticJob\n\tclass NActionsJob(AbstractSyntheticJob):\n\t    def main(self, cfg):\n\t        return self.run(cfg, \"n_actions\", cfg.n_actions_list)\n"]}
{"filename": "jobs/n_unobs_cat_dim_job.py", "chunked_list": ["from jobs.abstracts.abstract_synthetic_job import AbstractSyntheticJob\n\tclass NUnobsCatDimJob(AbstractSyntheticJob):\n\t    def main(self, cfg):\n\t        return self.run(cfg, \"n_unobserved_cat_dim\", cfg.n_unobserved_cat_dim_list)\n"]}
{"filename": "jobs/real_dataset_job.py", "chunked_list": ["import os\n\tfrom logging import getLogger\n\tfrom os.path import dirname\n\tfrom time import time\n\tfrom pathlib import Path\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom obp.policy import BernoulliTS, Random\n\tfrom torch.utils.data import DataLoader\n", "from .abstracts.abstract_ope_job import AbstractOpeJob\n\tfrom .utils.learn_embed import LearnEmbedLinear, TorchBanditDataset\n\tfrom .utils.ope import run_real_dataset_ope\n\tfrom .utils.dataset import ModifiedOpenBanditDataset\n\tfrom experiments.utils.configs import RealOpeTrialConfig\n\tlogger = getLogger(__name__)\n\tclass RealDatasetJob(AbstractOpeJob):\n\t    def main(self, cfg: RealOpeTrialConfig):\n\t        logger.info(cfg)\n\t        logger.info(f\"The current working directory is {os.getcwd()}\")\n", "        start_time = time()\n\t        if not cfg.s3_path.startswith(\"s3://\"):\n\t            Path(cfg.s3_path).mkdir(parents=True, exist_ok=True)\n\t        # configurations\n\t        sample_size = cfg.sample_size\n\t        random_state = cfg.random_state\n\t        obd_path = dirname(dirname(dirname(cfg.s3_path))) + \"/open_bandit_dataset\"\n\t        OBD_N_ACTIONS = 80\n\t        OBD_LEN_LIST = 3\n\t        # define policies\n", "        policy_ur = Random(\n\t            n_actions=OBD_N_ACTIONS,\n\t            len_list=OBD_LEN_LIST,\n\t            random_state=random_state,\n\t        )\n\t        policy_ts = BernoulliTS(\n\t            n_actions=OBD_N_ACTIONS,\n\t            len_list=OBD_LEN_LIST,\n\t            random_state=random_state,\n\t            is_zozotown_prior=True,\n", "            campaign=\"all\",\n\t        )\n\t        # calc ground-truth policy value (on-policy)\n\t        policy_value = ModifiedOpenBanditDataset.calc_on_policy_policy_value_estimate(\n\t            behavior_policy=\"bts\", campaign=\"all\", data_path=obd_path\n\t        )\n\t        # define a dataset class\n\t        dataset = ModifiedOpenBanditDataset(\n\t            behavior_policy=\"random\",\n\t            data_path=obd_path,\n", "            campaign=\"all\",\n\t        )\n\t        elapsed_prev = 0.0\n\t        squared_error_list = []\n\t        relative_squared_error_list = []\n\t        # iterate over n_seeds bootstrap runs\n\t        for t in np.arange(cfg.n_seeds):\n\t            pi_b = policy_ur.compute_batch_action_dist(n_rounds=sample_size)\n\t            pi_e = policy_ts.compute_batch_action_dist(n_rounds=sample_size)\n\t            pi_e = pi_e.reshape(sample_size, OBD_N_ACTIONS * OBD_LEN_LIST, 1) / OBD_LEN_LIST\n", "            val_bandit_data = dataset.sample_bootstrap_bandit_feedback(\n\t                sample_size=sample_size,\n\t                random_state=t,\n\t            )\n\t            val_bandit_data[\"pi_b\"] = pi_b.reshape(sample_size, OBD_N_ACTIONS * OBD_LEN_LIST, 1) / OBD_LEN_LIST\n\t            # learn the reward model for DM and DR methods - same model as Learned MIPS OneHot\n\t            model = LearnEmbedLinear(\n\t                action_dim=val_bandit_data['action_context'].shape[1],\n\t                action_cat_dim=len(np.unique(val_bandit_data['action_context'])),\n\t                n_actions=val_bandit_data['n_actions'],\n", "                context_dim=val_bandit_data['context'].shape[1],\n\t                config=cfg.embed_model_config\n\t            )\n\t            model_dataset = TorchBanditDataset(\n\t                n_actions=val_bandit_data['n_actions'],\n\t                n_dim_action=len(np.unique(val_bandit_data['action_context'])), \n\t                context=val_bandit_data['context'], \n\t                action=val_bandit_data['action'], \n\t                action_embed=val_bandit_data['action_context'], \n\t                reward=val_bandit_data['reward']\n", "            )\n\t            train_dataloader = DataLoader(model_dataset, batch_size=32, shuffle=True)\n\t            loss_fn = torch.nn.MSELoss()\n\t            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\t            for _ in range(cfg.learned_embed_params.epochs):\n\t                model.train_loop(train_dataloader, loss_fn, optimizer)\n\t            estimated_rewards = np.zeros((val_bandit_data['n_rounds'], val_bandit_data['n_actions'], 1))\n\t            for i in range(val_bandit_data['n_actions']):\n\t                indices = np.where(dataset.action == i)\n\t                if len(indices) != 0:\n", "                    _, action, action_embed, _ = model_dataset.__getitem__(i)\n\t                else:\n\t                    action_embed = np.zeros(val_bandit_data['action_context'].shape[1]).astype(int)\n\t                estimated_rewards[:,i,:] = model(\n\t                    torch.from_numpy(val_bandit_data['context']).float(),\n\t                    action.repeat(val_bandit_data['n_rounds'], 1),\n\t                    action_embed.repeat(val_bandit_data['n_rounds'], 1, 1)\n\t                ).detach().numpy()     \n\t            estimated_rewards_dict = {\n\t                \"DR\": estimated_rewards,\n", "                \"DM\": estimated_rewards,\n\t                \"SwitchDR\": estimated_rewards,\n\t            }\n\t            # estimate policy values and calculate MSE of estimators\n\t            squared_errors, relative_squared_errors = run_real_dataset_ope(\n\t                val_bandit_data=val_bandit_data,\n\t                action_dist_val=pi_e,\n\t                estimated_rewards=estimated_rewards_dict,\n\t                policy_value=policy_value,\n\t                embed_model_config=cfg.embed_model_config,\n", "                learned_embed_params=cfg.learned_embed_params,\n\t                logging_losses_file = f\"{cfg.s3_path}/model_losses/{time()}.parquet\"\n\t            )\n\t            squared_error_list.append(squared_errors)\n\t            relative_squared_error_list.append(relative_squared_errors)\n\t            elapsed = np.round((time() - start_time) / 60, 2)\n\t            diff = np.round(elapsed - elapsed_prev, 2)\n\t            logger.info(f\"t={t}: {elapsed}min (diff {diff}min)\")\n\t            elapsed_prev = elapsed\n\t            # aggregate all results\n", "            result_df = (\n\t                pd.DataFrame(pd.DataFrame(squared_error_list).stack())\n\t                .reset_index(1)\n\t                .rename(columns={\"level_1\": \"est\", 0: \"se\"})\n\t            )\n\t            result_df.reset_index(inplace=True, drop=True)\n\t            result_df.to_csv(f\"{cfg.s3_path}/result_df.csv\")\n\t        rel_result_df = (\n\t            pd.DataFrame(pd.DataFrame(relative_squared_error_list).stack())\n\t            .reset_index(1)\n", "            .rename(columns={\"level_1\": \"est\", 0: \"se\"})\n\t        )\n\t        rel_result_df.reset_index(inplace=True, drop=True)\n\t        rel_result_df.to_csv(f\"{cfg.s3_path}/rel_result_df.csv\")\n"]}
{"filename": "jobs/ope_hpo_job.py", "chunked_list": ["import os\n\tfrom functools import reduce\n\tfrom logging import getLogger\n\tfrom time import time\n\tfrom pathlib import Path\n\timport numpy as np\n\timport pandas as pd\n\tfrom obp.dataset import SyntheticBanditDatasetWithActionEmbeds, linear_reward_function\n\tfrom obp.ope import InverseProbabilityWeighting as IPS\n\tfrom obp.ope import OffPolicyEvaluation\n", "from experiments.utils.configs import HpoTrialConfig\n\tfrom .abstracts.abstract_ope_job import AbstractOpeJob\n\tfrom .utils.policy import gen_eps_greedy\n\tfrom .utils.learn_embed import LearnedEmbedMIPS, LearnEmbedMF\n\tlogger = getLogger(__name__)\n\tESTIMATOR = \"Learned NMF MIPS\"\n\tclass OpeHpoJob(AbstractOpeJob):\n\t    def main(self, cfg: HpoTrialConfig):\n\t        logger.info(cfg)\n\t        logger.info(f\"The current working directory is {os.getcwd()}\")\n", "        start_time = time()\n\t        if not cfg.s3_path.startswith(\"s3://\"):\n\t            Path(cfg.s3_path).mkdir(parents=True, exist_ok=True)\n\t        dataset = SyntheticBanditDatasetWithActionEmbeds(  \n\t            n_actions=cfg.n_actions,\n\t            dim_context=cfg.dim_context,\n\t            beta=cfg.beta,\n\t            reward_type=\"continuous\",\n\t            n_cat_per_dim=cfg.n_cat_per_dim,\n\t            latent_param_mat_dim=cfg.latent_param_mat_dim,\n", "            n_cat_dim=cfg.n_cat_dim,\n\t            n_unobserved_cat_dim=cfg.n_unobserved_cat_dim,\n\t            n_deficient_actions=int(cfg.n_actions * cfg.n_def_actions),\n\t            reward_function=linear_reward_function,\n\t            reward_std=cfg.reward_std,\n\t            random_state=cfg.random_state,\n\t        )\n\t        test_bandit_data = dataset.obtain_batch_bandit_feedback(\n\t            n_rounds=cfg.n_test_data\n\t        )\n", "        action_dist_test = gen_eps_greedy(\n\t            expected_reward=test_bandit_data[\"expected_reward\"],\n\t            is_optimal=cfg.is_optimal,\n\t            eps=cfg.eps,\n\t        )\n\t        policy_value = dataset.calc_ground_truth_policy_value(\n\t            expected_reward=test_bandit_data[\"expected_reward\"],\n\t            action_dist=action_dist_test,\n\t        )\n\t        elapsed_prev = 0.0\n", "        squared_error_list = []\n\t        relative_squared_error_list = []\n\t        estimated_policy_value_list = []\n\t        # iterate over n_seeds bootstrap runs\n\t        for t in np.arange(cfg.n_seeds):\n\t            # generate validation data\n\t            val_bandit_data = dataset.obtain_batch_bandit_feedback(\n\t                n_rounds=cfg.n_val_data,\n\t            )\n\t            # make decisions on validation data\n", "            action_dist_val = gen_eps_greedy(\n\t                expected_reward=val_bandit_data[\"expected_reward\"],\n\t                is_optimal=cfg.is_optimal,\n\t                eps=cfg.eps,\n\t            )\n\t            ope = OffPolicyEvaluation(\n\t                bandit_feedback=val_bandit_data,\n\t                ope_estimators=[\n\t                    IPS(estimator_name=\"IPS\"),\n\t                    LearnedEmbedMIPS(\n", "                        estimator_name=ESTIMATOR, \n\t                        n_actions=val_bandit_data[\"n_actions\"], \n\t                        embed_model=LearnEmbedMF, \n\t                        learned_embed_params=cfg.learned_embed_params,\n\t                        embed_model_config=cfg.embed_model_config,\n\t                    ),\n\t                ],\n\t            )\n\t            estimated_policy_values = ope.estimate_policy_values(\n\t                action_dist=action_dist_val,\n", "                action_embed=val_bandit_data[\"action_embed\"],\n\t                pi_b=val_bandit_data[\"pi_b\"],\n\t            )\n\t            estimated_policy_value_list.append(estimated_policy_values)\n\t            squared_errors = reduce(lambda acc, val: {**acc, val[0]: (val[1] - policy_value) ** 2}, estimated_policy_values.items(), {})\n\t            baseline = squared_errors[\"IPS\"]\n\t            relative_squared_errors = reduce(lambda acc, val: {**acc, val[0]: val[1] / baseline }, squared_errors.items(), {})\n\t            # estimate policy values and calculate MSE of estimators\n\t            squared_error_list.append(squared_errors)\n\t            relative_squared_error_list.append(relative_squared_errors)\n", "            elapsed = np.round((time() - start_time) / 60, 2)\n\t            diff = np.round(elapsed - elapsed_prev, 2)\n\t            logger.info(f\"t={t}: {elapsed}min (diff {diff}min)\")\n\t            elapsed_prev = elapsed\n\t        # aggregate all results\n\t        value_df = (\n\t            pd.DataFrame(pd.DataFrame(estimated_policy_value_list).stack())\n\t            .reset_index(1)\n\t            .rename(columns={\"level_1\": \"est\", 0: \"value\"})\n\t        )\n", "        value_df.reset_index(inplace=True, drop=True)\n\t        value_df[\"se\"] = (value_df.value - policy_value) ** 2\n\t        value_df[\"bias\"] = 0\n\t        value_df[\"variance\"] = 0\n\t        sample_mean = pd.DataFrame(value_df.groupby([\"est\"]).mean().value).reset_index()\n\t        for est_ in sample_mean[\"est\"]:\n\t            estimates = value_df.loc[value_df[\"est\"] == est_, \"value\"].values\n\t            mean_estimates = sample_mean.loc[sample_mean[\"est\"] == est_, \"value\"].values\n\t            mean_estimates = np.ones_like(estimates) * mean_estimates\n\t            value_df.loc[value_df[\"est\"] == est_, \"bias\"] = np.sqrt((\n", "                policy_value - mean_estimates\n\t            ) ** 2)\n\t            value_df.loc[value_df[\"est\"] == est_, \"variance\"] = np.sqrt((\n\t                estimates - mean_estimates\n\t            ) ** 2)\n\t        value_df.set_index(\"est\", inplace=True)\n\t        value_df.to_csv(f\"{cfg.s3_path}/value_df.csv\")\n\t        result_df = (\n\t            pd.DataFrame(pd.DataFrame(squared_error_list).stack())\n\t            .reset_index(1)\n", "            .rename(columns={\"level_1\": \"est\", 0: \"se\"})\n\t        )\n\t        result_df.reset_index(inplace=True, drop=True)\n\t        result_df.to_csv(f\"{cfg.s3_path}/result_df.csv\")\n\t        rel_result_df = (\n\t            pd.DataFrame(pd.DataFrame(relative_squared_error_list).stack())\n\t            .reset_index(1)\n\t            .rename(columns={\"level_1\": \"est\", 0: \"se\"})\n\t        )\n\t        rel_result_df.reset_index(inplace=True, drop=True)\n", "        rel_result_df.to_csv(f\"{cfg.s3_path}/rel_result_df.csv\")\n\t        mse = rel_result_df.groupby('est').apply(lambda x: np.power(np.e, np.log(x).mean()))['se'][ESTIMATOR]\n\t        lcb = rel_result_df.groupby('est').apply(lambda x: np.power(np.e, np.log(x).mean() - np.log(x).std() / np.sqrt(len(x) - 1)))['se'][ESTIMATOR]\n\t        ucb = rel_result_df.groupby('est').apply(lambda x: np.power(np.e, np.log(x).mean() + np.log(x).std() / np.sqrt(len(x) - 1)))['se'][ESTIMATOR]\n\t        logger.info(f'Relative MSE: {mse:.4f}')\n\t        logger.info(f'Relative MSE LCB: {lcb:.4f}')\n\t        logger.info(f'Relative MSE UCB: {ucb:.4f}')\n\t        logger.info(f'Bias: {value_df[\"bias\"][ESTIMATOR].mean():.10f}')\n\t        logger.info(f'Variance: {value_df[\"variance\"][ESTIMATOR].mean():.10f}')\n"]}
{"filename": "jobs/utils/learn_embed.py", "chunked_list": ["from dataclasses import asdict\n\tfrom typing import Optional\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom obp.ope import MarginalizedInverseProbabilityWeighting\n\tfrom scipy import stats\n\tfrom sklearn.decomposition import NMF\n\tfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\tfrom torch import nn\n", "from torch.utils.data import DataLoader, Dataset, random_split\n\tfrom experiments.utils.configs import LearnEmbedConfig, LearnedEmbedParams\n\tdef disable_training(f):\n\t    def wrapped(self, *args, **kwargs):\n\t        self.action_embed_stack.training = False\n\t        output = f(self, *args, **kwargs)\n\t        self.action_embed_stack.training = True\n\t        return output\n\t    return wrapped\n\tclass LearnEmbedNetwork(nn.Module):\n", "    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n\t        super(LearnEmbedNetwork, self).__init__()\n\t    def action_embeddings(self, action=None, action_embed=None):\n\t        \"\"\"Returns learned action embeddings for the given action and/or pre-defined action embedding\"\"\"\n\t        return NotImplementedError\n\t    def train_loop(self, dataloader, loss_fn, optimizer):\n\t        for x, a, e, r in dataloader:\n\t            # Compute prediction and loss\n\t            pred = self(x, a, e)\n\t            loss = loss_fn(pred, r)\n", "            # Backpropagation\n\t            optimizer.zero_grad()\n\t            loss.backward()\n\t            optimizer.step()\n\t    def report_loss(self, dataloader, loss_fn):\n\t        loss = 0\n\t        num_batches = len(dataloader)\n\t        self.eval()\n\t        for x, a, e, r in dataloader:\n\t            pred = self(x, a, e)\n", "            loss += loss_fn(pred, r).item()\n\t        self.train()\n\t        return loss / num_batches\n\tclass LearnEmbedLinear(LearnEmbedNetwork):\n\t    \"\"\"Linear learning objective, where the action network only uses the action identity\"\"\"\n\t    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n\t        super().__init__()\n\t        self.action_embed_stack = nn.Linear(n_actions, context_dim)\n\t        self.output = nn.Linear(context_dim, 1)\n\t        nn.init.ones_(self.output.weight)\n", "        self.output.requires_grad_(False)\n\t    def forward(self, context, action, action_embed):\n\t        x1 = self.action_embed_stack(action)\n\t        out = self.output(torch.mul(x1, context))\n\t        return out\n\t    @disable_training\n\t    def action_embeddings(self, action=None, action_embed=None):\n\t        return self.action_embed_stack(action)\n\tclass LearnEmbedLinearB(LearnEmbedNetwork):\n\t    \"\"\"Linear learning objective, where the action network uses the action embeddings\"\"\"\n", "    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n\t        super().__init__()\n\t        self.action_embed_stack = nn.Sequential(\n\t            nn.Flatten(),\n\t            nn.Linear(action_cat_dim * action_dim, context_dim)\n\t        )\n\t        self.output = nn.Linear(context_dim, 1)\n\t        nn.init.ones_(self.output.weight)\n\t        self.output.requires_grad_(False)\n\t    def forward(self, context, action, action_embed):\n", "        x1 = self.action_embed_stack(action_embed)\n\t        out = self.output(torch.mul(x1, context))\n\t        return out\n\t    @disable_training\n\t    def action_embeddings(self, action=None, action_embed=None):\n\t        return self.action_embed_stack(action_embed)\n\tclass LearnEmbedLinearC(LearnEmbedNetwork):\n\t    \"\"\"Linear learning objective, where the action network only uses both the action identity and a pre-defined embedding\"\"\"\n\t    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n\t        super().__init__()\n", "        self.action_embed_stack = nn.Linear(action_cat_dim * action_dim + n_actions, context_dim)\n\t        self.output = nn.Linear(context_dim, 1)\n\t        nn.init.ones_(self.output.weight)\n\t        self.output.requires_grad_(False)\n\t    def forward(self, context, action, action_embed):\n\t        x1 = self.action_embed_stack(torch.cat([nn.Flatten()(action_embed), action], dim=1))\n\t        out = self.output(torch.mul(x1, context))\n\t        return out\n\t    @disable_training\n\t    def action_embeddings(self, action=None, action_embed=None):\n", "        return self.action_embed_stack(torch.cat([nn.Flatten()(action_embed), action], dim=1))\n\tclass LearnEmbedMF(LearnEmbedNetwork):\n\t    \"\"\"Linear learning objective with low rank MF, where the action network only uses the action identity\"\"\"\n\t    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n\t        super().__init__()\n\t        self.action_embed_stack = nn.Linear(n_actions, config.learned_embed_dim)\n\t        self.context_stack = nn.Linear(context_dim, config.learned_embed_dim)\n\t        self.output = nn.Linear(config.learned_embed_dim, 1)\n\t        nn.init.ones_(self.output.weight)\n\t        self.output.requires_grad_(False)\n", "    def forward(self, context, action, action_embed):\n\t        x1 = self.action_embed_stack(action)\n\t        x2 = self.context_stack(context)\n\t        out = self.output(torch.mul(x1, x2))\n\t        return out\n\t    @disable_training\n\t    def action_embeddings(self, action=None, action_embed=None):\n\t        return self.action_embed_stack(action)\n\tclass TorchBanditDataset(Dataset):\n\t    def __init__(self, n_actions, n_dim_action, context, action, action_embed, reward):\n", "        self.context = torch.from_numpy(context).float()\n\t        self.action = torch.nn.functional.one_hot(torch.from_numpy(action), num_classes=n_actions).float()\n\t        self.action_embed = torch.nn.functional.one_hot(torch.from_numpy(action_embed), num_classes=n_dim_action).float()\n\t        self.reward = torch.unsqueeze(torch.from_numpy(reward), 1).float()\n\t    def __len__(self):\n\t        return len(self.reward)\n\t    def __getitem__(self, idx):\n\t        return self.context[idx], self.action[idx], self.action_embed[idx], self.reward[idx]\n\tclass LearnedEmbedMIPS(MarginalizedInverseProbabilityWeighting):\n\t    def __init__(\n", "        self,\n\t        embed_model: nn.Module = None,\n\t        learned_embed_params: LearnedEmbedParams = LearnedEmbedParams(),\n\t        embed_model_config: LearnEmbedConfig = LearnEmbedConfig(),\n\t        logging_losses_file=None,\n\t        **kwargs\n\t    ):\n\t        self.embed_model = embed_model\n\t        self.learned_embed_params = learned_embed_params\n\t        self.embed_model_config = embed_model_config\n", "        self.logging_losses_file = logging_losses_file\n\t        super().__init__(**kwargs)\n\t    def _estimate_round_rewards(\n\t        self,\n\t        context: np.ndarray,\n\t        reward: np.ndarray,\n\t        action: np.ndarray,\n\t        action_embed: np.ndarray,\n\t        pi_b: np.ndarray,\n\t        action_dist: np.ndarray,\n", "        position: Optional[np.ndarray] = None,\n\t        p_e_a: Optional[np.ndarray] = None,\n\t        with_dev: bool = False,\n\t        **kwargs,\n\t    ) -> np.ndarray:\n\t        \"\"\"Estimate round-wise (or sample-wise) rewards.\n\t        Parameters\n\t        ----------\n\t        reward: array-like, shape (n_rounds,)\n\t            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n", "        action: array-like, shape (n_rounds,)\n\t            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\t        action_embed: array-like, shape (n_rounds, dim_action_embed)\n\t            Context vectors characterizing actions or action embeddings such as item category information.\n\t            This is used to estimate the marginal importance weights.\n\t        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n\t            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\t        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n\t            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\t        position: array-like, shape (n_rounds,), default=None\n", "            Indices to differentiate positions in a recommendation interface where the actions are presented.\n\t            If None, the effect of position on the reward will be ignored.\n\t            (If only a single action is chosen for each data, you can just ignore this argument.)\n\t        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n\t            Conditional distribution of action embeddings given each action.\n\t            This distribution is available only when we use synthetic bandit data, i.e.,\n\t            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n\t            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n\t            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n\t            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n", "        with_dev: bool, default=False.\n\t            Whether to output a deviation bound with the estimated sample-wise rewards.\n\t        Returns\n\t        ----------\n\t        estimated_rewards: array-like, shape (n_rounds,)\n\t            Estimated rewards for each observation.\n\t        \"\"\"\n\t        n = reward.shape[0]\n\t        learned_embed = self.learn_action_embed(\n\t            context=context,\n", "            action=action,\n\t            action_embed=action_embed,\n\t            reward=reward,\n\t            **self.learned_embed_params.__dict__\n\t        )\n\t        w_x_e = self._estimate_w_x_e(\n\t            action=action,\n\t            action_embed=learned_embed,\n\t            pi_e=action_dist[np.arange(n), :, position],\n\t            pi_b=pi_b[np.arange(n), :, position],\n", "        )\n\t        self.max_w_x_e = w_x_e.max()\n\t        if with_dev:\n\t            r_hat = reward * w_x_e\n\t            cnf = np.sqrt(np.var(r_hat) / (n - 1))\n\t            cnf *= stats.t.ppf(1.0 - (self.delta / 2), n - 1)\n\t            return r_hat.mean(), cnf\n\t        return reward * w_x_e\n\t    def _estimate_w_x_e(\n\t        self,\n", "        action: np.ndarray,\n\t        action_embed: np.ndarray,\n\t        pi_b: np.ndarray,\n\t        pi_e: np.ndarray,\n\t    ) -> np.ndarray:\n\t        \"\"\"Estimate the marginal importance weights.\"\"\"\n\t        n = action.shape[0]\n\t        w_x_a = pi_e / pi_b\n\t        w_x_a = np.where(w_x_a < np.inf, w_x_a, 0)\n\t        pi_a_e = np.zeros((n, self.n_actions))\n", "        self.pi_a_x_e_estimator.fit(action_embed, action)\n\t        pi_a_e[:, np.unique(action)] = self.pi_a_x_e_estimator.predict_proba(action_embed)\n\t        w_x_e = (w_x_a * pi_a_e).sum(1)\n\t        return w_x_e\n\t    def learn_action_embed(\n\t        self,\n\t        context: np.ndarray,\n\t        action: np.ndarray,\n\t        action_embed: np.ndarray,\n\t        reward: np.ndarray,\n", "        lr=1e-2,\n\t        epochs=250,\n\t        batch_size=32,\n\t        train_test_ratio=0.9,\n\t    ) -> np.ndarray:\n\t        \"\"\"Learn action embeddings\"\"\"\n\t        n_dim_action = len(np.unique(action_embed))\n\t        dataset = TorchBanditDataset(self.n_actions, n_dim_action, context, action, action_embed, reward)\n\t        if train_test_ratio >= 1:\n\t            train_data = dataset\n", "            train_positives = dataset.reward.sum().item()\n\t            train_length = len(dataset)\n\t        else:\n\t            train_data, test_data = random_split(dataset, [round(train_test_ratio * len(dataset)), round((1 - train_test_ratio) * len(dataset))])\n\t            test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\t            test_positives = dataset.reward[test_data.indices].sum().item()\n\t            test_length = len(dataset.reward[test_data.indices])\n\t            train_length = len(dataset.reward[train_data.indices])\n\t            train_positives = dataset.reward[train_data.indices].sum().item()\n\t        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n", "        model = self.embed_model(n_actions=self.n_actions, action_dim=action_embed.shape[1], action_cat_dim=n_dim_action, context_dim=context.shape[1], config=self.embed_model_config)\n\t        loss_fn = nn.MSELoss()\n\t        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\t        logs = pd.DataFrame()\n\t        for e in range(epochs):\n\t            model.train_loop(train_dataloader, loss_fn, optimizer)\n\t            if self.logging_losses_file and train_test_ratio < 1:\n\t                train_mse = model.report_loss(train_dataloader, loss_fn)\n\t                test_mse = model.report_loss(test_dataloader, loss_fn)\n\t                logs = logs.append(pd.Series(\n", "                    [e, train_mse, test_mse, self.estimator_name, asdict(self.learned_embed_params), asdict(self.embed_model_config), train_positives, train_length, test_positives, test_length],\n\t                    index=[\"epoch\", \"train_mse\", \"test_mse\", \"model\", \"learned_embed_params\", \"embed_model_config\", \"train_positives\", \"train_length\", \"test_positives\", \"test_length\"]\n\t                ), ignore_index=True\n\t                )\n\t        if self.logging_losses_file and train_test_ratio < 1:\n\t            logs.to_parquet(self.logging_losses_file, index=False)\n\t        return model.action_embeddings(dataset.action, dataset.action_embed).detach().numpy()\n"]}
{"filename": "jobs/utils/policy.py", "chunked_list": ["import numpy as np\n\tdef gen_eps_greedy(\n\t        expected_reward: np.ndarray,\n\t        is_optimal: bool = True,\n\t        eps: float = 0.0,\n\t) -> np.ndarray:\n\t    \"Generate an evaluation policy via the epsilon-greedy rule.\"\n\t    base_pol = np.zeros_like(expected_reward)\n\t    if is_optimal:\n\t        a = np.argmax(expected_reward, axis=1)\n", "    else:\n\t        a = np.argmin(expected_reward, axis=1)\n\t    base_pol[\n\t        np.arange(expected_reward.shape[0]),\n\t        a,\n\t    ] = 1\n\t    pol = (1.0 - eps) * base_pol\n\t    pol += eps / expected_reward.shape[1]\n\t    return pol[:, :, np.newaxis]\n"]}
{"filename": "jobs/utils/dataset.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom typing import Optional\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom obp.dataset import OpenBanditDataset, SyntheticBanditDatasetWithActionEmbeds\n\tfrom obp.dataset.reward_type import RewardType\n\tfrom obp.types import BanditFeedback\n\tfrom obp.utils import sample_action_fast, softmax\n\tfrom scipy.stats import rankdata\n", "from sklearn.preprocessing import LabelEncoder\n\tfrom sklearn.utils import check_random_state, check_scalar\n\tfrom torch import nn\n\t@dataclass\n\tclass ModifiedOpenBanditDataset(OpenBanditDataset):\n\t    \"\"\"Flattening the list structure of OBD according to item-position click model.\n\t    As OBD has 80 unique actions and 3 different positions in its recommendation interface,\n\t    the resulting action space has the cardinality of 80 * 3 = 240.\"\"\"\n\t    @property\n\t    def n_actions(self) -> int:\n", "        \"\"\"Number of actions.\"\"\"\n\t        return int(self.action.max() + 1)\n\t    def __post_init__(self) -> None:\n\t        \"\"\"Initialize Open Bandit Dataset Class.\"\"\"\n\t        if self.behavior_policy not in [\n\t            \"bts\",\n\t            \"random\",\n\t        ]:\n\t            raise ValueError(\n\t                f\"`behavior_policy` must be either of 'bts' or 'random', but {self.behavior_policy} is given\"\n", "            )\n\t        if self.campaign not in [\n\t            \"all\",\n\t            \"men\",\n\t            \"women\",\n\t        ]:\n\t            raise ValueError(\n\t                f\"`campaign` must be one of 'all', 'men', or 'women', but {self.campaign} is given\"\n\t            )\n\t        self.data_path = f\"{self.data_path}/{self.behavior_policy}/{self.campaign}\"\n", "        self.raw_data_file = f\"{self.campaign}.csv\"\n\t        self.load_raw_data()\n\t        self.pre_process()\n\t    def load_raw_data(self) -> None:\n\t        \"\"\"Load raw open bandit dataset.\"\"\"\n\t        self.data = pd.read_csv(f\"{self.data_path}/{self.raw_data_file}\", index_col=0)\n\t        self.item_context = pd.read_csv(\n\t            f\"{self.data_path}/item_context.csv\", index_col=0\n\t        )\n\t        self.data.sort_values(\"timestamp\", inplace=True)\n", "        self.action = self.data[\"item_id\"].values\n\t        self.position = (rankdata(self.data[\"position\"].values, \"dense\") - 1).astype(\n\t            int\n\t        )\n\t        self.reward = self.data[\"click\"].values\n\t        self.pscore = self.data[\"propensity_score\"].values\n\t    def pre_process(self) -> None:\n\t        \"\"\"Preprocess raw open bandit dataset.\"\"\"\n\t        user_cols = self.data.columns.str.contains(\"user_feature\")\n\t        self.context = pd.get_dummies(\n", "            self.data.loc[:, user_cols], drop_first=True\n\t        ).values\n\t        pos = pd.DataFrame(self.position)\n\t        self.action_context = (\n\t            self.item_context.drop(columns=[\"item_id\", \"item_feature_0\"], axis=1)\n\t            .apply(LabelEncoder().fit_transform)\n\t            .values\n\t        )\n\t        self.action_context = self.action_context[self.action]\n\t        self.action_context = np.c_[self.action_context, pos]\n", "        self.action = self.position * self.n_actions + self.action\n\t        self.position = np.zeros_like(self.position)\n\t        self.pscore /= 3\n\t    def sample_bootstrap_bandit_feedback(\n\t        self,\n\t        sample_size: Optional[int] = None,\n\t        test_size: float = 0.3,\n\t        is_timeseries_split: bool = False,\n\t        random_state: Optional[int] = None,\n\t    ) -> BanditFeedback:\n", "        if is_timeseries_split:\n\t            bandit_feedback = self.obtain_batch_bandit_feedback(\n\t                test_size=test_size, is_timeseries_split=is_timeseries_split\n\t            )[0]\n\t        else:\n\t            bandit_feedback = self.obtain_batch_bandit_feedback(\n\t                test_size=test_size, is_timeseries_split=is_timeseries_split\n\t            )\n\t        n_rounds = bandit_feedback[\"n_rounds\"]\n\t        if sample_size is None:\n", "            sample_size = bandit_feedback[\"n_rounds\"]\n\t        else:\n\t            check_scalar(\n\t                sample_size,\n\t                name=\"sample_size\",\n\t                target_type=(int),\n\t                min_val=0,\n\t                max_val=n_rounds,\n\t            )\n\t        random_ = check_random_state(random_state)\n", "        bootstrap_idx = random_.choice(\n\t            np.arange(n_rounds), size=sample_size, replace=True\n\t        )\n\t        for key_ in [\n\t            \"action\",\n\t            \"position\",\n\t            \"reward\",\n\t            \"pscore\",\n\t            \"context\",\n\t            \"action_context\",\n", "        ]:\n\t            bandit_feedback[key_] = bandit_feedback[key_][bootstrap_idx]\n\t        bandit_feedback[\"n_rounds\"] = sample_size\n\t        return bandit_feedback\n\tclass DataGeneratingNetwork(nn.Module):\n\t    def __init__(self, n_features=3, feature_dim=5, context_dim=10, hidden_layer_size=50):\n\t        super().__init__()\n\t        self.layers = nn.Sequential(\n\t            nn.Linear(n_features * feature_dim + context_dim, hidden_layer_size),\n\t            nn.ReLU(),\n", "            nn.Linear(hidden_layer_size, hidden_layer_size),\n\t            nn.ReLU(),\n\t            nn.Linear(hidden_layer_size, 1)\n\t        )\n\t    def forward(self, context, action_embed):\n\t        x = torch.cat([context, nn.Flatten()(action_embed)], dim=1)\n\t        return self.layers(x)\n\t    def generate_reward(self, context, action_embed, batch_size=32):\n\t        rewards = np.zeros(context.shape[0])\n\t        for i in range(0, context.shape[0], batch_size):\n", "            context_batch = torch.from_numpy(context[i:i + batch_size]).float()\n\t            action_embed_batch = torch.from_numpy(np.tile(action_embed, (context_batch.shape[0], 1, 1))).float()\n\t            rewards_batch = self.forward(context_batch, action_embed_batch)\n\t            rewards[i:i + batch_size] = rewards_batch.detach().numpy().flatten()\n\t        return rewards\n\t@ dataclass\n\tclass NonLinearSyntheticBanditDatasetWithActionEmbeds(SyntheticBanditDatasetWithActionEmbeds):\n\t    def __post_init__(self) -> None:\n\t        self.random_ = check_random_state(self.random_state)\n\t        self._define_action_embed()\n", "        self.model = DataGeneratingNetwork(n_features=self.n_cat_dim, feature_dim=self.latent_param_mat_dim, context_dim=self.dim_context)\n\t    def obtain_batch_bandit_feedback(self, n_rounds: int) -> BanditFeedback:\n\t        \"\"\"Obtain batch logged bandit data.\n\t        Parameters\n\t        ----------\n\t        n_rounds: int\n\t            Data size of the synthetic logged bandit data.\n\t        Returns\n\t        ---------\n\t        bandit_feedback: BanditFeedback\n", "            Synthesized logged bandit data with action category information.\n\t        \"\"\"\n\t        check_scalar(n_rounds, \"n_rounds\", int, min_val=1)\n\t        contexts = self.random_.normal(size=(n_rounds, self.dim_context))\n\t        # calc expected rewards given context and action (n_data, n_actions)\n\t        q_x_e = np.zeros((n_rounds, *([self.n_cat_per_dim] * self.n_cat_dim)))\n\t        q_x_a = np.zeros((n_rounds, self.n_actions))\n\t        for index in np.ndindex(q_x_e.shape[1:]):\n\t            embed_param = np.zeros(self.latent_cat_param.shape[::2])\n\t            p_e_a = np.ones(self.n_actions)\n", "            for feature, category in list(zip(range(self.n_cat_dim), index)):\n\t                embed_param[feature] = self.latent_cat_param[feature, category]\n\t                p_e_a *= self.p_e_a[(Ellipsis, category, feature)]\n\t            q_x_e[(Ellipsis, *index)] = self.model.generate_reward(\n\t                context=contexts,\n\t                action_embed=embed_param,\n\t            )            \n\t            # q_x_e[(Ellipsis, *index)] = np.random.randn(n_rounds)\n\t            q_x_a += np.outer(q_x_e[(Ellipsis, *index)], p_e_a)\n\t        # sample actions for each round based on the behavior policy\n", "        if self.behavior_policy_function is None:\n\t            pi_b_logits = q_x_a\n\t        else:\n\t            pi_b_logits = self.behavior_policy_function(\n\t                context=contexts,\n\t                action_context=self.action_context,\n\t                random_state=self.random_state,\n\t            )\n\t        if self.n_deficient_actions > 0:\n\t            pi_b = np.zeros_like(q_x_a)\n", "            n_supported_actions = self.n_actions - self.n_deficient_actions\n\t            supported_actions = np.argsort(\n\t                self.random_.gumbel(size=(n_rounds, self.n_actions)), axis=1\n\t            )[:, ::-1][:, :n_supported_actions]\n\t            supported_actions_idx = (\n\t                np.tile(np.arange(n_rounds), (n_supported_actions, 1)).T,\n\t                supported_actions,\n\t            )\n\t            pi_b[supported_actions_idx] = softmax(\n\t                self.beta * pi_b_logits[supported_actions_idx]\n", "            )\n\t        else:\n\t            pi_b = softmax(self.beta * pi_b_logits)\n\t        actions = sample_action_fast(pi_b, random_state=self.random_state)\n\t        # sample action embeddings based on sampled actions\n\t        action_embed = np.zeros((n_rounds, self.n_cat_dim), dtype=int)\n\t        for d in np.arange(self.n_cat_dim):\n\t            action_embed[:, d] = sample_action_fast(\n\t                self.p_e_a[actions, :, d],\n\t                random_state=d,\n", "            )\n\t        # sample rewards given the context and action embeddings\n\t        expected_rewards_factual = q_x_e[(np.arange(n_rounds), *action_embed.T)]\n\t        if RewardType(self.reward_type) == RewardType.BINARY:\n\t            rewards = self.random_.binomial(n=1, p=expected_rewards_factual)\n\t        elif RewardType(self.reward_type) == RewardType.CONTINUOUS:\n\t            rewards = self.random_.normal(\n\t                loc=expected_rewards_factual, scale=self.reward_std, size=n_rounds\n\t            )\n\t        return dict(\n", "            n_rounds=n_rounds,\n\t            n_actions=self.n_actions,\n\t            action_context=self.action_context_reg[\n\t                :, self.n_unobserved_cat_dim :\n\t            ].copy(),  # action context used for training a reg model\n\t            action_embed=action_embed[\n\t                :, self.n_unobserved_cat_dim :\n\t            ].copy(),  # action embeddings used for OPE with MIPW\n\t            context=contexts,\n\t            action=actions,\n", "            position=None,  # position effect is not considered in synthetic data\n\t            reward=rewards,\n\t            expected_reward=q_x_a,\n\t            q_x_e=q_x_e[:, :, self.n_unobserved_cat_dim :].copy(),\n\t            p_e_a=self.p_e_a[\n\t                :, :, self.n_unobserved_cat_dim :\n\t            ].copy(),  # true probability distribution of the action embeddings\n\t            pi_b=pi_b[:, :, np.newaxis].copy(),\n\t            pscore=pi_b[np.arange(n_rounds), actions].copy(),\n\t        )\n"]}
{"filename": "jobs/utils/__init__.py", "chunked_list": []}
{"filename": "jobs/utils/ope.py", "chunked_list": ["from typing import Dict, Optional\n\timport numpy as np\n\tfrom obp.ope import DirectMethod as DM\n\tfrom obp.ope import DoublyRobust as DR\n\tfrom obp.ope import InverseProbabilityWeighting as IPS\n\tfrom obp.ope import MarginalizedInverseProbabilityWeighting as MIPS\n\tfrom obp.ope import OffPolicyEvaluation\n\tfrom obp.ope import SelfNormalizedInverseProbabilityWeighting as SNIPS\n\tfrom obp.ope import SwitchDoublyRobustTuning as SwitchDR\n\tfrom .learn_embed import LearnedEmbedMIPS, LearnEmbedLinear, LearnEmbedLinearB, LearnEmbedLinearC\n", "def run_ope(\n\t    val_bandit_data: Dict,\n\t    action_dist_val: np.ndarray,\n\t    estimated_rewards: Optional[np.ndarray] = None,\n\t    **kwargs\n\t) -> Dict[str, float]:\n\t    n_actions = val_bandit_data[\"n_actions\"]\n\t    ope_estimators = [\n\t        IPS(estimator_name=\"IPS\"),\n\t        DR(estimator_name=\"DR\"),\n", "        DM(estimator_name=\"DM\"),\n\t        MIPS(n_actions=n_actions, estimator_name=\"MIPS\"),\n\t        MIPS(n_actions=n_actions, estimator_name=\"MIPS (true)\"),\n\t        LearnedEmbedMIPS(estimator_name=\"Learned MIPS OneHot\", n_actions=n_actions, embed_model=LearnEmbedLinear, **kwargs),\n\t        LearnedEmbedMIPS(estimator_name=\"Learned MIPS FineTune\", n_actions=n_actions, embed_model=LearnEmbedLinearB, **kwargs),\n\t        LearnedEmbedMIPS(estimator_name=\"Learned MIPS Combined\", n_actions=n_actions, embed_model=LearnEmbedLinearC, **kwargs),\n\t    ]\n\t    ope = OffPolicyEvaluation(\n\t        bandit_feedback=val_bandit_data,\n\t        ope_estimators=ope_estimators,\n", "    )\n\t    estimated_policy_values = ope.estimate_policy_values(\n\t        action_dist=action_dist_val,\n\t        estimated_rewards_by_reg_model=estimated_rewards,\n\t        action_embed=val_bandit_data[\"action_embed\"],\n\t        pi_b=val_bandit_data[\"pi_b\"],\n\t        p_e_a={\"MIPS (true)\": val_bandit_data[\"p_e_a\"]},\n\t    )\n\t    return estimated_policy_values\n\tdef run_real_dataset_ope(\n", "    val_bandit_data: Dict,\n\t    action_dist_val: np.ndarray,\n\t    estimated_rewards: np.ndarray,\n\t    policy_value: float,\n\t    **kwargs\n\t) -> np.ndarray:\n\t    n_actions = val_bandit_data[\"n_actions\"]\n\t    lambdas = [10, 50, 100, 500, 1e3, 5e3, 1e4, 5e4, 1e5, 5e5, np.inf]\n\t    ope = OffPolicyEvaluation(\n\t        bandit_feedback=val_bandit_data,\n", "        ope_estimators=[\n\t            IPS(estimator_name=\"IPS\"),\n\t            DR(estimator_name=\"DR\"),\n\t            DM(estimator_name=\"DM\"),\n\t            SNIPS(estimator_name=\"SNIPS\"),\n\t            SwitchDR(lambdas=lambdas, tuning_method=\"slope\", estimator_name=\"SwitchDR\"),\n\t            MIPS(estimator_name=\"MIPS\", n_actions=n_actions),\n\t            MIPS(estimator_name=\"MIPS (w/ SLOPE)\", n_actions=n_actions, embedding_selection_method=\"exact\"),\n\t            LearnedEmbedMIPS(estimator_name=\"Learned MIPS OneHot\", n_actions=n_actions, embed_model=LearnEmbedLinear, **kwargs),\n\t            LearnedEmbedMIPS(estimator_name=\"Learned MIPS FineTune\", n_actions=n_actions, embed_model=LearnEmbedLinearB, **kwargs),\n", "            LearnedEmbedMIPS(estimator_name=\"Learned MIPS Combined\", n_actions=n_actions, embed_model=LearnEmbedLinearC, **kwargs),\n\t        ],\n\t    )\n\t    squared_errors = ope.evaluate_performance_of_estimators(\n\t        ground_truth_policy_value=policy_value,\n\t        action_dist=action_dist_val,\n\t        estimated_rewards_by_reg_model=estimated_rewards,\n\t        action_embed=val_bandit_data[\"action_context\"],\n\t        pi_b=val_bandit_data[\"pi_b\"],\n\t        metric=\"se\",\n", "    )\n\t    relative_squared_errors = {}\n\t    baseline = squared_errors[\"MIPS (w/ SLOPE)\"]\n\t    for key, value in squared_errors.items():\n\t        relative_squared_errors[key] = value / baseline\n\t    return squared_errors, relative_squared_errors\n"]}
{"filename": "jobs/abstracts/abstract_job.py", "chunked_list": ["import abc\n\timport argparse\n\timport dataclasses\n\tfrom abc import ABC, ABCMeta\n\tfrom logging import getLogger\n\tfrom types import SimpleNamespace\n\tfrom experiments.utils.configs import LearnedEmbedParams, LearnEmbedConfig, SyntheticOpeTrialConfig\n\tlogger = getLogger(__name__)\n\tclass HandleOpeArgs(ABCMeta):\n\t    def __init__(cls, name, bases, clsdict):\n", "        if 'main' in clsdict:\n\t            def ope_args_main(self, cfg: SyntheticOpeTrialConfig):\n\t                parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)\n\t                parser.add_argument(\"--batch-size\", type=int)\n\t                parser.add_argument(\"--epochs\", type=int)\n\t                parser.add_argument(\"--lr\", type=float)\n\t                # See LearnEmbedConfig for the description of the parameters\n\t                parser.add_argument(\"--learned-embed-dim\", type=int)\n\t                args, unknown = parser.parse_known_args()\n\t                logger.info(f\"Uknown args: {unknown}\")\n", "                if 'embed_model_config' not in cfg.__dict__:\n\t                    cfg.embed_model_config = LearnEmbedConfig()\n\t                else:\n\t                    cfg.embed_model_config = LearnEmbedConfig(**{\n\t                        **cfg.embed_model_config.__dict__,\n\t                        **dict([(key, value) for key, value in vars(args).items() if key in vars(LearnEmbedConfig()).keys()])\n\t                    })\n\t                if 'learned_embed_params' not in cfg.__dict__:\n\t                    cfg.learned_embed_params = LearnedEmbedParams()\n\t                else:\n", "                    cfg.learned_embed_params = LearnedEmbedParams(**{\n\t                        **cfg.learned_embed_params.__dict__,\n\t                        **dict([(key, value) for key, value in vars(args).items() if key in vars(LearnedEmbedParams()).keys()])\n\t                    })\n\t                cfg = SimpleNamespace(**{**SyntheticOpeTrialConfig(name=None).__dict__, **cfg.__dict__})\n\t                clsdict['main'](self, cfg)\n\t            setattr(cls, 'main', ope_args_main)\n\t@dataclasses.dataclass\n\tclass AbstractJob(ABC, metaclass=HandleOpeArgs):\n\t    @abc.abstractmethod\n", "    def main(self, cfg):\n\t        \"\"\"\n\t        This the main compute function for the training job.\n\t        @param cfg: config object that contains all parameters passed to the job from the experiment\n\t        \"\"\"\n\t        pass\n"]}
{"filename": "jobs/abstracts/__init__.py", "chunked_list": []}
{"filename": "jobs/abstracts/abstract_ope_job.py", "chunked_list": ["import abc\n\timport dataclasses\n\tfrom logging import getLogger\n\tfrom experiments.utils.configs import OpeTrialConfig\n\tfrom .abstract_job import AbstractJob\n\t@dataclasses.dataclass\n\tclass AbstractOpeJob(AbstractJob):\n\t    @abc.abstractmethod\n\t    def main(self, cfg: OpeTrialConfig):\n\t        \"\"\"\n", "        This the main compute function for the training job.\n\t        @param cfg: config object that contains all parameters passed to the job from the experiment\n\t        \"\"\"\n\t        pass\n"]}
{"filename": "jobs/abstracts/abstract_synthetic_job.py", "chunked_list": ["import abc\n\timport warnings\n\tfrom logging import getLogger\n\tfrom pathlib import Path\n\tfrom time import time\n\tfrom types import SimpleNamespace\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom obp.dataset import SyntheticBanditDatasetWithActionEmbeds, linear_reward_function\n", "from pandas import DataFrame\n\tfrom sklearn.exceptions import ConvergenceWarning\n\tfrom torch.utils.data import DataLoader\n\tfrom experiments.utils.configs import SyntheticOpeTrialConfig\n\tfrom ..utils.ope import run_ope\n\tfrom ..utils.learn_embed import LearnEmbedLinear, TorchBanditDataset\n\tfrom ..utils.policy import gen_eps_greedy\n\tfrom ..utils.dataset import NonLinearSyntheticBanditDatasetWithActionEmbeds\n\tfrom .abstract_ope_job import AbstractOpeJob\n\twarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n", "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\twarnings.filterwarnings(\"ignore\", category=UserWarning)\n\tlogger = getLogger(__name__)\n\tclass AbstractSyntheticJob(AbstractOpeJob):\n\t    @abc.abstractmethod\n\t    def main(self, cfg: SyntheticOpeTrialConfig):\n\t        pass\n\t    def run(self, cfg, hyperparam_name, hyperparam_list):\n\t        logger.info(f\"The current working directory is {Path().cwd()}\")\n\t        if not cfg.s3_path.startswith(\"s3://\"):\n", "            Path(cfg.s3_path).mkdir(parents=True, exist_ok=True)\n\t        start_time = time()\n\t        # log path\n\t        random_state = cfg.random_state\n\t        elapsed_prev = 0.0\n\t        result_df_list = []\n\t        for hyperparam in hyperparam_list:\n\t            setattr(cfg, hyperparam_name, hyperparam)\n\t            estimated_policy_value_list = []\n\t            # define a dataset class\n", "            dataset = SyntheticBanditDatasetWithActionEmbeds(\n\t            # dataset = NonLinearSyntheticBanditDatasetWithActionEmbeds(\n\t                n_actions=cfg.n_actions,\n\t                dim_context=cfg.dim_context,\n\t                beta=cfg.beta,\n\t                reward_type=\"continuous\",\n\t                n_cat_per_dim=cfg.n_cat_per_dim,\n\t                latent_param_mat_dim=cfg.latent_param_mat_dim,\n\t                n_cat_dim=cfg.n_cat_dim,\n\t                n_unobserved_cat_dim=cfg.n_unobserved_cat_dim,\n", "                n_deficient_actions=int(cfg.n_actions * cfg.n_def_actions),\n\t                reward_function=linear_reward_function,\n\t                reward_std=cfg.reward_std,\n\t                random_state=random_state,\n\t            )\n\t            # test bandit data is used to approximate the ground-truth policy value\n\t            test_bandit_data = dataset.obtain_batch_bandit_feedback(\n\t                n_rounds=cfg.n_test_data\n\t            )\n\t            action_dist_test = gen_eps_greedy(\n", "                expected_reward=test_bandit_data[\"expected_reward\"],\n\t                is_optimal=cfg.is_optimal,\n\t                eps=cfg.eps,\n\t            )\n\t            policy_value = dataset.calc_ground_truth_policy_value(\n\t                expected_reward=test_bandit_data[\"expected_reward\"],\n\t                action_dist=action_dist_test,\n\t            )\n\t            for t in range(cfg.n_seeds):\n\t                # generate validation data\n", "                val_bandit_data = dataset.obtain_batch_bandit_feedback(\n\t                    n_rounds=cfg.n_val_data,\n\t                )\n\t                # make decisions on validation data\n\t                action_dist_val = gen_eps_greedy(\n\t                    expected_reward=val_bandit_data[\"expected_reward\"],\n\t                    is_optimal=cfg.is_optimal,\n\t                    eps=cfg.eps,\n\t                )\n\t                # Direct Method\n", "                model = LearnEmbedLinear(\n\t                    action_dim=val_bandit_data['action_context'].shape[1],\n\t                    action_cat_dim=len(np.unique(val_bandit_data['action_embed'])),\n\t                    n_actions=val_bandit_data['n_actions'],\n\t                    context_dim=val_bandit_data['context'].shape[1],\n\t                    config=cfg.embed_model_config\n\t                )\n\t                model_dataset = TorchBanditDataset(\n\t                    n_actions=val_bandit_data['n_actions'],\n\t                    n_dim_action=len(np.unique(val_bandit_data['action_embed'])),\n", "                    context=val_bandit_data['context'],\n\t                    action=val_bandit_data['action'],\n\t                    action_embed=val_bandit_data['action_embed'],\n\t                    reward=val_bandit_data['reward']\n\t                )\n\t                train_dataloader = DataLoader(model_dataset, batch_size=32, shuffle=True)\n\t                loss_fn = torch.nn.MSELoss()\n\t                optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\t                for _ in range(cfg.learned_embed_params.epochs):\n\t                    model.train_loop(train_dataloader, loss_fn, optimizer)\n", "                estimated_rewards = np.zeros((val_bandit_data['n_rounds'], val_bandit_data['n_actions'], 1))\n\t                for i in range(val_bandit_data['n_actions']):\n\t                    _, action, action_embed, _ = model_dataset.__getitem__(i)\n\t                    estimated_rewards[:, i, :] = model(\n\t                        torch.from_numpy(val_bandit_data['context']).float(),\n\t                        action.repeat(val_bandit_data['n_rounds'], 1),\n\t                        action_embed.repeat(val_bandit_data['n_rounds'], 1, 1)\n\t                    ).detach().numpy() \n\t                estimated_rewards_dict = {\n\t                    \"DM\": estimated_rewards,\n", "                    \"DR\": estimated_rewards,\n\t                }\n\t                estimated_policy_values = run_ope(\n\t                    val_bandit_data=val_bandit_data,\n\t                    action_dist_val=action_dist_val,\n\t                    estimated_rewards=estimated_rewards_dict,\n\t                    embed_model_config=cfg.embed_model_config,\n\t                    learned_embed_params=cfg.learned_embed_params,\n\t                    logging_losses_file=f\"{cfg.s3_path}/model_losses/{time()}.parquet\"\n\t                )\n", "                estimated_policy_value_list.append(estimated_policy_values)\n\t                elapsed = np.round((time() - start_time) / 60, 2)\n\t                diff = np.round(elapsed - elapsed_prev, 2)\n\t                logger.info(f\"t={t}: {elapsed}min (diff {diff}min)\")\n\t                elapsed_prev = elapsed\n\t                # summarize results\n\t                result_df = (\n\t                    DataFrame(DataFrame(estimated_policy_value_list).stack())\n\t                    .reset_index(1)\n\t                    .rename(columns={\"level_1\": \"est\", 0: \"value\"})\n", "                )\n\t                result_df[hyperparam_name] = hyperparam\n\t                result_df[\"se\"] = (result_df.value - policy_value) ** 2\n\t                result_df[\"bias\"] = 0\n\t                result_df[\"variance\"] = 0\n\t                sample_mean = DataFrame(result_df.groupby([\"est\"]).mean().value).reset_index()\n\t                for est_ in sample_mean[\"est\"]:\n\t                    estimates = result_df.loc[result_df[\"est\"] == est_, \"value\"].values\n\t                    mean_estimates = sample_mean.loc[sample_mean[\"est\"] == est_, \"value\"].values\n\t                    mean_estimates = np.ones_like(estimates) * mean_estimates\n", "                    result_df.loc[result_df[\"est\"] == est_, \"bias\"] = (\n\t                        policy_value - mean_estimates\n\t                    ) ** 2\n\t                    result_df.loc[result_df[\"est\"] == est_, \"variance\"] = (\n\t                        estimates - mean_estimates\n\t                    ) ** 2\n\t                result_df_list.append(result_df)\n\t                # aggregate all results\n\t                result_df = pd.concat(result_df_list).reset_index(level=0)\n\t                result_df.to_csv(f\"{cfg.s3_path}/result_df.csv\")\n", "        return result_df\n"]}
{"filename": "experiments/n_val_data_experiment.py", "chunked_list": ["from logging import getLogger\n\tfrom pathlib import Path\n\tfrom typing import List\n\timport pandas as pd\n\tfrom .abstracts.abstract_experiments import AbstractExperiment\n\tfrom .utils.configs import SyntheticOpeTrialConfig\n\tfrom .utils.plots import plot_line\n\tlogger = getLogger(__name__)\n\tclass NValDataExperiment(AbstractExperiment):\n\t    @property\n", "    def job_class_name(self) -> str:\n\t        return \"NValDataJob\"\n\t    @property\n\t    def trial_configs(self) -> List[SyntheticOpeTrialConfig]:\n\t        return [\n\t            SyntheticOpeTrialConfig(\n\t                name=\"800\",\n\t                n_val_data_list=[800]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n", "                name=\"1600\",\n\t                n_val_data_list=[1600]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"3200\",\n\t                n_val_data_list=[3200]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"6400\",\n\t                n_val_data_list=[6400]\n", "            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"12800\",\n\t                n_val_data_list=[12800]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"25600\",\n\t                n_val_data_list=[25600]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n", "                name=\"51200\",\n\t                n_val_data_list=[51200]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"102400\",\n\t                n_val_data_list=[102400]\n\t            ),\n\t        ]\n\t    @property\n\t    def instance_type(self) -> str:\n", "        return \"ml.c5.18xlarge\"\n\t    def get_output(self, experiment_name: str, local_path: str = None):\n\t        result = pd.DataFrame()\n\t        exclude = [\n\t            # 'MIPS (true)',\n\t            # \"Learned MIPS OneHot\", \n\t            # \"Learned MIPS FineTune\",\n\t            # \"Learned MIPS Combined\",\n\t        ]\n\t        if local_path:\n", "            result = pd.read_csv(f\"{local_path}/result_df.csv\", index_col=0)\n\t            output_path = Path(local_path)\n\t        else:\n\t            output_path = Path(self.get_output_path(experiment_name))\n\t            for trial in self.trial_configs:\n\t                s3_path = self.get_s3_path(experiment_name, trial.name)\n\t                try:\n\t                    result = result.append(pd.read_csv(f\"{s3_path}/result_df.csv\", index_col=0), ignore_index=True)\n\t                except:\n\t                    logger.error(f\"No result found for {trial.name}\")\n", "        plot_line(\n\t            result_df=result,\n\t            fig_path=output_path,\n\t            x=\"n_val_data\",\n\t            xlabel=\"Number of training samples\",\n\t            xticklabels=result.n_val_data.unique(),\n\t            exclude=exclude\n\t        )\n\t        if not local_path:\n\t            result.to_csv(f\"{self.get_output_path(experiment_name)}/result_df.csv\")\n"]}
{"filename": "experiments/real_dataset_experiment.py", "chunked_list": ["from typing import List\n\tfrom pathlib import Path\n\timport pandas as pd\n\tfrom .utils.configs import RealOpeTrialConfig\n\tfrom .abstracts.abstract_experiments import AbstractExperiment\n\tfrom .utils.plots import plot_cdf\n\tclass RealDatasetExperiment(AbstractExperiment):\n\t    @property\n\t    def job_class_name(self) -> str:\n\t        return \"RealDatasetJob\"\n", "    @property\n\t    def trial_configs(self) -> List[RealOpeTrialConfig]:\n\t        return [\n\t            RealOpeTrialConfig(\n\t                name=\"1000\",\n\t                sample_size=1000,\n\t            ),\n\t            RealOpeTrialConfig(\n\t                name=\"10000\",\n\t                sample_size=10000,\n", "            ),\n\t            RealOpeTrialConfig(\n\t                name=\"50000\",\n\t                sample_size=50000,\n\t            ),\n\t            RealOpeTrialConfig(\n\t                name=\"100000\",\n\t                sample_size=100000,\n\t            )\n\t        ]\n", "    @property\n\t    def instance_type(self) -> str:\n\t        return \"ml.c5.18xlarge\"\n\t    def get_output(self, experiment_name: str, local_path: str = None):\n\t        exclude = [\n\t            # \"Learned MIPS OneHot\", \n\t            # \"Learned MIPS FineTune\",\n\t            # \"Learned MIPS Combined\",\n\t            # \"SNIPS\",\n\t            # \"SwitchDR\",\n", "        ]\n\t        for trial in self.trial_configs:\n\t            s3_path = self.get_s3_path(experiment_name, trial.name) if not local_path else f\"{local_path}/{trial.name}\"\n\t            fig_dir = self.get_output_path(experiment_name, trial.name) if not local_path else f\"{local_path}/{trial.name}\"\n\t            Path(fig_dir).mkdir(parents=True, exist_ok=True)\n\t            result_df = pd.read_csv(f\"{s3_path}/result_df.csv\")\n\t            result_df.to_csv(f\"{fig_dir}/result_df.csv\")\n\t            plot_cdf(\n\t                result_df=result_df,\n\t                fig_path=f\"{fig_dir}/cdf_IPS.png\",\n", "                relative_to=\"IPS\",\n\t                exclude=exclude\n\t            )\n\t            plot_cdf(\n\t                result_df=result_df,\n\t                fig_path=f\"{fig_dir}/cdf_MIPS.png\",\n\t                exclude=exclude\n\t            )\n\t            plot_cdf(\n\t                result_df=result_df,\n", "                fig_path=f\"{fig_dir}/cdf_onehot.png\",\n\t                relative_to=\"Learned MIPS OneHot\",\n\t                exclude=exclude,\n\t            )\n\t            plot_cdf(\n\t                result_df=result_df,\n\t                fig_path=f\"{fig_dir}/cdf_IPS.pdf\",\n\t                relative_to=\"IPS\",\n\t                exclude=exclude,\n\t                remove_legend=True\n", "            )\n"]}
{"filename": "experiments/__init__.py", "chunked_list": []}
{"filename": "experiments/n_actions_experiment.py", "chunked_list": ["from logging import getLogger\n\tfrom pathlib import Path\n\tfrom typing import List\n\timport pandas as pd\n\tfrom .abstracts.abstract_experiments import AbstractExperiment\n\tfrom .utils.configs import SyntheticOpeTrialConfig\n\tfrom .utils.plots import plot_line\n\tlogger = getLogger(__name__)\n\tclass NActionsExperiment(AbstractExperiment):\n\t    @property\n", "    def job_class_name(self) -> str:\n\t        return \"NActionsJob\"\n\t    @property\n\t    def instance_type(self) -> str:\n\t        return \"ml.c5.18xlarge\"\n\t    @property\n\t    def trial_configs(self) -> List[SyntheticOpeTrialConfig]:\n\t        return [\n\t            SyntheticOpeTrialConfig(\n\t                name=\"10\",\n", "                n_actions_list=[10]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"20\",\n\t                n_actions_list=[20]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"50\",\n\t                n_actions_list=[50]\n\t            ),\n", "            SyntheticOpeTrialConfig(\n\t                name=\"100\",\n\t                n_actions_list=[100]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"200\",\n\t                n_actions_list=[200]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"500\",\n", "                n_actions_list=[500]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"1000\",\n\t                n_actions_list=[1000]\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"2000\",\n\t                n_actions_list=[2000]\n\t            ),\n", "        ]\n\t    def get_output(self, experiment_name: str, local_path: str = None):\n\t        result = pd.DataFrame()\n\t        exclude = [\n\t            # 'MIPS (true)',\n\t            # \"Learned MIPS OneHot\", \n\t            # \"Learned MIPS FineTune\",\n\t            # \"Learned MIPS Combined\",\n\t        ]\n\t        if local_path:\n", "            result = pd.read_csv(f\"{local_path}/result_df.csv\", index_col=0)\n\t            output_path = Path(local_path)\n\t        else:\n\t            output_path = Path(self.get_output_path(experiment_name))\n\t            for trial in self.trial_configs:\n\t                s3_path = self.get_s3_path(experiment_name, trial.name)\n\t                try:\n\t                    result = result.append(pd.read_csv(f\"{s3_path}/result_df.csv\", index_col=0), ignore_index=True)\n\t                except:\n\t                    logger.error(f\"No result found for {trial.name}\")\n", "        plot_line(\n\t            result_df = result,\n\t            fig_path = output_path,\n\t            x = \"n_actions\",\n\t            xlabel = \"Number of actions\",\n\t            xticklabels = result.n_actions.unique(),\n\t            exclude = exclude\n\t        )\n\t        if not local_path:\n\t            result.to_csv(f\"{self.get_output_path(experiment_name)}/result_df.csv\")\n"]}
{"filename": "experiments/n_unobs_cat_dim_experiment.py", "chunked_list": ["from logging import getLogger\n\tfrom pathlib import Path\n\tfrom typing import List\n\timport pandas as pd\n\tfrom .abstracts.abstract_experiments import AbstractExperiment\n\tfrom .utils.configs import SyntheticOpeTrialConfig\n\tfrom .utils.plots import plot_line\n\tlogger = getLogger(__name__)\n\tclass NUnobsCatDimExperiment(AbstractExperiment):\n\t    @property\n", "    def job_class_name(self) -> str:\n\t        return \"NUnobsCatDimJob\"\n\t    @property\n\t    def trial_configs(self) -> List[SyntheticOpeTrialConfig]:\n\t        return [\n\t            SyntheticOpeTrialConfig(\n\t                name=\"0\",\n\t                n_unobserved_cat_dim_list=[0],\n\t                n_cat_dim=20,\n\t            ),\n", "            SyntheticOpeTrialConfig(\n\t                name=\"2\",\n\t                n_unobserved_cat_dim_list=[2],\n\t                n_cat_dim=20,\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"4\",\n\t                n_unobserved_cat_dim_list=[4],\n\t                n_cat_dim=20,\n\t            ),\n", "            SyntheticOpeTrialConfig(\n\t                name=\"6\",\n\t                n_unobserved_cat_dim_list=[6],\n\t                n_cat_dim=20,\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"8\",\n\t                n_unobserved_cat_dim_list=[8],\n\t                n_cat_dim=20,\n\t            ),\n", "            SyntheticOpeTrialConfig(\n\t                name=\"10\",\n\t                n_unobserved_cat_dim_list=[10],\n\t                n_cat_dim=20,\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"12\",\n\t                n_unobserved_cat_dim_list=[12],\n\t                n_cat_dim=20,\n\t            ),\n", "            SyntheticOpeTrialConfig(\n\t                name=\"14\",\n\t                n_unobserved_cat_dim_list=[14],\n\t                n_cat_dim=20,\n\t            ),\n\t            SyntheticOpeTrialConfig(\n\t                name=\"16\",\n\t                n_unobserved_cat_dim_list=[16],\n\t                n_cat_dim=20,\n\t            ),\n", "            SyntheticOpeTrialConfig(\n\t                name=\"18\",\n\t                n_unobserved_cat_dim_list=[18],\n\t                n_cat_dim=20,\n\t            ),\n\t        ]\n\t    @property\n\t    def instance_type(self) -> str:\n\t        return \"ml.c5.18xlarge\"\n\t    def get_output(self, experiment_name: str, local_path: str = None):\n", "        exclude = [\n\t            # 'MIPS (true)',\n\t            # \"Learned MIPS OneHot\", \n\t            # \"Learned MIPS FineTune\",\n\t            # \"Learned MIPS Combined\",\n\t        ]\n\t        result = pd.DataFrame()\n\t        if local_path:\n\t            result = pd.read_csv(f\"{local_path}/result_df.csv\", index_col=0)\n\t            output_path = Path(local_path)\n", "        else:\n\t            output_path = Path(self.get_output_path(experiment_name))\n\t            for trial in self.trial_configs:\n\t                s3_path = self.get_s3_path(experiment_name, trial.name)\n\t                try:\n\t                    result = result.append(pd.read_csv(f\"{s3_path}/result_df.csv\", index_col=0), ignore_index=True)\n\t                except:\n\t                    logger.error(f\"No result found for {trial.name}\")\n\t        plot_line(\n\t            result_df=result,\n", "            fig_path=output_path,\n\t            x=\"n_unobserved_cat_dim\",\n\t            xlabel=\"Number of unobserved embedding dimensions\",\n\t            xticklabels=result.n_unobserved_cat_dim.unique(),\n\t            exclude=exclude\n\t        )\n\t        if not local_path:\n\t            result.to_csv(f\"{self.get_output_path(experiment_name)}/result_df.csv\")\n"]}
{"filename": "experiments/ablation_hpo_experiment.py", "chunked_list": ["from logging import getLogger\n\tfrom pathlib import Path\n\tfrom typing import List\n\timport matplotlib.pyplot as plt\n\timport seaborn as sns\n\tfrom sagemaker.tuner import IntegerParameter\n\tfrom .abstracts.abstract_hpo_experiment import AbstractHpoExperiment\n\tfrom .utils.configs import HpoTrialConfig\n\tplt.style.use(['science', 'no-latex'])\n\tlogger = getLogger(__name__)\n", "class AblationHpoExperiment(AbstractHpoExperiment):\n\t    @property\n\t    def job_class_name(self) -> str:\n\t        return \"OpeHpoJob\"\n\t    @property\n\t    def max_parallel_jobs(self) -> int:\n\t        return 5\n\t    @property\n\t    def instance_type(self) -> str:\n\t        return \"ml.c5.18xlarge\"\n", "    @property\n\t    def trial_configs(self) -> List[HpoTrialConfig]:\n\t        return [\n\t            HpoTrialConfig(\n\t                name=\"learned-embed-dim\",\n\t                hyperparameter_ranges={\n\t                    \"learned-embed-dim\": IntegerParameter(1, 100),\n\t                },\n\t                max_jobs=100\n\t            ),\n", "        ]\n\t    @property\n\t    def metric_definitions(self) -> List[dict]:\n\t        \"\"\"Metrics for HyperparameterTuner object to extract from logs\"\"\"\n\t        return [\n\t            {\"Name\": \"Relative MSE\", \"Regex\": \"Relative MSE: ([0-9\\\\.]+)\"},\n\t            {\"Name\": \"Relative MSE LCB\", \"Regex\": \"Relative MSE LCB: ([0-9\\\\.]+)\"},\n\t            {\"Name\": \"Relative MSE UCB\", \"Regex\": \"Relative MSE UCB: ([0-9\\\\.]+)\"},\n\t            {\"Name\": \"Bias\", \"Regex\": \"Bias: ([0-9\\\\.]+)\"},\n\t            {\"Name\": \"Variance\", \"Regex\": \"Variance: ([0-9\\\\.]+)\"},\n", "        ]\n\t    def get_output(self, experiment_name: str):\n\t        for trial in self.trial_configs:\n\t            try:\n\t                job_name = f\"{experiment_name}-{trial.name}\"\n\t                tuner = self.get_tuner(job_name)\n\t            except:\n\t                logger.error(f\"Tuning job {experiment_name}-{trial.name} not found\")\n\t                continue\n\t            df = tuner.analytics().dataframe()\n", "            df = df.join(self.get_metrics(job_name), on=\"TrainingJobName\")\n\t            hyperparameter_name = list(trial.hyperparameter_ranges.keys())[0]\n\t            df[hyperparameter_name] = df[hyperparameter_name].astype(str).str.replace('\"', '').astype(float)\n\t            df.sort_values(by=hyperparameter_name, inplace=True)\n\t            ax = sns.lineplot(\n\t                x=df[hyperparameter_name].rename(\"Size of the learned embeddings\"),\n\t                y=df['FinalObjectiveValue'].rename(self.objective_metric_name)\n\t            )\n\t            ax.fill_between(df[hyperparameter_name], df[\"Relative MSE LCB\"], df[\"Relative MSE UCB\"], alpha=0.2)\n\t            plt.yscale('log')\n", "            plt.ylim(top=5, bottom=4e-1)\n\t            output_dir = self.get_output_path(experiment_name, trial.name)\n\t            Path(output_dir).mkdir(parents=True, exist_ok=True)\n\t            tuner.analytics().export_csv(f\"{output_dir}/tuner_results.csv\")\n\t            plt.savefig(f\"{output_dir}/{hyperparameter_name}.pdf\", bbox_inches='tight')\n\t            plt.savefig(f\"{output_dir}/{hyperparameter_name}.png\", bbox_inches='tight')\n\t            plt.close()\n\t            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9, 3))\n\t            sns.lineplot(\n\t                x=df[hyperparameter_name].rename(\"Size of the learned embeddings\"),\n", "                y=df['Variance'],\n\t                ax=ax[0]\n\t            )\n\t            sns.lineplot(\n\t                x=df[hyperparameter_name].rename(\"Size of the learned embeddings\"),\n\t                y=df['Bias'],\n\t                ax=ax[1]\n\t            )\n\t            plt.savefig(f\"{output_dir}/{hyperparameter_name}_bias-variance.pdf\", bbox_inches='tight')\n\t            plt.savefig(f\"{output_dir}/{hyperparameter_name}_bias-variance.png\", bbox_inches='tight')\n", "            plt.close()\n"]}
{"filename": "experiments/utils/configs.py", "chunked_list": ["from dataclasses import dataclass, field\n\tfrom typing import List\n\t@dataclass\n\tclass TrialConfig:\n\t    name: str # name of the trial\n\t@dataclass\n\tclass LearnEmbedConfig:\n\t    learned_embed_dim: int = 5 # size of the learned action embedding\n\t@dataclass\n\tclass LearnedEmbedParams:\n", "    epochs: int = 100\n\t    batch_size: int = 64\n\t    lr: float = 1e-4\n\t    train_test_ratio: float = 1\n\t@dataclass\n\tclass OpeTrialConfig(TrialConfig):\n\t    embed_model_config: LearnEmbedConfig = field(default_factory=lambda: LearnEmbedConfig())\n\t    learned_embed_params: LearnedEmbedParams = field(default_factory=lambda: LearnedEmbedParams())\n\t@dataclass\n\tclass SyntheticOpeTrialConfig(OpeTrialConfig):\n", "    n_cat_dim: int = 3 # number of dimensions in the action embedding\n\t    n_unobserved_cat_dim: int = 0 # number of unobserved dimensions in the action embedding\n\t    dim_context: int = 10 # number of context dimensions\n\t    n_seeds: int = 100 # number of runs to average over\n\t    n_val_data: int = 10000 # default number of training samples\n\t    n_actions: int = 100 # default number of distinct actions\n\t    n_cat_per_dim: int = 10 # number of categories per dimension in the action embedding\n\t    n_test_data: int = 200000 # number of test samples\n\t    n_def_actions: int = 0 # number of actions in which we do not have any observations\n\t    latent_param_mat_dim: int = 5 # size of the random parameters matrix to generate the reward\n", "    beta: int = -1 # entropy of the logging policy, -1 means almost random uniform, 1 means almost deterministic\n\t    eps: float = 0.05 # the amount of exploration in eps-greedy evaluation policy\n\t    reward_std: float = 2.5 # amount of gaussian noise in the reward\n\t    is_optimal: bool = True # whether the policy selects the best or the worst action\n\t    embed_selection: bool = False # whether to use the SLOPE algorithm for embedding selection\n\t    random_state: int = 12345 # fixed seed to replicate the same results\n\t    n_val_data_list: List[int] = field(default_factory=lambda: [800, 1600, 3200, 6400, 12800, 25600]) # values when varying the number of training samples\n\t    n_unobserved_cat_dim_list: List[int] = field(\n\t        default_factory=lambda: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # values when varying the number of unobserved dimensions in the action embedding\n\t    eps_list: List[float] = field(default_factory=lambda: [0, 0.2, 0.4, 0.6, 0.8, 1]) # values when varying the amount of exploration in eps-greedy evaluation policy\n", "    beta_list: List[float] = field(default_factory=lambda: [-3, -2, -1, -0.5, 0, 0.5, 1, 2, 3]) # values when varying the entropy of the logging policy\n\t    noise_list: List[float] = field(default_factory=lambda: [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]) # values when varying the amount of gaussian noise in the reward\n\t    n_def_actions_list: List[float] = field(default_factory=lambda: [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]) # values when varying the number of actions in which we do not have any observations\n\t    n_actions_list: List[float] = field(\n\t        default_factory=lambda: [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n\t    ) # values when varying the number of distinct actions\n\t@dataclass\n\tclass RealOpeTrialConfig(OpeTrialConfig):\n\t    n_seeds: int = 150 # number of bootstrap runs\n\t    sample_size: int = 1000 # number of observations in one sample\n", "    random_state: str = 12345 # fixed seed to replicate the same results\n\t@dataclass\n\tclass HpoTrialConfig(SyntheticOpeTrialConfig):\n\t    random_state: str = 12345 # fixed seed to replicate the same results\n\t    hyperparameter_ranges: dict = field(default_factory=lambda: {}) # hyperparameter space defined by sagemaker hyperparameter ranges https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html\n\t    fixed_hyperparameters: dict = field(default_factory=lambda: {}) # fixed values for already optimized hyperparameters, ignored if present in hyperparameter_ranges\n\t    max_jobs: int = 100 # number of maximum trials when searching for the hyperparameters\n\t    strategy: str = \"Bayesian\" # strategy used to search over the hyperparameter space. Either 'Bayesian' or 'Random'\n"]}
{"filename": "experiments/utils/__init__.py", "chunked_list": []}
{"filename": "experiments/utils/plots.py", "chunked_list": ["from pathlib import Path\n\timport matplotlib.pyplot as plt\n\timport pandas as pd\n\timport seaborn as sns\n\timport numpy as np\n\tplt.style.use(['science', 'no-latex'])\n\tregistered_colors = {\n\t    \"MIPS\": \"tab:gray\",\n\t    \"MIPS (true)\": \"tab:orange\",\n\t    \"MIPS (w/ SLOPE)\": \"tab:orange\",\n", "    \"IPS\": \"tab:red\",\n\t    \"DR\": \"tab:blue\",\n\t    \"DM\": \"tab:purple\",\n\t    \"SNIPS\": \"lightblue\",\n\t    \"SwitchDR\": \"tab:pink\",\n\t    \"Learned MIPS OneHot\": \"tab:olive\",\n\t    \"Learned MIPS FineTune\": \"green\",\n\t    \"Learned MIPS Combined\": \"tab:brown\",\n\t}\n\tmarkers_dict = {\n", "    \"MIPS\": \"X\",\n\t    \"MIPS (true)\": \"X\",\n\t    \"MIPS (w/ SLOPE)\": \"X\",\n\t    \"IPS\": \"v\",\n\t    \"DR\": \"v\",\n\t    \"DM\": \"v\",\n\t    \"SNIPS\": \"v\",\n\t    \"SwitchDR\": \"v\",\n\t    \"Learned MIPS OneHot\": \"v\",\n\t    \"Learned MIPS FineTune\": \"X\",\n", "    \"Learned MIPS Combined\": \"X\",\n\t}\n\tdef export_legend(ax, filename=\"legend.pdf\"):\n\t    fig2 = plt.figure()\n\t    ax2 = fig2.add_subplot()\n\t    ax2.axis('off')\n\t    legend = ax2.legend(*ax.get_legend_handles_labels(), frameon=False, loc='lower center', ncol=10, handlelength=1.5)\n\t    for legobj in legend.legendHandles:\n\t        legobj.set_linewidth(2.5)\n\t        legobj._markeredgecolor = 'white'\n", "        legobj._markeredgewidth = 0.5\n\t        legobj._markersize = 8\n\t    fig = legend.figure\n\t    fig.canvas.draw()\n\t    bbox = legend.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n\t    fig.savefig(filename, dpi=\"figure\", bbox_inches=bbox)\n\tdef remove_estimators(df: pd.DataFrame, estimators: list):\n\t    return df[~df.est.isin(estimators)]\n\tdef plot_line(\n\t    result_df: pd.DataFrame,\n", "    fig_path: Path,\n\t    x: str,\n\t    xlabel: str,\n\t    xticklabels: list,\n\t    estimators: list = None,\n\t    exclude=[],\n\t    markers=True\n\t):\n\t    result_df = remove_estimators(result_df, exclude)\n\t    def plot_part(column, ylabel, fig_name, legend=True, log_scale=True):\n", "        plt.close()\n\t        fig, ax = plt.subplots(figsize=(11, 7), tight_layout=True)\n\t        sns.lineplot(\n\t            linewidth=5,\n\t            legend=legend,\n\t            markers=markers_dict if markers else False,\n\t            markersize=15,\n\t            markeredgecolor='white',\n\t            dashes=False,\n\t            x=x,\n", "            y=column,\n\t            hue=\"est\",\n\t            style=\"est\",\n\t            ax=ax,\n\t            palette=palette,\n\t            data=result_df.query(query),\n\t        )\n\t        if legend:\n\t            l = ax.legend(\n\t                loc=\"upper left\",\n", "                fontsize=25,\n\t            )\n\t            for legobj in l.legendHandles:\n\t                legobj.set_linewidth(4)\n\t                legobj.set_markersize(12)\n\t        # yaxis\n\t        if log_scale:\n\t            ax.set_yscale(\"log\")\n\t        ax.set_ylabel(ylabel, fontsize=25)\n\t        ax.tick_params(axis=\"y\", labelsize=18)\n", "        ax.yaxis.set_label_coords(-0.08, 0.5)\n\t        # ax.set_ylim(top=1.05e-1, bottom=4e-3)\n\t        # xaxis\n\t        if x in [\"n_actions\", \"n_val_data\"]:\n\t            ax.set_xscale(\"log\")\n\t        ax.set_xlabel(xlabel, fontsize=25)\n\t        ax.set_xticks(xticklabels)\n\t        ax.set_xticklabels(xticklabels, fontsize=18)\n\t        ax.xaxis.set_label_coords(0.5, -0.1)\n\t        plt.savefig(fig_path / fig_name, bbox_inches=\"tight\")\n", "        return ax\n\t    if estimators is None:\n\t        estimators = [est for est in result_df.est.unique() if est in registered_colors]\n\t    query = \"(\" + \" or \".join([f\"est == '{est}'\" for est in estimators]) + \")\"\n\t    palette = [registered_colors[est] for est in estimators]\n\t    fig_path.mkdir(exist_ok=True, parents=True)\n\t    print(estimators)\n\t    plot_part(\"se\", \"MSE\", \"mse.png\")\n\t    ax = plot_part(\"se\", \"MSE\", \"mse.pdf\")\n\t    export_legend(ax, fig_path / \"legend.pdf\")\n", "    plot_part(\"se\", \"MSE\", \"mse_no_legend.pdf\", legend=False)\n\t    plot_part(\"se\", \"MSE\", \"mse_no_log.pdf\", log_scale=False)\n\t    plot_part(\"se\", \"MSE\", \"mse_no_log_no_legend.pdf\", legend=False, log_scale=False)\n\t    plot_part(\"variance\", \"Variance\", \"variance.pdf\")\n\t    plot_part(\"variance\", \"Variance\", \"variance_no_legend.pdf\", legend=False)\n\t    plot_part(\"variance\", \"Variance\", \"variance_no_log.pdf\", log_scale=False)\n\t    plot_part(\"variance\", \"Variance\", \"variance_no_log_no_legend.pdf\", legend=False, log_scale=False)\n\t    plot_part(\"bias\", \"Squared bias\", \"bias_no_log.pdf\", log_scale=False)\n\t    plot_part(\"bias\", \"Squared bias\", \"bias_no_log_no_legend.pdf\", legend=False, log_scale=False)\n\tdef plot_cdf(\n", "    result_df: pd.DataFrame,\n\t    fig_path: str,\n\t    relative_to: str = 'MIPS (w/ SLOPE)',\n\t    remove_legend=False,\n\t    exclude=[]\n\t):\n\t    result_df = remove_estimators(result_df, exclude)\n\t    estimators = [est for est in result_df.est.unique() if est in registered_colors]\n\t    query = \"(\" + \" or \".join([f\"est == '{est}'\" for est in estimators]) + \")\"\n\t    result_df = result_df.query(query).reset_index()\n", "    relative_index = result_df[result_df.est == relative_to].index[0]\n\t    rel_result_df = result_df.groupby(result_df.index // len(estimators)) \\\n\t        .apply(lambda df: pd.DataFrame({'est': df['est'], 'se': df['se'] / df.iloc[relative_index]['se']}))\n\t    palette = [registered_colors[est] for est in estimators[::-1]]\n\t    fig, ax = plt.subplots(figsize=(10, 6.5), tight_layout=True)\n\t    sns.ecdfplot(\n\t        linewidth=3.5,\n\t        palette=palette,\n\t        data=rel_result_df,\n\t        x=\"se\",\n", "        hue=\"est\",\n\t        hue_order=estimators[::-1],\n\t        ax=ax\n\t    )\n\t    ax.legend(estimators, loc=\"upper left\", fontsize=22)\n\t    if remove_legend:\n\t        ax.legend(estimators, loc=\"upper left\", fontsize=22).remove()\n\t    for i in range(len(ax.lines)):\n\t        line = ax.lines[i]\n\t        if line._x.max() == 1:\n", "            continue\n\t        idx = np.abs(line._x - 1).argmin()\n\t        y = line._y[idx]\n\t        ax.axhline(y, 0, 0.52, color=line.get_color(), linewidth=1.5, linestyle=(0, (5, 3)))\n\t    ax.set_ylabel(\"Cumulative distribution\", fontsize=22)\n\t    ax.tick_params(axis=\"y\", labelsize=18)\n\t    ax.yaxis.set_label_coords(-0.08, 0.5)\n\t    ax.set_xscale(\"log\")\n\t    ax.set_xlim(0.3, 3)\n\t    ax.set_xlabel(f\"Relative MSE w.r.t. {relative_to}\", fontsize=22)\n", "    ax.tick_params(axis=\"x\", labelsize=18)\n\t    ax.xaxis.set_label_coords(0.5, -0.1)\n\t    plt.savefig(fig_path, bbox_inches='tight')\n\t    plt.close()\n"]}
{"filename": "experiments/utils/constants.py", "chunked_list": ["BUCKET = \"ope-learn-action-embeddings\"\n\tROLE = \"SageMakerUser\"\n"]}
{"filename": "experiments/abstracts/abstract_hpo_experiment.py", "chunked_list": ["import dataclasses\n\timport json\n\timport logging\n\tfrom enum import Enum\n\tfrom typing import List\n\timport pandas as pd\n\timport sagemaker\n\tfrom sagemaker.pytorch import PyTorch\n\tfrom sagemaker.tuner import HyperparameterTuner\n\tfrom experiments.utils.configs import HpoTrialConfig\n", "from experiments.utils.constants import ROLE\n\tfrom .abstract_experiments import AbstractExperiment\n\tsm_sess = sagemaker.Session()\n\tsm = sm_sess.sagemaker_client\n\tlog = logging.getLogger()\n\tclass Status(Enum):\n\t    FAILED = \"Failed\"\n\t    IN_PROGRESS = \"InProgress\"\n\t@dataclasses.dataclass\n\tclass AbstractHpoExperiment(AbstractExperiment):\n", "    def fit(self, trial_config: HpoTrialConfig, experiment_name):\n\t        job_parameters = self.get_job_config_parameter(trial_config, experiment_name)\n\t        estimator = PyTorch(\n\t            py_version=\"py38\",\n\t            entry_point=self.entry_point,\n\t            role=ROLE,\n\t            sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n\t            framework_version=\"1.9.0\",\n\t            instance_count=1,\n\t            instance_type=self.instance_type,\n", "            hyperparameters=job_parameters,\n\t            metric_definitions=[],\n\t            enable_sagemaker_metrics=True,\n\t            source_dir=\"./\",\n\t            max_run=5 * 24 * 60 * 60  # set the training limit to 5 days (maximum allowed time by default)\n\t        )\n\t        tuner = HyperparameterTuner(\n\t            estimator,\n\t            self.objective_metric_name,\n\t            trial_config.hyperparameter_ranges,\n", "            self.metric_definitions,\n\t            max_jobs=trial_config.max_jobs,\n\t            max_parallel_jobs=self.max_parallel_jobs,\n\t            objective_type=self.objective_type,\n\t            strategy=trial_config.strategy\n\t        )\n\t        training_job_name = f\"{experiment_name}-{trial_config.name}\"\n\t        trial_name = f\"trial-{training_job_name}\"\n\t        tuner.fit(\n\t            job_name=training_job_name,\n", "            experiment_config={\n\t                \"TrialName\": trial_name,\n\t                \"TrialComponentDisplayName\": \"Training\",\n\t            },\n\t            wait=False,\n\t        )\n\t        return tuner\n\t    def get_tuner(self, training_job_name):\n\t        return HyperparameterTuner.attach(training_job_name, sagemaker_session=sm_sess)\n\t    def get_job_config_parameter(self, trial: HpoTrialConfig, experiment_name: str) -> dict:\n", "        parameters = {**dataclasses.asdict(trial),\n\t                      **{\"s3_path\": self.get_s3_path(experiment_name, trial.name),\n\t                         \"job_class_name\": self.job_class_name}}\n\t        del parameters['hyperparameter_ranges']\n\t        return {\"config\": f\"'{json.dumps(parameters)}'\"}\n\t    def get_metrics(self, training_job_name):\n\t        jobs = []\n\t        while True:\n\t            result = sm.list_training_jobs_for_hyper_parameter_tuning_job(\n\t                HyperParameterTuningJobName=training_job_name,\n", "                MaxResults=100,\n\t                SortBy='FinalObjectiveMetricValue',\n\t                SortOrder='Descending'\n\t            )\n\t            jobs += result['TrainingJobSummaries']\n\t            if 'NextToken' not in result:\n\t                break\n\t        result = pd.DataFrame()\n\t        for job in jobs:\n\t            job_description = sm.describe_training_job(TrainingJobName=job['TrainingJobName'])\n", "            metrics = {m['MetricName']: m['Value'] for m in job_description['FinalMetricDataList']}\n\t            result = result.append(pd.Series(metrics, name=job['TrainingJobName']))\n\t        return result\n\t    @property\n\t    def metric_definitions(self) -> List[dict]:\n\t        \"\"\"Metrics for HyperparameterTuner object to extract from logs\"\"\"\n\t        return [\n\t            {\"Name\": \"Relative MSE\", \"Regex\": \"Relative MSE: ([0-9\\\\.]+)\"},\n\t        ]\n\t    @property\n", "    def objective_metric_name(self) -> str:\n\t        \"\"\"Metric that tuner optimizes for. Must be specified in the @property metrics_definitions field\"\"\"\n\t        return \"Relative MSE\"\n\t    @property\n\t    def objective_type(self) -> str:\n\t        \"\"\"Whether the tuner should 'Minimize' or 'Maximize' the objective metric\"\"\"\n\t        return \"Minimize\"\n\t    @property\n\t    def max_parallel_jobs(self) -> int:\n\t        \"\"\"Maximum number of jobs to run simultaneously\"\"\"\n", "        return 8\n\t    @property\n\t    def instance_type(self) -> str:\n\t        \"\"\"AWS SageMaker Training instance type used for the job execution\"\"\"\n\t        return \"ml.c5.4xlarge\"\n"]}
{"filename": "experiments/abstracts/abstract_experiments.py", "chunked_list": ["import abc\n\timport dataclasses\n\timport json\n\timport logging\n\tfrom abc import ABC\n\timport time\n\tfrom enum import Enum\n\tfrom typing import List\n\timport sagemaker\n\tfrom sagemaker.pytorch import PyTorch\n", "from smexperiments import tracker, experiment\n\tfrom smexperiments.experiment import Experiment\n\tfrom smexperiments.trial import Trial\n\tfrom smexperiments.trial_component import TrialComponent\n\tfrom experiments.utils.configs import TrialConfig\n\tfrom experiments.utils.constants import ROLE, BUCKET\n\tsm_sess = sagemaker.Session()\n\tsm = sm_sess.sagemaker_client\n\tlog = logging.getLogger()\n\tclass Status(Enum):\n", "    FAILED = \"Failed\"\n\t    IN_PROGRESS = \"InProgress\"\n\t@dataclasses.dataclass\n\tclass AbstractExperiment(ABC):\n\t    def run(self, experiment_name: str):\n\t        \"\"\"\n\t        All experiments should extend this abstract class and implement\n\t        all abstract functions\n\t        @param experiment_name: unique experiment name\n\t        \"\"\"\n", "        experiment = Experiment.create(\n\t            experiment_name=experiment_name,\n\t            description=self.description,\n\t            sagemaker_boto_client=sm,\n\t        )\n\t        estimators = []\n\t        for trial_config in self.trial_configs:\n\t            job_parameters = self.get_job_config_parameter(trial_config, experiment_name)\n\t            trial_name = f\"trial-{experiment_name}-{trial_config.name}\"\n\t            log.info(trial_name)\n", "            sm_trial = Trial.create(\n\t                trial_name=trial_name,\n\t                experiment_name=experiment.experiment_name,\n\t                sagemaker_boto_client=sm,\n\t            )\n\t            log.info(job_parameters)\n\t            with tracker.Tracker.create() as trail_tracker:\n\t                trail_tracker.log_parameters({**job_parameters})\n\t                sm_trial.add_trial_component(trail_tracker)\n\t            estimator = self.fit(trial_config, experiment_name)\n", "            estimators.append(estimator)\n\t        return estimators\n\t    def fit(self, trial_config, experiment_name):\n\t        job_parameters = self.get_job_config_parameter(trial_config, experiment_name)\n\t        estimator = PyTorch(\n\t            py_version=\"py38\",\n\t            entry_point=self.entry_point,\n\t            role=ROLE,\n\t            sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n\t            framework_version=\"1.9.0\",\n", "            instance_count=1,\n\t            instance_type=self.instance_type,\n\t            hyperparameters=job_parameters,\n\t            metric_definitions=[],\n\t            enable_sagemaker_metrics=True,\n\t            source_dir=\"./\",\n\t            max_run=5 * 24 * 60 * 60  # set the training limit to 5 days (maximum allowed time by default)\n\t        )\n\t        training_job_name = f\"{experiment_name}-{trial_config.name}\"\n\t        trial_name = f\"trial-{training_job_name}\"\n", "        estimator.fit(\n\t            job_name=training_job_name,\n\t            experiment_config={\n\t                \"TrialName\": trial_name,\n\t                \"TrialComponentDisplayName\": \"Training\",\n\t            },\n\t            wait=False,\n\t        )\n\t        return estimator\n\t    @staticmethod\n", "    def training_jobs_has_finished(experiment_name) -> bool:\n\t        sm_experiment = experiment.Experiment.load(experiment_name)\n\t        jobs = []\n\t        for sm_trial_summary in sm_experiment.list_trials():\n\t            for trial_component_summary in Trial.load(sm_trial_summary.trial_name).list_trial_components():\n\t                trial_component = TrialComponent.load(trial_component_summary.trial_component_name)\n\t                if \"sagemaker_job_name\" in trial_component.parameters:\n\t                    job_name = trial_component.parameters[\"sagemaker_job_name\"].replace(\"'\", \"\").replace('\"', \"\")\n\t                    jobs.append(sm_sess.describe_training_job(job_name))\n\t        [log.info((job[\"TrainingJobName\"], job[\"TrainingJobStatus\"])) for job in jobs]\n", "        failed_jobs = [job for job in jobs if job[\"TrainingJobStatus\"] == Status.FAILED.value]\n\t        in_progress_jobs = [job for job in jobs if job[\"TrainingJobStatus\"] == Status.IN_PROGRESS.value]\n\t        if failed_jobs:\n\t            log.info(\"Jobs have failed\")\n\t            return False\n\t        if in_progress_jobs:\n\t            log.info(\"Jobs are in progress\")\n\t            return False\n\t        return True\n\t    @abc.abstractmethod\n", "    def get_output(self, experiment_name: str):\n\t        \"\"\"\n\t        Generates output of the experiment from the data published\n\t        to the s3 bucket, graphs or tables can be the output of an experiment\n\t        @param experiment_name: The name of the experiment which the output is related to\n\t        \"\"\"\n\t        pass\n\t    def get_s3_path(self, experiment_name, trial_name=None) -> str:\n\t        result = f\"s3://{BUCKET}/experiments/experiment={experiment_name}\"\n\t        return result if not trial_name else result + f\"/trial={trial_name}\"\n", "    def get_local_path(self, experiment_name, trial_name=None) -> str:\n\t        result = f\"./results/{experiment_name}\"\n\t        return result if not trial_name else result + f\"/{trial_name}\"\n\t    def list_s3_files(self, experiment_name, trial_name) -> List[str]:\n\t        objects = []\n\t        request_params = {}\n\t        while True:\n\t            response = sm_sess.boto_session.client('s3').list_objects_v2(\n\t                Bucket=BUCKET, Prefix=f\"experiments/experiment={experiment_name}/trial={trial_name}\", **request_params\n\t            )\n", "            objects += response[\"Contents\"]\n\t            if \"NextContinuationToken\" in response:\n\t                request_params[\"ContinuationToken\"] = response[\"NextContinuationToken\"]\n\t            else:\n\t                break\n\t        return [f\"s3://{BUCKET}/{obj['Key']}\" for obj in objects]\n\t    def get_output_path(self, experiment_name, trial_name=None) -> str:\n\t        result = f\"./results/{experiment_name}\"\n\t        return result if not trial_name else result + f\"/{trial_name}\"\n\t    def get_job_config_parameter(self, trial: TrialConfig, experiment_name: str) -> dict:\n", "        parameters = {**dataclasses.asdict(trial),\n\t                      **{\"s3_path\": self.get_s3_path(experiment_name, trial.name),\n\t                         \"job_class_name\": self.job_class_name}}\n\t        return {\"config\": f\"'{json.dumps(parameters)}'\"}\n\t    @property\n\t    @abc.abstractmethod\n\t    def trial_configs(self) -> List[TrialConfig]:\n\t        \"\"\"\n\t        This function returns the trial configs for each training job,\n\t        it needs to be overridden by each subclass\n", "        \"\"\"\n\t        pass\n\t    @property\n\t    @abc.abstractmethod\n\t    def job_class_name(self) -> str:\n\t        \"\"\"\n\t        This function returns the training job class name e.g ActionsJob,\n\t        it needs to be overridden by each subclass\n\t        \"\"\"\n\t        pass\n", "    @property\n\t    def entry_point(self) -> str:\n\t        return \"entry_point.py\"\n\t    @property\n\t    def instance_type(self) -> str:\n\t        return \"ml.c5.xlarge\"\n\t    @property\n\t    def description(self) -> str:\n\t        \"\"\"\n\t        This function returns the training job class name e.g ActionsJob,\n", "        it needs to be overridden by each subclass\n\t        \"\"\"\n\t        return \" \"\n"]}
{"filename": "experiments/abstracts/__init__.py", "chunked_list": []}
