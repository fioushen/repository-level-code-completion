{"filename": "skllm/gpt4all_client.py", "chunked_list": ["from typing import Dict\n\ttry:\n\t    from gpt4all import GPT4All\n\texcept (ImportError, ModuleNotFoundError):\n\t    GPT4All = None\n\t_loaded_models = {}\n\tdef _make_openai_compatabile(message: str) -> Dict:\n\t    return {\"choices\": [{\"message\": {\"content\": message, \"role\": \"assistant\"}}]}\n\tdef get_chat_completion(\n\t    messages: Dict, model: str = \"ggml-model-gpt4all-falcon-q4_0.bin\"\n", ") -> Dict:\n\t    \"\"\"Gets a chat completion from GPT4All.\n\t    Parameters\n\t    ----------\n\t    messages : Dict\n\t        The messages to use as a prompt for the chat completion.\n\t    model : str\n\t        The model to use for the chat completion. Defaults to \"ggml-gpt4all-j-v1.3-groovy\".\n\t    Returns\n\t    -------\n", "    completion : Dict\n\t    \"\"\"\n\t    if GPT4All is None:\n\t        raise ImportError(\n\t            \"gpt4all is not installed, try `pip install scikit-llm[gpt4all]`\"\n\t        )\n\t    if model not in _loaded_models.keys():\n\t        loaded_model = GPT4All(model)\n\t        _loaded_models[model] = loaded_model\n\t        loaded_model._current_prompt_template = loaded_model.config[\"promptTemplate\"]\n", "    prompt = _loaded_models[model]._format_chat_prompt_template(\n\t        messages, _loaded_models[model].config[\"systemPrompt\"]\n\t    )\n\t    generated = _loaded_models[model].generate(\n\t        prompt,\n\t        streaming=False,\n\t        temp=1e-10,\n\t    )\n\t    return _make_openai_compatabile(generated)\n\tdef unload_models() -> None:\n", "    global _loaded_models\n\t    _loaded_models = {}\n"]}
{"filename": "skllm/config.py", "chunked_list": ["import os\n\tfrom typing import Optional\n\t_OPENAI_KEY_VAR = \"SKLLM_CONFIG_OPENAI_KEY\"\n\t_OPENAI_ORG_VAR = \"SKLLM_CONFIG_OPENAI_ORG\"\n\t_AZURE_API_BASE_VAR = \"SKLLM_CONFIG_AZURE_API_BASE\"\n\t_AZURE_API_VERSION_VAR = \"SKLLM_CONFIG_AZURE_API_VERSION\"\n\t_GOOGLE_PROJECT = \"GOOGLE_CLOUD_PROJECT\"\n\tclass SKLLMConfig:\n\t    @staticmethod\n\t    def set_openai_key(key: str) -> None:\n", "        \"\"\"Sets the OpenAI key.\n\t        Parameters\n\t        ----------\n\t        key : str\n\t            OpenAI key.\n\t        \"\"\"\n\t        os.environ[_OPENAI_KEY_VAR] = key\n\t    @staticmethod\n\t    def get_openai_key() -> Optional[str]:\n\t        \"\"\"Gets the OpenAI key.\n", "        Returns\n\t        -------\n\t        Optional[str]\n\t            OpenAI key.\n\t        \"\"\"\n\t        return os.environ.get(_OPENAI_KEY_VAR, None)\n\t    @staticmethod\n\t    def set_openai_org(key: str) -> None:\n\t        \"\"\"Sets OpenAI organization ID.\n\t        Parameters\n", "        ----------\n\t        key : str\n\t            OpenAI organization ID.\n\t        \"\"\"\n\t        os.environ[_OPENAI_ORG_VAR] = key\n\t    @staticmethod\n\t    def get_openai_org() -> str:\n\t        \"\"\"Gets the OpenAI organization ID.\n\t        Returns\n\t        -------\n", "        str\n\t            OpenAI organization ID.\n\t        \"\"\"\n\t        return os.environ.get(_OPENAI_ORG_VAR, \"\")\n\t    @staticmethod\n\t    def get_azure_api_base() -> str:\n\t        \"\"\"Gets the API base for Azure.\n\t        Returns\n\t        -------\n\t        str\n", "            URL to be used as the base for the Azure API.\n\t        \"\"\"\n\t        base = os.environ.get(_AZURE_API_BASE_VAR, None)\n\t        if base is None:\n\t            raise RuntimeError(\"Azure API base is not set\")\n\t        return base\n\t    @staticmethod\n\t    def set_azure_api_base(base: str) -> None:\n\t        \"\"\"Set the API base for Azure.\n\t        Parameters\n", "        ----------\n\t        base : str\n\t            URL to be used as the base for the Azure API.\n\t        \"\"\"\n\t        os.environ[_AZURE_API_BASE_VAR] = base\n\t    @staticmethod\n\t    def set_azure_api_version(ver: str) -> None:\n\t        \"\"\"Set the API version for Azure.\n\t        Parameters\n\t        ----------\n", "        ver : str\n\t            Azure API version.\n\t        \"\"\"\n\t        os.environ[_AZURE_API_VERSION_VAR] = ver\n\t    @staticmethod\n\t    def get_azure_api_version() -> str:\n\t        \"\"\"Gets the API version for Azure.\n\t        Returns\n\t        -------\n\t        str\n", "            Azure API version.\n\t        \"\"\"\n\t        return os.environ.get(_AZURE_API_VERSION_VAR, \"2023-05-15\")\n\t    @staticmethod\n\t    def get_google_project() -> Optional[str]:\n\t        \"\"\"Gets the Google Cloud project ID.\n\t        Returns\n\t        -------\n\t        Optional[str]\n\t            Google Cloud project ID.\n", "        \"\"\"\n\t        return os.environ.get(_GOOGLE_PROJECT, None)\n\t    @staticmethod\n\t    def set_google_project(project: str) -> None:\n\t        \"\"\"Sets the Google Cloud project ID.\n\t        Parameters\n\t        ----------\n\t        project : str\n\t            Google Cloud project ID.\n\t        \"\"\"\n", "        os.environ[_GOOGLE_PROJECT] = project\n"]}
{"filename": "skllm/completions.py", "chunked_list": ["from skllm.gpt4all_client import get_chat_completion as _g4a_get_chat_completion\n\tfrom skllm.openai.chatgpt import get_chat_completion as _oai_get_chat_completion\n\tdef get_chat_completion(\n\t    messages: dict,\n\t    openai_key: str = None,\n\t    openai_org: str = None,\n\t    model: str = \"gpt-3.5-turbo\",\n\t    max_retries: int = 3,\n\t):\n\t    \"\"\"Gets a chat completion from the OpenAI API.\"\"\"\n", "    if model.startswith(\"gpt4all::\"):\n\t        return _g4a_get_chat_completion(messages, model[9:])\n\t    else:\n\t        api = \"azure\" if model.startswith(\"azure::\") else \"openai\"\n\t        if api == \"azure\":\n\t            model = model[7:]\n\t        return _oai_get_chat_completion(\n\t            messages, openai_key, openai_org, model, max_retries, api=api\n\t        )\n"]}
{"filename": "skllm/__init__.py", "chunked_list": ["# ordering is important here to prevent circular imports\n\tfrom skllm.models.gpt.gpt_zero_shot_clf import (\n\t    MultiLabelZeroShotGPTClassifier,\n\t    ZeroShotGPTClassifier,\n\t)\n\tfrom skllm.models.gpt.gpt_few_shot_clf import FewShotGPTClassifier\n\tfrom skllm.models.gpt.gpt_dyn_few_shot_clf import DynamicFewShotGPTClassifier\n"]}
{"filename": "skllm/utils.py", "chunked_list": ["import json\n\tfrom typing import Any\n\timport numpy as np\n\timport pandas as pd\n\tdef to_numpy(X: Any) -> np.ndarray:\n\t    \"\"\"Converts a pandas Series or list to a numpy array.\n\t    Parameters\n\t    ----------\n\t    X : Any\n\t        The data to convert to a numpy array.\n", "    Returns\n\t    -------\n\t    X : np.ndarray\n\t    \"\"\"\n\t    if isinstance(X, pd.Series):\n\t        X = X.to_numpy().astype(object)\n\t    elif isinstance(X, list):\n\t        X = np.asarray(X, dtype=object)\n\t    if isinstance(X, np.ndarray) and len(X.shape) > 1:\n\t        X = np.squeeze(X)\n", "    return X\n\tdef find_json_in_string(string: str) -> str:\n\t    \"\"\"Finds the JSON object in a string.\n\t    Parameters\n\t    ----------\n\t    string : str\n\t        The string to search for a JSON object.\n\t    Returns\n\t    -------\n\t    json_string : str\n", "    \"\"\"\n\t    start = string.find(\"{\")\n\t    end = string.rfind(\"}\")\n\t    if start != -1 and end != -1:\n\t        json_string = string[start : end + 1]\n\t    else:\n\t        json_string = \"{}\"\n\t    return json_string\n\tdef extract_json_key(json_: str, key: str):\n\t    \"\"\"Extracts JSON key from a string.\n", "    json_ : str\n\t        The JSON string to extract the key from.\n\t    key : str\n\t        The key to extract.\n\t    \"\"\"\n\t    original_json = json_\n\t    for i in range(2):\n\t        try:\n\t            json_ = original_json.replace(\"\\n\", \"\")\n\t            if i == 1:\n", "                json_ = json_.replace(\"'\", '\"')\n\t            json_ = find_json_in_string(json_)\n\t            as_json = json.loads(json_)\n\t            if key not in as_json.keys():\n\t                raise KeyError(\"The required key was not found\")\n\t            return as_json[key]\n\t        except Exception:\n\t            if i == 0:\n\t                continue\n\t            return None\n"]}
{"filename": "skllm/preprocessing/gpt_vectorizer.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import Any, List, Optional, Union\n\timport numpy as np\n\timport pandas as pd\n\tfrom numpy import ndarray\n\tfrom sklearn.base import BaseEstimator as _BaseEstimator\n\tfrom sklearn.base import TransformerMixin as _TransformerMixin\n\tfrom tqdm import tqdm\n\tfrom skllm.openai.embeddings import get_embedding as _get_embedding\n\tfrom skllm.openai.mixin import OpenAIMixin as _OAIMixin\n", "from skllm.utils import to_numpy as _to_numpy\n\tclass GPTVectorizer(_BaseEstimator, _TransformerMixin, _OAIMixin):\n\t    \"\"\"\n\t    A class that uses OPEN AI embedding model that converts text to GPT embeddings.\n\t    Parameters\n\t    ----------\n\t    openai_embedding_model : str\n\t        The OPEN AI embedding model to use. Defaults to \"text-embedding-ada-002\".\n\t    openai_key : str, optional\n\t        The OPEN AI key to use. Defaults to None.\n", "    openai_org : str, optional\n\t        The OPEN AI organization ID to use. Defaults to None.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        openai_embedding_model: str = \"text-embedding-ada-002\",\n\t        openai_key: Optional[str] = None,\n\t        openai_org: Optional[str] = None,\n\t    ):\n\t        self.openai_embedding_model = openai_embedding_model\n", "        self._set_keys(openai_key, openai_org)\n\t    def fit(self, X: Any = None, y: Any = None, **kwargs) -> GPTVectorizer:\n\t        \"\"\"\n\t        Fits the GPTVectorizer to the data.\n\t        This is modelled to function as the sklearn fit method.\n\t        Parameters\n\t        ----------\n\t        X : Any, optional\n\t        y : Any, optional\n\t        kwargs : dict, optional\n", "        Returns\n\t        -------\n\t        self : GPTVectorizer\n\t        \"\"\"\n\t        return self\n\t    def transform(self, X: Optional[Union[np.ndarray, pd.Series, List[str]]]) -> ndarray:\n\t        \"\"\"\n\t        Transforms a list of strings into a list of GPT embeddings.\n\t        This is modelled to function as the sklearn transform method\n\t        Parameters\n", "        ----------\n\t        X : Optional[Union[np.ndarray, pd.Series, List[str]]]\n\t            The input array of strings to transform into GPT embeddings.\n\t        Returns\n\t        -------\n\t        embeddings : np.ndarray\n\t        \"\"\"\n\t        X = _to_numpy(X)\n\t        embeddings = []\n\t        for i in tqdm(range(len(X))):\n", "            embeddings.append(\n\t                _get_embedding(X[i], self._get_openai_key(), self._get_openai_org())\n\t            )\n\t        embeddings = np.asarray(embeddings)\n\t        return embeddings\n\t    def fit_transform(self, X: Optional[Union[np.ndarray, pd.Series, List[str]]], y=None, **fit_params) -> ndarray:\n\t        \"\"\"\n\t        Fits and transforms a list of strings into a list of GPT embeddings.\n\t        This is modelled to function as the sklearn fit_transform method\n\t        Parameters\n", "        ----------\n\t        X : Optional[Union[np.ndarray, pd.Series, List[str]]]\n\t            The input array of strings to transform into GPT embeddings.\n\t        y : Any, optional\n\t        Returns\n\t        -------\n\t        embeddings : np.ndarray\n\t        \"\"\"\n\t        return self.fit(X, y).transform(X)\n"]}
{"filename": "skllm/preprocessing/gpt_summarizer.py", "chunked_list": ["from typing import Any, List, Optional, Union\n\timport numpy as np\n\tfrom numpy import ndarray\n\tfrom pandas import Series\n\tfrom skllm.openai.base_gpt import BaseZeroShotGPTTransformer as _BaseGPT\n\tfrom skllm.prompts.builders import build_focused_summary_prompt, build_summary_prompt\n\tclass GPTSummarizer(_BaseGPT):\n\t    \"\"\"\n\t    A text summarizer.\n\t    Parameters\n", "    ----------\n\t    openai_key : str, optional\n\t        The OPEN AI key to use. Defaults to None.\n\t    openai_org : str, optional\n\t        The OPEN AI organization ID to use. Defaults to None.\n\t    openai_model : str, optional\n\t        The OPEN AI model to use. Defaults to \"gpt-3.5-turbo\".\n\t    max_words : int, optional\n\t        The maximum number of words to use in the summary. Defaults to 15.\n\t    \"\"\"\n", "    system_msg = \"You are a text summarizer.\"\n\t    default_output = \"Summary is unavailable.\"\n\t    def __init__(\n\t        self,\n\t        openai_key: Optional[str] = None,\n\t        openai_org: Optional[str] = None,\n\t        openai_model: str = \"gpt-3.5-turbo\",\n\t        max_words: int = 15,\n\t        focus: Optional[str] = None,\n\t    ):\n", "        self._set_keys(openai_key, openai_org)\n\t        self.openai_model = openai_model\n\t        self.max_words = max_words\n\t        self.focus = focus\n\t    def _get_prompt(self, X: str) -> str:\n\t        \"\"\"\n\t        Generates the prompt for the given input.\n\t        Parameters\n\t        ----------\n\t        X : str\n", "            sample to summarize\n\t        Returns\n\t        -------\n\t        str\n\t        \"\"\"\n\t        if self.focus:\n\t            return build_focused_summary_prompt(X, self.max_words, self.focus)\n\t        else:\n\t            return build_summary_prompt(X, self.max_words)\n\t    def transform(self, X: Union[ndarray, Series, List[str]], **kwargs: Any) -> ndarray:\n", "        y = super().transform(X, **kwargs)\n\t        if self.focus:\n\t            # remove \"Mentioned concept is not present in the text.\" from the output\n\t            y = np.asarray(\n\t                [\n\t                    i.replace(\"Mentioned concept is not present in the text.\", \"\")\n\t                    .replace(\"The general summary is:\", \"\")\n\t                    .strip()\n\t                    for i in y\n\t                ],\n", "                dtype=object,\n\t            )\n\t        return y\n"]}
{"filename": "skllm/preprocessing/__init__.py", "chunked_list": ["from skllm.preprocessing.gpt_summarizer import GPTSummarizer\n\tfrom skllm.preprocessing.gpt_translator import GPTTranslator\n\tfrom skllm.preprocessing.gpt_vectorizer import GPTVectorizer\n"]}
{"filename": "skllm/preprocessing/gpt_translator.py", "chunked_list": ["from typing import Any, List, Optional, Union\n\timport numpy as np\n\tfrom numpy import ndarray\n\tfrom pandas import Series\n\tfrom skllm.openai.base_gpt import BaseZeroShotGPTTransformer as _BaseGPT\n\tfrom skllm.prompts.builders import build_translation_prompt\n\tclass GPTTranslator(_BaseGPT):\n\t    \"\"\"A text translator.\"\"\"\n\t    system_msg = \"You are a text translator.\"\n\t    default_output = \"Translation is unavailable.\"\n", "    def __init__(\n\t        self,\n\t        openai_key: Optional[str] = None,\n\t        openai_org: Optional[str] = None,\n\t        openai_model: str = \"gpt-3.5-turbo\",\n\t        output_language: str = \"English\",\n\t    ) -> None:\n\t        self._set_keys(openai_key, openai_org)\n\t        self.openai_model = openai_model\n\t        self.output_language = output_language\n", "    def _get_prompt(self, X: str) -> str:\n\t        \"\"\"Generates the prompt for the given input.\n\t        Parameters\n\t        ----------\n\t        X : str\n\t            sample to translate\n\t        Returns\n\t        -------\n\t        str\n\t            translated sample\n", "        \"\"\"\n\t        return build_translation_prompt(X, self.output_language)\n\t    def transform(self, X: Union[ndarray, Series, List[str]], **kwargs: Any) -> ndarray:\n\t        y = super().transform(X, **kwargs)\n\t        y = np.asarray(\n\t            [i.replace(\"[Translated text:]\", \"\").replace(\"```\", \"\").strip() for i in y],\n\t            dtype=object,\n\t        )\n\t        return y\n"]}
{"filename": "skllm/prompts/templates.py", "chunked_list": ["ZERO_SHOT_CLF_PROMPT_TEMPLATE = \"\"\"\n\tYou will be provided with the following information:\n\t1. An arbitrary text sample. The sample is delimited with triple backticks.\n\t2. List of categories the text sample can be assigned to. The list is delimited with square brackets. The categories in the list are enclosed in the single quotes and comma separated.\n\tPerform the following tasks:\n\t1. Identify to which category the provided text belongs to with the highest probability.\n\t2. Assign the provided text to that category.\n\t3. Provide your response in a JSON format containing a single key `label` and a value corresponding to the assigned category. Do not provide any additional information except the JSON.\n\tList of categories: {labels}\n\tText sample: ```{x}```\n", "Your JSON response:\n\t\"\"\"\n\tFEW_SHOT_CLF_PROMPT_TEMPLATE = \"\"\"\n\tYou will be provided with the following information:\n\t1. An arbitrary text sample. The sample is delimited with triple backticks.\n\t2. List of categories the text sample can be assigned to. The list is delimited with square brackets. The categories in the list are enclosed in the single quotes and comma separated.\n\t3. Examples of text samples and their assigned categories. The examples are delimited with triple backticks. The assigned categories are enclosed in a list-like structure. These examples are to be used as training data.\n\tPerform the following tasks:\n\t1. Identify to which category the provided text belongs to with the highest probability.\n\t2. Assign the provided text to that category.\n", "3. Provide your response in a JSON format containing a single key `label` and a value corresponding to the assigned category. Do not provide any additional information except the JSON.\n\tList of categories: {labels}\n\tTraining data:\n\t{training_data}\n\tText sample: ```{x}```\n\tYour JSON response:\n\t\"\"\"\n\tZERO_SHOT_MLCLF_PROMPT_TEMPLATE = \"\"\"\n\tYou will be provided with the following information:\n\t1. An arbitrary text sample. The sample is delimited with triple backticks.\n", "2. List of categories the text sample can be assigned to. The list is delimited with square brackets. The categories in the list are enclosed in the single quotes and comma separated. The text sample belongs to at least one category but cannot exceed {max_cats}.\n\tPerform the following tasks:\n\t1. Identify to which categories the provided text belongs to with the highest probability.\n\t2. Assign the text sample to at least 1 but up to {max_cats} categories based on the probabilities.\n\t3. Provide your response in a JSON format containing a single key `label` and a value corresponding to the array of assigned categories. Do not provide any additional information except the JSON.\n\tList of categories: {labels}\n\tText sample: ```{x}```\n\tYour JSON response:\n\t\"\"\"\n\tSUMMARY_PROMPT_TEMPLATE = \"\"\"\n", "Your task is to generate a summary of the text sample.\n\tSummarize the text sample provided below, delimited by triple backticks, in at most {max_words} words.\n\tText sample: ```{x}```\n\tSummarized text:\n\t\"\"\"\n\tFOCUSED_SUMMARY_PROMPT_TEMPLATE = \"\"\"\n\tAs an input you will receive:\n\t1. A focus parameter delimited with square brackets.\n\t2. A single text sample delimited with triple backticks.\n\tPerform the following actions:\n", "1. Determine whether there is something in the text that matches focus. Do not output anything.\n\t2. Summarise the text in at most {max_words} words.\n\t3. If possible, make the summarisation focused on the concept provided in the focus parameter. Otherwise, provide a general summarisation. Do not state that general summary is provided.\n\t4. Do not output anything except of the summary. Do not output any text that was not present in the original text.\n\t5. If no focused summary possible, or the mentioned concept is not present in the text, output \"Mentioned concept is not present in the text.\" and the general summary. Do not state that general summary is provided.\n\tFocus: [{focus}]\n\tText sample: ```{x}```\n\tSummarized text:\n\t\"\"\"\n\tTRANSLATION_PROMPT_TEMPLATE = \"\"\"\n", "If the original text, delimited by triple backticks, is already in {output_language} language, output the original text.\n\tOtherwise, translate the original text, delimited by triple backticks, to {output_language} language, and output the translated text only. Do not output any additional information except the translated text.\n\tOriginal text: ```{x}```\n\tOutput:\n\t\"\"\"\n"]}
{"filename": "skllm/prompts/builders.py", "chunked_list": ["from typing import Union\n\tfrom skllm.prompts.templates import (\n\t    FEW_SHOT_CLF_PROMPT_TEMPLATE,\n\t    FOCUSED_SUMMARY_PROMPT_TEMPLATE,\n\t    SUMMARY_PROMPT_TEMPLATE,\n\t    TRANSLATION_PROMPT_TEMPLATE,\n\t    ZERO_SHOT_CLF_PROMPT_TEMPLATE,\n\t    ZERO_SHOT_MLCLF_PROMPT_TEMPLATE,\n\t)\n\t# TODO add validators\n", "def build_zero_shot_prompt_slc(\n\t    x: str, labels: str, template: str = ZERO_SHOT_CLF_PROMPT_TEMPLATE\n\t) -> str:\n\t    \"\"\"Builds a prompt for zero-shot single-label classification.\n\t    Parameters\n\t    ----------\n\t    x : str\n\t        sample to classify\n\t    labels : str\n\t        candidate labels in a list-like representation\n", "    template : str\n\t        prompt template to use, must contain placeholders for all variables, by default ZERO_SHOT_CLF_PROMPT_TEMPLATE\n\t    Returns\n\t    -------\n\t    str\n\t        prepared prompt\n\t    \"\"\"\n\t    return template.format(x=x, labels=labels)\n\tdef build_few_shot_prompt_slc(\n\t    x: str,\n", "    labels: str,\n\t    training_data: str,\n\t    template: str = FEW_SHOT_CLF_PROMPT_TEMPLATE,\n\t) -> str:\n\t    \"\"\"Builds a prompt for zero-shot single-label classification.\n\t    Parameters\n\t    ----------\n\t    x : str\n\t        sample to classify\n\t    labels : str\n", "        candidate labels in a list-like representation\n\t    training_data : str\n\t        training data to be used for few-shot learning\n\t    template : str\n\t        prompt template to use, must contain placeholders for all variables, by default ZERO_SHOT_CLF_PROMPT_TEMPLATE\n\t    Returns\n\t    -------\n\t    str\n\t        prepared prompt\n\t    \"\"\"\n", "    return template.format(x=x, labels=labels, training_data=training_data)\n\tdef build_zero_shot_prompt_mlc(\n\t    x: str,\n\t    labels: str,\n\t    max_cats: Union[int, str],\n\t    template: str = ZERO_SHOT_MLCLF_PROMPT_TEMPLATE,\n\t) -> str:\n\t    \"\"\"Builds a prompt for zero-shot multi-label classification.\n\t    Parameters\n\t    ----------\n", "    x : str\n\t        sample to classify\n\t    labels : str\n\t        candidate labels in a list-like representation\n\t    max_cats : Union[int,str]\n\t        maximum number of categories to assign\n\t    template : str\n\t        prompt template to use, must contain placeholders for all variables, by default ZERO_SHOT_MLCLF_PROMPT_TEMPLATE\n\t    Returns\n\t    -------\n", "    str\n\t        prepared prompt\n\t    \"\"\"\n\t    return template.format(x=x, labels=labels, max_cats=max_cats)\n\tdef build_summary_prompt(\n\t    x: str, max_words: Union[int, str], template: str = SUMMARY_PROMPT_TEMPLATE\n\t) -> str:\n\t    \"\"\"Builds a prompt for text summarization.\n\t    Parameters\n\t    ----------\n", "    x : str\n\t        sample to summarize\n\t    max_words : Union[int,str]\n\t        maximum number of words to use in the summary\n\t    template : str\n\t        prompt template to use, must contain placeholders for all variables, by default SUMMARY_PROMPT_TEMPLATE\n\t    Returns\n\t    -------\n\t    str\n\t        prepared prompt\n", "    \"\"\"\n\t    return template.format(x=x, max_words=max_words)\n\tdef build_focused_summary_prompt(\n\t    x: str,\n\t    max_words: Union[int, str],\n\t    focus: Union[int, str],\n\t    template: str = FOCUSED_SUMMARY_PROMPT_TEMPLATE,\n\t) -> str:\n\t    \"\"\"Builds a prompt for focused text summarization.\n\t    Parameters\n", "    ----------\n\t    x : str\n\t        sample to summarize\n\t    max_words : Union[int,str]\n\t        maximum number of words to use in the summary\n\t    focus : Union[int,str]\n\t        the topic(s) to focus on\n\t    template : str\n\t        prompt template to use, must contain placeholders for all variables, by default FOCUSED_SUMMARY_PROMPT_TEMPLATE\n\t    Returns\n", "    -------\n\t    str\n\t        prepared prompt\n\t    \"\"\"\n\t    return template.format(x=x, max_words=max_words, focus=focus)\n\tdef build_translation_prompt(\n\t    x: str, output_language: str, template: str = TRANSLATION_PROMPT_TEMPLATE\n\t) -> str:\n\t    \"\"\"Builds a prompt for text translation.\n\t    Parameters\n", "    ----------\n\t    x : str\n\t        sample to translate\n\t    output_language : str\n\t        language to translate to\n\t    template : str\n\t        prompt template to use, must contain placeholders for all variables, by default TRANSLATION_PROMPT_TEMPLATE\n\t    Returns\n\t    -------\n\t    str\n", "        prepared prompt\n\t    \"\"\"\n\t    return template.format(x=x, output_language=output_language)\n"]}
{"filename": "skllm/models/_base.py", "chunked_list": ["import random\n\tfrom abc import ABC, abstractmethod\n\tfrom collections import Counter\n\tfrom typing import Any, List, Optional, Union\n\timport numpy as np\n\timport pandas as pd\n\tfrom sklearn.base import BaseEstimator, ClassifierMixin\n\tfrom tqdm import tqdm\n\tfrom skllm.completions import get_chat_completion\n\tfrom skllm.openai.chatgpt import construct_message\n", "from skllm.openai.mixin import OpenAIMixin as _OAIMixin\n\tfrom skllm.utils import extract_json_key\n\tfrom skllm.utils import to_numpy as _to_numpy\n\tclass BaseClassifier(ABC, BaseEstimator, ClassifierMixin):\n\t    default_label: Optional[str] = \"Random\"\n\t    def _to_np(self, X):\n\t        \"\"\"Converts X to a numpy array.\n\t        Parameters\n\t        ----------\n\t        X : Any\n", "            The input data to convert to a numpy array.\n\t        Returns\n\t        -------\n\t        np.ndarray\n\t            The input data as a numpy array.\n\t        \"\"\"\n\t        return _to_numpy(X)\n\t    @abstractmethod\n\t    def _predict_single(self, x: str) -> Any:\n\t        \"\"\"Predicts the class of a single input.\"\"\"\n", "        pass\n\t    def fit(\n\t        self,\n\t        X: Optional[Union[np.ndarray, pd.Series, List[str]]],\n\t        y: Union[np.ndarray, pd.Series, List[str], List[List[str]]],\n\t    ):\n\t        \"\"\"Extracts the target for each datapoint in X.\n\t        Parameters\n\t        ----------\n\t        X : Optional[Union[np.ndarray, pd.Series, List[str]]]\n", "            The input array data to fit the model to.\n\t        y : Union[np.ndarray, pd.Series, List[str], List[List[str]]]\n\t            The target array data to fit the model to.\n\t        \"\"\"\n\t        X = self._to_np(X)\n\t        self.classes_, self.probabilities_ = self._get_unique_targets(y)\n\t        return self\n\t    def predict(self, X: Union[np.ndarray, pd.Series, List[str]]):\n\t        \"\"\"Predicts the class of each input.\n\t        Parameters\n", "        ----------\n\t        X : Union[np.ndarray, pd.Series, List[str]]\n\t            The input data to predict the class of.\n\t        Returns\n\t        -------\n\t        List[str]\n\t        \"\"\"\n\t        X = self._to_np(X)\n\t        predictions = []\n\t        for i in tqdm(range(len(X))):\n", "            predictions.append(self._predict_single(X[i]))\n\t        return predictions\n\t    def _get_unique_targets(self, y: Any):\n\t        labels = self._extract_labels(y)\n\t        counts = Counter(labels)\n\t        total = sum(counts.values())\n\t        classes, probs = [], []\n\t        for l, c in counts.items():\n\t            classes.append(l)\n\t            probs.append(c / total)\n", "        return classes, probs\n\t    def _extract_labels(self, y: Any) -> List[str]:\n\t        \"\"\"Return the class labels as a list.\n\t        Parameters\n\t        ----------\n\t        y : Any\n\t        Returns\n\t        -------\n\t        List[str]\n\t        \"\"\"\n", "        if isinstance(y, (pd.Series, np.ndarray)):\n\t            labels = y.tolist()\n\t        else:\n\t            labels = y\n\t        return labels\n\t    def _get_default_label(self):\n\t        \"\"\"Returns the default label based on the default_label argument.\"\"\"\n\t        if self.default_label == \"Random\":\n\t            return random.choices(self.classes_, self.probabilities_)[0]\n\t        else:\n", "            return self.default_label\n\tclass _BaseZeroShotGPTClassifier(BaseClassifier, _OAIMixin):\n\t    \"\"\"Base class for zero-shot classifiers.\n\t    Parameters\n\t    ----------\n\t    openai_key : Optional[str] , default : None\n\t        Your OpenAI API key. If None, the key will be read from the SKLLM_CONFIG_OPENAI_KEY environment variable.\n\t    openai_org : Optional[str] , default : None\n\t        Your OpenAI organization. If None, the organization will be read from the SKLLM_CONFIG_OPENAI_ORG\n\t         environment variable.\n", "    openai_model : str , default : \"gpt-3.5-turbo\"\n\t        The OpenAI model to use. See https://beta.openai.com/docs/api-reference/available-models for a list of\n\t        available models.\n\t    default_label : Optional[Union[List[str], str]] , default : 'Random'\n\t        The default label to use if the LLM could not generate a response for a sample. If set to 'Random' a random\n\t        label will be chosen based on probabilities from the training set.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        openai_key: Optional[str] = None,\n", "        openai_org: Optional[str] = None,\n\t        openai_model: str = \"gpt-3.5-turbo\",\n\t        default_label: Optional[Union[List[str], str]] = \"Random\",\n\t    ):\n\t        self._set_keys(openai_key, openai_org)\n\t        self.openai_model = openai_model\n\t        self.default_label = default_label\n\t    @abstractmethod\n\t    def _get_prompt(self, x: str) -> str:\n\t        \"\"\"Generates a prompt for the given input.\"\"\"\n", "        pass\n\t    def _get_chat_completion(self, x):\n\t        prompt = self._get_prompt(x)\n\t        msgs = []\n\t        msgs.append(construct_message(\"system\", \"You are a text classification model.\"))\n\t        msgs.append(construct_message(\"user\", prompt))\n\t        completion = get_chat_completion(\n\t            msgs, self._get_openai_key(), self._get_openai_org(), self.openai_model\n\t        )\n\t        return completion\n", "    def _predict_single(self, x):\n\t        \"\"\"Predicts the labels for a single sample.\n\t        Should work for all (single label) GPT based classifiers.\n\t        \"\"\"\n\t        completion = self._get_chat_completion(x)\n\t        try:\n\t            label = str(\n\t                extract_json_key(\n\t                    completion[\"choices\"][0][\"message\"][\"content\"], \"label\"\n\t                )\n", "            )\n\t        except Exception as e:\n\t            print(completion)\n\t            print(f\"Could not extract the label from the completion: {str(e)}\")\n\t            label = \"\"\n\t        if label not in self.classes_:\n\t            label = label.replace(\"'\", \"\").replace('\"', \"\")\n\t            if label not in self.classes_:  # try again\n\t                label = self._get_default_label()\n\t        return label\n", "class _BasePaLMClassifier(BaseClassifier):\n\t    def __init__(self, model: str, default_label: Optional[str] = \"Random\"):\n\t        self.model = model\n\t        self.default_label = default_label\n"]}
{"filename": "skllm/models/gpt/gpt_zero_shot_clf.py", "chunked_list": ["import random\n\tfrom typing import List, Literal, Optional, Union\n\timport numpy as np\n\timport pandas as pd\n\tfrom skllm.models._base import _BaseZeroShotGPTClassifier\n\tfrom skllm.prompts.builders import (\n\t    build_zero_shot_prompt_mlc,\n\t    build_zero_shot_prompt_slc,\n\t)\n\tfrom skllm.utils import extract_json_key\n", "class ZeroShotGPTClassifier(_BaseZeroShotGPTClassifier):\n\t    \"\"\"Zero-shot classifier for multiclass classification.\n\t    Parameters\n\t    ----------\n\t    openai_key : Optional[str] , default : None\n\t        Your OpenAI API key. If None, the key will be read from the SKLLM_CONFIG_OPENAI_KEY environment variable.\n\t    openai_org : Optional[str] , default : None\n\t        Your OpenAI organization. If None, the organization will be read from the SKLLM_CONFIG_OPENAI_ORG\n\t         environment variable.\n\t    openai_model : str , default : \"gpt-3.5-turbo\"\n", "        The OpenAI model to use. See https://beta.openai.com/docs/api-reference/available-models for a list of\n\t        available models.\n\t    default_label : Optional[str] , default : 'Random'\n\t        The default label to use if the LLM could not generate a response for a sample. If set to 'Random' a random\n\t        label will be chosen based on probabilities from the training set.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        openai_key: Optional[str] = None,\n\t        openai_org: Optional[str] = None,\n", "        openai_model: str = \"gpt-3.5-turbo\",\n\t        default_label: Optional[str] = \"Random\",\n\t    ):\n\t        super().__init__(openai_key, openai_org, openai_model, default_label)\n\t    def _get_prompt(self, x) -> str:\n\t        return build_zero_shot_prompt_slc(x, repr(self.classes_))\n\t    def fit(\n\t        self,\n\t        X: Optional[Union[np.ndarray, pd.Series, List[str]]],\n\t        y: Union[np.ndarray, pd.Series, List[str]],\n", "    ):\n\t        y = self._to_np(y)\n\t        return super().fit(X, y)\n\tclass MultiLabelZeroShotGPTClassifier(_BaseZeroShotGPTClassifier):\n\t    \"\"\"Zero-shot classifier for multilabel classification.\n\t    Parameters\n\t    ----------\n\t    openai_key : Optional[str] , default : None\n\t        Your OpenAI API key. If None, the key will be read from the SKLLM_CONFIG_OPENAI_KEY environment variable.\n\t    openai_org : Optional[str] , default : None\n", "        Your OpenAI organization. If None, the organization will be read from the SKLLM_CONFIG_OPENAI_ORG\n\t         environment variable.\n\t    openai_model : str , default : \"gpt-3.5-turbo\"\n\t        The OpenAI model to use. See https://beta.openai.com/docs/api-reference/available-models for a list of\n\t        available models.\n\t    default_label : Optional[Union[List[str], Literal['Random']] , default : 'Random'\n\t        The default label to use if the LLM could not generate a response for a sample. If set to 'Random' a random\n\t        label will be chosen based on probabilities from the training set.\n\t    max_labels : int , default : 3\n\t        The maximum number of labels to predict for each sample.\n", "    \"\"\"\n\t    def __init__(\n\t        self,\n\t        openai_key: Optional[str] = None,\n\t        openai_org: Optional[str] = None,\n\t        openai_model: str = \"gpt-3.5-turbo\",\n\t        default_label: Optional[Union[List[str], Literal[\"Random\"]]] = \"Random\",\n\t        max_labels: int = 3,\n\t    ):\n\t        super().__init__(openai_key, openai_org, openai_model, default_label)\n", "        if max_labels < 2:\n\t            raise ValueError(\"max_labels should be at least 2\")\n\t        if isinstance(default_label, str) and default_label != \"Random\":\n\t            raise ValueError(\"default_label should be a list of strings or 'Random'\")\n\t        self.max_labels = max_labels\n\t    def _extract_labels(self, y) -> List[str]:\n\t        \"\"\"Extracts the labels into a list.\n\t        Parameters\n\t        ----------\n\t        y : Any\n", "        Returns\n\t        -------\n\t        List[str]\n\t        \"\"\"\n\t        labels = []\n\t        for l in y:\n\t            for j in l:\n\t                labels.append(j)\n\t        return labels\n\t    def _get_prompt(self, x) -> str:\n", "        return build_zero_shot_prompt_mlc(x, repr(self.classes_), self.max_labels)\n\t    def _get_default_label(self):\n\t        \"\"\"Returns the default label based on the default_label argument.\"\"\"\n\t        result = []\n\t        if isinstance(self.default_label, str) and self.default_label == \"Random\":\n\t            for cls, probability in zip(self.classes_, self.probabilities_):\n\t                coin_flip = random.choices([0, 1], [1 - probability, probability])[0]\n\t                if coin_flip == 1:\n\t                    result.append(cls)\n\t        else:\n", "            result = self.default_label\n\t        return result\n\t    def _predict_single(self, x):\n\t        \"\"\"Predicts the labels for a single sample.\"\"\"\n\t        completion = self._get_chat_completion(x)\n\t        try:\n\t            labels = extract_json_key(\n\t                completion[\"choices\"][0][\"message\"][\"content\"], \"label\"\n\t            )\n\t            if not isinstance(labels, list):\n", "                labels = labels.split(\",\")\n\t                labels = [l.strip() for l in labels]\n\t        except Exception as e:\n\t            print(completion)\n\t            print(f\"Could not extract the label from the completion: {str(e)}\")\n\t            labels = []\n\t        labels = list(filter(lambda l: l in self.classes_, labels))\n\t        if len(labels) == 0:\n\t            labels = self._get_default_label()\n\t        if labels is not None and len(labels) > self.max_labels:\n", "            labels = labels[: self.max_labels - 1]\n\t        return labels\n\t    def fit(\n\t        self,\n\t        X: Optional[Union[np.ndarray, pd.Series, List[str]]],\n\t        y: List[List[str]],\n\t    ):\n\t        \"\"\"Calls the parent fit method on input data.\n\t        Parameters\n\t        ----------\n", "        X : Optional[Union[np.ndarray, pd.Series, List[str]]]\n\t            Input array data\n\t        y : List[List[str]]\n\t            The labels.\n\t        \"\"\"\n\t        return super().fit(X, y)\n"]}
{"filename": "skllm/models/gpt/gpt_dyn_few_shot_clf.py", "chunked_list": ["from __future__ import annotations\n\timport numpy as np\n\timport pandas as pd\n\tfrom skllm.memory import AnnoyMemoryIndex\n\tfrom skllm.models._base import _BaseZeroShotGPTClassifier\n\tfrom skllm.preprocessing import GPTVectorizer\n\tfrom skllm.prompts.builders import build_few_shot_prompt_slc\n\tfrom skllm.utils import to_numpy\n\t_TRAINING_SAMPLE_PROMPT_TEMPLATE = \"\"\"\n\tSample input:\n", "```{x}```\n\tSample target: {label}\n\t\"\"\"\n\tclass DynamicFewShotGPTClassifier(_BaseZeroShotGPTClassifier):\n\t    \"\"\"Dynamic few-shot single-label classifier.\n\t    Parameters\n\t    ----------\n\t    n_examples : int, optional\n\t        number of examples per class, by default 3\n\t    openai_key : Optional[str] , default : None\n", "        Your OpenAI API key. If None, the key will be read from the SKLLM_CONFIG_OPENAI_KEY environment variable.\n\t    openai_org : Optional[str] , default : None\n\t        Your OpenAI organization. If None, the organization will be read from the SKLLM_CONFIG_OPENAI_ORG\n\t        environment variable.\n\t    openai_model : str , default : \"gpt-3.5-turbo\"\n\t        The OpenAI model to use. See https://beta.openai.com/docs/api-reference/available-models for a list of\n\t        available models.\n\t    default_label : Optional[Union[List[str], str]] , default : 'Random'\n\t        The default label to use if the LLM could not generate a response for a sample. If set to 'Random' a random\n\t        label will be chosen based on probabilities from the training set.\n", "    \"\"\"\n\t    def __init__(\n\t        self,\n\t        n_examples: int = 3,\n\t        openai_key: str | None = None,\n\t        openai_org: str | None = None,\n\t        openai_model: str = \"gpt-3.5-turbo\",\n\t        default_label: str | None = \"Random\",\n\t    ):\n\t        super().__init__(openai_key, openai_org, openai_model, default_label)\n", "        self.n_examples = n_examples\n\t    def fit(\n\t        self,\n\t        X: np.ndarray | pd.Series | list[str],\n\t        y: np.ndarray | pd.Series | list[str],\n\t    ) -> DynamicFewShotGPTClassifier:\n\t        \"\"\"Fits the model to the given data.\n\t        Parameters\n\t        ----------\n\t        X : Union[np.ndarray, pd.Series, List[str]]\n", "            training data\n\t        y : Union[np.ndarray, pd.Series, List[str]]\n\t            training labels\n\t        Returns\n\t        -------\n\t        DynamicFewShotGPTClassifier\n\t            self\n\t        \"\"\"\n\t        X = to_numpy(X)\n\t        y = to_numpy(y)\n", "        self.embedding_model_ = GPTVectorizer().fit(X)\n\t        self.classes_, self.probabilities_ = self._get_unique_targets(y)\n\t        self.data_ = {}\n\t        for cls in self.classes_:\n\t            print(f\"Building index for class `{cls}` ...\")\n\t            self.data_[cls] = {}\n\t            partition = X[y == cls]\n\t            self.data_[cls][\"partition\"] = partition\n\t            embeddings = self.embedding_model_.transform(partition)\n\t            index = AnnoyMemoryIndex(embeddings.shape[1])\n", "            for i, embedding in enumerate(embeddings):\n\t                index.add(i, embedding)\n\t            index.build()\n\t            self.data_[cls][\"index\"] = index\n\t        return self\n\t    def _get_prompt(self, x: str) -> str:\n\t        \"\"\"Generates the prompt for the given input.\n\t        Parameters\n\t        ----------\n\t        x : str\n", "            sample to classify\n\t        Returns\n\t        -------\n\t        str\n\t            final prompt\n\t        \"\"\"\n\t        embedding = self.embedding_model_.transform([x])\n\t        training_data = []\n\t        for cls in self.classes_:\n\t            index = self.data_[cls][\"index\"]\n", "            partition = self.data_[cls][\"partition\"]\n\t            neighbors = index.retrieve(embedding, min(self.n_examples, len(partition)))\n\t            neighbors = [partition[i] for i in neighbors[0]]\n\t            training_data.extend(\n\t                [\n\t                    _TRAINING_SAMPLE_PROMPT_TEMPLATE.format(x=neighbor, label=cls)\n\t                    for neighbor in neighbors\n\t                ]\n\t            )\n\t        training_data_str = \"\\n\".join(training_data)\n", "        return build_few_shot_prompt_slc(\n\t            x=x, training_data=training_data_str, labels=repr(self.classes_)\n\t        )\n"]}
{"filename": "skllm/models/gpt/__init__.py", "chunked_list": ["from skllm.models.gpt.gpt_dyn_few_shot_clf import DynamicFewShotGPTClassifier\n\tfrom skllm.models.gpt.gpt_few_shot_clf import FewShotGPTClassifier\n\tfrom skllm.models.gpt.gpt_zero_shot_clf import (\n\t    ZeroShotGPTClassifier,\n\t    MultiLabelZeroShotGPTClassifier,\n\t)\n"]}
{"filename": "skllm/models/gpt/gpt_few_shot_clf.py", "chunked_list": ["from typing import List, Union\n\timport numpy as np\n\timport pandas as pd\n\tfrom skllm.models._base import _BaseZeroShotGPTClassifier\n\tfrom skllm.prompts.builders import build_few_shot_prompt_slc\n\tfrom skllm.utils import to_numpy as _to_numpy\n\t_TRAINING_SAMPLE_PROMPT_TEMPLATE = \"\"\"\n\tSample input:\n\t```{x}```\n\tSample target: {label}\n", "\"\"\"\n\tclass FewShotGPTClassifier(_BaseZeroShotGPTClassifier):\n\t    \"\"\"Few-shot single-label classifier.\"\"\"\n\t    def fit(\n\t        self,\n\t        X: Union[np.ndarray, pd.Series, List[str]],\n\t        y: Union[np.ndarray, pd.Series, List[str]],\n\t    ):\n\t        \"\"\"Fits the model to the given data.\n\t        Parameters\n", "        ----------\n\t        X : Union[np.ndarray, pd.Series, List[str]]\n\t            training data\n\t        y : Union[np.ndarray, pd.Series, List[str]]\n\t            training labels\n\t        Returns\n\t        -------\n\t        FewShotGPTClassifier\n\t            self\n\t        \"\"\"\n", "        if not len(X) == len(y):\n\t            raise ValueError(\"X and y must have the same length.\")\n\t        X = _to_numpy(X)\n\t        y = _to_numpy(y)\n\t        self.training_data_ = (X, y)\n\t        self.classes_, self.probabilities_ = self._get_unique_targets(y)\n\t        return self\n\t    def _get_prompt(self, x: str) -> str:\n\t        \"\"\"Generates the prompt for the given input.\n\t        Parameters\n", "        ----------\n\t        x : str\n\t            sample to classify\n\t        Returns\n\t        -------\n\t        str\n\t            final prompt\n\t        \"\"\"\n\t        training_data = []\n\t        for xt, yt in zip(*self.training_data_):\n", "            training_data.append(\n\t                _TRAINING_SAMPLE_PROMPT_TEMPLATE.format(x=xt, label=yt)\n\t            )\n\t        training_data_str = \"\\n\".join(training_data)\n\t        return build_few_shot_prompt_slc(\n\t            x=x, training_data=training_data_str, labels=repr(self.classes_)\n\t        )\n"]}
{"filename": "skllm/models/palm/palm_clf.py", "chunked_list": ["from typing import List, Optional, Union\n\timport numpy as np\n\timport pandas as pd\n\tfrom skllm.google.completions import get_completion, get_completion_chat_mode\n\tfrom skllm.google.tuning import tune\n\tfrom skllm.models._base import _BasePaLMClassifier\n\tfrom skllm.prompts.builders import build_zero_shot_prompt_slc\n\tfrom skllm.utils import extract_json_key\n\t_SHORT_PROMPT = \"\"\"\n\tClassify the following text into one of the following classes: {labels}.\n", "Text: ```{x}```\n\t\"\"\"\n\tclass ZeroShotPaLMClassifier(_BasePaLMClassifier):\n\t    \"\"\"Google PaLM zero-shot classifier for single-label classification.\n\t    Parameters\n\t    ----------\n\t    model : str\n\t        The name of the model to use. Must be one of the models supported by google. Defaults to \"text-bison@001\".\n\t    default_label : str, optional\n\t        The default label to use if the model does not return a valid label. Defaults to \"Random\".\n", "    \"\"\"\n\t    def __init__(\n\t        self, model: str = \"text-bison@001\", default_label: Optional[str] = \"Random\"\n\t    ) -> None:\n\t        super().__init__(model, default_label)\n\t    def _predict_single(self, x: str) -> str:\n\t        \"\"\"Predicts the class of a single input.\"\"\"\n\t        input_ = self._get_prompt(x)\n\t        if self.model.startswith(\"chat-\"):\n\t            context = \"You are a text classification model.\"\n", "            completion = get_completion_chat_mode(self.model, context, input_)\n\t        else:\n\t            completion = get_completion(self.model, input_)\n\t        try:\n\t            label = str(extract_json_key(str(completion), \"label\"))\n\t        except Exception as e:\n\t            print(completion)\n\t            print(f\"Could not extract the label from the completion: {str(e)}\")\n\t            label = \"\"\n\t        if label not in self.classes_:\n", "            label = label.replace(\"'\", \"\").replace('\"', \"\")\n\t            if label not in self.classes_:  # try again\n\t                label = self._get_default_label()\n\t        return label\n\t    def _get_prompt(self, x) -> str:\n\t        return build_zero_shot_prompt_slc(x, repr(self.classes_))\n\tclass PaLMClassifier(_BasePaLMClassifier):\n\t    \"\"\"Tunable Google PaLM classifier for single-label classification.\n\t    Parameters\n\t    ----------\n", "    model : str\n\t        The name of the model to use. Must be one of the models supported by google. Defaults to \"text-bison@001\".\n\t    n_update_steps : int, optional\n\t        The number of update steps to use when tuning the model. Defaults to 1.\n\t    default_label : str, optional\n\t        The default label to use if the model does not return a valid label. Defaults to \"Random\".\n\t    \"\"\"\n\t    _supported_models = [\"text-bison@001\"]\n\t    def __init__(\n\t        self,\n", "        model=\"text-bison@001\",\n\t        n_update_steps: int = 1,\n\t        default_label: Optional[str] = \"Random\",\n\t    ) -> None:\n\t        super().__init__(model, default_label)\n\t        if model not in self._supported_models:\n\t            raise ValueError(\n\t                f\"Model {model} not supported. Supported models:\"\n\t                f\" {self._supported_models}\"\n\t            )\n", "        self.n_update_steps = int(n_update_steps)\n\t        self.default_label = default_label\n\t    def fit(\n\t        self,\n\t        X: Union[np.ndarray, pd.Series, List[str]],\n\t        y: Union[np.ndarray, pd.Series, List[str]],\n\t    ):\n\t        \"\"\"Fits the model to the data.\n\t        Parameters\n\t        ----------\n", "        X : Union[np.ndarray, pd.Series, List[str]]\n\t            Inputs\n\t        y : Union[np.ndarray, pd.Series, List[str]]\n\t            Targets\n\t        \"\"\"\n\t        X = self._to_np(X)\n\t        y = self._to_np(y)\n\t        if len(X) != len(y):\n\t            raise ValueError(\n\t                f\"X and y must be the same length. Got {len(X)} and {len(y)}\"\n", "            )\n\t        if len(X) < 10:\n\t            raise ValueError(\n\t                f\"The dataset must have at least 10 datapoints. Got {len(X)}.\"\n\t            )\n\t        self.classes_, self.probabilities_ = self._get_unique_targets(y)\n\t        inputs = np.asarray(\n\t            [\n\t                build_zero_shot_prompt_slc(x, repr(self.classes_), _SHORT_PROMPT)\n\t                for x in X\n", "            ],\n\t            dtype=str,\n\t        )\n\t        df = pd.DataFrame({\"input_text\": inputs, \"output_text\": y})\n\t        job = tune(self.model, df, self.n_update_steps)._job\n\t        tuned_model = job.result()\n\t        self.tuned_model_ = tuned_model._model_resource_name\n\t        return self\n\t    def _predict_single(self, x: str) -> str:\n\t        input_ = self._get_prompt(x)\n", "        label = str(get_completion(self.tuned_model_, input_))\n\t        if label not in self.classes_:\n\t            label = label.replace(\"'\", \"\").replace('\"', \"\")\n\t            if label not in self.classes_:  # try again\n\t                label = self._get_default_label()\n\t        return label\n\t    def _get_prompt(self, x: str) -> str:\n\t        return build_zero_shot_prompt_slc(x, repr(self.classes_), _SHORT_PROMPT)\n"]}
{"filename": "skllm/models/palm/__init__.py", "chunked_list": ["from skllm.models.palm.palm_clf import ZeroShotPaLMClassifier, PaLMClassifier\n\tfrom skllm.models.palm.palm_est import PaLM\n"]}
{"filename": "skllm/models/palm/palm_est.py", "chunked_list": ["from typing import List, Union\n\timport numpy as np\n\timport pandas as pd\n\tfrom skllm.google.completions import get_completion\n\tfrom skllm.google.tuning import tune\n\tfrom skllm.models._base import _BasePaLMClassifier\n\t# This is not a classifier, but inherits from the base classifier anyway for simplicity reasons.\n\tclass PaLM(_BasePaLMClassifier):\n\t    \"\"\"Google PaLM for single-label classification.\n\t    Parameters\n", "    ----------\n\t    model : str\n\t        The name of the model to use. Must be one of the models supported by google. Defaults to \"text-bison@001\".\n\t    n_update_steps : int, optional\n\t        The number of update steps to perform during tuning. Defaults to 1.\n\t    \"\"\"\n\t    _supported_models = [\"text-bison@001\"]\n\t    def __init__(self, model=\"text-bison@001\", n_update_steps: int = 1) -> None:\n\t        super().__init__(model, default_label=None)\n\t        if model not in self._supported_models:\n", "            raise ValueError(\n\t                f\"Model {model} not supported. Supported models:\"\n\t                f\" {self._supported_models}\"\n\t            )\n\t        self.n_update_steps = int(n_update_steps)\n\t    def fit(\n\t        self,\n\t        X: Union[np.ndarray, pd.Series, List[str]],\n\t        y: Union[np.ndarray, pd.Series, List[str]],\n\t    ):\n", "        \"\"\"Fits the model to the data.\n\t        Parameters\n\t        ----------\n\t        X : Union[np.ndarray, pd.Series, List[str]]\n\t            Inputs\n\t        y : Union[np.ndarray, pd.Series, List[str]]\n\t            Targets\n\t        \"\"\"\n\t        X = self._to_np(X)\n\t        y = self._to_np(y)\n", "        if len(X) != len(y):\n\t            raise ValueError(\n\t                f\"X and y must be the same length. Got {len(X)} and {len(y)}\"\n\t            )\n\t        if len(X) < 10:\n\t            raise ValueError(\n\t                f\"The dataset must have at least 10 datapoints. Got {len(X)}.\"\n\t            )\n\t        df = pd.DataFrame({\"input_text\": X, \"output_text\": y})\n\t        job = tune(self.model, df, self.n_update_steps)._job\n", "        tuned_model = job.result()\n\t        self.tuned_model_ = tuned_model._model_resource_name\n\t        return self\n\t    def _predict_single(self, x: str) -> str:\n\t        label = str(get_completion(self.tuned_model_, x))\n\t        return label\n\t    def _get_prompt(self, x: str) -> str:\n\t        \"\"\"For compatibility with the parent class.\"\"\"\n\t        return x\n"]}
{"filename": "skllm/datasets/multi_label.py", "chunked_list": ["def get_multilabel_classification_dataset():\n\t    X = [\n\t    \"The product was of excellent quality, and the packaging was also very good. Highly recommend!\",\n\t    \"The delivery was super fast, but the product did not match the information provided on the website.\",\n\t    \"Great variety of products, but the customer support was quite unresponsive.\",\n\t    \"Affordable prices and an easy-to-use website. A great shopping experience overall.\",\n\t    \"The delivery was delayed, and the packaging was damaged. Not a good experience.\",\n\t    \"Excellent customer support, but the return policy is quite complicated.\",\n\t    \"The product was not as described. However, the return process was easy and quick.\",\n\t    \"Great service and fast delivery. The product was also of high quality.\",\n", "    \"The prices are a bit high. However, the product quality and user experience are worth it.\",\n\t    \"The website provides detailed information about products. The delivery was also very fast.\"\n\t    ]\n\t    y = [\n\t        [\"Quality\", \"Packaging\"],\n\t        [\"Delivery\", \"Product Information\"],\n\t        [\"Product Variety\", \"Customer Support\"],\n\t        [\"Price\", \"User Experience\"],\n\t        [\"Delivery\", \"Packaging\"],\n\t        [\"Customer Support\", \"Return Policy\"],\n", "        [\"Product Information\", \"Return Policy\"],\n\t        [\"Service\", \"Delivery\", \"Quality\"],\n\t        [\"Price\", \"Quality\", \"User Experience\"],\n\t        [\"Product Information\", \"Delivery\"],\n\t    ]\n\t    return X, y"]}
{"filename": "skllm/datasets/__init__.py", "chunked_list": ["from skllm.datasets.multi_class import get_classification_dataset\n\tfrom skllm.datasets.multi_label import get_multilabel_classification_dataset\n\tfrom skllm.datasets.summarization import get_summarization_dataset\n\tfrom skllm.datasets.translation import get_translation_dataset\n"]}
{"filename": "skllm/datasets/summarization.py", "chunked_list": ["def get_summarization_dataset():\n\t    X = [\n\t    r\"The AI research company, OpenAI, has launched a new language model called GPT-4. This model is the latest in a series of transformer-based AI systems designed to perform complex tasks, such as generating human-like text, translating languages, and answering questions. According to OpenAI, GPT-4 is even more powerful and versatile than its predecessors.\",\n\t    r\"John went to the grocery store in the morning to prepare for a small get-together at his house. He bought fresh apples, juicy oranges, and a bottle of milk. Once back home, he used the apples and oranges to make a delicious fruit salad, which he served to his guests in the evening.\",\n\t    r\"The first Mars rover, named Sojourner, was launched by NASA in 1996. The mission was a part of the Mars Pathfinder project and was a major success. The data Sojourner provided about Martian terrain and atmosphere greatly contributed to our understanding of the Red Planet.\",\n\t    r\"A new study suggests that regular exercise can improve memory and cognitive function in older adults. The study, which monitored the health and habits of 500 older adults, recommends 30 minutes of moderate exercise daily for the best results.\",\n\t    r\"The Eiffel Tower, a globally recognized symbol of Paris and France, was completed in 1889 for the World's Fair. Despite its initial criticism and controversy over its unconventional design, the Eiffel Tower has become a beloved landmark and a symbol of French architectural innovation.\",\n\t    r\"Microsoft has announced a new version of its flagship operating system, Windows. The update, which will be rolled out later this year, features improved security protocols and a redesigned user interface, promising a more streamlined and safer user experience.\",\n\t    r\"The WHO declared a new public health emergency due to an outbreak of a previously unknown virus. As the number of cases grows globally, the organization urges nations to ramp up their disease surveillance and response systems.\",\n\t    r\"The 2024 Olympics have been confirmed to take place in Paris, France. This marks the third time the city will host the games, with previous occasions in 1900 and 1924. Preparations are already underway to make the event a grand spectacle.\",\n", "    r\"Apple has introduced its latest iPhone model. The new device boasts a range of features, including an improved camera system, a faster processor, and a longer battery life. It is set to hit the market later this year.\",\n\t    r\"Scientists working in the Amazon rainforest have discovered a new species of bird. The bird, characterized by its unique bright plumage, has created excitement among the global ornithological community.\"\n\t    ]\n\t    return X"]}
{"filename": "skllm/datasets/multi_class.py", "chunked_list": ["def get_classification_dataset():\n\t    X = [\n\t        r\"I was absolutely blown away by the performances in 'Summer's End'. The acting was top-notch, and the plot had me gripped from start to finish. A truly captivating cinematic experience that I would highly recommend.\",\n\t        r\"The special effects in 'Star Battles: Nebula Conflict' were out of this world. I felt like I was actually in space. The storyline was incredibly engaging and left me wanting more. Excellent film.\",\n\t        r\"'The Lost Symphony' was a masterclass in character development and storytelling. The score was hauntingly beautiful and complimented the intense, emotional scenes perfectly. Kudos to the director and cast for creating such a masterpiece.\",\n\t        r\"I was pleasantly surprised by 'Love in the Time of Cholera'. The romantic storyline was heartwarming and the characters were incredibly realistic. The cinematography was also top-notch. A must-watch for all romance lovers.\",\n\t        r\"I went into 'Marble Street' with low expectations, but I was pleasantly surprised. The suspense was well-maintained throughout, and the twist at the end was something I did not see coming. Bravo!\",\n\t        r\"'The Great Plains' is a touching portrayal of life in rural America. The performances were heartfelt and the scenery was breathtaking. I was moved to tears by the end. It's a story that will stay with me for a long time.\",\n\t        r\"The screenwriting in 'Under the Willow Tree' was superb. The dialogue felt real and the characters were well-rounded. The performances were also fantastic. I haven't enjoyed a movie this much in a while.\",\n\t        r\"'Nightshade' is a brilliant take on the superhero genre. The protagonist was relatable and the villain was genuinely scary. The action sequences were thrilling and the storyline was engaging. I can't wait for the sequel.\",\n", "        r\"The cinematography in 'Awakening' was nothing short of spectacular. The visuals alone are worth the ticket price. The storyline was unique and the performances were solid. An overall fantastic film.\",\n\t        r\"'Eternal Embers' was a cinematic delight. The storytelling was original and the performances were exceptional. The director's vision was truly brought to life on the big screen. A must-see for all movie lovers.\",\n\t        r\"I was thoroughly disappointed with 'Silver Shadows'. The plot was confusing and the performances were lackluster. I wouldn't recommend wasting your time on this one.\",\n\t        r\"'The Darkened Path' was a disaster. The storyline was unoriginal, the acting was wooden and the special effects were laughably bad. Save your money and skip this one.\",\n\t        r\"I had high hopes for 'The Final Frontier', but it failed to deliver. The plot was full of holes and the characters were poorly developed. It was a disappointing experience.\",\n\t        r\"'The Fall of the Phoenix' was a letdown. The storyline was confusing and the characters were one-dimensional. I found myself checking my watch multiple times throughout the movie.\",\n\t        r\"I regret wasting my time on 'Emerald City'. The plot was nonsensical and the performances were uninspired. It was a major disappointment.\",\n\t        r\"I found 'Hollow Echoes' to be a complete mess. The plot was non-existent, the performances were overdone, and the pacing was all over the place. Definitely not worth the hype.\",\n\t        r\"'Underneath the Stars' was a huge disappointment. The storyline was predictable and the acting was mediocre at best. I was expecting so much more.\",\n\t        r\"I was left unimpressed by 'River's Edge'. The plot was convoluted, the characters were uninteresting, and the ending was unsatisfying. It's a pass for me.\",\n", "        r\"The acting in 'Desert Mirage' was subpar, and the plot was boring. I found myself yawning multiple times throughout the movie. Save your time and skip this one.\",\n\t        r\"'Crimson Dawn' was a major letdown. The plot was cliched and the characters were flat. The special effects were also poorly executed. I wouldn't recommend it.\",\n\t        r\"'Remember the Days' was utterly forgettable. The storyline was dull, the performances were bland, and the dialogue was cringeworthy. A big disappointment.\",\n\t        r\"'The Last Frontier' was simply okay. The plot was decent and the performances were acceptable. However, it lacked a certain spark to make it truly memorable.\",\n\t        r\"'Through the Storm' was not bad, but it wasn't great either. The storyline was somewhat predictable, and the characters were somewhat stereotypical. It was an average movie at best.\",\n\t        r\"I found 'After the Rain' to be pretty average. The plot was okay and the performances were decent, but it didn't leave a lasting impression on me.\",\n\t        r\"'Beyond the Horizon' was neither good nor bad. The plot was interesting enough, but the characters were not very well developed. It was an okay watch.\",\n\t        r\"'The Silent Echo' was a mediocre movie. The storyline was passable and the performances were fair, but it didn't stand out in any way.\",\n\t        r\"I thought 'The Scent of Roses' was pretty average. The plot was somewhat engaging, and the performances were okay, but it didn't live up to my expectations.\",\n\t        r\"'Under the Same Sky' was an okay movie. The plot was decent, and the performances were fine, but it lacked depth and originality. It's not a movie I would watch again.\",\n", "        r\"'Chasing Shadows' was fairly average. The plot was not bad, and the performances were passable, but it lacked a certain spark. It was just okay.\",\n\t        r\"'Beneath the Surface' was pretty run-of-the-mill. The plot was decent, the performances were okay, but it wasn't particularly memorable. It was an okay movie.\",\n\t    ]\n\t    y = (\n\t        [\"positive\" for _ in range(10)]\n\t        + [\"negative\" for _ in range(10)]\n\t        + [\"neutral\" for _ in range(10)]\n\t    )\n\t    return X, y"]}
{"filename": "skllm/datasets/translation.py", "chunked_list": ["def get_translation_dataset():\n\t    X = [\n\t        r\"Me encanta bailar salsa y bachata. Es una forma divertida de expresarme.\",\n\t        r\"J'ai pass mes dernires vacances en Grce. Les plages taient magnifiques.\",\n\t        (\n\t            r\"Ich habe gestern ein tolles Buch gelesen. Die Geschichte war fesselnd bis\"\n\t            r\" zum Ende.\"\n\t        ),\n\t        (\n\t            r\"Gosto de cozinhar pratos tradicionais italianos. O espaguete  carbonara\"\n", "            r\"  um dos meus favoritos.\"\n\t        ),\n\t        (\n\t            r\"Mm v plnu letos v lt vyrazit na vlet do Itlie. Doufm, e navtvm\"\n\t            r\" m a Bentky.\"\n\t        ),\n\t        (\n\t            r\"Mijn favoriete hobby is fotograferen. Ik hou ervan om mooie momenten vast\"\n\t            r\" te leggen.\"\n\t        ),\n", "    ]\n\t    return X\n"]}
{"filename": "skllm/openai/base_gpt.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import Any\n\timport numpy as np\n\timport pandas as pd\n\tfrom numpy import ndarray\n\tfrom sklearn.base import BaseEstimator as _BaseEstimator\n\tfrom sklearn.base import TransformerMixin as _TransformerMixin\n\tfrom tqdm import tqdm\n\tfrom skllm.openai.chatgpt import construct_message, get_chat_completion\n\tfrom skllm.openai.mixin import OpenAIMixin as _OAIMixin\n", "from skllm.utils import to_numpy as _to_numpy\n\tclass BaseZeroShotGPTTransformer(_BaseEstimator, _TransformerMixin, _OAIMixin):\n\t    system_msg = \"You are an scikit-learn transformer.\"\n\t    default_output = \"Output is unavailable\"\n\t    def _get_chat_completion(self, X):\n\t        \"\"\"Gets the chat completion for the given input using open ai API.\n\t        Parameters\n\t        ----------\n\t        X : str\n\t            Input string\n", "        Returns\n\t        -------\n\t        str\n\t        \"\"\"\n\t        prompt = self._get_prompt(X)\n\t        msgs = []\n\t        msgs.append(construct_message(\"system\", self.system_msg))\n\t        msgs.append(construct_message(\"user\", prompt))\n\t        completion = get_chat_completion(\n\t            msgs, self._get_openai_key(), self._get_openai_org(), self.openai_model\n", "        )\n\t        try:\n\t            return completion.choices[0].message[\"content\"]\n\t        except Exception as e:\n\t            print(f\"Skipping a sample due to the following error: {str(e)}\")\n\t            return self.default_output\n\t    def fit(\n\t        self, X: Any = None, y: Any = None, **kwargs: Any\n\t    ) -> BaseZeroShotGPTTransformer:\n\t        \"\"\"Fits the model to the data.\n", "        Parameters\n\t        ----------\n\t        X : Any, optional\n\t        y : Any, optional\n\t        kwargs : dict, optional\n\t        Returns\n\t        -------\n\t        self : BaseZeroShotGPTTransformer\n\t        \"\"\"\n\t        return self\n", "    def transform(\n\t        self, X: np.ndarray | pd.Series | list[str], **kwargs: Any\n\t    ) -> ndarray:\n\t        \"\"\"Converts a list of strings using the open ai API and a predefined\n\t        prompt.\n\t        Parameters\n\t        ----------\n\t        X : Union[np.ndarray, pd.Series, List[str]]\n\t        Returns\n\t        -------\n", "        ndarray\n\t        \"\"\"\n\t        X = _to_numpy(X)\n\t        transformed = []\n\t        for i in tqdm(range(len(X))):\n\t            transformed.append(self._get_chat_completion(X[i]))\n\t        transformed = np.asarray(transformed, dtype=object)\n\t        return transformed\n\t    def fit_transform(\n\t        self, X: np.ndarray | pd.Series | list[str], y=None, **fit_params\n", "    ) -> ndarray:\n\t        \"\"\"Fits and transforms a list of strings using the transform method.\n\t        This is modelled to function as the sklearn fit_transform method.\n\t        Parameters\n\t        ----------\n\t        X : np.ndarray, pd.Series, or list\n\t        Returns\n\t        -------\n\t        ndarray\n\t        \"\"\"\n", "        return self.fit(X, y).transform(X)\n"]}
{"filename": "skllm/openai/credentials.py", "chunked_list": ["import openai\n\tfrom skllm.config import SKLLMConfig as _Config\n\tdef set_credentials(key: str, org: str) -> None:\n\t    \"\"\"Set the OpenAI key and organization.\n\t    Parameters\n\t    ----------\n\t    key : str\n\t        The OpenAI key to use.\n\t    org : str\n\t        The OPEN AI organization ID to use.\n", "    \"\"\"\n\t    openai.api_key = key\n\t    openai.organization = org\n\t    openai.api_type = \"open_ai\"\n\t    openai.api_version = None\n\t    openai.api_base = \"https://api.openai.com/v1\"\n\tdef set_azure_credentials(key: str, org: str) -> None:\n\t    \"\"\"Sets OpenAI credentials for Azure.\n\t    Parameters\n\t    ----------\n", "    key : str\n\t        The OpenAI (Azure) key to use.\n\t    org : str\n\t        The OpenAI (Azure) organization ID to use.\n\t    \"\"\"\n\t    if not openai.api_type or not openai.api_type.startswith(\"azure\"):\n\t        openai.api_type = \"azure\"\n\t    openai.api_key = key\n\t    openai.organization = org\n\t    openai.api_base = _Config.get_azure_api_base()\n", "    openai.api_version = _Config.get_azure_api_version()"]}
{"filename": "skllm/openai/embeddings.py", "chunked_list": ["from time import sleep\n\timport openai\n\tfrom skllm.openai.credentials import set_credentials\n\tdef get_embedding(\n\t    text: str, key: str, org: str, model: str=\"text-embedding-ada-002\", max_retries: int=3\n\t):  \n\t    \"\"\"\n\t    Encodes a string and return the embedding for a string.\n\t    Parameters\n\t    ----------\n", "    text : str\n\t        The string to encode.\n\t    key : str\n\t        The OPEN AI key to use.\n\t    org : str\n\t        The OPEN AI organization ID to use.\n\t    model : str, optional  \n\t        The model to use. Defaults to \"text-embedding-ada-002\".\n\t    max_retries : int, optional\n\t        The maximum number of retries to use. Defaults to 3.\n", "    Returns\n\t    -------\n\t    emb : list\n\t        The GPT embedding for the string.\n\t    \"\"\"\n\t    set_credentials(key, org)\n\t    text = text.replace(\"\\n\", \" \")\n\t    error_msg = None\n\t    error_type = None\n\t    for _ in range(max_retries):\n", "        try:\n\t            emb = openai.Embedding.create(input=[text], model=model)[\"data\"][0][\n\t                \"embedding\"\n\t            ]\n\t            if not isinstance(emb, list):\n\t                raise ValueError(f\"Encountered unknown embedding format. Expected list, got {type(emb)}\")\n\t            return emb\n\t        except Exception as e:\n\t            error_msg = str(e)\n\t            error_type =  type(e).__name__\n", "            sleep(3)\n\t    raise RuntimeError(\n\t        f\"Could not obtain the embedding after {max_retries} retries: `{error_type} :: {error_msg}`\"\n\t    )\n"]}
{"filename": "skllm/openai/mixin.py", "chunked_list": ["from typing import Optional\n\tfrom skllm.config import SKLLMConfig as _Config\n\tclass OpenAIMixin:\n\t    \"\"\"\n\t    A mixin class that provides OpenAI key and organization to other classes.\n\t    \"\"\"\n\t    def _set_keys(self, key: Optional[str] = None, org: Optional[str] = None) -> None:\n\t        \"\"\"\n\t        Set the OpenAI key and organization.\n\t        \"\"\"\n", "        self.openai_key = key\n\t        self.openai_org = org\n\t    def _get_openai_key(self) -> str:\n\t        \"\"\"\n\t        Get the OpenAI key from the class or the config file.\n\t        Returns\n\t        -------\n\t        openai_key: str\n\t        \"\"\"\n\t        key = self.openai_key\n", "        if key is None:\n\t            key = _Config.get_openai_key()\n\t        if key is None:\n\t            raise RuntimeError(\"OpenAI key was not found\")\n\t        return key\n\t    def _get_openai_org(self) -> str:\n\t        \"\"\"\n\t        Get the OpenAI organization ID from the class or the config file.\n\t        Returns\n\t        -------\n", "        openai_org: str\n\t        \"\"\"\n\t        key = self.openai_org\n\t        if key is None:\n\t            key = _Config.get_openai_org()\n\t        if key is None:\n\t            raise RuntimeError(\"OpenAI organization was not found\")\n\t        return key\n"]}
{"filename": "skllm/openai/chatgpt.py", "chunked_list": ["from time import sleep\n\timport openai\n\tfrom skllm.openai.credentials import set_azure_credentials, set_credentials\n\tdef construct_message(role: str, content: str) -> dict:\n\t    \"\"\"Constructs a message for the OpenAI API.\n\t    Parameters\n\t    ----------\n\t    role : str\n\t        The role of the message. Must be one of \"system\", \"user\", or \"assistant\".\n\t    content : str\n", "        The content of the message.\n\t    Returns\n\t    -------\n\t    message : dict\n\t    \"\"\"\n\t    if role not in (\"system\", \"user\", \"assistant\"):\n\t        raise ValueError(\"Invalid role\")\n\t    return {\"role\": role, \"content\": content}\n\tdef get_chat_completion(\n\t    messages: dict,\n", "    key: str,\n\t    org: str,\n\t    model: str = \"gpt-3.5-turbo\",\n\t    max_retries: int = 3,\n\t    api=\"openai\",\n\t):\n\t    \"\"\"Gets a chat completion from the OpenAI API.\n\t    Parameters\n\t    ----------\n\t    messages : dict\n", "        input messages to use.\n\t    key : str\n\t        The OPEN AI key to use.\n\t    org : str\n\t        The OPEN AI organization ID to use.\n\t    model : str, optional\n\t        The OPEN AI model to use. Defaults to \"gpt-3.5-turbo\".\n\t    max_retries : int, optional\n\t        The maximum number of retries to use. Defaults to 3.\n\t    api : str\n", "        The API to use. Must be one of \"openai\" or \"azure\". Defaults to \"openai\".\n\t    Returns\n\t    -------\n\t    completion : dict\n\t    \"\"\"\n\t    if api == \"openai\":\n\t        set_credentials(key, org)\n\t        model_dict = {\"model\": model}\n\t    elif api == \"azure\":\n\t        set_azure_credentials(key, org)\n", "        model_dict = {\"engine\": model}\n\t    else:\n\t        raise ValueError(\"Invalid API\")\n\t    error_msg = None\n\t    error_type = None\n\t    for _ in range(max_retries):\n\t        try:\n\t            completion = openai.ChatCompletion.create(\n\t                temperature=0.0, messages=messages, **model_dict\n\t            )\n", "            return completion\n\t        except Exception as e:\n\t            error_msg = str(e)\n\t            error_type = type(e).__name__\n\t            sleep(3)\n\t    print(\n\t        f\"Could not obtain the completion after {max_retries} retries: `{error_type} ::\"\n\t        f\" {error_msg}`\"\n\t    )\n"]}
{"filename": "skllm/memory/base.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import Any, List\n\tfrom numpy import ndarray\n\tclass _BaseMemoryIndex(ABC):\n\t    @abstractmethod\n\t    def add(self, id: Any, vector: ndarray):\n\t        \"\"\"Adds a vector to the index.\n\t        Parameters\n\t        ----------\n\t        id : Any\n", "            identifier for the vector\n\t        vector : ndarray\n\t            vector to add to the index\n\t        \"\"\"\n\t        pass\n\t    @abstractmethod\n\t    def retrieve(self, vectors: ndarray, k: int) -> List:\n\t        \"\"\"Retrieves the k nearest neighbors for each vector.\n\t        Parameters\n\t        ----------\n", "        vectors : ndarray\n\t            vectors to retrieve nearest neighbors for\n\t        k : int\n\t            number of nearest neighbors to retrieve\n\t        Returns\n\t        -------\n\t        List\n\t            ids of retrieved nearest neighbors\n\t        \"\"\"\n\t        pass\n", "    @abstractmethod\n\t    def build(self) -> None:\n\t        \"\"\"Builds the index.\n\t        All build parameters should be passed to the constructor.\n\t        \"\"\"\n\t        pass\n"]}
{"filename": "skllm/memory/_annoy.py", "chunked_list": ["import os\n\timport tempfile\n\tfrom typing import Any, List\n\ttry:\n\t    from annoy import AnnoyIndex\n\texcept (ImportError, ModuleNotFoundError):\n\t    AnnoyIndex = None\n\tfrom numpy import ndarray\n\tfrom skllm.memory.base import _BaseMemoryIndex\n\tclass AnnoyMemoryIndex(_BaseMemoryIndex):\n", "    \"\"\"Memory index using Annoy.\n\t    Parameters\n\t    ----------\n\t    dim : int\n\t        dimensionality of the vectors\n\t    metric : str, optional\n\t        metric to use, by default \"euclidean\"\n\t    \"\"\"\n\t    def __init__(self, dim: int, metric: str = \"euclidean\", **kwargs: Any) -> None:\n\t        if AnnoyIndex is None:\n", "            raise ImportError(\n\t                \"Annoy is not installed. Please install annoy by running `pip install scikit-llm[annoy]`.\"\n\t            )\n\t        self._index = AnnoyIndex(dim, metric)\n\t        self.metric = metric\n\t        self.dim = dim\n\t        self.built = False\n\t    def add(self, id: int, vector: ndarray) -> None:\n\t        \"\"\"Adds a vector to the index.\n\t        Parameters\n", "        ----------\n\t        id : Any\n\t            identifier for the vector\n\t        vector : ndarray\n\t            vector to add to the index\n\t        \"\"\"\n\t        if self.built:\n\t            raise RuntimeError(\"Cannot add vectors after index is built.\")\n\t        self._index.add_item(id, vector)\n\t    def build(self) -> None:\n", "        \"\"\"Builds the index.\n\t        No new vectors can be added after building.\n\t        \"\"\"\n\t        self._index.build(-1)\n\t        self.built = True\n\t    def retrieve(self, vectors: ndarray, k: int) -> List[List[int]]:\n\t        \"\"\"Retrieves the k nearest neighbors for each vector.\n\t        Parameters\n\t        ----------\n\t        vectors : ndarray\n", "            vectors to retrieve nearest neighbors for\n\t        k : int\n\t            number of nearest neighbors to retrieve\n\t        Returns\n\t        -------\n\t        List\n\t            ids of retrieved nearest neighbors\n\t        \"\"\"\n\t        if not self.built:\n\t            raise RuntimeError(\"Cannot retrieve vectors before the index is built.\")\n", "        return [\n\t            self._index.get_nns_by_vector(v, k, search_k=-1, include_distances=False)\n\t            for v in vectors\n\t        ]\n\t    def __getstate__(self) -> dict:\n\t        \"\"\"Returns the state of the object. To store the actual annoy index, it\n\t        has to be written to a temporary file.\n\t        Returns\n\t        -------\n\t        dict\n", "            state of the object\n\t        \"\"\"\n\t        state = self.__dict__.copy()\n\t        # save index to temporary file\n\t        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n\t            temp_filename = tmp.name\n\t            self._index.save(temp_filename)\n\t        # read bytes from the file\n\t        with open(temp_filename, \"rb\") as tmp:\n\t            index_bytes = tmp.read()\n", "        # store bytes representation in state\n\t        state[\"_index\"] = index_bytes\n\t        # remove temporary file\n\t        os.remove(temp_filename)\n\t        return state\n\t    def __setstate__(self, state: dict) -> None:\n\t        \"\"\"Sets the state of the object. It restores the annoy index from the\n\t        bytes representation.\n\t        Parameters\n\t        ----------\n", "        state : dict\n\t            state of the object\n\t        \"\"\"\n\t        self.__dict__.update(state)\n\t        # restore index from bytes\n\t        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n\t            temp_filename = tmp.name\n\t            tmp.write(self._index)\n\t        self._index = AnnoyIndex(self.dim, self.metric)\n\t        self._index.load(temp_filename)\n", "        # remove temporary file\n\t        os.remove(temp_filename)\n"]}
{"filename": "skllm/memory/__init__.py", "chunked_list": ["from skllm.memory._annoy import AnnoyMemoryIndex\n"]}
{"filename": "skllm/google/tuning.py", "chunked_list": ["from pandas import DataFrame\n\tfrom vertexai.preview.language_models import TextGenerationModel\n\tdef tune(model: str, data: DataFrame, train_steps: int = 100):\n\t    model = TextGenerationModel.from_pretrained(model)\n\t    model.tune_model(\n\t        training_data=data,\n\t        train_steps=train_steps,\n\t        tuning_job_location=\"europe-west4\",  # the only supported training location atm\n\t        tuned_model_location=\"us-central1\",  # the only supported deployment location atm\n\t    )\n", "    return model  # ._job\n"]}
{"filename": "skllm/google/completions.py", "chunked_list": ["from time import sleep\n\tfrom vertexai.preview.language_models import ChatModel, TextGenerationModel\n\t# TODO reduce code duplication for retrying logic\n\tdef get_completion(model: str, text: str, max_retries: int = 3):\n\t    for _ in range(max_retries):\n\t        try:\n\t            if model.startswith(\"text-\"):\n\t                model = TextGenerationModel.from_pretrained(model)\n\t            else:\n\t                model = TextGenerationModel.get_tuned_model(model)\n", "            response = model.predict(text, temperature=0.0)\n\t            return response.text\n\t        except Exception as e:\n\t            error_msg = str(e)\n\t            error_type = type(e).__name__\n\t            sleep(3)\n\t    print(\n\t        f\"Could not obtain the completion after {max_retries} retries: `{error_type} ::\"\n\t        f\" {error_msg}`\"\n\t    )\n", "def get_completion_chat_mode(model: str, context: str, text: str, max_retries: int = 3):\n\t    for _ in range(max_retries):\n\t        try:\n\t            model = ChatModel.from_pretrained(model)\n\t            chat = model.start_chat(context=context)\n\t            response = chat.send_message(text, temperature=0.0)\n\t            return response.text\n\t        except Exception as e:\n\t            error_msg = str(e)\n\t            error_type = type(e).__name__\n", "            sleep(3)\n\t    print(\n\t        f\"Could not obtain the completion after {max_retries} retries: `{error_type} ::\"\n\t        f\" {error_msg}`\"\n\t    )\n"]}
{"filename": "tests/test_gpt_zero_shot_clf.py", "chunked_list": ["import json\n\timport random\n\timport unittest\n\tfrom unittest.mock import MagicMock, patch\n\timport numpy as np\n\tfrom skllm.models.gpt_zero_shot_clf import (\n\t    MultiLabelZeroShotGPTClassifier,\n\t    ZeroShotGPTClassifier,\n\t)\n\tdef _get_ret(label):\n", "    return {\"choices\": [{\"message\": {\"content\": json.dumps({\"label\": label})}}]}\n\tclass TestZeroShotGPTClassifier(unittest.TestCase):\n\t    def get_mock_clf_model(self):\n\t        clf = ZeroShotGPTClassifier(\n\t            openai_key=\"mock_key\", openai_org=\"mock_org\"\n\t        )  # Mock keys\n\t        X = np.array([\"text1\", \"text2\", \"text3\"])\n\t        y = np.array([\"class1\", \"class2\", \"class1\"])\n\t        clf.fit(X, y)\n\t        self.assertEqual(set(clf.classes_), set([\"class1\", \"class2\"]))\n", "        self.assertEqual(clf.probabilities_, [2 / 3, 1 / 3])\n\t        return clf\n\t    @patch(\"skllm.models._base.get_chat_completion\", return_value=MagicMock())\n\t    def test_fit_predict(self, mock_get_chat_completion):\n\t        clf = self.get_mock_clf_model()\n\t        mock_get_chat_completion.return_value.choices[0].message = {\n\t            \"content\": json.dumps({\"label\": \"class1\"})\n\t        }\n\t        predictions = clf.predict([\"text1\", \"text2\", \"text3\"])\n\t        self.assertEqual(predictions, [\"class1\", \"class1\", \"class1\"])\n", "    @patch(\"skllm.models._base.get_chat_completion\", return_value=MagicMock())\n\t    def test_fit_predict_unknown_label_set_default(self, mock_get_chat_completion):\n\t        clf = self.get_mock_clf_model()\n\t        # Random choice\n\t        mock_get_chat_completion.return_value.choices[0].message = {\n\t            \"content\": json.dumps({\"label\": \"new_class\"})\n\t        }\n\t        clf.probabilities_ = [0, 1]\n\t        predictions = clf.predict([\"text1\", \"text2\", \"text3\"])\n\t        random.seed(42)\n", "        np.random.seed(42)\n\t        self.assertEqual(predictions, [\"class2\", \"class2\", \"class2\"])  # Random choice\n\t        clf.default_label = \"default_class\"\n\t        predictions = clf.predict([\"text1\", \"text2\", \"text3\"])\n\t        self.assertEqual(\n\t            predictions, [\"default_class\", \"default_class\", \"default_class\"]\n\t        )\n\t        clf.default_label = None\n\t        predictions = clf.predict([\"text1\", \"text2\", \"text3\"])\n\t        self.assertEqual(predictions, [None, None, None])\n", "class TestMultiLabelZeroShotGPTClassifier(unittest.TestCase):\n\t    def get_mock_clf_model(self):\n\t        clf = MultiLabelZeroShotGPTClassifier(\n\t            openai_key=\"mock_key\", openai_org=\"mock_org\", default_label=\"Random\"\n\t        )  # Mock keys\n\t        X = np.array([\"text1\", \"text2\", \"text3\"])\n\t        y = [\n\t            [\"class1\", \"class2\"],\n\t            [\"class1\", \"class2\"],\n\t            [\"class1\", \"class2\"],\n", "        ]  # Adjusted y to ensure [0.5, 0.5] probability\n\t        clf.fit(X, y)\n\t        self.assertEqual(set(clf.classes_), set([\"class1\", \"class2\"]))\n\t        self.assertEqual(clf.probabilities_, [0.5, 0.5])\n\t        return clf\n\t    @patch(\"skllm.models._base.get_chat_completion\", return_value=MagicMock())\n\t    def test_fit_predict(self, mock_get_chat_completion):\n\t        clf = self.get_mock_clf_model()\n\t        mock_get_chat_completion.return_value = _get_ret([\"class1\", \"class2\"])\n\t        predictions = clf.predict([\"text1\", \"text2\", \"text3\"])\n", "        self.assertEqual(\n\t            predictions,\n\t            [[\"class1\", \"class2\"], [\"class1\", \"class2\"], [\"class1\", \"class2\"]],\n\t        )\n\t    @patch(\"skllm.models._base.get_chat_completion\", return_value=MagicMock())\n\t    def test_fit_predict_unknown_label_set_default(self, mock_get_chat_completion):\n\t        clf = self.get_mock_clf_model()\n\t        mock_get_chat_completion.return_value.choices[0].message = {\n\t            \"content\": json.dumps({\"label\": \"new_class\"})\n\t        }\n", "        clf.probabilities_ = [0.0, 1.0]\n\t        predictions = clf.predict([\"text1\", \"text2\", \"text3\"])\n\t        random.seed(42)\n\t        np.random.seed(42)\n\t        self.assertEqual(\n\t            predictions, [[\"class2\"], [\"class2\"], [\"class2\"]]\n\t        )  # Random choice\n\t        clf.default_label = [\"default_class\"]\n\t        predictions = clf.predict([\"text1\", \"text2\", \"text3\"])\n\t        self.assertEqual(\n", "            predictions, [[\"default_class\"], [\"default_class\"], [\"default_class\"]]\n\t        )\n\t        clf.default_label = None\n\t        predictions = clf.predict([\"text1\", \"text2\", \"text3\"])\n\t        self.assertEqual(predictions, [None, None, None])\n\tif __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "tests/test_gpt_few_shot_clf.py", "chunked_list": ["import json\n\timport random\n\timport unittest\n\tfrom unittest.mock import MagicMock, patch\n\timport numpy as np\n\tfrom skllm.models.gpt_dyn_few_shot_clf import DynamicFewShotGPTClassifier\n\tfrom skllm.models.gpt_few_shot_clf import FewShotGPTClassifier\n\tclass TestFewShotGPTClassifier(unittest.TestCase):\n\t    def get_mock_clf_model(self, clf=\"dynamic\"):\n\t        if clf == \"dynamic\":\n", "            clf = DynamicFewShotGPTClassifier(\n\t                openai_key=\"mock_key\", openai_org=\"mock_org\"\n\t            )\n\t        else:\n\t            clf = FewShotGPTClassifier(\n\t                openai_key=\"mock_key\", openai_org=\"mock_org\"\n\t            )  # Mock keys\n\t        X = np.array([\"text1\", \"text2\", \"text3\"])\n\t        y = np.array([\"class1\", \"class2\", \"class1\"])\n\t        clf.fit(X, y)\n", "        self.assertEqual(set(clf.classes_), set([\"class1\", \"class2\"]))\n\t        self.assertEqual(clf.probabilities_, [2 / 3, 1 / 3])\n\t        return clf\n\t    def test_prompt_generation(self):\n\t        clf = self.get_mock_clf_model(\"few_shot\")\n\t        prompt = clf._get_prompt(\"new_text\")\n\t        self.assertIn(\"text1\", prompt)\n\t        self.assertIn(\"text2\", prompt)\n\t        self.assertIn(\"text3\", prompt)\n\t        self.assertIn(\"class1\", prompt)\n", "        self.assertIn(\"class2\", prompt)\n\t        self.assertIn(\"new_text\", prompt)\n\t    def test_fit(self):\n\t        clf = self.get_mock_clf_model(\"few_shot\")\n\t        X = [\"text1\", \"text2\", \"text3\"]\n\t        y = [\"class1\", \"class2\", \"class1\"]\n\t        self.assertEqual(len(clf.training_data_[0]), len(X))\n\t        self.assertEqual(len(clf.training_data_[1]), len(y))\n\t        for x in X:\n\t            self.assertIn(x, clf.training_data_[0])\n", "        for y_ in y:\n\t            self.assertIn(y_, clf.training_data_[1])\n\t    # TODO Add prompt generation test for dynamic few shot\n\t    # TODO Add tests for dynamic few_show fit\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["from . import test_chatgpt, test_gpt_zero_shot_clf\n"]}
{"filename": "tests/test_chatgpt.py", "chunked_list": ["import unittest\n\tfrom unittest.mock import patch\n\tfrom skllm.openai.chatgpt import (\n\t    construct_message,\n\t    extract_json_key,\n\t    get_chat_completion,\n\t)\n\tclass TestChatGPT(unittest.TestCase):\n\t    @patch(\"skllm.openai.credentials.set_credentials\")\n\t    @patch(\"openai.ChatCompletion.create\")\n", "    def test_get_chat_completion(self, mock_create, mock_set_credentials):\n\t        messages = [{\"role\": \"system\", \"content\": \"Hello\"}]\n\t        key = \"some_key\"\n\t        org = \"some_org\"\n\t        model = \"gpt-3.5-turbo\"\n\t        mock_create.side_effect = [Exception(\"API error\"), \"success\"]\n\t        result = get_chat_completion(messages, key, org, model)\n\t        self.assertTrue(mock_set_credentials.call_count <= 1, \"set_credentials should be called at most once\")\n\t        self.assertEqual(mock_create.call_count, 2, \"ChatCompletion.create should be called twice due to an exception \"\n\t                                                    \"on the first call\")\n", "        self.assertEqual(result, \"success\")\n\t    def test_construct_message(self):\n\t        role = \"user\"\n\t        content = \"Hello, World!\"\n\t        message = construct_message(role, content)\n\t        self.assertEqual(message, {\"role\": role, \"content\": content})\n\t        with self.assertRaises(ValueError):\n\t            construct_message(\"invalid_role\", content)\n\t    def test_extract_json_key(self):\n\t        json_ = '{\"key\": \"value\"}'\n", "        key = \"key\"\n\t        result = extract_json_key(json_, key)\n\t        self.assertEqual(result, \"value\")\n\t        # Given that the function returns None when a KeyError occurs, adjust the assertion\n\t        result_with_invalid_key = extract_json_key(json_, \"invalid_key\")\n\t        self.assertEqual(result_with_invalid_key, None)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
