{"filename": "create_agent.py", "chunked_list": ["\"\"\"\n\tcreate_agent.py\n\tThis file sets up the agent executor for the chatbot application.\n\t\"\"\"\n\timport logging\n\timport os\n\tfrom langchain.callbacks.manager import CallbackManager\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.memory import ConversationBufferMemory\n\tfrom chatweb3.agents.agent_toolkits.snowflake.base import (\n", "    create_snowflake_chat_agent, create_snowflake_conversational_chat_agent)\n\tfrom chatweb3.agents.agent_toolkits.snowflake.prompt import (\n\t    CUSTOM_CONV_SNOWFLAKE_PREFIX, CUSTOM_CONV_SNOWFLAKE_SUFFIX,\n\t    CUSTOM_FORMAT_INSTRUCTIONS, CUSTOM_SNOWFLAKE_PREFIX,\n\t    CUSTOM_SNOWFLAKE_SUFFIX)\n\tfrom chatweb3.agents.agent_toolkits.snowflake.toolkit_custom import \\\n\t    CustomSnowflakeDatabaseToolkit\n\tfrom chatweb3.callbacks.logger_callback import LoggerCallbackHandler\n\tfrom chatweb3.snowflake_database import SnowflakeContainer\n\tfrom config.config import agent_config\n", "from config.logging_config import get_logger\n\tlogger = get_logger(\n\t    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n\t)\n\tPROJ_ROOT_DIR = agent_config.get(\"proj_root_dir\")\n\tLOCAL_INDEX_FILE_PATH = os.path.join(\n\t    PROJ_ROOT_DIR, agent_config.get(\"metadata.context_ethereum_core_file\")\n\t)\n\tINDEX_ANNOTATION_FILE_PATH = os.path.join(\n\t    PROJ_ROOT_DIR, agent_config.get(\"metadata.annotation_ethereum_core_file\")\n", ")\n\tQUERY_DATABASE_TOOL_TOP_K = agent_config.get(\"tool.query_database_tool_top_k\")\n\t# AGENT_EXECUTOR_RETURN_INTERMEDIDATE_STEPS = agent_config.get(\n\t#    \"agent_chain.agent_executor_return_intermediate_steps\"\n\t# )\n\tdef create_agent_executor(conversation_mode=False):\n\t    \"\"\"\n\t    Creates and returns an agent executor.\n\t    Returns:\n\t    agent_executor: The created agent executor\n", "    \"\"\"\n\t    callbacks = CallbackManager([LoggerCallbackHandler()])\n\t    llm = ChatOpenAI(\n\t        model_name=agent_config.get(\"model.llm_name\"),\n\t        temperature=0,\n\t        callbacks=callbacks,\n\t        max_tokens=256,\n\t        verbose=True,\n\t    )\n\t    snowflake_container_eth_core = SnowflakeContainer(\n", "        **agent_config.get(\"snowflake_params\")\n\t        if agent_config.get(\"snowflake_params\")\n\t        else {},\n\t        **agent_config.get(\"shroomdk_params\")\n\t        if agent_config.get(\"shroomdk_params\")\n\t        else {},\n\t        local_index_file_path=LOCAL_INDEX_FILE_PATH,\n\t        index_annotation_file_path=INDEX_ANNOTATION_FILE_PATH,\n\t        verbose=False,\n\t    )\n", "    snowflake_toolkit = CustomSnowflakeDatabaseToolkit(\n\t        db=snowflake_container_eth_core,\n\t        llm=llm,\n\t        readonly_shared_memory=None,\n\t        verbose=True,\n\t    )\n\t    if conversation_mode:\n\t        snowflake_toolkit.instructions = \"\"\n\t        memory = ConversationBufferMemory(\n\t            memory_key=\"chat_history\", return_messages=True\n", "        )\n\t        agent_executor = create_snowflake_conversational_chat_agent(\n\t            llm=llm,\n\t            toolkit=snowflake_toolkit,\n\t            prefix=CUSTOM_CONV_SNOWFLAKE_PREFIX,\n\t            suffix=CUSTOM_CONV_SNOWFLAKE_SUFFIX,\n\t            format_instructions=CUSTOM_FORMAT_INSTRUCTIONS,\n\t            memory=memory,\n\t            return_intermediate_steps=False,\n\t            top_k=QUERY_DATABASE_TOOL_TOP_K,\n", "            max_iterations=15,\n\t            max_execution_time=300,\n\t            early_stopping_method=\"force\",\n\t            callbacks=callbacks,\n\t            verbose=True,\n\t        )\n\t    else:\n\t        agent_executor = create_snowflake_chat_agent(\n\t            llm=llm,\n\t            toolkit=snowflake_toolkit,\n", "            prefix=CUSTOM_SNOWFLAKE_PREFIX,\n\t            suffix=CUSTOM_SNOWFLAKE_SUFFIX,\n\t            format_instructions=CUSTOM_FORMAT_INSTRUCTIONS,\n\t            return_intermediate_steps=True,\n\t            top_k=QUERY_DATABASE_TOOL_TOP_K,\n\t            max_iterations=15,\n\t            max_execution_time=300,\n\t            early_stopping_method=\"generate\",\n\t            callbacks=callbacks,\n\t            verbose=True,\n", "        )\n\t    return agent_executor\n"]}
{"filename": "chat_ui.py", "chunked_list": ["\"\"\"\n\tchat.py\n\tThis file contains the main code for the chatbot application.\n\t\"\"\"\n\timport datetime\n\timport logging\n\timport os\n\timport re\n\t# from dotenv import load_dotenv\n\timport gradio as gr  # type: ignore\n", "from dotenv import load_dotenv\n\t# from config import config\n\tfrom config.logging_config import get_logger\n\tfrom create_agent import create_agent_executor\n\tlogger = get_logger(\n\t    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n\t)\n\tload_dotenv()\n\tCONVERSATION_MODE = False\n\tdef set_openai_api_key(api_key, agent):\n", "    \"\"\"\n\t    Set the OpenAI API Key to the provided value and create an agent executor.\n\t    Parameters:\n\t    api_key (str): OpenAI API Key\n\t    agent: The agent to execute tasks\n\t    Returns:\n\t    agent_executor: The created agent executor\n\t    \"\"\"\n\t    if api_key:\n\t        # set the OpenAI API Key\n", "        os.environ[\"OPENAI_API_KEY\"] = api_key\n\t        agent_executor = create_agent_executor(conversation_mode=CONVERSATION_MODE)\n\t        os.environ[\"OPENAI_API_KEY\"] = \"\"\n\t        return agent_executor\n\tdef format_response(response: dict) -> str:\n\t    \"\"\"\n\t    Formats the response dictionary into a readable string.\n\t    Parameters:\n\t    response (dict): The response dictionary\n\t    Returns:\n", "    formatted_output (str): The formatted output string\n\t    \"\"\"\n\t    output = response[\"output\"]\n\t    intermediate_steps = response[\"intermediate_steps\"]\n\t    logger.debug(f\"intermediate_steps: {intermediate_steps}\")\n\t    formatted_steps = []\n\t    for i, step in enumerate(intermediate_steps, start=1):\n\t        agent_action, text = step\n\t        text = re.sub(r\"`|\\\\\", \"\", str(text))  # remove problematic characters\n\t        text = text.strip().replace(\"\\n\", \"\\n  \")\n", "        log = agent_action.log\n\t        thought, action = log.strip().split(\"\\nAction:\")\n\t        thought = thought.replace(\"Thought: \", \"\") if i == 1 else thought\n\t        formatted_steps.append(\n\t            f\"**Thought {i}**: {thought}\\n\\n*Action:*\\n\\n\\tTool: {agent_action.tool}\"\n\t            f\"\\n\\n\\tTool input: {agent_action.tool_input}\\n\\n*Observation:*\\n\\n{text}\"\n\t        )\n\t    formatted_output = \"\\n\\n\".join(formatted_steps)\n\t    formatted_output += f\"\\n\\n**Final answer**: {output}\"\n\t    return formatted_output\n", "def split_thought_process_text(text: str):\n\t    \"\"\"\n\t    Splits the thought process text into sections.\n\t    Parameters:\n\t    text (str): The thought process text\n\t    Returns:\n\t    sections: The sections of the thought process\n\t    final_answer: The final answer from the thought process\n\t    \"\"\"\n\t    thoughts = text.split(\"_Thought\")\n", "    sections = []\n\t    for t in thoughts[1:]:\n\t        t = t.split(\"_Final answer\")[0]\n\t        thought, action, observation = (\n\t            t.split(\"\\n\\nAction:\\n\\tTool:\")[0],\n\t            \"Tool:\" + t.split(\"Tool:\")[1].split(\"\\n\\nObservation:\\n\\t\")[0],\n\t            t.split(\"Observation:\\n\\t\")[1],\n\t        )\n\t        sections.append((thought, action, observation))\n\t    final_answer = text.split(\"_Final answer_: \")[1]\n", "    return sections, final_answer\n\tdef chat(inp, history, agent):\n\t    \"\"\"\n\t    Handles the chat conversation. If the agent is None,\n\t    it adds a message to the history asking for the OpenAI API Key.\n\t    If the agent is set, it generates a response to the user's input and\n\t    adds it to the history.\n\t    Parameters:\n\t    inp (str): The user's input\n\t    history (list): The chat history\n", "    agent: The chat agent\n\t    Returns:\n\t    history: The updated chat history\n\t    thought_process_text: The formatted thought process text\n\t    \"\"\"\n\t    history = history or []\n\t    if agent is None:\n\t        history.append((inp, \"Please paste your OpenAI API Key to use\"))\n\t        thought_process_text = \"Please paste your OpenAI API Key to use\"\n\t        return history, history, thought_process_text\n", "    else:\n\t        print(\"\\n==== date/time: \" + str(datetime.datetime.now()) + \" ====\")\n\t        print(\"inp: \" + inp)\n\t        response = agent(inp)\n\t        answer = str(response[\"output\"])\n\t        thought_process_text = format_response(response)\n\t        history.append((inp, answer))\n\t    # Debugging: log the types and values of the returned variables\n\t    logger.debug(\n\t        f\"Returning from chat(): history type: {type(history)}, history value: {history}\"\n", "    )\n\t    logger.debug(\n\t        f\"Returning from chat(): thought_process_text type: {type(thought_process_text)}, thought_process_text value: {thought_process_text}\"\n\t    )\n\t    return history, history, thought_process_text\n\t    # return history, history, \"\\n\".join(thought_process_text)\n\tblock = gr.Blocks(css=\".gradio-container {background-color: #f5f5f5;}\")\n\twith block:\n\t    # first row contains the title and the api key textbox\n\t    with gr.Row():\n", "        gr.Markdown(\"<h3><center>Let's ChatWeb3 !</center></h3>\")\n\t        openai_api_key_textbox = gr.Textbox(\n\t            placeholder=\"Paste your OpenAI API Key here\",\n\t            show_label=False,\n\t            lines=1,\n\t            type=\"password\",\n\t        )\n\t    # second row contains the chatbot\n\t    chatbot = gr.Chatbot()\n\t    # third row contains the question textbox and the submit button\n", "    with gr.Row():\n\t        message = gr.Textbox(\n\t            label=\"What's your question?\",\n\t            placeholder=\"What is the total daily trading volume on Uniswap in USD \"\n\t            \"in the last 7 days?\",\n\t            lines=1,\n\t        )\n\t        submit = gr.Button(value=\"Send\", variant=\"Secondary\").style(full_width=False)\n\t    gr.Examples(\n\t        examples=[\n", "            \"What is yesterday's total trading volume on Uniswap in USD?\",\n\t            \"What is the total daily trading volume on Uniswap in USD \"\n\t            \"in the last 7 days?\",\n\t        ],\n\t        inputs=message,\n\t    )\n\t    # gr.HTML(\n\t    #     \"<center> Built by <a href='https://inweb3.com/'>inWeb3</a>\"\n\t    # )\n\t    gr.HTML(\n", "        \"<center> Built by <a href='https://inweb3.com/'>inWeb3</a>, \"\n\t        \"Powered by <a href='https://openai.com/'>OpenAI</a>, \"\n\t        \"<a href='https://github.com/hwchase17/langchain'>LangChain 🦜️🔗</a>, \"\n\t        \"<a href='https://flipsidecrypto.xyz/'>Flipsidecrypto</a></center>\"\n\t    )\n\t    state = gr.State()  # this holds the chat history\n\t    agent_state = gr.State()  # this is the agent\n\t    with gr.Row():\n\t        with gr.Column():\n\t            thought_process_label = gr.Markdown(\"<h4>Thought process:</h4>\")\n", "            thought_process_text = gr.Markdown(value=\"\")\n\t    # \"submit\" Button.click is triggered when the user clicks the button\n\t    submit.click(\n\t        chat,\n\t        inputs=[message, state, agent_state],\n\t        # outputs=[chatbot, state, thought_process_textbox],\n\t        outputs=[chatbot, state, thought_process_text],\n\t    )\n\t    message.submit(\n\t        chat,\n", "        inputs=[message, state, agent_state],\n\t        outputs=[chatbot, state, thought_process_text],\n\t    )\n\t    # configure the \"openai_api_key_textbox\" textbox to change\n\t    openai_api_key_textbox.change(\n\t        set_openai_api_key,\n\t        inputs=[openai_api_key_textbox, agent_state],\n\t        outputs=[agent_state],\n\t    )\n\tif __name__ == \"__main__\":\n", "    block.launch(debug=True)\n"]}
{"filename": "chatweb3/snowflake_database.py", "chunked_list": ["# %%\n\timport logging\n\tfrom typing import Any, Dict, List, Optional, Tuple, Union\n\tfrom langchain.sql_database import SQLDatabase\n\tfrom shroomdk import ShroomDK\n\tfrom sqlalchemy import MetaData, create_engine, text\n\tfrom sqlalchemy.engine import Engine\n\t# from sqlalchemy.engine.cursor import LegacyCursorResult\n\tfrom sqlalchemy.engine.cursor import CursorResult\n\tfrom sqlalchemy.engine.row import Row\n", "from sqlalchemy.exc import SQLAlchemyError\n\tfrom sqlalchemy.schema import CreateTable\n\tfrom config.logging_config import get_logger\n\tlogger = get_logger(\n\t    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n\t)\n\tclass SnowflakeDatabase(SQLDatabase):\n\t    def __init__(\n\t        self,\n\t        engine: Engine,\n", "        # schema: Optional[str] = None,\n\t        schema: str,\n\t        metadata: Optional[MetaData] = None,\n\t        ignore_tables: Optional[List[str]] = None,\n\t        include_tables: Optional[List[str]] = None,\n\t        sample_rows_in_table_info: int = 3,\n\t        indexes_in_table_info: bool = False,\n\t        custom_table_info: Optional[dict] = None,\n\t        view_support: bool = True,\n\t    ):\n", "        \"\"\"\n\t        We make schema a required parameter for SnowflakeDatabase, because we know it is needed for our databases.\n\t        We also make view_support required and default to True, because we know it is needed for our databases.\n\t        \"\"\"\n\t        super().__init__(\n\t            engine=engine,\n\t            schema=schema,\n\t            metadata=metadata,\n\t            ignore_tables=ignore_tables,\n\t            include_tables=include_tables,\n", "            sample_rows_in_table_info=sample_rows_in_table_info,\n\t            indexes_in_table_info=indexes_in_table_info,\n\t            custom_table_info=custom_table_info,\n\t            view_support=view_support,\n\t        )\n\t    def run(  # type: ignore\n\t        self, command: str, fetch: str = \"all\", return_string: bool = True\n\t    ) -> Union[str, CursorResult]:\n\t        \"\"\"Execute a SQL command and return a string representing the results.\n\t        If the statement returns rows, a string of the results is returned.\n", "        If the statement returns no rows, an empty string is returned.\n\t        \"\"\"\n\t        # logger.debug(f\"Entering run with command: {command}\")\n\t        with self._engine.begin() as connection:\n\t            if self._schema is not None:\n\t                # Set the session-level default schema\n\t                if self.dialect == \"snowflake\":\n\t                    set_schema_command = f\"USE SCHEMA {self._schema}\"\n\t                    connection.execute(text(set_schema_command))\n\t                else:\n", "                    connection.exec_driver_sql(f\"SET search_path TO {self._schema}\")\n\t            cursor: CursorResult = connection.execute(text(command))\n\t            if cursor.returns_rows:\n\t                if return_string:\n\t                    if fetch == \"all\":\n\t                        result_all: List[Row] = cursor.fetchall()\n\t                        return str(result_all)\n\t                    elif fetch == \"one\":\n\t                        fetched = cursor.fetchone()\n\t                        result_one: Optional[Tuple[Any, ...]] = (\n", "                            fetched[0] if fetched else None\n\t                        )\n\t                        return str(result_one)\n\t                    else:\n\t                        raise ValueError(\n\t                            \"Fetch parameter must be either 'one' or 'all'\"\n\t                        )\n\t                else:\n\t                    return cursor\n\t        return \"\" if return_string else cursor\n", "    def run_no_throw(  # type: ignore\n\t        self, command: str, fetch: str = \"all\", return_string: bool = True\n\t    ) -> Union[str, CursorResult]:\n\t        \"\"\"Execute a SQL command and return a string representing the results.\n\t        If the statement returns rows, a string of the results is returned.\n\t        If the statement returns no rows, an empty string is returned.\n\t        If the statement throws an error, the error message is returned.\n\t        \"\"\"\n\t        try:\n\t            return self.run(command, fetch, return_string)\n", "        except SQLAlchemyError as e:\n\t            \"\"\"Format the error message\"\"\"\n\t            return f\"Error: {e}\"\n\t    def get_table_info_no_throw(  # type: ignore\n\t        self, table_names: Optional[List[str]] = None, as_dict: bool = False\n\t    ) -> Union[str, Dict[str, str]]:\n\t        \"\"\"\n\t        Get the table info for the given table names.\n\t        This is a temperary hack to get around the fact that the parent method returns a string, but we want to return a dictionary of table names to table info strings\n\t        We duplicate the parent method here to avoid having to modify the parent method.\n", "        \"\"\"\n\t        try:\n\t            all_table_names = self.get_usable_table_names()\n\t            if table_names is not None:\n\t                missing_tables = set(table_names).difference(all_table_names)\n\t                if missing_tables:\n\t                    raise ValueError(\n\t                        f\"table_names {missing_tables} not found in database\"\n\t                    )\n\t                all_table_names = table_names\n", "            meta_tables = [\n\t                tbl\n\t                for tbl in self._metadata.sorted_tables\n\t                if tbl.name in set(all_table_names)\n\t                and not (self.dialect == \"sqlite\" and tbl.name.startswith(\"sqlite_\"))\n\t            ]\n\t            tables = []\n\t            for table in meta_tables:\n\t                if self._custom_table_info and table.name in self._custom_table_info:\n\t                    tables.append(self._custom_table_info[table.name])\n", "                    continue\n\t                # add create table command\n\t                create_table = str(CreateTable(table).compile(self._engine))\n\t                table_info = f\"{create_table.rstrip()}\"\n\t                has_extra_info = (\n\t                    self._indexes_in_table_info or self._sample_rows_in_table_info\n\t                )\n\t                if has_extra_info:\n\t                    table_info += \"\\n\\n/*\"\n\t                if self._indexes_in_table_info:\n", "                    table_info += f\"\\n{self._get_table_indexes(table)}\\n\"\n\t                if self._sample_rows_in_table_info:\n\t                    table_info += f\"\\n{self._get_sample_rows(table)}\\n\"\n\t                if has_extra_info:\n\t                    table_info += \"*/\"\n\t                tables.append(table_info)\n\t            if as_dict:\n\t                return {\n\t                    table.name: table_info\n\t                    for table, table_info in zip(meta_tables, tables)\n", "                }\n\t            else:\n\t                final_str = \"\\n\\n\".join(tables)\n\t                return final_str\n\t        except ValueError as e:\n\t            if table_names is not None and as_dict:\n\t                return {table_name: str(e) for table_name in table_names}\n\t            else:\n\t                return f\"Error: {e}\"\n\tclass SnowflakeContainer:\n", "    def __init__(\n\t        self,\n\t        user: str,\n\t        password: str,\n\t        account_identifier: str,\n\t        local_index_file_path: Optional[str] = None,\n\t        index_annotation_file_path: Optional[str] = None,\n\t        shroomdk_api_key: Optional[str] = None,\n\t        verbose: bool = False,\n\t    ):\n", "        \"\"\"Create a Snowflake container.\n\t        It stores the user, password, and account identifier for a Snowflake account.\n\t        It has a _databases attribute that stores SQLDatabase objects, which are created on demand.\n\t        These databases can be accessed via the (database, schema) key pair\n\t        It can later append the database and schema to the URL to create a Snowflake db engine.\n\t        \"\"\"\n\t        # delay import to avoid circular import\n\t        from chatweb3.metadata_parser import MetadataParser\n\t        self._user = user\n\t        self._password = password\n", "        self._account_identifier = account_identifier\n\t        # Store SQLDatabase objects with (database, schema) as the key\n\t        self._databases: Dict[str, SnowflakeDatabase] = {}\n\t        # Keep the dialect attribute for compatibility with SQLDatabase object\n\t        self.dialect = \"snowflake\"\n\t        self.metadata_parser = MetadataParser(\n\t            file_path=local_index_file_path,\n\t            annotation_file_path=index_annotation_file_path,\n\t            verbose=verbose,\n\t        )\n", "        self._shroomdk = (\n\t            ShroomDK(shroomdk_api_key) if shroomdk_api_key is not None else None\n\t        )\n\t    @property\n\t    def shroomdk(self):\n\t        if self._shroomdk is None:\n\t            raise AttributeError(\n\t                \"Shroomdk attribute is not found in the SnowflakeContainer; please double check whether your SHROOMDK_API_KEY is set correctly in the .env file\"\n\t            )\n\t        return self._shroomdk\n", "    def _create_engine(self, database: str) -> Engine:\n\t        \"\"\"Create a Snowflake engine with the given database\n\t        We do not need to specify the schema here, since we can specify it when we create the SQLDatabase object\n\t        \"\"\"\n\t        # create the base Snowflake URL\n\t        logger.debug(f\"Creating Snowflake engine for {database=}\")\n\t        engine_url = (\n\t            f\"snowflake://{self._user}:{self._password}@{self._account_identifier}/\"\n\t        )\n\t        engine_url += f\"{database}/\"\n", "        engine = create_engine(\n\t            engine_url,\n\t            connect_args={\n\t                \"client_session_keep_alive\": True,\n\t            },\n\t            # echo=True,\n\t        )\n\t        logger.debug(f\"to return {engine=}\")\n\t        return engine\n\t    def get_database(self, database: str, schema: str) -> SnowflakeDatabase:\n", "        key = f\"{database}.{schema}\"\n\t        if key in self._databases:\n\t            return self._databases[key]\n\t        else:\n\t            try:\n\t                engine = self._create_engine(database)\n\t                snowflake_database = SnowflakeDatabase(engine=engine, schema=schema)\n\t                logger.debug(f\"Created {snowflake_database=}\")\n\t                # sql_database = SQLDatabase(engine=engine, schema=schema)\n\t                self._databases[key] = snowflake_database\n", "                return snowflake_database\n\t            except Exception as e:\n\t                raise ValueError(\n\t                    f\"Error getting snowflake database for {key}: {str(e)}\"\n\t                )\n\t    def run_no_throw(\n\t        self,\n\t        command: str,\n\t        database: str,\n\t        schema: str,\n", "        fetch: str = \"all\",\n\t        return_string: bool = True,\n\t    ) -> Union[str, CursorResult]:\n\t        \"\"\"\n\t        submit a query to Snowflake by providing a database and a schema\n\t        \"\"\"\n\t        try:\n\t            return self.get_database(database=database, schema=schema).run_no_throw(\n\t                command=command, fetch=fetch, return_string=return_string\n\t            )\n", "        except Exception as e:\n\t            return f\"Error snowflake container running {command=} on {database}.{schema}: {str(e)}\"\n"]}
{"filename": "chatweb3/__init__.py", "chunked_list": []}
{"filename": "chatweb3/utils.py", "chunked_list": ["\"\"\"\n\tutils.py\n\tThis file contains the utility functions for the chatweb3 package.\n\t\"\"\"\n\timport datetime\n\timport re\n\timport warnings\n\tfrom decimal import Decimal\n\tfrom typing import Dict, List, Optional\n\tdef check_table_long_name(table_name_input):\n", "    parts = table_name_input.split(\".\")\n\t    if len(parts) != 3:\n\t        raise ValueError(\n\t            f\"Invalid table name format. Expected table long name format: 'database_name.schema_name.table_name', got: {table_name_input}\"\n\t        )\n\tdef parse_table_long_name(table_long_name: str):\n\t    pattern = r\"(?P<database>[\\w]+)\\.(?P<schema>[\\w]+)\\.(?P<table>[\\w]+)\"\n\t    match = re.match(pattern, table_long_name)\n\t    if match:\n\t        database = match.group(\"database\")\n", "        schema = match.group(\"schema\")\n\t        table = match.group(\"table\")\n\t        return database, schema, table\n\t    else:\n\t        raise ValueError(\"Invalid table long name format.\")\n\tdef format_dict_to_string(d: dict) -> str:\n\t    return \", \".join([f\"{k}={v!r}\" for k, v in d.items()])\n\tdef convert_rows_to_serializable(rows):\n\t    converted_rows = []\n\t    for row in rows:\n", "        converted_row = []\n\t        for item in row:\n\t            if isinstance(item, datetime.datetime):\n\t                item = item.isoformat()\n\t            elif isinstance(item, datetime.date):\n\t                item = item.isoformat()\n\t            elif isinstance(item, Decimal):\n\t                item = float(item)\n\t            # Add more type conversions here as needed\n\t            converted_row.append(item)\n", "        converted_rows.append(converted_row)\n\t    return converted_rows\n\tdef parse_table_long_name_to_json_list(table_long_names: str):\n\t    \"\"\"\n\t    Parse the table long names into a list of jsons with their database_name, schema_name and table_names.\n\t    The list should be sorted based on the database_name, then schema_name, then table_names.\n\t    Combine the tables with the same database_name and schema_name into one json.\n\t    Examples:\n\t    input1: \"ethereum.core.ez_dex_swaps, ethereum.core.ez_nft_mints, ethereum.core.ez_nft_transfers\"\n\t    output1: \"[{\"database\": \"ethereum\", \"schema\": \"core\", \"table_names\": [\"ez_dex_swaps\", \"ez_nft_mints\", \"ez_nft_transfers\"]}]\"\n", "    input2: \"ethereum.core.ez_dex_swaps, ethereum.core.ez_nft_mints, ethereum.uniswapv3.ez_swaps, ethereum.beacon_chain.fact_deposits, ethereum.uniswapv3.ez_pools\"\n\t    output2: \"[\n\t        {\"database\": \"ethereum\", \"schema\": \"beacon_chain\", \"tables\": [\"fact_deposits\"]},\n\t        {\"database\": \"ethereum\", \"schema\": \"core\", \"tables\": [\"ez_dex_swaps\", \"ez_nft_mints\"]},\n\t        {\"database\": \"ethereum\", \"schema\": \"uniswapv3\", \"tables\": [\"ez_swaps\", \"ez_pools\"]}]\"\n\t    \"\"\"\n\t    table_names = re.split(r\",\\s*\", table_long_names)\n\t    table_data: Dict[str, Dict[str, List[str]]] = {}\n\t    for name in table_names:\n\t        check_table_long_name(name)\n", "        database, schema, table = name.split(\".\")\n\t        if database not in table_data:\n\t            table_data[database] = {}\n\t        if schema not in table_data[database]:\n\t            table_data[database][schema] = []\n\t        table_data[database][schema].append(table)\n\t    result = []\n\t    for database, schemas in table_data.items():\n\t        for schema, tables in schemas.items():\n\t            result.append(\n", "                {\"database\": database, \"schema\": schema, \"tables\": sorted(tables)}\n\t            )\n\t    result.sort(key=lambda x: (x[\"database\"], x[\"schema\"]))\n\t    return result\n\tdef parse_str_to_dict(\n\t    input_str: str, expected_keys: Optional[List[str]] = None\n\t) -> Dict[str, str]:\n\t    \"\"\"Parse a string to a dictionary\n\t    The string contains key-value pairs separated by a colon and comma.\n\t    The keys are expected to be in the expected_keys list.\n", "    The values are expected to be strings.\n\t    If the value contains multiple sub-elements separated by a comma,\n\t    the entire value is needs to be enclosed in a pair of square brackets, or quotes\n\t    For example: tables: [table1, table2], or tables: \"table1, table2\", or tables: 'table1, table2'\n\t    \"\"\"\n\t    if expected_keys is None:\n\t        expected_keys = []\n\t    # Split the string into key-value pairs\n\t    # The regex is to split the string on commas\n\t    # but not if the comma is inside square brackets or inside quotes\n", "    key_value_pairs = [\n\t        pair.strip()\n\t        for pair in re.split(\n\t            r\",(?![^[]*\\])(?=(?:[^']*'[^']*')*[^']*$)(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\",\n\t            input_str,\n\t        )\n\t    ]\n\t    input_dict = {}\n\t    for pair in key_value_pairs:\n\t        try:\n", "            key, value = [s.strip() for s in pair.split(\":\")]\n\t        except ValueError:\n\t            raise ValueError(\n\t                f\"Error extracting key, value pair: {pair=}, {key_value_pairs=}, {input_str=}\"\n\t            )\n\t        if (\n\t            value.startswith(\"[\")\n\t            and value.endswith(\"]\")\n\t            or value.startswith(\"'\")\n\t            and value.endswith(\"'\")\n", "            or value.startswith('\"')\n\t            and value.endswith('\"')\n\t        ):\n\t            # remove the square brackets or quotes if they exist\n\t            value = value[1:-1].strip()\n\t        input_dict[key] = value\n\t    # Check for missing expected keys\n\t    missing_keys = set(expected_keys) - set(input_dict.keys())\n\t    if missing_keys:\n\t        raise ValueError(f\"Missing expected keys: {', '.join(missing_keys)}\")\n", "    # Warn for unexpected keys\n\t    unexpected_keys = set(input_dict.keys()) - set(expected_keys)\n\t    if unexpected_keys:\n\t        warnings.warn(f\"Unexpected keys: {', '.join(unexpected_keys)}\")\n\t    return input_dict\n"]}
{"filename": "chatweb3/metadata_parser.py", "chunked_list": ["# %%\n\timport json\n\timport logging\n\timport re\n\tfrom collections import defaultdict\n\tfrom typing import Dict, List, Optional\n\tfrom chatweb3.utils import (\n\t    parse_table_long_name,\n\t    parse_table_long_name_to_json_list,\n\t)\n", "from config.logging_config import get_logger\n\tlogger = get_logger(\n\t    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n\t)\n\tdef nested_dict_to_dict(d):\n\t    return {\n\t        key: nested_dict_to_dict(value) if isinstance(value, defaultdict) else value\n\t        for key, value in d.items()\n\t    }\n\tdef dict_to_nested_dict(d):\n", "    nested = nested_dict()\n\t    for key, value in d.items():\n\t        if isinstance(value, dict):\n\t            nested[key] = dict_to_nested_dict(value)\n\t        else:\n\t            nested[key] = value\n\t    return nested\n\tdef len_nested_dict(nested_dict):\n\t    count = 0\n\t    for value in nested_dict.values():\n", "        if isinstance(value, dict):\n\t            count += len_nested_dict(value)\n\t        else:\n\t            count += 1\n\t    return count\n\tdef nested_dict():\n\t    return defaultdict(nested_dict)\n\tclass Column:\n\t    def __init__(\n\t        self,\n", "        name,\n\t        table_name=None,\n\t        schema_name=None,\n\t        database_name=None,\n\t        data_type=None,\n\t        comment=None,\n\t        verbose=False,\n\t    ):\n\t        self.name = name.lower()\n\t        self.table_name = table_name.lower() if table_name else None\n", "        self.schema_name = schema_name.lower() if schema_name else None\n\t        self.database_name = database_name.lower() if database_name else None\n\t        self.data_type = data_type\n\t        self.comment = comment\n\t        self.sample_values_list = []\n\t        self.verbose = verbose\n\t    def __repr__(self) -> str:\n\t        return f\"Column('{self.database_name}, {self.schema_name}, {self.table_name}, {self.name}, {self.data_type}, {self.comment}')\"\n\t    def __eq__(self, other):\n\t        if isinstance(other, Column):\n", "            return (\n\t                self.name == other.name\n\t                and self.table_name == other.table_name\n\t                and self.schema_name == other.schema_name\n\t                and self.database_name == other.database_name\n\t                and self.data_type == other.data_type\n\t                and self.comment == other.comment\n\t            )\n\t        return False\n\t    # property for verbose\n", "    @property\n\t    def verbose(self):\n\t        return self._verbose\n\t    @verbose.setter\n\t    def verbose(self, value):\n\t        self._verbose = value\n\t    def _parse_data_type_from_create_table_stmt(self, create_table_stmt):\n\t        pattern = rf\"{self.name}\\s+(?P<data_type>.+?(\\([^\\)]+\\))?(?=\\s*(,|\\n\\s*\\))))\"\n\t        match = re.search(pattern, create_table_stmt, re.IGNORECASE)\n\t        if match:\n", "            data_type = match.group(\"data_type\")\n\t        else:\n\t            if self.verbose:\n\t                logger.warning(\n\t                    f\"{self.database_name}.{self.schema_name}.{self.table_name}: {self.name} data type not parsed from create table statement.\"\n\t                )\n\t            data_type = None\n\t        return data_type\n\t    def _parse_comment_from_ddl(self, get_ddl_create_table):\n\t        pattern = rf\"{self.name}\\s+([\\w\\(\\),]*)?\\s*COMMENT\\s+'(?P<comment>.*?)'\"\n", "        match = re.search(pattern, get_ddl_create_table, re.IGNORECASE | re.DOTALL)\n\t        if match:\n\t            comment = match.group(\"comment\")\n\t        else:\n\t            if self.verbose:\n\t                logger.warning(\n\t                    f\"{self.database_name}.{self.schema_name}.{self.table_name}: {self.name} comment not parsed from get_ddl_create_table.\"\n\t                )\n\t            comment = None\n\t        return comment\n", "    def _parse_value_from_sample_rows(self, sample_row_column_names, sample_rows):\n\t        if self.name in sample_row_column_names:\n\t            index = sample_row_column_names.index(self.name)\n\t            values = [row[index] for row in sample_rows]\n\t            sample_values_list = list(values)\n\t        else:\n\t            if self.verbose:\n\t                logger.warning(\n\t                    f\"{self.database_name}.{self.schema_name}.{self.table_name}: {self.name} value not parsed from sample rows.\"\n\t                )\n", "            sample_values_list = None\n\t        return sample_values_list\n\t    def to_dict(self):\n\t        return {\n\t            \"name\": self.name,\n\t            \"table_name\": self.table_name,\n\t            \"schema_name\": self.schema_name,\n\t            \"database_name\": self.database_name,\n\t            \"data_type\": self.data_type,\n\t            \"comment\": self.comment,\n", "        }\n\t    @classmethod\n\t    def from_dict(cls, data):\n\t        return cls(\n\t            data.get(\"name\"),\n\t            data.get(\"table_name\"),\n\t            data.get(\"schema_name\"),\n\t            data.get(\"database_name\"),\n\t            data.get(\"data_type\"),\n\t            data.get(\"comment\"),\n", "        )\n\tclass Table:\n\t    def __init__(self, table_name, schema_name, database_name, verbose=False):\n\t        self.name = table_name.lower()\n\t        self.schema_name = schema_name.lower()\n\t        self.database_name = database_name.lower()\n\t        self.long_name = f\"{database_name}.{schema_name}.{table_name}\".lower()\n\t        self.comment = \"\"\n\t        self.summary = \"\"\n\t        self.column_names = []\n", "        self.columns = {}\n\t        self.create_table_stmt = None\n\t        self.select_sample_rows_stmt = None\n\t        self.sample_row_column_names = None\n\t        self.sample_rows = None\n\t        self.select_get_ddl_table_stmt = None\n\t        self.get_ddl_create_table = None\n\t        self.select_information_schema_columns_stmt = None\n\t        self.information_schema_columns_names = None\n\t        self.information_schema_columns_values = None\n", "        self.verbose = verbose\n\t    def __repr__(self) -> str:\n\t        return f\"Table(name='{self.name}', long_name='{self.long_name}', database_name='{self.database_name}', schema_name='{self.schema_name}', column_names={self.column_names})\"\n\t    def __eq__(self, other):\n\t        if isinstance(other, Table):\n\t            return (\n\t                self.name == other.name\n\t                and self.database_name == other.database_name\n\t                and self.schema_name == other.schema_name\n\t                and self.long_name == other.long_name\n", "                and self.comment == other.comment\n\t                and self.summary == other.summary\n\t                and self.column_names == other.column_names\n\t                and self.create_table_stmt == other.create_table_stmt\n\t                and self.select_sample_rows_stmt == other.select_sample_rows_stmt\n\t                and self.sample_row_column_names == other.sample_row_column_names\n\t                and self.sample_rows == other.sample_rows\n\t                and self.select_get_ddl_table_stmt == other.select_get_ddl_table_stmt\n\t                and self.get_ddl_create_table == other.get_ddl_create_table\n\t                and self.select_information_schema_columns_stmt\n", "                == other.select_information_schema_columns_stmt\n\t                and self.information_schema_columns_names\n\t                == other.information_schema_columns_names\n\t                and self.information_schema_columns_values\n\t                == other.information_schema_columns_values\n\t                and self.columns == other.columns\n\t            )\n\t        return False\n\t    # create a property for summary\n\t    @property\n", "    def summary(self):\n\t        return self._summary\n\t    # setter for summary for the table and its columns\n\t    @summary.setter\n\t    def summary(self, summary):\n\t        self._summary = summary\n\t        # for column in self.columns.values():\n\t        #     column.summary = summary\n\t    # create a property for verbose\n\t    @property\n", "    def verbose(self):\n\t        return self._verbose\n\t    # setter for verbose for the table and its columns\n\t    @verbose.setter\n\t    def verbose(self, verbose):\n\t        self._verbose = verbose\n\t        for column in self.columns.values():\n\t            column.verbose = verbose\n\t    def _parse_comment_from_ddl(self, get_ddl_create_table):\n\t        comment_pattern = re.compile(\n", "            r\"COMMENT='{1,3}(.*?)'{1,3}(?:[\\s\\\\n]*as[\\s\\\\n]*\\((?:.|[\\r\\n])*?SELECT|[\\s\\\\n]*;)\",\n\t            re.IGNORECASE,\n\t        )\n\t        match = comment_pattern.search(get_ddl_create_table)\n\t        if match:\n\t            comment = match.group(1).strip().replace(\"\\\\n\", \"\\n\")\n\t        else:\n\t            if self.verbose:\n\t                logger.warning(\n\t                    f\"Could not parse comment for table {self.database_name}.{self.schema_name}.{self.name}\"\n", "                )\n\t            comment = \"\"\n\t        return comment\n\t    def _create_columns(self):\n\t        \"\"\"Create Column objects for each column in the table if they do not already exist.\"\"\"\n\t        for column_name in self.column_names:\n\t            if column_name not in self.columns:\n\t                self.columns[column_name] = Column(\n\t                    name=column_name,\n\t                    table_name=self.name,\n", "                    schema_name=self.schema_name,\n\t                    database_name=self.database_name,\n\t                )\n\t    @staticmethod\n\t    def _format_value(value):\n\t        if isinstance(value, str):\n\t            replaced_value = value.replace(\"\\n\", \" \")\n\t            # return f\"'{replaced_value}'\"\n\t            return replaced_value\n\t        return value\n", "    def _get_metadata(\n\t        self,\n\t        include_table_name: bool = True,\n\t        include_table_summary: bool = True,\n\t        include_column_names: bool = False,\n\t        include_column_info: bool = True,\n\t        column_info_format: Optional[List[str]] = None,\n\t    ) -> str:\n\t        \"\"\"\n\t        By default, this method returns a string with the table name, summary.\n", "        Optionally it can return column names, or more detailed column information.\n\t        \"\"\"\n\t        output = \"\"\n\t        if include_table_name:\n\t            output += f\"'{self.database_name}.{self.schema_name}.{self.name}': the '{self.name}' table in '{self.schema_name}' schema of '{self.database_name}' database. \"\n\t        if include_table_summary and not include_column_info:\n\t            # if include_column_info if True, then we will use the table comment as the summary and not the table summary\n\t            output += f\"{self.summary}\"\n\t        if include_column_names and not include_column_info:\n\t            column_names = \", \".join(self.column_names)\n", "            output += f\"This table has the following columns: '{column_names}'\\n\"\n\t        if include_column_info:\n\t            if column_info_format is None:\n\t                column_info_format = [\n\t                    \"name\",\n\t                    \"comment\",\n\t                    \"data_type\",\n\t                    \"sample_values_list\",\n\t                ]\n\t            column_info = {\n", "                \"name\": \"Name\",\n\t                \"comment\": \"Comment\",\n\t                \"data_type\": \"Data type\",\n\t                \"sample_values_list\": \"List of sample values\",\n\t            }\n\t            output += f\"\\nComment: {self.comment}\\n\"\n\t            if len(column_info_format) > 0:\n\t                output += \"Columns in this table:\\n\"\n\t                headers = [column_info[col] for col in column_info_format]\n\t                output += \"\\t\" + \" | \".join(headers) + \"\\n\"\n", "                output += \"\\t\" + \"--- | \" * (len(column_info_format) - 1) + \"---\\n\"\n\t                for column in self.columns.values():\n\t                    column_values = [getattr(column, col) for col in column_info_format]\n\t                    formatted_values = [\n\t                        \", \".join(str(self._format_value(v)) for v in value)\n\t                        if isinstance(value, list)\n\t                        else self._format_value(value)\n\t                        for value in column_values\n\t                    ]\n\t                    output += (\n", "                        \"\\t\" + \" | \".join([str(val) for val in formatted_values]) + \"\\n\"\n\t                    )\n\t        return output.strip()\n\t    def to_dict(self):\n\t        return {\n\t            key: value\n\t            for key, value in {\n\t                \"name\": self.name,\n\t                \"database_name\": self.database_name,\n\t                \"schema_name\": self.schema_name,\n", "                \"long_name\": self.long_name,\n\t                \"comment\": self.comment,\n\t                \"summary\": self.summary,\n\t                \"column_names\": self.column_names,\n\t                \"columns\": {\n\t                    name: column.to_dict() for name, column in self.columns.items()\n\t                },\n\t                \"create_table_stmt\": self.create_table_stmt,\n\t                \"select_sample_rows_stmt\": self.select_sample_rows_stmt,\n\t                \"sample_row_column_names\": self.sample_row_column_names,\n", "                \"sample_rows\": self.sample_rows,\n\t                \"select_get_ddl_table_stmt\": self.select_get_ddl_table_stmt,\n\t                \"get_ddl_create_table\": self.get_ddl_create_table,\n\t                \"select_information_schema_columns_stmt\": self.select_information_schema_columns_stmt,\n\t                \"information_schema_columns_names\": self.information_schema_columns_names,\n\t                \"information_schema_columns_values\": self.information_schema_columns_values,\n\t            }.items()\n\t            if value\n\t        }\n\t    @classmethod\n", "    def from_dict(cls, data):\n\t        table = cls(\n\t            data.get(\"name\"), data.get(\"schema_name\"), data.get(\"database_name\")\n\t        )\n\t        # data[\"name\"], data[\"schema_name\"], data[\"database_name\"])\n\t        table.long_name = data.get(\"long_name\").lower()\n\t        table.comment = data.get(\"comment\")\n\t        table.summary = data.get(\"summary\")\n\t        table.column_names = [x.lower() for x in data.get(\"column_names\")]\n\t        table.columns = {\n", "            name.lower(): Column.from_dict(col_data)\n\t            for name, col_data in data.get(\"columns\", {}).items()\n\t        }\n\t        table.create_table_stmt = data.get(\"create_table_stmt\")\n\t        table.select_sample_rows_stmt = data.get(\"select_sample_rows_stmt\")\n\t        table.sample_row_column_names = data.get(\"sample_row_column_names\")\n\t        table.sample_rows = data.get(\"sample_rows\")\n\t        table.select_get_ddl_table_stmt = data.get(\"select_get_ddl_table_stmt\")\n\t        table.get_ddl_create_table = data.get(\"get_ddl_create_table\")\n\t        table.select_information_schema_columns_stmt = data.get(\n", "            \"select_information_schema_columns_stmt\"\n\t        )\n\t        table.information_schema_columns_names = data.get(\n\t            \"information_schema_columns_names\"\n\t        )\n\t        table.information_schema_columns_values = data.get(\n\t            \"information_schema_columns_values\"\n\t        )\n\t        # adjust for columns\n\t        return table\n", "class Schema:\n\t    def __init__(self, schema_name, database_name, verbose=False):\n\t        self.name = schema_name\n\t        self.database_name = database_name.lower()\n\t        self.long_name = f\"{database_name}.{schema_name}\".lower()\n\t        self.tables = {}\n\t        self.verbose = verbose\n\t    def __repr__(self) -> str:\n\t        return f\"Schema(name='{self.name}', long_name='{self.long_name}', database_name='{self.database_name}', tables={self.tables})\"\n\t    def __eq__(self, other):\n", "        if isinstance(other, Schema):\n\t            return (\n\t                self.name == other.name\n\t                and self.database_name == other.database_name\n\t                and self.long_name == other.long_name\n\t                and self.tables == other.tables\n\t            )\n\t        return False\n\t    # property for verbose\n\t    @property\n", "    def verbose(self):\n\t        return self._verbose\n\t    @verbose.setter\n\t    def verbose(self, value):\n\t        self._verbose = value\n\t        for table in self.tables.values():\n\t            table.verbose = value\n\t    def to_dict(self):\n\t        return {\n\t            \"name\": self.name,\n", "            \"database_name\": self.database_name,\n\t            \"tables\": {name: table.to_dict() for name, table in self.tables.items()},\n\t        }\n\t    @classmethod\n\t    def from_dict(cls, data):\n\t        schema = cls(data[\"name\"], data[\"database_name\"])\n\t        schema.tables = {\n\t            name.lower(): Table.from_dict(table_data)\n\t            for name, table_data in data[\"tables\"].items()\n\t        }\n", "        return schema\n\tclass Database:\n\t    def __init__(self, database_name, verbose=False):\n\t        self.name = database_name\n\t        self.schemas = {}\n\t        self.verbose = verbose\n\t    def __repr__(self) -> str:\n\t        return f\"Database(name='{self.name}', schemas={self.schemas})\"\n\t    def __eq__(self, other):\n\t        if isinstance(other, Database):\n", "            return self.name == other.name and self.schemas == other.schemas\n\t        return False\n\t    # property for verbose\n\t    @property\n\t    def verbose(self):\n\t        return self._verbose\n\t    @verbose.setter\n\t    def verbose(self, value):\n\t        self._verbose = value\n\t        for schema in self.schemas.values():\n", "            schema.verbose = value\n\t    def to_dict(self):\n\t        return {\n\t            \"name\": self.name,\n\t            \"schemas\": {\n\t                name: schema.to_dict() for name, schema in self.schemas.items()\n\t            },\n\t        }\n\t    @classmethod\n\t    def from_dict(cls, data):\n", "        database = cls(data.get(\"name\"))\n\t        # [\"name\"])\n\t        database.schemas = {\n\t            name.lower(): Schema.from_dict(schema_data)\n\t            for name, schema_data in data[\"schemas\"].items()\n\t        }\n\t        return database\n\tclass RootSchema:\n\t    def __init__(self):\n\t        self.databases = {}  # dictionary of databases\n", "        self.verbose = False\n\t    def __repr__(self) -> str:\n\t        return f\"RootSchema(databases={self.databases})\"\n\t    def __eq__(self, other):\n\t        if isinstance(other, RootSchema):\n\t            return self.databases == other.databases\n\t        return False\n\t    # property for verbose\n\t    @property\n\t    def verbose(self):\n", "        return self._verbose\n\t    @verbose.setter\n\t    def verbose(self, value):\n\t        self._verbose = value\n\t        for database in self.databases.values():\n\t            database.verbose = value\n\t    def to_dict(self):\n\t        return {\n\t            \"databases\": {\n\t                name: database.to_dict() for name, database in self.databases.items()\n", "            },\n\t        }\n\t    @classmethod\n\t    def from_dict(cls, data):\n\t        root_schema = cls()\n\t        root_schema.databases = {\n\t            name.lower(): Database.from_dict(cat_data)\n\t            for name, cat_data in data[\"databases\"].items()\n\t        }\n\t        return root_schema\n", "class MetadataParser:\n\t    def __init__(\n\t        self,\n\t        file_path: Optional[str] = None,\n\t        annotation_file_path: Optional[str] = None,\n\t        verbose: bool = False,\n\t    ):\n\t        \"\"\"\n\t        Note: the verbose flag is only effective when the file_path is provided. Otherwise, we have to manually set it after the contents of the root_schema_obj is set.\n\t        The verbose flag is useful when we want to print out the processing warning messages, e.g., parsing issues for comments and other fields of metadata.\n", "        \"\"\"\n\t        self.file_path = file_path\n\t        self._verbose = verbose\n\t        self._initialize_nested_dicts()\n\t        if file_path is not None:\n\t            # If a file_path is provided, load metadata from the file\n\t            data = self.load_metadata_from_json()\n\t            # then deserialize the metadata into the RootSchema object\n\t            self.from_dict(data, verbose=verbose)\n\t            if annotation_file_path is not None:\n", "                # If an annotation_file_path is provided, load the annotation file and add the summary to the RootSchema object\n\t                self.add_table_summary(file_path_json=annotation_file_path)\n\t            # set the verbose flag for the RootSchema object\n\t            # self.verbose = verbose\n\t        else:\n\t            # If no file_path is provided, create an empty RootSchema object\n\t            self.root_schema_obj = RootSchema()\n\t    def from_dict(self, data, verbose: bool = False):\n\t        \"\"\"Takes in the metadata dictionary loaded from the JSON file and deserializes it into the RootSchema object\"\"\"\n\t        self.root_schema_obj = RootSchema.from_dict(data=data[\"root_schema_obj\"])\n", "        self._unify_names_to_lower_cases()\n\t        # Create the column objects for each table\n\t        self._create_table_columns()\n\t        if verbose:\n\t            self.root_schema_obj.verbose = verbose\n\t        # Populate the comment for each table\n\t        self._populate_table_comment()\n\t        # Populate various column attributes\n\t        self._populate_column_table_schema_database_names()\n\t        self._populate_column_data_type()\n", "        self._populate_column_comment()\n\t        self._populate_column_sample_values_list()\n\t        # NOTE: the use of nested dicts will be removed in the future\n\t        # Initialize the nested dictionaries\n\t        self._initialize_nested_dicts()\n\t        # Populate the nested dictionaries\n\t        self._populate_nested_dicts()\n\t    def to_dict(self):\n\t        self._unify_names_to_lower_cases()\n\t        data = {\n", "            \"root_schema_obj\": self.root_schema_obj.to_dict(),\n\t        }\n\t        return data\n\t    def _initialize_nested_dicts(self):\n\t        self.create_table_stmt = nested_dict()\n\t        self.select_sample_rows_stmt = nested_dict()\n\t        self.table_sample_rows = nested_dict()\n\t        self.table_sample_row_column_names = nested_dict()\n\t        self.select_get_ddl_table_stmt = nested_dict()\n\t        self.get_ddl_create_table = nested_dict()\n", "        self.select_information_schema_columns_stmt = nested_dict()\n\t        self.information_schema_columns_names = nested_dict()\n\t        self.information_schema_columns_values = nested_dict()\n\t    def _populate_nested_dicts(self):\n\t        for database_name, database in self.root_schema_obj.databases.items():\n\t            for schema_name, schema in database.schemas.items():\n\t                for table_name, table in schema.tables.items():\n\t                    self.create_table_stmt[database_name][schema_name][table_name] = (\n\t                        table.create_table_stmt or None\n\t                    )\n", "                    self.select_sample_rows_stmt[database_name][schema_name][\n\t                        table_name\n\t                    ] = (table.select_sample_rows_stmt or None)\n\t                    self.table_sample_row_column_names[database_name][schema_name][\n\t                        table_name\n\t                    ] = (table.sample_row_column_names or None)\n\t                    self.table_sample_rows[database_name][schema_name][table_name] = (\n\t                        table.sample_rows or None\n\t                    )\n\t                    self.select_get_ddl_table_stmt[database_name][schema_name][\n", "                        table_name\n\t                    ] = (table.select_get_ddl_table_stmt or None)\n\t                    self.get_ddl_create_table[database_name][schema_name][\n\t                        table_name\n\t                    ] = (table.get_ddl_create_table or None)\n\t                    self.select_information_schema_columns_stmt[database_name][\n\t                        schema_name\n\t                    ][table_name] = (\n\t                        table.select_information_schema_columns_stmt or None\n\t                    )\n", "                    self.information_schema_columns_names[database_name][schema_name][\n\t                        table_name\n\t                    ] = (table.information_schema_columns_names or None)\n\t                    self.information_schema_columns_values[database_name][schema_name][\n\t                        table_name\n\t                    ] = (table.information_schema_columns_values or None)\n\t    def _populate_column_table_schema_database_names(self):\n\t        for database_name, database in self.root_schema_obj.databases.items():\n\t            for schema_name, schema in database.schemas.items():\n\t                for table_name, table in schema.tables.items():\n", "                    for column in table.columns.values():\n\t                        column.table_name = table_name\n\t                        column.schema_name = schema_name\n\t                        column.database_name = database_name\n\t    def _create_table_columns(self):\n\t        \"\"\"Create Column objects for each column in each table in each schema in each database, if they do not already exists.\"\"\"\n\t        for database_name, database in self.root_schema_obj.databases.items():\n\t            for schema_name, schema in database.schemas.items():\n\t                for table_name, table in schema.tables.items():\n\t                    table._create_columns()\n", "    def _populate_column_comment(self):\n\t        for database_name, database in self.root_schema_obj.databases.items():\n\t            for schema_name, schema in database.schemas.items():\n\t                for table_name, table in schema.tables.items():\n\t                    ddl_create_table = table.get_ddl_create_table\n\t                    for column in table.columns.values():\n\t                        column.comment = (\n\t                            column._parse_comment_from_ddl(ddl_create_table)\n\t                            if ddl_create_table\n\t                            else None\n", "                        )\n\t    #                    ddl_create_table = self.get_ddl_create_table[database_name][ schema_name ][table_name]\n\t    def _populate_column_data_type(self):\n\t        for database_name, database in self.root_schema_obj.databases.items():\n\t            for schema_name, schema in database.schemas.items():\n\t                for table_name, table in schema.tables.items():\n\t                    create_table_stmt = table.create_table_stmt\n\t                    for column in table.columns.values():\n\t                        column.data_type = (\n\t                            (\n", "                                column._parse_data_type_from_create_table_stmt(\n\t                                    create_table_stmt\n\t                                )\n\t                            )\n\t                            if create_table_stmt\n\t                            else None\n\t                        )\n\t                    # create_table_stmt = self.create_table_stmt[database_name][ schema_name ][table_name]\n\t    def _populate_column_sample_values_list(self):\n\t        for database in self.root_schema_obj.databases.values():\n", "            for schema in database.schemas.values():\n\t                for table in schema.tables.values():\n\t                    for column in table.columns.values():\n\t                        column.sample_values_list = (\n\t                            column._parse_value_from_sample_rows(\n\t                                table.sample_row_column_names,\n\t                                table.sample_rows,\n\t                            )\n\t                            if table.sample_rows\n\t                            else None\n", "                        )\n\t    def _populate_table_comment(self):\n\t        for db in self.root_schema_obj.databases.values():\n\t            for schema in db.schemas.values():\n\t                for table in schema.tables.values():\n\t                    table.comment = (\n\t                        table._parse_comment_from_ddl(table.get_ddl_create_table)\n\t                        if table.get_ddl_create_table\n\t                        else None\n\t                    )\n", "    def _unify_names_to_lower_cases(self):\n\t        for database in self.root_schema_obj.databases.values():\n\t            database.name = database.name.lower()\n\t            for schema in database.schemas.values():\n\t                schema.name = schema.name.lower()\n\t                for table in schema.tables.values():\n\t                    table.name = table.name.lower()\n\t                    for column in table.columns.values():\n\t                        column.name = column.name.lower()\n\t    def add_table_summary(\n", "        self,\n\t        table_summary_json: Optional[Dict] = None,\n\t        file_path_json: Optional[str] = None,\n\t    ):\n\t        \"\"\"Add a table summary to the metadata.\n\t        Input file must be a JSON file with the following structure:\n\t        {\n\t            \"table_summary\": {\n\t            \"table_long_name\": \"table_summary\",\n\t            \"table_long_name\": \"table_summary\",\n", "            ...\n\t            }\n\t        }\n\t        \"\"\"\n\t        if table_summary_json is None and file_path_json is None:\n\t            raise ValueError(\n\t                \"Either table_summary_json or file_path_json must be provided.\"\n\t            )\n\t        if table_summary_json and file_path_json:\n\t            logging.warning(\n", "                \"Both table_summary_json and file_path_json are provided, use file_path_json only.\"\n\t            )\n\t        if file_path_json:\n\t            try:\n\t                with open(file_path_json, \"r\") as f:\n\t                    data = json.load(f)\n\t                    table_summary = data.get(\"table_summary\", None)\n\t            except FileNotFoundError:\n\t                print(f\"File not found: {file_path_json}\")\n\t        elif table_summary_json:\n", "            # table_summary = json.loads(table_summary_json)\n\t            table_summary = table_summary_json\n\t        else:\n\t            raise ValueError(\n\t                f\"Unable to load table summary from {file_path_json} or {table_summary_json}.\"\n\t            )\n\t        for table_long_name, summary in table_summary.items():\n\t            # parse the table long name\n\t            database_name, schema_name, table_name = parse_table_long_name(\n\t                table_long_name\n", "            )\n\t            # add the summary to the table\n\t            self.root_schema_obj.databases[database_name].schemas[schema_name].tables[\n\t                table_name\n\t            ].summary = summary\n\t    # create a property to access the verbose attribute\n\t    @property\n\t    def verbose(self):\n\t        return self._verbose\n\t    # create a setter for the verbose attribute\n", "    @verbose.setter\n\t    def verbose(self, value: bool):\n\t        self._verbose = value\n\t        self.root_schema_obj.verbose = value\n\t    def save_metadata_to_json(self, file_path):\n\t        with open(file_path, \"w\") as f:\n\t            json.dump(self.to_dict(), f)\n\t    def load_metadata_from_json(self, file_path=None):\n\t        \"\"\"Load metadata from a JSON file and returns it.\"\"\"\n\t        if file_path is None:\n", "            file_path = self.file_path\n\t        with open(file_path, \"r\") as f:\n\t            metadata = json.load(f)\n\t        return metadata\n\t    def get_tables_from_database_schema_table_names(\n\t        self, database_name=None, schema_name=None, table_name=None\n\t    ):\n\t        \"\"\"return a table or a list of tables\n\t        filtered by database_name, schema_name, and table_name if provided return all table names if database_name or schema_name is None\n\t        \"\"\"\n", "        # check for error conditions in database_name\n\t        if database_name is not None and not isinstance(database_name, str):\n\t            raise ValueError(\"database_name must be a string\")\n\t        if schema_name is not None and not isinstance(schema_name, str):\n\t            raise ValueError(\"schema_name must be a string\")\n\t        if table_name is not None and not isinstance(schema_name, str):\n\t            raise ValueError(\"schema_name must be a string\")\n\t        tables = []\n\t        if table_name:\n\t            # if table_name is provided, then we only return one table\n", "            if database_name is None or schema_name is None:\n\t                raise ValueError(\n\t                    \"database_name and schema_name must be provided if table_name is provided\"\n\t                )\n\t            table = (\n\t                self.root_schema_obj.databases[database_name]\n\t                .schemas[schema_name]\n\t                .tables[table_name]\n\t            )\n\t            tables.append(table)\n", "        else:\n\t            for (\n\t                cat_name,\n\t                database,\n\t            ) in (\n\t                self.root_schema_obj.databases.items()\n\t            ):  # database could be e.g., \"ethereum\"\n\t                if database_name is None or cat_name == database_name:\n\t                    for sch_name, schema in database.schemas.items():\n\t                        if schema_name is None or sch_name == schema_name:\n", "                            for table in schema.tables.values():\n\t                                tables.append(table)\n\t        return tables\n\t    def get_table_metadata(\n\t        self,\n\t        database: Optional[str] = None,\n\t        schema: Optional[str] = None,\n\t        tables: Optional[List] = None,\n\t        include_table_name: Optional[bool] = True,\n\t        include_table_summary: Optional[bool] = True,\n", "        include_column_names: Optional[bool] = False,\n\t        include_column_info: Optional[bool] = True,\n\t        column_info_format: Optional[List] = None,\n\t    ):\n\t        \"\"\"\n\t        Process and return metadata information given database, schema and table.\n\t        Args:\n\t            database (str, optional): The database of the table.\n\t            schema (str, optional): The schema of the table.\n\t            tables (list of str, optional): The names of the tables.\n", "        Returns:\n\t            str: The concatenated table information.\n\t        \"\"\"\n\t        target_tables = self._find_target_tables(database, schema, tables)\n\t        output = \"\"\n\t        for table in target_tables:\n\t            output += table._get_metadata(\n\t                include_table_name=include_table_name,\n\t                include_table_summary=include_table_summary,\n\t                include_column_names=include_column_names,\n", "                include_column_info=include_column_info,\n\t                column_info_format=column_info_format,\n\t            )\n\t            output += \"\\n\\n\"\n\t        return output.strip()\n\t    def _find_target_tables(self, database=None, schema=None, tables=None):\n\t        matched_tables = []\n\t        if database is not None and database not in self.root_schema_obj.databases:\n\t            logger.warning(f\"Database '{database}' does not exist.\")\n\t        for db_name, db in self.root_schema_obj.databases.items():\n", "            if database is None or db_name == database:\n\t                for sch_name, sch in db.schemas.items():\n\t                    if schema is None or sch_name == schema:\n\t                        if tables is None:\n\t                            matched_tables.extend(list(sch.tables.values()))\n\t                        else:\n\t                            for table_name in tables:\n\t                                if table_name in sch.tables:\n\t                                    matched_tables.append(sch.tables[table_name])\n\t        return matched_tables\n", "    def get_metadata_by_table_long_names(\n\t        self,\n\t        table_long_names: str,\n\t        include_table_name: Optional[bool] = True,\n\t        include_table_summary: Optional[bool] = True,\n\t        include_column_names: Optional[bool] = False,\n\t        include_column_info: Optional[bool] = True,\n\t        column_info_format: Optional[List] = None,\n\t    ) -> str:\n\t        \"\"\"\n", "        Process and return metadata information given a list of table long names in the form of\n\t        database_name.schema_name.table_name.\n\t        Args:\n\t            table_long_names (str): The list of table long names.\n\t        Returns:\n\t            str: The concatenated table information.\n\t        \"\"\"\n\t        parsed_table_info = parse_table_long_name_to_json_list(table_long_names)\n\t        output = \"\"\n\t        for info in parsed_table_info:\n", "            database = info[\"database\"]\n\t            schema = info[\"schema\"]\n\t            tables = info[\"tables\"]\n\t            output += self.get_table_metadata(\n\t                database,\n\t                schema,\n\t                tables,\n\t                include_table_name=include_table_name,\n\t                include_table_summary=include_table_summary,\n\t                include_column_names=include_column_names,\n", "                include_column_info=include_column_info,\n\t                column_info_format=column_info_format,\n\t            )\n\t            output += \"\\n\\n\"\n\t        return output.strip()\n"]}
{"filename": "chatweb3/tools/base.py", "chunked_list": ["from pydantic import BaseModel\n\tclass BaseToolInput(BaseModel):\n\t    \"\"\"A base class for tool input models.\"\"\"\n\t    pass\n"]}
{"filename": "chatweb3/tools/__init__.py", "chunked_list": []}
{"filename": "chatweb3/tools/snowflake_database/tool_custom.py", "chunked_list": ["\"\"\"\n\ttool_custom.py\n\tThis file contains the custom tools for the snowflake_database toolkit.\n\t\"\"\"\n\tfrom typing import Any, List, Optional, Union\n\tfrom langchain.callbacks.manager import (\n\t    CallbackManagerForToolRun,\n\t)\n\tfrom chatweb3.tools.snowflake_database.tool import (\n\t    GetSnowflakeDatabaseTableMetadataTool,\n", "    ListSnowflakeDatabaseTableNamesTool,\n\t    QuerySnowflakeDatabaseTool,\n\t    SnowflakeQueryCheckerTool,\n\t)\n\tfrom config.config import agent_config\n\tQUERY_DATABASE_TOOL_MODE = agent_config.get(\"tool.query_database_tool_mode\")\n\tCHECK_TABLE_SUMMARY_TOOL_MODE = agent_config.get(\"tool.check_table_summary_tool_mode\")\n\tCHECK_TABLE_METADATA_TOOL_MODE = agent_config.get(\"tool.check_table_metadata_tool_mode\")\n\tCHECK_TABLE_SUMMARY_TOOL_NAME = \"check_available_tables_summary\"\n\tCHECK_TABLE_METADATA_TOOL_NAME = \"check_table_metadata_details\"\n", "CHECK_QUERY_SYNTAX_TOOL_NAME = \"check_snowflake_query_syntax\"\n\tQUERY_DATABASE_TOOL_NAME = \"query_snowflake_database\"\n\tTOOLKIT_INSTRUCTIONS = f\"\"\"\n\tWhen using these tools, you MUST follow the instructions below:\n\t1. You MUST always start with the {CHECK_TABLE_SUMMARY_TOOL_NAME} tool to check the available tables in the databases, and make selection of one or multiple tables you want to work with if applicable.\n\t2. If you need to construct any SQL query for any table, you MUST always use the {CHECK_TABLE_METADATA_TOOL_NAME} tool to check the metadata detail of the table before you can create a query for that table. Note that you can check the metadata details of multiple tables at the same time.\n\t3. When constructing a query containing a token or NFT, if you have both its address and and its symbol, you MUST always prefer using the token address over the token symbol since token symbols are often not unique.\n\t4. If you receive and error from  {QUERY_DATABASE_TOOL_NAME} tool, you MUST always analyze the error message and determine how to resolve it. If it is a general syntax error, you MUST use the {CHECK_QUERY_SYNTAX_TOOL_NAME} tool to double check the query before you can run it again through the {QUERY_DATABASE_TOOL_NAME} tool. If it is due to invalid table or column names, you MUST double check the {CHECK_TABLE_METADATA_TOOL_NAME} tool and re-construct the query accordingly.\n\t\"\"\"\n\tclass CheckTableSummaryTool(ListSnowflakeDatabaseTableNamesTool):\n", "    name = CHECK_TABLE_SUMMARY_TOOL_NAME\n\t    description = \"\"\"\n\t    Input is an empty string.\n\t    Output is the list of available tables in their full names (database.schema.table), accompanied by their summary descriptions to help you understand what each table is about.\n\t    \"\"\"\n\t    def _run(  # type: ignore[override]\n\t        self,\n\t        tool_input: str = \"\",\n\t        run_manager: Optional[CallbackManagerForToolRun] = None,\n\t        mode: str = CHECK_TABLE_SUMMARY_TOOL_MODE,\n", "    ) -> str:\n\t        return super()._run(tool_input=tool_input, mode=mode)\n\tclass CheckTableMetadataTool(GetSnowflakeDatabaseTableMetadataTool):\n\t    name = CHECK_TABLE_METADATA_TOOL_NAME\n\t    description = \"\"\"\n\t    Input is one or more table names specified in their full names (database.schema.table) and seperated by a COMMA.\n\t    Output is the detailed metadata including column specifics of those tables so that you can construct SQL query to them.\n\t    \"\"\"\n\t    def _run(self, table_names: str, run_manager: Optional[CallbackManagerForToolRun] = None, mode: str = CHECK_TABLE_METADATA_TOOL_MODE) -> str:  # type: ignore[override]\n\t        return super()._run(table_names=table_names, run_manager=run_manager, mode=mode)\n", "class CheckQuerySyntaxTool(SnowflakeQueryCheckerTool):\n\t    name = CHECK_QUERY_SYNTAX_TOOL_NAME\n\t    description = \"\"\"\n\t    Input is a Snowflake SQL query.\n\t    Output is the syntax check result of the query.\n\t    \"\"\"\n\t    def _run(\n\t        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n\t    ) -> str:\n\t        return super()._run(query=query, run_manager=run_manager)\n", "class QueryDatabaseTool(QuerySnowflakeDatabaseTool):\n\t    name = QUERY_DATABASE_TOOL_NAME\n\t    description = \"\"\"\n\t    Input to this tool contains a Snowflake SQL query in correct syntax. It should be in JSON format with EXACTLY ONE key \"query\" that has the value of the query string.\n\t    Output is the query result from the database.\n\t    \"\"\"\n\t    def _run(  # type: ignore[override]\n\t        self,\n\t        *args,\n\t        mode: str = QUERY_DATABASE_TOOL_MODE,\n", "        run_manager: Optional[CallbackManagerForToolRun] = None,\n\t        **kwargs,\n\t    ) -> Union[str, List[Any]]:\n\t        return super()._run(*args, mode=mode, run_manager=run_manager, **kwargs)\n"]}
{"filename": "chatweb3/tools/snowflake_database/__init__.py", "chunked_list": []}
{"filename": "chatweb3/tools/snowflake_database/prompt.py", "chunked_list": ["# flake8: noqa\n\tSNOWFLAKE_QUERY_CHECKER = \"\"\"\n\tDouble check the {dialect} query for common mistakes, including:\n\t- Using NOT IN with NULL values\n\t- Using UNION when UNION ALL should have been used\n\t- Using BETWEEN for exclusive ranges\n\t- Data type mismatch in predicates\n\t- Properly quoting identifiers\n\t- Using the correct number of arguments for functions\n\t- Casting to the correct data type\n", "- Using the proper columns for joins\n\tIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\"\"\"\n"]}
{"filename": "chatweb3/tools/snowflake_database/tool.py", "chunked_list": ["import json\n\timport logging\n\timport re\n\tfrom typing import Any, Dict, List, Optional, Union, cast\n\tfrom langchain.base_language import BaseLanguageModel\n\tfrom langchain.callbacks.manager import (AsyncCallbackManagerForToolRun,\n\t                                         CallbackManagerForToolRun)\n\tfrom langchain.chains.llm import LLMChain\n\tfrom langchain.chat_models.base import BaseChatModel\n\tfrom langchain.prompts.chat import (BaseMessagePromptTemplate,\n", "                                    ChatPromptTemplate,\n\t                                    HumanMessagePromptTemplate,\n\t                                    SystemMessagePromptTemplate)\n\tfrom langchain.schema import BaseMessage\n\tfrom langchain.tools.sql_database.tool import (InfoSQLDatabaseTool,\n\t                                               ListSQLDatabaseTool,\n\t                                               QueryCheckerTool,\n\t                                               QuerySQLDataBaseTool)\n\tfrom pydantic import Field, root_validator\n\tfrom chatweb3.snowflake_database import SnowflakeContainer\n", "from chatweb3.tools.base import BaseToolInput\n\tfrom chatweb3.tools.snowflake_database.prompt import SNOWFLAKE_QUERY_CHECKER\n\tfrom chatweb3.utils import \\\n\t    parse_table_long_name_to_json_list  # parse_str_to_dict\n\tfrom config.config import agent_config\n\tfrom config.logging_config import get_logger\n\tlogger = get_logger(\n\t    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n\t)\n\tLIST_SNOWFLAKE_DATABASE_TABLE_NAMES_TOOL_NAME = \"list_snowflake_db_table_names\"\n", "GET_SNOWFLAKE_DATABASE_TABLE_METADATA_TOOL_NAME = \"get_snowflake_db_table_metadata\"\n\tQUERY_SNOWFLAKE_DATABASE_TOOL_NAME = \"query_snowflake_db\"\n\tSNOWFLAKE_QUERY_CHECKER_TOOL_NAME = \"snowflake_query_checker\"\n\tSELECT_SNOWFLAKE_DATABASE_SCHEMA_TOOL_NAME = \"select_snowflake_db_schema\"\n\tDEFAULT_DATABASE = agent_config.get(\"database.default_database\")\n\tDEFAULT_SCHEMA = agent_config.get(\"database.default_schema\")\n\tclass ListSnowflakeDatabaseTableNamesToolInput(BaseToolInput):\n\t    database_name: str = Field(\n\t        alias=\"database\", description=\"The name of the database.\"\n\t    )\n", "    schema_name: str = Field(alias=\"schema\", description=\"The name of the schema.\")\n\tclass ListSnowflakeDatabaseTableNamesTool(ListSQLDatabaseTool):\n\t    db: SnowflakeContainer = Field(exclude=True)  # type: ignore\n\t    name = LIST_SNOWFLAKE_DATABASE_TABLE_NAMES_TOOL_NAME\n\t    description = \"\"\"\n\t    Input is an empty string, output is a list of tables in long form, e.g., 'ethereum.core.ez_nft_sales' means the 'ez_nft_sales' table in 'core' schema of 'ethereum' database, and optionally with their brief summary information.\n\t    \"\"\"\n\t    def _get_table_long_names_from_snowflake(self, tool_input: str) -> str:\n\t        database = DEFAULT_DATABASE\n\t        schema = DEFAULT_SCHEMA\n", "        # Locate the SQLDatabase object with the given database and schema\n\t        snowflake_database = self.db.get_database(database, schema)\n\t        # Call its get_table_names() method\n\t        table_names = snowflake_database.get_usable_table_names()\n\t        # Add the database and schema to the table names\n\t        table_long_names = [f\"{database}.{schema}.{table}\" for table in table_names]\n\t        return \", \".join(table_long_names)\n\t    def _run(\n\t        self,\n\t        tool_input: str = \"\",\n", "        run_manager: Optional[CallbackManagerForToolRun] = None,\n\t        mode: str = \"default\",\n\t    ) -> str:\n\t        \"\"\"Get available tables in the databases\n\t        mode:\n\t        - \"default\": use local index to get the info, if not found, use snowflake as fallback\n\t        - \"snowflake\": use snowflake to get the info\n\t        - \"local\": use local index to get the info\n\t        Note: since local index is currently enforced by the table_long_names_enabled variable, while snowflake is enforced by its own Magicdatabase and MagicSchema, the two modes often produce different results.\n\t        \"\"\"\n", "        logger.debug(\n\t            f\"Entering list snowflake database table names tool _run with tool_input: {tool_input} and mode: {mode}\"\n\t        )\n\t        if mode not in [\"local\", \"snowflake\", \"default\"]:\n\t            raise ValueError(f\"Invalid mode: {mode}\")\n\t        table_long_names_enabled_list = agent_config.get(\n\t            \"ethereum_core_table_long_name.enabled_list\"\n\t        )\n\t        table_long_names_enabled = \", \".join(table_long_names_enabled_list)\n\t        include_column_names = False\n", "        # include_column_names = True\n\t        include_column_info = False\n\t        if mode == \"local\":\n\t            # use local index to get the info\n\t            return self.db.metadata_parser.get_metadata_by_table_long_names(\n\t                table_long_names=table_long_names_enabled,\n\t                include_column_names=include_column_names,\n\t                include_column_info=include_column_info,\n\t            )\n\t        if mode == \"snowflake\":\n", "            return self._get_table_long_names_from_snowflake(tool_input=tool_input)\n\t        # default mode\n\t        # use local index to get the info, if not found, use snowflake as fallback\n\t        if mode == \"default\":\n\t            try:\n\t                # logger.debug(f\"mode: {mode}\")\n\t                result = self.db.metadata_parser.get_metadata_by_table_long_names(\n\t                    table_long_names=table_long_names_enabled,\n\t                    include_column_names=include_column_names,\n\t                    include_column_info=include_column_info,\n", "                )\n\t                if result:\n\t                    return result\n\t                else:\n\t                    raise Exception(\"Table name list not found in local index\")\n\t            except Exception as e:\n\t                logger.debug(f\"Table name list not found in local index: {e}\")\n\t                return self._get_table_long_names_from_snowflake(tool_input=tool_input)\n\t        return \"\"  # this is a dummy return, just to make mypy happy\n\t    async def _arun(\n", "        self,\n\t        tool_input: str = \"\",\n\t        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n\t    ) -> str:\n\t        raise NotImplementedError(\"ListSQLDatabaseNameTool does not support async\")\n\tclass GetSnowflakeDatabaseTableMetadataTool(InfoSQLDatabaseTool):\n\t    \"\"\"Tool for getting metadata about a SQL database schema.\"\"\"\n\t    db: SnowflakeContainer = Field(exclude=True)  # type: ignore\n\t    name = GET_SNOWFLAKE_DATABASE_TABLE_METADATA_TOOL_NAME\n\t    description = f\"\"\"\n", "    Input to this tool is a list of tables specified by the long_name format (database_name.schema_name.table_name), make sure they are separated by a COMMA, not a colon or other characters!\n\t    Output is the metadata including schema and sample rows for those tables.\n\t    Be sure that the tables actually exist by calling {LIST_SNOWFLAKE_DATABASE_TABLE_NAMES_TOOL_NAME} first!\n\t    Example Input: \"database_name.schema_name.table_name1: database_name.schema_name.table_name2: database_name.schema_name.table_name3\"\n\t    \"\"\"\n\t    def _get_metadata_from_snowflake(self, tool_input: str) -> str:\n\t        \"\"\"First parse the input into a list of jsons with database and schema separated.\n\t            Example:\n\t            input: \"ethereum.core.ez_dex_swaps, ethereum.core.ez_nft_mints, ethereum.uniswapv3.ez_swaps, ethereum.beacon_chain.fact_deposits, ethereum.uniswapv3.ez_pools\"\n\t            parsed_input: \"[\n", "                {\"database\": \"ethereum\", \"schema\": \"beacon_chain\", \"tables\": [\"fact_deposits\"]},\n\t                {\"database\": \"ethereum\", \"schema\": \"core\", \"tables\": [\"ez_dex_swaps\", \"ez_nft_mints\"]},\n\t                {\"database\": \"ethereum\", \"schema\": \"uniswapv3\", \"tables\": [\"ez_swaps\", \"ez_pools\"]}\n\t            ]\"\n\t        Then iterate through the list and call self.db['database']['schema'].get_table_info_no_throw(tables)\n\t        Sample final output:\n\t        \"database_name.schema_name.table_name1: metadata1;\\n database_name.schema_name.table_name2: metadata2;\\n database_name.schema_name.table_name3: metadata3\"\n\t        \"\"\"\n\t        input_table_json_list = parse_table_long_name_to_json_list(tool_input)\n\t        logger.debug(\n", "            f\"Entering _get_metadata_from_snowflake with {input_table_json_list=}\"\n\t        )\n\t        metadata_list = []\n\t        for input_dict in input_table_json_list:\n\t            logging.debug(f\"\\n{input_dict=}\")\n\t            database, schema, tables = (\n\t                input_dict[\"database\"],\n\t                input_dict[\"schema\"],\n\t                input_dict[\"tables\"],\n\t            )\n", "            snowflake_database = self.db.get_database(database, schema)\n\t            # table_names = \", \".join(tables)\n\t            metadata = snowflake_database.get_table_info_no_throw(\n\t                table_names=tables, as_dict=True\n\t            )\n\t            assert isinstance(\n\t                metadata, dict\n\t            ), \"Expected a dict from get_table_info_no_throw\"  # make mypy happy\n\t            logger.debug(f\"\\n Retrieved {metadata=}\")\n\t            # Add the formatted metadata string for each table to metadata_list\n", "            for table, table_metadata in metadata.items():\n\t                table_parts = table.split(\".\")\n\t                table_name = table_parts[-1]\n\t                table_schema = table_parts[-2] if len(table_parts) > 1 else schema\n\t                table_database = table_parts[-3] if len(table_parts) > 2 else database\n\t                # logger.debug( f\"\\n{table=}, {table_parts=}, {table_name=}, {table_schema=}, {table_database=}\")\n\t                formatted_metadata = (\n\t                    f\"{table_database}.{table_schema}.{table_name}: {table_metadata}\"\n\t                )\n\t                metadata_list.append(formatted_metadata)\n", "        return (\n\t            \";\\n\\n\\n\".join(metadata_list)\n\t            if len(metadata_list) > 1\n\t            else metadata_list[0]\n\t        )\n\t    def _run(\n\t        self,\n\t        table_names: str,\n\t        run_manager: Optional[CallbackManagerForToolRun] = None,\n\t        mode: str = \"local\",\n", "    ) -> str:\n\t        # def _run(self, tool_input: str, mode: str = \"default\") -> str:\n\t        \"\"\"Get the metadata for tables in a comma-separated list.\n\t        mode:\n\t        - \"default\": use local index to get metadata, if not found, use snowflake to get metadata\n\t        - \"snowflake\": use snowflake to get metadata\n\t        - \"local\": use local index to get metadata\n\t        \"\"\"\n\t        logger.debug(f\"\\n Entering get metadata tool _run with {table_names=}, {mode=}\")\n\t        if mode not in [\"local\", \"snowflake\", \"default\"]:\n", "            raise ValueError(f\"Invalid mode: {mode}\")\n\t        if mode == \"local\":\n\t            # use local index to get metadata\n\t            return self.db.metadata_parser.get_metadata_by_table_long_names(table_names)\n\t        if mode == \"snowflake\":\n\t            # use snowflake to get metadata\n\t            return self._get_metadata_from_snowflake(table_names)\n\t        if mode == \"default\":\n\t            try:\n\t                # use local index to get metadata\n", "                logger.debug(f\"{self.db.metadata_parser=}\")\n\t                result = self.db.metadata_parser.get_metadata_by_table_long_names(\n\t                    table_names\n\t                )\n\t                if result:\n\t                    return result\n\t                else:\n\t                    raise Exception(\"Not found in local index\")\n\t            except Exception:\n\t                # if not found in local index, use snowflake to get metadata\n", "                logger.warning(\n\t                    \"Metadata not found in local index, retrieving using snowflake\"\n\t                )\n\t                return self._get_metadata_from_snowflake(table_names)\n\t        return \"\"  # dummy return, just to make mypy happy\n\tclass QuerySnowflakeDatabaseTool(QuerySQLDataBaseTool):\n\t    \"\"\"Tool for querying a Snowflake database.\"\"\"\n\t    db: SnowflakeContainer = Field(exclude=True)  # type: ignore\n\t    name = QUERY_SNOWFLAKE_DATABASE_TOOL_NAME\n\t    description = f\"\"\"\n", "    Never call this tool directly,\n\t    - always use the {GET_SNOWFLAKE_DATABASE_TABLE_METADATA_TOOL_NAME} to get the metadata of the tables you want to query first, and make sure your query does NOT have any non-existent column names and other identifiers!\n\t    - always use the {SNOWFLAKE_QUERY_CHECKER_TOOL_NAME} to check if your query is correct before executing it!\n\t    Input to this tool is a database and a schema, along with a detailed and correct SQL query, output is a result from the database.\n\t    If the query is not correct, an error message will be returned.\n\t    If an error is returned, rewrite the query, check the query, and try again.\n\t    Example Input: 'database: database_name, schema: schema_name, query: SELECT * FROM table_name'\n\t    \"\"\"\n\t    def _process_tool_input(self, tool_input: Union[str, Dict]) -> Dict[str, str]:\n\t        logger.debug(f\"\\nEntering with {tool_input=}\")\n", "        if isinstance(tool_input, str):\n\t            pattern = r\"database: (.*?), schema: (.*?), query: (.*)\"\n\t            match = re.search(pattern, tool_input)\n\t            if match:\n\t                input_dict = {\n\t                    \"database\": match.group(1),\n\t                    \"schema\": match.group(2),\n\t                    \"query\": match.group(3),\n\t                }\n\t            else:\n", "                # # try to see if the input is a query only\n\t                # input_dict = {\n\t                #     \"database\": DEFAULT_DATABASE,\n\t                #     \"schema\": DEFAULT_SCHEMA,\n\t                #     \"query\": tool_input,\n\t                # }\n\t                # try to see if the input is a string of a dict\n\t                try:\n\t                    input_dict = json.loads(tool_input)\n\t                    if \"query\" in input_dict.keys():\n", "                        if \"database\" not in input_dict.keys():\n\t                            input_dict[\"database\"] = DEFAULT_DATABASE\n\t                        if \"schema\" not in input_dict.keys():\n\t                            input_dict[\"schema\"] = DEFAULT_SCHEMA\n\t                except json.JSONDecodeError:\n\t                    # try to see if the input is a query only\n\t                    input_dict = {\n\t                        \"database\": DEFAULT_DATABASE,\n\t                        \"schema\": DEFAULT_SCHEMA,\n\t                        \"query\": tool_input,\n", "                    }\n\t        elif isinstance(tool_input, dict):\n\t            full_keys = {\"database\", \"schema\", \"query\"}\n\t            if full_keys.issubset(tool_input.keys()):\n\t                input_dict = tool_input\n\t            elif \"query\" in tool_input.keys():\n\t                # try to see if the input is a query only\n\t                input_dict = {\n\t                    \"database\": DEFAULT_DATABASE,\n\t                    \"schema\": DEFAULT_SCHEMA,\n", "                    \"query\": tool_input.get(\"query\"),\n\t                }\n\t            # required_keys = {\"database\", \"schema\", \"query\"}\n\t            # if required_keys.issubset(tool_input.keys()):\n\t            #     input_dict = tool_input\n\t            else:\n\t                raise ValueError(\n\t                    f\"Invalid tool_input. Unable to parse 'query' from dictionary {tool_input=}\"\n\t                )\n\t        else:\n", "            raise ValueError(\"Invalid tool_input. Expected a string or a dictionary.\")\n\t        return input_dict\n\t    #    def _run(self, *args, **kwargs) -> str:\n\t    def _run(  # type: ignore\n\t        self,\n\t        *args,\n\t        mode=\"default\",\n\t        run_manager: Optional[CallbackManagerForToolRun] = None,\n\t        **kwargs,\n\t    ) -> Union[str, List[Any]]:\n", "        \"\"\"Execute the query, return the results or an error message.\"\"\"\n\t        if mode not in [\"shroomdk\", \"snowflake\", \"default\"]:\n\t            raise ValueError(f\"Invalid mode: {mode}\")\n\t        if args:\n\t            tool_input = args[0]\n\t        else:\n\t            tool_input = kwargs\n\t        input_dict = self._process_tool_input(tool_input)\n\t        logger.debug(f\"\\nParsed input: {input_dict=}\")\n\t        # Retrieve values\n", "        database = input_dict[\"database\"]\n\t        schema = input_dict[\"schema\"]\n\t        query = input_dict[\"query\"]\n\t        if mode == \"snowflake\":\n\t            snowflake_database = self.db.get_database(database, schema)\n\t            result_snowflake = snowflake_database.run_no_throw(query)\n\t            assert isinstance(result_snowflake, str)\n\t            logger.debug(f\"snowflake {result_snowflake=}\")\n\t            return result_snowflake\n\t        if mode == \"shroomdk\":\n", "            result_set = self.db.shroomdk.query(query)\n\t            logger.debug(f\"shroomdk {result_set.rows=}\")\n\t            result_shroomdk: List[Any] = result_set.rows\n\t            return result_shroomdk\n\t        if mode == \"default\":\n\t            # try to use shroomdk first\n\t            try:\n\t                result_set = self.db.shroomdk.query(query)\n\t                logger.debug(\n\t                    f\"shroomdk result: {result_set=}, to return {result_set.rows=}\"\n", "                )\n\t                result_shroomdk = result_set.rows\n\t                return result_shroomdk\n\t            except Exception:\n\t                # if shroomdk fails, use snowflake\n\t                try:\n\t                    snowflake_database = self.db.get_database(database, schema)\n\t                    result_snowflake = snowflake_database.run_no_throw(query)\n\t                    assert isinstance(result_snowflake, str)\n\t                    return result_snowflake\n", "                except Exception:\n\t                    raise Exception(\n\t                        f\"Unable to execute query {query=} on {database=}.{schema=} via either shroomdk or snowflake.\"\n\t                    )\n\t        return \"\"  # dummy return, just to make mypy happy\n\tclass SnowflakeQueryCheckerTool(QueryCheckerTool):\n\t    \"\"\"Use an LLM to check if a query is correct.\n\t    Adapted from https://www.patterns.app/blog/2023/01/18/crunchbot-sql-analyst-gpt/\"\"\"\n\t    template: str = SNOWFLAKE_QUERY_CHECKER\n\t    llm: BaseChatModel\n", "    db: SnowflakeContainer = Field(exclude=True)  # type: ignore\n\t    name = SNOWFLAKE_QUERY_CHECKER_TOOL_NAME\n\t    description = f\"\"\"\n\t    Use this tool to double check if your snowflake query is correct before executing it.\n\t    Always use this tool before executing a query with {QUERY_SNOWFLAKE_DATABASE_TOOL_NAME}!\n\t    \"\"\"\n\t    @root_validator(pre=True)\n\t    def initialize_llm_chain(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n\t        if \"llm_chain\" not in values:\n\t            input_variables = [\"query\", \"dialect\"]\n", "            messages = [\n\t                SystemMessagePromptTemplate.from_template(\n\t                    template=SNOWFLAKE_QUERY_CHECKER\n\t                ),\n\t                HumanMessagePromptTemplate.from_template(template=\"\\n\\n{query}\"),\n\t            ]\n\t            assert isinstance(messages, list) and all(\n\t                isinstance(item, (BaseMessagePromptTemplate, BaseMessage))\n\t                for item in messages\n\t            ), \"Expected a list of BaseMessagePromptTemplate or BaseMessage\"\n", "            llm = values.get(\"llm\")\n\t            assert isinstance(llm, BaseLanguageModel)\n\t            values[\"llm_chain\"] = LLMChain(\n\t                llm=llm,\n\t                prompt=ChatPromptTemplate(\n\t                    input_variables=input_variables,\n\t                    messages=cast(\n\t                        List[Union[BaseMessagePromptTemplate, BaseMessage]], messages\n\t                    ),  # casting it for now\n\t                ),\n", "            )\n\t        if values[\"llm_chain\"].prompt.input_variables != [\"query\", \"dialect\"]:\n\t            raise ValueError(\n\t                \"LLM chain for QueryCheckerTool must have input variables ['query', 'dialect']\"\n\t            )\n\t        return values\n"]}
{"filename": "chatweb3/agents/__init__.py", "chunked_list": ["\"\"\"Interface for agents.\"\"\"\n\tfrom chatweb3.agents.agent_toolkits import (\n\t    create_snowflake_agent,\n\t    create_snowflake_chat_agent,\n\t    create_sql_chat_agent,\n\t)\n\t__all__ = [\"create_snowflake_agent\", \"create_snowflake_chat_agent\", \"create_sql_chat_agent\"]  # type: ignore\n"]}
{"filename": "chatweb3/agents/conversational_chat/base.py", "chunked_list": ["\"\"\"Base class for snowflake conversational chat agents.\"\"\"\n\tfrom typing import Sequence, Optional, List\n\tfrom langchain.prompts.base import BasePromptTemplate\n\tfrom langchain.prompts.chat import (\n\t    ChatPromptTemplate,\n\t    HumanMessagePromptTemplate,\n\t    MessagesPlaceholder,\n\t    SystemMessagePromptTemplate,\n\t)\n\tfrom langchain.tools import BaseTool\n", "from chatweb3.agents.agent_toolkits.snowflake.prompt import (\n\t    CONV_SNOWFLAKE_PREFIX,\n\t    CONV_SNOWFLAKE_SUFFIX,\n\t)\n\tfrom langchain.agents.conversational_chat.base import ConversationalChatAgent\n\tfrom langchain.schema import BaseOutputParser\n\tclass SnowflakeConversationalChatAgent(ConversationalChatAgent):\n\t    @classmethod\n\t    def create_prompt(\n\t        cls,\n", "        tools: Sequence[BaseTool],\n\t        toolkit_instructions: Optional[str] = \"\",\n\t        system_message: str = CONV_SNOWFLAKE_PREFIX,\n\t        human_message: str = CONV_SNOWFLAKE_SUFFIX,\n\t        input_variables: Optional[List[str]] = None,\n\t        output_parser: Optional[BaseOutputParser] = None,\n\t        format_instructions: Optional[str] = None,\n\t    ) -> BasePromptTemplate:\n\t        \"\"\"Create a prompt template for the Snowflake conversational chat agent.\"\"\"\n\t        # tool descriptions\n", "        tool_strings = \"\\n\".join(\n\t            [f\"> {tool.name}: {tool.description}\" for tool in tools]\n\t        )\n\t        if toolkit_instructions:\n\t            tool_strings += f\"\\n{toolkit_instructions}\"\n\t        # tool names\n\t        tool_names = \", \".join([tool.name for tool in tools])\n\t        # set up the output parser\n\t        _output_parser = output_parser or cls._get_default_output_parser()\n\t        human_message_final_prompt = human_message\n", "        system_message_final_prompt = system_message\n\t        # fill in the format instructions\n\t        if \"{format_instructions}\" in human_message:\n\t            human_message_format_instructions = human_message.format(\n\t                format_instructions=_output_parser.get_format_instructions()\n\t                if not format_instructions\n\t                else format_instructions\n\t            )\n\t            # fill in the tools\n\t            if \"{tools}\" in human_message:\n", "                human_message_final_prompt = human_message_format_instructions.format(\n\t                    tool_names=tool_names, tools=tool_strings\n\t                )\n\t            else:\n\t                raise ValueError(\n\t                    \"{format_instructions} in human message but {tools} are not, yet to be implemented\"\n\t                )\n\t        elif \"{format_instructions}\" in system_message:\n\t            system_message_format_instructions = system_message.format(\n\t                format_instructions=_output_parser.get_format_instructions()\n", "                if not format_instructions\n\t                else format_instructions\n\t            )\n\t            # fill in the tools\n\t            if \"{tools}\" in system_message:\n\t                system_message_final_prompt = system_message_format_instructions.format(\n\t                    tool_names=tool_names, tools=tool_strings\n\t                )\n\t            else:\n\t                raise ValueError(\n", "                    \"{format_instructions} in system message but {tools} are not, yet to be implemented\"\n\t                )\n\t        else:\n\t            Warning(\"Format_instructions not found in system_message or human_message\")\n\t        if input_variables is None:\n\t            input_variables = [\"input\", \"chat_history\", \"agent_scratchpad\"]\n\t        messages = [\n\t            SystemMessagePromptTemplate.from_template(system_message_final_prompt),\n\t            MessagesPlaceholder(variable_name=\"chat_history\"),\n\t            HumanMessagePromptTemplate.from_template(human_message_final_prompt),\n", "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n\t        ]\n\t        return ChatPromptTemplate(input_variables=input_variables, messages=messages)\n"]}
{"filename": "chatweb3/agents/chat/base.py", "chunked_list": ["\"\"\"Base class for snowflake chat agents.\"\"\"\n\tfrom typing import List, Optional, Sequence, Union, cast\n\tfrom langchain.agents.chat.base import ChatAgent\n\tfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\n\tfrom langchain.prompts.base import BasePromptTemplate\n\tfrom langchain.prompts.chat import (\n\t    BaseMessagePromptTemplate,\n\t    ChatPromptTemplate,\n\t    HumanMessagePromptTemplate,\n\t    SystemMessagePromptTemplate,\n", ")\n\tfrom langchain.schema import BaseMessage\n\tfrom langchain.tools import BaseTool\n\tfrom chatweb3.agents.agent_toolkits.snowflake.prompt import (\n\t    SNOWFLAKE_PREFIX,\n\t    SNOWFLAKE_SUFFIX,\n\t)\n\tclass SnowflakeChatAgent(ChatAgent):\n\t    @classmethod\n\t    def create_prompt(  # type: ignore[override]\n", "        cls,\n\t        tools: Sequence[BaseTool],\n\t        prefix: str = SNOWFLAKE_PREFIX,\n\t        suffix: str = SNOWFLAKE_SUFFIX,\n\t        format_instructions: str = FORMAT_INSTRUCTIONS,\n\t        toolkit_instructions: Optional[str] = \"\",\n\t        input_variables: Optional[List[str]] = None,\n\t        system_template: Optional[str] = None,\n\t        human_template: Optional[str] = None,\n\t        # **kwargs: Any,\n", "    ) -> BasePromptTemplate:\n\t        \"\"\"Create a prompt template for the Snowflake chat agent.\"\"\"\n\t        tool_strings = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n\t        if toolkit_instructions:\n\t            tool_strings += f\"\\n{toolkit_instructions}\"\n\t        tool_names = \", \".join([tool.name for tool in tools])\n\t        format_instructions = format_instructions.format(tool_names=tool_names)\n\t        if system_template is None:\n\t            system_template = \"\\n\\n\".join(\n\t                [prefix, tool_strings, format_instructions, suffix]\n", "            )\n\t        if human_template is None:\n\t            human_template = \"Question: {input}\\n\\n{agent_scratchpad}\"\n\t        messages = [\n\t            SystemMessagePromptTemplate.from_template(system_template),\n\t            HumanMessagePromptTemplate.from_template(human_template),\n\t        ]\n\t        if input_variables is None:\n\t            input_variables = [\"input\", \"agent_scratchpad\"]\n\t        return ChatPromptTemplate(\n", "            input_variables=input_variables,\n\t            messages=cast(\n\t                List[Union[BaseMessagePromptTemplate, BaseMessage]], messages\n\t            ),\n\t        )\n"]}
{"filename": "chatweb3/agents/chat/__init__.py", "chunked_list": []}
{"filename": "chatweb3/agents/chat/prompt.py", "chunked_list": ["# flake8: noqa\n\tCUSTOM_FORMAT_INSTRUCTIONS = \"\"\"The way you use the tools is by specifying a json blob.\n\tSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n\tThe ONLY values that should be in the 'action' field are: {tool_names}\n\tThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n\t```\n\t{{{{\n\t  \"action\": $TOOL_NAME,\n\t  \"action_input\": $INPUT\n\t}}}}\n", "```\n\tSince all the tools you have access to requires this $JSON_BLOB format, you MUST ALWAYS return the above $JSON_BLOB for any follow-up tools. \n\tMeanwhile, ALWAYS use the following format:\n\tQuestion: the input question you must answer\n\tThought: you should always think about what to do\n\tAction:\n\t```\n\t$JSON_BLOB\n\t```\n\tObservation: the result of the action\n", "... (this Thought/Action/Observation can repeat N times)\n\tThought: I now know the final answer\n\tFinal Answer: the final answer to the original input question\"\"\"\n"]}
{"filename": "chatweb3/agents/agent_toolkits/__init__.py", "chunked_list": ["\"\"\"Agent toolkits.\"\"\"\n\tfrom chatweb3.agents.agent_toolkits.snowflake.base import (\n\t    create_snowflake_agent,\n\t    create_snowflake_chat_agent,\n\t    create_sql_chat_agent,\n\t)\n\tfrom chatweb3.agents.agent_toolkits.snowflake.toolkit import (\n\t    SnowflakeDatabaseToolkit,\n\t)\n\t__all__ = [\n", "    \"create_sql_chat_agent\",\n\t    \"create_snowflake_agent\",\n\t    \"create_snowflake_chat_agent\",\n\t    \"SnowflakeDatabaseToolkit\",\n\t]\n"]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/base.py", "chunked_list": ["\"\"\"Snowflake agent that uses a Chat model\"\"\"\n\tfrom typing import Any, List, Optional, Sequence\n\tfrom langchain.agents.agent import AgentExecutor, AgentOutputParser\n\tfrom langchain.agents.agent_toolkits.sql.base import create_sql_agent\n\tfrom langchain.agents.agent_toolkits.sql.prompt import SQL_PREFIX, SQL_SUFFIX\n\tfrom langchain.agents.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n\tfrom langchain.agents.chat.base import ChatAgent\n\tfrom langchain.agents.chat.output_parser import ChatOutputParser\n\tfrom langchain.agents.chat.prompt import FORMAT_INSTRUCTIONS\n\tfrom langchain.callbacks.base import BaseCallbackManager\n", "from langchain.chains.llm import LLMChain\n\tfrom langchain.chat_models.base import BaseChatModel\n\tfrom langchain.llms.base import BaseLLM\n\tfrom langchain.schema import BaseMemory\n\tfrom langchain.tools import BaseTool\n\tfrom chatweb3.agents.agent_toolkits.snowflake.prompt import (\n\t    CONV_SNOWFLAKE_PREFIX, CONV_SNOWFLAKE_SUFFIX, SNOWFLAKE_PREFIX,\n\t    SNOWFLAKE_SUFFIX)\n\tfrom chatweb3.agents.agent_toolkits.snowflake.toolkit import \\\n\t    SnowflakeDatabaseToolkit\n", "from chatweb3.agents.agent_toolkits.snowflake.toolkit_custom import \\\n\t    CustomSnowflakeDatabaseToolkit\n\tfrom chatweb3.agents.chat.base import SnowflakeChatAgent\n\tfrom chatweb3.agents.conversational_chat.base import \\\n\t    SnowflakeConversationalChatAgent\n\tdef create_snowflake_conversational_chat_agent(\n\t    # for llm chain of agent\n\t    llm: BaseChatModel,\n\t    # for prompt of llm chain\n\t    prefix: str = CONV_SNOWFLAKE_PREFIX,\n", "    suffix: str = CONV_SNOWFLAKE_SUFFIX,\n\t    format_instructions: Optional[str] = None,\n\t    input_variables: Optional[List[str]] = None,\n\t    top_k: int = 10,\n\t    # system_template: Optional[str] = None,\n\t    # human_template: Optional[str] = None,\n\t    # shared by agent and agent executor chain\n\t    tools: Optional[Sequence[BaseTool]] = None,\n\t    toolkit: Optional[SnowflakeDatabaseToolkit] = None,\n\t    # for agent\n", "    output_parser: Optional[AgentOutputParser] = None,\n\t    # for agent executor\n\t    max_iterations: Optional[int] = 10,\n\t    max_execution_time: Optional[float] = 20,\n\t    early_stopping_method: Optional[str] = \"force\",\n\t    return_intermediate_steps: Optional[bool] = False,\n\t    # for chains\n\t    callbacks: Optional[BaseCallbackManager] = None,\n\t    verbose: Optional[bool] = False,\n\t    memory: Optional[BaseMemory] = None,\n", "    # additional kwargs\n\t    toolkit_kwargs: Optional[dict] = None,\n\t    prompt_kwargs: Optional[dict] = None,\n\t    agent_executor_kwargs: Optional[dict] = None,\n\t    agent_kwargs: Optional[dict] = None,\n\t    llm_chain_kwargs: Optional[dict] = None,\n\t    # **kwargs: Any,\n\t) -> AgentExecutor:\n\t    \"\"\"\n\t    Construct a sql chat agent from an LLM and tools.\n", "    \"\"\"\n\t    # get tools\n\t    # if tools is None then get tools from toolkit\n\t    if tools is None:\n\t        if toolkit is None:\n\t            raise ValueError(\"Must provide either tools or toolkit\")\n\t        else:\n\t            toolkit_kwargs = toolkit_kwargs or {}\n\t            tools = toolkit.get_tools(\n\t                callbacks=callbacks, verbose=verbose, **toolkit_kwargs\n", "            )\n\t    else:\n\t        if toolkit is not None:\n\t            raise ValueError(\"Cannot provide both tools and toolkit, choose one\")\n\t    # output parser\n\t    output_parser = (\n\t        output_parser or SnowflakeConversationalChatAgent._get_default_output_parser()\n\t    )\n\t    # prompt for llm chain\n\t    prompt_kwargs = prompt_kwargs or {}\n", "    if toolkit is not None:\n\t        prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)\n\t        toolkit_instructions = toolkit.instructions\n\t    else:\n\t        toolkit_instructions = None\n\t    prompt = SnowflakeConversationalChatAgent.create_prompt(\n\t        tools,\n\t        toolkit_instructions=toolkit_instructions,\n\t        system_message=prefix,\n\t        human_message=suffix,\n", "        input_variables=input_variables,\n\t        output_parser=output_parser,\n\t        # prefix=prefix,\n\t        # suffix=suffix,\n\t        format_instructions=format_instructions if format_instructions else None\n\t        # system_template=system_template,\n\t        # human_template=human_template,\n\t        ** prompt_kwargs,\n\t    )\n\t    # llm chain\n", "    llm_chain_kwargs = llm_chain_kwargs or {}\n\t    llm_chain = LLMChain(\n\t        llm=llm,\n\t        prompt=prompt,\n\t        # memory=memory,\n\t        callbacks=callbacks,\n\t        verbose=verbose,\n\t        **llm_chain_kwargs,\n\t    )\n\t    # agent\n", "    agent_kwargs = agent_kwargs or {}\n\t    tool_names = [tool.name for tool in tools]\n\t    agent = SnowflakeConversationalChatAgent(\n\t        llm_chain=llm_chain,\n\t        allowed_tools=tool_names,\n\t        output_parser=output_parser,\n\t        **agent_kwargs,\n\t        # **kwargs,\n\t    )\n\t    # agent executor\n", "    agent_executor_kwargs = agent_executor_kwargs or {}\n\t    return AgentExecutor.from_agent_and_tools(\n\t        agent=agent,\n\t        tools=tools,\n\t        max_iterations=max_iterations,\n\t        max_execution_time=max_execution_time,\n\t        early_stopping_method=early_stopping_method,\n\t        return_intermediate_steps=return_intermediate_steps,\n\t        memory=memory,\n\t        callbacks=callbacks,\n", "        verbose=verbose,\n\t        **agent_executor_kwargs,\n\t    )\n\tdef create_snowflake_chat_agent(\n\t    # shared by agent and agent executor chain\n\t    toolkit: SnowflakeDatabaseToolkit,\n\t    # for llm chain of agent\n\t    llm: BaseChatModel,\n\t    # for prompt of llm chain\n\t    prefix: str = SNOWFLAKE_PREFIX,\n", "    suffix: str = SNOWFLAKE_SUFFIX,\n\t    format_instructions: str = FORMAT_INSTRUCTIONS,\n\t    input_variables: Optional[List[str]] = None,\n\t    top_k: int = 10,\n\t    system_template: Optional[str] = None,\n\t    human_template: Optional[str] = None,\n\t    # for agent\n\t    output_parser: AgentOutputParser = ChatOutputParser(),\n\t    # for agent executor\n\t    max_iterations: Optional[int] = 10,\n", "    max_execution_time: Optional[float] = 20,\n\t    early_stopping_method: Optional[str] = \"force\",\n\t    return_intermediate_steps: Optional[bool] = False,\n\t    # for chains\n\t    callbacks: Optional[BaseCallbackManager] = None,\n\t    verbose: bool = False,\n\t    memory: Optional[BaseMemory] = None,\n\t    # additional kwargs\n\t    toolkit_kwargs: Optional[dict] = None,\n\t    prompt_kwargs: Optional[dict] = None,\n", "    agent_executor_kwargs: Optional[dict] = None,\n\t    agent_kwargs: Optional[dict] = None,\n\t    llm_chain_kwargs: Optional[dict] = None,\n\t    # **kwargs: Any,\n\t) -> AgentExecutor:\n\t    \"\"\"\n\t    Construct a sql chat agent from an LLM and tools.\n\t    \"\"\"\n\t    # tools from toolkit\n\t    toolkit_kwargs = toolkit_kwargs or {}\n", "    tools = toolkit.get_tools(callbacks=callbacks, verbose=verbose, **toolkit_kwargs)\n\t    # prompt for llm chain\n\t    prompt_kwargs = prompt_kwargs or {}\n\t    prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)\n\t    if isinstance(toolkit, CustomSnowflakeDatabaseToolkit):\n\t        instructions = toolkit.instructions\n\t    else:\n\t        instructions = \"\"\n\t    prompt = SnowflakeChatAgent.create_prompt(\n\t        tools,\n", "        toolkit_instructions=instructions,\n\t        prefix=prefix,\n\t        suffix=suffix,\n\t        format_instructions=format_instructions,\n\t        input_variables=input_variables,\n\t        system_template=system_template,\n\t        human_template=human_template,\n\t        **prompt_kwargs,\n\t    )\n\t    # llm chain\n", "    llm_chain_kwargs = llm_chain_kwargs or {}\n\t    llm_chain = LLMChain(\n\t        llm=llm,\n\t        prompt=prompt,\n\t        memory=memory,\n\t        callbacks=callbacks,\n\t        verbose=verbose,\n\t        **llm_chain_kwargs,\n\t    )\n\t    # agent\n", "    agent_kwargs = agent_kwargs or {}\n\t    tool_names = [tool.name for tool in tools]\n\t    agent = SnowflakeChatAgent(\n\t        llm_chain=llm_chain,\n\t        allowed_tools=tool_names,\n\t        output_parser=output_parser,\n\t        **agent_kwargs,\n\t        # **kwargs,\n\t    )\n\t    # agent executor\n", "    agent_executor_kwargs = agent_executor_kwargs or {}\n\t    return AgentExecutor.from_agent_and_tools(\n\t        agent=agent,\n\t        tools=tools,\n\t        max_iterations=max_iterations,\n\t        max_execution_time=max_execution_time,\n\t        early_stopping_method=early_stopping_method,\n\t        return_intermediate_steps=return_intermediate_steps,\n\t        memory=memory,\n\t        callbacks=callbacks,\n", "        verbose=verbose,\n\t        **agent_executor_kwargs,\n\t    )\n\tdef create_snowflake_agent(\n\t    llm: BaseLLM,\n\t    toolkit: SnowflakeDatabaseToolkit,\n\t    callback_manager: Optional[BaseCallbackManager] = None,\n\t    prefix: str = SNOWFLAKE_PREFIX,\n\t    suffix: str = SNOWFLAKE_SUFFIX,\n\t    format_instructions: str = FORMAT_INSTRUCTIONS,\n", "    input_variables: Optional[List[str]] = None,\n\t    top_k: int = 10,\n\t    max_iterations: Optional[int] = 15,\n\t    max_execution_time: Optional[float] = None,\n\t    early_stopping_method: str = \"force\",\n\t    verbose: bool = False,\n\t    **kwargs: Any,\n\t) -> AgentExecutor:\n\t    return create_sql_agent(\n\t        llm=llm,\n", "        toolkit=toolkit,\n\t        callback_manager=callback_manager,\n\t        prefix=prefix,\n\t        suffix=suffix,\n\t        format_instructions=format_instructions,\n\t        input_variables=input_variables,\n\t        top_k=top_k,\n\t        max_iterations=max_iterations,\n\t        max_execution_time=max_execution_time,\n\t        early_stopping_method=early_stopping_method,\n", "        verbose=verbose,\n\t        **kwargs,\n\t    )\n\tdef create_sql_chat_agent(\n\t    # shared by agent and agent executor chain\n\t    toolkit: SQLDatabaseToolkit,\n\t    # for llm chain of agent\n\t    llm: BaseChatModel,\n\t    # for prompt of llm chain\n\t    prefix: str = SQL_PREFIX,\n", "    suffix: str = SQL_SUFFIX,\n\t    format_instructions: str = FORMAT_INSTRUCTIONS,\n\t    input_variables: Optional[List[str]] = None,\n\t    top_k: int = 10,\n\t    # for agent\n\t    output_parser: AgentOutputParser = ChatOutputParser(),\n\t    # for agent executor\n\t    max_iterations: Optional[int] = 10,\n\t    max_execution_time: Optional[float] = 20,\n\t    early_stopping_method: Optional[str] = \"force\",\n", "    return_intermeidate_steps: Optional[bool] = False,\n\t    # for chains\n\t    callback_manager: Optional[BaseCallbackManager] = None,\n\t    verbose: bool = False,\n\t    # memory: Optional[BaseMemory] = None,\n\t    # additional kwargs\n\t    # toolkit_kwargs: Optional[dict] = None,\n\t    # prompt_kwargs: Optional[dict] = None,\n\t    # agent_executor_kwargs: Optional[dict] = None,\n\t    # agent_kwargs: Optional[dict] = None,\n", "    # llm_chain_kwargs: Optional[dict] = None,\n\t    **kwargs: Any,\n\t) -> AgentExecutor:\n\t    \"\"\"\n\t    Construct a sql chat agent from an LLM and tools.\n\t    \"\"\"\n\t    tools = toolkit.get_tools()\n\t    prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)\n\t    prompt = ChatAgent.create_prompt(\n\t        tools,\n", "        prefix=prefix,\n\t        suffix=suffix,\n\t        format_instructions=format_instructions,\n\t        input_variables=input_variables,\n\t        #    **prompt_kwargs,\n\t    )\n\t    llm_chain = LLMChain(\n\t        llm=llm,\n\t        prompt=prompt,\n\t        callback_manager=callback_manager,\n", "        verbose=verbose,\n\t    )\n\t    tool_names = [tool.name for tool in tools]\n\t    agent = ChatAgent(\n\t        llm_chain=llm_chain,\n\t        allowed_tools=tool_names,\n\t        output_parser=output_parser,\n\t    )\n\t    return AgentExecutor.from_agent_and_tools(\n\t        agent=agent,\n", "        tools=tools,\n\t        max_iterations=max_iterations,\n\t        max_execution_time=max_execution_time,\n\t        early_stopping_method=early_stopping_method,\n\t        return_intermeidate_steps=return_intermeidate_steps,\n\t        callback_manager=callback_manager,\n\t        verbose=verbose,\n\t    )\n"]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/toolkit.py", "chunked_list": ["\"\"\"Toolkit for interacting with Snowflake databases.\"\"\"\n\tfrom typing import List, Optional, Union, cast\n\tfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\n\tfrom langchain.callbacks.base import BaseCallbackManager\n\tfrom langchain.chains.llm import LLMChain\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom langchain.chat_models.base import BaseChatModel\n\tfrom langchain.memory.readonly import ReadOnlySharedMemory\n\tfrom langchain.prompts.chat import (\n\t    BaseMessagePromptTemplate,\n", "    ChatPromptTemplate,\n\t    HumanMessagePromptTemplate,\n\t    SystemMessagePromptTemplate,\n\t)\n\tfrom langchain.schema import BaseMessage\n\tfrom langchain.tools import BaseTool\n\tfrom pydantic import Field\n\tfrom chatweb3.snowflake_database import SnowflakeContainer\n\tfrom chatweb3.tools.snowflake_database.prompt import SNOWFLAKE_QUERY_CHECKER\n\tfrom chatweb3.tools.snowflake_database.tool import (\n", "    GetSnowflakeDatabaseTableMetadataTool,\n\t    ListSnowflakeDatabaseTableNamesTool,\n\t    QuerySnowflakeDatabaseTool,\n\t    SnowflakeQueryCheckerTool,\n\t)\n\tclass SnowflakeDatabaseToolkit(SQLDatabaseToolkit):\n\t    \"\"\"Snowflake database toolkit.\"\"\"\n\t    # override the db attribute to be a SnowflakeContainer\n\t    # it contains a dictionary of SnowflakeDatabase objects\n\t    db: SnowflakeContainer = Field(exclude=True)  # type: ignore[assignment]\n", "    # override the llm attribute to be a ChatOpenAI object\n\t    llm: BaseChatModel = Field(default_factory=lambda: ChatOpenAI(temperature=0))  # type: ignore[call-arg]\n\t    readonly_shared_memory: ReadOnlySharedMemory = Field(default=None)\n\t    def get_tools(\n\t        self,\n\t        callback_manager: Optional[BaseCallbackManager] = None,\n\t        verbose: bool = False,\n\t        **kwargs,\n\t    ) -> List[BaseTool]:\n\t        \"\"\"Get the tools available in the toolkit.\n", "        Returns:\n\t            The tools available in the toolkit.\n\t        \"\"\"\n\t        # if input_variables is None:\n\t        messages = [\n\t            SystemMessagePromptTemplate.from_template(template=SNOWFLAKE_QUERY_CHECKER),\n\t            HumanMessagePromptTemplate.from_template(template=\"\\n\\n{query}\"),\n\t        ]\n\t        input_variables = [\"query\", \"dialect\"]\n\t        checker_llm_chain = LLMChain(\n", "            llm=self.llm,\n\t            prompt=ChatPromptTemplate(\n\t                input_variables=input_variables,\n\t                messages=cast(\n\t                    List[Union[BaseMessagePromptTemplate, BaseMessage]], messages\n\t                ),  # casting it for now XXX\n\t            ),\n\t            callback_manager=callback_manager,\n\t            memory=self.readonly_shared_memory,\n\t            verbose=verbose,\n", "        )\n\t        return [\n\t            QuerySnowflakeDatabaseTool(db=self.db),  # type: ignore[arg-type]\n\t            ListSnowflakeDatabaseTableNamesTool(db=self.db),  # type: ignore[arg-type]\n\t            GetSnowflakeDatabaseTableMetadataTool(db=self.db),  # type: ignore[arg-type]\n\t            SnowflakeQueryCheckerTool(\n\t                db=self.db,  # type: ignore[arg-type]\n\t                template=SNOWFLAKE_QUERY_CHECKER,\n\t                llm=self.llm,\n\t                llm_chain=checker_llm_chain,\n", "                callback_manager=callback_manager,\n\t                verbose=verbose,\n\t            ),  # type: ignore[call-arg]\n\t        ]\n"]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/__init__.py", "chunked_list": []}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/prompt.py", "chunked_list": ["# flake8: noqa\n\t# these prompts have been tuned for the chat model output parser\n\tSNOWFLAKE_PREFIX = \"\"\"You are an agent designed to interact with Snowflake databases.\n\tGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n\tUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\n\tYou can order the results by a relevant column to return the most interesting examples in the database.\n\tNever query for all the columns from a specific table, only ask for the relevant columns given the question.\n\tYou have access to tools for interacting with the database.\n\tOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\n\tYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n", "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\tIf the question does not seem related to the database, just return \"Sorry I don't know ...\" as the answer.\n\t\"\"\"\n\tSNOWFLAKE_SUFFIX = \"\"\"\n\tYou MUST ALWAYS first find out what database(s), schema(s) and table(s) are available before you take any other actions such as retrieving metadata about tables or creating queries for tables. \n\tYou MUST ALWAYS first confirm that a table exists in the database AND then retrieve and examined the table's metadata BEFORE you can create a query for that table and submit to the query checker.\n\tBegin!\n\t\"\"\"\n\tCUSTOM_SNOWFLAKE_PREFIX = \"\"\"You are an agent designed to interact with Snowflake databases.\n\tGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n", "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\n\tNever query for all the columns from a specific table, only ask for the relevant columns given the question.\n\tYou have access to specific tools for interacting with the database.\n\tYou must ONLY use these specified tools. Only use the information returned by the below tools to construct your final answer.\n\tDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. You can use `date_trunc` to group dates in the queries when you need to.\n\tIf the question does not seem related to the database, just return 'I don't know' as the answer.\n\t\"\"\"\n\tCUSTOM_SNOWFLAKE_SUFFIX = \"\"\"\n\tFinally, remember to use concise responses so you have space to include the action and action inputs in the response whenever needed.  \n\tBegin!\n", "\"\"\"\n\tCONV_SNOWFLAKE_PREFIX = \"\"\"Assistant is a large language model trained by OpenAI.\n\tAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\tAssistant is especially capable of leveraging a list of tools specfied by the human to interact with Snowflake databases. Given an input question, assistant can help human select the right tool to use and provide correct inputs to these tools. Based on humans' response and their observation from using the tools assistant suggested, assistant can create a syntactically correct {dialect} query for human to run in order to obtain answer to the input question. \n\tWhen generating {dialect} queries: \n\t1. Unless the human specifies a specific number of examples they wish to obtain, assistant will always limit its query to at most {top_k} results. \n\t2. Assistant's query may order the results by a relevant column to return the most interesting examples in the database. \n\t3. Assistant will never query for all the columns from a specific table, only ask for the relevant columns given the question. \n\t4. Assistant must ONLY use tools specified by the users and MUST follow the tool usage instructions provided by human.\n\t5. Assistant MUST NOT generate any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. \n", "6. Assistant may use `date_trunc` to group dates in the queries when you need to. \n\tAssistant MUST ONLY generate {dialect} queries based on database metadata information as human provided. If it is not possible to generate {dialect} queries based on the provided database metadata information, assistant can just return 'I don't know' as the answer.\n\t\"\"\"\n\tCONV_SNOWFLAKE_SUFFIX = \"\"\"TOOLS\n\t------\n\tAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n\t{{tools}}\n\t{format_instructions}\n\tUSER'S INPUT\n\t--------------------\n", "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\t{{{{input}}}}\n\t\"\"\"\n\tCUSTOM_CONV_SNOWFLAKE_PREFIX = \"\"\"Assistant is a large language model trained by OpenAI.\n\tAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\tAssistant is especially capable of leveraging a list of tools specfied by the human to interact with Snowflake databases. Given an input question, assistant can help human select the right tool to use and provide correct inputs to these tools. Based on humans' response and their observation from using the tools assistant suggested, assistant can create a syntactically correct {dialect} query for human to run in order to obtain answer to the input question. \n\tWhen generating {dialect} queries: \n\t- Unless the human specifies a specific number of examples they wish to obtain, assistant will always limit its query to at most {top_k} results. \n\t- Assistant's query may order the results by a relevant column to return the most interesting examples in the database. \n\t- Assistant MUST NOT generate any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. \n", "- Assistant may use `date_trunc` to group dates in the queries when you need to. \n\tAssistant can ask the human to use the following database related tools to help generate {dialect} queries to answer the human's original question. \n\t{{{{tools}}}}\n\tIMPORTANT: \n\t1. Assistant must ALWAYS check available tables first! That is, NEVER EVER start with checking metadata tools or query database tools, ALWAYS start with the tool that tells you what tables are available in the database. \n\t2. Before generating ANY {dialect} query, assistant MUST first check the metadata of the table the query will be run against. NEVER EVER generate a {dialect} query without checking the metadata of the table first.\n\t3. If the assistant checked the tables in the database and found no table is related to the the human's specific question, assistant MUST NOT generate any {dialect} queries, and MUST respond 'I don't know' as the answer, and ask the human to provide more information.\n\t------\n\t{{format_instructions}}\n\t\"\"\"\n", "CUSTOM_FORMAT_INSTRUCTIONS = \"\"\"RESPONSE FORMAT INSTRUCTIONS\n\t----------------------------\n\tWhen responding to me, please output a response in one of two formats:\n\t**Option 1:**\n\tUse this if you want the human to use a tool.\n\tMarkdown code snippet formatted in the following schema:\n\t```json\n\t{{{{\n\t    \"action\": string \\\\ The action to take. Must be one of {tool_names}\n\t    \"action_input\": string \\\\ The input to the action\n", "}}}}\n\t```\n\tBoth `action` and `action_input` MUST be provided, even if  `action_input` is an empty string!\n\t**Option #2:**\n\tUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n\t```json\n\t{{{{\n\t    \"action\": \"Final Answer\",\n\t    \"action_input\": string \\\\ You should put what you want to return to use here\n\t}}}}\n", "```\"\"\"\n\tCUSTOM_CONV_SNOWFLAKE_SUFFIX = \"\"\"\n\t------\n\tUSER'S INPUT\n\t--------------------\n\tHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\t{input}\n\t\"\"\"\n"]}
{"filename": "chatweb3/agents/agent_toolkits/snowflake/toolkit_custom.py", "chunked_list": ["from typing import List, Optional, Union, cast\n\tfrom langchain.callbacks.base import BaseCallbackManager\n\tfrom langchain.chains.llm import LLMChain\n\tfrom langchain.prompts.chat import (\n\t    BaseMessagePromptTemplate,\n\t    ChatPromptTemplate,\n\t    HumanMessagePromptTemplate,\n\t    SystemMessagePromptTemplate,\n\t)\n\tfrom langchain.schema import BaseMessage\n", "from langchain.tools import BaseTool\n\tfrom pydantic import Field\n\tfrom chatweb3.agents.agent_toolkits.snowflake.toolkit import (\n\t    SnowflakeDatabaseToolkit,\n\t)\n\tfrom chatweb3.tools.snowflake_database.prompt import SNOWFLAKE_QUERY_CHECKER\n\tfrom chatweb3.tools.snowflake_database.tool_custom import (\n\t    TOOLKIT_INSTRUCTIONS,\n\t    CheckQuerySyntaxTool,\n\t    CheckTableMetadataTool,\n", "    CheckTableSummaryTool,\n\t    QueryDatabaseTool,\n\t)\n\tfrom config.config import agent_config\n\tclass CustomSnowflakeDatabaseToolkit(SnowflakeDatabaseToolkit):\n\t    \"\"\"Toolkit for interacting with FPS databases.\"\"\"\n\t    instructions = Field(default=TOOLKIT_INSTRUCTIONS)\n\t    def get_tools(\n\t        self,\n\t        callback_manager: Optional[BaseCallbackManager] = None,\n", "        verbose: bool = False,\n\t        # input_variables: Optional[List[str]] = None,\n\t        **kwargs,\n\t    ) -> List[BaseTool]:\n\t        \"\"\"Get the tools available in the toolkit.\n\t        Returns:\n\t            The tools available in the toolkit.\n\t        \"\"\"\n\t        # if input_variables is None:\n\t        input_variables = [\"query\", \"dialect\"]\n", "        messages = [\n\t            SystemMessagePromptTemplate.from_template(template=SNOWFLAKE_QUERY_CHECKER),\n\t            HumanMessagePromptTemplate.from_template(template=\"\\n\\n{query}\"),\n\t        ]\n\t        checker_llm_chain = LLMChain(\n\t            llm=self.llm,\n\t            prompt=ChatPromptTemplate(\n\t                input_variables=input_variables,\n\t                messages=cast(\n\t                    List[Union[BaseMessagePromptTemplate, BaseMessage]], messages\n", "                ),\n\t                # messages=messages\n\t            ),\n\t            callback_manager=callback_manager,\n\t            verbose=verbose,\n\t        )\n\t        return [\n\t            CheckTableSummaryTool(\n\t                db=self.db, callback_manager=callback_manager, verbose=verbose  # type: ignore[call-arg, arg-type]\n\t            ),\n", "            CheckTableMetadataTool(\n\t                db=self.db, callback_manager=callback_manager, verbose=verbose  # type: ignore[call-arg, arg-type]\n\t            ),\n\t            CheckQuerySyntaxTool(  # type: ignore[call-arg]\n\t                db=self.db,  # type: ignore[arg-type]\n\t                template=SNOWFLAKE_QUERY_CHECKER,\n\t                llm=self.llm,\n\t                llm_chain=checker_llm_chain,\n\t                callback_manager=callback_manager,\n\t                verbose=verbose,\n", "            ),\n\t            QueryDatabaseTool(  # type: ignore[call-arg]\n\t                db=self.db,  # type: ignore[arg-type]\n\t                return_direct=agent_config.get(\n\t                    \"tool.query_database_tool_return_direct\"\n\t                ),\n\t                callback_manager=callback_manager,\n\t                verbose=verbose,\n\t            ),\n\t        ]\n"]}
{"filename": "chatweb3/callbacks/__init__.py", "chunked_list": []}
{"filename": "chatweb3/callbacks/logger_callback.py", "chunked_list": ["\"\"\"Callback Handler that logs debugging information\"\"\"\n\timport logging\n\tfrom typing import Any, Dict, List, Optional, Union\n\tfrom uuid import UUID\n\tfrom langchain.callbacks.base import BaseCallbackHandler\n\tfrom langchain.schema import AgentAction, AgentFinish, LLMResult\n\tfrom config.logging_config import get_logger\n\tlogger = get_logger(\n\t    __name__, log_level=logging.INFO, log_to_console=True, log_to_file=True\n\t)\n", "class LoggerCallbackHandler(BaseCallbackHandler):\n\t    \"\"\"Callback Handler that prints to std out.\"\"\"\n\t    def __init__(self, color: Optional[str] = None) -> None:\n\t        \"\"\"Initialize callback handler.\"\"\"\n\t        self.color = color\n\t    def on_llm_start(\n\t        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Print out the prompts.\"\"\"\n\t        class_name = serialized[\"name\"]\n", "        logger.debug(f\"Starting lLM: {class_name} with prompts: {prompts}\")\n\t    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n\t        \"\"\"Print out the response.\"\"\"\n\t        logger.debug(f\"LLM response: {response}\")\n\t    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n\t        \"\"\"Print out new token.\"\"\"\n\t        logger.debug(f\"LLM new token: {token}\")\n\t    def on_llm_error(\n\t        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n\t    ) -> None:\n", "        \"\"\"Print out LLM error.\"\"\"\n\t        logger.debug(f\"LLM error: {error}\")\n\t    def on_chain_start(\n\t        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Print out that we are entering a chain.\"\"\"\n\t        class_name = serialized[\"name\"]\n\t        logger.debug(\n\t            f\"\\n\\n\\033[1m> Entering new {class_name} chain\\033[0m with inputs: {inputs}\"\n\t        )\n", "        # print(f\"\\n\\n\\033[1m> Entering new {class_name} chain...\\033[0m\")\n\t    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n\t        \"\"\"Print out that we finished a chain.\"\"\"\n\t        logger.debug(f\"\\n\\033[1m> Finished chain.\\033[0m with outputs: {outputs}\")\n\t        # print(\"\\n\\033[1m> Finished chain.\\033[0m\")\n\t    def on_chain_error(\n\t        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Print out chain error\"\"\"\n\t        logger.debug(f\"Chain error: {error}\")\n", "    def on_tool_start(\n\t        self,\n\t        serialized: Dict[str, Any],\n\t        input_str: str,\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        \"\"\"Print out tool start.\"\"\"\n\t        # tool_name = serialized[\"name\"]\n\t        # tool_description = serialized[\"description\"]\n\t        logger.debug(f\"Starting tool: {serialized} with input: {input_str}\")\n", "    def on_agent_action(\n\t        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n\t    ) -> Any:\n\t        \"\"\"Run on agent action.\"\"\"\n\t        # print_text(action.log, color=color if color else self.color)\n\t        logger.debug(f\"Agent action: {action}\")\n\t    def on_tool_end(\n\t        self,\n\t        output: str,\n\t        color: Optional[str] = None,\n", "        observation_prefix: Optional[str] = None,\n\t        llm_prefix: Optional[str] = None,\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        \"\"\"If not the final action, print out observation.\"\"\"\n\t        if observation_prefix is not None:\n\t            # print_text(f\"\\n{observation_prefix}\")\n\t            logger.debug(f\"Not final action since {observation_prefix=} is not None\")\n\t        # print_text(output, color=color if color else self.color)\n\t        logger.debug(f\"Tool ended with {output=}\")\n", "        if llm_prefix is not None:\n\t            # print_text(f\"\\n{llm_prefix}\")\n\t            logger.debug(f\"{llm_prefix=} is not None\")\n\t    def on_tool_error(\n\t        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Print out tool error.\"\"\"\n\t        logger.debug(f\"Tool error: {error}\")\n\t    def on_text(\n\t        self,\n", "        text: str,\n\t        *,\n\t        run_id: UUID,\n\t        parent_run_id: Optional[UUID] = None,\n\t        **kwargs: Any,\n\t    ) -> None:\n\t        \"\"\"Run when agent ends.\"\"\"\n\t        # print_text(text, color=color if color else self.color, end=end)\n\t        logger.debug(f\"on text: {text}\")\n\t    def on_agent_finish(\n", "        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n\t    ) -> None:\n\t        \"\"\"Run on agent end.\"\"\"\n\t        logger.debug(f\"Agent finished with {finish=}\")\n\t    def log_with_context(\n\t        self, msg: str, pathname: str, lineno: int, func_name: str\n\t    ) -> None:\n\t        extra = {\n\t            \"pathname\": pathname,\n\t            \"lineno\": lineno,\n", "            \"funcName\": func_name,\n\t        }\n\t        logger.debug(msg, extra=extra)\n"]}
{"filename": "config/logging_config.py", "chunked_list": ["\"\"\"\n\tlogging_config.py\n\tThis file contains the code for configuring the logger.\n\t\"\"\"\n\timport inspect\n\timport logging\n\timport os\n\timport sys\n\tfrom typing import Optional\n\t# Define CustomLoggerAdapter class\n", "class CustomLoggerAdapter(logging.LoggerAdapter):\n\t    def process(self, msg, kwargs):\n\t        class_name = \"\"\n\t        for frame_info in inspect.stack()[2:]:\n\t            frame = frame_info.frame\n\t            local_self = frame.f_locals.get(\"self\")\n\t            if local_self and not isinstance(local_self, CustomLoggerAdapter):\n\t                class_name = local_self.__class__.__name__\n\t                break\n\t        if class_name:\n", "            msg = f\"{class_name}: {msg}\"\n\t        return msg, kwargs\n\t# Helper function to configure logger handlers\n\tdef _configure_handlers(\n\t    logger,\n\t    log_level,\n\t    log_to_console,\n\t    log_to_file,\n\t    log_format,\n\t    date_format,\n", "    log_file_path,\n\t):\n\t    handlers = []\n\t    if log_to_console:\n\t        console_handler = logging.StreamHandler(sys.stdout)\n\t        console_handler.setLevel(log_level)\n\t        console_handler.setFormatter(logging.Formatter(log_format, datefmt=date_format))\n\t        handlers.append(console_handler)\n\t    if log_to_file:\n\t        file_handler = logging.FileHandler(log_file_path)\n", "        file_handler.setLevel(log_level)\n\t        file_handler.setFormatter(logging.Formatter(log_format, datefmt=date_format))\n\t        handlers.append(file_handler)\n\t    for handler in handlers:\n\t        logger.addHandler(handler)\n\tdef _get_log_file_path():\n\t    # Get the path of the current module\n\t    current_module_path = os.path.abspath(__file__)\n\t    # Get the path of the directory where the current module is located\n\t    config_directory = os.path.dirname(current_module_path)\n", "    # Get the path to the project root (assuming the config directory is located one level below the project root)\n\t    project_root = os.path.dirname(config_directory)\n\t    # Define the path for the log directory\n\t    log_directory = os.path.join(project_root, \"logs\")\n\t    os.makedirs(log_directory, exist_ok=True)\n\t    # Determine the project name from the project_root path\n\t    project_name = os.path.basename(project_root)\n\t    # Define the log file name and path\n\t    log_file_name = f\"{project_name}.log\"\n\t    log_file_path = os.path.join(log_directory, log_file_name)\n", "    return log_file_path\n\t# default parameters\n\tlog_level = logging.INFO\n\tlog_to_console = True\n\tlog_to_file = False\n\tlog_format = \"%(asctime)s [%(name)s] [%(levelname)s] [%(module)s:%(lineno)d] [%(funcName)s]: %(message)s\"\n\tdate_format = \"%Y-%m-%d %H:%M:%S\"\n\tlog_file_path = _get_log_file_path()\n\t# Configure the root logger\n\troot_logger = logging.getLogger()\n", "_configure_handlers(\n\t    root_logger,\n\t    log_level=log_level,\n\t    log_to_console=log_to_console,\n\t    log_to_file=log_to_file,\n\t    log_format=log_format,\n\t    date_format=date_format,\n\t    log_file_path=log_file_path,\n\t)\n\troot_logger_adapter = CustomLoggerAdapter(root_logger, {})\n", "# Log a statement during the root logger adapter initialization\n\troot_logger_adapter.info(\n\t    f\"\"\"LOGGER initialized with the following options:\n\tLog level: {log_level}\n\tLog to console: {log_to_console}\n\tLog to file: {log_to_file}\n\tLog file path: {log_file_path}\n\t\"\"\"\n\t)\n\t# Child loggers will inherit the attributes from the root logger unless they are explicitly overridden\n", "def get_logger(\n\t    name: str,\n\t    log_level: Optional[int] = None,\n\t    log_to_console: Optional[bool] = None,\n\t    log_to_file: Optional[bool] = None,\n\t    log_format: Optional[str] = None,\n\t    date_format: Optional[str] = None,\n\t    log_file_path: Optional[str] = None,\n\t):\n\t    logger = logging.getLogger(name)\n", "    if logger.handlers:\n\t        return CustomLoggerAdapter(logger, {})\n\t    if log_level is not None:\n\t        logger.setLevel(log_level)\n\t    else:\n\t        logger.setLevel(logging.getLogger().level)  # Inherit log level from root logger\n\t    if log_to_file and log_file_path is None:\n\t        log_file_path = next(\n\t            (\n\t                handler.baseFilename\n", "                for handler in root_logger.handlers\n\t                if isinstance(handler, logging.FileHandler)\n\t            ),\n\t            None,\n\t        )  # Use root logger's log_file_path\n\t    if log_to_console is None:\n\t        log_to_console = any(\n\t            isinstance(handler, logging.StreamHandler)\n\t            for handler in root_logger.handlers\n\t        )\n", "    if log_to_file is None:\n\t        log_to_file = any(\n\t            isinstance(handler, logging.FileHandler) for handler in root_logger.handlers\n\t        )\n\t    if log_format is None:\n\t        if root_logger.handlers[0].formatter is not None:\n\t            log_format = root_logger.handlers[0].formatter._fmt\n\t    if date_format is None:\n\t        if root_logger.handlers[0].formatter is not None:\n\t            date_format = root_logger.handlers[0].formatter.datefmt\n", "    if log_to_file and log_file_path is None:\n\t        log_file_path = (\n\t            _get_log_file_path()\n\t        )  # Assign default log file path if not found in root_logger and log_to_file is True\n\t    _configure_handlers(\n\t        logger,\n\t        log_level=logger.level,\n\t        log_to_console=log_to_console,\n\t        log_to_file=log_to_file,\n\t        log_format=log_format,\n", "        date_format=date_format,\n\t        log_file_path=log_file_path,\n\t    )\n\t    # Add this line to prevent propagation of log messages to ancestor loggers\n\t    logger.propagate = False\n\t    logger_adapter = CustomLoggerAdapter(logger, {})\n\t    if logger.level == logging.DEBUG:\n\t        logger_adapter.info(\n\t            f\"\\n\\n\\n *** Initialized '{name}' LOGGER with level '{logging.getLevelName(logger.level)}'\"\n\t        )\n", "    return logger_adapter\n"]}
{"filename": "config/config.py", "chunked_list": ["\"\"\"\n\tconfig.py\n\tThis file contains the configuration for the chatbot application.\n\t\"\"\"\n\timport os\n\timport yaml\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tclass Config:\n\t    def __init__(self, config_file):\n", "        with open(config_file, \"r\") as f:\n\t            self.config = yaml.safe_load(f)\n\t        self.config[\"proj_root_dir\"] = os.path.dirname(\n\t            os.path.dirname(os.path.abspath(__file__))\n\t        )\n\t        self.config[\"tool\"][\"query_database_tool_return_direct\"] = (\n\t            True if self.config[\"tool\"][\"query_database_tool_top_k\"] > 10 else False\n\t        )\n\t        # Load from environment variables\n\t        self.config[\"snowflake_params\"] = {\n", "            \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n\t            \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n\t            \"account_identifier\": os.getenv(\"SNOWFLAKE_ACCOUNT_IDENTIFIER\"),\n\t        }\n\t        self.config[\"shroomdk_params\"] = {\n\t            \"shroomdk_api_key\": os.getenv(\"SHROOMDK_API_KEY\"),\n\t        }\n\t    def get(self, path, default=None):\n\t        keys = path.split(\".\")\n\t        value = self.config\n", "        for key in keys:\n\t            if isinstance(value, dict):\n\t                value = value.get(key)\n\t            else:\n\t                return default\n\t        return value\n\t    def set(self, path, value):\n\t        keys = path.split(\".\")\n\t        current_level = self.config\n\t        for key in keys[:-1]:\n", "            current_level = current_level.setdefault(key, {})\n\t        current_level[keys[-1]] = value\n\tagent_config = Config(\n\t    os.path.join(os.path.dirname(os.path.abspath(__file__)), \"config.yaml\")\n\t)\n"]}
{"filename": "config/__init__.py", "chunked_list": []}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/conftest.py", "chunked_list": ["\"\"\"\n\tconftest.py\n\tThis file contains the fixtures for the tests.\n\t\"\"\"\n\timport pytest\n\tfrom chatweb3.metadata_parser import Column, Database, MetadataParser, Schema, Table\n\tfrom chatweb3.snowflake_database import SnowflakeContainer\n\tfrom config.config import agent_config\n\tfrom create_agent import INDEX_ANNOTATION_FILE_PATH, LOCAL_INDEX_FILE_PATH\n\t@pytest.fixture(scope=\"module\")\n", "def snowflake_params():\n\t    return {\n\t        **agent_config.get(\"snowflake_params\"),\n\t        **agent_config.get(\"shroomdk_params\"),\n\t    }\n\t@pytest.fixture(scope=\"module\")\n\tdef snowflake_container(snowflake_params):\n\t    return SnowflakeContainer(**snowflake_params)\n\t@pytest.fixture(scope=\"module\")\n\tdef snowflake_container_eth_core(snowflake_params):\n", "    return SnowflakeContainer(\n\t        **snowflake_params,\n\t        local_index_file_path=LOCAL_INDEX_FILE_PATH,\n\t        index_annotation_file_path=INDEX_ANNOTATION_FILE_PATH,\n\t    )\n\t@pytest.fixture(scope=\"module\")\n\tdef eth_core_table_long_names_select_list():\n\t    return agent_config.get(\"ethereum_core_table_long_name.enabled_list\")\n\t@pytest.fixture\n\tdef metadata_parser_with_sample_data():\n", "    # Create the MetadataParser object\n\t    metadata_parser = MetadataParser()\n\t    # Add sample data to the parser's custom objects\n\t    # Database 1: ethereum\n\t    ethereum_db = Database(\"ethereum\")\n\t    ethereum_core_schema = Schema(\"core\", \"ethereum\")\n\t    ethereum_aave_schema = Schema(\"aave\", \"ethereum\")\n\t    # ethereum.core.ez_nft_sales\n\t    ez_nft_sales = Table(\"ez_nft_sales\", \"core\", \"ethereum\")\n\t    ez_nft_sales.comment = \"The sales of NFTs.\"\n", "    ez_nft_sales.summary = \"Summary: This table contains the sales of NFTs.\"\n\t    columns1 = {\n\t        \"block_number\": Column(\n\t            \"block_number\",\n\t            \"NUMBER(38,0)\",\n\t            \"The block number at which the NFT event occurred.\",\n\t        ),\n\t        \"event_type\": Column(\n\t            \"event_type\",\n\t            \"VARCHAR(16777216)\",\n", "            \"The type of NFT event in this transaction, either sale`, `bid_won` or `redeem`.\",\n\t        ),\n\t    }\n\t    columns1[\"block_number\"].sample_values_list = [1, 2, 3]\n\t    columns1[\"event_type\"].sample_values_list = [\"sale\", \"bid_won\", \"redeem\"]\n\t    ez_nft_sales.columns = columns1\n\t    ez_nft_sales.column_names = [\"block_number\", \"event_type\"]\n\t    ethereum_core_schema.tables[\"ez_nft_sales\"] = ez_nft_sales\n\t    ethereum_db.schemas[\"core\"] = ethereum_core_schema\n\t    # ethereum.aave.ez_proposals\n", "    ez_proposals = Table(\"ez_proposals\", \"aave\", \"ethereum\")\n\t    ez_proposals.comment = \"Aave proposals.\"\n\t    ez_proposals.summary = \"Summary: This table contains Aave proposals.\"\n\t    columns2 = {\n\t        \"block_number\": Column(\n\t            \"block_number\",\n\t            \"NUMBER(38,0)\",\n\t            \"The block number at which the NFT event occurred.\",\n\t        ),\n\t    }\n", "    columns2[\"block_number\"].sample_values_list = [10, 11, 12]\n\t    ez_proposals.columns = columns2\n\t    ez_proposals.column_names = [\"block_number\"]\n\t    ethereum_aave_schema.tables[\"ez_proposals\"] = ez_proposals\n\t    ethereum_db.schemas[\"aave\"] = ethereum_aave_schema\n\t    metadata_parser.root_schema_obj.databases[\"ethereum\"] = ethereum_db\n\t    # Database 2: polygon\n\t    polygon_db = Database(\"polygon\")\n\t    polygon_core_schema = Schema(\"core\", \"polygon\")\n\t    # polygon.core.fact_blocks\n", "    fact_blocks = Table(\"fact_blocks\", \"core\", \"polygon\")\n\t    fact_blocks.comment = \"The fact blocks on Polygon.\"\n\t    fact_blocks.summary = \"Summary: This table contains the fact blocks on Polygon.\"\n\t    columns3 = {\n\t        \"difficulty\": Column(\n\t            \"difficulty\", \"NUMBER(38,0)\", \"The effort required to mine the block\"\n\t        ),\n\t    }\n\t    columns3[\"difficulty\"].sample_values_list = [6, 7, 8]\n\t    fact_blocks.columns = columns3\n", "    fact_blocks.column_names = [\"difficulty\"]\n\t    polygon_core_schema.tables[\"fact_blocks\"] = fact_blocks\n\t    polygon_db.schemas[\"core\"] = polygon_core_schema\n\t    metadata_parser.root_schema_obj.databases[\"polygon\"] = polygon_db\n\t    return metadata_parser\n"]}
{"filename": "tests/integration_tests/test_metadata_parser.py", "chunked_list": ["\"\"\"\n\ttest_metadata_parser.py\n\tThis file contains the tests for the metadata_parser module.\n\t\"\"\"\n\tdef test_get_metadata_by_table_long_names(metadata_parser_with_sample_data):\n\t    table_long_names = \"ethereum.core.ez_nft_sales, ethereum.aave.ez_proposals, polygon.core.fact_blocks\"\n\t    expected_output = \"'ethereum.aave.ez_proposals': the 'ez_proposals' table in 'aave' schema of 'ethereum' database. \\nComment: Aave proposals.\\nColumns in this table:\\n\\tName | Comment | Data type | List of sample values\\n\\t--- | --- | --- | ---\\n\\tblock_number | None | None | 10, 11, 12\\n\\n'ethereum.core.ez_nft_sales': the 'ez_nft_sales' table in 'core' schema of 'ethereum' database. \\nComment: The sales of NFTs.\\nColumns in this table:\\n\\tName | Comment | Data type | List of sample values\\n\\t--- | --- | --- | ---\\n\\tblock_number | None | None | 1, 2, 3\\n\\tevent_type | None | None | sale, bid_won, redeem\\n\\n'polygon.core.fact_blocks': the 'fact_blocks' table in 'core' schema of 'polygon' database. \\nComment: The fact blocks on Polygon.\\nColumns in this table:\\n\\tName | Comment | Data type | List of sample values\\n\\t--- | --- | --- | ---\\n\\tdifficulty | None | None | 6, 7, 8\"\n\t    result = metadata_parser_with_sample_data.get_metadata_by_table_long_names(\n\t        table_long_names\n\t    )\n", "    assert result == expected_output\n\t    assert \"ethereum.aave.ez_proposals\" in result\n\t    assert \"ethereum.core.ez_nft_sales\" in result\n\t    assert \"polygon.core.fact_blocks\" in result\n\t    assert \"Aave proposals\" in result\n\t    assert \"The sales of NFTs\" in result\n\t    assert \"The fact blocks on Polygon\" in result\n\t    assert \"block_number\" in result\n\t    assert \"event_type\" in result\n\t    assert \"difficulty\" in result\n", "def test_get_metadata_by_table_long_names_table_summary(\n\t    metadata_parser_with_sample_data,\n\t):\n\t    table_long_names = \"ethereum.core.ez_nft_sales, ethereum.aave.ez_proposals, polygon.core.fact_blocks\"\n\t    result1 = metadata_parser_with_sample_data.get_metadata_by_table_long_names(\n\t        table_long_names, include_column_info=False\n\t    )\n\t    result2 = metadata_parser_with_sample_data.get_metadata_by_table_long_names(\n\t        table_long_names, include_column_names=True, include_column_info=False\n\t    )\n", "    expected_result_1 = \"'ethereum.aave.ez_proposals': the 'ez_proposals' table in 'aave' schema of 'ethereum' database. Summary: This table contains Aave proposals.\\n\\n'ethereum.core.ez_nft_sales': the 'ez_nft_sales' table in 'core' schema of 'ethereum' database. Summary: This table contains the sales of NFTs.\\n\\n'polygon.core.fact_blocks': the 'fact_blocks' table in 'core' schema of 'polygon' database. Summary: This table contains the fact blocks on Polygon.\"\n\t    expected_result_2 = \"'ethereum.aave.ez_proposals': the 'ez_proposals' table in 'aave' schema of 'ethereum' database. Summary: This table contains Aave proposals.This table has the following columns: 'block_number'\\n\\n'ethereum.core.ez_nft_sales': the 'ez_nft_sales' table in 'core' schema of 'ethereum' database. Summary: This table contains the sales of NFTs.This table has the following columns: 'block_number, event_type'\\n\\n'polygon.core.fact_blocks': the 'fact_blocks' table in 'core' schema of 'polygon' database. Summary: This table contains the fact blocks on Polygon.This table has the following columns: 'difficulty'\"\n\t    assert result1 == expected_result_1\n\t    assert result2 == expected_result_2\n"]}
{"filename": "tests/integration_tests/test_snowflake_database.py", "chunked_list": ["\"\"\"\n\ttest_snowflake_database.py\n\tThis file contains the tests for the snowflake_database module.\n\t\"\"\"\n\timport logging\n\timport pytest\n\tfrom chatweb3.snowflake_database import SnowflakeContainer\n\tfrom chatweb3.tools.snowflake_database.tool import (\n\t    GetSnowflakeDatabaseTableMetadataTool,\n\t    ListSnowflakeDatabaseTableNamesTool,\n", "    QuerySnowflakeDatabaseTool,\n\t)\n\tlogging.getLogger(\"sqlalchemy.engine\").setLevel(logging.ERROR)\n\tlogging.getLogger(\"snowflake.connector\").setLevel(logging.ERROR)\n\tdef test_snowflake_container_initialization_shroomdk(snowflake_params):\n\t    container = SnowflakeContainer(**snowflake_params)\n\t    assert container.shroomdk is not None\n\t    assert container.metadata_parser is not None\n\tdef test_list_snowflake_database_table_name_tool_run_local(\n\t    snowflake_container_eth_core, eth_core_table_long_names_select_list\n", "):\n\t    tool = ListSnowflakeDatabaseTableNamesTool(db=snowflake_container_eth_core)\n\t    tool_input = \"\"\n\t    result = tool._run(tool_input, mode=\"local\")\n\t    for i in range(len(eth_core_table_long_names_select_list)):\n\t        assert eth_core_table_long_names_select_list[i] in result\n\tdef test_get_snowflake_database_table_metadata_tool_run_local_one_table(\n\t    snowflake_container_eth_core,\n\t):\n\t    tool = GetSnowflakeDatabaseTableMetadataTool(db=snowflake_container_eth_core)\n", "    tool_input = \"ethereum.core.ez_nft_transfers\"\n\t    result = tool._run(tool_input, mode=\"local\")\n\t    # logger.debug(f\"{result=}\")\n\t    print(f\"{result=}\")\n\t    assert \"ethereum.core.ez_nft_transfers\" in result\n\tdef test_get_snowflake_database_table_metadata_tool_run_local_single_schema(\n\t    snowflake_container_eth_core,\n\t):\n\t    tool = GetSnowflakeDatabaseTableMetadataTool(db=snowflake_container_eth_core)\n\t    tool_input = \"ethereum.core.ez_dex_swaps,ethereum.core.ez_nft_mints, ethereum.core.ez_nft_transfers\"\n", "    # tool_input = \"ethereum.core.dim_labels,ethereum.core.ez_dex_swaps\"\n\t    print(f\"\\n{tool_input=}\")\n\t    result = tool._run(tool_input, mode=\"local\")\n\t    # logger.debug(f\"{result=}\")\n\t    print(f\"{result=}\")\n\t    assert \"ethereum.core.ez_dex_swaps\" in result\n\t    assert \"ethereum.core.ez_nft_mints\" in result\n\t    assert \"ethereum.core.ez_nft_transfers\" in result\n\t    # Test invalid input\n\t    with pytest.raises(ValueError):\n", "        tool._run(\"invalid_input\")\n\tdef test_query_snowflake_database_tool_dict_shroomdk(snowflake_container):\n\t    tool = QuerySnowflakeDatabaseTool(db=snowflake_container)\n\t    tool_input = {\n\t        \"database\": \"ethereum\",\n\t        \"schema\": \"beacon_chain\",\n\t        \"query\": \"select * from ethereum.beacon_chain.fact_attestations limit 3\",\n\t    }\n\t    print(f\"\\n{tool_input=}\")\n\t    result = tool._run(tool_input, mode=\"shroomdk\")\n", "    print(f\"{result=}\")\n\t    num_items = len(result)\n\t    assert num_items == 3\n\tdef test_query_snowflake_database_tool_dict_str_shroomdk(snowflake_container):\n\t    tool = QuerySnowflakeDatabaseTool(db=snowflake_container)\n\t    tool_input = \"database: ethereum, schema: beacon_chain, query: select * from ethereum.beacon_chain.fact_attestations limit 3\"\n\t    print(f\"\\n{tool_input=}\")\n\t    result = tool._run(tool_input, mode=\"shroomdk\")\n\t    print(f\"{result=}\")\n\t    num_items = len(result)\n", "    assert num_items == 3\n\tdef test_query_snowflake_database_tool_str_shroomdk(snowflake_container):\n\t    tool = QuerySnowflakeDatabaseTool(db=snowflake_container)\n\t    tool_input = \"select * from ethereum.beacon_chain.fact_attestations limit 3\"\n\t    print(f\"\\n{tool_input=}\")\n\t    result = tool._run(tool_input, mode=\"shroomdk\")\n\t    print(f\"{result=}\")\n\t    num_items = len(result)\n\t    assert num_items == 3\n"]}
{"filename": "tests/integration_tests/test_tool_custom.py", "chunked_list": ["\"\"\"\n\ttest_tool_custom.py\n\tThis file contains the tests for the tool_custom module.\n\t\"\"\"\n\timport pytest\n\tfrom chatweb3.tools.snowflake_database.tool_custom import (\n\t    CheckTableMetadataTool, CheckTableSummaryTool, QueryDatabaseTool)\n\tdef test_check_table_summary_tool_local(\n\t    snowflake_container_eth_core, eth_core_table_long_names_select_list\n\t):\n", "    tool = CheckTableSummaryTool(db=snowflake_container_eth_core)\n\t    tool_input = \"\"\n\t    result = tool._run(tool_input=tool_input, mode=\"local\")\n\t    for i in range(len(eth_core_table_long_names_select_list)):\n\t        assert eth_core_table_long_names_select_list[i] in result\n\tdef test_check_table_metadata_tool_local(snowflake_container_eth_core):\n\t    tool = CheckTableMetadataTool(db=snowflake_container_eth_core)\n\t    table_names = \"ethereum.core.ez_dex_swaps, ethereum.core.ez_nft_mints, ethereum.core.ez_nft_transfers\"\n\t    result = tool._run(table_names=table_names, mode=\"local\")\n\t    assert \"ethereum.core.ez_dex_swaps\" in result\n", "    assert \"ethereum.core.ez_nft_mints\" in result\n\t    assert \"ethereum.core.ez_nft_transfers\" in result\n\t    with pytest.raises(ValueError):\n\t        tool._run(\"invalid_input\")\n\tdef test_query_database_tool_str_shroomdk(snowflake_container_eth_core):\n\t    tool = QueryDatabaseTool(db=snowflake_container_eth_core)\n\t    tool_input = \"select * from ethereum.beacon_chain.fact_attestations limit 3\"\n\t    result = tool._run(tool_input, mode=\"shroomdk\")\n\t    print(f\"{result=}\")\n\t    num_items = len(result)\n", "    assert num_items == 3\n\tdef test_query_database_tool_dict_shroomdk(snowflake_container_eth_core):\n\t    tool = QueryDatabaseTool(db=snowflake_container_eth_core)\n\t    tool_input = {\n\t        \"query\": \"select * from ethereum.beacon_chain.fact_attestations limit 3\",\n\t    }\n\t    print(f\"\\n{tool_input=}\")\n\t    result = tool._run(tool_input, mode=\"shroomdk\")\n\t    print(f\"{result=}\")\n\t    num_items = len(result)\n", "    assert num_items == 3\n\tdef test_query_database_tool_dict_str_shroomdk(snowflake_container_eth_core):\n\t    tool = QueryDatabaseTool(db=snowflake_container_eth_core)\n\t    tool_input = (\n\t        '{\"query\": \"select * from ethereum.beacon_chain.fact_attestations limit 3\"}'\n\t    )\n\t    print(f\"\\n{tool_input=}\")\n\t    result = tool._run(tool_input, mode=\"shroomdk\")\n\t    print(f\"{result=}\")\n\t    num_items = len(result)\n", "    assert num_items == 3\n\t@pytest.mark.skip(reason=\"requires snowflake credentials\")\n\tdef test_query_database_tool_dict_snowflake(snowflake_container_eth_core):\n\t    tool = QueryDatabaseTool(db=snowflake_container_eth_core)\n\t    tool_input = {\n\t        \"query\": \"select * from ethereum.beacon_chain.fact_attestations limit 3\",\n\t    }\n\t    print(f\"\\n{tool_input=}\")\n\t    result = tool._run(tool_input, mode=\"snowflake\")\n\t    print(f\"{result=}\")\n", "    num_items = result.count(\"'), (\") + 1\n\t    assert num_items == 3\n"]}
{"filename": "tests/unit_tests/test_create_agent.py", "chunked_list": ["\"\"\"\n\ttest_create_agent.py\n\tThis file contains the tests for the create_agent module.\n\t\"\"\"\n\tfrom unittest.mock import Mock, patch\n\tfrom create_agent import create_agent_executor\n\t@patch(\"create_agent.create_snowflake_chat_agent\")\n\t@patch(\"create_agent.CustomSnowflakeDatabaseToolkit\")\n\t@patch(\"create_agent.SnowflakeContainer\")\n\t@patch(\"create_agent.ChatOpenAI\")\n", "def test_create_agent_executor(\n\t    mock_chat_openai,\n\t    mock_snowflake_container,\n\t    mock_snowflake_toolkit,\n\t    mock_create_snowflake_chat_agent,\n\t):\n\t    mock_agent_executor = Mock()\n\t    mock_create_snowflake_chat_agent.return_value = mock_agent_executor\n\t    agent_executor = create_agent_executor()\n\t    assert agent_executor == mock_agent_executor\n"]}
{"filename": "tests/unit_tests/test_chat.py", "chunked_list": ["\"\"\"\n\ttest_chat.py\n\tThis file contains the tests for the main chat module.\n\t\"\"\"\n\timport os\n\tfrom unittest.mock import Mock\n\tfrom chat_ui import (\n\t    chat,\n\t    format_response,\n\t    set_openai_api_key,\n", "    split_thought_process_text,\n\t)\n\tdef test_set_openai_api_key():\n\t    original_key = os.getenv(\"OPENAI_API_KEY\")\n\t    api_key = \"test_key\"\n\t    agent = \"test_agent\"\n\t    set_openai_api_key(api_key, agent)\n\t    assert os.getenv(\"OPENAI_API_KEY\") == \"\"\n\t    if original_key is None:\n\t        # if the original API key was None, delete the environment variable\n", "        del os.environ[\"OPENAI_API_KEY\"]\n\t    else:\n\t        # otherwise, restore the original API key\n\t        os.environ[\"OPENAI_API_KEY\"] = original_key\n\tdef test_format_response():\n\t    step1 = Mock()\n\t    step1.log = \"Thought: step1\\nAction: \"\n\t    step1.tool = None\n\t    step1.tool_input = None\n\t    step2 = Mock()\n", "    step2.log = \"Thought: step2\\nAction: \"\n\t    step2.tool = None\n\t    step2.tool_input = None\n\t    response = {\n\t        \"output\": \"output\",\n\t        \"intermediate_steps\": [\n\t            (step1, \"step1_text\"),\n\t            (step2, \"step2_text\"),\n\t        ],\n\t    }\n", "    expected_output = \"**Thought 1**: step1\\n\\n*Action:*\\n\\n\\tTool: None\\n\\n\\tTool input: None\\n\\n*Observation:*\\n\\nstep1_text\\n\\n**Thought 2**: Thought: step2\\n\\n*Action:*\\n\\n\\tTool: None\\n\\n\\tTool input: None\\n\\n*Observation:*\\n\\nstep2_text\\n\\n**Final answer**: output\"\n\t    assert format_response(response) == expected_output\n\tdef test_split_thought_process_text():\n\t    text = \"_Thought 1: Thought1\\n\\nAction:\\n\\tTool: Tool1\\n\\nTool input: Input1\\n\\nObservation:\\n\\tObservation1\\n\\n_Thought 2: Thought2\\n\\nAction:\\n\\tTool: Tool2\\n\\nTool input: Input2\\n\\nObservation:\\n\\tObservation2\\n\\n_Final answer_: FinalAnswer\"\n\t    expected_sections = [\n\t        (\" 1: Thought1\", \"Tool: Tool1\\n\\nTool input: Input1\", \"Observation1\\n\\n\"),\n\t        (\" 2: Thought2\", \"Tool: Tool2\\n\\nTool input: Input2\", \"Observation2\\n\\n\"),\n\t    ]\n\t    expected_final_answer = \"FinalAnswer\"\n\t    sections, final_answer = split_thought_process_text(text)\n", "    assert sections == expected_sections\n\t    assert final_answer == expected_final_answer\n\tdef test_chat():\n\t    inp = \"test_input\"\n\t    history = [(\"question\", \"answer\")]\n\t    agent = None\n\t    new_history, _, _ = chat(inp, history, agent)\n\t    assert new_history == [\n\t        (\"question\", \"answer\"),\n\t        (\"test_input\", \"Please paste your OpenAI API Key to use\"),\n", "    ]\n"]}
