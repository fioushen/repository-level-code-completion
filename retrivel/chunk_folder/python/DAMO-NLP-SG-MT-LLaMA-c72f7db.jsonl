{"filename": "mtllama/llama_flash_attn_monkey_patch.py", "chunked_list": ["\"\"\"\n\tReplace vanilla attention in Huggingface's Llama implementation with flash attention.\n\tAdapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py \n\t\"\"\"\n\tfrom typing import List, Optional, Tuple\n\timport torch\n\tfrom torch import nn\n\timport transformers\n\tfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\tfrom einops import rearrange\n", "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n\tfrom flash_attn.bert_padding import unpad_input, pad_input\n\tdef forward(\n\t    self,\n\t    hidden_states: torch.Tensor,\n\t    attention_mask: Optional[torch.Tensor] = None,\n\t    position_ids: Optional[torch.Tensor] = None,\n\t    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n\t    output_attentions: bool = False,\n\t    use_cache: bool = False,\n", ") -> Tuple[torch.Tensor, Optional[torch.Tensor],\n\t            Optional[Tuple[torch.Tensor]]]:\n\t    \"\"\"Input shape: Batch x Time x Channel\n\t    attention_mask: [bsz, q_len]\n\t    \"\"\"\n\t    bsz, q_len, _ = hidden_states.size()\n\t    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    # [bsz, q_len, nh, hd]\n", "    # [bsz, nh, q_len, hd]\n\t    kv_seq_len = key_states.shape[-2]\n\t    assert past_key_value is None, \"past_key_value is not supported\"\n\t    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\t    # [bsz, nh, t, hd]\n\t    assert not output_attentions, \"output_attentions is not supported\"\n\t    assert not use_cache, \"use_cache is not supported\"\n\t    # Flash attention codes from\n\t    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n", "    # transform the data into the format required by flash attention\n\t    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n\t    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n\t    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n\t    # the attention_mask should be the same as the key_padding_mask\n\t    key_padding_mask = attention_mask\n\t    if key_padding_mask is None:\n\t        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n\t        max_s = q_len\n\t        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n", "                                device=qkv.device)\n\t        output = flash_attn_unpadded_qkvpacked_func(\n\t            qkv, cu_q_lens, max_s, 0.0,\n\t            softmax_scale=None, causal=True\n\t        )\n\t        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n\t    else:\n\t        nheads = qkv.shape[-2]\n\t        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n\t        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n", "        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n\t        output_unpad = flash_attn_unpadded_qkvpacked_func(\n\t            x_unpad, cu_q_lens, max_s, 0.0,\n\t            softmax_scale=None, causal=True\n\t        )\n\t        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n\t                                    indices, bsz, q_len),\n\t                        'b s (h d) -> b s h d', h=nheads)\n\t    return self.o_proj(rearrange(output,\n\t                                    'b s h d -> b s (h d)')), None, None\n", "# Disable the transformation of the attention mask in LlamaModel as the flash attention\n\t# requires the attention mask to be the same as the key_padding_mask\n\tdef _prepare_decoder_attention_mask(self, attention_mask, input_shape,\n\t                                    inputs_embeds, past_key_values_length):\n\t    # [bsz, seq_len]\n\t    return attention_mask\n\tdef replace_llama_attn_with_flash_attn():\n\t    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n\t    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"]}
{"filename": "mtllama/train.py", "chunked_list": ["# Adopted from lm-sys@FastChat and tatsu-lab@stanford_alpaca. Below is the original copyright:\n\t#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n\t#\n\t#    Licensed under the Apache License, Version 2.0 (the \"License\");\n\t#    you may not use this file except in compliance with the License.\n\t#    You may obtain a copy of the License at\n\t#\n\t#        http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t#    Unless required by applicable law or agreed to in writing, software\n", "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n\t#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t#    See the License for the specific language governing permissions and\n\t#    limitations under the License.\n\timport copy\n\tfrom dataclasses import dataclass, fieldjson\n\timport logging\n\timport pathlib\n\tfrom typing import Dict, Optional, Sequence\n\timport random\n", "import torch\n\tfrom accelerate.utils import set_seed\n\timport transformers\n\tfrom torch.utils.data import Dataset\n\tfrom transformers import Trainer\n\tset_seed(42)\n\timport conversation as conversation_lib\n\tfrom utils import construct_input\n\t# TODO: import and use code from ../data/dataset.py\n\tIGNORE_INDEX = -100\n", "DEFAULT_PAD_TOKEN = \"[PAD]\"\n\tDEFAULT_EOS_TOKEN = \"</s>\"\n\tDEFAULT_BOS_TOKEN = \"</s>\"\n\tDEFAULT_UNK_TOKEN = \"[UNK]\"\n\t@dataclass\n\tclass ModelArguments:\n\t    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n\t@dataclass\n\tclass DataArguments:\n\t    data_path: str = field(default=None,\n", "                           metadata={\"help\": \"Path to the training data.\"})\n\t    lazy_preprocess: bool = False\n\t@dataclass\n\tclass TrainingArguments(transformers.TrainingArguments):\n\t    cache_dir: Optional[str] = field(default=None)\n\t    optim: str = field(default=\"adamw_torch\")\n\t    model_max_length: int = field(\n\t        default=512,\n\t        metadata={\n\t            \"help\":\n", "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n\t        },\n\t    )\n\tdef convertBTC(data, lang2answer):\n\t    placeholder = \"IAmAPalacehodler\"\n\t    sources = []\n\t    while len(data) > 0:\n\t        data_one = data.pop(0)\n\t        if data_one[0] == \"P3\":\n\t            try:\n", "                source_one = [\n\t                    {\n\t                        \"from\": \"human\",\n\t                        \"value\": data_one[1]['inputs'].strip()\n\t                    },\n\t                    {\n\t                        \"from\": \"gpt\",\n\t                        \"value\": data_one[1]['targets'].strip()\n\t                    },\n\t                ]\n", "            except:\n\t                source_one = [\n\t                    {\n\t                        \"from\": \"human\",\n\t                        \"value\": data_one[1][0].strip()\n\t                    },\n\t                    {\n\t                        \"from\": \"gpt\",\n\t                        \"value\": data_one[1][1].strip()\n\t                    },\n", "                ]\n\t            sources.append(source_one)\n\t        elif data_one[0] == \"cls\":\n\t            lang = data_one[1].lang\n\t            query = data_one[1].query\n\t            query = query.replace(placeholder, DEFAULT_UNK_TOKEN)\n\t            answer = data_one[1].answer\n\t            num_neg = random.randint(1, 10)\n\t            num_all = len(lang2answer[lang])\n\t            neg_ans = random.sample(range(num_all), min(num_all, num_neg + 1))\n", "            neg_ans = [lang2answer[lang][x] for x in neg_ans]\n\t            if answer in neg_ans:\n\t                neg_ans.remove(answer)\n\t            else:\n\t                neg_ans = neg_ans[1:]\n\t            assert answer not in neg_ans\n\t            random_index = random.randint(0, len(neg_ans))\n\t            all_answers = neg_ans[: random_index] + [answer] + neg_ans[random_index:]\n\t            all_answers = sorted(set(all_answers), key=all_answers.index)\n\t            all_answers = [\"(\" + chr(ord('A') + ix) + \") \" + x + \" (/\" + chr(ord('A') + ix) + \")\" for ix, x in\n", "                           enumerate(all_answers)]\n\t            # input_str = \"Determine the category of the text from choices. Choices: %s. Text: %s. Category:\" % (\" \".join(all_answer), query)\n\t            input_str = construct_input(lang, 'cls', \" \".join(all_answers), query)\n\t            output_str = answer\n\t            source_one = [\n\t                {\n\t                    \"from\": \"human\",\n\t                    \"value\": input_str\n\t                },\n\t                {\n", "                    \"from\": \"gpt\",\n\t                    \"value\": output_str\n\t                },\n\t            ]\n\t            sources.append(source_one)\n\t        elif data_one[0] == \"ext\":\n\t            lang = data_one[1].lang\n\t            query = data_one[1].query\n\t            query = query.replace(placeholder, DEFAULT_UNK_TOKEN)\n\t            answer = data_one[1].answer\n", "            context = data_one[1].context\n\t            input_str = construct_input(lang, 'ext', query, context)\n\t            output_str = answer\n\t            source_one = [\n\t                {\n\t                    \"from\": \"human\",\n\t                    \"value\": input_str\n\t                },\n\t                {\n\t                    \"from\": \"gpt\",\n", "                    \"value\": output_str\n\t                },\n\t            ]\n\t            sources.append(source_one)\n\t    return sources\n\tdef safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n\t                                   output_dir: str):\n\t    \"\"\"Collects the state dict and dump to disk.\"\"\"\n\t    state_dict = trainer.model.state_dict()\n\t    if trainer.args.should_save:\n", "        cpu_state_dict = {\n\t            key: value.cpu()\n\t            for key, value in state_dict.items()\n\t        }\n\t        del state_dict\n\t        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\tdef smart_tokenizer_and_embedding_resize(\n\t    special_tokens_dict: Dict,\n\t    tokenizer: transformers.PreTrainedTokenizer,\n\t    model: transformers.PreTrainedModel,\n", "):\n\t    \"\"\"Resize tokenizer and embedding.\n\t    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n\t    \"\"\"\n\t    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\t    model.resize_token_embeddings(len(tokenizer))\n\t    if num_new_tokens > 0:\n\t        input_embeddings = model.get_input_embeddings().weight.data\n\t        output_embeddings = model.get_output_embeddings().weight.data\n\t        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n", "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\t        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n\t        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\tdef _tokenize_fn(strings: Sequence[str],\n\t                 tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n\t    \"\"\"Tokenize a list of strings.\"\"\"\n\t    tokenized_list = [\n\t        tokenizer(\n\t            text,\n\t            return_tensors=\"pt\",\n", "            padding=\"longest\",\n\t            max_length=tokenizer.model_max_length,\n\t            truncation=True,\n\t        ) for text in strings\n\t    ]\n\t    input_ids = labels = [\n\t        tokenized.input_ids[0] for tokenized in tokenized_list\n\t    ]\n\t    input_ids_lens = labels_lens = [\n\t        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n", "        for tokenized in tokenized_list\n\t    ]\n\t    return dict(\n\t        input_ids=input_ids,\n\t        labels=labels,\n\t        input_ids_lens=input_ids_lens,\n\t        labels_lens=labels_lens,\n\t    )\n\tdef _mask_targets(target, tokenized_lens, speakers, header_len, s_ids):\n\t    cur_idx = header_len\n", "    tgt_len = target.shape[0]\n\t    for tokenized_len, speaker, s_id in zip(tokenized_lens, speakers, s_ids):\n\t        if cur_idx >= tgt_len:\n\t            break\n\t        elif cur_idx + tokenized_len < tgt_len:\n\t            # Check whether the mask is applied to the correct position\n\t            if not torch.equal(target[cur_idx + 2:cur_idx + tokenized_len],\n\t                               s_id[2:]):\n\t                logging.warning(\"a sentence mismatches the corresponding piece \"\n\t                                \"in the conversation\")\n", "        if speaker == \"human\":\n\t            target[cur_idx:cur_idx + tokenized_len] = IGNORE_INDEX\n\t        cur_idx += tokenized_len\n\tdef _add_speaker_and_signal(header, source, get_conversation=True):\n\t    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n\t    BEGIN_SIGNAL = \"### \"\n\t    END_SIGNAL = \"\\n\"\n\t    conversation = header\n\t    for sentence in source:\n\t        from_str = sentence[\"from\"]\n", "        if from_str.lower() == \"human\":\n\t            from_str = conversation_lib.default_conversation.roles[0]\n\t        elif from_str.lower() == \"gpt\":\n\t            from_str = conversation_lib.default_conversation.roles[1]\n\t        else:\n\t            from_str = 'unknown'\n\t        sentence[\"value\"] = (BEGIN_SIGNAL + from_str + \": \" +\n\t                             sentence[\"value\"] + END_SIGNAL)\n\t        if get_conversation:\n\t            conversation += sentence[\"value\"]\n", "    return conversation\n\tdef preprocess(\n\t    sources: Sequence[str],\n\t    tokenizer: transformers.PreTrainedTokenizer,\n\t) -> Dict:\n\t    \"\"\"\n\t    Given a list of sources, each is a conversation list. This transform:\n\t    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n\t    2. Concatenate conversations together;\n\t    3. Tokenize the concatenated conversation;\n", "    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n\t    \"\"\"\n\t    # add end signal and concatenate together\n\t    conversations = []\n\t    header = f\"{conversation_lib.default_conversation.system}\\n\\n\"\n\t    for source in sources:\n\t        conversation = _add_speaker_and_signal(header, source)\n\t        conversations.append(conversation)\n\t    # print(conversations)\n\t    # tokenize conversations\n", "    conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n\t    input_ids = conversations_tokenized[\"input_ids\"]\n\t    targets = copy.deepcopy(input_ids)\n\t    header_len = _tokenize_fn([header], tokenizer)[\"input_ids_lens\"][0]\n\t    for target, source in zip(targets, sources):\n\t        tokenized_sentence = _tokenize_fn([s[\"value\"] for s in source], tokenizer)\n\t        tokenized_lens = tokenized_sentence[\"input_ids_lens\"]\n\t        # Currently, \"###\" is tokenized into 2 tokens in the whole conversation,\n\t        # and 1 token in a single sentence, so we do not need to use the line below.\n\t        # tokenized_lens = [l-1 for l in tokenized_lens]\n", "        speakers = [sentence[\"from\"] for sentence in source]\n\t        ids = tokenized_sentence[\"input_ids\"]\n\t        _mask_targets(target, tokenized_lens, speakers, header_len, ids)\n\t    return dict(input_ids=input_ids, labels=targets)\n\tclass SupervisedDataset(Dataset):\n\t    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\t    def __init__(self, data_path: str,\n\t                 tokenizer: transformers.PreTrainedTokenizer):\n\t        super(SupervisedDataset, self).__init__()\n\t        logging.warning(\"Loading data...\")\n", "        raw_data = torch.load(data_path)\n\t        logging.warning(\"Formatting inputs...\")\n\t        sources = convertBTC(raw_data['data'], raw_data['lang2answer'])\n\t        data_dict = preprocess(sources, tokenizer)\n\t        self.input_ids = data_dict[\"input_ids\"]\n\t        self.labels = data_dict[\"labels\"]\n\t    def __len__(self):\n\t        return len(self.input_ids)\n\t    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n\t        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n", "class LazySupervisedDataset(Dataset):\n\t    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\t    def __init__(self, data_path: str,\n\t                 tokenizer: transformers.PreTrainedTokenizer):\n\t        super(LazySupervisedDataset, self).__init__()\n\t        logging.warning(\"Loading data...\")\n\t        raw_data = torch.load(data_path)\n\t        logging.warning(\"Formatting inputs...Skip in lazy mode\")\n\t        self.tokenizer = tokenizer\n\t        self.list_data_dict = raw_data['data']\n", "        self.lang2answer = raw_data['lang2answer']\n\t    def __len__(self):\n\t        return len(self.list_data_dict)\n\t    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n\t        sources = self.list_data_dict[i]\n\t        if isinstance(i, int):\n\t            sources = [sources]\n\t            sources = convertBTC(sources, self.lang2answer)\n\t            # print(sources)\n\t        data_dict = preprocess(\n", "            copy.deepcopy(sources),\n\t            self.tokenizer)\n\t        if isinstance(i, int):\n\t            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n\t                             labels=data_dict[\"labels\"][0])\n\t        return data_dict\n\t@dataclass\n\tclass DataCollatorForSupervisedDataset(object):\n\t    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n\t    tokenizer: transformers.PreTrainedTokenizer\n", "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n\t        input_ids, labels = tuple([instance[key] for instance in instances]\n\t                                  for key in (\"input_ids\", \"labels\"))\n\t        input_ids = torch.nn.utils.rnn.pad_sequence(\n\t            input_ids,\n\t            batch_first=True,\n\t            padding_value=self.tokenizer.pad_token_id)\n\t        labels = torch.nn.utils.rnn.pad_sequence(labels,\n\t                                                 batch_first=True,\n\t                                                 padding_value=IGNORE_INDEX)\n", "        return dict(\n\t            input_ids=input_ids,\n\t            labels=labels,\n\t            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n\t        )\n\tdef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n\t                                data_args) -> Dict:\n\t    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n\t    dataset_cls = (LazySupervisedDataset\n\t                   if data_args.lazy_preprocess else SupervisedDataset)\n", "    train_dataset = dataset_cls(tokenizer=tokenizer,\n\t                                data_path=data_args.data_path)\n\t    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n\t    return dict(train_dataset=train_dataset,\n\t                eval_dataset=None,\n\t                data_collator=data_collator)\n\tdef train():\n\t    parser = transformers.HfArgumentParser(\n\t        (ModelArguments, DataArguments, TrainingArguments))\n\t    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n", "    model = transformers.LlamaForCausalLM.from_pretrained(\n\t        model_args.model_name_or_path,\n\t        cache_dir=training_args.cache_dir,\n\t        use_cache=False,\n\t    )\n\t    tokenizer = transformers.AutoTokenizer.from_pretrained(\n\t        model_args.model_name_or_path,\n\t        cache_dir=training_args.cache_dir,\n\t        model_max_length=training_args.model_max_length,\n\t        padding_side=\"right\",\n", "        use_fast=False,\n\t    )\n\t    if tokenizer.pad_token is None:\n\t        smart_tokenizer_and_embedding_resize(\n\t            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n\t            tokenizer=tokenizer,\n\t            model=model,\n\t        )\n\t    if \"llama\" in model_args.model_name_or_path:\n\t        tokenizer.add_special_tokens({\n", "            \"eos_token\": DEFAULT_EOS_TOKEN,\n\t            \"bos_token\": DEFAULT_BOS_TOKEN,\n\t            \"unk_token\": DEFAULT_UNK_TOKEN,\n\t        })\n\t    data_module = make_supervised_data_module(tokenizer=tokenizer,\n\t                                              data_args=data_args)\n\t    trainer = Trainer(model=model,\n\t                    tokenizer=tokenizer,\n\t                    args=training_args,\n\t                    **data_module)\n", "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n\t        trainer.train(resume_from_checkpoint=True)\n\t    else:\n\t        trainer.train()\n\t    trainer.save_state()\n\t    safe_save_model_for_hf_trainer(trainer=trainer,\n\t                                   output_dir=training_args.output_dir)\n\tif __name__ == \"__main__\":\n\t    train()\n"]}
{"filename": "mtllama/utils.py", "chunked_list": ["def construct_input(lang, task, text1, text2):\n\t    if lang == \"ar\":\n\t        if task == 'cls':\n\t            input_str = \"حدد فئة النص من الاختيارات. الاختيارات: %s. النص: %s. الفئة:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"تحديد الامتدادات من السياق وفقًا للاستعلام. استفسار: %s. سياق: %s. يمتد:\" % (\n\t                text1, text2)\n\t    elif lang == \"bn\":\n\t        if task == 'cls':\n", "            input_str = \"পছন্দ থেকে পাঠ্যের বিভাগ নির্ধারণ করুন। পছন্দ: %s পাঠ্য: %s বিভাগ:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"ক্যোয়ারী অনুযায়ী প্রসঙ্গ থেকে স্প্যান সনাক্ত করুন। প্রশ্ন: %s প্রসঙ্গ: %s স্প্যান:\" % (\n\t                text1, text2)\n\t    elif lang == \"de\":\n\t        if task == 'cls':\n\t            input_str = \"Bestimmen Sie die Kategorie des Textes aus Auswahlmöglichkeiten. Auswahlmöglichkeiten: %s. Text: %s. Kategorie:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n", "            input_str = \"Identifizieren Sie Spannen aus dem Kontext gemäß der Abfrage. Anfrage: %s. Kontext: %s. Spannweiten:\" % (\n\t                text1, text2)\n\t    elif lang == \"fi\":\n\t        if task == 'cls':\n\t            input_str = \"Määritä tekstin luokka vaihtoehdoista. Vaihtoehdot: %s. Teksti: %s. Kategoria:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Tunnista jänteet kontekstista kyselyn mukaan. Kysely: %s. Konteksti: %s. Kantavuus:\" % (\n\t                text1, text2)\n\t    elif lang == \"fr\":\n", "        if task == 'cls':\n\t            input_str = \"Déterminez la catégorie du texte parmi les choix. Les choix: %s. Texte: %s. Catégorie:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Identifiez les étendues du contexte en fonction de la requête. Mettre en doute: %s. Contexte: %s. Portées:\" % (\n\t                text1, text2)\n\t    elif lang == \"el\":\n\t        if task == 'cls':\n\t            input_str = \"Προσδιορίστε την κατηγορία του κειμένου από επιλογές. Επιλογές: %s. Κείμενο: %s. Κατηγορία:\" % (\n\t            text1, text2)\n", "        elif task == 'ext':\n\t            input_str = \"Προσδιορίστε τις εκτάσεις από το περιβάλλον σύμφωνα με το ερώτημα. Ερώτημα: %s. Συμφραζόμενα: %s. Εκτείνεται:\" % (\n\t                text1, text2)\n\t    elif lang == \"en\":\n\t        if task == 'cls':\n\t            input_str = \"Determine the category of the text from choices. Choices: %s. Text: %s. Category:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Identify spans from the context according to the query. Query: %s. Context: %s. Spans:\" % (\n\t                text1, text2)\n", "    elif lang == \"es\":\n\t        if task == 'cls':\n\t            input_str = \"Determinar la categoría del texto a partir de las opciones. Opciones: %s. Texto: %s. Categoría:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Identifique tramos del contexto de acuerdo con la consulta. Consulta: %s. Contexto: %s. Se extiende:\" % (\n\t                text1, text2)\n\t    elif lang == \"hi\":\n\t        if task == 'cls':\n\t            input_str = \"विकल्पों में से पाठ की श्रेणी निर्धारित करें। विकल्प: %s। मूलपाठ: %s। वर्ग:\" % (\n", "            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"क्वेरी के अनुसार संदर्भ से स्पैन की पहचान करें। जिज्ञासा: %s। प्रसंग: %s। स्पैन:\" % (\n\t                text1, text2)\n\t    elif lang == \"id\":\n\t        if task == 'cls':\n\t            input_str = \"Tentukan kategori teks dari pilihan. Pilihan: %s. Teks: %s. Kategori:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Identifikasi rentang dari konteks sesuai dengan kueri. Kueri: %s. Konteks: %s. Rentang:\" % (\n", "                text1, text2)\n\t    elif lang == \"it\":\n\t        if task == 'cls':\n\t            input_str = \"Determinare la categoria del testo dalle scelte. Scelte: %s. Testo: %s. Categoria:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Identifica gli intervalli dal contesto in base alla query. Domanda: %s. Contesto: %s. Campate:\" % (\n\t                text1, text2)\n\t    elif lang == \"ja\":\n\t        if task == 'cls':\n", "            input_str = \"選択肢からテキストのカテゴリを決定します。 選択肢：%s。 文章：%s。 カテゴリー：\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"クエリに従ってコンテキストからスパンを識別します。 クエリ：%s。 コンテクスト：%s。 スパン:\" % (\n\t                text1, text2)\n\t    elif lang == \"ko\":\n\t        if task == 'cls':\n\t            input_str = \"선택에서 텍스트의 범주를 결정합니다. 선택: %s. 텍스트: %s. 범주:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n", "            input_str = \"쿼리에 따라 컨텍스트에서 범위를 식별합니다. 쿼리: %s. 문맥: %s. 스팬:\" % (\n\t                text1, text2)\n\t    elif lang == \"nl\":\n\t        if task == 'cls':\n\t            input_str = \"Bepaal de categorie van de tekst uit keuzes. Keuzes: %s. Tekst: %s. Categorie:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Identificeer overspanningen uit de context volgens de query. Vraag: %s. Context: %s. Overspanningen:\" % (\n\t                text1, text2)\n\t    elif lang == \"pl\":\n", "        if task == 'cls':\n\t            input_str = \"Określ kategorię tekstu z wyborów. Wybory: %s. Tekst: %s. Kategoria:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Zidentyfikuj rozpiętości z kontekstu zgodnie z zapytaniem. Zapytanie: %s. Kontekst: %s. Rozpiętości:\" % (\n\t                text1, text2)\n\t    elif lang == \"pt\":\n\t        if task == 'cls':\n\t            input_str = \"Determine a categoria do texto a partir das opções. Escolhas: %s. Texto: %s. Categoria:\" % (\n\t            text1, text2)\n", "        elif task == 'ext':\n\t            input_str = \"Identifique spans a partir do contexto de acordo com a consulta. Consulta: %s. Contexto: %s. Períodos:\" % (\n\t                text1, text2)\n\t    elif lang == \"ru\":\n\t        if task == 'cls':\n\t            input_str = \"Определите категорию текста из вариантов. Выбор: %s. Текст: %s. Категория:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Идентифицируйте промежутки из контекста в соответствии с запросом. Запрос: %s. Контекст: %s. Пролеты:\" % (\n\t                text1, text2)\n", "    elif lang == \"sv\":\n\t        if task == 'cls':\n\t            input_str = \"Bestäm kategorin för texten från val. Alternativ: %s. Text: %s. Kategori:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Identifiera spann från sammanhanget enligt frågan. Fråga: %s. Sammanhang: %s. Spännvidder:\" % (\n\t                text1, text2)\n\t    elif lang == \"sw\":\n\t        if task == 'cls':\n\t            input_str = \"Amua aina ya maandishi kutoka kwa chaguo. Chaguo: %s. Maandishi: %s. Kategoria:\" % (\n", "            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Tambua vipindi kutoka kwa muktadha kulingana na hoja. Swali: %s. Muktadha: %s. Vipindi:\" % (\n\t                text1, text2)\n\t    elif lang == \"te\":\n\t        if task == 'cls':\n\t            input_str = \"ఎంపికల నుండి టెక్స్ట్ యొక్క వర్గాన్ని నిర్ణయించండి. ఎంపికలు: %s. వచనం: %s. వర్గం:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"ప్రశ్నకు అనుగుణంగా సందర్భం నుండి పరిధులను గుర్తించండి. ప్రశ్న: %s. సందర్భం: %s. పరిధులు:\" % (\n", "                text1, text2)\n\t    elif lang == \"th\":\n\t        if task == 'cls':\n\t            input_str = \"กำหนดหมวดหมู่ของข้อความจากตัวเลือก ตัวเลือก: %s ข้อความ: %s. หมวดหมู่:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"ระบุช่วงจากบริบทตามแบบสอบถาม ข้อความค้นหา: %s บริบท: %s. ช่วง:\" % (\n\t                text1, text2)\n\t    elif lang == \"tr\":\n\t        if task == 'cls':\n", "            input_str = \"Seçeneklerden metnin kategorisini belirleyin. Seçenekler: %s. Metin: %s. Kategori:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Sorguya göre bağlamdan açıklıkları tanımlayın. Sorgu: %s. Bağlam: %s. açıklıklar:\" % (\n\t                text1, text2)\n\t    elif lang == \"vi\":\n\t        if task == 'cls':\n\t            input_str = \"Xác định thể loại của văn bản từ các lựa chọn. Lựa chọn: %s. Chữ: %s. Loại:\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n", "            input_str = \"Xác định các khoảng từ ngữ cảnh theo truy vấn. Truy vấn: %s. Bối cảnh: %s. nhịp:\" % (\n\t                text1, text2)\n\t    elif lang == \"zh\":\n\t        if task == 'cls':\n\t            input_str = \"根据选项确定文本的类别。选项：%s。文本：%s。类别：\" % (\n\t            text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"根据查询从上下文中识别跨度。查询：%s。上下文：%s。跨度：\" % (\n\t                text1, text2)\n\t    else:\n", "        if task == 'cls':\n\t            input_str = \"Determine the category of the text from choices. Choices: %s. Text: %s. Category:\" % (\n\t                text1, text2)\n\t        elif task == 'ext':\n\t            input_str = \"Identify spans from the context according to the query. Query: %s. Context: %s. Spans:\" % (\n\t                text1, text2)\n\t    return input_str\n\tclass S2SFeatures:\n\t    '''\n\t    MRC features\n", "    '''\n\t    def __init__(\n\t        self,\n\t        query,\n\t        context,\n\t        answer,\n\t        target_entity,\n\t        len_query,\n\t        len_context,\n\t        lang=None,\n", "    ):\n\t        self.query = query\n\t        self.context = context\n\t        self.answer = answer\n\t        self.target_entity = target_entity\n\t        self.len_query = len_query\n\t        self.len_context = len_context\n\t        self.lang = lang"]}
{"filename": "mtllama/conversation.py", "chunked_list": ["\"\"\"\n\tConversation prompt templates.\n\tAdopted from https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n\t\"\"\"\n\timport dataclasses\n\tfrom enum import auto, Enum\n\tfrom typing import List, Tuple, Any\n\tclass SeparatorStyle(Enum):\n\t    \"\"\"Different separator style.\"\"\"\n\t    SINGLE = auto()\n", "    TWO = auto()\n\t    DOLLY = auto()\n\t@dataclasses.dataclass\n\tclass Conversation:\n\t    \"\"\"A class that keeps all conversation history.\"\"\"\n\t    system: str\n\t    roles: List[str]\n\t    messages: List[List[str]]\n\t    offset: int\n\t    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n", "    sep: str = \"###\"\n\t    sep2: str = None\n\t    skip_next: bool = False\n\t    conv_id: Any = None\n\t    def get_prompt(self):\n\t        if self.sep_style == SeparatorStyle.SINGLE:\n\t            ret = self.system\n\t            for role, message in self.messages:\n\t                if message:\n\t                    ret += self.sep + \" \" + role + \": \" + message\n", "                else:\n\t                    ret += self.sep + \" \" + role + \":\"\n\t            return ret\n\t        elif self.sep_style == SeparatorStyle.TWO:\n\t            seps = [self.sep, self.sep2]\n\t            ret = self.system + seps[0]\n\t            for i, (role, message) in enumerate(self.messages):\n\t                if message:\n\t                    ret += role + \": \" + message + seps[i % 2]\n\t                else:\n", "                    ret += role + \":\"\n\t            return ret\n\t        elif self.sep_style == SeparatorStyle.DOLLY:\n\t            seps = [self.sep, self.sep2]\n\t            ret = self.system\n\t            for i, (role, message) in enumerate(self.messages):\n\t                if message:\n\t                    ret += role + \":\\n\" + message + seps[i % 2]\n\t                    if i % 2 == 1:\n\t                        ret += \"\\n\\n\"\n", "                else:\n\t                    ret += role + \":\\n\"\n\t            return ret\n\t        else:\n\t            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\t    def append_message(self, role, message):\n\t        self.messages.append([role, message])\n\t    def to_gradio_chatbot(self):\n\t        ret = []\n\t        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n", "            if i % 2 == 0:\n\t                ret.append([msg, None])\n\t            else:\n\t                ret[-1][-1] = msg\n\t        return ret\n\t    def copy(self):\n\t        return Conversation(\n\t            system=self.system,\n\t            roles=self.roles,\n\t            messages=[[x, y] for x, y in self.messages],\n", "            offset=self.offset,\n\t            sep_style=self.sep_style,\n\t            sep=self.sep,\n\t            sep2=self.sep2,\n\t            conv_id=self.conv_id)\n\t    def dict(self):\n\t        return {\n\t            \"system\": self.system,\n\t            \"roles\": self.roles,\n\t            \"messages\": self.messages,\n", "            \"offset\": self.offset,\n\t            \"sep\": self.sep,\n\t            \"sep2\": self.sep2,\n\t            \"conv_id\": self.conv_id,\n\t        }\n\tconv_one_shot = Conversation(\n\t    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n\t           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n\t    roles=(\"Human\", \"Assistant\"),\n\t    messages=(\n", "        (\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"),\n\t        (\"Assistant\",\n\t            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n\t            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n\t            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n\t            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n\t            \"renewable and non-renewable energy sources:\\n\"\n\t            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n\t            \"energy sources are finite and will eventually run out.\\n\"\n\t            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n", "            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n\t            \"and other negative effects.\\n\"\n\t            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n\t            \"have lower operational costs than non-renewable sources.\\n\"\n\t            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n\t            \"locations than non-renewable sources.\\n\"\n\t            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n\t            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n\t            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n\t            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\")\n", "    ),\n\t    offset=2,\n\t    sep_style=SeparatorStyle.SINGLE,\n\t    sep=\"###\",\n\t)\n\tconv_vicuna_v1_1 = Conversation(\n\t    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n\t           \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n\t    roles=(\"USER\", \"ASSISTANT\"),\n\t    messages=(),\n", "    offset=0,\n\t    sep_style=SeparatorStyle.TWO,\n\t    sep=\" \",\n\t    sep2=\"</s>\",\n\t)\n\tconv_koala_v1 = Conversation(\n\t    system=\"BEGINNING OF CONVERSATION:\",\n\t    roles=(\"USER\", \"GPT\"),\n\t    messages=(),\n\t    offset=0,\n", "    sep_style=SeparatorStyle.TWO,\n\t    sep=\" \",\n\t    sep2=\"</s>\",\n\t)\n\tconv_dolly = Conversation(\n\t    system=\n\t    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\",\n\t    roles=('### Instruction', '### Response'),\n\t    messages=(),\n\t    offset=0,\n", "    sep_style=SeparatorStyle.DOLLY,\n\t    sep=\"\\n\\n\",\n\t    sep2=\"### End\",\n\t)\n\tconv_templates = {\n\t    \"conv_one_shot\": conv_one_shot,\n\t    \"vicuna_v1.1\": conv_vicuna_v1_1,\n\t    \"koala_v1\": conv_koala_v1,\n\t    \"dolly\": conv_dolly,\n\t}\n", "def get_default_conv_template(model_name):\n\t    model_name = model_name.lower()\n\t    if \"vicuna\" in model_name or \"output\" in model_name:\n\t        return conv_vicuna_v1_1\n\t    elif \"koala\" in model_name:\n\t        return conv_koala_v1\n\t    elif \"dolly\" in model_name:\n\t        return conv_dolly\n\t    return conv_one_shot\n\tdefault_conversation = conv_vicuna_v1_1\n", "if __name__ == \"__main__\":\n\t    print(default_conversation.get_prompt())\n"]}
{"filename": "mtllama/train_mem.py", "chunked_list": ["\"\"\"\n\tEnable flash attention during fine-tuning.\n\tAdopted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train_mem.py\n\t\"\"\"\n\tfrom llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n\treplace_llama_attn_with_flash_attn()\n\tfrom train import train\n\tif __name__ == \"__main__\":\n\t    train()\n"]}
{"filename": "mtllama/cli.py", "chunked_list": ["\"\"\"\n\tUsage:\n\tpython3 cli.py --model /path/to/mt-llama-7b\n\t\"\"\"\n\timport argparse\n\timport torch\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n\tfrom conversation import Conversation, SeparatorStyle\n\tmy_conv = Conversation(\n\t    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n", "           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n\t    roles=(\"Human\", \"Assistant\"),\n\t    messages=(),\n\t    offset=2,\n\t    sep_style=SeparatorStyle.SINGLE,\n\t    sep=\"###\",\n\t)\n\tdef load_model(model_name, device, num_gpus, load_8bit=False):\n\t    if device == \"cuda\":\n\t        kwargs = {\"torch_dtype\": torch.float16}\n", "        if load_8bit:\n\t            if num_gpus != \"auto\" and int(num_gpus) != 1:\n\t                print(\"8-bit weights are not supported on multiple GPUs. Revert to use one GPU.\")\n\t            kwargs.update({\"load_in_8bit\": True, \"device_map\": \"auto\"})\n\t        else:\n\t            if num_gpus == \"auto\":\n\t                kwargs[\"device_map\"] = \"auto\"\n\t            else:\n\t                num_gpus = int(num_gpus)\n\t                if num_gpus != 1:\n", "                    kwargs.update({\n\t                        \"device_map\": \"auto\",\n\t                        \"max_memory\": {i: \"13GiB\" for i in range(num_gpus)},\n\t                    })\n\t    elif device == \"cpu\":\n\t        kwargs = {}\n\t    else:\n\t        raise ValueError(f\"Invalid device: {device}\")\n\t    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\t    model = AutoModelForCausalLM.from_pretrained(model_name,\n", "        low_cpu_mem_usage=True, **kwargs)\n\t    # calling model.cuda() mess up weights if loading 8-bit weights\n\t    if device == \"cuda\" and num_gpus == 1 and not load_8bit:\n\t        model.cuda()\n\t    return model, tokenizer\n\t@torch.inference_mode()\n\tdef generate_stream(tokenizer, model, params, device,\n\t                    context_len=2048, stream_interval=2):\n\t    \"\"\"Adapted from fastchat/serve/model_worker.py::generate_stream\"\"\"\n\t    prompt = params[\"prompt\"]\n", "    l_prompt = len(prompt)\n\t    temperature = float(params.get(\"temperature\", 1.0))\n\t    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n\t    stop_str = params.get(\"stop\", None)\n\t    input_ids = tokenizer(prompt).input_ids\n\t    output_ids = list(input_ids)\n\t    max_src_len = context_len - max_new_tokens - 8\n\t    input_ids = input_ids[-max_src_len:]\n\t    for i in range(max_new_tokens):\n\t        if i == 0:\n", "            out = model(\n\t                torch.as_tensor([input_ids], device=device), use_cache=True)\n\t            logits = out.logits\n\t            past_key_values = out.past_key_values\n\t        else:\n\t            attention_mask = torch.ones(\n\t                1, past_key_values[0][0].shape[-2] + 1, device=device)\n\t            out = model(input_ids=torch.as_tensor([[token]], device=device),\n\t                        use_cache=True,\n\t                        attention_mask=attention_mask,\n", "                        past_key_values=past_key_values)\n\t            logits = out.logits\n\t            past_key_values = out.past_key_values\n\t        last_token_logits = logits[0][-1]\n\t        if temperature < 1e-4:\n\t            token = int(torch.argmax(last_token_logits))\n\t        else:\n\t            probs = torch.softmax(last_token_logits / temperature, dim=-1)\n\t            token = int(torch.multinomial(probs, num_samples=1))\n\t        output_ids.append(token)\n", "        if token == tokenizer.eos_token_id:\n\t            stopped = True\n\t        else:\n\t            stopped = False\n\t        if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:\n\t            output = tokenizer.decode(output_ids, skip_special_tokens=True)\n\t            pos = output.rfind(stop_str, l_prompt)\n\t            if pos != -1:\n\t                output = output[:pos]\n\t                stopped = True\n", "            yield output\n\t        if stopped:\n\t            break\n\t    del past_key_values\n\tdef main(args):\n\t    model_name = args.model_name\n\t    # Model\n\t    model, tokenizer = load_model(args.model_name, args.device,\n\t        args.num_gpus, args.load_8bit)\n\t    # Chat\n", "    conv = my_conv.copy()\n\t    while True:\n\t        try:\n\t            inp = input(f\"{conv.roles[0]}: \")\n\t        except EOFError:\n\t            inp = \"\"\n\t        if not inp:\n\t            print(\"exit...\")\n\t            break\n\t        conv.append_message(conv.roles[0], inp)\n", "        conv.append_message(conv.roles[1], None)\n\t        prompt = conv.get_prompt()\n\t        params = {\n\t            \"model\": model_name,\n\t            \"prompt\": prompt,\n\t            \"temperature\": args.temperature,\n\t            \"max_new_tokens\": args.max_new_tokens,\n\t            \"stop\": \"\\n\" #conv.sep if conv.sep_style == SeparatorStyle.SINGLE else conv.sep2,\n\t        }\n\t        print(f\"{conv.roles[1]}: \", end=\"\", flush=True)\n", "        pre = 0\n\t        for outputs in generate_stream(tokenizer, model, params, args.device):\n\t            outputs = outputs[len(prompt) + 1:].strip()\n\t            outputs = outputs.split(\" \")\n\t            now = len(outputs)\n\t            if now - 1 > pre:\n\t                print(\" \".join(outputs[pre:now-1]), end=\" \", flush=True)\n\t                pre = now - 1\n\t        print(\" \".join(outputs[pre:]), flush=True)\n\t        conv.messages[-1][-1] = \" \".join(outputs)\n", "        if args.debug:\n\t            print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n\t    parser.add_argument(\"--device\", type=str, choices=[\"cuda\", \"cpu\"], default=\"cuda\")\n\t    parser.add_argument(\"--num-gpus\", type=str, default=\"1\")\n\t    parser.add_argument(\"--load-8bit\", action=\"store_true\")\n\t    parser.add_argument(\"--temperature\", type=float, default=0)\n\t    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n", "    parser.add_argument(\"--debug\", action=\"store_true\")\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "mtllama/model/make_delta.py", "chunked_list": ["\"\"\"\n\thttps://github.com/lm-sys/FastChat/blob/main/fastchat/model/make_delta.py\n\tUsage:\n\tpython3 -m mtllama.model.make_delta --base-model-path /path/to/llama-7b --target-model-path /path/to/mt-llama-7b --delta-path /output/path/to/mt-llama-delta-7b\n\t\"\"\"\n\timport argparse\n\timport torch\n\tfrom tqdm import tqdm\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\tdef make_delta(base_model_path, target_model_path, delta_path):\n", "    print(\"Loading base model\")\n\t    base = AutoModelForCausalLM.from_pretrained(\n\t        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\t    print(\"Loading target model\")\n\t    target = AutoModelForCausalLM.from_pretrained(target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\t    DEFAULT_PAD_TOKEN = \"[PAD]\"\n\t    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n\t    num_new_tokens = base_tokenizer.add_special_tokens(dict(pad_token=DEFAULT_PAD_TOKEN))\n\t    base.resize_token_embeddings(len(base_tokenizer))\n\t    input_embeddings = base.get_input_embeddings().weight.data\n", "    output_embeddings = base.get_output_embeddings().weight.data\n\t    input_embeddings[-num_new_tokens:] = 0\n\t    output_embeddings[-num_new_tokens:] = 0\n\t    print(\"Calculating delta\")\n\t    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n\t        assert name in base.state_dict()\n\t        param.data -= base.state_dict()[name]\n\t    print(\"Saving delta\")\n\t    if args.hub_repo_id:\n\t        kwargs = {\"push_to_hub\": True, \"repo_id\": args.hub_repo_id, \"use_auth_token\": args.user_key}\n", "    else:\n\t        kwargs = {}\n\t    target.save_pretrained(delta_path, **kwargs)\n\t    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path)\n\t    target_tokenizer.save_pretrained(delta_path, **kwargs)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--base-model-path\", type=str, required=True)\n\t    parser.add_argument(\"--target-model-path\", type=str, required=True)\n\t    parser.add_argument(\"--delta-path\", type=str, required=True)\n", "    parser.add_argument(\"--hub-repo-id\", type=str, required=False)\n\t    parser.add_argument(\"--user-key\", type=str, required=False)\n\t    args = parser.parse_args()\n\t    make_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"]}
{"filename": "mtllama/model/apply_delta.py", "chunked_list": ["\"\"\"\n\tAdapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/model/apply_delta.py\n\tUsage:\n\tpython3 -m mtllama.model.apply_delta --base-model-path /path/to/llama-7b --target-model-path /output/path/to/mt-llama-7b --delta-path /path/to/mt-llama-7b-delta\n\t\"\"\"\n\timport argparse\n\timport torch\n\tfrom tqdm import tqdm\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\tdef apply_delta(base_model_path, target_model_path, delta_path):\n", "    print(\"Loading base model\")\n\t    base = AutoModelForCausalLM.from_pretrained(\n\t        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\t    print(\"Loading delta\")\n\t    delta = AutoModelForCausalLM.from_pretrained(delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\t    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\n\t    DEFAULT_PAD_TOKEN = \"[PAD]\"\n\t    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n\t    num_new_tokens = base_tokenizer.add_special_tokens(dict(pad_token=DEFAULT_PAD_TOKEN))\n\t    base.resize_token_embeddings(len(base_tokenizer))\n", "    input_embeddings = base.get_input_embeddings().weight.data\n\t    output_embeddings = base.get_output_embeddings().weight.data\n\t    input_embeddings[-num_new_tokens:] = 0\n\t    output_embeddings[-num_new_tokens:] = 0\n\t    print(\"Applying delta\")\n\t    for name, param in tqdm(base.state_dict().items(), desc=\"Applying delta\"):\n\t        assert name in delta.state_dict()\n\t        param.data += delta.state_dict()[name]\n\t    print(\"Saving target model\")\n\t    base.save_pretrained(target_model_path)\n", "    delta_tokenizer.save_pretrained(target_model_path)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--base-model-path\", type=str, required=True)\n\t    parser.add_argument(\"--target-model-path\", type=str, required=True)\n\t    parser.add_argument(\"--delta-path\", type=str, required=True)\n\t    args = parser.parse_args()\n\t    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"]}
{"filename": "mtllama/model/__init__.py", "chunked_list": []}
{"filename": "mCLS/data_clm.py", "chunked_list": ["import torch\n\tfrom torch.utils.data import Dataset\n\tfrom transformers.utils import logging\n\timport random\n\timport numpy as np\n\tfrom conversation import Conversation, SeparatorStyle\n\tmy_conv = Conversation(\n\t    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n\t           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n\t    roles=(\"Human\", \"Assistant\"),\n", "    messages=(),\n\t    offset=2,\n\t    sep_style=SeparatorStyle.SINGLE,\n\t    sep=\"###\",\n\t)\n\tlogger = logging.get_logger(__name__)\n\tMULTI_SEP_TOKENS_TOKENIZERS_SET = {\"roberta\", \"camembert\", \"bart\", \"mpnet\"}\n\trandom.seed(42)\n\tdef compute_acc(preds, labels):\n\t    \"\"\" simple accuracy \"\"\"\n", "    preds = np.array(preds)\n\t    labels = np.array(labels)\n\t    acc = float((preds == labels).mean())\n\t    return {'acc': acc}\n\tMetrics = {\n\t    'acc':compute_acc,\n\t}\n\tclass GLUEDataset(Dataset):\n\t    \"\"\"\n\t    MRC NER Dataset\n", "    Args:\n\t        data_path: path to data\n\t        tokenizer: BertTokenizer\n\t    \"\"\"\n\t    def __init__(self, dataset, data_path,  tokenizer: None, lang='en'):\n\t        self.data_name = data_path\n\t        self.tokenizer = tokenizer\n\t        self.lang = lang\n\t        self.get_data(data_path, dataset)\n\t    def get_data(self, data_path, dataset):\n", "        if data_path == \"xnli\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            map_dict = {\"neutral\": 0, 'entailment': 1, 'contradiction': 2}\n\t            self.label_dict = {\"maybe\": 0, 'yes': 1, 'no': 2,\n\t                               \"a\": 0, \"b\": 1, 'c':2,\n\t                              \"inconclusive\":0, \"true\":1, \"false\":2 }\n\t            labels = [\"maybe.\",\n\t                      'yes.',\n\t                      'no.']\n", "            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                passage = item[0] + \" Question: Does this imply that \" + item[1] + \"? Please response with 'Yes', 'No', or 'Maybe'.\"\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": map_dict[item[2]],\n", "                }\n\t                self.all_data.append(data_one)\n\t        elif data_path == \"xstorycloze\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {\"1\": 0, '2': 1,\n\t                               \"(a)\": 0, '(b)': 1,\n\t                               \"a\": 0, 'b': 1,\n\t                                \"(/a)\": 0, '(/b)': 1}\n\t            for i, item in enumerate(dataset):\n", "                conv = my_conv.copy()\n\t                labels = [item['sentence_quiz1'], item['sentence_quiz2']]\n\t                passage = item['context'] + \" As a consequence... Help me pick the more plausible option: - \" \\\n\t                          + item['sentence_quiz1'] + \" - \" + item['sentence_quiz2']\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n", "                    \"gold_label\": item['answer_right_ending'] - 1,\n\t                }\n\t                self.all_data.append(data_one)\n\t        elif data_path == \"xcopa\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {\"1\": 0, '2': 1,\n\t                               \"(a)\": 0, '(b)': 1,\n\t                               \"a\": 0, 'b': 1,\n\t                                \"(/a)\": 0, '(/b)': 1}\n", "            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                labels = [item['choice1'], item['choice2'],]\n\t                if item['question'] == 'cause':\n\t                    passage = item['premise'] +\" This happened because... Help me pick the more plausible option: - \" \\\n\t                              + item['choice1'] + \". - \" + item['choice2']\n\t                elif item['question'] == 'effect':\n\t                    passage = item['premise'] +\" As a consequence... Help me pick the more plausible option: - \"\\\n\t                              + item['choice1'] + \". - \" + item['choice2']\n\t                else:\n", "                    raise NotImplementedError\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": item['label'],\n\t                }\n\t                self.all_data.append(data_one)\n", "        elif data_path == \"xwinograd\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {\"1.\": 0, '2.': 1,\n\t                               \"a\": 0, 'b': 1,\n\t                               \"(a)\": 0, '(b)': 1,\n\t                               \"(/a)\": 0, '(/b)': 1,\n\t                               \"True\": 0, \"False\": 1}\n\t            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n", "                labels = [f\"{item['option1']}\", f\"{item['option2']}\"]\n\t                passage = f\"{item['sentence']} In the previous sentence, does _ refer to {item['option1']} or {item['option2']}?\"\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": int(item['answer']) - 1,\n\t                }\n", "                self.all_data.append(data_one)\n\t        elif data_path == \"mnlim\" or data_path == \"mnlimm\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {\"maybe\": 0, 'yes': 1, 'no': 2,\n\t                               \"a\": 0, \"b\": 1, 'c': 2,\n\t                               \"inconclusive\": 0, \"true\": 1, \"false\": 2}\n\t            labels = [\"maybe.\",\n\t                      'yes.',\n\t                      'no.']\n", "            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                passage = item['premise'] + \" Question: Does this imply that \" + item['hypothesis'] + \\\n\t                          \"? Please response with 'Yes', 'No', or 'Maybe'.\"\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n", "                    \"gold_label\": item['label'],\n\t                }\n\t                self.all_data.append(data_one)\n\t        elif data_path == \"copa\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {\"1\": 0, '2': 1,\n\t                               \"(a)\": 0, '(b)': 1,\n\t                               \"a\": 0, 'b': 1,\n\t                                \"(/a)\": 0, '(/b)': 1}\n", "            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                labels = [item['choice1'], item['choice2'],]\n\t                if item['question'] == 'cause':\n\t                    passage = item['premise'] +\" This happened because... Help me pick the more plausible option: - \" \\\n\t                              + item['choice1'] + \". - \" + item['choice2']\n\t                elif item['question'] == 'effect':\n\t                    passage = item['premise'] +\" As a consequence... Help me pick the more plausible option: - \"\\\n\t                              + item['choice1'] + \". - \" + item['choice2']\n\t                else:\n", "                    raise NotImplementedError\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": item['label'],\n\t                }\n\t                self.all_data.append(data_one)\n", "        elif data_path == \"wic\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {'no': 0, 'yes': 1,}\n\t            labels = ['no.',\n\t                      'yes.']\n\t            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                passage = f'Does the word \"{item[\"word\"]}\" have the same meaning in these two sentences? Yes, No? {item[\"sentence1\"]} {item[\"sentence2\"]}'\n\t                conv.append_message(conv.roles[0], passage)\n", "                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": item['label'],\n\t                }\n\t                self.all_data.append(data_one)\n\t        elif data_path == \"rte\":\n\t            self.all_data = []\n", "            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {'yes': 0, 'no': 1,}\n\t            labels = ['yes.',\n\t                      'no.']\n\t            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                passage = f'Given {item[\"premise\"]} Is it guaranteed true that \"{item[\"hypothesis\"]}\"? Yes or no?'\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n", "                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": item['label'],\n\t                }\n\t                self.all_data.append(data_one)\n\t        elif data_path == \"winogrande\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {\"1.\": 0, '2.': 1,\n", "                               \"a\": 0, 'b': 1,\n\t                               \"(a)\": 0, '(b)': 1,\n\t                               \"(/a)\": 0, '(/b)': 1,\n\t                               \"True\": 0, \"False\": 1}\n\t            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                labels = [f\"{item['option1']}\", f\"{item['option2']}\"]\n\t                passage = f\"{item['sentence']} In the previous sentence, does _ refer to {item['option1']} or {item['option2']}?\"\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n", "                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": int(item['answer']) - 1,\n\t                }\n\t                self.all_data.append(data_one)\n\t        elif data_path == \"obqa\":\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n", "            self.label_dict = {\"1.\": 0, '2.': 1, \"3.\": 2, '4.': 3,\n\t                               \"a\": 0, 'b': 1, \"c\":2, \"d\":3,\n\t                               \"(a)\": 0, '(b)': 1,\"(c)\": 2, '(d)': 3,\n\t                               }\n\t            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                labels = [f\"{item['choices']['text'][0]}\", f\"{item['choices']['text'][1]}\", f\"{item['choices']['text'][2]}\", f\"{item['choices']['text'][3]}\"]\n\t                passage = f\"{item['question_stem']} Which is the correct answer? - (A) {item['choices']['text'][0]} \" \\\n\t                          f\"- (B) {item['choices']['text'][1]} - (C) {item['choices']['text'][2]}\" \\\n\t                          f\" - (D) {item['choices']['text'][3]}\"\n", "                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": ord(item['answerKey']) - ord(\"A\"),\n\t                }\n\t                self.all_data.append(data_one)\n\t        elif data_path == 'pawsx':\n", "            self.all_data = []\n\t            self.compute_metric = compute_acc\n\t            self.label_dict = {'no': 0, 'yes': 1}\n\t            labels = ['no.', 'yes.']\n\t            for i, item in enumerate(dataset):\n\t                passage = \"Sentence 1: \" + item[0] + \" Sentence 2: \" + item[\n\t                    1] + \" Question: Does Sentence 1 paraphrase Sentence 2? Yes or No?\"\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n", "                    \"gold_label\": int(item[2]),\n\t                }\n\t                self.all_data.append(data_one)\n\t        elif data_path.startswith(\"SST-2\"):\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {\n\t                \"1\": 0, \"2\": 0, \"4\": 1, \"5\": 1, \"★\": 0, \"★★\": 0, \"★★★★\": 1, \"★★★★★\": 1,\n\t                \"no\": 0, \"yes\": 1,\n\t            }\n", "            labels = [\"no.\", \"yes.\"]\n\t            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                passage =  f\"{item[0]} Based on this review, would the user recommend this product? No or Yes?\"\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": int(item[1]),\n", "                }\n\t                self.all_data.append(data_one)\n\t        elif data_path.startswith(\"marc-2\"):\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n\t            self.label_dict = {\n\t                               \"1\": 0, \"2\": 0,  \"4\": 1, \"5\": 1, \"★\": 0, \"★★\": 0, \"★★★★\": 1, \"★★★★★\": 1,\n\t                                \"no\": 0, \"yes\":1,\n\t                               }\n\t            labels = [\"no.\", \"yes.\"]\n", "            label_map = {\"1\": 0, \"2\": 0, \"4\": 1, \"5\": 1}\n\t            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                passage = \"Title: \" + item['review_title'] + \" Product Review: \" + item['review_body'] \\\n\t                          + \" Based on this review, would the user recommend this product? No or Yes?\"\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                star = item['stars']\n\t                if star not in label_map:\n\t                    continue\n", "                else:\n\t                    data_one = {\n\t                        'passage': passage,\n\t                        'labels': labels,\n\t                        \"gold_label\": label_map[star],\n\t                    }\n\t                self.all_data.append(data_one)\n\t        elif data_path.startswith(\"marc-5\"):\n\t            self.all_data = []\n\t            self.compute_metric = Metrics['acc']\n", "            self.label_dict = {\n\t                               \"1\":0, \"2\":1, \"3\":2, \"4\":3, \"5\":4, \"★\":0, \"★★\":1, \"★★★\":2, \"★★★★\":3, \"★★★★★\":4,\n\t                               \"not at all\": 0, \"no\": 1, \"maybe\": 2, \"yes\": 3, \"definitely\": 4,\n\t                               }\n\t            labels = [\"not at all.\", \"no.\", \"maybe.\", \"yes.\", \"definitely.\"]\n\t            label_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n\t            for i, item in enumerate(dataset):\n\t                conv = my_conv.copy()\n\t                passage = \"Title: \" + item['review_title'] + \" Product Review: \" + item[\n\t                    'review_body'] + \" Based on this review, would the user recommend this product?\" \\\n", "                                     \" Not at all, No, Maybe, Yes, or Definitely?\"\n\t                conv.append_message(conv.roles[0], passage)\n\t                conv.append_message(conv.roles[1], None)\n\t                passage = conv.get_prompt()\n\t                star = item['stars']\n\t                data_one = {\n\t                    'passage': passage,\n\t                    'labels': labels,\n\t                    \"gold_label\": label_map[star],\n\t                }\n", "                self.all_data.append(data_one)\n\t        else:\n\t            raise NotImplementedError\n\t    def __len__(self):\n\t        return len(self.all_data)\n\t    def __getitem__(self, item):\n\t        \"\"\"\n\t        Args:\n\t            item: int, idx\n\t        \"\"\"\n", "        data = self.all_data[item]\n\t        passage = data['passage']\n\t        labels = data['labels']\n\t        gold_label = data['gold_label']\n\t        answer = labels[gold_label]\n\t        tokenizer = self.tokenizer\n\t        inputs = tokenizer(passage, return_tensors='pt')\n\t        outputs = tokenizer(answer, return_tensors='pt')\n\t        return [\n\t            torch.LongTensor(inputs['input_ids']),\n", "            torch.LongTensor(inputs['attention_mask']),\n\t            torch.LongTensor(outputs['input_ids']),\n\t            torch.LongTensor(outputs['attention_mask']),\n\t            gold_label,\n\t        ]\n"]}
{"filename": "mCLS/llama_flash_attn_monkey_patch.py", "chunked_list": ["\"\"\"\n\tReplace vanilla attention in Huggingface's Llama implementation with flash attention.\n\tAdapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py \n\t\"\"\"\n\tfrom typing import List, Optional, Tuple\n\timport torch\n\tfrom torch import nn\n\timport transformers\n\tfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\tfrom einops import rearrange\n", "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n\tfrom flash_attn.bert_padding import unpad_input, pad_input\n\tdef forward(\n\t    self,\n\t    hidden_states: torch.Tensor,\n\t    attention_mask: Optional[torch.Tensor] = None,\n\t    position_ids: Optional[torch.Tensor] = None,\n\t    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n\t    output_attentions: bool = False,\n\t    use_cache: bool = False,\n", ") -> Tuple[torch.Tensor, Optional[torch.Tensor],\n\t            Optional[Tuple[torch.Tensor]]]:\n\t    \"\"\"Input shape: Batch x Time x Channel\n\t    attention_mask: [bsz, q_len]\n\t    \"\"\"\n\t    bsz, q_len, _ = hidden_states.size()\n\t    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    # [bsz, q_len, nh, hd]\n", "    # [bsz, nh, q_len, hd]\n\t    kv_seq_len = key_states.shape[-2]\n\t    assert past_key_value is None, \"past_key_value is not supported\"\n\t    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\t    # [bsz, nh, t, hd]\n\t    assert not output_attentions, \"output_attentions is not supported\"\n\t    assert not use_cache, \"use_cache is not supported\"\n\t    # Flash attention codes from\n\t    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n", "    # transform the data into the format required by flash attention\n\t    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n\t    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n\t    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n\t    # the attention_mask should be the same as the key_padding_mask\n\t    key_padding_mask = attention_mask\n\t    if key_padding_mask is None:\n\t        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n\t        max_s = q_len\n\t        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n", "                                device=qkv.device)\n\t        output = flash_attn_unpadded_qkvpacked_func(\n\t            qkv, cu_q_lens, max_s, 0.0,\n\t            softmax_scale=None, causal=True\n\t        )\n\t        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n\t    else:\n\t        nheads = qkv.shape[-2]\n\t        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n\t        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n", "        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n\t        output_unpad = flash_attn_unpadded_qkvpacked_func(\n\t            x_unpad, cu_q_lens, max_s, 0.0,\n\t            softmax_scale=None, causal=True\n\t        )\n\t        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n\t                                    indices, bsz, q_len),\n\t                        'b s (h d) -> b s h d', h=nheads)\n\t    return self.o_proj(rearrange(output,\n\t                                    'b s h d -> b s (h d)')), None, None\n", "# Disable the transformation of the attention mask in LlamaModel as the flash attention\n\t# requires the attention mask to be the same as the key_padding_mask\n\tdef _prepare_decoder_attention_mask(self, attention_mask, input_shape,\n\t                                    inputs_embeds, past_key_values_length):\n\t    # [bsz, seq_len]\n\t    return attention_mask\n\tdef replace_llama_attn_with_flash_attn():\n\t    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n\t    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"]}
{"filename": "mCLS/train-CLS.py", "chunked_list": ["# coding=utf-8\n\t# Copyright 2018 The Google AI Language Team Authors, The HuggingFace Inc. team and Alibaba DAMO Academy.\n\t# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n", "# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t\"\"\" Finetuning the library models for question-answering on SQuAD (DistilBERT, Bert, XLM, XLNet).\"\"\"\n\t\"\"\"Adapted from the HuggingFace SQuAD finetuning script for CLS.\"\"\"\n\timport argparse\n\tfrom decimal import Decimal\n\tfrom typing import Dict\n", "import logging\n\timport os\n\timport json\n\timport random\n\timport timeit\n\timport transformers\n\tfrom transformers import (\n\t    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n\t    AutoTokenizer,\n\t    AutoModelForCausalLM,\n", ")\n\tfrom utils import collate_to_max_length_llama\n\timport difflib\n\tfrom data_clm import GLUEDataset as GLUEDataset_clm\n\tfrom transformers.trainer_utils import is_main_process\n\timport numpy as np\n\timport torch\n\tfrom torch.utils.data import DataLoader, SequentialSampler\n\tfrom tqdm import tqdm\n\ttry:\n", "    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n\texcept:\n\t    print('not applicable for flash attention')\n\tIGNORE_INDEX = -100\n\tDEFAULT_PAD_TOKEN = \"[PAD]\"\n\tDEFAULT_EOS_TOKEN = \"</s>\"\n\tDEFAULT_BOS_TOKEN = \"</s>\"\n\tDEFAULT_UNK_TOKEN = \"[UNK]\"\n\tlogger = logging.getLogger(__name__)\n\tMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n", "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\tdef string_compare(s, s1, s2):\n\t    score1 = difflib.SequenceMatcher(None, s, s1).quick_ratio()\n\t    score2 = difflib.SequenceMatcher(None, s, s2).quick_ratio()\n\t    if score1 >= score2:\n\t        return 0\n\t    else:\n\t        return 1\n\tdef set_seed(args):\n\t    random.seed(args.seed)\n", "    np.random.seed(args.seed)\n\t    torch.manual_seed(args.seed)\n\t    if args.n_gpu > 0:\n\t        torch.cuda.manual_seed_all(args.seed)\n\tdef smart_tokenizer_and_embedding_resize(\n\t    special_tokens_dict: Dict,\n\t    tokenizer: transformers.PreTrainedTokenizer,\n\t    model: transformers.PreTrainedModel,\n\t):\n\t    \"\"\"Resize tokenizer and embedding.\n", "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n\t    \"\"\"\n\t    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n\t    model.resize_token_embeddings(len(tokenizer))\n\t    if num_new_tokens > 0:\n\t        input_embeddings = model.get_input_embeddings().weight.data\n\t        output_embeddings = model.get_output_embeddings().weight.data\n\t        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n\t            dim=0, keepdim=True)\n\t        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n", "            dim=0, keepdim=True)\n\t        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n\t        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\tdef evaluate(args, data_path, model, tokenizer, prefix=\"\", split=\"dev\", lang='en'):\n\t    dataset = load_and_cache_examples(args, tokenizer, evaluate=True, data_path=data_path, split=split, lang=lang)\n\t    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n\t        os.makedirs(args.output_dir)\n\t    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n\t    # Note that DistributedSampler samples randomly\n\t    eval_sampler = SequentialSampler(dataset)\n", "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate_to_max_length_llama)\n\t    # multi-gpu evaluate\n\t    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n\t        model = torch.nn.DataParallel(model)\n\t    # Eval!\n\t    logger.warning(\"***** Running evaluation {} *****\".format(prefix))\n\t    logger.warning(\"  Num examples = %d\", len(dataset))\n\t    logger.warning(\"  Batch size = %d\", args.eval_batch_size)\n\t    all_preds_text = []\n\t    start_time = timeit.default_timer()\n", "    for batch in tqdm(eval_dataloader, desc=f\"Evaluating {lang} at {data_path}\"):\n\t        model.eval()\n\t        batch = tuple(t.to(args.device) for t in batch)\n\t        with torch.no_grad():\n\t            inputs = {\n\t                \"input_ids\": batch[0],\n\t                \"attention_mask\": batch[1],\n\t            }\n\t            outputs = model.generate(**inputs,\n\t                                     max_new_tokens=64,\n", "                                     eos_token_id=13,\n\t                                     )\n\t        dec = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\t        if args.model_type == 'llama' or args.model_type == 'bloom':\n\t            input_text = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[0]]\n\t            dec = [dec[i][len(input_text[i]):] for i in range(len(dec))]\n\t            dec = [dec[i].strip() for i in range(len(dec))]\n\t        all_preds_text.extend(dec)\n\t    all_preds = []\n\t    all_golds = []\n", "    fail_count = 0\n\t    label_dict = dataset.label_dict\n\t    for i_item, item in enumerate(dataset.all_data):\n\t        gold = item['gold_label']\n\t        label_text = item['labels']\n\t        pred_text = all_preds_text[i_item].lower()\n\t        if data_path in ['xcopa', \"xwinograd\", \"xstorycloze\", 'winogrande', 'copa']:\n\t            pred = string_compare(pred_text, label_text[0].lower(), label_text[1].lower())\n\t        else:\n\t            for i_label, label in enumerate(label_dict):\n", "                if label in pred_text:\n\t                    pred = label_dict[label]\n\t                    break\n\t            else:\n\t                logger.warning('unable to extract label with the following preditction: {}'.format(pred_text))\n\t                pred = 0\n\t                fail_count += 1\n\t        all_golds.append(gold)\n\t        all_preds.append(pred)\n\t    scores = dataset.compute_metric(all_preds, all_golds)\n", "    logger.warning(f\"the number of failure is {fail_count}.\")\n\t    logger.warning(f\"EVAL {lang} {split} at {data_path} -> acc is: {scores['acc']}. \")\n\t    return scores[\"acc\"]\n\tdef load_and_cache_examples(args, tokenizer, evaluate=False, data_path=None, split=\"train\", lang='en'):\n\t    if args.local_rank not in [-1, 0] and not evaluate:\n\t        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\t        torch.distributed.barrier()\n\t    if data_path is None:\n\t        data_path = args.data_path\n\t    logger.info(\"load data from {}.\".format(data_path))\n", "    if data_path == 'xnli' or data_path == 'pawsx':\n\t        tsv_path = os.path.join(\"./Data\", data_path, split + \"-\" + lang + \".tsv\")\n\t        dataset = []\n\t        with open(tsv_path, 'r') as reader:\n\t            for item in reader:\n\t                dataset.append(item.strip().split('\\t'))\n\t    elif data_path.startswith(\"marc\"):\n\t        json_path = os.path.join(\"./Data\", 'marc',  \"dataset_\" + lang + \"_\" + split + \".json\")\n\t        dataset = []\n\t        with open(json_path, 'r') as reader:\n", "            for line in reader:\n\t                dataset.append(json.loads(line))\n\t    elif data_path in ['xcopa', 'xwinograd', 'xstorycloze']:\n\t        json_path = os.path.join(\"./Data\", data_path,  lang + '-' + data_path + '-' + split + '.json')\n\t        with open(json_path, 'r') as reader:\n\t            for line in reader:\n\t                dataset = json.loads(line)\n\t    elif data_path in ['mnlim', 'mnlimm']:\n\t        json_path = os.path.join(\"./Data\", 'mnli',   data_path + '-' + split + '.json')\n\t        with open(json_path, 'r') as reader:\n", "            for line in reader:\n\t                dataset = json.loads(line)\n\t    elif data_path in [ 'copa', 'winogrande', 'wic', 'rte', \"obqa\"]:\n\t        json_path = os.path.join(\"./Data\", data_path, data_path + '-' + split + '.json')\n\t        with open(json_path, 'r') as reader:\n\t            for line in reader:\n\t                dataset = json.loads(line)\n\t    elif data_path in ['SST-2']:\n\t        tsv_path = os.path.join(\"./Data\", data_path,  split + '.tsv')\n\t        dataset = []\n", "        with open(tsv_path, 'r') as reader:\n\t            for il, line in enumerate(reader):\n\t                if il != 0:\n\t                    sen, label = line.strip().split(\"\\t\")\n\t                    dataset.append((sen, label))\n\t    else:\n\t        raise NotImplementedError\n\t    dataset = GLUEDataset_clm(\n\t        dataset=dataset,\n\t        data_path=data_path,\n", "        tokenizer=tokenizer,\n\t        lang=lang,\n\t    )\n\t    if args.local_rank == 0 and not evaluate:\n\t        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\t        torch.distributed.barrier()\n\t    return dataset\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    # Required parameters\n", "    parser.add_argument(\n\t        \"--model_type\",\n\t        default=None,\n\t        type=str,\n\t        required=True,\n\t        help=\"Model type selected in the list: \" + \", \".join(MODEL_TYPES),\n\t    )\n\t    parser.add_argument(\n\t        \"--model_name_or_path\",\n\t        default=None,\n", "        type=str,\n\t        required=True,\n\t        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n\t    )\n\t    parser.add_argument(\n\t        \"--output_dir\",\n\t        default=None,\n\t        type=str,\n\t        required=True,\n\t        help=\"The output directory where the model checkpoints and predictions will be written.\",\n", "    )\n\t    parser.add_argument(\n\t        \"--data_path\",\n\t        default=None,\n\t        type=str,\n\t        help=\"The input file.\",\n\t    )\n\t    parser.add_argument(\n\t        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n\t    )\n", "    parser.add_argument(\n\t        \"--tokenizer_name\",\n\t        default=\"\",\n\t        type=str,\n\t        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n\t    )\n\t    parser.add_argument(\n\t        \"--cache_dir\",\n\t        default=\"\",\n\t        type=str,\n", "        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n\t    )\n\t    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n\t    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n\t    parser.add_argument(\n\t        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n\t    )\n\t    parser.add_argument(\n\t        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n\t    )\n", "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n\t    parser.add_argument(\n\t        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n\t    )\n\t    parser.add_argument(\n\t        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n\t    )\n\t    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n\t    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n\t    parser.add_argument(\n", "        \"--fp16\",\n\t        action=\"store_true\",\n\t        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n\t    )\n\t    parser.add_argument(\n\t        \"--fp16_opt_level\",\n\t        type=str,\n\t        default=\"O1\",\n\t        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n\t        \"See details at https://nvidia.github.io/apex/amp.html\",\n", "    )\n\t    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n\t    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n\t    parser.add_argument(\"--keep_frac\", type=float, default=1.0, help=\"The fraction of the balanced dataset to keep.\")\n\t    parser.add_argument(\n\t        \"--flash_attn\", action=\"store_true\", help=\"use flash attn\"\n\t    )\n\t    args = parser.parse_args()\n\t    if (\n\t        os.path.exists(args.output_dir)\n", "        and os.listdir(args.output_dir)\n\t        and args.do_train\n\t        and not args.overwrite_output_dir\n\t    ):\n\t        raise ValueError(\n\t            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n\t                args.output_dir\n\t            )\n\t        )\n\t    # Setup distant debugging if needed\n", "    if args.server_ip and args.server_port:\n\t        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n\t        import ptvsd\n\t        print(\"Waiting for debugger attach\")\n\t        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n\t        ptvsd.wait_for_attach()\n\t    # Setup CUDA, GPU & distributed training\n\t    if args.local_rank == -1 or args.no_cuda:\n\t        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n\t        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n", "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n\t        torch.cuda.set_device(args.local_rank)\n\t        device = torch.device(\"cuda\", args.local_rank)\n\t        torch.distributed.init_process_group(backend=\"nccl\")\n\t        args.n_gpu = 1\n\t    args.device = device\n\t    # Setup logging\n\t    logging.basicConfig(\n\t        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n\t        datefmt=\"%m/%d/%Y %H:%M:%S\",\n", "        level=  logging.WARN,\n\t    )\n\t    logger.warning(\n\t        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n\t        args.local_rank,\n\t        device,\n\t        args.n_gpu,\n\t        bool(args.local_rank != -1),\n\t        args.fp16,\n\t    )\n", "    # Set the verbosity to info of the Transformers logger (on main process only):\n\t    if is_main_process(args.local_rank):\n\t        transformers.utils.logging.set_verbosity_info()\n\t        transformers.utils.logging.enable_default_handler()\n\t        transformers.utils.logging.enable_explicit_format()\n\t    # Set seed\n\t    set_seed(args)\n\t    # Load pretrained model and tokenizer\n\t    if args.local_rank not in [-1, 0]:\n\t        # Make sure only the first process in distributed training will download model & vocab\n", "        torch.distributed.barrier()\n\t    if args.flash_attn:\n\t        replace_llama_attn_with_flash_attn()\n\t    args.model_type = args.model_type.lower()\n\t    kwargs = {\"torch_dtype\": torch.float16}\n\t    model = transformers.LlamaForCausalLM.from_pretrained(\n\t        args.model_name_or_path,\n\t        cache_dir=args.cache_dir,\n\t        low_cpu_mem_usage=True, **kwargs,\n\t    )\n", "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n\t        args.model_name_or_path,\n\t        cache_dir=args.cache_dir,\n\t        padding_side=\"right\",\n\t        use_fast=False,\n\t    )\n\t    if tokenizer.pad_token is None:\n\t        smart_tokenizer_and_embedding_resize(\n\t            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n\t            tokenizer=tokenizer,\n", "            model=model,\n\t        )\n\t    if \"llama\" in args.model_name_or_path:\n\t        tokenizer.add_special_tokens({\n\t            \"eos_token\": DEFAULT_EOS_TOKEN,\n\t            \"bos_token\": DEFAULT_BOS_TOKEN,\n\t            \"unk_token\": DEFAULT_UNK_TOKEN,\n\t        })\n\t    else:\n\t        raise NotImplementedError\n", "    # model = model_PMR[args.model_type](config)\n\t    if args.local_rank == 0:\n\t        # Make sure only the first process in distributed training will download model & vocab\n\t        torch.distributed.barrier()\n\t    model.to(args.device)\n\t    logger.info(\"Training/evaluation parameters %s\", args)\n\t    # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n\t    # Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n\t    # remove the need for this code, but it is still valid.\n\t    if args.fp16:\n", "        try:\n\t            import apex\n\t            apex.amp.register_half_function(torch, \"einsum\")\n\t        except ImportError:\n\t            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n\t    # Set the verbosity to info of the Transformers logger (on main process only):\n\t    if is_main_process(args.local_rank):\n\t        transformers.utils.logging.set_verbosity_warning()\n\t    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n\t    if args.do_eval and args.local_rank in [-1, 0]:\n", "        if args.do_train:\n\t            pass\n\t        else:\n\t            if \"pawsx\" in args.data_path:\n\t                all_test = {}\n\t                for lang in ['en', 'de', 'es', 'fr', 'ja', 'ko', 'zh']:\n\t                    test_results = evaluate(args, \"pawsx\", model, tokenizer, prefix='', split=\"test\",\n\t                                            lang=lang)\n\t                    all_test[lang] = test_results\n\t                avg_acc = 0\n", "                for lang in ['en', 'de', 'es', 'fr', 'ja', 'ko', 'zh']:\n\t                    avg_acc += all_test[lang]\n\t                all_test[\"pawsx-avg\"] = avg_acc / 7\n\t                logger.warning(f\"EVAL INFO mix of pawsx -> valid_f1 is: {all_test['pawsx-avg']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"pawsx\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n", "                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if 'xwinograd' in args.data_path:\n\t                all_test = {}\n\t                for lang in ['en', 'fr', 'jp', 'pt', 'ru', 'zh']:\n\t                    test_results = evaluate(args, 'xwinograd', model, tokenizer, prefix='', split=\"test\",\n\t                                            lang=lang)\n\t                    all_test[lang] = test_results\n\t                avg_acc = 0\n\t                for lang in ['en', 'fr', 'jp', 'pt', 'ru', 'zh']:\n", "                    avg_acc += all_test[lang]\n\t                all_test[\"xwinograd-avg\"] = avg_acc / 6\n\t                logger.warning(f\"EVAL INFO mix of xwinograd -> valid_f1 is: {all_test['xwinograd-avg']}\")\n\t                save_line = [Decimal(all_test[x]*100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"xwinograd\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n", "            if 'xnli' in args.data_path:\n\t                all_test = {}\n\t                for lang in ['en', 'zh', 'es', 'de', 'ar', 'ur', 'ru', 'bg', 'el', 'fr', 'hi', 'sw', 'th', 'tr', 'vi']:\n\t                    test_results = evaluate(args, 'xnli', model, tokenizer, prefix='', split=\"test\",\n\t                                            lang=lang)\n\t                    all_test[lang] = test_results\n\t                avg_acc = 0\n\t                for lang in ['en', 'zh', 'es', 'de', 'ar', 'ur', 'ru', 'bg', 'el', 'fr', 'hi', 'sw', 'th', 'tr', 'vi']:\n\t                    avg_acc += all_test[lang]\n\t                all_test[\"xnli-avg\"] = avg_acc / 15\n", "                logger.warning(f\"EVAL INFO mix of xnli -> valid_f1 is: {all_test['xnli-avg']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join(['xnli'] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"xcopa\" in args.data_path :\n", "                all_test = {}\n\t                for lang in ['et', 'ht', 'id', 'it', 'qu', 'sw', 'ta', 'th', 'tr', 'vi', 'zh']:\n\t                    test_results = evaluate(args, \"xcopa\", model, tokenizer, prefix='', split=\"test\",\n\t                                            lang=lang)\n\t                    all_test[lang] = test_results\n\t                avg_acc = 0\n\t                for lang in ['et', 'ht', 'id', 'it', 'qu', 'sw', 'ta', 'th', 'tr', 'vi', 'zh']:\n\t                    avg_acc += all_test[lang]\n\t                all_test[\"xcopa-avg\"] = avg_acc / 11\n\t                logger.warning(f\"EVAL INFO mix of xcopa -> valid_f1 is: {all_test['xcopa-avg']}\")\n", "                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"xcopa\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"xstorycloze\" in args.data_path :\n\t                all_test = {}\n", "                for lang in [\"en\", \"ru\", \"zh\", \"es\", \"ar\", \"hi\", \"id\", \"te\", \"sw\", \"eu\", \"my\"]:\n\t                    test_results = evaluate(args, \"xstorycloze\", model, tokenizer, prefix='', split=\"test\",\n\t                                            lang=lang)\n\t                    all_test[lang] = test_results\n\t                avg_acc = 0\n\t                for lang in [\"en\", \"ru\", \"zh\", \"es\", \"ar\", \"hi\", \"id\", \"te\", \"sw\", \"eu\", \"my\"]:\n\t                    avg_acc += all_test[lang]\n\t                all_test[\"xstorycloze-avg\"] = avg_acc / 11\n\t                logger.warning(f\"EVAL INFO mix of xstorycloze -> valid_f1 is: {all_test['xstorycloze-avg']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n", "                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"xstorycloze\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"marc-2\" in args.data_path:\n\t                all_test = {}\n\t                for lang in ['en', 'es', 'fr', 'de', 'ja', 'zh']:\n", "                    test_results = evaluate(args, \"marc-2\", model, tokenizer, prefix='', split=\"test\",\n\t                                            lang=lang)\n\t                    all_test[lang] = test_results\n\t                avg_acc = 0\n\t                for lang in ['en', 'es', 'fr', 'de', 'ja', 'zh']:\n\t                    avg_acc += all_test[lang]\n\t                all_test[\"marc-avg\"] = avg_acc / 6\n\t                logger.warning(f\"EVAL INFO mix of marc -> valid_f1 is: {all_test['marc-avg']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n", "                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"marc-2\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"marc-5\" in args.data_path:\n\t                all_test = {}\n\t                for lang in ['en', 'es', 'fr', 'de', 'ja', 'zh']:\n\t                    test_results = evaluate(args, \"marc-5\", model, tokenizer, prefix='', split=\"test\",\n", "                                            lang=lang)\n\t                    all_test[lang] = test_results\n\t                avg_acc = 0\n\t                for lang in ['en', 'es', 'fr', 'de', 'ja', 'zh']:\n\t                    avg_acc += all_test[lang]\n\t                all_test[\"marc-avg\"] = avg_acc / 6\n\t                logger.warning(f\"EVAL INFO mix of marc -> valid_f1 is: {all_test['marc-avg']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n", "                save_line = '\\t'.join([\"marc-5\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"SST-2\" in args.data_path:\n\t                all_test = {}\n\t                test_results = evaluate(args, \"SST-2\", model, tokenizer, prefix='', split=\"test\",\n\t                                        lang='en')\n\t                all_test['en'] = test_results\n", "                logger.warning(f\"EVAL INFO mix of SST-2 -> valid_f1 is: {all_test['en']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"SST-2\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"copa\" in args.data_path:\n", "                all_test = {}\n\t                test_results = evaluate(args, \"copa\", model, tokenizer, prefix='', split=\"test\",\n\t                                        lang='en')\n\t                all_test['en'] = test_results\n\t                logger.warning(f\"EVAL INFO mix of copa -> valid_f1 is: {all_test['en']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"copa\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n", "                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"wic\" in args.data_path:\n\t                all_test = {}\n\t                test_results = evaluate(args, \"wic\", model, tokenizer, prefix='', split=\"test\",\n\t                                        lang='en')\n\t                all_test['en'] = test_results\n\t                logger.warning(f\"EVAL INFO mix of wic -> valid_f1 is: {all_test['en']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n", "                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"wic\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"rte\" in args.data_path:\n\t                all_test = {}\n\t                test_results = evaluate(args, \"rte\", model, tokenizer, prefix='', split=\"test\",\n", "                                        lang='en')\n\t                all_test['en'] = test_results\n\t                logger.warning(f\"EVAL INFO mix of rte -> valid_f1 is: {all_test['en']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"rte\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n", "                    writer2.write(save_string + '\\n')\n\t            if \"winogrande\" in args.data_path:\n\t                all_test = {}\n\t                test_results = evaluate(args, \"winogrande\", model, tokenizer, prefix='', split=\"test\",\n\t                                        lang='en')\n\t                all_test['en'] = test_results\n\t                logger.warning(f\"EVAL INFO mix of winogrande -> valid_f1 is: {all_test['en']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n", "                save_line = '\\t'.join([\"winogrande\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"obqa\" in args.data_path:\n\t                all_test = {}\n\t                test_results = evaluate(args, \"obqa\", model, tokenizer, prefix='', split=\"test\",\n\t                                        lang='en')\n\t                all_test['en'] = test_results\n", "                logger.warning(f\"EVAL INFO mix of obqa -> valid_f1 is: {all_test['en']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"obqa\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t            if \"mnli\" in args.data_path:\n", "                all_test = {}\n\t                test_results = evaluate(args, \"mnlim\", model, tokenizer, prefix='', split=\"test\",\n\t                                        lang='en')\n\t                all_test['en'] = test_results\n\t                logger.warning(f\"EVAL INFO mix of mnlim -> valid_f1 is: {all_test['en']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n\t                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"mnlim\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n", "                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t                all_test = {}\n\t                test_results = evaluate(args, \"mnlimm\", model, tokenizer, prefix='', split=\"test\",\n\t                                        lang='en')\n\t                all_test['en'] = test_results\n\t                logger.warning(f\"EVAL INFO mix of mnlimm -> valid_f1 is: {all_test['en']}\")\n\t                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n\t                             all_test]\n", "                save_line = [str(x) for x in save_line]\n\t                save_line = '\\t'.join([\"mnlimm\"] + save_line)\n\t                with open('temp.txt', 'a') as writer2:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "mCLS/utils.py", "chunked_list": ["import torch\n\tfrom transformers.utils import logging\n\tlogger = logging.get_logger(__name__)\n\tdef collate_to_max_length_llama(batch):\n\t    \"\"\"\n\t    adapted form https://github.com/ShannonAI/mrc-for-flat-nested-ner\n\t    pad to maximum length of this batch\n\t    \"\"\"\n\t    batch_size = len(batch)\n\t    max_length_enc = max(x[0].shape[1] for x in batch)\n", "    output = []\n\t    for field_idx in range(3):\n\t        if field_idx != 1:\n\t            pad_output = torch.full([batch_size, max_length_enc], 32000, dtype=batch[0][field_idx].dtype) # 32000 as pad token\n\t        else:\n\t            pad_output = torch.full([batch_size, max_length_enc], 0, dtype=batch[0][field_idx].dtype)\n\t        for sample_idx in range(batch_size):\n\t            data = batch[sample_idx][field_idx]\n\t            pad_output[sample_idx][max_length_enc - data.shape[1]:] = data # padding left\n\t        output.append(pad_output)\n", "    return output\n"]}
{"filename": "mQA/data_clm.py", "chunked_list": ["import torch\n\tfrom torch.utils.data import Dataset\n\tfrom transformers.utils import logging\n\timport random\n\timport os\n\tfrom tqdm import tqdm\n\tfrom utils import SquadV1Processor\n\tlogger = logging.get_logger(__name__)\n\tMULTI_SEP_TOKENS_TOKENIZERS_SET = {\"roberta\", \"camembert\", \"bart\", \"mpnet\"}\n\trandom.seed(42)\n", "class QADataset(Dataset):\n\t    \"\"\"\n\t    MRC QA Dataset\n\t    \"\"\"\n\t    def __init__(self, features, tokenizer: None, data_name: None,):\n\t        self.all_data = features\n\t        self.data_name = data_name\n\t        self.tokenizer = tokenizer\n\t    def __len__(self):\n\t        return len(self.all_data)\n", "    def __getitem__(self, item):\n\t        \"\"\"\n\t        Args:\n\t            item: int, idx\n\t        \"\"\"\n\t        data = self.all_data[item]\n\t        qid, inputs, outputs = data\n\t        return [\n\t            torch.LongTensor(inputs['input_ids']),\n\t            torch.LongTensor(inputs['attention_mask']),\n", "            torch.LongTensor(outputs['input_ids']),\n\t            torch.LongTensor(outputs['attention_mask']),\n\t        ]\n\tdef cache_mQAexamples(args, tokenizer, dataset_name, split, lang='en'):\n\t    num_qa = 0\n\t    processor = SquadV1Processor()\n\t    if split == \"train\" and dataset_name == \"TyDiQA\":\n\t        input_data = processor.get_examples(os.path.join('./Data', 'tydiqa', 'tydiqa-goldp-v1.1-train'),\n\t                                            \"tydiqa.goldp.en.train.json\")\n\t    elif split == \"dev\" and dataset_name == \"TyDiQA\":\n", "        input_data = processor.get_examples(os.path.join('./Data', 'tydiqa', 'tydiqa-goldp-v1.1-dev'),\n\t                                            \"tydiqa.goldp.\" + lang + \".dev.json\")\n\t    elif split == \"train\" and dataset_name == \"SQuAD\":\n\t        input_data = processor.get_examples(os.path.join('./Data', 'squad'), \"train-v1.1.json\")\n\t    elif split == \"dev\" and dataset_name == \"SQuAD\":\n\t        input_data = processor.get_examples(os.path.join('./Data', 'squad'), \"dev-v1.1.json\")\n\t    elif split == \"dev\" and dataset_name == \"XQuAD\":\n\t        input_data = processor.get_examples(os.path.join('./Data', 'xquad'), \"xquad.\" + lang + \".json\")\n\t    elif split == \"dev\" and dataset_name == \"MLQA\":\n\t        input_data = processor.get_examples(os.path.join('./Data', 'mlqa/test'), 'test-context-' + lang + \"-question-\" + lang + \".json\")\n", "    features = []\n\t    for i_t, item in enumerate(tqdm(input_data, desc=\"creating {} features from {}\".format(split, dataset_name))):\n\t        context = item.doc_tokens\n\t        context_str = \" \".join([x[0] for x in context])\n\t        num_qa += 1\n\t        qid = item.qas_id\n\t        answers = item.answers\n\t        query_str = item.question_text\n\t        answer_str = answers[0]['text']\n\t        if split != 'train':\n", "            input_str = f'Answer the question according to the context. Question: {query_str}. Context: {context_str}. Answer:'\n\t            inputs = tokenizer(input_str, return_tensors='pt', truncation=True, max_length=1024)\n\t            outputs = tokenizer(answer_str, return_tensors='pt')\n\t            features.append((qid, inputs, outputs))\n\t        else:\n\t            raise NotImplementedError\n\t    logger.warning(\"finish creating {} features on {} questions\".format(len(features), num_qa))\n\t    return features"]}
{"filename": "mQA/llama_flash_attn_monkey_patch.py", "chunked_list": ["\"\"\"\n\tReplace vanilla attention in Huggingface's Llama implementation with flash attention.\n\tAdapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py \n\t\"\"\"\n\tfrom typing import List, Optional, Tuple\n\timport torch\n\tfrom torch import nn\n\timport transformers\n\tfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\tfrom einops import rearrange\n", "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n\tfrom flash_attn.bert_padding import unpad_input, pad_input\n\tdef forward(\n\t    self,\n\t    hidden_states: torch.Tensor,\n\t    attention_mask: Optional[torch.Tensor] = None,\n\t    position_ids: Optional[torch.Tensor] = None,\n\t    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n\t    output_attentions: bool = False,\n\t    use_cache: bool = False,\n", ") -> Tuple[torch.Tensor, Optional[torch.Tensor],\n\t            Optional[Tuple[torch.Tensor]]]:\n\t    \"\"\"Input shape: Batch x Time x Channel\n\t    attention_mask: [bsz, q_len]\n\t    \"\"\"\n\t    bsz, q_len, _ = hidden_states.size()\n\t    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t    # [bsz, q_len, nh, hd]\n", "    # [bsz, nh, q_len, hd]\n\t    kv_seq_len = key_states.shape[-2]\n\t    assert past_key_value is None, \"past_key_value is not supported\"\n\t    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\t    # [bsz, nh, t, hd]\n\t    assert not output_attentions, \"output_attentions is not supported\"\n\t    assert not use_cache, \"use_cache is not supported\"\n\t    # Flash attention codes from\n\t    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n", "    # transform the data into the format required by flash attention\n\t    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n\t    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n\t    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n\t    # the attention_mask should be the same as the key_padding_mask\n\t    key_padding_mask = attention_mask\n\t    if key_padding_mask is None:\n\t        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n\t        max_s = q_len\n\t        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n", "                                device=qkv.device)\n\t        output = flash_attn_unpadded_qkvpacked_func(\n\t            qkv, cu_q_lens, max_s, 0.0,\n\t            softmax_scale=None, causal=True\n\t        )\n\t        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n\t    else:\n\t        nheads = qkv.shape[-2]\n\t        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n\t        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n", "        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n\t        output_unpad = flash_attn_unpadded_qkvpacked_func(\n\t            x_unpad, cu_q_lens, max_s, 0.0,\n\t            softmax_scale=None, causal=True\n\t        )\n\t        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n\t                                    indices, bsz, q_len),\n\t                        'b s (h d) -> b s h d', h=nheads)\n\t    return self.o_proj(rearrange(output,\n\t                                    'b s h d -> b s (h d)')), None, None\n", "# Disable the transformation of the attention mask in LlamaModel as the flash attention\n\t# requires the attention mask to be the same as the key_padding_mask\n\tdef _prepare_decoder_attention_mask(self, attention_mask, input_shape,\n\t                                    inputs_embeds, past_key_values_length):\n\t    # [bsz, seq_len]\n\t    return attention_mask\n\tdef replace_llama_attn_with_flash_attn():\n\t    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n\t    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"]}
{"filename": "mQA/utils.py", "chunked_list": ["import torch\n\tfrom transformers.utils import logging\n\tfrom transformers.data.processors.squad import DataProcessor, _is_whitespace\n\timport os\n\tfrom tqdm import tqdm\n\timport json\n\tlogger = logging.get_logger(__name__)\n\tdef collate_to_max_length_llama(batch):\n\t    \"\"\"\n\t    adapted form https://github.com/ShannonAI/mrc-for-flat-nested-ner\n", "    pad to maximum length of this batch\n\t    Args:\n\t        batch: a batch of samples, each contains a list of field data(Tensor):\n\t            tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, match_labels, sample_idx, label_idx\n\t    Returns:\n\t        output: list of field batched data, which shape is [batch, max_length]\n\t    \"\"\"\n\t    batch_size = len(batch)\n\t    max_length_enc = max(x[0].shape[1] for x in batch)\n\t    output = []\n", "    for field_idx in range(3):\n\t        if field_idx != 1:\n\t            pad_output = torch.full([batch_size, max_length_enc], 32000, dtype=batch[0][field_idx].dtype) # 32000 as pad token\n\t        else:\n\t            pad_output = torch.full([batch_size, max_length_enc], 0, dtype=batch[0][field_idx].dtype)\n\t        for sample_idx in range(batch_size):\n\t            data = batch[sample_idx][field_idx]\n\t            pad_output[sample_idx][max_length_enc - data.shape[1]:] = data # padding left\n\t        output.append(pad_output)\n\t    return output\n", "class SquadProcessor(DataProcessor):\n\t    \"\"\"\n\t    Processor for the SQuAD data set. overridden by SquadV1Processor and SquadV2Processor, used by the version 1.1 and\n\t    version 2.0 of SQuAD, respectively.\n\t    \"\"\"\n\t    train_file = None\n\t    dev_file = None\n\t    def _get_example_from_tensor_dict(self, tensor_dict, evaluate=False):\n\t        if not evaluate:\n\t            answer = tensor_dict[\"answers\"][\"text\"][0].numpy().decode(\"utf-8\")\n", "            answer_start = tensor_dict[\"answers\"][\"answer_start\"][0].numpy()\n\t            answers = []\n\t        else:\n\t            answers = [\n\t                {\"answer_start\": start.numpy(), \"text\": text.numpy().decode(\"utf-8\")}\n\t                for start, text in zip(tensor_dict[\"answers\"][\"answer_start\"], tensor_dict[\"answers\"][\"text\"])\n\t            ]\n\t            answer = None\n\t            answer_start = None\n\t        return SquadExample(\n", "            qas_id=tensor_dict[\"id\"].numpy().decode(\"utf-8\"),\n\t            question_text=tensor_dict[\"question\"].numpy().decode(\"utf-8\"),\n\t            context_text=tensor_dict[\"context\"].numpy().decode(\"utf-8\"),\n\t            answer_text=answer,\n\t            start_position_character=answer_start,\n\t            title=tensor_dict[\"title\"].numpy().decode(\"utf-8\"),\n\t            answers=answers,\n\t        )\n\t    def get_examples_from_dataset(self, dataset, evaluate=False):\n\t        \"\"\"\n", "        Creates a list of :class:`~transformers.data.processors.squad.SquadExample` using a TFDS dataset.\n\t        Args:\n\t            dataset: The tfds dataset loaded from `tensorflow_datasets.load(\"squad\")`\n\t            evaluate: Boolean specifying if in evaluation mode or in training mode\n\t        Returns:\n\t            List of SquadExample\n\t        \"\"\"\n\t        if evaluate:\n\t            dataset = dataset[\"validation\"]\n\t        else:\n", "            dataset = dataset[\"train\"]\n\t        examples = []\n\t        for tensor_dict in tqdm(dataset):\n\t            examples.append(self._get_example_from_tensor_dict(tensor_dict, evaluate=evaluate))\n\t        return examples\n\t    def get_examples(self, data_dir, filename=None):\n\t        \"\"\"\n\t        Returns the evaluation example from the data directory.\n\t        Args:\n\t            data_dir: Directory containing the data files used for training and evaluating.\n", "            filename: None by default, specify this if the evaluation file has a different name than the original one\n\t                which is `dev-v1.1.json` and `dev-v2.0.json` for squad versions 1.1 and 2.0 respectively.\n\t        \"\"\"\n\t        if data_dir is None:\n\t            data_dir = \"\"\n\t        if self.dev_file is None:\n\t            raise ValueError(\"SquadProcessor should be instantiated via SquadV1Processor or SquadV2Processor\")\n\t        with open(\n\t            os.path.join(data_dir, self.dev_file if filename is None else filename), \"r\", encoding=\"utf-8\"\n\t        ) as reader:\n", "            input_data = json.load(reader)[\"data\"]\n\t        return self._create_examples(input_data, \"dev\")\n\t    def _create_examples(self, input_data, set_type):\n\t        is_training = set_type == \"train\"\n\t        examples = []\n\t        for entry in tqdm(input_data):\n\t            title = entry[\"title\"] if entry.get('title') else None\n\t            for paragraph in entry[\"paragraphs\"]:\n\t                context_text = paragraph[\"context\"]\n\t                for qa in paragraph[\"qas\"]:\n", "                    qas_id = qa[\"id\"]\n\t                    question_text = qa[\"question\"]\n\t                    start_position_character = None\n\t                    answer_text = None\n\t                    answers = []\n\t                    is_impossible = qa.get(\"is_impossible\", False)\n\t                    if not is_impossible:\n\t                        if is_training:\n\t                            answer = qa[\"answers\"][0]\n\t                            answer_text = answer[\"text\"]\n", "                            start_position_character = answer[\"answer_start\"]\n\t                        else:\n\t                            answers = qa[\"answers\"]\n\t                    example = SquadExample(\n\t                        qas_id=qas_id,\n\t                        question_text=question_text,\n\t                        context_text=context_text,\n\t                        answer_text=answer_text,\n\t                        start_position_character=start_position_character,\n\t                        title=title,\n", "                        is_impossible=is_impossible,\n\t                        answers=answers,\n\t                    )\n\t                    examples.append(example)\n\t        return examples\n\tclass SquadV1Processor(SquadProcessor):\n\t    train_file = \"train-v1.1.json\"\n\t    dev_file = \"dev-v1.1.json\"\n\tclass SquadV2Processor(SquadProcessor):\n\t    train_file = \"train-v2.0.json\"\n", "    dev_file = \"dev-v2.0.json\"\n\tclass SquadExample:\n\t    \"\"\"\n\t    A single training/test example for the Squad dataset, as loaded from disk.\n\t    Args:\n\t        qas_id: The example's unique identifier\n\t        question_text: The question string\n\t        context_text: The context string\n\t        answer_text: The answer string\n\t        start_position_character: The character position of the start of the answer\n", "        title: The title of the example\n\t        answers: None by default, this is used during evaluation. Holds answers as well as their start positions.\n\t        is_impossible: False by default, set to True if the example has no possible answer.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        qas_id,\n\t        question_text,\n\t        context_text,\n\t        answer_text,\n", "        start_position_character,\n\t        title,\n\t        answers=[],\n\t        is_impossible=False,\n\t    ):\n\t        self.qas_id = qas_id\n\t        self.question_text = question_text\n\t        self.context_text = context_text\n\t        self.answer_text = answer_text\n\t        self.title = title\n", "        self.is_impossible = is_impossible\n\t        self.answers = answers\n\t        self.start_position, self.end_position = 0, 0\n\t        doc_tokens = []\n\t        doc_idx = []\n\t        char_to_word_offset = []\n\t        prev_is_whitespace = True\n\t        # Split on whitespace so that different tokens may be attributed to their original position.\n\t        for i_c, c in enumerate(self.context_text):\n\t            if _is_whitespace(c):\n", "                prev_is_whitespace = True\n\t            else:\n\t                if prev_is_whitespace:\n\t                    doc_tokens.append(c)\n\t                    doc_idx.append(i_c)\n\t                else:\n\t                    doc_tokens[-1] += c\n\t                prev_is_whitespace = False\n\t            char_to_word_offset.append(len(doc_tokens) - 1)\n\t        assert len(doc_tokens) == len(doc_idx)\n", "        self.doc_tokens = list(zip(doc_tokens, doc_idx))\n\t        self.char_to_word_offset = char_to_word_offset\n\t        # Start and end positions only has a value during evaluation.\n\t        if start_position_character is not None and not is_impossible:\n\t            self.start_position = char_to_word_offset[start_position_character]\n\t            self.end_position = char_to_word_offset[\n\t                min(start_position_character + len(answer_text) - 1, len(char_to_word_offset) - 1)\n\t            ]"]}
{"filename": "mQA/squad_evaluation_v1.py", "chunked_list": ["\"\"\"Official evaluation script for the MRQA Workshop Shared Task.\n\tAdapted fromt the SQuAD v1.1 official evaluation script.\n\tUsage:\n\t    python official_eval.py dataset_file.jsonl.gz prediction_file.json\n\t\"\"\"\n\tfrom __future__ import absolute_import\n\tfrom __future__ import division\n\tfrom __future__ import print_function\n\timport argparse\n\timport string\n", "import re\n\timport json\n\timport sys\n\tfrom collections import Counter\n\tdef normalize_answer(s):\n\t    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\t    def remove_articles(text):\n\t        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\t    def white_space_fix(text):\n\t        return ' '.join(text.split())\n", "    def remove_punc(text):\n\t        exclude = set(string.punctuation)\n\t        return ''.join(ch for ch in text if ch not in exclude)\n\t    def lower(text):\n\t        return text.lower()\n\t    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\tdef f1_score(prediction, ground_truth):\n\t    prediction_tokens = normalize_answer(prediction).split()\n\t    ground_truth_tokens = normalize_answer(ground_truth).split()\n\t    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n", "    num_same = sum(common.values())\n\t    if num_same == 0:\n\t        return 0\n\t    precision = 1.0 * num_same / len(prediction_tokens)\n\t    recall = 1.0 * num_same / len(ground_truth_tokens)\n\t    f1 = (2 * precision * recall) / (precision + recall)\n\t    return f1\n\tdef exact_match_score(prediction, ground_truth):\n\t    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\tdef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n", "    scores_for_ground_truths = []\n\t    for ground_truth in ground_truths:\n\t        score = metric_fn(prediction, ground_truth)\n\t        scores_for_ground_truths.append(score)\n\t    return max(scores_for_ground_truths)\n\tdef read_predictions(prediction_file):\n\t    with open(prediction_file) as f:\n\t        predictions = json.load(f)\n\t    return predictions\n\tdef evaluate(dataset, predictions):\n", "    f1 = exact_match = total = 0\n\t    for article in dataset:\n\t        for paragraph in article['paragraphs']:\n\t            for qa in paragraph['qas']:\n\t                total += 1\n\t                if qa['id'] not in predictions:\n\t                    message = 'Unanswered question ' + qa['id'] + \\\n\t                              ' will receive score 0.'\n\t                    print(message, file=sys.stderr)\n\t                    continue\n", "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n\t                prediction = predictions[qa['id']]\n\t                exact_match += metric_max_over_ground_truths(\n\t                    exact_match_score, prediction, ground_truths)\n\t                f1 += metric_max_over_ground_truths(\n\t                    f1_score, prediction, ground_truths)\n\t    exact_match = 100.0 * exact_match / total\n\t    f1 = 100.0 * f1 / total\n\t    return {'exact_match': exact_match, 'f1': f1}\n\tdef compare(dataset, predictions_str, predictions_tok):\n", "    f1 = exact_match_str = exact_match_tok = total = 0\n\t    for article in dataset:\n\t        for paragraph in article['paragraphs']:\n\t            for qa in paragraph['qas']:\n\t                total += 1\n\t                if qa['id'] not in predictions_str and qa['id'] not in predictions_tok:\n\t                    message = 'Unanswered question ' + qa['id'] + \\\n\t                              ' will receive score 0.'\n\t                    print(message, file=sys.stderr)\n\t                    continue\n", "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n\t                prediction_str = predictions_str[qa['id']]\n\t                prediction_tok = predictions_tok[qa['id']]\n\t                exact_match_str_one = metric_max_over_ground_truths(\n\t                    exact_match_score, prediction_str, ground_truths)\n\t                exact_match_str += exact_match_str_one\n\t                exact_match_tok_one = metric_max_over_ground_truths(\n\t                    exact_match_score, prediction_tok, ground_truths)\n\t                exact_match_tok += exact_match_tok_one\n\t                f1 += metric_max_over_ground_truths(\n", "                    f1_score, prediction_str, ground_truths)\n\t                if exact_match_str_one == 1 and exact_match_tok_one == 0:\n\t                    print(qa['id'])\n\t                    print('str answer:', prediction_str)\n\t                    print('tok answer:', prediction_tok)\n\t                    print('ground truth:', ground_truths)\n\t                    print()\n\t    exact_match = 100.0 * exact_match_str / total\n\t    f1 = 100.0 * f1 / total\n\t    return {'exact_match': exact_match, 'f1': f1}\n", "if __name__ == '__main__':\n\t    parser = argparse.ArgumentParser(\n\t        description='Evaluation for MRQA Workshop Shared Task')\n\t    parser.add_argument('dataset_file', type=str, help='Dataset File')\n\t    parser.add_argument('prediction_file', type=str, help='Prediction File')\n\t    parser.add_argument('--skip-no-answer', action='store_true')\n\t    args = parser.parse_args()\n\t    with open(args.dataset_file) as dataset_file:\n\t        dataset_json = json.load(dataset_file)\n\t        dataset = dataset_json['data']\n", "    with open(args.prediction_file+'_tok.json') as prediction_file:\n\t        predictions_tok = json.load(prediction_file)\n\t    with open(args.prediction_file+'_str.json') as prediction_file:\n\t        predictions_str = json.load(prediction_file)\n\t    compare(dataset, predictions_str, predictions_tok)\n"]}
{"filename": "mQA/mlqa_evaluation_v1.py", "chunked_list": ["# Copyright (c) 2019-present, Facebook, Inc\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t#\n\t\"\"\" Official evaluation script for the MLQA dataset. \"\"\"\n\tfrom __future__ import print_function\n\tfrom collections import Counter\n\timport string\n", "import re\n\timport argparse\n\timport json\n\timport sys\n\timport unicodedata\n\tPUNCT = {chr(i) for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P')}.union(string.punctuation)\n\tWHITESPACE_LANGS = ['en', 'es', 'hi', 'vi', 'de', 'ar']\n\tMIXED_SEGMENTATION_LANGS = ['zh']\n\tdef whitespace_tokenize(text):\n\t    return text.split()\n", "def mixed_segmentation(text):\n\t    segs_out = []\n\t    temp_str = \"\"\n\t    for char in text:\n\t        if re.search(r'[\\u4e00-\\u9fa5]', char) or char in PUNCT:\n\t            if temp_str != \"\":\n\t                ss = whitespace_tokenize(temp_str)\n\t                segs_out.extend(ss)\n\t                temp_str = \"\"\n\t            segs_out.append(char)\n", "        else:\n\t            temp_str += char\n\t    if temp_str != \"\":\n\t        ss = whitespace_tokenize(temp_str)\n\t        segs_out.extend(ss)\n\t    return segs_out\n\tdef normalize_answer(s, lang):\n\t    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\t    def remove_articles(text, lang):\n\t        if lang == 'en':\n", "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\t        elif lang == 'es':\n\t            return re.sub(r'\\b(un|una|unos|unas|el|la|los|las)\\b', ' ', text)\n\t        elif lang == 'hi':\n\t            return text # Hindi does not have formal articles\n\t        elif lang == 'vi':\n\t            return re.sub(r'\\b(của|là|cái|chiếc|những)\\b', ' ', text)\n\t        elif lang == 'de':\n\t            return re.sub(r'\\b(ein|eine|einen|einem|eines|einer|der|die|das|den|dem|des)\\b', ' ', text)\n\t        elif lang == 'ar':\n", "            return re.sub('\\sال^|ال', ' ', text)\n\t        elif lang == 'zh':\n\t            return text # Chinese does not have formal articles\n\t        else:\n\t            raise Exception('Unknown Language {}'.format(lang))\n\t    def white_space_fix(text, lang):\n\t        if lang in WHITESPACE_LANGS:\n\t            tokens = whitespace_tokenize(text)\n\t        elif lang in MIXED_SEGMENTATION_LANGS:\n\t            tokens = mixed_segmentation(text)\n", "        else:\n\t            raise Exception('Unknown Language {}'.format(lang))\n\t        return ' '.join([t for t in tokens if t.strip() != ''])\n\t    def remove_punc(text):\n\t        return ''.join(ch for ch in text if ch not in PUNCT)\n\t    def lower(text):\n\t        return text.lower()\n\t    return white_space_fix(remove_articles(remove_punc(lower(s)), lang), lang)\n\tdef f1_score(prediction, ground_truth, lang):\n\t    prediction_tokens = normalize_answer(prediction, lang).split()\n", "    ground_truth_tokens = normalize_answer(ground_truth, lang).split()\n\t    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n\t    num_same = sum(common.values())\n\t    if num_same == 0:\n\t        return 0\n\t    precision = 1.0 * num_same / len(prediction_tokens)\n\t    recall = 1.0 * num_same / len(ground_truth_tokens)\n\t    f1 = (2 * precision * recall) / (precision + recall)\n\t    return f1\n\tdef exact_match_score(prediction, ground_truth, lang):\n", "    return (normalize_answer(prediction, lang) == normalize_answer(ground_truth, lang))\n\tdef metric_max_over_ground_truths(metric_fn, prediction, ground_truths, lang):\n\t    scores_for_ground_truths = []\n\t    for ground_truth in ground_truths:\n\t        score = metric_fn(prediction, ground_truth, lang)\n\t        scores_for_ground_truths.append(score)\n\t    return max(scores_for_ground_truths)\n\tdef evaluate(dataset, predictions, lang):\n\t    f1 = exact_match = total = 0\n\t    for article in dataset:\n", "        for paragraph in article['paragraphs']:\n\t            for qa in paragraph['qas']:\n\t                total += 1\n\t                if qa['id'] not in predictions:\n\t                    message = 'Unanswered question ' + qa['id'] + \\\n\t                              ' will receive score 0.'\n\t                    print(message, file=sys.stderr)\n\t                    continue\n\t                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n\t                prediction = predictions[qa['id']]\n", "                exact_match += metric_max_over_ground_truths(\n\t                    exact_match_score, prediction, ground_truths, lang)\n\t                f1 += metric_max_over_ground_truths(\n\t                    f1_score, prediction, ground_truths, lang)\n\t    exact_match = 100.0 * exact_match / total\n\t    f1 = 100.0 * f1 / total\n\t    return {'exact_match': exact_match, 'f1': f1}\n\tdef compare(dataset, predictions_mpmr, lang):\n\t    count = 0\n\t    f1 = exact_match_mpmr = exact_match_xlmr = total = 0\n", "    for i_a, article in enumerate(dataset):\n\t        for paragraph in article['paragraphs']:\n\t            for qa in paragraph['qas']:\n\t                total += 1\n\t                if qa['id'] not in predictions_mpmr :\n\t                    message = 'Unanswered question ' + qa['id'] + \\\n\t                              ' will receive score 0.'\n\t                    print(message, file=sys.stderr)\n\t                    continue\n\t                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n", "                prediction_mpmr = predictions_mpmr[qa['id']]\n\t                # prediction_xlmr = predictions_xlmr[qa['id']]\n\t                exact_match_mpmr_one = metric_max_over_ground_truths(\n\t                    exact_match_score, prediction_mpmr, ground_truths, lang)\n\t                exact_match_mpmr += exact_match_mpmr_one\n\t                # exact_match_xlmr_one = metric_max_over_ground_truths(\n\t                #     exact_match_score, prediction_xlmr, ground_truths, lang)\n\t                # exact_match_xlmr += exact_match_xlmr_one\n\t                f1 += metric_max_over_ground_truths(\n\t                    f1_score, prediction_mpmr, ground_truths, lang)\n", "                print('id', i_a)\n\t                print(qa['id'])\n\t                print('question', qa['question'])\n\t                print('mpmr answer:', prediction_mpmr, normalize_answer(prediction_mpmr, lang))\n\t                print('ground truth:', ground_truths, [normalize_answer(x, lang) for x in ground_truths])\n\t                print()\n\t                # if \"（\" in ground_truths[0] or  \"）\" in ground_truths[0]:\n\t    exact_match = 100.0 * exact_match_mpmr / total\n\t    f1 = 100.0 * f1 / total\n\t    print({'exact_match': exact_match, 'f1': f1})\n", "    return {'exact_match': exact_match, 'f1': f1}\n\tif __name__ == '__main__':\n\t    expected_version = '1.0'\n\t    parser = argparse.ArgumentParser(\n\t        description='Evaluation for MLQA ' + expected_version)\n\t    parser.add_argument('dataset_file', help='Dataset file')\n\t    parser.add_argument('prediction_file', help='Prediction File')\n\t    parser.add_argument('answer_language', help='Language code of answer language')\n\t    args = parser.parse_args()\n\t    with open(args.dataset_file) as dataset_file:\n", "        dataset_json = json.load(dataset_file)\n\t        dataset = dataset_json['data']\n\t    with open(args.prediction_file) as prediction_file:\n\t        predictions_mpmr = json.load(prediction_file)\n\t    # with open(args.prediction_file+'.json') as prediction_file:\n\t    #     predictions_xlmr = json.load(prediction_file)\n\t    compare(dataset, predictions_mpmr, args.answer_language)"]}
{"filename": "mQA/mrqa_official_eval.py", "chunked_list": ["\"\"\"Official evaluation script for the MRQA Workshop Shared Task.\n\tAdapted fromt the SQuAD v1.1 official evaluation script.\n\tUsage:\n\t    python official_eval.py dataset_file.jsonl.gz prediction_file.json\n\t\"\"\"\n\tfrom __future__ import absolute_import\n\tfrom __future__ import division\n\tfrom __future__ import print_function\n\timport argparse\n\timport string\n", "import re\n\timport json\n\timport gzip\n\timport sys\n\tfrom collections import Counter\n\tdef normalize_answer(s):\n\t    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\t    def remove_articles(text):\n\t        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\t    def white_space_fix(text):\n", "        return ' '.join(text.split())\n\t    def remove_punc(text):\n\t        exclude = set(string.punctuation)\n\t        return ''.join(ch for ch in text if ch not in exclude)\n\t    def lower(text):\n\t        return text.lower()\n\t    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\tdef f1_score(prediction, ground_truth):\n\t    prediction_tokens = normalize_answer(prediction).split()\n\t    ground_truth_tokens = normalize_answer(ground_truth).split()\n", "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n\t    num_same = sum(common.values())\n\t    if num_same == 0:\n\t        return 0\n\t    precision = 1.0 * num_same / len(prediction_tokens)\n\t    recall = 1.0 * num_same / len(ground_truth_tokens)\n\t    f1 = (2 * precision * recall) / (precision + recall)\n\t    return f1\n\tdef exact_match_score(prediction, ground_truth):\n\t    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n", "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n\t    scores_for_ground_truths = []\n\t    for ground_truth in ground_truths:\n\t        score = metric_fn(prediction, ground_truth)\n\t        scores_for_ground_truths.append(score)\n\t    return max(scores_for_ground_truths)\n\tdef read_predictions(prediction_file):\n\t    with open(prediction_file) as f:\n\t        predictions = json.load(f)\n\t    return predictions\n", "def read_answers(gold_file):\n\t    answers = {}\n\t    if gold_file.endswith('.gz'):\n\t        with gzip.open(gold_file, 'rb') as f:\n\t            for i, line in enumerate(f):\n\t                example = json.loads(line)\n\t                if i == 0 and 'header' in example:\n\t                    continue\n\t                for qa in example['qas']:\n\t                    answers[qa['qid']] = qa['answers']\n", "    else:\n\t        with open(gold_file, 'r') as f:\n\t            for i, line in enumerate(f):\n\t                example = json.loads(line)\n\t                if i == 0 and ('header' in example or isinstance(example, list)):\n\t                    continue\n\t                for qa in example['qas']:\n\t                    answers[qa['qid']] = qa['answers']\n\t    return answers\n\tdef evaluate(answers, predictions, skip_no_answer=False):\n", "    f1 = exact_match = total = 0\n\t    for qid, ground_truths in answers.items():\n\t        if qid not in predictions:\n\t            if not skip_no_answer:\n\t                message = 'Unanswered question %s will receive score 0.' % qid\n\t                print(message)\n\t                total += 1\n\t            continue\n\t        total += 1\n\t        prediction = predictions[qid]\n", "        exact_match += metric_max_over_ground_truths(\n\t            exact_match_score, prediction, ground_truths)\n\t        f1 += metric_max_over_ground_truths(\n\t            f1_score, prediction, ground_truths)\n\t    exact_match = 100.0 * exact_match / total\n\t    f1 = 100.0 * f1 / total\n\t    return {'exact_match': exact_match, 'f1': f1}\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser(\n\t        description='Evaluation for MRQA Workshop Shared Task')\n", "    parser.add_argument('dataset_file', type=str, help='Dataset File')\n\t    parser.add_argument('prediction_file', type=str, help='Prediction File')\n\t    parser.add_argument('--skip-no-answer', action='store_true')\n\t    args = parser.parse_args()\n\t    # answers = read_answers(cached_path(args.dataset_file))\n\t    # predictions = read_predictions(cached_path(args.prediction_file))\n\t    # metrics = evaluate(answers, predictions, args.skip_no_answer)\n\t    #\n\t    # print(json.dumps(metrics))\n"]}
{"filename": "mQA/train-QA.py", "chunked_list": ["# coding=utf-8\n\t# Copyright 2018 The Google AI Language Team Authors, The HuggingFace Inc. team and Alibaba DAMO Academy.\n\t# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n", "# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t\"\"\" Finetuning the library models for question-answering on SQuAD (DistilBERT, Bert, XLM, XLNet).\"\"\"\n\t\"\"\"Adapted from the HuggingFace SQuAD finetuning script for EQA.\"\"\"\n\timport argparse\n\tfrom decimal import Decimal\n\timport logging\n", "from typing import Dict\n\timport os\n\timport random\n\timport json\n\timport transformers\n\tfrom transformers import (\n\t    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n\t    AutoTokenizer,\n\t    AutoModelForCausalLM,\n\t)\n", "from utils import collate_to_max_length_s2s, collate_to_max_length_bloom, collate_to_max_length_llama\n\tfrom data_clm import cache_mQAexamples as cache_mQAexamples_clm\n\tfrom transformers.trainer_utils import is_main_process\n\timport numpy as np\n\timport torch\n\tfrom torch.utils.data import DataLoader, SequentialSampler\n\tfrom tqdm import tqdm\n\tfrom squad_evaluation_v1 import evaluate as evaluateSQuAD\n\tfrom mlqa_evaluation_v1 import evaluate as evaluateMLQA\n\ttry:\n", "    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n\texcept:\n\t    print('not applicable for flash attention')\n\tIGNORE_INDEX = -100\n\tDEFAULT_PAD_TOKEN = \"[PAD]\"\n\tDEFAULT_EOS_TOKEN = \"</s>\"\n\tDEFAULT_BOS_TOKEN = \"</s>\"\n\tDEFAULT_UNK_TOKEN = \"[UNK]\"\n\tlogger = logging.getLogger(__name__)\n\tMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n", "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\tdef smart_tokenizer_and_embedding_resize(\n\t    special_tokens_dict: Dict,\n\t    tokenizer: transformers.PreTrainedTokenizer,\n\t    model: transformers.PreTrainedModel,\n\t):\n\t    \"\"\"Resize tokenizer and embedding.\n\t    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n\t    \"\"\"\n\t    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n", "    model.resize_token_embeddings(len(tokenizer))\n\t    if num_new_tokens > 0:\n\t        input_embeddings = model.get_input_embeddings().weight.data\n\t        output_embeddings = model.get_output_embeddings().weight.data\n\t        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n\t            dim=0, keepdim=True)\n\t        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n\t            dim=0, keepdim=True)\n\t        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n\t        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n", "def set_seed(args):\n\t    random.seed(args.seed)\n\t    np.random.seed(args.seed)\n\t    torch.manual_seed(args.seed)\n\t    if args.n_gpu > 0:\n\t        torch.cuda.manual_seed_all(args.seed)\n\tdef evaluate(args, data_path, model, tokenizer, prefix=\"\", split=\"dev\", lang='en'):\n\t    if data_path.startswith(\"SQuAD\"):\n\t        data_path = \"SQuAD\"\n\t    dataset = load_and_cache_examples(args, tokenizer, evaluate=True, data_path=data_path, split=split, lang=lang)\n", "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n\t        os.makedirs(args.output_dir)\n\t    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n\t    # Note that DistributedSampler samples randomly\n\t    eval_sampler = SequentialSampler(dataset)\n\t    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate_to_max_length_llama)\n\t    # multi-gpu evaluate\n\t    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n\t        model = torch.nn.DataParallel(model)\n\t    # Eval!\n", "    logger.warning(\"***** Running evaluation {} *****\".format(prefix))\n\t    logger.warning(\"  Num examples = %d\", len(dataset))\n\t    logger.warning(\"  Batch size = %d\", args.eval_batch_size)\n\t    all_results = []\n\t    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n\t        model.eval()\n\t        batch = tuple(t.to(args.device) for t in batch)\n\t        with torch.no_grad():\n\t            inputs = {\n\t                \"input_ids\": batch[0],\n", "                \"attention_mask\": batch[1],\n\t            }\n\t            outputs = model.generate(**inputs,\n\t                                     max_new_tokens=64,\n\t                                     eos_token_id=13,\n\t                                     use_cache=False,\n\t                                     )\n\t        dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n\t        input_text = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[0]]\n\t        dec = [dec[i].replace(input_text[i], \"\").strip() for i in range(len(dec))]\n", "        dec = [dec[i].strip() for i in range(len(dec))]\n\t        all_results.extend(dec)\n\t    predictions = {}\n\t    for i_i, item in enumerate(all_results):\n\t        feature = dataset.all_data[i_i]\n\t        qid = feature[0]\n\t        item = item\n\t        if \"None\" in item:\n\t            span = \"\"\n\t        else:\n", "            span = item.strip()\n\t        predictions[qid] = (span,)\n\t    predictions = {x: predictions[x][0] for x in predictions}\n\t    preds_file = os.path.join(args.output_dir, args.data_path + '-predictions.json')\n\t    with open(preds_file, 'w') as writer:\n\t        json.dump(predictions, writer)\n\t    if data_path == \"SQuAD\":\n\t        with open(os.path.join(\"Data\", \"squad\", \"dev-v1.1.json\")) as f:\n\t            gold_answers = json.load(f)['data']\n\t        scores = evaluateSQuAD(gold_answers, predictions)\n", "        logger.warning(\n\t            f\"EVAL INFO {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n\t    elif data_path == \"XQuAD\":\n\t        with open(os.path.join(\"Data\", \"xquad\", \"xquad.\" + lang + \".json\")) as f:\n\t            gold_answers = json.load(f)['data']\n\t        scores = evaluateSQuAD(gold_answers, predictions)\n\t        logger.warning(\n\t            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n\t    elif data_path == \"MLQA\":\n\t        with open(os.path.join(\"Data\", \"mlqa/test\", \"test-context-\" + lang + \"-question-\" + lang + \".json\")) as f:\n", "            gold_answers = json.load(f)['data']\n\t        scores = evaluateMLQA(gold_answers, predictions, lang=lang)\n\t        logger.warning(\n\t            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n\t    elif data_path == \"TyDiQA\":\n\t        with open(os.path.join(\"Data\", \"tydiqa/tydiqa-goldp-v1.1-dev\", \"tydiqa.goldp.\" + lang + \".dev.json\")) as f:\n\t            gold_answers = json.load(f)['data']\n\t        scores = evaluateSQuAD(gold_answers, predictions)\n\t        logger.warning(\n\t            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n", "    else:\n\t        raise NotImplementedError\n\t    return scores, predictions\n\tdef load_and_cache_examples(args, tokenizer, evaluate=False, data_path=None, split=\"train\", lang='en'):\n\t    if args.local_rank not in [-1, 0] and not evaluate:\n\t        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\t        torch.distributed.barrier()\n\t    if data_path is None:\n\t        data_path = args.data_path\n\t    features = cache_mQAexamples_clm(args, tokenizer, data_path, split, lang=lang)\n", "    logger.warning(\"load data from {}.\".format(data_path))\n\t    dataset = QADataset(features=features,\n\t                        data_name=data_path,\n\t                        tokenizer=tokenizer,\n\t                        )\n\t    if args.local_rank == 0 and not evaluate:\n\t        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\t        torch.distributed.barrier()\n\t    return dataset\n\tdef main():\n", "    parser = argparse.ArgumentParser()\n\t    # Required parameters\n\t    parser.add_argument(\n\t        \"--model_type\",\n\t        default=None,\n\t        type=str,\n\t        required=True,\n\t        help=\"Model type selected in the list: \" + \", \".join(MODEL_TYPES),\n\t    )\n\t    parser.add_argument(\n", "        \"--model_name_or_path\",\n\t        default=None,\n\t        type=str,\n\t        required=True,\n\t        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n\t    )\n\t    parser.add_argument(\n\t        \"--output_dir\",\n\t        default=None,\n\t        type=str,\n", "        required=True,\n\t        help=\"The output directory where the model checkpoints and predictions will be written.\",\n\t    )\n\t    # Other parameters\n\t    parser.add_argument(\n\t        \"--data_dir\",\n\t        default=None,\n\t        type=str,\n\t        help=\"The input data dir. Should contain the .json files for the task.\"\n\t        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n", "    )\n\t    parser.add_argument(\n\t        \"--data_path\",\n\t        # choices=[\"out_dev\", \"SQuAD\", \"NewsQA\", \"TriviaQA\", 'SearchQA', 'HotpotQA', \"NaturalQuestions\", \"SQuAD2\"],\n\t        default=\"SQuAD\",\n\t        type=str,\n\t        help=\"The input file.\",\n\t    )\n\t    parser.add_argument(\n\t        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n", "    )\n\t    parser.add_argument(\n\t        \"--tokenizer_name\",\n\t        default=\"\",\n\t        type=str,\n\t        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n\t    )\n\t    parser.add_argument(\n\t        \"--cache_dir\",\n\t        default=\"\",\n", "        type=str,\n\t        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n\t    )\n\t    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n\t    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n\t    parser.add_argument(\n\t        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n\t    )\n\t    parser.add_argument(\n\t        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n", "    )\n\t    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n\t    parser.add_argument(\n\t        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n\t    )\n\t    parser.add_argument(\n\t        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n\t    )\n\t    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n\t    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n", "    parser.add_argument(\n\t        \"--fp16\",\n\t        action=\"store_true\",\n\t        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n\t    )\n\t    parser.add_argument(\n\t        \"--fp16_opt_level\",\n\t        type=str,\n\t        default=\"O1\",\n\t        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n", "        \"See details at https://nvidia.github.io/apex/amp.html\",\n\t    )\n\t    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n\t    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n\t    parser.add_argument(\"--keep_frac\", type=float, default=1.0, help=\"The fraction of the balanced dataset to keep.\")\n\t    parser.add_argument(\n\t        \"--flash_attn\", action=\"store_true\", help=\"use flash attn\"\n\t    )\n\t    args = parser.parse_args()\n\t    if (\n", "        os.path.exists(args.output_dir)\n\t        and os.listdir(args.output_dir)\n\t        and args.do_train\n\t        and not args.overwrite_output_dir\n\t    ):\n\t        raise ValueError(\n\t            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n\t                args.output_dir\n\t            )\n\t        )\n", "    # Setup distant debugging if needed\n\t    if args.server_ip and args.server_port:\n\t        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n\t        import ptvsd\n\t        print(\"Waiting for debugger attach\")\n\t        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n\t        ptvsd.wait_for_attach()\n\t    # Setup CUDA, GPU & distributed training\n\t    if args.local_rank == -1 or args.no_cuda:\n\t        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n", "        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n\t    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n\t        torch.cuda.set_device(args.local_rank)\n\t        device = torch.device(\"cuda\", args.local_rank)\n\t        torch.distributed.init_process_group(backend=\"nccl\")\n\t        args.n_gpu = 1\n\t    args.device = device\n\t    # Setup logging\n\t    logging.basicConfig(\n\t        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n", "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n\t        level=  logging.WARN,\n\t    )\n\t    logger.warning(\n\t        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n\t        args.local_rank,\n\t        device,\n\t        args.n_gpu,\n\t        bool(args.local_rank != -1),\n\t        args.fp16,\n", "    )\n\t    # Set the verbosity to info of the Transformers logger (on main process only):\n\t    if is_main_process(args.local_rank):\n\t        transformers.utils.logging.set_verbosity_info()\n\t        transformers.utils.logging.enable_default_handler()\n\t        transformers.utils.logging.enable_explicit_format()\n\t    # Set seed\n\t    set_seed(args)\n\t    # Load pretrained model and tokenizer\n\t    if args.local_rank not in [-1, 0]:\n", "        # Make sure only the first process in distributed training will download model & vocab\n\t        torch.distributed.barrier()\n\t    if args.flash_attn:\n\t        replace_llama_attn_with_flash_attn()\n\t    args.model_type = args.model_type.lower()\n\t    kwargs = {\"torch_dtype\": torch.float16}\n\t    model = transformers.LlamaForCausalLM.from_pretrained(\n\t        args.model_name_or_path,\n\t        cache_dir=args.cache_dir,\n\t        low_cpu_mem_usage=True, **kwargs,\n", "    )\n\t    tokenizer = transformers.AutoTokenizer.from_pretrained(\n\t        args.model_name_or_path,\n\t        cache_dir=args.cache_dir,\n\t        padding_side=\"right\",\n\t        use_fast=False,\n\t    )\n\t    if tokenizer.pad_token is None:\n\t        smart_tokenizer_and_embedding_resize(\n\t            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n", "            tokenizer=tokenizer,\n\t            model=model,\n\t        )\n\t    if \"llama\" in args.model_name_or_path:\n\t        tokenizer.add_special_tokens({\n\t            \"eos_token\": DEFAULT_EOS_TOKEN,\n\t            \"bos_token\": DEFAULT_BOS_TOKEN,\n\t            \"unk_token\": DEFAULT_UNK_TOKEN,\n\t        })\n\t    # model = model_PMR[args.model_type](config)\n", "    if args.local_rank == 0:\n\t        # Make sure only the first process in distributed training will download model & vocab\n\t        torch.distributed.barrier()\n\t    model.to(args.device)\n\t    logger.info(\"Training/evaluation parameters %s\", args)\n\t    # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n\t    # Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n\t    # remove the need for this code, but it is still valid.\n\t    if args.fp16:\n\t        try:\n", "            import apex\n\t            apex.amp.register_half_function(torch, \"einsum\")\n\t        except ImportError:\n\t            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n\t    # Set the verbosity to info of the Transformers logger (on main process only):\n\t    if is_main_process(args.local_rank):\n\t        transformers.utils.logging.set_verbosity_warning()\n\t    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n\t    if args.do_eval and args.local_rank in [-1, 0]:\n\t        if args.do_train:\n", "            pass\n\t        else:\n\t            global_step = \"\"\n\t            if args.data_path == 'XQuAD':\n\t                results, predictions = evaluate(args, 'SQuAD', model, tokenizer, split=\"dev\", prefix='')\n\t                all_test = {}\n\t                for lang in ['en', 'ar', 'de', 'el', 'es', 'hi', 'ru', 'th', 'tr', 'vi', 'zh', 'ro']:\n\t                    test_results, predictions = evaluate(args, \"XQuAD\", model, tokenizer, prefix='', split=\"dev\", lang=lang)\n\t                    all_test[\"XQuAD-\" + lang] = (test_results['f1'], test_results['exact_match'])\n\t                avg_f1 = avg_em = 0\n", "                for lang in ['en', 'ar', 'de', 'el', 'es', 'hi', 'ru', 'th', 'tr', 'vi', 'zh', ]:\n\t                    avg_f1 += all_test[\"XQuAD-\" + lang][0]\n\t                    avg_em += all_test[\"XQuAD-\" + lang][1]\n\t                all_test[\"XQuAD-avg\"] = (avg_f1 / 11, avg_em / 11)\n\t                logger.warning(\n\t                    f\"EVAL INFO mix of XQuAD -> valid_f1 is: {all_test['XQuAD-avg'][0]}; exact match is: {all_test['XQuAD-avg'][1]}.\")\n\t            elif args.data_path == 'MLQA':\n\t                # results, predictions = evaluate(args, 'SQuAD', model, tokenizer, split=\"dev\", prefix='')\n\t                all_test = {}\n\t                for lang in ['en', 'es', 'de', 'ar', 'hi', 'vi', 'zh']: # 'en', 'es', 'de', 'ar',\n", "                    test_results, predictions = evaluate(args, \"MLQA\", model, tokenizer, prefix='', split=\"dev\", lang=lang)\n\t                    all_test[\"MLQA-\" + lang] = (test_results['f1'], test_results['exact_match'])\n\t                avg_f1 = avg_em = 0\n\t                for lang in ['en', 'es', 'de', 'ar', 'hi', 'vi', 'zh']:\n\t                    avg_f1 += all_test[\"MLQA-\" + lang][0]\n\t                    avg_em += all_test[\"MLQA-\" + lang][1]\n\t                all_test[\"MLQA-avg\"] = (avg_f1 / 7, avg_em / 7)\n\t                logger.warning(\n\t                    f\"EVAL INFO mix of MLQA -> valid_f1 is: {all_test['MLQA-avg'][0]}; exact match is: {all_test['MLQA-avg'][1]}.\")\n\t            elif args.data_path == \"TyDiQA\":\n", "                # results, _ = evaluate(args, args.data_path, model, tokenizer, prefix=global_step, split=\"dev\",)\n\t                all_test = {}\n\t                for lang in ['en', 'ar', 'bn', 'fi', 'id', 'ko', 'ru', 'sw', 'te']: #'en', 'ar', 'bn', 'fi', 'id', 'ko', 'ru', 'sw',\n\t                    test_results, predictions = evaluate(args, \"TyDiQA\", model, tokenizer, prefix=global_step, split=\"dev\",\n\t                                                         lang=lang)\n\t                    all_test[\"TyDiQA-\" + lang] = (test_results['f1'], test_results['exact_match'])\n\t                avg_f1 = avg_em = 0\n\t                for lang in ['en', 'ar', 'bn', 'fi', 'id', 'ko', 'ru', 'sw', 'te']:\n\t                    avg_f1 += all_test[\"TyDiQA-\" + lang][0]\n\t                    avg_em += all_test[\"TyDiQA-\" + lang][1]\n", "                all_test[\"TyDiQA-avg\"] = (avg_f1 / 9, avg_em / 9)\n\t                logger.warning(\n\t                    f\"EVAL INFO mix of TyDiQA -> valid_f1 is: {all_test['TyDiQA-avg'][0]}; exact match is: {all_test['TyDiQA-avg'][1]}.\")\n\t            else:\n\t                raise NotImplementedError\n\t            save_line = [(Decimal(all_test[x][0]).quantize(Decimal(\"0.01\"), rounding=\"ROUND_HALF_UP\"),\n\t                          Decimal(all_test[x][1]).quantize(Decimal(\"0.01\"), rounding=\"ROUND_HALF_UP\")) for x in\n\t                         all_test]\n\t            save_line = [str(x[0]) + '/' + str(x[1]) for x in save_line]\n\t            save_line = '\\t'.join([args.data_path] + save_line)\n", "            with open('temp.txt', 'a') as writer2:\n\t                if args.data_path != 'XQuAD':\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, save_line])\n\t                    writer2.write(save_string + '\\n')\n\t                else:\n\t                    save_string = \"\\t\".join(\n\t                        [args.model_name_or_path, str(results['f1']),\n\t                         str(results['exact_match']), save_line])\n\t                    writer2.write(save_string + '\\n')\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
