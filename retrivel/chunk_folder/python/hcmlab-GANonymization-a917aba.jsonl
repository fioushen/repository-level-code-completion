{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\twith open('requirements.txt') as f:\n\t    required = f.read().splitlines()\n\tsetup(\n\t    name=\"GANonymization\",\n\t    version=\"1.0.0\",\n\t    description=\"GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions\",\n\t    author=\"Fabio Hellmann, Silvan Mertes, Mohamed Benouis, Alexander Hustinx, Tzung-Chien Hsieh, Cristina Conati, Peter Krawitz, Elisabeth AndrÃ©\",\n\t    author_email=\"fabio.hellmann@informatik.uni-augsburg.de\",\n\t    url=\"https://github.com/hcmlab/GANonymization\",\n", "    packages=find_packages(),\n\t    install_requires=required,\n\t)\n"]}
{"filename": "main.py", "chunked_list": ["import os.path\n\timport pathlib\n\timport shutil\n\timport cv2\n\timport fire\n\timport pandas as pd\n\timport pytorch_lightning\n\timport torchvision.transforms\n\tfrom loguru import logger\n\tfrom sklearn.model_selection import train_test_split\n", "from torchvision.transforms import RandomHorizontalFlip, Compose\n\tfrom torchvision.utils import save_image\n\tfrom lib.datasets import DatasetSplit\n\tfrom lib.datasets.label_extractor import extract_labels\n\tfrom lib.datasets.labeled_dataset import LabeledDataset\n\tfrom lib.evaluator import eval_classifier\n\tfrom lib.models.generic_classifier import GenericClassifier\n\tfrom lib.models.pix2pix import Pix2Pix\n\tfrom lib.trainer import setup_torch_device, setup\n\tfrom lib.transform import transform, FacialLandmarks478, FaceCrop, FaceSegmentation, ZeroPaddingResize, \\\n", "    Pix2PixTransformer\n\tfrom lib.utils import glob_dir, get_last_ckpt, move_files\n\tSEED = 42\n\tdef preprocess(input_path: str, img_size: int = 512, align: bool = True, test_size: float = 0.1, shuffle: bool = True,\n\t               output_dir: str = None, num_workers: int = 8):\n\t    \"\"\"\n\t    Run all pre-processing steps (split, crop, segmentation, landmark) at once.\n\t    @param input_path: The path to a directory to be processed.\n\t    @param img_size: The size of the image after the processing step.\n\t    @param align: True if the images should be aligned centered after padding.\n", "    @param test_size: The size of the test split. (Default: 0.1)\n\t    @param shuffle: Shuffle the split randomly.\n\t    @param output_dir: The output directory.\n\t    @param num_workers: The number of cpu-workers.\n\t    \"\"\"\n\t    logger.info(f\"Parameters: {', '.join([f'{key}: {value}' for key, value in locals().items()])}\")\n\t    if output_dir is None:\n\t        output_dir = input_path\n\t    output_dir = os.path.join(output_dir, 'original')\n\t    # Split dataset\n", "    split_file = extract_labels(input_path, output_dir)\n\t    if split_file:\n\t        df = pd.read_csv(split_file)\n\t        for split, folder in enumerate(['train', 'val', 'test']):\n\t            move_files([os.path.join(input_path, img_path.replace('\\\\', os.sep)) for img_path in\n\t                        df[df['split'] == split]['image_path'].values], os.path.join(output_dir, folder))\n\t        shutil.copyfile(split_file, os.path.join(output_dir, pathlib.Path(split_file).name))\n\t    else:\n\t        files = glob_dir(os.path.join(input_path))\n\t        logger.debug(f'Found {len(files)} images')\n", "        train_files, test_files = train_test_split(files, test_size=test_size, shuffle=shuffle)\n\t        move_files(train_files, os.path.join(output_dir, 'train'))\n\t        move_files(test_files, os.path.join(output_dir, 'val'))\n\t    # Apply Transformers\n\t    output_dir = transform(output_dir, img_size, False, FaceCrop(align, True), num_workers=num_workers)\n\t    output_dir = transform(output_dir, img_size, False, FaceSegmentation(), num_workers=num_workers)\n\t    output_dir = transform(output_dir, img_size, True, FacialLandmarks478(), num_workers=num_workers)\n\tdef train_pix2pix(data_dir: str, log_dir: str, models_dir: str, output_dir: str, dataset_name: str, epoch: int = 0,\n\t                  n_epochs: int = 200, batch_size: int = 1, lr: float = 0.0002, b1: float = 0.5, b2: float = 0.999,\n\t                  n_cpu: int = 2, img_size: int = 256, checkpoint_interval: int = 500, device: int = 0):\n", "    \"\"\"\n\t    Train the pix2pix GAN for generating faces based on given landmarks.\n\t    @param data_dir: The root path to the data folder.\n\t    @param log_dir: The log folder path.\n\t    @param models_dir: The path to the models.\n\t    @param output_dir: The output directory.\n\t    @param dataset_name: name of the dataset.\n\t    @param epoch: epoch to start training from.\n\t    @param n_epochs: number of epochs of training.\n\t    @param batch_size: size of the batches.\n", "    @param lr: adam: learning rate.\n\t    @param b1: adam: decay of first order momentum of gradient.\n\t    @param b2: adam: decay of first order momentum of gradient.\n\t    @param n_cpu: number of cpu threads to use during batch generation.\n\t    @param img_size: size of image.\n\t    @param checkpoint_interval: interval between model checkpoints.\n\t    @param device: The device to run the task on (e.g., device >= 0: cuda; device=-1: cpu).\n\t    \"\"\"\n\t    logger.info(f\"Parameters: {', '.join([f'{key}: {value}' for key, value in locals().items()])}\")\n\t    setup_torch_device(device, SEED)\n", "    ckpt_file = get_last_ckpt(models_dir)\n\t    resume_ckpt = None\n\t    if ckpt_file is not None:\n\t        resume_ckpt = os.path.join(models_dir, ckpt_file)\n\t        model = Pix2Pix.load_from_checkpoint(resume_ckpt)\n\t    else:\n\t        model = Pix2Pix(data_dir, models_dir, output_dir, n_epochs, dataset_name, batch_size, lr, b1,\n\t                        b2, n_cpu, img_size, device)\n\t    trainer = setup(model, log_dir, models_dir, n_epochs, device, checkpoint_interval=checkpoint_interval)\n\t    trainer.fit(model, ckpt_path=resume_ckpt)\n", "def train_classifier(data_dir: str, num_classes: int, learning_rate: float = 0.0003, batch_size: int = 128,\n\t                     n_epochs: int = 100, device: int = 0, output_dir: str = 'output', monitor: str = 'val_loss',\n\t                     metric_mode: str = 'min', save_top_k: int = 1, early_stop_n: int = 5, num_workers: int = 8):\n\t    \"\"\"\n\t    Run the training.\n\t    @param data_dir:\n\t    @param num_classes:\n\t    @param learning_rate: The learning rate.\n\t    @param batch_size: The batch size.\n\t    @param n_epochs: The number of epochs to train.\n", "    @param device: The device to work on.\n\t    @param output_dir: The path to the output directory.\n\t    @param monitor: The metric variable to monitor.\n\t    @param metric_mode: The mode of the metric to decide which checkpoint to choose (min or max).\n\t    @param save_top_k: Save checkpoints every k epochs - or every epoch if k=0.\n\t    @param early_stop_n: Stops training after n epochs of no improvement - default is deactivated.\n\t    @param num_workers:\n\t    \"\"\"\n\t    logger.info(f\"Parameters: {', '.join([f'{key}: {value}' for key, value in locals().items()])}\")\n\t    pytorch_lightning.seed_everything(SEED, workers=True)\n", "    train_db = LabeledDataset(data_dir, DatasetSplit.TRAIN, Compose([\n\t        GenericClassifier.weights.transforms(),\n\t        RandomHorizontalFlip(),\n\t    ]))\n\t    val_db = LabeledDataset(data_dir, DatasetSplit.VALIDATION,\n\t                            Compose([GenericClassifier.weights.transforms()]))\n\t    model = GenericClassifier(train_db=train_db, val_db=val_db, multi_label=train_db.is_multi_label,\n\t                              batch_size=batch_size, learning_rate=learning_rate, num_workers=num_workers,\n\t                              device=setup_torch_device(device, SEED), classes=train_db.classes,\n\t                              class_weights=train_db.class_weight)\n", "    models_dir = os.path.join(output_dir, 'models')\n\t    trainer = setup(model=model, log_dir=os.path.join(output_dir, 'logs'),\n\t                    models_dir=models_dir, num_epoch=n_epochs,\n\t                    device=device, monitor=monitor, metric_mode=metric_mode,\n\t                    save_top_k=save_top_k, early_stop_n=early_stop_n)\n\t    trainer.fit(model)\n\t    eval_classifier(models_dir, data_dir, batch_size, device, output_dir, num_workers)\n\tdef anonymize_image(model_file: str, input_file: str, output_file: str, img_size: int = 512, align: bool = True,\n\t                    device: int = 0):\n\t    \"\"\"\n", "    Anonymize one face in a single image.\n\t    @param model_file: The GANonymization model to be used for anonymization.\n\t    @param input_file: The input image file.\n\t    @param output_file: The output image file.\n\t    @param img_size: The size of the image for processing by the model.\n\t    @param align: Whether to align the image based on the facial orientation.\n\t    @param device: The device to run the process on.\n\t    \"\"\"\n\t    img = cv2.imread(input_file)\n\t    transform = torchvision.transforms.Compose([\n", "        FaceCrop(align, False),\n\t        ZeroPaddingResize(img_size),\n\t        FacialLandmarks478(),\n\t        Pix2PixTransformer(model_file, img_size, device)\n\t    ])\n\t    transformed_img = transform(img)\n\t    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\t    save_image(transformed_img, output_file, normalize=True)\n\tdef anonymize_directory(model_file: str, input_directory: str, output_directory: str, img_size: int = 512,\n\t                        align: bool = True, device: int = 0):\n", "    \"\"\"\n\t    Anonymize a set of images in a directory.\n\t    @param model_file: The GANonymization model to be used for anonymization.\n\t    @param input_directory: The input image directory.\n\t    @param output_directory: The output image directory.\n\t    @param img_size: The size of the image for processing by the model.\n\t    @param align: Whether to align the image based on the facial orientation.\n\t    @param device: The device to run the process on.\n\t    \"\"\"\n\t    for file in os.listdir(input_directory):\n", "        anonymize_image(model_file, file, os.path.join(output_directory, os.path.basename(file)), img_size, align,\n\t                        device)\n\tif __name__ == '__main__':\n\t    fire.Fire()\n"]}
{"filename": "lib/analysis.py", "chunked_list": ["import math\n\timport os.path\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom PIL import Image\n\tfrom deepface import DeepFace\n\tfrom loguru import logger\n\tfrom matplotlib import pyplot as plt\n\tfrom scipy.stats import shapiro, wilcoxon\n", "from statsmodels.stats.multitest import multipletests\n\tfrom torchvision.transforms import Compose, Resize, ToTensor\n\tfrom torchvision.utils import save_image\n\tfrom tqdm import tqdm\n\tfrom lib.datasets import DatasetSplit\n\tfrom lib.datasets.labeled_dataset import LabeledDataset\n\tfrom lib.models.generic_classifier import GenericClassifier\n\tfrom lib.transform import FaceSegmentation, Pix2PixTransformer\n\tdef emotion_analysis(db_path: str, result_path: str):\n\t    db_name = os.path.basename(db_path)\n", "    logger.debug(f'Analysis for {db_name}')\n\t    # Load labeled datasets\n\t    db_seg = LabeledDataset(os.path.join(db_path, 'segmentation'), DatasetSplit.VALIDATION, Compose([]))\n\t    db_gan = LabeledDataset(os.path.join(db_path, 'ganonymization'), DatasetSplit.VALIDATION, Compose([]))\n\t    db_dp2 = LabeledDataset(os.path.join(db_path, 'deepprivacy2'), DatasetSplit.VALIDATION, Compose([]))\n\t    # Load predicted labels\n\t    npz_seg = np.load(os.path.join(result_path, 'segmentation', 'result_pred_label.npz'))\n\t    npz_gan = np.load(os.path.join(result_path, 'ganonymization', 'result_pred_label.npz'))\n\t    npz_dp2 = np.load(os.path.join(result_path, 'deepprivacy2', 'result_pred_label.npz'))\n\t    # Create dataframes with predicted labels and image names\n", "    df_seg = db_seg.meta_data\n\t    df_gan = db_gan.meta_data\n\t    df_dp2 = db_dp2.meta_data\n\t    df_seg_pred = pd.DataFrame(npz_seg['pred'], columns=db_seg.classes)\n\t    df_gan_pred = pd.DataFrame(npz_gan['pred'], columns=db_seg.classes)\n\t    df_dp2_pred = pd.DataFrame(npz_dp2['pred'], columns=db_seg.classes)\n\t    df_seg_pred['image_name'] = [os.path.basename(f).split('-')[-1] for f in df_seg['image_path']]\n\t    df_gan_pred['image_name'] = [os.path.basename(f).split('-')[-1] for f in df_gan['image_path']]\n\t    df_dp2_pred['image_name'] = [os.path.basename(f).split('-')[-1] for f in df_dp2['image_path']]\n\t    # Calculate mean distance between original and synthesized images for each category\n", "    synthesized_label = [f'{c}_x' for c in db_seg.classes]\n\t    original_label = [f'{c}_y' for c in db_seg.classes]\n\t    df = pd.DataFrame(index=[c.replace('_', ' ') for c in db_seg.classes])\n\t    df_seg_gan_pred = pd.merge(df_gan_pred, df_seg_pred, on='image_name')\n\t    df_seg_dp2_pred = pd.merge(df_dp2_pred, df_seg_pred, on='image_name')\n\t    df['GANonymization'] = [(df_seg_gan_pred[x] - df_seg_gan_pred[y]).abs().mean() for x, y in zip(synthesized_label, original_label)]\n\t    df['DeepPrivacy2'] = [(df_seg_dp2_pred[x] - df_seg_dp2_pred[y]).abs().mean() for x, y in zip(synthesized_label, original_label)]\n\t    # Save results\n\t    df.to_csv(os.path.join(result_path, f'{db_name}_mean_distance.csv'))\n\t    df.to_latex(os.path.join(result_path, f'{db_name}_mean_distance.latex'))\n", "    if len(original_label) < 20:\n\t        fig_width = int(round(len(df.index) / 2))\n\t        if fig_width < 4:\n\t            fig_width = 4\n\t        df.plot.bar(figsize=(fig_width, 4))\n\t        plt.ylabel('Mean Distance')\n\t        plt.xlabel('Category')\n\t        plt.legend(loc='upper right')\n\t        plt.tight_layout()\n\t        plt.savefig(os.path.join(result_path, f'{db_name}_mean_distance.png'))\n", "        plt.close()\n\t    else:\n\t        df_split = np.array_split(df, int(round(len(original_label) / 20)))\n\t        for idx, df_tmp in enumerate(df_split):\n\t            fig_width = int(round(len(df_tmp.index) / 2))\n\t            if fig_width < 4:\n\t                fig_width = 4\n\t            df_tmp.plot.bar(figsize=(fig_width, 4))\n\t            plt.ylabel('Mean Distance')\n\t            plt.xlabel('Category')\n", "            plt.legend(loc='upper center')\n\t            plt.tight_layout()\n\t            plt.savefig(os.path.join(result_path, f'{db_name}_{idx}_mean_distance.png'))\n\t            plt.close()\n\t    # T-Test\n\t    df_seg_gan_pred = pd.merge(df_gan_pred, df_seg_pred, left_on='image_name', right_on='image_name')\n\t    ganonymization = np.asarray(\n\t        [(df_seg_gan_pred[x] - df_seg_gan_pred[y]).abs().to_numpy() for x, y in zip(synthesized_label, original_label)])\n\t    df_seg_dp2_pred = pd.merge(df_dp2_pred, df_seg_pred, left_on='image_name', right_on='image_name')[\n\t        df_dp2_pred['image_name'].isin(df_gan_pred['image_name'])]\n", "    deepprivacy2 = np.asarray(\n\t        [(df_seg_dp2_pred[x] - df_seg_dp2_pred[y]).abs().to_numpy() for x, y in zip(synthesized_label, original_label)])\n\t    ganonymization = ganonymization.transpose()\n\t    deepprivacy2 = deepprivacy2.transpose()\n\t    results = {}\n\t    for col_idx in range(len(ganonymization[0])):\n\t        _, pvalue = shapiro(np.concatenate([ganonymization[:, col_idx], deepprivacy2[:, col_idx]]))\n\t        logger.debug(f'Shapiro: {pvalue}')\n\t        result = wilcoxon(ganonymization[:, col_idx], deepprivacy2[:, col_idx], method='approx')\n\t        n = len(ganonymization[:, col_idx])\n", "        r = result.zstatistic / math.sqrt(n)\n\t        logger.info(\n\t            f'{wilcoxon.__name__}-{original_label[col_idx]}: p-value={result.pvalue}, statistic={result.statistic}, zstatistic={result.zstatistic}, n={n}, r={r}')\n\t        results.setdefault('pvalue', []).append(result.pvalue)\n\t        # results.setdefault('statistic', []).append(result.statistic)\n\t        results.setdefault('zstatistic', []).append(result.zstatistic)\n\t        results.setdefault('r', []).append(r)\n\t        results.setdefault('n', []).append(n)\n\t    reject, pvals_corrected, _, _ = multipletests(np.asarray(results['pvalue']), method='bonferroni')\n\t    logger.info(f'\\nP-Values (corrected): {pvals_corrected}')\n", "    df = pd.DataFrame(results,\n\t                      index=[c.replace('_', ' ') for c in db_seg.classes])\n\t    df.rename({'pvalue': 'P-Value', 'zstatistic': 'Z-Statistic', 'n': 'N'}, axis=1, inplace=True)\n\t    df.to_csv(os.path.join(result_path, f'statistic_pvalue.csv'))\n\t    df.to_latex(os.path.join(result_path, f'statistic_pvalue.latex'))\n\tdef face_comparison_analysis(output_dir: str, img_orig_path: str, *img_paths, img_size: int = 512):\n\t    def original_filename(file):\n\t        return file.split('-')[-1]\n\t    os.makedirs(output_dir, exist_ok=True)\n\t    sub_dir = 'val'\n", "    img_dirs = {\n\t        os.path.basename(path): [os.path.join(path, sub_dir, f) for f in os.listdir(os.path.join(path, sub_dir))]\n\t        for path in img_paths}\n\t    img_dirs_short = {method: [original_filename(path) for path in paths] for method, paths in img_dirs.items()}\n\t    comp_result = []\n\t    transformers = Compose([Resize(img_size), ToTensor()])\n\t    for orig_img in tqdm(os.listdir(os.path.join(img_orig_path, sub_dir)), desc='Comparison'):\n\t        images = {'original': os.path.join(img_orig_path, sub_dir, orig_img)}\n\t        for method, paths in img_dirs.items():\n\t            if original_filename(orig_img) in img_dirs_short[method]:\n", "                images[method] = img_dirs[method][\n\t                    [original_filename(p) for p in paths].index(original_filename(orig_img))]\n\t        if len(images) < len(img_paths) + 1:\n\t            continue\n\t        for idx, items in enumerate(images.items()):\n\t            db_name, img_path = items\n\t            # Predict similarity of faces\n\t            verify_result = {'distance': 0, 'threshold': 0}\n\t            if idx > 0:\n\t                tmp_result = DeepFace.verify(images['original'], img_path, model_name=\"VGG-Face\",\n", "                                             detector_backend='skip', enforce_detection=False, align=False)\n\t                if len(verify_result) > 0:\n\t                    verify_result = tmp_result\n\t                comp_result.append([db_name, original_filename(img_path), verify_result['distance'],\n\t                                    verify_result['threshold']])\n\t        img_sample = torch.cat(\n\t            [transformers(Image.open(images['original'])), transformers(Image.open(images['deepprivacy2'])),\n\t             transformers(Image.open(images['ganonymization']))], -2)\n\t        save_image(img_sample, os.path.join(output_dir, original_filename(orig_img)), normalize=True)\n\t    df = pd.DataFrame(comp_result, columns=['dataset', 'filename', 'distance', 'threshold'])\n", "    df.to_csv(os.path.join(output_dir, 'prediction_result.csv'))\n\t    df_distance_mean = df.groupby(['dataset'])['distance'].mean()\n\t    threshold_mean = df.groupby(['dataset'])['threshold'].mean()['ganonymization']\n\t    df_distance_mean.to_csv(os.path.join(output_dir, 'mean_prediction_result.csv'))\n\t    df_distance_mean.transpose().plot.bar(rot=0)\n\t    plt.axhline(y=threshold_mean, ls='dashed', color='b')\n\t    plt.ylabel('Mean Cosine Distance')\n\t    plt.xlabel('Anonymization Method')\n\t    plt.savefig(os.path.join(output_dir, 'mean_distance_per_method.png'))\n\t    plt.close()\n", "    logger.debug(f'Analyze comparison for: {output_dir}')\n\t    df = pd.read_csv(os.path.join(output_dir, 'prediction_result.csv'))\n\t    df_gan = df[df['dataset'] == 'ganonymization'].reset_index()\n\t    df_dp2 = df[df['dataset'] == 'deepprivacy2'].reset_index()\n\t    df_gan_nan = df_gan[df_gan['distance'].isna()]['filename'].index\n\t    df_gan.drop(df_gan_nan, inplace=True)\n\t    df_dp2.drop(df_gan_nan, inplace=True)\n\t    df_gan = df_gan['distance'].to_numpy()\n\t    df_dp2 = df_dp2['distance'].to_numpy()\n\t    _, pvalue = shapiro(np.concatenate([df_gan, df_dp2]))\n", "    logger.debug(f'Shapiro: {pvalue}')\n\t    result = wilcoxon(df_gan, df_dp2, method='approx')\n\t    n = len(df_gan)\n\t    r = result.zstatistic / math.sqrt(n)\n\t    logger.info(\n\t        f'{wilcoxon.__name__}: p-value={result.pvalue}, statistic={result.statistic}, zstatistic={result.zstatistic}, n={n}, r={r}')\n\t    reject, pvals_corrected, _, _ = multipletests(result.pvalue, method='bonferroni')\n\t    logger.info(\n\t        f'{wilcoxon.__name__}: p-value={result.pvalue}, statistic={result.statistic}, zstatistic={result.zstatistic}, n={n}, r={r}')\n\t    df = pd.DataFrame({'P-Value': pvals_corrected, 'Z-Statistic': result.zstatistic, 'r': r, 'N': n})\n", "    df.to_csv(os.path.join(output_dir, f'statistic_pvalue.csv'))\n\t    df.to_latex(os.path.join(output_dir, f'statistic_pvalue.latex'))\n\tdef facial_traits_analysis(data_dir: str, model_file: str, output_dir: str):\n\t    db_seg = LabeledDataset(os.path.join(data_dir, FaceSegmentation.__class__.__name__), DatasetSplit.VALIDATION,\n\t                            Compose([GenericClassifier.weights.transforms()]))\n\t    db_seg.meta_data['image_name'] = db_seg.meta_data['image_path'].apply(\n\t        lambda f: os.path.basename(f).split('-')[-1]).values.tolist()\n\t    db_gan = LabeledDataset(os.path.join(data_dir, Pix2PixTransformer.__class__.__name__), DatasetSplit.VALIDATION,\n\t                            Compose([GenericClassifier.weights.transforms()]))\n\t    db_gan.meta_data['image_name'] = db_gan.meta_data['image_path'].apply(\n", "        lambda f: os.path.basename(f).split('-')[-1]).values.tolist()\n\t    db_seg.meta_data.drop(db_seg.meta_data[~db_seg.meta_data['image_name'].isin(db_gan.meta_data['image_name'])].index,\n\t                          inplace=True)\n\t    db_seg.meta_data.reset_index(inplace=True)\n\t    model = GenericClassifier.load_from_checkpoint(model_file, classes=db_seg.classes)\n\t    result_positive = {label: 0 for label in db_seg.classes}\n\t    result_negative = {label: 0 for label in db_seg.classes}\n\t    for idx in tqdm(range(len(db_seg)), total=len(db_seg)):\n\t        pred_seg = model.predict(torch.unsqueeze(db_seg[idx][0].to(model.device), dim=0))[0]\n\t        if torch.any(pred_seg > 0.5):\n", "            pred_gan = model.predict(torch.unsqueeze(db_gan[idx][0].to(model.device), dim=0))[0]\n\t            pred_seg = pred_seg.cpu().detach().numpy()\n\t            pred_gan = pred_gan.cpu().detach().numpy()\n\t            for label_idx in range(db_seg.num_classes):\n\t                if pred_seg[label_idx] > 0.5 and pred_gan[label_idx] > 0.5:\n\t                    result_positive[db_seg.classes[label_idx]] += 1\n\t                elif pred_seg[label_idx] > 0.5 and pred_gan[label_idx] <= 0.5:\n\t                    result_negative[db_seg.classes[label_idx]] += 1\n\t    df_pos = pd.read_csv(os.path.join(output_dir, 'result', 'positive.csv'), index_col=0)\n\t    df_neg = pd.read_csv(os.path.join(output_dir, 'result', 'negative.csv'), index_col=0)\n", "    df = pd.merge(df_pos, df_neg, right_index=True, left_index=True)\n\t    result = df['Negative'] / df.sum(axis=1)\n\t    result = pd.DataFrame(result, columns=['GANonymization'])\n\t    result = result.rename(index={s: s.replace('_', ' ') for s in result.index.values})\n\t    result = result.sort_values('GANonymization', ascending=False)\n\t    result.to_csv(os.path.join(output_dir, 'result', 'traits_removed.csv'))\n\t    result.to_latex(os.path.join(output_dir, 'result', 'traits_removed.latex'))\n\t    df_split = np.array_split(result, int(round(len(result) / 20)))\n\t    for idx, df_tmp in enumerate(df_split):\n\t        fig_width = int(round(len(df_tmp.index) / 2))\n", "        if fig_width < 4:\n\t            fig_width = 4\n\t        df_tmp.plot.bar(figsize=(fig_width, 4))\n\t        plt.gca().set_ylim([0, 1])\n\t        plt.ylabel('Removed (in %)')\n\t        plt.xlabel('Category')\n\t        plt.tight_layout()\n\t        plt.savefig(os.path.join(output_dir, 'result', f'{idx}_traits_removed.png'))\n\t        plt.close()\n"]}
{"filename": "lib/__init__.py", "chunked_list": []}
{"filename": "lib/utils.py", "chunked_list": ["import os\n\timport os.path\n\timport pathlib\n\timport shutil\n\tfrom glob import glob\n\tfrom typing import Optional, List\n\tfrom tqdm import tqdm\n\tdef move_files(files: List[str], output_dir: str):\n\t    \"\"\"\n\t    Move a list of files to another directory.\n", "    @param files: Files to move.\n\t    @param output_dir: The output directory.\n\t    \"\"\"\n\t    os.makedirs(output_dir, exist_ok=True)\n\t    for file_path in tqdm(files, desc=output_dir):\n\t        shutil.copyfile(file_path, os.path.join(output_dir, pathlib.Path(file_path).name))\n\tdef get_last_ckpt(ckpt_directory: str) -> Optional[str]:\n\t    \"\"\"\n\t    Retrieve the last checkpoint based on the creation timestamp.\n\t    @param ckpt_directory: The directory where the checkpoints are saved.\n", "    @return: The checkpoint or None if none was found.\n\t    \"\"\"\n\t    ckpt_files = glob(os.path.join(ckpt_directory, '*.ckpt'))\n\t    if len(ckpt_files) > 0:\n\t        return max(ckpt_files, key=os.path.getctime)\n\t    return None\n\tdef glob_dir(directory: str, exclude: List[str] = ['Thumbs.db', '.DS_Store']):\n\t    \"\"\"\n\t    Recursively search and list all files in the directory filtered by the exclusion clause.\n\t    @param directory: The directory to search in.\n", "    @param exclude: The files or file-endings to filter out.\n\t    @return: A list of files.\n\t    \"\"\"\n\t    return list(\n\t        filter(lambda f: not f.endswith(tuple(exclude)), glob(os.path.join(directory, '**', f'*.*'), recursive=True)))\n"]}
{"filename": "lib/trainer.py", "chunked_list": ["import os\n\timport random\n\timport numpy as np\n\timport pytorch_lightning\n\timport torch\n\tfrom loguru import logger\n\tfrom pytorch_lightning import LightningModule\n\tfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n\tfrom pytorch_lightning.loggers import TensorBoardLogger\n\tdef setup_torch_device(device: int, seed: int) -> str:\n", "    \"\"\"\n\t    Set up the torch device with a seed.\n\t    @param device: The device the model should be run on.\n\t    @param seed: The seed for the run.\n\t    @return: The string of the device.\n\t    \"\"\"\n\t    torch_device = f'cuda:{device}' if device >= 0 else 'cpu'\n\t    logger.info(f\"Using {'GPU.' if device >= 0 else 'CPU, as was explicitly requested, or as GPU is not available.'}\")\n\t    np.random.seed(seed)\n\t    torch.manual_seed(seed)\n", "    random.seed(seed)\n\t    pytorch_lightning.seed_everything(seed)\n\t    if device >= 0:\n\t        torch.backends.cudnn.deterministic = True\n\t        torch.backends.cudnn.benchmark = False\n\t        torch.cuda.manual_seed_all(seed)\n\t        torch.cuda.manual_seed(seed)\n\t    return torch_device\n\tdef setup(model: LightningModule, log_dir: str, models_dir: str, num_epoch: int, device: int,\n\t          save_top_k: int = 0, monitor: str = None, metric_mode: str = None, early_stop_n: int = 0,\n", "          checkpoint_interval: int = None) -> pytorch_lightning.Trainer:\n\t    \"\"\"\n\t    Run the lightning model.\n\t    :param model: The model to train.\n\t    :param num_epoch: The number of epochs to train.\n\t    :param device: The device to work on.\n\t    :param save_top_k: Save top k checkpoints - or every epoch if k=0.\n\t    :param monitor: The metric variable to monitor.\n\t    :param metric_mode: The mode of the metric to decide which checkpoint to choose (min or max).\n\t    :param early_stop_n: Stops training after n epochs of no improvement - default is deactivated.\n", "    :param checkpoint_interval: The interval a checkpoint should be saved if save_top_k is 0.\n\t    \"\"\"\n\t    model_name = model.__class__.__name__\n\t    os.makedirs(models_dir, exist_ok=True)\n\t    os.makedirs(log_dir, exist_ok=True)\n\t    tb_logger = TensorBoardLogger(log_dir, name='', version='')\n\t    callbacks = [LearningRateMonitor()]\n\t    if save_top_k > 0:\n\t        callbacks.append(ModelCheckpoint(dirpath=models_dir, filename='{epoch}-{' + monitor + ':.6f}',\n\t                                         save_top_k=save_top_k, monitor=monitor, mode=metric_mode))\n", "    else:\n\t        callbacks.append(\n\t            ModelCheckpoint(dirpath=models_dir, filename='{epoch}-{step}', monitor='step', mode='max',\n\t                            save_top_k=-1, every_n_train_steps=checkpoint_interval))\n\t    if early_stop_n > 0:\n\t        callbacks.append(\n\t            EarlyStopping(monitor=monitor, min_delta=0.00, patience=early_stop_n, verbose=False, mode=metric_mode))\n\t    trainer = pytorch_lightning.Trainer(deterministic=True,\n\t                                        accelerator=\"gpu\" if device >= 0 else \"cpu\",\n\t                                        devices=[device] if device >= 0 else None,\n", "                                        callbacks=callbacks,\n\t                                        logger=tb_logger,\n\t                                        max_epochs=num_epoch,\n\t                                        val_check_interval=checkpoint_interval,\n\t                                        log_every_n_steps=1,\n\t                                        detect_anomaly=True)\n\t    logger.info(\n\t        f'Train {model_name} for {num_epoch} epochs and save logs under {tb_logger.log_dir} and models under {models_dir}')\n\t    model.train()\n\t    model.to(device)\n", "    return trainer\n"]}
{"filename": "lib/evaluator.py", "chunked_list": ["import os.path\n\timport numpy as np\n\timport pandas as pd\n\timport seaborn as sn\n\timport torch\n\tfrom loguru import logger\n\tfrom matplotlib import pyplot as plt\n\tfrom mpl_toolkits.axes_grid1 import ImageGrid\n\tfrom sklearn.metrics import classification_report\n\tfrom torch.utils.data import DataLoader\n", "from torchmetrics.classification import MulticlassConfusionMatrix, MultilabelConfusionMatrix\n\tfrom torchvision.transforms import Compose\n\tfrom tqdm import tqdm\n\tfrom lib.datasets import DatasetSplit\n\tfrom lib.datasets.labeled_dataset import LabeledDataset\n\tfrom lib.models.generic_classifier import GenericClassifier\n\tfrom lib.utils import get_last_ckpt\n\tdef eval_classifier(models_dir: str, data_dir: str, batch_size: int = 128, device: int = 0,\n\t                    output_dir: str = 'output', num_workers: int = 2):\n\t    logger.info(f\"Parameters: {', '.join([f'{key}: {value}' for key, value in locals().items()])}\")\n", "    os.makedirs(output_dir, exist_ok=True)\n\t    val_db = LabeledDataset(data_dir, DatasetSplit.VALIDATION,\n\t                            Compose([GenericClassifier.weights.transforms()]))\n\t    model = GenericClassifier.load_from_checkpoint(get_last_ckpt(models_dir), classes=val_db.classes,\n\t                                                   multi_label=val_db.is_multi_label)\n\t    model.eval()\n\t    model.to(device)\n\t    if model.multi_label:\n\t        cm = MultilabelConfusionMatrix(num_labels=val_db.num_classes, normalize='true').to(device)\n\t        logger.debug('Setting up evaluation for multi-label classification')\n", "    else:\n\t        cm = MulticlassConfusionMatrix(num_classes=val_db.num_classes, normalize='true').to(device)\n\t        logger.debug('Setting up evaluation for multi-class classification')\n\t    dl = DataLoader(val_db, batch_size=batch_size, num_workers=num_workers)\n\t    all_label = []\n\t    all_pred = []\n\t    for batch in tqdm(dl, desc='Evaluation'):\n\t        images, labels = batch\n\t        pred = model.predict(images.to(device))\n\t        labels = labels.to(device)\n", "        if val_db.is_multi_label:\n\t            ml_pred = torch.round(pred).int()\n\t            ml_labels = labels.int()\n\t            cm.update(ml_pred, ml_labels)\n\t        else:\n\t            mc_labels = torch.argmax(labels, dim=1)\n\t            cm.update(pred, mc_labels)\n\t        all_label.extend(labels.cpu().detach().numpy())\n\t        all_pred.extend(pred.cpu().detach().numpy())\n\t    # Confusion Matrix\n", "    all_label = np.asarray(all_label)\n\t    all_pred = np.asarray(all_pred)\n\t    np.savez_compressed(os.path.join(output_dir, 'result_pred_label.npz'), pred=all_pred, label=all_label)\n\t    conf_matrix = cm.compute()\n\t    conf_matrix = conf_matrix.cpu().detach().numpy()\n\t    logger.debug('Creating Confusion Matrix...')\n\t    if model.multi_label:\n\t        n_columns = int(model.num_classes / 4)\n\t        n_rows = int(model.num_classes / n_columns)\n\t        fig = plt.figure(figsize=(n_columns * 2, n_rows * 2))\n", "        plt.tight_layout()\n\t        grid = ImageGrid(fig, 111,\n\t                         nrows_ncols=(n_rows, n_columns),\n\t                         axes_pad=0.5,\n\t                         share_all=True,\n\t                         cbar_mode='single',\n\t                         cbar_location='right',\n\t                         cbar_size='5%',\n\t                         cbar_pad=0.5)\n\t        for idx in tqdm(range(conf_matrix.shape[0]), desc='Multi-Label Confusion Matrices'):\n", "            label = model.classes[idx]\n\t            binary_labels = ['Present', 'Absent']\n\t            df_cm = pd.DataFrame(conf_matrix[idx, :, :], index=binary_labels, columns=binary_labels)\n\t            df_cm.to_csv(os.path.join(output_dir, f'{os.path.basename(data_dir)}_{label}_confusion_matrix.csv'))\n\t            sn.set(font_scale=1.3)\n\t            sn.heatmap(df_cm, annot=True, fmt='.2f', xticklabels=binary_labels, yticklabels=binary_labels,\n\t                       ax=grid[idx], cbar_ax=grid[0].cax, annot_kws={'fontsize': 13})\n\t            grid[idx].set_ylabel('Actual')\n\t            grid[idx].set_xlabel('Predicted')\n\t            grid[idx].set_title(label.replace('_', ' ').replace('-', ' '))\n", "        plt.tight_layout()\n\t        plt.savefig(os.path.join(output_dir, f'{os.path.basename(data_dir)}_confusion_matrix.png'))\n\t        plt.close()\n\t    else:\n\t        df_cm = pd.DataFrame(conf_matrix, index=val_db.labels, columns=val_db.labels)\n\t        df_cm.to_csv(os.path.join(output_dir, f'{os.path.basename(data_dir)}_confusion_matrix.csv'))\n\t        plt.figure(figsize=(\n\t            len(val_db.labels) + int(len(val_db.labels) / 2), len(val_db.labels) + int(len(val_db.labels) / 2)))\n\t        plt.tight_layout()\n\t        sn.set(font_scale=1.3)\n", "        sn.heatmap(df_cm, annot=True, fmt='.2f', xticklabels=val_db.labels, yticklabels=val_db.labels, annot_kws={\n\t            'fontsize': 13\n\t        })\n\t        plt.ylabel('Actual')\n\t        plt.xlabel('Predicted')\n\t        plt.savefig(os.path.join(output_dir, f'{os.path.basename(data_dir)}_confusion_matrix.png'))\n\t        plt.close()\n\t    logger.debug('Confusion Matrix created!')\n\t    # Classification Report\n\t    logger.debug('Creating Classification Report...')\n", "    if model.multi_label:\n\t        all_label = np.round(all_label)\n\t        all_pred = np.round(all_pred)\n\t    else:\n\t        all_label = np.squeeze(np.argmax(all_label, axis=1))\n\t        all_pred = np.squeeze(np.argmax(all_pred, axis=1))\n\t    report = classification_report(all_label, all_pred, target_names=val_db.labels)\n\t    with open(os.path.join(output_dir, f'{os.path.basename(data_dir)}_classification_report.txt'),\n\t              'w+') as f:\n\t        f.write(report)\n", "    logger.debug('Classification Report created!')\n"]}
{"filename": "lib/models/generic_classifier.py", "chunked_list": ["from typing import List\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom loguru import logger\n\tfrom pytorch_lightning.cli import ReduceLROnPlateau\n\tfrom torch import nn, Tensor\n\tfrom torch.utils.data import DataLoader\n\tfrom torchmetrics.classification import MulticlassAccuracy, MultilabelAccuracy\n\tfrom torchvision.models import convnext_base, ConvNeXt_Base_Weights\n\tfrom lib.datasets.labeled_dataset import LabeledDataset\n", "class GenericClassifier(pl.LightningModule):\n\t    \"\"\"\n\t    The GenericClassifier is based on the ConvNeXt architecture and\n\t    can be used to train a multi-class or multi-label problem.\n\t    \"\"\"\n\t    weights = ConvNeXt_Base_Weights.DEFAULT\n\t    def __init__(self, multi_label: bool, classes: List[str], batch_size: int,\n\t                 learning_rate: float, num_workers: int, device: str, val_db: LabeledDataset = None,\n\t                 train_db: LabeledDataset = None, class_weights: Tensor = None):\n\t        \"\"\"\n", "        Create a new GenericClassifier.\n\t        @param multi_label: If the problem at hand is multi-label then True otherwise False for multi-class.\n\t        @param classes: The class names as a list.\n\t        @param batch_size: The size of the batches.\n\t        @param learning_rate: The learning rate.\n\t        @param num_workers: The number of cpu-workers.\n\t        @param device: The device to run the model on.\n\t        @param val_db: The validation dataset.\n\t        @param train_db: The training dataset.\n\t        @param class_weights: The class weights of the dataset.\n", "        \"\"\"\n\t        super().__init__()\n\t        # Settings\n\t        self.train_db = train_db\n\t        self.val_db = val_db\n\t        self.learning_rate = learning_rate\n\t        self.batch_size = batch_size\n\t        self.num_workers = num_workers\n\t        self.multi_label = multi_label\n\t        self.classes = classes\n", "        self.num_classes = len(classes)\n\t        # Define model architecture\n\t        self.model = convnext_base(weights=self.weights)\n\t        self.model.classifier[-1] = nn.Linear(self.model.classifier[-1].in_features, self.num_classes)\n\t        self.class_weights = class_weights.float().to(device)\n\t        # Loss function, accuracy\n\t        if self.multi_label:\n\t            logger.debug(f'Model is a Multi-Label Classificator')\n\t            self.loss_function = torch.nn.BCEWithLogitsLoss(reduction='none').to(device)\n\t            self.activation = torch.nn.Sigmoid().to(device)\n", "            self.metrics = {\n\t                'accuracy': MultilabelAccuracy(num_labels=self.num_classes).to(device)\n\t            }\n\t        else:\n\t            logger.debug(f'Model is a Multi-Class Classificator')\n\t            self.loss_function = torch.nn.CrossEntropyLoss(weight=self.class_weights).to(device)\n\t            self.activation = torch.nn.Softmax(dim=1).to(device)\n\t            self.metrics = {\n\t                'top_1_acc': MulticlassAccuracy(top_k=1, num_classes=self.num_classes).to(device),\n\t                'top_5_acc': MulticlassAccuracy(top_k=5, num_classes=self.num_classes).to(device),\n", "            }\n\t        self.save_hyperparameters(ignore=['val_db', 'train_db'])\n\t    def forward(self, x):\n\t        return self.model(x)\n\t    def predict(self, x):\n\t        return self.activation(self.forward(x))\n\t    def configure_optimizers(self):\n\t        optimizer = torch.optim.AdamW(params=self.parameters(), lr=self.learning_rate, weight_decay=0.001)\n\t        scheduler = ReduceLROnPlateau(optimizer, monitor='val_loss', patience=3)\n\t        return [optimizer], [{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'val_loss'}]\n", "    def training_step(self, train_batch, batch_idx):\n\t        loss, _ = self.step('train', train_batch)\n\t        return loss\n\t    def train_dataloader(self):\n\t        return DataLoader(self.train_db, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n\t    def validation_step(self, val_batch, batch_idx):\n\t        _, labels = val_batch\n\t        _, output = self.step('val', val_batch)\n\t        if not self.multi_label:\n\t            labels = torch.argmax(labels, dim=1)\n", "        for key, metric in self.metrics.items():\n\t            self.log(f'val_{key}', metric(output, labels), prog_bar=True)\n\t    def val_dataloader(self):\n\t        return DataLoader(self.val_db, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n\t    def step(self, tag: str, batch):\n\t        images, labels = batch\n\t        output = self.forward(images)\n\t        if self.multi_label:\n\t            loss = self.loss_function(output, labels)\n\t            loss = (loss * self.class_weights).mean()\n", "        else:\n\t            loss = self.loss_function(output, torch.argmax(labels, dim=1))\n\t        self.log(f'{tag}_loss', loss.item(), prog_bar=True)\n\t        return loss, output\n"]}
{"filename": "lib/models/pix2pix.py", "chunked_list": ["import os.path\n\timport pytorch_lightning\n\timport torch\n\tfrom torch import nn\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision.utils import save_image, make_grid\n\tfrom lib.datasets.image_dataset import ImageDataset\n\tdef weights_init_normal(m):\n\t    classname = m.__class__.__name__\n", "    if classname.find(\"Conv\") != -1:\n\t        nn.init.normal_(m.weight.data, 0.0, 0.02)\n\t    elif classname.find(\"BatchNorm2d\") != -1:\n\t        nn.init.normal_(m.weight.data, 1.0, 0.02)\n\t        nn.init.constant_(m.bias.data, 0.0)\n\t##############################\n\t#           U-NET\n\t##############################\n\tclass UNetDown(nn.Module):\n\t    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n", "        super(UNetDown, self).__init__()\n\t        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n\t        if normalize:\n\t            layers.append(nn.InstanceNorm2d(out_size))\n\t        layers.append(nn.LeakyReLU(0.2))\n\t        if dropout:\n\t            layers.append(nn.Dropout(dropout))\n\t        self.model = nn.Sequential(*layers)\n\t    def forward(self, x):\n\t        return self.model(x)\n", "class UNetUp(nn.Module):\n\t    def __init__(self, in_size, out_size, dropout=0.0):\n\t        super(UNetUp, self).__init__()\n\t        layers = [\n\t            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n\t            nn.InstanceNorm2d(out_size),\n\t            nn.ReLU(inplace=True),\n\t        ]\n\t        if dropout:\n\t            layers.append(nn.Dropout(dropout))\n", "        self.model = nn.Sequential(*layers)\n\t    def forward(self, x, skip_input):\n\t        x = self.model(x)\n\t        x = torch.cat((x, skip_input), 1)\n\t        return x\n\tclass GeneratorUNet(nn.Module):\n\t    def __init__(self, in_channels=3, out_channels=3):\n\t        super(GeneratorUNet, self).__init__()\n\t        self.down1 = UNetDown(in_channels, 64, normalize=False)\n\t        self.down2 = UNetDown(64, 128)\n", "        self.down3 = UNetDown(128, 256)\n\t        self.down4 = UNetDown(256, 512, dropout=0.5)\n\t        self.down5 = UNetDown(512, 512, dropout=0.5)\n\t        self.down6 = UNetDown(512, 512, dropout=0.5)\n\t        self.down7 = UNetDown(512, 512, dropout=0.5)\n\t        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\t        self.up1 = UNetUp(512, 512, dropout=0.5)\n\t        self.up2 = UNetUp(1024, 512, dropout=0.5)\n\t        self.up3 = UNetUp(1024, 512, dropout=0.5)\n\t        self.up4 = UNetUp(1024, 512, dropout=0.5)\n", "        self.up5 = UNetUp(1024, 256)\n\t        self.up6 = UNetUp(512, 128)\n\t        self.up7 = UNetUp(256, 64)\n\t        self.final = nn.Sequential(\n\t            nn.Upsample(scale_factor=2),\n\t            nn.ZeroPad2d((1, 0, 1, 0)),\n\t            nn.Conv2d(128, out_channels, 4, padding=1),\n\t            nn.Tanh(),\n\t        )\n\t    def forward(self, x):\n", "        # U-Net generator with skip connections from encoder to decoder\n\t        d1 = self.down1(x)\n\t        d2 = self.down2(d1)\n\t        d3 = self.down3(d2)\n\t        d4 = self.down4(d3)\n\t        d5 = self.down5(d4)\n\t        d6 = self.down6(d5)\n\t        d7 = self.down7(d6)\n\t        d8 = self.down8(d7)\n\t        u1 = self.up1(d8, d7)\n", "        u2 = self.up2(u1, d6)\n\t        u3 = self.up3(u2, d5)\n\t        u4 = self.up4(u3, d4)\n\t        u5 = self.up5(u4, d3)\n\t        u6 = self.up6(u5, d2)\n\t        u7 = self.up7(u6, d1)\n\t        return self.final(u7)\n\t##############################\n\t#        Discriminator\n\t##############################\n", "class Discriminator(nn.Module):\n\t    def __init__(self, in_channels=3):\n\t        super(Discriminator, self).__init__()\n\t        def discriminator_block(in_filters, out_filters, normalization=True):\n\t            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n\t            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n\t            if normalization:\n\t                layers.append(nn.InstanceNorm2d(out_filters))\n\t            layers.append(nn.LeakyReLU(0.2, inplace=True))\n\t            return layers\n", "        self.model = nn.Sequential(\n\t            *discriminator_block(in_channels * 2, 64, normalization=False),\n\t            *discriminator_block(64, 128),\n\t            *discriminator_block(128, 256),\n\t            *discriminator_block(256, 512),\n\t            nn.ZeroPad2d((1, 0, 1, 0)),\n\t            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n\t        )\n\t    def forward(self, img_A, img_B):\n\t        # Concatenate image and condition image by channels to produce input\n", "        img_input = torch.cat((img_A, img_B), 1)\n\t        return self.model(img_input)\n\t##############################\n\t#        Lightning\n\t##############################\n\tclass Pix2Pix(pytorch_lightning.LightningModule):\n\t    def __init__(self, data_dir: str, models_dir: str, output_dir: str, n_epochs: int,\n\t                 dataset_name: str, batch_size: int, lr: float, b1: float, b2: float, n_cpu: int, img_size: int,\n\t                 device: int):\n\t        \"\"\"\n", "        Create a Pix2Pix Network.\n\t        @param data_dir: The directory of the data.\n\t        @param models_dir: The directory of the models.\n\t        @param output_dir: The directory of the output.\n\t        @param n_epochs: The number of epochs.\n\t        @param dataset_name: The name of the dataset which is appended to the output_dir.\n\t        @param batch_size: The size of the batches to process.\n\t        @param lr: The learning rate.\n\t        @param b1: The beta 1 value for the optimizer.\n\t        @param b2: The beta 2 value for the optimizer.\n", "        @param n_cpu: The number of cpus.\n\t        @param img_size: The size of the image.\n\t        @param device: The device to run the computation on.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.save_hyperparameters()\n\t        self.automatic_optimization = False\n\t        self.data_dir = data_dir\n\t        self.models_dir = models_dir\n\t        self.output_dir = output_dir\n", "        self.n_epochs = n_epochs\n\t        self.dataset_name = dataset_name\n\t        self.batch_size = batch_size\n\t        self.lr = lr\n\t        self.b1 = b1\n\t        self.b2 = b2\n\t        self.n_cpu = n_cpu\n\t        self.img_size = img_size\n\t        self.out_dir = os.path.join(output_dir, dataset_name)\n\t        os.makedirs(self.out_dir, exist_ok=True)\n", "        self.transforms_ = [\n\t            transforms.Resize((img_size, img_size)),\n\t            transforms.ToTensor(),\n\t            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n\t        ]\n\t        self.criterion_GAN = torch.nn.MSELoss()\n\t        self.criterion_GAN.to(device)\n\t        self.criterion_pixelwise = torch.nn.L1Loss()\n\t        self.criterion_pixelwise.to(device)\n\t        self.lambda_pixel = 100\n", "        self.generator = GeneratorUNet()\n\t        self.generator.to(device)\n\t        self.discriminator = Discriminator()\n\t        self.discriminator.to(device)\n\t        # Initialize weights\n\t        self.generator.apply(weights_init_normal)\n\t        self.discriminator.apply(weights_init_normal)\n\t    def forward(self, x):\n\t        return self.generator(x)\n\t    def training_step(self, batch):\n", "        optimizer_g, optimizer_d = self.optimizers()\n\t        # Model inputs\n\t        real_A = batch[\"B\"]\n\t        real_B = batch[\"A\"]\n\t        # Calculate output of image discriminator (PatchGAN)\n\t        patch = (1, self.img_size // 2 ** 4, self.img_size // 2 ** 4)\n\t        # Adversarial ground truths\n\t        valid = torch.ones((real_A.size(0), *patch), requires_grad=False).to(self.device)\n\t        fake = torch.zeros((real_A.size(0), *patch), requires_grad=False).to(self.device)\n\t        # ------------------\n", "        #  Train Generators\n\t        # ------------------\n\t        self.toggle_optimizer(optimizer_g)\n\t        # GAN loss\n\t        fake_B = self.generator(real_A)\n\t        pred_fake = self.discriminator(fake_B, real_A)\n\t        loss_GAN = self.criterion_GAN(pred_fake, valid)\n\t        # Pixel-wise loss\n\t        loss_pixel = self.criterion_pixelwise(fake_B, real_B)\n\t        # Total loss\n", "        loss_G = loss_GAN + self.lambda_pixel * loss_pixel\n\t        self.log('G loss', loss_G, prog_bar=True)\n\t        self.log('G pixel', loss_pixel, prog_bar=True)\n\t        self.log('G adv', loss_GAN, prog_bar=True)\n\t        self.manual_backward(loss_G)\n\t        optimizer_g.step()\n\t        optimizer_g.zero_grad()\n\t        self.untoggle_optimizer(optimizer_g)\n\t        # ---------------------\n\t        #  Train Discriminator\n", "        # ---------------------\n\t        self.toggle_optimizer(optimizer_d)\n\t        # Real loss\n\t        pred_real = self.discriminator(real_B, real_A)\n\t        loss_real = self.criterion_GAN(pred_real, valid)\n\t        # Fake loss\n\t        fake_B = self.generator(real_A)\n\t        pred_fake = self.discriminator(fake_B.detach(), real_A)\n\t        loss_fake = self.criterion_GAN(pred_fake, fake)\n\t        # Total loss\n", "        loss_D = 0.5 * (loss_real + loss_fake)\n\t        self.log('D loss', loss_D, prog_bar=True)\n\t        self.manual_backward(loss_D)\n\t        optimizer_d.step()\n\t        optimizer_d.zero_grad()\n\t        self.untoggle_optimizer(optimizer_d)\n\t    def validation_step(self, batch, batch_idx):\n\t        if batch_idx == 0:\n\t            real_A = batch[\"B\"]\n\t            real_B = batch[\"A\"]\n", "            fake_B = self.generator(real_A)\n\t            img_sample = torch.cat((real_B.data, real_A.data, fake_B.data), -2)\n\t            save_image(img_sample, os.path.join(self.out_dir, f'{self.current_epoch}-{self.global_step}.png'), nrow=5,\n\t                       normalize=True)\n\t            grid = make_grid(img_sample, nrow=5, normalize=True)\n\t            self.logger.experiment.add_image('images', grid, self.global_step)\n\t    def configure_optimizers(self):\n\t        optimizer_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n\t        optimizer_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n\t        return [optimizer_g, optimizer_d], []\n", "    def train_dataloader(self):\n\t        return DataLoader(\n\t            ImageDataset(os.path.join(self.data_dir, self.dataset_name), transforms_=self.transforms_),\n\t            batch_size=self.batch_size,\n\t            shuffle=True,\n\t            num_workers=self.n_cpu,\n\t        )\n\t    def val_dataloader(self):\n\t        return DataLoader(\n\t            ImageDataset(os.path.join(self.data_dir, self.dataset_name), transforms_=self.transforms_, mode=\"val\"),\n", "            batch_size=10,\n\t            shuffle=True,\n\t            num_workers=self.n_cpu,\n\t        )\n"]}
{"filename": "lib/transform/zero_padding_resize_transformer.py", "chunked_list": ["import numpy as np\n\tfrom PIL import Image\n\tclass ZeroPaddingResize:\n\t    \"\"\"\n\t    The ZeroPaddingResize transformer resizes the image into the given size and applies zero-padding to center\n\t    the output image.\n\t    \"\"\"\n\t    def __init__(self, size: int):\n\t        \"\"\"\n\t        @param size: The size the image will be resized to.\n", "        \"\"\"\n\t        self.size = size\n\t    def __call__(self, pic):\n\t        \"\"\"\n\t        @param pic (PIL Image or numpy.ndarray): Image to be converted to center zero-padded and resized image.\n\t        @return: numpy.ndarray: Converted image.\n\t        \"\"\"\n\t        if isinstance(pic, np.ndarray):\n\t            pic = Image.fromarray(pic)\n\t        if pic.width != self.size or pic.height != self.size:\n", "            ratio = min(self.size / pic.width, self.size / pic.height)\n\t            face_img = pic.resize((int(ratio * pic.width), int(ratio * pic.height)), Image.LANCZOS)\n\t            new_im = Image.new(\"RGB\", (self.size, self.size))\n\t            new_im.paste(face_img, ((self.size - face_img.width) // 2, (self.size - face_img.height) // 2))\n\t            pic = np.array(new_im)\n\t        return pic\n\t    def __repr__(self) -> str:\n\t        return f\"{self.__class__.__name__}\"\n"]}
{"filename": "lib/transform/facial_landmarks_478_transformer.py", "chunked_list": ["import cv2\n\timport mediapipe\n\timport numpy as np\n\tfrom mediapipe.python.solutions.drawing_utils import DrawingSpec, WHITE_COLOR\n\tclass FacialLandmarks478:\n\t    \"\"\"Extract 468 facial landmark points from the picture and return it in a 2-dimensional picture.\"\"\"\n\t    def __call__(self, pic):\n\t        \"\"\"\n\t        @param pic (PIL Image or numpy.ndarray): Image to be converted to a facial landmark image with 468 points.\n\t        @return: numpy.ndarray: Converted image.\n", "        \"\"\"\n\t        point_image = np.zeros(pic.shape, np.uint8)\n\t        with mediapipe.solutions.face_mesh.FaceMesh(\n\t                static_image_mode=True,\n\t                max_num_faces=1,\n\t                refine_landmarks=True,\n\t                min_detection_confidence=0.5) as face_mesh:\n\t            results = face_mesh.process(cv2.cvtColor(pic, cv2.COLOR_BGR2RGB))\n\t            if results.multi_face_landmarks is not None and len(results.multi_face_landmarks) > 0:\n\t                mediapipe.solutions.drawing_utils.draw_landmarks(\n", "                    image=point_image,\n\t                    landmark_list=results.multi_face_landmarks[0],\n\t                    landmark_drawing_spec=DrawingSpec(color=WHITE_COLOR, thickness=1, circle_radius=0))\n\t        return point_image\n\t    def __repr__(self) -> str:\n\t        return f\"{self.__class__.__name__}\"\n"]}
{"filename": "lib/transform/__init__.py", "chunked_list": ["import os\n\timport pathlib\n\timport shutil\n\tfrom typing import List, Optional\n\timport cv2\n\timport numpy as np\n\tfrom loguru import logger\n\tfrom p_tqdm import p_tqdm\n\tfrom tqdm import tqdm\n\tfrom lib.transform.face_crop_transformer import FaceCrop\n", "from lib.transform.face_segmentation_transformer import FaceSegmentation\n\tfrom lib.transform.facial_landmarks_478_transformer import FacialLandmarks478\n\tfrom lib.transform.zero_padding_resize_transformer import ZeroPaddingResize\n\tfrom lib.transform.pix2pix_transformer import Pix2PixTransformer\n\tfrom lib.utils import glob_dir\n\tdef exec(files: List[str], output_dir: str, input_dir: str, size: int, gallery: bool, transformer):\n\t    \"\"\"\n\t    Executes the augmentation.\n\t    @param files: The files to be augmented.\n\t    @param output_dir: The directory of the output.\n", "    @param input_dir: The directory of the input.\n\t    @param size: The image size.\n\t    @param gallery: Whether the image should be saved besides its original.\n\t    @param transformer: The transformer to be applied.\n\t    \"\"\"\n\t    name = str(transformer)\n\t    for image_file in files:\n\t        sub_path_image = os.path.dirname(image_file[len(input_dir):])\n\t        sub_output_dir = os.path.join(output_dir, *sub_path_image.split(os.sep))\n\t        img = cv2.imread(image_file)\n", "        if img is not None:\n\t            pred = transformer(img)\n\t            for idx, sub_pred in enumerate(pred):\n\t                sub_pred = ZeroPaddingResize(size)(sub_pred)\n\t                os.makedirs(sub_output_dir, exist_ok=True)\n\t                output_file = os.path.join(sub_output_dir, f'{name}_{idx}-{pathlib.Path(image_file).name}')\n\t                if gallery:\n\t                    cv2.imwrite(output_file, cv2.hconcat([img, sub_pred]))\n\t                else:\n\t                    cv2.imwrite(output_file, sub_pred)\n", "    return 0\n\tdef transform(input_dir: str, size: int, gallery: bool, transformer,\n\t              output_dir: Optional[str] = None, num_workers: int = 1) -> str:\n\t    \"\"\"\n\t    Transform all images found in the input directory.\n\t    @param input_dir: The input directory.\n\t    @param size: The size of the images afterwards.\n\t    @param gallery: Whether the image should be saved besides its original.\n\t    @param transformer: The transformer to be applied.\n\t    @param output_dir: The output directory.\n", "    @param num_workers: The number of parallel workers.\n\t    @return: The output path.\n\t    \"\"\"\n\t    name = str(transformer)\n\t    if output_dir is None:\n\t        output_dir = os.path.dirname(input_dir)\n\t    output_dir = os.path.join(output_dir, name)\n\t    logger.debug(f'Preparing output directory: {output_dir}')\n\t    os.makedirs(output_dir, exist_ok=True)\n\t    # Copy other files to destination\n", "    for f in os.listdir(input_dir):\n\t        file = os.path.join(input_dir, f)\n\t        if os.path.isfile(file):\n\t            shutil.copyfile(file, os.path.join(output_dir, f))\n\t    # Search for every image\n\t    files = glob_dir(input_dir)\n\t    logger.debug(f'Found {len(files)} files in source directory: {input_dir}')\n\t    # Search for already processed files\n\t    out_files = glob_dir(output_dir)\n\t    out_files = [pathlib.Path(f).name.split('-')[-1] for f in out_files]\n", "    logger.debug(f'Found {len(out_files)} files in destination directory: {output_dir}')\n\t    files_augment = []\n\t    for f in tqdm(files, desc='Skip Check'):\n\t        if pathlib.Path(f).name.split('-')[-1] not in out_files:\n\t            files_augment.append(f)\n\t    if len(files_augment) > 0:\n\t        logger.debug(f'Processing {len(files_augment)} files...')\n\t        list_chunks = np.array_split(files_augment, num_workers)\n\t        logger.debug(f'Distribute workload to {num_workers} workers with a chunk size of {len(list_chunks[0])} each')\n\t        p_tqdm.p_umap(exec, list_chunks, [output_dir] * len(list_chunks), [input_dir] * len(list_chunks),\n", "                      [size] * len(list_chunks), [gallery] * len(list_chunks), [transformer] * len(list_chunks),\n\t                      num_cpus=num_workers)\n\t    else:\n\t        logger.debug('Data has already been fully processed!')\n\t    return output_dir\n"]}
{"filename": "lib/transform/face_segmentation_transformer.py", "chunked_list": ["import cv2\n\tfrom head_segmentation import HumanHeadSegmentationPipeline\n\tclass FaceSegmentation:\n\t    \"\"\"\n\t    The FaceSegmentation transformer eliminates everything besides the face in the image.\n\t    \"\"\"\n\t    def __call__(self, pic):\n\t        \"\"\"\n\t        @param pic (PIL Image or numpy.ndarray): Image to be converted to a face segmentation.\n\t        @return: numpy.ndarray: Converted image.\n", "        \"\"\"\n\t        segmentation_pipeline = HumanHeadSegmentationPipeline()\n\t        face_mask = segmentation_pipeline.predict(pic)\n\t        segmented_region = pic * cv2.cvtColor(face_mask, cv2.COLOR_GRAY2RGB)\n\t        return segmented_region\n\t    def __repr__(self) -> str:\n\t        return f\"{self.__class__.__name__}\"\n"]}
{"filename": "lib/transform/pix2pix_transformer.py", "chunked_list": ["import torch\n\tfrom torchvision.transforms import transforms\n\tfrom lib.models.pix2pix import Pix2Pix\n\tclass Pix2PixTransformer:\n\t    \"\"\"\n\t    The GANonymization transformer synthesizes images based on facial landmarks images.\n\t    \"\"\"\n\t    def __init__(self, model_file: str, img_size: int, device: int):\n\t        self.model = Pix2Pix.load_from_checkpoint(model_file)\n\t        self.model.eval()\n", "        self.model.to(device)\n\t        self.device = device\n\t        self.transforms_ = transforms.Compose([\n\t            transforms.ToTensor(),\n\t            transforms.Resize((img_size, img_size)),\n\t            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n\t        ])\n\t    def __call__(self, pic):\n\t        \"\"\"\n\t        @param pic (PIL Image or numpy.ndarray): Image to be converted to a face image.\n", "        @return: Tensor: Converted image.\n\t        \"\"\"\n\t        pic_transformed = self.transforms_(pic)\n\t        pic_transformed_device = pic_transformed.to(self.device)\n\t        pic_transformed_device_batched = torch.unsqueeze(pic_transformed_device, dim=0)\n\t        return self.model(pic_transformed_device_batched)\n\t    def __repr__(self) -> str:\n\t        return f\"{self.__class__.__name__}\"\n"]}
{"filename": "lib/transform/face_crop_transformer.py", "chunked_list": ["from retinaface import RetinaFace\n\tclass FaceCrop:\n\t    \"\"\"\n\t    The FaceCrop transformer is based on the RetinaFace library which extracts all available faces from the given image.\n\t    \"\"\"\n\t    def __init__(self, align: bool, multiple_faces: bool):\n\t        \"\"\"\n\t        @param align: Whether the face should be aligned.\n\t        @param multiple_faces: Whether multiple faces should be detected.\n\t        \"\"\"\n", "        self.align = align\n\t        self.multiple_faces = multiple_faces\n\t    def __call__(self, pic):\n\t        \"\"\"\n\t        @param pic (PIL Image or numpy.ndarray): Image to be converted to cropped faces.\n\t        @return: numpy.ndarray: Converted image.\n\t        \"\"\"\n\t        faces = [i[:, :, ::-1] for i in RetinaFace.extract_faces(pic, align=self.align)]\n\t        if not self.multiple_faces:\n\t            if len(faces) > 0:\n", "                return faces[0]\n\t            else:\n\t                return None\n\t        return faces\n\t    def __repr__(self) -> str:\n\t        return f\"{self.__class__.__name__}\"\n"]}
{"filename": "lib/datasets/labeled_dataset.py", "chunked_list": ["import json\n\timport os\n\tfrom typing import List, Tuple\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom PIL import Image\n\tfrom loguru import logger\n\tfrom sklearn.utils import compute_class_weight\n\tfrom torch import Tensor\n", "from torch.utils.data import DataLoader\n\tfrom torchvision.transforms import Compose\n\tfrom lib.datasets import DatasetSplit\n\tfrom lib.utils import glob_dir\n\tclass LabeledDataset(torch.utils.data.Dataset):\n\t    \"\"\"\n\t    The LabeledDataset wraps a folder structure of a dataset with train, val, and test folders\n\t    and the corresponding labels.csv with the meta information.\n\t    \"\"\"\n\t    def __init__(self, data_dir: str, split: DatasetSplit, transforms: Compose):\n", "        \"\"\"\n\t        Create a new LabeledDataset.\n\t        @param data_dir: The root directory of the dataset.\n\t        @param split: Which dataset split should be considered.\n\t        @param transforms: The transformations that are applied to the images before returning.\n\t        \"\"\"\n\t        data_dir = os.path.join(data_dir, split.value['folder'])\n\t        logger.debug(f'Setup LabeledDataset on path: {data_dir}')\n\t        # Load meta-data from labels.csv and filter by the split\n\t        df = pd.read_csv(os.path.join(os.path.dirname(data_dir), 'labels.csv'))\n", "        self.meta_data = df[df['split'] == split.value['value']]\n\t        # Search for all files available in the filesystem\n\t        files_found = glob_dir(data_dir)\n\t        file_names_found = [os.path.basename(f).split('-')[-1] for f in files_found]\n\t        logger.debug(f'Files found: {len(file_names_found)}')\n\t        # Extract base name from image_path in meta-data\n\t        self.meta_data['image_path'] = self.meta_data['image_path'].apply(lambda p: p.split('\\\\')[-1].split('/')[-1])\n\t        # Filter for all available images in files_found\n\t        self.meta_data = self.meta_data[self.meta_data['image_path'].isin(file_names_found)]\n\t        logger.debug(f'Files not available: {len(file_names_found) - len(self.meta_data)}')\n", "        self.meta_data['image_path'] = self.meta_data['image_path'].apply(\n\t            lambda p: files_found[file_names_found.index(p)] if p in file_names_found else p)\n\t        # Extract labels from meta-data\n\t        self.labels = list(self.meta_data.drop(['image_path', 'split'], axis=1).columns.values)[1:]\n\t        self.transforms = transforms\n\t        logger.info(f'Finished \"{os.path.basename(data_dir)}\" Dataset (total={len(self.meta_data)}) Setup!')\n\t    def __len__(self) -> int:\n\t        return len(self.meta_data)\n\t    def __getitem__(self, idx) -> Tuple[Tensor, np.ndarray]:\n\t        df = self.meta_data.iloc[idx]\n", "        img = Image.open(df[\"image_path\"])\n\t        img = self.transforms(img)\n\t        labels = df[self.labels].astype(float).to_numpy()\n\t        return img, labels\n\t    @property\n\t    def classes(self) -> List[str]:\n\t        \"\"\"\n\t        @return: The class labels as list.\n\t        \"\"\"\n\t        return self.labels\n", "    @property\n\t    def num_classes(self) -> int:\n\t        \"\"\"\n\t        @return: The number of classes.\n\t        \"\"\"\n\t        return len(self.classes)\n\t    @property\n\t    def is_multi_label(self) -> bool:\n\t        \"\"\"\n\t        @return: If the dataset represents a multiple labels at once.\n", "        \"\"\"\n\t        return (self.meta_data[self.labels].sum(axis=1) > 1).any()\n\t    @property\n\t    def class_weight(self) -> Tensor:\n\t        \"\"\"\n\t        Compute the class weights for the dataset.\n\t        @return: The class weights as tensor.\n\t        \"\"\"\n\t        logger.debug(f'Compute class weights with {self.num_classes} classes: {\", \".join(self.classes)}')\n\t        logger.debug(f'Sum of samples per class:\\n{self.meta_data[self.labels].sum(axis=0)}')\n", "        if self.is_multi_label:\n\t            y = self.meta_data[self.labels].to_numpy()\n\t            class_weight = np.empty([self.num_classes, 2])\n\t            for idx in range(self.num_classes):\n\t                class_weight[idx] = compute_class_weight(class_weight='balanced', classes=[0., 1.], y=y[:, idx])\n\t            class_weight = np.mean(class_weight, axis=1)\n\t        else:\n\t            y = np.argmax(self.meta_data[self.labels].to_numpy(), axis=1)\n\t            class_weight = compute_class_weight(class_weight='balanced', classes=range(self.num_classes), y=y)\n\t        class_weights_per_label = {self.labels[idx]: class_weight[idx] for idx in range(len(self.labels))}\n", "        logger.debug(f'Class-Weights for each Label={json.dumps(class_weights_per_label, indent=3)}')\n\t        return torch.from_numpy(class_weight)\n"]}
{"filename": "lib/datasets/image_dataset.py", "chunked_list": ["import os\n\timport numpy as np\n\timport torchvision.transforms as transforms\n\tfrom PIL import Image\n\tfrom torch.utils.data import Dataset\n\tfrom lib.utils import glob_dir\n\tclass ImageDataset(Dataset):\n\t    \"\"\"\n\t    An ImageDataset manages the access to images in a folder structure of train, val, and test.\n\t    \"\"\"\n", "    def __init__(self, root, transforms_=None, mode=\"train\"):\n\t        \"\"\"\n\t        Create a new ImageDataset.\n\t        @param root: The root folder of the dataset.\n\t        @param transforms_: The transformers to be applied to the image before the images are handed over.\n\t        @param mode: The mode either \"train\", \"val\", or \"test\".\n\t        \"\"\"\n\t        # Set True if training data shall be mirrored for augmentation purposes:\n\t        self._MIRROR_IMAGES = False\n\t        self.transform = transforms.Compose(transforms_)\n", "        self.files = sorted(glob_dir(os.path.join(root, mode)))\n\t        self.files.extend(sorted(glob_dir(os.path.join(root, mode))))\n\t        if mode == \"train\":\n\t            self.files.extend(sorted(glob_dir(os.path.join(root, \"test\"))))\n\t            self.files.extend(sorted(glob_dir(os.path.join(root, \"test\"))))\n\t    def __getitem__(self, index):\n\t        file_path = self.files[index % len(self.files)]\n\t        img = Image.open(file_path)\n\t        w, h = img.size\n\t        img_A = img.crop((0, 0, int(w / 2), h))\n", "        img_B = img.crop((int(w / 2), 0, w, h))\n\t        if self._MIRROR_IMAGES:\n\t            if np.random.random() < 0.5:\n\t                img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n\t                img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\t        img_A = self.transform(img_A)\n\t        img_B = self.transform(img_B)\n\t        return {\"A\": img_A, \"B\": img_B, \"Filepath\": file_path}\n\t    def __len__(self):\n\t        return len(self.files)\n"]}
{"filename": "lib/datasets/__init__.py", "chunked_list": ["from enum import Enum\n\tclass DatasetSplit(Enum):\n\t    \"\"\"\n\t    The split for a dataset and their specific folders.\n\t    \"\"\"\n\t    TRAIN = {'value': 0, 'folder': 'train'}\n\t    VALIDATION = {'value': 1, 'folder': 'val'}\n\t    TEST = {'value': 2, 'folder': 'test'}\n"]}
{"filename": "lib/datasets/label_extractor.py", "chunked_list": ["import os\n\timport pathlib\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import Optional\n\timport pandas as pd\n\tfrom loguru import logger\n\tfrom tqdm import tqdm\n\t# i.e. 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise).\n\tfrom lib.utils import glob_dir\n\tSEED = 42\n", "label_map = {\n\t    # Neutral\n\t    0: 0,\n\t    # Angry\n\t    1: 6,\n\t    # Contempt\n\t    2: 7,\n\t    # Disgust\n\t    3: 5,\n\t    # Fear\n", "    4: 4,\n\t    # Happy\n\t    5: 1,\n\t    # Sad\n\t    6: 2,\n\t    # Surprise\n\t    7: 3\n\t}\n\tclass LabelExtractor(ABC):\n\t    def __init__(self, dataset_dir: str):\n", "        self.dataset_dir = dataset_dir\n\t    @abstractmethod\n\t    def __call__(self) -> pd.DataFrame:\n\t        pass\n\tclass CelebALabelExtractor(LabelExtractor):\n\t    def __init__(self, dataset_dir: str):\n\t        super().__init__(os.path.join(dataset_dir, 'celeba'))\n\t    def __call__(self) -> pd.DataFrame:\n\t        df = pd.read_csv(os.path.join(self.dataset_dir, 'list_eval_partition.txt'), delimiter=' ',\n\t                         header=None)\n", "        df = df.rename(columns={0: 'image_path', 1: 'split'})\n\t        df = df.merge(\n\t            pd.read_csv(os.path.join(self.dataset_dir, 'list_attr_celeba.txt'), delimiter='\\s+',\n\t                        skiprows=1).rename_axis('image_path').reset_index(), on='image_path', how='left')\n\t        labels = list(df.keys())[2:]\n\t        df[labels] = (df[labels] + 1) / 2\n\t        return df\n\tclass CKPlusLabelExtractor(LabelExtractor):\n\t    def __init__(self, dataset_dir: str):\n\t        super().__init__(dataset_dir)\n", "    def __call__(self) -> pd.DataFrame:\n\t        label_base_dir = os.path.join(self.dataset_dir, 'Emotion')\n\t        image_base_dir = os.path.join(self.dataset_dir, 'cohn-kanade-images')\n\t        val_split = 0.2\n\t        result = {\n\t            'subject_id': [],\n\t            'image_path': [],\n\t            'anger': [],\n\t            'contempt': [],\n\t            'disgust': [],\n", "            'fear': [],\n\t            'happy': [],\n\t            'sadness': [],\n\t            'surprise': [],\n\t        }\n\t        labels = list(result.keys())[2:]\n\t        for file in tqdm(glob_dir(image_base_dir)):\n\t            folder_structure = file.replace(image_base_dir, '').split(os.sep)\n\t            img_path = os.path.join(*folder_structure[:-1], os.path.basename(file))\n\t            labels_path = os.path.join(label_base_dir, *folder_structure[:-1])\n", "            label_path = os.path.join(labels_path, f'{pathlib.Path(file).stem}_emotion.txt')\n\t            # creating on hot encoding\n\t            if os.path.exists(label_path):\n\t                with open(label_path, 'r') as label_file:\n\t                    mapped_idx = int(float(label_file.readline().strip()))\n\t                    sid = folder_structure[-3]\n\t                    result['subject_id'].append(sid)\n\t                    result['image_path'].append(img_path)\n\t                    for idx, key in enumerate(labels):\n\t                        result[key].append(1 if idx == mapped_idx - 1 else 0)\n", "        df = pd.DataFrame(result)\n\t        df_grouped = df.groupby(['subject_id'])[labels].sum().sample(frac=val_split, random_state=SEED)\n\t        df_train = df.loc[~df['subject_id'].isin(df_grouped.index)]\n\t        df_train['split'] = [0] * len(df_train)\n\t        df_val = df.loc[df['subject_id'].isin(df_grouped.index)]\n\t        df_val['split'] = [1] * len(df_val)\n\t        return pd.concat([df_train, df_val]).drop('subject_id', axis=1)\n\tclass AffectNetLabelExtractor(LabelExtractor):\n\t    def __init__(self, dataset_dir: str):\n\t        super().__init__(dataset_dir)\n", "    def __call__(self) -> pd.DataFrame:\n\t        label_base_dir = os.path.join(self.dataset_dir, 'Manually_Annotated_file_lists')\n\t        result = self.parse_csv_images(os.path.join(label_base_dir, 'training.csv'), 0)\n\t        result.update(self.parse_csv_images(os.path.join(label_base_dir, 'validation.csv'), 1))\n\t        df = pd.DataFrame(result)\n\t        return df\n\t    def parse_csv_images(self, csv_file: str, split: int):\n\t        result = {\n\t            'image_path': [],\n\t            'neutral': [],\n", "            'anger': [],\n\t            'contempt': [],\n\t            'disgust': [],\n\t            'fear': [],\n\t            'happy': [],\n\t            'sadness': [],\n\t            'surprise': [],\n\t            'split': [],\n\t        }\n\t        df = pd.read_csv(csv_file)\n", "        for index, row in tqdm(df.iterrows()):\n\t            label = row['expression']\n\t            if label > 7:\n\t                continue\n\t            result['image_path'].append(row['#subDirectory_filePath'])\n\t            for idx, key in enumerate(list(result.keys())[2:-1]):\n\t                result[key].append(1 if idx == label else 0)\n\t            result['split'].append(split)\n\t        return result\n\tclass FacesLabelExtractor(LabelExtractor):\n", "    def __init__(self, dataset_dir: str):\n\t        super().__init__(dataset_dir)\n\t    def __call__(self) -> pd.DataFrame:\n\t        emotion_map = {\n\t            'n': 0,  # Neutrality\n\t            'h': 1,  # Happy\n\t            's': 2,  # Sadness\n\t            'f': 3,  # Fear\n\t            'd': 4,  # Disgust\n\t            'a': 5,  # Anger\n", "        }\n\t        result = {\n\t            'image_path': [],\n\t            'subject_id': [],\n\t            'neutral': [],\n\t            'happy': [],\n\t            'sadness': [],\n\t            'fear': [],\n\t            'disgust': [],\n\t            'anger': [],\n", "        }\n\t        val_set = ['004', '066', '079', '116', '140', '168']  # Subject photos officially usable for publications\n\t        val_split = 0.2\n\t        labels = list(result.keys())[2:]\n\t        for file in tqdm(glob_dir(os.path.join(self.dataset_dir, 'bilder'))):\n\t            file_split = pathlib.Path(file).stem.split('_')\n\t            subject_id = file_split[0]\n\t            emotion = file_split[3]\n\t            result['image_path'].append(os.path.basename(file))\n\t            result['subject_id'].append(subject_id)\n", "            mapped_idx = emotion_map[emotion]\n\t            for idx, key in enumerate(labels):\n\t                result[key].append(1 if idx == mapped_idx else 0)\n\t        df = pd.DataFrame(result)\n\t        df_grouped = df.drop(val_set).groupby(['subject_id'])[list(result.keys())[2:]].sum().sample(frac=val_split)\n\t        df_train = df[~df['subject_id'].isin(df_grouped.index)]\n\t        df_train['split'] = [0] * len(df_train)\n\t        df_val = df[df['subject_id'].isin(df_grouped.index)]\n\t        df_val['split'] = [1] * len(df_val)\n\t        return pd.concat([df_train, df_val]).drop('subject_id', axis=1)\n", "label_extractors = {\n\t    'CelebA': CelebALabelExtractor,\n\t    'CK+': CKPlusLabelExtractor,\n\t    'AffectNet': AffectNetLabelExtractor,\n\t    'FACES': FacesLabelExtractor,\n\t}\n\tdef extract_labels(dataset_path: str, output_path: str) -> Optional[str]:\n\t    \"\"\"\n\t    Extract the labels from the given dataset and bring it in a uniform format.\n\t    @param dataset_path: The path to the dataset.\n", "    @param output_path: The path to save the results.\n\t    @return: The path to the csv file or None if no extractor could be found.\n\t    \"\"\"\n\t    path_parts = dataset_path.split(os.sep)\n\t    for dataset_name in path_parts:\n\t        if dataset_name in label_extractors:\n\t            dataset_path = dataset_path[:dataset_path.index(dataset_name) + len(dataset_name)]\n\t            os.makedirs(output_path, exist_ok=True)\n\t            logger.debug(f'Extracting labels for {dataset_name}...')\n\t            # csv_file =\n", "            extractor_class = label_extractors[dataset_name]\n\t            extractor = extractor_class(dataset_path)\n\t            df = extractor()\n\t            csv_file = os.path.join(output_path, 'labels.csv')\n\t            df.to_csv(csv_file)\n\t            logger.info(f'Extracted labels from {dataset_name} to {output_path}')\n\t            df_anaylser = df.drop(['image_path'], axis=1)\n\t            logger.debug(f'Meta-Data:\\n{df_anaylser.drop([\"split\"], axis=1).sum()}')\n\t            logger.debug(f'Train-Split:\\n{df_anaylser[df_anaylser[\"split\"] == 0].drop([\"split\"], axis=1).sum()}')\n\t            logger.debug(f'Validation-Split:\\n{df_anaylser[df_anaylser[\"split\"] == 1].drop([\"split\"], axis=1).sum()}')\n", "            return csv_file\n\t    logger.warning(f'No label extractor found for {dataset_path}')\n\t    return None\n"]}
{"filename": "lib/datasets/unlabeled_dataset.py", "chunked_list": ["import os\n\tfrom typing import Tuple\n\timport numpy as np\n\timport torch\n\tfrom PIL import Image\n\tfrom loguru import logger\n\tfrom torch import Tensor\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision.transforms import Compose\n\tfrom lib.datasets import DatasetSplit\n", "from lib.utils import glob_dir\n\tclass UnlabeledDataset(torch.utils.data.Dataset):\n\t    \"\"\"\n\t    The LabeledDataset wraps a folder structure of a dataset with train, val, and test folders\n\t    and the corresponding labels.csv with the meta information.\n\t    \"\"\"\n\t    def __init__(self, data_dir: str, split: DatasetSplit, transforms: Compose):\n\t        \"\"\"\n\t        Create a new LabeledDataset.\n\t        @param data_dir: The root directory of the dataset.\n", "        @param split: Which dataset split should be considered.\n\t        @param transforms: The transformations that are applied to the images before returning.\n\t        \"\"\"\n\t        data_dir = os.path.join(data_dir, split.value['folder'])\n\t        logger.debug(f'Setup LabeledDataset on path: {data_dir}')\n\t        # Search for all files available in the filesystem\n\t        self.files_found = glob_dir(data_dir)\n\t        self.transforms = transforms\n\t        logger.info(f'Finished \"{os.path.basename(data_dir)}\" Dataset (total={len(self)}) Setup!')\n\t    def __len__(self) -> int:\n", "        return len(self.files_found)\n\t    def __getitem__(self, idx) -> Tuple[Tensor, np.ndarray]:\n\t        img = Image.open(self.files_found[idx])\n\t        img = self.transforms(img)\n\t        return img, np.asarray([-1])\n"]}
