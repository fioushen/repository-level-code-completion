{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\tdef get_requirements():\n\t    with open(\"requirements.txt\") as f:\n\t        return f.read().splitlines()\n\tdef get_long_description():\n\t    with open(\"README.rst\", encoding=\"utf-8\") as f:\n\t        return f.read()\n\tsetup(\n\t    name=\"typedspark\",\n\t    url=\"https://github.com/kaiko-ai/typedspark\",\n", "    license=\"Apache-2.0\",\n\t    author=\"Nanne Aben\",\n\t    author_email=\"nanne@kaiko.ai\",\n\t    description=\"Column-wise type annotations for pyspark DataFrames\",\n\t    keywords=\"pyspark spark typing type checking annotations\",\n\t    long_description=get_long_description(),\n\t    long_description_content_type=\"text/x-rst\",\n\t    packages=find_packages(include=[\"typedspark\", \"typedspark.*\"]),\n\t    install_requires=get_requirements(),\n\t    python_requires=\">=3.9.0\",\n", "    classifiers=[\"Programming Language :: Python\", \"Typing :: Typed\"],\n\t    version_config=True,\n\t    setup_requires=[\"setuptools-git-versioning\"],\n\t    package_data={\"typedspark\": [\"py.typed\"]},\n\t    extras_require={\n\t        \"pyspark\": [\"pyspark\"],\n\t    },\n\t)\n"]}
{"filename": "typedspark/__init__.py", "chunked_list": ["\"\"\"Typedspark: column-wise type annotations for pyspark DataFrames.\"\"\"\n\tfrom typedspark._core.column import Column\n\tfrom typedspark._core.column_meta import ColumnMeta\n\tfrom typedspark._core.dataset import DataSet\n\tfrom typedspark._core.datatypes import (\n\t    ArrayType,\n\t    DayTimeIntervalType,\n\t    DecimalType,\n\t    MapType,\n\t    StructType,\n", ")\n\tfrom typedspark._core.literaltype import IntervalType\n\tfrom typedspark._schema.schema import MetaSchema, Schema\n\tfrom typedspark._transforms.structtype_column import structtype_column\n\tfrom typedspark._transforms.transform_to_schema import transform_to_schema\n\tfrom typedspark._utils.create_dataset import (\n\t    create_empty_dataset,\n\t    create_partially_filled_dataset,\n\t    create_structtype_row,\n\t)\n", "from typedspark._utils.databases import Catalogs, Database, Databases\n\tfrom typedspark._utils.load_table import create_schema, load_table\n\tfrom typedspark._utils.register_schema_to_dataset import register_schema_to_dataset\n\t__all__ = [\n\t    \"ArrayType\",\n\t    \"Catalogs\",\n\t    \"Column\",\n\t    \"ColumnMeta\",\n\t    \"Database\",\n\t    \"Databases\",\n", "    \"DataSet\",\n\t    \"DayTimeIntervalType\",\n\t    \"DecimalType\",\n\t    \"IntervalType\",\n\t    \"MapType\",\n\t    \"MetaSchema\",\n\t    \"Schema\",\n\t    \"StructType\",\n\t    \"create_empty_dataset\",\n\t    \"create_partially_filled_dataset\",\n", "    \"create_structtype_row\",\n\t    \"create_schema\",\n\t    \"load_table\",\n\t    \"register_schema_to_dataset\",\n\t    \"structtype_column\",\n\t    \"transform_to_schema\",\n\t]\n"]}
{"filename": "typedspark/_transforms/structtype_column.py", "chunked_list": ["\"\"\"Functionality for dealing with StructType columns.\"\"\"\n\tfrom typing import Dict, Optional, Type\n\tfrom pyspark.sql import Column as SparkColumn\n\tfrom pyspark.sql.functions import struct\n\tfrom typedspark._core.column import Column\n\tfrom typedspark._schema.schema import Schema\n\tfrom typedspark._transforms.utils import add_nulls_for_unspecified_columns, convert_keys_to_strings\n\tdef structtype_column(\n\t    schema: Type[Schema],\n\t    transformations: Optional[Dict[Column, SparkColumn]] = None,\n", "    fill_unspecified_columns_with_nulls: bool = False,\n\t) -> SparkColumn:\n\t    \"\"\"Helps with creating new ``StructType`` columns of a certain schema, for\n\t    example:\n\t    .. code-block:: python\n\t        transform_to_schema(\n\t            df,\n\t            Output,\n\t            {\n\t                Output.values: structtype_column(\n", "                    Value,\n\t                    {\n\t                        Value.a: Input.a + 2,\n\t                        ...\n\t                    }\n\t                )\n\t            }\n\t        )\n\t    \"\"\"\n\t    _transformations = convert_keys_to_strings(transformations)\n", "    if fill_unspecified_columns_with_nulls:\n\t        _transformations = add_nulls_for_unspecified_columns(_transformations, schema)\n\t    _transformations = _order_columns(_transformations, schema)\n\t    return struct([v.alias(k) for k, v in _transformations.items()])\n\tdef _order_columns(\n\t    transformations: Dict[str, SparkColumn], schema: Type[Schema]\n\t) -> Dict[str, SparkColumn]:\n\t    \"\"\"Chispa's DataFrame comparer doesn't deal nicely with StructTypes whose columns\n\t    are ordered differently, hence we order them the same as in the schema here.\"\"\"\n\t    transformations_ordered = {}\n", "    for field in schema.get_structtype().fields:\n\t        transformations_ordered[field.name] = transformations[field.name]\n\t    return transformations_ordered\n"]}
{"filename": "typedspark/_transforms/__init__.py", "chunked_list": []}
{"filename": "typedspark/_transforms/utils.py", "chunked_list": ["\"\"\"Util functions for typedspark._transforms.\"\"\"\n\tfrom typing import Dict, List, Optional, Type\n\tfrom pyspark.sql import Column as SparkColumn\n\tfrom pyspark.sql.functions import lit\n\tfrom typedspark._core.column import Column\n\tfrom typedspark._schema.schema import Schema\n\tdef add_nulls_for_unspecified_columns(\n\t    transformations: Dict[str, SparkColumn],\n\t    schema: Type[Schema],\n\t    previously_existing_columns: Optional[List[str]] = None,\n", ") -> Dict[str, SparkColumn]:\n\t    \"\"\"Takes the columns from the schema that are not present in the transformation\n\t    dictionary and sets their values to Null (casted to the corresponding type defined\n\t    in the schema).\"\"\"\n\t    _previously_existing_columns = (\n\t        [] if previously_existing_columns is None else previously_existing_columns\n\t    )\n\t    for field in schema.get_structtype().fields:\n\t        if field.name not in transformations and field.name not in _previously_existing_columns:\n\t            transformations[field.name] = lit(None).cast(field.dataType)\n", "    return transformations\n\tdef convert_keys_to_strings(\n\t    transformations: Optional[Dict[Column, SparkColumn]]\n\t) -> Dict[str, SparkColumn]:\n\t    \"\"\"Takes the Column keys in transformations and converts them to strings.\"\"\"\n\t    if transformations is None:\n\t        return {}\n\t    _transformations = {k.str: v for k, v in transformations.items()}\n\t    if len(transformations) != len(_transformations):\n\t        raise ValueError(\n", "            \"The transformations dictionary requires columns with unique names as keys.\"\n\t            + \"It is currently not possible to have ambiguous column names here,\"\n\t            + \"even when used in combination with register_schema_to_dataset().\"\n\t        )\n\t    return _transformations\n"]}
{"filename": "typedspark/_transforms/transform_to_schema.py", "chunked_list": ["\"\"\"Module containing functions that are related to transformations to DataSets.\"\"\"\n\tfrom functools import reduce\n\tfrom typing import Dict, Optional, Type, TypeVar\n\tfrom pyspark.sql import Column as SparkColumn\n\tfrom pyspark.sql import DataFrame\n\tfrom typedspark._core.column import Column\n\tfrom typedspark._core.dataset import DataSet\n\tfrom typedspark._schema.schema import Schema\n\tfrom typedspark._transforms.utils import add_nulls_for_unspecified_columns, convert_keys_to_strings\n\tT = TypeVar(\"T\", bound=Schema)\n", "def transform_to_schema(\n\t    dataframe: DataFrame,\n\t    schema: Type[T],\n\t    transformations: Optional[Dict[Column, SparkColumn]] = None,\n\t    fill_unspecified_columns_with_nulls: bool = False,\n\t) -> DataSet[T]:\n\t    \"\"\"On the provided DataFrame ``df``, it performs the ``transformations``\n\t    (if provided), and subsequently subsets the resulting DataFrame to the\n\t    columns specified in ``schema``.\n\t    .. code-block:: python\n", "        transform_to_schema(\n\t            df_a.join(df_b, A.a == B.f),\n\t            AB,\n\t            {\n\t                AB.a: A.a + 3,\n\t                AB.b: A.b + 7,\n\t                AB.i: B.i - 5,\n\t                AB.j: B.j + 1,\n\t            }\n\t        )\n", "    \"\"\"\n\t    _transformations = convert_keys_to_strings(transformations)\n\t    if fill_unspecified_columns_with_nulls:\n\t        _transformations = add_nulls_for_unspecified_columns(\n\t            _transformations, schema, previously_existing_columns=dataframe.columns\n\t        )\n\t    return DataSet[schema](  # type: ignore\n\t        reduce(\n\t            lambda acc, key: DataFrame.withColumn(acc, key, _transformations[key]),\n\t            _transformations.keys(),\n", "            dataframe,\n\t        ).select(*schema.all_column_names())\n\t    )\n"]}
{"filename": "typedspark/_utils/databases.py", "chunked_list": ["\"\"\"Loads all catalogs, databases and tables in a SparkSession.\"\"\"\n\tfrom abc import ABC\n\tfrom datetime import datetime\n\tfrom typing import Optional, Tuple, TypeVar\n\tfrom warnings import warn\n\tfrom pyspark.sql import Row, SparkSession\n\tfrom typedspark._core.dataset import DataSet\n\tfrom typedspark._schema.schema import Schema\n\tfrom typedspark._utils.camelcase import to_camel_case\n\tfrom typedspark._utils.load_table import load_table\n", "T = TypeVar(\"T\", bound=Schema)\n\tclass Timeout(ABC):\n\t    \"\"\"Warns the user if loading databases or catalogs is taking too long.\"\"\"\n\t    _TIMEOUT_WARNING: str\n\t    def __init__(self, silent: bool, n: int):  # pylint: disable=invalid-name\n\t        self._start = datetime.now()\n\t        self._silent = silent\n\t        self._n = n\n\t    def check_for_warning(self, i: int):  # pragma: no cover\n\t        \"\"\"Checks if a warning should be issued.\"\"\"\n", "        if self._silent:\n\t            return\n\t        diff = datetime.now() - self._start\n\t        if diff.seconds > 10:\n\t            warn(self._TIMEOUT_WARNING.format(i, self._n))\n\t            self._silent = True\n\tclass DatabasesTimeout(Timeout):\n\t    \"\"\"Warns the user if Databases() is taking too long.\"\"\"\n\t    _TIMEOUT_WARNING = \"\"\"\n\tDatabases() is taking longer than 10 seconds. So far, {} out of {} databases have been loaded.\n", "If this is too slow, consider loading a single database using:\n\tfrom typedspark import Database\n\tdb = Database(spark, db_name=...)\n\t\"\"\"\n\tclass CatalogsTimeout(Timeout):\n\t    \"\"\"Warns the user if Catalogs() is taking too long.\"\"\"\n\t    _TIMEOUT_WARNING = \"\"\"\n\tCatalogs() is taking longer than 10 seconds. So far, {} out of {} catalogs have been loaded.\n\tIf this is too slow, consider loading a single catalog using:\n\tfrom typedspark import Databases\n", "db = Databases(spark, catalog_name=...)\n\t\"\"\"\n\tclass Table:\n\t    \"\"\"Loads a table in a database.\"\"\"\n\t    def __init__(self, spark: SparkSession, db_name: str, table_name: str, is_temporary: bool):\n\t        self._spark = spark\n\t        self._db_name = db_name\n\t        self._table_name = table_name\n\t        self._is_temporary = is_temporary\n\t    @property\n", "    def str(self) -> str:\n\t        \"\"\"Returns the path to the table, e.g. ``default.person``.\n\t        While temporary tables are always stored in the ``default`` db, they are saved and\n\t        loaded directly from their table name, e.g. ``person``.\n\t        Non-temporary tables are saved and loaded from their full name, e.g.\n\t        ``default.person``.\n\t        \"\"\"\n\t        if self._is_temporary:\n\t            return self._table_name\n\t        return f\"{self._db_name}.{self._table_name}\"\n", "    def load(self) -> Tuple[DataSet[T], T]:\n\t        \"\"\"Loads the table as a DataSet[T] and returns the schema.\"\"\"\n\t        return load_table(  # type: ignore\n\t            self._spark,\n\t            self.str,\n\t            to_camel_case(self._table_name),\n\t        )\n\tclass Database:\n\t    \"\"\"Loads all tables in a database.\"\"\"\n\t    def __init__(self, spark: SparkSession, db_name: str, catalog_name: Optional[str] = None):\n", "        if catalog_name is None:\n\t            self._db_name = db_name\n\t        else:\n\t            self._db_name = f\"{catalog_name}.{db_name}\"\n\t        tables = spark.sql(f\"show tables from {self._db_name}\").collect()\n\t        for table in tables:\n\t            table_name = table.tableName\n\t            self.__setattr__(\n\t                table_name,\n\t                Table(spark, self._db_name, table_name, table.isTemporary),\n", "            )\n\t    @property\n\t    def str(self) -> str:\n\t        \"\"\"Returns the database name.\"\"\"\n\t        return self._db_name\n\tclass Databases:\n\t    \"\"\"Loads all databases and tables in a SparkSession.\"\"\"\n\t    def __init__(\n\t        self, spark: SparkSession, silent: bool = False, catalog_name: Optional[str] = None\n\t    ):\n", "        if catalog_name is None:\n\t            query = \"show databases\"\n\t        else:\n\t            query = f\"show databases in {catalog_name}\"\n\t        databases = spark.sql(query).collect()\n\t        timeout = DatabasesTimeout(silent, n=len(databases))\n\t        for i, database in enumerate(databases):\n\t            timeout.check_for_warning(i)\n\t            db_name = self._extract_db_name(database)\n\t            self.__setattr__(db_name, Database(spark, db_name, catalog_name))\n", "    def _extract_db_name(self, database: Row) -> str:\n\t        \"\"\"Extracts the database name from a Row.\n\t        Old versions of Spark use ``databaseName``, newer versions use ``namespace``.\n\t        \"\"\"\n\t        if hasattr(database, \"databaseName\"):  # pragma: no cover\n\t            return database.databaseName\n\t        if hasattr(database, \"namespace\"):\n\t            return database.namespace\n\t        raise ValueError(f\"Could not find database name in {database}.\")  # pragma: no cover\n\tclass Catalogs:\n", "    \"\"\"Loads all catalogs, databases and tables in a SparkSession.\"\"\"\n\t    def __init__(self, spark: SparkSession, silent: bool = False):\n\t        catalogs = spark.sql(\"show catalogs\").collect()\n\t        timeout = CatalogsTimeout(silent, n=len(catalogs))\n\t        for i, catalog in enumerate(catalogs):\n\t            timeout.check_for_warning(i)\n\t            catalog_name: str = catalog.catalog\n\t            self.__setattr__(\n\t                catalog_name,\n\t                Databases(spark, silent=True, catalog_name=catalog_name),\n", "            )\n"]}
{"filename": "typedspark/_utils/camelcase.py", "chunked_list": ["\"\"\"Utility function for converting from snake case to camel case.\"\"\"\n\tdef to_camel_case(name: str) -> str:\n\t    \"\"\"Converts a string to camel case.\"\"\"\n\t    return \"\".join([word.capitalize() for word in name.split(\"_\")])\n"]}
{"filename": "typedspark/_utils/load_table.py", "chunked_list": ["\"\"\"Functions for loading `DataSet` and `Schema` in notebooks.\"\"\"\n\timport re\n\tfrom typing import Dict, Literal, Optional, Tuple, Type\n\tfrom pyspark.sql import DataFrame, SparkSession\n\tfrom pyspark.sql.types import ArrayType as SparkArrayType\n\tfrom pyspark.sql.types import DataType\n\tfrom pyspark.sql.types import DayTimeIntervalType as SparkDayTimeIntervalType\n\tfrom pyspark.sql.types import DecimalType as SparkDecimalType\n\tfrom pyspark.sql.types import MapType as SparkMapType\n\tfrom pyspark.sql.types import StructType as SparkStructType\n", "from typedspark._core.column import Column\n\tfrom typedspark._core.dataset import DataSet\n\tfrom typedspark._core.datatypes import (\n\t    ArrayType,\n\t    DayTimeIntervalType,\n\t    DecimalType,\n\t    MapType,\n\t    StructType,\n\t)\n\tfrom typedspark._schema.schema import MetaSchema, Schema\n", "from typedspark._utils.camelcase import to_camel_case\n\tfrom typedspark._utils.register_schema_to_dataset import register_schema_to_dataset\n\tdef _replace_illegal_column_names(dataframe: DataFrame) -> DataFrame:\n\t    \"\"\"Replaces illegal column names with a legal version.\"\"\"\n\t    mapping = _create_mapping(dataframe)\n\t    for column, column_renamed in mapping.items():\n\t        if column != column_renamed:\n\t            dataframe = dataframe.withColumnRenamed(column, column_renamed)\n\t    return dataframe\n\tdef _create_mapping(dataframe: DataFrame) -> Dict[str, str]:\n", "    \"\"\"Checks if there are duplicate columns after replacing illegal characters.\"\"\"\n\t    mapping = {column: _replace_illegal_characters(column) for column in dataframe.columns}\n\t    renamed_columns = list(mapping.values())\n\t    duplicates = {\n\t        column: column_renamed\n\t        for column, column_renamed in mapping.items()\n\t        if renamed_columns.count(column_renamed) > 1\n\t    }\n\t    if len(duplicates) > 0:\n\t        raise ValueError(\n", "            \"You're trying to dynamically generate a Schema from a DataFrame. \"\n\t            + \"However, typedspark has detected that the DataFrame contains duplicate columns \"\n\t            + \"after replacing illegal characters (e.g. whitespaces, dots, etc.).\\n\"\n\t            + \"The folowing columns have lead to duplicates:\\n\"\n\t            + f\"{duplicates}\\n\\n\"\n\t            + \"Please rename these columns in your DataFrame.\"\n\t        )\n\t    return mapping\n\tdef _replace_illegal_characters(column_name: str) -> str:\n\t    \"\"\"Replaces illegal characters in a column name with an underscore.\"\"\"\n", "    return re.sub(\"[^A-Za-z0-9]\", \"_\", column_name)\n\tdef _create_schema(structtype: SparkStructType, schema_name: Optional[str] = None) -> Type[Schema]:\n\t    \"\"\"Dynamically builds a ``Schema`` based on a ``DataFrame``'s\n\t    ``StructType``\"\"\"\n\t    type_annotations = {}\n\t    attributes: Dict[str, None] = {}\n\t    for column in structtype:\n\t        name = column.name\n\t        data_type = _extract_data_type(column.dataType, name)\n\t        type_annotations[name] = Column[data_type]  # type: ignore\n", "        attributes[name] = None\n\t    if not schema_name:\n\t        schema_name = \"DynamicallyLoadedSchema\"\n\t    schema = MetaSchema(schema_name, tuple([Schema]), attributes)\n\t    schema.__annotations__ = type_annotations\n\t    return schema  # type: ignore\n\tdef _extract_data_type(dtype: DataType, name: str) -> Type[DataType]:\n\t    \"\"\"Given an instance of a ``DataType``, it extracts the corresponding\n\t    ``DataType`` class, potentially including annotations (e.g.\n\t    ``ArrayType[StringType]``).\"\"\"\n", "    if isinstance(dtype, SparkArrayType):\n\t        element_type = _extract_data_type(dtype.elementType, name)\n\t        return ArrayType[element_type]  # type: ignore\n\t    if isinstance(dtype, SparkMapType):\n\t        key_type = _extract_data_type(dtype.keyType, name)\n\t        value_type = _extract_data_type(dtype.valueType, name)\n\t        return MapType[key_type, value_type]  # type: ignore\n\t    if isinstance(dtype, SparkStructType):\n\t        subschema = _create_schema(dtype, to_camel_case(name))\n\t        return StructType[subschema]  # type: ignore\n", "    if isinstance(dtype, SparkDayTimeIntervalType):\n\t        start_field = dtype.startField\n\t        end_field = dtype.endField\n\t        return DayTimeIntervalType[Literal[start_field], Literal[end_field]]  # type: ignore\n\t    if isinstance(dtype, SparkDecimalType):\n\t        precision = dtype.precision\n\t        scale = dtype.scale\n\t        return DecimalType[Literal[precision], Literal[scale]]  # type: ignore\n\t    return type(dtype)\n\tdef create_schema(\n", "    dataframe: DataFrame, schema_name: Optional[str] = None\n\t) -> Tuple[DataSet[Schema], Type[Schema]]:\n\t    \"\"\"This function inferres a ``Schema`` in a notebook based on a the provided ``DataFrame``.\n\t    This allows for autocompletion on column names, amongst other\n\t    things.\n\t    .. code-block:: python\n\t        df, Person = create_schema(df)\n\t    \"\"\"\n\t    dataframe = _replace_illegal_column_names(dataframe)\n\t    schema = _create_schema(dataframe.schema, schema_name)\n", "    dataset = DataSet[schema](dataframe)  # type: ignore\n\t    schema = register_schema_to_dataset(dataset, schema)\n\t    return dataset, schema\n\tdef load_table(\n\t    spark: SparkSession, table_name: str, schema_name: Optional[str] = None\n\t) -> Tuple[DataSet[Schema], Type[Schema]]:\n\t    \"\"\"This function loads a ``DataSet``, along with its inferred ``Schema``,\n\t    in a notebook.\n\t    This allows for autocompletion on column names, amongst other\n\t    things.\n", "    .. code-block:: python\n\t        df, Person = load_table(spark, \"path.to.table\")\n\t    \"\"\"\n\t    dataframe = spark.table(table_name)\n\t    return create_schema(dataframe, schema_name)\n"]}
{"filename": "typedspark/_utils/__init__.py", "chunked_list": []}
{"filename": "typedspark/_utils/create_dataset.py", "chunked_list": ["\"\"\"Module containing functions related to creating a DataSet from scratch.\"\"\"\n\tfrom typing import Any, Dict, List, Type, TypeVar, Union, get_type_hints\n\tfrom pyspark.sql import Row, SparkSession\n\tfrom typedspark._core.column import Column\n\tfrom typedspark._core.dataset import DataSet\n\tfrom typedspark._schema.schema import Schema\n\tT = TypeVar(\"T\", bound=Schema)\n\tdef create_empty_dataset(spark: SparkSession, schema: Type[T], n_rows: int = 3) -> DataSet[T]:\n\t    \"\"\"Creates a ``DataSet`` with ``Schema`` schema, containing ``n_rows``\n\t    rows, filled with ``None`` values.\n", "    .. code-block:: python\n\t        class Person(Schema):\n\t            name: Column[StringType]\n\t            age: Column[LongType]\n\t        df = create_empty_dataset(spark, Person)\n\t    \"\"\"\n\t    n_cols = len(get_type_hints(schema))\n\t    rows = tuple([None] * n_cols)\n\t    data = [rows] * n_rows\n\t    spark_schema = schema.get_structtype()\n", "    dataframe = spark.createDataFrame(data, spark_schema)\n\t    return DataSet[schema](dataframe)  # type: ignore\n\tdef create_partially_filled_dataset(\n\t    spark: SparkSession,\n\t    schema: Type[T],\n\t    data: Union[Dict[Column, List[Any]], List[Dict[Column, Any]]],\n\t) -> DataSet[T]:\n\t    \"\"\"Creates a ``DataSet`` with ``Schema`` schema, where ``data`` can\n\t    be defined in either of the following two ways:\n\t    .. code-block:: python\n", "        class Person(Schema):\n\t            name: Column[StringType]\n\t            age: Column[LongType]\n\t            job: Column[StringType]\n\t        df = create_partially_filled_dataset(\n\t            spark,\n\t            Person,\n\t            {\n\t                Person.name: [\"John\", \"Jack\", \"Jane\"],\n\t                Person.age: [30, 40, 50],\n", "            }\n\t        )\n\t    Or:\n\t    .. code-block:: python\n\t        df = create_partially_filled_dataset(\n\t            spark,\n\t            Person,\n\t            [\n\t                {Person.name: \"John\", Person.age: 30},\n\t                {Person.name: \"Jack\", Person.age: 40},\n", "                {Person.name: \"Jane\", Person.age: 50},\n\t            ]\n\t        )\n\t    Any columns in the schema that are not present in the data will be\n\t    initialized with ``None`` values.\n\t    \"\"\"\n\t    if isinstance(data, list):\n\t        col_data = _create_column_wise_data_from_list(schema, data)\n\t    elif isinstance(data, dict):\n\t        col_data = _create_column_wise_data_from_dict(schema, data)\n", "    else:\n\t        raise ValueError(\"The provided data is not a list or a dict.\")\n\t    row_data = zip(*col_data)\n\t    spark_schema = schema.get_structtype()\n\t    dataframe = spark.createDataFrame(row_data, spark_schema)\n\t    return DataSet[schema](dataframe)  # type: ignore\n\tdef create_structtype_row(schema: Type[T], data: Dict[Column, Any]) -> Row:\n\t    \"\"\"Creates a ``Row`` with ``StructType`` schema, where ``data`` is a\n\t    mapping from column to data in the respective column.\"\"\"\n\t    data_with_string_index = {k.str: v for k, v in data.items()}\n", "    data_converted = {\n\t        k: data_with_string_index[k] if k in data_with_string_index else None\n\t        for k in get_type_hints(schema).keys()\n\t    }\n\t    return Row(**data_converted)\n\tdef _create_column_wise_data_from_dict(\n\t    schema: Type[T], data: Dict[Column, List[Any]]\n\t) -> List[List[Any]]:\n\t    \"\"\"Converts a dict of column to data to a list of lists, where each inner list\n\t    contains the data for a column.\"\"\"\n", "    data_converted = {k.str: v for k, v in data.items()}\n\t    n_rows_unique = {len(v) for _, v in data.items()}\n\t    if len(n_rows_unique) > 1:\n\t        raise ValueError(\"The number of rows in the provided data differs per column.\")\n\t    n_rows = list(n_rows_unique)[0]\n\t    col_data = []\n\t    for col in get_type_hints(schema).keys():\n\t        if col in data_converted:\n\t            col_data += [data_converted[col]]\n\t        else:\n", "            col_data += [[None] * n_rows]\n\t    return col_data\n\tdef _create_column_wise_data_from_list(\n\t    schema: Type[T], data: List[Dict[Column, Any]]\n\t) -> List[List[Any]]:\n\t    \"\"\"Converts a list of dicts of column to data to a list of lists, where each inner\n\t    list contains the data for a column.\"\"\"\n\t    data_converted = [{k.str: v for k, v in row.items()} for row in data]\n\t    col_data = []\n\t    for col in get_type_hints(schema).keys():\n", "        col_data += [[row[col] if col in row else None for row in data_converted]]\n\t    return col_data\n"]}
{"filename": "typedspark/_utils/register_schema_to_dataset.py", "chunked_list": ["\"\"\"Module containing functions that are related to registering schema's to DataSets.\"\"\"\n\timport itertools\n\tfrom typing import Type, TypeVar\n\tfrom typedspark._core.dataset import DataSet\n\tfrom typedspark._schema.schema import Schema\n\tT = TypeVar(\"T\", bound=Schema)\n\tdef _counter(count: itertools.count = itertools.count()):\n\t    return next(count)\n\tdef register_schema_to_dataset(dataframe: DataSet[T], schema: Type[T]) -> Type[T]:\n\t    \"\"\"Helps combat column ambiguity. For example:\n", "    .. code-block:: python\n\t        class Person(Schema):\n\t            id: Column[IntegerType]\n\t            name: Column[StringType]\n\t        class Job(Schema):\n\t            id: Column[IntegerType]\n\t            salary: Column[IntegerType]\n\t        class PersonWithJob(Person, Job):\n\t            pass\n\t        def foo(df_a: DataSet[Person], df_b: DataSet[Job]) -> DataSet[PersonWithJob]:\n", "            return DataSet[PersonWithSalary](\n\t                df_a.join(\n\t                    df_b,\n\t                    Person.id == Job.id\n\t                )\n\t            )\n\t    Calling ``foo()`` would result in a ``AnalysisException``, because Spark can't figure out\n\t    whether ``id`` belongs to ``df_a`` or ``df_b``. To deal with this, you need to register\n\t    your ``Schema`` to the ``DataSet``.\n\t    .. code-block:: python\n", "        from typedspark import register_schema_to_dataset\n\t        def foo(df_a: DataSet[Person], df_b: DataSet[Job]) -> DataSet[PersonWithSalary]:\n\t            person = register_schema_to_dataset(df_a, Person)\n\t            job = register_schema_to_dataset(df_b, Job)\n\t            return DataSet[PersonWithSalary](\n\t                df_a.join(\n\t                    df_b,\n\t                    person.id == job.id\n\t                )\n\t            )\n", "    \"\"\"\n\t    class LinkedSchema(schema):  # type: ignore\n\t        \"\"\"TypedSpark LinkedSchema.\n\t        Contains the DataFrame that this Schema is linked to.\n\t        \"\"\"\n\t        _parent = dataframe\n\t        _current_id = _counter()\n\t        _original_name = schema.get_schema_name()\n\t    return LinkedSchema  # type: ignore\n"]}
{"filename": "typedspark/_core/datatypes.py", "chunked_list": ["\"\"\"Here, we make our own definitions of ``MapType``, ``ArrayType`` and\n\t``StructType`` in order to allow e.g. for ``ArrayType[StringType]``.\"\"\"\n\tfrom __future__ import annotations\n\tfrom typing import TYPE_CHECKING, Any, Dict, Generic, Type, TypeVar\n\tfrom pyspark.sql.types import DataType\n\tif TYPE_CHECKING:  # pragma: no cover\n\t    from typedspark._schema.schema import Schema\n\t    _Schema = TypeVar(\"_Schema\", bound=Schema)\n\telse:\n\t    _Schema = TypeVar(\"_Schema\")\n", "_KeyType = TypeVar(\"_KeyType\", bound=DataType)  # pylint: disable=invalid-name\n\t_ValueType = TypeVar(\"_ValueType\", bound=DataType)  # pylint: disable=invalid-name\n\t_Precision = TypeVar(\"_Precision\", bound=int)  # pylint: disable=invalid-name\n\t_Scale = TypeVar(\"_Scale\", bound=int)  # pylint: disable=invalid-name\n\t_StartField = TypeVar(\"_StartField\", bound=int)  # pylint: disable=invalid-name\n\t_EndField = TypeVar(\"_EndField\", bound=int)  # pylint: disable=invalid-name\n\tclass TypedSparkDataType(DataType):\n\t    \"\"\"Base class for typedspark specific ``DataTypes``.\"\"\"\n\t    @classmethod\n\t    def get_name(cls) -> str:\n", "        \"\"\"Return the name of the type.\"\"\"\n\t        return cls.__name__\n\tclass StructTypeMeta(type):\n\t    \"\"\"Initializes the schema attribute as None.\n\t    This allows for auto-complete in Databricks notebooks (uninitialized variables don't\n\t    show up in auto-complete there).\n\t    \"\"\"\n\t    def __new__(cls, name: str, bases: Any, dct: Dict[str, Any]):\n\t        dct[\"schema\"] = None\n\t        return super().__new__(cls, name, bases, dct)\n", "class StructType(Generic[_Schema], TypedSparkDataType, metaclass=StructTypeMeta):\n\t    \"\"\"Allows for type annotations such as:\n\t    .. code-block:: python\n\t        class Job(Schema):\n\t            position: Column[StringType]\n\t            salary: Column[LongType]\n\t        class Person(Schema):\n\t            job: Column[StructType[Job]]\n\t    \"\"\"\n\t    schema: Type[_Schema]\n", "class MapType(Generic[_KeyType, _ValueType], TypedSparkDataType):\n\t    \"\"\"Allows for type annotations such as.\n\t    .. code-block:: python\n\t        class Basket(Schema):\n\t            items: Column[MapType[StringType, StringType]]\n\t    \"\"\"\n\tclass ArrayType(Generic[_ValueType], TypedSparkDataType):\n\t    \"\"\"Allows for type annotations such as.\n\t    .. code-block:: python\n\t        class Basket(Schema):\n", "            items: Column[ArrayType[StringType]]\n\t    \"\"\"\n\tclass DecimalType(Generic[_Precision, _Scale], TypedSparkDataType):\n\t    \"\"\"Allows for type annotations such as.\n\t    .. code-block:: python\n\t        class Numbers(Schema):\n\t            number: Column[DecimalType[Literal[10], Literal[0]]]\n\t    \"\"\"\n\tclass DayTimeIntervalType(Generic[_StartField, _EndField], TypedSparkDataType):\n\t    \"\"\"Allows for type annotations such as.\n", "    .. code-block:: python\n\t        class TimeInterval(Schema):\n\t            interval: Column[DayTimeIntervalType[IntervalType.HOUR, IntervalType.SECOND]\n\t    \"\"\"\n"]}
{"filename": "typedspark/_core/column_meta.py", "chunked_list": ["\"\"\"Metadata for ``Column`` objects that can be accessed during runtime.\"\"\"\n\tfrom dataclasses import dataclass\n\tfrom typing import Dict, Optional\n\t@dataclass\n\tclass ColumnMeta:\n\t    \"\"\"Contains the metadata for a ``Column``. Used as:\n\t    .. code-block:: python\n\t        class A(Schema):\n\t            a: Annotated[\n\t                Column[IntegerType],\n", "                ColumnMeta(\n\t                    comment=\"This is a comment\",\n\t                )\n\t            ]\n\t    \"\"\"\n\t    comment: Optional[str] = None\n\t    def get_metadata(self) -> Optional[Dict[str, str]]:\n\t        \"\"\"Returns the metadata of this column.\"\"\"\n\t        return {\"comment\": self.comment} if self.comment else None\n"]}
{"filename": "typedspark/_core/dataset.py", "chunked_list": ["\"\"\"Module containing classes and functions related to TypedSpark DataSets.\"\"\"\n\tfrom copy import deepcopy\n\tfrom typing import (\n\t    Any,\n\t    Callable,\n\t    Generic,\n\t    List,\n\t    Literal,\n\t    Optional,\n\t    Type,\n", "    TypeVar,\n\t    Union,\n\t    get_args,\n\t    overload,\n\t)\n\tfrom pyspark.sql import Column as SparkColumn\n\tfrom pyspark.sql import DataFrame\n\tfrom typing_extensions import Concatenate, ParamSpec\n\tfrom typedspark._core.validate_schema import validate_schema\n\tfrom typedspark._schema.schema import Schema\n", "T = TypeVar(\"T\", bound=Schema)\n\t_ReturnType = TypeVar(\"_ReturnType\", bound=DataFrame)  # pylint: disable=C0103\n\tP = ParamSpec(\"P\")\n\tclass DataSet(DataFrame, Generic[T]):\n\t    \"\"\"``DataSet`` subclasses pyspark ``DataFrame`` and hence has all the same\n\t    functionality, with in addition the possibility to define a schema.\n\t    .. code-block:: python\n\t        class Person(Schema):\n\t            name: Column[StringType]\n\t            age: Column[LongType]\n", "        def foo(df: DataSet[Person]) -> DataSet[Person]:\n\t            # do stuff\n\t            return df\n\t    \"\"\"\n\t    def __new__(cls, dataframe: DataFrame) -> \"DataSet[T]\":\n\t        \"\"\"``__new__()`` instantiates the object (prior to ``__init__()``).\n\t        Here, we simply take the provided ``df`` and cast it to a\n\t        ``DataSet``. This allows us to bypass the ``DataFrame``\n\t        constuctor in ``__init__()``, which requires parameters that may\n\t        be difficult to access.\n", "        \"\"\"\n\t        dataframe.__class__ = DataSet\n\t        return dataframe  # type: ignore\n\t    def __init__(self, dataframe: DataFrame):\n\t        pass\n\t    def __setattr__(self, name: str, value: Any) -> None:\n\t        \"\"\"Python base function that sets attributes.\n\t        We listen here for the setting of ``__orig_class__``, which\n\t        contains the ``Schema`` of the ``DataSet``. Note that this gets\n\t        set after ``__new__()`` and ``__init__()`` are finished.\n", "        \"\"\"\n\t        object.__setattr__(self, name, value)\n\t        if name == \"__orig_class__\":\n\t            orig_class_args = get_args(self.__orig_class__)\n\t            if orig_class_args and issubclass(orig_class_args[0], Schema):\n\t                self._schema_annotations: Type[Schema] = orig_class_args[0]\n\t                validate_schema(\n\t                    self._schema_annotations.get_structtype(),\n\t                    deepcopy(self.schema),\n\t                    self._schema_annotations.get_schema_name(),\n", "                )\n\t                self._add_schema_metadata()\n\t    def _add_schema_metadata(self) -> None:\n\t        \"\"\"Adds the ``ColumnMeta`` comments as metadata to the ``DataSet``.\n\t        Previously set metadata is deleted. Hence, if ``foo(dataframe: DataSet[A]) -> DataSet[B]``,\n\t        then ``DataSet[B]`` will not inherrit any metadata from ``DataSet[A]``.\n\t        Assumes validate_schema() in __setattr__() has been run.\n\t        \"\"\"\n\t        for field in self._schema_annotations.get_structtype().fields:\n\t            self.schema[field.name].metadata = field.metadata\n", "    \"\"\"The following functions are equivalent to their parents in ``DataFrame``, but since they\n\t    don't affect the ``Schema``, we can add type annotations here. We're omitting docstrings,\n\t    such that the docstring from the parent will appear.\"\"\"\n\t    def distinct(self) -> \"DataSet[T]\":  # pylint: disable=C0116\n\t        return DataSet[self._schema_annotations](super().distinct())  # type: ignore\n\t    def filter(self, condition) -> \"DataSet[T]\":  # pylint: disable=C0116\n\t        return DataSet[self._schema_annotations](super().filter(condition))  # type: ignore\n\t    @overload\n\t    def join(  # type: ignore\n\t        self,\n", "        other: DataFrame,\n\t        on: Optional[  # pylint: disable=C0103\n\t            Union[str, List[str], SparkColumn, List[SparkColumn]]\n\t        ] = ...,\n\t        how: None = ...,\n\t    ) -> DataFrame:\n\t        ...  # pragma: no cover\n\t    @overload\n\t    def join(\n\t        self,\n", "        other: DataFrame,\n\t        on: Optional[  # pylint: disable=C0103\n\t            Union[str, List[str], SparkColumn, List[SparkColumn]]\n\t        ] = ...,\n\t        how: Literal[\"semi\"] = ...,\n\t    ) -> \"DataSet[T]\":\n\t        ...  # pragma: no cover\n\t    @overload\n\t    def join(\n\t        self,\n", "        other: DataFrame,\n\t        on: Optional[  # pylint: disable=C0103\n\t            Union[str, List[str], SparkColumn, List[SparkColumn]]\n\t        ] = ...,\n\t        how: Optional[str] = ...,\n\t    ) -> DataFrame:\n\t        ...  # pragma: no cover\n\t    def join(  # pylint: disable=C0116\n\t        self,\n\t        other: DataFrame,\n", "        on: Optional[  # pylint: disable=C0103\n\t            Union[str, List[str], SparkColumn, List[SparkColumn]]\n\t        ] = None,\n\t        how: Optional[str] = None,\n\t    ) -> DataFrame:\n\t        return super().join(other, on, how)  # type: ignore\n\t    def orderBy(self, *args, **kwargs) -> \"DataSet[T]\":  # type: ignore  # noqa: N802, E501  # pylint: disable=C0116, C0103\n\t        return DataSet[self._schema_annotations](super().orderBy(*args, **kwargs))  # type: ignore\n\t    @overload\n\t    def transform(\n", "        self,\n\t        func: Callable[Concatenate[\"DataSet[T]\", P], _ReturnType],\n\t        *args: P.args,\n\t        **kwargs: P.kwargs,\n\t    ) -> _ReturnType:\n\t        ...  # pragma: no cover\n\t    @overload\n\t    def transform(self, func: Callable[..., \"DataFrame\"], *args: Any, **kwargs: Any) -> \"DataFrame\":\n\t        ...  # pragma: no cover\n\t    def transform(  # pylint: disable=C0116\n", "        self, func: Callable[..., \"DataFrame\"], *args: Any, **kwargs: Any\n\t    ) -> \"DataFrame\":\n\t        return super().transform(func, *args, **kwargs)\n\t    @overload\n\t    def unionByName(  # noqa: N802  # pylint: disable=C0116, C0103\n\t        self,\n\t        other: \"DataSet[T]\",\n\t        allowMissingColumns: Literal[False] = ...,  # noqa: N803\n\t    ) -> \"DataSet[T]\":\n\t        ...  # pragma: no cover\n", "    @overload\n\t    def unionByName(  # noqa: N802  # pylint: disable=C0116, C0103\n\t        self,\n\t        other: DataFrame,\n\t        allowMissingColumns: bool = ...,  # noqa: N803\n\t    ) -> DataFrame:\n\t        ...  # pragma: no cover\n\t    def unionByName(  # noqa: N802  # pylint: disable=C0116, C0103\n\t        self,\n\t        other: DataFrame,\n", "        allowMissingColumns: bool = False,  # noqa: N803\n\t    ) -> DataFrame:\n\t        res = super().unionByName(other, allowMissingColumns)\n\t        if isinstance(other, DataSet) and other._schema_annotations == self._schema_annotations:\n\t            return DataSet[self._schema_annotations](res)  # type: ignore\n\t        return res  # pragma: no cover\n"]}
{"filename": "typedspark/_core/__init__.py", "chunked_list": []}
{"filename": "typedspark/_core/validate_schema.py", "chunked_list": ["\"\"\"Module containing functions that are related to validating schema's at runtime.\"\"\"\n\tfrom typing import Dict, Set\n\tfrom pyspark.sql.types import ArrayType, DataType, MapType, StructField, StructType\n\tdef validate_schema(\n\t    structtype_expected: StructType, structtype_observed: StructType, schema_name: str\n\t) -> None:\n\t    \"\"\"Checks whether the expected and the observed StructType match.\"\"\"\n\t    expected = unpack_schema(structtype_expected)\n\t    observed = unpack_schema(structtype_observed)\n\t    check_names(set(expected.keys()), set(observed.keys()), schema_name)\n", "    check_dtypes(expected, observed, schema_name)\n\tdef unpack_schema(schema: StructType) -> Dict[str, StructField]:\n\t    \"\"\"Converts the observed schema to a dictionary mapping column name to StructField.\n\t    We ignore columns that start with ``__``.\n\t    \"\"\"\n\t    res = {}\n\t    for field in schema.fields:\n\t        if field.name.startswith(\"__\"):\n\t            continue\n\t        field.nullable = True\n", "        field.metadata = {}\n\t        res[field.name] = field\n\t    return res\n\tdef check_names(names_expected: Set[str], names_observed: Set[str], schema_name: str) -> None:\n\t    \"\"\"Checks whether the observed and expected list of column names overlap.\n\t    Is order insensitive.\n\t    \"\"\"\n\t    diff = names_observed - names_expected\n\t    if diff:\n\t        raise TypeError(\n", "            f\"Data contains the following columns not present in schema {schema_name}: {diff}\"\n\t        )\n\t    diff = names_expected - names_observed\n\t    if diff:\n\t        raise TypeError(\n\t            f\"Schema {schema_name} contains the following columns not present in data: {diff}\"\n\t        )\n\tdef check_dtypes(\n\t    schema_expected: Dict[str, StructField],\n\t    schema_observed: Dict[str, StructField],\n", "    schema_name: str,\n\t) -> None:\n\t    \"\"\"Checks for each column whether the observed and expected data type match.\n\t    Is order insensitive.\n\t    \"\"\"\n\t    for name, structfield_expected in schema_expected.items():\n\t        structfield_observed = schema_observed[name]\n\t        check_dtype(\n\t            name,\n\t            structfield_expected.dataType,\n", "            structfield_observed.dataType,\n\t            schema_name,\n\t        )\n\tdef check_dtype(\n\t    colname: str, dtype_expected: DataType, dtype_observed: DataType, schema_name: str\n\t) -> None:\n\t    \"\"\"Checks whether the observed and expected data type match.\"\"\"\n\t    if dtype_expected == dtype_observed:\n\t        return None\n\t    if isinstance(dtype_expected, ArrayType) and isinstance(dtype_observed, ArrayType):\n", "        return check_dtype(\n\t            f\"{colname}.element_type\",\n\t            dtype_expected.elementType,\n\t            dtype_observed.elementType,\n\t            schema_name,\n\t        )\n\t    if isinstance(dtype_expected, MapType) and isinstance(dtype_observed, MapType):\n\t        check_dtype(\n\t            f\"{colname}.key\",\n\t            dtype_expected.keyType,\n", "            dtype_observed.keyType,\n\t            schema_name,\n\t        )\n\t        return check_dtype(\n\t            f\"{colname}.value\",\n\t            dtype_expected.valueType,\n\t            dtype_observed.valueType,\n\t            schema_name,\n\t        )\n\t    if isinstance(dtype_expected, StructType) and isinstance(dtype_observed, StructType):\n", "        return validate_schema(dtype_expected, dtype_observed, f\"{schema_name}.{colname}\")\n\t    raise TypeError(\n\t        f\"Column {colname} is of type {dtype_observed}, but {schema_name}.{colname} \"\n\t        + f\"suggests {dtype_expected}.\"\n\t    )\n"]}
{"filename": "typedspark/_core/column.py", "chunked_list": ["\"\"\"Module containing classes and functions related to TypedSpark Columns.\"\"\"\n\tfrom logging import warn\n\tfrom typing import Generic, Optional, TypeVar, Union, get_args, get_origin\n\tfrom pyspark.sql import Column as SparkColumn\n\tfrom pyspark.sql import DataFrame, SparkSession\n\tfrom pyspark.sql.functions import col\n\tfrom pyspark.sql.types import DataType\n\tfrom typedspark._core.datatypes import StructType\n\tT = TypeVar(\"T\", bound=DataType)\n\tclass EmptyColumn(SparkColumn):\n", "    \"\"\"Column object to be instantiated when there is no active Spark session.\"\"\"\n\t    def __init__(self, *args, **kwargs) -> None:  # pragma: no cover\n\t        pass\n\tclass Column(SparkColumn, Generic[T]):\n\t    \"\"\"Represents a ``Column`` in a ``Schema``. Can be used as:\n\t    .. code-block:: python\n\t        class A(Schema):\n\t            a: Column[IntegerType]\n\t            b: Column[StringType]\n\t    \"\"\"\n", "    def __new__(\n\t        cls,\n\t        name: str,\n\t        dataframe: Optional[DataFrame] = None,\n\t        curid: Optional[int] = None,\n\t        dtype: Optional[T] = None,\n\t        parent: Union[DataFrame, \"Column\", None] = None,\n\t    ):\n\t        \"\"\"``__new__()`` instantiates the object (prior to ``__init__()``).\n\t        Here, we simply take the provided ``name``, create a pyspark\n", "        ``Column`` object and cast it to a typedspark ``Column`` object.\n\t        This allows us to bypass the pypsark ``Column`` constuctor in\n\t        ``__init__()``, which requires parameters that may be difficult\n\t        to access.\n\t        \"\"\"\n\t        # pylint: disable=unused-argument\n\t        if dataframe is not None and parent is None:\n\t            parent = dataframe\n\t            warn(\"The use of Column(dataframe=...) is deprecated, use Column(parent=...) instead.\")\n\t        column: SparkColumn\n", "        if SparkSession.getActiveSession() is None:\n\t            column = EmptyColumn()  # pragma: no cover\n\t        elif parent is None:\n\t            column = col(name)\n\t        else:\n\t            column = parent[name]\n\t        column.__class__ = Column\n\t        return column\n\t    def __init__(\n\t        self,\n", "        name: str,\n\t        dataframe: Optional[DataFrame] = None,\n\t        curid: Optional[int] = None,\n\t        dtype: Optional[T] = None,\n\t        parent: Union[DataFrame, \"Column\", None] = None,\n\t    ):\n\t        # pylint: disable=unused-argument\n\t        self.str = name\n\t        self._dtype = dtype if dtype is not None else DataType\n\t        self._curid = curid\n", "    def __hash__(self) -> int:\n\t        return hash((self.str, self._curid))\n\t    @property\n\t    def dtype(self) -> T:\n\t        \"\"\"Get the datatype of the column, e.g. Column[IntegerType] -> IntegerType.\"\"\"\n\t        dtype = self._dtype\n\t        if get_origin(dtype) == StructType:\n\t            dtype.schema = get_args(dtype)[0]  # type: ignore\n\t            dtype.schema._parent = self  # type: ignore\n\t        return dtype  # type: ignore\n"]}
{"filename": "typedspark/_core/literaltype.py", "chunked_list": ["\"\"\"Defines ``LiteralTypes``, e.g. ``IntervalType.DAY``, that map their class attribute to a\n\t``Literal`` integer. Can be used for example in ``DayTimeIntervalType``.\"\"\"\n\tfrom typing import Dict, Literal\n\tclass LiteralType:\n\t    \"\"\"Base class for literal types, that map their class attribute to a Literal\n\t    integer.\"\"\"\n\t    @classmethod\n\t    def get_dict(cls) -> Dict[str, str]:\n\t        \"\"\"Returns a dictionary mapping e.g. \"IntervalType.DAY\" -> \"Literal[0]\".\"\"\"\n\t        dictionary = {}\n", "        for key, value in cls.__dict__.items():\n\t            if key.startswith(\"_\"):\n\t                continue\n\t            key = f\"{cls.__name__}.{key}\"\n\t            value = str(value).replace(\"typing.\", \"\")\n\t            dictionary[key] = value\n\t        return dictionary\n\t    @classmethod\n\t    def get_inverse_dict(cls) -> Dict[str, str]:\n\t        \"\"\"Returns a dictionary mapping e.g. \"Literal[0]\" -> \"IntervalType.DAY\".\"\"\"\n", "        return {v: k for k, v in cls.get_dict().items()}\n\tclass IntervalType(LiteralType):\n\t    \"\"\"Interval types for ``DayTimeIntervalType``.\"\"\"\n\t    DAY = Literal[0]\n\t    HOUR = Literal[1]\n\t    MINUTE = Literal[2]\n\t    SECOND = Literal[3]\n"]}
{"filename": "typedspark/_schema/schema.py", "chunked_list": ["\"\"\"Module containing classes and functions related to TypedSpark Schemas.\"\"\"\n\timport inspect\n\timport re\n\tfrom typing import Any, Dict, List, Optional, Type, Union, get_args, get_type_hints\n\tfrom pyspark.sql import DataFrame\n\tfrom pyspark.sql.types import DataType, StructType\n\tfrom typedspark._core.column import Column\n\tfrom typedspark._schema.dlt_kwargs import DltKwargs\n\tfrom typedspark._schema.get_schema_definition import get_schema_definition_as_string\n\tfrom typedspark._schema.structfield import get_structfield\n", "class MetaSchema(type):\n\t    \"\"\"``MetaSchema`` is the metaclass of ``Schema``.\n\t    It basically implements all functionality of ``Schema``. But since\n\t    classes are typically considered more convenient than metaclasses,\n\t    we provide ``Schema`` as the public interface.\n\t    .. code-block:: python\n\t        class A(Schema):\n\t            a: Column[IntegerType]\n\t            b: Column[StringType]\n\t        DataSet[A](df)\n", "    The class methods of ``Schema`` are described here.\n\t    \"\"\"\n\t    _parent: Optional[Union[DataFrame, Column]] = None\n\t    _current_id: Optional[int] = None\n\t    _original_name: Optional[str] = None\n\t    def __new__(cls, name: str, bases: Any, dct: Dict[str, Any]):\n\t        cls._attributes = dir(cls)\n\t        # initializes all uninitialied variables with a type annotation as None\n\t        # this allows for auto-complete in Databricks notebooks (uninitialized variables\n\t        # don't show up in auto-complete there).\n", "        if \"__annotations__\" in dct.keys():\n\t            extra = {attr: None for attr in dct[\"__annotations__\"] if attr not in dct}\n\t            dct = dict(dct, **extra)\n\t        return type.__new__(cls, name, bases, dct)\n\t    def __repr__(cls) -> str:\n\t        return f\"\\n{str(cls)}\"\n\t    def __str__(cls) -> str:\n\t        return cls.get_schema_definition_as_string(add_subschemas=False)\n\t    def __getattribute__(cls, name: str) -> Any:\n\t        \"\"\"Python base function that gets attributes.\n", "        We listen here for anyone getting a ``Column`` from the ``Schema``.\n\t        Even though they're not explicitely instantiated, we can instantiate\n\t        them here whenever someone attempts to get them. This allows us to do the following:\n\t        .. code-block:: python\n\t            class A(Schema):\n\t                a: Column[IntegerType]\n\t            (\n\t                df.withColumn(A.a.str, lit(1))\n\t                .select(A.a)\n\t            )\n", "        \"\"\"\n\t        if name.startswith(\"__\") or name == \"_attributes\" or name in cls._attributes:\n\t            return object.__getattribute__(cls, name)\n\t        if name in get_type_hints(cls):\n\t            return Column(\n\t                name,\n\t                dtype=cls._get_dtype(name),  # type: ignore\n\t                parent=cls._parent,\n\t                curid=cls._current_id,\n\t            )\n", "        raise TypeError(f\"Schema {cls.get_schema_name()} does not have attribute {name}.\")\n\t    def _get_dtype(cls, name: str) -> Type[DataType]:\n\t        \"\"\"Returns the datatype of a column, e.g. Column[IntegerType] -> IntegerType.\"\"\"\n\t        column = get_type_hints(cls)[name]\n\t        args = get_args(column)\n\t        if not args:\n\t            raise TypeError(\n\t                f\"Column {cls.get_schema_name()}.{name} does not have an annotated type.\"\n\t            )\n\t        dtype = args[0]\n", "        return dtype\n\t    def all_column_names(cls) -> List[str]:\n\t        \"\"\"Returns all column names for a given schema.\"\"\"\n\t        return list(get_type_hints(cls).keys())\n\t    def all_column_names_except_for(cls, except_for: List[str]) -> List[str]:\n\t        \"\"\"Returns all column names for a given schema except for the columns\n\t        specified in the ``except_for`` parameter.\"\"\"\n\t        return list(name for name in get_type_hints(cls).keys() if name not in except_for)\n\t    def get_snake_case(cls) -> str:\n\t        \"\"\"Return the class name transformed into snakecase.\"\"\"\n", "        word = cls.get_schema_name()\n\t        word = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r\"\\1_\\2\", word)\n\t        word = re.sub(r\"([a-z\\d])([A-Z])\", r\"\\1_\\2\", word)\n\t        word = word.replace(\"-\", \"_\")\n\t        return word.lower()\n\t    def get_schema_definition_as_string(\n\t        cls,\n\t        schema_name: Optional[str] = None,\n\t        include_documentation: bool = False,\n\t        generate_imports: bool = True,\n", "        add_subschemas: bool = True,\n\t    ) -> str:\n\t        \"\"\"Return the code for the ``Schema`` as a string.\"\"\"\n\t        if schema_name is None:\n\t            schema_name = cls.get_schema_name()\n\t        return get_schema_definition_as_string(\n\t            cls,  # type: ignore\n\t            include_documentation,\n\t            generate_imports,\n\t            add_subschemas,\n", "            schema_name,\n\t        )\n\t    def print_schema(\n\t        cls,\n\t        schema_name: Optional[str] = None,\n\t        include_documentation: bool = False,\n\t        generate_imports: bool = True,\n\t        add_subschemas: bool = False,\n\t    ):  # pragma: no cover\n\t        \"\"\"Print the code for the ``Schema``.\"\"\"\n", "        print(\n\t            cls.get_schema_definition_as_string(\n\t                schema_name=schema_name,\n\t                include_documentation=include_documentation,\n\t                generate_imports=generate_imports,\n\t                add_subschemas=add_subschemas,\n\t            )\n\t        )\n\t    def get_docstring(cls) -> Union[str, None]:\n\t        \"\"\"Returns the docstring of the schema.\"\"\"\n", "        return inspect.getdoc(cls)\n\t    def get_structtype(cls) -> StructType:\n\t        \"\"\"Creates the spark StructType for the schema.\"\"\"\n\t        return StructType(\n\t            [\n\t                get_structfield(name, column)\n\t                for name, column in get_type_hints(cls, include_extras=True).items()\n\t            ]\n\t        )\n\t    def get_dlt_kwargs(cls, name: Optional[str] = None) -> DltKwargs:\n", "        \"\"\"Creates a representation of the ``Schema`` to be used by Delta Live\n\t        Tables.\n\t        .. code-block:: python\n\t            @dlt.table(**DimPatient.get_dlt_kwargs())\n\t            def table_definition() -> DataSet[DimPatient]:\n\t                <your table definition here>\n\t        \"\"\"\n\t        return {\n\t            \"name\": name if name else cls.get_snake_case(),\n\t            \"comment\": cls.get_docstring(),\n", "            \"schema\": cls.get_structtype(),\n\t        }\n\t    def get_schema_name(cls):\n\t        \"\"\"Returns the name with which the schema was initialized.\"\"\"\n\t        return cls._original_name if cls._original_name else cls.__name__\n\tclass Schema(metaclass=MetaSchema):\n\t    # pylint: disable=missing-class-docstring\n\t    # Since docstrings are inherrited, and since we use docstrings to\n\t    # annotate tables (see MetaSchema.get_dlt_kwargs()), we have chosen\n\t    # to not add a docstring to the Schema class (otherwise the Schema\n", "    # docstring would be added to any schema without a docstring).\n\t    pass\n"]}
{"filename": "typedspark/_schema/__init__.py", "chunked_list": []}
{"filename": "typedspark/_schema/get_schema_imports.py", "chunked_list": ["\"\"\"Builds an import statement for everything imported by a given ``Schema``.\"\"\"\n\tfrom __future__ import annotations\n\tfrom typing import TYPE_CHECKING, Optional, Type, get_args, get_origin, get_type_hints\n\tfrom pyspark.sql.types import DataType\n\tfrom typedspark._core.datatypes import (\n\t    ArrayType,\n\t    DayTimeIntervalType,\n\t    DecimalType,\n\t    MapType,\n\t    StructType,\n", "    TypedSparkDataType,\n\t)\n\tif TYPE_CHECKING:  # pragma: no cover\n\t    from typedspark._schema.schema import Schema\n\tdef get_schema_imports(schema: Type[Schema], include_documentation: bool) -> str:\n\t    \"\"\"Builds an import statement for everything imported by the ``Schema``.\"\"\"\n\t    dtypes = _get_imported_dtypes(schema)\n\t    return _build_import_string(dtypes, include_documentation)\n\tdef _get_imported_dtypes(schema: Type[Schema]) -> set[Type[DataType]]:\n\t    \"\"\"Returns a set of DataTypes that are imported by the given schema.\"\"\"\n", "    encountered_datatypes: set[Type[DataType]] = set()\n\t    for column in get_type_hints(schema).values():\n\t        args = get_args(column)\n\t        if not args:\n\t            continue\n\t        dtype = args[0]\n\t        encountered_datatypes |= _process_datatype(dtype)\n\t    return encountered_datatypes\n\tdef _process_datatype(dtype: Type[DataType]) -> set[Type[DataType]]:\n\t    \"\"\"Returns a set of DataTypes that are imported for a given DataType.\n", "    Handles nested DataTypes recursively.\n\t    \"\"\"\n\t    encountered_datatypes: set[Type[DataType]] = set()\n\t    origin: Optional[Type[DataType]] = get_origin(dtype)\n\t    if origin:\n\t        encountered_datatypes.add(origin)\n\t    else:\n\t        encountered_datatypes.add(dtype)\n\t    if origin == MapType:\n\t        key, value = get_args(dtype)\n", "        encountered_datatypes |= _process_datatype(key)\n\t        encountered_datatypes |= _process_datatype(value)\n\t    if origin == ArrayType:\n\t        element = get_args(dtype)[0]\n\t        encountered_datatypes |= _process_datatype(element)\n\t    if get_origin(dtype) == StructType:\n\t        subschema = get_args(dtype)[0]\n\t        encountered_datatypes |= _get_imported_dtypes(subschema)\n\t    return encountered_datatypes\n\tdef _build_import_string(\n", "    encountered_datatypes: set[Type[DataType]], include_documentation: bool\n\t) -> str:\n\t    \"\"\"Returns a multiline string with the imports required for the given\n\t    encountered_datatypes.\n\t    Import sorting is applied.\n\t    If the schema uses IntegerType, BooleanType, StringType, this functions result would be\n\t    .. code-block:: python\n\t        from pyspark.sql.types import BooleanType, IntegerType, StringType\n\t        from typedspark import Column, Schema\n\t    \"\"\"\n", "    return (\n\t        _typing_imports(encountered_datatypes, include_documentation)\n\t        + _pyspark_imports(encountered_datatypes)\n\t        + _typedspark_imports(encountered_datatypes, include_documentation)\n\t    )\n\tdef _typing_imports(encountered_datatypes: set[Type[DataType]], include_documentation: bool) -> str:\n\t    \"\"\"Returns the import statement for the typing library.\"\"\"\n\t    imports = []\n\t    if any([dtype == DecimalType for dtype in encountered_datatypes]):\n\t        imports += [\"Literal\"]\n", "    if include_documentation:\n\t        imports += [\"Annotated\"]\n\t    if len(imports) > 0:\n\t        imports = sorted(imports)\n\t        imports_string = \", \".join(imports)\n\t        return f\"from typing import {imports_string}\\n\\n\"\n\t    return \"\"\n\tdef _pyspark_imports(encountered_datatypes: set[Type[DataType]]) -> str:\n\t    \"\"\"Returns the import statement for the pyspark library.\"\"\"\n\t    dtypes = sorted(\n", "        [\n\t            dtype.__name__\n\t            for dtype in encountered_datatypes\n\t            if not issubclass(dtype, TypedSparkDataType)\n\t        ]\n\t    )\n\t    if len(dtypes) > 0:\n\t        dtypes_string = \", \".join(dtypes)\n\t        return f\"from pyspark.sql.types import {dtypes_string}\\n\\n\"\n\t    return \"\"\n", "def _typedspark_imports(\n\t    encountered_datatypes: set[Type[DataType]], include_documentation: bool\n\t) -> str:\n\t    \"\"\"Returns the import statement for the typedspark library.\"\"\"\n\t    dtypes = [\n\t        dtype.__name__ for dtype in encountered_datatypes if issubclass(dtype, TypedSparkDataType)\n\t    ] + [\"Column\", \"Schema\"]\n\t    if any([dtype == DayTimeIntervalType for dtype in encountered_datatypes]):\n\t        dtypes += [\"IntervalType\"]\n\t    if include_documentation:\n", "        dtypes.append(\"ColumnMeta\")\n\t    dtypes = sorted(dtypes)\n\t    dtypes_string = \", \".join(dtypes)\n\t    return f\"from typedspark import {dtypes_string}\\n\\n\\n\"\n"]}
{"filename": "typedspark/_schema/get_schema_definition.py", "chunked_list": ["\"\"\"Module to output a string with the ``Schema`` definition of a given\n\t``DataFrame``.\"\"\"\n\tfrom __future__ import annotations\n\timport re\n\tfrom typing import TYPE_CHECKING, Type, get_args, get_origin, get_type_hints\n\tfrom typedspark._core.datatypes import DayTimeIntervalType, StructType, TypedSparkDataType\n\tfrom typedspark._core.literaltype import IntervalType, LiteralType\n\tfrom typedspark._schema.get_schema_imports import get_schema_imports\n\tif TYPE_CHECKING:  # pragma: no cover\n\t    from typedspark._schema.schema import Schema\n", "def get_schema_definition_as_string(\n\t    schema: Type[Schema],\n\t    include_documentation: bool,\n\t    generate_imports: bool,\n\t    add_subschemas: bool,\n\t    class_name: str = \"MyNewSchema\",\n\t) -> str:\n\t    \"\"\"Return the code for a given ``Schema`` as a string.\n\t    Typically used when you load a dataset using\n\t    ``load_dataset_from_table()`` in a notebook and you want to save the\n", "    schema in your code base. When ``generate_imports`` is True, the\n\t    required imports for the schema are included in the string.\n\t    \"\"\"\n\t    imports = get_schema_imports(schema, include_documentation) if generate_imports else \"\"\n\t    schema_string = _build_schema_definition_string(\n\t        schema, include_documentation, add_subschemas, class_name\n\t    )\n\t    return imports + schema_string\n\tdef _build_schema_definition_string(\n\t    schema: Type[Schema],\n", "    include_documentation: bool,\n\t    add_subschemas: bool,\n\t    class_name: str = \"MyNewSchema\",\n\t) -> str:\n\t    \"\"\"Return the code for a given ``Schema`` as a string.\"\"\"\n\t    lines = f\"class {class_name}(Schema):\\n\"\n\t    if include_documentation:\n\t        lines += '    \"\"\"Add documentation here.\"\"\"\\n\\n'\n\t    for k, val in get_type_hints(schema).items():\n\t        typehint = (\n", "            str(val)\n\t            .replace(\"typedspark._core.column.\", \"\")\n\t            .replace(\"typedspark._core.datatypes.\", \"\")\n\t            .replace(\"typedspark._schema.schema.\", \"\")\n\t            .replace(\"pyspark.sql.types.\", \"\")\n\t            .replace(\"typing.\", \"\")\n\t        )\n\t        typehint = _replace_literals(\n\t            typehint, replace_literals_in=DayTimeIntervalType, replace_literals_by=IntervalType\n\t        )\n", "        if include_documentation:\n\t            lines += f'    {k}: Annotated[{typehint}, ColumnMeta(comment=\"\")]\\n'\n\t        else:\n\t            lines += f\"    {k}: {typehint}\\n\"\n\t    if add_subschemas:\n\t        lines += _add_subschemas(schema, add_subschemas, include_documentation)\n\t    return lines\n\tdef _replace_literals(\n\t    typehint: str,\n\t    replace_literals_in: Type[TypedSparkDataType],\n", "    replace_literals_by: Type[LiteralType],\n\t) -> str:\n\t    \"\"\"Replace all Literals in a LiteralType, e.g.\n\t    \"DayTimeIntervalType[Literal[0], Literal[1]]\" ->\n\t    \"DayTimeIntervalType[IntervalType.DAY, IntervalType.HOUR]\"\n\t    \"\"\"\n\t    mapping = replace_literals_by.get_inverse_dict()\n\t    for original, replacement in mapping.items():\n\t        typehint = _replace_literal(typehint, replace_literals_in, original, replacement)\n\t    return typehint\n", "def _replace_literal(\n\t    typehint: str,\n\t    replace_literals_in: Type[TypedSparkDataType],\n\t    original: str,\n\t    replacement: str,\n\t) -> str:\n\t    \"\"\"Replaces a single Literal in a LiteralType, e.g.\n\t    \"DayTimeIntervalType[Literal[0], Literal[1]]\" ->\n\t    \"DayTimeIntervalType[IntervalType.DAY, Literal[1]]\"\n\t    \"\"\"\n", "    return re.sub(\n\t        rf\"{replace_literals_in.get_name()}\\[[^]]*\\]\",\n\t        lambda x: x.group(0).replace(original, replacement),\n\t        typehint,\n\t    )\n\tdef _add_subschemas(schema: Type[Schema], add_subschemas: bool, include_documentation: bool) -> str:\n\t    \"\"\"Identifies whether any ``Column`` are of the ``StructType`` type and\n\t    generates their schema recursively.\"\"\"\n\t    lines = \"\"\n\t    for val in get_type_hints(schema).values():\n", "        args = get_args(val)\n\t        if not args:\n\t            continue\n\t        dtype = args[0]\n\t        if get_origin(dtype) == StructType:\n\t            lines += \"\\n\\n\"\n\t            subschema: Type[Schema] = get_args(dtype)[0]\n\t            lines += _build_schema_definition_string(\n\t                subschema, include_documentation, add_subschemas, subschema.get_schema_name()\n\t            )\n", "    return lines\n"]}
{"filename": "typedspark/_schema/structfield.py", "chunked_list": ["\"\"\"Module responsible for generating StructFields from Columns in a Schema.\"\"\"\n\tfrom __future__ import annotations\n\timport inspect\n\tfrom typing import TYPE_CHECKING, Annotated, Type, TypeVar, Union, get_args, get_origin\n\tfrom pyspark.sql.types import ArrayType as SparkArrayType\n\tfrom pyspark.sql.types import DataType\n\tfrom pyspark.sql.types import DayTimeIntervalType as SparkDayTimeIntervalType\n\tfrom pyspark.sql.types import DecimalType as SparkDecimalType\n\tfrom pyspark.sql.types import MapType as SparkMapType\n\tfrom pyspark.sql.types import StructField\n", "from pyspark.sql.types import StructType as SparkStructType\n\tfrom typedspark._core.column import Column\n\tfrom typedspark._core.column_meta import ColumnMeta\n\tfrom typedspark._core.datatypes import (\n\t    ArrayType,\n\t    DayTimeIntervalType,\n\t    DecimalType,\n\t    MapType,\n\t    StructType,\n\t    TypedSparkDataType,\n", ")\n\tif TYPE_CHECKING:  # pragma: no cover\n\t    from typedspark._schema.schema import Schema\n\t_DataType = TypeVar(\"_DataType\", bound=DataType)  # pylint: disable=invalid-name\n\tdef get_structfield(\n\t    name: str,\n\t    column: Union[Type[Column[_DataType]], Annotated[Type[Column[_DataType]], ColumnMeta]],\n\t) -> StructField:\n\t    \"\"\"Generates a ``StructField`` for a given ``Column`` in a ``Schema``.\"\"\"\n\t    meta = get_structfield_meta(column)\n", "    return StructField(\n\t        name=name,\n\t        dataType=_get_structfield_dtype(column, name),\n\t        nullable=True,\n\t        metadata=meta.get_metadata(),\n\t    )\n\tdef get_structfield_meta(\n\t    column: Union[Type[Column[_DataType]], Annotated[Type[Column[_DataType]], ColumnMeta]]\n\t) -> ColumnMeta:\n\t    \"\"\"Get the spark column metadata from the ``ColumnMeta`` data, when\n", "    available.\"\"\"\n\t    return next((x for x in get_args(column) if isinstance(x, ColumnMeta)), ColumnMeta())\n\tdef _get_structfield_dtype(\n\t    column: Union[Type[Column[_DataType]], Annotated[Type[Column[_DataType]], ColumnMeta]],\n\t    colname: str,\n\t) -> DataType:\n\t    \"\"\"Get the spark ``DataType`` from the ``Column`` type annotation.\"\"\"\n\t    origin = get_origin(column)\n\t    if origin not in [Annotated, Column]:\n\t        raise TypeError(f\"Column {colname} needs to be of type Column or Annotated.\")\n", "    if origin == Annotated:\n\t        column = _get_column_from_annotation(column, colname)\n\t    args = get_args(column)\n\t    dtype = _get_dtype(args[0], colname)\n\t    return dtype\n\tdef _get_column_from_annotation(\n\t    column: Annotated[Type[Column[_DataType]], ColumnMeta],\n\t    colname: str,\n\t) -> Type[Column[_DataType]]:\n\t    \"\"\"Takes an ``Annotation[Column[...], ...]`` and returns the\n", "    ``Column[...]``.\"\"\"\n\t    column = get_args(column)[0]\n\t    if get_origin(column) != Column:\n\t        raise TypeError(f\"Column {colname} needs to have a Column[] within Annotated[].\")\n\t    return column\n\tdef _get_dtype(dtype: Type[DataType], colname: str) -> DataType:\n\t    \"\"\"Takes a ``DataType`` class and returns a DataType object.\"\"\"\n\t    origin = get_origin(dtype)\n\t    if origin == ArrayType:\n\t        return _extract_arraytype(dtype, colname)\n", "    if origin == MapType:\n\t        return _extract_maptype(dtype, colname)\n\t    if origin == StructType:\n\t        return _extract_structtype(dtype)\n\t    if origin == DecimalType:\n\t        return _extract_decimaltype(dtype)\n\t    if origin == DayTimeIntervalType:\n\t        return _extract_daytimeintervaltype(dtype)\n\t    if (\n\t        inspect.isclass(dtype)\n", "        and issubclass(dtype, DataType)\n\t        and not issubclass(dtype, TypedSparkDataType)\n\t    ):\n\t        return dtype()\n\t    raise TypeError(\n\t        f\"Column {colname} does not have a correctly formatted DataType as a parameter.\"\n\t    )\n\tdef _extract_arraytype(arraytype: Type[DataType], colname: str) -> SparkArrayType:\n\t    \"\"\"Takes e.g. an ``ArrayType[StringType]`` and creates an\n\t    ``ArrayType(StringType(), True)``.\"\"\"\n", "    params = get_args(arraytype)\n\t    element_type = _get_dtype(params[0], colname)\n\t    return SparkArrayType(element_type)\n\tdef _extract_maptype(maptype: Type[DataType], colname: str) -> SparkMapType:\n\t    \"\"\"Takes e.g. a ``MapType[StringType, StringType]`` and creates a ``\n\t    MapType(StringType(), StringType(), True)``.\"\"\"\n\t    params = get_args(maptype)\n\t    key_type = _get_dtype(params[0], colname)\n\t    value_type = _get_dtype(params[1], colname)\n\t    return SparkMapType(key_type, value_type)\n", "def _extract_structtype(structtype: Type[DataType]) -> SparkStructType:\n\t    \"\"\"Takes a ``StructType[Schema]`` annotation and creates a\n\t    ``StructType(schema_list)``, where ``schema_list`` contains all\n\t    ``StructField()`` defined in the ``Schema``.\"\"\"\n\t    params = get_args(structtype)\n\t    schema: Type[Schema] = params[0]\n\t    return schema.get_structtype()\n\tdef _extract_decimaltype(decimaltype: Type[DataType]) -> SparkDecimalType:\n\t    \"\"\"Takes e.g. a ``DecimalType[Literal[10], Literal[12]]`` and returns\n\t    ``DecimalType(10, 12)``.\"\"\"\n", "    params = get_args(decimaltype)\n\t    key_type: int = _unpack_literal(params[0])\n\t    value_type: int = _unpack_literal(params[1])\n\t    return SparkDecimalType(key_type, value_type)\n\tdef _extract_daytimeintervaltype(daytimeintervaltype: Type[DataType]) -> SparkDayTimeIntervalType:\n\t    \"\"\"Takes e.g. a ``DayTimeIntervalType[Literal[1], Literal[2]]`` and returns\n\t    ``DayTimeIntervalType(1, 2)``.\"\"\"\n\t    params = get_args(daytimeintervaltype)\n\t    start_field: int = _unpack_literal(params[0])\n\t    end_field: int = _unpack_literal(params[1])\n", "    return SparkDayTimeIntervalType(start_field, end_field)\n\tdef _unpack_literal(literal):\n\t    \"\"\"Takes as input e.g. ``Literal[10]`` and returns ``10``.\"\"\"\n\t    return get_args(literal)[0]\n"]}
{"filename": "typedspark/_schema/dlt_kwargs.py", "chunked_list": ["\"\"\"A representation of the ``Schema`` to be used by Delta Live Tables.\"\"\"\n\tfrom typing import Optional, TypedDict\n\tfrom pyspark.sql.types import StructType\n\tclass DltKwargs(TypedDict):\n\t    \"\"\"A representation of the ``Schema`` to be used by Delta Live Tables.\n\t    .. code-block:: python\n\t        @dlt.table(**Person.get_dlt_kwargs())\n\t        def table_definition() -> DataSet[Person]:\n\t            <your table definition here>\n\t    \"\"\"\n", "    name: str\n\t    comment: Optional[str]\n\t    schema: StructType\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\n\timport sys\n\timport pytest\n\tfrom pyspark.sql import SparkSession\n\t@pytest.fixture(scope=\"session\")\n\tdef spark():\n\t    \"\"\"Fixture for creating a spark session.\"\"\"\n\t    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n\t    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n\t    spark = SparkSession.Builder().getOrCreate()\n", "    yield spark\n\t    spark.stop()\n"]}
{"filename": "tests/_transforms/test_structtype_column.py", "chunked_list": ["import pytest\n\tfrom chispa.dataframe_comparer import assert_df_equality  # type: ignore\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import IntegerType\n\tfrom typedspark import Column, Schema, StructType, structtype_column\n\tfrom typedspark._transforms.transform_to_schema import transform_to_schema\n\tfrom typedspark._utils.create_dataset import create_partially_filled_dataset\n\tclass SubSchema(Schema):\n\t    a: Column[IntegerType]\n\t    b: Column[IntegerType]\n", "class MainSchema(Schema):\n\t    a: Column[IntegerType]\n\t    b: Column[StructType[SubSchema]]\n\tdef test_structtype_column(spark: SparkSession):\n\t    df = create_partially_filled_dataset(spark, MainSchema, {MainSchema.a: [1, 2, 3]})\n\t    observed = transform_to_schema(\n\t        df,\n\t        MainSchema,\n\t        {\n\t            MainSchema.b: structtype_column(\n", "                SubSchema,\n\t                {SubSchema.a: MainSchema.a + 2, SubSchema.b: MainSchema.a + 4},\n\t            )\n\t        },\n\t    )\n\t    expected = create_partially_filled_dataset(\n\t        spark,\n\t        MainSchema,\n\t        {\n\t            MainSchema.a: [1, 2, 3],\n", "            MainSchema.b: create_partially_filled_dataset(\n\t                spark, SubSchema, {SubSchema.a: [3, 4, 5], SubSchema.b: [5, 6, 7]}\n\t            ).collect(),\n\t        },\n\t    )\n\t    assert_df_equality(observed, expected, ignore_nullable=True)\n\tdef test_structtype_column_different_column_order(spark: SparkSession):\n\t    df = create_partially_filled_dataset(spark, MainSchema, {MainSchema.a: [1, 2, 3]})\n\t    observed = transform_to_schema(\n\t        df,\n", "        MainSchema,\n\t        {\n\t            MainSchema.b: structtype_column(\n\t                SubSchema,\n\t                {SubSchema.b: MainSchema.a + 4, SubSchema.a: MainSchema.a + 2},\n\t            )\n\t        },\n\t    )\n\t    expected = create_partially_filled_dataset(\n\t        spark,\n", "        MainSchema,\n\t        {\n\t            MainSchema.a: [1, 2, 3],\n\t            MainSchema.b: create_partially_filled_dataset(\n\t                spark, SubSchema, {SubSchema.a: [3, 4, 5], SubSchema.b: [5, 6, 7]}\n\t            ).collect(),\n\t        },\n\t    )\n\t    assert_df_equality(observed, expected, ignore_nullable=True)\n\tdef test_structtype_column_partial(spark: SparkSession):\n", "    df = create_partially_filled_dataset(spark, MainSchema, {MainSchema.a: [1, 2, 3]})\n\t    observed = transform_to_schema(\n\t        df,\n\t        MainSchema,\n\t        {\n\t            MainSchema.b: structtype_column(\n\t                SubSchema,\n\t                {SubSchema.a: MainSchema.a + 2},\n\t                fill_unspecified_columns_with_nulls=True,\n\t            )\n", "        },\n\t    )\n\t    expected = create_partially_filled_dataset(\n\t        spark,\n\t        MainSchema,\n\t        {\n\t            MainSchema.a: [1, 2, 3],\n\t            MainSchema.b: create_partially_filled_dataset(\n\t                spark,\n\t                SubSchema,\n", "                {SubSchema.a: [3, 4, 5], SubSchema.b: [None, None, None]},\n\t            ).collect(),\n\t        },\n\t    )\n\t    assert_df_equality(observed, expected, ignore_nullable=True)\n\tdef test_structtype_column_with_double_column(spark: SparkSession):\n\t    df = create_partially_filled_dataset(spark, MainSchema, {MainSchema.a: [1, 2, 3]})\n\t    with pytest.raises(ValueError):\n\t        transform_to_schema(\n\t            df,\n", "            MainSchema,\n\t            {\n\t                MainSchema.b: structtype_column(\n\t                    SubSchema,\n\t                    {SubSchema.a: MainSchema.a + 2, SubSchema.a: MainSchema.a + 2},\n\t                )\n\t            },\n\t        )\n"]}
{"filename": "tests/_transforms/test_transform_to_schema.py", "chunked_list": ["import pytest\n\tfrom chispa.dataframe_comparer import assert_df_equality  # type: ignore\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import IntegerType, StringType\n\tfrom typedspark import (\n\t    Column,\n\t    Schema,\n\t    create_empty_dataset,\n\t    register_schema_to_dataset,\n\t    transform_to_schema,\n", ")\n\tfrom typedspark._utils.create_dataset import create_partially_filled_dataset\n\tclass Person(Schema):\n\t    a: Column[IntegerType]\n\t    b: Column[IntegerType]\n\t    c: Column[IntegerType]\n\tclass PersonLessData(Schema):\n\t    a: Column[IntegerType]\n\t    b: Column[IntegerType]\n\tclass PersonDifferentData(Schema):\n", "    a: Column[IntegerType]\n\t    d: Column[IntegerType]\n\tdef test_transform_to_schema_without_transformations(spark: SparkSession):\n\t    df = create_empty_dataset(spark, Person)\n\t    observed = transform_to_schema(df, PersonLessData)\n\t    expected = create_empty_dataset(spark, PersonLessData)\n\t    assert_df_equality(observed, expected)\n\tdef test_transform_to_schema_with_transformation(spark: SparkSession):\n\t    df = create_partially_filled_dataset(spark, Person, {Person.c: [1, 2, 3]})\n\t    observed = transform_to_schema(df, PersonDifferentData, {PersonDifferentData.d: Person.c + 3})\n", "    expected = create_partially_filled_dataset(\n\t        spark, PersonDifferentData, {PersonDifferentData.d: [4, 5, 6]}\n\t    )\n\t    assert_df_equality(observed, expected)\n\tdef test_transform_to_schema_with_missing_column(spark):\n\t    df = create_partially_filled_dataset(spark, Person, {Person.c: [1, 2, 3]}).drop(Person.a)\n\t    with pytest.raises(Exception):\n\t        transform_to_schema(df, PersonDifferentData, {PersonDifferentData.d: Person.c + 3})\n\t    observed = transform_to_schema(\n\t        df,\n", "        PersonDifferentData,\n\t        {PersonDifferentData.d: Person.c + 3},\n\t        fill_unspecified_columns_with_nulls=True,\n\t    )\n\t    expected = create_partially_filled_dataset(\n\t        spark,\n\t        PersonDifferentData,\n\t        {PersonDifferentData.d: [4, 5, 6]},\n\t    )\n\t    assert_df_equality(observed, expected)\n", "def test_transform_to_schema_with_pre_existing_column(spark):\n\t    df = create_partially_filled_dataset(spark, Person, {Person.a: [0, 1, 2], Person.c: [1, 2, 3]})\n\t    observed = transform_to_schema(\n\t        df,\n\t        PersonDifferentData,\n\t        {PersonDifferentData.d: Person.c + 3},\n\t        fill_unspecified_columns_with_nulls=True,\n\t    )\n\t    expected = create_partially_filled_dataset(\n\t        spark,\n", "        PersonDifferentData,\n\t        {PersonDifferentData.a: [0, 1, 2], PersonDifferentData.d: [4, 5, 6]},\n\t    )\n\t    assert_df_equality(observed, expected)\n\tclass PersonA(Schema):\n\t    name: Column[StringType]\n\t    age: Column[StringType]\n\tclass PersonB(Schema):\n\t    name: Column[StringType]\n\t    age: Column[StringType]\n", "def test_transform_to_schema_with_column_disambiguation(spark: SparkSession):\n\t    df_a = create_partially_filled_dataset(\n\t        spark,\n\t        PersonA,\n\t        {PersonA.name: [\"John\", \"Jane\", \"Bob\"], PersonA.age: [30, 40, 50]},\n\t    )\n\t    df_b = create_partially_filled_dataset(\n\t        spark,\n\t        PersonB,\n\t        {PersonB.name: [\"John\", \"Jane\", \"Bob\"], PersonB.age: [30, 40, 50]},\n", "    )\n\t    person_a = register_schema_to_dataset(df_a, PersonA)\n\t    person_b = register_schema_to_dataset(df_b, PersonB)\n\t    with pytest.raises(ValueError):\n\t        transform_to_schema(\n\t            df_a.join(df_b, person_a.name == person_b.name),\n\t            PersonA,\n\t            {\n\t                person_a.age: person_a.age + 3,\n\t                person_b.age: person_b.age + 5,\n", "            },\n\t        )\n\tdef test_transform_to_schema_with_double_column(spark: SparkSession):\n\t    df = create_partially_filled_dataset(spark, Person, {Person.a: [1, 2, 3], Person.b: [1, 2, 3]})\n\t    with pytest.raises(ValueError):\n\t        transform_to_schema(\n\t            df,\n\t            Person,\n\t            {\n\t                Person.a: Person.a + 3,\n", "                Person.a: Person.a + 5,\n\t            },\n\t        )\n\tdef test_transform_to_schema_sequential(spark: SparkSession):\n\t    df = create_partially_filled_dataset(spark, Person, {Person.a: [1, 2, 3], Person.b: [1, 2, 3]})\n\t    observed = transform_to_schema(\n\t        df,\n\t        Person,\n\t        {\n\t            Person.a: Person.a + 3,\n", "            Person.b: Person.a + 5,\n\t        },\n\t    )\n\t    expected = create_partially_filled_dataset(\n\t        spark, Person, {Person.a: [4, 5, 6], Person.b: [9, 10, 11]}\n\t    )\n\t    assert_df_equality(observed, expected)\n"]}
{"filename": "tests/_utils/test_load_table.py", "chunked_list": ["from typing import Literal\n\timport pytest\n\tfrom chispa.dataframe_comparer import assert_df_equality  # type: ignore\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.functions import first\n\tfrom pyspark.sql.types import IntegerType, StringType\n\tfrom typedspark import (\n\t    ArrayType,\n\t    Column,\n\t    Databases,\n", "    DecimalType,\n\t    MapType,\n\t    Schema,\n\t    StructType,\n\t    create_empty_dataset,\n\t    load_table,\n\t)\n\tfrom typedspark._core.datatypes import DayTimeIntervalType\n\tfrom typedspark._core.literaltype import IntervalType\n\tfrom typedspark._utils.create_dataset import create_partially_filled_dataset\n", "from typedspark._utils.databases import Catalogs\n\tfrom typedspark._utils.load_table import create_schema\n\tclass SubSchema(Schema):\n\t    a: Column[IntegerType]\n\tclass A(Schema):\n\t    a: Column[IntegerType]\n\t    b: Column[ArrayType[IntegerType]]\n\t    c: Column[ArrayType[MapType[IntegerType, IntegerType]]]\n\t    d: Column[DayTimeIntervalType[IntervalType.HOUR, IntervalType.MINUTE]]\n\t    e: Column[DecimalType[Literal[7], Literal[2]]]\n", "    value_container: Column[StructType[SubSchema]]\n\tdef test_load_table(spark: SparkSession) -> None:\n\t    df = create_empty_dataset(spark, A)\n\t    df.createOrReplaceTempView(\"temp\")\n\t    df_loaded, schema = load_table(spark, \"temp\")\n\t    assert_df_equality(df, df_loaded)\n\t    assert schema.get_structtype() == A.get_structtype()\n\t    assert schema.get_schema_name() != \"A\"\n\tdef test_load_table_with_schema_name(spark: SparkSession) -> None:\n\t    df = create_empty_dataset(spark, A)\n", "    df.createOrReplaceTempView(\"temp\")\n\t    df_loaded, schema = load_table(spark, \"temp\", schema_name=\"A\")\n\t    assert_df_equality(df, df_loaded)\n\t    assert schema.get_structtype() == A.get_structtype()\n\t    assert schema.get_schema_name() == \"A\"\n\tclass B(Schema):\n\t    a: Column[StringType]\n\t    b: Column[IntegerType]\n\t    c: Column[StringType]\n\tdef test_create_schema(spark: SparkSession) -> None:\n", "    df = (\n\t        create_partially_filled_dataset(\n\t            spark,\n\t            B,\n\t            {\n\t                B.a: [\"a\", \"b!!\", \"c\", \"a\", \"b!!\", \"c\", \"a\", \"b!!\", \"c\"],\n\t                B.b: [1, 1, 1, 2, 2, 2, 3, 3, 3],\n\t                B.c: [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"eta\", \"theta\", \"iota\"],\n\t            },\n\t        )\n", "        .groupby(B.b)\n\t        .pivot(B.a.str)\n\t        .agg(first(B.c))\n\t    )\n\t    df, MySchema = create_schema(df, \"B\")\n\t    assert MySchema.get_schema_name() == \"B\"\n\t    assert \"a\" in MySchema.all_column_names()\n\t    assert \"b__\" in MySchema.all_column_names()\n\t    assert \"c\" in MySchema.all_column_names()\n\tdef test_create_schema_with_duplicated_column_names(spark: SparkSession) -> None:\n", "    df = (\n\t        create_partially_filled_dataset(\n\t            spark,\n\t            B,\n\t            {\n\t                B.a: [\"a\", \"b??\", \"c\", \"a\", \"b!!\", \"c\", \"a\", \"b!!\", \"c\"],\n\t                B.b: [1, 1, 1, 2, 2, 2, 3, 3, 3],\n\t                B.c: [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"eta\", \"theta\", \"iota\"],\n\t            },\n\t        )\n", "        .groupby(B.b)\n\t        .pivot(B.a.str)\n\t        .agg(first(B.c))\n\t    )\n\t    with pytest.raises(ValueError):\n\t        create_schema(df, \"B\")\n\tdef test_name_of_structtype_schema(spark):\n\t    df = create_empty_dataset(spark, A)\n\t    df, MySchema = create_schema(df, \"A\")\n\t    assert MySchema.value_container.dtype.schema.get_schema_name() == \"ValueContainer\"\n", "def test_databases_with_temp_view(spark):\n\t    df = create_empty_dataset(spark, A)\n\t    df.createOrReplaceTempView(\"table_a\")\n\t    db = Databases(spark)\n\t    df_loaded, schema = db.default.table_a.load()  # type: ignore\n\t    assert_df_equality(df, df_loaded)\n\t    assert schema.get_structtype() == A.get_structtype()\n\t    assert schema.get_schema_name() == \"TableA\"\n\t    assert db.default.table_a.str == \"table_a\"  # type: ignore\n\t    assert db.default.str == \"default\"  # type: ignore\n", "def _drop_table(spark: SparkSession, table_name: str) -> None:\n\t    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n\tdef test_databases_with_table(spark):\n\t    df = create_empty_dataset(spark, A)\n\t    df.write.saveAsTable(\"default.table_b\")\n\t    try:\n\t        db = Databases(spark)\n\t        df_loaded, schema = db.default.table_b.load()  # type: ignore\n\t        assert_df_equality(df, df_loaded)\n\t        assert schema.get_structtype() == A.get_structtype()\n", "        assert schema.get_schema_name() == \"TableB\"\n\t        assert db.default.table_b.str == \"default.table_b\"  # type: ignore\n\t        assert db.default.str == \"default\"  # type: ignore\n\t    except Exception as exception:\n\t        _drop_table(spark, \"default.table_b\")\n\t        raise exception\n\t    _drop_table(spark, \"default.table_b\")\n\tdef test_catalogs(spark):\n\t    df = create_empty_dataset(spark, A)\n\t    df.write.saveAsTable(\"spark_catalog.default.table_b\")\n", "    try:\n\t        db = Catalogs(spark)\n\t        df_loaded, schema = db.spark_catalog.default.table_b.load()  # type: ignore\n\t        assert_df_equality(df, df_loaded)\n\t        assert schema.get_structtype() == A.get_structtype()\n\t        assert schema.get_schema_name() == \"TableB\"\n\t        assert db.spark_catalog.default.table_b.str == \"spark_catalog.default.table_b\"  # type: ignore  # noqa: E501\n\t    except Exception as exception:\n\t        _drop_table(spark, \"spark_catalog.default.table_b\")\n\t        raise exception\n", "    _drop_table(spark, \"spark_catalog.default.table_b\")\n"]}
{"filename": "tests/_utils/test_register_schema_to_dataset.py", "chunked_list": ["import pytest\n\tfrom pyspark.errors import AnalysisException\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import IntegerType\n\tfrom typedspark import Column, Schema, create_partially_filled_dataset, register_schema_to_dataset\n\tclass Person(Schema):\n\t    a: Column[IntegerType]\n\t    b: Column[IntegerType]\n\tclass Job(Schema):\n\t    a: Column[IntegerType]\n", "    c: Column[IntegerType]\n\tdef test_register_schema_to_dataset(spark: SparkSession):\n\t    df_a = create_partially_filled_dataset(spark, Person, {Person.a: [1, 2, 3]})\n\t    df_b = create_partially_filled_dataset(spark, Job, {Job.a: [1, 2, 3]})\n\t    with pytest.raises(AnalysisException):\n\t        df_a.join(df_b, Person.a == Job.a)\n\t    person = register_schema_to_dataset(df_a, Person)\n\t    job = register_schema_to_dataset(df_b, Job)\n\t    assert person.get_schema_name() == \"Person\"\n\t    df_a.join(df_b, person.a == job.a)\n"]}
{"filename": "tests/_utils/test_create_dataset.py", "chunked_list": ["from decimal import Decimal\n\tfrom typing import Literal\n\timport pytest\n\tfrom chispa.dataframe_comparer import assert_df_equality  # type: ignore\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import StringType\n\tfrom typedspark import (\n\t    ArrayType,\n\t    Column,\n\t    DataSet,\n", "    MapType,\n\t    Schema,\n\t    StructType,\n\t    create_empty_dataset,\n\t    create_partially_filled_dataset,\n\t    create_structtype_row,\n\t)\n\tfrom typedspark._core.datatypes import DecimalType\n\tclass A(Schema):\n\t    a: Column[DecimalType[Literal[38], Literal[18]]]\n", "    b: Column[StringType]\n\tdef test_create_empty_dataset(spark: SparkSession):\n\t    n_rows = 2\n\t    result: DataSet[A] = create_empty_dataset(spark, A, n_rows)\n\t    spark_schema = A.get_structtype()\n\t    data = [(None, None), (None, None)]\n\t    expected = spark.createDataFrame(data, spark_schema)\n\t    assert_df_equality(result, expected)\n\tdef test_create_partially_filled_dataset(spark: SparkSession):\n\t    data = {A.a: [Decimal(x) for x in [1, 2, 3]]}\n", "    result: DataSet[A] = create_partially_filled_dataset(spark, A, data)\n\t    spark_schema = A.get_structtype()\n\t    row_data = [(Decimal(1), None), (Decimal(2), None), (Decimal(3), None)]\n\t    expected = spark.createDataFrame(row_data, spark_schema)\n\t    assert_df_equality(result, expected)\n\tdef test_create_partially_filled_dataset_with_different_number_of_rows(\n\t    spark: SparkSession,\n\t):\n\t    with pytest.raises(ValueError):\n\t        create_partially_filled_dataset(spark, A, {A.a: [1], A.b: [\"a\", \"b\"]})\n", "class B(Schema):\n\t    a: Column[ArrayType[StringType]]\n\t    b: Column[MapType[StringType, StringType]]\n\t    c: Column[StructType[A]]\n\tdef test_create_empty_dataset_with_complex_data(spark: SparkSession):\n\t    df_a = create_partially_filled_dataset(spark, A, {A.a: [Decimal(x) for x in [1, 2, 3]]})\n\t    result = create_partially_filled_dataset(\n\t        spark,\n\t        B,\n\t        {\n", "            B.a: [[\"a\"], [\"b\", \"c\"], [\"d\"]],\n\t            B.b: [{\"a\": \"1\"}, {\"b\": \"2\", \"c\": \"3\"}, {\"d\": \"4\"}],\n\t            B.c: df_a.collect(),\n\t        },\n\t    )\n\t    spark_schema = B.get_structtype()\n\t    row_data = [\n\t        ([\"a\"], {\"a\": \"1\"}, (Decimal(1), None)),\n\t        ([\"b\", \"c\"], {\"b\": \"2\", \"c\": \"3\"}, (Decimal(2), None)),\n\t        ([\"d\"], {\"d\": \"4\"}, (Decimal(3), None)),\n", "    ]\n\t    expected = spark.createDataFrame(row_data, spark_schema)\n\t    assert_df_equality(result, expected)\n\tdef test_create_partially_filled_dataset_from_list(spark: SparkSession):\n\t    result = create_partially_filled_dataset(\n\t        spark,\n\t        A,\n\t        [\n\t            {A.a: Decimal(1), A.b: \"a\"},\n\t            {A.a: Decimal(2)},\n", "            {A.b: \"c\", A.a: Decimal(3)},\n\t        ],\n\t    )\n\t    spark_schema = A.get_structtype()\n\t    row_data = [(Decimal(1), \"a\"), (Decimal(2), None), (Decimal(3), \"c\")]\n\t    expected = spark.createDataFrame(row_data, spark_schema)\n\t    assert_df_equality(result, expected)\n\tdef test_create_partially_filled_dataset_from_list_with_complex_data(spark: SparkSession):\n\t    result = create_partially_filled_dataset(\n\t        spark,\n", "        B,\n\t        [\n\t            {\n\t                B.a: [\"a\"],\n\t                B.b: {\"a\": \"1\"},\n\t                B.c: create_structtype_row(A, {A.a: Decimal(1), A.b: \"a\"}),\n\t            },\n\t            {\n\t                B.a: [\"b\", \"c\"],\n\t                B.b: {\"b\": \"2\", \"c\": \"3\"},\n", "                B.c: create_structtype_row(A, {A.a: Decimal(2)}),\n\t            },\n\t            {\n\t                B.a: [\"d\"],\n\t                B.b: {\"d\": \"4\"},\n\t                B.c: create_structtype_row(A, {A.b: \"c\", A.a: Decimal(3)}),\n\t            },\n\t        ],\n\t    )\n\t    spark_schema = B.get_structtype()\n", "    row_data = [\n\t        ([\"a\"], {\"a\": \"1\"}, (Decimal(1), \"a\")),\n\t        ([\"b\", \"c\"], {\"b\": \"2\", \"c\": \"3\"}, (Decimal(2), None)),\n\t        ([\"d\"], {\"d\": \"4\"}, (Decimal(3), \"c\")),\n\t    ]\n\t    expected = spark.createDataFrame(row_data, spark_schema)\n\t    assert_df_equality(result, expected)\n\tdef test_create_partially_filled_dataset_with_invalid_argument(spark: SparkSession):\n\t    with pytest.raises(ValueError):\n\t        create_partially_filled_dataset(spark, A, ())  # type: ignore\n"]}
{"filename": "tests/_core/test_datatypes.py", "chunked_list": ["import pytest\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import ArrayType as SparkArrayType\n\tfrom pyspark.sql.types import LongType\n\tfrom pyspark.sql.types import MapType as SparkMapType\n\tfrom pyspark.sql.types import StringType, StructField\n\tfrom pyspark.sql.types import StructType as SparkStructType\n\tfrom typedspark import ArrayType, Column, DataSet, MapType, Schema, StructType, create_empty_dataset\n\tclass SubSchema(Schema):\n\t    a: Column[StringType]\n", "    b: Column[StringType]\n\tclass Example(Schema):\n\t    a: Column[MapType[StringType, StringType]]\n\t    b: Column[ArrayType[StringType]]\n\t    c: Column[StructType[SubSchema]]\n\tdef test_complex_datatypes_equals(spark: SparkSession):\n\t    df = create_empty_dataset(spark, Example)\n\t    assert df.schema[\"a\"] == StructField(\"a\", SparkMapType(StringType(), StringType()))\n\t    assert df.schema[\"b\"] == StructField(\"b\", SparkArrayType(StringType()))\n\t    structfields = [\n", "        StructField(\"a\", StringType(), True),\n\t        StructField(\"b\", StringType(), True),\n\t    ]\n\t    assert df.schema[\"c\"] == StructField(\"c\", SparkStructType(structfields))\n\tclass ArrayTypeSchema(Schema):\n\t    a: Column[ArrayType[StringType]]\n\tclass DifferentArrayTypeSchema(Schema):\n\t    a: Column[ArrayType[LongType]]\n\tclass MapTypeSchema(Schema):\n\t    a: Column[MapType[StringType, StringType]]\n", "class DifferentKeyMapTypeSchema(Schema):\n\t    a: Column[MapType[LongType, StringType]]\n\tclass DifferentValueMapTypeSchema(Schema):\n\t    a: Column[MapType[StringType, LongType]]\n\tclass DifferentSubSchema(Schema):\n\t    a: Column[LongType]\n\t    b: Column[LongType]\n\tclass StructTypeSchema(Schema):\n\t    a: Column[StructType[SubSchema]]\n\tclass DifferentStructTypeSchema(Schema):\n", "    a: Column[StructType[DifferentSubSchema]]\n\tdef test_complex_datatypes_not_equals(spark: SparkSession):\n\t    with pytest.raises(TypeError):\n\t        df1 = create_empty_dataset(spark, ArrayTypeSchema)\n\t        DataSet[DifferentArrayTypeSchema](df1)\n\t    df2 = create_empty_dataset(spark, MapTypeSchema)\n\t    with pytest.raises(TypeError):\n\t        DataSet[DifferentKeyMapTypeSchema](df2)\n\t    with pytest.raises(TypeError):\n\t        DataSet[DifferentValueMapTypeSchema](df2)\n", "    with pytest.raises(TypeError):\n\t        df3 = create_empty_dataset(spark, StructTypeSchema)\n\t        DataSet[DifferentStructTypeSchema](df3)\n"]}
{"filename": "tests/_core/test_metadata.py", "chunked_list": ["from typing import Annotated\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import LongType\n\tfrom typedspark import Column, DataSet, Schema, create_empty_dataset\n\tfrom typedspark._core.column_meta import ColumnMeta\n\tclass A(Schema):\n\t    a: Annotated[Column[LongType], ColumnMeta(comment=\"test\")]\n\t    b: Column[LongType]\n\tdef test_add_schema_metadata(spark: SparkSession):\n\t    df: DataSet[A] = create_empty_dataset(spark, A, 1)\n", "    assert df.schema[\"a\"].metadata == {\"comment\": \"test\"}\n\t    assert df.schema[\"b\"].metadata == {}\n\tclass B(Schema):\n\t    a: Column[LongType]\n\t    b: Annotated[Column[LongType], ColumnMeta(comment=\"test\")]\n\tdef test_refresh_metadata(spark: SparkSession):\n\t    df_a = create_empty_dataset(spark, A, 1)\n\t    df_b = DataSet[B](df_a)\n\t    assert df_b.schema[\"a\"].metadata == {}\n\t    assert df_b.schema[\"b\"].metadata == {\"comment\": \"test\"}\n"]}
{"filename": "tests/_core/test_dataset.py", "chunked_list": ["import pandas as pd\n\timport pytest\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import LongType, StringType\n\tfrom typedspark import Column, DataSet, Schema\n\tfrom typedspark._utils.create_dataset import create_empty_dataset\n\tclass A(Schema):\n\t    a: Column[LongType]\n\t    b: Column[StringType]\n\tdef create_dataframe(spark: SparkSession, d):\n", "    return spark.createDataFrame(pd.DataFrame(d))\n\tdef test_dataset(spark: SparkSession):\n\t    d = dict(\n\t        a=[1, 2, 3],\n\t        b=[\"a\", \"b\", \"c\"],\n\t    )\n\t    df = create_dataframe(spark, d)\n\t    DataSet[A](df)\n\tdef test_dataset_allow_underscored_columns_not_in_schema(spark: SparkSession):\n\t    d = {\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"], \"__c\": [1, 2, 3]}\n", "    df = create_dataframe(spark, d)\n\t    DataSet[A](df)\n\tdef test_dataset_single_underscored_column_should_raise(spark: SparkSession):\n\t    d = {\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"], \"_c\": [1, 2, 3]}\n\t    df = create_dataframe(spark, d)\n\t    with pytest.raises(TypeError):\n\t        DataSet[A](df)\n\tdef test_dataset_missing_colnames(spark: SparkSession):\n\t    d = dict(\n\t        a=[1, 2, 3],\n", "    )\n\t    df = create_dataframe(spark, d)\n\t    with pytest.raises(TypeError):\n\t        DataSet[A](df)\n\tdef test_dataset_too_many_colnames(spark: SparkSession):\n\t    d = dict(\n\t        a=[1, 2, 3],\n\t        b=[\"a\", \"b\", \"c\"],\n\t        c=[1, 2, 3],\n\t    )\n", "    df = create_dataframe(spark, d)\n\t    with pytest.raises(TypeError):\n\t        DataSet[A](df)\n\tdef test_wrong_type(spark: SparkSession):\n\t    d = dict(\n\t        a=[1, 2, 3],\n\t        b=[1, 2, 3],\n\t    )\n\t    df = create_dataframe(spark, d)\n\t    with pytest.raises(TypeError):\n", "        DataSet[A](df)\n\tdef test_inherrited_functions(spark: SparkSession):\n\t    df = create_empty_dataset(spark, A)\n\t    df.distinct()\n\t    df.filter(A.a == 1)\n\t    df.orderBy(A.a)\n\t    df.transform(lambda df: df)\n\tdef test_inherrited_functions_with_other_dataset(spark: SparkSession):\n\t    df_a = create_empty_dataset(spark, A)\n\t    df_b = create_empty_dataset(spark, A)\n", "    df_a.join(df_b, A.a.str)\n\t    df_a.unionByName(df_b)\n"]}
{"filename": "tests/_core/test_column.py", "chunked_list": ["import pandas as pd\n\timport pytest\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.functions import lit\n\tfrom pyspark.sql.types import LongType, StringType\n\tfrom typedspark import Column, Schema\n\tfrom typedspark._utils.create_dataset import create_partially_filled_dataset\n\tclass A(Schema):\n\t    a: Column[LongType]\n\t    b: Column[StringType]\n", "def test_column(spark: SparkSession):\n\t    (\n\t        spark.createDataFrame(\n\t            pd.DataFrame(\n\t                dict(\n\t                    a=[1, 2, 3],\n\t                )\n\t            )\n\t        )\n\t        .filter(A.a == 1)\n", "        .withColumn(A.b.str, lit(\"a\"))\n\t    )\n\tdef test_column_doesnt_exist():\n\t    with pytest.raises(TypeError):\n\t        A.z\n\t@pytest.mark.no_spark_session\n\tdef test_column_reference_without_spark_session():\n\t    a = A.a\n\t    assert a.str == \"a\"\n\tdef test_column_with_deprecated_dataframe_param(spark: SparkSession):\n", "    df = create_partially_filled_dataset(spark, A, {A.a: [1, 2, 3]})\n\t    Column(\"a\", dataframe=df)\n"]}
{"filename": "tests/_schema/test_create_spark_schema.py", "chunked_list": ["import pytest\n\tfrom pyspark.sql.types import LongType, StringType, StructField, StructType\n\tfrom typedspark import Column, Schema\n\tclass A(Schema):\n\t    a: Column[LongType]\n\t    b: Column[StringType]\n\tclass B(Schema):\n\t    a: Column\n\tdef test_create_spark_schema():\n\t    result = A.get_structtype()\n", "    expected = StructType(\n\t        [\n\t            StructField(\"a\", LongType(), True),\n\t            StructField(\"b\", StringType(), True),\n\t        ]\n\t    )\n\t    assert result == expected\n\tdef test_create_spark_schema_with_faulty_schema():\n\t    with pytest.raises(TypeError):\n\t        B.get_structtype()\n"]}
{"filename": "tests/_schema/test_get_schema_definition.py", "chunked_list": ["from typedspark._core.datatypes import DayTimeIntervalType\n\tfrom typedspark._core.literaltype import IntervalType\n\tfrom typedspark._schema.get_schema_definition import _replace_literal, _replace_literals\n\tdef test_replace_literal():\n\t    result = _replace_literal(\n\t        \"DayTimeIntervalType[Literal[0], Literal[1]]\",\n\t        replace_literals_in=DayTimeIntervalType,\n\t        original=\"Literal[0]\",\n\t        replacement=\"IntervalType.DAY\",\n\t    )\n", "    expected = \"DayTimeIntervalType[IntervalType.DAY, Literal[1]]\"\n\t    assert result == expected\n\tdef test_replace_literals():\n\t    result = _replace_literals(\n\t        \"DayTimeIntervalType[Literal[0], Literal[1]]\",\n\t        replace_literals_in=DayTimeIntervalType,\n\t        replace_literals_by=IntervalType,\n\t    )\n\t    expected = \"DayTimeIntervalType[IntervalType.DAY, IntervalType.HOUR]\"\n\t    assert result == expected\n"]}
{"filename": "tests/_schema/test_offending_schemas.py", "chunked_list": ["from typing import Annotated, List, Type\n\timport pytest\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import StringType\n\tfrom typedspark import ArrayType, Column, ColumnMeta, MapType, Schema, create_empty_dataset\n\tfrom typedspark._core.datatypes import DecimalType\n\tclass InvalidColumn(Schema):\n\t    a: int\n\tclass ColumnWithoutType(Schema):\n\t    a: Column\n", "class AnnotationWithoutColumn(Schema):\n\t    a: Annotated  # type: ignore\n\tclass InvalidColumnMeta(Schema):\n\t    a: Annotated[StringType, str]\n\tclass InvalidDataTypeWithinAnnotation(Schema):\n\t    a: Annotated[str, ColumnMeta()]  # type: ignore\n\tclass InvalidDataType(Schema):\n\t    a: Column[int]  # type: ignore\n\tclass ComplexTypeWithoutSubtype(Schema):\n\t    a: Column[ArrayType]\n", "class ComplexTypeWithInvalidSubtype(Schema):\n\t    a: Column[ArrayType[int]]  # type: ignore\n\tclass InvalidDataTypeWithArguments(Schema):\n\t    a: Column[List[str]]  # type: ignore\n\tclass DecimalTypeWithoutArguments(Schema):\n\t    a: Column[DecimalType]  # type: ignore\n\tclass DecimalTypeWithIncorrectArguments(Schema):\n\t    a: Column[DecimalType[int, int]]  # type: ignore\n\toffending_schemas: List[Type[Schema]] = [\n\t    InvalidColumn,\n", "    ColumnWithoutType,\n\t    AnnotationWithoutColumn,\n\t    InvalidColumnMeta,\n\t    InvalidDataTypeWithinAnnotation,\n\t    InvalidDataType,\n\t    ComplexTypeWithoutSubtype,\n\t    ComplexTypeWithInvalidSubtype,\n\t    InvalidDataTypeWithArguments,\n\t]\n\tdef test_offending_schema_exceptions(spark: SparkSession):\n", "    for schema in offending_schemas:\n\t        with pytest.raises(TypeError):\n\t            create_empty_dataset(spark, schema)\n\tdef test_offending_schemas_repr_exceptions():\n\t    for schema in offending_schemas:\n\t        schema.get_schema_definition_as_string(generate_imports=True)\n\tdef test_offending_schemas_dtype():\n\t    with pytest.raises(TypeError):\n\t        ColumnWithoutType.a.dtype\n\tdef test_offending_schemas_runtime_error_on_load():\n", "    with pytest.raises(TypeError):\n\t        class WrongNumberOfArguments(Schema):\n\t            a: Column[MapType[StringType]]  # type: ignore\n"]}
{"filename": "tests/_schema/test_schema.py", "chunked_list": ["from typing import Annotated, Literal, Type\n\timport pytest\n\tfrom pyspark.sql import SparkSession\n\tfrom pyspark.sql.types import LongType, StringType, StructField, StructType\n\timport typedspark\n\tfrom typedspark import Column, ColumnMeta, Schema, create_partially_filled_dataset\n\tfrom typedspark._core.literaltype import IntervalType\n\tfrom typedspark._schema.schema import DltKwargs\n\tclass A(Schema):\n\t    a: Column[LongType]\n", "    b: Column[StringType]\n\tschema_a_string = \"\"\"\n\tfrom pyspark.sql.types import LongType, StringType\n\tfrom typedspark import Column, Schema\n\tclass A(Schema):\n\t    a: Column[LongType]\n\t    b: Column[StringType]\n\t\"\"\"\n\tschema_a_string_with_documentation = '''from typing import Annotated\n\tfrom pyspark.sql.types import LongType, StringType\n", "from typedspark import Column, ColumnMeta, Schema\n\tclass A(Schema):\n\t    \"\"\"Add documentation here.\"\"\"\n\t    a: Annotated[Column[LongType], ColumnMeta(comment=\"\")]\n\t    b: Annotated[Column[StringType], ColumnMeta(comment=\"\")]\n\t'''\n\tclass B(Schema):\n\t    b: Column[LongType]\n\t    a: Column[StringType]\n\tclass Values(Schema):\n", "    a: Column[typedspark.DecimalType[Literal[38], Literal[18]]]\n\t    b: Column[StringType]\n\tclass ComplexDatatypes(Schema):\n\t    value: Column[typedspark.StructType[Values]]\n\t    items: Column[typedspark.ArrayType[StringType]]\n\t    consequences: Column[typedspark.MapType[StringType, typedspark.ArrayType[StringType]]]\n\t    diff: Column[typedspark.DayTimeIntervalType[IntervalType.DAY, IntervalType.SECOND]]\n\tschema_complex_datatypes = '''from typing import Annotated, Literal\n\tfrom pyspark.sql.types import StringType\n\tfrom typedspark import ArrayType, Column, ColumnMeta, DayTimeIntervalType, DecimalType, IntervalType, MapType, Schema, StructType\n", "class ComplexDatatypes(Schema):\n\t    \"\"\"Add documentation here.\"\"\"\n\t    value: Annotated[Column[StructType[test_schema.Values]], ColumnMeta(comment=\"\")]\n\t    items: Annotated[Column[ArrayType[StringType]], ColumnMeta(comment=\"\")]\n\t    consequences: Annotated[Column[MapType[StringType, ArrayType[StringType]]], ColumnMeta(comment=\"\")]\n\t    diff: Annotated[Column[DayTimeIntervalType[IntervalType.DAY, IntervalType.SECOND]], ColumnMeta(comment=\"\")]\n\tclass Values(Schema):\n\t    \"\"\"Add documentation here.\"\"\"\n\t    a: Annotated[Column[DecimalType[Literal[38], Literal[18]]], ColumnMeta(comment=\"\")]\n\t    b: Annotated[Column[StringType], ColumnMeta(comment=\"\")]\n", "'''  # noqa: E501\n\tclass PascalCase(Schema):\n\t    \"\"\"Schema docstring.\"\"\"\n\t    a: Annotated[Column[StringType], ColumnMeta(comment=\"some\")]\n\t    b: Annotated[Column[LongType], ColumnMeta(comment=\"other\")]\n\tdef test_all_column_names():\n\t    assert A.all_column_names() == [\"a\", \"b\"]\n\t    assert B.all_column_names() == [\"b\", \"a\"]\n\tdef test_all_column_names_except_for():\n\t    assert A.all_column_names_except_for([\"a\"]) == [\"b\"]\n", "    assert B.all_column_names_except_for([]) == [\"b\", \"a\"]\n\t    assert B.all_column_names_except_for([\"b\", \"a\"]) == []\n\tdef test_get_snake_case():\n\t    assert A.get_snake_case() == \"a\"\n\t    assert PascalCase.get_snake_case() == \"pascal_case\"\n\tdef test_get_docstring():\n\t    assert A.get_docstring() is None\n\t    assert PascalCase.get_docstring() == \"Schema docstring.\"\n\tdef test_get_structtype():\n\t    assert A.get_structtype() == StructType(\n", "        [StructField(\"a\", LongType(), True), StructField(\"b\", StringType(), True)]\n\t    )\n\t    assert PascalCase.get_structtype() == StructType(\n\t        [\n\t            StructField(\"a\", StringType(), metadata={\"comment\": \"some\"}),\n\t            StructField(\"b\", LongType(), metadata={\"comment\": \"other\"}),\n\t        ]\n\t    )\n\tdef test_get_dlt_kwargs():\n\t    assert A.get_dlt_kwargs() == DltKwargs(\n", "        name=\"a\",\n\t        comment=None,\n\t        schema=StructType(\n\t            [StructField(\"a\", LongType(), True), StructField(\"b\", StringType(), True)]\n\t        ),\n\t    )\n\t    assert PascalCase.get_dlt_kwargs() == DltKwargs(\n\t        name=\"pascal_case\",\n\t        comment=\"Schema docstring.\",\n\t        schema=StructType(\n", "            [\n\t                StructField(\"a\", StringType(), metadata={\"comment\": \"some\"}),\n\t                StructField(\"b\", LongType(), metadata={\"comment\": \"other\"}),\n\t            ]\n\t        ),\n\t    )\n\tdef test_repr():\n\t    assert repr(A) == schema_a_string\n\t@pytest.mark.parametrize(\n\t    \"schema, expected_schema_definition\",\n", "    [\n\t        (A, schema_a_string_with_documentation),\n\t        (ComplexDatatypes, schema_complex_datatypes),\n\t    ],\n\t)\n\tdef test_get_schema(schema: Type[Schema], expected_schema_definition: str):\n\t    schema_definition = schema.get_schema_definition_as_string(include_documentation=True)\n\t    assert schema_definition == expected_schema_definition\n\tdef test_dtype_attributes(spark: SparkSession):\n\t    assert ComplexDatatypes.value.dtype == typedspark.StructType[Values]\n", "    assert ComplexDatatypes.value.dtype.schema == Values\n\t    assert ComplexDatatypes.value.dtype.schema.b.dtype == StringType\n\t    df = create_partially_filled_dataset(\n\t        spark,\n\t        ComplexDatatypes,\n\t        {\n\t            ComplexDatatypes.value: create_partially_filled_dataset(\n\t                spark,\n\t                Values,\n\t                {\n", "                    Values.b: [\"a\", \"b\", \"c\"],\n\t                },\n\t            ).collect(),\n\t        },\n\t    )\n\t    assert df.filter(ComplexDatatypes.value.dtype.schema.b == \"b\").count() == 1\n"]}
{"filename": "tests/_schema/test_structfield.py", "chunked_list": ["from typing import Annotated, get_type_hints\n\timport pytest\n\tfrom pyspark.sql.types import BooleanType, LongType, StringType, StructField\n\tfrom typedspark import Column, ColumnMeta, Schema\n\tfrom typedspark._schema.structfield import (\n\t    _get_structfield_dtype,\n\t    get_structfield,\n\t    get_structfield_meta,\n\t)\n\tclass A(Schema):\n", "    a: Column[LongType]\n\t    b: Column[StringType]\n\t    c: Annotated[Column[StringType], ColumnMeta(comment=\"comment\")]\n\t    d: Annotated[Column[BooleanType], ColumnMeta(comment=\"comment2\")]\n\t@pytest.fixture()\n\tdef type_hints():\n\t    return get_type_hints(A, include_extras=True)\n\tdef test_get_structfield_dtype(type_hints):\n\t    assert _get_structfield_dtype(Column[LongType], \"a\") == LongType()\n\t    assert _get_structfield_dtype(type_hints[\"b\"], \"b\") == StringType()\n", "    assert (\n\t        _get_structfield_dtype(Annotated[Column[StringType], ColumnMeta(comment=\"comment\")], \"c\")\n\t        == StringType()\n\t    )\n\t    assert _get_structfield_dtype(type_hints[\"d\"], \"d\") == BooleanType()\n\tdef test_get_structfield_metadata(type_hints):\n\t    assert get_structfield_meta(Column[LongType]) == ColumnMeta()\n\t    assert get_structfield_meta(type_hints[\"b\"]) == ColumnMeta()\n\t    assert get_structfield_meta(\n\t        Annotated[Column[StringType], ColumnMeta(comment=\"comment\")]\n", "    ) == ColumnMeta(comment=\"comment\")\n\t    assert get_structfield_meta(type_hints[\"d\"]) == ColumnMeta(comment=\"comment2\")\n\tdef test_get_structfield(type_hints):\n\t    assert get_structfield(\"a\", Column[LongType]) == StructField(name=\"a\", dataType=LongType())\n\t    assert get_structfield(\"b\", type_hints[\"b\"]) == StructField(name=\"b\", dataType=StringType())\n\t    assert get_structfield(\n\t        \"c\", Annotated[Column[StringType], ColumnMeta(comment=\"comment\")]\n\t    ) == StructField(name=\"c\", dataType=StringType(), metadata={\"comment\": \"comment\"})\n\t    assert get_structfield(\"d\", type_hints[\"d\"]) == StructField(\n\t        name=\"d\", dataType=BooleanType(), metadata={\"comment\": \"comment2\"}\n", "    )\n"]}
{"filename": "docs/remove_metadata.py", "chunked_list": ["\"\"\"Removes the metadata from a notebook.\n\tAlso removes the spark warnings from cells where the sparksession is initialized.\n\t\"\"\"\n\timport sys\n\timport nbformat\n\tdef clear_metadata(cell):\n\t    \"\"\"Clears the metadata of a notebook cell.\"\"\"\n\t    cell.metadata = {}\n\tdef remove_spark_warnings(cell):\n\t    \"\"\"Removes the spark warnings from a notebook cell.\"\"\"\n", "    if \"outputs\" in cell.keys():\n\t        outputs = []\n\t        for output in cell.outputs:\n\t            if \"text\" in output.keys():\n\t                if 'Setting default log level to \"WARN\"' in output.text:\n\t                    continue\n\t                if (\n\t                    \"WARN NativeCodeLoader: Unable to load native-hadoop library for your platform.\"\n\t                    in output.text\n\t                ):\n", "                    continue\n\t                if \"WARN Utils: Service 'SparkUI' could not bind on port\" in output.text:\n\t                    continue\n\t            outputs.append(output)\n\t        cell.outputs = outputs\n\tdef remove_papermill_metadata(nb):\n\t    \"\"\"Removes the papermill metadata from a notebook.\"\"\"\n\t    if \"papermill\" in nb.metadata.keys():\n\t        nb.metadata.pop(\"papermill\")\n\tif __name__ == \"__main__\":\n", "    FILENAME = sys.argv[1]\n\t    nb = nbformat.read(FILENAME, as_version=4)\n\t    for nb_cell in nb[\"cells\"]:\n\t        clear_metadata(nb_cell)\n\t        remove_spark_warnings(nb_cell)\n\t    remove_papermill_metadata(nb)\n\t    nbformat.write(nb, FILENAME)\n"]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# For the full list of built-in configuration values, see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\tproject = \"typedspark\"\n\tcopyright = \"2023, Nanne Aben, Marijn Valk\"\n\tauthor = \"Nanne Aben, Marijn Valk\"\n\t# -- General configuration ---------------------------------------------------\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\textensions = [\"sphinx.ext.autodoc\", \"sphinx_rtd_theme\", \"nbsphinx\"]\n\ttemplates_path = [\"_templates\"]\n\texclude_patterns = []\n\t# -- Options for HTML output -------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\thtml_theme = \"sphinx_rtd_theme\"\n\thtml_static_path = [\"_static\"]\n\tautodoc_inherit_docstrings = False\n"]}
