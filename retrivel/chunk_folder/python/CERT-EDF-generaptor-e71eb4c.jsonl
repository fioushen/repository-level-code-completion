{"filename": "setup.py", "chunked_list": ["\"\"\"Setup script\n\t\"\"\"\n\tfrom setuptools import setup\n\tsetup(package_data={'generaptor': ['data/*']})\n"]}
{"filename": "generaptor/main.py", "chunked_list": ["#!/usr/bin/env python3\n\t\"\"\"Application\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tfrom .command import setup_commands\n\tfrom .__version__ import version\n\tfrom .helper.cache import Cache\n\tfrom .helper.logging import LOGGER\n\tdef _parse_args():\n\t    parser = ArgumentParser(\n", "        description=\"Generate Velociraptor-based collectors in no time\"\n\t    )\n\t    parser.add_argument(\n\t        '--cache',\n\t        type=Cache,\n\t        default=Cache(),\n\t        help=\"Set cache directory\",\n\t    )\n\t    cmd = parser.add_subparsers(dest='cmd')\n\t    cmd.required = True\n", "    setup_commands(cmd)\n\t    return parser.parse_args()\n\tdef app():\n\t    \"\"\"Application entry point\"\"\"\n\t    LOGGER.info(\"Generaptor v%s\", version)\n\t    args = _parse_args()\n\t    args.func(args)\n\tif __name__ == '__main__':\n\t    app()\n"]}
{"filename": "generaptor/__init__.py", "chunked_list": ["\"\"\"Generaptor module\n\t\"\"\"\n"]}
{"filename": "generaptor/data/linux.generate.targets.py", "chunked_list": ["#!/usr/bin/env python3\n\t\"\"\"Generate linux.targets.csv from linux.rules.csv\n\t\"\"\"\n\tfrom csv import reader, writer, QUOTE_MINIMAL\n\tfrom json import dumps\n\tfrom pathlib import Path\n\tfrom collections import defaultdict\n\tHERE = Path(__file__).resolve().parent\n\tRULES = HERE / 'linux.rules.csv'\n\tTARGETS = HERE / 'linux.targets.csv'\n", "DELIMITER = ','\n\tQUOTECHAR = '\"'\n\tdef app():\n\t    # parse rules\n\t    group_ruleids = defaultdict(set)\n\t    all_ruleids = set()\n\t    with RULES.open(newline='') as rules_fp:\n\t        csv_reader = reader(\n\t            rules_fp,\n\t            delimiter=DELIMITER,\n", "            quotechar=QUOTECHAR,\n\t        )\n\t        next(csv_reader)  # skip header\n\t        for row in csv_reader:\n\t            rule_id = int(row[0])\n\t            all_ruleids.add(rule_id)\n\t            group_ruleids[row[2]].add(rule_id)\n\t    # write targets\n\t    with TARGETS.open('w', newline='') as targets_fp:\n\t        csv_writer = writer(\n", "            targets_fp,\n\t            delimiter=DELIMITER,\n\t            quotechar=QUOTECHAR,\n\t            quoting=QUOTE_MINIMAL,\n\t        )\n\t        csv_writer.writerow(['Group', 'RuleIds'])\n\t        for group, ruleids in group_ruleids.items():\n\t            csv_writer.writerow([group, dumps(list(ruleids))])\n\t        csv_writer.writerow(['LinuxTriage', list(all_ruleids)])\n\tif __name__ == '__main__':\n", "    app()\n"]}
{"filename": "generaptor/command/get_secret.py", "chunked_list": ["\"\"\"get-secret command\n\t\"\"\"\n\tfrom pathlib import Path\n\tfrom ..helper.crypto import RSAPrivateKey, decrypt_secret, load_private_key\n\tfrom ..helper.logging import LOGGER\n\tfrom ..helper.collection import collection_metadata\n\tdef _print_collection_secret(private_key: RSAPrivateKey, collection: Path):\n\t    metadata = collection_metadata(collection)\n\t    if not metadata:\n\t        LOGGER.error(\"failed to retrieve metadata from collection.\")\n", "        return\n\t    for field in ('b64_enc_secret', 'fingerprint_hex'):\n\t        if field not in metadata:\n\t            LOGGER.error(\"metadata field not found: %s\", field)\n\t            return\n\t    LOGGER.info(\n\t        \"collection certificate fingerprint: %s\", metadata['fingerprint_hex']\n\t    )\n\t    b64_enc_secret = metadata['b64_enc_secret']\n\t    secret = decrypt_secret(private_key, b64_enc_secret)\n", "    print(f\"{secret.decode()}:{collection}\")\n\tdef _get_secret_cmd(args):\n\t    private_key = load_private_key(args.private_key)\n\t    if not private_key:\n\t        return\n\t    for collection in args.collections:\n\t        if collection.is_file():\n\t            _print_collection_secret(private_key, collection)\n\t            continue\n\t        if collection.is_dir():\n", "            for item in collection.glob('Collection_*.zip'):\n\t                _print_collection_secret(private_key, item)\n\t            continue\n\t        LOGGER.warning(\"skipped %s\", collection)\n\tdef setup_get_secret(cmd):\n\t    \"\"\"Setup get-secret command\"\"\"\n\t    get_secret = cmd.add_parser(\n\t        'get-secret', help=\"get the collection archive secret\"\n\t    )\n\t    get_secret.add_argument(\n", "        'private_key',\n\t        type=Path,\n\t        help=\"private key, given collections must share the same certificate fingerprint\",\n\t    )\n\t    get_secret.add_argument(\n\t        'collections',\n\t        metavar='collection',\n\t        nargs='+',\n\t        type=Path,\n\t        help=\"collection archives\",\n", "    )\n\t    get_secret.set_defaults(func=_get_secret_cmd)\n"]}
{"filename": "generaptor/command/__init__.py", "chunked_list": ["\"\"\"Command module\n\t\"\"\"\n\tfrom .refresh import setup_refresh\n\tfrom .generate import setup_generate\n\tfrom .get_secret import setup_get_secret\n\tfrom .get_fingerprint import setup_get_fingerprint\n\tdef setup_commands(cmd):\n\t    \"\"\"Setup commands\"\"\"\n\t    setup_refresh(cmd)\n\t    setup_generate(cmd)\n", "    setup_get_secret(cmd)\n\t    setup_get_fingerprint(cmd)\n"]}
{"filename": "generaptor/command/get_fingerprint.py", "chunked_list": ["\"\"\"get-fingerprint command\n\t\"\"\"\n\tfrom pathlib import Path\n\tfrom ..helper.logging import LOGGER\n\tfrom ..helper.collection import collection_metadata\n\tdef _print_collection_fingerprint(collection: Path):\n\t    metadata = collection_metadata(collection)\n\t    if not metadata:\n\t        LOGGER.error(\"failed to retrieve metadata from collection.\")\n\t        return\n", "    if 'fingerprint_hex' not in metadata:\n\t        LOGGER.error(\"metadata field not found: fingerprint_hex\")\n\t        return\n\t    fingerprint_hex = metadata['fingerprint_hex']\n\t    print(f\"{fingerprint_hex}:{collection}\")\n\tdef _get_fingerprint_cmd(args):\n\t    for collection in args.collections:\n\t        if collection.is_file():\n\t            _print_collection_fingerprint(collection)\n\t            continue\n", "        if collection.is_dir():\n\t            for item in collection.glob('Collection_*.zip'):\n\t                _print_collection_fingerprint(item)\n\t            continue\n\t        LOGGER.warning(\"skipped %s\", collection)\n\tdef setup_get_fingerprint(cmd):\n\t    \"\"\"Setup get-fingerprint command\"\"\"\n\t    get_fingerprint = cmd.add_parser(\n\t        'get-fingerprint',\n\t        help=\"get the collection archive certificate fingerprint\",\n", "    )\n\t    get_fingerprint.add_argument(\n\t        'collections',\n\t        metavar='collection',\n\t        nargs='+',\n\t        type=Path,\n\t        help=\"collection archives\",\n\t    )\n\t    get_fingerprint.set_defaults(func=_get_fingerprint_cmd)\n"]}
{"filename": "generaptor/command/refresh.py", "chunked_list": ["\"\"\"refresh command\n\t\"\"\"\n\tfrom ..helper.http import http_set_proxies, http_download\n\tfrom ..helper.prompt import confirm\n\tfrom ..helper.github import github_release\n\tfrom ..helper.logging import LOGGER\n\tfrom ..helper.distrib import SUPPORTED_DISTRIBUTIONS\n\tdef _refresh_cmd(args):\n\t    if not (args.yes or confirm(\"Refreshing cache will flush current cache.\")):\n\t        return\n", "    LOGGER.info(\"refreshing cache...\")\n\t    args.cache.flush(args.refresh_config, args.do_not_fetch)\n\t    args.cache.ensure()\n\t    if args.do_not_fetch:\n\t        return\n\t    LOGGER.info(\"downloading %s release...\", args.fetch_tag)\n\t    if args.proxy_url:\n\t        http_set_proxies({'https': args.proxy_url})\n\t    gh_release = github_release('velocidex', 'velociraptor', args.fetch_tag)\n\t    LOGGER.info(\"velociraptor release matched: %s\", gh_release.tag)\n", "    downloaded = set()\n\t    for asset in gh_release.assets:\n\t        for distrib in SUPPORTED_DISTRIBUTIONS:\n\t            if distrib in downloaded:\n\t                continue\n\t            if not distrib.match_asset_name(asset.name):\n\t                continue\n\t            downloaded.add(distrib)\n\t            LOGGER.info(\n\t                \"%s matched asset '%s' (size=%d)\",\n", "                distrib,\n\t                asset.name,\n\t                asset.size,\n\t            )\n\t            http_download(asset.url, args.cache.path(asset.url.split('/')[-1]))\n\tdef setup_refresh(cmd):\n\t    \"\"\"Setup refresh command\"\"\"\n\t    refresh = cmd.add_parser('refresh', help=\"refresh environment cache\")\n\t    refresh.add_argument(\n\t        '--yes', '-y', action='store_true', help=\"non-interactive confirmation\"\n", "    )\n\t    refresh.add_argument(\n\t        '--refresh-config',\n\t        action='store_true',\n\t        help=\"refresh configuration files as well\",\n\t    )\n\t    refresh.add_argument(\n\t        '--do-not-fetch',\n\t        action='store_true',\n\t        help=\"do not fetch latest velociraptor release\",\n", "    )\n\t    refresh.add_argument(\n\t        '--fetch-tag',\n\t        default='v0.6.9',\n\t        help=(\n\t            \"fetch this tag, use 'latest' to fetch the latest version, warning:\"\n\t            \" fecthing another version than the default might break the collector\"\n\t        ),\n\t    )\n\t    refresh.add_argument('--proxy-url', help=\"set proxy url\")\n", "    refresh.set_defaults(func=_refresh_cmd)\n"]}
{"filename": "generaptor/command/generate.py", "chunked_list": ["\"\"\"generate command\n\t\"\"\"\n\tfrom json import loads\n\tfrom pathlib import Path\n\tfrom ..helper.crypto import provide_x509_certificate\n\tfrom ..helper.logging import LOGGER\n\tfrom ..helper.distrib import Distribution, Architecture, OperatingSystem\n\tfrom ..helper.validation import check_device\n\tfrom ..helper.generation import Generator\n\tdef _load_custom_profile_targets(custom_profile):\n", "    try:\n\t        custom_profile = loads(custom_profile.read_text())\n\t        return custom_profile.get('targets')\n\t    except:\n\t        LOGGER.exception(\"failed to load custom profile!\")\n\t    return None\n\tdef _select_default_targets(args, default_targets):\n\t    if args.custom:\n\t        return None\n\t    if args.custom_profile and args.custom_profile.is_file():\n", "        targets = _load_custom_profile_targets(args.custom_profile)\n\t        if not targets:\n\t            LOGGER.warning(\n\t                \"failed to load custom profile, using default targets instead.\"\n\t            )\n\t            return default_targets\n\t        return targets\n\t    return default_targets\n\tdef _generate_linux_cmd(args):\n\t    LOGGER.info(\"starting linux collector generator...\")\n", "    default_targets = _select_default_targets(args, ['LinuxTriage'])\n\t    if not check_device(args.device):\n\t        return\n\t    try:\n\t        certificate = provide_x509_certificate(\n\t            args.output_directory,\n\t            args.x509_certificate,\n\t            args.ask_password,\n\t        )\n\t    except KeyboardInterrupt:\n", "        print()\n\t        LOGGER.warning(\"operation canceled.\")\n\t        return\n\t    artifacts = ['Linux.Collector']\n\t    Generator(\n\t        Distribution(OperatingSystem.LINUX, Architecture(args.architecture)),\n\t        args.cache,\n\t        certificate,\n\t        args.output_directory,\n\t    ).generate(\n", "        {\n\t            'device': args.device,\n\t            'artifacts': ','.join([f'\"{artifact}\"' for artifact in artifacts]),\n\t        },\n\t        default_targets,\n\t    )\n\tdef _generate_windows_cmd(args):\n\t    LOGGER.info(\"starting windows collector generator...\")\n\t    default_targets = _select_default_targets(args, ['KapeTriage'])\n\t    if not check_device(args.device):\n", "        return\n\t    if args.device and not args.device.endswith(':'):\n\t        LOGGER.warning(\"assuming device name is '%s:'\", args.device)\n\t        args.device += ':'\n\t    try:\n\t        certificate = provide_x509_certificate(\n\t            args.output_directory,\n\t            args.x509_certificate,\n\t            args.ask_password,\n\t        )\n", "    except KeyboardInterrupt:\n\t        print()\n\t        LOGGER.warning(\"operation canceled.\")\n\t        return\n\t    artifacts = ['Windows.Collector']\n\t    Generator(\n\t        Distribution(OperatingSystem.WINDOWS, Architecture(args.architecture)),\n\t        args.cache,\n\t        certificate,\n\t        args.output_directory,\n", "    ).generate(\n\t        {\n\t            'device': args.device,\n\t            'artifacts': ','.join([f'\"{artifact}\"' for artifact in artifacts]),\n\t            'use_auto_accessor': 'N' if args.no_auto_accessor else 'Y',\n\t            'vss_analysis': 'N' if args.no_vss_analysis else 'Y',\n\t            'dont_be_lazy': 'Y' if args.dont_be_lazy else 'N',\n\t        },\n\t        default_targets,\n\t    )\n", "def setup_generate(cmd):\n\t    \"\"\"Setup generate command\"\"\"\n\t    generate = cmd.add_parser('generate', help=\"generate a collector\")\n\t    generate.add_argument(\n\t        '--custom',\n\t        '-c',\n\t        action='store_true',\n\t        help=\"enable collector targets customization (interactive)\",\n\t    )\n\t    generate.add_argument(\n", "        '--custom-profile',\n\t        '--cp',\n\t        type=Path,\n\t        help=\"use given customization profile (non interactive)\",\n\t    )\n\t    generate.add_argument(\n\t        '--output-directory',\n\t        '-o',\n\t        type=Path,\n\t        default=Path('output').resolve(),\n", "        help=\"set output folder\",\n\t    )\n\t    generate.add_argument(\n\t        '--x509-certificate',\n\t        '-x',\n\t        type=Path,\n\t        help=\"bring your own x509 certificate instead of generating one \"\n\t        \"(must be RSA)\",\n\t    )\n\t    generate.add_argument(\n", "        '--ask-password',\n\t        '-p',\n\t        action='store_true',\n\t        help=\"prompt for private key password instead of generating one or \"\n\t        \"reading GENERAPTOR_PK_SECRET environment variable (ignored if \"\n\t        \"--x509 is used)\",\n\t    )\n\t    target = generate.add_subparsers(dest='target')\n\t    target.required = True\n\t    linux = target.add_parser('linux', help=\"generate linux collector\")\n", "    linux.set_defaults(func=_generate_linux_cmd)\n\t    linux.add_argument(\n\t        '--architecture',\n\t        '-a',\n\t        default=Architecture.AMD64.value,\n\t        choices=[arch.value for arch in Architecture],\n\t        help=\"set released binary architecture\",\n\t    )\n\t    linux.add_argument(\n\t        '--device',\n", "        '-d',\n\t        default='',\n\t        help=\"set root directory (absolute path), empty means '/'\",\n\t    )\n\t    windows = target.add_parser('windows', help=\"Generate windows collector\")\n\t    windows.set_defaults(func=_generate_windows_cmd)\n\t    windows.add_argument(\n\t        '--architecture',\n\t        '-a',\n\t        default=Architecture.AMD64.value,\n", "        choices=[arch.value for arch in Architecture],\n\t        help=\"set released binary architecture\",\n\t    )\n\t    windows.add_argument(\n\t        '--device',\n\t        '-d',\n\t        default='',\n\t        help=\"set root directory (absolute path), empty means all filesystems\",\n\t    )\n\t    windows.add_argument(\n", "        '--no-auto-accessor',\n\t        action='store_true',\n\t        help=\"disable auto accessor (which automatically select fastest collection technique)\",\n\t    )\n\t    windows.add_argument(\n\t        '--no-vss-analysis',\n\t        action='store_true',\n\t        help=\"disable windows volume shadow copies analysis\",\n\t    )\n\t    windows.add_argument(\n", "        '--dont-be-lazy',\n\t        action='store_true',\n\t        help=\"disable lazy_ntfs accessor (which uses OS api calls to collect files)\",\n\t    )\n"]}
{"filename": "generaptor/helper/collection.py", "chunked_list": ["\"\"\"Collection archive helper\n\t\"\"\"\n\tfrom json import loads\n\tfrom zipfile import ZipFile\n\tfrom pathlib import Path\n\tdef collection_metadata(archive: Path):\n\t    \"\"\"Load JSON metadata from archive\"\"\"\n\t    with ZipFile(archive) as zipf:\n\t        try:\n\t            zipinf = zipf.getinfo('metadata.json')\n", "        except KeyError:\n\t            return None\n\t        data = zipf.read(zipinf)\n\t        metadata_objects = loads(data.decode())\n\t        return metadata_objects[0]\n"]}
{"filename": "generaptor/helper/distrib.py", "chunked_list": ["\"\"\"Distribution\n\t\"\"\"\n\tfrom enum import Enum\n\tfrom dataclasses import dataclass\n\tclass Architecture(Enum):\n\t    \"\"\"Software architecture\"\"\"\n\t    X86 = '386'\n\t    AMD64 = 'amd64'\n\t    AMD64_MUSL = 'amd64-musl'\n\t    ARM64 = 'arm64'\n", "class OperatingSystem(Enum):\n\t    \"\"\"Operating system\"\"\"\n\t    WINDOWS = 'windows'\n\t    DARWIN = 'darwin'\n\t    LINUX = 'linux'\n\t@dataclass\n\tclass Distribution:\n\t    \"\"\"Combination of operating system and architecture\"\"\"\n\t    os: OperatingSystem\n\t    arch: Architecture\n", "    def __hash__(self):\n\t        return hash((self.os, self.arch))\n\t    @property\n\t    def suffix(self):\n\t        \"\"\"Filename suffix\"\"\"\n\t        suffix = '-'.join([self.os.value, self.arch.value])\n\t        suffix += '.exe' if self.os == OperatingSystem.WINDOWS else ''\n\t        return suffix\n\t    def match_asset_name(self, name: str):\n\t        \"\"\"Determine if asset name matches this distribution\"\"\"\n", "        return name.endswith(self.suffix)\n\tSUPPORTED_DISTRIBUTIONS = [\n\t    Distribution(OperatingSystem.LINUX, Architecture.AMD64),\n\t    Distribution(OperatingSystem.LINUX, Architecture.AMD64_MUSL),\n\t    Distribution(OperatingSystem.WINDOWS, Architecture.X86),\n\t    Distribution(OperatingSystem.WINDOWS, Architecture.AMD64),\n\t]\n"]}
{"filename": "generaptor/helper/cache.py", "chunked_list": ["\"\"\"Cache helpers\n\t\"\"\"\n\timport typing as t\n\tfrom csv import reader\n\tfrom json import loads\n\tfrom shutil import copy\n\tfrom pathlib import Path\n\tfrom platform import system, architecture\n\tfrom dataclasses import dataclass\n\tfrom jinja2 import FileSystemLoader, Environment, Template\n", "from .logging import LOGGER\n\tfrom .distrib import Distribution, OperatingSystem, Architecture\n\tHERE = Path(__file__).resolve()\n\tPKG_DATA_DIR = HERE.parent.parent / 'data'\n\tPLATFORM_DISTRIB_MAP = {\n\t    'Linux': Distribution(OperatingSystem.LINUX, Architecture.AMD64),\n\t    'Windows': Distribution(OperatingSystem.WINDOWS, Architecture.AMD64),\n\t}\n\tdef _stream_csv(csv_filepath: Path):\n\t    with csv_filepath.open(newline='') as csv_fp:\n", "        csv_reader = reader(csv_fp, delimiter=',', quotechar='\"')\n\t        try:\n\t            next(csv_reader)  # skip csv header\n\t        except StopIteration:\n\t            return\n\t        for row in csv_reader:\n\t            yield row\n\tdef _copy_pkg_data_to_cache(pattern, cache_dir):\n\t    for src_path in PKG_DATA_DIR.glob(pattern):\n\t        dst_path = cache_dir / src_path.name\n", "        if not dst_path.is_file():\n\t            copy(src_path, dst_path)\n\t@dataclass\n\tclass Cache:\n\t    \"\"\"Cache directory\"\"\"\n\t    directory: Path = Path.home() / '.cache' / 'generaptor'\n\t    @property\n\t    def program(self):\n\t        \"\"\"Cache program directory\"\"\"\n\t        return self.directory / 'program'\n", "    def path(self, filename: str) -> Path:\n\t        \"\"\"Generate program path for filename\"\"\"\n\t        filepath = (self.program / filename).resolve()\n\t        if not filepath.is_relative_to(self.program):\n\t            LOGGER.warning(\"path traversal attempt!\")\n\t            return None\n\t        return filepath\n\t    def flush(self, update_config=False, do_not_fetch=False):\n\t        \"\"\"Flush cached config and/or programs\"\"\"\n\t        if self.program.is_dir() and not do_not_fetch:\n", "            for filepath in self.program.iterdir():\n\t                filepath.unlink()\n\t        if self.directory.is_dir() and update_config:\n\t            for filepath in self.directory.iterdir():\n\t                if not filepath.is_file():\n\t                    continue\n\t                filepath.unlink()\n\t    def ensure(self) -> bool:\n\t        \"\"\"Ensure that the cache directory is valid and mandatory files are present\"\"\"\n\t        # attempt to create directory anyway\n", "        self.program.mkdir(parents=True, exist_ok=True)\n\t        # copy configuration templates\n\t        _copy_pkg_data_to_cache('*.collector.yml', self.directory)\n\t        # copy targets datasets\n\t        _copy_pkg_data_to_cache('*.targets.csv', self.directory)\n\t        # copy rules datasets\n\t        _copy_pkg_data_to_cache('*.rules.csv', self.directory)\n\t        return True\n\t    def load_rules(self, distrib: Distribution):\n\t        \"\"\"Load rules from cache matching given distribution\"\"\"\n", "        filepath = self.directory / f'{distrib.os.value}.rules.csv'\n\t        rules = {int(row[0]): row[1:] for row in _stream_csv(filepath)}\n\t        LOGGER.info(\"loaded %d rules.\", len(rules))\n\t        return rules\n\t    def load_targets(self, distrib: Distribution):\n\t        \"\"\"Load targets from cache matching given distribution\"\"\"\n\t        filepath = self.directory / f'{distrib.os.value}.targets.csv'\n\t        targets = {}\n\t        for row in _stream_csv(filepath):\n\t            targets[row[0]] = set(loads(row[1]))\n", "        LOGGER.info(\"loaded %d targets.\", len(targets))\n\t        return targets\n\t    def template_config(self, distrib: Distribution) -> Template:\n\t        \"\"\"Load jinja template matching given distribution\"\"\"\n\t        loader = FileSystemLoader(self.directory)\n\t        environment = Environment(\n\t            loader=loader,\n\t            autoescape=False,  # worst case scenario: we generate invalid YAML\n\t            trim_blocks=False,\n\t            lstrip_blocks=False,\n", "            keep_trailing_newline=True,\n\t        )\n\t        return environment.get_template(f'{distrib.os.value}.collector.yml')\n\t    def template_binary(self, distrib: Distribution) -> t.Optional[Path]:\n\t        \"\"\"Return template binary for distrib\"\"\"\n\t        try:\n\t            return next(self.program.glob(f'*{distrib.suffix}'))\n\t        except StopIteration:\n\t            LOGGER.critical(\n\t                \"distribution file not found in cache! Please update the cache.\"\n", "            )\n\t            return None\n\t    def platform_binary(self) -> t.Optional[Path]:\n\t        \"\"\"Platform binary to be used to produce collectors\"\"\"\n\t        if architecture()[0] != '64bit':\n\t            LOGGER.critical(\"current machine architecture is not supported!\")\n\t            return None\n\t        distrib = PLATFORM_DISTRIB_MAP.get(system())\n\t        if not distrib:\n\t            LOGGER.critical(\"current machine distribution is not supported!\")\n", "            return None\n\t        return self.template_binary(distrib)\n"]}
{"filename": "generaptor/helper/__init__.py", "chunked_list": ["\"\"\"Helper package\n\t\"\"\"\n"]}
{"filename": "generaptor/helper/prompt.py", "chunked_list": ["\"\"\"Prompt helpers\n\t\"\"\"\n\tfrom pick import pick\n\tdef confirm(warning):\n\t    \"\"\"Prompt for confirmation\"\"\"\n\t    print(warning)\n\t    try:\n\t        answer = input(\"Do you want to proceed? [yes/NO]: \")\n\t    except KeyboardInterrupt:\n\t        return False\n", "    return answer.lower() == 'yes'\n\tdef multiselect(title, options):\n\t    \"\"\"Pick multiselection wrapper\"\"\"\n\t    return [\n\t        option\n\t        for option, _ in pick(\n\t            options, title, multiselect=True, min_selection_count=1\n\t        )\n\t    ]\n"]}
{"filename": "generaptor/helper/github.py", "chunked_list": ["\"\"\"Github helpers\n\t\"\"\"\n\timport typing as t\n\tfrom dataclasses import dataclass\n\tfrom .http import http_get_json\n\t@dataclass\n\tclass GithubAsset:\n\t    \"\"\"Github asset data\"\"\"\n\t    name: str\n\t    size: int\n", "    url: str\n\t@dataclass\n\tclass GithubRelease:\n\t    \"\"\"Github release data\"\"\"\n\t    name: str\n\t    tag: str\n\t    assets: t.List[GithubAsset]\n\t    @classmethod\n\t    def from_dict(cls, dct):\n\t        return cls(\n", "            name=dct['name'],\n\t            tag=dct['tag_name'],\n\t            assets=[\n\t                GithubAsset(\n\t                    name=asset['name'],\n\t                    size=asset['size'],\n\t                    url=asset['browser_download_url']\n\t                )\n\t                for asset in dct['assets']\n\t            ],\n", "        )\n\tdef github_release(\n\t    owner: str, repository: str, tag: str = 'latest'\n\t) -> GithubRelease:\n\t    \"\"\"Get a summary of the latest release published in a Github repository\"\"\"\n\t    page = 1\n\t    while page:\n\t        url = f'https://api.github.com/repos/{owner}/{repository}/releases?per_page=10&page={page}'\n\t        releases = http_get_json(url)\n\t        if not releases:\n", "            return None\n\t        for release in releases:\n\t            if release['draft'] or release['prerelease']:\n\t                continue\n\t            if tag == 'latest':\n\t                return GithubRelease.from_dict(release)\n\t            if tag == release['tag_name']:\n\t                return GithubRelease.from_dict(release)\n\t        page += 1\n\t    return None\n"]}
{"filename": "generaptor/helper/generation.py", "chunked_list": ["\"\"\"Generation\n\t\"\"\"\n\tfrom io import StringIO\n\tfrom csv import writer, QUOTE_MINIMAL\n\tfrom pathlib import Path\n\tfrom platform import system\n\tfrom datetime import datetime\n\tfrom subprocess import run\n\tfrom .cache import Cache\n\tfrom .crypto import Certificate, fingerprint, pem_string\n", "from .prompt import multiselect\n\tfrom .logging import LOGGER\n\tfrom .distrib import Distribution\n\tdef _dump_file_globs(selected_rules):\n\t    imstr = StringIO()\n\t    csv_writer = writer(\n\t        imstr, delimiter=',', quotechar='\"', quoting=QUOTE_MINIMAL\n\t    )\n\t    for rule in selected_rules:\n\t        csv_writer.writerow(rule[2:4])\n", "    file_globs = imstr.getvalue()\n\t    imstr.close()\n\t    return file_globs\n\tclass Generator:\n\t    \"\"\"Generate configuration file and velociraptor pre-configured binary\"\"\"\n\t    def __init__(\n\t        self,\n\t        distrib: Distribution,\n\t        cache: Cache,\n\t        certificate: Certificate,\n", "        output_directory: Path,\n\t    ):\n\t        self._distrib = distrib\n\t        self._cache = cache\n\t        self._certificate = certificate\n\t        self._output_directory = output_directory\n\t    def _select_globs(self, default_targets=None):\n\t        rules = self._cache.load_rules(self._distrib)\n\t        targets = self._cache.load_targets(self._distrib)\n\t        title = \"Pick one or more collection targets\"\n", "        options = list(sorted(targets.keys()))\n\t        selected_targets = default_targets\n\t        if not selected_targets:\n\t            selected_targets = multiselect(title, options)\n\t        selected_indices = set()\n\t        LOGGER.info(\"generating for targets:\")\n\t        for target in selected_targets:\n\t            indices = targets[target]\n\t            LOGGER.info(\" * %s (%d rules)\", target, len(indices))\n\t            selected_indices.update(indices)\n", "        return _dump_file_globs(rules[index] for index in selected_indices)\n\t    def _generate_config(self, context, output_config):\n\t        with output_config.open('wb') as fstream:\n\t            template = self._cache.template_config(self._distrib)\n\t            stream = template.stream(context)\n\t            stream.dump(fstream, encoding='utf-8')\n\t    def generate(self, context, default_targets=None):\n\t        \"\"\"Generate a configuration file and a pre-configured binary\"\"\"\n\t        # check platform binary availability\n\t        platform_binary = self._cache.platform_binary()\n", "        if not platform_binary:\n\t            LOGGER.critical(\"unsupported platform!\")\n\t            return\n\t        if system() == 'Linux':\n\t            platform_binary.chmod(0o700)\n\t        try:\n\t            file_globs = self._select_globs(default_targets)\n\t        except KeyboardInterrupt:\n\t            LOGGER.warning(\"operation canceled by user.\")\n\t            return\n", "        context.update(\n\t            {\n\t                'cert_data_pem_str': pem_string(self._certificate),\n\t                'cert_fingerprint_hex': fingerprint(self._certificate),\n\t                'file_globs': file_globs,\n\t            }\n\t        )\n\t        self._output_directory.mkdir(parents=True, exist_ok=True)\n\t        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n\t        output_config = self._output_directory / f'collector-{timestamp}.yml'\n", "        output_binary = (\n\t            self._output_directory\n\t            / f'collector-{timestamp}-{self._distrib.suffix}'\n\t        )\n\t        template_binary = self._cache.template_binary(self._distrib)\n\t        LOGGER.info(\"generating configuration...\")\n\t        self._generate_config(context, output_config)\n\t        LOGGER.info(\"configuration written to: %s\", output_config)\n\t        LOGGER.info(\"generating release binary...\")\n\t        args = [\n", "            str(platform_binary),\n\t            'config',\n\t            'repack',\n\t            '--exe',\n\t            str(template_binary),\n\t            str(output_config),\n\t            str(output_binary),\n\t        ]\n\t        LOGGER.info(\"running command: %s\", args)\n\t        run(args, check=True)\n", "        LOGGER.info(\"release binary written to: %s\", output_binary)\n"]}
{"filename": "generaptor/helper/http.py", "chunked_list": ["\"\"\"HTTP helpers\n\t\"\"\"\n\tfrom json import load, JSONDecodeError\n\tfrom shutil import copyfileobj\n\tfrom pathlib import Path\n\tfrom urllib.request import (\n\t    install_opener,\n\t    build_opener,\n\t    urlopen,\n\t    ProxyHandler,\n", ")\n\tfrom rich.progress import wrap_file\n\tfrom .logging import LOGGER\n\tdef http_set_proxies(proxies):\n\t    \"\"\"Configure proxies\"\"\"\n\t    LOGGER.info(\"using proxies %s\", proxies)\n\t    install_opener(build_opener(ProxyHandler(proxies)))\n\tdef http_download(url: str, filepath: Path):\n\t    \"\"\"Download a resource and store it inside a file\"\"\"\n\t    LOGGER.info(\"downloading from %s\", url)\n", "    with urlopen(url) as response:\n\t        size = int(response.headers['Content-Length'])\n\t        with wrap_file(\n\t            response, total=size, description=\"Downloading\"\n\t        ) as wrapped:\n\t            with filepath.open('wb') as file:\n\t                copyfileobj(wrapped, file)\n\tdef http_get_json(url: str):\n\t    \"\"\"GET a JSON resource\"\"\"\n\t    LOGGER.info(\"requesting %s\", url)\n", "    with urlopen(url) as response:\n\t        if response.status != 200:\n\t            LOGGER.error(\"response status %d\", response.status)\n\t            return None\n\t        try:\n\t            return load(response)\n\t        except JSONDecodeError:\n\t            LOGGER.error(\"failed to decode json data!\")\n\t    return None\n"]}
{"filename": "generaptor/helper/crypto.py", "chunked_list": ["\"\"\"Cryptography helper\n\t\"\"\"\n\timport typing as t\n\tfrom os import getenv\n\tfrom base64 import b64decode\n\tfrom pathlib import Path\n\tfrom getpass import getpass\n\tfrom secrets import token_urlsafe\n\tfrom datetime import datetime, timedelta\n\tfrom cryptography.x509 import (\n", "    load_pem_x509_certificate,\n\t    random_serial_number,\n\t    SubjectAlternativeName,\n\t    CertificateBuilder,\n\t    NameAttribute,\n\t    Certificate,\n\t    DNSName,\n\t    Name,\n\t)\n\tfrom cryptography.x509.oid import NameOID\n", "from cryptography.hazmat.primitives.hashes import SHA256, SHA512\n\tfrom cryptography.hazmat.primitives.serialization import (\n\t    load_pem_private_key,\n\t    BestAvailableEncryption,\n\t    PrivateFormat,\n\t    Encoding,\n\t)\n\tfrom cryptography.hazmat.primitives.asymmetric.rsa import (\n\t    generate_private_key,\n\t    RSAPrivateKey,\n", ")\n\tfrom cryptography.hazmat.primitives.asymmetric.padding import OAEP, MGF1\n\tfrom .logging import LOGGER\n\tVALIDITY = timedelta(days=30)\n\tRSA_KEY_SIZE = 4096\n\tRSA_PUBLIC_EXPONENT = 65537\n\tdef fingerprint(certificate: Certificate):\n\t    \"\"\"Certificate SHA256 fingerprint\"\"\"\n\t    return certificate.fingerprint(SHA256()).hex()\n\tdef pem_string(certificate: Certificate):\n", "    \"\"\"Certificate as a PEM string\"\"\"\n\t    crt_pem_bytes = certificate.public_bytes(Encoding.PEM)\n\t    crt_pem_string = crt_pem_bytes.decode()\n\t    crt_pem_string = crt_pem_string.replace('\\n', '\\\\n')\n\t    return crt_pem_string\n\tdef _provide_private_key_secret(\n\t    ask_password: bool = False, raise_if_generate: bool = False\n\t) -> str:\n\t    # attempt to load the secret from the environment\n\t    private_key_secret = getenv('GENERAPTOR_PK_SECRET')\n", "    # interactively ask the user for the secret if necessary\n\t    if not private_key_secret and ask_password:\n\t        private_key_secret = getpass(\"private key secret: \")\n\t    # generate and display the secret if necessary\n\t    if not private_key_secret:\n\t        if raise_if_generate:\n\t            raise ValueError(\"failed to provide private key secret\")\n\t        private_key_secret = token_urlsafe(16)\n\t        LOGGER.warning(\"private key secret is %s\", private_key_secret)\n\t        LOGGER.warning(\"store this secret in a vault please!\")\n", "    return private_key_secret\n\tdef _generate_self_signed_certificate(\n\t    output_directory: Path, ask_password: bool = False\n\t) -> Certificate:\n\t    LOGGER.info(\"generating private key... please wait...\")\n\t    private_key = generate_private_key(\n\t        public_exponent=RSA_PUBLIC_EXPONENT,\n\t        key_size=RSA_KEY_SIZE,\n\t    )\n\t    subject_name = issuer_name = Name(\n", "        [\n\t            NameAttribute(NameOID.COMMON_NAME, \"generaptor\"),\n\t        ]\n\t    )\n\t    utc_now = datetime.utcnow()\n\t    LOGGER.info(\"generating certificate...\")\n\t    certificate = (\n\t        CertificateBuilder()\n\t        .subject_name(\n\t            subject_name,\n", "        )\n\t        .issuer_name(\n\t            issuer_name,\n\t        )\n\t        .public_key(\n\t            private_key.public_key(),\n\t        )\n\t        .serial_number(\n\t            random_serial_number(),\n\t        )\n", "        .not_valid_before(\n\t            utc_now,\n\t        )\n\t        .not_valid_after(\n\t            utc_now + VALIDITY,\n\t        )\n\t        .add_extension(\n\t            SubjectAlternativeName([DNSName(\"generaptor\")]),\n\t            critical=False,\n\t        )\n", "        .sign(private_key, SHA256())\n\t    )\n\t    # ensure output directory exists\n\t    output_directory.mkdir(parents=True, exist_ok=True)\n\t    # retrieve private key password\n\t    private_key_secret = _provide_private_key_secret(ask_password=ask_password)\n\t    # store encrypted private key in a file\n\t    fingerprint_hex = fingerprint(certificate)\n\t    (output_directory / f'{fingerprint_hex}.key.pem').write_bytes(\n\t        private_key.private_bytes(\n", "            encoding=Encoding.PEM,\n\t            format=PrivateFormat.TraditionalOpenSSL,\n\t            encryption_algorithm=BestAvailableEncryption(\n\t                private_key_secret.encode()\n\t            ),\n\t        ),\n\t    )\n\t    # store certificate in a file\n\t    crt_pem_bytes = certificate.public_bytes(Encoding.PEM)\n\t    (output_directory / f'{fingerprint_hex}.crt.pem').write_bytes(\n", "        crt_pem_bytes\n\t    )\n\t    return certificate\n\tdef provide_x509_certificate(\n\t    output_directory: Path,\n\t    cert_filepath: t.Optional[Path] = None,\n\t    ask_password: bool = False,\n\t) -> str:\n\t    \"\"\"Provide x509 certificate\"\"\"\n\t    if cert_filepath and cert_filepath.is_file():\n", "        crt_pem_bytes = cert_filepath.read_bytes()\n\t        certificate = load_pem_x509_certificate(crt_pem_bytes)\n\t        LOGGER.info(\n\t            \"using certificate %s fingerprint %s\",\n\t            cert_filepath,\n\t            fingerprint(certificate),\n\t        )\n\t    else:\n\t        certificate = _generate_self_signed_certificate(\n\t            output_directory, ask_password\n", "        )\n\t    return certificate\n\tdef load_private_key(private_key_path: Path) -> t.Optional[RSAPrivateKey]:\n\t    try:\n\t        private_key_secret = _provide_private_key_secret(\n\t            ask_password=True, raise_if_generate=True\n\t        )\n\t    except (ValueError, KeyboardInterrupt):\n\t        private_key_secret = None\n\t    if not private_key_secret:\n", "        LOGGER.warning(\"failed to provide private key secret\")\n\t        return None\n\t    return load_pem_private_key(\n\t        private_key_path.read_bytes(), private_key_secret.encode()\n\t    )\n\tdef decrypt_secret(private_key: RSAPrivateKey, b64_enc_secret: str) -> bytes:\n\t    \"\"\"Decrypt a base64-encoded secret using given private key\"\"\"\n\t    enc_secret = b64decode(b64_enc_secret)\n\t    secret = private_key.decrypt(\n\t        enc_secret,\n", "        OAEP(mgf=MGF1(algorithm=SHA512()), algorithm=SHA512(), label=None),\n\t    )\n\t    return secret\n"]}
{"filename": "generaptor/helper/validation.py", "chunked_list": ["\"\"\"Validation helpers\n\t\"\"\"\n\tfrom .logging import LOGGER\n\tdef check_device(device: str):\n\t    \"\"\"Check device name\"\"\"\n\t    if '\"' in device:\n\t        LOGGER.critical(\"device name cannot contain '\\\"'.\")\n\t        return False\n\t    return True\n"]}
{"filename": "generaptor/helper/logging.py", "chunked_list": ["\"\"\"Logging\n\t\"\"\"\n\tfrom logging import basicConfig, getLogger\n\tfrom rich.console import Console\n\tfrom rich.logging import RichHandler\n\tbasicConfig(\n\t    level='INFO',\n\t    format=\"%(message)s\",\n\t    datefmt=\"[%Y-%m-%dT%H:%M:%S]\",\n\t    handlers=[RichHandler(console=Console(stderr=True))],\n", ")\n\tLOGGER = getLogger('generaptor')\n"]}
