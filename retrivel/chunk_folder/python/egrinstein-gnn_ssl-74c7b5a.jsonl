{"filename": "train.py", "chunked_list": ["import hydra\n\timport pytorch_lightning as pl\n\timport torch.cuda as torch_gpu\n\t# import torch.backends.mps as torch_mps\n\tfrom hydra.utils import get_class\n\tfrom omegaconf import DictConfig\n\tfrom pytorch_lightning import loggers as pl_loggers\n\tfrom pytorch_lightning.callbacks import (\n\t    ModelCheckpoint, RichProgressBar, TQDMProgressBar,\n\t    EarlyStopping\n", ")\n\tfrom gnn_ssl.models.base.utils import load_checkpoint\n\tSAVE_DIR = \"logs/\"\n\tclass Trainer(pl.Trainer):\n\t    def __init__(self, config):\n\t        # 1. Load method (trainer type) specified in the config.yaml file\n\t        method = get_class(config[\"class\"])(config)\n\t        accelerator = \"cpu\"\n\t        if torch_gpu.is_available():\n\t            accelerator = \"cuda\"\n", "        # elif torch_mps.is_available():\n\t        #     accelerator = \"mps\"\n\t        # Currently disabled, as mps doesn't support fft\n\t        training_config = config[\"training\"]\n\t        strategy = training_config[\"multi_gpu_strategy\"]\n\t        n_devices = torch_gpu.device_count()\n\t        if (not training_config[\"multi_gpu\"]) or (n_devices <= 1):\n\t            # Strategy is only necessary when using multiple GPUs\n\t            strategy = None\n\t            n_devices = 1\n", "        if training_config[\"progress_bar_type\"] == \"rich\":\n\t            progress_bar = RichProgressBar(\n\t                refresh_rate=training_config[\"progress_bar_refresh_rate\"])\n\t        elif training_config[\"progress_bar_type\"] == \"tqdm\":\n\t            progress_bar = TQDMProgressBar(\n\t                refresh_rate=training_config[\"progress_bar_refresh_rate\"])\n\t        # Create callbacks (Progress bar, early stopping and weight saving)\n\t        callbacks=[\n\t            progress_bar,\n\t            ModelCheckpoint(monitor=\"validation_loss\",\n", "                            save_last=True,\n\t                            filename='weights-{epoch:02d}-{validation_loss:.2f}'\n\t            )\n\t        ]\n\t        early_stopping_config = training_config[\"early_stopping\"]\n\t        if early_stopping_config[\"enabled\"]:\n\t            callbacks.append(\n\t                EarlyStopping(early_stopping_config[\"key_to_monitor\"],\n\t                              early_stopping_config[\"min_delta\"],\n\t                              early_stopping_config[\"patience_in_epochs\"]\n", "                )\n\t            )\n\t        super().__init__(\n\t            max_epochs=training_config[\"n_epochs\"],\n\t            callbacks=callbacks,\n\t            logger=[pl_loggers.TensorBoardLogger(save_dir=SAVE_DIR)],\n\t            accelerator=accelerator,\n\t            strategy=strategy,\n\t            log_every_n_steps=25,\n\t            check_val_every_n_epoch=training_config[\"check_val_every_n_epoch\"],\n", "            devices=n_devices,\n\t        )\n\t        ckpt_path = config[\"inputs_train\"][\"checkpoint_path\"]\n\t        if ckpt_path is not None:\n\t            load_checkpoint(method.model, ckpt_path)\n\t        self._method = method\n\t        self.config = config\n\t    def fit(self, train_dataloaders, val_dataloaders=None):\n\t        super().fit(self._method, train_dataloaders,\n\t                    val_dataloaders=val_dataloaders)\n", "    def fit_multiple(self, train_dataloaders, val_dataloaders=None):\n\t        \"\"\"Fit the model sequentially for multiple datasets\"\"\"\n\t        for i, train_dataloader in enumerate(train_dataloaders):\n\t            val_dataloader = None\n\t            if val_dataloaders is not None:\n\t                val_dataloader = val_dataloaders[i]\n\t            super().fit(self._method, train_dataloader,\n\t                        val_dataloaders=val_dataloader)\n\t    def test(self, test_dataloaders):\n\t        super().test(self._method, test_dataloaders, ckpt_path=\"best\")\n", "def _create_torch_dataloaders(config):\n\t    dataset_paths = config[\"inputs_train\"][\"dataset_paths\"]\n\t    dataloader = get_class(config[\"dataset\"][\"class\"])\n\t    batch_size = config[\"training\"][\"batch_size\"]\n\t    n_workers = config[\"training\"][\"n_workers\"]\n\t    training_dataloader = dataloader(config, dataset_paths[\"training\"],\n\t                                     batch_size=batch_size, n_workers=n_workers, shuffle=True)\n\t    val_dataloader = None\n\t    test_dataloader = None\n\t    if dataset_paths[\"validation\"]:\n", "        val_dataloader = dataloader(config, dataset_paths[\"validation\"],\n\t                                    batch_size=batch_size, n_workers=n_workers)\n\t    if dataset_paths[\"test\"]:\n\t        test_dataloader = dataloader(config, dataset_paths[\"test\"],\n\t                                     batch_size=batch_size, n_workers=n_workers)\n\t    return (\n\t        training_dataloader,\n\t        val_dataloader,\n\t        test_dataloader\n\t    )\n", "@hydra.main(config_path=\"config\", config_name=\"config\", version_base=\"1.1\")\n\tdef main(config: DictConfig):\n\t    \"\"\"Runs the training procedure using Pytorch lightning\n\t    And tests the model with the best validation score against the test dataset. \n\t    Args:\n\t        config (DictConfig): Configuration automatically loaded by Hydra.\n\t                                        See the config/ directory for the configuration\n\t    \"\"\"\n\t    dataset_train, dataset_val, dataset_test = _create_torch_dataloaders(config)\n\t    trainer = Trainer(config)\n", "    if config[\"training\"][\"transfer_learning\"]:\n\t        trainer.fit_multiple(dataset_train, val_dataloaders=dataset_val)\n\t    else:\n\t        trainer.fit(dataset_train, val_dataloaders=dataset_val)\n\t    trainer.test(dataset_test)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "evaluate.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport hydra\n\timport pandas as pd\n\timport seaborn as sns\n\timport torch\n\tfrom hydra.utils import get_class\n\tfrom omegaconf import DictConfig\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm import tqdm\n\tfrom gnn_ssl.models.base.utils import load_checkpoint\n", "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tdef _compute_batch_metrics(batch, batch_idx, models, metrics):\n\t    x, y = batch[0]\n\t    x = _dict_to_device(x, DEVICE)\n\t    y = _dict_to_device(y, DEVICE)\n\t    results = {}\n\t    for model_name, model in models.items():\n\t        y_hat = model(x)\n\t        results[model_name] = {}\n\t        for metric_name, metric in metrics.items():\n", "            values = metric(y_hat, y)\n\t            if isinstance(values, dict): # It computed multiple metrics\n\t                results[model_name].update(values)\n\t            else:\n\t                results[model_name][metric_name] = values.detach().cpu()\n\t    return results\n\tdef _evaluate_dataset(dataset_name: str, config: DictConfig, models: dict, metrics: dict, batch_size: int):\n\t    print(f\"Evaluating models on dataset '{dataset_name}'\")\n\t    # 1. Load evaluation dataset\n\t    dataset_config = config[\"dataset\"]\n", "    inputs = config[\"inputs_eval\"]\n\t    dataloader = get_class(dataset_config[\"class\"])\n\t    dataset = dataloader(config, inputs[\"dataset_paths\"][dataset_name], batch_size=batch_size)\n\t    # Only evaluate on compatible models\n\t    models = {\n\t        model_name: model[\"model\"] for model_name, model in models.items()\n\t        if dataset_name in model[\"evaluate_on\"]\n\t    }\n\t    metrics = _compute_metrics(dataset_name, dataset, models, metrics)\n\t    return metrics\n", "def load_models(config):\n\t    evaluation_config = config[\"evaluation\"]\n\t    inputs = config[\"inputs_eval\"]\n\t    # 1. Load models\n\t    models = {}\n\t    for model_name, model_config in evaluation_config[\"models\"].items():\n\t        evaluate_on = model_config[\"evaluate_on\"]\n\t        model_config = model_config[\"model\"]\n\t        model = get_class(model_config[\"class\"])(model_config)\n\t        models[model_name] = {\n", "            \"model\": model.eval(),\n\t            \"evaluate_on\": evaluate_on\n\t        }\n\t    # 2. Load model checkpoints for trained methods (i.e. not classical signal processing ones)\n\t    for model_name, checkpoint_path in inputs[\"checkpoint_paths\"].items():\n\t        if model_name in models:\n\t            load_checkpoint(models[model_name][\"model\"], checkpoint_path)\n\t    return models\n\tdef _load_metrics(config):\n\t    metrics = {}\n", "    for metric_name, metric_config in config[\"evaluation\"][\"metrics\"].items():\n\t        metric_class = metric_config[\"class\"]\n\t        metric_config = metric_config[\"config\"] if \"config\" in metric_config else {}\n\t        metric = get_class(metric_class)(**metric_config)\n\t        metrics[metric_name] = metric\n\t    return metrics\n\tdef _compute_metrics(dataset_name: str, dataloader: DataLoader, models: dict, metrics: dict):\n\t    if not models:\n\t        return pd.DataFrame()\n\t    # 1. Compute metrics for all batches\n", "    batch_metrics = []\n\t    for i, batch in enumerate(tqdm(dataloader)):\n\t        batch_metrics.append(\n\t            _compute_batch_metrics(batch, i, models, metrics)\n\t        )\n\t    # 2. Group metrics for all batches\n\t    metrics = _group_batch_metrics(batch_metrics, dataset_name)\n\t    # 3. Compute aggregate metrics\n\t    _plot_histograms(metrics, dataset_name)\n\t    return metrics\n", "def _group_batch_metrics(batch_metrics: dict, dataset_name: str):\n\t    \"Group batch metrics into a Pandas dataframe\"\n\t    metrics = {}\n\t    df_metrics = []\n\t    model_names = list(batch_metrics[0].keys())\n\t    metric_names = list(batch_metrics[0][model_names[0]].keys())\n\t    for model_name in batch_metrics[0].keys():\n\t        metrics[model_name] = {}\n\t        metric_names = batch_metrics[0][model_name].keys()\n\t        for metric_name in metric_names:\n", "            errors = torch.cat([result[model_name][metric_name] for result in batch_metrics])\n\t            metrics[model_name][metric_name] = errors\n\t            df = pd.DataFrame.from_dict({\n\t                \"value\": errors.tolist(),\n\t                \"model\": [model_name]*len(errors),\n\t                \"metric\": [metric_name]*len(errors),\n\t                \"dataset\": [dataset_name]*len(errors)} ,orient='index').transpose()\n\t            df_metrics.append(df)\n\t    return pd.concat(df_metrics)\n\tdef _plot_histograms(df: pd.DataFrame, dataset_name: str):\n", "    \"\"\"Plot one histogram per metric, comparing all models for a given dataset\n\t    \"\"\"\n\t    # the first key in the dict is the model, the second is the metric\n\t    metric_names = df[\"metric\"].unique()\n\t    # Create a plot for each metric\n\t    for metric_name in metric_names:\n\t        fig, ax = plt.subplots()\n\t        ax.set_title(f\"{metric_name}\")\n\t        df_metric = df[df[\"metric\"] == metric_name]\n\t        try:\n", "            sns.histplot(\n\t                df_metric, x=\"value\",\n\t                hue=\"model\", multiple=\"layer\",\n\t                palette=\"Blues\",\n\t                ax=ax\n\t            )\n\t        except TypeError:\n\t            continue\n\t        ax.set_xlabel(\"Error (m)\")\n\t        ax.set_ylabel(\"Num. test cases\")\n", "        plt.savefig(f\"outputs/{dataset_name}_{metric_name}.pdf\")\n\t@hydra.main(config_path=\"config\", config_name=\"config\", version_base=None)\n\tdef main(config: DictConfig):\n\t    \"\"\"Evaluate the model and produce histograms\n\t    Args:\n\t        config (DictConfig): Configuration automatically loaded by Hydra.\n\t                                        See the config/ directory for the configuration\n\t    \"\"\"\n\t    # 1. Load models and checkpoints\n\t    models = load_models(config)\n", "    # 2. Load metrics\n\t    metrics = _load_metrics(config)\n\t    # 3. Evaluate metrics for every model, for each dataset\n\t    inputs = config[\"inputs_eval\"]\n\t    dataset_names = list(inputs[\"dataset_paths\"].keys())\n\t    batch_size = config[\"evaluation\"][\"batch_size\"]\n\t    results = []\n\t    for dataset_name in dataset_names:\n\t        dataset_results = _evaluate_dataset(dataset_name, config, models, metrics, batch_size)\n\t        results.append(dataset_results)\n", "    results = pd.concat(results, ignore_index=True)\n\t    results.to_csv(\"results.csv\")\n\tdef _dict_to_device(d, device):\n\t    new_d = {}\n\t    for key, value in d.items():\n\t        if isinstance(value, torch.Tensor):\n\t            new_d[key] = value.to(device)\n\t        elif isinstance(value, dict):\n\t            new_d[key] = _dict_to_device(value, device)\n\t    return d\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "plots.py", "chunked_list": ["# Plots for the interspeech2023 submission\n\timport matplotlib as mpl\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport pandas as pd\n\tdef make_plots(results_csv_path):\n\t    results = pd.read_csv(results_csv_path)\n\t    # TODO: Make lots of \"group by\" instead enumerating specifics.\n\t    # Create a \"dataset_type\" parameter and a \"model_type\" parameter\n\t    # Which would allow grouping of datasets of the same class (4 mics, for example)\n", "    # and methods of the same class (CRNN for example).\n\t    dataset_names = list(results[\"dataset\"].unique())\n\t    model_names = list(results[\"model\"].unique())\n\t    # 1. Results for individual datasets\n\t    for dataset_name in dataset_names:\n\t        df = results[results[\"dataset\"] == dataset_name]\n\t        _plot_histogram(df, dataset_name, \"model\")\n\t    # 2. Results for number of mics\n\t    for dataset_group in [\"4 mics\", \"6 mics\"]:\n\t        df = results[results[\"dataset\"].str.contains(dataset_group)]\n", "        _plot_histogram(df, dataset_group, \"model\")\n\t    # 3. Results for reverb/recorded\n\t    for dataset_group in [\"reverb\", \"recorded\"]:\n\t        df = results[results[\"dataset\"].str.contains(dataset_group)]\n\t        _plot_histogram(df, dataset_group, \"model\")\n\t    # 4. Results for individual methods\n\t    for model_name in model_names:\n\t        df = results[results[\"model\"] == model_name]\n\t        _plot_histogram(df, model_name, \"dataset\")\n\t# def _plot_histogram(df, dataset_name, hue, ax=None):\n", "#     if not ax:\n\t#         fig, ax = plt.subplots()\n\t#     sns.histplot(\n\t#         df, x=\"value\",\n\t#         hue=hue, multiple=\"dodge\",\n\t#         palette=\"Blues\",\n\t#         ax=ax,\n\t#         bins=15\n\t#     )\n\t#     ax.set_title(dataset_name)\n", "#     ax.set_xlabel(\"Error (m)\")\n\t#     ax.set_ylabel(\"Num. test cases\")\n\t#     plt.savefig(f\"outputs/{dataset_name}.pdf\")\n\tdef _plot_histogram(df, dataset_name, group_by, ax=None):\n\t    fig, ax = plt.subplots()\n\t    cmap = mpl.cm.get_cmap('Set2')\n\t    ax.set_title(f\"{group_by}\")\n\t    groups = df[group_by].unique()\n\t    n_groups = len(groups)\n\t    for i, key in enumerate(groups):\n", "        color = cmap((i+1)/n_groups)\n\t        values = df[df[group_by] == key][\"value\"]\n\t        counts, bins = np.histogram(values, bins=30)\n\t        ax.stairs(counts, bins, label=key, color=color)\n\t        try:\n\t            mean = values.mean()\n\t            ax.axvline(x=mean, color=color, alpha=0.2, linestyle=\"--\",\n\t                        label=\"mean={:.2f} m\".format(mean))\n\t        except RuntimeError:\n\t            pass\n", "    ax.legend()\n\t    ax.set_xlabel(\"Error (m)\")\n\t    ax.set_ylabel(\"Num. test cases\")\n\t    plt.savefig(f\"outputs/{dataset_name}.pdf\")\n\tdef make_tables(results_csv_path):\n\t    results = pd.read_csv(results_csv_path)\n\t    df_mean = []\n\t    dataset_names = results[\"dataset\"].unique()\n\t    model_names = results[\"model\"].unique()\n\t    for dataset_name in dataset_names:\n", "        df_dataset = results[results[\"dataset\"] == dataset_name]\n\t        for model_name in model_names:\n\t            df_model = df_dataset[df_dataset[\"model\"] == model_name]\n\t            df_mean.append({\n\t                \"Dataset\": dataset_name,\n\t                \"Model\": model_name,\n\t                \"Value\": df_model[\"value\"].mean(),\n\t                \"Std.\": df_model[\"value\"].std()\n\t            })\n\t    df_mean = pd.DataFrame(df_mean)\n", "    df_mean.to_csv(\"means.csv\")\n\tif __name__ == \"__main__\":\n\t    make_plots(\"results.csv\")\n\t    make_tables(\"results.csv\")\n"]}
{"filename": "visualize_outputs.py", "chunked_list": ["import hydra\n\tfrom hydra.utils import get_class\n\tfrom omegaconf import DictConfig\n\tfrom gnn_ssl.visualization import create_visualizations\n\tfrom evaluate import load_models\n\t@hydra.main(config_path=\"config\", config_name=\"config\", version_base=None)\n\tdef main(config: DictConfig):\n\t    \"\"\"Produce visualization of the model output\n\t    Args:\n\t        config (DictConfig): Configuration automatically loaded by Hydra.\n", "                                        See the config/ directory for the configuration\n\t    \"\"\"\n\t    # 1. Load evaluation dataset\n\t    dataloader = get_class(config[\"dataset\"][\"class\"])\n\t    inputs = config[\"inputs_eval\"]\n\t    dataset_names = list(inputs[\"dataset_paths\"].keys()) \n\t    dataset_path = inputs[\"dataset_paths\"][dataset_names[0]] # Only select first dataset right now, change this in future\n\t    dataset_test = dataloader(config, dataset_path, shuffle=False)\n\t    # 2. Load models\n\t    models = load_models(config)\n", "    # 3. Load loss function\n\t    loss = get_class(config[\"targets\"][\"loss_class\"])(config[\"targets\"])\n\t    # Compute outputs for a single batch\n\t    x, y = batch = next(iter(dataset_test))[0]\n\t    model_outputs = {\n\t        model_name: model[\"model\"](x)\n\t        for model_name, model in models.items()\n\t    }\n\t    create_visualizations(model_outputs, y, loss, plot_target=False)\n\tif __name__ == \"__main__\":\n", "    main()\n"]}
{"filename": "gnn_ssl/metrics.py", "chunked_list": ["from turtle import forward\n\timport torch\n\tfrom torch.nn import Module\n\timport pysoundloc.pysoundloc.metrics as ssl_metrics\n\tfrom pysoundloc.pysoundloc.target_grids import (\n\t    create_target_gaussian_grids, create_target_hyperbolic_grids\n\t)\n\tclass Loss(Module):\n\t    def __init__(self, config, dim=None):\n\t        super().__init__()\n", "        self.config = config\n\t        self.loss = NormLoss(config[\"loss\"], dim=dim)\n\t        self.weighted = config[\"weighted_loss\"]\n\t        self.network_output_type = config[\"type\"]\n\t        if self.network_output_type == \"grid\":\n\t            self.grid_generator = LikelihoodGrid(\n\t                config[\"sigma\"],\n\t                config[\"n_points_per_axis\"],\n\t                config[\"hyperbolic_likelihood_weight\"],\n\t                config[\"gaussian_likelihood_weight\"],\n", "                normalize=config[\"normalize\"],\n\t                squared=config[\"squared\"]\n\t            )\n\t    def forward(self, model_output, targets, mean_reduce=True):\n\t        # 1. Prepare targets\n\t        if self.network_output_type == \"grid\":\n\t            targets = self.grid_generator(\n\t                targets[\"room_dims\"][..., :2], # :2 is because it's a 2-D grid\n\t                targets[\"source_coordinates\"][..., :2],\n\t                targets[\"mic_coordinates\"][..., :2],\n", "            )\n\t        elif self.network_output_type == \"source_coordinates\":\n\t            targets = targets[\"source_coordinates\"]\n\t        model_output = model_output[self.network_output_type]\n\t        # 2. Assert they have the same shape as output\n\t        if model_output.shape != targets.shape:\n\t            raise ValueError(\n\t                \"Model output's shape is {}, target's is {}\".format(\n\t                    model_output.shape, targets.shape\n\t            ))\n", "        # Compute loss\n\t        loss = self.loss(model_output, targets)\n\t        if self.weighted:\n\t            loss *= targets\n\t        if mean_reduce:\n\t            if self.weighted:\n\t                loss = loss.sum()/loss.shape[0]\n\t            else:\n\t                loss = loss.mean()\n\t        return loss\n", "class LikelihoodGrid(Module):\n\t    def __init__(self, sigma=1, n_points_per_axis=25,\n\t                 hyperbolic_weight=0.5, gaussian_weight=0.5, normalize=True,\n\t                 squared=False):\n\t        super().__init__()\n\t        self.sigma = sigma\n\t        self.n_points_per_axis = n_points_per_axis\n\t        self.hyperbolic_weight = hyperbolic_weight\n\t        self.gaussian_weight = gaussian_weight\n\t        self.normalize = normalize\n", "        self.squared = squared\n\t    def forward(self, room_dims, source_coordinates, mic_coordinates=None):\n\t        batch_size = room_dims.shape[0]\n\t        if mic_coordinates.shape[1] == 2:\n\t            # Mixed grid is only used for two microphones\n\t            gaussian_weight = self.gaussian_weight\n\t            hyperbolic_weight = self.hyperbolic_weight\n\t        else:\n\t            # Compute gaussian grid only\n\t            gaussian_weight = 1\n", "            hyperbolic_weight = 0\n\t        grids = []\n\t        if gaussian_weight > 0:\n\t            grids.append(\n\t                create_target_gaussian_grids(\n\t                        source_coordinates, room_dims,\n\t                        self.n_points_per_axis,\n\t                        self.sigma,\n\t                        squared=self.squared)*gaussian_weight\n\t            )\n", "        if hyperbolic_weight > 0:\n\t            if mic_coordinates is None:\n\t                raise ValueError(\n\t                    \"mic_coordinates must be provided when computing hyperbolic grid\")\n\t            grids.append(\n\t                create_target_hyperbolic_grids(\n\t                    source_coordinates,\n\t                    mic_coordinates[:, 0], mic_coordinates[:, 1],\n\t                    room_dims, self.n_points_per_axis,\n\t                    self.sigma,\n", "                    squared=self.squared)*hyperbolic_weight\n\t            )\n\t        grids = torch.stack(grids)\n\t        grids = grids.sum(dim=0)\n\t        if self.normalize:\n\t            max_grids = grids.abs().flatten(start_dim=1).max(dim=1)[0]\n\t            min_grids = grids.abs().flatten(start_dim=1).min(dim=1)[0]\n\t            max_grids = max_grids.unsqueeze(dim=1).unsqueeze(dim=2)\n\t            min_grids = min_grids.unsqueeze(dim=1).unsqueeze(dim=2)\n\t            # perform min-max normalization\n", "            grids = (grids - min_grids)/(max_grids - min_grids)\n\t        return grids\n\tclass NormLoss(Module):\n\t    def __init__(self, norm_type=\"l1\", dim=1, key=None):\n\t        super().__init__()\n\t        if norm_type not in [\"l1\", \"l2\", \"squared\"]:\n\t            raise ValueError(\"Supported norms are 'l1', 'l2', and 'squared'\")\n\t        self.norm_type = norm_type\n\t        self.dim = dim\n\t        self.key = key\n", "    def forward(self, model_output, targets, mean_reduce=False):\n\t        # targets = targets.to(model_output.device)\n\t        if self.key:\n\t            model_output = model_output[self.key]\n\t            targets = targets[self.key]\n\t        error = model_output - targets\n\t        if self.norm_type == \"l1\":\n\t            normed_error = error.abs()\n\t        elif self.norm_type == \"l2\" or self.norm_type == \"squared\":\n\t            normed_error = error**2\n", "        if self.dim is not None: # Sum along the vector dimension\n\t            normed_error = torch.sum(normed_error, dim=self.dim)\n\t        if self.norm_type == \"l2\":\n\t            normed_error = torch.sqrt(normed_error)\n\t        if mean_reduce:\n\t            normed_error = normed_error.mean()\n\t        return normed_error\n\tclass SourceDistance(NormLoss):\n\t    def __init__(self):\n\t        super().__init__(\"l2\", 1, \"source_coordinates\")\n", "    def forward(self, model_output, targets):\n\t        if len(targets[\"source_coordinates\"].shape) == 3: \n\t            targets[\"source_coordinates\"] = targets[\"source_coordinates\"][:, 0, :2]\n\t        return super().forward(model_output, targets)\n\tclass SslMetrics(ssl_metrics.SslMetrics):\n\t    def forward(self, model_output, targets):\n\t        model_output = model_output[\"source_coordinates\"]\n\t        room_dims = targets[\"room_dims\"]\n\t        targets = targets[\"source_coordinates\"][:, :2]\n\t        return super().forward(model_output, targets, room_dims)\n", "class ExampleLoss(torch.nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.config = config\n\t    def forward(self, model_output, targets, mean_reduce=True):\n\t        x = model_output[\"example_output\"]\n\t        # Don't do anything with targets, just return the output\n\t        if mean_reduce:\n\t            x = x.mean()\n\t        return x\n"]}
{"filename": "gnn_ssl/__init__.py", "chunked_list": []}
{"filename": "gnn_ssl/visualization.py", "chunked_list": ["import matplotlib.pyplot as plt\n\tfrom pysoundloc.pysoundloc.visualization import plot_grid\n\tdef create_visualizations(model_outputs, y, loss, plot_target=False):\n\t    # This function is specific for the NeuralSrp method\n\t    model_names = list(model_outputs.keys())\n\t    n_models = len(model_names)\n\t    batch_size = model_outputs[model_names[0]][\"grid\"].shape[0]\n\t    mic_coords = y[\"mic_coordinates\"][:, :, :2]\n\t    room_dims = y[\"room_dims\"][..., :2]\n\t    source_coords = y[\"source_coordinates\"][..., :2]\n", "    target_grids = loss.grid_generator(\n\t            room_dims,\n\t            source_coords,\n\t            mic_coords,\n\t    )\n\t    n_plots = n_models\n\t    if plot_target:\n\t        n_plots +=1\n\t    for i in range(batch_size):\n\t        fig, axs = plt.subplots(nrows=n_plots, figsize=(5, 5))\n", "        # Plot model outputs\n\t        for j, model_name in enumerate(model_names):\n\t            plot_grid(model_outputs[model_name][\"grid\"][i].detach(),\n\t                      room_dims[i], source_coords=source_coords[i],\n\t                      microphone_coords=mic_coords[i], log=False, ax=axs[j])\n\t            axs[j].set_title(model_name)\n\t            if j < n_plots - 1:\n\t                axs[j].get_xaxis().set_visible(False)\n\t        # Plot target\n\t        if plot_target:\n", "            plot_grid(target_grids[i], room_dims[i], source_coords=source_coords[i],\n\t                    microphone_coords=mic_coords[i], log=False, ax=axs[n_models])\n\t            axs[n_models].set_title(\"Target grid\")\n\t        plt.tight_layout()\n\t        plt.savefig(f\"{i}.pdf\")\n"]}
{"filename": "gnn_ssl/feature_extractors/metadata.py", "chunked_list": ["import torch\n\tfrom copy import deepcopy\n\tdef format_metadata(metadata, use_rt60_as_metadata=False):\n\t    mic_coords = metadata[\"mic_coordinates\"]\n\t    if len(mic_coords.shape) == 3: # Multi device\n\t        mic_coords = mic_coords[0]\n\t    room_dims = metadata[\"room_dims\"]\n\t    m = {\n\t        \"local\": {\n\t            \"mic_coordinates\": mic_coords\n", "        },\n\t        \"global\": {\n\t            \"room_dims\": room_dims,\n\t        }\n\t    }\n\t    if use_rt60_as_metadata:\n\t        m[\"global\"][\"rt60\"] = torch.Tensor([metadata[\"rt60\"]])\n\t    return m\n\tdef flatten_metadata(metadata, metadata_dim=3):\n\t    \"\"\"Transform the metadata dictionary where the values are batches of metadata\n", "        into a batch of 1D torch vectors.\n\t    Args:\n\t        metadata (dict): Dictionary containing the keys [\"local\"][\"mic_coords\"]\n\t                         [\"global\"][\"room_dims\"] and [\"global\"][\"rt60\"] (optional)\n\t        metadata_dim (int): Whether to give 3D or 2D mic coordinates and room dims \n\t    Returns:\n\t        torch.Tensor: a matrix with flattened metadata for each batch\n\t    \"\"\"\n\t    mic_coords = metadata[\"local\"][\"mic_coordinates\"][:, :, :metadata_dim]\n\t    room_dims = metadata[\"global\"][\"room_dims\"][:, :metadata_dim]\n", "    flattened_metadata = torch.cat([\n\t        mic_coords.flatten(start_dim=1),\n\t        room_dims\n\t    ], dim=1)\n\t    if \"rt60\" in metadata[\"global\"].keys():\n\t        flattened_metadata = torch.cat([\n\t            flattened_metadata,\n\t            metadata[\"global\"][\"rt60\"]], dim=1\n\t        )\n\t    return flattened_metadata\n", "def filter_local_metadata(metadata, idxs):\n\t    result = deepcopy(metadata)\n\t    result[\"local\"][\"mic_coordinates\"] = result[\"local\"][\"mic_coordinates\"][:, idxs] \n\t    return result\n"]}
{"filename": "gnn_ssl/feature_extractors/__init__.py", "chunked_list": ["import math\n\timport torch\n\timport torch.nn as nn\n\tfrom omegaconf import OmegaConf\n\tfrom .stft import CrossSpectra, StftArray\n\tclass FeatureExtractor(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.config = OmegaConf.to_object(config)\n\t        self.n_input_seconds = config[\"dataset\"][\"n_input_seconds\"]\n", "        self.config = config\n\t        feature_name = config[\"type\"]\n\t        self.n_output_channels = 2\n\t        if feature_name.startswith(\"stft\"):\n\t            if feature_name == \"stft\":\n\t                self.n_output_channels = 4 # 2 mics, each with a real and imaginary channel\n\t                mag_only = phase_only = real_only = False\n\t            elif feature_name == \"stft_phase\":\n\t                mag_only = real_only = False\n\t                phase_only = True\n", "            elif feature_name == \"stft_mag\":\n\t                phase_only = real_only = False\n\t                mag_only = True\n\t            elif feature_name == \"stft_real\":\n\t                phase_only = mag_only = False\n\t                real_only = True\n\t            else:\n\t                raise ValueError(f\"{feature_name} is not a valid feature extractor\")\n\t            self.model = StftArray(\n\t                is_complex=False, complex_as_channels=True,\n", "                real_only=real_only, mag_only=mag_only, phase_only=phase_only,\n\t                n_dft=config[\"n_dft\"], hop_size=config[\"hop_size\"]\n\t            )\n\t        elif feature_name == \"cross_spectral_phase\":\n\t            self.model = CrossSpectra(phase_only=True,\n\t                            n_dft=config[\"n_dft\"], hop_size=config[\"hop_size\"])\n\t            self.n_output_channels = 1\n\t        self.n_output = self._get_output_shape()\n\t    def forward(self, x):\n\t        return self.model(x[\"signal\"])\n", "    def _get_output_shape(self):\n\t        n_input_samples = self.n_input_seconds*self.config[\"dataset\"][\"sr\"]\n\t        out_width = math.ceil((n_input_samples)/self.config[\"hop_size\"])\n\t        out_height = self.config[\"n_dft\"]//2 # /2 as we use \"onesided\" dft\n\t        return (out_width, out_height)\n\tdef get_stft_output_shape(feature_config):\n\t    n_input_samples = feature_config[\"dataset\"][\"n_input_seconds\"]*feature_config[\"dataset\"][\"sr\"]\n\t    out_width = math.ceil((n_input_samples)/feature_config[\"hop_size\"])\n\t    out_height = feature_config[\"n_dft\"]//2 # /2 as we use \"onesided\" dft\n\t    return torch.Tensor((out_width, out_height)).int()\n"]}
{"filename": "gnn_ssl/feature_extractors/pairwise_feature_extractors.py", "chunked_list": ["import torch\n\tfrom torch.nn import Module\n\tfrom pysoundloc.pysoundloc.gcc import gcc_phat_batch\n\tfrom pysoundloc.pysoundloc.srp import compute_pairwise_srp_grids\n\t# Every pairwise feature extractor inherits from the torch.nn.Module class\n\t# and has a n_output variable.\n\t# Furthermore, it implements a \"forward\" function with three parameters:\n\t# A tensor x and two indices i and j\n\tclass GccPhat(Module):\n\t    def __init__(self, sr, n_bins, normalize=True):\n", "        self.sr = sr\n\t        self.n_output = n_bins\n\t        self.normalize = normalize\n\t        super().__init__()\n\t    def forward(self, x, i, j):\n\t        x_i = x[\"signal\"][:, i]\n\t        x_j = x[\"signal\"][:, j]\n\t        batch_size, n_signal = x_i.shape\n\t        results = []\n\t        results = gcc_phat_batch(x_i, x_j, self.sr)[0]\n", "        # Only select central self.n_output\n\t        results = results[:, (n_signal - self.n_output)//2:(n_signal + self.n_output)//2]\n\t        if self.normalize:\n\t            results /= results.max(dim=1, keepdims=True)[0]\n\t        return results\n\tclass SpatialLikelihoodGrid(Module):\n\t    def __init__(self, sr, n_grid_points, flatten=True, normalize=False):\n\t        super().__init__()\n\t        self.sr = sr\n\t        self.n_grid_points = n_grid_points\n", "        self.flatten = flatten\n\t        self.normalize = normalize\n\t        if flatten:\n\t            self.n_output = n_grid_points**2\n\t        else:\n\t            self.n_output = (n_grid_points, n_grid_points)\n\t    def forward(self, x, i, j):\n\t        x_i = x[\"signal\"][:, i]\n\t        x_j = x[\"signal\"][:, j]\n\t        mic_i_coords = x[\"local\"][\"mic_coordinates\"][:, i]\n", "        mic_j_coords = x[\"local\"][\"mic_coordinates\"][:, j]\n\t        room_dims = x[\"global\"][\"room_dims\"]\n\t        grids = compute_pairwise_srp_grids(\n\t            x_i, x_j, self.sr,\n\t            mic_i_coords, mic_j_coords,\n\t            room_dims,\n\t            n_grid_points=self.n_grid_points\n\t        )\n\t        if self.flatten:\n\t            grids = grids.flatten(start_dim=1)\n", "            if self.normalize:\n\t                grids /= grids.max(dim=-1)[0].unsqueeze(1)\n\t        return grids\n\tclass MetadataAwarePairwiseFeatureExtractor(Module):\n\t    def __init__(self, n_input, pairwise_feature_extractor=None,\n\t                 is_metadata_aware=True, use_rt60_as_metadata=True):\n\t        super().__init__()\n\t        self.pairwise_feature_extractor = pairwise_feature_extractor\n\t        if pairwise_feature_extractor is None:\n\t            self.n_output = 2*n_input # 2 channels will be concatenated\n", "        else:\n\t            self.n_output = pairwise_feature_extractor.n_output\n\t        self.is_metadata_aware = is_metadata_aware\n\t        self.use_rt60_as_metadata = use_rt60_as_metadata\n\t        if is_metadata_aware:\n\t            self.n_output += 2*2*2 + 2 # 2 coordinates times 2 microphones per array + 2 room_dims\n\t            if use_rt60_as_metadata:\n\t                self.n_output += 1\n\t    def forward(self, x, i, j):\n\t        # 1. Apply pairwise feature extractor, if provided. Else, simply concat the signals\n", "        if self.pairwise_feature_extractor is not None:\n\t            x_ij = self.pairwise_feature_extractor(x, i, j)\n\t        else:\n\t            x_ij = torch.cat([x[\"signal\"][:, i], x[\"signal\"][:, j]], dim=1)\n\t        if not self.is_metadata_aware:\n\t            return x_ij\n\t        # 2. Concatenate local metadata\n\t        mic_coords = x[\"local\"][\"mic_coordinates\"]\n\t        room_dims = x[\"global\"][\"room_dims\"]\n\t        x_ij = torch.cat([\n", "            x_ij, mic_coords[:, i].flatten(start_dim=1), mic_coords[:, j].flatten(start_dim=1),\n\t            room_dims], dim=1)\n\t        if not self.use_rt60_as_metadata:\n\t            return x_ij\n\t        # 3. Concatenate global_metadata        \n\t        rt60 = x[\"global\"][\"rt60\"]\n\t        x_ij = torch.cat([x_ij, room_dims, rt60], dim=1)\n\t        return x_ij\n\tclass ArrayWiseSpatialLikelihoodGrid(Module):\n\t    \"\"\"\n", "    Compute the cumulative Spatial Likelihood Function (SLF)\n\t    using the signals of a microphone array, their respective microphone coordinates\n\t    and the room dimensions.\n\t    If microphone_wise == True, one cumulative SLF will be produced for each microphone,\n\t    using the correlations between it and the other ones.\n\t    \"\"\"\n\t    def __init__(self, sr, n_grid_points, flatten=True, thickness=10):\n\t        super().__init__()\n\t        self.sr = sr\n\t        self.n_grid_points = n_grid_points\n", "        self.thickness = thickness\n\t        self.flatten = flatten\n\t        if flatten:\n\t            self.n_output = n_grid_points**2\n\t        else:\n\t            self.n_output = (n_grid_points, n_grid_points)\n\t    def forward(self, x):\n\t        x_signal = x[\"signal\"]\n\t        mic_coords = x[\"metadata\"][\"local\"][\"mic_coordinates\"]\n\t        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"]\n", "        if x_signal.ndim == 3:\n\t            # Single array\n\t            x_signal = x_signal.unsqueeze(1)\n\t        batch_size, n_arrays, n_array, n_signal = x_signal.shape\n\t        grids = torch.zeros((\n\t            batch_size, n_arrays,\n\t            self.n_grid_points, self.n_grid_points\n\t        ))\n\t        for k in range(n_arrays):\n\t            for i in range(n_array):\n", "                for j in range(i + 1, n_array):\n\t                    grid_ij = compute_pairwise_srp_grids(\n\t                            x_signal[:, k, i], x_signal[:, k, j], self.sr,\n\t                            mic_coords[:, k, i], mic_coords[:, k, j],\n\t                            room_dims, n_grid_points=self.n_grid_points,\n\t                            n_correlation_neighbours=self.thickness\n\t                    )\n\t                    grids[:, k] += grid_ij\n\t        if self.flatten:\n\t            grids = grids.flatten(start_dim=2)\n", "        # Normalize grids\n\t        # Only works on flattened!\n\t        max_grids = grids.max(dim=2)[0].unsqueeze(2)\n\t        grids /= max_grids\n\t        x[\"signal\"] = grids\n\t        return x\n"]}
{"filename": "gnn_ssl/feature_extractors/stft.py", "chunked_list": ["from calendar import c\n\timport torch\n\tfrom torch.nn import Module\n\tclass StftArray(Module):\n\t    def __init__(self, n_dft=1024, hop_size=512, window_length=None,\n\t                 onesided=True, is_complex=True, complex_as_channels=False,\n\t                 mag_only=False, phase_only=False, real_only=False):\n\t        super().__init__()\n\t        self.n_dft = n_dft\n\t        self.hop_size = hop_size\n", "        self.onesided = onesided\n\t        self.is_complex = is_complex\n\t        self.complex_as_channels = complex_as_channels\n\t        self.mag_only = mag_only\n\t        self.phase_only = phase_only\n\t        self.real_only = real_only\n\t        self.window_length = n_dft if window_length is None else window_length\n\t    def forward(self, x: torch.Tensor):\n\t        \"Expected input has shape (batch_size, n_channels, time_steps)\"\n\t        input_shape = x.shape\n", "        if len(input_shape) == 3:\n\t            # (batch_size, n_channels, time_steps) => Microphone array\n\t            # Collapse channels into batch\n\t            x = x.flatten(end_dim=1)\n\t        window = torch.hann_window(self.window_length, device=x.device)\n\t        y = torch.stft(x, self.n_dft, hop_length=self.hop_size, \n\t                       onesided=self.onesided, return_complex=True,\n\t                       win_length=self.window_length, window=window)\n\t        y = y[:, 1:] # Remove DC component (f=0hz)\n\t        y = y.transpose(1, 2)\n", "        # y.shape == (batch_size*channels, time, freqs)\n\t        if len(input_shape) == 3:\n\t            batch_size, num_channels, _ = input_shape\n\t            # De-collapse first dim into batch and channels\n\t            y = y.unflatten(0, (batch_size, num_channels))\n\t        if self.mag_only:\n\t            return y.abs()\n\t        if self.phase_only:\n\t            return y.angle()\n\t        if self.real_only:\n", "            return y.real\n\t        if not self.is_complex:\n\t            y = _complex_to_real(y, self.complex_as_channels)\n\t        return y\n\tclass StftPhaseArray(StftArray):\n\t    def __init__(self, features):\n\t        super().__init__(features[\"n_dft\"],\n\t                         features[\"hop_size\"],\n\t                         features[\"onesided\"],\n\t                         phase_only=True)\n", "        self.n_output_channels = 2\n\tclass CrossSpectra(StftArray):\n\t    def __init__(self, n_dft=1024, hop_size=512,\n\t                 onesided=True, is_complex=True, complex_as_channels=False,\n\t                 phase_only=False):        \n\t        super().__init__(n_dft, hop_size, onesided=onesided)\n\t        self._is_complex = is_complex # _ is added not to conflict with StftArray\n\t        self.complex_as_channels = complex_as_channels\n\t        self.phase_only = phase_only\n\t    def forward(self, X):\n", "        \"Expected input has shape (batch_size, n_channels, time_steps)\"\n\t        batch_size, n_channels, time_steps = X.shape\n\t        stfts = super().forward(X)\n\t        # (batch_size, n_channels, n_time_bins, n_freq_bins)\n\t        y = []\n\t        # Compute the cross-spectrum between each pair of channels\n\t        for i in range(n_channels):\n\t            for j in range(i + 1, n_channels):\n\t                y_ij = stfts[:, i]*stfts[:, j].conj()\n\t                y.append(y_ij)\n", "        y = torch.stack(y, dim=1)\n\t        if self.phase_only:\n\t            return y.angle()\n\t        if not self._is_complex:\n\t            _complex_to_real(y, self.complex_as_channels)\n\t        return y\n\tdef _complex_to_real(x, as_channels=False): \n\t    y = torch.view_as_real(x)\n\t    if as_channels:\n\t        # Merge channels and real and imaginary parts (last dim) as separate input channels\n", "        y = y.transpose(2, -1).flatten(start_dim=1, end_dim=2)\n\t    return y\n"]}
{"filename": "gnn_ssl/models/di_nn.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom hydra.utils import get_class\n\tfrom omegaconf import OmegaConf\n\tfrom gnn_ssl.feature_extractors import get_stft_output_shape\n\tfrom pysoundloc.pysoundloc.utils.math import grid_argmax\n\tfrom .base.rnn import RNN, init_gru\n\tfrom .base.mlp import MLP\n\tfrom .base.cnn import ConvBlock\n\tfrom ..feature_extractors.metadata import flatten_metadata\n", "class DINN(nn.Module):\n\t    \"\"\"\n\t    CRNN model that accepts a secondary input\n\t    consisting of metadata (microphone positions, room dimensions, rt60)\n\t    \"\"\"\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        feature_config = config[\"features\"]\n\t        dataset_config = feature_config[\"dataset\"]\n\t        targets_config = config[\"targets\"]\n", "        self.feature_extractor = get_class(\n\t            config[\"features\"][\"class\"])(config[\"features\"])\n\t        self.config = config = OmegaConf.to_object(config)\n\t        # 1. Store configuration\n\t        self.n_input_channels = config[\"n_mics\"]\n\t        self.pool_type = config[\"pool_type\"]\n\t        self.pool_size = config[\"pool_size\"]\n\t        self.kernel_size = config[\"kernel_size\"]\n\t        self.input_shape = get_stft_output_shape(feature_config)\n\t        self.is_metadata_aware = config[\"is_metadata_aware\"]\n", "        self.metadata_dim = dataset_config[\"metadata_dim\"]\n\t        self.grid_size = targets_config[\"n_points_per_axis\"]\n\t        self.output_type = targets_config[\"type\"] # grid or regression vector\n\t        if self.output_type == \"source_coordinates\":\n\t            config[\"encoder_mlp_config\"][\"n_output_channels\"] = targets_config[\"n_output_coordinates\"]\n\t        # 3. Create encoder\n\t        self.encoder = Encoder(self.n_input_channels,\n\t                               self.input_shape,\n\t                               config[\"conv_layers_config\"],\n\t                               config[\"rnn_config\"],\n", "                               config[\"encoder_mlp_config\"],\n\t                               config[\"init_layers\"],\n\t                               config[\"pool_type\"],\n\t                               config[\"pool_size\"],\n\t                               config[\"kernel_size\"],\n\t                               flatten_dims=config[\"flatten_dims\"],\n\t                               batch_norm=config[\"batch_norm\"],\n\t                               is_metadata_aware=self.is_metadata_aware,\n\t                               metadata_dim=dataset_config[\"metadata_dim\"])\n\t        if config[\"output_activation\"] == \"relu\":\n", "            self.output_activation = nn.ReLU()\n\t        elif config[\"output_activation\"] == \"sigmoid\":\n\t            self.output_activation = nn.Sigmoid()\n\t        else:\n\t            self.output_activation = None\n\t    def forward(self, x):\n\t        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"][:, :2]\n\t        x = {\n\t            \"signal\": self.feature_extractor(x),\n\t            \"metadata\": x[\"metadata\"]\n", "        }        \n\t        x = self.encoder(x)\n\t        if self.output_activation is not None:\n\t            x = self.output_activation(x)\n\t        if self.output_type == \"grid\":\n\t            batch_size = x.shape[0]\n\t            x = x.reshape((batch_size, self.grid_size, self.grid_size))\n\t            return {\n\t                \"source_coordinates\": grid_argmax(x, room_dims),\n\t                \"grid\": x\n", "            }\n\t        else:\n\t            return {\n\t                \"source_coordinates\": x\n\t            }\n\tclass Encoder(nn.Module):\n\t    def __init__(self, n_input_channels,\n\t                       input_shape,\n\t                       conv_layers_config,\n\t                       rnn_config,\n", "                       mlp_config,\n\t                       init_layers,\n\t                       pool_type,\n\t                       pool_size,\n\t                       kernel_size,\n\t                       flatten_dims=False,\n\t                       batch_norm=False,\n\t                       is_metadata_aware=True,\n\t                       metadata_dim=3):\n\t        super().__init__()\n", "        # 1. Store configuration\n\t        self.n_input_channels = n_input_channels\n\t        self.input_shape = input_shape\n\t        self.pool_type = pool_type\n\t        self.pool_size = pool_size\n\t        self.kernel_size = kernel_size\n\t        self.n_conv_output_output = conv_layers_config[-1][\"n_channels\"]\n\t        self.n_rnn_output_channels = rnn_config[\"n_output_channels\"]\n\t        self.flatten_dims = flatten_dims\n\t        self.batch_norm = batch_norm\n", "        self.metadata_dim = metadata_dim # 3 for 3D localization, 2 for 2 for 2D localization\n\t        self.is_metadata_aware = is_metadata_aware\n\t        # 2. Create convolutional blocks\n\t        self.conv_blocks, self.conv_output_shape = self._create_conv_blocks(\n\t            conv_layers_config, batch_norm=batch_norm\n\t        )\n\t        # 3. Create recurrent block\n\t        self.n_rnn_input = self.n_conv_output_output\n\t        if flatten_dims:\n\t            # The input for the RNN will be\n", "            # The frequency_bins x conv_output_channels\n\t            self.n_rnn_input = int(self.n_rnn_input*self.conv_output_shape[-1])\n\t        self.rnn = RNN(self.n_rnn_input,\n\t                       self.n_rnn_output_channels,\n\t                       rnn_config[\"bidirectional\"],\n\t                       rnn_config[\"output_mode\"],\n\t                       n_layers=rnn_config[\"n_layers\"]\n\t        )\n\t        # 4. Create mlp block\n\t        n_input_mlp = self.n_rnn_output_channels\n", "        if is_metadata_aware:\n\t            n_metadata = metadata_dim*(self.n_input_channels + 1) # 1 => room dims \n\t            n_input_mlp += n_metadata\n\t        self.mlp = MLP(n_input_mlp, mlp_config[\"n_output_channels\"],\n\t                        n_hidden_features=mlp_config[\"n_hidden_channels\"],\n\t                        n_layers=mlp_config[\"n_layers\"],\n\t                        batch_norm=False, # Batch normalization on the MLP slowed training down.\n\t                        dropout_rate=mlp_config[\"dropout_rate\"]) \n\t        if init_layers:\n\t            init_gru(self.rnn.rnn)\n", "    def forward(self, x):\n\t        if self.is_metadata_aware:\n\t            metadata = flatten_metadata(x[\"metadata\"])\n\t        x = x[\"signal\"]\n\t        # (batch_size, num_channels, time_steps, freqs)\n\t        # 1. Extract features using convolutional layers\n\t        for conv_block in self.conv_blocks:\n\t            x = conv_block(x)\n\t        # (batch_size, feature_maps, time_steps', freqs')\n\t        # 2. Average across all frequency bins,\n", "        # or flatten the frequency and channels\n\t        if self.flatten_dims:\n\t            x = x.transpose(2, 3)\n\t            x = x.flatten(start_dim=1, end_dim=2)\n\t        else:\n\t            if self.pool_type == \"avg\":\n\t                x = torch.mean(x, dim=3)\n\t            elif self.pool_type == \"max\":\n\t                x = torch.max(x, dim=3)[0]\n\t        # (batch_size, feature_maps, time_steps)\n", "        # Preprocessing for RNN\n\t        x = x.transpose(1,2)\n\t        # (batch_size, time_steps, feature_maps):\n\t        # 3. Apply RNN\n\t        x = self.rnn(x)\n\t        if self.is_metadata_aware:\n\t            # Concatenate metadata before sending to fully connected layer,\n\t            x = torch.cat([x, metadata], dim=1)\n\t            # (batch_size, n_metadata_unaware_features + n_metadata)\n\t        # 5. Fully connected layer\n", "        x = self.mlp(x)\n\t        # (batch_size, class_num)\n\t        return x\n\t    def _create_conv_blocks(self, conv_layers_config, batch_norm):\n\t        conv_blocks = [\n\t            ConvBlock(self.n_input_channels, conv_layers_config[0][\"n_channels\"],\n\t                      block_type=conv_layers_config[0][\"type\"],\n\t                      pool_size=self.pool_size,\n\t                      pool_type=self.pool_type,\n\t                      kernel_size=self.kernel_size,\n", "                      batch_norm=batch_norm)\n\t        ]\n\t        current_output_shape = conv_blocks[-1].get_output_shape(self.input_shape)\n\t        for i, config in enumerate(conv_layers_config[1:]):\n\t            last_layer = conv_blocks[-1]\n\t            in_channels = last_layer.out_channels\n\t            conv_blocks.append(\n\t                ConvBlock(in_channels, config[\"n_channels\"],\n\t                          block_type=config[\"type\"],\n\t                          pool_size=self.pool_size,\n", "                          pool_type=self.pool_type,\n\t                          kernel_size=self.kernel_size,\n\t                          batch_norm=batch_norm)\n\t            )\n\t            current_output_shape = conv_blocks[-1].get_output_shape(current_output_shape)\n\t        return nn.ModuleList(conv_blocks), current_output_shape\n"]}
{"filename": "gnn_ssl/models/example.py", "chunked_list": ["import torch.nn as nn\n\tfrom hydra.utils import get_class\n\tfrom omegaconf import OmegaConf\n\tfrom gnn_ssl.feature_extractors import get_stft_output_shape\n\tfrom .base.mlp import MLP\n\tclass ExampleNet(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        feature_config, targets_config = config[\"features\"], config[\"targets\"]\n\t        self.feature_extractor = get_class(\n", "            config[\"features\"][\"class\"])(config[\"features\"])\n\t        self.config = config = OmegaConf.to_object(config)\n\t        n_frames, n_frame = get_stft_output_shape(feature_config)\n\t        self.mlp = MLP(\n\t            n_frame, config[\"n_output_features\"], config[\"n_hidden_features\"],\n\t            config[\"n_layers\"], config[\"activation\"], config[\"output_activation\"],\n\t            config[\"batch_norm\"], config[\"dropout_rate\"]\n\t        )\n\t    def forward(self, x):\n\t        x = {\n", "            \"signal\": self.feature_extractor(x),\n\t            \"metadata\": x[\"metadata\"]\n\t        }\n\t        batch_size, n_mics, n_frames, n_frame = x[\"signal\"].shape\n\t        x = self.mlp(x[\"signal\"])\n\t        # x.shape == (batch_size, n_mics, n_frames, n_output_features)\n\t        return {\n\t            \"example_output\": x\n\t        }\n"]}
{"filename": "gnn_ssl/models/__init__.py", "chunked_list": []}
{"filename": "gnn_ssl/models/neural_srp.py", "chunked_list": ["import torch\n\tfrom pysoundloc.pysoundloc.utils.math import grid_argmax\n\tfrom .pairwise_neural_srp import PairwiseNeuralSrp\n\tfrom ..feature_extractors.metadata import filter_local_metadata\n\tclass NeuralSrp(PairwiseNeuralSrp):\n\t    \"\"\"Multi-microphone version of PairwiseNeuralSrp,\n\t    which works for a single pair of microphones.\n\t    NeuralSrp computes a PairwiseNeuralSrp grid for each microphone pair,\n\t    then sums them together.\n\t    \"\"\"\n", "    def __init__(self, config):\n\t        super().__init__(config)\n\t    def forward(self, x, estimate_coords=True, mean=True):\n\t        # x = self.feature_extractor(x) TODO: Extract features before for efficiency\n\t        # batch_size, n_channels, n_time_bins, n_freq_bins = x[\"signal\"].shape\n\t        batch_size, n_channels, n_time_samples = x[\"signal\"].shape\n\t        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"][:, :2]\n\t        y = []\n\t        n_pairs = n_channels*(n_channels - 1)/2\n\t        for i in range(n_channels):\n", "            for j in range(i + 1, n_channels):\n\t                x_ij = {\n\t                    \"signal\": x[\"signal\"][:, [i, j]],\n\t                    \"metadata\": filter_local_metadata(x[\"metadata\"], [i, j])\n\t                }\n\t                y.append(super().forward(x_ij)[\"grid\"])\n\t                #count += 1\n\t        y = torch.stack(y, dim=1).sum(dim=1)\n\t        if mean:\n\t            y /= n_pairs\n", "        if estimate_coords:\n\t            estimated_coords = grid_argmax(y, room_dims)\n\t            return {\n\t                \"source_coordinates\": estimated_coords,\n\t                \"grid\": y    \n\t            }\n\t        return y\n"]}
{"filename": "gnn_ssl/models/pairwise_neural_srp.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom hydra.utils import get_class\n\tfrom omegaconf import OmegaConf\n\tfrom gnn_ssl.feature_extractors import get_stft_output_shape\n\tfrom .base.rnn import RNN, init_gru\n\tfrom .base.mlp import MLP\n\tfrom .base.cnn import ConvBlock\n\tfrom ..feature_extractors.metadata import flatten_metadata\n\tclass PairwiseNeuralSrp(nn.Module):\n", "    def __init__(self, config):\n\t        super().__init__()\n\t        feature_config, targets_config = config[\"features\"], config[\"targets\"]\n\t        self.feature_extractor = get_class(\n\t            config[\"features\"][\"class\"])(config[\"features\"])\n\t        self.config = config = OmegaConf.to_object(config)\n\t        # 1. Store configuration\n\t        self.n_input_channels = 2 # Neural SRP is pairwise\n\t        self.pool_type = config[\"pool_type\"]\n\t        self.pool_size = config[\"pool_size\"]\n", "        self.kernel_size = config[\"kernel_size\"]\n\t        self.normalize_output = config[\"normalize_output\"]\n\t        self.input_shape = get_stft_output_shape(feature_config)\n\t        self.grid_size = targets_config[\"n_points_per_axis\"]\n\t        self.is_metadata_aware = config[\"is_metadata_aware\"]\n\t        self.metadata_dim = feature_config[\"dataset\"][\"metadata_dim\"]\n\t        # 3. Create encoder\n\t        self.encoder = Encoder(self.n_input_channels,\n\t                               self.input_shape,\n\t                               config[\"conv_layers_config\"],\n", "                               config[\"rnn_config\"],\n\t                               config[\"encoder_mlp_config\"],\n\t                               config[\"init_layers\"],\n\t                               config[\"pool_type\"],\n\t                               config[\"pool_size\"],\n\t                               config[\"kernel_size\"],\n\t                               flatten_dims=config[\"flatten_dims\"],\n\t                               batch_norm=config[\"batch_norm\"],\n\t                               is_metadata_aware=config[\"is_metadata_aware\"],\n\t                               metadata_dim=feature_config[\"dataset\"][\"metadata_dim\"])\n", "        if config[\"output_activation\"] == \"relu\":\n\t            self.output_activation = nn.ReLU()\n\t        elif config[\"output_activation\"] == \"sigmoid\":\n\t            self.output_activation = nn.Sigmoid()\n\t        else:\n\t            self.output_activation = None\n\t    def forward(self, x):\n\t        x = {\n\t            \"signal\": self.feature_extractor(x),\n\t            \"metadata\": x[\"metadata\"]\n", "        }\n\t        x = self.encoder(x)\n\t        batch_size = x.shape[0]\n\t        x = x.reshape((batch_size, self.grid_size, self.grid_size))\n\t        if self.output_activation is not None:\n\t            x = self.output_activation(x)\n\t        if self.normalize_output:\n\t            x_max = x.abs().flatten(start_dim=1).max(dim=1)[0].unsqueeze(dim=1).unsqueeze(dim=2)\n\t            x = x/x_max\n\t        return {\n", "            \"grid\": x\n\t        }\n\tclass Encoder(nn.Module):\n\t    def __init__(self, n_input_channels,\n\t                       input_shape,\n\t                       conv_layers_config,\n\t                       rnn_config,\n\t                       mlp_config,\n\t                       init_layers,\n\t                       pool_type,\n", "                       pool_size,\n\t                       kernel_size,\n\t                       flatten_dims=False,\n\t                       batch_norm=False,\n\t                       is_metadata_aware=True,\n\t                       metadata_dim=3):\n\t        super().__init__()\n\t        # 1. Store configuration\n\t        self.n_input_channels = n_input_channels\n\t        self.input_shape = input_shape\n", "        self.pool_type = pool_type\n\t        self.pool_size = pool_size\n\t        self.kernel_size = kernel_size\n\t        self.n_conv_output_output = conv_layers_config[-1][\"n_channels\"]\n\t        self.n_rnn_output_channels = rnn_config[\"n_output_channels\"]\n\t        self.flatten_dims = flatten_dims\n\t        self.batch_norm = batch_norm\n\t        self.metadata_dim = metadata_dim\n\t        self.is_metadata_aware = is_metadata_aware\n\t        # 2. Create convolutional blocks\n", "        self.conv_blocks, self.conv_output_shape = self._create_conv_blocks(\n\t            conv_layers_config, batch_norm=batch_norm\n\t        )\n\t        # 3. Create recurrent block\n\t        self.n_rnn_input = self.n_conv_output_output\n\t        if flatten_dims:\n\t            # The input for the RNN will be\n\t            # The frequency_bins x conv_output_channels\n\t            self.n_rnn_input = int(self.n_rnn_input*self.conv_output_shape[-1])\n\t        self.rnn = RNN(self.n_rnn_input,\n", "                       self.n_rnn_output_channels,\n\t                       rnn_config[\"bidirectional\"],\n\t                       rnn_config[\"output_mode\"],\n\t                       n_layers=rnn_config[\"n_layers\"]\n\t        )\n\t        # 4. Create mlp block\n\t        n_input_mlp = self.n_rnn_output_channels\n\t        if is_metadata_aware:\n\t            n_metadata = metadata_dim*(n_input_channels + 1) # 3 = 2 mic coords, + room dims \n\t            n_input_mlp += n_metadata\n", "        self.mlp = MLP(n_input_mlp, mlp_config[\"n_output_channels\"],\n\t                           n_hidden_features=mlp_config[\"n_hidden_channels\"],\n\t                           n_layers=mlp_config[\"n_layers\"],\n\t                           batch_norm=False) # Batch normalization on the MLP slowed training down. \n\t        if init_layers:\n\t            init_gru(self.rnn.rnn)\n\t    def forward(self, x):\n\t        if self.is_metadata_aware > 0:\n\t            metadata = flatten_metadata(x[\"metadata\"])\n\t        x = x[\"signal\"]\n", "        # (batch_size, num_channels, time_steps, freqs)\n\t        # 1. Extract features using convolutional layers\n\t        for conv_block in self.conv_blocks:\n\t            x = conv_block(x)\n\t        # (batch_size, feature_maps, time_steps', freqs')\n\t        # 2. Average across all frequency bins,\n\t        # or flatten the frequency and channels\n\t        if self.flatten_dims:\n\t            x = x.transpose(2, 3)\n\t            x = x.flatten(start_dim=1, end_dim=2)\n", "        else:\n\t            if self.pool_type == \"avg\":\n\t                x = torch.mean(x, dim=3)\n\t            elif self.pool_type == \"max\":\n\t                x = torch.max(x, dim=3)[0]\n\t        # (batch_size, feature_maps, time_steps)\n\t        # Preprocessing for RNN\n\t        x = x.transpose(1,2)\n\t        # (batch_size, time_steps, feature_maps):\n\t        # 3. Apply RNN\n", "        x = self.rnn(x)\n\t        if self.is_metadata_aware:\n\t            # Concatenate metadata before sending to fully connected layer,\n\t            x = torch.cat([x, metadata], dim=1)\n\t            # (batch_size, n_metadata_unaware_features + n_metadata)\n\t        # 5. Fully connected layer\n\t        x = self.mlp(x)\n\t        # (batch_size, class_num)\n\t        # (batch_size, feature_maps)\n\t        return x\n", "    def _create_conv_blocks(self, conv_layers_config, batch_norm):\n\t        conv_blocks = [\n\t            ConvBlock(self.n_input_channels, conv_layers_config[0][\"n_channels\"],\n\t                      block_type=conv_layers_config[0][\"type\"],\n\t                      pool_size=self.pool_size,\n\t                      pool_type=self.pool_type,\n\t                      kernel_size=self.kernel_size,\n\t                      batch_norm=batch_norm)\n\t        ]\n\t        current_output_shape = conv_blocks[-1].get_output_shape(self.input_shape)\n", "        for i, config in enumerate(conv_layers_config[1:]):\n\t            last_layer = conv_blocks[-1]\n\t            in_channels = last_layer.out_channels\n\t            conv_blocks.append(\n\t                ConvBlock(in_channels, config[\"n_channels\"],\n\t                          block_type=config[\"type\"],\n\t                          pool_size=self.pool_size,\n\t                          pool_type=self.pool_type,\n\t                          kernel_size=self.kernel_size,\n\t                          batch_norm=batch_norm)\n", "            )\n\t            current_output_shape = conv_blocks[-1].get_output_shape(current_output_shape)\n\t        return nn.ModuleList(conv_blocks), current_output_shape\n"]}
{"filename": "gnn_ssl/models/gnn_ssl.py", "chunked_list": ["from hydra.utils import get_class\n\tfrom omegaconf import OmegaConf\n\tfrom gnn_ssl.feature_extractors.pairwise_feature_extractors import ArrayWiseSpatialLikelihoodGrid\n\tfrom gnn_ssl.feature_extractors.pairwise_feature_extractors import (\n\t    GccPhat, MetadataAwarePairwiseFeatureExtractor, SpatialLikelihoodGrid\n\t)\n\tfrom pysoundloc.pysoundloc.utils.math import grid_argmax\n\tfrom .base.mlp import MLP\n\tfrom .base.base_relation_network import BaseRelationNetwork\n\tSIGNAL_KEY = \"signal\"\n", "class GnnSslNet(BaseRelationNetwork):\n\t    def __init__(self,\n\t                 config,\n\t                 **kwargs):\n\t        self.config = config = OmegaConf.to_object(config)\n\t        feature_config, target_config = config[\"features\"], config[\"targets\"]\n\t        dataset_config = feature_config[\"dataset\"]\n\t        # 1. Store configuration\n\t        self.is_metadata_aware = config[\"is_metadata_aware\"]\n\t        self.use_rt60_as_metadata = dataset_config[\"use_rt60_as_metadata\"]\n", "        self.n_likelihood_grid_points_per_axis = target_config[\"n_points_per_axis\"]\n\t        self.sr = dataset_config[\"sr\"]\n\t        n_input_features = int(dataset_config[\"n_input_seconds\"]*self.sr)\n\t        # Set output size\n\t        self.output_target = target_config[\"type\"]\n\t        if self.output_target == \"source_coordinates\":\n\t            n_output_features = 2 # x, y coords of the microphones\n\t        else:\n\t            n_output_features = self.n_likelihood_grid_points_per_axis**2\n\t        # 2. Create local feature extractor\n", "        if config[\"local_feature_extractor\"] and config[\"pairwise_feature_extractor\"]:\n\t            raise ValueError(\n\t                \"\"\"Simultaneously using local and pairwise\n\t                feature extractors is not yet supported.\"\"\")\n\t        if config[\"local_feature_extractor\"] == \"mlp\":\n\t            config[\"local_feature_extractor\"] = MLP(n_input_features,\n\t                                          config[\"n_pairwise_features\"],\n\t                                          config[\"n_pairwise_features\"],\n\t                                          config[\"activation\"],\n\t                                          None,\n", "                                          config[\"batch_norm\"],\n\t                                          config[\"dropout_rate\"],\n\t                                          config[\"n_layers\"])\n\t        elif config[\"local_feature_extractor\"] == \"slf\":\n\t            config[\"local_feature_extractor\"] = ArrayWiseSpatialLikelihoodGrid(\n\t                self.sr, self.n_likelihood_grid_points_per_axis,\n\t                thickness=feature_config[\"srp_thickness\"]\n\t            )\n\t        if config[\"local_feature_extractor\"] is not None:\n\t            n_input_features = config[\"local_feature_extractor\"].n_output\n", "        super().__init__(n_input_features, n_output_features, config[\"n_pairwise_features\"], config[\"local_feature_extractor\"],\n\t                    config[\"pairwise_feature_extractor\"], config[\"pairwise_network_only\"],\n\t                    config[\"activation\"], config[\"output_activation\"], config[\"init_layers\"], config[\"batch_norm\"],\n\t                    config[\"dropout_rate\"], config[\"n_layers\"], SIGNAL_KEY)\n\t        # 3. Create pairwise feature extractor\n\t        self.use_pairwise_feature_extractor = config[\"pairwise_feature_extractor\"] is not None\n\t        if self.use_pairwise_feature_extractor:\n\t            if config[\"pairwise_feature_extractor\"] == \"gcc_phat\":\n\t                config[\"pairwise_feature_extractor\"] = GccPhat(self.sr, feature_config[\"n_dft\"])\n\t            elif config[\"pairwise_feature_extractor\"] == \"spatial_likelihood_grid\":\n", "                config[\"pairwise_feature_extractor\"] = SpatialLikelihoodGrid(self.sr, self.n_likelihood_grid_points_per_axis)\n\t            else:\n\t                raise ValueError(\"pairwise_feature_extractor must be 'gcc_phat' or 'spatial_likelihood_grid'\")\n\t            n_input_features = pairwise_feature_extractor.n_output\n\t        pairwise_feature_extractor = MetadataAwarePairwiseFeatureExtractor(\n\t            n_input_features,\n\t            config[\"pairwise_feature_extractor\"],\n\t            self.is_metadata_aware,\n\t            self.use_rt60_as_metadata\n\t        )\n", "    def forward(self, x, estimate_coords=False):\n\t        y = super().forward(x)\n\t        if estimate_coords and self.output_target == \"likelihood_grid\":\n\t            batch_size = y.shape[0]\n\t            estimated_coords = grid_argmax(\n\t                y.reshape(batch_size, self.n_likelihood_grid_points_per_axis, self.n_likelihood_grid_points_per_axis),\n\t                x[\"global\"][\"room_dims\"])\n\t            return estimated_coords, y\n\t        else:\n\t            return y\n"]}
{"filename": "gnn_ssl/models/base/mlp.py", "chunked_list": ["import torch.nn as nn\n\tACTIVATIONS = {\n\t    \"relu\": nn.ReLU,\n\t    \"prelu\": nn.PReLU,\n\t    \"sigmoid\": nn.Sigmoid,\n\t    None: None\n\t}\n\tclass MLP(nn.Module):\n\t    def __init__(self, n_input_features, n_output_features,\n\t                n_hidden_features, n_layers, activation=\"relu\",\n", "                output_activation=None, batch_norm=False, dropout_rate=0):\n\t        super().__init__()\n\t        activation = ACTIVATIONS[activation]\n\t        output_activation = ACTIVATIONS[output_activation]\n\t        layers = [\n\t            fc_block(n_input_features, n_hidden_features, activation, batch_norm, dropout_rate)\n\t        ]\n\t        for _ in range(n_layers - 2): # -2 = skipping input and output layers\n\t            layers.append(\n\t                fc_block(n_hidden_features,\n", "                          n_hidden_features,\n\t                          activation,\n\t                          batch_norm=batch_norm\n\t                )\n\t            )\n\t        layers.append(fc_block(\n\t            n_hidden_features, n_output_features,\n\t            activation=output_activation, batch_norm=False)\n\t        )\n\t        self.layers = nn.Sequential(*layers)\n", "        self.n_output = n_output_features\n\t    def forward(self, x):\n\t        return self.layers(x)\n\tdef fc_block(n_input, n_output, activation, batch_norm=False, dropout_rate=0):\n\t    layers = [nn.Linear(n_input, n_output)]\n\t    if activation:\n\t        layers.append(activation())\n\t    if batch_norm:\n\t        layers.append(nn.BatchNorm1d(n_output))\n\t    if dropout_rate > 0:\n", "        layers.append(nn.Dropout(dropout_rate))\n\t    return nn.Sequential(*layers)\n"]}
{"filename": "gnn_ssl/models/base/rnn.py", "chunked_list": ["import math\n\timport torch\n\timport torch.nn as nn\n\tclass RNN(nn.Module):\n\t    def __init__(self, n_input, n_hidden,\n\t                 bidirectional=False, output_mode=\"last_time\", n_layers=1):\n\t        self.n_input = n_input\n\t        self.n_hidden = n_hidden\n\t        self.bidirectional = bidirectional\n\t        self.output_mode = output_mode\n", "        if bidirectional:\n\t            n_hidden //=2\n\t        super().__init__()\n\t        self.rnn = nn.GRU(input_size=n_input,\n\t                          hidden_size=n_hidden,\n\t                          batch_first=True,\n\t                          bidirectional=bidirectional,\n\t                          num_layers=n_layers)\n\t    def forward(self, x):\n\t        (x, _) = self.rnn(x)\n", "        # (batch_size, time_steps, feature_maps):\n\t        # Select last time step\n\t        if self.output_mode == \"last_time\":\n\t            return x[:, -1]\n\t        elif self.output_mode == \"avg_time\":\n\t            return x.mean(dim=1)\n\t        elif self.output_mode == \"avg_channels\":\n\t            return x.mean(dim=2)\n\tdef init_gru(rnn):\n\t    \"\"\"Initialize a GRU layer. \"\"\"\n", "    def _concat_init(tensor, init_funcs):\n\t        (length, fan_out) = tensor.shape\n\t        fan_in = length // len(init_funcs)\n\t        for (i, init_func) in enumerate(init_funcs):\n\t            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])\n\t    def _inner_uniform(tensor):\n\t        fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n\t        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n\t    for i in range(rnn.num_layers):\n\t        _concat_init(\n", "            getattr(rnn, 'weight_ih_l{}'.format(i)),\n\t            [_inner_uniform, _inner_uniform, _inner_uniform]\n\t        )\n\t        torch.nn.init.constant_(getattr(rnn, 'bias_ih_l{}'.format(i)), 0)\n\t        _concat_init(\n\t            getattr(rnn, 'weight_hh_l{}'.format(i)),\n\t            [_inner_uniform, _inner_uniform, nn.init.orthogonal_]\n\t        )\n\t        torch.nn.init.constant_(getattr(rnn, 'bias_hh_l{}'.format(i)), 0)\n"]}
{"filename": "gnn_ssl/models/base/utils.py", "chunked_list": ["import math\n\timport torch\n\timport torch.nn as nn\n\tACTIVATIONS = {\n\t    \"relu\": nn.ReLU,\n\t    \"prelu\": nn.PReLU,\n\t    \"sigmoid\": nn.Sigmoid,\n\t    None: None\n\t}\n\tclass MaxLayer(nn.Module):\n", "    def __init__(self, dim=1):\n\t        super().__init__()\n\t        self.dim = dim\n\t    def forward(self, x):\n\t        return x.max(dim=self.dim)\n\tclass AvgLayer(nn.Module):\n\t    def __init__(self, dim=1):\n\t        super().__init__()\n\t        self.dim = dim\n\t    def forward(self, x):\n", "        return x.mean(dim=self.dim)\n\tdef init_layer(layer, nonlinearity='relu'):\n\t    \"\"\"Initialize a convolutional or linear layer\n\t    Credits to Yin Cao et al:\n\t    https://github.com/yinkalario/Two-Stage-Polyphonic-Sound-Event-Detection-and-Localization/blob/master/models/model_utilities.py\n\t    \"\"\"\n\t    classname = layer.__class__.__name__\n\t    if (classname.find('Conv') != -1) or (classname.find('Linear') != -1):\n\t        nn.init.kaiming_uniform_(layer.weight, nonlinearity=nonlinearity)\n\t        #nn.init.normal_(layer.weight, 1.0, 0.02)\n", "        if hasattr(layer, 'bias'):\n\t            if layer.bias is not None:\n\t                nn.init.constant_(layer.bias, 0.0)\n\t    elif classname.find('BatchNorm') != -1:\n\t        nn.init.normal_(layer.weight, 1.0, 0.02)\n\t        nn.init.constant_(layer.bias, 0.0)\n\tdef load_checkpoint(model, checkpoint_path):\n\t    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\t    checkpoint = torch.load(checkpoint_path, map_location=device)\n\t    state_dict = {}\n", "    for k, v in checkpoint[\"state_dict\"].items():\n\t        k = _remove_prefix(k, \"model.\")\n\t        state_dict[k] = v\n\t    model.load_state_dict(state_dict)\n\tdef _remove_prefix(s, prefix):\n\t    return s[len(prefix):] if s.startswith(prefix) else s\n"]}
{"filename": "gnn_ssl/models/base/base_relation_network.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom .mlp import MLP\n\tfrom .utils import ACTIVATIONS\n\t# Base generic class to be inherited by the SSL specific network\n\tclass BaseRelationNetwork(nn.Module):\n\t    def __init__(self, n_input_features, n_output_features,\n\t                 n_pairwise_features,\n\t                 on_input_hook=None, # None or nn.Module\n\t                 on_pairwise_relation_network_start_hook=None, # None or nn.Module\n", "                 pairwise_network_only=False,\n\t                 activation=\"relu\",\n\t                 output_activation=\"sigmoid\",\n\t                 init_layers=True,\n\t                 batch_norm=False,\n\t                 dropout_rate=0,\n\t                 n_layers=3,\n\t                 signal_key=None):\n\t        super().__init__()\n\t        # 1. Store configuration\n", "        self.n_input_features = n_input_features\n\t        self.n_output_features = n_output_features\n\t        self.n_pairwise_features = n_pairwise_features\n\t        self.pairwise_network_only = pairwise_network_only\n\t        self.n_input_relation_fusion_network = n_pairwise_features\n\t        self.activation = ACTIVATIONS[activation]\n\t        self.signal_key = signal_key\n\t        # 2. Create local feature extractor, if provided\n\t        self.on_input_hook = on_input_hook\n\t        if on_input_hook is None:\n", "            self.on_input_hook = DefaultOnInputHook(n_input_features)\n\t        # 2. Create pairwise relation network\n\t        self.pairwise_relation_network = PairwiseRelationNetwork(\n\t            self.on_input_hook.n_output, self.n_pairwise_features,\n\t            on_pairwise_relation_network_start_hook,\n\t            activation, None, init_layers,\n\t            batch_norm, dropout_rate, n_layers=n_layers,\n\t            signal_key=signal_key, standalone=pairwise_network_only\n\t        )\n\t        # 3. Create relation fusion network\n", "        self.relation_fusion_network = RelationFusionNetwork(\n\t            self.n_input_relation_fusion_network, n_output_features,\n\t            activation, output_activation, init_layers, batch_norm,\n\t            dropout_rate, n_layers=n_layers, signal_key=signal_key\n\t        )\n\t    def forward(self, x):\n\t        # x.shape == (batch_size, num_channels, feature_size)\n\t        x = self.on_input_hook(x)\n\t        # x.shape == (batch_size, num_channels, self.on_input_hook.n_output)\n\t        x = self.pairwise_relation_network(x)\n", "        # x.shape == (batch_size, self.n_pairwise_features)\n\t        if self.pairwise_network_only:\n\t            return x\n\t        x = self.relation_fusion_network(x)\n\t        # x.shape == (batch_size, self.n_output_features)\n\t        return x\n\tclass PairwiseRelationNetwork(nn.Module):\n\t    def __init__(self, n_input_features,\n\t                       n_output_features,\n\t                       on_start_hook=None,\n", "                       activation=\"relu\",\n\t                       output_activation=None,\n\t                       init_layers=True,\n\t                       batch_norm=False,\n\t                       dropout_rate=0,\n\t                       n_layers=3,\n\t                       signal_key=None,\n\t                       standalone=False,\n\t                       mask=True):\n\t        super().__init__()\n", "        self.signal_key = signal_key\n\t        self.standalone = standalone\n\t        self.output_activation = ACTIVATIONS[output_activation]\n\t        self.mask = mask\n\t        # 1. Create hook to be executed before network (Like extracting GCC-PHAT, or simply concatenating the pair)\n\t        self.on_start_hook = on_start_hook\n\t        if on_start_hook is None:\n\t            self.on_start_hook = DefaultPairwiseRelationNetworkStartHook(n_input_features)\n\t            # If pairwise feature extractor is provided, n_input_features refers to its output.\n\t            # Else, it refers to the input of each object, therefore we double it.\n", "        n_input_features = self.on_start_hook.n_output\n\t        # 2. Create neural network\n\t        self.pairwise_relation_network = MLP(n_input_features,\n\t                                                n_output_features,\n\t                                                n_output_features,\n\t                                                n_layers,\n\t                                                activation,\n\t                                                activation,\n\t                                                batch_norm,\n\t                                                dropout_rate)\n", "        if init_layers:\n\t            for layer in self.pairwise_relation_network.layers:\n\t                torch.nn.init.ones_(layer[0].weight)\n\t            #init_layer(self.pairwise_relation_network)\n\t    def forward(self, x):\n\t        if self.signal_key is None:\n\t            x_signal = x\n\t        else:\n\t            x_signal = x[self.signal_key]\n\t        batch_size, n_channels, n_input_features = x_signal.shape\n", "        # TODO: parallelize this?\n\t        pairwise_relations = []\n\t        for i in range(n_channels):\n\t            for j in range(i + 1, n_channels):\n\t                x_ij = self.on_start_hook(x, i, j)\n\t                pairwise_relation = self.pairwise_relation_network(x_ij)#*x_ij[:, :625]\n\t                # if self.mask:\n\t                #     pairwise_relation *= x_ij[:, :pairwise_relation.shape[1]]\n\t                pairwise_relations.append(pairwise_relation)\n\t        pairwise_relations = torch.stack(pairwise_relations, dim=1)\n", "        if self.standalone:\n\t            x = pairwise_relations.mean(dim=1)\n\t            if self.output_activation:\n\t                x = self.output_activation(x)\n\t            return x\n\t        if self.signal_key is not None:\n\t            x[\"signal\"] = pairwise_relations\n\t            return x\n\t        else:\n\t            pairwise_relations\n", "class RelationFusionNetwork(nn.Module):\n\t    def __init__(self, n_input_features,\n\t                       n_output_features,\n\t                       activation=\"relu\",\n\t                       output_activation=\"sigmoid\",\n\t                       init_layers=True,\n\t                       batch_norm=False,\n\t                       dropout_rate=0,\n\t                       n_layers=3,\n\t                       signal_key=None):\n", "        super().__init__()\n\t        self.relation_fusion_network = MLP(n_input_features,\n\t                                           n_output_features,\n\t                                           n_input_features,\n\t                                           n_layers,\n\t                                           activation,\n\t                                           output_activation,\n\t                                           batch_norm,\n\t                                           dropout_rate)\n\t        self.signal_key = signal_key\n", "        if init_layers:\n\t            for layer in self.relation_fusion_network.layers:\n\t                torch.nn.init.eye_(layer[0].weight)\n\t            #init_layer(self.relation_fusion_network)\n\t    def forward(self, x):\n\t        if self.signal_key is None:\n\t            x_signal = x\n\t        else:\n\t            x_signal = x[self.signal_key]\n\t        batch_size, n_channels, n_pairwise_features = x_signal.shape\n", "        x_signal = x_signal.mean(dim=1)\n\t        # x.shape == (batch_size, n_pairwise_features) \n\t        x_signal = self.relation_fusion_network(x_signal)\n\t        # x.shape == (batch_size, self.n_output_features) \n\t        return x_signal\n\tclass DefaultPairwiseRelationNetworkStartHook(nn.Module):\n\t    def __init__(self, n_input_features):\n\t        super().__init__()\n\t        self.n_output = 2*n_input_features\n\t    def forward(self, x, i, j):\n", "        x_i = x[:, i]\n\t        x_j = x[:, j]\n\t        x_ij = torch.cat([x_i, x_j], axis=-1)\n\t        return x_ij\n\tclass DefaultOnInputHook(nn.Module):\n\t    \"\"\"This is a placeholder for a torch.nn.Module which may be\n\t    provider by a user who'd like for some preprocessing to occur on their input\n\t    before sending it to the pairwise feature extractor.\n\t    An example of an on_input_hook function may be a feature extractor such as the \n\t    Discrete Fourier transform to be applied individually to each channel.\n", "    \"\"\"\n\t    def __init__(self, n_input_features):\n\t        super().__init__()\n\t        self.n_output = n_input_features\n\t    def forward(self, x):\n\t        return x"]}
{"filename": "gnn_ssl/models/base/cnn.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom .mlp import fc_block\n\tfrom .utils import ACTIVATIONS, AvgLayer, init_layer\n\tclass ConvBlock(nn.Module):\n\t    def __init__(self, in_channels, out_channels, \n\t                kernel_size=(3,3), stride=(1,1),\n\t                padding=(1,1), dilation=(1, 1),\n\t                pool_size=(2, 2), pool_type=\"avg\",\n\t                block_type=\"double\",\n", "                dropout_rate=0, batch_norm=False):\n\t        super().__init__()\n\t        # Dump parameters\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.kernel_size = kernel_size\n\t        self.stride = stride\n\t        self.padding = padding\n\t        self.dilation = dilation\n\t        self.pool_size = pool_size\n", "        self.block_type = block_type\n\t        self.dropout_rate = dropout_rate\n\t        self.batch_norm = batch_norm\n\t        blocks = [\n\t            nn.Conv2d(in_channels=in_channels, \n\t                      out_channels=out_channels,\n\t                      kernel_size=kernel_size, stride=stride,\n\t                      padding=padding, dilation=dilation),\n\t        ]\n\t        if batch_norm:\n", "            blocks.append(nn.BatchNorm2d(out_channels))\n\t        blocks.append(nn.ReLU())\n\t        if block_type == \"double\": \n\t            blocks.append(nn.Conv2d(in_channels=out_channels, \n\t                                   out_channels=out_channels,\n\t                                   kernel_size=kernel_size, stride=stride,\n\t                                   padding=padding, dilation=dilation))\n\t            if batch_norm:\n\t                blocks.append(nn.BatchNorm2d(out_channels))\n\t            blocks.append(nn.ReLU())\n", "        # Create activation, dropout and pooling blocks\n\t        if pool_type == \"avg\":\n\t            blocks.append(nn.AvgPool2d(pool_size))\n\t        elif pool_type == \"max\":\n\t            blocks.append(nn.MaxPool2d(pool_size))\n\t        else:\n\t            raise ValueError(f\"pool_type '{pool_type}' is invalid. Must be 'max' or 'avg'\")\n\t        if dropout_rate > 0:\n\t            blocks.append(nn.Dropout(dropout_rate))\n\t        self.model = nn.Sequential(*blocks)\n", "    def forward(self, x):\n\t        x = self.model(x)\n\t        if self.dropout_rate > 0:\n\t            x = self.dropout(x)\n\t        return x\n\t    def get_output_shape(self, input_shape):\n\t        \"\"\"\n\t        Args:\n\t            input_shape (tuple): (in_width, in_height)\n\t        Returns:\n", "            tuple: (out_width, out_height)\n\t        \"\"\"\n\t        output_shape = get_conv2d_output_shape(\n\t            input_shape, self.kernel_size, self.stride, self.dilation, self.padding)\n\t        if self.block_type == \"double\":\n\t            output_shape = get_conv2d_output_shape(\n\t                output_shape, self.kernel_size, self.stride, self.dilation, self.padding)\n\t        output_shape = get_pool2d_output_shape(output_shape, self.pool_size)\n\t        return output_shape\n\tclass Cnn1d(nn.Module):\n", "    def __init__(self, n_input_channels, n_output_features, n_hidden_channels,\n\t                 activation, output_activation, batch_norm, n_layers, n_metadata):\n\t        super().__init__()\n\t        activation = ACTIVATIONS[activation]\n\t        output_activation = ACTIVATIONS[output_activation]\n\t        layers = [\n\t            _conv_1d_block(n_input_channels, n_hidden_channels, activation, batch_norm=batch_norm)\n\t        ]\n\t        for _ in range(n_layers - 1): # -1 = skipping input layer\n\t            layers.append(\n", "                _conv_1d_block(n_hidden_channels,\n\t                            n_hidden_channels,\n\t                            activation,\n\t                            batch_norm=batch_norm\n\t                )\n\t            )\n\t        layers.append(AvgLayer(dim=2)) # dim0 = batch, dim1 = channels, dim2 = time_steps\n\t        self.conv_layers = nn.Sequential(*layers)\n\t        self.fc_layer = fc_block(\n\t            n_hidden_channels + 2*n_metadata, n_output_features,\n", "            activation=output_activation, batch_norm=False\n\t        )\n\t        self.n_metadata = n_metadata\n\t    def forward(self, x):\n\t        batch_size, n_channels, n_time_steps = x.shape\n\t        if self.n_metadata > 0:\n\t            x = x[:, :, :-self.n_metadata]\n\t            metadata = x[:, :, -self.n_metadata:]\n\t            metadata = metadata.reshape((batch_size, n_channels*self.n_metadata))\n\t        x = self.conv_layers(x)\n", "        if self.n_metadata > 0:\n\t            x = torch.cat([x, metadata], dim=1)\n\t        x = self.fc_layer(x)\n\t        return x\n\tdef _conv_1d_block(n_input_channels, n_output_channels,\n\t                   activation, kernel_size=3, stride=1,\n\t                   padding=1, pool_size=1, pool_type=\"avg\",\n\t                   batch_norm=False):\n\t    layers = [\n\t        nn.Conv1d(n_input_channels, n_output_channels,\n", "                    kernel_size, stride, padding)\n\t    ]\n\t    if activation:\n\t        layers.append(activation())\n\t    if batch_norm:\n\t        layers.append(nn.BatchNorm1d(n_output_channels))\n\t    if pool_size > 1:\n\t        if pool_type == \"avg\":\n\t            layers.append(nn.AvgPool1d(pool_size))\n\t        elif pool_type == \"max\":\n", "            layers.append(nn.MaxPool1d(pool_size))\n\t        else:\n\t            raise ValueError(\"Pooling layer must be 'max' or 'avg'\")\n\t    return nn.Sequential(*layers)\n\tdef get_conv2d_output_shape(input_shape: tuple,\n\t                            kernel_size: tuple,\n\t                            stride=(1, 1),\n\t                            dilation=(1, 1),\n\t                            padding=(0, 0)):\n\t    \"\"\"Compute the output of a convolutional layer.\n", "    See https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n\t    for more information.\n\t    \"\"\"\n\t    input_shape = torch.Tensor(input_shape)\n\t    kernel_size = torch.Tensor(kernel_size)\n\t    stride = torch.Tensor(stride)\n\t    dilation = torch.Tensor(dilation)\n\t    padding = torch.Tensor(padding)\n\t    n_output_shape = (input_shape + 2*padding - kernel_size - (dilation - 1)*(kernel_size - 1))/stride + 1\n\t    #print(n_output_shape)\n", "    n_output_shape = torch.floor(n_output_shape)\n\t    return n_output_shape\n\tdef get_pool2d_output_shape(n_input_shape, pool_size):\n\t    return get_conv2d_output_shape(n_input_shape,\n\t                                   pool_size,\n\t                                   pool_size)\n\tdef get_conv_pool_output_shape(n_input_shape: tuple,\n\t                               kernel_size: tuple,\n\t                               pool_size: tuple,\n\t                               stride=(1, 1),\n", "                               dilation=(1, 1),\n\t                               padding=(0, 0)):\n\t    conv_output = get_conv2d_output_shape(\n\t        n_input_shape, kernel_size, stride,\n\t        dilation, padding\n\t    )\n\t    pool_output = get_pool2d_output_shape(conv_output, pool_size)\n\t    return pool_output\n"]}
{"filename": "gnn_ssl/models/baselines/srp_phat.py", "chunked_list": ["import torch.nn as nn\n\tfrom pysoundloc.pysoundloc.models import srp_phat\n\tclass SrpPhat(nn.Module):\n\t    def __init__(self, config):\n\t        super().__init__()\n\t        self.feature_config, self.targets_config = config[\"features\"], config[\"targets\"]\n\t        self.n_points_per_axis = self.targets_config[\"n_points_per_axis\"]\n\t        self.sr = config[\"features\"][\"dataset\"][\"sr\"]\n\t        self.thickness = config[\"features\"][\"srp_thickness\"]\n\t    def forward(self, x):\n", "        mic_signals = x[\"signal\"]\n\t        mic_coords = x[\"metadata\"][\"local\"][\"mic_coordinates\"][..., :2]\n\t        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"][..., :2]\n\t        return srp_phat(mic_signals, mic_coords, room_dims,\n\t                        self.sr, self.n_points_per_axis,\n\t                        thickness=self.thickness)\n"]}
{"filename": "gnn_ssl/trainers/base.py", "chunked_list": ["import pytorch_lightning as pl\n\timport torch\n\tfrom hydra.utils import get_class\n\tfrom torch.optim.lr_scheduler import MultiStepLR\n\tclass BaseTrainer(pl.LightningModule):\n\t    \"\"\"Class which abstracts interactions with Hydra\n\t    and basic training/testing/validation conventions\n\t    \"\"\"\n\t    def __init__(self, config):\n\t        super().__init__()\n", "        model, training = config[\"model\"], config[\"training\"]\n\t        features, targets = config[\"features\"], config[\"targets\"]\n\t        self.model = get_class(model[\"class\"])(model)\n\t        # self.feature_extractor = get_class(features[\"class\"])(features)\n\t        self.loss = get_class(targets[\"loss_class\"])(targets)\n\t        self.model_config =  model\n\t        self.features_config = features\n\t        self.targets_config = targets\n\t        self.training_config = training\n\t        self.log_step = self.training_config[\"log_step\"]\n", "    def forward(self, x):\n\t        return self.model(x)\n\t    def _step(self, batch, batch_idx, epoch_type):\n\t        x, y = batch[0] # 0 is to ignore the microphone array index\n\t        # 1. Compute model output and loss\n\t        x = self.forward(x)\n\t        loss = self.loss(x, y, mean_reduce=True)\n\t        self.log_dict({f\"{epoch_type}_loss_step\": loss})\n\t        return {\n\t            \"loss\": loss\n", "        }\n\t    def training_step(self, batch, batch_idx):\n\t        return self._step(batch, batch_idx, \"train\")\n\t    def validation_step(self, batch, batch_idx):\n\t        return self._step(batch, batch_idx, \"validation\")\n\t    def test_step(self, batch, batch_idx):\n\t        return self._step(batch, batch_idx, \"test\")\n\t    def _epoch_end(self, outputs, epoch_type=\"train\"):\n\t        # 1. Compute epoch metrics\n\t        outputs = _merge_list_of_dicts(outputs)\n", "        tensorboard_logs = {\n\t            f\"{epoch_type}_loss\": outputs[\"loss\"].mean(),\n\t            # f\"{epoch_type}_std\": outputs[\"loss\"].std(),\n\t            \"step\": self.current_epoch\n\t        }\n\t        print(tensorboard_logs)\n\t        self.log_dict(tensorboard_logs)\n\t    def training_epoch_end(self, outputs):\n\t        self._epoch_end(outputs)\n\t    def validation_epoch_end(self, outputs):\n", "        self._epoch_end(outputs, epoch_type=\"validation\")\n\t    def test_epoch_end(self, outputs):\n\t        self._epoch_end(outputs, epoch_type=\"test\")\n\t    def forward(self, x):\n\t        return self.model(x)\n\t    def fit(self, dataset_train, dataset_val):\n\t        super().fit(self.model, dataset_train, val_dataloaders=dataset_val)\n\t    def test(self, dataset_test, ckpt_path=\"best\"):\n\t        super().test(self.model, dataset_test, ckpt_path=ckpt_path)\n\t    def configure_optimizers(self):\n", "        lr = self.training_config[\"learning_rate\"]\n\t        decay_step = self.training_config[\"learning_rate_decay_steps\"]\n\t        decay_value = self.training_config[\"learning_rate_decay_values\"]\n\t        if self.training_config[\"optimizer\"] == \"sgd\":\n\t            optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n\t        elif self.training_config[\"optimizer\"] == \"adam\":\n\t            optimizer = torch.optim.Adam(self.parameters(), lr=lr,\n\t                            betas=self.training_config[\"betas\"])\n\t        scheduler = MultiStepLR(optimizer, decay_step, decay_value)\n\t        return [optimizer], [scheduler]\n", "def _merge_list_of_dicts(list_of_dicts):\n\t    \"\"\"Function used at the end of an epoch.\n\t    It is used to merge together the many vectors generated at each step\n\t    \"\"\"\n\t    result = {}\n\t    def _add_to_dict(key, value):\n\t        if len(value.shape) == 0: # 0-dimensional tensor\n\t            value = value.unsqueeze(0)\n\t        if key not in result:\n\t            result[key] = value\n", "        else:\n\t            result[key] = torch.cat([\n\t                result[key], value\n\t            ])\n\t    for d in list_of_dicts:\n\t        for key, value in d.items():\n\t            _add_to_dict(key, value)\n\t    return result"]}
{"filename": "gnn_ssl/trainers/neural_srp.py", "chunked_list": ["from gnn_ssl.metrics import NormLoss\n\tfrom .base import BaseTrainer\n\tclass NeuralSrpTrainer(BaseTrainer):\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t        self.norm = NormLoss(\"l2\")\n\t    def _step(self, batch, batch_idx, epoch_type):\n\t        x, y = batch[0] # 0 is because the current dataloader is multi-microphone\n\t        if self.targets_config[\"type\"] == \"source_coordinates\":\n\t            n_output_coords = self.targets_config[\"n_output_coordinates\"]\n", "            y[\"source_coordinates\"] = y[\"source_coordinates\"][:, 0, :n_output_coords] # Only select first source\n\t        return super()._step((x, y), batch_idx, epoch_type)\n"]}
{"filename": "gnn_ssl/trainers/gnn_ssl.py", "chunked_list": ["from omegaconf import OmegaConf\n\timport torch\n\tfrom torch.optim.lr_scheduler import MultiStepLR\n\tfrom ..models.gnn_ssl import GnnSslNet\n\tfrom ..metrics import Loss\n\tfrom .base import BaseTrainer\n\tclass GnnSslNetTrainer(BaseTrainer):\n\t    \"\"\"This class abstracts the\n\t       training/validation/testing procedures\n\t       used for training a GnnSslNet\n", "    \"\"\"\n\t    def __init__(self, config):\n\t        super().__init__(config)\n\t        self.rmse = Loss(config[\"targets\"], mode=\"l2\", dim=1)\n\t    def _step(self, batch, batch_idx, log_model_output=False, log_labels=False):\n\t        \"\"\"This is the base step done on every training, validation and testing batch iteration.\n\t        This had to be overriden since we are using multiple datasets\n\t        \"\"\"\n\t        loss_vectors = []\n\t        outputs = []\n", "        targets = []\n\t        n_mics = []\n\t        # Loop for every microphone number\n\t        for (x, y) in batch:\n\t            n = x[\"signal\"].shape[1]\n\t            # 1. Compute model output and loss\n\t            output = self.model(x)\n\t            loss = self.loss(output, y, mean_reduce=False)\n\t            outputs.append(output)\n\t            targets.append(y)\n", "            loss_vectors.append(loss)\n\t            n_mics.append(n)\n\t            # 2. RMSE\n\t            rmse_error = self.rmse(output, y, mean_reduce=True)\n\t            self.log(f\"rmse_{n}_mics\", rmse_error, on_step=True, prog_bar=False, on_epoch=False)\n\t        output_dict = {}\n\t        for i, n in enumerate(n_mics):\n\t            loss_vector = loss_vectors[i]\n\t            mean_loss = loss_vector.mean()\n\t            output_dict[f\"loss_vector_{n}\"] = loss_vector.detach().cpu()\n", "            output_dict[f\"mean_loss_{n}\"] = mean_loss.detach().cpu()\n\t            # TODO: Add these to a callback\n\t            # 2. Log model output\n\t            if log_model_output:\n\t                output_dict[f\"model_outputs_{n}\"] = outputs[i]\n\t        output_dict[\"loss\"] = torch.stack(loss_vectors).mean()\n\t        return output_dict\n"]}
{"filename": "gnn_ssl/datasets/__init__.py", "chunked_list": []}
{"filename": "gnn_ssl/datasets/distributed_ssl.py", "chunked_list": ["import torch\n\tfrom torch.utils.data import DataLoader\n\tfrom sydra.sydra.dataset import SydraDataset\n\tfrom ..feature_extractors.metadata import format_metadata\n\tclass DistributedSslDataset(SydraDataset):\n\t    def __init__(self, dataset_dir, config,\n\t                 trim_signals_mode=\"random\", flatten_metadata=True):\n\t        super().__init__(dataset_dir, config[\"sr\"],\n\t                         config[\"n_input_seconds\"], trim_signals_mode)\n\t        self.use_rt60_as_metadata = config[\"use_rt60_as_metadata\"]\n", "        self.flatten_metadata = flatten_metadata\n\t    def __getitem__(self, index):\n\t        (x, y) = super().__getitem__(index)\n\t        # 1. Add metadata to input\n\t        # Collapse channels and arrays\n\t        x = x.flatten(start_dim=0, end_dim=1)\n\t        x = {\n\t            \"signal\": x,\n\t            \"metadata\": format_metadata(y, self.use_rt60_as_metadata)\n\t        }\n", "        if len(y[\"mic_coordinates\"].shape) == 3:\n\t            # Array\n\t            y[\"mic_coordinates\"] = y[\"mic_coordinates\"][0]\n\t        if len(y[\"source_coordinates\"].shape) == 1:\n\t            # Expand to multi-source\n\t            y[\"source_coordinates\"] = y[\"source_coordinates\"].unsqueeze(0)\n\t        y = {\n\t            \"source_coordinates\": y[\"source_coordinates\"],\n\t            \"mic_coordinates\": y[\"mic_coordinates\"],\n\t            \"room_dims\": y[\"room_dims\"],\n", "        }\n\t        return (x, y)\n\tclass DistributedSslDataLoader(DataLoader):\n\t    def __init__(self, config, dataset_path, shuffle=False,\n\t                 batch_size=16, n_workers=1, **kwargs):\n\t        self.config = config\n\t        self.dataset_path = dataset_path\n\t        if type(dataset_path) == str:\n\t            dataset_path = [dataset_path] # Convert to a 1-element list\n\t        dataset_config = config[\"dataset\"]\n", "        datasets = [\n\t            DistributedSslDataset(d, dataset_config)\n\t            for d in dataset_path\n\t        ]\n\t        dataset = ConcatDataset(datasets)\n\t        super().__init__(\n\t            dataset,\n\t            batch_size=batch_size,\n\t            shuffle=shuffle,\n\t            pin_memory=True,\n", "            drop_last=False,\n\t            num_workers=n_workers,\n\t        )\n\t# Source: https://forums.pytorchlightning.ai/t/how-to-use-multiple-train-dataloaders-with-different-lengths/214/2\n\tclass ConcatDataset(torch.utils.data.Dataset):\n\t    def __init__(self, datasets):\n\t        self.datasets = datasets\n\t    def __getitem__(self, i):\n\t        return tuple(d[i] for d in self.datasets)\n\t    def __len__(self):\n", "        return min(len(d) for d in self.datasets)\n"]}
