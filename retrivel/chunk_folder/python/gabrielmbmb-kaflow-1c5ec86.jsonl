{"filename": "tests/test_serializers.py", "chunked_list": ["from __future__ import annotations\n\tfrom pydantic import BaseModel\n\tfrom kaflow.serializers import (\n\t    AvroSerializer,\n\t    JsonSerializer,\n\t    ProtobufSerializer,\n\t    Serializer,\n\t)\n\tfrom tests.key_value_pb2 import KeyValue\n\tclass KeyValueModel(BaseModel):\n", "    key: str\n\t    value: str\n\tdef test_serializer_extra_annotation_keys() -> None:\n\t    assert Serializer.extra_annotations_keys() == []\n\tdef test_json_serializer_serialize() -> None:\n\t    serializer = JsonSerializer()\n\t    assert serializer.serialize({\"key\": \"value\"}) == b'{\"key\": \"value\"}'\n\tdef test_json_serializer_deserialize() -> None:\n\t    serializer = JsonSerializer()\n\t    assert serializer.deserialize(b'{\"key\": \"value\"}') == {\"key\": \"value\"}\n", "def test_avro_serializer_serialize() -> None:\n\t    serializer = AvroSerializer(\n\t        avro_schema={\n\t            \"type\": \"record\",\n\t            \"name\": \"test\",\n\t            \"fields\": [\n\t                {\"name\": \"key\", \"type\": \"string\"},\n\t                {\"name\": \"value\", \"type\": \"string\"},\n\t            ],\n\t        }\n", "    )\n\t    assert (\n\t        serializer.serialize({\"key\": \"unit_test_key\", \"value\": \"unit_test_value\"})\n\t        == b\"\\x1aunit_test_key\\x1eunit_test_value\"\n\t    )\n\tdef test_avro_serializer_deserialize() -> None:\n\t    serializer = AvroSerializer(\n\t        avro_schema={\n\t            \"type\": \"record\",\n\t            \"name\": \"test\",\n", "            \"fields\": [\n\t                {\"name\": \"key\", \"type\": \"string\"},\n\t                {\"name\": \"value\", \"type\": \"string\"},\n\t            ],\n\t        }\n\t    )\n\t    assert serializer.deserialize(b\"\\x1aunit_test_key\\x1eunit_test_value\") == {\n\t        \"key\": \"unit_test_key\",\n\t        \"value\": \"unit_test_value\",\n\t    }\n", "def test_avro_extra_annotation_keys() -> None:\n\t    assert AvroSerializer.extra_annotations_keys() == [\"avro_schema\"]\n\tdef test_protobuf_serializer_serialize() -> None:\n\t    serializer = ProtobufSerializer(protobuf_schema=KeyValue)\n\t    assert (\n\t        serializer.serialize({\"key\": \"unit_test_key\", \"value\": \"unit_test_value\"})\n\t        == b\"\\n\\runit_test_key\\x12\\x0funit_test_value\"\n\t    )\n\tdef test_protobuf_serializer_deserialize() -> None:\n\t    serializer = ProtobufSerializer(protobuf_schema=KeyValue)\n", "    assert serializer.deserialize(b\"\\n\\runit_test_key\\x12\\x0funit_test_value\") == {\n\t        \"key\": \"unit_test_key\",\n\t        \"value\": \"unit_test_value\",\n\t    }\n\tdef test_protobuf_extra_annotation_keys() -> None:\n\t    assert ProtobufSerializer.extra_annotations_keys() == [\"protobuf_schema\"]\n"]}
{"filename": "tests/test_application.py", "chunked_list": ["def test_application() -> None:\n\t    assert True\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/key_value_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: key_value.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\tfrom google.protobuf.internal import builder as _builder\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n\t    b'\\n\\x0fkey_value.proto\\x12\\x06kaflow\"&\\n\\x08KeyValue\\x12\\x0b\\n\\x03key\\x18\\x01'\n\t    b\" \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\tb\\x06proto3\"\n\t)\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, \"key_value_pb2\", globals())\n\tif _descriptor._USE_C_DESCRIPTORS is False:\n\t    DESCRIPTOR._options = None\n\t    _KEYVALUE._serialized_start = 27  # noqa\n\t    _KEYVALUE._serialized_end = 65  # noqa\n", "# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "tests/_utils/__init__.py", "chunked_list": []}
{"filename": "tests/_utils/test_asyncio.py", "chunked_list": ["import asyncio\n\tfrom kaflow._utils.asyncio import asyncify\n\tdef test_asyncify() -> None:\n\t    def func(a: int, b: int) -> int:\n\t        return a + b\n\t    func = asyncify(func)\n\t    assert asyncio.iscoroutinefunction(func)\n\t    assert asyncio.run(func(1, 2)) == 3\n"]}
{"filename": "tests/_utils/test_inspect.py", "chunked_list": ["import inspect\n\tfrom typing import Any, Callable\n\timport pytest\n\tfrom typing_extensions import Annotated\n\tfrom kaflow._utils.inspect import (\n\t    annotated_param_with,\n\t    has_return_annotation,\n\t    is_annotated_param,\n\t    is_not_coroutine_function,\n\t)\n", "def func_with_none_return_annotation() -> None:\n\t    pass\n\tdef func_with_no_return_annotation():\n\t    pass\n\tdef func_with_return_annotation() -> int:\n\t    pass\n\tasync def coroutine() -> None:\n\t    pass\n\t@pytest.mark.parametrize(\n\t    \"param, expected\", [(Annotated[int, \"extra\", \"metadata\"], True), (int, False)]\n", ")\n\tdef test_is_annotated_param_true(param: Any, expected: bool) -> None:\n\t    assert is_annotated_param(param) == expected\n\tclass DummyUnitTest:\n\t    pass\n\t@pytest.mark.parametrize(\n\t    \"param, annotated_with, expected\",\n\t    [\n\t        (Annotated[int, \"magic_unit_test_flag\"], \"magic_unit_test_flag\", True),\n\t        (Annotated[int, \"extra\", \"metadata\"], \"magic_unit_test_flag\", False),\n", "        (Annotated[int, DummyUnitTest()], DummyUnitTest, True),\n\t    ],\n\t)\n\tdef test_annotated_param_with(param: Any, annotated_with: Any, expected: bool) -> None:\n\t    assert annotated_param_with(annotated_with, param) == expected\n\t@pytest.mark.parametrize(\n\t    \"func, expected\",\n\t    [\n\t        (func_with_none_return_annotation, False),\n\t        (func_with_no_return_annotation, False),\n", "        (func_with_return_annotation, True),\n\t    ],\n\t)\n\tdef test_has_return_annotation(func: Callable[..., Any], expected: bool) -> None:\n\t    signature = inspect.signature(func)\n\t    assert has_return_annotation(signature) == expected\n\t@pytest.mark.parametrize(\n\t    \"func, expected\", [(coroutine, False), (func_with_return_annotation, True)]\n\t)\n\tdef test_is_not_coroutine_function(func, expected: bool) -> None:\n", "    assert is_not_coroutine_function(func) == expected\n"]}
{"filename": "kaflow/logger.py", "chunked_list": ["import logging\n\tlogger = logging.getLogger(\"kaflow\")\n"]}
{"filename": "kaflow/dependencies.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import Any, Literal\n\tfrom di.dependent import Marker\n\tScope = Literal[\"app\", \"consumer\"]\n\tScopes = (\"app\", \"consumer\")\n\tdef Depends(\n\t    call: Any = None,\n\t    *,\n\t    use_cache: bool = True,\n\t    wire: bool = True,\n", "    scope: Scope | None = None,\n\t) -> Marker:\n\t    return Marker(\n\t        call=call,\n\t        use_cache=use_cache,\n\t        wire=wire,\n\t        scope=scope,\n\t    )\n"]}
{"filename": "kaflow/message.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import Any, NamedTuple, Union\n\tfrom kaflow.typing import TopicValueKeyHeader\n\tclass ReadMessage(NamedTuple):\n\t    value: Union[TopicValueKeyHeader, None] = None\n\t    key: Union[TopicValueKeyHeader, None] = None\n\t    headers: Union[dict[str, Any], None] = None\n\t    offset: Union[int, None] = None\n\t    partition: Union[int, None] = None\n\t    timestamp: Union[int, None] = None\n", "class Message(NamedTuple):\n\t    value: Union[bytes, None] = None\n\t    key: Union[bytes, None] = None\n\t    headers: Union[dict[str, bytes], None] = None\n\t    offset: Union[int, None] = None\n\t    partition: Union[int, None] = None\n\t    timestamp: Union[int, None] = None\n"]}
{"filename": "kaflow/_consumer.py", "chunked_list": ["from __future__ import annotations\n\timport asyncio\n\tfrom typing import TYPE_CHECKING, Any, Awaitable, Callable, Coroutine, Sequence\n\tfrom di.dependent import Dependent\n\tfrom di.executors import AsyncExecutor\n\tfrom pydantic import BaseModel\n\tfrom kaflow.dependencies import Scopes\n\tfrom kaflow.exceptions import KaflowDeserializationException\n\tfrom kaflow.message import Message, ReadMessage\n\tif TYPE_CHECKING:\n", "    from aiokafka import ConsumerRecord\n\t    from di import Container, ScopeState\n\t    from kaflow.applications import ConsumerFunc, DeserializationErrorHandlerFunc\n\t    from kaflow.serializers import Serializer\n\t    from kaflow.typing import TopicValueKeyHeader\n\tdef _deserialize(\n\t    value: bytes,\n\t    param_type: type[TopicValueKeyHeader] | None,\n\t    deserializer: Serializer | None,\n\t) -> TopicValueKeyHeader:\n", "    if deserializer:\n\t        deserialized: dict[str, Any] = deserializer.deserialize(value)\n\t        if (\n\t            param_type is not None\n\t            and hasattr(param_type, \"__bases__\")\n\t            and BaseModel in param_type.__bases__\n\t        ):\n\t            return param_type(**deserialized)\n\t        return deserialized\n\t    return value\n", "class TopicConsumerFunc:\n\t    __slots__ = (\n\t        \"name\",\n\t        \"container\",\n\t        \"publish_fn\",\n\t        \"exception_handlers\",\n\t        \"deserialization_error_handler\",\n\t        \"func\",\n\t        \"value_param_type\",\n\t        \"value_deserializer\",\n", "        \"key_param_type\",\n\t        \"key_deserializer\",\n\t        \"headers_type_deserializers\",\n\t        \"sink_topics\",\n\t        \"executor\",\n\t        \"container_state\",\n\t        \"dependent\",\n\t    )\n\t    def __init__(\n\t        self,\n", "        *,\n\t        name: str,\n\t        container: Container,\n\t        publish_fn: Callable[\n\t            [\n\t                str,\n\t                bytes | None,\n\t                bytes | None,\n\t                dict[str, bytes] | None,\n\t                int | None,\n", "                int | None,\n\t            ],\n\t            Coroutine[Any, Any, None],\n\t        ],\n\t        exception_handlers: dict[type[Exception], Callable[..., Awaitable[None]]],\n\t        deserialization_error_handler: DeserializationErrorHandlerFunc | None = None,\n\t        func: ConsumerFunc,\n\t        value_param_type: type[TopicValueKeyHeader],\n\t        value_deserializer: Serializer | None = None,\n\t        key_param_type: type[TopicValueKeyHeader] | None = None,\n", "        key_deserializer: Serializer | None = None,\n\t        headers_type_deserializers: dict[\n\t            str, tuple[type[TopicValueKeyHeader], Serializer | None]\n\t        ]\n\t        | None = None,\n\t        sink_topics: Sequence[str] | None = None,\n\t    ) -> None:\n\t        self.name = name\n\t        self.container = container\n\t        self.publish_fn = publish_fn\n", "        self.exception_handlers = exception_handlers\n\t        self.deserialization_error_handler = deserialization_error_handler\n\t        self.func = func\n\t        self.value_param_type = value_param_type\n\t        self.value_deserializer = value_deserializer\n\t        self.key_param_type = key_param_type\n\t        self.key_deserializer = key_deserializer\n\t        self.headers_type_deserializers = headers_type_deserializers\n\t        self.sink_topics = sink_topics\n\t        self.executor = AsyncExecutor()\n", "    def prepare(self, state: ScopeState) -> None:\n\t        self.container_state = state\n\t        self.dependent = self.container.solve(\n\t            Dependent(self.func, scope=\"consumer\"), scopes=Scopes\n\t        )\n\t    def _deserialize_value(self, value: bytes) -> TopicValueKeyHeader:\n\t        return _deserialize(value, self.value_param_type, self.value_deserializer)\n\t    def _deserialize_key(self, key: bytes) -> TopicValueKeyHeader | None:\n\t        if self.key_param_type:\n\t            return _deserialize(key, self.key_param_type, self.key_deserializer)\n", "        return None\n\t    def _deserialize_headers(\n\t        self, headers: Sequence[tuple[str, bytes]]\n\t    ) -> dict[str, Any] | None:\n\t        if self.headers_type_deserializers:\n\t            headers_ = {}\n\t            for key, value in headers:\n\t                header_type, deserializer = self.headers_type_deserializers.get(\n\t                    key, (None, None)\n\t                )\n", "                headers_[key] = _deserialize(value, header_type, deserializer)\n\t            return headers_\n\t        return None\n\t    async def _deserialize(\n\t        self, record: ConsumerRecord\n\t    ) -> tuple[\n\t        TopicValueKeyHeader | None,\n\t        TopicValueKeyHeader | None,\n\t        dict[str, TopicValueKeyHeader] | None,\n\t        bool,\n", "    ]:\n\t        async def handle_deserialization_error(\n\t            error_message: str, record: ConsumerRecord, exception: Exception\n\t        ) -> None:\n\t            exc = KaflowDeserializationException(\n\t                error_message.format(self.name), record=record\n\t            )\n\t            if not self.deserialization_error_handler:\n\t                raise exc from exception\n\t            await self.deserialization_error_handler(exc)\n", "        deserialized = True\n\t        try:\n\t            value = self._deserialize_value(record.value)\n\t        except Exception as e:\n\t            await handle_deserialization_error(\n\t                (\n\t                    \"Failed to deserialize value of message comming from topic\"\n\t                    f\" `{self.name}`\"\n\t                ),\n\t                record,\n", "                e,\n\t            )\n\t            value = None\n\t            deserialized = False\n\t        try:\n\t            key = self._deserialize_key(record.key)\n\t        except Exception as e:\n\t            await handle_deserialization_error(\n\t                (\n\t                    \"Failed to deserialize key of message comming from topic\"\n", "                    f\" `{self.name}`\"\n\t                ),\n\t                record,\n\t                e,\n\t            )\n\t            key = None\n\t            deserialized = False\n\t        try:\n\t            headers = self._deserialize_headers(record.headers)\n\t        except Exception as e:\n", "            await handle_deserialization_error(\n\t                (\n\t                    \"Failed to deserialize headers of message comming from topic\"\n\t                    f\" `{self.name}`\"\n\t                ),\n\t                record,\n\t                e,\n\t            )\n\t            headers = None\n\t            deserialized\n", "        return value, key, headers, deserialized\n\t    def _lookup_exception_handler(\n\t        self, exc: Exception\n\t    ) -> Callable[..., Awaitable[None]] | None:\n\t        for cls in type(exc).__mro__:\n\t            if cls in self.exception_handlers:\n\t                return self.exception_handlers[cls]\n\t        return None\n\t    async def _execute_dependent(\n\t        self,\n", "        consumer_state: ScopeState,\n\t        message: ReadMessage,\n\t    ) -> Any:\n\t        try:\n\t            return await self.dependent.execute_async(\n\t                executor=self.executor,\n\t                state=consumer_state,\n\t                values={ReadMessage: message},\n\t            )\n\t        except tuple(self.exception_handlers.keys()) as e:\n", "            handler = self._lookup_exception_handler(e)\n\t            if not handler:\n\t                raise e\n\t            await handler(e)\n\t            return None\n\t    async def _publish_messages(self, message: Message) -> None:\n\t        if self.sink_topics:\n\t            await asyncio.gather(\n\t                *[\n\t                    self.publish_fn(\n", "                        topic,\n\t                        message.value,\n\t                        message.key,\n\t                        message.headers,\n\t                        message.partition,\n\t                        message.offset,\n\t                    )\n\t                    for topic in self.sink_topics\n\t                ]\n\t            )\n", "    async def _process(self, read_message: ReadMessage) -> Message | None:\n\t        async with self.container.enter_scope(\n\t            \"consumer\", state=self.container_state\n\t        ) as consumer_state:\n\t            message = await self._execute_dependent(\n\t                consumer_state=consumer_state, message=read_message\n\t            )\n\t        if message and isinstance(message, Message):\n\t            await self._publish_messages(message)\n\t            return message\n", "        return None\n\t    async def consume(self, record: ConsumerRecord) -> Message | None:\n\t        value, key, headers, deserialized = await self._deserialize(record)\n\t        if not deserialized:\n\t            return None\n\t        message = ReadMessage(\n\t            value=value,\n\t            key=key,\n\t            headers=headers,\n\t            offset=record.offset,\n", "            partition=record.partition,\n\t            timestamp=record.timestamp,\n\t        )\n\t        return await self._process(read_message=message)\n"]}
{"filename": "kaflow/__init__.py", "chunked_list": ["from kaflow.applications import Kaflow\n\tfrom kaflow.dependencies import Depends\n\tfrom kaflow.message import Message\n\tfrom kaflow.parameters import (\n\t    FromHeader,\n\t    FromKey,\n\t    FromValue,\n\t    MessageOffset,\n\t    MessagePartition,\n\t    MessageTimestamp,\n", ")\n\tfrom kaflow.serializers import Json, String, has_fastavro, has_protobuf\n\t__all__ = [\n\t    \"Kaflow\",\n\t    \"Depends\",\n\t    \"Message\",\n\t    \"FromHeader\",\n\t    \"FromKey\",\n\t    \"FromValue\",\n\t    \"MessageOffset\",\n", "    \"MessagePartition\",\n\t    \"MessageTimestamp\",\n\t    \"Json\",\n\t    \"String\",\n\t]\n\tif has_fastavro:\n\t    from kaflow.serializers import Avro  # noqa: F401\n\t    __all__.append(\"Avro\")\n\tif has_protobuf:\n\t    from kaflow.serializers import Protobuf  # noqa: F401\n", "    __all__.append(\"Protobuf\")\n\t__version__ = \"0.2.2\"\n"]}
{"filename": "kaflow/typing.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import Union\n\tfrom pydantic import BaseModel\n\tTopicValueKeyHeader = Union[bytes, object, BaseModel]\n"]}
{"filename": "kaflow/serializers.py", "chunked_list": ["from __future__ import annotations\n\timport io\n\timport json\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import TYPE_CHECKING, Any, TypeVar, cast\n\tfrom typing_extensions import Annotated\n\tif TYPE_CHECKING:\n\t    pass\n\ttry:\n\t    import fastavro\n", "    has_fastavro = True\n\texcept ImportError:\n\t    has_fastavro = False\n\ttry:\n\t    from google.protobuf import json_format\n\t    has_protobuf = True\n\texcept ImportError:\n\t    has_protobuf = False\n\tMESSAGE_SERIALIZER_FLAG = \"MessageSerializer\"\n\tT = TypeVar(\"T\")\n", "class Serializer(ABC):\n\t    @abstractmethod\n\t    def serialize(self, data: Any) -> bytes:\n\t        ...\n\t    @abstractmethod\n\t    def deserialize(self, data: bytes) -> Any:\n\t        ...\n\t    @staticmethod\n\t    def extra_annotations_keys() -> list[str]:\n\t        return []\n", "class StringSerializer(Serializer):\n\t    def __init__(self, **kwargs: Any) -> None:\n\t        pass\n\t    def serialize(self, data: Any) -> bytes:\n\t        return str(data).encode()\n\t    def deserialize(self, data: bytes) -> Any:\n\t        return data.decode()\n\tString = Annotated[T, StringSerializer, MESSAGE_SERIALIZER_FLAG]\n\tclass JsonSerializer(Serializer):\n\t    def __init__(self, **kwargs: Any) -> None:\n", "        pass\n\t    def serialize(self, data: Any) -> bytes:\n\t        return json.dumps(data).encode()\n\t    def deserialize(self, data: bytes) -> Any:\n\t        return json.loads(data)\n\tJson = Annotated[T, JsonSerializer, MESSAGE_SERIALIZER_FLAG]\n\tif has_fastavro:\n\t    class AvroSerializer(Serializer):\n\t        def __init__(\n\t            self,\n", "            avro_schema: dict[str, Any] | None = None,\n\t            include_schema: bool = False,\n\t            seek_offset: int | None = None,\n\t            **kwargs: Any,\n\t        ) -> None:\n\t            self.avro_schema = avro_schema\n\t            self.include_schema = include_schema\n\t            self.seek_offset = seek_offset\n\t        def serialize(self, data: Any) -> bytes:\n\t            bytes_io = io.BytesIO()\n", "            if self.include_schema:\n\t                fastavro.writer(bytes_io, self.avro_schema, [data])\n\t            else:\n\t                fastavro.schemaless_writer(bytes_io, self.avro_schema, data)\n\t            return bytes_io.getvalue()\n\t        def deserialize(self, data: bytes) -> Any:\n\t            bytes_io = io.BytesIO(data)\n\t            if self.seek_offset is not None:\n\t                bytes_io.seek(self.seek_offset)\n\t            if self.avro_schema:\n", "                return fastavro.schemaless_reader(bytes_io, self.avro_schema)\n\t            return list(fastavro.reader(bytes_io))[0]\n\t        @staticmethod\n\t        def extra_annotations_keys() -> list[str]:\n\t            return [\"avro_schema\"]\n\t    Avro = Annotated[T, AvroSerializer, MESSAGE_SERIALIZER_FLAG]\n\tif has_protobuf:\n\t    class ProtobufSerializer(Serializer):\n\t        def __init__(self, protobuf_schema: type[Any], **kwargs: Any) -> None:\n\t            self.protobuf_schema = protobuf_schema\n", "        def serialize(self, data: Any) -> bytes:\n\t            entity = self.protobuf_schema()\n\t            for key, value in data.items():\n\t                setattr(entity, key, value)\n\t            return cast(bytes, entity.SerializeToString())\n\t        def deserialize(self, data: bytes) -> Any:\n\t            entity = self.protobuf_schema()\n\t            entity.ParseFromString(data)\n\t            return json_format.MessageToDict(entity)\n\t        @staticmethod\n", "        def extra_annotations_keys() -> list[str]:\n\t            return [\"protobuf_schema\"]\n\t    Protobuf = Annotated[T, ProtobufSerializer, MESSAGE_SERIALIZER_FLAG]\n"]}
{"filename": "kaflow/testclient.py", "chunked_list": ["from __future__ import annotations\n\timport asyncio\n\tfrom functools import wraps\n\tfrom time import time\n\tfrom typing import TYPE_CHECKING, Any, Awaitable, Callable\n\tfrom aiokafka import ConsumerRecord\n\tif TYPE_CHECKING:\n\t    from kaflow.applications import Kaflow\n\t    from kaflow.message import Message\n\tdef intercept_publish(\n", "    func: Callable[..., Awaitable[None]]\n\t) -> Callable[..., Awaitable[None]]:\n\t    @wraps(func)\n\t    async def wrapper(*args: Any, **kwargs: Any) -> None:\n\t        pass\n\t    return wrapper\n\tclass TestClient:\n\t    \"\"\"Test client for testing a `Kaflow` application.\"\"\"\n\t    def __init__(self, app: Kaflow) -> None:\n\t        self.app = app\n", "        self.app._publish = intercept_publish(self.app._publish)  # type: ignore\n\t        self._loop = asyncio.get_event_loop()\n\t    def publish(\n\t        self,\n\t        topic: str,\n\t        value: bytes,\n\t        key: bytes | None = None,\n\t        headers: dict[str, bytes] | None = None,\n\t        partition: int = 0,\n\t        offset: int = 0,\n", "        timestamp: int | None = None,\n\t    ) -> Message | None:\n\t        if timestamp is None:\n\t            timestamp = int(time())\n\t        record = ConsumerRecord(\n\t            topic=topic,\n\t            partition=partition,\n\t            offset=offset,\n\t            timestamp=timestamp,\n\t            timestamp_type=0,\n", "            key=key,\n\t            value=value,\n\t            checksum=0,\n\t            serialized_key_size=len(key) if key else 0,\n\t            serialized_value_size=len(value),\n\t            headers=headers,\n\t        )\n\t        async def _publish() -> Message | None:\n\t            consumer = self.app._get_consumer(topic)\n\t            async with self.app.lifespan():\n", "                return await consumer.consume(record)\n\t        return self._loop.run_until_complete(_publish())\n"]}
{"filename": "kaflow/parameters.py", "chunked_list": ["from __future__ import annotations\n\timport inspect\n\tfrom typing import TYPE_CHECKING, Any, TypeVar\n\tfrom di.dependent import Dependent, Marker\n\tfrom typing_extensions import Annotated\n\tfrom kaflow._utils.inspect import annotated_param_with\n\tfrom kaflow.message import ReadMessage\n\tfrom kaflow.serializers import Serializer\n\tif TYPE_CHECKING:\n\t    from kaflow.applications import ConsumerFunc\n", "    from kaflow.typing import TopicValueKeyHeader\n\tFROM_VALUE_FLAG = \"from_value\"\n\tFROM_KEY_FLAG = \"from_key\"\n\tFROM_HEADER_FLAG = \"from_header\"\n\tdef _get_serializer_info(param: Any) -> tuple[type[Serializer] | None, dict[str, Any]]:\n\t    serializer: type[Serializer] | None = None\n\t    serializer_extra: dict[str, Any] = {}\n\t    for item in param.__metadata__:\n\t        if inspect.isclass(item) and issubclass(item, Serializer):\n\t            serializer = item\n", "        elif isinstance(item, dict):\n\t            serializer_extra = item\n\t    return serializer, serializer_extra\n\tdef _annotated_serializer_info(\n\t    param: tuple[str, Any], func_name: str\n\t) -> tuple[type[TopicValueKeyHeader], type[Serializer] | None, dict[str, Any]]:\n\t    \"\"\"Get the type and serializer of a parameter annotated (`Annotated[...]`) with\n\t    a Kaflow serializer. This function expect the parameter to be `Annotated` with a\n\t    Kaflow serializer, the `kaflow.serializers.MESSAGE_SERIALIZER_FLAG`, and optionally\n\t    with a `dict` containing extra information that could be used by the serializer.\n", "    Args:\n\t        param: the annotated parameter to get the type and serializer from.\n\t        func_name: the name of the function that contains the annotated parameter.\n\t    Returns:\n\t        A tuple with the type and serializer of the annotated parameter, and a `dict`\n\t        containing extra information that could be used by the serializer.\n\t    \"\"\"\n\t    param_type = param[1].__args__[0]\n\t    serializer, serializer_extra = _get_serializer_info(param[1])\n\t    if not serializer and param_type is not bytes:\n", "        raise ValueError(\n\t            f\"'{param[0]}' parameter of '{func_name}' function has not been annotated\"\n\t            \" with a Kaflow serializer and its type is not `bytes`. Please annotate\"\n\t            \" the parameter with a Kaflow serializer or use `bytes` as the type.\"\n\t        )\n\t    return param_type, serializer, serializer_extra\n\tdef _serializer_info(\n\t    param: tuple[str, Any], func_name: str\n\t) -> tuple[type[TopicValueKeyHeader], Serializer | None]:\n\t    param_type, serializer, serializer_extra = _annotated_serializer_info(\n", "        param, func_name\n\t    )\n\t    if serializer:\n\t        return param_type, serializer(**serializer_extra)\n\t    return param_type, None\n\tdef _get_params(signature: inspect.Signature) -> dict[str, list[tuple[str, Any]]]:\n\t    params: dict[str, list[Any]] = {\n\t        FROM_VALUE_FLAG: [],\n\t        FROM_KEY_FLAG: [],\n\t        FROM_HEADER_FLAG: [],\n", "    }\n\t    for param in signature.parameters.values():\n\t        if annotated_param_with(Value, param.annotation):\n\t            params[FROM_VALUE_FLAG].append((param.name, param.annotation))\n\t        elif annotated_param_with(Key, param.annotation):\n\t            params[FROM_KEY_FLAG].append((param.name, param.annotation))\n\t        elif annotated_param_with(Header, param.annotation):\n\t            params[FROM_HEADER_FLAG].append((param.name, param.annotation))\n\t    return params\n\tdef _get_from_value_param_info(\n", "    params: dict[str, list[tuple[str, Any]]], func_name: str\n\t) -> tuple[type[TopicValueKeyHeader], Serializer | None]:\n\t    if not params[FROM_VALUE_FLAG]:\n\t        raise ValueError(\n\t            f\"'{func_name}' function does not have a parameter annotated\"\n\t            \" `FromValue` to receive the value of the message from the topic.\"\n\t        )\n\t    if len(params[FROM_VALUE_FLAG]) > 1:\n\t        raise ValueError(\n\t            f\"'{func_name}' function has more than one parameter\"\n", "            \" annotated `FromValue`. Only one parameter can be annotated\"\n\t            \" `FromValue` to receive the value of the message from the topic.\"\n\t        )\n\t    return _serializer_info(params[FROM_VALUE_FLAG][0], func_name)\n\tdef _get_from_key_param_info(\n\t    params: dict[str, list[tuple[str, Any]]], func_name: str\n\t) -> tuple[type[TopicValueKeyHeader] | None, Serializer | None]:\n\t    key_param_type: type[TopicValueKeyHeader] | None = None\n\t    key_serializer: Serializer | None = None\n\t    if params[FROM_KEY_FLAG]:\n", "        key_param_type, key_serializer = _serializer_info(\n\t            param=params[FROM_KEY_FLAG][0], func_name=func_name\n\t        )\n\t    return key_param_type, key_serializer\n\tdef _get_from_headers_param_info(\n\t    params: dict[str, list[Any]], func_name: str\n\t) -> dict[str, tuple[type[TopicValueKeyHeader], Serializer | None]] | None:\n\t    headers = {}\n\t    for header_param in params[FROM_HEADER_FLAG]:\n\t        name, _ = header_param\n", "        (header_param_type, header_serializer) = _serializer_info(\n\t            header_param, func_name\n\t        )\n\t        headers[name] = (header_param_type, header_serializer)\n\t    if headers:\n\t        return headers\n\t    return None\n\tdef get_function_parameters_info(\n\t    func: ConsumerFunc,\n\t) -> tuple[\n", "    type[TopicValueKeyHeader],\n\t    Serializer | None,\n\t    type[TopicValueKeyHeader] | None,\n\t    Serializer | None,\n\t    dict[str, tuple[type[TopicValueKeyHeader], Serializer | None]] | None,\n\t]:\n\t    signature = inspect.signature(func)\n\t    params = _get_params(signature)\n\t    value_param_type, value_serializer = _get_from_value_param_info(\n\t        params=params, func_name=func.__name__\n", "    )\n\t    key_param_type, key_serializer = _get_from_key_param_info(\n\t        params=params, func_name=func.__name__\n\t    )\n\t    headers_type_serializers = _get_from_headers_param_info(\n\t        params=params, func_name=func.__name__\n\t    )\n\t    return (\n\t        value_param_type,\n\t        value_serializer,\n", "        key_param_type,\n\t        key_serializer,\n\t        headers_type_serializers,\n\t    )\n\tclass _MessageAttr(Marker):\n\t    def __init__(self, attr_name: str) -> None:\n\t        self.attr_name = attr_name\n\t    def register_parameter(self, param: inspect.Parameter) -> Dependent[Any]:\n\t        def get_value(message: Annotated[ReadMessage, Marker(scope=\"consumer\")]) -> Any:\n\t            return getattr(message, self.attr_name)\n", "        return Dependent(get_value, scope=\"consumer\", use_cache=False)\n\tclass Value(_MessageAttr):\n\t    def __init__(self) -> None:\n\t        super().__init__(\"value\")\n\tclass Key(_MessageAttr):\n\t    def __init__(self) -> None:\n\t        super().__init__(\"key\")\n\tclass Header(Marker):\n\t    def __init__(self, alias: str | None = None) -> None:\n\t        self.alias = alias\n", "        super().__init__(call=None, scope=\"consumer\", use_cache=False)\n\t    def register_parameter(self, param: inspect.Parameter) -> Dependent[Any]:\n\t        if self.alias:\n\t            name = self.alias\n\t        else:\n\t            name = param.name\n\t        def get_header(\n\t            message: Annotated[ReadMessage, Marker(scope=\"consumer\")]\n\t        ) -> Any:\n\t            if message.headers:\n", "                return message.headers.get(name)\n\t            return None\n\t        return Dependent(get_header, scope=\"consumer\")\n\tclass Partition(_MessageAttr):\n\t    def __init__(self) -> None:\n\t        super().__init__(\"partition\")\n\tclass Timestamp(_MessageAttr):\n\t    def __init__(self) -> None:\n\t        super().__init__(\"timestamp\")\n\tclass Offset(_MessageAttr):\n", "    def __init__(self) -> None:\n\t        super().__init__(\"offset\")\n\t_T = TypeVar(\"_T\")\n\tFromValue = Annotated[_T, Value()]\n\tFromKey = Annotated[_T, Key()]\n\tFromHeader = Annotated[_T, Header()]\n\tMessageOffset = Annotated[int, Offset()]\n\tMessagePartition = Annotated[int, Partition()]\n\tMessageTimestamp = Annotated[int, Timestamp()]\n"]}
{"filename": "kaflow/exceptions.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import TYPE_CHECKING\n\tif TYPE_CHECKING:\n\t    from aiokafka import ConsumerRecord\n\tclass KaflowException(Exception):\n\t    ...\n\tclass KaflowDeserializationException(KaflowException):\n\t    def __init__(self, message: str, record: ConsumerRecord) -> None:\n\t        super().__init__(message)\n\t        self.record = record\n"]}
{"filename": "kaflow/applications.py", "chunked_list": ["from __future__ import annotations\n\timport asyncio\n\timport inspect\n\tfrom collections import defaultdict\n\tfrom contextlib import asynccontextmanager\n\tfrom functools import wraps\n\tfrom typing import (\n\t    TYPE_CHECKING,\n\t    Any,\n\t    AsyncContextManager,\n", "    AsyncIterator,\n\t    Awaitable,\n\t    Callable,\n\t    Coroutine,\n\t    Literal,\n\t    Sequence,\n\t    Union,\n\t)\n\tfrom uuid import uuid4\n\tfrom aiokafka import AIOKafkaConsumer, AIOKafkaProducer\n", "from aiokafka.helpers import create_ssl_context\n\tfrom di import Container, ScopeState\n\tfrom di.dependent import Dependent\n\tfrom di.executors import AsyncExecutor\n\tfrom kafka.coordinator.assignors.roundrobin import RoundRobinPartitionAssignor\n\tfrom kafka.partitioner.default import DefaultPartitioner\n\tfrom kaflow import parameters\n\tfrom kaflow._consumer import TopicConsumerFunc\n\tfrom kaflow._utils.asyncio import asyncify\n\tfrom kaflow._utils.inspect import is_not_coroutine_function\n", "from kaflow._utils.overrides import DependencyOverrideManager\n\tfrom kaflow.dependencies import Scopes\n\tfrom kaflow.exceptions import KaflowDeserializationException\n\tfrom kaflow.message import Message\n\tif TYPE_CHECKING:\n\t    from aiokafka.abc import AbstractTokenProvider\n\t    from kafka.coordinator.assignors.abstract import AbstractPartitionAssignor\n\t    from kaflow.asyncapi.models import AsyncAPI\n\t    from kaflow.serializers import Serializer\n\t    from kaflow.typing import TopicValueKeyHeader\n", "ConsumerFunc = Callable[..., Union[Message, Awaitable[Union[Message, None]], None]]\n\tProducerFunc = Callable[..., Union[Message, Awaitable[Message]]]\n\tExceptionHandlerFunc = Callable[[Exception], Awaitable]\n\tDeserializationErrorHandlerFunc = Callable[[KaflowDeserializationException], Awaitable]\n\tclass Kaflow:\n\t    def __init__(\n\t        self,\n\t        brokers: str | list[str],\n\t        client_id: str | None = None,\n\t        group_id: str | None = None,\n", "        acks: Literal[0, 1, \"all\"] = 1,\n\t        compression_type: Literal[\"gzip\", \"snappy\", \"lz4\", \"zstd\", None] = None,\n\t        max_batch_size: int = 16384,\n\t        partitioner: Callable[\n\t            [bytes, list[int], list[int]], int\n\t        ] = DefaultPartitioner(),\n\t        max_request_size: int = 1048576,\n\t        linger_ms: int = 0,\n\t        send_backoff_ms: int = 100,\n\t        connections_max_idle_ms: int = 540000,\n", "        enable_idempotence: bool = False,\n\t        transactional_id: str | None = None,\n\t        transaction_timeout_ms: int = 60000,\n\t        fetch_max_wait_ms: int = 500,\n\t        fetch_max_bytes: int = 52428800,\n\t        fetch_min_bytes: int = 1,\n\t        max_partition_fetch_bytes: int = 1 * 1024 * 1024,\n\t        request_timeout_ms: int = 40 * 1000,\n\t        retry_backoff_ms: int = 100,\n\t        auto_offset_reset: Literal[\"earliest\", \"latest\", \"none\"] = \"latest\",\n", "        enable_auto_commit: bool = True,\n\t        auto_commit_interval_ms: int = 5000,\n\t        check_crcs: bool = True,\n\t        metadata_max_age_ms: int = 5 * 60 * 1000,\n\t        partition_assignment_strategy: list[AbstractPartitionAssignor] | None = None,\n\t        max_poll_interval_ms: int = 300000,\n\t        rebalance_timeout_ms: int | None = None,\n\t        session_timeout_ms: int = 10000,\n\t        heartbeat_interval_ms: int = 3000,\n\t        consumer_timeout_ms: int = 200,\n", "        max_poll_records: int | None = None,\n\t        kafka_api_version: str = \"auto\",\n\t        security_protocol: Literal[\n\t            \"PLAINTEXT\", \"SSL\", \"SASL_PLAINTEXT\", \"SASL_SSL\"\n\t        ] = \"PLAINTEXT\",\n\t        exclude_internal_topics: bool = True,\n\t        connection_max_idle_ms: int = 540000,\n\t        isolation_level: Literal[\n\t            \"read_committed\", \"read_uncommitted\"\n\t        ] = \"read_committed\",\n", "        cafile: str | None = None,\n\t        capath: str | None = None,\n\t        cadata: bytes | None = None,\n\t        certfile: str | None = None,\n\t        keyfile: str | None = None,\n\t        cert_password: str | None = None,\n\t        sasl_mechanism: Literal[\n\t            \"PLAIN\", \"GSSAPI\", \"OAUTHBEARER\", \"SCRAM-SHA-256\", \"SCRAM-SHA-512\"\n\t        ]\n\t        | None = None,\n", "        sasl_plain_username: str | None = None,\n\t        sasl_plain_password: str | None = None,\n\t        sasl_kerberos_service_name: str = \"kafka\",\n\t        sasl_kerberos_domain_name: str | None = None,\n\t        sasl_oauth_token_provider: AbstractTokenProvider | None = None,\n\t        lifespan: Callable[..., AsyncContextManager[None]] | None = None,\n\t        asyncapi_version: str = \"2.6.0\",\n\t        title: str = \"Kaflow\",\n\t        version: str = \"0.0.1\",\n\t        description: str | None = None,\n", "        terms_of_service: str | None = None,\n\t        contact: dict[str, str | Any] | None = None,\n\t        license_info: dict[str, str | Any] | None = None,\n\t    ) -> None:\n\t        # AIOKafka\n\t        self.brokers = brokers\n\t        self.client_id = client_id or f\"kaflow-{uuid4()}\"\n\t        self.group_id = group_id\n\t        self.acks = acks\n\t        self.compression_type = compression_type\n", "        self.max_batch_size = max_batch_size\n\t        self.partitioner = partitioner\n\t        self.max_request_size = max_request_size\n\t        self.linger_ms = linger_ms\n\t        self.send_backoff_ms = send_backoff_ms\n\t        self.connections_max_idle_ms = connections_max_idle_ms\n\t        self.enable_idempotence = enable_idempotence\n\t        self.transactional_id = transactional_id\n\t        self.transaction_timeout_ms = transaction_timeout_ms\n\t        self.fetch_max_wait_ms = fetch_max_wait_ms\n", "        self.fetch_max_bytes = fetch_max_bytes\n\t        self.fetch_min_bytes = fetch_min_bytes\n\t        self.max_partition_fetch_bytes = max_partition_fetch_bytes\n\t        self.request_timeout_ms = request_timeout_ms\n\t        self.retry_backoff_ms = retry_backoff_ms\n\t        self.auto_offset_reset = auto_offset_reset\n\t        self.enable_auto_commit = enable_auto_commit\n\t        self.auto_commit_interval_ms = auto_commit_interval_ms\n\t        self.check_crcs = check_crcs\n\t        self.metadata_max_age_ms = metadata_max_age_ms\n", "        self.partition_assignment_strategy = partition_assignment_strategy or (\n\t            RoundRobinPartitionAssignor,\n\t        )\n\t        self.max_poll_interval_ms = max_poll_interval_ms\n\t        self.rebalance_timeout_ms = rebalance_timeout_ms\n\t        self.session_timeout_ms = session_timeout_ms\n\t        self.heartbeat_interval_ms = heartbeat_interval_ms\n\t        self.consumer_timeout_ms = consumer_timeout_ms\n\t        self.max_poll_records = max_poll_records\n\t        self.kafka_api_version = kafka_api_version\n", "        self.security_protocol = security_protocol\n\t        self.exclude_internal_topics = exclude_internal_topics\n\t        self.connection_max_idle_ms = connection_max_idle_ms\n\t        self.isolation_level = isolation_level\n\t        self.cafile = cafile\n\t        self.capath = capath\n\t        self.cadata = cadata\n\t        self.certfile = certfile\n\t        self.keyfile = keyfile\n\t        self.cert_password = cert_password\n", "        self.sasl_mechanism = sasl_mechanism\n\t        self.sasl_plain_username = sasl_plain_username\n\t        self.sasl_plain_password = sasl_plain_password\n\t        self.sasl_kerberos_service_name = sasl_kerberos_service_name\n\t        self.sasl_kerberos_domain_name = sasl_kerberos_domain_name\n\t        self.sasl_oauth_token_provider = sasl_oauth_token_provider\n\t        if security_protocol in [\"SSL\", \"SASL_SSL\"]:\n\t            self.ssl_context = create_ssl_context(\n\t                cafile=cafile,\n\t                capath=capath,\n", "                cadata=cadata,\n\t                certfile=certfile,\n\t                keyfile=keyfile,\n\t                password=cert_password,\n\t            )\n\t        else:\n\t            self.ssl_context = None\n\t        # AsyncAPI\n\t        self.asyncapi_version = asyncapi_version\n\t        self.title = title\n", "        self.version = version\n\t        self.description = description\n\t        self.terms_of_service = terms_of_service\n\t        self.contact = contact\n\t        self.license_info = license_info\n\t        self.asyncapi_schema: AsyncAPI | None = None\n\t        # di\n\t        self._container = Container()\n\t        self._container_state = ScopeState()\n\t        self.dependency_overrides = DependencyOverrideManager(self._container)\n", "        self._loop = asyncio.get_event_loop()\n\t        self._consumer: AIOKafkaConsumer | None = None\n\t        self._producer: AIOKafkaProducer | None = None\n\t        self._consumers: dict[str, TopicConsumerFunc] = {}\n\t        self._producers: dict[str, list[ProducerFunc]] = defaultdict(list)\n\t        self._sink_topics: set[str] = set()\n\t        self._exception_handlers: dict[\n\t            type[Exception], Callable[..., Awaitable[None]]\n\t        ] = {}\n\t        self._deserialization_error_handler: DeserializationErrorHandlerFunc | None = (\n", "            None\n\t        )\n\t        @asynccontextmanager\n\t        async def lifespan_ctx() -> AsyncIterator[None]:\n\t            executor = AsyncExecutor()\n\t            dep: Dependent[Any]\n\t            async with self._container_state.enter_scope(\n\t                scope=\"app\"\n\t            ) as self._container_state:\n\t                if lifespan:\n", "                    dep = Dependent(\n\t                        _wrap_lifespan_as_async_generator(lifespan), scope=\"app\"\n\t                    )\n\t                else:\n\t                    dep = Dependent(lambda: None, scope=\"app\")\n\t                solved = self._container.solve(dep, scopes=Scopes)\n\t                try:\n\t                    await solved.execute_async(\n\t                        executor=executor, state=self._container_state\n\t                    )\n", "                    self._prepare()\n\t                    yield\n\t                finally:\n\t                    self._container_state = ScopeState()\n\t        self.lifespan = lifespan_ctx\n\t    def _prepare(self) -> None:\n\t        for topic_processor in self._consumers.values():\n\t            topic_processor.prepare(self._container_state)\n\t    def _add_topic_consumer_func(\n\t        self,\n", "        topic: str,\n\t        func: ConsumerFunc,\n\t        value_param_type: type[TopicValueKeyHeader],\n\t        value_deserializer: Serializer | None = None,\n\t        key_param_type: type[TopicValueKeyHeader] | None = None,\n\t        key_deserializer: Serializer | None = None,\n\t        headers_type_deserializers: dict[\n\t            str, tuple[type[TopicValueKeyHeader], Serializer | None]\n\t        ]\n\t        | None = None,\n", "        sink_topics: Sequence[str] | None = None,\n\t    ) -> None:\n\t        topic_processor = TopicConsumerFunc(\n\t            name=topic,\n\t            container=self._container,\n\t            publish_fn=lambda *args, **kwargs: self._publish(*args, **kwargs),\n\t            exception_handlers=self._exception_handlers,\n\t            deserialization_error_handler=self._deserialization_error_handler,\n\t            func=func,\n\t            value_param_type=value_param_type,\n", "            value_deserializer=value_deserializer,\n\t            key_param_type=key_param_type,\n\t            key_deserializer=key_deserializer,\n\t            headers_type_deserializers=headers_type_deserializers,\n\t            sink_topics=sink_topics,\n\t        )\n\t        self._consumers[topic] = topic_processor\n\t    def _create_consumer(self) -> AIOKafkaConsumer:\n\t        return AIOKafkaConsumer(\n\t            *self._consumers.keys(),\n", "            bootstrap_servers=self.brokers,\n\t            client_id=self.client_id,\n\t            group_id=self.group_id,\n\t            fetch_min_bytes=self.fetch_min_bytes,\n\t            fetch_max_bytes=self.fetch_max_bytes,\n\t            fetch_max_wait_ms=self.fetch_max_wait_ms,\n\t            max_partition_fetch_bytes=self.max_partition_fetch_bytes,\n\t            max_poll_records=self.max_poll_records,\n\t            request_timeout_ms=self.request_timeout_ms,\n\t            retry_backoff_ms=self.retry_backoff_ms,\n", "            auto_offset_reset=self.auto_offset_reset,\n\t            enable_auto_commit=self.enable_auto_commit,\n\t            auto_commit_interval_ms=self.auto_commit_interval_ms,\n\t            check_crcs=self.check_crcs,\n\t            metadata_max_age_ms=self.metadata_max_age_ms,\n\t            partition_assignment_strategy=self.partition_assignment_strategy,\n\t            max_poll_interval_ms=self.max_poll_interval_ms,\n\t            rebalance_timeout_ms=self.rebalance_timeout_ms,\n\t            session_timeout_ms=self.session_timeout_ms,\n\t            heartbeat_interval_ms=self.heartbeat_interval_ms,\n", "            consumer_timeout_ms=self.consumer_timeout_ms,\n\t            api_version=self.kafka_api_version,\n\t            security_protocol=self.security_protocol,\n\t            ssl_context=self.ssl_context,\n\t            exclude_internal_topics=self.exclude_internal_topics,\n\t            connections_max_idle_ms=self.connections_max_idle_ms,\n\t            isolation_level=self.isolation_level,\n\t            sasl_mechanism=self.sasl_mechanism,\n\t            sasl_plain_username=self.sasl_plain_username,\n\t            sasl_plain_password=self.sasl_plain_password,\n", "            sasl_kerberos_domain_name=self.sasl_kerberos_domain_name,\n\t            sasl_kerberos_service_name=self.sasl_kerberos_service_name,\n\t            sasl_oauth_token_provider=self.sasl_oauth_token_provider,\n\t        )\n\t    def _create_producer(self) -> AIOKafkaProducer:\n\t        return AIOKafkaProducer(\n\t            bootstrap_servers=self.brokers,\n\t            client_id=self.client_id,\n\t            metadata_max_age_ms=self.metadata_max_age_ms,\n\t            request_timeout_ms=self.request_timeout_ms,\n", "            api_version=self.kafka_api_version,\n\t            acks=self.acks,\n\t            compression_type=self.compression_type,\n\t            max_batch_size=self.max_batch_size,\n\t            partitioner=self.partitioner,\n\t            max_request_size=self.max_request_size,\n\t            linger_ms=self.linger_ms,\n\t            send_backoff_ms=self.send_backoff_ms,\n\t            retry_backoff_ms=self.retry_backoff_ms,\n\t            security_protocol=self.security_protocol,\n", "            ssl_context=self.ssl_context,\n\t            connections_max_idle_ms=self.connections_max_idle_ms,\n\t            enable_idempotence=self.enable_idempotence,\n\t            transactional_id=self.transactional_id,\n\t            transaction_timeout_ms=self.transaction_timeout_ms,\n\t            sasl_mechanism=self.sasl_mechanism,\n\t            sasl_plain_username=self.sasl_plain_username,\n\t            sasl_plain_password=self.sasl_plain_password,\n\t            sasl_kerberos_service_name=self.sasl_kerberos_service_name,\n\t            sasl_kerberos_domain_name=self.sasl_kerberos_domain_name,\n", "            sasl_oauth_token_provider=self.sasl_oauth_token_provider,\n\t        )\n\t    def consume(\n\t        self,\n\t        topic: str,\n\t        sink_topics: Sequence[str] | None = None,\n\t    ) -> Callable[[ConsumerFunc], ConsumerFunc]:\n\t        def register_consumer(func: ConsumerFunc) -> ConsumerFunc:\n\t            (\n\t                value_param_type,\n", "                value_deserializer,\n\t                key_param_type,\n\t                key_deserializer,\n\t                headers_type_deserializers,\n\t            ) = parameters.get_function_parameters_info(func)\n\t            self._add_topic_consumer_func(\n\t                topic=topic,\n\t                func=func,\n\t                value_param_type=value_param_type,\n\t                value_deserializer=value_deserializer,\n", "                key_param_type=key_param_type,\n\t                key_deserializer=key_deserializer,\n\t                headers_type_deserializers=headers_type_deserializers,\n\t                sink_topics=sink_topics,\n\t            )\n\t            if sink_topics:\n\t                self._sink_topics.update(sink_topics)\n\t            if is_not_coroutine_function(func):\n\t                func = asyncify(func)\n\t            return func\n", "        return register_consumer\n\t    def produce(self, sink_topic: str) -> Callable[[ProducerFunc], ProducerFunc]:\n\t        def register_producer(func: ProducerFunc) -> Callable[..., Any]:\n\t            self._sink_topics.update([sink_topic])\n\t            def _create_coro(topic: str, message: Any) -> Coroutine[Any, Any, None]:\n\t                if not isinstance(message, Message):\n\t                    raise ValueError(\n\t                        \"Kaflow producer function has to return an instance of\"\n\t                        \" `Message` containing the information of the message to be\"\n\t                        f\" send. Update `{func.__name__}` to return a `Message`\"\n", "                        \" instance.\"\n\t                    )\n\t                return self._publish(\n\t                    topic=topic,\n\t                    value=message.value,\n\t                    key=message.key,\n\t                    headers=message.headers,\n\t                    partition=message.partition,\n\t                    timestamp=message.timestamp,\n\t                )\n", "            if is_not_coroutine_function(func):\n\t                @wraps(func)\n\t                def sync_wrapper(*args: Any, **kwargs: Any) -> Any:\n\t                    message = func(*args, **kwargs)\n\t                    asyncio.run_coroutine_threadsafe(\n\t                        coro=_create_coro(sink_topic, message),\n\t                        loop=self._loop,\n\t                    )\n\t                    return message\n\t                return sync_wrapper\n", "            @wraps(func)\n\t            async def async_wrapper(*args: Any, **kwargs: Any) -> Any:\n\t                message = await func(*args, **kwargs)  # type: ignore\n\t                await _create_coro(sink_topic, message)\n\t                return message\n\t            return async_wrapper\n\t        return register_producer\n\t    def exception_handler(\n\t        self, exception: type[Exception]\n\t    ) -> Callable[[ExceptionHandlerFunc], ExceptionHandlerFunc]:\n", "        def register_exception_handler(\n\t            func: ExceptionHandlerFunc,\n\t        ) -> ExceptionHandlerFunc:\n\t            if is_not_coroutine_function(func):\n\t                func = asyncify(func)\n\t            self._exception_handlers[exception] = func\n\t            return func\n\t        return register_exception_handler\n\t    def deserialization_error_handler(\n\t        self,\n", "    ) -> Callable[[DeserializationErrorHandlerFunc], DeserializationErrorHandlerFunc]:\n\t        def register_deserialization_error_handler(\n\t            func: DeserializationErrorHandlerFunc,\n\t        ) -> DeserializationErrorHandlerFunc:\n\t            self._deserialization_error_handler = func\n\t            if is_not_coroutine_function(func):\n\t                self._deserialization_error_handler = asyncify(func)\n\t            return func\n\t        return register_deserialization_error_handler\n\t    def asyncapi(self) -> AsyncAPI:\n", "        # if not self.asyncapi_schema:\n\t        #     self.asyncapi_schema = build_asyncapi(\n\t        #         asyncapi_version=self.asyncapi_version,\n\t        #         title=self.title,\n\t        #         version=self.version,\n\t        #         description=self.description,\n\t        #         terms_of_service=self.terms_of_service,\n\t        #         contact=self.contact,\n\t        #         license_info=self.license_info,\n\t        #         consumers=self._consumers,\n", "        #         producers=self._producers,\n\t        #     )\n\t        # return self.asyncapi_schema\n\t        raise NotImplementedError(\"AsyncAPI is not implemented yet.\")\n\t    async def _publish(\n\t        self,\n\t        topic: str,\n\t        value: bytes | None = None,\n\t        key: bytes | None = None,\n\t        headers: dict[str, bytes] | None = None,\n", "        partition: int | None = None,\n\t        timestamp: int | None = None,\n\t    ) -> None:\n\t        if not self._producer:\n\t            raise RuntimeError(\n\t                \"The producer has not been started yet. You're probably seeing this\"\n\t                f\" error because `{self.__class__.__name__}.start` method has not been\"\n\t                \" called yet.\"\n\t            )\n\t        if isinstance(headers, dict):\n", "            headers_ = [(k, v) for k, v in headers.items()]\n\t        else:\n\t            headers_ = None\n\t        await self._producer.send_and_wait(\n\t            topic=topic,\n\t            value=value,\n\t            key=key,\n\t            partition=partition,\n\t            timestamp_ms=timestamp,\n\t            headers=headers_,\n", "        )\n\t    def _get_consumer(self, topic: str) -> TopicConsumerFunc:\n\t        return self._consumers[topic]\n\t    async def _consuming_loop(self) -> None:\n\t        if not self._consumer:\n\t            raise RuntimeError(\n\t                \"The consumer has not been started yet. You're probably seeing this\"\n\t                f\" error because `{self.__class__.__name__}.start` method has not been\"\n\t                \" called yet.\"\n\t            )\n", "        async for record in self._consumer:\n\t            consumer = self._get_consumer(record.topic)\n\t            await consumer.consume(record=record)\n\t    async def start(self) -> None:\n\t        self._consumer = self._create_consumer()\n\t        self._producer = self._create_producer()\n\t        async with self.lifespan():\n\t            await self._consumer.start()\n\t            await self._producer.start()\n\t            await self._consuming_loop()\n", "    async def stop(self) -> None:\n\t        if self._consumer:\n\t            await self._consumer.stop()\n\t        if self._producer:\n\t            await self._producer.stop()\n\t    def run(self) -> None:\n\t        try:\n\t            self._loop.run_until_complete(self.start())\n\t        except asyncio.CancelledError:\n\t            pass\n", "        except KeyboardInterrupt:\n\t            pass\n\t        finally:\n\t            self._loop.run_until_complete(self.stop())\n\t            self._loop.close()\n\t    @property\n\t    def consumed_topics(self) -> list[str]:\n\t        return list(self._consumers.keys())\n\t    @property\n\t    def sink_topics(self) -> list[str]:\n", "        return list(self._sink_topics)\n\t# Taken from adriandg/xpresso\n\t# https://github.com/adriangb/xpresso/blob/0a69b5131440cd114baeab7243db7bd3255e66ed/xpresso/applications.py#L392\n\t# Thanks :)\n\tdef _wrap_lifespan_as_async_generator(\n\t    lifespan: Callable[..., AsyncContextManager[None]]\n\t) -> Callable[..., AsyncIterator[None]]:\n\t    # wrap true context managers in an async generator\n\t    # so that the dependency injection system recognizes it\n\t    async def gen(*args: Any, **kwargs: Any) -> AsyncIterator[None]:\n", "        async with lifespan(*args, **kwargs):\n\t            yield\n\t    # this is so that the dependency injection system\n\t    # still picks up parameters from the function signature\n\t    sig = inspect.signature(gen)\n\t    sig = sig.replace(parameters=list(inspect.signature(lifespan).parameters.values()))\n\t    setattr(gen, \"__signature__\", sig)  # noqa: B010\n\t    return gen\n"]}
{"filename": "kaflow/_utils/overrides.py", "chunked_list": ["# Taken from: https://github.com/adriangb/xpresso/blob/main/xpresso/_utils/overrides.py\n\tfrom __future__ import annotations\n\timport contextlib\n\timport inspect\n\timport typing\n\tfrom types import TracebackType\n\tfrom typing import cast\n\tfrom di import Container\n\tfrom di.api.dependencies import DependentBase\n\tfrom di.api.providers import DependencyProvider\n", "from di.dependent import Dependent\n\tfrom typing_extensions import get_args\n\tfrom kaflow._utils.inspect import is_annotated_param\n\tdef get_type(param: inspect.Parameter) -> type:\n\t    if is_annotated_param(param):\n\t        type_ = next(iter(get_args(param.annotation)))\n\t    else:\n\t        type_ = param.annotation\n\t    return cast(type, type_)\n\tclass DependencyOverrideManager:\n", "    _stacks: typing.List[contextlib.ExitStack]\n\t    def __init__(self, container: Container) -> None:\n\t        self._container = container\n\t        self._stacks = []\n\t    def __setitem__(\n\t        self, target: DependencyProvider, replacement: DependencyProvider\n\t    ) -> None:\n\t        def hook(\n\t            param: typing.Optional[inspect.Parameter],\n\t            dependent: DependentBase[typing.Any],\n", "        ) -> typing.Optional[DependentBase[typing.Any]]:\n\t            if not isinstance(dependent, Dependent):\n\t                return None\n\t            scope = dependent.scope\n\t            dep = Dependent(\n\t                replacement,\n\t                scope=scope,\n\t                use_cache=dependent.use_cache,\n\t                wire=dependent.wire,\n\t            )\n", "            if param is not None and param.annotation is not param.empty:\n\t                type_ = get_type(param)\n\t                if type_ is target:\n\t                    return dep\n\t            if dependent.call is not None and dependent.call is target:\n\t                return dep\n\t            return None\n\t        cm = self._container.bind(hook)\n\t        if self._stacks:\n\t            self._stacks[-1].enter_context(cm)\n", "    def __enter__(self) -> DependencyOverrideManager:\n\t        self._stacks.append(contextlib.ExitStack().__enter__())\n\t        return self\n\t    def __exit__(\n\t        self,\n\t        __exc_type: typing.Optional[typing.Type[BaseException]],\n\t        __exc_value: typing.Optional[BaseException],\n\t        __traceback: typing.Optional[TracebackType],\n\t    ) -> typing.Optional[bool]:\n\t        return self._stacks.pop().__exit__(__exc_type, __exc_value, __traceback)\n"]}
{"filename": "kaflow/_utils/inspect.py", "chunked_list": ["from __future__ import annotations\n\timport inspect\n\tfrom typing import TYPE_CHECKING, Any, Awaitable, Callable, TypeVar\n\tfrom typing_extensions import Annotated, ParamSpec, TypeGuard, get_args, get_origin\n\tif TYPE_CHECKING:\n\t    from inspect import Signature\n\tdef is_annotated_param(param: Any) -> bool:\n\t    return get_origin(param) is Annotated\n\tdef annotated_param_with(item: Any, param: Any) -> bool:\n\t    if is_annotated_param(param):\n", "        for arg in get_args(param):\n\t            if arg == item or (type(item) == type and isinstance(arg, item)):\n\t                return True\n\t    return False\n\tdef has_return_annotation(signature: Signature) -> bool:\n\t    return (\n\t        signature.return_annotation is not None\n\t        and signature.return_annotation != inspect.Signature.empty\n\t    )\n\tP = ParamSpec(\"P\")\n", "R = TypeVar(\"R\")\n\tdef is_not_coroutine_function(\n\t    func: Callable[P, R | Awaitable[R]]\n\t) -> TypeGuard[Callable[P, R]]:\n\t    \"\"\"Check if a function is not a coroutine function. This function narrows the type\n\t    of the function to a synchronous function.\n\t    Args:\n\t        func: The function to check.\n\t    Returns:\n\t        `True` if the function is not a coroutine function, `False` otherwise.\n", "    \"\"\"\n\t    return not inspect.iscoroutinefunction(func)\n"]}
{"filename": "kaflow/_utils/__init__.py", "chunked_list": []}
{"filename": "kaflow/_utils/asyncio.py", "chunked_list": ["from __future__ import annotations\n\timport asyncio\n\timport contextvars\n\timport functools\n\tfrom typing import Awaitable, Callable, TypeVar\n\tfrom typing_extensions import ParamSpec\n\tP = ParamSpec(\"P\")\n\tR = TypeVar(\"R\")\n\t# Inspired by https://github.com/tiangolo/asyncer\n\tdef asyncify(func: Callable[P, R]) -> Callable[P, Awaitable[R]]:\n", "    \"\"\"A decorator to convert a synchronous function into an asynchronous one.\n\t    Args:\n\t        func: The synchronous function to convert.\n\t    Returns:\n\t        The asynchronous function.\n\t    \"\"\"\n\t    async def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n\t        # TODO: update to use `asyncio.to_thread`, once Python 3.8 is deprecated\n\t        loop = asyncio.get_running_loop()\n\t        ctx = contextvars.copy_context()\n", "        func_call = functools.partial(ctx.run, func, *args, **kwargs)\n\t        return await loop.run_in_executor(None, func_call)  # type: ignore\n\t    return wrapper\n"]}
{"filename": "kaflow/asyncapi/_builder.py", "chunked_list": ["from __future__ import annotations\n\tfrom enum import Enum\n\tfrom typing import TYPE_CHECKING, Any\n\tfrom pydantic import BaseModel\n\tfrom pydantic.schema import get_model_name_map, model_process_schema\n\tfrom typing_extensions import TypeGuard\n\tfrom kaflow.asyncapi import models\n\tif TYPE_CHECKING:\n\t    from kaflow._consumer import TopicConsumerFunc\n\t    from kaflow.applications import ProducerFunc\n", "def not_bytes_and_base_model(cls: type) -> TypeGuard[type[BaseModel]]:\n\t    return cls is not None and cls is not bytes and issubclass(cls, BaseModel)\n\tdef get_flat_models(\n\t    consumers: dict[str, TopicConsumerFunc] | None = None\n\t) -> set[type[BaseModel] | type[Enum]]:\n\t    models: set[type[BaseModel] | type[Enum]] = set()\n\t    if consumers:\n\t        for consumer in consumers.values():\n\t            if not_bytes_and_base_model(consumer.value_param_type):\n\t                models.add(consumer.value_param_type)\n", "            if consumer.key_param_type:\n\t                if not_bytes_and_base_model(consumer.key_param_type):\n\t                    models.add(consumer.key_param_type)\n\t            if consumer.headers_type_deserializers:\n\t                for header in consumer.headers_type_deserializers:\n\t                    header_type = consumer.headers_type_deserializers[header][0]\n\t                    if not_bytes_and_base_model(header_type):\n\t                        models.add(header_type)\n\t    return models\n\tdef get_model_definitions(\n", "    flat_models: set[type[BaseModel] | type[Enum]],\n\t    model_name_map: dict[type[BaseModel] | type[Enum], str],\n\t) -> dict[str, Any]:\n\t    definitions: dict[str, dict[str, Any]] = {}\n\t    for model in flat_models:\n\t        model_schema, model_definitions, model_nested_models = model_process_schema(\n\t            model, model_name_map=model_name_map\n\t        )\n\t        definitions.update(model_definitions)\n\t        model_name = model_name_map[model]\n", "        definitions[model_name] = model_schema\n\t    return definitions\n\tdef build_asyncapi(\n\t    asyncapi_version: str,\n\t    title: str,\n\t    version: str,\n\t    description: str | None = None,\n\t    terms_of_service: str | None = None,\n\t    contact: dict[str, Any] | None = None,\n\t    license_info: dict[str, Any] | None = None,\n", "    consumers: dict[str, TopicConsumerFunc] | None = None,\n\t    producers: dict[str, list[ProducerFunc]] | None = None,\n\t) -> models.AsyncAPI:\n\t    asyncapi_info: dict[str, Any] = {\"title\": title, \"version\": version}\n\t    if description:\n\t        asyncapi_info[\"description\"] = description\n\t    if terms_of_service:\n\t        asyncapi_info[\"termsOfService\"] = terms_of_service\n\t    if contact:\n\t        asyncapi_info[\"contact\"] = contact\n", "    if license_info:\n\t        asyncapi_info[\"license\"] = license_info\n\t    components: dict[str, dict[str, Any]] = {}\n\t    flat_models = get_flat_models(consumers)\n\t    model_name_map = get_model_name_map(flat_models)\n\t    definitions = get_model_definitions(flat_models, model_name_map)\n\t    if definitions:\n\t        components[\"schemas\"] = definitions\n\t    output = {\n\t        \"asyncapi\": asyncapi_version,\n", "        \"info\": asyncapi_info,\n\t        \"components\": components,\n\t    }\n\t    return models.AsyncAPI(**output)\n"]}
{"filename": "kaflow/asyncapi/docs.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import TYPE_CHECKING\n\tif TYPE_CHECKING:\n\t    from kaflow.asyncapi.models import AsyncAPI\n\tdef get_asyncapi_html(\n\t    title: str,\n\t    asyncapi_schema: AsyncAPI,\n\t    asyncapi_react_component_js_url: str = \"https://unpkg.com/@asyncapi/web-component@1.0.0-next.47/lib/asyncapi-web-component.js\",\n\t    asyncapi_react_component_css_url: str = \"https://unpkg.com/@asyncapi/react-component@1.0.0-next.12/styles/default.min.css\",\n\t) -> str:\n", "    html = f\"\"\"\n\t    <!DOCTYPE html>\n\t    <html>\n\t    <head>\n\t        <link\n\t            rel=\"stylesheet\"\n\t            href=\"{asyncapi_react_component_css_url}\"\n\t        />\n\t        <title>{title}</title>\n\t    </head>\n", "    <body>\n\t        <script\n\t            src=\"{asyncapi_react_component_js_url}\"\n\t            defer\n\t        ></script>\n\t        <asyncapi-component\n\t            schema='{asyncapi_schema.json(by_alias=True, exclude_none=True)}'\n\t            cssImportPath=\"{asyncapi_react_component_css_url}\"\n\t        ></asyncapi-component>\n\t    </body>\n", "    </html>\n\t    \"\"\"\n\t    return html\n"]}
{"filename": "kaflow/asyncapi/models.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import (\n\t    Any,\n\t    Callable,\n\t    Dict,\n\t    Iterable,\n\t    List,\n\t    Literal,\n\t    Mapping,\n\t    Optional,\n", "    Union,\n\t)\n\tfrom pydantic import AnyUrl, BaseModel, Field\n\tfrom kaflow.logger import logger\n\t# Taken from: https://github.com/tiangolo/fastapi/blob/master/fastapi/openapi/models.py\n\ttry:\n\t    import email_validator  # type: ignore\n\t    assert email_validator\n\t    from pydantic import EmailStr\n\texcept ImportError:  # pragma: no cover\n", "    class EmailStr(str):  # type: ignore\n\t        @classmethod\n\t        def __get_validators__(cls) -> Iterable[Callable[..., Any]]:\n\t            yield cls.validate\n\t        @classmethod\n\t        def validate(cls, v: Any) -> str:\n\t            logger.warning(\n\t                \"`email-validator` not installed, email fields will be treated as\"\n\t                \" str.\\nTo install, run: `pip install email-validator`\"\n\t            )\n", "            return str(v)\n\tclass Contact(BaseModel):\n\t    name: Optional[str] = None\n\t    url: Optional[AnyUrl] = None\n\t    email: Optional[EmailStr] = None\n\tclass License(BaseModel):\n\t    name: str\n\t    url: Optional[AnyUrl] = None\n\tclass Info(BaseModel):\n\t    title: str\n", "    version: str\n\t    description: Optional[str] = None\n\t    termsOfService: Optional[str] = None\n\t    contact: Optional[Contact] = None\n\t    license: Optional[License] = None\n\tclass ServerVariable(BaseModel):\n\t    enum: Optional[List[str]] = None\n\t    default: Optional[str] = None\n\t    description: Optional[str] = None\n\t    examples: Optional[List[str]] = None\n", "class ServerBinding(BaseModel):\n\t    schemaRegistryUrl: Optional[AnyUrl] = None\n\t    schemaRegistryVendor: Optional[str] = None\n\t    bindingVersion: Optional[str] = None\n\tclass TopicConfiguration(BaseModel):\n\t    cleanup_policy: Optional[str] = Field(None, alias=\"cleanup.policy\")\n\t    retention_ms: Optional[int] = Field(None, alias=\"retention.ms\")\n\t    retention_bytes: Optional[int] = Field(None, alias=\"retention.bytes\")\n\t    delete_retention_ms: Optional[int] = Field(None, alias=\"delete.retention.ms\")\n\t    max_message_bytes: Optional[int] = Field(None, alias=\"max.message.bytes\")\n", "class ChannelBinding(BaseModel):\n\t    topic: Optional[str] = None\n\t    partitions: Optional[int] = None\n\t    replicas: Optional[int] = None\n\t    topicConfiguration: Optional[TopicConfiguration] = None\n\t    bindingVersion: Optional[str] = None\n\tclass OperationBinding(BaseModel):\n\t    groupId: Optional[Schema] = None\n\t    clientId: Optional[Schema] = None\n\t    bindingVersion: Optional[str] = None\n", "class MessageBinding(BaseModel):\n\t    key: Optional[Schema] = None\n\t    schemaIdLocation: Optional[str] = None\n\t    schemaIdPayloadEncoding: Optional[str] = None\n\t    schemaLookupStrategy: Optional[str] = None\n\t    bindingVersion: Optional[str] = None\n\tclass Server(BaseModel):\n\t    url: str\n\t    protocol: str\n\t    protocolVersion: Optional[str] = None\n", "    description: Optional[str] = None\n\t    variables: Optional[Mapping[str, Union[ServerVariable, Reference]]] = None\n\t    security: Optional[List[Mapping[str, Any]]] = None\n\t    bindings: Optional[Mapping[str, Union[ServerBinding, Reference]]] = None\n\tclass ExternalDocs(BaseModel):\n\t    description: Optional[str] = None\n\t    url: AnyUrl\n\tclass Tag(BaseModel):\n\t    name: str\n\t    description: Optional[str] = None\n", "    externalDocs: Optional[ExternalDocs] = None\n\tclass BaseOperation(BaseModel):\n\t    operationId: Optional[str] = None\n\t    summary: Optional[str] = None\n\t    description: Optional[str] = None\n\t    tags: Optional[List[Tag]] = None\n\t    externalDocs: Optional[ExternalDocs] = None\n\t    bindings: Optional[Mapping[str, Union[OperationBinding, Reference]]] = None\n\tclass OperationTrait(BaseOperation):\n\t    pass\n", "class Operation(BaseOperation):\n\t    traits: Optional[List[OperationTrait]] = None\n\tclass Reference(BaseModel):\n\t    ref: str = Field(..., alias=\"$ref\")\n\tclass Schema(BaseModel):\n\t    title: Optional[str] = None\n\t    type: Optional[str] = None\n\t    required: Optional[List[str]] = None\n\t    multipleOf: Optional[float] = None\n\t    maximum: Optional[float] = None\n", "    exclusiveMaximum: Optional[float] = None\n\t    minimum: Optional[float] = None\n\t    exclusiveMinimum: Optional[float] = None\n\t    maxLength: Optional[int] = None\n\t    minLength: Optional[int] = None\n\t    pattern: Optional[str] = None\n\t    maxItems: Optional[int] = None\n\t    minItems: Optional[int] = None\n\t    uniqueItems: Optional[bool] = None\n\t    maxProperties: Optional[int] = None\n", "    minProperties: Optional[int] = None\n\t    enum: Optional[List[Any]] = None\n\t    const: Optional[str] = None\n\t    examples: Optional[List[str]] = None\n\t    readOnly: Optional[bool] = None\n\t    writeOnly: Optional[bool] = None\n\t    properties: Optional[Dict[str, Schema]] = None\n\t    patternProperties: Optional[Dict[str, Schema]] = None\n\t    additionalProperties: Optional[Schema] = None\n\t    additionalItems: Optional[Schema] = None\n", "    items: Optional[Union[Schema, List[Schema]]] = None\n\t    propertyNames: Optional[Schema] = None\n\t    contains: Optional[Schema] = None\n\t    allOf: Optional[List[Schema]] = None\n\t    oneOf: Optional[List[Schema]] = None\n\t    anyOf: Optional[List[Schema]] = None\n\t    not_: Optional[Schema] = Field(None, alias=\"not\")\n\t    description: Optional[str] = None\n\t    format: Optional[str] = None\n\t    default: Optional[Any] = None\n", "    discriminator: Optional[str] = None\n\t    externalDocs: Optional[ExternalDocs] = None\n\t    deprecated: Optional[bool] = None\n\tclass Parameter(BaseModel):\n\t    description: Optional[str] = None\n\t    schema_: Optional[Schema] = Field(None, alias=\"schema\")\n\t    location: Optional[str] = None\n\tclass Channel(BaseModel):\n\t    ref: Optional[str] = Field(None, alias=\"$ref\")\n\t    description: Optional[str] = None\n", "    servers: Optional[List[str]] = None\n\t    subscribe: Optional[Operation] = None\n\t    publish: Optional[Operation] = None\n\t    parameters: Optional[Dict[str, Union[Parameter, Reference]]] = None\n\t    bindings: Optional[Mapping[str, Union[ChannelBinding, Reference]]] = None\n\tclass CorrelationId(BaseModel):\n\t    description: Optional[str] = None\n\t    location: str\n\tclass MessageExample(BaseModel):\n\t    headers: Optional[Mapping[str, Any]] = None\n", "    payload: Any\n\t    name: Optional[str] = None\n\t    summary: Optional[str] = None\n\tclass BaseMessage(BaseModel):\n\t    messageId: Optional[str] = None\n\t    headers: Optional[Union[Schema, Reference]] = None\n\t    payload: Any\n\t    correlationId: Optional[Union[CorrelationId, Reference]] = None\n\t    schemaFormat: Optional[str] = None\n\t    contentType: Optional[str] = None\n", "    name: Optional[str] = None\n\t    title: Optional[str] = None\n\t    summary: Optional[str] = None\n\t    description: Optional[str] = None\n\t    tags: Optional[List[Tag]] = None\n\t    externalDocs: Optional[ExternalDocs] = None\n\t    bindings: Optional[Mapping[str, Union[MessageBinding, Reference]]] = None\n\t    examples: Optional[List[MessageExample]] = None\n\tclass MessageTrait(BaseMessage):\n\t    pass\n", "class Message(BaseMessage):\n\t    traits: Optional[List[Union[MessageTrait, Reference]]] = None\n\tclass OAuthFlow(BaseModel):\n\t    authorizationUrl: Optional[AnyUrl] = None\n\t    tokenUrl: Optional[AnyUrl] = None\n\t    refreshUrl: Optional[AnyUrl] = None\n\t    scopes: Mapping[str, str]\n\tclass OAuthFlows(BaseModel):\n\t    implicit: Optional[OAuthFlow] = None\n\t    password: Optional[OAuthFlow] = None\n", "    clientCredentials: Optional[OAuthFlow] = None\n\t    authorizationCode: Optional[OAuthFlow] = None\n\tclass SecurityScheme(BaseModel):\n\t    type: Literal[\n\t        \"userPassword\",\n\t        \"apiKey\",\n\t        \"X509\",\n\t        \"symmetrictEncryption\",\n\t        \"asymmetricEncryption\",\n\t        \"httpApiKey\",\n", "        \"http\",\n\t        \"oauth2\",\n\t        \"openIdConnect\",\n\t        \"plain\",\n\t        \"scramSha256\",\n\t        \"scramSha512\",\n\t        \"gssapi\",\n\t    ]\n\t    description: Optional[str] = None\n\t    name: Optional[str] = None\n", "    in_: Optional[Literal[\"user\", \"password\", \"query\", \"header\", \"cookie\"]] = None\n\t    scheme: Optional[str] = None\n\t    bearerFormat: Optional[str] = None\n\t    flows: Optional[OAuthFlows] = None\n\t    openIdConnectUrl: Optional[AnyUrl] = None\n\tclass Components(BaseModel):\n\t    schemas: Optional[Mapping[str, Union[Schema, Reference]]] = None\n\t    servers: Optional[Mapping[str, Union[Server, Reference]]] = None\n\t    serverVariables: Optional[Mapping[str, Union[ServerVariable, Reference]]] = None\n\t    channels: Optional[Mapping[str, Union[Channel, Reference]]] = None\n", "    messages: Optional[Mapping[str, Union[Message, Reference]]] = None\n\t    securitySchemes: Optional[Mapping[str, Union[SecurityScheme, Reference]]] = None\n\t    parameters: Optional[Mapping[str, Union[Parameter, Reference]]] = None\n\t    correlationIds: Optional[Mapping[str, Union[CorrelationId, Reference]]] = None\n\t    operationTraits: Optional[Mapping[str, Union[OperationTrait, Reference]]] = None\n\t    messageTraits: Optional[Mapping[str, Union[MessageTrait, Reference]]] = None\n\t    serverBindings: Optional[Mapping[str, Union[ServerBinding, Reference]]] = None\n\t    channelBindings: Optional[Mapping[str, Union[ChannelBinding, Reference]]] = None\n\t    operationBindings: Optional[Mapping[str, Union[OperationBinding, Reference]]] = None\n\t    messageBindings: Optional[Mapping[str, Union[MessageBinding, Reference]]] = None\n", "class AsyncAPI(BaseModel):\n\t    \"\"\"A model containing all the information required to generate an AsyncAPI spec.\n\t    https://www.asyncapi.com/docs/reference/specification/v2.6.0\n\t    \"\"\"\n\t    asyncapi: str\n\t    id: Optional[str] = None\n\t    info: Info\n\t    servers: Optional[Mapping[str, Server]] = None\n\t    defaultContentType: Optional[str] = None\n\t    channels: Mapping[str, Channel] = {}\n", "    components: Optional[Components] = None\n\t    tags: Optional[List[Tag]] = None\n\t    externalDocs: Optional[ExternalDocs] = None\n"]}
{"filename": "kaflow/asyncapi/__init__.py", "chunked_list": []}
