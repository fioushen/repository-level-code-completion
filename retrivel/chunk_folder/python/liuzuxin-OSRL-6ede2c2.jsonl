{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python3\n\t# -*- coding: utf-8 -*-\n\timport os\n\tfrom setuptools import find_packages, setup\n\tdef get_version() -> str:\n\t    # https://packaging.python.org/guides/single-sourcing-package-version/\n\t    init = open(os.path.join(\"osrl\", \"__init__.py\"), \"r\").read().split()\n\t    return init[init.index(\"__version__\") + 2][1:-1]\n\tdef get_install_requires() -> str:\n\t    return [\n", "        \"dsrl\",\n\t        \"fast-safe-rl\",\n\t        \"pyrallis==0.3.1\",\n\t        \"pyyaml~=6.0\",\n\t        \"scipy~=1.10.1\",\n\t        \"tqdm\",\n\t        \"numpy>1.16.0\",  # https://github.com/numpy/numpy/issues/12793\n\t        \"tensorboard>=2.5.0\",\n\t        \"torch~=1.13.0\",\n\t        \"numba>=0.51.0\",\n", "        \"wandb~=0.14.0\",\n\t        \"h5py>=2.10.0\",\n\t        \"protobuf~=3.19.0\",  # breaking change, sphinx fail\n\t        \"python-dateutil==2.8.2\",\n\t        \"easy_runner\",\n\t        \"swig==4.1.1\",\n\t    ]\n\tdef get_extras_require() -> str:\n\t    req = {\n\t        \"dev\": [\n", "            \"sphinx==6.2.1\",\n\t            \"sphinx_rtd_theme==1.2.0\",\n\t            \"jinja2==3.0.3\",  # temporary fix\n\t            \"sphinxcontrib-bibtex==2.5.0\",\n\t            \"flake8\",\n\t            \"flake8-bugbear\",\n\t            \"yapf\",\n\t            \"isort\",\n\t            \"pytest~=7.3.1\",\n\t            \"pytest-cov~=4.0.0\",\n", "            \"networkx\",\n\t            \"mypy\",\n\t            \"pydocstyle\",\n\t            \"doc8==0.11.2\",\n\t            \"scipy\",\n\t            \"pre-commit\",\n\t        ]\n\t    }\n\t    return req\n\tsetup(\n", "    name=\"osrl-lib\",\n\t    version=get_version(),\n\t    description=\n\t    \"Elegant Implementations of Offline Safe Reinforcement Learning Algorithms\",\n\t    long_description=open(\"README.md\", encoding=\"utf8\").read(),\n\t    long_description_content_type=\"text/markdown\",\n\t    url=\"https://github.com/liuzuxin/offline-safe-rl-baselines.git\",\n\t    author=\"Zijian Guo; Zuxin Liu\",\n\t    author_email=\"zuxin1997@gmail.com\",\n\t    license=\"MIT\",\n", "    python_requires=\">=3.8\",\n\t    classifiers=[\n\t        # How mature is this project? Common values are\n\t        #   3 - Alpha\n\t        #   4 - Beta\n\t        #   5 - Production/Stable\n\t        \"Development Status :: 3 - Alpha\",\n\t        # Indicate who your project is intended for\n\t        \"Intended Audience :: Science/Research\",\n\t        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n", "        \"Topic :: Software Development :: Libraries :: Python Modules\",\n\t        # Pick your license as you wish (should match \"license\" above)\n\t        \"License :: OSI Approved :: MIT License\",\n\t        # Specify the Python versions you support here. In particular, ensure\n\t        # that you indicate whether you support Python 2, Python 3 or both.\n\t        \"Programming Language :: Python :: 3.8\",\n\t        \"Programming Language :: Python :: 3.9\",\n\t        \"Programming Language :: Python :: 3.10\",\n\t    ],\n\t    keywords=\"offline safe reinforcement learning algorithms pytorch\",\n", "    packages=find_packages(\n\t        exclude=[\"test\", \"test.*\", \"examples\", \"examples.*\", \"docs\", \"docs.*\"]),\n\t    install_requires=get_install_requires(),\n\t    extras_require=get_extras_require(),\n\t)\n"]}
{"filename": "osrl/__init__.py", "chunked_list": ["__version__ = \"0.1.0\"\n\t__all__ = [\n\t    \"algorithms\",\n\t    \"common\",\n\t]\n"]}
{"filename": "osrl/common/net.py", "chunked_list": ["import math\n\tfrom typing import Optional\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch import distributions as pyd\n\tfrom torch.distributions.normal import Normal\n\tdef mlp(sizes, activation, output_activation=nn.Identity):\n\t    \"\"\"\n", "    Creates a multi-layer perceptron with the specified sizes and activations.\n\t    Args:\n\t        sizes (list): A list of integers specifying the size of each layer in the MLP.\n\t        activation (nn.Module): The activation function to use for all layers except the output layer.\n\t        output_activation (nn.Module): The activation function to use for the output layer. Defaults to nn.Identity.\n\t    Returns:\n\t        nn.Sequential: A PyTorch Sequential model representing the MLP.\n\t    \"\"\"\n\t    layers = []\n\t    for j in range(len(sizes) - 1):\n", "        act = activation if j < len(sizes) - 2 else output_activation\n\t        layer = nn.Linear(sizes[j], sizes[j + 1])\n\t        layers += [layer, act()]\n\t    return nn.Sequential(*layers)\n\tclass MLPGaussianPerturbationActor(nn.Module):\n\t    \"\"\"\n\t    A MLP actor that adds Gaussian noise to the output.\n\t    Args:\n\t        obs_dim (int): The dimension of the observation space.\n\t        act_dim (int): The dimension of the action space.\n", "        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n\t        activation (Type[nn.Module]): The activation function to use between layers.\n\t        phi (float): The standard deviation of the Gaussian noise to add to the output.\n\t        act_limit (float): The absolute value of the limits of the action space.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 obs_dim,\n\t                 act_dim,\n\t                 hidden_sizes,\n\t                 activation,\n", "                 phi=0.05,\n\t                 act_limit=1):\n\t        super().__init__()\n\t        pi_sizes = [obs_dim + act_dim] + list(hidden_sizes) + [act_dim]\n\t        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n\t        self.act_limit = act_limit\n\t        self.phi = phi\n\t    def forward(self, obs, act):\n\t        # Return output from network scaled to action space limits.\n\t        a = self.phi * self.act_limit * self.pi(torch.cat([obs, act], 1))\n", "        return (a + act).clamp(-self.act_limit, self.act_limit)\n\tclass MLPActor(nn.Module):\n\t    \"\"\"\n\t    A MLP actor\n\t    Args:\n\t        obs_dim (int): The dimension of the observation space.\n\t        act_dim (int): The dimension of the action space.\n\t        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n\t        activation (Type[nn.Module]): The activation function to use between layers.\n\t        act_limit (float, optional): The upper limit of the action space.\n", "    \"\"\"\n\t    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit=1):\n\t        super().__init__()\n\t        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n\t        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n\t        self.act_limit = act_limit\n\t    def forward(self, obs):\n\t        # Return output from network scaled to action space limits.\n\t        return self.act_limit * self.pi(obs)\n\tclass MLPGaussianActor(nn.Module):\n", "    \"\"\"\n\t    A MLP Gaussian actor\n\t    Args:\n\t        obs_dim (int): The dimension of the observation space.\n\t        act_dim (int): The dimension of the action space.\n\t        action_low (np.ndarray): A 1D numpy array of lower bounds for each action dimension.\n\t        action_high (np.ndarray): A 1D numpy array of upper bounds for each action dimension.\n\t        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n\t        activation (Type[nn.Module]): The activation function to use between layers.\n\t        device (str): The device to use for computation (cpu or cuda).\n", "    \"\"\"\n\t    def __init__(self,\n\t                 obs_dim,\n\t                 act_dim,\n\t                 action_low,\n\t                 action_high,\n\t                 hidden_sizes,\n\t                 activation,\n\t                 device=\"cpu\"):\n\t        super().__init__()\n", "        self.device = device\n\t        self.action_low = torch.nn.Parameter(torch.tensor(action_low,\n\t                                                          device=device)[None, ...],\n\t                                             requires_grad=False)  # (1, act_dim)\n\t        self.action_high = torch.nn.Parameter(torch.tensor(action_high,\n\t                                                           device=device)[None, ...],\n\t                                              requires_grad=False)  # (1, act_dim)\n\t        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n\t        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n\t        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n", "    def _distribution(self, obs):\n\t        mu = torch.sigmoid(self.mu_net(obs))\n\t        mu = self.action_low + (self.action_high - self.action_low) * mu\n\t        std = torch.exp(self.log_std)\n\t        return mu, Normal(mu, std)\n\t    def _log_prob_from_distribution(self, pi, act):\n\t        return pi.log_prob(act).sum(\n\t            axis=-1)  # Last axis sum needed for Torch Normal distribution\n\t    def forward(self, obs, act=None, deterministic=False):\n\t        '''\n", "        Produce action distributions for given observations, and\n\t        optionally compute the log likelihood of given actions under\n\t        those distributions.\n\t        If act is None, sample an action\n\t        '''\n\t        mu, pi = self._distribution(obs)\n\t        if act is None:\n\t            act = pi.sample()\n\t        if deterministic:\n\t            act = mu\n", "        logp_a = self._log_prob_from_distribution(pi, act)\n\t        return pi, act, logp_a\n\tLOG_STD_MAX = 2\n\tLOG_STD_MIN = -20\n\tclass SquashedGaussianMLPActor(nn.Module):\n\t    '''\n\t    A MLP Gaussian actor, can also be used as a deterministic actor\n\t    Args:\n\t        obs_dim (int): The dimension of the observation space.\n\t        act_dim (int): The dimension of the action space.\n", "        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n\t        activation (Type[nn.Module]): The activation function to use between layers.\n\t    '''\n\t    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n\t        super().__init__()\n\t        self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)\n\t        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n\t        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n\t    def forward(self,\n\t                obs,\n", "                deterministic=False,\n\t                with_logprob=True,\n\t                with_distribution=False,\n\t                return_pretanh_value=False):\n\t        net_out = self.net(obs)\n\t        mu = self.mu_layer(net_out)\n\t        log_std = self.log_std_layer(net_out)\n\t        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n\t        std = torch.exp(log_std)\n\t        # Pre-squash distribution and sample\n", "        pi_distribution = Normal(mu, std)\n\t        if deterministic:\n\t            # Only used for evaluating policy at test time.\n\t            pi_action = mu\n\t        else:\n\t            pi_action = pi_distribution.rsample()\n\t        if with_logprob:\n\t            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n\t            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n\t            logp_pi -= (2 *\n", "                        (np.log(2) - pi_action - F.softplus(-2 * pi_action))).sum(axis=1)\n\t        else:\n\t            logp_pi = None\n\t        # for BEARL only\n\t        if return_pretanh_value:\n\t            return torch.tanh(pi_action), pi_action\n\t        pi_action = torch.tanh(pi_action)\n\t        if with_distribution:\n\t            return pi_action, logp_pi, pi_distribution\n\t        return pi_action, logp_pi\n", "class EnsembleQCritic(nn.Module):\n\t    '''\n\t    An ensemble of Q network to address the overestimation issue.\n\t    Args:\n\t        obs_dim (int): The dimension of the observation space.\n\t        act_dim (int): The dimension of the action space.\n\t        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n\t        activation (Type[nn.Module]): The activation function to use between layers.\n\t        num_q (float): The number of Q networks to include in the ensemble.\n\t    '''\n", "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, num_q=2):\n\t        super().__init__()\n\t        assert num_q >= 1, \"num_q param should be greater than 1\"\n\t        self.q_nets = nn.ModuleList([\n\t            mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], nn.ReLU)\n\t            for i in range(num_q)\n\t        ])\n\t    def forward(self, obs, act=None):\n\t        # Squeeze is critical to ensure value has the right shape.\n\t        # Without squeeze, the training stability will be greatly affected!\n", "        # For instance, shape [3] - shape[3,1] = shape [3, 3] instead of shape [3]\n\t        data = obs if act is None else torch.cat([obs, act], dim=-1)\n\t        return [torch.squeeze(q(data), -1) for q in self.q_nets]\n\t    def predict(self, obs, act):\n\t        q_list = self.forward(obs, act)\n\t        qs = torch.vstack(q_list)  # [num_q, batch_size]\n\t        return torch.min(qs, dim=0).values, q_list\n\t    def loss(self, target, q_list=None):\n\t        losses = [((q - target)**2).mean() for q in q_list]\n\t        return sum(losses)\n", "class EnsembleDoubleQCritic(nn.Module):\n\t    '''\n\t    An ensemble of double Q network to address the overestimation issue.\n\t    Args:\n\t        obs_dim (int): The dimension of the observation space.\n\t        act_dim (int): The dimension of the action space.\n\t        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n\t        activation (Type[nn.Module]): The activation function to use between layers.\n\t        num_q (float): The number of Q networks to include in the ensemble.\n\t    '''\n", "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, num_q=2):\n\t        super().__init__()\n\t        assert num_q >= 1, \"num_q param should be greater than 1\"\n\t        self.q1_nets = nn.ModuleList([\n\t            mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], nn.ReLU)\n\t            for i in range(num_q)\n\t        ])\n\t        self.q2_nets = nn.ModuleList([\n\t            mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], nn.ReLU)\n\t            for i in range(num_q)\n", "        ])\n\t    def forward(self, obs, act):\n\t        # Squeeze is critical to ensure value has the right shape.\n\t        # Without squeeze, the training stability will be greatly affected!\n\t        # For instance, shape [3] - shape[3,1] = shape [3, 3] instead of shape [3]\n\t        data = torch.cat([obs, act], dim=-1)\n\t        q1 = [torch.squeeze(q(data), -1) for q in self.q1_nets]\n\t        q2 = [torch.squeeze(q(data), -1) for q in self.q2_nets]\n\t        return q1, q2\n\t    def predict(self, obs, act):\n", "        q1_list, q2_list = self.forward(obs, act)\n\t        qs1, qs2 = torch.vstack(q1_list), torch.vstack(q2_list)\n\t        # qs = torch.vstack(q_list)  # [num_q, batch_size]\n\t        qs1_min, qs2_min = torch.min(qs1, dim=0).values, torch.min(qs2, dim=0).values\n\t        return qs1_min, qs2_min, q1_list, q2_list\n\t    def loss(self, target, q_list=None):\n\t        losses = [((q - target)**2).mean() for q in q_list]\n\t        return sum(losses)\n\tclass VAE(nn.Module):\n\t    \"\"\"\n", "    Variational Auto-Encoder\n\t    Args:\n\t        obs_dim (int): The dimension of the observation space.\n\t        act_dim (int): The dimension of the action space.\n\t        hidden_size (int): The number of hidden units in the encoder and decoder networks.\n\t        latent_dim (int): The dimensionality of the latent space.\n\t        act_lim (float): The upper limit of the action space.\n\t        device (str): The device to use for computation (cpu or cuda).\n\t    \"\"\"\n\t    def __init__(self, obs_dim, act_dim, hidden_size, latent_dim, act_lim, device=\"cpu\"):\n", "        super(VAE, self).__init__()\n\t        self.e1 = nn.Linear(obs_dim + act_dim, hidden_size)\n\t        self.e2 = nn.Linear(hidden_size, hidden_size)\n\t        self.mean = nn.Linear(hidden_size, latent_dim)\n\t        self.log_std = nn.Linear(hidden_size, latent_dim)\n\t        self.d1 = nn.Linear(obs_dim + latent_dim, hidden_size)\n\t        self.d2 = nn.Linear(hidden_size, hidden_size)\n\t        self.d3 = nn.Linear(hidden_size, act_dim)\n\t        self.act_lim = act_lim\n\t        self.latent_dim = latent_dim\n", "        self.device = device\n\t    def forward(self, obs, act):\n\t        z = F.relu(self.e1(torch.cat([obs, act], 1)))\n\t        z = F.relu(self.e2(z))\n\t        mean = self.mean(z)\n\t        # Clamped for numerical stability\n\t        log_std = self.log_std(z).clamp(-4, 15)\n\t        std = torch.exp(log_std)\n\t        z = mean + std * torch.randn_like(std)\n\t        u = self.decode(obs, z)\n", "        return u, mean, std\n\t    def decode(self, obs, z=None):\n\t        if z is None:\n\t            z = torch.randn((obs.shape[0], self.latent_dim)).clamp(-0.5,\n\t                                                                   0.5).to(self.device)\n\t        a = F.relu(self.d1(torch.cat([obs, z], 1)))\n\t        a = F.relu(self.d2(a))\n\t        return self.act_lim * torch.tanh(self.d3(a))\n\t    # for BEARL only\n\t    def decode_multiple(self, obs, z=None, num_decode=10):\n", "        if z is None:\n\t            z = torch.randn(\n\t                (obs.shape[0], num_decode, self.latent_dim)).clamp(-0.5,\n\t                                                                   0.5).to(self.device)\n\t        a = F.relu(\n\t            self.d1(\n\t                torch.cat(\n\t                    [obs.unsqueeze(0).repeat(num_decode, 1, 1).permute(1, 0, 2), z], 2)))\n\t        a = F.relu(self.d2(a))\n\t        return torch.tanh(self.d3(a)), self.d3(a)\n", "class LagrangianPIDController:\n\t    '''\n\t    Lagrangian multiplier controller\n\t    Args:\n\t        KP (float): The proportional gain.\n\t        KI (float): The integral gain.\n\t        KD (float): The derivative gain.\n\t        thres (float): The setpoint for the controller.\n\t    '''\n\t    def __init__(self, KP, KI, KD, thres) -> None:\n", "        super().__init__()\n\t        self.KP = KP\n\t        self.KI = KI\n\t        self.KD = KD\n\t        self.thres = thres\n\t        self.error_old = 0\n\t        self.error_integral = 0\n\t    def control(self, qc):\n\t        '''\n\t        @param qc [batch,]\n", "        '''\n\t        error_new = torch.mean(qc - self.thres)  # [batch]\n\t        error_diff = F.relu(error_new - self.error_old)\n\t        self.error_integral = torch.mean(F.relu(self.error_integral + error_new))\n\t        self.error_old = error_new\n\t        multiplier = F.relu(self.KP * F.relu(error_new) + self.KI * self.error_integral +\n\t                            self.KD * error_diff)\n\t        return torch.mean(multiplier)\n\t# Decision Transformer implementation\n\tclass TransformerBlock(nn.Module):\n", "    def __init__(\n\t        self,\n\t        seq_len: int,\n\t        embedding_dim: int,\n\t        num_heads: int,\n\t        attention_dropout: float,\n\t        residual_dropout: float,\n\t    ):\n\t        super().__init__()\n\t        self.norm1 = nn.LayerNorm(embedding_dim)\n", "        self.norm2 = nn.LayerNorm(embedding_dim)\n\t        self.drop = nn.Dropout(residual_dropout)\n\t        self.attention = nn.MultiheadAttention(embedding_dim,\n\t                                               num_heads,\n\t                                               attention_dropout,\n\t                                               batch_first=True)\n\t        self.mlp = nn.Sequential(\n\t            nn.Linear(embedding_dim, 4 * embedding_dim),\n\t            nn.GELU(),\n\t            nn.Linear(4 * embedding_dim, embedding_dim),\n", "            nn.Dropout(residual_dropout),\n\t        )\n\t        # True value indicates that the corresponding position is not allowed to attend\n\t        self.register_buffer(\"causal_mask\",\n\t                             ~torch.tril(torch.ones(seq_len, seq_len)).to(bool))\n\t        self.seq_len = seq_len\n\t    # [batch_size, seq_len, emb_dim] -> [batch_size, seq_len, emb_dim]\n\t    def forward(self,\n\t                x: torch.Tensor,\n\t                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n", "        causal_mask = self.causal_mask[:x.shape[1], :x.shape[1]]\n\t        norm_x = self.norm1(x)\n\t        attention_out = self.attention(\n\t            query=norm_x,\n\t            key=norm_x,\n\t            value=norm_x,\n\t            attn_mask=causal_mask,\n\t            key_padding_mask=padding_mask,\n\t            need_weights=False,\n\t        )[0]\n", "        # by default pytorch attention does not use dropout\n\t        # after final attention weights projection, while minGPT does:\n\t        # https://github.com/karpathy/minGPT/blob/7218bcfa527c65f164de791099de715b81a95106/mingpt/model.py#L70 # noqa\n\t        x = x + self.drop(attention_out)\n\t        x = x + self.mlp(self.norm2(x))\n\t        return x\n\tclass TanhTransform(pyd.transforms.Transform):\n\t    domain = pyd.constraints.real\n\t    codomain = pyd.constraints.interval(-1.0, 1.0)\n\t    bijective = True\n", "    sign = +1\n\t    def __init__(self, cache_size=1):\n\t        super().__init__(cache_size=cache_size)\n\t    @staticmethod\n\t    def atanh(x):\n\t        return 0.5 * (x.log1p() - (-x).log1p())\n\t    def __eq__(self, other):\n\t        return isinstance(other, TanhTransform)\n\t    def _call(self, x):\n\t        return x.tanh()\n", "    def _inverse(self, y):\n\t        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n\t        # one should use `cache_size=1` instead\n\t        return self.atanh(y)\n\t    def log_abs_det_jacobian(self, x, y):\n\t        # We use a formula that is more numerically stable, see details in the following link\n\t        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n\t        return 2.0 * (math.log(2.0) - x - F.softplus(-2.0 * x))\n\tclass SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n\t    \"\"\"\n", "    Squashed Normal Distribution(s)\n\t    If loc/std is of size (batch_size, sequence length, d),\n\t    this returns batch_size * sequence length * d\n\t    independent squashed univariate normal distributions.\n\t    \"\"\"\n\t    def __init__(self, loc, std):\n\t        self.loc = loc\n\t        self.std = std\n\t        self.base_dist = pyd.Normal(loc, std)\n\t        transforms = [TanhTransform()]\n", "        super().__init__(self.base_dist, transforms)\n\t    @property\n\t    def mean(self):\n\t        mu = self.loc\n\t        for tr in self.transforms:\n\t            mu = tr(mu)\n\t        return mu\n\t    def entropy(self, N=1):\n\t        # sample from the distribution and then compute the empirical entropy:\n\t        x = self.rsample((N, ))\n", "        log_p = self.log_prob(x)\n\t        return -log_p.mean(axis=0).sum(axis=2)\n\t    def log_likelihood(self, x):\n\t        # sum up along the action dimensions\n\t        return self.log_prob(x).sum(axis=2)\n\tclass DiagGaussianActor(nn.Module):\n\t    \"\"\"\n\t    torch.distributions implementation of an diagonal Gaussian policy.\n\t    \"\"\"\n\t    def __init__(self, hidden_dim, act_dim, log_std_bounds=[-5.0, 2.0]):\n", "        super().__init__()\n\t        self.mu = torch.nn.Linear(hidden_dim, act_dim)\n\t        self.log_std = torch.nn.Linear(hidden_dim, act_dim)\n\t        self.log_std_bounds = log_std_bounds\n\t        def weight_init(m):\n\t            \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n\t            if isinstance(m, torch.nn.Linear):\n\t                nn.init.orthogonal_(m.weight.data)\n\t                if hasattr(m.bias, \"data\"):\n\t                    m.bias.data.fill_(0.0)\n", "        self.apply(weight_init)\n\t    def forward(self, obs):\n\t        mu, log_std = self.mu(obs), self.log_std(obs)\n\t        std = log_std.exp()\n\t        return Normal(mu, std)\n"]}
{"filename": "osrl/common/exp_util.py", "chunked_list": ["import os\n\timport os.path as osp\n\timport random\n\timport uuid\n\tfrom typing import Dict, Optional, Sequence\n\timport numpy as np\n\timport torch\n\timport yaml\n\tdef seed_all(seed=1029, others: Optional[list] = None):\n\t    random.seed(seed)\n", "    os.environ['PYTHONHASHSEED'] = str(seed)\n\t    np.random.seed(seed)\n\t    # torch.use_deterministic_algorithms(True)\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n\t    torch.backends.cudnn.benchmark = False\n\t    torch.backends.cudnn.deterministic = True\n\t    if others is not None:\n\t        if hasattr(others, \"seed\"):\n", "            others.seed(seed)\n\t            return True\n\t        try:\n\t            for item in others:\n\t                if hasattr(item, \"seed\"):\n\t                    item.seed(seed)\n\t        except:\n\t            pass\n\tdef get_cfg_value(config, key):\n\t    if key in config:\n", "        value = config[key]\n\t        if isinstance(value, list):\n\t            suffix = \"\"\n\t            for i in value:\n\t                suffix += str(i)\n\t            return suffix\n\t        return str(value)\n\t    for k in config.keys():\n\t        if isinstance(config[k], dict):\n\t            res = get_cfg_value(config[k], key)\n", "            if res is not None:\n\t                return res\n\t    return \"None\"\n\tdef load_config_and_model(path: str, best: bool = False):\n\t    '''\n\t    Load the configuration and trained model from a specified directory.\n\t    :param path: the directory path where the configuration and trained model are stored.\n\t    :param best: whether to load the best-performing model or the most recent one. Defaults to False.\n\t    :return: a tuple containing the configuration dictionary and the trained model.\n\t    :raises ValueError: if the specified directory does not exist.\n", "    '''\n\t    if osp.exists(path):\n\t        config_file = osp.join(path, \"config.yaml\")\n\t        print(f\"load config from {config_file}\")\n\t        with open(config_file) as f:\n\t            config = yaml.load(f.read(), Loader=yaml.FullLoader)\n\t        model_file = \"model.pt\"\n\t        if best:\n\t            model_file = \"model_best.pt\"\n\t        model_path = osp.join(path, \"checkpoint/\" + model_file)\n", "        print(f\"load model from {model_path}\")\n\t        model = torch.load(model_path)\n\t        return config, model\n\t    else:\n\t        raise ValueError(f\"{path} doesn't exist!\")\n\tdef to_string(values):\n\t    '''\n\t    Recursively convert a sequence or dictionary of values to a string representation.\n\t    :param values: the sequence or dictionary of values to be converted to a string.\n\t    :return: a string representation of the input values.\n", "    '''\n\t    name = \"\"\n\t    if isinstance(values, Sequence) and not isinstance(values, str):\n\t        for i, v in enumerate(values):\n\t            prefix = \"\" if i == 0 else \"_\"\n\t            name += prefix + to_string(v)\n\t        return name\n\t    elif isinstance(values, Dict):\n\t        for i, k in enumerate(sorted(values.keys())):\n\t            prefix = \"\" if i == 0 else \"_\"\n", "            name += prefix + to_string(values[k])\n\t        return name\n\t    else:\n\t        return str(values)\n\tDEFAULT_SKIP_KEY = [\n\t    \"task\", \"reward_threshold\", \"logdir\", \"worker\", \"project\", \"group\", \"name\", \"prefix\",\n\t    \"suffix\", \"save_interval\", \"render\", \"verbose\", \"save_ckpt\", \"training_num\",\n\t    \"testing_num\", \"epoch\", \"device\", \"thread\"\n\t]\n\tDEFAULT_KEY_ABBRE = {\n", "    \"cost_limit\": \"cost\",\n\t    \"mstep_iter_num\": \"mnum\",\n\t    \"estep_iter_num\": \"enum\",\n\t    \"estep_kl\": \"ekl\",\n\t    \"mstep_kl_mu\": \"kl_mu\",\n\t    \"mstep_kl_std\": \"kl_std\",\n\t    \"mstep_dual_lr\": \"mlr\",\n\t    \"estep_dual_lr\": \"elr\",\n\t    \"update_per_step\": \"update\"\n\t}\n", "def auto_name(default_cfg: dict,\n\t              current_cfg: dict,\n\t              prefix: str = \"\",\n\t              suffix: str = \"\",\n\t              skip_keys: list = DEFAULT_SKIP_KEY,\n\t              key_abbre: dict = DEFAULT_KEY_ABBRE) -> str:\n\t    '''\n\t    Automatic generate the experiment name by comparing the current config with the default one.\n\t    :param dict default_cfg: a dictionary containing the default configuration values.\n\t    :param dict current_cfg: a dictionary containing the current configuration values.\n", "    :param str prefix: (optional) a string to be added at the beginning of the generated name.\n\t    :param str suffix: (optional) a string to be added at the end of the generated name.\n\t    :param list skip_keys: (optional) a list of keys to be skipped when generating the name.\n\t    :param dict key_abbre: (optional) a dictionary containing abbreviations for keys in the generated name.\n\t    :return str: a string representing the generated experiment name.\n\t    '''\n\t    name = prefix\n\t    for i, k in enumerate(sorted(default_cfg.keys())):\n\t        if default_cfg[k] == current_cfg[k] or k in skip_keys:\n\t            continue\n", "        prefix = \"_\" if len(name) else \"\"\n\t        value = to_string(current_cfg[k])\n\t        # replace the name with abbreviation if key has abbreviation in key_abbre\n\t        if k in key_abbre:\n\t            k = key_abbre[k]\n\t        # Add the key-value pair to the name variable with the prefix\n\t        name += prefix + k + value\n\t    if len(suffix):\n\t        name = name + \"_\" + suffix if len(name) else suffix\n\t    name = \"default\" if not len(name) else name\n", "    name = f\"{name}-{str(uuid.uuid4())[:4]}\"\n\t    return name\n"]}
{"filename": "osrl/common/dataset.py", "chunked_list": ["import copy\n\timport heapq\n\timport random\n\tfrom collections import Counter, defaultdict\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple, Union\n\timport numpy as np\n\ttry:\n\t    import oapackage\n\texcept ImportError:\n\t    print(\"OApackage is not installed, can not use CDT.\")\n", "from scipy.optimize import minimize\n\tfrom torch.nn import functional as F  # noqa\n\tfrom torch.utils.data import IterableDataset\n\tfrom tqdm.auto import trange  # noqa\n\tdef discounted_cumsum(x: np.ndarray, gamma: float) -> np.ndarray:\n\t    \"\"\"\n\t    Calculate the discounted cumulative sum of x (can be rewards or costs).\n\t    \"\"\"\n\t    cumsum = np.zeros_like(x)\n\t    cumsum[-1] = x[-1]\n", "    for t in reversed(range(x.shape[0] - 1)):\n\t        cumsum[t] = x[t] + gamma * cumsum[t + 1]\n\t    return cumsum\n\tdef process_bc_dataset(dataset: dict, cost_limit: float, gamma: float, bc_mode: str):\n\t    \"\"\"\n\t    Processes a givne dataset for behavior cloning and its variants.\n\t    Args:\n\t        dataset (dict): A dictionary containing the dataset to be processed.\n\t        cost_limit (float): The maximum cost allowed for the dataset.\n\t        gamma (float): The discount factor used to compute the returns.\n", "        bc_mode (str): The behavior cloning mode to use. Can be one of:\n\t            - \"all\": All trajectories are used for behavior cloning.\n\t            - \"multi-task\": All trajectories are used for behavior cloning, and the cost is appended as a feature.\n\t            - \"safe\": Only trajectories with cost below the cost limit are used for behavior cloning.\n\t            - \"risky\": Only trajectories with cost above twice the cost limit are used for behavior cloning.\n\t            - \"frontier\": Only trajectories near the Pareto frontier are used for behavior cloning.\n\t            - \"boundary\": Only trajectories near the cost limit are used for behavior cloning.\n\t        frontier_fn (function, optional): A function used to compute the frontier. \n\t                                          Required if bc_mode is \"frontier\".\n\t        frontier_range (float, optional): The range around the frontier to use for selecting trajectories. \n", "                                           Required if bc_mode is \"frontier\".\n\t    Returns:\n\t        dict: A dictionary containing the processed dataset.\n\t    \"\"\"\n\t    # get the indices of the transitions after terminal states or timeouts\n\t    done_idx = np.where((dataset[\"terminals\"] == 1) | (dataset[\"timeouts\"] == 1))[0]\n\t    n_transitions = dataset[\"observations\"].shape[0]\n\t    dataset[\"cost_returns\"] = np.zeros_like(dataset[\"costs\"])\n\t    dataset[\"rew_returns\"] = np.zeros_like(dataset[\"rewards\"])\n\t    cost_ret, rew_ret = [], []\n", "    pareto_frontier, pf_mask = None, None\n\t    # compute episode returns\n\t    for i in range(done_idx.shape[0]):\n\t        start = 0 if i == 0 else done_idx[i - 1] + 1\n\t        end = done_idx[i] + 1\n\t        # compute the cost and reward returns for the segment\n\t        cost_returns = discounted_cumsum(dataset[\"costs\"][start:end], gamma=gamma)\n\t        reward_returns = discounted_cumsum(dataset[\"rewards\"][start:end], gamma=gamma)\n\t        dataset[\"cost_returns\"][start:end] = cost_returns[0]\n\t        dataset[\"rew_returns\"][start:end] = reward_returns[0]\n", "        cost_ret.append(cost_returns[0])\n\t        rew_ret.append(reward_returns[0])\n\t    # compute Pareto Frontier\n\t    if bc_mode == \"frontier\":\n\t        cost_ret = np.array(cost_ret, dtype=np.float64)\n\t        rew_ret = np.array(rew_ret, dtype=np.float64)\n\t        rmax, rmin = np.max(rew_ret), np.min(rew_ret)\n\t        pareto = oapackage.ParetoDoubleLong()\n\t        for i in range(rew_ret.shape[0]):\n\t            w = oapackage.doubleVector((-cost_ret[i], rew_ret[i]))\n", "            pareto.addvalue(w, i)\n\t        pareto.show(verbose=1)\n\t        pareto_idx = list(pareto.allindices())\n\t        cost_ret_pareto = cost_ret[pareto_idx]\n\t        rew_ret_pareto = rew_ret[pareto_idx]\n\t        for deg in [0, 1, 2]:\n\t            pareto_frontier = np.poly1d(\n\t                np.polyfit(cost_ret_pareto, rew_ret_pareto, deg=deg))\n\t            pf_rew_ret = pareto_frontier(cost_ret_pareto)\n\t            ss_total = np.sum((rew_ret_pareto - np.mean(rew_ret_pareto))**2)\n", "            ss_residual = np.sum((rew_ret_pareto - pf_rew_ret)**2)\n\t            r_squared = 1 - (ss_residual / ss_total)\n\t            if r_squared >= 0.9:\n\t                break\n\t        pf_rew_ret = pareto_frontier(dataset[\"cost_returns\"])\n\t        pf_mask = np.logical_and(\n\t            pf_rew_ret - (rmax - rmin) / 5 <= dataset[\"rew_returns\"],\n\t            dataset[\"rew_returns\"] <= pf_rew_ret + (rmax - rmin) / 5)\n\t    # select the transitions for behavior cloning based on the mode\n\t    selected_transition = np.zeros((n_transitions, ), dtype=int)\n", "    if bc_mode == \"all\" or bc_mode == \"multi-task\":\n\t        selected_transition = np.ones((n_transitions, ), dtype=int)\n\t    elif bc_mode == \"safe\":\n\t        # safe trajectories\n\t        selected_transition[dataset[\"cost_returns\"] <= cost_limit] = 1\n\t    elif bc_mode == \"risky\":\n\t        # high cost trajectories\n\t        selected_transition[dataset[\"cost_returns\"] >= 2 * cost_limit] = 1\n\t    elif bc_mode == \"boundary\":\n\t        # trajectories that are near the cost limit\n", "        mask = np.logical_and(0.5 * cost_limit < dataset[\"cost_returns\"],\n\t                              dataset[\"cost_returns\"] <= 1.5 * cost_limit)\n\t        selected_transition[mask] = 1\n\t    elif bc_mode == \"frontier\":\n\t        selected_transition[pf_mask] = 1\n\t    else:\n\t        raise NotImplementedError\n\t    for k, v in dataset.items():\n\t        dataset[k] = v[selected_transition == 1]\n\t    if bc_mode == \"multi-task\":\n", "        dataset[\"observations\"] = np.hstack(\n\t            (dataset[\"observations\"], dataset[\"cost_returns\"].reshape(-1, 1)))\n\t    print(\n\t        f\"original size = {n_transitions}, cost limit = {cost_limit}, filtered size = {np.sum(selected_transition == 1)}\"\n\t    )\n\tdef process_sequence_dataset(dataset: dict, cost_reverse: bool = False):\n\t    '''\n\t    Processe a given dataset into a list of trajectories, each containing information about \n\t    the observations, actions, rewards, costs, returns, and cost returns for a single episode.\n\t    Args:\n", "        dataset (dict): A dictionary representing the dataset, \n\t                        with keys \"observations\", \"actions\", \"rewards\", \"costs\", \"terminals\", and \"timeouts\", \n\t                        each containing numpy arrays of corresponding data.\n\t        cost_reverse (bool): An optional boolean parameter that indicates whether the cost should be reversed.\n\t    Returns:\n\t        traj (list): A list of dictionaries, each representing a trajectory.\n\t        info (dict): A dictionary containing additional information about the trajectories\n\t    '''\n\t    traj, traj_len = [], []\n\t    data_, episode_step = defaultdict(list), 0\n", "    for i in trange(dataset[\"rewards\"].shape[0], desc=\"Processing trajectories\"):\n\t        data_[\"observations\"].append(dataset[\"observations\"][i])\n\t        data_[\"actions\"].append(dataset[\"actions\"][i])\n\t        data_[\"rewards\"].append(dataset[\"rewards\"][i])\n\t        if cost_reverse:\n\t            data_[\"costs\"].append(1.0 - dataset[\"costs\"][i])\n\t        else:\n\t            data_[\"costs\"].append(dataset[\"costs\"][i])\n\t        if dataset[\"terminals\"][i] or dataset[\"timeouts\"][i]:\n\t            episode_data = {k: np.array(v, dtype=np.float32) for k, v in data_.items()}\n", "            # return-to-go if gamma=1.0, just discounted returns else\n\t            episode_data[\"returns\"] = discounted_cumsum(episode_data[\"rewards\"], gamma=1)\n\t            episode_data[\"cost_returns\"] = discounted_cumsum(episode_data[\"costs\"],\n\t                                                             gamma=1)\n\t            traj.append(episode_data)\n\t            traj_len.append(episode_step)\n\t            # reset trajectory buffer\n\t            data_, episode_step = defaultdict(list), 0\n\t        episode_step += 1\n\t    # needed for normalization, weighted sampling, other stats can be added also\n", "    info = {\n\t        \"obs_mean\": dataset[\"observations\"].mean(0, keepdims=True),\n\t        \"obs_std\": dataset[\"observations\"].std(0, keepdims=True) + 1e-6,\n\t        \"traj_lens\": np.array(traj_len),\n\t    }\n\t    return traj, info\n\tdef get_nearest_point(original_data: np.ndarray,\n\t                      sampled_data: np.ndarray,\n\t                      max_rew_decrease: float = 1,\n\t                      beta: float = 1):\n", "    \"\"\"\n\t    Given two arrays of data, finds the indices of the original data that are closest\n\t    to each sample in the sampled data, and returns a list of those indices.\n\t    Args:\n\t        original_data: A 2D numpy array of the original data.\n\t        sampled_data: A 2D numpy array of the sampled data.\n\t        max_rew_decrease: A float representing the maximum reward decrease allowed.\n\t        beta: A float used in calculating the distance between points.\n\t    Returns:\n\t        A list of integers representing the indices of the original data that are closest\n", "        to each sample in the sampled data.\n\t    \"\"\"\n\t    idxes = []\n\t    original_idx = np.arange(0, original_data.shape[0])\n\t    # for i in trange(sampled_data.shape[0], desc=\"Calculating nearest point\"):\n\t    for i in range(sampled_data.shape[0]):\n\t        p = sampled_data[i, :]\n\t        mask = original_data[:, 0] <= p[0]\n\t        # mask = np.logical_and(original_data[:, 0] <= p[0], original_data[:, 0] >= p[0] - 5)\n\t        delta = original_data[mask, :] - p\n", "        dist = np.hypot(delta[:, 0], delta[:, 1])\n\t        idx = np.argmin(dist)\n\t        idxes.append(original_idx[mask][idx])\n\t    counts = dict(Counter(idxes))\n\t    new_idxes = []\n\t    dist_fun = lambda x: 1 / (x + beta)\n\t    for idx, num in counts.items():\n\t        new_idxes.append(idx)\n\t        if num > 1:\n\t            p = original_data[idx, :]\n", "            mask = original_data[:, 0] <= p[0]\n\t            # the associated data should be: 1) smaller than the current cost 2) greater than certain reward\n\t            mask = np.logical_and(original_data[:, 0] <= p[0],\n\t                                  original_data[:, 1] >= p[1] - max_rew_decrease)\n\t            delta = original_data[mask, :] - p\n\t            dist = np.hypot(delta[:, 0], delta[:, 1])\n\t            dist = dist_fun(dist)\n\t            sample_idx = np.random.choice(dist.shape[0],\n\t                                          size=num - 1,\n\t                                          p=dist / np.sum(dist))\n", "            new_idxes.extend(original_idx[mask][sample_idx.tolist()])\n\t    return new_idxes\n\tdef grid_filter(x,\n\t                y,\n\t                xmin=-np.inf,\n\t                xmax=np.inf,\n\t                ymin=-np.inf,\n\t                ymax=np.inf,\n\t                xbins=10,\n\t                ybins=10,\n", "                max_num_per_bin=10,\n\t                min_num_per_bin=1):\n\t    xmin, xmax = max(min(x), xmin), min(max(x), xmax)\n\t    ymin, ymax = max(min(y), ymin), min(max(y), ymax)\n\t    xbin_step = (xmax - xmin) / xbins\n\t    ybin_step = (ymax - ymin) / ybins\n\t    # the key is x y bin index, the value is a list of indices\n\t    bin_hashmap = defaultdict(list)\n\t    for i in range(len(x)):\n\t        if x[i] < xmin or x[i] > xmax or y[i] < ymin or y[i] > ymax:\n", "            continue\n\t        x_bin_idx = (x[i] - xmin) // xbin_step\n\t        y_bin_idx = (y[i] - ymin) // ybin_step\n\t        bin_hashmap[(x_bin_idx, y_bin_idx)].append(i)\n\t    # start filtering\n\t    indices = []\n\t    for v in bin_hashmap.values():\n\t        if len(v) > max_num_per_bin:\n\t            # random sample max_num_per_bin indices\n\t            indices += random.sample(v, max_num_per_bin)\n", "        elif len(v) <= min_num_per_bin:\n\t            continue\n\t        else:\n\t            indices += v\n\t    return indices\n\tdef filter_trajectory(cost,\n\t                      rew,\n\t                      traj,\n\t                      cost_min=-np.inf,\n\t                      cost_max=np.inf,\n", "                      rew_min=-np.inf,\n\t                      rew_max=np.inf,\n\t                      cost_bins=60,\n\t                      rew_bins=50,\n\t                      max_num_per_bin=10,\n\t                      min_num_per_bin=1):\n\t    indices = grid_filter(cost,\n\t                          rew,\n\t                          cost_min,\n\t                          cost_max,\n", "                          rew_min,\n\t                          rew_max,\n\t                          xbins=cost_bins,\n\t                          ybins=rew_bins,\n\t                          max_num_per_bin=max_num_per_bin,\n\t                          min_num_per_bin=min_num_per_bin)\n\t    cost2, rew2, traj2 = [], [], []\n\t    for i in indices:\n\t        cost2.append(cost[i])\n\t        rew2.append(rew[i])\n", "        traj2.append(traj[i])\n\t    return cost2, rew2, traj2, indices\n\tdef augmentation(trajs: list,\n\t                 deg: int = 3,\n\t                 max_rew_decrease: float = 1,\n\t                 beta: float = 1,\n\t                 augment_percent: float = 0.3,\n\t                 max_reward: float = 1000.0,\n\t                 min_reward: float = 0.0):\n\t    \"\"\"\n", "    Applies data augmentation to a list of trajectories, \n\t    returning the augmented trajectories along with their indices \n\t    and the Pareto frontier of the original data.\n\t    Args:\n\t        trajs: A list of dictionaries representing the original trajectories.\n\t        deg: The degree of the polynomial used to fit the Pareto frontier.\n\t        max_rew_decrease: The maximum amount by which the reward of an augmented trajectory can decrease compared to the original.\n\t        beta: The scaling factor used to weigh the distance between cost and reward when finding nearest neighbors.\n\t        augment_percent: The percentage of original trajectories to use for augmentation.\n\t        max_reward: The maximum reward value for augmented trajectories.\n", "        min_reward: The minimum reward value for augmented trajectories.\n\t    Returns:\n\t        nearest_idx: A list of indices of the original trajectories that are nearest to each augmented trajectory.\n\t        aug_trajs: A list of dictionaries representing the augmented trajectories.\n\t        pareto_frontier: A polynomial function representing the Pareto frontier of the original data.\n\t    \"\"\"\n\t    rew_ret, cost_ret = [], []\n\t    for i, traj in enumerate(trajs):\n\t        r, c = traj[\"returns\"][0], traj[\"cost_returns\"][0]\n\t        rew_ret.append(r)\n", "        cost_ret.append(c)\n\t    rew_ret = np.array(rew_ret, dtype=np.float64)\n\t    cost_ret = np.array(cost_ret, dtype=np.float64)\n\t    # grid filer to filter outliers\n\t    cmin, cmax = np.min(cost_ret), np.max(cost_ret)\n\t    rmin, rmax = np.min(rew_ret), np.max(rew_ret)\n\t    cbins, rbins = 10, 50\n\t    max_npb, min_npb = 10, 2\n\t    cost_ret, rew_ret, trajs, indices = filter_trajectory(cost_ret,\n\t                                                          rew_ret,\n", "                                                          trajs,\n\t                                                          cost_min=cmin,\n\t                                                          cost_max=cmax,\n\t                                                          rew_min=rmin,\n\t                                                          rew_max=rmax,\n\t                                                          cost_bins=cbins,\n\t                                                          rew_bins=rbins,\n\t                                                          max_num_per_bin=max_npb,\n\t                                                          min_num_per_bin=min_npb)\n\t    print(f\"after filter {len(trajs)}\")\n", "    rew_ret = np.array(rew_ret, dtype=np.float64)\n\t    cost_ret = np.array(cost_ret, dtype=np.float64)\n\t    pareto = oapackage.ParetoDoubleLong()\n\t    for i in range(rew_ret.shape[0]):\n\t        w = oapackage.doubleVector((-cost_ret[i], rew_ret[i]))\n\t        pareto.addvalue(w, i)\n\t    # print pareto number\n\t    pareto.show(verbose=1)\n\t    pareto_idx = list(pareto.allindices())\n\t    cost_ret_pareto = cost_ret[pareto_idx]\n", "    rew_ret_pareto = rew_ret[pareto_idx]\n\t    pareto_frontier = np.poly1d(np.polyfit(cost_ret_pareto, rew_ret_pareto, deg=deg))\n\t    sample_num = int(augment_percent * cost_ret.shape[0])\n\t    # the augmented data should be within the cost return range of the dataset\n\t    cost_ret_range = np.linspace(np.min(cost_ret), np.max(cost_ret), sample_num)\n\t    pf_rew_ret = pareto_frontier(cost_ret_range)\n\t    max_reward = max_reward * np.ones(pf_rew_ret.shape)\n\t    min_reward = min_reward * np.ones(pf_rew_ret.shape)\n\t    # sample the rewards that are above the pf curve and within the max_reward\n\t    sampled_rew_ret = np.random.uniform(low=pf_rew_ret + min_reward,\n", "                                        high=max_reward,\n\t                                        size=sample_num)\n\t    # associate each sampled (cost, reward) pair with a trajectory index\n\t    original_data = np.hstack([cost_ret[:, None], rew_ret[:, None]])\n\t    sampled_data = np.hstack([cost_ret_range[:, None], sampled_rew_ret[:, None]])\n\t    nearest_idx = get_nearest_point(original_data, sampled_data, max_rew_decrease, beta)\n\t    # relabel the dataset\n\t    aug_trajs = []\n\t    for i, target in zip(nearest_idx, sampled_data):\n\t        target_cost_ret, target_rew_ret = target[0], target[1]\n", "        associated_traj = copy.deepcopy(trajs[i])\n\t        cost_ret, rew_ret = associated_traj[\"cost_returns\"], associated_traj[\"returns\"]\n\t        cost_ret += target_cost_ret - cost_ret[0]\n\t        rew_ret += target_rew_ret - rew_ret[0]\n\t        aug_trajs.append(associated_traj)\n\t    return nearest_idx, aug_trajs, pareto_frontier, indices\n\tdef compute_sample_prob(dataset, pareto_frontier, beta):\n\t    \"\"\"\n\t    Computes the probability of sampling each trajectory in a given dataset.\n\t    Args:\n", "        dataset (list): A list of dictionaries containing the trajectories \n\t                        to compute the sample probabilities for.\n\t        pareto_frontier (callable): A function that takes in a cost value and \n\t                                    returns the corresponding maximum reward value \n\t                                    on the Pareto frontier.\n\t        beta (float): A hyperparameter that controls the shape of the probability distribution.\n\t    Returns:\n\t        np.ndarray: A 1D numpy array of the same length as the dataset, \n\t                    containing the probability of sampling each trajectory.\n\t    \"\"\"\n", "    rew_ret, cost_ret = [], []\n\t    for i, traj in enumerate(dataset):\n\t        r, c = traj[\"returns\"][0], traj[\"cost_returns\"][0]\n\t        rew_ret.append(r)\n\t        cost_ret.append(c)\n\t    rew_ret = np.array(rew_ret, dtype=np.float64)  # type should be float64\n\t    cost_ret = np.array(cost_ret, dtype=np.float64)\n\t    prob_fun = lambda x: 1 / (x + beta)\n\t    sample_prob = []\n\t    for i in trange(cost_ret.shape[0], desc=\"Calculating sample prob\"):\n", "        r, c = rew_ret[i], cost_ret[i]\n\t        dist_fun = lambda x: (x - c)**2 + (pareto_frontier(x) - r)**2\n\t        sol = minimize(dist_fun, x0=c, method=\"bfgs\", tol=1e-4)\n\t        x = np.max([0, (sol.x)[0]])\n\t        dist = np.sqrt(dist_fun(x))\n\t        prob = prob_fun(dist)\n\t        sample_prob.append(prob)\n\t    sample_prob /= np.sum(sample_prob)\n\t    return sample_prob\n\tdef compute_cost_sample_prob(dataset, cost_transform=lambda x: 50 - x):\n", "    \"\"\"\n\t    Computes the sample probabilities for a given dataset based on its costs.\n\t    Args:\n\t        dataset (list): A list of trajectories, where each trajectory is a dictionary containing\n\t                        a \"cost_returns\" key with a list of cost values.\n\t        cost_transform (function): A function that transforms cost values.\n\t    Returns:\n\t        np.ndarray: A 1D numpy array of sample probabilities, normalized to sum to 1.\n\t    \"\"\"\n\t    sample_prob = []\n", "    for i, traj in enumerate(dataset):\n\t        c = cost_transform(traj[\"cost_returns\"][0])\n\t        sample_prob.append(c)\n\t    sample_prob = np.array(sample_prob)\n\t    sample_prob[sample_prob < 0] = 0\n\t    sample_prob /= np.sum(sample_prob)\n\t    return sample_prob\n\tdef gauss_kernel(size, std=1.0):\n\t    \"\"\"\n\t    Computes a 1D Gaussian kernel with the given size and standard deviation.\n", "    \"\"\"\n\t    size = int(size)\n\t    x = np.linspace(-size, size, 2 * size + 1)\n\t    g = np.exp(-(x**2 / std))\n\t    return g\n\tdef compute_start_index_sample_prob(dataset, prob=0.4):\n\t    \"\"\"\n\t    computes every trajectories start index sampling probability\n\t    \"\"\"\n\t    sample_prob_list = []\n", "    for i, traj in enumerate(dataset):\n\t        n = np.sum(traj[\"costs\"])\n\t        l = len(traj[\"costs\"])\n\t        if prob * l - n <= 0:\n\t            x = 100\n\t        else:\n\t            x = n * (1 - prob) / (prob * l - n)\n\t        # in case n=0\n\t        if x <= 0:\n\t            x = 1\n", "        costs = np.array(traj[\"costs\"])\n\t        kernel = gauss_kernel(10, 10)\n\t        costs = np.convolve(costs, kernel)\n\t        costs = costs[10:-10] + x\n\t        sample_prob = costs / costs.sum()\n\t        sample_prob_list.append(sample_prob)\n\t    return sample_prob_list\n\t# some utils functionalities specific for Decision Transformer\n\tdef pad_along_axis(arr: np.ndarray,\n\t                   pad_to: int,\n", "                   axis: int = 0,\n\t                   fill_value: float = 0.0) -> np.ndarray:\n\t    pad_size = pad_to - arr.shape[axis]\n\t    if pad_size <= 0:\n\t        return arr\n\t    npad = [(0, 0)] * arr.ndim\n\t    npad[axis] = (0, pad_size)\n\t    return np.pad(arr, pad_width=npad, mode=\"constant\", constant_values=fill_value)\n\tdef select_optimal_trajectory(trajs, rmin=0, cost_bins=60, max_num_per_bin=1):\n\t    \"\"\"\n", "    Selects the optimal trajectories from a list of trajectories based on their returns and costs.\n\t    Args:\n\t        trajs (list): A list of dictionaries, where each dictionary represents a trajectory and contains\n\t                      the keys \"returns\" and \"cost_returns\".\n\t        rmin (float): The minimum return that a trajectory must have in order to be considered optimal.\n\t        cost_bins (int): The number of bins to divide the cost range into.\n\t        max_num_per_bin (int): The maximum number of trajectories to select from each cost bin.\n\t    Returns:\n\t        list: A list of dictionaries representing the optimal trajectories.\n\t    \"\"\"\n", "    rew, cost = [], []\n\t    for i, traj in enumerate(trajs):\n\t        r, c = traj[\"returns\"][0], traj[\"cost_returns\"][0]\n\t        rew.append(r)\n\t        cost.append(c)\n\t    xmin, xmax = min(cost), max(cost)\n\t    xbin_step = (xmax - xmin) / cost_bins\n\t    # the key is x y bin index, the value is a list of indices\n\t    bin_hashmap = defaultdict(list)\n\t    for i in range(len(cost)):\n", "        if rew[i] < rmin:\n\t            continue\n\t        x_bin_idx = (cost[i] - xmin) // xbin_step\n\t        bin_hashmap[x_bin_idx].append(i)\n\t    # start filtering\n\t    def sort_index(idx):\n\t        return rew[idx]\n\t    indices = []\n\t    for v in bin_hashmap.values():\n\t        idx = heapq.nlargest(max_num_per_bin, v, key=sort_index)\n", "        indices += idx\n\t    traj2 = []\n\t    for i in indices:\n\t        traj2.append(trajs[i])\n\t    return traj2\n\tdef random_augmentation(trajs: list,\n\t                        augment_percent: float = 0.3,\n\t                        aug_rmin: float = 0,\n\t                        aug_rmax: float = 600,\n\t                        aug_cmin: float = 5,\n", "                        aug_cmax: float = 50,\n\t                        cgap: float = 5,\n\t                        rstd: float = 1,\n\t                        cstd: float = 0.25):\n\t    \"\"\"\n\t    Augments a list of trajectories with random noise.\n\t    Args:\n\t        trajs (list): A list of dictionaries, where each dictionary represents a trajectory\n\t            and contains \"returns\" and \"cost_returns\" keys that hold the returns and cost returns\n\t            for each time step of the trajectory.\n", "        augment_percent (float, optional): The percentage of trajectories to augment.\n\t        aug_rmin (float, optional): The minimum value for the augmented returns.\n\t        aug_rmax (float, optional): The maximum value for the augmented returns.\n\t        aug_cmin (float, optional): The minimum value for the augmented cost returns.\n\t        aug_cmax (float, optional): The maximum value for the augmented cost returns.\n\t        cgap (float, optional): The minimum distance between the augmented cost returns\n\t        rstd (float, optional): The standard deviation of the noise to add to the returns.\n\t        cstd (float, optional): The standard deviation of the noise to add to the cost returns.\n\t    Returns:\n\t        Tuple[List[int], List[Dict]]: A tuple containing two lists. The first list contains\n", "            the indices of the original trajectories that were augmented. The second list contains\n\t            the augmented trajectories, represented as dictionaries with \"returns\" and \"cost_returns\"\n\t            keys.\n\t    \"\"\"\n\t    rew_ret, cost_ret = [], []\n\t    for i, traj in enumerate(trajs):\n\t        r, c = traj[\"returns\"][0], traj[\"cost_returns\"][0]\n\t        rew_ret.append(r)\n\t        cost_ret.append(c)\n\t    # [traj_num]\n", "    rew_ret = np.array(rew_ret, dtype=np.float64)  # type should be float64\n\t    cost_ret = np.array(cost_ret, dtype=np.float64)\n\t    cmin = np.min(cost_ret)\n\t    num = int(augment_percent * cost_ret.shape[0])\n\t    sampled_cr = np.random.uniform(low=(aug_cmin, aug_rmin),\n\t                                   high=(aug_cmax, aug_rmax),\n\t                                   size=(num, 2))\n\t    idxes = []\n\t    original_data = np.hstack([cost_ret[:, None], rew_ret[:, None]])\n\t    original_idx = np.arange(0, original_data.shape[0])\n", "    # for i in trange(sampled_data.shape[0], desc=\"Calculating nearest point\"):\n\t    for i in range(sampled_cr.shape[0]):\n\t        p = sampled_cr[i, :]\n\t        boundary = max(p[0] - cgap, cmin + 1)\n\t        mask = original_data[:, 0] <= boundary\n\t        # mask = np.logical_and(original_data[:, 0] <= p[0], original_data[:, 0] >= p[0] - 5)\n\t        delta = original_data[mask, :] - p\n\t        dist = np.hypot(delta[:, 0], delta[:, 1])\n\t        idx = np.argmin(dist)\n\t        idxes.append(original_idx[mask][idx])\n", "    # relabel the dataset\n\t    aug_trajs = []\n\t    for i, target in zip(idxes, sampled_cr):\n\t        target_cost_ret, target_rew_ret = target[0], target[1]\n\t        associated_traj = copy.deepcopy(trajs[i])\n\t        cost_ret, rew_ret = associated_traj[\"cost_returns\"], associated_traj[\"returns\"]\n\t        cost_ret += target_cost_ret - cost_ret[0] + np.random.normal(\n\t            loc=0, scale=cstd, size=cost_ret.shape)\n\t        rew_ret += target_rew_ret - rew_ret[0] + np.random.normal(\n\t            loc=0, scale=rstd, size=rew_ret.shape)\n", "        aug_trajs.append(associated_traj)\n\t    return idxes, aug_trajs\n\tclass SequenceDataset(IterableDataset):\n\t    \"\"\"\n\t    A dataset of sequential data.\n\t    Args:\n\t        dataset (dict): Input dataset, containing trajectory IDs and sequences of observations.\n\t        seq_len (int): Length of sequence to use for training.\n\t        reward_scale (float): Scaling factor for reward values.\n\t        cost_scale (float): Scaling factor for cost values.\n", "        deg (int): Degree of polynomial used for Pareto frontier augmentation.\n\t        pf_sample (bool): Whether to sample data from the Pareto frontier.\n\t        max_rew_decrease (float): Maximum reward decrease for Pareto frontier augmentation.\n\t        beta (float): Parameter used for cost-based augmentation.\n\t        augment_percent (float): Percentage of data to augment.\n\t        max_reward (float): Maximum reward value for augmentation.\n\t        min_reward (float): Minimum reward value for augmentation.\n\t        cost_reverse (bool): Whether to reverse the cost values.\n\t        pf_only (bool): Whether to use only Pareto frontier data points.\n\t        rmin (float): Minimum reward value for random augmentation.\n", "        cost_bins (int): Number of cost bins for random augmentation.\n\t        npb (int): Number of data points to select from each cost bin for random augmentation.\n\t        cost_sample (bool): Whether to sample data based on cost.\n\t        cost_transform (callable): Function used to transform cost values.\n\t        prob (float): Probability of sampling from each trajectory start index.\n\t        start_sampling (bool): Whether to sample from each trajectory start index.\n\t        random_aug (float): Percentage of data to augment randomly.\n\t        aug_rmin (float): Minimum reward value for random augmentation.\n\t        aug_rmax (float): Maximum reward value for random augmentation.\n\t        aug_cmin (float): Minimum cost value for random augmentation.\n", "        aug_cmax (float): Maximum cost value for random augmentation.\n\t        cgap (float): Cost gap for random augmentation.\n\t        rstd (float): Standard deviation of reward values for random augmentation.\n\t        cstd (float): Standard deviation of cost values for random augmentation.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        dataset: dict,\n\t        seq_len: int = 10,\n\t        reward_scale: float = 1.0,\n", "        cost_scale: float = 1.0,\n\t        deg: int = 3,\n\t        pf_sample: bool = False,\n\t        max_rew_decrease: float = 1.0,\n\t        beta: float = 1.0,\n\t        augment_percent: float = 0,\n\t        max_reward: float = 1000.0,\n\t        min_reward: float = 5,\n\t        cost_reverse: bool = False,\n\t        pf_only: bool = False,\n", "        rmin: float = 0,\n\t        cost_bins: int = 60,\n\t        npb: int = 5,\n\t        cost_sample: bool = False,\n\t        cost_transform=lambda x: 50 - x,\n\t        prob: float = 0.4,\n\t        start_sampling: bool = False,\n\t        random_aug: float = 0,\n\t        aug_rmin: float = 0,\n\t        aug_rmax: float = 600,\n", "        aug_cmin: float = 5,\n\t        aug_cmax: float = 50,\n\t        cgap: float = 5,\n\t        rstd: float = 1,\n\t        cstd: float = 0.2,\n\t    ):\n\t        self.original_data, info = process_sequence_dataset(dataset, cost_reverse)\n\t        self.reward_scale = reward_scale\n\t        self.cost_scale = cost_scale\n\t        self.seq_len = seq_len\n", "        self.start_sampling = start_sampling\n\t        self.aug_data = []\n\t        if pf_only:\n\t            print(\"*\" * 100)\n\t            print(\"Using pareto frontier data points only!!!!!\")\n\t            print(\"*\" * 100)\n\t            self.dataset = select_optimal_trajectory(self.original_data, rmin, cost_bins,\n\t                                                     npb)\n\t        elif random_aug > 0:\n\t            self.idx, self.aug_data = random_augmentation(\n", "                self.original_data,\n\t                random_aug,\n\t                aug_rmin,\n\t                aug_rmax,\n\t                aug_cmin,\n\t                aug_cmax,\n\t                cgap,\n\t                rstd,\n\t                cstd,\n\t            )\n", "        elif augment_percent > 0:\n\t            # sampled data and the index of its \"nearest\" point in the dataset\n\t            self.idx, self.aug_data, self.pareto_frontier, self.indices = augmentation(\n\t                self.original_data, deg, max_rew_decrease, beta, augment_percent,\n\t                max_reward, min_reward)\n\t        self.dataset = self.original_data + self.aug_data\n\t        print(\n\t            f\"original data: {len(self.original_data)}, augment data: {len(self.aug_data)}, total: {len(self.dataset)}\"\n\t        )\n\t        if cost_sample:\n", "            self.sample_prob = compute_cost_sample_prob(self.dataset, cost_transform)\n\t        elif pf_sample:\n\t            self.sample_prob = compute_sample_prob(self.dataset, self.pareto_frontier, 1)\n\t        else:\n\t            self.sample_prob = None\n\t        # compute every trajectories start index sampling prob:\n\t        if start_sampling:\n\t            self.start_idx_sample_prob = compute_start_index_sample_prob(\n\t                dataset=self.dataset, prob=prob)\n\t    def compute_pareto_return(self, cost):\n", "        return self.pareto_frontier(cost)\n\t    def __prepare_sample(self, traj_idx, start_idx):\n\t        traj = self.dataset[traj_idx]\n\t        states = traj[\"observations\"][start_idx:start_idx + self.seq_len]\n\t        actions = traj[\"actions\"][start_idx:start_idx + self.seq_len]\n\t        returns = traj[\"returns\"][start_idx:start_idx + self.seq_len]\n\t        cost_returns = traj[\"cost_returns\"][start_idx:start_idx + self.seq_len]\n\t        time_steps = np.arange(start_idx, start_idx + self.seq_len)\n\t        costs = traj[\"costs\"][start_idx:start_idx + self.seq_len]\n\t        episode_cost = traj[\"cost_returns\"][0] * self.cost_scale\n", "        # states = (states - self.state_mean) / self.state_std\n\t        returns = returns * self.reward_scale\n\t        cost_returns = cost_returns * self.cost_scale\n\t        # pad up to seq_len if needed\n\t        mask = np.hstack(\n\t            [np.ones(states.shape[0]),\n\t             np.zeros(self.seq_len - states.shape[0])])\n\t        if states.shape[0] < self.seq_len:\n\t            states = pad_along_axis(states, pad_to=self.seq_len)\n\t            actions = pad_along_axis(actions, pad_to=self.seq_len)\n", "            returns = pad_along_axis(returns, pad_to=self.seq_len)\n\t            cost_returns = pad_along_axis(cost_returns, pad_to=self.seq_len)\n\t            costs = pad_along_axis(costs, pad_to=self.seq_len)\n\t        return states, actions, returns, cost_returns, time_steps, mask, episode_cost, costs\n\t    def __iter__(self):\n\t        while True:\n\t            traj_idx = np.random.choice(len(self.dataset), p=self.sample_prob)\n\t            # compute start index sampling prob\n\t            if self.start_sampling:\n\t                start_idx = np.random.choice(self.dataset[traj_idx][\"rewards\"].shape[0],\n", "                                             p=self.start_idx_sample_prob[traj_idx])\n\t            else:\n\t                start_idx = random.randint(\n\t                    0, self.dataset[traj_idx][\"rewards\"].shape[0] - 1)\n\t            yield self.__prepare_sample(traj_idx, start_idx)\n\tclass TransitionDataset(IterableDataset):\n\t    \"\"\"\n\t    A dataset of transitions (state, action, reward, next state) used for training RL agents.\n\t    Args:\n\t        dataset (dict): A dictionary of NumPy arrays containing the observations, actions, rewards, etc.\n", "        reward_scale (float): The scale factor for the rewards.\n\t        cost_scale (float): The scale factor for the costs.\n\t        state_init (bool): If True, the dataset will include an \"is_init\" flag indicating if a transition\n\t            corresponds to the initial state of an episode.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 dataset: dict,\n\t                 reward_scale: float = 1.0,\n\t                 cost_scale: float = 1.0,\n\t                 state_init: bool = False):\n", "        self.dataset = dataset\n\t        self.reward_scale = reward_scale\n\t        self.cost_scale = cost_scale\n\t        self.sample_prob = None\n\t        self.state_init = state_init\n\t        self.dataset_size = self.dataset[\"observations\"].shape[0]\n\t        self.dataset[\"done\"] = np.logical_or(self.dataset[\"terminals\"],\n\t                                             self.dataset[\"timeouts\"]).astype(np.float32)\n\t        if self.state_init:\n\t            self.dataset[\"is_init\"] = self.dataset[\"done\"].copy()\n", "            self.dataset[\"is_init\"][1:] = self.dataset[\"is_init\"][:-1]\n\t            self.dataset[\"is_init\"][0] = 1.0\n\t    def get_dataset_states(self):\n\t        \"\"\"\n\t        Returns the proportion of initial states in the dataset, \n\t        as well as the standard deviations of the observation and action spaces.\n\t        \"\"\"\n\t        init_state_propotion = self.dataset[\"is_init\"].mean()\n\t        obs_std = self.dataset[\"observations\"].std(0, keepdims=True)\n\t        act_std = self.dataset[\"actions\"].std(0, keepdims=True)\n", "        return init_state_propotion, obs_std, act_std\n\t    def __prepare_sample(self, idx):\n\t        observations = self.dataset[\"observations\"][idx, :]\n\t        next_observations = self.dataset[\"next_observations\"][idx, :]\n\t        actions = self.dataset[\"actions\"][idx, :]\n\t        rewards = self.dataset[\"rewards\"][idx] * self.reward_scale\n\t        costs = self.dataset[\"costs\"][idx] * self.cost_scale\n\t        done = self.dataset[\"done\"][idx]\n\t        if self.state_init:\n\t            is_init = self.dataset[\"is_init\"][idx]\n", "            return observations, next_observations, actions, rewards, costs, done, is_init\n\t        return observations, next_observations, actions, rewards, costs, done\n\t    def __iter__(self):\n\t        while True:\n\t            idx = np.random.choice(self.dataset_size, p=self.sample_prob)\n\t            yield self.__prepare_sample(idx)\n"]}
{"filename": "osrl/common/__init__.py", "chunked_list": ["from osrl.common.dataset import SequenceDataset, TransitionDataset\n\tfrom osrl.common.exp_util import *\n\tfrom osrl.common.net import *\n"]}
{"filename": "osrl/algorithms/bcql.py", "chunked_list": ["# reference: https://github.com/sfujim/BCQ\n\tfrom copy import deepcopy\n\timport gymnasium as gym\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom fsrl.utils import DummyLogger, WandbLogger\n\tfrom tqdm.auto import trange  # noqa\n\tfrom osrl.common.net import (VAE, EnsembleDoubleQCritic, LagrangianPIDController,\n\t                             MLPGaussianPerturbationActor)\n", "class BCQL(nn.Module):\n\t    \"\"\"\n\t        Batch-Constrained deep Q-learning with PID Lagrangian (BCQL)\n\t    Args:\n\t        state_dim (int): dimension of the state space.\n\t        action_dim (int): dimension of the action space.\n\t        max_action (float): Maximum action value.\n\t        a_hidden_sizes (list): List of integers specifying the sizes \n\t            of the layers in the actor network.\n\t        c_hidden_sizes (list): List of integers specifying the sizes \n", "            of the layers in the critic network.\n\t        vae_hidden_sizes (int): Number of hidden units in the VAE. \n\t        sample_action_num (int): Number of action samples to draw. \n\t        gamma (float): Discount factor for the reward.\n\t        tau (float): Soft update coefficient for the target networks. \n\t        phi (float): Scale parameter for the Gaussian perturbation \n\t            applied to the actor's output.\n\t        lmbda (float): Weight of the Lagrangian term.\n\t        beta (float): Weight of the KL divergence term.\n\t        PID (list): List of three floats containing the coefficients \n", "            of the PID controller.\n\t        num_q (int): Number of Q networks in the ensemble.\n\t        num_qc (int): Number of cost Q networks in the ensemble.\n\t        cost_limit (int): Upper limit on the cost per episode.\n\t        episode_len (int): Maximum length of an episode.\n\t        device (str): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n\t    \"\"\"\n\t    def __init__(self,\n\t                 state_dim: int,\n\t                 action_dim: int,\n", "                 max_action: float,\n\t                 a_hidden_sizes: list = [128, 128],\n\t                 c_hidden_sizes: list = [128, 128],\n\t                 vae_hidden_sizes: int = 64,\n\t                 sample_action_num: int = 10,\n\t                 gamma: float = 0.99,\n\t                 tau: float = 0.005,\n\t                 phi: float = 0.05,\n\t                 lmbda: float = 0.75,\n\t                 beta: float = 0.5,\n", "                 PID: list = [0.1, 0.003, 0.001],\n\t                 num_q: int = 1,\n\t                 num_qc: int = 1,\n\t                 cost_limit: int = 10,\n\t                 episode_len: int = 300,\n\t                 device: str = \"cpu\"):\n\t        super().__init__()\n\t        self.state_dim = state_dim\n\t        self.action_dim = action_dim\n\t        self.max_action = max_action\n", "        self.latent_dim = self.action_dim * 2\n\t        self.a_hidden_sizes = a_hidden_sizes\n\t        self.c_hidden_sizes = c_hidden_sizes\n\t        self.vae_hidden_sizes = vae_hidden_sizes\n\t        self.sample_action_num = sample_action_num\n\t        self.gamma = gamma\n\t        self.tau = tau\n\t        self.phi = phi\n\t        self.lmbda = lmbda\n\t        self.beta = beta\n", "        self.KP, self.KI, self.KD = PID\n\t        self.num_q = num_q\n\t        self.num_qc = num_qc\n\t        self.cost_limit = cost_limit\n\t        self.episode_len = episode_len\n\t        self.device = device\n\t        ################ create actor critic model ###############\n\t        self.actor = MLPGaussianPerturbationActor(self.state_dim, self.action_dim,\n\t                                                  self.a_hidden_sizes, nn.Tanh, self.phi,\n\t                                                  self.max_action).to(self.device)\n", "        self.critic = EnsembleDoubleQCritic(self.state_dim,\n\t                                            self.action_dim,\n\t                                            self.c_hidden_sizes,\n\t                                            nn.ReLU,\n\t                                            num_q=self.num_q).to(self.device)\n\t        self.cost_critic = EnsembleDoubleQCritic(self.state_dim,\n\t                                                 self.action_dim,\n\t                                                 self.c_hidden_sizes,\n\t                                                 nn.ReLU,\n\t                                                 num_q=self.num_qc).to(self.device)\n", "        self.vae = VAE(self.state_dim, self.action_dim, self.vae_hidden_sizes,\n\t                       self.latent_dim, self.max_action, self.device).to(self.device)\n\t        self.actor_old = deepcopy(self.actor)\n\t        self.actor_old.eval()\n\t        self.critic_old = deepcopy(self.critic)\n\t        self.critic_old.eval()\n\t        self.cost_critic_old = deepcopy(self.cost_critic)\n\t        self.cost_critic_old.eval()\n\t        self.qc_thres = cost_limit * (1 - self.gamma**self.episode_len) / (\n\t            1 - self.gamma) / self.episode_len\n", "        self.controller = LagrangianPIDController(self.KP, self.KI, self.KD,\n\t                                                  self.qc_thres)\n\t    def _soft_update(self, tgt: nn.Module, src: nn.Module, tau: float) -> None:\n\t        \"\"\"\n\t        Softly update the parameters of target module \n\t        towards the parameters of source module.\n\t        \"\"\"\n\t        for tgt_param, src_param in zip(tgt.parameters(), src.parameters()):\n\t            tgt_param.data.copy_(tau * src_param.data + (1 - tau) * tgt_param.data)\n\t    def vae_loss(self, observations, actions):\n", "        recon, mean, std = self.vae(observations, actions)\n\t        recon_loss = nn.functional.mse_loss(recon, actions)\n\t        KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean()\n\t        loss_vae = recon_loss + self.beta * KL_loss\n\t        self.vae_optim.zero_grad()\n\t        loss_vae.backward()\n\t        self.vae_optim.step()\n\t        stats_vae = {\"loss/loss_vae\": loss_vae.item()}\n\t        return loss_vae, stats_vae\n\t    def critic_loss(self, observations, next_observations, actions, rewards, done):\n", "        _, _, q1_list, q2_list = self.critic.predict(observations, actions)\n\t        with torch.no_grad():\n\t            batch_size = next_observations.shape[0]\n\t            obs_next = torch.repeat_interleave(next_observations, self.sample_action_num,\n\t                                               0).to(self.device)\n\t            act_targ_next = self.actor_old(obs_next, self.vae.decode(obs_next))\n\t            q1_targ, q2_targ, _, _ = self.critic_old.predict(obs_next, act_targ_next)\n\t            q_targ = self.lmbda * torch.min(\n\t                q1_targ, q2_targ) + (1. - self.lmbda) * torch.max(q1_targ, q2_targ)\n\t            q_targ = q_targ.reshape(batch_size, -1).max(1)[0]\n", "            backup = rewards + self.gamma * (1 - done) * q_targ\n\t        loss_critic = self.critic.loss(backup, q1_list) + self.critic.loss(\n\t            backup, q2_list)\n\t        self.critic_optim.zero_grad()\n\t        loss_critic.backward()\n\t        self.critic_optim.step()\n\t        stats_critic = {\"loss/critic_loss\": loss_critic.item()}\n\t        return loss_critic, stats_critic\n\t    def cost_critic_loss(self, observations, next_observations, actions, costs, done):\n\t        _, _, q1_list, q2_list = self.cost_critic.predict(observations, actions)\n", "        with torch.no_grad():\n\t            batch_size = next_observations.shape[0]\n\t            obs_next = torch.repeat_interleave(next_observations, self.sample_action_num,\n\t                                               0).to(self.device)\n\t            act_targ_next = self.actor_old(obs_next, self.vae.decode(obs_next))\n\t            q1_targ, q2_targ, _, _ = self.cost_critic_old.predict(\n\t                obs_next, act_targ_next)\n\t            q_targ = self.lmbda * torch.min(\n\t                q1_targ, q2_targ) + (1. - self.lmbda) * torch.max(q1_targ, q2_targ)\n\t            q_targ = q_targ.reshape(batch_size, -1).max(1)[0]\n", "            backup = costs + self.gamma * q_targ\n\t        loss_cost_critic = self.cost_critic.loss(\n\t            backup, q1_list) + self.cost_critic.loss(backup, q2_list)\n\t        self.cost_critic_optim.zero_grad()\n\t        loss_cost_critic.backward()\n\t        self.cost_critic_optim.step()\n\t        stats_cost_critic = {\"loss/cost_critic_loss\": loss_cost_critic.item()}\n\t        return loss_cost_critic, stats_cost_critic\n\t    def actor_loss(self, observations):\n\t        for p in self.critic.parameters():\n", "            p.requires_grad = False\n\t        for p in self.cost_critic.parameters():\n\t            p.requires_grad = False\n\t        for p in self.vae.parameters():\n\t            p.requires_grad = False\n\t        actions = self.actor(observations, self.vae.decode(observations))\n\t        q1_pi, q2_pi, _, _ = self.critic.predict(observations, actions)  # [batch_size]\n\t        qc1_pi, qc2_pi, _, _ = self.cost_critic.predict(observations, actions)\n\t        qc_pi = torch.min(qc1_pi, qc2_pi)\n\t        q_pi = torch.min(q1_pi, q2_pi)\n", "        with torch.no_grad():\n\t            multiplier = self.controller.control(qc_pi).detach()\n\t        qc_penalty = ((qc_pi - self.qc_thres) * multiplier).mean()\n\t        loss_actor = -q_pi.mean() + qc_penalty\n\t        self.actor_optim.zero_grad()\n\t        loss_actor.backward()\n\t        self.actor_optim.step()\n\t        stats_actor = {\n\t            \"loss/actor_loss\": loss_actor.item(),\n\t            \"loss/qc_penalty\": qc_penalty.item(),\n", "            \"loss/lagrangian\": multiplier.item()\n\t        }\n\t        for p in self.critic.parameters():\n\t            p.requires_grad = True\n\t        for p in self.cost_critic.parameters():\n\t            p.requires_grad = True\n\t        for p in self.vae.parameters():\n\t            p.requires_grad = True\n\t        return loss_actor, stats_actor\n\t    def setup_optimizers(self, actor_lr, critic_lr, vae_lr):\n", "        \"\"\"\n\t        Sets up optimizers for the actor, critic, cost critic, and VAE models.\n\t        \"\"\"\n\t        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n\t        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n\t        self.cost_critic_optim = torch.optim.Adam(self.cost_critic.parameters(),\n\t                                                  lr=critic_lr)\n\t        self.vae_optim = torch.optim.Adam(self.vae.parameters(), lr=vae_lr)\n\t    def sync_weight(self):\n\t        \"\"\"\n", "        Soft-update the weight for the target network.\n\t        \"\"\"\n\t        self._soft_update(self.critic_old, self.critic, self.tau)\n\t        self._soft_update(self.cost_critic_old, self.cost_critic, self.tau)\n\t        self._soft_update(self.actor_old, self.actor, self.tau)\n\t    def act(self, obs, deterministic=False, with_logprob=False):\n\t        '''\n\t        Given a single obs, return the action, value, logp.\n\t        '''\n\t        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n", "        act = self.actor(obs, self.vae.decode(obs))\n\t        act = act.data.numpy() if self.device == \"cpu\" else act.data.cpu().numpy()\n\t        return np.squeeze(act, axis=0), None\n\tclass BCQLTrainer:\n\t    \"\"\"\n\t    Constraints Penalized Q-learning Trainer\n\t    Args:\n\t        model (BCQL): The BCQL model to be trained.\n\t        env (gym.Env): The OpenAI Gym environment to train the model in.\n\t        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n", "        actor_lr (float): learning rate for actor\n\t        critic_lr (float): learning rate for critic\n\t        vae_lr (float): learning rate for vae\n\t        reward_scale (float): The scaling factor for the reward signal.\n\t        cost_scale (float): The scaling factor for the constraint cost.\n\t        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n\t            model: BCQL,\n", "            env: gym.Env,\n\t            logger: WandbLogger = DummyLogger(),\n\t            # training params\n\t            actor_lr: float = 1e-4,\n\t            critic_lr: float = 1e-4,\n\t            vae_lr: float = 1e-4,\n\t            reward_scale: float = 1.0,\n\t            cost_scale: float = 1.0,\n\t            device=\"cpu\"):\n\t        self.model = model\n", "        self.logger = logger\n\t        self.env = env\n\t        self.reward_scale = reward_scale\n\t        self.cost_scale = cost_scale\n\t        self.device = device\n\t        self.model.setup_optimizers(actor_lr, critic_lr, vae_lr)\n\t    def train_one_step(self, observations, next_observations, actions, rewards, costs,\n\t                       done):\n\t        \"\"\"\n\t        Trains the model by updating the VAE, critic, cost critic, and actor.\n", "        \"\"\"\n\t        # update VAE\n\t        loss_vae, stats_vae = self.model.vae_loss(observations, actions)\n\t        # update critic\n\t        loss_critic, stats_critic = self.model.critic_loss(observations,\n\t                                                           next_observations, actions,\n\t                                                           rewards, done)\n\t        # update cost critic\n\t        loss_cost_critic, stats_cost_critic = self.model.cost_critic_loss(\n\t            observations, next_observations, actions, costs, done)\n", "        # update actor\n\t        loss_actor, stats_actor = self.model.actor_loss(observations)\n\t        self.model.sync_weight()\n\t        self.logger.store(**stats_vae)\n\t        self.logger.store(**stats_critic)\n\t        self.logger.store(**stats_cost_critic)\n\t        self.logger.store(**stats_actor)\n\t    def evaluate(self, eval_episodes):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a number of episodes.\n", "        \"\"\"\n\t        self.model.eval()\n\t        episode_rets, episode_costs, episode_lens = [], [], []\n\t        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n\t            epi_ret, epi_len, epi_cost = self.rollout()\n\t            episode_rets.append(epi_ret)\n\t            episode_lens.append(epi_len)\n\t            episode_costs.append(epi_cost)\n\t        self.model.train()\n\t        return np.mean(episode_rets) / self.reward_scale, np.mean(\n", "            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\t    @torch.no_grad()\n\t    def rollout(self):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a single episode.\n\t        \"\"\"\n\t        obs, info = self.env.reset()\n\t        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n\t        for _ in range(self.model.episode_len):\n\t            act, _ = self.model.act(obs)\n", "            obs_next, reward, terminated, truncated, info = self.env.step(act)\n\t            cost = info[\"cost\"] * self.cost_scale\n\t            obs = obs_next\n\t            episode_ret += reward\n\t            episode_len += 1\n\t            episode_cost += cost\n\t            if terminated or truncated:\n\t                break\n\t        return episode_ret, episode_len, episode_cost\n"]}
{"filename": "osrl/algorithms/coptidice.py", "chunked_list": ["# reference: https://github.com/deepmind/constrained_optidice.git\n\timport gymnasium as gym\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom fsrl.utils import DummyLogger, WandbLogger\n\tfrom torch import distributions as pyd\n\tfrom torch.distributions.beta import Beta\n\tfrom torch.nn import functional as F  # noqa\n\tfrom tqdm.auto import trange  # noqa\n", "from osrl.common.net import EnsembleQCritic, SquashedGaussianMLPActor\n\tdef get_f_div_fn(f_type: str):\n\t    \"\"\"\n\t    Returns a function that computes the provided f-divergence type.\n\t    \"\"\"\n\t    f_fn = None\n\t    f_prime_inv_fn = None\n\t    if f_type == 'chi2':\n\t        f_fn = lambda x: 0.5 * (x - 1)**2\n\t        f_prime_inv_fn = lambda x: x + 1\n", "    elif f_type == 'softchi':\n\t        f_fn = lambda x: torch.where(x < 1,\n\t                                     x * (torch.log(x + 1e-10) - 1) + 1, 0.5 *\n\t                                     (x - 1)**2)\n\t        f_prime_inv_fn = lambda x: torch.where(x < 0, torch.exp(x.clamp(max=0.0)), x + 1)\n\t    elif f_type == 'kl':\n\t        f_fn = lambda x: x * torch.log(x + 1e-10)\n\t        f_prime_inv_fn = lambda x: torch.exp(x - 1)\n\t    else:\n\t        raise NotImplementedError('Not implemented f_fn:', f_type)\n", "    return f_fn, f_prime_inv_fn\n\tclass COptiDICE(nn.Module):\n\t    \"\"\"\n\t    Offline Constrained Policy Optimization \n\t    via stationary DIstribution Correction Estimation (COptiDICE)\n\t    Args:\n\t        state_dim (int): dimension of the state space.\n\t        action_dim (int): dimension of the action space.\n\t        max_action (float): Maximum action value.\n\t        f_type (str): The type of f-divergence function to use.\n", "        init_state_propotion (float): The proportion of initial states to include in the optimization.\n\t        observations_std (np.ndarray): The standard deviation of the observation space.\n\t        actions_std (np.ndarray): The standard deviation of the action space.\n\t        a_hidden_sizes (list): List of integers specifying the sizes \n\t                               of the layers in the actor network.\n\t        c_hidden_sizes (list): List of integers specifying the sizes \n\t                               of the layers in the critic network (nu and chi networks).\n\t        gamma (float): Discount factor for the reward.\n\t        alpha (float): The coefficient for the cost term in the loss function.\n\t        cost_ub_epsilon (float): A small value added to the upper bound on the cost term.\n", "        num_nu (int): The number of critics to use for the nu-network.\n\t        num_chi (int): The number of critics to use for the chi-network.\n\t        cost_limit (int): Upper limit on the cost per episode.\n\t        episode_len (int): Maximum length of an episode.\n\t        device (str): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n\t    \"\"\"\n\t    def __init__(self,\n\t                 state_dim: int,\n\t                 action_dim: int,\n\t                 max_action: float,\n", "                 f_type: str,\n\t                 init_state_propotion: float,\n\t                 observations_std: np.ndarray,\n\t                 actions_std: np.ndarray,\n\t                 a_hidden_sizes: list = [128, 128],\n\t                 c_hidden_sizes: list = [128, 128],\n\t                 gamma: float = 0.99,\n\t                 alpha: float = 0.5,\n\t                 cost_ub_epsilon: float = 0.01,\n\t                 num_nu: int = 1,\n", "                 num_chi: int = 1,\n\t                 cost_limit: int = 10,\n\t                 episode_len: int = 300,\n\t                 device: str = \"cpu\"):\n\t        super().__init__()\n\t        self.state_dim = state_dim\n\t        self.action_dim = action_dim\n\t        self.max_action = max_action\n\t        self.a_hidden_sizes = a_hidden_sizes\n\t        self.c_hidden_sizes = c_hidden_sizes\n", "        self.gamma = gamma\n\t        self.alpha = alpha\n\t        self.cost_ub_epsilon = cost_ub_epsilon\n\t        self.num_nu = num_nu\n\t        self.num_chi = num_chi\n\t        self.cost_limit = cost_limit\n\t        self.episode_len = episode_len\n\t        self.device = device\n\t        self.qc_thres = cost_limit * (1 - self.gamma**self.episode_len) / (\n\t            1 - self.gamma) / self.episode_len\n", "        self.tau = torch.ones(1, requires_grad=True, device=self.device)\n\t        self.lmbda = torch.ones(1, requires_grad=True, device=self.device)\n\t        self.actor = SquashedGaussianMLPActor(self.state_dim, self.action_dim,\n\t                                              self.a_hidden_sizes,\n\t                                              nn.ReLU).to(self.device)\n\t        self.nu_network = EnsembleQCritic(self.state_dim,\n\t                                          0,\n\t                                          self.c_hidden_sizes,\n\t                                          nn.ReLU,\n\t                                          num_q=self.num_nu).to(self.device)\n", "        self.chi_network = EnsembleQCritic(self.state_dim,\n\t                                           0,\n\t                                           self.c_hidden_sizes,\n\t                                           nn.ReLU,\n\t                                           num_q=self.num_chi).to(self.device)\n\t        self.f_fn, self.f_prime_inv_fn = get_f_div_fn(f_type)\n\t        self.init_state_propotion = init_state_propotion\n\t        self.observations_std = torch.tensor(observations_std, device=self.device)\n\t        self.actions_std = torch.tensor(actions_std, device=self.device)\n\t    def _optimal_w(self, observations, next_observations, rewards, costs, done):\n", "        nu_s, _ = self.nu_network.predict(observations, None)\n\t        nu_s_next, _ = self.nu_network.predict(next_observations, None)\n\t        # \\hat{e}_{\\lambda, \\nu}(s,a,s')\n\t        e_nu_lambda = rewards - self._lmbda.detach() * costs\n\t        e_nu_lambda += self.gamma * (1.0 - done) * nu_s_next - nu_s\n\t        # w_{lambda,\\nu}^*(s,a)\n\t        w_sa = F.relu(self.f_prime_inv_fn(e_nu_lambda / self.alpha))\n\t        return nu_s, nu_s_next, e_nu_lambda, w_sa\n\t    def update(self, batch):\n\t        observations, next_observations, actions, rewards, costs, done, is_init = batch\n", "        # 1. Learn the optimal distribution\n\t        self._lmbda = F.softplus(self.lmbda)  # lmbda >= 0\n\t        nu_s, nu_s_next, e_nu_lambda, w_sa = self._optimal_w(observations,\n\t                                                             next_observations, rewards,\n\t                                                             costs, done)\n\t        nu_init = nu_s * is_init / self.init_state_propotion\n\t        w_sa_no_grad = w_sa.detach()\n\t        # divergence between distributions of policy & dataset\n\t        Df = self.f_fn(w_sa_no_grad).mean()\n\t        # 1.1 (chi, tau) loss\n", "        if self.cost_ub_epsilon == 0:\n\t            weighted_c = (w_sa_no_grad * costs).mean()\n\t            chi_loss = torch.zeros(1, device=self.device)\n\t            tau_loss = torch.zeros(1, device=self.device)\n\t            D_kl = torch.zeros(1, device=self.device)\n\t            self._tau = F.softplus(self.tau)\n\t        else:\n\t            self._tau = F.softplus(self.tau)\n\t            batch_size = observations.shape[0]\n\t            chi_s, _ = self.chi_network.predict(observations, None)\n", "            chi_s_next, _ = self.chi_network.predict(next_observations, None)\n\t            chi_init = chi_s * is_init / self.init_state_propotion\n\t            ell = (1- self.gamma) * chi_init + \\\n\t                    w_sa_no_grad * (costs + self.gamma * (1 - done) * chi_s_next - chi_s)\n\t            logits = ell / self._tau.detach()\n\t            weights = torch.softmax(logits, dim=0) * batch_size\n\t            log_weights = torch.log_softmax(logits, dim=0) + np.log(batch_size)\n\t            D_kl = (weights * log_weights - weights + 1).mean()\n\t            # an upper bound estimation\n\t            weighted_c = (weights * w_sa_no_grad * costs).mean()\n", "            chi_loss = (weights * ell).mean()\n\t            self.chi_optim.zero_grad()\n\t            chi_loss.backward(retain_graph=True)\n\t            self.chi_optim.step()\n\t            tau_loss = self._tau * (self.cost_ub_epsilon - D_kl.detach())\n\t            self.tau_optim.zero_grad()\n\t            tau_loss.backward()\n\t            self.tau_optim.step()\n\t        # 1.2 nu loss\n\t        nu_loss = (1 - self.gamma) * nu_init.mean() + \\\n", "            (w_sa * e_nu_lambda - self.alpha * self.f_fn(w_sa)).mean()\n\t        td_error = e_nu_lambda.pow(2).mean()\n\t        self.nu_optim.zero_grad()\n\t        nu_loss.backward(retain_graph=True)\n\t        self.nu_optim.step()\n\t        # 1.3 lambda loss\n\t        lmbda_loss = self._lmbda * (self.qc_thres - weighted_c.detach())\n\t        self.lmbda_optim.zero_grad()\n\t        lmbda_loss.backward()\n\t        self.lmbda_optim.step()\n", "        # 2. Extract policy\n\t        obs_eps = torch.randn_like(observations) * self.observations_std * 0.1\n\t        act_eps = torch.randn_like(actions) * self.actions_std * 0.1\n\t        _, _, dist = self.actor.forward(observations + obs_eps, False, True, True)\n\t        with torch.no_grad():\n\t            _, _, e_nu_lambda, w_sa = self._optimal_w(observations, next_observations,\n\t                                                      rewards, costs, done)\n\t        actor_loss = -(w_sa * dist.log_prob(actions + act_eps).sum(axis=-1)).mean()\n\t        self.actor_optim.zero_grad()\n\t        actor_loss.backward()\n", "        self.actor_optim.step()\n\t        stats_loss = {\n\t            \"loss/chi_loss\": chi_loss.item(),\n\t            \"loss/tau_loss\": tau_loss.item(),\n\t            \"loss/D_kl\": D_kl.item(),\n\t            \"loss/Df\": Df.item(),\n\t            \"loss/td_error\": td_error.item(),\n\t            \"loss/nu_loss\": nu_loss.item(),\n\t            \"loss/lmbda_loss\": lmbda_loss.item(),\n\t            \"loss/actor_loss\": actor_loss.item(),\n", "            \"loss/tau\": self._tau.item(),\n\t            \"loss/lmbda\": self._lmbda.item()\n\t        }\n\t        return stats_loss\n\t    def setup_optimizers(self, actor_lr, critic_lr, scalar_lr):\n\t        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n\t        self.nu_optim = torch.optim.Adam(self.nu_network.parameters(), lr=critic_lr)\n\t        self.chi_optim = torch.optim.Adam(self.chi_network.parameters(), lr=critic_lr)\n\t        self.lmbda_optim = torch.optim.Adam([self.lmbda], lr=scalar_lr)\n\t        self.tau_optim = torch.optim.Adam([self.tau], lr=scalar_lr)\n", "    def act(self,\n\t            obs: np.ndarray,\n\t            deterministic: bool = False,\n\t            with_logprob: bool = False):\n\t        \"\"\"\n\t        Given a single obs, return the action, logp.\n\t        \"\"\"\n\t        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n\t        a, logp_a = self.actor.forward(obs, deterministic, with_logprob)\n\t        a = a.data.numpy() if self.device == \"cpu\" else a.data.cpu().numpy()\n", "        logp_a = logp_a.data.numpy() if self.device == \"cpu\" else logp_a.data.cpu(\n\t        ).numpy()\n\t        return np.squeeze(a, axis=0), np.squeeze(logp_a)\n\tclass COptiDICETrainer:\n\t    \"\"\"\n\t    COptiDICE trainer\n\t    Args:\n\t        model (COptiDICE): The COptiDICE model to train.\n\t        env (gym.Env): The OpenAI Gym environment to train the model in.\n\t        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n", "        actor_lr (float): learning rate for actor\n\t        critic_lr (float): learning rate for critic (nu and chi networks)\n\t        scalar_lr (float, optional): The learning rate for the scalar (tau, lmbda).\n\t        reward_scale (float): The scaling factor for the reward signal.\n\t        cost_scale (float): The scaling factor for the constraint cost.\n\t        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n\t    \"\"\"\n\t    def __init__(self,\n\t                 model: COptiDICE,\n\t                 env: gym.Env,\n", "                 logger: WandbLogger = DummyLogger(),\n\t                 actor_lr: float = 1e-3,\n\t                 critic_lr: float = 1e-3,\n\t                 scalar_lr: float = 1e-3,\n\t                 reward_scale: float = 1.0,\n\t                 cost_scale: float = 1.0,\n\t                 device=\"cpu\"):\n\t        self.model = model\n\t        self.logger = logger\n\t        self.env = env\n", "        self.reward_scale = reward_scale\n\t        self.cost_scale = cost_scale\n\t        self.device = device\n\t        self.model.setup_optimizers(actor_lr, critic_lr, scalar_lr)\n\t    def train_one_step(self, batch):\n\t        stats_loss = self.model.update(batch)\n\t        self.logger.store(**stats_loss)\n\t    def evaluate(self, eval_episodes):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a number of episodes.\n", "        \"\"\"\n\t        self.model.eval()\n\t        episode_rets, episode_costs, episode_lens = [], [], []\n\t        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n\t            epi_ret, epi_len, epi_cost = self.rollout()\n\t            episode_rets.append(epi_ret)\n\t            episode_lens.append(epi_len)\n\t            episode_costs.append(epi_cost)\n\t        self.model.train()\n\t        return np.mean(episode_rets) / self.reward_scale, np.mean(\n", "            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\t    @torch.no_grad()\n\t    def rollout(self):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a single episode.\n\t        \"\"\"\n\t        obs, info = self.env.reset()\n\t        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n\t        for _ in range(self.model.episode_len):\n\t            act, _ = self.model.act(obs, True, True)\n", "            obs_next, reward, terminated, truncated, info = self.env.step(act)\n\t            cost = info[\"cost\"] * self.cost_scale\n\t            obs = obs_next\n\t            episode_ret += reward\n\t            episode_len += 1\n\t            episode_cost += cost\n\t            if terminated or truncated:\n\t                break\n\t        return episode_ret, episode_len, episode_cost\n"]}
{"filename": "osrl/algorithms/__init__.py", "chunked_list": ["from .bc import BC, BCTrainer\n\tfrom .bcql import BCQL, BCQLTrainer\n\tfrom .bearl import BEARL, BEARLTrainer\n\tfrom .cdt import CDT, CDTTrainer\n\tfrom .coptidice import COptiDICE, COptiDICETrainer\n\tfrom .cpq import CPQ, CPQTrainer"]}
{"filename": "osrl/algorithms/cpq.py", "chunked_list": ["from copy import deepcopy\n\timport gymnasium as gym\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom fsrl.utils import DummyLogger, WandbLogger\n\tfrom tqdm.auto import trange  # noqa\n\tfrom osrl.common.net import VAE, EnsembleQCritic, SquashedGaussianMLPActor\n\tclass CPQ(nn.Module):\n\t    \"\"\"\n", "    Constraints Penalized Q-Learning (CPQ)\n\t    Args:\n\t        state_dim (int): dimension of the state space.\n\t        action_dim (int): dimension of the action space.\n\t        max_action (float): Maximum action value.\n\t        a_hidden_sizes (list): List of integers specifying the sizes \n\t                               of the layers in the actor network.\n\t        c_hidden_sizes (list): List of integers specifying the sizes \n\t                               of the layers in the critic network.\n\t        vae_hidden_sizes (int): Number of hidden units in the VAE. \n", "        sample_action_num (int): Number of action samples to draw. \n\t        gamma (float): Discount factor for the reward.\n\t        tau (float): Soft update coefficient for the target networks. \n\t        beta (float): Weight of the KL divergence term.\n\t        num_q (int): Number of Q networks in the ensemble.\n\t        num_qc (int): Number of cost Q networks in the ensemble.\n\t        qc_scalar (float): Scaling factor for the cost critic threshold.\n\t        cost_limit (int): Upper limit on the cost per episode.\n\t        episode_len (int): Maximum length of an episode.\n\t        device (str): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n", "    \"\"\"\n\t    def __init__(self,\n\t                 state_dim: int,\n\t                 action_dim: int,\n\t                 max_action: float,\n\t                 a_hidden_sizes: list = [128, 128],\n\t                 c_hidden_sizes: list = [128, 128],\n\t                 vae_hidden_sizes: int = 64,\n\t                 sample_action_num: int = 10,\n\t                 gamma: float = 0.99,\n", "                 tau: float = 0.005,\n\t                 beta: float = 1.5,\n\t                 num_q: int = 1,\n\t                 num_qc: int = 1,\n\t                 qc_scalar: float = 1.5,\n\t                 cost_limit: int = 10,\n\t                 episode_len: int = 300,\n\t                 device: str = \"cpu\"):\n\t        super().__init__()\n\t        self.a_hidden_sizes = a_hidden_sizes\n", "        self.c_hidden_sizes = c_hidden_sizes\n\t        self.vae_hidden_sizes = vae_hidden_sizes\n\t        self.gamma = gamma\n\t        self.tau = tau\n\t        self.beta = beta\n\t        self.cost_limit = cost_limit\n\t        self.num_q = num_q\n\t        self.num_qc = num_qc\n\t        self.qc_scalar = qc_scalar\n\t        self.sample_action_num = sample_action_num\n", "        self.state_dim = state_dim\n\t        self.action_dim = action_dim\n\t        self.latent_dim = self.action_dim * 2\n\t        self.episode_len = episode_len\n\t        self.max_action = max_action\n\t        self.device = device\n\t        ################ create actor critic model ###############\n\t        self.actor = SquashedGaussianMLPActor(self.state_dim, self.action_dim,\n\t                                              self.a_hidden_sizes,\n\t                                              nn.ReLU).to(self.device)\n", "        self.critic = EnsembleQCritic(self.state_dim,\n\t                                      self.action_dim,\n\t                                      self.c_hidden_sizes,\n\t                                      nn.ReLU,\n\t                                      num_q=self.num_q).to(self.device)\n\t        self.vae = VAE(self.state_dim, self.action_dim, self.vae_hidden_sizes,\n\t                       self.latent_dim, self.max_action, self.device).to(self.device)\n\t        self.cost_critic = EnsembleQCritic(self.state_dim,\n\t                                           self.action_dim,\n\t                                           self.c_hidden_sizes,\n", "                                           nn.ReLU,\n\t                                           num_q=self.num_qc).to(self.device)\n\t        self.log_alpha = torch.tensor(0.0, device=self.device)\n\t        self.actor_old = deepcopy(self.actor)\n\t        self.actor_old.eval()\n\t        self.critic_old = deepcopy(self.critic)\n\t        self.critic_old.eval()\n\t        self.cost_critic_old = deepcopy(self.cost_critic)\n\t        self.cost_critic_old.eval()\n\t        # set critic and cost critic threshold\n", "        self.q_thres = cost_limit * (1 - self.gamma**self.episode_len) / (\n\t            1 - self.gamma) / self.episode_len\n\t        self.qc_thres = qc_scalar * self.q_thres\n\t    def _soft_update(self, tgt: nn.Module, src: nn.Module, tau: float) -> None:\n\t        \"\"\"\n\t        Softly update the parameters of target module \n\t        towards the parameters of source module.\n\t        \"\"\"\n\t        for tgt_param, src_param in zip(tgt.parameters(), src.parameters()):\n\t            tgt_param.data.copy_(tau * src_param.data + (1 - tau) * tgt_param.data)\n", "    def _actor_forward(self,\n\t                       obs: torch.tensor,\n\t                       deterministic: bool = False,\n\t                       with_logprob: bool = True):\n\t        \"\"\"\n\t        Return action distribution and action log prob [optional].\n\t        \"\"\"\n\t        a, logp = self.actor(obs, deterministic, with_logprob)\n\t        return a * self.max_action, logp\n\t    def vae_loss(self, observations, actions):\n", "        recon, mean, std = self.vae(observations, actions)\n\t        recon_loss = nn.functional.mse_loss(recon, actions)\n\t        KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean()\n\t        loss_vae = recon_loss + self.beta * KL_loss\n\t        self.vae_optim.zero_grad()\n\t        loss_vae.backward()\n\t        self.vae_optim.step()\n\t        stats_vae = {\"loss/loss_vae\": loss_vae.item()}\n\t        return loss_vae, stats_vae\n\t    def critic_loss(self, observations, next_observations, actions, rewards, done):\n", "        _, q_list = self.critic.predict(observations, actions)\n\t        # Bellman backup for Q functions\n\t        with torch.no_grad():\n\t            next_actions, _ = self._actor_forward(next_observations, False, True)\n\t            q_targ, _ = self.critic_old.predict(next_observations, next_actions)\n\t            qc_targ, _ = self.cost_critic_old.predict(next_observations, next_actions)\n\t            # Constraints Penalized Bellman operator\n\t            backup = rewards + self.gamma * (1 -\n\t                                             done) * (qc_targ <= self.q_thres) * q_targ\n\t        # MSE loss against Bellman backup\n", "        loss_critic = self.critic.loss(backup, q_list)\n\t        self.critic_optim.zero_grad()\n\t        loss_critic.backward()\n\t        self.critic_optim.step()\n\t        stats_critic = {\"loss/critic_loss\": loss_critic.item()}\n\t        return loss_critic, stats_critic\n\t    def cost_critic_loss(self, observations, next_observations, actions, costs, done):\n\t        _, qc_list = self.cost_critic.predict(observations, actions)\n\t        # Bellman backup for Q functions\n\t        with torch.no_grad():\n", "            next_actions, _ = self._actor_forward(next_observations, False, True)\n\t            qc_targ, _ = self.cost_critic_old.predict(next_observations, next_actions)\n\t            backup = costs + self.gamma * qc_targ\n\t            batch_size = observations.shape[0]\n\t            _, _, pi_dist = self.actor(observations, False, True, True)\n\t            # sample actions\n\t            sampled_actions = pi_dist.sample(\n\t                [self.sample_action_num])  # [sample_action_num, batch_size, act_dim]\n\t            sampled_actions = sampled_actions.reshape(\n\t                self.sample_action_num * batch_size, self.action_dim)\n", "            stacked_obs = torch.tile(observations[None, :, :],\n\t                                     (self.sample_action_num, 1,\n\t                                      1))  # [sample_action_num, batch_size, obs_dim]\n\t            stacked_obs = stacked_obs.reshape(self.sample_action_num * batch_size,\n\t                                              self.state_dim)\n\t            qc_sampled, _ = self.cost_critic_old.predict(stacked_obs, sampled_actions)\n\t            qc_sampled = qc_sampled.reshape(self.sample_action_num, batch_size)\n\t            # get latent mean and std\n\t            _, mean, std = self.vae(stacked_obs, sampled_actions)\n\t            mean = mean.reshape(self.sample_action_num, batch_size, self.latent_dim)\n", "            std = std.reshape(self.sample_action_num, batch_size, self.latent_dim)\n\t            KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean(\n\t                2)  # [sample_action_num, batch_size]\n\t            quantile = torch.quantile(KL_loss, 0.75)\n\t            qc_ood = ((KL_loss >= quantile) * qc_sampled).mean(0)\n\t        loss_cost_critic = self.cost_critic.loss(\n\t            backup, qc_list) - self.log_alpha.exp() * (qc_ood.mean() - self.qc_thres)\n\t        self.cost_critic_optim.zero_grad()\n\t        loss_cost_critic.backward()\n\t        self.cost_critic_optim.step()\n", "        # update alpha\n\t        self.log_alpha += self.alpha_lr * self.log_alpha.exp() * (\n\t            self.qc_thres - qc_ood.mean()).detach()\n\t        self.log_alpha.data.clamp_(min=-5.0, max=5.0)\n\t        stats_cost_critic = {\n\t            \"loss/cost_critic_loss\": loss_cost_critic.item(),\n\t            \"loss/alpha_value\": self.log_alpha.exp().item()\n\t        }\n\t        return loss_cost_critic, stats_cost_critic\n\t    def actor_loss(self, observations):\n", "        for p in self.critic.parameters():\n\t            p.requires_grad = False\n\t        for p in self.cost_critic.parameters():\n\t            p.requires_grad = False\n\t        actions, _ = self._actor_forward(observations, False, True)\n\t        q_pi, _ = self.critic.predict(observations, actions)\n\t        qc_pi, _ = self.cost_critic.predict(observations, actions)\n\t        loss_actor = -((qc_pi <= self.q_thres) * q_pi).mean()\n\t        self.actor_optim.zero_grad()\n\t        loss_actor.backward()\n", "        self.actor_optim.step()\n\t        stats_actor = {\"loss/actor_loss\": loss_actor.item()}\n\t        for p in self.critic.parameters():\n\t            p.requires_grad = True\n\t        for p in self.cost_critic.parameters():\n\t            p.requires_grad = True\n\t        return loss_actor, stats_actor\n\t    def sync_weight(self):\n\t        \"\"\"\n\t        Soft-update the weight for the target network.\n", "        \"\"\"\n\t        self._soft_update(self.critic_old, self.critic, self.tau)\n\t        self._soft_update(self.cost_critic_old, self.cost_critic, self.tau)\n\t        self._soft_update(self.actor_old, self.actor, self.tau)\n\t    def setup_optimizers(self, actor_lr, critic_lr, alpha_lr, vae_lr):\n\t        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n\t        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n\t        self.cost_critic_optim = torch.optim.Adam(self.cost_critic.parameters(),\n\t                                                  lr=critic_lr)\n\t        self.vae_optim = torch.optim.Adam(self.vae.parameters(), lr=vae_lr)\n", "        self.alpha_lr = alpha_lr\n\t    def act(self,\n\t            obs: np.ndarray,\n\t            deterministic: bool = False,\n\t            with_logprob: bool = False):\n\t        \"\"\"\n\t        Given a single obs, return the action, logp.\n\t        \"\"\"\n\t        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n\t        a, logp_a = self._actor_forward(obs, deterministic, with_logprob)\n", "        a = a.data.numpy() if self.device == \"cpu\" else a.data.cpu().numpy()\n\t        logp_a = logp_a.data.numpy() if self.device == \"cpu\" else logp_a.data.cpu(\n\t        ).numpy()\n\t        return np.squeeze(a, axis=0), np.squeeze(logp_a)\n\tclass CPQTrainer:\n\t    \"\"\"\n\t    Constraints Penalized Q-learning Trainer\n\t    Args:\n\t        model (CPQ): The CPQ model to be trained.\n\t        env (gym.Env): The OpenAI Gym environment to train the model in.\n", "        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n\t        actor_lr (float): learning rate for actor\n\t        critic_lr (float): learning rate for critic\n\t        alpha_lr (float): learning rate for alpha\n\t        vae_lr (float): learning rate for vae\n\t        reward_scale (float): The scaling factor for the reward signal.\n\t        cost_scale (float): The scaling factor for the constraint cost.\n\t        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n\t    \"\"\"\n\t    def __init__(\n", "            self,\n\t            model: CPQ,\n\t            env: gym.Env,\n\t            logger: WandbLogger = DummyLogger(),\n\t            # training params\n\t            actor_lr: float = 1e-4,\n\t            critic_lr: float = 1e-4,\n\t            alpha_lr: float = 1e-4,\n\t            vae_lr: float = 1e-4,\n\t            reward_scale: float = 1.0,\n", "            cost_scale: float = 1.0,\n\t            device=\"cpu\") -> None:\n\t        self.model = model\n\t        self.logger = logger\n\t        self.env = env\n\t        self.reward_scale = reward_scale\n\t        self.cost_scale = cost_scale\n\t        self.device = device\n\t        self.model.setup_optimizers(actor_lr, critic_lr, alpha_lr, vae_lr)\n\t    def train_one_step(self, observations, next_observations, actions, rewards, costs,\n", "                       done):\n\t        # update VAE\n\t        loss_vae, stats_vae = self.model.vae_loss(observations, actions)\n\t        # update critic\n\t        loss_critic, stats_critic = self.model.critic_loss(observations,\n\t                                                           next_observations, actions,\n\t                                                           rewards, done)\n\t        # update cost critic\n\t        loss_cost_critic, stats_cost_critic = self.model.cost_critic_loss(\n\t            observations, next_observations, actions, costs, done)\n", "        # update actor\n\t        loss_actor, stats_actor = self.model.actor_loss(observations)\n\t        self.model.sync_weight()\n\t        self.logger.store(**stats_vae)\n\t        self.logger.store(**stats_critic)\n\t        self.logger.store(**stats_cost_critic)\n\t        self.logger.store(**stats_actor)\n\t    def evaluate(self, eval_episodes):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a number of episodes.\n", "        \"\"\"\n\t        self.model.eval()\n\t        episode_rets, episode_costs, episode_lens = [], [], []\n\t        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n\t            epi_ret, epi_len, epi_cost = self.rollout()\n\t            episode_rets.append(epi_ret)\n\t            episode_lens.append(epi_len)\n\t            episode_costs.append(epi_cost)\n\t        self.model.train()\n\t        return np.mean(episode_rets) / self.reward_scale, np.mean(\n", "            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\t    @torch.no_grad()\n\t    def rollout(self):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a single episode.\n\t        \"\"\"\n\t        obs, info = self.env.reset()\n\t        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n\t        for _ in range(self.model.episode_len):\n\t            act, _ = self.model.act(obs, True, True)\n", "            obs_next, reward, terminated, truncated, info = self.env.step(act)\n\t            cost = info[\"cost\"] * self.cost_scale\n\t            obs = obs_next\n\t            episode_ret += reward\n\t            episode_len += 1\n\t            episode_cost += cost\n\t            if terminated or truncated:\n\t                break\n\t        return episode_ret, episode_len, episode_cost\n"]}
{"filename": "osrl/algorithms/cdt.py", "chunked_list": ["from typing import Optional, Tuple\n\timport gymnasium as gym\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom fsrl.utils import DummyLogger, WandbLogger\n\tfrom torch.distributions.beta import Beta\n\tfrom torch.nn import functional as F  # noqa\n\tfrom tqdm.auto import trange  # noqa\n\tfrom osrl.common.net import DiagGaussianActor, TransformerBlock, mlp\n", "class CDT(nn.Module):\n\t    \"\"\"\n\t    Constrained Decision Transformer (CDT)\n\t    Args:\n\t        state_dim (int): dimension of the state space.\n\t        action_dim (int): dimension of the action space.\n\t        max_action (float): Maximum action value.\n\t        seq_len (int): The length of the sequence to process.\n\t        episode_len (int): The length of the episode.\n\t        embedding_dim (int): The dimension of the embeddings.\n", "        num_layers (int): The number of transformer layers to use.\n\t        num_heads (int): The number of heads to use in the multi-head attention.\n\t        attention_dropout (float): The dropout probability for attention layers.\n\t        residual_dropout (float): The dropout probability for residual layers.\n\t        embedding_dropout (float): The dropout probability for embedding layers.\n\t        time_emb (bool): Whether to include time embeddings.\n\t        use_rew (bool): Whether to include return embeddings.\n\t        use_cost (bool): Whether to include cost embeddings.\n\t        cost_transform (bool): Whether to transform the cost values.\n\t        add_cost_feat (bool): Whether to add cost features.\n", "        mul_cost_feat (bool): Whether to multiply cost features.\n\t        cat_cost_feat (bool): Whether to concatenate cost features.\n\t        action_head_layers (int): The number of layers in the action head.\n\t        cost_prefix (bool): Whether to include a cost prefix.\n\t        stochastic (bool): Whether to use stochastic actions.\n\t        init_temperature (float): The initial temperature value for stochastic actions.\n\t        target_entropy (float): The target entropy value for stochastic actions.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        state_dim: int,\n\t        action_dim: int,\n\t        max_action: float,\n\t        seq_len: int = 10,\n\t        episode_len: int = 1000,\n\t        embedding_dim: int = 128,\n\t        num_layers: int = 4,\n\t        num_heads: int = 8,\n\t        attention_dropout: float = 0.0,\n\t        residual_dropout: float = 0.0,\n", "        embedding_dropout: float = 0.0,\n\t        time_emb: bool = True,\n\t        use_rew: bool = False,\n\t        use_cost: bool = False,\n\t        cost_transform: bool = False,\n\t        add_cost_feat: bool = False,\n\t        mul_cost_feat: bool = False,\n\t        cat_cost_feat: bool = False,\n\t        action_head_layers: int = 1,\n\t        cost_prefix: bool = False,\n", "        stochastic: bool = False,\n\t        init_temperature=0.1,\n\t        target_entropy=None,\n\t    ):\n\t        super().__init__()\n\t        self.seq_len = seq_len\n\t        self.embedding_dim = embedding_dim\n\t        self.state_dim = state_dim\n\t        self.action_dim = action_dim\n\t        self.episode_len = episode_len\n", "        self.max_action = max_action\n\t        if cost_transform:\n\t            self.cost_transform = lambda x: 50 - x\n\t        else:\n\t            self.cost_transform = None\n\t        self.add_cost_feat = add_cost_feat\n\t        self.mul_cost_feat = mul_cost_feat\n\t        self.cat_cost_feat = cat_cost_feat\n\t        self.stochastic = stochastic\n\t        self.emb_drop = nn.Dropout(embedding_dropout)\n", "        self.emb_norm = nn.LayerNorm(embedding_dim)\n\t        self.out_norm = nn.LayerNorm(embedding_dim)\n\t        # additional seq_len embeddings for padding timesteps\n\t        self.time_emb = time_emb\n\t        if self.time_emb:\n\t            self.timestep_emb = nn.Embedding(episode_len + seq_len, embedding_dim)\n\t        self.state_emb = nn.Linear(state_dim, embedding_dim)\n\t        self.action_emb = nn.Linear(action_dim, embedding_dim)\n\t        self.seq_repeat = 2\n\t        self.use_rew = use_rew\n", "        self.use_cost = use_cost\n\t        if self.use_cost:\n\t            self.cost_emb = nn.Linear(1, embedding_dim)\n\t            self.seq_repeat += 1\n\t        if self.use_rew:\n\t            self.return_emb = nn.Linear(1, embedding_dim)\n\t            self.seq_repeat += 1\n\t        dt_seq_len = self.seq_repeat * seq_len\n\t        self.cost_prefix = cost_prefix\n\t        if self.cost_prefix:\n", "            self.prefix_emb = nn.Linear(1, embedding_dim)\n\t            dt_seq_len += 1\n\t        self.blocks = nn.ModuleList([\n\t            TransformerBlock(\n\t                seq_len=dt_seq_len,\n\t                embedding_dim=embedding_dim,\n\t                num_heads=num_heads,\n\t                attention_dropout=attention_dropout,\n\t                residual_dropout=residual_dropout,\n\t            ) for _ in range(num_layers)\n", "        ])\n\t        action_emb_dim = 2 * embedding_dim if self.cat_cost_feat else embedding_dim\n\t        if self.stochastic:\n\t            if action_head_layers >= 2:\n\t                self.action_head = nn.Sequential(\n\t                    nn.Linear(action_emb_dim, action_emb_dim), nn.GELU(),\n\t                    DiagGaussianActor(action_emb_dim, action_dim))\n\t            else:\n\t                self.action_head = DiagGaussianActor(action_emb_dim, action_dim)\n\t        else:\n", "            self.action_head = mlp([action_emb_dim] * action_head_layers + [action_dim],\n\t                                   activation=nn.GELU,\n\t                                   output_activation=nn.Identity)\n\t        self.state_pred_head = nn.Linear(embedding_dim, state_dim)\n\t        # a classification problem\n\t        self.cost_pred_head = nn.Linear(embedding_dim, 2)\n\t        if self.stochastic:\n\t            self.log_temperature = torch.tensor(np.log(init_temperature))\n\t            self.log_temperature.requires_grad = True\n\t            self.target_entropy = target_entropy\n", "        self.apply(self._init_weights)\n\t    def temperature(self):\n\t        if self.stochastic:\n\t            return self.log_temperature.exp()\n\t        else:\n\t            return None\n\t    @staticmethod\n\t    def _init_weights(module: nn.Module):\n\t        if isinstance(module, (nn.Linear, nn.Embedding)):\n\t            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n", "            if isinstance(module, nn.Linear) and module.bias is not None:\n\t                torch.nn.init.zeros_(module.bias)\n\t        elif isinstance(module, nn.LayerNorm):\n\t            torch.nn.init.zeros_(module.bias)\n\t            torch.nn.init.ones_(module.weight)\n\t    def forward(\n\t            self,\n\t            states: torch.Tensor,  # [batch_size, seq_len, state_dim]\n\t            actions: torch.Tensor,  # [batch_size, seq_len, action_dim]\n\t            returns_to_go: torch.Tensor,  # [batch_size, seq_len]\n", "            costs_to_go: torch.Tensor,  # [batch_size, seq_len]\n\t            time_steps: torch.Tensor,  # [batch_size, seq_len]\n\t            padding_mask: Optional[torch.Tensor] = None,  # [batch_size, seq_len]\n\t            episode_cost: torch.Tensor = None,  # [batch_size, ]\n\t    ) -> torch.FloatTensor:\n\t        batch_size, seq_len = states.shape[0], states.shape[1]\n\t        # [batch_size, seq_len, emb_dim]\n\t        if self.time_emb:\n\t            timestep_emb = self.timestep_emb(time_steps)\n\t        else:\n", "            timestep_emb = 0.0\n\t        state_emb = self.state_emb(states) + timestep_emb\n\t        act_emb = self.action_emb(actions) + timestep_emb\n\t        seq_list = [state_emb, act_emb]\n\t        if self.cost_transform is not None:\n\t            costs_to_go = self.cost_transform(costs_to_go.detach())\n\t        if self.use_cost:\n\t            costs_emb = self.cost_emb(costs_to_go.unsqueeze(-1)) + timestep_emb\n\t            seq_list.insert(0, costs_emb)\n\t        if self.use_rew:\n", "            returns_emb = self.return_emb(returns_to_go.unsqueeze(-1)) + timestep_emb\n\t            seq_list.insert(0, returns_emb)\n\t        # [batch_size, seq_len, 2-4, emb_dim], (c_0 s_0, a_0, c_1, s_1, a_1, ...)\n\t        sequence = torch.stack(seq_list, dim=1).permute(0, 2, 1, 3)\n\t        sequence = sequence.reshape(batch_size, self.seq_repeat * seq_len,\n\t                                    self.embedding_dim)\n\t        if padding_mask is not None:\n\t            # [batch_size, seq_len * self.seq_repeat], stack mask identically to fit the sequence\n\t            padding_mask = torch.stack([padding_mask] * self.seq_repeat,\n\t                                       dim=1).permute(0, 2, 1).reshape(batch_size, -1)\n", "        if self.cost_prefix:\n\t            episode_cost = episode_cost.unsqueeze(-1).unsqueeze(-1)\n\t            episode_cost = episode_cost.to(states.dtype)\n\t            # [batch, 1, emb_dim]\n\t            episode_cost_emb = self.prefix_emb(episode_cost)\n\t            # [batch, 1+seq_len * self.seq_repeat, emb_dim]\n\t            sequence = torch.cat([episode_cost_emb, sequence], dim=1)\n\t            if padding_mask is not None:\n\t                # [batch_size, 1+ seq_len * self.seq_repeat]\n\t                padding_mask = torch.cat([padding_mask[:, :1], padding_mask], dim=1)\n", "        # LayerNorm and Dropout (!!!) as in original implementation,\n\t        # while minGPT & huggingface uses only embedding dropout\n\t        out = self.emb_norm(sequence)\n\t        out = self.emb_drop(out)\n\t        for block in self.blocks:\n\t            out = block(out, padding_mask=padding_mask)\n\t        # [batch_size, seq_len * self.seq_repeat, embedding_dim]\n\t        out = self.out_norm(out)\n\t        if self.cost_prefix:\n\t            # [batch_size, seq_len * seq_repeat, embedding_dim]\n", "            out = out[:, 1:]\n\t        # [batch_size, seq_len, self.seq_repeat, embedding_dim]\n\t        out = out.reshape(batch_size, seq_len, self.seq_repeat, self.embedding_dim)\n\t        # [batch_size, self.seq_repeat, seq_len, embedding_dim]\n\t        out = out.permute(0, 2, 1, 3)\n\t        # [batch_size, seq_len, embedding_dim]\n\t        action_feature = out[:, self.seq_repeat - 1]\n\t        state_feat = out[:, self.seq_repeat - 2]\n\t        if self.add_cost_feat and self.use_cost:\n\t            state_feat = state_feat + costs_emb.detach()\n", "        if self.mul_cost_feat and self.use_cost:\n\t            state_feat = state_feat * costs_emb.detach()\n\t        if self.cat_cost_feat and self.use_cost:\n\t            # cost_prefix feature, deprecated\n\t            # episode_cost_emb = episode_cost_emb.repeat_interleave(seq_len, dim=1)\n\t            # [batch_size, seq_len, 2 * embedding_dim]\n\t            state_feat = torch.cat([state_feat, costs_emb.detach()], dim=2)\n\t        # get predictions\n\t        action_preds = self.action_head(\n\t            state_feat\n", "        )  # predict next action given state, [batch_size, seq_len, action_dim]\n\t        # [batch_size, seq_len, 2]\n\t        cost_preds = self.cost_pred_head(\n\t            action_feature)  # predict next cost return given state and action\n\t        cost_preds = F.log_softmax(cost_preds, dim=-1)\n\t        state_preds = self.state_pred_head(\n\t            action_feature)  # predict next state given state and action\n\t        return action_preds, cost_preds, state_preds\n\tclass CDTTrainer:\n\t    \"\"\"\n", "    Constrained Decision Transformer Trainer\n\t    Args:\n\t        model (CDT): A CDT model to train.\n\t        env (gym.Env): The OpenAI Gym environment to train the model in.\n\t        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n\t        learning_rate (float): The learning rate for the optimizer.\n\t        weight_decay (float): The weight decay for the optimizer.\n\t        betas (Tuple[float, ...]): The betas for the optimizer.\n\t        clip_grad (float): The clip gradient value.\n\t        lr_warmup_steps (int): The number of warmup steps for the learning rate scheduler.\n", "        reward_scale (float): The scaling factor for the reward signal.\n\t        cost_scale (float): The scaling factor for the constraint cost.\n\t        loss_cost_weight (float): The weight for the cost loss.\n\t        loss_state_weight (float): The weight for the state loss.\n\t        cost_reverse (bool): Whether to reverse the cost.\n\t        no_entropy (bool): Whether to use entropy.\n\t        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n", "            model: CDT,\n\t            env: gym.Env,\n\t            logger: WandbLogger = DummyLogger(),\n\t            # training params\n\t            learning_rate: float = 1e-4,\n\t            weight_decay: float = 1e-4,\n\t            betas: Tuple[float, ...] = (0.9, 0.999),\n\t            clip_grad: float = 0.25,\n\t            lr_warmup_steps: int = 10000,\n\t            reward_scale: float = 1.0,\n", "            cost_scale: float = 1.0,\n\t            loss_cost_weight: float = 0.0,\n\t            loss_state_weight: float = 0.0,\n\t            cost_reverse: bool = False,\n\t            no_entropy: bool = False,\n\t            device=\"cpu\") -> None:\n\t        self.model = model\n\t        self.logger = logger\n\t        self.env = env\n\t        self.clip_grad = clip_grad\n", "        self.reward_scale = reward_scale\n\t        self.cost_scale = cost_scale\n\t        self.device = device\n\t        self.cost_weight = loss_cost_weight\n\t        self.state_weight = loss_state_weight\n\t        self.cost_reverse = cost_reverse\n\t        self.no_entropy = no_entropy\n\t        self.optim = torch.optim.AdamW(\n\t            self.model.parameters(),\n\t            lr=learning_rate,\n", "            weight_decay=weight_decay,\n\t            betas=betas,\n\t        )\n\t        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n\t            self.optim,\n\t            lambda steps: min((steps + 1) / lr_warmup_steps, 1),\n\t        )\n\t        self.stochastic = self.model.stochastic\n\t        if self.stochastic:\n\t            self.log_temperature_optimizer = torch.optim.Adam(\n", "                [self.model.log_temperature],\n\t                lr=1e-4,\n\t                betas=[0.9, 0.999],\n\t            )\n\t        self.max_action = self.model.max_action\n\t        self.beta_dist = Beta(torch.tensor(2, dtype=torch.float, device=self.device),\n\t                              torch.tensor(5, dtype=torch.float, device=self.device))\n\t    def train_one_step(self, states, actions, returns, costs_return, time_steps, mask,\n\t                       episode_cost, costs):\n\t        # True value indicates that the corresponding key value will be ignored\n", "        padding_mask = ~mask.to(torch.bool)\n\t        action_preds, cost_preds, state_preds = self.model(\n\t            states=states,\n\t            actions=actions,\n\t            returns_to_go=returns,\n\t            costs_to_go=costs_return,\n\t            time_steps=time_steps,\n\t            padding_mask=padding_mask,\n\t            episode_cost=episode_cost,\n\t        )\n", "        if self.stochastic:\n\t            log_likelihood = action_preds.log_prob(actions)[mask > 0].mean()\n\t            entropy = action_preds.entropy()[mask > 0].mean()\n\t            entropy_reg = self.model.temperature().detach()\n\t            entropy_reg_item = entropy_reg.item()\n\t            if self.no_entropy:\n\t                entropy_reg = 0.0\n\t                entropy_reg_item = 0.0\n\t            act_loss = -(log_likelihood + entropy_reg * entropy)\n\t            self.logger.store(tab=\"train\",\n", "                              nll=-log_likelihood.item(),\n\t                              ent=entropy.item(),\n\t                              ent_reg=entropy_reg_item)\n\t        else:\n\t            act_loss = F.mse_loss(action_preds, actions.detach(), reduction=\"none\")\n\t            # [batch_size, seq_len, action_dim] * [batch_size, seq_len, 1]\n\t            act_loss = (act_loss * mask.unsqueeze(-1)).mean()\n\t        # cost_preds: [batch_size * seq_len, 2], costs: [batch_size * seq_len]\n\t        cost_preds = cost_preds.reshape(-1, 2)\n\t        costs = costs.flatten().long().detach()\n", "        cost_loss = F.nll_loss(cost_preds, costs, reduction=\"none\")\n\t        # cost_loss = F.mse_loss(cost_preds, costs.detach(), reduction=\"none\")\n\t        cost_loss = (cost_loss * mask.flatten()).mean()\n\t        # compute the accuracy, 0 value, 1 indice, [batch_size, seq_len]\n\t        pred = cost_preds.data.max(dim=1)[1]\n\t        correct = pred.eq(costs.data.view_as(pred)) * mask.flatten()\n\t        correct = correct.sum()\n\t        total_num = mask.sum()\n\t        acc = correct / total_num\n\t        # [batch_size, seq_len, state_dim]\n", "        state_loss = F.mse_loss(state_preds[:, :-1],\n\t                                states[:, 1:].detach(),\n\t                                reduction=\"none\")\n\t        state_loss = (state_loss * mask[:, :-1].unsqueeze(-1)).mean()\n\t        loss = act_loss + self.cost_weight * cost_loss + self.state_weight * state_loss\n\t        self.optim.zero_grad()\n\t        loss.backward()\n\t        if self.clip_grad is not None:\n\t            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n\t        self.optim.step()\n", "        if self.stochastic:\n\t            self.log_temperature_optimizer.zero_grad()\n\t            temperature_loss = (self.model.temperature() *\n\t                                (entropy - self.model.target_entropy).detach())\n\t            temperature_loss.backward()\n\t            self.log_temperature_optimizer.step()\n\t        self.scheduler.step()\n\t        self.logger.store(\n\t            tab=\"train\",\n\t            all_loss=loss.item(),\n", "            act_loss=act_loss.item(),\n\t            cost_loss=cost_loss.item(),\n\t            cost_acc=acc.item(),\n\t            state_loss=state_loss.item(),\n\t            train_lr=self.scheduler.get_last_lr()[0],\n\t        )\n\t    def evaluate(self, num_rollouts, target_return, target_cost):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a number of episodes.\n\t        \"\"\"\n", "        self.model.eval()\n\t        episode_rets, episode_costs, episode_lens = [], [], []\n\t        for _ in trange(num_rollouts, desc=\"Evaluating...\", leave=False):\n\t            epi_ret, epi_len, epi_cost = self.rollout(self.model, self.env,\n\t                                                      target_return, target_cost)\n\t            episode_rets.append(epi_ret)\n\t            episode_lens.append(epi_len)\n\t            episode_costs.append(epi_cost)\n\t        self.model.train()\n\t        return np.mean(episode_rets) / self.reward_scale, np.mean(\n", "            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\t    @torch.no_grad()\n\t    def rollout(\n\t        self,\n\t        model: CDT,\n\t        env: gym.Env,\n\t        target_return: float,\n\t        target_cost: float,\n\t    ) -> Tuple[float, float]:\n\t        \"\"\"\n", "        Evaluates the performance of the model on a single episode.\n\t        \"\"\"\n\t        states = torch.zeros(1,\n\t                             model.episode_len + 1,\n\t                             model.state_dim,\n\t                             dtype=torch.float,\n\t                             device=self.device)\n\t        actions = torch.zeros(1,\n\t                              model.episode_len,\n\t                              model.action_dim,\n", "                              dtype=torch.float,\n\t                              device=self.device)\n\t        returns = torch.zeros(1,\n\t                              model.episode_len + 1,\n\t                              dtype=torch.float,\n\t                              device=self.device)\n\t        costs = torch.zeros(1,\n\t                            model.episode_len + 1,\n\t                            dtype=torch.float,\n\t                            device=self.device)\n", "        time_steps = torch.arange(model.episode_len,\n\t                                  dtype=torch.long,\n\t                                  device=self.device)\n\t        time_steps = time_steps.view(1, -1)\n\t        obs, info = env.reset()\n\t        states[:, 0] = torch.as_tensor(obs, device=self.device)\n\t        returns[:, 0] = torch.as_tensor(target_return, device=self.device)\n\t        costs[:, 0] = torch.as_tensor(target_cost, device=self.device)\n\t        epi_cost = torch.tensor(np.array([target_cost]),\n\t                                dtype=torch.float,\n", "                                device=self.device)\n\t        # cannot step higher than model episode len, as timestep embeddings will crash\n\t        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n\t        for step in range(model.episode_len):\n\t            # first select history up to step, then select last seq_len states,\n\t            # step + 1 as : operator is not inclusive, last action is dummy with zeros\n\t            # (as model will predict last, actual last values are not important) # fix this noqa!!!\n\t            s = states[:, :step + 1][:, -model.seq_len:]  # noqa\n\t            a = actions[:, :step + 1][:, -model.seq_len:]  # noqa\n\t            r = returns[:, :step + 1][:, -model.seq_len:]  # noqa\n", "            c = costs[:, :step + 1][:, -model.seq_len:]  # noqa\n\t            t = time_steps[:, :step + 1][:, -model.seq_len:]  # noqa\n\t            acts, _, _ = model(s, a, r, c, t, None, epi_cost)\n\t            if self.stochastic:\n\t                acts = acts.mean\n\t            acts = acts.clamp(-self.max_action, self.max_action)\n\t            act = acts[0, -1].cpu().numpy()\n\t            # act = self.get_ensemble_action(1, model, s, a, r, c, t, epi_cost)\n\t            obs_next, reward, terminated, truncated, info = env.step(act)\n\t            if self.cost_reverse:\n", "                cost = (1.0 - info[\"cost\"]) * self.cost_scale\n\t            else:\n\t                cost = info[\"cost\"] * self.cost_scale\n\t            # at step t, we predict a_t, get s_{t + 1}, r_{t + 1}\n\t            actions[:, step] = torch.as_tensor(act)\n\t            states[:, step + 1] = torch.as_tensor(obs_next)\n\t            returns[:, step + 1] = torch.as_tensor(returns[:, step] - reward)\n\t            costs[:, step + 1] = torch.as_tensor(costs[:, step] - cost)\n\t            obs = obs_next\n\t            episode_ret += reward\n", "            episode_len += 1\n\t            episode_cost += info[\"cost\"]\n\t            if terminated or truncated:\n\t                break\n\t        return episode_ret, episode_len, episode_cost\n\t    def get_ensemble_action(self, size: int, model, s, a, r, c, t, epi_cost):\n\t        # [size, seq_len, state_dim]\n\t        s = torch.repeat_interleave(s, size, 0)\n\t        # [size, seq_len, act_dim]\n\t        a = torch.repeat_interleave(a, size, 0)\n", "        # [size, seq_len]\n\t        r = torch.repeat_interleave(r, size, 0)\n\t        c = torch.repeat_interleave(c, size, 0)\n\t        t = torch.repeat_interleave(t, size, 0)\n\t        epi_cost = torch.repeat_interleave(epi_cost, size, 0)\n\t        acts, _, _ = model(s, a, r, c, t, None, epi_cost)\n\t        if self.stochastic:\n\t            acts = acts.mean\n\t        # [size, seq_len, act_dim]\n\t        acts = torch.mean(acts, dim=0, keepdim=True)\n", "        acts = acts.clamp(-self.max_action, self.max_action)\n\t        act = acts[0, -1].cpu().numpy()\n\t        return act\n\t    def collect_random_rollouts(self, num_rollouts):\n\t        episode_rets = []\n\t        for _ in range(num_rollouts):\n\t            obs, info = self.env.reset()\n\t            episode_ret = 0.0\n\t            for step in range(self.model.episode_len):\n\t                act = self.env.action_space.sample()\n", "                obs_next, reward, terminated, truncated, info = self.env.step(act)\n\t                obs = obs_next\n\t                episode_ret += reward\n\t                if terminated or truncated:\n\t                    break\n\t            episode_rets.append(episode_ret)\n\t        return np.mean(episode_rets)\n"]}
{"filename": "osrl/algorithms/bc.py", "chunked_list": ["import gymnasium as gym\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom fsrl.utils import DummyLogger, WandbLogger\n\tfrom tqdm.auto import trange  # noqa\n\tfrom osrl.common.net import MLPActor\n\tclass BC(nn.Module):\n\t    \"\"\"\n", "    Behavior Cloning (BC)\n\t    Args:\n\t        state_dim (int): dimension of the state space.\n\t        action_dim (int): dimension of the action space.\n\t        max_action (float): Maximum action value.\n\t        a_hidden_sizes (list, optional): List of integers specifying the sizes \n\t            of the layers in the actor network.\n\t        episode_len (int, optional): Maximum length of an episode.\n\t        device (str, optional): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n\t    \"\"\"\n", "    def __init__(self,\n\t                 state_dim: int,\n\t                 action_dim: int,\n\t                 max_action: float,\n\t                 a_hidden_sizes: list = [128, 128],\n\t                 episode_len: int = 300,\n\t                 device: str = \"cpu\"):\n\t        super().__init__()\n\t        self.state_dim = state_dim\n\t        self.action_dim = action_dim\n", "        self.max_action = max_action\n\t        self.a_hidden_sizes = a_hidden_sizes\n\t        self.episode_len = episode_len\n\t        self.device = device\n\t        self.actor = MLPActor(self.state_dim, self.action_dim, self.a_hidden_sizes,\n\t                              nn.ReLU, self.max_action).to(self.device)\n\t    def actor_loss(self, observations, actions):\n\t        pred_actions = self.actor(observations)\n\t        loss_actor = F.mse_loss(pred_actions, actions)\n\t        self.actor_optim.zero_grad()\n", "        loss_actor.backward()\n\t        self.actor_optim.step()\n\t        stats_actor = {\"loss/actor_loss\": loss_actor.item()}\n\t        return loss_actor, stats_actor\n\t    def setup_optimizers(self, actor_lr):\n\t        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n\t    def act(self, obs):\n\t        '''\n\t        Given a single obs, return the action.\n\t        '''\n", "        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n\t        act = self.actor(obs)\n\t        act = act.data.numpy() if self.device == \"cpu\" else act.data.cpu().numpy()\n\t        return np.squeeze(act, axis=0)\n\tclass BCTrainer:\n\t    \"\"\"\n\t    Behavior Cloning Trainer\n\t    Args:\n\t        model (BC): The BC model to be trained.\n\t        env (gym.Env): The OpenAI Gym environment to train the model in.\n", "        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n\t        actor_lr (float): learning rate for actor\n\t        bc_mode (str): specify bc mode\n\t        cost_limit (int): Upper limit on the cost per episode.\n\t        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n\t    \"\"\"\n\t    def __init__(\n\t            self,\n\t            model: BC,\n\t            env: gym.Env,\n", "            logger: WandbLogger = DummyLogger(),\n\t            # training params\n\t            actor_lr: float = 1e-4,\n\t            bc_mode: str = \"all\",\n\t            cost_limit: int = 10,\n\t            device=\"cpu\"):\n\t        self.model = model\n\t        self.logger = logger\n\t        self.env = env\n\t        self.device = device\n", "        self.bc_mode = bc_mode\n\t        self.cost_limit = cost_limit\n\t        self.model.setup_optimizers(actor_lr)\n\t    def set_target_cost(self, target_cost):\n\t        self.cost_limit = target_cost\n\t    def train_one_step(self, observations, actions):\n\t        \"\"\"\n\t        Trains the model by updating the actor.\n\t        \"\"\"\n\t        # update actor\n", "        loss_actor, stats_actor = self.model.actor_loss(observations, actions)\n\t        self.logger.store(**stats_actor)\n\t    def evaluate(self, eval_episodes):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a number of episodes.\n\t        \"\"\"\n\t        self.model.eval()\n\t        episode_rets, episode_costs, episode_lens = [], [], []\n\t        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n\t            epi_ret, epi_len, epi_cost = self.rollout()\n", "            episode_rets.append(epi_ret)\n\t            episode_lens.append(epi_len)\n\t            episode_costs.append(epi_cost)\n\t        self.model.train()\n\t        return np.mean(episode_rets), np.mean(episode_costs), np.mean(episode_lens)\n\t    @torch.no_grad()\n\t    def rollout(self):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a single episode.\n\t        \"\"\"\n", "        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n\t        obs, info = self.env.reset()\n\t        if self.bc_mode == \"multi-task\":\n\t            obs = np.append(obs, self.cost_limit)\n\t        for _ in range(self.model.episode_len):\n\t            act = self.model.act(obs)\n\t            obs_next, reward, terminated, truncated, info = self.env.step(act)\n\t            if self.bc_mode == \"multi-task\":\n\t                obs_next = np.append(obs_next, self.cost_limit)\n\t            obs = obs_next\n", "            episode_ret += reward\n\t            episode_len += 1\n\t            episode_cost += info[\"cost\"]\n\t            if terminated or truncated:\n\t                break\n\t        return episode_ret, episode_len, episode_cost\n"]}
{"filename": "osrl/algorithms/bearl.py", "chunked_list": ["# reference: https://github.com/aviralkumar2907/BEAR\n\tfrom copy import deepcopy\n\timport gymnasium as gym\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom fsrl.utils import DummyLogger, WandbLogger\n\tfrom tqdm.auto import trange  # noqa\n\tfrom osrl.common.net import (VAE, EnsembleDoubleQCritic, LagrangianPIDController,\n\t                             SquashedGaussianMLPActor)\n", "class BEARL(nn.Module):\n\t    \"\"\"\n\t    Bootstrapping Error Accumulation Reduction with PID Lagrangian (BEARL)\n\t    Args:\n\t        state_dim (int): dimension of the state space.\n\t        action_dim (int): dimension of the action space.\n\t        max_action (float): Maximum action value.\n\t        a_hidden_sizes (list): List of integers specifying the sizes \n\t            of the layers in the actor network.\n\t        c_hidden_sizes (list): List of integers specifying the sizes \n", "            of the layers in the critic network.\n\t        vae_hidden_sizes (int): Number of hidden units in the VAE. \n\t            sample_action_num (int): Number of action samples to draw. \n\t        gamma (float): Discount factor for the reward.\n\t        tau (float): Soft update coefficient for the target networks. \n\t        beta (float): Weight of the KL divergence term.            \n\t        lmbda (float): Weight of the Lagrangian term.\n\t        mmd_sigma (float): Width parameter for the Gaussian kernel used in the MMD loss.\n\t        target_mmd_thresh (float): Target threshold for the MMD loss.\n\t        num_samples_mmd_match (int): Number of samples to use in the MMD loss calculation.\n", "        PID (list): List of three floats containing the coefficients of the PID controller.\n\t        kernel (str): Kernel function to use in the MMD loss calculation.\n\t        num_q (int): Number of Q networks in the ensemble.\n\t        num_qc (int): Number of cost Q networks in the ensemble.\n\t        cost_limit (int): Upper limit on the cost per episode.\n\t        episode_len (int): Maximum length of an episode.\n\t        start_update_policy_step (int): Number of steps to wait before updating the policy.\n\t        device (str): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n\t    \"\"\"\n\t    def __init__(self,\n", "                 state_dim: int,\n\t                 action_dim: int,\n\t                 max_action: float,\n\t                 a_hidden_sizes: list = [128, 128],\n\t                 c_hidden_sizes: list = [128, 128],\n\t                 vae_hidden_sizes: int = 64,\n\t                 sample_action_num: int = 10,\n\t                 gamma: float = 0.99,\n\t                 tau: float = 0.005,\n\t                 beta: float = 0.5,\n", "                 lmbda: float = 0.75,\n\t                 mmd_sigma: float = 50,\n\t                 target_mmd_thresh: float = 0.05,\n\t                 num_samples_mmd_match: int = 10,\n\t                 PID: list = [0.1, 0.003, 0.001],\n\t                 kernel: str = \"gaussian\",\n\t                 num_q: int = 1,\n\t                 num_qc: int = 1,\n\t                 cost_limit: int = 10,\n\t                 episode_len: int = 300,\n", "                 start_update_policy_step: int = 20_000,\n\t                 device: str = \"cpu\"):\n\t        super().__init__()\n\t        self.state_dim = state_dim\n\t        self.action_dim = action_dim\n\t        self.latent_dim = self.action_dim * 2\n\t        self.max_action = max_action\n\t        self.a_hidden_sizes = a_hidden_sizes\n\t        self.c_hidden_sizes = c_hidden_sizes\n\t        self.vae_hidden_sizes = vae_hidden_sizes\n", "        self.sample_action_num = sample_action_num\n\t        self.gamma = gamma\n\t        self.tau = tau\n\t        self.beta = beta\n\t        self.lmbda = lmbda\n\t        self.mmd_sigma = mmd_sigma\n\t        self.target_mmd_thresh = target_mmd_thresh\n\t        self.num_samples_mmd_match = num_samples_mmd_match\n\t        self.start_update_policy_step = start_update_policy_step\n\t        self.KP, self.KI, self.KD = PID\n", "        self.kernel = kernel\n\t        self.num_q = num_q\n\t        self.num_qc = num_qc\n\t        self.cost_limit = cost_limit\n\t        self.episode_len = episode_len\n\t        self.device = device\n\t        self.n_train_steps = 0\n\t        ################ create actor critic model ###############\n\t        self.actor = SquashedGaussianMLPActor(self.state_dim, self.action_dim,\n\t                                              self.a_hidden_sizes,\n", "                                              nn.ReLU).to(self.device)\n\t        self.critic = EnsembleDoubleQCritic(self.state_dim,\n\t                                            self.action_dim,\n\t                                            self.c_hidden_sizes,\n\t                                            nn.ReLU,\n\t                                            num_q=self.num_q).to(self.device)\n\t        self.cost_critic = EnsembleDoubleQCritic(self.state_dim,\n\t                                                 self.action_dim,\n\t                                                 self.c_hidden_sizes,\n\t                                                 nn.ReLU,\n", "                                                 num_q=self.num_qc).to(self.device)\n\t        self.vae = VAE(self.state_dim, self.action_dim, self.vae_hidden_sizes,\n\t                       self.latent_dim, self.max_action, self.device).to(self.device)\n\t        self.log_alpha = torch.tensor(0.0, device=self.device)\n\t        self.actor_old = deepcopy(self.actor)\n\t        self.actor_old.eval()\n\t        self.critic_old = deepcopy(self.critic)\n\t        self.critic_old.eval()\n\t        self.cost_critic_old = deepcopy(self.cost_critic)\n\t        self.cost_critic_old.eval()\n", "        self.qc_thres = cost_limit * (1 - self.gamma**self.episode_len) / (\n\t            1 - self.gamma) / self.episode_len\n\t        self.controller = LagrangianPIDController(self.KP, self.KI, self.KD,\n\t                                                  self.qc_thres)\n\t    def _soft_update(self, tgt: nn.Module, src: nn.Module, tau: float) -> None:\n\t        \"\"\"\n\t        Softly update the parameters of target module towards the parameters\n\t        of source module.\n\t        \"\"\"\n\t        for tgt_param, src_param in zip(tgt.parameters(), src.parameters()):\n", "            tgt_param.data.copy_(tau * src_param.data + (1 - tau) * tgt_param.data)\n\t    def _actor_forward(self,\n\t                       obs: torch.tensor,\n\t                       deterministic: bool = False,\n\t                       with_logprob: bool = True):\n\t        \"\"\"\n\t        Return action distribution and action log prob [optional].\n\t        \"\"\"\n\t        a, logp = self.actor(obs, deterministic, with_logprob)\n\t        return a * self.max_action, logp\n", "    def vae_loss(self, observations, actions):\n\t        recon, mean, std = self.vae(observations, actions)\n\t        recon_loss = nn.functional.mse_loss(recon, actions)\n\t        KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean()\n\t        loss_vae = recon_loss + self.beta * KL_loss\n\t        self.vae_optim.zero_grad()\n\t        loss_vae.backward()\n\t        self.vae_optim.step()\n\t        stats_vae = {\"loss/loss_vae\": loss_vae.item()}\n\t        return loss_vae, stats_vae\n", "    def critic_loss(self, observations, next_observations, actions, rewards, done):\n\t        _, _, q1_list, q2_list = self.critic.predict(observations, actions)\n\t        with torch.no_grad():\n\t            batch_size = next_observations.shape[0]\n\t            obs_next = torch.repeat_interleave(next_observations, self.sample_action_num,\n\t                                               0).to(self.device)\n\t            act_targ_next, _ = self.actor_old(obs_next, False, True, False)\n\t            q1_targ, q2_targ, _, _ = self.critic_old.predict(obs_next, act_targ_next)\n\t            q_targ = self.lmbda * torch.min(\n\t                q1_targ, q2_targ) + (1. - self.lmbda) * torch.max(q1_targ, q2_targ)\n", "            q_targ = q_targ.reshape(batch_size, -1).max(1)[0]\n\t            backup = rewards + self.gamma * (1 - done) * q_targ\n\t        loss_critic = self.critic.loss(backup, q1_list) + self.critic.loss(\n\t            backup, q2_list)\n\t        self.critic_optim.zero_grad()\n\t        loss_critic.backward()\n\t        self.critic_optim.step()\n\t        stats_critic = {\"loss/critic_loss\": loss_critic.item()}\n\t        return loss_critic, stats_critic\n\t    def cost_critic_loss(self, observations, next_observations, actions, costs, done):\n", "        _, _, qc1_list, qc2_list = self.cost_critic.predict(observations, actions)\n\t        with torch.no_grad():\n\t            batch_size = next_observations.shape[0]\n\t            obs_next = torch.repeat_interleave(next_observations, self.sample_action_num,\n\t                                               0).to(self.device)\n\t            act_targ_next, _ = self.actor_old(obs_next, False, True, False)\n\t            qc1_targ, qc2_targ, _, _ = self.cost_critic_old.predict(\n\t                obs_next, act_targ_next)\n\t            qc_targ = self.lmbda * torch.min(\n\t                qc1_targ, qc2_targ) + (1. - self.lmbda) * torch.max(qc1_targ, qc2_targ)\n", "            qc_targ = qc_targ.reshape(batch_size, -1).max(1)[0]\n\t            backup = costs + self.gamma * qc_targ\n\t        loss_cost_critic = self.cost_critic.loss(\n\t            backup, qc1_list) + self.cost_critic.loss(backup, qc2_list)\n\t        self.cost_critic_optim.zero_grad()\n\t        loss_cost_critic.backward()\n\t        self.cost_critic_optim.step()\n\t        stats_cost_critic = {\"loss/cost_critic_loss\": loss_cost_critic.item()}\n\t        return loss_cost_critic, stats_cost_critic\n\t    def actor_loss(self, observations):\n", "        for p in self.critic.parameters():\n\t            p.requires_grad = False\n\t        for p in self.cost_critic.parameters():\n\t            p.requires_grad = False\n\t        for p in self.vae.parameters():\n\t            p.requires_grad = False\n\t        _, raw_sampled_actions = self.vae.decode_multiple(\n\t            observations, num_decode=self.num_samples_mmd_match)\n\t        batch_size = observations.shape[0]\n\t        stacked_obs = torch.repeat_interleave(\n", "            observations, self.num_samples_mmd_match,\n\t            0)  # [batch_size*num_samples_mmd_match, obs_dim]\n\t        actor_samples, raw_actor_actions = self.actor(stacked_obs,\n\t                                                      return_pretanh_value=True)\n\t        actor_samples = actor_samples.reshape(batch_size, self.num_samples_mmd_match,\n\t                                              self.action_dim)\n\t        raw_actor_actions = raw_actor_actions.view(batch_size,\n\t                                                   self.num_samples_mmd_match,\n\t                                                   self.action_dim)\n\t        if self.kernel == 'laplacian':\n", "            mmd_loss = self.mmd_loss_laplacian(raw_sampled_actions,\n\t                                               raw_actor_actions,\n\t                                               sigma=self.mmd_sigma)\n\t        elif self.kernel == 'gaussian':\n\t            mmd_loss = self.mmd_loss_gaussian(raw_sampled_actions,\n\t                                              raw_actor_actions,\n\t                                              sigma=self.mmd_sigma)\n\t        q_val1, q_val2, _, _ = self.critic.predict(observations, actor_samples[:, 0, :])\n\t        qc_val1, qc_val2, _, _ = self.cost_critic.predict(observations,\n\t                                                          actor_samples[:, 0, :])\n", "        qc_val = torch.min(qc_val1, qc_val2)\n\t        with torch.no_grad():\n\t            multiplier = self.controller.control(qc_val).detach()\n\t        qc_penalty = ((qc_val - self.qc_thres) * multiplier).mean()\n\t        q_val = torch.min(q_val1, q_val2)\n\t        if self.n_train_steps >= self.start_update_policy_step:\n\t            loss_actor = (-q_val + self.log_alpha.exp() *\n\t                          (mmd_loss - self.target_mmd_thresh)).mean()\n\t        else:\n\t            loss_actor = (self.log_alpha.exp() *\n", "                          (mmd_loss - self.target_mmd_thresh)).mean()\n\t        loss_actor += qc_penalty\n\t        self.actor_optim.zero_grad()\n\t        loss_actor.backward()\n\t        self.actor_optim.step()\n\t        self.log_alpha += self.alpha_lr * self.log_alpha.exp() * (\n\t            mmd_loss - self.target_mmd_thresh).mean().detach()\n\t        self.log_alpha.data.clamp_(min=-5.0, max=5.0)\n\t        self.n_train_steps += 1\n\t        stats_actor = {\n", "            \"loss/actor_loss\": loss_actor.item(),\n\t            \"loss/mmd_loss\": mmd_loss.mean().item(),\n\t            \"loss/qc_penalty\": qc_penalty.item(),\n\t            \"loss/lagrangian\": multiplier.item(),\n\t            \"loss/alpha_value\": self.log_alpha.exp().item()\n\t        }\n\t        for p in self.critic.parameters():\n\t            p.requires_grad = True\n\t        for p in self.cost_critic.parameters():\n\t            p.requires_grad = True\n", "        for p in self.vae.parameters():\n\t            p.requires_grad = True\n\t        return loss_actor, stats_actor\n\t    # from https://github.com/Farama-Foundation/D4RL-Evaluations\n\t    def mmd_loss_laplacian(self, samples1, samples2, sigma=0.2):\n\t        \"\"\"MMD constraint with Laplacian kernel for support matching\"\"\"\n\t        # sigma is set to 20.0 for hopper, cheetah and 50 for walker/ant\n\t        diff_x_x = samples1.unsqueeze(2) - samples1.unsqueeze(1)  # B x N x N x d\n\t        diff_x_x = torch.mean((-(diff_x_x.abs()).sum(-1) / (2.0 * sigma)).exp(),\n\t                              dim=(1, 2))\n", "        diff_x_y = samples1.unsqueeze(2) - samples2.unsqueeze(1)\n\t        diff_x_y = torch.mean((-(diff_x_y.abs()).sum(-1) / (2.0 * sigma)).exp(),\n\t                              dim=(1, 2))\n\t        diff_y_y = samples2.unsqueeze(2) - samples2.unsqueeze(1)  # B x N x N x d\n\t        diff_y_y = torch.mean((-(diff_y_y.abs()).sum(-1) / (2.0 * sigma)).exp(),\n\t                              dim=(1, 2))\n\t        overall_loss = (diff_x_x + diff_y_y - 2.0 * diff_x_y + 1e-6).sqrt()\n\t        return overall_loss\n\t    # from https://github.com/Farama-Foundation/D4RL-Evaluations\n\t    def mmd_loss_gaussian(self, samples1, samples2, sigma=0.2):\n", "        \"\"\"MMD constraint with Gaussian Kernel support matching\"\"\"\n\t        # sigma is set to 20.0 for hopper, cheetah and 50 for walker/ant\n\t        diff_x_x = samples1.unsqueeze(2) - samples1.unsqueeze(1)  # B x N x N x d\n\t        diff_x_x = torch.mean((-(diff_x_x.pow(2)).sum(-1) / (2.0 * sigma)).exp(),\n\t                              dim=(1, 2))\n\t        diff_x_y = samples1.unsqueeze(2) - samples2.unsqueeze(1)\n\t        diff_x_y = torch.mean((-(diff_x_y.pow(2)).sum(-1) / (2.0 * sigma)).exp(),\n\t                              dim=(1, 2))\n\t        diff_y_y = samples2.unsqueeze(2) - samples2.unsqueeze(1)  # B x N x N x d\n\t        diff_y_y = torch.mean((-(diff_y_y.pow(2)).sum(-1) / (2.0 * sigma)).exp(),\n", "                              dim=(1, 2))\n\t        overall_loss = (diff_x_x + diff_y_y - 2.0 * diff_x_y + 1e-6).sqrt()\n\t        return overall_loss\n\t    def setup_optimizers(self, actor_lr, critic_lr, vae_lr, alpha_lr):\n\t        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n\t        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n\t        self.cost_critic_optim = torch.optim.Adam(self.cost_critic.parameters(),\n\t                                                  lr=critic_lr)\n\t        self.vae_optim = torch.optim.Adam(self.vae.parameters(), lr=vae_lr)\n\t        # self.alpha_optim = torch.optim.Adam([self.log_alpha], lr=alpha_lr)\n", "        self.alpha_lr = alpha_lr\n\t    def sync_weight(self):\n\t        \"\"\"\n\t        Soft-update the weight for the target network.\n\t        \"\"\"\n\t        self._soft_update(self.critic_old, self.critic, self.tau)\n\t        self._soft_update(self.cost_critic_old, self.cost_critic, self.tau)\n\t        self._soft_update(self.actor_old, self.actor, self.tau)\n\t    def act(self,\n\t            obs: np.ndarray,\n", "            deterministic: bool = False,\n\t            with_logprob: bool = False):\n\t        \"\"\"\n\t        Given a single obs, return the action, logp.\n\t        \"\"\"\n\t        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n\t        a, logp_a = self._actor_forward(obs, deterministic, with_logprob)\n\t        a = a.data.numpy() if self.device == \"cpu\" else a.data.cpu().numpy()\n\t        logp_a = logp_a.data.numpy() if self.device == \"cpu\" else logp_a.data.cpu(\n\t        ).numpy()\n", "        return np.squeeze(a, axis=0), np.squeeze(logp_a)\n\tclass BEARLTrainer:\n\t    \"\"\"\n\t    BEARL Trainer\n\t    Args:\n\t        model (BEARL): The BEARL model to be trained.\n\t        env (gym.Env): The OpenAI Gym environment to train the model in.\n\t        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n\t        actor_lr (float): learning rate for actor\n\t        critic_lr (float): learning rate for critic\n", "        alpha_lr (float): learning rate for alpha\n\t        vae_lr (float): learning rate for vae\n\t        reward_scale (float): The scaling factor for the reward signal.\n\t        cost_scale (float): The scaling factor for the constraint cost.\n\t        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n\t    \"\"\"\n\t    def __init__(self,\n\t                 model: BEARL,\n\t                 env: gym.Env,\n\t                 logger: WandbLogger = DummyLogger(),\n", "                 actor_lr: float = 1e-3,\n\t                 critic_lr: float = 1e-3,\n\t                 alpha_lr: float = 1e-3,\n\t                 vae_lr: float = 1e-3,\n\t                 reward_scale: float = 1.0,\n\t                 cost_scale: float = 1.0,\n\t                 device=\"cpu\"):\n\t        self.model = model\n\t        self.logger = logger\n\t        self.env = env\n", "        self.reward_scale = reward_scale\n\t        self.cost_scale = cost_scale\n\t        self.device = device\n\t        self.model.setup_optimizers(actor_lr, critic_lr, vae_lr, alpha_lr)\n\t    def train_one_step(self, observations, next_observations, actions, rewards, costs,\n\t                       done):\n\t        \"\"\"\n\t        Trains the model by updating the VAE, critic, cost critic, and actor.\n\t        \"\"\"\n\t        # update VAE\n", "        loss_vae, stats_vae = self.model.vae_loss(observations, actions)\n\t        # update critic\n\t        loss_critic, stats_critic = self.model.critic_loss(observations,\n\t                                                           next_observations, actions,\n\t                                                           rewards, done)\n\t        # update cost critic\n\t        loss_cost_critic, stats_cost_critic = self.model.cost_critic_loss(\n\t            observations, next_observations, actions, costs, done)\n\t        # update actor\n\t        loss_actor, stats_actor = self.model.actor_loss(observations)\n", "        self.model.sync_weight()\n\t        self.logger.store(**stats_vae)\n\t        self.logger.store(**stats_critic)\n\t        self.logger.store(**stats_cost_critic)\n\t        self.logger.store(**stats_actor)\n\t    def evaluate(self, eval_episodes):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a number of episodes.\n\t        \"\"\"\n\t        self.model.eval()\n", "        episode_rets, episode_costs, episode_lens = [], [], []\n\t        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n\t            epi_ret, epi_len, epi_cost = self.rollout()\n\t            episode_rets.append(epi_ret)\n\t            episode_lens.append(epi_len)\n\t            episode_costs.append(epi_cost)\n\t        self.model.train()\n\t        return np.mean(episode_rets) / self.reward_scale, np.mean(\n\t            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\t    @torch.no_grad()\n", "    def rollout(self):\n\t        \"\"\"\n\t        Evaluates the performance of the model on a single episode.\n\t        \"\"\"\n\t        obs, info = self.env.reset()\n\t        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n\t        for _ in range(self.model.episode_len):\n\t            act, _ = self.model.act(obs, True, True)\n\t            obs_next, reward, terminated, truncated, info = self.env.step(act)\n\t            cost = info[\"cost\"] * self.cost_scale\n", "            obs = obs_next\n\t            episode_ret += reward\n\t            episode_len += 1\n\t            episode_cost += cost\n\t            if terminated or truncated:\n\t                break\n\t        return episode_ret, episode_len, episode_cost\n"]}
{"filename": "examples/__init__.py", "chunked_list": []}
{"filename": "examples/train_all_tasks.py", "chunked_list": ["from easy_runner import EasyRunner\n\tif __name__ == \"__main__\":\n\t    exp_name = \"benchmark\"\n\t    runner = EasyRunner(log_name=exp_name)\n\t    task = [\n\t        # bullet safety gym envs\n\t        \"OfflineAntCircle-v0\",\n\t        \"OfflineAntRun-v0\",\n\t        \"OfflineCarCircle-v0\",\n\t        \"OfflineDroneCircle-v0\",\n", "        \"OfflineDroneRun-v0\",\n\t        \"OfflineBallCircle-v0\",\n\t        \"OfflineBallRun-v0\",\n\t        \"OfflineCarRun-v0\",\n\t        # safety gymnasium: car\n\t        \"OfflineCarButton1Gymnasium-v0\",\n\t        \"OfflineCarButton2Gymnasium-v0\",\n\t        \"OfflineCarCircle1Gymnasium-v0\",\n\t        \"OfflineCarCircle2Gymnasium-v0\",\n\t        \"OfflineCarGoal1Gymnasium-v0\",\n", "        \"OfflineCarGoal2Gymnasium-v0\",\n\t        \"OfflineCarPush1Gymnasium-v0\",\n\t        \"OfflineCarPush2Gymnasium-v0\",\n\t        # safety gymnasium: point\n\t        \"OfflinePointButton1Gymnasium-v0\",\n\t        \"OfflinePointButton2Gymnasium-v0\",\n\t        \"OfflinePointCircle1Gymnasium-v0\",\n\t        \"OfflinePointCircle2Gymnasium-v0\",\n\t        \"OfflinePointGoal1Gymnasium-v0\",\n\t        \"OfflinePointGoal2Gymnasium-v0\",\n", "        \"OfflinePointPush1Gymnasium-v0\",\n\t        \"OfflinePointPush2Gymnasium-v0\",\n\t        # safety gymnasium: velocity\n\t        \"OfflineAntVelocityGymnasium-v1\",\n\t        \"OfflineHalfCheetahVelocityGymnasium-v1\",\n\t        \"OfflineHopperVelocityGymnasium-v1\",\n\t        \"OfflineSwimmerVelocityGymnasium-v1\",\n\t        \"OfflineWalker2dVelocityGymnasium-v1\",\n\t        # metadrive envs\n\t        \"OfflineMetadrive-easysparse-v0\",\n", "        \"OfflineMetadrive-easymean-v0\",\n\t        \"OfflineMetadrive-easydense-v0\",\n\t        \"OfflineMetadrive-mediumsparse-v0\",\n\t        \"OfflineMetadrive-mediummean-v0\",\n\t        \"OfflineMetadrive-mediumdense-v0\",\n\t        \"OfflineMetadrive-hardsparse-v0\",\n\t        \"OfflineMetadrive-hardmean-v0\",\n\t        \"OfflineMetadrive-harddense-v0\",\n\t    ]\n\t    policy = [\"train_bc\", \"train_bcql\", \"train_bearl\", \"train_coptidice\", \"train_cpq\"]\n", "    # Do not write & to the end of the command, it will be added automatically.\n\t    template = \"nohup python examples/train/{}.py --task {} --device cpu\"\n\t    train_instructions = runner.compose(template, [policy, task])\n\t    runner.start(train_instructions, max_parallel=15)\n"]}
{"filename": "examples/configs/coptidice_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\tfrom pyrallis import field\n\t@dataclass\n\tclass COptiDICETrainConfig:\n\t    # wandb params\n\t    project: str = \"OSRL-baselines\"\n\t    group: str = None\n\t    name: Optional[str] = None\n\t    prefix: Optional[str] = \"COptiDICE\"\n", "    suffix: Optional[str] = \"\"\n\t    logdir: Optional[str] = \"logs\"\n\t    verbose: bool = True\n\t    # dataset params\n\t    outliers_percent: float = None\n\t    noise_scale: float = None\n\t    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n\t    epsilon: float = None\n\t    density: float = 1.0\n\t    # training params\n", "    task: str = \"OfflineCarCircle-v0\"\n\t    dataset: str = None\n\t    seed: int = 0\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t    reward_scale: float = 0.1\n\t    cost_scale: float = 1\n\t    actor_lr: float = 0.0001\n\t    critic_lr: float = 0.0001\n\t    scalar_lr: float = 0.0001\n", "    cost_limit: int = 10\n\t    episode_len: int = 300\n\t    batch_size: int = 512\n\t    update_steps: int = 100_000\n\t    num_workers: int = 8\n\t    # model params\n\t    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n\t    c_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n\t    alpha: float = 0.5\n\t    gamma: float = 0.99\n", "    cost_ub_epsilon: float = 0.01\n\t    f_type: str = \"softchi\"\n\t    num_nu: int = 2\n\t    num_chi: int = 2\n\t    # evaluation params\n\t    eval_episodes: int = 10\n\t    eval_every: int = 2500\n\t@dataclass\n\tclass COptiDICECarCircleConfig(COptiDICETrainConfig):\n\t    pass\n", "@dataclass\n\tclass COptiDICEAntRunConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass COptiDICEDroneRunConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineDroneRun-v0\"\n\t    episode_len: int = 200\n", "@dataclass\n\tclass COptiDICEDroneCircleConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineDroneCircle-v0\"\n\t    episode_len: int = 300\n\t@dataclass\n\tclass COptiDICECarRunConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarRun-v0\"\n\t    episode_len: int = 200\n", "@dataclass\n\tclass COptiDICEAntCircleConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntCircle-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass COptiDICEBallRunConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineBallRun-v0\"\n\t    episode_len: int = 100\n", "@dataclass\n\tclass COptiDICEBallCircleConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineBallCircle-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass COptiDICECarButton1Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarButton1Gymnasium-v0\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICECarButton2Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass COptiDICECarCircle1Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n\t    episode_len: int = 500\n", "@dataclass\n\tclass COptiDICECarCircle2Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass COptiDICECarGoal1Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICECarGoal2Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass COptiDICECarPush1Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarPush1Gymnasium-v0\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICECarPush2Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass COptiDICEPointButton1Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointButton1Gymnasium-v0\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICEPointButton2Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass COptiDICEPointCircle1Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n\t    episode_len: int = 500\n", "@dataclass\n\tclass COptiDICEPointCircle2Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass COptiDICEPointGoal1Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICEPointGoal2Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass COptiDICEPointPush1Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointPush1Gymnasium-v0\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICEPointPush2Config(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass COptiDICEAntVelocityConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICEHalfCheetahVelocityConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass COptiDICEHopperVelocityConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICESwimmerVelocityConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass COptiDICEWalker2dVelocityConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n", "@dataclass\n\tclass COptiDICEEasySparseConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easysparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass COptiDICEEasyMeanConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easymean-v0\"\n", "    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass COptiDICEEasyDenseConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easydense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass COptiDICEMediumSparseConfig(COptiDICETrainConfig):\n", "    # training params\n\t    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass COptiDICEMediumMeanConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediummean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n", "@dataclass\n\tclass COptiDICEMediumDenseConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumdense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass COptiDICEHardSparseConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardsparse-v0\"\n", "    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass COptiDICEHardMeanConfig(COptiDICETrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardmean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass COptiDICEHardDenseConfig(COptiDICETrainConfig):\n", "    # training params\n\t    task: str = \"OfflineMetadrive-harddense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\tCOptiDICE_DEFAULT_CONFIG = {\n\t    # bullet_safety_gym\n\t    \"OfflineCarCircle-v0\": COptiDICECarCircleConfig,\n\t    \"OfflineAntRun-v0\": COptiDICEAntRunConfig,\n\t    \"OfflineDroneRun-v0\": COptiDICEDroneRunConfig,\n\t    \"OfflineDroneCircle-v0\": COptiDICEDroneCircleConfig,\n", "    \"OfflineCarRun-v0\": COptiDICECarRunConfig,\n\t    \"OfflineAntCircle-v0\": COptiDICEAntCircleConfig,\n\t    \"OfflineBallCircle-v0\": COptiDICEBallCircleConfig,\n\t    \"OfflineBallRun-v0\": COptiDICEBallRunConfig,\n\t    # safety_gymnasium\n\t    \"OfflineCarButton1Gymnasium-v0\": COptiDICECarButton1Config,\n\t    \"OfflineCarButton2Gymnasium-v0\": COptiDICECarButton2Config,\n\t    \"OfflineCarCircle1Gymnasium-v0\": COptiDICECarCircle1Config,\n\t    \"OfflineCarCircle2Gymnasium-v0\": COptiDICECarCircle2Config,\n\t    \"OfflineCarGoal1Gymnasium-v0\": COptiDICECarGoal1Config,\n", "    \"OfflineCarGoal2Gymnasium-v0\": COptiDICECarGoal2Config,\n\t    \"OfflineCarPush1Gymnasium-v0\": COptiDICECarPush1Config,\n\t    \"OfflineCarPush2Gymnasium-v0\": COptiDICECarPush2Config,\n\t    # safety_gymnasium: point\n\t    \"OfflinePointButton1Gymnasium-v0\": COptiDICEPointButton1Config,\n\t    \"OfflinePointButton2Gymnasium-v0\": COptiDICEPointButton2Config,\n\t    \"OfflinePointCircle1Gymnasium-v0\": COptiDICEPointCircle1Config,\n\t    \"OfflinePointCircle2Gymnasium-v0\": COptiDICEPointCircle2Config,\n\t    \"OfflinePointGoal1Gymnasium-v0\": COptiDICEPointGoal1Config,\n\t    \"OfflinePointGoal2Gymnasium-v0\": COptiDICEPointGoal2Config,\n", "    \"OfflinePointPush1Gymnasium-v0\": COptiDICEPointPush1Config,\n\t    \"OfflinePointPush2Gymnasium-v0\": COptiDICEPointPush2Config,\n\t    # safety_gymnasium: velocity\n\t    \"OfflineAntVelocityGymnasium-v1\": COptiDICEAntVelocityConfig,\n\t    \"OfflineHalfCheetahVelocityGymnasium-v1\": COptiDICEHalfCheetahVelocityConfig,\n\t    \"OfflineHopperVelocityGymnasium-v1\": COptiDICEHopperVelocityConfig,\n\t    \"OfflineSwimmerVelocityGymnasium-v1\": COptiDICESwimmerVelocityConfig,\n\t    \"OfflineWalker2dVelocityGymnasium-v1\": COptiDICEWalker2dVelocityConfig,\n\t    # safe_metadrive\n\t    \"OfflineMetadrive-easysparse-v0\": COptiDICEEasySparseConfig,\n", "    \"OfflineMetadrive-easymean-v0\": COptiDICEEasyMeanConfig,\n\t    \"OfflineMetadrive-easydense-v0\": COptiDICEEasyDenseConfig,\n\t    \"OfflineMetadrive-mediumsparse-v0\": COptiDICEMediumSparseConfig,\n\t    \"OfflineMetadrive-mediummean-v0\": COptiDICEMediumMeanConfig,\n\t    \"OfflineMetadrive-mediumdense-v0\": COptiDICEMediumDenseConfig,\n\t    \"OfflineMetadrive-hardsparse-v0\": COptiDICEHardSparseConfig,\n\t    \"OfflineMetadrive-hardmean-v0\": COptiDICEHardMeanConfig,\n\t    \"OfflineMetadrive-harddense-v0\": COptiDICEHardDenseConfig\n\t}\n"]}
{"filename": "examples/configs/bc_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\tfrom pyrallis import field\n\t@dataclass\n\tclass BCTrainConfig:\n\t    # wandb params\n\t    project: str = \"OSRL-baselines\"\n\t    group: str = None\n\t    name: Optional[str] = None\n\t    prefix: Optional[str] = \"BC\"\n", "    suffix: Optional[str] = \"\"\n\t    logdir: Optional[str] = \"logs\"\n\t    verbose: bool = True\n\t    # dataset params\n\t    outliers_percent: float = None\n\t    noise_scale: float = None\n\t    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n\t    epsilon: float = None\n\t    density: float = 1.0\n\t    # training params\n", "    task: str = \"OfflineCarCircle-v0\"\n\t    dataset: str = None\n\t    seed: int = 0\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t    actor_lr: float = 0.001\n\t    cost_limit: int = 10\n\t    episode_len: int = 300\n\t    batch_size: int = 512\n\t    update_steps: int = 100_000\n", "    num_workers: int = 8\n\t    bc_mode: str = \"all\"  # \"all\", \"safe\", \"risky\", \"frontier\", \"boundary\", \"multi-task\"\n\t    # model params\n\t    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n\t    gamma: float = 1.0\n\t    # evaluation params\n\t    eval_episodes: int = 10\n\t    eval_every: int = 2500\n\t@dataclass\n\tclass BCCarCircleConfig(BCTrainConfig):\n", "    pass\n\t@dataclass\n\tclass BCAntRunConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass BCDroneRunConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineDroneRun-v0\"\n", "    episode_len: int = 200\n\t@dataclass\n\tclass BCDroneCircleConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineDroneCircle-v0\"\n\t    episode_len: int = 300\n\t@dataclass\n\tclass BCCarRunConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarRun-v0\"\n", "    episode_len: int = 200\n\t@dataclass\n\tclass BCAntCircleConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntCircle-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BCBallRunConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineBallRun-v0\"\n", "    episode_len: int = 100\n\t@dataclass\n\tclass BCBallCircleConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineBallCircle-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass BCCarButton1Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarButton1Gymnasium-v0\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCCarButton2Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCCarCircle1Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n", "    episode_len: int = 500\n\t@dataclass\n\tclass BCCarCircle2Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BCCarGoal1Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCCarGoal2Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCCarPush1Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarPush1Gymnasium-v0\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCCarPush2Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCPointButton1Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointButton1Gymnasium-v0\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCPointButton2Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCPointCircle1Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n", "    episode_len: int = 500\n\t@dataclass\n\tclass BCPointCircle2Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BCPointGoal1Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCPointGoal2Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCPointPush1Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointPush1Gymnasium-v0\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCPointPush2Config(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCAntVelocityConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntVelocityGymnasium-v1\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCHalfCheetahVelocityConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCHopperVelocityConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCSwimmerVelocityConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCWalker2dVelocityConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n", "    episode_len: int = 1000\n\t@dataclass\n\tclass BCEasySparseConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easysparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCEasyMeanConfig(BCTrainConfig):\n\t    # training params\n", "    task: str = \"OfflineMetadrive-easymean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCEasyDenseConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easydense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n", "class BCMediumSparseConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCMediumMeanConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediummean-v0\"\n\t    episode_len: int = 1000\n", "    update_steps: int = 200_000\n\t@dataclass\n\tclass BCMediumDenseConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumdense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCHardSparseConfig(BCTrainConfig):\n\t    # training params\n", "    task: str = \"OfflineMetadrive-hardsparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCHardMeanConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardmean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n", "class BCHardDenseConfig(BCTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-harddense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\tBC_DEFAULT_CONFIG = {\n\t    # bullet_safety_gym\n\t    \"OfflineCarCircle-v0\": BCCarCircleConfig,\n\t    \"OfflineAntRun-v0\": BCAntRunConfig,\n\t    \"OfflineDroneRun-v0\": BCDroneRunConfig,\n", "    \"OfflineDroneCircle-v0\": BCDroneCircleConfig,\n\t    \"OfflineCarRun-v0\": BCCarRunConfig,\n\t    \"OfflineAntCircle-v0\": BCAntCircleConfig,\n\t    \"OfflineBallCircle-v0\": BCBallCircleConfig,\n\t    \"OfflineBallRun-v0\": BCBallRunConfig,\n\t    # safety_gymnasium: car\n\t    \"OfflineCarButton1Gymnasium-v0\": BCCarButton1Config,\n\t    \"OfflineCarButton2Gymnasium-v0\": BCCarButton2Config,\n\t    \"OfflineCarCircle1Gymnasium-v0\": BCCarCircle1Config,\n\t    \"OfflineCarCircle2Gymnasium-v0\": BCCarCircle2Config,\n", "    \"OfflineCarGoal1Gymnasium-v0\": BCCarGoal1Config,\n\t    \"OfflineCarGoal2Gymnasium-v0\": BCCarGoal2Config,\n\t    \"OfflineCarPush1Gymnasium-v0\": BCCarPush1Config,\n\t    \"OfflineCarPush2Gymnasium-v0\": BCCarPush2Config,\n\t    # safety_gymnasium: point\n\t    \"OfflinePointButton1Gymnasium-v0\": BCPointButton1Config,\n\t    \"OfflinePointButton2Gymnasium-v0\": BCPointButton2Config,\n\t    \"OfflinePointCircle1Gymnasium-v0\": BCPointCircle1Config,\n\t    \"OfflinePointCircle2Gymnasium-v0\": BCPointCircle2Config,\n\t    \"OfflinePointGoal1Gymnasium-v0\": BCPointGoal1Config,\n", "    \"OfflinePointGoal2Gymnasium-v0\": BCPointGoal2Config,\n\t    \"OfflinePointPush1Gymnasium-v0\": BCPointPush1Config,\n\t    \"OfflinePointPush2Gymnasium-v0\": BCPointPush2Config,\n\t    # safety_gymnasium: velocity\n\t    \"OfflineAntVelocityGymnasium-v1\": BCAntVelocityConfig,\n\t    \"OfflineHalfCheetahVelocityGymnasium-v1\": BCHalfCheetahVelocityConfig,\n\t    \"OfflineHopperVelocityGymnasium-v1\": BCHopperVelocityConfig,\n\t    \"OfflineSwimmerVelocityGymnasium-v1\": BCSwimmerVelocityConfig,\n\t    \"OfflineWalker2dVelocityGymnasium-v1\": BCWalker2dVelocityConfig,\n\t    # safe_metadrive\n", "    \"OfflineMetadrive-easysparse-v0\": BCEasySparseConfig,\n\t    \"OfflineMetadrive-easymean-v0\": BCEasyMeanConfig,\n\t    \"OfflineMetadrive-easydense-v0\": BCEasyDenseConfig,\n\t    \"OfflineMetadrive-mediumsparse-v0\": BCMediumSparseConfig,\n\t    \"OfflineMetadrive-mediummean-v0\": BCMediumMeanConfig,\n\t    \"OfflineMetadrive-mediumdense-v0\": BCMediumDenseConfig,\n\t    \"OfflineMetadrive-hardsparse-v0\": BCHardSparseConfig,\n\t    \"OfflineMetadrive-hardmean-v0\": BCHardMeanConfig,\n\t    \"OfflineMetadrive-harddense-v0\": BCHardDenseConfig\n\t}"]}
{"filename": "examples/configs/__init__.py", "chunked_list": []}
{"filename": "examples/configs/cpq_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\tfrom pyrallis import field\n\t@dataclass\n\tclass CPQTrainConfig:\n\t    # wandb params\n\t    project: str = \"OSRL-baselines\"\n\t    group: str = None\n\t    name: Optional[str] = None\n\t    prefix: Optional[str] = \"CPQ\"\n", "    suffix: Optional[str] = \"\"\n\t    logdir: Optional[str] = \"logs\"\n\t    verbose: bool = True\n\t    # dataset params\n\t    outliers_percent: float = None\n\t    noise_scale: float = None\n\t    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n\t    epsilon: float = None\n\t    density: float = 1.0\n\t    # training params\n", "    task: str = \"OfflineCarCircle-v0\"\n\t    dataset: str = None\n\t    seed: int = 0\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t    reward_scale: float = 0.1\n\t    cost_scale: float = 1\n\t    actor_lr: float = 0.0001\n\t    critic_lr: float = 0.001\n\t    alpha_lr: float = 0.0001\n", "    vae_lr: float = 0.001\n\t    cost_limit: int = 10\n\t    episode_len: int = 300\n\t    batch_size: int = 512\n\t    update_steps: int = 100_000\n\t    num_workers: int = 8\n\t    # model params\n\t    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n\t    c_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n\t    vae_hidden_sizes: int = 400\n", "    sample_action_num: int = 10\n\t    gamma: float = 0.99\n\t    tau: float = 0.005\n\t    beta: float = 0.5\n\t    num_q: int = 2\n\t    num_qc: int = 2\n\t    qc_scalar: float = 1.5\n\t    # evaluation params\n\t    eval_episodes: int = 10\n\t    eval_every: int = 2500\n", "@dataclass\n\tclass CPQCarCircleConfig(CPQTrainConfig):\n\t    pass\n\t@dataclass\n\tclass CPQAntRunConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass CPQDroneRunConfig(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineDroneRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass CPQDroneCircleConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineDroneCircle-v0\"\n\t    episode_len: int = 300\n\t@dataclass\n\tclass CPQCarRunConfig(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass CPQAntCircleConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntCircle-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass CPQBallRunConfig(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineBallRun-v0\"\n\t    episode_len: int = 100\n\t@dataclass\n\tclass CPQBallCircleConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineBallCircle-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass CPQCarButton1Config(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarButton1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQCarButton2Config(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQCarCircle1Config(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass CPQCarCircle2Config(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass CPQCarGoal1Config(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQCarGoal2Config(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQCarPush1Config(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarPush1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQCarPush2Config(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQPointButton1Config(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflinePointButton1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQPointButton2Config(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQPointCircle1Config(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass CPQPointCircle2Config(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass CPQPointGoal1Config(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQPointGoal2Config(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQPointPush1Config(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflinePointPush1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQPointPush2Config(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQAntVelocityConfig(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineAntVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQHalfCheetahVelocityConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQHopperVelocityConfig(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQSwimmerVelocityConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQWalker2dVelocityConfig(CPQTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass CPQEasySparseConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easysparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n", "class CPQEasyMeanConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easymean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass CPQEasyDenseConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easydense-v0\"\n\t    episode_len: int = 1000\n", "    update_steps: int = 200_000\n\t@dataclass\n\tclass CPQMediumSparseConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass CPQMediumMeanConfig(CPQTrainConfig):\n\t    # training params\n", "    task: str = \"OfflineMetadrive-mediummean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass CPQMediumDenseConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumdense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n", "class CPQHardSparseConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardsparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass CPQHardMeanConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardmean-v0\"\n\t    episode_len: int = 1000\n", "    update_steps: int = 200_000\n\t@dataclass\n\tclass CPQHardDenseConfig(CPQTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-harddense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\tCPQ_DEFAULT_CONFIG = {\n\t    # bullet_safety_gym\n\t    \"OfflineCarCircle-v0\": CPQCarCircleConfig,\n", "    \"OfflineAntRun-v0\": CPQAntRunConfig,\n\t    \"OfflineDroneRun-v0\": CPQDroneRunConfig,\n\t    \"OfflineDroneCircle-v0\": CPQDroneCircleConfig,\n\t    \"OfflineCarRun-v0\": CPQCarRunConfig,\n\t    \"OfflineAntCircle-v0\": CPQAntCircleConfig,\n\t    \"OfflineBallCircle-v0\": CPQBallCircleConfig,\n\t    \"OfflineBallRun-v0\": CPQBallRunConfig,\n\t    # safety_gymnasium\n\t    \"OfflineCarButton1Gymnasium-v0\": CPQCarButton1Config,\n\t    \"OfflineCarButton2Gymnasium-v0\": CPQCarButton2Config,\n", "    \"OfflineCarCircle1Gymnasium-v0\": CPQCarCircle1Config,\n\t    \"OfflineCarCircle2Gymnasium-v0\": CPQCarCircle2Config,\n\t    \"OfflineCarGoal1Gymnasium-v0\": CPQCarGoal1Config,\n\t    \"OfflineCarGoal2Gymnasium-v0\": CPQCarGoal2Config,\n\t    \"OfflineCarPush1Gymnasium-v0\": CPQCarPush1Config,\n\t    \"OfflineCarPush2Gymnasium-v0\": CPQCarPush2Config,\n\t    # safety_gymnasium: point\n\t    \"OfflinePointButton1Gymnasium-v0\": CPQPointButton1Config,\n\t    \"OfflinePointButton2Gymnasium-v0\": CPQPointButton2Config,\n\t    \"OfflinePointCircle1Gymnasium-v0\": CPQPointCircle1Config,\n", "    \"OfflinePointCircle2Gymnasium-v0\": CPQPointCircle2Config,\n\t    \"OfflinePointGoal1Gymnasium-v0\": CPQPointGoal1Config,\n\t    \"OfflinePointGoal2Gymnasium-v0\": CPQPointGoal2Config,\n\t    \"OfflinePointPush1Gymnasium-v0\": CPQPointPush1Config,\n\t    \"OfflinePointPush2Gymnasium-v0\": CPQPointPush2Config,\n\t    # safety_gymnasium: velocity\n\t    \"OfflineAntVelocityGymnasium-v1\": CPQAntVelocityConfig,\n\t    \"OfflineHalfCheetahVelocityGymnasium-v1\": CPQHalfCheetahVelocityConfig,\n\t    \"OfflineHopperVelocityGymnasium-v1\": CPQHopperVelocityConfig,\n\t    \"OfflineSwimmerVelocityGymnasium-v1\": CPQSwimmerVelocityConfig,\n", "    \"OfflineWalker2dVelocityGymnasium-v1\": CPQWalker2dVelocityConfig,\n\t    # safe_metadrive\n\t    \"OfflineMetadrive-easysparse-v0\": CPQEasySparseConfig,\n\t    \"OfflineMetadrive-easymean-v0\": CPQEasyMeanConfig,\n\t    \"OfflineMetadrive-easydense-v0\": CPQEasyDenseConfig,\n\t    \"OfflineMetadrive-mediumsparse-v0\": CPQMediumSparseConfig,\n\t    \"OfflineMetadrive-mediummean-v0\": CPQMediumMeanConfig,\n\t    \"OfflineMetadrive-mediumdense-v0\": CPQMediumDenseConfig,\n\t    \"OfflineMetadrive-hardsparse-v0\": CPQHardSparseConfig,\n\t    \"OfflineMetadrive-hardmean-v0\": CPQHardMeanConfig,\n", "    \"OfflineMetadrive-harddense-v0\": CPQHardDenseConfig\n\t}"]}
{"filename": "examples/configs/cdt_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\t@dataclass\n\tclass CDTTrainConfig:\n\t    # wandb params\n\t    project: str = \"OSRL-baselines\"\n\t    group: str = None\n\t    name: Optional[str] = None\n\t    prefix: Optional[str] = \"CDT\"\n\t    suffix: Optional[str] = \"\"\n", "    logdir: Optional[str] = \"logs\"\n\t    verbose: bool = True\n\t    # dataset params\n\t    outliers_percent: float = None\n\t    noise_scale: float = None\n\t    inpaint_ranges: Tuple[Tuple[float, float], ...] = None\n\t    epsilon: float = None\n\t    density: float = 1.0\n\t    # model params\n\t    embedding_dim: int = 128\n", "    num_layers: int = 3\n\t    num_heads: int = 8\n\t    action_head_layers: int = 1\n\t    seq_len: int = 10\n\t    episode_len: int = 300\n\t    attention_dropout: float = 0.1\n\t    residual_dropout: float = 0.1\n\t    embedding_dropout: float = 0.1\n\t    time_emb: bool = True\n\t    # training params\n", "    task: str = \"OfflineCarCircle-v0\"\n\t    dataset: str = None\n\t    learning_rate: float = 1e-4\n\t    betas: Tuple[float, float] = (0.9, 0.999)\n\t    weight_decay: float = 1e-4\n\t    clip_grad: Optional[float] = 0.25\n\t    batch_size: int = 2048\n\t    update_steps: int = 100_000\n\t    lr_warmup_steps: int = 500\n\t    reward_scale: float = 0.1\n", "    cost_scale: float = 1\n\t    num_workers: int = 8\n\t    # evaluation params\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((450.0, 10), (500.0, 20), (550.0, 50))  # reward, cost\n\t    cost_limit: int = 10\n\t    eval_episodes: int = 10\n\t    eval_every: int = 2500\n\t    # general params\n\t    seed: int = 0\n", "    device: str = \"cuda:2\"\n\t    threads: int = 6\n\t    # augmentation param\n\t    deg: int = 4\n\t    pf_sample: bool = False\n\t    beta: float = 1.0\n\t    augment_percent: float = 0.2\n\t    # maximum absolute value of reward for the augmented trajs\n\t    max_reward: float = 600.0\n\t    # minimum reward above the PF curve\n", "    min_reward: float = 1.0\n\t    # the max drecrease of ret between the associated traj\n\t    # w.r.t the nearest pf traj\n\t    max_rew_decrease: float = 100.0\n\t    # model mode params\n\t    use_rew: bool = True\n\t    use_cost: bool = True\n\t    cost_transform: bool = True\n\t    cost_prefix: bool = False\n\t    add_cost_feat: bool = False\n", "    mul_cost_feat: bool = False\n\t    cat_cost_feat: bool = False\n\t    loss_cost_weight: float = 0.02\n\t    loss_state_weight: float = 0\n\t    cost_reverse: bool = False\n\t    # pf only mode param\n\t    pf_only: bool = False\n\t    rmin: float = 300\n\t    cost_bins: int = 60\n\t    npb: int = 5\n", "    cost_sample: bool = True\n\t    linear: bool = True  # linear or inverse\n\t    start_sampling: bool = False\n\t    prob: float = 0.2\n\t    stochastic: bool = True\n\t    init_temperature: float = 0.1\n\t    no_entropy: bool = False\n\t    # random augmentation\n\t    random_aug: float = 0\n\t    aug_rmin: float = 400\n", "    aug_rmax: float = 500\n\t    aug_cmin: float = -2\n\t    aug_cmax: float = 25\n\t    cgap: float = 5\n\t    rstd: float = 1\n\t    cstd: float = 0.2\n\t@dataclass\n\tclass CDTCarCircleConfig(CDTTrainConfig):\n\t    pass\n\t@dataclass\n", "class CDTAntRunConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 200\n\t    # training params\n\t    task: str = \"OfflineAntRun-v0\"\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((700.0, 10), (750.0, 20), (800.0, 40))\n\t    # augmentation param\n\t    deg: int = 3\n", "    max_reward: float = 1000.0\n\t    max_rew_decrease: float = 150\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTDroneRunConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 200\n\t    # training params\n\t    task: str = \"OfflineDroneRun-v0\"\n", "    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((400.0, 10), (500.0, 20), (600.0, 40))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 700.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTDroneCircleConfig(CDTTrainConfig):\n", "    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 300\n\t    # training params\n\t    task: str = \"OfflineDroneCircle-v0\"\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((700.0, 10), (750.0, 20), (800.0, 40))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 1000.0\n", "    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTCarRunConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 200\n\t    # training params\n\t    task: str = \"OfflineCarRun-v0\"\n", "    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((575.0, 10), (575.0, 20), (575.0, 40))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 600.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTAntCircleConfig(CDTTrainConfig):\n", "    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 500\n\t    # training params\n\t    task: str = \"OfflineAntCircle-v0\"\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n\t    # augmentation param\n\t    deg: int = 2\n\t    max_reward: float = 500.0\n", "    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTBallRunConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 100\n\t    # training params\n\t    task: str = \"OfflineBallRun-v0\"\n", "    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((500.0, 10), (500.0, 20), (700.0, 40))\n\t    # augmentation param\n\t    deg: int = 2\n\t    max_reward: float = 1400.0\n\t    max_rew_decrease: float = 200\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTBallCircleConfig(CDTTrainConfig):\n", "    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 200\n\t    # training params\n\t    task: str = \"OfflineBallCircle-v0\"\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((700.0, 10), (750.0, 20), (800.0, 40))\n\t    # augmentation param\n\t    deg: int = 2\n\t    max_reward: float = 1000.0\n", "    max_rew_decrease: float = 200\n\t    min_reward: float = 1\n\t    device: str = \"cuda:1\"\n\t@dataclass\n\tclass CDTCarButton1Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineCarButton1Gymnasium-v0\"\n", "    target_returns: Tuple[Tuple[float, ...], ...] = ((35.0, 20), (35.0, 40), (35.0, 80))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 45.0\n\t    max_rew_decrease: float = 10\n\t    min_reward: float = 1\n\t    device: str = \"cuda:0\"\n\t@dataclass\n\tclass CDTCarButton2Config(CDTTrainConfig):\n\t    # model params\n", "    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineCarButton2Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((40.0, 20), (40.0, 40), (40.0, 80))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 50.0\n\t    max_rew_decrease: float = 10\n\t    min_reward: float = 1\n", "    device: str = \"cuda:0\"\n\t@dataclass\n\tclass CDTCarCircle1Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 500\n\t    # training params\n\t    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((20.0, 20), (22.5, 40), (25.0, 80))\n\t    # augmentation param\n", "    deg: int = 1\n\t    max_reward: float = 30.0\n\t    max_rew_decrease: float = 10\n\t    min_reward: float = 1\n\t    device: str = \"cuda:0\"\n\t@dataclass\n\tclass CDTCarCircle2Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 500\n", "    # training params\n\t    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((20.0, 20), (21.0, 40), (22.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 30.0\n\t    max_rew_decrease: float = 10\n\t    min_reward: float = 1\n\t    device: str = \"cuda:0\"\n\t@dataclass\n", "class CDTCarGoal1Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((40.0, 20), (40.0, 40), (40.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 50.0\n", "    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n\t    device: str = \"cuda:1\"\n\t@dataclass\n\tclass CDTCarGoal2Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n", "    target_returns: Tuple[Tuple[float, ...], ...] = ((30.0, 20), (30.0, 40), (30.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 35.0\n\t    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n\t    device: str = \"cuda:1\"\n\t@dataclass\n\tclass CDTCarPush1Config(CDTTrainConfig):\n\t    # model params\n", "    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineCarPush1Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((15.0, 20), (15.0, 40), (15.0, 80))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 20.0\n\t    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n", "    device: str = \"cuda:1\"\n\t@dataclass\n\tclass CDTCarPush2Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineCarPush2Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((12.0, 20), (12.0, 40), (12.0, 80))\n\t    # augmentation param\n", "    deg: int = 0\n\t    max_reward: float = 15.0\n\t    max_rew_decrease: float = 3\n\t    min_reward: float = 1\n\t    device: str = \"cuda:1\"\n\t@dataclass\n\tclass CDTPointButton1Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n", "    # training params\n\t    task: str = \"OfflinePointButton1Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((40.0, 20), (40.0, 40), (40.0, 80))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 45.0\n\t    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n", "class CDTPointButton2Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflinePointButton2Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((40.0, 20), (40.0, 40), (40.0, 80))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 50.0\n", "    max_rew_decrease: float = 10\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTPointCircle1Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 500\n\t    # training params\n\t    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n", "    target_returns: Tuple[Tuple[float, ...], ...] = ((50.0, 20), (52.5, 40), (55.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 65.0\n\t    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTPointCircle2Config(CDTTrainConfig):\n\t    # model params\n", "    seq_len: int = 10\n\t    episode_len: int = 500\n\t    # training params\n\t    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((45.0, 20), (47.5, 40), (50.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 55.0\n\t    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n", "    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTPointGoal1Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((30.0, 20), (30.0, 40), (30.0, 80))\n\t    # augmentation param\n", "    deg: int = 0\n\t    max_reward: float = 35.0\n\t    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTPointGoal2Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n", "    # training params\n\t    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((30.0, 20), (30.0, 40), (30.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 35.0\n\t    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n", "class CDTPointPush1Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflinePointPush1Gymnasium-v0\"\n\t    target_returns: Tuple[Tuple[float, ...], ...] = ((15.0, 20), (15.0, 40), (15.0, 80))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 20.0\n", "    max_rew_decrease: float = 5\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTPointPush2Config(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflinePointPush2Gymnasium-v0\"\n", "    target_returns: Tuple[Tuple[float, ...], ...] = ((12.0, 20), (12.0, 40), (12.0, 80))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 15.0\n\t    max_rew_decrease: float = 3\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTAntVelocityConfig(CDTTrainConfig):\n\t    # model params\n", "    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineAntVelocityGymnasium-v1\"\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((2800.0, 20), (2800.0, 40), (2800.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 3000.0\n\t    max_rew_decrease: float = 500\n", "    min_reward: float = 1\n\t    device: str = \"cuda:1\"\n\t@dataclass\n\tclass CDTHalfCheetahVelocityConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n\t    target_returns: Tuple[Tuple[float, ...],\n", "                          ...] = ((3000.0, 20), (3000.0, 40), (3000.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 3000.0\n\t    max_rew_decrease: float = 500\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTHopperVelocityConfig(CDTTrainConfig):\n\t    # model params\n", "    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((1750.0, 20), (1750.0, 40), (1750.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 2000.0\n\t    max_rew_decrease: float = 300\n", "    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTSwimmerVelocityConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n\t    target_returns: Tuple[Tuple[float, ...],\n", "                          ...] = ((160.0, 20), (160.0, 40), (160.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 250.0\n\t    max_rew_decrease: float = 50\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTWalker2dVelocityConfig(CDTTrainConfig):\n\t    # model params\n", "    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((2800.0, 20), (2800.0, 40), (2800.0, 80))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 3600.0\n\t    max_rew_decrease: float = 800\n", "    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTEasySparseConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easysparse-v0\"\n\t    update_steps: int = 200_000\n", "    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n\t    # augmentation param\n\t    deg: int = 2\n\t    max_reward: float = 500.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTEasyMeanConfig(CDTTrainConfig):\n", "    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easymean-v0\"\n\t    update_steps: int = 200_000\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n\t    # augmentation param\n\t    deg: int = 2\n", "    max_reward: float = 500.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTEasyDenseConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n", "    task: str = \"OfflineMetadrive-easydense-v0\"\n\t    update_steps: int = 200_000\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n\t    # augmentation param\n\t    deg: int = 2\n\t    max_reward: float = 500.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n", "@dataclass\n\tclass CDTMediumSparseConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n\t    update_steps: int = 200_000\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (300.0, 20), (300.0, 40))\n", "    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 300.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:3\"\n\t@dataclass\n\tclass CDTMediumMeanConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n", "    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediummean-v0\"\n\t    update_steps: int = 200_000\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (300.0, 20), (300.0, 40))\n\t    # augmentation param\n\t    deg: int = 0\n\t    max_reward: float = 300.0\n\t    max_rew_decrease: float = 100\n", "    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTMediumDenseConfig(CDTTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumdense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass CDTHardSparseConfig(CDTTrainConfig):\n", "    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardsparse-v0\"\n\t    update_steps: int = 200_000\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n\t    # augmentation param\n\t    deg: int = 1\n", "    max_reward: float = 500.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\t@dataclass\n\tclass CDTHardMeanConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n", "    task: str = \"OfflineMetadrive-hardmean-v0\"\n\t    update_steps: int = 200_000\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n\t    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 500.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n", "@dataclass\n\tclass CDTHardDenseConfig(CDTTrainConfig):\n\t    # model params\n\t    seq_len: int = 10\n\t    episode_len: int = 1000\n\t    # training params\n\t    task: str = \"OfflineMetadrive-harddense-v0\"\n\t    update_steps: int = 200_000\n\t    target_returns: Tuple[Tuple[float, ...],\n\t                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n", "    # augmentation param\n\t    deg: int = 1\n\t    max_reward: float = 500.0\n\t    max_rew_decrease: float = 100\n\t    min_reward: float = 1\n\t    device: str = \"cuda:2\"\n\tCDT_DEFAULT_CONFIG = {\n\t    # bullet_safety_gym\n\t    \"OfflineCarCircle-v0\": CDTCarCircleConfig,\n\t    \"OfflineAntRun-v0\": CDTAntRunConfig,\n", "    \"OfflineDroneRun-v0\": CDTDroneRunConfig,\n\t    \"OfflineDroneCircle-v0\": CDTDroneCircleConfig,\n\t    \"OfflineCarRun-v0\": CDTCarRunConfig,\n\t    \"OfflineAntCircle-v0\": CDTAntCircleConfig,\n\t    \"OfflineBallCircle-v0\": CDTBallCircleConfig,\n\t    \"OfflineBallRun-v0\": CDTBallRunConfig,\n\t    # safety_gymnasium\n\t    \"OfflineCarButton1Gymnasium-v0\": CDTCarButton1Config,\n\t    \"OfflineCarButton2Gymnasium-v0\": CDTCarButton2Config,\n\t    \"OfflineCarCircle1Gymnasium-v0\": CDTCarCircle1Config,\n", "    \"OfflineCarCircle2Gymnasium-v0\": CDTCarCircle2Config,\n\t    \"OfflineCarGoal1Gymnasium-v0\": CDTCarGoal1Config,\n\t    \"OfflineCarGoal2Gymnasium-v0\": CDTCarGoal2Config,\n\t    \"OfflineCarPush1Gymnasium-v0\": CDTCarPush1Config,\n\t    \"OfflineCarPush2Gymnasium-v0\": CDTCarPush2Config,\n\t    # safety_gymnasium: point\n\t    \"OfflinePointButton1Gymnasium-v0\": CDTPointButton1Config,\n\t    \"OfflinePointButton2Gymnasium-v0\": CDTPointButton2Config,\n\t    \"OfflinePointCircle1Gymnasium-v0\": CDTPointCircle1Config,\n\t    \"OfflinePointCircle2Gymnasium-v0\": CDTPointCircle2Config,\n", "    \"OfflinePointGoal1Gymnasium-v0\": CDTPointGoal1Config,\n\t    \"OfflinePointGoal2Gymnasium-v0\": CDTPointGoal2Config,\n\t    \"OfflinePointPush1Gymnasium-v0\": CDTPointPush1Config,\n\t    \"OfflinePointPush2Gymnasium-v0\": CDTPointPush2Config,\n\t    # safety_gymnasium: velocity\n\t    \"OfflineAntVelocityGymnasium-v1\": CDTAntVelocityConfig,\n\t    \"OfflineHalfCheetahVelocityGymnasium-v1\": CDTHalfCheetahVelocityConfig,\n\t    \"OfflineHopperVelocityGymnasium-v1\": CDTHopperVelocityConfig,\n\t    \"OfflineSwimmerVelocityGymnasium-v1\": CDTSwimmerVelocityConfig,\n\t    \"OfflineWalker2dVelocityGymnasium-v1\": CDTWalker2dVelocityConfig,\n", "    # safe_metadrive\n\t    \"OfflineMetadrive-easysparse-v0\": CDTEasySparseConfig,\n\t    \"OfflineMetadrive-easymean-v0\": CDTEasyMeanConfig,\n\t    \"OfflineMetadrive-easydense-v0\": CDTEasyDenseConfig,\n\t    \"OfflineMetadrive-mediumsparse-v0\": CDTMediumSparseConfig,\n\t    \"OfflineMetadrive-mediummean-v0\": CDTMediumMeanConfig,\n\t    \"OfflineMetadrive-mediumdense-v0\": CDTMediumDenseConfig,\n\t    \"OfflineMetadrive-hardsparse-v0\": CDTHardSparseConfig,\n\t    \"OfflineMetadrive-hardmean-v0\": CDTHardMeanConfig,\n\t    \"OfflineMetadrive-harddense-v0\": CDTHardDenseConfig\n", "}\n"]}
{"filename": "examples/configs/bearl_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\tfrom pyrallis import field\n\t@dataclass\n\tclass BEARLTrainConfig:\n\t    # wandb params\n\t    project: str = \"OSRL-baselines\"\n\t    group: str = None\n\t    name: Optional[str] = None\n\t    prefix: Optional[str] = \"BEARL\"\n", "    suffix: Optional[str] = \"\"\n\t    logdir: Optional[str] = \"logs\"\n\t    verbose: bool = True\n\t    # dataset params\n\t    outliers_percent: float = None\n\t    noise_scale: float = None\n\t    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n\t    epsilon: float = None\n\t    density: float = 1.0\n\t    # training params\n", "    task: str = \"OfflineCarCircle-v0\"\n\t    dataset: str = None\n\t    seed: int = 0\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t    reward_scale: float = 0.1\n\t    cost_scale: float = 1\n\t    actor_lr: float = 0.001\n\t    critic_lr: float = 0.001\n\t    vae_lr: float = 0.001\n", "    cost_limit: int = 10\n\t    episode_len: int = 300\n\t    batch_size: int = 512\n\t    update_steps: int = 300_000\n\t    num_workers: int = 8\n\t    # model params\n\t    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n\t    c_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n\t    vae_hidden_sizes: int = 400\n\t    sample_action_num: int = 10\n", "    gamma: float = 0.99\n\t    tau: float = 0.005\n\t    beta: float = 0.5\n\t    lmbda: float = 0.75\n\t    mmd_sigma: float = 50\n\t    target_mmd_thresh: float = 0.05\n\t    num_samples_mmd_match: int = 10\n\t    start_update_policy_step: int = 0\n\t    kernel: str = \"gaussian\"  # or \"laplacian\"\n\t    num_q: int = 2\n", "    num_qc: int = 2\n\t    PID: List[float] = field(default=[0.1, 0.003, 0.001], is_mutable=True)\n\t    # evaluation params\n\t    eval_episodes: int = 10\n\t    eval_every: int = 2500\n\t@dataclass\n\tclass BEARLCarCircleConfig(BEARLTrainConfig):\n\t    pass\n\t@dataclass\n\tclass BEARLAntRunConfig(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineAntRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass BEARLDroneRunConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineDroneRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass BEARLDroneCircleConfig(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineDroneCircle-v0\"\n\t    episode_len: int = 300\n\t@dataclass\n\tclass BEARLCarRunConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass BEARLAntCircleConfig(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineAntCircle-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BEARLBallRunConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineBallRun-v0\"\n\t    episode_len: int = 100\n\t@dataclass\n\tclass BEARLBallCircleConfig(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineBallCircle-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass BEARLCarButton1Config(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarButton1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLCarButton2Config(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLCarCircle1Config(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BEARLCarCircle2Config(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BEARLCarGoal1Config(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLCarGoal2Config(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLCarPush1Config(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarPush1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLCarPush2Config(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineCarPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLPointButton1Config(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointButton1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLPointButton2Config(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflinePointButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLPointCircle1Config(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BEARLPointCircle2Config(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BEARLPointGoal1Config(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLPointGoal2Config(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLPointPush1Config(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointPush1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLPointPush2Config(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflinePointPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLAntVelocityConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLHalfCheetahVelocityConfig(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLHopperVelocityConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLSwimmerVelocityConfig(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLWalker2dVelocityConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BEARLEasySparseConfig(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineMetadrive-easysparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BEARLEasyMeanConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easymean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n", "@dataclass\n\tclass BEARLEasyDenseConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easydense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BEARLMediumSparseConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n", "    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BEARLMediumMeanConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediummean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BEARLMediumDenseConfig(BEARLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineMetadrive-mediumdense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BEARLHardSparseConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardsparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n", "@dataclass\n\tclass BEARLHardMeanConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardmean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BEARLHardDenseConfig(BEARLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-harddense-v0\"\n", "    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\tBEARL_DEFAULT_CONFIG = {\n\t    # bullet_safety_gym\n\t    \"OfflineCarCircle-v0\": BEARLCarCircleConfig,\n\t    \"OfflineAntRun-v0\": BEARLAntRunConfig,\n\t    \"OfflineDroneRun-v0\": BEARLDroneRunConfig,\n\t    \"OfflineDroneCircle-v0\": BEARLDroneCircleConfig,\n\t    \"OfflineCarRun-v0\": BEARLCarRunConfig,\n\t    \"OfflineAntCircle-v0\": BEARLAntCircleConfig,\n", "    \"OfflineBallCircle-v0\": BEARLBallCircleConfig,\n\t    \"OfflineBallRun-v0\": BEARLBallRunConfig,\n\t    # safety_gymnasium\n\t    \"OfflineCarButton1Gymnasium-v0\": BEARLCarButton1Config,\n\t    \"OfflineCarButton2Gymnasium-v0\": BEARLCarButton2Config,\n\t    \"OfflineCarCircle1Gymnasium-v0\": BEARLCarCircle1Config,\n\t    \"OfflineCarCircle2Gymnasium-v0\": BEARLCarCircle2Config,\n\t    \"OfflineCarGoal1Gymnasium-v0\": BEARLCarGoal1Config,\n\t    \"OfflineCarGoal2Gymnasium-v0\": BEARLCarGoal2Config,\n\t    \"OfflineCarPush1Gymnasium-v0\": BEARLCarPush1Config,\n", "    \"OfflineCarPush2Gymnasium-v0\": BEARLCarPush2Config,\n\t    # safety_gymnasium: point\n\t    \"OfflinePointButton1Gymnasium-v0\": BEARLPointButton1Config,\n\t    \"OfflinePointButton2Gymnasium-v0\": BEARLPointButton2Config,\n\t    \"OfflinePointCircle1Gymnasium-v0\": BEARLPointCircle1Config,\n\t    \"OfflinePointCircle2Gymnasium-v0\": BEARLPointCircle2Config,\n\t    \"OfflinePointGoal1Gymnasium-v0\": BEARLPointGoal1Config,\n\t    \"OfflinePointGoal2Gymnasium-v0\": BEARLPointGoal2Config,\n\t    \"OfflinePointPush1Gymnasium-v0\": BEARLPointPush1Config,\n\t    \"OfflinePointPush2Gymnasium-v0\": BEARLPointPush2Config,\n", "    # safety_gymnasium: velocity\n\t    \"OfflineAntVelocityGymnasium-v1\": BEARLAntVelocityConfig,\n\t    \"OfflineHalfCheetahVelocityGymnasium-v1\": BEARLHalfCheetahVelocityConfig,\n\t    \"OfflineHopperVelocityGymnasium-v1\": BEARLHopperVelocityConfig,\n\t    \"OfflineSwimmerVelocityGymnasium-v1\": BEARLSwimmerVelocityConfig,\n\t    \"OfflineWalker2dVelocityGymnasium-v1\": BEARLWalker2dVelocityConfig,\n\t    # safe_metadrive\n\t    \"OfflineMetadrive-easysparse-v0\": BEARLEasySparseConfig,\n\t    \"OfflineMetadrive-easymean-v0\": BEARLEasyMeanConfig,\n\t    \"OfflineMetadrive-easydense-v0\": BEARLEasyDenseConfig,\n", "    \"OfflineMetadrive-mediumsparse-v0\": BEARLMediumSparseConfig,\n\t    \"OfflineMetadrive-mediummean-v0\": BEARLMediumMeanConfig,\n\t    \"OfflineMetadrive-mediumdense-v0\": BEARLMediumDenseConfig,\n\t    \"OfflineMetadrive-hardsparse-v0\": BEARLHardSparseConfig,\n\t    \"OfflineMetadrive-hardmean-v0\": BEARLHardMeanConfig,\n\t    \"OfflineMetadrive-harddense-v0\": BEARLHardDenseConfig\n\t}\n"]}
{"filename": "examples/configs/bcql_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\tfrom pyrallis import field\n\t@dataclass\n\tclass BCQLTrainConfig:\n\t    # wandb params\n\t    project: str = \"OSRL-baselines\"\n\t    group: str = None\n\t    name: Optional[str] = None\n\t    prefix: Optional[str] = \"BCQL\"\n", "    suffix: Optional[str] = \"\"\n\t    logdir: Optional[str] = \"logs\"\n\t    verbose: bool = True\n\t    # dataset params\n\t    outliers_percent: float = None\n\t    noise_scale: float = None\n\t    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n\t    epsilon: float = None\n\t    density: float = 1.0\n\t    # training params\n", "    task: str = \"OfflineCarCircle-v0\"\n\t    dataset: str = None\n\t    seed: int = 0\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t    reward_scale: float = 0.1\n\t    cost_scale: float = 1\n\t    actor_lr: float = 0.001\n\t    critic_lr: float = 0.001\n\t    vae_lr: float = 0.001\n", "    phi: float = 0.05\n\t    lmbda: float = 0.75\n\t    beta: float = 0.5\n\t    cost_limit: int = 10\n\t    episode_len: int = 300\n\t    batch_size: int = 512\n\t    update_steps: int = 100_000\n\t    num_workers: int = 8\n\t    # model params\n\t    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n", "    c_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n\t    vae_hidden_sizes: int = 400\n\t    sample_action_num: int = 10\n\t    gamma: float = 0.99\n\t    tau: float = 0.005\n\t    num_q: int = 2\n\t    num_qc: int = 2\n\t    PID: List[float] = field(default=[0.1, 0.003, 0.001], is_mutable=True)\n\t    # evaluation params\n\t    eval_episodes: int = 10\n", "    eval_every: int = 2500\n\t@dataclass\n\tclass BCQLCarCircleConfig(BCQLTrainConfig):\n\t    pass\n\t@dataclass\n\tclass BCQLAntRunConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n", "class BCQLDroneRunConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineDroneRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass BCQLDroneCircleConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineDroneCircle-v0\"\n\t    episode_len: int = 300\n\t@dataclass\n", "class BCQLCarRunConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarRun-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n\tclass BCQLAntCircleConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntCircle-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n", "class BCQLBallRunConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineBallRun-v0\"\n\t    episode_len: int = 100\n\t@dataclass\n\tclass BCQLBallCircleConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineBallCircle-v0\"\n\t    episode_len: int = 200\n\t@dataclass\n", "class BCQLCarButton1Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarButton1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLCarButton2Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n", "class BCQLCarCircle1Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BCQLCarCircle2Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n", "class BCQLCarGoal1Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLCarGoal2Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n", "class BCQLCarPush1Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarPush1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLCarPush2Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineCarPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n", "class BCQLPointButton1Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointButton1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLPointButton2Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointButton2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n", "class BCQLPointCircle1Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n\tclass BCQLPointCircle2Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n\t    episode_len: int = 500\n\t@dataclass\n", "class BCQLPointGoal1Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLPointGoal2Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n", "class BCQLPointPush1Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointPush1Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLPointPush2Config(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflinePointPush2Gymnasium-v0\"\n\t    episode_len: int = 1000\n\t@dataclass\n", "class BCQLAntVelocityConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineAntVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLHalfCheetahVelocityConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n", "class BCQLHopperVelocityConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLSwimmerVelocityConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n", "class BCQLWalker2dVelocityConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n\t    episode_len: int = 1000\n\t@dataclass\n\tclass BCQLEasySparseConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easysparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n", "@dataclass\n\tclass BCQLEasyMeanConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easymean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCQLEasyDenseConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-easydense-v0\"\n", "    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCQLMediumSparseConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCQLMediumMeanConfig(BCQLTrainConfig):\n", "    # training params\n\t    task: str = \"OfflineMetadrive-mediummean-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCQLMediumDenseConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-mediumdense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n", "@dataclass\n\tclass BCQLHardSparseConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardsparse-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCQLHardMeanConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-hardmean-v0\"\n", "    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\t@dataclass\n\tclass BCQLHardDenseConfig(BCQLTrainConfig):\n\t    # training params\n\t    task: str = \"OfflineMetadrive-harddense-v0\"\n\t    episode_len: int = 1000\n\t    update_steps: int = 200_000\n\tBCQL_DEFAULT_CONFIG = {\n\t    # bullet_safety_gym\n", "    \"OfflineCarCircle-v0\": BCQLCarCircleConfig,\n\t    \"OfflineAntRun-v0\": BCQLAntRunConfig,\n\t    \"OfflineDroneRun-v0\": BCQLDroneRunConfig,\n\t    \"OfflineDroneCircle-v0\": BCQLDroneCircleConfig,\n\t    \"OfflineCarRun-v0\": BCQLCarRunConfig,\n\t    \"OfflineAntCircle-v0\": BCQLAntCircleConfig,\n\t    \"OfflineBallCircle-v0\": BCQLBallCircleConfig,\n\t    \"OfflineBallRun-v0\": BCQLBallRunConfig,\n\t    # safety_gymnasium: car\n\t    \"OfflineCarButton1Gymnasium-v0\": BCQLCarButton1Config,\n", "    \"OfflineCarButton2Gymnasium-v0\": BCQLCarButton2Config,\n\t    \"OfflineCarCircle1Gymnasium-v0\": BCQLCarCircle1Config,\n\t    \"OfflineCarCircle2Gymnasium-v0\": BCQLCarCircle2Config,\n\t    \"OfflineCarGoal1Gymnasium-v0\": BCQLCarGoal1Config,\n\t    \"OfflineCarGoal2Gymnasium-v0\": BCQLCarGoal2Config,\n\t    \"OfflineCarPush1Gymnasium-v0\": BCQLCarPush1Config,\n\t    \"OfflineCarPush2Gymnasium-v0\": BCQLCarPush2Config,\n\t    # safety_gymnasium: point\n\t    \"OfflinePointButton1Gymnasium-v0\": BCQLPointButton1Config,\n\t    \"OfflinePointButton2Gymnasium-v0\": BCQLPointButton2Config,\n", "    \"OfflinePointCircle1Gymnasium-v0\": BCQLPointCircle1Config,\n\t    \"OfflinePointCircle2Gymnasium-v0\": BCQLPointCircle2Config,\n\t    \"OfflinePointGoal1Gymnasium-v0\": BCQLPointGoal1Config,\n\t    \"OfflinePointGoal2Gymnasium-v0\": BCQLPointGoal2Config,\n\t    \"OfflinePointPush1Gymnasium-v0\": BCQLPointPush1Config,\n\t    \"OfflinePointPush2Gymnasium-v0\": BCQLPointPush2Config,\n\t    # safety_gymnasium: velocity\n\t    \"OfflineAntVelocityGymnasium-v1\": BCQLAntVelocityConfig,\n\t    \"OfflineHalfCheetahVelocityGymnasium-v1\": BCQLHalfCheetahVelocityConfig,\n\t    \"OfflineHopperVelocityGymnasium-v1\": BCQLHopperVelocityConfig,\n", "    \"OfflineSwimmerVelocityGymnasium-v1\": BCQLSwimmerVelocityConfig,\n\t    \"OfflineWalker2dVelocityGymnasium-v1\": BCQLWalker2dVelocityConfig,\n\t    # safe_metadrive\n\t    \"OfflineMetadrive-easysparse-v0\": BCQLEasySparseConfig,\n\t    \"OfflineMetadrive-easymean-v0\": BCQLEasyMeanConfig,\n\t    \"OfflineMetadrive-easydense-v0\": BCQLEasyDenseConfig,\n\t    \"OfflineMetadrive-mediumsparse-v0\": BCQLMediumSparseConfig,\n\t    \"OfflineMetadrive-mediummean-v0\": BCQLMediumMeanConfig,\n\t    \"OfflineMetadrive-mediumdense-v0\": BCQLMediumDenseConfig,\n\t    \"OfflineMetadrive-hardsparse-v0\": BCQLHardSparseConfig,\n", "    \"OfflineMetadrive-hardmean-v0\": BCQLHardMeanConfig,\n\t    \"OfflineMetadrive-harddense-v0\": BCQLHardDenseConfig\n\t}"]}
{"filename": "examples/eval/eval_cpq.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n\timport torch\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom pyrallis import field\n\tfrom osrl.algorithms import CPQ, CPQTrainer\n", "from osrl.common.exp_util import load_config_and_model, seed_all\n\t@dataclass\n\tclass EvalConfig:\n\t    path: str = \"log/.../checkpoint/model.pt\"\n\t    noise_scale: List[float] = None\n\t    eval_episodes: int = 20\n\t    best: bool = False\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t@pyrallis.wrap()\n", "def eval(args: EvalConfig):\n\t    cfg, model = load_config_and_model(args.path, args.best)\n\t    seed_all(cfg[\"seed\"])\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    env = wrap_env(\n\t        env=gym.make(cfg[\"task\"]),\n\t        reward_scale=cfg[\"reward_scale\"],\n\t    )\n\t    env = OfflineEnvWrapper(env)\n", "    env.set_target_cost(cfg[\"cost_limit\"])\n\t    cpq_model = CPQ(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n\t        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n\t        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n\t        sample_action_num=cfg[\"sample_action_num\"],\n\t        gamma=cfg[\"gamma\"],\n", "        tau=cfg[\"tau\"],\n\t        beta=cfg[\"beta\"],\n\t        num_q=cfg[\"num_q\"],\n\t        num_qc=cfg[\"num_qc\"],\n\t        qc_scalar=cfg[\"qc_scalar\"],\n\t        cost_limit=cfg[\"cost_limit\"],\n\t        episode_len=cfg[\"episode_len\"],\n\t        device=args.device,\n\t    )\n\t    cpq_model.load_state_dict(model[\"model_state\"])\n", "    cpq_model.to(args.device)\n\t    trainer = CPQTrainer(cpq_model,\n\t                         env,\n\t                         reward_scale=cfg[\"reward_scale\"],\n\t                         cost_scale=cfg[\"cost_scale\"],\n\t                         device=args.device)\n\t    ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n\t    print(\n\t        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n", "    )\n\tif __name__ == \"__main__\":\n\t    eval()\n"]}
{"filename": "examples/eval/eval_bearl.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n\timport torch\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom pyrallis import field\n\tfrom osrl.algorithms import BEARL, BEARLTrainer\n", "from osrl.common.exp_util import load_config_and_model, seed_all\n\t@dataclass\n\tclass EvalConfig:\n\t    path: str = \"log/.../checkpoint/model.pt\"\n\t    noise_scale: List[float] = None\n\t    eval_episodes: int = 20\n\t    best: bool = False\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t@pyrallis.wrap()\n", "def eval(args: EvalConfig):\n\t    cfg, model = load_config_and_model(args.path, args.best)\n\t    seed_all(cfg[\"seed\"])\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    env = wrap_env(\n\t        env=gym.make(cfg[\"task\"]),\n\t        reward_scale=cfg[\"reward_scale\"],\n\t    )\n\t    env = OfflineEnvWrapper(env)\n", "    env.set_target_cost(cfg[\"cost_limit\"])\n\t    # model & optimizer & scheduler setup\n\t    bear_model = BEARL(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n\t        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n\t        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n\t        sample_action_num=cfg[\"sample_action_num\"],\n", "        gamma=cfg[\"gamma\"],\n\t        tau=cfg[\"tau\"],\n\t        beta=cfg[\"beta\"],\n\t        lmbda=cfg[\"lmbda\"],\n\t        mmd_sigma=cfg[\"mmd_sigma\"],\n\t        target_mmd_thresh=cfg[\"target_mmd_thresh\"],\n\t        start_update_policy_step=cfg[\"start_update_policy_step\"],\n\t        num_q=cfg[\"num_q\"],\n\t        num_qc=cfg[\"num_qc\"],\n\t        PID=cfg[\"PID\"],\n", "        cost_limit=cfg[\"cost_limit\"],\n\t        episode_len=cfg[\"episode_len\"],\n\t        device=args.device,\n\t    )\n\t    bear_model.load_state_dict(model[\"model_state\"])\n\t    bear_model.to(args.device)\n\t    trainer = BEARLTrainer(bear_model,\n\t                           env,\n\t                           reward_scale=cfg[\"reward_scale\"],\n\t                           cost_scale=cfg[\"cost_scale\"],\n", "                           device=args.device)\n\t    ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n\t    print(\n\t        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n\t    )\n\tif __name__ == \"__main__\":\n\t    eval()\n"]}
{"filename": "examples/eval/eval_bc.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n\timport torch\n\tfrom pyrallis import field\n\tfrom osrl.algorithms import BC, BCTrainer\n\tfrom osrl.common.exp_util import load_config_and_model, seed_all\n", "@dataclass\n\tclass EvalConfig:\n\t    path: str = \"log/.../checkpoint/model.pt\"\n\t    noise_scale: List[float] = None\n\t    costs: List[float] = field(default=[1, 10, 20, 30, 40], is_mutable=True)\n\t    eval_episodes: int = 20\n\t    best: bool = False\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t@pyrallis.wrap()\n", "def eval(args: EvalConfig):\n\t    cfg, model = load_config_and_model(args.path, args.best)\n\t    seed_all(cfg[\"seed\"])\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    env = gym.make(cfg[\"task\"])\n\t    env.set_target_cost(cfg[\"cost_limit\"])\n\t    # model & optimizer & scheduler setup\n\t    state_dim = env.observation_space.shape[0]\n\t    if cfg[\"bc_mode\"] == \"multi-task\":\n", "        state_dim += 1\n\t    bc_model = BC(\n\t        state_dim=state_dim,\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n\t        episode_len=cfg[\"episode_len\"],\n\t        device=args.device,\n\t    )\n\t    bc_model.load_state_dict(model[\"model_state\"])\n", "    bc_model.to(args.device)\n\t    trainer = BCTrainer(bc_model,\n\t                        env,\n\t                        bc_mode=cfg[\"bc_mode\"],\n\t                        cost_limit=cfg[\"cost_limit\"],\n\t                        device=args.device)\n\t    if cfg[\"bc_mode\"] == \"multi-task\":\n\t        for target_cost in args.costs:\n\t            env.set_target_cost(target_cost)\n\t            trainer.set_target_cost(target_cost)\n", "            ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t            normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n\t            print(\n\t                f\"Eval reward: {ret}, normalized reward: {normalized_ret}; target cost {target_cost}, real cost {cost}, normalized cost: {normalized_cost}\"\n\t            )\n\t    else:\n\t        ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t        normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n\t        print(\n\t            f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n", "        )\n\tif __name__ == \"__main__\":\n\t    eval()\n"]}
{"filename": "examples/eval/eval_bcql.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n\timport torch\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom pyrallis import field\n\tfrom osrl.algorithms import BCQL, BCQLTrainer\n", "from osrl.common.exp_util import load_config_and_model, seed_all\n\t@dataclass\n\tclass EvalConfig:\n\t    path: str = \"log/.../checkpoint/model.pt\"\n\t    noise_scale: List[float] = None\n\t    eval_episodes: int = 20\n\t    best: bool = False\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t@pyrallis.wrap()\n", "def eval(args: EvalConfig):\n\t    cfg, model = load_config_and_model(args.path, args.best)\n\t    seed_all(cfg[\"seed\"])\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    env = wrap_env(\n\t        env=gym.make(cfg[\"task\"]),\n\t        reward_scale=cfg[\"reward_scale\"],\n\t    )\n\t    env = OfflineEnvWrapper(env)\n", "    env.set_target_cost(cfg[\"cost_limit\"])\n\t    bcql_model = BCQL(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n\t        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n\t        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n\t        sample_action_num=cfg[\"sample_action_num\"],\n\t        PID=cfg[\"PID\"],\n", "        gamma=cfg[\"gamma\"],\n\t        tau=cfg[\"tau\"],\n\t        lmbda=cfg[\"lmbda\"],\n\t        beta=cfg[\"beta\"],\n\t        phi=cfg[\"phi\"],\n\t        num_q=cfg[\"num_q\"],\n\t        num_qc=cfg[\"num_qc\"],\n\t        cost_limit=cfg[\"cost_limit\"],\n\t        episode_len=cfg[\"episode_len\"],\n\t        device=args.device,\n", "    )\n\t    bcql_model.load_state_dict(model[\"model_state\"])\n\t    bcql_model.to(args.device)\n\t    trainer = BCQLTrainer(bcql_model,\n\t                          env,\n\t                          reward_scale=cfg[\"reward_scale\"],\n\t                          cost_scale=cfg[\"cost_scale\"],\n\t                          device=args.device)\n\t    ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n", "    print(\n\t        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n\t    )\n\tif __name__ == \"__main__\":\n\t    eval()\n"]}
{"filename": "examples/eval/eval_coptidice.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n\timport torch\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom pyrallis import field\n\tfrom osrl.algorithms import COptiDICE, COptiDICETrainer\n", "from osrl.common.exp_util import load_config_and_model, seed_all\n\t@dataclass\n\tclass EvalConfig:\n\t    path: str = \"log/.../checkpoint/model.pt\"\n\t    noise_scale: List[float] = None\n\t    eval_episodes: int = 20\n\t    best: bool = False\n\t    device: str = \"cpu\"\n\t    threads: int = 4\n\t@pyrallis.wrap()\n", "def eval(args: EvalConfig):\n\t    cfg, model = load_config_and_model(args.path, args.best)\n\t    seed_all(cfg[\"seed\"])\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    env = wrap_env(\n\t        env=gym.make(cfg[\"task\"]),\n\t        reward_scale=cfg[\"reward_scale\"],\n\t    )\n\t    env = OfflineEnvWrapper(env)\n", "    env.set_target_cost(cfg[\"cost_limit\"])\n\t    # setup model\n\t    coptidice_model = COptiDICE(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        f_type=cfg[\"f_type\"],\n\t        init_state_propotion=1.0,\n\t        observations_std=np.array([0]),\n\t        actions_std=np.array([0]),\n", "        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n\t        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n\t        gamma=cfg[\"gamma\"],\n\t        alpha=cfg[\"alpha\"],\n\t        cost_ub_epsilon=cfg[\"cost_ub_epsilon\"],\n\t        num_nu=cfg[\"num_nu\"],\n\t        num_chi=cfg[\"num_chi\"],\n\t        cost_limit=cfg[\"cost_limit\"],\n\t        episode_len=cfg[\"episode_len\"],\n\t        device=args.device,\n", "    )\n\t    coptidice_model.load_state_dict(model[\"model_state\"])\n\t    coptidice_model.to(args.device)\n\t    trainer = COptiDICETrainer(coptidice_model,\n\t                               env,\n\t                               reward_scale=cfg[\"reward_scale\"],\n\t                               cost_scale=cfg[\"cost_scale\"],\n\t                               device=args.device)\n\t    ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n", "    print(\n\t        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n\t    )\n\tif __name__ == \"__main__\":\n\t    eval()\n"]}
{"filename": "examples/eval/eval_cdt.py", "chunked_list": ["from dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n\timport torch\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom pyrallis import field\n\tfrom osrl.algorithms import CDT, CDTTrainer\n", "from osrl.common.exp_util import load_config_and_model, seed_all\n\t@dataclass\n\tclass EvalConfig:\n\t    path: str = \"log/.../checkpoint/model.pt\"\n\t    returns: List[float] = field(default=[300, 400, 500], is_mutable=True)\n\t    costs: List[float] = field(default=[10, 10, 10], is_mutable=True)\n\t    noise_scale: List[float] = None\n\t    eval_episodes: int = 20\n\t    best: bool = False\n\t    device: str = \"cpu\"\n", "    threads: int = 4\n\t@pyrallis.wrap()\n\tdef eval(args: EvalConfig):\n\t    cfg, model = load_config_and_model(args.path, args.best)\n\t    seed_all(cfg[\"seed\"])\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    env = wrap_env(\n\t        env=gym.make(cfg[\"task\"]),\n\t        reward_scale=cfg[\"reward_scale\"],\n", "    )\n\t    env = OfflineEnvWrapper(env)\n\t    env.set_target_cost(cfg[\"cost_limit\"])\n\t    target_entropy = -env.action_space.shape[0]\n\t    # model & optimizer & scheduler setup\n\t    cdt_model = CDT(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        embedding_dim=cfg[\"embedding_dim\"],\n", "        seq_len=cfg[\"seq_len\"],\n\t        episode_len=cfg[\"episode_len\"],\n\t        num_layers=cfg[\"num_layers\"],\n\t        num_heads=cfg[\"num_heads\"],\n\t        attention_dropout=cfg[\"attention_dropout\"],\n\t        residual_dropout=cfg[\"residual_dropout\"],\n\t        embedding_dropout=cfg[\"embedding_dropout\"],\n\t        time_emb=cfg[\"time_emb\"],\n\t        use_rew=cfg[\"use_rew\"],\n\t        use_cost=cfg[\"use_cost\"],\n", "        cost_transform=cfg[\"cost_transform\"],\n\t        add_cost_feat=cfg[\"add_cost_feat\"],\n\t        mul_cost_feat=cfg[\"mul_cost_feat\"],\n\t        cat_cost_feat=cfg[\"cat_cost_feat\"],\n\t        action_head_layers=cfg[\"action_head_layers\"],\n\t        cost_prefix=cfg[\"cost_prefix\"],\n\t        stochastic=cfg[\"stochastic\"],\n\t        init_temperature=cfg[\"init_temperature\"],\n\t        target_entropy=target_entropy,\n\t    )\n", "    cdt_model.load_state_dict(model[\"model_state\"])\n\t    cdt_model.to(args.device)\n\t    trainer = CDTTrainer(cdt_model,\n\t                         env,\n\t                         reward_scale=cfg[\"reward_scale\"],\n\t                         cost_scale=cfg[\"cost_scale\"],\n\t                         cost_reverse=cfg[\"cost_reverse\"],\n\t                         device=args.device)\n\t    rets = args.returns\n\t    costs = args.costs\n", "    assert len(rets) == len(\n\t        costs\n\t    ), f\"The length of returns {len(rets)} should be equal to costs {len(costs)}!\"\n\t    for target_ret, target_cost in zip(rets, costs):\n\t        seed_all(cfg[\"seed\"])\n\t        ret, cost, length = trainer.evaluate(args.eval_episodes,\n\t                                             target_ret * cfg[\"reward_scale\"],\n\t                                             target_cost * cfg[\"cost_scale\"])\n\t        normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n\t        print(\n", "            f\"Target reward {target_ret}, real reward {ret}, normalized reward: {normalized_ret}; target cost {target_cost}, real cost {cost}, normalized cost: {normalized_cost}\"\n\t        )\n\tif __name__ == \"__main__\":\n\t    eval()\n"]}
{"filename": "examples/train/train_bc.py", "chunked_list": ["import os\n\timport uuid\n\timport types\n\tfrom dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport bullet_safety_gym  # noqa\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n", "import torch\n\tfrom dsrl.infos import DENSITY_CFG\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom fsrl.utils import WandbLogger\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm.auto import trange  # noqa\n\tfrom examples.configs.bc_configs import BC_DEFAULT_CONFIG, BCTrainConfig\n\tfrom osrl.algorithms import BC, BCTrainer\n\tfrom osrl.common import TransitionDataset\n\tfrom osrl.common.dataset import process_bc_dataset\n", "from osrl.common.exp_util import auto_name, seed_all\n\t@pyrallis.wrap()\n\tdef train(args: BCTrainConfig):\n\t    # update config\n\t    cfg, old_cfg = asdict(args), asdict(BCTrainConfig())\n\t    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n\t    cfg = asdict(BC_DEFAULT_CONFIG[args.task]())\n\t    cfg.update(differing_values)\n\t    args = types.SimpleNamespace(**cfg)\n\t    # setup logger\n", "    default_cfg = asdict(BC_DEFAULT_CONFIG[args.task]())\n\t    if args.name is None:\n\t        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n\t    if args.group is None:\n\t        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n\t    if args.logdir is not None:\n\t        args.logdir = os.path.join(args.logdir, args.group, args.name)\n\t    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n\t    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n\t    logger.save_config(cfg, verbose=args.verbose)\n", "    # set seed\n\t    seed_all(args.seed)\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    # the cost scale is down in trainer rollout\n\t    env = gym.make(args.task)\n\t    data = env.get_dataset()\n\t    env.set_target_cost(args.cost_limit)\n\t    cbins, rbins, max_npb, min_npb = None, None, None, None\n\t    if args.density != 1.0:\n", "        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n\t        cbins = density_cfg[\"cbins\"]\n\t        rbins = density_cfg[\"rbins\"]\n\t        max_npb = density_cfg[\"max_npb\"]\n\t        min_npb = density_cfg[\"min_npb\"]\n\t    data = env.pre_process_data(data,\n\t                                args.outliers_percent,\n\t                                args.noise_scale,\n\t                                args.inpaint_ranges,\n\t                                args.epsilon,\n", "                                args.density,\n\t                                cbins=cbins,\n\t                                rbins=rbins,\n\t                                max_npb=max_npb,\n\t                                min_npb=min_npb)\n\t    process_bc_dataset(data, args.cost_limit, args.gamma, args.bc_mode)\n\t    # model & optimizer & scheduler setup\n\t    state_dim = env.observation_space.shape[0]\n\t    if args.bc_mode == \"multi-task\":\n\t        state_dim += 1\n", "    model = BC(\n\t        state_dim=state_dim,\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        a_hidden_sizes=args.a_hidden_sizes,\n\t        episode_len=args.episode_len,\n\t        device=args.device,\n\t    )\n\t    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\t    def checkpoint_fn():\n", "        return {\"model_state\": model.state_dict()}\n\t    logger.setup_checkpoint_fn(checkpoint_fn)\n\t    trainer = BCTrainer(model,\n\t                        env,\n\t                        logger=logger,\n\t                        actor_lr=args.actor_lr,\n\t                        bc_mode=args.bc_mode,\n\t                        cost_limit=args.cost_limit,\n\t                        device=args.device)\n\t    trainloader = DataLoader(\n", "        TransitionDataset(data),\n\t        batch_size=args.batch_size,\n\t        pin_memory=True,\n\t        num_workers=args.num_workers,\n\t    )\n\t    trainloader_iter = iter(trainloader)\n\t    # for saving the best\n\t    best_reward = -np.inf\n\t    best_cost = np.inf\n\t    best_idx = 0\n", "    for step in trange(args.update_steps, desc=\"Training\"):\n\t        batch = next(trainloader_iter)\n\t        observations, _, actions, _, _, _ = [b.to(args.device) for b in batch]\n\t        trainer.train_one_step(observations, actions)\n\t        # evaluation\n\t        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n\t            ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\t            # save the current weight\n\t            logger.save_checkpoint()\n", "            # save the best weight\n\t            if cost < best_cost or (cost == best_cost and ret > best_reward):\n\t                best_cost = cost\n\t                best_reward = ret\n\t                best_idx = step\n\t                logger.save_checkpoint(suffix=\"best\")\n\t            logger.store(tab=\"train\", best_idx=best_idx)\n\t            logger.write(step, display=False)\n\t        else:\n\t            logger.write_without_reset(step)\n", "if __name__ == \"__main__\":\n\t    train()\n"]}
{"filename": "examples/train/train_cpq.py", "chunked_list": ["import os\n\timport uuid\n\timport types\n\tfrom dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport bullet_safety_gym  # noqa\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n", "import torch\n\tfrom dsrl.infos import DENSITY_CFG\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom fsrl.utils import WandbLogger\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm.auto import trange  # noqa\n\tfrom examples.configs.cpq_configs import CPQ_DEFAULT_CONFIG, CPQTrainConfig\n\tfrom osrl.algorithms import CPQ, CPQTrainer\n\tfrom osrl.common import TransitionDataset\n\tfrom osrl.common.exp_util import auto_name, seed_all\n", "@pyrallis.wrap()\n\tdef train(args: CPQTrainConfig):\n\t    # update config\n\t    cfg, old_cfg = asdict(args), asdict(CPQTrainConfig())\n\t    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n\t    cfg = asdict(CPQ_DEFAULT_CONFIG[args.task]())\n\t    cfg.update(differing_values)\n\t    args = types.SimpleNamespace(**cfg)\n\t    # setup logger\n\t    default_cfg = asdict(CPQ_DEFAULT_CONFIG[args.task]())\n", "    if args.name is None:\n\t        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n\t    if args.group is None:\n\t        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n\t    if args.logdir is not None:\n\t        args.logdir = os.path.join(args.logdir, args.group, args.name)\n\t    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n\t    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n\t    logger.save_config(cfg, verbose=args.verbose)\n\t    # set seed\n", "    seed_all(args.seed)\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    # initialize environment\n\t    env = gym.make(args.task)\n\t    # pre-process offline dataset\n\t    data = env.get_dataset()\n\t    env.set_target_cost(args.cost_limit)\n\t    cbins, rbins, max_npb, min_npb = None, None, None, None\n\t    if args.density != 1.0:\n", "        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n\t        cbins = density_cfg[\"cbins\"]\n\t        rbins = density_cfg[\"rbins\"]\n\t        max_npb = density_cfg[\"max_npb\"]\n\t        min_npb = density_cfg[\"min_npb\"]\n\t    data = env.pre_process_data(data,\n\t                                args.outliers_percent,\n\t                                args.noise_scale,\n\t                                args.inpaint_ranges,\n\t                                args.epsilon,\n", "                                args.density,\n\t                                cbins=cbins,\n\t                                rbins=rbins,\n\t                                max_npb=max_npb,\n\t                                min_npb=min_npb)\n\t    # wrapper\n\t    env = wrap_env(\n\t        env=env,\n\t        reward_scale=args.reward_scale,\n\t    )\n", "    env = OfflineEnvWrapper(env)\n\t    # model & optimizer setup\n\t    model = CPQ(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        a_hidden_sizes=args.a_hidden_sizes,\n\t        c_hidden_sizes=args.c_hidden_sizes,\n\t        vae_hidden_sizes=args.vae_hidden_sizes,\n\t        sample_action_num=args.sample_action_num,\n", "        gamma=args.gamma,\n\t        tau=args.tau,\n\t        beta=args.beta,\n\t        num_q=args.num_q,\n\t        num_qc=args.num_qc,\n\t        qc_scalar=args.qc_scalar,\n\t        cost_limit=args.cost_limit,\n\t        episode_len=args.episode_len,\n\t        device=args.device,\n\t    )\n", "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\t    def checkpoint_fn():\n\t        return {\"model_state\": model.state_dict()}\n\t    logger.setup_checkpoint_fn(checkpoint_fn)\n\t    trainer = CPQTrainer(model,\n\t                         env,\n\t                         logger=logger,\n\t                         actor_lr=args.actor_lr,\n\t                         critic_lr=args.critic_lr,\n\t                         alpha_lr=args.alpha_lr,\n", "                         vae_lr=args.vae_lr,\n\t                         reward_scale=args.reward_scale,\n\t                         cost_scale=args.cost_scale,\n\t                         device=args.device)\n\t    dataset = TransitionDataset(data,\n\t                                reward_scale=args.reward_scale,\n\t                                cost_scale=args.cost_scale)\n\t    trainloader = DataLoader(\n\t        dataset,\n\t        batch_size=args.batch_size,\n", "        pin_memory=True,\n\t        num_workers=args.num_workers,\n\t    )\n\t    trainloader_iter = iter(trainloader)\n\t    # for saving the best\n\t    best_reward = -np.inf\n\t    best_cost = np.inf\n\t    best_idx = 0\n\t    for step in trange(args.update_steps, desc=\"Training\"):\n\t        batch = next(trainloader_iter)\n", "        observations, next_observations, actions, rewards, costs, done = [\n\t            b.to(args.device) for b in batch\n\t        ]\n\t        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n\t                               done)\n\t        # evaluation\n\t        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n\t            ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\t            # save the current weight\n", "            logger.save_checkpoint()\n\t            # save the best weight\n\t            if cost < best_cost or (cost == best_cost and ret > best_reward):\n\t                best_cost = cost\n\t                best_reward = ret\n\t                best_idx = step\n\t                logger.save_checkpoint(suffix=\"best\")\n\t            logger.store(tab=\"train\", best_idx=best_idx)\n\t            logger.write(step, display=False)\n\t        else:\n", "            logger.write_without_reset(step)\n\tif __name__ == \"__main__\":\n\t    train()\n"]}
{"filename": "examples/train/__init__.py", "chunked_list": []}
{"filename": "examples/train/train_bcql.py", "chunked_list": ["import os\n\timport uuid\n\timport types\n\tfrom dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport bullet_safety_gym  # noqa\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n", "import torch\n\tfrom dsrl.infos import DENSITY_CFG\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom fsrl.utils import WandbLogger\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm.auto import trange  # noqa\n\tfrom examples.configs.bcql_configs import BCQL_DEFAULT_CONFIG, BCQLTrainConfig\n\tfrom osrl.algorithms import BCQL, BCQLTrainer\n\tfrom osrl.common import TransitionDataset\n\tfrom osrl.common.exp_util import auto_name, seed_all\n", "@pyrallis.wrap()\n\tdef train(args: BCQLTrainConfig):\n\t    # update config\n\t    cfg, old_cfg = asdict(args), asdict(BCQLTrainConfig())\n\t    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n\t    cfg = asdict(BCQL_DEFAULT_CONFIG[args.task]())\n\t    cfg.update(differing_values)\n\t    args = types.SimpleNamespace(**cfg)\n\t    # setup logger\n\t    default_cfg = asdict(BCQL_DEFAULT_CONFIG[args.task]())\n", "    if args.name is None:\n\t        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n\t    if args.group is None:\n\t        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n\t    if args.logdir is not None:\n\t        args.logdir = os.path.join(args.logdir, args.group, args.name)\n\t    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n\t    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n\t    logger.save_config(cfg, verbose=args.verbose)\n\t    # set seed\n", "    seed_all(args.seed)\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    # initialize environment\n\t    env = gym.make(args.task)\n\t    # pre-process offline dataset\n\t    data = env.get_dataset()\n\t    env.set_target_cost(args.cost_limit)\n\t    cbins, rbins, max_npb, min_npb = None, None, None, None\n\t    if args.density != 1.0:\n", "        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n\t        cbins = density_cfg[\"cbins\"]\n\t        rbins = density_cfg[\"rbins\"]\n\t        max_npb = density_cfg[\"max_npb\"]\n\t        min_npb = density_cfg[\"min_npb\"]\n\t    data = env.pre_process_data(data,\n\t                                args.outliers_percent,\n\t                                args.noise_scale,\n\t                                args.inpaint_ranges,\n\t                                args.epsilon,\n", "                                args.density,\n\t                                cbins=cbins,\n\t                                rbins=rbins,\n\t                                max_npb=max_npb,\n\t                                min_npb=min_npb)\n\t    # wrapper\n\t    env = wrap_env(\n\t        env=env,\n\t        reward_scale=args.reward_scale,\n\t    )\n", "    env = OfflineEnvWrapper(env)\n\t    # model & optimizer setup\n\t    model = BCQL(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        a_hidden_sizes=args.a_hidden_sizes,\n\t        c_hidden_sizes=args.c_hidden_sizes,\n\t        vae_hidden_sizes=args.vae_hidden_sizes,\n\t        sample_action_num=args.sample_action_num,\n", "        PID=args.PID,\n\t        gamma=args.gamma,\n\t        tau=args.tau,\n\t        lmbda=args.lmbda,\n\t        beta=args.beta,\n\t        phi=args.phi,\n\t        num_q=args.num_q,\n\t        num_qc=args.num_qc,\n\t        cost_limit=args.cost_limit,\n\t        episode_len=args.episode_len,\n", "        device=args.device,\n\t    )\n\t    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\t    def checkpoint_fn():\n\t        return {\"model_state\": model.state_dict()}\n\t    logger.setup_checkpoint_fn(checkpoint_fn)\n\t    # trainer\n\t    trainer = BCQLTrainer(model,\n\t                          env,\n\t                          logger=logger,\n", "                          actor_lr=args.actor_lr,\n\t                          critic_lr=args.critic_lr,\n\t                          vae_lr=args.vae_lr,\n\t                          reward_scale=args.reward_scale,\n\t                          cost_scale=args.cost_scale,\n\t                          device=args.device)\n\t    # initialize pytorch dataloader\n\t    dataset = TransitionDataset(data,\n\t                                reward_scale=args.reward_scale,\n\t                                cost_scale=args.cost_scale)\n", "    trainloader = DataLoader(\n\t        dataset,\n\t        batch_size=args.batch_size,\n\t        pin_memory=True,\n\t        num_workers=args.num_workers,\n\t    )\n\t    trainloader_iter = iter(trainloader)\n\t    # for saving the best\n\t    best_reward = -np.inf\n\t    best_cost = np.inf\n", "    best_idx = 0\n\t    # training\n\t    for step in trange(args.update_steps, desc=\"Training\"):\n\t        batch = next(trainloader_iter)\n\t        observations, next_observations, actions, rewards, costs, done = [\n\t            b.to(args.device) for b in batch\n\t        ]\n\t        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n\t                               done)\n\t        # evaluation\n", "        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n\t            ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\t            # save the current weight\n\t            logger.save_checkpoint()\n\t            # save the best weight\n\t            if cost < best_cost or (cost == best_cost and ret > best_reward):\n\t                best_cost = cost\n\t                best_reward = ret\n\t                best_idx = step\n", "                logger.save_checkpoint(suffix=\"best\")\n\t            logger.store(tab=\"train\", best_idx=best_idx)\n\t            logger.write(step, display=False)\n\t        else:\n\t            logger.write_without_reset(step)\n\tif __name__ == \"__main__\":\n\t    train()\n"]}
{"filename": "examples/train/train_bearl.py", "chunked_list": ["import os\n\timport uuid\n\timport types\n\tfrom dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport bullet_safety_gym  # noqa\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n", "import torch\n\tfrom dsrl.infos import DENSITY_CFG\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom fsrl.utils import WandbLogger\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm.auto import trange  # noqa\n\tfrom examples.configs.bearl_configs import BEARL_DEFAULT_CONFIG, BEARLTrainConfig\n\tfrom osrl.algorithms import BEARL, BEARLTrainer\n\tfrom osrl.common import TransitionDataset\n\tfrom osrl.common.exp_util import auto_name, seed_all\n", "@pyrallis.wrap()\n\tdef train(args: BEARLTrainConfig):\n\t    # update config\n\t    cfg, old_cfg = asdict(args), asdict(BEARLTrainConfig())\n\t    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n\t    cfg = asdict(BEARL_DEFAULT_CONFIG[args.task]())\n\t    cfg.update(differing_values)\n\t    args = types.SimpleNamespace(**cfg)\n\t    # setup logger\n\t    default_cfg = asdict(BEARL_DEFAULT_CONFIG[args.task]())\n", "    if args.name is None:\n\t        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n\t    if args.group is None:\n\t        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n\t    if args.logdir is not None:\n\t        args.logdir = os.path.join(args.logdir, args.group, args.name)\n\t    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n\t    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n\t    logger.save_config(cfg, verbose=args.verbose)\n\t    # set seed\n", "    seed_all(args.seed)\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    # initialize environment\n\t    env = gym.make(args.task)\n\t    # pre-process offline dataset\n\t    data = env.get_dataset()\n\t    env.set_target_cost(args.cost_limit)\n\t    cbins, rbins, max_npb, min_npb = None, None, None, None\n\t    if args.density != 1.0:\n", "        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n\t        cbins = density_cfg[\"cbins\"]\n\t        rbins = density_cfg[\"rbins\"]\n\t        max_npb = density_cfg[\"max_npb\"]\n\t        min_npb = density_cfg[\"min_npb\"]\n\t    data = env.pre_process_data(data,\n\t                                args.outliers_percent,\n\t                                args.noise_scale,\n\t                                args.inpaint_ranges,\n\t                                args.epsilon,\n", "                                args.density,\n\t                                cbins=cbins,\n\t                                rbins=rbins,\n\t                                max_npb=max_npb,\n\t                                min_npb=min_npb)\n\t    # wrapper\n\t    env = wrap_env(\n\t        env=env,\n\t        reward_scale=args.reward_scale,\n\t    )\n", "    env = OfflineEnvWrapper(env)\n\t    # model & optimizer setup\n\t    model = BEARL(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        a_hidden_sizes=args.a_hidden_sizes,\n\t        c_hidden_sizes=args.c_hidden_sizes,\n\t        vae_hidden_sizes=args.vae_hidden_sizes,\n\t        sample_action_num=args.sample_action_num,\n", "        gamma=args.gamma,\n\t        tau=args.tau,\n\t        beta=args.beta,\n\t        lmbda=args.lmbda,\n\t        mmd_sigma=args.mmd_sigma,\n\t        target_mmd_thresh=args.target_mmd_thresh,\n\t        start_update_policy_step=args.start_update_policy_step,\n\t        num_q=args.num_q,\n\t        num_qc=args.num_qc,\n\t        PID=args.PID,\n", "        cost_limit=args.cost_limit,\n\t        episode_len=args.episode_len,\n\t        device=args.device,\n\t    )\n\t    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\t    def checkpoint_fn():\n\t        return {\"model_state\": model.state_dict()}\n\t    logger.setup_checkpoint_fn(checkpoint_fn)\n\t    # trainer\n\t    trainer = BEARLTrainer(model,\n", "                           env,\n\t                           logger=logger,\n\t                           actor_lr=args.actor_lr,\n\t                           critic_lr=args.critic_lr,\n\t                           vae_lr=args.vae_lr,\n\t                           reward_scale=args.reward_scale,\n\t                           cost_scale=args.cost_scale,\n\t                           device=args.device)\n\t    # initialize pytorch dataloader\n\t    dataset = TransitionDataset(data,\n", "                                reward_scale=args.reward_scale,\n\t                                cost_scale=args.cost_scale)\n\t    trainloader = DataLoader(\n\t        dataset,\n\t        batch_size=args.batch_size,\n\t        pin_memory=True,\n\t        num_workers=args.num_workers,\n\t    )\n\t    trainloader_iter = iter(trainloader)\n\t    # for saving the best\n", "    best_reward = -np.inf\n\t    best_cost = np.inf\n\t    best_idx = 0\n\t    # training\n\t    for step in trange(args.update_steps, desc=\"Training\"):\n\t        batch = next(trainloader_iter)\n\t        observations, next_observations, actions, rewards, costs, done = [\n\t            b.to(args.device) for b in batch\n\t        ]\n\t        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n", "                               done)\n\t        # evaluation\n\t        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n\t            ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\t            # save the current weight\n\t            logger.save_checkpoint()\n\t            # save the best weight\n\t            if cost < best_cost or (cost == best_cost and ret > best_reward):\n\t                best_cost = cost\n", "                best_reward = ret\n\t                best_idx = step\n\t                logger.save_checkpoint(suffix=\"best\")\n\t            logger.store(tab=\"train\", best_idx=best_idx)\n\t            logger.write(step, display=False)\n\t        else:\n\t            logger.write_without_reset(step)\n\tif __name__ == \"__main__\":\n\t    train()\n"]}
{"filename": "examples/train/train_cdt.py", "chunked_list": ["import os\n\timport uuid\n\timport types\n\tfrom dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport bullet_safety_gym  # noqa\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n", "import torch\n\tfrom dsrl.infos import DENSITY_CFG\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom fsrl.utils import WandbLogger\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm.auto import trange  # noqa\n\tfrom examples.configs.cdt_configs import CDT_DEFAULT_CONFIG, CDTTrainConfig\n\tfrom osrl.algorithms import CDT, CDTTrainer\n\tfrom osrl.common import SequenceDataset\n\tfrom osrl.common.exp_util import auto_name, seed_all\n", "@pyrallis.wrap()\n\tdef train(args: CDTTrainConfig):\n\t    # update config\n\t    cfg, old_cfg = asdict(args), asdict(CDTTrainConfig())\n\t    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n\t    cfg = asdict(CDT_DEFAULT_CONFIG[args.task]())\n\t    cfg.update(differing_values)\n\t    args = types.SimpleNamespace(**cfg)\n\t    # setup logger\n\t    default_cfg = asdict(CDT_DEFAULT_CONFIG[args.task]())\n", "    if args.name is None:\n\t        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n\t    if args.group is None:\n\t        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n\t    if args.logdir is not None:\n\t        args.logdir = os.path.join(args.logdir, args.group, args.name)\n\t    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n\t    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n\t    logger.save_config(cfg, verbose=args.verbose)\n\t    # set seed\n", "    seed_all(args.seed)\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    # initialize environment\n\t    env = gym.make(args.task)\n\t    # pre-process offline dataset\n\t    data = env.get_dataset()\n\t    env.set_target_cost(args.cost_limit)\n\t    cbins, rbins, max_npb, min_npb = None, None, None, None\n\t    if args.density != 1.0:\n", "        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n\t        cbins = density_cfg[\"cbins\"]\n\t        rbins = density_cfg[\"rbins\"]\n\t        max_npb = density_cfg[\"max_npb\"]\n\t        min_npb = density_cfg[\"min_npb\"]\n\t    data = env.pre_process_data(data,\n\t                                args.outliers_percent,\n\t                                args.noise_scale,\n\t                                args.inpaint_ranges,\n\t                                args.epsilon,\n", "                                args.density,\n\t                                cbins=cbins,\n\t                                rbins=rbins,\n\t                                max_npb=max_npb,\n\t                                min_npb=min_npb)\n\t    # wrapper\n\t    env = wrap_env(\n\t        env=env,\n\t        reward_scale=args.reward_scale,\n\t    )\n", "    env = OfflineEnvWrapper(env)\n\t    # model & optimizer & scheduler setup\n\t    model = CDT(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n\t        embedding_dim=args.embedding_dim,\n\t        seq_len=args.seq_len,\n\t        episode_len=args.episode_len,\n\t        num_layers=args.num_layers,\n", "        num_heads=args.num_heads,\n\t        attention_dropout=args.attention_dropout,\n\t        residual_dropout=args.residual_dropout,\n\t        embedding_dropout=args.embedding_dropout,\n\t        time_emb=args.time_emb,\n\t        use_rew=args.use_rew,\n\t        use_cost=args.use_cost,\n\t        cost_transform=args.cost_transform,\n\t        add_cost_feat=args.add_cost_feat,\n\t        mul_cost_feat=args.mul_cost_feat,\n", "        cat_cost_feat=args.cat_cost_feat,\n\t        action_head_layers=args.action_head_layers,\n\t        cost_prefix=args.cost_prefix,\n\t        stochastic=args.stochastic,\n\t        init_temperature=args.init_temperature,\n\t        target_entropy=-env.action_space.shape[0],\n\t    ).to(args.device)\n\t    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\t    def checkpoint_fn():\n\t        return {\"model_state\": model.state_dict()}\n", "    logger.setup_checkpoint_fn(checkpoint_fn)\n\t    # trainer\n\t    trainer = CDTTrainer(model,\n\t                         env,\n\t                         logger=logger,\n\t                         learning_rate=args.learning_rate,\n\t                         weight_decay=args.weight_decay,\n\t                         betas=args.betas,\n\t                         clip_grad=args.clip_grad,\n\t                         lr_warmup_steps=args.lr_warmup_steps,\n", "                         reward_scale=args.reward_scale,\n\t                         cost_scale=args.cost_scale,\n\t                         loss_cost_weight=args.loss_cost_weight,\n\t                         loss_state_weight=args.loss_state_weight,\n\t                         cost_reverse=args.cost_reverse,\n\t                         no_entropy=args.no_entropy,\n\t                         device=args.device)\n\t    ct = lambda x: 70 - x if args.linear else 1 / (x + 10)\n\t    dataset = SequenceDataset(\n\t        data,\n", "        seq_len=args.seq_len,\n\t        reward_scale=args.reward_scale,\n\t        cost_scale=args.cost_scale,\n\t        deg=args.deg,\n\t        pf_sample=args.pf_sample,\n\t        max_rew_decrease=args.max_rew_decrease,\n\t        beta=args.beta,\n\t        augment_percent=args.augment_percent,\n\t        cost_reverse=args.cost_reverse,\n\t        max_reward=args.max_reward,\n", "        min_reward=args.min_reward,\n\t        pf_only=args.pf_only,\n\t        rmin=args.rmin,\n\t        cost_bins=args.cost_bins,\n\t        npb=args.npb,\n\t        cost_sample=args.cost_sample,\n\t        cost_transform=ct,\n\t        start_sampling=args.start_sampling,\n\t        prob=args.prob,\n\t        random_aug=args.random_aug,\n", "        aug_rmin=args.aug_rmin,\n\t        aug_rmax=args.aug_rmax,\n\t        aug_cmin=args.aug_cmin,\n\t        aug_cmax=args.aug_cmax,\n\t        cgap=args.cgap,\n\t        rstd=args.rstd,\n\t        cstd=args.cstd,\n\t    )\n\t    trainloader = DataLoader(\n\t        dataset,\n", "        batch_size=args.batch_size,\n\t        pin_memory=True,\n\t        num_workers=args.num_workers,\n\t    )\n\t    trainloader_iter = iter(trainloader)\n\t    # for saving the best\n\t    best_reward = -np.inf\n\t    best_cost = np.inf\n\t    best_idx = 0\n\t    for step in trange(args.update_steps, desc=\"Training\"):\n", "        batch = next(trainloader_iter)\n\t        states, actions, returns, costs_return, time_steps, mask, episode_cost, costs = [\n\t            b.to(args.device) for b in batch\n\t        ]\n\t        trainer.train_one_step(states, actions, returns, costs_return, time_steps, mask,\n\t                               episode_cost, costs)\n\t        # evaluation\n\t        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n\t            average_reward, average_cost = [], []\n\t            log_cost, log_reward, log_len = {}, {}, {}\n", "            for target_return in args.target_returns:\n\t                reward_return, cost_return = target_return\n\t                if args.cost_reverse:\n\t                    # critical step, rescale the return!\n\t                    ret, cost, length = trainer.evaluate(\n\t                        args.eval_episodes, reward_return * args.reward_scale,\n\t                        (args.episode_len - cost_return) * args.cost_scale)\n\t                else:\n\t                    ret, cost, length = trainer.evaluate(\n\t                        args.eval_episodes, reward_return * args.reward_scale,\n", "                        cost_return * args.cost_scale)\n\t                average_cost.append(cost)\n\t                average_reward.append(ret)\n\t                name = \"c_\" + str(int(cost_return)) + \"_r_\" + str(int(reward_return))\n\t                log_cost.update({name: cost})\n\t                log_reward.update({name: ret})\n\t                log_len.update({name: length})\n\t            logger.store(tab=\"cost\", **log_cost)\n\t            logger.store(tab=\"ret\", **log_reward)\n\t            logger.store(tab=\"length\", **log_len)\n", "            # save the current weight\n\t            logger.save_checkpoint()\n\t            # save the best weight\n\t            mean_ret = np.mean(average_reward)\n\t            mean_cost = np.mean(average_cost)\n\t            if mean_cost < best_cost or (mean_cost == best_cost\n\t                                         and mean_ret > best_reward):\n\t                best_cost = mean_cost\n\t                best_reward = mean_ret\n\t                best_idx = step\n", "                logger.save_checkpoint(suffix=\"best\")\n\t            logger.store(tab=\"train\", best_idx=best_idx)\n\t            logger.write(step, display=False)\n\t        else:\n\t            logger.write_without_reset(step)\n\tif __name__ == \"__main__\":\n\t    train()\n"]}
{"filename": "examples/train/train_coptidice.py", "chunked_list": ["import os\n\timport uuid\n\timport types\n\tfrom dataclasses import asdict, dataclass\n\tfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\timport bullet_safety_gym  # noqa\n\timport dsrl\n\timport gymnasium as gym  # noqa\n\timport numpy as np\n\timport pyrallis\n", "import torch\n\tfrom dsrl.infos import DENSITY_CFG\n\tfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\n\tfrom fsrl.utils import WandbLogger\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm.auto import trange  # noqa\n\tfrom examples.configs.coptidice_configs import (COptiDICE_DEFAULT_CONFIG,\n\t                                                COptiDICETrainConfig)\n\tfrom osrl.algorithms import COptiDICE, COptiDICETrainer\n\tfrom osrl.common import TransitionDataset\n", "from osrl.common.exp_util import auto_name, seed_all\n\t@pyrallis.wrap()\n\tdef train(args: COptiDICETrainConfig):\n\t    # update config\n\t    cfg, old_cfg = asdict(args), asdict(COptiDICETrainConfig())\n\t    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n\t    cfg = asdict(COptiDICE_DEFAULT_CONFIG[args.task]())\n\t    cfg.update(differing_values)\n\t    args = types.SimpleNamespace(**cfg)\n\t    # setup logger\n", "    default_cfg = asdict(COptiDICE_DEFAULT_CONFIG[args.task]())\n\t    if args.name is None:\n\t        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n\t    if args.group is None:\n\t        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n\t    if args.logdir is not None:\n\t        args.logdir = os.path.join(args.logdir, args.group, args.name)\n\t    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n\t    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n\t    logger.save_config(cfg, verbose=args.verbose)\n", "    # set seed\n\t    seed_all(args.seed)\n\t    if args.device == \"cpu\":\n\t        torch.set_num_threads(args.threads)\n\t    # initialize environment\n\t    env = gym.make(args.task)\n\t    # pre-process offline dataset\n\t    data = env.get_dataset()\n\t    env.set_target_cost(args.cost_limit)\n\t    cbins, rbins, max_npb, min_npb = None, None, None, None\n", "    if args.density != 1.0:\n\t        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n\t        cbins = density_cfg[\"cbins\"]\n\t        rbins = density_cfg[\"rbins\"]\n\t        max_npb = density_cfg[\"max_npb\"]\n\t        min_npb = density_cfg[\"min_npb\"]\n\t    data = env.pre_process_data(data,\n\t                                args.outliers_percent,\n\t                                args.noise_scale,\n\t                                args.inpaint_ranges,\n", "                                args.epsilon,\n\t                                args.density,\n\t                                cbins=cbins,\n\t                                rbins=rbins,\n\t                                max_npb=max_npb,\n\t                                min_npb=min_npb)\n\t    # wrapper\n\t    env = wrap_env(\n\t        env=env,\n\t        reward_scale=args.reward_scale,\n", "    )\n\t    env = OfflineEnvWrapper(env)\n\t    # setup dataset\n\t    dataset = TransitionDataset(data,\n\t                                reward_scale=args.reward_scale,\n\t                                cost_scale=args.cost_scale,\n\t                                state_init=True)\n\t    trainloader = DataLoader(\n\t        dataset,\n\t        batch_size=args.batch_size,\n", "        pin_memory=True,\n\t        num_workers=args.num_workers,\n\t    )\n\t    trainloader_iter = iter(trainloader)\n\t    init_s_propotion, obs_std, act_std = dataset.get_dataset_states()\n\t    # setup model\n\t    model = COptiDICE(\n\t        state_dim=env.observation_space.shape[0],\n\t        action_dim=env.action_space.shape[0],\n\t        max_action=env.action_space.high[0],\n", "        f_type=args.f_type,\n\t        init_state_propotion=init_s_propotion,\n\t        observations_std=obs_std,\n\t        actions_std=act_std,\n\t        a_hidden_sizes=args.a_hidden_sizes,\n\t        c_hidden_sizes=args.c_hidden_sizes,\n\t        gamma=args.gamma,\n\t        alpha=args.alpha,\n\t        cost_ub_epsilon=args.cost_ub_epsilon,\n\t        num_nu=args.num_nu,\n", "        num_chi=args.num_chi,\n\t        cost_limit=args.cost_limit,\n\t        episode_len=args.episode_len,\n\t        device=args.device,\n\t    )\n\t    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\t    def checkpoint_fn():\n\t        return {\"model_state\": model.state_dict()}\n\t    logger.setup_checkpoint_fn(checkpoint_fn)\n\t    trainer = COptiDICETrainer(model,\n", "                               env,\n\t                               logger=logger,\n\t                               actor_lr=args.actor_lr,\n\t                               critic_lr=args.critic_lr,\n\t                               scalar_lr=args.scalar_lr,\n\t                               reward_scale=args.reward_scale,\n\t                               cost_scale=args.cost_scale,\n\t                               device=args.device)\n\t    # for saving the best\n\t    best_reward = -np.inf\n", "    best_cost = np.inf\n\t    best_idx = 0\n\t    for step in trange(args.update_steps, desc=\"Training\"):\n\t        batch = next(trainloader_iter)\n\t        batch = [b.to(args.device) for b in batch]\n\t        trainer.train_one_step(batch)\n\t        # evaluation\n\t        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n\t            ret, cost, length = trainer.evaluate(args.eval_episodes)\n\t            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n", "            # save the current weight\n\t            logger.save_checkpoint()\n\t            # save the best weight\n\t            if cost < best_cost or (cost == best_cost and ret > best_reward):\n\t                best_cost = cost\n\t                best_reward = ret\n\t                best_idx = step\n\t                logger.save_checkpoint(suffix=\"best\")\n\t            logger.store(tab=\"train\", best_idx=best_idx)\n\t            logger.write(step, display=False)\n", "        else:\n\t            logger.write_without_reset(step)\n\tif __name__ == \"__main__\":\n\t    train()\n"]}
